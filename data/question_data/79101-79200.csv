,title_raw,title_text,title_latex,body_raw,body_text,body_latex,tags
0,Note on Ring Homomorphisms of Matrices Rings,Note on Ring Homomorphisms of Matrices Rings,,"Assume that $\mathbb{F}$ is a field, and let $\mathbb{M}_{t}\left( \mathbb{F}\right)  $ be the ring of matrices of order $t$ over $\mathbb{F}$. Does there exist a non-trivial ring homomorphism from $\mathbb{M}_{n+1}\left( \mathbb{F}\right)  $ to $\mathbb{M}_{n}\left(  \mathbb{F}\right)  $?","Assume that $\mathbb{F}$ is a field, and let $\mathbb{M}_{t}\left( \mathbb{F}\right)  $ be the ring of matrices of order $t$ over $\mathbb{F}$. Does there exist a non-trivial ring homomorphism from $\mathbb{M}_{n+1}\left( \mathbb{F}\right)  $ to $\mathbb{M}_{n}\left(  \mathbb{F}\right)  $?",,"['matrices', 'ring-theory', 'field-theory']"
1,Derivative of $W^TW$ w.r.t $W$,Derivative of  w.r.t,W^TW W,"I am trying to find the following derivative: $$ \frac{\partial}{\partial W}W^TW $$ where $W \in \mathbb{R}^{n\times m}$ is a matrix. Also I am interested in finding the associated $$ \frac{\partial}{\partial W}\|W^TW\|_\mathcal{F}^2 $$ I am aware of the fact that $$ \frac{\partial}{\partial X}\|X\|_\mathcal{F}^2 = \frac{\partial}{\partial X}Tr(XX^T) = 2X $$ But I am not sure if the derivation in terms of $W$ is possible at all. Please advise. Thank you very much.","I am trying to find the following derivative: $$ \frac{\partial}{\partial W}W^TW $$ where $W \in \mathbb{R}^{n\times m}$ is a matrix. Also I am interested in finding the associated $$ \frac{\partial}{\partial W}\|W^TW\|_\mathcal{F}^2 $$ I am aware of the fact that $$ \frac{\partial}{\partial X}\|X\|_\mathcal{F}^2 = \frac{\partial}{\partial X}Tr(XX^T) = 2X $$ But I am not sure if the derivation in terms of $W$ is possible at all. Please advise. Thank you very much.",,"['calculus', 'matrices', 'numerical-methods']"
2,Why PA=LU matrix factorization is better than A=LU matrix factorization?,Why PA=LU matrix factorization is better than A=LU matrix factorization?,,While finding A=LU for a matrix if zero is encountered in pivot position then row exchange is required. However if PA=LU form is used then no row exchange is required and apparently this also requires less computation. What I am not able to understand it that how finding a correct permutation matrix involves less efforts that doing a row exchange during A=LU process? Edit: Matrix PA will already have a form in which all rows are in correct order. what I am not able to understand is that why PA=LU computation is going to be better than A=LU computation?,While finding A=LU for a matrix if zero is encountered in pivot position then row exchange is required. However if PA=LU form is used then no row exchange is required and apparently this also requires less computation. What I am not able to understand it that how finding a correct permutation matrix involves less efforts that doing a row exchange during A=LU process? Edit: Matrix PA will already have a form in which all rows are in correct order. what I am not able to understand is that why PA=LU computation is going to be better than A=LU computation?,,"['linear-algebra', 'matrices']"
3,Inverse of an upper-left triangular (partitioned) matrix,Inverse of an upper-left triangular (partitioned) matrix,,"I'd appreciate help finding the inverse of the upper-left triangular (partitioned) matrix $$     \left[         \begin{array}{ll}             \mathbf{K} & \mathbf{P} \\             \mathbf{P}^T    & \mathbf{0}         \end{array}     \right] $$ This matrix occurs frequently in scattered data interpolation with radial basis functions. If it matters, $\mathbf{K}$ is a square matrix, $\mathbf{P}$ is generally not square and $\mathbf{0}$ is the zero matrix.","I'd appreciate help finding the inverse of the upper-left triangular (partitioned) matrix $$     \left[         \begin{array}{ll}             \mathbf{K} & \mathbf{P} \\             \mathbf{P}^T    & \mathbf{0}         \end{array}     \right] $$ This matrix occurs frequently in scattered data interpolation with radial basis functions. If it matters, $\mathbf{K}$ is a square matrix, $\mathbf{P}$ is generally not square and $\mathbf{0}$ is the zero matrix.",,['matrices']
4,Optimizing Cholesky factorization for multiple sparse matrices with same nonzero pattern,Optimizing Cholesky factorization for multiple sparse matrices with same nonzero pattern,,"I'm using a Cholesky factorization to solve the linear step in a nonlinear system of equations (nonlinear finite element analysis). In the PETSc library, one can specify a parameter for SAME_NONZERO_PATTERN during successive solves. The speedup is good, almost too good (100 times faster for the second solve). This makes me wonder, what sort of optimization one can do for Cholesky factorizations when solving for many sparse matrices with same nonzero pattern (but with different, possibly similar, values)?","I'm using a Cholesky factorization to solve the linear step in a nonlinear system of equations (nonlinear finite element analysis). In the PETSc library, one can specify a parameter for SAME_NONZERO_PATTERN during successive solves. The speedup is good, almost too good (100 times faster for the second solve). This makes me wonder, what sort of optimization one can do for Cholesky factorizations when solving for many sparse matrices with same nonzero pattern (but with different, possibly similar, values)?",,"['matrices', 'numerical-methods', 'numerical-linear-algebra']"
5,Investigations about the trace form,Investigations about the trace form,,"Let us ( again ) consider the bilinear form $\beta(A,B)=\operatorname{Tr}(AB)$ for $A,B \in \mathbb{F}^{n,n}$ (quadratic matrices over a field $\mathbb{F}$). I am interested in finding the biggest subspace $U \subset \mathbb{F}^{n,n}$ such that for all $A \in U: \beta(A,A)=\operatorname{Tr}(A^2)=0$.","Let us ( again ) consider the bilinear form $\beta(A,B)=\operatorname{Tr}(AB)$ for $A,B \in \mathbb{F}^{n,n}$ (quadratic matrices over a field $\mathbb{F}$). I am interested in finding the biggest subspace $U \subset \mathbb{F}^{n,n}$ such that for all $A \in U: \beta(A,A)=\operatorname{Tr}(A^2)=0$.",,"['linear-algebra', 'matrices']"
6,Derivative of inverse quadratic function of a matrix,Derivative of inverse quadratic function of a matrix,,"I have been stuck with the following derivative for some time: $$ \frac{\partial\,\mathbf{b}^\mathrm{T}(\mathbf{X}\mathbf{C}\mathbf{X}^\mathrm{T})^{-1}\mathbf{b}}{\partial\,\mathbf{X}} $$, where $\mathbf{b}\in\mathbb{R}^{M\times1}$, $\mathbf{X}\in\mathbb{R}^{M\times N}$ and $\mathbf{C}\in\mathbb{R}^{N\times N}$ and $\mathbf{C}$ is symmetric. I had a look in the Matrix Cookbook , but I am still not sure how to deal with the inverse of a matrix in the second order form. Is it correct to apply the chain rule? $$\frac{\partial\,\mathbf{b}^\mathrm{T}(\mathbf{X}\mathbf{C}\mathbf{X}^\mathrm{T})^{-1}\mathbf{b}}{\partial\,\mathbf{X}} =  \frac{\partial\,\mathbf{b}^\mathrm{T}(\mathbf{X}\mathbf{C}\mathbf{X}^\mathrm{T})^{-1}\mathbf{b}}{\partial\,\mathbf{XCX}^\mathrm{T}}\cdot  \frac{\partial \, \mathbf{XCX}^{\mathrm{T}}}{\partial \, \mathbf{X}}.$$ In this case, the first partial derivative will be:  $$ \frac{\partial\,\mathbf{b}^\mathrm{T}(\mathbf{X}\mathbf{C}\mathbf{X}^\mathrm{T})^{-1}\mathbf{b}}{\partial\,\mathbf{XCX}^\mathrm{T}} =  -(\mathbf{X}\mathbf{C}\mathbf{X}^\mathrm{T})^\mathrm{-T}\mathbf{b}\mathbf{b}^\mathrm{T}(\mathbf{X}\mathbf{C}\mathbf{X}^\mathrm{T})^{-\mathrm{T}} $$  (using Eq. 55, from 1 ). The second part, $\frac{\partial \, \mathbf{XCX}^{\mathrm{T}}}{\partial \, \mathbf{X}}$, will be similar to a fourth-rank tensor. How can I arrive at a result that is a $M\times N $ matrix? I would really appreciate if someone could help me with this or provide some piece of advice.","I have been stuck with the following derivative for some time: $$ \frac{\partial\,\mathbf{b}^\mathrm{T}(\mathbf{X}\mathbf{C}\mathbf{X}^\mathrm{T})^{-1}\mathbf{b}}{\partial\,\mathbf{X}} $$, where $\mathbf{b}\in\mathbb{R}^{M\times1}$, $\mathbf{X}\in\mathbb{R}^{M\times N}$ and $\mathbf{C}\in\mathbb{R}^{N\times N}$ and $\mathbf{C}$ is symmetric. I had a look in the Matrix Cookbook , but I am still not sure how to deal with the inverse of a matrix in the second order form. Is it correct to apply the chain rule? $$\frac{\partial\,\mathbf{b}^\mathrm{T}(\mathbf{X}\mathbf{C}\mathbf{X}^\mathrm{T})^{-1}\mathbf{b}}{\partial\,\mathbf{X}} =  \frac{\partial\,\mathbf{b}^\mathrm{T}(\mathbf{X}\mathbf{C}\mathbf{X}^\mathrm{T})^{-1}\mathbf{b}}{\partial\,\mathbf{XCX}^\mathrm{T}}\cdot  \frac{\partial \, \mathbf{XCX}^{\mathrm{T}}}{\partial \, \mathbf{X}}.$$ In this case, the first partial derivative will be:  $$ \frac{\partial\,\mathbf{b}^\mathrm{T}(\mathbf{X}\mathbf{C}\mathbf{X}^\mathrm{T})^{-1}\mathbf{b}}{\partial\,\mathbf{XCX}^\mathrm{T}} =  -(\mathbf{X}\mathbf{C}\mathbf{X}^\mathrm{T})^\mathrm{-T}\mathbf{b}\mathbf{b}^\mathrm{T}(\mathbf{X}\mathbf{C}\mathbf{X}^\mathrm{T})^{-\mathrm{T}} $$  (using Eq. 55, from 1 ). The second part, $\frac{\partial \, \mathbf{XCX}^{\mathrm{T}}}{\partial \, \mathbf{X}}$, will be similar to a fourth-rank tensor. How can I arrive at a result that is a $M\times N $ matrix? I would really appreciate if someone could help me with this or provide some piece of advice.",,"['linear-algebra', 'matrices']"
7,Completing the squares for matrices,Completing the squares for matrices,,"I have something like this $$X^T A^T A X - X^T A^T B - B^T A X$$ or $$X^T A^T A X -2 X^T A^T B$$ since X and B are really vectors ($X^TA^TB=B^TAX$). I’d like to find the X that minimizes the expression. To do that I want to have something like this $$[X-(A^TA)^{-1}A^TB]^T(A^TA)[X-(A^TA)^{-1}A^TB]-B^TA(A^TA)^{-1}A^TB$$ so that the X that minimizes the expression is $(A^TA)^{-1}A^TB$. My question is: how does all this relate to the technique of completing the squares? How can I show the squares? To further explain my question, it’s easy to say, if you have $a^2 + 2ab$ just add and subtract $b^2$. But how can I abstract that to matrices? PS: I assume $A^TA$ is non singular, positive definite.","I have something like this $$X^T A^T A X - X^T A^T B - B^T A X$$ or $$X^T A^T A X -2 X^T A^T B$$ since X and B are really vectors ($X^TA^TB=B^TAX$). I’d like to find the X that minimizes the expression. To do that I want to have something like this $$[X-(A^TA)^{-1}A^TB]^T(A^TA)[X-(A^TA)^{-1}A^TB]-B^TA(A^TA)^{-1}A^TB$$ so that the X that minimizes the expression is $(A^TA)^{-1}A^TB$. My question is: how does all this relate to the technique of completing the squares? How can I show the squares? To further explain my question, it’s easy to say, if you have $a^2 + 2ab$ just add and subtract $b^2$. But how can I abstract that to matrices? PS: I assume $A^TA$ is non singular, positive definite.",,"['linear-algebra', 'matrices']"
8,Apply a fraction of a rotation matrix without extracting axis-angle,Apply a fraction of a rotation matrix without extracting axis-angle,,"In 3D, you can take any pure rotation matrix and find an axis-angle representation of the same transformation (although not necessarily unique).  From that representation, you could create a new matrix that represents a fractional part of the original transformation by taking a fraction of the original angle, keeping the original axis, and creating a new matrix. In high-dimensional space, axis-angle doesn't make sense since it seems that rotation actually happens within a plane and not necessarily around an axis per se.  Is it possible to find a rotation matrix that rotates a space by some fraction of the angle by which some other rotation matrix rotates without first finding the plane and angle of rotation?","In 3D, you can take any pure rotation matrix and find an axis-angle representation of the same transformation (although not necessarily unique).  From that representation, you could create a new matrix that represents a fractional part of the original transformation by taking a fraction of the original angle, keeping the original axis, and creating a new matrix. In high-dimensional space, axis-angle doesn't make sense since it seems that rotation actually happens within a plane and not necessarily around an axis per se.  Is it possible to find a rotation matrix that rotates a space by some fraction of the angle by which some other rotation matrix rotates without first finding the plane and angle of rotation?",,"['linear-algebra', 'matrices', 'rotations']"
9,What is the signature of a matrix?,What is the signature of a matrix?,,"Thank you very much! Some webpages say that the signature of a symmetric real matrix is an integer which equals to the number of positive eigenvalues minus the number of negative ones. However, I am confused by one problem (Problem 4.1.23, Sp81 on Berkeley Problems in Mathematics): The set of real $3 \times 3$ symmetric matrices is a real, finite-dimensional vector space isomorphic to $\mathbb{R}^6$. Show that the subset of such matrices of signature $(2,1)$ is an open connected subspace in the usual topology on $\mathbb{R}^6$. So, what are matrices ""of signature $(2,1)$"", and what is the signature of a matrix? Many thanks!","Thank you very much! Some webpages say that the signature of a symmetric real matrix is an integer which equals to the number of positive eigenvalues minus the number of negative ones. However, I am confused by one problem (Problem 4.1.23, Sp81 on Berkeley Problems in Mathematics): The set of real $3 \times 3$ symmetric matrices is a real, finite-dimensional vector space isomorphic to $\mathbb{R}^6$. Show that the subset of such matrices of signature $(2,1)$ is an open connected subspace in the usual topology on $\mathbb{R}^6$. So, what are matrices ""of signature $(2,1)$"", and what is the signature of a matrix? Many thanks!",,['matrices']
10,matrix properties if all the solutions are the same,matrix properties if all the solutions are the same,,"Assume I have linear equation system in $n$ variables and the solution is $x_1=x_2=\cdots = x_n$. What properties must the matrix of the system have, or how do I prove that the system has such a solution?","Assume I have linear equation system in $n$ variables and the solution is $x_1=x_2=\cdots = x_n$. What properties must the matrix of the system have, or how do I prove that the system has such a solution?",,['linear-algebra']
11,do Householder reflections describe all reflections?,do Householder reflections describe all reflections?,,"I'm familiar with Householder reflections ; they are a simple transformation that, given a normal vector, describes reflection in the hyperplane perpendicular to that vector. But do Householder reflections describe the complete space of matrices describing reflection about the origin, or are there other reflections that cannot be described by Householder reflections?","I'm familiar with Householder reflections ; they are a simple transformation that, given a normal vector, describes reflection in the hyperplane perpendicular to that vector. But do Householder reflections describe the complete space of matrices describing reflection about the origin, or are there other reflections that cannot be described by Householder reflections?",,"['linear-algebra', 'matrices', 'reflection']"
12,How to get inverse of this matrix using least amount of space?,How to get inverse of this matrix using least amount of space?,,"I'm working on a problem from a past exam and I'm stuck, so I'm asking for help. Here it is:  $A = \frac12 \left[\begin{array}{rrrr} 1 &  1 &  1 &  1 \\ 1 &  1 & -1 & -1 \\ 1 & -1 &  1 & -1 \\ 1 & -1 & -1 &  1 \end{array}\right]$ find  $\mathbf A^{-1}$. My problem isn't the inverse matrix itself. We just get the determinant, see if it's zero or not, get the adjoint matrix and divide it by determinant. My problem is space. As you can see, it's a 4x4 matrix meaning that I'd have to do 4x4 3x3 determinants to get the adjoint matrix plus 2 3x3 determinants to get determinant of the matrix. Now we get one A3 piece of paper for 6 problems. The problems are printed on one side and the other side is blank. This and the fact that inverse matrix is $A = \frac12 \left[\begin{array}{rrrr} 1 &  1 &  1 &  1 \\ 1 &  1 & -1 & -1 \\ 1 & -1 &  1 & -1 \\ 1 & -1 & -1 &  1 \end{array}\right]$ led me to believe that there's some catch that I do not see. Any ideas what could it be? Also if someone could edit these matrices from MATLAB format into something that this site will parse would be great! EDIT Unfortunately it seem that TeX code for matrices doesn't work here. Here's the matrix in MATLAB form, if anyone wants it A=(1/2)*[1,1,1,1;1,1,-1,-1;1,-1,1,-1;1,-1,-1,1]; EDIT 2 Answer by Jack Schmidt contains code for matrices.","I'm working on a problem from a past exam and I'm stuck, so I'm asking for help. Here it is:  $A = \frac12 \left[\begin{array}{rrrr} 1 &  1 &  1 &  1 \\ 1 &  1 & -1 & -1 \\ 1 & -1 &  1 & -1 \\ 1 & -1 & -1 &  1 \end{array}\right]$ find  $\mathbf A^{-1}$. My problem isn't the inverse matrix itself. We just get the determinant, see if it's zero or not, get the adjoint matrix and divide it by determinant. My problem is space. As you can see, it's a 4x4 matrix meaning that I'd have to do 4x4 3x3 determinants to get the adjoint matrix plus 2 3x3 determinants to get determinant of the matrix. Now we get one A3 piece of paper for 6 problems. The problems are printed on one side and the other side is blank. This and the fact that inverse matrix is $A = \frac12 \left[\begin{array}{rrrr} 1 &  1 &  1 &  1 \\ 1 &  1 & -1 & -1 \\ 1 & -1 &  1 & -1 \\ 1 & -1 & -1 &  1 \end{array}\right]$ led me to believe that there's some catch that I do not see. Any ideas what could it be? Also if someone could edit these matrices from MATLAB format into something that this site will parse would be great! EDIT Unfortunately it seem that TeX code for matrices doesn't work here. Here's the matrix in MATLAB form, if anyone wants it A=(1/2)*[1,1,1,1;1,1,-1,-1;1,-1,1,-1;1,-1,-1,1]; EDIT 2 Answer by Jack Schmidt contains code for matrices.",,"['matrices', 'inverse']"
13,8 people seated around a circular table. Each person has a connection value with each other person. How do you arrange the table to maximize?,8 people seated around a circular table. Each person has a connection value with each other person. How do you arrange the table to maximize?,,"8 people total.  Seated around a circular table.  Hibachi dinner?  You can only talk to the 2 people seated directly next to you. Each person has a connection value with each other person.  eg : #1 and #3  are friendly and have a ""conversation value"" of 7.  #1 and #2 are not friends, and have a ""conversation value"" of 3. Picture a star diagram to represent all connections between all people.  How do you represent this data?  Matrix?    How do you arrange the table to maximize the seating arrangement to result in the highest total connection value? How do I actually solve an example scenario with actual sample values? How do I store this data? How do I solve the maximization?","8 people total.  Seated around a circular table.  Hibachi dinner?  You can only talk to the 2 people seated directly next to you. Each person has a connection value with each other person.  eg : #1 and #3  are friendly and have a ""conversation value"" of 7.  #1 and #2 are not friends, and have a ""conversation value"" of 3. Picture a star diagram to represent all connections between all people.  How do you represent this data?  Matrix?    How do you arrange the table to maximize the seating arrangement to result in the highest total connection value? How do I actually solve an example scenario with actual sample values? How do I store this data? How do I solve the maximization?",,"['matrices', 'optimization', 'discrete-optimization']"
14,A matrix manipulation from Berezin's paper,A matrix manipulation from Berezin's paper,,"I am reading a paper by Berezin, entitled General Concept of Quantization . He writes: and then: This ought to be some clever manipulation of matrices and yet I am not able to show how (1.4) follows from (1.1) if $\omega$ is given to be invertible. Any leads?","I am reading a paper by Berezin, entitled General Concept of Quantization . He writes: and then: This ought to be some clever manipulation of matrices and yet I am not able to show how (1.4) follows from (1.1) if is given to be invertible. Any leads?",\omega,"['matrices', 'partial-derivative', 'matrix-calculus']"
15,For an endomorphism $f:V\to V$ the set of fixed points of $f$ is defined as Fix($f$) $= \{v\in V:f(v)=v\}$ Show Fix$(f)\subseteq V$ is subspace of $V$,For an endomorphism  the set of fixed points of  is defined as Fix()  Show Fix is subspace of,f:V\to V f f = \{v\in V:f(v)=v\} (f)\subseteq V V,"For an endomorphism $f: V \to V$ the set of fixed points of $f$ is defined as Fix( $f$ ) $= \{ v \in V: f(v) = v \} $ Show that Fix( $f$ ) $\subseteq V$ is a subspace of $V$ . To be a subspace, we must have the zero vector. As far as I know, the zero vector is always a fixed point because $f(0) = 0$ . We also need to have that it is closed under addition and scalar multiplication. After reading on Wikipedia, it seems an endomorphism is linear. https://en.wikipedia.org/wiki/Endomorphism That means we can use $f(v + u) = f(v) + f(u)$ because as said it is a linear map. If it is linear, we can also use $f(\alpha v) = \alpha f(v)$ . Because they are fixed points, we have $f(v) = v, f(u) = u$ . So we have $f(v + u) = f(v) + f(u) = v + u$ and $f(\alpha v) = \alpha f(v) = \alpha v$ . So $v + u$ and $\alpha v$ are also fixed points, so it is closed under addition and scalar multiplication. Is this correct ? I need your feedback because we have no solutions so I have no idea if what I did is even correct EDIT: I was actually especially unsure about the endomorphism part. Because if we use the linear property of endomorphism, then this seems too easy. We are basically just referring to a property of the function, it seems to me we aren't directly proving something. I should have highlighted that step better. But it is now clear with the answer and comments. Thank you.","For an endomorphism the set of fixed points of is defined as Fix( ) Show that Fix( ) is a subspace of . To be a subspace, we must have the zero vector. As far as I know, the zero vector is always a fixed point because . We also need to have that it is closed under addition and scalar multiplication. After reading on Wikipedia, it seems an endomorphism is linear. https://en.wikipedia.org/wiki/Endomorphism That means we can use because as said it is a linear map. If it is linear, we can also use . Because they are fixed points, we have . So we have and . So and are also fixed points, so it is closed under addition and scalar multiplication. Is this correct ? I need your feedback because we have no solutions so I have no idea if what I did is even correct EDIT: I was actually especially unsure about the endomorphism part. Because if we use the linear property of endomorphism, then this seems too easy. We are basically just referring to a property of the function, it seems to me we aren't directly proving something. I should have highlighted that step better. But it is now clear with the answer and comments. Thank you.","f: V \to V f f = \{ v \in V: f(v) = v \}  f \subseteq V V f(0) = 0 f(v + u) = f(v) + f(u) f(\alpha v) = \alpha f(v) f(v) = v, f(u) = u f(v + u) = f(v) + f(u) = v + u f(\alpha v) = \alpha f(v) = \alpha v v + u \alpha v","['linear-algebra', 'matrices', 'functions', 'vector-spaces', 'vectors']"
16,Conjecture about determinant of matrix with column $j$ with all numbers not divisible by $j+1$,Conjecture about determinant of matrix with column  with all numbers not divisible by,j j+1,"Consider an $n \times n$ matrix $A_n$ with elements $a_{i,j}$ such that $a_{1,j},a_{2,j},a_{3,j},\ldots$ is the sequence of numbers not divisible by $j+1$ in increasing order starting from $1$ (e.g. $1,2,4,5,7,8,10,\ldots$ for $j = 2$ ). For fun, I have computed the determinant for $n \le 8$ and then conjectured that: $$\lvert A_n \rvert = (-1)^{n+1}$$ Is the conjecture true? How would you prove it?","Consider an matrix with elements such that is the sequence of numbers not divisible by in increasing order starting from (e.g. for ). For fun, I have computed the determinant for and then conjectured that: Is the conjecture true? How would you prove it?","n \times n A_n a_{i,j} a_{1,j},a_{2,j},a_{3,j},\ldots j+1 1 1,2,4,5,7,8,10,\ldots j = 2 n \le 8 \lvert A_n \rvert = (-1)^{n+1}","['linear-algebra', 'matrices', 'elementary-number-theory', 'determinant', 'conjectures']"
17,Why does the semigroup of matrices form an epigroup?,Why does the semigroup of matrices form an epigroup?,,"An epigroup is a semigroup $S$ in which every element has a (positive) power that lies in a subgroup of $S$ . (The subgroup may depend on the element). Note that if $x^n\in G$ , where $G$ is a subgroup of $S$ , then $x^m\in G$ for all $m\ge n$ . It is stated on the wiki article on epigroups that: The semigroup of all matrices over a division ring is an epigroup. It seems to be taken directly from their source, The Concise Handbook of Algebra (Shevrin 2002), though I think what they mean must be the following (otherwise we can't talk of a semigroup at all): The semigroup of all square matrices of a given size over a division ring is an epigroup. Why is this true? I believe I have stumbled my way to a proof that works when the matrices are over an algebraically closed field, since this lets us use Jordan normal forms, but I don't know if it extends to even the case of general fields. Going from general fields to skew fields seems even harder, since this leaves the realm of standard linear algebra with which I'm familiar. I do suspect that there are nicer, higher-level approaches that don't use a concrete structure of the matrices like Jordan matrices. I would appreciate a more abstract proof. In my proof, we assume without loss of generality that $X$ is a Jordan matrix. This of course requires the existence of a Jordan normal form for any matrix, which is why I think I need an algebraically closed field. Let $n$ be the size of the largest Jordan block with eigenvalue $0$ . Then the nilpotent Jordan blocks of $X$ have vanished in $X^n$ , so we get a block matrix of the form $$ X^n = \begin{pmatrix}J_{\lambda_1}^n\\&\ddots\\&&J_{\lambda_k}^n\\&&&0\end{pmatrix}, $$ where $J_{\lambda_j}$ is a Jordan block of eigenvalue $\lambda_j\ne0$ . Let $E$ to be the diagonal matrix with $1$ 's at all the non-zero diagonal places of $X^n$ . Then $E$ is an idempotent, and $X^nE=X^n=EX^n$ . Furthermore, Jordan blocks are invertible (by the formula in this answer ), so if we take $$ Y=\begin{pmatrix}J_{\lambda_1}^{-n}\\&\ddots\\&&J_{\lambda_k}^{-n}\\&&&0\end{pmatrix}, $$ then $YX^n=E=X^nY$ , which shows that $X^n$ is included in a subgroup with identity $E$ . Translating the results to general (non-Jordan) $X$ is easy from here.","An epigroup is a semigroup in which every element has a (positive) power that lies in a subgroup of . (The subgroup may depend on the element). Note that if , where is a subgroup of , then for all . It is stated on the wiki article on epigroups that: The semigroup of all matrices over a division ring is an epigroup. It seems to be taken directly from their source, The Concise Handbook of Algebra (Shevrin 2002), though I think what they mean must be the following (otherwise we can't talk of a semigroup at all): The semigroup of all square matrices of a given size over a division ring is an epigroup. Why is this true? I believe I have stumbled my way to a proof that works when the matrices are over an algebraically closed field, since this lets us use Jordan normal forms, but I don't know if it extends to even the case of general fields. Going from general fields to skew fields seems even harder, since this leaves the realm of standard linear algebra with which I'm familiar. I do suspect that there are nicer, higher-level approaches that don't use a concrete structure of the matrices like Jordan matrices. I would appreciate a more abstract proof. In my proof, we assume without loss of generality that is a Jordan matrix. This of course requires the existence of a Jordan normal form for any matrix, which is why I think I need an algebraically closed field. Let be the size of the largest Jordan block with eigenvalue . Then the nilpotent Jordan blocks of have vanished in , so we get a block matrix of the form where is a Jordan block of eigenvalue . Let to be the diagonal matrix with 's at all the non-zero diagonal places of . Then is an idempotent, and . Furthermore, Jordan blocks are invertible (by the formula in this answer ), so if we take then , which shows that is included in a subgroup with identity . Translating the results to general (non-Jordan) is easy from here.","S S x^n\in G G S x^m\in G m\ge n X n 0 X X^n 
X^n = \begin{pmatrix}J_{\lambda_1}^n\\&\ddots\\&&J_{\lambda_k}^n\\&&&0\end{pmatrix},
 J_{\lambda_j} \lambda_j\ne0 E 1 X^n E X^nE=X^n=EX^n 
Y=\begin{pmatrix}J_{\lambda_1}^{-n}\\&\ddots\\&&J_{\lambda_k}^{-n}\\&&&0\end{pmatrix},
 YX^n=E=X^nY X^n E X","['linear-algebra', 'abstract-algebra', 'matrices', 'jordan-normal-form', 'semigroups']"
18,Finding the matrix of $f$ in the base $V$,Finding the matrix of  in the base,f V,"Problem . Let $V = {\vec{v_1} = (5, 3, 1), \vec{v_2}= (1, −3, −2), \vec{v_3} = (1, 2, 1)}$ base of $\mathbb{R^3}$ and let $f$ be the linear operator on $\mathbb{R^3}$ defined by $f(\vec{v_1}) = (−2, 1, 0)$ , $f(\vec{v_2}) = (−1, 3, 0)$ , $f(\vec{v_3}) = (−2, −3, 0)$ . Calculate the matrix of $f$ in the base $V$ ; How I was taught the lesson . Now the issue is that in my book and all of my notes they always define the matrices of change of base in this way: $$y'=F_v x'$$ for the matrix $F_v$ of $f$ in a base $V$ and the matrix $P$ to change basis from the canonical to the base $V$ : $$x'=Px$$ (I'm using ' for the base $V$ and no ' for the canonical). How I approached the problem . So I continued doing the problem: $$x'=Px\Rightarrow x=P^{-1} x'$$ since we were given the $y'$ in terms of $x$ (the canonical base) because $f(\vec{v_1})=-2\vec{e_1}+\vec{e_2}$ , $f(\vec{v_2})=-\vec{e_1}+2\vec{e_2}$ and $f(\vec{v_3})=-2\vec{e_1}-3\vec{e_2}$ , we have found that matrix $M$ such that $y'=Mx$ , so then: $$y'=Mx=M(P^{-1}x')=MP^{-1}x'=F_v x'$$ which I found the solution to be $F_v=MP^{-1}$ but my book now says it is $P^{-1}M$ , how is that possible? The way they approached the problem . The same was as I did, but instead of $y'=Mx$ they did $y'=xM$ , and likewise with $x'=xP$ changing the order of the matrix and putting it in the right? And they arrive to $F_v=P^{-1}M$ . My doubt . What happened? Have I done something wrong or is it just that both answers are valid? And why did they put the matrix on the right and not on the left, does it not matter as long as you are consistent with it?","Problem . Let base of and let be the linear operator on defined by , , . Calculate the matrix of in the base ; How I was taught the lesson . Now the issue is that in my book and all of my notes they always define the matrices of change of base in this way: for the matrix of in a base and the matrix to change basis from the canonical to the base : (I'm using ' for the base and no ' for the canonical). How I approached the problem . So I continued doing the problem: since we were given the in terms of (the canonical base) because , and , we have found that matrix such that , so then: which I found the solution to be but my book now says it is , how is that possible? The way they approached the problem . The same was as I did, but instead of they did , and likewise with changing the order of the matrix and putting it in the right? And they arrive to . My doubt . What happened? Have I done something wrong or is it just that both answers are valid? And why did they put the matrix on the right and not on the left, does it not matter as long as you are consistent with it?","V = {\vec{v_1} = (5, 3, 1), \vec{v_2}= (1, −3, −2), \vec{v_3} = (1, 2, 1)} \mathbb{R^3} f \mathbb{R^3} f(\vec{v_1}) = (−2, 1, 0) f(\vec{v_2}) = (−1, 3, 0) f(\vec{v_3}) = (−2, −3, 0) f V y'=F_v x' F_v f V P V x'=Px V x'=Px\Rightarrow x=P^{-1} x' y' x f(\vec{v_1})=-2\vec{e_1}+\vec{e_2} f(\vec{v_2})=-\vec{e_1}+2\vec{e_2} f(\vec{v_3})=-2\vec{e_1}-3\vec{e_2} M y'=Mx y'=Mx=M(P^{-1}x')=MP^{-1}x'=F_v x' F_v=MP^{-1} P^{-1}M y'=Mx y'=xM x'=xP F_v=P^{-1}M","['linear-algebra', 'matrices', 'linear-transformations', 'change-of-basis']"
19,Generalisation of Baker–Campbell–Hausdorff (Matrix exponential simplification),Generalisation of Baker–Campbell–Hausdorff (Matrix exponential simplification),,"Let there be some matrix exponential function for matrices of the form $$M=\sum_{i=1}^p c_i A_i$$ where $M,A_i\in\mathbb{R}^{n\times n}$ are given, real square matrices and $c_i \in \mathbb{R}$ . In other words, we are interested in the matrix exponential of the following form: $$ f(M)=e^M=e^{\sum c_i A_i}. $$ The matrix exponent is thus expressible in some finite real basis for the subspace we are interested in, as $M \in \mbox{Span}(A_i)$ . Does an approximation (or even identity) exist for the case in which this basis of matrices $A_i$ and their commutators is known? Concretely, I currently have a basis of less than 10 matrices (i.e., $p<10$ ), if this makes the problem easier. This problem arises as part of an optimization problem in which the $A_i$ are fixed (some chosen basis) and the $c_i$ are variable.","Let there be some matrix exponential function for matrices of the form where are given, real square matrices and . In other words, we are interested in the matrix exponential of the following form: The matrix exponent is thus expressible in some finite real basis for the subspace we are interested in, as . Does an approximation (or even identity) exist for the case in which this basis of matrices and their commutators is known? Concretely, I currently have a basis of less than 10 matrices (i.e., ), if this makes the problem easier. This problem arises as part of an optimization problem in which the are fixed (some chosen basis) and the are variable.","M=\sum_{i=1}^p c_i A_i M,A_i\in\mathbb{R}^{n\times n} c_i \in \mathbb{R} 
f(M)=e^M=e^{\sum c_i A_i}.
 M \in \mbox{Span}(A_i) A_i p<10 A_i c_i","['matrices', 'matrix-calculus', 'matrix-rank', 'matrix-exponential']"
20,Chinese remainder theorem for groups of matrices over rings,Chinese remainder theorem for groups of matrices over rings,,"Let $N=p_1^{k_1}p_2^{k_2}$ and consider the group $GL_2(\mathbb{Z}/N\mathbb{Z})$ . I would like to use Chinese remainder map to show that the group is isomorphic to $GL_2(\mathbb{Z}/p_1^{k_1}\mathbb{Z}) \times GL_2(\mathbb{Z}/p_2^{k_2}\mathbb{Z})$ . Does the following map gives an isomorphism: $$A \mapsto (A \bmod{p_1^{k_1}}, A \bmod{p_2^{k_2}})$$ where $A \bmod{p_i^{k_i}}$ just means all entries of $A$ taking modulo $p_i^{k_i}$ . This is clearly a well defined map since if the determinant of $A$ is coprime to $N$ , then each modulo $p_i^{k_i}$ is also a unit in each $\mathbb{Z}/p_i^{k_i}\mathbb{Z}$ . Moreover, this is one-to-one correspondence by CRT. However, I am a bit lost of showing the subjectivity (both of their sizes are equal). Any thought?","Let and consider the group . I would like to use Chinese remainder map to show that the group is isomorphic to . Does the following map gives an isomorphism: where just means all entries of taking modulo . This is clearly a well defined map since if the determinant of is coprime to , then each modulo is also a unit in each . Moreover, this is one-to-one correspondence by CRT. However, I am a bit lost of showing the subjectivity (both of their sizes are equal). Any thought?","N=p_1^{k_1}p_2^{k_2} GL_2(\mathbb{Z}/N\mathbb{Z}) GL_2(\mathbb{Z}/p_1^{k_1}\mathbb{Z}) \times GL_2(\mathbb{Z}/p_2^{k_2}\mathbb{Z}) A \mapsto (A \bmod{p_1^{k_1}}, A \bmod{p_2^{k_2}}) A \bmod{p_i^{k_i}} A p_i^{k_i} A N p_i^{k_i} \mathbb{Z}/p_i^{k_i}\mathbb{Z}","['matrices', 'group-theory', 'chinese-remainder-theorem']"
21,Proof about block matrices,Proof about block matrices,,"During a test today I had this question: Given $$ M = \begin{pmatrix} A & C \\ 0 & B\\ \end{pmatrix}$$ where $A$ and $B$ are $n \times n$ diagonalizables matrices without eigenvalues in common, prove that $M$ is diagonalizable. No information about $C$ was given. First, I tried $$\det(M - \lambda I) = \det(A - \lambda I)\cdot \det(B - \lambda I)$$ So $$p_M(\lambda)=p_A(\lambda)\cdot p_B(\lambda)$$ So the set of eigenvalues of $M$ is the union of the eigenvalues of $A$ and $B$ (given they don't have any in common). My next step was to do $$ M^k = \begin{pmatrix}      A^k & C'\\       0 & B^k\\ \end{pmatrix}$$ so if $m_M(x)$ is the minimal polynomial of $M$ , we have that $m_A(x)|m_M(x)$ and $m_B(x)|m_M(x)$ . But how can I conclude that hence the minimal polynomial of M will have just linear factors? I know that $m_M(x) = m_A(x)\cdot m_B(x)\cdot Q(x)$ , but how can I show that $Q(x) = 1$ ? If $m_M(x) = m_A(x)\cdot m_B(x)$ it´s clear that M is diagonalizable, but I can´t see how to prove this. In fact, in the end my approach was the same of the first answer, but after I still looking to a solution using the minimal polynomial.","During a test today I had this question: Given where and are diagonalizables matrices without eigenvalues in common, prove that is diagonalizable. No information about was given. First, I tried So So the set of eigenvalues of is the union of the eigenvalues of and (given they don't have any in common). My next step was to do so if is the minimal polynomial of , we have that and . But how can I conclude that hence the minimal polynomial of M will have just linear factors? I know that , but how can I show that ? If it´s clear that M is diagonalizable, but I can´t see how to prove this. In fact, in the end my approach was the same of the first answer, but after I still looking to a solution using the minimal polynomial."," M = \begin{pmatrix} A & C \\ 0 & B\\ \end{pmatrix} A B n \times n M C \det(M - \lambda I) = \det(A - \lambda I)\cdot \det(B - \lambda I) p_M(\lambda)=p_A(\lambda)\cdot p_B(\lambda) M A B  M^k = \begin{pmatrix}
     A^k & C'\\ 
     0 & B^k\\ \end{pmatrix} m_M(x) M m_A(x)|m_M(x) m_B(x)|m_M(x) m_M(x) = m_A(x)\cdot m_B(x)\cdot Q(x) Q(x) = 1 m_M(x) = m_A(x)\cdot m_B(x)","['linear-algebra', 'matrices', 'eigenvalues-eigenvectors', 'diagonalization', 'block-matrices']"
22,Using the divergence theorem to prove that $\frac{1}{|B_R(0)|} \int_{B_R(0)} M \textbf{y} . \textbf{y} dy = \frac{R^2}{ n + 2} \text{trace}(M)$,Using the divergence theorem to prove that,\frac{1}{|B_R(0)|} \int_{B_R(0)} M \textbf{y} . \textbf{y} dy = \frac{R^2}{ n + 2} \text{trace}(M),Here is the question I am trying to solve letter $(b)$ of it: Here is a solution to it: Which is a very very long solution. Does anyone have a more elegant and succinct solution please?,Here is the question I am trying to solve letter of it: Here is a solution to it: Which is a very very long solution. Does anyone have a more elegant and succinct solution please?,(b),"['real-analysis', 'matrices']"
23,On the order of matrix groups.,On the order of matrix groups.,,"I have two subgroups $A,B$ of $\text{GL}_2(\mathbb{C})$ such that $A \subset B$ and $[B:A]$ is finite. Then you can define the subgroups $A^1=\{a \in A: \det a =1\}$ and $B^1=\{b\in B: \det b =1\}$ . Do we always have that $[B^1:A^1]$ is finite? The problem I am trying to prove is that for any two orders $O,O'$ in a quaternion algebra $B$ over the rationals (or maybe even number fields) I want $\Gamma^1(O)$ and $\Gamma^1(O')$ to be commensurable. Here $\Gamma^1(O)$ is defined as the embedding of the units with norm $1$ into $M_2(\mathbb{C})$ as a group under multiplication, and then we mod out by $\{\pm 1\}$ . If we let $N:=\{\det b : b\in B\}$ and define for $n\in N$ that $B_n:=\{b\in B: \det b=n\}$ then we have a bijection between all pairs $B_n,B_{n'}$ for $n,n'\in N$ so I feel like $[B^1:A^1]\leq [B:A]$ should be true, but I am not sure about the details. Can anyone help?","I have two subgroups of such that and is finite. Then you can define the subgroups and . Do we always have that is finite? The problem I am trying to prove is that for any two orders in a quaternion algebra over the rationals (or maybe even number fields) I want and to be commensurable. Here is defined as the embedding of the units with norm into as a group under multiplication, and then we mod out by . If we let and define for that then we have a bijection between all pairs for so I feel like should be true, but I am not sure about the details. Can anyone help?","A,B \text{GL}_2(\mathbb{C}) A \subset B [B:A] A^1=\{a \in A: \det a =1\} B^1=\{b\in B: \det b =1\} [B^1:A^1] O,O' B \Gamma^1(O) \Gamma^1(O') \Gamma^1(O) 1 M_2(\mathbb{C}) \{\pm 1\} N:=\{\det b : b\in B\} n\in N B_n:=\{b\in B: \det b=n\} B_n,B_{n'} n,n'\in N [B^1:A^1]\leq [B:A]","['matrices', 'group-theory']"
24,"How to understand Eigenvalue/Eigenvectors of matrices, without considering linear transformations","How to understand Eigenvalue/Eigenvectors of matrices, without considering linear transformations",,"I am struggling to DEEPLY understand eigenvectors/eigenvalues. Mainly because almost everywhere they are introduced in the context of transformation where we use matrices as functions and eigenvalues and eigenvectors, similar to the roots of polynomials, help us better understand the behavior of the function/transformation. However, my question is what if we don't want to transform anything and the matrix is just a book keeping tool that hosts some information like an image or a dataset. Then what's special about the eigenvector/eigenvalues of such matrices to deserve the name ""Eigen (special)""? The answer to this question gets close when it tries explaining PCA but it doesn't explain how eigenvectors determine orientation and eigenvalues determine distortion of an ellipse! I think matrices (whether as a function or a book keeping tool) have some secrets carved into their eigenvectors/eigenvalues and one only truly understands why those are Eigen (special) if he/she understands both WHY and WHAT secrets are hidden in them. I would really appreciate if anyone could answer these questions.","I am struggling to DEEPLY understand eigenvectors/eigenvalues. Mainly because almost everywhere they are introduced in the context of transformation where we use matrices as functions and eigenvalues and eigenvectors, similar to the roots of polynomials, help us better understand the behavior of the function/transformation. However, my question is what if we don't want to transform anything and the matrix is just a book keeping tool that hosts some information like an image or a dataset. Then what's special about the eigenvector/eigenvalues of such matrices to deserve the name ""Eigen (special)""? The answer to this question gets close when it tries explaining PCA but it doesn't explain how eigenvectors determine orientation and eigenvalues determine distortion of an ellipse! I think matrices (whether as a function or a book keeping tool) have some secrets carved into their eigenvectors/eigenvalues and one only truly understands why those are Eigen (special) if he/she understands both WHY and WHAT secrets are hidden in them. I would really appreciate if anyone could answer these questions.",,"['matrices', 'eigenvalues-eigenvectors']"
25,Calculating the trace of a 3x3 matrix over the rationals when its determinant is 0,Calculating the trace of a 3x3 matrix over the rationals when its determinant is 0,,"I encountered a problem where I had a $3\times 3$ matrix $A$ with rational numbers entries and it was given that $A^3=2A$ and $A$ could not be the zero matix. I was asked to calculate the trace of the square of $A$ , namely $tr(A^2)$ . I could find that the determinant $\det(A)=0$ because otherwise it had to be $\sqrt8$ or $-\sqrt8$ . This comes from $\det(A)^3=8\det(A)$ . Since $\sqrt8$ is not rational the determinant has to be $0$ . Is there a way to express $tr(A^2)$ in terms of $\det(A)$ ?","I encountered a problem where I had a matrix with rational numbers entries and it was given that and could not be the zero matix. I was asked to calculate the trace of the square of , namely . I could find that the determinant because otherwise it had to be or . This comes from . Since is not rational the determinant has to be . Is there a way to express in terms of ?",3\times 3 A A^3=2A A A tr(A^2) \det(A)=0 \sqrt8 -\sqrt8 \det(A)^3=8\det(A) \sqrt8 0 tr(A^2) \det(A),"['linear-algebra', 'matrices']"
26,Bounding spectral radius of special matrix (extension),Bounding spectral radius of special matrix (extension),,"Let $A$ be an $n \times n$ matrix with all nonnegative entries and row sums strictly less than one, let $V$ be an $n \times n$ nonnegative diagonal matrix satisfying $V \leq I$ (entrywise), let $B\equiv\left(I-AV\right)^{-1}$ and $B^{*}\equiv\left(I-A\right)^{-1}$ , let $X$ be a vector in the $n$ -dimensional simplex (i.e., $x_j \geq 0,\sum_j^n x_j=1$ ), let $D_1$ and $D_2$ be two strictly positive diagonal $n \times n$ matrices, and finally let $$\tilde{M}\equiv\left(\mathrm{diag}\left\{ B^{T}X\right\} \right)^{-1}B^{T}\left[V\mathrm{diag}\left\{ X\right\} +\left(I-V\right)\mathrm{diag}\left\{ B^{T}X\right\} \right]D_{1}BD_2\mathrm{diag}\left(\iota-A\iota\right).$$ I want to show that the spectral radius of $\tilde{M}$ is less than or equal to one, $\rho(\tilde{M})\leq 1$ , provided that the following condition holds $$\tag{1} D_{1} B^{*} D_{2}  \left(I-A\right) \iota\leq\iota.$$ It is useful to note the connection between this question and two questions that have already been solved. First, in the special case with $D_2 = I$ , the problem above simplifies to showing that $\rho(M)\leq 1,$ where $$M\equiv\left(\mathrm{diag}\left\{ B^{T}X\right\} \right)^{-1}B^{T}\left[V\mathrm{diag}\left\{ X\right\} +\left(I-V\right)\mathrm{diag}\left\{ B^{T}X\right\} \right] B\mathrm{diag}\left(\iota-A\iota\right).$$ This is question Bounding spectral radius of special matrix which has already been solved, see https://math.stackexchange.com/a/4402778/165163 . Second, in the special case in which $D_1 = \eta I$ and $D_2 = \gamma \mathrm{diag}(e_i)$ , with $\eta,\gamma$ nonnegative constants and $e_i$ the vector with zeros everywhere except for a 1 in position $i$ , then $$\rho(\tilde{M}) = \tilde{M}_{ii}=\frac{\sum_{l}\left(v_{l}x_{l}+\left(1-v_{l}\right)\sum_{k}b_{kl}x_{k}\right)b_{li}^{2}\eta\left(1-s_{i}\right)\gamma_{i}}{\sum_{r}b_{ri}x_{r}}.$$ Since both the numerator and numerator are linear in $x$ , it is enough to consider this expression at corners, $X=e_j$ . Thus, we need to show that $$\tilde{M}_{ii}^{(j)}=v_{j}b_{ji}\eta\left(1-s_{i}\right)\gamma_{i}+\frac{\sum_{l}\left(1-v_{l}\right)b_{jl}b_{li}^{2}\eta\left(1-s_{i}\right)\gamma_{i}}{b_{ji}}\leq1.$$ Given our assumptions on $D_{1}$ and $D_{2}$ , condition (1) collapses to $\eta b_{ji}^{*}\left(1-s_{i}\right)\gamma_{i}\leq$ for all $j$ . Since $b_{ii}^{*}\geq b_{ji}^{*},\forall j,$ then we would need to show that $$v_{j}b_{ji}^{2}+\sum_{l}\left(1-v_{l}\right)b_{jl}b_{li}^{2}\leq b_{ji} b_{ii}^{*},$$ as formulated in this question Inequality involving matrix inverse elements , which has already been solved (see here ). To some extent, the challenge now is to somehow combine the ideas in the solutions to these two special cases to solve the more general problem postulated here. Finally, as in the case with $D_2 = I$ discussed in Bounding spectral radius of special matrix , two simple cases are illustrative. First, if $V = I$ then condition (1) implies that $\tilde{M}\iota \leq \iota$ and so $\rho(\tilde{M}) \leq 1$ . Second, if $A$ is diagonal then $\tilde{M}$ would be diagonal and so we would just need to show that each diagonal element is lower than one. But each of diagonal element of $\tilde{M}$ would be of the form $$d_1d_2\left(v+\frac{1-v}{1-av}\right)\frac{1-a}{1-av},$$ while (1) implies that $d_1d_2\leq 1$ , so it is enough to show that $$\left(v+\frac{1-v}{1-av}\right)\frac{1-a}{1-av}\leq 1.$$ Simple algebra shows this to be true.","Let be an matrix with all nonnegative entries and row sums strictly less than one, let be an nonnegative diagonal matrix satisfying (entrywise), let and , let be a vector in the -dimensional simplex (i.e., ), let and be two strictly positive diagonal matrices, and finally let I want to show that the spectral radius of is less than or equal to one, , provided that the following condition holds It is useful to note the connection between this question and two questions that have already been solved. First, in the special case with , the problem above simplifies to showing that where This is question Bounding spectral radius of special matrix which has already been solved, see https://math.stackexchange.com/a/4402778/165163 . Second, in the special case in which and , with nonnegative constants and the vector with zeros everywhere except for a 1 in position , then Since both the numerator and numerator are linear in , it is enough to consider this expression at corners, . Thus, we need to show that Given our assumptions on and , condition (1) collapses to for all . Since then we would need to show that as formulated in this question Inequality involving matrix inverse elements , which has already been solved (see here ). To some extent, the challenge now is to somehow combine the ideas in the solutions to these two special cases to solve the more general problem postulated here. Finally, as in the case with discussed in Bounding spectral radius of special matrix , two simple cases are illustrative. First, if then condition (1) implies that and so . Second, if is diagonal then would be diagonal and so we would just need to show that each diagonal element is lower than one. But each of diagonal element of would be of the form while (1) implies that , so it is enough to show that Simple algebra shows this to be true.","A n \times n V n \times n V \leq I B\equiv\left(I-AV\right)^{-1} B^{*}\equiv\left(I-A\right)^{-1} X n x_j \geq 0,\sum_j^n x_j=1 D_1 D_2 n \times n \tilde{M}\equiv\left(\mathrm{diag}\left\{ B^{T}X\right\} \right)^{-1}B^{T}\left[V\mathrm{diag}\left\{ X\right\} +\left(I-V\right)\mathrm{diag}\left\{ B^{T}X\right\} \right]D_{1}BD_2\mathrm{diag}\left(\iota-A\iota\right). \tilde{M} \rho(\tilde{M})\leq 1 \tag{1} D_{1} B^{*} D_{2}  \left(I-A\right) \iota\leq\iota. D_2 = I \rho(M)\leq 1, M\equiv\left(\mathrm{diag}\left\{ B^{T}X\right\} \right)^{-1}B^{T}\left[V\mathrm{diag}\left\{ X\right\} +\left(I-V\right)\mathrm{diag}\left\{ B^{T}X\right\} \right] B\mathrm{diag}\left(\iota-A\iota\right). D_1 = \eta I D_2 = \gamma \mathrm{diag}(e_i) \eta,\gamma e_i i \rho(\tilde{M}) = \tilde{M}_{ii}=\frac{\sum_{l}\left(v_{l}x_{l}+\left(1-v_{l}\right)\sum_{k}b_{kl}x_{k}\right)b_{li}^{2}\eta\left(1-s_{i}\right)\gamma_{i}}{\sum_{r}b_{ri}x_{r}}. x X=e_j \tilde{M}_{ii}^{(j)}=v_{j}b_{ji}\eta\left(1-s_{i}\right)\gamma_{i}+\frac{\sum_{l}\left(1-v_{l}\right)b_{jl}b_{li}^{2}\eta\left(1-s_{i}\right)\gamma_{i}}{b_{ji}}\leq1. D_{1} D_{2} \eta b_{ji}^{*}\left(1-s_{i}\right)\gamma_{i}\leq j b_{ii}^{*}\geq b_{ji}^{*},\forall j, v_{j}b_{ji}^{2}+\sum_{l}\left(1-v_{l}\right)b_{jl}b_{li}^{2}\leq b_{ji} b_{ii}^{*}, D_2 = I V = I \tilde{M}\iota \leq \iota \rho(\tilde{M}) \leq 1 A \tilde{M} \tilde{M} d_1d_2\left(v+\frac{1-v}{1-av}\right)\frac{1-a}{1-av}, d_1d_2\leq 1 \left(v+\frac{1-v}{1-av}\right)\frac{1-a}{1-av}\leq 1.","['linear-algebra', 'matrices', 'inequality', 'upper-lower-bounds', 'spectral-radius']"
27,Why $\frac{d}{dx}(f(g)) = \frac{d(g^T)}{dx}\cdot\frac{\partial f}{\partial g}$ for $f:\mathbb{R}^n\to\mathbb{R}$ and $g:\mathbb{R}^m\to\mathbb{R}^n$?,Why  for  and ?,\frac{d}{dx}(f(g)) = \frac{d(g^T)}{dx}\cdot\frac{\partial f}{\partial g} f:\mathbb{R}^n\to\mathbb{R} g:\mathbb{R}^m\to\mathbb{R}^n,"Reading about differentiation of matrices seems to state that, given $f:\mathbb{R}^n\rightarrow \mathbb{R}$ and $g:\mathbb{R}^m\rightarrow \mathbb{R}^n$ $$\frac{d}{dx}(f(g)) = \frac{d(g^T)}{dx} \cdot \frac{\partial f}{\partial g}$$ (I think I might be wrong) Why is the transposition needed? Moreover, does this mean that: $\nabla f = \frac{d(f^T)}{dx} = (\frac{d(f)}{dx})^T$ ?","Reading about differentiation of matrices seems to state that, given and (I think I might be wrong) Why is the transposition needed? Moreover, does this mean that: ?",f:\mathbb{R}^n\rightarrow \mathbb{R} g:\mathbb{R}^m\rightarrow \mathbb{R}^n \frac{d}{dx}(f(g)) = \frac{d(g^T)}{dx} \cdot \frac{\partial f}{\partial g} \nabla f = \frac{d(f^T)}{dx} = (\frac{d(f)}{dx})^T,"['linear-algebra', 'matrices', 'multivariable-calculus', 'linear-transformations', 'partial-derivative']"
28,"Finding the normalizer of $\left\{ \left(\begin{matrix} x &0 \\0 & y \end{matrix}\right) : x,y\in \mathbb R\setminus\{0\} \right\}$",Finding the normalizer of,"\left\{ \left(\begin{matrix} x &0 \\0 & y \end{matrix}\right) : x,y\in \mathbb R\setminus\{0\} \right\}","I'm having some trouble with the following question: Let $G=\text{GL}_2(\mathbb R)$ . What are the elements of the set: $$N_G \left( \underbrace{\left\{ \left(\begin{matrix} x &0 \\0 & y \end{matrix}\right) : x,y\in \mathbb R\setminus\{0\} \right\}}_S\right)$$ I tried solving this using the fact that if $A = \left(\begin{matrix} a &c \\b & d \end{matrix}\right) \in N_G(S)$ then: $$ASA^{-1} = S$$ And I arrived at the following conditions: $xbd=ycd$ $yac=xba$ $axd \neq yc^2$ $yad \neq xb^2$ Where $x,y \in \mathbb R \setminus \{0\}$ are fixed constants. I'm not being able to find all the values of $(a,b,c,d)$ that follow these conditions and this docent's seem like the best approach to me. Is there a better way to solve this?",I'm having some trouble with the following question: Let . What are the elements of the set: I tried solving this using the fact that if then: And I arrived at the following conditions: Where are fixed constants. I'm not being able to find all the values of that follow these conditions and this docent's seem like the best approach to me. Is there a better way to solve this?,"G=\text{GL}_2(\mathbb R) N_G \left( \underbrace{\left\{
\left(\begin{matrix} x &0 \\0 & y \end{matrix}\right) : x,y\in \mathbb R\setminus\{0\} \right\}}_S\right) A = \left(\begin{matrix} a &c \\b & d \end{matrix}\right) \in N_G(S) ASA^{-1} = S xbd=ycd yac=xba axd \neq yc^2 yad \neq xb^2 x,y \in \mathbb R \setminus \{0\} (a,b,c,d)","['abstract-algebra', 'matrices', 'group-theory', 'linear-groups', 'similar-matrices']"
29,Show that $\text{rank}(A-B) = \text{rank}(A) - 1 \iff B = \frac{Axy^{T}A}{y^{T}Ax}$ where $\text{rank}(B)=1$.,Show that  where .,\text{rank}(A-B) = \text{rank}(A) - 1 \iff B = \frac{Axy^{T}A}{y^{T}Ax} \text{rank}(B)=1,"Let $m,n \in \mathbb{N}$ and $A,B \in \mathbb{R}^{m\times n}$ with $\text{rank}(B) = 1$ . Show that $\text{rank}(A-B) = \text{rank}(A) - 1$ if and only if there exist vectors $x \in \mathbb{R}^n$ and $y \in \mathbb{R}^m$ such that $y^{T}Ax \neq 0$ and $B = \frac{Axy^{T}A}{y^{T}Ax}$ . I don't really know how to start this problem. I know that $y^{T}Ax$ is a scalar and that $B$ may be written as an outer product of two vectors $u^{T}v$ . Other than that, I am pretty lost. I imagine that alternative characterizations of rank and the rank-nullity theorem might come in handy at some point.","Let and with . Show that if and only if there exist vectors and such that and . I don't really know how to start this problem. I know that is a scalar and that may be written as an outer product of two vectors . Other than that, I am pretty lost. I imagine that alternative characterizations of rank and the rank-nullity theorem might come in handy at some point.","m,n \in \mathbb{N} A,B \in \mathbb{R}^{m\times n} \text{rank}(B) = 1 \text{rank}(A-B) = \text{rank}(A) - 1 x \in \mathbb{R}^n y \in \mathbb{R}^m y^{T}Ax \neq 0 B = \frac{Axy^{T}A}{y^{T}Ax} y^{T}Ax B u^{T}v","['linear-algebra', 'matrices', 'matrix-rank']"
30,Matrix whose spectral moments (traces of its powers) follow a geometric progression,Matrix whose spectral moments (traces of its powers) follow a geometric progression,,"I am looking for a square $n \times n$ matrix $A$ such that its spectral moments follow a geometric progression: $Tr(A^r) = k^r$ , for some $k$ . I believe this to be impossible since $Tr(A^0) = Tr(I) = n$ . So perhaps I should be looking for something like $$ Tr(A^r) = n^{r+1}, $$ or something like $$ Tr(A^r) = nk^{r}. $$ I'm interested in complex or real matrices, though other fields would be acceptable. Apologies if the question sounds vague, but I am dealing with an open ended question here! What I have tried: I know that the coefficients of the characteristic polynomial can be expressed in terms of the spectral moments using Newton's identities . In this way, I can fix the traces of $A,\ldots,A^n$ to be whatever I want, find the corresponding polynomial, and find a matrix whose characteristic polynomial matches it. However, this does not give me any control on the spectral moments of powers greater than $n$ . So I'm beginning to wonder if this is at all possible. Also note that for $n=1$ this is trivial since $A$ is just an element of the underlying field and we have $Tr(A^r) = A^r$ . I'm interested in $n>1$ . EDIT: if possible, I would also like $A$ to be invertible...","I am looking for a square matrix such that its spectral moments follow a geometric progression: , for some . I believe this to be impossible since . So perhaps I should be looking for something like or something like I'm interested in complex or real matrices, though other fields would be acceptable. Apologies if the question sounds vague, but I am dealing with an open ended question here! What I have tried: I know that the coefficients of the characteristic polynomial can be expressed in terms of the spectral moments using Newton's identities . In this way, I can fix the traces of to be whatever I want, find the corresponding polynomial, and find a matrix whose characteristic polynomial matches it. However, this does not give me any control on the spectral moments of powers greater than . So I'm beginning to wonder if this is at all possible. Also note that for this is trivial since is just an element of the underlying field and we have . I'm interested in . EDIT: if possible, I would also like to be invertible...","n \times n A Tr(A^r) = k^r k Tr(A^0) = Tr(I) = n 
Tr(A^r) = n^{r+1},
 
Tr(A^r) = nk^{r}.
 A,\ldots,A^n n n=1 A Tr(A^r) = A^r n>1 A","['matrices', 'characteristic-polynomial']"
31,pseudoinverse of a square block diagonal matrix with projected diagonal matirces on diagonal,pseudoinverse of a square block diagonal matrix with projected diagonal matirces on diagonal,,"Consider the following block-diagonal matrix: \begin{pmatrix} P \Sigma P & 0\\ 0 & (I-P) \Sigma (I-P) \end{pmatrix} Where $P$ is an orthogonal projection matrix, and $\Sigma$ is a diagonal matrix. The individual diagonal elements $P \Sigma P$ and $(I-P) \Sigma (I-P)$ are generally not invertible (unless P doesn't reduce the rank), however, the entire matrix seems to be invertible (validated numerically). Is there a simple expression for its inverse? Update: @Carl_Schildkraut has shown the matrix is not invertible. Is there a simple expression for the pseudo inverse?","Consider the following block-diagonal matrix: Where is an orthogonal projection matrix, and is a diagonal matrix. The individual diagonal elements and are generally not invertible (unless P doesn't reduce the rank), however, the entire matrix seems to be invertible (validated numerically). Is there a simple expression for its inverse? Update: @Carl_Schildkraut has shown the matrix is not invertible. Is there a simple expression for the pseudo inverse?","\begin{pmatrix}
P \Sigma P & 0\\
0 & (I-P) \Sigma (I-P)
\end{pmatrix} P \Sigma P \Sigma P (I-P) \Sigma (I-P)","['linear-algebra', 'matrices', 'inverse', 'orthogonality', 'projection-matrices']"
32,Characterize all sets of $2\times 2$ real matrices isomorphic to $\mathbb C$ as a field,Characterize all sets of  real matrices isomorphic to  as a field,2\times 2 \mathbb C,"Let $A$ be the set of all 2x2 real matrices of  the form $$\begin{bmatrix}a&-b\\b&a\end{bmatrix}$$ Show that a set $B$ of $2\times 2$ real matrices is isomorphic to $\mathbb C$ as a field iff $B=gAg^{-1}$ for some invertible real matrix $g$ . I've shown that any field of the above form is isomorphic to $\mathbb C$ , but I cannot figure out how to prove the other direction, that if $B$ is isomorphic to $\mathbb C$ then it is of the above form. I can't find any way to characterize what $g$ would have to be for a given $B$ . The statement is basically equivalent to saying the set $B$ is the set of all matrices representing the linear operators on the vector space $\mathbb C$ over $\mathbb R$ of the form $T_w(z)=wz$ under some basis, but I cannot prove that either. Help would be appreciated.","Let be the set of all 2x2 real matrices of  the form Show that a set of real matrices is isomorphic to as a field iff for some invertible real matrix . I've shown that any field of the above form is isomorphic to , but I cannot figure out how to prove the other direction, that if is isomorphic to then it is of the above form. I can't find any way to characterize what would have to be for a given . The statement is basically equivalent to saying the set is the set of all matrices representing the linear operators on the vector space over of the form under some basis, but I cannot prove that either. Help would be appreciated.",A \begin{bmatrix}a&-b\\b&a\end{bmatrix} B 2\times 2 \mathbb C B=gAg^{-1} g \mathbb C B \mathbb C g B B \mathbb C \mathbb R T_w(z)=wz,"['linear-algebra', 'matrices', 'complex-numbers']"
33,What's the relationship between linear transformations and systems of equations? [duplicate],What's the relationship between linear transformations and systems of equations? [duplicate],,"This question already has answers here : Why can a system of linear equations be represented as a linear combination of vectors? (7 answers) Closed 3 months ago . I began watching Gilbert Strang's lectures on Linear Algebra and soon realized that I lacked an intuitive understanding of matrices, especially as to why certain operations (e.g. matrix multiplication) are defined the way they are. Someone suggested to me 3Blue1Brown's video series ( https://youtube.com/playlist?list=PLZHQObOWTQDPD3MizzM2xVFitgF8hE_ab ) and it has helped immensely. However, it seems to me that they present matrices in completely different ways: 3Blue1Brown explains that they represent linear transformations, while Strang depicts matrices as systems of linear equations. What's the connection between these two different ideas? Furthermore, I understand why operations on matrices are defined the way they are when we think of them as linear maps, but this intuition breaks when matrices are thought of in different ways. Since matrices are used to represent all sorts of things (linear transformations, systems of equations, data, etc.), how come operations that are seemingly defined for use with linear maps the same across all these different contexts?","This question already has answers here : Why can a system of linear equations be represented as a linear combination of vectors? (7 answers) Closed 3 months ago . I began watching Gilbert Strang's lectures on Linear Algebra and soon realized that I lacked an intuitive understanding of matrices, especially as to why certain operations (e.g. matrix multiplication) are defined the way they are. Someone suggested to me 3Blue1Brown's video series ( https://youtube.com/playlist?list=PLZHQObOWTQDPD3MizzM2xVFitgF8hE_ab ) and it has helped immensely. However, it seems to me that they present matrices in completely different ways: 3Blue1Brown explains that they represent linear transformations, while Strang depicts matrices as systems of linear equations. What's the connection between these two different ideas? Furthermore, I understand why operations on matrices are defined the way they are when we think of them as linear maps, but this intuition breaks when matrices are thought of in different ways. Since matrices are used to represent all sorts of things (linear transformations, systems of equations, data, etc.), how come operations that are seemingly defined for use with linear maps the same across all these different contexts?",,"['linear-algebra', 'matrices', 'linear-transformations', 'systems-of-equations']"
34,How to show that this $2n \times n^2$ matrix has rank $2n-1$?,How to show that this  matrix has rank ?,2n \times n^2 2n-1,"The matrix is fairly messy to present, but quite easy to understand. When $n=3$ , the matrix is \begin{bmatrix} 1 & 0 & 0 & 1 & 0 & 0 & 1 & 0 & 0\\ 0 & 1 & 0 & 0 & 1 & 0 & 0 & 1 & 0\\ 0 & 0 & 1 & 0 & 0 & 1 & 0 & 0 & 1\\ 1 & 1 & 1 & 0 & 0 & 0 & 0 & 0 & 0\\ 0 & 0 & 0 & 1 & 1 & 1 & 0 & 0 & 0\\ 0 & 0 & 0 & 0 & 0 & 0 & 1 & 1 & 1 \end{bmatrix} So, basically, it splits into two parts: the upper part (from row $1$ to row $n$ ) has $n$ identity matrices $I_n$ . the lower part is $O_n^1, \ldots, O_n^n$ , where each $O_n^i$ is an $n \times n$ matrix whose $i$ -th row is all one whereas the other entries are zero. By some examples and intuition, I am aware that the rank of such a matrix is $2n-1$ , but how should I rigorously prove it?","The matrix is fairly messy to present, but quite easy to understand. When , the matrix is So, basically, it splits into two parts: the upper part (from row to row ) has identity matrices . the lower part is , where each is an matrix whose -th row is all one whereas the other entries are zero. By some examples and intuition, I am aware that the rank of such a matrix is , but how should I rigorously prove it?","n=3 \begin{bmatrix}
1 & 0 & 0 & 1 & 0 & 0 & 1 & 0 & 0\\
0 & 1 & 0 & 0 & 1 & 0 & 0 & 1 & 0\\
0 & 0 & 1 & 0 & 0 & 1 & 0 & 0 & 1\\
1 & 1 & 1 & 0 & 0 & 0 & 0 & 0 & 0\\
0 & 0 & 0 & 1 & 1 & 1 & 0 & 0 & 0\\
0 & 0 & 0 & 0 & 0 & 0 & 1 & 1 & 1
\end{bmatrix} 1 n n I_n O_n^1, \ldots, O_n^n O_n^i n \times n i 2n-1","['linear-algebra', 'matrices', 'matrix-rank']"
35,find the exponential form of a matrix [closed],find the exponential form of a matrix [closed],,"Closed. This question does not meet Mathematics Stack Exchange guidelines . It is not currently accepting answers. Please provide additional context , which ideally explains why the question is relevant to you and our  community. Some forms of context include: background and motivation, relevant definitions, source, possible strategies, your current progress, why the question is interesting or important, etc. Closed 3 years ago . Improve this question Let \begin{equation*} A =  \begin{pmatrix} 0 & 1 & 0\\ 0 & 0 & 1\\ 0 & 0 & 0 \end{pmatrix}\end{equation*} \begin{equation*} B =  \begin{pmatrix} 2 & 1 & 0\\ 0 & 2 & 0\\ 0 & 0 & 3 \end{pmatrix}\end{equation*} \begin{equation*} C =  \begin{pmatrix} 4 & 1 & 0 & -4 & 0 & 0\\ 0 & 4 & -1 & 0 & -4 & 2\\ 0 & 0 & 6 & 0 & 0 & -6\\ 2 & 0 & 0 & -2 & 1 & 0\\ 0 & 2 & -1 & 0 & -2 & 2\\ 0 & 0 & 3 & 0 & 0 & -3\\ \end{pmatrix}\end{equation*} Suppose $\exp(z)=e^z$ , for $z\in \mathbb{C}$ . Find $\exp(C)$ . Hint: \begin{equation*} D =  \begin{pmatrix} 2 & 1 \\ 1 & 1 \end{pmatrix}\end{equation*} is invertible. I'm very confused about how to approach this problem. It would be nice if someone could walk me through how to solve this. But any hints or ideas will be appreciated! Thank you!","Closed. This question does not meet Mathematics Stack Exchange guidelines . It is not currently accepting answers. Please provide additional context , which ideally explains why the question is relevant to you and our  community. Some forms of context include: background and motivation, relevant definitions, source, possible strategies, your current progress, why the question is interesting or important, etc. Closed 3 years ago . Improve this question Let Suppose , for . Find . Hint: is invertible. I'm very confused about how to approach this problem. It would be nice if someone could walk me through how to solve this. But any hints or ideas will be appreciated! Thank you!","\begin{equation*}
A = 
\begin{pmatrix}
0 & 1 & 0\\
0 & 0 & 1\\
0 & 0 & 0
\end{pmatrix}\end{equation*} \begin{equation*}
B = 
\begin{pmatrix}
2 & 1 & 0\\
0 & 2 & 0\\
0 & 0 & 3
\end{pmatrix}\end{equation*} \begin{equation*}
C = 
\begin{pmatrix}
4 & 1 & 0 & -4 & 0 & 0\\
0 & 4 & -1 & 0 & -4 & 2\\
0 & 0 & 6 & 0 & 0 & -6\\
2 & 0 & 0 & -2 & 1 & 0\\
0 & 2 & -1 & 0 & -2 & 2\\
0 & 0 & 3 & 0 & 0 & -3\\
\end{pmatrix}\end{equation*} \exp(z)=e^z z\in \mathbb{C} \exp(C) \begin{equation*}
D = 
\begin{pmatrix}
2 & 1 \\
1 & 1 \end{pmatrix}\end{equation*}","['linear-algebra', 'matrices', 'matrix-exponential']"
36,Determinant of associated real matrix from a complex one,Determinant of associated real matrix from a complex one,,"Given $X\in \operatorname{GL}(n, \mathbb{C}) $ , let $X_r\in \operatorname{GL}(2n, \mathbb{R}) $ be the real matrix obtained by substituting to each complex entry $a+ib$ the matrix $\begin{pmatrix} a & & - b \\ b & & a \end{pmatrix}$ . Does someone can give me the details of the proof that $\det(X_r) =|\det(X)|^2$ ? The only thing I have tried is to use the fact that the determinant is a wedge product but it seems not work.","Given , let be the real matrix obtained by substituting to each complex entry the matrix . Does someone can give me the details of the proof that ? The only thing I have tried is to use the fact that the determinant is a wedge product but it seems not work.","X\in \operatorname{GL}(n, \mathbb{C})  X_r\in \operatorname{GL}(2n, \mathbb{R})  a+ib \begin{pmatrix} a & & - b \\ b & & a \end{pmatrix} \det(X_r) =|\det(X)|^2","['linear-algebra', 'matrices', 'lie-groups', 'block-matrices']"
37,Calculate the determinant of the following $n \times n$ symmetric matrix: [duplicate],Calculate the determinant of the following  symmetric matrix: [duplicate],n \times n,"This question already has answers here : Determinant of an $n\times n$ Toeplitz matrix (2 answers) Closed 3 years ago . Let $n$ be a positive integer and $A = (a_{ij})_{n\times n}$ , where $a_{ij} = |i-j|$ , for $i = 1, 2, \dots, n$ and $j = 1, 2, \dots, n$ . Calculate $\det A$ . I noticed that $a_{ii} = 0$ and $a_{ij} = a_{ji}$ , so A is a symmetric matrix. Also, I saw that, if we make the notation $A_n$ for the A with n elements, $A_n$ is constructed from $A_{n-1}$ with $n-1, n-2, \dots, 0$ as elements for the last line and last column. I tried to use Laplace expansion but with no result. This is how $A_n$ looks like: $A_n=\begin{bmatrix} 0&1&2& .&.&. &n-1\\ 1&0&1&2& .&.&n-2 \\ 2&1&0&1&.&.&. \\ .&.&.&.&.&.&. \\ .&.&.&.&.&.&2 \\ .&.&.&.&.&.&1 \\ n-1&n-2&.&.&2&1&0 \end{bmatrix}$ I calculated for a few small numers: $\det A_1 = 0$ , $\det A_2 = -1$ , $\det A_3 = 4$ , $\det A_4 = -12$ , $\det A_5 = 32$ , but I didn't figure out a rule such that I could find the determinant through induction. Can you help me on this one?","This question already has answers here : Determinant of an $n\times n$ Toeplitz matrix (2 answers) Closed 3 years ago . Let be a positive integer and , where , for and . Calculate . I noticed that and , so A is a symmetric matrix. Also, I saw that, if we make the notation for the A with n elements, is constructed from with as elements for the last line and last column. I tried to use Laplace expansion but with no result. This is how looks like: I calculated for a few small numers: , , , , , but I didn't figure out a rule such that I could find the determinant through induction. Can you help me on this one?","n A = (a_{ij})_{n\times n} a_{ij} = |i-j| i = 1, 2, \dots, n j = 1, 2, \dots, n \det A a_{ii} = 0 a_{ij} = a_{ji} A_n A_n A_{n-1} n-1, n-2, \dots, 0 A_n A_n=\begin{bmatrix}
0&1&2& .&.&. &n-1\\
1&0&1&2& .&.&n-2 \\
2&1&0&1&.&.&. \\
.&.&.&.&.&.&. \\
.&.&.&.&.&.&2 \\
.&.&.&.&.&.&1 \\
n-1&n-2&.&.&2&1&0
\end{bmatrix} \det A_1 = 0 \det A_2 = -1 \det A_3 = 4 \det A_4 = -12 \det A_5 = 32","['linear-algebra', 'matrices', 'determinant', 'symmetric-matrices']"
38,Unpicking proof that det($AB$) = det$(A)$det$(B)$,Unpicking proof that det() = detdet,AB (A) (B),"I'm trying to understand this proof that $\text{det}(AB) =\text{det}(A)\text{det}(B)$ , but I'm a little stuck on the third line. Please could someone explain to me what the mapping $\kappa$ does? Let $\mathfrak{T}_n = {\rm Maps} (\{1, \ldots , n\}, \{1, \ldots , n\})$ \begin{eqnarray*} {\sf det}(AB) & = & \sum_{\sigma \in \mathfrak{S}_n} {\sf sgn}(\sigma) \prod_{i=1}^n (AB)_{i\sigma(i)} \\ & = & \sum_{\sigma \in \mathfrak{S}_n} {\sf sgn}(\sigma) \prod_{i=1}^n \sum_{j=1}^n a_{ij} b_{j\sigma(i)} \\ & = & \sum_{\sigma\in \mathfrak{S}_n, \kappa \in \mathfrak{T}_n} {\sf sgn}(\sigma) a_{1\kappa(1)}b_{\kappa(1) \sigma(1)} \ldots a_{n \kappa(n)} b_{\kappa (n) \sigma (n)} \\ & = &\sum_{\kappa\in \mathfrak{T}_n} a_{1\kappa(1)} \ldots a_{n \kappa(n)} \sum_{\sigma \in \mathfrak{S}_n} {\sf sgn}(\sigma) b_{\kappa(1) \sigma(1)}\ldots b_{\kappa (n) \sigma (n)} \\ & = & \sum_{\kappa\in \mathfrak{T}_n}  a_{1\kappa(1)} \ldots a_{n \kappa(n)} {\sf det} (B_{\kappa}) \end{eqnarray*} where $B_{\kappa}$ denotes the matrix which is obtained by using the rows of $B$ labelled $\kappa(1), \ldots , \kappa (n)$ for its rows $1$ to $n$ . . Therefore: $$ \sum_{\kappa\in \mathfrak{T}_n}  a_{1\kappa(1)} \ldots a_{n \kappa(n)} {\sf det} (B_{\kappa})  = \sum_{\kappa\in \mathfrak{S}_n}  {\sf sgn}(\kappa) a_{1\kappa(1)} \ldots a_{n \kappa(n)} {\sf det} (B) = \det (A) \det (B).$$ Thanks","I'm trying to understand this proof that , but I'm a little stuck on the third line. Please could someone explain to me what the mapping does? Let where denotes the matrix which is obtained by using the rows of labelled for its rows to . . Therefore: Thanks","\text{det}(AB) =\text{det}(A)\text{det}(B) \kappa \mathfrak{T}_n = {\rm Maps} (\{1, \ldots , n\}, \{1, \ldots , n\}) \begin{eqnarray*}
{\sf det}(AB) & = & \sum_{\sigma \in \mathfrak{S}_n} {\sf sgn}(\sigma) \prod_{i=1}^n (AB)_{i\sigma(i)} \\ & = & \sum_{\sigma \in \mathfrak{S}_n} {\sf sgn}(\sigma) \prod_{i=1}^n \sum_{j=1}^n a_{ij} b_{j\sigma(i)} \\
& = & \sum_{\sigma\in \mathfrak{S}_n, \kappa \in \mathfrak{T}_n} {\sf sgn}(\sigma) a_{1\kappa(1)}b_{\kappa(1) \sigma(1)} \ldots a_{n \kappa(n)} b_{\kappa (n) \sigma (n)} \\
& = &\sum_{\kappa\in \mathfrak{T}_n} a_{1\kappa(1)} \ldots a_{n \kappa(n)} \sum_{\sigma \in \mathfrak{S}_n} {\sf sgn}(\sigma) b_{\kappa(1) \sigma(1)}\ldots b_{\kappa (n) \sigma (n)} \\
& = & \sum_{\kappa\in \mathfrak{T}_n}  a_{1\kappa(1)} \ldots a_{n \kappa(n)} {\sf det} (B_{\kappa})
\end{eqnarray*} B_{\kappa} B \kappa(1), \ldots , \kappa (n) 1 n  \sum_{\kappa\in \mathfrak{T}_n}  a_{1\kappa(1)} \ldots a_{n \kappa(n)} {\sf det} (B_{\kappa})
 = \sum_{\kappa\in \mathfrak{S}_n}  {\sf sgn}(\kappa) a_{1\kappa(1)} \ldots a_{n \kappa(n)} {\sf det} (B) = \det (A) \det (B).","['matrices', 'determinant']"
39,Formula for cross product,Formula for cross product,,"The formula for the cross product of two vectors in $R^3$ , $\vec{a} = (a_1, a_2, a_3)$ and $\vec{b} = (b_1, b_2, b_3)$ is $$\det\begin{pmatrix}\mathbf{i} & \mathbf{j} & \mathbf{k}\\\ a_1 & a_2 & a_3\\\ b_1 & b_2 & b_3\end{pmatrix}$$ I know that in general for three 3D vectors the determinant represents the volume of the parallelepiped. But how is it valid to put (basis) vectors $\mathbf{i}, \mathbf{j}, \mathbf{k}$ into a vector, and what graphical/intuitive significance does it have? What would have been the initial motivation of this formula? Note: I already read through similar questions and corresponding answers but was not satisfied. So please do not downvote this question and if possible give me some insight.","The formula for the cross product of two vectors in , and is I know that in general for three 3D vectors the determinant represents the volume of the parallelepiped. But how is it valid to put (basis) vectors into a vector, and what graphical/intuitive significance does it have? What would have been the initial motivation of this formula? Note: I already read through similar questions and corresponding answers but was not satisfied. So please do not downvote this question and if possible give me some insight.","R^3 \vec{a} = (a_1, a_2, a_3) \vec{b} = (b_1, b_2, b_3) \det\begin{pmatrix}\mathbf{i} & \mathbf{j} & \mathbf{k}\\\ a_1 & a_2 & a_3\\\ b_1 & b_2 & b_3\end{pmatrix} \mathbf{i}, \mathbf{j}, \mathbf{k}","['linear-algebra', 'matrices', 'vector-spaces', 'determinant', 'cross-product']"
40,"Given matrices $A$ and $B$, prove that $A^{n}-B^{n}=\frac{1}{2}(7^{n}-5^{n})(A-B)$","Given matrices  and , prove that",A B A^{n}-B^{n}=\frac{1}{2}(7^{n}-5^{n})(A-B),"$$\text{Given}$$ $$A=\begin{pmatrix} 2 & 5 \\  -3 & 10 \end{pmatrix}$$ $$\text{and}$$ $$B=\begin{pmatrix} 3 & -2 \\  4 & 9 \end{pmatrix}$$ $$\text{prove that}$$ $$A^{n}-B^{n}=\frac{1}{2}(7^{n}-5^{n})(A-B) \  \forall n \in \mathbb{N}$$ The first thing that I tried was to look for a pattern for $A^n$ and $B^{n}$ , but, as I expected, I haven't been able to find any. Then I tried induction, and I think that I found a correct solution, but it is a very tedious one. Here's what I did: $$\text{Base case:}$$ $$p(1):A-B=\frac{1}{2}(7-5)(A-B) - true$$ $$\text{suppose true}$$ $$p(k): A^{k}-B^{k}=\frac{1}{2}(7^{k}-5^{k})(A-B)$$ $$\text{then we need to prove}$$ $$p(k+1): A^{k+1}-B^{k+1}=\frac{1}{2}(7^{k+1}-5^{k+1})(A-B)$$ Then I multiplied the equation from $p(k)$ by $B^k$ , and I subtracted it from the $p(k+1)$ equation to get rid of the $B^k$ . My idea was to try and isolate $A^k$ , and then prove by induction that the formula is, indeed correct, which would finish the proof. But as I said, this is a very tedious solution, and if someone could help me find a more intuitive and straight-forward one, I would appreciate that very much. Thanks!","The first thing that I tried was to look for a pattern for and , but, as I expected, I haven't been able to find any. Then I tried induction, and I think that I found a correct solution, but it is a very tedious one. Here's what I did: Then I multiplied the equation from by , and I subtracted it from the equation to get rid of the . My idea was to try and isolate , and then prove by induction that the formula is, indeed correct, which would finish the proof. But as I said, this is a very tedious solution, and if someone could help me find a more intuitive and straight-forward one, I would appreciate that very much. Thanks!",\text{Given} A=\begin{pmatrix} 2 & 5 \\  -3 & 10 \end{pmatrix} \text{and} B=\begin{pmatrix} 3 & -2 \\  4 & 9 \end{pmatrix} \text{prove that} A^{n}-B^{n}=\frac{1}{2}(7^{n}-5^{n})(A-B) \  \forall n \in \mathbb{N} A^n B^{n} \text{Base case:} p(1):A-B=\frac{1}{2}(7-5)(A-B) - true \text{suppose true} p(k): A^{k}-B^{k}=\frac{1}{2}(7^{k}-5^{k})(A-B) \text{then we need to prove} p(k+1): A^{k+1}-B^{k+1}=\frac{1}{2}(7^{k+1}-5^{k+1})(A-B) p(k) B^k p(k+1) B^k A^k,"['matrices', 'algebra-precalculus', 'matrix-equations']"
41,On a reversed Cauchy-Schwarz inequality for an indefinite quadratic form,On a reversed Cauchy-Schwarz inequality for an indefinite quadratic form,,"Let $f$ be a quadratic form over $\mathbb{R}$ and write $$f(x)=xAx^t,$$ where $A\in M_n(\mathbb{R_{+}})$ is symmetric with positive entries and $x=(x_1,x_2,\ldots,x_n)$ . Suppose that $A$ has exactly one positive eigenvalue and $n-1$ negative eigenvalues. IS IT TRUE that for any $0\neq u\in \mathbb{R}_{\geq 0}^n$ and $v\in \mathbb{R}^n$ that is not parallel to $u$ , $$(u^tAv)^2>(u^tAu)(v^tAv).$$ I tried to find proof of this result but failed. Anyone can help me or provide some references? Thanks a lot!","Let be a quadratic form over and write where is symmetric with positive entries and . Suppose that has exactly one positive eigenvalue and negative eigenvalues. IS IT TRUE that for any and that is not parallel to , I tried to find proof of this result but failed. Anyone can help me or provide some references? Thanks a lot!","f \mathbb{R} f(x)=xAx^t, A\in M_n(\mathbb{R_{+}}) x=(x_1,x_2,\ldots,x_n) A n-1 0\neq u\in \mathbb{R}_{\geq 0}^n v\in \mathbb{R}^n u (u^tAv)^2>(u^tAu)(v^tAv).","['linear-algebra', 'matrices', 'quadratic-forms', 'symmetric-matrices']"
42,Polynomial matrix equality and controllability,Polynomial matrix equality and controllability,,"Let $A\in\mathbb{R}^{n \times n}$ , $B\in\mathbb{R}^{n\times m}$ and $I$ be the $n\times n$ identity matrix. Show that for any polynomial $n$ -vector $X_0(s)$ with elements of degree $n-1$ or less, we can always find polynomial vectors $X(s)$ and $U(s)$ such that $(sI-A)X(s)-BU(s)=X_0(s)$ , if and only if $$\text{rank}[sI-A\quad B]=n\quad\forall s\in\mathbb{C}$$ i.e., $\{A,B\}$ is controllable. Thanks for any hint.","Let , and be the identity matrix. Show that for any polynomial -vector with elements of degree or less, we can always find polynomial vectors and such that , if and only if i.e., is controllable. Thanks for any hint.","A\in\mathbb{R}^{n \times n} B\in\mathbb{R}^{n\times m} I n\times n n X_0(s) n-1 X(s) U(s) (sI-A)X(s)-BU(s)=X_0(s) \text{rank}[sI-A\quad B]=n\quad\forall s\in\mathbb{C} \{A,B\}","['linear-algebra', 'matrices', 'polynomials', 'control-theory', 'matrix-rank']"
43,Unitary matrix commute with function,Unitary matrix commute with function,,"I'm wondering in which cases the following identity is satisfied : $$ f\left(UXU^T\right) = Uf\left(X\right)U^T $$ where $X \in \mathbb{R}^{n\times n}$ is a square matrix, $U$ is any permutation matrix and $ f:\mathbb{R}^{n\times n} \rightarrow \mathbb{R}^{n\times n}$ I already know of two cases : $f$ can be expressed as a matrix Taylor series (in this case $U$ could be any unitary matrix) $f$ is an element-wise function Are these the general cases? Bonus : Is there an extension of the preceding identity to tensors $T \in \mathbb{R}^{n^m}$ and $f:\mathbb{R}^{n^m} \rightarrow \mathbb{R}^{n^m}$ . I am not sure what form the product and the operator $U$ would take in that case.","I'm wondering in which cases the following identity is satisfied : where is a square matrix, is any permutation matrix and I already know of two cases : can be expressed as a matrix Taylor series (in this case could be any unitary matrix) is an element-wise function Are these the general cases? Bonus : Is there an extension of the preceding identity to tensors and . I am not sure what form the product and the operator would take in that case.","
f\left(UXU^T\right) = Uf\left(X\right)U^T
 X \in \mathbb{R}^{n\times n} U  f:\mathbb{R}^{n\times n} \rightarrow \mathbb{R}^{n\times n} f U f T \in \mathbb{R}^{n^m} f:\mathbb{R}^{n^m} \rightarrow \mathbb{R}^{n^m} U","['linear-algebra', 'matrices', 'linear-transformations', 'operator-theory', 'tensors']"
44,Multiplicity of Complex Conjugates of Repeated Complex Eigenvalues,Multiplicity of Complex Conjugates of Repeated Complex Eigenvalues,,"I know that for a real-valued matrix, complex eigenvalues come in complex conjugate pairs. However, I'm wondering what happens for repeated complex eigenvalues (i.e. complex eigenvalues with multiplicity greater than 1). In that case, does the complex conjugate of the repeated complex eigenvalue have the same multiplicity as that eigenvalue? If that statement holds, how can we show that it's true?","I know that for a real-valued matrix, complex eigenvalues come in complex conjugate pairs. However, I'm wondering what happens for repeated complex eigenvalues (i.e. complex eigenvalues with multiplicity greater than 1). In that case, does the complex conjugate of the repeated complex eigenvalue have the same multiplicity as that eigenvalue? If that statement holds, how can we show that it's true?",,"['linear-algebra', 'matrices', 'complex-numbers', 'eigenvalues-eigenvectors']"
45,"If $A$ is a normal matrix such that $A^{247} = A^{246}$, how to prove that $A$ is an orthogonal projection?","If  is a normal matrix such that , how to prove that  is an orthogonal projection?",A A^{247} = A^{246} A,"Let $A$ be a normal matrix and $A^{247} = A^{246}$ . Then I want to prove: $A^2 = A$ . $A$ is Hermitian. I'm not sure on how to prove this. If $A$ is normal, then $A$ is unitarily diagonalizable. Then $A^{247}=A^{246}$ implies $U^\ast A^{247}U = U^\ast  A^{246}U$ . Then if $D = U^\ast AU$ , can I say that $D^{247}=D^{246}$ ? I don't know how to continue or if my thinking was correct. Please help.","Let be a normal matrix and . Then I want to prove: . is Hermitian. I'm not sure on how to prove this. If is normal, then is unitarily diagonalizable. Then implies . Then if , can I say that ? I don't know how to continue or if my thinking was correct. Please help.",A A^{247} = A^{246} A^2 = A A A A A^{247}=A^{246} U^\ast A^{247}U = U^\ast  A^{246}U D = U^\ast AU D^{247}=D^{246},['linear-algebra']
46,If $ A $ is a $ 2 \times 2 $ real matrix such that $ \det (A) = 1 $ and $ A^n = I$ show that $ A ^tA = I $,If  is a  real matrix such that  and  show that, A   2 \times 2   \det (A) = 1   A^n = I  A ^tA = I ,"If $ A $ is a $ 2 \times 2 $ real matrix such that $\det (A) = 1 $ and $ A^n = I$ show that $ A ^tA = I $ IDEA: Since $ \text{det}(A) = 1 $ according to the Cayley-Hamilton theorem, it is true that $$A^2-\text{tr}(A)A+\text{det}(A)=0$$ then $A^{-1}=\text{tr}(A)I-A$ , just show that $ A^ {-1} = A^{t}$ , another way is to show that the columns of $ A $ form an orthonormal system of $\mathbb{R} ^ 2 $ but I don't see a way to test Can anyone give a suggestion ? Thank you.","If is a real matrix such that and show that IDEA: Since according to the Cayley-Hamilton theorem, it is true that then , just show that , another way is to show that the columns of form an orthonormal system of but I don't see a way to test Can anyone give a suggestion ? Thank you.", A   2 \times 2  \det (A) = 1   A^n = I  A ^tA = I   \text{det}(A) = 1  A^2-\text{tr}(A)A+\text{det}(A)=0 A^{-1}=\text{tr}(A)I-A  A^ {-1} = A^{t}  A  \mathbb{R} ^ 2 ,"['linear-algebra', 'matrices', 'orthogonal-matrices', 'cayley-hamilton']"
47,Do Approximate Eigenvalues Imply Approximate Eigenvectors?,Do Approximate Eigenvalues Imply Approximate Eigenvectors?,,"My apologies in advance if this has already been asked somewhere. Suppose I have two real symmetric matrices $A$ and $B$ in $\mathbb{R}^{d \times d}$ for which $\lVert A - B \rVert_{op} \le \varepsilon$ . Further, call the eigenvalue-eigenvector pairs for $A$ and $B$ as $(\lambda_i, u_i)$ and $(\tau_i, v_i)$ , for all $i \in [d]$ , and suppose that $\lVert u_i \rVert_2 = \lVert v_i \rVert_2 = 1$ for all $i \in [d]$ . My question is: under what condition can we say something interesting about $\lVert u_i - v_i \rVert_2$ ? So far, I've tried using the following facts. For all $i$ , $\lvert \lambda_i - \tau_i \rvert \le \varepsilon$ . If $\lvert \lambda_i - \tau_i \rvert \le \varepsilon$ , then we can write $\lVert Bu_i - \lambda_i u_i \rVert \le \varepsilon$ (the reason I thought this might be useful is that it shows that the eigenvalue-eigenvector pairs for $A$ are almost eigenvalue-eigenvector pairs for $B$ , in some sense) I'm not sure where to go from here, or if I should be looking someplace else entirely. Thank you in advance for the help!","My apologies in advance if this has already been asked somewhere. Suppose I have two real symmetric matrices and in for which . Further, call the eigenvalue-eigenvector pairs for and as and , for all , and suppose that for all . My question is: under what condition can we say something interesting about ? So far, I've tried using the following facts. For all , . If , then we can write (the reason I thought this might be useful is that it shows that the eigenvalue-eigenvector pairs for are almost eigenvalue-eigenvector pairs for , in some sense) I'm not sure where to go from here, or if I should be looking someplace else entirely. Thank you in advance for the help!","A B \mathbb{R}^{d \times d} \lVert A - B \rVert_{op} \le \varepsilon A B (\lambda_i, u_i) (\tau_i, v_i) i \in [d] \lVert u_i \rVert_2 = \lVert v_i \rVert_2 = 1 i \in [d] \lVert u_i - v_i \rVert_2 i \lvert \lambda_i - \tau_i \rvert \le \varepsilon \lvert \lambda_i - \tau_i \rvert \le \varepsilon \lVert Bu_i - \lambda_i u_i \rVert \le \varepsilon A B","['linear-algebra', 'matrices', 'eigenvalues-eigenvectors', 'symmetric-matrices', 'perturbation-theory']"
48,"How can I find real, symmetric matrices $A,B$ such that $Ax =\lambda B x $ has no real eigenvalues and eigenvectors? [closed]","How can I find real, symmetric matrices  such that  has no real eigenvalues and eigenvectors? [closed]","A,B Ax =\lambda B x ","Closed. This question does not meet Mathematics Stack Exchange guidelines . It is not currently accepting answers. Please provide additional context , which ideally explains why the question is relevant to you and our  community. Some forms of context include: background and motivation, relevant definitions, source, possible strategies, your current progress, why the question is interesting or important, etc. Closed 3 years ago . Improve this question $x \in \mathbb C^n$ is called eigenvector of the pair $(A,B)$ with eigenvalue $\lambda$ for two symmetric matrices $A,B \in \mathbb R^{n,n}$ if it holds $Ax = \lambda B x$ . What's the strategy behind finding two symmetric matrices $A,B \in \mathbb R^{n,n}$ such that the pair $(A,B)$ doesn't have any real eigenvalues or eigenvectors?","Closed. This question does not meet Mathematics Stack Exchange guidelines . It is not currently accepting answers. Please provide additional context , which ideally explains why the question is relevant to you and our  community. Some forms of context include: background and motivation, relevant definitions, source, possible strategies, your current progress, why the question is interesting or important, etc. Closed 3 years ago . Improve this question is called eigenvector of the pair with eigenvalue for two symmetric matrices if it holds . What's the strategy behind finding two symmetric matrices such that the pair doesn't have any real eigenvalues or eigenvectors?","x \in \mathbb C^n (A,B) \lambda A,B \in \mathbb R^{n,n} Ax = \lambda B x A,B \in \mathbb R^{n,n} (A,B)","['matrices', 'eigenvalues-eigenvectors', 'symmetric-matrices']"
49,Can an $n \times n$ matrix satisfy an $n$ degree polynomial equation other than its characteristic polynomial equation?,Can an  matrix satisfy an  degree polynomial equation other than its characteristic polynomial equation?,n \times n n,Can an $n \times n$ matrix satisfy an $n$ degree polynomial equation other than its characteristic polynomial equation? I was curious if the characteristic polynomial equation is the only $n$ degree equation that can be satisfied by a matrix. I have tried by trial and error to make up an equation for $2\times 2$ matrix but always end up with the characteristic polynomial.,Can an matrix satisfy an degree polynomial equation other than its characteristic polynomial equation? I was curious if the characteristic polynomial equation is the only degree equation that can be satisfied by a matrix. I have tried by trial and error to make up an equation for matrix but always end up with the characteristic polynomial.,n \times n n n 2\times 2,"['linear-algebra', 'matrices', 'matrix-equations', 'minimal-polynomials', 'characteristic-polynomial']"
50,Decomposing a $2\times 2$ matrix into rotation and scaling,Decomposing a  matrix into rotation and scaling,2\times 2,"How do you comprehensively decompose a 2x2 matrix into a scaling and a rotation matrix? I understand that a rotation matrix looks like: $$     \begin{pmatrix}     \cos \theta & -\sin\theta \\     \sin\theta  & \cos \theta\\         \end{pmatrix} $$ and a scaling matrix looks like: $$     \begin{pmatrix}     \alpha & 0 \\     0  & \alpha\\         \end{pmatrix} $$ The matrix I want to decompose is $$     \begin{pmatrix}     2 & -2 \\     2  & 2\\         \end{pmatrix} $$ The way they do it in my book is by defining the first column of a as vector $ r =(2,2)$ . Then $|r| = 2 \sqrt{2} $ . So the scaling factor $\alpha = 2\sqrt{2}$ and the rotation is $ \pi / 4 $ Can this be done with every matrix? And how about the second column. Doesn't that matter at all? Many thanks in advance!",How do you comprehensively decompose a 2x2 matrix into a scaling and a rotation matrix? I understand that a rotation matrix looks like: and a scaling matrix looks like: The matrix I want to decompose is The way they do it in my book is by defining the first column of a as vector . Then . So the scaling factor and the rotation is Can this be done with every matrix? And how about the second column. Doesn't that matter at all? Many thanks in advance!,"
    \begin{pmatrix}
    \cos \theta & -\sin\theta \\
    \sin\theta  & \cos \theta\\    
    \end{pmatrix}
 
    \begin{pmatrix}
    \alpha & 0 \\
    0  & \alpha\\    
    \end{pmatrix}
 
    \begin{pmatrix}
    2 & -2 \\
    2  & 2\\    
    \end{pmatrix}
  r =(2,2) |r| = 2 \sqrt{2}  \alpha = 2\sqrt{2}  \pi / 4 ","['linear-algebra', 'matrices', 'rotations', 'matrix-decomposition']"
51,Books on linear algebra more focused towards matrices and determinants rather than vector spaces,Books on linear algebra more focused towards matrices and determinants rather than vector spaces,,"In my syllabus of a competitive exam, we have matrices and determinants and solving linear equations with them instead of linear algebra but during examinations a lot of the times questions are derived from linear algebra and presented such that they can be tackled by matrices and determinants but are tedious. Eg, they will give a 2×2 matrix and then give 4 polynomial as options , where one of them would be a characteristic polynomial. So anyone who know about Cayley-Hamilton Theorem, will easily solve this quickly. So are there any books  available that have a lot of these properties like properties of eigenvectors, idempotent, nilpotent matrices,symmetric, skew symmetric etc. Most of the popular books revolve around teaching the vector spaces and that part really well. I have studied linear algebra from MIT OCW by Gilbert Strang and have partially read his book on the subject too, so even if the book has some parts from vector spaces (which it probably will) , I believe, I will probably be able to comprehend what the author is trying to convey.  Thanks","In my syllabus of a competitive exam, we have matrices and determinants and solving linear equations with them instead of linear algebra but during examinations a lot of the times questions are derived from linear algebra and presented such that they can be tackled by matrices and determinants but are tedious. Eg, they will give a 2×2 matrix and then give 4 polynomial as options , where one of them would be a characteristic polynomial. So anyone who know about Cayley-Hamilton Theorem, will easily solve this quickly. So are there any books  available that have a lot of these properties like properties of eigenvectors, idempotent, nilpotent matrices,symmetric, skew symmetric etc. Most of the popular books revolve around teaching the vector spaces and that part really well. I have studied linear algebra from MIT OCW by Gilbert Strang and have partially read his book on the subject too, so even if the book has some parts from vector spaces (which it probably will) , I believe, I will probably be able to comprehend what the author is trying to convey.  Thanks",,"['linear-algebra', 'matrices', 'soft-question', 'determinant', 'book-recommendation']"
52,$\operatorname{tr}(AB)$ in terms of $\operatorname{tr}(A)$,in terms of,\operatorname{tr}(AB) \operatorname{tr}(A),I have two symmetric and positive semi-definite matrices $A$ and $B$ . I know $\operatorname{tr}(AB) \neq \operatorname{tr}(A)\cdot \operatorname{tr}(B)$ . Are there any ways to think of $\operatorname{tr}(AB)$ in terms of $\operatorname{tr}(A)$ ?,I have two symmetric and positive semi-definite matrices and . I know . Are there any ways to think of in terms of ?,A B \operatorname{tr}(AB) \neq \operatorname{tr}(A)\cdot \operatorname{tr}(B) \operatorname{tr}(AB) \operatorname{tr}(A),"['linear-algebra', 'matrices']"
53,How to define the trace and deteraminant of a linear transformation in a basis-free manner?,How to define the trace and deteraminant of a linear transformation in a basis-free manner?,,Trace and determinant of a matrix can be shown to be invariant under basis transformation. This should suggest that they can be defined for a linear transformation in a basis-free manner. How does one do that? P.S. I'm really looking for a definition which can illustrate why the trace and determinant of matrices are invariant under basis transformations. Thanks very much!,Trace and determinant of a matrix can be shown to be invariant under basis transformation. This should suggest that they can be defined for a linear transformation in a basis-free manner. How does one do that? P.S. I'm really looking for a definition which can illustrate why the trace and determinant of matrices are invariant under basis transformations. Thanks very much!,,"['linear-algebra', 'matrices']"
54,Possible error in a STEP Paper: Matrix Group (STEP 1991 P2 Q9),Possible error in a STEP Paper: Matrix Group (STEP 1991 P2 Q9),,"I have been doing some STEP papers and I think I found a mistake on one of the papers. The paper is from 1991 (STEP 2) and the question (Q9) goes as follows: Let $G$ be the set of all matrices of the form $ \   \left[ {\begin{array}{cc}    a & b \\    0 & c \\   \end{array} } \right] \ $ where a,b and c are integers modulo $5$ , $a\neq0\neq b$ . Show that $G$ forms a group under matrix multiplication (which may be assumed to be associative). I've proven that the identity exists and is unique, however, I can't prove that the inverses are going to exist within G and ,moreover, that G is closed. Here are my arguments Inverses Suppose there exists $A$ such that $A \times \   \left[ {\begin{array}{cc}    a & b \\    0 & c \\   \end{array} } \right] \ $ $= \   \left[ {\begin{array}{cc}    1 & 0 \\    0 & 1 \\   \end{array} } \right] \ $ . It follows that $A= \   \left[ {\begin{array}{cc}    a & b \\    0 & c \\   \end{array} } \right]^{-1} \ $ $=\frac{1}{ac} \   \left[ {\begin{array}{cc}    c & -b \\    0 & a \\   \end{array} } \right] \ $ . The issues that arise are that $\frac{c}{ac}$ is not nesseraly an intiger for intiger c and similarly $\frac{a}{ac}$ is also not necceserally an intiger. Thus for $a\neq 1\neq c$ the inverses are not a member of $G$ . Closure Let $ \   \left[ {\begin{array}{cc}    a & b \\    0 & c \\   \end{array} } \right] \ $ and $ \   \left[ {\begin{array}{cc}    d & e \\    0 & f \\   \end{array} } \right] \ $ be members of $G$ . In doing matrix multiplication we get $ \   \left[ {\begin{array}{cc}    a & b \\    0 & c \\   \end{array} } \right] \ \times \   \left[ {\begin{array}{cc}    d & e \\    0 & f \\   \end{array} } \right] \ = \   \left[ {\begin{array}{cc}    ad & ae+bf \\    0 & cf \\   \end{array} } \right] \ $ We see that if $d$ is a multiple of $5$ then $ad\equiv0 \mod{5}$ . However, if the new matrix is formed $ad$ must obey $ad \neq 0$ which it obviously doesn't. Thus it is not closed. Can someone have a look at my arguments and let me know what I have done wrong or if I am right ? Here is the link to the full paper: https://pastpapercache.blob.core.windows.net/ppppapers/step/STEP%20II%201991.pdf Thanks in advance","I have been doing some STEP papers and I think I found a mistake on one of the papers. The paper is from 1991 (STEP 2) and the question (Q9) goes as follows: Let be the set of all matrices of the form where a,b and c are integers modulo , . Show that forms a group under matrix multiplication (which may be assumed to be associative). I've proven that the identity exists and is unique, however, I can't prove that the inverses are going to exist within G and ,moreover, that G is closed. Here are my arguments Inverses Suppose there exists such that . It follows that . The issues that arise are that is not nesseraly an intiger for intiger c and similarly is also not necceserally an intiger. Thus for the inverses are not a member of . Closure Let and be members of . In doing matrix multiplication we get We see that if is a multiple of then . However, if the new matrix is formed must obey which it obviously doesn't. Thus it is not closed. Can someone have a look at my arguments and let me know what I have done wrong or if I am right ? Here is the link to the full paper: https://pastpapercache.blob.core.windows.net/ppppapers/step/STEP%20II%201991.pdf Thanks in advance","G 
\
  \left[ {\begin{array}{cc}
   a & b \\
   0 & c \\
  \end{array} } \right]
\
 5 a\neq0\neq b G A A \times
\
  \left[ {\begin{array}{cc}
   a & b \\
   0 & c \\
  \end{array} } \right]
\
 =
\
  \left[ {\begin{array}{cc}
   1 & 0 \\
   0 & 1 \\
  \end{array} } \right]
\
 A=
\
  \left[ {\begin{array}{cc}
   a & b \\
   0 & c \\
  \end{array} } \right]^{-1}
\
 =\frac{1}{ac}
\
  \left[ {\begin{array}{cc}
   c & -b \\
   0 & a \\
  \end{array} } \right]
\
 \frac{c}{ac} \frac{a}{ac} a\neq 1\neq c G 
\
  \left[ {\begin{array}{cc}
   a & b \\
   0 & c \\
  \end{array} } \right]
\
 
\
  \left[ {\begin{array}{cc}
   d & e \\
   0 & f \\
  \end{array} } \right]
\
 G 
\
  \left[ {\begin{array}{cc}
   a & b \\
   0 & c \\
  \end{array} } \right]
\
\times
\
  \left[ {\begin{array}{cc}
   d & e \\
   0 & f \\
  \end{array} } \right]
\
=
\
  \left[ {\begin{array}{cc}
   ad & ae+bf \\
   0 & cf \\
  \end{array} } \right]
\
 d 5 ad\equiv0 \mod{5} ad ad \neq 0","['matrices', 'group-theory', 'solution-verification']"
55,Taking the gradient of $f(\mathbf{x}) = \frac{1}{2}\|\mathbf{A} \mathbf{x} - \mathbf{b}\|_2^2$,Taking the gradient of,f(\mathbf{x}) = \frac{1}{2}\|\mathbf{A} \mathbf{x} - \mathbf{b}\|_2^2,"Section 4.5 of the textbook Deep Learning by Goodfellow, Bengio, and Courville, says that the gradient of $$f(\mathbf{x}) = \dfrac{1}{2}\|\mathbf{A} \mathbf{x} - \mathbf{b}\|_2^2$$ is $$\nabla_{\mathbf{x}} f(\mathbf{x}) = \mathbf{A}^T (\mathbf{A}\mathbf{x} - \mathbf{b}) = \mathbf{A}^T \mathbf{A} \mathbf{x} - \mathbf{A}^T \mathbf{b}$$ My understanding is that $f(\mathbf{x}) = \dfrac{1}{2}\|\mathbf{A} \mathbf{x} - \mathbf{b}\|_2^2$ is the square of the Euclidean norm . So we have that $$\begin{align} f(\mathbf{x}) = \dfrac{1}{2}\|\mathbf{A} \mathbf{x} - \mathbf{b}\|_2^2 &= \dfrac{1}{2} \left( \sqrt{(\mathbf{A} \mathbf{x} - \mathbf{b})^2} \right)^2 \\ &= \dfrac{1}{2} (\mathbf{A} \mathbf{x} - \mathbf{b})^2 \\ &= \dfrac{1}{2} (\mathbf{A} \mathbf{x} - \mathbf{b})(\mathbf{A} \mathbf{x} - \mathbf{b}) \\ &= \dfrac{1}{2} [ (\mathbf{A}\mathbf{x})(\mathbf{A} \mathbf{x}) - (\mathbf{A} \mathbf{x})\mathbf{b} - (\mathbf{A} \mathbf{x})\mathbf{b} + \mathbf{b}^2 ] \ \ \text{(Since matrix multiplication is distributive.)} \\ &= \dfrac{1}{2} [(\mathbf{A} \mathbf{x})^2 - 2(\mathbf{A} \mathbf{x})\mathbf{b} + \mathbf{b}^2] \ \  \text{(Note: Matrix multiplication is not commutative.)} \end{align}$$ It's at this point that I realised that, since we're working with matrices, I'm not really sure how to take the gradient of this. Taking the gradient of $f(\mathbf{x})$ with respect to $\mathbf{x}$ , we get something like $$\nabla_{\mathbf{x}} f(\mathbf{x}) = \dfrac{1}{2} [2 (\mathbf{A} \mathbf{x}) \mathbf{A}] - \dfrac{1}{2}[2(\mathbf{A} \mathbf{A} \mathbf{x})\mathbf{b}]$$ So what is the reasoning that leads us to get $\nabla_{\mathbf{x}} f(\mathbf{x}) = \mathbf{A}^T (\mathbf{A}\mathbf{x} - \mathbf{b}) = \mathbf{A}^T \mathbf{A} \mathbf{x} - \mathbf{A}^T \mathbf{b}$ ? Where did the transposed matrices come from? I would greatly appreciate it if people would please take the time to clarify this.","Section 4.5 of the textbook Deep Learning by Goodfellow, Bengio, and Courville, says that the gradient of is My understanding is that is the square of the Euclidean norm . So we have that It's at this point that I realised that, since we're working with matrices, I'm not really sure how to take the gradient of this. Taking the gradient of with respect to , we get something like So what is the reasoning that leads us to get ? Where did the transposed matrices come from? I would greatly appreciate it if people would please take the time to clarify this.",f(\mathbf{x}) = \dfrac{1}{2}\|\mathbf{A} \mathbf{x} - \mathbf{b}\|_2^2 \nabla_{\mathbf{x}} f(\mathbf{x}) = \mathbf{A}^T (\mathbf{A}\mathbf{x} - \mathbf{b}) = \mathbf{A}^T \mathbf{A} \mathbf{x} - \mathbf{A}^T \mathbf{b} f(\mathbf{x}) = \dfrac{1}{2}\|\mathbf{A} \mathbf{x} - \mathbf{b}\|_2^2 \begin{align} f(\mathbf{x}) = \dfrac{1}{2}\|\mathbf{A} \mathbf{x} - \mathbf{b}\|_2^2 &= \dfrac{1}{2} \left( \sqrt{(\mathbf{A} \mathbf{x} - \mathbf{b})^2} \right)^2 \\ &= \dfrac{1}{2} (\mathbf{A} \mathbf{x} - \mathbf{b})^2 \\ &= \dfrac{1}{2} (\mathbf{A} \mathbf{x} - \mathbf{b})(\mathbf{A} \mathbf{x} - \mathbf{b}) \\ &= \dfrac{1}{2} [ (\mathbf{A}\mathbf{x})(\mathbf{A} \mathbf{x}) - (\mathbf{A} \mathbf{x})\mathbf{b} - (\mathbf{A} \mathbf{x})\mathbf{b} + \mathbf{b}^2 ] \ \ \text{(Since matrix multiplication is distributive.)} \\ &= \dfrac{1}{2} [(\mathbf{A} \mathbf{x})^2 - 2(\mathbf{A} \mathbf{x})\mathbf{b} + \mathbf{b}^2] \ \  \text{(Note: Matrix multiplication is not commutative.)} \end{align} f(\mathbf{x}) \mathbf{x} \nabla_{\mathbf{x}} f(\mathbf{x}) = \dfrac{1}{2} [2 (\mathbf{A} \mathbf{x}) \mathbf{A}] - \dfrac{1}{2}[2(\mathbf{A} \mathbf{A} \mathbf{x})\mathbf{b}] \nabla_{\mathbf{x}} f(\mathbf{x}) = \mathbf{A}^T (\mathbf{A}\mathbf{x} - \mathbf{b}) = \mathbf{A}^T \mathbf{A} \mathbf{x} - \mathbf{A}^T \mathbf{b},"['linear-algebra', 'matrices', 'multivariable-calculus', 'derivatives', 'matrix-calculus']"
56,Matrix right multiplication orbit invariant,Matrix right multiplication orbit invariant,,"Let M and N be two square matrices such that they have an equal image. Show that there exists an invertible matrix P, such that N = MP I have managed to prove the converse...but this is tougher because I'm guessing we need to build a linear map which then identifies with a matrix P.","Let M and N be two square matrices such that they have an equal image. Show that there exists an invertible matrix P, such that N = MP I have managed to prove the converse...but this is tougher because I'm guessing we need to build a linear map which then identifies with a matrix P.",,"['linear-algebra', 'matrices', 'linear-transformations', 'group-actions']"
57,Complex skew-symmetric matrices,Complex skew-symmetric matrices,,"Horn and Johnson's matrix analysis makes the following interesting statements about the Jordan canonical forms of symmetric and skew-symmetric matrices. Note: I am concerned here with matrices that have complex entries, and I am considering the entrywise transpose rather than the conjugate-transpose. Regarding symmetric matrices: Theorem 4.4.24: Each $A \in M_n$ is similar to a complex-symmetric matrix. Regarding skew-symmetric matrices: 4.4.P34: Although a symmetric complex matrix can have any given Jordan canonical form (4.4.24), the Jordan canonical form of a skew-symmetric complex matrix has a special form. It consists of only the following three types of direct summands: (a) pairs of the form $J_k(\lambda) \oplus J_k(-\lambda)$ , in which $\lambda \neq 0$ ; (b) pairs of the form $J_k(0) \oplus J_k(0)$ , in which k is even; and (c) $J_k(0)$ , in which k is odd. Explain why the Jordan canonical form of a complex skew-symmetric matrix $A$ ensures that $A$ is similar to $−A$ ; also deduce this fact from [similarity of a matrix to its transpose]. In the above, $J_k(\lambda)$ denotes the Jordan block of size $k$ associated with eigenvalue $\lambda$ . The exercise given is easy enough, but I'd like to prove the leading assertion. To that end, I have found a useful trick: if $A$ is skew-symmetric and $B$ is symmetric, then $A \otimes B$ is skew-symmetric (where $\otimes$ denotes a Kronecker product).  With this trick together with the theorem above, it is easy to find examples of skew-symmetric matrices similar to summands (a) and (b).  However, that's as far as I got, which leaves me with two questions. Questions: How can we construct a skew-symmetric matrix that is similar to $J_k(0)$ , where $k$ is odd? Why are there no skew-symmetric matrices similar to $J_k(0)$ , where $k$ is even? Thank you for your consideration. An update: one way to answer question 2 is as follows.  We have the following result: Corollary 4.4.19: Let $A \in M_n$ be skew-symmetric.  Then $r = \operatorname{rank}(A)$ is even, the non-zero singular values of $A$ occurs in pairs $\sigma_1 = \sigma_2 = s_1 \geq \sigma_3 = \sigma_4 = s_2 \geq \cdots \geq \sigma_{r-1} = \sigma_r = s_{r/2} \geq 0$ , and $A$ is unitarily congruent to $$ 0_{n-r} \oplus \pmatrix{0&s_1\\-s_1 & 0} \oplus \cdots \oplus \pmatrix{0&s_{r/2}\\-s_{r/2} & 0}. $$ By the way: $A$ is unitary congruent to $B$ means that $A = UBU^T$ for some unitary matrix $U$ ; note that this is not necessarily a matrix similarity. Because $A$ has singular values that occur in pairs, we can preclude the possibility that $A$ is similar to any matrix of odd rank.  For even $k$ , $J_k(0)$ is such a martix. I would still be interested in an argument that doesn't use this fact though; perhaps there is an easy way to see that a skew-symmetric matrix must have even rank. Possibly useful observations: The rank of $A$ is the same as that of the Hermitian matrix $A^*A = \overline{A^T}A = - \bar A A$ . Due to the above corollary, we will necessarily be able to write a matrix that is similar to $J_3(0)$ in the form $$ A = U\pmatrix{0&1&0\\-1&0&0\\0&0&0}U^T = u_1u_2^T - u_2u_1^T $$ where columns $u_1,u_2$ of $U$ are orthonormal.","Horn and Johnson's matrix analysis makes the following interesting statements about the Jordan canonical forms of symmetric and skew-symmetric matrices. Note: I am concerned here with matrices that have complex entries, and I am considering the entrywise transpose rather than the conjugate-transpose. Regarding symmetric matrices: Theorem 4.4.24: Each is similar to a complex-symmetric matrix. Regarding skew-symmetric matrices: 4.4.P34: Although a symmetric complex matrix can have any given Jordan canonical form (4.4.24), the Jordan canonical form of a skew-symmetric complex matrix has a special form. It consists of only the following three types of direct summands: (a) pairs of the form , in which ; (b) pairs of the form , in which k is even; and (c) , in which k is odd. Explain why the Jordan canonical form of a complex skew-symmetric matrix ensures that is similar to ; also deduce this fact from [similarity of a matrix to its transpose]. In the above, denotes the Jordan block of size associated with eigenvalue . The exercise given is easy enough, but I'd like to prove the leading assertion. To that end, I have found a useful trick: if is skew-symmetric and is symmetric, then is skew-symmetric (where denotes a Kronecker product).  With this trick together with the theorem above, it is easy to find examples of skew-symmetric matrices similar to summands (a) and (b).  However, that's as far as I got, which leaves me with two questions. Questions: How can we construct a skew-symmetric matrix that is similar to , where is odd? Why are there no skew-symmetric matrices similar to , where is even? Thank you for your consideration. An update: one way to answer question 2 is as follows.  We have the following result: Corollary 4.4.19: Let be skew-symmetric.  Then is even, the non-zero singular values of occurs in pairs , and is unitarily congruent to By the way: is unitary congruent to means that for some unitary matrix ; note that this is not necessarily a matrix similarity. Because has singular values that occur in pairs, we can preclude the possibility that is similar to any matrix of odd rank.  For even , is such a martix. I would still be interested in an argument that doesn't use this fact though; perhaps there is an easy way to see that a skew-symmetric matrix must have even rank. Possibly useful observations: The rank of is the same as that of the Hermitian matrix . Due to the above corollary, we will necessarily be able to write a matrix that is similar to in the form where columns of are orthonormal.","A \in M_n J_k(\lambda) \oplus J_k(-\lambda) \lambda \neq 0 J_k(0) \oplus J_k(0) J_k(0) A A −A J_k(\lambda) k \lambda A B A \otimes B \otimes J_k(0) k J_k(0) k A \in M_n r = \operatorname{rank}(A) A \sigma_1 = \sigma_2 = s_1 \geq \sigma_3 = \sigma_4 = s_2 \geq \cdots \geq \sigma_{r-1} = \sigma_r = s_{r/2} \geq 0 A 
0_{n-r} \oplus \pmatrix{0&s_1\\-s_1 & 0} \oplus \cdots \oplus \pmatrix{0&s_{r/2}\\-s_{r/2} & 0}.
 A B A = UBU^T U A A k J_k(0) A A^*A = \overline{A^T}A = - \bar A A J_3(0) 
A = U\pmatrix{0&1&0\\-1&0&0\\0&0&0}U^T = u_1u_2^T - u_2u_1^T
 u_1,u_2 U","['linear-algebra', 'matrices', 'complex-numbers', 'skew-symmetric-matrices']"
58,"How do ""Eigenbases"" and ""orthonormal bases"" relate?","How do ""Eigenbases"" and ""orthonormal bases"" relate?",,"So I have a definition for each of the above: Eigenbasis : when the matrix in question is in diagonal form. Only possible when there are n eigenvectors for a matrix in n-dimensional space. Orthonormal basis : when basis vectors are a)orthogonal, b)unit-length. What I'm trying to understand is - how do they relate? Like, is eigenbasis always orthonormal? Or does orthonormal basis always have an eigenbasis? Or are the two the same even? Thanks!","So I have a definition for each of the above: Eigenbasis : when the matrix in question is in diagonal form. Only possible when there are n eigenvectors for a matrix in n-dimensional space. Orthonormal basis : when basis vectors are a)orthogonal, b)unit-length. What I'm trying to understand is - how do they relate? Like, is eigenbasis always orthonormal? Or does orthonormal basis always have an eigenbasis? Or are the two the same even? Thanks!",,"['linear-algebra', 'matrices', 'eigenvalues-eigenvectors', 'change-of-basis']"
59,Does every bistochastic matrix $A$ have a unitary matrix $U$ s.t. $a_{ij} = |u_{ij}|^2$?,Does every bistochastic matrix  have a unitary matrix  s.t. ?,A U a_{ij} = |u_{ij}|^2,"Bistochastic matrix is a square matrix made of non-negative reals s.t. the sum of elements of any row or column equal 1. Unitary matrix is a square matrix with complex entries s.t. $UU^*=E$ It's known that if you take a Unitary matrix and take the squared modulus of every entry then you get a bistochastic one. Is the converse true? Does every bistochastic matrix $A$ have a unitary matrix $U$ s.t. $a_{ij} = |u_{ij}|^2$ ? If yes, what is the answer if we restrict the set of matrices to orthogonal real ones? I tried to write all the equations $U$ should satisfy in two-dimensional case. We have $6$ orthogonality relations and $4$ squared-modulus relations. Note that just two orthogonality relation don't hold automatically since we have squared-modulus relations. I tried to find such matrix for the matrix $\begin{pmatrix} \frac{1}{2} & \frac{1}{2} \\ \frac{1}{2} & \frac{1}{2} \end{pmatrix} $ , it lead me to $\frac{1}{\sqrt{2}}(\cos2\pi n / 4 + i \sin 2 \pi n /4)$ . We can take any four complex numbers forming a square. The modified construction works for any matrix $\begin{pmatrix} a & 1-a \\ 1-a & a \end{pmatrix} $ Furthermore, we can rotate all the complex numbers and the relations still hold.","Bistochastic matrix is a square matrix made of non-negative reals s.t. the sum of elements of any row or column equal 1. Unitary matrix is a square matrix with complex entries s.t. It's known that if you take a Unitary matrix and take the squared modulus of every entry then you get a bistochastic one. Is the converse true? Does every bistochastic matrix have a unitary matrix s.t. ? If yes, what is the answer if we restrict the set of matrices to orthogonal real ones? I tried to write all the equations should satisfy in two-dimensional case. We have orthogonality relations and squared-modulus relations. Note that just two orthogonality relation don't hold automatically since we have squared-modulus relations. I tried to find such matrix for the matrix , it lead me to . We can take any four complex numbers forming a square. The modified construction works for any matrix Furthermore, we can rotate all the complex numbers and the relations still hold.",UU^*=E A U a_{ij} = |u_{ij}|^2 U 6 4 \begin{pmatrix} \frac{1}{2} & \frac{1}{2} \\ \frac{1}{2} & \frac{1}{2} \end{pmatrix}  \frac{1}{\sqrt{2}}(\cos2\pi n / 4 + i \sin 2 \pi n /4) \begin{pmatrix} a & 1-a \\ 1-a & a \end{pmatrix} ,"['linear-algebra', 'matrices', 'unitary-matrices']"
60,Prove that $(\mathbf{AB})^{T} = \mathbf B^{T}\mathbf A^{T}$ where $\mathbf A$ and $\mathbf B$ are matrices,Prove that  where  and  are matrices,(\mathbf{AB})^{T} = \mathbf B^{T}\mathbf A^{T} \mathbf A \mathbf B,"I am asked to prove following: Let $\mathbf A$ and $\mathbf B$ be matrices. Prove that $(\mathbf{AB})^{T} = \mathbf B^{T}\mathbf A^{T}$ My attempt: Consider arbitrary entry of the $(\mathbf A \mathbf B)^{T}$ , namely $((\mathbf A \mathbf B)^{T})_{i,j}$ $$((\mathbf A \mathbf B)^{T})_{i,j} = (\mathbf A \mathbf B_{j,i})^{T} =\sum_{k=1}^{n}(a_{j,k}b_{k,i})^{T} = \sum_{k=1}^{n}a_{k,j}b_{i,k} =\sum_{k=1}^{n} b_{i,k}a_{k,j} = (B^{T}A^{T})_{i,j}$$ Since we considered arbitrary entry, we conclude that $(\mathbf{AB})^{T} = \mathbf B^{T}\mathbf A^{T}$ $\Box$ Is it correct? Although I can't tell for sure, I believe that something is wrong with the proof above. The step that concerns me the most (perhaps because of the notation involved) is $$\tag!\sum_{k=1}^{n}(a_{j,k}b_{k,i})^{T} = \sum_{k=1}^{n}a_{k,j}b_{i,k} $$","I am asked to prove following: Let and be matrices. Prove that My attempt: Consider arbitrary entry of the , namely Since we considered arbitrary entry, we conclude that Is it correct? Although I can't tell for sure, I believe that something is wrong with the proof above. The step that concerns me the most (perhaps because of the notation involved) is","\mathbf A \mathbf B (\mathbf{AB})^{T} = \mathbf B^{T}\mathbf A^{T} (\mathbf A \mathbf B)^{T} ((\mathbf A \mathbf B)^{T})_{i,j} ((\mathbf A \mathbf B)^{T})_{i,j} = (\mathbf A \mathbf B_{j,i})^{T} =\sum_{k=1}^{n}(a_{j,k}b_{k,i})^{T} = \sum_{k=1}^{n}a_{k,j}b_{i,k} =\sum_{k=1}^{n} b_{i,k}a_{k,j} = (B^{T}A^{T})_{i,j} (\mathbf{AB})^{T} = \mathbf B^{T}\mathbf A^{T} \Box \tag!\sum_{k=1}^{n}(a_{j,k}b_{k,i})^{T} = \sum_{k=1}^{n}a_{k,j}b_{i,k} ","['linear-algebra', 'matrices', 'proof-verification', 'transpose']"
61,Rank 3 tensor multiplied by vectors,Rank 3 tensor multiplied by vectors,,"Some background before the math: I am reading a paper that, in its appendix, looks at chemical reactions of the form $$X_i+Y_i\overset{k_i^+}{\underset{k_i^-}{\rightleftharpoons}} Z_i$$ where species $X_i$ and $Y_i$ reversibly combine to make species $Z_i$ with forward rate constant $k_i^+$ and reverse rate constant $k_i^-$ . The paper analyzes many coupled reactions of this form, where the $Z_i$ of reaction $i$ could be $X_j$ or even $Z_j$ for some other reaction $j$ . The paper uses the law of mass action to obtain differential equations for the concentrations of the various species: \begin{align} \frac{\text dX_i}{\text dt}=k_i^-Z_i-k_i^+X_iY_i \\\\ \frac{\text dY_i}{\text dt}=k_i^-Z_i-k_i^+X_iY_i \\\\ \frac{\text dZ_i}{\text dt}=k_i^+X_iY_i-k_i^-Z_i \end{align} where coupled reactions are combined into a single differential equation. For example, if we have two reactions $$A_i+B_i\overset{k_i^+}{\underset{k_i^-}{\rightleftharpoons}} C_i$$ $$A_j+B_j\overset{k_j^+}{\underset{k_j^-}{\rightleftharpoons}} C_j$$ and $C_i=C_j$ , then we have $$\frac{\text dC_i}{\text dt}=k_i^+A_iB_i+k_j^+A_jB_j-(k_i^-+k_j^-)C_i$$ The paper states at the beginning of section A4: Using the law of mass action, the kinetic schemes   presented above can be formulated in terms of a set of coupled ordinary differential equations. It suffices to give these equations in the shorthand form $$\frac{\text d\mathbf u}{\text dt}=\mathbf A\mathbf u+\mathbf B\mathbf u\mathbf u$$ $$\mathbf u=\mathbf u_0 \text{ at } t=0$$ where $\mathbf u$ is the vector of species concentrations, $\mathbf A$ is a matrix specifying the reverse reaction steps, and $\mathbf B$ is a rank 3 tensor specifying the forward reaction steps. I understand the $\mathbf A\mathbf u$ term, but I am having more trouble understanding the $\mathbf B\mathbf u\mathbf u$ term. I understand that each of the forward reaction terms involve a product of $XY$ . So I guess the $\mathbf u\mathbf u$ product is a sort of outer product that gives us a matrix with elements that are all possible products of species (correct me if I am wrong about this). But then what does the $\mathbf B$ tensor even look like, and how is it multiplying the $\mathbf u\mathbf u$ term?","Some background before the math: I am reading a paper that, in its appendix, looks at chemical reactions of the form where species and reversibly combine to make species with forward rate constant and reverse rate constant . The paper analyzes many coupled reactions of this form, where the of reaction could be or even for some other reaction . The paper uses the law of mass action to obtain differential equations for the concentrations of the various species: where coupled reactions are combined into a single differential equation. For example, if we have two reactions and , then we have The paper states at the beginning of section A4: Using the law of mass action, the kinetic schemes   presented above can be formulated in terms of a set of coupled ordinary differential equations. It suffices to give these equations in the shorthand form where is the vector of species concentrations, is a matrix specifying the reverse reaction steps, and is a rank 3 tensor specifying the forward reaction steps. I understand the term, but I am having more trouble understanding the term. I understand that each of the forward reaction terms involve a product of . So I guess the product is a sort of outer product that gives us a matrix with elements that are all possible products of species (correct me if I am wrong about this). But then what does the tensor even look like, and how is it multiplying the term?","X_i+Y_i\overset{k_i^+}{\underset{k_i^-}{\rightleftharpoons}} Z_i X_i Y_i Z_i k_i^+ k_i^- Z_i i X_j Z_j j \begin{align}
\frac{\text dX_i}{\text dt}=k_i^-Z_i-k_i^+X_iY_i \\\\
\frac{\text dY_i}{\text dt}=k_i^-Z_i-k_i^+X_iY_i \\\\
\frac{\text dZ_i}{\text dt}=k_i^+X_iY_i-k_i^-Z_i
\end{align} A_i+B_i\overset{k_i^+}{\underset{k_i^-}{\rightleftharpoons}} C_i A_j+B_j\overset{k_j^+}{\underset{k_j^-}{\rightleftharpoons}} C_j C_i=C_j \frac{\text dC_i}{\text dt}=k_i^+A_iB_i+k_j^+A_jB_j-(k_i^-+k_j^-)C_i \frac{\text d\mathbf u}{\text dt}=\mathbf A\mathbf u+\mathbf B\mathbf u\mathbf u \mathbf u=\mathbf u_0 \text{ at } t=0 \mathbf u \mathbf A \mathbf B \mathbf A\mathbf u \mathbf B\mathbf u\mathbf u XY \mathbf u\mathbf u \mathbf B \mathbf u\mathbf u","['matrices', 'ordinary-differential-equations', 'vectors', 'tensor-products']"
62,Prove this matrix to be unitary,Prove this matrix to be unitary,,"This is a homework question so, hints are appreciated. But if someone is generous enough, to show the full calculation, I'd be quite grateful! Say a matrix B is anti-hermitian: $$\begin{bmatrix} i & -1\\  1 & i \end{bmatrix}$$ And I want to calculate the matrix exponential $e^{Bt}$ , where the exponential is a function of time. Now after I attempted my own calculation for $e^{Bt}$ , using Taylor series and Caley Hamilton theorem, I ended up with this: \begin{bmatrix} \ \frac{e^{2it}+1}{2} & \frac{e^{2it}-1}{2i}\\   \frac{1- e^{2it}}{2i} & \frac{e^{2it}+1}{2} \end{bmatrix} But here's the problem, in theory if the exponent of a unitary operator is anti-hermitian, then the operator is unitary. But now that I have gotten the matrix representation of the matrix exponential, and I want to prove the above matrix to be unitary using the statement: $$U^{-1}=U^{\dagger}$$ I run into some problem. Can some just please check if my matrix exponential is correct or if I am missing something, that prevents me from proving $e^{Bt}$ as unitary using $U^{-1}=U^{\dagger}$ .  Thanks a ton, in advance!","This is a homework question so, hints are appreciated. But if someone is generous enough, to show the full calculation, I'd be quite grateful! Say a matrix B is anti-hermitian: And I want to calculate the matrix exponential , where the exponential is a function of time. Now after I attempted my own calculation for , using Taylor series and Caley Hamilton theorem, I ended up with this: But here's the problem, in theory if the exponent of a unitary operator is anti-hermitian, then the operator is unitary. But now that I have gotten the matrix representation of the matrix exponential, and I want to prove the above matrix to be unitary using the statement: I run into some problem. Can some just please check if my matrix exponential is correct or if I am missing something, that prevents me from proving as unitary using .  Thanks a ton, in advance!","\begin{bmatrix}
i & -1\\ 
1 & i
\end{bmatrix} e^{Bt} e^{Bt} \begin{bmatrix}
\ \frac{e^{2it}+1}{2} & \frac{e^{2it}-1}{2i}\\ 
 \frac{1- e^{2it}}{2i} & \frac{e^{2it}+1}{2}
\end{bmatrix} U^{-1}=U^{\dagger} e^{Bt} U^{-1}=U^{\dagger}","['linear-algebra', 'matrices', 'taylor-expansion', 'matrix-exponential', 'unitary-matrices']"
63,Condition number and $LU$ decomposition,Condition number and  decomposition,LU,"Consider $A: n \times n$ non-singular and the factors $L$ and $U$ of $A$ obtained with partial pivoting strategy, such as: $PA = LU$ .   Proof that $$\kappa_{\infty}(A) \geq \dfrac{||A||_{\infty}}{\min_{j}|u_{jj}|}.$$ The condition number $\kappa_{\infty}(A)$ is defined by $\kappa_{\infty}(A)=||A||_{\infty}||A^{-1}||_{\infty}$ . All I could show was that $\kappa_{\infty}(A) \geq \dfrac{||A||_{\infty}}{n ||U||_{\infty}}$ . But I can't get the "" $n$ "" from the denominator. This question seems to me to have some trick that I can´t get it. I discussed with some colleagues and we were thinking that this question is wrong. But still, we don´t know how to prove it.","Consider non-singular and the factors and of obtained with partial pivoting strategy, such as: .   Proof that The condition number is defined by . All I could show was that . But I can't get the "" "" from the denominator. This question seems to me to have some trick that I can´t get it. I discussed with some colleagues and we were thinking that this question is wrong. But still, we don´t know how to prove it.",A: n \times n L U A PA = LU \kappa_{\infty}(A) \geq \dfrac{||A||_{\infty}}{\min_{j}|u_{jj}|}. \kappa_{\infty}(A) \kappa_{\infty}(A)=||A||_{\infty}||A^{-1}||_{\infty} \kappa_{\infty}(A) \geq \dfrac{||A||_{\infty}}{n ||U||_{\infty}} n,"['matrices', 'numerical-linear-algebra', 'matrix-decomposition', 'condition-number', 'lu-decomposition']"
64,The Similarity Matrix of graph Laplacian Matrix has different names. What's the difference between these names?,The Similarity Matrix of graph Laplacian Matrix has different names. What's the difference between these names?,,"The graph Laplacian is defined: $$L=D-W$$ Where $W$ is the Similarity Matrix of the graph and $D$ is a diagonal matrix whose entries are column sums of $W$ (or row sums, by symmetry of $W$ ). $W$ has multiple names: the similarity matrix the weight matrix the affinity matrix the adjacency matrix  etc. Are these names equivalent? Or are there subtle differences in their use? References: Graph Regularized Nonnegative Matrix Factorization for Data Representation (p.3) The $W$ is called the weight matrix. Co-Clustering on Manifolds (p.2) The $W$ is called the affinity matrix.","The graph Laplacian is defined: Where is the Similarity Matrix of the graph and is a diagonal matrix whose entries are column sums of (or row sums, by symmetry of ). has multiple names: the similarity matrix the weight matrix the affinity matrix the adjacency matrix  etc. Are these names equivalent? Or are there subtle differences in their use? References: Graph Regularized Nonnegative Matrix Factorization for Data Representation (p.3) The is called the weight matrix. Co-Clustering on Manifolds (p.2) The is called the affinity matrix.",L=D-W W D W W W W W,"['matrices', 'graph-theory', 'manifolds', 'laplacian', 'graph-laplacian']"
65,confused about a matrix problem,confused about a matrix problem,,"question: $$A=\begin{bmatrix} 1&2 \\3&4\end{bmatrix} , B=\begin{bmatrix} k&2 \\3&9\end{bmatrix}, (AB)^5=A^5B^5$$ what's the value of k? I know I can let $AB=BA$ , which is $$\begin{bmatrix}\ k+6&20\\3k+12&42\end{bmatrix}=\begin{bmatrix}\ k+6&2k+8\\30&42\end{bmatrix},$$ and I can get $k=6$ . But I am not pretty sure that it's the only answer.  I want to know if there is any other answer or solution to this question.","question: what's the value of k? I know I can let , which is and I can get . But I am not pretty sure that it's the only answer.  I want to know if there is any other answer or solution to this question.","A=\begin{bmatrix} 1&2 \\3&4\end{bmatrix} ,
B=\begin{bmatrix} k&2 \\3&9\end{bmatrix},
(AB)^5=A^5B^5 AB=BA \begin{bmatrix}\ k+6&20\\3k+12&42\end{bmatrix}=\begin{bmatrix}\ k+6&2k+8\\30&42\end{bmatrix}, k=6","['linear-algebra', 'matrices']"
66,"Yaw, Pitch and Roll composition","Yaw, Pitch and Roll composition",,"I'm trying to understand composition of rotations using eulers angles and rotation matrices. I am facing a counterintuitive situation performing two rotations about different angles. My setting is the following: first rotation of an angle $\psi$ about the Z body-axis (yaw) second rotation of an angle $\theta$ about the Y body-axis (pitch) third rotation of an angle $\varphi$ about the X body-axis (roll) Denoting with $$R_X=\begin{bmatrix}1&0&0\\ 0&\cos(\varphi)&-\sin(\varphi)\\ 0&\sin(\varphi)&\cos(\varphi)\end{bmatrix}$$ $$R_Y=\begin{bmatrix}\cos(\theta)&0&\sin(\theta)\\ 0&1&0\\ -\sin(\theta)&0&\cos(\theta)\end{bmatrix}$$ $$R_Z=\begin{bmatrix}\cos(\psi)&-\sin(\psi)&0\\ \sin(\psi)&\cos(\psi)&0\\ 0&0&1\end{bmatrix}$$ the rotation matrices relative to elementary rotation about the three body axis, I obtain that a general rotation of eulers angles $(yaw,pitch,roll)=(\psi,\theta,\varphi)$ is given by: $$R=R_X\cdot R_Y\cdot R_Z$$ Suppose now that I would like to perform this 2 consecutive rotations: Rotation of $(yaw,pitch,roll)=(0,\frac{\pi}{4},0)$ Rotation of $(yaw,pitch,roll)=(\frac{\pi}{2},0,0)$ The matrices relative to these two rotations are the following: $$R_1=\begin{bmatrix}\frac{\sqrt{2}}{2}&0&\frac{\sqrt{2}}{2}\\ 0&1&0\\ -\frac{\sqrt{2}}{2}&0&\frac{\sqrt{2}}{2}\end{bmatrix}$$ $$R_2=\begin{bmatrix}1&0&0\\ 0&0&-1\\ 0&1&0\end{bmatrix}$$ and composing them I obtain the total rotation matrix $$R=R_2\cdot R_1 = \begin{bmatrix}0&-1&0\\ \frac{\sqrt{2}}{2}&0&\frac{\sqrt{2}}{2}\\ -\frac{\sqrt{2}}{2}&0&\frac{\sqrt{2}}{2}\end{bmatrix}$$ If I want to recover the euler angles sequence relative to $R$ I apply these equations: $$\theta=\arcsin(R_{13})\\ \psi=-\arctan2\left(\frac{R_{12}}{\cos(\theta)},\frac{R_{11}}{\cos(\theta)}\right)\\ \varphi=-\arctan2\left(\frac{R_{23}}{\cos(\theta)},\frac{R_{33}}{\cos(\theta)}\right)$$ obtaining $(yaw,pitch,roll)=(\frac{\pi}{2},0,-\frac{\pi}{4})$ . However applying this sequence of rotation I do not recover the initial frame (sequence of rotations 1-2) unless the rotations 1-2 are performed on different axis respect the body ones. Probably I miss something and I'm making confusion with these concepts. Please, could you help me to understand where I miss?","I'm trying to understand composition of rotations using eulers angles and rotation matrices. I am facing a counterintuitive situation performing two rotations about different angles. My setting is the following: first rotation of an angle about the Z body-axis (yaw) second rotation of an angle about the Y body-axis (pitch) third rotation of an angle about the X body-axis (roll) Denoting with the rotation matrices relative to elementary rotation about the three body axis, I obtain that a general rotation of eulers angles is given by: Suppose now that I would like to perform this 2 consecutive rotations: Rotation of Rotation of The matrices relative to these two rotations are the following: and composing them I obtain the total rotation matrix If I want to recover the euler angles sequence relative to I apply these equations: obtaining . However applying this sequence of rotation I do not recover the initial frame (sequence of rotations 1-2) unless the rotations 1-2 are performed on different axis respect the body ones. Probably I miss something and I'm making confusion with these concepts. Please, could you help me to understand where I miss?","\psi \theta \varphi R_X=\begin{bmatrix}1&0&0\\
0&\cos(\varphi)&-\sin(\varphi)\\
0&\sin(\varphi)&\cos(\varphi)\end{bmatrix} R_Y=\begin{bmatrix}\cos(\theta)&0&\sin(\theta)\\
0&1&0\\
-\sin(\theta)&0&\cos(\theta)\end{bmatrix} R_Z=\begin{bmatrix}\cos(\psi)&-\sin(\psi)&0\\
\sin(\psi)&\cos(\psi)&0\\
0&0&1\end{bmatrix} (yaw,pitch,roll)=(\psi,\theta,\varphi) R=R_X\cdot R_Y\cdot R_Z (yaw,pitch,roll)=(0,\frac{\pi}{4},0) (yaw,pitch,roll)=(\frac{\pi}{2},0,0) R_1=\begin{bmatrix}\frac{\sqrt{2}}{2}&0&\frac{\sqrt{2}}{2}\\
0&1&0\\
-\frac{\sqrt{2}}{2}&0&\frac{\sqrt{2}}{2}\end{bmatrix} R_2=\begin{bmatrix}1&0&0\\
0&0&-1\\
0&1&0\end{bmatrix} R=R_2\cdot R_1 = \begin{bmatrix}0&-1&0\\
\frac{\sqrt{2}}{2}&0&\frac{\sqrt{2}}{2}\\
-\frac{\sqrt{2}}{2}&0&\frac{\sqrt{2}}{2}\end{bmatrix} R \theta=\arcsin(R_{13})\\
\psi=-\arctan2\left(\frac{R_{12}}{\cos(\theta)},\frac{R_{11}}{\cos(\theta)}\right)\\
\varphi=-\arctan2\left(\frac{R_{23}}{\cos(\theta)},\frac{R_{33}}{\cos(\theta)}\right) (yaw,pitch,roll)=(\frac{\pi}{2},0,-\frac{\pi}{4})","['matrices', 'rotations']"
67,Product of skew symmetric matrices,Product of skew symmetric matrices,,"Let $A,B$ be skew symmetric 3-dimensional real non-zero matrices. Because dimension is odd they have non-trivial one-dimensional kernels. Is it true that $AB$ is nilpotent  iff $\text{ker}(A)$ $\perp$ $\text{ker}(B)$ ? How to prove it? The example illustrating one direction of the implication: $\begin{bmatrix} 0 & 1 & 2 \\ -1 & 0 & 4 \\ -2 & -4 & 0 \end{bmatrix}\begin{bmatrix} 0 & -2 & 3 \\ 2 & 0 & -1 \\ -3 & 1 & 0 \end{bmatrix}=\begin{bmatrix} -4 & 2 & -1 \\ -12 & 6 & -3 \\ -8 & 4 & -2 \end{bmatrix}=C $ we have here $C^2=0$ .",Let be skew symmetric 3-dimensional real non-zero matrices. Because dimension is odd they have non-trivial one-dimensional kernels. Is it true that is nilpotent  iff ? How to prove it? The example illustrating one direction of the implication: we have here .,"A,B AB \text{ker}(A) \perp \text{ker}(B) \begin{bmatrix} 0 & 1 & 2 \\ -1 & 0 & 4 \\ -2 & -4 & 0 \end{bmatrix}\begin{bmatrix} 0 & -2 & 3 \\ 2 & 0 & -1 \\ -3 & 1 & 0 \end{bmatrix}=\begin{bmatrix} -4 & 2 & -1 \\ -12 & 6 & -3 \\ -8 & 4 & -2 \end{bmatrix}=C  C^2=0","['linear-algebra', 'matrices']"
68,Dimension of a maximal vector subspace of $M_3(\mathbb{C})$ which is commutative under the multiplication,Dimension of a maximal vector subspace of  which is commutative under the multiplication,M_3(\mathbb{C}),"Let $V$ be a $\mathbb{C}$ -vector subspace of $M_3(\mathbb{C})$ satisfying following properties: (A) for every $A,B \in V$ , $AB = BA$ ; (B) if $W$ is a $\mathbb{C}$ -vector subspace of $M_3(\mathbb{C})$ which contains $V$ as a proper subset, then there exists $A,B \in W$ such that $AB \neq BA$ . Then what is $\dim V$ ? The vector space V generated by $$\begin{bmatrix}1 & 0 & 0\\0 & 0 & 0 \\ 0 & 0& 0\end{bmatrix}, \begin{bmatrix}0 & 0 & 0\\0 & 1 & 0 \\ 0 & 0& 0\end{bmatrix} \text{ and } \begin{bmatrix}0 & 0 & 0\\0 & 0 & 0 \\ 0 & 0& 1\end{bmatrix},$$ satisfies these properties. So I think $\dim V = 3$ in general. I think that,  if $\dim \lt 3$ , then $V$ is too small to be maximal, and if $\dim \gt 3$ , then $V$ is too big to be a commutative algebra. So it seems to be reasonable for me. Thank you very much.","Let be a -vector subspace of satisfying following properties: (A) for every , ; (B) if is a -vector subspace of which contains as a proper subset, then there exists such that . Then what is ? The vector space V generated by satisfies these properties. So I think in general. I think that,  if , then is too small to be maximal, and if , then is too big to be a commutative algebra. So it seems to be reasonable for me. Thank you very much.","V \mathbb{C} M_3(\mathbb{C}) A,B \in V AB = BA W \mathbb{C} M_3(\mathbb{C}) V A,B \in W AB \neq BA \dim V \begin{bmatrix}1 & 0 & 0\\0 & 0 & 0 \\ 0 & 0& 0\end{bmatrix},
\begin{bmatrix}0 & 0 & 0\\0 & 1 & 0 \\ 0 & 0& 0\end{bmatrix} \text{ and }
\begin{bmatrix}0 & 0 & 0\\0 & 0 & 0 \\ 0 & 0& 1\end{bmatrix}, \dim V = 3 \dim \lt 3 V \dim \gt 3 V","['linear-algebra', 'matrices', 'vector-spaces']"
69,Let $A$ be an $n*n$ matrix such that $A^3=A^2+A-I$. If $A$ Is diagonalizable Show that $A=A^{-1}$,Let  be an  matrix such that . If  Is diagonalizable Show that,A n*n A^3=A^2+A-I A A=A^{-1},"Let $A$ be an $n*n$ matrix such that $A^3=A^2+A-I$ . Show that $A$ is invertible Suppose in $A$ is diagonalizable. Show that $A=A^{-1}$ For the first part I managed to do it by a rearrangement, $$I=A^{2}+A-A^{3}=A(A+I-A^2)$$ Thus $A+I-A^2$ is the right inverse and simillarly we have $A+I-A^2$ as the left inverse. Hence $A$ is invertible. But for part 2. What I was able to do is: $A^{-1}=A+I-A^2=A+(PDP^{-1})(PDP^{-1})^{-1}-PD^{2}P^{-1}$ . But here after I don't see how should I proceed.","Let be an matrix such that . Show that is invertible Suppose in is diagonalizable. Show that For the first part I managed to do it by a rearrangement, Thus is the right inverse and simillarly we have as the left inverse. Hence is invertible. But for part 2. What I was able to do is: . But here after I don't see how should I proceed.",A n*n A^3=A^2+A-I A A A=A^{-1} I=A^{2}+A-A^{3}=A(A+I-A^2) A+I-A^2 A+I-A^2 A A^{-1}=A+I-A^2=A+(PDP^{-1})(PDP^{-1})^{-1}-PD^{2}P^{-1},"['matrices', 'inverse', 'diagonalization']"
70,Find the symmetrical matrix $A$ so that $Q(\vec x) = \vec x^TA \vec x$,Find the symmetrical matrix  so that,A Q(\vec x) = \vec x^TA \vec x,"$Q(\vec x) = x_1^2+x_1x_2+x_2^2$ The matrix $A=\begin{bmatrix}1 & 0.5 \\ 0.5 & 1\end{bmatrix}$ seems to do the job. But what's the general procedure for finding a solution? I can just think of setting it up like this for more clarity: $\begin{bmatrix}x_1 & x_2\end{bmatrix}\begin{bmatrix}? & ?\\ ? & ?\end{bmatrix}\begin{bmatrix}x_1\\ x_2\end{bmatrix} =\begin{bmatrix}x_1^2+x_1x_2+x_2^2\end{bmatrix}$ But after that I'm lost, there surely must be some concepts I can apply.","The matrix seems to do the job. But what's the general procedure for finding a solution? I can just think of setting it up like this for more clarity: But after that I'm lost, there surely must be some concepts I can apply.",Q(\vec x) = x_1^2+x_1x_2+x_2^2 A=\begin{bmatrix}1 & 0.5 \\ 0.5 & 1\end{bmatrix} \begin{bmatrix}x_1 & x_2\end{bmatrix}\begin{bmatrix}? & ?\\ ? & ?\end{bmatrix}\begin{bmatrix}x_1\\ x_2\end{bmatrix} =\begin{bmatrix}x_1^2+x_1x_2+x_2^2\end{bmatrix},"['linear-algebra', 'matrices', 'linear-transformations', 'matrix-equations']"
71,Time complexity for finding the nth Fibonacci number using matrices,Time complexity for finding the nth Fibonacci number using matrices,,"I have a question about the time complexity of finding the nth Fibonacci number using matrices. I know that you can find $F_n$ from: $ \begin{pmatrix} 1 & 1\\ 1 & 0\\ \end{pmatrix}^n = \begin{pmatrix} F_{n+1} & F_n\\ F_n & F_{n-1}\\ \end{pmatrix} $ From above, $F_n$ is the entry in row 1, column 2 of the matrix. I have read in an online source that raising the Fibonacci Q-matrix to the power of n takes O(n) time. I am not sure why this is the case. Is matrix multiplication an operation that can be done in constant time on computers? That is the only reason I can see the above operation taking O(n) time (since you would have to do n-1 multiplications to raise the Q-matrix to the power of n). Any insights are appreciated.","I have a question about the time complexity of finding the nth Fibonacci number using matrices. I know that you can find from: From above, is the entry in row 1, column 2 of the matrix. I have read in an online source that raising the Fibonacci Q-matrix to the power of n takes O(n) time. I am not sure why this is the case. Is matrix multiplication an operation that can be done in constant time on computers? That is the only reason I can see the above operation taking O(n) time (since you would have to do n-1 multiplications to raise the Q-matrix to the power of n). Any insights are appreciated.","F_n 
\begin{pmatrix}
1 & 1\\
1 & 0\\
\end{pmatrix}^n = \begin{pmatrix}
F_{n+1} & F_n\\
F_n & F_{n-1}\\
\end{pmatrix}
 F_n","['linear-algebra', 'matrices', 'asymptotics', 'computer-science', 'fibonacci-numbers']"
72,Derivative of a function of matrix,Derivative of a function of matrix,,"I am trying to derive the gradient of the function $f(X) = AXZ + XZX^TXZ$ where $A,X,Z \in R^{n \times n}$ with respect to $X$ matrix. I read a post Matrix-by-matrix derivative formula about matrix derivate, but I am not able to follow it. In my case $\frac{\partial f(X)}{\partial X)}$ would be a tensor, but If I try to use the formula given in the post I would get a matrix. How should I process to get the partial derivate?","I am trying to derive the gradient of the function where with respect to matrix. I read a post Matrix-by-matrix derivative formula about matrix derivate, but I am not able to follow it. In my case would be a tensor, but If I try to use the formula given in the post I would get a matrix. How should I process to get the partial derivate?","f(X) = AXZ + XZX^TXZ A,X,Z \in R^{n \times n} X \frac{\partial f(X)}{\partial X)}","['matrices', 'partial-derivative', 'matrix-calculus']"
73,Solve matrix equation $XAX^*=B$ for $X$ in least squares sense,Solve matrix equation  for  in least squares sense,XAX^*=B X,"How can the following optimization problem be solved? $$\arg\min_{\mathbf{X} \in \mathcal{C}^{n \times m}} \left\Vert \mathbf{X}\mathbf{A}\mathbf{X}^* - \mathbf{B} \right\Vert_F$$ where $\mathbf{A} \in \mathcal{C}^{m \times m}$ and $\mathbf{B} \in \mathcal{C}^{n \times n}$ are Hermitian, with $m > n$ , generally. Matrix $\mathbf{A}$ is diagonal and invertible. Let $^*$ denote the conjugate transpose. Solution attempt Solution attempt was wrong, see comments. See the answers for a better attempt. I realize it is possible to take the derivative of the expression and equating it to zero, but I end up with the following expression I have trouble with $\require{enclose}      \enclose{horizontalstrike}{\frac{d}{d \mathbf{X}}\mathrm{Tr}\left( \mathbf{X}\mathbf{A}\mathbf{X}^* \mathbf{X}\mathbf{A}\mathbf{X}^* - 2\mathbf{X}\mathbf{A}\mathbf{X}^*\mathbf{B} + \mathbf{B}^*\mathbf{B} \right) = 0}$ $\require{enclose}      \enclose{horizontalstrike}{\mathbf{X}\mathbf{A}\mathbf{X}^* \mathbf{X}\mathbf{A} - \mathbf{B}\mathbf{X}\mathbf{A} = 0}$ Because $\require{enclose}      \enclose{horizontalstrike}{\mathbf{A}}$ is invertible $\require{enclose}      \enclose{horizontalstrike}{\mathbf{X}\mathbf{A}\mathbf{X}^*\mathbf{X} - \mathbf{B}\mathbf{X} = 0}$ Which does not seem any simpler. Variant Would the following variant be harder to solve? I don't see it working with Måren W's answer below $\underset{\mathbf{X}}{\mathrm{argmin}} \left\Vert \mathbf{X}\mathbf{A}\mathbf{X}^T - \mathbf{B} \right\Vert_F$ with $m > n$ . Again the matrices are complex, but now $\mathbf{A}$ is a symmetric (not Hermitian), diagonal and invertible matrix and $\mathbf{B}$ is also symmetric.","How can the following optimization problem be solved? where and are Hermitian, with , generally. Matrix is diagonal and invertible. Let denote the conjugate transpose. Solution attempt Solution attempt was wrong, see comments. See the answers for a better attempt. I realize it is possible to take the derivative of the expression and equating it to zero, but I end up with the following expression I have trouble with Because is invertible Which does not seem any simpler. Variant Would the following variant be harder to solve? I don't see it working with Måren W's answer below with . Again the matrices are complex, but now is a symmetric (not Hermitian), diagonal and invertible matrix and is also symmetric.","\arg\min_{\mathbf{X} \in \mathcal{C}^{n \times m}} \left\Vert \mathbf{X}\mathbf{A}\mathbf{X}^* - \mathbf{B} \right\Vert_F \mathbf{A} \in \mathcal{C}^{m \times m} \mathbf{B} \in \mathcal{C}^{n \times n} m > n \mathbf{A} ^* \require{enclose}
     \enclose{horizontalstrike}{\frac{d}{d \mathbf{X}}\mathrm{Tr}\left( \mathbf{X}\mathbf{A}\mathbf{X}^* \mathbf{X}\mathbf{A}\mathbf{X}^* - 2\mathbf{X}\mathbf{A}\mathbf{X}^*\mathbf{B} + \mathbf{B}^*\mathbf{B} \right) = 0} \require{enclose}
     \enclose{horizontalstrike}{\mathbf{X}\mathbf{A}\mathbf{X}^* \mathbf{X}\mathbf{A} - \mathbf{B}\mathbf{X}\mathbf{A} = 0} \require{enclose}
     \enclose{horizontalstrike}{\mathbf{A}} \require{enclose}
     \enclose{horizontalstrike}{\mathbf{X}\mathbf{A}\mathbf{X}^*\mathbf{X} - \mathbf{B}\mathbf{X} = 0} \underset{\mathbf{X}}{\mathrm{argmin}} \left\Vert \mathbf{X}\mathbf{A}\mathbf{X}^T - \mathbf{B} \right\Vert_F m > n \mathbf{A} \mathbf{B}","['matrices', 'optimization', 'matrix-equations', 'least-squares', 'symmetry']"
74,"Showing there is a unique group table for $\{1, a,b,c\}$ such that there is no element of order $4$. [duplicate]",Showing there is a unique group table for  such that there is no element of order . [duplicate],"\{1, a,b,c\} 4","This question already has answers here : Cayley tables for two non-isomorphic groups of order 4. (2 answers) Closed 5 years ago . Assume $G = \{1, a,b,c\}$ is a group of order $4$ with identity $1.$ Assume also that $G$ has no elements of order $4$ . Show that there is a unique group table for $G$ . Also show that $G$ is abelian. If $G$ is abelian, then the group table matrix must be symmetric. How can I introduce a binary function and show it? I am new in this field, so I am not so familiar. I have proved many other exercises, but it is a little tough (for me). Can you please help? Edit: I know every element has order $\leq 3$ , but I do not understand how I will proceed.","This question already has answers here : Cayley tables for two non-isomorphic groups of order 4. (2 answers) Closed 5 years ago . Assume is a group of order with identity Assume also that has no elements of order . Show that there is a unique group table for . Also show that is abelian. If is abelian, then the group table matrix must be symmetric. How can I introduce a binary function and show it? I am new in this field, so I am not so familiar. I have proved many other exercises, but it is a little tough (for me). Can you please help? Edit: I know every element has order , but I do not understand how I will proceed.","G = \{1, a,b,c\} 4 1. G 4 G G G \leq 3","['abstract-algebra', 'matrices']"
75,Matrices commuting with the matrix of ones,Matrices commuting with the matrix of ones,,"Let $J_n$ be $n \times n$ matrix of ones , so that $J_{ij} = 1$ for all $i,j \in\{1,\ldots,n\}$ . I am interested to find the class of matrices which commute with $J_n$ , i.e. $$M J_n = J_n M.$$ It is not difficult to see that a permutation matrix satisfies this property. By linear superposition, the doubly stochastic matrix also commutes with $J_n$ . Are there any other matrices, apart from doubly stochastic ones, with this property?","Let be matrix of ones , so that for all . I am interested to find the class of matrices which commute with , i.e. It is not difficult to see that a permutation matrix satisfies this property. By linear superposition, the doubly stochastic matrix also commutes with . Are there any other matrices, apart from doubly stochastic ones, with this property?","J_n n \times n J_{ij} = 1 i,j \in\{1,\ldots,n\} J_n M J_n = J_n M. J_n","['matrices', 'matrix-equations']"
76,Orthogonal matrix with single $0$ entry,Orthogonal matrix with single  entry,0,"To my surprise there is an orthogonal matrix dimension $3 \times 3$ with a single $0$ entry as  it was shown in this   answer . Moreover it was possible to identify the pattern for matrix entries to satisfy this condition i.e. $$Q=\begin{bmatrix} 0 & -a & -b \\  a   &  -b^2 &  ab \\ b & ab & -a^2\end{bmatrix}$$ when $a,b$ are constrained by $a^2+b^2=1$ . For orthogonal matrix dimension $2 \times 2$ single $0$ entry is not possible. Here my new questions: Is it possible a single $0$ entry for orthogonal matrix dimension $4     \times 4$ ? Can we  also provide for this case some more general formula? (to be more specific in a similar form as it was shown above for 3d) Is the method listed above the only one for orthogonal matrices dimension $3     \times 3$ or can we generate such matrix also with some other substantially different algorithm?",To my surprise there is an orthogonal matrix dimension with a single entry as  it was shown in this   answer . Moreover it was possible to identify the pattern for matrix entries to satisfy this condition i.e. when are constrained by . For orthogonal matrix dimension single entry is not possible. Here my new questions: Is it possible a single entry for orthogonal matrix dimension ? Can we  also provide for this case some more general formula? (to be more specific in a similar form as it was shown above for 3d) Is the method listed above the only one for orthogonal matrices dimension or can we generate such matrix also with some other substantially different algorithm?,"3 \times 3 0 Q=\begin{bmatrix} 0 & -a & -b \\  a   &  -b^2 &  ab \\ b & ab & -a^2\end{bmatrix} a,b a^2+b^2=1 2 \times 2 0 0 4 
   \times 4 3 
   \times 3","['linear-algebra', 'matrices', 'orthogonal-matrices']"
77,How to show the derivative of $f(M)=\text{Tr}(M\log (M) -M)$ is $\log (M)$?,How to show the derivative of  is ?,f(M)=\text{Tr}(M\log (M) -M) \log (M),"Let $M$ be a positive definite matrix in $\mathbb{S}_+^n$ . Let $\log$ be natural matrix logarithm which $\log(M)$ is defined as $\log(M)=\sum_{i=1}^{n}\log(\lambda_i)v_iv_i^T$ where $(\lambda_i,v_i)$ are eigenpairs of $M$ . This function is called negative Von Neumann entropy or negative Quantum entropy. How can we show the derivative of $f(M)=\text{Tr}(M\log M -M)$ is $\log M?$",Let be a positive definite matrix in . Let be natural matrix logarithm which is defined as where are eigenpairs of . This function is called negative Von Neumann entropy or negative Quantum entropy. How can we show the derivative of is,"M \mathbb{S}_+^n \log \log(M) \log(M)=\sum_{i=1}^{n}\log(\lambda_i)v_iv_i^T (\lambda_i,v_i) M f(M)=\text{Tr}(M\log M -M) \log M?",['matrices']
78,"Show that if $H$ is an Hermitian matrix, then $U=(iI-H)(iI+H)^{-1}$ is unitary","Show that if  is an Hermitian matrix, then  is unitary",H U=(iI-H)(iI+H)^{-1},"How would you prove, that when $H$ is a hermitian matrix, then: $$U=(iI-H)(iI+H)^{-1},$$ where $U$ is unitary, assuming that $(iI+H)$ is invertible. I thought that because $U$ is unitary, $UU^*=U^*U=I$ . So I tried to take the conjugate transpose of $U$ where: $$U^*=(-iI-H)(H-iI)^{-1}.$$ Essentially this is where I am stuck: 1) How do I take the transpose of $U$ , so I don't just have the conjugate -- or does it not matter? 2) How do I get rid of the inverse because I don't know what to do with it and how to eventually prove $UU^*=I$ ? Thank you!","How would you prove, that when is a hermitian matrix, then: where is unitary, assuming that is invertible. I thought that because is unitary, . So I tried to take the conjugate transpose of where: Essentially this is where I am stuck: 1) How do I take the transpose of , so I don't just have the conjugate -- or does it not matter? 2) How do I get rid of the inverse because I don't know what to do with it and how to eventually prove ? Thank you!","H U=(iI-H)(iI+H)^{-1}, U (iI+H) U UU^*=U^*U=I U U^*=(-iI-H)(H-iI)^{-1}. U UU^*=I","['linear-algebra', 'matrices']"
79,The nonsingular matrix closest to singular one,The nonsingular matrix closest to singular one,,"It is well known that given nonsingular matrix $A$ , the distance to closest singular is given by $\|A^{-1}\|^{-1}$ . My question is given singular matrix $B$ what is the distance to closest nonsingular? Is it the same as first problem?","It is well known that given nonsingular matrix , the distance to closest singular is given by . My question is given singular matrix what is the distance to closest nonsingular? Is it the same as first problem?",A \|A^{-1}\|^{-1} B,"['linear-algebra', 'matrices', 'matrix-calculus']"
80,Characteristic polynomial modulo 12,Characteristic polynomial modulo 12,,"Consider the vector space $V =\left\{a_0+a_1x+\cdots+a_{11}x^{11},\;a_i\in\mathbb{R}\right\}$ . Define a linear operator $A$ on $V$ by $A(x^i) = x^{i+4}$ where $i + 4$ is taken modulo $12$ . Find $(a)$ the minimal polynomial of $A$ and $(b)$ the characteristic polynomial of $A$ . My try: I coulnot find another way so I tried the brute force method. $\:$ I found the matrix representation of the operator is $$A=\begin{bmatrix} 0&0&0&0&0&0&0&0&1&0&0&0\\0&0&0&0&0&0&0&0&0&1&0&0\\0&0&0&0&0&0&0&0&0&0&1&0\\0&0&0&0&0&0&0&0&0&0&0&1\\1&0&0&0&0&0&0&0&0&0&0&0\\0&1&0&0&0&0&0&0&0&0&0&0\\0&0&1&0&0&0&0&0&0&0&0&0\\0&0&0&1&0&0&0&0&0&0&0&0\\0&0&0&0&1&0&0&0&0&0&0&0\\0&0&0&0&0&1&0&0&0&0&0&0\\0&0&0&0&0&0&1&0&0&0&0&0\\0&0&0&0&0&0&0&1&0&0&0&0\\\end{bmatrix}.$$ The characteristic polynomial I found to be $\lambda^{12}-4\lambda^9+6\lambda^6-4\lambda^3+1$ . $\:$ (It took me almost 40 minutes. $\:$ Is there another way to do this problem? Provide hints or suggestions please. $\rule{17cm}{1pt}$ Taking forward the answer provided by $\textbf{Servaes}$ ""The minimal polynomial of $A|_{U_i}$ is still $X^3−1$ ."" Taking $U_1=span\{x_1,x_5,x_9\}$ we see $A(x)=x^5,\:A^2(x)=x^9,\:A^3(x)=x\implies (A^3-I)=0$ and since it factors into linear irredeucible factors, we have the minimal polynomial of $A|_{U_i}$ is $X^3−1$ . Completing the proof: We show that characteristic polynomial of $A$ is the product of characteristic polynomials of $A|_{U_i}$ where $V=\oplus U_i$ . We have seen that minimal polynomial of $A|_{U_i}$ is $X^3−1$ which is precisely the characteristic polynomial. So let $p_i(\lambda)$ is characteristic polynomial corresponding to eigenvalue $\lambda_i$ and invariant subspace $U_i$ and $p(\lambda)$ is the characteristic polynomial of A. Then $\displaystyle p(\lambda_i)=0 \:\forall\;i  \implies p_i(\lambda)|p(\lambda) \:\forall\;i \implies p(\lambda)=\prod_ip_i(\lambda)$ . $\Big($$p(\lambda)$ is atleast $\displaystyle\prod_ip_i(\lambda)$ . If $\exists\lambda\neq\lambda_i \forall\: i$ such that $p(\lambda)=0$ then $V$ is not $\oplus U_i$ $\Big)$ $\rule{17cm}{1pt}$ Minimal polynomial : $X^3-1\qquad$ Characteristic polynomial : $(X^3-1)^4$ .","Consider the vector space . Define a linear operator on by where is taken modulo . Find the minimal polynomial of and the characteristic polynomial of . My try: I coulnot find another way so I tried the brute force method. I found the matrix representation of the operator is The characteristic polynomial I found to be . (It took me almost 40 minutes. Is there another way to do this problem? Provide hints or suggestions please. Taking forward the answer provided by ""The minimal polynomial of is still ."" Taking we see and since it factors into linear irredeucible factors, we have the minimal polynomial of is . Completing the proof: We show that characteristic polynomial of is the product of characteristic polynomials of where . We have seen that minimal polynomial of is which is precisely the characteristic polynomial. So let is characteristic polynomial corresponding to eigenvalue and invariant subspace and is the characteristic polynomial of A. Then . is atleast . If such that then is not Minimal polynomial : Characteristic polynomial : .","V =\left\{a_0+a_1x+\cdots+a_{11}x^{11},\;a_i\in\mathbb{R}\right\} A V A(x^i) = x^{i+4} i + 4 12 (a) A (b) A \: A=\begin{bmatrix} 0&0&0&0&0&0&0&0&1&0&0&0\\0&0&0&0&0&0&0&0&0&1&0&0\\0&0&0&0&0&0&0&0&0&0&1&0\\0&0&0&0&0&0&0&0&0&0&0&1\\1&0&0&0&0&0&0&0&0&0&0&0\\0&1&0&0&0&0&0&0&0&0&0&0\\0&0&1&0&0&0&0&0&0&0&0&0\\0&0&0&1&0&0&0&0&0&0&0&0\\0&0&0&0&1&0&0&0&0&0&0&0\\0&0&0&0&0&1&0&0&0&0&0&0\\0&0&0&0&0&0&1&0&0&0&0&0\\0&0&0&0&0&0&0&1&0&0&0&0\\\end{bmatrix}. \lambda^{12}-4\lambda^9+6\lambda^6-4\lambda^3+1 \: \: \rule{17cm}{1pt} \textbf{Servaes} A|_{U_i} X^3−1 U_1=span\{x_1,x_5,x_9\} A(x)=x^5,\:A^2(x)=x^9,\:A^3(x)=x\implies (A^3-I)=0 A|_{U_i} X^3−1 A A|_{U_i} V=\oplus U_i A|_{U_i} X^3−1 p_i(\lambda) \lambda_i U_i p(\lambda) \displaystyle p(\lambda_i)=0 \:\forall\;i  \implies p_i(\lambda)|p(\lambda) \:\forall\;i \implies p(\lambda)=\prod_ip_i(\lambda) \Big(p(\lambda) \displaystyle\prod_ip_i(\lambda) \exists\lambda\neq\lambda_i \forall\: i p(\lambda)=0 V \oplus U_i \Big) \rule{17cm}{1pt} X^3-1\qquad (X^3-1)^4","['linear-algebra', 'matrices', 'minimal-polynomials']"
81,"The matrix $I_n - v^t x$ is invertible when $\langle v, x \rangle \neq 1$",The matrix  is invertible when,"I_n - v^t x \langle v, x \rangle \neq 1","Let $v \neq 0$ be any vector in $\Bbb R^n$ and let, $U_v = \{x \in \Bbb R^n : \langle v,x \rangle \neq 1\}$ . Then: (i) Show that the matrix $I_n - v^t x$ is invertible $\forall x \in U_v$ . (ii) Compute the derivative of the map $f:U_v \to GL_n(\Bbb k)$ ( $\Bbb k = \Bbb R \text{ or } \Bbb C$ ) defined by $$f(x)={(I-v^tx)}^{-1} .$$ My attempt: (i) I have tried to do it using determinant but couldn't do it. (ii) Observed $f=f_2 \circ f_1$ , where $$f_1 : U_v \to GL_n(\Bbb R)$$ $$x \mapsto (I_n - v^t x)$$ and $$ f_2 : GL_n(\Bbb R) \to GL_n(\Bbb R)$$ $$ A \mapsto A^{-1}$$ Then derived, ${Df_1}_{(x)} (h)= -v^t h$ and ${Df_2}_{(A)}(H)= -A^{-1}HA^{-1} $ . Applied Chain Rule to obtain, $$Df_x (h)= {(I_n - v^tx)}^{-1} (v^t h){(I_n - v^tx)}^{-1}$$ I think I have done part (ii) only for $\Bbb k = \Bbb R$ . How to prove part (i) and if there is any mistake in my attempt in part $(ii)$ , then please point it out. Thanks in advance for help!","Let be any vector in and let, . Then: (i) Show that the matrix is invertible . (ii) Compute the derivative of the map ( ) defined by My attempt: (i) I have tried to do it using determinant but couldn't do it. (ii) Observed , where and Then derived, and . Applied Chain Rule to obtain, I think I have done part (ii) only for . How to prove part (i) and if there is any mistake in my attempt in part , then please point it out. Thanks in advance for help!","v \neq 0 \Bbb R^n U_v = \{x \in \Bbb R^n : \langle v,x \rangle \neq 1\} I_n - v^t x \forall x \in U_v f:U_v \to GL_n(\Bbb k) \Bbb k = \Bbb R \text{ or } \Bbb C f(x)={(I-v^tx)}^{-1} . f=f_2 \circ f_1 f_1 : U_v \to GL_n(\Bbb R) x \mapsto (I_n - v^t x)  f_2 : GL_n(\Bbb R) \to GL_n(\Bbb R)  A \mapsto A^{-1} {Df_1}_{(x)} (h)= -v^t h {Df_2}_{(A)}(H)= -A^{-1}HA^{-1}  Df_x (h)= {(I_n - v^tx)}^{-1} (v^t h){(I_n - v^tx)}^{-1} \Bbb k = \Bbb R (ii)",['linear-algebra']
82,Nearest commuting matrix,Nearest commuting matrix,,"The space of matrices that commute with a given matrix $A\in \mathbb{C}^{n\times n}$ is a subspace of the vector space of all matrices $\mathbb{C}^{n\times n}$. There must exist a projection operator upon this subspace, some $P_A$ such that $$\forall M \in \mathbb{C}^{n\times n}: [P_A M,A]=0$$ Question: Is there some useful expression for $P_A$ in terms of $A$? Context I'm looking for a way to interpolate matrices without breaking commutation. For example, I may want to construct $$f:[0,1]\rightarrow \mathbb{C}^{n\times n}$$ $$g:[0,1]\rightarrow \mathbb{C}^{n\times n}$$ knowing $f(0),f(1),g(0),g(1)$ and $[f(0),g(0)]=[f(1),g(1)]=0$, in such at way that $[f(x),g(x)]=0$ remains true for all $x$.","The space of matrices that commute with a given matrix $A\in \mathbb{C}^{n\times n}$ is a subspace of the vector space of all matrices $\mathbb{C}^{n\times n}$. There must exist a projection operator upon this subspace, some $P_A$ such that $$\forall M \in \mathbb{C}^{n\times n}: [P_A M,A]=0$$ Question: Is there some useful expression for $P_A$ in terms of $A$? Context I'm looking for a way to interpolate matrices without breaking commutation. For example, I may want to construct $$f:[0,1]\rightarrow \mathbb{C}^{n\times n}$$ $$g:[0,1]\rightarrow \mathbb{C}^{n\times n}$$ knowing $f(0),f(1),g(0),g(1)$ and $[f(0),g(0)]=[f(1),g(1)]=0$, in such at way that $[f(x),g(x)]=0$ remains true for all $x$.",,['matrices']
83,Show that $8$ is an eigen value of $A$,Show that  is an eigen value of,8 A,Consider the following matrix: $$A=\begin{bmatrix}B&&&C\\D&&&F\end{bmatrix}$$ where $$B= \begin{bmatrix} 9 &1&1&1&1\\1&9&1&1&1\\1&1&9&1&1\\1&1&1&9&1\\1&1&1&1&9\end{bmatrix}$$ and $$C=\begin{bmatrix}1&1&1&1&1\\1&1&1&1&1\\1&1&1&1&1\\1&1&1&1&1\\1&1&1&1&1\end{bmatrix}$$ and $$D=C^T$$ and $$F=\begin{bmatrix} G&H \\I &J\end{bmatrix}$$ where $$G=\begin{bmatrix} 8 &1&1&1\\1&8&1&1\\1&1&8&1\\1&1&1&8\end{bmatrix}$$ and $$H=\begin{bmatrix} 0\\0\\0\\0\end{bmatrix}$$ and $$I=H^T$$ and $$J=\begin{bmatrix} 5\end{bmatrix}$$ Show that $8$ is an eigen value of the matrix $A$. How should I try to prove it?Please give some hints .I dont want a complete solution,Consider the following matrix: $$A=\begin{bmatrix}B&&&C\\D&&&F\end{bmatrix}$$ where $$B= \begin{bmatrix} 9 &1&1&1&1\\1&9&1&1&1\\1&1&9&1&1\\1&1&1&9&1\\1&1&1&1&9\end{bmatrix}$$ and $$C=\begin{bmatrix}1&1&1&1&1\\1&1&1&1&1\\1&1&1&1&1\\1&1&1&1&1\\1&1&1&1&1\end{bmatrix}$$ and $$D=C^T$$ and $$F=\begin{bmatrix} G&H \\I &J\end{bmatrix}$$ where $$G=\begin{bmatrix} 8 &1&1&1\\1&8&1&1\\1&1&8&1\\1&1&1&8\end{bmatrix}$$ and $$H=\begin{bmatrix} 0\\0\\0\\0\end{bmatrix}$$ and $$I=H^T$$ and $$J=\begin{bmatrix} 5\end{bmatrix}$$ Show that $8$ is an eigen value of the matrix $A$. How should I try to prove it?Please give some hints .I dont want a complete solution,,['linear-algebra']
84,Continuous analogy of the matrix inversion,Continuous analogy of the matrix inversion,,"I am thinking on matrix-like entities with continuous indexes. I think, maybe such ""continuous matrices"" could be defined as complex-valued functions on $\mathbb{R}^+\times\mathbb{R}^+$. I think it is trivially visible that the continuous analogy of the unit matrix is $\delta_{xy}$. Multiplication could be defined as $$(f \circ g)(x,y)=\int_0^\infty f(x,u)g(u,y)du$$ I am looking for the $f \in \mathbb{R}^+\times\mathbb{R}^+ \rightarrow \mathbb{C}$ with a given $g \in \mathbb{R}^+\times\mathbb{R}^+ \rightarrow \mathbb{C}$, for which $$\int_0^\infty f(x,u)g(u,y) du = \delta_{xy} | \forall \{x,y\} \subset \mathbb{R}^+ $$ Does this algebra has a name? What could be the continuous analogy of the matrix inversion in it?","I am thinking on matrix-like entities with continuous indexes. I think, maybe such ""continuous matrices"" could be defined as complex-valued functions on $\mathbb{R}^+\times\mathbb{R}^+$. I think it is trivially visible that the continuous analogy of the unit matrix is $\delta_{xy}$. Multiplication could be defined as $$(f \circ g)(x,y)=\int_0^\infty f(x,u)g(u,y)du$$ I am looking for the $f \in \mathbb{R}^+\times\mathbb{R}^+ \rightarrow \mathbb{C}$ with a given $g \in \mathbb{R}^+\times\mathbb{R}^+ \rightarrow \mathbb{C}$, for which $$\int_0^\infty f(x,u)g(u,y) du = \delta_{xy} | \forall \{x,y\} \subset \mathbb{R}^+ $$ Does this algebra has a name? What could be the continuous analogy of the matrix inversion in it?",,"['matrices', 'functional-analysis']"
85,(Linear Regression) Proving that Linear Regression is linear invariant,(Linear Regression) Proving that Linear Regression is linear invariant,,"I want to ask how to show that Linear Regression is linear invariant? The problem is specified in the following picture: Here is the ""solution"" for the problem. But I really get confused by its second step: Why $(Z^TZ)^{-1} = Z^{-1}Z^{-T}$?  The matrix Z is not a square matrix. I am wondering how this is legit? I will also greatly appreciate if anyone can give alternative solution to this problem. Thanks!","I want to ask how to show that Linear Regression is linear invariant? The problem is specified in the following picture: Here is the ""solution"" for the problem. But I really get confused by its second step: Why $(Z^TZ)^{-1} = Z^{-1}Z^{-T}$?  The matrix Z is not a square matrix. I am wondering how this is legit? I will also greatly appreciate if anyone can give alternative solution to this problem. Thanks!",,"['linear-algebra', 'matrices', 'linear-regression']"
86,Determinant of a matrix over a field of characteristic 2,Determinant of a matrix over a field of characteristic 2,,"I first saw a proof for the Leibniz formula for computing determinants when I was learning about tensors and the exterior product at college (a ""proof"" considering that the definition of determinant is an alternating multilinear map from the columns or rows of a square matrix to its field such, that the determinant of the identity matrix equals 1). We were told at some point that any alternating multilinear map $m$ satisfies $m(\vec{v}_1,...,\vec{v}_n)=0 \iff$ some $\vec{v}_i$ is a linear combination of all the other $\vec{v}_{j\neq i}$, only when we're working in a field of characteristic different from 2. We actually used this property in our proof of the Leibnitz formula, but maybe there exists another proof of the Leibnitz formula which doesn't need that property to hold. So... is the Leibnitz formula valid for matrices over a field of characteristic 2?, or does characteristc 2 only imply that $\mathrm{rang}(A)<n \iff \mathrm{det}(A)$ won't be true (for a $n \times n$ matrix $A$)?","I first saw a proof for the Leibniz formula for computing determinants when I was learning about tensors and the exterior product at college (a ""proof"" considering that the definition of determinant is an alternating multilinear map from the columns or rows of a square matrix to its field such, that the determinant of the identity matrix equals 1). We were told at some point that any alternating multilinear map $m$ satisfies $m(\vec{v}_1,...,\vec{v}_n)=0 \iff$ some $\vec{v}_i$ is a linear combination of all the other $\vec{v}_{j\neq i}$, only when we're working in a field of characteristic different from 2. We actually used this property in our proof of the Leibnitz formula, but maybe there exists another proof of the Leibnitz formula which doesn't need that property to hold. So... is the Leibnitz formula valid for matrices over a field of characteristic 2?, or does characteristc 2 only imply that $\mathrm{rang}(A)<n \iff \mathrm{det}(A)$ won't be true (for a $n \times n$ matrix $A$)?",,"['matrices', 'multilinear-algebra']"
87,On the anti-commuting matrices,On the anti-commuting matrices,,"I have a Hermitian matrix of size $n$, $H_{n\times n}$ with only off diagonal entries, actually all the entries are real in $H$ (very special case). Now my job is to find the anti-commuting matrix say $O_{n\times n}$ to $H_{n\times n}$, $O H O^{-1} = -H $, and with $O^{2} = 1 $ which is unitary matrix There are two cases, (i) when $n$ is odd, (ii) when $n$ is even. (i) If $H$ has entries where only at the odd sum of indices($i+j$ $\epsilon$ odd) exists then we can find $O$, e.g. $$H_{1} =\begin{bmatrix}0_{1,1} & a_{1,2} & 0_{1,3} \\a_{2,1} & 0_{2,2} & b_{2,3} \\ 0_{3,1} & b_{3,2} & 0_{3,3} \end{bmatrix}$$ $O = \{\{\sigma_{z},0\},\{0,1\}\} $, where $\sigma_{z}$ is Pauli Matrix If we have element also at the even site $(1,3)$ and $(3,1)$, say $c$, then $$H_{2} =\begin{bmatrix}0_{1,1} & a_{1,2} & c_{1,3} \\a_{2,1} & 0_{2,2} & b_{2,3} \\ c_{3,1} & b_{3,2} & 0_{3,3} \end{bmatrix}$$ Then we can't find $O$. Then the story is same for case (ii) even $n$. Is there a way to show this? (My background is in Physics, I'm not very good with maths. I strongly apologize for that). Both a sketch solution or a good starting point would help me.","I have a Hermitian matrix of size $n$, $H_{n\times n}$ with only off diagonal entries, actually all the entries are real in $H$ (very special case). Now my job is to find the anti-commuting matrix say $O_{n\times n}$ to $H_{n\times n}$, $O H O^{-1} = -H $, and with $O^{2} = 1 $ which is unitary matrix There are two cases, (i) when $n$ is odd, (ii) when $n$ is even. (i) If $H$ has entries where only at the odd sum of indices($i+j$ $\epsilon$ odd) exists then we can find $O$, e.g. $$H_{1} =\begin{bmatrix}0_{1,1} & a_{1,2} & 0_{1,3} \\a_{2,1} & 0_{2,2} & b_{2,3} \\ 0_{3,1} & b_{3,2} & 0_{3,3} \end{bmatrix}$$ $O = \{\{\sigma_{z},0\},\{0,1\}\} $, where $\sigma_{z}$ is Pauli Matrix If we have element also at the even site $(1,3)$ and $(3,1)$, say $c$, then $$H_{2} =\begin{bmatrix}0_{1,1} & a_{1,2} & c_{1,3} \\a_{2,1} & 0_{2,2} & b_{2,3} \\ c_{3,1} & b_{3,2} & 0_{3,3} \end{bmatrix}$$ Then we can't find $O$. Then the story is same for case (ii) even $n$. Is there a way to show this? (My background is in Physics, I'm not very good with maths. I strongly apologize for that). Both a sketch solution or a good starting point would help me.",,"['linear-algebra', 'matrices']"
88,Rank of Laplacian matrix,Rank of Laplacian matrix,,Let $ L $ be a Laplacian matrix of a balanced and strongly connected digraph having $n $ nodes. $ L[r]$ is a submatrix of $L$ which is obtained  by deleting $rth$ row and $rth$ column of Laplacian matrix $L$. It is observed for any $r$ the rank of $ L[r]$ is full$(n-1)$. How can I prove this fact?  Thanks in advance for your suggestions.,Let $ L $ be a Laplacian matrix of a balanced and strongly connected digraph having $n $ nodes. $ L[r]$ is a submatrix of $L$ which is obtained  by deleting $rth$ row and $rth$ column of Laplacian matrix $L$. It is observed for any $r$ the rank of $ L[r]$ is full$(n-1)$. How can I prove this fact?  Thanks in advance for your suggestions.,,"['matrices', 'graph-theory', 'matrix-rank', 'algebraic-graph-theory']"
89,Is there such a thing as an Even Matrix?,Is there such a thing as an Even Matrix?,,"An even function is one in which $f(x)=f(-x)$. For two variables I believe this is $f(x,y)=f(-x,-y)$ If I wish to make a 2D even matrix how would I do this? $$ \begin{matrix} (0,0) & (0,1) \\                   (1,0) & (1,1) \end{matrix}$$ Looking at the indices I can't see any pattern that would allow a even matrix.","An even function is one in which $f(x)=f(-x)$. For two variables I believe this is $f(x,y)=f(-x,-y)$ If I wish to make a 2D even matrix how would I do this? $$ \begin{matrix} (0,0) & (0,1) \\                   (1,0) & (1,1) \end{matrix}$$ Looking at the indices I can't see any pattern that would allow a even matrix.",,"['linear-algebra', 'matrices', 'even-and-odd-functions']"
90,Are eigenvectors of a symmetric matrix orthonormal or just orthogonal?,Are eigenvectors of a symmetric matrix orthonormal or just orthogonal?,,"For every matrix $A$ we have $$AV=V\Lambda\rightarrow A=V\Lambda V^{-1}$$ where $V$ is the matrix of eigenvectors and $\Lambda$ is the diagonal matrix including eigenvalues. When we have a symmetric matrix, one can write $$A^T=({V^{-1}})^T\Lambda V^T\rightarrow A=({V^{-1}})^T\Lambda V^T$$ where symmetry of $A$ results in $A^T=A$, and $\Lambda^T=\Lambda$ because it is diagonal. Comparing these two yields $$V^{-1}=V^T\rightarrow V^TV=I$$ which is the definition of the orthogonal matrix. That means not only eigenvectors of a symmetric matrix are orthogonal but also they are orthonormal, i.e., they are unit vector with length 1 and orthogonal. Is this correct for all symmetric matrices?","For every matrix $A$ we have $$AV=V\Lambda\rightarrow A=V\Lambda V^{-1}$$ where $V$ is the matrix of eigenvectors and $\Lambda$ is the diagonal matrix including eigenvalues. When we have a symmetric matrix, one can write $$A^T=({V^{-1}})^T\Lambda V^T\rightarrow A=({V^{-1}})^T\Lambda V^T$$ where symmetry of $A$ results in $A^T=A$, and $\Lambda^T=\Lambda$ because it is diagonal. Comparing these two yields $$V^{-1}=V^T\rightarrow V^TV=I$$ which is the definition of the orthogonal matrix. That means not only eigenvectors of a symmetric matrix are orthogonal but also they are orthonormal, i.e., they are unit vector with length 1 and orthogonal. Is this correct for all symmetric matrices?",,"['linear-algebra', 'matrices']"
91,How to find the trace of exponential of a matrix,How to find the trace of exponential of a matrix,,Given $$A=\begin{bmatrix}-3&2\\-1&0\end{bmatrix}$$ how to find $e^{trA}$ ? $e^A=pe^Dp^{-1}$ where p is invertible matrix and D is diagonal matrix whose diagonal entries are the eigen value of A. Should i proceed with this concept. Please suggest me. Thank you.,Given how to find ? where p is invertible matrix and D is diagonal matrix whose diagonal entries are the eigen value of A. Should i proceed with this concept. Please suggest me. Thank you.,A=\begin{bmatrix}-3&2\\-1&0\end{bmatrix} e^{trA} e^A=pe^Dp^{-1},"['matrices', 'matrix-exponential']"
92,Is there a name for a matrix formed by multiplying a column and a row?,Is there a name for a matrix formed by multiplying a column and a row?,,"I'm in the process of exploring Bra Ket notation.  In it, I often find operators in the form $\lvert a\rangle\langle b\rvert$, which can be thought of as multiplying a row vector $a$ with a column vector $b$. This strikes me as a construction which should probably have a name that I can research to understand the properties of matrices formed this way, but I'm having trouble finding sources that name such matrices. What is it called when a matrix can be decomposed into a row vector and a column vector?  I'd like to look up the properties of such a matrix. $$M=\begin{pmatrix}     a_0 \\     a_1 \\     \vdots \\     a_n \\     \end{pmatrix}     \begin{pmatrix}     b_0 & b_1 & \ldots & b_n \\     \end{pmatrix}$$","I'm in the process of exploring Bra Ket notation.  In it, I often find operators in the form $\lvert a\rangle\langle b\rvert$, which can be thought of as multiplying a row vector $a$ with a column vector $b$. This strikes me as a construction which should probably have a name that I can research to understand the properties of matrices formed this way, but I'm having trouble finding sources that name such matrices. What is it called when a matrix can be decomposed into a row vector and a column vector?  I'd like to look up the properties of such a matrix. $$M=\begin{pmatrix}     a_0 \\     a_1 \\     \vdots \\     a_n \\     \end{pmatrix}     \begin{pmatrix}     b_0 & b_1 & \ldots & b_n \\     \end{pmatrix}$$",,['matrices']
93,Matrix with only real eigenvalues is similar to upper triangular matrix,Matrix with only real eigenvalues is similar to upper triangular matrix,,"I want to show Let $A$ be a $n \times n$ matrix with only real eigenvalues. Then there is a basis of $\Bbb R^n$ with respect to which $A$ becomes upper triangular. There is a hint which says: Construct a basis $B := \{v_1,v_2,\ldots,v_n\}$ of $\Bbb R^n$ where $v_1$ is an eigenvector. I can see why $[T]_B$ would be upper triangular, if all $v_i$ are eigenvectors, because then $[T]_B$ is diagonal. But if not all are eigenvectors, how do I choose the one which are not eigenvectors, such that $[T]_B$ is upper triangular? I've included what I have so far below, but it's basically the above just written down. And I realized to late that those multiple indices are a real pain. I should've have just assumed w.l.o.g. that there is only one eigenvector for each eigenvalue. You don't need to read all of it through, the question is the same as above. This might be related to this question , but I don't understand the answer to it. We also didn't discuss the Jordan canonical form yet, so if there is maybe a simpler way to proof this, I'd appreciate your help! Let $T:\Bbb R^n \to \Bbb R^n$ be the linear transformation associated with $A$ and let $\lambda_1,\ldots,\lambda_k$ be its eigenvalues with algebraic multiplicity $m_1,\ldots,m_k$, which are all real. Let $B_i := \{v^i_1,\ldots,v^i_{d_i}\}$ be a basis of the eigenspace $E_i$ of $\lambda_i$ which has $\text{dim}(E_i) =: d_i$. Now we try to build a basis for $\Bbb R^n$ consisting of eigenvectors. Let $$B := B_1 \cup \ldots \cup B_k = \{v^1_1,\ldots,v^1_{d_1},v^2_1,\ldots,v^2_{d_2},\ldots\ldots,v^k_1,\ldots,v^k_{d_k}\}$$ If $B$ is a basis of $\Bbb R^n$ then we are done, since there exists a basis of $\Bbb R^n$ of eigenvectors of $A$ and hence $A$ is diagonalizable, which means it is diagonal in this basis $B$. And a diagonal matrix is also upper triangular. If $B$ is not a basis of $\Bbb R^n$, then we need to add additional vectors $\{w_1,\ldots,w_l\}$, so that $B \cup \{w_1,\ldots,w_l\}$ is a basis of $\Bbb R^n$. We have $$[T]_B = \begin{bmatrix} \vert & & & \vert & \vert & & \vert \\ [T(v^1_1)]_B & \ldots & \ldots & [T(v^k_{d_k})]_B & [T(w_1)]_B & \ldots & [T(w_l)]_B \\ \vert & & & \vert & \vert & & \vert \end{bmatrix}$$ Now since $T(v^i_j)=\lambda_i v_j$ we have $$[T(v^i_j)]_B=\begin{bmatrix} 0 \\ \vdots \\ \lambda_i \\ \vdots \\ 0 \end{bmatrix}$$ where only the one entry corresponding to $v^i_j$ is a $\lambda_i$ and the rest zero. Leaving the $T(w_j)$ random first, we get almost a diagonal matrix with the eigenvalues on the diagonal except for the last $l$ columns. $$[T]_B = \begin{bmatrix} \lambda_1 & & & & & * & \ldots & * \\ & \ddots & & & & & & \\ & & \lambda_1 & & & \vdots & & \vdots \\ & & & \ddots & & & & \\ & & & & \lambda_k & * & \ldots & * \\ & & & & & & & & \\ & & & & & \vdots & & \vdots \\ & & & & & & & \\ & & & & & * & \ldots & * \end{bmatrix}$$ For $[T]_B$ to be diagonal, the $w_j$ needs to be chosen, so that there are only stars above the diagonal. This means that $T(w_j)$ needs to be a linear combination of only the basis vectors before this specific $w_j$, which are all the $v^i_j$ and some of the $w_j$, i.e. $T(w_j) = \sum_{i,j} c^i_j v^i_j + \sum_{k}^j c_k w_k$ for some constants $c^i_j,c_k$, because then $$[T(w_j)]_B=\begin{bmatrix} c^1_1 \\ \vdots \\ c^k_{d_k} \\ c_1 \\ \vdots \\ c_j \\ 0 \\ \vdots \\ 0 \end{bmatrix}$$ where the last possible non-zero entry $c_j$ corresponds to $w_j$. Therefore $[T]_B$ would be upper triangular. How do I know that such a linear combination for $w_j$ exists and how could I choose it?","I want to show Let $A$ be a $n \times n$ matrix with only real eigenvalues. Then there is a basis of $\Bbb R^n$ with respect to which $A$ becomes upper triangular. There is a hint which says: Construct a basis $B := \{v_1,v_2,\ldots,v_n\}$ of $\Bbb R^n$ where $v_1$ is an eigenvector. I can see why $[T]_B$ would be upper triangular, if all $v_i$ are eigenvectors, because then $[T]_B$ is diagonal. But if not all are eigenvectors, how do I choose the one which are not eigenvectors, such that $[T]_B$ is upper triangular? I've included what I have so far below, but it's basically the above just written down. And I realized to late that those multiple indices are a real pain. I should've have just assumed w.l.o.g. that there is only one eigenvector for each eigenvalue. You don't need to read all of it through, the question is the same as above. This might be related to this question , but I don't understand the answer to it. We also didn't discuss the Jordan canonical form yet, so if there is maybe a simpler way to proof this, I'd appreciate your help! Let $T:\Bbb R^n \to \Bbb R^n$ be the linear transformation associated with $A$ and let $\lambda_1,\ldots,\lambda_k$ be its eigenvalues with algebraic multiplicity $m_1,\ldots,m_k$, which are all real. Let $B_i := \{v^i_1,\ldots,v^i_{d_i}\}$ be a basis of the eigenspace $E_i$ of $\lambda_i$ which has $\text{dim}(E_i) =: d_i$. Now we try to build a basis for $\Bbb R^n$ consisting of eigenvectors. Let $$B := B_1 \cup \ldots \cup B_k = \{v^1_1,\ldots,v^1_{d_1},v^2_1,\ldots,v^2_{d_2},\ldots\ldots,v^k_1,\ldots,v^k_{d_k}\}$$ If $B$ is a basis of $\Bbb R^n$ then we are done, since there exists a basis of $\Bbb R^n$ of eigenvectors of $A$ and hence $A$ is diagonalizable, which means it is diagonal in this basis $B$. And a diagonal matrix is also upper triangular. If $B$ is not a basis of $\Bbb R^n$, then we need to add additional vectors $\{w_1,\ldots,w_l\}$, so that $B \cup \{w_1,\ldots,w_l\}$ is a basis of $\Bbb R^n$. We have $$[T]_B = \begin{bmatrix} \vert & & & \vert & \vert & & \vert \\ [T(v^1_1)]_B & \ldots & \ldots & [T(v^k_{d_k})]_B & [T(w_1)]_B & \ldots & [T(w_l)]_B \\ \vert & & & \vert & \vert & & \vert \end{bmatrix}$$ Now since $T(v^i_j)=\lambda_i v_j$ we have $$[T(v^i_j)]_B=\begin{bmatrix} 0 \\ \vdots \\ \lambda_i \\ \vdots \\ 0 \end{bmatrix}$$ where only the one entry corresponding to $v^i_j$ is a $\lambda_i$ and the rest zero. Leaving the $T(w_j)$ random first, we get almost a diagonal matrix with the eigenvalues on the diagonal except for the last $l$ columns. $$[T]_B = \begin{bmatrix} \lambda_1 & & & & & * & \ldots & * \\ & \ddots & & & & & & \\ & & \lambda_1 & & & \vdots & & \vdots \\ & & & \ddots & & & & \\ & & & & \lambda_k & * & \ldots & * \\ & & & & & & & & \\ & & & & & \vdots & & \vdots \\ & & & & & & & \\ & & & & & * & \ldots & * \end{bmatrix}$$ For $[T]_B$ to be diagonal, the $w_j$ needs to be chosen, so that there are only stars above the diagonal. This means that $T(w_j)$ needs to be a linear combination of only the basis vectors before this specific $w_j$, which are all the $v^i_j$ and some of the $w_j$, i.e. $T(w_j) = \sum_{i,j} c^i_j v^i_j + \sum_{k}^j c_k w_k$ for some constants $c^i_j,c_k$, because then $$[T(w_j)]_B=\begin{bmatrix} c^1_1 \\ \vdots \\ c^k_{d_k} \\ c_1 \\ \vdots \\ c_j \\ 0 \\ \vdots \\ 0 \end{bmatrix}$$ where the last possible non-zero entry $c_j$ corresponds to $w_j$. Therefore $[T]_B$ would be upper triangular. How do I know that such a linear combination for $w_j$ exists and how could I choose it?",,"['linear-algebra', 'matrices']"
94,Computing the derivative of $Axx^TB^T$ with respect to $x$,Computing the derivative of  with respect to,Axx^TB^T x,"I want to compute the derivative of \begin{align} f(x) = Axx^\top B^\top \label{eqn} \end{align} with respect to $x$ where $A$ and $B$ are $n\times n$ matrices and $x$ is a (column) vector of size $n \times 1$. By this I mean the derivative of each component of $f(x)$ with respect to each component of $x$. I can prove that if  $$ g(x) = xx^\top $$ Then the derivative can be expressed as, $$ \frac{\partial g}{\partial x} = x \otimes I_n + I_n \otimes x $$ where $I_n$ is the $n\times n$ identity matrix. In here I am vectorizing $xx^\top$  and then taking the derivative with respect to each of the components of $x$. Question: Is there a way to extend this result to $f(x)$. My gut feeling is that this should be possible. Any thoughts?. If that's not possible how do I go about computing it? EDIT (After Rodrigo de Azevedo's comment): You are right. But I mean the derivative in the following flattened sense. I hope this makes it a bit clearer. Let us consider the $2 \times 2$ case. Then $ Y =f(x)$ is a $2 \times 2$ matrix. If I vectorize $f(x)$ then I can view $f$ as, $$ f: \mathbb{R}^2 \to \mathbb{R}^4 $$ More precisely \begin{align} f: \begin{bmatrix}   x_1 \\   x_2 \end{bmatrix} \to   \begin{bmatrix}     Y_{11} \\     Y_{21} \\     Y_{12} \\     Y_{22}   \end{bmatrix} \end{align} Then by the symbol $\frac{\partial{f(x)}}{\partial{x}}$ I mean the following: \begin{align}   \frac{\partial{f(x)}}{\partial{x}}   & =     \begin{bmatrix}       \frac{\partial{Y_{11}}}{\partial{x_1}} &       \frac{\partial{Y_{11}}}{\partial{x_2}} \\       \frac{\partial{Y_{21}}}{\partial{x_1}} &       \frac{\partial{Y_{21}}}{\partial{x_2}} \\       \frac{\partial{Y_{12}}}{\partial{x_1}} &       \frac{\partial{Y_{12}}}{\partial{x_2}} \\       \frac{\partial{Y_{22}}}{\partial{x_1}} &       \frac{\partial{Y_{22}}}{\partial{x_2}}     \end{bmatrix} \end{align}","I want to compute the derivative of \begin{align} f(x) = Axx^\top B^\top \label{eqn} \end{align} with respect to $x$ where $A$ and $B$ are $n\times n$ matrices and $x$ is a (column) vector of size $n \times 1$. By this I mean the derivative of each component of $f(x)$ with respect to each component of $x$. I can prove that if  $$ g(x) = xx^\top $$ Then the derivative can be expressed as, $$ \frac{\partial g}{\partial x} = x \otimes I_n + I_n \otimes x $$ where $I_n$ is the $n\times n$ identity matrix. In here I am vectorizing $xx^\top$  and then taking the derivative with respect to each of the components of $x$. Question: Is there a way to extend this result to $f(x)$. My gut feeling is that this should be possible. Any thoughts?. If that's not possible how do I go about computing it? EDIT (After Rodrigo de Azevedo's comment): You are right. But I mean the derivative in the following flattened sense. I hope this makes it a bit clearer. Let us consider the $2 \times 2$ case. Then $ Y =f(x)$ is a $2 \times 2$ matrix. If I vectorize $f(x)$ then I can view $f$ as, $$ f: \mathbb{R}^2 \to \mathbb{R}^4 $$ More precisely \begin{align} f: \begin{bmatrix}   x_1 \\   x_2 \end{bmatrix} \to   \begin{bmatrix}     Y_{11} \\     Y_{21} \\     Y_{12} \\     Y_{22}   \end{bmatrix} \end{align} Then by the symbol $\frac{\partial{f(x)}}{\partial{x}}$ I mean the following: \begin{align}   \frac{\partial{f(x)}}{\partial{x}}   & =     \begin{bmatrix}       \frac{\partial{Y_{11}}}{\partial{x_1}} &       \frac{\partial{Y_{11}}}{\partial{x_2}} \\       \frac{\partial{Y_{21}}}{\partial{x_1}} &       \frac{\partial{Y_{21}}}{\partial{x_2}} \\       \frac{\partial{Y_{12}}}{\partial{x_1}} &       \frac{\partial{Y_{12}}}{\partial{x_2}} \\       \frac{\partial{Y_{22}}}{\partial{x_1}} &       \frac{\partial{Y_{22}}}{\partial{x_2}}     \end{bmatrix} \end{align}",,"['matrices', 'derivatives', 'matrix-calculus', 'kronecker-product']"
95,How to generate a matrix group with two generators?,How to generate a matrix group with two generators?,,"Let $\text{GL}(n,q)$ denote the group of all the invertible $n$ by $n$ matrices over finite field $\mathbb{F}_q$. $\text{GL}(n,q)$ be generated by two elements for all $n>2$. See here . Now my first question is: Given two elements $A$, $B$ in $\text{GL}(n,q)$, how to generate the   group $G=\langle A, B \rangle$ ? Can we find out all the maximal subgroups of $\text{GL}(n,q)$ ? It seems this question may be to hard. See here . Thanks for provide any information about the first question. Then the second question is: Let $n=2$, $q=5$.  Denote $${\displaystyle A={\begin{pmatrix}1 & 3 \\  3 & 0 \end{pmatrix}}}\in \text{GL}(n,q),  {\displaystyle  B={\begin{pmatrix}4 & 4 \\ 1 & 0 \end{pmatrix}}}\in \text{GL}(n,q).$$   Please compute all the elements in $\langle A, B \rangle$. By the computation in magma software, $|\langle A, B \rangle|=|\langle A \rangle| \times |\langle B \rangle|$, where $|\langle A \rangle|=6$ and $|\langle B \rangle|=3$. It's special case. So my third question is: Let $n=2$, $q=7$.  Denote $${\displaystyle C={\begin{pmatrix}6 & 6 \\  1 & 0 \end{pmatrix}}}\in \text{GL}(n,q),  {\displaystyle  D={\begin{pmatrix}4 & 4 \\ 0 & 2 \end{pmatrix}}}\in \text{GL}(n,q).$$   Please compute all the elements in $\langle C, D \rangle$. In the third question, it has $|\langle C, D \rangle| \ne|\langle C \rangle| \times |\langle D \rangle|$. $|\langle C \rangle|=3$ and $|\langle D \rangle|=3$ while $|\langle C, D \rangle|=24$. Thanks for any replies.","Let $\text{GL}(n,q)$ denote the group of all the invertible $n$ by $n$ matrices over finite field $\mathbb{F}_q$. $\text{GL}(n,q)$ be generated by two elements for all $n>2$. See here . Now my first question is: Given two elements $A$, $B$ in $\text{GL}(n,q)$, how to generate the   group $G=\langle A, B \rangle$ ? Can we find out all the maximal subgroups of $\text{GL}(n,q)$ ? It seems this question may be to hard. See here . Thanks for provide any information about the first question. Then the second question is: Let $n=2$, $q=5$.  Denote $${\displaystyle A={\begin{pmatrix}1 & 3 \\  3 & 0 \end{pmatrix}}}\in \text{GL}(n,q),  {\displaystyle  B={\begin{pmatrix}4 & 4 \\ 1 & 0 \end{pmatrix}}}\in \text{GL}(n,q).$$   Please compute all the elements in $\langle A, B \rangle$. By the computation in magma software, $|\langle A, B \rangle|=|\langle A \rangle| \times |\langle B \rangle|$, where $|\langle A \rangle|=6$ and $|\langle B \rangle|=3$. It's special case. So my third question is: Let $n=2$, $q=7$.  Denote $${\displaystyle C={\begin{pmatrix}6 & 6 \\  1 & 0 \end{pmatrix}}}\in \text{GL}(n,q),  {\displaystyle  D={\begin{pmatrix}4 & 4 \\ 0 & 2 \end{pmatrix}}}\in \text{GL}(n,q).$$   Please compute all the elements in $\langle C, D \rangle$. In the third question, it has $|\langle C, D \rangle| \ne|\langle C \rangle| \times |\langle D \rangle|$. $|\langle C \rangle|=3$ and $|\langle D \rangle|=3$ while $|\langle C, D \rangle|=24$. Thanks for any replies.",,"['matrices', 'group-theory', 'finite-groups', 'finite-fields']"
96,Invertibility and rank,Invertibility and rank,,"How do you formally prove that a matrix A is invertible if and only if it has full rank, without using determinants?","How do you formally prove that a matrix A is invertible if and only if it has full rank, without using determinants?",,"['linear-algebra', 'matrices']"
97,Is there a $4\times4$ matrix $A$ such that $\det(A) = a^4+b^4+c^4+d^4$?,Is there a  matrix  such that ?,4\times4 A \det(A) = a^4+b^4+c^4+d^4,"There is a $2\times2$ matrix $A$ composed of reals $a$ and $b$ such that $\det(A) = a^2 + b^2$, namely $$A=\begin{bmatrix} a & -b \\ b & a \\ \end{bmatrix}.$$ It would seem there is not an analogy for $3\times3$ matrices.  That is, I don't think there is a matrix $A$ composed of three reals $a$, $b$, and $c$ such that $\det(A) = a^3 + b^3 + c^3$. However, matrices with even dimensions give an opportunity for cross-terms to cancel.  Is there then, for example, a $4\times4$ matrix $A$ composed of reals $a$, $b$, $c$ and $d$, such that $$\det(A) = a^4+b^4+c^4+d^4$$ Formalized statement : For all reals $a$, $b$, $c$ and $d$, I am looking for a $4\times4$ matrix $A$ such that $A_{ij} = \{\pm a,\pm b,\pm c,\pm d\}$ and $\det(A) = a^4+b^4+c^4+d^4$.  Is there such a matrix?","There is a $2\times2$ matrix $A$ composed of reals $a$ and $b$ such that $\det(A) = a^2 + b^2$, namely $$A=\begin{bmatrix} a & -b \\ b & a \\ \end{bmatrix}.$$ It would seem there is not an analogy for $3\times3$ matrices.  That is, I don't think there is a matrix $A$ composed of three reals $a$, $b$, and $c$ such that $\det(A) = a^3 + b^3 + c^3$. However, matrices with even dimensions give an opportunity for cross-terms to cancel.  Is there then, for example, a $4\times4$ matrix $A$ composed of reals $a$, $b$, $c$ and $d$, such that $$\det(A) = a^4+b^4+c^4+d^4$$ Formalized statement : For all reals $a$, $b$, $c$ and $d$, I am looking for a $4\times4$ matrix $A$ such that $A_{ij} = \{\pm a,\pm b,\pm c,\pm d\}$ and $\det(A) = a^4+b^4+c^4+d^4$.  Is there such a matrix?",,"['matrices', 'determinant']"
98,Matrix nilpotent problem different version,Matrix nilpotent problem different version,,"I have matrices $X,Y$ of dimension $n$ with real coefficients which satisfy the following: $XY+YX=c(YX-XY)$ where $c$ is a real number. If $c\neq0$ , prove that $(YX-XY)^n = 0$. So far, I've been able to show that $YX-XY$ is singular. Can someone help?","I have matrices $X,Y$ of dimension $n$ with real coefficients which satisfy the following: $XY+YX=c(YX-XY)$ where $c$ is a real number. If $c\neq0$ , prove that $(YX-XY)^n = 0$. So far, I've been able to show that $YX-XY$ is singular. Can someone help?",,"['linear-algebra', 'matrices']"
99,Determinant of circulant-like matrix,Determinant of circulant-like matrix,,"Given is an $n \times n$ matrix $M_n$ with the following structure — $$M_n = \left( \begin{array}{ccccc} a_1 & 0 & \dots & 0 & b_1 \\ b_2 & a_2 &  &  & 0\\  & \ddots & \ddots &  & \vdots \\  &  & b_{n-1} & a_{n-1} & 0\\  &  &  & b_n & a_n\end{array} \right),$$ with $a_i$ and $b_i \in \mathbb{R}$ where $i \in \{1,\dots,n\}$ . Is there a name for this specific type of matrix? The non-zero entries in each row are shifted one position to the right compared to the previous row, which reminds me of a circulant matrix (though in that case, the rows share the same values, which is generally not the case for $M_n$ ). Is there a short/fast way (i.e. not just using Leibniz's formula ) to compute the determinant of this type of matrix? It might help to consider an example, so here's $M_5$ : $$M_5 = \left( \begin{array}{ccccc} a_1 & 0 & 0 & 0 & b_1 \\ b_2 & a_2 & 0 & 0 & 0\\ 0 & b_3 & a_3 & 0 & 0\\ 0 & 0 & b_4 & a_4 & 0\\ 0 & 0 & 0 & b_5 & a_5\end{array} \right) = \left( \begin{array}{ccccc} a_1 &  &  &  & b_1 \\ b_2 & a_2 &  &  & \\  & b_3 & a_3 &  & \\  &  & b_4 & a_4 & \\  &  &  & b_5 & a_5\end{array} \right)$$","Given is an matrix with the following structure — with and where . Is there a name for this specific type of matrix? The non-zero entries in each row are shifted one position to the right compared to the previous row, which reminds me of a circulant matrix (though in that case, the rows share the same values, which is generally not the case for ). Is there a short/fast way (i.e. not just using Leibniz's formula ) to compute the determinant of this type of matrix? It might help to consider an example, so here's :","n \times n M_n M_n = \left(
\begin{array}{ccccc}
a_1 & 0 & \dots & 0 & b_1 \\
b_2 & a_2 &  &  & 0\\
 & \ddots & \ddots &  & \vdots \\
 &  & b_{n-1} & a_{n-1} & 0\\
 &  &  & b_n & a_n\end{array}
\right), a_i b_i \in \mathbb{R} i \in \{1,\dots,n\} M_n M_5 M_5 = \left(
\begin{array}{ccccc}
a_1 & 0 & 0 & 0 & b_1 \\
b_2 & a_2 & 0 & 0 & 0\\
0 & b_3 & a_3 & 0 & 0\\
0 & 0 & b_4 & a_4 & 0\\
0 & 0 & 0 & b_5 & a_5\end{array}
\right) = \left(
\begin{array}{ccccc}
a_1 &  &  &  & b_1 \\
b_2 & a_2 &  &  & \\
 & b_3 & a_3 &  & \\
 &  & b_4 & a_4 & \\
 &  &  & b_5 & a_5\end{array}
\right)","['matrices', 'determinant']"
