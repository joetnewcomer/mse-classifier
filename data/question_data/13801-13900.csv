,title_raw,title_text,title_latex,body_raw,body_text,body_latex,tags
0,How Can the Units of an Angle Measurement Affect the Derivative of Sine?,How Can the Units of an Angle Measurement Affect the Derivative of Sine?,,"I've read the question: ""Why does the derivative of sine only work for radians?"" and I can follow the derivation for the derivative of sine when measured in degrees, but the result confuses me. Does this mean the derivative of the sine changes values when measured in different units? For example, would the derivative of sine at $45$ degrees not be the same as the derivative of the sine at $\pi/4$ radians? How could this be the case?","I've read the question: ""Why does the derivative of sine only work for radians?"" and I can follow the derivation for the derivative of sine when measured in degrees, but the result confuses me. Does this mean the derivative of the sine changes values when measured in different units? For example, would the derivative of sine at degrees not be the same as the derivative of the sine at radians? How could this be the case?",45 \pi/4,"['calculus', 'derivatives', 'trigonometry']"
1,"Calculate, $f\bigg(\frac{1}{1997}\bigg)+f\bigg(\frac{2}{1997}\bigg)+f\bigg(\frac{3}{1997}\bigg)\ldots f\bigg(\frac{1996}{1997}\bigg)$ [duplicate]","Calculate,  [duplicate]",f\bigg(\frac{1}{1997}\bigg)+f\bigg(\frac{2}{1997}\bigg)+f\bigg(\frac{3}{1997}\bigg)\ldots f\bigg(\frac{1996}{1997}\bigg),"This question already has an answer here : find $f(\frac{1}{2014})+f(\frac{2}{2014})+.....+f(\frac{2013}{2014})$ of $f(x)=\frac{2}{2+4^x}$ (1 answer) Closed 5 years ago . If $$f(x)=\frac{4^x}{4^x+2}$$ Calculate, $$f\bigg(\frac{1}{1997}\bigg)+f\bigg(\frac{2}{1997}\bigg)+f\bigg(\frac{3}{1997}\bigg)\ldots  f\bigg(\frac{1996}{1997}\bigg)$$ My Attempt: I was not able to generalise the expression or get a solid pattern, so I started with smaller numbers and calculated, $$f\bigg(\frac{1}{2}\bigg)=\frac{1}{2}$$ $$f\bigg(\frac{1}{3}\bigg)+f\bigg(\frac{2}{3}\bigg)=1$$ $$f\bigg(\frac{1}{4}\bigg)+f\bigg(\frac{2}{4}\bigg)+f\bigg(\frac{3}{4}\bigg)=\frac{3}{2}$$ I could see that, $$f\bigg(\frac{1}{n}\bigg)+f\bigg(\frac{2}{n}\bigg)+f\bigg(\frac{3}{n}\bigg)\ldots  f\bigg(\frac{n-1}{n}\bigg)=\frac{n-1}{2}$$ So,  $$f\bigg(\frac{1}{1997}\bigg)+f\bigg(\frac{2}{1997}\bigg)+f\bigg(\frac{3}{1997}\bigg)\ldots  f\bigg(\frac{1996}{1997}\bigg)=998$$ which is indeed the right answer. But I am not satisfied with my method. How else can I solve it?","This question already has an answer here : find $f(\frac{1}{2014})+f(\frac{2}{2014})+.....+f(\frac{2013}{2014})$ of $f(x)=\frac{2}{2+4^x}$ (1 answer) Closed 5 years ago . If $$f(x)=\frac{4^x}{4^x+2}$$ Calculate, $$f\bigg(\frac{1}{1997}\bigg)+f\bigg(\frac{2}{1997}\bigg)+f\bigg(\frac{3}{1997}\bigg)\ldots  f\bigg(\frac{1996}{1997}\bigg)$$ My Attempt: I was not able to generalise the expression or get a solid pattern, so I started with smaller numbers and calculated, $$f\bigg(\frac{1}{2}\bigg)=\frac{1}{2}$$ $$f\bigg(\frac{1}{3}\bigg)+f\bigg(\frac{2}{3}\bigg)=1$$ $$f\bigg(\frac{1}{4}\bigg)+f\bigg(\frac{2}{4}\bigg)+f\bigg(\frac{3}{4}\bigg)=\frac{3}{2}$$ I could see that, $$f\bigg(\frac{1}{n}\bigg)+f\bigg(\frac{2}{n}\bigg)+f\bigg(\frac{3}{n}\bigg)\ldots  f\bigg(\frac{n-1}{n}\bigg)=\frac{n-1}{2}$$ So,  $$f\bigg(\frac{1}{1997}\bigg)+f\bigg(\frac{2}{1997}\bigg)+f\bigg(\frac{3}{1997}\bigg)\ldots  f\bigg(\frac{1996}{1997}\bigg)=998$$ which is indeed the right answer. But I am not satisfied with my method. How else can I solve it?",,"['calculus', 'functions']"
2,If $\int_{1}^{\infty}f\left(x\right)dx$ converges absolutely then $\int_{1}^{\infty}f^{2}\left(x\right)dx$ converges?,If  converges absolutely then  converges?,\int_{1}^{\infty}f\left(x\right)dx \int_{1}^{\infty}f^{2}\left(x\right)dx,"Let $f:[0,\infty)\to\mathbb{R}$ be a continuous function. I was asked to prove/disprove the following statement: If $\int_{1}^{\infty}f\left(x\right)dx$ converges absolutely then $\int_{1}^{\infty}f^{2}\left(x\right)dx$ converges as well. I figure that my only way of proving this is by direct comparison. However, for that to work I need $f(x)\leq1$ for sufficiently large $x$ , but $\int_{1}^{\infty}f\left(x\right)dx$ converging (Even absolutely) does not guarantee this. If the statement is false I'd appreciate a hint on how to construct a counter example, rather then one pulled out of thin air.","Let be a continuous function. I was asked to prove/disprove the following statement: If converges absolutely then converges as well. I figure that my only way of proving this is by direct comparison. However, for that to work I need for sufficiently large , but converging (Even absolutely) does not guarantee this. If the statement is false I'd appreciate a hint on how to construct a counter example, rather then one pulled out of thin air.","f:[0,\infty)\to\mathbb{R} \int_{1}^{\infty}f\left(x\right)dx \int_{1}^{\infty}f^{2}\left(x\right)dx f(x)\leq1 x \int_{1}^{\infty}f\left(x\right)dx","['calculus', 'integration', 'improper-integrals']"
3,What is the proof behind $\lim f(g(x)) = f(\lim g(x))$?,What is the proof behind ?,\lim f(g(x)) = f(\lim g(x)),In general I see that we can evaluate $\lim f(g(x)) = f(\lim g(x))$ if $f$ is continuous. How do we know this? How do we prove this? I tried looking at it with the epsilon-delta definition of a limit but I didn't get very far since I don't know how you even begin to analyze $f(\lim g(x))$.,In general I see that we can evaluate $\lim f(g(x)) = f(\lim g(x))$ if $f$ is continuous. How do we know this? How do we prove this? I tried looking at it with the epsilon-delta definition of a limit but I didn't get very far since I don't know how you even begin to analyze $f(\lim g(x))$.,,"['calculus', 'limits', 'continuity', 'epsilon-delta']"
4,Residue theorem for a rational function,Residue theorem for a rational function,,"Evaluate    $$\int_{-\infty}^{\infty}\frac{x^4}{1+x^8}\mathrm{d} x.$$ My concern: One of the consequences of the residue theorem states that, given a polynomial of the form $P/Q$ such that the degree of $Q$ exceeds $P$ by at least two, the integral can then be expressed as $$\int fdz=2\pi i\sum_{U}{\mathrm{Res}\left ( f;z_{i} \right )}.$$  The zeros of $Q$ in the Upper half plane  is here given by $z=e^{\frac{i \pi}{8}\left ( 2n+1 \right )}$ for $n\in \left \{0,1,2,3\right \}$. The residue at $z_{n}$ is now given by $\frac{P(z_{n})}{Q´(z_{n})}$. Should I derive the polynomial or exponential function?","Evaluate    $$\int_{-\infty}^{\infty}\frac{x^4}{1+x^8}\mathrm{d} x.$$ My concern: One of the consequences of the residue theorem states that, given a polynomial of the form $P/Q$ such that the degree of $Q$ exceeds $P$ by at least two, the integral can then be expressed as $$\int fdz=2\pi i\sum_{U}{\mathrm{Res}\left ( f;z_{i} \right )}.$$  The zeros of $Q$ in the Upper half plane  is here given by $z=e^{\frac{i \pi}{8}\left ( 2n+1 \right )}$ for $n\in \left \{0,1,2,3\right \}$. The residue at $z_{n}$ is now given by $\frac{P(z_{n})}{Q´(z_{n})}$. Should I derive the polynomial or exponential function?",,"['calculus', 'integration', 'complex-analysis', 'improper-integrals']"
5,"sequence of positive numbers satisfying $a_{n+1}=\frac{2}{a_n+a_{n-1}}$, prove it converges","sequence of positive numbers satisfying , prove it converges",a_{n+1}=\frac{2}{a_n+a_{n-1}},"Assume that $(a_n)$ is a sequence of positive real numbers satisfying $a_{n+1}=\frac{2}{a_n+a_{n-1}}$ for $n=2,3,\dots$. Prove that $(a_n)$ is convergent and find the limit. I have no idea how to prove convergence. I tried to prove that $(a_n)$ is bounded and monotone, but both trials failed. If convergence is proven, the limit is easy: if $g=\lim a_n$, then $g\ge 0$ because all $a_n$'s are positive, the equivalent relation $a_{n+1}(a_n+a_{n-1})=2$ gives $g(g+g)=2$, so finally $g=1$.","Assume that $(a_n)$ is a sequence of positive real numbers satisfying $a_{n+1}=\frac{2}{a_n+a_{n-1}}$ for $n=2,3,\dots$. Prove that $(a_n)$ is convergent and find the limit. I have no idea how to prove convergence. I tried to prove that $(a_n)$ is bounded and monotone, but both trials failed. If convergence is proven, the limit is easy: if $g=\lim a_n$, then $g\ge 0$ because all $a_n$'s are positive, the equivalent relation $a_{n+1}(a_n+a_{n-1})=2$ gives $g(g+g)=2$, so finally $g=1$.",,"['calculus', 'sequences-and-series', 'limits', 'convergence-divergence']"
6,What is $\lim_{n \to \infty} n a_n$?,What is ?,\lim_{n \to \infty} n a_n,"Let $L(x) = x - \frac{x^2}{2}$ and let $$ a_n = \underbrace{L(L(\cdots L(}_{n \text{ times}}\frac{17}{n}))\cdots).$$ I'm trying to find $\lim_{n \to \infty} n a_n$. I don't really know how to proceed. I tried calculating the first few values of $na_n$ and plotted them and got The limit seems to be somewhat above $1.6$. (By the way, I'm not sure why the horizontal axis is labeled like that. I plotted $na_n$ from for $n \in \{ 10, \cdots, 23 \}$, because the early values of $n a_n$ blow up. But for some reason the labeling started from $1$.) How could I go about finding the limit? There has to be some sort of trick (this is apparently from the high school math competition ""Math Prize for Girls"", but I found this problem on expii), but I'm just not seeing it... Edit: Frenzy Li linked to this question. The question is the same, but the answer given back then (2) appears to be incorrect. (Based on computing large values, it appears to be $\frac{34}{19}$.)","Let $L(x) = x - \frac{x^2}{2}$ and let $$ a_n = \underbrace{L(L(\cdots L(}_{n \text{ times}}\frac{17}{n}))\cdots).$$ I'm trying to find $\lim_{n \to \infty} n a_n$. I don't really know how to proceed. I tried calculating the first few values of $na_n$ and plotted them and got The limit seems to be somewhat above $1.6$. (By the way, I'm not sure why the horizontal axis is labeled like that. I plotted $na_n$ from for $n \in \{ 10, \cdots, 23 \}$, because the early values of $n a_n$ blow up. But for some reason the labeling started from $1$.) How could I go about finding the limit? There has to be some sort of trick (this is apparently from the high school math competition ""Math Prize for Girls"", but I found this problem on expii), but I'm just not seeing it... Edit: Frenzy Li linked to this question. The question is the same, but the answer given back then (2) appears to be incorrect. (Based on computing large values, it appears to be $\frac{34}{19}$.)",,"['calculus', 'limits', 'contest-math']"
7,How can we show that $\int_{0}^{\infty}x\sin{x}\ln{(1-e^{-x})}\mathrm dx=1-{\pi\over 2\tanh\pi}-{\pi^2\over 2\sinh^2{\pi}}?$,How can we show that,\int_{0}^{\infty}x\sin{x}\ln{(1-e^{-x})}\mathrm dx=1-{\pi\over 2\tanh\pi}-{\pi^2\over 2\sinh^2{\pi}}?,"Consider the integral $(1)$ $$\int_{0}^{\infty}x\sin{x}\ln{(1-e^{-x})}\mathrm dx=I\tag1$$   How can we show that $$I=1-{\pi\over 2\tanh\pi}-{\pi^2\over 2\sinh^2{\pi}}$$ An attempt: Dealing with indefinite integral $$\int x\sin{x}\ln{(1-e^{-x})}\mathrm dx=J\tag2$$ Apply integration by parts $u=\ln{(1-e^{-x})}$ then $du={e^{-x}\over 1-e^{-x}}\mathrm dx$ $v=-\int x\sin{x}\mathrm dx=-x\cos{x}+\sin{x}$ $$J=(-x\cos{x}+\sin{x})\ln{(1-e^{-x})}-\int{e^{-x}\over 1-e^{-x}}(\sin{x}-x\cos{x})\mathrm dx\tag3$$ $$J=(-x\cos{x}+\sin{x})\ln{(1-e^{-x})}-\int\sum_{n=0}^{\infty}e^{x(1-n)}(\sin{x}-x\cos{x})\mathrm dx\tag4$$ $$J=(-x\cos{x}+\sin{x})\ln{(1-e^{-x})}-\sum_{n=0}^{\infty}\int e^{x(1-n)}(\sin{x}-x\cos{x})\mathrm dx\tag5$$ Let Applying integration by parts $$J_1=\int e^{x(1-n)}\sin{x}\mathrm dx={e^{x(1-n)}[(1-n)\sin{x}-\cos{x}]\over (1-n)^2+1}$$ $$J_2=\int xe^{x(1-n)}\cos{x}\mathrm dx={xe^{x(1-n)}[(1-n)\cos{x}+\sin{x}]\over (1-n)^2+1}-{e^{x(1-n)}[(n^2-2n)\cos{x}-2(1-n)\sin{x}]\over ((1-n)^2+1)^2}$$ So far applying integration by parts seem bit hard to resolve problem $(1)$, how else can we tackle $(1)?$","Consider the integral $(1)$ $$\int_{0}^{\infty}x\sin{x}\ln{(1-e^{-x})}\mathrm dx=I\tag1$$   How can we show that $$I=1-{\pi\over 2\tanh\pi}-{\pi^2\over 2\sinh^2{\pi}}$$ An attempt: Dealing with indefinite integral $$\int x\sin{x}\ln{(1-e^{-x})}\mathrm dx=J\tag2$$ Apply integration by parts $u=\ln{(1-e^{-x})}$ then $du={e^{-x}\over 1-e^{-x}}\mathrm dx$ $v=-\int x\sin{x}\mathrm dx=-x\cos{x}+\sin{x}$ $$J=(-x\cos{x}+\sin{x})\ln{(1-e^{-x})}-\int{e^{-x}\over 1-e^{-x}}(\sin{x}-x\cos{x})\mathrm dx\tag3$$ $$J=(-x\cos{x}+\sin{x})\ln{(1-e^{-x})}-\int\sum_{n=0}^{\infty}e^{x(1-n)}(\sin{x}-x\cos{x})\mathrm dx\tag4$$ $$J=(-x\cos{x}+\sin{x})\ln{(1-e^{-x})}-\sum_{n=0}^{\infty}\int e^{x(1-n)}(\sin{x}-x\cos{x})\mathrm dx\tag5$$ Let Applying integration by parts $$J_1=\int e^{x(1-n)}\sin{x}\mathrm dx={e^{x(1-n)}[(1-n)\sin{x}-\cos{x}]\over (1-n)^2+1}$$ $$J_2=\int xe^{x(1-n)}\cos{x}\mathrm dx={xe^{x(1-n)}[(1-n)\cos{x}+\sin{x}]\over (1-n)^2+1}-{e^{x(1-n)}[(n^2-2n)\cos{x}-2(1-n)\sin{x}]\over ((1-n)^2+1)^2}$$ So far applying integration by parts seem bit hard to resolve problem $(1)$, how else can we tackle $(1)?$",,"['calculus', 'integration', 'sequences-and-series', 'definite-integrals', 'closed-form']"
8,$\lim_{n\to\infty}\int_{-\infty}^{\infty}{\sin(n+0.5)x\over \sin(x/2)}\cdot{\mathrm dx\over 1+x^2}=\pi\cdot{e+1\over e-1}$,,\lim_{n\to\infty}\int_{-\infty}^{\infty}{\sin(n+0.5)x\over \sin(x/2)}\cdot{\mathrm dx\over 1+x^2}=\pi\cdot{e+1\over e-1},"How can we show that $$\lim_{n\to\infty}\int_{-\infty}^{\infty}{\sin(n+0.5)x\over \sin(x/2)}\cdot{\mathrm dx\over 1+x^2}=\pi\cdot{e+1\over e-1}\tag1$$ $(1)$, substitution doesn't work, either integration by parts. We know $(2)$ $$\int_{-\infty}^{\infty}{\mathrm dx\over 1+x^2}=\pi\tag2$$ $${\sin(n+0.5)x\over \sin(x/2)}={\sin(nx)\cos(x/2)+\sin(x/2)\cos(nx)\over \sin(x/2)}\tag3$$ Simplified to $$=\sin(nx)\coth(x/2)+\cos(nx)\tag4$$ $$\lim_{n\to\infty}\int_{-\infty}^{\infty}\sin(nx)\cot(x/2)\cdot{\mathrm dx\over 1+x^2}+\int_{-\infty}^{\infty}\cos(nx)\cdot{\mathrm dx\over 1+x^2}=\pi\cdot{e+1\over e-1}\tag5$$ $$\lim_{n\to\infty}\int_{-\infty}^{\infty}\sin(nx)\cot(x/2)\cdot{\mathrm dx\over 1+x^2}+{\pi\over e^n}=\pi\cdot{e+1\over e-1}\tag6$$ $$\lim_{n\to\infty}\int_{-\infty}^{\infty}\sin(nx)\cot(x/2)\cdot{\mathrm dx\over 1+x^2}=\pi\cdot{e+1\over e-1}\tag7$$ I am not sure how to continue","How can we show that $$\lim_{n\to\infty}\int_{-\infty}^{\infty}{\sin(n+0.5)x\over \sin(x/2)}\cdot{\mathrm dx\over 1+x^2}=\pi\cdot{e+1\over e-1}\tag1$$ $(1)$, substitution doesn't work, either integration by parts. We know $(2)$ $$\int_{-\infty}^{\infty}{\mathrm dx\over 1+x^2}=\pi\tag2$$ $${\sin(n+0.5)x\over \sin(x/2)}={\sin(nx)\cos(x/2)+\sin(x/2)\cos(nx)\over \sin(x/2)}\tag3$$ Simplified to $$=\sin(nx)\coth(x/2)+\cos(nx)\tag4$$ $$\lim_{n\to\infty}\int_{-\infty}^{\infty}\sin(nx)\cot(x/2)\cdot{\mathrm dx\over 1+x^2}+\int_{-\infty}^{\infty}\cos(nx)\cdot{\mathrm dx\over 1+x^2}=\pi\cdot{e+1\over e-1}\tag5$$ $$\lim_{n\to\infty}\int_{-\infty}^{\infty}\sin(nx)\cot(x/2)\cdot{\mathrm dx\over 1+x^2}+{\pi\over e^n}=\pi\cdot{e+1\over e-1}\tag6$$ $$\lim_{n\to\infty}\int_{-\infty}^{\infty}\sin(nx)\cot(x/2)\cdot{\mathrm dx\over 1+x^2}=\pi\cdot{e+1\over e-1}\tag7$$ I am not sure how to continue",,"['calculus', 'integration', 'limits', 'definite-integrals']"
9,Evaluating $\int^{\pi}_0\arctan\left(\frac{p\sin x}{1-p\cos x}\right)\sin(nx) dx$ by differentiation under integral?,Evaluating  by differentiation under integral?,\int^{\pi}_0\arctan\left(\frac{p\sin x}{1-p\cos x}\right)\sin(nx) dx,I saw that $$ \int^{\pi}_{0}\arctan \left(\frac{p \sin x}{1-p \cos x}\right) \sin(nx) dx=\frac{\pi}{2n} p^n   $$ for $$p^2 <1$$ I tried to prove using differentiation under integral but got stuck at this step $$ I^{\prime} (p)=\int^{\pi}_{0} \frac{\sin x \sin (nx)}{1+p^2-2p \cos x}dx  $$ What to do next?,I saw that $$ \int^{\pi}_{0}\arctan \left(\frac{p \sin x}{1-p \cos x}\right) \sin(nx) dx=\frac{\pi}{2n} p^n   $$ for $$p^2 <1$$ I tried to prove using differentiation under integral but got stuck at this step $$ I^{\prime} (p)=\int^{\pi}_{0} \frac{\sin x \sin (nx)}{1+p^2-2p \cos x}dx  $$ What to do next?,,"['calculus', 'integration', 'definite-integrals']"
10,Alternative ways to show that the Harmonic series diverges,Alternative ways to show that the Harmonic series diverges,,"This question came to me from one of my calculus students today: Other than using the integral test $$\int_1^\infty \frac{dx}{x} \to \infty,$$ what are some other ways that we can prove the Harmonic series $\sum_{n=1}^\infty \frac{1}{n}$ diverges? I'm sure there are plenty of methods out there; any method where a typical student in Calculus could understand would be great.","This question came to me from one of my calculus students today: Other than using the integral test $$\int_1^\infty \frac{dx}{x} \to \infty,$$ what are some other ways that we can prove the Harmonic series $\sum_{n=1}^\infty \frac{1}{n}$ diverges? I'm sure there are plenty of methods out there; any method where a typical student in Calculus could understand would be great.",,"['calculus', 'sequences-and-series', 'divergent-series']"
11,Euler's constant greater than 0 for all values of n?,Euler's constant greater than 0 for all values of n?,,If Euler's constant is described as the limit as n approaches infinity of the following: $$t_n = 1 + \frac 12 + \frac 13 \cdots + \frac1n -\ln(n)$$ How can one prove that $t_n$ is greater than $0$ for all values of $n$? Thanks!,If Euler's constant is described as the limit as n approaches infinity of the following: $$t_n = 1 + \frac 12 + \frac 13 \cdots + \frac1n -\ln(n)$$ How can one prove that $t_n$ is greater than $0$ for all values of $n$? Thanks!,,['calculus']
12,For what positive value of $c$ does the equation $\log(x)=cx^4$ have exactly one real root?,For what positive value of  does the equation  have exactly one real root?,c \log(x)=cx^4,For what positive value of $c$ does the equation $\log(x)=cx^4$ have exactly  one real root? I think I should find a way to apply IMV and Rolle's theorem to $f(x) = \log(x) - cx^4$. I think I should first find a range of values for $c$ such that the given equation has a solution and then try to find one that gives only a unique solution. I have thought over it but nothing comes to my mind. Perhaps I'm over-complicating it. Any ideas on how to proceed are appreciated.,For what positive value of $c$ does the equation $\log(x)=cx^4$ have exactly  one real root? I think I should find a way to apply IMV and Rolle's theorem to $f(x) = \log(x) - cx^4$. I think I should first find a range of values for $c$ such that the given equation has a solution and then try to find one that gives only a unique solution. I have thought over it but nothing comes to my mind. Perhaps I'm over-complicating it. Any ideas on how to proceed are appreciated.,,['calculus']
13,What is a pullback in simple calculus context?,What is a pullback in simple calculus context?,,"The definition of a pullback provided by my text is quite accessible Let $\phi : M \to N$, $f:N \to \mathbb{R}$, then $f\circ \phi: M \to \mathbb{R}$, where $\phi^*f = f\circ\phi$ and $\phi^*$ is the pullback of   $f$ by $\phi$ But when I look up some examples, things like category theory and sheafs and differential forms pop up! I only have a background in calculus, can someone provide a simple examples from calculus to illustrate the idea of a pullback? and why is pullback problematic/pathological/not desired??","The definition of a pullback provided by my text is quite accessible Let $\phi : M \to N$, $f:N \to \mathbb{R}$, then $f\circ \phi: M \to \mathbb{R}$, where $\phi^*f = f\circ\phi$ and $\phi^*$ is the pullback of   $f$ by $\phi$ But when I look up some examples, things like category theory and sheafs and differential forms pop up! I only have a background in calculus, can someone provide a simple examples from calculus to illustrate the idea of a pullback? and why is pullback problematic/pathological/not desired??",,"['calculus', 'differential-geometry']"
14,What is rational integral function,What is rational integral function,,"The author of the book I am referring assumes two rational integral functions as shown below $p_0\cdot x^n+p_1\cdot x^{(n-1)}+p_2\cdot x^{(n-2)}+...+p_n$ $q_0\cdot x^n+q_1\cdot x^{(n-1)}+q_2\cdot x^{(n-2)}+...+q_n$ Is this term old and he means real functions ?; if not, what is meant by rational integral function ?, rational function, integral function ?","The author of the book I am referring assumes two rational integral functions as shown below $p_0\cdot x^n+p_1\cdot x^{(n-1)}+p_2\cdot x^{(n-2)}+...+p_n$ $q_0\cdot x^n+q_1\cdot x^{(n-1)}+q_2\cdot x^{(n-2)}+...+q_n$ Is this term old and he means real functions ?; if not, what is meant by rational integral function ?, rational function, integral function ?",,"['calculus', 'algebra-precalculus']"
15,Limit $\lim_\limits{x\to0} \frac{\ln\left(x+\sqrt{1+x^2}\right)-x}{\tan^3(x)}$,Limit,\lim_\limits{x\to0} \frac{\ln\left(x+\sqrt{1+x^2}\right)-x}{\tan^3(x)},"Evaluate the given limit: $$\lim_{x\to0} \frac{\ln\left(x+\sqrt{1+x^2}\right)-x}{\tan^3(x)} .$$ I've tried to evaluate it but I always get stuck... Obviously I need L'Hôpital's Rule here, but still get confused on the way. May someone show me what is the trick here? Thanks.","Evaluate the given limit: $$\lim_{x\to0} \frac{\ln\left(x+\sqrt{1+x^2}\right)-x}{\tan^3(x)} .$$ I've tried to evaluate it but I always get stuck... Obviously I need L'Hôpital's Rule here, but still get confused on the way. May someone show me what is the trick here? Thanks.",,"['calculus', 'limits']"
16,Show that $\lim_{x \to +\infty}\left(f(x)+f'(x)\right)=0 \Rightarrow \lim_{x \to +\infty} f(x)=0$,Show that,\lim_{x \to +\infty}\left(f(x)+f'(x)\right)=0 \Rightarrow \lim_{x \to +\infty} f(x)=0,How to show that $\lim_{x \to +\infty}(f(x)+f'(x))=0 $ implies $\lim_{x \to +\infty} f(x)=0$?,How to show that $\lim_{x \to +\infty}(f(x)+f'(x))=0 $ implies $\lim_{x \to +\infty} f(x)=0$?,,"['calculus', 'limits', 'derivatives']"
17,"Evaluation of $-\int e^{\cos(t)}\sin(\sin(t)+t)\,dt $",Evaluation of,"-\int e^{\cos(t)}\sin(\sin(t)+t)\,dt ","How would I integrate this: $$-\int e^{\cos(t)}\sin(\sin(t)+t)\,dt $$ I have tried several methods but can't seem to work this out.","How would I integrate this: $$-\int e^{\cos(t)}\sin(\sin(t)+t)\,dt $$ I have tried several methods but can't seem to work this out.",,"['calculus', 'integration', 'trigonometry', 'indefinite-integrals']"
18,Show that a polynomial $P(x)$ has $r$ as a double root if and only if $P'(r)=0$ and $P(r)=0$,Show that a polynomial  has  as a double root if and only if  and,P(x) r P'(r)=0 P(r)=0,"Assuming that $r$ is a double root. Then $$P(x)=(x-r)^2\cdot k(x).$$ We also have the derivative: $$P'(x) = 2(x-r)k(x) + (x-r)^2k'(x).$$ Hence, $$P(r) = (r-r^2)k(r)=0$$ and $$P'(r) = 2(r-r)k(r) + (r-r)^2k'(r) = 0.$$ What I cannot show is the converse, that is, assuming that $r$ is a root of $P(x)$ ($P(r)=0$) and $P'(x)$ ($P'(r)=0$); how can we prove that $r$ is a double root of $P(x)$?","Assuming that $r$ is a double root. Then $$P(x)=(x-r)^2\cdot k(x).$$ We also have the derivative: $$P'(x) = 2(x-r)k(x) + (x-r)^2k'(x).$$ Hence, $$P(r) = (r-r^2)k(r)=0$$ and $$P'(r) = 2(r-r)k(r) + (r-r)^2k'(r) = 0.$$ What I cannot show is the converse, that is, assuming that $r$ is a root of $P(x)$ ($P(r)=0$) and $P'(x)$ ($P'(r)=0$); how can we prove that $r$ is a double root of $P(x)$?",,"['calculus', 'polynomials', 'roots']"
19,Is calculus not rigorous?,Is calculus not rigorous?,,"While studying single and multivariable calculus during my first year some people complained that calculus wasn't rigorous enough, when I asked about this no one seemed to be able to really specify exactly what was not rigorous about it. So I want to ask if this is true or if my friends tried to be smarty pants? My professors mentioned nothing about this. The only such thing I can think of is that we considered $\mathbb{R}$ (and $\mathbb{R}^n$) to be given and just kind of ""the number line of every number you possibly can think of"". We didn't care about the construction of the reals at all. But I'm pretty sure that this is not what they meant. I don't think I will take any sturdy course in real analysis so I want to ask if the standard definitions and proofs involving limits, derivatives, differentiability, continuity, integrals etc one stumbles upon in calculus is somehow ""simplified"" in calculus and made more formal and ""clear"" in later and more advanced courses in real analysis? If this is true, does it exist any good examples which can illustrate this for someone who is slightly afraid of epsilon and deltas?","While studying single and multivariable calculus during my first year some people complained that calculus wasn't rigorous enough, when I asked about this no one seemed to be able to really specify exactly what was not rigorous about it. So I want to ask if this is true or if my friends tried to be smarty pants? My professors mentioned nothing about this. The only such thing I can think of is that we considered $\mathbb{R}$ (and $\mathbb{R}^n$) to be given and just kind of ""the number line of every number you possibly can think of"". We didn't care about the construction of the reals at all. But I'm pretty sure that this is not what they meant. I don't think I will take any sturdy course in real analysis so I want to ask if the standard definitions and proofs involving limits, derivatives, differentiability, continuity, integrals etc one stumbles upon in calculus is somehow ""simplified"" in calculus and made more formal and ""clear"" in later and more advanced courses in real analysis? If this is true, does it exist any good examples which can illustrate this for someone who is slightly afraid of epsilon and deltas?",,"['calculus', 'soft-question']"
20,Derivative of a Matrix with respect to a vector,Derivative of a Matrix with respect to a vector,,"I know that for two k-vectors, say $A$ and $B$, $\partial A/\partial B$ would be a square $k \times k$ matrix whose $(i,j)$-th element would be $\partial A_i/\partial B_j$. But could someone please explain how the partial derivative look like if we were differentiating $k \times k$ matrix instead? That is, $M$ is a $k \times k$ matrix, $x$ is a $k$-vector, how can we write $\partial M/\partial x$? I tried to use the first principles, but no luck so far.. Thanks","I know that for two k-vectors, say $A$ and $B$, $\partial A/\partial B$ would be a square $k \times k$ matrix whose $(i,j)$-th element would be $\partial A_i/\partial B_j$. But could someone please explain how the partial derivative look like if we were differentiating $k \times k$ matrix instead? That is, $M$ is a $k \times k$ matrix, $x$ is a $k$-vector, how can we write $\partial M/\partial x$? I tried to use the first principles, but no luck so far.. Thanks",,"['calculus', 'linear-algebra', 'matrices', 'derivatives']"
21,Real roots of a polynomial,Real roots of a polynomial,,Let $p$ be an even degree polynomial with real coefficients such that the product of the constant term and the leading coefficient is negative. Show that $p$ has at least two real roots. Thanks!,Let $p$ be an even degree polynomial with real coefficients such that the product of the constant term and the leading coefficient is negative. Show that $p$ has at least two real roots. Thanks!,,"['calculus', 'analysis', 'polynomials']"
22,Infinitesimal calculus,Infinitesimal calculus,,"I have been reading some non-standard analysis from Keisler's book and I think it is logically consistent till now but there are criticisms against it and why isn't non-standard analysis accepted more widely, the whole book is almost similar to the concept of limit. I have read Apostol's books earlier . Then why aren't we taught only the infinitesimal calculus only and what is special about the $\epsilon - \delta$ approach ?","I have been reading some non-standard analysis from Keisler's book and I think it is logically consistent till now but there are criticisms against it and why isn't non-standard analysis accepted more widely, the whole book is almost similar to the concept of limit. I have read Apostol's books earlier . Then why aren't we taught only the infinitesimal calculus only and what is special about the $\epsilon - \delta$ approach ?",,"['calculus', 'nonstandard-analysis', 'infinitesimals']"
23,What's the integral of a constant?,What's the integral of a constant?,,If the derivative of a constant is $0$ then what is the integral of a constant? What is the integral of $0$ ?,If the derivative of a constant is then what is the integral of a constant? What is the integral of ?,0 0,"['calculus', 'integration']"
24,"Need your help with the integral $\int_0^\infty\frac{dx}{e^{\,e^{-x}} \cdot e^{\,e^{x}}}$.",Need your help with the integral .,"\int_0^\infty\frac{dx}{e^{\,e^{-x}} \cdot e^{\,e^{x}}}","Is it possible to evaluate this integral in a closed form? $$\int_0^\infty\frac{dx}{e^{\,e^{-x}} \cdot e^{\,e^{x}}}$$","Is it possible to evaluate this integral in a closed form? $$\int_0^\infty\frac{dx}{e^{\,e^{-x}} \cdot e^{\,e^{x}}}$$",,"['calculus', 'integration', 'definite-integrals', 'exponential-function', 'closed-form']"
25,How prove this integral $\int_0^{2\pi }{\frac{{{e^{\cos x}}\cos\left({\sin x}\right)}}{{p-\cos\left({y-x}\right)}}}dx$,How prove this integral,\int_0^{2\pi }{\frac{{{e^{\cos x}}\cos\left({\sin x}\right)}}{{p-\cos\left({y-x}\right)}}}dx,"show that $$ \int\limits_0^{2\pi }{\frac{{{e^{\cos x}}\cos\left({\sin x}\right)}}{{p-\cos\left({y-x}\right)}}}dx =\frac{{2\pi }}{{\sqrt{{p^2}-1}}}\exp\left({\frac{{\cos y}}{{p+\sqrt{{p^2}-1}}}}\right)\cos\left({\frac{{\sin y}}{{p+\sqrt{{p^2}-1}}}}\right);\left({p > 1}\right) $$ I think this is nice integral,But I can't show it,Thank you","show that $$ \int\limits_0^{2\pi }{\frac{{{e^{\cos x}}\cos\left({\sin x}\right)}}{{p-\cos\left({y-x}\right)}}}dx =\frac{{2\pi }}{{\sqrt{{p^2}-1}}}\exp\left({\frac{{\cos y}}{{p+\sqrt{{p^2}-1}}}}\right)\cos\left({\frac{{\sin y}}{{p+\sqrt{{p^2}-1}}}}\right);\left({p > 1}\right) $$ I think this is nice integral,But I can't show it,Thank you",,"['calculus', 'integration', 'contour-integration']"
26,Find the value of $\large i^{i^{.^{.^.}}}$,Find the value of,\large i^{i^{.^{.^.}}},Find the value of $\large i^{i^{.^{.^.}}}$ ? How should we start to solve it ? Also you can see this one if it helps. Thanks,Find the value of $\large i^{i^{.^{.^.}}}$ ? How should we start to solve it ? Also you can see this one if it helps. Thanks,,"['calculus', 'complex-analysis']"
27,Evaluate the double integral,Evaluate the double integral,,"$$\int_0^3 \int_{-\sqrt{9-x^2}}^{\sqrt{9-x^2}} (x^2+y^2+\sin(\pi(x^2+y^2)))\,dy\,dx$$ *sorry if the mathjax is off, I'm new at it. Anyways, I can use the properties of double integrals to make it $$\int_0^3 \int_{-\sqrt{9-x^2}}^{\sqrt{9-x^2}} x^2+y^2 \,dy\,dx + \int_0^3 \int_{-\sqrt{9-x^2}}^{\sqrt{9-x^2}} \sin(\pi(x^2+y^2))\,dy\,dx$$ From there I can solve the first double integral expression but I'm not sure on the second double integral expression: $$\int_0^3 \int_{-\sqrt{9-x^2}}^{\sqrt{9-x^2}} \sin(\pi(x^2+y^2))\,dy\,dx$$ A nudge in the right direction would be appreciated on how to solve this second expression.","$$\int_0^3 \int_{-\sqrt{9-x^2}}^{\sqrt{9-x^2}} (x^2+y^2+\sin(\pi(x^2+y^2)))\,dy\,dx$$ *sorry if the mathjax is off, I'm new at it. Anyways, I can use the properties of double integrals to make it $$\int_0^3 \int_{-\sqrt{9-x^2}}^{\sqrt{9-x^2}} x^2+y^2 \,dy\,dx + \int_0^3 \int_{-\sqrt{9-x^2}}^{\sqrt{9-x^2}} \sin(\pi(x^2+y^2))\,dy\,dx$$ From there I can solve the first double integral expression but I'm not sure on the second double integral expression: $$\int_0^3 \int_{-\sqrt{9-x^2}}^{\sqrt{9-x^2}} \sin(\pi(x^2+y^2))\,dy\,dx$$ A nudge in the right direction would be appreciated on how to solve this second expression.",,['calculus']
28,"Multi variable integral : $\int_0^1 \int_\sqrt{y}^1 \sqrt{x^3+1} \, dx \, dy$",Multi variable integral :,"\int_0^1 \int_\sqrt{y}^1 \sqrt{x^3+1} \, dx \, dy","$$\int_0^1 \int_\sqrt{y}^1 \sqrt{x^3+1} \, dx \, dy$$ Here is my problem in my workbook. If I solve this problem by definition, that find integral for $x$, after that solve for $y$. so $\int_\sqrt{y}^1 \sqrt{x^3+1} \, dx$ is so complicate. I have used Maple, but the result still long and complicate that I cannot use it to find integral for $y$. Thanks :)","$$\int_0^1 \int_\sqrt{y}^1 \sqrt{x^3+1} \, dx \, dy$$ Here is my problem in my workbook. If I solve this problem by definition, that find integral for $x$, after that solve for $y$. so $\int_\sqrt{y}^1 \sqrt{x^3+1} \, dx$ is so complicate. I have used Maple, but the result still long and complicate that I cannot use it to find integral for $y$. Thanks :)",,"['calculus', 'integration']"
29,An integral about Bessel function,An integral about Bessel function,,"Is there somebody who knows the solution for the integral $$\int_0^\infty\frac{J^3_1(ax)J_0(bx)}{x^2} dx$$ where $a>0,b>0$ and $J(\cdot)$ the bessel function of the first kind with integer order? Reference, or solution from computer programs all are welcome. Thanks!","Is there somebody who knows the solution for the integral $$\int_0^\infty\frac{J^3_1(ax)J_0(bx)}{x^2} dx$$ where $a>0,b>0$ and $J(\cdot)$ the bessel function of the first kind with integer order? Reference, or solution from computer programs all are welcome. Thanks!",,"['calculus', 'integration', 'special-functions']"
30,Trig substitution integration of $\int1/(x^2\sqrt{x^2 - 9}) dx$ - stuck on a problem,Trig substitution integration of  - stuck on a problem,\int1/(x^2\sqrt{x^2 - 9}) dx,"I am getting stuck on this trig substitution problem. $$\int\frac1{x^2\sqrt{x^2 - 9}}~\mathrm dx.$$ $$x = 3 \sec\theta,\qquad\theta = \sec^{-1} \sqrt{\frac{x^2}{9}},\qquad\mathrm dx = \sec\theta\tan\theta\ \mathrm d\theta$$ I can get to here, but I don't know how to finish it (perhaps I have made a mistake before this point?) $$\int\frac{3\sec\theta\tan\theta}{9\sec^2\theta(3\sec\theta -3)}~\mathrm d\theta.$$ If anyone could help from here, I'd appreciate it. Thanks.","I am getting stuck on this trig substitution problem. $$\int\frac1{x^2\sqrt{x^2 - 9}}~\mathrm dx.$$ $$x = 3 \sec\theta,\qquad\theta = \sec^{-1} \sqrt{\frac{x^2}{9}},\qquad\mathrm dx = \sec\theta\tan\theta\ \mathrm d\theta$$ I can get to here, but I don't know how to finish it (perhaps I have made a mistake before this point?) $$\int\frac{3\sec\theta\tan\theta}{9\sec^2\theta(3\sec\theta -3)}~\mathrm d\theta.$$ If anyone could help from here, I'd appreciate it. Thanks.",,"['calculus', 'integration', 'indefinite-integrals']"
31,What is the formal definition of a one sided limit?,What is the formal definition of a one sided limit?,,"I'm looking for the formal definition of $\displaystyle \lim_{x \to a^+}f(x) = L$ and $\displaystyle\lim_{x \to a^-}g(x) = M$ I took a guess at it intuitively, but I need to make sure this is correct: $\displaystyle\lim_{x \to a^+} f(x) = L$ if and only if: For any $\epsilon > 0$ there is a $\delta >0$ so that for any $x$,          if $x-a<\delta$          then $f(x)-L < \epsilon$ $\displaystyle\lim_{x \to a^-}g(x) = M$ if and only if: For any $\epsilon > 0$ there is a $\delta>0$ so that for any $x$,           if $a-x<\delta$          then $M-g(x)<\epsilon$","I'm looking for the formal definition of $\displaystyle \lim_{x \to a^+}f(x) = L$ and $\displaystyle\lim_{x \to a^-}g(x) = M$ I took a guess at it intuitively, but I need to make sure this is correct: $\displaystyle\lim_{x \to a^+} f(x) = L$ if and only if: For any $\epsilon > 0$ there is a $\delta >0$ so that for any $x$,          if $x-a<\delta$          then $f(x)-L < \epsilon$ $\displaystyle\lim_{x \to a^-}g(x) = M$ if and only if: For any $\epsilon > 0$ there is a $\delta>0$ so that for any $x$,           if $a-x<\delta$          then $M-g(x)<\epsilon$",,"['calculus', 'limits', 'definition']"
32,How to prove this inequality without use of computers?,How to prove this inequality without use of computers?,,"With help from Maple, I got  $$\left(\frac{ax+by+cz}{x-y}\right)^2+\left(\frac{ay+bz+cx}{y-z}\right)^2+\left(\frac{az+bx+cy}{z-x}\right)^2-(c-a)^2-(c-b)^2$$ equal to $$\frac{(c(x^3+y^3+z^3)+(a-c)(x^2y+y^2z+z^2x)+(b-c)(x^2z+y^2x+z^2y)-3(a+b-c)xyz)^2}{(x-y)^2(y-z)^2(x-z)^2}$$ which of course is $\ge 0$. But with no help from a computer algebra, how would one prove:$$\left(\frac{ax+by+cz}{x-y}\right)^2+\left(\frac{ay+bz+cx}{y-z}\right)^2+\left(\frac{az+bx+cy}{z-x}\right)^2\ge (c-a)^2+(c-b)^2 ?$$","With help from Maple, I got  $$\left(\frac{ax+by+cz}{x-y}\right)^2+\left(\frac{ay+bz+cx}{y-z}\right)^2+\left(\frac{az+bx+cy}{z-x}\right)^2-(c-a)^2-(c-b)^2$$ equal to $$\frac{(c(x^3+y^3+z^3)+(a-c)(x^2y+y^2z+z^2x)+(b-c)(x^2z+y^2x+z^2y)-3(a+b-c)xyz)^2}{(x-y)^2(y-z)^2(x-z)^2}$$ which of course is $\ge 0$. But with no help from a computer algebra, how would one prove:$$\left(\frac{ax+by+cz}{x-y}\right)^2+\left(\frac{ay+bz+cx}{y-z}\right)^2+\left(\frac{az+bx+cy}{z-x}\right)^2\ge (c-a)^2+(c-b)^2 ?$$",,"['calculus', 'inequality', 'contest-math']"
33,Is the notion of density really needed to define integration on nonorientable manifolds?,Is the notion of density really needed to define integration on nonorientable manifolds?,,"I am trying to understand, in as simple terms as possible: How to define integration for non-orientable manifolds, and why it is impossible to do so using only differential forms. In particular, I've seen some discussion of using ""densities"" instead of $n$-forms for integration, but am not really clear on why densities are required.  In other words, is it really impossible to define integration on nonorientable manifolds using forms alone? I am of course aware that any $n$-form must vanish somewhere on a nonorientable manifold, so we cannot find a volume form, hence cannot use the standard definition of integration.  I think the reason I'm not finding this answer satisfying is that it is a bit tautological: we can't define integration with respect to volume forms because there are no volume forms. But why must we define integration with respect to a (global) volume form in the first place? Is there really no other way to do it using locally-defined forms? Thinking of a manifold as a collection of local charts is common in geometry, and I'm having trouble understanding why this approach doesn't work in the case of integration.","I am trying to understand, in as simple terms as possible: How to define integration for non-orientable manifolds, and why it is impossible to do so using only differential forms. In particular, I've seen some discussion of using ""densities"" instead of $n$-forms for integration, but am not really clear on why densities are required.  In other words, is it really impossible to define integration on nonorientable manifolds using forms alone? I am of course aware that any $n$-form must vanish somewhere on a nonorientable manifold, so we cannot find a volume form, hence cannot use the standard definition of integration.  I think the reason I'm not finding this answer satisfying is that it is a bit tautological: we can't define integration with respect to volume forms because there are no volume forms. But why must we define integration with respect to a (global) volume form in the first place? Is there really no other way to do it using locally-defined forms? Thinking of a manifold as a collection of local charts is common in geometry, and I'm having trouble understanding why this approach doesn't work in the case of integration.",,"['calculus', 'differential-geometry', 'differential-topology']"
34,"How to calculate $\int _0^1 \int _0^1\left(\frac{1}{1-xy} \ln (1-x)\ln (1-y)\right) \,dxdy$",How to calculate,"\int _0^1 \int _0^1\left(\frac{1}{1-xy} \ln (1-x)\ln (1-y)\right) \,dxdy","Let us calculate the sum $$ \displaystyle{\sum_{n=1}^{+\infty}\left(\frac{H_{n}}{n}\right)^2}, $$ where $\displaystyle{H_{n}=1+\frac{1}{2}+\cdots+\frac{1}{n}}$ the $n$ -th harmonic number. My try The main idea because the actions are many. However, we go directly to the sum without the individual decays. $$ \begin{split} {\left(\frac{H_{n}}{n}\right)^2} =\left(\frac{H_{n}}{n}\right) \left(\frac{H_{n}}{n}\right) & = \left(-\int _0^1 x^{n-1}\ln (1-x) dx\right) \left(-\int _0^1 y^{n-1} \ln (1-y)dy\right)\\ & =\int _0^1 \int _0^1 x^{n-1}y^{n-1} \ln (1-x)\ln (1-y) dxdy \end{split}$$ So the sum sought, with transfer within the integral, is $$ \begin{split} \int _0^1 \int _0^1 &\left( \sum_{n=1}^{\infty}x^{n-1}y^{n-1} \ln (1-x)\ln (1-y)\right) \,dxdy \\ &=  \int _0^1 \int _0^1 \left(\frac{1}{1-xy} \ln (1-x)\ln (1-y)\right) \,dxdy \end{split} $$ My main question is how to evaluate the final integral, not the main question (sum).","Let us calculate the sum where the -th harmonic number. My try The main idea because the actions are many. However, we go directly to the sum without the individual decays. So the sum sought, with transfer within the integral, is My main question is how to evaluate the final integral, not the main question (sum).","
\displaystyle{\sum_{n=1}^{+\infty}\left(\frac{H_{n}}{n}\right)^2},
 \displaystyle{H_{n}=1+\frac{1}{2}+\cdots+\frac{1}{n}} n 
\begin{split}
{\left(\frac{H_{n}}{n}\right)^2} =\left(\frac{H_{n}}{n}\right) \left(\frac{H_{n}}{n}\right) & = \left(-\int _0^1 x^{n-1}\ln (1-x) dx\right) \left(-\int _0^1 y^{n-1} \ln (1-y)dy\right)\\
& =\int _0^1 \int _0^1 x^{n-1}y^{n-1} \ln (1-x)\ln (1-y) dxdy
\end{split} 
\begin{split}
\int _0^1 \int _0^1 &\left( \sum_{n=1}^{\infty}x^{n-1}y^{n-1} \ln (1-x)\ln (1-y)\right) \,dxdy \\
&= 
\int _0^1 \int _0^1 \left(\frac{1}{1-xy} \ln (1-x)\ln (1-y)\right) \,dxdy
\end{split}
","['calculus', 'integration', 'multivariable-calculus', 'definite-integrals', 'summation']"
35,"Assume $\int_0^\infty f(x)dx$ converges. Prove there exists $\xi\in[1,\infty)$ such that $\int_1^\infty \frac{f(x)}x dx=\int_1^\xi f(x)dx$.",Assume  converges. Prove there exists  such that .,"\int_0^\infty f(x)dx \xi\in[1,\infty) \int_1^\infty \frac{f(x)}x dx=\int_1^\xi f(x)dx","Assume $\int_0^\infty f(x)dx$ converges. Prove there exists $\xi\in[1,\infty)$ such that $$\int_1^\infty \frac{f(x)}x dx=\int_1^\xi f(x)dx.$$ I used various mean value theorems but couldn't get the desired result. Another attempt beyond these is: Let $\displaystyle F(x)=\int_1^x f(t)dt$ , then \begin{align*} \int_1^\infty \frac{f(x)}x dx=\int_1^\infty \frac1x ~dF(x)&=\left.\frac {F(x)}x\right|_1^\infty-\int_1^\infty F(x) ~d\frac1x\\ &=0-0+\int_1^\infty \frac {F(x)}{x^2}dx\\ &=\int_1^\infty \frac {F(x)}{x^2}dx. \end{align*} How to analyze next? Any help would be appreciated!","Assume converges. Prove there exists such that I used various mean value theorems but couldn't get the desired result. Another attempt beyond these is: Let , then How to analyze next? Any help would be appreciated!","\int_0^\infty f(x)dx \xi\in[1,\infty) \int_1^\infty \frac{f(x)}x dx=\int_1^\xi f(x)dx. \displaystyle F(x)=\int_1^x f(t)dt \begin{align*}
\int_1^\infty \frac{f(x)}x dx=\int_1^\infty \frac1x ~dF(x)&=\left.\frac {F(x)}x\right|_1^\infty-\int_1^\infty F(x) ~d\frac1x\\
&=0-0+\int_1^\infty \frac {F(x)}{x^2}dx\\
&=\int_1^\infty \frac {F(x)}{x^2}dx.
\end{align*}","['calculus', 'integration', 'mean-value-theorem']"
36,Evaluate the integral $\int\limits_0^\infty {{{\left( {\frac{e}{x}} \right)}^x}\Gamma \left( x \right)\sin \left( {2\pi x} \right)dx}$,Evaluate the integral,\int\limits_0^\infty {{{\left( {\frac{e}{x}} \right)}^x}\Gamma \left( x \right)\sin \left( {2\pi x} \right)dx},I found this integral on Instagram $$I = \int\limits_0^\infty  {{{\left( {\frac{e}{x}} \right)}^x}\Gamma \left( x \right)\sin \left( {2\pi x} \right)dx}$$ My first try is: $$\begin{array}{l} \displaystyle I = \int\limits_0^\infty  {{{\left( {\frac{e}{x}} \right)}^x}\Gamma \left( x \right)\sin \left( {2\pi x} \right)dx}  = \sum\limits_{n = 0}^\infty  {\int\limits_n^{n + 1} {{{\left( {\frac{e}{x}} \right)}^x}\Gamma \left( x \right)\sin \left( {2\pi x} \right)dx} } \\ \displaystyle {\rm{Let}}:t = x - n \Rightarrow dt = dx \Rightarrow I = \sum\limits_{n = 0}^\infty  {\int\limits_0^1 {{{\left( {\frac{e}{{t + n}}} \right)}^{t + n}}\Gamma \left( {t + n} \right)\sin \left( {2\pi t} \right)dt} }  \end{array}$$ But I don't know how to process further from this step. May I ask for some advices? Thank you very much.,I found this integral on Instagram My first try is: But I don't know how to process further from this step. May I ask for some advices? Thank you very much.,"I = \int\limits_0^\infty  {{{\left( {\frac{e}{x}} \right)}^x}\Gamma \left( x \right)\sin \left( {2\pi x} \right)dx} \begin{array}{l}
\displaystyle I = \int\limits_0^\infty  {{{\left( {\frac{e}{x}} \right)}^x}\Gamma \left( x \right)\sin \left( {2\pi x} \right)dx}  = \sum\limits_{n = 0}^\infty  {\int\limits_n^{n + 1} {{{\left( {\frac{e}{x}} \right)}^x}\Gamma \left( x \right)\sin \left( {2\pi x} \right)dx} } \\
\displaystyle {\rm{Let}}:t = x - n \Rightarrow dt = dx \Rightarrow I = \sum\limits_{n = 0}^\infty  {\int\limits_0^1 {{{\left( {\frac{e}{{t + n}}} \right)}^{t + n}}\Gamma \left( {t + n} \right)\sin \left( {2\pi t} \right)dt} } 
\end{array}","['calculus', 'integration']"
37,What is the formal definition of a continuous function?,What is the formal definition of a continuous function?,,"Thomas' Calculus 14th Edition gives the following definition of a continuous function: We define a continuous function to be one that is continuous at every point in its domain. As an example, it declares the function f(x)=1/x a continuous function: The function ƒ(x) = 1/x (Figure 2.41) is a continuous function because it is continuous at every point of its domain. The point x = 0 is not in the domain of the function ƒ, so ƒ is not continuous on any interval containing x = 0. Moreover, there is no way to extend ƒ to a new function that is defined and continuous at x = 0. The function ƒ does not have a removable discontinuity at x = 0. On the other hand, this document I found on the MIT Math portal, has this to say about that same function 1/x: The function 1/x is continuous on (0, ∞) and on (−∞, 0), i.e., for x > 0 and for x < 0, in other words, at every point in its domain. However, it is not a continuous function since its domain is not an interval. It has a single point of discontinuity, namely x = 0, and it has an infinite discontinuity there. Unless I'm misreading something here, these two sources are in direct contradiction with each other. So my questions are: is f(x)=1/x a continuous or discontinuous function, and what is the generally accepted formal definition of an overall continuous function?","Thomas' Calculus 14th Edition gives the following definition of a continuous function: We define a continuous function to be one that is continuous at every point in its domain. As an example, it declares the function f(x)=1/x a continuous function: The function ƒ(x) = 1/x (Figure 2.41) is a continuous function because it is continuous at every point of its domain. The point x = 0 is not in the domain of the function ƒ, so ƒ is not continuous on any interval containing x = 0. Moreover, there is no way to extend ƒ to a new function that is defined and continuous at x = 0. The function ƒ does not have a removable discontinuity at x = 0. On the other hand, this document I found on the MIT Math portal, has this to say about that same function 1/x: The function 1/x is continuous on (0, ∞) and on (−∞, 0), i.e., for x > 0 and for x < 0, in other words, at every point in its domain. However, it is not a continuous function since its domain is not an interval. It has a single point of discontinuity, namely x = 0, and it has an infinite discontinuity there. Unless I'm misreading something here, these two sources are in direct contradiction with each other. So my questions are: is f(x)=1/x a continuous or discontinuous function, and what is the generally accepted formal definition of an overall continuous function?",,"['calculus', 'limits', 'functions']"
38,$\epsilon$-$\delta$ proof of $\lim_{x \to 3} x^2 = 9$,- proof of,\epsilon \delta \lim_{x \to 3} x^2 = 9,"I've been learning about $\epsilon$ - $\delta$ proofs and attempted to come up with my own proof that $$ \lim_{x \to 3} x^2 = 9 $$ exists (I did use some help from some textbooks). Is my proof valid and free of redundancies? My proof: Scratch work: Written formally, this is: $$ \forall \epsilon > 0, \exists \delta > 0 \text{ s.t. } 0 < |x - 3| < \delta \implies |x^2 - 9| < \epsilon $$ We start off by simplifying the conclusion of the implication: $$ |x^2 - 9| = |x + 3|\cdot  |x - 3| $$ Now we have $|x + 3| \cdot |x - 3| < \epsilon$ . Because $|x - 3| < \delta$ , we have $|x - 3| \cdot |x + 3|  < \delta |x + 3|$ . In an attempt to find a suitable $\delta$ , we let $\delta|x + 3| < \epsilon$ . Solving for $\delta$ , we get $\delta < \frac{\epsilon}{|x + 3|}$ . There is a problem though: $\delta$ is defined in terms of $\epsilon$ and the randomly chose point $x$ . The definition of $\delta$ can only depend on $\epsilon$ . To get around this problem, we will have to estimate the size of $|x + 3|$ . We start of by assuming that $\delta < 1$ , which implies that $|x - 3| < 1$ . Solving for $x + 3$ , we get $5 < x + 3 < 7$ . We now know that $|x + 3| < 7$ when $\delta < 1$ .  Since $|x^2 - 9| < \delta|x + 3|$ and $|x + 3| < 7$ (if $\delta < 1$ ), we have $|x^2 - 9| < 7 \delta$ . In an attempt to find a suitable $\delta$ , we let $7 \delta < \epsilon$ . Solving for $\delta$ , we get $\delta < \frac{\epsilon}{7}$ when $\delta < 1$ . So we have now deduced two restrictions: $\delta < 1$ and $\delta < \frac{\epsilon}{7}$ . To satisfy both restrictions, we let $\delta < \min \left\{ 1, \frac{\epsilon}{7} \right\}$ . When $\epsilon > 7$ , then $\delta < 1$ and when $\epsilon < 7$ , we have $\delta < \frac{\epsilon}{7}$ . We can now write up the proof. Proof: For every $\epsilon > 0$ there exists a $0 < \delta < \min \left\{ 1, \frac{\epsilon}{7} \right\}$ such that $0 < |x - 3| < \delta \implies |x^2 - 9| < \epsilon$ . There are two things that can happen: $\delta < 1$ and $\delta < \frac{\epsilon}{7}$ . Case 1 - $\delta < 1$ : The implication's hypothesis is $0 < |x - 3| < \delta$ . Multiply both sides by $|x + 3|$ to get $0 < |x^2 - 9| < |x + 3| \delta$ . Because $\delta < 1$ , we know (from our scratchwork) that $|x + 3| < 7$ and $\delta$ is also less that $\frac{\epsilon}{7}$ . This means that $0 < |x^2 - 9| < |x + 3| \delta < 7 \delta < [7 \frac{\epsilon}{7} = \epsilon]$ . Case 2 - $\delta < \frac{\epsilon}{7}$ : For $\delta$ to be less that $\frac{\epsilon}{7}$ , we must have that $\frac{\epsilon}{7} < 1$ (as per the definition of $\min$ ). Because $\delta < \frac{\epsilon}{7}$ and $\frac{\epsilon}{7} < 1$ we have $\delta < 1$ . Look to Case 1 for what follows after. $\square{}$","I've been learning about - proofs and attempted to come up with my own proof that exists (I did use some help from some textbooks). Is my proof valid and free of redundancies? My proof: Scratch work: Written formally, this is: We start off by simplifying the conclusion of the implication: Now we have . Because , we have . In an attempt to find a suitable , we let . Solving for , we get . There is a problem though: is defined in terms of and the randomly chose point . The definition of can only depend on . To get around this problem, we will have to estimate the size of . We start of by assuming that , which implies that . Solving for , we get . We now know that when .  Since and (if ), we have . In an attempt to find a suitable , we let . Solving for , we get when . So we have now deduced two restrictions: and . To satisfy both restrictions, we let . When , then and when , we have . We can now write up the proof. Proof: For every there exists a such that . There are two things that can happen: and . Case 1 - : The implication's hypothesis is . Multiply both sides by to get . Because , we know (from our scratchwork) that and is also less that . This means that . Case 2 - : For to be less that , we must have that (as per the definition of ). Because and we have . Look to Case 1 for what follows after.","\epsilon \delta 
\lim_{x \to 3} x^2 = 9
 
\forall \epsilon > 0, \exists \delta > 0 \text{ s.t. } 0 < |x - 3| < \delta \implies |x^2 - 9| < \epsilon
 
|x^2 - 9| = |x + 3|\cdot  |x - 3|
 |x + 3| \cdot |x - 3| < \epsilon |x - 3| < \delta |x - 3| \cdot |x + 3|  < \delta |x + 3| \delta \delta|x + 3| < \epsilon \delta \delta < \frac{\epsilon}{|x + 3|} \delta \epsilon x \delta \epsilon |x + 3| \delta < 1 |x - 3| < 1 x + 3 5 < x + 3 < 7 |x + 3| < 7 \delta < 1 |x^2 - 9| < \delta|x + 3| |x + 3| < 7 \delta < 1 |x^2 - 9| < 7 \delta \delta 7 \delta < \epsilon \delta \delta < \frac{\epsilon}{7} \delta < 1 \delta < 1 \delta < \frac{\epsilon}{7} \delta < \min \left\{ 1, \frac{\epsilon}{7} \right\} \epsilon > 7 \delta < 1 \epsilon < 7 \delta < \frac{\epsilon}{7} \epsilon > 0 0 < \delta < \min \left\{ 1, \frac{\epsilon}{7} \right\} 0 < |x - 3| < \delta \implies |x^2 - 9| < \epsilon \delta < 1 \delta < \frac{\epsilon}{7} \delta < 1 0 < |x - 3| < \delta |x + 3| 0 < |x^2 - 9| < |x + 3| \delta \delta < 1 |x + 3| < 7 \delta \frac{\epsilon}{7} 0 < |x^2 - 9| < |x + 3| \delta < 7 \delta < [7 \frac{\epsilon}{7} = \epsilon] \delta < \frac{\epsilon}{7} \delta \frac{\epsilon}{7} \frac{\epsilon}{7} < 1 \min \delta < \frac{\epsilon}{7} \frac{\epsilon}{7} < 1 \delta < 1 \square{}","['calculus', 'limits', 'solution-verification', 'quadratics', 'epsilon-delta']"
39,Alternative proofs of convergence of geometric series,Alternative proofs of convergence of geometric series,,"The usual proof for the convergence of a geometric series of ratio $C: |C|\in [0,1)$ makes use of the formula $$\sum_{0\leq k \leq n} C^k = \frac{1-C^{n+1}}{1-C}.$$ I'm looking for alternative ways to prove it. The motivation for this is that, if someone who never saw this formula tried to prove the geometric series converges might have a hard time, unless maybe there are other, perhaps more insightful ways to prove it.","The usual proof for the convergence of a geometric series of ratio makes use of the formula I'm looking for alternative ways to prove it. The motivation for this is that, if someone who never saw this formula tried to prove the geometric series converges might have a hard time, unless maybe there are other, perhaps more insightful ways to prove it.","C: |C|\in [0,1) \sum_{0\leq k \leq n} C^k = \frac{1-C^{n+1}}{1-C}.","['calculus', 'sequences-and-series', 'soft-question', 'geometric-series']"
40,A very interesting question: intersection point of $x^y=y^x$,A very interesting question: intersection point of,x^y=y^x,"I've been investigating the Cartesian graph of $x^y=y^x$ . Obviously, part of the graph is comprised of the line $y=x$ but there is also a curve that is symmetrical about the line $y=x$ . (We can prove this symmetry by noting that the function $x^y=y^x$ is self-inverse; all self-inverse functions are symmetrical about the line $y=x$ .) An image of the graph is shown below: I decided to find the intersection point and arrived at an intriguing result: the intersection point between the two curves is at $(e,e)$ . The following is my method: If the gradient of the line $y=x$ is $1$ , the gradient of the curve at the intersection point must be $-1$ as it's a normal to the line (as it's symmetrical about the line). This means that at that point $\frac{dy}{dx}=-1$ . Now to find $\frac{dy}{dx}$ . We have $x^y=y^x$ . I then used a very powerful tehnique for differentiating these sorts of functions. We know that eg $$x^y=e^{\ln{x^y}}=e^{y\ln{x}}$$ and $$\frac{d}{dx}e^{f(x)}=f'(x)e^{f(x)}$$ Applying it to our function $x^y=y^x$ and using implicit differentiation and the product rule gives us: $$(\frac{dy}{dx}\times \ln x +\frac{y}{x})x^y=(\ln y +\frac{dy}{dx}\times \frac{x}{y})y^x$$ So $$\frac{dy}{dx}x^y\ln x +yx^{y-1}=\frac{dy}{dx}xy^{x-1}+y^x \ln y$$ Extensively rearranging gives: $$\frac{dy}{dx}=\frac{y^x \ln y -yx^{y-1}}{x^y \ln x -xy^{x-1}}$$ Let $\frac{dy}{dx}=-1$ : $$y^x \ln y -yx^{y-1}=xy^{x-1}-x^y\ln x$$ But we know $x=y$ since we are at the intersection point with the line $y=x$ : $$x^x \ln x -x^x=x^x-x^x\ln x$$ So $2x^x \ln x -2x^x=0$ $$2x^x(\ln x -1)=0$$ This means either $2x^x=0$ or $\ln x -1=0$ but we know $x^x$ is always greater than $0$ so $\ln x =1$ , leaving us with: $$x=y=e$$ So I have my answer, but is there any other method of getting it? I have heard there is. Any help will be very welcome. Thanks in advance","I've been investigating the Cartesian graph of . Obviously, part of the graph is comprised of the line but there is also a curve that is symmetrical about the line . (We can prove this symmetry by noting that the function is self-inverse; all self-inverse functions are symmetrical about the line .) An image of the graph is shown below: I decided to find the intersection point and arrived at an intriguing result: the intersection point between the two curves is at . The following is my method: If the gradient of the line is , the gradient of the curve at the intersection point must be as it's a normal to the line (as it's symmetrical about the line). This means that at that point . Now to find . We have . I then used a very powerful tehnique for differentiating these sorts of functions. We know that eg and Applying it to our function and using implicit differentiation and the product rule gives us: So Extensively rearranging gives: Let : But we know since we are at the intersection point with the line : So This means either or but we know is always greater than so , leaving us with: So I have my answer, but is there any other method of getting it? I have heard there is. Any help will be very welcome. Thanks in advance","x^y=y^x y=x y=x x^y=y^x y=x (e,e) y=x 1 -1 \frac{dy}{dx}=-1 \frac{dy}{dx} x^y=y^x x^y=e^{\ln{x^y}}=e^{y\ln{x}} \frac{d}{dx}e^{f(x)}=f'(x)e^{f(x)} x^y=y^x (\frac{dy}{dx}\times \ln x +\frac{y}{x})x^y=(\ln y +\frac{dy}{dx}\times \frac{x}{y})y^x \frac{dy}{dx}x^y\ln x +yx^{y-1}=\frac{dy}{dx}xy^{x-1}+y^x \ln y \frac{dy}{dx}=\frac{y^x \ln y -yx^{y-1}}{x^y \ln x -xy^{x-1}} \frac{dy}{dx}=-1 y^x \ln y -yx^{y-1}=xy^{x-1}-x^y\ln x x=y y=x x^x \ln x -x^x=x^x-x^x\ln x 2x^x \ln x -2x^x=0 2x^x(\ln x -1)=0 2x^x=0 \ln x -1=0 x^x 0 \ln x =1 x=y=e","['calculus', 'limits', 'exponential-function', 'alternative-proof', 'exponential-diophantine-equations']"
41,"What does it mean for a vector field to be ""along"" $\partial M$? I think ""along"" is a generalization of ""on"".","What does it mean for a vector field to be ""along"" ? I think ""along"" is a generalization of ""on"".",\partial M,"My book is An Introduction to Manifolds by Loring W. Tu. The following is an entire subsection (Subsection 22.5) of the section that introduces manifolds with boundary (Section 22, Manifolds with Boundary). Note: I believe that all manifolds with or without boundary referred in this subsection have unique dimensions by some convention (either it's implicit, or it's explicit a I missed it) in the section (The convention of the book is that manifolds with or without boundary can be locally diffeomorphic to different $\mathbb R^n$ 's. See here and here ). According to an errata by Ehssan Khanmohammadi , the only erratum to be made in this subsection is that $c((0,\varepsilon[) \subset M^\circ$ should be changed to $c(]0,\varepsilon[) \subset M^\circ$ . I still have several concerns about this subsection. What exactly is a vector field along $\partial M$ , and what is its domain? Choice 1: It is a mapping whose domain is $\partial M$ and not the whole of $M$ and much like how a manifold with boundary is not a manifold but rather a generalization of a manifold, is not a vector field on $\partial M$ but rather a generalization of a vector field on $\partial M$ , which is defined the same as a vector field on any manifold (without boundary) because $\partial M$ is a manifold (without boundary) as proved in Subsection 22.3 . The generalization is as follows: Let $X$ be a vector field on $\partial M$ . $X$ is a mapping whose domain is $\partial M$ and whose image is the tangent bundle $\cup_{p} T_p(\partial M)$ because to each $p \in \partial M$ , $X$ assigns $p$ to $X_p \in T_p(\partial M)$ . Now, $T_p(\partial M) \subseteq T_pM$ , so $X_p \in T_pM$ . Therefore, $X$ is a vector field along $\partial M$ . However, if we let $Y$ be a vector field along $\partial M$ , then for any $p \in \partial M$ , we might not have the tangent vector at $Y_p$ to be $Y_p \in T_p(\partial M)$ because we are allowed to have that $Y_p \in T_pM \setminus T_p(\partial M)$ because all we are required is that $Y_p \in T_pM$ . Therefore, $Y$ is not necessarily a vector field on $\partial M$ . Choice 2: It is a mapping whose domain is the whole of $M$ and is indeed a vector field on $M$ that has certain properties for its values at $p \in \partial M$ (such as $X_p \in T_pM$ for each $p \in \partial M$ ). I guess this would mean that $X|_{\partial M}$ isn't a vector field on $\partial M$ , which is contrary to some expectation that restrictions of vector fields on $N$ , manifolds with or without boundary to subsets $S \subseteq N$ that are manifolds with or without boundary are vector fields on $S$ or something. In this case, it seems that every vector field on $M$ is a vector field along $\partial M$ ...but conversely as well. Maybe it is a mapping whose domain is the whole of $M$ but is not necessarily a vector field on $M$ . Update: I think this is the expectation in this link. In the link, the definition of ""along"" is for a ""submanifold"" (immersed or embedded) of a manifold which I'm not sure has boundary. I think there's some notion of a ""submanifold"" of a manifold with boundary that makes $\partial M$ as ""submanifold"" of $M$ and then I guess for some reason restrictions of vector fields to ""submanifolds"" are vector fields on the submanifolds, which leads to the generalizing notion of ""along"" I think there could be a convention (like with directional derivative ) that a vector field along $\partial M$ has domain to be all of $M$ but simply satisfies the property for $p \in \partial M$ . See the ""global vector field"" in the link above : I think Lemma 5 in the link is Tu's Proposition 22.10. Also Lee's Problem 8-4 , asked here Choice 3: Somehow there's an equivalence of being defined on $M$ and only on $\partial M$ with some kind of extension. Choice 4: Other I think the next questions shed some light on the answer to this question. For the local expression of $X$ , a vector field along $\partial M$ is the following understanding correct? Asked here . Is this a correct understanding of the smoothness definition? Asked here . $ \ $ Despite the title of the subsection, I don't think there's a definition for outward-pointing vector fields. What is it exactly? Asked here In the proof of Proposition 22.10, is it understood that we cover $\partial M$ by restrictions of the $(U_{\alpha}, x^1_{\alpha}, ..., x^n_{\alpha})$ 's like in questions 2 and 3? Asked here Actually, based on Lee's Problem 8-4 , asked about here , I think we can interpret Proposition 22.10 without the concept of ""along"" as follows: Asked here","My book is An Introduction to Manifolds by Loring W. Tu. The following is an entire subsection (Subsection 22.5) of the section that introduces manifolds with boundary (Section 22, Manifolds with Boundary). Note: I believe that all manifolds with or without boundary referred in this subsection have unique dimensions by some convention (either it's implicit, or it's explicit a I missed it) in the section (The convention of the book is that manifolds with or without boundary can be locally diffeomorphic to different 's. See here and here ). According to an errata by Ehssan Khanmohammadi , the only erratum to be made in this subsection is that should be changed to . I still have several concerns about this subsection. What exactly is a vector field along , and what is its domain? Choice 1: It is a mapping whose domain is and not the whole of and much like how a manifold with boundary is not a manifold but rather a generalization of a manifold, is not a vector field on but rather a generalization of a vector field on , which is defined the same as a vector field on any manifold (without boundary) because is a manifold (without boundary) as proved in Subsection 22.3 . The generalization is as follows: Let be a vector field on . is a mapping whose domain is and whose image is the tangent bundle because to each , assigns to . Now, , so . Therefore, is a vector field along . However, if we let be a vector field along , then for any , we might not have the tangent vector at to be because we are allowed to have that because all we are required is that . Therefore, is not necessarily a vector field on . Choice 2: It is a mapping whose domain is the whole of and is indeed a vector field on that has certain properties for its values at (such as for each ). I guess this would mean that isn't a vector field on , which is contrary to some expectation that restrictions of vector fields on , manifolds with or without boundary to subsets that are manifolds with or without boundary are vector fields on or something. In this case, it seems that every vector field on is a vector field along ...but conversely as well. Maybe it is a mapping whose domain is the whole of but is not necessarily a vector field on . Update: I think this is the expectation in this link. In the link, the definition of ""along"" is for a ""submanifold"" (immersed or embedded) of a manifold which I'm not sure has boundary. I think there's some notion of a ""submanifold"" of a manifold with boundary that makes as ""submanifold"" of and then I guess for some reason restrictions of vector fields to ""submanifolds"" are vector fields on the submanifolds, which leads to the generalizing notion of ""along"" I think there could be a convention (like with directional derivative ) that a vector field along has domain to be all of but simply satisfies the property for . See the ""global vector field"" in the link above : I think Lemma 5 in the link is Tu's Proposition 22.10. Also Lee's Problem 8-4 , asked here Choice 3: Somehow there's an equivalence of being defined on and only on with some kind of extension. Choice 4: Other I think the next questions shed some light on the answer to this question. For the local expression of , a vector field along is the following understanding correct? Asked here . Is this a correct understanding of the smoothness definition? Asked here . Despite the title of the subsection, I don't think there's a definition for outward-pointing vector fields. What is it exactly? Asked here In the proof of Proposition 22.10, is it understood that we cover by restrictions of the 's like in questions 2 and 3? Asked here Actually, based on Lee's Problem 8-4 , asked about here , I think we can interpret Proposition 22.10 without the concept of ""along"" as follows: Asked here","\mathbb R^n c((0,\varepsilon[) \subset M^\circ c(]0,\varepsilon[) \subset M^\circ \partial M \partial M M \partial M \partial M \partial M X \partial M X \partial M \cup_{p} T_p(\partial M) p \in \partial M X p X_p \in T_p(\partial M) T_p(\partial M) \subseteq T_pM X_p \in T_pM X \partial M Y \partial M p \in \partial M Y_p Y_p \in T_p(\partial M) Y_p \in T_pM \setminus T_p(\partial M) Y_p \in T_pM Y \partial M M M p \in \partial M X_p \in T_pM p \in \partial M X|_{\partial M} \partial M N S \subseteq N S M \partial M M M \partial M M \partial M M p \in \partial M M \partial M X \partial M  \  \partial M (U_{\alpha}, x^1_{\alpha}, ..., x^n_{\alpha})","['calculus', 'general-topology']"
42,How to compute a Jacobian using polar coordinates?,How to compute a Jacobian using polar coordinates?,,"Consider the transformation $F$ of $\mathbb R^2\setminus\{(0,0)\}$ onto itself defined as $$ F(x, y):=\left( \frac{x}{x^2+y^2}, \frac{y}{x^2+y^2}\right).$$ Its Jacobian matrix is $$\tag{1} \begin{bmatrix} \frac{y^2-x^2}{(x^2+y^2)^2} & -\frac{2xy}{(x^2+y^2)^2} \\ -\frac{2xy}{(x^2+y^2)^2} & \frac{x^2-y^2}{(x^2+y^2)^2} \end{bmatrix},\quad \text{and its determinant equals}\ \frac{-1}{(x^2+y^2)^2}.$$ The following alternative computation is wrong at (!) and (!!), and I cannot see why. Let $\phi\colon (0, \infty)\times (-\pi, \pi)\to \mathbb R^2$ be the map $$\phi(r, \theta) =(r\cos \theta, r\sin \theta).$$ Let moreover $$\tag{2}\tilde{F}:=\phi^{-1}\circ F\circ \phi;$$ then, by an easy direct computation, $$\tilde{F}(r, \theta)=\left( \frac1r, \theta\right).$$ The Jacobian matrix of $\tilde{F}$ is, thus, $$\tag{!}\begin{bmatrix} \frac{-1}{r^2} & 0 \\ 0 & 1\end{bmatrix} , \quad \text{and its determinant equals }\ \frac{-1}{r^2}.$$ On the other hand, by (2) and by the chain rule, the Jacobian determinants of $F$ and $\tilde{F}$ are equal. We conclude that the Jacobian determinant of $F$ is $$\tag{!!} \frac{-1}{r^2}=\frac{-1}{x^2+y^2}.$$ The result (!!) is off by a factor of $r^{-2}$ from the correct one, which is given in (1). Equation (!) must also be wrong. Indeed, computing the Jacobian matrix from (2) using the chain rule, and using that $$ D\phi = \begin{bmatrix} \cos \theta & \sin \theta \\  -r\sin \theta & r\cos \theta\end{bmatrix}$$ and that $$\tag{!!!} D(\phi^{-1})= \begin{bmatrix} \frac{x}{\sqrt{x^2+y^2}} &  \frac{y}{\sqrt{x^2+y^2}} \\ -\frac{y}{x^2+y^2} & \frac{x}{x^2+y^2}\end{bmatrix},$$ I obtain the result $$ \begin{bmatrix} \frac{x}{\sqrt{x^2+y^2}} &  \frac{y}{\sqrt{x^2+y^2}} \\ -\frac{y}{x^2+y^2} & \frac{x}{x^2+y^2}\end{bmatrix} \begin{bmatrix} \frac{y^2-x^2}{(x^2+y^2)^2} & -\frac{2xy}{(x^2+y^2)^2} \\ -\frac{2xy}{(x^2+y^2)^2} & \frac{x^2-y^2}{(x^2+y^2)^2} \end{bmatrix}\begin{bmatrix} \cos\theta & -r\sin\theta \\ \sin \theta & r\cos \theta\end{bmatrix} = \begin{bmatrix} -\frac1{r^2} & 0 \\ 0 & \frac{1}{r^2}\end{bmatrix},$$ which is different from the matrix in (!), and which gives the correct determinant of $-1/r^4$ , as it should be. Can you help me spot the mistake? SOLUTION ( added at a later time ). As answerers pointed out, there is a mistake in (!!!). The correct matrix to be used is $$ D(\phi^{-1})|_{F\circ \phi(r, \theta)}  = \begin{bmatrix} \frac{\frac{x}{x^2 + y^2}}{\sqrt{\left(\frac{x}{x^2 + y^2} \right)^2+\left( \frac{y}{x^2 + y^2}\right)^2}} &  \frac{\frac{y}{x^2 + y^2}}{\sqrt{\left(\frac{x}{x^2 + y^2} \right)^2+\left( \frac{y}{x^2 + y^2}\right)^2}}  \\ -\frac{\frac{y}{x^2 + y^2}}{\left(\frac{x}{x^2 + y^2} \right)^2+\left( \frac{y}{x^2 + y^2}\right)^2}  & \frac{\frac{x}{x^2 + y^2}}{\left(\frac{x}{x^2 + y^2} \right)^2+\left( \frac{y}{x^2 + y^2}\right)^2}\end{bmatrix}  = \begin{bmatrix}\cos\theta & \sin \theta \\ - r\sin \theta & r\cos\theta \end{bmatrix}.$$ Had I used this matrix, I would have found the correct result for the Jacobian matrix of $\tilde{F}$ , which is the equation marked (!). Thus, (!) is actually correct . My fundamental misunderstanding was the assumption that, because of (2), the Jacobian determinant should be invariant for coordinate changes. This is not true; what follows from (2) is only that $$ \det D\tilde{F}|_{(r, \theta)}= \det D\phi^{-1}|_{F\circ\phi(r, \theta)}\det D\phi|_{(r, \theta)} \det DF|_{\phi(r, \theta)}. $$ The first two factors in the right-hand side need not cancel , as I erroneously thought.","Consider the transformation of onto itself defined as Its Jacobian matrix is The following alternative computation is wrong at (!) and (!!), and I cannot see why. Let be the map Let moreover then, by an easy direct computation, The Jacobian matrix of is, thus, On the other hand, by (2) and by the chain rule, the Jacobian determinants of and are equal. We conclude that the Jacobian determinant of is The result (!!) is off by a factor of from the correct one, which is given in (1). Equation (!) must also be wrong. Indeed, computing the Jacobian matrix from (2) using the chain rule, and using that and that I obtain the result which is different from the matrix in (!), and which gives the correct determinant of , as it should be. Can you help me spot the mistake? SOLUTION ( added at a later time ). As answerers pointed out, there is a mistake in (!!!). The correct matrix to be used is Had I used this matrix, I would have found the correct result for the Jacobian matrix of , which is the equation marked (!). Thus, (!) is actually correct . My fundamental misunderstanding was the assumption that, because of (2), the Jacobian determinant should be invariant for coordinate changes. This is not true; what follows from (2) is only that The first two factors in the right-hand side need not cancel , as I erroneously thought.","F \mathbb R^2\setminus\{(0,0)\} 
F(x, y):=\left( \frac{x}{x^2+y^2}, \frac{y}{x^2+y^2}\right). \tag{1}
\begin{bmatrix} \frac{y^2-x^2}{(x^2+y^2)^2} & -\frac{2xy}{(x^2+y^2)^2} \\ -\frac{2xy}{(x^2+y^2)^2} & \frac{x^2-y^2}{(x^2+y^2)^2} \end{bmatrix},\quad \text{and its determinant equals}\ \frac{-1}{(x^2+y^2)^2}. \phi\colon (0, \infty)\times (-\pi, \pi)\to \mathbb R^2 \phi(r, \theta) =(r\cos \theta, r\sin \theta). \tag{2}\tilde{F}:=\phi^{-1}\circ F\circ \phi; \tilde{F}(r, \theta)=\left( \frac1r, \theta\right). \tilde{F} \tag{!}\begin{bmatrix} \frac{-1}{r^2} & 0 \\ 0 & 1\end{bmatrix} , \quad \text{and its determinant equals }\ \frac{-1}{r^2}. F \tilde{F} F \tag{!!} \frac{-1}{r^2}=\frac{-1}{x^2+y^2}. r^{-2} 
D\phi = \begin{bmatrix} \cos \theta & \sin \theta \\ 
-r\sin \theta & r\cos \theta\end{bmatrix} \tag{!!!}
D(\phi^{-1})= \begin{bmatrix} \frac{x}{\sqrt{x^2+y^2}} &  \frac{y}{\sqrt{x^2+y^2}} \\ -\frac{y}{x^2+y^2} & \frac{x}{x^2+y^2}\end{bmatrix}, 
\begin{bmatrix} \frac{x}{\sqrt{x^2+y^2}} &  \frac{y}{\sqrt{x^2+y^2}} \\ -\frac{y}{x^2+y^2} & \frac{x}{x^2+y^2}\end{bmatrix} \begin{bmatrix} \frac{y^2-x^2}{(x^2+y^2)^2} & -\frac{2xy}{(x^2+y^2)^2} \\ -\frac{2xy}{(x^2+y^2)^2} & \frac{x^2-y^2}{(x^2+y^2)^2} \end{bmatrix}\begin{bmatrix} \cos\theta & -r\sin\theta \\ \sin \theta & r\cos \theta\end{bmatrix} = \begin{bmatrix} -\frac1{r^2} & 0 \\ 0 & \frac{1}{r^2}\end{bmatrix}, -1/r^4  D(\phi^{-1})|_{F\circ \phi(r, \theta)} 
= \begin{bmatrix} \frac{\frac{x}{x^2 + y^2}}{\sqrt{\left(\frac{x}{x^2 + y^2} \right)^2+\left( \frac{y}{x^2 + y^2}\right)^2}} &  \frac{\frac{y}{x^2 + y^2}}{\sqrt{\left(\frac{x}{x^2 + y^2} \right)^2+\left( \frac{y}{x^2 + y^2}\right)^2}}  \\ -\frac{\frac{y}{x^2 + y^2}}{\left(\frac{x}{x^2 + y^2} \right)^2+\left( \frac{y}{x^2 + y^2}\right)^2}  & \frac{\frac{x}{x^2 + y^2}}{\left(\frac{x}{x^2 + y^2} \right)^2+\left( \frac{y}{x^2 + y^2}\right)^2}\end{bmatrix}  = \begin{bmatrix}\cos\theta & \sin \theta \\ - r\sin \theta & r\cos\theta \end{bmatrix}. \tilde{F} 
\det D\tilde{F}|_{(r, \theta)}= \det D\phi^{-1}|_{F\circ\phi(r, \theta)}\det D\phi|_{(r, \theta)} \det DF|_{\phi(r, \theta)}.
","['calculus', 'multivariable-calculus', 'differential-geometry']"
43,Proving inequality using double integral,Proving inequality using double integral,,"If it's a known inequality or a duplicate - sorry. I’ve searched the question archive and elsewhere. I didn't find anything similar. Let $f$ be a positive continuous function over the interval $[a,b]$ . Prove the inequality $$ \int_{a}^{b}f(x)dx \cdot \int_{a}^{b} \frac{1}{f(x)}dx \geq(b-a)^2 $$ using double integrals. I’ve tried defining $g(x,y)=\frac{f(y)}{f(x)}$ ,  and then integrating over the square $[a,b]\times[a,b]$ , so we get $\int_{a}^{b}\int_{a}^{b}\frac{f(y)}{f(x)}dxdy=\int_{a}^{b}f(y)dy \cdot \int_{a}^{b} \frac{1}{f(x)}dx$ , and the double integral is then equal to the volume under $g(x,y)$ and over the above mentioned square. I didn't manage to prove that the volume is at least $(b-a)^2$ .","If it's a known inequality or a duplicate - sorry. I’ve searched the question archive and elsewhere. I didn't find anything similar. Let be a positive continuous function over the interval . Prove the inequality using double integrals. I’ve tried defining ,  and then integrating over the square , so we get , and the double integral is then equal to the volume under and over the above mentioned square. I didn't manage to prove that the volume is at least .","f [a,b] 
\int_{a}^{b}f(x)dx \cdot \int_{a}^{b} \frac{1}{f(x)}dx \geq(b-a)^2
 g(x,y)=\frac{f(y)}{f(x)} [a,b]\times[a,b] \int_{a}^{b}\int_{a}^{b}\frac{f(y)}{f(x)}dxdy=\int_{a}^{b}f(y)dy \cdot \int_{a}^{b} \frac{1}{f(x)}dx g(x,y) (b-a)^2","['calculus', 'integration', 'multivariable-calculus', 'definite-integrals']"
44,Why does the concept of differentiation have the meaning only on open sets?,Why does the concept of differentiation have the meaning only on open sets?,,"I am just stuck on the same problem asked in the following question . The book asks to implicitly differentiate two equations. Then it turns out the answer is ""neither of them is implicitly differentiable"". Alex M. in the previous question answers saying: ... $y = \pm x$ but this equality does not define on open set, only a finite set of points, and the concept of derivability makes no sense on such sets. Can someone explain what is going on?","I am just stuck on the same problem asked in the following question . The book asks to implicitly differentiate two equations. Then it turns out the answer is ""neither of them is implicitly differentiable"". Alex M. in the previous question answers saying: ... $y = \pm x$ but this equality does not define on open set, only a finite set of points, and the concept of derivability makes no sense on such sets. Can someone explain what is going on?",,"['calculus', 'general-topology', 'derivatives', 'implicit-differentiation']"
45,Polynomial $P(x)$ with $P(x)\in\mathbb Q$ iff $x\in \mathbb Q$ [duplicate],Polynomial  with  iff  [duplicate],P(x) P(x)\in\mathbb Q x\in \mathbb Q,"This question already has answers here : $f(x)=ax+b$ for some $a,b\in\mathbb{Q}$ if $f(\mathbb{Q})\subset\mathbb{Q}$ and $f(\mathbb{R-Q})\subseteq \mathbb{R}-\mathbb{Q}$ (2 answers) Closed 6 years ago . Find all the polynomials $P \in \mathbb{R}[X]$ such that: $P(x) \in \mathbb{Q}$ if and only if $x \in \mathbb{Q}$. I think $P(x)=x+c$ or $P(x)=-x+c$ where $c$ is some rational constant. But I have no idea approach this.","This question already has answers here : $f(x)=ax+b$ for some $a,b\in\mathbb{Q}$ if $f(\mathbb{Q})\subset\mathbb{Q}$ and $f(\mathbb{R-Q})\subseteq \mathbb{R}-\mathbb{Q}$ (2 answers) Closed 6 years ago . Find all the polynomials $P \in \mathbb{R}[X]$ such that: $P(x) \in \mathbb{Q}$ if and only if $x \in \mathbb{Q}$. I think $P(x)=x+c$ or $P(x)=-x+c$ where $c$ is some rational constant. But I have no idea approach this.",,"['calculus', 'polynomials', 'contest-math']"
46,How to evaluate the integral $\int_0^1\frac{\arctan x}{x}\frac{1-x^3}{1+x^3}dx$,How to evaluate the integral,\int_0^1\frac{\arctan x}{x}\frac{1-x^3}{1+x^3}dx,"I'm looking for the value of this integral: $$\int_0^1\frac{\arctan x}{x}\frac{1-x^3}{1+x^3}dx$$ I try to integrate it: \begin{align} I&=\int_0^1\arctan x\left(\frac{1}{x}-\frac{2x^2}{1+x^3}\right)dx\\ &=\int_0^1\frac{\arctan x}{x}dx-2\int_0^1\frac{x^2\arctan x}{1+x^3}dx\\ &=G-\frac{2}{3}\left(\arctan x\ln (1+x^3)|_0^1-\int_0^1\frac{\ln (1+x^3)}{1+x^2}dx\right)\\ &=G-\frac{\pi}{6}\ln 2+\frac{2}{3}\int_0^1\frac{\ln (1+x)+\ln (x^2-x+1)}{1+x^2}dx\\ &=G-\frac{\pi}{6}\ln 2+\frac{2}{3}\frac{\pi}{8}\ln 2+\frac{2}{3}\int_0^1\frac{\ln (x^2-x+1)}{1+x^2}dx\\ &=G-\frac{\pi}{12}\ln 2+\frac{2}{3}\int_0^1\frac{\ln (x^2-x+1)}{1+x^2}dx\\ \end{align} Where G is the Catalan's constant,but I don't know how to do it next .","I'm looking for the value of this integral: $$\int_0^1\frac{\arctan x}{x}\frac{1-x^3}{1+x^3}dx$$ I try to integrate it: \begin{align} I&=\int_0^1\arctan x\left(\frac{1}{x}-\frac{2x^2}{1+x^3}\right)dx\\ &=\int_0^1\frac{\arctan x}{x}dx-2\int_0^1\frac{x^2\arctan x}{1+x^3}dx\\ &=G-\frac{2}{3}\left(\arctan x\ln (1+x^3)|_0^1-\int_0^1\frac{\ln (1+x^3)}{1+x^2}dx\right)\\ &=G-\frac{\pi}{6}\ln 2+\frac{2}{3}\int_0^1\frac{\ln (1+x)+\ln (x^2-x+1)}{1+x^2}dx\\ &=G-\frac{\pi}{6}\ln 2+\frac{2}{3}\frac{\pi}{8}\ln 2+\frac{2}{3}\int_0^1\frac{\ln (x^2-x+1)}{1+x^2}dx\\ &=G-\frac{\pi}{12}\ln 2+\frac{2}{3}\int_0^1\frac{\ln (x^2-x+1)}{1+x^2}dx\\ \end{align} Where G is the Catalan's constant,but I don't know how to do it next .",,"['calculus', 'integration', 'definite-integrals', 'logarithms', 'trigonometric-integrals']"
47,Find $x$ and $y$ where $20!=\overline{24329020081766xy\dots}$,Find  and  where,x y 20!=\overline{24329020081766xy\dots},Find $x$ and $y$ where $20!=\overline{24329020081766xy\dots}$(without using calculator.) My attempt :I first find how many zeroes does it have: $$\left\lfloor {\frac{20}{5}} \right\rfloor=4.$$ It can be solved easily if we know that after $y$ there are only three digits then we can know:  $$y=0.$$ Then $\overline {6x}$ is divisible by $4$ which gives us: $$x=4\ \ \ \text{ or }x=8.$$ Then if we check divisiblity role of $8$ we will get that $\overline {66x}$ is divisible by $8$ that tells to us $x$ can only be $4$. Thus $$x=4.$$ But know the biggest problem is that we don't know how many digits are there after $y$. Or in a bigger amount how many digits are there in $20!$. Thanks.,Find $x$ and $y$ where $20!=\overline{24329020081766xy\dots}$(without using calculator.) My attempt :I first find how many zeroes does it have: $$\left\lfloor {\frac{20}{5}} \right\rfloor=4.$$ It can be solved easily if we know that after $y$ there are only three digits then we can know:  $$y=0.$$ Then $\overline {6x}$ is divisible by $4$ which gives us: $$x=4\ \ \ \text{ or }x=8.$$ Then if we check divisiblity role of $8$ we will get that $\overline {66x}$ is divisible by $8$ that tells to us $x$ can only be $4$. Thus $$x=4.$$ But know the biggest problem is that we don't know how many digits are there after $y$. Or in a bigger amount how many digits are there in $20!$. Thanks.,,"['calculus', 'elementary-number-theory']"
48,Just got confused with what my friend asked (paradox and fake proofs). [duplicate],Just got confused with what my friend asked (paradox and fake proofs). [duplicate],,"This question already has answers here : Where is the flaw in this ""proof"" that 1=2? (Derivative of repeated addition) (11 answers) Closed 8 years ago . Take $x^2=x+x+x+\cdots$ ($x$ times). Now differentiating both sides wrt $x$, we get: $$2x=x.$$ This means $x=0$ or $2=1$. How? Where did I go wrong?","This question already has answers here : Where is the flaw in this ""proof"" that 1=2? (Derivative of repeated addition) (11 answers) Closed 8 years ago . Take $x^2=x+x+x+\cdots$ ($x$ times). Now differentiating both sides wrt $x$, we get: $$2x=x.$$ This means $x=0$ or $2=1$. How? Where did I go wrong?",,"['calculus', 'ordinary-differential-equations', 'fake-proofs', 'paradoxes']"
49,How to integrate $xe^x$ without using antiderivatives or integration by parts.,How to integrate  without using antiderivatives or integration by parts.,xe^x,"Yesterday, I sat for my Real Analysis II paper. There I found a question asking to integrate $\displaystyle\int_0^1 xe^x \, dx$ without using antiderivatives and integrating by parts. I tried it by choosing a partition $$P_n=(0,\frac{1}{n},\frac{2}{n},\ldots,\frac{n-1}{n},1),$$ but I was not able to show that $\displaystyle \lim_{n \to \infty} U(f,P_n)=\lim_{n \to \infty} L(f,P_n)=1$","Yesterday, I sat for my Real Analysis II paper. There I found a question asking to integrate $\displaystyle\int_0^1 xe^x \, dx$ without using antiderivatives and integrating by parts. I tried it by choosing a partition $$P_n=(0,\frac{1}{n},\frac{2}{n},\ldots,\frac{n-1}{n},1),$$ but I was not able to show that $\displaystyle \lim_{n \to \infty} U(f,P_n)=\lim_{n \to \infty} L(f,P_n)=1$",,"['calculus', 'integration', 'definite-integrals', 'riemann-integration', 'riemann-sum']"
50,$\sum^{\infty}_{n=1}x_n$ converges $\Rightarrow$ $\sum^{\infty}_{n=1}x_n^3$ converges also?,converges   converges also?,\sum^{\infty}_{n=1}x_n \Rightarrow \sum^{\infty}_{n=1}x_n^3,"Let the series $\sum^{\infty}_{n=1}x_n$ converge. $\{x_n\}_{n=1}^{\infty} \in \mathbb R$ Does $\sum^{\infty}_{n=1}x_n^3$ converge too? I tried to find some counter-examples but found none. I tried to prove this also, but I can't... I don't know even if it's true or not.","Let the series $\sum^{\infty}_{n=1}x_n$ converge. $\{x_n\}_{n=1}^{\infty} \in \mathbb R$ Does $\sum^{\infty}_{n=1}x_n^3$ converge too? I tried to find some counter-examples but found none. I tried to prove this also, but I can't... I don't know even if it's true or not.",,"['calculus', 'sequences-and-series', 'limits', 'convergence-divergence']"
51,Is there a non-constant function $f$ such that $f'(x) = f(x - 1)$?,Is there a non-constant function  such that ?,f f'(x) = f(x - 1),"In discrete calculus, where the difference operator $\Delta f = f(x + 1) - f(x)$ takes the place of $\frac{d}{dx}$, Fibonacci sequences are given by the functions satisfying: $$ \Delta f(x) = f(x - 1) $$ Is there a non-constant function such that $\frac{d}{dx}f(x) = f(x - 1)$? If it exists, it would be the ""continuous analogue of the Fibonacci sequence"" in this sense, which seems cool.","In discrete calculus, where the difference operator $\Delta f = f(x + 1) - f(x)$ takes the place of $\frac{d}{dx}$, Fibonacci sequences are given by the functions satisfying: $$ \Delta f(x) = f(x - 1) $$ Is there a non-constant function such that $\frac{d}{dx}f(x) = f(x - 1)$? If it exists, it would be the ""continuous analogue of the Fibonacci sequence"" in this sense, which seems cool.",,"['calculus', 'fibonacci-numbers', 'discrete-calculus']"
52,Why non-regular curves have cusp(s),Why non-regular curves have cusp(s),,"I'm very confused about the author's explanation of regular curves in my calculus book. The author says that a regular curve $\gamma:[a,b]\to\mathbb{R}^3$ is a curve such that $\gamma'(t)\neq 0$ for all $t\in[a,b]$. The author then says that this is to ensure that there are no ""kinks"" or ""cusps"" in the curve, but I'm having difficulty understanding, visually, or intuitively why this is so. More precisely, does $\gamma'(t_0)=0$ if and only if the curve has a kink the point $\gamma(t_0)$? And why does it look like a kink? In my mind, I imgaine an insect flying in space, slowing down, stopping at some point $p$, and continuing it's flight from $p$. On an intuitive level, I think there could be a kink at $p$ because as the insect approaches $p$, it slows down along one line spanned by a tangent vector $v_1$ at $p$, but once it stops, it can continue its flight along another line spanned by a different tangent vector $v_2$ at $p$ (option A below). But it would also have the choice of stopping at $p$ and continuing in the same fashion it was before it slowed down and stopped (option B). So with the option B, it seems like you could have $\gamma'(t)\neq 0$ at $p$, and yet there would be no cusp at $p$. So just by looking at the graph, you wouldn't be able to tell if $\gamma$ was regular or not. Could someone please clarify?","I'm very confused about the author's explanation of regular curves in my calculus book. The author says that a regular curve $\gamma:[a,b]\to\mathbb{R}^3$ is a curve such that $\gamma'(t)\neq 0$ for all $t\in[a,b]$. The author then says that this is to ensure that there are no ""kinks"" or ""cusps"" in the curve, but I'm having difficulty understanding, visually, or intuitively why this is so. More precisely, does $\gamma'(t_0)=0$ if and only if the curve has a kink the point $\gamma(t_0)$? And why does it look like a kink? In my mind, I imgaine an insect flying in space, slowing down, stopping at some point $p$, and continuing it's flight from $p$. On an intuitive level, I think there could be a kink at $p$ because as the insect approaches $p$, it slows down along one line spanned by a tangent vector $v_1$ at $p$, but once it stops, it can continue its flight along another line spanned by a different tangent vector $v_2$ at $p$ (option A below). But it would also have the choice of stopping at $p$ and continuing in the same fashion it was before it slowed down and stopped (option B). So with the option B, it seems like you could have $\gamma'(t)\neq 0$ at $p$, and yet there would be no cusp at $p$. So just by looking at the graph, you wouldn't be able to tell if $\gamma$ was regular or not. Could someone please clarify?",,"['calculus', 'intuition']"
53,Integration of Fundamental Solution of Laplace's equation.,Integration of Fundamental Solution of Laplace's equation.,,"I am currently reading Evan's PDE and am getting hung up on many of the more ""technical details"". This question may be very basic (multivariable calculus). I am given that the fundamental solution of Laplace's equation is $$ \Phi(x) := \begin{cases} -\frac{1}{2 \pi}  \log |x|  & (n=2) \\  \frac{1}{n(n-2) \alpha(n)} \frac{1}{|x|^{n-2}} & (n \ge 3) \end{cases}$$ How would I evaluate $$ \int_{B(0, \epsilon)} |\Phi(y) | dy ? $$","I am currently reading Evan's PDE and am getting hung up on many of the more ""technical details"". This question may be very basic (multivariable calculus). I am given that the fundamental solution of Laplace's equation is $$ \Phi(x) := \begin{cases} -\frac{1}{2 \pi}  \log |x|  & (n=2) \\  \frac{1}{n(n-2) \alpha(n)} \frac{1}{|x|^{n-2}} & (n \ge 3) \end{cases}$$ How would I evaluate $$ \int_{B(0, \epsilon)} |\Phi(y) | dy ? $$",,"['calculus', 'partial-differential-equations']"
54,Compute the limit $\lim_{n\to\infty}\frac{1}{\log n}\sum_{k=1}^n\left(1-\frac{1}{n}\right)^k\frac{1}{k}$,Compute the limit,\lim_{n\to\infty}\frac{1}{\log n}\sum_{k=1}^n\left(1-\frac{1}{n}\right)^k\frac{1}{k},"Similar to this problem, how can one compute the following limit:   $$\lim_{n\to\infty}\frac{1}{\log n}\sum_{k=1}^n\left(1-\frac{1}{n}\right)^k\frac{1}{k}\quad ?$$ Note that $$\log x = \sum_{k=1}^{\infty}\left(1-\frac{1}{x}\right)^k\frac{1}{k},\quad x\ge \frac{1}{2}.$$","Similar to this problem, how can one compute the following limit:   $$\lim_{n\to\infty}\frac{1}{\log n}\sum_{k=1}^n\left(1-\frac{1}{n}\right)^k\frac{1}{k}\quad ?$$ Note that $$\log x = \sum_{k=1}^{\infty}\left(1-\frac{1}{x}\right)^k\frac{1}{k},\quad x\ge \frac{1}{2}.$$",,"['calculus', 'limits']"
55,Find Minimum value of $P=\frac{1}{1+2x}+\frac{1}{1+2y}+\frac{3-2xy}{5-x^2-y^2}$,Find Minimum value of,P=\frac{1}{1+2x}+\frac{1}{1+2y}+\frac{3-2xy}{5-x^2-y^2},"Given: $x,y\in (-\sqrt2;\sqrt2)$ and $x^4+y^4+4=\dfrac{6}{xy}$ Find Minimum value Of $$P=\frac{1}{1+2x}+\frac{1}{1+2y}+\frac{3-2xy}{5-x^2-y^2}$$ Could someone help me ?","Given: $x,y\in (-\sqrt2;\sqrt2)$ and $x^4+y^4+4=\dfrac{6}{xy}$ Find Minimum value Of $$P=\frac{1}{1+2x}+\frac{1}{1+2y}+\frac{3-2xy}{5-x^2-y^2}$$ Could someone help me ?",,"['calculus', 'inequality']"
56,Indefinite Integral of Reciprocal of Trigonometric Functions,Indefinite Integral of Reciprocal of Trigonometric Functions,,How to evaluate following integral $$\int \frac{\mathrm dx}{\sin^4x+\cos^4x\:+\sin^2(x) \cos^2(x)}$$ Can you please also give me the steps of solving it?,How to evaluate following integral $$\int \frac{\mathrm dx}{\sin^4x+\cos^4x\:+\sin^2(x) \cos^2(x)}$$ Can you please also give me the steps of solving it?,,"['calculus', 'integration', 'indefinite-integrals']"
57,Analogy to the purpose of Taylor series,Analogy to the purpose of Taylor series,,"I want to know an analogy to the purpose of Taylor series. I did a google search for web and videos : all talks about what Taylor series and examples of it. But no analogies. I am not a math geek and this is my attempt to re-learn Calculus in a better way, to understand Physics and Linear Algebra. Having an analogy will indeed help to view its use in real life. Learning seems lacking, if  the concept can't be applied. Appreciate a more laymen term explanation at this challenging point (for me). Have read this post as of now: What are the practical applications of Taylor series? . That post (the answers, comments) indeed increases the bar of my expectation for a satisfying answer to my question .","I want to know an analogy to the purpose of Taylor series. I did a google search for web and videos : all talks about what Taylor series and examples of it. But no analogies. I am not a math geek and this is my attempt to re-learn Calculus in a better way, to understand Physics and Linear Algebra. Having an analogy will indeed help to view its use in real life. Learning seems lacking, if  the concept can't be applied. Appreciate a more laymen term explanation at this challenging point (for me). Have read this post as of now: What are the practical applications of Taylor series? . That post (the answers, comments) indeed increases the bar of my expectation for a satisfying answer to my question .",,"['calculus', 'soft-question', 'taylor-expansion']"
58,A problem on Mean Value Theorem,A problem on Mean Value Theorem,,"If $f''(x)$ exists on $[a,b]$ and $f'(a)=f'(b)$, then : $$f(\frac{a+b}{2})=\frac 1 2[f(a)+f(b)]+\frac{(b-a)^2}{8}f''(c)$$ for some $c\in(a,b)$. I tried but was unable to think of a function and was unable to use the given condition except for Rolle's Theorem which does not yield anything useful(yet). Any hints or help will be appreciated.","If $f''(x)$ exists on $[a,b]$ and $f'(a)=f'(b)$, then : $$f(\frac{a+b}{2})=\frac 1 2[f(a)+f(b)]+\frac{(b-a)^2}{8}f''(c)$$ for some $c\in(a,b)$. I tried but was unable to think of a function and was unable to use the given condition except for Rolle's Theorem which does not yield anything useful(yet). Any hints or help will be appreciated.",,['calculus']
59,Evaluate: $\int \frac{1}{x^7-x}\ \mathrm{d}x$,Evaluate:,\int \frac{1}{x^7-x}\ \mathrm{d}x,Evaluate: $$\int \frac{1}{x^7-x}\ \mathrm{d}x$$ My approach to this question: $$\int \frac{1}{x^7-x}\ \mathrm{d}x = \int \frac{1}{x(x^6-1)}\ \mathrm{d}x$$ $$\int \frac{1}{x(x^6-1)}\ \mathrm{d}x = \int \frac{1}{x(x-1)(x+1)(x^2-x+1)(x^2+x+1)}\ \mathrm{d}x$$ $$\frac{1}{x(x-1)(x+1)(x^2-x+1)(x^2+x+1)} = \frac{A}{x} + \frac{B}{x-1} + \frac{C}{x+1} + \frac{Dx+E}{x^2-x+1} + \frac{Fx+G}{x^2+x+1}$$ At this point I realized how brutal this question if going to be. Is there an easier way to solve the integral?,Evaluate: $$\int \frac{1}{x^7-x}\ \mathrm{d}x$$ My approach to this question: $$\int \frac{1}{x^7-x}\ \mathrm{d}x = \int \frac{1}{x(x^6-1)}\ \mathrm{d}x$$ $$\int \frac{1}{x(x^6-1)}\ \mathrm{d}x = \int \frac{1}{x(x-1)(x+1)(x^2-x+1)(x^2+x+1)}\ \mathrm{d}x$$ $$\frac{1}{x(x-1)(x+1)(x^2-x+1)(x^2+x+1)} = \frac{A}{x} + \frac{B}{x-1} + \frac{C}{x+1} + \frac{Dx+E}{x^2-x+1} + \frac{Fx+G}{x^2+x+1}$$ At this point I realized how brutal this question if going to be. Is there an easier way to solve the integral?,,['calculus']
60,Fourier series coefficients proof,Fourier series coefficients proof,,"Can somebody help me understanding the fouries series coefficients? I know that if we have: $$f(t) = \sum_{n=1}^N A_n \sin(2\pi nt + Ph_n) \tag{where $Ph_n$ = phase}$$ And because of the $\sin(a+b)$ formula: $$ f(t) = \sin(2\pi nt + Ph) = \sin(2\pi nt )\cos(Ph_n) + \cos(2\pi nt )\sin(Ph_n)$$ Then: $$\sum_{n=1}^N A_n \sin(2\pi nt + Ph) = \sum_{n=1}^N (A_n\sin(2\pi nt )\cos(Ph_n) + A_n\cos(2\pi nt )\sin(Ph_n))$$ And by definition: $$a_n = A_n\cos(Ph_n)$$ $$b_n = A_n\sin(Ph_n)$$ Then: $$f(t) = \sum_{n=1}^N A_n \sin(2\pi nt + Ph) = \sum_{n=1}^N (a_n\sin(2\pi nt ) + b_n\cos(2\pi nt ))$$ Where does the $A_n$ in the first formula come from? Could somebody explain me where does the $\frac{a_0}{2}$ in the formula below come from? $$f(x) = \frac{a_0}{2}+\sum_{n=1}^\infty (a_n \cos(nx) + b_n \sin(nx))$$ And the most important: How can I proof the formulas for the $a_n$ and $b_n$ coefficients? PLEASE, I'm searchinf for these answers for DAYS.","Can somebody help me understanding the fouries series coefficients? I know that if we have: $$f(t) = \sum_{n=1}^N A_n \sin(2\pi nt + Ph_n) \tag{where $Ph_n$ = phase}$$ And because of the $\sin(a+b)$ formula: $$ f(t) = \sin(2\pi nt + Ph) = \sin(2\pi nt )\cos(Ph_n) + \cos(2\pi nt )\sin(Ph_n)$$ Then: $$\sum_{n=1}^N A_n \sin(2\pi nt + Ph) = \sum_{n=1}^N (A_n\sin(2\pi nt )\cos(Ph_n) + A_n\cos(2\pi nt )\sin(Ph_n))$$ And by definition: $$a_n = A_n\cos(Ph_n)$$ $$b_n = A_n\sin(Ph_n)$$ Then: $$f(t) = \sum_{n=1}^N A_n \sin(2\pi nt + Ph) = \sum_{n=1}^N (a_n\sin(2\pi nt ) + b_n\cos(2\pi nt ))$$ Where does the $A_n$ in the first formula come from? Could somebody explain me where does the $\frac{a_0}{2}$ in the formula below come from? $$f(x) = \frac{a_0}{2}+\sum_{n=1}^\infty (a_n \cos(nx) + b_n \sin(nx))$$ And the most important: How can I proof the formulas for the $a_n$ and $b_n$ coefficients? PLEASE, I'm searchinf for these answers for DAYS.",,"['calculus', 'fourier-analysis', 'fourier-series']"
61,Common tangent lines of two quadratic functions,Common tangent lines of two quadratic functions,,Find all such lines that are tangent to the following curves: $$y=x^2$$ and $$y=-x^2+2x-2$$ I have been pounding my head against the wall on this. I used the derivatives and assumed that their derivatives must be equal at those tangent point but could not figure out the equations. An explanation will be appreciated.,Find all such lines that are tangent to the following curves: $$y=x^2$$ and $$y=-x^2+2x-2$$ I have been pounding my head against the wall on this. I used the derivatives and assumed that their derivatives must be equal at those tangent point but could not figure out the equations. An explanation will be appreciated.,,"['calculus', 'geometry']"
62,"Let $f(\frac ab)=ab$, where $\frac ab$ is irreducible, in $\mathbb Q^+$. What is $\sum_{x\in\mathbb Q^+}\frac 1{f(x)^2}$?","Let , where  is irreducible, in . What is ?",f(\frac ab)=ab \frac ab \mathbb Q^+ \sum_{x\in\mathbb Q^+}\frac 1{f(x)^2},"Let $f(\frac ab)=ab$, where $\frac ab$ is irreducible, in $\mathbb Q^+$. What is $\sum_{x\in\mathbb Q^+}\frac 1{f(x)^2}$? Club challenge problem. I don't think it's possible to do with only high school calculus. Help, please?","Let $f(\frac ab)=ab$, where $\frac ab$ is irreducible, in $\mathbb Q^+$. What is $\sum_{x\in\mathbb Q^+}\frac 1{f(x)^2}$? Club challenge problem. I don't think it's possible to do with only high school calculus. Help, please?",,"['calculus', 'sequences-and-series', 'analysis', 'summation', 'rational-numbers']"
63,$\sum \limits_{k=1}^{\infty} \frac{6^k}{\left(3^{k+1}-2^{k+1}\right)\left(3^k-2^k\right)} $ as a rational number.,as a rational number.,\sum \limits_{k=1}^{\infty} \frac{6^k}{\left(3^{k+1}-2^{k+1}\right)\left(3^k-2^k\right)} ,"$$\sum \limits_{k=1}^{\infty} \frac{6^k}{\left(3^{k+1}-2^{k+1}\right)\left(3^k-2^k\right)} $$ I know from the ratio test it convergest, and I graph it on wolfram alpha and I suspect the sum is 2; however, I am having trouble with the manipulation of the fraction to show the rational number. ps. When it says write as a rational number it means to write the value of $S_{\infty}$ or to rewrite the fraction?","$$\sum \limits_{k=1}^{\infty} \frac{6^k}{\left(3^{k+1}-2^{k+1}\right)\left(3^k-2^k\right)} $$ I know from the ratio test it convergest, and I graph it on wolfram alpha and I suspect the sum is 2; however, I am having trouble with the manipulation of the fraction to show the rational number. ps. When it says write as a rational number it means to write the value of $S_{\infty}$ or to rewrite the fraction?",,"['calculus', 'sequences-and-series']"
64,Compute $\lim\limits_{n\rightarrow\infty}\left(\frac{\left(2n\right)!}{\left(n!\right)^{2}}\right)^{\frac{1}{n}}$ [duplicate],Compute  [duplicate],\lim\limits_{n\rightarrow\infty}\left(\frac{\left(2n\right)!}{\left(n!\right)^{2}}\right)^{\frac{1}{n}},"This question already has answers here : Show that that $\lim_{n\to\infty}\sqrt[n]{\binom{2n}{n}} = 4$ (7 answers) Closed 1 year ago . Compute $$\lim_{n\rightarrow\infty}\left(\frac{\left(2n\right)!}{\left(n!\right)^{2}}\right)^{\frac{1}{n}}$$ If you have some nice proofs and you're willing to share them, then I thank you and you definitely have my upvote!","This question already has answers here : Show that that $\lim_{n\to\infty}\sqrt[n]{\binom{2n}{n}} = 4$ (7 answers) Closed 1 year ago . Compute $$\lim_{n\rightarrow\infty}\left(\frac{\left(2n\right)!}{\left(n!\right)^{2}}\right)^{\frac{1}{n}}$$ If you have some nice proofs and you're willing to share them, then I thank you and you definitely have my upvote!",,"['calculus', 'limits', 'binomial-coefficients', 'radicals']"
65,Is the derivative of a big-O class the same as the big-O class of the derivative?,Is the derivative of a big-O class the same as the big-O class of the derivative?,,"Basically, for every function $f(x) \in O(g(x))$, is $f'(x) \in O(g'(x))$?","Basically, for every function $f(x) \in O(g(x))$, is $f'(x) \in O(g'(x))$?",,"['calculus', 'asymptotics']"
66,Proof of the extreme value theorem without using subsequences,Proof of the extreme value theorem without using subsequences,,"I am preparing a lecture on the Weierstrass theorem (probably best known as the Extreme Value Theorem in english-speaking countries), and I would propose a proof that does not use the extraction of converging subsequences. I did not explain subsequences in my calculus course, and I must choose between skipping the proof of the theorem and finding some proof which works only for functions $\mathbb{R} \to \mathbb{R}$. I remember I once read a proof based on some bisection technique, but I can't find a reference right now. I would be grateful for any reference to books, papers, web sites about this alternative proof. Edit: since somebody modified my question, I will write down the precise theorem I want to prove. Theorem. Let $f \colon [a,b] \to \mathbb{R}$ be a continuous function. Then $f$ has at least a maximum and a minimum point.","I am preparing a lecture on the Weierstrass theorem (probably best known as the Extreme Value Theorem in english-speaking countries), and I would propose a proof that does not use the extraction of converging subsequences. I did not explain subsequences in my calculus course, and I must choose between skipping the proof of the theorem and finding some proof which works only for functions $\mathbb{R} \to \mathbb{R}$. I remember I once read a proof based on some bisection technique, but I can't find a reference right now. I would be grateful for any reference to books, papers, web sites about this alternative proof. Edit: since somebody modified my question, I will write down the precise theorem I want to prove. Theorem. Let $f \colon [a,b] \to \mathbb{R}$ be a continuous function. Then $f$ has at least a maximum and a minimum point.",,"['calculus', 'reference-request']"
67,How to find the oblique asymptote of root of a function?,How to find the oblique asymptote of root of a function?,,"In a test example I'm solving, the question asks to find the oblique asymptote of the following function: $f(x) = \sqrt{4x^2+x+6}$ $x$ at $+\infty$ We have only learned how to do so with rational functions. Is there any general way of finding the oblique asymptote that works with any kind of function? Perhaps using limits?","In a test example I'm solving, the question asks to find the oblique asymptote of the following function: $f(x) = \sqrt{4x^2+x+6}$ $x$ at $+\infty$ We have only learned how to do so with rational functions. Is there any general way of finding the oblique asymptote that works with any kind of function? Perhaps using limits?",,"['calculus', 'limits', 'graphing-functions']"
68,Can we use this formula for a certain indeterminate limit $1^{+\infty}$?,Can we use this formula for a certain indeterminate limit ?,1^{+\infty},"Someone told me if $\lim_{x\to{+\infty}} f(x)^{g(x)}=1^{+\infty}$ which is indeterminate limit then we can solve it by taking the following limit:  $$k =\lim_{x\to +\infty}\big(f(x)-1\big)g(x)$$ So $$\lim_{x\to{+\infty}} f(x)^{g(x)}=e^k$$ I used this formula a lot and have seen it is fast practical way for this kind of limits, especially when we just want to know the value of the limit . What is the proof of this formula if it is true? Thanks.","Someone told me if $\lim_{x\to{+\infty}} f(x)^{g(x)}=1^{+\infty}$ which is indeterminate limit then we can solve it by taking the following limit:  $$k =\lim_{x\to +\infty}\big(f(x)-1\big)g(x)$$ So $$\lim_{x\to{+\infty}} f(x)^{g(x)}=e^k$$ I used this formula a lot and have seen it is fast practical way for this kind of limits, especially when we just want to know the value of the limit . What is the proof of this formula if it is true? Thanks.",,"['calculus', 'analysis', 'limits']"
69,Fundamental Theorem of Calculus problem,Fundamental Theorem of Calculus problem,,"I don't understand the intuition behind this.  Why can we just plug in $x$ for $t$ here and that gives us the result?  I thought I was understanding the Fundamental Theorem of Calculus, but I don't see how it applies here.  I thought the Theorem mainly stated that the area under a function can be found by taking the the value of the anti derivative over the specified interval. It doesn't make sense to me why we just plug in $x$ and voila that's our answer. $$\frac {d}{dx} \int_{a}^{x} (t^3 + 1) \ dt = x^3 +  1$$","I don't understand the intuition behind this.  Why can we just plug in $x$ for $t$ here and that gives us the result?  I thought I was understanding the Fundamental Theorem of Calculus, but I don't see how it applies here.  I thought the Theorem mainly stated that the area under a function can be found by taking the the value of the anti derivative over the specified interval. It doesn't make sense to me why we just plug in $x$ and voila that's our answer. $$\frac {d}{dx} \int_{a}^{x} (t^3 + 1) \ dt = x^3 +  1$$",,"['calculus', 'integration']"
70,Finding derivative of $\sqrt{9-x}$,Finding derivative of,\sqrt{9-x},"I am trying to find the derivative of $\sqrt{9-x}$ using the definition of a derivative $$\lim_{h\to 0} \frac {f(a+h)-f(a)}{h} $$ $$\lim_{h\to 0} \frac {\sqrt{9-(a+h)}-\sqrt{9-a}}{h} $$ So to simplify I multiply by the conjugate $$\lim_{h\to0} \frac {\sqrt{9-(a+h)}-\sqrt{9-a}}{h}\cdot \frac{ \sqrt{9-(a+h)}+ \sqrt{9-a}}{\sqrt{9-(a+h)}+\sqrt{9-a}}$$ which gives me $$\frac {-2a-h}{h(\sqrt{9-(a+h)}+\sqrt{9-a})}$$ I have no idea what to do from here, obviously I can easily get the derivative using other methods but with this one I have no idea how to proceed.","I am trying to find the derivative of $\sqrt{9-x}$ using the definition of a derivative $$\lim_{h\to 0} \frac {f(a+h)-f(a)}{h} $$ $$\lim_{h\to 0} \frac {\sqrt{9-(a+h)}-\sqrt{9-a}}{h} $$ So to simplify I multiply by the conjugate $$\lim_{h\to0} \frac {\sqrt{9-(a+h)}-\sqrt{9-a}}{h}\cdot \frac{ \sqrt{9-(a+h)}+ \sqrt{9-a}}{\sqrt{9-(a+h)}+\sqrt{9-a}}$$ which gives me $$\frac {-2a-h}{h(\sqrt{9-(a+h)}+\sqrt{9-a})}$$ I have no idea what to do from here, obviously I can easily get the derivative using other methods but with this one I have no idea how to proceed.",,['calculus']
71,Find the limit of $(e^{2x}+1)^{1/x}$,Find the limit of,(e^{2x}+1)^{1/x},I want to find $\lim \limits_{x\to \infty}(e^{2x}+1)^{1/x}$. The first thing I thought is that $\lim  \limits_{x\to\infty}\frac{1}{x}=0$. So the limit would be $\lim  \limits_{x\to \infty}(e^{2x}+1)^{1/x}= 1$ but I am pretty sure that this is not the right answer. Then I read in wikipedia that I can apply L'Hospital on $\infty^{0}$. The only problem is that I don't know how to do it. Do I have to transform it in something like  $\infty^{\infty}$ or $0^{0}$?,I want to find $\lim \limits_{x\to \infty}(e^{2x}+1)^{1/x}$. The first thing I thought is that $\lim  \limits_{x\to\infty}\frac{1}{x}=0$. So the limit would be $\lim  \limits_{x\to \infty}(e^{2x}+1)^{1/x}= 1$ but I am pretty sure that this is not the right answer. Then I read in wikipedia that I can apply L'Hospital on $\infty^{0}$. The only problem is that I don't know how to do it. Do I have to transform it in something like  $\infty^{\infty}$ or $0^{0}$?,,"['calculus', 'limits']"
72,Why can't I use the disk method to compute surface area? [duplicate],Why can't I use the disk method to compute surface area? [duplicate],,"This question already has answers here : Closed 12 years ago . Possible Duplicate: Areas versus volumes of revolution I was trying to brush up on Calculus since I haven't used it in years. I wanted to derive formulas for volume and surface area of a sphere of radius r . I got volume right, but for surface area I'm getting the wrong answer. I start with the equation for the half of a circle of radius r centered at the origin with positive y coordinates: $$f(x) = \sqrt{r^2 - x^2}$$ So I started by breaking it up into disks, with radius f(x) and height dx , and integrating the surface area of the outside of the disks. (Which is to say, circumference of disk multiplied by height of disk.) $$dA = 2 \pi f(x) dx$$ $$A = \int_{-r}^r dA$$ $$ = 2 \pi \int_{-r}^r f(x) dx$$ Now, I don't know how to integrate f(x) symbolically, but logically if the integral is the area under the curve from -r to r , it would be half the area of a circle of radius r , giving: $$ = 2 \pi (\frac{\pi r^2}{2})$$ $$ = \pi^2 r^2$$ Of course, this is not the correct answer. Am I making an error in my math, or does the disk method just not work for surface area? (And if not, why does it work for volume?)","This question already has answers here : Closed 12 years ago . Possible Duplicate: Areas versus volumes of revolution I was trying to brush up on Calculus since I haven't used it in years. I wanted to derive formulas for volume and surface area of a sphere of radius r . I got volume right, but for surface area I'm getting the wrong answer. I start with the equation for the half of a circle of radius r centered at the origin with positive y coordinates: $$f(x) = \sqrt{r^2 - x^2}$$ So I started by breaking it up into disks, with radius f(x) and height dx , and integrating the surface area of the outside of the disks. (Which is to say, circumference of disk multiplied by height of disk.) $$dA = 2 \pi f(x) dx$$ $$A = \int_{-r}^r dA$$ $$ = 2 \pi \int_{-r}^r f(x) dx$$ Now, I don't know how to integrate f(x) symbolically, but logically if the integral is the area under the curve from -r to r , it would be half the area of a circle of radius r , giving: $$ = 2 \pi (\frac{\pi r^2}{2})$$ $$ = \pi^2 r^2$$ Of course, this is not the correct answer. Am I making an error in my math, or does the disk method just not work for surface area? (And if not, why does it work for volume?)",,"['calculus', 'integration']"
73,Fun calculus problem I can't seem to solve,Fun calculus problem I can't seem to solve,,"I've recently picked up a math book I haven't read since college (highly recommended reading by the way!). I was reviewing multi-dimentional derivatives and such, and I stumbled upon a problem I've been trying to solve for two days, and I can't get it out of my head, so please help me out! = ) Problem (from memory): There is a rabbit that runs in a perfect circle of radius $r$ with a constant speed $v$. A fox chases the rabbit, starting from the center of the circle and also moves with a constant speed $v$ such that it is always between the center of the circle and the rabbit. How long will it take for the fox to catch the rabbit? I tried using the fact that $|x'(t)| = |y'(t)| = v$ where $x(t)$ is fox's position and $y(t)$ is rabbit's position, and that $x'(t)x''(t) = 0$ because of constant speed restriction, but I'm still failing to find a solution. Anyone feel like attacking this one?","I've recently picked up a math book I haven't read since college (highly recommended reading by the way!). I was reviewing multi-dimentional derivatives and such, and I stumbled upon a problem I've been trying to solve for two days, and I can't get it out of my head, so please help me out! = ) Problem (from memory): There is a rabbit that runs in a perfect circle of radius $r$ with a constant speed $v$. A fox chases the rabbit, starting from the center of the circle and also moves with a constant speed $v$ such that it is always between the center of the circle and the rabbit. How long will it take for the fox to catch the rabbit? I tried using the fact that $|x'(t)| = |y'(t)| = v$ where $x(t)$ is fox's position and $y(t)$ is rabbit's position, and that $x'(t)x''(t) = 0$ because of constant speed restriction, but I'm still failing to find a solution. Anyone feel like attacking this one?",,['calculus']
74,"Finding the limit of $\frac{Q(n)}{P(n)}$ where $Q,P$ are polynomials",Finding the limit of  where  are polynomials,"\frac{Q(n)}{P(n)} Q,P","Suppose that $$Q(x)=a_{n}x^{n}+a_{n-1}x^{n-1}+\cdots+a_{1}x+a_{0} $$and $$P(x)=b_{m}x^{m}+b_{m-1}x^{m-1}+\cdots+b_{1}x+b_{0}.$$ How do I find $$\lim_{x\rightarrow\infty}\frac{Q(x)}{P(x)}$$ and what does the sequence $$\frac{Q(k)}{P(k)}$$ converge to? For example, how would I find what the sequence $$\frac{8k^2+2k-100}{3k^2+2k+1}$$ converges to?  Or what is $$\lim_{x\rightarrow\infty}\frac{3x+5}{-2x+9}?$$ This is being asked in an effort to cut down on duplicates, see here: Coping with abstract duplicate questions. and here: List of abstract duplicates .","Suppose that $$Q(x)=a_{n}x^{n}+a_{n-1}x^{n-1}+\cdots+a_{1}x+a_{0} $$and $$P(x)=b_{m}x^{m}+b_{m-1}x^{m-1}+\cdots+b_{1}x+b_{0}.$$ How do I find $$\lim_{x\rightarrow\infty}\frac{Q(x)}{P(x)}$$ and what does the sequence $$\frac{Q(k)}{P(k)}$$ converge to? For example, how would I find what the sequence $$\frac{8k^2+2k-100}{3k^2+2k+1}$$ converges to?  Or what is $$\lim_{x\rightarrow\infty}\frac{3x+5}{-2x+9}?$$ This is being asked in an effort to cut down on duplicates, see here: Coping with abstract duplicate questions. and here: List of abstract duplicates .",,"['calculus', 'sequences-and-series', 'limits', 'faq']"
75,Find the set of values of $\alpha$ so that $f(x)=\dfrac{\alpha x^2+6x-8}{\alpha+6x-8x^2}$ is one one.,Find the set of values of  so that  is one one.,\alpha f(x)=\dfrac{\alpha x^2+6x-8}{\alpha+6x-8x^2},"Let $f$ be a function defined in its domain given by $f(x)=\dfrac{\alpha x^2+6x-8}{\alpha+6x-8x^2}$ . Find the set of values of $\alpha$ so that $f(x)$ is one-one . My attempt As $f(x)$ have to be one-one so any line $\parallel$ to $x$ -axis must cut the graph only once. So, necessary condition is that $f(x)$ must be monotonic. $$f'(x)={(\alpha+6x-8x^2)(2\alpha x+6)-(\alpha x^2+6x-8)(6-16x)\over(\alpha+6x-8x^2)^2}$$ $$\implies f'(x)={6(\alpha+8)x^2+2(\alpha+8)(\alpha-8)x+6(\alpha+8)\over(\alpha+6x-8x^2)^2}=\frac{Q(x)}{(\alpha+6x-8x^2)^2}$$ In order to have $f'(x)\le0$ or $\ge0$ $\forall$ $x\in R$ , Discriminant of $Q(x)$ $\le0$ . $$\implies (\alpha+8)^2(\alpha-14)(\alpha-2)\le 0 \\ {\implies \alpha \in [ 2 \; , \; 14] \tag{1}}$$ But, for this range $f(x) $ is not one one (I have projected it in desmos for verification) and I found it later that $(1)$ is also the condition for which $f(x)$ is onto i.e. Range of $f \in \mathbb R$ . $\boxed{y=\dfrac{\alpha x^2+6x-8}{\alpha+6x-8x^2} \\ \implies (\alpha+8y)x^2+(6-6y)x+(-8-\alpha y)=0 \\ \text{as}\;  x\in \mathbb R  \;; D\ge0 \\ \implies 6-6y)^2+4(\alpha+8y)(8+\alpha y)\ge 0 \\ \implies (9+8\alpha)y^2+(\alpha^2+46)y+(8\alpha+9)\ge0\; \;  \text{now as} \;  y\in R\\ \implies (\alpha^2+46)^2-4(8\alpha+9)^2\le0 \implies (\alpha+8)^2(\alpha-14)(\alpha-2)\le0 \; \text{....same as (1) above}}$ Desmos Graph shows that it is many one for this range as line line parallel to x axis cut it 2 times although function continuously increases for all $x\in \mathbb R$ ...  Please tell what is happening here and how to solve this question... Link for the diagram Question Source: India's JEE math practice book.","Let be a function defined in its domain given by . Find the set of values of so that is one-one . My attempt As have to be one-one so any line to -axis must cut the graph only once. So, necessary condition is that must be monotonic. In order to have or , Discriminant of . But, for this range is not one one (I have projected it in desmos for verification) and I found it later that is also the condition for which is onto i.e. Range of . Desmos Graph shows that it is many one for this range as line line parallel to x axis cut it 2 times although function continuously increases for all ...  Please tell what is happening here and how to solve this question... Link for the diagram Question Source: India's JEE math practice book.","f f(x)=\dfrac{\alpha x^2+6x-8}{\alpha+6x-8x^2} \alpha f(x) f(x) \parallel x f(x) f'(x)={(\alpha+6x-8x^2)(2\alpha x+6)-(\alpha x^2+6x-8)(6-16x)\over(\alpha+6x-8x^2)^2} \implies f'(x)={6(\alpha+8)x^2+2(\alpha+8)(\alpha-8)x+6(\alpha+8)\over(\alpha+6x-8x^2)^2}=\frac{Q(x)}{(\alpha+6x-8x^2)^2} f'(x)\le0 \ge0 \forall x\in R Q(x) \le0 \implies (\alpha+8)^2(\alpha-14)(\alpha-2)\le 0 \\
{\implies \alpha \in [ 2 \; , \; 14] \tag{1}} f(x)  (1) f(x) f \in \mathbb R \boxed{y=\dfrac{\alpha x^2+6x-8}{\alpha+6x-8x^2} \\ \implies (\alpha+8y)x^2+(6-6y)x+(-8-\alpha y)=0 \\ \text{as}\;  x\in \mathbb R  \;; D\ge0 \\ \implies 6-6y)^2+4(\alpha+8y)(8+\alpha y)\ge 0 \\ \implies (9+8\alpha)y^2+(\alpha^2+46)y+(8\alpha+9)\ge0\; \;  \text{now as} \;  y\in R\\ \implies (\alpha^2+46)^2-4(8\alpha+9)^2\le0 \implies (\alpha+8)^2(\alpha-14)(\alpha-2)\le0 \; \text{....same as (1) above}} x\in \mathbb R","['calculus', 'algebra-precalculus', 'quadratics', 'graphing-functions']"
76,Proving that function proportional to derivative is monotonic indirectly,Proving that function proportional to derivative is monotonic indirectly,,The solution to the differential equation $f'(x)=Cf(x)$ is $f(x)=Ae^{Cx}$ The function $f(x)$ is monotonic. $$f(x)=Ae^{Cx}$$ $$f'(x)=ACe^{Cx}$$ $$\operatorname{sgn}(f'(x))=\operatorname{sgn}(AC)\operatorname{sgn}(e^{Cx})$$ $$\operatorname{sgn}(f'(x))=\operatorname{sgn}(AC)\cdot1$$ $$\operatorname{sgn}(f'(x))=\operatorname{sgn}(AC)=constant$$ This approach to proving the monotonicity depends on solving the diff eq. Is there a way to prove that any function that satisfies $f'(x)=Cf(x)$ is monotonic without solving for $f(x)$ ?,The solution to the differential equation is The function is monotonic. This approach to proving the monotonicity depends on solving the diff eq. Is there a way to prove that any function that satisfies is monotonic without solving for ?,f'(x)=Cf(x) f(x)=Ae^{Cx} f(x) f(x)=Ae^{Cx} f'(x)=ACe^{Cx} \operatorname{sgn}(f'(x))=\operatorname{sgn}(AC)\operatorname{sgn}(e^{Cx}) \operatorname{sgn}(f'(x))=\operatorname{sgn}(AC)\cdot1 \operatorname{sgn}(f'(x))=\operatorname{sgn}(AC)=constant f'(x)=Cf(x) f(x),"['calculus', 'ordinary-differential-equations', 'derivatives']"
77,Proving a closed form of an integral [closed],Proving a closed form of an integral [closed],,"Closed. This question does not meet Mathematics Stack Exchange guidelines . It is not currently accepting answers. Please provide additional context , which ideally explains why the question is relevant to you and our  community. Some forms of context include: background and motivation, relevant definitions, source, possible strategies, your current progress, why the question is interesting or important, etc. Closed 1 year ago . Improve this question Is there any proof for this integral? $$\int \limits _0^1\frac{1}{a^2x^2+1}\left [\left (1-\frac{x}{2}\ln \frac{1+x}{1-x}\right )^2+\frac{\pi^2x^2}{4}\right ]^{-1}\,dx=\frac{\arctan a}{a-\arctan a}-\frac{3}{a^2},\quad \operatorname{Re}(a)>0.$$ I tried substituting $x=\frac{1-x}{1+x}$ but the integral seems to be harder.","Closed. This question does not meet Mathematics Stack Exchange guidelines . It is not currently accepting answers. Please provide additional context , which ideally explains why the question is relevant to you and our  community. Some forms of context include: background and motivation, relevant definitions, source, possible strategies, your current progress, why the question is interesting or important, etc. Closed 1 year ago . Improve this question Is there any proof for this integral? I tried substituting but the integral seems to be harder.","\int \limits _0^1\frac{1}{a^2x^2+1}\left [\left (1-\frac{x}{2}\ln \frac{1+x}{1-x}\right )^2+\frac{\pi^2x^2}{4}\right ]^{-1}\,dx=\frac{\arctan a}{a-\arctan a}-\frac{3}{a^2},\quad \operatorname{Re}(a)>0. x=\frac{1-x}{1+x}","['calculus', 'integration', 'definite-integrals', 'closed-form']"
78,"Correct result for this integral $\int \frac{\sqrt{\sqrt{\sqrt{2 \cos \left(5 \sqrt{x}+4\right)+2}+2}+2}}{\sqrt{x}}\, dx$",Correct result for this integral,"\int \frac{\sqrt{\sqrt{\sqrt{2 \cos \left(5 \sqrt{x}+4\right)+2}+2}+2}}{\sqrt{x}}\, dx","Wolfram|Alpha and its CAS, Wolfram Mathematica are, as far as I know, the only website and software that give the correct solution to this integral , $$ f(x) = \frac{\sqrt{\sqrt{\sqrt{2 \cos \left(5 \sqrt{x}+4\right)+2}+2}+2}}{\sqrt{x}} $$ $$ F(x) = \int f(x)\, dx$$ because deriving the function given as result and using the FullSimplify function of Mathematica we get to the original function that we wanted to integrate. This is the solution: $$ F(x) = \frac{1}{5} (-8) \sqrt{2} \sqrt{\cos \left(5 \sqrt{x}+4\right)+1} \sqrt{\sqrt{2} \sqrt{\cos \left(5 \sqrt{x}+4\right)+1}+2} \left(\sqrt{\sqrt{2} \sqrt{\cos \left(5 \sqrt{x}+4\right)+1}+2}-2\right) \sqrt{\sqrt{\sqrt{2} \sqrt{\cos \left(5 \sqrt{x}+4\right)+1}+2}+2} \csc \left(5 \sqrt{x}+4\right) + C $$ Fricas finds another expression for the integral that I'm curious about: $$ F(x) = \frac{1}{5} (-8) \sqrt{2} \sqrt{\sqrt[4]{2} \sqrt{\sqrt{\cos \left(5 \sqrt{x}+4\right)+1}+\sqrt{2}}+2} \left(\sqrt{2} \cos \left(5 \sqrt{x}+4\right)-2 \sqrt{\sqrt{2} \sqrt{\cos \left(5 \sqrt{x}+4\right)+1} \left(\sqrt{\cos \left(5 \sqrt{x}+4\right)+1}+\sqrt{2}\right)+2 \sqrt{\cos \left(5 \sqrt{x}+4\right)+1}+\sqrt{2}}\right) \csc \left(5 \sqrt{x}+4\right) + C$$ However, in this video, an incorrect result is given although the integration process seems correct. As above, you know that the result is incorrect since deriving the resulting function doesn't result in the original function we wanted to integrate. Is the expression given by Fricas equivalent to the one given by Wolfram|Alpha and Mathematica ? Final update : According to Mathematica , the expression given by FriCAS is not equal to the correct solution given by Mathematica and Wolfram|Alpha. This means it is not the solution to this problem. Even though I have a Pro Premium subscription, the step-by-step solution is not available for this input. What are the steps taken by Mathematica /Wolfram|Alpha to get to the given result?","Wolfram|Alpha and its CAS, Wolfram Mathematica are, as far as I know, the only website and software that give the correct solution to this integral , because deriving the function given as result and using the FullSimplify function of Mathematica we get to the original function that we wanted to integrate. This is the solution: Fricas finds another expression for the integral that I'm curious about: However, in this video, an incorrect result is given although the integration process seems correct. As above, you know that the result is incorrect since deriving the resulting function doesn't result in the original function we wanted to integrate. Is the expression given by Fricas equivalent to the one given by Wolfram|Alpha and Mathematica ? Final update : According to Mathematica , the expression given by FriCAS is not equal to the correct solution given by Mathematica and Wolfram|Alpha. This means it is not the solution to this problem. Even though I have a Pro Premium subscription, the step-by-step solution is not available for this input. What are the steps taken by Mathematica /Wolfram|Alpha to get to the given result?"," f(x) = \frac{\sqrt{\sqrt{\sqrt{2 \cos \left(5 \sqrt{x}+4\right)+2}+2}+2}}{\sqrt{x}}   F(x) = \int f(x)\, dx  F(x) = \frac{1}{5} (-8) \sqrt{2} \sqrt{\cos \left(5 \sqrt{x}+4\right)+1} \sqrt{\sqrt{2} \sqrt{\cos \left(5 \sqrt{x}+4\right)+1}+2} \left(\sqrt{\sqrt{2} \sqrt{\cos \left(5 \sqrt{x}+4\right)+1}+2}-2\right) \sqrt{\sqrt{\sqrt{2} \sqrt{\cos \left(5 \sqrt{x}+4\right)+1}+2}+2} \csc \left(5 \sqrt{x}+4\right) + C   F(x) = \frac{1}{5} (-8) \sqrt{2} \sqrt{\sqrt[4]{2} \sqrt{\sqrt{\cos \left(5 \sqrt{x}+4\right)+1}+\sqrt{2}}+2} \left(\sqrt{2} \cos \left(5 \sqrt{x}+4\right)-2 \sqrt{\sqrt{2} \sqrt{\cos \left(5 \sqrt{x}+4\right)+1} \left(\sqrt{\cos \left(5 \sqrt{x}+4\right)+1}+\sqrt{2}\right)+2 \sqrt{\cos \left(5 \sqrt{x}+4\right)+1}+\sqrt{2}}\right) \csc \left(5 \sqrt{x}+4\right) + C","['calculus', 'integration', 'indefinite-integrals', 'wolfram-alpha']"
79,"How should one understand the ""indefinite integral"" notation $\int f(x)\;dx$ in calculus?","How should one understand the ""indefinite integral"" notation  in calculus?",\int f(x)\;dx,"In calculus, it is said that $$ \int f(x)\; dx=F(x)\quad\text{means}\quad  F'(x)=f(x)\tag{1} $$ where $F$ is a differentiable function on some open integral $I$ . But the mean value theorem implies that any differentiable function $G:I\to \mathbb{R}$ with the property $G'(x)=f(x)$ on $I$ can be determined only up to a constant. Since the object on the right of the first equality of (1) is not unique, we cannot use (1) as a definition for the symbol $\int f(x)\;dx$ . Formulas for antiderivatives are usually written in the form of $\displaystyle \int f(x)\;dx=F(x)+C$ . For example, $$ \int \cos x\;dx = \sin x+C\;\tag{2} $$ where $C$ is some ""arbitrary"" constant. One cannot define an object with an ""arbitrary"" constant. It is OK to think about (2) as a set identity: $$ \int \cos x\; dx = \{g:\mathbb{R}\to\mathbb{R}\mid g(x)=\sin x+C,\; C\in\mathbb{R}\}. \tag{3} $$ So sometimes, people say that $\int f(x)\;dx$ really means a family of functions. But interpreting it this way, one runs into trouble of writing something like $$ \int (2x+\cos x) \; dx = \int 2x\;dx+\int \cos x\; dx = \{x^2+\sin x+C:C\in\mathbb{R}\}\;\tag{4} $$ where one is basically doing the addition of two sets in the middle, which is not defined. So how should one understand the ""indefinite integral"" notation $\int f(x)\;dx$ ? In particular, what kind of mathematical objects is that?","In calculus, it is said that where is a differentiable function on some open integral . But the mean value theorem implies that any differentiable function with the property on can be determined only up to a constant. Since the object on the right of the first equality of (1) is not unique, we cannot use (1) as a definition for the symbol . Formulas for antiderivatives are usually written in the form of . For example, where is some ""arbitrary"" constant. One cannot define an object with an ""arbitrary"" constant. It is OK to think about (2) as a set identity: So sometimes, people say that really means a family of functions. But interpreting it this way, one runs into trouble of writing something like where one is basically doing the addition of two sets in the middle, which is not defined. So how should one understand the ""indefinite integral"" notation ? In particular, what kind of mathematical objects is that?","
\int f(x)\; dx=F(x)\quad\text{means}\quad  F'(x)=f(x)\tag{1}
 F I G:I\to \mathbb{R} G'(x)=f(x) I \int f(x)\;dx \displaystyle \int f(x)\;dx=F(x)+C 
\int \cos x\;dx = \sin x+C\;\tag{2}
 C 
\int \cos x\; dx = \{g:\mathbb{R}\to\mathbb{R}\mid g(x)=\sin x+C,\; C\in\mathbb{R}\}. \tag{3}
 \int f(x)\;dx 
\int (2x+\cos x) \; dx = \int 2x\;dx+\int \cos x\; dx = \{x^2+\sin x+C:C\in\mathbb{R}\}\;\tag{4}
 \int f(x)\;dx",['calculus']
80,"What is the difference between the derivative (the Jacobian), and the differential?","What is the difference between the derivative (the Jacobian), and the differential?",,"Let $f:M \subset \mathbb{R}^2 \rightarrow N \subset \mathbb{R}^3$ . The function $f$ is a vector function. Its differential $\mathrm{d}f \in \mathbb{R}^3$ represents the infinitesimal change in the function, where by $\mathrm{d}f$ , I mean $\mathrm{d}f(x)$ . Its Jacobian (matrix) $J \in \mathbb{R}^{3 \times 2}$ maps vectors between tangent spaces $T_x M$ and $T_{f(x)} N$ . The relation between the two is $\mathrm{d}f = J dx$ , where $\mathrm{d}x \in \mathbb{R}^2$ . However, if $f$ is considered a ""mapping"", then is the differential of the mapping $\mathrm{d}f$ equal to the Jacobian $J$ ? From some of the answers, it seems that I took some things for granted (common knowledge or agreed by all). Moreover, there seems to be a confusion between differential, derivative, and their notation. So first, let's agree that the differential (total derivative) and the derivative (Jacobian) are not the same thing: Difference between differential and derivative https://en.wikipedia.org/wiki/Differential_of_a_function Next, as per Wikipedia , let's agree on notation. Each of $f'(x)$ , $D f(x)$ , and $\frac{\mathrm{d} f}{\mathrm{d} x}$ , and $J$ refers to the derivative.  The notation $\mathrm{d}f$ is reserved to denote the differential. Now, back to my question. The derivative of $f$ is the Jacobian matrix $f'(x)=Df=J \in \mathbb{R}^{3 \times 2}$ . The differential of $f$ is the 3D vector $\mathrm{d}f = J \mathrm{d}x$ . For some reason, there are people who confusingly use the term ""differential of a mapping"" to refer to the derivative, as if they don't distinguish between the derivative and the differential: https://en.wikipedia.org/wiki/Pushforward_(differential)#The_differential_of_a_smooth_map Differential of a Map My question is: What's up with that, and what am I missing? Why is that important: for a long time, I wasn't clear about what exactly the differential is. It became an issue when I used matrix calculus to calculate the Hessian of a matrix function. The book Matrix Differential Calculus with Applications in Statistics and Econometrics cleared it all up for me. It properly and distinctively defines the Jacobian, gradient, Hessian, derivative, and differential. The distinction between the Jacobian and differential is crucial for the matrix function differentiation process and the identification of the Jacobian (e.g. the first identification table in the book). At this point, I am mildly annoyed (with myself) that previously I wrote things (which are too late to fix now) and blindly (relying on previous work) used the term ""differential of a mapping"". So, currently, I either look for some justification for this misnomer or otherwise suggest to the community to reconsider it. I tried to track down the culprit for this ""weird fashion"", and I went as far as the differential geometry bible. Looking at do Carmo , definition 1 in chapter 2 appendix, pg. 128 (pg. 127 in the first edition), the definition of $dF_p$ is fine (grammar aside): it's a linear map that is associated with each point in the domain. But then, in example 10 (pg. 130), he uses the same notation to denote both Jacobian and differential. (This is probably what Ulrich meant by almost the same thing.) More specifically, he ""applies it twice"": once to get the Jacobian and once to get the differential. He uses $df(\cdot)$ to denote the Jacobian, a non-linear map into a matrix target, and $df_{(\cdot)}(\cdot)$ to denote the differential, a linear map into a vector target, and he calls both a differential. Another point why I find it confusing is that for me the Jacobian is a matrix of partial derivatives and the differential is an operator. For example, to differentiate the matrix function $f:\mathbb{R}^{2 \times 2} \rightarrow \mathbb{R}$ : $f(X) = tr AX$ I would use the differential operator: $df(X; dX) = tr AdX$ And from the Jacobian identification table (Magnus19), I'll get: $Df(X) = A'$ Note that the differential isn't a trivial linear map anymore. It also leads to another point. The differential has a linear approximation meaning. Basically, it denotes the change in the function. If it's a scalar value function, the change would be scalar, and thus the differential (would map to a scalar). If the domain is matrices, then the Jacobian is a matrix (a non-linear map from matrices to matrices). I definitely would find it confusing if someone would treat them the same. Let's do another example, $f:\mathbb{R}^{2 \times 2} \rightarrow \mathbb{R}^{2 \times 2}$ : $f(X) = AX$ Using the differential operator: $df(X; dX) = AdX$ $vec\ df(X; dX) = (I_2 \otimes A) vec\ dX$ From the Jacobian identification table: $Df(X) = I_2 \otimes A$ In this case, I'm not sure I'd consider the differential $df$ and Jacobian $Df$ almost the same thing (I'm not so good with tensors). This is the root of my issue. It's not always a simple matrix multiplication, and one needs to be mindful about the difference between the differential and Jacobian. Not to mention the second order differential and the Hessian identification. I corresponded with a couple of Caltech guys who settled it for me, and I can live with that. To paraphrase: Math is a living language like any other, it evolves and changes. As long as we clearly define the terms in the context, there shouldn't be a problem--call it whatever you want.","Let . The function is a vector function. Its differential represents the infinitesimal change in the function, where by , I mean . Its Jacobian (matrix) maps vectors between tangent spaces and . The relation between the two is , where . However, if is considered a ""mapping"", then is the differential of the mapping equal to the Jacobian ? From some of the answers, it seems that I took some things for granted (common knowledge or agreed by all). Moreover, there seems to be a confusion between differential, derivative, and their notation. So first, let's agree that the differential (total derivative) and the derivative (Jacobian) are not the same thing: Difference between differential and derivative https://en.wikipedia.org/wiki/Differential_of_a_function Next, as per Wikipedia , let's agree on notation. Each of , , and , and refers to the derivative.  The notation is reserved to denote the differential. Now, back to my question. The derivative of is the Jacobian matrix . The differential of is the 3D vector . For some reason, there are people who confusingly use the term ""differential of a mapping"" to refer to the derivative, as if they don't distinguish between the derivative and the differential: https://en.wikipedia.org/wiki/Pushforward_(differential)#The_differential_of_a_smooth_map Differential of a Map My question is: What's up with that, and what am I missing? Why is that important: for a long time, I wasn't clear about what exactly the differential is. It became an issue when I used matrix calculus to calculate the Hessian of a matrix function. The book Matrix Differential Calculus with Applications in Statistics and Econometrics cleared it all up for me. It properly and distinctively defines the Jacobian, gradient, Hessian, derivative, and differential. The distinction between the Jacobian and differential is crucial for the matrix function differentiation process and the identification of the Jacobian (e.g. the first identification table in the book). At this point, I am mildly annoyed (with myself) that previously I wrote things (which are too late to fix now) and blindly (relying on previous work) used the term ""differential of a mapping"". So, currently, I either look for some justification for this misnomer or otherwise suggest to the community to reconsider it. I tried to track down the culprit for this ""weird fashion"", and I went as far as the differential geometry bible. Looking at do Carmo , definition 1 in chapter 2 appendix, pg. 128 (pg. 127 in the first edition), the definition of is fine (grammar aside): it's a linear map that is associated with each point in the domain. But then, in example 10 (pg. 130), he uses the same notation to denote both Jacobian and differential. (This is probably what Ulrich meant by almost the same thing.) More specifically, he ""applies it twice"": once to get the Jacobian and once to get the differential. He uses to denote the Jacobian, a non-linear map into a matrix target, and to denote the differential, a linear map into a vector target, and he calls both a differential. Another point why I find it confusing is that for me the Jacobian is a matrix of partial derivatives and the differential is an operator. For example, to differentiate the matrix function : I would use the differential operator: And from the Jacobian identification table (Magnus19), I'll get: Note that the differential isn't a trivial linear map anymore. It also leads to another point. The differential has a linear approximation meaning. Basically, it denotes the change in the function. If it's a scalar value function, the change would be scalar, and thus the differential (would map to a scalar). If the domain is matrices, then the Jacobian is a matrix (a non-linear map from matrices to matrices). I definitely would find it confusing if someone would treat them the same. Let's do another example, : Using the differential operator: From the Jacobian identification table: In this case, I'm not sure I'd consider the differential and Jacobian almost the same thing (I'm not so good with tensors). This is the root of my issue. It's not always a simple matrix multiplication, and one needs to be mindful about the difference between the differential and Jacobian. Not to mention the second order differential and the Hessian identification. I corresponded with a couple of Caltech guys who settled it for me, and I can live with that. To paraphrase: Math is a living language like any other, it evolves and changes. As long as we clearly define the terms in the context, there shouldn't be a problem--call it whatever you want.",f:M \subset \mathbb{R}^2 \rightarrow N \subset \mathbb{R}^3 f \mathrm{d}f \in \mathbb{R}^3 \mathrm{d}f \mathrm{d}f(x) J \in \mathbb{R}^{3 \times 2} T_x M T_{f(x)} N \mathrm{d}f = J dx \mathrm{d}x \in \mathbb{R}^2 f \mathrm{d}f J f'(x) D f(x) \frac{\mathrm{d} f}{\mathrm{d} x} J \mathrm{d}f f f'(x)=Df=J \in \mathbb{R}^{3 \times 2} f \mathrm{d}f = J \mathrm{d}x dF_p df(\cdot) df_{(\cdot)}(\cdot) f:\mathbb{R}^{2 \times 2} \rightarrow \mathbb{R} f(X) = tr AX df(X; dX) = tr AdX Df(X) = A' f:\mathbb{R}^{2 \times 2} \rightarrow \mathbb{R}^{2 \times 2} f(X) = AX df(X; dX) = AdX vec\ df(X; dX) = (I_2 \otimes A) vec\ dX Df(X) = I_2 \otimes A df Df,"['calculus', 'differential-geometry']"
81,Beautiful relation between $\pi$ & $\phi$ via logarithmic integral.,Beautiful relation between  &  via logarithmic integral.,\pi \phi,"Given that $$\int_{1/\phi}^{1/\phi^2}{ \dfrac{\ln(1-x)}{x}}dx=\dfrac{\pi^2}{30}$$ Find the value of $$\int_{1/\phi}^{1/\phi^2} \left(\dfrac{\ln(1-x)}{x}\right)^2 dx$$ in terms of $\phi$ and $\pi$ . Where $\phi=\frac{1+\sqrt 5}{2}$ is the golden ratio. I have tried to do it by taylor series, also tried integration by parts but it is getting ugly and too many terms are coming. Source: Made by Prof. Raghava.","Given that Find the value of in terms of and . Where is the golden ratio. I have tried to do it by taylor series, also tried integration by parts but it is getting ugly and too many terms are coming. Source: Made by Prof. Raghava.",\int_{1/\phi}^{1/\phi^2}{ \dfrac{\ln(1-x)}{x}}dx=\dfrac{\pi^2}{30} \int_{1/\phi}^{1/\phi^2} \left(\dfrac{\ln(1-x)}{x}\right)^2 dx \phi \pi \phi=\frac{1+\sqrt 5}{2},"['calculus', 'integration', 'definite-integrals', 'contest-math', 'improper-integrals']"
82,Evaluate $\int_0^1x(\tan^{-1}x)^2~\textrm{d}x$,Evaluate,\int_0^1x(\tan^{-1}x)^2~\textrm{d}x,"Evaluate $\int\limits_0^1x(\tan^{-1}x)^2~\textrm{d}x$ My Attempt Let, $\tan^{-1}x=y\implies x=\tan y\implies dx=\sec^2y.dy=(1+\tan^2y)dy$ $$ \begin{align} &\int\limits_0^1x(\tan^{-1}x)^2dx=\int\limits_0^{\pi/4}\tan y.y^2.(1+\tan^2y)dy\\ &=\int\limits_0^{\pi/4}\tan y.y^2dy+\int\limits_0^{\pi/4}\tan^3y.y^2dy\\ &=\bigg[y^2.\log|\sec y|-\int2y.\log|\sec y|dy\bigg]+\int_0^{\pi/4}\tan^3y.y^2dy\\ &=\bigg[y^2.\log|\sec y|-y^2.\log|\sec y|+\int\frac{\tan y\sec y}{\sec y}y^2dy\bigg]+\int_0^{\pi/4}\tan^3y.y^2dy\\ \end{align} $$ How do I proceed further and solve the integration?","Evaluate $\int\limits_0^1x(\tan^{-1}x)^2~\textrm{d}x$ My Attempt Let, $\tan^{-1}x=y\implies x=\tan y\implies dx=\sec^2y.dy=(1+\tan^2y)dy$ $$ \begin{align} &\int\limits_0^1x(\tan^{-1}x)^2dx=\int\limits_0^{\pi/4}\tan y.y^2.(1+\tan^2y)dy\\ &=\int\limits_0^{\pi/4}\tan y.y^2dy+\int\limits_0^{\pi/4}\tan^3y.y^2dy\\ &=\bigg[y^2.\log|\sec y|-\int2y.\log|\sec y|dy\bigg]+\int_0^{\pi/4}\tan^3y.y^2dy\\ &=\bigg[y^2.\log|\sec y|-y^2.\log|\sec y|+\int\frac{\tan y\sec y}{\sec y}y^2dy\bigg]+\int_0^{\pi/4}\tan^3y.y^2dy\\ \end{align} $$ How do I proceed further and solve the integration?",,"['calculus', 'integration', 'indefinite-integrals', 'inverse-function', 'trigonometric-integrals']"
83,Are functions considered continuous at endpoints?,Are functions considered continuous at endpoints?,,"Consider a function $f(x)$ that has no jump, infinite, or removable discontinuities in the middle anywhere -- but maybe the domain is limited: Would the endpoint $[a,$ be considered continuous? What about the endpoint $(a,$? I ask because I often see ""a function is continuous if we can draw it without lifting up the pencil"" but I didn't know to what extent this applies to the endpoints and whether or not it matters if the points themselves are defined.","Consider a function $f(x)$ that has no jump, infinite, or removable discontinuities in the middle anywhere -- but maybe the domain is limited: Would the endpoint $[a,$ be considered continuous? What about the endpoint $(a,$? I ask because I often see ""a function is continuous if we can draw it without lifting up the pencil"" but I didn't know to what extent this applies to the endpoints and whether or not it matters if the points themselves are defined.",,"['calculus', 'limits', 'continuity', 'definition']"
84,How is the Dirac function different from the indicator function,How is the Dirac function different from the indicator function,,My question is straightforward though I cant find an answer online. What is the difference between an indicator function and a Dirac function?,My question is straightforward though I cant find an answer online. What is the difference between an indicator function and a Dirac function?,,"['calculus', 'elementary-functions']"
85,Why $\sin\left( \frac 1 x \right) $ oscillates infinitely many times as $x \to 0$,Why  oscillates infinitely many times as,\sin\left( \frac 1 x \right)  x \to 0,"Below, I have tried to prove why $\sin\left( \frac 1 x \right) $ starts oscillating infinitely many times as $x$ approaches zero. We know that the Sine function is an oscillating function. Let us assume a period $p$ such that, $$\sin\left(\frac 1 x \right) = \sin\left(\frac 1 {x+p}\right)$$ We can write, $$\frac 1 {x+p}=\frac 1 x - K$$ Where $K$ is some positive constant, $p$ is positive and $x>0$ (for simplicity[ period can be computed for both positive and negative values ]). On rearranging we get, $$p= \frac x {1-Kx} - x$$ Here, $p$ is a function of $x$ and is therefore not constant. Now, $$\lim_{x\rightarrow 0} p(x) = 0$$ The period falls to zero as $x$ tends to zero. The duration of each cycle becomes infinitesimal. In summary, as $x$ gets infinitely close to zero, the function oscillates infinitely many times. I find this understanding simple. But is this approach acceptable?","Below, I have tried to prove why $\sin\left( \frac 1 x \right) $ starts oscillating infinitely many times as $x$ approaches zero. We know that the Sine function is an oscillating function. Let us assume a period $p$ such that, $$\sin\left(\frac 1 x \right) = \sin\left(\frac 1 {x+p}\right)$$ We can write, $$\frac 1 {x+p}=\frac 1 x - K$$ Where $K$ is some positive constant, $p$ is positive and $x>0$ (for simplicity[ period can be computed for both positive and negative values ]). On rearranging we get, $$p= \frac x {1-Kx} - x$$ Here, $p$ is a function of $x$ and is therefore not constant. Now, $$\lim_{x\rightarrow 0} p(x) = 0$$ The period falls to zero as $x$ tends to zero. The duration of each cycle becomes infinitesimal. In summary, as $x$ gets infinitely close to zero, the function oscillates infinitely many times. I find this understanding simple. But is this approach acceptable?",,['calculus']
86,Trignometric integral : $\int \frac{dx}{\sin x + \sec x}$,Trignometric integral :,\int \frac{dx}{\sin x + \sec x},"The integral I am trying to compute is : $$\int \dfrac{dx}{\sin x + \sec x}$$ I have tried manipulating trignometric functions and it took me nowhere. Then finally I tried putting $\tan\dfrac{x}{2} = t$ , and subsequently: $$\cos x=\frac{1-t^2}{1+t^2} ,\sin x=\frac{2t}{1+t^2},\ dx=\frac{2dt}{1+t^2}$$ AND therefore: $$=\int\dfrac{2(1-t^2)}{(1+t^2)^2-2t(t^2-1)}dt$$ I cannot see how to approach after this, I know we have to factor out two quadratics, but cant see how to. Also, if there was another method instead of this substitution, please hint on that too! Thanks!","The integral I am trying to compute is : I have tried manipulating trignometric functions and it took me nowhere. Then finally I tried putting , and subsequently: AND therefore: I cannot see how to approach after this, I know we have to factor out two quadratics, but cant see how to. Also, if there was another method instead of this substitution, please hint on that too! Thanks!","\int \dfrac{dx}{\sin x + \sec x} \tan\dfrac{x}{2} = t \cos x=\frac{1-t^2}{1+t^2} ,\sin x=\frac{2t}{1+t^2},\ dx=\frac{2dt}{1+t^2} =\int\dfrac{2(1-t^2)}{(1+t^2)^2-2t(t^2-1)}dt","['calculus', 'integration', 'trigonometry', 'indefinite-integrals', 'trigonometric-integrals']"
87,Very interesting integral limit,Very interesting integral limit,,"I found this interesting problem on AoPS forum but no one has posted an answer. I have no idea how to solve it. $$ \int_0^\infty \sin(x^n)\,dx $$   For all positive rationals $n>1$, $I_n$ denotes the integral as above. If $P_n$ denotes the product   $$ P_n=\prod_{r=1}^{n-1}I_{\bigl(\!\frac{n}{r}\!\bigr)}\,, $$   then evaluate the following limit $L$   $$ L=\lim_{n\to\infty}\bigl(\sqrt{n}\,P_n\bigr)^{\frac{1}{n}} $$","I found this interesting problem on AoPS forum but no one has posted an answer. I have no idea how to solve it. $$ \int_0^\infty \sin(x^n)\,dx $$   For all positive rationals $n>1$, $I_n$ denotes the integral as above. If $P_n$ denotes the product   $$ P_n=\prod_{r=1}^{n-1}I_{\bigl(\!\frac{n}{r}\!\bigr)}\,, $$   then evaluate the following limit $L$   $$ L=\lim_{n\to\infty}\bigl(\sqrt{n}\,P_n\bigr)^{\frac{1}{n}} $$",,"['calculus', 'integration', 'limits']"
88,Using right-hand Riemann sum to evaluate the limit of $ \frac{n}{n^2+1}+ \cdots+\frac{n}{n^2+n^2}$,Using right-hand Riemann sum to evaluate the limit of, \frac{n}{n^2+1}+ \cdots+\frac{n}{n^2+n^2},"I'm asked to prove that $$\lim_{n \to \infty}\left(\frac{n}{n^2+1}+\frac{n}{n^2+4}+\frac{n}{n^2+9}+\cdots+\frac{n}{n^2+n^2}\right)=\frac{\pi}{4}$$ This looks like it can be solved with Riemann sums, so I proceed: \begin{align*} \lim_{n \to \infty}\left(\frac{n}{n^2+1}+\frac{n}{n^2+4}+\frac{n}{n^2+9}+\cdots+\frac{n}{n^2+n^2}\right)&=\lim_{n \to \infty} \sum_{k=1}^{n}\frac{n}{n^2+k^2}\\ &=\lim_{n \to \infty} \sum_{k=1}^{n}(\frac{1}{n})(\frac{n^2}{n^2+k^2})\\ &=\lim_{n \to \infty} \sum_{k=1}^{n}(\frac{1}{n})(\frac{1}{1+(k/n)^2})\\ &=\lim_{n \to \infty} \sum_{k=1}^{n}f(\frac{k}{n})(\frac{k-(k-1)}{n})\\ &=\int_{0}^{1}\frac{1}{1+x^2}dx=\frac{\pi}{4} \end{align*} where $f(x)=\frac{1}{1+x^2}$. Is this correct, are there any steps where I am not clear?","I'm asked to prove that $$\lim_{n \to \infty}\left(\frac{n}{n^2+1}+\frac{n}{n^2+4}+\frac{n}{n^2+9}+\cdots+\frac{n}{n^2+n^2}\right)=\frac{\pi}{4}$$ This looks like it can be solved with Riemann sums, so I proceed: \begin{align*} \lim_{n \to \infty}\left(\frac{n}{n^2+1}+\frac{n}{n^2+4}+\frac{n}{n^2+9}+\cdots+\frac{n}{n^2+n^2}\right)&=\lim_{n \to \infty} \sum_{k=1}^{n}\frac{n}{n^2+k^2}\\ &=\lim_{n \to \infty} \sum_{k=1}^{n}(\frac{1}{n})(\frac{n^2}{n^2+k^2})\\ &=\lim_{n \to \infty} \sum_{k=1}^{n}(\frac{1}{n})(\frac{1}{1+(k/n)^2})\\ &=\lim_{n \to \infty} \sum_{k=1}^{n}f(\frac{k}{n})(\frac{k-(k-1)}{n})\\ &=\int_{0}^{1}\frac{1}{1+x^2}dx=\frac{\pi}{4} \end{align*} where $f(x)=\frac{1}{1+x^2}$. Is this correct, are there any steps where I am not clear?",,"['calculus', 'limits', 'summation', 'riemann-sum']"
89,A question regarding the constant of integration,A question regarding the constant of integration,,"Consider the indefinite integral $\int\sin(2x) dx$. There appear to be three seemingly different answers once this integral is evaluated. These are $\frac{-1}{2} \cos(2x) + C$, $\sin^2(x) + C$ and $-\cos^2(x) + C$. This is because all three functions differ only by a constant and thus differentiating them yields $\sin(2x)$. Is it therefore simply a matter of convention which one we decide to choose? In a somewhat more artificial sense, one could say $\int\ 2x dx = (x+n)(x-n) + C$ for any $n \in  \mathbb{R}$. Why this is invalid makes more sense because we expand it and add $-n^2$ to $C$ which will give the accepted answer of $x^2 + C$ (the new $C$ is different, of course). Is this simply done for convenience? Can this rule be generalized/formalized in some way such that more tricky functions, in which the extra constant isn't quite as conspicuous as it is with basic polynomials, can be tackled?","Consider the indefinite integral $\int\sin(2x) dx$. There appear to be three seemingly different answers once this integral is evaluated. These are $\frac{-1}{2} \cos(2x) + C$, $\sin^2(x) + C$ and $-\cos^2(x) + C$. This is because all three functions differ only by a constant and thus differentiating them yields $\sin(2x)$. Is it therefore simply a matter of convention which one we decide to choose? In a somewhat more artificial sense, one could say $\int\ 2x dx = (x+n)(x-n) + C$ for any $n \in  \mathbb{R}$. Why this is invalid makes more sense because we expand it and add $-n^2$ to $C$ which will give the accepted answer of $x^2 + C$ (the new $C$ is different, of course). Is this simply done for convenience? Can this rule be generalized/formalized in some way such that more tricky functions, in which the extra constant isn't quite as conspicuous as it is with basic polynomials, can be tackled?",,"['calculus', 'integration']"
90,How to differentiate $y=\sqrt{\frac{1+x}{1-x}}$?,How to differentiate ?,y=\sqrt{\frac{1+x}{1-x}},I'm trying to solve this problem but I think I'm missing something. Here's what I've done so far: $$g(x) = \frac{1+x}{1-x}$$ $$u = 1+x$$ $$u' = 1$$ $$v = 1-x$$ $$v' = -1$$ $$g'(x) = \frac{(1-x) -(-1)(1+x)}{(1-x)^2}$$ $$g'(x) = \frac{1-x+1+x}{(1-x)^2}$$ $$g'(x) = \frac{2}{(1-x)^2}$$ $$y' = \frac{1}{2}(\frac{1+x}{1-x})^{-\frac{1}{2}}(\frac{2}{(1-x)^2}) $$,I'm trying to solve this problem but I think I'm missing something. Here's what I've done so far: $$g(x) = \frac{1+x}{1-x}$$ $$u = 1+x$$ $$u' = 1$$ $$v = 1-x$$ $$v' = -1$$ $$g'(x) = \frac{(1-x) -(-1)(1+x)}{(1-x)^2}$$ $$g'(x) = \frac{1-x+1+x}{(1-x)^2}$$ $$g'(x) = \frac{2}{(1-x)^2}$$ $$y' = \frac{1}{2}(\frac{1+x}{1-x})^{-\frac{1}{2}}(\frac{2}{(1-x)^2}) $$,,"['calculus', 'derivatives']"
91,What is $\lim _{x\to \infty }\left(\frac{e^x}{x^n}\right)$?,What is ?,\lim _{x\to \infty }\left(\frac{e^x}{x^n}\right),What is $\lim _{x\to \infty }\left(\frac{e^x}{x^n}\right)$ ?,What is $\lim _{x\to \infty }\left(\frac{e^x}{x^n}\right)$ ?,,"['calculus', 'limits']"
92,Integral $\int \tan^{5}(x)\text{ d}x$,Integral,\int \tan^{5}(x)\text{ d}x,"I would like guidance on evaluating $$\displaystyle\int \tan^{5}(x)\text{ d}x\text{.}$$ I have attempted using the Pythagorean identities to get $$\int\tan^{5}(x)\text{ d}x = \int \tan(x)\left[\sec^{2}(x)-1\right]^{2}\text{ d}x\text{.}$$ This doesn't look helpful. So I thought, why not turn only ONE of the $\tan^{2}$ terms to the $\sec^{2}-1$ form? This gives $$\int\tan^{5}(x)\text{ d}x = \int\tan^{3}(x)\sec^{2}(x)\text{ d}x-\int \tan^{3}(x)\text{ d}x\text{.}$$ Clearly the second term is $\dfrac{\tan^{4}(x)}{4}$ (ignoring the constant term for now). Using a similar trick, $$\begin{align} \int\tan^{3}(x)\text{ d}x &= \int \tan(x)\sec^{2}(x)\text{ d}x-\int\tan(x)\text{ d}x \\ &= \dfrac{\tan^{2}(x)}{2} - (-1)\ln|\cos(x)| \\ &= \dfrac{\tan^{2}(x)}{2}+\ln|\cos(x)|\text{.} \end{align}$$ So this suggests to me that  $$\int\tan^{5}(x)\text{ d}x = \dfrac{\tan^{4}(x)}{4} - \dfrac{\tan^{2}(x)}{2}-\ln|\cos(x)| + C\text{.}$$ But the answer in Stewart (section 7.2., #31) is  $$\dfrac{1}{4}\sec^{4}(x)-\tan^{2}(x)+\ln|\sec(x)|+C\text{.}$$ It's very clear where the $\ln|\sec(x)|$ term is coming from - and I tried to take the difference of my answer and Stewart's answer using Wolfram Alpha and unfortunately, the difference is not a constant. Where did I go wrong?","I would like guidance on evaluating $$\displaystyle\int \tan^{5}(x)\text{ d}x\text{.}$$ I have attempted using the Pythagorean identities to get $$\int\tan^{5}(x)\text{ d}x = \int \tan(x)\left[\sec^{2}(x)-1\right]^{2}\text{ d}x\text{.}$$ This doesn't look helpful. So I thought, why not turn only ONE of the $\tan^{2}$ terms to the $\sec^{2}-1$ form? This gives $$\int\tan^{5}(x)\text{ d}x = \int\tan^{3}(x)\sec^{2}(x)\text{ d}x-\int \tan^{3}(x)\text{ d}x\text{.}$$ Clearly the second term is $\dfrac{\tan^{4}(x)}{4}$ (ignoring the constant term for now). Using a similar trick, $$\begin{align} \int\tan^{3}(x)\text{ d}x &= \int \tan(x)\sec^{2}(x)\text{ d}x-\int\tan(x)\text{ d}x \\ &= \dfrac{\tan^{2}(x)}{2} - (-1)\ln|\cos(x)| \\ &= \dfrac{\tan^{2}(x)}{2}+\ln|\cos(x)|\text{.} \end{align}$$ So this suggests to me that  $$\int\tan^{5}(x)\text{ d}x = \dfrac{\tan^{4}(x)}{4} - \dfrac{\tan^{2}(x)}{2}-\ln|\cos(x)| + C\text{.}$$ But the answer in Stewart (section 7.2., #31) is  $$\dfrac{1}{4}\sec^{4}(x)-\tan^{2}(x)+\ln|\sec(x)|+C\text{.}$$ It's very clear where the $\ln|\sec(x)|$ term is coming from - and I tried to take the difference of my answer and Stewart's answer using Wolfram Alpha and unfortunately, the difference is not a constant. Where did I go wrong?",,"['calculus', 'integration', 'indefinite-integrals', 'trigonometric-integrals']"
93,Understanding of the formal and intuitive definition of a limit,Understanding of the formal and intuitive definition of a limit,,"The intuitive definition for $\lim\limits_{x \to a} f(x) = L$ is the value of $f ( x )$ can be made arbitrarily close to $L$ by making $x$ sufficiently close,  but not equal to, $a$ . I can easily understand this ,but for the (ε, δ)-definition of limit:For every real ε > 0, there exists a real δ > 0 such that for all real x, 0 < | x − a | < δ implies | f(x) − L | < ε. Oh, god, I cannot understand it completely. As it is the formal definition of limit, I think it should be precise but somewhat should include the mean of the above intuitive definition, so as for ""$f ( x )$ can be made arbitrarily close to $L$"" in the intuitive definition correspond to “For every real ε > 0,| f(x) − L | < ε” in the formal definition, it's fine! but does  “making $x$ sufficiently close,  but not equal to, $a$ .” correspond to “there exists a real δ > 0 such that for all real x, 0 < | x − a | < δ” ? This is the point I cannot understand, because I am not sure if  “there exists a real δ > 0 such that for all real x, 0 < | x − a | < δ”  shows ""x close enough, but not equal, to $a$"". The other question is: does the biggest δ also get smaller as ε is getting smaller? Why? (Exclude the case when f(x) is a constant function.) Why do we need the formal definition of a limit? Does the intuitive definition have some flaw? P.S. Thank you everyone, but I must declare I only have some basic knowledge of limit, I only started to learn calculus a few days ago.","The intuitive definition for $\lim\limits_{x \to a} f(x) = L$ is the value of $f ( x )$ can be made arbitrarily close to $L$ by making $x$ sufficiently close,  but not equal to, $a$ . I can easily understand this ,but for the (ε, δ)-definition of limit:For every real ε > 0, there exists a real δ > 0 such that for all real x, 0 < | x − a | < δ implies | f(x) − L | < ε. Oh, god, I cannot understand it completely. As it is the formal definition of limit, I think it should be precise but somewhat should include the mean of the above intuitive definition, so as for ""$f ( x )$ can be made arbitrarily close to $L$"" in the intuitive definition correspond to “For every real ε > 0,| f(x) − L | < ε” in the formal definition, it's fine! but does  “making $x$ sufficiently close,  but not equal to, $a$ .” correspond to “there exists a real δ > 0 such that for all real x, 0 < | x − a | < δ” ? This is the point I cannot understand, because I am not sure if  “there exists a real δ > 0 such that for all real x, 0 < | x − a | < δ”  shows ""x close enough, but not equal, to $a$"". The other question is: does the biggest δ also get smaller as ε is getting smaller? Why? (Exclude the case when f(x) is a constant function.) Why do we need the formal definition of a limit? Does the intuitive definition have some flaw? P.S. Thank you everyone, but I must declare I only have some basic knowledge of limit, I only started to learn calculus a few days ago.",,"['calculus', 'limits', 'definition']"
94,Are Gilbert Strang's books on calculus and linear algebra suitable for math majors?,Are Gilbert Strang's books on calculus and linear algebra suitable for math majors?,,"I would like to know what are the best resources to use to teach and learn elementary subjects (calculus, linear algebra). I remember when learning calculus, I used Spivak's book, which had wonderful problems, but the explanations were not really memorable, the illustrations weren't that appealing and it didn't have enough applications. I would like to know the mathematician's opinion on Strang's books on calculus and linear algebra and the others. Are they not fit to be recommended to math majors? Are they rigorous on any level? What other books do you recommend that have what these don't? Also, I think that learning through examples first and some visualisation is superior to putting generalities upfront with countless formulas Therefore if you know any book that's written in this vein, please post it. P.S. Strang's books stress the importance of matrix notation. Is that good? Does it add any insight?","I would like to know what are the best resources to use to teach and learn elementary subjects (calculus, linear algebra). I remember when learning calculus, I used Spivak's book, which had wonderful problems, but the explanations were not really memorable, the illustrations weren't that appealing and it didn't have enough applications. I would like to know the mathematician's opinion on Strang's books on calculus and linear algebra and the others. Are they not fit to be recommended to math majors? Are they rigorous on any level? What other books do you recommend that have what these don't? Also, I think that learning through examples first and some visualisation is superior to putting generalities upfront with countless formulas Therefore if you know any book that's written in this vein, please post it. P.S. Strang's books stress the importance of matrix notation. Is that good? Does it add any insight?",,"['calculus', 'reference-request', 'soft-question', 'education', 'book-recommendation']"
95,How to evaluate the following integral? $\int \ln(e^x + c)~\mathrm dx$,How to evaluate the following integral?,\int \ln(e^x + c)~\mathrm dx,"I can't seem to find an answer for this kind of integration, and I'd like to know if there is an answer for it, and if yes what is it. $$\int \ln(e^x + {c})~\mathrm dx\,,$$ where $c$ is a constant. My teacher keeps avoiding me after I asked him, so I appreciate any help because I've tried everything I know.","I can't seem to find an answer for this kind of integration, and I'd like to know if there is an answer for it, and if yes what is it. $$\int \ln(e^x + {c})~\mathrm dx\,,$$ where $c$ is a constant. My teacher keeps avoiding me after I asked him, so I appreciate any help because I've tried everything I know.",,"['calculus', 'integration', 'indefinite-integrals']"
96,"Compute integral $\int_0^1\int_0^1\int_0^1 \sqrt{x^2+y^2+z^2} \,\mathrm{d}x\mathrm{d}y\mathrm{d}z$",Compute integral,"\int_0^1\int_0^1\int_0^1 \sqrt{x^2+y^2+z^2} \,\mathrm{d}x\mathrm{d}y\mathrm{d}z","How to Compute  $$\int_0^1\int_0^1\int_0^1 \sqrt{x^2+y^2+z^2} \,\mathrm{d}x\mathrm{d}y\mathrm{d}z $$ The Mathematica 9.01 give a result is $$-\frac{\pi}{24}+\frac{1}{4}(\sqrt{3}+\log(7+4\sqrt{3})).$$ I want to know how to get it.","How to Compute  $$\int_0^1\int_0^1\int_0^1 \sqrt{x^2+y^2+z^2} \,\mathrm{d}x\mathrm{d}y\mathrm{d}z $$ The Mathematica 9.01 give a result is $$-\frac{\pi}{24}+\frac{1}{4}(\sqrt{3}+\log(7+4\sqrt{3})).$$ I want to know how to get it.",,"['calculus', 'integration']"
97,"Is formal logic necessary for pure/""higher"" mathematics?","Is formal logic necessary for pure/""higher"" mathematics?",,"I'm asking this as an autodidact who wants to learn math rigorously for its own sake. And I was just wondering if understanding proofs could be achieved without a formal grounding in symbolic logic. I ask because I have all the books I need but I simply don't have the patience I'd like to have for the formal logic book, as I'm itching to get into continuous math. And to clarify, I'm interested predominantly in calculus/analysis, ODEs/PDES, and differential geometry.","I'm asking this as an autodidact who wants to learn math rigorously for its own sake. And I was just wondering if understanding proofs could be achieved without a formal grounding in symbolic logic. I ask because I have all the books I need but I simply don't have the patience I'd like to have for the formal logic book, as I'm itching to get into continuous math. And to clarify, I'm interested predominantly in calculus/analysis, ODEs/PDES, and differential geometry.",,"['calculus', 'logic', 'self-learning', 'education', 'foundations']"
98,is the following a decreasing function?,is the following a decreasing function?,,"I am stuck on figuring out why the following function is a decreasing function when I read a paper. The function is following  $$f(x)=-\frac{1}{x}\log[{pe^{-ax}+(1-p)e^{-bx}}]$$  where $a$ and $b$ are two arbitrary non-zero constants (can be negative or positive), but $a$ is not equal to $b$ and $p \in(0,1)$.  I am stuck on figuring out why $f(x)$ is globally decreasing in $x \in R$ for any choice of $a, b$. I find the first derivative as follows, for any  $a, b$. $$f'(x)=\frac{1}{x^2}\log[{pe^{-ax}+(1-p)e^{-bx}}]+\frac{1}{x}\frac{ape^{-ax}+b(1-p)e^{-bx}}{{pe^{-ax}+(1-p)e^{-bx}}}$$  But then I get stuck, how to show it's negative for all $x \in R$? I use mathematica to  plot this function, and it is indeed decreasing in $x$, but I can't proceed further on how to show this analytically Any hint or help is extremely appreciated! Edited: To make thing more interesting..in fact I am considering the case $a$ is not equal to $b$ and $p$ is interior in (0,1)..","I am stuck on figuring out why the following function is a decreasing function when I read a paper. The function is following  $$f(x)=-\frac{1}{x}\log[{pe^{-ax}+(1-p)e^{-bx}}]$$  where $a$ and $b$ are two arbitrary non-zero constants (can be negative or positive), but $a$ is not equal to $b$ and $p \in(0,1)$.  I am stuck on figuring out why $f(x)$ is globally decreasing in $x \in R$ for any choice of $a, b$. I find the first derivative as follows, for any  $a, b$. $$f'(x)=\frac{1}{x^2}\log[{pe^{-ax}+(1-p)e^{-bx}}]+\frac{1}{x}\frac{ape^{-ax}+b(1-p)e^{-bx}}{{pe^{-ax}+(1-p)e^{-bx}}}$$  But then I get stuck, how to show it's negative for all $x \in R$? I use mathematica to  plot this function, and it is indeed decreasing in $x$, but I can't proceed further on how to show this analytically Any hint or help is extremely appreciated! Edited: To make thing more interesting..in fact I am considering the case $a$ is not equal to $b$ and $p$ is interior in (0,1)..",,"['calculus', 'functions', 'derivatives']"
99,question about the limit $\lim_{h\to0}\frac{\arcsin(x+h)-\arcsin(x)}{h}$,question about the limit,\lim_{h\to0}\frac{\arcsin(x+h)-\arcsin(x)}{h},"Because $\sin'(x)=\cos(x)$ we can prove that $\arcsin'(x)=\frac{1}{\sqrt{1-x^2}}$. but, by definition we have $$\arcsin'(x)=\lim_{h\to0}\frac{\arcsin(x+h)-\arcsin(x)}{h}\tag{1}$$ therefore, $$\lim_{h\to0}\frac{\arcsin(x+h)-\arcsin(x)}{h}=\frac{1}{\sqrt{1-x^2}}\tag{2}$$ My question: Can we prove $(2)$ without using $(1)$? How?","Because $\sin'(x)=\cos(x)$ we can prove that $\arcsin'(x)=\frac{1}{\sqrt{1-x^2}}$. but, by definition we have $$\arcsin'(x)=\lim_{h\to0}\frac{\arcsin(x+h)-\arcsin(x)}{h}\tag{1}$$ therefore, $$\lim_{h\to0}\frac{\arcsin(x+h)-\arcsin(x)}{h}=\frac{1}{\sqrt{1-x^2}}\tag{2}$$ My question: Can we prove $(2)$ without using $(1)$? How?",,['calculus']
