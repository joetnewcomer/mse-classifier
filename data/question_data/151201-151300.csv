,title_raw,title_text,title_latex,body_raw,body_text,body_latex,tags
0,"Absolute Continuity, Lipschitz","Absolute Continuity, Lipschitz",,"I'm studying for a qualifying exam and can't seem to solve this problem.  Any suggestions would be appreciated! Let $f:[a,b] \rightarrow \mathbb R$ be absolutely continuous.  Show, for each $\epsilon>0$, that there is a uniformly Lipschitz function(global) $g:[a,b] \rightarrow \mathbb R$ such that $|f(x)-g(x)|<\epsilon$ for all $x\in [a,b]$.","I'm studying for a qualifying exam and can't seem to solve this problem.  Any suggestions would be appreciated! Let $f:[a,b] \rightarrow \mathbb R$ be absolutely continuous.  Show, for each $\epsilon>0$, that there is a uniformly Lipschitz function(global) $g:[a,b] \rightarrow \mathbb R$ such that $|f(x)-g(x)|<\epsilon$ for all $x\in [a,b]$.",,"['analysis', 'measure-theory', 'continuity']"
1,Why do you need tensors of rank $>2$?,Why do you need tensors of rank ?,>2,"Question from someone just starting to study tensors (sorry if it's silly): So I understand (maybe?) that tensors are basically about coordinate transformations (and things that are invariant under said transformations), and that in writing out a tensor, we use notation that represents functions that perform the coordinate transformations. But when I see a tensor of, say, rank 3 or 4 written out, it looks like we're jumping from one coordinate system to another to another, before we arrive from the original coordinate system to the final one. If you're only really starting at one coordinate system, and ending at another, why can't you treat each tensor as just a single coordinate transformation, i.e. a function that takes you directly from the original coordinate system to the final one?","Question from someone just starting to study tensors (sorry if it's silly): So I understand (maybe?) that tensors are basically about coordinate transformations (and things that are invariant under said transformations), and that in writing out a tensor, we use notation that represents functions that perform the coordinate transformations. But when I see a tensor of, say, rank 3 or 4 written out, it looks like we're jumping from one coordinate system to another to another, before we arrive from the original coordinate system to the final one. If you're only really starting at one coordinate system, and ending at another, why can't you treat each tensor as just a single coordinate transformation, i.e. a function that takes you directly from the original coordinate system to the final one?",,"['analysis', 'tensor-products', 'tensors', 'tensor-rank']"
2,Equivalence of norms on the space of smooth functions,Equivalence of norms on the space of smooth functions,,"Let $E, F$ be Banach spaces, $A$ be an open set in $E$ and $C^2(A,F)$ be the space of all functions $f:A\to F,$ which are twice continuously differentiable and bounded with all derivatives. The question is when following two norms in $C^2(A,F)$ are equivalent: $$ \|f\|_{1}=\sup_{x\in A}\sum^2_{k=0}\|f^{(k)}(x)\|, \ \|f\|_{2}=\sup_{x\in A}(\|f(x)\|+\|f^{(2)}(x)\|). $$ In the case $A=E$ they are equivalent. One can prove it in the following way. To bound $\|f^{(1)}(x)h\|$ consider a line $g(t)=f(x+th)$ through $x$ in the direction $h$ and use inequality $$ \sup_{t\in \mathbb{R}}\|g^{(1)}(t)\|\leq\sqrt{2\sup_{t\in \mathbb{R}}\|g(t)\|\sup_{t\in \mathbb{R}}\|g^{(2)}(t)\|}. $$ The case when $A$ is an open ball is unknown to me. Of course, one can try to consider not lines but segments. But the problem is the length of segment can't be bounded from below and inequalitites I know can't be applied.","Let $E, F$ be Banach spaces, $A$ be an open set in $E$ and $C^2(A,F)$ be the space of all functions $f:A\to F,$ which are twice continuously differentiable and bounded with all derivatives. The question is when following two norms in $C^2(A,F)$ are equivalent: $$ \|f\|_{1}=\sup_{x\in A}\sum^2_{k=0}\|f^{(k)}(x)\|, \ \|f\|_{2}=\sup_{x\in A}(\|f(x)\|+\|f^{(2)}(x)\|). $$ In the case $A=E$ they are equivalent. One can prove it in the following way. To bound $\|f^{(1)}(x)h\|$ consider a line $g(t)=f(x+th)$ through $x$ in the direction $h$ and use inequality $$ \sup_{t\in \mathbb{R}}\|g^{(1)}(t)\|\leq\sqrt{2\sup_{t\in \mathbb{R}}\|g(t)\|\sup_{t\in \mathbb{R}}\|g^{(2)}(t)\|}. $$ The case when $A$ is an open ball is unknown to me. Of course, one can try to consider not lines but segments. But the problem is the length of segment can't be bounded from below and inequalitites I know can't be applied.",,['analysis']
3,Square integrable gradient and equicontinuity,Square integrable gradient and equicontinuity,,"Background .I have been studying the proof of the unboundedness of the eigenvalues of a quadratic functional $$I[\phi]=\int_{\Omega}\left(p|\nabla\phi|^2+q\phi^2\right)d\boldsymbol{x}$$ subject to $$H[\phi]=\int_{\Omega}\rho\phi^2d\boldsymbol{x}=1$$ ($p,q,\rho>0$, $p\in C^1(\Omega)$) The crux of the argument by contradiction as set out in Courant & Hilbert and originally due to Rellich is that assuming the eigenvalues $\lambda_n=I[\phi_n]$ are bounded ($\phi_n$ being the corresponding eigenfunction) it is possible to extract a uniformly convergence subsequence of the eigenfunctions. Thus $$\lim_{m,n\to\infty}H[\phi_m-\phi_n]=0$$ whereas from orthonormality condition it follows that $$H[\phi_m-\phi_n]=2$$ for $n\ne m$. In this they rely on Arzela-Ascoli theorem calling it ""accumulation principle"". In one-dimensional case C&H prove uniform boundedness and equicontinuity directly, for example, using Schwarz inequality $$f(x_1)-f(x_2)=\int_{x_1}^{x_2}f'(x)dx \le\sqrt{\int_{x_1}^{x_2}f'^2(x)dx\cdot\int_{x_1}^{x_2}1^2dx}$$ Hence, if $\int_{x_1}^{x_2}f'^2(x)dx<M$ we have $$(f(x_1)-f(x_2))^2<M|x_2-x_1|$$ In case of 2 independent variables C&H refer to a lemma due to Rellich proved in the above mentioned article. This lemma asserts that if a for a set of functions $\phi(x,y)$ defined in the domain $\Omega$ $$\int_{\Omega}\phi^2dxdy\quad \text{and} \quad \int_{\Omega}(\phi_x^2+\phi_y^2)dxdy$$ are uniformly bounded it is possible to select a subsequence $\phi_n$ such that $$\lim_{m,n\to\infty}\int_{\Omega}(\phi_m-\phi_n)^2dxdy=0$$ What I tried. As opposed to Rellich's procedure I attempted to extend naively the above one-dimensional argument to the case of many variables to prove equicontinuity, hoping to invoke an generalised Arcela-Ascoli theorem to complete the task Consider the difference of the values of $\phi$   at two distinct points: $$\begin{aligned}\phi\left(x_{2},y_{2}\right)-\phi\left(x_{1},y_{1}\right) & =\phi\left(x_{2},y_{2}\right)-\phi\left(x_{2},y_{1}\right)+\phi\left(x_{2},y_{1}\right)-\phi\left(x_{1},y_{1}\right)\\  & =\int_{y_{1}}^{y_{2}}\left.\frac{\partial\phi}{\partial y}\right|_{x=x_{2}}dy+\int_{x_{1}}^{x_{2}}\left.\frac{\partial\phi}{\partial x}\right|_{y=y_{1}}dx \end{aligned}$$ Writing $\phi_{x}\left(x,y_{1}\right)=\left.\frac{\partial\phi}{\partial x}\right|_{y=y_{1}}$   and $\phi_{y}\left(x_{2},y\right)=\left.\frac{\partial\phi}{\partial y}\right|_{x=x_{2}}$   and Applying Cauchy-Schwarz inequality:$$\begin{aligned}\phi\left(x_{2},y_{2}\right)-\phi\left(x_{1},y_{1}\right) & \le\sqrt{\int_{y_{1}}^{y_{2}}\phi_{y}^{2}\left(x_{2},y\right)dy\cdot\int_{y_{1}}^{y_{2}}1^{2}dy}\\  & +\sqrt{\int_{x_{1}}^{x_{2}}\phi_{x}^{2}\left(x,y_{1}\right)dx\cdot\int_{x_{1}}^{x_{2}}1^{2}dx}\\  & =\sqrt{\int_{y_{1}}^{y_{2}}\phi_{y}^{2}\left(x_{2},y\right)dy}\cdot\sqrt{y_{2}-y_{1}}\\  & +\sqrt{\int_{x_{1}}^{x_{2}}\phi_{x}^{2}\left(x,y_{1}\right)dx}\cdot\sqrt{x_{2}-x_{1}} \end{aligned}$$ Now transform the last expression as follows:$$\begin{array}{cc} \phi\left(x_{2},y_{2}\right)-\phi\left(x_{1},y_{1}\right) & \le\sqrt{\int_{x_{1}}^{x_{2}}\int_{y_{1}}^{y_{2}}\phi_{y}^{2}\left(x_{2},y\right)dxdy}\cdot\sqrt{\frac{y_{2}-y_{1}}{x_{2}-x_{1}}}\\  & +\sqrt{\int_{x_{1}}^{x_{2}}\int_{y_{1}}^{y_{2}}\phi_{x}^{2}\left(x,y_{1}\right)dxdy}\sqrt{\frac{x_{2}-x_{1}}{y_{2}-y_{1}}} \end{array}$$ Applying HÃ¶lder's inequality:$$\phi\left(x_{2},y_{2}\right)-\phi\left(x_{1},y_{1}\right)\le\sqrt{\iint_{g}\left[\phi_{x}^{2}\left(x,y_{1}\right)+\phi_{y}^{2}\left(x_{2},y\right)\right]dxdy}\frac{\sqrt{\left(x_{2}-x_{1}\right)^{2}+\left(y_{2}-y_{1}\right)^{2}}}{\sqrt{S\left(g\right)}}$$ Since the integral of the squared gradient is uniformly bounded, we deduce: $$\sqrt{S\left(g\right)}\phi\left(x_{2},y_{2}\right)-\sqrt{S\left(g\right)}\phi\left(x_{1},y_{1}\right)\le C\sqrt{\left(x_{2}-x_{1}\right)^{2}+\left(y_{2}-y_{1}\right)^{2}}$$ Where $S(g)$ is the area of the rectangle. Where I got stuck . Now that I end up with an expression somewhat similar to the one that turns out in Rellich's article, I don't know how to proceed and whether it makes sense to. Will the generalised Arzela-Ascoli be really applicable here? What assumptions am I making, or should I make with regards to $\Omega$? If this draft argument is valid what would be the next step? Iterate the procedure in some way? This article by Terence Tao says that the answer to my attempt is ""barely no"", but what does it actually mean? Many thanks in advance for those who reads this till the end.","Background .I have been studying the proof of the unboundedness of the eigenvalues of a quadratic functional $$I[\phi]=\int_{\Omega}\left(p|\nabla\phi|^2+q\phi^2\right)d\boldsymbol{x}$$ subject to $$H[\phi]=\int_{\Omega}\rho\phi^2d\boldsymbol{x}=1$$ ($p,q,\rho>0$, $p\in C^1(\Omega)$) The crux of the argument by contradiction as set out in Courant & Hilbert and originally due to Rellich is that assuming the eigenvalues $\lambda_n=I[\phi_n]$ are bounded ($\phi_n$ being the corresponding eigenfunction) it is possible to extract a uniformly convergence subsequence of the eigenfunctions. Thus $$\lim_{m,n\to\infty}H[\phi_m-\phi_n]=0$$ whereas from orthonormality condition it follows that $$H[\phi_m-\phi_n]=2$$ for $n\ne m$. In this they rely on Arzela-Ascoli theorem calling it ""accumulation principle"". In one-dimensional case C&H prove uniform boundedness and equicontinuity directly, for example, using Schwarz inequality $$f(x_1)-f(x_2)=\int_{x_1}^{x_2}f'(x)dx \le\sqrt{\int_{x_1}^{x_2}f'^2(x)dx\cdot\int_{x_1}^{x_2}1^2dx}$$ Hence, if $\int_{x_1}^{x_2}f'^2(x)dx<M$ we have $$(f(x_1)-f(x_2))^2<M|x_2-x_1|$$ In case of 2 independent variables C&H refer to a lemma due to Rellich proved in the above mentioned article. This lemma asserts that if a for a set of functions $\phi(x,y)$ defined in the domain $\Omega$ $$\int_{\Omega}\phi^2dxdy\quad \text{and} \quad \int_{\Omega}(\phi_x^2+\phi_y^2)dxdy$$ are uniformly bounded it is possible to select a subsequence $\phi_n$ such that $$\lim_{m,n\to\infty}\int_{\Omega}(\phi_m-\phi_n)^2dxdy=0$$ What I tried. As opposed to Rellich's procedure I attempted to extend naively the above one-dimensional argument to the case of many variables to prove equicontinuity, hoping to invoke an generalised Arcela-Ascoli theorem to complete the task Consider the difference of the values of $\phi$   at two distinct points: $$\begin{aligned}\phi\left(x_{2},y_{2}\right)-\phi\left(x_{1},y_{1}\right) & =\phi\left(x_{2},y_{2}\right)-\phi\left(x_{2},y_{1}\right)+\phi\left(x_{2},y_{1}\right)-\phi\left(x_{1},y_{1}\right)\\  & =\int_{y_{1}}^{y_{2}}\left.\frac{\partial\phi}{\partial y}\right|_{x=x_{2}}dy+\int_{x_{1}}^{x_{2}}\left.\frac{\partial\phi}{\partial x}\right|_{y=y_{1}}dx \end{aligned}$$ Writing $\phi_{x}\left(x,y_{1}\right)=\left.\frac{\partial\phi}{\partial x}\right|_{y=y_{1}}$   and $\phi_{y}\left(x_{2},y\right)=\left.\frac{\partial\phi}{\partial y}\right|_{x=x_{2}}$   and Applying Cauchy-Schwarz inequality:$$\begin{aligned}\phi\left(x_{2},y_{2}\right)-\phi\left(x_{1},y_{1}\right) & \le\sqrt{\int_{y_{1}}^{y_{2}}\phi_{y}^{2}\left(x_{2},y\right)dy\cdot\int_{y_{1}}^{y_{2}}1^{2}dy}\\  & +\sqrt{\int_{x_{1}}^{x_{2}}\phi_{x}^{2}\left(x,y_{1}\right)dx\cdot\int_{x_{1}}^{x_{2}}1^{2}dx}\\  & =\sqrt{\int_{y_{1}}^{y_{2}}\phi_{y}^{2}\left(x_{2},y\right)dy}\cdot\sqrt{y_{2}-y_{1}}\\  & +\sqrt{\int_{x_{1}}^{x_{2}}\phi_{x}^{2}\left(x,y_{1}\right)dx}\cdot\sqrt{x_{2}-x_{1}} \end{aligned}$$ Now transform the last expression as follows:$$\begin{array}{cc} \phi\left(x_{2},y_{2}\right)-\phi\left(x_{1},y_{1}\right) & \le\sqrt{\int_{x_{1}}^{x_{2}}\int_{y_{1}}^{y_{2}}\phi_{y}^{2}\left(x_{2},y\right)dxdy}\cdot\sqrt{\frac{y_{2}-y_{1}}{x_{2}-x_{1}}}\\  & +\sqrt{\int_{x_{1}}^{x_{2}}\int_{y_{1}}^{y_{2}}\phi_{x}^{2}\left(x,y_{1}\right)dxdy}\sqrt{\frac{x_{2}-x_{1}}{y_{2}-y_{1}}} \end{array}$$ Applying HÃ¶lder's inequality:$$\phi\left(x_{2},y_{2}\right)-\phi\left(x_{1},y_{1}\right)\le\sqrt{\iint_{g}\left[\phi_{x}^{2}\left(x,y_{1}\right)+\phi_{y}^{2}\left(x_{2},y\right)\right]dxdy}\frac{\sqrt{\left(x_{2}-x_{1}\right)^{2}+\left(y_{2}-y_{1}\right)^{2}}}{\sqrt{S\left(g\right)}}$$ Since the integral of the squared gradient is uniformly bounded, we deduce: $$\sqrt{S\left(g\right)}\phi\left(x_{2},y_{2}\right)-\sqrt{S\left(g\right)}\phi\left(x_{1},y_{1}\right)\le C\sqrt{\left(x_{2}-x_{1}\right)^{2}+\left(y_{2}-y_{1}\right)^{2}}$$ Where $S(g)$ is the area of the rectangle. Where I got stuck . Now that I end up with an expression somewhat similar to the one that turns out in Rellich's article, I don't know how to proceed and whether it makes sense to. Will the generalised Arzela-Ascoli be really applicable here? What assumptions am I making, or should I make with regards to $\Omega$? If this draft argument is valid what would be the next step? Iterate the procedure in some way? This article by Terence Tao says that the answer to my attempt is ""barely no"", but what does it actually mean? Many thanks in advance for those who reads this till the end.",,"['analysis', 'multivariable-calculus']"
4,"Prove $\|F(x,Â·)\|_{L^{4,1}(E)}^2 \leq C\|F(x,Â·)\|_{L^{\infty}(E)}\|F(x,Â·)\|_{L^{2,2}(E)}$",Prove,"\|F(x,Â·)\|_{L^{4,1}(E)}^2 \leq C\|F(x,Â·)\|_{L^{\infty}(E)}\|F(x,Â·)\|_{L^{2,2}(E)}","How to derive this inequality? $$\|F(x,Â·)\|_{L^{4,1}(E)}^2 \leq C\|F(x,Â·)\|_{L^{\infty}(E)}\|F(x,Â·)\|_{L^{2,2}(E)},$$  where $C$ is constant and $$\|F(x,Â·)\|_{L^{p,n}(E)}=\left(\int_E\displaystyle\sum_{m\leq n}\left|\partial_y^m F(x,y)\right|^pdy\right)^{1/p},$$ $E$ is a bounded domain of $\mathbb{R}^2$.","How to derive this inequality? $$\|F(x,Â·)\|_{L^{4,1}(E)}^2 \leq C\|F(x,Â·)\|_{L^{\infty}(E)}\|F(x,Â·)\|_{L^{2,2}(E)},$$  where $C$ is constant and $$\|F(x,Â·)\|_{L^{p,n}(E)}=\left(\int_E\displaystyle\sum_{m\leq n}\left|\partial_y^m F(x,y)\right|^pdy\right)^{1/p},$$ $E$ is a bounded domain of $\mathbb{R}^2$.",,"['analysis', 'inequality', 'normed-spaces']"
5,Two questions from Dixmier's book on Von Neumann algebras,Two questions from Dixmier's book on Von Neumann algebras,,"It seems something is going wrong with the preview I linked in some of my previous questions, so I will just type out the question.  I am having trouble with Dixmier's proof of Corollary 5 on p. 46.  That one states ""Let $A$ be a Von Neumann Algebra, and $m$ a two-sided ideal of A, and $\bar m$ its weak closure.  For each $T \in (\bar m)^+$ there exists an increasing [net] $F \subset m^+$ such that $T$ is the supremum of $F$.""  He then proves it as follows: ""Let $(T_i)_{i \in I}$ be a maximal family of non-zero operators of $m^+$ such that $\sum_{i \in J} T_i \leq T$ for every finite subset $J$ of $I$.  The operators $\sum_{i \in J} T_i \leq T$ form an increasing [net in $m^+$] whose supremum $S$ is an element of $(\bar m )^+$ majorized by $T$.  Let $R=T-S \in (\bar m)^+$.  As $A \in m$ converges weakly to the greatest projection $E$ of $\bar m$, $R^{1/2}AR^{1/2}$ converges weakly to $R^{1/2}ER^{1/2}=R$; hence if $R \neq 0$, we have $R^{1/2}AR^{1/2}\neq 0$ for some $A \in m$, hence for some $A \in m^+$, such that $0 \leq A \leq I$.  But we then have $R^{1/2}AR^{1/2}\leq R$, $R^{1/2}AR^{1/2} \in m^+$ and this contradicts the maximality of the family $(T_i)_{i \in I}$.  Hence $R=0$."" Everything here is fine in my book until he speaks at the very end as if the $R^{1/2}AR^{1/2}$ he furnishes is not redundant with any of the $T_i$.  That is where I don't understand why this proof works. Another question I have is on the first page of Chapter 4.  Let $A$ be a *-algebra of operators on $H$.  He says that all positive linear functionals on $A$ $\phi$ (not necessarily norm continuous) turn out to be norm continuous, with norm $\phi(I)$  He argues for this using the following string of inequalities.  I disagree with the second inequality: $|\phi(T)|^2 \leq \phi(I)\phi(T^*T) \leq \phi(I)^2 ||T^*T||=\phi(I)^2||T||^2$. Can you explain why the second inequality holds?","It seems something is going wrong with the preview I linked in some of my previous questions, so I will just type out the question.  I am having trouble with Dixmier's proof of Corollary 5 on p. 46.  That one states ""Let $A$ be a Von Neumann Algebra, and $m$ a two-sided ideal of A, and $\bar m$ its weak closure.  For each $T \in (\bar m)^+$ there exists an increasing [net] $F \subset m^+$ such that $T$ is the supremum of $F$.""  He then proves it as follows: ""Let $(T_i)_{i \in I}$ be a maximal family of non-zero operators of $m^+$ such that $\sum_{i \in J} T_i \leq T$ for every finite subset $J$ of $I$.  The operators $\sum_{i \in J} T_i \leq T$ form an increasing [net in $m^+$] whose supremum $S$ is an element of $(\bar m )^+$ majorized by $T$.  Let $R=T-S \in (\bar m)^+$.  As $A \in m$ converges weakly to the greatest projection $E$ of $\bar m$, $R^{1/2}AR^{1/2}$ converges weakly to $R^{1/2}ER^{1/2}=R$; hence if $R \neq 0$, we have $R^{1/2}AR^{1/2}\neq 0$ for some $A \in m$, hence for some $A \in m^+$, such that $0 \leq A \leq I$.  But we then have $R^{1/2}AR^{1/2}\leq R$, $R^{1/2}AR^{1/2} \in m^+$ and this contradicts the maximality of the family $(T_i)_{i \in I}$.  Hence $R=0$."" Everything here is fine in my book until he speaks at the very end as if the $R^{1/2}AR^{1/2}$ he furnishes is not redundant with any of the $T_i$.  That is where I don't understand why this proof works. Another question I have is on the first page of Chapter 4.  Let $A$ be a *-algebra of operators on $H$.  He says that all positive linear functionals on $A$ $\phi$ (not necessarily norm continuous) turn out to be norm continuous, with norm $\phi(I)$  He argues for this using the following string of inequalities.  I disagree with the second inequality: $|\phi(T)|^2 \leq \phi(I)\phi(T^*T) \leq \phi(I)^2 ||T^*T||=\phi(I)^2||T||^2$. Can you explain why the second inequality holds?",,"['analysis', 'operator-theory', 'operator-algebras', 'von-neumann-algebras']"
6,Is periodic extension of Lipschitz function Lipschitz?,Is periodic extension of Lipschitz function Lipschitz?,,"Let $f: [0,T] \rightarrow \mathbb{R}$, where $T>0$, be a Lipschitz with constant $K$ and $f(0)=f(T)$.  Let us define $g(x)=f(x)$ for $x \in [0,T]$ and $g(x+T)=g(x)$ for $x \in \mathbb{R}$. Does $g$ satisfies $$|g(x)-g(y)| \leq K |x-y|$$ for $x,y \in \mathbb{R}$? It is clear that we may assume that $x<y$. When $|x-y| \geq T$ then there are $n,m\in N$ such that  $x-mT, y-nT\in [0,T]$ and $|g(x)-g(y)|=|f(x-mT)-f(y-nT)| \leq K \cdot T \leq K|x-y|$. When $|x-y|<T$ and, for some integer $n$, is $x,y \in [nT, (n+1)T]$ we have $|g(x)-g(y)|=|f(x-nT)-f(y-nT)|\leq K |x-y|$. It remains the case when  $|x-y| <T$ and, for some integer $n$, is  $x\in [nT,(n+1)T]$,  $y\in [(n+1)T, (n+2)T]$.","Let $f: [0,T] \rightarrow \mathbb{R}$, where $T>0$, be a Lipschitz with constant $K$ and $f(0)=f(T)$.  Let us define $g(x)=f(x)$ for $x \in [0,T]$ and $g(x+T)=g(x)$ for $x \in \mathbb{R}$. Does $g$ satisfies $$|g(x)-g(y)| \leq K |x-y|$$ for $x,y \in \mathbb{R}$? It is clear that we may assume that $x<y$. When $|x-y| \geq T$ then there are $n,m\in N$ such that  $x-mT, y-nT\in [0,T]$ and $|g(x)-g(y)|=|f(x-mT)-f(y-nT)| \leq K \cdot T \leq K|x-y|$. When $|x-y|<T$ and, for some integer $n$, is $x,y \in [nT, (n+1)T]$ we have $|g(x)-g(y)|=|f(x-nT)-f(y-nT)|\leq K |x-y|$. It remains the case when  $|x-y| <T$ and, for some integer $n$, is  $x\in [nT,(n+1)T]$,  $y\in [(n+1)T, (n+2)T]$.",,['analysis']
7,Evaluation of $L^p$ function,Evaluation of  function,L^p,"Functions in $L^p$ are only defined $Âµ$-almost everywhere, so for a given evaluation point $x$, $F(x)$, $f\in L^p$ can be changed to any value, so in general it would not be well-definied to just write $y=f(x)$. (Every representant $f$ of the equivalence class $F$ can be evaluated, okay. But this doesn't help.) What is needed to have a well-defined evaluation mapping $(F,x) \mapsto F(x)$ , and how is this problem solved canonically ?  It would be enough for example that F is piecewise continuous, then I could define the evualation at every continuous part. But isn't much less (for example piecewise one-sided continuity?) enough? And if you take for example Sobolev spaces, and consider the weak derivative of $F:x \mapsto |x|$. Does it follow from the definition of weak derivative in Sobolev spaces that we choose the continuous representant of $F'$ in $L^p$, and define the evaluation of the weak derivative only on the continuous part? How is this problem handled?","Functions in $L^p$ are only defined $Âµ$-almost everywhere, so for a given evaluation point $x$, $F(x)$, $f\in L^p$ can be changed to any value, so in general it would not be well-definied to just write $y=f(x)$. (Every representant $f$ of the equivalence class $F$ can be evaluated, okay. But this doesn't help.) What is needed to have a well-defined evaluation mapping $(F,x) \mapsto F(x)$ , and how is this problem solved canonically ?  It would be enough for example that F is piecewise continuous, then I could define the evualation at every continuous part. But isn't much less (for example piecewise one-sided continuity?) enough? And if you take for example Sobolev spaces, and consider the weak derivative of $F:x \mapsto |x|$. Does it follow from the definition of weak derivative in Sobolev spaces that we choose the continuous representant of $F'$ in $L^p$, and define the evaluation of the weak derivative only on the continuous part? How is this problem handled?",,"['analysis', 'measure-theory', 'sobolev-spaces']"
8,"How to show that $\int \phi \,d\mu=-\int\phi'(x)f(x)\,dx$",How to show that,"\int \phi \,d\mu=-\int\phi'(x)f(x)\,dx","Assume that $f\colon \Bbb R \rightarrow\Bbb R$ is left-continuous nondecreasing and let $\mu$ be a Borel measure in $\Bbb R$ such that $\mu([a,b))=f(b)-f(a)$ for $a<b$, $a,b \in\Bbb R$. I would like to prove that  $$\int \phi \,d\mu=-\int\phi'(x)f(x)\,dx$$ for each $\phi\colon \Bbb R\rightarrow\Bbb R$ smooth with compact support. How to show that LHS and RHS in the above equality are equal $$\int \int_{ \{(x,y)\in\Bbb R\times\Bbb R:x<y\} } \phi'(x)\,dx \,d\mu(y)  \  ?$$","Assume that $f\colon \Bbb R \rightarrow\Bbb R$ is left-continuous nondecreasing and let $\mu$ be a Borel measure in $\Bbb R$ such that $\mu([a,b))=f(b)-f(a)$ for $a<b$, $a,b \in\Bbb R$. I would like to prove that  $$\int \phi \,d\mu=-\int\phi'(x)f(x)\,dx$$ for each $\phi\colon \Bbb R\rightarrow\Bbb R$ smooth with compact support. How to show that LHS and RHS in the above equality are equal $$\int \int_{ \{(x,y)\in\Bbb R\times\Bbb R:x<y\} } \phi'(x)\,dx \,d\mu(y)  \  ?$$",,"['analysis', 'measure-theory']"
9,Completeness and separability of LÃ©vy's metric,Completeness and separability of LÃ©vy's metric,,"Let $D$ be the set of all functions $F: \mathbb{R} \rightarrow \mathbb{R}$ which are nondecreasing, left-hand-side continuous and $\lim_{x \rightarrow -\infty} F(x)=0$ and  $\lim_{x \rightarrow \infty} F(x)=1$. Let $d$ be a LÃ©vy metric in $D$, that is: $$d(F,G)=\inf \{ e >0: G(x-e)-e \leq F(x) \leq G(x+e)+e\text{ for }x\in \mathbb{R} \}\;.$$ How to prove completeness and separability of $(D, d)$ ? I know that a sequence  $(F_n)$ from $D$ is convergent to $F$ from $D$ iff $\lim_{n\rightarrow \infty} F_n(x)=F(x)$ in each $x \in \mathbb{R}$ in which $F$ is continuous.","Let $D$ be the set of all functions $F: \mathbb{R} \rightarrow \mathbb{R}$ which are nondecreasing, left-hand-side continuous and $\lim_{x \rightarrow -\infty} F(x)=0$ and  $\lim_{x \rightarrow \infty} F(x)=1$. Let $d$ be a LÃ©vy metric in $D$, that is: $$d(F,G)=\inf \{ e >0: G(x-e)-e \leq F(x) \leq G(x+e)+e\text{ for }x\in \mathbb{R} \}\;.$$ How to prove completeness and separability of $(D, d)$ ? I know that a sequence  $(F_n)$ from $D$ is convergent to $F$ from $D$ iff $\lim_{n\rightarrow \infty} F_n(x)=F(x)$ in each $x \in \mathbb{R}$ in which $F$ is continuous.",,"['probability', 'analysis', 'metric-spaces']"
10,"Show $g(a)= \operatorname*{arg}_{b \in B} \, f(a,b)=0$ is continuous when $g(a)$ is single-valued and $f$ continuous",Show  is continuous when  is single-valued and  continuous,"g(a)= \operatorname*{arg}_{b \in B} \, f(a,b)=0 g(a) f","Let $A$ and $B$ be two compact subsets of $\mathbb{R}$. Let $f:A \times B \to \mathbb{R}$ be a continuous function on $A \times B$. For each $a\in A$, define $B_a=\{b\in B:f(a,b)=0\}$, and suppose each $B_a$ is a singleton, so we may define a $g:A\to\Bbb R$ such that $g(a)$ is the unique element of $B_a$ for each $a\in A$. Is $g$ a continuous function of $a$? I tried the following. But I have the feeling it's not correct. Assume $g$ is not continuous at $a \in A$, then there exists a sequence $\{a_n\} \subset A$ converging to $a$ in $A$, for which the sequence $\{b_n\}=\{g(a_n)\} \subset B$ does not converges to $b=g(a)$. Since $B$ is compact, by the Bolzano-Weierstrass theorem, $\{b_n\}$ has a subsequence $\{b'_n\}$  converging to some $b'\ne b$ as $\{b_n\}$ does not converges to $b$.  Let $\{a'_n\} \subset A$ be the subsequence of $\{a_n\}$ induces by $\{b'_n\}$. Since $\{a_n\}$ converges to $a$ then every subsequence of $\{a_n\}$ converges to $a$ and $\{a'_n\}$ converges to $a$. Since $g(a)$ contains a unique element $b$, then  $b'=b$ which is a contraction since $\{b'_n\}$ is a subsequence of $\{b_n\}$. (The previous sentence seems suspicious). The claim follows.","Let $A$ and $B$ be two compact subsets of $\mathbb{R}$. Let $f:A \times B \to \mathbb{R}$ be a continuous function on $A \times B$. For each $a\in A$, define $B_a=\{b\in B:f(a,b)=0\}$, and suppose each $B_a$ is a singleton, so we may define a $g:A\to\Bbb R$ such that $g(a)$ is the unique element of $B_a$ for each $a\in A$. Is $g$ a continuous function of $a$? I tried the following. But I have the feeling it's not correct. Assume $g$ is not continuous at $a \in A$, then there exists a sequence $\{a_n\} \subset A$ converging to $a$ in $A$, for which the sequence $\{b_n\}=\{g(a_n)\} \subset B$ does not converges to $b=g(a)$. Since $B$ is compact, by the Bolzano-Weierstrass theorem, $\{b_n\}$ has a subsequence $\{b'_n\}$  converging to some $b'\ne b$ as $\{b_n\}$ does not converges to $b$.  Let $\{a'_n\} \subset A$ be the subsequence of $\{a_n\}$ induces by $\{b'_n\}$. Since $\{a_n\}$ converges to $a$ then every subsequence of $\{a_n\}$ converges to $a$ and $\{a'_n\}$ converges to $a$. Since $g(a)$ contains a unique element $b$, then  $b'=b$ which is a contraction since $\{b'_n\}$ is a subsequence of $\{b_n\}$. (The previous sentence seems suspicious). The claim follows.",,['real-analysis']
11,Dedekind complete â Sequentially complete,Dedekind complete â Sequentially complete,,"Let F be an ordered field with least upper bound property. 1.Let $\alpha: \mathbb{N} \to F$ be a Cauchy sequence. Since F is an ordered field, $x$ is bounded both above and below. 2.By assumption and dual of it, $A$={$\alpha(n)$|$n\in \mathbb{N}$} has a inf $a_0$ and sup $b_0$. 3.F is Archimedean 4.If subsequence of a Cauchy sequence is convergent to $a\in F$ then the Cauchy sequence is convergent to $a\in F$ These are all i know.. How do I prove all Cauchy sequences are convergent in $F$? Please consider my level. I want quite a direct proof not mentioning any topology & Cauchy net. *Comment button is not available to me now, (I don't know why), so i write this here. I just proved it with facts that (i)every cauchy sequence is convergent in the set of Cauchy reals and (ii)there exists a bijective homomorphism between two dedekind complete fields and (iii)the set of Cauchy reals is dedekind complete. Let $x:iâx(i):\mathbb{N}âF$ be a cauchy sequence in dedekind complete field $F$. Then use the bijective homomorphism $f$ to show that $x':iâf(x(i))$ is a cauchy sequence in the set of Cauchy reals. By the fact (i), $x'$ is convergent. Since inverse of $f$ is also homomorphism, use this to show that $x$ is convergent.","Let F be an ordered field with least upper bound property. 1.Let $\alpha: \mathbb{N} \to F$ be a Cauchy sequence. Since F is an ordered field, $x$ is bounded both above and below. 2.By assumption and dual of it, $A$={$\alpha(n)$|$n\in \mathbb{N}$} has a inf $a_0$ and sup $b_0$. 3.F is Archimedean 4.If subsequence of a Cauchy sequence is convergent to $a\in F$ then the Cauchy sequence is convergent to $a\in F$ These are all i know.. How do I prove all Cauchy sequences are convergent in $F$? Please consider my level. I want quite a direct proof not mentioning any topology & Cauchy net. *Comment button is not available to me now, (I don't know why), so i write this here. I just proved it with facts that (i)every cauchy sequence is convergent in the set of Cauchy reals and (ii)there exists a bijective homomorphism between two dedekind complete fields and (iii)the set of Cauchy reals is dedekind complete. Let $x:iâx(i):\mathbb{N}âF$ be a cauchy sequence in dedekind complete field $F$. Then use the bijective homomorphism $f$ to show that $x':iâf(x(i))$ is a cauchy sequence in the set of Cauchy reals. By the fact (i), $x'$ is convergent. Since inverse of $f$ is also homomorphism, use this to show that $x$ is convergent.",,"['analysis', 'field-theory']"
12,Schwartz class estimation.,Schwartz class estimation.,,"I have a function $f\in \mathcal{S}$ (i.e of Schwartz class), and I want to show there exist constants $C,k>0$ s.t  $$\|f\|_p \leq C(\sup_{x\in \mathbb{R}} |f(x)| + \sup_{\mathbb{x\in \mathbb{R}}} |x^k f(x)|)$$ for every $ p \in [1,\infty]$. For $p=1,\infty$ it's obvious from definition, I mean I can take f with compact support and this will prove for $p=1$, for $p=\infty$ it's trivial. But for $ p \in (1,\infty)$ I find myself at a mess, I need to do integration by parts inductively but I don't seem to find the right approach, I guess I need to use here Leibnitz general product rule, but I don't see how to come to suitable constants. Any help , is appreciated.","I have a function $f\in \mathcal{S}$ (i.e of Schwartz class), and I want to show there exist constants $C,k>0$ s.t  $$\|f\|_p \leq C(\sup_{x\in \mathbb{R}} |f(x)| + \sup_{\mathbb{x\in \mathbb{R}}} |x^k f(x)|)$$ for every $ p \in [1,\infty]$. For $p=1,\infty$ it's obvious from definition, I mean I can take f with compact support and this will prove for $p=1$, for $p=\infty$ it's trivial. But for $ p \in (1,\infty)$ I find myself at a mess, I need to do integration by parts inductively but I don't seem to find the right approach, I guess I need to use here Leibnitz general product rule, but I don't see how to come to suitable constants. Any help , is appreciated.",,"['real-analysis', 'analysis', 'fourier-analysis']"
13,How do I show this formula involving several variables?,How do I show this formula involving several variables?,,"This is from Woll's ""Functions of Several Variables,"" but there's no proof. If $g$ is of class $C^k$ ($k \ge 2$) on a convex open set $U$ about $p$ in $\mathbb{R}^d$, then for each $q \in U$, $ g(q) = g(p) + \sum_{i=1}^d \frac{\partial g}{\partial r_i} \bigg|_p (r_i(q) - r_i(p)) + \sum_{i,j} (r_i(q) - r_i(p)) (r_j(q) - r_j(p)) \int_0^1 (1-t) \frac{\partial^2g}{\partial r_i \partial r_j} \bigg|_{p + t(q - p)} dt. $ It looks like Taylor's or mean value theorem. I especially don't understand the integral part.","This is from Woll's ""Functions of Several Variables,"" but there's no proof. If $g$ is of class $C^k$ ($k \ge 2$) on a convex open set $U$ about $p$ in $\mathbb{R}^d$, then for each $q \in U$, $ g(q) = g(p) + \sum_{i=1}^d \frac{\partial g}{\partial r_i} \bigg|_p (r_i(q) - r_i(p)) + \sum_{i,j} (r_i(q) - r_i(p)) (r_j(q) - r_j(p)) \int_0^1 (1-t) \frac{\partial^2g}{\partial r_i \partial r_j} \bigg|_{p + t(q - p)} dt. $ It looks like Taylor's or mean value theorem. I especially don't understand the integral part.",,['analysis']
14,A way to split this integral/norm,A way to split this integral/norm,,"My last question hasn't got any replies so I'll try another.. Is there a way to split the following integral ($g$ is arbitrary) $$\int{f^2g}$$ so that I instead have an expression involving the $L^2$ norm on $f$ and either $L^2$ or $L^\infty$ norm on $g$? In particular, I want something like $$A\lVert f \rVert_{L^2}^2 + B\lVert g \rVert^c_{L^p} \leq \int{f^2g}$$ where $A$ and $B$ and $c$ are constants. I tried to use Holder's inequality but that gives me an upper bound instead.","My last question hasn't got any replies so I'll try another.. Is there a way to split the following integral ($g$ is arbitrary) $$\int{f^2g}$$ so that I instead have an expression involving the $L^2$ norm on $f$ and either $L^2$ or $L^\infty$ norm on $g$? In particular, I want something like $$A\lVert f \rVert_{L^2}^2 + B\lVert g \rVert^c_{L^p} \leq \int{f^2g}$$ where $A$ and $B$ and $c$ are constants. I tried to use Holder's inequality but that gives me an upper bound instead.",,"['analysis', 'integration']"
15,Does the inequality $0\leq a\leq b$ in a C*-algebra imply $\|a\|\leq\|b\|$?,Does the inequality  in a C*-algebra imply ?,0\leq a\leq b \|a\|\leq\|b\|,"In relation to this question of mine: C* algebra inequalities I am wondering if it is true that if $0\leq a \leq b$ in a C* algebra, does one have $||a||\leq||b||$?  If you need the C* algebra to be unital you can assume that since I could use the C* algebra unitization.  If it is not true, please state any reasonable conditions under which it is true because I see things like this used all the time, and it is possible that it merely applies in those special cases.","In relation to this question of mine: C* algebra inequalities I am wondering if it is true that if $0\leq a \leq b$ in a C* algebra, does one have $||a||\leq||b||$?  If you need the C* algebra to be unital you can assume that since I could use the C* algebra unitization.  If it is not true, please state any reasonable conditions under which it is true because I see things like this used all the time, and it is possible that it merely applies in those special cases.",,"['analysis', 'operator-theory', 'operator-algebras']"
16,Bilogarithmic function,Bilogarithmic function,,"I have shown that the power series $$\sum\limits_{i=1}^\infty \frac{x^n}{n^2} = -\int\limits_{0}^x \frac{\log(1-t)}{t}dt$$ for $x\in (-1,1)$. How can I show that this is also true for the boundary: $$\sum\limits_{k=1}^\infty \frac{1}{n^2}=-\int\limits_{0}^1 \frac{\log(1-t)}{t}dt$$ I hope somebody can help :) Best regards","I have shown that the power series $$\sum\limits_{i=1}^\infty \frac{x^n}{n^2} = -\int\limits_{0}^x \frac{\log(1-t)}{t}dt$$ for $x\in (-1,1)$. How can I show that this is also true for the boundary: $$\sum\limits_{k=1}^\infty \frac{1}{n^2}=-\int\limits_{0}^1 \frac{\log(1-t)}{t}dt$$ I hope somebody can help :) Best regards",,"['real-analysis', 'analysis']"
17,Converging sequence and subsequences,Converging sequence and subsequences,,"How might we rigorously argue that if we have a sequence $\{x_n\}\subset X$ such that every subsequence of it has a convergence subsequence that tends to $a$ and $X$ is a compact set then $\{x_n\}$ converges to $a$? In my mind, I am thinking that if otherwise then then we can pick a subsequence with such that all terms lie at least a finite distance away from $a$ then there will be no subsequence that converges to $a$. Is this a valid argument? How might I make it ""rigorous""? Thank you.","How might we rigorously argue that if we have a sequence $\{x_n\}\subset X$ such that every subsequence of it has a convergence subsequence that tends to $a$ and $X$ is a compact set then $\{x_n\}$ converges to $a$? In my mind, I am thinking that if otherwise then then we can pick a subsequence with such that all terms lie at least a finite distance away from $a$ then there will be no subsequence that converges to $a$. Is this a valid argument? How might I make it ""rigorous""? Thank you.",,['analysis']
18,Periodic solutions of $f^{(m)}=g$,Periodic solutions of,f^{(m)}=g,"Let $g:R \rightarrow R$ be continuous and $ 2\pi$-periodic, let $m \in N$. How many solution in  class of $m$-times continuously differentiable $2\pi$-periodic functions has equation $$f^{(m)}=g ?$$ Edit. Obviously, if $f$ is a solution in this class and $C$ is a constant then $f+C$ is also a solution. Are there another solutions?","Let $g:R \rightarrow R$ be continuous and $ 2\pi$-periodic, let $m \in N$. How many solution in  class of $m$-times continuously differentiable $2\pi$-periodic functions has equation $$f^{(m)}=g ?$$ Edit. Obviously, if $f$ is a solution in this class and $C$ is a constant then $f+C$ is also a solution. Are there another solutions?",,"['calculus', 'analysis']"
19,"The concept of gradient, related to lagrange multipliers, surface areas, tangent hyper planes","The concept of gradient, related to lagrange multipliers, surface areas, tangent hyper planes",,"As we all know, gradient is always perpendicular to the level curve. On the other hand, $\nabla f(a,b) \dot\ h$ where $h=(x-a\     \ \ \ \ \ y-b)^T$, give a  tangent hyper plane which is tangent to the point on a surface, you can find in many texts that such hyper plane is in general, $z=f(a,b)+\nabla f(a,b) \dot\ h$ at some point $(a,b,f(a,b))$.  For instance, when we are calculating the surface area, we would denote $T_s\triangle s$ and $T_t\triangle t$ as the vector which is spanning a paralleogram to approximate the rectangle $ \triangle s \times \triangle t$ where $T_s =\frac{\partial F}{\partial s} $ and F is the surface. In lagrange multiplier, we know f has extremum point if $\nabla f= \lambda\nabla g$, from this expression, we know the grandient f should be perpendicular to the level set g, where it is the domain of f. From the proof, the idea is if $\nabla f $ is not parallel to $\nabla g$ then there is change in f, but here come the question, why if $\nabla f $ is not parallel to $\nabla g$, then there will be a change in f either increase or decrease. Any explaination to gradients would be appreciate.d","As we all know, gradient is always perpendicular to the level curve. On the other hand, $\nabla f(a,b) \dot\ h$ where $h=(x-a\     \ \ \ \ \ y-b)^T$, give a  tangent hyper plane which is tangent to the point on a surface, you can find in many texts that such hyper plane is in general, $z=f(a,b)+\nabla f(a,b) \dot\ h$ at some point $(a,b,f(a,b))$.  For instance, when we are calculating the surface area, we would denote $T_s\triangle s$ and $T_t\triangle t$ as the vector which is spanning a paralleogram to approximate the rectangle $ \triangle s \times \triangle t$ where $T_s =\frac{\partial F}{\partial s} $ and F is the surface. In lagrange multiplier, we know f has extremum point if $\nabla f= \lambda\nabla g$, from this expression, we know the grandient f should be perpendicular to the level set g, where it is the domain of f. From the proof, the idea is if $\nabla f $ is not parallel to $\nabla g$ then there is change in f, but here come the question, why if $\nabla f $ is not parallel to $\nabla g$, then there will be a change in f either increase or decrease. Any explaination to gradients would be appreciate.d",,"['calculus', 'analysis', 'multivariable-calculus']"
20,Distance between bounded and compact sets,Distance between bounded and compact sets,,"Let $(X,d)$ be a metric space and define for $B\subset X$ bounded, i.e. $$\operatorname{diam}(B)= \sup \{ d(x,y) \colon x,y\in B \} < \infty,$$ the measure $$\beta(B) = \inf\{r > 0\colon\text{there exist finitely many balls of radius r which cover } B\},$$ or equivalently, $$\beta(B)=\inf\big\lbrace r > 0|\exists N=N(r)\in{\bf N} \text{ and } x_1,\ldots x_N\in X\colon B\subset\bigcup_{k=1}^N B(x_k,r)\big\rbrace,$$ where $B(x,r)=\{y\in X\colon d(x,y)\}$ denotes the open ball of radius $r$ centered at $x\in X$. Let ${\bf K}(X)$ denote the collection of (non-empty) compact subsets in $X$. I would like to prove $$\beta(B)=d_H\big(B,{\bf K}(X)\big),$$ where $d_H$ is the Hausdorff distance . I proved $d_H\big(B,{\bf K}(X)\big)\le\beta(B)$. Is there someone that knows how to prove the other inequality?","Let $(X,d)$ be a metric space and define for $B\subset X$ bounded, i.e. $$\operatorname{diam}(B)= \sup \{ d(x,y) \colon x,y\in B \} < \infty,$$ the measure $$\beta(B) = \inf\{r > 0\colon\text{there exist finitely many balls of radius r which cover } B\},$$ or equivalently, $$\beta(B)=\inf\big\lbrace r > 0|\exists N=N(r)\in{\bf N} \text{ and } x_1,\ldots x_N\in X\colon B\subset\bigcup_{k=1}^N B(x_k,r)\big\rbrace,$$ where $B(x,r)=\{y\in X\colon d(x,y)\}$ denotes the open ball of radius $r$ centered at $x\in X$. Let ${\bf K}(X)$ denote the collection of (non-empty) compact subsets in $X$. I would like to prove $$\beta(B)=d_H\big(B,{\bf K}(X)\big),$$ where $d_H$ is the Hausdorff distance . I proved $d_H\big(B,{\bf K}(X)\big)\le\beta(B)$. Is there someone that knows how to prove the other inequality?",,"['analysis', 'measure-theory', 'functional-analysis', 'metric-spaces']"
21,Uniformly Continuous Function sending Bounded Set to Unbounded One,Uniformly Continuous Function sending Bounded Set to Unbounded One,,"Let $(X,d)$ and $(Y,\rho)$ be metric spaces, and let $f: X \to Y$ be a uniformly continuous function. If $A \subset X$ is bounded, must $f(A) \subset Y$ be bounded? It is clear to me that in metric spaces that satisfy the Heine-Borel property, such as $\mathbb{R}^{n}$, the answer to this question is yes. However, I can see no reason why this should hold for arbitrary metric spaces. Any ideas? Thanks!","Let $(X,d)$ and $(Y,\rho)$ be metric spaces, and let $f: X \to Y$ be a uniformly continuous function. If $A \subset X$ is bounded, must $f(A) \subset Y$ be bounded? It is clear to me that in metric spaces that satisfy the Heine-Borel property, such as $\mathbb{R}^{n}$, the answer to this question is yes. However, I can see no reason why this should hold for arbitrary metric spaces. Any ideas? Thanks!",,"['analysis', 'metric-spaces']"
22,Reference for Radon measure,Reference for Radon measure,,"Can anyone provide references for Radon measures, Bounded variation functions space, and Lebesgue differentiation theorem for Radon Measures, Hausdorff dimensions? Note: Online references will be preferred but other references will also be welcomed!","Can anyone provide references for Radon measures, Bounded variation functions space, and Lebesgue differentiation theorem for Radon Measures, Hausdorff dimensions? Note: Online references will be preferred but other references will also be welcomed!",,"['analysis', 'reference-request', 'measure-theory', 'geometric-measure-theory']"
23,Graph and manifold,Graph and manifold,,"I needed help to prove the following: Let $k, n, m$ be elements of the natural numbers  and $g : R^m \to R^n$.  Prove that the graph of $g$ is an $m$-manifold of class $C^k$ if and only if $g$ is of class $C^k$ from munkres","I needed help to prove the following: Let $k, n, m$ be elements of the natural numbers  and $g : R^m \to R^n$.  Prove that the graph of $g$ is an $m$-manifold of class $C^k$ if and only if $g$ is of class $C^k$ from munkres",,"['analysis', 'differential-geometry', 'manifolds']"
24,About integrable functions.,About integrable functions.,,"Let $f_n\colon [0,1]\rightarrow R$ be Lebesgue mensurable with $\int_{0}^{1} |f_n(t)|^3dm(t)<1$ for all $n$. How we can show that $f_n$ is integrable uniformly i.e for all $\epsilon>0$ there exists $\delta>0$ so that if $E\subseteq[0,1]$ is Lebesgue measurable with $m(E)<\delta$ then $$\int_{E}|f_n(t)|dm(t)<\epsilon$$ for all $n$.","Let $f_n\colon [0,1]\rightarrow R$ be Lebesgue mensurable with $\int_{0}^{1} |f_n(t)|^3dm(t)<1$ for all $n$. How we can show that $f_n$ is integrable uniformly i.e for all $\epsilon>0$ there exists $\delta>0$ so that if $E\subseteq[0,1]$ is Lebesgue measurable with $m(E)<\delta$ then $$\int_{E}|f_n(t)|dm(t)<\epsilon$$ for all $n$.",,"['analysis', 'measure-theory']"
25,Regularity of the distance function,Regularity of the distance function,,"Let $\Omega \subseteq \mathbb{R}^N$ be open and bounded, and set: $$d(x):=\text{dist} (x,\partial \Omega) =\inf_{y\in \partial \Omega} |x-y|\;.$$ I would appreciate if somebody could verify my proof. I tried to show that it is Lipschitz continuous: Let $\forall x,y \in \Omega$, and WLOG assume that $d(x)\geq d(y)$. Let $\forall \varepsilon >0$. By definition of infimum, $\exists z \in \partial \Omega$ such that $d(y)+\varepsilon > |y-z|$ and $|x-z| \geq d(x)$. Putting everything together, we obtain that $0 \leq d(x)-d(y) \leq |x-z| - |y-z| +\varepsilon \leq |x-y| + \varepsilon $. Since $\varepsilon$ was arbitrary, done.","Let $\Omega \subseteq \mathbb{R}^N$ be open and bounded, and set: $$d(x):=\text{dist} (x,\partial \Omega) =\inf_{y\in \partial \Omega} |x-y|\;.$$ I would appreciate if somebody could verify my proof. I tried to show that it is Lipschitz continuous: Let $\forall x,y \in \Omega$, and WLOG assume that $d(x)\geq d(y)$. Let $\forall \varepsilon >0$. By definition of infimum, $\exists z \in \partial \Omega$ such that $d(y)+\varepsilon > |y-z|$ and $|x-z| \geq d(x)$. Putting everything together, we obtain that $0 \leq d(x)-d(y) \leq |x-z| - |y-z| +\varepsilon \leq |x-y| + \varepsilon $. Since $\varepsilon$ was arbitrary, done.",,['analysis']
26,Showing that a sequence converges (in metric space),Showing that a sequence converges (in metric space),,"In $(\ell ^\infty,{\Vert .\Vert_\infty)}$, how would I show that $x_n=\left(\frac{n+1}{n},\frac{n+2}{2n},\frac{n+3}{3n}, ...\right)$ converges and how would I find the limit? I tried using the fact that the uniform norm ${\Vert .\Vert_\infty}= \text{sup}|X_n|$ and the definition of convergence is that given $\epsilon > 0$, there exists $N \in \mathbb N$ such that $d(x_n,x)< \epsilon$ for all $n>N$, but I cant seem to show it converges. How would I show it converges and find the limit?","In $(\ell ^\infty,{\Vert .\Vert_\infty)}$, how would I show that $x_n=\left(\frac{n+1}{n},\frac{n+2}{2n},\frac{n+3}{3n}, ...\right)$ converges and how would I find the limit? I tried using the fact that the uniform norm ${\Vert .\Vert_\infty}= \text{sup}|X_n|$ and the definition of convergence is that given $\epsilon > 0$, there exists $N \in \mathbb N$ such that $d(x_n,x)< \epsilon$ for all $n>N$, but I cant seem to show it converges. How would I show it converges and find the limit?",,"['sequences-and-series', 'analysis', 'metric-spaces']"
27,Analysis on Manifolds Munkres Integration,Analysis on Manifolds Munkres Integration,,"I needed help in showing that the set $R^{n-1} \times 0$ has measure zero in $R^n$. What I have so far: Let $\epsilon > 0$. If $i_1,\dots,i_{n-1}$ are integers, then define $U_{i_1,\dots,i_{n-1}}=[i_1,i_1+1]\times \cdots \times [i_{n-1},i_{n-1}+1]$. Chose a bijection $f:Z\times Z\times \cdots \times Z\to N$ (the product has $n-1$ factors) where $N$ is the set of positive integers and define $A_{i_1,\dots,i_{n-1}}=U_{i_1,\dots,i_{n-1}} \times [-2^{-f(i_1,\dots,i_{n-1})-1}\epsilon,2^{-f(i_1,\dots,i_{n-1})-1}\epsilon]$. The collection of all such $A_{i_1,\dots,i_{n-1}}$'s covers $R^{n-1}\times 0$ and the sum of the measures of the $A_{i_1,\dots,i_{n-1}}$'s is less than $\epsilon$. (Why?)","I needed help in showing that the set $R^{n-1} \times 0$ has measure zero in $R^n$. What I have so far: Let $\epsilon > 0$. If $i_1,\dots,i_{n-1}$ are integers, then define $U_{i_1,\dots,i_{n-1}}=[i_1,i_1+1]\times \cdots \times [i_{n-1},i_{n-1}+1]$. Chose a bijection $f:Z\times Z\times \cdots \times Z\to N$ (the product has $n-1$ factors) where $N$ is the set of positive integers and define $A_{i_1,\dots,i_{n-1}}=U_{i_1,\dots,i_{n-1}} \times [-2^{-f(i_1,\dots,i_{n-1})-1}\epsilon,2^{-f(i_1,\dots,i_{n-1})-1}\epsilon]$. The collection of all such $A_{i_1,\dots,i_{n-1}}$'s covers $R^{n-1}\times 0$ and the sum of the measures of the $A_{i_1,\dots,i_{n-1}}$'s is less than $\epsilon$. (Why?)",,['analysis']
28,Maximize sum of reciprocals vs Minimize sums,Maximize sum of reciprocals vs Minimize sums,,"Will the returned result of the function $$\max\{\tfrac{1}{a}+\tfrac{1}{f}, \tfrac{1}{b}+\tfrac{1}{e}, \tfrac{1}{c}+\tfrac{1}{d}\}$$ return the same set $\{a,f\}$, $\{b,e\}$ or $\{c,d\}$ as the function $$\min\{a+f, b+e, c+d\}\quad?$$ Assume all numbers are positive and real-valued. In other words, if, for example, $$a+f < b+e\quad\text{ and }\quad a+f < c+d,$$  will it be true that $$\tfrac{1}{a}+\tfrac{1}{f} > \tfrac{1}{b}+\tfrac{1}{e}\quad\text{ and }\quad \tfrac{1}{a}+\tfrac{1}{f} > \tfrac{1}{c}+\tfrac{1}{d}\quad ?$$","Will the returned result of the function $$\max\{\tfrac{1}{a}+\tfrac{1}{f}, \tfrac{1}{b}+\tfrac{1}{e}, \tfrac{1}{c}+\tfrac{1}{d}\}$$ return the same set $\{a,f\}$, $\{b,e\}$ or $\{c,d\}$ as the function $$\min\{a+f, b+e, c+d\}\quad?$$ Assume all numbers are positive and real-valued. In other words, if, for example, $$a+f < b+e\quad\text{ and }\quad a+f < c+d,$$  will it be true that $$\tfrac{1}{a}+\tfrac{1}{f} > \tfrac{1}{b}+\tfrac{1}{e}\quad\text{ and }\quad \tfrac{1}{a}+\tfrac{1}{f} > \tfrac{1}{c}+\tfrac{1}{d}\quad ?$$",,"['real-analysis', 'analysis']"
29,How does one prove if a multivariate function is constant?,How does one prove if a multivariate function is constant?,,"Suppose we are given a function $f(x_{1}, x_{2})$. Does showing that $\frac{\partial f}{\partial x_{i}} = 0$ for $i = 1, 2$ imply that $f$ is a constant? Does this hold if we have $n$ variables instead?","Suppose we are given a function $f(x_{1}, x_{2})$. Does showing that $\frac{\partial f}{\partial x_{i}} = 0$ for $i = 1, 2$ imply that $f$ is a constant? Does this hold if we have $n$ variables instead?",,"['calculus', 'analysis', 'multivariable-calculus']"
30,Integration by substitution for line integrals,Integration by substitution for line integrals,,"Let $P, Q: D \rightarrow \mathbb{R}$ be continuous functions on an open set $D \subset \mathbb{R}$, let a curve $\gamma: [a,b] \rightarrow \mathbb{R}^2$ be of class $C^1$ such that $\gamma([a,b]) \subset D$. Let $\Phi=(\phi, \psi): D' \rightarrow D$ be a diffeomorphism of class $C^1$ from open $D' \subset \mathbb{R}^2$ onto $D$. Let $x=\phi(x',y')$, $y=\psi(x',y')$. I search a formula for a line integral $I:=\int_{\gamma} P(x,y)dx+Q(x,y)dy$   in coordinates $x',y'$. If we calculate differentials $dx$ and $dy$ and formally put it in integral $I$ we obtain that $\int_{\gamma} P(x,y)dx+Q(x,y)dy=\int_{\Phi^{-1} \circ \gamma} [P(\phi(x',y'), \psi(x',y')) \frac{\partial \phi}{\partial x'}+Q(\phi(x',y'), \psi(x',y')) \frac{\partial \psi}{\partial x'}] dx'$+$[P(\phi(x',y'), \psi(x',y')) \frac{\partial \phi}{\partial y'}+Q(\phi(x',y'), \psi(x',y')) \frac{\partial \psi}{\partial y'}]dy' $. Does it formula indeed hold? How to show it? (maybe by changing the both line integrals on Riemann integrals?) Thanks.","Let $P, Q: D \rightarrow \mathbb{R}$ be continuous functions on an open set $D \subset \mathbb{R}$, let a curve $\gamma: [a,b] \rightarrow \mathbb{R}^2$ be of class $C^1$ such that $\gamma([a,b]) \subset D$. Let $\Phi=(\phi, \psi): D' \rightarrow D$ be a diffeomorphism of class $C^1$ from open $D' \subset \mathbb{R}^2$ onto $D$. Let $x=\phi(x',y')$, $y=\psi(x',y')$. I search a formula for a line integral $I:=\int_{\gamma} P(x,y)dx+Q(x,y)dy$   in coordinates $x',y'$. If we calculate differentials $dx$ and $dy$ and formally put it in integral $I$ we obtain that $\int_{\gamma} P(x,y)dx+Q(x,y)dy=\int_{\Phi^{-1} \circ \gamma} [P(\phi(x',y'), \psi(x',y')) \frac{\partial \phi}{\partial x'}+Q(\phi(x',y'), \psi(x',y')) \frac{\partial \psi}{\partial x'}] dx'$+$[P(\phi(x',y'), \psi(x',y')) \frac{\partial \phi}{\partial y'}+Q(\phi(x',y'), \psi(x',y')) \frac{\partial \psi}{\partial y'}]dy' $. Does it formula indeed hold? How to show it? (maybe by changing the both line integrals on Riemann integrals?) Thanks.",,['analysis']
31,About real entire function,About real entire function,,"Assume that $f:(a,b) \rightarrow \mathbf{R}$, $x_0 \in (a,b)$ and $f(x)=\sum_{n=0}^\infty a_n (x-x_0)^n$ for $x \in (a,b)$ and radius of convergence $R$ of this power  series is infinite. (Then $f$ is smooth, $a_n=\frac{f^{(n)}(x_0)}{n!}$ for $n=0, 1, 2,  \ldots$ and $\frac{1}{R}=\lim_{n \rightarrow \infty} \sqrt[n] {  \frac{  |f^{(n)} (x_0)|} {n!}    }=0$). Is it true that $ \lim_{n \rightarrow \infty} \sqrt[n] {  \frac{\sup_{x \in K}  |f^{(n)} (x)|} {n!}    }=0$ for every compact $K \subset (a,b)$ ? Thanks.","Assume that $f:(a,b) \rightarrow \mathbf{R}$, $x_0 \in (a,b)$ and $f(x)=\sum_{n=0}^\infty a_n (x-x_0)^n$ for $x \in (a,b)$ and radius of convergence $R$ of this power  series is infinite. (Then $f$ is smooth, $a_n=\frac{f^{(n)}(x_0)}{n!}$ for $n=0, 1, 2,  \ldots$ and $\frac{1}{R}=\lim_{n \rightarrow \infty} \sqrt[n] {  \frac{  |f^{(n)} (x_0)|} {n!}    }=0$). Is it true that $ \lim_{n \rightarrow \infty} \sqrt[n] {  \frac{\sup_{x \in K}  |f^{(n)} (x)|} {n!}    }=0$ for every compact $K \subset (a,b)$ ? Thanks.",,['analysis']
32,How to prove Campanato space is a Banach space,How to prove Campanato space is a Banach space,,"Let $p\ge1$, $\mu\ge0$ and $\Omega$ be a bounded open set in $\mathbb{R}^n$. Campanato space embraces all $u$'s which $$[u]_{p,\mu}=[u]_{p,\mu;\Omega}=\sup_{\substack{x\in\Omega\\0<\rho<\mathrm{diam}\Omega}}\left(\rho^{-\mu}\int_{\Omega_\rho(x)}\left|u(y)-u_{x,\rho}\right|^p\,dy\right)^{\frac{1}{p}}<+\infty,$$  where $\Omega_\rho(x)=\Omega\cap B_\rho(x)$ ($B_\rho(x)$ denotes a ball centered at $x$ with a radium $\rho$) and $$u_{x,\rho}=\frac{1}{\left|\Omega_\rho\right|}\int_{\Omega_\rho(x)}u(y)\,dy, $$ equipped with a norm $$\|u\|_{L^{p,\mu}}=\|u\|_{L^{p,\mu}(\Omega)}=[u]_{p,\mu;\Omega}+\|u\|_{L^p(\Omega)}.$$ Let $\{u_k\}$ be a Cauchy sequence in Campanato space, one can determine a $u$ because of the completeness of $L^p$ space. What are the next steps to prove that $u$ is also the right limit of $\{u_k\}$ in the sense of $\|\cdot\|_{L^{p,\mu}(\Omega)}$ and therefore Campanato space is complete? Thank you~","Let $p\ge1$, $\mu\ge0$ and $\Omega$ be a bounded open set in $\mathbb{R}^n$. Campanato space embraces all $u$'s which $$[u]_{p,\mu}=[u]_{p,\mu;\Omega}=\sup_{\substack{x\in\Omega\\0<\rho<\mathrm{diam}\Omega}}\left(\rho^{-\mu}\int_{\Omega_\rho(x)}\left|u(y)-u_{x,\rho}\right|^p\,dy\right)^{\frac{1}{p}}<+\infty,$$  where $\Omega_\rho(x)=\Omega\cap B_\rho(x)$ ($B_\rho(x)$ denotes a ball centered at $x$ with a radium $\rho$) and $$u_{x,\rho}=\frac{1}{\left|\Omega_\rho\right|}\int_{\Omega_\rho(x)}u(y)\,dy, $$ equipped with a norm $$\|u\|_{L^{p,\mu}}=\|u\|_{L^{p,\mu}(\Omega)}=[u]_{p,\mu;\Omega}+\|u\|_{L^p(\Omega)}.$$ Let $\{u_k\}$ be a Cauchy sequence in Campanato space, one can determine a $u$ because of the completeness of $L^p$ space. What are the next steps to prove that $u$ is also the right limit of $\{u_k\}$ in the sense of $\|\cdot\|_{L^{p,\mu}(\Omega)}$ and therefore Campanato space is complete? Thank you~",,"['analysis', 'functional-analysis', 'banach-spaces']"
33,Rate of Convergence: Competing Poisson Processes,Rate of Convergence: Competing Poisson Processes,,"I am analyzing one of the properties of the Poisson Process, that of Competing Processes: Suppose $N_1(t), t \geq 0$ and $N_2(t), t \geq 0$ are independente Poisson processes with respective rates $\lambda_1$ and $\lambda_2$.  Let $S_n^i$ be the time of the $n$th event of process $i, i = 1,2$. We know that $P[S_n^1 < S_m^2] = \sum_{k=n}^{n+m-1} {n+m-1 \choose k} \left(\frac{\lambda_1}{\lambda_1 + \lambda_2}\right)^k \left(\frac{\lambda_2}{\lambda_1 + \lambda_2}\right)^{n+m-1-k}$.  I'm interested in the case where $n=m$ and $ n \rightarrow \infty$.  This converges to 1, and my intuition is that the rate of convergence depends on the ratio $\frac{\lambda_1}{\lambda_2}$, but I'm having a hard time proving it out.  Any insights into a closed-form rate of convergence would be great.","I am analyzing one of the properties of the Poisson Process, that of Competing Processes: Suppose $N_1(t), t \geq 0$ and $N_2(t), t \geq 0$ are independente Poisson processes with respective rates $\lambda_1$ and $\lambda_2$.  Let $S_n^i$ be the time of the $n$th event of process $i, i = 1,2$. We know that $P[S_n^1 < S_m^2] = \sum_{k=n}^{n+m-1} {n+m-1 \choose k} \left(\frac{\lambda_1}{\lambda_1 + \lambda_2}\right)^k \left(\frac{\lambda_2}{\lambda_1 + \lambda_2}\right)^{n+m-1-k}$.  I'm interested in the case where $n=m$ and $ n \rightarrow \infty$.  This converges to 1, and my intuition is that the rate of convergence depends on the ratio $\frac{\lambda_1}{\lambda_2}$, but I'm having a hard time proving it out.  Any insights into a closed-form rate of convergence would be great.",,"['analysis', 'stochastic-processes', 'convergence-divergence']"
34,Brouwer FPT and solutions to a system of equations,Brouwer FPT and solutions to a system of equations,,"I am trying to solve the following problem: Let f, g be continuous positive functions $\mathbb{R}^2 \to \mathbb{R}$: show that the system of equations $$(1-x^2)f^2(x,y) = x^2 g^2(x,y)$$ $$(1-y^2)g^2(x,y) = y^2 f^2(x,y)$$ has 4 distinct solutions on the unit circle. Now my thinking was to use Brouwer's fixed point theorem on the unit disc: rewrite the problem as $x = \pm \sqrt{\frac{f^2}{f^2 + g^2}},\,y = \pm  \sqrt{\frac{g^2}{f^2 + g^2}}$ Now it is clear then that both of these square roots must have modulus at most 1, and furthermore, if x and y satisfy these then $x^2 + y^2 = 1$: thus it comes down to showing that these 4 $\pm$s give us the 4 distinct solutions we require. Defining $F_{\pm \pm} = (\pm \sqrt{\frac{f^2}{f^2 + g^2}}, \pm  \sqrt{\frac{g^2}{f^2 + g^2}})$ in the obvious way, each $F_{\pm \pm}$ is a continuous map to the unit disc, so each has a fixed point: so far so good I think. (Though, do we need to worry about the case where $f^2 = g^2 = 0$?) Now, since each of the 4 functions $F_{* *}$ maps to a distinct quadrant in the unit disc, it is almost obvious that our 4 fixed points for $F_{* *}$ from Brouwer must be distinct: however, what if they both lie on the boundary where 2 quadrants meet? For example the point (1,0) is in both the ""x,y positive, positive"" and ""x,y positive, negative"" quadrant, so if 2 of our 4 fixed points coincide here then we have obviously not solved the problem. Is there a simple way around this? Or am I overlooking something? Many thanks, Peter","I am trying to solve the following problem: Let f, g be continuous positive functions $\mathbb{R}^2 \to \mathbb{R}$: show that the system of equations $$(1-x^2)f^2(x,y) = x^2 g^2(x,y)$$ $$(1-y^2)g^2(x,y) = y^2 f^2(x,y)$$ has 4 distinct solutions on the unit circle. Now my thinking was to use Brouwer's fixed point theorem on the unit disc: rewrite the problem as $x = \pm \sqrt{\frac{f^2}{f^2 + g^2}},\,y = \pm  \sqrt{\frac{g^2}{f^2 + g^2}}$ Now it is clear then that both of these square roots must have modulus at most 1, and furthermore, if x and y satisfy these then $x^2 + y^2 = 1$: thus it comes down to showing that these 4 $\pm$s give us the 4 distinct solutions we require. Defining $F_{\pm \pm} = (\pm \sqrt{\frac{f^2}{f^2 + g^2}}, \pm  \sqrt{\frac{g^2}{f^2 + g^2}})$ in the obvious way, each $F_{\pm \pm}$ is a continuous map to the unit disc, so each has a fixed point: so far so good I think. (Though, do we need to worry about the case where $f^2 = g^2 = 0$?) Now, since each of the 4 functions $F_{* *}$ maps to a distinct quadrant in the unit disc, it is almost obvious that our 4 fixed points for $F_{* *}$ from Brouwer must be distinct: however, what if they both lie on the boundary where 2 quadrants meet? For example the point (1,0) is in both the ""x,y positive, positive"" and ""x,y positive, negative"" quadrant, so if 2 of our 4 fixed points coincide here then we have obviously not solved the problem. Is there a simple way around this? Or am I overlooking something? Many thanks, Peter",,"['real-analysis', 'analysis', 'fixed-point-theorems']"
35,"What are the rules for transformation (translation, dilation, etc.) of integral on n-dimensional sphere?","What are the rules for transformation (translation, dilation, etc.) of integral on n-dimensional sphere?",,"We know that in $\mathbb{R}^n$, we have transformation rules such as: $\int_{\mathbb{R}^n}f(x-h) dx=\int_{\mathbb{R}^n}f(x) dx$ $\delta^n \int_{\mathbb{R}^n}f(\delta x) dx=\int_{\mathbb{R}^n}f(x) dx$. The proof of such formulas can be easily found in a book about real analysis. They reduce the problem into an integral of a characteristic function  of a measurable set and then use some properties of measure to prove it. My problem is: what are the similar rules for integral on n-dimensional sphere? For example, What is the relationship between $\int_{\partial B(0,r)}f(x) dS_r$ and $\int_{\partial B(0,1)}f(rx)dS_1$, where $S_r$ is the area element of $\partial B(0,r)$? If there are some, how to prove it? (In addition, how to define such an integral using abstract integration theory?) And why is $\int_{\partial B(0,r)}f(x) dS_r=\int_{\partial B(y,r)}f(x-y)dS_r$? ( This may be easy to imagine when the dimension is 1 or 2, but I want to know a proof starting from the definition of such a integral especially when the dimension is high.) Thanks.","We know that in $\mathbb{R}^n$, we have transformation rules such as: $\int_{\mathbb{R}^n}f(x-h) dx=\int_{\mathbb{R}^n}f(x) dx$ $\delta^n \int_{\mathbb{R}^n}f(\delta x) dx=\int_{\mathbb{R}^n}f(x) dx$. The proof of such formulas can be easily found in a book about real analysis. They reduce the problem into an integral of a characteristic function  of a measurable set and then use some properties of measure to prove it. My problem is: what are the similar rules for integral on n-dimensional sphere? For example, What is the relationship between $\int_{\partial B(0,r)}f(x) dS_r$ and $\int_{\partial B(0,1)}f(rx)dS_1$, where $S_r$ is the area element of $\partial B(0,r)$? If there are some, how to prove it? (In addition, how to define such an integral using abstract integration theory?) And why is $\int_{\partial B(0,r)}f(x) dS_r=\int_{\partial B(y,r)}f(x-y)dS_r$? ( This may be easy to imagine when the dimension is 1 or 2, but I want to know a proof starting from the definition of such a integral especially when the dimension is high.) Thanks.",,"['calculus', 'real-analysis', 'analysis', 'integration']"
36,What does it mean for a sequence of self-homeomorphism of $\mathbb{R}^n$ to converge to a point?,What does it mean for a sequence of self-homeomorphism of  to converge to a point?,\mathbb{R}^n,"Let $\{f_j\}$ be a family of self-homeomorphisms of $\overline{\mathbb{R}}^n$ and $x,y\in\overline{\mathbb{R}}^n$, where $\overline{\mathbb{R}}^n$ is the one-point compactification of $\mathbb{R}^n$.  What does it mean for $\{f_j\}$ to converge c-uniformly to y in $\overline{\mathbb{R}}^n\setminus \{x\}$? That is, what does ""$\lim_{j\rightarrow\infty} f_j =y$, c-uniformly in $\overline{\mathbb{R}}^n\setminus {x}$"" mean?  I'm only confused about what function the point $y$ represents.  I would have assumed it meant the function that sends everything to the point $y$, but this is not a bijection on $\overline{\mathbb{R}}^n$. The source for this question is the paper ""Discrete Quasiconformal Groups I"" by Gehring and Martin, Proc. London Math. Soc. (3) 55, 1987.  Definition 3.3 and Theorem 3.7.  My interest here is in understanding convergence groups, in particular their use in defining relatively hyperbolic groups.","Let $\{f_j\}$ be a family of self-homeomorphisms of $\overline{\mathbb{R}}^n$ and $x,y\in\overline{\mathbb{R}}^n$, where $\overline{\mathbb{R}}^n$ is the one-point compactification of $\mathbb{R}^n$.  What does it mean for $\{f_j\}$ to converge c-uniformly to y in $\overline{\mathbb{R}}^n\setminus \{x\}$? That is, what does ""$\lim_{j\rightarrow\infty} f_j =y$, c-uniformly in $\overline{\mathbb{R}}^n\setminus {x}$"" mean?  I'm only confused about what function the point $y$ represents.  I would have assumed it meant the function that sends everything to the point $y$, but this is not a bijection on $\overline{\mathbb{R}}^n$. The source for this question is the paper ""Discrete Quasiconformal Groups I"" by Gehring and Martin, Proc. London Math. Soc. (3) 55, 1987.  Definition 3.3 and Theorem 3.7.  My interest here is in understanding convergence groups, in particular their use in defining relatively hyperbolic groups.",,"['analysis', 'group-theory', 'analytic-geometry']"
37,"$A,B \subset (X,d)$ and $A$ is open dense subset, $B$ is dense then is $A \cap B$ dense?","and  is open dense subset,  is dense then is  dense?","A,B \subset (X,d) A B A \cap B","I am trying to solve this problem, and i think i did something, but finally i couldn't get the conclusion. The question is: Let $(X,d)$ be a metric space and let $A,B \subset X$. If $A$ is an open dense subset, and $B$ is a dense subset, then is $A \cap B$ dense in $X$?. Well, i think this is true. We have to show that $\overline{A \cap B}=X$. Or in other words, $B(x,r) \cap (A \cap B) \neq \emptyset$ for any $x \in X$. Since $A$ is dense in $X$, so we have $B(x,r) \cap A \neq\emptyset$. That means there is a $y \in B(x,r) \cap A$. Which means there is a $y \in A$. And since $A$ is open we have $B(y,r_{1}) \subseteq A$ for some $r_{1} > 0$. Couldn't get any further. I did try some more from here on, but couldn't get it. Any idea for proving it or giving a counter example.","I am trying to solve this problem, and i think i did something, but finally i couldn't get the conclusion. The question is: Let $(X,d)$ be a metric space and let $A,B \subset X$. If $A$ is an open dense subset, and $B$ is a dense subset, then is $A \cap B$ dense in $X$?. Well, i think this is true. We have to show that $\overline{A \cap B}=X$. Or in other words, $B(x,r) \cap (A \cap B) \neq \emptyset$ for any $x \in X$. Since $A$ is dense in $X$, so we have $B(x,r) \cap A \neq\emptyset$. That means there is a $y \in B(x,r) \cap A$. Which means there is a $y \in A$. And since $A$ is open we have $B(y,r_{1}) \subseteq A$ for some $r_{1} > 0$. Couldn't get any further. I did try some more from here on, but couldn't get it. Any idea for proving it or giving a counter example.",,['real-analysis']
38,Is this analysis problem involving induction flawed?,Is this analysis problem involving induction flawed?,,"I was recently asked to help someone with the following question on their first year analysis course. Recall that  $\mathbb{N}$ is the set of all positive integers. Use the principle of induction to show that $n \ge 1$ for all $ n \in \mathbb{N}.$ [ Hint: Let $ S = \lbrace n \in \mathbb{N} | n \ge 1 \rbrace.$ ] I am concerned that the first sentence of the question assumes that we already know exactly what the positive integers are, which renders the rest of the question redundant. I would prefer to see the question written: Recall that $\mathbb{N}$ is the set of $ x \in \mathbb{R}$ such that $x$ is a member of every inductive set. [ $P$ is an inductive set if (a) $ 1 \in P $ and (b) $ x \in P \Rightarrow x+1 \in P.$ ] Then we can answer as follows: With $S$ as in the hint, by definition $ S \subseteq \mathbb{N}.$ $1 \in S$ since $1 \in \mathbb{N},$ as it is an inductive set. For $ n \in S, $ $n+1 > n \ge 1,$ therefore $n+1 \in S.$ Hence $S$ is an inductive set, and so  $ \mathbb{N} \subseteq S.$ Therefore $ S=\mathbb{N}.$ Therefore $1$ is the least element of $\mathbb{N}.$ Am I being way too picky, or is the question flawed as it stands?","I was recently asked to help someone with the following question on their first year analysis course. Recall that  $\mathbb{N}$ is the set of all positive integers. Use the principle of induction to show that $n \ge 1$ for all $ n \in \mathbb{N}.$ [ Hint: Let $ S = \lbrace n \in \mathbb{N} | n \ge 1 \rbrace.$ ] I am concerned that the first sentence of the question assumes that we already know exactly what the positive integers are, which renders the rest of the question redundant. I would prefer to see the question written: Recall that $\mathbb{N}$ is the set of $ x \in \mathbb{R}$ such that $x$ is a member of every inductive set. [ $P$ is an inductive set if (a) $ 1 \in P $ and (b) $ x \in P \Rightarrow x+1 \in P.$ ] Then we can answer as follows: With $S$ as in the hint, by definition $ S \subseteq \mathbb{N}.$ $1 \in S$ since $1 \in \mathbb{N},$ as it is an inductive set. For $ n \in S, $ $n+1 > n \ge 1,$ therefore $n+1 \in S.$ Hence $S$ is an inductive set, and so  $ \mathbb{N} \subseteq S.$ Therefore $ S=\mathbb{N}.$ Therefore $1$ is the least element of $\mathbb{N}.$ Am I being way too picky, or is the question flawed as it stands?",,"['analysis', 'induction']"
39,Efficient Sampling,Efficient Sampling,,"I'm trying to sample a lot of points efficiently.  I'm wondering if the following method is possible. I sample points of a function (evaluate the function) mod $n$. I.e. I calculate f(element one), f(element 2)...f(element n).  I do this for all of the points in a ring.  Now I take a second ring (mod $m$, for instance) and calculate the points of a function for all elements of this ring. So my idea is this: I've sampled only $n+m$ points, but the combinations of these sample points number $n \cdot m$.  I'd like to calculate the sum of (evaluations of) this larger set of points efficiently.  I'm wondering if this is possible. I'd like to know as much about this idea as possible.  I'd like to know related ideas, what's possible, or just general ideas on how to do this.  Any pointers would constitute an acceptable answer to me, and an actual method would be absolutely wonderful. I realize that this may be more of a numerical method than mathematics, but I'm hoping that everyone will accept this question, since it may allow new methods of integration as a result.","I'm trying to sample a lot of points efficiently.  I'm wondering if the following method is possible. I sample points of a function (evaluate the function) mod $n$. I.e. I calculate f(element one), f(element 2)...f(element n).  I do this for all of the points in a ring.  Now I take a second ring (mod $m$, for instance) and calculate the points of a function for all elements of this ring. So my idea is this: I've sampled only $n+m$ points, but the combinations of these sample points number $n \cdot m$.  I'd like to calculate the sum of (evaluations of) this larger set of points efficiently.  I'm wondering if this is possible. I'd like to know as much about this idea as possible.  I'd like to know related ideas, what's possible, or just general ideas on how to do this.  Any pointers would constitute an acceptable answer to me, and an actual method would be absolutely wonderful. I realize that this may be more of a numerical method than mathematics, but I'm hoping that everyone will accept this question, since it may allow new methods of integration as a result.",,"['analysis', 'intuition', 'statistics', 'numerical-methods', 'ring-theory']"
40,Evaluating limit of Summation,Evaluating limit of Summation,,"How does one evaluate the limit: $$ \lim_{n \to \infty} \frac{1}{n}\sum\limits_{k=1}^{\lfloor{\frac{n}{2}\rfloor}} \cos\Bigl(\frac{k\pi}{n}\Bigr)$$ Yes, i recognize this as soon as i saw the problem: $$\int\limits_{0}^{1}f(x) \ dx = \lim_{n \to \infty} \frac{1}{n} \sum\limits_{k=1}^{n} f\Bigl(\frac{r}{n}\Bigr)$$ but the problem is there is $\lfloor{\frac{n}{2}\rfloor}$.","How does one evaluate the limit: $$ \lim_{n \to \infty} \frac{1}{n}\sum\limits_{k=1}^{\lfloor{\frac{n}{2}\rfloor}} \cos\Bigl(\frac{k\pi}{n}\Bigr)$$ Yes, i recognize this as soon as i saw the problem: $$\int\limits_{0}^{1}f(x) \ dx = \lim_{n \to \infty} \frac{1}{n} \sum\limits_{k=1}^{n} f\Bigl(\frac{r}{n}\Bigr)$$ but the problem is there is $\lfloor{\frac{n}{2}\rfloor}$.",,"['calculus', 'real-analysis']"
41,The discrete Bessel kernel,The discrete Bessel kernel,,"Theorem 2 in a paper by Borodin, Okounkov, and Olshanski  states that the discrete Bessel kernel $J(x,y,\theta)$ is given by \begin{equation*} \sqrt{\theta} \frac{J_x J_{y+1} - J_{x+1} J_y}{x-y} \end{equation*} where $J_x = J_x(2\sqrt{\theta})$ is the Bessel function of order $x$ and argument $2\sqrt{\theta}$. Does anybody have any ideas on how to determine the asymptotics of this kernel in the case $x = \alpha \sqrt{n}, y = \beta \sqrt{n}$ where $\alpha,\beta$ are distinct real numbers that differ from $\pm 2$?","Theorem 2 in a paper by Borodin, Okounkov, and Olshanski  states that the discrete Bessel kernel $J(x,y,\theta)$ is given by \begin{equation*} \sqrt{\theta} \frac{J_x J_{y+1} - J_{x+1} J_y}{x-y} \end{equation*} where $J_x = J_x(2\sqrt{\theta})$ is the Bessel function of order $x$ and argument $2\sqrt{\theta}$. Does anybody have any ideas on how to determine the asymptotics of this kernel in the case $x = \alpha \sqrt{n}, y = \beta \sqrt{n}$ where $\alpha,\beta$ are distinct real numbers that differ from $\pm 2$?",,['analysis']
42,"For some sets $S\subseteq T\subseteq U$, when is $\inf_T S=\inf_U S$?","For some sets , when is ?",S\subseteq T\subseteq U \inf_T S=\inf_U S,"I've been struggling with the following problem I found for a while now: Suppose $(T,\preceq)$ is a partially ordered subset of $(U,\preceq)$ and $S\subseteq T$. If $\inf_T S$ and $u=\inf_U S$ both exist, and there exists a subset $S'\subseteq T$ such that $u=\sup_U S'$, then $\inf_U S=\inf_T S$. First I noted that $\inf_T S\preceq\inf_U S$, since $\inf_T S\in U$. I also saw that $u=\inf_U([u)\cap T)$ by the following: For $x\in S$, $u\preceq x$ and $x\in T$, so $x\in [u)\cap T$, and so $S\subseteq [u)\cap T$. So $u$ is a lower bound of $[u)\cap T$. Now suppose there exists some $y\in U$ such that $u\prec y$, but $y\preceq x$ for all $x\in [u)\cap T$. Since $S\subseteq [u)\cap T$, $y$ would be a lower bound of $S$ larger than $u$, contradicting the fact that $u=\inf_U S$. I was hoping to show that $\inf_U S\preceq\inf_T S$ in order to show equality, but I can't piece it together. Can someone explain how to show this?","I've been struggling with the following problem I found for a while now: Suppose $(T,\preceq)$ is a partially ordered subset of $(U,\preceq)$ and $S\subseteq T$. If $\inf_T S$ and $u=\inf_U S$ both exist, and there exists a subset $S'\subseteq T$ such that $u=\sup_U S'$, then $\inf_U S=\inf_T S$. First I noted that $\inf_T S\preceq\inf_U S$, since $\inf_T S\in U$. I also saw that $u=\inf_U([u)\cap T)$ by the following: For $x\in S$, $u\preceq x$ and $x\in T$, so $x\in [u)\cap T$, and so $S\subseteq [u)\cap T$. So $u$ is a lower bound of $[u)\cap T$. Now suppose there exists some $y\in U$ such that $u\prec y$, but $y\preceq x$ for all $x\in [u)\cap T$. Since $S\subseteq [u)\cap T$, $y$ would be a lower bound of $S$ larger than $u$, contradicting the fact that $u=\inf_U S$. I was hoping to show that $\inf_U S\preceq\inf_T S$ in order to show equality, but I can't piece it together. Can someone explain how to show this?",,"['analysis', 'order-theory']"
43,A particular case of L'Hospital,A particular case of L'Hospital,,"Let $f$ be a function defined on an open interval $(a,b)$ which has continuous derivatives of order $1,2, \dots , n-1$, and there is a point $c \in (a,b)$ such that $f(c), f^{{1}}(c), \dots , f^{(n-1)}(c)$ are all $0$.  However $f^{n}(c)$ is not $0$ and we can assume that $f^{n}(x) > 0$ for $x \in (a,b)$, however no assumptions about the continuity of $f^{(n)}(x)$ are made .  In this case is it true that $\lim_{x\to c}\frac{f(x)}{(x-c)^n} = \frac{f^{(n)}(c)}{n!}$?","Let $f$ be a function defined on an open interval $(a,b)$ which has continuous derivatives of order $1,2, \dots , n-1$, and there is a point $c \in (a,b)$ such that $f(c), f^{{1}}(c), \dots , f^{(n-1)}(c)$ are all $0$.  However $f^{n}(c)$ is not $0$ and we can assume that $f^{n}(x) > 0$ for $x \in (a,b)$, however no assumptions about the continuity of $f^{(n)}(x)$ are made .  In this case is it true that $\lim_{x\to c}\frac{f(x)}{(x-c)^n} = \frac{f^{(n)}(c)}{n!}$?",,"['calculus', 'analysis']"
44,Derivation of a conditional expectation,Derivation of a conditional expectation,,"Setup. I have 3 random variables $Z_1,Z_2,\epsilon\in\mathbb{R}$ , where $$ (Z_1,Z_2)\sim \mathcal{N} \left( \begin{bmatrix} 0 \\ 0 \end{bmatrix}, \Sigma= \begin{bmatrix} \Sigma_{11},\Sigma_{22} \\ \Sigma_{22},\Sigma_{22} \end{bmatrix} \right). $$ Let $Y=Z_1+\epsilon$ , and $\epsilon$ is independent of $(Z_1,Z_2)$ . I would like to compute $\mathbb{E}[Z_1|Z_2,Y]$ . Question. Given that $\epsilon\sim\mathcal{N}(0,\sigma^2)$ , then using the joint property of Gaussians, we have $$ \mathbb{E}[Z_1\,|\,Z_2=z_2,Y=y] =z_2+\frac{\Sigma_{11}-\Sigma_{22}}{\Sigma_{11}-\Sigma_{22}+\sigma^2}y. $$ Can we still computing $\mathbb{E}[Z_1|Z_2,Y]$ if we only know that $\mathbb{E}[\epsilon]=0$ and $\mathbb{E}[\epsilon^2]=\sigma^2<\infty$ (without knowledge of the distribution). My attempt. By the law of total expectation, we have \begin{align*} \mathbb{E}[Z_1\,|\,Z_2=z_2,Y=y] &=\mathbb{E}\Big[\mathbb{E}[Z_1\,|\,Z_2=z_2,Y=y,\epsilon]\Big] \\ &=\mathbb{E}[y-\epsilon] \\ &=y. \end{align*} Is this correct? The final answer feels wrong to me because it does not tally with the case of a Gaussian $\epsilon$ , but I don't know where I went wrong. EDIT. I found my mistake, the law of total expectation formula used is wrong and should be this instead: $$ \mathbb{E}\Big[\mathbb{E}[Z_1\,|\,Z_2=z_2,Y=y,\epsilon]\Big|Z_2=z_2,Y=y\Big], $$ but I am still unable to evaluate $\mathbb{E}[Z_1\,|\,Z_2=z_2,Y=y]$ explicitly.","Setup. I have 3 random variables , where Let , and is independent of . I would like to compute . Question. Given that , then using the joint property of Gaussians, we have Can we still computing if we only know that and (without knowledge of the distribution). My attempt. By the law of total expectation, we have Is this correct? The final answer feels wrong to me because it does not tally with the case of a Gaussian , but I don't know where I went wrong. EDIT. I found my mistake, the law of total expectation formula used is wrong and should be this instead: but I am still unable to evaluate explicitly.","Z_1,Z_2,\epsilon\in\mathbb{R} 
(Z_1,Z_2)\sim
\mathcal{N}
\left(
\begin{bmatrix}
0 \\
0
\end{bmatrix},
\Sigma=
\begin{bmatrix}
\Sigma_{11},\Sigma_{22} \\
\Sigma_{22},\Sigma_{22}
\end{bmatrix}
\right).
 Y=Z_1+\epsilon \epsilon (Z_1,Z_2) \mathbb{E}[Z_1|Z_2,Y] \epsilon\sim\mathcal{N}(0,\sigma^2) 
\mathbb{E}[Z_1\,|\,Z_2=z_2,Y=y]
=z_2+\frac{\Sigma_{11}-\Sigma_{22}}{\Sigma_{11}-\Sigma_{22}+\sigma^2}y.
 \mathbb{E}[Z_1|Z_2,Y] \mathbb{E}[\epsilon]=0 \mathbb{E}[\epsilon^2]=\sigma^2<\infty \begin{align*}
\mathbb{E}[Z_1\,|\,Z_2=z_2,Y=y]
&=\mathbb{E}\Big[\mathbb{E}[Z_1\,|\,Z_2=z_2,Y=y,\epsilon]\Big] \\
&=\mathbb{E}[y-\epsilon] \\
&=y.
\end{align*} \epsilon 
\mathbb{E}\Big[\mathbb{E}[Z_1\,|\,Z_2=z_2,Y=y,\epsilon]\Big|Z_2=z_2,Y=y\Big],
 \mathbb{E}[Z_1\,|\,Z_2=z_2,Y=y]","['probability', 'analysis', 'statistics', 'expected-value', 'conditional-expectation']"
45,rational complex minmax,rational complex minmax,,"As the title says, I wanted to understand the following part of a proof in Guttel (Corollary 2, page 6). $f$ is an analytic function and $r_m \in \mathcal{P}_{m-1}/q_{m-1}$ is a rational function with coefficients in $\mathbb{C}$ where the numerator are the polynomial of degree at most $m-1$ while the denominator is a fixed polynomial degree $m-1$ whose poles are non contained in a compact set $K$ . I wanted to understand whether $$||f - r_m||_{\infty, K}$$ has a minimum, i.e a rational complex minmax approximation or other condition are needed. Surely it's bounded since I can take $q_{m-1} \in \mathcal{P}_{m-1}/q_{m-1}$ giving $||f - 1||_{\infty, K} < \infty$ Any solution, counterexample or reference is appreciated.","As the title says, I wanted to understand the following part of a proof in Guttel (Corollary 2, page 6). is an analytic function and is a rational function with coefficients in where the numerator are the polynomial of degree at most while the denominator is a fixed polynomial degree whose poles are non contained in a compact set . I wanted to understand whether has a minimum, i.e a rational complex minmax approximation or other condition are needed. Surely it's bounded since I can take giving Any solution, counterexample or reference is appreciated.","f r_m \in \mathcal{P}_{m-1}/q_{m-1} \mathbb{C} m-1 m-1 K ||f - r_m||_{\infty, K} q_{m-1} \in \mathcal{P}_{m-1}/q_{m-1} ||f - 1||_{\infty, K} < \infty","['analysis', 'numerical-methods', 'normed-spaces', 'approximation', 'rational-functions']"
46,"$\mathscr{L}^{p_1}(\mathbb{N},\mathscr{A},\mu)\subseteq\mathscr{L}^{p_2}(\mathbb{N},\mathscr{A},\mu)$: $1\leq p_1<p_2<+\infty$, $\mu$ counting measure",": ,  counting measure","\mathscr{L}^{p_1}(\mathbb{N},\mathscr{A},\mu)\subseteq\mathscr{L}^{p_2}(\mathbb{N},\mathscr{A},\mu) 1\leq p_1<p_2<+\infty \mu","I need to prove the following: Suppose that $1\leq p_1<p_2<+\infty$ . Let $\mu$ is the counting measure on the $\sigma$ -algebra $\mathscr{A}$ of all subsets of $\mathbb{N}$ . Then $\mathscr{L}^{p_1}(\mathbb{N},\mathscr{A},\mu)\subseteq\mathscr{L}^{p_2}(\mathbb{N},\mathscr{A},\mu)$ . My Question: I tried it myself, but I got stuck on a step of my attempt (see below). I am not sure if my existing steps are correct or not either. I would really appreciate it if someone could check my existing work and help me out with where I got stuck. Moreover, I would like to see if there is any easier solution. Thanks a lot for any help! My Attempt: Let $f\in\mathscr{L}^{p_1}(\mathbb{N},\mathscr{A},\mu)$ . Then $f$ is an $\mathscr{A}$ -measurable function on $\mathbb{N}$ such that $|f|^{p_1}$ is integrable. So $\int\left(|f|^{p_1}\right)^+d\mu = \int|f|^{p_1}d\mu < +\infty$ . Write $f=\sum_{i=1}^{\infty}a_i\chi_{\{i\}}$ . Then $|f|^{p_1} = \sum_{i=1}^{\infty}|a_i|^{p_1}\chi_{\{i\}}$ .  I want to prove that \begin{align*} \int|f|^{p_1}d\mu &= \sup\left\{\int gd\mu:g\in\mathscr{S}_+\ \text{and}\ g\leq|f|^{p_1}\right\}\\ &= \sum_{i=1}^{\infty}|a_i|^{p_1}\quad (< +\infty). \end{align*} I couldn't figure out how to prove that $\sum_{i=1}^{\infty}|a_i|^{p_1}$ is the least upper bound of the set $\left\{\int gd\mu:g\in\mathscr{S}_+\ \text{and}\ g\leq|f|^{p_1}\right\}$ . What remains is to apply the following result (see also this post ): Suppose that $1\leq p_1<p_2<=\infty$ . Then each sequence $\{a_n\}$ that satisfies $\sum|a_n|^{p_1}<+\infty$ also satisfies $\sum|a_n|^{p_2}<+\infty$ If the part where I got stuck is true, then this result would imply that $\int|f|^{p_2}d\mu<+\infty$ , and thus $f\in\mathscr{L}^{p_2}(X,\mathscr{A},\mu)$ . Note: $\mathscr{S}_+$ is the set of simple nonnegative real-valued $\mathscr{A}$ -measurable functoin. Note: I aware that related questions have been asked here and here . But this question is asking about different stuff. Thank you very much in advance! Update: Thanks to @ThÃ nhNguyá»n's comment. I wrote an answer for this post. I would really appreciate if someone could help me check if it is correct or not! Thank you very much! Reference: Example 3.3.5 from Measure Theory by Donald Cohn.","I need to prove the following: Suppose that . Let is the counting measure on the -algebra of all subsets of . Then . My Question: I tried it myself, but I got stuck on a step of my attempt (see below). I am not sure if my existing steps are correct or not either. I would really appreciate it if someone could check my existing work and help me out with where I got stuck. Moreover, I would like to see if there is any easier solution. Thanks a lot for any help! My Attempt: Let . Then is an -measurable function on such that is integrable. So . Write . Then .  I want to prove that I couldn't figure out how to prove that is the least upper bound of the set . What remains is to apply the following result (see also this post ): Suppose that . Then each sequence that satisfies also satisfies If the part where I got stuck is true, then this result would imply that , and thus . Note: is the set of simple nonnegative real-valued -measurable functoin. Note: I aware that related questions have been asked here and here . But this question is asking about different stuff. Thank you very much in advance! Update: Thanks to @ThÃ nhNguyá»n's comment. I wrote an answer for this post. I would really appreciate if someone could help me check if it is correct or not! Thank you very much! Reference: Example 3.3.5 from Measure Theory by Donald Cohn.","1\leq p_1<p_2<+\infty \mu \sigma \mathscr{A} \mathbb{N} \mathscr{L}^{p_1}(\mathbb{N},\mathscr{A},\mu)\subseteq\mathscr{L}^{p_2}(\mathbb{N},\mathscr{A},\mu) f\in\mathscr{L}^{p_1}(\mathbb{N},\mathscr{A},\mu) f \mathscr{A} \mathbb{N} |f|^{p_1} \int\left(|f|^{p_1}\right)^+d\mu = \int|f|^{p_1}d\mu < +\infty f=\sum_{i=1}^{\infty}a_i\chi_{\{i\}} |f|^{p_1} = \sum_{i=1}^{\infty}|a_i|^{p_1}\chi_{\{i\}} \begin{align*}
\int|f|^{p_1}d\mu &= \sup\left\{\int gd\mu:g\in\mathscr{S}_+\ \text{and}\ g\leq|f|^{p_1}\right\}\\
&= \sum_{i=1}^{\infty}|a_i|^{p_1}\quad (< +\infty).
\end{align*} \sum_{i=1}^{\infty}|a_i|^{p_1} \left\{\int gd\mu:g\in\mathscr{S}_+\ \text{and}\ g\leq|f|^{p_1}\right\} 1\leq p_1<p_2<=\infty \{a_n\} \sum|a_n|^{p_1}<+\infty \sum|a_n|^{p_2}<+\infty \int|f|^{p_2}d\mu<+\infty f\in\mathscr{L}^{p_2}(X,\mathscr{A},\mu) \mathscr{S}_+ \mathscr{A}","['real-analysis', 'integration', 'analysis', 'measure-theory', 'proof-writing']"
47,Best uniform approximation of $x^{n+2}$ in $\mathbb{P_n}$,Best uniform approximation of  in,x^{n+2} \mathbb{P_n},"Let $n\geq 1$ be an integer and $f(x)=x^{n+2}$ for all $ x \in [â1, 1]$ . Find the best uniform approximation of $f$ in $\mathbb{P}_n$ . Attempt: Let's solve this first for $f(x)=x^{n+1}$ instead. Suppose $p \in \mathbb{P}_n$ is the best uniform approximation. Then $g=f-p \in \mathbb{P}_{n+1}$ with leading coefficient $1$ . As $g$ has the smallest norm among all polynomials in $\mathbb{P}_{n+1}$ with leading coefficient $1$ , $g$ must be the $(n+1)$ st Chebyshev Polynomial. Now, for $f(x)=x^{n+2}$ the same argument does not work. But by Chebyshev Alternation theorem there must exist $n+2$ distinct points $-1 \leq x_1 < x_2 < \cdots < x_{n+2} \leq 1$ such that $g=f-p$ attains its maximum magnitude at those points with alternating signs. As $g' \in \mathbb{P}_{n+1}$ , either $x_1=-1$ or $x_{n+2}=1$ . However, I can't make any progress from here. Can I get any hints/insights?","Let be an integer and for all . Find the best uniform approximation of in . Attempt: Let's solve this first for instead. Suppose is the best uniform approximation. Then with leading coefficient . As has the smallest norm among all polynomials in with leading coefficient , must be the st Chebyshev Polynomial. Now, for the same argument does not work. But by Chebyshev Alternation theorem there must exist distinct points such that attains its maximum magnitude at those points with alternating signs. As , either or . However, I can't make any progress from here. Can I get any hints/insights?","n\geq 1 f(x)=x^{n+2}  x \in [â1, 1] f \mathbb{P}_n f(x)=x^{n+1} p \in \mathbb{P}_n g=f-p \in \mathbb{P}_{n+1} 1 g \mathbb{P}_{n+1} 1 g (n+1) f(x)=x^{n+2} n+2 -1 \leq x_1 < x_2 < \cdots < x_{n+2} \leq 1 g=f-p g' \in \mathbb{P}_{n+1} x_1=-1 x_{n+2}=1","['analysis', 'numerical-methods', 'chebyshev-polynomials']"
48,$L^{\infty}(X)$ is weak-star sequentially complete,is weak-star sequentially complete,L^{\infty}(X),"I'm trying to prove that if $(X,\mathcal{M},\mu)$ is $\sigma$ -finite, then $L^{\infty}(X)$ with the $w^{*}$ -topology is sequentially complete, but I am running into an issue at the last step. Suppose that $(f_n)_n \subset L^{\infty}(X)$ is a $w^{*}$ -Cauchy sequence.  Then by identifying $f_n \mapsto \varphi_n$ where: $$\varphi_n(g) = \int_{X} f_ng \: d\mu$$ via the duality $L^1(X)^{*} \cong L^{\infty}(X)$ (since the measure space is $\sigma$ -finite) we then conclude that for each $g \in L^1(X)$ , $|\varphi_n(g) - \varphi_m(g)| \longrightarrow 0$ by definition of being $w^{*}$ -Cauchy.  However then $(\varphi_n(g))_n \subset \mathbb{C}$ converges to a number $\varphi(g) \in \mathbb{C}$ by completeness. Define $\varphi: L^1(X) \longrightarrow \mathbb{C}$ by $\varphi(g) = \lim_n \varphi_n(g)$ .  Obviously $\varphi$ is linear, so if we just prove that $\varphi$ is bounded, then $\varphi \in L^1(X)^{*}$ , so by the duality there exists a map $f \in L^{\infty}(X)$ such that: $$\varphi(g) = \int_{X}fg \: d\mu = \lim_n \varphi_n(g) =\lim_n \int_{X} f_ng \: d\mu \: \: \forall g \in L^1(\mu).$$ In particular then, $f_n \longrightarrow f$ in the $w^{*}$ -topology on $L^{\infty}(X)$ by definition, which will finish the proof. I'm having an issue actually proving that $\varphi$ is bounded however.  I know by the Closed Graph theorem that it suffices to consider a sequence $(g_k,\varphi(g_k)) \longrightarrow (0,\alpha) \in L^1(X) \times \mathbb{C}$ and prove that $\alpha = 0$ .  Doing that, we see: $$|\alpha| = \lim_k |\varphi(g_k)| = \lim_k \lim_n |\varphi_n(g_k)| \leq \lim_k \sup_n \int_{X} |f_ng_k| \: d\mu$$ $$\leq \lim_k \sup_n \lVert f_n \lVert_{\infty} \lVert g_k \rVert_1.$$ Hence it suffices to know that $\sup_n \lVert f_n \rVert < \infty$ , which is equivalent to proving the family of bounded linear maps $M_n: g \mapsto f_ng$ is pointwise bounded by the Uniform Boundedness Principle, however I haven't been able to show that this is true. If I knew that $(\varphi_n) \subset V^{\circ}$ (the polar) for $V \subset L^1(X)$ an open neighborhood of $0$ , then I also think I could prove $\varphi$ is bounded, as then $V^{\circ}$ is $w^{*}$ -compact, and I at least get a convergent subsequence, but this also doesn't seem apparent to me. What am I missing here to finish this argument? $\textbf{A hint}$ would be great.","I'm trying to prove that if is -finite, then with the -topology is sequentially complete, but I am running into an issue at the last step. Suppose that is a -Cauchy sequence.  Then by identifying where: via the duality (since the measure space is -finite) we then conclude that for each , by definition of being -Cauchy.  However then converges to a number by completeness. Define by .  Obviously is linear, so if we just prove that is bounded, then , so by the duality there exists a map such that: In particular then, in the -topology on by definition, which will finish the proof. I'm having an issue actually proving that is bounded however.  I know by the Closed Graph theorem that it suffices to consider a sequence and prove that .  Doing that, we see: Hence it suffices to know that , which is equivalent to proving the family of bounded linear maps is pointwise bounded by the Uniform Boundedness Principle, however I haven't been able to show that this is true. If I knew that (the polar) for an open neighborhood of , then I also think I could prove is bounded, as then is -compact, and I at least get a convergent subsequence, but this also doesn't seem apparent to me. What am I missing here to finish this argument? would be great.","(X,\mathcal{M},\mu) \sigma L^{\infty}(X) w^{*} (f_n)_n \subset L^{\infty}(X) w^{*} f_n \mapsto \varphi_n \varphi_n(g) = \int_{X} f_ng \: d\mu L^1(X)^{*} \cong L^{\infty}(X) \sigma g \in L^1(X) |\varphi_n(g) - \varphi_m(g)| \longrightarrow 0 w^{*} (\varphi_n(g))_n \subset \mathbb{C} \varphi(g) \in \mathbb{C} \varphi: L^1(X) \longrightarrow \mathbb{C} \varphi(g) = \lim_n \varphi_n(g) \varphi \varphi \varphi \in L^1(X)^{*} f \in L^{\infty}(X) \varphi(g) = \int_{X}fg \: d\mu = \lim_n \varphi_n(g) =\lim_n \int_{X} f_ng \: d\mu \: \: \forall g \in L^1(\mu). f_n \longrightarrow f w^{*} L^{\infty}(X) \varphi (g_k,\varphi(g_k)) \longrightarrow (0,\alpha) \in L^1(X) \times \mathbb{C} \alpha = 0 |\alpha| = \lim_k |\varphi(g_k)| = \lim_k \lim_n |\varphi_n(g_k)| \leq \lim_k \sup_n \int_{X} |f_ng_k| \: d\mu \leq \lim_k \sup_n \lVert f_n \lVert_{\infty} \lVert g_k \rVert_1. \sup_n \lVert f_n \rVert < \infty M_n: g \mapsto f_ng (\varphi_n) \subset V^{\circ} V \subset L^1(X) 0 \varphi V^{\circ} w^{*} \textbf{A hint}","['real-analysis', 'functional-analysis', 'analysis', 'measure-theory']"
49,Product of uniformizable spaces,Product of uniformizable spaces,,"I say that a space $X$ is uniformizable if there exists a familily of pseudometrics $ \{d_{\alpha}\}_{\alpha \in A} $ such that induces its topology. I want to prove the following thm: If $\{X_{\mu}\}_{\mu \in L}$ is a familily of uniformizable spaces with family of pseudometrics $\{D_{\mu}\}_{\mu \in L}$ where $D_{\mu} = \{d_{\mu_{\alpha}}\}_{\alpha \in A}$ then $X = \prod_{\mu \in L}{X_{\mu}}$ is uniformizable My attemp: I want to prove that the product topology $\tau$ is the same as the topology $\tau_{D}$ induced by the family of pseudometrics $D = \{d_{\mu_{\alpha}}' \colon \alpha \in A, \mu \in L \}$ where $d'_{\mu_{\alpha}}(x,y) = d_{\mu_{\alpha}}(\pi_{\mu}(x),\pi_{\mu}(y))$ with $\pi_{\mu}$ the projection of $X$ onto $X_{\mu}$ . But I dont know how to prove that any open in $\tau$ is open in $\tau_{D}$ . Can someone help me?",I say that a space is uniformizable if there exists a familily of pseudometrics such that induces its topology. I want to prove the following thm: If is a familily of uniformizable spaces with family of pseudometrics where then is uniformizable My attemp: I want to prove that the product topology is the same as the topology induced by the family of pseudometrics where with the projection of onto . But I dont know how to prove that any open in is open in . Can someone help me?,"X  \{d_{\alpha}\}_{\alpha \in A}  \{X_{\mu}\}_{\mu \in L} \{D_{\mu}\}_{\mu \in L} D_{\mu} = \{d_{\mu_{\alpha}}\}_{\alpha \in A} X = \prod_{\mu \in L}{X_{\mu}} \tau \tau_{D} D = \{d_{\mu_{\alpha}}' \colon \alpha \in A, \mu \in L \} d'_{\mu_{\alpha}}(x,y) = d_{\mu_{\alpha}}(\pi_{\mu}(x),\pi_{\mu}(y)) \pi_{\mu} X X_{\mu} \tau \tau_{D}","['general-topology', 'analysis', 'metrizability']"
50,Every TVS is $T_{3.5}$ (Tychonoff) even if it is not $T_0$,Every TVS is  (Tychonoff) even if it is not,T_{3.5} T_0,"I'm studying the first properties of Topological Vector space, and I'm confused about the separation properties. Is every TVS $T_{3.5}$ even if it is not $T_0$ ? This is confirmed by this wikipedia link, in which it's stated: ""A vector space is an abelian group with respect to the operation of addition, and in a topological vector space the inverse operation is always continuous (since it is the same as multiplication by ${\displaystyle -1}$ ). Hence, every topological vector space is an abelian topological group. Every TVS is completely regular"" https://en.wikipedia.org/wiki/Topological_vector_space#Topological_structure The confusion arises because in every other book it's stated the the $T_0$ property is necessary.","I'm studying the first properties of Topological Vector space, and I'm confused about the separation properties. Is every TVS even if it is not ? This is confirmed by this wikipedia link, in which it's stated: ""A vector space is an abelian group with respect to the operation of addition, and in a topological vector space the inverse operation is always continuous (since it is the same as multiplication by ). Hence, every topological vector space is an abelian topological group. Every TVS is completely regular"" https://en.wikipedia.org/wiki/Topological_vector_space#Topological_structure The confusion arises because in every other book it's stated the the property is necessary.",T_{3.5} T_0 {\displaystyle -1} T_0,"['general-topology', 'functional-analysis', 'analysis', 'topological-groups', 'topological-vector-spaces']"
51,Constructive proof of a statement about a property of a Lebesgue-null set,Constructive proof of a statement about a property of a Lebesgue-null set,,"I was studying some measure theory and upon searching for additional exercises on the internet I came upon one that said: Given $H$ a Borel set subset of the real numbers, prove that if $\lambda(H)=0$ (its Lebesgue measure) then there exists a number $\alpha$ such that $\alpha + H$ , the set of all the reals of the form $\alpha + h$ where $h \in H$ , is a subset of the irrational numbers. I had no problems in proving this statement, but the proof that I came up with is by contradiction and thus does not find the wanted number (which is not even unique, since it's provable that the set of all such real numbers is a Borel set with measure $+\infty$ ). My question is, is it possible to find a constructive proof of this statement? Or maybe is it possible to find such a number for a special non trivial (like one made only of irrational or rational numbers) null set, for example the Cantor set?","I was studying some measure theory and upon searching for additional exercises on the internet I came upon one that said: Given a Borel set subset of the real numbers, prove that if (its Lebesgue measure) then there exists a number such that , the set of all the reals of the form where , is a subset of the irrational numbers. I had no problems in proving this statement, but the proof that I came up with is by contradiction and thus does not find the wanted number (which is not even unique, since it's provable that the set of all such real numbers is a Borel set with measure ). My question is, is it possible to find a constructive proof of this statement? Or maybe is it possible to find such a number for a special non trivial (like one made only of irrational or rational numbers) null set, for example the Cantor set?",H \lambda(H)=0 \alpha \alpha + H \alpha + h h \in H +\infty,"['analysis', 'measure-theory']"
52,On which sets can a function have zero derivatives?,On which sets can a function have zero derivatives?,,"Let $f:\mathbb{R}\rightarrow\mathbb{R}$ be infinitely differentiable not identically zero. Let $S_f=\{ x : f^{(n)}(x)=0 \ \forall n\in\mathbb{N} \}$ . $S_f$ can be non empty e.g. $e^{-1/x^2}$ -like examples. In fact, $S_f$ can contain an interval (integrate the previous example). Clearly $S$ is closed. Is there an example of a closed proper subset $K$ of $\mathbb{R}$ such that $K$ is not $S_f$ for any $f$ ? We can combine my two examples to make a lot of closed sets, but its not so obvious how to make sure a function with $S_f$ equal to the cantor set. If such a $K$ exists, is there another way to characterise which sets $S_f$ can be?","Let be infinitely differentiable not identically zero. Let . can be non empty e.g. -like examples. In fact, can contain an interval (integrate the previous example). Clearly is closed. Is there an example of a closed proper subset of such that is not for any ? We can combine my two examples to make a lot of closed sets, but its not so obvious how to make sure a function with equal to the cantor set. If such a exists, is there another way to characterise which sets can be?",f:\mathbb{R}\rightarrow\mathbb{R} S_f=\{ x : f^{(n)}(x)=0 \ \forall n\in\mathbb{N} \} S_f e^{-1/x^2} S_f S K \mathbb{R} K S_f f S_f K S_f,"['real-analysis', 'functional-analysis', 'analysis', 'reference-request']"
53,Show that the recursive sequence $x_{k+1} = |x_k - \frac{x_k}{1-2M^2x_k^2}|$ is monotone,Show that the recursive sequence  is monotone,x_{k+1} = |x_k - \frac{x_k}{1-2M^2x_k^2}|,"I'm doing some exercises for an upcoming exam, and as part of a larger problem, I want to show that the given recursive sequence: $$x_{k+1} = \left|x_k - \frac{x_k}{1-2M^2x_k^2}\right|$$ is monotonly increasing if $$|x_0| \geq \frac 1{2M}$$ and $M>1$ . I'm pretty sure that induction is the right approach, but I can't get the induction step to work. I tried messing around with the inverse triangle equation , but I couldn't get far. Do you have any pointers on how to approach the problem?","I'm doing some exercises for an upcoming exam, and as part of a larger problem, I want to show that the given recursive sequence: is monotonly increasing if and . I'm pretty sure that induction is the right approach, but I can't get the induction step to work. I tried messing around with the inverse triangle equation , but I couldn't get far. Do you have any pointers on how to approach the problem?",x_{k+1} = \left|x_k - \frac{x_k}{1-2M^2x_k^2}\right| |x_0| \geq \frac 1{2M} M>1,"['analysis', 'induction', 'monotone-functions', 'newton-raphson']"
54,"Covering number of the sphere (Vershynin): $\mathcal{N}\left(S^{n-1}, \varepsilon\right) \leq\left(1+\frac{2}{\varepsilon}\right)^n$",Covering number of the sphere (Vershynin):,"\mathcal{N}\left(S^{n-1}, \varepsilon\right) \leq\left(1+\frac{2}{\varepsilon}\right)^n","My reference is the text surrounding Lemma $5.2$ in Introduction to the non-asymptotic analysis of random matrices by Roman Vershynin. Definition. Let $(X, d)$ be a metric space and let $\varepsilon>0$ . A subset $\mathcal{N}_{\varepsilon}$ of $X$ is called an $\varepsilon$ -net of $X$ if every point $x \in X$ can be approximated to within $\varepsilon$ by some point $y \in \mathcal{N}_{\varepsilon}$ , i.e. so that $d(x, y) \leq \varepsilon$ . The minimal cardinality of an $\varepsilon$ -net of $X$ , if finite, is denoted $\mathcal{N}(X, \varepsilon)$ and is called the covering number of $X$ (at scale $\varepsilon)$ . Equivalently, $\mathcal{N}(X, \varepsilon)$ is the minimal number of balls with radii $\varepsilon$ and with centers in $X$ needed to cover $X$ . Suppose we have an $\varepsilon$ -net that covers $X$ in the optimal way , i.e., the number of balls used is $\mathcal{N}(X, \varepsilon)$ . Are these balls always disjoint? I don't think so. The following is the text of the main lemma, concerning covering numbers of the sphere. Lemma $5.2$ (Covering numbers of the sphere). The unit Euclidean sphere $S^{n-1}$ equipped with the Euclidean metric satisfies for every $\varepsilon>0$ that $$ \mathcal{N}\left(S^{n-1}, \varepsilon\right) \leq\left(1+\frac{2}{\varepsilon}\right)^n $$ Proof. This is a simple volume argument. Let us fix $\varepsilon>0$ and choose $\mathcal{N}_{\varepsilon}$ to be a maximal $\varepsilon$ -separated subset of $S^{n-1}$ . In other words, $\mathcal{N}_{\varepsilon}$ is such that $d(x, y) \geq \varepsilon$ for all $x, y \in \mathcal{N}_{\varepsilon}$ , $x \neq y$ , and no subset of $S^{n-1}$ containing $\mathcal{N}_{\varepsilon}$ has this property. One can in fact construct $\mathcal{N}_{\varepsilon}$ inductively by first selecting an arbitrary point on the sphere, and at each next step selecting a point that is at distance at least ${\varepsilon}$ from those already selected. By compactness, this algorithm will terminate after finitely many steps and it will yield a set $\mathcal{N}_{\varepsilon}$ as we required. How does compactness imply the algorithm's termination in finitely many steps? The maximality property implies that $\mathcal{N}_{\varepsilon}$ is an $\varepsilon$ -net of $S^{n-1}$ . Indeed, otherwise there would exist $x \in S^{n-1}$ that is at least $\varepsilon$ -far from all points in $\mathcal{N}_{\varepsilon}$ . So $\mathcal{N}_{\varepsilon} \cup\{x\}$ would still be an $\varepsilon$ -separated set, contradicting the maximality property. Moreover, the separation property implies via the triangle inequality that the balls of radii $\varepsilon / 2$ centered at the points in $\mathcal{N}_{\varepsilon}$ are disjoint. On the other hand, all such balls lie in $(1+\varepsilon / 2) B_2^n$ where $B_2^n$ denotes the unit Euclidean ball centered at the origin. Comparing the volume gives $\operatorname{vol}\left(\frac{\varepsilon}{2} B_2^n\right) \cdot\left|\mathcal{N}_{\varepsilon}\right| \leq \operatorname{vol}\left(\left(1+\frac{\varepsilon}{2}\right) B_2^n\right)$ . Since $\operatorname{vol}\left(r B_2^n\right)=r^n \operatorname{vol}\left(B_2^n\right)$ for all $r \geq 0$ , we conclude that $\left|\mathcal{N}_{\varepsilon}\right| \leq\left(1+\frac{\varepsilon}{2}\right)^n /\left(\frac{\varepsilon}{2}\right)^n=\left(1+\frac{2}{\varepsilon}\right)^n$ as required. This is probably straightforward, but I want to make sure I have the details correct. The conclusion follows from $\mathcal{N}(X, \varepsilon) \le \left|\mathcal{N}_{\varepsilon}\right| \leq\left(1+\frac{\varepsilon}{2}\right)^n /\left(\frac{\varepsilon}{2}\right)^n=\left(1+\frac{2}{\varepsilon}\right)^n$ , right? I ask because $\mathcal{N}(X, \varepsilon) < \left|\mathcal{N}_{\varepsilon}\right|$ is a possibility. Wouldn't the same proof work for $B^n_2$ also? I don't see why anything would change. I believe $\mathcal{N}\left(B^n_2, \varepsilon\right) \leq\left(1+\frac{2}{\varepsilon}\right)^n$ is also correct. Thanks for your thoughts and help!","My reference is the text surrounding Lemma in Introduction to the non-asymptotic analysis of random matrices by Roman Vershynin. Definition. Let be a metric space and let . A subset of is called an -net of if every point can be approximated to within by some point , i.e. so that . The minimal cardinality of an -net of , if finite, is denoted and is called the covering number of (at scale . Equivalently, is the minimal number of balls with radii and with centers in needed to cover . Suppose we have an -net that covers in the optimal way , i.e., the number of balls used is . Are these balls always disjoint? I don't think so. The following is the text of the main lemma, concerning covering numbers of the sphere. Lemma (Covering numbers of the sphere). The unit Euclidean sphere equipped with the Euclidean metric satisfies for every that Proof. This is a simple volume argument. Let us fix and choose to be a maximal -separated subset of . In other words, is such that for all , , and no subset of containing has this property. One can in fact construct inductively by first selecting an arbitrary point on the sphere, and at each next step selecting a point that is at distance at least from those already selected. By compactness, this algorithm will terminate after finitely many steps and it will yield a set as we required. How does compactness imply the algorithm's termination in finitely many steps? The maximality property implies that is an -net of . Indeed, otherwise there would exist that is at least -far from all points in . So would still be an -separated set, contradicting the maximality property. Moreover, the separation property implies via the triangle inequality that the balls of radii centered at the points in are disjoint. On the other hand, all such balls lie in where denotes the unit Euclidean ball centered at the origin. Comparing the volume gives . Since for all , we conclude that as required. This is probably straightforward, but I want to make sure I have the details correct. The conclusion follows from , right? I ask because is a possibility. Wouldn't the same proof work for also? I don't see why anything would change. I believe is also correct. Thanks for your thoughts and help!","5.2 (X, d) \varepsilon>0 \mathcal{N}_{\varepsilon} X \varepsilon X x \in X \varepsilon y \in \mathcal{N}_{\varepsilon} d(x, y) \leq \varepsilon \varepsilon X \mathcal{N}(X, \varepsilon) X \varepsilon) \mathcal{N}(X, \varepsilon) \varepsilon X X \varepsilon X \mathcal{N}(X, \varepsilon) 5.2 S^{n-1} \varepsilon>0 
\mathcal{N}\left(S^{n-1}, \varepsilon\right) \leq\left(1+\frac{2}{\varepsilon}\right)^n
 \varepsilon>0 \mathcal{N}_{\varepsilon} \varepsilon S^{n-1} \mathcal{N}_{\varepsilon} d(x, y) \geq \varepsilon x, y \in \mathcal{N}_{\varepsilon} x \neq y S^{n-1} \mathcal{N}_{\varepsilon} \mathcal{N}_{\varepsilon} {\varepsilon} \mathcal{N}_{\varepsilon} \mathcal{N}_{\varepsilon} \varepsilon S^{n-1} x \in S^{n-1} \varepsilon \mathcal{N}_{\varepsilon} \mathcal{N}_{\varepsilon} \cup\{x\} \varepsilon \varepsilon / 2 \mathcal{N}_{\varepsilon} (1+\varepsilon / 2) B_2^n B_2^n \operatorname{vol}\left(\frac{\varepsilon}{2} B_2^n\right) \cdot\left|\mathcal{N}_{\varepsilon}\right| \leq \operatorname{vol}\left(\left(1+\frac{\varepsilon}{2}\right) B_2^n\right) \operatorname{vol}\left(r B_2^n\right)=r^n \operatorname{vol}\left(B_2^n\right) r \geq 0 \left|\mathcal{N}_{\varepsilon}\right| \leq\left(1+\frac{\varepsilon}{2}\right)^n /\left(\frac{\varepsilon}{2}\right)^n=\left(1+\frac{2}{\varepsilon}\right)^n \mathcal{N}(X, \varepsilon) \le \left|\mathcal{N}_{\varepsilon}\right| \leq\left(1+\frac{\varepsilon}{2}\right)^n /\left(\frac{\varepsilon}{2}\right)^n=\left(1+\frac{2}{\varepsilon}\right)^n \mathcal{N}(X, \varepsilon) < \left|\mathcal{N}_{\varepsilon}\right| B^n_2 \mathcal{N}\left(B^n_2, \varepsilon\right) \leq\left(1+\frac{2}{\varepsilon}\right)^n","['geometry', 'analysis', 'euclidean-geometry', 'compactness', 'spheres']"
55,Urysohn Lemma for locally compact metric space: showing convergence of a finite union of ball covers to a compact set,Urysohn Lemma for locally compact metric space: showing convergence of a finite union of ball covers to a compact set,,"Let $X$ be a locally compact metric space and $\epsilon>0$ and fix a compact set $K$ . We cover $K$ with relatively compact open balls $B_{\epsilon(x)}(x)$ such that $\epsilon(x)\le \epsilon;$ at this point we use local compactness of $X$ . Since $K$ is compact, there are finitely many  points $x_1,\dots, x_n \in K$ such that $$K \subset \cup_{i=1}^n B_{\epsilon(x_i)}(x_i)=:U_\epsilon.$$ Obviously, $K_\epsilon:=\cup_{i=1}^n \overline{B_{\epsilon(x_i)}}(x_i)$ is compact, $U_{\epsilon} \subset K_\epsilon$ and $U_\epsilon \to K$ . In this case why is $U_\epsilon \to K$ ? I think the limit here means $\cap_{\epsilon>0} \cup_{\epsilon(x_i)\le \epsilon} \cup_{i=1}^n B_{\epsilon(x_i)}$ but I can't show why an element of this set belongs to $K$ . Actually the limit would not be the above since the points $x_i$ would not be the same as $\epsilon \to 0$ . So this makes the problem even more confusing for me.","Let be a locally compact metric space and and fix a compact set . We cover with relatively compact open balls such that at this point we use local compactness of . Since is compact, there are finitely many  points such that Obviously, is compact, and . In this case why is ? I think the limit here means but I can't show why an element of this set belongs to . Actually the limit would not be the above since the points would not be the same as . So this makes the problem even more confusing for me.","X \epsilon>0 K K B_{\epsilon(x)}(x) \epsilon(x)\le \epsilon; X K x_1,\dots, x_n \in K K \subset \cup_{i=1}^n B_{\epsilon(x_i)}(x_i)=:U_\epsilon. K_\epsilon:=\cup_{i=1}^n \overline{B_{\epsilon(x_i)}}(x_i) U_{\epsilon} \subset K_\epsilon U_\epsilon \to K U_\epsilon \to K \cap_{\epsilon>0} \cup_{\epsilon(x_i)\le \epsilon} \cup_{i=1}^n B_{\epsilon(x_i)} K x_i \epsilon \to 0","['general-topology', 'analysis', 'metric-spaces']"
56,Operator norm of integration operator to the power n,Operator norm of integration operator to the power n,,"I'm working on an assignment in 'Functional Analysis' by Haase. It is as follows: We define the integration operator $J: \mathrm{L}^2(a,b) \to \mathrm{C}[a, b]$ as $$ (Jf)(t) := \int_a^t f(x) \mathrm{d}x = \langle f, \mathrm{1}_{(a, t)}\rangle_{\mathrm{L}^2} \quad \quad (t\in[a,b],\,f\in\mathrm{L}^2(a,b)) $$ Determine the operator norm $\left\lVert J^n \right\rVert$ of $J^n$ , for $n\in \mathbb{N}$ acting on $\mathrm{C}[a,b]$ with the supremum norm. My attempted solution In the book it is already stated that the operator norm is equal to $\frac{1}{n!}$ . The definition of the operator norm is $$      \left\lVert J^n\right\rVert:= \sup_{\substack{f\in\mathrm{L}^2(a,b) \\ \left\lVert f\right\rVert\leq 1}} \left\lVert J^nf\right\rVert_\infty = \sup_{t\in[a,b]} \left| \frac{1}{(n-1)!} \int_a^t (t - s)^{n-1}f(s)\mathrm{d} s  \right|      \leq \frac{1}{(n-1)!} \sup_{t\in[a,b]} \int_a^t (t - s)^{n-1}|f(s)| \mathrm{d} s. $$ Lower bound Assuming that $b - a > 1$ , then certainly $f(s) = \frac{1}{(b - a)^n}$ has norm less than or equal to one. Then we can get $\left\lVert J^n\right\rVert \geq \frac{1}{n!}$ . But what about the case $b - a < 1$ ? Upper bound I can pull the absolute values inside the integral, but then I do not see how to proceed. I can use Cauchy-Schwarz, but then I get the Hilbert-Schmidt norm of this (integration) operator. This norm is too big I believe (and cannot involve a 'simple' $n!$ , as we square the kernel function) Any help/tips are appreciated! EDIT The book stated that the solution is $\frac{1}{n!}$ while it should have been $\frac{(b-a)^n}{n!}$ . Thanks to @geetha290krm for pointing this out! The big mistake is that I did not realize that the domain and the target of $J$ , is $\mathrm{C}[a,b]$ . This makes things a lot easier. If someone has a solution if we consider $J: \mathrm{L}^2(a,b) \to \mathrm{C}[a,b]$ feel free to post it!","I'm working on an assignment in 'Functional Analysis' by Haase. It is as follows: We define the integration operator as Determine the operator norm of , for acting on with the supremum norm. My attempted solution In the book it is already stated that the operator norm is equal to . The definition of the operator norm is Lower bound Assuming that , then certainly has norm less than or equal to one. Then we can get . But what about the case ? Upper bound I can pull the absolute values inside the integral, but then I do not see how to proceed. I can use Cauchy-Schwarz, but then I get the Hilbert-Schmidt norm of this (integration) operator. This norm is too big I believe (and cannot involve a 'simple' , as we square the kernel function) Any help/tips are appreciated! EDIT The book stated that the solution is while it should have been . Thanks to @geetha290krm for pointing this out! The big mistake is that I did not realize that the domain and the target of , is . This makes things a lot easier. If someone has a solution if we consider feel free to post it!","J: \mathrm{L}^2(a,b) \to \mathrm{C}[a, b] 
(Jf)(t) := \int_a^t f(x) \mathrm{d}x = \langle f, \mathrm{1}_{(a, t)}\rangle_{\mathrm{L}^2} \quad \quad (t\in[a,b],\,f\in\mathrm{L}^2(a,b))
 \left\lVert J^n \right\rVert J^n n\in \mathbb{N} \mathrm{C}[a,b] \frac{1}{n!} 
     \left\lVert J^n\right\rVert:= \sup_{\substack{f\in\mathrm{L}^2(a,b) \\ \left\lVert f\right\rVert\leq 1}} \left\lVert J^nf\right\rVert_\infty = \sup_{t\in[a,b]} \left| \frac{1}{(n-1)!} \int_a^t (t - s)^{n-1}f(s)\mathrm{d} s  \right|
     \leq \frac{1}{(n-1)!} \sup_{t\in[a,b]} \int_a^t (t - s)^{n-1}|f(s)| \mathrm{d} s.
 b - a > 1 f(s) = \frac{1}{(b - a)^n} \left\lVert J^n\right\rVert \geq \frac{1}{n!} b - a < 1 n! \frac{1}{n!} \frac{(b-a)^n}{n!} J \mathrm{C}[a,b] J: \mathrm{L}^2(a,b) \to \mathrm{C}[a,b]","['functional-analysis', 'analysis', 'normed-spaces']"
57,What does exactly $ âÎµ>0$ means?,What does exactly  means?, âÎµ>0,"I was proving a problem from textbook which states : Let $a < b$ be real numbers and consider the set $T= â â© [a,b]$ . Then $supT=b$ . My attempt : Since $a < b$ , so by density of $ â $ there exists atleast one rational number between $a$ and $b.$ Thus $T â  â$ . Moreover $b$ is an upper bound for $T$ , therefore by axiom of completeness supT exists. Suppose $Îµ > 0$ be arbitrary. Then $b â Îµ < b.$ By density of rational numbers there exists $q ââ $ s.t $b â Îµ < q < b$ . Here I have to show that $q \in â $ and i got stuck. Later I found a solution (online) : I saw my solution is exactly similar. But after looking the next step I still didn't understand How $r\in â$ and $b- Îµ < r < b$ implies $r \in T$ ? So my online friend told me ""this is because of arbitrary small $ Îµ $ . Moreover in real analysis we suppose $ â Îµ > 0$ as arbitrary small $ Îµ > 0$ . Even in your proof, we have $ 0 < Îµ  < b -a $ ."" (Notice this is exact his wording I am using all conclusion which I understood). My questions : what does $ â Îµ > 0$ shows? Some people say this is arbitrary small number, that is $0 < Îµ <$ (some positive number, whatever your needed). But if so, then why we write "" $ â Îµ > 0$ ? Because this shows actually $ âx â â (x > 0)$ , any positive real number not necessarily small. How does "" $ \cdots$ implies $r â T$ "" in above solution (pic)? I have spend almost whole day to understand the concept but I didn't. I have found many posts on MSE about epsilon but none of them was answering my question. Thank you.","I was proving a problem from textbook which states : Let be real numbers and consider the set . Then . My attempt : Since , so by density of there exists atleast one rational number between and Thus . Moreover is an upper bound for , therefore by axiom of completeness supT exists. Suppose be arbitrary. Then By density of rational numbers there exists s.t . Here I have to show that and i got stuck. Later I found a solution (online) : I saw my solution is exactly similar. But after looking the next step I still didn't understand How and implies ? So my online friend told me ""this is because of arbitrary small . Moreover in real analysis we suppose as arbitrary small . Even in your proof, we have ."" (Notice this is exact his wording I am using all conclusion which I understood). My questions : what does shows? Some people say this is arbitrary small number, that is (some positive number, whatever your needed). But if so, then why we write "" ? Because this shows actually , any positive real number not necessarily small. How does "" implies "" in above solution (pic)? I have spend almost whole day to understand the concept but I didn't. I have found many posts on MSE about epsilon but none of them was answering my question. Thank you.","a < b T= â â© [a,b] supT=b a < b  â  a b. T â  â b T Îµ > 0 b â Îµ < b. q ââ  b â Îµ < q < b q \in â  r\in â b- Îµ < r < b r \in T  Îµ   â Îµ > 0  Îµ > 0  0 < Îµ 
< b -a   â Îµ > 0 0 < Îµ <  â Îµ > 0  âx â â (x > 0)  \cdots r â T","['real-analysis', 'analysis', 'solution-verification', 'proof-writing', 'epsilon-delta']"
58,Set system containing no chain of length $3$,Set system containing no chain of length,3,"Let $n$ be even and let $\mathcal{A}\subset\mathcal{P}(n)$ be a set system that contains no chain of length three. Prove that \begin{equation}|\mathcal{A}|\le{n\choose{n/2}}+{n\choose{n/2-1}}.\end{equation} My thoughts are that this reminds me of Sperner's Lemma, but this is a larger number than Sperner would give. I suppose we could try and use induction. For $n=2$ this is immediate. But actually applying an induction seems tricky. I don't know how to apply the condition that there is no chain of length three.","Let be even and let be a set system that contains no chain of length three. Prove that My thoughts are that this reminds me of Sperner's Lemma, but this is a larger number than Sperner would give. I suppose we could try and use induction. For this is immediate. But actually applying an induction seems tricky. I don't know how to apply the condition that there is no chain of length three.",n \mathcal{A}\subset\mathcal{P}(n) \begin{equation}|\mathcal{A}|\le{n\choose{n/2}}+{n\choose{n/2-1}}.\end{equation} n=2,"['combinatorics', 'analysis']"
59,How to prove that $Im(T)$ is not closed?,How to prove that  is not closed?,Im(T),"Related to this question: Kernel of a bounded linear operator on a normed linear space need not be closed or open? Let $X, Y$ be normed spaces. Define a linear operator $T: X\to Y$ . I try to prove that (1)If T is bounded, then $ker(T)$ is closed. (2) There are such normed spaces $X$ and $Y$ ,and $T$ is bounded so that the image of $T$ , denoted by $Im(T)$ is not closed. My proof of (1) is as follows. But I am stuck on (2). Take a sequence $\{x_n\}\subset Ker(T)$ . Suppose that $x_n\to x\in X$ as $n\to \infty$ in $X$ . Since $T$ is bounded, then there exists some constants so that $\|T\|\le M$ . Then $$ \|Tx_n-Tx\|\le \|T\|\|x_n-x\|\le M\|x_n-x\| $$ As $n\to \infty$ , we have $$ 0\le \|Tx\|=\|Tx_n-Tx\|\to 0 $$ Then $x\in Ker(T)$ . Thus $ker(T)$ is closed. For (2) , I consider an example of an identical map from $c_{00}$ to $c_0$ equipped with sup norm: $$ T: c_{00}\to c_0 $$ Since $\|T\{x_k\}\|_\infty\le \|\{x_k\}\|_{\infty}$ , then for $x=\{x_k\}$ , $$ \|T\|=\sup_{\|x\|\le 1} \|Tx\|\le \|x\|\le 1 $$ Also, the image of $c_{00}$ is just $c_{00}$ which is not closed in $c_0$ . Because $c_{00}$ is not complete and by the result ""Let $X$ be a Banach space. The linear subspace $Y$ of $X$ is complete iff $Y$ is closed in $X$ "". So $Im(T)=c_{00}$ is not closed in $c_0$ .","Related to this question: Kernel of a bounded linear operator on a normed linear space need not be closed or open? Let be normed spaces. Define a linear operator . I try to prove that (1)If T is bounded, then is closed. (2) There are such normed spaces and ,and is bounded so that the image of , denoted by is not closed. My proof of (1) is as follows. But I am stuck on (2). Take a sequence . Suppose that as in . Since is bounded, then there exists some constants so that . Then As , we have Then . Thus is closed. For (2) , I consider an example of an identical map from to equipped with sup norm: Since , then for , Also, the image of is just which is not closed in . Because is not complete and by the result ""Let be a Banach space. The linear subspace of is complete iff is closed in "". So is not closed in .","X, Y T: X\to Y ker(T) X Y T T Im(T) \{x_n\}\subset Ker(T) x_n\to x\in X n\to \infty X T \|T\|\le M 
\|Tx_n-Tx\|\le \|T\|\|x_n-x\|\le M\|x_n-x\|
 n\to \infty 
0\le \|Tx\|=\|Tx_n-Tx\|\to 0
 x\in Ker(T) ker(T) c_{00} c_0 
T: c_{00}\to c_0
 \|T\{x_k\}\|_\infty\le \|\{x_k\}\|_{\infty} x=\{x_k\} 
\|T\|=\sup_{\|x\|\le 1} \|Tx\|\le \|x\|\le 1
 c_{00} c_{00} c_0 c_{00} X Y X Y X Im(T)=c_{00} c_0","['real-analysis', 'analysis', 'solution-verification', 'normed-spaces', 'banach-spaces']"
60,Is every sequence $\{x_n\}_{n=1}^\infty$ such that $\lim_{n\to\infty}x_n=0$ a member of $\ell^p$ for some $p>0$?,Is every sequence  such that  a member of  for some ?,\{x_n\}_{n=1}^\infty \lim_{n\to\infty}x_n=0 \ell^p p>0,"For every sequence of real numbers $(x_n)_{n\in N}$ converging to zero, does there exist a positive number p such that the sum $\sum_{n=1}^\infty |x_n|^p$ converges. I'm not 100% sure if this is correct, but I think that the answer is yes, there exists a positive number such that the sum converges because the sequence is the absolute value and converges to 0, so it wouldn't matter how $p$ would change as long as it is positive the sum will converge. Is that correct? Also I am struggling to show this thought with equations and calculations, so  if anybody has any ideas for that I would be very grateful.","For every sequence of real numbers converging to zero, does there exist a positive number p such that the sum converges. I'm not 100% sure if this is correct, but I think that the answer is yes, there exists a positive number such that the sum converges because the sequence is the absolute value and converges to 0, so it wouldn't matter how would change as long as it is positive the sum will converge. Is that correct? Also I am struggling to show this thought with equations and calculations, so  if anybody has any ideas for that I would be very grateful.",(x_n)_{n\in N} \sum_{n=1}^\infty |x_n|^p p,"['sequences-and-series', 'functional-analysis', 'analysis', 'convergence-divergence']"
61,Closed Linear Subspace of BFS $X$ .,Closed Linear Subspace of BFS  .,X,"This is the definition which would be useful: Let X be a Bannach Function Space on $[0;1]$ . Assume that the space $C([0;1])$ of continuous functions on $[0;1]$ is a closed linear subspace of $X$ . Then for every $f \in C([0;1])$ we have $$||f|||_X \leq ||f||_{C([0;1])} $$ . As I read It's because $||f||_X \leq ||\chi_{\Omega}||_X||f||_{C(I)}$ , assuming that $||\chi_{\Omega}||_X = 1 $ . I don't understand why do we have this inequality : $||f||_X \leq ||\chi_{\Omega}||_X||f||_{C(I)}$ . ? Any help would be appreciated.","This is the definition which would be useful: Let X be a Bannach Function Space on . Assume that the space of continuous functions on is a closed linear subspace of . Then for every we have . As I read It's because , assuming that . I don't understand why do we have this inequality : . ? Any help would be appreciated.",[0;1] C([0;1]) [0;1] X f \in C([0;1]) ||f|||_X \leq ||f||_{C([0;1])}  ||f||_X \leq ||\chi_{\Omega}||_X||f||_{C(I)} ||\chi_{\Omega}||_X = 1  ||f||_X \leq ||\chi_{\Omega}||_X||f||_{C(I)},"['real-analysis', 'functional-analysis', 'analysis', 'normed-spaces', 'banach-spaces']"
62,Area-preserving,Area-preserving,,"A diffeomorphism $\varphi: S \to \bar{S}$ is said to be area-preserving if the area of any region R \subset S is equal to the area of $\varphi(R)$ . Prove that if $\varphi$ is area-preserving and conformal, then Ï is an isometry. I have two ideas for this excersise. First, i consider a parametrization $x: U \to R \subset S$ and $\bar{x}:\varphi \circ x : X(U) \to \varphi(X(U)) $ a parametrization for $\bar{S}$ . We can note that if $\varphi$ is area-preserving then $A(R)=A(\varphi(R))$ . Here, i can use the definition of Area with respect to its partial derivatives and then we have that $$A(R)=\int\int_{R}||x_{u} \times x_{v}||dudv$$ But for $\bar{S}$ $$A(\varphi(R))=\int\int ||(\varphi \circ x)_{u} \times (\varphi \circ x)_{v}||dudv$$ here i'm stuck, because i'dont know how to proceed with $A(\varphi(R))$ or how to describe the partial derivatives of the composition or use the hypotesis that $\varphi$ is conformal, i'm sure that conformal does not implies that coeficients of x and $\bar{x}$ are $\lambda$ - times of another, i.e, $\bar{E}=\lambda  E, \bar{F}= \lambda F, \bar{G}= \lambda G$ , so i don't know what to do here. Another idea for this is use the definition of area with respect to first fundamental form but here the only way to develop this idea is using the wrong assumption about coefficients that i was talking about. I appreciate some idea. UPDATE i want to be sure that this attempt is good. I will proof that $$d\varphi_{q}(x_{u})=\bar{x}_{u}$$ where $x_{u}$ and $\bar{x}_{u}$ are the partial derivatives of x and $\bar{x}$ respectively. Now,for $(u_{0},v_{0}) \in U$ and $q=x(u_{0},v_{0}) $ and $\alpha$ a curve defined as $x(u,v_{0})=\alpha(u)$ , therefore $\alpha'(u)=x_{u}(u,v_{0})$ . Now, we have that $$d\varphi_{q}(x_{u}(u_{0},v_{0}))=d\varphi_{q}(\alpha'(u_{0}))=(\varphi \circ \alpha)'(u_{0})$$ then we have that $$d\varphi_{q}(x_{u}(u,v_{0}))=d\varphi_{q}(\alpha'(u))=(\varphi \circ \alpha)'(u)$$ for all $u$ but now, we can note that $$(\varphi \circ \alpha)(u)=\varphi(\alpha(u))=\varphi(x(u,v_{0})=(\varphi \circ x)(u,v_{0})=\bar{x}(u,v_{0})$$ so, finally we have that $$(\varphi \circ \alpha)'(u,v_{0})=\bar{x}_{u}(u,v_{0})$$ and we can conclude that $$(\varphi \circ \alpha)'=d\varphi_{q}(x_{u})=\bar{x}_{u}$$ this is enought or this proof is wrong?","A diffeomorphism is said to be area-preserving if the area of any region R \subset S is equal to the area of . Prove that if is area-preserving and conformal, then Ï is an isometry. I have two ideas for this excersise. First, i consider a parametrization and a parametrization for . We can note that if is area-preserving then . Here, i can use the definition of Area with respect to its partial derivatives and then we have that But for here i'm stuck, because i'dont know how to proceed with or how to describe the partial derivatives of the composition or use the hypotesis that is conformal, i'm sure that conformal does not implies that coeficients of x and are - times of another, i.e, , so i don't know what to do here. Another idea for this is use the definition of area with respect to first fundamental form but here the only way to develop this idea is using the wrong assumption about coefficients that i was talking about. I appreciate some idea. UPDATE i want to be sure that this attempt is good. I will proof that where and are the partial derivatives of x and respectively. Now,for and and a curve defined as , therefore . Now, we have that then we have that for all but now, we can note that so, finally we have that and we can conclude that this is enought or this proof is wrong?","\varphi: S \to \bar{S} \varphi(R) \varphi x: U \to R \subset S \bar{x}:\varphi \circ x : X(U) \to \varphi(X(U))  \bar{S} \varphi A(R)=A(\varphi(R)) A(R)=\int\int_{R}||x_{u} \times x_{v}||dudv \bar{S} A(\varphi(R))=\int\int ||(\varphi \circ x)_{u} \times (\varphi \circ x)_{v}||dudv A(\varphi(R)) \varphi \bar{x} \lambda \bar{E}=\lambda  E, \bar{F}= \lambda F, \bar{G}= \lambda G d\varphi_{q}(x_{u})=\bar{x}_{u} x_{u} \bar{x}_{u} \bar{x} (u_{0},v_{0}) \in U q=x(u_{0},v_{0})  \alpha x(u,v_{0})=\alpha(u) \alpha'(u)=x_{u}(u,v_{0}) d\varphi_{q}(x_{u}(u_{0},v_{0}))=d\varphi_{q}(\alpha'(u_{0}))=(\varphi \circ \alpha)'(u_{0}) d\varphi_{q}(x_{u}(u,v_{0}))=d\varphi_{q}(\alpha'(u))=(\varphi \circ \alpha)'(u) u (\varphi \circ \alpha)(u)=\varphi(\alpha(u))=\varphi(x(u,v_{0})=(\varphi \circ x)(u,v_{0})=\bar{x}(u,v_{0}) (\varphi \circ \alpha)'(u,v_{0})=\bar{x}_{u}(u,v_{0}) (\varphi \circ \alpha)'=d\varphi_{q}(x_{u})=\bar{x}_{u}","['geometry', 'analysis', 'differential-geometry']"
63,"$f$ is continuous on $[0, 1]$, $g(0)=1, g(1)=0$, and $f+g$ is monotonic increasing. Prove: $g$ can take any value between $[0, 1]$.","is continuous on , , and  is monotonic increasing. Prove:  can take any value between .","f [0, 1] g(0)=1, g(1)=0 f+g g [0, 1]","Function $f$ is continuous on $[0, 1]$ . Function $g$ is defined on $[0, 1]$ and $g(0)=1, g(1)=0$ . Function $f+g$ is monotonic increasing. Prove: $g$ can take any value between $[0, 1]$ . My attempt: Since $f+g$ is monotonic, then $f(0)+g(0)\le f(1)+g(1)\Rightarrow f(0)+1\le f(1)$ . We also have: $$f(0)+1\le f(x)+g(x)\le f(1)~~~~~\forall x\in [0,1]$$ move terms, $$f(0)+1-f(x)\le g(x)\le f(1)-f(x)$$ since $f$ is continuous, it can take any value between $[f(0), f(1)]$ , the LHS can take any value between $[f(0)+1-f(1), 1]$ , and $[0,1]\subseteq [f(0)+1-f(1), 1]$ . The RHS can take any value between $[0, f(1)-f(0)]$ , and $[0,1]\subseteq [0, f(1)-f(0)]$ . So for any value $a\in [0,1]$ , there exists $x_1$ , such that $a\le g(x_1)$ .Also there exists a point $x_2$ such that $g(x_2)\le a$ . I am stuck here. I can't conclude $x_1=x_2$ . How to proceed next? Thank you a lot.","Function is continuous on . Function is defined on and . Function is monotonic increasing. Prove: can take any value between . My attempt: Since is monotonic, then . We also have: move terms, since is continuous, it can take any value between , the LHS can take any value between , and . The RHS can take any value between , and . So for any value , there exists , such that .Also there exists a point such that . I am stuck here. I can't conclude . How to proceed next? Thank you a lot.","f [0, 1] g [0, 1] g(0)=1, g(1)=0 f+g g [0, 1] f+g f(0)+g(0)\le f(1)+g(1)\Rightarrow f(0)+1\le f(1) f(0)+1\le f(x)+g(x)\le f(1)~~~~~\forall x\in [0,1] f(0)+1-f(x)\le g(x)\le f(1)-f(x) f [f(0), f(1)] [f(0)+1-f(1), 1] [0,1]\subseteq [f(0)+1-f(1), 1] [0, f(1)-f(0)] [0,1]\subseteq [0, f(1)-f(0)] a\in [0,1] x_1 a\le g(x_1) x_2 g(x_2)\le a x_1=x_2","['real-analysis', 'analysis']"
64,Asymptotics of $F(\varepsilon)=\int^b_0\frac{dx}{\varepsilon +\phi(x)}$ as $\varepsilon\rightarrow0$ where $\phi(x)\sim Ax$ as $x\rightarrow0$.,Asymptotics of  as  where  as .,F(\varepsilon)=\int^b_0\frac{dx}{\varepsilon +\phi(x)} \varepsilon\rightarrow0 \phi(x)\sim Ax x\rightarrow0,"This is one part of an old qualifier analysis problem that I am trying to complete. Suppose $\phi$ is a continuous function on a finite interval $[0,b]$ and that $\phi(x)>0$ for all $x\in[0,b]$ . Assume that $\phi(x)\sim A x^r$ for some constant $A>0$ and $r\geq0$ . Define $$F(\varepsilon)=\int^b_0 \frac{dx}{\varepsilon + \phi(x)}$$ If $0\leq r<1$ , then $\lim_{\varepsilon\rightarrow0}=\int^b_0\frac{dx}{\phi(x)}<\infty$ . If $r>1$ , then $$F(\varepsilon)\sim \frac{\pi}{r A^{1/r}\sin(\pi/r)} \varepsilon^{-(1-\frac1r)}\quad\text{as}\quad\varepsilon\rightarrow0$$ If $r=1$ , then $$F(\varepsilon)\sim\frac{1}{A}\log(\varepsilon^{-1})\quad\text{as}\quad\varepsilon\rightarrow0$$ Part (1) is a simple application of monotone convergence. When $r\geq1$ , $\lim_{\varepsilon0}F(\varepsilon)=\infty$ by monotone convergence. Part (2) I have done by first applying the  of variable $u=\varepsilon^{-1}x^r$ , followed by application of dominated convergence. All that gives $$\lim_{\varepsilon\rightarrow0}\varepsilon^{1-\frac1r}F(\varepsilon)=\int^\infty_0\frac{u^{\frac1r-1}}{1+Au}\,du$$ The later is a integral of the Mellin transform type and the expression in the problem can be obtained by complex contour integration. Part (3) is where I am having difficulties.  The asymptotic seems given in the problem seems correct. By assuming that $\phi(t)=At$ , then a direct computation gives $$F(\varepsilon)=\frac{1}{A}\log\Big(\frac1A+\frac{b}{\varepsilon}\Big)\sim\frac{1}{A}\log(\varepsilon^{-1})$$ I thing dominated convergence still useful but a trick may be needed. For example, the function $x\mapsto x^{-1}\phi(x)$ can be extended to $[0,b]$ as a strictly positive continuous function on $[0,b]$ and so, there is $M>0$ such that $\phi(x)\geq Bx$ for $x\in[0,b]$ . Hence $$\frac{1}{\varepsilon +\phi(x)}\leq\frac{1}{\varepsilon+Bx}$$ Any hints (not necessarily a complete solution) are appreciated. Thank you!","This is one part of an old qualifier analysis problem that I am trying to complete. Suppose is a continuous function on a finite interval and that for all . Assume that for some constant and . Define If , then . If , then If , then Part (1) is a simple application of monotone convergence. When , by monotone convergence. Part (2) I have done by first applying the  of variable , followed by application of dominated convergence. All that gives The later is a integral of the Mellin transform type and the expression in the problem can be obtained by complex contour integration. Part (3) is where I am having difficulties.  The asymptotic seems given in the problem seems correct. By assuming that , then a direct computation gives I thing dominated convergence still useful but a trick may be needed. For example, the function can be extended to as a strictly positive continuous function on and so, there is such that for . Hence Any hints (not necessarily a complete solution) are appreciated. Thank you!","\phi [0,b] \phi(x)>0 x\in[0,b] \phi(x)\sim A x^r A>0 r\geq0 F(\varepsilon)=\int^b_0 \frac{dx}{\varepsilon + \phi(x)} 0\leq r<1 \lim_{\varepsilon\rightarrow0}=\int^b_0\frac{dx}{\phi(x)}<\infty r>1 F(\varepsilon)\sim \frac{\pi}{r A^{1/r}\sin(\pi/r)} \varepsilon^{-(1-\frac1r)}\quad\text{as}\quad\varepsilon\rightarrow0 r=1 F(\varepsilon)\sim\frac{1}{A}\log(\varepsilon^{-1})\quad\text{as}\quad\varepsilon\rightarrow0 r\geq1 \lim_{\varepsilon0}F(\varepsilon)=\infty u=\varepsilon^{-1}x^r \lim_{\varepsilon\rightarrow0}\varepsilon^{1-\frac1r}F(\varepsilon)=\int^\infty_0\frac{u^{\frac1r-1}}{1+Au}\,du \phi(t)=At F(\varepsilon)=\frac{1}{A}\log\Big(\frac1A+\frac{b}{\varepsilon}\Big)\sim\frac{1}{A}\log(\varepsilon^{-1}) x\mapsto x^{-1}\phi(x) [0,b] [0,b] M>0 \phi(x)\geq Bx x\in[0,b] \frac{1}{\varepsilon +\phi(x)}\leq\frac{1}{\varepsilon+Bx}","['real-analysis', 'integration', 'analysis', 'asymptotics', 'lebesgue-integral']"
65,"$f$ is convex if and only if $f =\sup\{\varphi:\mathbb{R}^M\to \mathbb{R}: \varphi \le f, \varphi \text { affine} \}$",is convex if and only if,"f f =\sup\{\varphi:\mathbb{R}^M\to \mathbb{R}: \varphi \le f, \varphi \text { affine} \}","Let $f : \mathbb{R}^M\to \mathbb{R} $ $f$ is convex if and only if $f =\sup\{\varphi:\mathbb{R}^M\to \mathbb{R}: \varphi \le f, \varphi  \text { affine} \}$ I am having trouble proving ( $\implies$ ) To prove it I have to follow the following hints: 1)Fix $x \in \mathbb{R}^M$ , and $t \in \mathbb{R}$ with $t < f (x)$ We claim that there exists an affine function $\varphi : \mathbb{R}^M \to \mathbb{R}$ with $\varphi \le f$ such that $\varphi(x) > t$ ; For this: 1a) Consider the point $P := (x, t)$ . Prove that there exists a point $Q$ which minimizes the distance from $P$ to the graph of $f$ 1b)Define the function $\varphi : \mathbb{R}^M \to \mathbb{R}$ as the affine function whose graph is the tangent plane to $\partial  B(P, |P â Q|)$ and passing by $Q$ 1c) Prove that $\varphi \le f$ Assume by contradiction that there exists a point $y \in \mathbb{R}^M$ such that $f (y) < \varphi(y)$ . Let $A := (y, f (y))$ . Prove that there exists a point $E$ on the graph of $f$ which is inside $B(P, |P â Q|)$ . contradicting the choice of $Q$ My try: I have worked out some of my ideas for each part , but I am missing pieces to complete it 1a) This one I almost have it.  Define the graph of $f$ as: $Graph (f)=\{(x,f(x)):x \in dom f\}$ and the epigraph: $Ep (f)=\{(x,y):x \in dom f , y>f(x)\}$ , which is convex because $f$ is convex. Ep(f) is also a closed set, and non-empty. Since we are in a Hilbert space and these are the hypotheses of Hilbert's projection theorem, I can conclude that there exists a point $Q\in Ep(f)$ such that the distance from $P$ to the set $Ep(f)$ is minimized. Now, I am missing the fact that this point should be in the graph, not in the epigraph Ep(f), how do I conclude that? It is kind of intuitive that points in the interior are further, but how would I formalize it? Maybe Hilbert's theorem is not the way to go then? 1b) The equation of $\varphi$ as described in the hint should be $\varphi (y)= a \cdot y+a_0$ , with $a \in \mathbb{R}^M$ and $a_0 \in \mathbb{R}$ . I am not sure if this way of writting it is convenient, because it is not clear by just looking at the equation that it passes by $Q$ and that the normal vector is $Q-P$ .  Maybe  a vector form is better: $n \cdot (R-Q)=(Q-P) \cdot (R-Q)=0, $ for all $R\in \mathbb{R}^{M+1}$ . The problem is that this equation is implicit, I don't see the function as in the cartesian form 1c) Proceding as in the hint: Assume by contradiction that there exists a point $y \in \mathbb{R}^M$ such that $f (y) < \varphi(y)$ . Let $A := (y, f (y))$ . Let $Q=(y_q,f(y_q))$ and let $E=(y,\varphi(y))$ (Which I claim is the point I am looking for) So I need to show that this point is inside $B(P, |P â Q|)$ , namely that $|E-P|<|P-Q| $ . I have tried to use the triangle inequality in many ways but I don't find what I need, for instance $|P-E|\le |P-Q|+|Q-A|+|A-E|=|P-Q|+|Q-A|+|f(y)-\varphi(y)|$ In this way $f(y)-\varphi(y)$ shows up, which should be non zero because of the hypothesis, but I don't know how to continue","Let is convex if and only if I am having trouble proving ( ) To prove it I have to follow the following hints: 1)Fix , and with We claim that there exists an affine function with such that ; For this: 1a) Consider the point . Prove that there exists a point which minimizes the distance from to the graph of 1b)Define the function as the affine function whose graph is the tangent plane to and passing by 1c) Prove that Assume by contradiction that there exists a point such that . Let . Prove that there exists a point on the graph of which is inside . contradicting the choice of My try: I have worked out some of my ideas for each part , but I am missing pieces to complete it 1a) This one I almost have it.  Define the graph of as: and the epigraph: , which is convex because is convex. Ep(f) is also a closed set, and non-empty. Since we are in a Hilbert space and these are the hypotheses of Hilbert's projection theorem, I can conclude that there exists a point such that the distance from to the set is minimized. Now, I am missing the fact that this point should be in the graph, not in the epigraph Ep(f), how do I conclude that? It is kind of intuitive that points in the interior are further, but how would I formalize it? Maybe Hilbert's theorem is not the way to go then? 1b) The equation of as described in the hint should be , with and . I am not sure if this way of writting it is convenient, because it is not clear by just looking at the equation that it passes by and that the normal vector is .  Maybe  a vector form is better: for all . The problem is that this equation is implicit, I don't see the function as in the cartesian form 1c) Proceding as in the hint: Assume by contradiction that there exists a point such that . Let . Let and let (Which I claim is the point I am looking for) So I need to show that this point is inside , namely that . I have tried to use the triangle inequality in many ways but I don't find what I need, for instance In this way shows up, which should be non zero because of the hypothesis, but I don't know how to continue","f : \mathbb{R}^M\to \mathbb{R}  f f =\sup\{\varphi:\mathbb{R}^M\to \mathbb{R}: \varphi \le f, \varphi  \text { affine} \} \implies x \in \mathbb{R}^M t \in \mathbb{R} t < f (x) \varphi : \mathbb{R}^M \to \mathbb{R} \varphi \le f \varphi(x) > t P := (x, t) Q P f \varphi : \mathbb{R}^M \to \mathbb{R} \partial  B(P, |P â Q|) Q \varphi \le f y \in \mathbb{R}^M f (y) < \varphi(y) A := (y, f (y)) E f B(P, |P â Q|) Q f Graph (f)=\{(x,f(x)):x \in dom f\} Ep (f)=\{(x,y):x \in dom f , y>f(x)\} f Q\in Ep(f) P Ep(f) \varphi \varphi (y)= a \cdot y+a_0 a \in \mathbb{R}^M a_0 \in \mathbb{R} Q Q-P n \cdot (R-Q)=(Q-P) \cdot (R-Q)=0,  R\in \mathbb{R}^{M+1} y \in \mathbb{R}^M f (y) < \varphi(y) A := (y, f (y)) Q=(y_q,f(y_q)) E=(y,\varphi(y)) B(P, |P â Q|) |E-P|<|P-Q|  |P-E|\le |P-Q|+|Q-A|+|A-E|=|P-Q|+|Q-A|+|f(y)-\varphi(y)| f(y)-\varphi(y)","['general-topology', 'analysis', 'convex-analysis']"
66,"Continuity, partial derivatives, differentiability questions for $f(x, y):= \frac{x^2 \sin y^2}{x^2+y^4}$","Continuity, partial derivatives, differentiability questions for","f(x, y):= \frac{x^2 \sin y^2}{x^2+y^4}","I would like to know if you think my answers are correct, I am not so sure about $1$ and $3$ and I don't know how to answer $4$ . Define $f: \mathbb{R}^2 \rightarrow \mathbb{R}$ by \begin{align*} f(x, y):= \frac{x^2 \sin y^2}{x^2+y^4}, & (x, y) \neq(0,0) & \\ f(x,y) := 0, & (x, y)=(0,0) \end{align*} Is the function continuous in (0,0)? Do the partial derivatives $(D_1f)(0,0)$ en $(D_2f)(0,0)$ exist? For which $u \in \mathbb{R}^2 \backslash \{0\}$ does the directional derivative $(D_uf)(0,0)$ exist? Is $f$ differentiable in $(0,0)$ ? Is $f$ a $C^1$ function? To show that $f$ is continuous at $(0,0)$ , we need to show that $\lim_{(x,y)\to(0,0)}f(x,y)=f(0,0)=0$ . We can do this by using the epsilon-delta definition of a limit. To do this, we need to show that for every $\epsilon > 0$ , there exists a $\delta > 0$ such that if $||(x,y) - (0,0)|| < \delta$ , then $|f(x,y) - f(0,0)| < \epsilon$ . Let $\epsilon > 0$ and take $\delta = \sqrt{\epsilon}$ . If $||(x,y) - (0,0)|| < \delta$ , then we have: \begin{align*} |f(x,y) - f(0,0)| &= ||\frac{x^2\sin y^2}{x^2+y^4} - 0|| \ &= |\frac{x^2\sin y^2}{x^2+y^4}| \ & \leq |\frac{x^2\sin y^2}{x^2}| \ & \leq |\frac{x^2y^2}{x^2}| = y^2 \ & \leq x^2 + y^2 = \delta^2 = \epsilon. \end{align*} Here, we used that $x^2 \leq x^2 + y^4$ , since $y^4 \geq 0$ for all $y$ and observed that $0 \leq |\sin y^2| \leq 1$ for all $y$ . Thus, the limit of the function exists at 0 and $f$ is continuous at 0. We prove this using the definition of partial derivative. We have $(D_jf)(a) := \lim_{t \to 0} \frac{f(a + te_j) - f(a)}{t}$ with $a = (0 \quad 0)^T$ . Thus: \begin{align*} (D_1f)(0,0) & = \lim_{t \to 0} \frac{f\left(\left(\begin{array}{c} 0 \ 0 \ \end{array}\right) + t\left(\begin{array}{c} 1 \ 0 \ \end{array}\right)\right) - 0}{t} \ & = \lim_{t \to 0} \frac{f(t,0) - 0}{t} \ & = \lim_{t \to 0} \frac{\frac{t^2\sin(0^2)}{t^2 + 0^4} - 0}{t} \ & = \lim_{t \to 0} \frac{\frac{t^2 \cdot 0}{t^2}}{t}\ & = \lim_{t \to 0} \frac{0}{t^3} \ & = \lim_{t \to 0} 0 \ & = 0 \end{align*} and \begin{align*} (D_2f)(0,0) & = \lim_{t \to 0} \frac{f\left(\left(\begin{array}{c} 0 \ 0 \ \end{array}\right) + t\left(\begin{array}{c} 0 \ 1 \ \end{array}\right)\right) - 0}{t} \ & = \lim_{t \to 0} \frac{f(0,t) - 0}{t} \ & = \lim_{t \to 0} \frac{\frac{0^2\sin(t^2)}{0^2 + t^4} - 0}{t}\ & = \lim_{t \to 0} \frac{\frac{0}{t^4}}{t}\ & = \lim_{t \to 0} \frac{0}{t^5} \ & = \lim_{t \to 0} 0 \ & = 0. \end{align*} So the partial derivatives $(D_1f)(0,0)$ and $(D_2f)(0,0)$ exist. We look at the definition of the directional derivative $(D_uf)(0,0) = \lim_{t \to 0} \frac{f(a + tu) - f(a)}{t}$ . For $a = (0 \quad 0)^T$ , this gives (with $u := (u_1, u_2)$ ): \begin{align*} \lim_{t \to 0} \frac{f(a + tu) - f(a)}{t} &= \lim_{t \to 0} \frac{f(tu)}{t} \ & = \lim_{t \to 0} \frac{f(t(u_1, u_2))}{t} \ &= \lim_{t \to 0} \frac{f(tu_1, tu_2)}{t} \ & = \lim_{t \to 0} \frac{\frac{t^2u_1^2 \cdot \sin(t^2u_2^2)}{t^2u_1^2 + t^4u_2^4}}{t} \ & = \lim_{t \to 0} \frac{t^2u_1^2 \cdot \sin(t^2u_2^2)}{t^3u_1^2 + t^5u_2^4} \ & = \lim_{t \to 0} \frac{u_1^2\sin(t^2u_2^2)}{tu_1^2 + t^3u_2^4}\ & = \frac{u_1^2\sin(0^2 u_2^2)}{0u_1^2 + 0^3u_2^4} \ & = \frac{0}{0}. \end{align*} So for no $u \in \mathbb{R}^2 \backslash {0}$ , the directional derivative $(D_uf)(0,0)$ exists. We use the theorem that says when $f$ is differentiable in $a$ , then the partial derivatives $(D_jf_i)(a)$ exist and we have the total derivative $Df(a)$ . ?? To show that $f$ is a $C^1$ function, we need to show that the partial derivatives exist and are continuous on $\mathbb{R}^2$ . The partial derivatives of $f$ are: \begin{align*} \frac{\partial f}{\partial x}(x,y) &= \frac{2x\sin y^2 (x^2+y^4) - x^2\sin y^2 \cdot 2x}{(x^2+y^4)^2} \ &= \frac{2x\sin y^2 (y^4)}{(x^2+y^4)^2}, \\ \frac{\partial f}{\partial y}(x,y) &= \frac{x^2 \cdot 2y\cos y^2 (x^2+y^4) - x^2\sin y^2 \cdot 4y^3}{(x^2+y^4)^2} \ \end{align*} Both partial derivatives are continuous on $\mathbb{R}^2$ since they are defined as quotients of continuous functions with denominators that do not become zero. Therefore, $f$ is a $C^1$ function on $\mathbb{R}^2$ .","I would like to know if you think my answers are correct, I am not so sure about and and I don't know how to answer . Define by Is the function continuous in (0,0)? Do the partial derivatives en exist? For which does the directional derivative exist? Is differentiable in ? Is a function? To show that is continuous at , we need to show that . We can do this by using the epsilon-delta definition of a limit. To do this, we need to show that for every , there exists a such that if , then . Let and take . If , then we have: Here, we used that , since for all and observed that for all . Thus, the limit of the function exists at 0 and is continuous at 0. We prove this using the definition of partial derivative. We have with . Thus: and So the partial derivatives and exist. We look at the definition of the directional derivative . For , this gives (with ): So for no , the directional derivative exists. We use the theorem that says when is differentiable in , then the partial derivatives exist and we have the total derivative . ?? To show that is a function, we need to show that the partial derivatives exist and are continuous on . The partial derivatives of are: Both partial derivatives are continuous on since they are defined as quotients of continuous functions with denominators that do not become zero. Therefore, is a function on .","1 3 4 f: \mathbb{R}^2 \rightarrow \mathbb{R} \begin{align*}
f(x, y):= \frac{x^2 \sin y^2}{x^2+y^4}, & (x, y) \neq(0,0) & \\ f(x,y) := 0, & (x, y)=(0,0)
\end{align*} (D_1f)(0,0) (D_2f)(0,0) u \in \mathbb{R}^2 \backslash \{0\} (D_uf)(0,0) f (0,0) f C^1 f (0,0) \lim_{(x,y)\to(0,0)}f(x,y)=f(0,0)=0 \epsilon > 0 \delta > 0 ||(x,y) - (0,0)|| < \delta |f(x,y) - f(0,0)| < \epsilon \epsilon > 0 \delta = \sqrt{\epsilon} ||(x,y) - (0,0)|| < \delta \begin{align*}
|f(x,y) - f(0,0)| &= ||\frac{x^2\sin y^2}{x^2+y^4} - 0|| \
&= |\frac{x^2\sin y^2}{x^2+y^4}| \
& \leq |\frac{x^2\sin y^2}{x^2}| \
& \leq |\frac{x^2y^2}{x^2}| = y^2 \
& \leq x^2 + y^2 = \delta^2 = \epsilon.
\end{align*} x^2 \leq x^2 + y^4 y^4 \geq 0 y 0 \leq |\sin y^2| \leq 1 y f (D_jf)(a) := \lim_{t \to 0} \frac{f(a + te_j) - f(a)}{t} a = (0 \quad 0)^T \begin{align*}
(D_1f)(0,0) & = \lim_{t \to 0} \frac{f\left(\left(\begin{array}{c} 0 \ 0 \ \end{array}\right) + t\left(\begin{array}{c} 1 \ 0 \ \end{array}\right)\right) - 0}{t} \
& = \lim_{t \to 0} \frac{f(t,0) - 0}{t} \
& = \lim_{t \to 0} \frac{\frac{t^2\sin(0^2)}{t^2 + 0^4} - 0}{t} \
& = \lim_{t \to 0} \frac{\frac{t^2 \cdot 0}{t^2}}{t}\
& = \lim_{t \to 0} \frac{0}{t^3} \
& = \lim_{t \to 0} 0 \
& = 0
\end{align*} \begin{align*}
(D_2f)(0,0) & = \lim_{t \to 0} \frac{f\left(\left(\begin{array}{c} 0 \ 0 \ \end{array}\right) + t\left(\begin{array}{c} 0 \ 1 \ \end{array}\right)\right) - 0}{t} \
& = \lim_{t \to 0} \frac{f(0,t) - 0}{t} \
& = \lim_{t \to 0} \frac{\frac{0^2\sin(t^2)}{0^2 + t^4} - 0}{t}\
& = \lim_{t \to 0} \frac{\frac{0}{t^4}}{t}\
& = \lim_{t \to 0} \frac{0}{t^5} \
& = \lim_{t \to 0} 0 \
& = 0.
\end{align*} (D_1f)(0,0) (D_2f)(0,0) (D_uf)(0,0) = \lim_{t \to 0} \frac{f(a + tu) - f(a)}{t} a = (0 \quad 0)^T u := (u_1, u_2) \begin{align*}
\lim_{t \to 0} \frac{f(a + tu) - f(a)}{t} &= \lim_{t \to 0} \frac{f(tu)}{t} \
& = \lim_{t \to 0} \frac{f(t(u_1, u_2))}{t} \
&= \lim_{t \to 0} \frac{f(tu_1, tu_2)}{t} \
& = \lim_{t \to 0} \frac{\frac{t^2u_1^2 \cdot \sin(t^2u_2^2)}{t^2u_1^2 + t^4u_2^4}}{t} \
& = \lim_{t \to 0} \frac{t^2u_1^2 \cdot \sin(t^2u_2^2)}{t^3u_1^2 + t^5u_2^4} \
& = \lim_{t \to 0} \frac{u_1^2\sin(t^2u_2^2)}{tu_1^2 + t^3u_2^4}\
& = \frac{u_1^2\sin(0^2 u_2^2)}{0u_1^2 + 0^3u_2^4} \
& = \frac{0}{0}.
\end{align*} u \in \mathbb{R}^2 \backslash {0} (D_uf)(0,0) f a (D_jf_i)(a) Df(a) f C^1 \mathbb{R}^2 f \begin{align*}
\frac{\partial f}{\partial x}(x,y) &= \frac{2x\sin y^2 (x^2+y^4) - x^2\sin y^2 \cdot 2x}{(x^2+y^4)^2} \
&= \frac{2x\sin y^2 (y^4)}{(x^2+y^4)^2}, \\
\frac{\partial f}{\partial y}(x,y) &= \frac{x^2 \cdot 2y\cos y^2 (x^2+y^4) - x^2\sin y^2 \cdot 4y^3}{(x^2+y^4)^2} \
\end{align*} \mathbb{R}^2 f C^1 \mathbb{R}^2","['analysis', 'derivatives', 'solution-verification', 'continuity', 'partial-derivative']"
67,Rudin's RCA The Hahn-Banach Theorem.,Rudin's RCA The Hahn-Banach Theorem.,,"There is the theorem: If $M$ is a subspace of a normed linear space $X$ and if $f$ is a bounded linear functional on $M$ , then $f$ can be extended to a bounded linear functional $F$ on $X$ so that $||F||$ $=$ $||f||$ . There is the proof: We first assume that $X$ is a real normed linear space and, consequently, that $f$ is a real-linear bounded functional on $M$ . If $||f||$ $=$ $0$ , the desired extension is $F$ $=$ $0$ . Omitting this case, there is no loss of generality in assuming that $||f||$ $=$ $1$ . Choose $x_0$ $\in$ $X$ , $x_0$ $\notin$ $M$ , and let $M_1$ be the vector space spanned by $M$ and $x_0$ . Then $M_1$ consists of all vectors of the form $x$ $+$ $\lambda x_0$ , where $x$ $\in$ $M$ and $\lambda$ is a real scalar. If we define $f_1(x + \lambda x_0)$ $=$ $f(x)$ $+$ $\lambda \alpha$ , where $\alpha$ is any fixed real number, it is trivial to verify that an extension of $f$ to a linear functional on $M_1$ is obtained. The problem is to choose $\alpha$ so that the extended functional still has norm $1$ . This will be the case provided that $|f(x) + \lambda \alpha |$ $\leq$ $|| x + \lambda x_0||$ ( $x$ $\in$ $M$ , $\lambda$ real). Replace $x$ by $-\lambda x$ and divide both sides of $|f(x) + \lambda \alpha |$ $\leq$ $|| x + \lambda x_0||$ by $|\lambda|$ . The requirement is then that $|f(x) - \alpha|$ $\leq$ $||x-x_0||$ ( $x$ $\in$ $M$ ), i.e., that $A_x$ $\leq$ $\alpha$ $\leq$ $B_x$ for all $x$ $\in$ $M$ , where $A_x$ $=$ $f(x)$ $-$ $||x-x_0||$ and $B_x$ $=$ $f(x)$ $+$ $||x-x_0||$ . There exists such an $\alpha$ if and only if all the intervals $[A_x,B_x]$ have a common point, i.e., if and only if $A_x$ $\leq$ $B_y$ for all $x$ and $y$ $\in$ $M$ . I have two questions : $(1)$ -   How is discussing the case where $||f||$ $=$ $1$ enough for the proof of the  theorem? $(2)$ -  How do we conclude that There exists such an $\alpha$ if and only if all the intervals $[A_x,B_x]$ have a common point, i.e., if and only if $A_x$ $\leq$ $B_y$ ? Any help would be appreciated.","There is the theorem: If is a subspace of a normed linear space and if is a bounded linear functional on , then can be extended to a bounded linear functional on so that . There is the proof: We first assume that is a real normed linear space and, consequently, that is a real-linear bounded functional on . If , the desired extension is . Omitting this case, there is no loss of generality in assuming that . Choose , , and let be the vector space spanned by and . Then consists of all vectors of the form , where and is a real scalar. If we define , where is any fixed real number, it is trivial to verify that an extension of to a linear functional on is obtained. The problem is to choose so that the extended functional still has norm . This will be the case provided that ( , real). Replace by and divide both sides of by . The requirement is then that ( ), i.e., that for all , where and . There exists such an if and only if all the intervals have a common point, i.e., if and only if for all and . I have two questions : -   How is discussing the case where enough for the proof of the  theorem? -  How do we conclude that There exists such an if and only if all the intervals have a common point, i.e., if and only if ? Any help would be appreciated.","M X f M f F X ||F|| = ||f|| X f M ||f|| = 0 F = 0 ||f|| = 1 x_0 \in X x_0 \notin M M_1 M x_0 M_1 x + \lambda x_0 x \in M \lambda f_1(x + \lambda x_0) = f(x) + \lambda \alpha \alpha f M_1 \alpha 1 |f(x) + \lambda \alpha | \leq || x + \lambda x_0|| x \in M \lambda x -\lambda x |f(x) + \lambda \alpha | \leq || x + \lambda x_0|| |\lambda| |f(x) - \alpha| \leq ||x-x_0|| x \in M A_x \leq \alpha \leq B_x x \in M A_x = f(x) - ||x-x_0|| B_x = f(x) + ||x-x_0|| \alpha [A_x,B_x] A_x \leq B_y x y \in M (1) ||f|| = 1 (2) \alpha [A_x,B_x] A_x \leq B_y","['functional-analysis', 'analysis', 'linear-transformations', 'normed-spaces', 'banach-spaces']"
68,"Prove that for $0< k \leq 1$ and $x \geq k$, $\ln\left(\frac{1}{e-1}+x\right)-\ln\left(\frac{1}{e-1}+x-k\right) \leq \frac{k}{x}$","Prove that for  and ,",0< k \leq 1 x \geq k \ln\left(\frac{1}{e-1}+x\right)-\ln\left(\frac{1}{e-1}+x-k\right) \leq \frac{k}{x},"I've verified that this inequality holds when graphed, but I have no idea how to prove the actual statement. I tried to use the Mean Value Theorem but that only gives me $$\ln\left(\frac{1}{e-1}+x\right)-\ln\left(\frac{1}{e-1}+x-k\right) \leq \frac{k}{x+\frac{1}{e-1}-k},$$ which is too weak of an inequality since $\frac{1}{e-1}-k$ can be less than $0$ . Any help with this would be appreciated.","I've verified that this inequality holds when graphed, but I have no idea how to prove the actual statement. I tried to use the Mean Value Theorem but that only gives me which is too weak of an inequality since can be less than . Any help with this would be appreciated.","\ln\left(\frac{1}{e-1}+x\right)-\ln\left(\frac{1}{e-1}+x-k\right) \leq \frac{k}{x+\frac{1}{e-1}-k}, \frac{1}{e-1}-k 0","['real-analysis', 'calculus', 'analysis', 'derivatives', 'inequality']"
69,PMA Rudin Theorem 7.26: Change of variable,PMA Rudin Theorem 7.26: Change of variable,,"After $(51)$ , Rudin changes the variable $P_n(x) = \int_{-1}^{1} f(x+t)Q_n(t)dt$ $\to$ $P_n(x) = \int_{-x}^{1-x} f(x+t)Q_n(t)dt$ How does he change it? Update: Now I know It's not the change of variable And I know that $f(x) = 0$ except $x \in [0,1]$ , but I want to give a rigorous process to convince me $$\int_{-1}^{1} f(x+t)Q_n(t)dt = \int_{-x}^{1-x} f(x+t)Q_n(t)dt$$ Update: Equivalent to prove $$\int_{-1}^{-x} f(x+t)Q_n(t)dt + \int_{1-x}^{1} f(x+t)Q_n(t)dt = 0 $$ Proof : $x+t \in [x-1,0]$ in the first integral, $x+t \in [1,x+1]$ in the second, because $x \in [0,1]$ , so both of these $x+t$ outside of $[0,1]$ , so both of integrals are $0$ .","After , Rudin changes the variable How does he change it? Update: Now I know It's not the change of variable And I know that except , but I want to give a rigorous process to convince me Update: Equivalent to prove Proof : in the first integral, in the second, because , so both of these outside of , so both of integrals are .","(51) P_n(x) = \int_{-1}^{1} f(x+t)Q_n(t)dt \to P_n(x) = \int_{-x}^{1-x} f(x+t)Q_n(t)dt f(x) = 0 x \in [0,1] \int_{-1}^{1} f(x+t)Q_n(t)dt = \int_{-x}^{1-x} f(x+t)Q_n(t)dt \int_{-1}^{-x} f(x+t)Q_n(t)dt + \int_{1-x}^{1} f(x+t)Q_n(t)dt = 0  x+t \in [x-1,0] x+t \in [1,x+1] x \in [0,1] x+t [0,1] 0","['real-analysis', 'calculus']"
70,Proving an inequality of Lipschitz continuous functions,Proving an inequality of Lipschitz continuous functions,,"Let $f : [0, \infty) \rightarrow \mathbb{R}$ be a function such that $|f(x) - f(y)| \leq |x - y|, \forall x, y \geq 0$ , and let $F$ be one of its antiderivatives such that $F(0) = 0$ . a) Prove that $|yF(x) - xF(y)| \leq \frac{xy|x-y|}{2}, \forall x, y \geq 0.$ b) Knowing that $f(x) \geq 0, \forall x \geq 0$ , how many solutions does the equation $F(x) = x^2$ have? My initial thought was observing that the function is Lipschitz with $L = 1$ , thus continuous, thus Riemann integrable. Because we are told that $F(0) = 0$ , I wanted to consider $F(x) = \int_0^x f(t)dt$ and then $F(x) - F(y) = \int_y^x f(t)dt$ , but since we need $|yF(x) - xF(y)|$ , I tried both integrating $\int_y^x tf(t)dt$ and seeing where that gets me, as well as integrating the first inequality with respect to x, using the fact that $|\int_a^b f(t)dt| \leq \int_a^b |f(t)|dt$ , but to no avail. For b), we know that $F'(x) = f(x) \geq 0$ , so then $F$ is an increasing function. I believe we have to show somehow that $x_0 = 0$ is the only solution to this equation using a), but I am missing something.","Let be a function such that , and let be one of its antiderivatives such that . a) Prove that b) Knowing that , how many solutions does the equation have? My initial thought was observing that the function is Lipschitz with , thus continuous, thus Riemann integrable. Because we are told that , I wanted to consider and then , but since we need , I tried both integrating and seeing where that gets me, as well as integrating the first inequality with respect to x, using the fact that , but to no avail. For b), we know that , so then is an increasing function. I believe we have to show somehow that is the only solution to this equation using a), but I am missing something.","f : [0, \infty) \rightarrow \mathbb{R} |f(x) - f(y)| \leq |x - y|, \forall x, y \geq 0 F F(0) = 0 |yF(x) - xF(y)| \leq \frac{xy|x-y|}{2}, \forall x, y \geq 0. f(x) \geq 0, \forall x \geq 0 F(x) = x^2 L = 1 F(0) = 0 F(x) = \int_0^x f(t)dt F(x) - F(y) = \int_y^x f(t)dt |yF(x) - xF(y)| \int_y^x tf(t)dt |\int_a^b f(t)dt| \leq \int_a^b |f(t)|dt F'(x) = f(x) \geq 0 F x_0 = 0","['real-analysis', 'calculus', 'integration', 'analysis', 'lipschitz-functions']"
71,"Doubt in the proof of Theorem 6.4 in Rudin, Functional Analysis, 1973","Doubt in the proof of Theorem 6.4 in Rudin, Functional Analysis, 1973",,"Let $X$ a vector space and $(Y, \tau)$ a topological vector space such that $Y \subset X$ . Consider $W \subset X$ such that $Y \cap W \in \tau$ .  If $f \in Y \cap W$ , I have to prove that there exists an $\delta > 0$ such that $f \in (1- \delta) W$ . Edit 1: W is a convex and balanced set, that is, $\lambda W \subset W$ , for all $|\lambda| \leq 1$ . Edit 2: Y is a vector subspace of $X$ . I don't know if this is true. This doubt appeared while I was studying the proof of Theorem 6.4 in Rudin, Functional Analysis, 1973. There, I think he takes $X = C^{\infty}_0(\Omega)$ , $(Y,\tau) = \mathfrak{D}_K$ and $f = \phi - \phi_i$ :","Let a vector space and a topological vector space such that . Consider such that .  If , I have to prove that there exists an such that . Edit 1: W is a convex and balanced set, that is, , for all . Edit 2: Y is a vector subspace of . I don't know if this is true. This doubt appeared while I was studying the proof of Theorem 6.4 in Rudin, Functional Analysis, 1973. There, I think he takes , and :","X (Y, \tau) Y \subset X W \subset X Y \cap W \in \tau f \in Y \cap W \delta > 0 f \in (1- \delta) W \lambda W \subset W |\lambda| \leq 1 X X = C^{\infty}_0(\Omega) (Y,\tau) = \mathfrak{D}_K f = \phi - \phi_i","['general-topology', 'functional-analysis', 'analysis', 'topological-groups', 'topological-vector-spaces']"
72,Question to first order PDE with Characteristics Method,Question to first order PDE with Characteristics Method,,"We have the given problem \begin{align}  -2yu_{x} + u_{y} + 2yu &= 2y \tag 1 \\ u(1,y) &= 1 + e^{-1-2y^2} \tag 2 \end{align} where $x > 0, y \in \mathbb{R^*}$ . We can modify the formula $(1)$ to get \begin{align} -2yu_{x} + u_{y} &= 2y - 2yu \\ \implies -2yu_{x} + u_{y} &= 2y(1-u) \\ \implies -u_{x} + \frac{u_{y}}{2y} &= 1-u \\ \implies u_{x} - \frac{u_{y}}{2y} &= u-1 \end{align} so our problem would be \begin{align} u_{x} - \frac{u_{y}}{2y} &= u-1 \tag 3 \\ u(1,y) &= 1 + e^{-1-2y^2} \tag 4 \end{align} The method of characteristics gives $$\dfrac{dx}{1} = -2y dy = \dfrac{du}{u-1}$$ Edited after @MatthewCassell suggestion Now, we have \begin{align} \dfrac{dx}{1} =  -2y dy \implies   C_{1} &= y^2+x \end{align} and \begin{align} \dfrac{dx}{1} = \dfrac{du}{u-1} \implies u &= C_{2} e^x + 1 \\ \implies C_{2} &= \frac{u-1}{e^x} \end{align} The general solution of the PDE expressed in the form of implicit equation $$C_2=G(C_1)$$ So $$u=e^x G(x+y^2)+1$$ Now, using the initial condition $u(1,y)=1+e^{-1-2y^2}$ gives $$1+e^{-1-2y^2} = e G(1+y^2)+1 \implies G(1+y^2) = \frac{e^{-1-2y^2}}{e}$$ I get something strange (from what I've seen at least). Is this possible?","We have the given problem where . We can modify the formula to get so our problem would be The method of characteristics gives Edited after @MatthewCassell suggestion Now, we have and The general solution of the PDE expressed in the form of implicit equation So Now, using the initial condition gives I get something strange (from what I've seen at least). Is this possible?","\begin{align} 
-2yu_{x} + u_{y} + 2yu &= 2y \tag 1 \\
u(1,y) &= 1 + e^{-1-2y^2} \tag 2
\end{align} x > 0, y \in \mathbb{R^*} (1) \begin{align}
-2yu_{x} + u_{y} &= 2y - 2yu \\
\implies -2yu_{x} + u_{y} &= 2y(1-u) \\
\implies -u_{x} + \frac{u_{y}}{2y} &= 1-u \\
\implies u_{x} - \frac{u_{y}}{2y} &= u-1
\end{align} \begin{align}
u_{x} - \frac{u_{y}}{2y} &= u-1 \tag 3 \\
u(1,y) &= 1 + e^{-1-2y^2} \tag 4
\end{align} \dfrac{dx}{1} = -2y dy = \dfrac{du}{u-1} \begin{align}
\dfrac{dx}{1} =  -2y dy \implies 
 C_{1} &= y^2+x
\end{align} \begin{align}
\dfrac{dx}{1} = \dfrac{du}{u-1} \implies u &= C_{2} e^x + 1 \\
\implies C_{2} &= \frac{u-1}{e^x}
\end{align} C_2=G(C_1) u=e^x G(x+y^2)+1 u(1,y)=1+e^{-1-2y^2} 1+e^{-1-2y^2} = e G(1+y^2)+1 \implies G(1+y^2) = \frac{e^{-1-2y^2}}{e}","['analysis', 'partial-differential-equations', 'partial-derivative', 'problem-solving', 'linear-pde']"
73,"If $c > 1$, show that $\sqrt[n]{c}$ approaches $1$ for large $n$.","If , show that  approaches  for large .",c > 1 \sqrt[n]{c} 1 n,"I know there's a duplicate about this question, but I came up with this solution on my own so I need a verification. Using Bernoulli's inequality, we obtain $\forall \varepsilon > 0 $ , $\exists N > 0$ such that if $n > N$ , then $1 < 1 + n\varepsilon \le (1 + \varepsilon)^n$ . However, as $n \to \infty$ , $1 + n \varepsilon \to \infty$ . By squeeze theorem, we have $(1 + \varepsilon)^n \to \infty$ as well. Here comes the part I am not sure of. For large enough $n$ , we always could have a fixed $c$ , such that $(1 + \varepsilon)^n > c$ $\forall \varepsilon > 0$ . This yields $c^{\frac{1}{n}} - 1 < \varepsilon$ . Since $c > 1$ , we have $c^{\frac{1}{n}} > 1$ , so $\forall \varepsilon > 0$ , $c^{\frac{1}{n}} - 1 < \varepsilon$ gives the desired result. Is this proof lacking in any manners? Can we assume the bold sentence? Any help is greatly appreciated.","I know there's a duplicate about this question, but I came up with this solution on my own so I need a verification. Using Bernoulli's inequality, we obtain , such that if , then . However, as , . By squeeze theorem, we have as well. Here comes the part I am not sure of. For large enough , we always could have a fixed , such that . This yields . Since , we have , so , gives the desired result. Is this proof lacking in any manners? Can we assume the bold sentence? Any help is greatly appreciated.",\forall \varepsilon > 0  \exists N > 0 n > N 1 < 1 + n\varepsilon \le (1 + \varepsilon)^n n \to \infty 1 + n \varepsilon \to \infty (1 + \varepsilon)^n \to \infty n c (1 + \varepsilon)^n > c \forall \varepsilon > 0 c^{\frac{1}{n}} - 1 < \varepsilon c > 1 c^{\frac{1}{n}} > 1 \forall \varepsilon > 0 c^{\frac{1}{n}} - 1 < \varepsilon,"['real-analysis', 'analysis', 'solution-verification', 'epsilon-delta']"
74,Non-symmetric example where Fubini's theorem fails but the iterated integrals are equal,Non-symmetric example where Fubini's theorem fails but the iterated integrals are equal,,"I was looking for measurable functions $f\colon \mathbb R^2 \to \mathbb R$ , for which the iterated integrals $$\int_{\mathbb R} \int_{\mathbb R} f(x,y) \, dy \, dx \quad \text{and} \quad \int_{\mathbb R} \int_{\mathbb R} f(x,y) \, dx \, dy $$ exist, are finite and equal, but yet $f$ is not integrable with respect to the $2$ -dimensional Lebesgue measure, that is $$\int_{\mathbb R^2}\lvert f(x,y) \rvert \, d(x,y) = \infty. $$ The standard (and the only I could find) counter example seems to be $$f(x,y) = \frac{xy}{(x^2+y^2)^2}, \quad x,y\in [-1,1].$$ My problem with this is twofold: $f$ satisfies $f(x,y) = f(y,x)$ , which (if I do not miss something) makes it obvious that the iterated integrals must be equal (without even calculating them), since $f(-x,y)=-f(x,y)$ and $f(x,-y)=-f(x,y)$ , the inner integral respectively vanishes because of symmetry reasons. I wonder if there are more interesting functions that do not satisfy one (or both) of the two points above, but yet provide a useful example for the original problem. Thanks in advance!","I was looking for measurable functions , for which the iterated integrals exist, are finite and equal, but yet is not integrable with respect to the -dimensional Lebesgue measure, that is The standard (and the only I could find) counter example seems to be My problem with this is twofold: satisfies , which (if I do not miss something) makes it obvious that the iterated integrals must be equal (without even calculating them), since and , the inner integral respectively vanishes because of symmetry reasons. I wonder if there are more interesting functions that do not satisfy one (or both) of the two points above, but yet provide a useful example for the original problem. Thanks in advance!","f\colon \mathbb R^2 \to \mathbb R \int_{\mathbb R} \int_{\mathbb R} f(x,y) \, dy \, dx \quad \text{and} \quad \int_{\mathbb R} \int_{\mathbb R} f(x,y) \, dx \, dy  f 2 \int_{\mathbb R^2}\lvert f(x,y) \rvert \, d(x,y) = \infty.  f(x,y) = \frac{xy}{(x^2+y^2)^2}, \quad x,y\in [-1,1]. f f(x,y) = f(y,x) f(-x,y)=-f(x,y) f(x,-y)=-f(x,y)","['real-analysis', 'integration', 'analysis', 'fubini-tonelli-theorems']"
75,Norm of the inner product on an inner product space when considered as a continuous linear functional,Norm of the inner product on an inner product space when considered as a continuous linear functional,,"Let $V$ be a vector space equipped with the inner product $\langle \cdot, \cdot \rangle$ . Fix some arbitrary $y \in V$ and define the map $f_y: V \to \mathbb{C}$ by $$f_y(x) = \langle x,y \rangle, \qquad x \in V.$$ A few questions on MSE have shown that $f_y$ is a continuous linear functional with respect to the standard norm on V $\lVert x \rVert_V = \sqrt{\langle x,x \rangle}$ , for example by showing that $f_y \in B(V,\mathbb{C})$ (the space of bounded linear functionals from $V$ to $\mathbb{C}$ ). I want to show that the norm of $f_y$ in $B(V, \mathbb{C})$ equals $\lVert y \rVert_V$ . By the Cauchy-Shwarz inequality we have that $\lvert f_y(x) \rvert = \lvert \langle x,y \rangle \rvert \leq \lVert x \rVert_V \lVert y \rVert_V$ , so $f_y$ is bounded and hence continuous. I suspect that I can use this result to find an upper and lower bound for $\lVert f_y \rVert$ that are equal (in other words showing that $\lVert f_y \rVert \leq K$ and $\lVert f_y \rVert \geq K$ , for $K = \lVert y \rVert_V$ ), but am not sure on how to proceed along this line of reasoning. Any help or guidance would be much appreciated. Thanks!","Let be a vector space equipped with the inner product . Fix some arbitrary and define the map by A few questions on MSE have shown that is a continuous linear functional with respect to the standard norm on V , for example by showing that (the space of bounded linear functionals from to ). I want to show that the norm of in equals . By the Cauchy-Shwarz inequality we have that , so is bounded and hence continuous. I suspect that I can use this result to find an upper and lower bound for that are equal (in other words showing that and , for ), but am not sure on how to proceed along this line of reasoning. Any help or guidance would be much appreciated. Thanks!","V \langle \cdot, \cdot \rangle y \in V f_y: V \to \mathbb{C} f_y(x) = \langle x,y \rangle, \qquad x \in V. f_y \lVert x \rVert_V = \sqrt{\langle x,x \rangle} f_y \in B(V,\mathbb{C}) V \mathbb{C} f_y B(V, \mathbb{C}) \lVert y \rVert_V \lvert f_y(x) \rvert = \lvert \langle x,y \rangle \rvert \leq \lVert x \rVert_V \lVert y \rVert_V f_y \lVert f_y \rVert \lVert f_y \rVert \leq K \lVert f_y \rVert \geq K K = \lVert y \rVert_V","['functional-analysis', 'analysis', 'operator-theory']"
76,Show that $\lim_{n \to \infty} \nu(T^{-n}A) = \mu(A)$,Show that,\lim_{n \to \infty} \nu(T^{-n}A) = \mu(A),"The following is an exercise from my lecture notes on Ergodic Theory. I managed to solve part a), but I have doubts on one step in my solution of part b). Let $(X, \mathcal{F}, \mu)$ be a probability space and $T \colon X \to X$ a transformation. a) Let $\nu$ be a probability measure on $(X, \mathcal{F})$ that is equivalent to $\mu$ and let $f$ be the Radon-Nikodym derivative $f = \frac{d\mu}{d\nu}$ . Let $P_{T, \mu}$ and $P_{T, \nu}$ denote the Perron-Frobenius operators of $T$ with respect to the measures $\mu$ and $\nu$ . Profe that for any $g \in L^1(X, \mathcal{F}, \mu), $$ p_{T, \mu}(g) = \frac{P_{T, \nu}(fg)}{f}. $$ (I was able to show this by first considering indicator functions and then extend to simple functions by linearity and using Monotone Convergence for general functions). b) Let $\nu$ be a probability measure on $(X, \mathcal{F})$ that is absolutely continuous with respect to $\mu$ . Assume that $\mu$ is $T$ -invariant and that $T$ is strongly mixing with respect to $\mu$ . Prove that $$ \lim_{n \to \infty} \nu(T^{-n}A) = \mu(A) $$ holds for any $A \in \mathcal{F}$ . My idea for b) was to use the following fact: $T$ is strongly mixing if and only if for all $f \in L^1(X, \mathcal{F}, \mu)$ and $g \in L^\infty(X, \mathcal{F}, \mu)$ , $$ \lim_{n \to \infty} \int_X (P_T^n f) \cdot g d\mu = \int_X f d\mu \int_X g d\mu. $$ Toward this end, i want to write $$ \nu(T^{-n}A) = \int_{T^{-n}A} \,d\nu = \int_A P_{T, \nu}^n(1) \,d \nu = \int_A P_{T^n, \nu}(1) \,d\nu = \int_X \frac{1}{f} P_{T^n, \mu}(f) \cdot \chi_A \,d\nu \\ \stackrel{*}{=} \int_X P_{T^n, \mu}(f) \cdot \chi_A \, d\mu, $$ where in * i would like to use the fact that $\int_X h \, d\nu = \int_X h\cdot f \, d\mu$ . The problem is that I don`t know that $f$ is nonzero $\mu$ -a.e. I know it is nonzero $\nu$ -a.e., why writing the integral just before * is fine. If $\mu$ and $\nu$ were equivalent (as in a), then i would know that $f$ is in fact nonzero $\mu$ -a.e. and it should be fine. Is there a way to fix it without that assumption? Thanks! Let me know if I should provide additional definitions.","The following is an exercise from my lecture notes on Ergodic Theory. I managed to solve part a), but I have doubts on one step in my solution of part b). Let be a probability space and a transformation. a) Let be a probability measure on that is equivalent to and let be the Radon-Nikodym derivative . Let and denote the Perron-Frobenius operators of with respect to the measures and . Profe that for any $g \in L^1(X, \mathcal{F}, \mu), (I was able to show this by first considering indicator functions and then extend to simple functions by linearity and using Monotone Convergence for general functions). b) Let be a probability measure on that is absolutely continuous with respect to . Assume that is -invariant and that is strongly mixing with respect to . Prove that holds for any . My idea for b) was to use the following fact: is strongly mixing if and only if for all and , Toward this end, i want to write where in * i would like to use the fact that . The problem is that I don`t know that is nonzero -a.e. I know it is nonzero -a.e., why writing the integral just before * is fine. If and were equivalent (as in a), then i would know that is in fact nonzero -a.e. and it should be fine. Is there a way to fix it without that assumption? Thanks! Let me know if I should provide additional definitions.","(X, \mathcal{F}, \mu) T \colon X \to X \nu (X, \mathcal{F}) \mu f f = \frac{d\mu}{d\nu} P_{T, \mu} P_{T, \nu} T \mu \nu 
p_{T, \mu}(g) = \frac{P_{T, \nu}(fg)}{f}.
 \nu (X, \mathcal{F}) \mu \mu T T \mu 
\lim_{n \to \infty} \nu(T^{-n}A) = \mu(A)
 A \in \mathcal{F} T f \in L^1(X, \mathcal{F}, \mu) g \in L^\infty(X, \mathcal{F}, \mu) 
\lim_{n \to \infty} \int_X (P_T^n f) \cdot g d\mu = \int_X f d\mu \int_X g d\mu.
 
\nu(T^{-n}A) = \int_{T^{-n}A} \,d\nu = \int_A P_{T, \nu}^n(1) \,d \nu = \int_A P_{T^n, \nu}(1) \,d\nu = \int_X \frac{1}{f} P_{T^n, \mu}(f) \cdot \chi_A \,d\nu \\ \stackrel{*}{=} \int_X P_{T^n, \mu}(f) \cdot \chi_A \, d\mu,
 \int_X h \, d\nu = \int_X h\cdot f \, d\mu f \mu \nu \mu \nu f \mu","['analysis', 'measure-theory', 'lebesgue-integral', 'ergodic-theory', 'radon-nikodym']"
77,$x \leq y \leq z \iff |x-y|+|y-z|=|x-z|$,,x \leq y \leq z \iff |x-y|+|y-z|=|x-z|,"I want to prove one statement. if $x,y,z \in \mathbb R$ $x \leq y \leq z \iff |x-y|+|y-z|=|x-z|$ So with $\Rightarrow$ I don't have a problem. But with another direciton... The solution says: To establish the converse, show that $y<x$ and $y>z$ are impossible. For example,if $y<x \leq z$ , it follows from what we have shown and the given relationshipthat $|xây|= 0$ , so that $y=x$ , a contradiction. I didn't understand anything. How did we get this contradiction? Why are $y<x$ and $y>z$ impossible, if it's possible. Because if $y<x$ and $y>z$ , then $z<y<x$ and we have $|x-y|+|y-z|=|x-z|$ $x-y+y-z=x-z$ $x-z=x-z$ and it seems ok. Where did I make a mistake or where my missunderstanding? Thank you for help!","I want to prove one statement. if So with I don't have a problem. But with another direciton... The solution says: To establish the converse, show that and are impossible. For example,if , it follows from what we have shown and the given relationshipthat , so that , a contradiction. I didn't understand anything. How did we get this contradiction? Why are and impossible, if it's possible. Because if and , then and we have and it seems ok. Where did I make a mistake or where my missunderstanding? Thank you for help!","x,y,z \in \mathbb R x \leq y \leq z \iff |x-y|+|y-z|=|x-z| \Rightarrow y<x y>z y<x \leq z |xây|= 0 y=x y<x y>z y<x y>z z<y<x |x-y|+|y-z|=|x-z| x-y+y-z=x-z x-z=x-z","['real-analysis', 'analysis', 'absolute-value']"
78,"Let $E,F$ be finite dimensional Banach spaces and define $\delta(E,F)=\inf\{\lVert T\rVert\lVert T^{-1}\rVert|\ T:E\to F\text{ is isomorphism}\}$",Let  be finite dimensional Banach spaces and define,"E,F \delta(E,F)=\inf\{\lVert T\rVert\lVert T^{-1}\rVert|\ T:E\to F\text{ is isomorphism}\}","Let $E,F$ be finite dimensional Banach spaces and define $\delta(E,F):=\inf\{\lVert T\rVert\lVert T^{-1}\rVert|\ T:E\to F\text{ is an isomorphism}\}$ . Prove that, $\delta(E,F)=1$ iff $E$ and $F$ are isometric. For any isomorphism $T:E\to F$ , $1=\lVert id_F\rVert=\lVert T\circ T^{-1}\rVert\le \lVert T\rVert\lVert T^{-1}\rVert$ , this implies $\delta(E,F)\ge 1$ . Suppose $T:E\to F$ is an isometry then $\lVert T\rVert=1=\lVert T^{-1}\rVert\implies\delta(E,F)\le1\implies\delta(E,F)=1$ . I'm stuck with the converse part. But I have observed the following- If there is an isomorphism $T:E\to F$ such that $\lVert T\rVert\lVert T^{-1}\rVert=1$ , then $$\lVert T^{-1}(Tx)\rVert\le \lVert T\rVert^{-1}\lVert Tx\rVert\implies \lVert T\rVert\lVert x\rVert\le \lVert Tx\rVert\le\lVert T\rVert\lVert x\rVert\implies \lVert Tx\rVert=c\lVert x\rVert\implies \lVert (c^{-1}T)x\rVert=\lVert x\rVert$$ where $c=\lVert T\rVert>0$ . Hence, $S:=c^{-1}T$ is isometry between $E$ and $F$ . But I don't know whether the set $\{\lVert T\rVert\lVert T^{-1}\rVert|\ T:E\to F\text{ is isomorphism}\}$ is closed or not, if yes then $1=\delta(E,F)$ belong to the set and the above observation will complete the proof. Can anyone help me to finish the proof? Thanks for your help in advance.","Let be finite dimensional Banach spaces and define . Prove that, iff and are isometric. For any isomorphism , , this implies . Suppose is an isometry then . I'm stuck with the converse part. But I have observed the following- If there is an isomorphism such that , then where . Hence, is isometry between and . But I don't know whether the set is closed or not, if yes then belong to the set and the above observation will complete the proof. Can anyone help me to finish the proof? Thanks for your help in advance.","E,F \delta(E,F):=\inf\{\lVert T\rVert\lVert T^{-1}\rVert|\ T:E\to F\text{ is an isomorphism}\} \delta(E,F)=1 E F T:E\to F 1=\lVert id_F\rVert=\lVert T\circ T^{-1}\rVert\le \lVert T\rVert\lVert T^{-1}\rVert \delta(E,F)\ge 1 T:E\to F \lVert T\rVert=1=\lVert T^{-1}\rVert\implies\delta(E,F)\le1\implies\delta(E,F)=1 T:E\to F \lVert T\rVert\lVert T^{-1}\rVert=1 \lVert T^{-1}(Tx)\rVert\le \lVert T\rVert^{-1}\lVert Tx\rVert\implies \lVert T\rVert\lVert x\rVert\le \lVert Tx\rVert\le\lVert T\rVert\lVert x\rVert\implies \lVert Tx\rVert=c\lVert x\rVert\implies \lVert (c^{-1}T)x\rVert=\lVert x\rVert c=\lVert T\rVert>0 S:=c^{-1}T E F \{\lVert T\rVert\lVert T^{-1}\rVert|\ T:E\to F\text{ is isomorphism}\} 1=\delta(E,F)","['functional-analysis', 'analysis', 'operator-theory', 'isometry']"
79,asymptotic bound on Galton-Watson extinction probability,asymptotic bound on Galton-Watson extinction probability,,"Consider a Galton-Watson process with offspring distribution $$Z \sim \text{Binomial}(d,1-e^{-\lambda/d})$$ with $d$ a large integer and $\lambda >0$ . Let $\zeta= \zeta(\lambda)$ be the probability of extinction. We would like to prove that $\zeta \leq e^{-\lambda + \epsilon}$ for any $\epsilon >0$ so long as $\lambda$ is large enough. This seems true since, classically, we know that $\zeta$ is the unique solution to $$\zeta = \mathbf E[\zeta^Z] = (e^{-\lambda/d} + (1-e^{-\lambda/d})\zeta)^d.$$ So we can write $$\zeta^{1/d} - (1- e^{-\lambda/d})\zeta = e^{-\lambda/d}.$$ Plugging in $e^{-\lambda}$ for $\zeta$ is almost a root, since the left side is $$e^{-\lambda/d} - (1- e^{-\lambda/d} )e^{-\lambda}$$ which is $O(e^{-\lambda})$ away from $e^{-\lambda/d}$ . Is there a rigorous way to reason that $\zeta \leq e^{-\lambda + \epsilon}$ for $\lambda$ large enough?","Consider a Galton-Watson process with offspring distribution with a large integer and . Let be the probability of extinction. We would like to prove that for any so long as is large enough. This seems true since, classically, we know that is the unique solution to So we can write Plugging in for is almost a root, since the left side is which is away from . Is there a rigorous way to reason that for large enough?","Z \sim \text{Binomial}(d,1-e^{-\lambda/d}) d \lambda >0 \zeta= \zeta(\lambda) \zeta \leq e^{-\lambda + \epsilon} \epsilon >0 \lambda \zeta \zeta = \mathbf E[\zeta^Z] = (e^{-\lambda/d} + (1-e^{-\lambda/d})\zeta)^d. \zeta^{1/d} - (1- e^{-\lambda/d})\zeta = e^{-\lambda/d}. e^{-\lambda} \zeta e^{-\lambda/d} - (1- e^{-\lambda/d} )e^{-\lambda} O(e^{-\lambda}) e^{-\lambda/d} \zeta \leq e^{-\lambda + \epsilon} \lambda","['probability', 'analysis']"
80,Another definition of Riemann integrability,Another definition of Riemann integrability,,"This is the definition of Riemann integrability in terms of Riemann sums instead of upper and lower Darboux sum definition. I want to show using ONLY this definition that The Dirichlet function $f : [0, 1] â \mathbb R$ , defined by $$f(x) = \begin{cases} 1, & x â  \mathbb Q \\ 0, & x â [0, 1] - \mathbb Q \end{cases}$$ is not Riemann Integrable. How do we do that? How do we negate the statement? Can anyone help?","This is the definition of Riemann integrability in terms of Riemann sums instead of upper and lower Darboux sum definition. I want to show using ONLY this definition that The Dirichlet function , defined by is not Riemann Integrable. How do we do that? How do we negate the statement? Can anyone help?","f : [0, 1] â \mathbb R f(x) = \begin{cases}
1, & x â  \mathbb Q \\
0, & x â [0, 1] - \mathbb Q
\end{cases}","['real-analysis', 'integration', 'analysis', 'riemann-integration']"
81,Integrating exponential function over all 3D rotation matrices (SO(3)) with Euler angle parameterization.,Integrating exponential function over all 3D rotation matrices (SO(3)) with Euler angle parameterization.,,"I have a random variable $\mathbf{X}$ following an isotropic matrix normal distribution with mean $\mathbf{M}$ , i.e. $\mathrm{vec}(\mathbf{X}) \sim \mathcal{N}(\mathbf{\mathrm{vec}({\mathbf{M})}}, \sigma \mathbf{I})$ . I am, essentially, interested in figuring out how the equivalence classes / orbits induced by the $\mathcal{SO}(3)$ group (the set of all 3D rotation matrices) is distributed. That is, how the random variable $[\mathbf{X}] = \{\mathbf{X} \mathbf{R}^T \mid \mathbf{R} \in \mathcal{SO}(3)\}$ is distributed. Working with ZYX Euler angles (and omitting some constants) led me to an integral of the following form, which I would like to solve: $$ \int_{0}^{2\pi}\int_{-\pi}^\pi \int_{0}^{2\pi} \cos(\beta) \exp\left( \sum_{n=1}^N \left(\mathbf{R}(\alpha,\beta,\gamma)\mathbf{A}_n\right)^T \mathbf{B}_n \right) \ d \alpha \ d\beta \ d\gamma \tag{1} $$ with $\mathbf{A}, \mathbf{B} \in \mathbb{R}^{N \times 3}$ and $$\mathbf{R}(\alpha,\beta,\gamma) = \begin{bmatrix} \cos(\gamma) & -\sin(\gamma) & 0 \\ \sin(\gamma) & \cos(\gamma) & 0\\ 0 & 0 & 1 \end{bmatrix} \cdot \begin{bmatrix} \cos(\beta) & 0 & \sin(\beta) \\ 0 & 1 & 0 \\ -\sin(\beta) & 0 & \sin(\beta) \end{bmatrix} \cdot \begin{bmatrix} 1 & 0 & 0\\ 0 & \cos(\alpha) & -\sin(\alpha) \\ 0 & \sin(\alpha) & \cos(\alpha) \end{bmatrix}. $$ I am aware that Eq. 1 is (up to some normalizing constant) equivalent to a Haar integral of the form $$ \int_{\mathcal{SO}(3)} f(\mathbf{R}) \mu(\mathbf{R}). $$ But I do not know how to use that information. There exist some posts (e.g. 1 , 2 , 3 ) about Haar integration for $\mathcal{SO}(3)$ , but they don't discuss a specific function or how to solve the integral. There is also a question about numerically integrating over Euler angles, but I am interested in an analytical solution. I have the following two questions 1.) Is there some way of deriving a closed-form analytic expression for Eq. 1? 2.) Or does the integral in Eq. 1 correspond to some special function (e.g. Bessel function?)","I have a random variable following an isotropic matrix normal distribution with mean , i.e. . I am, essentially, interested in figuring out how the equivalence classes / orbits induced by the group (the set of all 3D rotation matrices) is distributed. That is, how the random variable is distributed. Working with ZYX Euler angles (and omitting some constants) led me to an integral of the following form, which I would like to solve: with and I am aware that Eq. 1 is (up to some normalizing constant) equivalent to a Haar integral of the form But I do not know how to use that information. There exist some posts (e.g. 1 , 2 , 3 ) about Haar integration for , but they don't discuss a specific function or how to solve the integral. There is also a question about numerically integrating over Euler angles, but I am interested in an analytical solution. I have the following two questions 1.) Is there some way of deriving a closed-form analytic expression for Eq. 1? 2.) Or does the integral in Eq. 1 correspond to some special function (e.g. Bessel function?)","\mathbf{X} \mathbf{M} \mathrm{vec}(\mathbf{X}) \sim \mathcal{N}(\mathbf{\mathrm{vec}({\mathbf{M})}}, \sigma \mathbf{I}) \mathcal{SO}(3) [\mathbf{X}] = \{\mathbf{X} \mathbf{R}^T \mid \mathbf{R} \in \mathcal{SO}(3)\} 
\int_{0}^{2\pi}\int_{-\pi}^\pi \int_{0}^{2\pi}
\cos(\beta)
\exp\left(
\sum_{n=1}^N \left(\mathbf{R}(\alpha,\beta,\gamma)\mathbf{A}_n\right)^T \mathbf{B}_n
\right)
\ d \alpha \ d\beta \ d\gamma
\tag{1}
 \mathbf{A}, \mathbf{B} \in \mathbb{R}^{N \times 3} \mathbf{R}(\alpha,\beta,\gamma)
=
\begin{bmatrix}
\cos(\gamma) & -\sin(\gamma) & 0 \\
\sin(\gamma) & \cos(\gamma) & 0\\
0 & 0 & 1
\end{bmatrix}
\cdot
\begin{bmatrix}
\cos(\beta) & 0 & \sin(\beta) \\
0 & 1 & 0 \\
-\sin(\beta) & 0 & \sin(\beta)
\end{bmatrix}
\cdot
\begin{bmatrix}
1 & 0 & 0\\
0 & \cos(\alpha) & -\sin(\alpha) \\
0 & \sin(\alpha) & \cos(\alpha)
\end{bmatrix}.
 
\int_{\mathcal{SO}(3)} f(\mathbf{R}) \mu(\mathbf{R}).
 \mathcal{SO}(3)","['integration', 'group-theory', 'analysis', 'lie-groups', 'haar-measure']"
82,Rietz representation theorem for local statements of theorems,Rietz representation theorem for local statements of theorems,,"I've known about Rietz representation theorem for some time, but I've always wondered if I got it right or I am missing something about local statements of theorems which make use of it. The Riesz theorem (let me state it for $\mathbb{R}$ not to complicate things) says that the dual of $C_0 (\mathbb{R})$ is the set $\mathcal{M} (\mathbb{R})$ of finite Radon measures on $\mathbb{R}$ , where $C_0$ denotes the functions vanishing at infinity. So $$(C_0 (\mathbb{R}))^* = \mathcal{M} (\mathbb{R}) $$ The main advantage here is that for a bounded sequence of finite Radon measures (with respect to the total variation if I am not mistaken), we have weak-star compactness by standard functional analysis results. Really nice. Now, very often it's used (implicitly) another version of the theorem, which basically says $$ (C_c (\mathbb{R}))^* = \mathcal{M}_{loc} (\mathbb{R}),$$ where $C_c (\mathbb{R})$ denotes the set of continuous functions supported in a compact set and $\mathcal{M}_{loc}$ . And so the weak-star convergence here is with respect to the functions in $C_c$ . The question is: are these two always interchangeable? I mean, the first veriosn provides a sort of more ""global"" behavior, the second one a completely local one. After all, the restriction of a locally finite Radon measure to a compact set is a finite Radon measure. I am asking this because many times in various courses there is a theorem and in the book it talks about finite Radon measures, while in class it is proved for locally finite Radon measures (one example that comes to my mind now are, Reshetnyak semicontinuity/continuity theorems). I know it's not a big deal but I want to be sure once and for all. Is it really true that one can take a theorem which uses finite Radon measures weak-star converging (with respect to functions in $C_0$ ) to something and simply change the statement by taking locally Radon measures (and weak-star convergence with respect to $C_c$ ) to have a local version of the theorem? It sounds  quite right and it honestly sounds almost trivial, but the devil is in the details and maybe I'm missing something.","I've known about Rietz representation theorem for some time, but I've always wondered if I got it right or I am missing something about local statements of theorems which make use of it. The Riesz theorem (let me state it for not to complicate things) says that the dual of is the set of finite Radon measures on , where denotes the functions vanishing at infinity. So The main advantage here is that for a bounded sequence of finite Radon measures (with respect to the total variation if I am not mistaken), we have weak-star compactness by standard functional analysis results. Really nice. Now, very often it's used (implicitly) another version of the theorem, which basically says where denotes the set of continuous functions supported in a compact set and . And so the weak-star convergence here is with respect to the functions in . The question is: are these two always interchangeable? I mean, the first veriosn provides a sort of more ""global"" behavior, the second one a completely local one. After all, the restriction of a locally finite Radon measure to a compact set is a finite Radon measure. I am asking this because many times in various courses there is a theorem and in the book it talks about finite Radon measures, while in class it is proved for locally finite Radon measures (one example that comes to my mind now are, Reshetnyak semicontinuity/continuity theorems). I know it's not a big deal but I want to be sure once and for all. Is it really true that one can take a theorem which uses finite Radon measures weak-star converging (with respect to functions in ) to something and simply change the statement by taking locally Radon measures (and weak-star convergence with respect to ) to have a local version of the theorem? It sounds  quite right and it honestly sounds almost trivial, but the devil is in the details and maybe I'm missing something.","\mathbb{R} C_0 (\mathbb{R}) \mathcal{M} (\mathbb{R}) \mathbb{R} C_0 (C_0 (\mathbb{R}))^* = \mathcal{M} (\mathbb{R})   (C_c (\mathbb{R}))^* = \mathcal{M}_{loc} (\mathbb{R}), C_c (\mathbb{R}) \mathcal{M}_{loc} C_c C_0 C_c","['real-analysis', 'functional-analysis', 'analysis', 'measure-theory']"
83,Characterization of convex functions,Characterization of convex functions,,"From now on $I$ will always denote an open interval of $\mathbb{R}$ Let $f \; : \; I \to \mathbb{R}$ be a function, $f$ is said to be convex iff $$f((1-\gamma)x + \gamma y) \leq (1 - \gamma)f(x) + \gamma f(y) \hspace{0.4cm} \forall x,y \in I \; , \; \forall \gamma \in ]0,1[$$ A well known criterion for convexity is the following First criterion of convexity If $f \; I \to \mathbb{R}$ is a function such that $$(A1)\;\;f'(x) \text{ exists } \;\; \forall x \in I$$ $$(A2)\;\;f'(x) \leq f'(y) \hspace{0.3cm} \forall x,y \in I \; : \; x \leq y$$ Then $f$ is convex The criterion gives a sufficient condition which is not necessary, the example $f(x) = |x|$ shows that. I tried to generalize the criterion to find a condition which is sufficient and necessary for the convexity, this is what I managed to do. Let $f$ be a convex function, it can be proved that the function $$\begin{split} \Phi \; : \; (I \times I) \setminus \Delta &\to \mathbb{R} \\ (x,y) &\mapsto \Phi(x,y) = \frac{f(x) - f(y)}{x - y} \end{split}$$ where $(I \times I) \setminus \Delta = \{ (x,y) \in I \times I \; : \; x \neq y\}$ is increasing both in $x$ and in $y$ . From this property it's pretty easy to show that $$(B1) \; f'^-(x) \text{ and } f'^+(x) \hspace{0.4cm} \text{ both exists and are finite} \hspace{0.3cm} \forall x \in I$$ $$(B2) \; f'^-(x) \leq f'^+(x) \hspace{0.5cm} \forall x \in I$$ $$(B3) \; f'^+(x) \leq f'^-(y) \hspace{0.5cm} \forall x,y \in I \; : \; x < y$$ where $f'^-(x) := \lim_{y \to x^-}{\frac{f(x) - f(y)}{x-y}}$ , $f'^+(x) := \lim_{y \to x^+}{\frac{f(x) - f(y)}{x-y}}$ now let $D := \{ x \in I \; \text{ such that $f'(x)$ doesn't exist} \}$ , in other words $D$ is the set of the point of non differentiability of $f$ from the properties stated above it's easy to see that $D \subseteq \{ \text{ discontinuity points of $f'^-$} \}$ and because $f'^-$ is increasing ( what I mean with ""increasing"" is what some people mean with ""non-decreasing"" ) the discontinuity points of $f'^-$ are at most countable, therefore the point of non derivability of $f$ are at most countable, therefore $f$ is almost everywhere differentiable So although $f$ isn't necessarily everywhere differentiable it is almost everywhere differentiable Generalized first criterion of convexity Let $f \; : \; I \to \mathbb{R}$ be a function, if $f$ is convex if then (B1),(B2) and (B3) holds. furthermore the non differentiable points of $f$ are at most countable. My question is, are the condition $(B1),(B2)$ and $(B3)$ also sufficient for the convexity of $f$ ? If so, how can I prove it? If they're not, what is a counterexample? There is also another criterion of convexity Second criterion of convexity if $f \; : \; I \to \mathbb{R}$ is a function such that $$(C1) f''(x) \;\; \text{ exists } \forall x \in I$$ $$(C2) f''(x) \geq 0 \;\; \forall x \in I$$ then $f$ is convex Can the Second criterion of convexity be generalized in a similar way to the first? Is there a result about the maximum cardinality of the points where $f$ isn't twice differentabile ? (similar to the result ""the points where $f$ isn't differentiable are at most countable"")","From now on will always denote an open interval of Let be a function, is said to be convex iff A well known criterion for convexity is the following First criterion of convexity If is a function such that Then is convex The criterion gives a sufficient condition which is not necessary, the example shows that. I tried to generalize the criterion to find a condition which is sufficient and necessary for the convexity, this is what I managed to do. Let be a convex function, it can be proved that the function where is increasing both in and in . From this property it's pretty easy to show that where , now let , in other words is the set of the point of non differentiability of from the properties stated above it's easy to see that and because is increasing ( what I mean with ""increasing"" is what some people mean with ""non-decreasing"" ) the discontinuity points of are at most countable, therefore the point of non derivability of are at most countable, therefore is almost everywhere differentiable So although isn't necessarily everywhere differentiable it is almost everywhere differentiable Generalized first criterion of convexity Let be a function, if is convex if then (B1),(B2) and (B3) holds. furthermore the non differentiable points of are at most countable. My question is, are the condition and also sufficient for the convexity of ? If so, how can I prove it? If they're not, what is a counterexample? There is also another criterion of convexity Second criterion of convexity if is a function such that then is convex Can the Second criterion of convexity be generalized in a similar way to the first? Is there a result about the maximum cardinality of the points where isn't twice differentabile ? (similar to the result ""the points where isn't differentiable are at most countable"")","I \mathbb{R} f \; : \; I \to \mathbb{R} f f((1-\gamma)x + \gamma y) \leq (1 - \gamma)f(x) + \gamma f(y) \hspace{0.4cm} \forall x,y \in I \; , \; \forall \gamma \in ]0,1[ f \; I \to \mathbb{R} (A1)\;\;f'(x) \text{ exists } \;\; \forall x \in I (A2)\;\;f'(x) \leq f'(y) \hspace{0.3cm} \forall x,y \in I \; : \; x \leq y f f(x) = |x| f \begin{split} \Phi \; : \; (I \times I) \setminus \Delta &\to \mathbb{R} \\
(x,y) &\mapsto \Phi(x,y) = \frac{f(x) - f(y)}{x - y} \end{split} (I \times I) \setminus \Delta = \{ (x,y) \in I \times I \; : \; x \neq y\} x y (B1) \; f'^-(x) \text{ and } f'^+(x) \hspace{0.4cm} \text{ both exists and are finite} \hspace{0.3cm} \forall x \in I (B2) \; f'^-(x) \leq f'^+(x) \hspace{0.5cm} \forall x \in I (B3) \; f'^+(x) \leq f'^-(y) \hspace{0.5cm} \forall x,y \in I \; : \; x < y f'^-(x) := \lim_{y \to x^-}{\frac{f(x) - f(y)}{x-y}} f'^+(x) := \lim_{y \to x^+}{\frac{f(x) - f(y)}{x-y}} D := \{ x \in I \; \text{ such that f'(x) doesn't exist} \} D f D \subseteq \{ \text{ discontinuity points of f'^-} \} f'^- f'^- f f f f \; : \; I \to \mathbb{R} f f (B1),(B2) (B3) f f \; : \; I \to \mathbb{R} (C1) f''(x) \;\; \text{ exists } \forall x \in I (C2) f''(x) \geq 0 \;\; \forall x \in I f f f","['calculus', 'analysis', 'derivatives', 'convex-analysis']"
84,Extending a continuous function defined on a subset of $\mathbb{R}$,Extending a continuous function defined on a subset of,\mathbb{R},"Let $E$ be a subset of $\mathbb{R}$ and let $f$ be a continuous function defined on $E$ . Is it true that $f$ can always be extended to a function $\tilde{f}$ defined on $\mathbb{R}$ , which is still continuous on $E$ ? I know that we cannot ask $\tilde{f}$ be to continuous on all $\mathbb{R}$ , which is shown by the following example: Does every continuous map from $\mathbb{Q}$ to $\mathbb{Q}$ extends continuously as a map from $\mathbb{R}$ to $\mathbb{R}$? Thank you in advance! Edit: I wanted ask if $\tilde{f}$ could be continuous at every point of $E$ . I hope it's clearer phrased this way! Edit2: From comments. For example, if $E=\{0\}$ , then $f$ with $f(0)=0$ is continuous. If we define $\tilde{f}(x)=1$ for $x\in\mathbb{R}\backslash\{0\}$ (and $\tilde{f}(x)=f(x)$ for $x\in E$ ), then the restriction of $\tilde{f}$ to $E$ is continouous but $\tilde{f}$ is not continuous at $0$ .","Let be a subset of and let be a continuous function defined on . Is it true that can always be extended to a function defined on , which is still continuous on ? I know that we cannot ask be to continuous on all , which is shown by the following example: Does every continuous map from $\mathbb{Q}$ to $\mathbb{Q}$ extends continuously as a map from $\mathbb{R}$ to $\mathbb{R}$? Thank you in advance! Edit: I wanted ask if could be continuous at every point of . I hope it's clearer phrased this way! Edit2: From comments. For example, if , then with is continuous. If we define for (and for ), then the restriction of to is continouous but is not continuous at .",E \mathbb{R} f E f \tilde{f} \mathbb{R} E \tilde{f} \mathbb{R} \tilde{f} E E=\{0\} f f(0)=0 \tilde{f}(x)=1 x\in\mathbb{R}\backslash\{0\} \tilde{f}(x)=f(x) x\in E \tilde{f} E \tilde{f} 0,['analysis']
85,"Where's the error in my ""proof"" about iterated multivariable functions?","Where's the error in my ""proof"" about iterated multivariable functions?",,"I've tried to prove a property of a kind of iterated multivariable functions I am playing with and I thought I did it, but now I've found a lot of counter examples. I'm still going to school, but I hope the formalism isn't to horrible. I started by defining injectivity, surjectivity and bijectivity with respect to a single variable for multivariable functions: Def : A funtion $f(x,y):\mathbb{R}Â²\to\mathbb{R}$ is injective/surjective/bijective  with respect to $x$ if and only if $f_y(x)=f(x,y)$ is injective/surjective/bijective for all $y\in\mathbb{R}$ . I thought that I proved the Proposition that $f(x_0,y_0)=x_0$ was true if $f(f(x_0,y_0),y_0)=x_0$ is true and $f$ is bijective w.r.t. x and continuous, but I found some counter examples. One of them is $f(x,y)=\cos(y)-xÂ³$ . I wanted to illustrate this with some pictures, but sadly I haven't gotten any reputation yet, so I can't post any. First I ""proved"" a Lemma : If the the system of equations $$f(x)=y\\f(y)=x$$ has any solutions and $f$ is continuous, x=y is true for at least one of them (this seems kind of obvious, but I couldn't find a proof online). Proof by contradiction: assume $f(x)\neq x$ for all $x\in\mathbb{R}$ . It follows from continuity that $f(x)<x$ for all $x\in\mathbb{R}$ or $f(x)>x$ for all $x\in\mathbb{R}$ . For $f(x)<x$ this means that $f(f(x))<f(x)<x$ , but the original system of equations means that $f(f(x))=x$ . This is a contradiction. The proof is exactly the same for the case $f(x)>x$ . this proves the Lemma. Now I tried to prove the Proposition . Proof : Assume $f$ is bijective w.r.t $x$ and continuous and $f(f(x_0,y_0),y_0)=x_0$ . It follows from bijectivity that there is exactly one $x':=f(x_0,y_0)$ that solves this with $f(x',y_0)=x_0$ . Fix $y_0$ and set $f_y(x)=f(x,y_0)$ . Then we have $f_y(x_0)=x'$ and $f_y(x')=x_0$ . It follows from the Lemma that $x_0=x'$ for at least one solution, but since $f_y$ is bijective this is the only solution. This means that $f(x_0, y_0)=x_0.$ q.e.d. But as I said earlier there are some counter examples, though it does seem like $f(x_0, y_0)=x_0$ is really true when $f$ is bijective w.r.t. $x$ and continuous and $f(f(f(x_0,y_0),y_0),y_0)=x_0$ , though I haven't proved that yet. Thank you for your help in advance.","I've tried to prove a property of a kind of iterated multivariable functions I am playing with and I thought I did it, but now I've found a lot of counter examples. I'm still going to school, but I hope the formalism isn't to horrible. I started by defining injectivity, surjectivity and bijectivity with respect to a single variable for multivariable functions: Def : A funtion is injective/surjective/bijective  with respect to if and only if is injective/surjective/bijective for all . I thought that I proved the Proposition that was true if is true and is bijective w.r.t. x and continuous, but I found some counter examples. One of them is . I wanted to illustrate this with some pictures, but sadly I haven't gotten any reputation yet, so I can't post any. First I ""proved"" a Lemma : If the the system of equations has any solutions and is continuous, x=y is true for at least one of them (this seems kind of obvious, but I couldn't find a proof online). Proof by contradiction: assume for all . It follows from continuity that for all or for all . For this means that , but the original system of equations means that . This is a contradiction. The proof is exactly the same for the case . this proves the Lemma. Now I tried to prove the Proposition . Proof : Assume is bijective w.r.t and continuous and . It follows from bijectivity that there is exactly one that solves this with . Fix and set . Then we have and . It follows from the Lemma that for at least one solution, but since is bijective this is the only solution. This means that q.e.d. But as I said earlier there are some counter examples, though it does seem like is really true when is bijective w.r.t. and continuous and , though I haven't proved that yet. Thank you for your help in advance.","f(x,y):\mathbb{R}Â²\to\mathbb{R} x f_y(x)=f(x,y) y\in\mathbb{R} f(x_0,y_0)=x_0 f(f(x_0,y_0),y_0)=x_0 f f(x,y)=\cos(y)-xÂ³ f(x)=y\\f(y)=x f f(x)\neq x x\in\mathbb{R} f(x)<x x\in\mathbb{R} f(x)>x x\in\mathbb{R} f(x)<x f(f(x))<f(x)<x f(f(x))=x f(x)>x f x f(f(x_0,y_0),y_0)=x_0 x':=f(x_0,y_0) f(x',y_0)=x_0 y_0 f_y(x)=f(x,y_0) f_y(x_0)=x' f_y(x')=x_0 x_0=x' f_y f(x_0, y_0)=x_0. f(x_0, y_0)=x_0 f x f(f(f(x_0,y_0),y_0),y_0)=x_0","['analysis', 'multivariable-calculus', 'fake-proofs']"
86,How to derive the following result from the uniformly law of large number?,How to derive the following result from the uniformly law of large number?,,"For a $n$ -dim Brownian motion $B_t=(B_t^1,\dots, B_t^n)$ , I would like to ask how to derive the following result from the uniformly law of large number: as $n\to \infty$ , the following converges to zero almost surely on $[0,T]^2$ , $$ \sup_{(t,s)\in [0,T]^2}\left|\frac{1}{n}\sum B_t^iB_s^i-E[\frac{1}{n}\sum B_t^iB_s^i]\right|\to 0. $$ This means $$ \sup_{(t,s)\in [0,T]^2}\left|\frac{1}{n}\sum B_t^iB_s^i-E[B_t^1B_s^1]\right|\to 0 $$ and we take sup-norm. So $\frac{1}{n}\sum B_t^iB_s^i\to E[B_t^1B_s^1]$ . This question is enough to verify the condition of ULLN and I find one lecture note of this theorem: http://www.stat.cmu.edu/~arinaldo/Teaching/36709/S19/Scribed_Lectures/Apr23_Addison.pdf It seems that we need to check the Rademacher complexity is $o(1)$ ? How to check that?","For a -dim Brownian motion , I would like to ask how to derive the following result from the uniformly law of large number: as , the following converges to zero almost surely on , This means and we take sup-norm. So . This question is enough to verify the condition of ULLN and I find one lecture note of this theorem: http://www.stat.cmu.edu/~arinaldo/Teaching/36709/S19/Scribed_Lectures/Apr23_Addison.pdf It seems that we need to check the Rademacher complexity is ? How to check that?","n B_t=(B_t^1,\dots, B_t^n) n\to \infty [0,T]^2 
\sup_{(t,s)\in [0,T]^2}\left|\frac{1}{n}\sum B_t^iB_s^i-E[\frac{1}{n}\sum B_t^iB_s^i]\right|\to 0.
 
\sup_{(t,s)\in [0,T]^2}\left|\frac{1}{n}\sum B_t^iB_s^i-E[B_t^1B_s^1]\right|\to 0
 \frac{1}{n}\sum B_t^iB_s^i\to E[B_t^1B_s^1] o(1)","['real-analysis', 'probability', 'analysis', 'law-of-large-numbers']"
87,$\int_a^b x^\alpha dx=\frac{{b^{\alpha+1}}-{a^{\alpha+1}}}{\alpha+1}$,,\int_a^b x^\alpha dx=\frac{{b^{\alpha+1}}-{a^{\alpha+1}}}{\alpha+1},"I want to show that Let $\alpha$ be a positive number. Then $\int_a^b x^\alpha dx=\frac{{b^{\alpha+1}}-{a^{\alpha+1}}}{\alpha+1}$ for $0\leq a<b$ I understand that it can be directly proved using fundamental theorems of calculs. Instead, I should use the fact that $(\alpha+1)s^{\alpha}<\frac{t^{\alpha+1}-s^{\alpha+1}}{t-s}<(\alpha+1)t^{\alpha}$ for $0\leq a<b$ . I think I can prove the below fact by using the mean value theorem, however I can't see how the two facts are related. Any suggestions please?","I want to show that Let be a positive number. Then for I understand that it can be directly proved using fundamental theorems of calculs. Instead, I should use the fact that for . I think I can prove the below fact by using the mean value theorem, however I can't see how the two facts are related. Any suggestions please?",\alpha \int_a^b x^\alpha dx=\frac{{b^{\alpha+1}}-{a^{\alpha+1}}}{\alpha+1} 0\leq a<b (\alpha+1)s^{\alpha}<\frac{t^{\alpha+1}-s^{\alpha+1}}{t-s}<(\alpha+1)t^{\alpha} 0\leq a<b,"['real-analysis', 'calculus', 'integration', 'analysis', 'riemann-integration']"
88,Question about applying Dominated Convergence Theorem,Question about applying Dominated Convergence Theorem,,"Question: $\phi_n(x)=\int_{x_0}^xf(t,\phi_n(t))dt$ , where $\phi_n(x)$ is continuous on $[a,b]$ and $f$ is continuous and bounded on $[a,b]\times(-\infty,+\infty)$ . If $\phi_n(x)$ converges uniformly to $\phi(x)$ , prove that $\phi(x)=\int_{x_0}^xf(t,\phi(t))dt$ . If $\phi_n(x)$ just converges to $\phi(x)$ , can we draw the same conclusion by applying Dominated Convergence Theorem like this: $|f|\leqslant M,\lim\limits_{n\to\infty}\int_{x_0}^xf(t,\phi_n(t))dt=\int_{x_0}^x\lim\limits_{n\to\infty}f(t,\phi_n(t))dt=\int_{x_0}^xf(t,\phi(t))dt$ , the first equation by DCT and the second by continuity of $f$ . Attempt: Since $\phi_n$ uniformly converges to $\phi$ ,then $\phi_n$ is bounded and $\phi$ is continuous on $[a,b]$ . So $f$ is uniformly continuous on $[a,b]\times[-M,M]$ ,where $|\phi_n|,|\phi|<M$ . Therefore $f(t,\phi_n(t))$ converges uniformly, so we can exchange the limit and integral. Is that correct? Thanks for checking! I'm wondering if DCT is correctly used here. Can we use $M(|f|<M)$ as the dominating function?","Question: , where is continuous on and is continuous and bounded on . If converges uniformly to , prove that . If just converges to , can we draw the same conclusion by applying Dominated Convergence Theorem like this: , the first equation by DCT and the second by continuity of . Attempt: Since uniformly converges to ,then is bounded and is continuous on . So is uniformly continuous on ,where . Therefore converges uniformly, so we can exchange the limit and integral. Is that correct? Thanks for checking! I'm wondering if DCT is correctly used here. Can we use as the dominating function?","\phi_n(x)=\int_{x_0}^xf(t,\phi_n(t))dt \phi_n(x) [a,b] f [a,b]\times(-\infty,+\infty) \phi_n(x) \phi(x) \phi(x)=\int_{x_0}^xf(t,\phi(t))dt \phi_n(x) \phi(x) |f|\leqslant M,\lim\limits_{n\to\infty}\int_{x_0}^xf(t,\phi_n(t))dt=\int_{x_0}^x\lim\limits_{n\to\infty}f(t,\phi_n(t))dt=\int_{x_0}^xf(t,\phi(t))dt f \phi_n \phi \phi_n \phi [a,b] f [a,b]\times[-M,M] |\phi_n|,|\phi|<M f(t,\phi_n(t)) M(|f|<M)","['real-analysis', 'analysis', 'convergence-divergence', 'lebesgue-integral', 'uniform-convergence']"
89,Question about Hille Yosida Theorem proof,Question about Hille Yosida Theorem proof,,"I'm working on Hille-Yosida theorem on Vrabie's book. Here is the statement: In order to prove the sufficiency two lemmas are needed: and Here comes my question, is highlighted in yellow: Why can we deduce that there exists such operator $S(t)$ ? How can we justify that such limit exist? Because of boundedness? Once we know that there exits is easy to show that $$\|e^{tA_\lambda}x-S(t)x\|\to 0$$ since we have $(3.2.5)$ and $(3.2.3)$ . On the other hand, about the words that are highlighted in blue, the convergence in norm implies uniform convergence, but can we choose that convergence just for compact subsets of $R_+$ ?.","I'm working on Hille-Yosida theorem on Vrabie's book. Here is the statement: In order to prove the sufficiency two lemmas are needed: and Here comes my question, is highlighted in yellow: Why can we deduce that there exists such operator ? How can we justify that such limit exist? Because of boundedness? Once we know that there exits is easy to show that since we have and . On the other hand, about the words that are highlighted in blue, the convergence in norm implies uniform convergence, but can we choose that convergence just for compact subsets of ?.",S(t) \|e^{tA_\lambda}x-S(t)x\|\to 0 (3.2.5) (3.2.3) R_+,"['functional-analysis', 'analysis', 'operator-theory', 'semigroup-of-operators']"
90,Why is the function $\|\mathbf{J}\|_{\infty}$ $1$-Lipschitz w.r.t to the Euclidean norm?,Why is the function  -Lipschitz w.r.t to the Euclidean norm?,\|\mathbf{J}\|_{\infty} 1,"Assume that $\mathbf{J} = \{ J \}_{ij}$ are centered independent standard Gaussian with variance $1/n$ random variables for $i, j = 1, \dots, n$ . Why is the function $\|\mathbf{J}\|_{\infty}$ then $1/n$ -Lipschitz w.r.t to the Euclidean norm, that is, $$ \left|   \sup_{\|u\| = 1} \langle u, \mathbf{J}u \rangle - \sup_{\|v\| = 1} \langle v, \mathbf{J}v \rangle \right| \leq \frac{1}{n} \|u - v\|_2 . $$ Thus, it has a sub-Gaussian tail which is $$ P(\|\mathbf{J}\|_{\infty} - E[\|\mathbf{J}\|_{\infty}] \geq x) \leq e^{-n x^2 / 2}. $$ The original statement is as follows. Moreover, it is easy to see that $$ \|\mathbf{J}\|_\infty = \sup_{\|u\| = 1} \langle u, \mathbf{J} u \rangle $$ has sub-Gaussian tail. Indeed, is is a Lipschitz function of the Gaussian entries of $\mathbf{J}$ (with respect to the Euclidean norm) with constant bounded by $N^{-1/2}$ so that by Herbst argument (see [24, 25]), we have the concentration inequality $$   P(\|\mathbf{J}\|_\infty \geq E[\|\mathbf{J}\|_\infty] + x)   \leq   e^{-\frac{1}{2} N x^2} . $$","Assume that are centered independent standard Gaussian with variance random variables for . Why is the function then -Lipschitz w.r.t to the Euclidean norm, that is, Thus, it has a sub-Gaussian tail which is The original statement is as follows. Moreover, it is easy to see that has sub-Gaussian tail. Indeed, is is a Lipschitz function of the Gaussian entries of (with respect to the Euclidean norm) with constant bounded by so that by Herbst argument (see [24, 25]), we have the concentration inequality","\mathbf{J} = \{ J \}_{ij} 1/n i, j = 1, \dots, n \|\mathbf{J}\|_{\infty} 1/n 
\left|
  \sup_{\|u\| = 1} \langle u, \mathbf{J}u \rangle
- \sup_{\|v\| = 1} \langle v, \mathbf{J}v \rangle
\right|
\leq
\frac{1}{n} \|u - v\|_2 .
 
P(\|\mathbf{J}\|_{\infty} - E[\|\mathbf{J}\|_{\infty}] \geq x)
\leq
e^{-n x^2 / 2}.
 
\|\mathbf{J}\|_\infty = \sup_{\|u\| = 1} \langle u, \mathbf{J} u \rangle
 \mathbf{J} N^{-1/2} 
  P(\|\mathbf{J}\|_\infty \geq E[\|\mathbf{J}\|_\infty] + x)
  \leq
  e^{-\frac{1}{2} N x^2} .
","['probability', 'analysis', 'statistics', 'lipschitz-functions']"
91,"Is there a special name for a norm that satisfies $||x+y|| \leq \max\{||x||, ||y||\}$?",Is there a special name for a norm that satisfies ?,"||x+y|| \leq \max\{||x||, ||y||\}","If we have a vector space $V$ with a norm $||\cdot ||$ , is there a special name for it if it satisfies $$||x+y|| \leq \max\{||x||, ||y||\} $$ for any $x,y \in V$ ? Im writing up something and I need a norm that satisfies this, but I cant recall if it has a special name. Thanks!","If we have a vector space with a norm , is there a special name for it if it satisfies for any ? Im writing up something and I need a norm that satisfies this, but I cant recall if it has a special name. Thanks!","V ||\cdot || ||x+y|| \leq \max\{||x||, ||y||\}  x,y \in V","['analysis', 'vector-spaces', 'normed-spaces']"
92,A domain satisfying the weak cone condition but not the strong cone condition,A domain satisfying the weak cone condition but not the strong cone condition,,"In the 1977 paper ""Cone Conditions and Properties of Sobolev Spaces"" by Adams and Fournier, the authors say that an open set $\Omega\subset\mathbb{R}^n$ satisfies the cone condition if each $x\in\Omega$ is the vertex of a cone $C_x$ contained in $\Omega$ congruent to a fixed cone $C$ . They then go on to define the weak cone condition. Let $x\in\Omega$ . Let $R(x)$ consist of all points $y\in\Omega$ such that the line segment from $x$ to $y$ lies completey $\Omega$ . Let $\Gamma(x)=\{y\in R(x) : |x-y|<1\}$ . $\Omega$ satisfies the weak cone condition if there is a number $\delta>0$ such that $\mu_n(\Gamma(x))\geq \delta$ . Here $\mu_n(\Gamma(x))$ denotes the Lebesgue measure of $\Gamma(x)$ . The authors note that there are many domains satisfying the weak cone condition, but not the ""strong"" cone condition. Does anyone know an example of such a domain? I've tried to construct one but have had no luck. Here are a couple of my thoughts: Any domain $\Omega$ failing the cone condition must not contain a smallest cone. That is, there cannot be an $x\in\Omega$ such that for any other $y\in\Omega$ , the cone $C_x$ may be rigidly transformed to have its vertex coincide with $y$ , and $C_x\subset C_y$ . If such an $x$ exists, then we can take our fixed reference cone to be $C_x$ and then $\Omega$ satisfies the cone condition. If $\Omega$ fails the cone condition, but satisfies the weak cone condition, then there should be a sequence $\Gamma(x_n)$ for which the diameter of the interior of $\Gamma(x_n)$ tends to 0 as $n\to\infty$ . If the diameter of the interior of $\Gamma(x)$ was bounded below for all $x$ , there there would be a smallest ball contained in $\Gamma(x)$ for all $x$ , and thus a smallest cone.","In the 1977 paper ""Cone Conditions and Properties of Sobolev Spaces"" by Adams and Fournier, the authors say that an open set satisfies the cone condition if each is the vertex of a cone contained in congruent to a fixed cone . They then go on to define the weak cone condition. Let . Let consist of all points such that the line segment from to lies completey . Let . satisfies the weak cone condition if there is a number such that . Here denotes the Lebesgue measure of . The authors note that there are many domains satisfying the weak cone condition, but not the ""strong"" cone condition. Does anyone know an example of such a domain? I've tried to construct one but have had no luck. Here are a couple of my thoughts: Any domain failing the cone condition must not contain a smallest cone. That is, there cannot be an such that for any other , the cone may be rigidly transformed to have its vertex coincide with , and . If such an exists, then we can take our fixed reference cone to be and then satisfies the cone condition. If fails the cone condition, but satisfies the weak cone condition, then there should be a sequence for which the diameter of the interior of tends to 0 as . If the diameter of the interior of was bounded below for all , there there would be a smallest ball contained in for all , and thus a smallest cone.",\Omega\subset\mathbb{R}^n x\in\Omega C_x \Omega C x\in\Omega R(x) y\in\Omega x y \Omega \Gamma(x)=\{y\in R(x) : |x-y|<1\} \Omega \delta>0 \mu_n(\Gamma(x))\geq \delta \mu_n(\Gamma(x)) \Gamma(x) \Omega x\in\Omega y\in\Omega C_x y C_x\subset C_y x C_x \Omega \Omega \Gamma(x_n) \Gamma(x_n) n\to\infty \Gamma(x) x \Gamma(x) x,"['functional-analysis', 'analysis', 'sobolev-spaces', 'regularity-theory-of-pdes']"
93,If the homeomorphism has the non-negative or non-positive determinant?,If the homeomorphism has the non-negative or non-positive determinant?,,"Let $ \Omega_1 $ and $ \Omega_2 $ be domains(open and connected) in $ \mathbb{R}^2 $ . $ \psi:\Omega_1\to\mathbb{R} $ and $ \phi:\Omega_1\to\mathbb{R} $ are $ C^1 $ functions with two variables. Moreover, we assume that map $ (x,y)\to (\phi(x,y),\psi(x,y)) $ is homeomorphism from $ \Omega_1 $ to $ \Omega_2 $ , i.e. the map $ (x,y)\to (\phi(x,y),\psi(x,y)) $ is continuous from $ \Omega_1 $ to $ \Omega_2 $ and has continuous inverse map. I want to ask that if I can obtain that the Jacobi determunant of the map, denoted as $ \frac{\partial(\phi,\psi)}{\partial(x,y)} $ is either non-positive or non-negative in $ \Omega_1 $ , i.e. either $ \frac{\partial(\phi,\psi)}{\partial(x,y)}\geq 0 $ for all $ (x,y)\in\Omega_1 $ ,  or $ \frac{\partial(\phi,\psi)}{\partial(x,y)}\leq 0 $ for all $ (x,y)\in\Omega_1 $ , . I have tried by considering the image sets of a curve in $ \Omega_1 $ and by using the connectness of $ \Omega_1 $ but failed. Can you give me some references or hints?","Let and be domains(open and connected) in . and are functions with two variables. Moreover, we assume that map is homeomorphism from to , i.e. the map is continuous from to and has continuous inverse map. I want to ask that if I can obtain that the Jacobi determunant of the map, denoted as is either non-positive or non-negative in , i.e. either for all ,  or for all , . I have tried by considering the image sets of a curve in and by using the connectness of but failed. Can you give me some references or hints?"," \Omega_1   \Omega_2   \mathbb{R}^2   \psi:\Omega_1\to\mathbb{R}   \phi:\Omega_1\to\mathbb{R}   C^1   (x,y)\to (\phi(x,y),\psi(x,y))   \Omega_1   \Omega_2   (x,y)\to (\phi(x,y),\psi(x,y))   \Omega_1   \Omega_2   \frac{\partial(\phi,\psi)}{\partial(x,y)}   \Omega_1   \frac{\partial(\phi,\psi)}{\partial(x,y)}\geq 0   (x,y)\in\Omega_1   \frac{\partial(\phi,\psi)}{\partial(x,y)}\leq 0   (x,y)\in\Omega_1   \Omega_1   \Omega_1 ","['calculus', 'general-topology', 'analysis']"
94,Hormander's identity,Hormander's identity,,"Let $\alpha=(\alpha_{1},\dots,\alpha_{n})$ a multi-index. We define $$P^{(\alpha)}(\xi)=\dfrac{\partial^{|\alpha|}P}{\partial\xi_{1}^{\alpha_{1}}\dots\partial\xi_{n}^{\alpha_{n}}}(\xi)$$ for $\xi=(\xi_{1},\dots,\xi_{n})$ and $$D^{\alpha}=(-i)^{\alpha}\dfrac{\partial^{|\alpha|}}{\partial\xi_{1}^{\alpha_{1}}\dots\partial\xi_{n}^{\alpha_{n}}}.$$ Now, let $u,v :\Omega\subset \mathbb{R}^{n}\to \mathbb{C},$ where $\Omega$ is domain, that is, open and connected. Then, by Hormander's Formula $$P(D)(uv)=\sum_{|\alpha|\leq m}\dfrac{D^{\alpha}u}{\alpha !} P^{\alpha}(D)v.$$ We will consider P(D) a linear differential operator with constant coefficients and $u=x_{k}$ and $v=\varphi \in C_{0}^{\infty},$ this formula would be $$P(D)(x_{k}\varphi)=x_{k}P(D)\varphi+P^{k}(D)\varphi.$$ But I couldn't get to that. In this case, just have a sense in $\alpha=(0,\dots,0)$ and $\alpha=(0,\dots,1,0,\dots,0)$ kth coord. In $\alpha=(0,\dots,0),$ $\dfrac{D^{\alpha}x_{k}}{\alpha!}P(D)\varphi = x_{k}P(D)\varphi$ . Its ok. Now is the problem. In $\alpha=(0,\dots,1,0,\dots,0),$ $D^{\alpha}x_k= -i.$ We can write P like be $\displaystyle P(\xi)=\sum_{|\beta|\leq m}a_\beta\xi^{\beta}=\sum_{|\beta|\leq m}a_\beta\xi_{1}^{\beta_{1}}\dots\beta\xi_{n}^{\beta_{n}}$ Then $P^{k}(\xi)=\sum_{|\beta|\leq m}a_\beta \beta_{k}\xi_{1}^{\beta_{1}}\dots\beta_{k}^{\alpha_{k}-1}\dots\xi_{n}^{\beta_{n}}$ Now i guess we need to change $\xi_{j}$ by $-i\dfrac{\partial}{\partial \xi_{k}}$ then obtain $$P^{k}(\xi)=\sum_{|\beta|\leq m}a_\beta \beta_{k} (-i\dfrac{\partial}{\partial \xi_{1}})^{\beta_{1}})\dots (-i\dfrac{\partial}{\partial \xi_{k}})^{\beta_{k}-1})\dots (-i\dfrac{\partial}{\partial \xi_{n}})^{\beta_{n}})$$ And now, what can I do? Can anybody help me, please?","Let a multi-index. We define for and Now, let where is domain, that is, open and connected. Then, by Hormander's Formula We will consider P(D) a linear differential operator with constant coefficients and and this formula would be But I couldn't get to that. In this case, just have a sense in and kth coord. In . Its ok. Now is the problem. In We can write P like be Then Now i guess we need to change by then obtain And now, what can I do? Can anybody help me, please?","\alpha=(\alpha_{1},\dots,\alpha_{n}) P^{(\alpha)}(\xi)=\dfrac{\partial^{|\alpha|}P}{\partial\xi_{1}^{\alpha_{1}}\dots\partial\xi_{n}^{\alpha_{n}}}(\xi) \xi=(\xi_{1},\dots,\xi_{n}) D^{\alpha}=(-i)^{\alpha}\dfrac{\partial^{|\alpha|}}{\partial\xi_{1}^{\alpha_{1}}\dots\partial\xi_{n}^{\alpha_{n}}}. u,v :\Omega\subset \mathbb{R}^{n}\to \mathbb{C}, \Omega P(D)(uv)=\sum_{|\alpha|\leq m}\dfrac{D^{\alpha}u}{\alpha !} P^{\alpha}(D)v. u=x_{k} v=\varphi \in C_{0}^{\infty}, P(D)(x_{k}\varphi)=x_{k}P(D)\varphi+P^{k}(D)\varphi. \alpha=(0,\dots,0) \alpha=(0,\dots,1,0,\dots,0) \alpha=(0,\dots,0), \dfrac{D^{\alpha}x_{k}}{\alpha!}P(D)\varphi = x_{k}P(D)\varphi \alpha=(0,\dots,1,0,\dots,0), D^{\alpha}x_k= -i. \displaystyle P(\xi)=\sum_{|\beta|\leq m}a_\beta\xi^{\beta}=\sum_{|\beta|\leq m}a_\beta\xi_{1}^{\beta_{1}}\dots\beta\xi_{n}^{\beta_{n}} P^{k}(\xi)=\sum_{|\beta|\leq m}a_\beta \beta_{k}\xi_{1}^{\beta_{1}}\dots\beta_{k}^{\alpha_{k}-1}\dots\xi_{n}^{\beta_{n}} \xi_{j} -i\dfrac{\partial}{\partial \xi_{k}} P^{k}(\xi)=\sum_{|\beta|\leq m}a_\beta \beta_{k} (-i\dfrac{\partial}{\partial \xi_{1}})^{\beta_{1}})\dots (-i\dfrac{\partial}{\partial \xi_{k}})^{\beta_{k}-1})\dots (-i\dfrac{\partial}{\partial \xi_{n}})^{\beta_{n}})","['analysis', 'partial-differential-equations']"
95,"Set of all real numbers in $[0,1]$ which have 5 infinitely often in decimal representation has lebesgue measure 1",Set of all real numbers in  which have 5 infinitely often in decimal representation has lebesgue measure 1,"[0,1]","I am self-studying introduction to ergodic theory and found this problem (application of Poincare recurrence theorem) which I couldn't do. I need to show the set, $E = \{x \in [0,1]: \text{decimal  representation of $x$ has $5$ infinitely often}\}$ , has measure 1. How can this be extended to show that almost all elements in $[0,1]$ contain the block (this sequence of numbers appear consecutively and in the same order) $123$ in decimal representation infinitely many times? What I did: I know, by Poincare recurrence theorem, for almost all elements in $[0.5,0.6)$ have 5 infinitely often in the decimal representation. I am not sure how the Poincare recurrence theorem can be used to extend this to the whole of $[0,1]$ . Any hints would be helpful.","I am self-studying introduction to ergodic theory and found this problem (application of Poincare recurrence theorem) which I couldn't do. I need to show the set, , has measure 1. How can this be extended to show that almost all elements in contain the block (this sequence of numbers appear consecutively and in the same order) in decimal representation infinitely many times? What I did: I know, by Poincare recurrence theorem, for almost all elements in have 5 infinitely often in the decimal representation. I am not sure how the Poincare recurrence theorem can be used to extend this to the whole of . Any hints would be helpful.","E = \{x \in [0,1]: \text{decimal  representation of x has 5 infinitely often}\} [0,1] 123 [0.5,0.6) [0,1]","['analysis', 'measure-theory', 'ergodic-theory']"
96,Feynman-Kac proof,Feynman-Kac proof,,"I have seen numerous proofs, showing that if $u\left(x,t\right)$ satisfies a PDE of the form: $$\frac{\partial u}{\partial t}+\mu\left(x,t\right) \frac{\partial u}{\partial x}+\frac{1}{2}{\left(\sigma\left(x,t\right)\right)}^2 \frac{\partial^2 u}{{\partial x}^2}-V\left(x,t\right) u+f\left(x,t\right)=0$$ then, $M_s$ defined as: $$M_s=e^{-\int\limits_{t}^{s}{V\left(\tau,X_{\tau}\right)}d\tau}u\left(s,X_s\right)+\int\limits_{t}^{s}{e^{-\int\limits_{t}^{r}{V\left(\tau,X_{\tau}\right)d\tau}}f\left(r,X_r\right)dr}$$ where $X_t$ is a random process, with evolution defined as: $$dX\left(x,t\right)=\mu\left(x,t\right) dt+\sigma\left(x,t\right) dW$$ is a martingale, and as such, its average at multiple realizations at an arbitrary point in time (i.e. terminating time), could be used as an estimator for the solution. However, I could not find any proof that such a solution actually satisfies the original equation. Any good reference or tip would be greatly appreciated. Thanks in advance!","I have seen numerous proofs, showing that if satisfies a PDE of the form: then, defined as: where is a random process, with evolution defined as: is a martingale, and as such, its average at multiple realizations at an arbitrary point in time (i.e. terminating time), could be used as an estimator for the solution. However, I could not find any proof that such a solution actually satisfies the original equation. Any good reference or tip would be greatly appreciated. Thanks in advance!","u\left(x,t\right) \frac{\partial u}{\partial t}+\mu\left(x,t\right) \frac{\partial u}{\partial x}+\frac{1}{2}{\left(\sigma\left(x,t\right)\right)}^2 \frac{\partial^2 u}{{\partial x}^2}-V\left(x,t\right) u+f\left(x,t\right)=0 M_s M_s=e^{-\int\limits_{t}^{s}{V\left(\tau,X_{\tau}\right)}d\tau}u\left(s,X_s\right)+\int\limits_{t}^{s}{e^{-\int\limits_{t}^{r}{V\left(\tau,X_{\tau}\right)d\tau}}f\left(r,X_r\right)dr} X_t dX\left(x,t\right)=\mu\left(x,t\right) dt+\sigma\left(x,t\right) dW","['analysis', 'partial-differential-equations', 'reference-request', 'stochastic-processes', 'monte-carlo']"
97,Argue that $\gamma_0$ cannot be a maximum of the action,Argue that  cannot be a maximum of the action,\gamma_0,"We consider once more the one-dimensional harmonic oscillator with the Lagrangian given by: $$L(x,v)=\frac{m}{2}v^2-\frac{k}{2}x^2,k>0$$ and the corresponding action on trajectories $\gamma$ given by the functional: $$S(\gamma)=\int_{t_1}^{t_2}L(\gamma(t),\dot{\gamma(t)})dt.$$ I have found the Euler-Lagrange equation of the system to: $$-kx=m \ddot{x}$$ And the solution to: $$x(t) = c_1 \cos(\sqrt{\frac{k}{m}} t ) + c_2 \sin(\sqrt{\frac{k}{m}} t)$$ We assume that $\gamma_0(t)$ solution of the Euler-Lagrange-equation satisfying $\gamma_0(t_1 = 0) = \xi$ and $\gamma_0(t_2 = T) = \eta$ .  And variation of the trajectory $\gamma_0$ given as $\gamma(t) = \gamma_0(t) + \nu(t)$ with $\nu$ some curve satisfying $\nu(0) = \nu(T) = 0$ . We have that $S(\gamma)$ and $S(\gamma_0)$ is given by: $$\Delta S=S(\gamma)-S(\gamma_0)=\frac{1}{2} \int_0^T m \dot \nu^2(t)-k\nu^2(t)dt$$ We have to use this to argue that that $\gamma_0$ cannot be a maximum of the action. Can anyone help me what that argue could be? I'm a bit lost",We consider once more the one-dimensional harmonic oscillator with the Lagrangian given by: and the corresponding action on trajectories given by the functional: I have found the Euler-Lagrange equation of the system to: And the solution to: We assume that solution of the Euler-Lagrange-equation satisfying and .  And variation of the trajectory given as with some curve satisfying . We have that and is given by: We have to use this to argue that that cannot be a maximum of the action. Can anyone help me what that argue could be? I'm a bit lost,"L(x,v)=\frac{m}{2}v^2-\frac{k}{2}x^2,k>0 \gamma S(\gamma)=\int_{t_1}^{t_2}L(\gamma(t),\dot{\gamma(t)})dt. -kx=m \ddot{x} x(t) = c_1 \cos(\sqrt{\frac{k}{m}} t ) + c_2 \sin(\sqrt{\frac{k}{m}} t) \gamma_0(t) \gamma_0(t_1 = 0) = \xi \gamma_0(t_2 = T) = \eta \gamma_0 \gamma(t) = \gamma_0(t) + \nu(t) \nu \nu(0) = \nu(T) = 0 S(\gamma) S(\gamma_0) \Delta S=S(\gamma)-S(\gamma_0)=\frac{1}{2} \int_0^T m \dot \nu^2(t)-k\nu^2(t)dt \gamma_0","['geometry', 'analysis', 'partial-differential-equations', 'mathematical-physics']"
98,Question on Baby Rudin theorem 10.7,Question on Baby Rudin theorem 10.7,,"This is the definition which is usable in the proof . https://i.sstatic.net/W6UoC.png . Here is the theorem: Suppose $F$ is a $\mathscr C'$ - mapping ( that means continuously differentiability) of an open set E $\subset R^n$ into $R^n$ , $0 \in E $ , $F(0) = 0$ , and $F'(0)$ is invertible. Then there is a neighborhood of $0$ in $R^n$ in which a representation: $$\mathbf{F}(\mathbf{x})=B_1\cdots B_{n-1}\mathbf{G}_n\circ \cdots \mathbf{G}_1(\mathbf{x})$$ . is valid. with each $\mathbf{G}_i$ being a primitive $\mathscr{C'}$ mapping in some neighborhood of $0$ , $\mathbf{G}_i(\mathbf{0})=0$ , and $\mathbf{G'}_i(0)$ is invertible, and each $B_i$ is either a flip or the identity operator. Here is the proof: Put $F = F_1$ . Assume $1 \leq m \leq n - 1,$ and make the following induction hypothesis ( which evidently holds for $m$ = 1): $V_m$ is a neighborhood of $0$ , $F_m$ $\in$ $\mathscr C'(V_m)$ , $F_m(0)$ = $0$ , $F_m'(0)$ is invertible, and $$P_{m-1}F_m(x) = P_{m-1}x (  x \in V_m).  (1)$$ by $(1)$ , we  have: $$F_m(x) = P_{m-1}x + \sum_{i=m}^n \alpha_i(x)e_i$$ where $\alpha_m,...,\alpha_n$ are real $\mathscr C'$ -functions in $V_m$ . Hence $F_m'(0)$$e_m$ = $\sum_{i=m}^n$ $(D_m\alpha_i)(0)$$e_i$ . ( Mark this equality by ( $\oplus$ )). Since $F_m'(0)$ is invertible, the left side of $(\oplus)$ is not $0$ , and therefore there is a $k$ such that $m$ $\leq$ $k$ $\leq$ $n$ and ( $D_m$$\alpha_k$ )( $0$ ) $\neq$ $0$ . Let $B_m$ be the flip (the definition of this is in the first link) that interchanges $m$ and this $k$ ( if $k = m$ , $B_m$ is the identity ) and define $G_m(x)$ = $x$ + [ $\alpha_k(x)$ - $x_m$ ] $e_m$ ( $x$ $\in$ $V_m$ ). Then $G_m$ $\in$ $\mathscr C'(V_m),$ $G_m$ is primitive and $G_m'(0)$ is invertible, since ( $D_m$$\alpha_k$ )( $0$ ) $\neq$ $0$ . The inverse function theorem shows therefore that there is an open set $U_m$ , with $0$ $\in$ $U_m$ $\subset$ $V_m$ , such that $G_m$ is a $1-1$ mapping of $U_m$ onto a neighborhood $V_{m+1}$ of $0$ , in which $G_m^{-1}$ is continuously differentiable. https://i.sstatic.net/m7BkJ.png ( it's the inverse function theorem). Define $F_{m+1}$ by $F_{m+1}$ ( $y$ ) = $B_m$$F_m$ $\circ$ $G_m^{-1}$ ( $y$ ). ( $y$ $\in$ $V_{m+1}$ ). ( We mean composition in $\circ$ ) Then $F_{m+1}$ $\in$ $\mathscr C'(V_{m+1})$ , $F_{m+1}$ ( $0$ ) = $0$ , and $F_{m+1}'$ ( $0$ ) is invertible. I don't understand how do we get the ( $\oplus$ ) and I also don't understand why is $F_{m+1}(0)$ equal of $0$ . Any help would be appreciated.","This is the definition which is usable in the proof . https://i.sstatic.net/W6UoC.png . Here is the theorem: Suppose is a - mapping ( that means continuously differentiability) of an open set E into , , , and is invertible. Then there is a neighborhood of in in which a representation: . is valid. with each being a primitive mapping in some neighborhood of , , and is invertible, and each is either a flip or the identity operator. Here is the proof: Put . Assume and make the following induction hypothesis ( which evidently holds for = 1): is a neighborhood of , , = , is invertible, and by , we  have: where are real -functions in . Hence = . ( Mark this equality by ( )). Since is invertible, the left side of is not , and therefore there is a such that and ( )( ) . Let be the flip (the definition of this is in the first link) that interchanges and this ( if , is the identity ) and define = + [ - ] ( ). Then is primitive and is invertible, since ( )( ) . The inverse function theorem shows therefore that there is an open set , with , such that is a mapping of onto a neighborhood of , in which is continuously differentiable. https://i.sstatic.net/m7BkJ.png ( it's the inverse function theorem). Define by ( ) = ( ). ( ). ( We mean composition in ) Then , ( ) = , and ( ) is invertible. I don't understand how do we get the ( ) and I also don't understand why is equal of . Any help would be appreciated.","F \mathscr C' \subset R^n R^n 0 \in E  F(0) = 0 F'(0) 0 R^n \mathbf{F}(\mathbf{x})=B_1\cdots B_{n-1}\mathbf{G}_n\circ \cdots \mathbf{G}_1(\mathbf{x}) \mathbf{G}_i \mathscr{C'} 0 \mathbf{G}_i(\mathbf{0})=0 \mathbf{G'}_i(0) B_i F = F_1 1 \leq m \leq n - 1, m V_m 0 F_m \in \mathscr C'(V_m) F_m(0) 0 F_m'(0) P_{m-1}F_m(x) = P_{m-1}x (  x \in V_m).  (1) (1) F_m(x) = P_{m-1}x + \sum_{i=m}^n \alpha_i(x)e_i \alpha_m,...,\alpha_n \mathscr C' V_m F_m'(0)e_m \sum_{i=m}^n (D_m\alpha_i)(0)e_i \oplus F_m'(0) (\oplus) 0 k m \leq k \leq n D_m\alpha_k 0 \neq 0 B_m m k k = m B_m G_m(x) x \alpha_k(x) x_m e_m x \in V_m G_m \in \mathscr C'(V_m), G_m G_m'(0) D_m\alpha_k 0 \neq 0 U_m 0 \in U_m \subset V_m G_m 1-1 U_m V_{m+1} 0 G_m^{-1} F_{m+1} F_{m+1} y B_mF_m \circ G_m^{-1} y y \in V_{m+1} \circ F_{m+1} \in \mathscr C'(V_{m+1}) F_{m+1} 0 0 F_{m+1}' 0 \oplus F_{m+1}(0) 0","['real-analysis', 'linear-algebra', 'analysis']"
99,Prove inequality using Lagrange's remainder,Prove inequality using Lagrange's remainder,,"My problem is the following: Prove that $\left | e^{x}+e^{-x}-2-x^{2} \right |\leq \frac{1}{6}x^{4}$ when $\left | x \right|\leq1$ . Using Maclaurin's expansion formula, I get that $$e^{x}=1+x+\frac{x^{2}}{2}+\frac{x^{3}}{6}+x^{4}\frac{e^{\Theta_1x}}{24},0\leq\Theta_1\leq1$$ $$e^{-x}=1-x+\frac{x^{2}}{2}-\frac{x^{3}}{6}+x^{4}\frac{e^{-\Theta_2x}}{24},0\leq\Theta_2\leq1$$ I can now simplify the inequality to: $$\left | x^{4}\frac{e^{\Theta_1x}}{24}+x^{4}\frac{e^{-\Theta_2x}}{24} \right |=\frac{x^{4}}{24}\left | (e^{\Theta_1x}+e^{-\Theta_2x}) \right |\leq \frac{1}{6}x^{4}\Leftrightarrow\frac{x^{4}}{4}\left | (e^{\Theta_1x}+e^{-\Theta_2x}) \right |\leq x^{4}$$ Aaaand here I'm pretty much stuck. What am I supposed to do here? I guess I have to find something in between my LHS and my RHS and then prove that that thing is less than my RHS. Thankful for help!","My problem is the following: Prove that when . Using Maclaurin's expansion formula, I get that I can now simplify the inequality to: Aaaand here I'm pretty much stuck. What am I supposed to do here? I guess I have to find something in between my LHS and my RHS and then prove that that thing is less than my RHS. Thankful for help!","\left | e^{x}+e^{-x}-2-x^{2} \right |\leq \frac{1}{6}x^{4} \left | x \right|\leq1 e^{x}=1+x+\frac{x^{2}}{2}+\frac{x^{3}}{6}+x^{4}\frac{e^{\Theta_1x}}{24},0\leq\Theta_1\leq1 e^{-x}=1-x+\frac{x^{2}}{2}-\frac{x^{3}}{6}+x^{4}\frac{e^{-\Theta_2x}}{24},0\leq\Theta_2\leq1 \left | x^{4}\frac{e^{\Theta_1x}}{24}+x^{4}\frac{e^{-\Theta_2x}}{24} \right |=\frac{x^{4}}{24}\left | (e^{\Theta_1x}+e^{-\Theta_2x}) \right |\leq \frac{1}{6}x^{4}\Leftrightarrow\frac{x^{4}}{4}\left | (e^{\Theta_1x}+e^{-\Theta_2x}) \right |\leq x^{4}","['calculus', 'analysis', 'inequality', 'taylor-expansion', 'absolute-value']"
