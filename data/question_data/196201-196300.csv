,title_raw,title_text,title_latex,body_raw,body_text,body_latex,tags
0,Check on differentiability of a function,Check on differentiability of a function,,"I need a check on the following exercise: Consider the following function $$ f(x,y)= \begin{cases} \frac{x^3 y}{x^2 + y^4} \quad (x,y) \ne (0,0) \\ 0\quad \quad (x,y)=(0,0) \end{cases} $$ determine where the function is differentiable. I checked the function is continuous over the whole $\mathbb{R}^2$ and that the derivatives $\partial_x f(0,0)=\partial_y f(0,0)=0$ . Differentiability: first of all, for any point $(x,y)\ne (0,0)$ the partial derivatives are continuous at $(x,y)$ and they exist in a neigbourhood of $(x,y)$ . The tricky part is for $(x,y)=(0,0)$ . There I know only the value of the derivatives at $(0,0)$ but I have no other information, therefore I have to study the following limit: $$\lim_{(h,k) \rightarrow (0,0)} \frac{f(h,k)}{\sqrt{h^2+k^2}}$$ I note that $$|\frac{h^3 k}{(h^2 + k^4) \sqrt{h^2+k^2}}| = |\frac{h^2 \cdot h k}{(h^2 + k^4) \sqrt{h^2+k^2}}| $$ and since $$|\frac{h^2}{h^2 + k^4}|<1$$ I have $$|\frac{h^2 \cdot h k}{(h^2 + k^4) \sqrt{h^2+k^2}}|<|\frac{hk}{\sqrt{h^2+k^2}}| < |\frac{hk}{|h|}| =_{\text{h} \ne 0} |k|$$ (if $h=0$ then the limit is identically $0$ ) Now, as $(h,k) \rightarrow (0,0)$ the r.h.s goes to $0$ and hence the limit is $0$ and the function is differentiable. Therefore the function is differentiable on the whole $\mathbb{R}^2$ , right? is everything correct","I need a check on the following exercise: Consider the following function determine where the function is differentiable. I checked the function is continuous over the whole and that the derivatives . Differentiability: first of all, for any point the partial derivatives are continuous at and they exist in a neigbourhood of . The tricky part is for . There I know only the value of the derivatives at but I have no other information, therefore I have to study the following limit: I note that and since I have (if then the limit is identically ) Now, as the r.h.s goes to and hence the limit is and the function is differentiable. Therefore the function is differentiable on the whole , right? is everything correct","
f(x,y)=
\begin{cases}
\frac{x^3 y}{x^2 + y^4} \quad (x,y) \ne (0,0) \\
0\quad \quad (x,y)=(0,0)
\end{cases}
 \mathbb{R}^2 \partial_x f(0,0)=\partial_y f(0,0)=0 (x,y)\ne (0,0) (x,y) (x,y) (x,y)=(0,0) (0,0) \lim_{(h,k) \rightarrow (0,0)} \frac{f(h,k)}{\sqrt{h^2+k^2}} |\frac{h^3 k}{(h^2 + k^4) \sqrt{h^2+k^2}}| = |\frac{h^2 \cdot h k}{(h^2 + k^4) \sqrt{h^2+k^2}}|  |\frac{h^2}{h^2 + k^4}|<1 |\frac{h^2 \cdot h k}{(h^2 + k^4) \sqrt{h^2+k^2}}|<|\frac{hk}{\sqrt{h^2+k^2}}| < |\frac{hk}{|h|}| =_{\text{h} \ne 0} |k| h=0 0 (h,k) \rightarrow (0,0) 0 0 \mathbb{R}^2","['calculus', 'limits', 'multivariable-calculus', 'derivatives']"
1,What is the number of weekly tables that generate the most profit?,What is the number of weekly tables that generate the most profit?,,"A cabinetmaker designs and sells $q$ wooden tables per week. The relation $p=125\frac{\ln(q)}{q}$ is the price, in dollars, when $q$ tables are demanded. The average cost of producing $q$ units is: $C(q)=\frac{q^2}{3}$ with $2\leq q\leq 9$ What is the number of weekly tables that generate the most profit? In order to find the maximum value of $U$ , the critical points are determined in the usual way and then its nature is investigated. $I$ = income $C$ = costs $U$ = profit $C'(q)=\frac{2q}{3}$ $I(q)=pq=125\frac{\ln(q)}{q}q$ $I'(q)=\frac{125}{q}$ $U'(q)=I'(q)-C'(q)$ $0=\frac{375-2q^2}{3q}$ $q=13.69$","A cabinetmaker designs and sells wooden tables per week. The relation is the price, in dollars, when tables are demanded. The average cost of producing units is: with What is the number of weekly tables that generate the most profit? In order to find the maximum value of , the critical points are determined in the usual way and then its nature is investigated. = income = costs = profit",q p=125\frac{\ln(q)}{q} q q C(q)=\frac{q^2}{3} 2\leq q\leq 9 U I C U C'(q)=\frac{2q}{3} I(q)=pq=125\frac{\ln(q)}{q}q I'(q)=\frac{125}{q} U'(q)=I'(q)-C'(q) 0=\frac{375-2q^2}{3q} q=13.69,"['derivatives', 'optimization']"
2,differentiate $\hat\theta$ by $\theta$ = zero or 1?,differentiate  by  = zero or 1?,\hat\theta \theta,"In this picture, $\hat\theta$ is an unbiased estimator of $\theta$ . You may assume 'f(x)' as a standard normal distribution. the second line assume that, if you differentiate $\hat\theta$ by $\theta$ , it becomes 0. It means, $\hat\theta$ is just a constant number to $\theta$ . But isn't $\hat\theta$ a function of $\theta$ ? Because it imitates and follows $\theta$ . You may say that $$\hat\theta + e = \theta+\epsilon$$ then, it is associated together, so differentiating $\hat\theta$ by $\theta$ will be 1, not 0. How do you think?","In this picture, is an unbiased estimator of . You may assume 'f(x)' as a standard normal distribution. the second line assume that, if you differentiate by , it becomes 0. It means, is just a constant number to . But isn't a function of ? Because it imitates and follows . You may say that then, it is associated together, so differentiating by will be 1, not 0. How do you think?",\hat\theta \theta \hat\theta \theta \hat\theta \theta \hat\theta \theta \theta \hat\theta + e = \theta+\epsilon \hat\theta \theta,"['derivatives', 'regression', 'fisher-information']"
3,"Show that $\partial (f+g)(x_0)(a+b) = \partial f(x_0)(a) + \partial g(x_0)(a) + \partial f(x_0)(b) + \partial g(x_0)(b)$ for all $a,b \in X$",Show that  for all,"\partial (f+g)(x_0)(a+b) = \partial f(x_0)(a) + \partial g(x_0)(a) + \partial f(x_0)(b) + \partial g(x_0)(b) a,b \in X","My textbook Analysis II by Amann defines derivative as follows: Let $E=(E,\|\cdot\|)$ and $F=(F,\|\cdot\|)$ are Banach spaces over the field $\mathbb{K}$ ; $X$ is an open subset of $E$ . A function $f: X \rightarrow F$ is differentiable at $x_{0} \in X$ if there is an $A_{x_{0}} \in \mathcal{L}(E, F)$ such that $$ \lim _{x \rightarrow x_{0}} \frac{f(x)-f\left(x_{0}\right)-A_{x_{0}}\left(x-x_{0}\right)}{\left\|x-x_{0}\right\|}=0. $$ Then we write $\partial f(x_0)$ for $A_{x_0}$ . Could you please confirm if my below understanding is correct? Assume that $f,g:X \to F$ are differentiable at $x_0 \in X$ . Then there are $\partial f(x_0), \partial g(x_0) \in \mathcal{L}(E, F)$ such that $$ \lim _{x \rightarrow x_{0}} \frac{f(x)-f\left(x_{0}\right)-\partial f(x_0)\left(x-x_{0}\right)}{\left\|x-x_{0}\right\|}=0 $$ and $$ \lim _{x \rightarrow x_{0}} \frac{g(x)-g\left(x_{0}\right)-\partial g(x_0)\left(x-x_{0}\right)}{\left\|x-x_{0}\right\|}=0. $$ It follows that $$ \lim _{x \rightarrow x_{0}} \frac{(f+g)(x)-(f+g)\left(x_{0}\right)-(\partial f(x_0) + \partial g(x_0))\left(x-x_{0}\right)}{\left\|x-x_{0}\right\|}=0. $$ Because the sum of $2$ continuous linear maps is again continuous linear. Then $\partial (f+g)(x_0) = \partial f(x_0) + \partial g(x_0)$ . As such, $$\partial (f+g)(x_0)(a+b) = \partial f(x_0)(a) + \partial g(x_0)(a) + \partial f(x_0)(b) + \partial g(x_0)(b)$$ for all $a,b \in X$ .","My textbook Analysis II by Amann defines derivative as follows: Let and are Banach spaces over the field ; is an open subset of . A function is differentiable at if there is an such that Then we write for . Could you please confirm if my below understanding is correct? Assume that are differentiable at . Then there are such that and It follows that Because the sum of continuous linear maps is again continuous linear. Then . As such, for all .","E=(E,\|\cdot\|) F=(F,\|\cdot\|) \mathbb{K} X E f: X \rightarrow F x_{0} \in X A_{x_{0}} \in \mathcal{L}(E, F) 
\lim _{x \rightarrow x_{0}} \frac{f(x)-f\left(x_{0}\right)-A_{x_{0}}\left(x-x_{0}\right)}{\left\|x-x_{0}\right\|}=0.
 \partial f(x_0) A_{x_0} f,g:X \to F x_0 \in X \partial f(x_0), \partial g(x_0) \in \mathcal{L}(E, F) 
\lim _{x \rightarrow x_{0}} \frac{f(x)-f\left(x_{0}\right)-\partial f(x_0)\left(x-x_{0}\right)}{\left\|x-x_{0}\right\|}=0
 
\lim _{x \rightarrow x_{0}} \frac{g(x)-g\left(x_{0}\right)-\partial g(x_0)\left(x-x_{0}\right)}{\left\|x-x_{0}\right\|}=0.
 
\lim _{x \rightarrow x_{0}} \frac{(f+g)(x)-(f+g)\left(x_{0}\right)-(\partial f(x_0) + \partial g(x_0))\left(x-x_{0}\right)}{\left\|x-x_{0}\right\|}=0.
 2 \partial (f+g)(x_0) = \partial f(x_0) + \partial g(x_0) \partial (f+g)(x_0)(a+b) = \partial f(x_0)(a) + \partial g(x_0)(a) + \partial f(x_0)(b) + \partial g(x_0)(b) a,b \in X","['functional-analysis', 'derivatives', 'banach-spaces', 'frechet-derivative']"
4,"What exactly is the reasoning for why $\frac{d}{dx}u(x, Ce^x) = \frac{\partial{u}}{\partial{x}} + Ce^x \frac{\partial{u}}{\partial{y}}$?",What exactly is the reasoning for why ?,"\frac{d}{dx}u(x, Ce^x) = \frac{\partial{u}}{\partial{x}} + Ce^x \frac{\partial{u}}{\partial{y}}","I am currently studying the textbook Partial Differential Equations – An introduction , second edition, by Walter A. Strauss. The section The Variable Coefficient Equation of chapter 1 says the following: The equation $$u_x + y u_y = 0 \label{4}\tag{4}$$ is linear and homogeneous but has a variable coefficient ( $y$ ). We shall illustrate for equation \eqref{4} how to use the geometric method somewhat like Example 1. The PDE \eqref{4}  itself asserts that the directional derivative in the direction of the vector $(1, y)$ is zero . The curves in the $xy$ plane with $(1, y)$ as tangent vectors have slopes $y$ (see Figure 3). Their equations are $$\dfrac{dy}{dx} = \dfrac{y}{1} \label{5}\tag{5}$$ This ODE has the solutions $$y = Ce^x \label{6}\tag{6}$$ These curves are called the characteristic curves of the PDE \eqref{4} . As $C$ is changed, the curves fill out the $xy$ plane perfectly without intersecting. On each of the curves $u(x, y)$ is a constant because $$\dfrac{d}{dx}u(x, Ce^x) = \dfrac{\partial{u}}{\partial{x}} + Ce^x \dfrac{\partial{u}}{\partial{y}} = u_x + yu_y = 0.$$ What exactly is the reasoning for why $\dfrac{d}{dx}u(x, Ce^x) = \dfrac{\partial{u}}{\partial{x}} + Ce^x \dfrac{\partial{u}}{\partial{y}}$ ? This seems to be an application of the chain rule, but I don't understand the reasoning behind why the chain rule is appropriate for this case, or for how it is applied.","I am currently studying the textbook Partial Differential Equations – An introduction , second edition, by Walter A. Strauss. The section The Variable Coefficient Equation of chapter 1 says the following: The equation is linear and homogeneous but has a variable coefficient ( ). We shall illustrate for equation \eqref{4} how to use the geometric method somewhat like Example 1. The PDE \eqref{4}  itself asserts that the directional derivative in the direction of the vector is zero . The curves in the plane with as tangent vectors have slopes (see Figure 3). Their equations are This ODE has the solutions These curves are called the characteristic curves of the PDE \eqref{4} . As is changed, the curves fill out the plane perfectly without intersecting. On each of the curves is a constant because What exactly is the reasoning for why ? This seems to be an application of the chain rule, but I don't understand the reasoning behind why the chain rule is appropriate for this case, or for how it is applied.","u_x + y u_y = 0 \label{4}\tag{4} y (1, y) xy (1, y) y \dfrac{dy}{dx} = \dfrac{y}{1} \label{5}\tag{5} y = Ce^x \label{6}\tag{6} C xy u(x, y) \dfrac{d}{dx}u(x, Ce^x) = \dfrac{\partial{u}}{\partial{x}} + Ce^x \dfrac{\partial{u}}{\partial{y}} = u_x + yu_y = 0. \dfrac{d}{dx}u(x, Ce^x) = \dfrac{\partial{u}}{\partial{x}} + Ce^x \dfrac{\partial{u}}{\partial{y}}","['multivariable-calculus', 'derivatives', 'partial-derivative', 'chain-rule']"
5,Cross-entropy loss and stationary points,Cross-entropy loss and stationary points,,"I am trying to find the stationary points of the cross-entropy function for binary classification : $$ L(w) = -y \cdot \log(\sigma(wx)) - (1-y) \cdot \log (1-\sigma(wx)) $$ with $$ \sigma(wx) = \frac{1}{1+e^{-wx}} $$ If I develop and simplify I get: $$ L(w) = \log(1+e^{-wx}) + wx(1-y) $$ To find the stationary points in the close form I want to differentiate and solve for 0 $$ \frac{dL}{dw} = 0 $$ $$ \frac{dL}{dw} = \frac{-xe^{-wx}}{1+e^{-wx}} + x(1-y) = 0 $$ $$ \frac{xe^{-wx}}{1+e^{-wx}} = x(1-y) $$ $$ \frac{x}{1+e^{wx}} = x(1-y) $$ Here I start to face some problems: First, I am not sure that $x \neq 0$ Second, $y \in (0;1)$ , so obviously I am going to face some problems What am I doing wrong and how can I solve this problem? Thank you","I am trying to find the stationary points of the cross-entropy function for binary classification : with If I develop and simplify I get: To find the stationary points in the close form I want to differentiate and solve for 0 Here I start to face some problems: First, I am not sure that Second, , so obviously I am going to face some problems What am I doing wrong and how can I solve this problem? Thank you","
L(w) = -y \cdot \log(\sigma(wx)) - (1-y) \cdot \log (1-\sigma(wx))
 
\sigma(wx) = \frac{1}{1+e^{-wx}}
 
L(w) = \log(1+e^{-wx}) + wx(1-y)
 
\frac{dL}{dw} = 0
 
\frac{dL}{dw} = \frac{-xe^{-wx}}{1+e^{-wx}} + x(1-y) = 0
 
\frac{xe^{-wx}}{1+e^{-wx}} = x(1-y)
 
\frac{x}{1+e^{wx}} = x(1-y)
 x \neq 0 y \in (0;1)","['calculus', 'derivatives', 'entropy', 'logistic-regression', 'stationary-point']"
6,"Let $f:[-\frac 12,2]\to R$ and $g: [-\frac 12,2] \to R$ be defined as $f(x)=[x^2-3]$ and $g(x)=|x|f(x)+|4x-7|f(x)$",Let  and  be defined as  and,"f:[-\frac 12,2]\to R g: [-\frac 12,2] \to R f(x)=[x^2-3] g(x)=|x|f(x)+|4x-7|f(x)","CONT Let $f:[-\frac 12,2]\to R$ and $g: [-\frac 12,2] \to R$ be defined as $f(x)=[x^2-3]$ and $g(x)=|x|f(x)+|4x-7|f(x)$ . Find the number of points of non differentiability of $g(x)$ The points we need to check for are $0,1,\sqrt 2, \sqrt 3, 2, \frac 74$ I don’t really know the proper way to do this, but instinctually $1,\sqrt 3, \sqrt 2$ will be the correc points. By computation, I proved that the function is differentiable at $x=\frac 74$ In $g(x)$ , the first term is non differentiable at $x=0$ but the second term isn’t, so the function should be non differentiable at $0$ (Emphasis on should because the grap shows that the function is continuous and differentiable at $x=0$ ) I can’t speak for $2$ , because the function is continuous over that point, but I don’t if extremes are counted in differentiability Is my answer correct?","CONT Let and be defined as and . Find the number of points of non differentiability of The points we need to check for are I don’t really know the proper way to do this, but instinctually will be the correc points. By computation, I proved that the function is differentiable at In , the first term is non differentiable at but the second term isn’t, so the function should be non differentiable at (Emphasis on should because the grap shows that the function is continuous and differentiable at ) I can’t speak for , because the function is continuous over that point, but I don’t if extremes are counted in differentiability Is my answer correct?","f:[-\frac 12,2]\to R g: [-\frac 12,2] \to R f(x)=[x^2-3] g(x)=|x|f(x)+|4x-7|f(x) g(x) 0,1,\sqrt 2, \sqrt 3, 2, \frac 74 1,\sqrt 3, \sqrt 2 x=\frac 74 g(x) x=0 0 x=0 2","['derivatives', 'continuity']"
7,Could I ask where i mistook in using riccati matrix equation?,Could I ask where i mistook in using riccati matrix equation?,,"I was trying to solve $$ \dot{x}=ax^2+bxy\tag{1}\label{eq1}$$ $$ \dot{y}=cy^2+dxy \tag{2}\label{eq2}$$ According to riccati equation's initial showing $$ \frac{d}{dt} \left( \begin{bmatrix} M(t) \\ N(t) \end{bmatrix} \right)  =  \begin{bmatrix} -A&-B\\ C&D  \end{bmatrix} \begin{bmatrix} M(t) \\ N(t) \end{bmatrix} where  \ P=N(t)M(t)^{-1}, then \ \dot{P}= PBP + AP + PD + C \tag{3}\label{eq3}$$ To match \eqref{eq1} and \eqref{eq2} with the form \eqref{eq3}, I applied small change as follows, $$ \frac{d}{dt} \left( \begin{bmatrix} x \\ y \end{bmatrix} \right)  =  \begin{bmatrix} x& 0\\ 0& y  \end{bmatrix} \begin{bmatrix} a& b\\ c& d  \end{bmatrix} \begin{bmatrix} x \\ y \end{bmatrix} \tag{4}\label{eq4}$$ and changed to the following $$ \frac{d}{dt} \left( \begin{bmatrix} x& 0\\ 0& y  \end{bmatrix} \right)\begin{bmatrix} 1 \\ 1 \end{bmatrix}  =  \begin{bmatrix} x& 0\\ 0& y  \end{bmatrix} \begin{bmatrix} a& b\\ c& d  \end{bmatrix} \begin{bmatrix} x& 0\\ 0& y  \end{bmatrix} \begin{bmatrix} 1 \\ 1 \end{bmatrix}\tag{5}\label{eq5} $$ This form looks similar to \eqref{eq3} $$ \dot{P}=PBP \tag{6}\label{eq6} $$ Therefore, I transformed \eqref{eq6} to \eqref{eq3} $$ \frac{d}{dt} \left( \begin{bmatrix} M(t) \\ N(t) \end{bmatrix} \right)  =  \begin{bmatrix} 0&-B\\ 0&0  \end{bmatrix} \begin{bmatrix} M(t) \\ N(t) \end{bmatrix} where  \ X=N(t)M(t)^{-1} \tag{7}\label{eq7} \ and \ \, H=\begin{bmatrix} 0&-B\\ 0&0  \end{bmatrix}$$ Unlike riccati matrix equation, H in \eqref{eq7} is singular. So, directly applying riccati method seems not possible. Therefore, I chose exponential matrix of H. $$ \begin{bmatrix} M(t) \\ N(t) \end{bmatrix} = e^{Ht} \begin{bmatrix} M(0) \\ N(0) \end{bmatrix} , where \ X=N(t)M(t)^{-1} \tag{8}\label{eq8} $$ and I multiply $$ \begin{bmatrix} 1 \\ 1 \end{bmatrix}$$ to  X in \eqref{eq8}.$$ Ok, I tested the result at the 1 dimension such as $$\dot{x} =ax^2$$ I showed well. However, when I applied it into 2 dimensional problem that I wondered such as \eqref{eq1}  \eqref{eq2}, It does not satisfied. Could you let me know why and which part of my assumption was incorrect? Such as $$\begin{bmatrix} x& 0\\ 0& y  \end{bmatrix} \begin{bmatrix} 1 \\ 1 \end{bmatrix}$$ or etc.","I was trying to solve According to riccati equation's initial showing To match \eqref{eq1} and \eqref{eq2} with the form \eqref{eq3}, I applied small change as follows, and changed to the following This form looks similar to \eqref{eq3} Therefore, I transformed \eqref{eq6} to \eqref{eq3} Unlike riccati matrix equation, H in \eqref{eq7} is singular. So, directly applying riccati method seems not possible. Therefore, I chose exponential matrix of H. and I multiply to  X in \eqref{eq8}.$$ Ok, I tested the result at the 1 dimension such as I showed well. However, when I applied it into 2 dimensional problem that I wondered such as \eqref{eq1}  \eqref{eq2}, It does not satisfied. Could you let me know why and which part of my assumption was incorrect? Such as or etc."," \dot{x}=ax^2+bxy\tag{1}\label{eq1}  \dot{y}=cy^2+dxy \tag{2}\label{eq2} 
\frac{d}{dt} \left( \begin{bmatrix}
M(t) \\
N(t)
\end{bmatrix} \right)
 = 
\begin{bmatrix}
-A&-B\\
C&D 
\end{bmatrix}
\begin{bmatrix}
M(t) \\
N(t)
\end{bmatrix}
where  \ P=N(t)M(t)^{-1}, then \ \dot{P}= PBP + AP + PD + C
\tag{3}\label{eq3} 
\frac{d}{dt} \left( \begin{bmatrix}
x \\
y
\end{bmatrix} \right)
 = 
\begin{bmatrix}
x& 0\\
0& y 
\end{bmatrix}
\begin{bmatrix}
a& b\\
c& d 
\end{bmatrix}
\begin{bmatrix}
x \\
y
\end{bmatrix}
\tag{4}\label{eq4} 
\frac{d}{dt} \left( \begin{bmatrix}
x& 0\\
0& y 
\end{bmatrix} \right)\begin{bmatrix}
1 \\
1
\end{bmatrix}
 = 
\begin{bmatrix}
x& 0\\
0& y 
\end{bmatrix}
\begin{bmatrix}
a& b\\
c& d 
\end{bmatrix}
\begin{bmatrix}
x& 0\\
0& y 
\end{bmatrix} \begin{bmatrix}
1 \\
1
\end{bmatrix}\tag{5}\label{eq5}
 
\dot{P}=PBP \tag{6}\label{eq6}
 
\frac{d}{dt} \left( \begin{bmatrix}
M(t) \\
N(t)
\end{bmatrix} \right)
 = 
\begin{bmatrix}
0&-B\\
0&0 
\end{bmatrix}
\begin{bmatrix}
M(t) \\
N(t)
\end{bmatrix}
where  \ X=N(t)M(t)^{-1} \tag{7}\label{eq7} \ and \ \, H=\begin{bmatrix}
0&-B\\
0&0 
\end{bmatrix} 
\begin{bmatrix}
M(t) \\
N(t)
\end{bmatrix} = e^{Ht}
\begin{bmatrix}
M(0) \\
N(0)
\end{bmatrix}
, where \ X=N(t)M(t)^{-1} \tag{8}\label{eq8}
  \begin{bmatrix} 1 \\ 1 \end{bmatrix} \dot{x} =ax^2 \begin{bmatrix}
x& 0\\
0& y 
\end{bmatrix} \begin{bmatrix}
1 \\
1
\end{bmatrix}","['ordinary-differential-equations', 'derivatives', 'differential-topology', 'nonlinear-system']"
8,A Strange but seemingly easy functional equation problem,A Strange but seemingly easy functional equation problem,,I was solving a much larger problem and hence reduced it to a functional equation which looks a little strange but also seems easy to solve. Question : Find all differentiable functions f defined on entire reals and differentiable on reals such that is satisfies the following $$ f(2n) -f(-2n) = f'(2n) - f'(-2n) +4n^2 $$ for natural numbers n. I need some help to solve this out so if anyone can help it will be appreciated and helpful.,I was solving a much larger problem and hence reduced it to a functional equation which looks a little strange but also seems easy to solve. Question : Find all differentiable functions f defined on entire reals and differentiable on reals such that is satisfies the following for natural numbers n. I need some help to solve this out so if anyone can help it will be appreciated and helpful., f(2n) -f(-2n) = f'(2n) - f'(-2n) +4n^2 ,"['real-analysis', 'derivatives', 'functional-equations']"
9,"How prove this $x^3<\sin^2{x}\tan{x},x\in\left(0,\dfrac{\pi}{2}\right)$",How prove this,"x^3<\sin^2{x}\tan{x},x\in\left(0,\dfrac{\pi}{2}\right)","show that $$x^3<\sin^2{x}\tan{x},x\in\left(0,\dfrac{\pi}{2}\right)$$ have nice methods? Thank you my try: $$\Longleftrightarrow \cos{x}\cdot x^3<(\sin{x})^3$$ let $$f(x)=\cos{x}\cdot x^3-(\sin{x})^3$$ $$\Longrightarrow f'(x)=-\sin{x}\cdot x^3+3\cos{x}\cdot x^2-3(\sin{x})^2\cos{x}$$","show that $$x^3<\sin^2{x}\tan{x},x\in\left(0,\dfrac{\pi}{2}\right)$$ have nice methods? Thank you my try: $$\Longleftrightarrow \cos{x}\cdot x^3<(\sin{x})^3$$ let $$f(x)=\cos{x}\cdot x^3-(\sin{x})^3$$ $$\Longrightarrow f'(x)=-\sin{x}\cdot x^3+3\cos{x}\cdot x^2-3(\sin{x})^2\cos{x}$$",,['inequality']
10,Proof of Extended Law of the Mean (Taylor's Formula),Proof of Extended Law of the Mean (Taylor's Formula),,"I started reading ""The Mathematics of nonlinear Programming - AL peressini"" recently. Theorem : Suppose that $f(x) , f'(x) ,f''(x)$ exist on the closed interval $[a,b]$. If $x^* ,x $ are two different points of $[a,b]$ , then there exists a point $z$ strictly between $x^*$ and $x$ such that $$f(x)=f(x^*)+f'(x^*)(x-x^*)+\frac{f''(z)}{2}(x-x^*)^2$$ I tried to prove this theorem using The Fundamental theorem of calculus. But I can not stop it at the second derivative . Do I have to use mean value theorem ? or is there a simple proof for this ?","I started reading ""The Mathematics of nonlinear Programming - AL peressini"" recently. Theorem : Suppose that $f(x) , f'(x) ,f''(x)$ exist on the closed interval $[a,b]$. If $x^* ,x $ are two different points of $[a,b]$ , then there exists a point $z$ strictly between $x^*$ and $x$ such that $$f(x)=f(x^*)+f'(x^*)(x-x^*)+\frac{f''(z)}{2}(x-x^*)^2$$ I tried to prove this theorem using The Fundamental theorem of calculus. But I can not stop it at the second derivative . Do I have to use mean value theorem ? or is there a simple proof for this ?",,"['calculus', 'taylor-expansion', 'nonlinear-optimization']"
11,Maximize functional with dot product in it,Maximize functional with dot product in it,,"I have to find a function that maximizes some functional. This function can be divided into multiple functionals - as a first step, I wanted to find a function that maximizes only one of these (for which the correct solution can easily be seen even w/o calculus of variaton), to make sure I got the theory of calculus of variation still right. The functional I am interested in has the form $$ F[h] = \int_{S \times S} L(x, y, h(x), h(y)) dx dy= \int_{S \times S} g(x, y) h(x)^Th(y) dx dy, $$ with $h:S \to S$ and $g: X \times Y \to R$ and $g$ is symmetrical in its arguments, i.e. $g(x, y) = g(y, x)$ and $S \subseteq R^n$ Now I'd like to find the $h$ that maximizes said functional, i.e. the function $h$ that solves $$ \frac{\delta F}{\delta h} = 0. $$ For this, I tried two things: I used the Euler-Lagrange equation $\frac{\partial F}{\partial h} - \frac{d}{dx}\frac{\partial L}{\partial h'}$ , and $\frac{\delta F}{\delta h} = g(x, y)(h(x) + h(y))$ , but then I got confused because $h$ is evaluated for two different arguments in $L$ . How should I continue from here? I tried to calculate the derivate directly by applying the definition of the functional derivative $$ \frac{\delta F}{\delta h} = \lim_{\epsilon \to 0} \frac{F[h + \epsilon \eta] - F[h]}{\epsilon}.  $$ For this I got $$ \lim_{\epsilon \to 0} \int_{S \times S} g(x, y)[(h(x) + \epsilon \eta(x))^T(h(y) + \epsilon \eta(y))] \\ = \int_{S \times S} g(x, y)[\eta(x)^Th(y) + \eta(y)^Tg(x)] \\ = \int_{S \times S} g(x, y)\eta(x)^Th(y) + \int_{S \times S} g(y, x)\eta(y)^Th(x) \\ = \int_{S \times S} 2 g(x, y)h(x)^T\eta(x), $$ from which I concluded $\frac{\delta F}{\delta h} = 2 g(x, y)h(x)^T$ as the expression above must hold true for all test functions $\eta$ . Now I am wondering: Does any of the two approaches make sense and give the correct result How to explain the difference in their results. Is the latter just because I haven't used the symmetry property of $g$ yet? Both expressions I ended up with equal $0$ when $h = const = 0$ , which is the global minimum of the functional; how can I find the maximum of it (which should be $h = const = max_S$ ? I assume the problem here is that I haven't yet used that $h$ should also map to $S$ ; is that correct? If so, how can I do this?","I have to find a function that maximizes some functional. This function can be divided into multiple functionals - as a first step, I wanted to find a function that maximizes only one of these (for which the correct solution can easily be seen even w/o calculus of variaton), to make sure I got the theory of calculus of variation still right. The functional I am interested in has the form with and and is symmetrical in its arguments, i.e. and Now I'd like to find the that maximizes said functional, i.e. the function that solves For this, I tried two things: I used the Euler-Lagrange equation , and , but then I got confused because is evaluated for two different arguments in . How should I continue from here? I tried to calculate the derivate directly by applying the definition of the functional derivative For this I got from which I concluded as the expression above must hold true for all test functions . Now I am wondering: Does any of the two approaches make sense and give the correct result How to explain the difference in their results. Is the latter just because I haven't used the symmetry property of yet? Both expressions I ended up with equal when , which is the global minimum of the functional; how can I find the maximum of it (which should be ? I assume the problem here is that I haven't yet used that should also map to ; is that correct? If so, how can I do this?","
F[h] = \int_{S \times S} L(x, y, h(x), h(y)) dx dy= \int_{S \times S} g(x, y) h(x)^Th(y) dx dy,
 h:S \to S g: X \times Y \to R g g(x, y) = g(y, x) S \subseteq R^n h h 
\frac{\delta F}{\delta h} = 0.
 \frac{\partial F}{\partial h} - \frac{d}{dx}\frac{\partial L}{\partial h'} \frac{\delta F}{\delta h} = g(x, y)(h(x) + h(y)) h L 
\frac{\delta F}{\delta h} = \lim_{\epsilon \to 0} \frac{F[h + \epsilon \eta] - F[h]}{\epsilon}. 
 
\lim_{\epsilon \to 0} \int_{S \times S} g(x, y)[(h(x) + \epsilon \eta(x))^T(h(y) + \epsilon \eta(y))] \\
= \int_{S \times S} g(x, y)[\eta(x)^Th(y) + \eta(y)^Tg(x)] \\
= \int_{S \times S} g(x, y)\eta(x)^Th(y) + \int_{S \times S} g(y, x)\eta(y)^Th(x) \\
= \int_{S \times S} 2 g(x, y)h(x)^T\eta(x),
 \frac{\delta F}{\delta h} = 2 g(x, y)h(x)^T \eta g 0 h = const = 0 h = const = max_S h S","['calculus', 'derivatives', 'calculus-of-variations', 'functional-calculus']"
12,Backpropagation calculus,Backpropagation calculus,,"I'm trying to understand the backpropagation calculus, i made that but i'm not very sure if it ok. Someone can confirm me that?","I'm trying to understand the backpropagation calculus, i made that but i'm not very sure if it ok. Someone can confirm me that?",,"['matrices', 'derivatives', 'machine-learning']"
13,Lipschitz function is differentiable at a point iff its tangent set is a k-dimensional plane,Lipschitz function is differentiable at a point iff its tangent set is a k-dimensional plane,,"I'm reading a proof of Hadamard-Perron theorem from Katok's Introduction to the Modern Theory of Dynamical Systems . I'm having problems with the following part. Let $\varphi:\mathbb{R}^k\to\mathbb{R}^{n-k}$ be a Lipschitz function with Lipschitz constant = $\gamma$ . For $x\in\mathbb{R}^k$ define $$     \Delta_y\varphi := \frac{(y,\varphi(y))-(x,\varphi(x))}{||(y,\varphi(y))-(x,\varphi(x))||} \text{ for } x \neq y,     $$ $$ t_x\varphi := \{v \in T_x\mathbb{R}^n : \exists \{x_n\}_{n \in \mathbb{N}} \text{ such that } \lim_{n\to\infty}x_n=x \land \lim_{n\to \infty}\Delta_{x_n}\varphi=v \}. $$ Then by a tangent set to $\varphi$ at $x$ we mean the set $$\tau_x\varphi := \bigcup_{v\in t_x\varphi}\mathbb{R} v$$ Now we have: Lemma. $\varphi$ is differentiable at $x$ $\iff$ $\tau_x\varphi$ is a $k$ - dimensional plane , where by a $k$ - dimensional plane we mean a $k$ - dimensional subspace of $\mathbb{R}^n$ . There's no proof of this in the book. I know how to prove it in one dimensional case, where $\varphi:\mathbb{R}\to\mathbb{R}$ , because it's all about the limit-definition of a derivative. I tried to adopt one dimensional case to multidimensional by proving that if the tangent set was ""something more"" than a $k$ - dimensional plane, then one of the partial derivatives wouldn't exist but with no success. Also, I couldn't find anything helpful here. If I missed something, please redirect me. P.S. I'm not looking for a solution, hints would work better for me :) Many thanks for any help!","I'm reading a proof of Hadamard-Perron theorem from Katok's Introduction to the Modern Theory of Dynamical Systems . I'm having problems with the following part. Let be a Lipschitz function with Lipschitz constant = . For define Then by a tangent set to at we mean the set Now we have: Lemma. is differentiable at is a - dimensional plane , where by a - dimensional plane we mean a - dimensional subspace of . There's no proof of this in the book. I know how to prove it in one dimensional case, where , because it's all about the limit-definition of a derivative. I tried to adopt one dimensional case to multidimensional by proving that if the tangent set was ""something more"" than a - dimensional plane, then one of the partial derivatives wouldn't exist but with no success. Also, I couldn't find anything helpful here. If I missed something, please redirect me. P.S. I'm not looking for a solution, hints would work better for me :) Many thanks for any help!","\varphi:\mathbb{R}^k\to\mathbb{R}^{n-k} \gamma x\in\mathbb{R}^k 
    \Delta_y\varphi := \frac{(y,\varphi(y))-(x,\varphi(x))}{||(y,\varphi(y))-(x,\varphi(x))||} \text{ for } x \neq y,
     
t_x\varphi := \{v \in T_x\mathbb{R}^n : \exists \{x_n\}_{n \in \mathbb{N}} \text{ such that } \lim_{n\to\infty}x_n=x \land \lim_{n\to \infty}\Delta_{x_n}\varphi=v \}.
 \varphi x \tau_x\varphi := \bigcup_{v\in t_x\varphi}\mathbb{R} v \varphi x \iff \tau_x\varphi k k k \mathbb{R}^n \varphi:\mathbb{R}\to\mathbb{R} k","['derivatives', 'lipschitz-functions', 'tangent-spaces']"
14,Computing directional derivative of scalar field $F$ along a straight line,Computing directional derivative of scalar field  along a straight line,F,"In Pavel Grinfeld's Introduction to Tensor Analysis and the Calculus of Moving Surfaces , the directional derivative is defined as below: A directional derivative measures the rate of change of a scalar field $F$ along a straing line. Let a straight ray $\ell$ emanate from the point $P$ . Suppose that $P^*$ is a nearby point on $\ell$ and let $P^*$ approach $P$ in the sense that the distance $PP^*$ approaches zero. Then the directional derivative $\frac{\mathrm dF}{\mathrm d\ell}$ at the point $P$ is defined as the limit $$\frac{\mathrm dF(P)}{\mathrm d\ell}=\lim_{P^*\to P}\frac{F(P^*)-F(P)}{PP^*}$$ Several exercises follow, which I've started working through but I'm unsure whether what I'm doing is correct. Evaluate $\frac{\mathrm dF}{\mathrm d\ell}$ for $F(P)=$ ""Distance from point $P$ to point $A$ "" in a direction perpendicular to $AP$ . I made a sketch of the situation: By definition of $F(P)$ , $F(P^*)$ is the length of the line segment $AP^*$ and $F(P)$ is the length of $AP$ . So I think we have $$\begin{align} \frac{\mathrm dF(P)}{\mathrm d\ell}&=\lim_{P^*\to P}\frac{AP^*-AP}{PP^*}\\[1ex] &=\lim_{P^*\to P}\frac{AP^*-AP}{\sqrt{(AP^*)^2-(AP)^2}}\\[1ex] &=\lim_{P^*\to P}\sqrt{\frac{AP^*-AP}{AP^*+AP}}=0 \end{align}$$ because $AP^*\to AP$ as $P^*\to P$ . The next exercise is slightly more complicated: Evaluate $\frac{\mathrm dF}{\mathrm d\ell}$ for $F(P)=$ "" $1$ /(Distance from point $P$ to point $A$ )"" in the direction from $P$ to $A$ . The sketch is the same as the previous one, but this time it sounds like $A$ lies on $\ell$ somewhere between the point $P$ and the tip of the ray. Let $a$ be the distance $AP^*$ . Then $AP$ has length $ba$ for some $0<b<1$ , and $PP^*$ has length $(1-b)a$ . So I think we have $$\begin{align} \frac{\mathrm dF(P)}{\mathrm d\ell}&=\lim_{P^*\to P}\frac{\frac1{AP^*}-\frac1{AP}}{PP^*}\\[1ex] &=\lim_{b\to1}\frac{\frac1a-\frac1{ba}}{(1-b)a}\\[1ex] &=\lim_{b\to1}-\frac1{ba^2}=-\frac1{(AP)^2} \end{align}$$ Edit: Evaluate $\frac{\mathrm dF}{\mathrm d\ell}$ for $F(P)=$ ""Angle between $OA$ and $OP$ ,"" where $O$ and $A$ are two given points, in the direction from $P$ to $A$ . $$\begin{align} \frac{\mathrm dF(P)}{\mathrm d\ell}&=\lim_{P^*\to P}\frac{\theta^*-\theta}{PP^*}\\[1ex] &=\lim_{\theta^*\to\theta}\frac{\theta^*-\theta}{\sqrt{(OP^*)^2+(OP)^2-2(OP^*)(OP)\cos(\theta^*-\theta)}}\\[1ex] &=\lim_{\theta^*\to\theta}\frac{\theta^*-\theta}{\sqrt{a^2b^2+a^2-2a^2b\cos(\theta^*-\theta)}}=0&(\text{where }a=OP,b>1) \end{align}$$ Are the reasoning and solution correct? The first two solutions I've arrived at seem consistent with what I know about basic calculus, but I'm not sure about the last one.","In Pavel Grinfeld's Introduction to Tensor Analysis and the Calculus of Moving Surfaces , the directional derivative is defined as below: A directional derivative measures the rate of change of a scalar field along a straing line. Let a straight ray emanate from the point . Suppose that is a nearby point on and let approach in the sense that the distance approaches zero. Then the directional derivative at the point is defined as the limit Several exercises follow, which I've started working through but I'm unsure whether what I'm doing is correct. Evaluate for ""Distance from point to point "" in a direction perpendicular to . I made a sketch of the situation: By definition of , is the length of the line segment and is the length of . So I think we have because as . The next exercise is slightly more complicated: Evaluate for "" /(Distance from point to point )"" in the direction from to . The sketch is the same as the previous one, but this time it sounds like lies on somewhere between the point and the tip of the ray. Let be the distance . Then has length for some , and has length . So I think we have Edit: Evaluate for ""Angle between and ,"" where and are two given points, in the direction from to . Are the reasoning and solution correct? The first two solutions I've arrived at seem consistent with what I know about basic calculus, but I'm not sure about the last one.","F \ell P P^* \ell P^* P PP^* \frac{\mathrm dF}{\mathrm d\ell} P \frac{\mathrm dF(P)}{\mathrm d\ell}=\lim_{P^*\to P}\frac{F(P^*)-F(P)}{PP^*} \frac{\mathrm dF}{\mathrm d\ell} F(P)= P A AP F(P) F(P^*) AP^* F(P) AP \begin{align}
\frac{\mathrm dF(P)}{\mathrm d\ell}&=\lim_{P^*\to P}\frac{AP^*-AP}{PP^*}\\[1ex]
&=\lim_{P^*\to P}\frac{AP^*-AP}{\sqrt{(AP^*)^2-(AP)^2}}\\[1ex]
&=\lim_{P^*\to P}\sqrt{\frac{AP^*-AP}{AP^*+AP}}=0
\end{align} AP^*\to AP P^*\to P \frac{\mathrm dF}{\mathrm d\ell} F(P)= 1 P A P A A \ell P a AP^* AP ba 0<b<1 PP^* (1-b)a \begin{align}
\frac{\mathrm dF(P)}{\mathrm d\ell}&=\lim_{P^*\to P}\frac{\frac1{AP^*}-\frac1{AP}}{PP^*}\\[1ex]
&=\lim_{b\to1}\frac{\frac1a-\frac1{ba}}{(1-b)a}\\[1ex]
&=\lim_{b\to1}-\frac1{ba^2}=-\frac1{(AP)^2}
\end{align} \frac{\mathrm dF}{\mathrm d\ell} F(P)= OA OP O A P A \begin{align}
\frac{\mathrm dF(P)}{\mathrm d\ell}&=\lim_{P^*\to P}\frac{\theta^*-\theta}{PP^*}\\[1ex]
&=\lim_{\theta^*\to\theta}\frac{\theta^*-\theta}{\sqrt{(OP^*)^2+(OP)^2-2(OP^*)(OP)\cos(\theta^*-\theta)}}\\[1ex]
&=\lim_{\theta^*\to\theta}\frac{\theta^*-\theta}{\sqrt{a^2b^2+a^2-2a^2b\cos(\theta^*-\theta)}}=0&(\text{where }a=OP,b>1)
\end{align}","['calculus', 'derivatives', 'solution-verification', 'tensors']"
15,"prove that $g_\theta(t)$ is increasing on $[1,\infty)$.",prove that  is increasing on .,"g_\theta(t) [1,\infty)","Here's an aggravating problem.  Fix some constant $\theta\in(0,1)$ .  I have a function $$g_\theta(t)=t^\theta\left[(t+1)^{1-\theta}-\left(t+\frac{1}{2}\right)^{1-\theta}\right]$$ which, when plotted in wolfram for various values of $\theta$ , is clearly increasing on $[1,\infty)$ .  But, I need to prove this rigorously. The obvious thing to do is to try to show that $g'_\theta(t)>0$ on $[1,\infty)$ .  Unfortunately, we have this mess for the derivative: $$g'_\theta(t) =(\theta t^{\theta-1}+t^\theta)(t+1)^{-\theta} -(\theta t^{\theta-1}/2+t^\theta)\left(t+\frac{1}{2}\right)^{-\theta}$$ Maybe there's some special function I can use to make this easier.  Or some convexity trick I'm not seeing.  Idk.  What do you guys think? Thanks!","Here's an aggravating problem.  Fix some constant .  I have a function which, when plotted in wolfram for various values of , is clearly increasing on .  But, I need to prove this rigorously. The obvious thing to do is to try to show that on .  Unfortunately, we have this mess for the derivative: Maybe there's some special function I can use to make this easier.  Or some convexity trick I'm not seeing.  Idk.  What do you guys think? Thanks!","\theta\in(0,1) g_\theta(t)=t^\theta\left[(t+1)^{1-\theta}-\left(t+\frac{1}{2}\right)^{1-\theta}\right] \theta [1,\infty) g'_\theta(t)>0 [1,\infty) g'_\theta(t)
=(\theta t^{\theta-1}+t^\theta)(t+1)^{-\theta}
-(\theta t^{\theta-1}/2+t^\theta)\left(t+\frac{1}{2}\right)^{-\theta}","['real-analysis', 'derivatives', 'monotone-functions']"
16,Solving a differential equation using Integrating Factor method,Solving a differential equation using Integrating Factor method,,"I need to to model a raindrop's velocity as it is falling with respect to time. The assumptions made are that air resistance is negligible and that the raindrop is spherical I was able to to calculate and solve the differential equation for the change in radius over time to be: $$ r(t) = \frac{k}{p}t + r(0) $$ where $k$ is a proportionality constant, $p$ is the density of the raindrop and $r(0)$ is the initial radius. Where I am stuck is where the differential equation for velocity of the raindrop is given but I need to solve the equation with the integrating factor method. Given that $r(0) = 3$ $$ \frac{dv}{dt} + \frac{3r'}{r}v = g $$ where $r = r(t)$ as above and $g$ = gravitational force constant. The solution for this DE should be: $$ v(t) = \frac{pg}{4k} r + \frac{C}{r^3} $$ where $C$ is the integrating constant. I am unsure of how I would calculate the integrating factor when there is a $r$ and $r'$ in the integral as well as how I should use this integrating factor to solve the differential equation. I was given a hint that the chain rule would help with saving some work","I need to to model a raindrop's velocity as it is falling with respect to time. The assumptions made are that air resistance is negligible and that the raindrop is spherical I was able to to calculate and solve the differential equation for the change in radius over time to be: where is a proportionality constant, is the density of the raindrop and is the initial radius. Where I am stuck is where the differential equation for velocity of the raindrop is given but I need to solve the equation with the integrating factor method. Given that where as above and = gravitational force constant. The solution for this DE should be: where is the integrating constant. I am unsure of how I would calculate the integrating factor when there is a and in the integral as well as how I should use this integrating factor to solve the differential equation. I was given a hint that the chain rule would help with saving some work","
r(t) = \frac{k}{p}t + r(0)
 k p r(0) r(0) = 3 
\frac{dv}{dt} + \frac{3r'}{r}v = g
 r = r(t) g 
v(t) = \frac{pg}{4k} r + \frac{C}{r^3}
 C r r'","['integration', 'ordinary-differential-equations', 'derivatives', 'mathematical-modeling', 'integrating-factor']"
17,Is there a differentiable smooth max and min function that respects the distributive law?,Is there a differentiable smooth max and min function that respects the distributive law?,,"I'm looking for functions $S_{max}, S_{min}$ that is similar (converges to?) to a smooth max, but respects the distributive law and is differentiable (or is polynomial). Is there such a known function? So it should at least satisfy the following: $S_{max}(x, y)'$ exists assuming that $x', y'$ do. $min(max(x, y), z) = max(min(x, z), min(y, z))$ and vice versa (distributive)","I'm looking for functions that is similar (converges to?) to a smooth max, but respects the distributive law and is differentiable (or is polynomial). Is there such a known function? So it should at least satisfy the following: exists assuming that do. and vice versa (distributive)","S_{max}, S_{min} S_{max}(x, y)' x', y' min(max(x, y), z) = max(min(x, z), min(y, z))","['derivatives', 'optimization', 'maxima-minima', 'smooth-functions']"
18,Find a parametric equation of a plane curve,Find a parametric equation of a plane curve,,"The task is to find a parametric equation of a curve, if you are given its curvature in arclength parametrisation. I know that I need to integrate but I don’t get the idea of angle theta(what’s it?) and plus there are constants of integration. Would be very grateful if someone explained how to solve it. For example: $$k(s)=\frac1{as\sqrt2}, \text{where }a=\text{constant}$$","The task is to find a parametric equation of a curve, if you are given its curvature in arclength parametrisation. I know that I need to integrate but I don’t get the idea of angle theta(what’s it?) and plus there are constants of integration. Would be very grateful if someone explained how to solve it. For example:","k(s)=\frac1{as\sqrt2}, \text{where }a=\text{constant}","['derivatives', 'differential-geometry', 'curvature', 'differential', 'plane-curves']"
19,Does any inconsistency arise from these two interpretations of $\frac{df}{dx}$?,Does any inconsistency arise from these two interpretations of ?,\frac{df}{dx},"Using limits, $\dfrac{df(z)}{dx}$ is defined as what the slope of a line joining $(x,f(x))$ and $(x+h,f(x+h))$ approaches as $h \to 0$ which can be rephrased as precisely the slope of the tangent to $f$ at $(x,f(x))$ as the secant would approach the tangent as $h \to 0$ . So : $$\dfrac{d}{dx}f(x) = \lim_{h \to 0} \dfrac{f(x+h)-f(x)}{h}$$ Now, we also say that for a very small $\Delta x$ : $$\dfrac{\Delta y}{\Delta x} = \dfrac{df}{dx} \implies \Delta y = \dfrac{df}{dx}\Delta x$$ Which is actually an approximation but the error is so ""negligibly negligible"" that it can simply be ignored. So, the two ways of interpreting $\dfrac{df(x)}{dx}$ are : Precisely the slope of the tangent to $f$ at $(x,f(x))$ [ $0$ error] The ratio of change in $f(x)$ with an extremely small change in $x$ . The second interpretation is commonly used in Physics and also helps to understand partial derivatives better (for beginners like me, at least). I want to know if these two interpretations go hand in hand or at some point, they give contradicting results and cause inconsistencies. Also, which interpretation should be followed to derive rules like the sum rule, product rule, power rule, quotient rule etc? Thanks!","Using limits, is defined as what the slope of a line joining and approaches as which can be rephrased as precisely the slope of the tangent to at as the secant would approach the tangent as . So : Now, we also say that for a very small : Which is actually an approximation but the error is so ""negligibly negligible"" that it can simply be ignored. So, the two ways of interpreting are : Precisely the slope of the tangent to at [ error] The ratio of change in with an extremely small change in . The second interpretation is commonly used in Physics and also helps to understand partial derivatives better (for beginners like me, at least). I want to know if these two interpretations go hand in hand or at some point, they give contradicting results and cause inconsistencies. Also, which interpretation should be followed to derive rules like the sum rule, product rule, power rule, quotient rule etc? Thanks!","\dfrac{df(z)}{dx} (x,f(x)) (x+h,f(x+h)) h \to 0 f (x,f(x)) h \to 0 \dfrac{d}{dx}f(x) = \lim_{h \to 0} \dfrac{f(x+h)-f(x)}{h} \Delta x \dfrac{\Delta y}{\Delta x} = \dfrac{df}{dx} \implies \Delta y = \dfrac{df}{dx}\Delta x \dfrac{df(x)}{dx} f (x,f(x)) 0 f(x) x","['calculus', 'derivatives', 'soft-question']"
20,Derivative with summation operator in the exponent of $e$,Derivative with summation operator in the exponent of,e,"What is the derivative when the summation operator is in the exponent of $e$ , such as: $$\frac{d}{d\mathbf{x}}\left(\frac{1}{1+e^{-\mathbf{\hat{x}}}}\right)=\frac{d}{d\mathbf{x}}\left(\frac{1}{1+e^{-\sum_{i} a_ix_i+b_i}}\right),$$ where $\mathbf{a}=[a_1,a_2,\cdots,a_n]$ and $\mathbf{b}=[b_1,b_2,\cdots,b_n]$ are constant vectors. I tried the following using chain rule: $\frac{d}{d\mathbf{x}}\left(\frac{1}{1+e^{-\mathbf{\hat{x}}}}\right)=\left(\frac{1}{1+e^{-\mathbf{\hat{x}}}}\right)\times\left(1-\frac{1}{1+e^{-\mathbf{\hat{x}}}}\right)\times\mathbf{a}^T$","What is the derivative when the summation operator is in the exponent of , such as: where and are constant vectors. I tried the following using chain rule:","e \frac{d}{d\mathbf{x}}\left(\frac{1}{1+e^{-\mathbf{\hat{x}}}}\right)=\frac{d}{d\mathbf{x}}\left(\frac{1}{1+e^{-\sum_{i} a_ix_i+b_i}}\right), \mathbf{a}=[a_1,a_2,\cdots,a_n] \mathbf{b}=[b_1,b_2,\cdots,b_n] \frac{d}{d\mathbf{x}}\left(\frac{1}{1+e^{-\mathbf{\hat{x}}}}\right)=\left(\frac{1}{1+e^{-\mathbf{\hat{x}}}}\right)\times\left(1-\frac{1}{1+e^{-\mathbf{\hat{x}}}}\right)\times\mathbf{a}^T","['derivatives', 'vector-spaces', 'summation']"
21,"$\phi_{w}$ is differentiable in $a,$ if $ r (h) $ depends on $ h$",is differentiable in  if  depends on,"\phi_{w} a,  r (h)   h","Definition: Let $V\subseteq{\mathbb{R}^{m}}$ an open set, $a\in V$ y $f\colon V\to\mathbb{R}^{n}$ a function. We will say that f is differentiable in $a,$ if exists a linear transformation $f′(a)\colon\mathbb{R}^{m}\to\mathbb{R}^{n}$ such that \begin{equation} f(a+h)=f(a)+f'(a)(h)+r(h),\qquad\lim_{h\rightarrow 0}{\dfrac{r(h)}{\lVert h\rVert}}=0. \end{equation} Let $V\subseteq{\mathbb{R}^{m}}$ and $a\in V$ be. They are equivalent (1). Then the function $A\colon V\to\mathcal{L}(\mathbb{R}^{m},\mathbb{R}^{p})$ is differentiable in $a$ (2). For all $w\in\mathbb{R}^{m}$ the function $\phi_{w}\colon V\to\mathbb{R}^{p}$ given by $\phi_{w}(x)=A(x)\cdot w$ is differentiable in $a.$ In this case, $\phi_{w}'(a)\cdot v=(A'(a)\cdot v)\cdot w.$ My attempt: Suppose $ (1). $ Let $w\in\mathbb{R}^{m}.$ Let's see what $\phi_{w}\colon V\to\mathbb{R}^{p}$ given by $\phi_{w}(x)=A(x)\cdot w$ is differentiable in $a.$ By $(1),$ exists a linear transformation $A′(a)\colon\mathbb{R}^{m}\to\mathcal{L}(\mathbb{R}^{m},\mathbb{R}^{p})$ such that \begin{equation} A(a+h)=A(a)+A'(a)(h)+r(h),\qquad\lim_{h\rightarrow 0}{\dfrac{r(h)}{\lVert h\rVert}}=0. \end{equation} From this it follows that \begin{equation*}     A(x+h)\cdot w=A(a)\cdot w+A'(a)(h)\cdot w+r(h)\cdot w, \qquad\lim_{h\rightarrow{0}}{\dfrac{r(h)}{\lVert h\rVert}}=0, \end{equation*} that is to say, \begin{equation*}     \phi_{w}(x+h)=\phi_{w}(a)+(A'(a)\cdot h)\cdot w+r(h)\cdot w,\qquad\lim_{h\rightarrow{0}}{\dfrac{r(h)}{\lVert h\rVert}}=0 \end{equation*} and $\phi_{w}$ is f is differentiable in $a.$ I did not want to continue because several doubts arose with that test that I wrote. In this case, is the function $ r (h) $ a linear transformation? Also, can't I conclude that $ \phi_{w} $ is differentiable in $ a $ because that $ r(h) $ depends on $ h $ ? Any suggestion will be welcome. Also, I would like to know if you have seen that exercise in a book.","Definition: Let an open set, y a function. We will say that f is differentiable in if exists a linear transformation such that Let and be. They are equivalent (1). Then the function is differentiable in (2). For all the function given by is differentiable in In this case, My attempt: Suppose Let Let's see what given by is differentiable in By exists a linear transformation such that From this it follows that that is to say, and is f is differentiable in I did not want to continue because several doubts arose with that test that I wrote. In this case, is the function a linear transformation? Also, can't I conclude that is differentiable in because that depends on ? Any suggestion will be welcome. Also, I would like to know if you have seen that exercise in a book.","V\subseteq{\mathbb{R}^{m}} a\in V f\colon V\to\mathbb{R}^{n} a, f′(a)\colon\mathbb{R}^{m}\to\mathbb{R}^{n} \begin{equation}
f(a+h)=f(a)+f'(a)(h)+r(h),\qquad\lim_{h\rightarrow 0}{\dfrac{r(h)}{\lVert h\rVert}}=0.
\end{equation} V\subseteq{\mathbb{R}^{m}} a\in V A\colon V\to\mathcal{L}(\mathbb{R}^{m},\mathbb{R}^{p}) a w\in\mathbb{R}^{m} \phi_{w}\colon V\to\mathbb{R}^{p} \phi_{w}(x)=A(x)\cdot w a. \phi_{w}'(a)\cdot v=(A'(a)\cdot v)\cdot w.  (1).  w\in\mathbb{R}^{m}. \phi_{w}\colon V\to\mathbb{R}^{p} \phi_{w}(x)=A(x)\cdot w a. (1), A′(a)\colon\mathbb{R}^{m}\to\mathcal{L}(\mathbb{R}^{m},\mathbb{R}^{p}) \begin{equation}
A(a+h)=A(a)+A'(a)(h)+r(h),\qquad\lim_{h\rightarrow 0}{\dfrac{r(h)}{\lVert h\rVert}}=0.
\end{equation} \begin{equation*}
    A(x+h)\cdot w=A(a)\cdot w+A'(a)(h)\cdot w+r(h)\cdot w, \qquad\lim_{h\rightarrow{0}}{\dfrac{r(h)}{\lVert h\rVert}}=0,
\end{equation*} \begin{equation*}
    \phi_{w}(x+h)=\phi_{w}(a)+(A'(a)\cdot h)\cdot w+r(h)\cdot w,\qquad\lim_{h\rightarrow{0}}{\dfrac{r(h)}{\lVert h\rVert}}=0
\end{equation*} \phi_{w} a.  r (h)   \phi_{w}   a   r(h)   h ","['real-analysis', 'multivariable-calculus', 'derivatives', 'partial-derivative']"
22,Prove that there exists $c$ such that $\int_{1}^{\infty} \frac{(1-x+[x])(x^a-x^{1-a})}{x^2}dx=(c^a-c^{1-a})\int_{1}^{\infty} \frac{(1-x+[x])}{x^2}dx$,Prove that there exists  such that,c \int_{1}^{\infty} \frac{(1-x+[x])(x^a-x^{1-a})}{x^2}dx=(c^a-c^{1-a})\int_{1}^{\infty} \frac{(1-x+[x])}{x^2}dx,"$I = \int_{1}^{\infty}$$\frac{(1-x+[x])(x^a-x^{1-a})}{x^2} dx$ where $[x]$ denotes Greatest integer function and $a\in \mathbb{C}$ is a constant. Prove that there exists some $c \in (1,\infty)$ such that $$\int_{1}^{\infty}\frac{(1-x+[x])(x^a-x^{1-a})}{x^2}dx =  (c^a-c^{1-a})\int_{1}^{\infty}\frac{(1-x+[x])}{x^2}dx.$$ My try: $$0<1-x+[x]\leq 1 \Rightarrow 0< \frac{1-x+[x]}{x^2} \leq \frac{1}{x^2}$$ $$0<\int_{1}^{\infty}\frac{1-x+[x]}{x^2}dx \leq 1.$$ Consider, $$\frac{\int_{1}^{\infty}\frac{(1-x+[x])(x^a-x^{1-a})}{x^2}dx }{\int_{1}^{\infty}\frac{(1-x+[x])}{x^2}dx} = \lambda \Rightarrow \int_{1}^{\infty}\frac{(1-x+[x])[(x^a-x^{1-a})-\lambda]}{x^2}dx=0$$","where denotes Greatest integer function and is a constant. Prove that there exists some such that My try: Consider,","I = \int_{1}^{\infty}\frac{(1-x+[x])(x^a-x^{1-a})}{x^2} dx [x] a\in \mathbb{C} c \in (1,\infty) \int_{1}^{\infty}\frac{(1-x+[x])(x^a-x^{1-a})}{x^2}dx =  (c^a-c^{1-a})\int_{1}^{\infty}\frac{(1-x+[x])}{x^2}dx. 0<1-x+[x]\leq 1 \Rightarrow 0< \frac{1-x+[x]}{x^2} \leq \frac{1}{x^2} 0<\int_{1}^{\infty}\frac{1-x+[x]}{x^2}dx \leq 1. \frac{\int_{1}^{\infty}\frac{(1-x+[x])(x^a-x^{1-a})}{x^2}dx }{\int_{1}^{\infty}\frac{(1-x+[x])}{x^2}dx} = \lambda \Rightarrow \int_{1}^{\infty}\frac{(1-x+[x])[(x^a-x^{1-a})-\lambda]}{x^2}dx=0","['real-analysis', 'calculus']"
23,Uniqueness criterion for tetration based on signs of derivatives?,Uniqueness criterion for tetration based on signs of derivatives?,,"Consider the fractional iterations of the expontential function denoted $$ \exp^{[t]}(x) $$ $$ \exp^{[0]}(x) = x $$ $$ \exp^{[t]}(x) = \exp(\exp^{[t-1]}(x) $$ $$ \exp^{[l + m]} = \exp^{[l]}(\exp^{[m]}(x))$$ Where $t,l,m,x$ are real and $t > 0$ . My friend Tommy proposed once a condition for an infinitely differentiable solution. $t,x$ are real and $t > 0$ : $$ \frac{d^n}{d^n x} \exp^{[t]}(x) > 0 $$ It seems impossible to satisfy for analytic solutions because for $ 0 < t < 1$ we have singularities in the complex plane that force the radius being finite and the signs of the derivatives of the taylor series alternating ( because the singularities have taylor series with alternating signs and near the singularities we must have it too). For instance if you are close to a log singularity your taylor series must eventually copy the sign changes of the logarithm. However if the bundle of functions $exp^{[t]}$ are nowhere analytic on the real line but infinitely differentiable, this might be true ?? CONJECTURE 1 (existance conjecture) There exist a bundle of infinitely differentiable functions solution satisfying $$ \frac{d^n}{d^n x} \exp^{[t]}(x) > 0 $$ CONJECTURE 2 (uniqueness conjecture) The solution to conjecture 1 is unique (if it exists!) ; there is only one (non-analytic) solution for tetration satisfying the condition $$ \frac{d^n}{d^n x} \exp^{[t]}(x) > 0 $$ Can we prove conjecture 1 or 2 ? Is this a very general thing or exclusive for iterations of exponential functions ? I mean, do many entire functions $g(x)$ with all derivatives positive ( at any $x$ ) , no real fixpoint ( and thus above $id(x)$ btw )  (like exp) satisfy $$ \frac{d^n}{d^n x} g^{[t]}(x) > 0 $$ ?? If conjecture 1 or 2 fail for function $g(x)$ does it work for some real $v$ and function $g(x) + v$ ?? In particular the function $\exp(x) + v$ ?","Consider the fractional iterations of the expontential function denoted Where are real and . My friend Tommy proposed once a condition for an infinitely differentiable solution. are real and : It seems impossible to satisfy for analytic solutions because for we have singularities in the complex plane that force the radius being finite and the signs of the derivatives of the taylor series alternating ( because the singularities have taylor series with alternating signs and near the singularities we must have it too). For instance if you are close to a log singularity your taylor series must eventually copy the sign changes of the logarithm. However if the bundle of functions are nowhere analytic on the real line but infinitely differentiable, this might be true ?? CONJECTURE 1 (existance conjecture) There exist a bundle of infinitely differentiable functions solution satisfying CONJECTURE 2 (uniqueness conjecture) The solution to conjecture 1 is unique (if it exists!) ; there is only one (non-analytic) solution for tetration satisfying the condition Can we prove conjecture 1 or 2 ? Is this a very general thing or exclusive for iterations of exponential functions ? I mean, do many entire functions with all derivatives positive ( at any ) , no real fixpoint ( and thus above btw )  (like exp) satisfy ?? If conjecture 1 or 2 fail for function does it work for some real and function ?? In particular the function ?"," \exp^{[t]}(x)   \exp^{[0]}(x) = x   \exp^{[t]}(x) = \exp(\exp^{[t-1]}(x)   \exp^{[l + m]} = \exp^{[l]}(\exp^{[m]}(x)) t,l,m,x t > 0 t,x t > 0  \frac{d^n}{d^n x} \exp^{[t]}(x) > 0   0 < t < 1 exp^{[t]}  \frac{d^n}{d^n x} \exp^{[t]}(x) > 0   \frac{d^n}{d^n x} \exp^{[t]}(x) > 0  g(x) x id(x)  \frac{d^n}{d^n x} g^{[t]}(x) > 0  g(x) v g(x) + v \exp(x) + v","['real-analysis', 'calculus', 'derivatives', 'tetration', 'smooth-functions']"
24,Euler-Mascheroni constant as local minima of a function [closed],Euler-Mascheroni constant as local minima of a function [closed],,Closed . This question needs details or clarity . It is not currently accepting answers. Want to improve this question? Add details and clarify the problem by editing this post . Closed 3 years ago . Improve this question Let define the function $x>0$ : $$f(x)=\Gamma\Big(\frac{1}{x}\Big)-x$$ Then $x_0$ such that : $$f(x_0)=-\gamma$$ Define a local minima ( see here ) of the function $f(x)$ .(We have the Gamma function and the negative Euler-Mascheroni constant) To prove it I have tried derivative we have $x>0$ : $$f'(x)=-\frac{\Gamma\Big(\frac{1}{x}\Big)\psi\Big(\frac{1}{x}\Big)}{x^2}-1$$ But we want : $$f'(x)=0$$ Or : $$\Gamma\Big(\frac{1}{x}\Big)\psi\Big(\frac{1}{x}\Big)=x^2$$ And now the best things I can do is to use Newton's method to check my result . Question Is there an hidden closed form here ? How to solve it ? Thanks in advance .,Closed . This question needs details or clarity . It is not currently accepting answers. Want to improve this question? Add details and clarify the problem by editing this post . Closed 3 years ago . Improve this question Let define the function : Then such that : Define a local minima ( see here ) of the function .(We have the Gamma function and the negative Euler-Mascheroni constant) To prove it I have tried derivative we have : But we want : Or : And now the best things I can do is to use Newton's method to check my result . Question Is there an hidden closed form here ? How to solve it ? Thanks in advance .,x>0 f(x)=\Gamma\Big(\frac{1}{x}\Big)-x x_0 f(x_0)=-\gamma f(x) x>0 f'(x)=-\frac{\Gamma\Big(\frac{1}{x}\Big)\psi\Big(\frac{1}{x}\Big)}{x^2}-1 f'(x)=0 \Gamma\Big(\frac{1}{x}\Big)\psi\Big(\frac{1}{x}\Big)=x^2,"['derivatives', 'numerical-methods', 'gamma-function', 'constants', 'digamma-function']"
25,"How to show a given function satisfies the Cauchy Riemann equation at a point, but is not differentiable at that point.","How to show a given function satisfies the Cauchy Riemann equation at a point, but is not differentiable at that point.",,"This is my question: Prove that the function $f(z) = u+iv$ , where $$ f(z) = \begin{cases} \frac{x^3(1+i) - y^3(1-i)}{x^2+y^2}, &\text{ if }z\neq 0,\\ 0 &\text{if }z=0.\end{cases}$$ satisfies the Cauchy-Riemann equation at the origin, but the derivative of $f$ at $z=0$ does not exists. I solved the second part about the derivative but was not able to prove the first part, that is, $u_x=v_y$ at $z=0$ . Please kindly direct me how to solve it: I got \begin{align*} u_x&=\frac{x^4 +3x^2x^2 + 2xy^3}{(x^2 + y^2)^2}, \\  u_y&=\frac{ -y^4 -2x^3y -3x^2y^2}{(x^2 + y^2)^2}, \\ v_x&= \frac{x^4 - 2xy^3 + 3x^2y^2}{(x^2 + y^2)^2}, \\ v_y&=\frac{y^4+2x^2y^2-2x^3y}{(x^2 + y^2)^2}. \end{align*} and there remains the question of how to prove $u_x=v_y$ at $(x,y)=(0,0)$ .","This is my question: Prove that the function , where satisfies the Cauchy-Riemann equation at the origin, but the derivative of at does not exists. I solved the second part about the derivative but was not able to prove the first part, that is, at . Please kindly direct me how to solve it: I got and there remains the question of how to prove at .","f(z) = u+iv  f(z) = \begin{cases} \frac{x^3(1+i) - y^3(1-i)}{x^2+y^2}, &\text{ if }z\neq 0,\\ 0 &\text{if }z=0.\end{cases} f z=0 u_x=v_y z=0 \begin{align*}
u_x&=\frac{x^4 +3x^2x^2 + 2xy^3}{(x^2 + y^2)^2}, \\ 
u_y&=\frac{ -y^4 -2x^3y -3x^2y^2}{(x^2 + y^2)^2}, \\
v_x&= \frac{x^4 - 2xy^3 + 3x^2y^2}{(x^2 + y^2)^2}, \\
v_y&=\frac{y^4+2x^2y^2-2x^3y}{(x^2 + y^2)^2}.
\end{align*} u_x=v_y (x,y)=(0,0)","['complex-analysis', 'derivatives', 'cauchy-riemann-equations']"
26,"If $T_t$ is a diffeomorphism, show $\frac\partial{\partial t}{\rm D}T_t(x)={\rm D}\left(\frac\partial{\partial t}T_t\right)(x)$","If  is a diffeomorphism, show",T_t \frac\partial{\partial t}{\rm D}T_t(x)={\rm D}\left(\frac\partial{\partial t}T_t\right)(x),"Let $d\in\mathbb N$ , $U\subseteq\mathbb R^d$ be open, $\tau>0$ and $T_t$ be a $C^1$ -diffeomorphism from $U$ onto $U$ for $t\in[0,\tau)$ with $T_0=\operatorname{id}_U$ . Which assumption do we need to impose in order to conclude that $$[0,\tau)\ni t\mapsto{\rm D}T_t(x)\tag1$$ is $C^1$ -differentiable at $0$ and $$\left.\frac\partial{\partial t}{\rm D}T_t(x)\right|_{t=0}={\rm D}\left(\left.\frac\partial{\partial t}T_t\right|_{t=0}\right)(x)\tag2$$ for all $x\in U$ ? It clearly holds if $$[0,\tau)\times U\ni(t,x)\mapsto T_t(x)\tag3$$ is $C^2$ -differentiable at $(0,x)$ for all $x\in U$ by Schwarz' theorem .","Let , be open, and be a -diffeomorphism from onto for with . Which assumption do we need to impose in order to conclude that is -differentiable at and for all ? It clearly holds if is -differentiable at for all by Schwarz' theorem .","d\in\mathbb N U\subseteq\mathbb R^d \tau>0 T_t C^1 U U t\in[0,\tau) T_0=\operatorname{id}_U [0,\tau)\ni t\mapsto{\rm D}T_t(x)\tag1 C^1 0 \left.\frac\partial{\partial t}{\rm D}T_t(x)\right|_{t=0}={\rm D}\left(\left.\frac\partial{\partial t}T_t\right|_{t=0}\right)(x)\tag2 x\in U [0,\tau)\times U\ni(t,x)\mapsto T_t(x)\tag3 C^2 (0,x) x\in U","['real-analysis', 'derivatives', 'frechet-derivative']"
27,Hard problem :Prove that $f(x)<\frac{3}{2}x-\frac{1}{2}$,Hard problem :Prove that,f(x)<\frac{3}{2}x-\frac{1}{2},Define the function $1<x<\frac{103}{100}$ : $$f(x)=x^{2 \Bigg(1-0.5 \Bigg(\frac{\Gamma\Big(\frac{1}{x}\Big)}{\Gamma\Big(\frac{1}{2}\Big)}\Bigg)\Bigg)}+(1-x)^{2 \Bigg(1-0.5 \Bigg(\frac{\Gamma\Big(\frac{1}{1-x}\Big)}{\Gamma\Big(\frac{1}{2}\Big)}\Bigg)\Bigg)}$$ Prove that : $$f(x)<\frac{3}{2}x-\frac{1}{2}$$ I strongly believe that it is a problem of tangent . So the natural way is to use derivative but first there is a limit that I cannot evaluate $$\lim_{x\to 1^{+}}(1-x)^{2 \Bigg(1-0.5 \Bigg(\frac{\Gamma\Big(\frac{1}{1-x}\Big)}{\Gamma\Big(\frac{1}{2}\Big)}\Bigg)\Bigg)}=0$$ I have tried power series at the first order of the Gamma function ( $x=1$ ) we have : $$\Gamma\Big(\frac{1}{1-x}\Big)=1-\gamma\Big(\frac{1}{1-x}-1\Big)+O\Big(\Big(\frac{1}{1-x}-1\Big)^2\Big)$$ But it's ineffective unfortunately . Why this limit ? Because it implies that if: $$g(x)=(1-x)^{2 \Bigg(1-0.5 \Bigg(\frac{\Gamma\Big(\frac{1}{1-x}\Big)}{\Gamma\Big(\frac{1}{2}\Big)}\Bigg)\Bigg)}$$ Then : $$\lim_{x\to 1^{+}}g'(x)=0$$ Then we have just to evaluate the derivative of : $$h(x)=x^{2 \Bigg(1-0.5 \Bigg(\frac{\Gamma\Big(\frac{1}{x}\Big)}{\Gamma\Big(\frac{1}{2}\Big)}\Bigg)\Bigg)}$$ As $x\to 1^{+}$ An conclude with the formula : $$y=f'(x_0)(x-x_0)+f(x_0)$$ My question : How to prove the strict inequality ? Thanks in advance for your help !,Define the function : Prove that : I strongly believe that it is a problem of tangent . So the natural way is to use derivative but first there is a limit that I cannot evaluate I have tried power series at the first order of the Gamma function ( ) we have : But it's ineffective unfortunately . Why this limit ? Because it implies that if: Then : Then we have just to evaluate the derivative of : As An conclude with the formula : My question : How to prove the strict inequality ? Thanks in advance for your help !,1<x<\frac{103}{100} f(x)=x^{2 \Bigg(1-0.5 \Bigg(\frac{\Gamma\Big(\frac{1}{x}\Big)}{\Gamma\Big(\frac{1}{2}\Big)}\Bigg)\Bigg)}+(1-x)^{2 \Bigg(1-0.5 \Bigg(\frac{\Gamma\Big(\frac{1}{1-x}\Big)}{\Gamma\Big(\frac{1}{2}\Big)}\Bigg)\Bigg)} f(x)<\frac{3}{2}x-\frac{1}{2} \lim_{x\to 1^{+}}(1-x)^{2 \Bigg(1-0.5 \Bigg(\frac{\Gamma\Big(\frac{1}{1-x}\Big)}{\Gamma\Big(\frac{1}{2}\Big)}\Bigg)\Bigg)}=0 x=1 \Gamma\Big(\frac{1}{1-x}\Big)=1-\gamma\Big(\frac{1}{1-x}-1\Big)+O\Big(\Big(\frac{1}{1-x}-1\Big)^2\Big) g(x)=(1-x)^{2 \Bigg(1-0.5 \Bigg(\frac{\Gamma\Big(\frac{1}{1-x}\Big)}{\Gamma\Big(\frac{1}{2}\Big)}\Bigg)\Bigg)} \lim_{x\to 1^{+}}g'(x)=0 h(x)=x^{2 \Bigg(1-0.5 \Bigg(\frac{\Gamma\Big(\frac{1}{x}\Big)}{\Gamma\Big(\frac{1}{2}\Big)}\Bigg)\Bigg)} x\to 1^{+} y=f'(x_0)(x-x_0)+f(x_0),"['limits', 'derivatives', 'inequality', 'power-series', 'gamma-function']"
28,"If $f(x) + f'(x) + f''(x) \to A$ as $x \to \infty$, then show that $f(x) \to A$ as $x \to \infty$","If  as , then show that  as",f(x) + f'(x) + f''(x) \to A x \to \infty f(x) \to A x \to \infty,"This problem is an extension to the simpler problem which deals with $f(x) + f'(x) \to A$ as $x \to \infty$ (see problem 2 on my blog ). If $f$ is twice continuously differentiable in some interval $(a, \infty)$ and $f(x) + f'(x) + f''(x) \to A$ as $x \to \infty$ then show that $f(x) \to A$ as $x \to \infty$. However, the approach based on considering sign of $f'(x)$ for large $x$ (which applies to the simpler problem in the blog) does not seem to apply here. Any hints on this problem? I believe that a similar generalization concerning expression $\sum\limits_{k = 0}^{n}f^{(k)}(x) \to A$ is also true, but I don't have a clue to prove the general result.","This problem is an extension to the simpler problem which deals with $f(x) + f'(x) \to A$ as $x \to \infty$ (see problem 2 on my blog ). If $f$ is twice continuously differentiable in some interval $(a, \infty)$ and $f(x) + f'(x) + f''(x) \to A$ as $x \to \infty$ then show that $f(x) \to A$ as $x \to \infty$. However, the approach based on considering sign of $f'(x)$ for large $x$ (which applies to the simpler problem in the blog) does not seem to apply here. Any hints on this problem? I believe that a similar generalization concerning expression $\sum\limits_{k = 0}^{n}f^{(k)}(x) \to A$ is also true, but I don't have a clue to prove the general result.",,"['calculus', 'real-analysis', 'ordinary-differential-equations', 'limits', 'convergence-divergence']"
29,Integral for infinitely small interval,Integral for infinitely small interval,,"I was thinking about integration by parts on small interval, $[a, a + dx]$ when $dx \to 0.$ More precisely, suppose $f(x) = f, g(x) = g$ are both differentiable. I wanted to prove directly that $$f(a + dx)g(a + dx) - f(a)g(a) \tag{*} $$ is equal to $\int_{a}^{a + dx} g \, df + \int_{a}^{a + dx} f \, dg $ (this is formula for IBP). Indeed, we can simply write $f(a + dx) - f(a) = df$ and similarly $g(a + dx) - g(a) = dg.$ Now we have that (*) is $(f(a) + df)(g(a) + dg) - f(a)g(a) = f(a)dg + g(a)df + dfdg.$ We can intuitively exclude $dfdg$ from calculation (as product rule tells us). And now, the crushing part : if $\int_{a}^{a + dx}g\,df$ is equal to $gdf$ (and of course same for other integral) we are done. Let me explain: we can treate $df$ as some fixed value, so we want to prove $$df \cdot \int_{a}^{a + dx}g = g(a)df.$$ This looks very weird. Interval is getting smaller and smaller but we can always find input $x := \frac{2a + dx}{2}$ (midpoint of $[a, a + dx]$ ) and corresponding output $g(\frac{2a + dx}{2})$ (and midpoints of that two subintervals, etc.) Therefore, sum of $g$ -values is always (much) greater than $g(a)$ because there is $\infty$ many $g$ values for inputs between $a, a + dx.$ My intuition tells me that this is because somehow $dx < c, \forall c \in \mathbb{R},$ but that doesn't seem right. In short: how is possible (if is) that for real function defined on interval $[a, a + dx]$ is $\int_{a}^{a + dx}f(x) = f(a)$ ?","I was thinking about integration by parts on small interval, when More precisely, suppose are both differentiable. I wanted to prove directly that is equal to (this is formula for IBP). Indeed, we can simply write and similarly Now we have that (*) is We can intuitively exclude from calculation (as product rule tells us). And now, the crushing part : if is equal to (and of course same for other integral) we are done. Let me explain: we can treate as some fixed value, so we want to prove This looks very weird. Interval is getting smaller and smaller but we can always find input (midpoint of ) and corresponding output (and midpoints of that two subintervals, etc.) Therefore, sum of -values is always (much) greater than because there is many values for inputs between My intuition tells me that this is because somehow but that doesn't seem right. In short: how is possible (if is) that for real function defined on interval is ?","[a, a + dx] dx \to 0. f(x) = f, g(x) = g f(a + dx)g(a + dx) - f(a)g(a) \tag{*}  \int_{a}^{a + dx} g \, df + \int_{a}^{a + dx} f \, dg  f(a + dx) - f(a) = df g(a + dx) - g(a) = dg. (f(a) + df)(g(a) + dg) - f(a)g(a) = f(a)dg + g(a)df + dfdg. dfdg \int_{a}^{a + dx}g\,df gdf df df \cdot \int_{a}^{a + dx}g = g(a)df. x := \frac{2a + dx}{2} [a, a + dx] g(\frac{2a + dx}{2}) g g(a) \infty g a, a + dx. dx < c, \forall c \in \mathbb{R}, [a, a + dx] \int_{a}^{a + dx}f(x) = f(a)","['real-analysis', 'limits', 'derivatives', 'definite-integrals', 'real-numbers']"
30,Multiplying an equation by a derivative,Multiplying an equation by a derivative,,"Relevant page print screen Hi All, I'm following a derivation for the Quantum Harmonic Oscillator from the textbook 'Quantum Physics' by Gasiorowicz and have attached a print screen of the page in question for clarity. There is a step in the derivation that I would appreciate if someone could clarify for me. Starting from $$\frac{ d^{ 2} u_{0}(y)}{ dy^{2}} - y^{2} u_{0}(y) = 0$$ The author then multiplies this equation by $$2\frac{du_0}{dy}$$ (no idea why he scales it by 2?) which allows the equation to be rearranged to $$\frac{d}{dy}\left(\frac{du_0}{dy}\right)^2-y^2\frac{d}{dy}(u_0)^2 = 0$$ I understand how he arrives at the first term but not the second. I would have thought the correct way to multiply $y^2u_0(y)$ by $\frac{du_0}{dy}$ would be as follows: $$\frac{du_0}{dy}y^2u_0(y)=\frac{d}{dy}(u_0^2y^2)=u_0^2\frac{d}{dy}(y^2)+(y^2)\frac{d}{dy}(u_0^2)$$ How does the author obtain the $$y^2\frac{d}{dy}(u_0)^2$$ term in the second equation? How can he choose not to operate on $y^2$ with the differential operator when performing the multiplication? What are the rules for multiplying equations by derivatives? Thanks","Relevant page print screen Hi All, I'm following a derivation for the Quantum Harmonic Oscillator from the textbook 'Quantum Physics' by Gasiorowicz and have attached a print screen of the page in question for clarity. There is a step in the derivation that I would appreciate if someone could clarify for me. Starting from The author then multiplies this equation by (no idea why he scales it by 2?) which allows the equation to be rearranged to I understand how he arrives at the first term but not the second. I would have thought the correct way to multiply by would be as follows: How does the author obtain the term in the second equation? How can he choose not to operate on with the differential operator when performing the multiplication? What are the rules for multiplying equations by derivatives? Thanks",\frac{ d^{ 2} u_{0}(y)}{ dy^{2}} - y^{2} u_{0}(y) = 0 2\frac{du_0}{dy} \frac{d}{dy}\left(\frac{du_0}{dy}\right)^2-y^2\frac{d}{dy}(u_0)^2 = 0 y^2u_0(y) \frac{du_0}{dy} \frac{du_0}{dy}y^2u_0(y)=\frac{d}{dy}(u_0^2y^2)=u_0^2\frac{d}{dy}(y^2)+(y^2)\frac{d}{dy}(u_0^2) y^2\frac{d}{dy}(u_0)^2 y^2,"['derivatives', 'quantum-mechanics']"
31,Why is Kantorovich optimal transport dual not smooth?,Why is Kantorovich optimal transport dual not smooth?,,"Let $X$ and $Y$ be two separable metric spaces such that any probability measure on $X$ (or $Y$ ) is a Radon measure (i.e. they are Radon spaces). Let $c: X \times Y \rightarrow[0, \infty]$ be a Borel-measurable function. The Kantorovich problem is $$\inf \left\{\int_{X \times Y} c(x, y) \mathrm{d} \gamma(x, y) \mid \gamma \in \Gamma(\mu, \nu)\right\}$$ where $\Gamma(\mu, v)$ denotes the collection of all probability measures on $X \times Y$ with marginals $\mu$ on $X$ and $v$ on Y. It can be shown that a minimizer for this problem always exists when the cost function $c$ is lower semi-continuous and $\Gamma(\mu, v)$ is a tight collection of measures (which is guaranteed for Radon spaces $X$ . and $Y$ ). Under mild conditions, the minimum of the Kantorovich problem is equal to $$ \sup \left(\int_{X} \varphi(x) \mathrm{d} \mu(x)+\int_{Y} \psi(y) \mathrm{d} \nu(y)\right) $$ where the supremum runs over all pairs of bounded and continuous functions $\varphi: X \rightarrow \mathbf{R}$ and $\psi: Y \rightarrow \mathbf{R}$ such that $$ \varphi(x)+\psi(y) \leq c(x, y) $$ I read everywhere that this dual problem is not smooth in $(\varphi, \psi)$ . What does it mean and why is it the case ? By not smooth, do we mean that the Fréchet derivatives do not exist ?","Let and be two separable metric spaces such that any probability measure on (or ) is a Radon measure (i.e. they are Radon spaces). Let be a Borel-measurable function. The Kantorovich problem is where denotes the collection of all probability measures on with marginals on and on Y. It can be shown that a minimizer for this problem always exists when the cost function is lower semi-continuous and is a tight collection of measures (which is guaranteed for Radon spaces . and ). Under mild conditions, the minimum of the Kantorovich problem is equal to where the supremum runs over all pairs of bounded and continuous functions and such that I read everywhere that this dual problem is not smooth in . What does it mean and why is it the case ? By not smooth, do we mean that the Fréchet derivatives do not exist ?","X Y X Y c: X \times Y \rightarrow[0, \infty] \inf \left\{\int_{X \times Y} c(x, y) \mathrm{d} \gamma(x, y) \mid \gamma \in \Gamma(\mu, \nu)\right\} \Gamma(\mu, v) X \times Y \mu X v c \Gamma(\mu, v) X Y 
\sup \left(\int_{X} \varphi(x) \mathrm{d} \mu(x)+\int_{Y} \psi(y) \mathrm{d} \nu(y)\right)
 \varphi: X \rightarrow \mathbf{R} \psi: Y \rightarrow \mathbf{R} 
\varphi(x)+\psi(y) \leq c(x, y)
 (\varphi, \psi)","['real-analysis', 'probability', 'functional-analysis', 'derivatives', 'optimization']"
32,Differentiating powers of e where the power is an exponential function like $e^{a^x}$ when $a$ is a constant,Differentiating powers of e where the power is an exponential function like  when  is a constant,e^{a^x} a,"Using the fact that $e = \lim_{h \to \infty} (1+h)^{1/h}$ , you can answer $\frac{d}{dx} e^{p(x)}$ where $p(x)$ is a polynomial using the variable x. However, I have troubles finding the derivative of $e^{p(x)}$ where “something” is a exponential function. Is there a way/formula (or multiple formulae) finding the derivative of such functions? I’ll be glad if you include the proof of such ways.","Using the fact that , you can answer where is a polynomial using the variable x. However, I have troubles finding the derivative of where “something” is a exponential function. Is there a way/formula (or multiple formulae) finding the derivative of such functions? I’ll be glad if you include the proof of such ways.",e = \lim_{h \to \infty} (1+h)^{1/h} \frac{d}{dx} e^{p(x)} p(x) e^{p(x)},['calculus']
33,"Defining a general structure of ""Calculus"" [closed]","Defining a general structure of ""Calculus"" [closed]",,"Closed . This question is opinion-based . It is not currently accepting answers. Want to improve this question? Update the question so it can be answered with facts and citations by editing this post . Closed 3 years ago . Improve this question I've been thinking lately, is there a way to generalize the fundamental concepts of Calculus such as convergence, differentiability and integrability to it's ""maximum potential""? That is, defining a really long $n$ - uple in the algebraic sense (like in group theory, $(G,\,\cdot\,)$ ) that would include all the other ""cases of Calculus"" (in the real line, in $\mathbb{R}^n$ , in Banach Spaces, in a manifold, etc). I know we could generalize the notion of differentiability to normed spaces and manifolds, and the notion of integrability to any measurable space, including, again, manifolds, so I think that the most likely structure to harbor the strongest definition of what ""Calculus"" comes to be are the manifolds. That put, I've came across the notion of Difeological Space, in which the manifolds (even the infinite-dimensional ones) are replaced by stronger counterparts; similarly, I've encountered some sources claiming that the language of Category Theory could make such a powerful generalization of Calculus, creating it's own ""algebraic structure"", in some sort of manner. What do you guys think? Such a thing would be possible? Thanks in advance!","Closed . This question is opinion-based . It is not currently accepting answers. Want to improve this question? Update the question so it can be answered with facts and citations by editing this post . Closed 3 years ago . Improve this question I've been thinking lately, is there a way to generalize the fundamental concepts of Calculus such as convergence, differentiability and integrability to it's ""maximum potential""? That is, defining a really long - uple in the algebraic sense (like in group theory, ) that would include all the other ""cases of Calculus"" (in the real line, in , in Banach Spaces, in a manifold, etc). I know we could generalize the notion of differentiability to normed spaces and manifolds, and the notion of integrability to any measurable space, including, again, manifolds, so I think that the most likely structure to harbor the strongest definition of what ""Calculus"" comes to be are the manifolds. That put, I've came across the notion of Difeological Space, in which the manifolds (even the infinite-dimensional ones) are replaced by stronger counterparts; similarly, I've encountered some sources claiming that the language of Category Theory could make such a powerful generalization of Calculus, creating it's own ""algebraic structure"", in some sort of manner. What do you guys think? Such a thing would be possible? Thanks in advance!","n (G,\,\cdot\,) \mathbb{R}^n","['integration', 'derivatives', 'differential-geometry', 'manifolds', 'global-analysis']"
34,Extension of a differentiable function $f$ to an open superset,Extension of a differentiable function  to an open superset,f,"This is a question the book Munkres-Calculus on Manifolds pg.144(Exercise 3-b) If $f :S\to \mathbb R$ and $f$ is differentiable of class $C^r$ at each point $x_0$ of $S$ ,then $f$ may be extended to a $C^r$ function $h: A\to \mathbb R$ that is defined on an open set $A$ of $\mathbb R^n$ containing $S$ . My attempt, with $f:S \to \mathbb R$ is $C^r$ , for each $x \in S$ ,then for each $x \in S$ , I can choose $U_x$ open neighborhood of $x$ such that $\cup U_x = A$ open (arbitrary union of abiertos es abierto).The item before ""a""(pg.144.Exercise 3),it is show that if $f$ is $C^r$ then  always exists $g:U_x \to \mathbb R $ where $x \in U_x \subset \mathbb R^n$ such that $f$ is equal to $g$ when $U_x \cap S$ and $$h(x) =\left \{ \begin{matrix} \phi(x)g(x)& \mbox{if }x\mbox{ $\in U_x$ } \\ 0 & \mbox{if }x\mbox{ $\notin \operatorname{supp} \phi$}\end{matrix}\right.  $$ Is $C^r$ with $\phi:\mathbb R^n  \to \mathbb R $ is $C^r$ , where your support is in $U_x$ .Choosing $h:A \to \mathbb R $ with the extension of $f$ .","This is a question the book Munkres-Calculus on Manifolds pg.144(Exercise 3-b) If and is differentiable of class at each point of ,then may be extended to a function that is defined on an open set of containing . My attempt, with is , for each ,then for each , I can choose open neighborhood of such that open (arbitrary union of abiertos es abierto).The item before ""a""(pg.144.Exercise 3),it is show that if is then  always exists where such that is equal to when and Is with is , where your support is in .Choosing with the extension of .","f :S\to \mathbb R f C^r x_0 S f C^r h: A\to \mathbb R A \mathbb R^n S f:S \to \mathbb R C^r x \in S x \in S U_x x \cup U_x = A f C^r g:U_x \to \mathbb R  x \in U_x \subset \mathbb R^n f g U_x \cap S h(x) =\left \{ \begin{matrix} \phi(x)g(x)& \mbox{if }x\mbox{ \in U_x }
\\ 0 & \mbox{if }x\mbox{ \notin \operatorname{supp} \phi}\end{matrix}\right. 
 C^r \phi:\mathbb R^n  \to \mathbb R  C^r U_x h:A \to \mathbb R  f","['calculus', 'analysis', 'functions', 'derivatives']"
35,"Finding derivative of $f(z) =\sqrt r[\cos\theta/2 +i \sin\theta/2]$, where $r>0$ and $0<\theta<2π$.","Finding derivative of , where  and .",f(z) =\sqrt r[\cos\theta/2 +i \sin\theta/2] r>0 0<\theta<2π,"Show that $f(z) =\sqrt r [\cos\theta/2 +i \sin\theta/2]$ , where $r>0$ , $0<\theta<2π$ is differentiable. Find $f'(z)$ . I have tried to solve it by cauchy-rieman(CR equation) but got stuck in midway. Please help me to solve it. Thanks a lot","Show that , where , is differentiable. Find . I have tried to solve it by cauchy-rieman(CR equation) but got stuck in midway. Please help me to solve it. Thanks a lot",f(z) =\sqrt r [\cos\theta/2 +i \sin\theta/2] r>0 0<\theta<2π f'(z),"['complex-analysis', 'multivariable-calculus', 'derivatives', 'complex-numbers', 'complex-integration']"
36,Coordinate Transformation and Derivatives,Coordinate Transformation and Derivatives,,"I'm currently working on an assignment where I end up with the following differential equation in cylindrical coordinates: $$ 0 = \frac{\partial}{\partial r}\left(r \frac{\partial w_\varphi}{\partial r}\right), $$ where $r$ is the radius and $w_\varphi$ describes the angular velocity of a stirred liquid. If I assume $r$ to be a ""regular"" variable and I apply the chain rule I get the following equation: $$ 0 = \frac{\partial^2 w}{\partial r^2}r + \frac{\partial w}{\partial r} \tag{1} $$ The last task of the assignment is to compare the results of this equation with the results obtained from simplifying the Navier-Stokes equation (basically the holy grail of fluid mechanics) and to explain possible differences. The equation I obtain from the Navier-Stokes equation looks like this: $$ 0 = \frac{\partial^2w}{\partial r^2} r + \frac{\partial w}{\partial r} - \frac{w}{r} \tag{2} $$ so I get an extra $-\frac{w}{r}$ out of it. I'm fairly certain that both equations are correct, or at least the intended outcome of their task, because my professor said that everything was looking good up until now. The question is of course where does this extra $-\frac{w}{r}$ come from? My professor gave me the hint of looking at the $r$ variable more closely and to try and think of how  it would change in carthesian coordinates. Or to look at it in a vector form, so using $\vec{r} = \begin{pmatrix} r \> cos(\varphi)\\r \> sin(\varphi)\end{pmatrix}$ instead of just $r$ to account for the moving unit vectors in cylindrical coordinates. To be honest my courses in calculus happened quite a while ago and I forgot a good portion of them so I'm a bit confused by this problem and don't really know where to begin. If anyone has an idea on how to solve or approach this problem I would be very grateful. EDIT: Upon further research I found a chapter out of an ODE Textbook in which it seems the same or at least a similar problem is posed as an exercise. Unfortunately I couldn't find any solutions for the exercises of the book, but maybe it can shed some light into what my question is. https://faculty.missouri.edu/~chiconec/pdf/water1.pdf The exercise in question is 6.7","I'm currently working on an assignment where I end up with the following differential equation in cylindrical coordinates: where is the radius and describes the angular velocity of a stirred liquid. If I assume to be a ""regular"" variable and I apply the chain rule I get the following equation: The last task of the assignment is to compare the results of this equation with the results obtained from simplifying the Navier-Stokes equation (basically the holy grail of fluid mechanics) and to explain possible differences. The equation I obtain from the Navier-Stokes equation looks like this: so I get an extra out of it. I'm fairly certain that both equations are correct, or at least the intended outcome of their task, because my professor said that everything was looking good up until now. The question is of course where does this extra come from? My professor gave me the hint of looking at the variable more closely and to try and think of how  it would change in carthesian coordinates. Or to look at it in a vector form, so using instead of just to account for the moving unit vectors in cylindrical coordinates. To be honest my courses in calculus happened quite a while ago and I forgot a good portion of them so I'm a bit confused by this problem and don't really know where to begin. If anyone has an idea on how to solve or approach this problem I would be very grateful. EDIT: Upon further research I found a chapter out of an ODE Textbook in which it seems the same or at least a similar problem is posed as an exercise. Unfortunately I couldn't find any solutions for the exercises of the book, but maybe it can shed some light into what my question is. https://faculty.missouri.edu/~chiconec/pdf/water1.pdf The exercise in question is 6.7"," 0 = \frac{\partial}{\partial r}\left(r \frac{\partial w_\varphi}{\partial r}\right),  r w_\varphi r  0 = \frac{\partial^2 w}{\partial r^2}r + \frac{\partial w}{\partial r} \tag{1}   0 = \frac{\partial^2w}{\partial r^2} r + \frac{\partial w}{\partial r} - \frac{w}{r} \tag{2}  -\frac{w}{r} -\frac{w}{r} r \vec{r} = \begin{pmatrix} r \> cos(\varphi)\\r \> sin(\varphi)\end{pmatrix} r","['derivatives', 'fluid-dynamics', 'cylindrical-coordinates']"
37,Find the extremums of complicated function,Find the extremums of complicated function,,"I have a function: $$f(x,y) = x^2y^3(6-x-y)$$ I found partial derivatives: $$f_{x}(x,y)^{'} = xy^3(12-3x-2y)=0$$ $$f_{y}(x,y)^{'} = x^2y^2(18-3x-4y)=0$$ And extremums: $$(0,0);(0,3);(2,0);(2,3)$$ To find maximum and minimum I take second derivative: $$f_{xx}(x,y)^{''} = 24xy^3-6xy^3-2xy^3=0$$ $$f_{xy}(x,y)^{''} = 36x^2y^2-9x^2y^2-3x^2y^2=0$$ $$f_{yy}(x,y)^{''} = 36x^2y-6x^3y^2-6x^2y=0$$ And you see in cases with coordinates with zeros its not clear how to find out whether they are maximums or minimus. How to do that?",I have a function: I found partial derivatives: And extremums: To find maximum and minimum I take second derivative: And you see in cases with coordinates with zeros its not clear how to find out whether they are maximums or minimus. How to do that?,"f(x,y) = x^2y^3(6-x-y) f_{x}(x,y)^{'} = xy^3(12-3x-2y)=0 f_{y}(x,y)^{'} = x^2y^2(18-3x-4y)=0 (0,0);(0,3);(2,0);(2,3) f_{xx}(x,y)^{''} = 24xy^3-6xy^3-2xy^3=0 f_{xy}(x,y)^{''} = 36x^2y^2-9x^2y^2-3x^2y^2=0 f_{yy}(x,y)^{''} = 36x^2y-6x^3y^2-6x^2y=0","['multivariable-calculus', 'derivatives']"
38,"If $f$ is $C^1$ and $\Vert[Df(x)]^{-1}\Vert \leq c$ for all $x$, does the inverse $f^{-1}$ exists?","If  is  and  for all , does the inverse  exists?",f C^1 \Vert[Df(x)]^{-1}\Vert \leq c x f^{-1},"Let $V$ be a normed space, $U \subset V$ an open nonempty set and $f: U \rightarrow V$ be such that $f$ is $C^1$ , $Df(x)$ is invertable and $\Vert[Df(x)]^{-1}\Vert \leq c$ for all $x \in U$ . Does $f^{-1}$ exists? I tried to use the inverse function theorem or use the derivative to show something like $\Vert x-y\Vert \leq c \Vert f(x) - f(y) \Vert$ , but it only works locally. It seems to me that exists a $f$ with this properties with no inverse but trying to find it has been equally difficult.","Let be a normed space, an open nonempty set and be such that is , is invertable and for all . Does exists? I tried to use the inverse function theorem or use the derivative to show something like , but it only works locally. It seems to me that exists a with this properties with no inverse but trying to find it has been equally difficult.",V U \subset V f: U \rightarrow V f C^1 Df(x) \Vert[Df(x)]^{-1}\Vert \leq c x \in U f^{-1} \Vert x-y\Vert \leq c \Vert f(x) - f(y) \Vert f,"['real-analysis', 'derivatives']"
39,Counterexample for 2-22 of Spivak's Calculus on Manifolds?,Counterexample for 2-22 of Spivak's Calculus on Manifolds?,,"If $f: \mathbb{R}^2 \rightarrow \mathbb{R}$ and $D_2f = 0$ , show that $f$ is independent of the second variable. I was thinking of ways to show this, when I came across what I think might be a counterexample. Possible counterexample: Consider the function $f: \mathbb{R}^2 \rightarrow \mathbb{R}$ defined by $$f(x,y) = \begin{cases}  x & \text{ if $y \geq 0$} \\ x^2 & \text{ if $y < 0$.} \\ \end{cases}$$ Then $D_2f = 0$ , but $f(x,1) = x, f(x,-1) = x^2 \Rightarrow f(x,1) \neq f(x,-1)$ , showing that $f$ is not independent of the second variable. Am I missing something here? It seems like the above theorem should work, i.e. that $f$ is independent of the second variable, but the counterexample seems convincing enough that I'm afraid I might have overlooked something. An idea that just came to be is that $\lim_{y\rightarrow 0^-} \frac{f(x_0,y)-f(x_0,0)}{y-0}=\infty$ , which does not equal to $\lim_{y\rightarrow 0^+} \frac{f(x_0,y)-f(x_0,0)}{y-0} = 0$ . Does it sound right?","If and , show that is independent of the second variable. I was thinking of ways to show this, when I came across what I think might be a counterexample. Possible counterexample: Consider the function defined by Then , but , showing that is not independent of the second variable. Am I missing something here? It seems like the above theorem should work, i.e. that is independent of the second variable, but the counterexample seems convincing enough that I'm afraid I might have overlooked something. An idea that just came to be is that , which does not equal to . Does it sound right?","f: \mathbb{R}^2 \rightarrow \mathbb{R} D_2f = 0 f f: \mathbb{R}^2 \rightarrow \mathbb{R} f(x,y) =
\begin{cases} 
x & \text{ if y \geq 0} \\
x^2 & \text{ if y < 0.} \\
\end{cases} D_2f = 0 f(x,1) = x, f(x,-1) = x^2 \Rightarrow f(x,1) \neq f(x,-1) f f \lim_{y\rightarrow 0^-} \frac{f(x_0,y)-f(x_0,0)}{y-0}=\infty \lim_{y\rightarrow 0^+} \frac{f(x_0,y)-f(x_0,0)}{y-0} = 0","['calculus', 'multivariable-calculus', 'derivatives']"
40,Does the derivative of the Fourier series $\sum_{k\geq2}\frac{1}{k^2\log k}\cos kt$ exist?,Does the derivative of the Fourier series  exist?,\sum_{k\geq2}\frac{1}{k^2\log k}\cos kt,"Does the derivative of the Fourier series $\sum_{k\geq2}\frac{1}{k^2\log k}\cos kt$ exist? I wanted to use the following sufficient condition for this problem Theorem. Let $a_k(t)$ be differentiable. If a series of functions $\sum_{k}a_k(t)$ is such that $\sum_ka_k'(t)$ converges uniformly in $[a,b]$ , and $\sum_{k}a_k(t)$ converges at ont point in $(a,b)$ , then $\sum_ka(t)$ converges in $[a,b]$ and its derivative is $\sum_ka_k'(t)$ . But this theorem does not seem to be applicable to the series above, because I can't prove that $-\sum_k\frac{1}{k\log k}\sin kt$ is uniformly convergent. However, a book that I am reading says that it does have a derivative. How can I prove it?","Does the derivative of the Fourier series exist? I wanted to use the following sufficient condition for this problem Theorem. Let be differentiable. If a series of functions is such that converges uniformly in , and converges at ont point in , then converges in and its derivative is . But this theorem does not seem to be applicable to the series above, because I can't prove that is uniformly convergent. However, a book that I am reading says that it does have a derivative. How can I prove it?","\sum_{k\geq2}\frac{1}{k^2\log k}\cos kt a_k(t) \sum_{k}a_k(t) \sum_ka_k'(t) [a,b] \sum_{k}a_k(t) (a,b) \sum_ka(t) [a,b] \sum_ka_k'(t) -\sum_k\frac{1}{k\log k}\sin kt","['sequences-and-series', 'derivatives', 'fourier-series']"
41,Solve the following constrained maximization problem [closed],Solve the following constrained maximization problem [closed],,"Closed. This question does not meet Mathematics Stack Exchange guidelines . It is not currently accepting answers. Please provide additional context , which ideally explains why the question is relevant to you and our  community. Some forms of context include: background and motivation, relevant definitions, source, possible strategies, your current progress, why the question is interesting or important, etc. Closed 4 years ago . Improve this question Question: let $T\geq$ 1 be some finite integer, solve the following maximization problem. Maximize $\sum_{t=1}^T$ ( $\frac{1}{2}$ ) $^t$$\sqrt{x_t}$ subject to $\sum_{t=1}^{T}$$x_t\leq1$ , $x_t\geq0$ , t=1,...,T I have never had to maximize summations before and I do not know how to do so. Can someone show me a step by step break down of the solution?","Closed. This question does not meet Mathematics Stack Exchange guidelines . It is not currently accepting answers. Please provide additional context , which ideally explains why the question is relevant to you and our  community. Some forms of context include: background and motivation, relevant definitions, source, possible strategies, your current progress, why the question is interesting or important, etc. Closed 4 years ago . Improve this question Question: let 1 be some finite integer, solve the following maximization problem. Maximize ( ) subject to , , t=1,...,T I have never had to maximize summations before and I do not know how to do so. Can someone show me a step by step break down of the solution?",T\geq \sum_{t=1}^T \frac{1}{2} ^t\sqrt{x_t} \sum_{t=1}^{T}x_t\leq1 x_t\geq0,"['derivatives', 'optimization']"
42,Integrate probability of derivative with respect to time to find probability with respect to time,Integrate probability of derivative with respect to time to find probability with respect to time,,"I'm currently working on modelling systems with both continuous and discrete random variables that operate in continuous time, and I've run into a problem that I don't know how to solve or search for on a search engine.  I've also not encountered anything quite like it in any of my math classes in all my years in college, so I don't know what domain this sort of problem belongs to or what the proper terminology or notation is. I have a random variable $X$ that varies with time $t$ .  I have a probability density function of the form $P \left[ \frac{d X}{d t} = \frac{d x}{d t} | X = x \right] (t)$ .  This is meant to represent the probability that the derivative of $X$ is $\frac{d x}{d t}$ at time $t$ , given that the current value of $X$ is $x$ .  Let's assume that I can integrate and differentiate this function. I ultimately want to know how to obtain $P \left[ X = x \right] (t)$ (or the probability that $X$ has value $x$ at time $t$ ).  I have the initial conditions $P \left[ X = x \right] (0)$ .  I would like to take the approach of finding $\frac{d P \left [ X = x \right] (t)}{d t}$ and then integrating, if possible. Is it possible to obtain $\frac{d P \left[ X = x \right] (t)}{d t}$ from $P \left[ \frac{d X}{d t} = \frac{d x}{d t} | X = x \right] (t)$ ?  I suspect that it is not, but I'd love to be wrong. If so, how? If not, how else might I obtain $P \left[ X = x \right] (t)$ from $P \left[ \frac{d X}{d t} = \frac{d x}{d t} | X = x \right] (t)$ ? I'd like answers that generalize to dealing with higher order derivatives and systems of equations that relate multiple random variables.  In addition, some context on what domain this sort of problem belongs to would be appreciated. EDIT: I clarified a bit what I'm asking for.  I realized that I had some conditional probability in my problem, which might make things easier or harder. I can construct a recursive expression that seems like it would be a step toward obtaining $P \left[ X = x \right] (t)$ directly, but I am then confronted with an integral that I don't see how to solve. $$ P \left[ X = x \right] (t) = \int_{-\infty}^{x} P \left[ X = p \right] (t - d t) \cdot P \left[ \frac{d X}{d t} = \frac{x - p}{d t} | X = p \right] (t - d t) \cdot d p $$","I'm currently working on modelling systems with both continuous and discrete random variables that operate in continuous time, and I've run into a problem that I don't know how to solve or search for on a search engine.  I've also not encountered anything quite like it in any of my math classes in all my years in college, so I don't know what domain this sort of problem belongs to or what the proper terminology or notation is. I have a random variable that varies with time .  I have a probability density function of the form .  This is meant to represent the probability that the derivative of is at time , given that the current value of is .  Let's assume that I can integrate and differentiate this function. I ultimately want to know how to obtain (or the probability that has value at time ).  I have the initial conditions .  I would like to take the approach of finding and then integrating, if possible. Is it possible to obtain from ?  I suspect that it is not, but I'd love to be wrong. If so, how? If not, how else might I obtain from ? I'd like answers that generalize to dealing with higher order derivatives and systems of equations that relate multiple random variables.  In addition, some context on what domain this sort of problem belongs to would be appreciated. EDIT: I clarified a bit what I'm asking for.  I realized that I had some conditional probability in my problem, which might make things easier or harder. I can construct a recursive expression that seems like it would be a step toward obtaining directly, but I am then confronted with an integral that I don't see how to solve.","X t P \left[ \frac{d X}{d t} = \frac{d x}{d t} | X = x \right] (t) X \frac{d x}{d t} t X x P \left[ X = x \right] (t) X x t P \left[ X = x \right] (0) \frac{d P \left [ X = x \right] (t)}{d t} \frac{d P \left[ X = x \right] (t)}{d t} P \left[ \frac{d X}{d t} = \frac{d x}{d t} | X = x \right] (t) P \left[ X = x \right] (t) P \left[ \frac{d X}{d t} = \frac{d x}{d t} | X = x \right] (t) P \left[ X = x \right] (t) 
P \left[ X = x \right] (t) = \int_{-\infty}^{x} P \left[ X = p \right] (t - d t) \cdot P \left[ \frac{d X}{d t} = \frac{x - p}{d t} | X = p \right] (t - d t) \cdot d p
","['probability', 'derivatives', 'random-variables']"
43,"Substitution with $a = x$, $b=\frac{y-x^2}{x} $ in differential equation gives me $ u = a \frac{\partial u}{\partial a} $ - Why?","Substitution with ,  in differential equation gives me  - Why?",a = x b=\frac{y-x^2}{x}   u = a \frac{\partial u}{\partial a} ,If $$ \cases{a = x\\ b=\frac{y-x^2}{x}} $$ and $$ x\frac{\partial u}{\partial x}+\left(y+x^2\right)\frac{\partial u}{\partial y}=u $$ Why do we get $$ u = a \frac{\partial u}{\partial a} $$,If and Why do we get,"
\cases{a = x\\
b=\frac{y-x^2}{x}}
 
x\frac{\partial u}{\partial x}+\left(y+x^2\right)\frac{\partial u}{\partial y}=u
 
u = a \frac{\partial u}{\partial a}
","['derivatives', 'substitution']"
44,A conjectured upper bound for $\left(\frac{x^n+1}{x^{n-1}+1}\right)^n+\left(\frac{x+1}{2}\right)^n$ and $x\geq 1$,A conjectured upper bound for  and,\left(\frac{x^n+1}{x^{n-1}+1}\right)^n+\left(\frac{x+1}{2}\right)^n x\geq 1,Hi I have (related https://mathoverflow.net/questions/337457/prove-that-left-fracxn1xn-11-rightn-left-fracx12-rightn ): Let $x\geq 1$ a real number and $n\geq 2$ a natural number then we have : $$\Big(\frac{x^{n+1}+1}{x+1}\Big)\Bigg(\frac{\Big(\frac{x+1}{2}\Big)^n}{\Big(\frac{x^2+1}{x+1}\Big)^n}+\frac{(x^n+1)^2}{(x^{n-1}+1)(x^{n+1}+1)}\Bigg)\geq \left(\frac{x^n+1}{x^{n-1}+1}\right)^n+\left(\frac{x+1}{2}\right)^n$$ It's a conjecture so I have tested for $n\leq 20$ . Maybe it works also for $n$ a positive real number . If it's true I think we can make something similar to the Peter Mueller's answer. For the case $n=3$ here we have the derivative . Clearly the numerator (of the derivative) is positive for $x\geq 1$ .But in some way it's not so easy with the general case . So how to solve it ? Any helps is greatly appreciated. Thanks in advance for all your contribution .,Hi I have (related https://mathoverflow.net/questions/337457/prove-that-left-fracxn1xn-11-rightn-left-fracx12-rightn ): Let a real number and a natural number then we have : It's a conjecture so I have tested for . Maybe it works also for a positive real number . If it's true I think we can make something similar to the Peter Mueller's answer. For the case here we have the derivative . Clearly the numerator (of the derivative) is positive for .But in some way it's not so easy with the general case . So how to solve it ? Any helps is greatly appreciated. Thanks in advance for all your contribution .,x\geq 1 n\geq 2 \Big(\frac{x^{n+1}+1}{x+1}\Big)\Bigg(\frac{\Big(\frac{x+1}{2}\Big)^n}{\Big(\frac{x^2+1}{x+1}\Big)^n}+\frac{(x^n+1)^2}{(x^{n-1}+1)(x^{n+1}+1)}\Bigg)\geq \left(\frac{x^n+1}{x^{n-1}+1}\right)^n+\left(\frac{x+1}{2}\right)^n n\leq 20 n n=3 x\geq 1,"['real-analysis', 'derivatives', 'inequality', 'examples-counterexamples', 'conjectures']"
45,Integration by parts. Push It to the Limit,Integration by parts. Push It to the Limit,,"Given a function $f$ , a primitive $F$ is given by $$ F(x)=\int\,f(x)\,\mathrm{d}x. $$ Using repeated integration by parts it seems that one can rewrite $F$ as $$ F(x)=xf(x)-\frac{1}{2!}x^2f'(x)+\frac{1}{3!}x^3f''(x)-\frac{1}{4!}x^4f'''(x)+\cdots+(-1)^{k+1}\frac{x^{k+1}}{(k+1)!}f^{(k)}(x)+\cdots $$ or as \begin{align*} F(x)&=\Bigg[\sum_{k\geq0}(-1)^{k+1}\frac{x^{k+1}}{(k+1)!}\partial_x^k\Bigg]f(x)\\ &=\frac{e^{-x\partial_x}-1}{\partial_x}f(x). \end{align*} Does this have some utility, application or use somewhere?","Given a function , a primitive is given by Using repeated integration by parts it seems that one can rewrite as or as Does this have some utility, application or use somewhere?","f F 
F(x)=\int\,f(x)\,\mathrm{d}x.
 F 
F(x)=xf(x)-\frac{1}{2!}x^2f'(x)+\frac{1}{3!}x^3f''(x)-\frac{1}{4!}x^4f'''(x)+\cdots+(-1)^{k+1}\frac{x^{k+1}}{(k+1)!}f^{(k)}(x)+\cdots
 \begin{align*}
F(x)&=\Bigg[\sum_{k\geq0}(-1)^{k+1}\frac{x^{k+1}}{(k+1)!}\partial_x^k\Bigg]f(x)\\
&=\frac{e^{-x\partial_x}-1}{\partial_x}f(x).
\end{align*}","['integration', 'derivatives', 'exponential-function', 'differential-operators']"
46,"Show that the linear transformation $T$ is continuously differentiable at every point, and in fact $T'(x) = T$ for every $x$.","Show that the linear transformation  is continuously differentiable at every point, and in fact  for every .",T T'(x) = T x,"Let $T:\textbf{R}^{n}\to\textbf{R}^{m}$ be a linear transformation. Show that $T$ is continuously differentiable at every point, and in fact $T'(x) = T$ for every $x$ . My solution According to this result , $T$ is continuous because it is Lipschitz. On the other hand, due to the linearity of $T$ , for every $x_{0}\in\textbf{R}^{n}$ , we have that \begin{align*} \lim_{x\to x_{0};x\neq x_{0}}\frac{\|T(x) - T(x_{0}) - T(x - x_{0})\|}{\|x - x_{0}\|} = \lim_{x\to x_{0};x\neq x_{0}}\frac{\|T(x) - T(x_{0}) - T(x)  + T(x_{0})\|}{\|x-x_{0}\|} = 0 \end{align*} Hence $T'(x) = T$ . Once the derivative is unique, the result follows. Is the wording of my proof right? Is there another way to approach it?","Let be a linear transformation. Show that is continuously differentiable at every point, and in fact for every . My solution According to this result , is continuous because it is Lipschitz. On the other hand, due to the linearity of , for every , we have that Hence . Once the derivative is unique, the result follows. Is the wording of my proof right? Is there another way to approach it?","T:\textbf{R}^{n}\to\textbf{R}^{m} T T'(x) = T x T T x_{0}\in\textbf{R}^{n} \begin{align*}
\lim_{x\to x_{0};x\neq x_{0}}\frac{\|T(x) - T(x_{0}) - T(x - x_{0})\|}{\|x - x_{0}\|} = \lim_{x\to x_{0};x\neq x_{0}}\frac{\|T(x) - T(x_{0}) - T(x)  + T(x_{0})\|}{\|x-x_{0}\|} = 0
\end{align*} T'(x) = T","['real-analysis', 'derivatives', 'linear-transformations', 'solution-verification']"
47,Directional derivative along the intersection of two surfaces,Directional derivative along the intersection of two surfaces,,"How can i find the intersection curve between these two surfaces $$ \left\{ \begin{array}{cc}  2x^2 + 2y^2 − z^2 &= 50\\  x^2 + y^2 -z^2 &= 0 \end{array} \right. $$ I need it to find the directional derivative of $f(x, y, z) = x^2 + y^2 − z^2$ , with this point $(3, 4, 5)$ and along the intersection curve mentioned above. I know how to get the gradient, I just don't know how to aproach finding that intersection.","How can i find the intersection curve between these two surfaces I need it to find the directional derivative of , with this point and along the intersection curve mentioned above. I know how to get the gradient, I just don't know how to aproach finding that intersection.","
\left\{
\begin{array}{cc}
 2x^2 + 2y^2 − z^2 &= 50\\
 x^2 + y^2 -z^2 &= 0
\end{array}
\right.
 f(x, y, z) = x^2 + y^2 − z^2 (3, 4, 5)","['multivariable-calculus', 'derivatives']"
48,Connections and second derivatives of curves,Connections and second derivatives of curves,,"If $M$ is a smooth manifold, let $AM$ be the subspace of the double tangent bundle $TTM$ consisting of vectors $v$ such that $\pi^{TM}(v) = \pi^X_*(v)$ , where $\pi^X: TX \to X$ is the projection of a tangent bundle. It is a bundle of affine spaces on $TM$ , modeled on the ""tautological"" bundle $TM \times_M TM$ . The significance of $AM$ is that if $\gamma$ is a smooth curve on $M$ , its second derivative $\ddot \gamma$ lies in $AM$ . If the manifold $M$ is equipped with a section of $AM$ defined on all of $TM$ , we can define a sort of covariant derivative of the tangent vector of a curve, by taking the second derivative and subtracting the fixed section. In particular, we get a notion of geodesic. So it seems that such a section plays some of the roles of a connection on $TM$ . Do these objects have a name? Where can I learn more about them? Can I think of them as connections in some more general sense?","If is a smooth manifold, let be the subspace of the double tangent bundle consisting of vectors such that , where is the projection of a tangent bundle. It is a bundle of affine spaces on , modeled on the ""tautological"" bundle . The significance of is that if is a smooth curve on , its second derivative lies in . If the manifold is equipped with a section of defined on all of , we can define a sort of covariant derivative of the tangent vector of a curve, by taking the second derivative and subtracting the fixed section. In particular, we get a notion of geodesic. So it seems that such a section plays some of the roles of a connection on . Do these objects have a name? Where can I learn more about them? Can I think of them as connections in some more general sense?",M AM TTM v \pi^{TM}(v) = \pi^X_*(v) \pi^X: TX \to X TM TM \times_M TM AM \gamma M \ddot \gamma AM M AM TM TM,"['derivatives', 'smooth-manifolds', 'connections']"
49,"Can we justify the behavior ""taking derivative w.r.t. a variable""?","Can we justify the behavior ""taking derivative w.r.t. a variable""?",,"I have come across many times about this situation, which indicates that derivative of a function can be dependent on the ""variable"" that you are concerned with, e.g. one of the expression of the Chain Rule:(assume that all following functions are differentiable) $$ \frac{dy}{dx}= \frac{dy}{du} \cdot \frac{du}{dx} $$ What does $\displaystyle \frac{dy}{dx}$ and $\displaystyle \frac{dy}{du}$ mean? But with the other expression, if we can write $y=f \circ u$ , then $y'=(f' \circ u)\cdot u'$ . And by contrasting them, we know that $\displaystyle y'= \frac{dy}{dx}$ and $\displaystyle \frac{dy}{du} = f' \circ u, \frac{du}{dx} = u'$ . In my opinion, $\displaystyle \frac{dy}{du}$ tends to misleading people, since we are using functions $f$ and $u$ rather than function $y$ , and I  think the derivative of a function is uniquely determined, regardless of its ""variable"". So, are there insights with which one can justify this behabior?","I have come across many times about this situation, which indicates that derivative of a function can be dependent on the ""variable"" that you are concerned with, e.g. one of the expression of the Chain Rule:(assume that all following functions are differentiable) What does and mean? But with the other expression, if we can write , then . And by contrasting them, we know that and . In my opinion, tends to misleading people, since we are using functions and rather than function , and I  think the derivative of a function is uniquely determined, regardless of its ""variable"". So, are there insights with which one can justify this behabior?","
\frac{dy}{dx}= \frac{dy}{du} \cdot \frac{du}{dx}
 \displaystyle \frac{dy}{dx} \displaystyle \frac{dy}{du} y=f \circ u y'=(f' \circ u)\cdot u' \displaystyle y'= \frac{dy}{dx} \displaystyle \frac{dy}{du} = f' \circ u, \frac{du}{dx} = u' \displaystyle \frac{dy}{du} f u y","['real-analysis', 'derivatives']"
50,Basic question on automatic differentiation. Why we compute jacobian-vector product,Basic question on automatic differentiation. Why we compute jacobian-vector product,,"I am struggling a bit with automatic differentiation. Let us focus on the forward mode. There are two components of the method I can't put together: The ""seeds"" and the vector $v$ that multiplies the jacobian. Let's say we have a function $f(x): \mathbb{R}^n \rightarrow \mathbb{R}^m$ which is a composite function like $$ f = f^L \circ f^{L-1} \circ \dots \circ f^{1} $$ we want to find the Jacobian matrix $J$ of $f$ with respect to $x$ . Then with the chain rule we can write $$ J = J^L J^{L-1} \dots J^{1}  $$ now in practice I read that the AD algorithm does not actually computes $J$ but the product $Jv$ where $v$ is a vector. (I don't get why at the moment). From this blog post I understand that one has to provide the ""seeds"" of the derivatives. So I thought that $v$ are the seeds. Is that correct? If yes, to me it makes sense to always use seeds that are equal to $1$ . Is there a reason why sometimes it makes sense to use seeds not equal to $1$ ?","I am struggling a bit with automatic differentiation. Let us focus on the forward mode. There are two components of the method I can't put together: The ""seeds"" and the vector that multiplies the jacobian. Let's say we have a function which is a composite function like we want to find the Jacobian matrix of with respect to . Then with the chain rule we can write now in practice I read that the AD algorithm does not actually computes but the product where is a vector. (I don't get why at the moment). From this blog post I understand that one has to provide the ""seeds"" of the derivatives. So I thought that are the seeds. Is that correct? If yes, to me it makes sense to always use seeds that are equal to . Is there a reason why sometimes it makes sense to use seeds not equal to ?","v f(x): \mathbb{R}^n \rightarrow \mathbb{R}^m 
f = f^L \circ f^{L-1} \circ \dots \circ f^{1}
 J f x 
J = J^L J^{L-1} \dots J^{1} 
 J Jv v v 1 1","['derivatives', 'algorithms']"
51,How to apply Leibniz rule for this integral?,How to apply Leibniz rule for this integral?,,"I am struck with the problem of how to evaluate the following integration: $$\frac{d}{dt}\int_0^{a(t)}{(a (t)-s)^{\alpha-1}f (s) ds}$$ where $f(s)$ is differentiable and $\alpha\in (0,1)$ . The problem is the integrand might not converge when substituting the upper limit",I am struck with the problem of how to evaluate the following integration: where is differentiable and . The problem is the integrand might not converge when substituting the upper limit,"\frac{d}{dt}\int_0^{a(t)}{(a (t)-s)^{\alpha-1}f (s) ds} f(s) \alpha\in (0,1)","['calculus', 'integration', 'derivatives']"
52,How can I calculate the curl of this vector field?,How can I calculate the curl of this vector field?,,"I have a vector field which is originallly written as $$ \mathbf A = \frac{\mu_0~n~I}{2} ~\hat \theta$$ and I translated it like this $$\mathbf A = 0 ~\hat{r},~~ \frac{\mu_0 ~n~I~r}{2} ~\hat{\theta} , ~~0 ~\hat{\phi}$$ ( $r$ is the distance from origin, $\theta$ is azimuthal angle and $\phi$ is the polar angle). I want to calculate its divergence, so I have two options either change it into Cartesian system and then take the curl or take the curl in spherical coordinates. Let’s take the curl in spherical coordinates: $$(curl~\mathbf A)_r = \frac{1}{r\sin \theta} \frac{\partial (A_{\phi} \sin \theta)}{\theta} -\frac{1}{r\sin \theta} \frac{\partial A_{\theta}}{\partial \theta}$$ I really don't know what is $r$ and that $\sin \theta$ which appearing in the first term above, nevertheless, it is zero as $A_{\phi}$ is zero and even our $A_\theta$ is independent of $\theta$ , therefore, whole term above is zero. So, we have $(curl~\mathbf A)_r = 0$ . We have $$(curl~\mathbf A)_{\theta} = \frac{1}{r\sin \theta} \frac{\partial A_r}{\partial \phi} - \frac{1}{r} \frac{\partial ( r A_{\phi} )}{\partial r}$$ I want to repeat that I don't know if those $r$ and $\sin \theta$ are part of my vector field or not, nonetheless, both terms will be zero because $A_r$ and $A_{\phi}$ are zero. Therefore, $(curl~\mathbf A)_{\theta} = 0$ . The last component, $$(curl~\mathbf A)_{\phi}=\frac{1}{r} \frac{\partial (rA_{\theta})}{\partial r} - \frac{1}{r} \frac{\partial A_r}{\partial \theta}$$ I'm just assuming that $r$ which appears in the formula above is same as the one which appears in my vector field. $$(curl~\mathbf A)_{\phi}= \frac{\mu_0 ~n~I}{2r} \frac{\partial r^2}{\partial r}$$ $$ (curl~\mathbf A)_{\phi} = \mu_0~n~I $$ Now, I want to know if I'm correct. How can convert the above result into cartesian system? Well, I have something like this (if we call curl of $\mathbf A$ as $\mathbf B$ ) $$ \mathbf B( r, \theta, \phi) = \left( 0, 0 , \mu_0 ~n~I \right)$$ we don't have $\phi$ as an angle and all three $x, ~y, ~z$ involve either $sine$ or $cosine$ of $\phi$ and multiplication by $r$ as $$ x=r\sin\phi \cos\theta\\ \hspace{10px} y=r\sin\phi \sin\theta \\ z =r\cos\phi$$ so, in that sense all the cartesian components are zero because $r$ is zero. Please guide me through it.","I have a vector field which is originallly written as and I translated it like this ( is the distance from origin, is azimuthal angle and is the polar angle). I want to calculate its divergence, so I have two options either change it into Cartesian system and then take the curl or take the curl in spherical coordinates. Let’s take the curl in spherical coordinates: I really don't know what is and that which appearing in the first term above, nevertheless, it is zero as is zero and even our is independent of , therefore, whole term above is zero. So, we have . We have I want to repeat that I don't know if those and are part of my vector field or not, nonetheless, both terms will be zero because and are zero. Therefore, . The last component, I'm just assuming that which appears in the formula above is same as the one which appears in my vector field. Now, I want to know if I'm correct. How can convert the above result into cartesian system? Well, I have something like this (if we call curl of as ) we don't have as an angle and all three involve either or of and multiplication by as so, in that sense all the cartesian components are zero because is zero. Please guide me through it."," \mathbf A = \frac{\mu_0~n~I}{2} ~\hat \theta \mathbf A = 0 ~\hat{r},~~ \frac{\mu_0 ~n~I~r}{2} ~\hat{\theta} , ~~0 ~\hat{\phi} r \theta \phi (curl~\mathbf A)_r = \frac{1}{r\sin \theta} \frac{\partial (A_{\phi} \sin \theta)}{\theta} -\frac{1}{r\sin \theta} \frac{\partial A_{\theta}}{\partial \theta} r \sin \theta A_{\phi} A_\theta \theta (curl~\mathbf A)_r = 0 (curl~\mathbf A)_{\theta} = \frac{1}{r\sin \theta} \frac{\partial A_r}{\partial \phi} - \frac{1}{r} \frac{\partial ( r A_{\phi} )}{\partial r} r \sin \theta A_r A_{\phi} (curl~\mathbf A)_{\theta} = 0 (curl~\mathbf A)_{\phi}=\frac{1}{r} \frac{\partial (rA_{\theta})}{\partial r} - \frac{1}{r} \frac{\partial A_r}{\partial \theta} r (curl~\mathbf A)_{\phi}= \frac{\mu_0 ~n~I}{2r} \frac{\partial r^2}{\partial r}  (curl~\mathbf A)_{\phi} = \mu_0~n~I  \mathbf A \mathbf B  \mathbf B( r, \theta, \phi) = \left( 0, 0 , \mu_0 ~n~I \right) \phi x, ~y, ~z sine cosine \phi r  x=r\sin\phi \cos\theta\\ \hspace{10px} y=r\sin\phi \sin\theta \\ z =r\cos\phi r","['multivariable-calculus', 'derivatives', 'spherical-coordinates', 'curl']"
53,About weak convergence and derivative,About weak convergence and derivative,,"The following question had appeared in so many places, but none justify it, I tried a lot but . If someone can give me a hand. Let $X$ be a Hilbert space and $I:X\rightarrow \mathbb{R}$ a functional. If $(u_n)\subset X$ is such that $u_n$ converges weakly to $u$ in $X$ and $I'(u_n)\varphi \rightarrow 0$ for every $\varphi \in X$ , then $I'(u)=0$ , where $I'$ denotes the derivative of $I$ . Observations: 1) we know that $\|I'(u_n)\|\rightarrow 0$ (this is a fact in all the cases where this problem happened, so it can be used). 2) we don't know nothing special about $I$ besides being of class $C^1$ . 3) I tried to prove that I get convergence $I(u_n)\varphi \rightarrow I(u)\varphi$ on $\mathbb{R}$ so I could use weak lower semicontinuity (w.l.s.) of the module function. I also tried something about weak converge on $X^\ast$ so I could use w.l.s. of the dual norm, but once again I wasn't sucessful. I appreciate any help. Thanks in advance.","The following question had appeared in so many places, but none justify it, I tried a lot but . If someone can give me a hand. Let be a Hilbert space and a functional. If is such that converges weakly to in and for every , then , where denotes the derivative of . Observations: 1) we know that (this is a fact in all the cases where this problem happened, so it can be used). 2) we don't know nothing special about besides being of class . 3) I tried to prove that I get convergence on so I could use weak lower semicontinuity (w.l.s.) of the module function. I also tried something about weak converge on so I could use w.l.s. of the dual norm, but once again I wasn't sucessful. I appreciate any help. Thanks in advance.",X I:X\rightarrow \mathbb{R} (u_n)\subset X u_n u X I'(u_n)\varphi \rightarrow 0 \varphi \in X I'(u)=0 I' I \|I'(u_n)\|\rightarrow 0 I C^1 I(u_n)\varphi \rightarrow I(u)\varphi \mathbb{R} X^\ast,"['functional-analysis', 'analysis', 'derivatives', 'hilbert-spaces', 'weak-convergence']"
54,Differentiating $y=\exp(x+\exp(x+\exp(x+\exp(x+\cdots))))$ and writing derivatives of $\exp$ functions in terms of $y$ only,Differentiating  and writing derivatives of  functions in terms of  only,y=\exp(x+\exp(x+\exp(x+\exp(x+\cdots)))) \exp y,"I want to differentiate $$y=\exp(x+\exp(x+\exp(x+\exp(x+\cdots)))).$$ Is the following substitution and differentiation appropriate? \begin{align} y&=\exp(x+\exp(x+\exp(x+\exp(x+\cdots))))\\ y&=\exp(x+y)\\ \frac{dy}{dx}&=\exp(x+y)\left(1+\frac{dy}{dx}\right)\\ \frac{dy}{dx}&=y\left(1+\frac{dy}{dx}\right)\\ \frac{dy}{dx}&=\frac{y}{1-y}\\ \end{align} As well, I notice that I can write the derivative (and therefore any higher order derivative) in terms of $y$ only.  What is required of the $\exp$ function such that I will not be able to write the derivative in terms of $y$ only?  It does not seem like the reason is that the $x$ in the argument is just $x$ and not some function of $x$ :  for example, \begin{align} y&=\exp x^2\qquad\quad\implies x=\log y-\log2\\ \frac{dy}{dx}&=2x\exp x^2\\ \frac{dy}{dx}&=2xy\\ \frac{dy}{dx}&=2y\,(\log y-\log2)\\ \end{align} and now the derivative is free of $x$ .  Knowing when this is the case would be useful in constructing problems where we can find the slope of the tangent line given only a $y$ value.","I want to differentiate Is the following substitution and differentiation appropriate? As well, I notice that I can write the derivative (and therefore any higher order derivative) in terms of only.  What is required of the function such that I will not be able to write the derivative in terms of only?  It does not seem like the reason is that the in the argument is just and not some function of :  for example, and now the derivative is free of .  Knowing when this is the case would be useful in constructing problems where we can find the slope of the tangent line given only a value.","y=\exp(x+\exp(x+\exp(x+\exp(x+\cdots)))). \begin{align}
y&=\exp(x+\exp(x+\exp(x+\exp(x+\cdots))))\\
y&=\exp(x+y)\\
\frac{dy}{dx}&=\exp(x+y)\left(1+\frac{dy}{dx}\right)\\
\frac{dy}{dx}&=y\left(1+\frac{dy}{dx}\right)\\
\frac{dy}{dx}&=\frac{y}{1-y}\\
\end{align} y \exp y x x x \begin{align}
y&=\exp x^2\qquad\quad\implies x=\log y-\log2\\
\frac{dy}{dx}&=2x\exp x^2\\
\frac{dy}{dx}&=2xy\\
\frac{dy}{dx}&=2y\,(\log y-\log2)\\
\end{align} x y","['calculus', 'derivatives', 'exponential-function', 'implicit-differentiation']"
55,Is this expression OK? $d(xy)=xdy+ydx$,Is this expression OK?,d(xy)=xdy+ydx,Is it OK to write $d(xy)=xdy+ydx$ ? I thought it should be written as $\frac{d(xy)}{dx}=x\frac{dy}{dx}+y$ to clarify with respect to which variable the equation is differentiated. Is the first expression widely accepted in mathematics? What would happen if you combine it with an integral? $\int{\frac{d(xy)}{dx}dx}=\int{d(xy)}=xy$ ?,Is it OK to write ? I thought it should be written as to clarify with respect to which variable the equation is differentiated. Is the first expression widely accepted in mathematics? What would happen if you combine it with an integral? ?,d(xy)=xdy+ydx \frac{d(xy)}{dx}=x\frac{dy}{dx}+y \int{\frac{d(xy)}{dx}dx}=\int{d(xy)}=xy,"['calculus', 'derivatives']"
56,Analyzing the given condition for a twice differentiable function $(f(0))^2 + (f'(0))^2 = 85$. [duplicate],Analyzing the given condition for a twice differentiable function . [duplicate],(f(0))^2 + (f'(0))^2 = 85,"This question already has answers here : For every twice differentiable function $f : \bf R \rightarrow [–2, 2]$ with $(f(0))^2 + (f'(0))^2 = 85$, which of the following statements are TRUE? (2 answers) Closed 4 years ago . We are given that a twice differentiable function, $f:\Bbb R\rightarrow[-2,2]$ satisfies the condition $$(f(0))^2 + (f'(0))^2 = 85$$ We are asked if exists a value of $x$ , say $α \in (-4,4)$ , for which $f(α) + f''(α) = 0$ and $f'(α) \neq0$ . (Please note we are merely asked about the existence of such value and not the value itself) My attempt : Suppose I take a function $$p(x) = (f(x))^2 + (f'(x))^2$$ Taking the derivative I obtained, $$p'(x) = 2f(x)f'(x) + 2f'(x)f''(x)$$ Which can be rewritten as $$p'(x) = 2f'(x)[f'(x)+ f''(x)]$$ So if I can somehow prove that p'(x) = 0 at some point and f'(x) is not zero at that point, I know that there is a value of α. But not sure how to proceed any further. Can I apply LMVT somewhere? Have I missed something? Any help would be appreciated!","This question already has answers here : For every twice differentiable function $f : \bf R \rightarrow [–2, 2]$ with $(f(0))^2 + (f'(0))^2 = 85$, which of the following statements are TRUE? (2 answers) Closed 4 years ago . We are given that a twice differentiable function, satisfies the condition We are asked if exists a value of , say , for which and . (Please note we are merely asked about the existence of such value and not the value itself) My attempt : Suppose I take a function Taking the derivative I obtained, Which can be rewritten as So if I can somehow prove that p'(x) = 0 at some point and f'(x) is not zero at that point, I know that there is a value of α. But not sure how to proceed any further. Can I apply LMVT somewhere? Have I missed something? Any help would be appreciated!","f:\Bbb R\rightarrow[-2,2] (f(0))^2 + (f'(0))^2 = 85 x α \in (-4,4) f(α) + f''(α) = 0 f'(α) \neq0 p(x) = (f(x))^2 + (f'(x))^2 p'(x) = 2f(x)f'(x) + 2f'(x)f''(x) p'(x) = 2f'(x)[f'(x)+ f''(x)]","['calculus', 'limits', 'analysis', 'derivatives']"
57,Radon transform maps a Schwartz function to a differentiable function,Radon transform maps a Schwartz function to a differentiable function,,"This isn't supposed to be hard, but I just can't seem to be able to write this down. My Radon transform is defined as follows: Let $f\in C_c^{\infty}(\mathbb{R}^2)$ , then $$Rf(s,\omega)=\int_{\mathbb{R}}f(s\omega+t\omega^{\perp})\,dt,\quad s\in\mathbb{R}, \omega\in S^1,$$ where $\omega^{\perp}=(-\omega_2,\omega_1)$ . I am supposed to show that if $f\in C_c^{\infty}(\mathbb{R}^2)$ then $Rf\in C^{\infty}(\mathbb{R}\times S^1)$ . I have already used Dominated convergence to show that $Rf$ is continuous and I know that the derivatives can be shown to be continuous the same way. My problem is to show that the derivatives exist. I know this is a silly question, but I am just stuck. I am trying to show the existence using difference quotients and Dominated convergence, but I don't know what should be the dominating function here. EDIT: I used Leibniz integration rule for the above mentioned problem. But a related problem is this: If $f$ is a Schwartz function then I need to show that $Rf\in C^{\infty}(\mathbb{R}\times S^1)$ . For this I have shown that $Rf$ is continuous but I don't know how to prove the existence of derivatives. Again I am trying to use difference quotients. My definition for the Schwartz function is that $f$ is a Schwartz function if $f\in C^{\infty}(\mathbb{R}^2)$ and $x^a\partial^bf\in L^{\infty}(\mathbb{R}^2)$ for all multi-indices $a$ and $b$ .","This isn't supposed to be hard, but I just can't seem to be able to write this down. My Radon transform is defined as follows: Let , then where . I am supposed to show that if then . I have already used Dominated convergence to show that is continuous and I know that the derivatives can be shown to be continuous the same way. My problem is to show that the derivatives exist. I know this is a silly question, but I am just stuck. I am trying to show the existence using difference quotients and Dominated convergence, but I don't know what should be the dominating function here. EDIT: I used Leibniz integration rule for the above mentioned problem. But a related problem is this: If is a Schwartz function then I need to show that . For this I have shown that is continuous but I don't know how to prove the existence of derivatives. Again I am trying to use difference quotients. My definition for the Schwartz function is that is a Schwartz function if and for all multi-indices and .","f\in C_c^{\infty}(\mathbb{R}^2) Rf(s,\omega)=\int_{\mathbb{R}}f(s\omega+t\omega^{\perp})\,dt,\quad s\in\mathbb{R}, \omega\in S^1, \omega^{\perp}=(-\omega_2,\omega_1) f\in C_c^{\infty}(\mathbb{R}^2) Rf\in C^{\infty}(\mathbb{R}\times S^1) Rf f Rf\in C^{\infty}(\mathbb{R}\times S^1) Rf f f\in C^{\infty}(\mathbb{R}^2) x^a\partial^bf\in L^{\infty}(\mathbb{R}^2) a b","['derivatives', 'schwartz-space', 'inverse-problems']"
58,Information Bottleneck - Proof of Algorithm,Information Bottleneck - Proof of Algorithm,,"In the Information Bottleneck (IB) paper ( https://arxiv.org/pdf/physics/0004057.pdf ). Using lagrange multipliers we need to solve $\frac{\delta F}{\delta p(\tilde{x}|x)}=0$ , where $F = I(X;\tilde{X})+\beta d(x,\tilde{x})$ . By substituting $I(X;\tilde{X})=\sum_{x,\tilde{x}}p(x,\tilde{x})log(\frac{p(\tilde{x}|x)}{p(\tilde{x})})$ and taking the derivative of $F$ , the paper writes down the solution as follows, $\frac{\delta F}{\delta p(\tilde{x}|x)}= p(x)[log(\frac{p(\tilde{x}|x)}{p(\tilde{x})})+1-\frac{1}{p(\tilde{x})}\sum_{x'}p(x')p(\tilde{x}|x')+\beta d(x,\tilde{x})+\frac{\lambda(x)}{p(x)}]$ . Question: I don't understand how it gets the term "" $-\frac{1}{p(\tilde{x})}\sum_{x'}p(x')p(\tilde{x}|x')$ ""? The rest of the terms are clear but I cannot understand only this term.","In the Information Bottleneck (IB) paper ( https://arxiv.org/pdf/physics/0004057.pdf ). Using lagrange multipliers we need to solve , where . By substituting and taking the derivative of , the paper writes down the solution as follows, . Question: I don't understand how it gets the term "" ""? The rest of the terms are clear but I cannot understand only this term.","\frac{\delta F}{\delta p(\tilde{x}|x)}=0 F = I(X;\tilde{X})+\beta d(x,\tilde{x}) I(X;\tilde{X})=\sum_{x,\tilde{x}}p(x,\tilde{x})log(\frac{p(\tilde{x}|x)}{p(\tilde{x})}) F \frac{\delta F}{\delta p(\tilde{x}|x)}= p(x)[log(\frac{p(\tilde{x}|x)}{p(\tilde{x})})+1-\frac{1}{p(\tilde{x})}\sum_{x'}p(x')p(\tilde{x}|x')+\beta d(x,\tilde{x})+\frac{\lambda(x)}{p(x)}] -\frac{1}{p(\tilde{x})}\sum_{x'}p(x')p(\tilde{x}|x')",['derivatives']
59,Derivative of a matrix-vector multiplication with respect to the matrix,Derivative of a matrix-vector multiplication with respect to the matrix,,"Let's assume that we have $\textbf{x} \in \mathbb{R}^n$ and $A \in \mathbb{R}^{m \times n}$ . I want to show that $\frac{\partial(\textbf{Ax})}{\partial\textbf{A}} = \textbf{x}^T$ , e.g. in [0], where we have: $$\frac{\partial\textbf{W}_3\textbf{x}_2}{\partial\textbf{W}_3} = \textbf{x}_2^T$$ for $\frac{\partial E}{\partial\textbf{W}_3}$ . As far as I understand, we have $\textbf{Ax} \in \mathbb{R}^{m}$ , so in reality we differentiate the vector with respect to the matrix, what gives us a tensor with its 3D representation. I'm not sure how this can be equal to $\textbf{x}^T$ . [0] Sudeep Raja, A derivation of backpropagation in matrix form , August 17, 2016.","Let's assume that we have and . I want to show that , e.g. in [0], where we have: for . As far as I understand, we have , so in reality we differentiate the vector with respect to the matrix, what gives us a tensor with its 3D representation. I'm not sure how this can be equal to . [0] Sudeep Raja, A derivation of backpropagation in matrix form , August 17, 2016.",\textbf{x} \in \mathbb{R}^n A \in \mathbb{R}^{m \times n} \frac{\partial(\textbf{Ax})}{\partial\textbf{A}} = \textbf{x}^T \frac{\partial\textbf{W}_3\textbf{x}_2}{\partial\textbf{W}_3} = \textbf{x}_2^T \frac{\partial E}{\partial\textbf{W}_3} \textbf{Ax} \in \mathbb{R}^{m} \textbf{x}^T,"['matrices', 'derivatives', 'vectors', 'tensors']"
60,Exterior Derivative vs. Covariant Derivative vs. Lie Derivative,Exterior Derivative vs. Covariant Derivative vs. Lie Derivative,,"In differential geometry, there are several notions of differentiation, namely: Exterior Derivative, $d$ Covariant Derivative/Connection, $\nabla$ Lie Derivative, $\mathcal{L}$. I have listed them in order of appearance in my education/in descending order of my understanding of them. Note, there may be others that I am yet to encounter. Conceptually, I am not sure how these three notions fit together. Looking at their definitions, I can see that there is even some overlap between the collection of objects they can each act on. I am trying to get my head around why there are (at least) three different notions of differentiation. I suppose my confusion can be summarised by the following question. What does each one do that the other two can't? I don't just mean which objects can they act on that the other two can't, I would like a deeper explanation (if it exists, which I believe it does). In terms of their geometric intuition/interpretation, does it make sense that we need these different notions? Note, I have put the reference request tag on this question because I would be interested to find some resources which have a discussion of these notions concurrently, as opposed to being presented as individual concepts.","In differential geometry, there are several notions of differentiation, namely: Exterior Derivative, $d$ Covariant Derivative/Connection, $\nabla$ Lie Derivative, $\mathcal{L}$. I have listed them in order of appearance in my education/in descending order of my understanding of them. Note, there may be others that I am yet to encounter. Conceptually, I am not sure how these three notions fit together. Looking at their definitions, I can see that there is even some overlap between the collection of objects they can each act on. I am trying to get my head around why there are (at least) three different notions of differentiation. I suppose my confusion can be summarised by the following question. What does each one do that the other two can't? I don't just mean which objects can they act on that the other two can't, I would like a deeper explanation (if it exists, which I believe it does). In terms of their geometric intuition/interpretation, does it make sense that we need these different notions? Note, I have put the reference request tag on this question because I would be interested to find some resources which have a discussion of these notions concurrently, as opposed to being presented as individual concepts.",,"['reference-request', 'differential-geometry', 'differential-forms', 'exterior-algebra', 'big-picture']"
61,Teaching myself differential topology and differential geometry,Teaching myself differential topology and differential geometry,,"I have a hazy notion of some stuff in differential geometry and a better, but still not quite rigorous understanding of basics of differential topology. I have decided to fix this lacuna once for all. Unfortunately I cannot attend a course right now. I must teach myself all the stuff by reading books. Towards this purpose I want to know what are the most important basic theorems in differential geometry and differential topology. For a start, for differential topology, I think I must read Stokes' theorem and de Rham theorem with complete proofs. Differential geometry is a bit more difficult. What is a connection? Which notion should I use? I want to know about parallel transport and holonomy. What are the most important and basic theorems here? Are there concise books which can teach me the stuff faster than the voluminous Spivak books? Also finally I want to read into some algebraic geometry and Hodge/Kähler stuff. Suggestions about important theorems and concepts to learn, and book references, will be most helpful.","I have a hazy notion of some stuff in differential geometry and a better, but still not quite rigorous understanding of basics of differential topology. I have decided to fix this lacuna once for all. Unfortunately I cannot attend a course right now. I must teach myself all the stuff by reading books. Towards this purpose I want to know what are the most important basic theorems in differential geometry and differential topology. For a start, for differential topology, I think I must read Stokes' theorem and de Rham theorem with complete proofs. Differential geometry is a bit more difficult. What is a connection? Which notion should I use? I want to know about parallel transport and holonomy. What are the most important and basic theorems here? Are there concise books which can teach me the stuff faster than the voluminous Spivak books? Also finally I want to read into some algebraic geometry and Hodge/Kähler stuff. Suggestions about important theorems and concepts to learn, and book references, will be most helpful.",,"['differential-geometry', 'reference-request', 'differential-topology', 'book-recommendation']"
62,Direct proof that the wedge product preserves integral cohomology classes?,Direct proof that the wedge product preserves integral cohomology classes?,,"Let $H^k(M,\mathbb R)$ be the De Rham cohomology of a manifold $M$. There is a canonical map $H^k(M;\mathbb Z) \to H^k(M;\mathbb R)$ from the integral cohomology to the cohomology with coefficients in $\mathbb R$, which is isomorphic to the De Rham cohomology. As a previous question already revealed, the images of this map are precisely the classes of differential $k$-forms $[\omega]$ that yield integers when integrated over a $k$-cycle $\sigma$, $$ \int_{\sigma} \omega \in \mathbb{Z}  \quad\text{ whenever } d\sigma = 0$$ Let us call them ""integral forms"". Motivated by the cup product on cohomology, my question/request is the following: Give a direct proof that the wedge product $[\omega\wedge\eta]\in H^{k+l}(M,\mathbb R)$ of two integral forms $\omega\in \Omega^k(M)$ and $\eta\in \Omega^l(M)$ is again an integral form. This should be true because the cup product is mapped to the wedge product, but the point of the exercise is to prove this statement directly, without constructing the singular cohomology $H^k(M,\mathbb Z)$ or homology first. Maybe I also have to make sure that the condition of being an integral form is something that can be ""checked effectively"" without singular homology; this might be subject to a new question.","Let $H^k(M,\mathbb R)$ be the De Rham cohomology of a manifold $M$. There is a canonical map $H^k(M;\mathbb Z) \to H^k(M;\mathbb R)$ from the integral cohomology to the cohomology with coefficients in $\mathbb R$, which is isomorphic to the De Rham cohomology. As a previous question already revealed, the images of this map are precisely the classes of differential $k$-forms $[\omega]$ that yield integers when integrated over a $k$-cycle $\sigma$, $$ \int_{\sigma} \omega \in \mathbb{Z}  \quad\text{ whenever } d\sigma = 0$$ Let us call them ""integral forms"". Motivated by the cup product on cohomology, my question/request is the following: Give a direct proof that the wedge product $[\omega\wedge\eta]\in H^{k+l}(M,\mathbb R)$ of two integral forms $\omega\in \Omega^k(M)$ and $\eta\in \Omega^l(M)$ is again an integral form. This should be true because the cup product is mapped to the wedge product, but the point of the exercise is to prove this statement directly, without constructing the singular cohomology $H^k(M,\mathbb Z)$ or homology first. Maybe I also have to make sure that the condition of being an integral form is something that can be ""checked effectively"" without singular homology; this might be subject to a new question.",,"['differential-geometry', 'algebraic-topology', 'homology-cohomology', 'differential-forms']"
63,What is a covector and what is it used for?,What is a covector and what is it used for?,,"From what I understand, a covector is an object that takes a vector and returns a number. So given a vector $v \in V$ and a covector $\phi \in V^*$, you can act on $v$ with $\phi$ to get a real number $\phi(v)$. Is that ""it"" or is there more to it? I find this simple realization hard to reconcile with an example given in my textbook: ""Geometric Measure Theory"" by Frank Morgan, where he explains that given $\mathbb{R}^n$, the dual space $\mathbb{R}^{n*}$ has basis $dx_1, dx_2 \dots$. So lets say we are in $\mathbb{R}^2$, so $dx$ is a function on any vector, that will return a scalar? I can't imagine what $dx([1,2])$ is. Can someone explain to me this concept of covectors reconciled with the infinitesimal case? Edit: It has something to do with biorthogonality? I am really stumped on these concepts, like I'm ramming my head against a wall.","From what I understand, a covector is an object that takes a vector and returns a number. So given a vector $v \in V$ and a covector $\phi \in V^*$, you can act on $v$ with $\phi$ to get a real number $\phi(v)$. Is that ""it"" or is there more to it? I find this simple realization hard to reconcile with an example given in my textbook: ""Geometric Measure Theory"" by Frank Morgan, where he explains that given $\mathbb{R}^n$, the dual space $\mathbb{R}^{n*}$ has basis $dx_1, dx_2 \dots$. So lets say we are in $\mathbb{R}^2$, so $dx$ is a function on any vector, that will return a scalar? I can't imagine what $dx([1,2])$ is. Can someone explain to me this concept of covectors reconciled with the infinitesimal case? Edit: It has something to do with biorthogonality? I am really stumped on these concepts, like I'm ramming my head against a wall.",,"['differential-geometry', 'differential-forms', 'tensors']"
64,Why is a PDE a submanifold (and not just a subset)?,Why is a PDE a submanifold (and not just a subset)?,,"I struggle a bit with understanding the idea behind the definition of a PDE on a fibred manifold. Let $\pi: E \to M$ be a smooth locally trivial fibre bundle. In Gromovs words a partial differential relation of order $k$ is a subset of the $k$th jet bundle $J^k(E)$. Usually one defines a  partial differential equation of order $k$ to be a closed submanifold of the $k$th jet bundle $J^k(E)$. This last definition  brings in some kind of regularity condition. I do not really understand why one wants/needs this regularity. What is the advantage of asking a PDE to be a submanifold in lieu of beeing merely a subset of $J^k(E)$? Let me explain a bit further:  (First, remark that this is somehow a follow up for my earlier question: Why is a differential equation a submanifold of a jet bundle? ) Often a partial differntial relation comes from a differential operator. Let $\rho: H \to M$ be another fibred manifold. A differential operator of order $k$ is a map $D_f: \Gamma_{loc}(\pi) \to \Gamma_{loc}(\rho)$ between local sections, for which there exists a bundle morphism $f: J^k(E) \to H$ such that for every local section $\phi \in \Gamma_{loc}(\pi)$ the equality $D_f (\phi) (p) = f(j^k_p \phi)$ holds for all $p$ in the domain of $\phi$. Now the preimage of a given section $\eta$ of $\rho$ under $f$ is clearly a partial differential relation, but for it to be a partial differntial equation we need to ask $df$ to have constant rank. This previous paragraph is a bit opposed to my naiv interpretation of a PDE in $R^n$ to be an arbitrary equation in the indpendent variable, the funtion itself and it's partial derivatives. Just in the sense of wikipedia: A partial differential equation looks like $F(x_1, \dots, x_n, u(x), \frac{\partial u}{\partial x_1}, \dots ) = 0$ for an arbitrary function $F$ (with image, say, in $R$). Now, if we are looking for functions $u: R^n \to R^m$ we can set $E = R^n \times R^m$ with $\pi$ beeing the projection on the first factor and $H = R^n \times R$. $F$ is a bundle map, that implicitly also defines a differential operator $D_F$. So in the stricter sense of beeing a submanifold, $F$ defines only a partial differential equation, if it hat constant rank, otherwise it is ""merely"" a partial differential relation. Edit: Example : Why is such a thing as $\frac{\partial u}{\partial x} \frac{\partial u}{\partial y} = 0$ not a pde (in the narrower sense)? In this case $(E, M, \pi) = (R^2 \times R, R^2, pr_2)$ with coordinates $(x_1, x_2, y)$. The first jet bundle $J^1(E)$ has coordinates $(x_1, x_2, y, p_1, p_2)$. The partial differential relation $S = \lbrace (x_1, x_2, y, p_1, p_2) \, | \, p_1 p_2 = 0 \rbrace$ describes the equation I started with and is clearly not a smooth submanifold. Why is this gap between the naiv real analysis point of view and the somewhat more elaboratet differential geometric point of view? Last I'm curious if there is a nice way to see, that a specific relation is in deed a partial differential equation. For example the equality $\Phi^* g = g$ specifying local diffeomorphisms that are isometries with respect to a pseudo-Riemannian metric $g$ is often said to be a partial differential equation, without giving an argument. And in parts I agree: this really feels like a partial differential equation. But to show that it defines in deed a submanifold needs some works, as can be seen in my earlier question Why is $\phi^* g = g$ a PDE for a pseudo-Riemannian metric $g$ on a manifold? , which got a very nice answer. Other examples that are often said to be defined by partial differential equations are locally defined affine maps with respect to an arbitrary connection $\nabla$ in $TM$ or locally defined diffeomorphisms that preserve a given tensor field $T$ or even those affine diffeomorphisms that in addtion preserve a given tensor field. Maybe it would be helpful to see an argument in coordinates in one of this cases, to see how this intuitivly obvious statements can be made rigorous. Edit : To start with the ""coorinate argument"" I asked for in the last paragraph, set $E=M \times M$ and $\pi$ the projection to the first factor and we can restrict ourselfes to the open submanifold of $J^1(E)$, consisting of 1-jets of local diffeomorphisms. Note that for $(x,U)$ a chart arount $p$ and $(y, V)$ a chart arount $f(p)$, a chart arount $j^1_p f$ is given by $\phi_{xy}: j^1_q g \to (x(q), y(g(q)), d(y \circ g \circ x^{-1})_{x(q)}) \in x(U) \times y(V) \times GL(\Bbb R^n)$. Remark, that for $\phi_{xy}(j^1_q g) = (\xi, \eta, A)$ and $\Phi (\mu) := A(\mu) + \eta - A(\xi)$ the function $y^{-1} \circ \Phi \circ x$ (restricted to a suitable subset) is a representative of the equivalence class $j^1_q g$. Now we can pullback the connection $\nabla$ and/or the tensorfield $T$ to give connections/tensor fields $\nabla^x, \nabla^y, T^x, T^y$ on $x(U)$, $y(v)$ via $x^{-1}, y^{-1}$ respectively. The property to preserve the connection/tensor field spells out in coordinates as \begin{align*} F(\xi, \eta, A) &= (\Phi^* T^y)_{\xi} - T^x_{\xi} = 0 \\ G(\xi, \eta, A) &= (\Phi^* \nabla^y)_{\xi} - \nabla^x_{\xi} = 0 \end{align*} (The first term of $F$ resp. $G$ depends only on $\eta$ and $A$, whereas the second term depends only on $\xi$.) This can be translatet into equations for the components of $T^x$ and $T^y$ resp. the Christoffel symbols of $\nabla^x, \nabla^y$ with respect to the standard coordniate frame of $\Bbb R^n$. So far so good. But now I need to proove, that the rank of the differentials of $F, G, (F,G)$ respectively is constant for solutions and moreover does neither depend on the choice of chart, nor of the domain/image of the local solution $g$. Any hints, how to do this in any of this cases? Edit 2: As Hubert Goldschmidt points out in his 1967 article Integrability criteria for systems of nonlinear partial differential equations , it suffices that the bundle morphism defining a differential operator has locally constant rank. That means in the above discussion, it suffices to show, that $F$ etc. have constant rank for all pairs of charts $((x,U),(y,V))$. Second he points out, that there are PDEs (necessaryly nonlinear), that can't be written in such a way. But I doubt, that the above examples are of this kind.","I struggle a bit with understanding the idea behind the definition of a PDE on a fibred manifold. Let $\pi: E \to M$ be a smooth locally trivial fibre bundle. In Gromovs words a partial differential relation of order $k$ is a subset of the $k$th jet bundle $J^k(E)$. Usually one defines a  partial differential equation of order $k$ to be a closed submanifold of the $k$th jet bundle $J^k(E)$. This last definition  brings in some kind of regularity condition. I do not really understand why one wants/needs this regularity. What is the advantage of asking a PDE to be a submanifold in lieu of beeing merely a subset of $J^k(E)$? Let me explain a bit further:  (First, remark that this is somehow a follow up for my earlier question: Why is a differential equation a submanifold of a jet bundle? ) Often a partial differntial relation comes from a differential operator. Let $\rho: H \to M$ be another fibred manifold. A differential operator of order $k$ is a map $D_f: \Gamma_{loc}(\pi) \to \Gamma_{loc}(\rho)$ between local sections, for which there exists a bundle morphism $f: J^k(E) \to H$ such that for every local section $\phi \in \Gamma_{loc}(\pi)$ the equality $D_f (\phi) (p) = f(j^k_p \phi)$ holds for all $p$ in the domain of $\phi$. Now the preimage of a given section $\eta$ of $\rho$ under $f$ is clearly a partial differential relation, but for it to be a partial differntial equation we need to ask $df$ to have constant rank. This previous paragraph is a bit opposed to my naiv interpretation of a PDE in $R^n$ to be an arbitrary equation in the indpendent variable, the funtion itself and it's partial derivatives. Just in the sense of wikipedia: A partial differential equation looks like $F(x_1, \dots, x_n, u(x), \frac{\partial u}{\partial x_1}, \dots ) = 0$ for an arbitrary function $F$ (with image, say, in $R$). Now, if we are looking for functions $u: R^n \to R^m$ we can set $E = R^n \times R^m$ with $\pi$ beeing the projection on the first factor and $H = R^n \times R$. $F$ is a bundle map, that implicitly also defines a differential operator $D_F$. So in the stricter sense of beeing a submanifold, $F$ defines only a partial differential equation, if it hat constant rank, otherwise it is ""merely"" a partial differential relation. Edit: Example : Why is such a thing as $\frac{\partial u}{\partial x} \frac{\partial u}{\partial y} = 0$ not a pde (in the narrower sense)? In this case $(E, M, \pi) = (R^2 \times R, R^2, pr_2)$ with coordinates $(x_1, x_2, y)$. The first jet bundle $J^1(E)$ has coordinates $(x_1, x_2, y, p_1, p_2)$. The partial differential relation $S = \lbrace (x_1, x_2, y, p_1, p_2) \, | \, p_1 p_2 = 0 \rbrace$ describes the equation I started with and is clearly not a smooth submanifold. Why is this gap between the naiv real analysis point of view and the somewhat more elaboratet differential geometric point of view? Last I'm curious if there is a nice way to see, that a specific relation is in deed a partial differential equation. For example the equality $\Phi^* g = g$ specifying local diffeomorphisms that are isometries with respect to a pseudo-Riemannian metric $g$ is often said to be a partial differential equation, without giving an argument. And in parts I agree: this really feels like a partial differential equation. But to show that it defines in deed a submanifold needs some works, as can be seen in my earlier question Why is $\phi^* g = g$ a PDE for a pseudo-Riemannian metric $g$ on a manifold? , which got a very nice answer. Other examples that are often said to be defined by partial differential equations are locally defined affine maps with respect to an arbitrary connection $\nabla$ in $TM$ or locally defined diffeomorphisms that preserve a given tensor field $T$ or even those affine diffeomorphisms that in addtion preserve a given tensor field. Maybe it would be helpful to see an argument in coordinates in one of this cases, to see how this intuitivly obvious statements can be made rigorous. Edit : To start with the ""coorinate argument"" I asked for in the last paragraph, set $E=M \times M$ and $\pi$ the projection to the first factor and we can restrict ourselfes to the open submanifold of $J^1(E)$, consisting of 1-jets of local diffeomorphisms. Note that for $(x,U)$ a chart arount $p$ and $(y, V)$ a chart arount $f(p)$, a chart arount $j^1_p f$ is given by $\phi_{xy}: j^1_q g \to (x(q), y(g(q)), d(y \circ g \circ x^{-1})_{x(q)}) \in x(U) \times y(V) \times GL(\Bbb R^n)$. Remark, that for $\phi_{xy}(j^1_q g) = (\xi, \eta, A)$ and $\Phi (\mu) := A(\mu) + \eta - A(\xi)$ the function $y^{-1} \circ \Phi \circ x$ (restricted to a suitable subset) is a representative of the equivalence class $j^1_q g$. Now we can pullback the connection $\nabla$ and/or the tensorfield $T$ to give connections/tensor fields $\nabla^x, \nabla^y, T^x, T^y$ on $x(U)$, $y(v)$ via $x^{-1}, y^{-1}$ respectively. The property to preserve the connection/tensor field spells out in coordinates as \begin{align*} F(\xi, \eta, A) &= (\Phi^* T^y)_{\xi} - T^x_{\xi} = 0 \\ G(\xi, \eta, A) &= (\Phi^* \nabla^y)_{\xi} - \nabla^x_{\xi} = 0 \end{align*} (The first term of $F$ resp. $G$ depends only on $\eta$ and $A$, whereas the second term depends only on $\xi$.) This can be translatet into equations for the components of $T^x$ and $T^y$ resp. the Christoffel symbols of $\nabla^x, \nabla^y$ with respect to the standard coordniate frame of $\Bbb R^n$. So far so good. But now I need to proove, that the rank of the differentials of $F, G, (F,G)$ respectively is constant for solutions and moreover does neither depend on the choice of chart, nor of the domain/image of the local solution $g$. Any hints, how to do this in any of this cases? Edit 2: As Hubert Goldschmidt points out in his 1967 article Integrability criteria for systems of nonlinear partial differential equations , it suffices that the bundle morphism defining a differential operator has locally constant rank. That means in the above discussion, it suffices to show, that $F$ etc. have constant rank for all pairs of charts $((x,U),(y,V))$. Second he points out, that there are PDEs (necessaryly nonlinear), that can't be written in such a way. But I doubt, that the above examples are of this kind.",,"['differential-geometry', 'partial-differential-equations', 'fiber-bundles', 'jet-bundles']"
65,Is there an easy way to show which spheres can be Lie groups?,Is there an easy way to show which spheres can be Lie groups?,,"I heard that using some relatively basic differential geometry, you can show that the only spheres which are Lie groups are $S^0$, $S^1$, and $S^3$.  My friend who told me this thought that it involved de Rham cohomology, but I don't really know anything about the cohomology of Lie groups so this doesn't help me much.  Presumably there are some pretty strict conditions we can get from talking about invariant differential forms -- if you can tell me anything about this it will be a much-appreciated bonus :) (A necessary condition for a manifold to be a Lie group is that is must be parallelizable, since any Lie group is parallelized (?) by the left-invariant vector fields generated by a basis of the Lie algebra.  Which happens to mean, by some pretty fancy tricks, that the only spheres that even have a chance are the ones listed above plus $S^7$.  The usual parallelization of this last one comes from viewing it as the set of unit octonions, which don't form a group since their multiplication isn't associative; of course this doesn't immediately preclude $S^7$ from admitting the structure of a Lie group.  Whatever.  I'd like to avoid having to appeal to this whole parallelizability business, if possible.)","I heard that using some relatively basic differential geometry, you can show that the only spheres which are Lie groups are $S^0$, $S^1$, and $S^3$.  My friend who told me this thought that it involved de Rham cohomology, but I don't really know anything about the cohomology of Lie groups so this doesn't help me much.  Presumably there are some pretty strict conditions we can get from talking about invariant differential forms -- if you can tell me anything about this it will be a much-appreciated bonus :) (A necessary condition for a manifold to be a Lie group is that is must be parallelizable, since any Lie group is parallelized (?) by the left-invariant vector fields generated by a basis of the Lie algebra.  Which happens to mean, by some pretty fancy tricks, that the only spheres that even have a chance are the ones listed above plus $S^7$.  The usual parallelization of this last one comes from viewing it as the set of unit octonions, which don't form a group since their multiplication isn't associative; of course this doesn't immediately preclude $S^7$ from admitting the structure of a Lie group.  Whatever.  I'd like to avoid having to appeal to this whole parallelizability business, if possible.)",,"['differential-geometry', 'differential-topology', 'lie-groups']"
66,Is there any easy way to understand the definition of Gaussian Curvature?,Is there any easy way to understand the definition of Gaussian Curvature?,,I am new to differential geometry and I am trying to understand Gaussian curvature. The definitions found at Wikipedia and Wolfram sites are too mathematical. Is there any intuitive way to understand Gaussian curvature?,I am new to differential geometry and I am trying to understand Gaussian curvature. The definitions found at Wikipedia and Wolfram sites are too mathematical. Is there any intuitive way to understand Gaussian curvature?,,"['differential-geometry', 'curvature']"
67,Introductory texts on manifolds,Introductory texts on manifolds,,"I was studying some hyperbolic geometry previously and realised that I needed to understand things in a more general setting in terms of a ""manifold"" which I don't yet know of. I was wondering if someone can recommend to me some introductory texts on manifolds, suitable for those that have some background on analysis and several variable calculus. A lecturer recommended to me ""Analysis on Real and Complex Manifolds"" by R. Narasimhan, but it is too advanced. I had a look at Loring W. Tu's text on manifolds and it seemed accessible.","I was studying some hyperbolic geometry previously and realised that I needed to understand things in a more general setting in terms of a ""manifold"" which I don't yet know of. I was wondering if someone can recommend to me some introductory texts on manifolds, suitable for those that have some background on analysis and several variable calculus. A lecturer recommended to me ""Analysis on Real and Complex Manifolds"" by R. Narasimhan, but it is too advanced. I had a look at Loring W. Tu's text on manifolds and it seemed accessible.",,['differential-geometry']
68,Geometric understanding of differential forms.,Geometric understanding of differential forms.,,"I would like to understand differential forms more intuitively. I have yet to find a book which explains how the use of the exterior product in differential forms ties into the geometrical significance of it. Most books briefly introduce the exterior algebra and then go on to prove theorems about differential forms without ever geometrically motivating anything, and this does not help me to truly understand them. Could someone explain or give a reference to a text explaining differential forms more geometrically. I would very much like to have a deep understanding of them. Thanks very much","I would like to understand differential forms more intuitively. I have yet to find a book which explains how the use of the exterior product in differential forms ties into the geometrical significance of it. Most books briefly introduce the exterior algebra and then go on to prove theorems about differential forms without ever geometrically motivating anything, and this does not help me to truly understand them. Could someone explain or give a reference to a text explaining differential forms more geometrically. I would very much like to have a deep understanding of them. Thanks very much",,"['differential-geometry', 'reference-request', 'book-recommendation', 'differential-forms']"
69,Understanding the Laplace operator conceptually,Understanding the Laplace operator conceptually,,"The Laplace operator: those of you who now understand it, how would you explain what it ""does"" conceptually? How do you wish you had been taught it? Any good essays (combining both history and conceptual understanding) on the Laplace operator, and its subsequent variations (e.g. Laplace-Bertrami) that you would highly recommend?","The Laplace operator: those of you who now understand it, how would you explain what it ""does"" conceptually? How do you wish you had been taught it? Any good essays (combining both history and conceptual understanding) on the Laplace operator, and its subsequent variations (e.g. Laplace-Bertrami) that you would highly recommend?",,"['differential-geometry', 'reference-request', 'soft-question', 'laplacian', 'big-picture']"
70,How to calculate the pullback of a $k$-form explicitly,How to calculate the pullback of a -form explicitly,k,"I'm having trouble doing actual computations of the pullback of a $k$-form. I know that a given differentiable map $\alpha: \mathbb{R}^{m} \rightarrow \mathbb{R}^{n}$ induces a map $\alpha^{*}: \Omega^{k}(\mathbb{R}^{n}) \rightarrow \Omega^{k}(\mathbb{R}^{m})$ between $k$-forms. I can recite how this map is defined and understand why it is well defined, but when I'm given a particular $\alpha$ and a particular $\omega \in \Omega^{k}(\mathbb{R}^{n})$, I cannot compute $\alpha^{*}\omega$. For example I found an exercise ( Analysis on Manifolds , by Munkres) where $\omega = xy \, dx + 2z \, dy - y \, dz\in \Omega^{k}(\mathbb{R}^{3})$ and $\alpha: \mathbb{R}^{2} \rightarrow \mathbb{R}^{3}$ is defined as $\alpha(u, v) = (uv, u^{2}, 3u + v)$, but I got lost wile expanding the definition of $\alpha^{*} \omega$. How can I calculate this? Note: This exercise is not a homework, so feel free to illustrate the process with any $\alpha$ and $\omega$ you wish.","I'm having trouble doing actual computations of the pullback of a $k$-form. I know that a given differentiable map $\alpha: \mathbb{R}^{m} \rightarrow \mathbb{R}^{n}$ induces a map $\alpha^{*}: \Omega^{k}(\mathbb{R}^{n}) \rightarrow \Omega^{k}(\mathbb{R}^{m})$ between $k$-forms. I can recite how this map is defined and understand why it is well defined, but when I'm given a particular $\alpha$ and a particular $\omega \in \Omega^{k}(\mathbb{R}^{n})$, I cannot compute $\alpha^{*}\omega$. For example I found an exercise ( Analysis on Manifolds , by Munkres) where $\omega = xy \, dx + 2z \, dy - y \, dz\in \Omega^{k}(\mathbb{R}^{3})$ and $\alpha: \mathbb{R}^{2} \rightarrow \mathbb{R}^{3}$ is defined as $\alpha(u, v) = (uv, u^{2}, 3u + v)$, but I got lost wile expanding the definition of $\alpha^{*} \omega$. How can I calculate this? Note: This exercise is not a homework, so feel free to illustrate the process with any $\alpha$ and $\omega$ you wish.",,"['differential-geometry', 'differential-forms']"
71,Why is the Laplacian important in Riemannian geometry?,Why is the Laplacian important in Riemannian geometry?,,"As I've learned more Riemannian geometry, many of my teachers have said that studying the Laplacian (and its eigenvalues) is very important.  But I must admit, I've never fully understood why. Fundamentally, I would like to know why the Laplacian is important among all differential operators on a Riemannian manifold.  I would also like to know what geometric information the Laplacian is supposed to encode. That being said, I have spent a little time thinking about all this, and my current understanding is as follows: I've heard that the Laplacian is the ""simplest"" isometrically-invariant ""scalar differential operator"" on a Riemannian manifold.  If true, this statement would convince me of its importance. However, I don't know to what extent this is true. An isometric immersion $f \colon S \to M$ is harmonic iff it is a minimal submanifold of $M$.  In particular, an isometrically immersed submanifold of $\mathbb{R}^n$ is minimal iff its coordinate functions are harmonic. The Euler-Lagrange equation for the Dirichlet energy is $\Delta f = 0$.  (But why we care about minimzing energy is also somewhat mysterious to me.) Weitzenböck formulas comparing two elliptic second-order differential operators (and especially Laplacians) give Bochner-type vanishing theorems. I should point out that I'm aware that harmonic functions satisfy many of the nice properties that complex-analytic functions do (by virtue of elliptic regularity and maximum principle magic).  Still, this doesn't quite tell me why I should care about the Laplace operator itself. Note: I'm aware of this related question on the eigenvalues of the Laplacian.  But again, my interest is in Riemannian geometry; matters of applied mathematics (while interesting) are not my focus right now.","As I've learned more Riemannian geometry, many of my teachers have said that studying the Laplacian (and its eigenvalues) is very important.  But I must admit, I've never fully understood why. Fundamentally, I would like to know why the Laplacian is important among all differential operators on a Riemannian manifold.  I would also like to know what geometric information the Laplacian is supposed to encode. That being said, I have spent a little time thinking about all this, and my current understanding is as follows: I've heard that the Laplacian is the ""simplest"" isometrically-invariant ""scalar differential operator"" on a Riemannian manifold.  If true, this statement would convince me of its importance. However, I don't know to what extent this is true. An isometric immersion $f \colon S \to M$ is harmonic iff it is a minimal submanifold of $M$.  In particular, an isometrically immersed submanifold of $\mathbb{R}^n$ is minimal iff its coordinate functions are harmonic. The Euler-Lagrange equation for the Dirichlet energy is $\Delta f = 0$.  (But why we care about minimzing energy is also somewhat mysterious to me.) Weitzenböck formulas comparing two elliptic second-order differential operators (and especially Laplacians) give Bochner-type vanishing theorems. I should point out that I'm aware that harmonic functions satisfy many of the nice properties that complex-analytic functions do (by virtue of elliptic regularity and maximum principle magic).  Still, this doesn't quite tell me why I should care about the Laplace operator itself. Note: I'm aware of this related question on the eigenvalues of the Laplacian.  But again, my interest is in Riemannian geometry; matters of applied mathematics (while interesting) are not my focus right now.",,"['differential-geometry', 'partial-differential-equations', 'soft-question', 'riemannian-geometry']"
72,Geometrical interpretation of Ricci curvature,Geometrical interpretation of Ricci curvature,,"I see the scalar curvature $R$ as an indicator of how a manifold curves locally (the easiest example is for a $2$-dimensional manifold $M$, where the $R=0$ in a point means that it is flat there, $R>0$ that it makes like a hill and $R<0$ that it is a saddle point). Are there analogous interpretations for the Riemann tensor $Rm$ and for the Ricci curvature $Rc$? I tried to think about it but I really can't get anything making sense.","I see the scalar curvature $R$ as an indicator of how a manifold curves locally (the easiest example is for a $2$-dimensional manifold $M$, where the $R=0$ in a point means that it is flat there, $R>0$ that it makes like a hill and $R<0$ that it is a saddle point). Are there analogous interpretations for the Riemann tensor $Rm$ and for the Ricci curvature $Rc$? I tried to think about it but I really can't get anything making sense.",,"['differential-geometry', 'riemannian-geometry', 'curvature']"
73,Why do differential forms have a much richer structure than vector fields?,Why do differential forms have a much richer structure than vector fields?,,"I apologize in advance because this question might be a bit philosophical, but I do think it is probably a genuine question with non-vacuous content. We know as a fact that differential forms have a much richer structure than vector fields, to name a few constructions that are built on forms but not on vectors, we have: (1)Exterior derivative, and hence Stokes theorem, de Rham cohomology and etc. (2)Integration. (3)Functorality, i.e. we can always pull back a differential form but we cannot always push forward a vector field. However, I feel it's somewhat paradoxical considering the fact that differential forms are defined to be the dual of vector fields, and this gives me the intuition that they should be almost ""symmetric"". Clearly this intuition is in fact far off the mark. But why? I mean, is there an at least heuristic argument to show, just by looking at the definition from scratch, that differential forms and vector fields must be very ""asymmetric""?","I apologize in advance because this question might be a bit philosophical, but I do think it is probably a genuine question with non-vacuous content. We know as a fact that differential forms have a much richer structure than vector fields, to name a few constructions that are built on forms but not on vectors, we have: (1)Exterior derivative, and hence Stokes theorem, de Rham cohomology and etc. (2)Integration. (3)Functorality, i.e. we can always pull back a differential form but we cannot always push forward a vector field. However, I feel it's somewhat paradoxical considering the fact that differential forms are defined to be the dual of vector fields, and this gives me the intuition that they should be almost ""symmetric"". Clearly this intuition is in fact far off the mark. But why? I mean, is there an at least heuristic argument to show, just by looking at the definition from scratch, that differential forms and vector fields must be very ""asymmetric""?",,"['differential-geometry', 'differential-forms']"
74,Are there simple examples of Riemannian manifolds with zero curvature and nonzero torsion,Are there simple examples of Riemannian manifolds with zero curvature and nonzero torsion,,"I am trying to grasp the Riemann curvature tensor, the torsion tensor and their relationship. In particular, I'm interested in necessary and sufficient conditions for local isometry with Euclidean space (I'm talking about isometry of an open set - not the tangent space - with Euclidean space) - and I'd especially like to grasp these tensors in terms of their measuring the failure of Euclid's parallel postulate in a particular manifold. In a Riemannian manifold $M$, we can choose the Levi-Civita connexion and null out the torsion. So then the curvature wholly determines whether or not we can find the Euclidean open set: we seek an open set wherein $R(X_p,Y_p)Z_p = 0; \forall X_p,Y_p, Z_p \in T_p M, \forall p \in U \subset M$ and we are done. Question 1. However, what happens to the curvature $R$ in the Riemannian case if we use other connexions with the same geodesic sprays but with different, nonzero torsions $T$? Question 2. Is there a formula showing how $R$ and $T$ in the case of connexions with the same geodesic sprays? Question 3. Can we still test the curvature alone as above to see whether there is a local Euclidean open set, or do we now need to make sure that torsion also vanishes in that open set too? Question 4. If we relax the Riemannian condition and talk abstractly about a connexion alone, which of the above answers change? and lastly: Question 5. I'd really like to have an example of a space with zero curvature but nonzero torsion over a whole, open set, not just at a point, if such a thing exists. I think this would help build intuition.","I am trying to grasp the Riemann curvature tensor, the torsion tensor and their relationship. In particular, I'm interested in necessary and sufficient conditions for local isometry with Euclidean space (I'm talking about isometry of an open set - not the tangent space - with Euclidean space) - and I'd especially like to grasp these tensors in terms of their measuring the failure of Euclid's parallel postulate in a particular manifold. In a Riemannian manifold $M$, we can choose the Levi-Civita connexion and null out the torsion. So then the curvature wholly determines whether or not we can find the Euclidean open set: we seek an open set wherein $R(X_p,Y_p)Z_p = 0; \forall X_p,Y_p, Z_p \in T_p M, \forall p \in U \subset M$ and we are done. Question 1. However, what happens to the curvature $R$ in the Riemannian case if we use other connexions with the same geodesic sprays but with different, nonzero torsions $T$? Question 2. Is there a formula showing how $R$ and $T$ in the case of connexions with the same geodesic sprays? Question 3. Can we still test the curvature alone as above to see whether there is a local Euclidean open set, or do we now need to make sure that torsion also vanishes in that open set too? Question 4. If we relax the Riemannian condition and talk abstractly about a connexion alone, which of the above answers change? and lastly: Question 5. I'd really like to have an example of a space with zero curvature but nonzero torsion over a whole, open set, not just at a point, if such a thing exists. I think this would help build intuition.",,"['differential-geometry', 'riemannian-geometry', 'curvature']"
75,Definitions of Hessian in Riemannian Geometry,Definitions of Hessian in Riemannian Geometry,,"I am wondering if there is any quick way to see the following two definitions of Hessian coincide with each other without using local coordinates. $\operatorname{Hess}(f)(X,Y)= \langle \nabla_X \operatorname{grad}f,Y \ \rangle$ ; and $\operatorname{Hess}(f)(X,Y)=X (Yf) - (\nabla_XY) f$ .",I am wondering if there is any quick way to see the following two definitions of Hessian coincide with each other without using local coordinates. ; and .,"\operatorname{Hess}(f)(X,Y)= \langle \nabla_X \operatorname{grad}f,Y \ \rangle \operatorname{Hess}(f)(X,Y)=X (Yf) - (\nabla_XY) f","['differential-geometry', 'riemannian-geometry']"
76,Which manifolds are parallelizable?,Which manifolds are parallelizable?,,"Recall that a manifold $M$ of dimension $n$ is parallelizable if there are $n$ vector fields that form a basis of the tangent space $T_x M$ at every point $x \in M$. This is equivalent to the tangent bundle $TM$ being trivial or the frame bundle $FM$ having a global section. I know of some conditions (both necessary and sufficient), as well as counter-examples, both of which I provided in an answer to this recent question on tangent bundles. But the results I am familiar with are rather disparate and I was wondering whether some more coherent theory is known. In the ideal case giving a set of necessary and/or sufficient conditions (e.g. in terms of cohomology groups) or at least completely characterizing some nice class of manifolds (like for Lie groups, which are parallelizable; or for compact manifolds with non-zero Euler characteristic, which are not).","Recall that a manifold $M$ of dimension $n$ is parallelizable if there are $n$ vector fields that form a basis of the tangent space $T_x M$ at every point $x \in M$. This is equivalent to the tangent bundle $TM$ being trivial or the frame bundle $FM$ having a global section. I know of some conditions (both necessary and sufficient), as well as counter-examples, both of which I provided in an answer to this recent question on tangent bundles. But the results I am familiar with are rather disparate and I was wondering whether some more coherent theory is known. In the ideal case giving a set of necessary and/or sufficient conditions (e.g. in terms of cohomology groups) or at least completely characterizing some nice class of manifolds (like for Lie groups, which are parallelizable; or for compact manifolds with non-zero Euler characteristic, which are not).",,"['algebraic-topology', 'differential-geometry', 'differential-topology']"
77,$C^{k}$-manifolds: how and why?,-manifolds: how and why?,C^{k},"First of all, I have a specific question. Suppose $M$ is an $m$ -dimensional $C^k$ -manifold, for $1 \leq k < \infty$ . Is the tangent space to a point defined as the space of $C^k$ derivations on the germs of $C^k$ functions near that point? If so, is it $m$ -dimensional? Bredon's book Topology and Geometry comments that (p.77) only in the $C^\infty$ case can one prove that every derivation is given by a tangent vector to a curve. If so, this would suggest that (if indeed given this definition), the tangent space to a $C^k$ -manifold would be bigger in the case $k < \infty$ . Additionally, out of curiosity, would anybody have an example of a derivation that is not a tangent vector to a curve? Secondly, it would seem to me that a fair share of the things I learned about smooth manifolds should fail or at least require more elaborate proofs in the $C^k$ case. We only used higher derivatives in proving Sard's theorem, but all the time we used the identification that the tangent space is  given by tangent vectors to curves; the tubular neighborhood theorem comes to mind. What are the standard facts of smooth manifolds that do fail in the $C^k$ case? Thirdly, are they really important? It seems a lot of books deal only with smooth manifolds, but a fair share also seem to deal with $C^k$ -manifolds; Hirsch's Differential Topology deals with them all throughout, and Duistermaat & Kolk's book Lie groups (p.1) defines them as $C^2$ -manifolds. Should I, as a student of topology / geometry, be paying close attention to $C^k$ -manifolds and the distinctions with the smooth case?","First of all, I have a specific question. Suppose is an -dimensional -manifold, for . Is the tangent space to a point defined as the space of derivations on the germs of functions near that point? If so, is it -dimensional? Bredon's book Topology and Geometry comments that (p.77) only in the case can one prove that every derivation is given by a tangent vector to a curve. If so, this would suggest that (if indeed given this definition), the tangent space to a -manifold would be bigger in the case . Additionally, out of curiosity, would anybody have an example of a derivation that is not a tangent vector to a curve? Secondly, it would seem to me that a fair share of the things I learned about smooth manifolds should fail or at least require more elaborate proofs in the case. We only used higher derivatives in proving Sard's theorem, but all the time we used the identification that the tangent space is  given by tangent vectors to curves; the tubular neighborhood theorem comes to mind. What are the standard facts of smooth manifolds that do fail in the case? Thirdly, are they really important? It seems a lot of books deal only with smooth manifolds, but a fair share also seem to deal with -manifolds; Hirsch's Differential Topology deals with them all throughout, and Duistermaat & Kolk's book Lie groups (p.1) defines them as -manifolds. Should I, as a student of topology / geometry, be paying close attention to -manifolds and the distinctions with the smooth case?",M m C^k 1 \leq k < \infty C^k C^k m C^\infty C^k k < \infty C^k C^k C^k C^2 C^k,"['differential-geometry', 'manifolds', 'differential-topology']"
78,Roadmap to study Atiyah–Singer index theorem,Roadmap to study Atiyah–Singer index theorem,,"I am a physics undergrad and want to pursue a PhD in maths (geometry or topology). I study it almost completely by myself, as the program in my country offers very less flexibility to take non departmental courses, thought I will be able to take them is a couple of semesters. Anyway, So I was thinking about taking a mini-project of sorts which I can self-study  or maybe ask a prof. I had heard about this theorem while studying topological solitons in physics. What are the prerequisites for studying the Atiyah–Singer theorem. I had a look online, but I couldn't figure out exactly, in what field this is, or what are the prerequisites. Wikipedia says it is a theorem in differential geometry, but obviously what differential geometry I know is insufficient. I know about manifolds, differential forms, lie derivatives, lie groups, and some killing vectors stuff. A look at the material online also talks about some operators in DG which I have never come across. Are there an algebraic topology prerequisites? Please could you recommend me some references which can take me there? Thanks in advance!","I am a physics undergrad and want to pursue a PhD in maths (geometry or topology). I study it almost completely by myself, as the program in my country offers very less flexibility to take non departmental courses, thought I will be able to take them is a couple of semesters. Anyway, So I was thinking about taking a mini-project of sorts which I can self-study  or maybe ask a prof. I had heard about this theorem while studying topological solitons in physics. What are the prerequisites for studying the Atiyah–Singer theorem. I had a look online, but I couldn't figure out exactly, in what field this is, or what are the prerequisites. Wikipedia says it is a theorem in differential geometry, but obviously what differential geometry I know is insufficient. I know about manifolds, differential forms, lie derivatives, lie groups, and some killing vectors stuff. A look at the material online also talks about some operators in DG which I have never come across. Are there an algebraic topology prerequisites? Please could you recommend me some references which can take me there? Thanks in advance!",,"['differential-geometry', 'algebraic-topology']"
79,Why are smooth manifolds defined to be paracompact?,Why are smooth manifolds defined to be paracompact?,,"The way I understand things, roughly speaking, the importance of smooth manifolds is that they form the category of topological spaces on which we can do calculus. The definition of smooth manifolds requires that they be paracompact . I've looked all over, but I haven't found a clean statement for how paracompactness is a necessary condition to do calculus. I understand that, by a theorem of Stone, every metric space is paracompact, but I'm not sure why we need global metrizability either. Question : In what sense is paracompactness exactly the right condition to impose on a topological manifold to allow us to do calculus on it? Is there some theorem of the form ""X has [some structure we strictly need in calculus] if and only if it is paracompact""?","The way I understand things, roughly speaking, the importance of smooth manifolds is that they form the category of topological spaces on which we can do calculus. The definition of smooth manifolds requires that they be paracompact . I've looked all over, but I haven't found a clean statement for how paracompactness is a necessary condition to do calculus. I understand that, by a theorem of Stone, every metric space is paracompact, but I'm not sure why we need global metrizability either. Question : In what sense is paracompactness exactly the right condition to impose on a topological manifold to allow us to do calculus on it? Is there some theorem of the form ""X has [some structure we strictly need in calculus] if and only if it is paracompact""?",,"['differential-geometry', 'manifolds', 'big-picture']"
80,"I don't get the relationship between differentials, differential forms, and exterior derivatives.","I don't get the relationship between differentials, differential forms, and exterior derivatives.",,"I don't get the relationship between differentials, differential forms, and exterior derivatives. (Too many $d$'s getting me down!) Here are the relevant (partial) definitions from Wikipedia; essentially the same definitions/terminology/notations are to be found in my notes. Pushforward . Let $\varphi : M → N$ be a smooth map of smooth manifolds. Given some $x \in M$, the differential of $\varphi$ at $x$ is a linear map $d\varphi_x : T_x M \rightarrow T_{f(x)}N$... Differential form . Let $M$ be a smooth manifold. A differential form of degree $k$ is a smooth section of the $k$th exterior power of the cotangent bundle of $M$. At any point $p \in M$, a $k$-form $\beta$ defines an alternating multilinear map $\beta_p : T_p M \times \cdots \times T_p M \rightarrow \mathbb{R}$... Exterior derivative . The exterior derivative is defined to be the unique $\mathbb{R}$-linear mapping $f \mapsto df$ from $k$- forms to $(k + 1)$- forms satisfying the following properties... What I understand: You apply $d$ to differential $k$-forms to get differential $(k+1)$-forms. Implicitly, this means ""exterior derivative."" What I don't understand: If $\varphi : M \rightarrow N$ is a smooth map of smooth manifolds, in what sense, if at all, is the differential of $\varphi$ a differential form? Is there any reason not to just call this the pushforward and consistently denote it $\varphi_*$? If $f : M \rightarrow \mathbb{R}$ is a smooth map, does $df$ mean the differential of $f$, or does it mean the exterior derivative? Are these somehow miraculously the same? If so, why? It seems possible that they're the same, by identifying $T_x\mathbb{R}$ with $\mathbb{R}$. I don't understand the details. What, if anything, is the connection between the differential of a smooth mapping and the exterior derivative of a differential form?","I don't get the relationship between differentials, differential forms, and exterior derivatives. (Too many $d$'s getting me down!) Here are the relevant (partial) definitions from Wikipedia; essentially the same definitions/terminology/notations are to be found in my notes. Pushforward . Let $\varphi : M → N$ be a smooth map of smooth manifolds. Given some $x \in M$, the differential of $\varphi$ at $x$ is a linear map $d\varphi_x : T_x M \rightarrow T_{f(x)}N$... Differential form . Let $M$ be a smooth manifold. A differential form of degree $k$ is a smooth section of the $k$th exterior power of the cotangent bundle of $M$. At any point $p \in M$, a $k$-form $\beta$ defines an alternating multilinear map $\beta_p : T_p M \times \cdots \times T_p M \rightarrow \mathbb{R}$... Exterior derivative . The exterior derivative is defined to be the unique $\mathbb{R}$-linear mapping $f \mapsto df$ from $k$- forms to $(k + 1)$- forms satisfying the following properties... What I understand: You apply $d$ to differential $k$-forms to get differential $(k+1)$-forms. Implicitly, this means ""exterior derivative."" What I don't understand: If $\varphi : M \rightarrow N$ is a smooth map of smooth manifolds, in what sense, if at all, is the differential of $\varphi$ a differential form? Is there any reason not to just call this the pushforward and consistently denote it $\varphi_*$? If $f : M \rightarrow \mathbb{R}$ is a smooth map, does $df$ mean the differential of $f$, or does it mean the exterior derivative? Are these somehow miraculously the same? If so, why? It seems possible that they're the same, by identifying $T_x\mathbb{R}$ with $\mathbb{R}$. I don't understand the details. What, if anything, is the connection between the differential of a smooth mapping and the exterior derivative of a differential form?",,['differential-geometry']
81,What exactly is a Kähler Manifold?,What exactly is a Kähler Manifold?,,"Please scroll down to the bold subheaded section called Exact questions if you are too bored to read through the whole thing. I am a physics undergrad, trying to carry out some research work on topological solitons. I have been trying to read a paper that uses Kähler Manifolds. My guide just expects me to learn the mathematical definitions, without understanding it(or he expects me to study complex manifolds by scratch by myself), within 3-4 days, with exams going on, but I find this highly discomforting. So, it would be great if someone could tell me what is a Kähler manifold, highlighting the essential features of the definition and what they do, and intuitive explanations behind each. Why are they mathematically important? Also, some reasons as to why they are used in Physics? The definition on Wikipedia is very obscure, linking you to 7-8 pages, and you forget what you are actually looking for. I have read the following definition from Nakahara: A Kähler Manifold is an hermitian manifold, whose Kälher form is closed i.e. $d\Omega=0$. After searching the internet, I know the following: A Hermitian manifold is a complex manifold equipped with a metric $g$,   such that $g_p(X,Y)=g_p(J_pX,J_p Y)$, where $p \in M$ and $X,Y \in  T_pM$ Again the web tells me that, $J$ is a linear map between the tangent spaces at a point such that $J^2=-1$. Lastly, the Kähler form $\Omega$ is a tensor field whose action is given by $\Omega_p(X,Y)=(J_pX,Y)$. Exact questions: This is what I would really really want to understand. What is the meaning and motivation for $J^2=-1$? What is the intuitive meaning and motivation for the definition of the Hermitian manifold, and the Kähler form? Most importantly, what does the Kähler form is closed really mean? I am sorry for the long question, and would be delighted, even if I got a partial answer. Looking forward for the replies. I am not looking for exact arguments, but an intuitive overall picture. Background: I understand definitions of real manifolds, tangent spaces, and a differential forms. I have no intuition about exterior derivatives. I have a fair understanding of what is a complex manifold, and a few examples of Riemann surfaces.","Please scroll down to the bold subheaded section called Exact questions if you are too bored to read through the whole thing. I am a physics undergrad, trying to carry out some research work on topological solitons. I have been trying to read a paper that uses Kähler Manifolds. My guide just expects me to learn the mathematical definitions, without understanding it(or he expects me to study complex manifolds by scratch by myself), within 3-4 days, with exams going on, but I find this highly discomforting. So, it would be great if someone could tell me what is a Kähler manifold, highlighting the essential features of the definition and what they do, and intuitive explanations behind each. Why are they mathematically important? Also, some reasons as to why they are used in Physics? The definition on Wikipedia is very obscure, linking you to 7-8 pages, and you forget what you are actually looking for. I have read the following definition from Nakahara: A Kähler Manifold is an hermitian manifold, whose Kälher form is closed i.e. $d\Omega=0$. After searching the internet, I know the following: A Hermitian manifold is a complex manifold equipped with a metric $g$,   such that $g_p(X,Y)=g_p(J_pX,J_p Y)$, where $p \in M$ and $X,Y \in  T_pM$ Again the web tells me that, $J$ is a linear map between the tangent spaces at a point such that $J^2=-1$. Lastly, the Kähler form $\Omega$ is a tensor field whose action is given by $\Omega_p(X,Y)=(J_pX,Y)$. Exact questions: This is what I would really really want to understand. What is the meaning and motivation for $J^2=-1$? What is the intuitive meaning and motivation for the definition of the Hermitian manifold, and the Kähler form? Most importantly, what does the Kähler form is closed really mean? I am sorry for the long question, and would be delighted, even if I got a partial answer. Looking forward for the replies. I am not looking for exact arguments, but an intuitive overall picture. Background: I understand definitions of real manifolds, tangent spaces, and a differential forms. I have no intuition about exterior derivatives. I have a fair understanding of what is a complex manifold, and a few examples of Riemann surfaces.",,"['differential-geometry', 'differential-topology']"
82,Differential topology versus differential geometry,Differential topology versus differential geometry,,"I have just finished my undergraduate studies. During last two semesters I've taken two subjects dealing with manifolds: Analysis on manifolds, containing: definition of manifold, tangent space (as derivations and classes of curves), vector fields, vector bundles, flows, Lie derivatives, integration on manifolds (Stokes theorem), forms, Hodge decomposition theorem Introduction to differential geometry (for me it should be called introduction to Riemannian manifolds) containing: tensor calculus introduction, Riemannian manifold definition, connections, curvatures, geodesic, normal coordinates, geodesic completeness theorem, classification through curvature, Jacobi fields, harmonic maps. Now, for me differential geometry was/is a theory about manifolds, so anything dealing with manifolds is a branch of differential geometry. On the other hand, I am preparing for taking part in local conference called ""algebraic and differential topology"".  I have read: I- books references , II-  books references , III- wikipedia , yet I still don't really know if differential topology is subtheory of differential geometry or is it separate theory and how is it located between differential geometry and algebraic topology. For example I expect that studies on Riemannian manifolds are part of differential geometry but would problem of classification manifolds up to diffeomorphism be a part of differential topology or geometry? Request : I would be grateful for your characterisation of differential topology and differential geometry possibly with examples of problems, theorems present at them.","I have just finished my undergraduate studies. During last two semesters I've taken two subjects dealing with manifolds: Analysis on manifolds, containing: definition of manifold, tangent space (as derivations and classes of curves), vector fields, vector bundles, flows, Lie derivatives, integration on manifolds (Stokes theorem), forms, Hodge decomposition theorem Introduction to differential geometry (for me it should be called introduction to Riemannian manifolds) containing: tensor calculus introduction, Riemannian manifold definition, connections, curvatures, geodesic, normal coordinates, geodesic completeness theorem, classification through curvature, Jacobi fields, harmonic maps. Now, for me differential geometry was/is a theory about manifolds, so anything dealing with manifolds is a branch of differential geometry. On the other hand, I am preparing for taking part in local conference called ""algebraic and differential topology"".  I have read: I- books references , II-  books references , III- wikipedia , yet I still don't really know if differential topology is subtheory of differential geometry or is it separate theory and how is it located between differential geometry and algebraic topology. For example I expect that studies on Riemannian manifolds are part of differential geometry but would problem of classification manifolds up to diffeomorphism be a part of differential topology or geometry? Request : I would be grateful for your characterisation of differential topology and differential geometry possibly with examples of problems, theorems present at them.",,"['differential-geometry', 'soft-question', 'differential-topology']"
83,A proof of the Isoperimetric Inequality - how does it work?,A proof of the Isoperimetric Inequality - how does it work?,,"Here is a nice proof of the isoperimetric inequality (equality part ommited): Isoperimetric Inequality If $\gamma$ is any simple closed piecewise $C^1$ curve of length $l$, with it's interior having area $A$, then $4\pi A \le l^2$. Furthermore, if equality holds then $\gamma$ is a circle. Proof. Take two parallel straight lines $L$ and $L'$ such that $\gamma$ is between them and move them together until they first touch the curve. See my nice picture below. Let C be a circle as in the picture. Take $x$ and $y$ axes as shown. Let $\gamma = (x,y)$ be a parametrization of $\gamma$. Pick points $\gamma (s_0)$ and $\gamma (s_1)$ on both $L$ and $L'$ wherever the lines touch the curve, respectively. Let $C$ be parametrized by $(x, \overline{y})$ where $$ \overline{y}(s) = \begin{cases}  + \sqrt{r^2 - x^2 (s)}, & \text{if } s_0 \le s \le s_1 \\  - \sqrt{r^2 - x^2 (s)}, & \text{if } s_1 \le s \le s_0 + l \end{cases} $$ Denote the derivative of $f$ with respect to $s$ as $f_s$. Using Green's Theorem, we write: $$A + \pi r^2 = \int_{\gamma} x\,dy + \int_C -y\,dx = \int^l_0 x(s)y_s(s)\,ds - \int^l_0 \overline{y}(s)x_s(s)\,ds = $$ $$ = \int^l_0 ( x(s)y_s(s) - \overline{y}(s)x_s(s)) \,ds \le \int^l_0 \sqrt{ (x(s)y_s(s) - \overline{y}(s)x_s(s))^2} \,ds \stackrel{*}{\le}$$ $$ \stackrel{*}{\le} \int^l_0 \sqrt{ (x^2(s) + \overline{y}^2(s))} \,ds = lr$$ Where the starred inequality follows from the fact that: $$(x y_s - \overline{y} x_s)^2 = [(x, - \overline{y}) \cdot (y_s, x_s)]^2 \le (x^2 + \overline{y}^2) \cdot (y^2_s + x^2_s) = x^2 + \overline{y}^2 $$ So we have that $A + \pi r^2 \le lr$. Next we employ the Geometric-Arithmetic Mean Inequality to find that: $$\sqrt{A \pi r^2} \le \dfrac{A + \pi r^2}{2} \le \dfrac{lr}{2}$$ From which it directly follows that $4 \pi A \le l^2$, as needed. $\square$ I have seen other proofs of this Theorem, for example the one using Wirtinger's Inequality. This proof was presented to me by my professor, who said this it was rather mysterious , and I agree. I think this proof is rather beautiful and much simpler than the other proofs. Here are my questions: How? How does it work? I do not mean to ask how to we get from one step to another. I mean to ask what makes this work intrinsically. In particular, I am bothered by this constructed circle, and its radius. The next picture is what I have in mind: For this curve $\gamma$, choosing two different pairs of lines $L$, $L'$ and $K$, $K'$ gives us two circles with different radii. Furthermore, note that it appears that the area of the smaller circle is less that the area traced out by $\gamma$, whereas it is not the case for the larger circle. I guess my question here can be rephrased as: why do the $r$'s magically fall out of the equation in the last step? I am not looking for an anwser of the type ""because the math works out that way,"" rather some geometric insight/explanation. One observation I have thought about is that in case of equality $4 \pi A = l^2$, i.e. when $\gamma$ is a circle, any circle given by our construction will always have equall radius, in fact the radius of the circle $\gamma$. From that I was led to this question: Suppose we take $n$ pairs of parallel lines (where two distinct pairs of pairs are not mutually parallel), and construct circles for each. Now, as $n \rightarrow \infty$, what will happen to the average radius of these circles? What will a circle with this radius represent? EDIT: I have found an example where this last question turns out to be rather uninteresting. However, what will happen if we assume the curve to be convex as well? I do not know how to explore this last question with my knowledge whatsoever. Finally, I want to ask if anyone knows how this proof came to be; what is the hidden motivation. Thank you.","Here is a nice proof of the isoperimetric inequality (equality part ommited): Isoperimetric Inequality If $\gamma$ is any simple closed piecewise $C^1$ curve of length $l$, with it's interior having area $A$, then $4\pi A \le l^2$. Furthermore, if equality holds then $\gamma$ is a circle. Proof. Take two parallel straight lines $L$ and $L'$ such that $\gamma$ is between them and move them together until they first touch the curve. See my nice picture below. Let C be a circle as in the picture. Take $x$ and $y$ axes as shown. Let $\gamma = (x,y)$ be a parametrization of $\gamma$. Pick points $\gamma (s_0)$ and $\gamma (s_1)$ on both $L$ and $L'$ wherever the lines touch the curve, respectively. Let $C$ be parametrized by $(x, \overline{y})$ where $$ \overline{y}(s) = \begin{cases}  + \sqrt{r^2 - x^2 (s)}, & \text{if } s_0 \le s \le s_1 \\  - \sqrt{r^2 - x^2 (s)}, & \text{if } s_1 \le s \le s_0 + l \end{cases} $$ Denote the derivative of $f$ with respect to $s$ as $f_s$. Using Green's Theorem, we write: $$A + \pi r^2 = \int_{\gamma} x\,dy + \int_C -y\,dx = \int^l_0 x(s)y_s(s)\,ds - \int^l_0 \overline{y}(s)x_s(s)\,ds = $$ $$ = \int^l_0 ( x(s)y_s(s) - \overline{y}(s)x_s(s)) \,ds \le \int^l_0 \sqrt{ (x(s)y_s(s) - \overline{y}(s)x_s(s))^2} \,ds \stackrel{*}{\le}$$ $$ \stackrel{*}{\le} \int^l_0 \sqrt{ (x^2(s) + \overline{y}^2(s))} \,ds = lr$$ Where the starred inequality follows from the fact that: $$(x y_s - \overline{y} x_s)^2 = [(x, - \overline{y}) \cdot (y_s, x_s)]^2 \le (x^2 + \overline{y}^2) \cdot (y^2_s + x^2_s) = x^2 + \overline{y}^2 $$ So we have that $A + \pi r^2 \le lr$. Next we employ the Geometric-Arithmetic Mean Inequality to find that: $$\sqrt{A \pi r^2} \le \dfrac{A + \pi r^2}{2} \le \dfrac{lr}{2}$$ From which it directly follows that $4 \pi A \le l^2$, as needed. $\square$ I have seen other proofs of this Theorem, for example the one using Wirtinger's Inequality. This proof was presented to me by my professor, who said this it was rather mysterious , and I agree. I think this proof is rather beautiful and much simpler than the other proofs. Here are my questions: How? How does it work? I do not mean to ask how to we get from one step to another. I mean to ask what makes this work intrinsically. In particular, I am bothered by this constructed circle, and its radius. The next picture is what I have in mind: For this curve $\gamma$, choosing two different pairs of lines $L$, $L'$ and $K$, $K'$ gives us two circles with different radii. Furthermore, note that it appears that the area of the smaller circle is less that the area traced out by $\gamma$, whereas it is not the case for the larger circle. I guess my question here can be rephrased as: why do the $r$'s magically fall out of the equation in the last step? I am not looking for an anwser of the type ""because the math works out that way,"" rather some geometric insight/explanation. One observation I have thought about is that in case of equality $4 \pi A = l^2$, i.e. when $\gamma$ is a circle, any circle given by our construction will always have equall radius, in fact the radius of the circle $\gamma$. From that I was led to this question: Suppose we take $n$ pairs of parallel lines (where two distinct pairs of pairs are not mutually parallel), and construct circles for each. Now, as $n \rightarrow \infty$, what will happen to the average radius of these circles? What will a circle with this radius represent? EDIT: I have found an example where this last question turns out to be rather uninteresting. However, what will happen if we assume the curve to be convex as well? I do not know how to explore this last question with my knowledge whatsoever. Finally, I want to ask if anyone knows how this proof came to be; what is the hidden motivation. Thank you.",,"['differential-geometry', 'inequality', 'intuition']"
84,"""Immediate"" Applications of Differential Geometry","""Immediate"" Applications of Differential Geometry",,"My professor asked us to find and make a list of things/facts from real life which have a differential geometry interpretation or justification. One example is this older question of mine. Another example my teacher presented is proving that on a soccer ball which is made of regular pentagons and regular hexagons, the number of pentagons is fixed, as a consequence of Euler's polyhedral formula. I guess that there are many more of these. The idea is to find things/facts whose explanation is a theorem in differential geometry and eventually give a reference to a book/article where these connections are explained. My teacher wants us to make such a list and then each to pick a subject and make a project which presents the theorem which is applied (with proof, if the proof is not too long) and then present the application itself. Any reference or book on the subject is more than welcome.","My professor asked us to find and make a list of things/facts from real life which have a differential geometry interpretation or justification. One example is this older question of mine. Another example my teacher presented is proving that on a soccer ball which is made of regular pentagons and regular hexagons, the number of pentagons is fixed, as a consequence of Euler's polyhedral formula. I guess that there are many more of these. The idea is to find things/facts whose explanation is a theorem in differential geometry and eventually give a reference to a book/article where these connections are explained. My teacher wants us to make such a list and then each to pick a subject and make a project which presents the theorem which is applied (with proof, if the proof is not too long) and then present the application itself. Any reference or book on the subject is more than welcome.",,"['reference-request', 'differential-geometry', 'big-list', 'differential-topology']"
85,Are diffeomorphic smooth manifolds truly equivalent?,Are diffeomorphic smooth manifolds truly equivalent?,,"It seems to be an often repeated, ""folklore-ish"" statement, that diffeomorphism is an equivalence relation on smooth manifolds, and two smooth manifolds that are diffeomorphic are indistinguishable in terms of their smooth atlases. There is a strange counter example in Lee's Introduction to Smooth Manifolds though, let us define two smooth manifolds modelled on the real line. Let $\mathcal A$ be a smooth maximal atlas on $\mathbb{R}$ that is generated by the global chart $\varphi:\mathbb{R}\rightarrow\mathbb{R}$, $\varphi(x)=x$, and let $\bar{\mathcal A}$ be the maximal smooth atlas on $\mathbb{R}$ generated by the global chart $\bar{\varphi}(x)=x^3$. The transition function $\varphi\circ\bar{\varphi}^{-1}$ is not smooth, so these two smooth structures are incompatible. However the map $F:(\mathbb{R},\mathcal A)\rightarrow(\mathbb{R},\bar{\mathcal{A}})$ given by $F(x)=x^{1/3}$ is a diffeo, because $$ (\bar{\varphi}\circ F\circ \varphi^{-1})(x)=x, $$ and this map is smooth. So the smooth manifolds $(\mathbb{R},\mathcal A)$ and $(\mathbb{R},\bar{\mathcal{A}})$ are diffeomorphic. Yet the two manifolds have incompatible, thus, different smooth structures. Question: I guess I don't have a clear question, I am just somewhat confused. Because this is a counterexample , it seems to prove that the statement ""two diffeomorphic manifolds cannot be told apart by their smooth structures"" is wrong. However how wrong it is? Can we consider the two manifolds given in this example equivalent? Is there any practical difference between the two? Is differential geometry the same on them?","It seems to be an often repeated, ""folklore-ish"" statement, that diffeomorphism is an equivalence relation on smooth manifolds, and two smooth manifolds that are diffeomorphic are indistinguishable in terms of their smooth atlases. There is a strange counter example in Lee's Introduction to Smooth Manifolds though, let us define two smooth manifolds modelled on the real line. Let $\mathcal A$ be a smooth maximal atlas on $\mathbb{R}$ that is generated by the global chart $\varphi:\mathbb{R}\rightarrow\mathbb{R}$, $\varphi(x)=x$, and let $\bar{\mathcal A}$ be the maximal smooth atlas on $\mathbb{R}$ generated by the global chart $\bar{\varphi}(x)=x^3$. The transition function $\varphi\circ\bar{\varphi}^{-1}$ is not smooth, so these two smooth structures are incompatible. However the map $F:(\mathbb{R},\mathcal A)\rightarrow(\mathbb{R},\bar{\mathcal{A}})$ given by $F(x)=x^{1/3}$ is a diffeo, because $$ (\bar{\varphi}\circ F\circ \varphi^{-1})(x)=x, $$ and this map is smooth. So the smooth manifolds $(\mathbb{R},\mathcal A)$ and $(\mathbb{R},\bar{\mathcal{A}})$ are diffeomorphic. Yet the two manifolds have incompatible, thus, different smooth structures. Question: I guess I don't have a clear question, I am just somewhat confused. Because this is a counterexample , it seems to prove that the statement ""two diffeomorphic manifolds cannot be told apart by their smooth structures"" is wrong. However how wrong it is? Can we consider the two manifolds given in this example equivalent? Is there any practical difference between the two? Is differential geometry the same on them?",,"['differential-geometry', 'category-theory', 'differential-topology', 'smooth-manifolds']"
86,Why do people care about principal bundles?,Why do people care about principal bundles?,,"I've started to learn a little about principal bundles (in the smooth category) and while I see how notions like connections and curvature are abstracted from the setting of vector bundles and brought into the principal bundles world, I feel highly unmotivated regarding  to why this is done in the first place. So, How do principal bundles appear ""naturally""? Why did people started thinking about connections, curvature, parallel transport on principal bundles? Why would one want to put connections on them? Study their curvature? Even determine whether they are flat or not? Do we gain something by considering the frame bundle associated to a vector bundle and studying its connection and curvature and not working directly with the vector bundle itself? What kind of problems the machinery of principal bundles helps to solve? What notions does it clarifies? Feel free to provide examples of results or ideas from any field, including physics. Explicit geometric examples are especially welcomed.","I've started to learn a little about principal bundles (in the smooth category) and while I see how notions like connections and curvature are abstracted from the setting of vector bundles and brought into the principal bundles world, I feel highly unmotivated regarding  to why this is done in the first place. So, How do principal bundles appear ""naturally""? Why did people started thinking about connections, curvature, parallel transport on principal bundles? Why would one want to put connections on them? Study their curvature? Even determine whether they are flat or not? Do we gain something by considering the frame bundle associated to a vector bundle and studying its connection and curvature and not working directly with the vector bundle itself? What kind of problems the machinery of principal bundles helps to solve? What notions does it clarifies? Feel free to provide examples of results or ideas from any field, including physics. Explicit geometric examples are especially welcomed.",,"['differential-geometry', 'soft-question', 'differential-topology']"
87,Computing the Chern-Simons invariant of $SO(3)$,Computing the Chern-Simons invariant of,SO(3),"I am an undergraduate learning about gauge theory and I have been tasked with working through the two examples given on pages 65 and 66 of "" Characteristic forms and geometric invariants "" by Chern and Simon. I will recount the examples and my progress at a solution. For ease here is the relevant text: Example 1. Let $M = \mathbb{R}P^3 = SO(3)$ together with the standard metric of constant curvature 1. Let $E_1, E_2, E_3$ be an orthonormal basis of left invariant fields on $M$, oriented positively. Then it is easily seen that $\nabla_{E_1}E_2 = E_3, \nabla_{E_1}E_3 = - E_2, \text{ and } \nabla_{E_2}E_3 = E_1$. Let $\chi : M \rightarrow F(M)$ be the cross-section determined by this frame.   $$\Phi(SO(3)) = \frac{1}{2}.$$ Example 2. Again let $M = SO(3)$, but this time with left invariant metric $g_{\lambda}$, with respect to which $\lambda E_1, E_2, E_3$ is an orthonormal frame. Direct calculation shows   $$\Phi(SO(3),g_{\lambda}) = \frac{2\lambda^2 - 1}{2\lambda^4}.$$ For each of these examples I am expected to calculate $$\Phi(M) = \int_{\chi(M)} \frac{1}{2} TP_1(\theta)$$ which lies in $\mathbb{R}/\mathbb{Z}$. Previously in the paper they give an explicit formulation of $TP_1(\theta)$ in terms of the ""component"" forms of the connection $\theta$ and its curvature $\Omega$, $$TP_1(\theta) = \frac{1}{4\pi^2}\left( \theta_{12}\wedge\theta_{13}\wedge\theta_{23} + \theta_{12}\wedge\Omega_{12} + \theta_{13}\wedge\Omega_{13} + \theta_{23}\wedge\Omega_{23}\right).$$ I have verified this formula for myself given the information in the paper. Using the structural equation $\Omega = d\theta + \theta\wedge\theta$ I am able to reduce the expression for $TP_1(\theta)$ to $$TP_1(\theta) = \frac{-1}{2\pi^2}\left( \theta_{12}\wedge\theta_{13}\wedge\theta_{23} \right).$$ I don't believe I have assumed anything about the structure of $M$ during that reduction so I believe it should hold for both examples. I continue by claiming that since $E_1, E_2, E_3 \in so(3)$, the Lie algebra of $SO(3)$ I should be able to compute $\theta$ by considering $$\nabla_{E_i}E_j := (\nabla E_j)(E_i) = \sum_k E_k \otimes \theta^{k}{}_{ij}(E_i)$$ and comparing it with the given derivatives. For example one this yielded for me $\theta_{12} = E^3, \theta_{13} = -E^2, \theta_{23} = E^1$ where $E^i$ are the 1-forms dual to the basis $E_i$. Then I think that $\chi^*$ should act trivially on $TP_1(\theta)$ as it is a horizontal form in $\Lambda^*(T^*F(M))$. Therefore I find that $\chi^*(TP_1(\theta)) = \frac{1}{2\pi^2}\omega$, where $\omega$ is the volume form of $M$, and when integrated this yields the correct answer of $\frac{1}{2}$ for the first example. However, my approach fails completely for the second example. I assume that the set $\lambda E_1, E_2, E_3$ obeys the same derivate relationships as given in the first example, but this does not seem to give me enough factors of $\lambda$. I suspect that I am not handling the computation of the $\theta_{ij}$ forms or the application of $\chi^*$ correctly, however I am uncertain what my exact issue is. Is there a fundamental flaw in my understanding? I am hoping someone with more experience can point me in the right direction.","I am an undergraduate learning about gauge theory and I have been tasked with working through the two examples given on pages 65 and 66 of "" Characteristic forms and geometric invariants "" by Chern and Simon. I will recount the examples and my progress at a solution. For ease here is the relevant text: Example 1. Let $M = \mathbb{R}P^3 = SO(3)$ together with the standard metric of constant curvature 1. Let $E_1, E_2, E_3$ be an orthonormal basis of left invariant fields on $M$, oriented positively. Then it is easily seen that $\nabla_{E_1}E_2 = E_3, \nabla_{E_1}E_3 = - E_2, \text{ and } \nabla_{E_2}E_3 = E_1$. Let $\chi : M \rightarrow F(M)$ be the cross-section determined by this frame.   $$\Phi(SO(3)) = \frac{1}{2}.$$ Example 2. Again let $M = SO(3)$, but this time with left invariant metric $g_{\lambda}$, with respect to which $\lambda E_1, E_2, E_3$ is an orthonormal frame. Direct calculation shows   $$\Phi(SO(3),g_{\lambda}) = \frac{2\lambda^2 - 1}{2\lambda^4}.$$ For each of these examples I am expected to calculate $$\Phi(M) = \int_{\chi(M)} \frac{1}{2} TP_1(\theta)$$ which lies in $\mathbb{R}/\mathbb{Z}$. Previously in the paper they give an explicit formulation of $TP_1(\theta)$ in terms of the ""component"" forms of the connection $\theta$ and its curvature $\Omega$, $$TP_1(\theta) = \frac{1}{4\pi^2}\left( \theta_{12}\wedge\theta_{13}\wedge\theta_{23} + \theta_{12}\wedge\Omega_{12} + \theta_{13}\wedge\Omega_{13} + \theta_{23}\wedge\Omega_{23}\right).$$ I have verified this formula for myself given the information in the paper. Using the structural equation $\Omega = d\theta + \theta\wedge\theta$ I am able to reduce the expression for $TP_1(\theta)$ to $$TP_1(\theta) = \frac{-1}{2\pi^2}\left( \theta_{12}\wedge\theta_{13}\wedge\theta_{23} \right).$$ I don't believe I have assumed anything about the structure of $M$ during that reduction so I believe it should hold for both examples. I continue by claiming that since $E_1, E_2, E_3 \in so(3)$, the Lie algebra of $SO(3)$ I should be able to compute $\theta$ by considering $$\nabla_{E_i}E_j := (\nabla E_j)(E_i) = \sum_k E_k \otimes \theta^{k}{}_{ij}(E_i)$$ and comparing it with the given derivatives. For example one this yielded for me $\theta_{12} = E^3, \theta_{13} = -E^2, \theta_{23} = E^1$ where $E^i$ are the 1-forms dual to the basis $E_i$. Then I think that $\chi^*$ should act trivially on $TP_1(\theta)$ as it is a horizontal form in $\Lambda^*(T^*F(M))$. Therefore I find that $\chi^*(TP_1(\theta)) = \frac{1}{2\pi^2}\omega$, where $\omega$ is the volume form of $M$, and when integrated this yields the correct answer of $\frac{1}{2}$ for the first example. However, my approach fails completely for the second example. I assume that the set $\lambda E_1, E_2, E_3$ obeys the same derivate relationships as given in the first example, but this does not seem to give me enough factors of $\lambda$. I suspect that I am not handling the computation of the $\theta_{ij}$ forms or the application of $\chi^*$ correctly, however I am uncertain what my exact issue is. Is there a fundamental flaw in my understanding? I am hoping someone with more experience can point me in the right direction.",,['differential-geometry']
88,What is the metric tensor on the n-sphere (hypersphere)?,What is the metric tensor on the n-sphere (hypersphere)?,,"I am considering the unit sphere (but an extension to one of radius $r$ would be appreciated) centered at the origin. Any coordinate system will do, though the standard angular one (with 1 radial and $n-1$ angular coordinates) would be preferable. I know that on the 2-sphere we have $ds^2 = d\theta^2+\sin^2(\theta)d\phi^2$ (in spherical coordinates) but I'm not sure how this generalizes to $n$ dimensions. Added note: If anything can be discovered only about the determinant of the tensor (when presented in matrix form), that would also be quite helpful.","I am considering the unit sphere (but an extension to one of radius $r$ would be appreciated) centered at the origin. Any coordinate system will do, though the standard angular one (with 1 radial and $n-1$ angular coordinates) would be preferable. I know that on the 2-sphere we have $ds^2 = d\theta^2+\sin^2(\theta)d\phi^2$ (in spherical coordinates) but I'm not sure how this generalizes to $n$ dimensions. Added note: If anything can be discovered only about the determinant of the tensor (when presented in matrix form), that would also be quite helpful.",,"['differential-geometry', 'riemannian-geometry', 'tensors', 'spherical-geometry']"
89,"Is $M=\{(x,|x|): x \in (-1, 1)\}$ not a differentiable manifold?",Is  not a differentiable manifold?,"M=\{(x,|x|): x \in (-1, 1)\}","Let $M=\{(x,|x|): x \in (-1, 1)\}$. Then there is an atlas with only one coordinate chart $(M, (x, |x|) \mapsto x)$ for $M$. We don't need any coordinate transformation maps to worry about differentiablity. So I thought $M$ is a differentiable manifold. However my teacher says it is not. He says the sharp corner at $x = 0$ is a problem. I can't understand why it is a problem.","Let $M=\{(x,|x|): x \in (-1, 1)\}$. Then there is an atlas with only one coordinate chart $(M, (x, |x|) \mapsto x)$ for $M$. We don't need any coordinate transformation maps to worry about differentiablity. So I thought $M$ is a differentiable manifold. However my teacher says it is not. He says the sharp corner at $x = 0$ is a problem. I can't understand why it is a problem.",,"['differential-topology', 'manifolds', 'differential-geometry']"
90,What is magical about Cartan's magic formula?,What is magical about Cartan's magic formula?,,"Why is Cartan's magic formula $$\mathscr{L}_X\omega = i_Xd\omega + d(i_X\omega)$$ called ""magic""? Should it be considered a highly surprising result?  Does it ""magically"" prove several other theorems?  What is the etymology? (Why it is variously referred to as E.Cartan's formula and H.Cartan's formula?)","Why is Cartan's magic formula $$\mathscr{L}_X\omega = i_Xd\omega + d(i_X\omega)$$ called ""magic""? Should it be considered a highly surprising result?  Does it ""magically"" prove several other theorems?  What is the etymology? (Why it is variously referred to as E.Cartan's formula and H.Cartan's formula?)",,"['differential-geometry', 'soft-question', 'math-history']"
91,Origins of Differential Geometry and the Notion of Manifold,Origins of Differential Geometry and the Notion of Manifold,,"The title can potentially lend itself to a very broad discussion, so I'll try to narrow this post down to a few specific questions.  I've been studying differential geometry and manifold theory a couple of years now.  Over this time the notion of a manifold, as some object that locally looks Euclidean though globally may not be, has become a very comfortable notion for me.  However, the definition of a (smooth) manifold is quite abstract, and it's not particularly obvious from the outset why such an object would be of importance.  The modern definition as I've come to know it, which can be found in many textbooks ( John Lee's book, or this reprint of the definiton), I'm guessing is the byproduct of a long evolution of definitions.  It's my guess that many candidate definitions were adopted and discarded until we eventually settled on the one we use today. What is the origin of our modern definition of manifold?  As succinctly as possible, what was the evolution of this definition and   what was the original inspiration for defining such an object? What were the origins of some of the standard manifolds that are used in practice?  For instance, I'm certain that some of the objects   that gave impetus for the definition of a manifold consisted of $\mathbb{S}^2, SO(3)$, and   $\mathbb{T}^2$, but what are the origins of say the projective spaces   $\mathbb{RP}^n, \mathbb{CP}^n$, as well as the Grassmann and Stiefel   manifolds?  The role of manifolds in Hamiltonian mechanics? Are there any ""vestigial"" concepts that were at one time considered important but eventually discarded due to their   ineffectiveness, or ones that possibly yielded contradictory results? How did the evolution of topology as a subject intermingle with that of differential geometry? What are some good books tracing the history of differential geometry (that is, the evolution of the ideas)?  I know of a few math   history books, including Boyer's   book ,   but the parts about differential geometry/topology are left almost as   afterthoughts with the main text dealing with ancient civilizations   leading up to the calculus. This is a rather long question, and I don't expect anyone to answer each point in it's entirety.  Partial answers are welcome.","The title can potentially lend itself to a very broad discussion, so I'll try to narrow this post down to a few specific questions.  I've been studying differential geometry and manifold theory a couple of years now.  Over this time the notion of a manifold, as some object that locally looks Euclidean though globally may not be, has become a very comfortable notion for me.  However, the definition of a (smooth) manifold is quite abstract, and it's not particularly obvious from the outset why such an object would be of importance.  The modern definition as I've come to know it, which can be found in many textbooks ( John Lee's book, or this reprint of the definiton), I'm guessing is the byproduct of a long evolution of definitions.  It's my guess that many candidate definitions were adopted and discarded until we eventually settled on the one we use today. What is the origin of our modern definition of manifold?  As succinctly as possible, what was the evolution of this definition and   what was the original inspiration for defining such an object? What were the origins of some of the standard manifolds that are used in practice?  For instance, I'm certain that some of the objects   that gave impetus for the definition of a manifold consisted of $\mathbb{S}^2, SO(3)$, and   $\mathbb{T}^2$, but what are the origins of say the projective spaces   $\mathbb{RP}^n, \mathbb{CP}^n$, as well as the Grassmann and Stiefel   manifolds?  The role of manifolds in Hamiltonian mechanics? Are there any ""vestigial"" concepts that were at one time considered important but eventually discarded due to their   ineffectiveness, or ones that possibly yielded contradictory results? How did the evolution of topology as a subject intermingle with that of differential geometry? What are some good books tracing the history of differential geometry (that is, the evolution of the ideas)?  I know of a few math   history books, including Boyer's   book ,   but the parts about differential geometry/topology are left almost as   afterthoughts with the main text dealing with ancient civilizations   leading up to the calculus. This is a rather long question, and I don't expect anyone to answer each point in it's entirety.  Partial answers are welcome.",,"['differential-geometry', 'reference-request', 'soft-question', 'manifolds', 'smooth-manifolds']"
92,Elementary proof of the fact that any orientable 3-manifold is parallelizable,Elementary proof of the fact that any orientable 3-manifold is parallelizable,,"A parallelizable manifold $M$ is a smooth manifold such that there exist smooth vector fields $V_1,...,V_n$ where $n$ is the dimension of $M$, such that at any point $p\in M$, the tangent vectors $V_1(p),...,V_n(p)$ provide a basis for the tangent space at $p$. Equivalently, a manifold is parallelizable if its tangent bundle is trivial. There is a theorem that states that any compact orientable 3-manifold is parallelizable, and  there is a proof of this result which uses $spin^c$ structures and the Steifel-Whitney class. I am wondering whether there exists a more elementary, perhaps more straightforward proof. Otherwise, I would be grateful for some intuition on why this is true. Also, it the theorem still true without the compactness assumption? If so, is there a relatively simple proof in that case?","A parallelizable manifold $M$ is a smooth manifold such that there exist smooth vector fields $V_1,...,V_n$ where $n$ is the dimension of $M$, such that at any point $p\in M$, the tangent vectors $V_1(p),...,V_n(p)$ provide a basis for the tangent space at $p$. Equivalently, a manifold is parallelizable if its tangent bundle is trivial. There is a theorem that states that any compact orientable 3-manifold is parallelizable, and  there is a proof of this result which uses $spin^c$ structures and the Steifel-Whitney class. I am wondering whether there exists a more elementary, perhaps more straightforward proof. Otherwise, I would be grateful for some intuition on why this is true. Also, it the theorem still true without the compactness assumption? If so, is there a relatively simple proof in that case?",,"['differential-geometry', 'intuition', 'smooth-manifolds', 'fiber-bundles', 'characteristic-classes']"
93,Geometric meaning of symmetric connection,Geometric meaning of symmetric connection,,"If $(M, g)$ is Riemannian manifold, there is unique connection $\nabla$, called Levi-Civita connection, satisfying the following conditions: 1) Compatibility with Riemannian metric, i.e. $\nabla(g)$=0 2) Symmetricity, i.e. $\nabla_X(Y)-\nabla_Y(X)=[X, Y]$ While the former seems quite natural (equally, parallel transform of a vector along a curve does not change its length), what geometric intuition is hidden behind the later? The only idea I have, except for the fact it makes a compatible connection unique, is that it is in some way similar to the equality of partial derivates along commuting fields.","If $(M, g)$ is Riemannian manifold, there is unique connection $\nabla$, called Levi-Civita connection, satisfying the following conditions: 1) Compatibility with Riemannian metric, i.e. $\nabla(g)$=0 2) Symmetricity, i.e. $\nabla_X(Y)-\nabla_Y(X)=[X, Y]$ While the former seems quite natural (equally, parallel transform of a vector along a curve does not change its length), what geometric intuition is hidden behind the later? The only idea I have, except for the fact it makes a compatible connection unique, is that it is in some way similar to the equality of partial derivates along commuting fields.",,"['differential-geometry', 'riemannian-geometry', 'connections']"
94,"""the only odd dimensional spheres with a unique smooth structure are $S^1$, $S^3$, $S^5$, $S^{61}$""","""the only odd dimensional spheres with a unique smooth structure are , , , """,S^1 S^3 S^5 S^{61},"This (long) paper, Guozhen Wang, Zhouli Xu.   ""On the uniqueness of the smooth structure of the 61-sphere."" arXiv:1601.02184 [math.AT] . proves that the only odd dimensional spheres with a unique smooth structure are $S^1$, $S^3$, $S^5$, $S^{61}$. The new result is for $S^{61}$. Is it possible to give some intuition on this remarkable result, for those not steeped in algebraic and differential geometry, and so not intimately familiar with homotopy groups of spheres? Any attempt would be welcomed.","This (long) paper, Guozhen Wang, Zhouli Xu.   ""On the uniqueness of the smooth structure of the 61-sphere."" arXiv:1601.02184 [math.AT] . proves that the only odd dimensional spheres with a unique smooth structure are $S^1$, $S^3$, $S^5$, $S^{61}$. The new result is for $S^{61}$. Is it possible to give some intuition on this remarkable result, for those not steeped in algebraic and differential geometry, and so not intimately familiar with homotopy groups of spheres? Any attempt would be welcomed.",,"['differential-geometry', 'algebraic-topology', 'differential-topology']"
95,is there any good resource for video lectures of differential geometry?,is there any good resource for video lectures of differential geometry?,,I am wondering if there is some online resource for video lectures on the topic of differential geometry. Thanks a lot,I am wondering if there is some online resource for video lectures on the topic of differential geometry. Thanks a lot,,['differential-geometry']
96,Symmetric and wedge product in algebra and differential geometry,Symmetric and wedge product in algebra and differential geometry,,"Which is the correct identity? $dx \, dy = dx \otimes dy + dy \otimes dx$ $~~~$ or $~~~$ $dx \, dy = \dfrac{dx \otimes dy + dy \otimes dx}{2}~$ ? $dx \wedge dy=dx \otimes dy - dy \otimes dx$ $~~~$ or $~~~$ $dx \wedge dy=\dfrac{dx \otimes dy - dy \otimes dx}{2}~$ ? $$$$ Here is my understanding of the question from the point of view of: Linear algebra: Let $V$ be a vector space. The symmetric algebra $S(V)$ is a quotient of the tensor algebra $T(V)$ . The symmetric product $v \cdot w$ of elements of $V$ does not make sense a priori in $T(V)$ , but one can identify $S(V)$ with the space of symmetric tensors , which is a subspace of $T(V)$ where the restriction of the projection map $T(V) \to S(V)$ is an isomorphism. Under this isomorphism, the symmetric product $v \cdot w$ corresponds to the element $\dfrac{v \otimes w + w \otimes v}{2}$ of $T(V)$ . Same story for the exterior algebra $\Lambda(V)$ and alternating tensors: the wedge product $v \wedge w$ is identified with the alternating tensor $\dfrac{v \otimes w - w \otimes v}{2}$ . So contrary to what I have read in some places (e.g. accepted answer here ), in my opinion there is one natural way to identify symmetric products to symmetric tensors (resp. wedge products to alternating tensors) 1 . Conclusion: at least from the algebraic point of view, it seems to me that the natural thing to say is: $dx \, dy = \dfrac{dx \otimes dy + dy \otimes dx}{2}$ $dx \wedge dy = \dfrac{dx \otimes dy - dy \otimes dx}{2}$ Differential geometry : Again, I feel like there is only one choice we want to make here, contrary to what I have read sometimes: $dx \, dy = \dfrac{dx \otimes dy + dy \otimes dx}{2}$ , because $dxdx + dydy = dx^2 + dy^2 $ should be the standard metric (or inner product) on $\mathbb{R}^2$ (who would want $dx^2 + dy^2$ to mean something else?) $dx \wedge dy = dx \otimes dy - dy \otimes dx$ because $dx \wedge dy$ should be the standard area form (or determinant) on $\mathbb{R}^2$ (again, who would want $dx \wedge dy$ to mean something else 2 ?). Unfortunately, the answer 2. is different than what we found from the algebraic point of view. Worse, the choices made for the symmetric product and the wedge product do not seem to be consistent. Does anyone feel like they have a satisfying way to understand this issue? $$$$ 1 as I have tried to explain briefly. Said differently, it is natural to ask that the identification $\mathrm{Sym}^2 V \stackrel{\sim}{\to}  S^2V$ should be the restriction of the projection map $p: V\otimes V \to S^2V$ . (Same story for the wedge product). 2 Said differently, when one defines integration of differential forms, integrating $f(x, y)\, dx \wedge dy$ should produce the Lebesgue integral $\int f(x,y) dx\,dy$ . I don't think anyone uses a different convention (?). Other remark: in complex differential geometry, I find the most natural identity between a Kähler Hermitian metric $h$ , the Riemannian metric $g$ and the Kähler form $\omega$ to be $h = g - i\omega$ . Try $h = dz \otimes d\overline{z}$ : then $g = dx \otimes dx + dy \otimes dy$ and $\omega = dx \otimes dy - dy \otimes dx$ . It is nice to write $g = dx^2 + dy^2$ and $\omega = dx \wedge dy$ , in particular, the Kähler form is the area form of the Riemannian metric.","Which is the correct identity? or ? or ? Here is my understanding of the question from the point of view of: Linear algebra: Let be a vector space. The symmetric algebra is a quotient of the tensor algebra . The symmetric product of elements of does not make sense a priori in , but one can identify with the space of symmetric tensors , which is a subspace of where the restriction of the projection map is an isomorphism. Under this isomorphism, the symmetric product corresponds to the element of . Same story for the exterior algebra and alternating tensors: the wedge product is identified with the alternating tensor . So contrary to what I have read in some places (e.g. accepted answer here ), in my opinion there is one natural way to identify symmetric products to symmetric tensors (resp. wedge products to alternating tensors) 1 . Conclusion: at least from the algebraic point of view, it seems to me that the natural thing to say is: Differential geometry : Again, I feel like there is only one choice we want to make here, contrary to what I have read sometimes: , because should be the standard metric (or inner product) on (who would want to mean something else?) because should be the standard area form (or determinant) on (again, who would want to mean something else 2 ?). Unfortunately, the answer 2. is different than what we found from the algebraic point of view. Worse, the choices made for the symmetric product and the wedge product do not seem to be consistent. Does anyone feel like they have a satisfying way to understand this issue? 1 as I have tried to explain briefly. Said differently, it is natural to ask that the identification should be the restriction of the projection map . (Same story for the wedge product). 2 Said differently, when one defines integration of differential forms, integrating should produce the Lebesgue integral . I don't think anyone uses a different convention (?). Other remark: in complex differential geometry, I find the most natural identity between a Kähler Hermitian metric , the Riemannian metric and the Kähler form to be . Try : then and . It is nice to write and , in particular, the Kähler form is the area form of the Riemannian metric.","dx \, dy = dx \otimes dy + dy \otimes dx ~~~ ~~~ dx \, dy = \dfrac{dx \otimes dy + dy \otimes dx}{2}~ dx \wedge dy=dx \otimes dy - dy \otimes dx ~~~ ~~~ dx \wedge dy=\dfrac{dx \otimes dy - dy \otimes dx}{2}~  V S(V) T(V) v \cdot w V T(V) S(V) T(V) T(V) \to S(V) v \cdot w \dfrac{v \otimes w + w \otimes v}{2} T(V) \Lambda(V) v \wedge w \dfrac{v \otimes w - w \otimes v}{2} dx \, dy = \dfrac{dx \otimes dy + dy \otimes dx}{2} dx \wedge dy = \dfrac{dx \otimes dy - dy \otimes dx}{2} dx \, dy = \dfrac{dx \otimes dy + dy \otimes dx}{2} dxdx + dydy = dx^2 + dy^2  \mathbb{R}^2 dx^2 + dy^2 dx \wedge dy = dx \otimes dy - dy \otimes dx dx \wedge dy \mathbb{R}^2 dx \wedge dy  \mathrm{Sym}^2 V \stackrel{\sim}{\to}  S^2V p: V\otimes V \to S^2V f(x, y)\, dx \wedge dy \int f(x,y) dx\,dy h g \omega h = g - i\omega h = dz \otimes d\overline{z} g = dx \otimes dx + dy \otimes dy \omega = dx \otimes dy - dy \otimes dx g = dx^2 + dy^2 \omega = dx \wedge dy","['differential-geometry', 'tensor-products', 'differential-forms', 'multilinear-algebra', 'exterior-algebra']"
97,When does a space admit a flat metric?,When does a space admit a flat metric?,,"Once upon a time I was told that the torus is flat. This was supposed to be surprising, since the ordinary picture of a torus we have in our heads looks inherently curved. However, thinking instead of a torus as a square in the plane with opposite points identified, it becomes 'clear' that the torus at least admits a flat metric, because the plane admits a flat metric. However, a two-holed torus can also be obtained in this way: it is an octagon in the plane with appropriate pairs of edges identified. However, by the Gauss-Bonnet theorem, this surface does not admit a flat metric. Thus, something about the way we make the identifications for the 1-torus is compatible with the flat metric structure on the plane, and this is not so for the 2-torus. I am hence lead to ask: Given a smooth manifold $M$ obtained from $\mathbb{R}^n$ by appropriate identifications, is there some general criterion for determining whether the flat metric on $\mathbb{R}^n$ descends to $M$? Or is the $n$-dimensional torus  particularly special in its ability to inherit a metric from the plane? If so, what is special about it?","Once upon a time I was told that the torus is flat. This was supposed to be surprising, since the ordinary picture of a torus we have in our heads looks inherently curved. However, thinking instead of a torus as a square in the plane with opposite points identified, it becomes 'clear' that the torus at least admits a flat metric, because the plane admits a flat metric. However, a two-holed torus can also be obtained in this way: it is an octagon in the plane with appropriate pairs of edges identified. However, by the Gauss-Bonnet theorem, this surface does not admit a flat metric. Thus, something about the way we make the identifications for the 1-torus is compatible with the flat metric structure on the plane, and this is not so for the 2-torus. I am hence lead to ask: Given a smooth manifold $M$ obtained from $\mathbb{R}^n$ by appropriate identifications, is there some general criterion for determining whether the flat metric on $\mathbb{R}^n$ descends to $M$? Or is the $n$-dimensional torus  particularly special in its ability to inherit a metric from the plane? If so, what is special about it?",,"['differential-geometry', 'differential-topology', 'riemannian-geometry', 'surfaces']"
98,Proving that the pullback map commutes with the exterior derivative,Proving that the pullback map commutes with the exterior derivative,,"I'm trying to prove that the pullback map $\phi^{\ast}$ induced by a map $\phi:M\rightarrow N$ commutes with the exterior derivative. Here is my attempt so far: Let $\omega\;\in\Omega^{r}(N)$ and let $\phi :M\rightarrow N$. Also, let $\mathbf{v}\;\in T_{p}M$. Then, using that $df(\mathbf{v})=\mathbf{v}(f)$ and also, that $(\phi^{\ast}\omega)(\mathbf{v})=\omega (\phi_{\ast}\mathbf{v})$ where $\phi_{\ast}$ is the pushforward map induced by $\phi$, we have that $$\left(\phi^{\ast}df\right)(\mathbf{v})=df(\phi_{\ast}\mathbf{v})=(\phi_{\ast}\mathbf{v})(f)=\mathbf{v}(\phi^{\ast}f)=\left(d(\phi^{\ast}f)\right)(\mathbf{v})$$ Hence, as $\mathbf{v}\;\in T_{p}M$ was chosen arbitrarily, this implies that $$\phi^{\ast}df=d(\phi^{\ast}f).$$ Given this, we now consider an r-form $\omega\;\in\Omega^{r}(N)$ and expand in a coordinate basis $\lbrace dx^{\mu_{1}}\wedge\cdots\wedge dx^{\mu_{r}}\rbrace$ for $\Omega^{r}(N)$ such that $$\omega = f(x)dx^{\mu_{1}}\wedge\cdots\wedge dx^{\mu_{r}}$$ It then follows that $$d\omega=df\wedge dx^{\mu_{1}}\wedge\cdots\wedge dx^{\mu_{r}}$$ and also, $$\phi^{\ast}\omega = \phi^{\ast}(f(x)dx^{\mu_{1}}\wedge\cdots\wedge dx^{\mu_{r}})=(\phi^{\ast}f)\phi^{\ast}(dx^{\mu_{1}}\wedge\cdots\wedge dx^{\mu_{r}})$$ Therefore, $$\phi^{\ast}d\omega = \phi^{\ast}(df\wedge dx^{\mu_{1}}\wedge\cdots\wedge dx^{\mu_{r}})=\phi^{\ast}df\wedge\phi^{\ast}(dx^{\mu_{1}}\wedge\cdots\wedge dx^{\mu_{r}})\\=d(\phi^{\ast}f)\wedge \phi^{\ast}(dx^{\mu_{1}}\wedge\cdots\wedge dx^{\mu_{r}})\qquad\qquad\qquad\qquad\quad\,\,\\=d((\phi^{\ast}f)\;\phi^{\ast}(dx^{\mu_{1}}\wedge\cdots\wedge dx^{\mu_{r}}))\qquad\qquad\qquad\qquad\quad\,\,\\=d(\phi^{\ast}\omega)\qquad\qquad\qquad\qquad\qquad\qquad\qquad\qquad\qquad\quad\;$$ and so $\phi^{\ast}d\omega=d(\phi^{\ast}\omega)$. Would this be correct at all?","I'm trying to prove that the pullback map $\phi^{\ast}$ induced by a map $\phi:M\rightarrow N$ commutes with the exterior derivative. Here is my attempt so far: Let $\omega\;\in\Omega^{r}(N)$ and let $\phi :M\rightarrow N$. Also, let $\mathbf{v}\;\in T_{p}M$. Then, using that $df(\mathbf{v})=\mathbf{v}(f)$ and also, that $(\phi^{\ast}\omega)(\mathbf{v})=\omega (\phi_{\ast}\mathbf{v})$ where $\phi_{\ast}$ is the pushforward map induced by $\phi$, we have that $$\left(\phi^{\ast}df\right)(\mathbf{v})=df(\phi_{\ast}\mathbf{v})=(\phi_{\ast}\mathbf{v})(f)=\mathbf{v}(\phi^{\ast}f)=\left(d(\phi^{\ast}f)\right)(\mathbf{v})$$ Hence, as $\mathbf{v}\;\in T_{p}M$ was chosen arbitrarily, this implies that $$\phi^{\ast}df=d(\phi^{\ast}f).$$ Given this, we now consider an r-form $\omega\;\in\Omega^{r}(N)$ and expand in a coordinate basis $\lbrace dx^{\mu_{1}}\wedge\cdots\wedge dx^{\mu_{r}}\rbrace$ for $\Omega^{r}(N)$ such that $$\omega = f(x)dx^{\mu_{1}}\wedge\cdots\wedge dx^{\mu_{r}}$$ It then follows that $$d\omega=df\wedge dx^{\mu_{1}}\wedge\cdots\wedge dx^{\mu_{r}}$$ and also, $$\phi^{\ast}\omega = \phi^{\ast}(f(x)dx^{\mu_{1}}\wedge\cdots\wedge dx^{\mu_{r}})=(\phi^{\ast}f)\phi^{\ast}(dx^{\mu_{1}}\wedge\cdots\wedge dx^{\mu_{r}})$$ Therefore, $$\phi^{\ast}d\omega = \phi^{\ast}(df\wedge dx^{\mu_{1}}\wedge\cdots\wedge dx^{\mu_{r}})=\phi^{\ast}df\wedge\phi^{\ast}(dx^{\mu_{1}}\wedge\cdots\wedge dx^{\mu_{r}})\\=d(\phi^{\ast}f)\wedge \phi^{\ast}(dx^{\mu_{1}}\wedge\cdots\wedge dx^{\mu_{r}})\qquad\qquad\qquad\qquad\quad\,\,\\=d((\phi^{\ast}f)\;\phi^{\ast}(dx^{\mu_{1}}\wedge\cdots\wedge dx^{\mu_{r}}))\qquad\qquad\qquad\qquad\quad\,\,\\=d(\phi^{\ast}\omega)\qquad\qquad\qquad\qquad\qquad\qquad\qquad\qquad\qquad\quad\;$$ and so $\phi^{\ast}d\omega=d(\phi^{\ast}\omega)$. Would this be correct at all?",,"['differential-geometry', 'differential-forms', 'smooth-manifolds']"
99,Wanted: A purely algebraic proof of the Frobenius theorem on distributions,Wanted: A purely algebraic proof of the Frobenius theorem on distributions,,"Is there a purely algebraic proof of the Frobenius theorem? Here's a rough sketch of what i'm looking for: Let $Der(R)$ denote the $R$-module of ($R$-valued) derivations of the algebra $R$ endowed with the lie bracket given by the commutator. Definiton: A distribution $D$ is a submodule of $Der(R)$. ""Frobenius"" Theorem - Under certain restriction on the base algebra $R$    (and on the algebra $S$ that will be introduced) the following holds: A distribution $D \subset Der(R)$ is closed under the lie bracket of $Der(R)$ iff   for every maximal ideal $m \subset R$ there exists an epimorphism $f: R \to S$,  such that after localizing $R$ by $m$ and $S$ by $f(m)$ we have: $v \in D_m \iff \exists u \in Der(S)_{f(m)}$ satisfying $f_m \circ v = u \circ f_m$. I'm sure there is a ""nicer"" algebraic formulation of this problem but that's the best i could do with my current knowledge - any improvement suggestions would be very welcome. Is there such a general theorem? Does it even make sense? Denoting the exterior algebra of $Der(R)$ by $\mathcal{A}^*$. Am i right that the following equivalence is purely algebraic and no geometric input is neaded? (i did prove it, i think... need to be sure): A distribution $D \subset Der(R)$ is closed under the lie bracket $\iff$ $I(D) = \bigcup_k \{\omega \in \mathcal{A}^k : \omega(m_1,...,m_k)=0  \text{ for every tuple of elements } \{m_i\}_{i \le k} \subset D \} \subset \mathcal{A}^*$ is a differential ideal . ($d I(D) \subset I(D)$).","Is there a purely algebraic proof of the Frobenius theorem? Here's a rough sketch of what i'm looking for: Let $Der(R)$ denote the $R$-module of ($R$-valued) derivations of the algebra $R$ endowed with the lie bracket given by the commutator. Definiton: A distribution $D$ is a submodule of $Der(R)$. ""Frobenius"" Theorem - Under certain restriction on the base algebra $R$    (and on the algebra $S$ that will be introduced) the following holds: A distribution $D \subset Der(R)$ is closed under the lie bracket of $Der(R)$ iff   for every maximal ideal $m \subset R$ there exists an epimorphism $f: R \to S$,  such that after localizing $R$ by $m$ and $S$ by $f(m)$ we have: $v \in D_m \iff \exists u \in Der(S)_{f(m)}$ satisfying $f_m \circ v = u \circ f_m$. I'm sure there is a ""nicer"" algebraic formulation of this problem but that's the best i could do with my current knowledge - any improvement suggestions would be very welcome. Is there such a general theorem? Does it even make sense? Denoting the exterior algebra of $Der(R)$ by $\mathcal{A}^*$. Am i right that the following equivalence is purely algebraic and no geometric input is neaded? (i did prove it, i think... need to be sure): A distribution $D \subset Der(R)$ is closed under the lie bracket $\iff$ $I(D) = \bigcup_k \{\omega \in \mathcal{A}^k : \omega(m_1,...,m_k)=0  \text{ for every tuple of elements } \{m_i\}_{i \le k} \subset D \} \subset \mathcal{A}^*$ is a differential ideal . ($d I(D) \subset I(D)$).",,"['algebraic-geometry', 'differential-geometry', 'commutative-algebra', 'differential-forms']"
