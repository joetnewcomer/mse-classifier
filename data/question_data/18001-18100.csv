,title_raw,title_text,title_latex,body_raw,body_text,body_latex,tags
0,What is the difference between linear and affine function?,What is the difference between linear and affine function?,,I am a bit confused. What is the difference between a linear and affine function? Any suggestions will be appreciated.,I am a bit confused. What is the difference between a linear and affine function? Any suggestions will be appreciated.,,"['linear-algebra', 'affine-geometry']"
1,Why do we care about dual spaces?,Why do we care about dual spaces?,,"When I first took linear algebra, we never learned about dual spaces. Today in lecture we discussed them and I understand what they are, but I don't really understand why we want to study them within linear algebra. I was wondering if anyone knew a nice intuitive motivation for the study of dual spaces and whether or not they ""show up"" as often as other concepts in linear algebra? Is their usefulness something that just becomes more apparent as you learn more math and see them arise in different settings? Edit I understand that dual spaces show up in functional analysis and multilinear algebra, but I still don't really understand the intuition/motivation behind their definition in the standard topics covered in a linear algebra course. (Hopefully, this clarifies my question)","When I first took linear algebra, we never learned about dual spaces. Today in lecture we discussed them and I understand what they are, but I don't really understand why we want to study them within linear algebra. I was wondering if anyone knew a nice intuitive motivation for the study of dual spaces and whether or not they ""show up"" as often as other concepts in linear algebra? Is their usefulness something that just becomes more apparent as you learn more math and see them arise in different settings? Edit I understand that dual spaces show up in functional analysis and multilinear algebra, but I still don't really understand the intuition/motivation behind their definition in the standard topics covered in a linear algebra course. (Hopefully, this clarifies my question)",,"['linear-algebra', 'soft-question', 'intuition', 'dual-spaces']"
2,Proof that the trace of a matrix is the sum of its eigenvalues,Proof that the trace of a matrix is the sum of its eigenvalues,,I have looked extensively for a proof on the internet but all of them were too obscure. I would appreciate if someone could lay out a simple proof for this important result. Thank you.,I have looked extensively for a proof on the internet but all of them were too obscure. I would appreciate if someone could lay out a simple proof for this important result. Thank you.,,"['linear-algebra', 'matrices', 'eigenvalues-eigenvectors', 'trace']"
3,Inverse of the sum of matrices,Inverse of the sum of matrices,,"I have two square matrices: $A$ and $B$ . $A^{-1}$ is known and I want to calculate $(A+B)^{-1}$ . Are there theorems that help with calculating the inverse of the sum of matrices? In general case $B^{-1}$ is not known, but if it is necessary then it can be assumed that $B^{-1}$ is also known.","I have two square matrices: and . is known and I want to calculate . Are there theorems that help with calculating the inverse of the sum of matrices? In general case is not known, but if it is necessary then it can be assumed that is also known.",A B A^{-1} (A+B)^{-1} B^{-1} B^{-1},"['linear-algebra', 'matrices', 'inverse']"
4,Why does this matrix give the derivative of a function?,Why does this matrix give the derivative of a function?,,"I happened to stumble upon the following matrix: $$ A = \begin{bmatrix}     a       & 1  \\     0       & a  \end{bmatrix} $$ And after trying a bunch of different examples, I noticed the following remarkable pattern. If $P$ is a polynomial, then: $$ P(A)=\begin{bmatrix}     P(a)       & P'(a)  \\     0       & P(a) \end{bmatrix}$$ Where $P'(a)$ is the derivative evaluated at $a$. Futhermore, I tried extending this to other matrix functions, for example the matrix exponential, and wolfram alpha tells me: $$ \exp(A)=\begin{bmatrix}     e^a       & e^a  \\     0       & e^a \end{bmatrix}$$ and this does in fact follow the pattern since the derivative of $e^x$ is itself! Furthermore, I decided to look at the function $P(x)=\frac{1}{x}$. If we interpret the reciprocal of a matrix to be its inverse, then we get: $$ P(A)=\begin{bmatrix}     \frac{1}{a}       & -\frac{1}{a^2}  \\     0       & \frac{1}{a} \end{bmatrix}$$ And since $f'(a)=-\frac{1}{a^2}$, the pattern still holds! After trying a couple more examples, it seems that this pattern holds whenever $P$ is any rational function. I have two questions: Why is this happening? Are there any other known matrix functions (which can also be applied to real numbers) for which this property holds?","I happened to stumble upon the following matrix: $$ A = \begin{bmatrix}     a       & 1  \\     0       & a  \end{bmatrix} $$ And after trying a bunch of different examples, I noticed the following remarkable pattern. If $P$ is a polynomial, then: $$ P(A)=\begin{bmatrix}     P(a)       & P'(a)  \\     0       & P(a) \end{bmatrix}$$ Where $P'(a)$ is the derivative evaluated at $a$. Futhermore, I tried extending this to other matrix functions, for example the matrix exponential, and wolfram alpha tells me: $$ \exp(A)=\begin{bmatrix}     e^a       & e^a  \\     0       & e^a \end{bmatrix}$$ and this does in fact follow the pattern since the derivative of $e^x$ is itself! Furthermore, I decided to look at the function $P(x)=\frac{1}{x}$. If we interpret the reciprocal of a matrix to be its inverse, then we get: $$ P(A)=\begin{bmatrix}     \frac{1}{a}       & -\frac{1}{a^2}  \\     0       & \frac{1}{a} \end{bmatrix}$$ And since $f'(a)=-\frac{1}{a^2}$, the pattern still holds! After trying a couple more examples, it seems that this pattern holds whenever $P$ is any rational function. I have two questions: Why is this happening? Are there any other known matrix functions (which can also be applied to real numbers) for which this property holds?",,"['linear-algebra', 'matrices', 'derivatives', 'matrix-calculus', 'jordan-normal-form']"
5,What is the geometric interpretation of the transpose?,What is the geometric interpretation of the transpose?,,"I can follow the definition of the transpose algebraically, i.e. as a reflection of a matrix across its diagonal, or in terms of dual spaces, but I lack any sort of geometric understanding of the transpose, or even symmetric matrices. For example, if I have a linear transformation, say on the plane, my intuition is to visualize it as some linear distortion of the plane via scaling and rotation. I do not know how this distortion compares to the distortion that results from applying the transpose, or what one can say if the linear transformation is symmetric. Geometrically, why might we expect orthogonal matrices to be combinations of rotations and reflections?","I can follow the definition of the transpose algebraically, i.e. as a reflection of a matrix across its diagonal, or in terms of dual spaces, but I lack any sort of geometric understanding of the transpose, or even symmetric matrices. For example, if I have a linear transformation, say on the plane, my intuition is to visualize it as some linear distortion of the plane via scaling and rotation. I do not know how this distortion compares to the distortion that results from applying the transpose, or what one can say if the linear transformation is symmetric. Geometrically, why might we expect orthogonal matrices to be combinations of rotations and reflections?",,"['linear-algebra', 'matrices', 'orthogonal-matrices', 'transpose', 'geometric-interpretation']"
6,How could we define the factorial of a matrix?,How could we define the factorial of a matrix?,,"Suppose I have a square matrix $\mathsf{A}$ with $\det \mathsf{A}\neq 0$. How could we define the following operation? $$\mathsf{A}!$$ Maybe we could make some simple example, admitted it makes any sense, with $$\mathsf{A} = \left(\begin{matrix} 1 & 3 \\  2 & 1  \end{matrix} \right) $$","Suppose I have a square matrix $\mathsf{A}$ with $\det \mathsf{A}\neq 0$. How could we define the following operation? $$\mathsf{A}!$$ Maybe we could make some simple example, admitted it makes any sense, with $$\mathsf{A} = \left(\begin{matrix} 1 & 3 \\  2 & 1  \end{matrix} \right) $$",,"['linear-algebra', 'matrices', 'operator-theory', 'factorial', 'matrix-calculus']"
7,"Intuitively, what is the difference between Eigendecomposition and Singular Value Decomposition?","Intuitively, what is the difference between Eigendecomposition and Singular Value Decomposition?",,"I'm trying to intuitively understand the difference between SVD and eigendecomposition. From my understanding, eigendecomposition seeks to describe a linear transformation as a sequence of three basic operations ( $P^{-1}DP$ ) on a vector: Rotation of the coordinate system (change of basis): $P$ Independent scaling along each basis vector (of the rotated system): $D$ De-rotation of the coordinate system (undo change of basis): $P^{-1}$ But as far as I can see, SVD's goal is to do exactly the same thing, except that resulting decomposition is somehow different. What, then, is the conceptual difference between the two? For example: Is one of them more general than the other? Is either a special case of the other? Note: I'm specifically looking for an intuitive explanation, not a mathematical one. Wikipedia is already excellent at explaining the mathematical relationship between the two decompositions ( ""The right-singular vectors of M are eigenvectors of $M^*M$ "" , for example), but it completely fails to give me any intuitive understanding of what is going on intuitively. The best explanation I've found so far is this one , which is great, except it doesn't talk about eigendecompositions at all, which leaves me confused as to how SVD is any different from eigendecomposition in its goal.","I'm trying to intuitively understand the difference between SVD and eigendecomposition. From my understanding, eigendecomposition seeks to describe a linear transformation as a sequence of three basic operations ( ) on a vector: Rotation of the coordinate system (change of basis): Independent scaling along each basis vector (of the rotated system): De-rotation of the coordinate system (undo change of basis): But as far as I can see, SVD's goal is to do exactly the same thing, except that resulting decomposition is somehow different. What, then, is the conceptual difference between the two? For example: Is one of them more general than the other? Is either a special case of the other? Note: I'm specifically looking for an intuitive explanation, not a mathematical one. Wikipedia is already excellent at explaining the mathematical relationship between the two decompositions ( ""The right-singular vectors of M are eigenvectors of "" , for example), but it completely fails to give me any intuitive understanding of what is going on intuitively. The best explanation I've found so far is this one , which is great, except it doesn't talk about eigendecompositions at all, which leaves me confused as to how SVD is any different from eigendecomposition in its goal.",P^{-1}DP P D P^{-1} M^*M,"['linear-algebra', 'eigenvalues-eigenvectors', 'matrix-decomposition', 'diagonalization', 'svd']"
8,How to intuitively understand eigenvalue and eigenvector?,How to intuitively understand eigenvalue and eigenvector?,,"I’m learning multivariate analysis and I have learnt linear algebra for two semesters when I was a freshman. Eigenvalues and eigenvectors are easy to calculate and the concept is not difficult to understand. I found that there are many applications of eigenvalues and eigenvectors in multivariate analysis. For example: In principal components, proportion of total population variance due to $k$ th principal component equals $$\frac{\lambda_k}{\lambda_1+\lambda_2+\ldots+\lambda_k}$$ I think eigenvalue product corresponding eigenvector has same effect as the matrix product eigenvector geometrically. I think my former understanding may be too naive so that I cannot find the link between eigenvalue and its application in principal components and others. I know how to induce almost every step form the assumption to the result mathematically. I’d like to know how to intuitively or geometrically understand eigenvalue and eigenvector in the context of multivariate analysis (in linear algebra is also good). Thank you!","I’m learning multivariate analysis and I have learnt linear algebra for two semesters when I was a freshman. Eigenvalues and eigenvectors are easy to calculate and the concept is not difficult to understand. I found that there are many applications of eigenvalues and eigenvectors in multivariate analysis. For example: In principal components, proportion of total population variance due to th principal component equals I think eigenvalue product corresponding eigenvector has same effect as the matrix product eigenvector geometrically. I think my former understanding may be too naive so that I cannot find the link between eigenvalue and its application in principal components and others. I know how to induce almost every step form the assumption to the result mathematically. I’d like to know how to intuitively or geometrically understand eigenvalue and eigenvector in the context of multivariate analysis (in linear algebra is also good). Thank you!",k \frac{\lambda_k}{\lambda_1+\lambda_2+\ldots+\lambda_k},"['linear-algebra', 'statistics', 'eigenvalues-eigenvectors', 'intuition']"
9,Derivative of Softmax loss function,Derivative of Softmax loss function,,"I am trying to wrap my head around back-propagation in a neural network with a Softmax classifier, which uses the Softmax function: \begin{equation} p_j = \frac{e^{o_j}}{\sum_k e^{o_k}} \end{equation} This is used in a loss function of the form \begin{equation}L = -\sum_j y_j \log p_j,\end{equation} where $o$ is a vector. I need the derivative of $L$ with respect to $o$. Now if my derivatives are right, \begin{equation} \frac{\partial p_j}{\partial o_i} = p_i(1 - p_i),\quad i = j \end{equation} and \begin{equation} \frac{\partial p_j}{\partial o_i} = -p_i p_j,\quad i \neq j. \end{equation} Using this result we obtain \begin{eqnarray} \frac{\partial L}{\partial o_i} &=& - \left (y_i (1 - p_i) + \sum_{k\neq i}-p_k y_k \right )\\ &=&p_i y_i - y_i + \sum_{k\neq i} p_k y_k\\ &=& \left (\sum_i p_i y_i \right ) - y_i \end{eqnarray} According to slides I'm using, however, the result should be \begin{equation} \frac{\partial L}{\partial o_i} = p_i - y_i. \end{equation} Can someone please tell me where I'm going wrong?","I am trying to wrap my head around back-propagation in a neural network with a Softmax classifier, which uses the Softmax function: \begin{equation} p_j = \frac{e^{o_j}}{\sum_k e^{o_k}} \end{equation} This is used in a loss function of the form \begin{equation}L = -\sum_j y_j \log p_j,\end{equation} where $o$ is a vector. I need the derivative of $L$ with respect to $o$. Now if my derivatives are right, \begin{equation} \frac{\partial p_j}{\partial o_i} = p_i(1 - p_i),\quad i = j \end{equation} and \begin{equation} \frac{\partial p_j}{\partial o_i} = -p_i p_j,\quad i \neq j. \end{equation} Using this result we obtain \begin{eqnarray} \frac{\partial L}{\partial o_i} &=& - \left (y_i (1 - p_i) + \sum_{k\neq i}-p_k y_k \right )\\ &=&p_i y_i - y_i + \sum_{k\neq i} p_k y_k\\ &=& \left (\sum_i p_i y_i \right ) - y_i \end{eqnarray} According to slides I'm using, however, the result should be \begin{equation} \frac{\partial L}{\partial o_i} = p_i - y_i. \end{equation} Can someone please tell me where I'm going wrong?",,"['linear-algebra', 'derivatives', 'machine-learning']"
10,Calculate Rotation Matrix to align Vector $A$ to Vector $B$ in $3D$?,Calculate Rotation Matrix to align Vector  to Vector  in ?,A B 3D,"I have one triangle in $3D$ space that I am tracking in a simulation. Between time steps I have the previous normal of the triangle and the current normal of the triangle along with both the current and previous $3D$ vertex positions of the triangles. Using the normals of the triangular plane I would like to determine a rotation matrix that would align the normals of the triangles thereby setting the two triangles parallel to each other. I would then like to use a translation matrix to map the previous onto the current, however this is not my main concern right now. I have found this website that says I must determine the cross product of these two vectors (to determine a rotation axis) determine the dot product ( to find rotation angle) build quaternion (not sure what this means) the transformation matrix is the quaternion as a $3 \times 3$ (not sure) Any help on how I can solve this problem would be appreciated.","I have one triangle in space that I am tracking in a simulation. Between time steps I have the previous normal of the triangle and the current normal of the triangle along with both the current and previous vertex positions of the triangles. Using the normals of the triangular plane I would like to determine a rotation matrix that would align the normals of the triangles thereby setting the two triangles parallel to each other. I would then like to use a translation matrix to map the previous onto the current, however this is not my main concern right now. I have found this website that says I must determine the cross product of these two vectors (to determine a rotation axis) determine the dot product ( to find rotation angle) build quaternion (not sure what this means) the transformation matrix is the quaternion as a (not sure) Any help on how I can solve this problem would be appreciated.",3D 3D 3 \times 3,"['linear-algebra', 'matrices', 'vector-spaces', '3d', 'rotations']"
11,Looking for an intuitive explanation why the row rank is equal to the column rank for a matrix,Looking for an intuitive explanation why the row rank is equal to the column rank for a matrix,,"I am looking for an intuitive explanation as to why/how row rank of a matrix = column rank. I've read the proof on Wikipedia and I understand the proof, but I don't ""get it"". Can someone help me out with this ? I find it hard to wrap my head around the idea of how the column space and the row space is related at a fundamental level.","I am looking for an intuitive explanation as to why/how row rank of a matrix = column rank. I've read the proof on Wikipedia and I understand the proof, but I don't ""get it"". Can someone help me out with this ? I find it hard to wrap my head around the idea of how the column space and the row space is related at a fundamental level.",,"['linear-algebra', 'matrices', 'intuition', 'matrix-rank']"
12,Is there a quick proof as to why the vector space of $\mathbb{R}$ over $\mathbb{Q}$ is infinite-dimensional?,Is there a quick proof as to why the vector space of  over  is infinite-dimensional?,\mathbb{R} \mathbb{Q},It would seem that one way of proving this would be to show the existence of non-algebraic numbers. Is there a simpler way to show this?,It would seem that one way of proving this would be to show the existence of non-algebraic numbers. Is there a simpler way to show this?,,"['linear-algebra', 'vector-spaces', 'field-theory']"
13,Why is the eigenvector of a covariance matrix equal to a principal component?,Why is the eigenvector of a covariance matrix equal to a principal component?,,"If I have a covariance matrix for a data set and I multiply it times one of it's eigenvectors.  Let's say the eigenvector with the highest eigenvalue.  The result is the eigenvector or a scaled version of the eigenvector. What does this really tell me?  Why is this the principal component?  What property makes it a principal component?  Geometrically, I understand that the principal component (eigenvector) will be sloped at the general slope of the data (loosely speaking).  Again, can someone help understand why this happens?","If I have a covariance matrix for a data set and I multiply it times one of it's eigenvectors.  Let's say the eigenvector with the highest eigenvalue.  The result is the eigenvector or a scaled version of the eigenvector. What does this really tell me?  Why is this the principal component?  What property makes it a principal component?  Geometrically, I understand that the principal component (eigenvector) will be sloped at the general slope of the data (loosely speaking).  Again, can someone help understand why this happens?",,"['linear-algebra', 'statistics', 'eigenvalues-eigenvectors', 'covariance']"
14,What is the difference between a point and a vector?,What is the difference between a point and a vector?,,"I understand that a vector has direction and magnitude whereas a point doesn't. However, in the course notes that I am using, it is stated that a point is the same as a vector. Also, can you do cross product and dot product using two points instead of two vectors? I don't think so, but my roommate insists yes, and I'm kind of confused now.","I understand that a vector has direction and magnitude whereas a point doesn't. However, in the course notes that I am using, it is stated that a point is the same as a vector. Also, can you do cross product and dot product using two points instead of two vectors? I don't think so, but my roommate insists yes, and I'm kind of confused now.",,"['linear-algebra', 'vectors', 'terminology', 'definition']"
15,Show that the determinant of $A$ is equal to the product of its eigenvalues,Show that the determinant of  is equal to the product of its eigenvalues,A,"Show that the determinant of a matrix $A$ is equal to the product of its eigenvalues $\lambda_i$. So I'm having a tough time figuring this one out. I know that I have to work with the characteristic polynomial of the matrix $\det(A-\lambda I)$. But, when considering an $n \times n$ matrix, I do not know how to work out the proof. Should I just use the determinant formula for any $n \times n$ matrix? I'm guessing not, because that is quite complicated. Any insights would be great.","Show that the determinant of a matrix $A$ is equal to the product of its eigenvalues $\lambda_i$. So I'm having a tough time figuring this one out. I know that I have to work with the characteristic polynomial of the matrix $\det(A-\lambda I)$. But, when considering an $n \times n$ matrix, I do not know how to work out the proof. Should I just use the determinant formula for any $n \times n$ matrix? I'm guessing not, because that is quite complicated. Any insights would be great.",,"['linear-algebra', 'matrices', 'eigenvalues-eigenvectors', 'determinant']"
16,Is the following matrix invertible?,Is the following matrix invertible?,,"$$\begin{bmatrix} 1235 &2344 &1234 &1990\\ 2124 & 4123& 1990& 3026 \\ 1230 &1234 &9095 &1230\\ 1262 &2312& 2324 &3907  \end{bmatrix}$$ Clearly, its determinant is not zero and, hence, the matrix is invertible. Is there a more elegant way to do this? Is there a pattern among these entries?","$$\begin{bmatrix} 1235 &2344 &1234 &1990\\ 2124 & 4123& 1990& 3026 \\ 1230 &1234 &9095 &1230\\ 1262 &2312& 2324 &3907  \end{bmatrix}$$ Clearly, its determinant is not zero and, hence, the matrix is invertible. Is there a more elegant way to do this? Is there a pattern among these entries?",,"['linear-algebra', 'matrices', 'inverse']"
17,Is the inverse of a symmetric matrix also symmetric?,Is the inverse of a symmetric matrix also symmetric?,,"Let $A$ be a symmetric invertible matrix, $A^T=A$, $A^{-1}A = A A^{-1} = I$ Can it be shown that $A^{-1}$ is also symmetric? I seem to remember a proof similar to this from my linear algebra class, but it has been a long time, and I can't find it in my text book.","Let $A$ be a symmetric invertible matrix, $A^T=A$, $A^{-1}A = A A^{-1} = I$ Can it be shown that $A^{-1}$ is also symmetric? I seem to remember a proof similar to this from my linear algebra class, but it has been a long time, and I can't find it in my text book.",,"['linear-algebra', 'matrices', 'inverse', 'symmetric-matrices']"
18,Where to start learning Linear Algebra? [closed],Where to start learning Linear Algebra? [closed],,"Closed . This question is opinion-based . It is not currently accepting answers. Want to improve this question? Update the question so it can be answered with facts and citations by editing this post . Closed 9 years ago . The community reviewed whether to reopen this question 2 months ago and left it closed: Original close reason(s) were not resolved Improve this question I'm starting a very long quest to learn about math, so that I can program games. I'm mostly a corporate developer, and it's somewhat boring and non exciting. When I began my career, I chose it because I wanted to create games. I'm told that Linear Algebra is the best place to start. Where should I go?","Closed . This question is opinion-based . It is not currently accepting answers. Want to improve this question? Update the question so it can be answered with facts and citations by editing this post . Closed 9 years ago . The community reviewed whether to reopen this question 2 months ago and left it closed: Original close reason(s) were not resolved Improve this question I'm starting a very long quest to learn about math, so that I can program games. I'm mostly a corporate developer, and it's somewhat boring and non exciting. When I began my career, I chose it because I wanted to create games. I'm told that Linear Algebra is the best place to start. Where should I go?",,['linear-algebra']
19,"Why do we use the word ""scalar"" and not ""number"" in Linear Algebra?","Why do we use the word ""scalar"" and not ""number"" in Linear Algebra?",,"During a year and half of studying Linear Algebra in academy, I have never questioned why we use the word ""scalar"" and not ""number"". When I started the course our professor said we would use ""scalar"" but he never said why. So, why do we use the word ""scalar"" and not ""number"" in Linear Algebra?","During a year and half of studying Linear Algebra in academy, I have never questioned why we use the word ""scalar"" and not ""number"". When I started the course our professor said we would use ""scalar"" but he never said why. So, why do we use the word ""scalar"" and not ""number"" in Linear Algebra?",,"['linear-algebra', 'terminology']"
20,Physical meaning of the null space of a matrix,Physical meaning of the null space of a matrix,,"What is an intuitive meaning of the null space of a matrix? Why is it useful? I'm not looking for textbook definitions. My textbook gives me the definition, but I just don't ""get"" it. E.g.: I think of the rank $r$ of a matrix as the minimum number of dimensions that a linear combination of its columns would have; it tells me that, if I combined the vectors in its columns in some order, I'd get a set of coordinates for an $r$-dimensional space, where $r$ is minimum (please correct me if I'm wrong). So that means I can relate rank (and also dimension) to actual coordinate systems, and so it makes sense to me. But I can't think of any physical meaning for a null space... could someone explain what its meaning would be, for example, in a coordinate system? Thanks!","What is an intuitive meaning of the null space of a matrix? Why is it useful? I'm not looking for textbook definitions. My textbook gives me the definition, but I just don't ""get"" it. E.g.: I think of the rank $r$ of a matrix as the minimum number of dimensions that a linear combination of its columns would have; it tells me that, if I combined the vectors in its columns in some order, I'd get a set of coordinates for an $r$-dimensional space, where $r$ is minimum (please correct me if I'm wrong). So that means I can relate rank (and also dimension) to actual coordinate systems, and so it makes sense to me. But I can't think of any physical meaning for a null space... could someone explain what its meaning would be, for example, in a coordinate system? Thanks!",,"['linear-algebra', 'matrices', 'vector-spaces']"
21,When is matrix multiplication commutative?,When is matrix multiplication commutative?,,"I know that matrix multiplication in general is not commutative. So, in general: $A, B \in \mathbb{R}^{n \times n}: A \cdot B \neq B \cdot A$ But for some matrices, this equations holds, e.g. A = Identity or A = Null-matrix $\forall B \in \mathbb{R}^{n \times n}$. I think I remember that a group of special matrices (was it $O(n)$, the group of orthogonal matrices ?) exist, for which matrix multiplication is commutative. For which matrices $A, B \in \mathbb{R}^{n \times n}$ is $A\cdot B = B \cdot A$?","I know that matrix multiplication in general is not commutative. So, in general: $A, B \in \mathbb{R}^{n \times n}: A \cdot B \neq B \cdot A$ But for some matrices, this equations holds, e.g. A = Identity or A = Null-matrix $\forall B \in \mathbb{R}^{n \times n}$. I think I remember that a group of special matrices (was it $O(n)$, the group of orthogonal matrices ?) exist, for which matrix multiplication is commutative. For which matrices $A, B \in \mathbb{R}^{n \times n}$ is $A\cdot B = B \cdot A$?",,"['linear-algebra', 'matrices']"
22,Prove $\operatorname{rank}A^TA=\operatorname{rank}A$ for any $A\in M_{m \times n}$,Prove  for any,\operatorname{rank}A^TA=\operatorname{rank}A A\in M_{m \times n},"How can I prove $\operatorname{rank}A^TA=\operatorname{rank}A$ for any $A\in M_{m \times n}$ ? This is an exercise in my textbook associated with orthogonal projections and Gram-Schmidt process, but I am unsure how they are relevant.","How can I prove for any ? This is an exercise in my textbook associated with orthogonal projections and Gram-Schmidt process, but I am unsure how they are relevant.",\operatorname{rank}A^TA=\operatorname{rank}A A\in M_{m \times n},"['linear-algebra', 'matrices', 'matrix-rank', 'transpose']"
23,Why study linear algebra?,Why study linear algebra?,,"Simply as the title says. I've done some research, but still haven't arrived at an answer I am satisfied with. I know the answer varies in different fields, but in general, why would someone study linear algebra?","Simply as the title says. I've done some research, but still haven't arrived at an answer I am satisfied with. I know the answer varies in different fields, but in general, why would someone study linear algebra?",,"['linear-algebra', 'soft-question', 'motivation']"
24,Prove that simultaneously diagonalizable matrices commute,Prove that simultaneously diagonalizable matrices commute,,"Two $n\times n$ matrices $A, B$ are said to be simultaneously diagonalizable if there is a nonsingular matrix $S$ such that both $S^{-1}AS$ and $S^{-1}BS$ are diagonal matrices. a) Show that simultaneously diagonalizable matrices commute: $AB = BA$ . b) Prove that the converse is valid, provided that one of the matrices has no multiple eigenvalues. Is every pair of commuting matrices simultaneously diagonalizable? My attempt: a) Let $$M=S^{-1}AS \qquad\text{and}\qquad P=S^{-1}BS.$$ It follows that $$A= S^{-1}MS \qquad\text{and}\qquad B=S^{-1}PS. \tag{Eq. 1}$$ Thus, \begin{align} A\,B &=S^{-1}\,M\,S\,S^{-1}\,P\,S &&\text{substitution of diagonal forms} \\ A\,B &=S^{-1}\,M\,I\,P\,S &&\text{Identity element} \\ &= S^{-1}\,M\,P\,S  && \text{Multiplication by identity } \\ &= S^{-1}\,P\,M\,S &&\text{$M$ and $P$ diagonal } \\ &= S^{-1}\,P\,I\,M\,S &&\text{Multiplication by identity} \\ &=S^{-1}\,P\, \left[SS^{-1}\right]\,M\,  S && \text{$S$ has inverse by premise} \\ &=\left[S^{-1}\,P\, S\right] \left[S^{-1}\,M\,  S\right] && \text{associative properties of matrices} \\ &=B\,A &&\text{Eq. 1}. \end{align} b) How can I do this?","Two matrices are said to be simultaneously diagonalizable if there is a nonsingular matrix such that both and are diagonal matrices. a) Show that simultaneously diagonalizable matrices commute: . b) Prove that the converse is valid, provided that one of the matrices has no multiple eigenvalues. Is every pair of commuting matrices simultaneously diagonalizable? My attempt: a) Let It follows that Thus, b) How can I do this?","n\times n A, B S S^{-1}AS S^{-1}BS AB = BA M=S^{-1}AS \qquad\text{and}\qquad P=S^{-1}BS. A= S^{-1}MS \qquad\text{and}\qquad B=S^{-1}PS. \tag{Eq. 1} \begin{align}
A\,B &=S^{-1}\,M\,S\,S^{-1}\,P\,S &&\text{substitution of diagonal forms}
\\
A\,B &=S^{-1}\,M\,I\,P\,S &&\text{Identity element}
\\
&= S^{-1}\,M\,P\,S  && \text{Multiplication by identity }
\\
&= S^{-1}\,P\,M\,S &&\text{M and P diagonal }
\\
&= S^{-1}\,P\,I\,M\,S &&\text{Multiplication by identity}
\\
&=S^{-1}\,P\, \left[SS^{-1}\right]\,M\,  S
&& \text{S has inverse by premise}
\\
&=\left[S^{-1}\,P\, S\right] \left[S^{-1}\,M\,  S\right]
&& \text{associative properties of matrices}
\\
&=B\,A &&\text{Eq. 1}.
\end{align}","['linear-algebra', 'matrices']"
25,How to prove that eigenvectors from different eigenvalues are linearly independent [duplicate],How to prove that eigenvectors from different eigenvalues are linearly independent [duplicate],,"This question already has answers here : Finite sum of eigenspaces (with distinct eigenvalues) is a direct sum (3 answers) Closed 2 years ago . How can I prove that if I have $n$ eigenvectors from different eigenvalues, they are all linearly independent?","This question already has answers here : Finite sum of eigenspaces (with distinct eigenvalues) is a direct sum (3 answers) Closed 2 years ago . How can I prove that if I have $n$ eigenvectors from different eigenvalues, they are all linearly independent?",,"['linear-algebra', 'eigenvalues-eigenvectors']"
26,"Why, historically, do we multiply matrices as we do?","Why, historically, do we multiply matrices as we do?",,"Multiplication of matrices — taking the dot product of the $i$th row of the first matrix and the $j$th column of the second to yield the $ij$th entry of the product — is not a very intuitive operation: if you were to ask someone how to mutliply two matrices, he probably would not think of that method. Of course, it turns out to be very useful: matrix multiplication is precisely the operation that represents composition of transformations. But it's not intuitive. So my question is where it came from. Who thought of multiplying matrices in that way, and why? (Was it perhaps multiplication of a matrix and a vector first? If so, who thought of multiplying them in that way, and why?) My question is intact no matter whether matrix multiplication was done this way only after it was used as representation of composition of transformations, or whether, on the contrary, matrix multiplication came first. (Again, I'm not asking about the utility of multiplying matrices as we do: this is clear to me. I'm asking a question about history.)","Multiplication of matrices — taking the dot product of the $i$th row of the first matrix and the $j$th column of the second to yield the $ij$th entry of the product — is not a very intuitive operation: if you were to ask someone how to mutliply two matrices, he probably would not think of that method. Of course, it turns out to be very useful: matrix multiplication is precisely the operation that represents composition of transformations. But it's not intuitive. So my question is where it came from. Who thought of multiplying matrices in that way, and why? (Was it perhaps multiplication of a matrix and a vector first? If so, who thought of multiplying them in that way, and why?) My question is intact no matter whether matrix multiplication was done this way only after it was used as representation of composition of transformations, or whether, on the contrary, matrix multiplication came first. (Again, I'm not asking about the utility of multiplying matrices as we do: this is clear to me. I'm asking a question about history.)",,"['linear-algebra', 'matrices', 'math-history']"
27,What are differences between affine space and vector space?,What are differences between affine space and vector space?,,"I know smilar questions have been asked and I have looked at them but none of them seems to have satisfactory answer. I am reading the book a course in mathematics for student of physics vol. 1 by Paul Bamberg and Shlomo Sternberg. In Chapter 1 authors define affine space and writes: The space $\Bbb{R}^2$ is an example of a vector space . The distinction between vector space $\Bbb{R}^2$ and affine space $A\Bbb{R}^2$ lies in the fact that in $\Bbb{R}^2$ the point (0,0) has a special significance ( it is the additive identity) and the addition of two vectors in $\Bbb{R}^2$ makes sense. These do not hold for $A\Bbb{R}^2$. Please explain. Edit: How come $A\Bbb{R}^2$ has point (0,0) without special significance? and why the addition of two vectors in $A\Bbb{R}^2$ does not make sense? Please give concrete examples instead of abstract answers . I am a physics major and have done courses in Calculus, Linear Algebra and Complex Analysis.","I know smilar questions have been asked and I have looked at them but none of them seems to have satisfactory answer. I am reading the book a course in mathematics for student of physics vol. 1 by Paul Bamberg and Shlomo Sternberg. In Chapter 1 authors define affine space and writes: The space $\Bbb{R}^2$ is an example of a vector space . The distinction between vector space $\Bbb{R}^2$ and affine space $A\Bbb{R}^2$ lies in the fact that in $\Bbb{R}^2$ the point (0,0) has a special significance ( it is the additive identity) and the addition of two vectors in $\Bbb{R}^2$ makes sense. These do not hold for $A\Bbb{R}^2$. Please explain. Edit: How come $A\Bbb{R}^2$ has point (0,0) without special significance? and why the addition of two vectors in $A\Bbb{R}^2$ does not make sense? Please give concrete examples instead of abstract answers . I am a physics major and have done courses in Calculus, Linear Algebra and Complex Analysis.",,"['linear-algebra', 'affine-geometry']"
28,What does it mean to have a determinant equal to zero?,What does it mean to have a determinant equal to zero?,,"After looking in my book for a couple of hours, I'm still confused about what it means for a $(n\times n)$-matrix $A$ to have a determinant equal to zero, $\det(A)=0$. I hope someone can explain this to me in plain English.","After looking in my book for a couple of hours, I'm still confused about what it means for a $(n\times n)$-matrix $A$ to have a determinant equal to zero, $\det(A)=0$. I hope someone can explain this to me in plain English.",,"['linear-algebra', 'matrices', 'terminology', 'determinant']"
29,"Is there an ""inverted"" dot product?","Is there an ""inverted"" dot product?",,"The dot product of vectors $\mathbf{a}$ and $\mathbf{b}$ is defined as:  $$\mathbf{a} \cdot \mathbf{b} =\sum_{i=1}^{n}a_{i}b_{i}=a_{1}b_{1}+a_{2}b_{2}+\cdots +a_{n}b_{n}$$ What about the quantity? $$\mathbf{a} \star \mathbf{b} = \prod_{i=1}^{n} (a_{i} + b_{i}) = (a_{1} +b_{1})\,(a_{2}+b_{2})\cdots \,(a_{n}+b_{n})$$ Does it have a name? ""Dot sum"" seems largely inappropriate. Come to think of it, I find it interesting that the dot product is named as such, given that it is, after all, a ""sum of products"" (although I am aware that properties of $\mathbf{a} \cdot{} \mathbf{b}$, in particular distributivity, make it a meaningful name). $\mathbf{a} \star \mathbf{b}$ is commutative and has the following property: $\mathbf{a} \star (\mathbf{b} + \mathbf{c}) = \mathbf{b} \star (\mathbf{a} + \mathbf{c}) = \mathbf{c} \star (\mathbf{a} + \mathbf{b})$","The dot product of vectors $\mathbf{a}$ and $\mathbf{b}$ is defined as:  $$\mathbf{a} \cdot \mathbf{b} =\sum_{i=1}^{n}a_{i}b_{i}=a_{1}b_{1}+a_{2}b_{2}+\cdots +a_{n}b_{n}$$ What about the quantity? $$\mathbf{a} \star \mathbf{b} = \prod_{i=1}^{n} (a_{i} + b_{i}) = (a_{1} +b_{1})\,(a_{2}+b_{2})\cdots \,(a_{n}+b_{n})$$ Does it have a name? ""Dot sum"" seems largely inappropriate. Come to think of it, I find it interesting that the dot product is named as such, given that it is, after all, a ""sum of products"" (although I am aware that properties of $\mathbf{a} \cdot{} \mathbf{b}$, in particular distributivity, make it a meaningful name). $\mathbf{a} \star \mathbf{b}$ is commutative and has the following property: $\mathbf{a} \star (\mathbf{b} + \mathbf{c}) = \mathbf{b} \star (\mathbf{a} + \mathbf{c}) = \mathbf{c} \star (\mathbf{a} + \mathbf{b})$",,"['linear-algebra', 'abstract-algebra', 'vectors', 'terminology', 'products']"
30,Determinant of a non-square matrix,Determinant of a non-square matrix,,"I wrote an answer to this question based on determinants, but subsequently deleted it because the OP is interested in non-square matrices, which effectively blocks the use of determinants and thereby undermined the entire answer. However, it can be salvaged if there exists a function $\det$ defined on all real-valued matrices (not just the square ones) having the following properties. $\det$ is real-valued $\det$ has its usual value for square matrices $\det(AB)$ always equals $\det(A)\det(B)$ whenever the product $AB$ is defined. $\det(A) \neq 0$ iff $\det(A^\top) \neq 0$ Does such a function exist?","I wrote an answer to this question based on determinants, but subsequently deleted it because the OP is interested in non-square matrices, which effectively blocks the use of determinants and thereby undermined the entire answer. However, it can be salvaged if there exists a function $\det$ defined on all real-valued matrices (not just the square ones) having the following properties. $\det$ is real-valued $\det$ has its usual value for square matrices $\det(AB)$ always equals $\det(A)\det(B)$ whenever the product $AB$ is defined. $\det(A) \neq 0$ iff $\det(A^\top) \neq 0$ Does such a function exist?",,"['linear-algebra', 'matrices', 'definition', 'determinant']"
31,"Why, intuitively, is the order reversed when taking the transpose of the product?","Why, intuitively, is the order reversed when taking the transpose of the product?",,"It is well known that for invertible matrices $A,B$ of the same size we have $$(AB)^{-1}=B^{-1}A^{-1} $$ and a nice way for me to remember this is the following sentence: The opposite of putting on socks and shoes is taking the shoes off, followed by taking the socks off. Now, a similar law holds for the transpose, namely: $$(AB)^T=B^TA^T $$ for matrices $A,B$ such that the product $AB$ is defined. My question is: is there any intuitive reason as to why the order of the factors is reversed in this case? [Note that I'm aware of several proofs of this equality, and a proof is not what I'm after] Thank you!","It is well known that for invertible matrices $A,B$ of the same size we have $$(AB)^{-1}=B^{-1}A^{-1} $$ and a nice way for me to remember this is the following sentence: The opposite of putting on socks and shoes is taking the shoes off, followed by taking the socks off. Now, a similar law holds for the transpose, namely: $$(AB)^T=B^TA^T $$ for matrices $A,B$ such that the product $AB$ is defined. My question is: is there any intuitive reason as to why the order of the factors is reversed in this case? [Note that I'm aware of several proofs of this equality, and a proof is not what I'm after] Thank you!",,"['linear-algebra', 'matrices', 'soft-question', 'intuition', 'transpose']"
32,Is the vector cross product only defined for 3D?,Is the vector cross product only defined for 3D?,,"Wikipedia introduces the vector product for two vectors $\vec a$ and  $\vec b$ as $$ \vec a \times\vec b=(\| \vec a\| \|\vec b\|\sin\Theta)\vec n $$ It then mentions that $\vec n$ is the vector normal to the plane made by $\vec a$ and  $\vec b$, implying that $\vec a$ and  $\vec b$ are 3D vectors. Wikipedia mentions something about a 7D cross product, but I'm not going to pretend I understand that. My idea, which remains unconfirmed with any source, is that a cross product can be thought of a vector which is orthogonal to all vectors which you are crossing.  If, and that's a big IF, this is right over all dimensions, we know that for a set of $n-1$ $n$-dimensional vectors, there exists a vector which is orthogonal to all of them.  The magnitude would have something to do with the area/volume/hypervolume/etc. made by the vectors we are crossing. Am I right to guess that this multidimensional aspect of cross vectors exists or is that last part utter rubbish?","Wikipedia introduces the vector product for two vectors $\vec a$ and  $\vec b$ as $$ \vec a \times\vec b=(\| \vec a\| \|\vec b\|\sin\Theta)\vec n $$ It then mentions that $\vec n$ is the vector normal to the plane made by $\vec a$ and  $\vec b$, implying that $\vec a$ and  $\vec b$ are 3D vectors. Wikipedia mentions something about a 7D cross product, but I'm not going to pretend I understand that. My idea, which remains unconfirmed with any source, is that a cross product can be thought of a vector which is orthogonal to all vectors which you are crossing.  If, and that's a big IF, this is right over all dimensions, we know that for a set of $n-1$ $n$-dimensional vectors, there exists a vector which is orthogonal to all of them.  The magnitude would have something to do with the area/volume/hypervolume/etc. made by the vectors we are crossing. Am I right to guess that this multidimensional aspect of cross vectors exists or is that last part utter rubbish?",,"['linear-algebra', 'vector-spaces', 'vectors', 'cross-product']"
33,Why is it important for a matrix to be square?,Why is it important for a matrix to be square?,,"I am currently trying to self-study linear algebra. I've noticed that a lot of the definitions for terms (like eigenvectors, characteristic polynomials, determinants, and so on) require a square matrix instead of just any real-valued matrix. For example, Wolfram has this in its definition of the characteristic polynomial: The characteristic polynomial is the polynomial left-hand side of the characteristic equation $\det(A - I\lambda) = 0$, where $A$ is a square matrix. Why must the matrix be square? What happens if the matrix is not square? And why do square matrices come up so frequently in these definitions? Sorry if this is a really simple question, but I feel like I'm missing something fundamental.","I am currently trying to self-study linear algebra. I've noticed that a lot of the definitions for terms (like eigenvectors, characteristic polynomials, determinants, and so on) require a square matrix instead of just any real-valued matrix. For example, Wolfram has this in its definition of the characteristic polynomial: The characteristic polynomial is the polynomial left-hand side of the characteristic equation $\det(A - I\lambda) = 0$, where $A$ is a square matrix. Why must the matrix be square? What happens if the matrix is not square? And why do square matrices come up so frequently in these definitions? Sorry if this is a really simple question, but I feel like I'm missing something fundamental.",,"['linear-algebra', 'matrices']"
34,What is the difference between a Hamel basis and a Schauder basis?,What is the difference between a Hamel basis and a Schauder basis?,,"Let $V$ be a vector space with infinite dimensions. A Hamel basis for $V$ is an ordered set of linearly independent vectors $\{ v_i \ | \ i \in I\}$ such that any $v \in V$ can be expressed as a finite linear combination of the $v_i$'s; so $\{ v_i \ | \ i \in I\}$ spans $V$ algebraically: this is the obvious extension of the finite-dimensional notion. Moreover, by Zorn Lemma, such a basis always exists. If we endow $V$ with a topology, then we say that an ordered set of linearly independent vectors $\{ v_i \ | \ i \in I\}$ is a Schauder basis if its span is dense in $V$ with respect to the chosen topology. This amounts to say that any $v \in V$ can be expressed as an infinite linear combination of the $v_i$'s, i.e. as a series. As far as I understand, if a $v$ can be expressed as finite linear combination of some set $\{ v_i \ | \ i \in I\}$, then it lies in its span; in other words, if $\{ v_i \ | \ i \in I\}$ is a Hamel basis, then it spans the whole $V$, and so it is a Schauder basis with respect to any topology on $V$. However Per Enflo has constructed a Banach space without Schauder basis (ref. wiki ). So I guess I should conclude that my reasoning is wrong, but I can't see what's the problem. Any help appreciated, thanks in advance! UPDATE: (coming from the huge amount of answers and comments) Forgetting for a moment the concerns about cardinality and sticking to span-properties, it has turned out that we have two different notions of linear independence: one involving finite linear combinations (Hamel-span, Hamel-independence, in the terminology introduced by rschwieb below), and one allowing infinite linear combinations (Schauder-stuff). So the point is that the vectors in a Hamel basis are Hamel independent (by def) but need not be Schauder-independent in general. As far as I understand, this is the fundamental reason why a Hamel basis is not automatically a Schauder basis.","Let $V$ be a vector space with infinite dimensions. A Hamel basis for $V$ is an ordered set of linearly independent vectors $\{ v_i \ | \ i \in I\}$ such that any $v \in V$ can be expressed as a finite linear combination of the $v_i$'s; so $\{ v_i \ | \ i \in I\}$ spans $V$ algebraically: this is the obvious extension of the finite-dimensional notion. Moreover, by Zorn Lemma, such a basis always exists. If we endow $V$ with a topology, then we say that an ordered set of linearly independent vectors $\{ v_i \ | \ i \in I\}$ is a Schauder basis if its span is dense in $V$ with respect to the chosen topology. This amounts to say that any $v \in V$ can be expressed as an infinite linear combination of the $v_i$'s, i.e. as a series. As far as I understand, if a $v$ can be expressed as finite linear combination of some set $\{ v_i \ | \ i \in I\}$, then it lies in its span; in other words, if $\{ v_i \ | \ i \in I\}$ is a Hamel basis, then it spans the whole $V$, and so it is a Schauder basis with respect to any topology on $V$. However Per Enflo has constructed a Banach space without Schauder basis (ref. wiki ). So I guess I should conclude that my reasoning is wrong, but I can't see what's the problem. Any help appreciated, thanks in advance! UPDATE: (coming from the huge amount of answers and comments) Forgetting for a moment the concerns about cardinality and sticking to span-properties, it has turned out that we have two different notions of linear independence: one involving finite linear combinations (Hamel-span, Hamel-independence, in the terminology introduced by rschwieb below), and one allowing infinite linear combinations (Schauder-stuff). So the point is that the vectors in a Hamel basis are Hamel independent (by def) but need not be Schauder-independent in general. As far as I understand, this is the fundamental reason why a Hamel basis is not automatically a Schauder basis.",,"['linear-algebra', 'functional-analysis', 'schauder-basis', 'hamel-basis']"
35,Why are infinitely dimensional vector spaces not isomorphic to their duals?,Why are infinitely dimensional vector spaces not isomorphic to their duals?,,"Assuming the axiom of choice, set $\mathbb F$ to be some field (we can assume it has characteristics $0$). I was told, by more than one person, that if $\kappa$ is an infinite cardinal then the vector space $V=\mathbb F^{(\kappa)}$ (that is an infinitely dimensional space with basis of cardinality $\kappa$) is not isomorphic (as a vector space) to the algebraic dual, $V^*$. I have asked several professors in my department, and this seems to be completely folklore. I was directed to some book, but could not have find it in there as well. The Wikipedia entry tells me that this is indeed not a cardinality issue, for example $\mathbb R^{<\omega}$ (that is all the eventually zero sequences of real numbers) has the same cardinality as its dual $\mathbb R^\omega$ but they are not isomorphic. Of course being of the same cardinality is necessary but far from sufficient for two vector spaces to be isomorphic. What I am asking, really, is whether or not it is possible when given a basis and an embedding of a basis of $V$ into $V^*$, to say "" This guy is not in the span of the embedding""? Edit: I read the answers in the link given by Qiaochu. They did not satisfy me too much. My main problem is this: suppose $\kappa$ is our basis then $V$ consists of $\{f\colon\kappa\to\mathbb F\Big| |f^{-1}[\mathbb F\setminus\{0\}]|<\infty\}$ (that is finite support), while $V^*=\{f\colon\kappa\to\mathbb F\}$ (that is all the functions). In particular, the basis for $V$ is given by $f_\alpha(x) = \delta_{\alpha x}$ (i.e. $1$ on $\alpha$, and $0$ elsewhere), while $V^*$ needs a much larger basis. Why can't there by other linear functionals on $V$? Edit II: After the discussions in the comments and the answers, I have a better understanding of my question to begin with. I have no qualms that under the axiom of choice given an infinite set $\kappa$ there are a lot more functions from $\kappa$ into $\mathbb F$, than functions with finite support from $\kappa$ into $\mathbb F$. It is also clear to me that the basis of a vector space is actually the set of $\delta$ functions, whereas the basis for the dual is a subset of characteristic functions. My problem is, if so, why is the dual space composed from all functions from $A$ into $F$? (And if possible, not to just show by cardinality games that the basis is much larger but actually show the algorithm for the diagonalization.)","Assuming the axiom of choice, set $\mathbb F$ to be some field (we can assume it has characteristics $0$). I was told, by more than one person, that if $\kappa$ is an infinite cardinal then the vector space $V=\mathbb F^{(\kappa)}$ (that is an infinitely dimensional space with basis of cardinality $\kappa$) is not isomorphic (as a vector space) to the algebraic dual, $V^*$. I have asked several professors in my department, and this seems to be completely folklore. I was directed to some book, but could not have find it in there as well. The Wikipedia entry tells me that this is indeed not a cardinality issue, for example $\mathbb R^{<\omega}$ (that is all the eventually zero sequences of real numbers) has the same cardinality as its dual $\mathbb R^\omega$ but they are not isomorphic. Of course being of the same cardinality is necessary but far from sufficient for two vector spaces to be isomorphic. What I am asking, really, is whether or not it is possible when given a basis and an embedding of a basis of $V$ into $V^*$, to say "" This guy is not in the span of the embedding""? Edit: I read the answers in the link given by Qiaochu. They did not satisfy me too much. My main problem is this: suppose $\kappa$ is our basis then $V$ consists of $\{f\colon\kappa\to\mathbb F\Big| |f^{-1}[\mathbb F\setminus\{0\}]|<\infty\}$ (that is finite support), while $V^*=\{f\colon\kappa\to\mathbb F\}$ (that is all the functions). In particular, the basis for $V$ is given by $f_\alpha(x) = \delta_{\alpha x}$ (i.e. $1$ on $\alpha$, and $0$ elsewhere), while $V^*$ needs a much larger basis. Why can't there by other linear functionals on $V$? Edit II: After the discussions in the comments and the answers, I have a better understanding of my question to begin with. I have no qualms that under the axiom of choice given an infinite set $\kappa$ there are a lot more functions from $\kappa$ into $\mathbb F$, than functions with finite support from $\kappa$ into $\mathbb F$. It is also clear to me that the basis of a vector space is actually the set of $\delta$ functions, whereas the basis for the dual is a subset of characteristic functions. My problem is, if so, why is the dual space composed from all functions from $A$ into $F$? (And if possible, not to just show by cardinality games that the basis is much larger but actually show the algorithm for the diagonalization.)",,"['linear-algebra', 'vector-spaces', 'dual-spaces']"
36,Geometric interpretation of $\det(A^T) = \det(A)$,Geometric interpretation of,\det(A^T) = \det(A),"$$\det(A^T) = \det(A)$$ Using the geometric definition of the determinant as the area spanned by the columns , could someone give a geometric interpretation of the property?","Using the geometric definition of the determinant as the area spanned by the columns , could someone give a geometric interpretation of the property?",\det(A^T) = \det(A),"['linear-algebra', 'matrices', 'vector-spaces', 'determinant', 'geometric-interpretation']"
37,Understanding of the theorem that all norms are equivalent in finite dimensional vector spaces,Understanding of the theorem that all norms are equivalent in finite dimensional vector spaces,,"The following is a well-known result in functional analysis: If the vector space $X$ is finite dimensional, all norms are equivalent. Here is the standard proof in one textbook. First, pick a norm for $X$, say  $$\|x\|_1=\sum_{i=1}^n|\alpha_i|$$ where $x=\sum_{i=1}^n\alpha_ix_i$, and $(x_i)_{i=1}^n$ is a basis for $X$. Then show that every norm for $X$ is equivalent to $\|\cdot\|_1$, i.e.,  $$c\|x\|\leq\|x\|_1\leq C\|x\|.$$ For the first inequality, one can easily get $c$ by triangle inequality for the norm. For the second inequality, instead of constructing $C$, the Bolzano-Weierstrass theorem is applied to construct a contradiction. The strategies for proving these two inequalities are so different. Here is my question , Can one prove this theorem without Bolzano-Weierstrass theorem? UPDATE: Is the converse of the theorem true? In other words, if all norms for a vector space $X$ are equivalent, then can one conclude that $X$ is of finite dimension?","The following is a well-known result in functional analysis: If the vector space $X$ is finite dimensional, all norms are equivalent. Here is the standard proof in one textbook. First, pick a norm for $X$, say  $$\|x\|_1=\sum_{i=1}^n|\alpha_i|$$ where $x=\sum_{i=1}^n\alpha_ix_i$, and $(x_i)_{i=1}^n$ is a basis for $X$. Then show that every norm for $X$ is equivalent to $\|\cdot\|_1$, i.e.,  $$c\|x\|\leq\|x\|_1\leq C\|x\|.$$ For the first inequality, one can easily get $c$ by triangle inequality for the norm. For the second inequality, instead of constructing $C$, the Bolzano-Weierstrass theorem is applied to construct a contradiction. The strategies for proving these two inequalities are so different. Here is my question , Can one prove this theorem without Bolzano-Weierstrass theorem? UPDATE: Is the converse of the theorem true? In other words, if all norms for a vector space $X$ are equivalent, then can one conclude that $X$ is of finite dimension?",,['linear-algebra']
38,How to get a reflection vector?,How to get a reflection vector?,,"I'm doing a raytracing exercise. I have a vector representing the normal of a surface at an intersection point, and a vector of the ray to the surface. How can I determine what the reflection will be? In the below image, I have d and n . How can I get r ? Thanks.","I'm doing a raytracing exercise. I have a vector representing the normal of a surface at an intersection point, and a vector of the ray to the surface. How can I determine what the reflection will be? In the below image, I have d and n . How can I get r ? Thanks.",,['linear-algebra']
39,"In categorical terms, why is there no canonical isomorphism from a finite dimensional vector space to its dual?","In categorical terms, why is there no canonical isomorphism from a finite dimensional vector space to its dual?",,"I've read in several places that one motivation for category theory was to be able to give precise meaning to statements like, ""finite dimensional vector spaces are canonically isomorphic to their double duals; they are isomorphic to their duals as well, but not canonically."" I've finally sat down to work through this, and - Okay, yes, it is easy to see that the ""canonical isomorphism"" from $V$ to $V^{**}$ is a functor that has a natural isomorphism (in the sense of category theory) to the identity functor. Also, I see that there is no way that the functor $V\mapsto V^*$ could have a natural isomorphism to the identity functor, because it is contravariant whereas the identity functor is covariant. My question amounts to: Is contravariance the whole problem? To elaborate: I was initially disappointed by the realization that the definition of natural isomorphism doesn't apply to a pair of functors one of which is covariant and the other contravariant, because I was hoping that the lack of a canonical isomorphism $V\rightarrow V^*$ would feel more like a theorem as opposed to an artifact of the inapplicability of a definition. Then I tried to create a definition of a natural transformation from a covariant functor $F:\mathscr{A}\rightarrow\mathscr{B}$ to a contravariant functor $G:\mathscr{A}\rightarrow\mathscr{B}$. It seems to me that this definition should be that all objects $A\in\mathscr{A}$ get a morphism $m_A:F(A)\rightarrow G(A)$ such that for all morphisms $f:A\rightarrow A'$ of $\mathscr{A}$, the following diagram (in $\mathscr{B}$) commutes: $$\require{AMScd}\begin{CD} F(A) @>m_A>> G(A)\\ @VF(f)VV @AAG(f)A\\ F(A') @>>m_{A'}> G(A') \end{CD}$$ This is much more stringent a demand on the $m_A$ than the typical definition of a natural transformation.  Indeed, it is asking that $m_A=G(f)\circ m_{A'}\circ F(f)$, regardless of how $f$ or $A'$ may vary. Taking $\mathscr{A}=\mathscr{B}=\text{f.d.Vec}_k$, $F$ the identity functor and $G$ the dualizing functor, it is clear that this definition can never be satisfied unless $m_V$ is the zero map for all $V\in\text{f.d.Vec}_k$ (because take $f$ to be the zero map). In particular, it cannot be satisfied if $m_V$ is required to be an isomorphism. Is this the right way to understand (categorically) why there is no natural isomorphism $V\rightarrow V^*$? As an aside, are there any interesting cases of some kind of analog (the above definition or another) of natural transformations from covariant to contravariant functors? Note: I have read a number of math.SE answers regarding why $V^*$ is not naturally isomorphic to $V$. None that I have found are addressed to what I'm asking here, which is about how categories make the question and answer precise. ( This one was closest.) Hence my question here.","I've read in several places that one motivation for category theory was to be able to give precise meaning to statements like, ""finite dimensional vector spaces are canonically isomorphic to their double duals; they are isomorphic to their duals as well, but not canonically."" I've finally sat down to work through this, and - Okay, yes, it is easy to see that the ""canonical isomorphism"" from $V$ to $V^{**}$ is a functor that has a natural isomorphism (in the sense of category theory) to the identity functor. Also, I see that there is no way that the functor $V\mapsto V^*$ could have a natural isomorphism to the identity functor, because it is contravariant whereas the identity functor is covariant. My question amounts to: Is contravariance the whole problem? To elaborate: I was initially disappointed by the realization that the definition of natural isomorphism doesn't apply to a pair of functors one of which is covariant and the other contravariant, because I was hoping that the lack of a canonical isomorphism $V\rightarrow V^*$ would feel more like a theorem as opposed to an artifact of the inapplicability of a definition. Then I tried to create a definition of a natural transformation from a covariant functor $F:\mathscr{A}\rightarrow\mathscr{B}$ to a contravariant functor $G:\mathscr{A}\rightarrow\mathscr{B}$. It seems to me that this definition should be that all objects $A\in\mathscr{A}$ get a morphism $m_A:F(A)\rightarrow G(A)$ such that for all morphisms $f:A\rightarrow A'$ of $\mathscr{A}$, the following diagram (in $\mathscr{B}$) commutes: $$\require{AMScd}\begin{CD} F(A) @>m_A>> G(A)\\ @VF(f)VV @AAG(f)A\\ F(A') @>>m_{A'}> G(A') \end{CD}$$ This is much more stringent a demand on the $m_A$ than the typical definition of a natural transformation.  Indeed, it is asking that $m_A=G(f)\circ m_{A'}\circ F(f)$, regardless of how $f$ or $A'$ may vary. Taking $\mathscr{A}=\mathscr{B}=\text{f.d.Vec}_k$, $F$ the identity functor and $G$ the dualizing functor, it is clear that this definition can never be satisfied unless $m_V$ is the zero map for all $V\in\text{f.d.Vec}_k$ (because take $f$ to be the zero map). In particular, it cannot be satisfied if $m_V$ is required to be an isomorphism. Is this the right way to understand (categorically) why there is no natural isomorphism $V\rightarrow V^*$? As an aside, are there any interesting cases of some kind of analog (the above definition or another) of natural transformations from covariant to contravariant functors? Note: I have read a number of math.SE answers regarding why $V^*$ is not naturally isomorphic to $V$. None that I have found are addressed to what I'm asking here, which is about how categories make the question and answer precise. ( This one was closest.) Hence my question here.",,"['linear-algebra', 'abstract-algebra', 'vector-spaces', 'category-theory', 'dual-spaces']"
40,Importance of matrix rank,Importance of matrix rank,,"What is the importance of the rank of a matrix? I know that the rank of a matrix is the number of linearly independent rows or columns (whichever is smaller). Why is it a problem if a matrix is rank deficient? Also, why is the smaller value between row and column the rank? An intuitive or descriptive answer (also in terms of geometry) would help a lot.","What is the importance of the rank of a matrix? I know that the rank of a matrix is the number of linearly independent rows or columns (whichever is smaller). Why is it a problem if a matrix is rank deficient? Also, why is the smaller value between row and column the rank? An intuitive or descriptive answer (also in terms of geometry) would help a lot.",,"['linear-algebra', 'matrices', 'matrix-rank']"
41,"Is arrow notation for vectors ""not mathematically mature""?","Is arrow notation for vectors ""not mathematically mature""?",,"Assuming that we can't bold our variables (say, we're writing math as opposed to typing it), is it ""not mathematically mature"" to put an arrow over a vector? I ask this because in my linear algebra class, my professor never used arrow notation, so sometimes it wasn't obvious between distinguishing a scalar and a vector. (Granted, he did reserve $u$, $v$, and $w$ to mean vectors.) At the same time, my machine learning class used arrows to denote vectors, but I know some other machine learning literature chooses not to put arrows on top of their vectors. Ultimately, I just want a yes or no answer, so at least I do not seem like an immature writer when writing my own papers someday.","Assuming that we can't bold our variables (say, we're writing math as opposed to typing it), is it ""not mathematically mature"" to put an arrow over a vector? I ask this because in my linear algebra class, my professor never used arrow notation, so sometimes it wasn't obvious between distinguishing a scalar and a vector. (Granted, he did reserve $u$, $v$, and $w$ to mean vectors.) At the same time, my machine learning class used arrows to denote vectors, but I know some other machine learning literature chooses not to put arrows on top of their vectors. Ultimately, I just want a yes or no answer, so at least I do not seem like an immature writer when writing my own papers someday.",,"['linear-algebra', 'notation', 'vectors']"
42,Why is the product of two rotation matrices not commutative?,Why is the product of two rotation matrices not commutative?,,Is there any intuition why rotational matrices are not commutative? I assume the final rotation is the combination of all rotations. Then how does it matter in which order the rotations are applied?,Is there any intuition why rotational matrices are not commutative? I assume the final rotation is the combination of all rotations. Then how does it matter in which order the rotations are applied?,,"['linear-algebra', 'matrices', 'rotations']"
43,Mathematicians' Tensors vs. Physicists' Tensors,Mathematicians' Tensors vs. Physicists' Tensors,,"It seems, at times, that physicists and mathematicians mean different things when they say the word ""tensor."" From my perspective, when I say tensor, I mean ""an element of a tensor product of vector spaces."" For instance, here is a segment about tensors from Zee's book Einstein Gravity in a Nutshell : We already saw in the preceding chapter   that a vector is defined by how it transforms: $V^{'i}  = R^{ij}V^j$ . Consider a collection of “mathematical   entities” $T^{ij}$ with $i , j = 1, 2, . . . , D$ in $D$-dimensional space. If they transform   under rotations according to   $T^{ij} \to T^{'ij} = R^{ik}R^{jl}T^{kl}$ then we say that $T$ transforms like a tensor. This does not really make any sense to me. Even for ""vectors,"" and before we get to ""tensors,"" it seems like we'd have to be given a sense of what it means for an object to ""transform."" How do they divine these transformation rules? I am not completely formalism bound, but I have no idea how they would infer these transformation rules without a notion of what the object is first . For me, if I am given, say, $v \in \mathbb{R}^3$ endowed with whatever basis, I can derive that any linear map is given by matrix multiplication as it seems the physicists mean. But, I am having trouble even interpreting their statement. How do you derive how something ""transforms"" without having a notion of what it is? If you want to convince me that the moon is made of green cheese, I need to at least have a notion of what the moon is first. The same is true of tensors. My questions are: What exactly are the physicists saying, and can someone translate what they're saying into something more intelligible? How can they get these ""transformation rules"" without having a notion of what the thing is that they are transforming? What is the relationship between what physicists are expressing versus mathematicians? How can I talk about this with physicists without being accused of being a stickler for formalism and some kind of plague?","It seems, at times, that physicists and mathematicians mean different things when they say the word ""tensor."" From my perspective, when I say tensor, I mean ""an element of a tensor product of vector spaces."" For instance, here is a segment about tensors from Zee's book Einstein Gravity in a Nutshell : We already saw in the preceding chapter   that a vector is defined by how it transforms: $V^{'i}  = R^{ij}V^j$ . Consider a collection of “mathematical   entities” $T^{ij}$ with $i , j = 1, 2, . . . , D$ in $D$-dimensional space. If they transform   under rotations according to   $T^{ij} \to T^{'ij} = R^{ik}R^{jl}T^{kl}$ then we say that $T$ transforms like a tensor. This does not really make any sense to me. Even for ""vectors,"" and before we get to ""tensors,"" it seems like we'd have to be given a sense of what it means for an object to ""transform."" How do they divine these transformation rules? I am not completely formalism bound, but I have no idea how they would infer these transformation rules without a notion of what the object is first . For me, if I am given, say, $v \in \mathbb{R}^3$ endowed with whatever basis, I can derive that any linear map is given by matrix multiplication as it seems the physicists mean. But, I am having trouble even interpreting their statement. How do you derive how something ""transforms"" without having a notion of what it is? If you want to convince me that the moon is made of green cheese, I need to at least have a notion of what the moon is first. The same is true of tensors. My questions are: What exactly are the physicists saying, and can someone translate what they're saying into something more intelligible? How can they get these ""transformation rules"" without having a notion of what the thing is that they are transforming? What is the relationship between what physicists are expressing versus mathematicians? How can I talk about this with physicists without being accused of being a stickler for formalism and some kind of plague?",,"['linear-algebra', 'geometry', 'differential-geometry', 'tensor-products']"
44,Why did no student correctly find a pair of $2\times 2$ matrices with the same determinant and trace that are not similar?,Why did no student correctly find a pair of  matrices with the same determinant and trace that are not similar?,2\times 2,"I gave the following problem to students: Two $n\times n$ matrices $A$ and $B$ are similar if there exists a nonsingular matrix $P$ such that $A=P^{-1}BP$. Prove that if $A$ and $B$ are two similar $n\times n$ matrices, then they have the same determinant and the same trace. Give an example of two $2\times 2$ matrices $A$ and $B$ with same determinant, same trace but that are not similar. Most of the ~20 students got the first question right. However, almost none of them found a correct example to the second question. Most of them gave examples of matrices that have same determinant and same trace. But computations show that their examples are similar matrices. They didn't bother to check that though, so they just tried random matrices with same trace and same determinant, hoping it would be a correct example. Question : how to explain that none of the random trial gave non similar matrices? Any answer based on density or measure theory is fine. In particular, you can assume any reasonable distribution on the entries of the matrix. If it matters, the course is about matrices with real coefficients, but you can assume integer coefficients, since when choosing numbers at random , most people will choose integers.","I gave the following problem to students: Two $n\times n$ matrices $A$ and $B$ are similar if there exists a nonsingular matrix $P$ such that $A=P^{-1}BP$. Prove that if $A$ and $B$ are two similar $n\times n$ matrices, then they have the same determinant and the same trace. Give an example of two $2\times 2$ matrices $A$ and $B$ with same determinant, same trace but that are not similar. Most of the ~20 students got the first question right. However, almost none of them found a correct example to the second question. Most of them gave examples of matrices that have same determinant and same trace. But computations show that their examples are similar matrices. They didn't bother to check that though, so they just tried random matrices with same trace and same determinant, hoping it would be a correct example. Question : how to explain that none of the random trial gave non similar matrices? Any answer based on density or measure theory is fine. In particular, you can assume any reasonable distribution on the entries of the matrix. If it matters, the course is about matrices with real coefficients, but you can assume integer coefficients, since when choosing numbers at random , most people will choose integers.",,"['linear-algebra', 'probability', 'matrices', 'random-matrices', 'similar-matrices']"
45,Union of two vector subspaces not a subspace?,Union of two vector subspaces not a subspace?,,I'm having a difficult time understanding this statement. Can someone please explain with a concrete example?,I'm having a difficult time understanding this statement. Can someone please explain with a concrete example?,,['linear-algebra']
46,Probability for an $n\times n$ matrix to have only real eigenvalues,Probability for an  matrix to have only real eigenvalues,n\times n,"Let $A$ be an $n\times n$ random matrix where every entry is i.i.d. and uniformly distributed on $[0,1]$ . What is the probability that $A$ has only real eigenvalues? The answer cannot be $0$ or $1$ , since the set of matrices with distinct real eigenvalues is open, and also the set with distinct, but not all real, eigenvalues is open (the matrices with repeated eigenvalues have measure zero). I don't see any easy transformation that links the two sets, and working on the characteristic polynomial seems quite impractical. Also, I have the feeling that $[0,1]^{n^2}$ is not a good space to work in, due to its lack of rotational invariance.","Let be an random matrix where every entry is i.i.d. and uniformly distributed on . What is the probability that has only real eigenvalues? The answer cannot be or , since the set of matrices with distinct real eigenvalues is open, and also the set with distinct, but not all real, eigenvalues is open (the matrices with repeated eigenvalues have measure zero). I don't see any easy transformation that links the two sets, and working on the characteristic polynomial seems quite impractical. Also, I have the feeling that is not a good space to work in, due to its lack of rotational invariance.","A n\times n [0,1] A 0 1 [0,1]^{n^2}","['linear-algebra', 'probability', 'matrices', 'eigenvalues-eigenvectors', 'random-matrices']"
47,Are the eigenvalues of $AB$ equal to the eigenvalues of $BA$?,Are the eigenvalues of  equal to the eigenvalues of ?,AB BA,"First of all, am I being crazy in thinking that if $\lambda$ is an eigenvalue of $AB$, where $A$ and $B$ are both $N \times N$ matrices (not necessarily invertible), then $\lambda$ is also an eigenvalue of $BA$? If it's not true, then under what conditions is it true or not true? If it is true, can anyone point me to a citation?  I couldn't find it in a quick perusal of Horn & Johnson.  I have seen a couple proofs that the characteristic polynomial of $AB$ is equal to the characteristic polynomial of $BA$, but none with any citations. A trivial proof would be OK, but a citation is better.","First of all, am I being crazy in thinking that if $\lambda$ is an eigenvalue of $AB$, where $A$ and $B$ are both $N \times N$ matrices (not necessarily invertible), then $\lambda$ is also an eigenvalue of $BA$? If it's not true, then under what conditions is it true or not true? If it is true, can anyone point me to a citation?  I couldn't find it in a quick perusal of Horn & Johnson.  I have seen a couple proofs that the characteristic polynomial of $AB$ is equal to the characteristic polynomial of $BA$, but none with any citations. A trivial proof would be OK, but a citation is better.",,"['linear-algebra', 'matrices', 'eigenvalues-eigenvectors']"
48,A matrix and its transpose have the same set of eigenvalues/other version: $A$ and $A^T$ have the same spectrum,A matrix and its transpose have the same set of eigenvalues/other version:  and  have the same spectrum,A A^T,Let $ \sigma(A)$ be the set of all eigenvalues of $A$ . Show that $ \sigma(A) = \sigma\left(A^T\right)$ where $A^T$ is the transpose matrix of $A$ .,Let be the set of all eigenvalues of . Show that where is the transpose matrix of ., \sigma(A) A  \sigma(A) = \sigma\left(A^T\right) A^T A,"['linear-algebra', 'matrices']"
49,linear algebra over a division ring vs. over a field,linear algebra over a division ring vs. over a field,,"When I was studying linear algebra in the first year, from what I remember, vector spaces were always defined over a field, which was in every single concrete example equal to either $\mathbb{R}$ or $\mathbb{C}$. In Associative Algebra course, we sometimes mentioned (when talking about $R$-modules) that if $R$ is a division ring, everything becomes trivial and known from linear algebra. During the summer, I'm planning to revisit my notes of linear algebra, write them in tex, and try to prove as much as possible in a general setting. Are there any theorems in linear algebra, that hold for vector spaces over a field and not over a division ring? How much linear algebra can be done over a division ring? Also, what are some examples of division rings, that aren't fields? $\mathbb{H}$ is the only one that comes to mind. I know there aren't any finite ones (Wedderburn). Of course, I'm looking for reasonably nice ones...","When I was studying linear algebra in the first year, from what I remember, vector spaces were always defined over a field, which was in every single concrete example equal to either $\mathbb{R}$ or $\mathbb{C}$. In Associative Algebra course, we sometimes mentioned (when talking about $R$-modules) that if $R$ is a division ring, everything becomes trivial and known from linear algebra. During the summer, I'm planning to revisit my notes of linear algebra, write them in tex, and try to prove as much as possible in a general setting. Are there any theorems in linear algebra, that hold for vector spaces over a field and not over a division ring? How much linear algebra can be done over a division ring? Also, what are some examples of division rings, that aren't fields? $\mathbb{H}$ is the only one that comes to mind. I know there aren't any finite ones (Wedderburn). Of course, I'm looking for reasonably nice ones...",,"['linear-algebra', 'abstract-algebra', 'division-algebras']"
50,Why does Friedberg say that the role of the determinant is less central than in former times?,Why does Friedberg say that the role of the determinant is less central than in former times?,,"I am taking a proof-based introductory course to Linear Algebra as an undergrad student of Mathematics and Computer Science. The author of my textbook (Friedberg's Linear Algebra , 4th Edition) says in the introduction to Chapter 4: The determinant, which has played a prominent role in the theory of linear algebra, is a special scalar-valued function defined on the set of square matrices. Although it still has a place in the study of linear algebra and its applications, its role is less central than in former times. He even sets up the chapter in such a way that you can skip going into detail and move on: For the reader who prefers to treat determinants lightly, Section 4.4 contains the essential properties that are needed in later chapters. Could anyone offer a didactic and simple explanation that refutes or asserts the author's statement?","I am taking a proof-based introductory course to Linear Algebra as an undergrad student of Mathematics and Computer Science. The author of my textbook (Friedberg's Linear Algebra , 4th Edition) says in the introduction to Chapter 4: The determinant, which has played a prominent role in the theory of linear algebra, is a special scalar-valued function defined on the set of square matrices. Although it still has a place in the study of linear algebra and its applications, its role is less central than in former times. He even sets up the chapter in such a way that you can skip going into detail and move on: For the reader who prefers to treat determinants lightly, Section 4.4 contains the essential properties that are needed in later chapters. Could anyone offer a didactic and simple explanation that refutes or asserts the author's statement?",,"['linear-algebra', 'matrices', 'determinant', 'math-history']"
51,"What is the relation between rank of a matrix, its eigenvalues and eigenvectors","What is the relation between rank of a matrix, its eigenvalues and eigenvectors",,I am quite confused about this. I know that zero eigenvalue means that null space has non zero dimension. And that the rank of matrix is not the whole space. But is the number of distinct eigenvalues ( thus independent eigenvectos ) is the rank of matrix?,I am quite confused about this. I know that zero eigenvalue means that null space has non zero dimension. And that the rank of matrix is not the whole space. But is the number of distinct eigenvalues ( thus independent eigenvectos ) is the rank of matrix?,,"['linear-algebra', 'eigenvalues-eigenvectors', 'matrix-rank']"
52,Cute Determinant Question,Cute Determinant Question,,"I stumbled across the following problem and found it cute. Problem: We are given that $19$ divides $23028$, $31882$, $86469$, $6327$, and $61902$. Show that $19$ divides the following determinant: $$\left|  \begin{matrix}   2 & 3&0&2&8 \\   3 & 1&8&8&2\\ 8&6&4&6&9\\ 0&6&3&2&7\\ 6&1&9&0&2  \end{matrix}\right|$$","I stumbled across the following problem and found it cute. Problem: We are given that $19$ divides $23028$, $31882$, $86469$, $6327$, and $61902$. Show that $19$ divides the following determinant: $$\left|  \begin{matrix}   2 & 3&0&2&8 \\   3 & 1&8&8&2\\ 8&6&4&6&9\\ 0&6&3&2&7\\ 6&1&9&0&2  \end{matrix}\right|$$",,"['linear-algebra', 'determinant']"
53,Geometric interpretation for complex eigenvectors of a 2×2 rotation matrix,Geometric interpretation for complex eigenvectors of a 2×2 rotation matrix,,The rotation matrix  $$\pmatrix{ \cos \theta & \sin \theta \\ -\sin \theta & \cos \theta}$$ has complex eigenvalues $\{e^{\pm i\theta}\}$ corresponding to eigenvectors $\pmatrix{1 \\i}$ and $\pmatrix{1 \\ -i}$. The real eigenvector of a 3d rotation matrix has a natural interpretation as the axis of rotation. Is there a nice geometric interpretation of the eigenvectors of the $2 \times 2$ matrix?,The rotation matrix  $$\pmatrix{ \cos \theta & \sin \theta \\ -\sin \theta & \cos \theta}$$ has complex eigenvalues $\{e^{\pm i\theta}\}$ corresponding to eigenvectors $\pmatrix{1 \\i}$ and $\pmatrix{1 \\ -i}$. The real eigenvector of a 3d rotation matrix has a natural interpretation as the axis of rotation. Is there a nice geometric interpretation of the eigenvectors of the $2 \times 2$ matrix?,,"['linear-algebra', 'matrices', 'eigenvalues-eigenvectors', 'rotations', 'geometric-interpretation']"
54,Why determinant of a 2 by 2 matrix is the area of a parallelogram?,Why determinant of a 2 by 2 matrix is the area of a parallelogram?,,"Let $A=\begin{bmatrix}a & b\\ c & d\end{bmatrix}$. How could we show that $ad-bc$ is the area of a parallelogram with vertices $(0, 0),\ (a, b),\ (c, d),\ (a+b, c+d)$? Are the areas of the following parallelograms the same? $(1)$ parallelogram with vertices $(0, 0),\ (a, b),\ (c, d),\ (a+c, b+d)$. $(2)$ parallelogram with vertices $(0, 0),\ (a, c),\ (b, d),\ (a+b, c+d)$. $(3)$ parallelogram with vertices $(0, 0),\ (a, b),\ (c, d),\ (a+d, b+c)$. $(4)$ parallelogram with vertices $(0, 0),\ (a, c),\ (b, d),\ (a+d, b+c)$. Thank you very much.","Let $A=\begin{bmatrix}a & b\\ c & d\end{bmatrix}$. How could we show that $ad-bc$ is the area of a parallelogram with vertices $(0, 0),\ (a, b),\ (c, d),\ (a+b, c+d)$? Are the areas of the following parallelograms the same? $(1)$ parallelogram with vertices $(0, 0),\ (a, b),\ (c, d),\ (a+c, b+d)$. $(2)$ parallelogram with vertices $(0, 0),\ (a, c),\ (b, d),\ (a+b, c+d)$. $(3)$ parallelogram with vertices $(0, 0),\ (a, b),\ (c, d),\ (a+d, b+c)$. $(4)$ parallelogram with vertices $(0, 0),\ (a, c),\ (b, d),\ (a+d, b+c)$. Thank you very much.",,"['linear-algebra', 'matrices']"
55,Alice and Bob play the determinant game,Alice and Bob play the determinant game,,"Alice and Bob play the following game with an $n \times n$ matrix, where $n$ is odd. Alice fills in one of the entries of the matrix with a real number, then Bob, then Alice and so forth until the entire matrix is filled. At the end, the determinant of the matrix is taken. If it is nonzero, Alice wins; if it is zero, Bob wins. Determine who wins playing perfect strategy each time. When $n$ is even it's easy to see why Bob wins every time. and for $n$ equal to $3$ I have brute-forced it. Bob wins. But for $n = 5$ and above I can't see who will win on perfect strategy each time. Any clever approaches to solving this problem?","Alice and Bob play the following game with an $n \times n$ matrix, where $n$ is odd. Alice fills in one of the entries of the matrix with a real number, then Bob, then Alice and so forth until the entire matrix is filled. At the end, the determinant of the matrix is taken. If it is nonzero, Alice wins; if it is zero, Bob wins. Determine who wins playing perfect strategy each time. When $n$ is even it's easy to see why Bob wins every time. and for $n$ equal to $3$ I have brute-forced it. Bob wins. But for $n = 5$ and above I can't see who will win on perfect strategy each time. Any clever approaches to solving this problem?",,"['linear-algebra', 'matrices', 'determinant', 'combinatorial-game-theory']"
56,How to show that $\det(AB) =\det(A) \det(B)$?,How to show that ?,\det(AB) =\det(A) \det(B),"Given two square matrices $A$ and $B$ , how do you show that $$\det(AB) = \det(A) \det(B)$$ where $\det(\cdot)$ is the determinant of the matrix?","Given two square matrices and , how do you show that where is the determinant of the matrix?",A B \det(AB) = \det(A) \det(B) \det(\cdot),"['linear-algebra', 'matrices', 'determinant']"
57,"In Linear Algebra, what is a vector?","In Linear Algebra, what is a vector?",,"I understand that a vector space is a collection of vectors that can be added and scalar multiplied and satisfies the 8 axioms, however, I do not know what a vector is. I know in physics a vector is a geometric object that has a magnitude and a direction and it computer science a vector is a container that holds elements, expand, or shrink, but in linear algebra the definition of a vector isn't too clear. As a result, what is a vector in Linear Algebra?","I understand that a vector space is a collection of vectors that can be added and scalar multiplied and satisfies the 8 axioms, however, I do not know what a vector is. I know in physics a vector is a geometric object that has a magnitude and a direction and it computer science a vector is a container that holds elements, expand, or shrink, but in linear algebra the definition of a vector isn't too clear. As a result, what is a vector in Linear Algebra?",,"['linear-algebra', 'vector-spaces']"
58,difference between dot product and inner product,difference between dot product and inner product,,I was wondering if a dot product is technically a term used when discussing the product of $2$ vectors is equal to $0$.  And would anyone agree that an inner product is a term used when discussing the integral of the product of $2$ functions is equal to $0$?  Or is there no difference at all between a dot product and an inner product?,I was wondering if a dot product is technically a term used when discussing the product of $2$ vectors is equal to $0$.  And would anyone agree that an inner product is a term used when discussing the integral of the product of $2$ functions is equal to $0$?  Or is there no difference at all between a dot product and an inner product?,,"['linear-algebra', 'inner-products']"
59,"What is ""Bra"" and ""Ket"" notation and how does it relate to Hilbert spaces?","What is ""Bra"" and ""Ket"" notation and how does it relate to Hilbert spaces?",,"This is my first semester of quantum mechanics and higher mathematics and I am completely lost. I have tried to find help at my university, browsed similar questions on this site, looked at my textbook (Griffiths) and read countless of pdf's on the web but for some reason I am just not getting it. Can someone explain to me, in the simplest terms possible, what this ""Bra"" and ""Ket"" (Dirac) notation is, why it is so important in quantum mechanics and how it relates to Hilbert spaces? I would be infinitely grateful for an explanation that would actually help me understand this. Edit 1: I want to thank everyone for the amazing answers I have received so far. Unfortunately I am still on the road and unable to properly read some of the replies on my phone. When I get home I will read and respond to all of the replies and accept an answer. Edit 2: I just got home and had a chance to read and re-read all of the answers. I want to thank everyone again for the amazing help over the past few days. All individual answers were great. However, the combination of all answers is what really helped me understand bra-ket notation. For that reason I cannot really single out and accept a ""best answer"". Since I have to accept an answer, I will use a random number generator and accept a random answer. For anyone returning to this question at a later time: Please read all the answers! All of them are amazing.","This is my first semester of quantum mechanics and higher mathematics and I am completely lost. I have tried to find help at my university, browsed similar questions on this site, looked at my textbook (Griffiths) and read countless of pdf's on the web but for some reason I am just not getting it. Can someone explain to me, in the simplest terms possible, what this ""Bra"" and ""Ket"" (Dirac) notation is, why it is so important in quantum mechanics and how it relates to Hilbert spaces? I would be infinitely grateful for an explanation that would actually help me understand this. Edit 1: I want to thank everyone for the amazing answers I have received so far. Unfortunately I am still on the road and unable to properly read some of the replies on my phone. When I get home I will read and respond to all of the replies and accept an answer. Edit 2: I just got home and had a chance to read and re-read all of the answers. I want to thank everyone again for the amazing help over the past few days. All individual answers were great. However, the combination of all answers is what really helped me understand bra-ket notation. For that reason I cannot really single out and accept a ""best answer"". Since I have to accept an answer, I will use a random number generator and accept a random answer. For anyone returning to this question at a later time: Please read all the answers! All of them are amazing.",,"['linear-algebra', 'functional-analysis', 'notation', 'hilbert-spaces', 'mathematical-physics']"
60,"Are all eigenvectors, of any matrix, always orthogonal?","Are all eigenvectors, of any matrix, always orthogonal?",,"I have a very simple question that can be stated without any proof. Are all eigenvectors, of any matrix, always orthogonal? I am trying to understand principal components and it is crucial for me to see the basis of eigenvectors.","I have a very simple question that can be stated without any proof. Are all eigenvectors, of any matrix, always orthogonal? I am trying to understand principal components and it is crucial for me to see the basis of eigenvectors.",,"['linear-algebra', 'matrices', 'eigenvalues-eigenvectors']"
61,How to find perpendicular vector to another vector?,How to find perpendicular vector to another vector?,,"How do I find a vector perpendicular to a vector like this: $$3\mathbf{i}+4\mathbf{j}-2\mathbf{k}?$$ Could anyone explain this to me, please? I have a solution to this when I have $3\mathbf{i}+4\mathbf{j}$, but could not solve if I have $3$ components... When I googled, I saw the direct solution but did not find a process or method to follow. Kindly let me know the way to do it. Thanks.","How do I find a vector perpendicular to a vector like this: $$3\mathbf{i}+4\mathbf{j}-2\mathbf{k}?$$ Could anyone explain this to me, please? I have a solution to this when I have $3\mathbf{i}+4\mathbf{j}$, but could not solve if I have $3$ components... When I googled, I saw the direct solution but did not find a process or method to follow. Kindly let me know the way to do it. Thanks.",,"['linear-algebra', 'geometry', 'vector-spaces', 'vectors']"
62,"What is the ""standard basis"" for fields of complex numbers?","What is the ""standard basis"" for fields of complex numbers?",,"What is the ""standard basis"" for fields of complex numbers? For example, what is the standard basis for $\Bbb C^2$ (two-tuples of the form: $(a + bi, c + di)$)?  I know the standard for $\Bbb R^2$ is $((1, 0), (0, 1))$.  Is the standard basis exactly the same for complex numbers? P.S. - I realize this question is very simplistic, but I couldn't find an authoritative answer online.","What is the ""standard basis"" for fields of complex numbers? For example, what is the standard basis for $\Bbb C^2$ (two-tuples of the form: $(a + bi, c + di)$)?  I know the standard for $\Bbb R^2$ is $((1, 0), (0, 1))$.  Is the standard basis exactly the same for complex numbers? P.S. - I realize this question is very simplistic, but I couldn't find an authoritative answer online.",,"['linear-algebra', 'complex-numbers']"
63,Motivation for spectral graph theory.,Motivation for spectral graph theory.,,"Why do we care about eigenvalues of graphs? Of course, any novel question in mathematics is interesting, but there is an entire discipline of mathematics devoted to studying these eigenvalues, so they must be important. I always assumed that spectral graph theory extends graph theory by providing tools to prove things we couldn't otherwise, somewhat like how representation theory extends finite group theory.  But most results I see in spectral graph theory seem to concern eigenvalues  not as means to an end, but as objects of interest in their own right. I also considered practical value as motivation, e.g. using a given set of eigenvalues to put bounds on essential properties of graphs, such as maximum vertex degree.  But I can't imagine a situation in which I would have access to a graph's eigenvalues before I would know much more elementary information like maximum vertex degree. ( EDIT: for example, dtldarek points out that $\lambda_2$ is related to diameter, but then why would we need $\lambda_2$ when we already have diameter?  Is this somehow conceptually beneficial?) So, what is the meaning of graph spectra intuitively?  And for what practical purposes are they used?  Why is finding the eigenvalues of a graph's adjacency/Laplacian matrices more than just a novel problem?","Why do we care about eigenvalues of graphs? Of course, any novel question in mathematics is interesting, but there is an entire discipline of mathematics devoted to studying these eigenvalues, so they must be important. I always assumed that spectral graph theory extends graph theory by providing tools to prove things we couldn't otherwise, somewhat like how representation theory extends finite group theory.  But most results I see in spectral graph theory seem to concern eigenvalues  not as means to an end, but as objects of interest in their own right. I also considered practical value as motivation, e.g. using a given set of eigenvalues to put bounds on essential properties of graphs, such as maximum vertex degree.  But I can't imagine a situation in which I would have access to a graph's eigenvalues before I would know much more elementary information like maximum vertex degree. ( EDIT: for example, dtldarek points out that $\lambda_2$ is related to diameter, but then why would we need $\lambda_2$ when we already have diameter?  Is this somehow conceptually beneficial?) So, what is the meaning of graph spectra intuitively?  And for what practical purposes are they used?  Why is finding the eigenvalues of a graph's adjacency/Laplacian matrices more than just a novel problem?",,"['linear-algebra', 'graph-theory', 'intuition', 'spectral-graph-theory', 'motivation']"
64,Is linear algebra more “fully understood” than other maths disciplines?,Is linear algebra more “fully understood” than other maths disciplines?,,"In a recent question , it was discussed how LA is a foundation to other branches of mathematics, be they pure or applied. One answer argued that linear problems are fully understood , and hence a natural target to reduce pretty much anything to. Now, it's evident enough that such a linearisation, if possible, tends to make hard things easier. Find a Hilbert space hidden in your domain, obtain an orthonormal basis, and bam , any point/state can be described as a mere sequence of numbers, any mapping boils down to a matrix; we have some good theorems for existence of inverses / eigenvectors/exponential objects / etc.. So, LA sure is convenient . OTOH, it seems unlikely that any nontrivial mathematical system could ever be said to be thoroughly understood . Can't we always find new questions within any such framework that haven't been answered yet? I'm not firm enough with Gödel's incompleteness theorems to judge whether they are relevant here. The first incompleteness theorem says that discrete disciplines like number theory can't be both complete and consistent. Surely this is all the more true for e.g. topology. Is LA for some reason exempt from such arguments, or does it for some other reason deserve to be called the best understood branch of mathematics?","In a recent question , it was discussed how LA is a foundation to other branches of mathematics, be they pure or applied. One answer argued that linear problems are fully understood , and hence a natural target to reduce pretty much anything to. Now, it's evident enough that such a linearisation, if possible, tends to make hard things easier. Find a Hilbert space hidden in your domain, obtain an orthonormal basis, and bam , any point/state can be described as a mere sequence of numbers, any mapping boils down to a matrix; we have some good theorems for existence of inverses / eigenvectors/exponential objects / etc.. So, LA sure is convenient . OTOH, it seems unlikely that any nontrivial mathematical system could ever be said to be thoroughly understood . Can't we always find new questions within any such framework that haven't been answered yet? I'm not firm enough with Gödel's incompleteness theorems to judge whether they are relevant here. The first incompleteness theorem says that discrete disciplines like number theory can't be both complete and consistent. Surely this is all the more true for e.g. topology. Is LA for some reason exempt from such arguments, or does it for some other reason deserve to be called the best understood branch of mathematics?",,"['linear-algebra', 'soft-question', 'incompleteness']"
65,Intuitive explanation of a positive semidefinite matrix,Intuitive explanation of a positive semidefinite matrix,,"What is an intuitive explanation of a positive-semidefinite matrix? Or a simple example which gives more intuition for it rather than the bare definition. Say $x$ is some vector in space and $M$ is some operation on vectors. The definition is: A $n$ × $n$ Hermitian matrix M is called positive-semidefinite if $$x^{*} M x \geq 0$$ for all $x \in \mathbb{C}^n$ (or, all $x \in \mathbb{R}^n$ for the real matrix), where $x^*$ is the conjugate transpose of $x$.","What is an intuitive explanation of a positive-semidefinite matrix? Or a simple example which gives more intuition for it rather than the bare definition. Say $x$ is some vector in space and $M$ is some operation on vectors. The definition is: A $n$ × $n$ Hermitian matrix M is called positive-semidefinite if $$x^{*} M x \geq 0$$ for all $x \in \mathbb{C}^n$ (or, all $x \in \mathbb{R}^n$ for the real matrix), where $x^*$ is the conjugate transpose of $x$.",,"['linear-algebra', 'matrices']"
66,What exactly are eigen-things?,What exactly are eigen-things?,,"Wikipedia defines an eigenvector like this: An eigenvector of a square matrix is a non-zero vector that, when multiplied by the matrix, yields a vector that differs from the original vector at most by a multiplicative scalar. So basically in layman language: An eigenvector is a vector that when you multiply it by a square matrix, you get the same vector or the same vector multiplied by a scalar. There are a lot of terms which are related to this like eigenspaces and eigenvalues and eigenbases and such, which I don't quite understand, in fact, I don't understand at all. Can someone give an explanation connecting these terms? So that it is clear what they are and why they are related.","Wikipedia defines an eigenvector like this: An eigenvector of a square matrix is a non-zero vector that, when multiplied by the matrix, yields a vector that differs from the original vector at most by a multiplicative scalar. So basically in layman language: An eigenvector is a vector that when you multiply it by a square matrix, you get the same vector or the same vector multiplied by a scalar. There are a lot of terms which are related to this like eigenspaces and eigenvalues and eigenbases and such, which I don't quite understand, in fact, I don't understand at all. Can someone give an explanation connecting these terms? So that it is clear what they are and why they are related.",,"['linear-algebra', 'matrices', 'eigenvalues-eigenvectors']"
67,Do Diagonal Matrices Always Commute?,Do Diagonal Matrices Always Commute?,,"Let $A$ be an $n \times n$ matrix and let $\Lambda$ be an $n \times n$ diagonal matrix. Is it always the case that $A\Lambda = \Lambda A$ ? If not, when is it the case that $A \Lambda = \Lambda A$ ? If we restrict the diagonal entries of $\Lambda$ to being equal (i.e. $\Lambda = \text{diag}(a, a, \dots, a)$ ), then it is clear that $A\Lambda = AaI = aIA = \Lambda A$ . However, I can't seem to come up with an argument for the general case.","Let be an matrix and let be an diagonal matrix. Is it always the case that ? If not, when is it the case that ? If we restrict the diagonal entries of to being equal (i.e. ), then it is clear that . However, I can't seem to come up with an argument for the general case.","A n \times n \Lambda n \times n A\Lambda = \Lambda A A \Lambda = \Lambda A \Lambda \Lambda = \text{diag}(a, a, \dots, a) A\Lambda = AaI = aIA = \Lambda A","['linear-algebra', 'matrices']"
68,Eigenvalues of the rank one matrix $uv^T$,Eigenvalues of the rank one matrix,uv^T,"Suppose $A=uv^T$ where $u$ and $v$ are non-zero column vectors in ${\mathbb R}^n$, $n\geq 3$. $\lambda=0$ is an eigenvalue of $A$ since $A$ is not of full rank. $\lambda=v^Tu$ is also an eigenvalue of $A$ since $$Au = (uv^T)u=u(v^Tu)=(v^Tu)u.$$  Here is my question: Are there any other eigenvalues of $A$? Added: Thanks to Didier's comment and anon's answer, $A$ can not have other eigenvalues than $0$ and $v^Tu$. I would like to update the question: Can $A$ be diagonalizable?","Suppose $A=uv^T$ where $u$ and $v$ are non-zero column vectors in ${\mathbb R}^n$, $n\geq 3$. $\lambda=0$ is an eigenvalue of $A$ since $A$ is not of full rank. $\lambda=v^Tu$ is also an eigenvalue of $A$ since $$Au = (uv^T)u=u(v^Tu)=(v^Tu)u.$$  Here is my question: Are there any other eigenvalues of $A$? Added: Thanks to Didier's comment and anon's answer, $A$ can not have other eigenvalues than $0$ and $v^Tu$. I would like to update the question: Can $A$ be diagonalizable?",,"['linear-algebra', 'matrices']"
69,Do matrices $ AB $ and $ BA $ have the same minimal and characteristic polynomials?,Do matrices  and  have the same minimal and characteristic polynomials?, AB   BA ,"Let $ A, B $ be two square matrices of order $n$. Do $ AB $ and $ BA $ have same minimal and characteristic polynomials? I have a proof only if $ A$ or $ B $ is invertible. Is it true for all cases?","Let $ A, B $ be two square matrices of order $n$. Do $ AB $ and $ BA $ have same minimal and characteristic polynomials? I have a proof only if $ A$ or $ B $ is invertible. Is it true for all cases?",,"['linear-algebra', 'matrices', 'minimal-polynomials', 'characteristic-polynomial']"
70,What is the main difference between a vector space and a field?,What is the main difference between a vector space and a field?,,In my opinion both are almost same. However there should be some differenes like any two elements can be multiplied in a field but it is not allowed in vector space as only scalar multiplication is allowed where scalars are from the field. Could anyone give me atleast one counter- example where field and vector space are both same. Every field is a vector space but not every vectorspace is a field. I need an example for which a vector space is also a field. Thanks in advance. (I'm not from mathematical background.),In my opinion both are almost same. However there should be some differenes like any two elements can be multiplied in a field but it is not allowed in vector space as only scalar multiplication is allowed where scalars are from the field. Could anyone give me atleast one counter- example where field and vector space are both same. Every field is a vector space but not every vectorspace is a field. I need an example for which a vector space is also a field. Thanks in advance. (I'm not from mathematical background.),,"['linear-algebra', 'abstract-algebra', 'vector-spaces', 'field-theory']"
71,Expressing the determinant of a sum of two matrices?,Expressing the determinant of a sum of two matrices?,,"Can $\det(A + B)$ expressed in terms of $\det(A), \det(B), n$ where $A,B$ are $n\times n$ matrices? I made the edit to allow $n$ to be factored in.",Can expressed in terms of where are matrices? I made the edit to allow to be factored in.,"\det(A + B) \det(A), \det(B), n A,B n\times n n","['linear-algebra', 'matrices', 'multivariable-calculus', 'determinant']"
72,Matrix is conjugate to its own transpose,Matrix is conjugate to its own transpose,,"Mariano mentioned somewhere that everyone should prove once in their life that every matrix is conjugate to its transpose. I spent quite a bit of time on it now, and still could not prove it. At the risk of devaluing myself, might I ask someone else to show me a proof?","Mariano mentioned somewhere that everyone should prove once in their life that every matrix is conjugate to its transpose. I spent quite a bit of time on it now, and still could not prove it. At the risk of devaluing myself, might I ask someone else to show me a proof?",,"['linear-algebra', 'matrices', 'transpose']"
73,How do I tell if matrices are similar?,How do I tell if matrices are similar?,,"I have two $2\times 2$ matrices, $A$ and $B$, with the same determinant. I want to know if they are similar or not. I solved this by using a matrix called $S$: $$\left(\begin{array}{cc} a& b\\ c& d \end{array}\right)$$ and its inverse in terms of $a$, $b$, $c$, and $d$, then showing that there was no solution to $A = SBS^{-1}$. That worked fine, but what will I do if I have $3\times 3$ or $9\times 9$ matrices? I can't possibly make system that complex and solve it.  How can I know if any two matrices represent the ""same"" linear transformation with different bases? That is, how can I find $S$ that change of basis matrix? I tried making $A$ and $B$ into linear transformations... but without the bases for the linear transformations I had no way of comparing them. (I have read that similar matrices will have the same eigenvalues... and the same ""trace"" --but my class has not studied these yet. Also, it may be the case that some matrices with the same trace and eigenvalues are not similar so this will not solve my problem.) I have one idea. Maybe if I look at the reduced col. and row echelon forms that will tell me something about the basis for the linear transformation? I'm not really certain how this would work though? Please help.","I have two $2\times 2$ matrices, $A$ and $B$, with the same determinant. I want to know if they are similar or not. I solved this by using a matrix called $S$: $$\left(\begin{array}{cc} a& b\\ c& d \end{array}\right)$$ and its inverse in terms of $a$, $b$, $c$, and $d$, then showing that there was no solution to $A = SBS^{-1}$. That worked fine, but what will I do if I have $3\times 3$ or $9\times 9$ matrices? I can't possibly make system that complex and solve it.  How can I know if any two matrices represent the ""same"" linear transformation with different bases? That is, how can I find $S$ that change of basis matrix? I tried making $A$ and $B$ into linear transformations... but without the bases for the linear transformations I had no way of comparing them. (I have read that similar matrices will have the same eigenvalues... and the same ""trace"" --but my class has not studied these yet. Also, it may be the case that some matrices with the same trace and eigenvalues are not similar so this will not solve my problem.) I have one idea. Maybe if I look at the reduced col. and row echelon forms that will tell me something about the basis for the linear transformation? I'm not really certain how this would work though? Please help.",,"['linear-algebra', 'matrices', 'similar-matrices']"
74,"Determinant of a rank $1$ update of a scalar matrix, or characteristic polynomial of a rank $1$ matrix","Determinant of a rank  update of a scalar matrix, or characteristic polynomial of a rank  matrix",1 1,"This question aims to create an "" abstract duplicate ""  of numerous questions that ask about determinants of specific matrices (I may have missed a few): Characteristic polynomial of a matrix of $1$'s Eigenvalues of the rank one matrix $uv^T$ Calculating $\det(A+I)$ for matrix $A$ defined by products How to calculate the determinant of all-ones matrix minus the identity? Determinant of a specially structured matrix ($a$'s on the diagonal, all other entries equal to $b$) Determinant of a special $n\times n$ matrix Find the eigenvalues of a matrix with ones in the diagonal, and all the other elements equal Determinant of a matrix with $t$ in all off-diagonal entries. Characteristic polynomial - using rank? Caclulate $X_A(x) $ and $m_A(x) $ of a matrix $A\in \mathbb{C}^{n\times n}:a_{ij}=i\cdot j$ Determinant of rank-one perturbations of (invertible) matrices The general question of this type is Let $A$ be a square matrix of rank $~1$ , let $I$ the identity matrix of the same size, and $\lambda$ a scalar. What is the determinant of $A+\lambda I$ ? A clearly very closely related question is What is the characteristic polynomial of a matrix $A$ of rank $~1$ ?","This question aims to create an "" abstract duplicate ""  of numerous questions that ask about determinants of specific matrices (I may have missed a few): Characteristic polynomial of a matrix of $1$'s Eigenvalues of the rank one matrix $uv^T$ Calculating $\det(A+I)$ for matrix $A$ defined by products How to calculate the determinant of all-ones matrix minus the identity? Determinant of a specially structured matrix ($a$'s on the diagonal, all other entries equal to $b$) Determinant of a special $n\times n$ matrix Find the eigenvalues of a matrix with ones in the diagonal, and all the other elements equal Determinant of a matrix with $t$ in all off-diagonal entries. Characteristic polynomial - using rank? Caclulate $X_A(x) $ and $m_A(x) $ of a matrix $A\in \mathbb{C}^{n\times n}:a_{ij}=i\cdot j$ Determinant of rank-one perturbations of (invertible) matrices The general question of this type is Let be a square matrix of rank , let the identity matrix of the same size, and a scalar. What is the determinant of ? A clearly very closely related question is What is the characteristic polynomial of a matrix of rank ?",A ~1 I \lambda A+\lambda I A ~1,"['linear-algebra', 'matrices', 'determinant', 'characteristic-polynomial', 'faq']"
75,Is the product of symmetric positive semidefinite matrices positive definite?,Is the product of symmetric positive semidefinite matrices positive definite?,,I see on Wikipedia that the product of two commuting symmetric positive definite matrices is also positive definite. Does the same result hold for the product of two positive semidefinite matrices? My proof of the positive definite case falls apart for the semidefinite case because of the possibility of division by zero...,I see on Wikipedia that the product of two commuting symmetric positive definite matrices is also positive definite. Does the same result hold for the product of two positive semidefinite matrices? My proof of the positive definite case falls apart for the semidefinite case because of the possibility of division by zero...,,"['linear-algebra', 'matrices', 'positive-semidefinite']"
76,Is linear algebra laying the foundation for something important?,Is linear algebra laying the foundation for something important?,,"I'm majoring in mathematics and currently enrolled in Linear Algebra. It's very different, but I like it (I think). My question is this: What doors does this course open? (I saw a post about Linear Algebra being the foundation for Applied Mathematics -- but I like doing math for the sake of math, not so much the applications.) Is this a stand-alone class, or will the new things I'm learning come into play later on?","I'm majoring in mathematics and currently enrolled in Linear Algebra. It's very different, but I like it (I think). My question is this: What doors does this course open? (I saw a post about Linear Algebra being the foundation for Applied Mathematics -- but I like doing math for the sake of math, not so much the applications.) Is this a stand-alone class, or will the new things I'm learning come into play later on?",,"['linear-algebra', 'soft-question']"
77,"Why does this ""miracle method"" for matrix inversion work?","Why does this ""miracle method"" for matrix inversion work?",,"Recently, I answered this question about matrix invertibility using a solution technique I called a "" miracle method ."" The question and answer are reproduced below: Problem: Let $A$ be a matrix satisfying $A^3 = 2I$ . Show that $B = A^2 - 2A + 2I$ is invertible. Solution: Suspend your disbelief for a moment and suppose $A$ and $B$ were scalars, not matrices. Then, by power series expansion, we would simply be looking for $$ \frac{1}{B} = \frac{1}{A^2 - 2A + 2} = \frac{1}{2}+\frac{A}{2}+\frac{A^2}{4}-\frac{A^4}{8}-\frac{A^5}{8} + \cdots$$ where the coefficient of $A^n$ is $$ c_n = \frac{1+i}{2^{n+2}} \left((1-i)^n-i (1+i)^n\right). $$ But we know that $A^3 = 2$ , so $$ \frac{1}{2}+\frac{A}{2}+\frac{A^2}{4}-\frac{A^4}{8}-\frac{A^5}{8} + \cdots = \frac{1}{2}+\frac{A}{2}+\frac{A^2}{4}-\frac{A}{4}-\frac{A^2}{4} + \cdots $$ and by summing the resulting coefficients on $1$ , $A$ , and $A^2$ , we find that $$ \frac{1}{B} = \frac{2}{5} + \frac{3}{10}A + \frac{1}{10}A^2. $$ Now, what we've just done should be total nonsense if $A$ and $B$ are really matrices, not scalars. But try setting $B^{-1} = \frac{2}{5}I + \frac{3}{10}A + \frac{1}{10}A^2$ , compute the product $BB^{-1}$ , and you'll find that, miraculously , this answer works! I discovered this solution technique some time ago while exploring a similar problem in Wolfram Mathematica . However, I have no idea why any of these manipulations should produce a meaningful answer when scalar and matrix inversion are such different operations. Why does this method work? Is there something deeper going on here than a serendipitous coincidence in series expansion coefficients?","Recently, I answered this question about matrix invertibility using a solution technique I called a "" miracle method ."" The question and answer are reproduced below: Problem: Let be a matrix satisfying . Show that is invertible. Solution: Suspend your disbelief for a moment and suppose and were scalars, not matrices. Then, by power series expansion, we would simply be looking for where the coefficient of is But we know that , so and by summing the resulting coefficients on , , and , we find that Now, what we've just done should be total nonsense if and are really matrices, not scalars. But try setting , compute the product , and you'll find that, miraculously , this answer works! I discovered this solution technique some time ago while exploring a similar problem in Wolfram Mathematica . However, I have no idea why any of these manipulations should produce a meaningful answer when scalar and matrix inversion are such different operations. Why does this method work? Is there something deeper going on here than a serendipitous coincidence in series expansion coefficients?",A A^3 = 2I B = A^2 - 2A + 2I A B  \frac{1}{B} = \frac{1}{A^2 - 2A + 2} = \frac{1}{2}+\frac{A}{2}+\frac{A^2}{4}-\frac{A^4}{8}-\frac{A^5}{8} + \cdots A^n  c_n = \frac{1+i}{2^{n+2}} \left((1-i)^n-i (1+i)^n\right).  A^3 = 2  \frac{1}{2}+\frac{A}{2}+\frac{A^2}{4}-\frac{A^4}{8}-\frac{A^5}{8} + \cdots = \frac{1}{2}+\frac{A}{2}+\frac{A^2}{4}-\frac{A}{4}-\frac{A^2}{4} + \cdots  1 A A^2  \frac{1}{B} = \frac{2}{5} + \frac{3}{10}A + \frac{1}{10}A^2.  A B B^{-1} = \frac{2}{5}I + \frac{3}{10}A + \frac{1}{10}A^2 BB^{-1},"['linear-algebra', 'matrices', 'inverse']"
78,The relation between trace and determinant of a matrix,The relation between trace and determinant of a matrix,,Let $M$ be a symmetric $n \times n$ matrix. Is there any equality or inequality that relates the trace and determinant of $M$?,Let $M$ be a symmetric $n \times n$ matrix. Is there any equality or inequality that relates the trace and determinant of $M$?,,"['linear-algebra', 'matrices', 'determinant', 'trace']"
79,Order of general- and special linear groups over finite fields.,Order of general- and special linear groups over finite fields.,,"Let $\mathbb{F}_3$ be the field with three elements. Let $n\geq 1$. How many elements do the following groups have? $\text{GL}_n(\mathbb{F}_3)$ $\text{SL}_n(\mathbb{F}_3)$ Here GL is the general linear group , the group of invertible n × n matrices, and SL is the special linear group , the group of n × n matrices with determinant 1.","Let $\mathbb{F}_3$ be the field with three elements. Let $n\geq 1$. How many elements do the following groups have? $\text{GL}_n(\mathbb{F}_3)$ $\text{SL}_n(\mathbb{F}_3)$ Here GL is the general linear group , the group of invertible n × n matrices, and SL is the special linear group , the group of n × n matrices with determinant 1.",,"['linear-algebra', 'abstract-algebra']"
80,Distance/Similarity between two matrices,Distance/Similarity between two matrices,,"I'm in the process of writing an application which identifies the closest matrix from a set of square matrices $M$ to a given square matrix $A$. The closest can be defined as the most similar. I think finding the distance between two given matrices is a fair approach since the smallest Euclidean distance is used to identify the closeness of vectors. I found that the distance between two matrices ($A,B$) could be calculated using the Frobenius distance $F$: $$F_{A,B} = \sqrt{trace((A-B)*(A-B)')} $$ where $B'$ represents the conjugate transpose of B. I have the following points I need to clarify Is the distance between matrices a fair measure of similarity? If distance is used, is Frobenius distance a fair measure for this problem? any other suggestions?","I'm in the process of writing an application which identifies the closest matrix from a set of square matrices $M$ to a given square matrix $A$. The closest can be defined as the most similar. I think finding the distance between two given matrices is a fair approach since the smallest Euclidean distance is used to identify the closeness of vectors. I found that the distance between two matrices ($A,B$) could be calculated using the Frobenius distance $F$: $$F_{A,B} = \sqrt{trace((A-B)*(A-B)')} $$ where $B'$ represents the conjugate transpose of B. I have the following points I need to clarify Is the distance between matrices a fair measure of similarity? If distance is used, is Frobenius distance a fair measure for this problem? any other suggestions?",,"['linear-algebra', 'matrices', 'numerical-linear-algebra']"
81,Geometric intuition for the tensor product of vector spaces,Geometric intuition for the tensor product of vector spaces,,"First of all, I am very comfortable with the tensor product of vector spaces. I am also very familiar with the well-known generalizations, in particular the theory of monoidal categories. I have gained quite some intuition for tensor products and can work with them. Therefore, my question is not about the definition of tensor products, nor is it about its properties. It is rather about the mental images. My intuition for tensor products was never really geometric . Well, except for the tensor product of commutative algebras, which corresponds to the fiber product of the corresponding affine schemes. But let's just stick to real vector spaces here, for which I have some geometric intuition, for example from classical analytic geometry. The direct product of two (or more) vector spaces is quite easy to imagine: There are two (or more) ""directions"" or ""dimensions"" in which we ""insert"" the vectors of the individual vector spaces. For example, the direct product of a line with a plane is a three-dimensional space. The exterior algebra of a vector space consists of ""blades"", as is nicely explained in the Wikipedia article . Now what about the tensor product of two finite-dimensional real vector spaces $V,W$? Of course $V \otimes W$ is a direct product of $\dim(V)$ copies of $W$, but this description is not intrinsic, and also it doesn't really incorporate the symmetry $V \otimes W \cong W \otimes V$. How can we describe $V \otimes W$ geometrically in terms of $V$ and $W$? This description should be intrinsic and symmetric. Note that SE/115630 basically asked the same, but received no actual answer. The answer given at SE/309838 discusses where tensor products are used in differential geometry for more abstract notions such as tensor fields and tensor bundles, but this doesn't answer the question either. (Even if my question gets closed as a duplicate, then I hope that the other questions receive more attention and answers.) More generally, I would like to ask for a geometric picture of the tensor product of two vector bundles on nice topological spaces. For example, tensoring with a line bundle is some kind of twisting. But this is still some kind of vague. For example, consider the Möbius strip on the circle $S^1$, and pull it back to the torus $S^1 \times S^1$ along the first projection. Do the same with the second projection, and then tensor both. We get a line bundle on the torus, okay, but how does it look like geometrically? Perhaps the following related question is easier to answer: Assume we have a geometric understanding of two linear maps $f : \mathbb{R}^n \to \mathbb{R}^m$, $g : \mathbb{R}^{n'} \to \mathbb{R}^{m'}$. Then, how can we imagine their tensor product $f \otimes g : \mathbb{R}^n \otimes \mathbb{R}^{n'} \to \mathbb{R}^m \otimes \mathbb{R}^{m'}$ or the corresponding linear map $\mathbb{R}^{n n'} \to \mathbb{R}^{m m'}$ geometrically? This is connected to the question about vector bundles via their cocycle description.","First of all, I am very comfortable with the tensor product of vector spaces. I am also very familiar with the well-known generalizations, in particular the theory of monoidal categories. I have gained quite some intuition for tensor products and can work with them. Therefore, my question is not about the definition of tensor products, nor is it about its properties. It is rather about the mental images. My intuition for tensor products was never really geometric . Well, except for the tensor product of commutative algebras, which corresponds to the fiber product of the corresponding affine schemes. But let's just stick to real vector spaces here, for which I have some geometric intuition, for example from classical analytic geometry. The direct product of two (or more) vector spaces is quite easy to imagine: There are two (or more) ""directions"" or ""dimensions"" in which we ""insert"" the vectors of the individual vector spaces. For example, the direct product of a line with a plane is a three-dimensional space. The exterior algebra of a vector space consists of ""blades"", as is nicely explained in the Wikipedia article . Now what about the tensor product of two finite-dimensional real vector spaces $V,W$? Of course $V \otimes W$ is a direct product of $\dim(V)$ copies of $W$, but this description is not intrinsic, and also it doesn't really incorporate the symmetry $V \otimes W \cong W \otimes V$. How can we describe $V \otimes W$ geometrically in terms of $V$ and $W$? This description should be intrinsic and symmetric. Note that SE/115630 basically asked the same, but received no actual answer. The answer given at SE/309838 discusses where tensor products are used in differential geometry for more abstract notions such as tensor fields and tensor bundles, but this doesn't answer the question either. (Even if my question gets closed as a duplicate, then I hope that the other questions receive more attention and answers.) More generally, I would like to ask for a geometric picture of the tensor product of two vector bundles on nice topological spaces. For example, tensoring with a line bundle is some kind of twisting. But this is still some kind of vague. For example, consider the Möbius strip on the circle $S^1$, and pull it back to the torus $S^1 \times S^1$ along the first projection. Do the same with the second projection, and then tensor both. We get a line bundle on the torus, okay, but how does it look like geometrically? Perhaps the following related question is easier to answer: Assume we have a geometric understanding of two linear maps $f : \mathbb{R}^n \to \mathbb{R}^m$, $g : \mathbb{R}^{n'} \to \mathbb{R}^{m'}$. Then, how can we imagine their tensor product $f \otimes g : \mathbb{R}^n \otimes \mathbb{R}^{n'} \to \mathbb{R}^m \otimes \mathbb{R}^{m'}$ or the corresponding linear map $\mathbb{R}^{n n'} \to \mathbb{R}^{m m'}$ geometrically? This is connected to the question about vector bundles via their cocycle description.",,"['linear-algebra', 'geometry', 'intuition', 'tensor-products', 'vector-bundles']"
82,Is this determinant identity known?,Is this determinant identity known?,,"Let $A$ be an $n \times n$ matrix that is 'almost upper triangular' in the following sense: entries on and above the main diagonal can be whatever they want, entries on the diagonal just below the main diagonal all equal -1 and entries on even lower diagonals equal zero. For example: $$\begin{pmatrix}  a_{11} & a_{12} & a_{13}  & a_{14} \\  -1 & a_{22} & a_{23}  & a_{24} \\ 0 & -1 & a_{33}  & a_{34} \\ 0 & 0  & -1  & a_{44} \end{pmatrix}$$ I accidentally found a very nice expression for the determinant of these matrices. For $i \leq j$ in $\mathbb{N}$ let $[i, \ldots, j]$ denote the interval of successive integers starting at $i$ and ending at $j$. There are $2^{n-1}$ ways to partition the interval $[1, \ldots, n]$ into subintervals (e.g. $[1, 2, 3][4][5, \ldots, n]$ would be such a partition): for each of the $n-1$ comma's in $[1, \ldots, n]$ we can decide whether or not to replace it by a ']['. Now in order to compute the determinant of matrix $A$ of the form above, all we need to do is the following: Write down the $2^{n-1}$ partitions of $[1, \ldots, n]$ into subintervals. In each partition replace each subinterval $[i, \ldots, j]$ with the matrix-entry $a_{ij}$ and interpret the resulting string of matrix entries as a product Sum all the $2^{n-1}$ terms. (No minus signs involved!) The result is $\det A$. Weird, huh? As a free bonus we note that the determinant expression for the complete Bell polynomials can be computed in this way. Since this identity is not terribly hard to prove my question is: is this known? But also a more conceptual explanation, perhaps linking the interval partitions here to the set partitions in the definition of Bell polynomial would be welcome. UPDATE: Per request of Darij Grinberg I will give a proof and some easy applications to financial mathematics. Proof by induction on $n$: Write $A_{ij}$ for the matrix obtained by crossing out row $i$ and column $j$ from $A$. We divide the partitions of the interval $[1, \ldots, n]$ into two types: those that end in $[n]$ (short-tailed) and those that end in $[i, \ldots, n]$ for some $i < n$ (long tailed). The long tailed partitions do not contain any intervals starting with $n$ or ending in $n-1$ so if we apply our three step process described above to this set only we obtain an expression in the elements of $A_{n, n-1}$ which (after the right renumberings) can be seen to equal $\det(A_{n, n-1})$ by the induction hypothesis. When we apply our three step procedure to the short tailed partitions we obtain in an even more straightforward application of the induction hypothesis the number $a_{n,n} \cdot \det(A_{n, n})$. So what remains to be shown is that $\det(A) = \det(A_{n, n-1}) + a_{n,n}\det(A_{n,n})$ but this is just Laplace's cofactor expansion formula for the determinant, applied to the last row of $A$. Interpretation of the determinant in case $A$ is constant along diagonals: Suppose $A$ has $1$'s on the first (= main) and second (= the one just above) diagonal and $0$'s everywhere else (except for the $-1$'s on the zeroth diagonal of course) then the above states that $\det A$ equals the number of ways of tiling a path of length $n$ by tiles of length 1 and 2, which is well known to be the $n$'th Fibonacci number. By putting $1$'s on different diagonals we can similarly find determinant identities for the Tribonacci numbers or, more financially interesting, for the number of ways to pay an $n/100$-dollar bill with coins only. Staying in the realm of money: if instead of just $0$'s and $1$'s we fill the diagonals with numbers from the real interval $[0, 1]$ (just one number repeated over and over per diagonal) the above count turns into a probability. For instance if we take $n= 40$, put $0$'s in the main diagonal, $1/12$'s in the second diagonal, $1/6$ in the third diagonal etc, upto $0$'s in diagonals 13 to 40, the determinant of $A$ will give us the probability of landing exactly on Go (rather than just passing it) after one round trip in a game of Monopoly. What I would be really interested in are interpretations of these determinants in case the entries on each diagonal are not constant (but follow some other pattern enabling the interpretation).","Let $A$ be an $n \times n$ matrix that is 'almost upper triangular' in the following sense: entries on and above the main diagonal can be whatever they want, entries on the diagonal just below the main diagonal all equal -1 and entries on even lower diagonals equal zero. For example: $$\begin{pmatrix}  a_{11} & a_{12} & a_{13}  & a_{14} \\  -1 & a_{22} & a_{23}  & a_{24} \\ 0 & -1 & a_{33}  & a_{34} \\ 0 & 0  & -1  & a_{44} \end{pmatrix}$$ I accidentally found a very nice expression for the determinant of these matrices. For $i \leq j$ in $\mathbb{N}$ let $[i, \ldots, j]$ denote the interval of successive integers starting at $i$ and ending at $j$. There are $2^{n-1}$ ways to partition the interval $[1, \ldots, n]$ into subintervals (e.g. $[1, 2, 3][4][5, \ldots, n]$ would be such a partition): for each of the $n-1$ comma's in $[1, \ldots, n]$ we can decide whether or not to replace it by a ']['. Now in order to compute the determinant of matrix $A$ of the form above, all we need to do is the following: Write down the $2^{n-1}$ partitions of $[1, \ldots, n]$ into subintervals. In each partition replace each subinterval $[i, \ldots, j]$ with the matrix-entry $a_{ij}$ and interpret the resulting string of matrix entries as a product Sum all the $2^{n-1}$ terms. (No minus signs involved!) The result is $\det A$. Weird, huh? As a free bonus we note that the determinant expression for the complete Bell polynomials can be computed in this way. Since this identity is not terribly hard to prove my question is: is this known? But also a more conceptual explanation, perhaps linking the interval partitions here to the set partitions in the definition of Bell polynomial would be welcome. UPDATE: Per request of Darij Grinberg I will give a proof and some easy applications to financial mathematics. Proof by induction on $n$: Write $A_{ij}$ for the matrix obtained by crossing out row $i$ and column $j$ from $A$. We divide the partitions of the interval $[1, \ldots, n]$ into two types: those that end in $[n]$ (short-tailed) and those that end in $[i, \ldots, n]$ for some $i < n$ (long tailed). The long tailed partitions do not contain any intervals starting with $n$ or ending in $n-1$ so if we apply our three step process described above to this set only we obtain an expression in the elements of $A_{n, n-1}$ which (after the right renumberings) can be seen to equal $\det(A_{n, n-1})$ by the induction hypothesis. When we apply our three step procedure to the short tailed partitions we obtain in an even more straightforward application of the induction hypothesis the number $a_{n,n} \cdot \det(A_{n, n})$. So what remains to be shown is that $\det(A) = \det(A_{n, n-1}) + a_{n,n}\det(A_{n,n})$ but this is just Laplace's cofactor expansion formula for the determinant, applied to the last row of $A$. Interpretation of the determinant in case $A$ is constant along diagonals: Suppose $A$ has $1$'s on the first (= main) and second (= the one just above) diagonal and $0$'s everywhere else (except for the $-1$'s on the zeroth diagonal of course) then the above states that $\det A$ equals the number of ways of tiling a path of length $n$ by tiles of length 1 and 2, which is well known to be the $n$'th Fibonacci number. By putting $1$'s on different diagonals we can similarly find determinant identities for the Tribonacci numbers or, more financially interesting, for the number of ways to pay an $n/100$-dollar bill with coins only. Staying in the realm of money: if instead of just $0$'s and $1$'s we fill the diagonals with numbers from the real interval $[0, 1]$ (just one number repeated over and over per diagonal) the above count turns into a probability. For instance if we take $n= 40$, put $0$'s in the main diagonal, $1/12$'s in the second diagonal, $1/6$ in the third diagonal etc, upto $0$'s in diagonals 13 to 40, the determinant of $A$ will give us the probability of landing exactly on Go (rather than just passing it) after one round trip in a game of Monopoly. What I would be really interested in are interpretations of these determinants in case the entries on each diagonal are not constant (but follow some other pattern enabling the interpretation).",,"['linear-algebra', 'combinatorics', 'determinant', 'set-partition']"
83,What is the largest eigenvalue of the following matrix?,What is the largest eigenvalue of the following matrix?,,"Find the largest eigenvalue of the following matrix   $$\begin{bmatrix}  1 &  4 & 16\\  4 & 16 &  1\\ 16 &  1 &  4 \end{bmatrix}$$ This matrix is symmetric and, thus, the eigenvalues are real. I solved for the possible eigenvalues and, fortunately, I found that the answer is $21$. My approach: The determinant on simplification leads to the following third degree polynomial. $$\begin{vmatrix} 1-\lambda & 4 &16\\ 4 &16-\lambda&1\\ 16&1&4-\lambda \end{vmatrix} = \lambda^3-21\lambda^2-189\lambda+3969.$$ At a first glance seen how many people find the roots of this polynomial  with pen and paper using elementary algebra. I managed to find the roots and they are $21$, $\sqrt{189}$, and $-\sqrt{189}$ and the largest value is $21$. Now the problem is that my professor stared at this matrix for a few seconds and said that the largest eigenvalue is $21$. Obviously, he hadn't gone through all these steps to find that answer. So what enabled him answer this in a few seconds? Please don't say that he already knew the answer. Is there any easy way to find the answer in a few seconds? What property of this matrix makes it easy to compute that answer? Thanks in advance.","Find the largest eigenvalue of the following matrix   $$\begin{bmatrix}  1 &  4 & 16\\  4 & 16 &  1\\ 16 &  1 &  4 \end{bmatrix}$$ This matrix is symmetric and, thus, the eigenvalues are real. I solved for the possible eigenvalues and, fortunately, I found that the answer is $21$. My approach: The determinant on simplification leads to the following third degree polynomial. $$\begin{vmatrix} 1-\lambda & 4 &16\\ 4 &16-\lambda&1\\ 16&1&4-\lambda \end{vmatrix} = \lambda^3-21\lambda^2-189\lambda+3969.$$ At a first glance seen how many people find the roots of this polynomial  with pen and paper using elementary algebra. I managed to find the roots and they are $21$, $\sqrt{189}$, and $-\sqrt{189}$ and the largest value is $21$. Now the problem is that my professor stared at this matrix for a few seconds and said that the largest eigenvalue is $21$. Obviously, he hadn't gone through all these steps to find that answer. So what enabled him answer this in a few seconds? Please don't say that he already knew the answer. Is there any easy way to find the answer in a few seconds? What property of this matrix makes it easy to compute that answer? Thanks in advance.",,"['linear-algebra', 'matrices', 'eigenvalues-eigenvectors', 'symmetric-matrices']"
84,Proof that the largest eigenvalue of a stochastic matrix is $1$,Proof that the largest eigenvalue of a stochastic matrix is,1,"The largest eigenvalue of a stochastic matrix (i.e. a matrix whose entries are positive and whose rows add up to $1$) is $1$. Wikipedia marks this as a special case of the Perron-Frobenius theorem , but I wonder if there is a simpler (more direct) way to demonstrate this result.","The largest eigenvalue of a stochastic matrix (i.e. a matrix whose entries are positive and whose rows add up to $1$) is $1$. Wikipedia marks this as a special case of the Perron-Frobenius theorem , but I wonder if there is a simpler (more direct) way to demonstrate this result.",,"['linear-algebra', 'matrices', 'eigenvalues-eigenvectors', 'stochastic-matrices']"
85,Matrices commute if and only if they share a common basis of eigenvectors?,Matrices commute if and only if they share a common basis of eigenvectors?,,I've come across a paper that mentions the fact that matrices commute if and only if they share a common basis of eigenvectors. Where can I find a proof of this statement?,I've come across a paper that mentions the fact that matrices commute if and only if they share a common basis of eigenvectors. Where can I find a proof of this statement?,,"['linear-algebra', 'matrices', 'reference-request', 'eigenvalues-eigenvectors']"
86,"Is the rank of a matrix the same of its transpose? If yes, how can I prove it?","Is the rank of a matrix the same of its transpose? If yes, how can I prove it?",,"I am auditing a Linear Algebra class, and today we were taught about the rank of a matrix. The definition was given from the row point of view: ""The rank of a matrix A is the number   of non-zero rows in the reduced   row-echelon form of A"". The lecturer then explained that if the matrix $A$ has size $m  \times n$ , then $rank(A) \leq m$ and $rank(A) \leq n$ . The way I had been taught about rank was that it was the smallest of the number of rows bringing new information the number of columns bringing new information. I don't see how that would change if we transposed the matrix, so I said in the lecture: ""then the rank of a matrix is the same of its transpose, right?"" And the lecturer said: ""oh, not so fast! Hang on, I have to think about it"". As the class has about 100 students and the lecturer was just substituting for the ""normal"" lecturer, he was probably a bit nervous, so he just went on with the lecture. I have tested ""my theory"" with one matrix and it works, but even if I tried with 100 matrices and it worked, I wouldn't have proven that it always works because there might be a case where it doesn't. So my question is first whether I am right, that is, whether the rank of a matrix is the same as the rank of its transpose, and second, if that is true, how can I prove it? Thanks  :)","I am auditing a Linear Algebra class, and today we were taught about the rank of a matrix. The definition was given from the row point of view: ""The rank of a matrix A is the number   of non-zero rows in the reduced   row-echelon form of A"". The lecturer then explained that if the matrix has size , then and . The way I had been taught about rank was that it was the smallest of the number of rows bringing new information the number of columns bringing new information. I don't see how that would change if we transposed the matrix, so I said in the lecture: ""then the rank of a matrix is the same of its transpose, right?"" And the lecturer said: ""oh, not so fast! Hang on, I have to think about it"". As the class has about 100 students and the lecturer was just substituting for the ""normal"" lecturer, he was probably a bit nervous, so he just went on with the lecture. I have tested ""my theory"" with one matrix and it works, but even if I tried with 100 matrices and it worked, I wouldn't have proven that it always works because there might be a case where it doesn't. So my question is first whether I am right, that is, whether the rank of a matrix is the same as the rank of its transpose, and second, if that is true, how can I prove it? Thanks  :)","A m 
\times n rank(A) \leq m rank(A) \leq n","['linear-algebra', 'matrices', 'matrix-rank', 'transpose']"
87,"How to prove and interpret $\operatorname{rank}(AB) \leq \operatorname{min}(\operatorname{rank}(A), \operatorname{rank}(B))$?",How to prove and interpret ?,"\operatorname{rank}(AB) \leq \operatorname{min}(\operatorname{rank}(A), \operatorname{rank}(B))","Let $A$ and $B$ be two matrices which can be multiplied. Then $$\operatorname{rank}(AB) \leq \operatorname{min}(\operatorname{rank}(A), \operatorname{rank}(B)).$$ I proved $\operatorname{rank}(AB) \leq \operatorname{rank}(B)$ by interpreting $AB$ as a composition of linear maps, observing that $\operatorname{ker}(B) \subseteq \operatorname{ker}(AB)$ and using the kernel-image dimension formula. This also provides, in my opinion, a nice interpretation: if non-stable, under subsequent compositions the kernel can only get bigger, and the image can only get smaller, in a sort of loss of information . How do you manage $\operatorname{rank}(AB) \leq \operatorname{rank}(A)$ ? Is there a nice interpretation like the previous one?","Let and be two matrices which can be multiplied. Then I proved by interpreting as a composition of linear maps, observing that and using the kernel-image dimension formula. This also provides, in my opinion, a nice interpretation: if non-stable, under subsequent compositions the kernel can only get bigger, and the image can only get smaller, in a sort of loss of information . How do you manage ? Is there a nice interpretation like the previous one?","A B \operatorname{rank}(AB) \leq \operatorname{min}(\operatorname{rank}(A), \operatorname{rank}(B)). \operatorname{rank}(AB) \leq \operatorname{rank}(B) AB \operatorname{ker}(B) \subseteq \operatorname{ker}(AB) \operatorname{rank}(AB) \leq \operatorname{rank}(A)","['linear-algebra', 'matrices']"
88,What are some applications of elementary linear algebra outside of math?,What are some applications of elementary linear algebra outside of math?,,"I'm TAing linear algebra next quarter, and it strikes me that I only know one example of an application I can present to my students. I'm looking for applications of elementary linear algebra outside of mathematics that I might talk about in discussion section. In our class, we cover the basics (linear transformations; matrices; subspaces of $\Bbb R^n$; rank-nullity), orthogonal matrices and the dot product (incl. least squares!), diagonalization, quadratic forms, and singular-value decomposition. Showing my ignorance, the only application of these I know is the one that was presented in the linear algebra class I took: representing dynamical systems as Markov processes, and diagonalizing the matrix involved to get a nice formula for the $n$th state of the system. But surely there are more than these. What are some applications of the linear algebra covered in a first course that can motivate the subject for students?","I'm TAing linear algebra next quarter, and it strikes me that I only know one example of an application I can present to my students. I'm looking for applications of elementary linear algebra outside of mathematics that I might talk about in discussion section. In our class, we cover the basics (linear transformations; matrices; subspaces of $\Bbb R^n$; rank-nullity), orthogonal matrices and the dot product (incl. least squares!), diagonalization, quadratic forms, and singular-value decomposition. Showing my ignorance, the only application of these I know is the one that was presented in the linear algebra class I took: representing dynamical systems as Markov processes, and diagonalizing the matrix involved to get a nice formula for the $n$th state of the system. But surely there are more than these. What are some applications of the linear algebra covered in a first course that can motivate the subject for students?",,"['linear-algebra', 'matrices']"
89,Why is $A^TA$ invertible if $A$ has independent columns?,Why is  invertible if  has independent columns?,A^TA A,"How can I understand that $A^TA$ is invertible if $A$ has independent columns? I found a similar question , phrased the other way around, so I tried to use the theorem $$ rank(A^TA) \le min(rank(A^T),rank(A)) $$ Given $rank(A) = rank(A^T) = n$ and $A^TA$ produces an $n\times n$ matrix, I can't seem to prove that $rank(A^TA)$ is actually $n$. I also tried to look at the question another way with the matrices $$ A^TA = \begin{bmatrix}a_1^T \\ a_2^T \\ \ldots \\ a_n^T \end{bmatrix}   \begin{bmatrix}a_1 a_2 \ldots a_n \end{bmatrix} = \begin{bmatrix}A^Ta_1 A^Ta^2 \ldots A^Ta_n\end{bmatrix} $$ But I still can't seem to show that $A^TA$ is invertible. So, how should I get a better understanding of why $A^TA$ is invertible if $A$ has independent columns?","How can I understand that $A^TA$ is invertible if $A$ has independent columns? I found a similar question , phrased the other way around, so I tried to use the theorem $$ rank(A^TA) \le min(rank(A^T),rank(A)) $$ Given $rank(A) = rank(A^T) = n$ and $A^TA$ produces an $n\times n$ matrix, I can't seem to prove that $rank(A^TA)$ is actually $n$. I also tried to look at the question another way with the matrices $$ A^TA = \begin{bmatrix}a_1^T \\ a_2^T \\ \ldots \\ a_n^T \end{bmatrix}   \begin{bmatrix}a_1 a_2 \ldots a_n \end{bmatrix} = \begin{bmatrix}A^Ta_1 A^Ta^2 \ldots A^Ta_n\end{bmatrix} $$ But I still can't seem to show that $A^TA$ is invertible. So, how should I get a better understanding of why $A^TA$ is invertible if $A$ has independent columns?",,"['linear-algebra', 'matrices', 'symmetric-matrices']"
90,Does non-symmetric positive definite matrix have positive eigenvalues?,Does non-symmetric positive definite matrix have positive eigenvalues?,,"I found out that there exist positive definite matrices that are non-symmetric, and I know that symmetric positive definite matrices have positive eigenvalues. Does this hold for non-symmetric matrices as well?","I found out that there exist positive definite matrices that are non-symmetric, and I know that symmetric positive definite matrices have positive eigenvalues. Does this hold for non-symmetric matrices as well?",,"['linear-algebra', 'matrices']"
91,Similar matrices and field extensions,Similar matrices and field extensions,,"Given a field $F$ and a subfield $K$ of $F$. Let $A$, $B$ be $n\times n$ matrices such that all the entries of $A$ and $B$ are in $K$. Is it true that if $A$ is similar to $B$ in $F^{n\times n}$ then they are similar in $K^{n\times n}$? Any help ... thanks!","Given a field $F$ and a subfield $K$ of $F$. Let $A$, $B$ be $n\times n$ matrices such that all the entries of $A$ and $B$ are in $K$. Is it true that if $A$ is similar to $B$ in $F^{n\times n}$ then they are similar in $K^{n\times n}$? Any help ... thanks!",,"['linear-algebra', 'abstract-algebra', 'matrices', 'extension-field']"
92,Origin of the dot and cross product?,Origin of the dot and cross product?,,"Most questions usually just relate to what these can be used for, that's fairly obvious to me since I've been programming 3D games/simulations for a while, but I've never really understood the inner workings of them... I could get the cross product equation as a determinant of a carefully-constructed matrix, but what I want to ask is... How did the dot and cross product come to be? When were they ""invented""? Some detailed proofs? Did someone say: ""Hey, wouldn't it be nice if we could construct a way to calculate a vector that is perpendicular to two given operands?"" Basically, how/why do they work? I would appreciate explanations, links to other explanations, other web resources... I've been searching the Internet lately for explanations, but most of them are on how to use it and nothing that really gives substance to it.","Most questions usually just relate to what these can be used for, that's fairly obvious to me since I've been programming 3D games/simulations for a while, but I've never really understood the inner workings of them... I could get the cross product equation as a determinant of a carefully-constructed matrix, but what I want to ask is... How did the dot and cross product come to be? When were they ""invented""? Some detailed proofs? Did someone say: ""Hey, wouldn't it be nice if we could construct a way to calculate a vector that is perpendicular to two given operands?"" Basically, how/why do they work? I would appreciate explanations, links to other explanations, other web resources... I've been searching the Internet lately for explanations, but most of them are on how to use it and nothing that really gives substance to it.",,"['linear-algebra', 'math-history', 'cross-product']"
93,What are some mathematically interesting computations involving matrices?,What are some mathematically interesting computations involving matrices?,,"I am helping designing a course module that teaches basic python programming to applied math undergraduates. As a result, I'm looking for examples of mathematically interesting computations involving matrices. Preferably these examples would be easy to implement in a computer program. For instance, suppose $$\begin{eqnarray} F_0&=&0\\ F_1&=&1\\ F_{n+1}&=&F_n+F_{n-1}, \end{eqnarray}$$ so that $F_n$ is the $n^{th}$ term in the Fibonacci sequence. If we set $$A=\begin{pmatrix} 1 & 1 \\ 1 & 0  \end{pmatrix}$$ we see that $$A^1=\begin{pmatrix} 1 & 1 \\ 1 & 0 \end{pmatrix} = \begin{pmatrix} F_2 & F_1 \\ F_1 & F_0 \end{pmatrix},$$ and it can be shown that $$ A^n = \begin{pmatrix} F_{n+1} & F_{n} \\ F_{n} & F_{n-1} \end{pmatrix}.$$ This example is ""interesting"" in that it provides a novel way to compute the Fibonacci sequence. It is also relatively easy to implement a simple program to verify the above. Other examples like this will be much appreciated.","I am helping designing a course module that teaches basic python programming to applied math undergraduates. As a result, I'm looking for examples of mathematically interesting computations involving matrices. Preferably these examples would be easy to implement in a computer program. For instance, suppose $$\begin{eqnarray} F_0&=&0\\ F_1&=&1\\ F_{n+1}&=&F_n+F_{n-1}, \end{eqnarray}$$ so that $F_n$ is the $n^{th}$ term in the Fibonacci sequence. If we set $$A=\begin{pmatrix} 1 & 1 \\ 1 & 0  \end{pmatrix}$$ we see that $$A^1=\begin{pmatrix} 1 & 1 \\ 1 & 0 \end{pmatrix} = \begin{pmatrix} F_2 & F_1 \\ F_1 & F_0 \end{pmatrix},$$ and it can be shown that $$ A^n = \begin{pmatrix} F_{n+1} & F_{n} \\ F_{n} & F_{n-1} \end{pmatrix}.$$ This example is ""interesting"" in that it provides a novel way to compute the Fibonacci sequence. It is also relatively easy to implement a simple program to verify the above. Other examples like this will be much appreciated.",,"['linear-algebra', 'sequences-and-series', 'matrices', 'discrete-mathematics', 'big-list']"
94,"Why is $\text{Hom}(V,W)$ the same thing as $V^* \otimes W$?",Why is  the same thing as ?,"\text{Hom}(V,W) V^* \otimes W","I have a couple of questions about tensor products: Why is $\text{Hom}(V,W)$ the same thing as $V^* \otimes W$? Why is an element of $V^{*\otimes m}\otimes V^{\otimes n}$ the same thing as a multilinear map $V^m \to V^{\otimes n}$? What is the general formulation of this principle?","I have a couple of questions about tensor products: Why is $\text{Hom}(V,W)$ the same thing as $V^* \otimes W$? Why is an element of $V^{*\otimes m}\otimes V^{\otimes n}$ the same thing as a multilinear map $V^m \to V^{\otimes n}$? What is the general formulation of this principle?",,"['linear-algebra', 'tensor-products']"
95,Do all square matrices have eigenvectors?,Do all square matrices have eigenvectors?,,I came across a video lecture in which the professor stated that there may or may not be any eigenvectors for a given linear transformation. But I had previously thought every square matrix has eigenvectors.,I came across a video lecture in which the professor stated that there may or may not be any eigenvectors for a given linear transformation. But I had previously thought every square matrix has eigenvectors.,,"['linear-algebra', 'matrices', 'eigenvalues-eigenvectors']"
96,Necessity/Advantage of LU Decomposition over Gaussian Elimination,Necessity/Advantage of LU Decomposition over Gaussian Elimination,,"I am reading the book ""Introduction to Linear Algebra"" by Gilbert Strang and couldn't help wondering the advantages of LU decomposition over Gaussian Elimination! For a system of linear equations in the form $Ax = b$ , one of the methods to solve the unknowns is Gaussian Elimination, where you form a upper triangular matrix $U$ by forward elimination and then figure out the unknowns by backward substitution. This serves the purpose of solving a system of linear equations. What was necessity for LU Decomposition, i.e. after finding $U$ by forward elimination, why do we go about finding $L$ (the lower triangular matrix) when you already had $U$ and could have done a backward substitution?","I am reading the book ""Introduction to Linear Algebra"" by Gilbert Strang and couldn't help wondering the advantages of LU decomposition over Gaussian Elimination! For a system of linear equations in the form , one of the methods to solve the unknowns is Gaussian Elimination, where you form a upper triangular matrix by forward elimination and then figure out the unknowns by backward substitution. This serves the purpose of solving a system of linear equations. What was necessity for LU Decomposition, i.e. after finding by forward elimination, why do we go about finding (the lower triangular matrix) when you already had and could have done a backward substitution?",Ax = b U U L U,['linear-algebra']
97,"What does ""isomorphic"" mean in linear algebra?","What does ""isomorphic"" mean in linear algebra?",,"My professor keeps mentioning the word ""isomorphic"" in class, but has yet to define it... I've asked him and his response is that something that is isomorphic to something else means that they have the same vector structure. I'm not sure what that means, so I was hoping anyone could explain its meaning to me, using knowledge from elementary linear algebra only. He started discussing it in the current section of our textbook: General Vector Spaces. I've also heard that this is an abstract algebra term, so I'm not sure if isomorphic means the same thing in both subjects, but I know absolutely no abstract algebra, so in your definition if you keep either keep abstract algebra out completely, or use very basic abstract algebra knowledge, that would be appreciated.","My professor keeps mentioning the word ""isomorphic"" in class, but has yet to define it... I've asked him and his response is that something that is isomorphic to something else means that they have the same vector structure. I'm not sure what that means, so I was hoping anyone could explain its meaning to me, using knowledge from elementary linear algebra only. He started discussing it in the current section of our textbook: General Vector Spaces. I've also heard that this is an abstract algebra term, so I'm not sure if isomorphic means the same thing in both subjects, but I know absolutely no abstract algebra, so in your definition if you keep either keep abstract algebra out completely, or use very basic abstract algebra knowledge, that would be appreciated.",,"['linear-algebra', 'terminology']"
98,How to tell if a set of vectors spans a space?,How to tell if a set of vectors spans a space?,,"I want to know if the set $\{(1, 1, 1), (3, 2, 1), (1, 1, 0), (1, 0, 0)\}$ spans $\mathbb{R}^3$. I know that if it spans $\mathbb{R}^3$, then for any $x, y, z, \in \mathbb{R}$, there exist $c_1, c_2, c_3, c_4$ such that $(x, y, z) = c_1(1, 1, 1) + c_2(3, 2, 1) + c_3(1, 1, 0) + c_4(1, 0, 0)$. I've looked around the internet, but all the answers I found involve setting up a matrix and finding the determinant, and I can't do that here because my matrix isn't square. What am I missing here?","I want to know if the set $\{(1, 1, 1), (3, 2, 1), (1, 1, 0), (1, 0, 0)\}$ spans $\mathbb{R}^3$. I know that if it spans $\mathbb{R}^3$, then for any $x, y, z, \in \mathbb{R}$, there exist $c_1, c_2, c_3, c_4$ such that $(x, y, z) = c_1(1, 1, 1) + c_2(3, 2, 1) + c_3(1, 1, 0) + c_4(1, 0, 0)$. I've looked around the internet, but all the answers I found involve setting up a matrix and finding the determinant, and I can't do that here because my matrix isn't square. What am I missing here?",,"['linear-algebra', 'vector-spaces']"
99,Similar matrices have the same eigenvalues with the same geometric multiplicity,Similar matrices have the same eigenvalues with the same geometric multiplicity,,"Suppose $A$ and $B$ are similar matrices. Show that $A$ and $B$ have the same eigenvalues with the same geometric multiplicities. Similar matrices : Suppose $A$ and $B$ are $n\times n$ matrices over $\mathbb R$ or $\mathbb C$. We say $A$ and $B$ are similar, or that $A$ is similar to $B$, if there exists a matrix $P$ such that $B = P^{-1}AP$.","Suppose $A$ and $B$ are similar matrices. Show that $A$ and $B$ have the same eigenvalues with the same geometric multiplicities. Similar matrices : Suppose $A$ and $B$ are $n\times n$ matrices over $\mathbb R$ or $\mathbb C$. We say $A$ and $B$ are similar, or that $A$ is similar to $B$, if there exists a matrix $P$ such that $B = P^{-1}AP$.",,['linear-algebra']
