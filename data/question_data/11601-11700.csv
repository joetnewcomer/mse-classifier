,title_raw,title_text,title_latex,body_raw,body_text,body_latex,tags
0,How find the $\sqrt[3]{11+4\sqrt[3]{14+10\sqrt[3]{17+18\sqrt[3]{20+28\sqrt[3]{23+\cdots}}}}}$,How find the,\sqrt[3]{11+4\sqrt[3]{14+10\sqrt[3]{17+18\sqrt[3]{20+28\sqrt[3]{23+\cdots}}}}},find the value $$\sqrt[3]{11+4\sqrt[3]{14+10\sqrt[3]{17+18\sqrt[3]{20+28\sqrt[3]{23+\cdots}}}}}\cdots (1)$$ It is well kown this value $$\sqrt{1+2\sqrt{1+3\sqrt{1+4\sqrt{1+\cdots}}}}=3$$ But for $(1)$ I can't find it.Thank you,find the value $$\sqrt[3]{11+4\sqrt[3]{14+10\sqrt[3]{17+18\sqrt[3]{20+28\sqrt[3]{23+\cdots}}}}}\cdots (1)$$ It is well kown this value $$\sqrt{1+2\sqrt{1+3\sqrt{1+4\sqrt{1+\cdots}}}}=3$$ But for $(1)$ I can't find it.Thank you,,"['calculus', 'limits']"
1,"Prove that: $x^x-y^y\ge(x-y)^x-(x-y)^y$, where $x\ge y>0.$","Prove that: , where",x^x-y^y\ge(x-y)^x-(x-y)^y x\ge y>0.,"I want to analyze the following inequality: Let $x,y\in\Bbb R^{+}$ , then prove that: $$x^x-y^y\ge(x-y)^x-(x-y)^y$$ holds for all $x\ge y$ . I observed that, for possible simplification, dividing both side of the inequality into any of the expressions like $x^x, x^y, y^y, y^x$ doesn't work. Using the identity $x^x=e^{x\ln x}$ and $y^y=e^{y\ln y}$ doesn't help. I used the well-known inequality $e^x≥x+1$ and obtained: $$x^x=e^{x\ln x}≥x\ln x+1$$ or $$y^y=e^{y\ln y}≥y\ln y+1$$ But, this also doesn't help. Some observations: Big problem occurs when, $x=0.002, y=0.001$ . $$-0.0055≈x^x-y^y\ge(x-y)^x-(x-y)^y≈-0.0068$$ If $x=5,y=4.5$ then: $$(x-y)^x-(x-y)^y≈-0.001<0$$ If $x=5,y=3$ then: $$(x-y)^x-(x-y)^y≈24>0$$ If $x-y=1$ , then: $(x-y)^x-(x-y)^y=0$ . Thus $$x^x-(x-1)^{x-1}\ge 0$$ which is correct, since $x>1$ . If $x=y>0$ , then the equality trivially holds: $0≥0$ The possible inequality plot is as follows: I also include the reverse inequality plot: $$x^x-y^y<(x-y)^x-(x-y)^y$$ How can we analyze this inequality?  This doesn't seem possible without using calculus. Maybe the inequality is simple.  However, I could not find a good point, unfortunately.  Also, I'm having trouble understanding the first graph that Wolfram Alpha presents.","I want to analyze the following inequality: Let , then prove that: holds for all . I observed that, for possible simplification, dividing both side of the inequality into any of the expressions like doesn't work. Using the identity and doesn't help. I used the well-known inequality and obtained: or But, this also doesn't help. Some observations: Big problem occurs when, . If then: If then: If , then: . Thus which is correct, since . If , then the equality trivially holds: The possible inequality plot is as follows: I also include the reverse inequality plot: How can we analyze this inequality?  This doesn't seem possible without using calculus. Maybe the inequality is simple.  However, I could not find a good point, unfortunately.  Also, I'm having trouble understanding the first graph that Wolfram Alpha presents.","x,y\in\Bbb R^{+} x^x-y^y\ge(x-y)^x-(x-y)^y x\ge y x^x, x^y, y^y, y^x x^x=e^{x\ln x} y^y=e^{y\ln y} e^x≥x+1 x^x=e^{x\ln x}≥x\ln x+1 y^y=e^{y\ln y}≥y\ln y+1 x=0.002, y=0.001 -0.0055≈x^x-y^y\ge(x-y)^x-(x-y)^y≈-0.0068 x=5,y=4.5 (x-y)^x-(x-y)^y≈-0.001<0 x=5,y=3 (x-y)^x-(x-y)^y≈24>0 x-y=1 (x-y)^x-(x-y)^y=0 x^x-(x-1)^{x-1}\ge 0 x>1 x=y>0 0≥0 x^x-y^y<(x-y)^x-(x-y)^y","['calculus', 'algebra-precalculus', 'inequality', 'exponential-function', 'exponentiation']"
2,Integral ${\frac{1}{\pi^2}} \int_{0}^{\infty}\frac{{(\ln{x}})^2}{\sqrt{x}{(1-x)^2}} \mathrm d x$,Integral,{\frac{1}{\pi^2}} \int_{0}^{\infty}\frac{{(\ln{x}})^2}{\sqrt{x}{(1-x)^2}} \mathrm d x,$${\dfrac{1}{\pi^2}}\int_{0}^{\infty}\dfrac{{(\ln{x}})^2}{\sqrt{x}{(1-x)^2}} \mathrm d x$$ I tried substituting $1/x$ for $x$ but the the only change in the integral is that the $\sqrt{x}$ moves in the numerator from the denominator. I don't understand what to substitute. $\tan{x}$ doesn't seem to work.,I tried substituting for but the the only change in the integral is that the moves in the numerator from the denominator. I don't understand what to substitute. doesn't seem to work.,{\dfrac{1}{\pi^2}}\int_{0}^{\infty}\dfrac{{(\ln{x}})^2}{\sqrt{x}{(1-x)^2}} \mathrm d x 1/x x \sqrt{x} \tan{x},"['calculus', 'integration', 'improper-integrals']"
3,How does $\tan^{-1}(x-\sqrt{1+x^2})=\frac{1}{2}\tan^{-1}x+C$ directly?,How does  directly?,\tan^{-1}(x-\sqrt{1+x^2})=\frac{1}{2}\tan^{-1}x+C,"I'm teaching baby calculus recitation this semester, and I meet a problem to calculate the derivative of  $$y=\tan^{-1}(x-\sqrt{1+x^2})$$ Just apply the chain rule and after some preliminary algebra, I find  $$\frac{dy}{dx}=\frac{1}{2(1+x^2)}$$ What surprises me is that the result implies  $$y=\frac{1}{2}\tan^{-1}x+C$$ Can anyone tell me how to see that directly?","I'm teaching baby calculus recitation this semester, and I meet a problem to calculate the derivative of  $$y=\tan^{-1}(x-\sqrt{1+x^2})$$ Just apply the chain rule and after some preliminary algebra, I find  $$\frac{dy}{dx}=\frac{1}{2(1+x^2)}$$ What surprises me is that the result implies  $$y=\frac{1}{2}\tan^{-1}x+C$$ Can anyone tell me how to see that directly?",,"['calculus', 'trigonometry', 'derivatives']"
4,if $2^x+5^y=2^y+5^x=\frac{7}{10}$,if,2^x+5^y=2^y+5^x=\frac{7}{10},"let $x,y$ such $$2^x+5^y=2^y+5^x=\dfrac{7}{10}$$ prove or disprove $x=y=-1$ is the only solution for the system. My  try: since $$2^x-2^y=5^x-5^y$$ But How can prove  or disprove  $x=y$?","let $x,y$ such $$2^x+5^y=2^y+5^x=\dfrac{7}{10}$$ prove or disprove $x=y=-1$ is the only solution for the system. My  try: since $$2^x-2^y=5^x-5^y$$ But How can prove  or disprove  $x=y$?",,[]
5,"Can you have different integration constants for functions like $1/x^2$, one on each component of its domain?","Can you have different integration constants for functions like , one on each component of its domain?",1/x^2,"We all learned back in calculus class that $\int \frac{1}{x^2}dx$ is $\frac{-1}{x}+C$ via the power rule for integrals.  However, looking back at my calculus book, they define the indefinite integral of a function $f$ as the collection of all functions $F$ where $F$ is an antiderivative of $f$. But, isn't \begin{equation} F(x) = \left\{  \begin{array}{lr}        -\frac{1}{x}+C_1, & x>0\\\\         -\frac{1}{x}+C_2, & x<0  \end{array}\right. \end{equation} an antiderivative of $\frac{1}{x^2}$. I think the derivative of the function above is  $1/x^2$ on the relevant domain. The calculus books on my shelf do not speak on this issue.","We all learned back in calculus class that $\int \frac{1}{x^2}dx$ is $\frac{-1}{x}+C$ via the power rule for integrals.  However, looking back at my calculus book, they define the indefinite integral of a function $f$ as the collection of all functions $F$ where $F$ is an antiderivative of $f$. But, isn't \begin{equation} F(x) = \left\{  \begin{array}{lr}        -\frac{1}{x}+C_1, & x>0\\\\         -\frac{1}{x}+C_2, & x<0  \end{array}\right. \end{equation} an antiderivative of $\frac{1}{x^2}$. I think the derivative of the function above is  $1/x^2$ on the relevant domain. The calculus books on my shelf do not speak on this issue.",,"['calculus', 'integration']"
6,derivative with respect to constant.,derivative with respect to constant.,,"I have been beating my head against this question for quite some time, I do not know whether it has been asked before, but I can't find any information about it! I am taking Calculus 1 course and I cannot grasp the concept of a derivative. From what I understand, a derivative is a function with the following signature: $$(\text{derivative with respect to particular free variable}) :: (\lambda x \to (f)\; x) \to (\lambda x \to (f') x)$$ also phrased as $$(\text{derivative with respect to particular free variable}) = ((\lambda x \to (f) x) \to (\lambda x \to (f') x))$$ e.g: $$(\text{derivative with respect to x}) (\lambda x \to x^2) = (\lambda x\to 2\cdot x)$$ This makes sense but one thing bothers me: what does ""derivative with respect to x"" mean?  In particular, in single variable Calculus this notation assumes $x$ is always a particular variable such as ['a'..'z'] . This works fine for basic derivatives such as: $$(\text{derivative with respect to x}) (\lambda x \to \ln x) = (\lambda x \to \tfrac{1}{x})$$ What I would like to understand is: Why does (derivative with respect to x) make sense but  $(\text{derivative with respect to} (\lambda x \to 2))$ and $(\text{derivative with respect to} (\lambda x \to \ln(x)) $ do not seem to make any sense to me. in classical terms, I cannot do (derivative of $\ln x$ with respect to $1$) nor (derivative of $\ln x$ with respect to $\ln x$) without my head starting to hurt, because those concepts were not taught to me yet, or I have not payed enough attention to understand them. Can somebody please explain what the following two really mean? Derivative of $f(x)$ with respect to a constant such as $1,2,3,\ldots 9999$ Derivative of $f(x)$ with respect to a function such as $\ln(x)$, $\sin(x)$, $\cos(x)$ Thanks ahead of time, this has been bothering me for quite a few years! $\langle$Editor's note: I've left the following in the post for archival's sake.$\rangle$ PS: I am terrible at formatting so to the great ones responsible for   formatting noob's questions (I thank you much for your work) convert \ to lambdas convert d/dx to symbolic d/dx notation (not the worded derivative ones) convert arrows to arrows used in set theory/category theory keep the ""(derivative of ... with respect to ...)"" as they are, as I have no idea how to express them differently, dA/dB doesn't seem to   make sense to me since derivatives are taught to be polymorphic   function rather than a function of two variables, and division only   makes it even more confusing due to the abuse of notation. (Feel free   to give me a link to study formatting, I can't find it).","I have been beating my head against this question for quite some time, I do not know whether it has been asked before, but I can't find any information about it! I am taking Calculus 1 course and I cannot grasp the concept of a derivative. From what I understand, a derivative is a function with the following signature: $$(\text{derivative with respect to particular free variable}) :: (\lambda x \to (f)\; x) \to (\lambda x \to (f') x)$$ also phrased as $$(\text{derivative with respect to particular free variable}) = ((\lambda x \to (f) x) \to (\lambda x \to (f') x))$$ e.g: $$(\text{derivative with respect to x}) (\lambda x \to x^2) = (\lambda x\to 2\cdot x)$$ This makes sense but one thing bothers me: what does ""derivative with respect to x"" mean?  In particular, in single variable Calculus this notation assumes $x$ is always a particular variable such as ['a'..'z'] . This works fine for basic derivatives such as: $$(\text{derivative with respect to x}) (\lambda x \to \ln x) = (\lambda x \to \tfrac{1}{x})$$ What I would like to understand is: Why does (derivative with respect to x) make sense but  $(\text{derivative with respect to} (\lambda x \to 2))$ and $(\text{derivative with respect to} (\lambda x \to \ln(x)) $ do not seem to make any sense to me. in classical terms, I cannot do (derivative of $\ln x$ with respect to $1$) nor (derivative of $\ln x$ with respect to $\ln x$) without my head starting to hurt, because those concepts were not taught to me yet, or I have not payed enough attention to understand them. Can somebody please explain what the following two really mean? Derivative of $f(x)$ with respect to a constant such as $1,2,3,\ldots 9999$ Derivative of $f(x)$ with respect to a function such as $\ln(x)$, $\sin(x)$, $\cos(x)$ Thanks ahead of time, this has been bothering me for quite a few years! $\langle$Editor's note: I've left the following in the post for archival's sake.$\rangle$ PS: I am terrible at formatting so to the great ones responsible for   formatting noob's questions (I thank you much for your work) convert \ to lambdas convert d/dx to symbolic d/dx notation (not the worded derivative ones) convert arrows to arrows used in set theory/category theory keep the ""(derivative of ... with respect to ...)"" as they are, as I have no idea how to express them differently, dA/dB doesn't seem to   make sense to me since derivatives are taught to be polymorphic   function rather than a function of two variables, and division only   makes it even more confusing due to the abuse of notation. (Feel free   to give me a link to study formatting, I can't find it).",,['calculus']
7,polar coordinates and derivatives,polar coordinates and derivatives,,"Using the standard notation $(x,y)$ for cartesian coordinates, and $(r, \theta)$ for polar coordinates, it is true that $$ x = r \cos \theta$$ and so we can infer that  $$ \frac{\partial x}{\partial r} = \cos \theta.$$ This means that if we perturb $r$ to $r+\Delta r$, $x$ gets perturbed to approximately $x+ \cos(\theta) \Delta r$. This is equivalent to saying that if we perturb $x$ to $x+\Delta x$, then we perturb $r$ to approximately $r+ (1/\cos \theta) \Delta x$. Correspondingly, we expect that $$ \frac{\partial r}{\partial x} = \frac{1}{\cos \theta}.$$ In fact,  $$ \frac{ \partial r}{\partial x} = \frac{\partial}{\partial x} \sqrt{x^2+y^2}= \frac{2x}{2 \sqrt{x^2+y^2}} = \frac{x}{r} = \cos \theta.$$ What gives?","Using the standard notation $(x,y)$ for cartesian coordinates, and $(r, \theta)$ for polar coordinates, it is true that $$ x = r \cos \theta$$ and so we can infer that  $$ \frac{\partial x}{\partial r} = \cos \theta.$$ This means that if we perturb $r$ to $r+\Delta r$, $x$ gets perturbed to approximately $x+ \cos(\theta) \Delta r$. This is equivalent to saying that if we perturb $x$ to $x+\Delta x$, then we perturb $r$ to approximately $r+ (1/\cos \theta) \Delta x$. Correspondingly, we expect that $$ \frac{\partial r}{\partial x} = \frac{1}{\cos \theta}.$$ In fact,  $$ \frac{ \partial r}{\partial x} = \frac{\partial}{\partial x} \sqrt{x^2+y^2}= \frac{2x}{2 \sqrt{x^2+y^2}} = \frac{x}{r} = \cos \theta.$$ What gives?",,"['calculus', 'polar-coordinates']"
8,"$\lim_{x\to -\infty} x^x$ exists? and if so, what's its value?","exists? and if so, what's its value?",\lim_{x\to -\infty} x^x,"Disclaimer: I am completely and utterly new here. Please openly correct me if I'm doing anything wrong. I received the following question on a recent calc test: $$\lim_{x \to - \infty} x^x=?$$ $$a) 1 \quad b) \infty  \quad  c) 0   \quad  d) DNE$$ I selected $d) DNE$ as my answer as I believed the function $f(x) = x^x$ has a domain of $x \in (0, \infty)$ , and any negative value will cause the function to oscillate between positive and negative values and result in discontinuities. I was sincerely surprised when the test was returned and the correct answer was $c) 0$ . My instructor's argument was that the function did not have to be continuous for the limit to exist, despite it being outside of the domain. My argument was that for certain negative real numbers of the form $-\frac{2a+1}{2b}$ for $a, b \in \mathbb{N}$ , the function will yield imaginary results of the form: $$\frac{1}{\sqrt[2b]{\left(-\frac{2a+1}{2b}\right)^{2a+1}}}$$ . My friend also had a counter-example where $\left(-\frac{1}{2}\right)^{-\frac{1}{2}}$ does not exist, but $\left(-\frac{2}{4}\right)^{-\frac{2}{4}}$ does. I have done some research on my own, and I believe by the epsilon-delta definition of a limit, this limit does not exist as this limit does not satisfy that for all real $x$ in the needed range, $\left|f(x) - L\right| < \epsilon$ . Please lend help! Edit: is there a (more) rigorous way to show that the limit does not exist?","Disclaimer: I am completely and utterly new here. Please openly correct me if I'm doing anything wrong. I received the following question on a recent calc test: I selected as my answer as I believed the function has a domain of , and any negative value will cause the function to oscillate between positive and negative values and result in discontinuities. I was sincerely surprised when the test was returned and the correct answer was . My instructor's argument was that the function did not have to be continuous for the limit to exist, despite it being outside of the domain. My argument was that for certain negative real numbers of the form for , the function will yield imaginary results of the form: . My friend also had a counter-example where does not exist, but does. I have done some research on my own, and I believe by the epsilon-delta definition of a limit, this limit does not exist as this limit does not satisfy that for all real in the needed range, . Please lend help! Edit: is there a (more) rigorous way to show that the limit does not exist?","\lim_{x \to - \infty} x^x=? a) 1 \quad b) \infty  \quad  c) 0   \quad  d) DNE d) DNE f(x) = x^x x \in (0, \infty) c) 0 -\frac{2a+1}{2b} a, b \in \mathbb{N} \frac{1}{\sqrt[2b]{\left(-\frac{2a+1}{2b}\right)^{2a+1}}} \left(-\frac{1}{2}\right)^{-\frac{1}{2}} \left(-\frac{2}{4}\right)^{-\frac{2}{4}} x \left|f(x) - L\right| < \epsilon","['calculus', 'algebra-precalculus', 'limits', 'exponential-function']"
9,Isn't my book doing this math about differentiation wrongly?,Isn't my book doing this math about differentiation wrongly?,,"Problem: Differentiate with respect to $x$ : $\ln\left(e^x\left(\frac{x-1}{x+1}\right)^{\frac{3}{2}}\right)$ My attempt: Let, $$y=\ln\left(e^x\left(\frac{x-1}{x+1}\right)^{\frac{3}{2}}\right)$$ Both $e^x$ and $\frac{x-1}{x+1}$ are positive: $e^x$ can never be negative, and you can take the square root of only positive numbers, so $\frac{x-1}{x+1}$ is positive as well. So, we can apply logarithm properties: $$=\ln e^x+\ln\left(\frac{x-1}{x+1}\right)^{\frac{3}{2}}$$ $$=x+\frac{3}{2}\ln\left(\frac{x-1}{x+1}\right)$$ Now, $$\frac{dy}{dx}=1+\frac{3}{2}.\frac{x+1}{x-1}.\frac{d}{dx}\left(\frac{x-1}{x+1}\right)$$ $$=1+\frac{3}{2}.\frac{x+1}{x-1}.\frac{x+1-x+1}{(x+1)^2}$$ $$=1+\frac{3}{2}.\frac{x+1}{x-1}.\frac{2}{(x+1)^2}$$ We can cancel $(x+1)$ and $(x+1)$ in the numerator and denominator because the graph doesn't change. $$=1+\frac{3}{x^2-1}$$ $$=\frac{x^2+2}{x^2-1}$$ My book's attempt: Let, $$y=\ln\left(e^x\left(\frac{x-1}{x+1}\right)^{\frac{3}{2}}\right)$$ $$=\ln e^x+\ln\left(\frac{x-1}{x+1}\right)^{\frac{3}{2}}$$ $$=x+\frac{3}{2}\ln\left(\frac{x-1}{x+1}\right)$$ $$=x+\frac{3}{2}\left(\ln(x-1)-\ln(x+1)\right)\tag{1}$$ $$...$$ $$\frac{dy}{dx}=\frac{x^2+2}{x^2-1}$$ Question: In my book's attempt, is line $(1)$ valid? I think my book's assumption in $(1)$ that $(x-1)$ and $(x+1)$ must be positive is unfounded. I deduced that $\frac{x-1}{x+1}$ is positive; $\frac{x-1}{x+1}$ can be positive even when both $(x-1)$ and $(x+1)$ is negative. So, is line $(1)$ of my book valid?","Problem: Differentiate with respect to : My attempt: Let, Both and are positive: can never be negative, and you can take the square root of only positive numbers, so is positive as well. So, we can apply logarithm properties: Now, We can cancel and in the numerator and denominator because the graph doesn't change. My book's attempt: Let, Question: In my book's attempt, is line valid? I think my book's assumption in that and must be positive is unfounded. I deduced that is positive; can be positive even when both and is negative. So, is line of my book valid?",x \ln\left(e^x\left(\frac{x-1}{x+1}\right)^{\frac{3}{2}}\right) y=\ln\left(e^x\left(\frac{x-1}{x+1}\right)^{\frac{3}{2}}\right) e^x \frac{x-1}{x+1} e^x \frac{x-1}{x+1} =\ln e^x+\ln\left(\frac{x-1}{x+1}\right)^{\frac{3}{2}} =x+\frac{3}{2}\ln\left(\frac{x-1}{x+1}\right) \frac{dy}{dx}=1+\frac{3}{2}.\frac{x+1}{x-1}.\frac{d}{dx}\left(\frac{x-1}{x+1}\right) =1+\frac{3}{2}.\frac{x+1}{x-1}.\frac{x+1-x+1}{(x+1)^2} =1+\frac{3}{2}.\frac{x+1}{x-1}.\frac{2}{(x+1)^2} (x+1) (x+1) =1+\frac{3}{x^2-1} =\frac{x^2+2}{x^2-1} y=\ln\left(e^x\left(\frac{x-1}{x+1}\right)^{\frac{3}{2}}\right) =\ln e^x+\ln\left(\frac{x-1}{x+1}\right)^{\frac{3}{2}} =x+\frac{3}{2}\ln\left(\frac{x-1}{x+1}\right) =x+\frac{3}{2}\left(\ln(x-1)-\ln(x+1)\right)\tag{1} ... \frac{dy}{dx}=\frac{x^2+2}{x^2-1} (1) (1) (x-1) (x+1) \frac{x-1}{x+1} \frac{x-1}{x+1} (x-1) (x+1) (1),"['calculus', 'functions', 'derivatives']"
10,Evalute the definite integral: $\int_{0}^{3} (x^2+1) d[x]$,Evalute the definite integral:,\int_{0}^{3} (x^2+1) d[x],Evalute  the definite integral: $$\int_{0}^{3} (x^2+1) d[x]$$ $[x] -$ is integer part of the $x$. Is the solution correct?,Evalute  the definite integral: $$\int_{0}^{3} (x^2+1) d[x]$$ $[x] -$ is integer part of the $x$. Is the solution correct?,,"['calculus', 'proof-verification', 'definite-integrals']"
11,Finding $\int_{0}^{1}\int_{0}^{1}\int_{0}^{1} \left\{\frac{x}{y}\right\} \left\{\frac{y}{z}\right\}\left\{\frac{z}{x}\right\} dx\space dy\space dz $,Finding,\int_{0}^{1}\int_{0}^{1}\int_{0}^{1} \left\{\frac{x}{y}\right\} \left\{\frac{y}{z}\right\}\left\{\frac{z}{x}\right\} dx\space dy\space dz ,"$$\begin{equation}     \int_{0}^{1}\int_{0}^{1}\int_{0}^{1} \left\{\frac{x}{y}\right\} \left\{\frac{y}{z}\right\}\left\{\frac{z}{x}\right\} dx\space dy\space dz \end{equation}$$ I am struggling with the above, I can deal with the case of double integrals but here due to my poor knowledge of triple integrals I am unable to decide what the limits should be in the transformed integral when I apply the substitution $\displaystyle \frac{x}{y}=u\space,\frac{y}{z}=v $ . The integral turns to $\displaystyle \iiint \left\{u\right\}\left\{v\right\}\left\{\dfrac{1}{uv}\right\}z^2v\space du\space dv\space dz$ , but I have no idea about the limits. Or even I am doing some mistake in these  steps only. {.} Denotes Fractional Part","I am struggling with the above, I can deal with the case of double integrals but here due to my poor knowledge of triple integrals I am unable to decide what the limits should be in the transformed integral when I apply the substitution . The integral turns to , but I have no idea about the limits. Or even I am doing some mistake in these  steps only. {.} Denotes Fractional Part","\begin{equation}
    \int_{0}^{1}\int_{0}^{1}\int_{0}^{1} \left\{\frac{x}{y}\right\} \left\{\frac{y}{z}\right\}\left\{\frac{z}{x}\right\} dx\space dy\space dz \end{equation} \displaystyle \frac{x}{y}=u\space,\frac{y}{z}=v  \displaystyle \iiint \left\{u\right\}\left\{v\right\}\left\{\dfrac{1}{uv}\right\}z^2v\space du\space dv\space dz","['calculus', 'integration', 'definite-integrals']"
12,"Closed- form of $\int_0^1 \frac{{\text{Li}}_3^2(-x)}{x^2}\,dx$",Closed- form of,"\int_0^1 \frac{{\text{Li}}_3^2(-x)}{x^2}\,dx","Is there a possibility to find a closed-form for  $$\int_0^1 \frac{{\text{Li}}_3^2(-x)}{x^2}\,dx$$","Is there a possibility to find a closed-form for  $$\int_0^1 \frac{{\text{Li}}_3^2(-x)}{x^2}\,dx$$",,"['calculus', 'integration', 'definite-integrals', 'closed-form', 'polylogarithm']"
13,How do I evaluate the limit $\large\lim_{x\to \infty }\frac{\ln(x)^{\ln(x)^{\ln(x)}}}{x^x}$?,How do I evaluate the limit ?,\large\lim_{x\to \infty }\frac{\ln(x)^{\ln(x)^{\ln(x)}}}{x^x},"$$\large\lim_{x\to \infty }\frac{\ln(x)^{\ln(x)^{\ln(x)}}}{x^x}$$ As $x$ approaches infinity, both functions approach infinity. Therefore I should use the hopital rule, right? But it seems to complicate the answer.","$$\large\lim_{x\to \infty }\frac{\ln(x)^{\ln(x)^{\ln(x)}}}{x^x}$$ As $x$ approaches infinity, both functions approach infinity. Therefore I should use the hopital rule, right? But it seems to complicate the answer.",,['calculus']
14,Evaluating $\int_0^{\frac{1}{2}} \frac{1}{x} \cdot \ln(1+x) \cdot \ln(\frac{1}{x} -1)dx$,Evaluating,\int_0^{\frac{1}{2}} \frac{1}{x} \cdot \ln(1+x) \cdot \ln(\frac{1}{x} -1)dx,"I was trying to evaluate the definite integral $$\int_0^{\frac{1}{2}} \frac{1}{x} \cdot \ln(1+x) \cdot \ln\left(\frac{1}{x} -1\right)\,\mathrm{d}x.$$ On WolframAlpha, I found out that this converges to 0.651114. This seems to be pretty close (and possibly equal) to $\frac{13}{24}\cdot\zeta(3)$ , where $\zeta(3)$ is the value of the Riemann Zeta function $\zeta(n)$ at $\ n = 3$ . Is this true? How can we prove that they are equal? I tried substituting $\ x = e^{-t}$ and the integral became $$\ I = \int_0^{\ln2}\ln(1+e^{-t}) \cdot \ln\left(\frac{e^{-t}}{1-e^{-t}}\right)\,\mathrm{d}t.$$ How do I proceed further? Any help is appreciated. Thanks for reading.","I was trying to evaluate the definite integral On WolframAlpha, I found out that this converges to 0.651114. This seems to be pretty close (and possibly equal) to , where is the value of the Riemann Zeta function at . Is this true? How can we prove that they are equal? I tried substituting and the integral became How do I proceed further? Any help is appreciated. Thanks for reading.","\int_0^{\frac{1}{2}} \frac{1}{x} \cdot \ln(1+x) \cdot \ln\left(\frac{1}{x} -1\right)\,\mathrm{d}x. \frac{13}{24}\cdot\zeta(3) \zeta(3) \zeta(n) \ n = 3 \ x = e^{-t} \ I = \int_0^{\ln2}\ln(1+e^{-t}) \cdot \ln\left(\frac{e^{-t}}{1-e^{-t}}\right)\,\mathrm{d}t.",['calculus']
15,Prove $\int_{0}^{\infty} \frac{\left(\frac{\pi x}{2}-\log x\right)^3}{\left(x^2+1\right)^2 (\log^2x+\frac{\pi ^2}{4})} dx= \pi$,Prove,\int_{0}^{\infty} \frac{\left(\frac{\pi x}{2}-\log x\right)^3}{\left(x^2+1\right)^2 (\log^2x+\frac{\pi ^2}{4})} dx= \pi,"@integralsbot at twitter posted interesting integral and complicated integrals that gives $\pi$ as a result. Some of the problems are really difficult to prove, such as $$\int_{0}^{\infty} \frac{\left(\frac{\pi x}{2}-\log x\right)^3}{\left(x^2+1\right)^2 (\log^2x+\frac{\pi ^2}{4})} dx =\pi\tag{1}\label{eq1}$$ I've tried to calculate this and it appeared very difficult. The plot of the integrand doesn't look very extraordinary. Using partial fraction decomposition, the integral can be transformed to the following form: $$\int_{0}^{\infty} \left( \frac{3 \pi  x}{2 \left(x^2+1\right)^2}-\frac{\log (x)}{\left(x^2+1\right)^2}+\frac{\pi^3 x^3-6 \pi ^2 x^2 \log (x)-3 \pi ^3 x+2 \pi ^2 \log (x)}{2 \left(x^2+1\right)^2 \left(4\log ^2(x)+\pi ^2\right)} \right)$$ The first two integrals can be more or less solved $$ \int_{0}^{\infty} \frac{3 \pi x}{2 \left(x^2+1\right)^2} = \frac{3 \pi}{4}, \>\>\>\>\> \int_{0}^{\infty} -\frac{\log (x)}{\left(x^2+1\right)^2} = \frac{\pi}{4} $$ to yield: $$ \int_{0}^{\infty}  \frac{3 \pi  x}{2 \left(x^2+1\right)^2} - \frac{\log (x)}{\left(x^2+1\right)^2} \ dx= \pi \tag{2}\label{eq2} $$ This is interesting by itself, since the integrand displays interesting shape with two inflection points of the first derivative. I wouldn't expect it to be equal to $\pi$ (yet the algebra clearly shows that it is). What is even more interesting is $$ \int_{0}^{\infty} \frac{\pi^3 x^3-6 \pi ^2 x^2 \log (x)-3 \pi ^3 x+2 \pi ^2 \log (x)}{2 \left(x^2+1\right)^2 \left(4\log ^2(x)+\pi ^2\right)} = 0 \tag{3}\label{eq3}$$ which follows from \eqref{eq1} and \eqref{eq2}. This is correct (I've checked numerically with relatively high precision of ~100 decimals). It seems very hard to prove though. The plot of the integrand doesn't appear to suggest any trivial solution: An  obvious way to transform integral \eqref{eq3} is to expand the numerator: $$  \int_{0}^{\infty} \left( -\frac{3 \pi ^2 x^2 \log (x)}{\left(x^2+1\right)^2 \left(4 \log ^2(x)+\pi ^2\right)}-\frac{3    \pi ^3 x}{2 \left(x^2+1\right)^2 \left(4 \log ^2(x)+\pi ^2\right)}+\frac{\pi ^2 \log    (x)}{\left(x^2+1\right)^2 \left(4 \log ^2(x)+\pi ^2\right)}+\frac{\pi ^3 x^3}{2    \left(x^2+1\right)^2 \left(4 \log ^2(x)+\pi ^2\right)} \right), $$ But the resulting integrals are also difficult. (I've tried to solve the second one, since it seems to have the simplest numerator, but didn't managed to get any result). Neither their numerical values (-1.93789..., 3.229820..., -0.322982..., -0.968946...) nor the plots indicate any obvious cancellations. Maybe there is some indirect way to show the integral \eqref{eq3} vanishes without solving it at all. I don't know where \eqref{eq1} comes from but it seems to be true and interesting. I believe some solution of this integral exists. I post it as an challenge/puzzle. Maybe someone knows it, sees clever substitution, or provides any insight into. So, the question is: how to prove either \eqref{eq1} or \eqref{eq3}?","@integralsbot at twitter posted interesting integral and complicated integrals that gives as a result. Some of the problems are really difficult to prove, such as I've tried to calculate this and it appeared very difficult. The plot of the integrand doesn't look very extraordinary. Using partial fraction decomposition, the integral can be transformed to the following form: The first two integrals can be more or less solved to yield: This is interesting by itself, since the integrand displays interesting shape with two inflection points of the first derivative. I wouldn't expect it to be equal to (yet the algebra clearly shows that it is). What is even more interesting is which follows from \eqref{eq1} and \eqref{eq2}. This is correct (I've checked numerically with relatively high precision of ~100 decimals). It seems very hard to prove though. The plot of the integrand doesn't appear to suggest any trivial solution: An  obvious way to transform integral \eqref{eq3} is to expand the numerator: But the resulting integrals are also difficult. (I've tried to solve the second one, since it seems to have the simplest numerator, but didn't managed to get any result). Neither their numerical values (-1.93789..., 3.229820..., -0.322982..., -0.968946...) nor the plots indicate any obvious cancellations. Maybe there is some indirect way to show the integral \eqref{eq3} vanishes without solving it at all. I don't know where \eqref{eq1} comes from but it seems to be true and interesting. I believe some solution of this integral exists. I post it as an challenge/puzzle. Maybe someone knows it, sees clever substitution, or provides any insight into. So, the question is: how to prove either \eqref{eq1} or \eqref{eq3}?","\pi \int_{0}^{\infty} \frac{\left(\frac{\pi x}{2}-\log x\right)^3}{\left(x^2+1\right)^2 (\log^2x+\frac{\pi ^2}{4})} dx =\pi\tag{1}\label{eq1} \int_{0}^{\infty} \left( \frac{3 \pi  x}{2 \left(x^2+1\right)^2}-\frac{\log (x)}{\left(x^2+1\right)^2}+\frac{\pi^3 x^3-6 \pi ^2 x^2 \log (x)-3 \pi ^3 x+2 \pi ^2 \log (x)}{2 \left(x^2+1\right)^2 \left(4\log ^2(x)+\pi ^2\right)} \right)  \int_{0}^{\infty} \frac{3 \pi x}{2 \left(x^2+1\right)^2} = \frac{3 \pi}{4}, \>\>\>\>\>
\int_{0}^{\infty} -\frac{\log (x)}{\left(x^2+1\right)^2} = \frac{\pi}{4}   \int_{0}^{\infty}  \frac{3 \pi  x}{2 \left(x^2+1\right)^2} - \frac{\log (x)}{\left(x^2+1\right)^2} \ dx= \pi \tag{2}\label{eq2}  \pi  \int_{0}^{\infty} \frac{\pi^3 x^3-6 \pi ^2 x^2 \log (x)-3 \pi ^3 x+2 \pi ^2 \log (x)}{2 \left(x^2+1\right)^2 \left(4\log ^2(x)+\pi ^2\right)} = 0 \tag{3}\label{eq3}  
\int_{0}^{\infty} \left( -\frac{3 \pi ^2 x^2 \log (x)}{\left(x^2+1\right)^2 \left(4 \log ^2(x)+\pi ^2\right)}-\frac{3
   \pi ^3 x}{2 \left(x^2+1\right)^2 \left(4 \log ^2(x)+\pi ^2\right)}+\frac{\pi ^2 \log
   (x)}{\left(x^2+1\right)^2 \left(4 \log ^2(x)+\pi ^2\right)}+\frac{\pi ^3 x^3}{2
   \left(x^2+1\right)^2 \left(4 \log ^2(x)+\pi ^2\right)} \right), ","['calculus', 'integration', 'definite-integrals']"
16,"Closed-form of $\int_{0}^{\infty} \frac{{\text{Li}}_2^3(-x)}{x^3}\,dx$",Closed-form of,"\int_{0}^{\infty} \frac{{\text{Li}}_2^3(-x)}{x^3}\,dx","Is there a possibility to find a closed-form for $$\int_{0}^{\infty} \frac{{\text{Li}}_2^3(-x)}{x^3}\,dx$$  We have $$I=\int_0^1\frac{Li_2^3(-x)+x^4Li_2^3(-\frac{1}{x})}{x^3}\,dx$$  After repeatedly integrate by parts, I found  $$I=\frac{51}{8}\zeta(4)+\frac{15}{32}\zeta(4)\ln(2)+\frac{3}{8}\zeta(2)ln(2)+\frac{39}{8}\zeta(2)\zeta(3)+\frac{3}{2}\zeta(5)-\frac{15}{4}\zeta(3)-\frac{33}{4}\zeta(2)$$  Could anyome here help me to ascertain this result?Thanks","Is there a possibility to find a closed-form for $$\int_{0}^{\infty} \frac{{\text{Li}}_2^3(-x)}{x^3}\,dx$$  We have $$I=\int_0^1\frac{Li_2^3(-x)+x^4Li_2^3(-\frac{1}{x})}{x^3}\,dx$$  After repeatedly integrate by parts, I found  $$I=\frac{51}{8}\zeta(4)+\frac{15}{32}\zeta(4)\ln(2)+\frac{3}{8}\zeta(2)ln(2)+\frac{39}{8}\zeta(2)\zeta(3)+\frac{3}{2}\zeta(5)-\frac{15}{4}\zeta(3)-\frac{33}{4}\zeta(2)$$  Could anyome here help me to ascertain this result?Thanks",,"['calculus', 'integration', 'definite-integrals', 'closed-form', 'polylogarithm']"
17,Is it meaningful to take the derivative of a function a non-integer number of times?,Is it meaningful to take the derivative of a function a non-integer number of times?,,"If I want to take the derivative of $ax^n$, I will get $anx^{n-1}$. If I were to take the derivative again, I get $an(n-1)x^{n-2}$. We can generalize this for integer k easily to get the kth derivative $a\frac{n!}{(n-k)!} x ^{n-k}$. But what about for a more general k? Does this have some name? Has it been widely studied? If so, can you show how to generalize this formula for kth derivative of $ax^n$, and explain how it works? If not, is there a good reason it is impossible?","If I want to take the derivative of $ax^n$, I will get $anx^{n-1}$. If I were to take the derivative again, I get $an(n-1)x^{n-2}$. We can generalize this for integer k easily to get the kth derivative $a\frac{n!}{(n-k)!} x ^{n-k}$. But what about for a more general k? Does this have some name? Has it been widely studied? If so, can you show how to generalize this formula for kth derivative of $ax^n$, and explain how it works? If not, is there a good reason it is impossible?",,"['calculus', 'fractional-calculus']"
18,Show that $\int_{\arccos(1/4)}^{\pi/2}\arccos(\cos x (2\sin^2x+\sqrt{1+4\sin^4x})) \mathrm dx=\frac{\pi^2}{40}$,Show that,\int_{\arccos(1/4)}^{\pi/2}\arccos(\cos x (2\sin^2x+\sqrt{1+4\sin^4x})) \mathrm dx=\frac{\pi^2}{40},"There is numerical evidence that $$I=\int_{\arccos(1/4)}^{\pi/2}\arccos\left(\cos x\left(2\sin^2x+\sqrt{1+4\sin^4x}\right)\right)\mathrm dx=\frac{\pi^2}{40}$$ How can this be proved? I was trying to answer another question , and I got it down to this integral. Wolfram does not evaluate the indefinite integral. I tried techniques from a roughly similar integral . Letting $u=\tan \frac{x}{2}$ , I got $$I=\int_{\sqrt{3/5}}^1 \dfrac{2\arccos{\left(\frac{(1-u^2)\left(8u^2+\sqrt{u^8+4u^6+70u^4+4u^2+1}\right)}{(1+u^2)^3}\right)}}{1+u^2}\mathrm du$$ but I don't know what to do with this.","There is numerical evidence that How can this be proved? I was trying to answer another question , and I got it down to this integral. Wolfram does not evaluate the indefinite integral. I tried techniques from a roughly similar integral . Letting , I got but I don't know what to do with this.",I=\int_{\arccos(1/4)}^{\pi/2}\arccos\left(\cos x\left(2\sin^2x+\sqrt{1+4\sin^4x}\right)\right)\mathrm dx=\frac{\pi^2}{40} u=\tan \frac{x}{2} I=\int_{\sqrt{3/5}}^1 \dfrac{2\arccos{\left(\frac{(1-u^2)\left(8u^2+\sqrt{u^8+4u^6+70u^4+4u^2+1}\right)}{(1+u^2)^3}\right)}}{1+u^2}\mathrm du,"['calculus', 'integration', 'trigonometry', 'definite-integrals', 'closed-form']"
19,Why can't $\int_1^3 \frac{4}{(2x-3)^4} dx $ be evaluated by calculators or WolframAlpha?,Why can't  be evaluated by calculators or WolframAlpha?,\int_1^3 \frac{4}{(2x-3)^4} dx ,"This is the integral $$\int_1^3 \frac{4}{(2x-3)^4} dx $$ Solving by u-substitution, it works fine. $$ \text{let}~~ u = 2x-3 $$ $$\int_1^34 \cdot  ( 2x-3 )^{-4}dx $$ $$\ x = \frac{u+3}{2} $$ $$\ \frac{ dx }{ du }=\frac{1 }{2} $$ $$\ dx = \frac{ 1 }{ 2 }du $$ EDIT: $$\ u(3) = 2(3)- 3$$ $$\ u(3) = 3 $$ $$\ u(1) = 2(1)- 3$$ $$\ u(1) = -1 $$ $$\int_{-1}^3 4 \cdot  u^-4 \cdot \frac{ 1 }{ 2 }du $$ $$\int_{-1}^3 2\cdot u^{-4}du $$ $$\frac{ 2u^{-3}}{-3 } \Bigg \vert_{-1}^{3} \ $$ $$ = \frac{ 2}{-3(2x-3)^3 } \Bigg \vert_{-1}^{3} \ $$ $$ = \frac{  2}{ -3(2(3)-3)^3 } -  \frac{  2}{ -3(2(-1)-3)^3} $$ $$ = \frac{-2}{81} - \frac{2}{375} $$ $$ = \frac{-84}{125} $$ But plugging this integral into a TI-84 CE Plus throws an error ""Cannot divide by 0"" Also trying online calculators, Wolframalpha and Freemathhelp , the problem is unable to be solved. Why can't this problem be solved on these calculators?","This is the integral Solving by u-substitution, it works fine. EDIT: But plugging this integral into a TI-84 CE Plus throws an error ""Cannot divide by 0"" Also trying online calculators, Wolframalpha and Freemathhelp , the problem is unable to be solved. Why can't this problem be solved on these calculators?",\int_1^3 \frac{4}{(2x-3)^4} dx   \text{let}~~ u = 2x-3  \int_1^34 \cdot  ( 2x-3 )^{-4}dx  \ x = \frac{u+3}{2}  \ \frac{ dx }{ du }=\frac{1 }{2}  \ dx = \frac{ 1 }{ 2 }du  \ u(3) = 2(3)- 3 \ u(3) = 3  \ u(1) = 2(1)- 3 \ u(1) = -1  \int_{-1}^3 4 \cdot  u^-4 \cdot \frac{ 1 }{ 2 }du  \int_{-1}^3 2\cdot u^{-4}du  \frac{ 2u^{-3}}{-3 } \Bigg \vert_{-1}^{3} \   = \frac{ 2}{-3(2x-3)^3 } \Bigg \vert_{-1}^{3} \   = \frac{  2}{ -3(2(3)-3)^3 } -  \frac{  2}{ -3(2(-1)-3)^3}   = \frac{-2}{81} - \frac{2}{375}   = \frac{-84}{125} ,"['calculus', 'integration', 'definite-integrals']"
20,Evaluating $\int_{0}^{\frac{\pi}{2}} \frac{1}{\sin^m x+\cos^m x}dx$,Evaluating,\int_{0}^{\frac{\pi}{2}} \frac{1}{\sin^m x+\cos^m x}dx,"I evaluated the following integral: $$I=\int_{0}^{\frac{\pi}{2}} \frac{1}{\sin^8x+\cos^8x}dx$$ My Method: Divide up and down by $\cos^8x$ and substitute $t=\tan x$ , so the integral converts to $$I=\int_{0}^{\infty}\frac{(1+t^2)^3}{1+t^8}\mathrm{d}t$$ I evaluated this integral by substituting $t^8=u$ and using the general result $$\displaystyle \int_{0}^{\infty}\frac{u^{n-1}}{1+u}\mathrm{d}u=\frac{\pi}{\sin n\pi}$$ which can be evaluated easily. However, I wanted to generalise this integral $$I(m)=\int_{0}^{\frac{\pi}{2}}\frac{1}{\sin^mx+\cos^mx}dx$$ But the above method doesn't apply to any value of $m$ . So, what should be the approach for the above integral? We also know that $I(0)=\frac{\pi}{4}$ , so maybe we could make a recurrence relation or perhaps, feynmann's technique ...","I evaluated the following integral: My Method: Divide up and down by and substitute , so the integral converts to I evaluated this integral by substituting and using the general result which can be evaluated easily. However, I wanted to generalise this integral But the above method doesn't apply to any value of . So, what should be the approach for the above integral? We also know that , so maybe we could make a recurrence relation or perhaps, feynmann's technique ...",I=\int_{0}^{\frac{\pi}{2}} \frac{1}{\sin^8x+\cos^8x}dx \cos^8x t=\tan x I=\int_{0}^{\infty}\frac{(1+t^2)^3}{1+t^8}\mathrm{d}t t^8=u \displaystyle \int_{0}^{\infty}\frac{u^{n-1}}{1+u}\mathrm{d}u=\frac{\pi}{\sin n\pi} I(m)=\int_{0}^{\frac{\pi}{2}}\frac{1}{\sin^mx+\cos^mx}dx m I(0)=\frac{\pi}{4},"['calculus', 'integration', 'definite-integrals', 'trigonometric-integrals']"
21,$\int_{0}^{1}{(1-x)(1-2x^{\phi})+\phi(x-x^{\phi})\over (1-x)^2}\cdot{\left(1-x^{\phi}\over 1-x\right)^{1\over \phi}}\mathrm dx=\phi^{\phi}$,,\int_{0}^{1}{(1-x)(1-2x^{\phi})+\phi(x-x^{\phi})\over (1-x)^2}\cdot{\left(1-x^{\phi}\over 1-x\right)^{1\over \phi}}\mathrm dx=\phi^{\phi},How can we show that? $$\int_{0}^{1}{(1-x)(1-2x^{\phi})+\phi(x-x^{\phi})\over (1-x)^2}\cdot{\left(1-x^{\phi}\over 1-x\right)^{1\over \phi}}\mathrm dx=\phi^{\phi}\tag1$$   Where $\phi$;Golden ratio This integral look too complicated. I try and make a $u=1-x$ still not simplified $$\int_{0}^{1}{u[1-2(1-u)^{\phi}]+\phi[1-u-(1-u)^{\phi}]\over u^2}\cdot\left[1-(1-u)^{\phi}\over u\right]^{1\over \phi}\mathrm du$$ $$\int_{0}^{1}{\phi-{u\over \phi}-(1-u)^{\phi}(2u+\phi)\over u^2}\cdot\left[1-(1-u)^{\phi}\over u\right]^{1\over \phi}\mathrm du$$ Simplifed $(1)$: $$\int_{0}^{1}{1+{x\over \phi}+x^{\phi}(2x-\phi{\sqrt{5}})\over (1-x)^2}\cdot\left({1- x^\phi}\over 1-x\right)^{1\over \phi}\mathrm dx=\phi^{\phi} \tag2$$ I have no idea where to go from here.,How can we show that? $$\int_{0}^{1}{(1-x)(1-2x^{\phi})+\phi(x-x^{\phi})\over (1-x)^2}\cdot{\left(1-x^{\phi}\over 1-x\right)^{1\over \phi}}\mathrm dx=\phi^{\phi}\tag1$$   Where $\phi$;Golden ratio This integral look too complicated. I try and make a $u=1-x$ still not simplified $$\int_{0}^{1}{u[1-2(1-u)^{\phi}]+\phi[1-u-(1-u)^{\phi}]\over u^2}\cdot\left[1-(1-u)^{\phi}\over u\right]^{1\over \phi}\mathrm du$$ $$\int_{0}^{1}{\phi-{u\over \phi}-(1-u)^{\phi}(2u+\phi)\over u^2}\cdot\left[1-(1-u)^{\phi}\over u\right]^{1\over \phi}\mathrm du$$ Simplifed $(1)$: $$\int_{0}^{1}{1+{x\over \phi}+x^{\phi}(2x-\phi{\sqrt{5}})\over (1-x)^2}\cdot\left({1- x^\phi}\over 1-x\right)^{1\over \phi}\mathrm dx=\phi^{\phi} \tag2$$ I have no idea where to go from here.,,"['calculus', 'integration', 'definite-integrals', 'golden-ratio']"
22,Geometric intuition for derivatives of basic trig functions,Geometric intuition for derivatives of basic trig functions,,"I was inspired by this question to try and come up with geometric proofs for the derivatives of basic trig functions--basically, those that have simple representations on the unit circle ($\sin, \cos, \tan, \sec, \csc, \cot$): I was initially a bit skeptical about how easy it might be, but then I found this very simple proof for $\sin$ and $\cos$; the basic insight can be seen in this picture from an alternative version of the proof I found later: Basically, we use the fact arc $PQ$ and segment $PQ$ are the same as $\Delta\theta\rightarrow 0$, and the former has measure $\Delta\theta$. Nevertheless, I've had no luck so far getting a proof for $\sec$; I have a feeling the proofs of $sec$ and $tan$ are very closely related, as are the $\csc, \cot$ proofs. Has anyone seen a proof for the four remaining basic functions anywhere? Perhaps I just haven't drawn the right picture yet. Another possible avenue is this representation:","I was inspired by this question to try and come up with geometric proofs for the derivatives of basic trig functions--basically, those that have simple representations on the unit circle ($\sin, \cos, \tan, \sec, \csc, \cot$): I was initially a bit skeptical about how easy it might be, but then I found this very simple proof for $\sin$ and $\cos$; the basic insight can be seen in this picture from an alternative version of the proof I found later: Basically, we use the fact arc $PQ$ and segment $PQ$ are the same as $\Delta\theta\rightarrow 0$, and the former has measure $\Delta\theta$. Nevertheless, I've had no luck so far getting a proof for $\sec$; I have a feeling the proofs of $sec$ and $tan$ are very closely related, as are the $\csc, \cot$ proofs. Has anyone seen a proof for the four remaining basic functions anywhere? Perhaps I just haven't drawn the right picture yet. Another possible avenue is this representation:",,"['calculus', 'geometry', 'trigonometry']"
23,Interpolation and Taylor's Theorem,Interpolation and Taylor's Theorem,,"I just answered a question where I used the fact that a $(n+1)$ -times (continuously) differentiable function $f$ interpolated by a $n$ th degree polynomial $p(x)$ through the $n+1$ points $x_0,...,x_n$ has error given by $$f(x)-p_n(x)=\frac{f^{(n+1)}(\xi)}{(n+1)!}\prod_{i=0}^n{(x-x_i)}$$ This seems eerily similar to Taylor's Theorem $$f(x)-T_n(x)=\frac{f^{(n+1)}(\xi)}{(n+1)!}(x-a)^{n+1}$$ for some $\xi\in (a,x)$ . The wiki page http://en.wikipedia.org/wiki/Polynomial_interpolation#Interpolation_error says that ""the remainder term in the Lagrange form of the Taylor theorem is a special case of interpolation error when all interpolation nodes $x_i$ are identical"" but the article it cites for that is https://math.okstate.edu/people/binegar/4513-F98/4513-l16.pdf where I find ""Although this formula for the error is somewhat reminiscent of the error term associated with an nth order Taylor expansion, this theorem has little to do with Taylor expansions."" Obviously the claim given in the wikipedia article is not justified, but I am not inclined to dismiss such a remarkable resemblance to mere chance. The proof is as follows: write $R(x)$ for the remainder at $x$ and let $$g(t)=R(t)-\frac{R(x)}{W(x)}W(t),\;\;W(t)=\prod_{i=0}^n{(t-x_i)}$$ Then $g(x)=0$ - take $x\in (x_0,x_n)$ - and $g(x_i)=0,\;\;i=0,...,i=n$ because $R(x_i)=0$ . Thus it has $(n+2)$ zeros in the interval and by repeated application of Rolle's Theorem there exists a $\xi$ such that $$g^{(n+1)}(\xi)=f^{(n+1)}(\xi)-\frac{R(x)}{W(x)}(n+1)!=0$$ from which the error formula follows. We can do the same with Taylor's formula. Let $$g(t)=R(t)-\frac{R(x)}{W(x)}W(t),\;\;W(t)=(t-a)^{n+1}$$ Then $g(a)=g(x)=0$ so $\exists c_0\in (a,x): g'(c_0)=0$ . Now $g'(a)=g'(c_0)=0$ so $\exists c_1\in (a,c_0): g''(c_1)=0$ etc. until finally $$\exists\xi: g^{(n+1)}(\xi)=f^{(n+1)}(\xi)-\frac{R(x)}{W(x)}(n+1)!=0\implies R(x)=\frac{f^{(n+1)}(\xi)}{(n+1)!}(x-a)^{n+1}$$ This simply cannot be a coincidence - can anyone explanation the connection?","I just answered a question where I used the fact that a -times (continuously) differentiable function interpolated by a th degree polynomial through the points has error given by This seems eerily similar to Taylor's Theorem for some . The wiki page http://en.wikipedia.org/wiki/Polynomial_interpolation#Interpolation_error says that ""the remainder term in the Lagrange form of the Taylor theorem is a special case of interpolation error when all interpolation nodes are identical"" but the article it cites for that is https://math.okstate.edu/people/binegar/4513-F98/4513-l16.pdf where I find ""Although this formula for the error is somewhat reminiscent of the error term associated with an nth order Taylor expansion, this theorem has little to do with Taylor expansions."" Obviously the claim given in the wikipedia article is not justified, but I am not inclined to dismiss such a remarkable resemblance to mere chance. The proof is as follows: write for the remainder at and let Then - take - and because . Thus it has zeros in the interval and by repeated application of Rolle's Theorem there exists a such that from which the error formula follows. We can do the same with Taylor's formula. Let Then so . Now so etc. until finally This simply cannot be a coincidence - can anyone explanation the connection?","(n+1) f n p(x) n+1 x_0,...,x_n f(x)-p_n(x)=\frac{f^{(n+1)}(\xi)}{(n+1)!}\prod_{i=0}^n{(x-x_i)} f(x)-T_n(x)=\frac{f^{(n+1)}(\xi)}{(n+1)!}(x-a)^{n+1} \xi\in (a,x) x_i R(x) x g(t)=R(t)-\frac{R(x)}{W(x)}W(t),\;\;W(t)=\prod_{i=0}^n{(t-x_i)} g(x)=0 x\in (x_0,x_n) g(x_i)=0,\;\;i=0,...,i=n R(x_i)=0 (n+2) \xi g^{(n+1)}(\xi)=f^{(n+1)}(\xi)-\frac{R(x)}{W(x)}(n+1)!=0 g(t)=R(t)-\frac{R(x)}{W(x)}W(t),\;\;W(t)=(t-a)^{n+1} g(a)=g(x)=0 \exists c_0\in (a,x): g'(c_0)=0 g'(a)=g'(c_0)=0 \exists c_1\in (a,c_0): g''(c_1)=0 \exists\xi: g^{(n+1)}(\xi)=f^{(n+1)}(\xi)-\frac{R(x)}{W(x)}(n+1)!=0\implies R(x)=\frac{f^{(n+1)}(\xi)}{(n+1)!}(x-a)^{n+1}","['calculus', 'numerical-methods']"
24,"How to integrate $\int_{0}^{2} \sqrt{x+\sqrt{x+\sqrt{x+\cdots}}}\,dx$?",How to integrate ?,"\int_{0}^{2} \sqrt{x+\sqrt{x+\sqrt{x+\cdots}}}\,dx","I want to integrate this $$\int_{0}^{2} \sqrt{x+\sqrt{x+\sqrt{x+\cdots}}}\,dx$$ I tried making the substitution $u^2=x+u$ and  my integral turns into $$\int_{0}^{2}u(2u-1)\,du = \frac{10}{3}$$ However the answer is supposed to be $\dfrac{19}{6}$ , I would like to know where I am making a mistake or another way","I want to integrate this I tried making the substitution and  my integral turns into However the answer is supposed to be , I would like to know where I am making a mistake or another way","\int_{0}^{2} \sqrt{x+\sqrt{x+\sqrt{x+\cdots}}}\,dx u^2=x+u \int_{0}^{2}u(2u-1)\,du = \frac{10}{3} \dfrac{19}{6}","['calculus', 'integration', 'definite-integrals']"
25,Calculus book suggestion [duplicate],Calculus book suggestion [duplicate],,"This question already has answers here : What are the recommended textbooks for introductory calculus? (8 answers) Closed last year . I am a high schooler with no prior exposure to calculus. I want a calculus book to learn math for classical mechanics on my own, and perhaps learning math for math itself. I don't like memorizing formulas, I want some understanding but nothing too rigorous. A lot of people suggest Thomas and Stewart, but a lot people dislike them as well. Why do people dislike these books? Because they are too simplistic? And they came overly long to me (over 1000 pages). I think Lang, and perhaps Kline are nice, but I am not sure. And it came to me that these books are better in 'why's of formulas. And there is Simmons as well. I worked through Spivak a little, but it was too hard. Perhaps I may return to it after some exposure to calculus to refine my understanding of the concepts. Thanks for suggestions.","This question already has answers here : What are the recommended textbooks for introductory calculus? (8 answers) Closed last year . I am a high schooler with no prior exposure to calculus. I want a calculus book to learn math for classical mechanics on my own, and perhaps learning math for math itself. I don't like memorizing formulas, I want some understanding but nothing too rigorous. A lot of people suggest Thomas and Stewart, but a lot people dislike them as well. Why do people dislike these books? Because they are too simplistic? And they came overly long to me (over 1000 pages). I think Lang, and perhaps Kline are nice, but I am not sure. And it came to me that these books are better in 'why's of formulas. And there is Simmons as well. I worked through Spivak a little, but it was too hard. Perhaps I may return to it after some exposure to calculus to refine my understanding of the concepts. Thanks for suggestions.",,"['calculus', 'book-recommendation']"
26,How can we prove that $8\int_{0}^{\infty}{\ln x\over x}\left(e^{-x}-{1\over \sqrt[4]{1+8x}}\right)\mathrm dx=-32C+4\gamma^2-5\pi^2?$,How can we prove that,8\int_{0}^{\infty}{\ln x\over x}\left(e^{-x}-{1\over \sqrt[4]{1+8x}}\right)\mathrm dx=-32C+4\gamma^2-5\pi^2?,"An integral exhibits $3$ interesting constants. $$8\int_{0}^{\infty}{\ln x\over x}\left(e^{-x}-{1\over \sqrt[4]{1+8x}}\right)\mathrm dx=-32\color{red}C+4\color{blue}\gamma^2-5\color{green}\pi^2\tag1$$ I am only interested in $(1)$, because rarely Catalan's constant and Euler-Masheroni's constant they appear together! Making an attempt: It is too difficult here to make an attempt, apart from differentiating under the integral $$I(a)=8\int_{0}^{\infty}{\ln x\over x^{a}}\left(e^{-x}-{1\over \sqrt[4]{1+8x}}\right)\mathrm dx\tag2$$ $$I{'}(a)=8\int_{0}^{\infty}{1\over x^{a}}\left(e^{-x}-{1\over \sqrt[4]{1+8x}}\right)\mathrm dx\tag3$$ I guess $(3)$ diverges How may we prove $(1)?$","An integral exhibits $3$ interesting constants. $$8\int_{0}^{\infty}{\ln x\over x}\left(e^{-x}-{1\over \sqrt[4]{1+8x}}\right)\mathrm dx=-32\color{red}C+4\color{blue}\gamma^2-5\color{green}\pi^2\tag1$$ I am only interested in $(1)$, because rarely Catalan's constant and Euler-Masheroni's constant they appear together! Making an attempt: It is too difficult here to make an attempt, apart from differentiating under the integral $$I(a)=8\int_{0}^{\infty}{\ln x\over x^{a}}\left(e^{-x}-{1\over \sqrt[4]{1+8x}}\right)\mathrm dx\tag2$$ $$I{'}(a)=8\int_{0}^{\infty}{1\over x^{a}}\left(e^{-x}-{1\over \sqrt[4]{1+8x}}\right)\mathrm dx\tag3$$ I guess $(3)$ diverges How may we prove $(1)?$",,"['calculus', 'integration', 'definite-integrals']"
27,"On $\big(\tfrac{1+\sqrt{5}}{2}\big)^{12}=\small 161+72\sqrt{5}$ and $\int_{-1}^1\frac{dx}{\left(1-x^2\right)^{\small3/4} \sqrt[4]{161+72\sqrt{5}\,x}}$",On  and,"\big(\tfrac{1+\sqrt{5}}{2}\big)^{12}=\small 161+72\sqrt{5} \int_{-1}^1\frac{dx}{\left(1-x^2\right)^{\small3/4} \sqrt[4]{161+72\sqrt{5}\,x}}","( This summarizes scattered results from here , here , here and elsewhere. See also this older post .) I. Cubic Define $\beta= \tfrac{\Gamma\big(\tfrac56\big)}{\Gamma\big(\tfrac13\big)\sqrt{\pi}}= \frac{1}{48^{1/4}\,K(k_3)}$. Then we have the nice evaluations, $$\begin{aligned}\frac{3}{5^{5/6}} &=\,_2F_1\big(\tfrac{1}{3},\tfrac{1}{3};\tfrac{5}{6};-4\big)\\ &=\beta\,\int_0^1 \frac{dx}{\sqrt{1-x}\,\sqrt[3]{x^2+4x^3}}\\[1.7mm] &=\beta\,\int_{-1}^1\frac{dx}{\left(1-x^2\right)^{\small2/3} \sqrt[3]{\color{blue}{9+4\sqrt{5}}\,x}}\\[1.7mm] &=2^{1/3}\,\beta\,\int_0^\infty\frac{dx}{\sqrt[3]{9+\cosh x}} \end{aligned}\tag1$$ and, $$\begin{aligned}\frac{4}{7} &=\,_2F_1\big(\tfrac{1}{3},\tfrac{1}{3};\tfrac{5}{6};-27\big)\\ &=\beta\,\int_0^1 \frac{dx}{\sqrt{1-x}\,\sqrt[3]{x^2+27x^3}}\\[1.7mm] &=\beta\,\int_{-1}^1\frac{dx}{\left(1-x^2\right)^{\small2/3} \sqrt[3]{\color{blue}{55+12\sqrt{21}}\,x}}\\[1.7mm] &=2^{1/3}\,\beta\,\int_0^\infty\frac{dx}{\sqrt[3]{55+\cosh x}} \end{aligned}\tag2$$ Note the powers of fundamental units , $$U_{5}^6 = \big(\tfrac{1+\sqrt{5}}{2}\big)^6=\color{blue}{9+4\sqrt{5}}$$ $$U_{21}^3 = \big(\tfrac{5+\sqrt{21}}{2}\big)^3=\color{blue}{55+12\sqrt{21}}$$ Those two instances can't be coincidence. II. Quartic Define $\gamma= \tfrac{\sqrt{2\pi}}{\Gamma^2\big(\tfrac14\big)}= \frac{1}{2\sqrt2\,K(k_1)}=\frac1{2L}$ with lemniscate constant $L$. Then we have the nice, $$\begin{aligned}\frac{2}{3^{3/4}} &=\,_2F_1\big(\tfrac{1}{4},\tfrac{1}{4};\tfrac{3}{4};-3\big)\\ &=\gamma\,\int_0^1 \frac{dx}{\sqrt{1-x}\,\sqrt[4]{x^3+3x^4}}\\[1.7mm] &\overset{\color{red}?}=\gamma\,\int_{-1}^1\frac{dx}{\left(1-x^2\right)^{\small3/4} \sqrt[4]{\color{blue}{7+4\sqrt{3}}\,x}}\\[1.7mm] &=2^{1/4}\,\gamma\,\int_0^\infty\frac{dx}{\sqrt[4]{7+\cosh x}} \end{aligned}\tag3$$ and, $$\begin{aligned}\frac{3}{5}&=\,_2F_1\big(\tfrac{1}{4},\tfrac{1}{4};\tfrac{3}{4};-80\big)\\ &=\gamma\,\int_0^1 \frac{dx}{\sqrt{1-x}\,\sqrt[4]{x^3+80x^4}}\\[1.7mm] &\overset{\color{red}?}=\gamma\,\int_{-1}^1\frac{dx}{\left(1-x^2\right)^{\small3/4} \sqrt[4]{\color{blue}{161+72\sqrt{5}}\,x}}\\[1.7mm] &=2^{1/4}\,\gamma\,\int_0^\infty\frac{dx}{\sqrt[4]{161+\cosh x}} \end{aligned}\tag4$$ with $a=161$ given by Noam Elkies in this comment . (For $4$th roots, I just assumed the equality using the blue radicals based on the ones for cube roots.) Note again the powers of fundamental units, $$U_{3}^2 = \big(2+\sqrt3\big)^2=\color{blue}{7+4\sqrt{3}}$$ $$U_{5}^{12} = \big(\tfrac{1+\sqrt{5}}{2}\big)^{12}=\color{blue}{161+72\sqrt{5}}$$ Just like for the cube roots version, these can't be coincidence. Questions: Is it true these observations can be explained by, let $b=2a+1$, then, $$\int_0^1 \frac{dx}{\sqrt{1-x}\,\sqrt[3]{x^2+ax^3}}=\int_{-1}^1\frac{dx}{\left(1-x^2\right)^{\small2/3} \sqrt[3]{b+\sqrt{b^2-1}\,x}}=2^{1/3}\int_0^\infty\frac{dx}{\sqrt[3]{b+\cosh x}}$$ $$\int_0^1 \frac{dx}{\sqrt{1-x}\,\sqrt[4]{x^3+ax^4}}=\int_{-1}^1\frac{dx}{\left(1-x^2\right)^{\small3/4} \sqrt[4]{b+\sqrt{b^2-1}\,x}}=2^{1/4}\int_0^\infty\frac{dx}{\sqrt[4]{b+\cosh x}}$$","( This summarizes scattered results from here , here , here and elsewhere. See also this older post .) I. Cubic Define $\beta= \tfrac{\Gamma\big(\tfrac56\big)}{\Gamma\big(\tfrac13\big)\sqrt{\pi}}= \frac{1}{48^{1/4}\,K(k_3)}$. Then we have the nice evaluations, $$\begin{aligned}\frac{3}{5^{5/6}} &=\,_2F_1\big(\tfrac{1}{3},\tfrac{1}{3};\tfrac{5}{6};-4\big)\\ &=\beta\,\int_0^1 \frac{dx}{\sqrt{1-x}\,\sqrt[3]{x^2+4x^3}}\\[1.7mm] &=\beta\,\int_{-1}^1\frac{dx}{\left(1-x^2\right)^{\small2/3} \sqrt[3]{\color{blue}{9+4\sqrt{5}}\,x}}\\[1.7mm] &=2^{1/3}\,\beta\,\int_0^\infty\frac{dx}{\sqrt[3]{9+\cosh x}} \end{aligned}\tag1$$ and, $$\begin{aligned}\frac{4}{7} &=\,_2F_1\big(\tfrac{1}{3},\tfrac{1}{3};\tfrac{5}{6};-27\big)\\ &=\beta\,\int_0^1 \frac{dx}{\sqrt{1-x}\,\sqrt[3]{x^2+27x^3}}\\[1.7mm] &=\beta\,\int_{-1}^1\frac{dx}{\left(1-x^2\right)^{\small2/3} \sqrt[3]{\color{blue}{55+12\sqrt{21}}\,x}}\\[1.7mm] &=2^{1/3}\,\beta\,\int_0^\infty\frac{dx}{\sqrt[3]{55+\cosh x}} \end{aligned}\tag2$$ Note the powers of fundamental units , $$U_{5}^6 = \big(\tfrac{1+\sqrt{5}}{2}\big)^6=\color{blue}{9+4\sqrt{5}}$$ $$U_{21}^3 = \big(\tfrac{5+\sqrt{21}}{2}\big)^3=\color{blue}{55+12\sqrt{21}}$$ Those two instances can't be coincidence. II. Quartic Define $\gamma= \tfrac{\sqrt{2\pi}}{\Gamma^2\big(\tfrac14\big)}= \frac{1}{2\sqrt2\,K(k_1)}=\frac1{2L}$ with lemniscate constant $L$. Then we have the nice, $$\begin{aligned}\frac{2}{3^{3/4}} &=\,_2F_1\big(\tfrac{1}{4},\tfrac{1}{4};\tfrac{3}{4};-3\big)\\ &=\gamma\,\int_0^1 \frac{dx}{\sqrt{1-x}\,\sqrt[4]{x^3+3x^4}}\\[1.7mm] &\overset{\color{red}?}=\gamma\,\int_{-1}^1\frac{dx}{\left(1-x^2\right)^{\small3/4} \sqrt[4]{\color{blue}{7+4\sqrt{3}}\,x}}\\[1.7mm] &=2^{1/4}\,\gamma\,\int_0^\infty\frac{dx}{\sqrt[4]{7+\cosh x}} \end{aligned}\tag3$$ and, $$\begin{aligned}\frac{3}{5}&=\,_2F_1\big(\tfrac{1}{4},\tfrac{1}{4};\tfrac{3}{4};-80\big)\\ &=\gamma\,\int_0^1 \frac{dx}{\sqrt{1-x}\,\sqrt[4]{x^3+80x^4}}\\[1.7mm] &\overset{\color{red}?}=\gamma\,\int_{-1}^1\frac{dx}{\left(1-x^2\right)^{\small3/4} \sqrt[4]{\color{blue}{161+72\sqrt{5}}\,x}}\\[1.7mm] &=2^{1/4}\,\gamma\,\int_0^\infty\frac{dx}{\sqrt[4]{161+\cosh x}} \end{aligned}\tag4$$ with $a=161$ given by Noam Elkies in this comment . (For $4$th roots, I just assumed the equality using the blue radicals based on the ones for cube roots.) Note again the powers of fundamental units, $$U_{3}^2 = \big(2+\sqrt3\big)^2=\color{blue}{7+4\sqrt{3}}$$ $$U_{5}^{12} = \big(\tfrac{1+\sqrt{5}}{2}\big)^{12}=\color{blue}{161+72\sqrt{5}}$$ Just like for the cube roots version, these can't be coincidence. Questions: Is it true these observations can be explained by, let $b=2a+1$, then, $$\int_0^1 \frac{dx}{\sqrt{1-x}\,\sqrt[3]{x^2+ax^3}}=\int_{-1}^1\frac{dx}{\left(1-x^2\right)^{\small2/3} \sqrt[3]{b+\sqrt{b^2-1}\,x}}=2^{1/3}\int_0^\infty\frac{dx}{\sqrt[3]{b+\cosh x}}$$ $$\int_0^1 \frac{dx}{\sqrt{1-x}\,\sqrt[4]{x^3+ax^4}}=\int_{-1}^1\frac{dx}{\left(1-x^2\right)^{\small3/4} \sqrt[4]{b+\sqrt{b^2-1}\,x}}=2^{1/4}\int_0^\infty\frac{dx}{\sqrt[4]{b+\cosh x}}$$",,"['calculus', 'integration', 'definite-integrals', 'pell-type-equations']"
28,"Closed form for $\sum^\infty_{n=1}\frac{H_n}{2^n\,(2n+1)^2}$",Closed form for,"\sum^\infty_{n=1}\frac{H_n}{2^n\,(2n+1)^2}","(This is a slight variation of another question , already answered) Can we find a closed form of the following series? $$S=\sum^\infty_{n=1}\frac{H_n}{2^n\,(2n+1)^2}\tag1$$ Using some non-rigorous numerical methods, I found a conjectured form: \begin{align} S &\stackrel?=\sqrt{2}\left[\frac{\ln^32}{10}-\frac{\ln^3\!\left(1+\sqrt2\right)}{10}+\frac{27}{40}\,\ln\left(1+\sqrt{2}\right)\cdot\ln^22 \right. \\ & \hspace{5mm} \left. -\pi^2\left(\frac{5\ln2}{24}+\frac{\ln\left(1+\sqrt2\right)}{20}\right)+\operatorname{Li}_2\!\left(\frac1{\sqrt2}\right)\cdot\ln2+\frac65\,\operatorname{Li}_3\!\left(\frac1{\sqrt2}\right) \right. \\ & \hspace{10mm} \left. -\frac3{10}\,\operatorname{Li}_3\!\left(\frac{2-\sqrt{2}}4\right)+\frac3{10}\,\operatorname{Li}_3\!\left(\frac{2+\sqrt{2}}4\right)-\frac{21}{160}\zeta(3)\right]\tag2 \end{align} It it possible to prove this result or further simplify it?","(This is a slight variation of another question , already answered) Can we find a closed form of the following series? $$S=\sum^\infty_{n=1}\frac{H_n}{2^n\,(2n+1)^2}\tag1$$ Using some non-rigorous numerical methods, I found a conjectured form: \begin{align} S &\stackrel?=\sqrt{2}\left[\frac{\ln^32}{10}-\frac{\ln^3\!\left(1+\sqrt2\right)}{10}+\frac{27}{40}\,\ln\left(1+\sqrt{2}\right)\cdot\ln^22 \right. \\ & \hspace{5mm} \left. -\pi^2\left(\frac{5\ln2}{24}+\frac{\ln\left(1+\sqrt2\right)}{20}\right)+\operatorname{Li}_2\!\left(\frac1{\sqrt2}\right)\cdot\ln2+\frac65\,\operatorname{Li}_3\!\left(\frac1{\sqrt2}\right) \right. \\ & \hspace{10mm} \left. -\frac3{10}\,\operatorname{Li}_3\!\left(\frac{2-\sqrt{2}}4\right)+\frac3{10}\,\operatorname{Li}_3\!\left(\frac{2+\sqrt{2}}4\right)-\frac{21}{160}\zeta(3)\right]\tag2 \end{align} It it possible to prove this result or further simplify it?",,"['calculus', 'sequences-and-series', 'closed-form', 'harmonic-numbers', 'polylogarithm']"
29,Changing the order of integration without sketching?,Changing the order of integration without sketching?,,"When changing the order of double integrals, I have always relied on sketching the region. I have recently come across this example on MSE by @FelixMartin which seems to avoid visual-based reasoning, and a comment below it says that it's ""very nice and neat"". But I have no idea what's going on with the $\Theta()$. Can someone please give me a hint? \begin{eqnarray*} \int_{0}^{8}\int_{\sqrt[3]{\vphantom{\large a}y\,}}^{2}{\rm f}\left(x, y\right)\,{\rm d}x\,{\rm d}y & = & \int_{0}^{8}\left\lbrack\int_{0}^{2}\Theta\left(x - \sqrt[3]{\vphantom{\large a}y\,} \right) {\rm f}\left(x, y\right)\,{\rm d}x\right\rbrack{\rm d}y \\ & = & \int_{0}^{2}\left\lbrack\int_{0}^{8}\Theta\left(x^{3} - y\right) {\rm f}\left(x, y\right)\,{\rm d}y\right\rbrack{\rm d}x \\ & = & \int_{0}^{2}\left\lbrack\int_{0}^{x^{3}} {\rm f}\left(x, y\right)\,{\rm d}y\right\rbrack{\rm d}x \end{eqnarray*} PS: sorry for the verbatim copying, I just reckoned it would save people some time.","When changing the order of double integrals, I have always relied on sketching the region. I have recently come across this example on MSE by @FelixMartin which seems to avoid visual-based reasoning, and a comment below it says that it's ""very nice and neat"". But I have no idea what's going on with the $\Theta()$. Can someone please give me a hint? \begin{eqnarray*} \int_{0}^{8}\int_{\sqrt[3]{\vphantom{\large a}y\,}}^{2}{\rm f}\left(x, y\right)\,{\rm d}x\,{\rm d}y & = & \int_{0}^{8}\left\lbrack\int_{0}^{2}\Theta\left(x - \sqrt[3]{\vphantom{\large a}y\,} \right) {\rm f}\left(x, y\right)\,{\rm d}x\right\rbrack{\rm d}y \\ & = & \int_{0}^{2}\left\lbrack\int_{0}^{8}\Theta\left(x^{3} - y\right) {\rm f}\left(x, y\right)\,{\rm d}y\right\rbrack{\rm d}x \\ & = & \int_{0}^{2}\left\lbrack\int_{0}^{x^{3}} {\rm f}\left(x, y\right)\,{\rm d}y\right\rbrack{\rm d}x \end{eqnarray*} PS: sorry for the verbatim copying, I just reckoned it would save people some time.",,"['calculus', 'integration', 'multivariable-calculus', 'definite-integrals', 'self-learning']"
30,tough integral involving the Cosine integral,tough integral involving the Cosine integral,,"I ran across an integral on a German math site that has a friend of mine and I quite stuck. They give, without derivation, $$\int_0^\infty \mathrm{Ci}(\alpha x)\mathrm{Ci}(\beta x)dx=\frac{\pi}{2 \max(\alpha,\beta)}$$ The Cosine Integral is defined as $\displaystyle \mathrm{Ci}(x)=-\int_x^\infty\frac{\cos(t)}{t}dt$ Does anyone know how this is derived?. We have looked around but can not find anything. I ran it through Maple using specific values for $\alpha$ and $\beta$. For instance, I used $\alpha=2, \;\ \beta=3$ and it gave $\dfrac{\pi}{6}$. Which indeed relates to the formula. The max of $\alpha$ and $\beta$ in this case is $\beta=3$. So, $\dfrac{\pi}{2\cdot 3}=\dfrac{\pi}{6}$. Does anyone know of this integral or its derivation?.  Thanks very much. If anyone is interested, here is a link to the site: http://de.wikibooks.org/wiki/Formelsammlung_Mathematik:_Bestimmte_Integrale:_Form_R%28x,Ci%29","I ran across an integral on a German math site that has a friend of mine and I quite stuck. They give, without derivation, $$\int_0^\infty \mathrm{Ci}(\alpha x)\mathrm{Ci}(\beta x)dx=\frac{\pi}{2 \max(\alpha,\beta)}$$ The Cosine Integral is defined as $\displaystyle \mathrm{Ci}(x)=-\int_x^\infty\frac{\cos(t)}{t}dt$ Does anyone know how this is derived?. We have looked around but can not find anything. I ran it through Maple using specific values for $\alpha$ and $\beta$. For instance, I used $\alpha=2, \;\ \beta=3$ and it gave $\dfrac{\pi}{6}$. Which indeed relates to the formula. The max of $\alpha$ and $\beta$ in this case is $\beta=3$. So, $\dfrac{\pi}{2\cdot 3}=\dfrac{\pi}{6}$. Does anyone know of this integral or its derivation?.  Thanks very much. If anyone is interested, here is a link to the site: http://de.wikibooks.org/wiki/Formelsammlung_Mathematik:_Bestimmte_Integrale:_Form_R%28x,Ci%29",,"['calculus', 'integration', 'special-functions']"
31,Existence of a surjective non-linear polynomial $P \in \Bbb Q[X]$,Existence of a surjective non-linear polynomial,P \in \Bbb Q[X],Does their exist a non-linear polynomial $P(x)$ such that for every rational number $y$ there exists a rational number $x$ such that $y=P(x)$?,Does their exist a non-linear polynomial $P(x)$ such that for every rational number $y$ there exists a rational number $x$ such that $y=P(x)$?,,"['calculus', 'abstract-algebra', 'algebra-precalculus', 'analysis', 'polynomials']"
32,"A interesting improper integral, $ \int_{0}^1\frac{\ln x}{x^2-x-1}\text{d}x$","A interesting improper integral,", \int_{0}^1\frac{\ln x}{x^2-x-1}\text{d}x,$$\displaystyle \int_{0}^1\frac{\ln x}{x^2-x-1}\text{d}x$$ I think there should be a smart way to evaluate this. But I cant see..,$$\displaystyle \int_{0}^1\frac{\ln x}{x^2-x-1}\text{d}x$$ I think there should be a smart way to evaluate this. But I cant see..,,"['calculus', 'integration', 'improper-integrals']"
33,Restricted Three-Body Problem,Restricted Three-Body Problem,,"The movement of a spacecraft between Earth and the Moon is an example of the infamous Three Body Problem. It is said that a general analytical solution for TBP is not known because of the complexity of solving the effect of three bodies which all pull on each other while moving, a total of six interactions. Mathematician Richard Arenstorf while at NASA solved a special case of this problem, by simplifying the interactions to four, because, the effect of the spacecraft's gravity upon the motion of the vastly more massive Earth and Moon is practically non-existent. Arenstorf found a stable orbit for a spacecraft orbiting between the Earth and Moon, shaped like an '8' http://en.wikipedia.org/wiki/Richard_Arenstorf Was Arenstorf's solution purely analytical, or did he use numerical mechanisms? Is the '8' shape an optimal path, meaning the route on which the spacecraft would expand the least amount of energy? If yes, how was this requirement included in the derivation in mathematical form? If anyone has a clean derivation for this problem, that would be great, or any links to books, other papers, etc. Note: Apparently there was an earlier related mathoverflow question on this as well: https://mathoverflow.net/questions/52489/on-the-non-rigorous-calculations-of-the-trajectories-in-the-moon-landings Arenstorf's technical report is here http://hdl.handle.net/2060/19630005545 Regards,","The movement of a spacecraft between Earth and the Moon is an example of the infamous Three Body Problem. It is said that a general analytical solution for TBP is not known because of the complexity of solving the effect of three bodies which all pull on each other while moving, a total of six interactions. Mathematician Richard Arenstorf while at NASA solved a special case of this problem, by simplifying the interactions to four, because, the effect of the spacecraft's gravity upon the motion of the vastly more massive Earth and Moon is practically non-existent. Arenstorf found a stable orbit for a spacecraft orbiting between the Earth and Moon, shaped like an '8' http://en.wikipedia.org/wiki/Richard_Arenstorf Was Arenstorf's solution purely analytical, or did he use numerical mechanisms? Is the '8' shape an optimal path, meaning the route on which the spacecraft would expand the least amount of energy? If yes, how was this requirement included in the derivation in mathematical form? If anyone has a clean derivation for this problem, that would be great, or any links to books, other papers, etc. Note: Apparently there was an earlier related mathoverflow question on this as well: https://mathoverflow.net/questions/52489/on-the-non-rigorous-calculations-of-the-trajectories-in-the-moon-landings Arenstorf's technical report is here http://hdl.handle.net/2060/19630005545 Regards,",,"['calculus', 'reference-request', 'ordinary-differential-equations', 'numerical-methods', 'classical-mechanics']"
34,"Evaluating $\int _{-100}^{100}\lfloor {x^3}\rfloor \,dx$",Evaluating,"\int _{-100}^{100}\lfloor {x^3}\rfloor \,dx","Is there an alternative better solution? $I=\displaystyle\int_{-100}^{100}\lfloor x^3\rfloor\,dx$ $=\displaystyle\int_{-100}^{100}\lfloor(100-100-x)^3\rfloor\,dx$  $\quad$ [$\because\int_{a}^{b}f(x)\,dx=\int_{a}^{b}f(a+b-x)\,dx$] $=\displaystyle\int_{-100}^{100}\lfloor-x^3\rfloor\,dx$ $=\displaystyle\int_{-100}^{100}(-\lfloor x^3\rfloor-1)\,dx$  $\quad$ [$\because \lfloor x\rfloor+\lfloor-x\rfloor=-1$ when $x\notin \mathbb{Z}$] $\Rightarrow I=-I-200$ $\quad$ $\Rightarrow I=-100$ EDIT. Is there any area interpretation of the integral?","Is there an alternative better solution? $I=\displaystyle\int_{-100}^{100}\lfloor x^3\rfloor\,dx$ $=\displaystyle\int_{-100}^{100}\lfloor(100-100-x)^3\rfloor\,dx$  $\quad$ [$\because\int_{a}^{b}f(x)\,dx=\int_{a}^{b}f(a+b-x)\,dx$] $=\displaystyle\int_{-100}^{100}\lfloor-x^3\rfloor\,dx$ $=\displaystyle\int_{-100}^{100}(-\lfloor x^3\rfloor-1)\,dx$  $\quad$ [$\because \lfloor x\rfloor+\lfloor-x\rfloor=-1$ when $x\notin \mathbb{Z}$] $\Rightarrow I=-I-200$ $\quad$ $\Rightarrow I=-100$ EDIT. Is there any area interpretation of the integral?",,"['calculus', 'integration']"
35,"Inequality of numerical integration $\int _0^\infty x^{-x}\,dx$.",Inequality of numerical integration .,"\int _0^\infty x^{-x}\,dx","There was a friend asking me how to prove $$\int_0^\infty x^{-x}\,dx<2$$ Mathematica showed that its approximate value is 1.99546, so I think it isn't easy to solve it, can you provide me some ideas about this question?","There was a friend asking me how to prove $$\int_0^\infty x^{-x}\,dx<2$$ Mathematica showed that its approximate value is 1.99546, so I think it isn't easy to solve it, can you provide me some ideas about this question?",,"['calculus', 'definite-integrals', 'integral-inequality']"
36,The limit of $\lim\limits_{n\to\infty} \exp(-1+\exp(-2+\exp(-3+\cdots\exp(-n) \cdots)))$.,The limit of .,\lim\limits_{n\to\infty} \exp(-1+\exp(-2+\exp(-3+\cdots\exp(-n) \cdots))),"Does the following limit exist ? $$\lim_{n\to\infty} \exp(-1+\exp(-2+\exp(-3+\cdots\exp(-n)\cdots)))$$ If yes, can it be expressed in a closed form ? PARI shows the following numerical value : n=-100;x=exp(n);while(n<-1,n=n+1;x=exp(x+n));print(x) $0.4241685586940448516119410516$ Within this precision, $-200$ yields the same value.","Does the following limit exist ? $$\lim_{n\to\infty} \exp(-1+\exp(-2+\exp(-3+\cdots\exp(-n)\cdots)))$$ If yes, can it be expressed in a closed form ? PARI shows the following numerical value : n=-100;x=exp(n);while(n<-1,n=n+1;x=exp(x+n));print(x) $0.4241685586940448516119410516$ Within this precision, $-200$ yields the same value.",,"['calculus', 'limits']"
37,"Why ""integralis"" over ""summatorius""?","Why ""integralis"" over ""summatorius""?",,"It is written that Johann Bernoulli suggested to Leibniz that he (Leibniz) change the name of his calculus from ""calculus summatorius"" to ""calculus integralis"", but I cannot find their correspondence wherein Bernoulli explains why he thinks ""integralis"" is preferable to ""summatorius."" Can someone enlighten me? Thank you.","It is written that Johann Bernoulli suggested to Leibniz that he (Leibniz) change the name of his calculus from ""calculus summatorius"" to ""calculus integralis"", but I cannot find their correspondence wherein Bernoulli explains why he thinks ""integralis"" is preferable to ""summatorius."" Can someone enlighten me? Thank you.",,"['calculus', 'terminology', 'math-history']"
38,Calculate $\lim_{n\rightarrow\infty}\frac{\int_{0}^{1}f^n(x)\ln(x+2)dx}{\int_{0}^{1}f^n(x)dx}$,Calculate,\lim_{n\rightarrow\infty}\frac{\int_{0}^{1}f^n(x)\ln(x+2)dx}{\int_{0}^{1}f^n(x)dx},"Given $$f(x)=1-x^2+x^3 \qquad x\in[0,1]$$ calculate $$ \lim_{n\rightarrow\infty}\frac{\int_{0}^{1}f^n(x)\ln(x+2)dx}{\int_{0}^{1}f^n(x)dx} $$ where $f^n(x)=\underbrace{f(x)·f(x)·\dots\text{·}f(x)}_{n\ \text{times}}$ . This is a question from CMC(Mathematics competition of Chinese)in $2017$ . The solution provides an idea: given $s∈(0,\frac{1}{2}),$ prove: $$\lim_{n\rightarrow\infty}\frac{\int_{s}^{1}f^n(x)dx}{\int_{0}^{s}f^n(x)dx}=0\\$$ The final result is $\ln2.$ My approach For this: $$\lim_{n\rightarrow\infty}\frac{\int_{s}^{1}f^n(x)dx}{\int_{0}^{s}f^n(x)dx}=0\\$$ I want to do piecewise calculation: $$\int_{s}^{1-s}f^n(x)dx+\int_{1-s}^{1}f^n(x)dx.$$ For this: $$\lim_{n\rightarrow\infty}\frac{\int_{1-s}^{1}f^n(x)dx}{\int_{0}^{s}f^n(x)dx}=0.\\$$ Here is the proof: when $\ \ n≥\frac{1}{s^2}$ , $$\frac{\int_{1-s}^{1}f^n(x)dx}{\int_{0}^{s}f^n(x)dx}=\frac{\int_{0}^{s}(1-x^2(1-x))^ndx}{\int_{0}^{s}(1-x(1-x)^2)^ndx}\\\leq\frac{\int_{0}^{s}(1-\frac{x}{4})^ndx}{\int_{0}^{s}(1-x^2)^ndx}\leq\frac{\int_{0}^{s}(1-\frac{x}{4})^ndx}{\int_{0}^{1/\sqrt{n}}(1-\frac{x}{\sqrt{n}})^ndx}\\=\frac{\frac{4}{n+1}(1-(1-\frac{s}{4})^{n+1})}{\frac{\sqrt{n}}{n+1}(1-(1-\frac{1}{n})^{n+1})}\sim\frac{4}{\sqrt{n}(1-\frac{1}{e})}\rightarrow0.\\$$ For this: $$\lim_{n\rightarrow\infty}\frac{\int_{s}^{1-s}f^n(x)dx}{\int_{0}^{s}f^n(x)dx}=0.\\$$ Here is the proof: given $t,0<t<s<\frac{1}{2},$ then $$f(t)>f(s)>f(1-s).$$ Define $m_t=\min_{x\in[0,t]}f(x),M_s=\max_{x\in[s,1-s]}f(x),$ so $$m_t=f(t)>f(1-s)=M_s.$$ $$\frac{\int_{s}^{1-s}f^n(x)dx}{\int_{0}^{s}f^n(x)dx}\leq\frac{\int_{s}^{1-s}f^n(x)dx}{\int_{0}^{t}f^n(x)dx}$$ $$\leq\frac{(1-2s)M_s ^n}{tm_t ^n}=\frac{1-2s}{t}(\frac{M_s}{m_t})^n\rightarrow0.\\$$ In conclusion,we can get: $$\lim_{n\rightarrow\infty}\frac{\int_{s}^{1}f^n(x)dx}{\int_{0}^{s}f^n(x)dx}=0.$$","Given calculate where . This is a question from CMC(Mathematics competition of Chinese)in . The solution provides an idea: given prove: The final result is My approach For this: I want to do piecewise calculation: For this: Here is the proof: when , For this: Here is the proof: given then Define so In conclusion,we can get:","f(x)=1-x^2+x^3 \qquad x\in[0,1] 
\lim_{n\rightarrow\infty}\frac{\int_{0}^{1}f^n(x)\ln(x+2)dx}{\int_{0}^{1}f^n(x)dx}
 f^n(x)=\underbrace{f(x)·f(x)·\dots\text{·}f(x)}_{n\ \text{times}} 2017 s∈(0,\frac{1}{2}), \lim_{n\rightarrow\infty}\frac{\int_{s}^{1}f^n(x)dx}{\int_{0}^{s}f^n(x)dx}=0\\ \ln2. \lim_{n\rightarrow\infty}\frac{\int_{s}^{1}f^n(x)dx}{\int_{0}^{s}f^n(x)dx}=0\\ \int_{s}^{1-s}f^n(x)dx+\int_{1-s}^{1}f^n(x)dx. \lim_{n\rightarrow\infty}\frac{\int_{1-s}^{1}f^n(x)dx}{\int_{0}^{s}f^n(x)dx}=0.\\ \ \ n≥\frac{1}{s^2} \frac{\int_{1-s}^{1}f^n(x)dx}{\int_{0}^{s}f^n(x)dx}=\frac{\int_{0}^{s}(1-x^2(1-x))^ndx}{\int_{0}^{s}(1-x(1-x)^2)^ndx}\\\leq\frac{\int_{0}^{s}(1-\frac{x}{4})^ndx}{\int_{0}^{s}(1-x^2)^ndx}\leq\frac{\int_{0}^{s}(1-\frac{x}{4})^ndx}{\int_{0}^{1/\sqrt{n}}(1-\frac{x}{\sqrt{n}})^ndx}\\=\frac{\frac{4}{n+1}(1-(1-\frac{s}{4})^{n+1})}{\frac{\sqrt{n}}{n+1}(1-(1-\frac{1}{n})^{n+1})}\sim\frac{4}{\sqrt{n}(1-\frac{1}{e})}\rightarrow0.\\ \lim_{n\rightarrow\infty}\frac{\int_{s}^{1-s}f^n(x)dx}{\int_{0}^{s}f^n(x)dx}=0.\\ t,0<t<s<\frac{1}{2}, f(t)>f(s)>f(1-s). m_t=\min_{x\in[0,t]}f(x),M_s=\max_{x\in[s,1-s]}f(x), m_t=f(t)>f(1-s)=M_s. \frac{\int_{s}^{1-s}f^n(x)dx}{\int_{0}^{s}f^n(x)dx}\leq\frac{\int_{s}^{1-s}f^n(x)dx}{\int_{0}^{t}f^n(x)dx} \leq\frac{(1-2s)M_s ^n}{tm_t ^n}=\frac{1-2s}{t}(\frac{M_s}{m_t})^n\rightarrow0.\\ \lim_{n\rightarrow\infty}\frac{\int_{s}^{1}f^n(x)dx}{\int_{0}^{s}f^n(x)dx}=0.","['calculus', 'limits', 'contest-math']"
39,Struggling to understand epsilon-delta,Struggling to understand epsilon-delta,,"The definition of a limit is: $\lim_{x\to a}f(x)=L$ if for every $\epsilon > 0$ there is a $\delta > 0$ so that whenever $0 < \lvert x - a \rvert < \delta$ we have $\lvert f(x) - L \rvert < \epsilon$ Now it seems pretty intuitive. But I am hung up on a few problems: Many pictures show something like this: epsilon-delta This seems intuitive at first and it demonstrates that $\lvert x - a \rvert$ and $\lvert f(x) - L \rvert < \epsilon$ are not necessarily the same (as the graph can be deceptive, especially if $f(x)$ is a straight line) as when you are projecting from $L$ to the graph down to $a$, $\lvert x - a \rvert$ and $\lvert f(x) - L \rvert$ will be different. The problem in my understanding became apparent when I saw a similar graph in a textbook where the projected lines were not $\lvert f(x) - L \rvert$, but projection for aesthetic purposes and that it was bounded by $\lvert f(x) - L \rvert$. I then realized I don't get it geometrically at all (Google ""mooculus"", page 20). I don't understand what the ""verification"" in the proof is. It seems to be a tautology. For example take $\lvert f(x) - L \rvert < \epsilon \Longrightarrow \lvert (3x - 1) - 2 \rvert < \epsilon$. You will eventually get to $\lvert x - 1 \rvert < \epsilon/3$. Then the proof is ""completed"" by showing that $\lvert x - a \rvert < \delta \Longrightarrow \lvert x - 1 \rvert < \epsilon/3 \Longrightarrow \lvert f(x) - L \rvert < \epsilon$. But $\delta$ is taken to be $\epsilon/3$. It seems to be the equivalent of demonstrating that $x + 1 = 2$ by plugging in $-3$ into it. The proof starts by either assuming the limit exists or doesn't exist. In fact, I've found many textbooks or teachers to take this approach: Think of it as a game. You give me an $\epsilon > 0$ and I can give you a $\delta > 0 $... But they typically omit a glaring part: that if this doesn't hold, the limit doesn't exist. Also glaringly missing, I haven't seen an example of a proof that isn't simply ""proving"" or ""showing"" the premise that is already assumed! How would you for example use the epsilon-delta definition to show a limit doesn't exist if you don't already know in advance it is the case? To extend on number 3, I'm aware that you must choose an $\epsilon$ and that if you prove it for one, you prove it for all. However, the catch is in cases where $\lvert x - a \rvert$ needs to be restricted (i.e, $\lvert x - a \rvert < 1$) $1$ is typically chosen but this does not work in all cases! I have no intuition on how to choose an $\epsilon$ let alone know if I'm simply doing something wrong or if the limit does not exist. That is, proving the negative seems more difficult. Can someone explain it in a different way? I've resorted to many different .edu sources, free online textbooks and even questions on this site and the pedagogy doesn't seem to be reaching me.","The definition of a limit is: $\lim_{x\to a}f(x)=L$ if for every $\epsilon > 0$ there is a $\delta > 0$ so that whenever $0 < \lvert x - a \rvert < \delta$ we have $\lvert f(x) - L \rvert < \epsilon$ Now it seems pretty intuitive. But I am hung up on a few problems: Many pictures show something like this: epsilon-delta This seems intuitive at first and it demonstrates that $\lvert x - a \rvert$ and $\lvert f(x) - L \rvert < \epsilon$ are not necessarily the same (as the graph can be deceptive, especially if $f(x)$ is a straight line) as when you are projecting from $L$ to the graph down to $a$, $\lvert x - a \rvert$ and $\lvert f(x) - L \rvert$ will be different. The problem in my understanding became apparent when I saw a similar graph in a textbook where the projected lines were not $\lvert f(x) - L \rvert$, but projection for aesthetic purposes and that it was bounded by $\lvert f(x) - L \rvert$. I then realized I don't get it geometrically at all (Google ""mooculus"", page 20). I don't understand what the ""verification"" in the proof is. It seems to be a tautology. For example take $\lvert f(x) - L \rvert < \epsilon \Longrightarrow \lvert (3x - 1) - 2 \rvert < \epsilon$. You will eventually get to $\lvert x - 1 \rvert < \epsilon/3$. Then the proof is ""completed"" by showing that $\lvert x - a \rvert < \delta \Longrightarrow \lvert x - 1 \rvert < \epsilon/3 \Longrightarrow \lvert f(x) - L \rvert < \epsilon$. But $\delta$ is taken to be $\epsilon/3$. It seems to be the equivalent of demonstrating that $x + 1 = 2$ by plugging in $-3$ into it. The proof starts by either assuming the limit exists or doesn't exist. In fact, I've found many textbooks or teachers to take this approach: Think of it as a game. You give me an $\epsilon > 0$ and I can give you a $\delta > 0 $... But they typically omit a glaring part: that if this doesn't hold, the limit doesn't exist. Also glaringly missing, I haven't seen an example of a proof that isn't simply ""proving"" or ""showing"" the premise that is already assumed! How would you for example use the epsilon-delta definition to show a limit doesn't exist if you don't already know in advance it is the case? To extend on number 3, I'm aware that you must choose an $\epsilon$ and that if you prove it for one, you prove it for all. However, the catch is in cases where $\lvert x - a \rvert$ needs to be restricted (i.e, $\lvert x - a \rvert < 1$) $1$ is typically chosen but this does not work in all cases! I have no intuition on how to choose an $\epsilon$ let alone know if I'm simply doing something wrong or if the limit does not exist. That is, proving the negative seems more difficult. Can someone explain it in a different way? I've resorted to many different .edu sources, free online textbooks and even questions on this site and the pedagogy doesn't seem to be reaching me.",,"['calculus', 'limits']"
40,Finding the limit of $\sqrt[n]{{kn \choose n}}$,Finding the limit of,\sqrt[n]{{kn \choose n}},"As part of homework I'm trying to find the limit of $\sqrt[n]{{kn \choose n}}$ (with $k\in\mathbb{N}$ a given parameter). I've seen This limit: $\lim_{n \rightarrow \infty} \sqrt [n] {nk \choose n}$. , on which the only answer suggests using Stirling's approximation, which I've never learned and I can't seem to understand. Trying to solve this myself, I first tried finding the ratio directly, which gave me $$\frac{{kn+k \choose n+1}}{{kn \choose n}}=\frac{\frac{\left(kn+k\right)!}{\left(n+1\right)!\left(kn+k-n-1\right)!}}{\frac{kn!}{n!\left(kn-n\right)!}}=\frac{\left(kn+k\right)!n!\left(kn-n\right)!}{\left(n+1\right)!\left(kn+k-n-1\right)!kn!}  =\frac{\left(kn+k-n\right)\left(kn+k-n+1\right)\cdots\left(kn+k\right)}{\left(n+1\right)\left(kn-n+1\right)\cdots\left(kn-n\right)} $$ That I'm not really sure how to use. So instead I tried finding something that I might be able to say is bigger and smaller to use squeeze on, which gave me: $${kn \choose n}=\frac{kn!}{n!\left(kn-n\right)!}=\frac{\left(kn-n+1\right)\cdots\left(kn\right)}{n!}\leq k^{n}\frac{n^{n}}{n!}$$ And then showing that  $$ \frac{\frac{\left(n+1\right)^{n+1}}{\left(n+1\right)!}}{\frac{n^{n}}{n!}}=\frac{\left(n+1\right)^{n+1}n!}{n^{n}\left(n+1\right)!}=\frac{\left(n+1\right)^{n}\left(n+1\right)}{n^{n}\left(n+1\right)}=\left(1+\frac{1}{n}\right)^{n}\to\epsilon$$ gives the larger limit of $$\sqrt[n]{{kn \choose n}}\leq\sqrt[n]{k^{n}\frac{n^{n}}{n!}}=\sqrt[n]{k^{n}}\sqrt[n]{\frac{n^{n}}{n!}}=k\cdot\sqrt[n]{\frac{n^{n}}{n!}}\to k\cdot e  $$ But it's obviously not the limit itself since for i.e. $k=1$ the limit is 1 and $k=2$ the limit is 4. So I'm pretty much stuck, any hints on how I might be able to solve it?","As part of homework I'm trying to find the limit of $\sqrt[n]{{kn \choose n}}$ (with $k\in\mathbb{N}$ a given parameter). I've seen This limit: $\lim_{n \rightarrow \infty} \sqrt [n] {nk \choose n}$. , on which the only answer suggests using Stirling's approximation, which I've never learned and I can't seem to understand. Trying to solve this myself, I first tried finding the ratio directly, which gave me $$\frac{{kn+k \choose n+1}}{{kn \choose n}}=\frac{\frac{\left(kn+k\right)!}{\left(n+1\right)!\left(kn+k-n-1\right)!}}{\frac{kn!}{n!\left(kn-n\right)!}}=\frac{\left(kn+k\right)!n!\left(kn-n\right)!}{\left(n+1\right)!\left(kn+k-n-1\right)!kn!}  =\frac{\left(kn+k-n\right)\left(kn+k-n+1\right)\cdots\left(kn+k\right)}{\left(n+1\right)\left(kn-n+1\right)\cdots\left(kn-n\right)} $$ That I'm not really sure how to use. So instead I tried finding something that I might be able to say is bigger and smaller to use squeeze on, which gave me: $${kn \choose n}=\frac{kn!}{n!\left(kn-n\right)!}=\frac{\left(kn-n+1\right)\cdots\left(kn\right)}{n!}\leq k^{n}\frac{n^{n}}{n!}$$ And then showing that  $$ \frac{\frac{\left(n+1\right)^{n+1}}{\left(n+1\right)!}}{\frac{n^{n}}{n!}}=\frac{\left(n+1\right)^{n+1}n!}{n^{n}\left(n+1\right)!}=\frac{\left(n+1\right)^{n}\left(n+1\right)}{n^{n}\left(n+1\right)}=\left(1+\frac{1}{n}\right)^{n}\to\epsilon$$ gives the larger limit of $$\sqrt[n]{{kn \choose n}}\leq\sqrt[n]{k^{n}\frac{n^{n}}{n!}}=\sqrt[n]{k^{n}}\sqrt[n]{\frac{n^{n}}{n!}}=k\cdot\sqrt[n]{\frac{n^{n}}{n!}}\to k\cdot e  $$ But it's obviously not the limit itself since for i.e. $k=1$ the limit is 1 and $k=2$ the limit is 4. So I'm pretty much stuck, any hints on how I might be able to solve it?",,"['calculus', 'sequences-and-series', 'limits-without-lhopital']"
41,Intuition behind convex functions,Intuition behind convex functions,,"For me, possibly the most out-of-nowhere definition of the first semester of Calculus was the following definition of a convex function and its equivalents. Function $f$ is convex on the interval $J$ if for $\forall x,y\in J$ and $\forall\lambda\in (0,1)$ is $$f(\lambda x + (1-\lambda )y)\leq\lambda f(x) + (1-\lambda )f(y)\tag1$$ Equivalently if $\forall u,v,w\in J:u<v<w$ $$f(v)(w-u)\leq f(w)(v-u)+f(u)(w-v)\tag2$$ or $$\frac{f(v)-f(u)}{v-u}\leq \frac{f(w)-f(v)}{w-v}\tag3$$ I'm looking for an intuition, or visual representation of what these three definitions ""actually"" mean. (3) , being very similar to the definition of a derivative, is the only one that makes sense to me, that is: a function is convex if the slope between points $(u,f(u))$ and $(v,f(v))$ is lesser than the slope between $(v,f(v))$ and $(w,f(w))$ . (2) seems to look at areas of rectangles, however, that is about everything I could say about it. (1) Got it! $f(\lambda x + (1-\lambda )y)$ is the functional value of a point between $x$ and $y$ and $\lambda f(x) + (1-\lambda )f(y)$ is a point between $f(x)$ and $f(y)$ on a slope between the two points, thus represented by the fact that the slope is never below the functional value. I can now see that it represents the fact that the slope between $x$ and $y$ is always above the function, I don't see, however, how $\lambda f(x) + (1-\lambda )f(y)$ is a point on the slope. Thanks for any help!","For me, possibly the most out-of-nowhere definition of the first semester of Calculus was the following definition of a convex function and its equivalents. Function is convex on the interval if for and is Equivalently if or I'm looking for an intuition, or visual representation of what these three definitions ""actually"" mean. (3) , being very similar to the definition of a derivative, is the only one that makes sense to me, that is: a function is convex if the slope between points and is lesser than the slope between and . (2) seems to look at areas of rectangles, however, that is about everything I could say about it. (1) Got it! is the functional value of a point between and and is a point between and on a slope between the two points, thus represented by the fact that the slope is never below the functional value. I can now see that it represents the fact that the slope between and is always above the function, I don't see, however, how is a point on the slope. Thanks for any help!","f J \forall x,y\in J \forall\lambda\in (0,1) f(\lambda x + (1-\lambda )y)\leq\lambda f(x) + (1-\lambda )f(y)\tag1 \forall u,v,w\in J:u<v<w f(v)(w-u)\leq f(w)(v-u)+f(u)(w-v)\tag2 \frac{f(v)-f(u)}{v-u}\leq \frac{f(w)-f(v)}{w-v}\tag3 (u,f(u)) (v,f(v)) (v,f(v)) (w,f(w)) f(\lambda x + (1-\lambda )y) x y \lambda f(x) + (1-\lambda )f(y) f(x) f(y) x y \lambda f(x) + (1-\lambda )f(y)","['calculus', 'functions']"
42,Complete statistic: Poisson Distribution,Complete statistic: Poisson Distribution,,"Context I am having difficulty trying to understand a step of a proof which relies on a property of series. Proof Suppose that $X_1, X_2, \ldots , X_n$ is a random sample of size $n$ from a Poisson distribution with parameter $\lambda > 0$. The goal is to show that $T = \sum_{i=1}^n X_i$ is a complete statistic. Since we know that $T =  \sum_{i=1}^n X_i \sim \mathrm{Poisson}(n\lambda)$: $$ \mathbb{E}(h(T)) = \sum_{k=0}^{\infty} h(k) \, e^{-n\lambda} \, \frac{(n\lambda)^k}{k!} = 0\Longrightarrow \sum_{k=0}^{\infty} h(k)  \, \frac{(n\lambda)^k}{k!} = 0  $$ The textbook I am using and some others sources I've found argue that: $$ \boxed{\displaystyle\sum_{k=0}^{\infty} h(k)  \, \frac{(n\lambda)^k}{k!} = 0 \Longrightarrow h(k)  \, \frac{(n\lambda)^k}{k!}  = 0 \qquad \forall k} $$ It probably is an obvious result from calculus, but I am unable to prove it. If $ h(k) \, (n\lambda)^k/k!  = 0$ for all $k$ then $T$ is a complete statistic because $\lambda$ is nonnegative and then $h(k) = 0$ for all $k$ .","Context I am having difficulty trying to understand a step of a proof which relies on a property of series. Proof Suppose that $X_1, X_2, \ldots , X_n$ is a random sample of size $n$ from a Poisson distribution with parameter $\lambda > 0$. The goal is to show that $T = \sum_{i=1}^n X_i$ is a complete statistic. Since we know that $T =  \sum_{i=1}^n X_i \sim \mathrm{Poisson}(n\lambda)$: $$ \mathbb{E}(h(T)) = \sum_{k=0}^{\infty} h(k) \, e^{-n\lambda} \, \frac{(n\lambda)^k}{k!} = 0\Longrightarrow \sum_{k=0}^{\infty} h(k)  \, \frac{(n\lambda)^k}{k!} = 0  $$ The textbook I am using and some others sources I've found argue that: $$ \boxed{\displaystyle\sum_{k=0}^{\infty} h(k)  \, \frac{(n\lambda)^k}{k!} = 0 \Longrightarrow h(k)  \, \frac{(n\lambda)^k}{k!}  = 0 \qquad \forall k} $$ It probably is an obvious result from calculus, but I am unable to prove it. If $ h(k) \, (n\lambda)^k/k!  = 0$ for all $k$ then $T$ is a complete statistic because $\lambda$ is nonnegative and then $h(k) = 0$ for all $k$ .",,"['calculus', 'statistics']"
43,Geometric Proofs of Calculus Theorems,Geometric Proofs of Calculus Theorems,,"I just started learning ""rigorous"" calculus, and I noticed that a lot of calculus theorems are rather obvious from the geometrical point of few. Some examples: 1. Prove that the derivative of an odd (resp. even) function, when exists, is even (resp. odd) Since the graph is symmetric with respect to the origin, the slopes of the tangent lines at $x=a$ and $x=-a$ must equal, hence the derivative is even. Similarly for the even function. 2. If $f$ is one-one and continuous, then $f^{-1}$ is also continuous. This can't be more obvious. $f^{-1}$ is just the reflection of $f$ about the line $y=x$ . 3. $\displaystyle\int_a^bf(x)dx+\int_b^cf(x)dx=\int_a^cf(x)dx$ The sum of the area from $a$ to $b$ and the area from $b$ to $c$ is, of course, the area from $a$ to $c$ . 4. If $f$ is a one-one function one $[a,b]$ , then $\displaystyle\int_a^bf(x)dx+\int_{f(a)}^{f(b)}f^{-1}(x)dx=bf(b)-af(a).$ Quite clear if you draw a diagram. The sum of the two integrals is the difference of two rectangles. 5. If $f$ is increasing and $f(0)=0$ , then for $a,b>0$ we have $\displaystyle\int_0^af(a)dx+\int_0^bf^{-1}(x)dx\ge ab$ . Also clear from the diagram. There's a ""leftover"" part outside the rectangle of area $ab$ . Of course, these proofs are not rigorous, and possibly not valid at all. Firstly, I only consider the easy cases. Secondly, I used the intuitive properties of geometric objects without proofs. So I'm wondering, is there any theory that connects calculus with geometry rigorously? Such a theory would help simplify calculus proofs tremendously, as I roughly outlined above.","I just started learning ""rigorous"" calculus, and I noticed that a lot of calculus theorems are rather obvious from the geometrical point of few. Some examples: 1. Prove that the derivative of an odd (resp. even) function, when exists, is even (resp. odd) Since the graph is symmetric with respect to the origin, the slopes of the tangent lines at and must equal, hence the derivative is even. Similarly for the even function. 2. If is one-one and continuous, then is also continuous. This can't be more obvious. is just the reflection of about the line . 3. The sum of the area from to and the area from to is, of course, the area from to . 4. If is a one-one function one , then Quite clear if you draw a diagram. The sum of the two integrals is the difference of two rectangles. 5. If is increasing and , then for we have . Also clear from the diagram. There's a ""leftover"" part outside the rectangle of area . Of course, these proofs are not rigorous, and possibly not valid at all. Firstly, I only consider the easy cases. Secondly, I used the intuitive properties of geometric objects without proofs. So I'm wondering, is there any theory that connects calculus with geometry rigorously? Such a theory would help simplify calculus proofs tremendously, as I roughly outlined above.","x=a x=-a f f^{-1} f^{-1} f y=x \displaystyle\int_a^bf(x)dx+\int_b^cf(x)dx=\int_a^cf(x)dx a b b c a c f [a,b] \displaystyle\int_a^bf(x)dx+\int_{f(a)}^{f(b)}f^{-1}(x)dx=bf(b)-af(a). f f(0)=0 a,b>0 \displaystyle\int_0^af(a)dx+\int_0^bf^{-1}(x)dx\ge ab ab","['calculus', 'geometry']"
44,Confusion with Courant: Which of his two calculus books is THE one?,Confusion with Courant: Which of his two calculus books is THE one?,,"Since I've worked my way through Spivak's Calculus book, I thought I'd give Courant's allegedly fantastic exposition of the subject a go as well. However, I've run into a problem. People in stackexchange threads always praise and suggest Courant's Calculus without specifying whether that's supposed to be his ""Differential and Integral Calculus"" or his ""Introduction to Calculus and Analysis"" book. So my question is: which one of these two is the ""famous"" and ""legendary"" Calculus book that everybody always talks about when the ""great three"" - Spivak, Apostol and Courant - are mentioned or recommended to people asking for a first course in calculus? Note: I'm a math major. I'm mostly interested in what the oft-mentioned calculus book by Courant is, and not which of his two books would suit my prior exposure to calculus (Spivak) best in terms of the follow-up level. That is not to say I wouldn't appreciate an informed opinion on that matter, I certainly would (and I hope some experienced readers will be able to enlighten me), but I'm primarily asking this question to find out which is the more canonical one.","Since I've worked my way through Spivak's Calculus book, I thought I'd give Courant's allegedly fantastic exposition of the subject a go as well. However, I've run into a problem. People in stackexchange threads always praise and suggest Courant's Calculus without specifying whether that's supposed to be his ""Differential and Integral Calculus"" or his ""Introduction to Calculus and Analysis"" book. So my question is: which one of these two is the ""famous"" and ""legendary"" Calculus book that everybody always talks about when the ""great three"" - Spivak, Apostol and Courant - are mentioned or recommended to people asking for a first course in calculus? Note: I'm a math major. I'm mostly interested in what the oft-mentioned calculus book by Courant is, and not which of his two books would suit my prior exposure to calculus (Spivak) best in terms of the follow-up level. That is not to say I wouldn't appreciate an informed opinion on that matter, I certainly would (and I hope some experienced readers will be able to enlighten me), but I'm primarily asking this question to find out which is the more canonical one.",,"['calculus', 'reference-request']"
45,Non-Circular Proof of $\lim_{x \to 0} \frac{\sin x}{x} = 1$,Non-Circular Proof of,\lim_{x \to 0} \frac{\sin x}{x} = 1,"I'm looking for a convincing proof, using first principles, that $$\lim_{x \to 0}\frac{\sin x}{x} = 1$$ (Please use ordinary unit circle definitions of trigonometric functions.) It occurred to me that the classic proof, which compares three areas, uses the formula ${1\over 2}r^2\theta$ for the area of a circular sector of angle $\theta$, which in turn assumes the area of a circle is $\pi r^2$. But this fact is almost always proven in texts using an integral, which ends up using the derivatives of $\sin$ and $\cos$, and we're back to that limit again. So I need a non-circular proof that doesn't rely on playing definition games (""let $\sin$ be the following power series...""). The answer to this question is definitely playing definition games. Sorry for the pun.","I'm looking for a convincing proof, using first principles, that $$\lim_{x \to 0}\frac{\sin x}{x} = 1$$ (Please use ordinary unit circle definitions of trigonometric functions.) It occurred to me that the classic proof, which compares three areas, uses the formula ${1\over 2}r^2\theta$ for the area of a circular sector of angle $\theta$, which in turn assumes the area of a circle is $\pi r^2$. But this fact is almost always proven in texts using an integral, which ends up using the derivatives of $\sin$ and $\cos$, and we're back to that limit again. So I need a non-circular proof that doesn't rely on playing definition games (""let $\sin$ be the following power series...""). The answer to this question is definitely playing definition games. Sorry for the pun.",,['calculus']
46,Applications of functions of the form $f(x)^{g(x)}$,Applications of functions of the form,f(x)^{g(x)},"Early on in my calculus education, I learned how to take the derivative of $x^x$ by re-writing it in the form $e^{x\ln x}$.  More generally, this technique is helpful in finding the derivative of functions of the form $f(x)^{g(x)}$, where $f(x)$ and $g(x)$ are non-constant, positive, differentiable functions. Now, as a teacher of calculus, I'm starting to ask why we care about such examples, and why we teach this ""logarithmic differentiation"" technique.  Of course, computing derivatives is a fine end in itself, but one of the appealing aspects of calculus to most beginning students is that its techniques are saturated with application in engineering/physics/economics/etc.  However, I don't think I've ever seen an application of a function of the form $f(x)^{g(x)}$ where neither $f(x)$ nor $g(x)$ are constant. Has anyone ever seen such a function ($x^{2\sin^2 x+2}$, $(x^4+1)^{\frac{1}{x^2}}$, $\ldots$) arise in a suitably 'natural' way?","Early on in my calculus education, I learned how to take the derivative of $x^x$ by re-writing it in the form $e^{x\ln x}$.  More generally, this technique is helpful in finding the derivative of functions of the form $f(x)^{g(x)}$, where $f(x)$ and $g(x)$ are non-constant, positive, differentiable functions. Now, as a teacher of calculus, I'm starting to ask why we care about such examples, and why we teach this ""logarithmic differentiation"" technique.  Of course, computing derivatives is a fine end in itself, but one of the appealing aspects of calculus to most beginning students is that its techniques are saturated with application in engineering/physics/economics/etc.  However, I don't think I've ever seen an application of a function of the form $f(x)^{g(x)}$ where neither $f(x)$ nor $g(x)$ are constant. Has anyone ever seen such a function ($x^{2\sin^2 x+2}$, $(x^4+1)^{\frac{1}{x^2}}$, $\ldots$) arise in a suitably 'natural' way?",,"['calculus', 'derivatives', 'education', 'applications', 'motivation']"
47,Sum of the first integer powers of $n$ up to k,Sum of the first integer powers of  up to k,n,"Pascal's triangle has a lot of interesting patterns in it; one of which is the triangular numbers and their extensions. Mathematically: $$\sum_{n=1}^k1=\frac{k}{1}$$ $$\sum_{n=1}^kn=\frac{k}{1}\cdot\frac{k+1}{2}$$ $$\sum_{n=1}^kn^2=\frac{k}{1}\cdot\frac{k+1}{2}\cdot\frac{2k+1}{3}$$ At first, we could guess that the next summation is: $$\sum_{n=1}^kn^3 ?=\frac{k}{1}\cdot\frac{k+1}{2}\cdot\frac{2k+1}{3}\cdot\frac{3k+1}{4}$$ Yet this is off. However, it is off geometrically . Notice: $$\left(\sum_{n=1}^kn^3\right)-\frac{k}{1}\cdot\frac{k+1}{2}\cdot\frac{2k+1}{3}\cdot\frac{3k+1}{4}=error$$ $$k=1, r=0$$ $$k=2, r=0.25$$ $$k=3, r=1$$ $$k=4, r=2.5$$ $$k=5, r=5$$ $$k=6, r=8.75$$ ... Consider the ratios of the errors: $$er(k)=\frac{r(k+1)}{r(k)}$$ $$k=1, r=udf$$ $$k=2, r=4$$ $$k=3, r=2.5$$ $$k=4, r=2$$ $$k=5, r=1.75$$ $$k=6, r=1.6$$ Then, rewriting the error as a function of n starting at k = 5: $$1.75=2.5-\frac{1.5}{2}$$ $$1.6=2.5-\frac{1.5}{2}-\frac{1.5}{10}$$ $$1.5=2.5-\frac{1.5}{2}-\frac{1.5}{10}-\frac{1.5}{15}$$ $$1.42857=2.5-\frac{1.5}{2}-\frac{1.5}{10}-\frac{1.5}{15}-\frac{1.5}{21}$$ The denominators in the series are from pascals triangle: (3rd columns, or dependent again on the triangular numbers) Then the total formula for equating the two is: $$\left(\frac{k}{1}\cdot\frac{\left(k+1\right)}{2}\cdot\frac{\left(2k+1\right)}{3}\cdot\frac{\left(3k+1\right)}{4}\right)-\left(\sum_{n=1}^kn^3\right)+\frac{1}{24}\left(k-1\right)k\left(k+1\right)=0$$ Super interesting! At least, I thought it was interesting how this the error is related back to the previous power's formula. Am I missing something obvious? Any input is greatly appreciated. (I'm not smart, so in the likely event I missed something obvious try not to be too harsh) Update: For the next power (4), I found the formula with trial and error: $$\left(\frac{k}{1}\cdot\frac{\left(k+1\right)}{2}\cdot\frac{\left(2k+1\right)}{3}\cdot\frac{\left(3k+1\right)}{4}\cdot\frac{\left(4k+1\right)}{5}\right)+\frac{1}{24}\left(k-1\right)k\left(k+1\right)+\frac{1}{12}\left(k-1\right)k\left(k+1\right)k$$ Any ideas on power (5) and so on? I'll continue to try and generalize it.","Pascal's triangle has a lot of interesting patterns in it; one of which is the triangular numbers and their extensions. Mathematically: $$\sum_{n=1}^k1=\frac{k}{1}$$ $$\sum_{n=1}^kn=\frac{k}{1}\cdot\frac{k+1}{2}$$ $$\sum_{n=1}^kn^2=\frac{k}{1}\cdot\frac{k+1}{2}\cdot\frac{2k+1}{3}$$ At first, we could guess that the next summation is: $$\sum_{n=1}^kn^3 ?=\frac{k}{1}\cdot\frac{k+1}{2}\cdot\frac{2k+1}{3}\cdot\frac{3k+1}{4}$$ Yet this is off. However, it is off geometrically . Notice: $$\left(\sum_{n=1}^kn^3\right)-\frac{k}{1}\cdot\frac{k+1}{2}\cdot\frac{2k+1}{3}\cdot\frac{3k+1}{4}=error$$ $$k=1, r=0$$ $$k=2, r=0.25$$ $$k=3, r=1$$ $$k=4, r=2.5$$ $$k=5, r=5$$ $$k=6, r=8.75$$ ... Consider the ratios of the errors: $$er(k)=\frac{r(k+1)}{r(k)}$$ $$k=1, r=udf$$ $$k=2, r=4$$ $$k=3, r=2.5$$ $$k=4, r=2$$ $$k=5, r=1.75$$ $$k=6, r=1.6$$ Then, rewriting the error as a function of n starting at k = 5: $$1.75=2.5-\frac{1.5}{2}$$ $$1.6=2.5-\frac{1.5}{2}-\frac{1.5}{10}$$ $$1.5=2.5-\frac{1.5}{2}-\frac{1.5}{10}-\frac{1.5}{15}$$ $$1.42857=2.5-\frac{1.5}{2}-\frac{1.5}{10}-\frac{1.5}{15}-\frac{1.5}{21}$$ The denominators in the series are from pascals triangle: (3rd columns, or dependent again on the triangular numbers) Then the total formula for equating the two is: $$\left(\frac{k}{1}\cdot\frac{\left(k+1\right)}{2}\cdot\frac{\left(2k+1\right)}{3}\cdot\frac{\left(3k+1\right)}{4}\right)-\left(\sum_{n=1}^kn^3\right)+\frac{1}{24}\left(k-1\right)k\left(k+1\right)=0$$ Super interesting! At least, I thought it was interesting how this the error is related back to the previous power's formula. Am I missing something obvious? Any input is greatly appreciated. (I'm not smart, so in the likely event I missed something obvious try not to be too harsh) Update: For the next power (4), I found the formula with trial and error: $$\left(\frac{k}{1}\cdot\frac{\left(k+1\right)}{2}\cdot\frac{\left(2k+1\right)}{3}\cdot\frac{\left(3k+1\right)}{4}\cdot\frac{\left(4k+1\right)}{5}\right)+\frac{1}{24}\left(k-1\right)k\left(k+1\right)+\frac{1}{12}\left(k-1\right)k\left(k+1\right)k$$ Any ideas on power (5) and so on? I'll continue to try and generalize it.",,"['calculus', 'algebra-precalculus', 'summation', 'binomial-coefficients']"
48,Calculate $\int_0^\infty {\frac{x}{{\left( {x + 1} \right)\sqrt {4{x^4} + 8{x^3} + 12{x^2} + 8x + 1} }}dx}$,Calculate,\int_0^\infty {\frac{x}{{\left( {x + 1} \right)\sqrt {4{x^4} + 8{x^3} + 12{x^2} + 8x + 1} }}dx},"Prove $$I=\int_0^\infty  {\frac{x}{{\left( {x + 1} \right)\sqrt {4{x^4} + 8{x^3} + 12{x^2} + 8x + 1} }}dx}  = \frac{{\ln 3}}{2} - \frac{{\ln 2}}{3}.$$ First note that $$4{x^4} + 8{x^3} + 12{x^2} + 8x + 1 = 4{\left( {{x^2} + x + 1} \right)^2} - 3,$$ we let $${x^2} + x + 1 = \frac{{\sqrt 3 }}{{2\cos \theta }} \Rightarrow x = \sqrt { - \frac{3}{4} + \frac{{\sqrt 3 }}{{2\cos \theta }}}  - \frac{1}{2},$$ then $$I=\frac{1}{2}\int_{\frac{\pi }{6}}^{\frac{\pi }{2}} {\frac{{\left( {\sqrt {2\sqrt 3 \sec \theta  - 3}  - 1} \right)\sec \theta }}{{\left( {\sqrt {2\sqrt 3 \sec \theta  - 3}  + 1} \right)\sqrt {2\sqrt 3 \sec \theta  - 3} }}d\theta } .$$ we have \begin{align*} &\frac{{\left( {\sqrt {2\sqrt 3 \sec \theta  - 3}  - 1} \right)\sec \theta }}{{\left( {\sqrt {2\sqrt 3 \sec \theta  - 3}  + 1} \right)\sqrt {2\sqrt 3 \sec \theta  - 3} }} = \frac{{{{\left( {\sqrt {2\sqrt 3 \sec \theta  - 3}  - 1} \right)}^2}\sec \theta }}{{\left( {2\sqrt 3 \sec \theta  - 4} \right)\sqrt {2\sqrt 3 \sec \theta  - 3} }}\\  =& \frac{{\left( {2\sqrt 3 \sec \theta  - 2 - 2\sqrt {2\sqrt 3 \sec \theta  - 3} } \right)\sec \theta }}{{\left( {2\sqrt 3 \sec \theta  - 4} \right)\sqrt {2\sqrt 3 \sec \theta  - 3} }} = \frac{{\left( {\sqrt 3 \sec \theta  - 1 - \sqrt {2\sqrt 3 \sec \theta  - 3} } \right)\sec \theta }}{{\left( {\sqrt 3 \sec \theta  - 2} \right)\sqrt {2\sqrt 3 \sec \theta  - 3} }}\\  = &\frac{{\left( {\sqrt 3 \sec \theta  - 1} \right)\sec \theta }}{{\left( {\sqrt 3 \sec \theta  - 2} \right)\sqrt {2\sqrt 3 \sec \theta  - 3} }} - \frac{{\sec \theta }}{{\sqrt 3 \sec \theta  - 2}}. \end{align*} and $$\int {\frac{{\sec \theta }}{{\sqrt 3 \sec \theta  - 2}}d\theta }  = \ln \frac{{\left( {2 + \sqrt 3 } \right)\tan \frac{\theta }{2} - 1}}{{\left( {2 + \sqrt 3 } \right)\tan \frac{\theta }{2} + 1}}+ C.$$ while \begin{align*}&\int {\frac{{\left( {\sqrt 3 \sec \theta  - 1} \right)\sec \theta }}{{\left( {\sqrt 3 \sec \theta  - 2} \right)\sqrt {2\sqrt 3 \sec \theta  - 3} }}d\theta }  = \int {\frac{{\sqrt 3  - \cos \theta }}{{\left( {\sqrt 3  - 2\cos \theta } \right)\sqrt {2\sqrt 3 \cos \theta  - 3{{\left( {\cos \theta } \right)}^2}} }}d\theta } \\  = &\frac{1}{2}\int {\frac{1}{{\sqrt {2\sqrt 3 \cos \theta  - 3{{\left( {\cos \theta } \right)}^2}} }}d\theta }  + \frac{{\sqrt 3 }}{2}\int {\frac{1}{{\left( {\sqrt 3  - 2\cos \theta } \right)\sqrt {2\sqrt 3 \cos \theta  - 3{{\left( {\cos \theta } \right)}^2}} }}d\theta } . \end{align*} But how can we continue? It is related to elliptic integral.","Prove $$I=\int_0^\infty  {\frac{x}{{\left( {x + 1} \right)\sqrt {4{x^4} + 8{x^3} + 12{x^2} + 8x + 1} }}dx}  = \frac{{\ln 3}}{2} - \frac{{\ln 2}}{3}.$$ First note that $$4{x^4} + 8{x^3} + 12{x^2} + 8x + 1 = 4{\left( {{x^2} + x + 1} \right)^2} - 3,$$ we let $${x^2} + x + 1 = \frac{{\sqrt 3 }}{{2\cos \theta }} \Rightarrow x = \sqrt { - \frac{3}{4} + \frac{{\sqrt 3 }}{{2\cos \theta }}}  - \frac{1}{2},$$ then $$I=\frac{1}{2}\int_{\frac{\pi }{6}}^{\frac{\pi }{2}} {\frac{{\left( {\sqrt {2\sqrt 3 \sec \theta  - 3}  - 1} \right)\sec \theta }}{{\left( {\sqrt {2\sqrt 3 \sec \theta  - 3}  + 1} \right)\sqrt {2\sqrt 3 \sec \theta  - 3} }}d\theta } .$$ we have \begin{align*} &\frac{{\left( {\sqrt {2\sqrt 3 \sec \theta  - 3}  - 1} \right)\sec \theta }}{{\left( {\sqrt {2\sqrt 3 \sec \theta  - 3}  + 1} \right)\sqrt {2\sqrt 3 \sec \theta  - 3} }} = \frac{{{{\left( {\sqrt {2\sqrt 3 \sec \theta  - 3}  - 1} \right)}^2}\sec \theta }}{{\left( {2\sqrt 3 \sec \theta  - 4} \right)\sqrt {2\sqrt 3 \sec \theta  - 3} }}\\  =& \frac{{\left( {2\sqrt 3 \sec \theta  - 2 - 2\sqrt {2\sqrt 3 \sec \theta  - 3} } \right)\sec \theta }}{{\left( {2\sqrt 3 \sec \theta  - 4} \right)\sqrt {2\sqrt 3 \sec \theta  - 3} }} = \frac{{\left( {\sqrt 3 \sec \theta  - 1 - \sqrt {2\sqrt 3 \sec \theta  - 3} } \right)\sec \theta }}{{\left( {\sqrt 3 \sec \theta  - 2} \right)\sqrt {2\sqrt 3 \sec \theta  - 3} }}\\  = &\frac{{\left( {\sqrt 3 \sec \theta  - 1} \right)\sec \theta }}{{\left( {\sqrt 3 \sec \theta  - 2} \right)\sqrt {2\sqrt 3 \sec \theta  - 3} }} - \frac{{\sec \theta }}{{\sqrt 3 \sec \theta  - 2}}. \end{align*} and $$\int {\frac{{\sec \theta }}{{\sqrt 3 \sec \theta  - 2}}d\theta }  = \ln \frac{{\left( {2 + \sqrt 3 } \right)\tan \frac{\theta }{2} - 1}}{{\left( {2 + \sqrt 3 } \right)\tan \frac{\theta }{2} + 1}}+ C.$$ while \begin{align*}&\int {\frac{{\left( {\sqrt 3 \sec \theta  - 1} \right)\sec \theta }}{{\left( {\sqrt 3 \sec \theta  - 2} \right)\sqrt {2\sqrt 3 \sec \theta  - 3} }}d\theta }  = \int {\frac{{\sqrt 3  - \cos \theta }}{{\left( {\sqrt 3  - 2\cos \theta } \right)\sqrt {2\sqrt 3 \cos \theta  - 3{{\left( {\cos \theta } \right)}^2}} }}d\theta } \\  = &\frac{1}{2}\int {\frac{1}{{\sqrt {2\sqrt 3 \cos \theta  - 3{{\left( {\cos \theta } \right)}^2}} }}d\theta }  + \frac{{\sqrt 3 }}{2}\int {\frac{1}{{\left( {\sqrt 3  - 2\cos \theta } \right)\sqrt {2\sqrt 3 \cos \theta  - 3{{\left( {\cos \theta } \right)}^2}} }}d\theta } . \end{align*} But how can we continue? It is related to elliptic integral.",,"['calculus', 'integration', 'complex-analysis', 'analysis']"
49,Continued fraction estimation of error in Leibniz series for $\pi$.,Continued fraction estimation of error in Leibniz series for .,\pi,"The following arctan formula for $\pi$ is quite well known $$\frac{\pi}{4} = 1 - \frac{1}{3} + \frac{1}{5} - \frac{1}{7} + \cdots\tag{1}$$ and bears the name of Madhava-Gregory-Leibniz series after its discoverers. The formula has an easy proof via integration. While reading a paper by Ranjan Roy titled ""The Discovery of the Series Formula for $\pi$ by Leibniz, Gregory and Nilakantha"" I was surprised to find that error in approximation via $n$ terms of series $(1)$ can be expressed in terms of a continued fraction studied by both Rogers and Ramanujan . Let $$f(x) = \cfrac{1}{x +\cfrac{1^{2}}{x +\cfrac{2^{2}}{x +\cfrac{3^{2}}{x + \cdots}}}}\tag{2}$$ for $x > 0$ and let $n$ be a positive integer. Let us define $S_{n}$ by $$S_{n} = \sum_{i = 1}^{n}(-1)^{i - 1}\cdot\frac{1}{2i - 1}$$ so that $S_{n}$ is the $n^{\text{th}}$ partial sum of series $(1)$. Then we have the formula $$\frac{\pi}{4} = S_{n} + (-1)^{n}\cdot\frac{f(2n)}{2}\tag{3}$$ The formula above is so marvelous that first 2-3 convergents of the continued fraction $(2)$ are sufficient to give a very good approximation of $\pi$ for small values of $n$. For example if $n = 4$ so that $$S_{4} = 1 - \frac{1}{3} + \frac{1}{5} - \frac{1}{7} = \frac{2}{3} + \frac{2}{35} = \frac{76}{105}$$ and taking 3rd convergent of $f(8)$ we see that $$f(8) \approx \cfrac{1}{8 +\cfrac{1^{2}}{8 +\cfrac{2^{2}}{8}}} = \frac{17}{138}$$ and hence $$\frac{\pi}{4} = S_{4} + \frac{f(8)}{2} \approx \frac{76}{105} + \frac{17}{276} = 0.78540372670807$$ which is greater than $\pi/4$ by approximately $5.5 \times 10^{-6}$ so that the approximation is really great. I would like to know the proof of identity $(3)$. Any reference for the proof would also be helpful. Update : Sorry people! I found the answer myself when I used the information in related link about the continued fraction $f(x)$. See my answer below. On the other hand I would be more than happy if we can obtain a proof of $(3)$ without using Rogers proof mentioned in my answer. Perhaps there was some simpler proof available with Nilkantha.","The following arctan formula for $\pi$ is quite well known $$\frac{\pi}{4} = 1 - \frac{1}{3} + \frac{1}{5} - \frac{1}{7} + \cdots\tag{1}$$ and bears the name of Madhava-Gregory-Leibniz series after its discoverers. The formula has an easy proof via integration. While reading a paper by Ranjan Roy titled ""The Discovery of the Series Formula for $\pi$ by Leibniz, Gregory and Nilakantha"" I was surprised to find that error in approximation via $n$ terms of series $(1)$ can be expressed in terms of a continued fraction studied by both Rogers and Ramanujan . Let $$f(x) = \cfrac{1}{x +\cfrac{1^{2}}{x +\cfrac{2^{2}}{x +\cfrac{3^{2}}{x + \cdots}}}}\tag{2}$$ for $x > 0$ and let $n$ be a positive integer. Let us define $S_{n}$ by $$S_{n} = \sum_{i = 1}^{n}(-1)^{i - 1}\cdot\frac{1}{2i - 1}$$ so that $S_{n}$ is the $n^{\text{th}}$ partial sum of series $(1)$. Then we have the formula $$\frac{\pi}{4} = S_{n} + (-1)^{n}\cdot\frac{f(2n)}{2}\tag{3}$$ The formula above is so marvelous that first 2-3 convergents of the continued fraction $(2)$ are sufficient to give a very good approximation of $\pi$ for small values of $n$. For example if $n = 4$ so that $$S_{4} = 1 - \frac{1}{3} + \frac{1}{5} - \frac{1}{7} = \frac{2}{3} + \frac{2}{35} = \frac{76}{105}$$ and taking 3rd convergent of $f(8)$ we see that $$f(8) \approx \cfrac{1}{8 +\cfrac{1^{2}}{8 +\cfrac{2^{2}}{8}}} = \frac{17}{138}$$ and hence $$\frac{\pi}{4} = S_{4} + \frac{f(8)}{2} \approx \frac{76}{105} + \frac{17}{276} = 0.78540372670807$$ which is greater than $\pi/4$ by approximately $5.5 \times 10^{-6}$ so that the approximation is really great. I would like to know the proof of identity $(3)$. Any reference for the proof would also be helpful. Update : Sorry people! I found the answer myself when I used the information in related link about the continued fraction $f(x)$. See my answer below. On the other hand I would be more than happy if we can obtain a proof of $(3)$ without using Rogers proof mentioned in my answer. Perhaps there was some simpler proof available with Nilkantha.",,"['calculus', 'reference-request', 'pi', 'continued-fractions']"
50,Why solutions of $y''+(w^2+b(t))y=0$ behave like solutions of $y''+w^2y=0$ at infinity,Why solutions of  behave like solutions of  at infinity,y''+(w^2+b(t))y=0 y''+w^2y=0,"Assume $w>0$ and $b(t)$ be continuous on $[0,+\infty)$ and $\int_0^\infty |b(t)| dt <\infty$ show that $y''+(w^2+b(t))y=0$ has solution $\phi(t)$ such that $$\lim_{t\to\infty} [(\phi(t)-\sin(wt))^2+(\phi'(t)-w\cos(wt))^2]=0$$ thanks","Assume $w>0$ and $b(t)$ be continuous on $[0,+\infty)$ and $\int_0^\infty |b(t)| dt <\infty$ show that $y''+(w^2+b(t))y=0$ has solution $\phi(t)$ such that $$\lim_{t\to\infty} [(\phi(t)-\sin(wt))^2+(\phi'(t)-w\cos(wt))^2]=0$$ thanks",,"['calculus', 'analysis', 'ordinary-differential-equations', 'dynamical-systems']"
51,how to calculuate $\int_0^ \pi \sqrt{1+x^2 \sin^2x}dx$,how to calculuate,\int_0^ \pi \sqrt{1+x^2 \sin^2x}dx,"I was finding arc length of $y=\sin x - x \cos x$ $(0 \leq x\leq \pi)$ and I found I've to solve $$\int_0^\pi \sqrt{1 + x^2\sin^2{x}}\, dx $$ but I have no idea about this. I tried using $\sin^2x + \cos^2x=1$, $\sin^2x =(1-\cos 2x)/2$  but failed.","I was finding arc length of $y=\sin x - x \cos x$ $(0 \leq x\leq \pi)$ and I found I've to solve $$\int_0^\pi \sqrt{1 + x^2\sin^2{x}}\, dx $$ but I have no idea about this. I tried using $\sin^2x + \cos^2x=1$, $\sin^2x =(1-\cos 2x)/2$  but failed.",,"['calculus', 'integration', 'definite-integrals', 'closed-form']"
52,"Examples of non-Riemann integrable functions that appear ""in nature""?","Examples of non-Riemann integrable functions that appear ""in nature""?",,"I am teaching an honours calculus class, and am looking for examples on non-integrable functions that occur somewhere real in mathematics. (The standard example of 1 on $\mathbb{Q}$ and 0 elsewhere always felt slightly manufactured to me. If you can give a context for it, I would be happy about that, too).","I am teaching an honours calculus class, and am looking for examples on non-integrable functions that occur somewhere real in mathematics. (The standard example of 1 on $\mathbb{Q}$ and 0 elsewhere always felt slightly manufactured to me. If you can give a context for it, I would be happy about that, too).",,"['calculus', 'soft-question', 'examples-counterexamples']"
53,Show this $\int_0^\infty \frac{t\ln(2\sinh t)}{\left(3t^2+\ln^2(2\sinh t)\right)^2}~dt=0$,Show this,\int_0^\infty \frac{t\ln(2\sinh t)}{\left(3t^2+\ln^2(2\sinh t)\right)^2}~dt=0,"While evaluating the integral $$ I_1=\int_{0}^\infty\frac{\sin\pi x~dx}{x\prod\limits_{k=1}^\infty\left(1-\frac{x^3}{k^3}\right)},\tag{1} $$ I came to this integral of elementary function $$ I_2=\int_0^\infty \frac{dt}{\left(i t\sqrt{3}+\ln(2\sinh t)\right)^2}.\tag{2} $$ In fact $I_2$ is real and $$ I_1=-2\pi I_2. $$ These formulas imply the closed form $$ \int_{0}^{\infty}\frac{t\ln\left(\,2\sinh\left(\,t\,\right)\,\right)}{\left[\,3t^{2} + \ln^{2}\left(\,2\sinh\left(\,t\,\right)\,\right)\right]^{\,2}}\,{d}t = 0,\tag{3} $$ or alternatively $$ \text{Im}\int_0^\infty \frac{dt}{\left(i t\sqrt{3}+\ln(2\sinh t)\right)^2}=0. $$ Brief outline of proof is as follows. Write the infinite product in terms of Gamma functions, apply reflection formula for Gamma function to get rid of $\sin\pi x$, then use integral representation for Beta function and change the order of integration. Then one can integrate over $x$ to obtain the desired formula. It seems that this should have a simple proof, but I don't see it. Q: Can anybody provide a direct proof ?. Such a direct proof may shed light on possible routes to calculation or simplification of $(2)$. Here is a numerical demonstration using Mathematica that the integral under consideration is $0$ up to at least $100$ digits: The integrand for $t>w$ has been replaced by $\frac{1}{16t^2}$, resulting in the term $\frac{1}{16w}$.","While evaluating the integral $$ I_1=\int_{0}^\infty\frac{\sin\pi x~dx}{x\prod\limits_{k=1}^\infty\left(1-\frac{x^3}{k^3}\right)},\tag{1} $$ I came to this integral of elementary function $$ I_2=\int_0^\infty \frac{dt}{\left(i t\sqrt{3}+\ln(2\sinh t)\right)^2}.\tag{2} $$ In fact $I_2$ is real and $$ I_1=-2\pi I_2. $$ These formulas imply the closed form $$ \int_{0}^{\infty}\frac{t\ln\left(\,2\sinh\left(\,t\,\right)\,\right)}{\left[\,3t^{2} + \ln^{2}\left(\,2\sinh\left(\,t\,\right)\,\right)\right]^{\,2}}\,{d}t = 0,\tag{3} $$ or alternatively $$ \text{Im}\int_0^\infty \frac{dt}{\left(i t\sqrt{3}+\ln(2\sinh t)\right)^2}=0. $$ Brief outline of proof is as follows. Write the infinite product in terms of Gamma functions, apply reflection formula for Gamma function to get rid of $\sin\pi x$, then use integral representation for Beta function and change the order of integration. Then one can integrate over $x$ to obtain the desired formula. It seems that this should have a simple proof, but I don't see it. Q: Can anybody provide a direct proof ?. Such a direct proof may shed light on possible routes to calculation or simplification of $(2)$. Here is a numerical demonstration using Mathematica that the integral under consideration is $0$ up to at least $100$ digits: The integrand for $t>w$ has been replaced by $\frac{1}{16t^2}$, resulting in the term $\frac{1}{16w}$.",,"['calculus', 'integration', 'definite-integrals', 'contour-integration', 'closed-form']"
54,Important identities that can be obtained by manipulating the function $\frac{x}{e^x-1} = \frac{B_0}{0!} + \frac{B_1}{1!}x + \frac{B_2}{2!}x^2 + ...$?,Important identities that can be obtained by manipulating the function ?,\frac{x}{e^x-1} = \frac{B_0}{0!} + \frac{B_1}{1!}x + \frac{B_2}{2!}x^2 + ...,"Note that $B_n$ denotes the nth Bernoulli number. Also note that $\frac{x}{2}\frac{e^x+1}{e^x-1} = \frac{x}{e^x-1} + \frac{x}{2} = 1 + \frac{|B_2|}{2!}x^2 - \frac{|B_4|}{4!}x^4+ \frac{|B_6|}{6!}x^6 - \cdots$ since $B_1 = -\frac{1}{2}$, the odd Bernoulli numbers are equal to zero, and $|B_{2n}| = (-1)^{n+1}B_{2n}$. Two of the most interesting identities that I've read about thus far in my studies have proofs which rely upon the generating function $\frac{x}{e^x-1} = \frac{B_0}{0!} + \frac{B_1}{1!}x + \frac{B_2}{2!}x^2 + \cdots$ in an essential way. The first is (Jacobi's?) proof of Faulhaber's formula, and the second is (Euler's?) proof that connects the Bernoulli numbers to values of the Riemann Zeta function for  positive even integers. Faulhaber's Formula:$$1^c+2^c+\cdots+n^c = \frac{1}{c+1}\left(\binom{c+1}{0}B_0n^{c+1}-\binom{c+1}{1}B_1n^c + \cdots +(-1)^c\binom{c+1}{c}B_cn\right)$$ Riemann Zeta Function Identity: $$\frac{1}{1^{2n}} + \frac{1}{2^{2n}}+\frac{1}{3^{2n}}+\cdots = (-1)^{n+1}\frac{{(2\pi)}^{2n}B_{2n}}{2(2n)!}$$ Below I give the manipulations which give rise to these identities, you'll notice that the generating function $\frac{x}{e^x-1} = \frac{B_0}{0!} + \frac{B_1}{1!}x + \frac{B_2}{2!}x^2 + \cdots$ is the key piece of equipment that allows the connection with the Bernoulli numbers to be established in both cases. Given the importance of these two identities and their connection with this generating function I'm left to wonder if there are other important identities that require the manipulation of this generating function. Perhaps someone can provide other examples. Edit-08/09/16 I feel I should clarify. I realize that this function can be manipulated to produce many identities, but Faulhaber's Formula and this Zeta function identity seem somehow special, especially in light of how beautiful their manipulations are. I'm looking for identities that 1) Are considered to be rather important. 2) Can be established be means of the generating function $\frac{x}{e^x-1} = \frac{B_0}{0!} + \frac{B_1}{1!}x + \frac{B_2}{2!}x^2 + \cdots$. 3) Perhaps have some special quality about their manipulations. Proof of Faulhaber's formula. This proof can be found in sigma notation here https://en.wikipedia.org/wiki/Faulhaber%27s_formula#Proof $$\{1^0+2^0+\cdots+n^0\} + \{1^1+2^1+\cdots+n^1\}\frac{x}{1!} + \{1^2+2^2+\cdots+n^2\}\frac{x^2}{2!} +\cdots$$ $$= \{{1^0}+\frac{1^1x^1}{1!} + \frac{1^2x^2}{2!} + \cdots\} + \{{2^0}+\frac{2^1x^1}{1!} + \frac{2^2x^2}{2!} + \cdots\} +\cdots+ \{{n^0}+\frac{n^1x^1}{1!} + \frac{n^2x^2}{2!} + \cdots\}$$ $$=e^x + e^{2x} + \cdots+e^{nx} = e^x\frac{1-e^{nx}}{1-e^x} = \frac{1-e^{nx}}{e^{-x}-1} = \frac{1}{x}\{\frac{-x}{e^{-x}-1}\}\{e^{nx}-1 \} = \frac{1}{x}\{\frac{B_0}{0!}-\frac{B_1}{1!}x+\frac{B_2}{2!}x^2 - \cdots \}\{{e^{nx}-1}\} $$$$= \frac{B_0}{0!}\frac{1}{x}\{{e^{nx}-1}\}-\frac{B_1}{1!}\{{e^{nx}-1}\}+\frac{B_2}{2!}x^1\{{e^{nx}-1}\} - \cdots  $$ $$\begin{align} & =\frac{B_0}{0!}\{\frac{n^1}{1!}x^0 + \frac{n^2}{2!}x^1 + \frac{n^3}{3!}x^2 + \frac{n^4}{4!}x^3+\cdots\} \\ & -\frac{B_1}{1!}\{\frac{n^1}{1!}x^1 + \frac{n^2}{2!}x^2 + \frac{n^3}{3!}x^3+\frac{n^4}{4!}x^4+ \cdots\} \\ & +\frac{B_2}{2!}\{\frac{n^1}{1!}x^2 + \frac{n^2}{2!}x^3 + \frac{n^3}{3!}x^4 +\frac{n^4}{4!}x^5+ \cdots\}\\ &-\frac{B_3}{3!}\{\frac{n^1}{1!}x^3 + \frac{n^2}{2!}x^4 + \frac{n^3}{3!}x^5 +\frac{n^4}{4!}x^6+ \cdots\}+\cdots \end{align}$$  $$=\frac{B_0}{0!}\frac{n}{1!} + \{\frac{B_0}{0!}\frac{n^2}{2!} - \frac{B_1}{1!}\frac{n}{1!}\}x + \{\frac{B_0}{0!}\frac{n^3}{3!} - \frac{B_1}{1!}\frac{n^2}{2!} + \frac{B_2}{2!}\frac{n}{1!}\}x^2 + \{\frac{B_0}{0!}\frac{n^4}{4!} - \frac{B_1}{1!}\frac{n^3}{3!} + \frac{B_2}{2!}\frac{n^2}{2!} - \frac{B_3}{3!}\frac{n}{1!}\}x^3+ \cdots$$ Equating coefficients, we see that $$\{1^c+2^c+\cdots+n^c\}\frac{1}{c!} = \frac{B_0}{0!}\frac{n^{c+1}}{(c+1)!}-\frac{B_1}{1!}\frac{n^c}{c!} + \cdots+ (-1)^c\frac{B_c}{c!}\frac{n}{1!}$$Thus $$1^c+2^c+\cdots+n^c = \frac{c!}{0!(c+1)!}B_0n^{c+1}-\frac{c!}{1!c!}B_1n^c+\cdots+(-1)^c\frac{c!}{c!1!}B_cn = \frac{1}{c+1}\left(\binom{c+1}{0}B_0n^{c+1}-\binom{c+1}{1}B_1n^c + \cdots +(-1)^c\binom{c+1}{c}B_cn\right)$$ Proof of Zeta Identity for positive even integers: I found this proof on pages 276-277 in Carr's Synopsis found here https://archive.org/details/synopsisofelemen00carrrich . I'm kind of just assuming that this proof traces back to Euler. Carr's Synopsis is rough in places but I would highly recommend reading certain pages that are intimately connected to Ramanujan's intuition. (I first heard about Carr's Synopsis in this video https://www.youtube.com/watch?v=QUnmAhXe9bg ) [ 2 It mistakenly refers to 1540 instead of 1541 lol.  Note that Carr defines the Bernoulli numbers according to their absolute values. Here is what Carr is saying: $$sin(x) = x\{1-{\left(\frac{x}{\pi}\right)}^{2}\}\{1-{\left(\frac{x}{2\pi}\right)}^{2}\}\{1-{\left(\frac{x}{3\pi}\right)}^{2}\}\cdots$$ Thus $$log\{sin(x)\} = log(x) + log\{1-{\frac{x^2}{\pi^2}}\}+log\{1-{\frac{x^2}{(2\pi)^2}}\}+log\{1-{\frac{x^2}{(3\pi)^2}}\}+\cdots$$ Thus $$\frac{cos(x)}{sin(x)} = \frac{1}{x} + \frac{-2\frac{x}{\pi^2}}{1-\frac{x^2}{\pi^2}} + \frac{-2\frac{x}{(2\pi)^2}}{1-\frac{x^2}{(2\pi)^2}}+ \frac{-2\frac{x}{(3\pi)^2}}{1-\frac{x^2}{(3\pi)^2}}+\cdots$$ Thus $$xcot(x) = 1-2\frac{x^2}{\pi^2}\{1+\frac{x^2}{\pi^2}+\frac{x^4}{\pi^4}+\cdots\}-2\frac{x^2}{(2\pi)^2}\{1+\frac{x^2}{(2\pi)^2}+\frac{x^4}{(2\pi)^4}+\cdots\}-2\frac{x^2}{(3\pi)^2}\{1+\frac{x^2}{(3\pi)^2}+\frac{x^4}{(3\pi)^4}+\cdots\}-\cdots$$ $$= 1-\frac{2x^2}{\pi^2}\{\frac{1}{1^2}+\frac{1}{2^2}+\frac{1}{3^2}+\cdots\}-\frac{2x^4}{\pi^4}\{\frac{1}{1^4}+\frac{1}{2^4}+\frac{1}{3^4}+\cdots\}-\frac{2x^6}{\pi^6}\{\frac{1}{1^6}+\frac{1}{2^6}+\frac{1}{3^6}+\cdots\}-\cdots$$ Now, if $y = 2ix$ $$xcot(x) = ix\frac{2cos(x)}{2isin(x)} = ix\frac{\{cos(x)+isin(x)\} + \{cos(x)-isin(x)\}}{\{cos(x)+isin(x)\} - \{cos(x)-isin(x)\}} = ix\frac{e^{ix}+e^{-ix}}{e^{ix}-e^{-ix}} = ix\frac{e^{2ix}+1}{e^{2ix}-1} = \frac{y}{2}\frac{e^y+1}{e^y-1} = \frac{y}{e^{y}-1}+\frac{y}{2} $$$$= 1+\frac{|B_2|}{2!}y^2-\frac{|B_4|}{4!}y^4+\frac{|B_6|}{6!}y^6-\cdots $$$$= 1+\frac{|B_2|}{2!}(2ix)^2-\frac{|B_4|}{4!}(2ix)^4+\frac{|B_6|}{6!}(2ix)^6-\cdots$$$$= 1-\frac{|B_2|}{2!}(2x)^2-\frac{|B_4|}{4!}(2x)^4-\frac{|B_6|}{6!}(2x)^6-\cdots$$ Thus by combining both expressions for $xcot(x)$ we obtain $$1-\frac{2x^2}{\pi^2}\{\frac{1}{1^2}+\frac{1}{2^2}+\frac{1}{3^2}+\cdots\}-\frac{2x^4}{\pi^4}\{\frac{1}{1^4}+\frac{1}{2^4}+\frac{1}{3^4}+\cdots\}-\frac{2x^6}{\pi^6}\{\frac{1}{1^6}+\frac{1}{2^6}+\frac{1}{3^6}+\cdots\}-\cdots = 1-\frac{|B_2|}{2!}(2x)^2-\frac{|B_4|}{4!}(2x)^4-\frac{|B_6|}{6!}(2x)^6-\cdots$$ Equating coefficients, we see that $$\frac{2}{\pi^{2n}}\{\frac{1}{1^{2n}} + \frac{1}{2^{2n}}+\frac{1}{3^{2n}}+\cdots\} = \frac{|B_{2n}|}{(2n)!}2^{2n} $$ Therefore, $$\frac{1}{1^{2n}} + \frac{1}{2^{2n}}+\frac{1}{3^{2n}}+\cdots= (-1)^{n+1} \frac{{(2\pi)}^{2n} B_{2n}}{2(2n)!}$$ Edit 08/09/16 I removed my second question since it seemed to cause confusion. I think that my thoughts in this area need to be focused a bit more.","Note that $B_n$ denotes the nth Bernoulli number. Also note that $\frac{x}{2}\frac{e^x+1}{e^x-1} = \frac{x}{e^x-1} + \frac{x}{2} = 1 + \frac{|B_2|}{2!}x^2 - \frac{|B_4|}{4!}x^4+ \frac{|B_6|}{6!}x^6 - \cdots$ since $B_1 = -\frac{1}{2}$, the odd Bernoulli numbers are equal to zero, and $|B_{2n}| = (-1)^{n+1}B_{2n}$. Two of the most interesting identities that I've read about thus far in my studies have proofs which rely upon the generating function $\frac{x}{e^x-1} = \frac{B_0}{0!} + \frac{B_1}{1!}x + \frac{B_2}{2!}x^2 + \cdots$ in an essential way. The first is (Jacobi's?) proof of Faulhaber's formula, and the second is (Euler's?) proof that connects the Bernoulli numbers to values of the Riemann Zeta function for  positive even integers. Faulhaber's Formula:$$1^c+2^c+\cdots+n^c = \frac{1}{c+1}\left(\binom{c+1}{0}B_0n^{c+1}-\binom{c+1}{1}B_1n^c + \cdots +(-1)^c\binom{c+1}{c}B_cn\right)$$ Riemann Zeta Function Identity: $$\frac{1}{1^{2n}} + \frac{1}{2^{2n}}+\frac{1}{3^{2n}}+\cdots = (-1)^{n+1}\frac{{(2\pi)}^{2n}B_{2n}}{2(2n)!}$$ Below I give the manipulations which give rise to these identities, you'll notice that the generating function $\frac{x}{e^x-1} = \frac{B_0}{0!} + \frac{B_1}{1!}x + \frac{B_2}{2!}x^2 + \cdots$ is the key piece of equipment that allows the connection with the Bernoulli numbers to be established in both cases. Given the importance of these two identities and their connection with this generating function I'm left to wonder if there are other important identities that require the manipulation of this generating function. Perhaps someone can provide other examples. Edit-08/09/16 I feel I should clarify. I realize that this function can be manipulated to produce many identities, but Faulhaber's Formula and this Zeta function identity seem somehow special, especially in light of how beautiful their manipulations are. I'm looking for identities that 1) Are considered to be rather important. 2) Can be established be means of the generating function $\frac{x}{e^x-1} = \frac{B_0}{0!} + \frac{B_1}{1!}x + \frac{B_2}{2!}x^2 + \cdots$. 3) Perhaps have some special quality about their manipulations. Proof of Faulhaber's formula. This proof can be found in sigma notation here https://en.wikipedia.org/wiki/Faulhaber%27s_formula#Proof $$\{1^0+2^0+\cdots+n^0\} + \{1^1+2^1+\cdots+n^1\}\frac{x}{1!} + \{1^2+2^2+\cdots+n^2\}\frac{x^2}{2!} +\cdots$$ $$= \{{1^0}+\frac{1^1x^1}{1!} + \frac{1^2x^2}{2!} + \cdots\} + \{{2^0}+\frac{2^1x^1}{1!} + \frac{2^2x^2}{2!} + \cdots\} +\cdots+ \{{n^0}+\frac{n^1x^1}{1!} + \frac{n^2x^2}{2!} + \cdots\}$$ $$=e^x + e^{2x} + \cdots+e^{nx} = e^x\frac{1-e^{nx}}{1-e^x} = \frac{1-e^{nx}}{e^{-x}-1} = \frac{1}{x}\{\frac{-x}{e^{-x}-1}\}\{e^{nx}-1 \} = \frac{1}{x}\{\frac{B_0}{0!}-\frac{B_1}{1!}x+\frac{B_2}{2!}x^2 - \cdots \}\{{e^{nx}-1}\} $$$$= \frac{B_0}{0!}\frac{1}{x}\{{e^{nx}-1}\}-\frac{B_1}{1!}\{{e^{nx}-1}\}+\frac{B_2}{2!}x^1\{{e^{nx}-1}\} - \cdots  $$ $$\begin{align} & =\frac{B_0}{0!}\{\frac{n^1}{1!}x^0 + \frac{n^2}{2!}x^1 + \frac{n^3}{3!}x^2 + \frac{n^4}{4!}x^3+\cdots\} \\ & -\frac{B_1}{1!}\{\frac{n^1}{1!}x^1 + \frac{n^2}{2!}x^2 + \frac{n^3}{3!}x^3+\frac{n^4}{4!}x^4+ \cdots\} \\ & +\frac{B_2}{2!}\{\frac{n^1}{1!}x^2 + \frac{n^2}{2!}x^3 + \frac{n^3}{3!}x^4 +\frac{n^4}{4!}x^5+ \cdots\}\\ &-\frac{B_3}{3!}\{\frac{n^1}{1!}x^3 + \frac{n^2}{2!}x^4 + \frac{n^3}{3!}x^5 +\frac{n^4}{4!}x^6+ \cdots\}+\cdots \end{align}$$  $$=\frac{B_0}{0!}\frac{n}{1!} + \{\frac{B_0}{0!}\frac{n^2}{2!} - \frac{B_1}{1!}\frac{n}{1!}\}x + \{\frac{B_0}{0!}\frac{n^3}{3!} - \frac{B_1}{1!}\frac{n^2}{2!} + \frac{B_2}{2!}\frac{n}{1!}\}x^2 + \{\frac{B_0}{0!}\frac{n^4}{4!} - \frac{B_1}{1!}\frac{n^3}{3!} + \frac{B_2}{2!}\frac{n^2}{2!} - \frac{B_3}{3!}\frac{n}{1!}\}x^3+ \cdots$$ Equating coefficients, we see that $$\{1^c+2^c+\cdots+n^c\}\frac{1}{c!} = \frac{B_0}{0!}\frac{n^{c+1}}{(c+1)!}-\frac{B_1}{1!}\frac{n^c}{c!} + \cdots+ (-1)^c\frac{B_c}{c!}\frac{n}{1!}$$Thus $$1^c+2^c+\cdots+n^c = \frac{c!}{0!(c+1)!}B_0n^{c+1}-\frac{c!}{1!c!}B_1n^c+\cdots+(-1)^c\frac{c!}{c!1!}B_cn = \frac{1}{c+1}\left(\binom{c+1}{0}B_0n^{c+1}-\binom{c+1}{1}B_1n^c + \cdots +(-1)^c\binom{c+1}{c}B_cn\right)$$ Proof of Zeta Identity for positive even integers: I found this proof on pages 276-277 in Carr's Synopsis found here https://archive.org/details/synopsisofelemen00carrrich . I'm kind of just assuming that this proof traces back to Euler. Carr's Synopsis is rough in places but I would highly recommend reading certain pages that are intimately connected to Ramanujan's intuition. (I first heard about Carr's Synopsis in this video https://www.youtube.com/watch?v=QUnmAhXe9bg ) [ 2 It mistakenly refers to 1540 instead of 1541 lol.  Note that Carr defines the Bernoulli numbers according to their absolute values. Here is what Carr is saying: $$sin(x) = x\{1-{\left(\frac{x}{\pi}\right)}^{2}\}\{1-{\left(\frac{x}{2\pi}\right)}^{2}\}\{1-{\left(\frac{x}{3\pi}\right)}^{2}\}\cdots$$ Thus $$log\{sin(x)\} = log(x) + log\{1-{\frac{x^2}{\pi^2}}\}+log\{1-{\frac{x^2}{(2\pi)^2}}\}+log\{1-{\frac{x^2}{(3\pi)^2}}\}+\cdots$$ Thus $$\frac{cos(x)}{sin(x)} = \frac{1}{x} + \frac{-2\frac{x}{\pi^2}}{1-\frac{x^2}{\pi^2}} + \frac{-2\frac{x}{(2\pi)^2}}{1-\frac{x^2}{(2\pi)^2}}+ \frac{-2\frac{x}{(3\pi)^2}}{1-\frac{x^2}{(3\pi)^2}}+\cdots$$ Thus $$xcot(x) = 1-2\frac{x^2}{\pi^2}\{1+\frac{x^2}{\pi^2}+\frac{x^4}{\pi^4}+\cdots\}-2\frac{x^2}{(2\pi)^2}\{1+\frac{x^2}{(2\pi)^2}+\frac{x^4}{(2\pi)^4}+\cdots\}-2\frac{x^2}{(3\pi)^2}\{1+\frac{x^2}{(3\pi)^2}+\frac{x^4}{(3\pi)^4}+\cdots\}-\cdots$$ $$= 1-\frac{2x^2}{\pi^2}\{\frac{1}{1^2}+\frac{1}{2^2}+\frac{1}{3^2}+\cdots\}-\frac{2x^4}{\pi^4}\{\frac{1}{1^4}+\frac{1}{2^4}+\frac{1}{3^4}+\cdots\}-\frac{2x^6}{\pi^6}\{\frac{1}{1^6}+\frac{1}{2^6}+\frac{1}{3^6}+\cdots\}-\cdots$$ Now, if $y = 2ix$ $$xcot(x) = ix\frac{2cos(x)}{2isin(x)} = ix\frac{\{cos(x)+isin(x)\} + \{cos(x)-isin(x)\}}{\{cos(x)+isin(x)\} - \{cos(x)-isin(x)\}} = ix\frac{e^{ix}+e^{-ix}}{e^{ix}-e^{-ix}} = ix\frac{e^{2ix}+1}{e^{2ix}-1} = \frac{y}{2}\frac{e^y+1}{e^y-1} = \frac{y}{e^{y}-1}+\frac{y}{2} $$$$= 1+\frac{|B_2|}{2!}y^2-\frac{|B_4|}{4!}y^4+\frac{|B_6|}{6!}y^6-\cdots $$$$= 1+\frac{|B_2|}{2!}(2ix)^2-\frac{|B_4|}{4!}(2ix)^4+\frac{|B_6|}{6!}(2ix)^6-\cdots$$$$= 1-\frac{|B_2|}{2!}(2x)^2-\frac{|B_4|}{4!}(2x)^4-\frac{|B_6|}{6!}(2x)^6-\cdots$$ Thus by combining both expressions for $xcot(x)$ we obtain $$1-\frac{2x^2}{\pi^2}\{\frac{1}{1^2}+\frac{1}{2^2}+\frac{1}{3^2}+\cdots\}-\frac{2x^4}{\pi^4}\{\frac{1}{1^4}+\frac{1}{2^4}+\frac{1}{3^4}+\cdots\}-\frac{2x^6}{\pi^6}\{\frac{1}{1^6}+\frac{1}{2^6}+\frac{1}{3^6}+\cdots\}-\cdots = 1-\frac{|B_2|}{2!}(2x)^2-\frac{|B_4|}{4!}(2x)^4-\frac{|B_6|}{6!}(2x)^6-\cdots$$ Equating coefficients, we see that $$\frac{2}{\pi^{2n}}\{\frac{1}{1^{2n}} + \frac{1}{2^{2n}}+\frac{1}{3^{2n}}+\cdots\} = \frac{|B_{2n}|}{(2n)!}2^{2n} $$ Therefore, $$\frac{1}{1^{2n}} + \frac{1}{2^{2n}}+\frac{1}{3^{2n}}+\cdots= (-1)^{n+1} \frac{{(2\pi)}^{2n} B_{2n}}{2(2n)!}$$ Edit 08/09/16 I removed my second question since it seemed to cause confusion. I think that my thoughts in this area need to be focused a bit more.",,"['calculus', 'complex-numbers', 'power-series', 'intuition', 'generating-functions']"
55,Show that $\frac{x}{3!}-\frac{x^3}{5!}+\frac{x^5}{7!}-\cdots\leq \frac{1}{\pi}$.,Show that .,\frac{x}{3!}-\frac{x^3}{5!}+\frac{x^5}{7!}-\cdots\leq \frac{1}{\pi},"My problem is to show that   $$\frac{x}{3!}-\frac{x^3}{5!}+\frac{x^5}{7!}-\cdots\leq \frac{1}{\pi}$$   for all $x\in\Bbb R$. I was thinking of first finding the max and then show that its less than $1/\pi$. But it is hard to find it. I get that the series is equal to $$f(x)=\frac{x-\sin x}{x^2}.$$ Then, $$f'(x)=\frac{1-\cos x}{x^2}-\frac{2(x-\sin x)}{x^3}=0$$ if and only if $$2\sin x=x(1+\cos x),$$ which I am unable to solve, appart from the obvious solutions $x=0$ and $x=\pi$. But if $x=\pi$ is the max, then we are done because $f(\pi)=1/\pi$.","My problem is to show that   $$\frac{x}{3!}-\frac{x^3}{5!}+\frac{x^5}{7!}-\cdots\leq \frac{1}{\pi}$$   for all $x\in\Bbb R$. I was thinking of first finding the max and then show that its less than $1/\pi$. But it is hard to find it. I get that the series is equal to $$f(x)=\frac{x-\sin x}{x^2}.$$ Then, $$f'(x)=\frac{1-\cos x}{x^2}-\frac{2(x-\sin x)}{x^3}=0$$ if and only if $$2\sin x=x(1+\cos x),$$ which I am unable to solve, appart from the obvious solutions $x=0$ and $x=\pi$. But if $x=\pi$ is the max, then we are done because $f(\pi)=1/\pi$.",,"['calculus', 'inequality', 'power-series']"
56,Limit of $\sin (a^{n}\theta\pi)$ as $ n \to \infty$ where $a$ is an integer greater than $2$,Limit of  as  where  is an integer greater than,\sin (a^{n}\theta\pi)  n \to \infty a 2,"In Hardy's Pure Mathematics, Hardy discusses the limit $$\lim_{n\to\infty}\sin (2^{n}\theta\pi)$$ and says that if this limit exists it must be zero and then $\theta$ must be a rational number whose denominator is a power of $2$. Then he asks the reader to consider the limit $$\lim_{n\to\infty}\sin (a^{n}\theta\pi)$$ where $a$ is an integer greater than $2$. It is also mentioned that when $a > 2$ then the limit can be non-zero and as an example Hardy states that if $a = 9$ and $\theta = 1/2$ then the above limit is $1$. In this case I reasoned (based on Hardy's technique for the case $a = 2$) that $\lim_{n\to\infty}\sin((9^{n}\pi)/2)$ will be $1$ only when we can write $$\frac{9^{n}\pi}{2} = 2b_{n}\pi + \frac{\pi}{2} + c_{n}$$ for all sufficiently large values of $n$, where $\{b_{n}\}$ is a sequence which takes only integer values and $\{c_{n}\}$ is a sequence which tends to zero as $n \to \infty$. Thus we get $\displaystyle \begin{aligned}9^{n}\pi &= 4b_{n}\pi + \pi + 2c_{n}\\ \Rightarrow (9^{n} - 1)\pi &= 4b_{n}\pi + 2c_{n}\end{aligned}$ Since $9^{n} - 1$ is divisible by $9 - 1 = 8$ and hence also by $4$, it follows that we can take $b_{n} = (9^{n} - 1)/4$ and $c_{n} = 0$. The same logic could be applied when $a$ is any integer of the form $a = 4m + 1$ and $\theta = 1/2$. But, and this is my question, how does one handle the case for general integer $a > 2$ and any real $\theta$ rational or irrational? EDIT: I should also provide more details so that readers get the full context. For the specific case when $a = 2$ Hardy reasons that if the limit exists and, say is equal to $l$, then we must have $|l| \leq 1$ so that we have a constant $\alpha = \sin^{-1} l$ lying in interval $[-\pi/2, \pi/2]$. Since the solution of $\sin x = \sin \alpha$ is given by $x = m\pi + (-1)^{m}\alpha$ for all integers $m$, we can see that the existence of limit $l$ entails that $$2^{n}\theta\pi = b_{n}\pi + (-1)^{b_{n}}\alpha + c_{n}$$ where $b_{n}$ takes integer values and $c_{n} \to 0$ as $n\to\infty$. We can now see that $$2^{n}\theta = b_{n} + (-1)^{b_{n}}\beta + d_{n}$$ where $\beta = \alpha/\pi$ so that $\beta \in [-1/2, 1/2]$ and $d_{n} = c_{n}/\pi \to 0$ as $n \to\infty$. Hardy somehow assumes that $b_{n}$ will always be even and then proceeds as follows: $\displaystyle 2^{n}\theta = b_{n} + \beta + d_{n}$ so that multiplication by $2$ gives $\displaystyle 2^{n + 1}\theta = 2b_{n} + 2\beta + 2d_{n}$ and also we have $\displaystyle 2^{n + 1}\theta = b_{n + 1} + \beta + d_{n + 1}$ and therefore we get $\displaystyle (b_{n + 1} - 2b_{n}) - \beta + (d_{n + 1} - 2d_{n}) = 0$ After this Hardy uses simple arguments to show that $\beta = 0$ and $d_{n}$ should be identically zero from a certain value of $n = n_{0}$ so that $2^{n_{0}}\theta = b_{n_{0}}$ and thus $\theta$ is rational with denominator a power of $2$. In the above I don't understand why he assumes $b_{n}$ as even. Also I think the argument can be continued without assuming parity of $b_{n}$ but I am not sure. I wonder how this can be carried forward for higher values of $a$.","In Hardy's Pure Mathematics, Hardy discusses the limit $$\lim_{n\to\infty}\sin (2^{n}\theta\pi)$$ and says that if this limit exists it must be zero and then $\theta$ must be a rational number whose denominator is a power of $2$. Then he asks the reader to consider the limit $$\lim_{n\to\infty}\sin (a^{n}\theta\pi)$$ where $a$ is an integer greater than $2$. It is also mentioned that when $a > 2$ then the limit can be non-zero and as an example Hardy states that if $a = 9$ and $\theta = 1/2$ then the above limit is $1$. In this case I reasoned (based on Hardy's technique for the case $a = 2$) that $\lim_{n\to\infty}\sin((9^{n}\pi)/2)$ will be $1$ only when we can write $$\frac{9^{n}\pi}{2} = 2b_{n}\pi + \frac{\pi}{2} + c_{n}$$ for all sufficiently large values of $n$, where $\{b_{n}\}$ is a sequence which takes only integer values and $\{c_{n}\}$ is a sequence which tends to zero as $n \to \infty$. Thus we get $\displaystyle \begin{aligned}9^{n}\pi &= 4b_{n}\pi + \pi + 2c_{n}\\ \Rightarrow (9^{n} - 1)\pi &= 4b_{n}\pi + 2c_{n}\end{aligned}$ Since $9^{n} - 1$ is divisible by $9 - 1 = 8$ and hence also by $4$, it follows that we can take $b_{n} = (9^{n} - 1)/4$ and $c_{n} = 0$. The same logic could be applied when $a$ is any integer of the form $a = 4m + 1$ and $\theta = 1/2$. But, and this is my question, how does one handle the case for general integer $a > 2$ and any real $\theta$ rational or irrational? EDIT: I should also provide more details so that readers get the full context. For the specific case when $a = 2$ Hardy reasons that if the limit exists and, say is equal to $l$, then we must have $|l| \leq 1$ so that we have a constant $\alpha = \sin^{-1} l$ lying in interval $[-\pi/2, \pi/2]$. Since the solution of $\sin x = \sin \alpha$ is given by $x = m\pi + (-1)^{m}\alpha$ for all integers $m$, we can see that the existence of limit $l$ entails that $$2^{n}\theta\pi = b_{n}\pi + (-1)^{b_{n}}\alpha + c_{n}$$ where $b_{n}$ takes integer values and $c_{n} \to 0$ as $n\to\infty$. We can now see that $$2^{n}\theta = b_{n} + (-1)^{b_{n}}\beta + d_{n}$$ where $\beta = \alpha/\pi$ so that $\beta \in [-1/2, 1/2]$ and $d_{n} = c_{n}/\pi \to 0$ as $n \to\infty$. Hardy somehow assumes that $b_{n}$ will always be even and then proceeds as follows: $\displaystyle 2^{n}\theta = b_{n} + \beta + d_{n}$ so that multiplication by $2$ gives $\displaystyle 2^{n + 1}\theta = 2b_{n} + 2\beta + 2d_{n}$ and also we have $\displaystyle 2^{n + 1}\theta = b_{n + 1} + \beta + d_{n + 1}$ and therefore we get $\displaystyle (b_{n + 1} - 2b_{n}) - \beta + (d_{n + 1} - 2d_{n}) = 0$ After this Hardy uses simple arguments to show that $\beta = 0$ and $d_{n}$ should be identically zero from a certain value of $n = n_{0}$ so that $2^{n_{0}}\theta = b_{n_{0}}$ and thus $\theta$ is rational with denominator a power of $2$. In the above I don't understand why he assumes $b_{n}$ as even. Also I think the argument can be continued without assuming parity of $b_{n}$ but I am not sure. I wonder how this can be carried forward for higher values of $a$.",,['calculus']
57,Discontinuities of a function whose graph is invariant under rotation by 90 degrees,Discontinuities of a function whose graph is invariant under rotation by 90 degrees,,"Prove that there is no function on open interval $(-1,1)$, which has only finite number of discontinuity point, such that its graph is invariant under rotation by the right angle around the origin.","Prove that there is no function on open interval $(-1,1)$, which has only finite number of discontinuity point, such that its graph is invariant under rotation by the right angle around the origin.",,"['calculus', 'functions']"
58,"Prove that $\,\int_0^1 \sqrt{1+x^2}\,\mathrm dx\le \int_0^1 \sqrt{1+x}\,\mathrm dx\,,\,$ proof verification.",Prove that  proof verification.,"\,\int_0^1 \sqrt{1+x^2}\,\mathrm dx\le \int_0^1 \sqrt{1+x}\,\mathrm dx\,,\,","Prove that $\displaystyle\int_0^1 \sqrt{1+x^2}\mathrm dx\le \int_0^1 \sqrt{1+x}\,\mathrm dx$ So these are my first days of integral calulus. I think that's well done and is all clear, but would like to know if there's a better way to work it, or even if I've omitted something, would like to know what you will add or improve, thanks! My proof: Let $f(x)=\sqrt{1+x^2}$ and $g(x)=\sqrt{1+x}$ continuous functions on the interval $[0,1]$ , then, for both functions, the condition $x\in [0,1] \to 0\le x\le 1$ must be satisfied. So, we start by building the functions based on this condition: $$x\in [0,1] \to 0\le x\le 1$$ $$0\le x^2\le x$$ $$1\le 1+x^2\le 1+x$$ $$1\le \sqrt{1+x^2}\le \sqrt{1+x}$$ So, that implies that: $$f(x)\le g(x), \forall x\in[0,1]$$ (For this last step, I'm just applying a known proposition: If $f(x)\ge g(x), $ then $\int_a^b f(x)\,\mathrm dx\ge \int_a^b g(x)\,\mathrm dx$ ) $$\int_0^1 \sqrt{1+x^2}\,\mathrm dx\le \int_0^1 \sqrt{1+x}\,\mathrm dx$$","Prove that So these are my first days of integral calulus. I think that's well done and is all clear, but would like to know if there's a better way to work it, or even if I've omitted something, would like to know what you will add or improve, thanks! My proof: Let and continuous functions on the interval , then, for both functions, the condition must be satisfied. So, we start by building the functions based on this condition: So, that implies that: (For this last step, I'm just applying a known proposition: If then )","\displaystyle\int_0^1 \sqrt{1+x^2}\mathrm dx\le \int_0^1 \sqrt{1+x}\,\mathrm dx f(x)=\sqrt{1+x^2} g(x)=\sqrt{1+x} [0,1] x\in [0,1] \to 0\le x\le 1 x\in [0,1] \to 0\le x\le 1 0\le x^2\le x 1\le 1+x^2\le 1+x 1\le \sqrt{1+x^2}\le \sqrt{1+x} f(x)\le g(x), \forall x\in[0,1] f(x)\ge g(x),  \int_a^b f(x)\,\mathrm dx\ge \int_a^b g(x)\,\mathrm dx \int_0^1 \sqrt{1+x^2}\,\mathrm dx\le \int_0^1 \sqrt{1+x}\,\mathrm dx","['calculus', 'solution-verification', 'definite-integrals', 'proof-writing']"
59,Prove that $\lim\limits_{x\rightarrow 4} \sqrt{2x+7} = \sqrt{15}$.,Prove that .,\lim\limits_{x\rightarrow 4} \sqrt{2x+7} = \sqrt{15},"Prove that $\displaystyle \lim_{x\rightarrow 4} \sqrt{2x+7} = \sqrt{15}$ using the epsilon-delta definition. This is what I have, but I know my delta value is incorrect. My professor said that it was the right path but my delta is incorrect. Proof: Let $\varepsilon>0$. Choose $\delta$ such that $0<\delta<\min(\varepsilon,1)$. This means that both $\delta<1$ and $\delta<\varepsilon$. Let $x\in\mathbb{R}$ such that $0<|2x-8|<\delta$. Since $\delta<1$, we have $$\begin{array}{cccccc}  &-1 &< & 2x-8 & < & 1\\ \Rightarrow & 7 &<& 2x &<& 9 \\ \Rightarrow & 7/2 & < & x & < & 9/2  \end{array}$$ Since $7/2<x<9/2$, $$\begin{array}{cccccc}  &7/2 & < & x & < & 9/2\\ \Rightarrow & 7 &<& 2x &<& 9 \\ \Rightarrow & 7+7 & < & 2x+7 & < & 9+7 \\ \Rightarrow & \sqrt{14} & < & \sqrt{2x+7} & < & \sqrt{16} \\ \Rightarrow & \sqrt{14} + \sqrt{15} & < & \sqrt{2x+7}+\sqrt{15} & < & \sqrt{16}+\sqrt{15}\\ \Rightarrow & \displaystyle \frac{1}{\sqrt{14} + \sqrt{15}} & > & \displaystyle \frac{1}{\sqrt{2x+7}+\sqrt{15}} & > & \displaystyle \frac{1}{\sqrt{16} + \sqrt{15}}\\  \end{array}$$ This implies $$\left|\frac{1}{\sqrt{2x+7}+\sqrt{15}}\right|< \frac{1}{\sqrt{14} + \sqrt{15}}<1.$$ Therefore, $$\begin{align*} \left|\sqrt{2x+7}-\sqrt{15}\right| &= \left|\left(\sqrt{2x+7}-\sqrt{15}\right) \cdot \left(\frac{\sqrt{2x+7}+\sqrt{15}}{\sqrt{2x+7}+\sqrt{15}}\right)\right| \\ &= \left|2x+7-15\right| \cdot \left| \frac{1}{\sqrt{2x+7}+\sqrt{15}}\right|\\ &=\left|2x-8\right|\cdot \left|\frac{1}{\sqrt{2x+7}+\sqrt{15}}\right|\\ &< \delta \cdot 1 \\ &< \varepsilon \cdot 1\\ \end{align*}$$ Thus, $|\sqrt{2x+7}-\sqrt{15}|<\varepsilon$. So, $\displaystyle \lim_{x\rightarrow 4} \sqrt{2x+7} = \sqrt{15}$.","Prove that $\displaystyle \lim_{x\rightarrow 4} \sqrt{2x+7} = \sqrt{15}$ using the epsilon-delta definition. This is what I have, but I know my delta value is incorrect. My professor said that it was the right path but my delta is incorrect. Proof: Let $\varepsilon>0$. Choose $\delta$ such that $0<\delta<\min(\varepsilon,1)$. This means that both $\delta<1$ and $\delta<\varepsilon$. Let $x\in\mathbb{R}$ such that $0<|2x-8|<\delta$. Since $\delta<1$, we have $$\begin{array}{cccccc}  &-1 &< & 2x-8 & < & 1\\ \Rightarrow & 7 &<& 2x &<& 9 \\ \Rightarrow & 7/2 & < & x & < & 9/2  \end{array}$$ Since $7/2<x<9/2$, $$\begin{array}{cccccc}  &7/2 & < & x & < & 9/2\\ \Rightarrow & 7 &<& 2x &<& 9 \\ \Rightarrow & 7+7 & < & 2x+7 & < & 9+7 \\ \Rightarrow & \sqrt{14} & < & \sqrt{2x+7} & < & \sqrt{16} \\ \Rightarrow & \sqrt{14} + \sqrt{15} & < & \sqrt{2x+7}+\sqrt{15} & < & \sqrt{16}+\sqrt{15}\\ \Rightarrow & \displaystyle \frac{1}{\sqrt{14} + \sqrt{15}} & > & \displaystyle \frac{1}{\sqrt{2x+7}+\sqrt{15}} & > & \displaystyle \frac{1}{\sqrt{16} + \sqrt{15}}\\  \end{array}$$ This implies $$\left|\frac{1}{\sqrt{2x+7}+\sqrt{15}}\right|< \frac{1}{\sqrt{14} + \sqrt{15}}<1.$$ Therefore, $$\begin{align*} \left|\sqrt{2x+7}-\sqrt{15}\right| &= \left|\left(\sqrt{2x+7}-\sqrt{15}\right) \cdot \left(\frac{\sqrt{2x+7}+\sqrt{15}}{\sqrt{2x+7}+\sqrt{15}}\right)\right| \\ &= \left|2x+7-15\right| \cdot \left| \frac{1}{\sqrt{2x+7}+\sqrt{15}}\right|\\ &=\left|2x-8\right|\cdot \left|\frac{1}{\sqrt{2x+7}+\sqrt{15}}\right|\\ &< \delta \cdot 1 \\ &< \varepsilon \cdot 1\\ \end{align*}$$ Thus, $|\sqrt{2x+7}-\sqrt{15}|<\varepsilon$. So, $\displaystyle \lim_{x\rightarrow 4} \sqrt{2x+7} = \sqrt{15}$.",,"['calculus', 'limits', 'epsilon-delta']"
60,Prove that the limit of $\lim\limits_{x \to 4}{\frac{\sqrt{x}-2}{x-4}} = \frac{1}{4}$,Prove that the limit of,\lim\limits_{x \to 4}{\frac{\sqrt{x}-2}{x-4}} = \frac{1}{4},"I was asked to prove, using the definition of limit, that $$\lim\limits_{x \to 4}{\frac{\sqrt{x}-2}{x-4}} = \frac{1}{4}$$ I tried doing it but I am not completely sure my prove is correct. I am publishing it here so that more knowledgeable people can check it for me, maybe pointing any pitfalls. I need to prove that for any $\epsilon > 0$ there is a $\delta > 0$ such that if $0 < \left|x - 4\right| < \delta$ then $\left|\frac{\sqrt{x}-2}{x-4} - \frac{1}{4}\right| < \epsilon$ First part: Tying to find a suitable $\delta$, given an $\epsilon$ $$\left|\frac{\sqrt{x}-2}{x-4} - \frac{1}{4}\right| < \epsilon$$ $$\left|\frac{4(\sqrt{x}-2) - (x-4)}{4(x-4)}\right| < \epsilon$$ $$\left|\frac{4\sqrt{x} - 8 - x + 4}{4(x-4)}\right| < \epsilon$$ $$\left|\frac{4\sqrt{x} - x - 4}{4(x-4)}\right| < \epsilon$$ $$\left|\frac{-x + 4\sqrt{x} - 4}{4(x-4)}\right| < \epsilon$$ $$\left|\frac{-(x - 2\cdot\sqrt{x}\cdot 2 + 4)}{4(x-4)}\right| < \epsilon$$ $$\left|\frac{-(\sqrt{x} - 2)^2}{4(\sqrt{x}+2)(\sqrt{x}-2)}\right| < \epsilon$$ $$\left|\frac{-(\sqrt{x} - 2)}{4(\sqrt{x}+2)}\right| < \epsilon$$ $$\left|-\frac{1}{4}\right| \left|\frac{\sqrt{x} - 2}{\sqrt{x}+2}\right| < \epsilon$$ $$\frac{1}{4} \left|\frac{\sqrt{x} - 2}{\sqrt{x}+2}\right| < \epsilon$$ $$\left|\frac{\sqrt{x} - 2}{\sqrt{x}+2}\right| < 4\epsilon$$ $$-4\epsilon < \frac{\sqrt{x} - 2}{\sqrt{x}+2} < 4\epsilon$$ First inequation: $$-4\epsilon < \frac{\sqrt{x} - 2}{\sqrt{x}+2}$$ Multiplying by $\sqrt{x} + 2$ (which is positive): $$-4\epsilon (\sqrt{x}+2) < \sqrt{x} - 2$$ $$-4\epsilon \sqrt{x} -8\epsilon < \sqrt{x} - 2$$ $$ - 8\epsilon + 2 < \sqrt{x} + 4\epsilon \sqrt{x}$$ $$ 2(1 - 4\epsilon) < (1 + 4\epsilon) \sqrt{x}$$ $$ \frac{2(1 - 4\epsilon)}{1+4\epsilon} < \sqrt{x}$$ $$ \left[\frac{2(1 - 4\epsilon)}{1+4\epsilon}\right]^2 < (\sqrt{x})^2$$ $$ \frac{4(1 - 4\epsilon)^2}{(1+4\epsilon)^2} < x$$ $$ \frac{4(1 - 4\epsilon)^2}{(1+4\epsilon)^2} - 4 < x - 4$$ $$ 4\left[\frac{(1 - 4\epsilon)^2}{(1+4\epsilon)^2} - 1\right] < x - 4$$ $$ 4\left[\frac{(1 - 4\epsilon)^2 - (1 + 4\epsilon)^2}{(1+4\epsilon)^2}\right] < x - 4$$ $$ 4\left[\frac{1 - 8\epsilon + 16\epsilon^2 - 1 - 8\epsilon - 16\epsilon^2}{(1+4\epsilon)^2}\right] < x - 4$$ $$ 4\left[\frac{-16\epsilon}{(1+4\epsilon)^2}\right] < x - 4$$ $$ \frac{-72\epsilon}{(1+4\epsilon)^2} < x - 4$$ Second inequation: $$\frac{\sqrt{x} - 2}{\sqrt{x}+2} < 4\epsilon$$ Multiplying by $\sqrt{x} + 2$ (which is positive): $$\sqrt{x} - 2 < 4\epsilon (\sqrt{x}+2)$$ $$\sqrt{x} - 2 < 4\epsilon \sqrt{x} + 8\epsilon$$ $$\sqrt{x} - 4\epsilon \sqrt{x} < 8\epsilon + 2$$ $$ (1 - 4\epsilon) \sqrt{x} < 2(1 + 4\epsilon)$$ $$\sqrt{x} <  \frac{2(1 + 4\epsilon)}{1-4\epsilon}$$ $$(\sqrt{x})^2 <  \left[\frac{2(1 + 4\epsilon)}{1-4\epsilon}\right]^2$$ $$x <  \frac{4(1 + 4\epsilon)^2}{(1-4\epsilon)^2}$$ $$x - 4 < \frac{4(1 + 4\epsilon)^2}{(1-4\epsilon)^2} - 4$$ $$x - 4 < 4\left[\frac{(1 + 4\epsilon)^2}{(1-4\epsilon)^2} - 1\right]$$ $$x - 4 < 4\left[\frac{(1 + 4\epsilon)^2 - (1 - 4\epsilon)^2}{(1-4\epsilon)^2}\right]$$ $$x - 4 < 4\left[\frac{1 + 8\epsilon + 16\epsilon^2 - 1 + 8\epsilon - 16\epsilon^2}{(1-4\epsilon)^2}\right]$$ $$x - 4 < 4\left[\frac{16\epsilon}{(1-4\epsilon)^2}\right]$$ $$x - 4 < \frac{72\epsilon}{(1-4\epsilon)^2}$$ Therefore: $$- \frac{72\epsilon}{(1+4\epsilon)^2} < x - 4 < \frac{72\epsilon}{(1-4\epsilon)^2}$$ Taking the minimum of $\frac{72\epsilon}{(1+4\epsilon)^2}$ and $\frac{72\epsilon}{(1-4\epsilon)^2}$ as the value for $\delta$: $$ \delta = \min\left[\frac{72\epsilon}{(1+4\epsilon)^2} , \frac{72\epsilon}{(1-4\epsilon)^2}\right]$$ $$ \delta = \frac{72\epsilon}{(1+4\epsilon)^2}$$ Second part: Prove Prove that $$ 0 < \left|x-4\right| < \frac{72\epsilon}{(1+4\epsilon)^2} \implies \left|\frac{\sqrt{x}-2}{x-4} - \frac{1}{4}\right| < \epsilon$$ Is it correct so far? How to proceed from here?","I was asked to prove, using the definition of limit, that $$\lim\limits_{x \to 4}{\frac{\sqrt{x}-2}{x-4}} = \frac{1}{4}$$ I tried doing it but I am not completely sure my prove is correct. I am publishing it here so that more knowledgeable people can check it for me, maybe pointing any pitfalls. I need to prove that for any $\epsilon > 0$ there is a $\delta > 0$ such that if $0 < \left|x - 4\right| < \delta$ then $\left|\frac{\sqrt{x}-2}{x-4} - \frac{1}{4}\right| < \epsilon$ First part: Tying to find a suitable $\delta$, given an $\epsilon$ $$\left|\frac{\sqrt{x}-2}{x-4} - \frac{1}{4}\right| < \epsilon$$ $$\left|\frac{4(\sqrt{x}-2) - (x-4)}{4(x-4)}\right| < \epsilon$$ $$\left|\frac{4\sqrt{x} - 8 - x + 4}{4(x-4)}\right| < \epsilon$$ $$\left|\frac{4\sqrt{x} - x - 4}{4(x-4)}\right| < \epsilon$$ $$\left|\frac{-x + 4\sqrt{x} - 4}{4(x-4)}\right| < \epsilon$$ $$\left|\frac{-(x - 2\cdot\sqrt{x}\cdot 2 + 4)}{4(x-4)}\right| < \epsilon$$ $$\left|\frac{-(\sqrt{x} - 2)^2}{4(\sqrt{x}+2)(\sqrt{x}-2)}\right| < \epsilon$$ $$\left|\frac{-(\sqrt{x} - 2)}{4(\sqrt{x}+2)}\right| < \epsilon$$ $$\left|-\frac{1}{4}\right| \left|\frac{\sqrt{x} - 2}{\sqrt{x}+2}\right| < \epsilon$$ $$\frac{1}{4} \left|\frac{\sqrt{x} - 2}{\sqrt{x}+2}\right| < \epsilon$$ $$\left|\frac{\sqrt{x} - 2}{\sqrt{x}+2}\right| < 4\epsilon$$ $$-4\epsilon < \frac{\sqrt{x} - 2}{\sqrt{x}+2} < 4\epsilon$$ First inequation: $$-4\epsilon < \frac{\sqrt{x} - 2}{\sqrt{x}+2}$$ Multiplying by $\sqrt{x} + 2$ (which is positive): $$-4\epsilon (\sqrt{x}+2) < \sqrt{x} - 2$$ $$-4\epsilon \sqrt{x} -8\epsilon < \sqrt{x} - 2$$ $$ - 8\epsilon + 2 < \sqrt{x} + 4\epsilon \sqrt{x}$$ $$ 2(1 - 4\epsilon) < (1 + 4\epsilon) \sqrt{x}$$ $$ \frac{2(1 - 4\epsilon)}{1+4\epsilon} < \sqrt{x}$$ $$ \left[\frac{2(1 - 4\epsilon)}{1+4\epsilon}\right]^2 < (\sqrt{x})^2$$ $$ \frac{4(1 - 4\epsilon)^2}{(1+4\epsilon)^2} < x$$ $$ \frac{4(1 - 4\epsilon)^2}{(1+4\epsilon)^2} - 4 < x - 4$$ $$ 4\left[\frac{(1 - 4\epsilon)^2}{(1+4\epsilon)^2} - 1\right] < x - 4$$ $$ 4\left[\frac{(1 - 4\epsilon)^2 - (1 + 4\epsilon)^2}{(1+4\epsilon)^2}\right] < x - 4$$ $$ 4\left[\frac{1 - 8\epsilon + 16\epsilon^2 - 1 - 8\epsilon - 16\epsilon^2}{(1+4\epsilon)^2}\right] < x - 4$$ $$ 4\left[\frac{-16\epsilon}{(1+4\epsilon)^2}\right] < x - 4$$ $$ \frac{-72\epsilon}{(1+4\epsilon)^2} < x - 4$$ Second inequation: $$\frac{\sqrt{x} - 2}{\sqrt{x}+2} < 4\epsilon$$ Multiplying by $\sqrt{x} + 2$ (which is positive): $$\sqrt{x} - 2 < 4\epsilon (\sqrt{x}+2)$$ $$\sqrt{x} - 2 < 4\epsilon \sqrt{x} + 8\epsilon$$ $$\sqrt{x} - 4\epsilon \sqrt{x} < 8\epsilon + 2$$ $$ (1 - 4\epsilon) \sqrt{x} < 2(1 + 4\epsilon)$$ $$\sqrt{x} <  \frac{2(1 + 4\epsilon)}{1-4\epsilon}$$ $$(\sqrt{x})^2 <  \left[\frac{2(1 + 4\epsilon)}{1-4\epsilon}\right]^2$$ $$x <  \frac{4(1 + 4\epsilon)^2}{(1-4\epsilon)^2}$$ $$x - 4 < \frac{4(1 + 4\epsilon)^2}{(1-4\epsilon)^2} - 4$$ $$x - 4 < 4\left[\frac{(1 + 4\epsilon)^2}{(1-4\epsilon)^2} - 1\right]$$ $$x - 4 < 4\left[\frac{(1 + 4\epsilon)^2 - (1 - 4\epsilon)^2}{(1-4\epsilon)^2}\right]$$ $$x - 4 < 4\left[\frac{1 + 8\epsilon + 16\epsilon^2 - 1 + 8\epsilon - 16\epsilon^2}{(1-4\epsilon)^2}\right]$$ $$x - 4 < 4\left[\frac{16\epsilon}{(1-4\epsilon)^2}\right]$$ $$x - 4 < \frac{72\epsilon}{(1-4\epsilon)^2}$$ Therefore: $$- \frac{72\epsilon}{(1+4\epsilon)^2} < x - 4 < \frac{72\epsilon}{(1-4\epsilon)^2}$$ Taking the minimum of $\frac{72\epsilon}{(1+4\epsilon)^2}$ and $\frac{72\epsilon}{(1-4\epsilon)^2}$ as the value for $\delta$: $$ \delta = \min\left[\frac{72\epsilon}{(1+4\epsilon)^2} , \frac{72\epsilon}{(1-4\epsilon)^2}\right]$$ $$ \delta = \frac{72\epsilon}{(1+4\epsilon)^2}$$ Second part: Prove Prove that $$ 0 < \left|x-4\right| < \frac{72\epsilon}{(1+4\epsilon)^2} \implies \left|\frac{\sqrt{x}-2}{x-4} - \frac{1}{4}\right| < \epsilon$$ Is it correct so far? How to proceed from here?",,"['calculus', 'limits', 'epsilon-delta']"
61,A difficult contest question from the former Soviet Union,A difficult contest question from the former Soviet Union,,Let $(a_n)$ be a positive sequence such that $\varlimsup\limits_{n\to\infty} a_n^{1/n}=1$ and $\varliminf\limits_{n\to\infty} a_n^{1/n}<1$. Prove there exists a subsequence $(a_{n_i})$ such that $$\lim\limits_{i\to\infty}\left(a_{n_i}\right)^{1/{n_i}}=1$$ and $$\lim\limits_{i\to\infty}{\lvert(a_{n_i})^2-a_{n_i+1}a_{n_i-1}\rvert}^{1/{n_i}}=1.$$,Let $(a_n)$ be a positive sequence such that $\varlimsup\limits_{n\to\infty} a_n^{1/n}=1$ and $\varliminf\limits_{n\to\infty} a_n^{1/n}<1$. Prove there exists a subsequence $(a_{n_i})$ such that $$\lim\limits_{i\to\infty}\left(a_{n_i}\right)^{1/{n_i}}=1$$ and $$\lim\limits_{i\to\infty}{\lvert(a_{n_i})^2-a_{n_i+1}a_{n_i-1}\rvert}^{1/{n_i}}=1.$$,,"['calculus', 'limits', 'contest-math']"
62,The same bit of trivial algebra in two different places?,The same bit of trivial algebra in two different places?,,"The Villarceau circles are things whose existence is surprising.  To find radii of Villarceau circles, I stupidly went through a bit of trigonometry and got a much simpler result than I expected, and then realized there was a glaringly obvious way to do it that I hadn't thought of. In the $xyz$-space imagine a circle of radius $r>0$ in the $xz$-plane, whose center is at a distance $R>r$ from the $z$-axis, and revolve it about the $z$-axis, getting a torus embedded in $\mathbb R^3$. The intersection of that surface with the $xz$-plane is two circles not crossing each other.  A line $\ell_1$ touches one of those circles on one side and another on the other side, and that line is in a plane parallel to the $y$-axis, and the intersection of that plane with the torus is the union of two Villarceau circles.  So I thought: let's draw a line $\ell_2$ touching both circles on the same side, and the other line $\ell_3$ touching both circles on the same side, and the distance from the intersection of $\ell_1$ with $\ell_2$ to the intersection of $\ell_1$ with $\ell_3$ is the diameter of the Villarceau circle.  So I thought: first, the distance from the center to the point of tangency of $\ell_1$ with one circle, is $$\sqrt{R^2-r^2}.\tag{1}$$  Add to that the distance the distance from that point of tangency to the point of intersection of $\ell_1$ with the nearest of those two parallel $\ell$s, and that distance is $$ \frac{r^2}{R+\sqrt{R^2 - r^2}}.\tag{2} $$ So the sum of $(1)$ and $(2)$ involves finding a common denominator, doing some routine cancelations, getting $$ \frac{R\left(R+\sqrt{R^2-r^2}\,\right)}{R+\sqrt{R^2-r^2}} $$ and one more cancellation gives you $R$.  Then I realized that the obvious way to see that the radius is $R$ doesn't involve doing any of that. But I recognized that bit of trivial algebra from a routine calculus problem: $$ \frac{d}{dx} \log\left(x+\sqrt{x^2-1}\,\right). $$ Going through the usual algorithms, you get this down to $$ \frac{x+\sqrt{x^2-1}}{\sqrt{x^2-1}\left(x+\sqrt{x^2-1}\,\right)} $$ and again to one last cancelation to get $\dfrac{1}{\sqrt{x^2-1}}$. SO MY QUESTION IS: What particular commonality between these two problems causes this same bit of algebra to occur in both places?","The Villarceau circles are things whose existence is surprising.  To find radii of Villarceau circles, I stupidly went through a bit of trigonometry and got a much simpler result than I expected, and then realized there was a glaringly obvious way to do it that I hadn't thought of. In the $xyz$-space imagine a circle of radius $r>0$ in the $xz$-plane, whose center is at a distance $R>r$ from the $z$-axis, and revolve it about the $z$-axis, getting a torus embedded in $\mathbb R^3$. The intersection of that surface with the $xz$-plane is two circles not crossing each other.  A line $\ell_1$ touches one of those circles on one side and another on the other side, and that line is in a plane parallel to the $y$-axis, and the intersection of that plane with the torus is the union of two Villarceau circles.  So I thought: let's draw a line $\ell_2$ touching both circles on the same side, and the other line $\ell_3$ touching both circles on the same side, and the distance from the intersection of $\ell_1$ with $\ell_2$ to the intersection of $\ell_1$ with $\ell_3$ is the diameter of the Villarceau circle.  So I thought: first, the distance from the center to the point of tangency of $\ell_1$ with one circle, is $$\sqrt{R^2-r^2}.\tag{1}$$  Add to that the distance the distance from that point of tangency to the point of intersection of $\ell_1$ with the nearest of those two parallel $\ell$s, and that distance is $$ \frac{r^2}{R+\sqrt{R^2 - r^2}}.\tag{2} $$ So the sum of $(1)$ and $(2)$ involves finding a common denominator, doing some routine cancelations, getting $$ \frac{R\left(R+\sqrt{R^2-r^2}\,\right)}{R+\sqrt{R^2-r^2}} $$ and one more cancellation gives you $R$.  Then I realized that the obvious way to see that the radius is $R$ doesn't involve doing any of that. But I recognized that bit of trivial algebra from a routine calculus problem: $$ \frac{d}{dx} \log\left(x+\sqrt{x^2-1}\,\right). $$ Going through the usual algorithms, you get this down to $$ \frac{x+\sqrt{x^2-1}}{\sqrt{x^2-1}\left(x+\sqrt{x^2-1}\,\right)} $$ and again to one last cancelation to get $\dfrac{1}{\sqrt{x^2-1}}$. SO MY QUESTION IS: What particular commonality between these two problems causes this same bit of algebra to occur in both places?",,"['calculus', 'geometry', 'algebra-precalculus']"
63,Is there an analytic solution for such problem?,Is there an analytic solution for such problem?,,"Given function $$f_n(x) = \cos x - (\cos \cos x) + (\cos \cos \cos x) - (\cos \cos \cos \cos x) + \dots + (-1)^{n-1} \underbrace{ \cos \cos \dots \cos }_n x,$$ where $n \in \mathbb{N}$ and $\underbrace{ \cos \cos \dots \cos }_n$ means cosine of cosine of cosine and so on $n$ times, find value of $$\sup_{n \rightarrow \infty, x \in \mathbb{R}} \{f_k(x)\}^{n}_{k=1}$$","Given function where and means cosine of cosine of cosine and so on times, find value of","f_n(x) = \cos x - (\cos \cos x) + (\cos \cos \cos x) - (\cos \cos \cos \cos x) + \dots + (-1)^{n-1} \underbrace{ \cos \cos \dots \cos }_n x, n \in \mathbb{N} \underbrace{ \cos \cos \dots \cos }_n n \sup_{n \rightarrow \infty, x \in \mathbb{R}} \{f_k(x)\}^{n}_{k=1}","['calculus', 'sequences-and-series', 'trigonometry', 'recurrence-relations']"
64,prove or disprove an inequality on bounds of derivatives for radial functions,prove or disprove an inequality on bounds of derivatives for radial functions,,"Suppose $f$ is a radial function, i.e., $f(x)=f(|x|)$, and $f \in C^\infty(\bar{B})$, where $\bar{B}$ is the closure of the unit ball in $\mathbb{R}^n$. Prove or disprove the following. Given any positive integer $k$, $$\sup_{|\alpha|=k,x\in B} |D^\alpha f(x)| \leq \sup_{r < 1} \lvert f^{(k)}(r) \rvert,$$ where $\alpha$ is a multi-index and $D^\alpha f$ is the corresponding derivative of $f$. By $f^{(k)}(r)$, we mean the $k^{th}$ derivative of $f$ as a function of $r=|x|.$ The norm on the left is in fact $|\cdot|_{W^{k,\infty}(B)}.$ I try some functions and the inequality hold for all of them.  The case where $k=1$ is easy to prove but I can't prove for a general $k$. Update: Instead of a general smooth $f$,  can we prove the assertion for polynomials in which $f(r) = \sum_{j=0}^m c_j r^{2j}$ ?","Suppose $f$ is a radial function, i.e., $f(x)=f(|x|)$, and $f \in C^\infty(\bar{B})$, where $\bar{B}$ is the closure of the unit ball in $\mathbb{R}^n$. Prove or disprove the following. Given any positive integer $k$, $$\sup_{|\alpha|=k,x\in B} |D^\alpha f(x)| \leq \sup_{r < 1} \lvert f^{(k)}(r) \rvert,$$ where $\alpha$ is a multi-index and $D^\alpha f$ is the corresponding derivative of $f$. By $f^{(k)}(r)$, we mean the $k^{th}$ derivative of $f$ as a function of $r=|x|.$ The norm on the left is in fact $|\cdot|_{W^{k,\infty}(B)}.$ I try some functions and the inequality hold for all of them.  The case where $k=1$ is easy to prove but I can't prove for a general $k$. Update: Instead of a general smooth $f$,  can we prove the assertion for polynomials in which $f(r) = \sum_{j=0}^m c_j r^{2j}$ ?",,"['calculus', 'inequality', 'partial-differential-equations', 'sobolev-spaces', 'partial-derivative']"
65,Did Einstein introduce anything new to mathematics? [duplicate],Did Einstein introduce anything new to mathematics? [duplicate],,"This question already has an answer here : Did Albert Einstein contribute to math? (1 answer) Closed 9 years ago . Newton introduced calculus, so I am wondering, did Einstein introduce anything important to mathematics?","This question already has an answer here : Did Albert Einstein contribute to math? (1 answer) Closed 9 years ago . Newton introduced calculus, so I am wondering, did Einstein introduce anything important to mathematics?",,"['calculus', 'soft-question', 'notation']"
66,"Indefinite Integral with ""sin"" and ""cos"": $\int\frac{3\sin(x) + 2\cos(x)}{2\sin(x) + 3\cos(x)} \; dx $","Indefinite Integral with ""sin"" and ""cos"":",\int\frac{3\sin(x) + 2\cos(x)}{2\sin(x) + 3\cos(x)} \; dx ,Indefinite Integral with sin/cos I can't find a good way to integrate: $$\int\dfrac{3\sin(x) + 2\cos(x)}{2\sin(x) + 3\cos(x)} \; dx $$,Indefinite Integral with sin/cos I can't find a good way to integrate: $$\int\dfrac{3\sin(x) + 2\cos(x)}{2\sin(x) + 3\cos(x)} \; dx $$,,"['calculus', 'integration', 'trigonometry', 'indefinite-integrals']"
67,Is $\int_a^b f(x) dx = \int_{f(a)}^{f(b)} f^{-1}(x) dx$?,Is ?,\int_a^b f(x) dx = \int_{f(a)}^{f(b)} f^{-1}(x) dx,"Is it true that $$\int_a^b f(x) dx = \int_{f(a)}^{f(b)} f^{-1}(x) dx$$ Just making sure. If not, how about: $$\int_a^b f(x) dx = (f(b)-f(a))b - \int_{f(a)}^{f(b)}f^{-1}(x)dx$$ I'm having a hard time concentrating right now, and I'm trying to figure out how to get the area under a curve when the function is inverted.","Is it true that $$\int_a^b f(x) dx = \int_{f(a)}^{f(b)} f^{-1}(x) dx$$ Just making sure. If not, how about: $$\int_a^b f(x) dx = (f(b)-f(a))b - \int_{f(a)}^{f(b)}f^{-1}(x)dx$$ I'm having a hard time concentrating right now, and I'm trying to figure out how to get the area under a curve when the function is inverted.",,"['calculus', 'integration']"
68,Can the difference of 2 undefined limits be defined?,Can the difference of 2 undefined limits be defined?,,"Is this limit defined or undefined? $$\lim\limits_{x \to 0+} \left(\sqrt{\frac{1}{x}+2}-\sqrt{\frac{1}{x}}\right)$$ When I apply the rule of difference of limits, it's undefined. But, when I manipulate it, it gives me zero. And the graph of the function indicates it's defined on the right side. By multiplying by $\frac{\sqrt{\frac{1}{x}+2}+\sqrt{\frac{1}{x}}}{\sqrt{\frac{1}{x}+2}+\sqrt{\frac{1}{x}}}$: $$\lim\limits_{x \to 0+} \frac{\left( \sqrt{\frac{1}{x}+2}-\sqrt{\frac{1}{x}} \, \right) \left(\sqrt{\frac{1}{x}+2}+\sqrt{\frac{1}{x}} \, \right)}{\sqrt{\frac{1}{x}+2}+\sqrt{\frac{1}{x}}}$$ $$=\lim\limits_{x \to 0+} \frac{\frac{1}{x}+2-\frac{1}{x}}{\sqrt{\frac{1}{x}+2}+\sqrt{\frac{1}{x}}}$$ $$=\lim\limits_{x \to 0+} \frac{2}{\sqrt{\frac{1}{x}+2}+\sqrt{\frac{1}{x}}}$$ Then, we multiply by $\frac{\sqrt{x}}{\sqrt{x}}$: $$=\lim\limits_{x \to 0} \frac{2\sqrt{x}}{\sqrt{1+2x}+1}$$ And, we substitute: $$=\frac{2\sqrt{0}}{\sqrt{1+2\times0}+1} = 0$$ So, is this limit defined or not? and what's my error, if any?","Is this limit defined or undefined? $$\lim\limits_{x \to 0+} \left(\sqrt{\frac{1}{x}+2}-\sqrt{\frac{1}{x}}\right)$$ When I apply the rule of difference of limits, it's undefined. But, when I manipulate it, it gives me zero. And the graph of the function indicates it's defined on the right side. By multiplying by $\frac{\sqrt{\frac{1}{x}+2}+\sqrt{\frac{1}{x}}}{\sqrt{\frac{1}{x}+2}+\sqrt{\frac{1}{x}}}$: $$\lim\limits_{x \to 0+} \frac{\left( \sqrt{\frac{1}{x}+2}-\sqrt{\frac{1}{x}} \, \right) \left(\sqrt{\frac{1}{x}+2}+\sqrt{\frac{1}{x}} \, \right)}{\sqrt{\frac{1}{x}+2}+\sqrt{\frac{1}{x}}}$$ $$=\lim\limits_{x \to 0+} \frac{\frac{1}{x}+2-\frac{1}{x}}{\sqrt{\frac{1}{x}+2}+\sqrt{\frac{1}{x}}}$$ $$=\lim\limits_{x \to 0+} \frac{2}{\sqrt{\frac{1}{x}+2}+\sqrt{\frac{1}{x}}}$$ Then, we multiply by $\frac{\sqrt{x}}{\sqrt{x}}$: $$=\lim\limits_{x \to 0} \frac{2\sqrt{x}}{\sqrt{1+2x}+1}$$ And, we substitute: $$=\frac{2\sqrt{0}}{\sqrt{1+2\times0}+1} = 0$$ So, is this limit defined or not? and what's my error, if any?",,"['calculus', 'limits']"
69,Integrate $\left[\arctan\left(x\right)/x\right]^{2}$ between $-\infty$ and $+\infty$,Integrate  between  and,\left[\arctan\left(x\right)/x\right]^{2} -\infty +\infty,"I have tried to calculate $$ \int_{-\infty}^{\infty}\left[\arctan\left(x\right) \over x\right]^{2}\,{\rm d}x $$ with integration by parts and that didn't work. I looked up the indefinite integral and found it contained a polylogarithm which I don't know how to use so I tried contour integration  but got stuck. $${\tt\mbox{Wolfram Alpha said the answer is}}\,\,\,{\large \pi\log\left(4\right)}$$ Can anyone show me how to do this integral ?.","I have tried to calculate $$ \int_{-\infty}^{\infty}\left[\arctan\left(x\right) \over x\right]^{2}\,{\rm d}x $$ with integration by parts and that didn't work. I looked up the indefinite integral and found it contained a polylogarithm which I don't know how to use so I tried contour integration  but got stuck. $${\tt\mbox{Wolfram Alpha said the answer is}}\,\,\,{\large \pi\log\left(4\right)}$$ Can anyone show me how to do this integral ?.",,"['calculus', 'integration']"
70,Purpose Of Adding A Constant After Integrating A Function,Purpose Of Adding A Constant After Integrating A Function,,"I would like to know the whole purpose of adding a constant termed constant of integration everytime we integrate an indefinite integral $\int f(x)dx$. I am aware that this constant ""goes away"" when evaluating definite integral $\int_{a}^{b}f(x)dx $. What has that constant have to do with anything? Why is it termed as the constant of integration ? Where does it come from? The motivation for asking this question actually comes from solving a differential equation $$x \frac{dy}{dx} = 5x^3 + 4$$ By separation of $dy$ and $dx$ and integrating both sides, $$\int dy = \int\left(5x^2 + \frac{4}{x}\right)dx$$  yields $$y = \frac{5x^3}{3} + 4 \ln(x) + C .$$ I've understood that $\int dy$ represents adding infinitesimal quantity of $dy$'s yielding $y$ but I'am doubtful about the arbitrary constant $C$.","I would like to know the whole purpose of adding a constant termed constant of integration everytime we integrate an indefinite integral $\int f(x)dx$. I am aware that this constant ""goes away"" when evaluating definite integral $\int_{a}^{b}f(x)dx $. What has that constant have to do with anything? Why is it termed as the constant of integration ? Where does it come from? The motivation for asking this question actually comes from solving a differential equation $$x \frac{dy}{dx} = 5x^3 + 4$$ By separation of $dy$ and $dx$ and integrating both sides, $$\int dy = \int\left(5x^2 + \frac{4}{x}\right)dx$$  yields $$y = \frac{5x^3}{3} + 4 \ln(x) + C .$$ I've understood that $\int dy$ represents adding infinitesimal quantity of $dy$'s yielding $y$ but I'am doubtful about the arbitrary constant $C$.",,"['calculus', 'integration']"
71,Finding the $N$-th derivative of $f(x)=\frac {x} {x^2-1}$,Finding the -th derivative of,N f(x)=\frac {x} {x^2-1},I'm practicing some problems from past exams and found this one: Find the n-th derivative of this function: $$f(x)=\frac {x} {x^2-1}$$ I have no idea how to start solving this problems. Is there any theorem for finding nth derivative?,I'm practicing some problems from past exams and found this one: Find the n-th derivative of this function: I have no idea how to start solving this problems. Is there any theorem for finding nth derivative?,f(x)=\frac {x} {x^2-1},"['calculus', 'derivatives']"
72,"What is the intuition behind why the integration of $f(x) = x$ for closed interval of negative to positive infinity diverges, rather than being zero?","What is the intuition behind why the integration of  for closed interval of negative to positive infinity diverges, rather than being zero?",f(x) = x,"I'm currently studying integrals in calculus. In teaching improper integrals over infinite intervals, I come across this example in my lecture notes, $$\int_{-\infty}^{\infty} x \ dx$$ Naturally, the integration is split into two halves, such as $\int_{-\infty}^{0}x\ dx=-\infty$ and $\int_{0}^{\infty}x\ dx=\infty$. The lecture notes conclude that since both these improper integrals diverge, then so does the above improper integral, i.e. $$\Rightarrow\int_{-\infty}^{\infty}x\ dx\ \mathrm{diverges}$$ Intuitively, however, I would expect that the first improper integral, $\int_{-\infty}^{\infty}x\ dx$ should evaluate to $0$, given that, An intuitive meaning of the definite integral is the area under the curve The curve of $y=x$ gives a negative area for $(-\infty,0)$ and positive area for $(0, \infty)$ The curve of $y=x$ is symmetrical about $y=0$ So, my questions are Why is this not the case, and Is $\int_{-\infty}^{\infty}x\ dx\ \mathrm{diverges}$ even what I think it means, that $\int_{-\infty}^{\infty}x\ dx=\infty$?","I'm currently studying integrals in calculus. In teaching improper integrals over infinite intervals, I come across this example in my lecture notes, $$\int_{-\infty}^{\infty} x \ dx$$ Naturally, the integration is split into two halves, such as $\int_{-\infty}^{0}x\ dx=-\infty$ and $\int_{0}^{\infty}x\ dx=\infty$. The lecture notes conclude that since both these improper integrals diverge, then so does the above improper integral, i.e. $$\Rightarrow\int_{-\infty}^{\infty}x\ dx\ \mathrm{diverges}$$ Intuitively, however, I would expect that the first improper integral, $\int_{-\infty}^{\infty}x\ dx$ should evaluate to $0$, given that, An intuitive meaning of the definite integral is the area under the curve The curve of $y=x$ gives a negative area for $(-\infty,0)$ and positive area for $(0, \infty)$ The curve of $y=x$ is symmetrical about $y=0$ So, my questions are Why is this not the case, and Is $\int_{-\infty}^{\infty}x\ dx\ \mathrm{diverges}$ even what I think it means, that $\int_{-\infty}^{\infty}x\ dx=\infty$?",,"['calculus', 'integration', 'limits', 'improper-integrals', 'infinity']"
73,"Evaluating $\int \frac{\sec^2 x}{(\sec x + \tan x )^{{9}/{2}}}\,\mathrm dx$",Evaluating,"\int \frac{\sec^2 x}{(\sec x + \tan x )^{{9}/{2}}}\,\mathrm dx","How do I evaluate $$\int \frac{\sec^2 x}{(\sec x + \tan x )^{{9}/{2}}}\,\mathrm dx$$ I've started doing this problem by taking $u=\tan x$.","How do I evaluate $$\int \frac{\sec^2 x}{(\sec x + \tan x )^{{9}/{2}}}\,\mathrm dx$$ I've started doing this problem by taking $u=\tan x$.",,"['calculus', 'integration', 'indefinite-integrals']"
74,Definition of e,Definition of e,,"I'm very eager to know and understand the definition of $e$ . Textbooks define $e$ as follows $$ e = \lim_{p\to\infty} \left[1+\frac1{p}\right]^p \approx 2.71828 $$ Is there an ""easy to understand"" proof of this? I'm really looking for a derivation of this which is very intuitive and easy to comprehend. By the way I'm watching this video lecture .","I'm very eager to know and understand the definition of . Textbooks define as follows Is there an ""easy to understand"" proof of this? I'm really looking for a derivation of this which is very intuitive and easy to comprehend. By the way I'm watching this video lecture .",e e  e = \lim_{p\to\infty} \left[1+\frac1{p}\right]^p \approx 2.71828 ,"['calculus', 'intuition']"
75,"Why Limit of $0/x$ is $0$, if $x$ approaches $0$? [duplicate]","Why Limit of  is , if  approaches ? [duplicate]",0/x 0 x 0,"This question already has answers here : Limit of $0/x$ as x goes to 0 (3 answers) Closed 9 years ago . It might be a silly question, but to me it is not obvious why the following expression holds: $$ \lim\limits_{x\rightarrow 0}\frac{0}{x}=0 ? $$","This question already has answers here : Limit of $0/x$ as x goes to 0 (3 answers) Closed 9 years ago . It might be a silly question, but to me it is not obvious why the following expression holds: $$ \lim\limits_{x\rightarrow 0}\frac{0}{x}=0 ? $$",,"['calculus', 'limits']"
76,Non-Numerical proof of $e<\pi$,Non-Numerical proof of,e<\pi,"This is a ""coffee-time-style"" problem ( to have a taste of this style, you may like to browse the book https://www.amazon.com/Art-Mathematics-Coffee-Time-Memphis/dp/0521693950 ) interpreted from an anonymous problem once on the interactive whiteboard at my department, namely how to prove $e<\pi$ without much numerical computation like Taylor expansion or so. I once tried to use some ""intrinsic connection"" between $e$ and $\pi$ like $\sqrt{\pi}=\int_{-\infty}^{+\infty}e^{-x^2}\mathrm{d}x$ ( you can even find it in this movie http://www.imdb.com/title/tt4481414/ for testing children) and one possible way of reducing the problem is in the next paragraph. However it seems to be not that easy, any suggestion or new ideas? A stronger version of this question is : can we construct an explicit function $f(x)$ on $\mathbb{R}$ so that $f(x)\leq e^{-x^2}$ for all $x\in\mathbb{R}$ with $f(x)< e^{-x^2}$ on an open interval, and that $\int_{-\infty}^{\infty}f(x)\mathrm{d}x=\sqrt{e}$ ? We know from standard measure theory that there are $\beth_2$ such kind of Lebesgue-integrable functions, but this is the thing: how simple and explicit can what we're looking for be? Examples of very simple and explicit functions include but are not limited to piecewise elementary functions ( https://en.wikipedia.org/wiki/Elementary_function ). Unfortunately a function $f(x)$ defined piecewisely by  $$f(x)|_{(-1,1)}=e^{-|x|^r}\ \text{where}\ r\in\mathbb{Q}\cap(-\infty,2)\ \text{or}\ \mathbb{Q}\cap (-\infty,2]\ \text{respectively}$$  and $$f(x)|_{(-\infty,-1]\cup[1,\infty)}=e^{-|x|^s}\ \text{where}\ s\in\mathbb{Q}\cap [2,\infty)\ \text{or}\ \mathbb{Q}\cap(2,\infty)$$  would NOT satisfy $\int_{-\infty}^{\infty}f(x)\mathrm{d}x=\sqrt{e}$, if the values of the Gamma  function at rational points are linearly (or even algebraically) independent with $\sqrt{e}$ ( https://en.wikipedia.org/wiki/Particular_values_of_the_gamma_function ). The question is then how to move on from this first failure to search other explicit functions. I am aware that it is probably hard to ask such a question as solid as ""can we prove that CH is independent from ZFC""; after all, one can argue that any numerical inequality essentially also comes from some intrinsic inequality and hence not numerical at all. However one might try to ask in a relatively sloppy way: is there something that is at least seemingly simpler or less numerical, if not completely non-numerical ?","This is a ""coffee-time-style"" problem ( to have a taste of this style, you may like to browse the book https://www.amazon.com/Art-Mathematics-Coffee-Time-Memphis/dp/0521693950 ) interpreted from an anonymous problem once on the interactive whiteboard at my department, namely how to prove $e<\pi$ without much numerical computation like Taylor expansion or so. I once tried to use some ""intrinsic connection"" between $e$ and $\pi$ like $\sqrt{\pi}=\int_{-\infty}^{+\infty}e^{-x^2}\mathrm{d}x$ ( you can even find it in this movie http://www.imdb.com/title/tt4481414/ for testing children) and one possible way of reducing the problem is in the next paragraph. However it seems to be not that easy, any suggestion or new ideas? A stronger version of this question is : can we construct an explicit function $f(x)$ on $\mathbb{R}$ so that $f(x)\leq e^{-x^2}$ for all $x\in\mathbb{R}$ with $f(x)< e^{-x^2}$ on an open interval, and that $\int_{-\infty}^{\infty}f(x)\mathrm{d}x=\sqrt{e}$ ? We know from standard measure theory that there are $\beth_2$ such kind of Lebesgue-integrable functions, but this is the thing: how simple and explicit can what we're looking for be? Examples of very simple and explicit functions include but are not limited to piecewise elementary functions ( https://en.wikipedia.org/wiki/Elementary_function ). Unfortunately a function $f(x)$ defined piecewisely by  $$f(x)|_{(-1,1)}=e^{-|x|^r}\ \text{where}\ r\in\mathbb{Q}\cap(-\infty,2)\ \text{or}\ \mathbb{Q}\cap (-\infty,2]\ \text{respectively}$$  and $$f(x)|_{(-\infty,-1]\cup[1,\infty)}=e^{-|x|^s}\ \text{where}\ s\in\mathbb{Q}\cap [2,\infty)\ \text{or}\ \mathbb{Q}\cap(2,\infty)$$  would NOT satisfy $\int_{-\infty}^{\infty}f(x)\mathrm{d}x=\sqrt{e}$, if the values of the Gamma  function at rational points are linearly (or even algebraically) independent with $\sqrt{e}$ ( https://en.wikipedia.org/wiki/Particular_values_of_the_gamma_function ). The question is then how to move on from this first failure to search other explicit functions. I am aware that it is probably hard to ask such a question as solid as ""can we prove that CH is independent from ZFC""; after all, one can argue that any numerical inequality essentially also comes from some intrinsic inequality and hence not numerical at all. However one might try to ask in a relatively sloppy way: is there something that is at least seemingly simpler or less numerical, if not completely non-numerical ?",,"['calculus', 'geometry', 'inequality', 'pi', 'eulers-number-e']"
77,Information on Horseshoe Integration [closed],Information on Horseshoe Integration [closed],,"Closed. This question does not meet Mathematics Stack Exchange guidelines . It is not currently accepting answers. This question is not about mathematics, within the scope defined in the help center . Closed 6 years ago . Improve this question I was watching the following video (around 2:56), and he mentioned ""horseshoe mathematics/horseshoe integration"", though I was unable to find any information on what this is. If someone know's more about the topic could you perhaps provide some resources to learn more about it? Thanks","Closed. This question does not meet Mathematics Stack Exchange guidelines . It is not currently accepting answers. This question is not about mathematics, within the scope defined in the help center . Closed 6 years ago . Improve this question I was watching the following video (around 2:56), and he mentioned ""horseshoe mathematics/horseshoe integration"", though I was unable to find any information on what this is. If someone know's more about the topic could you perhaps provide some resources to learn more about it? Thanks",,"['calculus', 'integration', 'reference-request']"
78,Integral that evaluates to 42 [closed],Integral that evaluates to 42 [closed],,"Closed . This question needs to be more focused . It is not currently accepting answers. Want to improve this question? Update the question so it focuses on one problem only by editing this post . Closed 7 years ago . Improve this question I know this isn't a specific math question, but I need some help coming up with an integral that evaluates to 42 for a calc shirt. Obviously, I'm striving for something more complicated than $\int_{10}^{11} 4x dx$ or $\int_{0}^{42} 1 dx$. Try and stick to integral content that would be covered in the BC calc curriculum, because that is the class I'm trying to design the shirt for. Thanks!","Closed . This question needs to be more focused . It is not currently accepting answers. Want to improve this question? Update the question so it focuses on one problem only by editing this post . Closed 7 years ago . Improve this question I know this isn't a specific math question, but I need some help coming up with an integral that evaluates to 42 for a calc shirt. Obviously, I'm striving for something more complicated than $\int_{10}^{11} 4x dx$ or $\int_{0}^{42} 1 dx$. Try and stick to integral content that would be covered in the BC calc curriculum, because that is the class I'm trying to design the shirt for. Thanks!",,"['calculus', 'integration', 'definite-integrals', 'improper-integrals']"
79,integral of $x^2e^{-x^2}~dx$ from $-\infty$ to $+\infty$,integral of  from  to,x^2e^{-x^2}~dx -\infty +\infty,"I know that the $$\int^{+\infty}_{-\infty}e^{-x^2}~dx$$ is equal to $\sqrt\pi$ It's also very clear that  $$\int^{+\infty}_{-\infty}xe^{-x^2}~dx$$ is equal to 0; However, I cannot manage to calculate this really similar integral. $$\int^{+\infty}_{-\infty}x^2e^{-x^2}~dx$$ I know that the result is $\frac{\sqrt\pi}{2}$ but I don't know how to get to this result. I tried different substitution, but it doesn't seem to help.  Any idea? Thank you very much.","I know that the $$\int^{+\infty}_{-\infty}e^{-x^2}~dx$$ is equal to $\sqrt\pi$ It's also very clear that  $$\int^{+\infty}_{-\infty}xe^{-x^2}~dx$$ is equal to 0; However, I cannot manage to calculate this really similar integral. $$\int^{+\infty}_{-\infty}x^2e^{-x^2}~dx$$ I know that the result is $\frac{\sqrt\pi}{2}$ but I don't know how to get to this result. I tried different substitution, but it doesn't seem to help.  Any idea? Thank you very much.",,"['calculus', 'integration', 'definite-integrals', 'improper-integrals', 'exponentiation']"
80,How do I show that $\int_{-\infty}^\infty \frac{ \sin x \sin nx}{x^2} \ dx = \pi$?,How do I show that ?,\int_{-\infty}^\infty \frac{ \sin x \sin nx}{x^2} \ dx = \pi,Quick question. Could somebody please explain to me why it is that $$\int_{-\infty}^\infty \frac{ \sin x \sin nx}{x^2} \ dx = \pi$$ for every positive integer $n$? This integral showed up when I was computing a certain normalization constant. I was planning on just labeling it $I_n$ and moving on with my life but then Wolfram Alpha informed me it always equals $\pi$. Thanks!,Quick question. Could somebody please explain to me why it is that $$\int_{-\infty}^\infty \frac{ \sin x \sin nx}{x^2} \ dx = \pi$$ for every positive integer $n$? This integral showed up when I was computing a certain normalization constant. I was planning on just labeling it $I_n$ and moving on with my life but then Wolfram Alpha informed me it always equals $\pi$. Thanks!,,"['calculus', 'integration', 'definite-integrals']"
81,Can we always establish whether an infinite series converges or diverges?,Can we always establish whether an infinite series converges or diverges?,,"I'm currently working with infinite series for my calculus class, and I'm wondering whether we always (in theory) can establish whether a series is divergent or convergent? Of course, it might be computationally hard, but is there a class of series where we simply lack the tools to determine whether the series converges or diverges?","I'm currently working with infinite series for my calculus class, and I'm wondering whether we always (in theory) can establish whether a series is divergent or convergent? Of course, it might be computationally hard, but is there a class of series where we simply lack the tools to determine whether the series converges or diverges?",,"['calculus', 'sequences-and-series', 'convergence-divergence']"
82,Is this function necessarily a polynomial?,Is this function necessarily a polynomial?,,Given a function with two characteristics: $$f'(x_0)=f''(x_0)= \cdots =f^{(k-1)}(x_0)=0$$ and $$f^{(k)}(x_0)\ne0$$ for some $k\ge2$ Does this function necessarily have to be a polynomial? Why / why not?,Given a function with two characteristics: $$f'(x_0)=f''(x_0)= \cdots =f^{(k-1)}(x_0)=0$$ and $$f^{(k)}(x_0)\ne0$$ for some $k\ge2$ Does this function necessarily have to be a polynomial? Why / why not?,,"['calculus', 'polynomials']"
83,Solve $\int \limits_{0}^{\infty} \frac{\cos(x)}{\cosh(x)} dx$ without complex integration.,Solve  without complex integration.,\int \limits_{0}^{\infty} \frac{\cos(x)}{\cosh(x)} dx,"Solve $$\int \limits_{0}^{\infty} \frac{\cos(x)}{\cosh(x)} dx$$ without complex integration. This integral can be very easily solved with contour integration, but how can you solve it without taking a tour in the complex plane?","Solve $$\int \limits_{0}^{\infty} \frac{\cos(x)}{\cosh(x)} dx$$ without complex integration. This integral can be very easily solved with contour integration, but how can you solve it without taking a tour in the complex plane?",,"['calculus', 'integration', 'definite-integrals', 'improper-integrals']"
84,"Is there an analytic function satisfying $\,\,f\big(\!\frac 1 n\!\big)=\frac 1 {\sqrt{n}}$ for all $n\in\mathbb N$?",Is there an analytic function satisfying  for all ?,"\,\,f\big(\!\frac 1 n\!\big)=\frac 1 {\sqrt{n}} n\in\mathbb N","Is there a function which is analytic in an open neighbourhood of $z=0$ and satisfies $$ f\left(\!\dfrac 1 n\!\right)=\dfrac 1  {\sqrt{n}}, $$  for all natural numbers $n$? I guess this problem requires the Identity Theorem but I can't use it the natural way because  the function $$f(z)=\sqrt z,$$ isn't analytic in $z=0$. I would like a hint.","Is there a function which is analytic in an open neighbourhood of $z=0$ and satisfies $$ f\left(\!\dfrac 1 n\!\right)=\dfrac 1  {\sqrt{n}}, $$  for all natural numbers $n$? I guess this problem requires the Identity Theorem but I can't use it the natural way because  the function $$f(z)=\sqrt z,$$ isn't analytic in $z=0$. I would like a hint.",,"['calculus', 'sequences-and-series', 'complex-analysis', 'analysis', 'limits']"
85,Convergence of a Harmonic Continued Fraction,Convergence of a Harmonic Continued Fraction,,"Does this continued fraction converge? $$\cfrac { 1 }{ 1+\cfrac { 1 }{ 2+\cfrac { 1 }{ 3+\cfrac { 1 }{ 4+\ddots  }  }  }  } $$ ($[0;1,2,3,4, \dots]$) I tried approximating a few values but I couldn't make out whether it converges or diverges. Can anyone provide a proof whether this converges or diverges. If it converges, please add what it converges to.","Does this continued fraction converge? $$\cfrac { 1 }{ 1+\cfrac { 1 }{ 2+\cfrac { 1 }{ 3+\cfrac { 1 }{ 4+\ddots  }  }  }  } $$ ($[0;1,2,3,4, \dots]$) I tried approximating a few values but I couldn't make out whether it converges or diverges. Can anyone provide a proof whether this converges or diverges. If it converges, please add what it converges to.",,"['calculus', 'sequences-and-series', 'number-theory', 'continued-fractions']"
86,How to prove $\lim_{n \to \infty} \sqrt{n}(\sqrt[n]{n} - 1) = 0$?,How to prove ?,\lim_{n \to \infty} \sqrt{n}(\sqrt[n]{n} - 1) = 0,"I want to show that $$\lim_{n \to \infty} \sqrt{n}(\sqrt[n]{n}-1) = 0$$ and my assistant teacher gave me the hint to find a proper estimate for $\sqrt[n]{n}-1$ in order to do this. I know how one shows that $\lim_{n \to \infty} \sqrt[n]{n} = 1$, to do this we can write $\sqrt[n]{n} = 1+x_n$, raise both sides to the n-th power and then use the binomial theorem (or to be more specific: the term to the second power). However, I don't see how this or any other trivial term (i.e. the first or the n-th) could be used here. What estimate am I supposed to find or is there even a simpler way to show this limit? Thanks for any answers in advance.","I want to show that $$\lim_{n \to \infty} \sqrt{n}(\sqrt[n]{n}-1) = 0$$ and my assistant teacher gave me the hint to find a proper estimate for $\sqrt[n]{n}-1$ in order to do this. I know how one shows that $\lim_{n \to \infty} \sqrt[n]{n} = 1$, to do this we can write $\sqrt[n]{n} = 1+x_n$, raise both sides to the n-th power and then use the binomial theorem (or to be more specific: the term to the second power). However, I don't see how this or any other trivial term (i.e. the first or the n-th) could be used here. What estimate am I supposed to find or is there even a simpler way to show this limit? Thanks for any answers in advance.",,"['calculus', 'limits', 'radicals']"
87,It is possible to apply a derivative to both sides of a given equation and maintain the equivalence of both sides? [duplicate],It is possible to apply a derivative to both sides of a given equation and maintain the equivalence of both sides? [duplicate],,"This question already has answers here : Can we differentiate equations without changing the solutions? (3 answers) Closed 7 years ago . I have an equation with this shape, where $k,t_1,r \in \Bbb N$: $$2^{(x+1)^2}=k+t_1(x^2+r)$$ And I noticed that I can find $t_1$ in terms of $x$ as follows: $$ \frac{{\rm d} }{{\rm d}x} 2^{(x+1)^2} = \frac{{\rm d} }{{\rm d}x} \left( k+t_1(x^2+r) \right)$$ And then I would continue as follows: $$ 2(x+1)\cdot2^{(x+1)^2}\cdot\ln(2) = t_12x$$ $$ (x+1)\cdot2^{(x+1)^2}\cdot\ln(2) = t_1x$$ So finally: $$ t_1=\frac{(x+1)\cdot2^{(x+1)^2}\cdot\ln(2)}{x}$$ Is it possible to use a derivative in such case? I think that if both functions $2^{(x+1)^2}$ and $t_1(x^2+r)$ are equivalent and the derivative exists it might be possible, but I am not very sure about the validity of that step. Thank you!","This question already has answers here : Can we differentiate equations without changing the solutions? (3 answers) Closed 7 years ago . I have an equation with this shape, where $k,t_1,r \in \Bbb N$: $$2^{(x+1)^2}=k+t_1(x^2+r)$$ And I noticed that I can find $t_1$ in terms of $x$ as follows: $$ \frac{{\rm d} }{{\rm d}x} 2^{(x+1)^2} = \frac{{\rm d} }{{\rm d}x} \left( k+t_1(x^2+r) \right)$$ And then I would continue as follows: $$ 2(x+1)\cdot2^{(x+1)^2}\cdot\ln(2) = t_12x$$ $$ (x+1)\cdot2^{(x+1)^2}\cdot\ln(2) = t_1x$$ So finally: $$ t_1=\frac{(x+1)\cdot2^{(x+1)^2}\cdot\ln(2)}{x}$$ Is it possible to use a derivative in such case? I think that if both functions $2^{(x+1)^2}$ and $t_1(x^2+r)$ are equivalent and the derivative exists it might be possible, but I am not very sure about the validity of that step. Thank you!",,"['calculus', 'derivatives']"
88,How to evaluate $\lim\limits_{n\to \infty}\prod\limits_{r=2}^{n}\cos\left(\frac{\pi}{2^{r}}\right)$,How to evaluate,\lim\limits_{n\to \infty}\prod\limits_{r=2}^{n}\cos\left(\frac{\pi}{2^{r}}\right),How do I evaluate this limit ? $$\lim_{n\to \infty}\cos\left(\frac{\pi}{2^{2}}\right)\cos\left(\frac{\pi}{2^{3}}\right)\cdots\cos\left(\frac{\pi}{2^{n}}\right)$$ I assumed it is using this formaula $\displaystyle \cos(A)=\sqrt{\frac{1+\cos(2A)}{2}}$ But I am stuck,How do I evaluate this limit ? $$\lim_{n\to \infty}\cos\left(\frac{\pi}{2^{2}}\right)\cos\left(\frac{\pi}{2^{3}}\right)\cdots\cos\left(\frac{\pi}{2^{n}}\right)$$ I assumed it is using this formaula $\displaystyle \cos(A)=\sqrt{\frac{1+\cos(2A)}{2}}$ But I am stuck,,"['calculus', 'sequences-and-series', 'limits', 'trigonometry', 'infinite-product']"
89,How to evaluate $\lim_{n\to\infty}\sqrt[n]{\frac{1\cdot3\cdot5\cdot\ldots\cdot(2n-1)}{2\cdot4\cdot6\cdot\ldots\cdot2n}}$,How to evaluate,\lim_{n\to\infty}\sqrt[n]{\frac{1\cdot3\cdot5\cdot\ldots\cdot(2n-1)}{2\cdot4\cdot6\cdot\ldots\cdot2n}},Im tempted to say that the limit of this sequence is 1 because infinite root of infinite number is close to 1 but maybe Im mising here something? What will be inside the root? This is the sequence: $$\lim_{n\to\infty}\sqrt[n]{\frac{1\cdot3\cdot5\cdot\ldots\cdot(2n-1)}{2\cdot4\cdot6\cdot\ldots\cdot2n}}$$,Im tempted to say that the limit of this sequence is 1 because infinite root of infinite number is close to 1 but maybe Im mising here something? What will be inside the root? This is the sequence: $$\lim_{n\to\infty}\sqrt[n]{\frac{1\cdot3\cdot5\cdot\ldots\cdot(2n-1)}{2\cdot4\cdot6\cdot\ldots\cdot2n}}$$,,"['calculus', 'limits', 'factorial', 'radicals']"
90,Integration with $\ln(x)$ in the denominator,Integration with  in the denominator,\ln(x),Find $$\displaystyle\int_1^\infty\frac{(x^2-1)(x^4-1)(x^6-1)}{\ln(x)(x^{14}-1)} dx$$ I tried simplifying the terms without logarithm $x^2-1=(x-1)(x+1)\\x^{14}-1=(x^7-1)(x^7+1)$ to see if any substitution is possible but since the logarithm is in the denominator it is getting hard and I am not sure if there is any substitution that would work. I am looking for hints to get started,Find I tried simplifying the terms without logarithm to see if any substitution is possible but since the logarithm is in the denominator it is getting hard and I am not sure if there is any substitution that would work. I am looking for hints to get started,\displaystyle\int_1^\infty\frac{(x^2-1)(x^4-1)(x^6-1)}{\ln(x)(x^{14}-1)} dx x^2-1=(x-1)(x+1)\\x^{14}-1=(x^7-1)(x^7+1),"['calculus', 'integration', 'definite-integrals']"
91,$\lim_{n \to \infty} (\frac{(n+1)(n+2)\dots(3n)}{n^{2n}})^{\frac{1}{n}}$ is equal to :,is equal to :,\lim_{n \to \infty} (\frac{(n+1)(n+2)\dots(3n)}{n^{2n}})^{\frac{1}{n}},$\lim_{n \to \infty} (\frac{(n+1)(n+2)\dots(3n)}{n^{2n}})^{\frac{1}{n}}$ is equal to : $\frac{9}{e^2}$ $3 \log3−2$ $\frac{18}{e^4}$ $\frac{27}{e^2}$ My attempt : $\lim_{n \to \infty} (\frac{(n+1)(n+2)\dots(3n)}{n^{2n}})^{\frac{1}{n}}$ $=\lim_{n \to \infty} (\frac{(n+1)(n+2)\dots(n+2n)}{n^{2n}})^{\frac{1}{n}}$ $=\lim_{n \to \infty} (\frac{{n^{2n}}\{(1+1/n)(1+2/n)\dots(1+2n/n)\}}{n^{2n}})^{\frac{1}{n}}$ $=\lim_{n \to \infty} (\frac{{n^{2n}}\{(1+1/n)(1+2/n)\dots(1+2)\}}{n^{2n}})^{\frac{1}{n}}$ $=\lim_{n \to \infty} (\frac{{n^{2n}}\{(1+1/n)(1+2/n)\dots(3)\}}{n^{2n}})^{\frac{1}{n}}$ $=\lim_{n \to \infty} (\{(1+1/n)(1+2/n)\dots(3)\})^{\frac{1}{n}}$ I'm stuck here. Can you please explain?,is equal to : My attempt : I'm stuck here. Can you please explain?,\lim_{n \to \infty} (\frac{(n+1)(n+2)\dots(3n)}{n^{2n}})^{\frac{1}{n}} \frac{9}{e^2} 3 \log3−2 \frac{18}{e^4} \frac{27}{e^2} \lim_{n \to \infty} (\frac{(n+1)(n+2)\dots(3n)}{n^{2n}})^{\frac{1}{n}} =\lim_{n \to \infty} (\frac{(n+1)(n+2)\dots(n+2n)}{n^{2n}})^{\frac{1}{n}} =\lim_{n \to \infty} (\frac{{n^{2n}}\{(1+1/n)(1+2/n)\dots(1+2n/n)\}}{n^{2n}})^{\frac{1}{n}} =\lim_{n \to \infty} (\frac{{n^{2n}}\{(1+1/n)(1+2/n)\dots(1+2)\}}{n^{2n}})^{\frac{1}{n}} =\lim_{n \to \infty} (\frac{{n^{2n}}\{(1+1/n)(1+2/n)\dots(3)\}}{n^{2n}})^{\frac{1}{n}} =\lim_{n \to \infty} (\{(1+1/n)(1+2/n)\dots(3)\})^{\frac{1}{n}},"['calculus', 'limits']"
92,Intuition behind arc length formula,Intuition behind arc length formula,,"I understand the arc length formula is derived from adding the distances between a series of points on the curve, and using the mean value theorem to get: $ L = \int_a^b \sqrt{ 1 + (f'(x))^2 } dx $ But is there an intuition here I'm missing?  Something about taking the integral of the derivative seems like it should mean something..","I understand the arc length formula is derived from adding the distances between a series of points on the curve, and using the mean value theorem to get: $ L = \int_a^b \sqrt{ 1 + (f'(x))^2 } dx $ But is there an intuition here I'm missing?  Something about taking the integral of the derivative seems like it should mean something..",,"['calculus', 'functions', 'intuition']"
93,"Why is this famous proof of the chain rule called ""technically incorrect"" in this pdf? [duplicate]","Why is this famous proof of the chain rule called ""technically incorrect"" in this pdf? [duplicate]",,"This question already has an answer here : Why does this proof of the chain rule not work? (1 answer) Closed 7 years ago . So I was looking through various proofs of the chain rule...and I came across this paper. The first proof given is complete and quite well-explained. But another simplistic proof is given in the end...which is mentioned as ""technically incorrect"". Can anyone tell me why? Here is the incorrect proof in question: $$\begin{aligned} (f \circ g)'(x) &= \lim_{h \to 0}\frac{f(g(x+h)) - f(g(x))}{h} \\ \implies (f \circ g)'(x) \cdot \left(\frac{1}{g'(x)}\right) &= \lim_{h \to 0}\left(\frac{f(g(x+h)) - f(g(x))}{h}\right)\cdot\left(\frac{h}{g(x+h)-g(x)}\right)\\ &= \lim_{h \to 0}\left(\frac{f(g(x+h)) - f(g(x))}{g(x+h)-g(x)}\right) \\ &= f'(g(x)) \\ \end{aligned}$$","This question already has an answer here : Why does this proof of the chain rule not work? (1 answer) Closed 7 years ago . So I was looking through various proofs of the chain rule...and I came across this paper. The first proof given is complete and quite well-explained. But another simplistic proof is given in the end...which is mentioned as ""technically incorrect"". Can anyone tell me why? Here is the incorrect proof in question: $$\begin{aligned} (f \circ g)'(x) &= \lim_{h \to 0}\frac{f(g(x+h)) - f(g(x))}{h} \\ \implies (f \circ g)'(x) \cdot \left(\frac{1}{g'(x)}\right) &= \lim_{h \to 0}\left(\frac{f(g(x+h)) - f(g(x))}{h}\right)\cdot\left(\frac{h}{g(x+h)-g(x)}\right)\\ &= \lim_{h \to 0}\left(\frac{f(g(x+h)) - f(g(x))}{g(x+h)-g(x)}\right) \\ &= f'(g(x)) \\ \end{aligned}$$",,"['calculus', 'functions', 'derivatives', 'chain-rule']"
94,Inverse Trigonometric Integrals,Inverse Trigonometric Integrals,,"How to calculate the value of the integrals $$\int_0^1\left(\frac{\arctan x}{x}\right)^2\,dx,$$ $$\int_0^1\left(\frac{\arctan x}{x}\right)^3\,dx $$ and $$\int_0^1\frac{\arctan^2 x\ln x}{x}\,dx?$$","How to calculate the value of the integrals $$\int_0^1\left(\frac{\arctan x}{x}\right)^2\,dx,$$ $$\int_0^1\left(\frac{\arctan x}{x}\right)^3\,dx $$ and $$\int_0^1\frac{\arctan^2 x\ln x}{x}\,dx?$$",,"['calculus', 'integration', 'definite-integrals', 'improper-integrals', 'closed-form']"
95,Difference between approaching and being exactly a number,Difference between approaching and being exactly a number,,"When we take a limit, we say that the value is never equals that number, but approaches it, like in $$\lim_{n\to\infty}\frac{1}{n} = 0.$$ It never reaches $0$, but becomes closer and closer to $0$. In this case, isn't it wrong to say things like: $$2 = 1 + \frac{1}{2} + \frac{1}{4}+\frac{1}{8} + \frac{1}{16}+\cdots $$ Or that a derivative of $\sin(x)$ is $\cos(x)$ since the limit of $$\frac{\sin(x+\Delta x)-\sin(x)}{\Delta x}$$ with $\Delta x$ approaching $0$ is never equal to $\cos(x)$ but infinitely closer ? Is there a good article about this that I can read and understand it better?","When we take a limit, we say that the value is never equals that number, but approaches it, like in $$\lim_{n\to\infty}\frac{1}{n} = 0.$$ It never reaches $0$, but becomes closer and closer to $0$. In this case, isn't it wrong to say things like: $$2 = 1 + \frac{1}{2} + \frac{1}{4}+\frac{1}{8} + \frac{1}{16}+\cdots $$ Or that a derivative of $\sin(x)$ is $\cos(x)$ since the limit of $$\frac{\sin(x+\Delta x)-\sin(x)}{\Delta x}$$ with $\Delta x$ approaching $0$ is never equal to $\cos(x)$ but infinitely closer ? Is there a good article about this that I can read and understand it better?",,"['calculus', 'sequences-and-series', 'limits', 'infinity']"
96,Finding $\lim\limits_{n \to \infty }n\int_{0}^{1}\frac{x^{n}}{1+x+x^{n}}dx$,Finding,\lim\limits_{n \to \infty }n\int_{0}^{1}\frac{x^{n}}{1+x+x^{n}}dx,"$$\lim_{n \to \infty }n\int_{0}^{1}\frac{x^{n}}{1+x+x^{n}}dx$$ I factorize $x^n$ then I tried with $u=1+x^{1-n}$ but I didn't get too far. Also I tried to get something from $0<x<1$ . I know that if $x$ is from $(0,1)$ then $x^n$ tends to $0$ as $n \to \infty$ .",I factorize then I tried with but I didn't get too far. Also I tried to get something from . I know that if is from then tends to as .,"\lim_{n \to \infty }n\int_{0}^{1}\frac{x^{n}}{1+x+x^{n}}dx x^n u=1+x^{1-n} 0<x<1 x (0,1) x^n 0 n \to \infty","['calculus', 'limits']"
97,"How prove this integral $ \int\limits_0^1 \int\limits_0^1 \ln\Gamma(x+y^3)\,dx\,dy =-\frac 7 {16}+\frac{1}{2}\ln 2\pi$",How prove this integral," \int\limits_0^1 \int\limits_0^1 \ln\Gamma(x+y^3)\,dx\,dy =-\frac 7 {16}+\frac{1}{2}\ln 2\pi","show that $$ I=\int_0^1 \int_0^1 \ln\Gamma(x+y^3) \, dx \, dy =-\frac 7 {16} + \frac 1 2 \ln 2\pi$$ where $$\Gamma(a)=\int_0^\infty x^{a-1}e^{-x} \, dx$$ then $$I=\int_0^1 \int_0^1 \ln\left(\int_0^\infty t^{x+y^3-1}e^{-t} \, dt\right) \, dx \, dy$$ Then I can't works,Thank you","show that $$ I=\int_0^1 \int_0^1 \ln\Gamma(x+y^3) \, dx \, dy =-\frac 7 {16} + \frac 1 2 \ln 2\pi$$ where $$\Gamma(a)=\int_0^\infty x^{a-1}e^{-x} \, dx$$ then $$I=\int_0^1 \int_0^1 \ln\left(\int_0^\infty t^{x+y^3-1}e^{-t} \, dt\right) \, dx \, dy$$ Then I can't works,Thank you",,"['calculus', 'integration', 'multivariable-calculus', 'definite-integrals', 'gamma-function']"
98,Use Fourier series for computing $\sum_{n=1}^{\infty}\frac{1}{(2n-1)^2}$,Use Fourier series for computing,\sum_{n=1}^{\infty}\frac{1}{(2n-1)^2},"I need to compute Fourier series for the following function: $f(x)=\frac{-\pi}{4} $ for $-\pi \leq x <0$, and $\frac{\pi}{4} $ for $ 0 \leq x \leq \pi$, and then to use it and compute $\sum_{n=1}^{\infty}\frac{1}{(2n-1)^2}$ I tried to use Parseval equality: $$\widehat{f(n)}=\frac{1}{2\pi}\int_{-\pi}^{\pi}f(x)e^{-inx}=\frac{1}{4in}-\frac{(-1)^n}{4in}, \sum_{-\infty}^{\infty}|\widehat{f(n)}|^2=\frac{1}{2\pi}\int_{-\pi}^{\pi}|f(x)|^2.$$ $$\sum_{-\infty}^{\infty}|\frac{1}{4in}-\frac{(-1)^n}{4in}|=\frac{1}{2\pi}\int_{-\pi}^{\pi}f(x)=\frac{\pi^2}{16}.$$ Does someone see how can I compute form that the requsted sum? Thanks!","I need to compute Fourier series for the following function: $f(x)=\frac{-\pi}{4} $ for $-\pi \leq x <0$, and $\frac{\pi}{4} $ for $ 0 \leq x \leq \pi$, and then to use it and compute $\sum_{n=1}^{\infty}\frac{1}{(2n-1)^2}$ I tried to use Parseval equality: $$\widehat{f(n)}=\frac{1}{2\pi}\int_{-\pi}^{\pi}f(x)e^{-inx}=\frac{1}{4in}-\frac{(-1)^n}{4in}, \sum_{-\infty}^{\infty}|\widehat{f(n)}|^2=\frac{1}{2\pi}\int_{-\pi}^{\pi}|f(x)|^2.$$ $$\sum_{-\infty}^{\infty}|\frac{1}{4in}-\frac{(-1)^n}{4in}|=\frac{1}{2\pi}\int_{-\pi}^{\pi}f(x)=\frac{\pi^2}{16}.$$ Does someone see how can I compute form that the requsted sum? Thanks!",,"['calculus', 'sequences-and-series', 'fourier-series']"
99,convex function in open interval is continuous [duplicate],convex function in open interval is continuous [duplicate],,This question already has answers here : Is every convex function on an open interval continuous? [duplicate] (2 answers) Closed 9 years ago . How can I prove that a convex function ƒ defined on some open interval C is continuous on C? Thank you.,This question already has answers here : Is every convex function on an open interval continuous? [duplicate] (2 answers) Closed 9 years ago . How can I prove that a convex function ƒ defined on some open interval C is continuous on C? Thank you.,,[]
