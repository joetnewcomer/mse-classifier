,title_raw,title_text,title_latex,body_raw,body_text,body_latex,tags
0,compute $P(X\le 2 \textrm{ or } 3<X<8)$ For exponentially distributed x,compute  For exponentially distributed x,P(X\le 2 \textrm{ or } 3<X<8),"Suppose X is exponentially distributed with $\lambda=2$ . I want to compute $P(X\le 2 \textrm{ or } 3<X<8)$ . The probability density function of $x$ is $f(x)=2e^{-2x}$ . Then I will find its distribution function as $F(x)=1-e^{-2x}$ . I don’t know exactly, but I think that $P(X\le 2 \textrm{ or } 3<X<8)$ can be calculated as follows: $$P(X\le 2 \textrm{ or } 3<X<8) = P(X\le 2)+ P(3<X<8)-P(X\le 2, 3<X<8)$$ $$P(X\le 2)=F(2)= 1-e^{-4}$$ $$P(3<X<8)=F(8)-F(3)= e^{-16}-e^{-6}$$ But what is $P(X\le 2, 3<X<8)$ ? Or what is the correct way to solve this question?","Suppose X is exponentially distributed with . I want to compute . The probability density function of is . Then I will find its distribution function as . I don’t know exactly, but I think that can be calculated as follows: But what is ? Or what is the correct way to solve this question?","\lambda=2 P(X\le 2 \textrm{ or } 3<X<8) x f(x)=2e^{-2x} F(x)=1-e^{-2x} P(X\le 2 \textrm{ or } 3<X<8) P(X\le 2 \textrm{ or } 3<X<8) = P(X\le 2)+ P(3<X<8)-P(X\le 2, 3<X<8) P(X\le 2)=F(2)= 1-e^{-4} P(3<X<8)=F(8)-F(3)= e^{-16}-e^{-6} P(X\le 2, 3<X<8)","['statistics', 'probability-distributions', 'exponential-function', 'self-learning']"
1,Finding the mle of a log normal distribution,Finding the mle of a log normal distribution,,"So let $X1,X2,..,XN$ be an independent sample from log normal distribution with the pdf $f(x,\theta)=(x^2 \sigma^2*2\pi)^{(-1/2)}e^{-(log(x)-\theta)^2/{2\sigma^2}}$ and we have $\sigma^2=1$ and $\theta$ uknown So I did the following we have the $L(\theta,x)=(x_1^2\sigma^22\pi)^{-1/2}e^{-(log(x_1)-\theta)^2/{2\sigma^2}}*(x_2^2\sigma^22\pi)^{-1/2}e^{-(log(x_2)-\theta)^2/{2\sigma^2}}*...*(x_n^2\sigma^22\pi)^{-1/2}e^{-(log(x_n)-\theta)^2/{2\sigma^2}}$ and I get the following $L(\theta,x)=(2\pi)^{(-n/2)}*(\sigma^2)^{-n/2}*(1)/(x_1*x_2*..*x_n)e^{(-1/2\sigma^2)\sum(log(x_i)-\theta)^2}$ So I take the $log(L(\theta,x)$ and I get $(-n/2)log(2\pi)+log(1/x^n)-(1/2\sigma^2)\sum(log(x_i)-\theta)^2$ So now to find the mle of $\theta$ I do $d/d(\theta)log(L(\theta,x))=0$ take the derivative and I get $=log(x_1)-\theta+log(x_2)-\theta+...+log(x_n)-\theta$ so I get $log(x_1)+log(x_2)+..+log(x_n)-\theta*n$ so the mle $\theta[hat]$ of $\theta$ is $\theta[hat]=((log(x_1)+log(x_2)+..+log(x_n))/n$ I am not sure if this is right.",So let be an independent sample from log normal distribution with the pdf and we have and uknown So I did the following we have the and I get the following So I take the and I get So now to find the mle of I do take the derivative and I get so I get so the mle of is I am not sure if this is right.,"X1,X2,..,XN f(x,\theta)=(x^2 \sigma^2*2\pi)^{(-1/2)}e^{-(log(x)-\theta)^2/{2\sigma^2}} \sigma^2=1 \theta L(\theta,x)=(x_1^2\sigma^22\pi)^{-1/2}e^{-(log(x_1)-\theta)^2/{2\sigma^2}}*(x_2^2\sigma^22\pi)^{-1/2}e^{-(log(x_2)-\theta)^2/{2\sigma^2}}*...*(x_n^2\sigma^22\pi)^{-1/2}e^{-(log(x_n)-\theta)^2/{2\sigma^2}} L(\theta,x)=(2\pi)^{(-n/2)}*(\sigma^2)^{-n/2}*(1)/(x_1*x_2*..*x_n)e^{(-1/2\sigma^2)\sum(log(x_i)-\theta)^2} log(L(\theta,x) (-n/2)log(2\pi)+log(1/x^n)-(1/2\sigma^2)\sum(log(x_i)-\theta)^2 \theta d/d(\theta)log(L(\theta,x))=0 =log(x_1)-\theta+log(x_2)-\theta+...+log(x_n)-\theta log(x_1)+log(x_2)+..+log(x_n)-\theta*n \theta[hat] \theta \theta[hat]=((log(x_1)+log(x_2)+..+log(x_n))/n",['statistics']
2,Standard deviation (sample) / average leads to constant when all values but one are zero,Standard deviation (sample) / average leads to constant when all values but one are zero,,"I was doing some database work for a client, and I came across an oddity that I hoped someone could explain. Take the sequence 0,0,0...,0,n (where n is any non-zero number), calculate the standard deviation (sample), and divide that by the average, and you always get the square root of the total number of samples. e.g.(for 5 samples) 0,0,0,0,100 = 2.236067977 0,0,0,0,-0.21 = -2.236067977 (for 12 samples) 0,0,0,0,0,0,0,0,0,0,0,22 = 3.464101615 0,0,0,0,0,0,0,0,0,0,0,-0.1 = -3.464101615 Is there an obvious explanation for this?","I was doing some database work for a client, and I came across an oddity that I hoped someone could explain. Take the sequence 0,0,0...,0,n (where n is any non-zero number), calculate the standard deviation (sample), and divide that by the average, and you always get the square root of the total number of samples. e.g.(for 5 samples) 0,0,0,0,100 = 2.236067977 0,0,0,0,-0.21 = -2.236067977 (for 12 samples) 0,0,0,0,0,0,0,0,0,0,0,22 = 3.464101615 0,0,0,0,0,0,0,0,0,0,0,-0.1 = -3.464101615 Is there an obvious explanation for this?",,"['statistics', 'constants']"
3,consistency of MLE estimator example,consistency of MLE estimator example,,"Given the density function $f(x;\theta)=\theta x^{\theta -1}$ with $\theta \gt 0$ and $x \in (0,1)$ , once found the MLE estimator $\hat{\theta}=\frac{-n}{\sum{\log{X_i}}}$ i want to show the consistency of such an estimator. I was suggested to apply the strong law of large numbers, although i am not sure how. Should I first obtain the expected value of the random variable $\log(X_i)$ and then apply the SLLN on them or is there another way?","Given the density function with and , once found the MLE estimator i want to show the consistency of such an estimator. I was suggested to apply the strong law of large numbers, although i am not sure how. Should I first obtain the expected value of the random variable and then apply the SLLN on them or is there another way?","f(x;\theta)=\theta x^{\theta -1} \theta \gt 0 x \in (0,1) \hat{\theta}=\frac{-n}{\sum{\log{X_i}}} \log(X_i)",['statistics']
4,Standardized residuals,Standardized residuals,,"I am having a hard time understanding the concept of standardizing residuals and how the variance of a residual is decomposed. In a linear model, we defined residuals as: $e = y - \hat{y} = (I-H)y$ where H is the hat matrix $X(X^TX)^{-1}X^T$ and we defined standardized residuals as: $r_i = \frac{e_i}{s\sqrt{1-h_{ii}}}$ , $i = 1,...,n$ where $s^2$ is the usual estimate of $\sigma^2$ , $var(e_i) = \sigma^2h_{ii}$ , and $h_{ii}$ is the diagonal entry of H at the $i^{th}$ row and $i^{th}$ column However, I am not sure why $r_i$ and $e_i$ are functions of $h_{ii}$ rather than the whole row $h_i$ . Basically I am confused about what $h_{ii}$ stands for as opposed to row $h_{i}$ .","I am having a hard time understanding the concept of standardizing residuals and how the variance of a residual is decomposed. In a linear model, we defined residuals as: where H is the hat matrix and we defined standardized residuals as: , where is the usual estimate of , , and is the diagonal entry of H at the row and column However, I am not sure why and are functions of rather than the whole row . Basically I am confused about what stands for as opposed to row .","e = y - \hat{y} = (I-H)y X(X^TX)^{-1}X^T r_i = \frac{e_i}{s\sqrt{1-h_{ii}}} i = 1,...,n s^2 \sigma^2 var(e_i) = \sigma^2h_{ii} h_{ii} i^{th} i^{th} r_i e_i h_{ii} h_i h_{ii} h_{i}","['linear-algebra', 'statistics', 'regression', 'variance', 'standard-error']"
5,N needed for probability > .001 of two people being a match at six genetic markers?,N needed for probability > .001 of two people being a match at six genetic markers?,,"If for each marker there is a 1/9 chance that any two people are a match, how large would the sample need to be for the probability to exceed .001 that two people are a match at six markers?","If for each marker there is a 1/9 chance that any two people are a match, how large would the sample need to be for the probability to exceed .001 that two people are a match at six markers?",,"['probability', 'statistics']"
6,Find the value of p such that X wins with probability of 1/2,Find the value of p such that X wins with probability of 1/2,,"Consider a badminton game where there is no deuce and the first player to score four points wins the game. Suppose player X wins each point independently with probability p and Y wins with probability q=1-p Suppose that player Y won the first two points, so now Y needs 2 points to win and X still needs 4 points. Find approximately the value of p such that X wins with probability 1/2 . I have so far found the equation of X wins to be $p^4+4[p^4(1-p)]=1/2$ as X needs 4 points to win so $p^4$ with Y having 4 possible point sequences to gain an additional point. But I'm now stuck with the previous equation of $5p^4-4p^5=1/2$ I'm not sure what to do next. Is my equation wrong? If not, then can someone advise me on what to do next?","Consider a badminton game where there is no deuce and the first player to score four points wins the game. Suppose player X wins each point independently with probability p and Y wins with probability q=1-p Suppose that player Y won the first two points, so now Y needs 2 points to win and X still needs 4 points. Find approximately the value of p such that X wins with probability 1/2 . I have so far found the equation of X wins to be as X needs 4 points to win so with Y having 4 possible point sequences to gain an additional point. But I'm now stuck with the previous equation of I'm not sure what to do next. Is my equation wrong? If not, then can someone advise me on what to do next?",p^4+4[p^4(1-p)]=1/2 p^4 5p^4-4p^5=1/2,"['probability', 'statistics']"
7,"What does ""random sampling"" (usually) mean?","What does ""random sampling"" (usually) mean?",,"I have two definitions. (1) A sampling procedure is random if all individuals have the same chance of being sampled. (2) A sampling procedure is random if all samples have the same probability of occurring. Consider this example. I have a population of 1000 individuals, 500 men and 500 women, and I take a sample of size 100 with the constraint that 50 of the selected individuals must be men and 50 women. This would be considered random by (1) (there is nothing that makes an individual more likely to be selected), but not by (2) (some combinations, like 2 men and 98 women, aren't possible). EDIT: GENIVI-LEARNER has pointed out that my example doesn't fit either definition, because chances of a men being selected decrease after a men is selected. This is true only if sampling is seen as a repeated selection of one individual, and we require that all these events obey the same probability model. But if this is true, then sampling without replacement can never be random. That is, if I take out the selected item from the population after its selection, I won't have random sampling. However I have seen the adjective random applied also to this situation. So I am more inclined to think that this isn't the case.","I have two definitions. (1) A sampling procedure is random if all individuals have the same chance of being sampled. (2) A sampling procedure is random if all samples have the same probability of occurring. Consider this example. I have a population of 1000 individuals, 500 men and 500 women, and I take a sample of size 100 with the constraint that 50 of the selected individuals must be men and 50 women. This would be considered random by (1) (there is nothing that makes an individual more likely to be selected), but not by (2) (some combinations, like 2 men and 98 women, aren't possible). EDIT: GENIVI-LEARNER has pointed out that my example doesn't fit either definition, because chances of a men being selected decrease after a men is selected. This is true only if sampling is seen as a repeated selection of one individual, and we require that all these events obey the same probability model. But if this is true, then sampling without replacement can never be random. That is, if I take out the selected item from the population after its selection, I won't have random sampling. However I have seen the adjective random applied also to this situation. So I am more inclined to think that this isn't the case.",,"['statistics', 'terminology']"
8,How to measure points scattering in a circle,How to measure points scattering in a circle,,"I have circles containing points (x,y). I would like to measure the scattering of the points within the circle. For example, in the following picture, circle A will have a higher value since the points are much more scattered across the circle. Notice that the circles have varying value - so we can have a circles with different radiuses. For example in the following picture, although the points are the same - circle C will have a higher value because the points are scattered across the whole circle. Do you know a measurement which I can use for such purpose? Thanks!","I have circles containing points (x,y). I would like to measure the scattering of the points within the circle. For example, in the following picture, circle A will have a higher value since the points are much more scattered across the circle. Notice that the circles have varying value - so we can have a circles with different radiuses. For example in the following picture, although the points are the same - circle C will have a higher value because the points are scattered across the whole circle. Do you know a measurement which I can use for such purpose? Thanks!",,"['linear-algebra', 'geometry', 'statistics', 'trigonometry', 'circles']"
9,Probability of randomly selecting a number in the set ${2^n}$ from positive natural numbers?,Probability of randomly selecting a number in the set  from positive natural numbers?,{2^n},"From substituting up to the first $m$ natural numbers I found that the probability of an integer being in the set of integers $2^n$ is approximately $\frac{\ln(m)}{m\ln(2)}$ . For example, for the first 10 natural numbers (1,2,3,...,10 where $m=10$ ), the probability of randomly selecting a integer in $2^n$ (there are three here – 2,4,8) is $\frac{3}{10}$ or 0.3. The approximation defined this as 0.332. However, I don't know the intuition behind this approximation (i.e. if it will work for all $m$ ) and I can't seem to find an exact form of this. I would be grateful for any help (btw, my main issue is finding an exact form of the problem).","From substituting up to the first natural numbers I found that the probability of an integer being in the set of integers is approximately . For example, for the first 10 natural numbers (1,2,3,...,10 where ), the probability of randomly selecting a integer in (there are three here – 2,4,8) is or 0.3. The approximation defined this as 0.332. However, I don't know the intuition behind this approximation (i.e. if it will work for all ) and I can't seem to find an exact form of this. I would be grateful for any help (btw, my main issue is finding an exact form of the problem).",m 2^n \frac{\ln(m)}{m\ln(2)} m=10 2^n \frac{3}{10} m,"['probability', 'statistics', 'logarithms', 'approximation']"
10,Actuarial Mathematics for Life Contingent Risks Questions $2.12.$ Part (b),Actuarial Mathematics for Life Contingent Risks Questions  Part (b),2.12.,"I am stuck on the following problem: (a) Construct a table of $Px$ for Makeham's law with parameters $A= 0.0001, B = 0.00035$ and $C = 1.075,$ for integer $x$ from age $0$ to age $130,$ using Excel or other appropriate computer software. You should set the parameters so that they can be easily changed, and you should keep the table, as many exercises and examples in future chapters will use Makeham's law. (b) Use the table to determine the age last birthday at which a life currently aged $70$ is most likely to die. I have found the solutions manual that solves part (b) in the most ""descriptive"" way to leave me still completely confused. I can't take anything from this to be able to use this to solve similar problems. Please help! ""The probability that $(70)$ dies at age $70 + k$ last birthday is $Pr[K70 = k]$ where $Kx$ is the curtate future lifetime. The most likely age at death is the value of $k$ that maximizes $Pr[K70 = k] = k\mid q70.$ The maximum value for $k \mid q70$ can be found by constructing a table of values and selecting the largest value; it is $3 \mid q70 = 0.05719,$ so the most likely age at death is $73$ "" I still have no idea how that was deemed as the largest value.","I am stuck on the following problem: (a) Construct a table of for Makeham's law with parameters and for integer from age to age using Excel or other appropriate computer software. You should set the parameters so that they can be easily changed, and you should keep the table, as many exercises and examples in future chapters will use Makeham's law. (b) Use the table to determine the age last birthday at which a life currently aged is most likely to die. I have found the solutions manual that solves part (b) in the most ""descriptive"" way to leave me still completely confused. I can't take anything from this to be able to use this to solve similar problems. Please help! ""The probability that dies at age last birthday is where is the curtate future lifetime. The most likely age at death is the value of that maximizes The maximum value for can be found by constructing a table of values and selecting the largest value; it is so the most likely age at death is "" I still have no idea how that was deemed as the largest value.","Px A= 0.0001, B = 0.00035 C = 1.075, x 0 130, 70 (70) 70 + k Pr[K70 = k] Kx k Pr[K70 = k] = k\mid q70. k \mid q70 3 \mid q70 = 0.05719, 73","['statistics', 'actuarial-science']"
11,Understanding Standard Error ($\frac{\sigma}{\sqrt{n}}$) from the definition,Understanding Standard Error () from the definition,\frac{\sigma}{\sqrt{n}},"In the ""normal"" sense, I have understood standard error to be the standard deviation of the means. Hence, you would need to calculate multiple averages ( $\bar{x}_1,..., \bar{x}_n$ ) and calculate the standard deviation (let $u$ be the average of the average): $$SE = \sqrt{\frac{\sum\limits_{i=1}^{n} (\mu - \bar{x}_i)^2}{n-1}}$$ I do not understand how this eventually becomes $(1) \ SE = \frac{\sigma}{\sqrt{n}}$ . Afterall, don't you need multiple averages to get this value? How are we able to calculate it with just one set of data? An explanation on how $(1)$ is derived would be greatly appeciated.","In the ""normal"" sense, I have understood standard error to be the standard deviation of the means. Hence, you would need to calculate multiple averages ( ) and calculate the standard deviation (let be the average of the average): I do not understand how this eventually becomes . Afterall, don't you need multiple averages to get this value? How are we able to calculate it with just one set of data? An explanation on how is derived would be greatly appeciated.","\bar{x}_1,..., \bar{x}_n u SE = \sqrt{\frac{\sum\limits_{i=1}^{n} (\mu - \bar{x}_i)^2}{n-1}} (1) \ SE = \frac{\sigma}{\sqrt{n}} (1)","['statistics', 'standard-deviation']"
12,Lehman Scheffe and Conditioned Expectation,Lehman Scheffe and Conditioned Expectation,,"In the proof of the Lehmann-Scheffe Theorem I came across an equality, which I dont understand. It is the last equality at the bottom. But still, I give the setting, since I dont know, if any of that is relevant. Let $(\chi,\sigma,\mathbb{P}_\theta : \theta \in \Omega)$ be a statistical model and $S:\chi\rightarrow \mathbb{R}$ be a sufficient and complete statistic. And let $T:\chi\rightarrow \mathbb{R}^d$ be an unbiased estimator for the statistical quantity $\tau:\theta \rightarrow \mathbb{R}^d$ . By the Rao-Blackwell theorem, we get an improved version $T^*$ of $T$ w.r.t. its variance by setting $$T^* = \mathbb{E}[T\mid\sigma(S)].$$ To show uniqueness of $T^*$ , we assume another unbiased estimator $H$ and its Blackwellized version $H^* = \mathbb{E}[H\mid\sigma(S)].$ Then there exist a measurable functions $t,h$ with $$H^*(X)=h(S(X)) \\ T^*(X)=t(S(X)) $$ Then, by their unbiasedness, we can write $$0=\mathbb{E}_\theta[H^*] - \mathbb{E}_\theta[T^*] = \mathbb{E}_\theta[H^* - T^*] = \mathbb{E}_\theta[t(S)-h(S)] = \mathbb{E}_\theta[t-h \mid \sigma(S)]\\ $$ Why does the last equality hold? Is there an intuitive way of explaining that?","In the proof of the Lehmann-Scheffe Theorem I came across an equality, which I dont understand. It is the last equality at the bottom. But still, I give the setting, since I dont know, if any of that is relevant. Let be a statistical model and be a sufficient and complete statistic. And let be an unbiased estimator for the statistical quantity . By the Rao-Blackwell theorem, we get an improved version of w.r.t. its variance by setting To show uniqueness of , we assume another unbiased estimator and its Blackwellized version Then there exist a measurable functions with Then, by their unbiasedness, we can write Why does the last equality hold? Is there an intuitive way of explaining that?","(\chi,\sigma,\mathbb{P}_\theta : \theta \in \Omega) S:\chi\rightarrow \mathbb{R} T:\chi\rightarrow \mathbb{R}^d \tau:\theta \rightarrow \mathbb{R}^d T^* T T^* = \mathbb{E}[T\mid\sigma(S)]. T^* H H^* = \mathbb{E}[H\mid\sigma(S)]. t,h H^*(X)=h(S(X)) \\
T^*(X)=t(S(X))
 0=\mathbb{E}_\theta[H^*] - \mathbb{E}_\theta[T^*] = \mathbb{E}_\theta[H^* - T^*] = \mathbb{E}_\theta[t(S)-h(S)] = \mathbb{E}_\theta[t-h \mid \sigma(S)]\\
","['probability-theory', 'statistics', 'conditional-expectation']"
13,What is the packing number of the unit cube?,What is the packing number of the unit cube?,,"The $\varepsilon$ - packing number of the unit cube $[0,1]^d$ with respect to the infinity norm is the biggest number of $\varepsilon$ -strictly-separated points, i.e., the biggest cardinality of a set of points $E \subset [0,1]^d$ such that each two distinct $x,y\in E$ satisfy $\|x-y\|_\infty > \varepsilon$ . It seems geometrically intuitive to me that this is at most $\left( \bigl\lfloor \frac{1}{\varepsilon}\bigr\rfloor + 1 \right)^d$ , where $\lfloor x \rfloor$ is the biggest integer lower or equal to $x$ (I am very naively taking a uniform grid of $(\varepsilon + \varepsilon')$ -spaced points, for a small $\varepsilon'$ ). Is this true? If so, how is it proven? I can only found bounds like $(2/\varepsilon + 1)^d$ that upper bound the packing number with the related notion of covering number and proceed to find a covering.","The - packing number of the unit cube with respect to the infinity norm is the biggest number of -strictly-separated points, i.e., the biggest cardinality of a set of points such that each two distinct satisfy . It seems geometrically intuitive to me that this is at most , where is the biggest integer lower or equal to (I am very naively taking a uniform grid of -spaced points, for a small ). Is this true? If so, how is it proven? I can only found bounds like that upper bound the packing number with the related notion of covering number and proceed to find a covering.","\varepsilon [0,1]^d \varepsilon E \subset [0,1]^d x,y\in E \|x-y\|_\infty > \varepsilon \left( \bigl\lfloor \frac{1}{\varepsilon}\bigr\rfloor + 1 \right)^d \lfloor x \rfloor x (\varepsilon + \varepsilon') \varepsilon' (2/\varepsilon + 1)^d","['geometry', 'statistics', 'packing-problem']"
14,Distribution of $(X_1-\mu)^T\Sigma^{-1}(X_1-\mu)$ is chi-squared? [duplicate],Distribution of  is chi-squared? [duplicate],(X_1-\mu)^T\Sigma^{-1}(X_1-\mu),"This question already has an answer here : Finding the distribution of $\|X-\mu \|_\Sigma^2$ with $X \sim N(\mu,\Sigma)$ (1 answer) Closed 3 years ago . If $(X_i)_{i=1}^{20}\sim N_6(\mu,\Sigma),$ then find the distribution of $$ (X_1-\mu)^T\Sigma^{-1}(X_1-\mu)$$ The solution is $\chi^2_6,$ but could someone show why? I only know that the sum of standard normal variables is itself chi-squared but I'm not sure how to approach this.","This question already has an answer here : Finding the distribution of $\|X-\mu \|_\Sigma^2$ with $X \sim N(\mu,\Sigma)$ (1 answer) Closed 3 years ago . If then find the distribution of The solution is but could someone show why? I only know that the sum of standard normal variables is itself chi-squared but I'm not sure how to approach this.","(X_i)_{i=1}^{20}\sim N_6(\mu,\Sigma),  (X_1-\mu)^T\Sigma^{-1}(X_1-\mu) \chi^2_6,","['statistics', 'probability-distributions', 'normal-distribution', 'chi-squared']"
15,How can $t$-statistic be used to test hypothesis?,How can -statistic be used to test hypothesis?,t,"I have the following question: A random sample of size 25 from a normal distribution has mean 47 and standard deviation 7. Based on $t$ -statistics, can we say that the given information supports the conjecture that the mean of the population is 42? I'm really confused how $t$ -statistics works to reject or fail to reject a hypothesis. An explanation would be really helpful. Thanks!","I have the following question: A random sample of size 25 from a normal distribution has mean 47 and standard deviation 7. Based on -statistics, can we say that the given information supports the conjecture that the mean of the population is 42? I'm really confused how -statistics works to reject or fail to reject a hypothesis. An explanation would be really helpful. Thanks!",t t,"['statistics', 'statistical-inference', 'hypothesis-testing']"
16,"Generalized Likelihood Ratio Test for $p_1=p_2$ when $X_1\sim \text{Bin} (n_1,p_1)$ and $X_2\sim\text{Bin}(n_2,p_2)$",Generalized Likelihood Ratio Test for  when  and,"p_1=p_2 X_1\sim \text{Bin} (n_1,p_1) X_2\sim\text{Bin}(n_2,p_2)","Let $X_1\sim \text{Bin} (n_1,p_1)$ , $X_2\sim\text{Bin}(n_2,p_2)$ be two independent random variables. I am trying to find the Generalized Likelihood Ratio Test for the the null hypothesis: $$H_{0}: p_1=p_2$$ The only thing I could come up with is under the null I know that $X_1+X_2\sim \text{Bin}(N=n_1+n_2,p)$ . Then I can find my size $\alpha$ test by finding the  values $K_1, K_2$ such that $$P(X_1+X_2\le K_1)\le \frac{\alpha}{2}$$ and $$P(X_1+X_2\ge K_2)\le \frac{\alpha}{2}.$$ I am just wondering  if this  is the right approach or if is there another approach that gets me my GLT. Update from comments below: Then my ratio becomes: $$\frac{(1-\bar{X})^{n_1+n_2-2x}\bar{X}^{2x}}{(1-\bar{X}_1)^{n_1-x}(1-\bar{X}_2)^{n_1-x}(\bar{X}_1\bar{X}_2)^x}$$","Let , be two independent random variables. I am trying to find the Generalized Likelihood Ratio Test for the the null hypothesis: The only thing I could come up with is under the null I know that . Then I can find my size test by finding the  values such that and I am just wondering  if this  is the right approach or if is there another approach that gets me my GLT. Update from comments below: Then my ratio becomes:","X_1\sim \text{Bin} (n_1,p_1) X_2\sim\text{Bin}(n_2,p_2) H_{0}: p_1=p_2 X_1+X_2\sim \text{Bin}(N=n_1+n_2,p) \alpha K_1, K_2 P(X_1+X_2\le K_1)\le \frac{\alpha}{2} P(X_1+X_2\ge K_2)\le \frac{\alpha}{2}. \frac{(1-\bar{X})^{n_1+n_2-2x}\bar{X}^{2x}}{(1-\bar{X}_1)^{n_1-x}(1-\bar{X}_2)^{n_1-x}(\bar{X}_1\bar{X}_2)^x}","['statistics', 'statistical-inference', 'hypothesis-testing']"
17,"Finding Bayes estimator for $\theta$ of Unif$(0,\theta)$",Finding Bayes estimator for  of Unif,"\theta (0,\theta)","Finding Bayes estimator for $\theta$ of Unif $(0,\theta)$ Let $Y = \max{X_i}$ where $(X_1,\ldots,X_n)$ is a random sample from Unif $(0,\theta)$ . $Y$ is sufficient for $\theta$ . Find the Bayes estimator $w(Y)$ for $\theta$ based on $Y$ using the loss function $L(\theta,a) = \lvert a- \theta\rvert$ The prior density of $\theta$ is $\displaystyle \pi(\theta) = \frac{2}{\theta^3}1_{(1 < \theta < \infty)}$ I am pretty unfamiliar with Bayesian inference. From what I understand the posterior is given by $\displaystyle p(\theta \mid \underline{x}) = \frac{L(\theta \mid \underline x)\pi(\theta)}{\int L(\theta \mid \underline x)\pi(\theta) \, d\theta }\, ;  $ where $$ L(\theta \mid \underline{x})\pi(\theta) = \frac{1}{\theta^n}1_{(0 \le \min(x_i))}1_{(y \le \theta)}\frac{2}{\theta^3}1_{(1<\theta<\infty)} $$ Aside from this I am not sure how I set this up to solve or where I use the loss function or how I base it off $Y$ .",Finding Bayes estimator for of Unif Let where is a random sample from Unif . is sufficient for . Find the Bayes estimator for based on using the loss function The prior density of is I am pretty unfamiliar with Bayesian inference. From what I understand the posterior is given by where Aside from this I am not sure how I set this up to solve or where I use the loss function or how I base it off .,"\theta (0,\theta) Y = \max{X_i} (X_1,\ldots,X_n) (0,\theta) Y \theta w(Y) \theta Y L(\theta,a) = \lvert a- \theta\rvert \theta \displaystyle \pi(\theta) = \frac{2}{\theta^3}1_{(1 < \theta < \infty)} \displaystyle p(\theta \mid \underline{x}) = \frac{L(\theta \mid \underline x)\pi(\theta)}{\int L(\theta \mid \underline x)\pi(\theta) \, d\theta }\, ;   
L(\theta \mid \underline{x})\pi(\theta) = \frac{1}{\theta^n}1_{(0 \le \min(x_i))}1_{(y \le \theta)}\frac{2}{\theta^3}1_{(1<\theta<\infty)}
 Y","['statistics', 'self-learning', 'statistical-inference', 'bayesian', 'order-statistics']"
18,Contribution of Time-Dependent Variable to Change in Function,Contribution of Time-Dependent Variable to Change in Function,,"Given a function $$C(x(t),y(t))=x*y$$ and discrete data for variables $x(t),y(t)$ at the points $t_0,t_1$ , what is the contribution (total or percentage) of the change in variable $x$ to the change $\Delta C$ $$\Delta C = C(x(t_0),y(t_0))-C(x(t_1),y(t_1))$$ in the function $C$ ? Background: Going through a paper recently I got stuck on the approach that the authors were using. I had not come across this before, so maybe there is an elegant way to explain this. In a 2018 paper on solar photovoltaics (P.9 Main Body, P.1 in Supplementary Material), the authors have a cost function $C$ which describes the cost associated with manufacturing one unit. It depends on manufacturing variables $x,y$ , which change over time (e.g. price of silicon, price of chemicals, etc.) $$ C(x(t),y(t)) $$ They want to determine the contribution of a single variable $x$ to the total change of the cost function between two points in time $\Delta C (t_0, t_1)$ . Variables are known only at discrete points in time ( $t_0,t_1$ ). They start by writing out the differential of the cost function $C$ as $$ dC (x(t), y(t)) = \frac{ \partial C }{ \partial x } \frac{ \text{d} x }{ \text{d} t} \text{d} t + \frac{ \partial C }{ \partial y } \frac{ \text{d} y }{ \text{d} t} \text{d} t $$ where the contribution of the change in variable x over time $t_0 < t < t_1$ is then $$ \Delta C_x = \int_{t=t_0}^{t_1} \frac{ \partial C }{ \partial x } \frac{ \text{d} x }{ \text{d} t} \text{d} t $$ Here they say If it were possible to observe the (...) variables x in continuous time, (...) [this equation] would provide all that is needed to compute the contribution of each variable x. Using logarithmic differentiation , they go on to rewrite the expression as $$ \Delta C_x = \int_{t=t_0}^{t_1} C(t) \frac{ \partial \ln C }{ \partial x } \frac{ \text{d} x }{ \text{d} t} \text{d} t $$ and then for $C(t)$ assume a constant $C(t) \approx \tilde{C} $ which is ultimately chosen to be $\tilde{C} = \frac{ \Delta \tilde{C} }{ \Delta \ln \tilde{C} }$ , such that $\Delta C_x + \Delta C_y = \Delta C$ . Questions: Even if the time dependence of variables was known (eg. daily data on the price of silicon, etc.), then integrating would not yield what the authors are actually looking for. They are interested in the contribution of single variables to the total change in $C$ (eg. what percentage of total manufacturing cost reductions are due to decrease in silicon price). But integrating using $$\Delta C_x = \int_{t=t_0}^{t_1} \frac{ \partial C }{ \partial x } \frac{ \text{d} x }{ \text{d} t} \text{d} t $$ is dependent on the path of curves $x(t),y(t)$ . This would yield different results for different time dependency of variables. A variable $x(t)$ would yield a different $\Delta C_x$ than a variable $x'(t)$ , which is not what the authors seek to describe.","Given a function and discrete data for variables at the points , what is the contribution (total or percentage) of the change in variable to the change in the function ? Background: Going through a paper recently I got stuck on the approach that the authors were using. I had not come across this before, so maybe there is an elegant way to explain this. In a 2018 paper on solar photovoltaics (P.9 Main Body, P.1 in Supplementary Material), the authors have a cost function which describes the cost associated with manufacturing one unit. It depends on manufacturing variables , which change over time (e.g. price of silicon, price of chemicals, etc.) They want to determine the contribution of a single variable to the total change of the cost function between two points in time . Variables are known only at discrete points in time ( ). They start by writing out the differential of the cost function as where the contribution of the change in variable x over time is then Here they say If it were possible to observe the (...) variables x in continuous time, (...) [this equation] would provide all that is needed to compute the contribution of each variable x. Using logarithmic differentiation , they go on to rewrite the expression as and then for assume a constant which is ultimately chosen to be , such that . Questions: Even if the time dependence of variables was known (eg. daily data on the price of silicon, etc.), then integrating would not yield what the authors are actually looking for. They are interested in the contribution of single variables to the total change in (eg. what percentage of total manufacturing cost reductions are due to decrease in silicon price). But integrating using is dependent on the path of curves . This would yield different results for different time dependency of variables. A variable would yield a different than a variable , which is not what the authors seek to describe.","C(x(t),y(t))=x*y x(t),y(t) t_0,t_1 x \Delta C \Delta C = C(x(t_0),y(t_0))-C(x(t_1),y(t_1)) C C x,y 
C(x(t),y(t))
 x \Delta C (t_0, t_1) t_0,t_1 C 
dC (x(t), y(t)) = \frac{ \partial C }{ \partial x } \frac{ \text{d} x }{ \text{d} t} \text{d} t + \frac{ \partial C }{ \partial y } \frac{ \text{d} y }{ \text{d} t} \text{d} t
 t_0 < t < t_1 
\Delta C_x = \int_{t=t_0}^{t_1} \frac{ \partial C }{ \partial x } \frac{ \text{d} x }{ \text{d} t} \text{d} t
 
\Delta C_x = \int_{t=t_0}^{t_1} C(t) \frac{ \partial \ln C }{ \partial x } \frac{ \text{d} x }{ \text{d} t} \text{d} t
 C(t) C(t) \approx \tilde{C}  \tilde{C} = \frac{ \Delta \tilde{C} }{ \Delta \ln \tilde{C} } \Delta C_x + \Delta C_y = \Delta C C \Delta C_x = \int_{t=t_0}^{t_1} \frac{ \partial C }{ \partial x } \frac{ \text{d} x }{ \text{d} t} \text{d} t  x(t),y(t) x(t) \Delta C_x x'(t)","['real-analysis', 'analysis', 'statistics', 'economics', 'differential']"
19,$\sigma(T(X)) = \sigma(X) \iff T$ is bijective,is bijective,\sigma(T(X)) = \sigma(X) \iff T,"Consider the following claim: Let $(\mathcal{X},\mathcal{M})$ , $(\mathcal{Y},\mathcal{N})$ and $(\mathcal{Z},\mathcal{O})$ be measurable spaces, and let $f \colon \mathcal{X} \to \mathcal{Y}$ and $g \colon \mathcal{Y} \to \mathcal{Z}$ be measurable functions. Then $\sigma(g \circ f) = \sigma(f)$ if and only if $g$ is bijective, where $\sigma(g \circ f) = (g \circ f)^{-1}(\mathcal{O}) = f^{-1}(g^{-1}(\mathcal{O}))$ and $\sigma(f) = f^{-1}(\mathcal{N})$ . This is false: for the “if” part of the statement, just consider $\mathcal{X} = \mathcal{Y} = \mathcal{Z}$ , $\mathcal{M} = \mathcal{N}$ , $\mathcal{O} = \{ \emptyset, \mathcal{X} \}$ and $f = g = \mathrm{id}_{\mathcal{X}}$ . I was going through Shao’s Mathematical Statistics , and on page 100 it is claimed that if $X$ is a random vector ( $\mathbb{R}^{n}-$ valued) and $T(X)$ is a statistic (which to my understanding means that $T$ is a measurable function from $\mathbb{R}^{n}$ to $\mathbb{R}^{m}$ , both endowed with the Borel $\sigma-$ algebra), then $\sigma(T(X)) = \sigma(X)$ if and only if $T$ is bijective. What makes this statement true, when the previous was not?","Consider the following claim: Let , and be measurable spaces, and let and be measurable functions. Then if and only if is bijective, where and . This is false: for the “if” part of the statement, just consider , , and . I was going through Shao’s Mathematical Statistics , and on page 100 it is claimed that if is a random vector ( valued) and is a statistic (which to my understanding means that is a measurable function from to , both endowed with the Borel algebra), then if and only if is bijective. What makes this statement true, when the previous was not?","(\mathcal{X},\mathcal{M}) (\mathcal{Y},\mathcal{N}) (\mathcal{Z},\mathcal{O}) f \colon \mathcal{X} \to \mathcal{Y} g \colon \mathcal{Y} \to \mathcal{Z} \sigma(g \circ f) = \sigma(f) g \sigma(g \circ f) = (g \circ f)^{-1}(\mathcal{O}) = f^{-1}(g^{-1}(\mathcal{O})) \sigma(f) = f^{-1}(\mathcal{N}) \mathcal{X} = \mathcal{Y} = \mathcal{Z} \mathcal{M} = \mathcal{N} \mathcal{O} = \{ \emptyset, \mathcal{X} \} f = g = \mathrm{id}_{\mathcal{X}} X \mathbb{R}^{n}- T(X) T \mathbb{R}^{n} \mathbb{R}^{m} \sigma- \sigma(T(X)) = \sigma(X) T","['probability-theory', 'measure-theory', 'statistics', 'measurable-functions']"
20,Are there applications of martingales other than in finance?,Are there applications of martingales other than in finance?,,"I’ve only had a brief introduction to martingales and was wondering if there are applications of the theory in other areas of mathematics and in real world applications other than in finance (finance being the obvious one)? Do they come up at all in differential equations, statistics, physics or biology, for example?","I’ve only had a brief introduction to martingales and was wondering if there are applications of the theory in other areas of mathematics and in real world applications other than in finance (finance being the obvious one)? Do they come up at all in differential equations, statistics, physics or biology, for example?",,"['probability-theory', 'statistics', 'conditional-expectation', 'martingales', 'applications']"
21,Every mixed Poisson process is also a compound Poisson process?,Every mixed Poisson process is also a compound Poisson process?,,"In section 3.6 of this paper: https://www.jstor.org/stable/25472639?read-now=1&seq=8#page_scan_tab_contents , the authors state that mixed Poisson distributions that are infinitely divisible can be represented as some Compound Poisson distribution. An example of this is the Negative Binomial point process (which is a Poisson mixture with the rate parameter being gamma distributed). It can also be considered a Compound Poisson point process with a logarithmic distribution being the compounding distribution. What I don't understand is how these two kinds of point processes can be the same. For example, consider the number of events from the point process in some interval, $\delta t$ . As $\delta t \to 0$ , the number of events with the Mixed Poisson process should be either $0$ or $1$ . It should be very unlikely to encounter $2$ events in a very small interval. For a Compound Poisson, this shouldn't be so unlikely since we only need a Poisson arrival in the small interval and then the compounding distribution can easily be $2$ or more. What is wrong with my reasoning?","In section 3.6 of this paper: https://www.jstor.org/stable/25472639?read-now=1&seq=8#page_scan_tab_contents , the authors state that mixed Poisson distributions that are infinitely divisible can be represented as some Compound Poisson distribution. An example of this is the Negative Binomial point process (which is a Poisson mixture with the rate parameter being gamma distributed). It can also be considered a Compound Poisson point process with a logarithmic distribution being the compounding distribution. What I don't understand is how these two kinds of point processes can be the same. For example, consider the number of events from the point process in some interval, . As , the number of events with the Mixed Poisson process should be either or . It should be very unlikely to encounter events in a very small interval. For a Compound Poisson, this shouldn't be so unlikely since we only need a Poisson arrival in the small interval and then the compounding distribution can easily be or more. What is wrong with my reasoning?",\delta t \delta t \to 0 0 1 2 2,"['probability', 'statistics', 'poisson-distribution', 'poisson-process']"
22,Bayes classifier for binary decision problem with Reject option,Bayes classifier for binary decision problem with Reject option,,"Consider the decision problem where three decisions are valid: $0, 1$ and a third option $reject$ . An optimal rule has the lowest probability of error at a fixed ""reject"" probability. More formally, given a context $X \in \mathcal{X}$ and a target $Y \in \{0, 1\}$ , a decision rule $g: \mathcal{X} \rightarrow \{0, 1, reject\}$ is optimal if for any decision rule $g'$ with $\mathbb{P}(g'(X) = reject) \leq \mathbb{P}(g(X) = reject)$ we have that the error probability of $g'$ is at least as large as that of $g$ : $$\mathbb{P}(g'(X) \neq Y | g'(X) \neq reject) \geq \mathbb{P}(g(X) \neq Y | g(X) \neq reject)$$ Show that the following decision function is optimal for $c \in (0, 1/2)$ : $$ g_c(X) =  \begin{cases} 1,  & \eta(X) > 1/2 + c \\ 0, & \eta(X) \leq 1/2 - c \\ reject, & \text{otherwise} \end{cases} $$ where $\eta(x) = \mathbb{P}(Y = 1 | X=x)$ . What I've tried I've tried showing this in the same way as showing that $\eta(x)$ is the Bayes optimal predictor in the case without the rejection option: Using the identity: $$ \mathbb{P}(g(X) \neq Y | X = x) = 1- (\mathbb{I}(g(X) = 1)\eta(x) + \mathbb{I}(g(X) = 0)(1-\eta(x))) $$ We then have: $$ \begin{align} &\mathbb{P}(g(X) \neq Y | X = x) - \mathbb{P}(g_c(X) \neq Y | X = x)\\ & = (\mathbb{I}[g_c(X) = 1]\eta(x) + \mathbb{I}[g_c(X) = 0](1-\eta(x))) -(\mathbb{I}[g(X) = 1]\eta(x) + \mathbb{I}[g(X) = 0](1-\eta(x))) \\ &= (\eta(x)(\mathbb{I}[g_c(X) = 1] - \mathbb{I}[g(X) = 1)] + (1-\eta(x))(\mathbb{I}[g_c(X) = 0] - \mathbb{I}[g(X) = 0])) \\ &= (2\eta(x) -1)(\mathbb{I}[g_c(X) = 1] - \mathbb{I}[g(X) = 1]) + (1-\eta(x))(\mathbb{I}[g_c(X) = reject] - \mathbb{I}[g(X) = reject]) \end{align} $$ The last equality follows from considering the complement of the events in the indicators in the second term. In the case of the Bayes optimal rule, the first term is always nonnegative. In our case this isn't clear to me since the decision boundary is no longer $1/2$ . We also have an extra term involving the rejection probability. I have tried a few other manipulations but I think there is a simpler way that I am missing.","Consider the decision problem where three decisions are valid: and a third option . An optimal rule has the lowest probability of error at a fixed ""reject"" probability. More formally, given a context and a target , a decision rule is optimal if for any decision rule with we have that the error probability of is at least as large as that of : Show that the following decision function is optimal for : where . What I've tried I've tried showing this in the same way as showing that is the Bayes optimal predictor in the case without the rejection option: Using the identity: We then have: The last equality follows from considering the complement of the events in the indicators in the second term. In the case of the Bayes optimal rule, the first term is always nonnegative. In our case this isn't clear to me since the decision boundary is no longer . We also have an extra term involving the rejection probability. I have tried a few other manipulations but I think there is a simpler way that I am missing.","0, 1 reject X \in \mathcal{X} Y \in \{0, 1\} g: \mathcal{X} \rightarrow \{0, 1, reject\} g' \mathbb{P}(g'(X) = reject) \leq \mathbb{P}(g(X) = reject) g' g \mathbb{P}(g'(X) \neq Y | g'(X) \neq reject) \geq \mathbb{P}(g(X) \neq Y | g(X) \neq reject) c \in (0, 1/2) 
g_c(X) = 
\begin{cases}
1,  & \eta(X) > 1/2 + c \\
0, & \eta(X) \leq 1/2 - c \\
reject, & \text{otherwise}
\end{cases}
 \eta(x) = \mathbb{P}(Y = 1 | X=x) \eta(x) 
\mathbb{P}(g(X) \neq Y | X = x) = 1- (\mathbb{I}(g(X) = 1)\eta(x) + \mathbb{I}(g(X) = 0)(1-\eta(x)))
 
\begin{align}
&\mathbb{P}(g(X) \neq Y | X = x) - \mathbb{P}(g_c(X) \neq Y | X = x)\\
& = (\mathbb{I}[g_c(X) = 1]\eta(x) + \mathbb{I}[g_c(X) = 0](1-\eta(x))) -(\mathbb{I}[g(X) = 1]\eta(x) + \mathbb{I}[g(X) = 0](1-\eta(x))) \\
&= (\eta(x)(\mathbb{I}[g_c(X) = 1] - \mathbb{I}[g(X) = 1)] + (1-\eta(x))(\mathbb{I}[g_c(X) = 0] - \mathbb{I}[g(X) = 0])) \\
&= (2\eta(x) -1)(\mathbb{I}[g_c(X) = 1] - \mathbb{I}[g(X) = 1]) + (1-\eta(x))(\mathbb{I}[g_c(X) = reject] - \mathbb{I}[g(X) = reject])
\end{align}
 1/2","['probability', 'statistics', 'machine-learning']"
23,Is Student's t-distribution valid when samples themselves have uncertainty - such as quantisation errors?,Is Student's t-distribution valid when samples themselves have uncertainty - such as quantisation errors?,,"NB: I was gonna post on physics stack exchange, not really sure where this fits in. But I'm only a lowly Engineer so please go easy on the notation if you can Using Student's t-distribution I can infer the parameters ( $\mu,\sigma^2$ ) of a probability distribution based on $n$ samples of data that I assume will fit a gaussian prior. However in all the examples I've seen, the $n$ samples are all simple values. How can I infer a probability distribution based on samples of data with uncertainty; if my $n$ samples are not simple values but probability distributions themselves? What is the effect of measurement uncertainty on the shape of the inferred distribution? Context I'm trying to measure how long some code takes to run on a computer. The timer is low resolution - similar order of magnitude to the duration I'm trying to measure - and so the true timestamps are quantized into 100 ms bins. Assuming a uniform rectangular probability distribution within these bins, then the time differences have a triangular probability distribution. i.e. A task starting at $142ms$ and ending at $331 ms$ when quantised will appear to start at $100\pm50ms$ and end at $300\pm50ms$ . Then the difference will be a triangular probability distribution, centered on $200ms$ and with a width of $\pm 100ms$ . I have several of these triangular timespan measurements, and I'd like to use them to determine the parameters of a distribution. As I say, I could just ignore the quantisation errors in my samples, and plug the modal (centre) values into the t-distribution, but surely those errors will increase the uncertainty ( $\sigma$ ) of my inferred gaussian?","NB: I was gonna post on physics stack exchange, not really sure where this fits in. But I'm only a lowly Engineer so please go easy on the notation if you can Using Student's t-distribution I can infer the parameters ( ) of a probability distribution based on samples of data that I assume will fit a gaussian prior. However in all the examples I've seen, the samples are all simple values. How can I infer a probability distribution based on samples of data with uncertainty; if my samples are not simple values but probability distributions themselves? What is the effect of measurement uncertainty on the shape of the inferred distribution? Context I'm trying to measure how long some code takes to run on a computer. The timer is low resolution - similar order of magnitude to the duration I'm trying to measure - and so the true timestamps are quantized into 100 ms bins. Assuming a uniform rectangular probability distribution within these bins, then the time differences have a triangular probability distribution. i.e. A task starting at and ending at when quantised will appear to start at and end at . Then the difference will be a triangular probability distribution, centered on and with a width of . I have several of these triangular timespan measurements, and I'd like to use them to determine the parameters of a distribution. As I say, I could just ignore the quantisation errors in my samples, and plug the modal (centre) values into the t-distribution, but surely those errors will increase the uncertainty ( ) of my inferred gaussian?","\mu,\sigma^2 n n n 142ms 331 ms 100\pm50ms 300\pm50ms 200ms \pm 100ms \sigma","['probability', 'statistics', 'probability-distributions', 'statistical-inference']"
24,What does conditioning on an observed sample mean?,What does conditioning on an observed sample mean?,,"Suppose we have a collection of samples $\mathcal{D} = \{x_1, \ldots, x_n\}$ drawn independently from a fixed but unknown distribution $p(x)$ , Bayesian estimation uses $\mathcal{D}$ to determine $p(x|\mathcal{D})$ . I am having trouble transforming the above to rigorous mathematical language. The formal definition for conditional distribution is $f|_{X|Y}(x|y) = \frac{f(x,y)}{f(y)}$ , but that requires both $X$ and $Y$ to be random variables. I do not know how to think of $\mathcal{D}$ as a random variable. Should it be thought of as $P(X = x| (X_1, \ldots, X_n) = (x_1, \ldots, x_n))$ where $X_1, \ldots, X_n$ are i.i.d randome variables.","Suppose we have a collection of samples drawn independently from a fixed but unknown distribution , Bayesian estimation uses to determine . I am having trouble transforming the above to rigorous mathematical language. The formal definition for conditional distribution is , but that requires both and to be random variables. I do not know how to think of as a random variable. Should it be thought of as where are i.i.d randome variables.","\mathcal{D} = \{x_1, \ldots, x_n\} p(x) \mathcal{D} p(x|\mathcal{D}) f|_{X|Y}(x|y) = \frac{f(x,y)}{f(y)} X Y \mathcal{D} P(X = x| (X_1, \ldots, X_n) = (x_1, \ldots, x_n)) X_1, \ldots, X_n","['probability', 'probability-theory', 'statistics']"
25,Help understanding the Weak Law of Large Numbers?,Help understanding the Weak Law of Large Numbers?,,"In the book I'm currently reading, this law is stated as: The Weak Law of Large Numbers provides proof of the notion that if $n$ independent and identically distributed random variables, $X_1,X_2,...,X_n$ , from a distribution with ﬁnite variance are observed, then the sample mean, $\bar{X}$ , should be very close to $\mu$ provided $n$ is large. The problem is, the book hasn't formally defined $\bar{X}$ , so I'm unclear as the the relationship between $X$ and the $X_i's$ . From the context I assuming that: $$\bar{X} = \frac{X_1+X_2+\cdots+X_n}{n}$$ But this makes no sense to me since the $X_i's$ may not necessarily all even be the same unit, so this would be like adding apples and oranges. In other words, I can say, in an experiment of three coin tosses, let $X_1$ be the number of tails that appear and $X_2$ be the number of heads that appear, thus the units for $X_1$ and $X_2$ are tails and heads, respectively. Understanding the relationship between the $X_i's$ and $X$ is important to me to make sense of the expression: $$\lim_{n \rightarrow \infty} \mathbb P \left( \left| \frac{X_1+X_2+\cdots+X_n}{n}-\mu \right| \ge \epsilon \right) = 0$$ which, upon replacing $\mu$ with its definition, I obtain: $$\lim_{n \rightarrow \infty} \mathbb P \left( \left| \frac{X_1+X_2+\cdots+X_n}{n}-E[X] \right| \ge \epsilon \right) = 0$$ Perhaps this is totally wrong but my interpretation of this law is essentially if $\mu_i=E[X_i]$ , then, as $n \rightarrow \infty$ , $\mu_1=\mu_2=\cdots=\mu_n=\mu$ , which I feel is intuitively obvious as all the $X$ 's are identically distributed (again, ignoring units). So my questions boil down to: What is the definition of $\bar{X}$ ? What is the conceptual meaning of $\frac{X_1+X_2+\cdots+X_n}{n}$ ? How do I reconcile the unit differences of the expression in question 2? What is the relationship between the $X_i$ 's and $X$ ?","In the book I'm currently reading, this law is stated as: The Weak Law of Large Numbers provides proof of the notion that if independent and identically distributed random variables, , from a distribution with ﬁnite variance are observed, then the sample mean, , should be very close to provided is large. The problem is, the book hasn't formally defined , so I'm unclear as the the relationship between and the . From the context I assuming that: But this makes no sense to me since the may not necessarily all even be the same unit, so this would be like adding apples and oranges. In other words, I can say, in an experiment of three coin tosses, let be the number of tails that appear and be the number of heads that appear, thus the units for and are tails and heads, respectively. Understanding the relationship between the and is important to me to make sense of the expression: which, upon replacing with its definition, I obtain: Perhaps this is totally wrong but my interpretation of this law is essentially if , then, as , , which I feel is intuitively obvious as all the 's are identically distributed (again, ignoring units). So my questions boil down to: What is the definition of ? What is the conceptual meaning of ? How do I reconcile the unit differences of the expression in question 2? What is the relationship between the 's and ?","n X_1,X_2,...,X_n \bar{X} \mu n \bar{X} X X_i's \bar{X} = \frac{X_1+X_2+\cdots+X_n}{n} X_i's X_1 X_2 X_1 X_2 X_i's X \lim_{n \rightarrow \infty} \mathbb P \left( \left| \frac{X_1+X_2+\cdots+X_n}{n}-\mu \right| \ge \epsilon \right) = 0 \mu \lim_{n \rightarrow \infty} \mathbb P \left( \left| \frac{X_1+X_2+\cdots+X_n}{n}-E[X] \right| \ge \epsilon \right) = 0 \mu_i=E[X_i] n \rightarrow \infty \mu_1=\mu_2=\cdots=\mu_n=\mu X \bar{X} \frac{X_1+X_2+\cdots+X_n}{n} X_i X","['probability', 'statistics', 'probability-limit-theorems', 'law-of-large-numbers']"
26,"A ""paradox"" in the odds of being the minimum variable in a set of IID variables","A ""paradox"" in the odds of being the minimum variable in a set of IID variables",,"This question arose when I was trying to find the odds that a customer is served before the customer directly ahead of him in an M/M/m queueing system. For a a RV in a set of IID RV of size N, is the probability of being the minimum value the same as the probability of being less than the minimum value of a set of size N - 1? Let's say you have 10 IID exponentially distributed random variables. You pick label one variable as ""A"" arbitrarily and ask this question: What is the probability that A is the minimum of the set of 10? Obviously because they are IID, the answer is 1/10. Yet I have an issue. It seems true that the question ""Is A the minimum"" is equivalent to the question ""Is A less than the minimum of the other 9"". The minimum of 9 exponentially distributed variables with parameter lambda has mean $$\frac {1}{9\lambda} $$ So to find the probability of A being less than the other nine, I took the integral $$\int_{0}^{\frac {1}{9\lambda}} \lambda e^{-\lambda x} d x $$ But this evaluates to $$ 1 - e^{\frac{1}{9}} \ne \frac{1}{10}$$ Where have I gone wrong?","This question arose when I was trying to find the odds that a customer is served before the customer directly ahead of him in an M/M/m queueing system. For a a RV in a set of IID RV of size N, is the probability of being the minimum value the same as the probability of being less than the minimum value of a set of size N - 1? Let's say you have 10 IID exponentially distributed random variables. You pick label one variable as ""A"" arbitrarily and ask this question: What is the probability that A is the minimum of the set of 10? Obviously because they are IID, the answer is 1/10. Yet I have an issue. It seems true that the question ""Is A the minimum"" is equivalent to the question ""Is A less than the minimum of the other 9"". The minimum of 9 exponentially distributed variables with parameter lambda has mean So to find the probability of A being less than the other nine, I took the integral But this evaluates to Where have I gone wrong?",\frac {1}{9\lambda}  \int_{0}^{\frac {1}{9\lambda}} \lambda e^{-\lambda x} d x   1 - e^{\frac{1}{9}} \ne \frac{1}{10},"['probability', 'probability-theory', 'statistics']"
27,"Optimizing expectation, unknown parameters, normal distribution. How many restaurants should I try before choosing one for the rest of my n-m meals?","Optimizing expectation, unknown parameters, normal distribution. How many restaurants should I try before choosing one for the rest of my n-m meals?",,"Suppose you move to a new city with an infinite number of restaurants and you plan to stay there for a predetermined amount of time.  You plan to have n meals at restaurants over the course of your stay in the city.  Assume there are no reviews on any of the restaurants and the only way to determine how good a restaurant is is by eating there and giving it a rating.  Also assume the quality of restaurants follows a normal distribution but you don't know the parameters, µ and σ, of the distribution.  Your goal is to optimize the expectation of your combined restaurant experiences. You do not value variety and would happily eat every single meal at the best restaurant if you could find it. For example, if you simply ate at a different restaurant for every meal you would have an expected combined experience of n $\times$ µ. Also if you had a meal at a random restaurant then decided to have all your meals there without trying any others you would again have an expected combined experience of n $\times$ µ. But you could improve upon that by trying two restaurants then choosing the better of the two and having all your remaining meals there.  Then your expected combined experience would be (n-1) $\times$ a 1 +a 2 . where a 1 is the expected rating of the better of the two restaurants and a 2 is the expectation of the worse restaurant.  (What would a 1 and a 2 be in terms of µ and σ for this case? I know (a 1 +a 2 )/2=µ but don't know how far apart they would be). You could improve further by trying 3 restaurants and choosing the best of those and so on.  If you sampled m restaurants before settling on one for your remaining meals your combined expectation would be (n-m+1) $\times$ a 1 +a 2 +a 3 +...+a m where again a 1 is the highest expectation of these m drawings. Main question: How many restaurants, m, should you try before picking the best of those restaurants for your remaining n-m meals?","Suppose you move to a new city with an infinite number of restaurants and you plan to stay there for a predetermined amount of time.  You plan to have n meals at restaurants over the course of your stay in the city.  Assume there are no reviews on any of the restaurants and the only way to determine how good a restaurant is is by eating there and giving it a rating.  Also assume the quality of restaurants follows a normal distribution but you don't know the parameters, µ and σ, of the distribution.  Your goal is to optimize the expectation of your combined restaurant experiences. You do not value variety and would happily eat every single meal at the best restaurant if you could find it. For example, if you simply ate at a different restaurant for every meal you would have an expected combined experience of n µ. Also if you had a meal at a random restaurant then decided to have all your meals there without trying any others you would again have an expected combined experience of n µ. But you could improve upon that by trying two restaurants then choosing the better of the two and having all your remaining meals there.  Then your expected combined experience would be (n-1) a 1 +a 2 . where a 1 is the expected rating of the better of the two restaurants and a 2 is the expectation of the worse restaurant.  (What would a 1 and a 2 be in terms of µ and σ for this case? I know (a 1 +a 2 )/2=µ but don't know how far apart they would be). You could improve further by trying 3 restaurants and choosing the best of those and so on.  If you sampled m restaurants before settling on one for your remaining meals your combined expectation would be (n-m+1) a 1 +a 2 +a 3 +...+a m where again a 1 is the highest expectation of these m drawings. Main question: How many restaurants, m, should you try before picking the best of those restaurants for your remaining n-m meals?",\times \times \times \times,"['probability', 'statistics', 'optimization', 'normal-distribution', 'expected-value']"
28,Calculated Chi-Square probability by hand (using R) seems quite off - did I something wrong?,Calculated Chi-Square probability by hand (using R) seems quite off - did I something wrong?,,"I try to understand the Chi-Square distribution with df=4-1. Therefore I tried to calculate the following (using R): I have a four-sided dice and I threw this dice n=100 times. I get the results (23,28,27,22) instead of (25,25,25,25). This gives me a chi-square value of 1.04: gemessen <- c(23,28,27,22) ideal <- c(25,25,25,25) chiquadrat <- sum((gemessen - ideal)^2 / ideal) Now I calculated pchisq(1.04, df = 4 - 1, lower.tail = FALSE) and got 0.7915744. I know that this is only an approximation and therefore I wanted to calculate it by hand (using multinomial distribution) for (x1 in 0:100){   for (x2 in 0:100){     for (x3 in 0:100){       x4 <- 100-x1-x2-x3       gg <- c(x1,x2,x3,x4)       if (x4>= 0 && sum((gg-ideal)^2/ideal)>=1.04)       {vonhand <- vonhand + factorial(100)*0.25^100/(factorial(x1)*factorial(x2)*factorial(x3)*factorial(x4))       }}}} This gives me 0.8107929. Is this correct? The error between 0.8107929 and 0.7915744 seems quite big. Furthermore I tried to support my calculation with brut force and the following code yielded 0.810845 which would confirm my calculation of 0.8107929. samples <- rmultinom(10000000, size = 100, prob = c(0.25,0.25,0.25,0.25)) ideal <- c(25,25,25,25) chiquadrats <- colSums((samples - ideal)^2 / ideal) #sort(chiquadrats) #plot(sort(chiquadrats)) sum(chiquadrats>=chiquadrat)/10000000","I try to understand the Chi-Square distribution with df=4-1. Therefore I tried to calculate the following (using R): I have a four-sided dice and I threw this dice n=100 times. I get the results (23,28,27,22) instead of (25,25,25,25). This gives me a chi-square value of 1.04: gemessen <- c(23,28,27,22) ideal <- c(25,25,25,25) chiquadrat <- sum((gemessen - ideal)^2 / ideal) Now I calculated pchisq(1.04, df = 4 - 1, lower.tail = FALSE) and got 0.7915744. I know that this is only an approximation and therefore I wanted to calculate it by hand (using multinomial distribution) for (x1 in 0:100){   for (x2 in 0:100){     for (x3 in 0:100){       x4 <- 100-x1-x2-x3       gg <- c(x1,x2,x3,x4)       if (x4>= 0 && sum((gg-ideal)^2/ideal)>=1.04)       {vonhand <- vonhand + factorial(100)*0.25^100/(factorial(x1)*factorial(x2)*factorial(x3)*factorial(x4))       }}}} This gives me 0.8107929. Is this correct? The error between 0.8107929 and 0.7915744 seems quite big. Furthermore I tried to support my calculation with brut force and the following code yielded 0.810845 which would confirm my calculation of 0.8107929. samples <- rmultinom(10000000, size = 100, prob = c(0.25,0.25,0.25,0.25)) ideal <- c(25,25,25,25) chiquadrats <- colSums((samples - ideal)^2 / ideal) #sort(chiquadrats) #plot(sort(chiquadrats)) sum(chiquadrats>=chiquadrat)/10000000",,"['statistics', 'probability-distributions', 'chi-squared', 'multinomial-distribution']"
29,"Karlin-Rubin theorem, normal distribution","Karlin-Rubin theorem, normal distribution",,"Let $X_1,...,X_n \sim N(\mu, \sigma^2)$ , where $\mu$ is known and $\sigma^2$ is unknown. We have a pair of hypotheses: $  \begin{cases} H_0: \sigma^2\leq a   \\ H_1: \sigma^2 > a \end{cases}$ So I use Karlin-Rubin theorem: $\phi(x)= \begin{cases} 1& \text{if $T(x) \geq k$ }    \\ 0& \text{if $T(x) < k$ } \end{cases}$ My PDF is: $f(x)= \exp[\frac{-1}{2\sigma^2}(x-\mu)^2 -\frac{1}{2}\ln2\pi\sigma^2]$ Of course it is an exponential family, with $C(\sigma^2)=-\frac{1}{2\sigma^2}$ which is increasing, so my $T(X)=\sum(X_i-\mu)^2$ . Here comes the struggle: I've see someone do it in this fashion: $\phi(x)= \begin{cases} 1& \text{if $\sum(X_i-\mu)^2 \leq k$ }    \\ 0& \text{if $\sum(X_i-\mu)^2 > k$ } \end{cases}$ so the inequalities are reversed. Here comes the question: why? EDIT Another question is about $k$ . We use a condition that $E_a[ϕ(X)]=α$ to obtain $k$ and thus (at some point) we set our $σ^2$ to be equal to $a$ . We do it because if the condtion is fulfilled for $σ^2=a$ then is fulfilled also for $σ^2>a$ ? So, to put it in the simple way, we reject the null hypothesis the ""more"" the greater $σ^2$ is than $a$ ? An example: in this case $\Bbb P(\sum(\frac{X_i -\mu}{\sigma})^2\leq\frac{k}{\sigma^2})=1-\alpha$ as $\sum(\frac{X_i -\mu}{\sigma})^2 \sim \mathcal X_n^2$ so: $\mathcal X_{n,1-\alpha}^2=\frac{k}{\sigma^2}$ $k=\mathcal X_{n,1-\alpha}^2a^2$ Why do we put $a$ up there? Is it because: $  \begin{cases} H_0: \sigma^2\leq a   \\ H_1: \sigma^2 > a \end{cases} \equiv   \begin{cases} H_0: \sigma^2= a   \\ H_1: \sigma^2 > a \end{cases}$ ?","Let , where is known and is unknown. We have a pair of hypotheses: So I use Karlin-Rubin theorem: My PDF is: Of course it is an exponential family, with which is increasing, so my . Here comes the struggle: I've see someone do it in this fashion: so the inequalities are reversed. Here comes the question: why? EDIT Another question is about . We use a condition that to obtain and thus (at some point) we set our to be equal to . We do it because if the condtion is fulfilled for then is fulfilled also for ? So, to put it in the simple way, we reject the null hypothesis the ""more"" the greater is than ? An example: in this case as so: Why do we put up there? Is it because: ?","X_1,...,X_n \sim N(\mu, \sigma^2) \mu \sigma^2  
\begin{cases}
H_0: \sigma^2\leq a   \\
H_1: \sigma^2 > a
\end{cases} \phi(x)=
\begin{cases}
1& \text{if T(x) \geq k }    \\
0& \text{if T(x) < k }
\end{cases} f(x)= \exp[\frac{-1}{2\sigma^2}(x-\mu)^2 -\frac{1}{2}\ln2\pi\sigma^2] C(\sigma^2)=-\frac{1}{2\sigma^2} T(X)=\sum(X_i-\mu)^2 \phi(x)=
\begin{cases}
1& \text{if \sum(X_i-\mu)^2 \leq k }    \\
0& \text{if \sum(X_i-\mu)^2 > k }
\end{cases} k E_a[ϕ(X)]=α k σ^2 a σ^2=a σ^2>a σ^2 a \Bbb P(\sum(\frac{X_i -\mu}{\sigma})^2\leq\frac{k}{\sigma^2})=1-\alpha \sum(\frac{X_i -\mu}{\sigma})^2 \sim \mathcal X_n^2 \mathcal X_{n,1-\alpha}^2=\frac{k}{\sigma^2} k=\mathcal X_{n,1-\alpha}^2a^2 a  
\begin{cases}
H_0: \sigma^2\leq a   \\
H_1: \sigma^2 > a
\end{cases} \equiv  
\begin{cases}
H_0: \sigma^2= a   \\
H_1: \sigma^2 > a
\end{cases}","['statistics', 'normal-distribution', 'hypothesis-testing']"
30,How are joint probability distributions constructed from product measures?,How are joint probability distributions constructed from product measures?,,"I often see a construction in measure theory in regards to product measures. This is outlined below (taken from Wikipedia because it's very generic) Let $ (X_{1},\Sigma _{1})$ and $(X_{2},\Sigma _{2})$ be two measurable   spaces, that is, $\Sigma _{1} $ and $\Sigma _{2}$ are sigma algebras   on $X_{1}$ and $X_{2}$ respectively, and let $\mu _{1} $ and $\mu _{2}$ be measures on these spaces. Denote by $\Sigma _{1}\otimes \Sigma_{2}$ the sigma algebra > on the Cartesian product $X_{1}\times X_{2} $ generated by subsets of the form $B_{1}\times B_{2}$ , where $B_{1}\in > \Sigma _{1}$ and $B_{2}\in \Sigma _{2}.$ This sigma algebra is called   the tensor-product σ-algebra on the product space. A product measure $\mu _{1}\times \mu _{2}$ is defined to be a measure on the measurable   space $(X_{1}\times X_{2},\Sigma _{1}\otimes \Sigma _{2})$ satisfying   the property $(\mu _{1}\times \mu _{2})(B_{1}\times B_{2})=\mu > _{1}(B_{1})\mu _{2}(B_{2})$ for all $B_{1}\in \Sigma _{1},\ B_{2}\in \Sigma _{2}.$ Questions: From the perspective of probability theory, this product measure construction looks  a lot like the construction of a joint probability $P(X,Y)$ where the $X$ and $Y$ are independent. Is this assumption correct? If (1.) is correct, then how does the notion of correlation come into the structure of product measures? How is correlation built into measure theory so that it can pass onto probability theory?","I often see a construction in measure theory in regards to product measures. This is outlined below (taken from Wikipedia because it's very generic) Let and be two measurable   spaces, that is, and are sigma algebras   on and respectively, and let and be measures on these spaces. Denote by the sigma algebra > on the Cartesian product generated by subsets of the form , where and This sigma algebra is called   the tensor-product σ-algebra on the product space. A product measure is defined to be a measure on the measurable   space satisfying   the property for all Questions: From the perspective of probability theory, this product measure construction looks  a lot like the construction of a joint probability where the and are independent. Is this assumption correct? If (1.) is correct, then how does the notion of correlation come into the structure of product measures? How is correlation built into measure theory so that it can pass onto probability theory?"," (X_{1},\Sigma _{1}) (X_{2},\Sigma _{2}) \Sigma _{1}  \Sigma _{2} X_{1} X_{2} \mu _{1}  \mu _{2} \Sigma _{1}\otimes \Sigma_{2} X_{1}\times X_{2}  B_{1}\times B_{2} B_{1}\in
> \Sigma _{1} B_{2}\in \Sigma _{2}. \mu _{1}\times \mu _{2} (X_{1}\times X_{2},\Sigma _{1}\otimes \Sigma _{2}) (\mu _{1}\times \mu _{2})(B_{1}\times B_{2})=\mu
> _{1}(B_{1})\mu _{2}(B_{2}) B_{1}\in \Sigma _{1},\ B_{2}\in \Sigma _{2}. P(X,Y) X Y","['probability-theory', 'statistics', 'measure-theory', 'products']"
31,Why don't we just take the root of the numerator instead of taking the root of the whole thing in the expression for standard deviation?,Why don't we just take the root of the numerator instead of taking the root of the whole thing in the expression for standard deviation?,,The equation of the standard deviation of a dataset is given by $\sqrt{\frac{\sum{(x_{i} - \bar{x}})^2}{N}}$ . Why is that the case and why can't we use $\frac{\sqrt{\sum{(x_{i} - \bar{x}})^2}}{N}$ instead? The units line up and we don't have to worry about negatives in this case too. Thanks a million in advance!,The equation of the standard deviation of a dataset is given by . Why is that the case and why can't we use instead? The units line up and we don't have to worry about negatives in this case too. Thanks a million in advance!,\sqrt{\frac{\sum{(x_{i} - \bar{x}})^2}{N}} \frac{\sqrt{\sum{(x_{i} - \bar{x}})^2}}{N},"['statistics', 'variance', 'standard-deviation']"
32,Determine unknown probability by observing results,Determine unknown probability by observing results,,"Disclaimer: Sorry, i'm a self-taught non-native, apart from basics, i don't know the proper terms. I'm pretty sure there has to be a part of statistics that would help me deal with my problem, but somehow i'm unable to word my question well enough for google to be helpful. What if i'm observing fake coin being tossed. Say the coin's heads-chance is some random number (could be anything from 0% to 100%, all numbers equal...i hope uniform distribution is the correct term). Now i saw the coin being flipped X times (could be 10, 100 or millions) and get heads Y times. Now obviously i could say ""This is most probably Y/X head-chance coin"", but what if i wanted something like a histogram? Something that would let me say ""I'm 90% sure that coins head-chance is between 60% and 80%, 4% sure it's between 80% and 90%, 4% sure it's between 50% and 60% and so on..."" I'd appreciate even non-complete answers or pointers, ""Go study X-E-omega-effect"" or such would be helpful as well.","Disclaimer: Sorry, i'm a self-taught non-native, apart from basics, i don't know the proper terms. I'm pretty sure there has to be a part of statistics that would help me deal with my problem, but somehow i'm unable to word my question well enough for google to be helpful. What if i'm observing fake coin being tossed. Say the coin's heads-chance is some random number (could be anything from 0% to 100%, all numbers equal...i hope uniform distribution is the correct term). Now i saw the coin being flipped X times (could be 10, 100 or millions) and get heads Y times. Now obviously i could say ""This is most probably Y/X head-chance coin"", but what if i wanted something like a histogram? Something that would let me say ""I'm 90% sure that coins head-chance is between 60% and 80%, 4% sure it's between 80% and 90%, 4% sure it's between 50% and 60% and so on..."" I'd appreciate even non-complete answers or pointers, ""Go study X-E-omega-effect"" or such would be helpful as well.",,"['probability', 'statistics', 'parameter-estimation']"
33,Is my six sided dice weighted?,Is my six sided dice weighted?,,"I was playing a game with my son, and it seemed that we rolled two or five about half the time. He was wondering if the die was weighted. So I re-rolled it a bunch of times and jotted down the results: 1: 4 times 2: 12 times 3: 13 times 4: 4 times 5: 13 times 6: 8 times I realise that this is a small sample (only 54 datapoints), but to my very untrained eye, the 1 and 4 seem very low. I was wondering how I would go about determining the probability that the die is fair. I was expecting about 9 each. I did a ChiTest in excel, which I am not sure how to calculate manually and got 0.069 Does this mean that there's a 6.9% chance that the die is unweighted, and therefore a 93.1% chance it's weighted, or have I misinterpreted the results entirely?","I was playing a game with my son, and it seemed that we rolled two or five about half the time. He was wondering if the die was weighted. So I re-rolled it a bunch of times and jotted down the results: 1: 4 times 2: 12 times 3: 13 times 4: 4 times 5: 13 times 6: 8 times I realise that this is a small sample (only 54 datapoints), but to my very untrained eye, the 1 and 4 seem very low. I was wondering how I would go about determining the probability that the die is fair. I was expecting about 9 each. I did a ChiTest in excel, which I am not sure how to calculate manually and got 0.069 Does this mean that there's a 6.9% chance that the die is unweighted, and therefore a 93.1% chance it's weighted, or have I misinterpreted the results entirely?",,"['probability', 'statistics', 'dice', 'hypothesis-testing']"
34,Can this binary operation of vectors on a simplex be mapped to addition in a vector space in general?,Can this binary operation of vectors on a simplex be mapped to addition in a vector space in general?,,"I'm working with a problem where I have $N$ -dimensional vectors in the first orthant confined to the unit simplex, i.e. their components satisfy \begin{align}      v_i & > 0 \ \forall\, i\text{ and} \\      \sum_{i=1}^N v_i & = 1. \end{align} Call the space these vectors are in $\Delta^{N-1}$ for the simplex it is (excluding the boundary). Define the binary operation for $v$ , $w\in \Delta^{N-1}$ \begin{align}     v\odot w & = \frac{v_i w_i}{\sum_j v_j w_j} \\      &\equiv u. \end{align} This operation defines an abelian group. Clearly $u\in \Delta^{N-1}\ \forall \ v,\ w$ , so it is closed. It is obviously commutative. It is also associative \begin{align}    u\odot(v\odot w) & = \frac{u_i \frac{v_i w_i}{\sum_j v_j w_j}}{\sum_k u_k \frac{v_k w_k}{\sum_j v_j w_j}} \\    & = \frac{u_i v_i w_i}{\sum_k u_k v_k w_k} \\    & = (u\odot v)\odot w. \end{align} The identity element is obvious $e_i = \frac{1}{N}\ \forall\ i$ . The inverse element is likewise obvious $[v^{-1}]_i = \frac{v_i^{-1}}{\sum_{j=1}^N v_j^{-1}}$ . (The inverse element requirement is the reason for excluding the boundary vectors). Is there a mapping from $\Delta^{N-1}$ to $\mathbb{R}^{N-1}$ that maps the $\odot$ operation to vector addition? The case for $N=2$ is actually pretty straightforward. If we map vectors to \begin{align}     \phi_v &= \ln\left(\frac{v_1}{v_2}\right) \end{align} then $\phi_v + \phi_w$ will have the same value as $\ln(u_1/u_2)$ . How can this be generalized?","I'm working with a problem where I have -dimensional vectors in the first orthant confined to the unit simplex, i.e. their components satisfy Call the space these vectors are in for the simplex it is (excluding the boundary). Define the binary operation for , This operation defines an abelian group. Clearly , so it is closed. It is obviously commutative. It is also associative The identity element is obvious . The inverse element is likewise obvious . (The inverse element requirement is the reason for excluding the boundary vectors). Is there a mapping from to that maps the operation to vector addition? The case for is actually pretty straightforward. If we map vectors to then will have the same value as . How can this be generalized?","N \begin{align}
     v_i & > 0 \ \forall\, i\text{ and} \\
     \sum_{i=1}^N v_i & = 1.
\end{align} \Delta^{N-1} v w\in \Delta^{N-1} \begin{align}
    v\odot w & = \frac{v_i w_i}{\sum_j v_j w_j} \\
     &\equiv u.
\end{align} u\in \Delta^{N-1}\ \forall \ v,\ w \begin{align}
   u\odot(v\odot w) & = \frac{u_i \frac{v_i w_i}{\sum_j v_j w_j}}{\sum_k u_k \frac{v_k w_k}{\sum_j v_j w_j}} \\
   & = \frac{u_i v_i w_i}{\sum_k u_k v_k w_k} \\
   & = (u\odot v)\odot w.
\end{align} e_i = \frac{1}{N}\ \forall\ i [v^{-1}]_i = \frac{v_i^{-1}}{\sum_{j=1}^N v_j^{-1}} \Delta^{N-1} \mathbb{R}^{N-1} \odot N=2 \begin{align}
    \phi_v &= \ln\left(\frac{v_1}{v_2}\right)
\end{align} \phi_v + \phi_w \ln(u_1/u_2)","['probability', 'group-theory', 'statistics']"
35,Identifying binomial distribution for finding variance,Identifying binomial distribution for finding variance,,"If a variable $x$ takes values $0,1,2,....n$ with frequencies equal to the binomial coefficients $\binom {12}0,\binom {12}1,\binom {12}2,.....\binom {12}{12}$ , then variance of distribution is \begin{array}{|c|c|c|c|} \hline x& 0 & 1 & 2&......&12 \\ \hline f &\binom {12}0 &\binom {12}1 &\binom {12}2&......&\binom {12}{12}\\ \hline \end{array} It is solved in my reference as $$ \sigma^2=n/4=12/4=3 $$ as if it is a binomial distribution with $p=q=1/2$ . I understand it must be a binomial distribution but where do we have the clue that the probability of success in each trial is $1/2$ ? My thinking says irrespective of the probability of success the frequency of each case is the above binomial coefficients. So where am I thinking wrong about it ?","If a variable takes values with frequencies equal to the binomial coefficients , then variance of distribution is It is solved in my reference as as if it is a binomial distribution with . I understand it must be a binomial distribution but where do we have the clue that the probability of success in each trial is ? My thinking says irrespective of the probability of success the frequency of each case is the above binomial coefficients. So where am I thinking wrong about it ?","x 0,1,2,....n \binom {12}0,\binom {12}1,\binom {12}2,.....\binom {12}{12} \begin{array}{|c|c|c|c|}
\hline
x& 0 & 1 & 2&......&12 \\ \hline
f &\binom {12}0 &\binom {12}1 &\binom {12}2&......&\binom {12}{12}\\ \hline
\end{array} 
\sigma^2=n/4=12/4=3
 p=q=1/2 1/2","['probability', 'statistics']"
36,Cramer-Rao lower bound for exponential distribution,Cramer-Rao lower bound for exponential distribution,,"Given a sample $X_1,\dots , X_n$ from a population $X\sim \operatorname {Exp} (\lambda )$ , I have to calculate Cramer-Rao bounds for the estimation of $\lambda$ and $\frac 1 \lambda$ ; I also must determine if there are estimators that limit. Now, we have that $\frac {\partial}{\partial \lambda} \operatorname {ln(\lambda e ^{-\lambda x}) }= \frac 1 \lambda -x$ , and so $\mathbb E[ (\frac 1 \lambda -x)^2]=\frac 1 {\lambda^2}$ , since it is the variance by definition. So in the case that we are estimating $\lambda$ , the Cramer-Rao bound is $\frac {\lambda^2} n$ , while in the other case the bound is $\frac {\lambda^2} n \cdot \frac 1 {\lambda^4}= \frac 1 {n\lambda^2}$ . It is clear that the sample mean is an estimator for $\frac 1 \lambda$ with variance exactly $\frac 1 {n\lambda^2}$ ; however I don't know how to proved in the case of estimating $\lambda $ . If a estimator $T_n $ for $\lambda $ equals Cramer-Rao bound, the we would have $\sum_i\frac {\partial}{\partial \lambda} \operatorname {ln(\lambda e ^{-\lambda x_i}) }=K (n,\lambda) ( T_n -\lambda )$ ; so that $\sum (\frac 1 \lambda -x_i)= K (n,\lambda) ( T_n -\lambda )$ . Since we want $\lambda $ and not $\frac 1 \lambda$ , the only way is to multiply everything for $\lambda^2$ ; however with this operation we can't obtain a estimator from the $x_i $ , because we will still have a dependence from $\lambda$ . I'm not sure about th last statement: did I actually prove that there are no estimators that equal Cramer-Rao bound for $\lambda$ ? Thanks in advance for your help","Given a sample from a population , I have to calculate Cramer-Rao bounds for the estimation of and ; I also must determine if there are estimators that limit. Now, we have that , and so , since it is the variance by definition. So in the case that we are estimating , the Cramer-Rao bound is , while in the other case the bound is . It is clear that the sample mean is an estimator for with variance exactly ; however I don't know how to proved in the case of estimating . If a estimator for equals Cramer-Rao bound, the we would have ; so that . Since we want and not , the only way is to multiply everything for ; however with this operation we can't obtain a estimator from the , because we will still have a dependence from . I'm not sure about th last statement: did I actually prove that there are no estimators that equal Cramer-Rao bound for ? Thanks in advance for your help","X_1,\dots , X_n X\sim \operatorname {Exp} (\lambda ) \lambda \frac 1 \lambda \frac {\partial}{\partial \lambda} \operatorname {ln(\lambda e ^{-\lambda x}) }= \frac 1 \lambda -x \mathbb E[ (\frac 1 \lambda -x)^2]=\frac 1 {\lambda^2} \lambda \frac {\lambda^2} n \frac {\lambda^2} n \cdot \frac 1 {\lambda^4}= \frac 1 {n\lambda^2} \frac 1 \lambda \frac 1 {n\lambda^2} \lambda  T_n  \lambda  \sum_i\frac {\partial}{\partial \lambda} \operatorname {ln(\lambda e ^{-\lambda x_i}) }=K (n,\lambda) ( T_n -\lambda ) \sum (\frac 1 \lambda -x_i)= K (n,\lambda) ( T_n -\lambda ) \lambda  \frac 1 \lambda \lambda^2 x_i  \lambda \lambda",['statistics']
37,How to derive the probability density function (PDF) of a continuous random variable from a set of data?,How to derive the probability density function (PDF) of a continuous random variable from a set of data?,,"I am interested to derive an expression for the probability density function (PDF) of a continuous random variable from a given set of data. To further explain, let us consider that we have the data of time spent by visitors to a web page for a 24 hours period. At certain hours, say during the busy hours of day, the time spent on the web page is short. However, in the afternoon the time spent is long. I would like to derive an expression for the PDF of the continuous random variable X representing the time spent by the visitor, such as, $$ f_X(x)= \begin{cases}  24x-x^2, \quad x > 0\\ 0, \quad\quad\quad\quad \text{otherwise.} \end{cases} $$ This is only an assumed PDF. I have tried to search but did not find an appropriate answer to this question. Most of the books on probability teach you how to derive probability values when given a PDF and all other sorts of things. However, the PDF is always given or assumed. So, my questions are: Do we always assume or try to map a suitable PDF from the set of popular distributions, such as Gaussian, exponential, log normal and so on for a given set of data? If yes, is there any standard way to do this? Is it possible to derive a mathematical equation for the PDF of the random variable from a given set of sample data? If yes, how this could be done? Is there any branch of Statistics and Probability Theory dealing with this? I would much appreciate any answers to these questions. Pointers to any resources or books or chapters will also be helpful. Thanks in advance for help.","I am interested to derive an expression for the probability density function (PDF) of a continuous random variable from a given set of data. To further explain, let us consider that we have the data of time spent by visitors to a web page for a 24 hours period. At certain hours, say during the busy hours of day, the time spent on the web page is short. However, in the afternoon the time spent is long. I would like to derive an expression for the PDF of the continuous random variable X representing the time spent by the visitor, such as, This is only an assumed PDF. I have tried to search but did not find an appropriate answer to this question. Most of the books on probability teach you how to derive probability values when given a PDF and all other sorts of things. However, the PDF is always given or assumed. So, my questions are: Do we always assume or try to map a suitable PDF from the set of popular distributions, such as Gaussian, exponential, log normal and so on for a given set of data? If yes, is there any standard way to do this? Is it possible to derive a mathematical equation for the PDF of the random variable from a given set of sample data? If yes, how this could be done? Is there any branch of Statistics and Probability Theory dealing with this? I would much appreciate any answers to these questions. Pointers to any resources or books or chapters will also be helpful. Thanks in advance for help.","
f_X(x)=
\begin{cases} 
24x-x^2, \quad x > 0\\
0, \quad\quad\quad\quad \text{otherwise.}
\end{cases}
","['statistics', 'probability-distributions', 'random-variables', 'density-function', 'sampling']"
38,Probabilty that none of n i.i.d uniform samples from R^2 are larger in both coordinates than first.,Probabilty that none of n i.i.d uniform samples from R^2 are larger in both coordinates than first.,,"I am randomly sampling n elements from a square within R^2. They are independently uniformly distributed. What is the probability that for any arbitrarily but fixed previously selected sample index, it is true that no other sample is greater in both coordinates? I do not know anything else about the preselected sample. This is part of a larger problem which I have reduced to this question, I can explain it should it be neccessary.","I am randomly sampling n elements from a square within R^2. They are independently uniformly distributed. What is the probability that for any arbitrarily but fixed previously selected sample index, it is true that no other sample is greater in both coordinates? I do not know anything else about the preselected sample. This is part of a larger problem which I have reduced to this question, I can explain it should it be neccessary.",,"['combinatorics', 'statistics', 'order-statistics']"
39,Probability about lifetime of 100 bulbs (exponential distribution),Probability about lifetime of 100 bulbs (exponential distribution),,"I have some doubts about the following problem: I have 100 bulbs with a lifetime represented by an exponential distribution, with an expected value of 1000 hours. Find the probability that, at least one bulb, blown down after at most 500 hours. I have calculated the probability about one bulb with this method: $P(X \leq 500)=\int_{0}^{500}\lambda e^{-\lambda x}dx = 1-e^{\frac{1}{2}} = 0.394$ now, how can I extend this method for all the 100 bulbs? A step-by-step solution is really appreciated, I'm really newbie about statistics/probability arguments. Thank you so much and best regards. EDIT: $\frac{1}{\lambda}=1000$ hours so $ \lambda = \frac{1}{1000} $","I have some doubts about the following problem: I have 100 bulbs with a lifetime represented by an exponential distribution, with an expected value of 1000 hours. Find the probability that, at least one bulb, blown down after at most 500 hours. I have calculated the probability about one bulb with this method: now, how can I extend this method for all the 100 bulbs? A step-by-step solution is really appreciated, I'm really newbie about statistics/probability arguments. Thank you so much and best regards. EDIT: hours so",P(X \leq 500)=\int_{0}^{500}\lambda e^{-\lambda x}dx = 1-e^{\frac{1}{2}} = 0.394 \frac{1}{\lambda}=1000  \lambda = \frac{1}{1000} ,"['probability', 'statistics', 'random-variables', 'exponential-distribution']"
40,Testing a discrete random variable for uniformity through some order statistics,Testing a discrete random variable for uniformity through some order statistics,,"I would like to develop a simple test for the uniform distribution of a discrete random variable, but I did not manage to find on Wikipedia or here the relevant informations, and I am pretty sure that someone will be able to help me. Let us assume that an experiment has $n$ possible outcomes, $\{1,2,\ldots,n\}$ , all with the same probability. Once we perform $n^3$ experiments, we denote as $M$ the number of experiments leading to the most successful outcome, $L$ the number of experiments leading to the least successful outcome. What is the average value of $M-L$ ? What is the distribution of $M-L$ ? I would guess that the typical outcome has frequency $n^2\pm cn$ , such that $M-L$ is expected to be $2cn$ for some explicit constant $c$ . I am not really sure about the second point, I am just guessing a Beta distribution of some sort.","I would like to develop a simple test for the uniform distribution of a discrete random variable, but I did not manage to find on Wikipedia or here the relevant informations, and I am pretty sure that someone will be able to help me. Let us assume that an experiment has possible outcomes, , all with the same probability. Once we perform experiments, we denote as the number of experiments leading to the most successful outcome, the number of experiments leading to the least successful outcome. What is the average value of ? What is the distribution of ? I would guess that the typical outcome has frequency , such that is expected to be for some explicit constant . I am not really sure about the second point, I am just guessing a Beta distribution of some sort.","n \{1,2,\ldots,n\} n^3 M L M-L M-L n^2\pm cn M-L 2cn c","['probability', 'statistics', 'statistical-inference', 'uniform-distribution', 'order-statistics']"
41,Binomial in Statistics,Binomial in Statistics,,"I was asked a question; A student was late for college 0.25 of the time, what is the probability he is late 4 days in one college week. My answer was this: L = Late, N = Not Late L = 0.25, N = 0.75 if its a 7 day week P(L+N)7 = L7 + (7C1)L6 N + (7C2)L5N2+(7C3)L4N3+(7C4)L3N4+… P(4L, 3N) = (7C3)(0.25)4(0.75)3 = 0.057 If it’s a 5 day week P(L+N)5 = L5+ (5C1)L4 N + (5C2)L3N2+(5C3)L2N3+(5C4)LN4+… P(4L1N) = (5C1)(0.25)4(0.75) = 0.0146 Have I got this ok?","I was asked a question; A student was late for college 0.25 of the time, what is the probability he is late 4 days in one college week. My answer was this: L = Late, N = Not Late L = 0.25, N = 0.75 if its a 7 day week P(L+N)7 = L7 + (7C1)L6 N + (7C2)L5N2+(7C3)L4N3+(7C4)L3N4+… P(4L, 3N) = (7C3)(0.25)4(0.75)3 = 0.057 If it’s a 5 day week P(L+N)5 = L5+ (5C1)L4 N + (5C2)L3N2+(5C3)L2N3+(5C4)LN4+… P(4L1N) = (5C1)(0.25)4(0.75) = 0.0146 Have I got this ok?",,['statistics']
42,How to find quartiles using histogram?,How to find quartiles using histogram?,,"Can you suggest how can I find first and third quartiles and median after building histogram from raw data? I can sort the data and find the values, but still how it can be done using the chart...","Can you suggest how can I find first and third quartiles and median after building histogram from raw data? I can sort the data and find the values, but still how it can be done using the chart...",,['statistics']
43,Can the expected value be applied to non-linear functions?,Can the expected value be applied to non-linear functions?,,"I'm doing a research project about the bias of different methods of integral estimation.  One of these methods involves the following math: $$ E\left[\sum_{i=1}^nf(x_i)\right] = \sum_{i=1}^nf(E[x_i])$$ for $ x_1, x_2,\ldots, x_n \sim G$ are i.i.d where $G$ is some probability distribution function. Does this math only hold when f is a linear function? Does it never hold? Does it always hold?",I'm doing a research project about the bias of different methods of integral estimation.  One of these methods involves the following math: for are i.i.d where is some probability distribution function. Does this math only hold when f is a linear function? Does it never hold? Does it always hold?," E\left[\sum_{i=1}^nf(x_i)\right] = \sum_{i=1}^nf(E[x_i])  x_1, x_2,\ldots, x_n \sim G G","['statistics', 'expected-value']"
44,Maximum likelihood estimation of parameter $N$,Maximum likelihood estimation of parameter,N,"Every competitor in a marathon has a unique number on their shirt, from 1 to N. N is unknown. The observation is $n_1, \ldots ,n_K$ , which are randomly sampled from the $N$ competitors with equal probability. What is the MLE for $N$ ? My intuition is that the MLE is simply the maximum observed in that set but how do I prove this? At first I thought this was a multinomial but that doesn't make sense since there is a single observation or there would be K observations without replacement. Is this a categorical distribution? How do I derive the MLE for that?","Every competitor in a marathon has a unique number on their shirt, from 1 to N. N is unknown. The observation is , which are randomly sampled from the competitors with equal probability. What is the MLE for ? My intuition is that the MLE is simply the maximum observed in that set but how do I prove this? At first I thought this was a multinomial but that doesn't make sense since there is a single observation or there would be K observations without replacement. Is this a categorical distribution? How do I derive the MLE for that?","n_1, \ldots ,n_K N N","['statistics', 'statistical-inference', 'maximum-likelihood', 'parameter-estimation', 'multinomial-distribution']"
45,Unbiased estimator of probability density function,Unbiased estimator of probability density function,,"Given a random variable $X \sim f(x) = \frac{1}{x\log (\theta)}$ when $1 < x < \theta$ , I am trying to find an unbiased estimator of a simple random sample of size $n$ for $d(\theta) = \log (\theta)$ . The pdf of the simple random sample is $f(x_1, \dots, x_n) = \frac{\operatorname{I}_{(-\infty, \theta)}(X_{(n)})\operatorname{I}_{(1, \infty)}(X_{(1)})}{\log ^n(\theta)\prod_{i=0}^n x_i}$ I have tried every single candidate I can think of, but cannot find any suitable function. The candidates I have tried so far are: 1) The maximum, $T(X_1, \dots, X_n) = X_{(n)}$ , whose pdf is given by: \begin{equation} P_T(X_{(n)} \leq Y) = P_X(X_i \leq Y\quad\forall i\in\{1,\dots,n\}) = (P_X(X\leq Y))^n \implies\\ f_T(x_1,\dots,x_n) = n(P_X(X\leq Y))^{n-1}f_X(x_1,\dots,x_n) = \frac{n\log ^{n-1}(x)}{x\log ^n(\theta)} \end{equation} The expected value of $T$ is thus: \begin{equation} E[X_{(n)}] =\int_1^\theta x\frac{n\log ^{n-1}(x)}{x\log ^n(\theta)} dx = \frac{n}{\log ^n(\theta)}\int_1^\theta \log ^{n-1}(x) \end{equation} which is not $\log (\theta)$ . 2) $T(X_1, \dots, X_n) = \frac{1}{\prod_{i=0}^n X_i}$ . Since the random variables are independent: \begin{equation} E\left[\frac{1}{\prod_{i=0}^n X_i}\right] = \left(E\left[\frac{1}{X}\right]\right)^n = \left(\int_1^\theta \frac{1}{x^2\log (\theta)} dx\right)^n = \frac{1}{\log ^n(\theta)}\left(1-\frac{1}{\theta}\right)^n \neq \log (\theta) \end{equation} 3) The minimum, $T(X_1, \dots, X_n) = X_{(1)}$ , whose pdf is given by: \begin{equation} P_T(X_{(1)} \leq Y) = 1 - P_T(X_{(1)} > Y) = 1 - P_X(X_i > Y\quad\forall i\in\{1,\dots,n\}) = 1 - (P_X(X> Y))^n = 1 - (1 - P_X(X \leq Y))^n \implies\\ f_T(x_1,\dots,x_n) = n(1 - P_X(X\leq Y))^{n-1}f_X(x_1,\dots,x_n) = n\left(1-\frac{\log (x)}{\log (\theta)}\right)^{n-1}\frac{1}{x\log (\theta)} \end{equation} The expected value of $T$ is thus: \begin{equation} E[X_{(1)}] =\int_1^\theta xn\left(1-\frac{\log (x)}{\log (\theta)}\right)^{n-1}\frac{1}{x\log (\theta)} dx = \frac{n}{\log ^n(\theta)}\int_1^\theta \log ^{n-1}(x) = \frac{n}{\log ^n(\theta)}\int_1^\theta (\log (\theta)-\log (x))^{n-1} \end{equation} which, again, is not $\log (\theta)$ . Can anyone shine a light on how to calculate de MVUE? Thank you.","Given a random variable when , I am trying to find an unbiased estimator of a simple random sample of size for . The pdf of the simple random sample is I have tried every single candidate I can think of, but cannot find any suitable function. The candidates I have tried so far are: 1) The maximum, , whose pdf is given by: The expected value of is thus: which is not . 2) . Since the random variables are independent: 3) The minimum, , whose pdf is given by: The expected value of is thus: which, again, is not . Can anyone shine a light on how to calculate de MVUE? Thank you.","X \sim f(x) = \frac{1}{x\log (\theta)} 1 < x < \theta n d(\theta) = \log (\theta) f(x_1, \dots, x_n) = \frac{\operatorname{I}_{(-\infty, \theta)}(X_{(n)})\operatorname{I}_{(1, \infty)}(X_{(1)})}{\log ^n(\theta)\prod_{i=0}^n x_i} T(X_1, \dots, X_n) = X_{(n)} \begin{equation}
P_T(X_{(n)} \leq Y) = P_X(X_i \leq Y\quad\forall i\in\{1,\dots,n\}) = (P_X(X\leq Y))^n \implies\\ f_T(x_1,\dots,x_n) = n(P_X(X\leq Y))^{n-1}f_X(x_1,\dots,x_n) = \frac{n\log ^{n-1}(x)}{x\log ^n(\theta)}
\end{equation} T \begin{equation}
E[X_{(n)}] =\int_1^\theta x\frac{n\log ^{n-1}(x)}{x\log ^n(\theta)} dx = \frac{n}{\log ^n(\theta)}\int_1^\theta \log ^{n-1}(x)
\end{equation} \log (\theta) T(X_1, \dots, X_n) = \frac{1}{\prod_{i=0}^n X_i} \begin{equation}
E\left[\frac{1}{\prod_{i=0}^n X_i}\right] = \left(E\left[\frac{1}{X}\right]\right)^n = \left(\int_1^\theta \frac{1}{x^2\log (\theta)} dx\right)^n = \frac{1}{\log ^n(\theta)}\left(1-\frac{1}{\theta}\right)^n \neq \log (\theta)
\end{equation} T(X_1, \dots, X_n) = X_{(1)} \begin{equation}
P_T(X_{(1)} \leq Y) = 1 - P_T(X_{(1)} > Y) = 1 - P_X(X_i > Y\quad\forall i\in\{1,\dots,n\}) = 1 - (P_X(X> Y))^n = 1 - (1 - P_X(X \leq Y))^n \implies\\ f_T(x_1,\dots,x_n) = n(1 - P_X(X\leq Y))^{n-1}f_X(x_1,\dots,x_n) = n\left(1-\frac{\log (x)}{\log (\theta)}\right)^{n-1}\frac{1}{x\log (\theta)}
\end{equation} T \begin{equation}
E[X_{(1)}] =\int_1^\theta xn\left(1-\frac{\log (x)}{\log (\theta)}\right)^{n-1}\frac{1}{x\log (\theta)} dx = \frac{n}{\log ^n(\theta)}\int_1^\theta \log ^{n-1}(x) = \frac{n}{\log ^n(\theta)}\int_1^\theta (\log (\theta)-\log (x))^{n-1}
\end{equation} \log (\theta)","['statistics', 'variance']"
46,Why we can draw a random number $s$ in a normal distribution when we are taught that $p(s)=0$?,Why we can draw a random number  in a normal distribution when we are taught that ?,s p(s)=0,"In various programming languages such as Matlab and Python, we can draw a random number s from virtually any continuous distribution, such as the normal and uniform. But we're also taught that $p(s)=0$ for any specific value $s$ in the support of a continuous distribution. So, my question is, how can we explain this paradox? Thank you very much!","In various programming languages such as Matlab and Python, we can draw a random number s from virtually any continuous distribution, such as the normal and uniform. But we're also taught that for any specific value in the support of a continuous distribution. So, my question is, how can we explain this paradox? Thank you very much!",p(s)=0 s,"['probability', 'statistics', 'probability-distributions', 'programming']"
47,A corollary of the Chernoff bound,A corollary of the Chernoff bound,,"During my Statistic course, we were asked the following question: Let $ X_1, \ldots , X_n $ be a $n$ observations that are i.i.d and assume $ X_i \sim \mathcal{N} (0,\sigma^2) $ . Use the Chernoff Bound, i.e. $$ \Pr( X \geq t ) \leq \frac{E(e^{\lambda X})}{e^{\lambda t}} $$ And the fact that the Moment Generating Function of $X_i$ is $$ M_{X_i} = E(e^{\lambda X_i}) = E(e^{\frac{1}{2} \sigma^2 \lambda^2}) $$ to prove that, for all $ t > 0$ $$ \Pr\left( \frac{1}{n} \sum_i^n X_i \geq t \right) \leq  e^{-n \frac{t^2}{2\sigma^2} } .$$ Using the MGM of the mean, I have: $$ \Pr\left( \frac{1}{n} \sum_i^n X_i \geq t \right) \leq  \frac{e^{-n^2 \frac{1}{2}\sigma^2 \lambda^2 }}{e^{\lambda t}} $$ (If I didn't miscalculate something). But I can't get any further...","During my Statistic course, we were asked the following question: Let be a observations that are i.i.d and assume . Use the Chernoff Bound, i.e. And the fact that the Moment Generating Function of is to prove that, for all Using the MGM of the mean, I have: (If I didn't miscalculate something). But I can't get any further..."," X_1, \ldots , X_n  n  X_i \sim \mathcal{N} (0,\sigma^2)   \Pr( X \geq t ) \leq \frac{E(e^{\lambda X})}{e^{\lambda t}}  X_i  M_{X_i} = E(e^{\lambda X_i}) = E(e^{\frac{1}{2} \sigma^2 \lambda^2})   t > 0  \Pr\left( \frac{1}{n} \sum_i^n X_i \geq t \right) \leq  e^{-n \frac{t^2}{2\sigma^2} } .  \Pr\left( \frac{1}{n} \sum_i^n X_i \geq t \right) \leq  \frac{e^{-n^2 \frac{1}{2}\sigma^2 \lambda^2 }}{e^{\lambda t}} ","['probability-theory', 'statistics', 'inequality', 'proof-writing', 'solution-verification']"
48,Notation for assigning observations to a random variable,Notation for assigning observations to a random variable,,"I want to assign some observations to a variable $X$ , but I am unsure what the correct notation (if any) would be. At the moment I am doing it like this: For the following set of observations for X, $$1,8,1,5,8,6,3,3,3,7$$ ... But I was wondering if there was any actual notation where I could write something like this: For the following set of observations $$X = \{1,8,1,5,8,6,3,3,3,7\}$$ ... I dont think the latter is correct, since the random variable $X$ is not actually a set, but I wanted to know if there was any sort of similar notation to indicate the values observed for $X$ .","I want to assign some observations to a variable , but I am unsure what the correct notation (if any) would be. At the moment I am doing it like this: For the following set of observations for X, ... But I was wondering if there was any actual notation where I could write something like this: For the following set of observations ... I dont think the latter is correct, since the random variable is not actually a set, but I wanted to know if there was any sort of similar notation to indicate the values observed for .","X 1,8,1,5,8,6,3,3,3,7 X = \{1,8,1,5,8,6,3,3,3,7\} X X","['statistics', 'notation', 'random-variables']"
49,What is the inverse cdf / ppf of the logit-normal distribution?,What is the inverse cdf / ppf of the logit-normal distribution?,,"In this post , I am trying to implement the logit-normal distribution in Python. The provided answer works for me, however, the rvs method that draws random variates failes for me. According to the documentation of the pdf class that I am using: ""The default method _rvs relies on the inverse of the cdf, _ppf, applied to a uniform random variate. In order to generate random variates efficiently, either the default _ppf needs to be overwritten (e.g. if the inverse cdf can expressed in an explicit form) or a sampling method needs to be implemented in a custom _rvs method."" This is what I am trying to figure out, but I couldn't find a description of the inverse logit-normal cdf anywhere. How do I do this?","In this post , I am trying to implement the logit-normal distribution in Python. The provided answer works for me, however, the rvs method that draws random variates failes for me. According to the documentation of the pdf class that I am using: ""The default method _rvs relies on the inverse of the cdf, _ppf, applied to a uniform random variate. In order to generate random variates efficiently, either the default _ppf needs to be overwritten (e.g. if the inverse cdf can expressed in an explicit form) or a sampling method needs to be implemented in a custom _rvs method."" This is what I am trying to figure out, but I couldn't find a description of the inverse logit-normal cdf anywhere. How do I do this?",,"['statistics', 'probability-distributions', 'python']"
50,Find the best unbiased estimator of $\theta^2e^{-\theta}$ from a Poi($\theta$) sample.,Find the best unbiased estimator of  from a Poi() sample.,\theta^2e^{-\theta} \theta,"Suppose $X_1....X_n$ is a random sample from a Poi( $\theta$ ) population.  Find the best unbiased estimator of $\theta^2e^{-\theta}$ My attempt: Let $\sum_1^nX_i=T$ .  We know $T$ is complete and sufficient. So we seek an unbiased estimator of $\theta^2e^{-\theta}$ then condition it on T then find the expected value. An unbiased estimator of $\theta^2e^{-\theta}$ is $2 \chi_{[X_1=2]}$ We calculate $E(2\chi_{[X_1=2]}\mid T=t) = 2\Pr(X=2\mid T=t)$ By Bayes this is $$2(tC2) \left( 1-\frac{1}{n}\right)^{t-2} \left( \frac{1}{n} \right)^2$$ I'm very unsure of this result. It matches none of my classmates, but I cannot see where I'm making an error.  I also would be curious to see alternative approaches.","Suppose is a random sample from a Poi( ) population.  Find the best unbiased estimator of My attempt: Let .  We know is complete and sufficient. So we seek an unbiased estimator of then condition it on T then find the expected value. An unbiased estimator of is We calculate By Bayes this is I'm very unsure of this result. It matches none of my classmates, but I cannot see where I'm making an error.  I also would be curious to see alternative approaches.",X_1....X_n \theta \theta^2e^{-\theta} \sum_1^nX_i=T T \theta^2e^{-\theta} \theta^2e^{-\theta} 2 \chi_{[X_1=2]} E(2\chi_{[X_1=2]}\mid T=t) = 2\Pr(X=2\mid T=t) 2(tC2) \left( 1-\frac{1}{n}\right)^{t-2} \left( \frac{1}{n} \right)^2,"['statistics', 'solution-verification', 'statistical-inference', 'parameter-estimation']"
51,How is Kolmogorov–Smirnov statistic non-constant?,How is Kolmogorov–Smirnov statistic non-constant?,,I'm reading about Kolmogorov–Smirnov test . The Kolmogorov–Smirnov statistic is given by $$D_{n}=\sup _{x}\left|F_{n}(x)-F(x)\right|$$ It seems to me that $D_n$ is a constant for each $n$ . Could you please elaborate on how $D_n$ is non constant?,I'm reading about Kolmogorov–Smirnov test . The Kolmogorov–Smirnov statistic is given by It seems to me that is a constant for each . Could you please elaborate on how is non constant?,D_{n}=\sup _{x}\left|F_{n}(x)-F(x)\right| D_n n D_n,"['statistics', 'probability-distributions', 'hypothesis-testing', 'probability-limit-theorems']"
52,What is the difference between min and argmin in the context of random variables,What is the difference between min and argmin in the context of random variables,,"Consider the indepdently distributed expenontial variables $T_1,T_2,T_3,...T_N$ and let $M=\min_{i=1,...,N} T_i$ and $i_s=arg min_{i=1,...,N} T_i$ . Now I understand the difference between min f(x) and argmin f(x) in the context of deterministic function, but in the context of random variables I'm lost as to the difference. Isn't the argument the variables themselves? So let's say that the outcome of $T_3$ is the smallest of the set, so that the outcome of $M$ is $T_3$ , then isn't the outcome of $i_s$ also $T_3$ ?","Consider the indepdently distributed expenontial variables and let and . Now I understand the difference between min f(x) and argmin f(x) in the context of deterministic function, but in the context of random variables I'm lost as to the difference. Isn't the argument the variables themselves? So let's say that the outcome of is the smallest of the set, so that the outcome of is , then isn't the outcome of also ?","T_1,T_2,T_3,...T_N M=\min_{i=1,...,N} T_i i_s=arg min_{i=1,...,N} T_i T_3 M T_3 i_s T_3","['probability', 'statistics']"
53,Branching Process $P[Z_3 > 0]$ where $𝑍_3$ is the size of the third generation.,Branching Process  where  is the size of the third generation.,P[Z_3 > 0] 𝑍_3,"In a branching process, suppose $𝑃(𝑠) = 𝑝 + 𝑞𝑠^2$ for $0 < 𝑝 < 1$ and $𝑝 + 𝑞 = 1$ . Assume that the population starts with one ancestor. Find $𝑃[𝑍_3 > 0]$ where $𝑍_3$ is the size of the third generation. For this question, I compute that $P(S)=p+qs^{2}$ follows that $s=p+qs^2$ . Therefore the probability of extinction is $s=\frac{1-\sqrt{1-4pq}}{2q}$ and that the mean progeny is $P'(1)=2q$ . However, I honestly don't know how to move from this to find $P[Z_3 > 0]$ where $𝑍_3$ is the size of the third generation.","In a branching process, suppose for and . Assume that the population starts with one ancestor. Find where is the size of the third generation. For this question, I compute that follows that . Therefore the probability of extinction is and that the mean progeny is . However, I honestly don't know how to move from this to find where is the size of the third generation.",𝑃(𝑠) = 𝑝 + 𝑞𝑠^2 0 < 𝑝 < 1 𝑝 + 𝑞 = 1 𝑃[𝑍_3 > 0] 𝑍_3 P(S)=p+qs^{2} s=p+qs^2 s=\frac{1-\sqrt{1-4pq}}{2q} P'(1)=2q P[Z_3 > 0] 𝑍_3,"['probability', 'statistics', 'stochastic-processes']"
54,Why do we use determinant for multivariate normal distribution?,Why do we use determinant for multivariate normal distribution?,,"While learning statistics, I have a question why is the determinant used in the multivariate normal distribution. When I look for the answer on the internet, so far every answer I looked at was basically saying that it works, so we use that. But what I want is if there is a mathematical relation between multivariate normal distribution and determinant (volume factor of linear transformation or some other definition). There was one answer that by using determinant we can make the integral of the density over $R^{n}$ equal to $1$ . This sounds nice, but if there is another intuition, please share it.","While learning statistics, I have a question why is the determinant used in the multivariate normal distribution. When I look for the answer on the internet, so far every answer I looked at was basically saying that it works, so we use that. But what I want is if there is a mathematical relation between multivariate normal distribution and determinant (volume factor of linear transformation or some other definition). There was one answer that by using determinant we can make the integral of the density over equal to . This sounds nice, but if there is another intuition, please share it.",R^{n} 1,"['linear-algebra', 'statistics', 'normal-distribution', 'determinant']"
55,Likelihood Ratio Test for the Normal Distribution with unknown mean,Likelihood Ratio Test for the Normal Distribution with unknown mean,,"Let $(X_{1},...,X_{n})$ a $n$ sample of the law $N( \mu, \sigma^{2})$ . We assumed we don't know $\mu$ and $\sigma^{2}$ . Let $\mu_{0} \in \mathbb{R}$ . Show that the Likelihood-ratio test for $\mu = \mu_{0}$ against $\mu  \ne \mu_{0}$ is function of $$ 1 + \frac{(\bar{X_{n}} - \mu_{0}  )^{2}}{\sigma_{n}^{2}} $$ with $\sigma_{n}^{2}  = \sum_{i=1}^{n}  \frac{(X_{i} - \bar{X_{n}})^{2}}{n}$ EDIT : I showed that the Likelihood-ratio test is $$ \exp\left( \frac{n}{2 s_{n}^{2}(X)} (\bar{X_{n}} - \mu_{0} ) \right) $$ with $s_{n}^{2}(X) = \sum_{i=1}^n\frac{(X_i-\mu_0)^2}{n}$ But I don't know how to conclude. Thanks and regards.",Let a sample of the law . We assumed we don't know and . Let . Show that the Likelihood-ratio test for against is function of with EDIT : I showed that the Likelihood-ratio test is with But I don't know how to conclude. Thanks and regards.,"(X_{1},...,X_{n}) n N( \mu, \sigma^{2}) \mu \sigma^{2} \mu_{0} \in \mathbb{R} \mu = \mu_{0} \mu
 \ne \mu_{0}  1 + \frac{(\bar{X_{n}} - \mu_{0}
 )^{2}}{\sigma_{n}^{2}}  \sigma_{n}^{2}  = \sum_{i=1}^{n}
 \frac{(X_{i} - \bar{X_{n}})^{2}}{n} 
\exp\left( \frac{n}{2 s_{n}^{2}(X)} (\bar{X_{n}} - \mu_{0} ) \right)
 s_{n}^{2}(X) = \sum_{i=1}^n\frac{(X_i-\mu_0)^2}{n}",['statistics']
56,Venn diagramm: at least one not,Venn diagramm: at least one not,,"I have trouble understanding the solution for a venn-diagram exercise our statistics professor gave us.  The question is: Of the three events $A, B, C\subset W$ occurs at least one not . Write the   corresponding event and draw the venn diagram. The correct solution was: $(\overline {A\text{ }\cup B\text{ } \cup C}\text{ })$ My solution was: $(\overline{A\text{ }\cap B\text{ }\cap\text{ }C}\text{ })$ I just can't wrap my head around where I went wrong. If only A, or A and B happens for example, the requirement of one not is still met, isn't it? So the overlap of one or only one should be colored in too. That's at least my thought","I have trouble understanding the solution for a venn-diagram exercise our statistics professor gave us.  The question is: Of the three events occurs at least one not . Write the   corresponding event and draw the venn diagram. The correct solution was: My solution was: I just can't wrap my head around where I went wrong. If only A, or A and B happens for example, the requirement of one not is still met, isn't it? So the overlap of one or only one should be colored in too. That's at least my thought","A, B, C\subset W (\overline {A\text{ }\cup B\text{ } \cup C}\text{ }) (\overline{A\text{ }\cap B\text{ }\cap\text{ }C}\text{ })","['statistics', 'elementary-set-theory']"
57,Why is Standard Deviation not defined as the expected value of the distances of points from mean? [duplicate],Why is Standard Deviation not defined as the expected value of the distances of points from mean? [duplicate],,This question already has answers here : Motivation behind standard deviation? (6 answers) Closed 4 years ago . Variance is defined as $$V(X) = \sum (x-\mu)^2 .p(x)$$ And standard deviation is $\sigma_X = \sqrt{V(x)}$ But I feel it makes more sense to define $\sigma_X$ as $\sum( |x-\mu|.p(x))$ instead because the mod takes care of negative distances and multiplication by p(x) would give us expected value of the deviation we should expect. Then why is SD defined the way it is?,This question already has answers here : Motivation behind standard deviation? (6 answers) Closed 4 years ago . Variance is defined as And standard deviation is But I feel it makes more sense to define as instead because the mod takes care of negative distances and multiplication by p(x) would give us expected value of the deviation we should expect. Then why is SD defined the way it is?,V(X) = \sum (x-\mu)^2 .p(x) \sigma_X = \sqrt{V(x)} \sigma_X \sum( |x-\mu|.p(x)),"['statistics', 'variance', 'standard-deviation']"
58,"Given the distance $d$ between two random points on a segment, find the MLE of its length $L$.","Given the distance  between two random points on a segment, find the MLE of its length .",d L,"I could solve this problem, but I made a logic jump that I'm not sure how to justify. We have a segment of length $L$ , and two random points with uniform distribution over that segment. That is, $p_{X_{1}X_{2}}(x_{1}, x_{2}) = \frac{1}{L^2}$ . The given is the distance between those two points, so $d = |x_{1} - x_{2}|$ . I don't know any easy way of dealing with the absolute value, so I took a leap of faith and assumed that $x_{1}$ is the farthest point from $0$ , so $d = x_{1} - x_{2}$ . Then I use the delta method: $$\begin{align}  p_{D}(d) &= \iint p_{X_{1}X_{2}}(x_{1}, x_{2})\delta(d - x_{1} + x_{2})dx_{1}dx_{2} \\ &= \int p_{X_{1}X_{2}}(d + x_{2}, x_{2}) dx_{2} \\ &= \int_{-d} ^{L-d}\frac{1}{L^2}\mathbb{1}\{0 < x_{2} < L\}dx_{2} \\ &= \int_{0} ^{L-d}\frac{1}{L^2}dx_{2} \\ &=  \frac{1}{L} - \frac{d}{L^2} \end{align}$$ Then: $$\frac{dp_{D}(d)}{dL} = \frac{2d}{\hat{L}^3} - \frac{1}{\hat{L}^2} = 0 \iff \hat{L} = 2d$$ According to my teacher this is right, but I feel like I ignored half of the possible cases by getting rid of the absolute value in $d$ . Why does this yield the same result as if I solved the problem with $d = |x_{1} - x_{2}|$ ? Thanks.","I could solve this problem, but I made a logic jump that I'm not sure how to justify. We have a segment of length , and two random points with uniform distribution over that segment. That is, . The given is the distance between those two points, so . I don't know any easy way of dealing with the absolute value, so I took a leap of faith and assumed that is the farthest point from , so . Then I use the delta method: Then: According to my teacher this is right, but I feel like I ignored half of the possible cases by getting rid of the absolute value in . Why does this yield the same result as if I solved the problem with ? Thanks.","L p_{X_{1}X_{2}}(x_{1}, x_{2}) = \frac{1}{L^2} d = |x_{1} - x_{2}| x_{1} 0 d = x_{1} - x_{2} \begin{align}
 p_{D}(d) &= \iint p_{X_{1}X_{2}}(x_{1}, x_{2})\delta(d - x_{1} + x_{2})dx_{1}dx_{2} \\ &= \int p_{X_{1}X_{2}}(d + x_{2}, x_{2}) dx_{2} \\ &= \int_{-d} ^{L-d}\frac{1}{L^2}\mathbb{1}\{0 < x_{2} < L\}dx_{2} \\ &= \int_{0} ^{L-d}\frac{1}{L^2}dx_{2} \\ &=  \frac{1}{L} - \frac{d}{L^2}
\end{align} \frac{dp_{D}(d)}{dL} = \frac{2d}{\hat{L}^3} - \frac{1}{\hat{L}^2} = 0 \iff \hat{L} = 2d d d = |x_{1} - x_{2}|","['probability', 'statistics', 'probability-distributions', 'random-variables', 'maximum-likelihood']"
59,Sufficiency for AR(1) model,Sufficiency for AR(1) model,,"Consider the following AR(1) model: $$X_1=\epsilon_0\,,\,X_t=\rho X_{t-1} + \epsilon_t\,,$$ where $t=2,3\ldots,n$ and $\epsilon_t \sim N(0 , \sigma^2)$ independently. It is given that $|\rho| <1$ . Let $X_1,X_2,\ldots,X_n$ be a random sample drawn from this model. Find the minimal sufficient statistic for this model. The thing is I cannot write the joint distribution of $X_i$ 's for this model explicitly as it is necessary to determine whether it belongs to the exponential family. Please help.",Consider the following AR(1) model: where and independently. It is given that . Let be a random sample drawn from this model. Find the minimal sufficient statistic for this model. The thing is I cannot write the joint distribution of 's for this model explicitly as it is necessary to determine whether it belongs to the exponential family. Please help.,"X_1=\epsilon_0\,,\,X_t=\rho X_{t-1} + \epsilon_t\,, t=2,3\ldots,n \epsilon_t \sim N(0 , \sigma^2) |\rho| <1 X_1,X_2,\ldots,X_n X_i","['statistics', 'probability-distributions']"
60,Conditional Variance for Bivariate Normal Random Variables is Constant,Conditional Variance for Bivariate Normal Random Variables is Constant,,"Below is a problem I just did. My question for MSE is not how to solve it - but I provide it to illustrate what exactly I am asking. Suppose X,Y are bivariate normal random variables with $E[X] = 40$ , $\mathrm{Var}(X) = 76$ , $E[Y] = 30$ , $\mathrm{Var}(Y) = 32$ , and $\mathrm{Var}(X | Y = 28.5) = 57.$ Calculate $\mathrm{Var}(Y | X = 25)$ . Although I know very little about bivariate random variables, I was able to solve this problem because I have a formula: $$\mathrm{Var}(Y | X = x) = \sigma_{Y}^2(1 - \rho^2).$$ I am not certain, but based on convention I assume $\rho$ = $\rho_{X,Y}$ = $\frac{\mathrm{Cov}(X,Y)}{\sigma_X \sigma_Y}$ . Looking at the information given and my formula, I saw I could use the second formula to solve for $\rho$ , and then re-use the formula to find the desired value. This is when I realized - the question in no way depends on the values of the conditioning variables ( $Y = 28.5, X=25)$ . This seemed strange to me. Keep in mind, my solution is just number crunching for me, I don't have a lot of background knowledge to provide intuition. Can someone explain to me how this is intuitive that the function $f(x) =  \mathrm{Var}(Y | X = x)$ is a constant function? In my head when I picture a bivariate normal distribution I see what looks like an ant-hill centered over (0,0) in $\mathbb{R}^2$ (yes, technically I'm picturing a standard-bivariate normal). But then if I consider the cross sections cut out by fixing values of $X$ , it seems the ones closer to the origin have a bigger hump - thus less variance? Is each cross section for different values of $X$ actually just like.. a scaling of the others? Thus variance stays fixed? Was this intentional in the construction of bivariate normals?","Below is a problem I just did. My question for MSE is not how to solve it - but I provide it to illustrate what exactly I am asking. Suppose X,Y are bivariate normal random variables with , , , , and Calculate . Although I know very little about bivariate random variables, I was able to solve this problem because I have a formula: I am not certain, but based on convention I assume = = . Looking at the information given and my formula, I saw I could use the second formula to solve for , and then re-use the formula to find the desired value. This is when I realized - the question in no way depends on the values of the conditioning variables ( . This seemed strange to me. Keep in mind, my solution is just number crunching for me, I don't have a lot of background knowledge to provide intuition. Can someone explain to me how this is intuitive that the function is a constant function? In my head when I picture a bivariate normal distribution I see what looks like an ant-hill centered over (0,0) in (yes, technically I'm picturing a standard-bivariate normal). But then if I consider the cross sections cut out by fixing values of , it seems the ones closer to the origin have a bigger hump - thus less variance? Is each cross section for different values of actually just like.. a scaling of the others? Thus variance stays fixed? Was this intentional in the construction of bivariate normals?","E[X] = 40 \mathrm{Var}(X) = 76 E[Y] = 30 \mathrm{Var}(Y) = 32 \mathrm{Var}(X | Y = 28.5) = 57. \mathrm{Var}(Y | X = 25) \mathrm{Var}(Y | X = x) = \sigma_{Y}^2(1 - \rho^2). \rho \rho_{X,Y} \frac{\mathrm{Cov}(X,Y)}{\sigma_X \sigma_Y} \rho Y = 28.5, X=25) f(x) =
 \mathrm{Var}(Y | X = x) \mathbb{R}^2 X X","['probability', 'probability-theory', 'statistics', 'normal-distribution', 'conditional-probability']"
61,"Expectation of $Y$ when $X,Y$ are jointly distributed.",Expectation of  when  are jointly distributed.,"Y X,Y","Suppose $X,Y$ are jointly distributed continuous random variables with probability density function $f_{X,Y}(x,y)$ . I know that in order to recover the marginal distribution of one of the random variables, say $Y$ , we can compute $$f_{Y}(y) = \int_{-\infty}^{\infty} f_{X,Y}(x,y) \, dx .$$ My question is about computing $E[Y]$ when starting from the above situation. Considering the fact that the definition of the expectation is $$E[Y] := \int_{-\infty}^{\infty} y \cdot f_{Y}(y) \, dy, $$ My approach is then to compute the expectation as $$E[Y] = \int_{-\infty}^{\infty} y \cdot f_Y(y) \, dy = \int_{-\infty}^{\infty} y \left[ \int_{\infty}^{\infty} f_{X,Y}(x,y) \, dx \,\right] dy = \int_{-\infty}^{\infty} \int_{-\infty}^{\infty} y \cdot f_{X,Y}(x,y) \, dx \, dy. $$ However, I regularly see solutions that will compute it as $$E[Y] = \int_{-\infty}^{\infty} \int_{-\infty}^{\infty} y \cdot f_{X,Y}(x,y) \, dy \, dx. $$ I am familiar with the concept of changing the order of integration, but this seems like more than that. It is no longer clear to me why this still fits the definition of the expected value, because I don't see how we are recovering the marginal pdf of $Y$ and then integrating it against $y$ to arrive at the expectation. I have asked several friends who also solved problems this way why it is valid and none of them seem to have an answer and they just say ""why wouldn't you be able to calculate it this way?"". So either I am crazy or they are unconscious statisticians. On that note, I did notice on a formula page in a text book it has an identity: $$E[g(X,Y)] = \int_{-\infty}^{\infty} \int_{-\infty}^{\infty} g(x,y) \cdot f_{X,Y}(x,y) \, dy \, dx.$$ In the above I agree the order of integration wont matter (by setting X = Y and vice versa). I suppose in this result if you take the function $g(X,Y) = Y$ it will permit the computation that I am hesitant about. Is this how we know we can do that? Or is it simpler than that and I am just being crazy? As a concrete example, here is a specific problem where the solution provided uses the method I am hesitant about. Let $X$ and $Y$ denote the values of two stocks at the end of a five-year period. $X$ is uniformly distributed on the interval $(0,12)$ . Given $X = x$ , $Y$ is uniformly distributed on the interval $(0,x)$ . Find $E[Y]$ . Please remember, my question is not how to solve this problem. It is why a specific method works. My method of solving this would be to first discover that the support of $(X,Y)$ is $0 < y < x < 12$ . Then since $f_{Y|X}(y|x) = x^{-1}$ and $f_{X}(x) = 12^{-1}$ we can deduce that $f_{X,Y}(x,y) = (12x)^{-1}$ . Then compute $$f_{Y}(y) = \int_{y}^{12} (12x)^{-1} \, dx = (1/12)[\ln(12) - \ln(y)].$$ Then use this to compute $$E[Y] = \int_{-\infty}^{\infty} y \cdot f_{Y}(y) \, dy = \int_{0}^{12} y \cdot (1/12)[\ln(12) - \ln(y)] \, dy = 3.$$ Computing the last integral was... 'do-able' for a well-practiced integrater, but it was not ideal. The solution posted was the following: $$E[Y] = \int_{0}^{12} \int_{0}^{x} (y/12x) \, dy \, dx = 3.$$ The above integral is much easier to solve, so once I understand this is a valid way to compute the expectation I will happily add this to my tool belt for solving problems. But again, I don't see how it fits the definition of expectation, because I don't see how it is recovering the marginal distribution for $Y$ . Unless, doing it this way is using the identity that I mention in the middle block of text. So why is the other method valid?","Suppose are jointly distributed continuous random variables with probability density function . I know that in order to recover the marginal distribution of one of the random variables, say , we can compute My question is about computing when starting from the above situation. Considering the fact that the definition of the expectation is My approach is then to compute the expectation as However, I regularly see solutions that will compute it as I am familiar with the concept of changing the order of integration, but this seems like more than that. It is no longer clear to me why this still fits the definition of the expected value, because I don't see how we are recovering the marginal pdf of and then integrating it against to arrive at the expectation. I have asked several friends who also solved problems this way why it is valid and none of them seem to have an answer and they just say ""why wouldn't you be able to calculate it this way?"". So either I am crazy or they are unconscious statisticians. On that note, I did notice on a formula page in a text book it has an identity: In the above I agree the order of integration wont matter (by setting X = Y and vice versa). I suppose in this result if you take the function it will permit the computation that I am hesitant about. Is this how we know we can do that? Or is it simpler than that and I am just being crazy? As a concrete example, here is a specific problem where the solution provided uses the method I am hesitant about. Let and denote the values of two stocks at the end of a five-year period. is uniformly distributed on the interval . Given , is uniformly distributed on the interval . Find . Please remember, my question is not how to solve this problem. It is why a specific method works. My method of solving this would be to first discover that the support of is . Then since and we can deduce that . Then compute Then use this to compute Computing the last integral was... 'do-able' for a well-practiced integrater, but it was not ideal. The solution posted was the following: The above integral is much easier to solve, so once I understand this is a valid way to compute the expectation I will happily add this to my tool belt for solving problems. But again, I don't see how it fits the definition of expectation, because I don't see how it is recovering the marginal distribution for . Unless, doing it this way is using the identity that I mention in the middle block of text. So why is the other method valid?","X,Y f_{X,Y}(x,y) Y f_{Y}(y) = \int_{-\infty}^{\infty} f_{X,Y}(x,y) \, dx . E[Y] E[Y] := \int_{-\infty}^{\infty} y \cdot f_{Y}(y) \, dy,  E[Y] = \int_{-\infty}^{\infty} y \cdot f_Y(y) \, dy = \int_{-\infty}^{\infty} y \left[ \int_{\infty}^{\infty} f_{X,Y}(x,y) \, dx \,\right] dy = \int_{-\infty}^{\infty} \int_{-\infty}^{\infty} y \cdot f_{X,Y}(x,y) \, dx \, dy.  E[Y] = \int_{-\infty}^{\infty} \int_{-\infty}^{\infty} y \cdot f_{X,Y}(x,y) \, dy \, dx.  Y y E[g(X,Y)] = \int_{-\infty}^{\infty} \int_{-\infty}^{\infty} g(x,y) \cdot f_{X,Y}(x,y) \, dy \, dx. g(X,Y) = Y X Y X (0,12) X = x Y (0,x) E[Y] (X,Y) 0 < y < x < 12 f_{Y|X}(y|x) = x^{-1} f_{X}(x) = 12^{-1} f_{X,Y}(x,y) = (12x)^{-1} f_{Y}(y) = \int_{y}^{12} (12x)^{-1} \, dx = (1/12)[\ln(12) - \ln(y)]. E[Y] = \int_{-\infty}^{\infty} y \cdot f_{Y}(y) \, dy = \int_{0}^{12} y \cdot (1/12)[\ln(12) - \ln(y)] \, dy = 3. E[Y] = \int_{0}^{12} \int_{0}^{x} (y/12x) \, dy \, dx = 3. Y","['calculus', 'probability', 'probability-theory', 'statistics', 'probability-distributions']"
62,Complete Sufficient Statistic for double parameter exponential,Complete Sufficient Statistic for double parameter exponential,,"I am trying to show that $(X_{(1)}, \sum_{i=1}^{n}(X_i-X_{(1)})$ are joint complete sufficient for $(a,b)$ where $\{X_i\}_{i}^{n}\sim exp(a,b)$ . I know the joint pdf is $$\prod_{i=1}^{n}\frac{1}{b}e^{(X_i-a)}\chi_{>a}(x_i)=\frac{1}{b}^{n}e^{\sum_{i=1}^{n}(X_i-a)}\chi_{>a}(x_{(1)})$$ By adding a zero in the form of $nX_{(1)}-nX_{(1)}$ the above can be rearranged to: $$e^{-\sum_{i=1}^{n}(X_i-X_{(1)})+nX_{(1)}+na-nlog(b)}\chi_{>a}(x_{(1)})$$ I know since $T(X)=((X_{(1)}, \sum_{i=1}^{n}(X_i-X_{(1)}))$ then it is a complete sufficient statistic but I am having trouble in getting rid of $\chi_{>a}(x_{(1)})$ to get it into proper exponential family form i.e $h(x)=\chi_{>a}(x_{(1)})$ only dependent on the data. Any help?",I am trying to show that are joint complete sufficient for where . I know the joint pdf is By adding a zero in the form of the above can be rearranged to: I know since then it is a complete sufficient statistic but I am having trouble in getting rid of to get it into proper exponential family form i.e only dependent on the data. Any help?,"(X_{(1)}, \sum_{i=1}^{n}(X_i-X_{(1)}) (a,b) \{X_i\}_{i}^{n}\sim exp(a,b) \prod_{i=1}^{n}\frac{1}{b}e^{(X_i-a)}\chi_{>a}(x_i)=\frac{1}{b}^{n}e^{\sum_{i=1}^{n}(X_i-a)}\chi_{>a}(x_{(1)}) nX_{(1)}-nX_{(1)} e^{-\sum_{i=1}^{n}(X_i-X_{(1)})+nX_{(1)}+na-nlog(b)}\chi_{>a}(x_{(1)}) T(X)=((X_{(1)}, \sum_{i=1}^{n}(X_i-X_{(1)})) \chi_{>a}(x_{(1)}) h(x)=\chi_{>a}(x_{(1)})","['statistics', 'statistical-inference', 'exponential-distribution', 'sufficient-statistics']"
63,"Why isn't this property preserved going from pmfs to pdfs? $\Pr(cX=x)=\Pr(X=x/c)$, but $f_X(x/c)\neq f_Y(x),\ Y=cX$","Why isn't this property preserved going from pmfs to pdfs? , but","\Pr(cX=x)=\Pr(X=x/c) f_X(x/c)\neq f_Y(x),\ Y=cX","I've tried searching for this but I think the question is a bit too specific. I was working through a problem and along the way I had to write the probability density of some variable $cX$ at the point $x$ . In my case $X\sim N(0,1)$ . Then I wrote ""The event $cX=x$ is equivalent to the event $X=\frac{x}{c}$ , so $f_{Y}(x)=f_X\left(\frac{x}{c}\right)$ , where $f_Y(\cdot)$ is the density function for the variable $Y=cX$ ."" But then I thought about it for a while and realised it wasn't true. We have \begin{equation*}     f_X\left(\frac{x}{c}\right)=\frac{1}{\sqrt{2\pi}}e^{-\frac{\left(\frac{x}{c}\right)^2}{2}}=\frac{1}{\sqrt{2\pi}}e^{-\frac{x^2}{2c^2}} \end{equation*} whereas we have \begin{equation*}     f_Y(x)=\frac{1}{\sqrt{2\pi c^2}}e^{-\frac{x^2}{2c^2}} \end{equation*} So my statement above was false. However, I gather that it would be true if $X$ was discrete. I am aware of the transformation-of-variables formula $Y=g(X)\implies f_Y(y)=f_X(g^{-1}(y))\bigg|\frac{\mathrm{d}}{\mathrm{d}y}g^{-1}(y)\bigg|$ , and I just need a bit of help understanding which step in my logic was faulty. Is it because probability mass functions for discrete variables are not trivially analogous to probability density functions for continuous variables? Thank you!","I've tried searching for this but I think the question is a bit too specific. I was working through a problem and along the way I had to write the probability density of some variable at the point . In my case . Then I wrote ""The event is equivalent to the event , so , where is the density function for the variable ."" But then I thought about it for a while and realised it wasn't true. We have whereas we have So my statement above was false. However, I gather that it would be true if was discrete. I am aware of the transformation-of-variables formula , and I just need a bit of help understanding which step in my logic was faulty. Is it because probability mass functions for discrete variables are not trivially analogous to probability density functions for continuous variables? Thank you!","cX x X\sim N(0,1) cX=x X=\frac{x}{c} f_{Y}(x)=f_X\left(\frac{x}{c}\right) f_Y(\cdot) Y=cX \begin{equation*}
    f_X\left(\frac{x}{c}\right)=\frac{1}{\sqrt{2\pi}}e^{-\frac{\left(\frac{x}{c}\right)^2}{2}}=\frac{1}{\sqrt{2\pi}}e^{-\frac{x^2}{2c^2}}
\end{equation*} \begin{equation*}
    f_Y(x)=\frac{1}{\sqrt{2\pi c^2}}e^{-\frac{x^2}{2c^2}}
\end{equation*} X Y=g(X)\implies f_Y(y)=f_X(g^{-1}(y))\bigg|\frac{\mathrm{d}}{\mathrm{d}y}g^{-1}(y)\bigg|","['probability', 'statistics']"
64,Entropy Maximizers / Techniques,Entropy Maximizers / Techniques,,"I have no background on statistics, and am trying to learn the basics. In particular, I’m trying to prove the following: Among all continuous functions on $(0,\infty)$ with mean $1$ , prove that the entropy maximizer $f$ has entropy $H(f) := \int_0^\infty -f(x) \operatorname{log}(f(x)) dx$ being $1$ . Questions What are some techniques worth paying attention to when proving entropy maximizers? Even in the easiest example over a finite set with volume $1$ , I had to use the inequality $log(X) \leq X-1$ , which seems like a clever trick to me, i.e. it would have taken me more than a half hour to figure out if I’ve never seen it before. Note that the proof ask for the maximized among all continuous distributions. Can one use variational methods for this type of problems still? If I change $log$ to other functions in the definition of entropy and ask for maximizers, are there still statistical interpretations?","I have no background on statistics, and am trying to learn the basics. In particular, I’m trying to prove the following: Among all continuous functions on with mean , prove that the entropy maximizer has entropy being . Questions What are some techniques worth paying attention to when proving entropy maximizers? Even in the easiest example over a finite set with volume , I had to use the inequality , which seems like a clever trick to me, i.e. it would have taken me more than a half hour to figure out if I’ve never seen it before. Note that the proof ask for the maximized among all continuous distributions. Can one use variational methods for this type of problems still? If I change to other functions in the definition of entropy and ask for maximizers, are there still statistical interpretations?","(0,\infty) 1 f H(f) := \int_0^\infty -f(x) \operatorname{log}(f(x)) dx 1 1 log(X) \leq X-1 log","['statistics', 'entropy']"
65,Sampling with and without replacement,Sampling with and without replacement,,"A box contains 5 tickets. An unknown number of them are red, the rest are green. Suppose that to start off with you think there are equally likely to be 0, 1, 2, 3, 4, or 5 red tickets in the box. Three tickets are drawn from the box between draws. The tickets are red, green, and red. Given this information, what is the chance that there are actually 3 red tickets in the box? a) with replacement (Answer: 0.36) b) without replacement (Answer: 0.4) So far I have done this: P(RRRGG|RGR) = P(RRRGG & RGR)/P(RGR) = P(RRRGG)/P(RGR), but I do not know what to do next because I keep getting the wrong answer.","A box contains 5 tickets. An unknown number of them are red, the rest are green. Suppose that to start off with you think there are equally likely to be 0, 1, 2, 3, 4, or 5 red tickets in the box. Three tickets are drawn from the box between draws. The tickets are red, green, and red. Given this information, what is the chance that there are actually 3 red tickets in the box? a) with replacement (Answer: 0.36) b) without replacement (Answer: 0.4) So far I have done this: P(RRRGG|RGR) = P(RRRGG & RGR)/P(RGR) = P(RRRGG)/P(RGR), but I do not know what to do next because I keep getting the wrong answer.",,"['probability', 'statistics']"
66,Let $m_X(t)$ be the moment generating function of random variable $X$. Prove $m_X(t)=\sum_{k=0}^\infty E(X^k)\frac{t^k}{k!}$,Let  be the moment generating function of random variable . Prove,m_X(t) X m_X(t)=\sum_{k=0}^\infty E(X^k)\frac{t^k}{k!},Let $m_X(t)$ be the moment generating function of random variable $X$ . Prove $m_X(t)=\sum_{k=0}^\infty E(X^k)\frac{t^k}{k!}$ So I have: $$ \begin{split} m_X(t)  &= \mathbb{E}\left[ e^{tX} \right]\\  &= \mathbb{E}\left[ \sum_{k=0}^\infty \frac{(tX)^k}{k!} \right] \\  &=\sum_{k=0}^\infty \mathbb{E}\left[ X^k \right] \\  &=\sum_{k=0}^\infty \frac{t^k}{k!} \mathbb{E}\left[ X^k \right] \end{split} $$,Let be the moment generating function of random variable . Prove So I have:,"m_X(t) X m_X(t)=\sum_{k=0}^\infty E(X^k)\frac{t^k}{k!} 
\begin{split}
m_X(t)
 &= \mathbb{E}\left[ e^{tX} \right]\\
 &= \mathbb{E}\left[ \sum_{k=0}^\infty \frac{(tX)^k}{k!} \right] \\
 &=\sum_{k=0}^\infty \mathbb{E}\left[ X^k \right] \\
 &=\sum_{k=0}^\infty \frac{t^k}{k!} \mathbb{E}\left[ X^k \right]
\end{split}
","['statistics', 'probability-distributions', 'moment-generating-functions']"
67,Multiplication of random matrices with independent entries,Multiplication of random matrices with independent entries,,"Let A and B be two random matrices with zero-mean i.i.d. entries. Then, are the entries of C = A*B independent? From intuition, each entry of C is the dot product of two different independent random vectors. But, I am not quite sure how to formally show whether the entries are/are not independent.","Let A and B be two random matrices with zero-mean i.i.d. entries. Then, are the entries of C = A*B independent? From intuition, each entry of C is the dot product of two different independent random vectors. But, I am not quite sure how to formally show whether the entries are/are not independent.",,"['statistics', 'random-matrices', 'random-functions']"
68,"Solving an estimating problem with MLE of a function, condition of $\theta$","Solving an estimating problem with MLE of a function, condition of",\theta,"I was given a question to find the estimator $\theta$ by moments method of the function: $$f_{X}(x)= \begin{Bmatrix} \frac{3x^2}{2\theta^3} & -\theta<x<\theta\\   0& \text{else} \end{Bmatrix}$$ The first moment is zero, and by the second moment I got that: $$\hat{\theta}= \sqrt{\frac{5S^2_x}{3}}$$ Now if I were to find the estimator with MLE, for the sports, I would find the function $L(\theta)$ : $$L(\theta) = \prod_{i=1}^n \frac{3x_i^2}{2\theta^3} \times{I_{(-\theta<x_i<\theta)}}$$ Now I'll play a little with the condition of the Indicator variable I so that: $$ L(\theta) = \prod_{i=1}^{n}\Bigl(\frac{3x_i^2}{2\theta^3}\Bigr) \times I_{(-\theta<\min(x_i))} \times I_{(\max(x_i)<\theta}$$ Suppose I ignore the constant $ \prod_{i=1}^{n}\Bigl(\frac{3x_i^2}{2}\Bigr)$ part, and I'm only trying to convert the condition on $x$ to a condition of $\theta$ , but I'm in a little problem here: $$-\theta<\min(x_i) \rightarrow-\min(x_i)<\theta$$ And $$\max(x_i)<\theta $$ Eventually I can only tell what is smallr than $\theta$ , but not what bounds it from above. Virtually I'm looking for a condition such as $$f(x_i) <\theta<g(x_i)$$ where $f$ and $g$ are some functions that probably involve $\max$ or $\min$ . Can anyone direct me where can I extract a condition of an upper bound?","I was given a question to find the estimator by moments method of the function: The first moment is zero, and by the second moment I got that: Now if I were to find the estimator with MLE, for the sports, I would find the function : Now I'll play a little with the condition of the Indicator variable I so that: Suppose I ignore the constant part, and I'm only trying to convert the condition on to a condition of , but I'm in a little problem here: And Eventually I can only tell what is smallr than , but not what bounds it from above. Virtually I'm looking for a condition such as where and are some functions that probably involve or . Can anyone direct me where can I extract a condition of an upper bound?","\theta f_{X}(x)= \begin{Bmatrix}
\frac{3x^2}{2\theta^3} & -\theta<x<\theta\\ 
 0& \text{else}
\end{Bmatrix} \hat{\theta}= \sqrt{\frac{5S^2_x}{3}} L(\theta) L(\theta) = \prod_{i=1}^n \frac{3x_i^2}{2\theta^3} \times{I_{(-\theta<x_i<\theta)}}  L(\theta) = \prod_{i=1}^{n}\Bigl(\frac{3x_i^2}{2\theta^3}\Bigr) \times I_{(-\theta<\min(x_i))} \times I_{(\max(x_i)<\theta}  \prod_{i=1}^{n}\Bigl(\frac{3x_i^2}{2}\Bigr) x \theta -\theta<\min(x_i) \rightarrow-\min(x_i)<\theta \max(x_i)<\theta  \theta f(x_i) <\theta<g(x_i) f g \max \min","['statistics', 'maximum-likelihood']"
69,"Given joint density $f_{X,Y}(x,y)$ find covariance, correlation, and specific expectations","Given joint density  find covariance, correlation, and specific expectations","f_{X,Y}(x,y)","Given the joint density of $X$ and $Y$ , $$f_{X,Y}(x,y)=\begin{cases}e^{-y}&\text{for }0\le x<y<\infty\\0&\text{otherwise}\end{cases},$$ (a) find the covariance and correlation of $X$ and $Y$ ; (b) find $E[X\mid Y=y]$ and $E[Y\mid X=x]$ ; and (c) find $E[X]$ and $\mathrm{Var}[X]$ . I know that $$\mathrm{Cov}[X,Y]=E[XY]-E[X]E[Y],$$ $$\mathrm{Corr}[X,Y]=\frac{\mathrm{Cov}[X,Y]}{\sqrt{\mathrm{Var}[X]\mathrm{Var}[Y]}},\text{ and}$$ $$E[XY]=\int_0^\infty\int_0^yxye^{-y}\,\mathrm dx\,\mathrm dy=3,$$ so theoretically, I should be able to compute the covariance by first finding the marginal densities for $X$ and $Y$ , then computing the expectations of $X$ and $Y$ . Alternatively, since $$E[X]=E[E[X\mid Y=y]],$$ I can accomplish the same thing by finding the conditional densities and their expectations. However, the ordering of the questions has me wondering: (1) Can I compute the covariance and correlation directly knowing just the joint PDF? As for finding the (necessary?) expectations, I've run into a complication... The marginal density for $Y$ is $$f_Y(y)=\int_0^ye^{-y}\,\mathrm dx=xe^{-y}\bigg|_0^y=\begin{cases}ye^{-y}&\text{for }y\ge0\\0&\text{otherwise}\end{cases}$$ so the conditional density of $X$ given $Y=y$ is $$f_{X\mid Y}(x\mid y)=\begin{cases}\frac{e^{-y}}{ye^{-y}}=\frac1y&\text{for }0\le x<y<\infty\\0&\text{otherwise}\end{cases}$$ Then the conditional expectation of $X$ given $Y=y$ is $$E[X\mid Y=y]=\int_0^y\frac xy\,\mathrm dx=\frac{x^2}{2y}\bigg|_0^y=\frac y2$$ and so $$\boxed{E[X]=E[E[X\mid Y=y]]=E\left[\frac y2\right]=\frac y2}$$ Similarly, the marginal density for $X$ is $$f_X(x)=\int_x^\infty e^{-y}\,\mathrm dy=-e^{-y}\bigg|_x^\infty=\begin{cases}e^{-x}&\text{for }x\ge0\\0&\text{otherwise}\end{cases}$$ so the conditional density of $Y$ given $X=x$ is $$f_{Y\mid X}(y\mid x)=\begin{cases}\frac{e^{-y}}{e^{-x}}=e^{x-y}&\text{for }0\le x<y<\infty\\0&\text{otherwise}\end{cases}$$ and the expectation of $Y$ given $X=x$ would be $$E[Y\mid X=x]=\int_x^\infty ye^{x-y}\,\mathrm dy=-(y+1)e^{x-y}\bigg|_x^\infty=x+1$$ and so the expectation of $Y$ is $$\boxed{E[Y]=E[E[Y\mid X=x]]=E[x+1]=x+1}$$ The complication is that I get different expectations for $X$ and $Y$ when trying to verify their values using the corresponding marginal densities: $$\boxed{E[X]=\int_0^\infty xe^{-x}\,\mathrm dx=1\\E[Y]=\int_0^\infty y^2e^{-y}\,\mathrm dy=2}$$ I thought that perhaps the problem lies with the supports of $f_X$ and $f_Y$ . We have $0\le x<y<\infty$ to start with, so the marginal densities could instead be $$f_X(x)=\begin{cases}e^{-x}&\text{for }0\le x<y\\0&\text{otherwise}\end{cases}$$ $$f_Y(y)=\begin{cases}ye^{-y}&\text{for }x\le y\\0&\text{otherwise}\end{cases}$$ But even then, $$\boxed{E[X]=\int_0^yxe^{-x}\,\mathrm dx=1-(y+1)e^{-y}\neq\frac y2\\E[Y]=\int_x^\infty y^2e^{-y}\,\mathrm dy=(x^2+2x+2)e^{-x}\neq x+1}$$ (2) Which expectations are correct? Why the discrepancy between methods?","Given the joint density of and , (a) find the covariance and correlation of and ; (b) find and ; and (c) find and . I know that so theoretically, I should be able to compute the covariance by first finding the marginal densities for and , then computing the expectations of and . Alternatively, since I can accomplish the same thing by finding the conditional densities and their expectations. However, the ordering of the questions has me wondering: (1) Can I compute the covariance and correlation directly knowing just the joint PDF? As for finding the (necessary?) expectations, I've run into a complication... The marginal density for is so the conditional density of given is Then the conditional expectation of given is and so Similarly, the marginal density for is so the conditional density of given is and the expectation of given would be and so the expectation of is The complication is that I get different expectations for and when trying to verify their values using the corresponding marginal densities: I thought that perhaps the problem lies with the supports of and . We have to start with, so the marginal densities could instead be But even then, (2) Which expectations are correct? Why the discrepancy between methods?","X Y f_{X,Y}(x,y)=\begin{cases}e^{-y}&\text{for }0\le x<y<\infty\\0&\text{otherwise}\end{cases}, X Y E[X\mid Y=y] E[Y\mid X=x] E[X] \mathrm{Var}[X] \mathrm{Cov}[X,Y]=E[XY]-E[X]E[Y], \mathrm{Corr}[X,Y]=\frac{\mathrm{Cov}[X,Y]}{\sqrt{\mathrm{Var}[X]\mathrm{Var}[Y]}},\text{ and} E[XY]=\int_0^\infty\int_0^yxye^{-y}\,\mathrm dx\,\mathrm dy=3, X Y X Y E[X]=E[E[X\mid Y=y]], Y f_Y(y)=\int_0^ye^{-y}\,\mathrm dx=xe^{-y}\bigg|_0^y=\begin{cases}ye^{-y}&\text{for }y\ge0\\0&\text{otherwise}\end{cases} X Y=y f_{X\mid Y}(x\mid y)=\begin{cases}\frac{e^{-y}}{ye^{-y}}=\frac1y&\text{for }0\le x<y<\infty\\0&\text{otherwise}\end{cases} X Y=y E[X\mid Y=y]=\int_0^y\frac xy\,\mathrm dx=\frac{x^2}{2y}\bigg|_0^y=\frac y2 \boxed{E[X]=E[E[X\mid Y=y]]=E\left[\frac y2\right]=\frac y2} X f_X(x)=\int_x^\infty e^{-y}\,\mathrm dy=-e^{-y}\bigg|_x^\infty=\begin{cases}e^{-x}&\text{for }x\ge0\\0&\text{otherwise}\end{cases} Y X=x f_{Y\mid X}(y\mid x)=\begin{cases}\frac{e^{-y}}{e^{-x}}=e^{x-y}&\text{for }0\le x<y<\infty\\0&\text{otherwise}\end{cases} Y X=x E[Y\mid X=x]=\int_x^\infty ye^{x-y}\,\mathrm dy=-(y+1)e^{x-y}\bigg|_x^\infty=x+1 Y \boxed{E[Y]=E[E[Y\mid X=x]]=E[x+1]=x+1} X Y \boxed{E[X]=\int_0^\infty xe^{-x}\,\mathrm dx=1\\E[Y]=\int_0^\infty y^2e^{-y}\,\mathrm dy=2} f_X f_Y 0\le x<y<\infty f_X(x)=\begin{cases}e^{-x}&\text{for }0\le x<y\\0&\text{otherwise}\end{cases} f_Y(y)=\begin{cases}ye^{-y}&\text{for }x\le y\\0&\text{otherwise}\end{cases} \boxed{E[X]=\int_0^yxe^{-x}\,\mathrm dx=1-(y+1)e^{-y}\neq\frac y2\\E[Y]=\int_x^\infty y^2e^{-y}\,\mathrm dy=(x^2+2x+2)e^{-x}\neq x+1}","['statistics', 'conditional-expectation', 'expected-value']"
70,"Let $X_1, X_2,X_3\sim\rm{ Bernoulli}(\theta)$. Show that $I_{X_1+X_2>X_3}$ is an unbiased estimator of $h(\theta)$ and find UMVUE of $h(\theta)$",Let . Show that  is an unbiased estimator of  and find UMVUE of,"X_1, X_2,X_3\sim\rm{ Bernoulli}(\theta) I_{X_1+X_2>X_3} h(\theta) h(\theta)","Let $X_1, X_2,X_3\sim\rm{ Bernoulli}(\theta)$ . Show that $I_{X_1+X_2>X_3}$ is an unbiased estimator of $h(\theta)=P_{\theta}(X_1+X_2>X_3)$ and find UMVUE of $h(\theta)$ . Assuming $A = (X_1+X_2 > X_3)$ , is it correct that $E[I_A]= P(A)$ in this situation? If this is correct, then I know that for the estimator to be unbiased, $E[I_A] = P(A) = h(\theta)$ , which checks out. I also know that $I_A$ is the best unbiased estimator of $h(\theta)$ if it attains the Cramer-Rao lower bound: $$\operatorname{Var}\left(I_A\right) \geq \frac{\left(\frac{d}{d\theta}E[I_A]\right)^2}{E\left[\left(\frac{d}{d\theta} \ln\left(f(X\mid\theta)\right)\right)^2\right]}$$ When I try solving for $\operatorname{Var}\left(I_A\right)$ : \begin{align} \operatorname{Var}\left(I_A\right) &= E[(I_A - E[I_A])^2] \\ &= E[(I_A - h(\theta))^2] \\ \end{align} I get stuck because I don't know how to simplify further (i.e. I don't know how to evaluate $h(\theta)$ ). Trying to solve for the left-hand side, I am also stuck because I don't know the joint pmf $f(X\mid\theta)$ that gives rise to $h(\theta)$ when integrated. Edit: Because $I_A$ is not the UMVUE, I need to find the best unbiased estimator of $h(\theta)$ . Toward this end, I figured out that the joint pmf is $$f(X\mid\theta) = \theta^{(X_1+X_2+X_3)}(1-\theta)^{(3-(X_1+X_2+X_3))}$$ and $$h(\theta) = \theta^2 + 2 \theta (1-\theta)^2$$ but how do I use this information to obtain an estimator $\delta$ such that $E[\delta] = h(\theta)$ ?","Let . Show that is an unbiased estimator of and find UMVUE of . Assuming , is it correct that in this situation? If this is correct, then I know that for the estimator to be unbiased, , which checks out. I also know that is the best unbiased estimator of if it attains the Cramer-Rao lower bound: When I try solving for : I get stuck because I don't know how to simplify further (i.e. I don't know how to evaluate ). Trying to solve for the left-hand side, I am also stuck because I don't know the joint pmf that gives rise to when integrated. Edit: Because is not the UMVUE, I need to find the best unbiased estimator of . Toward this end, I figured out that the joint pmf is and but how do I use this information to obtain an estimator such that ?","X_1, X_2,X_3\sim\rm{ Bernoulli}(\theta) I_{X_1+X_2>X_3} h(\theta)=P_{\theta}(X_1+X_2>X_3) h(\theta) A = (X_1+X_2 > X_3) E[I_A]= P(A) E[I_A] = P(A) = h(\theta) I_A h(\theta) \operatorname{Var}\left(I_A\right) \geq \frac{\left(\frac{d}{d\theta}E[I_A]\right)^2}{E\left[\left(\frac{d}{d\theta} \ln\left(f(X\mid\theta)\right)\right)^2\right]} \operatorname{Var}\left(I_A\right) \begin{align}
\operatorname{Var}\left(I_A\right) &= E[(I_A - E[I_A])^2] \\
&= E[(I_A - h(\theta))^2] \\
\end{align} h(\theta) f(X\mid\theta) h(\theta) I_A h(\theta) f(X\mid\theta) = \theta^{(X_1+X_2+X_3)}(1-\theta)^{(3-(X_1+X_2+X_3))} h(\theta) = \theta^2 + 2 \theta (1-\theta)^2 \delta E[\delta] = h(\theta)","['statistics', 'parameter-estimation']"
71,How can I safely make statistical inferences on abnormal data?,How can I safely make statistical inferences on abnormal data?,,"I have a data set in which the key data point is lap times in seconds around a small set of known tracks in a small set of known vehicles. (Karts around an indoor kart track.) The data comprises about two months of business and growing; about 4000 racers have participated, completing about 200k laps. When bucketed into tenth-of-second results, the data looks mostly like a normal distribution, with one important caveat: it is not possible to be faster than (insert theoretical limit here), where that limit is only marginally quicker than the peak of the curve. A small number of racers approach that limit, but the left tail of the bell curve is dramatically smaller than the right tail. The right tail goes on impressively; sometimes racers spin out or otherwise find themselves in a jam and it can take several minutes to straighten things out and get running again. The longest lap time I've seen is 8.5 minutes; a typical lap is around 24-28 seconds. The fastest time anyone has ever run is 23.036 seconds. I'm working on improving my input data; ideally I would discard any lap where the racers stopped entirely as junk data. For now, I'm discarding anything >40 seconds as a rough approximation. The mean, when discarding >40, is 27.1 -- a decent bit to the right of the peak of the curve, which is ~26.0. My stats knowledge is spotty at best. I'd like to be able to work with standard deviations and other common statistical operations on this data. Can I do so safely with this data set? If not, what should I do, and more importantly, how do you know? For context, things I’d like to be able to do with the data: Compare individual racers against the norm, against each other, against their past selves See if the distribution changes over time (and correlate that with, for example, tire changes on the karts) Identify “hot” and “cold” karts Identify outliers and flag them for inspection (the timing system sometimes produces bad data) Rank drivers on their consistency Identify times when the track is likely to be warm (consecutive sessions warm the track, and a warm track yields better traction and therefore faster times) To be clear, I’m not asking how to do the above, although I would appreciate any guidance. And yes, the raw lap data includes identifiers for karts and racers, as well as dates.","I have a data set in which the key data point is lap times in seconds around a small set of known tracks in a small set of known vehicles. (Karts around an indoor kart track.) The data comprises about two months of business and growing; about 4000 racers have participated, completing about 200k laps. When bucketed into tenth-of-second results, the data looks mostly like a normal distribution, with one important caveat: it is not possible to be faster than (insert theoretical limit here), where that limit is only marginally quicker than the peak of the curve. A small number of racers approach that limit, but the left tail of the bell curve is dramatically smaller than the right tail. The right tail goes on impressively; sometimes racers spin out or otherwise find themselves in a jam and it can take several minutes to straighten things out and get running again. The longest lap time I've seen is 8.5 minutes; a typical lap is around 24-28 seconds. The fastest time anyone has ever run is 23.036 seconds. I'm working on improving my input data; ideally I would discard any lap where the racers stopped entirely as junk data. For now, I'm discarding anything >40 seconds as a rough approximation. The mean, when discarding >40, is 27.1 -- a decent bit to the right of the peak of the curve, which is ~26.0. My stats knowledge is spotty at best. I'd like to be able to work with standard deviations and other common statistical operations on this data. Can I do so safely with this data set? If not, what should I do, and more importantly, how do you know? For context, things I’d like to be able to do with the data: Compare individual racers against the norm, against each other, against their past selves See if the distribution changes over time (and correlate that with, for example, tire changes on the karts) Identify “hot” and “cold” karts Identify outliers and flag them for inspection (the timing system sometimes produces bad data) Rank drivers on their consistency Identify times when the track is likely to be warm (consecutive sessions warm the track, and a warm track yields better traction and therefore faster times) To be clear, I’m not asking how to do the above, although I would appreciate any guidance. And yes, the raw lap data includes identifiers for karts and racers, as well as dates.",,['statistics']
72,Law of Large Numbers and Bernoulli Distribution,Law of Large Numbers and Bernoulli Distribution,,"Let ${X_i}$ be a sequence of random variables which follows the Bernoulli distribution with $Bernoulli(\frac1i)$ , for each $i = 1,...,n$ . In other words, $P(X_i=1) = \frac 1i$ and $P(X_i=0) = 1 - \frac 1i$ for each $i = 1,...,n$ . Show that $X_n$ tends in probability to $0$ . I have no idea how to even get started on this, so any tips would help greatly. My lecturer has just covered the Law of Large Numbers, convergence in probability and what makes a good estimator (unbiased, efficient and consistent) but I think I speak for the whole class when I say that we understood little of what she was teaching. EDIT I would first like to thank the kind souls who gave me hints on how I should go about solving this and also vetting what I had come up with. This is my answer: To show $X_n \to 0$ , we should aim to show $P(|X_n - 0| > \epsilon) = 0$ for any positive $\epsilon$ . Then, since $X_n$ follows a Bernoulli Distribution, so $X_n$ can only take on non-negative integer values. Thus, $P(|X_n - 0| > \epsilon) = P(X_n > \epsilon)$ . Now, I make use of Markov's Inequality, so $P(X_n  > \epsilon) \leqslant \frac 1\epsilon E(X_n) = \frac 1\epsilon \frac 1n$ . As $n \to \infty$ , $\frac 1n \to 0$ , so $P(|X_n - 0| > \epsilon) = 0$ and we can conclude that $X_n$ tends in probability to $0$ . Any further comments on how I can improve this answer are welcome :) or if anyone has any other more elegant ways to answer the question, they are welcome too!","Let be a sequence of random variables which follows the Bernoulli distribution with , for each . In other words, and for each . Show that tends in probability to . I have no idea how to even get started on this, so any tips would help greatly. My lecturer has just covered the Law of Large Numbers, convergence in probability and what makes a good estimator (unbiased, efficient and consistent) but I think I speak for the whole class when I say that we understood little of what she was teaching. EDIT I would first like to thank the kind souls who gave me hints on how I should go about solving this and also vetting what I had come up with. This is my answer: To show , we should aim to show for any positive . Then, since follows a Bernoulli Distribution, so can only take on non-negative integer values. Thus, . Now, I make use of Markov's Inequality, so . As , , so and we can conclude that tends in probability to . Any further comments on how I can improve this answer are welcome :) or if anyone has any other more elegant ways to answer the question, they are welcome too!","{X_i} Bernoulli(\frac1i) i = 1,...,n P(X_i=1) = \frac 1i P(X_i=0) = 1 - \frac 1i i = 1,...,n X_n 0 X_n \to 0 P(|X_n - 0| > \epsilon) = 0 \epsilon X_n X_n P(|X_n - 0| > \epsilon) = P(X_n > \epsilon) P(X_n  > \epsilon) \leqslant \frac 1\epsilon E(X_n) = \frac 1\epsilon \frac 1n n \to \infty \frac 1n \to 0 P(|X_n - 0| > \epsilon) = 0 X_n 0","['probability', 'statistics', 'law-of-large-numbers']"
73,Verification of a process is Markovian or not?,Verification of a process is Markovian or not?,,"Let $\left(X_t\right)_{t \in \mathbb{N}}$ is a $(\lambda,P)$ markov chain on $\mathbb{Z}$ , where $\lambda$ is the initial propability and $P$ is the transition matrix. Now define the following processes as $~(1)~Q_t:=\left(X_t\right)^3~,~(2) ~Y_t :=\left(X_t\right)^2~,~(3)~Z_t:=e^{X_t}~,~ (4)~W_t:=\left| X_t\right|$ Are these valid markov chains ? what is the initial probability and the transition matrix ? For the first case i tried like this : $$\begin{align}\mathbb{P}&(Q_{t+1}=j|Q_t=i,Q_{t-1}=i_{t-1},\ldots,Q_0=i_0)\\&=\mathbb{P}(X^3_{t+1}=j|X^3_t=i,X^3_{t-1}=i_{t-1},\ldots,X^3_0=i_0)\\&=\mathbb{P}(X_{t+1}=j^{1/3}|X_t=i^{1/3})\end{align}$$ So this is a markov chain and the initial probability will be $\mathbb{P}(Q_0=i)=\mathbb{P}(X_0=i^{1/3})=\lambda^{1/3}$ and the transition matrix would be then $p_{i^{1/3}j^{1/3}}$ . Is it correct ? And i'm confused about the other ones $Y_t;=(X_t)^2$ and $W_t;=|X_t|$ is  problematic i think as these maps are not injective...how to prove or disprove this formally and find out $(\lambda,P)$ ?","Let is a markov chain on , where is the initial propability and is the transition matrix. Now define the following processes as Are these valid markov chains ? what is the initial probability and the transition matrix ? For the first case i tried like this : So this is a markov chain and the initial probability will be and the transition matrix would be then . Is it correct ? And i'm confused about the other ones and is  problematic i think as these maps are not injective...how to prove or disprove this formally and find out ?","\left(X_t\right)_{t \in \mathbb{N}} (\lambda,P) \mathbb{Z} \lambda P ~(1)~Q_t:=\left(X_t\right)^3~,~(2) ~Y_t :=\left(X_t\right)^2~,~(3)~Z_t:=e^{X_t}~,~ (4)~W_t:=\left| X_t\right| \begin{align}\mathbb{P}&(Q_{t+1}=j|Q_t=i,Q_{t-1}=i_{t-1},\ldots,Q_0=i_0)\\&=\mathbb{P}(X^3_{t+1}=j|X^3_t=i,X^3_{t-1}=i_{t-1},\ldots,X^3_0=i_0)\\&=\mathbb{P}(X_{t+1}=j^{1/3}|X_t=i^{1/3})\end{align} \mathbb{P}(Q_0=i)=\mathbb{P}(X_0=i^{1/3})=\lambda^{1/3} p_{i^{1/3}j^{1/3}} Y_t;=(X_t)^2 W_t;=|X_t| (\lambda,P)","['probability', 'statistics', 'stochastic-processes', 'markov-chains']"
74,Bounded in Probability and smaller order in probability,Bounded in Probability and smaller order in probability,,"I wanted to prove that if $X_n$ is bounded in probability and $Y_n = o_p(X_n)$ , then $Y_n \rightarrow 0$ in probability I  know the following definitions that is $X_n$ is bounded in probability meaning that $P(|X_n|<M)>1-\epsilon$ and I know that $Y_n = o_p(X_n)$ implies that $Y_n/X_n \rightarrow0$ in probability","I wanted to prove that if is bounded in probability and , then in probability I  know the following definitions that is is bounded in probability meaning that and I know that implies that in probability",X_n Y_n = o_p(X_n) Y_n \rightarrow 0 X_n P(|X_n|<M)>1-\epsilon Y_n = o_p(X_n) Y_n/X_n \rightarrow0,"['probability-theory', 'statistics', 'asymptotics']"
75,convergence almost surely that uses lim sup of probabilities,convergence almost surely that uses lim sup of probabilities,,"What's wrong with my intuition here? I read that given the following sequence of random numbers $X_i \in (0,1]$ , with $I$ as indicator function: $$ X_1 = I(0,1/2] \quad X_2 = I(1/2,1] \quad X_3 = I(0,1/4] \quad X_4 = I(1/4,1/2] \quad X_5 = I(1/2,3/4] \quad X_6 = I(3/4,1] \quad X_7 = I(0,1/8] \quad X_8 = I(1/8,1/4] \quad ... $$ The sequence $X_i$ converges in probability as $i \rightarrow \infty$ since given any $\epsilon > 0$ , we have: $$ P[X_i \geq \epsilon] \rightarrow 0 $$ However, $X_i$ does not converge almost surely (or almost everywhere), since given any $i$ , there is a $n \geq i$ , s.t. $P[X_n] > \epsilon$ for some $\epsilon > 0$ (which means that $X_n$ is above $0$ infinitely)... However, I could not relate this with an alternative definition for almost sure convergence which is as follows given any $\epsilon > 0$ : $$ \text{lim sup}_{i \rightarrow \infty} \{ P[X_i \geq \epsilon] \} \rightarrow 0 $$ I guess my confusion in this definition is when it starts to use probabilities $P$ . Since the 'size' of the intervals where $P[X_i] > \epsilon$ also approaches zero, $0$ , i.e. it gets narrower and narrower, shouldn't it also follow that the Lebesgue measure of $P[X_i] > \epsilon \rightarrow 0$ such that its supremum $\rightarrow 0$ ?","What's wrong with my intuition here? I read that given the following sequence of random numbers , with as indicator function: The sequence converges in probability as since given any , we have: However, does not converge almost surely (or almost everywhere), since given any , there is a , s.t. for some (which means that is above infinitely)... However, I could not relate this with an alternative definition for almost sure convergence which is as follows given any : I guess my confusion in this definition is when it starts to use probabilities . Since the 'size' of the intervals where also approaches zero, , i.e. it gets narrower and narrower, shouldn't it also follow that the Lebesgue measure of such that its supremum ?","X_i \in (0,1] I 
X_1 = I(0,1/2] \quad X_2 = I(1/2,1] \quad X_3 = I(0,1/4] \quad X_4 = I(1/4,1/2] \quad X_5 = I(1/2,3/4] \quad X_6 = I(3/4,1] \quad X_7 = I(0,1/8] \quad X_8 = I(1/8,1/4] \quad ...
 X_i i \rightarrow \infty \epsilon > 0 
P[X_i \geq \epsilon] \rightarrow 0
 X_i i n \geq i P[X_n] > \epsilon \epsilon > 0 X_n 0 \epsilon > 0 
\text{lim sup}_{i \rightarrow \infty} \{ P[X_i \geq \epsilon] \} \rightarrow 0
 P P[X_i] > \epsilon 0 P[X_i] > \epsilon \rightarrow 0 \rightarrow 0","['statistics', 'convergence-divergence', 'uniform-convergence', 'almost-everywhere', 'pointwise-convergence']"
76,How to find the cube of a uniform distribution?,How to find the cube of a uniform distribution?,,"I came across this question in a quiz and I was not sure on how to do it since my lecturer didn't clearly teach this. Can anyone assist me with this: Let $X$ ~ $U(0,3)$ , find the density $f(u)$ for $U = X^3$ and calculate its value at $u=2$ I first tried attempting it by doing: $F_{X^3}(x) = \mathbb{P}(X^3 \leq x) = \mathbb{P}(X \in [0,\sqrt[3]{x}]) = \sqrt[3]{x}$ . But this is completely wrong apparently, can someone please clarify this for me?","I came across this question in a quiz and I was not sure on how to do it since my lecturer didn't clearly teach this. Can anyone assist me with this: Let ~ , find the density for and calculate its value at I first tried attempting it by doing: . But this is completely wrong apparently, can someone please clarify this for me?","X U(0,3) f(u) U = X^3 u=2 F_{X^3}(x) = \mathbb{P}(X^3 \leq x) = \mathbb{P}(X \in [0,\sqrt[3]{x}]) = \sqrt[3]{x}","['probability', 'statistics', 'uniform-distribution']"
77,Probability of satisfying an event if I only have 3 attempts to do so,Probability of satisfying an event if I only have 3 attempts to do so,,"For example, if I want to attempt to win a challenge, and I only have three tries to win the challenge, then what would $P$ (I win the challenge) be if the probability of me winning the challenge in 1 attempt is $\frac{1}{3}$ ? The attempts are independent, and if I win the challenge on an attempt or fail all 3 tries, then I will not attempt the challenge anymore. To try and solve this question, I did the following: $P$ (Lose an attempt) = 1 - $\frac{1}{3}$ = $\frac{2}{3}$ $P$ (Win all 3 attempts) = 1 - $(\frac{2}{3})^3$ But I don't know if this is correct, since I would stop making further attempts if I won on the first try, or on the second try, etc.","For example, if I want to attempt to win a challenge, and I only have three tries to win the challenge, then what would (I win the challenge) be if the probability of me winning the challenge in 1 attempt is ? The attempts are independent, and if I win the challenge on an attempt or fail all 3 tries, then I will not attempt the challenge anymore. To try and solve this question, I did the following: (Lose an attempt) = 1 - = (Win all 3 attempts) = 1 - But I don't know if this is correct, since I would stop making further attempts if I won on the first try, or on the second try, etc.",P \frac{1}{3} P \frac{1}{3} \frac{2}{3} P (\frac{2}{3})^3,"['probability', 'statistics']"
78,Geometric Distribution - Coin Flip [closed],Geometric Distribution - Coin Flip [closed],,"Closed. This question does not meet Mathematics Stack Exchange guidelines . It is not currently accepting answers. Please provide additional context , which ideally explains why the question is relevant to you and our  community. Some forms of context include: background and motivation, relevant definitions, source, possible strategies, your current progress, why the question is interesting or important, etc. Closed 4 years ago . Improve this question You have a coin with unknown probability p of coming up heads. You wish to generate a random variable which takes the values 0 and 1, each with probability 1/2. Assume 0 < p < 1. You adopt the following procedure. You start by flipping the coin twice. If both flips produce the same side of the coin, you start again. If the result of the first flip is different from the result of the second flip, you report the result of the first flip and you are finished (this is a trick originally due to John von Neumann). (a) Show that, in this case, the probability of reporting heads is 1/2. (b) What is the expected number of flips you must make before you report a result? I don't know how to start, any help would be appreciated!!!","Closed. This question does not meet Mathematics Stack Exchange guidelines . It is not currently accepting answers. Please provide additional context , which ideally explains why the question is relevant to you and our  community. Some forms of context include: background and motivation, relevant definitions, source, possible strategies, your current progress, why the question is interesting or important, etc. Closed 4 years ago . Improve this question You have a coin with unknown probability p of coming up heads. You wish to generate a random variable which takes the values 0 and 1, each with probability 1/2. Assume 0 < p < 1. You adopt the following procedure. You start by flipping the coin twice. If both flips produce the same side of the coin, you start again. If the result of the first flip is different from the result of the second flip, you report the result of the first flip and you are finished (this is a trick originally due to John von Neumann). (a) Show that, in this case, the probability of reporting heads is 1/2. (b) What is the expected number of flips you must make before you report a result? I don't know how to start, any help would be appreciated!!!",,"['probability', 'statistics']"
79,What is that $t$ in $E[e^{itX}]$?,What is that  in ?,t E[e^{itX}],"I'm looking at the Characteristic Function Of A Random Variable. I don't understand why it's less than or equals to 1, since I see Mean Function on Random Variable ( $E[e^{iX}]$ ), which might be bigger than 1. And t, which I don't understand in the first place is my 1st suspect. Could someone explain me the role of that $t$ ? Referring to my latest comment after research on my own question, on How do the complex functions get built (here: How from Fourier Transform with imaginary numbers we get the one with real numbers? ).","I'm looking at the Characteristic Function Of A Random Variable. I don't understand why it's less than or equals to 1, since I see Mean Function on Random Variable ( ), which might be bigger than 1. And t, which I don't understand in the first place is my 1st suspect. Could someone explain me the role of that ? Referring to my latest comment after research on my own question, on How do the complex functions get built (here: How from Fourier Transform with imaginary numbers we get the one with real numbers? ).",E[e^{iX}] t,"['statistics', 'characteristic-functions']"
80,How to find the probability that we need to weigh at least 20 eggs before we find the 12th medium-sized egg,How to find the probability that we need to weigh at least 20 eggs before we find the 12th medium-sized egg,,"The mass of eggs in a farm is normally distributed with a mean of $575~\text{g}$ and standard deviation of $80~\text{g}$ . (a) An egg can be classified as having a “medium size” if its mass is between $500$ and $600$ grams. What is the probability that a randomly selected egg from the farm is a medium-sized egg? Use R to find the answer. (b) Let $M$ be the count of medium-sized eggs in a random sample of $100$ . What is the distribution of $M$ ? Using R, find the probability that at least half of the $100$ eggs are medium-sized. (c) Eggs from this farm are sold by the dozen (12 eggs). Let $D$ be the number of randomly selected eggs we must weigh until we find the 12th medium-sized egg. What is the distribution of $D$ ? Using R, find the probability that we need to weigh at least 20 eggs before we find the 12th medium-sized egg My attempt using Rstudio for a and b p <- pnorm(600,575,80)-pnorm(500,575,80) p [1] 0.448419 sum(dbinom(50:100, 50, p)) [1] 3.838845e-18 Now for part c do you use the part b and change 50 to 12 to find the distribution and then somehow find the probability using that to find the second part of part c? $$EDIT$$ Possible answer to part C, by $$P(X=10) = {20-1 \choose 12-1}0.0448419^{12}(1-0.044819)^{20-12}$$ Thoughts?","The mass of eggs in a farm is normally distributed with a mean of and standard deviation of . (a) An egg can be classified as having a “medium size” if its mass is between and grams. What is the probability that a randomly selected egg from the farm is a medium-sized egg? Use R to find the answer. (b) Let be the count of medium-sized eggs in a random sample of . What is the distribution of ? Using R, find the probability that at least half of the eggs are medium-sized. (c) Eggs from this farm are sold by the dozen (12 eggs). Let be the number of randomly selected eggs we must weigh until we find the 12th medium-sized egg. What is the distribution of ? Using R, find the probability that we need to weigh at least 20 eggs before we find the 12th medium-sized egg My attempt using Rstudio for a and b p <- pnorm(600,575,80)-pnorm(500,575,80) p [1] 0.448419 sum(dbinom(50:100, 50, p)) [1] 3.838845e-18 Now for part c do you use the part b and change 50 to 12 to find the distribution and then somehow find the probability using that to find the second part of part c? Possible answer to part C, by Thoughts?",575~\text{g} 80~\text{g} 500 600 M 100 M 100 D D EDIT P(X=10) = {20-1 \choose 12-1}0.0448419^{12}(1-0.044819)^{20-12},"['probability', 'statistics']"
81,which statistic test should I use for this question?,which statistic test should I use for this question?,,"there is a question like this: Two producers produce a cream with hydrating ingredient. According to the table state, whether the amount of the ingredient is significantly different or not. Use the level of significance 0.05 and assume that data are not from a normal distribution. 1st producer samples {1,52  ,1,57   ,1,71   ,1,34   ,1,68} 2nd producer samples {1,75  ,1,67   ,1,56   ,1,66   ,1,72   ,1,79   ,1,64   1,55} I know I have to solve it in with a non-parametric test, can I use Man-Whitney U test? or you suggest something else for this?","there is a question like this: Two producers produce a cream with hydrating ingredient. According to the table state, whether the amount of the ingredient is significantly different or not. Use the level of significance 0.05 and assume that data are not from a normal distribution. 1st producer samples {1,52  ,1,57   ,1,71   ,1,34   ,1,68} 2nd producer samples {1,75  ,1,67   ,1,56   ,1,66   ,1,72   ,1,79   ,1,64   1,55} I know I have to solve it in with a non-parametric test, can I use Man-Whitney U test? or you suggest something else for this?",,['statistics']
82,The proof of lemma 1 in Dempster's 1977 EM algorithm paper,The proof of lemma 1 in Dempster's 1977 EM algorithm paper,,"I am learning the proofs about general properties for the EM algorithm in Dempster's 1977 EM algorithm paper 1 . In it I found that the proof of Lemma 1, which is attached below, is referred to a conclusion in Rao's statistical inference book 2 . But when I checked the (1e5.6) formulae, I found it is just the Jensen's equation. While (1e6.6), which is attached below, has an extra application condition that $\int_S(f-g)\geq0$ , I suppose there shall be a corresponding  relationship between $\phi$ and $\phi'$ . In contrast, Dempster's lemma 1 says that $\phi$ and $\phi'$ could be any pairs, which makes me confused. Could you please tell me which part of my thinking is wrong or an entire proof for this lemma? I also found a related question on this topic: Jensen's inequality in derivation of EM algorithm . However, it is not totally the same. 1 Dempster, Arthur P., Nan M. Laird, and Donald B. Rubin. ""Maximum likelihood from incomplete data via the EM algorithm."" Journal of the Royal Statistical Society: Series B (Methodological) 39, no. 1 (1977): 1-22. 2 Rao, C.R., 1973. Linear statistical inference and its applications. New York: Wiley.","I am learning the proofs about general properties for the EM algorithm in Dempster's 1977 EM algorithm paper 1 . In it I found that the proof of Lemma 1, which is attached below, is referred to a conclusion in Rao's statistical inference book 2 . But when I checked the (1e5.6) formulae, I found it is just the Jensen's equation. While (1e6.6), which is attached below, has an extra application condition that , I suppose there shall be a corresponding  relationship between and . In contrast, Dempster's lemma 1 says that and could be any pairs, which makes me confused. Could you please tell me which part of my thinking is wrong or an entire proof for this lemma? I also found a related question on this topic: Jensen's inequality in derivation of EM algorithm . However, it is not totally the same. 1 Dempster, Arthur P., Nan M. Laird, and Donald B. Rubin. ""Maximum likelihood from incomplete data via the EM algorithm."" Journal of the Royal Statistical Society: Series B (Methodological) 39, no. 1 (1977): 1-22. 2 Rao, C.R., 1973. Linear statistical inference and its applications. New York: Wiley.",\int_S(f-g)\geq0 \phi \phi' \phi \phi',"['probability', 'statistics', 'expected-value']"
83,Fundamental question on test for normal distribution,Fundamental question on test for normal distribution,,"I have a fundamental question on statistical tests, particularly tests for normal distribution. As I understand statistical tests in general, they have a null hypothesis $H_0$ (e.g. the samples were drawn from a normal distribution) and an alternative hypothesis $H_1$ (e.g. the samples were not drawn from a normal distribution). If the test is significant ( $p < p_\alpha$ ) one can reject $H_0$ and assume that $H_1$ is true. However, if the test is not significant, one can not automatically assume that $H_0$ is true. Now, all tests for normal distribution that I read about have a $H_0$ that the samples were drawn from a normal distribution. Hence, the only thing you can do with these tests is to assume that the samples were not drawn from a normal distribution if the test is significant. You can't assume that the samples are drawn from a normal distribution if the test is not significant. But that's what everybody seems to be doing. Is there anything fundamentally wrong with my understanding of statistical tests? How can I ""prove"" that a given sample was drawn from a normal distribution?","I have a fundamental question on statistical tests, particularly tests for normal distribution. As I understand statistical tests in general, they have a null hypothesis (e.g. the samples were drawn from a normal distribution) and an alternative hypothesis (e.g. the samples were not drawn from a normal distribution). If the test is significant ( ) one can reject and assume that is true. However, if the test is not significant, one can not automatically assume that is true. Now, all tests for normal distribution that I read about have a that the samples were drawn from a normal distribution. Hence, the only thing you can do with these tests is to assume that the samples were not drawn from a normal distribution if the test is significant. You can't assume that the samples are drawn from a normal distribution if the test is not significant. But that's what everybody seems to be doing. Is there anything fundamentally wrong with my understanding of statistical tests? How can I ""prove"" that a given sample was drawn from a normal distribution?",H_0 H_1 p < p_\alpha H_0 H_1 H_0 H_0,"['statistics', 'hypothesis-testing']"
84,Can i use the coefficients of a trained model of Logistic Regression as a result itself without using the model on unseen data?,Can i use the coefficients of a trained model of Logistic Regression as a result itself without using the model on unseen data?,,"I'm trying to figure out if i can use a logistic regression as a predictive model, to estimate the probability of response of a user in CRM by having the predictors and i also have the class (detractor, not detractor) but the thing is that i don't want to estimate the probability of detraction since i already know the detractors, i already have the class and i'm always going to have it. What i was thinking was to train the model, use the probability given the predictors and then study the behavior of coefficients to know how this affects the probability. I will get the data periodically but is always going to have the class, so would it be ok to train the model everytime we get the data labeled (since we are going to make decisions everytime we train the model data should change and also the coefficients) and the results be the value of coefficients and influence in probability without having to apply a model on not seen data? Basically i want to know if this is valid in a statistical sense and also if this could be a good result to business, since what they want to know is how the independent variables that we capture, affect the result of a client saying that they will not recommend the use of the product. Thanks so much in advanced guys, sorry if i'm saying silly things, i'm not an expert in data science yet. Just starting.","I'm trying to figure out if i can use a logistic regression as a predictive model, to estimate the probability of response of a user in CRM by having the predictors and i also have the class (detractor, not detractor) but the thing is that i don't want to estimate the probability of detraction since i already know the detractors, i already have the class and i'm always going to have it. What i was thinking was to train the model, use the probability given the predictors and then study the behavior of coefficients to know how this affects the probability. I will get the data periodically but is always going to have the class, so would it be ok to train the model everytime we get the data labeled (since we are going to make decisions everytime we train the model data should change and also the coefficients) and the results be the value of coefficients and influence in probability without having to apply a model on not seen data? Basically i want to know if this is valid in a statistical sense and also if this could be a good result to business, since what they want to know is how the independent variables that we capture, affect the result of a client saying that they will not recommend the use of the product. Thanks so much in advanced guys, sorry if i'm saying silly things, i'm not an expert in data science yet. Just starting.",,"['statistics', 'machine-learning', 'data-analysis', 'data-mining']"
85,why with least squares I get a minimum?,why with least squares I get a minimum?,,I was reading about least squares method and every book I read just said that we can get the minimum value solving a equations system. For example. If I have $$ Q=\sum(Y_i-\beta_0-\beta_1X_i)^2 $$ then solving this $$ \frac{\partial Q}{\partial \beta_0}=0 $$ $$ \frac{\partial Q}{\partial \beta_1}=0 $$ We get a minimum value. But my question is how I know that the solution is a minimum and not a maximum nor a saddle point?,I was reading about least squares method and every book I read just said that we can get the minimum value solving a equations system. For example. If I have then solving this We get a minimum value. But my question is how I know that the solution is a minimum and not a maximum nor a saddle point?,"
Q=\sum(Y_i-\beta_0-\beta_1X_i)^2
 
\frac{\partial Q}{\partial \beta_0}=0
 
\frac{\partial Q}{\partial \beta_1}=0
","['statistics', 'optimization', 'least-squares', 'linear-regression']"
86,Loss functions for Regression task,Loss functions for Regression task,,"I am trying to understand the idea of Loss functions For Regression Task perfectly. I have read many textbooks and articles, and I came up with questions related to this subject. Several different uses of loss functions can be distinguished. (a) In prediction problems: a loss function depending on predicted and observed value defines the quality of a prediction. (b) In estimation problems: a loss function depending on the true parameter and the estimated value defines the quality of estimation. (c) Many estimators (such as least squares or M-estimators) are defined as optimizers of certain loss functions which then depend on the data and the estimated value. Now, since my focus is on Loss Functions For Regression Task ( $y_i=\theta_0 +\theta_1x_{i1}+\dots + \theta_px_{ip} +\epsilon_i$ ,y is the dependent variable and x is the independent one) My questions are as follows. Should I write the loss function formula as a function of the parameter or of the variables ( $\mathrm{L}\big(\theta,\hat\theta\big)$ or $\mathrm{L}\big(y,\hat y\big)$ )? Should I consider the Loss function formula for one point or Not (with sums or not)? Note My thought is to introduce Loss function first and then to use the standard notation for all Loss functions (least square, absolute value and Huber Loss, Quntile Loss and so on). UPDATED I did the following but I am not sure L2 Loss $$ \mathrm{L}\big(\{y_{(i)}, \hat{y}_{(i)}\}_{i=1}^n\big) = \mathrm{\sum}_{i=1}^n\big(y_i-\hat y_i\big)^2;$$ L1 Loss $$ \mathrm{L}\big(\{y_{(i)}, \hat{y}_{(i)}\}_{i=1}^n\big) = \mathrm{\sum}_{i=1}^n|y_i-\hat y_i|;$$ Huber Loss $$ \mathrm{L}\big(\{y_{(i)}, \hat{y}_{(i)}\}_{i=1}^n\big) = \begin{cases} \frac{1}{2}\mathrm{\sum}_{i=1}^n(y_i-\hat y_i)^2 & |y_i-\hat y_i| \leq \delta \\ \delta\mathrm{\sum}_{i=1}^n|y_i-\hat y_i|-\frac{1}{2}\delta^2 & \text{otherwise} \end{cases} $$ Log-Cosh Loss $$\mathrm{L}\big(\{y_{(i)}, \hat{y}_{(i)}\}_{i=1}^n\big)= \log[\cosh\mathrm{\sum}_{i=1}^n(y_i-\hat y_i)];$$ Quantile Loss $$\mathrm{L}\big(\{y_{(i)}, \hat{y}_{(i)}\}_{i=1}^n\big) = \big[\tau \mathrm{\sum}_{i=1}^n|y_i-\hat y_i| + (1 + \tau)\mathrm{\sum}_{i=1}^n(y_i-\hat y_i)\big].$$","I am trying to understand the idea of Loss functions For Regression Task perfectly. I have read many textbooks and articles, and I came up with questions related to this subject. Several different uses of loss functions can be distinguished. (a) In prediction problems: a loss function depending on predicted and observed value defines the quality of a prediction. (b) In estimation problems: a loss function depending on the true parameter and the estimated value defines the quality of estimation. (c) Many estimators (such as least squares or M-estimators) are defined as optimizers of certain loss functions which then depend on the data and the estimated value. Now, since my focus is on Loss Functions For Regression Task ( ,y is the dependent variable and x is the independent one) My questions are as follows. Should I write the loss function formula as a function of the parameter or of the variables ( or )? Should I consider the Loss function formula for one point or Not (with sums or not)? Note My thought is to introduce Loss function first and then to use the standard notation for all Loss functions (least square, absolute value and Huber Loss, Quntile Loss and so on). UPDATED I did the following but I am not sure L2 Loss L1 Loss Huber Loss Log-Cosh Loss Quantile Loss","y_i=\theta_0 +\theta_1x_{i1}+\dots + \theta_px_{ip} +\epsilon_i \mathrm{L}\big(\theta,\hat\theta\big) \mathrm{L}\big(y,\hat y\big)  \mathrm{L}\big(\{y_{(i)}, \hat{y}_{(i)}\}_{i=1}^n\big) = \mathrm{\sum}_{i=1}^n\big(y_i-\hat y_i\big)^2;  \mathrm{L}\big(\{y_{(i)}, \hat{y}_{(i)}\}_{i=1}^n\big) = \mathrm{\sum}_{i=1}^n|y_i-\hat y_i|; 
\mathrm{L}\big(\{y_{(i)}, \hat{y}_{(i)}\}_{i=1}^n\big) = \begin{cases}
\frac{1}{2}\mathrm{\sum}_{i=1}^n(y_i-\hat y_i)^2 & |y_i-\hat y_i| \leq \delta \\ \delta\mathrm{\sum}_{i=1}^n|y_i-\hat y_i|-\frac{1}{2}\delta^2 & \text{otherwise}
\end{cases}
 \mathrm{L}\big(\{y_{(i)}, \hat{y}_{(i)}\}_{i=1}^n\big)= \log[\cosh\mathrm{\sum}_{i=1}^n(y_i-\hat y_i)]; \mathrm{L}\big(\{y_{(i)}, \hat{y}_{(i)}\}_{i=1}^n\big) = \big[\tau \mathrm{\sum}_{i=1}^n|y_i-\hat y_i| + (1 + \tau)\mathrm{\sum}_{i=1}^n(y_i-\hat y_i)\big].","['statistics', 'functions', 'optimization', 'regression', 'machine-learning']"
87,Is Correlation a linear operator? and is it a measure in terms of measure thoery?,Is Correlation a linear operator? and is it a measure in terms of measure thoery?,,"Let X, Y be two vectors belong to $R^n$ , and $X_{c}$ and $Y_{c}$ are the centered versions of X and Y. Cov(X,Y) = $\frac{<X-\bar{X}, Y-\bar{Y}>}{n-1}$ and Corr(X,Y) = $\frac{<X_{c},Y_{c}>}{||X_{c}||*||Y_{c}||}$ . Is corr a linear operator in terms of algebra? and Is corr a measure in terms of measure theory? And how to verify that? Comparing 0.1, 0.5, 0.9 of corr, does the difference between 0.1 and 0.5 and the difference between 0.5 and 0.9 tell the same magnitude of difference in the correlation of data?","Let X, Y be two vectors belong to , and and are the centered versions of X and Y. Cov(X,Y) = and Corr(X,Y) = . Is corr a linear operator in terms of algebra? and Is corr a measure in terms of measure theory? And how to verify that? Comparing 0.1, 0.5, 0.9 of corr, does the difference between 0.1 and 0.5 and the difference between 0.5 and 0.9 tell the same magnitude of difference in the correlation of data?","R^n X_{c} Y_{c} \frac{<X-\bar{X}, Y-\bar{Y}>}{n-1} \frac{<X_{c},Y_{c}>}{||X_{c}||*||Y_{c}||}","['real-analysis', 'linear-algebra', 'probability', 'statistics']"
88,How many elements in a 3rd order tensor with certain symmetries?,How many elements in a 3rd order tensor with certain symmetries?,,"I have a tensor $A_{ijk}$ were indices $i$ , $j$ , and $k$ run from $1$ to $N$ . Obviously there are $N^3$ elements, but how many unique elements are there if the following symmetries exist? $A_{iij} = A_{iji} = A_{jii} $ $A_{ijk} = A_{ikj} = A_{jik} = A_{jki} = A_{kij} = A_{kji}$ These are all the symmetries that would exist with differentiation, since this tensor actually represents the third derivative of a function.","I have a tensor were indices , , and run from to . Obviously there are elements, but how many unique elements are there if the following symmetries exist? These are all the symmetries that would exist with differentiation, since this tensor actually represents the third derivative of a function.",A_{ijk} i j k 1 N N^3 A_{iij} = A_{iji} = A_{jii}  A_{ijk} = A_{ikj} = A_{jik} = A_{jki} = A_{kij} = A_{kji},"['combinatorics', 'statistics', 'tensors']"
89,"MLE for uniform distribution around $[-\theta,\theta]$ [duplicate]",MLE for uniform distribution around  [duplicate],"[-\theta,\theta]","This question already has answers here : Maximum Likelihood Estimator for $\theta$ when $X_1,\dots, X_n \sim U(-\theta,\theta)$ (2 answers) Closed 4 years ago . Given $X_1,\ldots,X_n$ , where $X_i\sim U(-\theta,\theta)$ , what the MLE for $\theta$ ? Apparently the answer is $\max\{|X_1|,\dots,|X_n|\}$ but I can't figure out why. The density function is $$f(x,\theta) = \begin{cases} \frac{1}{2\theta}, & x\in[-\theta,\theta] \\ 0, & \text{else} \end{cases}$$ I get a likelihood function that may decrease or increase for $\theta<0$ , depending on the parity of $n$ . I'm not sure if that's the way to solve this.","This question already has answers here : Maximum Likelihood Estimator for $\theta$ when $X_1,\dots, X_n \sim U(-\theta,\theta)$ (2 answers) Closed 4 years ago . Given , where , what the MLE for ? Apparently the answer is but I can't figure out why. The density function is I get a likelihood function that may decrease or increase for , depending on the parity of . I'm not sure if that's the way to solve this.","X_1,\ldots,X_n X_i\sim U(-\theta,\theta) \theta \max\{|X_1|,\dots,|X_n|\} f(x,\theta) = \begin{cases}
\frac{1}{2\theta}, & x\in[-\theta,\theta] \\
0, & \text{else}
\end{cases} \theta<0 n","['statistics', 'uniform-distribution', 'maximum-likelihood']"
90,why is the median an average of the two values?,why is the median an average of the two values?,,"Consider the following population [1, 2, 2, 3, 3, 3] Under the definition of median, it should be 2.5 I am curious why the convention is set for it to be 2.5 specifically. To me, if you were to choose a value of say, 2.2, it is still the case that half the population is less than this value, and half is more than it. Median under a continuous distribution is easier for me to understand, if you were to integrate to the median, half your mass would lie below you and other half would lie above. However in the discrete setting, it is the case that any value between 2 and 3 will satisfy this condition. I believe there is a deeper reason than just ""it's just a convention we adopted"", if you can tell me the exact reason it would be nice. thanks in advance","Consider the following population [1, 2, 2, 3, 3, 3] Under the definition of median, it should be 2.5 I am curious why the convention is set for it to be 2.5 specifically. To me, if you were to choose a value of say, 2.2, it is still the case that half the population is less than this value, and half is more than it. Median under a continuous distribution is easier for me to understand, if you were to integrate to the median, half your mass would lie below you and other half would lie above. However in the discrete setting, it is the case that any value between 2 and 3 will satisfy this condition. I believe there is a deeper reason than just ""it's just a convention we adopted"", if you can tell me the exact reason it would be nice. thanks in advance",,"['statistics', 'median']"
91,Chi squared test,Chi squared test,,"In the Chi Squared Test we build up a statistic $Q$ which converges in law to a $\chi^2$ as the number $n$ of observations goes to infinity.  So, if $n$ is ""big enough"", we choose to approximate $Q$ with the $\chi^2$ . I don't understand this approximation.  How can we deduce information on $Q$ by knowing its limiting law?  The limit does depend only on an arbitrary tail of the sequence, hence doesn't depend on $Q$ .","In the Chi Squared Test we build up a statistic which converges in law to a as the number of observations goes to infinity.  So, if is ""big enough"", we choose to approximate with the . I don't understand this approximation.  How can we deduce information on by knowing its limiting law?  The limit does depend only on an arbitrary tail of the sequence, hence doesn't depend on .",Q \chi^2 n n Q \chi^2 Q Q,"['probability', 'statistics', 'probability-distributions']"
92,Good test statistic,Good test statistic,,"If we have a hypothesis $H_0$ and alternative hypothesis $H_1$ , a test statistic $T$ and a data set $x_1, ..., x_n$ taken from some random sample $X_1, ..., X_n$ , we use $T(x_1, ..., x_n) = t$ to decide whether or not to reject $H_0$ . What I thought is, if we choose adequate $T$ , we can make the probability of observing an event at least as extreme as $t$ ( $P(T>t)$ , for example ), be low, and thus in favor of $H_1$ . So if we choose an appropriate $T$ we can deny $H_0$ . My question is: is this an easy job, is it reasonable to search for such $T$ and how can we say that a test statistic is efficient?","If we have a hypothesis and alternative hypothesis , a test statistic and a data set taken from some random sample , we use to decide whether or not to reject . What I thought is, if we choose adequate , we can make the probability of observing an event at least as extreme as ( , for example ), be low, and thus in favor of . So if we choose an appropriate we can deny . My question is: is this an easy job, is it reasonable to search for such and how can we say that a test statistic is efficient?","H_0 H_1 T x_1, ..., x_n X_1, ..., X_n T(x_1, ..., x_n) = t H_0 T t P(T>t) H_1 T H_0 T","['statistics', 'hypothesis-testing']"
93,Is sample variance always less than or equal to population variance,Is sample variance always less than or equal to population variance,,"I was reading this wikipedia article on Bessel's correction: https://en.wikipedia.org/wiki/Bessel%27s_correction .  The article says that sample variance is always less than or equal to population variance when sample variance is calculated using the sample mean.  However, if I create a numpy array containing 100,000 random normal data points, calculate the variance, then take 1000 element samples from the random normal data, I find that many of my samples have a higher variance than the 100,000 element population. import numpy as np  rand_norm = np.random.normal(size=100000)  # save the population variance pop_var = np.var(rand_norm, ddof=0)  # execute the following 2 lines a few times and and I find a variance of the  # sample that is higher than the variance of rand_normal samp=np.random.choice(rand_norm, 1000, replace=True)  # calculate the sample variance without correcting the bias (ddof = 0)  # I thought that the variance would always be less than or equal to pop_var. np.var(samp,ddof=0) Why am I getting sample variance which is greater than the population variance?","I was reading this wikipedia article on Bessel's correction: https://en.wikipedia.org/wiki/Bessel%27s_correction .  The article says that sample variance is always less than or equal to population variance when sample variance is calculated using the sample mean.  However, if I create a numpy array containing 100,000 random normal data points, calculate the variance, then take 1000 element samples from the random normal data, I find that many of my samples have a higher variance than the 100,000 element population. import numpy as np  rand_norm = np.random.normal(size=100000)  # save the population variance pop_var = np.var(rand_norm, ddof=0)  # execute the following 2 lines a few times and and I find a variance of the  # sample that is higher than the variance of rand_normal samp=np.random.choice(rand_norm, 1000, replace=True)  # calculate the sample variance without correcting the bias (ddof = 0)  # I thought that the variance would always be less than or equal to pop_var. np.var(samp,ddof=0) Why am I getting sample variance which is greater than the population variance?",,"['statistics', 'python']"
94,"Determining when a set is ordered, with noise and missing values","Determining when a set is ordered, with noise and missing values",,"The problem: I have repeated observations of the ordering of about 25-30 objects, and I have maybe 1000 sets of these objects. I wish to determine which sets of objects are ordered and which are not, but there are a couple complications. First, there are missing values in each observation. For each observation, about 60% of the objects are observed (on average, but this varies between sets). Second, I would like to account for some noise, so that even if a small number of the objects swap order or are in the wrong place on a given day, but most of the time most of the objects are in the same order, then it would be labeled as ordered. What I've done (and some problems with it): So far, I have created a variance-like measure (M) based off a “true” rank. The “true” rank is constructed by comparing each object i to each object j and determining the percent of observations for which rank(i) $<$ rank(j). From this, I pull a vector of row averages (averaging the percent of times object i is before every other object j), and I rank these. The object whose rank is truly “first” should have a high average, and so on. I then compare this “true” rank to the observed rank in the following way: $M=\frac{1}{D}\sum_d^D \sum_i^n \frac{1}{n_d^*} ( rank_{id}-\bar {rank}_{id} )^2$ where D is the number of observations d, i is the individual, and $\bar {rank}$ is the ""true"" rank for any given observation d. It is specific to the observation, because the ""true"" rank is adjusted for which objects are present in the observation (since some are missing each observation). Similarly, n* is the number of objects observed that day (observation). I then use some threshold value as a cutoff point. This seems to work, and I've used several thresholds, but the (main) issue is that it is not invariant to set size. That is, sets with higher missing rates appear to have a lower measure (M) than those with lower missing rates, simply because the potential difference in values is greater. Sets with higher missing rates are observably different in other areas, and so this method of dividing the data fails a balancing test. The second issue is that determining the threshold is a bit ad hoc, but I could probably live with that if the other issue was not so glaring. Other Thoughts: I've started looking at clustering, with the hope that I could test H0: single cluster vs H1: more than one cluster. This is in part because I want to do some hierarchical clustering with the unordered sets once I have determined which they are. There seem to be a few ways to test for the number of clusters, but (particularly in hierarchical clustering) I don't see many ways to conclude that there is a single cluster. It looks like the gap statistic might be able to do this in more of a k-means context (as well as some others in the k-means context), but testing for the existence of clusters with one type of clustering algorithm and then creating clusters later in the process via another seems strange to me. I'm sure there are other (possibly obvious) ways to think about an ordering problem like this, and am open to suggestions outside of these areas I've been thinking about.","The problem: I have repeated observations of the ordering of about 25-30 objects, and I have maybe 1000 sets of these objects. I wish to determine which sets of objects are ordered and which are not, but there are a couple complications. First, there are missing values in each observation. For each observation, about 60% of the objects are observed (on average, but this varies between sets). Second, I would like to account for some noise, so that even if a small number of the objects swap order or are in the wrong place on a given day, but most of the time most of the objects are in the same order, then it would be labeled as ordered. What I've done (and some problems with it): So far, I have created a variance-like measure (M) based off a “true” rank. The “true” rank is constructed by comparing each object i to each object j and determining the percent of observations for which rank(i) rank(j). From this, I pull a vector of row averages (averaging the percent of times object i is before every other object j), and I rank these. The object whose rank is truly “first” should have a high average, and so on. I then compare this “true” rank to the observed rank in the following way: where D is the number of observations d, i is the individual, and is the ""true"" rank for any given observation d. It is specific to the observation, because the ""true"" rank is adjusted for which objects are present in the observation (since some are missing each observation). Similarly, n* is the number of objects observed that day (observation). I then use some threshold value as a cutoff point. This seems to work, and I've used several thresholds, but the (main) issue is that it is not invariant to set size. That is, sets with higher missing rates appear to have a lower measure (M) than those with lower missing rates, simply because the potential difference in values is greater. Sets with higher missing rates are observably different in other areas, and so this method of dividing the data fails a balancing test. The second issue is that determining the threshold is a bit ad hoc, but I could probably live with that if the other issue was not so glaring. Other Thoughts: I've started looking at clustering, with the hope that I could test H0: single cluster vs H1: more than one cluster. This is in part because I want to do some hierarchical clustering with the unordered sets once I have determined which they are. There seem to be a few ways to test for the number of clusters, but (particularly in hierarchical clustering) I don't see many ways to conclude that there is a single cluster. It looks like the gap statistic might be able to do this in more of a k-means context (as well as some others in the k-means context), but testing for the existence of clusters with one type of clustering algorithm and then creating clusters later in the process via another seems strange to me. I'm sure there are other (possibly obvious) ways to think about an ordering problem like this, and am open to suggestions outside of these areas I've been thinking about.",< M=\frac{1}{D}\sum_d^D \sum_i^n \frac{1}{n_d^*} ( rank_{id}-\bar {rank}_{id} )^2 \bar {rank},"['statistics', 'data-analysis', 'clustering']"
95,Hypothesis testing: Two-tailed tests and deciding $H_0$,Hypothesis testing: Two-tailed tests and deciding,H_0,"I'm kind of studying Hypothesis testing by myself and was looking for a clarification on the next topic: In order to confirm my $H_1$ I basically need to reject $H_0$ . What do I do in case I cas assume either way? Let's assume there's a packing maching in a factory and I need to decide whether it's well-calibrated so the packings have the average weight of: $\mu_x=80g$ and suppose I also have the S.D. $\sigma_x=2g$ . Given a certain $\alpha$ , let's suppose $\alpha = 0.4$ . since it's a two-tailed test I need to devide 0.2 worth of area in either side. My question is: Do I need to hypothesize my assumptions in such a way that the area of rejection is always on the edges of the Gaussian curve? Let us suppose: $$H_0: \mu_x=0.8$$ $$H_1: \mu_x\neq 0.8$$ Then the rejection area is necessarily on the edges. But since im not asked to assume anything about this machine I can assume in advance it's not calibrated and then: $$H_0: \mu_x\not=0.8$$ $$H_1: \mu_x= 0.8$$ And now I supposedly have a limited rejection area symetrically around $\mu_x$ . I tryed to look around for a thumb-rule. All of the examples I've seen used the first method I described but never justified it or explained why. Is it a thumb-rule that the rejection area has to be infinite, or under the circumstances of the problem the latter way can be equally fine? If there is just one right way I'd like to know.","I'm kind of studying Hypothesis testing by myself and was looking for a clarification on the next topic: In order to confirm my I basically need to reject . What do I do in case I cas assume either way? Let's assume there's a packing maching in a factory and I need to decide whether it's well-calibrated so the packings have the average weight of: and suppose I also have the S.D. . Given a certain , let's suppose . since it's a two-tailed test I need to devide 0.2 worth of area in either side. My question is: Do I need to hypothesize my assumptions in such a way that the area of rejection is always on the edges of the Gaussian curve? Let us suppose: Then the rejection area is necessarily on the edges. But since im not asked to assume anything about this machine I can assume in advance it's not calibrated and then: And now I supposedly have a limited rejection area symetrically around . I tryed to look around for a thumb-rule. All of the examples I've seen used the first method I described but never justified it or explained why. Is it a thumb-rule that the rejection area has to be infinite, or under the circumstances of the problem the latter way can be equally fine? If there is just one right way I'd like to know.",H_1 H_0 \mu_x=80g \sigma_x=2g \alpha \alpha = 0.4 H_0: \mu_x=0.8 H_1: \mu_x\neq 0.8 H_0: \mu_x\not=0.8 H_1: \mu_x= 0.8 \mu_x,"['statistics', 'hypothesis-testing']"
96,"What does the notation $Z \sim N(\mu,\sigma^2)$ stand for in statistics?",What does the notation  stand for in statistics?,"Z \sim N(\mu,\sigma^2)","What does the notation $Z \sim N(\mu,\sigma^2)$ stand for in statistics? I was reading an answer in stats exchange and a user mentioned that. What does it mean?",What does the notation stand for in statistics? I was reading an answer in stats exchange and a user mentioned that. What does it mean?,"Z \sim N(\mu,\sigma^2)","['statistics', 'random-variables']"
97,How to argue why one dice is more rigged than the other?,How to argue why one dice is more rigged than the other?,,"Let $\omega$ be a finite set and $P : \Omega \rightarrow \mathbb{R}$ be a probability measure. You are given a set of three dices $\{A, B, C\}$ . The following table describes the outcome of six rollouts for these dices, where each column shows the outcome of the respective dice. (Note: assume the dices are standard six-sided dices with values between 1-6) $$ \begin{array}{|c|cccccc}{A} & {4} & {4} & {2} & {4} & {1} & {1} \\ \hline B & {3} & {6} & {3} & {3} & {4} & {3} \\ \hline C & {5} & {5} & {2} & {1} & {1} & {1}\end{array} $$ (1) Estimate the expectation and the variance for each dice using unbiased estimators. (Show your computations). (2) According to the data, which of them is the “most rigged”? Why? So for (1) I calculated: \begin{equation*}     \begin{aligned}         \overline{x}_A &= \frac{8}{3}   &, \qquad     s_A^2 &= \frac{34}{15} \\         \overline{x}_B &= \frac{11}{3}  &, \qquad     s_B^2 &= \frac{22}{15} \\         \overline{x}_A &= \frac{5}{2}   &, \qquad     s_C^2 &= \frac{51}{10} \\     \end{aligned} \end{equation*} In order to figure out which one is the most ""rigged"" one I calculated the expectation and variance of a ""perfect"" dice: \begin{equation*}     \begin{aligned}         \mathbb{E}[X] = \frac{7}{2} \qquad \mathbb{V}[X] = \frac{35}{12}     \end{aligned} \end{equation*} As the dices differ in expectation and variance from a ""perfect"" dice I am trying to figure out how I could argue which one is the most ""rigged"" one? The professor mentioned the Kullback–Leibler divergence in another setting very briefly and said that the KL divergence is a measure for the ""difference"" of distributions. Is this the tool he wants us to use here?","Let be a finite set and be a probability measure. You are given a set of three dices . The following table describes the outcome of six rollouts for these dices, where each column shows the outcome of the respective dice. (Note: assume the dices are standard six-sided dices with values between 1-6) (1) Estimate the expectation and the variance for each dice using unbiased estimators. (Show your computations). (2) According to the data, which of them is the “most rigged”? Why? So for (1) I calculated: In order to figure out which one is the most ""rigged"" one I calculated the expectation and variance of a ""perfect"" dice: As the dices differ in expectation and variance from a ""perfect"" dice I am trying to figure out how I could argue which one is the most ""rigged"" one? The professor mentioned the Kullback–Leibler divergence in another setting very briefly and said that the KL divergence is a measure for the ""difference"" of distributions. Is this the tool he wants us to use here?","\omega P : \Omega \rightarrow \mathbb{R} \{A, B, C\} 
\begin{array}{|c|cccccc}{A} & {4} & {4} & {2} & {4} & {1} & {1} \\ \hline B & {3} & {6} & {3} & {3} & {4} & {3} \\ \hline C & {5} & {5} & {2} & {1} & {1} & {1}\end{array}
 \begin{equation*}
    \begin{aligned}
        \overline{x}_A &= \frac{8}{3}   &, \qquad     s_A^2 &= \frac{34}{15} \\
        \overline{x}_B &= \frac{11}{3}  &, \qquad     s_B^2 &= \frac{22}{15} \\
        \overline{x}_A &= \frac{5}{2}   &, \qquad     s_C^2 &= \frac{51}{10} \\
    \end{aligned}
\end{equation*} \begin{equation*}
    \begin{aligned}
        \mathbb{E}[X] = \frac{7}{2} \qquad \mathbb{V}[X] = \frac{35}{12}
    \end{aligned}
\end{equation*}","['statistics', 'statistical-inference', 'machine-learning', 'information-theory', 'entropy']"
98,Characterization of the geometric distribution,Characterization of the geometric distribution,,"$X,Y$ are i.i.d. random variables with mean $\mu$ , and taking values in { $0,1,2,...$ }.Suppose for all $m \ge 0$ , $P(X=k|X+Y=m)=\frac{1}{m+1}$ , $k=0,1,...m$ . Find the distribution of $X$ in terms of $\mu$ . I have a feeling that $X$ is a geometric random variable. I wrote $P(X=k)=p_k$ and tried to equate the coefficient of $p_k$ on both sides but it was futile. My teacher gave me a hint to try with probability generating functions. But I really can't implement it. Please help.","are i.i.d. random variables with mean , and taking values in { }.Suppose for all , , . Find the distribution of in terms of . I have a feeling that is a geometric random variable. I wrote and tried to equate the coefficient of on both sides but it was futile. My teacher gave me a hint to try with probability generating functions. But I really can't implement it. Please help.","X,Y \mu 0,1,2,... m \ge 0 P(X=k|X+Y=m)=\frac{1}{m+1} k=0,1,...m X \mu X P(X=k)=p_k p_k","['probability', 'probability-theory']"
99,Why is the sum of two independent geometrically distributed RVs not geometrically distributed again?,Why is the sum of two independent geometrically distributed RVs not geometrically distributed again?,,"Let X = Geo(1/4) and Y = Geo(1/4) (both independent) and let Z = X + Y. Why is X + Y not Geo(0.5) for example and how could I, as an example, calculate P(Z = 3)?","Let X = Geo(1/4) and Y = Geo(1/4) (both independent) and let Z = X + Y. Why is X + Y not Geo(0.5) for example and how could I, as an example, calculate P(Z = 3)?",,"['probability', 'statistics', 'probability-distributions']"
