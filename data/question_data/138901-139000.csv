,title_raw,title_text,title_latex,body_raw,body_text,body_latex,tags
0,Can an differential equation be a member of many classes of differential equations?,Can an differential equation be a member of many classes of differential equations?,,"I just started learning ODE online and I'm not sure I fully understand when I should use one method or the other. For example, I know that $\frac{dy}{dx}=\frac{y}{x}$ is separable, but is it homogeneous? I tried to solve it using the homogeneous approach but I get nonsense. Another example is bernoulli equations, like $\frac{dy}{dx}-\frac{y}{x}=xy^{n}$ whenever $n=1$ I also get nonsense, can anyone help me?","I just started learning ODE online and I'm not sure I fully understand when I should use one method or the other. For example, I know that is separable, but is it homogeneous? I tried to solve it using the homogeneous approach but I get nonsense. Another example is bernoulli equations, like whenever I also get nonsense, can anyone help me?",\frac{dy}{dx}=\frac{y}{x} \frac{dy}{dx}-\frac{y}{x}=xy^{n} n=1,['ordinary-differential-equations']
1,First Order Linear Differential Equations having two solutions,First Order Linear Differential Equations having two solutions,,I have a linear first order ordinary differential equation $$\frac{dy}{dx}+\tan (x) y = 2 \cos^2 x \sin x - \sec x$$ with an initial condition as $y(\frac{\pi}{4})=3 \sqrt2$ My integrating factor $\mu(x)=\sec x$ After multiplication with the integration factor what I get is: $$(secx\ y)'=sin2x-sec^2x$$ or $$(secx\ y)'=2 sinx cosx-sec^2x$$ If I use the first equation I get: $$y(x)=\frac{\frac{1}{2}cos2x-tanx+c}{secx}$$ and using the second equation I get: $$y(x)=\frac{sin^2x-tanx+c}{secx}$$ ( $\int 2\ sinx\ cosx\ dx = 2 \frac{sin^2x}{2}=sin^2x$ ) with the first equation I get $c=7$ and second equation I get $c=\frac{13}{2}$ . It is a very simple differential equation but when I solve it I get two different answers. Is this ok?,I have a linear first order ordinary differential equation with an initial condition as My integrating factor After multiplication with the integration factor what I get is: or If I use the first equation I get: and using the second equation I get: ( ) with the first equation I get and second equation I get . It is a very simple differential equation but when I solve it I get two different answers. Is this ok?,\frac{dy}{dx}+\tan (x) y = 2 \cos^2 x \sin x - \sec x y(\frac{\pi}{4})=3 \sqrt2 \mu(x)=\sec x (secx\ y)'=sin2x-sec^2x (secx\ y)'=2 sinx cosx-sec^2x y(x)=\frac{\frac{1}{2}cos2x-tanx+c}{secx} y(x)=\frac{sin^2x-tanx+c}{secx} \int 2\ sinx\ cosx\ dx = 2 \frac{sin^2x}{2}=sin^2x c=7 c=\frac{13}{2},"['calculus', 'ordinary-differential-equations']"
2,ODE with discontinuous function,ODE with discontinuous function,,"This exercise was given to me: to solve the following ODE over $[0,+\infty)$ $$y'+y=H(t),$$ where $$H(t) = \begin{cases}            1 & 0\leq t\leq 1 \\           -1 & t\gt 1         \end{cases}       $$ So i've multiplied both sides by $e^t$ to get that the ODE is equivalent to $$(e^ty)' = e^tH(t).$$ However RHS of this clearly doesn't satisfy IVT, so it is not the derivative of a function, therefore the ODE has no solutions. Is this correct or am i missing something?","This exercise was given to me: to solve the following ODE over where So i've multiplied both sides by to get that the ODE is equivalent to However RHS of this clearly doesn't satisfy IVT, so it is not the derivative of a function, therefore the ODE has no solutions. Is this correct or am i missing something?","[0,+\infty) y'+y=H(t), H(t) = \begin{cases} 
          1 & 0\leq t\leq 1 \\
          -1 & t\gt 1 
       \end{cases}
       e^t (e^ty)' = e^tH(t).","['calculus', 'ordinary-differential-equations']"
3,Differential Equation - I'm missing something stupid,Differential Equation - I'm missing something stupid,,"I have the follwoing ODE to solve: $\dfrac{dy}{dx}+3y=28e^{2x}y^{-3}$ This is a Bernoulli equation so I make the substitution $ u=y^{1-n} \; \;u=y^4 \; \;y=u^{1/4}$ . $$ \begin{split} \frac{du^{{1/4}}}{dx} + 3u^{{1/4}} &= 28e^{2x}u^{-3/4} \\ u^{{-3/4}}\frac{du}{dx}+3u^{1/4} &= 28e^{2x}u^{-3/4} \\ \frac{du}{dx} + 3u &=28e^{2x} \end{split} $$ So we have a linear differential equation $\dfrac{dy}{dx}+p(x)y=f(x)$ with $p(x)=3$ . Multiplying through by ${e}^{\int{{3}dx}}\;\;$ $$ \begin{split} e^{3x}\frac{du}{dx}+3ue^{3x} &= 28e^{5x}  \quad \text{(expanded form from the product rule)} \\ \frac{d\left[u e^{3x}\right]}{dx} &= 28e^{5x} \\ {u}{e^{3x}} &= \int{28e^{5x}dx} \\ u &=\frac{28}{5}e^{2x} + \frac{c}{e^{3x}} \\ y &= \left(\frac{28}{5}e^{2x}+\frac{c}{e^{3x}}\right)^{1/4} \end{split} $$ However, the answer I am given is $$ y = \left(8e^{2x}+\frac{c}{e^{12x}}\right)^{1/4} $$ I've tried this about 10 times now and can't see where I am making a mistake. Note:  I'm an amateur doing self study so there's nobody I can ask.  I'm sure there is an obvious error but I can't find it.","I have the follwoing ODE to solve: This is a Bernoulli equation so I make the substitution . So we have a linear differential equation with . Multiplying through by However, the answer I am given is I've tried this about 10 times now and can't see where I am making a mistake. Note:  I'm an amateur doing self study so there's nobody I can ask.  I'm sure there is an obvious error but I can't find it.","\dfrac{dy}{dx}+3y=28e^{2x}y^{-3}  u=y^{1-n} \; \;u=y^4 \; \;y=u^{1/4} 
\begin{split}
\frac{du^{{1/4}}}{dx} + 3u^{{1/4}} &= 28e^{2x}u^{-3/4} \\
u^{{-3/4}}\frac{du}{dx}+3u^{1/4} &= 28e^{2x}u^{-3/4} \\
\frac{du}{dx} + 3u &=28e^{2x}
\end{split}
 \dfrac{dy}{dx}+p(x)y=f(x) p(x)=3 {e}^{\int{{3}dx}}\;\; 
\begin{split}
e^{3x}\frac{du}{dx}+3ue^{3x} &= 28e^{5x}
 \quad \text{(expanded form from the product rule)} \\
\frac{d\left[u e^{3x}\right]}{dx} &= 28e^{5x} \\
{u}{e^{3x}} &= \int{28e^{5x}dx} \\
u &=\frac{28}{5}e^{2x} + \frac{c}{e^{3x}} \\
y &= \left(\frac{28}{5}e^{2x}+\frac{c}{e^{3x}}\right)^{1/4}
\end{split}
 
y = \left(8e^{2x}+\frac{c}{e^{12x}}\right)^{1/4}
","['ordinary-differential-equations', 'solution-verification', 'substitution']"
4,Is there a solution $y(t)$ for $y''+e^ty=0$ satisfying $\lim\limits_{t\to \infty} y(t)=1$,Is there a solution  for  satisfying,y(t) y''+e^ty=0 \lim\limits_{t\to \infty} y(t)=1,Is there a solution $y(t)$ for $y''+e^ty=0$ satisfying $\lim\limits_{t\to \infty} y(t)=2021$ If we write the eq as $\dfrac{y''}{e^{-t}}+y=0$ then $y$ cannot be equal to non zero constant because of uniqueness? How to approach that kind of problem?,Is there a solution for satisfying If we write the eq as then cannot be equal to non zero constant because of uniqueness? How to approach that kind of problem?,y(t) y''+e^ty=0 \lim\limits_{t\to \infty} y(t)=2021 \dfrac{y''}{e^{-t}}+y=0 y,['ordinary-differential-equations']
5,Differential equation non homogeneous [closed],Differential equation non homogeneous [closed],,"Closed. This question does not meet Mathematics Stack Exchange guidelines . It is not currently accepting answers. Please provide additional context , which ideally explains why the question is relevant to you and our  community. Some forms of context include: background and motivation, relevant definitions, source, possible strategies, your current progress, why the question is interesting or important, etc. Closed 3 years ago . Improve this question Pls help me with this $$2y''–y'–3y= 5e^{3x/2}$$ I've solved the $yc$ part but the particular solution is given me zero I also try to use $y=Axe^{3x/2}$ but I keep getting $x$ in my answer please help","Closed. This question does not meet Mathematics Stack Exchange guidelines . It is not currently accepting answers. Please provide additional context , which ideally explains why the question is relevant to you and our  community. Some forms of context include: background and motivation, relevant definitions, source, possible strategies, your current progress, why the question is interesting or important, etc. Closed 3 years ago . Improve this question Pls help me with this I've solved the part but the particular solution is given me zero I also try to use but I keep getting in my answer please help",2y''–y'–3y= 5e^{3x/2} yc y=Axe^{3x/2} x,['ordinary-differential-equations']
6,Solving PDE with Laplace Transform - Non Homogenous ODE,Solving PDE with Laplace Transform - Non Homogenous ODE,,"Given the following PDE: $$u_{tt}(x,t)-u_{xx}(x,t)=e^{-t}$$ $$-\infty <x<+ \infty , t>0$$ $$u(x,0)=0,u_t(x,0)=\frac{1}{1+x^2}$$ solve it by using the Laplace transform. Here's my attempt so far: Let $$\mathcal L \{u(x,t) \} \equiv U(x,s)\equiv\int_0^\infty u(x,t)e^{-st} \,dt .$$ and the result I've calculated is: $$U_{xx}(x,s)-s^2 U(x,s) = -\frac{1}{1+x^2}-\frac{1}{s+1}=f(x,s)$$ I've concluded that the relation above is a non-homogenous 2nd order ODE in respect to the variable x. I have tried to solve it but so far it hasn't worked. Which method do you think would be the best to solve it? Could you provide some guiding steps?",Given the following PDE: solve it by using the Laplace transform. Here's my attempt so far: Let and the result I've calculated is: I've concluded that the relation above is a non-homogenous 2nd order ODE in respect to the variable x. I have tried to solve it but so far it hasn't worked. Which method do you think would be the best to solve it? Could you provide some guiding steps?,"u_{tt}(x,t)-u_{xx}(x,t)=e^{-t} -\infty <x<+ \infty , t>0 u(x,0)=0,u_t(x,0)=\frac{1}{1+x^2} \mathcal L \{u(x,t) \} \equiv U(x,s)\equiv\int_0^\infty u(x,t)e^{-st} \,dt . U_{xx}(x,s)-s^2 U(x,s) = -\frac{1}{1+x^2}-\frac{1}{s+1}=f(x,s)","['ordinary-differential-equations', 'partial-differential-equations', 'laplace-transform']"
7,Potentially Ambiguous Stochastic Calculus Question,Potentially Ambiguous Stochastic Calculus Question,,"In the Lecture Notes for my Stochastic Calculus module, my lecturer provides the following question as an essential exercise in preparation for the exam. It reads as follows: Let $(W_t)_{t≥0}$ be a one-dimensional Wiener process. In the solution that you found in the previous exercise, replace $G_t$ by $W_t$ , that is, set $X_t = xe^{at + bW_t}$ . Write an equation for X. Now, the previous exercise asked us to find a solution to the equation: $$d X_t = aX_t\, dt + bX_t \, dG_t, \,\,\,\,\,\,\,\,\,\,\,\, X_0 = x,$$ where $G \in C^1$ , i.e. $G$ is a continuously differentiable function and $G(0) = 0$ , and $a,b \in \mathbb{R}$ . My solution, as suggested in the original exercise was $$X_t = x e^{at + bG_t}$$ I'm not too sure what the process would be to answer the original question. It seems fairly ambiguous - am I essentially just reverse-engineering the 'previous exercise' and trying to find some sort of differential equation? If so, how would I do this?","In the Lecture Notes for my Stochastic Calculus module, my lecturer provides the following question as an essential exercise in preparation for the exam. It reads as follows: Let be a one-dimensional Wiener process. In the solution that you found in the previous exercise, replace by , that is, set . Write an equation for X. Now, the previous exercise asked us to find a solution to the equation: where , i.e. is a continuously differentiable function and , and . My solution, as suggested in the original exercise was I'm not too sure what the process would be to answer the original question. It seems fairly ambiguous - am I essentially just reverse-engineering the 'previous exercise' and trying to find some sort of differential equation? If so, how would I do this?","(W_t)_{t≥0} G_t W_t X_t = xe^{at + bW_t} d X_t = aX_t\, dt + bX_t \, dG_t, \,\,\,\,\,\,\,\,\,\,\,\, X_0 = x, G \in C^1 G G(0) = 0 a,b \in \mathbb{R} X_t = x e^{at + bG_t}","['ordinary-differential-equations', 'stochastic-processes', 'stochastic-calculus', 'brownian-motion']"
8,How to solve this homogeneous partial differential equation,How to solve this homogeneous partial differential equation,,"I'm trying to solve this second order differential equation, but it seems my solution isn’t accurate, since i could not find the correct $\sigma$ in $$ \ddot \sigma - p e^\sigma - q e^{2\sigma} =0\qquad\qquad(1)$$ or $$\frac{d^2 \sigma}{dt^2} - p e^\sigma - q e^{2\sigma} =0$$ where $p$ and $q$ are constants. So any help is appreciated. Here's what I have tried: Let $\sigma = \log ~ r$ , then: $ \dot \sigma= \frac{\dot r}{ r}$ , and $\ddot\sigma= \frac{\ddot r}{ r} -  \frac{\dot r^2}{r^2}$ . Sub in (1) $$ \frac{\ddot r}{ r} -  \frac{\dot r^2}{r^2} - r^2 q - r p  =0\qquad\qquad(2)$$ Now to solve (2), will I use something like $ r = e^{\lambda t}$ again? Then (2) becomes: $$ \lambda^2 - \lambda^2 -  e^{\lambda t} q - p =0 \qquad\qquad(3) $$ therefore $\lambda = \frac{1}{t}~ \log~ \frac{p}{q} $ , or $ r = \frac{p}{q} $ and $ \sigma = \log \frac{p}{q} $ . This solution can not be, cause it means $ \dot \sigma = \ddot \sigma =0!!! $ Have I missed something?? Thanks.","I'm trying to solve this second order differential equation, but it seems my solution isn’t accurate, since i could not find the correct in or where and are constants. So any help is appreciated. Here's what I have tried: Let , then: , and . Sub in (1) Now to solve (2), will I use something like again? Then (2) becomes: therefore , or and . This solution can not be, cause it means Have I missed something?? Thanks.",\sigma  \ddot \sigma - p e^\sigma - q e^{2\sigma} =0\qquad\qquad(1) \frac{d^2 \sigma}{dt^2} - p e^\sigma - q e^{2\sigma} =0 p q \sigma = \log ~ r  \dot \sigma= \frac{\dot r}{ r} \ddot\sigma= \frac{\ddot r}{ r} -  \frac{\dot r^2}{r^2}  \frac{\ddot r}{ r} -  \frac{\dot r^2}{r^2} - r^2 q - r p  =0\qquad\qquad(2)  r = e^{\lambda t}  \lambda^2 - \lambda^2 -  e^{\lambda t} q - p =0 \qquad\qquad(3)  \lambda = \frac{1}{t}~ \log~ \frac{p}{q}   r = \frac{p}{q}   \sigma = \log \frac{p}{q}   \dot \sigma = \ddot \sigma =0!!! ,['ordinary-differential-equations']
9,How to convert an integral equation to a boundary value problem.,How to convert an integral equation to a boundary value problem.,,"I have the integral equation $$f(x)=\int_0^x (x-t)f(t)dt$$ and want to turn it into an boundary value problem, so I differentiate both sides with respect to $x$ and I get $$f'(x)=\int_0^x f(t)dt$$ Differentiate again and I get $$f''(x)=f(x)$$ However I don't know how to obtain the boundary values for $$f''(x)=f(x)$$ , there must be two boundary values of the form $$f(a)+f'(a)=A$$ and $$f(b)+f'(b)=B$$ , can you explain how to get them?","I have the integral equation and want to turn it into an boundary value problem, so I differentiate both sides with respect to and I get Differentiate again and I get However I don't know how to obtain the boundary values for , there must be two boundary values of the form and , can you explain how to get them?",f(x)=\int_0^x (x-t)f(t)dt x f'(x)=\int_0^x f(t)dt f''(x)=f(x) f''(x)=f(x) f(a)+f'(a)=A f(b)+f'(b)=B,"['ordinary-differential-equations', 'boundary-value-problem', 'integral-equations']"
10,"Is the real part of a complex solution to a linear differential equation always a solution? If so, why?","Is the real part of a complex solution to a linear differential equation always a solution? If so, why?",,So the general solution $f(t)$ to a linear differential equation with real coefficients can be written as $$f(t)=a_1e^{A_1t}+a_2e^{A_2t}+a_3e^{A_3t}+...$$ Would $\text{Im}(f(t))$ and $\text{Re}(f(t))$ also be solutions? That's the idea I'm getting from the following questions: What is the meaning of having imaginary solutions to a differential equation Imaginey and real part of Answer of differential equation although I can't quite understand why this works.,So the general solution to a linear differential equation with real coefficients can be written as Would and also be solutions? That's the idea I'm getting from the following questions: What is the meaning of having imaginary solutions to a differential equation Imaginey and real part of Answer of differential equation although I can't quite understand why this works.,f(t) f(t)=a_1e^{A_1t}+a_2e^{A_2t}+a_3e^{A_3t}+... \text{Im}(f(t)) \text{Re}(f(t)),['ordinary-differential-equations']
11,Classification of strange critical point in the plane,Classification of strange critical point in the plane,,"What type is the critical point $(0, 0)$ for the following system? \begin{align} &x' = y\, ,\\ &y' = -x^2 - x^3\, . \end{align} The critical points for this system are: $(-1, 0)$ , a center; and $(0, 0)$ . The Jacobian for this problem is $$\pmatrix{0&1\cr -3\,x^2-2\,x&0\cr }\, ,$$ and in $(0,0)$ is $$\pmatrix{0&1\cr 0&0\cr }\, .$$ That  is not-normal and has only one eigenvector. The following is the phase portrait for the system, with a contour plot for the conserved quantity, $$V(x, y) = \frac{(3x + 4)x^3}{12} + \frac{y^2}{2}\, ,$$ and a trajectory for the value of $0$ , the one that cross the $(0, 0)$ critical point. Perturbed critical point If we perturb the system as \begin{align} &x' = y\, ,\\ &y' = -(x^2 - \epsilon^2) (1 + x)\, , \end{align} We have the following critical points: $(-1, 0)$ , a center; $(-\epsilon, 0)$ , a saddle point; and $(\epsilon, 0)$ , a center. So, the original point seems to be a merging of these last two points.","What type is the critical point for the following system? The critical points for this system are: , a center; and . The Jacobian for this problem is and in is That  is not-normal and has only one eigenvector. The following is the phase portrait for the system, with a contour plot for the conserved quantity, and a trajectory for the value of , the one that cross the critical point. Perturbed critical point If we perturb the system as We have the following critical points: , a center; , a saddle point; and , a center. So, the original point seems to be a merging of these last two points.","(0, 0) \begin{align}
&x' = y\, ,\\
&y' = -x^2 - x^3\, .
\end{align} (-1, 0) (0, 0) \pmatrix{0&1\cr -3\,x^2-2\,x&0\cr }\, , (0,0) \pmatrix{0&1\cr 0&0\cr }\, . V(x, y) = \frac{(3x + 4)x^3}{12} + \frac{y^2}{2}\, , 0 (0, 0) \begin{align}
&x' = y\, ,\\
&y' = -(x^2 - \epsilon^2) (1 + x)\, ,
\end{align} (-1, 0) (-\epsilon, 0) (\epsilon, 0)","['ordinary-differential-equations', 'dynamical-systems']"
12,Analytic solution to a Volterra integral equation with kernel divided by continous differentiable function,Analytic solution to a Volterra integral equation with kernel divided by continous differentiable function,,"I have been struggling with the following Volterra integral equation for days: $$f(x)=A_{1}-\frac{A_{2}}{h(x)}+\lambda \int_{a}^{x}\frac{h^{\prime }(s)}{h(x)}f(s)ds$$ , where a, A1 and A2 are positive constants, $\lambda$ is a negative constant, and $h(x)$ is a continuously differentiable function. It is a Volterra integral equation of second kind, and I have tried Adomian Decomposition Method or converting the problem to an ODE of initial value, but I am not able to solve it even if I consider the simplest case: h(x)=x. Does this type of integral equation have analytic solution? May Laplace transformation help to solve it? If I have to resort to numerical methods, are there available libraries for Mathematica, Matlab or other software I could use? Any help will be greatly appreciated.","I have been struggling with the following Volterra integral equation for days: , where a, A1 and A2 are positive constants, is a negative constant, and is a continuously differentiable function. It is a Volterra integral equation of second kind, and I have tried Adomian Decomposition Method or converting the problem to an ODE of initial value, but I am not able to solve it even if I consider the simplest case: h(x)=x. Does this type of integral equation have analytic solution? May Laplace transformation help to solve it? If I have to resort to numerical methods, are there available libraries for Mathematica, Matlab or other software I could use? Any help will be greatly appreciated.","f(x)=A_{1}-\frac{A_{2}}{h(x)}+\lambda \int_{a}^{x}\frac{h^{\prime
}(s)}{h(x)}f(s)ds \lambda h(x)","['integration', 'ordinary-differential-equations', 'laplace-transform']"
13,Finding equilibrium point from a differential equation [closed],Finding equilibrium point from a differential equation [closed],,"Closed. This question does not meet Mathematics Stack Exchange guidelines . It is not currently accepting answers. Please provide additional context , which ideally explains why the question is relevant to you and our  community. Some forms of context include: background and motivation, relevant definitions, source, possible strategies, your current progress, why the question is interesting or important, etc. Closed 3 years ago . Improve this question I have the differential equation of a system as given below, $$ m\frac{d^2H}{dt^2} = mg-k\frac{I^2}{H^2} $$ How do I find the equilibrium points of this?","Closed. This question does not meet Mathematics Stack Exchange guidelines . It is not currently accepting answers. Please provide additional context , which ideally explains why the question is relevant to you and our  community. Some forms of context include: background and motivation, relevant definitions, source, possible strategies, your current progress, why the question is interesting or important, etc. Closed 3 years ago . Improve this question I have the differential equation of a system as given below, How do I find the equilibrium points of this?", m\frac{d^2H}{dt^2} = mg-k\frac{I^2}{H^2} ,"['calculus', 'ordinary-differential-equations', 'derivatives']"
14,Nontrivial periodic solutions of second order ODE,Nontrivial periodic solutions of second order ODE,,"We consider the ODE $$x''(t)+x^3(t)=0.$$ Clearly, $x\equiv 0$ is a solution. But also, numerically solving this ODE shows that nontrivial periodic solutions exist, too. I am interested in proving the statement that the above ODE has a nontrivial periodic (say, $2\pi$ periodic) solution. How does one go about proving such a statement?","We consider the ODE Clearly, is a solution. But also, numerically solving this ODE shows that nontrivial periodic solutions exist, too. I am interested in proving the statement that the above ODE has a nontrivial periodic (say, periodic) solution. How does one go about proving such a statement?",x''(t)+x^3(t)=0. x\equiv 0 2\pi,"['ordinary-differential-equations', 'analysis']"
15,Solving a Second Order Linear Homogeneous ODE Without an Ansatz/Characteristic Polynomial.,Solving a Second Order Linear Homogeneous ODE Without an Ansatz/Characteristic Polynomial.,,"I wanted to try solving the following differential equation: $$y''+ay'+by=0$$ without using the ansatz of $y=e^{kt}$ . This means using separation of variables, substitutions, or an integrating factor to possibly find the solution. If we let $a=0$ , it becomes easier to find a solution with that constraint: $$y''=-by$$ $$\int y''\cdot y'dt=-b\int y\cdot y'dt$$ $$\left(y'\right)^{2}=-by^{2}+c$$ $$\frac{dy}{dt}=\sqrt{c-by^{2}}$$ $$\frac{1}{\sqrt{c}}\int \frac{dy}{\sqrt{1-\frac{b}{c}y^{2}}}=\int_{ }^{ }dt$$ let $\omega=\sqrt{b}$ , and $R=\sqrt{\frac{c}{b}}$ : $$\frac{1}{\omega}\sin^{-1}\left(\frac{1}{R}y\right)=t+D$$ let $D\cdot\omega=\delta$ $$y(t)=R\sin\left(\omega t+\delta\right)$$ In fact any equation of the form $$y''=-by^{n}$$ can be simplified into a solution containing an indefinite integral at least (since u sub can be done on the right). If I tried the same technique, but when a is nonzero. I end up with: $$\int_{ }^{ }y''\cdot y'dt+\int_{ }^{ }a\left(y'\right)^{2}dt+\int_{ }^{ }by\cdot y'dt=0$$ I don't know how to integrate the $\left(y'\right)^{2}$ term though. I then tried using an integration factor. I wrote the equation as: $$-\frac{1}{a}y''=y'+\frac{b}{a}y$$ let $\mu\left(t\right)=e^{\frac{b}{a}t}$ . By doing some rearranging: $$-\frac{1}{a}\int_{ }^{ }e^{\frac{b}{a}t}y''dt=e^{\frac{b}{a}t}y$$ I don't know how to the integral on the left either :S I know that the actual solution ends up being an exponential function times a trig function (through the ansatz method, which is admittedly much faster), but I would like to derive the solution without using that method. Thanks EDIT: I think I specifically want to preserve the part where you integrate to get arcsin. Using an ansatz, and then eulers method sort of gets around that step. I guess I wanted to see specifically where that trig function comes in, and that was easy to see when a=0.","I wanted to try solving the following differential equation: without using the ansatz of . This means using separation of variables, substitutions, or an integrating factor to possibly find the solution. If we let , it becomes easier to find a solution with that constraint: let , and : let In fact any equation of the form can be simplified into a solution containing an indefinite integral at least (since u sub can be done on the right). If I tried the same technique, but when a is nonzero. I end up with: I don't know how to integrate the term though. I then tried using an integration factor. I wrote the equation as: let . By doing some rearranging: I don't know how to the integral on the left either :S I know that the actual solution ends up being an exponential function times a trig function (through the ansatz method, which is admittedly much faster), but I would like to derive the solution without using that method. Thanks EDIT: I think I specifically want to preserve the part where you integrate to get arcsin. Using an ansatz, and then eulers method sort of gets around that step. I guess I wanted to see specifically where that trig function comes in, and that was easy to see when a=0.",y''+ay'+by=0 y=e^{kt} a=0 y''=-by \int y''\cdot y'dt=-b\int y\cdot y'dt \left(y'\right)^{2}=-by^{2}+c \frac{dy}{dt}=\sqrt{c-by^{2}} \frac{1}{\sqrt{c}}\int \frac{dy}{\sqrt{1-\frac{b}{c}y^{2}}}=\int_{ }^{ }dt \omega=\sqrt{b} R=\sqrt{\frac{c}{b}} \frac{1}{\omega}\sin^{-1}\left(\frac{1}{R}y\right)=t+D D\cdot\omega=\delta y(t)=R\sin\left(\omega t+\delta\right) y''=-by^{n} \int_{ }^{ }y''\cdot y'dt+\int_{ }^{ }a\left(y'\right)^{2}dt+\int_{ }^{ }by\cdot y'dt=0 \left(y'\right)^{2} -\frac{1}{a}y''=y'+\frac{b}{a}y \mu\left(t\right)=e^{\frac{b}{a}t} -\frac{1}{a}\int_{ }^{ }e^{\frac{b}{a}t}y''dt=e^{\frac{b}{a}t}y,['ordinary-differential-equations']
16,Why linear independency of solutions of ordinary differential equations or functions is important?,Why linear independency of solutions of ordinary differential equations or functions is important?,,There are so many methods for checking linear independency of functions. But why do we require this? I mean what is the use to find out whether functions or solutions are linear independent or not.,There are so many methods for checking linear independency of functions. But why do we require this? I mean what is the use to find out whether functions or solutions are linear independent or not.,,"['linear-algebra', 'ordinary-differential-equations', 'linear-transformations']"
17,If $X$ and $Y$ are fundamental matrices then exists $C$ invertible such that $Y=XC$,If  and  are fundamental matrices then exists  invertible such that,X Y C Y=XC,"If $X(t)$ and $Y(t)$ are fundamental matrices then exists $C\in M_n$ invertible such thta $Y(t)=X(t)C$ Both for the homogeneous linear system $x'=A(t)x$ . attempt: Since $X$ is fundamental, it spans the solution space i.e., $$X'(t)=A(t)X(t)$$ but doing the same for $Y$ I go nowhere on finding $C$ invertible. I now that since $X$ is fundamental there is $t_0$ such that $\det(X(t_0))\neq 0$ , even more $\det(X(t))\neq 0$ for all $t$ , but again go nowhere.","If and are fundamental matrices then exists invertible such thta Both for the homogeneous linear system . attempt: Since is fundamental, it spans the solution space i.e., but doing the same for I go nowhere on finding invertible. I now that since is fundamental there is such that , even more for all , but again go nowhere.",X(t) Y(t) C\in M_n Y(t)=X(t)C x'=A(t)x X X'(t)=A(t)X(t) Y C X t_0 \det(X(t_0))\neq 0 \det(X(t))\neq 0 t,"['matrices', 'ordinary-differential-equations', 'fundamental-solution']"
18,Differential equation with one univariate function and several variables,Differential equation with one univariate function and several variables,,"I have a functional equation where I need to find one univariate function which satisfies conditions involving two variables, for example, find a function $f$ which satisfies, for all $x,y$ : $$ x (x+y) f'(x)(f(y)-f(x)) = (x-y)(f(x)-f^2(x)) $$ How are such equations called? I would like to find literature, but I don't even know where to begin, and what terms to use. This is not an ODE, but also not a PDE.","I have a functional equation where I need to find one univariate function which satisfies conditions involving two variables, for example, find a function which satisfies, for all : How are such equations called? I would like to find literature, but I don't even know where to begin, and what terms to use. This is not an ODE, but also not a PDE.","f x,y 
x (x+y) f'(x)(f(y)-f(x)) = (x-y)(f(x)-f^2(x))
","['ordinary-differential-equations', 'partial-differential-equations', 'functional-equations']"
19,$y''+p(t)y'+q(t)y=0$ has finite number of zeros,has finite number of zeros,y''+p(t)y'+q(t)y=0,"Given $y''+p(t)y'+q(t)y=0$ and $p,q$ are continuous on $\mathbb{R}$ and let $y$ be a non-trivial solution of the system. Prove that in any finite interval $[a,b]$ exists at most a finite number of zeros $\{t_k\}_{k=1}^{n}\subset[a,b]$ such that $y(t_k)=0$ for every $k$ I've thought transforming into S-L form by multiply the equation with $e^{\int p(s)}$ and then I get : $(y'e^{\int p(s)})'+e^{\int p(s)}q(t)y=0$ but I don't know how can I say something about the number of zeros from this equation. any hint?",Given and are continuous on and let be a non-trivial solution of the system. Prove that in any finite interval exists at most a finite number of zeros such that for every I've thought transforming into S-L form by multiply the equation with and then I get : but I don't know how can I say something about the number of zeros from this equation. any hint?,"y''+p(t)y'+q(t)y=0 p,q \mathbb{R} y [a,b] \{t_k\}_{k=1}^{n}\subset[a,b] y(t_k)=0 k e^{\int p(s)} (y'e^{\int p(s)})'+e^{\int p(s)}q(t)y=0","['ordinary-differential-equations', 'sturm-liouville']"
20,Solutions of $x'' + 2x' + x = 0$ such that $x(1) = 0$,Solutions of  such that,x'' + 2x' + x = 0 x(1) = 0,"I am trying to solve this question from an older exam that my teacher uploaded. The marks given for answering this question were very low, so I am thinking there is an easy way to solve it. The question asks to find the solutions of $x'' + 2x' + x = 0$ such that $x(1) = 0$ . My approach is using the change $y=x'$ , and therefore getting: $$ x' = y $$ $$ y' = -x -2y $$ Which is a linear system. What I would normally do here is finding the eigenvalues (in this case there is only one eigenvalue, -1 with eigenvector (1,-1)), and solve the corresponding system using the Jordan matrix and the Jordan basis, but I was thinkin whether there was a faster approach, maybe playing with the fact that $x(1)=0$ , or something else. Thanks","I am trying to solve this question from an older exam that my teacher uploaded. The marks given for answering this question were very low, so I am thinking there is an easy way to solve it. The question asks to find the solutions of such that . My approach is using the change , and therefore getting: Which is a linear system. What I would normally do here is finding the eigenvalues (in this case there is only one eigenvalue, -1 with eigenvector (1,-1)), and solve the corresponding system using the Jordan matrix and the Jordan basis, but I was thinkin whether there was a faster approach, maybe playing with the fact that , or something else. Thanks","x'' + 2x' + x = 0 x(1) = 0 y=x' 
x' = y
 
y' = -x -2y
 x(1)=0",['ordinary-differential-equations']
21,What is the necessary condition on $f$ such that the DE $dy/dx=f(y)$ has a solution?,What is the necessary condition on  such that the DE  has a solution?,f dy/dx=f(y),"In my book, it is written that we shall assume throughout the discussion that follows that $f$ and its derivative $f'$ are continuous functions of $y$ on some interval $I$ . I am guessing that this assumption is made in order that the autonomous DE $dy/dx=f(y)$ has a solution. However, I feel like the continuity of $f$ is enough to guarantee that the DE has a solution since this would imply that $dy/dx$ is continuous on some $I$ . What am I missing?","In my book, it is written that we shall assume throughout the discussion that follows that and its derivative are continuous functions of on some interval . I am guessing that this assumption is made in order that the autonomous DE has a solution. However, I feel like the continuity of is enough to guarantee that the DE has a solution since this would imply that is continuous on some . What am I missing?",f f' y I dy/dx=f(y) f dy/dx I,"['ordinary-differential-equations', 'derivatives', 'continuity']"
22,"Evaluate $ \int_0^{2π} e^{\cos (x)}\cos(\sin x) \, \mathrm{d}x $ [duplicate]",Evaluate  [duplicate]," \int_0^{2π} e^{\cos (x)}\cos(\sin x) \, \mathrm{d}x ","This question already has answers here : How to evaluate $\int_{0}^{2\pi}e^{\cos \theta}\cos( \sin \theta) d\theta$? (6 answers) Closed 2 years ago . I have again a doubt regarding an exercise of differentiation under the integral sign. In this case, it concerns the integral: $$ \int_0^{2π} e^{\cos(x)}\cos(\sin x) \, \mathrm{d}x $$ I tried the substitution: $f(a,b) =  \int_0^{2π} e^{acos(x)}\cos(b\sin x) \, \mathrm{d}x  $ So then: $\frac {\partial f} {\partial a} = \int_0^{2π} e^{a\cos(x)}\cos(b\sin x)\cos x \, \mathrm{d}x  $ and: $\frac {\partial f} {\partial b} = -\int_0^{2π} e^{a\cos(x)}\sin(b\sin x)\sin x \, \mathrm{d}x $ Then what i did was try to integrate $\frac{\partial f}{\partial b}$ by parts, making: $dv = -e^{a\cos x}\sin x\, \mathrm{d}x$ so: $v = \frac{1}{a}e^{a\cos x}$ and: $u = \sin(b\sin x)$ so: $du = \cos(b\sin x)b\cos x$ Then we got that: $\frac{\partial f}{\partial b} = - \frac{b}{a} \int_0^{2π} e^{a\cos(x)}\cos(b\sin x)\cos x \, \mathrm{d}x = -\frac{b}{a} \frac{\partial f}{\partial a} $ (Because evaluating $uv$ from $0$ to $2π$ gives us $0$ ) So we get the partial differential equation: $\frac{1}{b} \frac{\partial f}{\partial b} = - \frac{1}{a} \frac{\partial f}{\partial a}$ I solved this by separation of variables, making $f(a,b) = A(a)B(b)$ . The end result was: $f(a,b) = Ce^{\frac{h^2}{2}(a^2 - b^2)}$ Where $C$ is an integration constant and $h$ comes from solving the ODE associated with each $A$ and $B$ . So, my problem begins with this, because I am not sure of my result, this is mostly because of the $h$ , should I pick an specific value? Or my result is just wrong? If this is the case, where did I made the mistake? Because evaluating $f(0,0)$ we get: $f(0,0) = C = 2π$ , and so: $f(1,1) = 2π$ , that is the correct result evaluating in wolfram alfa. I just want to know if there is a way in wich I should get the value.","This question already has answers here : How to evaluate $\int_{0}^{2\pi}e^{\cos \theta}\cos( \sin \theta) d\theta$? (6 answers) Closed 2 years ago . I have again a doubt regarding an exercise of differentiation under the integral sign. In this case, it concerns the integral: I tried the substitution: So then: and: Then what i did was try to integrate by parts, making: so: and: so: Then we got that: (Because evaluating from to gives us ) So we get the partial differential equation: I solved this by separation of variables, making . The end result was: Where is an integration constant and comes from solving the ODE associated with each and . So, my problem begins with this, because I am not sure of my result, this is mostly because of the , should I pick an specific value? Or my result is just wrong? If this is the case, where did I made the mistake? Because evaluating we get: , and so: , that is the correct result evaluating in wolfram alfa. I just want to know if there is a way in wich I should get the value."," \int_0^{2π} e^{\cos(x)}\cos(\sin x) \, \mathrm{d}x  f(a,b) =  \int_0^{2π} e^{acos(x)}\cos(b\sin x) \, \mathrm{d}x   \frac {\partial f} {\partial a} = \int_0^{2π} e^{a\cos(x)}\cos(b\sin x)\cos x \, \mathrm{d}x   \frac {\partial f} {\partial b} = -\int_0^{2π} e^{a\cos(x)}\sin(b\sin x)\sin x \, \mathrm{d}x  \frac{\partial f}{\partial b} dv = -e^{a\cos x}\sin x\, \mathrm{d}x v = \frac{1}{a}e^{a\cos x} u = \sin(b\sin x) du = \cos(b\sin x)b\cos x \frac{\partial f}{\partial b} = - \frac{b}{a} \int_0^{2π} e^{a\cos(x)}\cos(b\sin x)\cos x \, \mathrm{d}x = -\frac{b}{a} \frac{\partial f}{\partial a}  uv 0 2π 0 \frac{1}{b} \frac{\partial f}{\partial b} = - \frac{1}{a} \frac{\partial f}{\partial a} f(a,b) = A(a)B(b) f(a,b) = Ce^{\frac{h^2}{2}(a^2 - b^2)} C h A B h f(0,0) f(0,0) = C = 2π f(1,1) = 2π","['integration', 'ordinary-differential-equations']"
23,Question about Lyapunov stability for a nonlinear system with inputs,Question about Lyapunov stability for a nonlinear system with inputs,,"I am studying Lyapunov stability for systems of the type: $\dot{x}=f(x)$ and I think I have well understood the theory. But, suppose now I have a system with an input, so of the type: $\dot{x}=f(x)+g(x)u$ Can I still use the Lyapunov analysis for analizing the stability of this system?","I am studying Lyapunov stability for systems of the type: and I think I have well understood the theory. But, suppose now I have a system with an input, so of the type: Can I still use the Lyapunov analysis for analizing the stability of this system?",\dot{x}=f(x) \dot{x}=f(x)+g(x)u,"['calculus', 'ordinary-differential-equations', 'control-theory', 'nonlinear-system', 'stability-theory']"
24,"When we write $y'' + y' + y = x$, for a function $y$, do we really mean $y''(x) + y'(x) + y(x) = x$?","When we write , for a function , do we really mean ?",y'' + y' + y = x y y''(x) + y'(x) + y(x) = x,"I have recently started exploring differential equations at a graduate level (partial and ordinary) and really started thinking about why we write them the way we do. Let (since this seems to be the common symbol of choice) $y: \mathbb{R} \to \mathbb{R}$ such that $x \mapsto y(x)$ . Just to be fussy, we will consider $y \in C^{\infty}(\mathbb{R})$ . Now, for example, consider the second order differential equation $$y'' + y' + y = x \tag{i}$$ My question is in regards to this notation: Why do we write the function and it's derivatives this way in the equation rather than writing: $$y''(x) + y'(x) + y(x) = x \tag{ii}$$ using the function values? When we write (i), do we really mean (ii)? This may be a bit pedantic, but I just want to be thorough.","I have recently started exploring differential equations at a graduate level (partial and ordinary) and really started thinking about why we write them the way we do. Let (since this seems to be the common symbol of choice) such that . Just to be fussy, we will consider . Now, for example, consider the second order differential equation My question is in regards to this notation: Why do we write the function and it's derivatives this way in the equation rather than writing: using the function values? When we write (i), do we really mean (ii)? This may be a bit pedantic, but I just want to be thorough.",y: \mathbb{R} \to \mathbb{R} x \mapsto y(x) y \in C^{\infty}(\mathbb{R}) y'' + y' + y = x \tag{i} y''(x) + y'(x) + y(x) = x \tag{ii},"['ordinary-differential-equations', 'partial-differential-equations', 'notation', 'dynamical-systems']"
25,ODE and separations of variables: what about $0$?,ODE and separations of variables: what about ?,0,"Considering the following ODE: $$ y'=f(x)g(y)$$ as long as $g(y) \neq 0$ , I can divide by $g(y)$ , and obtain $$ \frac{y'}{g(y)}=f(x)$$ and by solving $$ \int \frac{dy}{g(y)}= \int f(x)dx$$ I eventually find a solution $y$ on a domain $I$ (for the sake of clarity, let's put $I=(a,b)$ ) such that $g(y(I)) \neq 0$ . But what if, for example, $y(a)=0$ is well defined, $y$ in $a$ is continuous and with a continuous derivative such that $y'(a)=0$ , and $g(y(a))=0$ ? Can it even be possibile? In this case the method above is not working anymore in $a$ , but since our original equation was $y'=f(x)g(y)$ , would $y$ be a solution on $[a,b)$ and not only on $(a,b)$ ? These may sound like trivial questions, but I'm currently studing ODE and their resolution methods and I'm having an hard time grasping all the concepts. Hopefully everything makes sense and it's written in proper English. Thanks everyone!","Considering the following ODE: as long as , I can divide by , and obtain and by solving I eventually find a solution on a domain (for the sake of clarity, let's put ) such that . But what if, for example, is well defined, in is continuous and with a continuous derivative such that , and ? Can it even be possibile? In this case the method above is not working anymore in , but since our original equation was , would be a solution on and not only on ? These may sound like trivial questions, but I'm currently studing ODE and their resolution methods and I'm having an hard time grasping all the concepts. Hopefully everything makes sense and it's written in proper English. Thanks everyone!"," y'=f(x)g(y) g(y) \neq 0 g(y)  \frac{y'}{g(y)}=f(x)  \int \frac{dy}{g(y)}= \int f(x)dx y I I=(a,b) g(y(I)) \neq 0 y(a)=0 y a y'(a)=0 g(y(a))=0 a y'=f(x)g(y) y [a,b) (a,b)","['real-analysis', 'ordinary-differential-equations', 'analysis']"
26,Formula for Green function of Sturm-Liouville problem.,Formula for Green function of Sturm-Liouville problem.,,"I'm trying to remember exact formula for Green function of Sturm-Liouville problem for operator $L\::y\longmapsto (p(t)y')' + q(t)y$ , left condition $D_0$ and right condition $D_1$ (that looks like $a_0y'(0) + b_0y(0)$ and $a_1y'(1) + b_1y(1)$ ), when we have $h_0$ and $h_1$ such that $$Lh_0 = 0, \;\; D_0h_0 = 0$$ $$Lh_1 = 0, \;\; D_1h_1 = 0$$ My notes tell me that Green function in that case is $$ h_0(x)h_1(y) ;\; x < y$$ $$ h_1(x)h_0(y) ;\; x < y$$ multiplied on $$(h_0(x)h_1'(x) - h_1(x)h_0'(x)) \frac{1}{p(x)}$$ I'm in doubt if my notes are correct and would like to ask someone to verify them, because I can't find any information about this formula at all.","I'm trying to remember exact formula for Green function of Sturm-Liouville problem for operator , left condition and right condition (that looks like and ), when we have and such that My notes tell me that Green function in that case is multiplied on I'm in doubt if my notes are correct and would like to ask someone to verify them, because I can't find any information about this formula at all.","L\::y\longmapsto (p(t)y')' + q(t)y D_0 D_1 a_0y'(0) + b_0y(0) a_1y'(1) + b_1y(1) h_0 h_1 Lh_0 = 0, \;\; D_0h_0 = 0 Lh_1 = 0, \;\; D_1h_1 = 0  h_0(x)h_1(y) ;\; x < y  h_1(x)h_0(y) ;\; x < y (h_0(x)h_1'(x) - h_1(x)h_0'(x)) \frac{1}{p(x)}","['ordinary-differential-equations', 'greens-function', 'sturm-liouville']"
27,Showing that all points inside an enclosed region converge to origin,Showing that all points inside an enclosed region converge to origin,,"I have the dynamical system $\dot{x_1}=x_2+x_1^2-\frac{1}{4} x_1(x_2-1+2x_1^2)$ $\dot{x_2}=-2(1+x_2)x_1$ and I have shown that the curves $x_2=1-2x_1^2$ and $x_2=-1$ are invariant. Let $D$ be the region enclosed by the two curves just described. I am trying to make use of the function $H=x_1^2(1+x_2)+\frac{x_2^2}{2}$ to show that trajectories inside this region $D$ converge to the origin as $t\rightarrow-\infty$ . I am not sure what to do here because the only problems of this type that I have seen have to do with convergence as $t\rightarrow\infty$ , and it is suitable to use Lyapunov's stability theorem in such a case. So how can I approach this problem?","I have the dynamical system and I have shown that the curves and are invariant. Let be the region enclosed by the two curves just described. I am trying to make use of the function to show that trajectories inside this region converge to the origin as . I am not sure what to do here because the only problems of this type that I have seen have to do with convergence as , and it is suitable to use Lyapunov's stability theorem in such a case. So how can I approach this problem?",\dot{x_1}=x_2+x_1^2-\frac{1}{4} x_1(x_2-1+2x_1^2) \dot{x_2}=-2(1+x_2)x_1 x_2=1-2x_1^2 x_2=-1 D H=x_1^2(1+x_2)+\frac{x_2^2}{2} D t\rightarrow-\infty t\rightarrow\infty,"['ordinary-differential-equations', 'dynamical-systems', 'stability-in-odes']"
28,Runge Kutta Fehlberg goes crazy when using adaptive h in python,Runge Kutta Fehlberg goes crazy when using adaptive h in python,,"So basically what i'm trying to do here is try to define a function that integrates for runge kutta. The most ""physical"" aspects of it i have already controlled for (it's not a problem of the values of the functions , or the physical parameters (initial conditions, etc), since i have controlled for those and they don't change the result) . The problem i'm solvin is an ODE of second order (harmonic oscilator), x''-μ(1-x^2 )x'+x=0 + CI x(0)=2, x'(0)=0, for t in [0,25], and μ=4 which i have separated into two first-order ODES; μ*(1-x**2)*v-x = v' v = x' and we have inital conditions x0,v0,t0 and a interval on which to integrate t0,tf, and a step size h with which to integrate. ε is my error tolerance. The error I have is that when i activate the adaptivetoggle, so that i should use an adaptive h, x just goes to a value and v goes to 0, instead of oscilating like a harmonic oscilator should. I suspect the problem is only about that bit of code, because when i deactivate the adaptive toggle , everything runs just fine. I am not sure of what is happening, my values should be oscilating, and small, and instead the error just goes to 0 ( it shouldn't, i think ) , v does too and x tends to something instead of oscilating. The code i'm running is: def rk452D(v, f, x0, v0, t0, tf, ε, h, adaptivetoggle):  x = x0; t = t0  valuesx = [] valuesv = [] while t<tf : #We define the runge kutta functions on which to iterate while t is in [t0,tf], and they are of the kind (t,x,v)        f1v = f(t,               x,                                                                                    v0                                                                                       )     f1  = v(t,               x,                                                                                    v0                                                                                       )          f2v = f(t + (1/4)*h,     x + (1/4)*f1,                                                                         v0 + (1/4)*f1v                                                                           )     f2  = v(t + (1/4)*h,     x + (1/4)*f1,                                                                         v0 + (1/4)*f1v                                                                           )          f3v = f(t + (3/8)*h,     x + (3/32)*f1      + (9/32)*f2  ,                                                     v0 + (3/32)*f1v      + (9/32)*f2v                                                        )     f3  = v(t + (3/8)*h,     x + (3/32)*f1      + (9/32)*f2  ,                                                     v0 + (3/32)*f1v      + (9/32)*f2v                                                        )          f4v = f(t + (12/13)*h,   x + (1932/2197)*f1 - (7200/2197)*f2 + (7296/2197)*f3,                                 v0 + (1932/2197)*f1v - (7200/2197)*f2v + (7296/2197)*f3v                                 )     f4  = v(t + (12/13)*h,   x + (1932/2197)*f1 - (7200/2197)*f2 + (7296/2197)*f3,                                 v0 + (1932/2197)*f1v - (7200/2197)*f2v + (7296/2197)*f3v                                 )           f5v = f(t + h,           x + (439/216)*f1   - 8*f2           + (3680/513)*f3  - (845/4104)*f4,                 v0 + (439/216)*f1v   - 8*f2v           + (3680/513)*f3v  - (845/4104)*f4v                )     f5  = v(t + h,           x + (439/216)*f1   - 8*f2           + (3680/513)*f3  - (845/4104)*f4,                 v0 + (439/216)*f1v   - 8*f2v           + (3680/513)*f3v  - (845/4104)*f4v                )      f6v = f(t + h/2,         x - (8/27)*f1      + 2*f2           - (3544/2565)*f3 + (1859/4104)*f4 - (11/40)*f5,   v0 - (8/27)*f1v      + 2*f2v           - (3544/2565)*f3v + (1859/4104)*f4v - (11/40)*f5v )     f6  = v(t + h/2,         x - (8/27)*f1      + 2*f2           - (3544/2565)*f3 + (1859/4104)*f4 - (11/40)*f5,   v0 - (8/27)*f1v      + 2*f2v           - (3544/2565)*f3v + (1859/4104)*f4v - (11/40)*f5v )                              #Now we calculate the positions and velocities, for the fourth order runge kutta aproximation. Commented we have the fifth order approxiation results, which we use to estimate the error ( Error = abs(order5approximation-order4approximation)     x4 = x + h*((25/216)*f1  + (1408/2565)*f3   + (2197/4104)*f4     -(1/5)*f5              )     #x5 = x + h*((16/135)*f1  + (6656/12825)*f3  + (28561/56430)*f4   -(9/50)*f5  +(2/55)*f6 )         v4 = v0 + h*((25/216)*f1v + (1408/2565)*f3v  + (2197/4104)*f4v    -(1/5)*f5v             )     #v5 = v0 + h*((16/135)*f1v + (6656/12825)*f3v + (28561/56430)*f4v  -(9/50)*f5v +(2/55)*f6v)               #If we want to use an adaptive h for our calculations,     if adaptivetoggle == True :        #We calculate error in x using fs, eror in v using fvs, and we take the smaller hnew of the two         Errorx = abs((1/360)*f1  - (128/4275)*f3  - (2197/75240)*f4  + (1/50)*f5  + (2/55)*f6  )        Errorv = abs((1/360)*f1v - (128/4275)*f3v - (2197/75240)*f4v + (1/50)*f5v + (2/55)*f6v )                  hnewx  = 0.9*h*((ε)/Errorx)**(0.25)        hnewv  = 0.9*h*((ε)/Errorv)**(0.25)                hnew = min(hnewx,hnewv)                print(hnew)        #if hnewx < hnewv:         #   hnew = hnewx        #if hnewx > hnewv:          #  hnew = hnewv         #After calculating hnew, we compare it to the integration step h we have used, to decide if we can keep our calculation or we have to repeat it using hnew.         if hnew >= h :           #we increment the loop,and take hnew for the next loop           t += h           h = hnew           x = x4           v0 = v4           valuesx.append(x4)           valuesv.append(v4)               elif hnew < h:            h  = hnew #we don't increment t , the loop variable, when we repeat the integration step using the new h integration step      else :#if we don't want an adaptive h ( this works just fine)        valuesx.append(x4)        valuesv.append(v4)        x = x4        v0 = v4        t+=h #increment the loop return valuesx, valuesv #Then we implement the function #We define the two functions ( of the ODEs) to feed the RK solver def f(t,x,v):     return μ*(1-x**2)*v-x def v(t,x,v):     return v  #we feed it the parameters μ  = 4; x0 = 2;  v0 = 0; t0 = 0; tf = 25; h  = 1; ε  = 10**-3  adaptivetoggle = True  solution = rk452D(v, f, x0, v0, t0, tf, ε, h, adaptivetoggle) print(solution)","So basically what i'm trying to do here is try to define a function that integrates for runge kutta. The most ""physical"" aspects of it i have already controlled for (it's not a problem of the values of the functions , or the physical parameters (initial conditions, etc), since i have controlled for those and they don't change the result) . The problem i'm solvin is an ODE of second order (harmonic oscilator), x''-μ(1-x^2 )x'+x=0 + CI x(0)=2, x'(0)=0, for t in [0,25], and μ=4 which i have separated into two first-order ODES; μ*(1-x**2)*v-x = v' v = x' and we have inital conditions x0,v0,t0 and a interval on which to integrate t0,tf, and a step size h with which to integrate. ε is my error tolerance. The error I have is that when i activate the adaptivetoggle, so that i should use an adaptive h, x just goes to a value and v goes to 0, instead of oscilating like a harmonic oscilator should. I suspect the problem is only about that bit of code, because when i deactivate the adaptive toggle , everything runs just fine. I am not sure of what is happening, my values should be oscilating, and small, and instead the error just goes to 0 ( it shouldn't, i think ) , v does too and x tends to something instead of oscilating. The code i'm running is: def rk452D(v, f, x0, v0, t0, tf, ε, h, adaptivetoggle):  x = x0; t = t0  valuesx = [] valuesv = [] while t<tf : #We define the runge kutta functions on which to iterate while t is in [t0,tf], and they are of the kind (t,x,v)        f1v = f(t,               x,                                                                                    v0                                                                                       )     f1  = v(t,               x,                                                                                    v0                                                                                       )          f2v = f(t + (1/4)*h,     x + (1/4)*f1,                                                                         v0 + (1/4)*f1v                                                                           )     f2  = v(t + (1/4)*h,     x + (1/4)*f1,                                                                         v0 + (1/4)*f1v                                                                           )          f3v = f(t + (3/8)*h,     x + (3/32)*f1      + (9/32)*f2  ,                                                     v0 + (3/32)*f1v      + (9/32)*f2v                                                        )     f3  = v(t + (3/8)*h,     x + (3/32)*f1      + (9/32)*f2  ,                                                     v0 + (3/32)*f1v      + (9/32)*f2v                                                        )          f4v = f(t + (12/13)*h,   x + (1932/2197)*f1 - (7200/2197)*f2 + (7296/2197)*f3,                                 v0 + (1932/2197)*f1v - (7200/2197)*f2v + (7296/2197)*f3v                                 )     f4  = v(t + (12/13)*h,   x + (1932/2197)*f1 - (7200/2197)*f2 + (7296/2197)*f3,                                 v0 + (1932/2197)*f1v - (7200/2197)*f2v + (7296/2197)*f3v                                 )           f5v = f(t + h,           x + (439/216)*f1   - 8*f2           + (3680/513)*f3  - (845/4104)*f4,                 v0 + (439/216)*f1v   - 8*f2v           + (3680/513)*f3v  - (845/4104)*f4v                )     f5  = v(t + h,           x + (439/216)*f1   - 8*f2           + (3680/513)*f3  - (845/4104)*f4,                 v0 + (439/216)*f1v   - 8*f2v           + (3680/513)*f3v  - (845/4104)*f4v                )      f6v = f(t + h/2,         x - (8/27)*f1      + 2*f2           - (3544/2565)*f3 + (1859/4104)*f4 - (11/40)*f5,   v0 - (8/27)*f1v      + 2*f2v           - (3544/2565)*f3v + (1859/4104)*f4v - (11/40)*f5v )     f6  = v(t + h/2,         x - (8/27)*f1      + 2*f2           - (3544/2565)*f3 + (1859/4104)*f4 - (11/40)*f5,   v0 - (8/27)*f1v      + 2*f2v           - (3544/2565)*f3v + (1859/4104)*f4v - (11/40)*f5v )                              #Now we calculate the positions and velocities, for the fourth order runge kutta aproximation. Commented we have the fifth order approxiation results, which we use to estimate the error ( Error = abs(order5approximation-order4approximation)     x4 = x + h*((25/216)*f1  + (1408/2565)*f3   + (2197/4104)*f4     -(1/5)*f5              )     #x5 = x + h*((16/135)*f1  + (6656/12825)*f3  + (28561/56430)*f4   -(9/50)*f5  +(2/55)*f6 )         v4 = v0 + h*((25/216)*f1v + (1408/2565)*f3v  + (2197/4104)*f4v    -(1/5)*f5v             )     #v5 = v0 + h*((16/135)*f1v + (6656/12825)*f3v + (28561/56430)*f4v  -(9/50)*f5v +(2/55)*f6v)               #If we want to use an adaptive h for our calculations,     if adaptivetoggle == True :        #We calculate error in x using fs, eror in v using fvs, and we take the smaller hnew of the two         Errorx = abs((1/360)*f1  - (128/4275)*f3  - (2197/75240)*f4  + (1/50)*f5  + (2/55)*f6  )        Errorv = abs((1/360)*f1v - (128/4275)*f3v - (2197/75240)*f4v + (1/50)*f5v + (2/55)*f6v )                  hnewx  = 0.9*h*((ε)/Errorx)**(0.25)        hnewv  = 0.9*h*((ε)/Errorv)**(0.25)                hnew = min(hnewx,hnewv)                print(hnew)        #if hnewx < hnewv:         #   hnew = hnewx        #if hnewx > hnewv:          #  hnew = hnewv         #After calculating hnew, we compare it to the integration step h we have used, to decide if we can keep our calculation or we have to repeat it using hnew.         if hnew >= h :           #we increment the loop,and take hnew for the next loop           t += h           h = hnew           x = x4           v0 = v4           valuesx.append(x4)           valuesv.append(v4)               elif hnew < h:            h  = hnew #we don't increment t , the loop variable, when we repeat the integration step using the new h integration step      else :#if we don't want an adaptive h ( this works just fine)        valuesx.append(x4)        valuesv.append(v4)        x = x4        v0 = v4        t+=h #increment the loop return valuesx, valuesv #Then we implement the function #We define the two functions ( of the ODEs) to feed the RK solver def f(t,x,v):     return μ*(1-x**2)*v-x def v(t,x,v):     return v  #we feed it the parameters μ  = 4; x0 = 2;  v0 = 0; t0 = 0; tf = 25; h  = 1; ε  = 10**-3  adaptivetoggle = True  solution = rk452D(v, f, x0, v0, t0, tf, ε, h, adaptivetoggle) print(solution)",,"['ordinary-differential-equations', 'numerical-methods', 'recursion', 'python', 'runge-kutta-methods']"
29,Stability of 2nd order linear vector differential equations?,Stability of 2nd order linear vector differential equations?,,Given the system $\ddot{x} +B \dot{x}+ C x =0$ where $x \in R^{2}$ and $B$ is positive definite symmetric matrix and I know that the eigenvalues of $C$ are either $\lambda_1<0<\lambda_2$ or $\lambda=\pm \omega j$ . My question is: Can I conclude from these givens that the system is unstable and can the type the equilibrium point be determined?,Given the system where and is positive definite symmetric matrix and I know that the eigenvalues of are either or . My question is: Can I conclude from these givens that the system is unstable and can the type the equilibrium point be determined?,\ddot{x} +B \dot{x}+ C x =0 x \in R^{2} B C \lambda_1<0<\lambda_2 \lambda=\pm \omega j,"['linear-algebra', 'matrices', 'ordinary-differential-equations', 'control-theory']"
30,Struggling to find solution: question 2,Struggling to find solution: question 2,,"$$xy'-2y=\frac{x^6}{(y+x^2)}$$ Can someone please help with this one? It looks like a bernoulli equation, I have tried to multiply both sides by denominator and gather them in parantheses to leave the $y'$ alone but to no avail.","Can someone please help with this one? It looks like a bernoulli equation, I have tried to multiply both sides by denominator and gather them in parantheses to leave the alone but to no avail.",xy'-2y=\frac{x^6}{(y+x^2)} y',['ordinary-differential-equations']
31,Show that orbitally equivalent autonomous systems do have same orbits,Show that orbitally equivalent autonomous systems do have same orbits,,"According to Wikipedia , Two dynamical systems on the same state space, defined by $\dot {x}=f(x)$ and $\dot {x}=g(x)$ , are said to be orbitally equivalent if there is a positive function, $\mu :X\to \mathbb {R}$ , such that $g(x)=\mu (x)f(x)$ . Orbitally equivalent system differ only in the time parametrization. I wonder why do orbitally equivalent have same orbits. Intuitively, the two systems would have the same ""direction of velocity"" at any given point, but this is not a mathematical proof, right? I can not found any relating information except this question where lhf implicitly uses this property.","According to Wikipedia , Two dynamical systems on the same state space, defined by and , are said to be orbitally equivalent if there is a positive function, , such that . Orbitally equivalent system differ only in the time parametrization. I wonder why do orbitally equivalent have same orbits. Intuitively, the two systems would have the same ""direction of velocity"" at any given point, but this is not a mathematical proof, right? I can not found any relating information except this question where lhf implicitly uses this property.",\dot {x}=f(x) \dot {x}=g(x) \mu :X\to \mathbb {R} g(x)=\mu (x)f(x),"['ordinary-differential-equations', 'dynamical-systems', 'equivalence-relations']"
32,Why do we say dN/dt = kN?,Why do we say dN/dt = kN?,,"So, I just started doing differential equations and I'm trying to understand the basic idea of dN/dt = kN. I understand the whole process of solving a differential equation (the separation of variables, isolating N, finding k etc...) and I'm getting proficient at it. However, the only thing I'm still having difficulty grappling with is why we say that dN/dt = kN to begin with. The examples used in my textbook: Radioactive decay . After 3 days, 50 percent of the radioactivity produced by a nuclear explosion has disappeared. How long does it take for 99 percent of this radioactivity to disappear? The rate of change of the mass of our substance is negative, and is proportional at each moment to the mass of the substance at that moment. This statement means that if x = x(t) is the mass of the radioactive substance at time t, then dx/dt = -kx  (k > 0). Population growth . Consider a laboratory culture of bacteria with unlimited food and no enemies. If N = N(t) denotes the number of bacteria present at time t, it is natural to assume that the rate of change of N is proportional to N itself, or dN/dt = kN  (k > 0). If the number of bacteria present at the beginning is N_0, and this number doubles after 2 hours (the “doubling time”), how many are there after 6 hours? After t hours? There's always this idea present in these problems that the rate of change of _____ is proportional at each moment in time to _______ at that moment, or dN/dt = kN. Both mathematically and intuitively, why is this relationship ""natural to assume""? What (if any) mathematical property dictates this? P.S. It might also be worth noting that I don't know anything about population growth outside the context of the ODEs I've been solving.","So, I just started doing differential equations and I'm trying to understand the basic idea of dN/dt = kN. I understand the whole process of solving a differential equation (the separation of variables, isolating N, finding k etc...) and I'm getting proficient at it. However, the only thing I'm still having difficulty grappling with is why we say that dN/dt = kN to begin with. The examples used in my textbook: Radioactive decay . After 3 days, 50 percent of the radioactivity produced by a nuclear explosion has disappeared. How long does it take for 99 percent of this radioactivity to disappear? The rate of change of the mass of our substance is negative, and is proportional at each moment to the mass of the substance at that moment. This statement means that if x = x(t) is the mass of the radioactive substance at time t, then dx/dt = -kx  (k > 0). Population growth . Consider a laboratory culture of bacteria with unlimited food and no enemies. If N = N(t) denotes the number of bacteria present at time t, it is natural to assume that the rate of change of N is proportional to N itself, or dN/dt = kN  (k > 0). If the number of bacteria present at the beginning is N_0, and this number doubles after 2 hours (the “doubling time”), how many are there after 6 hours? After t hours? There's always this idea present in these problems that the rate of change of _____ is proportional at each moment in time to _______ at that moment, or dN/dt = kN. Both mathematically and intuitively, why is this relationship ""natural to assume""? What (if any) mathematical property dictates this? P.S. It might also be worth noting that I don't know anything about population growth outside the context of the ODEs I've been solving.",,"['calculus', 'ordinary-differential-equations']"
33,A variation of a 'leaky bucket' problem,A variation of a 'leaky bucket' problem,,"I'm looking at a 'leaky bucket' problem with a few differences, the system is set up as follows: the tank has a practically unlimited capacity water is added with a known flow rate periodically the drain valve is adjusted so that all of the water is drained over say 10 seconds we are interested to know how much water is contained at time $t$ To give a concrete example, lets assume the water is added with a rate of 10L per second, every 4 seconds the drain is adjusted. When the drain is adjusted it is adjusted to $\frac{1}{10}$ of current volume of water. Initially I started with the following diff. eq. $dV=I-\frac{1}{10}V$ where $V$ stands for volume and $I$ is the incoming flow rate in L per sec. The issue is that in this equation the drain is constant with respect to volume, whereas in the actual problem the drain is adjusted periodically and in between those events the water is draining with a set flow rate. I have a feeling it involves using a $\sin$ function to model the period, but I'm not certain as to how to go about it.","I'm looking at a 'leaky bucket' problem with a few differences, the system is set up as follows: the tank has a practically unlimited capacity water is added with a known flow rate periodically the drain valve is adjusted so that all of the water is drained over say 10 seconds we are interested to know how much water is contained at time To give a concrete example, lets assume the water is added with a rate of 10L per second, every 4 seconds the drain is adjusted. When the drain is adjusted it is adjusted to of current volume of water. Initially I started with the following diff. eq. where stands for volume and is the incoming flow rate in L per sec. The issue is that in this equation the drain is constant with respect to volume, whereas in the actual problem the drain is adjusted periodically and in between those events the water is draining with a set flow rate. I have a feeling it involves using a function to model the period, but I'm not certain as to how to go about it.",t \frac{1}{10} dV=I-\frac{1}{10}V V I \sin,['ordinary-differential-equations']
34,Prove Laplace's equation is rotation invariant,Prove Laplace's equation is rotation invariant,,"Prove that Laplace's equation $\Delta u = 0$ is rotation invariant; that is, if $O$ is an orthogonal $n\times n$ matrix and we define $$       v(x) := u(Ox) \quad (x \in \mathbb{R}^n)$$ then $\Delta v = 0$ . I wanted to see if what I had below was correct and complete. Any feedback on rigor is greatly appreciated. In cartesian coordinates, we have $\Delta u = \sum_{i = 1}^n \frac{\partial^2 u}{\partial x_i^2}$ . Letting the $i$ th row and $j$ th column of the matrix $O$ be $a_{ij}$ we have \begin{equation}  \frac{\partial v}{\partial x_j} = \sum_{i =1}^n\frac{\partial u}{\partial x_i}a_{ij} \end{equation} Thus, \begin{equation}  \frac{\partial^2 v}{\partial x_j^2} = \sum_{i =1}^n\sum_{k = 1}^n\frac{\partial^2 u}{\partial x_i\partial x_k}a_{ij}a_{kj} \end{equation} Therefore: \begin{align} \Delta v &= \sum_{j = 1}^n \frac{\partial^2 v}{\partial x_j^2}\\     &= \sum_{j = 1}^n \left(\sum_{i =1}^n\sum_{k = 1}^n\frac{\partial^2 u}{\partial x_i\partial x_k}a_{ij}a_{kj}\right)\\     &= \sum_{i =1}^n\sum_{k = 1}^n\left(\sum_{j = 1}^n \frac{\partial^2 u}{\partial x_i\partial x_k}a_{ij}a_{kj}\right)\\     &= \sum_{i =1}^n\sum_{k = 1}^n \frac{\partial^2 u}{\partial x_i\partial x_k}\left(\sum_{j = 1}^na_{ij}a_{kj}\right)\\     &=\sum_{i =1}^n\sum_{k = 1}^n \frac{\partial^2 u}{\partial x_i\partial x_k}\delta_{i, k}\\     &= \sum_{i =1}^n\frac{\partial^2 u}{\partial x_i^2} = 0 \end{align}","Prove that Laplace's equation is rotation invariant; that is, if is an orthogonal matrix and we define then . I wanted to see if what I had below was correct and complete. Any feedback on rigor is greatly appreciated. In cartesian coordinates, we have . Letting the th row and th column of the matrix be we have Thus, Therefore:","\Delta u = 0 O n\times n        v(x) := u(Ox) \quad (x \in \mathbb{R}^n) \Delta v = 0 \Delta u = \sum_{i = 1}^n \frac{\partial^2 u}{\partial x_i^2} i j O a_{ij} \begin{equation}
 \frac{\partial v}{\partial x_j} = \sum_{i =1}^n\frac{\partial u}{\partial x_i}a_{ij}
\end{equation} \begin{equation}
 \frac{\partial^2 v}{\partial x_j^2} = \sum_{i =1}^n\sum_{k = 1}^n\frac{\partial^2 u}{\partial x_i\partial x_k}a_{ij}a_{kj}
\end{equation} \begin{align}
\Delta v &= \sum_{j = 1}^n \frac{\partial^2 v}{\partial x_j^2}\\
    &= \sum_{j = 1}^n \left(\sum_{i =1}^n\sum_{k = 1}^n\frac{\partial^2 u}{\partial x_i\partial x_k}a_{ij}a_{kj}\right)\\
    &= \sum_{i =1}^n\sum_{k = 1}^n\left(\sum_{j = 1}^n \frac{\partial^2 u}{\partial x_i\partial x_k}a_{ij}a_{kj}\right)\\
    &= \sum_{i =1}^n\sum_{k = 1}^n \frac{\partial^2 u}{\partial x_i\partial x_k}\left(\sum_{j = 1}^na_{ij}a_{kj}\right)\\
    &=\sum_{i =1}^n\sum_{k = 1}^n \frac{\partial^2 u}{\partial x_i\partial x_k}\delta_{i, k}\\
    &= \sum_{i =1}^n\frac{\partial^2 u}{\partial x_i^2} = 0
\end{align}","['ordinary-differential-equations', 'partial-differential-equations']"
35,Why are the eigenfunctions of my Hermitian operator not orthogonal?,Why are the eigenfunctions of my Hermitian operator not orthogonal?,,"I am finding that the eigenfunctions of my Hermitian differential operator are not orthogonal and I do not know why. Consider the differential operator $$ \mathcal{L} = x^2 \frac{d^2}{dx^2} + 2x \frac{d}{dx} + c $$ where $c$ is a constant. This is an Hermitian operator with respect to the inner product $$ \langle \psi , \phi \rangle =\int_{- \infty}^\infty dx\psi^*\phi$$ We have $$ \langle \psi , \mathcal{L} \phi \rangle =\int_{- \infty}^\infty dx\psi^*\left( x^2 \frac{d^2 \phi}{dx^2} + 2x \frac{d \phi}{dx} + c \phi\right) \\  = \int_{- \infty}^\infty dx \left(\frac{d^2}{dx^2} \left( x^2 \psi^* \right) \phi - \frac{d}{dx}\left( 2x \psi^* \right) \phi + c \psi^* \phi \right) \\ = \int_{- \infty}^\infty dx\left( 2\psi^* + 4x \frac{d \psi^*}{dx} + x^2 \frac{d^2 \psi^*}{dx^2} - 2\psi^* - 2x \frac{d \psi^*}{dx} + c\psi^* \phi\right) \\  = \langle \mathcal{L} \psi , \phi \rangle $$ where I have assumed my solutions vanish at $\pm \infty$ so the boundary terms vanish when I integrate by parts. So my operator is Hermitian and I expect my eigenfunctions to be orthogonal. Consdider the eigenvalue equation $\mathcal{L} \psi = \lambda \psi$ , this yields the differential equation $$  \quad x^2 \psi''(x) + 2x \psi'(x) + (c - \lambda)\psi = 0$$ The eigenvalue equation is therefore an Euler differential equation. If we take a trial solution $\psi(x) =  x^n$ , then substituting this in yields the quadratic equation $$ n^2 + n + (c- \lambda) = 0 \quad \Rightarrow \quad n=-\frac{1}{2} \pm \frac{1}{2} \sqrt{1- 4(c-\lambda)}$$ Suppose we took the special case where the eigenvalues are negative and of the form $\lambda = -E^2$ , for some $E$ , and let $ c = \frac{1}{4}$ , then we have $ n = -\frac{1}{2} \pm i E$ and the solutions will be given by $$ \psi_\pm(x) = \frac{1}{\sqrt{x}} x^{\pm i E}$$ My problem is that these solutions do not appear to be orthogonal for different eigenvalues. If we take the solutions whose eigenvalues are $\lambda $ and $\lambda'$ , then the inner product would be $$ \langle \psi , \psi' \rangle = \int_{-\infty}^\infty dx \frac{1}{x} x^{\pm i (E'-E)} $$ which according to Wolfram is divergent. I am not sure why my solutions for different eigenvalues are not orthogonal. Any hints would be greatly appreciated.","I am finding that the eigenfunctions of my Hermitian differential operator are not orthogonal and I do not know why. Consider the differential operator where is a constant. This is an Hermitian operator with respect to the inner product We have where I have assumed my solutions vanish at so the boundary terms vanish when I integrate by parts. So my operator is Hermitian and I expect my eigenfunctions to be orthogonal. Consdider the eigenvalue equation , this yields the differential equation The eigenvalue equation is therefore an Euler differential equation. If we take a trial solution , then substituting this in yields the quadratic equation Suppose we took the special case where the eigenvalues are negative and of the form , for some , and let , then we have and the solutions will be given by My problem is that these solutions do not appear to be orthogonal for different eigenvalues. If we take the solutions whose eigenvalues are and , then the inner product would be which according to Wolfram is divergent. I am not sure why my solutions for different eigenvalues are not orthogonal. Any hints would be greatly appreciated."," \mathcal{L} = x^2 \frac{d^2}{dx^2} + 2x \frac{d}{dx} + c  c  \langle \psi , \phi \rangle =\int_{- \infty}^\infty dx\psi^*\phi  \langle \psi , \mathcal{L} \phi \rangle =\int_{- \infty}^\infty dx\psi^*\left( x^2 \frac{d^2 \phi}{dx^2} + 2x \frac{d \phi}{dx} + c \phi\right) \\ 
= \int_{- \infty}^\infty dx \left(\frac{d^2}{dx^2} \left( x^2 \psi^* \right) \phi - \frac{d}{dx}\left( 2x \psi^* \right) \phi + c \psi^* \phi \right) \\
= \int_{- \infty}^\infty dx\left( 2\psi^* + 4x \frac{d \psi^*}{dx} + x^2 \frac{d^2 \psi^*}{dx^2} - 2\psi^* - 2x \frac{d \psi^*}{dx} + c\psi^* \phi\right) \\
 = \langle \mathcal{L} \psi , \phi \rangle  \pm \infty \mathcal{L} \psi = \lambda \psi   \quad x^2 \psi''(x) + 2x \psi'(x) + (c - \lambda)\psi = 0 \psi(x) =  x^n  n^2 + n + (c- \lambda) = 0 \quad \Rightarrow \quad n=-\frac{1}{2} \pm \frac{1}{2} \sqrt{1- 4(c-\lambda)} \lambda = -E^2 E  c = \frac{1}{4}  n = -\frac{1}{2} \pm i E  \psi_\pm(x) = \frac{1}{\sqrt{x}} x^{\pm i E} \lambda  \lambda'  \langle \psi , \psi' \rangle = \int_{-\infty}^\infty dx \frac{1}{x} x^{\pm i (E'-E)} ","['functional-analysis', 'ordinary-differential-equations', 'inner-products', 'adjoint-operators']"
36,"Given that $a<0$, how to compute the limit of $\int_{0}^t e^{a(t-s)}f(s)ds$ when $t\rightarrow{+\infty}$?","Given that , how to compute the limit of  when ?",a<0 \int_{0}^t e^{a(t-s)}f(s)ds t\rightarrow{+\infty},"Given that $a<0$ , how to prove $I(t):=\int_{0}^t e^{a(t-s)}f(s)ds$ converges to $0$ when $t$ goes to positive infinity? This problem emerged when I tried to determine how the solution(s) of the following ODE converges when $f(t)\rightarrow 0(t\rightarrow +\infty)$ : $$a_0\frac{d^2x}{dt^2}+a_1\frac{dx}{dt}+a_2x=f(t).$$ From the theorems I've known about ODE, the solutions are of the form $$ x=e^{at}\left( c_1\cos b t+c_2\sin b t \right) +\int_0^t{K\left( t-s \right) f\left( s \right) ds} $$ when the solution of $a_0\lambda^2+a_1\lambda+a_2=0$ are conjugate imaginary, i.e. , $\lambda_1=a+bi,\lambda_2=a-bi$ , where $K(t)$ denotes the kernel function $$ K\left( t \right) =\frac{e^{\lambda _1t}-e^{\lambda _2t}}{\lambda _1-\lambda _2}=\frac{e^{at}\sin bt}{b}. $$ The first part of solution $e^{at}\left( c_1\cos b t+c_2\sin b t \right)$ obviously converges to $0$ because $\left( c_1\cos b t+c_2\sin b t \right)$ is bounded and $e^{at}$ converges to $0$ when $a<0$ . For the second part $ \int_0^t{}\frac{e^{a\left( t-s \right)}\sin b\left( t-s \right)}{b}f(s)ds$ , $\frac{\sin b(t-s)}{b}$ is bounded, so the uncertain part is $$\int_{0}^t e^{a(t-s)}f(s)ds\xrightarrow{?}0.$$ This is tricky for me because it contains 2 variables and I need to regard $t$ as a parameter primarily, which is beyond my knowledge and capability. Any help would be appreciated! Thank you in advance!","Given that , how to prove converges to when goes to positive infinity? This problem emerged when I tried to determine how the solution(s) of the following ODE converges when : From the theorems I've known about ODE, the solutions are of the form when the solution of are conjugate imaginary, i.e. , , where denotes the kernel function The first part of solution obviously converges to because is bounded and converges to when . For the second part , is bounded, so the uncertain part is This is tricky for me because it contains 2 variables and I need to regard as a parameter primarily, which is beyond my knowledge and capability. Any help would be appreciated! Thank you in advance!","a<0 I(t):=\int_{0}^t e^{a(t-s)}f(s)ds 0 t f(t)\rightarrow 0(t\rightarrow +\infty) a_0\frac{d^2x}{dt^2}+a_1\frac{dx}{dt}+a_2x=f(t). 
x=e^{at}\left( c_1\cos b t+c_2\sin b t \right) +\int_0^t{K\left( t-s \right) f\left( s \right) ds}
 a_0\lambda^2+a_1\lambda+a_2=0 \lambda_1=a+bi,\lambda_2=a-bi K(t) 
K\left( t \right) =\frac{e^{\lambda _1t}-e^{\lambda _2t}}{\lambda _1-\lambda _2}=\frac{e^{at}\sin bt}{b}.
 e^{at}\left( c_1\cos b t+c_2\sin b t \right) 0 \left( c_1\cos b t+c_2\sin b t \right) e^{at} 0 a<0 
\int_0^t{}\frac{e^{a\left( t-s \right)}\sin b\left( t-s \right)}{b}f(s)ds \frac{\sin b(t-s)}{b} \int_{0}^t e^{a(t-s)}f(s)ds\xrightarrow{?}0. t","['integration', 'ordinary-differential-equations', 'multivariable-calculus']"
37,Laplace equation with initial conditions,Laplace equation with initial conditions,,"So I have Laplace equation: $$ u_{xx}+u_{yy}= 0 $$ and initial conditions $$ u(0,y)=0, \;\: u_x(0,y)=y  $$ And I have to solve it. My solution: If we assume that the solution is of the form: $$ u(x,y)=X(x)Y(y) $$ Then follows: $$ u(0,y)=X(0)Y(y)=0; \:\; u_x(0,y)=X'(0)Y(y)=y.$$ Furthermore: $$ u_{xx}(x,y)=X''(x)Y(y); \:\; u_{yy}(x,y)=X(x)Y''(y) $$ From that, we get: $$ \begin{aligned}&\;u_{xx}+u_{yy}=X''(x)Y(y)+X(x)Y''(y)=0 \\ \\\Leftrightarrow& \; X''(x)Y(y)=-X(x)Y''(y)  \\\\  \Leftrightarrow& \; -\frac{X(x)}{X''(x)}=\frac{Y(y)}{Y''(y)}  \end{aligned}$$ My problem: Every time I try to go further I get: $X(0)=0$ , which then implies, that $X'(0)=0$ , which furthermore is in contradiction with $X'(0)Y(y)=y$ This is a part where I didn't know where to go further, everything I try to insert doesn't end well and I am not sure, whether my approach was good at all. I would appreciate any kind of help!","So I have Laplace equation: and initial conditions And I have to solve it. My solution: If we assume that the solution is of the form: Then follows: Furthermore: From that, we get: My problem: Every time I try to go further I get: , which then implies, that , which furthermore is in contradiction with This is a part where I didn't know where to go further, everything I try to insert doesn't end well and I am not sure, whether my approach was good at all. I would appreciate any kind of help!"," u_{xx}+u_{yy}= 0   u(0,y)=0, \;\: u_x(0,y)=y    u(x,y)=X(x)Y(y)   u(0,y)=X(0)Y(y)=0; \:\; u_x(0,y)=X'(0)Y(y)=y.  u_{xx}(x,y)=X''(x)Y(y); \:\; u_{yy}(x,y)=X(x)Y''(y)   \begin{aligned}&\;u_{xx}+u_{yy}=X''(x)Y(y)+X(x)Y''(y)=0 \\ \\\Leftrightarrow& \; X''(x)Y(y)=-X(x)Y''(y)  \\\\
 \Leftrightarrow& \; -\frac{X(x)}{X''(x)}=\frac{Y(y)}{Y''(y)}  \end{aligned} X(0)=0 X'(0)=0 X'(0)Y(y)=y","['ordinary-differential-equations', 'partial-differential-equations', 'harmonic-functions', 'elliptic-equations']"
38,Shooting method on non-linear third ordered differential equation with boundary conditions.,Shooting method on non-linear third ordered differential equation with boundary conditions.,,"I know that how to apply the shooting method on first and second ordered non linear ordinary differential equations. I want to apply this method on third ordered case. I need the standard references for this problem. For example $f^{'''}+ff^{''}-f^{'2}=0$ with boundary condition $f(0)=0,\, f^{'}(0)=1,\,f^{'}(\infty)=0.$",I know that how to apply the shooting method on first and second ordered non linear ordinary differential equations. I want to apply this method on third ordered case. I need the standard references for this problem. For example with boundary condition,"f^{'''}+ff^{''}-f^{'2}=0 f(0)=0,\, f^{'}(0)=1,\,f^{'}(\infty)=0.","['ordinary-differential-equations', 'numerical-methods']"
39,Solve $y'''-6y''+11y'-3y=2\frac{e^{3x}}{e^{x}+1}$ [closed],Solve  [closed],y'''-6y''+11y'-3y=2\frac{e^{3x}}{e^{x}+1},"Closed. This question does not meet Mathematics Stack Exchange guidelines . It is not currently accepting answers. Please provide additional context , which ideally explains why the question is relevant to you and our  community. Some forms of context include: background and motivation, relevant definitions, source, possible strategies, your current progress, why the question is interesting or important, etc. Closed 3 years ago . Improve this question Solve the following differential equation $$y'''-6y''+11y'-3y=2\frac{e^{3x}}{e^{x}+1}~.$$ This question was asked during a test at Toronto University. I tried to find a homogeneous solution and I got very bizarre numbers, which seems to hint this is not the way to solve this ode. I gave to WA as it is to try to solve it, it surrendered (at least without pro) Does anyone have any suggestions?","Closed. This question does not meet Mathematics Stack Exchange guidelines . It is not currently accepting answers. Please provide additional context , which ideally explains why the question is relevant to you and our  community. Some forms of context include: background and motivation, relevant definitions, source, possible strategies, your current progress, why the question is interesting or important, etc. Closed 3 years ago . Improve this question Solve the following differential equation This question was asked during a test at Toronto University. I tried to find a homogeneous solution and I got very bizarre numbers, which seems to hint this is not the way to solve this ode. I gave to WA as it is to try to solve it, it surrendered (at least without pro) Does anyone have any suggestions?",y'''-6y''+11y'-3y=2\frac{e^{3x}}{e^{x}+1}~.,[]
40,Solve $y’ = 1 + y^2$,Solve,y’ = 1 + y^2,"Let: $$y’ = 1 + y^2 \\ y(0) = 0 $$ We need to find a maximal interval $(a,b)$ so that the problem with the initial conditions, has a solution. Therefore, I think I need to find a solution, there I can look at the validity interval. Yet, I don’t have even a direction how to solve this equation. It’s a non linear equation. Not separate, can’t use integration constant, not Bernoulli form, don’t see how substitution can help me... If I could guess I would say it something of the direction: $$\text{d}y = (1+y^2) \text{d}x $$ But $y$ Is a function of $x$ So I can't just take the integral of both sides... Can someone give me a hint? Thanks.","Let: We need to find a maximal interval so that the problem with the initial conditions, has a solution. Therefore, I think I need to find a solution, there I can look at the validity interval. Yet, I don’t have even a direction how to solve this equation. It’s a non linear equation. Not separate, can’t use integration constant, not Bernoulli form, don’t see how substitution can help me... If I could guess I would say it something of the direction: But Is a function of So I can't just take the integral of both sides... Can someone give me a hint? Thanks.","y’ = 1 + y^2 \\ y(0) = 0  (a,b) \text{d}y = (1+y^2) \text{d}x  y x","['calculus', 'ordinary-differential-equations']"
41,Discuss existence and uniqueness for a Cauchy problem,Discuss existence and uniqueness for a Cauchy problem,,"I don't know what's happening with this exercise. I need a help becuase I'm quite puzzled. Consider the Cauchy problem \begin{cases} y'=\frac{2}{t} y + 2 t \sqrt{y} \\ y(1)=0 \end{cases} Study the existence and uniqueness Here $$f(t,y)=\frac{2}{t} y + 2 t \sqrt{y}$$ Since $y\geq0$ (I have the square root), I consider as open neigbourhood $K = \{t: |t-1|< r_1 \} \times \{y: 0 < y < r_2 \}$ , but in this way I am in trouble with $$f_y(t,y)= \frac{2}{t} + \frac{t}{\sqrt{y}}$$ because it's discontinuous at $y=0$ . So I should look for a weaker condition as Lipschitz continuity: I take $(t,y_1)$ and $(t,y_2)$ in $K$ : $$|\frac{2}{t} \bigl(y_1 - y_2 \bigr) + 2t \bigl( \sqrt{y_1} - \sqrt{y_2} \bigr)|  \leq |\frac{2}{t} \bigl(y_1 - y_2 \bigr)| + |2t \bigl( \sqrt{y_1} - \sqrt{y_2} \bigr)| $$ but the second term of the inequality is quite problematic: it is like proving that $x \mapsto \sqrt{x}$ is Lipschitz for $x\geq0$ , which is known to be false. So, I can't apply the theorem actually...Am I wrong? If so, what are my mistakes?","I don't know what's happening with this exercise. I need a help becuase I'm quite puzzled. Consider the Cauchy problem Study the existence and uniqueness Here Since (I have the square root), I consider as open neigbourhood , but in this way I am in trouble with because it's discontinuous at . So I should look for a weaker condition as Lipschitz continuity: I take and in : but the second term of the inequality is quite problematic: it is like proving that is Lipschitz for , which is known to be false. So, I can't apply the theorem actually...Am I wrong? If so, what are my mistakes?","\begin{cases}
y'=\frac{2}{t} y + 2 t \sqrt{y} \\
y(1)=0
\end{cases} f(t,y)=\frac{2}{t} y + 2 t \sqrt{y} y\geq0 K = \{t: |t-1|< r_1 \} \times \{y: 0 < y < r_2 \} f_y(t,y)= \frac{2}{t} + \frac{t}{\sqrt{y}} y=0 (t,y_1) (t,y_2) K |\frac{2}{t} \bigl(y_1 - y_2 \bigr) + 2t \bigl( \sqrt{y_1} - \sqrt{y_2} \bigr)|  \leq |\frac{2}{t} \bigl(y_1 - y_2 \bigr)| + |2t \bigl( \sqrt{y_1} - \sqrt{y_2} \bigr)|  x \mapsto \sqrt{x} x\geq0","['real-analysis', 'ordinary-differential-equations', 'lipschitz-functions', 'cauchy-problem']"
42,Eigenvalues of a DAE and stability analysis,Eigenvalues of a DAE and stability analysis,,"I have a nonlinear system of differential algebraic equations (DAE) of index -1,  and I was wondering: what is the ""meaning"" of eigenvalues in that case? I know they do not indicate stability, as with regular ODEs, unless the Algebraic terms are eliminated, which is not an option in my case. What do they mean, then? How do I investigate the stability, in such a DAE?","I have a nonlinear system of differential algebraic equations (DAE) of index -1,  and I was wondering: what is the ""meaning"" of eigenvalues in that case? I know they do not indicate stability, as with regular ODEs, unless the Algebraic terms are eliminated, which is not an option in my case. What do they mean, then? How do I investigate the stability, in such a DAE?",,"['ordinary-differential-equations', 'systems-of-equations', 'dynamical-systems']"
43,Can one prove that there are exactly $n$ arbitrary constants exist in the solution of a $n$th order differential equation?,Can one prove that there are exactly  arbitrary constants exist in the solution of a th order differential equation?,n n,"I've heard that an $n$ -th order differential equation will always have exactly $n$ arbitrary constants in its solution; that is, if $y$ satisfies the differential equation $f(y(t),y^{(1)}(t),\cdots, y^{(n)}(t), t) = 0$ then it will always be of the form $y(t) = g(t; C_1, \cdots, C_n)$ where $C_1, \cdots C_n \in \mathbb{C}$ . This makes intuitive sense, as one might have to integrate $n$ times to get to the solution, which would mean $n$ constants of integration. It can even be proven quite easily in the constant-coefficient case; given that $\sum _{i = 1} ^{n} a_i \hat{D}^i y(t) = f_0(t)$ , one can factor the differential operator $\sum _{i = 1} ^{n} a_i \hat{D}^i$ that acts on $y$ into $n$ first order derivatives according to solutions to the characteristic equation, which are all integrated to create $n$ arbitrary constants. However, I am not satisfied with this argument. Consider, for example, the following second-order differential equation: \begin{align} \sin(y'') + (y')^2 = y\cos(t^2y''). \end{align} I cannot imagine how one would be able to apply either of the previous arguments (factorisation of the differential operator or $n$ integrations) to show that this has exactly two arbitrary constants. And even if one could show that there are two integrations in this case, the general case is still uncertain: Are there always going to be $n$ arbitrary constants? Is there perhaps a rigorous proof of this?","I've heard that an -th order differential equation will always have exactly arbitrary constants in its solution; that is, if satisfies the differential equation then it will always be of the form where . This makes intuitive sense, as one might have to integrate times to get to the solution, which would mean constants of integration. It can even be proven quite easily in the constant-coefficient case; given that , one can factor the differential operator that acts on into first order derivatives according to solutions to the characteristic equation, which are all integrated to create arbitrary constants. However, I am not satisfied with this argument. Consider, for example, the following second-order differential equation: I cannot imagine how one would be able to apply either of the previous arguments (factorisation of the differential operator or integrations) to show that this has exactly two arbitrary constants. And even if one could show that there are two integrations in this case, the general case is still uncertain: Are there always going to be arbitrary constants? Is there perhaps a rigorous proof of this?","n n y f(y(t),y^{(1)}(t),\cdots, y^{(n)}(t), t) = 0 y(t) = g(t; C_1, \cdots, C_n) C_1, \cdots C_n \in \mathbb{C} n n \sum _{i = 1} ^{n} a_i \hat{D}^i y(t) = f_0(t) \sum _{i = 1} ^{n} a_i \hat{D}^i y n n \begin{align} \sin(y'') + (y')^2 = y\cos(t^2y''). \end{align} n n",['ordinary-differential-equations']
44,Conditions for bounded solutions to inhomogeneous system of linear ODEs,Conditions for bounded solutions to inhomogeneous system of linear ODEs,,"Consider the simple autonomous, non-homogeneous system of linear ODE's $$\dot{\mathbf{x}}(t) = M\mathbf{x}(t) + \mathbf{b}$$ where $\mathbf{x} :\mathbb{R}\to\mathbb{C}^n$ is a vector valued function of time, $M$ is a constant $n\times n$ complex matrix, and $\mathbf{b}\in\mathbb{C}^n$ is constant. My main question is the following: Q : What are the necessary conditions placed on $M$ and $\mathbf{b}$ such that all solutions $\mathbf{x}(t)$ are bounded, in norm, for all $t$ ? That is, all solutions satisfy $\|\mathbf{x}(t)\|\leq R$ for all $t$ , with $R\in(0,\infty)$ fixed. When $\mathbf{b} = 0$ , I know the answer: the eigenvalues $\lambda$ of $M$ must satisfy $\text{Re}(\lambda)\leq 0$ and the Jordan blocks (in a Jordan normal form of $M$ ) corresponding to any eigenvalue with $\text{Re}(\lambda)=0$ must size $1\times 1$ . For $\mathbf{b} \neq 0$ , can the Jordan normal form of $M$ alone determine whether all solutions are bounded? Or, does $\mathbf{b}$ play a non-trivial role in diagnosing this notion of ""stability""? I think I can prove that all solutions are bounded whenever $M$ is diagonalizable and has eigenvalues that lie strictly in the left half plane ( $\text{Re}(\lambda)<0$ ) but am not sure how to move beyond. I should also say that I am aware of other notions of stability such as Lyapunov stability, asymptotic stability, etc, and I know various theorems relating the Jordan normal form of $M$ to these notions. I'm simply having trouble finding results relating to this boundedness criterion I state. Thanks to anyone who may be able to answer or point me in the right direction!","Consider the simple autonomous, non-homogeneous system of linear ODE's where is a vector valued function of time, is a constant complex matrix, and is constant. My main question is the following: Q : What are the necessary conditions placed on and such that all solutions are bounded, in norm, for all ? That is, all solutions satisfy for all , with fixed. When , I know the answer: the eigenvalues of must satisfy and the Jordan blocks (in a Jordan normal form of ) corresponding to any eigenvalue with must size . For , can the Jordan normal form of alone determine whether all solutions are bounded? Or, does play a non-trivial role in diagnosing this notion of ""stability""? I think I can prove that all solutions are bounded whenever is diagonalizable and has eigenvalues that lie strictly in the left half plane ( ) but am not sure how to move beyond. I should also say that I am aware of other notions of stability such as Lyapunov stability, asymptotic stability, etc, and I know various theorems relating the Jordan normal form of to these notions. I'm simply having trouble finding results relating to this boundedness criterion I state. Thanks to anyone who may be able to answer or point me in the right direction!","\dot{\mathbf{x}}(t) = M\mathbf{x}(t) + \mathbf{b} \mathbf{x} :\mathbb{R}\to\mathbb{C}^n M n\times n \mathbf{b}\in\mathbb{C}^n M \mathbf{b} \mathbf{x}(t) t \|\mathbf{x}(t)\|\leq R t R\in(0,\infty) \mathbf{b} = 0 \lambda M \text{Re}(\lambda)\leq 0 M \text{Re}(\lambda)=0 1\times 1 \mathbf{b} \neq 0 M \mathbf{b} M \text{Re}(\lambda)<0 M","['linear-algebra', 'ordinary-differential-equations', 'stability-theory']"
45,How to solve the ODE $y' = \frac{x+y-2}{y-x-4}$?,How to solve the ODE ?,y' = \frac{x+y-2}{y-x-4},"I am trying to solve the ODE $$y' = \frac{x+y-2}{y-x-4} \tag1 $$ This is a homogeneous special form ODE. Let $x = u -1$ and $y=v+3$ in order to transform it to a homogeneous ODE. Hence, $$ (1) \iff v'(u) = \frac{u+v(u)}{v-u} \tag 2$$ At last let $v(u) = z(u)u \iff v'(u) = z'(u)u+z(u)$ therefore, $$ (2) \iff \frac{z'(u)}{z(u)+1} = \frac1u \tag 3$$ $(3)$ is a seperate variable ODE. Therefore we integrate both sides. $$ \int \frac{z'(u)}{z(u)+1} \,du = \int \frac1u \,du $$ This integral seems to be easy to evaluate $$ \int \frac{z'(u)}{z(u)+1} \, du $$ but I can't get my head around it. Any ideas?","I am trying to solve the ODE This is a homogeneous special form ODE. Let and in order to transform it to a homogeneous ODE. Hence, At last let therefore, is a seperate variable ODE. Therefore we integrate both sides. This integral seems to be easy to evaluate but I can't get my head around it. Any ideas?","y' = \frac{x+y-2}{y-x-4} \tag1  x = u -1 y=v+3  (1) \iff v'(u) = \frac{u+v(u)}{v-u} \tag 2 v(u) = z(u)u \iff v'(u) = z'(u)u+z(u)  (2) \iff \frac{z'(u)}{z(u)+1} = \frac1u \tag 3 (3)  \int \frac{z'(u)}{z(u)+1} \,du = \int \frac1u \,du   \int \frac{z'(u)}{z(u)+1} \, du ","['calculus', 'integration', 'ordinary-differential-equations', 'indefinite-integrals']"
46,Prove that $f(x) \leq K \cdot\exp(L\cdot \int_a^x g(t)dt)$,Prove that,f(x) \leq K \cdot\exp(L\cdot \int_a^x g(t)dt),"Suppose that $f, g$ are non negative continuous functions in $[a, b]$ , and $K, L$ positives constants such that $$ f(x) \leq K + L \int_a^x f(t)g(t) dt,\quad \forall x \in [a, b] . $$ Prove that $$ f(x) \leq K\exp\left(L\int_a^x g(t)dt\right). $$ I tried to use the continous proprierty by applying the first inequality for $x = a$ For $x = a,$ $$ f(a) \leq K \implies \exists I \subset [a, b]\ \text{such that}\  f(x) \leq 2K\ \forall x\in I $$ However I couldn't go much further.","Suppose that are non negative continuous functions in , and positives constants such that Prove that I tried to use the continous proprierty by applying the first inequality for For However I couldn't go much further.","f, g [a, b] K, L 
f(x) \leq K + L \int_a^x f(t)g(t) dt,\quad \forall x \in [a, b] .
 
f(x) \leq K\exp\left(L\int_a^x g(t)dt\right).
 x = a x = a, 
f(a) \leq K \implies \exists I \subset [a, b]\ \text{such that}\  f(x) \leq 2K\ \forall x\in I
","['real-analysis', 'ordinary-differential-equations', 'inequality']"
47,Linearized system for $ \begin{cases} \frac{d}{dt} x_1 = -x_1 + x_2 \\ \frac{d}{dt} x_2 = x_1 - x_2^3 \end{cases} $ is not resting at rest point?,Linearized system for  is not resting at rest point?, \begin{cases} \frac{d}{dt} x_1 = -x_1 + x_2 \\ \frac{d}{dt} x_2 = x_1 - x_2^3 \end{cases} ,"Assume there is the dynamical system $$ \begin{align} \frac{d}{dt} x_1 &= -x_1 + x_2 \\ \frac{d}{dt} x_2 &=  x_1 - x_2^3 \end{align} $$ The system is at rest at the point $(x_1, x_2) = (1, 1)$ and the point is stable. At this point of course $$ \begin{align} \frac{d}{dt} x_1 &= 0 \\ \frac{d}{dt} x_2 &= 0 \end{align} $$ I want to investigate the rest point more and so I use the linear model from the Taylor series at the rest point: $$ \frac{d}{dt}x = \begin{pmatrix} -1 & 1 \\ 1 & -3 \end{pmatrix}x $$ I want to simulate both nonlinear and linear model. But something is strange. At the rest point I have: $$ \frac{d}{dt}x = \begin{pmatrix} -1 & 1 \\ 1 & -3 \end{pmatrix} \begin{pmatrix} 1 \\ 1 \end{pmatrix} = \begin{pmatrix} 0 \\ -2 \end{pmatrix} $$ So although the nonlinear model is at rest at $(1, 1)$ the linear model is not at rest there! So when I simulate both systems they are very different even at the start and even if the start point is very near to the rest point. Look: The red $x_2$ trajectory is going even in the wrong direction at the start. What is the cause of the problem? Shouldn't the linear system approximate the nonlinear system at least when it starts near the rest point?",Assume there is the dynamical system The system is at rest at the point and the point is stable. At this point of course I want to investigate the rest point more and so I use the linear model from the Taylor series at the rest point: I want to simulate both nonlinear and linear model. But something is strange. At the rest point I have: So although the nonlinear model is at rest at the linear model is not at rest there! So when I simulate both systems they are very different even at the start and even if the start point is very near to the rest point. Look: The red trajectory is going even in the wrong direction at the start. What is the cause of the problem? Shouldn't the linear system approximate the nonlinear system at least when it starts near the rest point?,"
\begin{align}
\frac{d}{dt} x_1 &= -x_1 + x_2 \\
\frac{d}{dt} x_2 &=  x_1 - x_2^3
\end{align}
 (x_1, x_2) = (1, 1) 
\begin{align}
\frac{d}{dt} x_1 &= 0 \\
\frac{d}{dt} x_2 &= 0
\end{align}
 
\frac{d}{dt}x = \begin{pmatrix} -1 & 1 \\ 1 & -3 \end{pmatrix}x
 
\frac{d}{dt}x = \begin{pmatrix} -1 & 1 \\ 1 & -3 \end{pmatrix} \begin{pmatrix} 1 \\ 1 \end{pmatrix} = \begin{pmatrix} 0 \\ -2 \end{pmatrix}
 (1, 1) x_2","['ordinary-differential-equations', 'numerical-methods', 'dynamical-systems', 'control-theory', 'linearization']"
48,Exists $t^*\in \mathbb{R}$ such that $y(t^*)=-1$? [closed],Exists  such that ? [closed],t^*\in \mathbb{R} y(t^*)=-1,"Closed. This question does not meet Mathematics Stack Exchange guidelines . It is not currently accepting answers. Please provide additional context , which ideally explains why the question is relevant to you and our  community. Some forms of context include: background and motivation, relevant definitions, source, possible strategies, your current progress, why the question is interesting or important, etc. Closed 3 years ago . Improve this question $y'=y^2-3y+2, y(0) = \frac{3}{2}$ Exists $t^*\in \mathbb{R}$ such that $y(t^*)=-1$ ?. How to prove without solving the ode? Any hint?","Closed. This question does not meet Mathematics Stack Exchange guidelines . It is not currently accepting answers. Please provide additional context , which ideally explains why the question is relevant to you and our  community. Some forms of context include: background and motivation, relevant definitions, source, possible strategies, your current progress, why the question is interesting or important, etc. Closed 3 years ago . Improve this question Exists such that ?. How to prove without solving the ode? Any hint?","y'=y^2-3y+2, y(0) = \frac{3}{2} t^*\in \mathbb{R} y(t^*)=-1",[]
49,Is it possible to solve $\frac{dy}{dx}=1+ax^my^2$?,Is it possible to solve ?,\frac{dy}{dx}=1+ax^my^2,"I have a problem with the following equation, $\frac{dy}{dx}=1+ax^my^2$ I had solution for $m=1$ using Mathematica. I need to find a general solution for $m$ . The equation can be rewritten by parametrizeing $y=\frac{u}{u'}$ as $u''+ax^mu=0$ ; i want declare that problem i introduced is a simplified form from  such equation! I need absolute solution (not approximate) for the general case. And its OK if the solution is in special functions (because, actually, its the only way to do it). Can anyone here help me please? Thank you guys","I have a problem with the following equation, I had solution for using Mathematica. I need to find a general solution for . The equation can be rewritten by parametrizeing as ; i want declare that problem i introduced is a simplified form from  such equation! I need absolute solution (not approximate) for the general case. And its OK if the solution is in special functions (because, actually, its the only way to do it). Can anyone here help me please? Thank you guys",\frac{dy}{dx}=1+ax^my^2 m=1 m y=\frac{u}{u'} u''+ax^mu=0,"['calculus', 'ordinary-differential-equations']"
50,Euler's method to approximate a differential equation $\frac{dy}{dx} = x - y$,Euler's method to approximate a differential equation,\frac{dy}{dx} = x - y,"Question: Use Euler's method to find approximate values for the solution of the initial value $-$ problem $$\frac{dy}{dx} = x-y$$ $$y(0)=1$$ on the interval $[0,1]$ using five steps of size $h = 0.2$ . My attempts: I know that the recurrence relation $y_{n+1} = y_{n} + hf(x_n,y_n)$ however I am unable to see how the interval comes into play. An idea I had was to consider the bounds of the interval and approximate $y(0)$ and $y(1)$ however this does not include $h$ so I am extremely skeptical. Any help or guidance is greatly appreciated!",Question: Use Euler's method to find approximate values for the solution of the initial value problem on the interval using five steps of size . My attempts: I know that the recurrence relation however I am unable to see how the interval comes into play. An idea I had was to consider the bounds of the interval and approximate and however this does not include so I am extremely skeptical. Any help or guidance is greatly appreciated!,"- \frac{dy}{dx} = x-y y(0)=1 [0,1] h = 0.2 y_{n+1} = y_{n} + hf(x_n,y_n) y(0) y(1) h","['calculus', 'ordinary-differential-equations', 'numerical-methods', 'approximation', 'eulers-method']"
51,How to derive a (first-order) differential equation from a slope field,How to derive a (first-order) differential equation from a slope field,,"A first order differential equation has a slope field shown in the following. Question: a) Suggest, with reasons, the simplest first order differential equation consistent with the slope field shown. b) Suggest a possible general solution for your differential equation. So what are the things that I should look for or things that I need to keep in mind while trying to derive a differential equation from a slope field? In the first glance, I think it resembles a cubic? But that is just from observation, how can I confirm this?","A first order differential equation has a slope field shown in the following. Question: a) Suggest, with reasons, the simplest first order differential equation consistent with the slope field shown. b) Suggest a possible general solution for your differential equation. So what are the things that I should look for or things that I need to keep in mind while trying to derive a differential equation from a slope field? In the first glance, I think it resembles a cubic? But that is just from observation, how can I confirm this?",,"['calculus', 'ordinary-differential-equations']"
52,Reduce the differential equation $y= 2px+p^{2}y^{2}$ to Clairaut’s form,Reduce the differential equation  to Clairaut’s form,y= 2px+p^{2}y^{2},"Reduce the following differential equation to Clairaut’s form by using the substitution and hence solve: $y= 2px+p^{2}y^{2}$ where $p={dy\over dx}$ I used $y^{2}=v$ then I get $v-2p_{1}x + {(x p_{1})^{2}\over v}= ({p_{1}\over2})^{4}$ where $p_{1}={dv\over dx}$ so this is not useful to reduce to Clairaut's form, please give me a hint to solve this or give me a suitable substitution Thank you.","Reduce the following differential equation to Clairaut’s form by using the substitution and hence solve: where I used then I get where so this is not useful to reduce to Clairaut's form, please give me a hint to solve this or give me a suitable substitution Thank you.",y= 2px+p^{2}y^{2} p={dy\over dx} y^{2}=v v-2p_{1}x + {(x p_{1})^{2}\over v}= ({p_{1}\over2})^{4} p_{1}={dv\over dx},['ordinary-differential-equations']
53,How to solve particular solution of a general differential equation involving $\cos^3 x$?,How to solve particular solution of a general differential equation involving ?,\cos^3 x,"My question was to solve, $(D^2 +2D +5) y= e^x \cos^3 x. \space$ I solved complementary function solution but got stuck in solving particular solution. I am attaching my solution where I stuck, please help me to solve it. Thanks a lot. Image of question and my attempt to solve","My question was to solve, I solved complementary function solution but got stuck in solving particular solution. I am attaching my solution where I stuck, please help me to solve it. Thanks a lot. Image of question and my attempt to solve",(D^2 +2D +5) y= e^x \cos^3 x. \space,"['calculus', 'ordinary-differential-equations']"
54,checking uniqueness and boundedness of an initial value problem,checking uniqueness and boundedness of an initial value problem,,"Consider the initial value problem $$\frac{dy}{dx}=x^2+y^2$$ with $y(0)=1$ where $0≤x≤1$ . Then which of the following statements are true? $(a)$ There exists a unique solution in $\displaystyle\bigg[0,\frac{\pi}{4}\bigg]$ . $(b)$ Every solution is bounded in $\displaystyle\bigg[0,\frac{\pi}{4}\bigg]$ . $(c)$ The solution exhibits a singularity at some point in $[0,1]$ . $(d)$ The solution becomes unbounded in some subinterval of $\displaystyle\bigg[\frac{\pi}{4},1\bigg]$ . For $(a)$ ; I started out by finding the largest interval of existence by Picard's theorem. Considering a rectangular strip $|x|≤h$ and $|y−1|≤k$ , we see that $|x^2+y^2|≤|x|^2+|(y−1)+1|^2≤h^2+k^2+1=M$ . Now the maximum interval of existence is $|x|≤h′$ where $\displaystyle h′=\min\bigg\{h,\frac{k}{M}\bigg\}=\min\bigg\{h,\frac{k}{h^2+k^2+1}\bigg\}$ . But I'm unable to check that minimum to see whether $\displaystyle h'>\frac{\pi}{4}$ or $\displaystyle h'<\frac{\pi}{4}$ . Also how to check the boundedness/singularity of solutions in the above intervals I don't understand. Any help is appreciated. I know the solution can be found using Bessel function. But that's not what I'm asking here. I want a method without explicitly finding the solution.","Consider the initial value problem with where . Then which of the following statements are true? There exists a unique solution in . Every solution is bounded in . The solution exhibits a singularity at some point in . The solution becomes unbounded in some subinterval of . For ; I started out by finding the largest interval of existence by Picard's theorem. Considering a rectangular strip and , we see that . Now the maximum interval of existence is where . But I'm unable to check that minimum to see whether or . Also how to check the boundedness/singularity of solutions in the above intervals I don't understand. Any help is appreciated. I know the solution can be found using Bessel function. But that's not what I'm asking here. I want a method without explicitly finding the solution.","\frac{dy}{dx}=x^2+y^2 y(0)=1 0≤x≤1 (a) \displaystyle\bigg[0,\frac{\pi}{4}\bigg] (b) \displaystyle\bigg[0,\frac{\pi}{4}\bigg] (c) [0,1] (d) \displaystyle\bigg[\frac{\pi}{4},1\bigg] (a) |x|≤h |y−1|≤k |x^2+y^2|≤|x|^2+|(y−1)+1|^2≤h^2+k^2+1=M |x|≤h′ \displaystyle h′=\min\bigg\{h,\frac{k}{M}\bigg\}=\min\bigg\{h,\frac{k}{h^2+k^2+1}\bigg\} \displaystyle h'>\frac{\pi}{4} \displaystyle h'<\frac{\pi}{4}",['ordinary-differential-equations']
55,Are there any explicit solutions of $yy' = 5x$ that pass through the origin?,Are there any explicit solutions of  that pass through the origin?,yy' = 5x,First part: Use the fact that $5x^2 − y^2 = c$ is a one-parameter family of solutions of the differential equation: $y y'= 5x$ to find an implicit solution of the initial-value problem: $$y \dfrac {dy}{dx} = 5x \\ y(2) = −6$$ I obtained the answer as $y^2=5x^2+16$ . Then it asks if there are any explicit solutions of $yy' = 5x$ that pass through the origin? I'm not sure how to go about this part.,First part: Use the fact that is a one-parameter family of solutions of the differential equation: to find an implicit solution of the initial-value problem: I obtained the answer as . Then it asks if there are any explicit solutions of that pass through the origin? I'm not sure how to go about this part.,5x^2 − y^2 = c y y'= 5x y \dfrac {dy}{dx} = 5x \\ y(2) = −6 y^2=5x^2+16 yy' = 5x,['ordinary-differential-equations']
56,Is every (sufficiently nice) function a solution to an autonomous ode?,Is every (sufficiently nice) function a solution to an autonomous ode?,,"For whatever reason, I'm suddenly interested in whether or not one can characterized those functions which are solutions to autonomous ode. The first natural question to ask, then, is: are all $C^1([a,b])$ functions solutions to some autonomous ode? If not, why not?","For whatever reason, I'm suddenly interested in whether or not one can characterized those functions which are solutions to autonomous ode. The first natural question to ask, then, is: are all functions solutions to some autonomous ode? If not, why not?","C^1([a,b])","['real-analysis', 'calculus', 'ordinary-differential-equations', 'derivatives']"
57,Integral Curves of Vector Fields with Zero Divergence or Zero Curl,Integral Curves of Vector Fields with Zero Divergence or Zero Curl,,"Say we've got some vector field which at every point indicates the instantaneous velocity of a particle moving through that point. I'm trying to gain some intuition for what the possible trajectories for particles would look like in the cases that this vector field has zero divergence, zero curl, or both... There's GOT to be something special about the trajectories in such vector fields... I'm still having trouble wrapping my head around divergence and curl...although, I think intuitively understanding possible trajectories will help. As for my background...I have not taken differential equations, and am just about done with an introductory course on multi-variable calculus. Thanks again! Edit: After receiving some answers and hints, I'd like to write out some of my thoughts... Say we have the vector-field $\vec{F} = (2x,-2y)$ . This vector-field has both zero divergence and zero curl. There are many possible ways to interpret such a vector-field: ie, an acceleration, a force...etc...but, lets focus on these two interpretations: First, lets consider it the gradient of a function. In this case, the function would be $f(x,y)=x^2-y^2$ , and thus $\nabla(f)=\vec{F}$ . As I understand it, the fact that $\vec{F}$ has no curl means that it can be the gradient of a function in the first place, because a line-integral in a closed circle is zero. As for what it means for the divergence to be zero everywhere...well, the divergence of the gradient is the Laplacian of the original function. It means that the function is harmonic, so that at each point, the ""bending"" is equal in all directions. If this function represented some stretched surface, no point would have any reason to bend, because the force on any point would cancel out from the points around it. Those are both beautiful and intuitive results...but, that wasn't what my question was driving it. I want to interpret $\vec{F}$ as a velocity-field . That is, at each point, $2x=\frac{dx}{dt}$ and $2y=\frac{dy}{dt}$ . In that case, integral curves (flow-lines) starting from some point $(x_0,y_0)$ would look like: $$(x_0e^{2t}, y_0e^{-2t})$$ And as for those flow-lines...I have absolutely no intuition what's special about them, coming from the fact that the divergence is zero, or that the curl is zero. These are what I wish to understand! Thanks!","Say we've got some vector field which at every point indicates the instantaneous velocity of a particle moving through that point. I'm trying to gain some intuition for what the possible trajectories for particles would look like in the cases that this vector field has zero divergence, zero curl, or both... There's GOT to be something special about the trajectories in such vector fields... I'm still having trouble wrapping my head around divergence and curl...although, I think intuitively understanding possible trajectories will help. As for my background...I have not taken differential equations, and am just about done with an introductory course on multi-variable calculus. Thanks again! Edit: After receiving some answers and hints, I'd like to write out some of my thoughts... Say we have the vector-field . This vector-field has both zero divergence and zero curl. There are many possible ways to interpret such a vector-field: ie, an acceleration, a force...etc...but, lets focus on these two interpretations: First, lets consider it the gradient of a function. In this case, the function would be , and thus . As I understand it, the fact that has no curl means that it can be the gradient of a function in the first place, because a line-integral in a closed circle is zero. As for what it means for the divergence to be zero everywhere...well, the divergence of the gradient is the Laplacian of the original function. It means that the function is harmonic, so that at each point, the ""bending"" is equal in all directions. If this function represented some stretched surface, no point would have any reason to bend, because the force on any point would cancel out from the points around it. Those are both beautiful and intuitive results...but, that wasn't what my question was driving it. I want to interpret as a velocity-field . That is, at each point, and . In that case, integral curves (flow-lines) starting from some point would look like: And as for those flow-lines...I have absolutely no intuition what's special about them, coming from the fact that the divergence is zero, or that the curl is zero. These are what I wish to understand! Thanks!","\vec{F} = (2x,-2y) f(x,y)=x^2-y^2 \nabla(f)=\vec{F} \vec{F} \vec{F} 2x=\frac{dx}{dt} 2y=\frac{dy}{dt} (x_0,y_0) (x_0e^{2t}, y_0e^{-2t})","['ordinary-differential-equations', 'multivariable-calculus', 'vector-analysis', 'intuition', 'vector-fields']"
58,"Orthonormal basis of $L^2(0,1)$",Orthonormal basis of,"L^2(0,1)","Show that the only functions that satisfy $f''(\xi) = \lambda f(\xi)$ , $f'(0) = f(1) = 0$ , $\|f\|_2 = 1$ for some $\lambda \in \mathbb{R}$ are the functions: $$ \phi_n(\xi) = \sqrt{2}\cos\left(\frac{(2n-1)\pi}{2}\xi\right),  \quad n =1,2,\dots $$ thus making them an orthonormal basis of $L^2(0,1)$ It's obvious to me why this family of functions satisfy the conditions $f'(0) = f(1) = 0$ and $\|f\|_2 = 1$ . What confuses me is the constant $\lambda$ . I fail to see why for every $\lambda$ the solutions are $\phi_n$ . Won't positive $\lambda$ 's give exponential functions as solutions? It'd be very helpful if someone could elaborate on how this family of functions is yielded from the original equation $f''(\xi) = \lambda f(\xi)$ .","Show that the only functions that satisfy , , for some are the functions: thus making them an orthonormal basis of It's obvious to me why this family of functions satisfy the conditions and . What confuses me is the constant . I fail to see why for every the solutions are . Won't positive 's give exponential functions as solutions? It'd be very helpful if someone could elaborate on how this family of functions is yielded from the original equation .","f''(\xi) = \lambda f(\xi) f'(0) = f(1) = 0 \|f\|_2 = 1 \lambda \in \mathbb{R}  \phi_n(\xi) = \sqrt{2}\cos\left(\frac{(2n-1)\pi}{2}\xi\right),  \quad n =1,2,\dots  L^2(0,1) f'(0) = f(1) = 0 \|f\|_2 = 1 \lambda \lambda \phi_n \lambda f''(\xi) = \lambda f(\xi)","['calculus', 'ordinary-differential-equations', 'orthonormal']"
59,Symmetric coefficients imply symmetric solutions?,Symmetric coefficients imply symmetric solutions?,,"Let us consider any one-diomensional ODE of the form: $$ \dfrac{d^{2}y}{dx^{2}}+a(x)y(x)+b(x)=0, $$ where $a,b:\mathbb{R}\to\mathbb{R}$ are both even functions. Does these hypothesis implies that the solution $y(x)$ must to be even, regardless what $a(x)$ and $b(x)$ actually? Are there some non-symmetric solution for some specific even functions $a(x)$ and $b(x)$ ? Same question if I change the hypothesis on $b(x)$ for odd function. Does that implies that the solution must to be odd? Edit: I just realize that $y(x)$ could be odd, even if $a(x)$ is even just by assuming $b(x)\equiv0$ . But, what about if I add the hypothesis of $y(x)>0$ for all $x\in\mathbb{R}$ ?","Let us consider any one-diomensional ODE of the form: where are both even functions. Does these hypothesis implies that the solution must to be even, regardless what and actually? Are there some non-symmetric solution for some specific even functions and ? Same question if I change the hypothesis on for odd function. Does that implies that the solution must to be odd? Edit: I just realize that could be odd, even if is even just by assuming . But, what about if I add the hypothesis of for all ?","
\dfrac{d^{2}y}{dx^{2}}+a(x)y(x)+b(x)=0,
 a,b:\mathbb{R}\to\mathbb{R} y(x) a(x) b(x) a(x) b(x) b(x) y(x) a(x) b(x)\equiv0 y(x)>0 x\in\mathbb{R}","['real-analysis', 'ordinary-differential-equations', 'analysis']"
60,How to solve $\frac{dy}{dx} = \frac{1}{\sqrt{2^2 - x^2}}$?,How to solve ?,\frac{dy}{dx} = \frac{1}{\sqrt{2^2 - x^2}},"I've just started with differential equations and in the textbook I was given two, with one of which I have trouble. The task was to solve them with software, but I considered it'd be better to solve by hand. But since it's just the start of the topic in the textbook, not much has been provided about how to do it. The equation and the initial value problem are: $$ \frac{dy}{dx} = \frac{1}{\sqrt{2^2 - x^2}} \\\ y(0) = 2 $$ I looked into two tables of integrals (so it's doublechecked) and found this: $$ \int{\frac{1}{\sqrt{a^2 - x^2}}}{dx} = \frac{1}{\sin{\frac{x}{a}}} + C $$ But the problem is that y is not defined at 0 (if the tables are correct). Since y(0) = 2 might be a typo in the textbook, I tried to check whether the solution would work for some other point. I took x = 1 and C = 0 and tried to plot y and t (the tangent) in software: $$ y(x) = \frac{1}{\sin{\frac{x}{a}}} \\\ y'(1) = \frac{1}{\sqrt{3}} \\\ t(x) = \frac{x - x_0}{\sqrt{3}} + t(x_0) = \frac{x - 1}{\sqrt{3}} + \frac{1}{\sin{\frac{1}{2}}} $$ As you can see, t (the yellow curve) is definitely not tangent to y(x). But it seems normal, and indeed, when I changed the yellow to $t(x) = -\sqrt{3}(x - 1) + \frac{1}{\sin{\frac{1}{2}}}$ it became tangent at x = 1. So could anyone, please, explain, what's going on here? Did I use the integral tables correctly? How come the function under the integral sign (which is a derivative) gives not a tangent but normal to the right hand side of the integral solution? Thank you.","I've just started with differential equations and in the textbook I was given two, with one of which I have trouble. The task was to solve them with software, but I considered it'd be better to solve by hand. But since it's just the start of the topic in the textbook, not much has been provided about how to do it. The equation and the initial value problem are: I looked into two tables of integrals (so it's doublechecked) and found this: But the problem is that y is not defined at 0 (if the tables are correct). Since y(0) = 2 might be a typo in the textbook, I tried to check whether the solution would work for some other point. I took x = 1 and C = 0 and tried to plot y and t (the tangent) in software: As you can see, t (the yellow curve) is definitely not tangent to y(x). But it seems normal, and indeed, when I changed the yellow to it became tangent at x = 1. So could anyone, please, explain, what's going on here? Did I use the integral tables correctly? How come the function under the integral sign (which is a derivative) gives not a tangent but normal to the right hand side of the integral solution? Thank you.","
\frac{dy}{dx} = \frac{1}{\sqrt{2^2 - x^2}} \\\
y(0) = 2
 
\int{\frac{1}{\sqrt{a^2 - x^2}}}{dx} = \frac{1}{\sin{\frac{x}{a}}} + C
 
y(x) = \frac{1}{\sin{\frac{x}{a}}} \\\
y'(1) = \frac{1}{\sqrt{3}} \\\
t(x) = \frac{x - x_0}{\sqrt{3}} + t(x_0) = \frac{x - 1}{\sqrt{3}} + \frac{1}{\sin{\frac{1}{2}}}
 t(x) = -\sqrt{3}(x - 1) + \frac{1}{\sin{\frac{1}{2}}}","['calculus', 'ordinary-differential-equations', 'derivatives', 'indefinite-integrals', 'tangent-line']"
61,Solving an ODE with singular point: is this correct?,Solving an ODE with singular point: is this correct?,,"This is from a text book by Tsai Tai-Peng ""Lectures on Navier-Stokes Equations"", page 148 if anyone wants to read. But I am only asking about how to solve this ODE, $$(1-t^2) L'' +2L+LL' = 0, \quad t\in(-1,1)\\L(-1)=L(1)=0.$$ The author begins by noticing that $(1-t^2)$ is a solution to the linear problem (just deleting the $LL'$ term) $$ (1-t^2) L'' +2L = 0.$$ ""Therefore"", as a kind of variation of parameters method, he decides to set $$ L(t) \overset{\Delta}=u(t)(1-t^2).$$ Apparently, it follows that $u$ solves $$ u' + \frac{u^2}2 = 0?$$ I really can't prove this, and must be going crazy. If you want some scratch work to see: \begin{align} L &= (1-t^2)u\\ L’ &= (1-t^2)u’ -2tu\\ L’’ &= (1-t^2)u’’ - 2tu’ - 2tu’ - 2u\\ &= (1-t^2)u’’ - 4tu’ - 2u  \end{align} divide L equation by $1-t^2$ , \begin{align} (1-t^2)u’’ - 4tu’ - 2u + 2u + u L’ &= 0\\ (1-t^2)u’’ - 4tu’              + u[ (1-t^2)u’ -2tu ]&=0 \end{align} Any pointers? This is apparently from the thesis of the author Tsai Tai-Peng but I don't have access to it at the moment.","This is from a text book by Tsai Tai-Peng ""Lectures on Navier-Stokes Equations"", page 148 if anyone wants to read. But I am only asking about how to solve this ODE, The author begins by noticing that is a solution to the linear problem (just deleting the term) ""Therefore"", as a kind of variation of parameters method, he decides to set Apparently, it follows that solves I really can't prove this, and must be going crazy. If you want some scratch work to see: divide L equation by , Any pointers? This is apparently from the thesis of the author Tsai Tai-Peng but I don't have access to it at the moment.","(1-t^2) L'' +2L+LL' = 0, \quad t\in(-1,1)\\L(-1)=L(1)=0. (1-t^2) LL'  (1-t^2) L'' +2L = 0.  L(t) \overset{\Delta}=u(t)(1-t^2). u  u' + \frac{u^2}2 = 0? \begin{align} L &= (1-t^2)u\\ L’ &= (1-t^2)u’ -2tu\\
L’’ &= (1-t^2)u’’ - 2tu’ - 2tu’ - 2u\\
&= (1-t^2)u’’ - 4tu’ - 2u 
\end{align} 1-t^2 \begin{align}
(1-t^2)u’’ - 4tu’ - 2u + 2u + u L’ &= 0\\
(1-t^2)u’’ - 4tu’ 
            + u[ (1-t^2)u’ -2tu ]&=0
\end{align}","['ordinary-differential-equations', 'singular-solution']"
62,Why does the solution to our system imply the origin is a nonlinear center?,Why does the solution to our system imply the origin is a nonlinear center?,,"Can someone please tell me why our solution implies the origin is a nonlinear center? Thank you for your time and help! Consider the system $$x'=-y-x^2$$ $$y'=x.$$ Find a reversor $S$ , identify Fix $(S)$ and show that the origin is a nonlinear center. $\textbf{Solution:}$ $S(x,y) = \begin{pmatrix} -x \\ y \end{pmatrix}$ is a reversor for our system. Sending $x\to -x, y\to y,$ and $t\to -t$ we get $-(-x') = -y-(-x)^2 \implies x' = -y - x^2$ and $-y' = -x \implies y'=x.$ If $\begin{pmatrix} x \\ y \end{pmatrix} = \begin{pmatrix} -x \\ y \end{pmatrix}$ then $\begin{pmatrix} x \\ y \end{pmatrix} \in$ Fix $(S)$ . So, Fix $(S)$ $$= \left \{\begin{pmatrix} 0 \\ y \end{pmatrix}\colon y \in \mathbb{R} \right\}.$$","Can someone please tell me why our solution implies the origin is a nonlinear center? Thank you for your time and help! Consider the system Find a reversor , identify Fix and show that the origin is a nonlinear center. is a reversor for our system. Sending and we get and If then Fix . So, Fix","x'=-y-x^2 y'=x. S (S) \textbf{Solution:} S(x,y) = \begin{pmatrix} -x \\ y \end{pmatrix} x\to -x, y\to y, t\to -t -(-x') = -y-(-x)^2 \implies x' = -y - x^2 -y' = -x \implies y'=x. \begin{pmatrix} x \\ y \end{pmatrix} = \begin{pmatrix} -x \\ y \end{pmatrix} \begin{pmatrix} x \\ y \end{pmatrix} \in (S) (S) = \left \{\begin{pmatrix} 0 \\ y \end{pmatrix}\colon y \in \mathbb{R} \right\}.","['ordinary-differential-equations', 'manifolds', 'dynamical-systems', 'nonlinear-system']"
63,Existence theorem for the SIR epidemic differential equations,Existence theorem for the SIR epidemic differential equations,,"Well these days, everybody is talking about the epidemic model SIR which is given by the following differential equation: $$ \begin{cases} S'(t)=-aS(t)I(t)\\ I'(t)=aS(t)I(t)-bI(t)\\ R'(t)=bI(t)\\ S(0)>0,I(0)>0 \end{cases} $$ We can see that $S'(t)+I'(t)+R'(t)=0$ , so $S(t)+I(t)+R(t)=N$ is a constant. Thus this model reduces to two differential equations: $$ \begin{cases} S'(t)=-aS(t)I(t)\\ I'(t)=aS(t)I(t)-bI(t)\\ S(0)>0,I(0)>0 \end{cases} $$ All the studies I saw start with the fact that a solution of this model is global (defined on $[0,\infty)$ ) and positive. So I wanted to prove that this differential equation has a global solution. However I see that this cannot be done by the global Lipschitz Picard-Lindelöf/contraction theorem, since the function $$ f(x,y)=\left(-axy,axy-by\right) $$ is not globally Lipschitz. How can we prove the existence of a global solution and its positivity?","Well these days, everybody is talking about the epidemic model SIR which is given by the following differential equation: We can see that , so is a constant. Thus this model reduces to two differential equations: All the studies I saw start with the fact that a solution of this model is global (defined on ) and positive. So I wanted to prove that this differential equation has a global solution. However I see that this cannot be done by the global Lipschitz Picard-Lindelöf/contraction theorem, since the function is not globally Lipschitz. How can we prove the existence of a global solution and its positivity?","
\begin{cases}
S'(t)=-aS(t)I(t)\\
I'(t)=aS(t)I(t)-bI(t)\\
R'(t)=bI(t)\\
S(0)>0,I(0)>0
\end{cases}
 S'(t)+I'(t)+R'(t)=0 S(t)+I(t)+R(t)=N 
\begin{cases}
S'(t)=-aS(t)I(t)\\
I'(t)=aS(t)I(t)-bI(t)\\
S(0)>0,I(0)>0
\end{cases}
 [0,\infty) 
f(x,y)=\left(-axy,axy-by\right)
","['real-analysis', 'ordinary-differential-equations', 'lipschitz-functions']"
64,Intuition behind topological conjugacy between the flows of two systems of differential equations,Intuition behind topological conjugacy between the flows of two systems of differential equations,,"My book defines topological conjugacy between the flows of two differential equations as follows: Suppose $X' = AX \text{ and } X' = BX$ have flows $\phi ^A$ and $\phi ^B$ . These two systems are topologically conjugate if there exists a homeomorphism $h: \mathbb{R}^2 \to \mathbb{R}^2$ that satisfies $\phi ^B (t, h(X_0))= h(\phi ^A (t,X_0))$ . I understand the idea behind homemorphism, but why is conjugacy defined the way it is? Why is $h$ of the initial condition taken on one side while h of the flow is taken on the other? Why isn't it defined like $\phi ^B (t, X_0)= h(\phi ^A (t,X_0))$ instead?","My book defines topological conjugacy between the flows of two differential equations as follows: Suppose have flows and . These two systems are topologically conjugate if there exists a homeomorphism that satisfies . I understand the idea behind homemorphism, but why is conjugacy defined the way it is? Why is of the initial condition taken on one side while h of the flow is taken on the other? Why isn't it defined like instead?","X' = AX \text{ and } X' = BX \phi ^A \phi ^B h: \mathbb{R}^2 \to \mathbb{R}^2 \phi ^B (t, h(X_0))= h(\phi ^A (t,X_0)) h \phi ^B (t, X_0)= h(\phi ^A (t,X_0))","['ordinary-differential-equations', 'dynamical-systems']"
65,Find inverse laplace transform of $f(s) = \frac{1}{(s-2)^2+9}$,Find inverse laplace transform of,f(s) = \frac{1}{(s-2)^2+9},"Find inverse laplace transform of $f(s) = \frac{1}{(s-2)^2+9}$ Here is what I have gotten from partial fractions. I observed that $s^2-4s + 13$ is irreducible (doesn't have real roots). $$\frac{1}{(s-2)^2 + 9} =  \frac{1}{s^2-4s + 13} = \frac{As + B}{s^2-4s + 13}$$ A=0, B=1 The corresponding value in my table is $e^{at}\sin(bt)$ for the corresponding laplace transform $\frac{b}{(s-a)^2 + b^2}$ . so plugging in A and B, I get $e^{0t}\sin(1\cdot t) = \sin(t)$ . However my textbook answer is $\frac{1}{3}e^{2t}\sin{3t}$ . I am not sure how to trouble shoot this problem. I am pretty sure I did the partial fractions correctly, but the textbook answer implies that my approach to finding the inverse laplace transform is very wrong because I have no idea where the $\frac{1}{3}$ comes from in the solution.","Find inverse laplace transform of Here is what I have gotten from partial fractions. I observed that is irreducible (doesn't have real roots). A=0, B=1 The corresponding value in my table is for the corresponding laplace transform . so plugging in A and B, I get . However my textbook answer is . I am not sure how to trouble shoot this problem. I am pretty sure I did the partial fractions correctly, but the textbook answer implies that my approach to finding the inverse laplace transform is very wrong because I have no idea where the comes from in the solution.",f(s) = \frac{1}{(s-2)^2+9} s^2-4s + 13 \frac{1}{(s-2)^2 + 9} =  \frac{1}{s^2-4s + 13} = \frac{As + B}{s^2-4s + 13} e^{at}\sin(bt) \frac{b}{(s-a)^2 + b^2} e^{0t}\sin(1\cdot t) = \sin(t) \frac{1}{3}e^{2t}\sin{3t} \frac{1}{3},"['ordinary-differential-equations', 'laplace-transform']"
66,Finding the domain of ODE without calculating a solution,Finding the domain of ODE without calculating a solution,,"We have two ODEs: \begin{cases}     x'(t) = 2x^2-t \\     x(1) = 1   \end{cases} and \begin{cases}     x'(t) = t+e^x \\     x(1) = 0   \end{cases} I am to find(without calculating, only using theorems and slope fields) an interval on which the solution exists. They both are not Lipschitz so Picard–Lindelöf's doesn't work, but Peano does. We know that a solution exists in $[1 - \eta, 1 + \eta]$ . I drew a slope field which seems to be very steep for points outside $[-2, 2]$ for the first one and very steep for $x > 1$ for the second one. What can I do next?","We have two ODEs: and I am to find(without calculating, only using theorems and slope fields) an interval on which the solution exists. They both are not Lipschitz so Picard–Lindelöf's doesn't work, but Peano does. We know that a solution exists in . I drew a slope field which seems to be very steep for points outside for the first one and very steep for for the second one. What can I do next?","\begin{cases}
    x'(t) = 2x^2-t \\
    x(1) = 1
  \end{cases} \begin{cases}
    x'(t) = t+e^x \\
    x(1) = 0
  \end{cases} [1 - \eta, 1 + \eta] [-2, 2] x > 1","['real-analysis', 'ordinary-differential-equations']"
67,Show $S^2$ cannot have a smooth vector field with two zeros that are either both sources or both sinks,Show  cannot have a smooth vector field with two zeros that are either both sources or both sinks,S^2,"Backround I have just learned the Poincaré-Hopf Index Theorem which says that if $\overrightarrow{v}$ is a smooth vector field on a compact, oriented manifold $X$ with only finitely many zeros, then the global sum of the indices of $\overrightarrow{v}$ equals the Euler characteristic of $X$ . This is great, but it may not be the whole story regarding the ""rules"" of which combinations of different qualitative types of zeros a vector field can and can't have on a given space. Particular Question Edit: my ""particular question"" assumed something that was wrong, so only the general question makes sense For example, my intuition tells me that on $S^2$ one cannot have just two zeros where both are sources or both are sinks. But this is not ruled out by Poincaré-Hopf since index doesn't distinguish between source and sink in two dimensions. So how can we rigorously rule this out? General Quesiton What are the key theorems or theories used for which combinations of different qualitative types of zeros a vector field can and can't have on a given space/manifold? I am generally interested in simple spaces such as balls and spheres (in arbitrary dimensions) and also cartesian products thereof. I am not so interested in spaces with complicated combinations of holes of various dimensions and so on. So far I have been reading Guilleman and Pollack's Differential Topology.","Backround I have just learned the Poincaré-Hopf Index Theorem which says that if is a smooth vector field on a compact, oriented manifold with only finitely many zeros, then the global sum of the indices of equals the Euler characteristic of . This is great, but it may not be the whole story regarding the ""rules"" of which combinations of different qualitative types of zeros a vector field can and can't have on a given space. Particular Question Edit: my ""particular question"" assumed something that was wrong, so only the general question makes sense For example, my intuition tells me that on one cannot have just two zeros where both are sources or both are sinks. But this is not ruled out by Poincaré-Hopf since index doesn't distinguish between source and sink in two dimensions. So how can we rigorously rule this out? General Quesiton What are the key theorems or theories used for which combinations of different qualitative types of zeros a vector field can and can't have on a given space/manifold? I am generally interested in simple spaces such as balls and spheres (in arbitrary dimensions) and also cartesian products thereof. I am not so interested in spaces with complicated combinations of holes of various dimensions and so on. So far I have been reading Guilleman and Pollack's Differential Topology.",\overrightarrow{v} X \overrightarrow{v} X S^2,"['ordinary-differential-equations', 'differential-geometry', 'reference-request', 'differential-topology']"
68,First-order initial-value problem,First-order initial-value problem,,Consider this first-order initial value problem: $$\begin{cases} y'=1-x+y^2-xy^2\\ y(0)=1 \end{cases}$$ I've tried simplifying the right-hand side to $(y^2+1)(-x+1)$ and then integrating it. I obtained a general solution as a tangent function but it seems that this is not the answer.,Consider this first-order initial value problem: I've tried simplifying the right-hand side to and then integrating it. I obtained a general solution as a tangent function but it seems that this is not the answer.,"\begin{cases}
y'=1-x+y^2-xy^2\\
y(0)=1
\end{cases} (y^2+1)(-x+1)","['calculus', 'ordinary-differential-equations', 'initial-value-problems']"
69,Spectrum of the bi-Laplacian,Spectrum of the bi-Laplacian,,"I have the following operator: $$\begin{align*}D(A)&=\{f \in C[0,1]:f''''\in C[0,1] \text{ and } f^{(k)}(0)=0 \ \forall\ 0\leq k\leq 3\}\\Af&=-f''''.\end{align*}$$ I'm trying to find the spectrum of $A.$ My attempt: Solving the equation $(\lambda - A) g =f$ with the given boundary condition gives: $$g(x)=\frac{1}{4^{1/4}\lambda^{3/4}}\int_0^x f(y) \left(\sin (c_{\lambda}(x-y))\cosh (c_{\lambda}(x-y))-\cos (c_{\lambda}(x-y))\sinh (c_{\lambda}(x-y)) \right) \ dy$$ where $c_{\lambda}=\left(\frac{\lambda}{4}\right)^{1/4}.$ I'm unsure how to proceed from here. I think from the expression of $g,$ we must have $0\in \sigma(A)$ but I am not so sure since the integrand also vanishes at $\lambda=0.$ How do I proceed from here?",I have the following operator: I'm trying to find the spectrum of My attempt: Solving the equation with the given boundary condition gives: where I'm unsure how to proceed from here. I think from the expression of we must have but I am not so sure since the integrand also vanishes at How do I proceed from here?,"\begin{align*}D(A)&=\{f \in C[0,1]:f''''\in C[0,1] \text{ and } f^{(k)}(0)=0 \ \forall\ 0\leq k\leq 3\}\\Af&=-f''''.\end{align*} A. (\lambda - A) g =f g(x)=\frac{1}{4^{1/4}\lambda^{3/4}}\int_0^x f(y) \left(\sin (c_{\lambda}(x-y))\cosh (c_{\lambda}(x-y))-\cos (c_{\lambda}(x-y))\sinh (c_{\lambda}(x-y)) \right) \ dy c_{\lambda}=\left(\frac{\lambda}{4}\right)^{1/4}. g, 0\in \sigma(A) \lambda=0.","['functional-analysis', 'ordinary-differential-equations', 'spectral-theory', 'laplacian']"
70,How to solve the differential equation: yy''+2y'''=0 [closed],How to solve the differential equation: yy''+2y'''=0 [closed],,Closed . This question needs details or clarity . It is not currently accepting answers. Want to improve this question? Add details and clarify the problem by editing this post . Closed 4 years ago . Improve this question How do I solve yy''+2y'''=0 with B.Cs y(0)=y'(0)=0 & y'(∞)=1 ? Can somebody please hint at some substitution or refer any text related to these type of ode?,Closed . This question needs details or clarity . It is not currently accepting answers. Want to improve this question? Add details and clarify the problem by editing this post . Closed 4 years ago . Improve this question How do I solve yy''+2y'''=0 with B.Cs y(0)=y'(0)=0 & y'(∞)=1 ? Can somebody please hint at some substitution or refer any text related to these type of ode?,,['ordinary-differential-equations']
71,Solution of a homogeneous system is the null solution if it converges to $0$ for $x\to\pm\infty$,Solution of a homogeneous system is the null solution if it converges to  for,0 x\to\pm\infty,"Let $A\in\mathbb R^{d\times d}$ be diagonizable with eigenvalues $\lambda_1,\dots,\lambda_d\in\mathbb R$ . Show that: If $y\in C^1(\mathbb R\to\mathbb R^d)$ is a solution of the system $y'=Ay$ satisifying $\lim_{x\to\pm\infty} y(x)=0$ , then $y(x)\equiv 0$ . Thoughts : Let $v_1,\dots,v_d$ be eigenvectors of $A$ for the eigenvalues $\lambda_1,\dots,\lambda_d$ . If $\lambda_i\neq\lambda_j$ for $i\neq j$ , then $\{x\mapsto v_1\exp(\lambda_1x),\dots,x\mapsto v_d\exp(\lambda_dx)\}$ is a basis for the vector space of solutions. The condition $y(x)\to 0$ for $x\to\pm\infty$ implies that all components $y_i$ of $y$ must go to $0$ as well. These can be written as sums of exponential functions of the form $\exp(\lambda_i x)$ in each component, which implies that all coefficients have to be $0$ (using the linear independence of these functions) and thus $y(x)\equiv 0$ . Why exactly is this the case, and how can one show this in the more general case where $\lambda_i=\lambda_j$ is allowed?","Let be diagonizable with eigenvalues . Show that: If is a solution of the system satisifying , then . Thoughts : Let be eigenvectors of for the eigenvalues . If for , then is a basis for the vector space of solutions. The condition for implies that all components of must go to as well. These can be written as sums of exponential functions of the form in each component, which implies that all coefficients have to be (using the linear independence of these functions) and thus . Why exactly is this the case, and how can one show this in the more general case where is allowed?","A\in\mathbb R^{d\times d} \lambda_1,\dots,\lambda_d\in\mathbb R y\in C^1(\mathbb R\to\mathbb R^d) y'=Ay \lim_{x\to\pm\infty} y(x)=0 y(x)\equiv 0 v_1,\dots,v_d A \lambda_1,\dots,\lambda_d \lambda_i\neq\lambda_j i\neq j \{x\mapsto v_1\exp(\lambda_1x),\dots,x\mapsto v_d\exp(\lambda_dx)\} y(x)\to 0 x\to\pm\infty y_i y 0 \exp(\lambda_i x) 0 y(x)\equiv 0 \lambda_i=\lambda_j","['real-analysis', 'calculus', 'linear-algebra', 'ordinary-differential-equations']"
72,A matrix whose off-diagonal entries are $>0$ has its exponential with all positive entries,A matrix whose off-diagonal entries are  has its exponential with all positive entries,>0,"I am taking a course in differential equations and while I was doing some exercises I came across with the following statement: Let $A=(a_{ij})$ be a matrix such that $(a_{ij})>0$ , $i\neq j$ , then all entries of $e^A$ are positive. I've been trying to find some pattern in the exponents of the matrix $A$ but without success. I also have been thinking of $$.^T:\mathcal{M}_{n\times n}\to \mathbb{R}_{n\times n}$$ as an operator and try to prove that $e^A=e^{\frac{A}{2}}e^{\frac{A}{2}}$ and then proving that $x^TAx>0$ but I'm not sure if this is what I really want. Does $x^TAx>0$ implies that all entries of the matrix $A$ are positive? Or is there a more ""correct"" way of proving the statement? Thank you for your attention!","I am taking a course in differential equations and while I was doing some exercises I came across with the following statement: Let be a matrix such that , , then all entries of are positive. I've been trying to find some pattern in the exponents of the matrix but without success. I also have been thinking of as an operator and try to prove that and then proving that but I'm not sure if this is what I really want. Does implies that all entries of the matrix are positive? Or is there a more ""correct"" way of proving the statement? Thank you for your attention!",A=(a_{ij}) (a_{ij})>0 i\neq j e^A A .^T:\mathcal{M}_{n\times n}\to \mathbb{R}_{n\times n} e^A=e^{\frac{A}{2}}e^{\frac{A}{2}} x^TAx>0 x^TAx>0 A,"['linear-algebra', 'matrices', 'ordinary-differential-equations', 'matrix-exponential', 'positive-matrices']"
73,Solving ODE to find the unit tangent vector,Solving ODE to find the unit tangent vector,,"A parametric curve is described by the following equations $$\frac{dx}{dt} = x, y = \cos(t), z = \sin(t)$$ and passes through $(1,1,0)$ when $t = 0$ . By solving the ODE for $x(t)$ , or otherwise,   find an expression for $x$ in terms of $t$ and use this to write the   space curve as a vector function. Hence, find the unit tangent to the   curve $T(t)$ at the point $(1,1,0)$ . First I started by solving the differential equation and obtained $x=Ae^t$ , where $A$ is an arbitrary constant (so $A=e^c$ or $A=0$ or $A=-e^c$ ). And then I found that $T(0)$ is $$T(0)=\Bigl(\frac{A}{\sqrt{A^2+1}},0,\frac{1}{\sqrt {A^2+1}}\Bigl)$$ But I'm not sure if this is correct. I can't seem to find what the constant's value would be. I thought of putting the initial condition to be $x(0)=1$ but that doesn't seem right. Please somebody help!","A parametric curve is described by the following equations and passes through when . By solving the ODE for , or otherwise,   find an expression for in terms of and use this to write the   space curve as a vector function. Hence, find the unit tangent to the   curve at the point . First I started by solving the differential equation and obtained , where is an arbitrary constant (so or or ). And then I found that is But I'm not sure if this is correct. I can't seem to find what the constant's value would be. I thought of putting the initial condition to be but that doesn't seem right. Please somebody help!","\frac{dx}{dt} = x, y = \cos(t), z = \sin(t) (1,1,0) t = 0 x(t) x t T(t) (1,1,0) x=Ae^t A A=e^c A=0 A=-e^c T(0) T(0)=\Bigl(\frac{A}{\sqrt{A^2+1}},0,\frac{1}{\sqrt {A^2+1}}\Bigl) x(0)=1","['calculus', 'ordinary-differential-equations', 'parametric']"
74,Are differential-algebraic equations more expressive than ordinary differential equations?,Are differential-algebraic equations more expressive than ordinary differential equations?,,"I am interested in systems of differential-algebraic equations (DAE), i.e., systems of equations of the following form $$\dot{x} = f(x,y,t)\\0 = g(x,y,t)$$ I am confused about their relation to ordinary differential equations (ODE): Are there functions that can be described by DAEs but not by ODEs? I.e., functions that are a solution for some DAE but not for any ODE?","I am interested in systems of differential-algebraic equations (DAE), i.e., systems of equations of the following form I am confused about their relation to ordinary differential equations (ODE): Are there functions that can be described by DAEs but not by ODEs? I.e., functions that are a solution for some DAE but not for any ODE?","\dot{x} = f(x,y,t)\\0 = g(x,y,t)",['ordinary-differential-equations']
75,Solve $y''+y=p(x)$ where $p$ is a polynomial function,Solve  where  is a polynomial function,y''+y=p(x) p,"The equation is $$y'' + y=p(x)$$ Let $p(x)=\sum\limits_{k=0}^n a_k x^k$ The homogeneous solution to the equation is $y_h = a\cos(x)+b\sin(x)$ . I can look for a particular solution of the form $y_p=\sum\limits_{k=0}^n b_k x^k$ .  We get $$\sum\limits_{k=0}^{n-2} ((k+2)(k+1)b_{k+2}+b_k)x^k+b_{n-1}x^{n-1}+b_nx^n=\sum\limits_{k=0}^n a_kx^k$$ Hence, if $0\le k \le n-2$ , $(k+2)(k+1)b_{k+2}+b_k=a_k$ and $b_{n-1}=a_{n-1}$ , $b_n=a_n$ I'm trying to show that $y_p=\sum\limits_{n=0}^{\infty}(-1)^n p^{(2n)}(x)$ from here, how do I proceed?","The equation is Let The homogeneous solution to the equation is . I can look for a particular solution of the form .  We get Hence, if , and , I'm trying to show that from here, how do I proceed?",y'' + y=p(x) p(x)=\sum\limits_{k=0}^n a_k x^k y_h = a\cos(x)+b\sin(x) y_p=\sum\limits_{k=0}^n b_k x^k \sum\limits_{k=0}^{n-2} ((k+2)(k+1)b_{k+2}+b_k)x^k+b_{n-1}x^{n-1}+b_nx^n=\sum\limits_{k=0}^n a_kx^k 0\le k \le n-2 (k+2)(k+1)b_{k+2}+b_k=a_k b_{n-1}=a_{n-1} b_n=a_n y_p=\sum\limits_{n=0}^{\infty}(-1)^n p^{(2n)}(x),"['ordinary-differential-equations', 'recurrence-relations']"
76,undetermined coefficient method question for second-order (particular solution),undetermined coefficient method question for second-order (particular solution),,"Here are two second-order differential equations. $$ y''+9y=\sin(2t) \tag 1 $$ $$ y'' +4y =\sin(2t)  \tag 2 $$ I am told to use undetermine coefficients method to solve. For 1), I use $y_p=A \cos(2t)+B \sin(2t)$ to get $A=0$ and B= $\frac{1}{5}$ and get $y_p=\frac{1}{5} \sin(2t)$ For 2), I realize that that method doesn't work and told to do $y_p=t(A \cos(2t)+B \sin(2t)$ Why does it work then?","Here are two second-order differential equations. I am told to use undetermine coefficients method to solve. For 1), I use to get and B= and get For 2), I realize that that method doesn't work and told to do Why does it work then?", y''+9y=\sin(2t) \tag 1   y'' +4y =\sin(2t)  \tag 2  y_p=A \cos(2t)+B \sin(2t) A=0 \frac{1}{5} y_p=\frac{1}{5} \sin(2t) y_p=t(A \cos(2t)+B \sin(2t),['ordinary-differential-equations']
77,How to solve a differential equation $(e^x+2\ln y)ydx+xdy=0$,How to solve a differential equation,(e^x+2\ln y)ydx+xdy=0,"Solve the following differential equation: $$ (e^x+2\ln y)ydx+xdy=0 $$ It is clear that the equation is not exact. So, I tried to express $y'$ : $$ \begin{aligned} e^xy+2y\ln y+xy'=0\iff \begin{cases} \left[  \begin{aligned} &y\equiv0\\ &y\equiv e^{-{1/2}} \end{aligned} \right. \ \ \text{if}\ \ x=0\\ y'=-\frac{e^x}{x}\cdot y-\frac{1}{x}\cdot2y\ln y\ \ \ \text{otherwise} \end{cases} \end{aligned} $$ The problem is that the differential equation seems to be non-linear, and I don't know the ways of solving those. Maybe there's an easier way of solving the initial differential equation?","Solve the following differential equation: It is clear that the equation is not exact. So, I tried to express : The problem is that the differential equation seems to be non-linear, and I don't know the ways of solving those. Maybe there's an easier way of solving the initial differential equation?","
(e^x+2\ln y)ydx+xdy=0
 y' 
\begin{aligned}
e^xy+2y\ln y+xy'=0\iff
\begin{cases}
\left[ 
\begin{aligned}
&y\equiv0\\
&y\equiv e^{-{1/2}}
\end{aligned}
\right. \ \ \text{if}\ \ x=0\\
y'=-\frac{e^x}{x}\cdot y-\frac{1}{x}\cdot2y\ln y\ \ \ \text{otherwise}
\end{cases}
\end{aligned}
","['calculus', 'ordinary-differential-equations']"
78,"$f(a)=f(b), f'(a)=f'(b)$, Existence of zeros of $f''(x)-\lambda (f'(x))^2 =0$ in $(a, b)$",", Existence of zeros of  in","f(a)=f(b), f'(a)=f'(b) f''(x)-\lambda (f'(x))^2 =0 (a, b)","Question: $f:[a, b]\to\mathbb R $ is a function, which is contiunous and twice differentiable. If $f(a)=f(b)$ and $f'(a)=f'(b)$ , for $\forall\lambda\in\mathbb R$ , show that there exists at least one zero of the equation $$f''(x)-\lambda (f'(x))^2 =0$$ in the interval (a, b). I first wanted to use the fact that $$\exists c_1\space\space s.t\space\space \frac{f(a)-f(b)}{a-b}=f'(c_1)=0$$ $$\exists c_2\space\space s.t\space\space \frac{f'(a)-f'(b)}{a-b}=f''(c_2)=0$$ However, I still do not know how these facts can be applied to the differential equation. Also, I multiplied LHS and RHS with $e^{\lambda x}$ but nothing happened. Could you please give me some key ideas about this problem? Thanks for answering.","Question: is a function, which is contiunous and twice differentiable. If and , for , show that there exists at least one zero of the equation in the interval (a, b). I first wanted to use the fact that However, I still do not know how these facts can be applied to the differential equation. Also, I multiplied LHS and RHS with but nothing happened. Could you please give me some key ideas about this problem? Thanks for answering.","f:[a, b]\to\mathbb R  f(a)=f(b) f'(a)=f'(b) \forall\lambda\in\mathbb R f''(x)-\lambda (f'(x))^2 =0 \exists c_1\space\space s.t\space\space \frac{f(a)-f(b)}{a-b}=f'(c_1)=0 \exists c_2\space\space s.t\space\space \frac{f'(a)-f'(b)}{a-b}=f''(c_2)=0 e^{\lambda x}","['calculus', 'ordinary-differential-equations', 'derivatives']"
79,Error in Gilbert Strang Differential Equations 2.1 Fundamental Solution to 2nd Order Differential Equation?,Error in Gilbert Strang Differential Equations 2.1 Fundamental Solution to 2nd Order Differential Equation?,,"In Strang's Differential Equations and Linear Algebra book, Section 2.1, he introduces the fundamental solution / impulse response $g(t)$ to the delta forcing function: $$m g'' + kg = \delta(t) $$ with given initial conditions $g(0) = 0$ and $g'(0) = 0$ . He then says this implies that $g'(0) = 1/m$ , since $$ m g''(0) + k g(0) = \delta(t) \rightarrow m g''(0) = \delta(t) \rightarrow m g'(0) = 1 \rightarrow g'(0) = \frac{1}{m}$$ I'm confused how this doesn't contradict the initial given condition $g'(0) = 0$ . Could someone please clarify? Edit 1: Maybe I'm misunderstanding the text. I don't find Strang easy to follow. Photo below:","In Strang's Differential Equations and Linear Algebra book, Section 2.1, he introduces the fundamental solution / impulse response to the delta forcing function: with given initial conditions and . He then says this implies that , since I'm confused how this doesn't contradict the initial given condition . Could someone please clarify? Edit 1: Maybe I'm misunderstanding the text. I don't find Strang easy to follow. Photo below:",g(t) m g'' + kg = \delta(t)  g(0) = 0 g'(0) = 0 g'(0) = 1/m  m g''(0) + k g(0) = \delta(t) \rightarrow m g''(0) = \delta(t) \rightarrow m g'(0) = 1 \rightarrow g'(0) = \frac{1}{m} g'(0) = 0,['ordinary-differential-equations']
80,A Sine Wave Where Alternate Distances Between 'Wave-center' Points Are Powers of φ,A Sine Wave Where Alternate Distances Between 'Wave-center' Points Are Powers of φ,,"This may be hard to visualize without my graph, see here If $\phi=\left(\frac{1+5^{1/2}}{2}\right), \alpha=\phi^{-2}, \beta=1$ , then the parametric equations, $$ (x, y)=\left(\sin(t)\cdot\left(\alpha\cdot\phi^{t-{\pi/2}/\pi}+\beta-\frac{\alpha}{\phi^{1/2}}\right)^{-1},\; \left(\alpha\cdot\phi^{t-{\pi/2}/\pi}+\beta-\frac{\alpha}{\phi^{1/2}}\right)\right) $$ produce a graph where the vertical distances between points of tangency with $x·y=±1$ on alternate sides are powers of $\phi$ . (when only positive numbers are graphed, starting at $\phi^{-1}$ and proceeding as follows: $\phi^{-1}, \phi^{0}, \phi^{1}, \phi^{2}, \phi^{3}$ ). To get some context on why the above is the case, see the update to the answer at this link: https://math.stackexchange.com/a/3515756/708680 I would like to slightly reformulate the above expression so that instead of the aforementioned distances between points of tangency being powers of $\phi$ starting at $\phi^{-1}$ (for positive numbers) and increasing by powers of $\phi$ on alternate sides, the distance between 'wave-center' points are powers of $\phi$ starting at $\phi^{-1}$ (for positive numbers) and increasing by powers of $\phi$ on alternate sides, instead. Here 'wave-center' points are defined as points on the wave whose $y$ is half of the distance between any point where the curve crosses $y$ and the closest next point of crossing to that point. See here for a graph showing the expression, points of tangency, 'wave-center' points, etc..., ['Wave-center' points are in red] ( Please note that the 'wave-center' points in my graph are not the ones I want for the new expression but are instead just to show what I mean by 'wave-center' points ): https://www.desmos.com/calculator/v7pmwr5oj9 I want the adjusted parametric equations to retain the following while being altered in the aforementioned manner; they should: 0 . Be of the form: $(x,y)=\big(f(t)^{-1}\cdot\sin(t), f(t)\big)$ . For the sake of clarity I add that, for the original equations, this $f(t)$ was in the form $$ f(t)=\left(\alpha\cdot\phi^{t-{\pi/2}/\pi}+\beta-\frac{\alpha}{\phi^{1/2}}\right). $$ 1. Start at $(0, 1)$ for positive and negative numbers. 2. Have points of tangency to $x\cdot y=±1$ (as a result of 0. ). 3. Maintain a smooth, sinusoidal, 2-D spiral nature. 4. Be written in terms of $\sin(t)$ . Thanks for your help.","This may be hard to visualize without my graph, see here If , then the parametric equations, produce a graph where the vertical distances between points of tangency with on alternate sides are powers of . (when only positive numbers are graphed, starting at and proceeding as follows: ). To get some context on why the above is the case, see the update to the answer at this link: https://math.stackexchange.com/a/3515756/708680 I would like to slightly reformulate the above expression so that instead of the aforementioned distances between points of tangency being powers of starting at (for positive numbers) and increasing by powers of on alternate sides, the distance between 'wave-center' points are powers of starting at (for positive numbers) and increasing by powers of on alternate sides, instead. Here 'wave-center' points are defined as points on the wave whose is half of the distance between any point where the curve crosses and the closest next point of crossing to that point. See here for a graph showing the expression, points of tangency, 'wave-center' points, etc..., ['Wave-center' points are in red] ( Please note that the 'wave-center' points in my graph are not the ones I want for the new expression but are instead just to show what I mean by 'wave-center' points ): https://www.desmos.com/calculator/v7pmwr5oj9 I want the adjusted parametric equations to retain the following while being altered in the aforementioned manner; they should: 0 . Be of the form: . For the sake of clarity I add that, for the original equations, this was in the form 1. Start at for positive and negative numbers. 2. Have points of tangency to (as a result of 0. ). 3. Maintain a smooth, sinusoidal, 2-D spiral nature. 4. Be written in terms of . Thanks for your help.","\phi=\left(\frac{1+5^{1/2}}{2}\right), \alpha=\phi^{-2}, \beta=1 
(x, y)=\left(\sin(t)\cdot\left(\alpha\cdot\phi^{t-{\pi/2}/\pi}+\beta-\frac{\alpha}{\phi^{1/2}}\right)^{-1},\; \left(\alpha\cdot\phi^{t-{\pi/2}/\pi}+\beta-\frac{\alpha}{\phi^{1/2}}\right)\right)
 x·y=±1 \phi \phi^{-1} \phi^{-1}, \phi^{0}, \phi^{1}, \phi^{2}, \phi^{3} \phi \phi^{-1} \phi \phi \phi^{-1} \phi y y (x,y)=\big(f(t)^{-1}\cdot\sin(t), f(t)\big) f(t) 
f(t)=\left(\alpha\cdot\phi^{t-{\pi/2}/\pi}+\beta-\frac{\alpha}{\phi^{1/2}}\right).
 (0, 1) x\cdot y=±1 \sin(t)","['calculus', 'ordinary-differential-equations', 'trigonometry', 'wave-equation', 'golden-ratio']"
81,Inhomogeneous Cauchy-Euler equation: $x^2y''(x)+xy'(x)-y(x)=x$ for $x>0$,Inhomogeneous Cauchy-Euler equation:  for,x^2y''(x)+xy'(x)-y(x)=x x>0,"I am trying to solve this DE (for $x>0$ ) by a substitution $x=e^t$ : $$x^2y''(x)+xy'(x)-y(x)=x \implies \color{blue}{e^{2t}y''(e^t)+e^ty'(e^t)}-\color{green}{y(e^t)}=e^t$$ Let $z(t)=y(x)$ and let $x=e^t \implies \color{green}{z(t)}= y(e^t)$ By the chain rule we get: $z'(t)=y'(e^t)e^t$ $\color{blue}{z''(t)}=y''(e^t)e^{2t}+y'(e^t)e^{t}$ The green and blue part can be substituted into the equation and I get: $$z''(t)-z(t)=e^t$$ The solution to the homogeneous equation $z''-z=0$ is: $$z_h(t)=c_1 e^t+c_2e^{-t}$$ By choosing a suitible ""ansatz"" $z_s(t)=Ate^t$ , I get the special solution to the inhomogeneous equation $z''-z=e^t$ : $$z_s(t)=\frac{1}{2}te^t$$ Therefore the general solution to the inhomogeneous equation is: $$z(t)=c_1 e^t+c_2e^{-t}+\frac{1}{2}te^t$$ Because I made the substitution $x=e^t$ I need to substitute $t=\ln{x}$ $$\implies y(x)=c_1e^{\ln{x}}+c_2e^{-\ln{x}}+\frac{1}{2}(\ln{x})e^{\ln{x}}\\=c_1 x+\frac{c_2}{x}+\color{red}{\frac{1}{2}x\ln{x}}$$ Here is my problem: The red part $\frac{1}{2}x\ln{x}$ doesn't solve the differential equation and I am not sure where I have gone wrong. Wolfram alpha also gives me the same (real) solution the homogeneous equation but the general solution looks like this: Where am I making a mistake?","I am trying to solve this DE (for ) by a substitution : Let and let By the chain rule we get: The green and blue part can be substituted into the equation and I get: The solution to the homogeneous equation is: By choosing a suitible ""ansatz"" , I get the special solution to the inhomogeneous equation : Therefore the general solution to the inhomogeneous equation is: Because I made the substitution I need to substitute Here is my problem: The red part doesn't solve the differential equation and I am not sure where I have gone wrong. Wolfram alpha also gives me the same (real) solution the homogeneous equation but the general solution looks like this: Where am I making a mistake?",x>0 x=e^t x^2y''(x)+xy'(x)-y(x)=x \implies \color{blue}{e^{2t}y''(e^t)+e^ty'(e^t)}-\color{green}{y(e^t)}=e^t z(t)=y(x) x=e^t \implies \color{green}{z(t)}= y(e^t) z'(t)=y'(e^t)e^t \color{blue}{z''(t)}=y''(e^t)e^{2t}+y'(e^t)e^{t} z''(t)-z(t)=e^t z''-z=0 z_h(t)=c_1 e^t+c_2e^{-t} z_s(t)=Ate^t z''-z=e^t z_s(t)=\frac{1}{2}te^t z(t)=c_1 e^t+c_2e^{-t}+\frac{1}{2}te^t x=e^t t=\ln{x} \implies y(x)=c_1e^{\ln{x}}+c_2e^{-\ln{x}}+\frac{1}{2}(\ln{x})e^{\ln{x}}\\=c_1 x+\frac{c_2}{x}+\color{red}{\frac{1}{2}x\ln{x}} \frac{1}{2}x\ln{x},['ordinary-differential-equations']
82,Finite Difference Method for the second order ODE $y''=\frac{y}{y+1}$,Finite Difference Method for the second order ODE,y''=\frac{y}{y+1},"I would like to solve a non-linear second order ODE for $y \in (0,1)$ using a numerical method, preferably finite differences. The ODE is $$ \frac{d^2 y}{d x^2} = \frac{y}{y+1}, \quad y(0)=\alpha, \ y(1)=\beta. $$ I understand that a relaxation method can be used. I have worked out the iteration process to be $$y_i =  \frac{1}{2}(y_{i-1}+y_{i+1}) + \frac{1}{h^2}y_i - y_i(y_{i-1}-2y_i+y_{i+1}),$$ for a mesh step $h$ , but I'm unsure as to whether this is an efficient or incorrect method for this particular problem. A solution need not follow this particular method, any will do!","I would like to solve a non-linear second order ODE for using a numerical method, preferably finite differences. The ODE is I understand that a relaxation method can be used. I have worked out the iteration process to be for a mesh step , but I'm unsure as to whether this is an efficient or incorrect method for this particular problem. A solution need not follow this particular method, any will do!","y \in (0,1) 
\frac{d^2 y}{d x^2} = \frac{y}{y+1}, \quad y(0)=\alpha, \ y(1)=\beta.
 y_i =  \frac{1}{2}(y_{i-1}+y_{i+1}) + \frac{1}{h^2}y_i - y_i(y_{i-1}-2y_i+y_{i+1}), h","['ordinary-differential-equations', 'numerical-methods', 'boundary-value-problem', 'numerical-calculus', 'finite-difference-methods']"
83,Field lines as solutions to differential equation,Field lines as solutions to differential equation,,"Instead of using arrows to represent a planar vector field, one sometimes uses families of curves called field lines. A curve $y = y(x)$ is a field line of the vector field $F(x, y)$ if at each point $(x_0, y_0)$ on the curve, $F(x_0, y_0)$ is tangent to the curve. Show that the field lines $y = y(x)$ of a vector field $F(x,y) = P(x,y)i + Q(x,y)j$ are solutions to the differential equation $dy/dx = Q/P$ . Find the field lines of $F(x, y) = yi + xj$ .","Instead of using arrows to represent a planar vector field, one sometimes uses families of curves called field lines. A curve is a field line of the vector field if at each point on the curve, is tangent to the curve. Show that the field lines of a vector field are solutions to the differential equation . Find the field lines of .","y = y(x) F(x, y) (x_0, y_0) F(x_0, y_0) y = y(x) F(x,y) = P(x,y)i + Q(x,y)j dy/dx = Q/P F(x, y) = yi + xj","['calculus', 'ordinary-differential-equations', 'vectors', 'vector-analysis', 'vector-fields']"
84,Why are we allowed to divide by $x$ in the following differential equation?,Why are we allowed to divide by  in the following differential equation?,x,"Suppose we have a second order linear differential equation $x\frac{d^2y}{dx^2}+\frac{dy}{dx}-xy=xe^x$ .Often we divide both sides by $x$ to get $\frac{d^2y}{dx^2}+\frac{1}{x}\frac{dy}{dx}-y=e^x$ ,and then proceed towards solution.Why does this make no harm as far as the solution of the differential equation is considered.We were trying to find $y=\phi(x)$ whose domain is $\mathbb R$ .But,now we are finding a solution $y=\psi(x)$ whose domain is $\mathbb R-\{0\}$ .Of course it is clear that the family of solutions $\phi(x)$ and $\psi(x)$ are equal except one has $0$ in domain and other does not ,I mean we can extend the other one to $\mathbb R$ by assigning some value at $0$ so that the extension is continuous.Does this make sense?Is this why we can divide by $0$ without worrying about losing anything?","Suppose we have a second order linear differential equation .Often we divide both sides by to get ,and then proceed towards solution.Why does this make no harm as far as the solution of the differential equation is considered.We were trying to find whose domain is .But,now we are finding a solution whose domain is .Of course it is clear that the family of solutions and are equal except one has in domain and other does not ,I mean we can extend the other one to by assigning some value at so that the extension is continuous.Does this make sense?Is this why we can divide by without worrying about losing anything?",x\frac{d^2y}{dx^2}+\frac{dy}{dx}-xy=xe^x x \frac{d^2y}{dx^2}+\frac{1}{x}\frac{dy}{dx}-y=e^x y=\phi(x) \mathbb R y=\psi(x) \mathbb R-\{0\} \phi(x) \psi(x) 0 \mathbb R 0 0,"['real-analysis', 'ordinary-differential-equations', 'multivariable-calculus', 'partial-differential-equations', 'singularity']"
85,Differential equations and linearly independent solutions,Differential equations and linearly independent solutions,,"If $$y_1(x)=\sin 2x$$ and $$y_2(x)=\cos 2x$$ are two solutions of $$y^{,,}+4y=0,$$ show that $y_1(x)$ and $y_2(x)$ are linearly independent solutions. I think to prove linearly independent,the equation will be $$c_1\sin 2x+c_2\cos 2x=0$$ ,where $c_1$ and $c_2$ have to be equal to zero,but how to prove that??,please help me..","If and are two solutions of show that and are linearly independent solutions. I think to prove linearly independent,the equation will be ,where and have to be equal to zero,but how to prove that??,please help me..","y_1(x)=\sin 2x y_2(x)=\cos 2x y^{,,}+4y=0, y_1(x) y_2(x) c_1\sin 2x+c_2\cos 2x=0 c_1 c_2",['ordinary-differential-equations']
86,"Solve the initial value problem $xy'(x) = y, y(0)=0,x≥0$",Solve the initial value problem,"xy'(x) = y, y(0)=0,x≥0","Solve the initial value problem $xy'(x) = y, y(0)=0,x≥0$ My attempt: $\frac{dy}{y} = \frac{dx}{x}$ $ \text{ln}y = \text{ln}x + c$ ..... $(1)$ Now $c$ can't be find because $0$ is not in domain of log function, so there should be no solution. Or in other way- Equation $(1)$ can be written as $y = c_{1} x$ No we can have uncountable solutions. Which one is the right method $?$","Solve the initial value problem My attempt: ..... Now can't be find because is not in domain of log function, so there should be no solution. Or in other way- Equation can be written as No we can have uncountable solutions. Which one is the right method","xy'(x) = y, y(0)=0,x≥0 \frac{dy}{y} = \frac{dx}{x}  \text{ln}y = \text{ln}x + c (1) c 0 (1) y = c_{1} x ?",['ordinary-differential-equations']
87,alternative equivalent of mass spring ODE with a dirac delta stimulus,alternative equivalent of mass spring ODE with a dirac delta stimulus,,"A mass spring system ODE with a dirac delta stimulus reads: $$mx’’+cx’+kx=\delta(t)$$ $$x(0)=0, x’(0)=0$$ where ’‚’ denotes t derivative. Textbooks usually say, according to momentum theorem, the ODE can be rewritten as: $$mx’’+cx’+kx=0, t>0$$ $$x(0+)=0, x’(0+)=\frac 1m$$ However, momentum theorem I think is just one way of treating integration here. In theory, one can always integrate mx’’+cx’+kx=dirac(t) on both sides w.r.t t within (0-,0+). Then momentum theorem chooses to keep the integral of mx’’ dropping the latter 2 integrals. What if instead keep the integral of cx’ dropping the other 2 integrals? Then the ODE will become: $$mx’’+cx’+kx=0, t>0$$ $$x(0+)=\frac 1 c, x’(0+)=0$$ However, physically this gives a different solution/motion than the original ODE! Where does the error come from?","A mass spring system ODE with a dirac delta stimulus reads: where ’‚’ denotes t derivative. Textbooks usually say, according to momentum theorem, the ODE can be rewritten as: However, momentum theorem I think is just one way of treating integration here. In theory, one can always integrate mx’’+cx’+kx=dirac(t) on both sides w.r.t t within (0-,0+). Then momentum theorem chooses to keep the integral of mx’’ dropping the latter 2 integrals. What if instead keep the integral of cx’ dropping the other 2 integrals? Then the ODE will become: However, physically this gives a different solution/motion than the original ODE! Where does the error come from?","mx’’+cx’+kx=\delta(t) x(0)=0, x’(0)=0 mx’’+cx’+kx=0, t>0 x(0+)=0, x’(0+)=\frac 1m mx’’+cx’+kx=0, t>0 x(0+)=\frac 1 c, x’(0+)=0",['ordinary-differential-equations']
88,Solve $x^2(xdx+ydy)+2y(xdy-ydx)=0$,Solve,x^2(xdx+ydy)+2y(xdy-ydx)=0,"Solve $x^2(xdx+ydy)+2y(xdy-ydx)=0$ My Attempts $$x^2(xdx+ydy)+2y(xdy-ydx)=0$$ $$\dfrac {xdx+ydy}{xdy-ydx}=-\dfrac {2y}{x}$$ Put $x=r\cos (\theta)$ and $y=r\sin (\theta)$ So, $r^2=x^2+y^2$ and $\tan (\theta)=\dfrac {y}{x}$ Now, $$x^2+y^2=r^2$$ Differentiating both sides, $$2xdx+2ydy=2rdr$$ $$xdx+ydy=rdr$$","Solve My Attempts Put and So, and Now, Differentiating both sides,",x^2(xdx+ydy)+2y(xdy-ydx)=0 x^2(xdx+ydy)+2y(xdy-ydx)=0 \dfrac {xdx+ydy}{xdy-ydx}=-\dfrac {2y}{x} x=r\cos (\theta) y=r\sin (\theta) r^2=x^2+y^2 \tan (\theta)=\dfrac {y}{x} x^2+y^2=r^2 2xdx+2ydy=2rdr xdx+ydy=rdr,"['calculus', 'ordinary-differential-equations']"
89,How to understand the concept of the Picard mapping?,How to understand the concept of the Picard mapping?,,"The texts below are from Ordinary Differential Equations by Arnol'd Consider the differential equation $\dot{\boldsymbol{x}} =  \boldsymbol{v}(t,\boldsymbol{x})$ defined by a vector field $\boldsymbol{v}$ in some domain of the extended phase space $\mathbf{R}^{n+1}$ . We define the Picard mapping to be the mapping $A$ that takes the function $\varphi:t \to \boldsymbol{x}$ to the    function $A \phi: t \to \boldsymbol{x}$ where $$ (A \varphi)(t) =  \boldsymbol{x}_0+ \int_{t_0}^t \boldsymbol{v}(\tau,\varphi(\tau))d\tau $$ And the book says the following I can't understand this geometrical representation at all. How does the tangent for each $t$ be parallel to $\varphi$ ? What does it mean "" for then $A \varphi$ would be a solution ""? A solution of what? Any help is appreciated!!","The texts below are from Ordinary Differential Equations by Arnol'd Consider the differential equation defined by a vector field in some domain of the extended phase space . We define the Picard mapping to be the mapping that takes the function to the    function where And the book says the following I can't understand this geometrical representation at all. How does the tangent for each be parallel to ? What does it mean "" for then would be a solution ""? A solution of what? Any help is appreciated!!","\dot{\boldsymbol{x}} =
 \boldsymbol{v}(t,\boldsymbol{x}) \boldsymbol{v} \mathbf{R}^{n+1} A \varphi:t \to \boldsymbol{x} A \phi: t \to \boldsymbol{x}  (A \varphi)(t) =
 \boldsymbol{x}_0+ \int_{t_0}^t \boldsymbol{v}(\tau,\varphi(\tau))d\tau  t \varphi A \varphi",['ordinary-differential-equations']
90,Can't find the solution to a quasilinear PDE $y{u_{x}}$ + $x{u_{y}}$ = 0,Can't find the solution to a quasilinear PDE  +  = 0,y{u_{x}} x{u_{y}},"I get that $x'' - x = 0$ and similar for $y$ , and that I need to solve these ODEs to start. This means that I get $x = Ae^t + Be^t$ and $y = Ae^t - Be^t$ , however,  the solution I have been given gives these to be $x = A \cosh(t) + B \sinh(t)$ and $y = A \sinh(t) - B \cosh(t)$ . I don't get how these have come out of the hyperbolic expressions since $\sinh(x) = \dfrac{e^{x} - e^{-x}}{2}$ and $\cosh(x) = \dfrac{e^{x} = e^{-x}}{2}$ Can someone explain how these terms have come from the quasilinear PDE? I know how to solve the rest, I just don't get how these terms are coming out.","I get that and similar for , and that I need to solve these ODEs to start. This means that I get and , however,  the solution I have been given gives these to be and . I don't get how these have come out of the hyperbolic expressions since and Can someone explain how these terms have come from the quasilinear PDE? I know how to solve the rest, I just don't get how these terms are coming out.",x'' - x = 0 y x = Ae^t + Be^t y = Ae^t - Be^t x = A \cosh(t) + B \sinh(t) y = A \sinh(t) - B \cosh(t) \sinh(x) = \dfrac{e^{x} - e^{-x}}{2} \cosh(x) = \dfrac{e^{x} = e^{-x}}{2},"['ordinary-differential-equations', 'partial-differential-equations']"
91,Asymptotes of the integral curve,Asymptotes of the integral curve,,"One of the first tasks in differential equation problem book. Obviously that, it’s separable equation, but integrals can’t be computed. The task is to proof that integral curve have two assymptoties","One of the first tasks in differential equation problem book. Obviously that, it’s separable equation, but integrals can’t be computed. The task is to proof that integral curve have two assymptoties",,"['real-analysis', 'calculus', 'integration', 'ordinary-differential-equations', 'asymptotics']"
92,"Solving a nonlinear, first order ODE with a trig function","Solving a nonlinear, first order ODE with a trig function",,"I have an equation of the form, $$ \frac{dy}{dx} = A(x) + B(x) \sin^2(y)$$ How would I go about solving this? Obviously were $A(x) = 0$ then this would be separable and all would be OK, however the presence of the $A(x)$ term is confusing me. Thanks in advance for any guidance","I have an equation of the form, How would I go about solving this? Obviously were then this would be separable and all would be OK, however the presence of the term is confusing me. Thanks in advance for any guidance", \frac{dy}{dx} = A(x) + B(x) \sin^2(y) A(x) = 0 A(x),"['ordinary-differential-equations', 'nonlinear-system']"
93,How to draw the trajectories of the solutions in the canonical basis?,How to draw the trajectories of the solutions in the canonical basis?,,"I'm trying to understand the steps to solve this exercise: Consider the following differential system $$\begin{cases}{x'=-2 x-y} \\ {y'=3 x+y}\end{cases}$$ Draw the trajectories of the solutions in the canonical basis. Here is the solution given by my professor: In the basis $\left \{\begin{pmatrix}2 \\ -3 \\ \end{pmatrix}, \begin{pmatrix}0 \\ \sqrt 3 \\ \end{pmatrix} \right \}$ , we have to solve $$\begin{bmatrix}f' \\ g' \\ \end{bmatrix} = \begin{bmatrix} -\dfrac{1}{2} & \dfrac{\sqrt 3}{2} \\ -\dfrac{\sqrt 3}{2} & -\dfrac{1}{2} \\ \end{bmatrix} \begin{bmatrix}f \\ g \\ \end{bmatrix}$$ Setting $z = f + i g$ , we have to solve $$z' = \left  (-\frac{1}{2} - i \frac{\sqrt 3}{2} \right)z$$ We have $$\begin{bmatrix}x' \\ y' \\ \end{bmatrix} = \begin{bmatrix} -2 & -1 \\ 3 & 1 \\ \end{bmatrix} \begin{bmatrix}x \\ y \\ \end{bmatrix}$$ The characteristic polynomial has two complex roots $$\lambda = -\frac{1}{2} \pm i \frac{\sqrt 3}{2}$$ Could you please explain how to come up with the basis $\left \{\begin{pmatrix}2 \\ -3 \\ \end{pmatrix}, \begin{pmatrix}0 \\ \sqrt 3 \\ \end{pmatrix} \right \}$ ? Thank you so much for your help! Update: I include the screenshot of solution.","I'm trying to understand the steps to solve this exercise: Consider the following differential system Draw the trajectories of the solutions in the canonical basis. Here is the solution given by my professor: In the basis , we have to solve Setting , we have to solve We have The characteristic polynomial has two complex roots Could you please explain how to come up with the basis ? Thank you so much for your help! Update: I include the screenshot of solution.","\begin{cases}{x'=-2 x-y} \\ {y'=3 x+y}\end{cases} \left \{\begin{pmatrix}2 \\ -3 \\ \end{pmatrix}, \begin{pmatrix}0 \\ \sqrt 3 \\ \end{pmatrix} \right \} \begin{bmatrix}f' \\ g' \\ \end{bmatrix} = \begin{bmatrix} -\dfrac{1}{2} & \dfrac{\sqrt 3}{2} \\ -\dfrac{\sqrt 3}{2} & -\dfrac{1}{2} \\ \end{bmatrix} \begin{bmatrix}f \\ g \\ \end{bmatrix} z = f + i g z' = \left  (-\frac{1}{2} - i \frac{\sqrt 3}{2} \right)z \begin{bmatrix}x' \\ y' \\ \end{bmatrix} = \begin{bmatrix} -2 & -1 \\ 3 & 1 \\ \end{bmatrix} \begin{bmatrix}x \\ y \\ \end{bmatrix} \lambda = -\frac{1}{2} \pm i \frac{\sqrt 3}{2} \left \{\begin{pmatrix}2 \\ -3 \\ \end{pmatrix}, \begin{pmatrix}0 \\ \sqrt 3 \\ \end{pmatrix} \right \}","['linear-algebra', 'ordinary-differential-equations', 'eigenvalues-eigenvectors', 'proof-explanation']"
94,Solve $x^3+3y-xy^\prime=0$,Solve,x^3+3y-xy^\prime=0,Solve $x^3+3y-xy^\prime=0$ Divided by $x$ to get $x^2+3\frac{y}{x}-y^\prime=0$ Let $v=\frac{y}{x}$ then $y^\prime=v^\prime x+v$ then $x^2+3v=v^\prime x+v$ then $v^\prime=x+2\frac{v}{x}$ Used integrating factor $e^{\int -2/x dx}=e^{-x^2}$ then $e^{-x^2}v=\int e^{-x^2}\cdot x dx$ But when I solve this $e^{-x^2}v=\frac{-1}{2}e^{-x^2}$ $v=\frac{-1}{2}$ But this isn't the right answer.,Solve Divided by to get Let then then then Used integrating factor then But when I solve this But this isn't the right answer.,x^3+3y-xy^\prime=0 x x^2+3\frac{y}{x}-y^\prime=0 v=\frac{y}{x} y^\prime=v^\prime x+v x^2+3v=v^\prime x+v v^\prime=x+2\frac{v}{x} e^{\int -2/x dx}=e^{-x^2} e^{-x^2}v=\int e^{-x^2}\cdot x dx e^{-x^2}v=\frac{-1}{2}e^{-x^2} v=\frac{-1}{2},['ordinary-differential-equations']
95,particular solution of $(D^2+4)y=4x^2\cos 2x$,particular solution of,(D^2+4)y=4x^2\cos 2x,Find a particular solution to the equation $$(D^2+4)y=4x^2\cos 2x$$ \begin{align} y_p&=\frac{1}{D^2+4}(4x^2\cos 2x)\\ &=\frac{1}{D^2+4}\left[2x^2(e^{2ix}+e^{-2ix})\right]\\ &=2\left[e^{2ix}\frac{1}{(D+2i)^2+4}x^2+e^{-2ix}\frac{1}{(D-2i)^2+4}x^2\right]\\ &=2\left[e^{2ix}\frac{1}{D(D+4i)}x^2+e^{-2ix}\frac{1}{D(D-4i)}x^2\right]\\ \end{align} But I stuck at this point because how to solve $\frac{1}{D(D+4i)}x^2?$ The solution provided this \begin{align} &=2\left[e^{2ix}\frac{1}{D(D+4i)}x^2+e^{-2ix}\frac{1}{D(D-4i)}x^2\right]\\&=\frac{1}{4i}\left[e^{2ix}D^{-1}\left(2x^2+ix-\frac{1}{4}\right)-e^{-2ix}D^{-1}\left(2x^2-ix-\frac{1}{4}\right)\right]\\ &=\frac{1}{24}[6x^2\cos 2x+x(8x^2-3)\sin 2x] \end{align} I still can't figure out how they do this. Any help will be appreciated. Update : Using @Isham answer I tired \begin{align} &=\frac{1}{4i}\left[e^{2ix}D^{-1}\left(2x^2+ix-\frac{1}{4}\right)-e^{-2ix}D^{-1}\left(2x^2-ix-\frac{1}{4}\right)\right]\\ &=\frac{1}{4i}\left[e^{2ix}\left(2\frac{x^3}{3}+i\frac{x^2}{2}-\frac{x}{4}\right)-e^{-2ix}\left(2\frac{x^3}{3}-i\frac{x^2}{2}-\frac{x}{4}\right)\right]\\ &=\frac{1}{4i}\left[\frac{2x^3}{3}(e^{2ix}-e^{-2ix})+\frac{x^2i}{2}(e^{2ix}-e^{-2ix})-\frac{x}{4}(e^{2ix}-e^{-2ix})\right]\\ &=\frac{1}{4i}\left[\frac{2x^3}{3}2\cos 2x+\frac{x^2i}{2}2\cos 2x-\frac{x}{4}2\cos 2x\right] \end{align} But I think I am again Lost.,Find a particular solution to the equation But I stuck at this point because how to solve The solution provided this I still can't figure out how they do this. Any help will be appreciated. Update : Using @Isham answer I tired But I think I am again Lost.,"(D^2+4)y=4x^2\cos 2x \begin{align}
y_p&=\frac{1}{D^2+4}(4x^2\cos 2x)\\
&=\frac{1}{D^2+4}\left[2x^2(e^{2ix}+e^{-2ix})\right]\\
&=2\left[e^{2ix}\frac{1}{(D+2i)^2+4}x^2+e^{-2ix}\frac{1}{(D-2i)^2+4}x^2\right]\\
&=2\left[e^{2ix}\frac{1}{D(D+4i)}x^2+e^{-2ix}\frac{1}{D(D-4i)}x^2\right]\\
\end{align} \frac{1}{D(D+4i)}x^2? \begin{align}
&=2\left[e^{2ix}\frac{1}{D(D+4i)}x^2+e^{-2ix}\frac{1}{D(D-4i)}x^2\right]\\&=\frac{1}{4i}\left[e^{2ix}D^{-1}\left(2x^2+ix-\frac{1}{4}\right)-e^{-2ix}D^{-1}\left(2x^2-ix-\frac{1}{4}\right)\right]\\
&=\frac{1}{24}[6x^2\cos 2x+x(8x^2-3)\sin 2x]
\end{align} \begin{align}
&=\frac{1}{4i}\left[e^{2ix}D^{-1}\left(2x^2+ix-\frac{1}{4}\right)-e^{-2ix}D^{-1}\left(2x^2-ix-\frac{1}{4}\right)\right]\\
&=\frac{1}{4i}\left[e^{2ix}\left(2\frac{x^3}{3}+i\frac{x^2}{2}-\frac{x}{4}\right)-e^{-2ix}\left(2\frac{x^3}{3}-i\frac{x^2}{2}-\frac{x}{4}\right)\right]\\
&=\frac{1}{4i}\left[\frac{2x^3}{3}(e^{2ix}-e^{-2ix})+\frac{x^2i}{2}(e^{2ix}-e^{-2ix})-\frac{x}{4}(e^{2ix}-e^{-2ix})\right]\\
&=\frac{1}{4i}\left[\frac{2x^3}{3}2\cos 2x+\frac{x^2i}{2}2\cos 2x-\frac{x}{4}2\cos 2x\right]
\end{align}",['ordinary-differential-equations']
96,How to solve this integral w.r.t. the solution of the ODE,How to solve this integral w.r.t. the solution of the ODE,,"I'm currently studying ODE's and had a question regarding the solution to a problem. The exercise problem is relatively simple: Find the solution for the following ODE: $$y' = 1 - y^2$$ I'm experiencing some confusion regarding the solution. Probably because it's been a very long time since I've studied calculus. My approach is: $$ \begin{align} y' & = 1 - y^2 \\ \frac{1}{1 - y^2}y' & = 1 \\ \int \frac{1}{1 - y^2}\,dy & = \int 1 \, dx \\ \end{align} $$ Solving the integral on the left: $$ \begin{align} \int \frac{1}{1 - y^2}\,dy & = \int \left( \frac{1}{2(1 + y)} + \frac{1}{2(1 - y)} \right)\,dy \\ & = \frac{1}{2}\int \frac{1}{1 + y}\,dy + \frac{1}{2}\int \frac{1}{1 - y}\,dy \\ & = \frac{1}{2} \ln(\vert1 + y\vert) + \frac{1}{2} \ln{(\vert 1 - y\vert)} + C \end{align} $$ And therefore I concluded that the solution to the ODE is: $$ \begin{align} \frac{1}{2} \ln{(\vert 1 + y \vert)} + \frac{1}{2} \ln(\vert 1 - y \vert ) & = x + C \\  \ln(\vert 1 + y \vert) + \ln(\vert 1 - y \vert) & = 2x + C \\ \ln(\vert 1 - y^2 \vert ) & = 2x + C \\ \vert 1 - y^2 \vert & = Ce^{2x} \end{align} $$ and therefore: $$ y =  \begin{cases} \begin{align} \sqrt{-Ce^{2x} + 1} && 1 - y^2 \ge 0 \\ \sqrt{Ce^{2x} + 1} && 1 - y^2 \lt 0 \end{align} \end{cases} $$ However, the solution is simply: $$ y = \tanh{(x + C)} $$ My questions are the following: How should I be able to recognize that apparently the integral that the solution uses to go to $\tanh$ is a standard integral? I've thought of ways that I could derive it, but other than memorization it seems difficult to find the connection. When do I know whether or not to include the absolute values for the solutions when a natural logarithm is involved? I've noticed in many cases the final solution doesn't include the absolute value forms, but there's rarely a justification or explanation as to why that conclusion was made. Thanks in advance.","I'm currently studying ODE's and had a question regarding the solution to a problem. The exercise problem is relatively simple: Find the solution for the following ODE: I'm experiencing some confusion regarding the solution. Probably because it's been a very long time since I've studied calculus. My approach is: Solving the integral on the left: And therefore I concluded that the solution to the ODE is: and therefore: However, the solution is simply: My questions are the following: How should I be able to recognize that apparently the integral that the solution uses to go to is a standard integral? I've thought of ways that I could derive it, but other than memorization it seems difficult to find the connection. When do I know whether or not to include the absolute values for the solutions when a natural logarithm is involved? I've noticed in many cases the final solution doesn't include the absolute value forms, but there's rarely a justification or explanation as to why that conclusion was made. Thanks in advance.","y' = 1 - y^2 
\begin{align}
y' & = 1 - y^2 \\
\frac{1}{1 - y^2}y' & = 1 \\
\int \frac{1}{1 - y^2}\,dy & = \int 1 \, dx \\
\end{align}
 
\begin{align}
\int \frac{1}{1 - y^2}\,dy & = \int \left( \frac{1}{2(1 + y)} + \frac{1}{2(1 - y)} \right)\,dy \\
& = \frac{1}{2}\int \frac{1}{1 + y}\,dy + \frac{1}{2}\int \frac{1}{1 - y}\,dy \\
& = \frac{1}{2} \ln(\vert1 + y\vert) + \frac{1}{2} \ln{(\vert 1 - y\vert)} + C
\end{align}
 
\begin{align}
\frac{1}{2} \ln{(\vert 1 + y \vert)} + \frac{1}{2} \ln(\vert 1 - y \vert ) & = x + C \\ 
\ln(\vert 1 + y \vert) + \ln(\vert 1 - y \vert) & = 2x + C \\
\ln(\vert 1 - y^2 \vert ) & = 2x + C \\
\vert 1 - y^2 \vert & = Ce^{2x}
\end{align}
 
y = 
\begin{cases}
\begin{align}
\sqrt{-Ce^{2x} + 1} && 1 - y^2 \ge 0 \\
\sqrt{Ce^{2x} + 1} && 1 - y^2 \lt 0
\end{align}
\end{cases}
 
y = \tanh{(x + C)}
 \tanh","['ordinary-differential-equations', 'logarithms', 'absolute-value']"
97,$f(x) = \ln(x)f'(x)$.,.,f(x) = \ln(x)f'(x),"I would like to find a differentiable $f:[1,\infty) \to \mathbb{R}$ satisfying: $$f(x) = \ln(x)f'(x)$$ . My work: We have $f(1) = 0$ and $f(e)=f^k(e)$ . A Taylor series centered at $x_0=e$ gives us the following polynomial: $$f(e)+f(e)(x-e)+f(e)\frac{1}{2}(x-e)^2 \dots$$ $$=f(e) \sum_{k=1}^\infty \frac{(x-e)^k}{k!}$$ although I don't see how this might help me, and I also am not sure how else to approach this problem. Does the condition $f(e)=f^k(e)$ force $f(x)$ to be $e^x$ , which would contradict $f(x)=\ln(x)f'(x)$ , and thus no $f$ exists? I wouldn't think so, although I know that $Ae^x$ is the unique solution to $f(x)=f'(x)$ from this post .","I would like to find a differentiable satisfying: . My work: We have and . A Taylor series centered at gives us the following polynomial: although I don't see how this might help me, and I also am not sure how else to approach this problem. Does the condition force to be , which would contradict , and thus no exists? I wouldn't think so, although I know that is the unique solution to from this post .","f:[1,\infty) \to \mathbb{R} f(x) = \ln(x)f'(x) f(1) = 0 f(e)=f^k(e) x_0=e f(e)+f(e)(x-e)+f(e)\frac{1}{2}(x-e)^2 \dots =f(e) \sum_{k=1}^\infty \frac{(x-e)^k}{k!} f(e)=f^k(e) f(x) e^x f(x)=\ln(x)f'(x) f Ae^x f(x)=f'(x)","['calculus', 'ordinary-differential-equations', 'derivatives']"
98,Meaning of power series expansion in perturbation theory,Meaning of power series expansion in perturbation theory,,"I encountered a power series expansion of $x$ in $\epsilon$ when solving for the general solution to Mathieu's equation in this paper . $x(\xi, \eta) = x_0(\xi, \eta) + \epsilon x_1(\xi, \eta) + \epsilon^2x_2(\xi,\eta) + \cdots$ Is there an intuitive reason as to why we perform such power series expansion (are we assuming some perturbed $\tilde{x}(t)$ around the exact solution $x(t)$ using the perturbation parameter as $\epsilon$ )? If I didn't know what the right-hand side of the above expansion, how could I have figured it out? Is it a standard expansion for such problems? I don't think I appreciate why we introduce a perturbation factor either. Is it simply because it makes an approximation in first-order convenient?","I encountered a power series expansion of in when solving for the general solution to Mathieu's equation in this paper . Is there an intuitive reason as to why we perform such power series expansion (are we assuming some perturbed around the exact solution using the perturbation parameter as )? If I didn't know what the right-hand side of the above expansion, how could I have figured it out? Is it a standard expansion for such problems? I don't think I appreciate why we introduce a perturbation factor either. Is it simply because it makes an approximation in first-order convenient?","x \epsilon x(\xi, \eta) = x_0(\xi, \eta) + \epsilon x_1(\xi, \eta) + \epsilon^2x_2(\xi,\eta) + \cdots \tilde{x}(t) x(t) \epsilon","['calculus', 'ordinary-differential-equations', 'perturbation-theory']"
99,Solve intial value problem using power series,Solve intial value problem using power series,,"$xy''+y'+2y=0$ with y(1) =2, y'(1) =4. What I tried: \begin{equation} 	xy'' +y'+2y = 0 	\end{equation} Let $y=\sum_{k=0}^{\infty}c_kx^k$ , $y^\prime=\sum_{k=1}^{\infty}kc_kx^{k-1}$ , $y^{\prime\prime}=\sum_{k=2}^{\infty}k(k-1)c_kx^{k-2}$ Then \begin{equation} 	x\sum_{k=2}^{\infty}k(k-1)c_kx^{k-2}+\sum_{k=1}^{\infty}kc_{k+1}x^{k-1}+2\sum_{k=0}^{\infty}c_kx^k=0 	\end{equation} \begin{equation} \sum_{k=2}^{\infty}k(k+1)c_kx^{k-1}+\sum_{k=1}^{\infty}kc_kx^{k-1}+\sum_{k=0}^{\infty}2c_kx^k=0 	\end{equation} \begin{equation} 		\sum_{k=0}^{\infty}k(k+1)c_{k+1}x^{k}+\sum_{k=0}^{\infty}(k+1)c_{k+1}x^{k}+\sum_{k=0}^{\infty}2kc_kx^{k}=0 		\end{equation} \begin{equation} \sum_{k=0}^{\infty}[k(k+1)c_{k+1}+(k+1)c_{k+1}+2kc_k]x^k=0 \end{equation} Therefore \begin{equation} k(k+1)c_{k+1}+(k+1)c_{k+1}+2kc_k=0 \text{ for }k\ge0 \end{equation} So \begin{equation} c_{k+1}=\frac{-2k}{(k+1)^2}c_k \end{equation} This is how much I have tried. The answer is given to be: $y=2+4(x-1)-4(x-1)^2+\frac{4}{5}(x-1)^3-\frac{1}{3}(x-1)^4+\frac{2}{15}(x-1)^5+.... $ How do I obtain this form. Pls help. Question link: https://i.sstatic.net/tyvT8.jpg","with y(1) =2, y'(1) =4. What I tried: Let , , Then Therefore So This is how much I have tried. The answer is given to be: How do I obtain this form. Pls help. Question link: https://i.sstatic.net/tyvT8.jpg","xy''+y'+2y=0 \begin{equation}
	xy'' +y'+2y = 0
	\end{equation} y=\sum_{k=0}^{\infty}c_kx^k y^\prime=\sum_{k=1}^{\infty}kc_kx^{k-1} y^{\prime\prime}=\sum_{k=2}^{\infty}k(k-1)c_kx^{k-2} \begin{equation}
	x\sum_{k=2}^{\infty}k(k-1)c_kx^{k-2}+\sum_{k=1}^{\infty}kc_{k+1}x^{k-1}+2\sum_{k=0}^{\infty}c_kx^k=0
	\end{equation} \begin{equation}
\sum_{k=2}^{\infty}k(k+1)c_kx^{k-1}+\sum_{k=1}^{\infty}kc_kx^{k-1}+\sum_{k=0}^{\infty}2c_kx^k=0
	\end{equation} \begin{equation}
		\sum_{k=0}^{\infty}k(k+1)c_{k+1}x^{k}+\sum_{k=0}^{\infty}(k+1)c_{k+1}x^{k}+\sum_{k=0}^{\infty}2kc_kx^{k}=0
		\end{equation} \begin{equation}
\sum_{k=0}^{\infty}[k(k+1)c_{k+1}+(k+1)c_{k+1}+2kc_k]x^k=0
\end{equation} \begin{equation}
k(k+1)c_{k+1}+(k+1)c_{k+1}+2kc_k=0 \text{ for }k\ge0
\end{equation} \begin{equation}
c_{k+1}=\frac{-2k}{(k+1)^2}c_k
\end{equation} y=2+4(x-1)-4(x-1)^2+\frac{4}{5}(x-1)^3-\frac{1}{3}(x-1)^4+\frac{2}{15}(x-1)^5+.... ",['ordinary-differential-equations']
