,title_raw,title_text,title_latex,body_raw,body_text,body_latex,tags
0,Real Analysis Convergence question,Real Analysis Convergence question,,"My question is: For $f_n, g, h \in L^p(X)$, where $X$ is a finite measure space,  if $f_n$ converges to $g$ a.e and $f_n$ converges to $h$ weakly in $L^p$, can we conclude any relationship between $g$ and $h$? Thanks!","My question is: For $f_n, g, h \in L^p(X)$, where $X$ is a finite measure space,  if $f_n$ converges to $g$ a.e and $f_n$ converges to $h$ weakly in $L^p$, can we conclude any relationship between $g$ and $h$? Thanks!",,"['real-analysis', 'measure-theory', 'functional-analysis']"
1,Does almost everywhere convergence imply convergence in $L^p$?,Does almost everywhere convergence imply convergence in ?,L^p,"Is it true that if $\Omega$ is an open, bounded subset of $R^{N}$, $u_{n} \to u$ almost everywhere, $1<p<\infty$, then $\|u_{n} - u\|_{L^{p}} \to 0$? Here sequence and the function are in $L^{p}$ My proof uses dominated convergence theorem to $v_{n}(x) = |u_{n}(x) - u(x)|^{p}.$ and the fact that $\Omega$ is bounded. thanks","Is it true that if $\Omega$ is an open, bounded subset of $R^{N}$, $u_{n} \to u$ almost everywhere, $1<p<\infty$, then $\|u_{n} - u\|_{L^{p}} \to 0$? Here sequence and the function are in $L^{p}$ My proof uses dominated convergence theorem to $v_{n}(x) = |u_{n}(x) - u(x)|^{p}.$ and the fact that $\Omega$ is bounded. thanks",,"['real-analysis', 'analysis', 'convergence-divergence']"
2,The ring of germs of functions $C^\infty (M)$,The ring of germs of functions,C^\infty (M),"Define $C^\infty (M)_x := \{ (U,f) | x \in U $ open $ , f \in C^\infty (U) \} / \sim $ where $M$ is a manifold and $(U,f) \sim (V,g)$ if $\exists W$ open, $x \in W$ such that $W \subset V \cap U$ and $f|_W = g|_W$. This is a ring with the following operations: $[(U,f)] + [(V,g)] := [(U \cap V, f  + g)]$ and $[(U,f)]\cdot [(V,g)]:= [(U \cap V, fg)]$. I'm trying to understand what multiplicative inverses look like, i.e. if I have $[(U, f)]$ then I want to show that there exists an open set $V$ containing $x$ such that $\frac{1}{f}$ is smooth on $V$. Is this right? And can someone explain to me how I can show this? Many thanks for your help.","Define $C^\infty (M)_x := \{ (U,f) | x \in U $ open $ , f \in C^\infty (U) \} / \sim $ where $M$ is a manifold and $(U,f) \sim (V,g)$ if $\exists W$ open, $x \in W$ such that $W \subset V \cap U$ and $f|_W = g|_W$. This is a ring with the following operations: $[(U,f)] + [(V,g)] := [(U \cap V, f  + g)]$ and $[(U,f)]\cdot [(V,g)]:= [(U \cap V, fg)]$. I'm trying to understand what multiplicative inverses look like, i.e. if I have $[(U, f)]$ then I want to show that there exists an open set $V$ containing $x$ such that $\frac{1}{f}$ is smooth on $V$. Is this right? And can someone explain to me how I can show this? Many thanks for your help.",,"['real-analysis', 'algebraic-geometry', 'commutative-algebra']"
3,Can you cover $\mathbb R$ with countably many smol sets?,Can you cover  with countably many smol sets?,\mathbb R,"Say a set $X\subset\mathbb R$ is smol if one can't cover $\mathbb R$ with countably many translates of $X$ . So, for example, null-measure sets are smol. I guess sets of positive measure are not, but I have not been able to prove it yet. Non-measurable sets are a mystery. I wonder: can you cover $\mathbb R$ with countably many smol sets? This question came as an effort to find a maximal family $\mathcal F\subset\mathcal P(\mathbb R)$ such that any countable union over $\mathcal F$ is not $\mathbb R$ . This would give a rough notion of measure (only telling if a set is big or small) to every subset of $\mathbb R$ .","Say a set is smol if one can't cover with countably many translates of . So, for example, null-measure sets are smol. I guess sets of positive measure are not, but I have not been able to prove it yet. Non-measurable sets are a mystery. I wonder: can you cover with countably many smol sets? This question came as an effort to find a maximal family such that any countable union over is not . This would give a rough notion of measure (only telling if a set is big or small) to every subset of .",X\subset\mathbb R \mathbb R X \mathbb R \mathcal F\subset\mathcal P(\mathbb R) \mathcal F \mathbb R \mathbb R,"['real-analysis', 'measure-theory', 'recreational-mathematics']"
4,Prove that $\frac{{x_1}^2+{x_2}^2+\cdots+{x_n}^2}{n}x_1x_2\cdots x_n\le\left(\frac{x_1+x_2+\cdots+x_n}{n}\right)^{n+2}$,Prove that,\frac{{x_1}^2+{x_2}^2+\cdots+{x_n}^2}{n}x_1x_2\cdots x_n\le\left(\frac{x_1+x_2+\cdots+x_n}{n}\right)^{n+2},"Show that for any non negative real numbers $x_1,x_2,\cdots x_n,$ $$\frac{{x_1}^2+{x_2}^2+\cdots+{x_n}^2}{n}x_1x_2\cdots x_n\le\left(\frac{x_1+x_2+\cdots+x_n}{n}\right)^{n+2}$$ My work: Let $$S(n)=\frac{{x_1}^2+{x_2}^2+\cdots+{x_n}^2}{n}x_1x_2\cdots x_n\le\left(\frac{x_1+x_2+\cdots+x_n}{n}\right)^{n+2}$$ By theorem of triviality, if any of $x_i$ 's are $0$ the inequality is certainly true. So assume all numbers are $\gt0$ $S(1)$ says ${x_1}^3\le {x_1}^3$ which is certainly true. $S(2)$ says $({x_1}^2+{x_2}^2)x_1x_2\le\frac18(x_1+x_2)^4$ which reduces to $0\le(x_1-x_2)^4$ which is certainly true. Assume $S(k)$ is true. Now we just needs to prove that $S(k+1)$ is true. But I'm having a hard time in doing that. Any help is greatly appreciated. Or is there any better method than induction $?$","Show that for any non negative real numbers My work: Let By theorem of triviality, if any of 's are the inequality is certainly true. So assume all numbers are says which is certainly true. says which reduces to which is certainly true. Assume is true. Now we just needs to prove that is true. But I'm having a hard time in doing that. Any help is greatly appreciated. Or is there any better method than induction","x_1,x_2,\cdots x_n, \frac{{x_1}^2+{x_2}^2+\cdots+{x_n}^2}{n}x_1x_2\cdots x_n\le\left(\frac{x_1+x_2+\cdots+x_n}{n}\right)^{n+2} S(n)=\frac{{x_1}^2+{x_2}^2+\cdots+{x_n}^2}{n}x_1x_2\cdots x_n\le\left(\frac{x_1+x_2+\cdots+x_n}{n}\right)^{n+2} x_i 0 \gt0 S(1) {x_1}^3\le {x_1}^3 S(2) ({x_1}^2+{x_2}^2)x_1x_2\le\frac18(x_1+x_2)^4 0\le(x_1-x_2)^4 S(k) S(k+1) ?","['real-analysis', 'inequality', 'induction']"
5,"Is $f(x)=\sum_{n=1}^{\infty}\frac{x}{1+n^2x^2}$, $x\in[0,1]$ continuous on $[0,1]$","Is ,  continuous on","f(x)=\sum_{n=1}^{\infty}\frac{x}{1+n^2x^2} x\in[0,1] [0,1]","Let $f(x)=\sum_{n=1}^{\infty}\frac{x}{1+n^2x^2}$ , $x\in[0,1]$ . Question: Show that $f$ is Lebesgue integrable and determine whether $f$ is continuous on $[0,1]$ . For the first part, I have no problem, I showed that it is Lebesgue integrable. But for the second part, I could not reach any result by using the definition. Also, I'm not sure if I should use the first part.  Can you give me a hint for this part?","Let , . Question: Show that is Lebesgue integrable and determine whether is continuous on . For the first part, I have no problem, I showed that it is Lebesgue integrable. But for the second part, I could not reach any result by using the definition. Also, I'm not sure if I should use the first part.  Can you give me a hint for this part?","f(x)=\sum_{n=1}^{\infty}\frac{x}{1+n^2x^2} x\in[0,1] f f [0,1]","['real-analysis', 'continuity', 'lebesgue-integral']"
6,Distance from a point in a set to a subset of that set is Lipschitz,Distance from a point in a set to a subset of that set is Lipschitz,,"The question I'm stuck on is the following: Let $(X,d)$ be a metric space and let $Y$ be a subset of $X$ . If $x\in X$ , define the distance $d(x,Y)$ as $\inf\{(d(x,y):y\in Y\}$ . Show that the mapping from $X$ to $\mathbb{R}:x\rightarrow d(x,Y)$ is Lipschitz , i.e. that there exists a constant $C>0$ such that $|d(x,Y)-d(x',Y)|\le Cd(x,x'), x,x'\in X$ . I'm quite lost as to how to approach it because there is no upper bound for $d(x,x')$ and thus the left side can easily go off to infinity. How can I relate distance between two points and the difference in their distances to $Y$ in a way that one constant works for the entire set?","The question I'm stuck on is the following: Let be a metric space and let be a subset of . If , define the distance as . Show that the mapping from to is Lipschitz , i.e. that there exists a constant such that . I'm quite lost as to how to approach it because there is no upper bound for and thus the left side can easily go off to infinity. How can I relate distance between two points and the difference in their distances to in a way that one constant works for the entire set?","(X,d) Y X x\in X d(x,Y) \inf\{(d(x,y):y\in Y\} X \mathbb{R}:x\rightarrow d(x,Y) C>0 |d(x,Y)-d(x',Y)|\le Cd(x,x'), x,x'\in X d(x,x') Y","['real-analysis', 'metric-spaces', 'lipschitz-functions']"
7,Polar coordinates for the evaluating limits,Polar coordinates for the evaluating limits,,"I've seen many pages on this site about the using of polar coordinates for evaluating limits but I'm really confused . I don't know when we can apply that or when it shows the correct limit . In fact , I'm looking for a theorem (with proof) that covers this problem completely . Also the references to the reliable books will be helpful .","I've seen many pages on this site about the using of polar coordinates for evaluating limits but I'm really confused . I don't know when we can apply that or when it shows the correct limit . In fact , I'm looking for a theorem (with proof) that covers this problem completely . Also the references to the reliable books will be helpful .",,"['real-analysis', 'limits', 'multivariable-calculus', 'reference-request']"
8,"Integrate $\frac{1}{x\,\log{x}}$ by parts [duplicate]",Integrate  by parts [duplicate],"\frac{1}{x\,\log{x}}","This question already has answers here : $\int \frac{dx}{x\log(x)}$ (3 answers) Closed 5 years ago . A naive indefinite integration of the function $\dfrac{1}{x\,\log{x}}$ can be performed as follows: Let $ \begin{eqnarray} I  &=& \int\dfrac{dx}{x\,\log{x}}\\ \therefore I &=& \dfrac{1}{\log{x}}\int\dfrac{dx}{x} - \int\left\{\dfrac{d}{dx} \left(\dfrac{1}{\log{x}}\right) \int \dfrac{dx}{x} \right\}dx\\ &=& \dfrac{1}{\log{x}} \cdot \log{x}-\int - \dfrac{1}{(\log{x})^2}\cdot\dfrac{1}{x}\cdot\log{x}\,dx\\ &=& 1 + \int\dfrac{dx}{x\,\log{x}}\\ &=& 1+I \end{eqnarray} $ This obviously leads to something like $1=0$ . Can anyone please tell me what is going wrong? Thanks in advance. PS. I know that the corrct answer would be $\log(\log{x})$ .",This question already has answers here : $\int \frac{dx}{x\log(x)}$ (3 answers) Closed 5 years ago . A naive indefinite integration of the function can be performed as follows: Let This obviously leads to something like . Can anyone please tell me what is going wrong? Thanks in advance. PS. I know that the corrct answer would be .,"\dfrac{1}{x\,\log{x}} 
\begin{eqnarray}
I  &=& \int\dfrac{dx}{x\,\log{x}}\\
\therefore I &=& \dfrac{1}{\log{x}}\int\dfrac{dx}{x} - \int\left\{\dfrac{d}{dx} \left(\dfrac{1}{\log{x}}\right) \int \dfrac{dx}{x} \right\}dx\\
&=& \dfrac{1}{\log{x}} \cdot \log{x}-\int - \dfrac{1}{(\log{x})^2}\cdot\dfrac{1}{x}\cdot\log{x}\,dx\\
&=& 1 + \int\dfrac{dx}{x\,\log{x}}\\
&=& 1+I
\end{eqnarray}
 1=0 \log(\log{x})","['real-analysis', 'calculus', 'integration', 'indefinite-integrals']"
9,Bounded Harmonic Functions on the Disk,Bounded Harmonic Functions on the Disk,,Denote by $\mathbb{D}$ the open unit disk in $\mathbb{R}^2$ .  Is it possible to find a bounded harmonic function $u : \mathbb{D} \to \mathbb{R}$ that is not uniformly continuous? I tried using functions that oscillate near $\partial \mathbb{D}$ but was unable to get anything substantial.,Denote by the open unit disk in .  Is it possible to find a bounded harmonic function that is not uniformly continuous? I tried using functions that oscillate near but was unable to get anything substantial.,\mathbb{D} \mathbb{R}^2 u : \mathbb{D} \to \mathbb{R} \partial \mathbb{D},"['real-analysis', 'complex-analysis']"
10,An integral involving Bessel function of the first kind of the Sonine-Gegenbauer sort,An integral involving Bessel function of the first kind of the Sonine-Gegenbauer sort,,"Do you know how to do this integral? $$\int\limits_{0}^{2\pi}\mathrm{d}\phi\,\frac{J_2\left(\sqrt{a^2+b^2-2ab\cos(\phi)}\right)}{a^2+b^2-2ab\cos(\phi)}\,,$$ where $J_2$ is the Bessel function of the first kind of second order, and a and b are two positive constants. I have tried various different tricks: using integral representation of the Bessel function, series expansion of the Bessel function, or converting the integral into complex integral over the unit circle, but I couldn't simplify the results I got afterward. Thanks.","Do you know how to do this integral? $$\int\limits_{0}^{2\pi}\mathrm{d}\phi\,\frac{J_2\left(\sqrt{a^2+b^2-2ab\cos(\phi)}\right)}{a^2+b^2-2ab\cos(\phi)}\,,$$ where $J_2$ is the Bessel function of the first kind of second order, and a and b are two positive constants. I have tried various different tricks: using integral representation of the Bessel function, series expansion of the Bessel function, or converting the integral into complex integral over the unit circle, but I couldn't simplify the results I got afterward. Thanks.",,"['real-analysis', 'integration', 'complex-analysis', 'definite-integrals', 'bessel-functions']"
11,Formula for the floor function,Formula for the floor function,,I found the following formula for the floor function: $$\lfloor x \rfloor = -\frac12+x+\frac{\arctan(\cot\pi x)}{\pi}$$ for all $x$ not an integer. My question is where I can find the proof of this formula.,I found the following formula for the floor function: $$\lfloor x \rfloor = -\frac12+x+\frac{\arctan(\cot\pi x)}{\pi}$$ for all $x$ not an integer. My question is where I can find the proof of this formula.,,['real-analysis']
12,Image of Borel set under continuous and injective map,Image of Borel set under continuous and injective map,,"Let $f: \mathbb{R}^n \rightarrow \mathbb{R}^n$ a continuous and injective map. Let $A \subset \mathbb{R}^n$ a Borel set, i.e. $A \in \mathscr{B}(\mathbb{R}^n)$, where $\mathscr{B}(\mathbb{R}^n)$ denote the Borel $\sigma$- algebra. Show that the image of $A$ is also a Borel set, i.e. $f(A) \in \mathscr{B}(\mathbb{R}^n)$. My attempt: Let $A \subset \mathbb{R}^n$ a Borel set. Since $f$ is injective, then we have that $A = f^{-1}(f(A))$. NTS: $f(A)$ Borel set. We suppose by contradiction that $f(A)$ is not a Borel set, so $f(A)$ is not an open or a closed set. I'd like to find a contradiction using the continuity of $f$ but I don't see how. Any suggestions? Thanks!","Let $f: \mathbb{R}^n \rightarrow \mathbb{R}^n$ a continuous and injective map. Let $A \subset \mathbb{R}^n$ a Borel set, i.e. $A \in \mathscr{B}(\mathbb{R}^n)$, where $\mathscr{B}(\mathbb{R}^n)$ denote the Borel $\sigma$- algebra. Show that the image of $A$ is also a Borel set, i.e. $f(A) \in \mathscr{B}(\mathbb{R}^n)$. My attempt: Let $A \subset \mathbb{R}^n$ a Borel set. Since $f$ is injective, then we have that $A = f^{-1}(f(A))$. NTS: $f(A)$ Borel set. We suppose by contradiction that $f(A)$ is not a Borel set, so $f(A)$ is not an open or a closed set. I'd like to find a contradiction using the continuity of $f$ but I don't see how. Any suggestions? Thanks!",,"['real-analysis', 'analysis', 'measure-theory', 'lebesgue-measure', 'borel-sets']"
13,Number of real solutions.,Number of real solutions.,,Question : Let $\{a_i\}$ be a sequence of real numbers such that $0<a_1<a_2\cdots <a_n$. Show that the equation :    $$\frac{a_1}{a_1−x}+\cdots+\frac{a_n}{a_n−x}=2015$$   has exactly $n$ real solutions. My try: I know that this is an nth degree polynomial. But I really have no idea how to show the required.,Question : Let $\{a_i\}$ be a sequence of real numbers such that $0<a_1<a_2\cdots <a_n$. Show that the equation :    $$\frac{a_1}{a_1−x}+\cdots+\frac{a_n}{a_n−x}=2015$$   has exactly $n$ real solutions. My try: I know that this is an nth degree polynomial. But I really have no idea how to show the required.,,['real-analysis']
14,Prove that there exists a sequence $(x_n)$ such that $\sum_n a_n x_n$ diverges,Prove that there exists a sequence  such that  diverges,(x_n) \sum_n a_n x_n,"So, here's a nice little result that I deduced using the closed graph theorem from functional analysis, but I'm wondering if there's a more elementary approach: Fact: Let $(a_n)$ be a sequence with $a_n > 0$ and $a_n \to \infty$ . Then there exists a sequence $(x_n)$ with $\sum |x_n| < \infty$ for which $\sum_{n=1}^\infty a_n x_n$ diverges. I'm thinking there may be a relatively easy to construct sequence $x_n$ here, but I myself can't think of any.  The reason that I know this must hold is that the map $(x_n) \mapsto (a_n x_n)$ is an unbounded operator from $\ell^1$ to $\ell^\infty$ with a continuous inverse defined over the image, but this provides me with no intuition as to how a suitable $(x_n)$ should be constructed.","So, here's a nice little result that I deduced using the closed graph theorem from functional analysis, but I'm wondering if there's a more elementary approach: Fact: Let be a sequence with and . Then there exists a sequence with for which diverges. I'm thinking there may be a relatively easy to construct sequence here, but I myself can't think of any.  The reason that I know this must hold is that the map is an unbounded operator from to with a continuous inverse defined over the image, but this provides me with no intuition as to how a suitable should be constructed.",(a_n) a_n > 0 a_n \to \infty (x_n) \sum |x_n| < \infty \sum_{n=1}^\infty a_n x_n x_n (x_n) \mapsto (a_n x_n) \ell^1 \ell^\infty (x_n),"['real-analysis', 'sequences-and-series', 'functional-analysis', 'alternative-proof']"
15,Largest root as exponent goes to $+\infty$,Largest root as exponent goes to,+\infty,"Let $a\geq 1$ and consider  $$ x^{a+2}-x^{a+1}-1. $$ I am interested to see what is the largest root of this polynomial as $a\to +\infty$. In order to find a root, we surely have to have $$ x^{a+2}-x^{a+1}=x^{a+1}(x-1)=1. $$ Hence, I guess we have to look for which $x$ we have that $$ x^{a+1}(x-1)\to 1\text{ as }a\to+\infty. $$ Intuitively, if $x$ tends to some value larger than $1$ for $a\to\infty$, the whole thing should diverge. On the other side, if $x$ tends to some value smaller than $1$, then the whole expression should converge to $0$. Hence I guess that $x\to 1$ as $a\to\infty$ in order to get a root.","Let $a\geq 1$ and consider  $$ x^{a+2}-x^{a+1}-1. $$ I am interested to see what is the largest root of this polynomial as $a\to +\infty$. In order to find a root, we surely have to have $$ x^{a+2}-x^{a+1}=x^{a+1}(x-1)=1. $$ Hence, I guess we have to look for which $x$ we have that $$ x^{a+1}(x-1)\to 1\text{ as }a\to+\infty. $$ Intuitively, if $x$ tends to some value larger than $1$ for $a\to\infty$, the whole thing should diverge. On the other side, if $x$ tends to some value smaller than $1$, then the whole expression should converge to $0$. Hence I guess that $x\to 1$ as $a\to\infty$ in order to get a root.",,"['real-analysis', 'roots']"
16,Lebesgue measure of an intersection of a sequence of subsets,Lebesgue measure of an intersection of a sequence of subsets,,"This is exercise 1.19 from ""A User-Friendly Introduction to Lebesgue Measure and Integration"" by Gail S. Nelson, and $m(E)$ is notation for Lebesgue measure of set $E$: Let ${E_{k}}$ be a sequence of Lebesgue measurable sets with $E_{1}\supseteq E_{2}\supseteq E_{3}\supseteq\ldots$. Define the set $E$ to be $E=\cap_{k=1}^{\infty}E_{k}$. If $m(E_{1})<\infty$, show that $m(E)=\lim_{k\to\infty}m(E_{k})$. Show by example that this need not to be the case if we remove the assumption that $m(E_{1})<\infty$. My attempt of proof: Since $E$ is subset of $E_{k}$ for each $k$, then $m(E)\leq m(E_{k})$ - this is according to a theorem stating that Lebesgue measure of a subset is less or equal than Lebesgue measure of its superset. Thus, it's certainly true that $m(E)\leq\lim_{k\to\infty}m(E_{k})$ too. If the limit is 0, then $m(E)$ is equal to the limit, as Lebesgue measure of any set is greater or equal to zero.  However, now I don't see how to eventually prove that $m(E)$ cannot be less that the limit, in case when limit greater than 0.  Also, for the second part of the question, I cannot find a counter-example for the case when $m(E_{1})=\infty$.","This is exercise 1.19 from ""A User-Friendly Introduction to Lebesgue Measure and Integration"" by Gail S. Nelson, and $m(E)$ is notation for Lebesgue measure of set $E$: Let ${E_{k}}$ be a sequence of Lebesgue measurable sets with $E_{1}\supseteq E_{2}\supseteq E_{3}\supseteq\ldots$. Define the set $E$ to be $E=\cap_{k=1}^{\infty}E_{k}$. If $m(E_{1})<\infty$, show that $m(E)=\lim_{k\to\infty}m(E_{k})$. Show by example that this need not to be the case if we remove the assumption that $m(E_{1})<\infty$. My attempt of proof: Since $E$ is subset of $E_{k}$ for each $k$, then $m(E)\leq m(E_{k})$ - this is according to a theorem stating that Lebesgue measure of a subset is less or equal than Lebesgue measure of its superset. Thus, it's certainly true that $m(E)\leq\lim_{k\to\infty}m(E_{k})$ too. If the limit is 0, then $m(E)$ is equal to the limit, as Lebesgue measure of any set is greater or equal to zero.  However, now I don't see how to eventually prove that $m(E)$ cannot be less that the limit, in case when limit greater than 0.  Also, for the second part of the question, I cannot find a counter-example for the case when $m(E_{1})=\infty$.",,"['real-analysis', 'measure-theory', 'lebesgue-measure']"
17,Are all computable functions continuous or vice-versa?,Are all computable functions continuous or vice-versa?,,"A famous result in intuitionistic mathematics is that all real-valued total functions are continuous. Since the requirements for a function to be admitted intuitionistically is that it must define a procedure or algorithm, all functions are computable. This seems to suggest that all computable functions are continuous. My questions are: Is this true for all total recursive or just primitive recursive functions? Where can I find such a proof? Conversely, what are examples of continuous functions that are not computable? By the way, I heard here and here that most intuitionistic formal systems can only prove that there are no real-valued total discontinuous functions, not the stronger positive result that all functions are continuous.","A famous result in intuitionistic mathematics is that all real-valued total functions are continuous. Since the requirements for a function to be admitted intuitionistically is that it must define a procedure or algorithm, all functions are computable. This seems to suggest that all computable functions are continuous. My questions are: Is this true for all total recursive or just primitive recursive functions? Where can I find such a proof? Conversely, what are examples of continuous functions that are not computable? By the way, I heard here and here that most intuitionistic formal systems can only prove that there are no real-valued total discontinuous functions, not the stronger positive result that all functions are continuous.",,"['real-analysis', 'general-topology', 'computability', 'constructive-mathematics', 'intuitionistic-logic']"
18,"If $u \in H^s(\mathbb{R}^n)$ for $s > n/2$, then $u \in L^\infty(\mathbb{R}^n)$?","If  for , then ?",u \in H^s(\mathbb{R}^n) s > n/2 u \in L^\infty(\mathbb{R}^n),"How do I use the Fourier transform to see that if $u \in H^s(\mathbb{R}^n)$ for $s > n/2$, then $u \in L^\infty(\mathbb{R}^n)$, with the bound$$\|u\|_{L^\infty(\mathbb{R}^n)} \le C\|u\|_{H^s(\mathbb{R}^n)},$$the constant $C$ depending only on $s$ and $n$?","How do I use the Fourier transform to see that if $u \in H^s(\mathbb{R}^n)$ for $s > n/2$, then $u \in L^\infty(\mathbb{R}^n)$, with the bound$$\|u\|_{L^\infty(\mathbb{R}^n)} \le C\|u\|_{H^s(\mathbb{R}^n)},$$the constant $C$ depending only on $s$ and $n$?",,"['real-analysis', 'partial-differential-equations', 'fourier-analysis', 'sobolev-spaces', 'lp-spaces']"
19,Can we take a logarithm of an infinite product?,Can we take a logarithm of an infinite product?,,"Suppose we have an infinite product $S = \prod_{n=1}^{\infty} a_n$ of positive real numbers. Then is it always the case that $$ \log(S) = \sum_{n=1}^{\infty} \log a_n ? $$ I am sure this is the case, but I wanted to make sure.  Thank you!","Suppose we have an infinite product $S = \prod_{n=1}^{\infty} a_n$ of positive real numbers. Then is it always the case that $$ \log(S) = \sum_{n=1}^{\infty} \log a_n ? $$ I am sure this is the case, but I wanted to make sure.  Thank you!",,"['calculus', 'real-analysis']"
20,Every $\sigma$ finite measure is absolutely continuous with respect to a finite measure.,Every  finite measure is absolutely continuous with respect to a finite measure.,\sigma,"Let $\mu$ be a $\sigma$- finite measure on $(X,M)$. Prove that there exists a finite measure $\lambda$ on $M$ such that $\lambda\ll\mu$ and $\mu\ll\lambda$. Can anyone give me a hint on how to start on this problem?","Let $\mu$ be a $\sigma$- finite measure on $(X,M)$. Prove that there exists a finite measure $\lambda$ on $M$ such that $\lambda\ll\mu$ and $\mu\ll\lambda$. Can anyone give me a hint on how to start on this problem?",,"['real-analysis', 'measure-theory']"
21,Strictly convex if and only if derivative strictly increasing?,Strictly convex if and only if derivative strictly increasing?,,Suppose $f$ is a real-valued function that is differentiable on an open interval $I$. It is well-known that $f^{\prime}$ is increasing on $I$ if and only if $f$ is convex on $I$. Is the following true? $f^{\prime}$ is strictly increasing on $I$ if and only if $f$ is strictly convex on $I$. I'm pretty sure the $\Rightarrow$ direction is true. I'm less confident about the other direction. Is it easier if we also assume $f^{\prime \prime}$ exists on $I$. References or counterexamples greatly appreciated.,Suppose $f$ is a real-valued function that is differentiable on an open interval $I$. It is well-known that $f^{\prime}$ is increasing on $I$ if and only if $f$ is convex on $I$. Is the following true? $f^{\prime}$ is strictly increasing on $I$ if and only if $f$ is strictly convex on $I$. I'm pretty sure the $\Rightarrow$ direction is true. I'm less confident about the other direction. Is it easier if we also assume $f^{\prime \prime}$ exists on $I$. References or counterexamples greatly appreciated.,,"['real-analysis', 'analysis', 'convex-analysis']"
22,Möbius transformation in the complex plane.,Möbius transformation in the complex plane.,,"Assume that $U$ be a line in the complex plane. And assume a Möbius transformation $\phi $ sends $ U $ again to a line. How can I classify all such $\phi$? I want to write my ideas. But, I cannot do anything. Please explain. I saw this question in a textbook as an exercise. This seems so interesting to me. I just want to learn this question.","Assume that $U$ be a line in the complex plane. And assume a Möbius transformation $\phi $ sends $ U $ again to a line. How can I classify all such $\phi$? I want to write my ideas. But, I cannot do anything. Please explain. I saw this question in a textbook as an exercise. This seems so interesting to me. I just want to learn this question.",,"['real-analysis', 'complex-analysis', 'analysis', 'differential-geometry', 'manifolds']"
23,Infinite Series $\sum\limits_{n=1}^{\infty}\frac{1}{\prod\limits_{k=1}^{m}(n+k)}$,Infinite Series,\sum\limits_{n=1}^{\infty}\frac{1}{\prod\limits_{k=1}^{m}(n+k)},How to prove the following equality? $$\sum_{n=1}^{\infty}\frac{1}{\prod\limits_{k=1}^{m}(n+k)}=\frac{1}{(m-1)m!}.$$,How to prove the following equality? $$\sum_{n=1}^{\infty}\frac{1}{\prod\limits_{k=1}^{m}(n+k)}=\frac{1}{(m-1)m!}.$$,,"['real-analysis', 'sequences-and-series', 'closed-form']"
24,"if $f$ is continuous on $\mathbb{R}$ and $f(r)=0,r \in \mathbb{Q}$, then $f(x)=0,x \in \mathbb{R}$","if  is continuous on  and , then","f \mathbb{R} f(r)=0,r \in \mathbb{Q} f(x)=0,x \in \mathbb{R}","Suppose that $f:\mathbb{R} \rightarrow \mathbb{R}$ is continuous on $\mathbb{R}$ and that $f(r)=0$ for all $r \in \mathbb{Q}$. Prove that $f(x)=0$ for all $x \in \mathbb{R}$. My attempt: Define a sequence $(x_n)$ where $x_n \in \mathbb{Q}$ for all $n \in \mathbb{N}$ and assume that $(x_n) \rightarrow a \not\in \mathbb{Q}$. Since $f$ is continuous, we have $\lim_n{f(x_n)}=f(a)=0$. Since $a$ is arbitrary irrational number, we have $f(a)=0$ for all $a \not\in \mathbb{Q}$. Hence, we proved the statement. Is my proof valid? or is there any flaw ?","Suppose that $f:\mathbb{R} \rightarrow \mathbb{R}$ is continuous on $\mathbb{R}$ and that $f(r)=0$ for all $r \in \mathbb{Q}$. Prove that $f(x)=0$ for all $x \in \mathbb{R}$. My attempt: Define a sequence $(x_n)$ where $x_n \in \mathbb{Q}$ for all $n \in \mathbb{N}$ and assume that $(x_n) \rightarrow a \not\in \mathbb{Q}$. Since $f$ is continuous, we have $\lim_n{f(x_n)}=f(a)=0$. Since $a$ is arbitrary irrational number, we have $f(a)=0$ for all $a \not\in \mathbb{Q}$. Hence, we proved the statement. Is my proof valid? or is there any flaw ?",,['real-analysis']
25,Approximating Integrable Simple Functions by Step Functions,Approximating Integrable Simple Functions by Step Functions,,"Definitions: An integrable simple function $f = a_1 \chi_{A_1} + \ldots + a_n \chi_{A_n},$ where $\chi_{A_k}$ is an indicator function and the $A_j$'s are disjoint measurable sets of finite measure. A step function is a piecewise constant function with a finite number of pieces (basically it is an ISF with intervals as the sets). Prove that for any ISF $f$ and $\epsilon > 0,$ there is a step function $g$ such that $\displaystyle\int |f-g| < \epsilon.$ The question says to use the approximation property for measurable sets (measurable sets can be approximated by open and closed sets) and the fact that $m(A\triangle B) = \displaystyle\int |\chi_A - \chi_B|,$ where $A\triangle B$ is the symmetric difference. Basically, I'm having trouble finding a finite set of intervals for this approximation - any advice would be appreciated.","Definitions: An integrable simple function $f = a_1 \chi_{A_1} + \ldots + a_n \chi_{A_n},$ where $\chi_{A_k}$ is an indicator function and the $A_j$'s are disjoint measurable sets of finite measure. A step function is a piecewise constant function with a finite number of pieces (basically it is an ISF with intervals as the sets). Prove that for any ISF $f$ and $\epsilon > 0,$ there is a step function $g$ such that $\displaystyle\int |f-g| < \epsilon.$ The question says to use the approximation property for measurable sets (measurable sets can be approximated by open and closed sets) and the fact that $m(A\triangle B) = \displaystyle\int |\chi_A - \chi_B|,$ where $A\triangle B$ is the symmetric difference. Basically, I'm having trouble finding a finite set of intervals for this approximation - any advice would be appreciated.",,"['real-analysis', 'measure-theory']"
26,Heisenberg uncertainty principle in $d$ dimensions,Heisenberg uncertainty principle in  dimensions,d,"Suppose $f(x)$ is a $d$-dimensional real function and $$\int_{R^{d}}|f(x)|^2 \,\mathrm d x=1$$  Show that   $$ \left( \int_{R^{d}}|x|^2|f(x)|^2 \,\mathrm d x \right) \left( \int_{R^{d}}|\xi|^2|\hat f(\xi)|^2 \,\mathrm d\xi \right) \geq \frac{d^2}{16\pi^2}$$ I derived that $$1=\int_{R^{d}}x \left(\frac{d}{dx}\right)|f(x)|^2 \,\mathrm dx$$ but I lost my way. I need your help.","Suppose $f(x)$ is a $d$-dimensional real function and $$\int_{R^{d}}|f(x)|^2 \,\mathrm d x=1$$  Show that   $$ \left( \int_{R^{d}}|x|^2|f(x)|^2 \,\mathrm d x \right) \left( \int_{R^{d}}|\xi|^2|\hat f(\xi)|^2 \,\mathrm d\xi \right) \geq \frac{d^2}{16\pi^2}$$ I derived that $$1=\int_{R^{d}}x \left(\frac{d}{dx}\right)|f(x)|^2 \,\mathrm dx$$ but I lost my way. I need your help.",,"['real-analysis', 'analysis', 'integration', 'fourier-analysis']"
27,Proof that$ (a+A)\cap A=\varnothing$ if $A$ is countable,Proof that if  is countable, (a+A)\cap A=\varnothing A,"I finished my test and there is a question I completely failed but that my teacher did not go over, so I was hoping someone could post a correction of it, so that I understand what I was supposed to do for next time. Suppose that A is a countable set of real numbers. Show that there exists a real number a such that $(a+A)\cap A=\varnothing$. (Note: By definition, $a+A= \{a+r\mid r\in A\}$.)","I finished my test and there is a question I completely failed but that my teacher did not go over, so I was hoping someone could post a correction of it, so that I understand what I was supposed to do for next time. Suppose that A is a countable set of real numbers. Show that there exists a real number a such that $(a+A)\cap A=\varnothing$. (Note: By definition, $a+A= \{a+r\mid r\in A\}$.)",,"['real-analysis', 'elementary-set-theory']"
28,Why is this sequence of functions not uniformly convergent?,Why is this sequence of functions not uniformly convergent?,,"For the following sequence of functions and its limit function, we can see that $f_n(x)$ is clearly pointwise convergent $$f_n(x) = x^n\text{ }\forall x\in[0,1]\text{ and }\forall n\in\mathbb N^*\\ f(x) = \begin{cases}0&\text{if } x\in[0,1)\\1&\text{if } x=1\end{cases}$$ However, I was wondering why this is not uniformly convergent. The condition for uniform convergence is: $$|f_n(x) - f(x)| < \epsilon,\ \ \ \forall x \text{ when } n > N$$ Now most sources present an argument along the lines of: assume that $f_n(x)$ is uniformly convergent and that $0 < x < 1$, this means that $x^n<\epsilon$ whenever $n>N$. Specifically, this would mean $x^{N+1}<\epsilon$ for some fixed $N$. But if we now pick $x$ such that $1 > x > ε^{\frac{1}{N+1}}$, then this would lead to a contradiction, therefore $f_n(x)$ is not uniformly convergent. However, I was wondering why couldn't we take $n$ to infinity. If $0 < x < 1$, then $\lim_{n\rightarrow \infty} |f_n(x) - f(x)| = 0$ (which is less than $\epsilon$). Now since $|f_n(x) - f(x)|$ will always be $0$ if $n > \infty$, then wouldn't this be uniformly convergent?","For the following sequence of functions and its limit function, we can see that $f_n(x)$ is clearly pointwise convergent $$f_n(x) = x^n\text{ }\forall x\in[0,1]\text{ and }\forall n\in\mathbb N^*\\ f(x) = \begin{cases}0&\text{if } x\in[0,1)\\1&\text{if } x=1\end{cases}$$ However, I was wondering why this is not uniformly convergent. The condition for uniform convergence is: $$|f_n(x) - f(x)| < \epsilon,\ \ \ \forall x \text{ when } n > N$$ Now most sources present an argument along the lines of: assume that $f_n(x)$ is uniformly convergent and that $0 < x < 1$, this means that $x^n<\epsilon$ whenever $n>N$. Specifically, this would mean $x^{N+1}<\epsilon$ for some fixed $N$. But if we now pick $x$ such that $1 > x > ε^{\frac{1}{N+1}}$, then this would lead to a contradiction, therefore $f_n(x)$ is not uniformly convergent. However, I was wondering why couldn't we take $n$ to infinity. If $0 < x < 1$, then $\lim_{n\rightarrow \infty} |f_n(x) - f(x)| = 0$ (which is less than $\epsilon$). Now since $|f_n(x) - f(x)|$ will always be $0$ if $n > \infty$, then wouldn't this be uniformly convergent?",,"['real-analysis', 'elementary-number-theory', 'convergence-divergence']"
29,Evaluating: $\lim_{n\to\infty} \frac{\sqrt n}{\sqrt {2}^{n}}\int_{0}^{\frac{\pi}{2}} (\sin x+\cos x)^n dx $,Evaluating:,\lim_{n\to\infty} \frac{\sqrt n}{\sqrt {2}^{n}}\int_{0}^{\frac{\pi}{2}} (\sin x+\cos x)^n dx ,"I'm supposed to compute the following limit: $$\lim_{n\to\infty} \frac{\sqrt n}{\sqrt {2}^{n}}\int_{0}^{\frac{\pi}{2}} (\sin x+\cos x)^n dx  $$ I'm looking for a resonable approach in this case, if possible. Thanks.","I'm supposed to compute the following limit: $$\lim_{n\to\infty} \frac{\sqrt n}{\sqrt {2}^{n}}\int_{0}^{\frac{\pi}{2}} (\sin x+\cos x)^n dx  $$ I'm looking for a resonable approach in this case, if possible. Thanks.",,"['real-analysis', 'integration', 'limits']"
30,Show that $\operatorname{int}(A \cap B)= \operatorname{int}(A) \cap \operatorname{int}(B)$,Show that,\operatorname{int}(A \cap B)= \operatorname{int}(A) \cap \operatorname{int}(B),"It's kind of a simple proof (I think) but I´m stuck! I have to show that $\operatorname{int} (A \cap B)=\operatorname{int} (A) \cap \operatorname{int}(B)$. (The interior point of the intersection is the intersection of the interior point.) I thought like this: Intersection: there's a point that is both in $A$ and $B$, so there is a point $x$, so $\exists ε>0$ such $(x-ε,x+ε) \subset A \cap B$.I don´t know if this is right. Now $\operatorname{int} (A) \cap \operatorname{int}(B)$, but again with the definition ,there is a point that is in both sets,there's an interior point that is in both sets,an $x$ such $(x-ε,x+ε)\subset A \cap B$. There we have the equality. I think it may be wrong. Please, I'm confused!","It's kind of a simple proof (I think) but I´m stuck! I have to show that $\operatorname{int} (A \cap B)=\operatorname{int} (A) \cap \operatorname{int}(B)$. (The interior point of the intersection is the intersection of the interior point.) I thought like this: Intersection: there's a point that is both in $A$ and $B$, so there is a point $x$, so $\exists ε>0$ such $(x-ε,x+ε) \subset A \cap B$.I don´t know if this is right. Now $\operatorname{int} (A) \cap \operatorname{int}(B)$, but again with the definition ,there is a point that is in both sets,there's an interior point that is in both sets,an $x$ such $(x-ε,x+ε)\subset A \cap B$. There we have the equality. I think it may be wrong. Please, I'm confused!",,['real-analysis']
31,Power series $x f''(x) + f'(x) + xf(x) = 0$,Power series,x f''(x) + f'(x) + xf(x) = 0,"Find a power series with radius of convergence $R = \infty$ such that $$f(x) = \sum_{n=1}^{\infty} a_{n}x^{n}$$ satisfies $$x f''(x) + f'(x) + xf(x)= 0, \forall \mbox{ } x \in \mathbb R.$$ How should I go about solving this question? I have a gut feeling that it has to do with trig functions because I know that the terms in each power series have to cancel out in the resulting addition to satisfy the equation.","Find a power series with radius of convergence $R = \infty$ such that $$f(x) = \sum_{n=1}^{\infty} a_{n}x^{n}$$ satisfies $$x f''(x) + f'(x) + xf(x)= 0, \forall \mbox{ } x \in \mathbb R.$$ How should I go about solving this question? I have a gut feeling that it has to do with trig functions because I know that the terms in each power series have to cancel out in the resulting addition to satisfy the equation.",,"['real-analysis', 'ordinary-differential-equations', 'special-functions', 'power-series']"
32,Measurable functions with the same integral over a set,Measurable functions with the same integral over a set,,"Suppose $f:\mathbb R \to \mathbb R,g:\mathbb R \to \mathbb R $ are Lebesgue measurable with $\int_{\mathbb R}f(x)=\int_{\mathbb R}g(x)=1$. How to show that for every $r\in(0,1)$, there is a measurable $E \subset \mathbb R$ such that $\int_{ E}f(x)=\int_{ E}g(x)=r$?","Suppose $f:\mathbb R \to \mathbb R,g:\mathbb R \to \mathbb R $ are Lebesgue measurable with $\int_{\mathbb R}f(x)=\int_{\mathbb R}g(x)=1$. How to show that for every $r\in(0,1)$, there is a measurable $E \subset \mathbb R$ such that $\int_{ E}f(x)=\int_{ E}g(x)=r$?",,"['real-analysis', 'measure-theory']"
33,Functions of Bounded and Unbounded Variations,Functions of Bounded and Unbounded Variations,,"I have two questions, I would like to  be helped in. Here they are: Show that $$f(x) = \begin{cases} x^2\sin\left(\frac{1}{x^2}\right) &\mbox{if } x \neq 0\\ 0 & \mbox{if } x = 0. \end{cases} $$ is not of bounded variation on $[-1,1]$. Show that $$f(x) = \begin{cases} x^2\sin\left(\frac{1}{x}\right) &\mbox{if } x \neq 0\\ 0 & \mbox{if } x = 0. \end{cases} $$  is of bounded variation on $[-1,1]$. My Attempt. (for 2.) Let $g(x) = x^2\sin\left(\frac{1}{x}\right) + 2x~~~;~~h(x) = 2x$. Then both $g(x)$ and $h(x)$ are increasing (by the derivative test) on $[-1,1]$. Since $f(x) = g(x) - h(x)$, $f$ is of bounded variation on $[-1,1]$. Is my attempt for (2) okay? For (1) I know I have to show that the total variation of $f$ is unbounded, but I don't how to do that. Any suggestions? Thanks.","I have two questions, I would like to  be helped in. Here they are: Show that $$f(x) = \begin{cases} x^2\sin\left(\frac{1}{x^2}\right) &\mbox{if } x \neq 0\\ 0 & \mbox{if } x = 0. \end{cases} $$ is not of bounded variation on $[-1,1]$. Show that $$f(x) = \begin{cases} x^2\sin\left(\frac{1}{x}\right) &\mbox{if } x \neq 0\\ 0 & \mbox{if } x = 0. \end{cases} $$  is of bounded variation on $[-1,1]$. My Attempt. (for 2.) Let $g(x) = x^2\sin\left(\frac{1}{x}\right) + 2x~~~;~~h(x) = 2x$. Then both $g(x)$ and $h(x)$ are increasing (by the derivative test) on $[-1,1]$. Since $f(x) = g(x) - h(x)$, $f$ is of bounded variation on $[-1,1]$. Is my attempt for (2) okay? For (1) I know I have to show that the total variation of $f$ is unbounded, but I don't how to do that. Any suggestions? Thanks.",,['real-analysis']
34,Sequence that approaches integers,Sequence that approaches integers,,I'm referring to an answer posted on Math Overflow (see the post by fedja on https://mathoverflow.net/questions/59115/a-set-for-which-it-is-hard-to-determine-whether-or-not-it-is-countable ) The question is whether the set of real numbers $a > 1$ so that for $K > 0$ the distance between $K a^n$ and its nearest integer approaches $0$ for $n \to \infty$ is countable. The integers are obviously in that set. However I couldn't come up with a proof that for all other reals the limit does not exist.,I'm referring to an answer posted on Math Overflow (see the post by fedja on https://mathoverflow.net/questions/59115/a-set-for-which-it-is-hard-to-determine-whether-or-not-it-is-countable ) The question is whether the set of real numbers $a > 1$ so that for $K > 0$ the distance between $K a^n$ and its nearest integer approaches $0$ for $n \to \infty$ is countable. The integers are obviously in that set. However I couldn't come up with a proof that for all other reals the limit does not exist.,,"['real-analysis', 'limits']"
35,Calculate the integral $\int\limits_{0}^{1}\frac{x^{1-p}(1-x)^p}{(1+x)^2}dx$,Calculate the integral,\int\limits_{0}^{1}\frac{x^{1-p}(1-x)^p}{(1+x)^2}dx,"Calculate the integral $$\int\limits_{0}^{1}\frac{x^{1-p}(1-x)^p}{(1+x)^2}dx, \; -1<p<2$$ My attempt: I tried to use the Laplace transform $$\mathcal{L}\left \{ \frac{x^{1-p}(1-x)^p}{(1+x)^2} \right \}(s)=\int_0^1 \frac{x^{1-p}(1-x)^p}{(1+x)^2}e^{-sx}dx=\int_0^\infty e^{-sx}\frac{x^{1-p}(1-x)^p}{(1+x)^2}dx=\int_0^\infty e^{-sx}\int_0^\infty e^{-tu}\frac{(tu)^{1-p}[(1-t)+(1-u)]^p}{(1+tu)^2}dudt$$ I don't know what to do next. Is it possible to reduce it to a beta function? This integral is very similar to this function. But I haven't been able to find any substitute",Calculate the integral My attempt: I tried to use the Laplace transform I don't know what to do next. Is it possible to reduce it to a beta function? This integral is very similar to this function. But I haven't been able to find any substitute,"\int\limits_{0}^{1}\frac{x^{1-p}(1-x)^p}{(1+x)^2}dx, \; -1<p<2 \mathcal{L}\left \{ \frac{x^{1-p}(1-x)^p}{(1+x)^2} \right \}(s)=\int_0^1 \frac{x^{1-p}(1-x)^p}{(1+x)^2}e^{-sx}dx=\int_0^\infty e^{-sx}\frac{x^{1-p}(1-x)^p}{(1+x)^2}dx=\int_0^\infty e^{-sx}\int_0^\infty e^{-tu}\frac{(tu)^{1-p}[(1-t)+(1-u)]^p}{(1+tu)^2}dudt","['real-analysis', 'calculus', 'integration']"
36,"Convergence of $\sum \frac{b_n}{n}$ Where $b_n = 1, -1, -1, 1, 1, 1, -1, -1,-1,-1,1,1,1,1,1,....$",Convergence of  Where,"\sum \frac{b_n}{n} b_n = 1, -1, -1, 1, 1, 1, -1, -1,-1,-1,1,1,1,1,1,....","How can I show convergence or divergence of the following sequence? $$\sum \frac{b_n}{n}$$ Where $b_n = 1, -1, -1, 1, 1, 1, -1, -1,-1,-1,1,1,1,1,1,....$ Im not sure how to use any of the standart theorems to show convergence or divergence. Any attempts to use comparison test, or finding an upper bound failed. Any ideas?","How can I show convergence or divergence of the following sequence? Where Im not sure how to use any of the standart theorems to show convergence or divergence. Any attempts to use comparison test, or finding an upper bound failed. Any ideas?","\sum \frac{b_n}{n} b_n = 1, -1, -1, 1, 1, 1, -1, -1,-1,-1,1,1,1,1,1,....","['real-analysis', 'calculus', 'sequences-and-series']"
37,Prove $\pi=\lim_{n\to\infty}2^{4n}\frac{\Gamma ^4(n+3/4)}{\Gamma ^2(2n+1)}$,Prove,\pi=\lim_{n\to\infty}2^{4n}\frac{\Gamma ^4(n+3/4)}{\Gamma ^2(2n+1)},"How could it be proved that $$\pi=\lim_{n\to\infty}2^{4n}\frac{\Gamma ^4(n+3/4)}{\Gamma ^2(2n+1)}?$$ What I tried Let $$L=\lim_{n\to\infty}2^{4n}\frac{\Gamma ^4(n+3/4)}{\Gamma ^2(2n+1)}.$$ Unwinding $\Gamma (n+3/4)$ into a product gives $$\Gamma \left(n+\frac{3}{4}\right)=\Gamma\left(\frac{3}{4}\right)\prod_{k=0}^{n-1}\left(k+\frac{3}{4}\right).$$ Then $$\lim_{n\to\infty}\frac{(2n)!}{4^n}\prod_{k=0}^{n-1}\frac{16}{(3+4k)^2}=\frac{\Gamma ^2(3/4)}{\sqrt{L}}.$$ Since $$\frac{(2n)!}{4^n}\prod_{k=0}^{n-1}\frac{16}{(3+4k)^2}=\prod_{k=1}^n \frac{4k(4k-2)}{(4k-1)^2}$$ for all $n\in\mathbb{N}$ , it follows that $$\prod_{k=1}^\infty \frac{4k(4k-2)}{(4k-1)^2}=\frac{\Gamma ^2(3/4)}{\sqrt{L}}.$$ But note that this actually gives an interesting Wallis-like product: $$\frac{2\cdot 4\cdot 6\cdot 8\cdot 10\cdot 12\cdots}{3\cdot 3\cdot 7\cdot 7\cdot 11\cdot 11\cdots}=\frac{\Gamma ^2(3/4)}{\sqrt{L}}.$$ I'm stuck at the Wallis-like product, though.","How could it be proved that What I tried Let Unwinding into a product gives Then Since for all , it follows that But note that this actually gives an interesting Wallis-like product: I'm stuck at the Wallis-like product, though.",\pi=\lim_{n\to\infty}2^{4n}\frac{\Gamma ^4(n+3/4)}{\Gamma ^2(2n+1)}? L=\lim_{n\to\infty}2^{4n}\frac{\Gamma ^4(n+3/4)}{\Gamma ^2(2n+1)}. \Gamma (n+3/4) \Gamma \left(n+\frac{3}{4}\right)=\Gamma\left(\frac{3}{4}\right)\prod_{k=0}^{n-1}\left(k+\frac{3}{4}\right). \lim_{n\to\infty}\frac{(2n)!}{4^n}\prod_{k=0}^{n-1}\frac{16}{(3+4k)^2}=\frac{\Gamma ^2(3/4)}{\sqrt{L}}. \frac{(2n)!}{4^n}\prod_{k=0}^{n-1}\frac{16}{(3+4k)^2}=\prod_{k=1}^n \frac{4k(4k-2)}{(4k-1)^2} n\in\mathbb{N} \prod_{k=1}^\infty \frac{4k(4k-2)}{(4k-1)^2}=\frac{\Gamma ^2(3/4)}{\sqrt{L}}. \frac{2\cdot 4\cdot 6\cdot 8\cdot 10\cdot 12\cdots}{3\cdot 3\cdot 7\cdot 7\cdot 11\cdot 11\cdots}=\frac{\Gamma ^2(3/4)}{\sqrt{L}}.,"['real-analysis', 'limits', 'gamma-function', 'pi']"
38,Oscillatory integral with absolute value,Oscillatory integral with absolute value,,"Suppose $f:[0,1]\to[0,+\infty)$ is a continuous function. Are there general conditions under which the following limit exists? $$\lim_{n\rightarrow \infty} \int_0^1 |\cos(nf(x))|dx$$ Obviously if $f$ is a constant that is not an integer multiple of $\pi$ the limit does not exist. Does the limit exist if $f'(x)\neq 0$ for all $x \in [0,1]$ ? If so, what if we drop differentiability and only require $f$ not be locally constant? When $f$ is not differentiable, can one still show $$\liminf_{n \rightarrow \infty} \int_0^1 |\cos(nf(x))|dx >0$$ under suitable conditions on $f$ ?","Suppose is a continuous function. Are there general conditions under which the following limit exists? Obviously if is a constant that is not an integer multiple of the limit does not exist. Does the limit exist if for all ? If so, what if we drop differentiability and only require not be locally constant? When is not differentiable, can one still show under suitable conditions on ?","f:[0,1]\to[0,+\infty) \lim_{n\rightarrow \infty} \int_0^1 |\cos(nf(x))|dx f \pi f'(x)\neq 0 x \in [0,1] f f \liminf_{n \rightarrow \infty} \int_0^1 |\cos(nf(x))|dx >0 f","['real-analysis', 'absolute-value', 'oscillatory-integral']"
39,"Example of a bounded linear functional $T:L^ \infty \to \mathbb{R}$ which cannot be expressed by an integral $\int_{[a,b]}fg$ for an integrable $f$",Example of a bounded linear functional  which cannot be expressed by an integral  for an integrable,"T:L^ \infty \to \mathbb{R} \int_{[a,b]}fg f","Let $m$ be the Lebesgue measure on $[a,b]$ . What would be an example of a bounded linear functional $T:L^ \infty \to \mathbb{R}$ which cannot be expressed by an integral $\int_{[a,b]}fg$ for an integrable $f$ ? I know that, if $1 \leq p < \infty$ , then any bounded linear functional $T:L^p \to \mathbb{R}$ can be expressed by an integral $\int_{[a,b]}fg$ for an $f \in L^q$ . (Where $q$ is the conjugate of $p$ .) But the proof I am familiar with relies on the fact that $p$ is finite.","Let be the Lebesgue measure on . What would be an example of a bounded linear functional which cannot be expressed by an integral for an integrable ? I know that, if , then any bounded linear functional can be expressed by an integral for an . (Where is the conjugate of .) But the proof I am familiar with relies on the fact that is finite.","m [a,b] T:L^ \infty \to \mathbb{R} \int_{[a,b]}fg f 1 \leq p < \infty T:L^p \to \mathbb{R} \int_{[a,b]}fg f \in L^q q p p","['real-analysis', 'measure-theory', 'lebesgue-integral', 'lebesgue-measure']"
40,Proving that $\sum_{n=1}^\infty \frac{\sin^2 n}{n^2}=\sum_{n=1}^\infty \frac{\sin n}{n}$.,Proving that .,\sum_{n=1}^\infty \frac{\sin^2 n}{n^2}=\sum_{n=1}^\infty \frac{\sin n}{n},"Proving that $$\sum_{n=1}^\infty \frac{\sin^2 n}{n^2}=\frac{\pi -1}{2}$$ I've known a similar conclusion $$ \sum_{n=1}^\infty \frac{\sin nx}{n}= \begin{cases} \dfrac{\pi - x}{2} & x \in (0, 2\pi),\\  \quad 0 & x = 0, \\ f(x+2\pi) & x \in \Bbb{R}. \end{cases} $$ And one of my classmates found the equation mentioned above by mathematica. I was amazed by the equation $$ \sum_{n=1}^\infty \frac{\sin^2 n}{n^2}=\sum_{n=1}^\infty \frac{\sin n}{n} $$ My attempt \begin{align} \sum_{n=1}^\infty \frac{\sin^2 n}{n^2} & = \sum_{n=1}^\infty \frac{1-\cos 2n}{2n^2} \\ & = \sum_{n=1}^\infty \int_0^1 \frac{\sin 2n\theta}{n} d\theta \\ & = \int_0^1 \sum_{n=1}^\infty \frac{\sin 2n\theta}{n} d\theta \\ & = \int_0^1 \frac{\pi}{2}-\theta \,d\theta \\ & = \frac{\pi-1}{2} \end{align} Oh. Actually I hadn't solved it before I edited this question, but I seemed to have worked it out. So is there any other method to solve this problem? And deeper insights? I've heard that it can be worked out via complex analysis and fourier analysis. Thanks in advance! Added Thanks for your comments! Here is another possible generalization $$ \sum_{n \in \mathbb{Z} } \left[\frac{\sin (n \alpha + \theta)  }{ n \alpha + \theta} \right]^2 = \frac{\pi}{\alpha} \,\, \forall \alpha , \theta \in \mathbb{R} $$ I got stuck on it. For $\theta=0$ we can use the following equation $$ \sum_{n \in \mathbb{Z} }  \frac{\cos n\theta}{n^2} = \frac{\pi^2}{6}-\frac{\pi \theta}{2} + \frac{\theta^2}{4} \,\, \theta \in [0,2\pi] $$ But how to deal with the situation that $\theta \ne 0$ ? Can you give me some hints? Thanks in advance!","Proving that I've known a similar conclusion And one of my classmates found the equation mentioned above by mathematica. I was amazed by the equation My attempt Oh. Actually I hadn't solved it before I edited this question, but I seemed to have worked it out. So is there any other method to solve this problem? And deeper insights? I've heard that it can be worked out via complex analysis and fourier analysis. Thanks in advance! Added Thanks for your comments! Here is another possible generalization I got stuck on it. For we can use the following equation But how to deal with the situation that ? Can you give me some hints? Thanks in advance!","\sum_{n=1}^\infty \frac{\sin^2 n}{n^2}=\frac{\pi -1}{2}  \sum_{n=1}^\infty \frac{\sin nx}{n}= \begin{cases}
\dfrac{\pi - x}{2} & x \in (0, 2\pi),\\ 
\quad 0 & x = 0, \\
f(x+2\pi) & x \in \Bbb{R}.
\end{cases}  
\sum_{n=1}^\infty \frac{\sin^2 n}{n^2}=\sum_{n=1}^\infty \frac{\sin n}{n}
 \begin{align}
\sum_{n=1}^\infty \frac{\sin^2 n}{n^2}
& = \sum_{n=1}^\infty \frac{1-\cos 2n}{2n^2} \\
& = \sum_{n=1}^\infty \int_0^1 \frac{\sin 2n\theta}{n} d\theta \\
& = \int_0^1 \sum_{n=1}^\infty \frac{\sin 2n\theta}{n} d\theta \\
& = \int_0^1 \frac{\pi}{2}-\theta \,d\theta \\
& = \frac{\pi-1}{2}
\end{align} 
\sum_{n \in \mathbb{Z} } \left[\frac{\sin (n \alpha + \theta)  }{ n \alpha + \theta} \right]^2 = \frac{\pi}{\alpha} \,\, \forall \alpha , \theta \in \mathbb{R}
 \theta=0 
\sum_{n \in \mathbb{Z} }  \frac{\cos n\theta}{n^2} = \frac{\pi^2}{6}-\frac{\pi \theta}{2} + \frac{\theta^2}{4} \,\, \theta \in [0,2\pi]
 \theta \ne 0","['real-analysis', 'sequences-and-series', 'complex-analysis', 'power-series', 'fourier-series']"
41,Example of norm on $\mathbb{R}^2$ that's NOT absolutely monotonic.,Example of norm on  that's NOT absolutely monotonic.,\mathbb{R}^2,"We call a norm $\|\cdot\|$ on $\mathbb{R}^n$ absolutely monotonic if $$ |a_i| \leq |b_i|, i=1,\cdots,n \implies \|a\| \leq \|b\|. $$ What's an example of norm on $\mathbb{R}^2$ that's not absolutely monotonic? This is an exercise left to the reader -- so presumably not too difficult. But it's giving me some trouble. The usual suspects that come to mind, i.e. the $\ell^p$ norms, $p \in [1,\infty]$ , are all absolutely monotonic.","We call a norm on absolutely monotonic if What's an example of norm on that's not absolutely monotonic? This is an exercise left to the reader -- so presumably not too difficult. But it's giving me some trouble. The usual suspects that come to mind, i.e. the norms, , are all absolutely monotonic.","\|\cdot\| \mathbb{R}^n 
|a_i| \leq |b_i|, i=1,\cdots,n \implies \|a\| \leq \|b\|.
 \mathbb{R}^2 \ell^p p \in [1,\infty]","['real-analysis', 'normed-spaces']"
42,"Riesz representation theorem for $C([0,1])$",Riesz representation theorem for,"C([0,1])","I’m trying to prove the special case of Riesz representation theorem: Every positive (non-negative on non-negative functions) linear continuous functional $\phi$ on the normed space $C([0,1])$ is given by some measure $\mu$ by the rule: $\phi\left(f\right)=\int_{\left[0,1\right]}fd\mu$ I want to do it with using measure extension theorem: First I need to build $\mu$ on elementary sets. But I don't know what it should be like. Can you help me with this? (for open interval, for example)","I’m trying to prove the special case of Riesz representation theorem: Every positive (non-negative on non-negative functions) linear continuous functional on the normed space is given by some measure by the rule: I want to do it with using measure extension theorem: First I need to build on elementary sets. But I don't know what it should be like. Can you help me with this? (for open interval, for example)","\phi C([0,1]) \mu \phi\left(f\right)=\int_{\left[0,1\right]}fd\mu \mu","['real-analysis', 'functional-analysis', 'measure-theory', 'riesz-representation-theorem', 'stieltjes-integral']"
43,Proving that the set of continuous nowhere differentiable functions is dense using Baire's Category Theorem,Proving that the set of continuous nowhere differentiable functions is dense using Baire's Category Theorem,,"I'm trying to prove the next problem: Let $C([0,1],\mathbb{R})$ the space of continuous function $f:[0,1]\to \mathbb{R}$ with the supremum(uniform convergence) metric and let $\mathbb{B}\subset C([0,1],\mathbb{R})$ be the subset of continuous nowhere differentiable functions. I have to show that B contains a countable intersection of dense open sets. In order to do that, we consider the set: $$A_{n}:=\{f\in C([0,1],\mathbb{R}): \forall t\in [0,1]\space \exists h \space s.t \mid \frac{f(t+h)-f(t)}{h}\mid > n \}$$ And then, if we prove: $A_{n}$ is open in $C([0,1],\mathbb{R})$ $A_{n}$ is dense in $C([0,1],\mathbb{R})$ Then we can conclude that $\mathbb{B}$ contains a countable intersection of open dense subsets. Finally, this means that the set $\mathbb{B}$ is dense because of the Baire's category theorem . I've already proven 1) and 2) but I cant get to the conclusion. It is probably a very elemental thing. I hope you can help.","I'm trying to prove the next problem: Let the space of continuous function with the supremum(uniform convergence) metric and let be the subset of continuous nowhere differentiable functions. I have to show that B contains a countable intersection of dense open sets. In order to do that, we consider the set: And then, if we prove: is open in is dense in Then we can conclude that contains a countable intersection of open dense subsets. Finally, this means that the set is dense because of the Baire's category theorem . I've already proven 1) and 2) but I cant get to the conclusion. It is probably a very elemental thing. I hope you can help.","C([0,1],\mathbb{R}) f:[0,1]\to \mathbb{R} \mathbb{B}\subset C([0,1],\mathbb{R}) A_{n}:=\{f\in C([0,1],\mathbb{R}): \forall t\in [0,1]\space \exists h \space s.t \mid \frac{f(t+h)-f(t)}{h}\mid > n \} A_{n} C([0,1],\mathbb{R}) A_{n} C([0,1],\mathbb{R}) \mathbb{B} \mathbb{B}","['real-analysis', 'metric-spaces', 'baire-category']"
44,"How does this nursery rhyme pertain to power series: “There was a little girl Who had a little curl Right in the middle of her forehead...""","How does this nursery rhyme pertain to power series: “There was a little girl Who had a little curl Right in the middle of her forehead...""",,"This is from The Way of Analysis by Strichartz, chapter $7$ , section $7.4$ , page $276$ . He writes ""In discussing power series it is good to recall a nursery rhyme:"" ""There was a little girl Who had a little curl Right in the middle of her forehead When she was good She was very, very good But when she was bad She was horrid."" I don't understand why or how this nursery rhyme is related to power series. Kudos to the comment by Barry Cipra. The rest of the poem can be seen here . I wonder if the rest of the poem can be turned into having some mathematical significance.","This is from The Way of Analysis by Strichartz, chapter , section , page . He writes ""In discussing power series it is good to recall a nursery rhyme:"" ""There was a little girl Who had a little curl Right in the middle of her forehead When she was good She was very, very good But when she was bad She was horrid."" I don't understand why or how this nursery rhyme is related to power series. Kudos to the comment by Barry Cipra. The rest of the poem can be seen here . I wonder if the rest of the poem can be turned into having some mathematical significance.",7 7.4 276,"['real-analysis', 'soft-question', 'power-series', 'education', 'motivation']"
45,How the Bayes rule for density functions is formulated in probability theory?,How the Bayes rule for density functions is formulated in probability theory?,,"Given a probability space $\left( \Omega\mathcal{,F,}\mathbb{P} \right)$, and two $\mathcal{F}$-measurable real-valued random variables $X,Y$, then the joint random variable $\left( X,Y \right)$ can be defined on a product space $\left( \Omega^{2},\sigma\left( \mathcal{F}^{2} \right),\mathbb{P \times P} \right)$ where $\mathbb{P \times P}$ is the product measure of $\mathbb{P}$. Let $f\left( x,y \right),f_{X}\left( x,y \right),f_{Y}\left( y \right)$ be the density functions (Randon-Nikodym derivatives) of $\left( X,Y \right),X,Y$ respectively, and let $f_{X|Y}\left( x,y \right)$ be the density function of $X$ conditioned on $Y$. Anyone can help with a construction, or proof or related materials about the Bayes rule $f_{X|Y}\left( x|y \right) = \frac{f\left( x,y \right)}{f_{Y}\left( y \right)}$?  We may also instead consider the other version $f_{X|Y}\left( x|y \right) = \frac{f_{Y|X}\left( y|x \right)f_{X}\left( x \right)}{f_{Y}\left( y \right)}$ which does involve the joint random variable. I do not understand how the this Bayes rule is formulated in measure theory. This is a widely used formula, while I cannot find any construction or proof from my probability books. I can find related definition for ""conditional density"" in the following way. There could be other definitions. We denote the integration w.r.t. the measure   $\mathbb{P \circ}X^{- 1}$ of a RV as   $\int_{B}^{}{dX} := \int_{B}^{}{d\left( \mathbb{P \circ}X^{- 1} \right)}$   for simplicity. Define the conditional probability measures $\mathbb{P}_{y},y \in Y\left( \Omega \right)$ as a family of probability   measures on $\left( \Omega\mathcal{,F} \right)$ s.t. two axioms hold: 1)   $\mathbb{P}_{y}\left( A \right)$ is   $\left( \mathbb{R,}\mathcal{B}\left( \mathbb{R} \right) \right)$-measurable   for any $A \in \mathcal{F}$ (given a fixed   $A \in \mathcal{F}$,$\ \mathbb{P}_{y}\left( A \right)$ is a   $\mathbb{R \rightarrow}\left\lbrack 0,1 \right\rbrack$ function w.r.t.   index $y$); and 2) the general version of law of total   probability $$\int_{B}^{}{\mathbb{P}_{y}\left( A \right)dY}\mathbb{= P}\left( A\bigcap Y^{- 1}\left( B \right) \right),\forall A \in \mathcal{F}, B \in \mathcal{B}\left ( \mathbb R \right)$$ We then denote   $\mathbb{P}\left( A|Y = y \right) = \mathbb{P}_{y}\left( A \right),\forall A \in \mathcal{F}$   as the conditional probability measure given event $Y = y$. Then for any RV $X$, the conditional probability density function $f_{X|Y}\left( x|y \right)$ is the Radon-Nikodym derivative of   distribution $\mathbb{P}_{y} \circ X^{- 1}$ I list all relations I can conceive, based on above definition, $$\int_{B}^{}{\mathbb{P}_{y}\left( A \right)dY}\mathbb{= P}\left( A\bigcap Y^{- 1}\left( B \right) \right),\forall A\mathcal{\in F,}B \in \mathcal{B}\left( \mathbb{R} \right)$$ $$\int_{B}^{}{\mathbb{P}_{x}\left( A \right)dY}\mathbb{= P}\left( A\bigcap X^{- 1}\left( B \right) \right),\forall A\mathcal{\in F,}X \in \mathcal{B}\left( \mathbb{R} \right)$$ $$\int_{B}^{}{f_{X|Y}\left( x|y \right)} = \mathbb{P}_{y}\left\{ X^{- 1}\left( B \right) \right\},\forall B \in \mathcal{B}\left( \mathbb{R} \right)$$ $$\int_{B}^{}{f_{Y|X}\left( y|x \right)} = \mathbb{P}_{x}\left\{ X^{- 1}\left( B \right) \right\},\forall B \in \mathcal{B}\left( \mathbb{R} \right)$$ $$\int_{B}^{}{f_{Y}\left( y \right)} = \mathbb{P}\left\{ Y^{- 1}\left( B \right) \right\},\forall B \in \mathcal{B}\left( \mathbb{R} \right)$$ $$\int_{B}^{}{f_{X}\left( x \right)} = \mathbb{P}\left\{ X^{- 1}\left( B \right) \right\},\forall B \in \mathcal{B}\left( \mathbb{R} \right)$$","Given a probability space $\left( \Omega\mathcal{,F,}\mathbb{P} \right)$, and two $\mathcal{F}$-measurable real-valued random variables $X,Y$, then the joint random variable $\left( X,Y \right)$ can be defined on a product space $\left( \Omega^{2},\sigma\left( \mathcal{F}^{2} \right),\mathbb{P \times P} \right)$ where $\mathbb{P \times P}$ is the product measure of $\mathbb{P}$. Let $f\left( x,y \right),f_{X}\left( x,y \right),f_{Y}\left( y \right)$ be the density functions (Randon-Nikodym derivatives) of $\left( X,Y \right),X,Y$ respectively, and let $f_{X|Y}\left( x,y \right)$ be the density function of $X$ conditioned on $Y$. Anyone can help with a construction, or proof or related materials about the Bayes rule $f_{X|Y}\left( x|y \right) = \frac{f\left( x,y \right)}{f_{Y}\left( y \right)}$?  We may also instead consider the other version $f_{X|Y}\left( x|y \right) = \frac{f_{Y|X}\left( y|x \right)f_{X}\left( x \right)}{f_{Y}\left( y \right)}$ which does involve the joint random variable. I do not understand how the this Bayes rule is formulated in measure theory. This is a widely used formula, while I cannot find any construction or proof from my probability books. I can find related definition for ""conditional density"" in the following way. There could be other definitions. We denote the integration w.r.t. the measure   $\mathbb{P \circ}X^{- 1}$ of a RV as   $\int_{B}^{}{dX} := \int_{B}^{}{d\left( \mathbb{P \circ}X^{- 1} \right)}$   for simplicity. Define the conditional probability measures $\mathbb{P}_{y},y \in Y\left( \Omega \right)$ as a family of probability   measures on $\left( \Omega\mathcal{,F} \right)$ s.t. two axioms hold: 1)   $\mathbb{P}_{y}\left( A \right)$ is   $\left( \mathbb{R,}\mathcal{B}\left( \mathbb{R} \right) \right)$-measurable   for any $A \in \mathcal{F}$ (given a fixed   $A \in \mathcal{F}$,$\ \mathbb{P}_{y}\left( A \right)$ is a   $\mathbb{R \rightarrow}\left\lbrack 0,1 \right\rbrack$ function w.r.t.   index $y$); and 2) the general version of law of total   probability $$\int_{B}^{}{\mathbb{P}_{y}\left( A \right)dY}\mathbb{= P}\left( A\bigcap Y^{- 1}\left( B \right) \right),\forall A \in \mathcal{F}, B \in \mathcal{B}\left ( \mathbb R \right)$$ We then denote   $\mathbb{P}\left( A|Y = y \right) = \mathbb{P}_{y}\left( A \right),\forall A \in \mathcal{F}$   as the conditional probability measure given event $Y = y$. Then for any RV $X$, the conditional probability density function $f_{X|Y}\left( x|y \right)$ is the Radon-Nikodym derivative of   distribution $\mathbb{P}_{y} \circ X^{- 1}$ I list all relations I can conceive, based on above definition, $$\int_{B}^{}{\mathbb{P}_{y}\left( A \right)dY}\mathbb{= P}\left( A\bigcap Y^{- 1}\left( B \right) \right),\forall A\mathcal{\in F,}B \in \mathcal{B}\left( \mathbb{R} \right)$$ $$\int_{B}^{}{\mathbb{P}_{x}\left( A \right)dY}\mathbb{= P}\left( A\bigcap X^{- 1}\left( B \right) \right),\forall A\mathcal{\in F,}X \in \mathcal{B}\left( \mathbb{R} \right)$$ $$\int_{B}^{}{f_{X|Y}\left( x|y \right)} = \mathbb{P}_{y}\left\{ X^{- 1}\left( B \right) \right\},\forall B \in \mathcal{B}\left( \mathbb{R} \right)$$ $$\int_{B}^{}{f_{Y|X}\left( y|x \right)} = \mathbb{P}_{x}\left\{ X^{- 1}\left( B \right) \right\},\forall B \in \mathcal{B}\left( \mathbb{R} \right)$$ $$\int_{B}^{}{f_{Y}\left( y \right)} = \mathbb{P}\left\{ Y^{- 1}\left( B \right) \right\},\forall B \in \mathcal{B}\left( \mathbb{R} \right)$$ $$\int_{B}^{}{f_{X}\left( x \right)} = \mathbb{P}\left\{ X^{- 1}\left( B \right) \right\},\forall B \in \mathcal{B}\left( \mathbb{R} \right)$$",,"['real-analysis', 'probability', 'probability-theory', 'measure-theory']"
46,Understanding a definition of convergence,Understanding a definition of convergence,,"I have been trying to understand the definition of convergence, but one small detail in the definition is bother me. The detail is '$n \geq N$' So the definition I am using is $(a_{n}) \rightarrow a$ if for every $\epsilon>0 , \exists N \in \mathbb{N}$ such that whenever $n \geq N$ it follows that $|a_{n}-a|< \epsilon$ So in my view it seems like a sequence converges if you can pick a point in the sequence s.t. after this point in the sequence the sequence will KEEP staying inside the epsilon neighborhood. Just to stay concrete suppose $a_{n}=\frac{1}{n}$ then when I look at ''$\exists N \in \mathbb{N}$ s.t. whenever $n \geq N$'' I get confused. If I pick $\epsilon = \frac {1}{10}$ Then the sequence is inside the epsilon neighborhood whenever $n>10$ and here my question is what does $n \geq N$ means? does it mean that this $N$ is the index ? so we have that $a_{3}(n) \geq a_{2}(N)$ or is the meaning that the output you get when you plugin the number into the sequence so we have $a_{2}(n) \geq a_{100}(N)$? because $1/2 \geq 1/100$?","I have been trying to understand the definition of convergence, but one small detail in the definition is bother me. The detail is '$n \geq N$' So the definition I am using is $(a_{n}) \rightarrow a$ if for every $\epsilon>0 , \exists N \in \mathbb{N}$ such that whenever $n \geq N$ it follows that $|a_{n}-a|< \epsilon$ So in my view it seems like a sequence converges if you can pick a point in the sequence s.t. after this point in the sequence the sequence will KEEP staying inside the epsilon neighborhood. Just to stay concrete suppose $a_{n}=\frac{1}{n}$ then when I look at ''$\exists N \in \mathbb{N}$ s.t. whenever $n \geq N$'' I get confused. If I pick $\epsilon = \frac {1}{10}$ Then the sequence is inside the epsilon neighborhood whenever $n>10$ and here my question is what does $n \geq N$ means? does it mean that this $N$ is the index ? so we have that $a_{3}(n) \geq a_{2}(N)$ or is the meaning that the output you get when you plugin the number into the sequence so we have $a_{2}(n) \geq a_{100}(N)$? because $1/2 \geq 1/100$?",,['real-analysis']
47,Cantor set is compact?,Cantor set is compact?,,"Cantor set See the link, I am referring to cantor set on the real line. I wish to show that it is compact. I am doing this by pointing following arguments. I am not sure if this is enough. Cantor set is bounded by definition in the region $[0,1]$ Cantor set is the union of closed intervals, and hence it is a closed set. Since the Cantor set is both bounded and closed it is compact by Heine-Borel Theorem.","Cantor set See the link, I am referring to cantor set on the real line. I wish to show that it is compact. I am doing this by pointing following arguments. I am not sure if this is enough. Cantor set is bounded by definition in the region Cantor set is the union of closed intervals, and hence it is a closed set. Since the Cantor set is both bounded and closed it is compact by Heine-Borel Theorem.","[0,1]","['real-analysis', 'solution-verification', 'cantor-set']"
48,Is there an intuitive explanation for the equality of mixed partials?,Is there an intuitive explanation for the equality of mixed partials?,,"The fact that the mixed second order partial derivatives of a $C^2$ smooth scalar valued function are equal seems, to me, quite surprising. For example, if you interpret $\frac{\partial ^2f}{\partial y \partial x}$ as the change of the slope of a tangent line along the $x$-axis when moving along the $y$-axis, it's not at all obvious to me that this should equal $\frac{\partial ^2f}{\partial x \partial y}$. Does there exist some intuitive or visual way to explain this equality? Just to be clear, I'm not looking for a formal proof (this can be found in most textbooks), but some intuitive reason or hint as to why this might be true.","The fact that the mixed second order partial derivatives of a $C^2$ smooth scalar valued function are equal seems, to me, quite surprising. For example, if you interpret $\frac{\partial ^2f}{\partial y \partial x}$ as the change of the slope of a tangent line along the $x$-axis when moving along the $y$-axis, it's not at all obvious to me that this should equal $\frac{\partial ^2f}{\partial x \partial y}$. Does there exist some intuitive or visual way to explain this equality? Just to be clear, I'm not looking for a formal proof (this can be found in most textbooks), but some intuitive reason or hint as to why this might be true.",,"['real-analysis', 'partial-derivative', 'intuition']"
49,Find $f(1)$ from equation $\int_{0}^{x}f(t)dt = x+\int_{x}^{1}tf(t)dt$,Find  from equation,f(1) \int_{0}^{x}f(t)dt = x+\int_{x}^{1}tf(t)dt,"My brother brought me this same question, also on this website . $$\int_{0}^{x}f(t)dt = x+\int_{x}^{1}tf(t)dt \tag{1}$$ When I try to solve it, I also did same thing. Differentiation leads to $$f(x) = 1-xf(x)\\  f(x) = \frac{1}{1+x} \\ \int_{0}^{1}f(x) dx = \ln 2 $$ But try putting $x = 1$ in original equation $(1)$, $$\int_{0}^{1} f(t) dt = 1$$ so what has happened? We get two contradictory results.  Is it still true that $f(1) = 1/2$? Where is the mistake?","My brother brought me this same question, also on this website . $$\int_{0}^{x}f(t)dt = x+\int_{x}^{1}tf(t)dt \tag{1}$$ When I try to solve it, I also did same thing. Differentiation leads to $$f(x) = 1-xf(x)\\  f(x) = \frac{1}{1+x} \\ \int_{0}^{1}f(x) dx = \ln 2 $$ But try putting $x = 1$ in original equation $(1)$, $$\int_{0}^{1} f(t) dt = 1$$ so what has happened? We get two contradictory results.  Is it still true that $f(1) = 1/2$? Where is the mistake?",,"['calculus', 'real-analysis', 'integration']"
50,Prove convergence / divergence of $\sum_{n=2}^\infty(-1)^n\frac {\sqrt n}{(-1)^n+\sqrt n}\sin\left(\frac {1}{\sqrt n}\right)$,Prove convergence / divergence of,\sum_{n=2}^\infty(-1)^n\frac {\sqrt n}{(-1)^n+\sqrt n}\sin\left(\frac {1}{\sqrt n}\right),"How can I prove or disprove the following series converges? $$\sum_{n=2}^\infty(-1)^n\cfrac {\sqrt n}{(-1)^n+\sqrt n}\sin\left(\frac {1}{\sqrt n}\right)$$ I tried several things, none of which worked. I wanted to use Abel's test or Dirichlet's test. I know that $\sin(\frac {1}{\sqrt n})$ is monotonically decreasing to $0$, but I wasn't able to show that $\Sigma_{n=2}^\infty(-1)^n\frac {\sqrt n}{(-1)^n+\sqrt n}$ is convergent, as it does not converge absolutely since $\frac {\sqrt n}{(-1)^n+\sqrt n}$ converges to 1. Neither was I able to show that the partial sum sequence of $\frac {\sqrt n}{(-1)^n+\sqrt n}$ is bounded. I'm at a loss. Would love any help. Note - This exact question was discussed here a few years ago, but was not answered then and the hint provided in the responses was not useful.","How can I prove or disprove the following series converges? $$\sum_{n=2}^\infty(-1)^n\cfrac {\sqrt n}{(-1)^n+\sqrt n}\sin\left(\frac {1}{\sqrt n}\right)$$ I tried several things, none of which worked. I wanted to use Abel's test or Dirichlet's test. I know that $\sin(\frac {1}{\sqrt n})$ is monotonically decreasing to $0$, but I wasn't able to show that $\Sigma_{n=2}^\infty(-1)^n\frac {\sqrt n}{(-1)^n+\sqrt n}$ is convergent, as it does not converge absolutely since $\frac {\sqrt n}{(-1)^n+\sqrt n}$ converges to 1. Neither was I able to show that the partial sum sequence of $\frac {\sqrt n}{(-1)^n+\sqrt n}$ is bounded. I'm at a loss. Would love any help. Note - This exact question was discussed here a few years ago, but was not answered then and the hint provided in the responses was not useful.",,"['calculus', 'real-analysis', 'sequences-and-series']"
51,Is the pasting lemma also true in case of uniform continuity?,Is the pasting lemma also true in case of uniform continuity?,,"Let $\left(X, d_X \right)$ and  $\left( Y, d_Y \right)$ be metric spaces; let $A$ and $B$ be (non-empty) closed subsets of $X$ such that $X = A \cup B$; let $f \colon A \to Y$ and $g \colon B \to Y$ be uniformly continuous functions such that $f(x) = g(x)$ for all $x \in A \cap B$; and, let the function $h \colon X \to Y$ be defined as follows: $$ h(x) \colon= \begin{cases} f(x) \ & \mbox{ if } \ x \in A, \\ g(x) \ & \mbox{ if } \ x \in B. \end{cases} $$ Then is $h$ also uniformly continuous on $X$? I know that $h$ is continuous, even if both $f$ and $g$ were merely continuous, which is Theorem 18.3 (The Pasting Lemma) in the book Topology by James R. Munkres, 2nd edition. My Attempt: Let $\varepsilon > 0$ be given. We need to find a real number $\delta > 0$ such that $$ d_Y \left(  h \left( x \right) , h \left( x^\prime \right) \right)  < \varepsilon $$    for every pair of points $x, x^\prime \in X$ for which    $$ d_X \left( x, x^\prime \right) < \delta. $$ Now if $x, x^\prime \in A$ or if $x, x^\prime \in B$, then this is of course possible. What if $x \in A \setminus B$ and $x^\prime \in B \setminus A$?","Let $\left(X, d_X \right)$ and  $\left( Y, d_Y \right)$ be metric spaces; let $A$ and $B$ be (non-empty) closed subsets of $X$ such that $X = A \cup B$; let $f \colon A \to Y$ and $g \colon B \to Y$ be uniformly continuous functions such that $f(x) = g(x)$ for all $x \in A \cap B$; and, let the function $h \colon X \to Y$ be defined as follows: $$ h(x) \colon= \begin{cases} f(x) \ & \mbox{ if } \ x \in A, \\ g(x) \ & \mbox{ if } \ x \in B. \end{cases} $$ Then is $h$ also uniformly continuous on $X$? I know that $h$ is continuous, even if both $f$ and $g$ were merely continuous, which is Theorem 18.3 (The Pasting Lemma) in the book Topology by James R. Munkres, 2nd edition. My Attempt: Let $\varepsilon > 0$ be given. We need to find a real number $\delta > 0$ such that $$ d_Y \left(  h \left( x \right) , h \left( x^\prime \right) \right)  < \varepsilon $$    for every pair of points $x, x^\prime \in X$ for which    $$ d_X \left( x, x^\prime \right) < \delta. $$ Now if $x, x^\prime \in A$ or if $x, x^\prime \in B$, then this is of course possible. What if $x \in A \setminus B$ and $x^\prime \in B \setminus A$?",,"['real-analysis', 'analysis', 'metric-spaces', 'continuity', 'uniform-continuity']"
52,Convergence of $\sum_{n=0}^{\infty}\sin(x\pi n!)$,Convergence of,\sum_{n=0}^{\infty}\sin(x\pi n!),"Since this question is closed, I'm asking it myself. Let $M=\{x\in \mathbb R|\sum_{n=0}^{\infty}\sin(x\pi n!)\text{ converges}\}$ Prove that there is no $a,b\in \mathbb R$ such that $(a,b)\subset M$ (ie $M$ has empty interior). It has been proved that $e\in M$ , and it's easy to prove that $\mathbb Q \subset M$ (essentially because if $n\in \mathbb N$ , then $\sin(n\pi)=0$ ). I haven't managed to prove $M$ has empty interior. The idea would be to point out a dense subset of $M^c$ . Noting that $\sin(x\pi n!)\approx 0 \iff \exists k_n\in \mathbb Z, x\approx \frac{k_n}{n!}$ , we're looking for $x$ 's that are badly approximated by rationals. A result by Liouville states that if $x$ is an irrational algebraic number of degree $m$ , there exists some $c_x$ such that for all $p,q\in \mathbb Z$ , $\left|x-\frac pq \right|> \frac{c_x}{q^m}$ . In our case, if $x$ is an irrational algebraic number of degree $m$ , then for each $n$ and any $p\in \mathbb Z$ , $$\left|xn!\pi -p\pi \right|>\frac{c_x\pi}{(n!)^{m-1}}$$ Hence $|\sin(xn!\pi)|>\left|\sin\left(\frac{c_x\pi}{(n!)^{m-1}}\right) \right|$ . However, nothing may be derived from this lower bound (the RHS goes to $0$ too fast).","Since this question is closed, I'm asking it myself. Let Prove that there is no such that (ie has empty interior). It has been proved that , and it's easy to prove that (essentially because if , then ). I haven't managed to prove has empty interior. The idea would be to point out a dense subset of . Noting that , we're looking for 's that are badly approximated by rationals. A result by Liouville states that if is an irrational algebraic number of degree , there exists some such that for all , . In our case, if is an irrational algebraic number of degree , then for each and any , Hence . However, nothing may be derived from this lower bound (the RHS goes to too fast).","M=\{x\in \mathbb R|\sum_{n=0}^{\infty}\sin(x\pi n!)\text{ converges}\} a,b\in \mathbb R (a,b)\subset M M e\in M \mathbb Q \subset M n\in \mathbb N \sin(n\pi)=0 M M^c \sin(x\pi n!)\approx 0 \iff \exists k_n\in \mathbb Z, x\approx \frac{k_n}{n!} x x m c_x p,q\in \mathbb Z \left|x-\frac pq \right|> \frac{c_x}{q^m} x m n p\in \mathbb Z \left|xn!\pi -p\pi \right|>\frac{c_x\pi}{(n!)^{m-1}} |\sin(xn!\pi)|>\left|\sin\left(\frac{c_x\pi}{(n!)^{m-1}}\right) \right| 0","['real-analysis', 'sequences-and-series', 'diophantine-approximation']"
53,How to prove that the series $\sum\limits_{n=1}^\infty \left(\left(n+\frac12\right)\ln\left(1+\frac1n\right) - 1 \right)$ converges,How to prove that the series  converges,\sum\limits_{n=1}^\infty \left(\left(n+\frac12\right)\ln\left(1+\frac1n\right) - 1 \right),"How would you show that the following series is convergent: $$\sum_{n=1}^\infty  \left(\left(n+\frac12\right)\ln\left(1+\frac1n\right) - 1 \right)$$ Here is what I have tried so far: I can show that the terms tend to zero, but have been unsuccessful in showing that the series is Cauchy as this seems to amount to just showing that the series as a whole converges (i.e. it does no to simplify the problem) The ratio test does not give an answer The integral test might work, but I have been unable to evaluate the integral Wolfram Alpha suggests the comparison test, so I have tried the following: Comparison with $\frac1{n^2}$ (plotting the graphs, I know that this is a 'correct' comparison) but I cannot get the answer out) Comparison by using the inequality $\ln(1+\frac1n) \le \frac1n$ but this only shows that the series is less than the divergent harmonic series, so has not helped Taylor series expansion for log... though this initially seemed promising, it got a bit messy, and crucially, I wanted to avoid this method as this has not been covered in our analysis course thus far, so the question should not require it I have also tried rewriting it by collecting together the log terms in the sum to get that the series sum is equal to $\lim\limits_{n\to \infty}\ln(\frac{n^n e^{-n}}{n!})$, which seems reminiscent of Stirling's approximation - however, our lecturer said that this could be used to show that Stirling's approximation converges (I can see how to do this part), so I would rather avoid a proof that directly makes use of Stirling's approximation (the implication seems to be that there is a more 'elementary' way to show convergence). I wanted to work out the answer myself, but have got to the point where I feel I am staring at the question but am unable to make much progress. For this reason, with any solutions, could you possibly include the 'steps'/'clues' that led you to the solution, and how similar questions could be approached. Thanks... Edit: Initially I was a bit wary of using anything involving Taylor polynomials as it is not something that has been remotely touched on in lectures (only the 'obvious' properties of functions such as log have been used/manipulated in inequalities), but now that I think about it, log is (usually) just defined in terms of its series expansion (correct me if I'm wrong here... I know it is also sometimes defined in terms of an integral). Hence, I feel that answers involving Taylor polynomial are good too... nevertheless, given the fact that none of this has been used in any of the other questions or examples from lectures, I am still wondering if there is another way of proving convergence just using inequality manipulation and only 'basic' logarithm inequalities. ... Also, whilst I am very grateful for all answers, I am interested to see how the problem may be solved in different ways so as to give me more tools examples to help solve further problems","How would you show that the following series is convergent: $$\sum_{n=1}^\infty  \left(\left(n+\frac12\right)\ln\left(1+\frac1n\right) - 1 \right)$$ Here is what I have tried so far: I can show that the terms tend to zero, but have been unsuccessful in showing that the series is Cauchy as this seems to amount to just showing that the series as a whole converges (i.e. it does no to simplify the problem) The ratio test does not give an answer The integral test might work, but I have been unable to evaluate the integral Wolfram Alpha suggests the comparison test, so I have tried the following: Comparison with $\frac1{n^2}$ (plotting the graphs, I know that this is a 'correct' comparison) but I cannot get the answer out) Comparison by using the inequality $\ln(1+\frac1n) \le \frac1n$ but this only shows that the series is less than the divergent harmonic series, so has not helped Taylor series expansion for log... though this initially seemed promising, it got a bit messy, and crucially, I wanted to avoid this method as this has not been covered in our analysis course thus far, so the question should not require it I have also tried rewriting it by collecting together the log terms in the sum to get that the series sum is equal to $\lim\limits_{n\to \infty}\ln(\frac{n^n e^{-n}}{n!})$, which seems reminiscent of Stirling's approximation - however, our lecturer said that this could be used to show that Stirling's approximation converges (I can see how to do this part), so I would rather avoid a proof that directly makes use of Stirling's approximation (the implication seems to be that there is a more 'elementary' way to show convergence). I wanted to work out the answer myself, but have got to the point where I feel I am staring at the question but am unable to make much progress. For this reason, with any solutions, could you possibly include the 'steps'/'clues' that led you to the solution, and how similar questions could be approached. Thanks... Edit: Initially I was a bit wary of using anything involving Taylor polynomials as it is not something that has been remotely touched on in lectures (only the 'obvious' properties of functions such as log have been used/manipulated in inequalities), but now that I think about it, log is (usually) just defined in terms of its series expansion (correct me if I'm wrong here... I know it is also sometimes defined in terms of an integral). Hence, I feel that answers involving Taylor polynomial are good too... nevertheless, given the fact that none of this has been used in any of the other questions or examples from lectures, I am still wondering if there is another way of proving convergence just using inequality manipulation and only 'basic' logarithm inequalities. ... Also, whilst I am very grateful for all answers, I am interested to see how the problem may be solved in different ways so as to give me more tools examples to help solve further problems",,"['real-analysis', 'sequences-and-series', 'inequality', 'convergence-divergence']"
54,Construct a merely finitely additive measure on a $\sigma$-algebra,Construct a merely finitely additive measure on a -algebra,\sigma,"Is it possible to give an explicit construction of a set function, defined on a $\sigma$-algebra, with all the properties of a measure except that it is merely finitely additive and not countably additive? Let me elaborate. By ""explicit"" I mean that the example should not appeal to non-constructive methods like the Hahn-Banach theorem or the existence of free ultrafilters. I'm aware that such examples exist, but I'm looking for something more concrete. If such constructions are not possible, I'm especially interested in understanding why that is so. This question is similar, but, so far as I can tell, not identical to several other questions asked on this site and MO. For example, I've learned that proving the existence of the ""integer lottery"" on $P(\mathbb{N})$ requires the Axiom of Choice ( https://mathoverflow.net/questions/95954/how-to-construct-a-continuous-finite-additive-measure-on-the-natural-numbers ). That's the sort of result I'm interested in, but it doesn't fully answer my question. My question doesn't require that the $\sigma$-algebra in question be $P(\Omega)$, and I'm interested in general $\Omega$, not just $\Omega = \mathbb{N}$.","Is it possible to give an explicit construction of a set function, defined on a $\sigma$-algebra, with all the properties of a measure except that it is merely finitely additive and not countably additive? Let me elaborate. By ""explicit"" I mean that the example should not appeal to non-constructive methods like the Hahn-Banach theorem or the existence of free ultrafilters. I'm aware that such examples exist, but I'm looking for something more concrete. If such constructions are not possible, I'm especially interested in understanding why that is so. This question is similar, but, so far as I can tell, not identical to several other questions asked on this site and MO. For example, I've learned that proving the existence of the ""integer lottery"" on $P(\mathbb{N})$ requires the Axiom of Choice ( https://mathoverflow.net/questions/95954/how-to-construct-a-continuous-finite-additive-measure-on-the-natural-numbers ). That's the sort of result I'm interested in, but it doesn't fully answer my question. My question doesn't require that the $\sigma$-algebra in question be $P(\Omega)$, and I'm interested in general $\Omega$, not just $\Omega = \mathbb{N}$.",,"['real-analysis', 'measure-theory', 'set-theory']"
55,"Bounds on $f(k ;a,b) =\frac{ \int_0^\infty \cos(a x) e^{-x^k} \, dx}{ \int_0^\infty \cos(b x) e^{-x^k}\, dx}$",Bounds on,"f(k ;a,b) =\frac{ \int_0^\infty \cos(a x) e^{-x^k} \, dx}{ \int_0^\infty \cos(b x) e^{-x^k}\, dx}","Suppose we define a function  \begin{align} f(k ;a,b) =\frac{ \int_0^\infty \cos(a x) e^{-x^k} \,dx}{ \int_0^\infty \cos(b x) e^{-x^k} \,dx} \end{align} can we show that  \begin{align} |f(k ;a,b)| \le 1 \end{align} for $ 0<k \le 2$  and $a\ge b$? This question was motivated by the discussion here . Note that for $k=1$ and $k=2$ this can be done, since \begin{align}  \int_0^\infty \cos(a x) e^{-x^1} \,dx=\frac{1}{1+a^2}\\  \int_0^\infty \cos(a x) e^{-x^2} \,dx=\frac{\sqrt{\pi}}{2}e^{-a^2/4}\\ \end{align} So, we have that  \begin{align} f(1;a,b)&=\frac{1+b^2}{1+a^2} \\ f(2;a,b)&=e^{ \frac{b^2-a^2}{4}} \end{align} In which case, we have that the conjectured bound is true. Edit : The bounty was posted specifically to address this question and a question raised by Jack D'Aurizio in the comments. The question is: Let  \begin{align} g_k(z)=\int_0^\infty \cos(zx) e^{-x^k} dx \end{align} What is the largest value of $k$ such that $g_k(z)$ is non-negative and decreasing for $z\in \mathbb{R}^{+}$?","Suppose we define a function  \begin{align} f(k ;a,b) =\frac{ \int_0^\infty \cos(a x) e^{-x^k} \,dx}{ \int_0^\infty \cos(b x) e^{-x^k} \,dx} \end{align} can we show that  \begin{align} |f(k ;a,b)| \le 1 \end{align} for $ 0<k \le 2$  and $a\ge b$? This question was motivated by the discussion here . Note that for $k=1$ and $k=2$ this can be done, since \begin{align}  \int_0^\infty \cos(a x) e^{-x^1} \,dx=\frac{1}{1+a^2}\\  \int_0^\infty \cos(a x) e^{-x^2} \,dx=\frac{\sqrt{\pi}}{2}e^{-a^2/4}\\ \end{align} So, we have that  \begin{align} f(1;a,b)&=\frac{1+b^2}{1+a^2} \\ f(2;a,b)&=e^{ \frac{b^2-a^2}{4}} \end{align} In which case, we have that the conjectured bound is true. Edit : The bounty was posted specifically to address this question and a question raised by Jack D'Aurizio in the comments. The question is: Let  \begin{align} g_k(z)=\int_0^\infty \cos(zx) e^{-x^k} dx \end{align} What is the largest value of $k$ such that $g_k(z)$ is non-negative and decreasing for $z\in \mathbb{R}^{+}$?",,"['real-analysis', 'integration', 'fourier-analysis', 'integral-inequality']"
56,Is there a nonabelian topological group operation on the reals?,Is there a nonabelian topological group operation on the reals?,,"Inspired by A binary operation, closed over the reals, that is associative, but not commutative . That question asks for a noncommutative semigroup operation on $\Bbb R$, for which right projection is a continuous solution. If you ask for inverses as well, that is, a nonabelian group operation, then you can use bijection tricks from any other nonabelian group of cardinality $\frak c$, for example the group $M_2(\Bbb R)$ of $2\times 2$ real matrices. But this will not usually give a continuous group operation on $\Bbb R$ because the bijection is usually exotic. To ""upper bound"" the properties needed, according to @PseudoNeo , if we require that the group operation be not just continuous but $C_1$, then it is necessarily of the form $x\ast y=\phi^{−1}(\phi(x)+\phi(y))$ for some $C_1$ diffeomorphism $\phi$, which is manifestly abelian. (Anyone have a reference for this result?) My question lies between these extremes: Is there a nonabelian topological group operation on the reals? That is, a group operation $\ast$ such that $\ast:\Bbb R\times\Bbb R\to\Bbb R$ is continuous and so is ${}^{-1}:\Bbb R\to\Bbb R$.","Inspired by A binary operation, closed over the reals, that is associative, but not commutative . That question asks for a noncommutative semigroup operation on $\Bbb R$, for which right projection is a continuous solution. If you ask for inverses as well, that is, a nonabelian group operation, then you can use bijection tricks from any other nonabelian group of cardinality $\frak c$, for example the group $M_2(\Bbb R)$ of $2\times 2$ real matrices. But this will not usually give a continuous group operation on $\Bbb R$ because the bijection is usually exotic. To ""upper bound"" the properties needed, according to @PseudoNeo , if we require that the group operation be not just continuous but $C_1$, then it is necessarily of the form $x\ast y=\phi^{−1}(\phi(x)+\phi(y))$ for some $C_1$ diffeomorphism $\phi$, which is manifestly abelian. (Anyone have a reference for this result?) My question lies between these extremes: Is there a nonabelian topological group operation on the reals? That is, a group operation $\ast$ such that $\ast:\Bbb R\times\Bbb R\to\Bbb R$ is continuous and so is ${}^{-1}:\Bbb R\to\Bbb R$.",,"['real-analysis', 'general-topology', 'group-theory']"
57,Chain Rule and Vector valued functions?,Chain Rule and Vector valued functions?,,"Let $f: R^n \to R$ be given by $f(x) = \frac{||x||^4} {1 + ||x||^2}$ . Use the chain rule to show that $f$ is differentiable at each $x \in R^n$ and compute $Df(x)$. This vector valued stuff just came out of nowhere. It's not in our book and we went through it super fast... so I, and many others, are completely lost. From what I can guess, each $x$ is a vector of length ""n"", and $||x||$ is supposed to be the norm... Since the question doesn't specify, I'm assuming it's the standard euclidean norm. Also, $D$ must be an n*n matrix of partial derivatives? Assuming all of that is correct, I've been sitting trying to figure out how to make use of the chain rule, but I've got nothing :/","Let $f: R^n \to R$ be given by $f(x) = \frac{||x||^4} {1 + ||x||^2}$ . Use the chain rule to show that $f$ is differentiable at each $x \in R^n$ and compute $Df(x)$. This vector valued stuff just came out of nowhere. It's not in our book and we went through it super fast... so I, and many others, are completely lost. From what I can guess, each $x$ is a vector of length ""n"", and $||x||$ is supposed to be the norm... Since the question doesn't specify, I'm assuming it's the standard euclidean norm. Also, $D$ must be an n*n matrix of partial derivatives? Assuming all of that is correct, I've been sitting trying to figure out how to make use of the chain rule, but I've got nothing :/",,"['real-analysis', 'derivatives']"
58,Applications of the Mean Value Theorem (but not Mean Value Inequality),Applications of the Mean Value Theorem (but not Mean Value Inequality),,"The mean value theorem , found in every calculus textbook since the time of Cauchy (or before), says the following: (MVT) Suppose $f : [a,b] \to \mathbb{R}$ is continuous on $[a,b]$ and differentiable on $(a,b)$.  Then there exists $c \in (a,b)$ such that $f'(c) = \frac{f(b) - f(a)}{b-a}$. An immediate corollary is the mean value inequality : (MVI) Suppose $f : [a,b] \to \mathbb{R}$ is continuous on $[a,b]$ and differentiable on $(a,b)$.  Suppose further than $|f'| \le M$ on $(a,b)$.  Then $|f(b) - f(a)| \le M |b-a|$. MVI is in some sense a weaker statement than MVT, since for instance, analogous versions of MVI hold in higher dimensions, while analogous versions of MVT fail there.  So I  would not really expect that we can use MVI to easily prove MVT. Now, every mathematically interesting consequence of the MVT I've ever seen can actually be proved using MVI.  (By ""mathematically interesting"" I mean to exclude exercises designed specifically to illustrate the theorem, as well as statements like ""there was some time at which your speedometer read exactly 100 km/hr"" that really just restate the result).  In particular, I think MVI can prove all the ""interesting"" statements in the question Applications of the Mean Value Theorem . Are there any mathematically interesting consequences of MVT which cannot be proved from MVI? (You can interpret ""cannot"" in as formal or informal a sense as you want.  Anywhere from precise statements in model theory or reverse mathematics, to ""I don't see any easy way to prove this from MVI"".)","The mean value theorem , found in every calculus textbook since the time of Cauchy (or before), says the following: (MVT) Suppose $f : [a,b] \to \mathbb{R}$ is continuous on $[a,b]$ and differentiable on $(a,b)$.  Then there exists $c \in (a,b)$ such that $f'(c) = \frac{f(b) - f(a)}{b-a}$. An immediate corollary is the mean value inequality : (MVI) Suppose $f : [a,b] \to \mathbb{R}$ is continuous on $[a,b]$ and differentiable on $(a,b)$.  Suppose further than $|f'| \le M$ on $(a,b)$.  Then $|f(b) - f(a)| \le M |b-a|$. MVI is in some sense a weaker statement than MVT, since for instance, analogous versions of MVI hold in higher dimensions, while analogous versions of MVT fail there.  So I  would not really expect that we can use MVI to easily prove MVT. Now, every mathematically interesting consequence of the MVT I've ever seen can actually be proved using MVI.  (By ""mathematically interesting"" I mean to exclude exercises designed specifically to illustrate the theorem, as well as statements like ""there was some time at which your speedometer read exactly 100 km/hr"" that really just restate the result).  In particular, I think MVI can prove all the ""interesting"" statements in the question Applications of the Mean Value Theorem . Are there any mathematically interesting consequences of MVT which cannot be proved from MVI? (You can interpret ""cannot"" in as formal or informal a sense as you want.  Anywhere from precise statements in model theory or reverse mathematics, to ""I don't see any easy way to prove this from MVI"".)",,"['calculus', 'real-analysis', 'derivatives', 'reverse-math']"
59,When are these two definitions equivalent?,When are these two definitions equivalent?,,"I can't remember how to show this but I feel like it must be true: if I have a continuous function $f$, then how do I show that $$\lim_{n\rightarrow \infty}\frac{f(x+\frac{1}{n})-f(x)}{\frac{1}{n}}= c \quad\text{implies}\quad \lim_{\delta\rightarrow 0}\frac{f(x+\delta)-f(x)}{\delta}=c$$ EDIT: Sorry for the confusion, I did indeed mean $n\in \mathbb{N}$.","I can't remember how to show this but I feel like it must be true: if I have a continuous function $f$, then how do I show that $$\lim_{n\rightarrow \infty}\frac{f(x+\frac{1}{n})-f(x)}{\frac{1}{n}}= c \quad\text{implies}\quad \lim_{\delta\rightarrow 0}\frac{f(x+\delta)-f(x)}{\delta}=c$$ EDIT: Sorry for the confusion, I did indeed mean $n\in \mathbb{N}$.",,"['calculus', 'real-analysis']"
60,$\lim_{n \to \infty}f_n(x_n)=f(x)$ if $f_n \to f$ and $x_n \to x$?,if  and ?,\lim_{n \to \infty}f_n(x_n)=f(x) f_n \to f x_n \to x,"$f_1,f_2,...$ is a sequence of continuous functions defined on $\mathbb R$ such that $\forall x \in \mathbb R$ $\lim_{n \to \infty}f_n(x)=f(x)$, and $\{x_n\}\subseteq \mathbb R$ is a sequence converging to $x$. Can we prove that $\lim_{n \to \infty}f_n(x_n)=f(x)$? My attempt (motivated by Show that $f_n(x_n) \to f(x).$ ) \begin{align*}|f_n(x_n)-f(x)|&=|f_n(x_n)-f(x_n)+f(x_n)-f(x)|\\ & \leq |f_n(x_n)-f(x_n)|+|f(x_n)-f(x)|\\ & \leq \sup_{t \in \mathbb{R}}  |f_n(t)-f(t)|+|f(x_n)-f(x)|\\ & \to 0\end{align*} To complete the proof we need to show that $\sup_{t \in \mathbb{R}}  |f_n(t)-f(t)|  \to 0$. Using the definition of convergence, we know that $$\forall \epsilon\, \forall t\, \exists n^*(t) \text{ such that }|f_n(t)-f(t)|<\epsilon\quad \forall n \ge n^*(t)$$ We could complete the proof by showing that the $n*(t)$ are bounded above; that is, we need to show that there exists an integer $n^*$ such that $$n^*=\max_{t \in \mathbb R}n^*(t)$$ since then we will have $$\forall t\, \forall n \ge n^*\quad|f_n(t)-f(t)|< \epsilon$$ and therefore $$\forall n \ge n^*\quad\sup_{t \in \mathbb{R}}  |f_n(t)-f(t)| \le \epsilon$$ How can we show that $n^*$ exists?","$f_1,f_2,...$ is a sequence of continuous functions defined on $\mathbb R$ such that $\forall x \in \mathbb R$ $\lim_{n \to \infty}f_n(x)=f(x)$, and $\{x_n\}\subseteq \mathbb R$ is a sequence converging to $x$. Can we prove that $\lim_{n \to \infty}f_n(x_n)=f(x)$? My attempt (motivated by Show that $f_n(x_n) \to f(x).$ ) \begin{align*}|f_n(x_n)-f(x)|&=|f_n(x_n)-f(x_n)+f(x_n)-f(x)|\\ & \leq |f_n(x_n)-f(x_n)|+|f(x_n)-f(x)|\\ & \leq \sup_{t \in \mathbb{R}}  |f_n(t)-f(t)|+|f(x_n)-f(x)|\\ & \to 0\end{align*} To complete the proof we need to show that $\sup_{t \in \mathbb{R}}  |f_n(t)-f(t)|  \to 0$. Using the definition of convergence, we know that $$\forall \epsilon\, \forall t\, \exists n^*(t) \text{ such that }|f_n(t)-f(t)|<\epsilon\quad \forall n \ge n^*(t)$$ We could complete the proof by showing that the $n*(t)$ are bounded above; that is, we need to show that there exists an integer $n^*$ such that $$n^*=\max_{t \in \mathbb R}n^*(t)$$ since then we will have $$\forall t\, \forall n \ge n^*\quad|f_n(t)-f(t)|< \epsilon$$ and therefore $$\forall n \ge n^*\quad\sup_{t \in \mathbb{R}}  |f_n(t)-f(t)| \le \epsilon$$ How can we show that $n^*$ exists?",,"['real-analysis', 'limits', 'convergence-divergence']"
61,"How to show $\lim_{n \to \infty} \sqrt[n]{a^n+b^n}=\max \{a,b\}$? [duplicate]",How to show ? [duplicate],"\lim_{n \to \infty} \sqrt[n]{a^n+b^n}=\max \{a,b\}","This question already has answers here : Convergence of $\sqrt[n]{x^n+y^n}$ (for $x, y > 0$) (2 answers) Closed 8 years ago . Let $a\geq 0$ and $ b\geq 0$. Prove that $\lim_{n \to \infty} \sqrt[n]{a^n+b^n}=\max \{a,b\}$. [Hint: Use the identity $(a^n -b^n)=(a-b)(\sum_{i=0}^{n-1}a^ib^{n-1-i})$] I need some help! I cannot do it even with the hint... :(","This question already has answers here : Convergence of $\sqrt[n]{x^n+y^n}$ (for $x, y > 0$) (2 answers) Closed 8 years ago . Let $a\geq 0$ and $ b\geq 0$. Prove that $\lim_{n \to \infty} \sqrt[n]{a^n+b^n}=\max \{a,b\}$. [Hint: Use the identity $(a^n -b^n)=(a-b)(\sum_{i=0}^{n-1}a^ib^{n-1-i})$] I need some help! I cannot do it even with the hint... :(",,"['real-analysis', 'sequences-and-series', 'limits', 'radicals', 'means']"
62,Reference request: approximation theory,Reference request: approximation theory,,"I am somewhat intrigued by the ideas behind approximation theory. So, I would like to know (1) what are some thorough clear reference books to get acquainted with approximation theory; (2) what are the necessary prerequisites.","I am somewhat intrigued by the ideas behind approximation theory. So, I would like to know (1) what are some thorough clear reference books to get acquainted with approximation theory; (2) what are the necessary prerequisites.",,"['calculus', 'real-analysis', 'reference-request', 'soft-question', 'approximation-theory']"
63,Proof of $\lim_{n \to \infty} {a_n}^{1/n} = \lim_{n \to \infty}(a_{n+1}/a_n)$ [duplicate],Proof of  [duplicate],\lim_{n \to \infty} {a_n}^{1/n} = \lim_{n \to \infty}(a_{n+1}/a_n),This question already has answers here : Limit of ${a_n}^{1/n}$ is equal to $\lim_{n\to\infty} a_{n+1}/a_n$ (3 answers) Closed 8 years ago . Why would $\lim_{n \to \infty} {a_n}^{1/n} = \lim_{n \to \infty}(a_{n+1}/a_n)$ be true where $(a_n)$ is a sequence in $\mathbb{R}$? Edit: Let all $a_n$ be positive.,This question already has answers here : Limit of ${a_n}^{1/n}$ is equal to $\lim_{n\to\infty} a_{n+1}/a_n$ (3 answers) Closed 8 years ago . Why would $\lim_{n \to \infty} {a_n}^{1/n} = \lim_{n \to \infty}(a_{n+1}/a_n)$ be true where $(a_n)$ is a sequence in $\mathbb{R}$? Edit: Let all $a_n$ be positive.,,"['real-analysis', 'sequences-and-series', 'limits', 'convergence-divergence']"
64,How to conclusively determine the interior of a set,How to conclusively determine the interior of a set,,"I'm fairly confident I understand the meaning of the 'interior' of a set, but I can't figure out how to prove conclusively what said interior is for a given set. Consider this definition: Let $S$ be a subset of $\mathbb{R}$. A point $x\in\mathbb{R}$ is an interior point of $S$ if there exists a neighborhood $N$ of $x$ such that $N\subseteq S$. The set of all interior points of $S$ is denoted by $\operatorname{int} S$ (or in some texts, $S^\circ$) These examples were pulled from practice problems in my textbook, which only gives the final answer without showing why. Given the set $[0,3]\cup(3,5)$, what is the interior? Intuitively I can just look at this and conclude that $[0,3]\cup(3,5)=[0,5)$, and the interior is everything but the boundary points -- namely $(0,5)$. This may be conclusive enough for such a simple example, but what about the sets $S=\{1/n\mid n\in\mathbb{N}\}$ and $T=\{r\in\mathbb{Q}\mid 0<r<\sqrt{2}\}$? I know that $S^0=T^0=\emptyset$, as all the points in $S$ and $T$ are isolated points. But I don't know how to back up that claim. Taking $S$, using the definition I pasted above, I feel like I should try to show that for every $x\in S$, there does not exist a neighborhood of $x$ that is a subset of $S$, but I don't know how to proceed with this.","I'm fairly confident I understand the meaning of the 'interior' of a set, but I can't figure out how to prove conclusively what said interior is for a given set. Consider this definition: Let $S$ be a subset of $\mathbb{R}$. A point $x\in\mathbb{R}$ is an interior point of $S$ if there exists a neighborhood $N$ of $x$ such that $N\subseteq S$. The set of all interior points of $S$ is denoted by $\operatorname{int} S$ (or in some texts, $S^\circ$) These examples were pulled from practice problems in my textbook, which only gives the final answer without showing why. Given the set $[0,3]\cup(3,5)$, what is the interior? Intuitively I can just look at this and conclude that $[0,3]\cup(3,5)=[0,5)$, and the interior is everything but the boundary points -- namely $(0,5)$. This may be conclusive enough for such a simple example, but what about the sets $S=\{1/n\mid n\in\mathbb{N}\}$ and $T=\{r\in\mathbb{Q}\mid 0<r<\sqrt{2}\}$? I know that $S^0=T^0=\emptyset$, as all the points in $S$ and $T$ are isolated points. But I don't know how to back up that claim. Taking $S$, using the definition I pasted above, I feel like I should try to show that for every $x\in S$, there does not exist a neighborhood of $x$ that is a subset of $S$, but I don't know how to proceed with this.",,"['real-analysis', 'general-topology']"
65,Baby Rudin 2.26 Infinite subsets with limit points implies compactness,Baby Rudin 2.26 Infinite subsets with limit points implies compactness,,"Having some trouble with this question. Let $X$ be a metric space in which every infinite subset has a limit   point. Prove that $X$ is compact.  Hint: By Exercises 23 and 24, $X$   has a countable base. It follows that every open cover of $X$ has a   countable subcover ${G_n}$, $n = 1, 2, 3, ....$ If no finite   subcollection of ${G_n}$ covers $X$, then complement $F_n$ of $G_1 \cup \dots \cup G_n$ is nonempty for each $n$, but $\bigcap F_n$ is empty. If $E$   is a set which contains a point from each $F_n$, consider a limit   point of $E$, and obtain a contradiction. I cannot justify the phrase ""It follows that every open cover of $X$ has a countable subcover ${G_n}$, $n = 1, 2, 3, ....$"", am I overlooking something simple? Is there a simple map between the countable base and any open subcover?","Having some trouble with this question. Let $X$ be a metric space in which every infinite subset has a limit   point. Prove that $X$ is compact.  Hint: By Exercises 23 and 24, $X$   has a countable base. It follows that every open cover of $X$ has a   countable subcover ${G_n}$, $n = 1, 2, 3, ....$ If no finite   subcollection of ${G_n}$ covers $X$, then complement $F_n$ of $G_1 \cup \dots \cup G_n$ is nonempty for each $n$, but $\bigcap F_n$ is empty. If $E$   is a set which contains a point from each $F_n$, consider a limit   point of $E$, and obtain a contradiction. I cannot justify the phrase ""It follows that every open cover of $X$ has a countable subcover ${G_n}$, $n = 1, 2, 3, ....$"", am I overlooking something simple? Is there a simple map between the countable base and any open subcover?",,"['real-analysis', 'general-topology', 'metric-spaces', 'compactness']"
66,Radius of convergence of power series $\sum c_n x^{2n}$ and $\sum c_n x^{n^2}$,Radius of convergence of power series  and,\sum c_n x^{2n} \sum c_n x^{n^2},"I've got a start on the question I've written below. I'm hoping for some help to finish it off. Suppose that the power series $\sum_{n=0}^{\infty}c_n x^n$ has a radius of convergence $R \in (0, \infty)$. Find the radii of convergence of the power series $\sum_{n=0}^{\infty}c_n x^{2n}$ and $\sum_{n=0}^{\infty}c_n x^{n^2}$. From Hadamard's Theorem I know that the radius of convergence for $\sum_{n=0}^{\infty}c_n x^n$ is $R=\frac{1}{\alpha}$, where $$\alpha = \limsup_{n \to \infty} |a_n|^{\frac{1}{n}}.$$ Now, applying the Root Test to $\sum_{n=0}^{\infty}c_n x^{2n}$ gives $$\limsup |a_nx^{2n}|^{\frac{1}{n}}=x^2 \cdot \limsup |a_n|^{\frac{1}{n}}=x^2 \alpha$$ which gives a radius of convergence $R_1 = \frac{1}{\sqrt{\alpha}}$. Now for the second power series. My first thought was to take $$\limsup |a_nx^{n^2}|^{\frac{1}{n^2}}=|x| \cdot \limsup |a_n|^{\frac{1}{n^2}}$$ but then I'm stuck. I was trying to write the radius of convergence once again in terms of $\alpha$. Any input appreciated and thanks a bunch.","I've got a start on the question I've written below. I'm hoping for some help to finish it off. Suppose that the power series $\sum_{n=0}^{\infty}c_n x^n$ has a radius of convergence $R \in (0, \infty)$. Find the radii of convergence of the power series $\sum_{n=0}^{\infty}c_n x^{2n}$ and $\sum_{n=0}^{\infty}c_n x^{n^2}$. From Hadamard's Theorem I know that the radius of convergence for $\sum_{n=0}^{\infty}c_n x^n$ is $R=\frac{1}{\alpha}$, where $$\alpha = \limsup_{n \to \infty} |a_n|^{\frac{1}{n}}.$$ Now, applying the Root Test to $\sum_{n=0}^{\infty}c_n x^{2n}$ gives $$\limsup |a_nx^{2n}|^{\frac{1}{n}}=x^2 \cdot \limsup |a_n|^{\frac{1}{n}}=x^2 \alpha$$ which gives a radius of convergence $R_1 = \frac{1}{\sqrt{\alpha}}$. Now for the second power series. My first thought was to take $$\limsup |a_nx^{n^2}|^{\frac{1}{n^2}}=|x| \cdot \limsup |a_n|^{\frac{1}{n^2}}$$ but then I'm stuck. I was trying to write the radius of convergence once again in terms of $\alpha$. Any input appreciated and thanks a bunch.",,"['real-analysis', 'power-series']"
67,Integration theory for Banach-valued functions,Integration theory for Banach-valued functions,,"I am actually studying integration theory for vector-valued functions in a general Banach space, defining the integral with Riemann's sums. Everything seems to work exactly as in the finite dimensional case: Let X be a Banach space, $f,g \colon I = [a,b] \to X$, $\alpha$, $\beta \in \mathbb{R}$ then: $\int_I \alpha f + \beta g = \alpha \int_i f + \beta \int_i g$, $\|\int_I f\| \le \int_I \|f\|$, etc... The fundamental theorem of calculus holds. If $f_n$ are continuous and uniformly convergent to $f$ it is also true that $\lim_n \int_I f_n = \int_I f$. My question is: is there any property that hold only in the finite dimensional case? Is it possible to generalize the construction of the integral as Lebesgue did? If so, does it make sense? Thank you for your help and suggestions","I am actually studying integration theory for vector-valued functions in a general Banach space, defining the integral with Riemann's sums. Everything seems to work exactly as in the finite dimensional case: Let X be a Banach space, $f,g \colon I = [a,b] \to X$, $\alpha$, $\beta \in \mathbb{R}$ then: $\int_I \alpha f + \beta g = \alpha \int_i f + \beta \int_i g$, $\|\int_I f\| \le \int_I \|f\|$, etc... The fundamental theorem of calculus holds. If $f_n$ are continuous and uniformly convergent to $f$ it is also true that $\lim_n \int_I f_n = \int_I f$. My question is: is there any property that hold only in the finite dimensional case? Is it possible to generalize the construction of the integral as Lebesgue did? If so, does it make sense? Thank you for your help and suggestions",,"['real-analysis', 'integration']"
68,Convergence to function that is not measurable,Convergence to function that is not measurable,,"Suppose ($X,\cal M, \mu$) is not complete. Show that there is a sequence {$f_n$} of measurable functions on $X$ that converges pointwise a.e. on $X$ to a function $f$ that is not measurable.","Suppose ($X,\cal M, \mu$) is not complete. Show that there is a sequence {$f_n$} of measurable functions on $X$ that converges pointwise a.e. on $X$ to a function $f$ that is not measurable.",,"['real-analysis', 'functional-analysis', 'measure-theory']"
69,Limit of some integral,Limit of some integral,,"My question is how to find:  $\displaystyle  \lim_{n \rightarrow \infty} \int\limits_0^n \frac {1}{n+n^2\sin(xn^{-2})} dx $? I've tried with a dominated convergence theorem, but it didn't work. Now, I how absolutely no idea what I can do to solve it. Please, help me.","My question is how to find:  $\displaystyle  \lim_{n \rightarrow \infty} \int\limits_0^n \frac {1}{n+n^2\sin(xn^{-2})} dx $? I've tried with a dominated convergence theorem, but it didn't work. Now, I how absolutely no idea what I can do to solve it. Please, help me.",,"['calculus', 'real-analysis', 'integration']"
70,Finite unions and intersections $F_\sigma$ and $G_\delta$ sets,Finite unions and intersections  and  sets,F_\sigma G_\delta,"Why is the intersection of finitely many $F_\sigma$ sets an $F_\sigma$ set, and the union of finitely many $G_\delta$ sets a $G_\delta$ set?","Why is the intersection of finitely many $F_\sigma$ sets an $F_\sigma$ set, and the union of finitely many $G_\delta$ sets a $G_\delta$ set?",,"['real-analysis', 'general-topology']"
71,Compute $\lim_{n\to\infty}\frac{1}{n^3}\sum_{1\le i<j<k\le n} \sin\left(\frac{i+j+k}{n}\right)$,Compute,\lim_{n\to\infty}\frac{1}{n^3}\sum_{1\le i<j<k\le n} \sin\left(\frac{i+j+k}{n}\right),Compute $$\lim_{n\to\infty}\frac{1}{n^3}\sum_{1\le i<j<k\le n} \sin\left(\frac{i+j+k}{n}\right)$$,Compute $$\lim_{n\to\infty}\frac{1}{n^3}\sum_{1\le i<j<k\le n} \sin\left(\frac{i+j+k}{n}\right)$$,,"['calculus', 'real-analysis', 'sequences-and-series', 'limits']"
72,"Bijection from (0,1) to [0,1)","Bijection from (0,1) to [0,1)",,"I'm trying to solve the following question: Let $f:(0,1)\to [0,1)$ and $g:[0,1)\to (0,1)$ be maps defined as $f(x)=x$ and $g(x)=\frac{x+1}{2}$. Use these maps to build a bijection   $h:(0,1)\to [0,1)$ I've already proved that these maps are injectives, and following the others questions on the site such as Continuous bijection from $(0,1)$ to $[0,1]$ How to define a bijection between $(0,1)$ and $(0,1]$? I think I can found such $h$, but the problem is that we have to use only $f$ and $g$ to build $h$. I need help. Thanks a lot.","I'm trying to solve the following question: Let $f:(0,1)\to [0,1)$ and $g:[0,1)\to (0,1)$ be maps defined as $f(x)=x$ and $g(x)=\frac{x+1}{2}$. Use these maps to build a bijection   $h:(0,1)\to [0,1)$ I've already proved that these maps are injectives, and following the others questions on the site such as Continuous bijection from $(0,1)$ to $[0,1]$ How to define a bijection between $(0,1)$ and $(0,1]$? I think I can found such $h$, but the problem is that we have to use only $f$ and $g$ to build $h$. I need help. Thanks a lot.",,"['real-analysis', 'functions']"
73,Find all the continuous functions such that $\int_{-1}^{1}f(x)x^ndx=0$.,Find all the continuous functions such that .,\int_{-1}^{1}f(x)x^ndx=0,"Find all the continuous functions on $[-1,1]$ such that $\int_{-1}^{1}f(x)x^ndx=0$ fof all the even integers $n$. Clearly, if $f$ is an odd function, then it satisfies this condition. What else?","Find all the continuous functions on $[-1,1]$ such that $\int_{-1}^{1}f(x)x^ndx=0$ fof all the even integers $n$. Clearly, if $f$ is an odd function, then it satisfies this condition. What else?",,['real-analysis']
74,Show that a connected metric space is $\epsilon$-chainable for $\epsilon>0$,Show that a connected metric space is -chainable for,\epsilon \epsilon>0,"Show a connected metric space (X,d) is $\epsilon$-chainable for $\epsilon >0$ $\epsilon$-chainable Definition (X,d) is $\epsilon$-chainable if given any two points $a,b \in X$, $\exists$ a finite set $\{ x_0,x_1,...,x_n\}$ of points in X with $x_0 = a$, $x_n=b$ and $d(x_{n-1},x_n)<\epsilon$, $\forall$ n Clopen Definition A set is clopen $iff$ its boundary is empty Connected Metric Definition Let $(X,d)$ be a metric space. It is connected $iff$ it has no proper subsets which are clopen The following are equivalent: $X$ is connected The only subsets of $X$ that are clopen are $\emptyset$ and $X$ $\nexists$ non-empty, disjoint, open sets $A$ and $B$ such that $X=A\bigcup B$ $\nexists$ non-empty, disjoint, closed sets $A$ and $B$ such that $X=A\bigcup B$ Theorem Let $(X,d)$ be a metric space, and let $S$ be any subset of $X$. Then $S$ is closed $iff$ $S^c$ is open. So how would someone show this exactly? Let the set $A$ be the set of all elements in $(X,d)$ that can be reached from $a\in X$ in a finite number of steps with length less than $\epsilon$. Does this mean that there should be no isolated points? (an isolated point would be at distance infinity from any other point, or distance zero from itself, since $\epsilon >0$ we don't want this?) Knowing that $X$ is connected $\implies$the only subsets of X that are clopen are $\emptyset$ and $X$ Also, by construction of A; A is the set of all $\epsilon$-chains between any element and $a$ And, so $A^c = \emptyset$ if $A$ was $\epsilon$-chainable (because A has to have a path from all elements to a, for this to be true, the complement wouldn't have anything in it) Leaving only $A$ as the remaining clopen set Does this make any sense? Or should an alternative be used, and how to formalize it. Thanks.","Show a connected metric space (X,d) is $\epsilon$-chainable for $\epsilon >0$ $\epsilon$-chainable Definition (X,d) is $\epsilon$-chainable if given any two points $a,b \in X$, $\exists$ a finite set $\{ x_0,x_1,...,x_n\}$ of points in X with $x_0 = a$, $x_n=b$ and $d(x_{n-1},x_n)<\epsilon$, $\forall$ n Clopen Definition A set is clopen $iff$ its boundary is empty Connected Metric Definition Let $(X,d)$ be a metric space. It is connected $iff$ it has no proper subsets which are clopen The following are equivalent: $X$ is connected The only subsets of $X$ that are clopen are $\emptyset$ and $X$ $\nexists$ non-empty, disjoint, open sets $A$ and $B$ such that $X=A\bigcup B$ $\nexists$ non-empty, disjoint, closed sets $A$ and $B$ such that $X=A\bigcup B$ Theorem Let $(X,d)$ be a metric space, and let $S$ be any subset of $X$. Then $S$ is closed $iff$ $S^c$ is open. So how would someone show this exactly? Let the set $A$ be the set of all elements in $(X,d)$ that can be reached from $a\in X$ in a finite number of steps with length less than $\epsilon$. Does this mean that there should be no isolated points? (an isolated point would be at distance infinity from any other point, or distance zero from itself, since $\epsilon >0$ we don't want this?) Knowing that $X$ is connected $\implies$the only subsets of X that are clopen are $\emptyset$ and $X$ Also, by construction of A; A is the set of all $\epsilon$-chains between any element and $a$ And, so $A^c = \emptyset$ if $A$ was $\epsilon$-chainable (because A has to have a path from all elements to a, for this to be true, the complement wouldn't have anything in it) Leaving only $A$ as the remaining clopen set Does this make any sense? Or should an alternative be used, and how to formalize it. Thanks.",,"['real-analysis', 'general-topology', 'metric-spaces', 'connectedness']"
75,How to prove these inequalities: $\liminf(a_n + b_n) \leq \liminf(a_n) + \limsup(b_n) \leq \limsup(a_n + b_n)$ [duplicate],How to prove these inequalities:  [duplicate],\liminf(a_n + b_n) \leq \liminf(a_n) + \limsup(b_n) \leq \limsup(a_n + b_n),This question already has answers here : Prove $\limsup\limits_{n \to \infty} (a_n+b_n) \le \limsup\limits_{n \to \infty} a_n + \limsup\limits_{n \to \infty} b_n$ (5 answers) Closed 9 years ago . The inequalities are: $$\liminf(a_n + b_n) \leq \liminf(a_n) + \limsup(b_n) \leq \limsup(a_n + b_n)$$,This question already has answers here : Prove $\limsup\limits_{n \to \infty} (a_n+b_n) \le \limsup\limits_{n \to \infty} a_n + \limsup\limits_{n \to \infty} b_n$ (5 answers) Closed 9 years ago . The inequalities are: $$\liminf(a_n + b_n) \leq \liminf(a_n) + \limsup(b_n) \leq \limsup(a_n + b_n)$$,,"['real-analysis', 'limits', 'inequality', 'limsup-and-liminf']"
76,Baby Rudin Problem 7.16,Baby Rudin Problem 7.16,,"Problem 7.16: Suppose $\{f_n\}$ is an equicontinuous family of functions on a compact set $K$ and $\{f_n\}$ converges pointwise to some $f$ on $K$. Prove that $f_n \to f$ uniformly. Now for this problem I assume that $f_n,f : K \subset \Bbb{R} \rightarrow \Bbb{R}$ with the usual euclidean metric. Even though this is not assumed in the problem, I assume this to simplify matters first. Now I believe I have proven that $f_n$ is uniformly cauchy on $K$ as follows. By equicontinuity of the family $\{f_n\}$ I can choose $\delta> 0$ such that $|x-p_i|< \delta$ will imply that $|f_m(x) - f_m(p_i)| < \epsilon$, and $|f_n(x) - f_n(p_i)| < \epsilon$. Consider the collection $\{B_\delta(x)\}_{x \in K}$ that clearly covers $K$, by compactness of $K$ we get that there are finitely many points $p_1,\ldots p_n \in K$ such that $\{B_\delta(p_i)\}_{i=1}^n$ is a cover for $K$. Furthermore, because $f_n \rightarrow f$ pointwise for each $x \in K$ we get a cauchy sequence of numbers, so in particular given any $\epsilon > 0$, for each $p_i$ there exists $N_i$ such that $m,n \geq N_i$ implies that $|f_m(p_i) - f_n(p_i) | < \epsilon$. Taking $$N = \max_{1 \leq i \leq n} N_i$$ gives that $m,n\geq N$ implies that $|f_m(p_i) - f_n(p_i)| < \epsilon$ for all $i$. Now we can finally put everything together to prove uniform cauchyness, take any $x \in K$ so that $x \in B_\delta(p_j)$ for some $1 \leq j \leq n$. Then $$\begin{eqnarray*} |f_n(x) - f_m(x)| &\leq& |f_n(x) - f_n(p_i) | + | f_n(p_i) - f_m(p_i)| + |f_m(p_i) - f_m(x)| \\ &<& \epsilon + \epsilon + \epsilon \\ &=& 3\epsilon. \end{eqnarray*}$$ The first and last term being less than $\epsilon$ come from equicontinuity, the middle term being less than $\epsilon$ comes from the derivation just before. Now what I am thinking of doing now to prove uniform cauchyness is to take the sup on the left, is this something legal I can do? Also are there are any mistakes in the proof above? Here is some context why I want to prove uniform cauchyness: Suppose I know that $\{f_n\}$ is uniformly cauchy. Then I know that given any $\epsilon > 0$, there exists $N \in \Bbb{N}$ such that $m,n\geq N$ implies that $|f_n(x) - f_m(x)| < \epsilon$ for all $x \in K$. Now we do this trick of fixing one of the indices. Fix $n$ to be some integer greater than $N$ and let $m\rightarrow \infty$, we see that $$\begin{eqnarray*} |f_n - f| &=& \lim_{m\rightarrow \infty} | f_n - f_m| \\ &\leq& \epsilon \end{eqnarray*} $$ by the limit comparison test. Recall that $f$ was the pointwise limit of $\{f_n\}$. But then since $n$ was any arbitrary integer greater than $N$ we have that $f_n \rightarrow f$ uniformly.","Problem 7.16: Suppose $\{f_n\}$ is an equicontinuous family of functions on a compact set $K$ and $\{f_n\}$ converges pointwise to some $f$ on $K$. Prove that $f_n \to f$ uniformly. Now for this problem I assume that $f_n,f : K \subset \Bbb{R} \rightarrow \Bbb{R}$ with the usual euclidean metric. Even though this is not assumed in the problem, I assume this to simplify matters first. Now I believe I have proven that $f_n$ is uniformly cauchy on $K$ as follows. By equicontinuity of the family $\{f_n\}$ I can choose $\delta> 0$ such that $|x-p_i|< \delta$ will imply that $|f_m(x) - f_m(p_i)| < \epsilon$, and $|f_n(x) - f_n(p_i)| < \epsilon$. Consider the collection $\{B_\delta(x)\}_{x \in K}$ that clearly covers $K$, by compactness of $K$ we get that there are finitely many points $p_1,\ldots p_n \in K$ such that $\{B_\delta(p_i)\}_{i=1}^n$ is a cover for $K$. Furthermore, because $f_n \rightarrow f$ pointwise for each $x \in K$ we get a cauchy sequence of numbers, so in particular given any $\epsilon > 0$, for each $p_i$ there exists $N_i$ such that $m,n \geq N_i$ implies that $|f_m(p_i) - f_n(p_i) | < \epsilon$. Taking $$N = \max_{1 \leq i \leq n} N_i$$ gives that $m,n\geq N$ implies that $|f_m(p_i) - f_n(p_i)| < \epsilon$ for all $i$. Now we can finally put everything together to prove uniform cauchyness, take any $x \in K$ so that $x \in B_\delta(p_j)$ for some $1 \leq j \leq n$. Then $$\begin{eqnarray*} |f_n(x) - f_m(x)| &\leq& |f_n(x) - f_n(p_i) | + | f_n(p_i) - f_m(p_i)| + |f_m(p_i) - f_m(x)| \\ &<& \epsilon + \epsilon + \epsilon \\ &=& 3\epsilon. \end{eqnarray*}$$ The first and last term being less than $\epsilon$ come from equicontinuity, the middle term being less than $\epsilon$ comes from the derivation just before. Now what I am thinking of doing now to prove uniform cauchyness is to take the sup on the left, is this something legal I can do? Also are there are any mistakes in the proof above? Here is some context why I want to prove uniform cauchyness: Suppose I know that $\{f_n\}$ is uniformly cauchy. Then I know that given any $\epsilon > 0$, there exists $N \in \Bbb{N}$ such that $m,n\geq N$ implies that $|f_n(x) - f_m(x)| < \epsilon$ for all $x \in K$. Now we do this trick of fixing one of the indices. Fix $n$ to be some integer greater than $N$ and let $m\rightarrow \infty$, we see that $$\begin{eqnarray*} |f_n - f| &=& \lim_{m\rightarrow \infty} | f_n - f_m| \\ &\leq& \epsilon \end{eqnarray*} $$ by the limit comparison test. Recall that $f$ was the pointwise limit of $\{f_n\}$. But then since $n$ was any arbitrary integer greater than $N$ we have that $f_n \rightarrow f$ uniformly.",,['real-analysis']
77,Moments and weak convergence of probability measures,Moments and weak convergence of probability measures,,"I was wondering, if you have a sequence of probability measures $(\mu_n)_n$ on $\mathbb R$ and you know that there is a probability measure $\mu$ such that for all $k\in\mathbb N=\{0,1,2,\cdots\}$  $$ \lim_{n\rightarrow\infty}\int x^kd\mu_n(x)=\int x^kd\mu(x), $$ does it imply that for any continuous and bounded function $f$ you have $$ \lim_{n\rightarrow\infty}\int f(x)d\mu_n(x)=\int f(x)d\mu(x) \qquad ? $$ And if no, what if $\mu$ has compact support ? EDIT : I'm kind of lost : I understand reading the answers that it is somehow necessary that $\mu$ is characterized by its moments, but on an other hand, I come up with this proof, where I don't see what's wrong with. Could you help ? Since $\mu_n$ converges towards $\mu$ in moments, we have by density of the polynomials in $C_c(\mathbb R)$ that for any $h\in C_c(\mathbb R)$ $$ \lim_{n\rightarrow\infty}\int h(x)d\mu_n(x)=\int h(x)d\mu(x). $$ Now, let $f\in C_b(\mathbb R)$, take any $h\in C_c(\mathbb R)$ satisfying $0\leq h \leq 1$, and write $$ \left|\int f(x)d\mu_n(x)-\int f(x)d\mu(x)\right| $$ $$ \leq \left|\int f(x)d\mu_n(x)-\int f(x)h(x)d\mu_n(x)\right|+\left|\int f(x)h(x)d\mu_n(x)-\int f(x)h(x)d\mu(x)\right|+\left|\int f(x)h(x)d\mu(x)-\int f(x)d\mu(x)\right|. $$ Thus, using $\lim_n\mu_n(\mathbb R)=\mu(\mathbb R)$ (i.e the convergence of the ""$0$-th moment""), we obtain $$ \limsup_n\left|\int f(x)d\mu_n(x)-\int f(x)d\mu(x)\right|\leq 2\|f\|_{\infty}\int(1-h(x))d\mu(x). $$ Finally, given $\epsilon >0$, one can choose $g$ such that $$ \int(1-h(x))d\mu(x)\leq \frac{\epsilon}{2\|f\|_{\infty}}, $$ I have the impression that I obtain the result... Where's the mistake ? Thanks in advance !","I was wondering, if you have a sequence of probability measures $(\mu_n)_n$ on $\mathbb R$ and you know that there is a probability measure $\mu$ such that for all $k\in\mathbb N=\{0,1,2,\cdots\}$  $$ \lim_{n\rightarrow\infty}\int x^kd\mu_n(x)=\int x^kd\mu(x), $$ does it imply that for any continuous and bounded function $f$ you have $$ \lim_{n\rightarrow\infty}\int f(x)d\mu_n(x)=\int f(x)d\mu(x) \qquad ? $$ And if no, what if $\mu$ has compact support ? EDIT : I'm kind of lost : I understand reading the answers that it is somehow necessary that $\mu$ is characterized by its moments, but on an other hand, I come up with this proof, where I don't see what's wrong with. Could you help ? Since $\mu_n$ converges towards $\mu$ in moments, we have by density of the polynomials in $C_c(\mathbb R)$ that for any $h\in C_c(\mathbb R)$ $$ \lim_{n\rightarrow\infty}\int h(x)d\mu_n(x)=\int h(x)d\mu(x). $$ Now, let $f\in C_b(\mathbb R)$, take any $h\in C_c(\mathbb R)$ satisfying $0\leq h \leq 1$, and write $$ \left|\int f(x)d\mu_n(x)-\int f(x)d\mu(x)\right| $$ $$ \leq \left|\int f(x)d\mu_n(x)-\int f(x)h(x)d\mu_n(x)\right|+\left|\int f(x)h(x)d\mu_n(x)-\int f(x)h(x)d\mu(x)\right|+\left|\int f(x)h(x)d\mu(x)-\int f(x)d\mu(x)\right|. $$ Thus, using $\lim_n\mu_n(\mathbb R)=\mu(\mathbb R)$ (i.e the convergence of the ""$0$-th moment""), we obtain $$ \limsup_n\left|\int f(x)d\mu_n(x)-\int f(x)d\mu(x)\right|\leq 2\|f\|_{\infty}\int(1-h(x))d\mu(x). $$ Finally, given $\epsilon >0$, one can choose $g$ such that $$ \int(1-h(x))d\mu(x)\leq \frac{\epsilon}{2\|f\|_{\infty}}, $$ I have the impression that I obtain the result... Where's the mistake ? Thanks in advance !",,"['real-analysis', 'analysis', 'measure-theory', 'convergence-divergence']"
78,Continuous increasing function with different Dini derivatives at 0,Continuous increasing function with different Dini derivatives at 0,,"Is there an increasing continuous function $f\colon[0,1]\to\mathbb{R}$ such that the right upper derivative $D^+f(0)$ does not equal the right lower derivative $D_+f(0)$? Recall: $$D^+f(0)=\limsup_{h\downarrow0}\frac{f(0+h)-f(0)}{h},\quad D_f(0)=\liminf_{h\downarrow0}\frac{f(0+h)-f(0)}{h}. $$","Is there an increasing continuous function $f\colon[0,1]\to\mathbb{R}$ such that the right upper derivative $D^+f(0)$ does not equal the right lower derivative $D_+f(0)$? Recall: $$D^+f(0)=\limsup_{h\downarrow0}\frac{f(0+h)-f(0)}{h},\quad D_f(0)=\liminf_{h\downarrow0}\frac{f(0+h)-f(0)}{h}. $$",,"['real-analysis', 'analysis']"
79,Can an Ordered Ring be Considered as a Metric Space?,Can an Ordered Ring be Considered as a Metric Space?,,"Consider an ordered ring $R$. One can define a function $$|\cdot|: R \rightarrow R$$  by $$ |x| = \left\{   \begin{align} x &, x \geq 0 \\ -x&, x < 0 \end{align}    \right. $$ If we set $d(x,y) = |x - y|$, it can be shown that $d$ satisfies: $d(x,y) \geq 0$ and $d(x,y) = 0$ iff $x=y$ $d(x,y) = d(y,x)$ $d(x,z) < d(x,y) + d(y,z)$ Here, $d$ looks very much like a metric except for the fact that $d$ actually produces an element of $R$, not an element $\mathbb{R}$. Otherwise, $(R, d)$ seems to have all of the features one would expect of a metric space. So, is there a way in which $(R, d)$ can be considered a metric space? Using this ""metric"", we can define Cauchy sequences and other items of interest but since the ""metric"" doesn't actually produce an element $\mathbb{R}$, anything that depends on the output of $d$ being a number would fail.","Consider an ordered ring $R$. One can define a function $$|\cdot|: R \rightarrow R$$  by $$ |x| = \left\{   \begin{align} x &, x \geq 0 \\ -x&, x < 0 \end{align}    \right. $$ If we set $d(x,y) = |x - y|$, it can be shown that $d$ satisfies: $d(x,y) \geq 0$ and $d(x,y) = 0$ iff $x=y$ $d(x,y) = d(y,x)$ $d(x,z) < d(x,y) + d(y,z)$ Here, $d$ looks very much like a metric except for the fact that $d$ actually produces an element of $R$, not an element $\mathbb{R}$. Otherwise, $(R, d)$ seems to have all of the features one would expect of a metric space. So, is there a way in which $(R, d)$ can be considered a metric space? Using this ""metric"", we can define Cauchy sequences and other items of interest but since the ""metric"" doesn't actually produce an element $\mathbb{R}$, anything that depends on the output of $d$ being a number would fail.",,"['real-analysis', 'abstract-algebra']"
80,Solutions to the equation $y^{(n)} y = 1$ for even $n$,Solutions to the equation  for even,y^{(n)} y = 1 n,"A long time ago I was curious about the closed-form solutions to the equation: \begin{equation*} \frac{d^{n}y}{dx^n} y = 1. \end{equation*} For $n$ an odd number, try $y = A x^k$. Then $y^{(n)} = A k(k-1)...(k-n)x^{k-n}$. This gives the formula \begin{equation*} A^2 k(k-1)...(k-n) x^{2k - n - 1} = 1 \end{equation*} which can only be true if $k = \frac{n+1}{2}$ and $n$ is odd ($k$ cannot be an integer for the formula to work, check this yourself). Furthermore one has to have that \begin{equation*} A = (k(k-1)...(k-n))^{-1/2} \end{equation*} which is real if $n$ is odd. Thus there are closed-form solutions to my problem for $n$ odd, and my question is if anyone can find a closed-form solution for $n=2$ or in general if $n$ is even.","A long time ago I was curious about the closed-form solutions to the equation: \begin{equation*} \frac{d^{n}y}{dx^n} y = 1. \end{equation*} For $n$ an odd number, try $y = A x^k$. Then $y^{(n)} = A k(k-1)...(k-n)x^{k-n}$. This gives the formula \begin{equation*} A^2 k(k-1)...(k-n) x^{2k - n - 1} = 1 \end{equation*} which can only be true if $k = \frac{n+1}{2}$ and $n$ is odd ($k$ cannot be an integer for the formula to work, check this yourself). Furthermore one has to have that \begin{equation*} A = (k(k-1)...(k-n))^{-1/2} \end{equation*} which is real if $n$ is odd. Thus there are closed-form solutions to my problem for $n$ odd, and my question is if anyone can find a closed-form solution for $n=2$ or in general if $n$ is even.",,"['real-analysis', 'ordinary-differential-equations']"
81,When will the moons and the planet all be on one straight line again?,When will the moons and the planet all be on one straight line again?,,"Suppose we have a planet $P$ (which we will assume to be stationary) and $n \in \mathbb N_{> 1}$ moons $M_1, \ldots, M_n$ orbiting $P$ in a circular fashion, all in the clockwise direction, but at different distances, so that it takes them $t_1, \ldots, t_n \in \mathbb N_{> 0}$ (say $t_1 < \ldots < t_n$ ) time units to circumnavigate $P$ , respectively. Suppose that at time $t = 0$ , there is a straight line segment on which all moons and the planet lies. What is the first time $t > 0$ that this will happen again? (We assume that $P, M_1, \ldots, M_n$ are points without any width, so they can't obstruct each other or influence each others positions in any other way.) The motivation for this question came from an easy question we conceived for 7-th graders: if we instead ask what the earliest time is that the moons will all be again at the same position, the answer is $\hat{t} := \text{lcm}(t_1, \ldots, t_n)$ , the least common multiple of all orbiting times. Unfortunately, it can happen that the planets lie on one line segment with the planet much earlier, even for $n = 2$ . We couldn't figure out any line of attack for the difficult version of this question state above. Maybe an a priori assumption that will simplify this problem is that all moons start off at the same side of the planet, i.e., the line segment $M_1, \ldots, M_n, P$ lie on does not intersect $P$ , it only touches it. I'd be also grateful for any improvements on the statement of the question, thanks to @Albert for the first suggestion in that direction.","Suppose we have a planet (which we will assume to be stationary) and moons orbiting in a circular fashion, all in the clockwise direction, but at different distances, so that it takes them (say ) time units to circumnavigate , respectively. Suppose that at time , there is a straight line segment on which all moons and the planet lies. What is the first time that this will happen again? (We assume that are points without any width, so they can't obstruct each other or influence each others positions in any other way.) The motivation for this question came from an easy question we conceived for 7-th graders: if we instead ask what the earliest time is that the moons will all be again at the same position, the answer is , the least common multiple of all orbiting times. Unfortunately, it can happen that the planets lie on one line segment with the planet much earlier, even for . We couldn't figure out any line of attack for the difficult version of this question state above. Maybe an a priori assumption that will simplify this problem is that all moons start off at the same side of the planet, i.e., the line segment lie on does not intersect , it only touches it. I'd be also grateful for any improvements on the statement of the question, thanks to @Albert for the first suggestion in that direction.","P n \in \mathbb N_{> 1} M_1, \ldots, M_n P t_1, \ldots, t_n \in \mathbb N_{> 0} t_1 < \ldots < t_n P t = 0 t > 0 P, M_1, \ldots, M_n \hat{t} := \text{lcm}(t_1, \ldots, t_n) n = 2 M_1, \ldots, M_n, P P","['real-analysis', 'dynamical-systems', 'recreational-mathematics']"
82,A gamma summation: $\sum_{n=0}^{\infty} \frac{2}{\Gamma ( a + n) \Gamma ( a - n )} = \frac{2^{2a-2}}{\Gamma ( 2a - 1 )} + \frac{1}{\Gamma^2 (a)}$,A gamma summation:,\sum_{n=0}^{\infty} \frac{2}{\Gamma ( a + n) \Gamma ( a - n )} = \frac{2^{2a-2}}{\Gamma ( 2a - 1 )} + \frac{1}{\Gamma^2 (a)},"Let $a \notin \mathbb{Z}$ and $a \neq \frac{1}{2}$ . Prove that $$\sum_{n=0}^{\infty} \frac{2}{\Gamma \left ( a + n \right ) \Gamma \left ( a - n \right )} = \frac{2^{2a-2}}{\Gamma \left ( 2a - 1 \right )} + \frac{1}{\Gamma^2 (a)}$$ Attempt Using the fact that \begin{align*}  \frac{1}{\Gamma\left ( a+x \right ) \Gamma \left ( \beta - x \right )} &= \frac{1}{\left ( a+x-1 \right )! \left ( \beta-x-1 \right )!} \\ &=\frac{1}{\Gamma \left ( a + \beta - 1 \right )} \frac{\left ( a + \beta-2 \right )!}{\left ( a + x -1 \right )! \left ( \beta - x -1 \right )!} \\ &=\frac{1}{\Gamma \left ( a + \beta - 1 \right )} \binom{a + \beta - 2}{a + x -1}  \end{align*} the question really boils down to the sum $$\mathcal{S} = \sum_{n=0}^{\infty} \binom{2a-2}{a+n-1}$$ To this end, \begin{align*}  \sum_{n=0}^{\infty} \binom{2a-1}{a+n-1} &=\frac{1}{2\pi i} \sum_{n=0}^{\infty} \oint \limits_{|z|=1} \frac{\left ( 1+z \right )^{2a-1}}{z^{a+n}}\, \mathrm{d}z \\   &= \frac{1}{2\pi i} \oint \limits_{|z|=1} \frac{\left ( 1 + z \right )^{2a-1}}{z^a} \sum_{n=0}^{\infty} \frac{1}{z^n} \, \mathrm{d}z  \\   &= \frac{1}{2\pi i} \oint \limits_{|z|=1} \frac{\left ( 1+z \right )^{2a-1}}{z^{a-1} \left ( z-1 \right )} \, \mathrm{d}z \end{align*} using the handy identity $\displaystyle \binom{n}{k} = \frac{1}{2\pi i } \oint \limits_{\gamma} \frac{\left ( 1+z \right )^n}{z^{k+1}} \, \mathrm{d}z$ . I think I'm on the right track, but I'm having a difficult time evaluating the last contour integral. Any help?","Let and . Prove that Attempt Using the fact that the question really boils down to the sum To this end, using the handy identity . I think I'm on the right track, but I'm having a difficult time evaluating the last contour integral. Any help?","a \notin \mathbb{Z} a \neq \frac{1}{2} \sum_{n=0}^{\infty} \frac{2}{\Gamma \left ( a + n \right ) \Gamma \left ( a - n \right )} = \frac{2^{2a-2}}{\Gamma \left ( 2a - 1 \right )} + \frac{1}{\Gamma^2 (a)} \begin{align*} 
\frac{1}{\Gamma\left ( a+x \right ) \Gamma \left ( \beta - x \right )} &= \frac{1}{\left ( a+x-1 \right )! \left ( \beta-x-1 \right )!} \\ &=\frac{1}{\Gamma \left ( a + \beta - 1 \right )} \frac{\left ( a + \beta-2 \right )!}{\left ( a + x -1 \right )! \left ( \beta - x -1 \right )!} \\ &=\frac{1}{\Gamma \left ( a + \beta - 1 \right )} \binom{a + \beta - 2}{a + x -1} 
\end{align*} \mathcal{S} = \sum_{n=0}^{\infty} \binom{2a-2}{a+n-1} \begin{align*}
 \sum_{n=0}^{\infty} \binom{2a-1}{a+n-1} &=\frac{1}{2\pi i} \sum_{n=0}^{\infty} \oint \limits_{|z|=1} \frac{\left ( 1+z \right )^{2a-1}}{z^{a+n}}\, \mathrm{d}z \\ 
 &= \frac{1}{2\pi i} \oint \limits_{|z|=1} \frac{\left ( 1 + z \right )^{2a-1}}{z^a} \sum_{n=0}^{\infty} \frac{1}{z^n} \, \mathrm{d}z  \\ 
 &= \frac{1}{2\pi i} \oint \limits_{|z|=1} \frac{\left ( 1+z \right )^{2a-1}}{z^{a-1} \left ( z-1 \right )} \, \mathrm{d}z
\end{align*} \displaystyle \binom{n}{k} = \frac{1}{2\pi i } \oint \limits_{\gamma} \frac{\left ( 1+z \right )^n}{z^{k+1}} \, \mathrm{d}z","['real-analysis', 'sequences-and-series', 'complex-analysis', 'special-functions']"
83,Why is $(1+\frac{1}{n})^n < (1+\frac{1}{m})^{m+1}$?,Why is ?,(1+\frac{1}{n})^n < (1+\frac{1}{m})^{m+1},"Why is it that $$  \left(1+\frac{1}{n}\right)^n < \left(1+\frac{1}{m}\right)^{m+1}, $$ for any natural numbers $m, n$ ? I have tried expanding using the binomial series and splitting into cases. I understand why it is trivially true when $m=n$ but I am not sure if there is a rigorous proof for other cases?",Why is it that for any natural numbers ? I have tried expanding using the binomial series and splitting into cases. I understand why it is trivially true when but I am not sure if there is a rigorous proof for other cases?," 
\left(1+\frac{1}{n}\right)^n < \left(1+\frac{1}{m}\right)^{m+1},
 m, n m=n","['real-analysis', 'calculus', 'sequences-and-series', 'inequality', 'convergence-divergence']"
84,"Terence Tao, Analysis I, Ex. 5.4.5: There is a rational between any two reals","Terence Tao, Analysis I, Ex. 5.4.5: There is a rational between any two reals",,"Terence Tao, Analysis I, 3e, Exercise 5.4.5: Prove Proposition 5.4.14. (Hint: use Exercise 5.4.4. You may also need   to argue by contradiction.) Proposition 5.4.14: Given any two real numbers $x < y$ , we can find a   rational number q such that $x < q < y$ . Exercise 5.4.4: Show that for any positive real number $x > 0$ there   exists a positive integer $N$ such that $x > 1/N > 0$ . What I've found so far (with the help of this answer, and Pratik Apshinge's comment): From Exercise 5.4.4, there is a positive real number $$y - x > 1/N > 0, $$ $$yN - xN > 1, $$ $$yN - 1 > xN.$$ Since there is an integer $m$ between $yN$ and $yN - 1$ , we have that $$yN > m \ge yN - 1 > xN$$ $$yN > m > xN$$ $$y > m/N > x$$ Since $m$ and $N$ are integers, there exists a rational between $y$ and $x$ . But how could a proof by contradiction help in this case?","Terence Tao, Analysis I, 3e, Exercise 5.4.5: Prove Proposition 5.4.14. (Hint: use Exercise 5.4.4. You may also need   to argue by contradiction.) Proposition 5.4.14: Given any two real numbers , we can find a   rational number q such that . Exercise 5.4.4: Show that for any positive real number there   exists a positive integer such that . What I've found so far (with the help of this answer, and Pratik Apshinge's comment): From Exercise 5.4.4, there is a positive real number Since there is an integer between and , we have that Since and are integers, there exists a rational between and . But how could a proof by contradiction help in this case?","x < y x < q < y x > 0 N x > 1/N > 0 y - x > 1/N > 0,  yN - xN > 1,  yN - 1 > xN. m yN yN - 1 yN > m \ge yN - 1 > xN yN > m > xN y > m/N > x m N y x","['real-analysis', 'proof-verification']"
85,Evaluate $\int_{0}^{\frac{\pi}{4}}\tan xdx $ using idea of Riemann Sum,Evaluate  using idea of Riemann Sum,\int_{0}^{\frac{\pi}{4}}\tan xdx ,"Evaluate $$\int_{0}^{\frac{\pi}{4}}\tan x\,dx$$ using Riemann Sum. My Attempt: $$\int_{0}^{\frac{\pi}{4}}\tan x\, dx=\frac{\pi}{4}\int_{0}^1\tan\left(\frac{\pi}{4}x\right)dx=\frac{\pi}{4}\lim_{n\to\infty}\frac{1}{n}\sum_{r=1}^{n}\tan\left(\frac{{\pi}r}{4n}\right)$$ After this I am not able to proceed",Evaluate using Riemann Sum. My Attempt: After this I am not able to proceed,"\int_{0}^{\frac{\pi}{4}}\tan x\,dx \int_{0}^{\frac{\pi}{4}}\tan x\, dx=\frac{\pi}{4}\int_{0}^1\tan\left(\frac{\pi}{4}x\right)dx=\frac{\pi}{4}\lim_{n\to\infty}\frac{1}{n}\sum_{r=1}^{n}\tan\left(\frac{{\pi}r}{4n}\right)","['real-analysis', 'calculus', 'integration', 'definite-integrals']"
86,Is it true that every closed set is a countable intersection of open intervals?,Is it true that every closed set is a countable intersection of open intervals?,,Motivation: We know that every open set is a countable union of open intervals with rational endpoints and that every open interval is a countable union of closed intervals. Hence every open set is a countable union of closed intervals. It follows by De Morgan's laws that every closed set is a countable intersection of open sets. I would like to ask if we can prove a stronger result that every closed set is a countable intersection of open intervals. Thank you for your help!,Motivation: We know that every open set is a countable union of open intervals with rational endpoints and that every open interval is a countable union of closed intervals. Hence every open set is a countable union of closed intervals. It follows by De Morgan's laws that every closed set is a countable intersection of open sets. I would like to ask if we can prove a stronger result that every closed set is a countable intersection of open intervals. Thank you for your help!,,"['real-analysis', 'metric-spaces']"
87,Does $\int_0^\infty \frac{\sin(\sin(x))}{x}dx$ converge or diverge?,Does  converge or diverge?,\int_0^\infty \frac{\sin(\sin(x))}{x}dx,"I'm trying to stud the convergence of $$\int_0^\infty  \frac{\sin(\sin(x))}{x}dx.$$ I really have no idea on how solve this integral. Unfortunately, I can't majorate absolutely $\frac{\sin(\sin(x))}{x}$ by a $L^1(0,\infty )$ function, so I'm thinking that it's doesn't converge, but I'm not able to prove it. Moreover, graphically it looks to converge slower than $x\longmapsto \frac{1}{x}$, so I imagine that it doesn't converge, but since $\frac{\sin(\sin(x))}{x}$ often change of sign, I could be surprised. For the context, it's a question of an exam of 1st year bachelor in mathematic, so we can't use theorem as dominated or monotone convergence theorem. And the argument should also be more or less elementary (I hope).","I'm trying to stud the convergence of $$\int_0^\infty  \frac{\sin(\sin(x))}{x}dx.$$ I really have no idea on how solve this integral. Unfortunately, I can't majorate absolutely $\frac{\sin(\sin(x))}{x}$ by a $L^1(0,\infty )$ function, so I'm thinking that it's doesn't converge, but I'm not able to prove it. Moreover, graphically it looks to converge slower than $x\longmapsto \frac{1}{x}$, so I imagine that it doesn't converge, but since $\frac{\sin(\sin(x))}{x}$ often change of sign, I could be surprised. For the context, it's a question of an exam of 1st year bachelor in mathematic, so we can't use theorem as dominated or monotone convergence theorem. And the argument should also be more or less elementary (I hope).",,"['real-analysis', 'integration', 'improper-integrals']"
88,A limit exists iff and only the left limit and the right limit exist and are equal to each other,A limit exists iff and only the left limit and the right limit exist and are equal to each other,,"It is well known that $$\lim_{x \to a} f(x) = L \iff \lim_{x \to a+}f(x) = L = \lim_{x \to  a-}f(x)$$ Consider the function $\sqrt{.}: \mathbb{R}^+ \to \mathbb{R}$ Now, consider $\lim_{x \to 0} \sqrt{x}$ We can prove this limit is equal to $0$. Indeed, let $\epsilon > 0$. Choose $\delta = \epsilon^2$. Then, for $x \in \mathbb{R}^+$ satysfying $0 < |x| < \delta$ or equivalently $0 < x < \delta$, we have $\sqrt{x} < \sqrt{\delta} = \epsilon$, which establishes the result. However, my confusion lies in the following: the limit from the left does not seem to exist, making the above theorem untrue. Where lies my mistake?","It is well known that $$\lim_{x \to a} f(x) = L \iff \lim_{x \to a+}f(x) = L = \lim_{x \to  a-}f(x)$$ Consider the function $\sqrt{.}: \mathbb{R}^+ \to \mathbb{R}$ Now, consider $\lim_{x \to 0} \sqrt{x}$ We can prove this limit is equal to $0$. Indeed, let $\epsilon > 0$. Choose $\delta = \epsilon^2$. Then, for $x \in \mathbb{R}^+$ satysfying $0 < |x| < \delta$ or equivalently $0 < x < \delta$, we have $\sqrt{x} < \sqrt{\delta} = \epsilon$, which establishes the result. However, my confusion lies in the following: the limit from the left does not seem to exist, making the above theorem untrue. Where lies my mistake?",,['real-analysis']
89,A function with no inflection point,A function with no inflection point,,"Suppose $g:\mathbb{R} \to \mathbb{R}$ is a twice differentiable function with second derivative continuous such that for every $a,b \in \mathbb{R}$ with $a<b$, there exist a unique $c \in (a,b)$ such that $g'(c)=\frac{g(b)-g(a)}{b-a}$. Prove that $f$ has no inflection points. I think this is not correct. Consider the function $f(x)=(x-1)(x-2)(x-3)$ thus function satisfies the above conditions, but we know that between a maxima and a minima lies a point of inflection. Is my reasoning correct? Else can you give a proof?","Suppose $g:\mathbb{R} \to \mathbb{R}$ is a twice differentiable function with second derivative continuous such that for every $a,b \in \mathbb{R}$ with $a<b$, there exist a unique $c \in (a,b)$ such that $g'(c)=\frac{g(b)-g(a)}{b-a}$. Prove that $f$ has no inflection points. I think this is not correct. Consider the function $f(x)=(x-1)(x-2)(x-3)$ thus function satisfies the above conditions, but we know that between a maxima and a minima lies a point of inflection. Is my reasoning correct? Else can you give a proof?",,['calculus']
90,"Extremely ugly integral $\int_{-\pi}^{\pi} \frac{\operatorname{sign}(x)\arctan(x^2)-|x|}{\sin^2(x)+|\cos(x)|}\,dx$",Extremely ugly integral,"\int_{-\pi}^{\pi} \frac{\operatorname{sign}(x)\arctan(x^2)-|x|}{\sin^2(x)+|\cos(x)|}\,dx","Evaluate: $\DeclareMathOperator{\sign}{sign}$ $$\int_{-\pi}^{\pi} \dfrac{\sign(x)\arctan(x^2)-|x|}{\sin^2(x)+|\cos(x)|}\,dx$$ My idea: \begin{align*}I=\int_{-\pi}^{\pi} \frac{\sign(x)\arctan(x^2)-|x|}{\sin^2(x)+|\cos(x)| }\,dx &=\int_{-\pi}^{\pi} \dfrac{\sign(-x)\arctan((-x)^2)-|-x|}{\sin^2(x)+|\cos(x)|}\,dx\\ &=\int_{-\pi}^{\pi} \frac{-\sign(x)\arctan(x^2)-|x|}{ \sin^2(x)+|\cos(x)| }\,dx. \end{align*} So, it means that the integral $$\int_{-\pi}^{\pi} \frac{\sign(x)\arctan(x^2)}{\sin^2(x)+|\cos(x)| }\,dx=0.$$ Therefore, my integral now looks like $$I=\int_{-\pi}^{\pi} \frac{-|x|}{\sin^2(x)+|\cos(x)|}\,dx,$$ and since my integrand is even function, I have: $$I=-2\int_{0}^{\pi} \frac{|x|}{ \sin^2(x)+|\cos(x)| }\,dx=-2\int_{0}^{\pi} \frac{x}{ \sin^2(x)+|\cos(x)| }\,dx.$$ And so, \begin{align*} \frac{I}{2}&=-\int_{0}^{\pi} \frac{x}{ \sin^2(x)+|\cos(x)| }\,dx\\ &=-\int_{0}^{\pi} \frac{\pi-x}{ \sin^2(\pi-x)+|\cos(\pi-x)|}\,dx\\ &=-\pi\int_{0}^{\pi} \frac{dx}{ \sin^2(x)+|\cos(x)|}+\int_{0}^{\pi} \frac{x}{ \sin^2(x)+|\cos(x)| }\,dx\\ &=-\pi\int_{0}^{\pi} \frac{dx}{ \sin^2(x)+|\cos(x)| }-\frac{I}{2}. \end{align*} Hence, $$I=-\pi\int_{0}^{\pi} \dfrac{dx}{ \sin^2(x)+|\cos(x)| }.$$ That is the end of the road. I tried to eliminate absolute value over the cosine but always get some divergent integral. Is this computation correct and if it is, what next? Bonus question: Where can I find more problems like this?","Evaluate: $\DeclareMathOperator{\sign}{sign}$ $$\int_{-\pi}^{\pi} \dfrac{\sign(x)\arctan(x^2)-|x|}{\sin^2(x)+|\cos(x)|}\,dx$$ My idea: \begin{align*}I=\int_{-\pi}^{\pi} \frac{\sign(x)\arctan(x^2)-|x|}{\sin^2(x)+|\cos(x)| }\,dx &=\int_{-\pi}^{\pi} \dfrac{\sign(-x)\arctan((-x)^2)-|-x|}{\sin^2(x)+|\cos(x)|}\,dx\\ &=\int_{-\pi}^{\pi} \frac{-\sign(x)\arctan(x^2)-|x|}{ \sin^2(x)+|\cos(x)| }\,dx. \end{align*} So, it means that the integral $$\int_{-\pi}^{\pi} \frac{\sign(x)\arctan(x^2)}{\sin^2(x)+|\cos(x)| }\,dx=0.$$ Therefore, my integral now looks like $$I=\int_{-\pi}^{\pi} \frac{-|x|}{\sin^2(x)+|\cos(x)|}\,dx,$$ and since my integrand is even function, I have: $$I=-2\int_{0}^{\pi} \frac{|x|}{ \sin^2(x)+|\cos(x)| }\,dx=-2\int_{0}^{\pi} \frac{x}{ \sin^2(x)+|\cos(x)| }\,dx.$$ And so, \begin{align*} \frac{I}{2}&=-\int_{0}^{\pi} \frac{x}{ \sin^2(x)+|\cos(x)| }\,dx\\ &=-\int_{0}^{\pi} \frac{\pi-x}{ \sin^2(\pi-x)+|\cos(\pi-x)|}\,dx\\ &=-\pi\int_{0}^{\pi} \frac{dx}{ \sin^2(x)+|\cos(x)|}+\int_{0}^{\pi} \frac{x}{ \sin^2(x)+|\cos(x)| }\,dx\\ &=-\pi\int_{0}^{\pi} \frac{dx}{ \sin^2(x)+|\cos(x)| }-\frac{I}{2}. \end{align*} Hence, $$I=-\pi\int_{0}^{\pi} \dfrac{dx}{ \sin^2(x)+|\cos(x)| }.$$ That is the end of the road. I tried to eliminate absolute value over the cosine but always get some divergent integral. Is this computation correct and if it is, what next? Bonus question: Where can I find more problems like this?",,"['calculus', 'real-analysis', 'integration', 'riemann-integration', 'trigonometric-integrals']"
91,$ L ^ 2 $ norm of a function and its derivative,norm of a function and its derivative, L ^ 2 ,"Assume that $ f $ is a real $ C ^ 1 $ function, and $ f ( a ) = f ( b ) = 0 $ . Show that $$ \| f \| _ { L ^ 2 ( a , b ) } \le \frac { b - a } 2 \| f ' \| _ { L ^ 2 ( a , b ) } \text . $$ My attempt: I have been mostly trying to do integration by parts and then apply Cauchy-Schwarz. I also tried with mean value theorem but got no luck. Also I found a result that might be useful: Suppose that $ f $ is continuously differentiable on $ [ a , b ] $ and $ f ( a ) = f ( b ) = 0 $ . Then $$ \sup _ { a \le t \le b } | f ( t ) | \le \frac { b - a } 2 \int _ a ^ b | f ' ( t ) | \ \mathrm d t \text . $$","Assume that is a real function, and . Show that My attempt: I have been mostly trying to do integration by parts and then apply Cauchy-Schwarz. I also tried with mean value theorem but got no luck. Also I found a result that might be useful: Suppose that is continuously differentiable on and . Then"," f   C ^ 1   f ( a ) = f ( b ) = 0   \| f \| _ { L ^ 2 ( a , b ) } \le \frac { b - a } 2 \| f ' \| _ { L ^ 2 ( a , b ) } \text .   f   [ a , b ]   f ( a ) = f ( b ) = 0   \sup _ { a \le t \le b } | f ( t ) | \le \frac { b - a } 2 \int _ a ^ b | f ' ( t ) | \ \mathrm d t \text . ","['calculus', 'real-analysis', 'normed-spaces']"
92,Two disjoint convex closed sets that cannot be separated by a closed hyperplane?,Two disjoint convex closed sets that cannot be separated by a closed hyperplane?,,"When I was reading ""Functional Analysis"" written by Brezis, I noticed the following counterexample, saying that two disjoint closed convex sets may not be separated by a closed hyperplane. Let $E=l^{1}$ (the space of real, absolutely convergent series) and define  $$X=\{x=(x_{n})_{n\geq1}\in E:x_{2n}=0 \ \forall n\geq 1\}$$ and  $$Y=\{y=(y_{n})_{n\geq1}\in E:y_{2n}=\frac {1}{2^{n}}y_{2n-1} \ \forall n\geq 1\}$$ (i) Show that $X$ and $Y$ are closed linear spaces and $\overline{X+Y}=E$. (ii) Let $c=(c_{n})_{n\geq1}\in E$ be defined by $c_{2n}=\frac{1}{2^{n}}$ and $c_{2n-1}=0$ for all $n\geq 1$. Check that $c\notin X+Y$. (iii) Set $Z=X-c$ (then $Y\bigcap Z=\emptyset$). Show that $Y$ and $Z$ cannot be separated by a closed hyperplane. I can prove that (i) and (ii) hold. But I'm stuck in part (iii). How can I prove $Y$ and $Z$ cannot be separated by a closed hyperplane? Thanks!","When I was reading ""Functional Analysis"" written by Brezis, I noticed the following counterexample, saying that two disjoint closed convex sets may not be separated by a closed hyperplane. Let $E=l^{1}$ (the space of real, absolutely convergent series) and define  $$X=\{x=(x_{n})_{n\geq1}\in E:x_{2n}=0 \ \forall n\geq 1\}$$ and  $$Y=\{y=(y_{n})_{n\geq1}\in E:y_{2n}=\frac {1}{2^{n}}y_{2n-1} \ \forall n\geq 1\}$$ (i) Show that $X$ and $Y$ are closed linear spaces and $\overline{X+Y}=E$. (ii) Let $c=(c_{n})_{n\geq1}\in E$ be defined by $c_{2n}=\frac{1}{2^{n}}$ and $c_{2n-1}=0$ for all $n\geq 1$. Check that $c\notin X+Y$. (iii) Set $Z=X-c$ (then $Y\bigcap Z=\emptyset$). Show that $Y$ and $Z$ cannot be separated by a closed hyperplane. I can prove that (i) and (ii) hold. But I'm stuck in part (iii). How can I prove $Y$ and $Z$ cannot be separated by a closed hyperplane? Thanks!",,"['real-analysis', 'general-topology', 'functional-analysis']"
93,Good closed form approximation for iterates of $x^2+(1-x^2)x$,Good closed form approximation for iterates of,x^2+(1-x^2)x,"Let $f(x) := x^2+(1-x^2)x$. Is there a nice nontrivial closed form approximation $g_n(x)$ over $[0,1]$ for the $n$-fold composition $f^{\circ n}(x)$? Obviously near $0$ we have that $f^{\circ n}(x) = x+nx^2+...$ but this is not much use to me. Rather than try to pin down what ""nice"" ought to mean, I'll channel Potter Stewart and just say I (and I'm sure also a respondent) would know it upon sight. One might be tempted to mumble ""solve Schroder's equation"" but I don't see how that helps. Nor do I see how computing the Carleman matrix of $f$ helps (but for what it's worth, I believe the matrix elements are $M_{jk} := \sum_{r=0}^j \binom{j}{r} (-1)^{j-r} \binom{r}{k-3j-2r}$). Such tactics are suggested in How would I go about finding a closed form solution for $g(x,n) = f(f(f(...(x))))$, $n$ times?","Let $f(x) := x^2+(1-x^2)x$. Is there a nice nontrivial closed form approximation $g_n(x)$ over $[0,1]$ for the $n$-fold composition $f^{\circ n}(x)$? Obviously near $0$ we have that $f^{\circ n}(x) = x+nx^2+...$ but this is not much use to me. Rather than try to pin down what ""nice"" ought to mean, I'll channel Potter Stewart and just say I (and I'm sure also a respondent) would know it upon sight. One might be tempted to mumble ""solve Schroder's equation"" but I don't see how that helps. Nor do I see how computing the Carleman matrix of $f$ helps (but for what it's worth, I believe the matrix elements are $M_{jk} := \sum_{r=0}^j \binom{j}{r} (-1)^{j-r} \binom{r}{k-3j-2r}$). Such tactics are suggested in How would I go about finding a closed form solution for $g(x,n) = f(f(f(...(x))))$, $n$ times?",,"['real-analysis', 'dynamical-systems', 'function-and-relation-composition']"
94,Is a weakly differentiable function differentiable almost everywhere?,Is a weakly differentiable function differentiable almost everywhere?,,"I am working with Sobolev spaces. Let's suppose $\Omega \subset \mathbb{R}^n$ is an open set. A function $u: \mathbb{R}^n \to \mathbb{R}$ in $L^1(\Omega)$ is said to be weakly differentiable if there exist functions $ g_1,...,g_n $  such  that  $$\qquad\qquad\qquad\qquad\qquad\int_{\Omega}u\varphi_{x_i}=-\int_{\Omega}g_i \varphi \quad \quad   \forall \varphi \in C^{1}_c(\Omega), \forall i=1,...,n. $$ Can every weakly differentiable function be restricted to an open set $\tilde{\Omega}$ such that $u$ is differentiable (classical derivative!) with $m(\Omega - \tilde{\Omega})=0$? I know this is true for dimension $1$. Is it true for every $\Omega \subset \mathbb{R}^n$?","I am working with Sobolev spaces. Let's suppose $\Omega \subset \mathbb{R}^n$ is an open set. A function $u: \mathbb{R}^n \to \mathbb{R}$ in $L^1(\Omega)$ is said to be weakly differentiable if there exist functions $ g_1,...,g_n $  such  that  $$\qquad\qquad\qquad\qquad\qquad\int_{\Omega}u\varphi_{x_i}=-\int_{\Omega}g_i \varphi \quad \quad   \forall \varphi \in C^{1}_c(\Omega), \forall i=1,...,n. $$ Can every weakly differentiable function be restricted to an open set $\tilde{\Omega}$ such that $u$ is differentiable (classical derivative!) with $m(\Omega - \tilde{\Omega})=0$? I know this is true for dimension $1$. Is it true for every $\Omega \subset \mathbb{R}^n$?",,"['real-analysis', 'derivatives', 'sobolev-spaces', 'weak-derivatives']"
95,Evaluation of $\sum_{n=1}^\infty \frac{(-1)^{n-1}\eta(n)}{n} $ without using the Wallis Product,Evaluation of  without using the Wallis Product,\sum_{n=1}^\infty \frac{(-1)^{n-1}\eta(n)}{n} ,"In THIS ANSWER , I showed that $$2\sum_{s=1}^{\infty}\frac{1-\beta(2s+1)}{2s+1}=\ln\left(\frac{\pi}{2}\right)-2+\frac{\pi}{2}$$ where $\beta(s)=\sum_{n=0}^\infty \frac{(-1)^n}{(2n+1)^s}$ is the Dirichlet Beta Function . In the development, it was noted that $$\begin{align} \sum_{n=1}^\infty(-1)^{n-1}\log\left(\frac{n+1}{n}\right)&=\log\left(\frac21\cdot \frac23\cdot \frac43\cdot \frac45\cdots\right)\\\\ &=\log\left(\prod_{n=1}^\infty \frac{2n}{2n-1}\frac{2n}{2n+1}\right)\\\\ &=\log\left(\frac{\pi}{2}\right) \tag 1 \end{align}$$ where I used Wallis's Product for $\pi/2$. If instead of that approach, I had used the Taylor series for the logarithm function, then the analysis would have led to $$\sum_{n=1}^\infty(-1)^{n-1}\log\left(\frac{n+1}{n}\right)=\sum_{n=1}^\infty \frac{(-1)^{n-1}\eta(n)}{n} \tag 2$$ where $\eta(s)=\sum_{n=1}^\infty \frac{(-1)^{n-1}}{n^s}$ is the Dirichlet eta function. Given the series on the right-hand side of $(2)$ as a starting point, it is evident that we could simply reverse steps and arrive at $(1)$. But, what are some other distinct ways that one can take to evaluate the right-hand side of $(2)$? For example, one might try to use the integral representation $$\eta(s)=\frac{1}{\Gamma(s)}\int_0^\infty \frac{x^{s-1}}{1+e^x}\,dx$$ and arrive at $$\sum_{n=1}^\infty \frac{(-1)^{n-1}\eta(n)}{n} =\int_0^\infty \frac{1-e^{-x}}{x(1+e^x)}\,dx =\int_1^\infty \frac{x-1}{x^2(x+1)\log(x)}\,dx \tag 3$$ Yet, neither of these integrals is trivial to evaluate (without reversing the preceding steps). And what are some other ways to handle the integrals in $(3)$?","In THIS ANSWER , I showed that $$2\sum_{s=1}^{\infty}\frac{1-\beta(2s+1)}{2s+1}=\ln\left(\frac{\pi}{2}\right)-2+\frac{\pi}{2}$$ where $\beta(s)=\sum_{n=0}^\infty \frac{(-1)^n}{(2n+1)^s}$ is the Dirichlet Beta Function . In the development, it was noted that $$\begin{align} \sum_{n=1}^\infty(-1)^{n-1}\log\left(\frac{n+1}{n}\right)&=\log\left(\frac21\cdot \frac23\cdot \frac43\cdot \frac45\cdots\right)\\\\ &=\log\left(\prod_{n=1}^\infty \frac{2n}{2n-1}\frac{2n}{2n+1}\right)\\\\ &=\log\left(\frac{\pi}{2}\right) \tag 1 \end{align}$$ where I used Wallis's Product for $\pi/2$. If instead of that approach, I had used the Taylor series for the logarithm function, then the analysis would have led to $$\sum_{n=1}^\infty(-1)^{n-1}\log\left(\frac{n+1}{n}\right)=\sum_{n=1}^\infty \frac{(-1)^{n-1}\eta(n)}{n} \tag 2$$ where $\eta(s)=\sum_{n=1}^\infty \frac{(-1)^{n-1}}{n^s}$ is the Dirichlet eta function. Given the series on the right-hand side of $(2)$ as a starting point, it is evident that we could simply reverse steps and arrive at $(1)$. But, what are some other distinct ways that one can take to evaluate the right-hand side of $(2)$? For example, one might try to use the integral representation $$\eta(s)=\frac{1}{\Gamma(s)}\int_0^\infty \frac{x^{s-1}}{1+e^x}\,dx$$ and arrive at $$\sum_{n=1}^\infty \frac{(-1)^{n-1}\eta(n)}{n} =\int_0^\infty \frac{1-e^{-x}}{x(1+e^x)}\,dx =\int_1^\infty \frac{x-1}{x^2(x+1)\log(x)}\,dx \tag 3$$ Yet, neither of these integrals is trivial to evaluate (without reversing the preceding steps). And what are some other ways to handle the integrals in $(3)$?",,"['real-analysis', 'sequences-and-series', 'definite-integrals']"
96,Evaluate $\lim_{R\to\infty}\left(\int_0^R\left|\frac{\sin x}{x}\right|dx-\frac{2}{\pi}\log R\right)$,Evaluate,\lim_{R\to\infty}\left(\int_0^R\left|\frac{\sin x}{x}\right|dx-\frac{2}{\pi}\log R\right),"Is there a closed form of  $$\lim_{R\to\infty}\left(\int_0^R\left|\frac{\sin x}{x}\right|dx-\frac{2}{\pi}\log R\right)$$ I am pretty interested whether we can find out a closed form of this limit.  We can show that for $R=n\pi,n\in\mathbb{N}$, we have $$\begin{aligned} \int_0^R\left|\frac{\sin x}{x}\right|dx&=\sum_{k=0}^{n-1}\int_{k\pi}^{(k+1)\pi}\frac{|\sin x|}{x}dx\\ &=\sum_{k=0}^{n-1}\int_{0}^{\pi}\frac{\sin x}{(k+1)\pi-x}dx\\ &\leq \int_0^\pi\frac{\sin x}{\pi-x}dx+\sum_{k=1}^{n-1}\int_{0}^{\pi}\frac{\sin x}{k\pi}dx\\ &=\int_0^\pi\frac{|\sin(\pi-x)|}{x}dx+\sum_{k=1}^{n-1}\frac{2}{k\pi}dx\\ &=\int_0^\pi\frac{\sin x}{x}dx+\frac{2}{\pi}\sum_{k=1}^{n-1}\frac{1}{k} \end{aligned}$$ On the other hand we have $$\begin{aligned} \int_0^R\left|\frac{\sin x}{x}\right|dx&\geq \sum_{k=0}^{n-1}\int_{0}^{\pi}\frac{\sin x}{(k+1)\pi}dx\\ &=\sum_{k=1}^n\frac{1}{k\pi}\int_0^\pi\sin xdx\\ &=\frac{2}{\pi}\sum_{k=1}^n\frac{1}{k} \end{aligned}$$ Then I tried to apply the squeeze rule, but this does not lead to anything appetizing. Anybody know any tricks for this problem?","Is there a closed form of  $$\lim_{R\to\infty}\left(\int_0^R\left|\frac{\sin x}{x}\right|dx-\frac{2}{\pi}\log R\right)$$ I am pretty interested whether we can find out a closed form of this limit.  We can show that for $R=n\pi,n\in\mathbb{N}$, we have $$\begin{aligned} \int_0^R\left|\frac{\sin x}{x}\right|dx&=\sum_{k=0}^{n-1}\int_{k\pi}^{(k+1)\pi}\frac{|\sin x|}{x}dx\\ &=\sum_{k=0}^{n-1}\int_{0}^{\pi}\frac{\sin x}{(k+1)\pi-x}dx\\ &\leq \int_0^\pi\frac{\sin x}{\pi-x}dx+\sum_{k=1}^{n-1}\int_{0}^{\pi}\frac{\sin x}{k\pi}dx\\ &=\int_0^\pi\frac{|\sin(\pi-x)|}{x}dx+\sum_{k=1}^{n-1}\frac{2}{k\pi}dx\\ &=\int_0^\pi\frac{\sin x}{x}dx+\frac{2}{\pi}\sum_{k=1}^{n-1}\frac{1}{k} \end{aligned}$$ On the other hand we have $$\begin{aligned} \int_0^R\left|\frac{\sin x}{x}\right|dx&\geq \sum_{k=0}^{n-1}\int_{0}^{\pi}\frac{\sin x}{(k+1)\pi}dx\\ &=\sum_{k=1}^n\frac{1}{k\pi}\int_0^\pi\sin xdx\\ &=\frac{2}{\pi}\sum_{k=1}^n\frac{1}{k} \end{aligned}$$ Then I tried to apply the squeeze rule, but this does not lead to anything appetizing. Anybody know any tricks for this problem?",,"['real-analysis', 'sequences-and-series', 'limits']"
97,How can I prove every positive real number has square root?,How can I prove every positive real number has square root?,,Does it ask that we can express any positive real number as square root of something? like 4 is equal to square root of 16?,Does it ask that we can express any positive real number as square root of something? like 4 is equal to square root of 16?,,"['calculus', 'real-analysis']"
98,If $C$ is midpoint-convex and closed then it is a convex set,If  is midpoint-convex and closed then it is a convex set,C,"Midpoint convexity. A set $C$ is midpoint convex if whenever two points $a, b$ are in $C$ , the average or midpoint $(a + b)/2$ is in $C$ . Prove that if $C$ is closed and midpoint convex, then $C$ is convex. Convex set means that $\forall x_1, x_2 \in C, \theta x_1+(1-\theta)x_2 \in C \forall \theta \in[0,1]$ I know $$ \left(x_1+x_2\right)/2 \in C \implies \dfrac{x_1+\dfrac{x_1+x_2}{2}}{2}= \dfrac{x_1}{2}+\dfrac{x_1}{4}+\dfrac{x_2}{4} = \dfrac{3}{4}{x_1} +  \dfrac{1}{4}{x_2} \in C $$ Applying this k times I get the following: $$ (1-2^{-k})x_1+2^{-k}x_2=(1-\theta_k)x_1+\theta_k x_2 \in C $$ but I have showed it only for $\theta$ values that takes the form of $2^{-k}$ where $k \in N$ What should I do next?","Midpoint convexity. A set is midpoint convex if whenever two points are in , the average or midpoint is in . Prove that if is closed and midpoint convex, then is convex. Convex set means that I know Applying this k times I get the following: but I have showed it only for values that takes the form of where What should I do next?","C a, b C (a + b)/2 C C C \forall x_1, x_2 \in C, \theta x_1+(1-\theta)x_2 \in C \forall \theta \in[0,1] 
\left(x_1+x_2\right)/2 \in C
\implies \dfrac{x_1+\dfrac{x_1+x_2}{2}}{2}= \dfrac{x_1}{2}+\dfrac{x_1}{4}+\dfrac{x_2}{4} = \dfrac{3}{4}{x_1} +  \dfrac{1}{4}{x_2} \in C
 
(1-2^{-k})x_1+2^{-k}x_2=(1-\theta_k)x_1+\theta_k x_2 \in C
 \theta 2^{-k} k \in N","['real-analysis', 'convex-analysis']"
99,Difference of consecutive pairs of sequence terms tends to $0$,Difference of consecutive pairs of sequence terms tends to,0,"This seems an elementary problem, but I don't know of any reference to it in the literature. Consider the sequence $(a_n)_{n=1}^\infty$ of real numbers. Suppose $|a_{n+1}-a_n|\rightarrow0~(n\rightarrow\infty)$, and suppose furthermore that $|a_n|\nrightarrow\infty$. May we conclude that $a_n$ converges? The second condition precludes the standard example $a_n=\sum_{j=1}^nj^{-1}$, which obviously tends to $+\infty$.","This seems an elementary problem, but I don't know of any reference to it in the literature. Consider the sequence $(a_n)_{n=1}^\infty$ of real numbers. Suppose $|a_{n+1}-a_n|\rightarrow0~(n\rightarrow\infty)$, and suppose furthermore that $|a_n|\nrightarrow\infty$. May we conclude that $a_n$ converges? The second condition precludes the standard example $a_n=\sum_{j=1}^nj^{-1}$, which obviously tends to $+\infty$.",,"['real-analysis', 'sequences-and-series']"
