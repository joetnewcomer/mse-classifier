,title_raw,title_text,title_latex,body_raw,body_text,body_latex,tags
0,Proving monotonicity of a function $g$,Proving monotonicity of a function,g,"Let $r\ge 1$ . For $r-1\leq x \le r+1$ we define $f(x)=\arccos\left(\frac{x^2 + r^2 - 1}{2 r x}\right)$ . Now let $g:[1,\infty)\to\mathbb{R}$ be given by $$g(r)=\frac{\int_{r-1}^{r+1}{r(f(x))^2\,dx}}{\int_{r-1}^{r+1}{ f(x)\,dx}}.$$ I want to show that $g$ is a monotonically decreasing function. Notes Numerically, this looks to be almost certainly true. I have already proved that $g$ has some interesting properties; for instance $g(1)=\pi-2$ and $\lim_{r\to \infty} g(r)=\frac{8}{3\pi}$ . But ideally I'd like to show that the function decreases monotonically between these two values. I've tried looking at the derivative - but it seems too ghastly to be useful!","Let . For we define . Now let be given by I want to show that is a monotonically decreasing function. Notes Numerically, this looks to be almost certainly true. I have already proved that has some interesting properties; for instance and . But ideally I'd like to show that the function decreases monotonically between these two values. I've tried looking at the derivative - but it seems too ghastly to be useful!","r\ge 1 r-1\leq x \le r+1 f(x)=\arccos\left(\frac{x^2 + r^2 - 1}{2 r x}\right) g:[1,\infty)\to\mathbb{R} g(r)=\frac{\int_{r-1}^{r+1}{r(f(x))^2\,dx}}{\int_{r-1}^{r+1}{ f(x)\,dx}}. g g g(1)=\pi-2 \lim_{r\to \infty} g(r)=\frac{8}{3\pi}","['real-analysis', 'calculus', 'monotone-functions']"
1,Does $f(n\theta) \to 0$ for all $\theta>0$ and $f$ Darboux imply $f(x) \to 0$ as $x \to \infty$?,Does  for all  and  Darboux imply  as ?,f(n\theta) \to 0 \theta>0 f f(x) \to 0 x \to \infty,"Recall that a Darboux function $f:\mathbb{R} \to \mathbb{R}$ is one which satisfies the conclusion of the intermediate value theorem (i.e., connected sets are mapped to connected sets). Being Darboux is a weaker condition than continuity. If a theorem about continuous functions only uses the intermediate value theorem, then chances are it also holds for the entire class of Darboux functions. I find it interesting to study which theorems about continuous functions also hold for Darboux functions. We have the following theorem, which is fairly well known and hinges on the Baire Categoery Theorem. If $f:\mathbb{R} \to \mathbb{R}$ is continuous and $f(n\theta) \xrightarrow[n \in \mathbb{N}, \ n\to\infty]{} 0$ for every $\theta \in (0, \infty)$ , then $f(x) \xrightarrow[x \in \mathbb{R}, \ \ x\to\infty]{} 0$ . A counterexample if we drop continuity is $f(x) = \mathbf{1}_{\{ \exp(n) : n \in \mathbb{N}\}}$ . However, this counterexample isn't Darboux, and I haven't been able to come up with any counterexample which is Darboux. Thus, this leads me to my question. Can the continuity condition in the theorem stated above be relaxed to Darboux? In searching for counterexamples of this sort, one approach is playing around with $\sin \frac{1}{x}$ . An alternative approach is considering highly pathological functions with the property that every nonempty open set is mapped to $\mathbb{R}$ (for instance, Conway Base-13 , or Brian's example here ) and modifying these in such a way that they satisfy the hypotheses of the problem.","Recall that a Darboux function is one which satisfies the conclusion of the intermediate value theorem (i.e., connected sets are mapped to connected sets). Being Darboux is a weaker condition than continuity. If a theorem about continuous functions only uses the intermediate value theorem, then chances are it also holds for the entire class of Darboux functions. I find it interesting to study which theorems about continuous functions also hold for Darboux functions. We have the following theorem, which is fairly well known and hinges on the Baire Categoery Theorem. If is continuous and for every , then . A counterexample if we drop continuity is . However, this counterexample isn't Darboux, and I haven't been able to come up with any counterexample which is Darboux. Thus, this leads me to my question. Can the continuity condition in the theorem stated above be relaxed to Darboux? In searching for counterexamples of this sort, one approach is playing around with . An alternative approach is considering highly pathological functions with the property that every nonempty open set is mapped to (for instance, Conway Base-13 , or Brian's example here ) and modifying these in such a way that they satisfy the hypotheses of the problem.","f:\mathbb{R} \to \mathbb{R} f:\mathbb{R} \to \mathbb{R} f(n\theta) \xrightarrow[n \in \mathbb{N}, \ n\to\infty]{} 0 \theta \in (0, \infty) f(x) \xrightarrow[x \in \mathbb{R}, \ \ x\to\infty]{} 0 f(x) = \mathbf{1}_{\{ \exp(n) : n \in \mathbb{N}\}} \sin \frac{1}{x} \mathbb{R}",['real-analysis']
2,"Let $ f(x)=\frac{1+\cos(2\pi x)}2$ and $f^n=f\circ f^{n-1}$. Is it true that for almost every $x$, $\lim_{n \to \infty} f^n(x)=1$?","Let  and . Is it true that for almost every , ?", f(x)=\frac{1+\cos(2\pi x)}2 f^n=f\circ f^{n-1} x \lim_{n \to \infty} f^n(x)=1,"Let $\displaystyle f(x)=\frac{1+\cos(2\pi x)}2$ for $x\in\mathbb R$ , and $f^n=\underbrace{ f \circ \cdots \circ f}_{n}$ . Is it true that for Lebesgue almost every $x$ , $\displaystyle\lim_{n \to \infty} f^n(x)=1$ ? I'm more inclined to believe that the answer is ""yes"". This is the Problem $5$ of $2021$ Miklós Schweitzer . Recently, a related question reminds me of this problem. After spending some time on it, I found that it is a hard problem, as always as Miklós Schweitzer does. Almost every problem from that competition is very difficult for me. First of all, for a fixed $x_0\in\mathbb R$ , if $f^n(x_0)$ is convergent, then its limit $\ell$ must be a fixed point of $f$ . Since $f(x)=\cos^2(\pi x)\in[0,1]$ , we must have $\ell\in[0,1]$ . Let's find the fixed points of $f$ . Let $g(x)=f(x)-x$ for $x\in[0,1]$ , then we need to find the zeroes of $g$ . Since $g'(x)=-\pi\sin(2\pi x)-1$ , $g'$ has two zeroes $\eta_1,\eta_2\in[0,1]$ with $1/2<\eta_1<3/4$ , $3/4<\eta_2<1$ , and $\sin(2\pi\eta_1)=\sin(2\pi\eta_2)=-1/\pi$ . Hence, $g$ is decreasing in $[0, \eta_1)$ , then increasing in $(\eta_1, \eta_2)$ , and then decreasing in $(\eta_2,1]$ . Note that $g(1/2)=-1/2<0, g(1)=0$ , we know that $g(\eta_1)<0$ and $g(\eta_2)>0$ . Therefore, we can find three zeroes of $g$ , named by $\ell_1$ , $\ell_2$ and $\ell$ with $\ell_1\in(0,1/2)$ , $\ell_2\in(\eta_1, \eta_2)$ and $\ell=1$ . We can find the locations the fixed points $\ell_1, \ell_2$ more accurately. Indeed, since $$g\left(\frac14\right)=\cos^2\left(\frac\pi4\right)-\frac14=\frac12-\frac14>0,\qquad g\left(\frac13\right)=\cos^2\left(\frac\pi3\right)-\frac13=\frac14-\frac13<0,$$ we have $\ell_1\in(1/4,1/3)$ , hence $$f'(\ell_1)=-\pi\sin(2\pi\ell_1)<-\pi\sin\left(\frac{2\pi}3\right)=-\frac{\sqrt 3}2\pi<-1.$$ Also, $$g\left(\frac56\right)=\cos^2\left(\frac56\pi\right)-\frac56=\frac34-\frac56<0,\qquad g\left(\frac{11}{12}\right)=\frac{1+\cos\left(\frac{11}6\pi\right)}2-\frac{11}{12}=\frac{\sqrt3-2}4>0,$$ we have $\ell_2\in(5/6,11/12)$ , hence $$f'(\ell_2)=-\pi\sin(2\pi\ell_2)>-\pi\sin\left(\frac{11\pi}6\right)=\frac{1}2\pi>1.$$ The following are not rigorous. Therefore, locally, near $\ell_1$ , $f$ behaves like $-A(x-\ell_1)$ with $A>1$ . Consider the map $f_1: x\mapsto -Ax$ , then $f_1^n(x)$ converges if and only if $x=0$ . This indicates that, for fixed $x_0$ , if the sequence $\{f^n(x_0)\}$ doesn't reach $\ell_1$ , it will not converge to $\ell_1$ ; a similar analysis on $\ell_2$ indicates that $\{f^n(x_0)\}$ will not converge to $\ell_2$ if it doesn't touch $\ell_2$ ; hence, if $\{f^n(x_0)\}$ converges without touching $\ell_1, \ell_2$ , then the limit should must be $\ell=1$ . I think the ideas in this paragraph can be written down rigorously, although I don't know how to write a clean one. Another question I've not had any ideas: What if $\{f^n(x_0)\}$ diverges? To finish the problem, even if we write down a proof about the above paragraph, we also need to prove that for a.e. $x$ , the sequence $\{f^n(x)\}$ is convergent. Any help would be appreciated!","Let for , and . Is it true that for Lebesgue almost every , ? I'm more inclined to believe that the answer is ""yes"". This is the Problem of Miklós Schweitzer . Recently, a related question reminds me of this problem. After spending some time on it, I found that it is a hard problem, as always as Miklós Schweitzer does. Almost every problem from that competition is very difficult for me. First of all, for a fixed , if is convergent, then its limit must be a fixed point of . Since , we must have . Let's find the fixed points of . Let for , then we need to find the zeroes of . Since , has two zeroes with , , and . Hence, is decreasing in , then increasing in , and then decreasing in . Note that , we know that and . Therefore, we can find three zeroes of , named by , and with , and . We can find the locations the fixed points more accurately. Indeed, since we have , hence Also, we have , hence The following are not rigorous. Therefore, locally, near , behaves like with . Consider the map , then converges if and only if . This indicates that, for fixed , if the sequence doesn't reach , it will not converge to ; a similar analysis on indicates that will not converge to if it doesn't touch ; hence, if converges without touching , then the limit should must be . I think the ideas in this paragraph can be written down rigorously, although I don't know how to write a clean one. Another question I've not had any ideas: What if diverges? To finish the problem, even if we write down a proof about the above paragraph, we also need to prove that for a.e. , the sequence is convergent. Any help would be appreciated!","\displaystyle f(x)=\frac{1+\cos(2\pi x)}2 x\in\mathbb R f^n=\underbrace{ f \circ \cdots \circ f}_{n} x \displaystyle\lim_{n \to \infty} f^n(x)=1 5 2021 x_0\in\mathbb R f^n(x_0) \ell f f(x)=\cos^2(\pi x)\in[0,1] \ell\in[0,1] f g(x)=f(x)-x x\in[0,1] g g'(x)=-\pi\sin(2\pi x)-1 g' \eta_1,\eta_2\in[0,1] 1/2<\eta_1<3/4 3/4<\eta_2<1 \sin(2\pi\eta_1)=\sin(2\pi\eta_2)=-1/\pi g [0, \eta_1) (\eta_1, \eta_2) (\eta_2,1] g(1/2)=-1/2<0, g(1)=0 g(\eta_1)<0 g(\eta_2)>0 g \ell_1 \ell_2 \ell \ell_1\in(0,1/2) \ell_2\in(\eta_1, \eta_2) \ell=1 \ell_1, \ell_2 g\left(\frac14\right)=\cos^2\left(\frac\pi4\right)-\frac14=\frac12-\frac14>0,\qquad g\left(\frac13\right)=\cos^2\left(\frac\pi3\right)-\frac13=\frac14-\frac13<0, \ell_1\in(1/4,1/3) f'(\ell_1)=-\pi\sin(2\pi\ell_1)<-\pi\sin\left(\frac{2\pi}3\right)=-\frac{\sqrt 3}2\pi<-1. g\left(\frac56\right)=\cos^2\left(\frac56\pi\right)-\frac56=\frac34-\frac56<0,\qquad g\left(\frac{11}{12}\right)=\frac{1+\cos\left(\frac{11}6\pi\right)}2-\frac{11}{12}=\frac{\sqrt3-2}4>0, \ell_2\in(5/6,11/12) f'(\ell_2)=-\pi\sin(2\pi\ell_2)>-\pi\sin\left(\frac{11\pi}6\right)=\frac{1}2\pi>1. \ell_1 f -A(x-\ell_1) A>1 f_1: x\mapsto -Ax f_1^n(x) x=0 x_0 \{f^n(x_0)\} \ell_1 \ell_1 \ell_2 \{f^n(x_0)\} \ell_2 \ell_2 \{f^n(x_0)\} \ell_1, \ell_2 \ell=1 \{f^n(x_0)\} x \{f^n(x)\}","['real-analysis', 'analysis', 'measure-theory', 'contest-math', 'dynamical-systems']"
3,Definitions of Measurability: Outer-inner measure convergence vs. Caratheodory criterion,Definitions of Measurability: Outer-inner measure convergence vs. Caratheodory criterion,,"If we are looking over subsets of $\mathbb R$ and considering the outer measure defined exactly as $$\mu^*(A) = \inf\left\{ \sum_{k=1}^\infty \ell(I_k) \text{ where the $I_k$ are open intervals such that } A\subset \bigcup_{k=1}^\infty I_k \right\}$$ and inner measure to be $$\mu_*(A) = \sup\left\{\mu^*(F) \text{ where the $F$ is a closed subset of $\mathbb R$ such that } F\subset A \right\}$$ then measurability in the sense of Lebesgue: $A$ is measurable if $\color{red}{\mu^*(A)=\mu_*(A)}$ is equivalent to the Caratheodory criterion: $A$ is measurable if $\color{red}{\mu^*(T) = \mu^*(T\cap A) + \mu^*(T\cap A^C)}$ for all test sets $T\subset \mathbb R$ See the proof from page 41-45 of this pdf: http://www.math.harvard.edu/~shlomo/212a/11.pdf ; definitions and preceding theorems are found pages 1-40). First question $\color{red}{\text{STILL OPEN}}$ (perhaps refer to Nate’s comment below)?: If these two definitions are in fact equivalent, has anyone tried proving the Caratheodory Extension Theorem without the Caratheodory criterion , i.e. from scratch using outer and inner measure, or at least the equivalent definition of $\color{red}{\mu(G\setminus F)<\epsilon \text{ for some } G\supset A\supset F}$ (the ""inner-outer"" characterization from the Harvard pdf)? If so, could you provide a rundown of the proof (or a link to some article)? If it doesn't work generally, how about in the specific case where outer measure is defined the way I defined it above (e.g. as in the proof presented here, page 28 of the pdf: https://www.stat.washington.edu/jaw/COURSES/520s/521/bk521reJaw2012.pdf )? And second question $\color{red}{\text{(resolved I believe by the comment by Daniel Fischer)}}$ : Consider a weaker criterion than Caratheodory's: I just want that $\mu^*(X) = \mu^*(X\cap A) + \mu^*(X\cap A^C)$ for just the ""biggest set"" $X$ (I'm talking about the set for which $A^C=X\setminus A$ for all $A$ , i.e. the universal set; assume also that $\mu(X)$ is not infinite). We are still using outer measure as I defined it above. Is this an equivalent condition to the notions of measurability I described before? If not, is there an explicit counterexample (one that I can use pedagogically to motivate Caratheodory's stronger criterion)? Related questions: Intuition behind the definition of a measurable set Proving the *Carathedory Criterion* for *Lebesgue Measurability* Measurability criterion of Caratheodory https://mathoverflow.net/questions/34007/demystifying-the-caratheodory-approach-to-measurability Proving equivalent of definitions of measurability Is the Carathéodory definition of Measurable set the most general? $\color{red}{\text{Thanks to Daniel Fischer to a comment for what I believe to be a resolution to Question 2:}}$ Proving Caratheodory measurability if and only if the measure of a set summed with the measure of its complement is the measure of the whole space.","If we are looking over subsets of and considering the outer measure defined exactly as and inner measure to be then measurability in the sense of Lebesgue: is measurable if is equivalent to the Caratheodory criterion: is measurable if for all test sets See the proof from page 41-45 of this pdf: http://www.math.harvard.edu/~shlomo/212a/11.pdf ; definitions and preceding theorems are found pages 1-40). First question (perhaps refer to Nate’s comment below)?: If these two definitions are in fact equivalent, has anyone tried proving the Caratheodory Extension Theorem without the Caratheodory criterion , i.e. from scratch using outer and inner measure, or at least the equivalent definition of (the ""inner-outer"" characterization from the Harvard pdf)? If so, could you provide a rundown of the proof (or a link to some article)? If it doesn't work generally, how about in the specific case where outer measure is defined the way I defined it above (e.g. as in the proof presented here, page 28 of the pdf: https://www.stat.washington.edu/jaw/COURSES/520s/521/bk521reJaw2012.pdf )? And second question : Consider a weaker criterion than Caratheodory's: I just want that for just the ""biggest set"" (I'm talking about the set for which for all , i.e. the universal set; assume also that is not infinite). We are still using outer measure as I defined it above. Is this an equivalent condition to the notions of measurability I described before? If not, is there an explicit counterexample (one that I can use pedagogically to motivate Caratheodory's stronger criterion)? Related questions: Intuition behind the definition of a measurable set Proving the *Carathedory Criterion* for *Lebesgue Measurability* Measurability criterion of Caratheodory https://mathoverflow.net/questions/34007/demystifying-the-caratheodory-approach-to-measurability Proving equivalent of definitions of measurability Is the Carathéodory definition of Measurable set the most general? Proving Caratheodory measurability if and only if the measure of a set summed with the measure of its complement is the measure of the whole space.",\mathbb R \mu^*(A) = \inf\left\{ \sum_{k=1}^\infty \ell(I_k) \text{ where the I_k are open intervals such that } A\subset \bigcup_{k=1}^\infty I_k \right\} \mu_*(A) = \sup\left\{\mu^*(F) \text{ where the F is a closed subset of \mathbb R such that } F\subset A \right\} A \color{red}{\mu^*(A)=\mu_*(A)} A \color{red}{\mu^*(T) = \mu^*(T\cap A) + \mu^*(T\cap A^C)} T\subset \mathbb R \color{red}{\text{STILL OPEN}} \color{red}{\mu(G\setminus F)<\epsilon \text{ for some } G\supset A\supset F} \color{red}{\text{(resolved I believe by the comment by Daniel Fischer)}} \mu^*(X) = \mu^*(X\cap A) + \mu^*(X\cap A^C) X A^C=X\setminus A A \mu(X) \color{red}{\text{Thanks to Daniel Fischer to a comment for what I believe to be a resolution to Question 2:}},"['real-analysis', 'measure-theory', 'lebesgue-measure', 'outer-measure', 'measurable-sets']"
4,"Why do we treat differentials as infinitesimals, even when it's not rigorous","Why do we treat differentials as infinitesimals, even when it's not rigorous",,"From single-variable calculus where we first encounter differentials we are told fairly often that differentials are not to be treated as infinitesimal quantities/objects (but we are never really told why) , and we kind of hand-wave things and manipulate them as fractions (e.g $\frac{dy}{dx}$) in things like $u$-Substitution. $$\int f(x) \ dx$$ In the above integral, the differential, $dx$, seems to be doing absolutely nothing other than signifying where the expression for the integrand finishes. But the differential, seems to have more of a part to play in multivariable calculus. In multivariable calculus we often encounter integrals like $$\oint \vec{f(x)}\bullet\vec{dX} = \oint \|\vec{f(x)}\| \cdot \|\vec{dX}\| \cos(\theta)$$ where we can treat vector differentials as if they were infinitesimal vectors, and perform normal vector operations (such as taking the dot product) Questions and comments It doesn't seem all that clear to me why we can't treat either scalar or vector differentials as infinitesimals. Furthermore why do we even treat differentials as infinitesimals, even when it's not technically rigorous? I'm sure there must be conditions that need to be met that allow us to treat differentials as infinitesimals. It seems like in single-variable calculus the differentials we encountered just a special case of the differentials we encounter in multivariable calculus, which we can do more with. I've heard of integration on differential forms (which I'm about to start learning), and I'm assuming that all the differentials we've encountered thus far in single and multivariable calculus must be special cases of this general form . Am I correct? If not then where do we rigorously define differentials and the operations we can perform with them?","From single-variable calculus where we first encounter differentials we are told fairly often that differentials are not to be treated as infinitesimal quantities/objects (but we are never really told why) , and we kind of hand-wave things and manipulate them as fractions (e.g $\frac{dy}{dx}$) in things like $u$-Substitution. $$\int f(x) \ dx$$ In the above integral, the differential, $dx$, seems to be doing absolutely nothing other than signifying where the expression for the integrand finishes. But the differential, seems to have more of a part to play in multivariable calculus. In multivariable calculus we often encounter integrals like $$\oint \vec{f(x)}\bullet\vec{dX} = \oint \|\vec{f(x)}\| \cdot \|\vec{dX}\| \cos(\theta)$$ where we can treat vector differentials as if they were infinitesimal vectors, and perform normal vector operations (such as taking the dot product) Questions and comments It doesn't seem all that clear to me why we can't treat either scalar or vector differentials as infinitesimals. Furthermore why do we even treat differentials as infinitesimals, even when it's not technically rigorous? I'm sure there must be conditions that need to be met that allow us to treat differentials as infinitesimals. It seems like in single-variable calculus the differentials we encountered just a special case of the differentials we encounter in multivariable calculus, which we can do more with. I've heard of integration on differential forms (which I'm about to start learning), and I'm assuming that all the differentials we've encountered thus far in single and multivariable calculus must be special cases of this general form . Am I correct? If not then where do we rigorously define differentials and the operations we can perform with them?",,"['real-analysis', 'multivariable-calculus', 'vectors', 'differential-topology']"
5,How to prove $\frac{\pi^2}{6}\le \int_0^{\infty} \sin(x^{\log x}) \ \mathrm dx $?,How to prove ?,\frac{\pi^2}{6}\le \int_0^{\infty} \sin(x^{\log x}) \ \mathrm dx ,"I want to prove the inequality $$\frac{\pi^2}{6}\le \int_0^{\infty} \sin(x^{\log x}) \ \mathrm dx $$ There are some obstacles I face: the indefinite integral cannot be expressed in terms of elementary functions, Taylor series leads to a another function that cannot be expressed in terms of elementary functions. What else to try?","I want to prove the inequality $$\frac{\pi^2}{6}\le \int_0^{\infty} \sin(x^{\log x}) \ \mathrm dx $$ There are some obstacles I face: the indefinite integral cannot be expressed in terms of elementary functions, Taylor series leads to a another function that cannot be expressed in terms of elementary functions. What else to try?",,"['real-analysis', 'inequality', 'definite-integrals', 'improper-integrals']"
6,Proving a certain map on the closed unit disc must be the identity,Proving a certain map on the closed unit disc must be the identity,,"Bounty expired . Will gladly re-create one if a satisfactory answer is posted in the future. Prove: Let $f$ be a continuous function on the closed unit disc with two properties: 1. $f$ is the identity on the boundary, i.e., on the unit circle. That is, if $|z| = 1$, then $f(z) = z$. 2. $f^2$ is the identity, i.e., for all $z$ in the closed unit disc, we have $f(f(z)) = z$. Then $f$ must be the identity function. Motivation: I came across this question for the closed unit sphere here on MathOverflow. It seemed to me like considering the two-dimensional case might be a good place to start in trying to tackle the problem there. Ultimately, the MO question was resolved using some nontrivial results. I am wondering whether there is a proof of the question here, for the closed unit disc , which uses methods that don't extend beyond those of basic point-set topology or a first course in real analysis. Remark: Please note that I am looking for a fundamentally different proof of the proposition above, that is, not a modification of either of the responses given at the MathOverflow link into more digestable language. In response to a comment: If you have thought up a proof and are concerned the level of presentation is too high, then I hope you will still post it as an answer.","Bounty expired . Will gladly re-create one if a satisfactory answer is posted in the future. Prove: Let $f$ be a continuous function on the closed unit disc with two properties: 1. $f$ is the identity on the boundary, i.e., on the unit circle. That is, if $|z| = 1$, then $f(z) = z$. 2. $f^2$ is the identity, i.e., for all $z$ in the closed unit disc, we have $f(f(z)) = z$. Then $f$ must be the identity function. Motivation: I came across this question for the closed unit sphere here on MathOverflow. It seemed to me like considering the two-dimensional case might be a good place to start in trying to tackle the problem there. Ultimately, the MO question was resolved using some nontrivial results. I am wondering whether there is a proof of the question here, for the closed unit disc , which uses methods that don't extend beyond those of basic point-set topology or a first course in real analysis. Remark: Please note that I am looking for a fundamentally different proof of the proposition above, that is, not a modification of either of the responses given at the MathOverflow link into more digestable language. In response to a comment: If you have thought up a proof and are concerned the level of presentation is too high, then I hope you will still post it as an answer.",,"['real-analysis', 'general-topology', 'analysis', 'education']"
7,Is the max of two differentiable functions differentiable?,Is the max of two differentiable functions differentiable?,,"Given that $f$ and $g$ are two real functions and both are differentiable, is it true to say that $h=\max{(f,g)} $ is differentiable too? Thanks","Given that $f$ and $g$ are two real functions and both are differentiable, is it true to say that $h=\max{(f,g)} $ is differentiable too? Thanks",,"['calculus', 'real-analysis', 'multivariable-calculus']"
8,Evaluating integral $\int_0^1 \frac{x-x^2}{\sin \pi x} dx = \frac{7 \zeta(3)}{\pi^3}$,Evaluating integral,\int_0^1 \frac{x-x^2}{\sin \pi x} dx = \frac{7 \zeta(3)}{\pi^3},To evaluate the integral \begin{equation} \int_0^1 \frac{x-x^2}{\sin \pi x} dx =  \frac{7 \zeta(3)}{\pi^3} \end{equation} I tried to use the series for $\sin \pi x$ and maybe find something related to $\zeta(3)$ .  But it didn't work. I'm guessing this integral needs more than the little amount of calculus that I know.,To evaluate the integral I tried to use the series for and maybe find something related to .  But it didn't work. I'm guessing this integral needs more than the little amount of calculus that I know.,"\begin{equation}
\int_0^1 \frac{x-x^2}{\sin \pi x} dx =  \frac{7 \zeta(3)}{\pi^3}
\end{equation} \sin \pi x \zeta(3)","['real-analysis', 'calculus', 'integration', 'definite-integrals', 'trigonometric-integrals']"
9,Prove that the equation $x^{10000} + x^{100} - 1 = 0$ has a solution with $0 < x < 1$,Prove that the equation  has a solution with,x^{10000} + x^{100} - 1 = 0 0 < x < 1,"Prove that the equation $x^{10000} + x^{100} - 1 = 0$ has a solution with $0 < x < 1$. This is a homework question. I know I could probably find a solution that would complete the proof, but I don't think that is what this question is asking. What proof-techniques should I use to prove this is true?","Prove that the equation $x^{10000} + x^{100} - 1 = 0$ has a solution with $0 < x < 1$. This is a homework question. I know I could probably find a solution that would complete the proof, but I don't think that is what this question is asking. What proof-techniques should I use to prove this is true?",,"['real-analysis', 'polynomials']"
10,"Is this alternative notion of continuity in metric spaces weaker than, or equivalent to the usual one?","Is this alternative notion of continuity in metric spaces weaker than, or equivalent to the usual one?",,"I will try to be as clear as possible. For simplicity I will assume that the function $f$ for which we define continuity at some point is real function of a real variable $f: \mathbb R \to \mathbb R$, although the same line of reasoning should be the same even if we talk about continuity of a function at a point that is defined at some metric space and that has values in some metric space. To define continuity of a function $f$ at some point $x_0$ from its domain we have the following standard and famous definition, the $\varepsilon$-$\delta$ definition of continuity, which goes like this: Definition 1: $f$ is continuous at the point $x_0$ from its domain if for every $\varepsilon>0$ there exists $\delta>0$ such that when $|x-x_0|<\delta$ then $|f(x)-f(x_0)|<\varepsilon$. It is clear that we could write $\delta (\varepsilon)$ instead of $\delta$ because there really is dependence of $\delta$ on $\varepsilon$. Now, I was thinking about alternative definition of continuity that would go like this: Definition 2: $f$ is continuous at the point $x_0$ from its domain if there exist two sequences $\varepsilon_n$ and $\delta_n$ such that for every $n \in \mathbb N$ we have $\varepsilon_n>0$ and $\delta_n>0$ and $\lim_{n\to\infty}\varepsilon_n=\lim_{n\to\infty}\delta_n=0$ and when $|x-x_0|<\delta_n$ then $|f(x)-f(x_0)|<\varepsilon_n$. We could also write here $\delta_n(\varepsilon_n)$ instead of $\delta_n$ because obviously there is dependence of $\delta_n$ on $\varepsilon_n$. It is clear that second definition does not require that for every $\varepsilon$ there exist $\delta(\varepsilon)$ (which includes in itself an uncountable number of choices for $\varepsilon$) but instead requires that for every member of the sequence $\varepsilon_n$ there is some $\delta_n$ (and this includes in itself only countable number of choices because the set $\{\varepsilon_n : n \in \mathbb N\})$ is countable). It is clearly obvious that definition 1 implies definition 2, and the real question is is the converse true, in other words: If the function is continuous at some point according to definition 2, is it also continuous at the same point according to definition 1?","I will try to be as clear as possible. For simplicity I will assume that the function $f$ for which we define continuity at some point is real function of a real variable $f: \mathbb R \to \mathbb R$, although the same line of reasoning should be the same even if we talk about continuity of a function at a point that is defined at some metric space and that has values in some metric space. To define continuity of a function $f$ at some point $x_0$ from its domain we have the following standard and famous definition, the $\varepsilon$-$\delta$ definition of continuity, which goes like this: Definition 1: $f$ is continuous at the point $x_0$ from its domain if for every $\varepsilon>0$ there exists $\delta>0$ such that when $|x-x_0|<\delta$ then $|f(x)-f(x_0)|<\varepsilon$. It is clear that we could write $\delta (\varepsilon)$ instead of $\delta$ because there really is dependence of $\delta$ on $\varepsilon$. Now, I was thinking about alternative definition of continuity that would go like this: Definition 2: $f$ is continuous at the point $x_0$ from its domain if there exist two sequences $\varepsilon_n$ and $\delta_n$ such that for every $n \in \mathbb N$ we have $\varepsilon_n>0$ and $\delta_n>0$ and $\lim_{n\to\infty}\varepsilon_n=\lim_{n\to\infty}\delta_n=0$ and when $|x-x_0|<\delta_n$ then $|f(x)-f(x_0)|<\varepsilon_n$. We could also write here $\delta_n(\varepsilon_n)$ instead of $\delta_n$ because obviously there is dependence of $\delta_n$ on $\varepsilon_n$. It is clear that second definition does not require that for every $\varepsilon$ there exist $\delta(\varepsilon)$ (which includes in itself an uncountable number of choices for $\varepsilon$) but instead requires that for every member of the sequence $\varepsilon_n$ there is some $\delta_n$ (and this includes in itself only countable number of choices because the set $\{\varepsilon_n : n \in \mathbb N\})$ is countable). It is clearly obvious that definition 1 implies definition 2, and the real question is is the converse true, in other words: If the function is continuous at some point according to definition 2, is it also continuous at the same point according to definition 1?",,"['real-analysis', 'analysis', 'continuity', 'definition']"
11,Is this proof for divergence of harmonic series rigorous? Can it be made so?,Is this proof for divergence of harmonic series rigorous? Can it be made so?,,"Bartle & Sherbert, edition 4, page-97 gives the following proof: Assume that the series converges to some $S$ : $$\implies S= 1+\frac{1}{2}+\frac{1}{3}+\frac{1}{4}+ \dots$$ Then they proceed to derive an inequality by converting the terms of the type $\frac{1}{2n-1}+\frac{1}{2n}$ into $\frac{1}{2n}+\frac{1}{2n}=\frac{1}{n}$ $$1+\frac{1}{2}+\frac{1}{3}+\frac{1}{4}+ \dots > \left(\frac{1}{2}+\frac{1}{2}\right)+\left(\frac{1}{4}+\frac{1}{4}\right) +\dots=1+\frac{1}{2}+\frac{1}{3} \dots=S$$ They have shown $S>S$ from the assumption, hence, there's a contradiction. But I do not like their insistence upon using the "" $\dots$ "", as I feel like they're hiding something behind that. If we do a similar analysis a bit more rigorously, i.e, working solely with sequences of partial sums up to a defined, fixed $n$ , (instead of those dots), we have: $$S_{2n}=1+\frac{1}{2}+\frac{1}{3}+\frac{1}{4} \dots +\frac{1}{2n-1}+\frac{1}{2n}$$ $$S_{2n} >\left(\frac{1}{2}+\frac{1}{2}\right)+\left(\frac{1}{4}+\frac{1}{4}\right) \dots +\left(\frac{1}{2n}+\frac{1}{2n}\right)=1+\frac{1}{2}+\frac{1}{3} \dots \frac{1}{n}=S_n \implies S_{2n} > S_{n}$$ This is hardly news, since the 1-harmonic is monotone increasing. Now assuming that the sequence $S_n$ converges, and passing onto the limit in the above equation yields: $$S \geq S$$ which is fine ?","Bartle & Sherbert, edition 4, page-97 gives the following proof: Assume that the series converges to some : Then they proceed to derive an inequality by converting the terms of the type into They have shown from the assumption, hence, there's a contradiction. But I do not like their insistence upon using the "" "", as I feel like they're hiding something behind that. If we do a similar analysis a bit more rigorously, i.e, working solely with sequences of partial sums up to a defined, fixed , (instead of those dots), we have: This is hardly news, since the 1-harmonic is monotone increasing. Now assuming that the sequence converges, and passing onto the limit in the above equation yields: which is fine ?",S \implies S= 1+\frac{1}{2}+\frac{1}{3}+\frac{1}{4}+ \dots \frac{1}{2n-1}+\frac{1}{2n} \frac{1}{2n}+\frac{1}{2n}=\frac{1}{n} 1+\frac{1}{2}+\frac{1}{3}+\frac{1}{4}+ \dots > \left(\frac{1}{2}+\frac{1}{2}\right)+\left(\frac{1}{4}+\frac{1}{4}\right) +\dots=1+\frac{1}{2}+\frac{1}{3} \dots=S S>S \dots n S_{2n}=1+\frac{1}{2}+\frac{1}{3}+\frac{1}{4} \dots +\frac{1}{2n-1}+\frac{1}{2n} S_{2n} >\left(\frac{1}{2}+\frac{1}{2}\right)+\left(\frac{1}{4}+\frac{1}{4}\right) \dots +\left(\frac{1}{2n}+\frac{1}{2n}\right)=1+\frac{1}{2}+\frac{1}{3} \dots \frac{1}{n}=S_n \implies S_{2n} > S_{n} S_n S \geq S,"['real-analysis', 'calculus', 'sequences-and-series']"
12,Set that is bounded but not totally bounded: Reading textbook,Set that is bounded but not totally bounded: Reading textbook,,"I've been reading a Real Analysis textbook that my friend loaned to me. I have come across a proposition that says that a totally bounded set is bounded, but a bounded set is not always totally bounded. This makes sense, however I am having trouble thinking of an example of a set that is bounded but not totally bounded. Could anyone shed some light on this? Thanks!","I've been reading a Real Analysis textbook that my friend loaned to me. I have come across a proposition that says that a totally bounded set is bounded, but a bounded set is not always totally bounded. This makes sense, however I am having trouble thinking of an example of a set that is bounded but not totally bounded. Could anyone shed some light on this? Thanks!",,"['real-analysis', 'metric-spaces']"
13,Question about Riemann integral and total variation,Question about Riemann integral and total variation,,"Let $g$ be Riemann integrable on $[a,b]$, $f(x)=\int_a^x g(t)dt $ for $x \in[a,b]$. Can I show that the total variation of $f$ is equal to $\int_a^b |g(x)| dx $?","Let $g$ be Riemann integrable on $[a,b]$, $f(x)=\int_a^x g(t)dt $ for $x \in[a,b]$. Can I show that the total variation of $f$ is equal to $\int_a^b |g(x)| dx $?",,"['real-analysis', 'integration', 'bounded-variation']"
14,How many non-differentiable functions exist?,How many non-differentiable functions exist?,,The size of the set of functions that map $\mathbb{R}\to \mathbb{R}$ equals $(\#\mathbb{R})^{\#\mathbb{R}}$. How many non-differentiable functions are there in this set?,The size of the set of functions that map $\mathbb{R}\to \mathbb{R}$ equals $(\#\mathbb{R})^{\#\mathbb{R}}$. How many non-differentiable functions are there in this set?,,"['real-analysis', 'functions', 'elementary-set-theory', 'derivatives', 'cardinals']"
15,Is there an analytic approximation to the minimum function?,Is there an analytic approximation to the minimum function?,,"I am looking for an analytic function that approximates the minimum function. i.e., $|f(x_1,x_2) - \min(x_1,x_2)| < \zeta$ for some $\zeta$ that may be related to $|x_1 - x_2|$. Or may be a series $f_1,f_2,....$, where $\lim_{n \to \infty} \zeta = 0$.","I am looking for an analytic function that approximates the minimum function. i.e., $|f(x_1,x_2) - \min(x_1,x_2)| < \zeta$ for some $\zeta$ that may be related to $|x_1 - x_2|$. Or may be a series $f_1,f_2,....$, where $\lim_{n \to \infty} \zeta = 0$.",,"['real-analysis', 'functional-analysis', 'approximation']"
16,"Showing that the space $C[0,1]$ with the $L_1$ norm is incomplete",Showing that the space  with the  norm is incomplete,"C[0,1] L_1","Can anyone think of a relatively easy counter example to remember, which demonstrates that the space $C[0,1]$ with the $L_1$ norm is incomplete? Thanks!","Can anyone think of a relatively easy counter example to remember, which demonstrates that the space $C[0,1]$ with the $L_1$ norm is incomplete? Thanks!",,[]
17,"Find $\int_1^a \sqrt[5]{x^5-1}\ dx + \int_0^b \sqrt[5]{x^5+1}\ dx$, where $a^5-b^5 = 1$","Find , where",\int_1^a \sqrt[5]{x^5-1}\ dx + \int_0^b \sqrt[5]{x^5+1}\ dx a^5-b^5 = 1,"So I am preparing to go to this olympiad, the Latvian Sophomore's dream calculus olympiad. I received previous years problems and the toughest problem in the definite integral section was this $$\text{Find } \int_1^a \sqrt[5]{x^5-1}\ dx  + \int_0^b \sqrt[5]{x^5+1}\ dx$$ $$\text{where }\  a^5-b^5 = 1$$ I tried substituting the whole root sign in the respective integrals but that led to nowhere. I don't see how trigonometric substitution could be used, dummy variables or the DI method. I am really at a loss here. Any ideas? I added a picture of all the problems.","So I am preparing to go to this olympiad, the Latvian Sophomore's dream calculus olympiad. I received previous years problems and the toughest problem in the definite integral section was this I tried substituting the whole root sign in the respective integrals but that led to nowhere. I don't see how trigonometric substitution could be used, dummy variables or the DI method. I am really at a loss here. Any ideas? I added a picture of all the problems.",\text{Find } \int_1^a \sqrt[5]{x^5-1}\ dx  + \int_0^b \sqrt[5]{x^5+1}\ dx \text{where }\  a^5-b^5 = 1,"['real-analysis', 'calculus', 'integration', 'definite-integrals', 'contest-math']"
18,How prove this integral limit $=f(\frac{1}{2})$,How prove this integral limit,=f(\frac{1}{2}),"Let $f$ be a continuous function on the unit interval $[0,1]$. Show that $$\lim_{n\to\infty}\int_{0}^{1}\cdots\int_0^1\int_{0}^{1}f\left(\dfrac{x_{1}+x_{2}+\cdots+x_{n}}{n}\right)dx_{1}dx_{2}\cdots dx_{n}=f\left(\dfrac{1}{2}\right)$$ This problem is from Selected Problems in Real Analysis . But the author doesn't include a solution. Maybe there is a method for this sort of problem? Maybe we can use this: $$\int_0^1\cdots\int_{0}^{1}\int_{0}^{1}\dfrac{x_{1}+x_{2}+\cdots+x_{n}}{n}dx_{1}dx_{2}\cdots dx_{n}=\dfrac{1}{2}?$$ I also found a similar problem in this question .","Let $f$ be a continuous function on the unit interval $[0,1]$. Show that $$\lim_{n\to\infty}\int_{0}^{1}\cdots\int_0^1\int_{0}^{1}f\left(\dfrac{x_{1}+x_{2}+\cdots+x_{n}}{n}\right)dx_{1}dx_{2}\cdots dx_{n}=f\left(\dfrac{1}{2}\right)$$ This problem is from Selected Problems in Real Analysis . But the author doesn't include a solution. Maybe there is a method for this sort of problem? Maybe we can use this: $$\int_0^1\cdots\int_{0}^{1}\int_{0}^{1}\dfrac{x_{1}+x_{2}+\cdots+x_{n}}{n}dx_{1}dx_{2}\cdots dx_{n}=\dfrac{1}{2}?$$ I also found a similar problem in this question .",,"['real-analysis', 'limits', 'multivariable-calculus']"
19,"$f\geq 0$, continuous and $\int_a^b f=0$ implies $f=0$ everywhere on $[a,b]$",", continuous and  implies  everywhere on","f\geq 0 \int_a^b f=0 f=0 [a,b]","This is problem 6.2 from the 3rd edition of Principles of Mathematical Analysis. Problem 6.2 : Suppose $f\geq 0$, f is continuous on $[a, b]$, and $\int_a^b f(x) \, dx = 0$. Prove that $f(x)=0$ for all $x \in [a, b]$. I'm looking for a critique of my proof. It's a pretty easy problem, but I am always wary of making too bold of assumptions, especially on these low level/fundamental proofs. I'll be using Rudin's notation and refer to theorems from the text (If I should include the text of each theorem, feel free to leave a comment... I'm lazy but could probably use the TeX practice :p) Proof : Assume, for contradiction, that $f>0$. Then, for any partition $P$ we have the Lower Riemann Sum: $L(P, f)=\sum_{i=1}^n m_i \, \Delta x_i$. At least one $\Delta x_i$ must be positive, since $a < b$, and each $m_i$ must be positive since we have $f>0$ by assumption, so certainly $\sup f > 0$. That means $L(P, f)>0$. Thus, we have: $$0 = 0(b-a) < L(P,f)\leq \sup L(P, f) = \inf U(P,f) =L$$ where the last string of equalities holds because our function is continuous on a compact interval, so is integrable by theorems 6.8 and 6.6. So our integral has value $L>0$. This is in contradiction to our given assumption that $\int_a^b f(x) \, dx = 0$, so we must have that $f=0$ on $[a, b]$. $\Box$ So, I am wondering if my proof is correct (and is presented well). Also if someone could enlighten me as to what Rudin means when he say ""Compare this with exercise 1,"" I'd be appreciative. Is it a hint or is there something else he expects you to notice? there are a lot of things I could compare :)... Exercise 6.1: Suppose $\alpha$ increases on $[a, b]$, $a \leq x_0 \leq b$, $\alpha$ is continuous at $x_0$, $f(x_0)=1$, and $f(x)=0$ if $x\neq x_0$. Prove that $f$ is Riemann-Stieltjes Integrable and that $\int f \, d\alpha = 0$","This is problem 6.2 from the 3rd edition of Principles of Mathematical Analysis. Problem 6.2 : Suppose $f\geq 0$, f is continuous on $[a, b]$, and $\int_a^b f(x) \, dx = 0$. Prove that $f(x)=0$ for all $x \in [a, b]$. I'm looking for a critique of my proof. It's a pretty easy problem, but I am always wary of making too bold of assumptions, especially on these low level/fundamental proofs. I'll be using Rudin's notation and refer to theorems from the text (If I should include the text of each theorem, feel free to leave a comment... I'm lazy but could probably use the TeX practice :p) Proof : Assume, for contradiction, that $f>0$. Then, for any partition $P$ we have the Lower Riemann Sum: $L(P, f)=\sum_{i=1}^n m_i \, \Delta x_i$. At least one $\Delta x_i$ must be positive, since $a < b$, and each $m_i$ must be positive since we have $f>0$ by assumption, so certainly $\sup f > 0$. That means $L(P, f)>0$. Thus, we have: $$0 = 0(b-a) < L(P,f)\leq \sup L(P, f) = \inf U(P,f) =L$$ where the last string of equalities holds because our function is continuous on a compact interval, so is integrable by theorems 6.8 and 6.6. So our integral has value $L>0$. This is in contradiction to our given assumption that $\int_a^b f(x) \, dx = 0$, so we must have that $f=0$ on $[a, b]$. $\Box$ So, I am wondering if my proof is correct (and is presented well). Also if someone could enlighten me as to what Rudin means when he say ""Compare this with exercise 1,"" I'd be appreciative. Is it a hint or is there something else he expects you to notice? there are a lot of things I could compare :)... Exercise 6.1: Suppose $\alpha$ increases on $[a, b]$, $a \leq x_0 \leq b$, $\alpha$ is continuous at $x_0$, $f(x_0)=1$, and $f(x)=0$ if $x\neq x_0$. Prove that $f$ is Riemann-Stieltjes Integrable and that $\int f \, d\alpha = 0$",,"['real-analysis', 'integration', 'proof-writing']"
20,"If $f:\mathbb{R}\to\mathbb{R}$ is continuous and $f(x)\neq x$ for all $x$, must it be true that $f(f(x))\neq x$ for all $x$?","If  is continuous and  for all , must it be true that  for all ?",f:\mathbb{R}\to\mathbb{R} f(x)\neq x x f(f(x))\neq x x,Let $f: \Bbb R → \Bbb R$ be a continuous function such that $f(x)=x$ has no real solution  . Then is it true that $f(f(x))=x$ also has no real solution ?,Let $f: \Bbb R → \Bbb R$ be a continuous function such that $f(x)=x$ has no real solution  . Then is it true that $f(f(x))=x$ also has no real solution ?,,"['real-analysis', 'general-topology', 'continuity', 'functional-equations']"
21,What does it mean for a function to be Riemann integrable?,What does it mean for a function to be Riemann integrable?,,"I need to come up with a precise mathematical definition of what a Riemann integrable function is. I know what the Riemann integral is but when I look for definitions all I find are proofs of how to prove that a function is Riemann integrable. I need help creating a definition of what it means for a function to be Riemann integrable that does not include any notation, just a couple of mathematical sentences that defines Riemann integrals.","I need to come up with a precise mathematical definition of what a Riemann integrable function is. I know what the Riemann integral is but when I look for definitions all I find are proofs of how to prove that a function is Riemann integrable. I need help creating a definition of what it means for a function to be Riemann integrable that does not include any notation, just a couple of mathematical sentences that defines Riemann integrals.",,"['real-analysis', 'riemann-integration']"
22,Distance to a closed set is continuous. [duplicate],Distance to a closed set is continuous. [duplicate],,"This question already has an answer here : Distance to a set is continuous, revisited (1 answer) Closed 2 years ago . I want to prove that given a metric space $(M,d)$ and $F \subset M$ , then the function $f_F: M \to \Bbb R$ given by $f_F(x) = d(x,F) = \inf\{d(x,y) \ : \ y \in F\}$ is continuous. Take $x \in M$ . If $x \in F$ it is obvious. I am convinced that it is continuous, since if you take a sequence $(x_n)_{n \in \Bbb N}$ which converges to $x \in M$ , then the distances from the $x_n$ to $F$ go close to the distance from $x$ to $F$ , since the $x_n$ go close to $x$ . But I'm having much trouble writing it. My teacher said to ""just forget about the $\inf$ and work with sequences"", but that doesn't help me at all. Given $\epsilon > 0$ and a sequence $(x_n)_{x \in \Bbb N}$ such that $x_n \to x$ , I must prove that $f_F(x_n) \to f_F(x)$ , that is, find $n_\epsilon \in \Bbb N$ such that: $$n > n_\epsilon \implies |d(x_n,F) - d(x,F)| < \epsilon $$ Surely if $y \in F$ , we have $d(x,y) \geq d(x,F)$ . Other than this, I don't know how to deal with these infs, and this absolute value. I suppose I'll have to use the triangle inequality. Can someone help me do it?","This question already has an answer here : Distance to a set is continuous, revisited (1 answer) Closed 2 years ago . I want to prove that given a metric space and , then the function given by is continuous. Take . If it is obvious. I am convinced that it is continuous, since if you take a sequence which converges to , then the distances from the to go close to the distance from to , since the go close to . But I'm having much trouble writing it. My teacher said to ""just forget about the and work with sequences"", but that doesn't help me at all. Given and a sequence such that , I must prove that , that is, find such that: Surely if , we have . Other than this, I don't know how to deal with these infs, and this absolute value. I suppose I'll have to use the triangle inequality. Can someone help me do it?","(M,d) F \subset M f_F: M \to \Bbb R f_F(x) = d(x,F) = \inf\{d(x,y) \ : \ y \in F\} x \in M x \in F (x_n)_{n \in \Bbb N} x \in M x_n F x F x_n x \inf \epsilon > 0 (x_n)_{x \in \Bbb N} x_n \to x f_F(x_n) \to f_F(x) n_\epsilon \in \Bbb N n > n_\epsilon \implies |d(x_n,F) - d(x,F)| < \epsilon  y \in F d(x,y) \geq d(x,F)","['real-analysis', 'metric-spaces']"
23,Integrate $ \int_0^\infty \frac{ \ln^2(1+x)}{x^{3/2}} dx=8\pi \ln 2$,Integrate, \int_0^\infty \frac{ \ln^2(1+x)}{x^{3/2}} dx=8\pi \ln 2,"I am trying to evaluate this integral. $$ I=\int_0^\infty \frac{ \ln^2(1+x)}{x^{3/2}} dx=8\pi \ln 2 $$  Note $$ \ln(1+x)=\sum_{n=1}^\infty \frac{(-1)^{n+1}x^n}{n}, \ |x| < 1. $$ I was trying to do use this series expansion but wasn't sure how to go about it because of the square of the logarithm.  And also it seems than we then will have $I\propto \int_0^\infty x^{n-3/2}dx$ which will diverge. Thanks","I am trying to evaluate this integral. $$ I=\int_0^\infty \frac{ \ln^2(1+x)}{x^{3/2}} dx=8\pi \ln 2 $$  Note $$ \ln(1+x)=\sum_{n=1}^\infty \frac{(-1)^{n+1}x^n}{n}, \ |x| < 1. $$ I was trying to do use this series expansion but wasn't sure how to go about it because of the square of the logarithm.  And also it seems than we then will have $I\propto \int_0^\infty x^{n-3/2}dx$ which will diverge. Thanks",,"['calculus', 'real-analysis', 'integration', 'definite-integrals', 'contour-integration']"
24,Translation invariant measures on $\mathbb R$.,Translation invariant measures on .,\mathbb R,What are all the translation invariant measures on $\mathbb{R}$? Except Lebesgue measure on $\mathbb R$ I didn't find any translation invariant measure. So I put this question? I know that if $\mu$ is a measure then $c \times \mu$ is again a measure where $c>0$.,What are all the translation invariant measures on $\mathbb{R}$? Except Lebesgue measure on $\mathbb R$ I didn't find any translation invariant measure. So I put this question? I know that if $\mu$ is a measure then $c \times \mu$ is again a measure where $c>0$.,,"['real-analysis', 'measure-theory']"
25,Compute: $\sum\limits_{n=1}^{\infty}\frac {1\cdot 3\cdots (2n-1)} {2\cdot 4\cdots (2n)\cdot (2n+1)}$,Compute:,\sum\limits_{n=1}^{\infty}\frac {1\cdot 3\cdots (2n-1)} {2\cdot 4\cdots (2n)\cdot (2n+1)},"Compute the sum: $$\sum_{n=1}^{\infty}\dfrac {1\cdot 3\cdots (2n-1)} {2\cdot 4\cdots (2n)\cdot (2n+1)}$$ At the moment, I only know that it's convergent and this is not hard to see if you look at the answers here I received for other problem with a similar series. For the further steps I need some hints if possible. Thanks!","Compute the sum: $$\sum_{n=1}^{\infty}\dfrac {1\cdot 3\cdots (2n-1)} {2\cdot 4\cdots (2n)\cdot (2n+1)}$$ At the moment, I only know that it's convergent and this is not hard to see if you look at the answers here I received for other problem with a similar series. For the further steps I need some hints if possible. Thanks!",,"['calculus', 'real-analysis', 'sequences-and-series']"
26,$e^{-nx}\cdot\sum_{k=0}^\infty\frac{(nx)^k}{k!}f\left(\frac{k}{n}\right)\to f(x)$ for $f$ continuous and bounded,for  continuous and bounded,e^{-nx}\cdot\sum_{k=0}^\infty\frac{(nx)^k}{k!}f\left(\frac{k}{n}\right)\to f(x) f,"Let $f:\mathbb R\to\mathbb R$ be continuous and bounded. Prove that for each $x>0$ we have $$f(x)=\lim_{n\to\infty}\left(e^{-nx}\cdot\sum_{k=0}^\infty\frac{(nx)^k}{k!}f\left(\frac{k}{n}\right)\right).$$ When $x$ is an integer we have $$e^{-nx}\cdot\sum_{k=0}^\infty\frac{(nx)^k}{k!}f\left(\frac{k}{n}\right)=\frac{e^{-nx}}{(nx)!}\cdot\sum_{k=0}^\infty\binom{nx}{k}(nx-k)!(nx)^k.$$ I then substitute the gamma function integral formula for the factorial, swap the order of $\sum$ and $\int$ and apply the binomial theorem to get that this is equal to $$\frac{e^{-nx}}{(nx)!}\int_0^\infty(nx+y)^{nx}e^{-y}\,dy.$$ This doesn't look easy to solve. When $x$ is not an integer, I am having even more difficulties. I would appreciate any help.","Let be continuous and bounded. Prove that for each we have When is an integer we have I then substitute the gamma function integral formula for the factorial, swap the order of and and apply the binomial theorem to get that this is equal to This doesn't look easy to solve. When is not an integer, I am having even more difficulties. I would appreciate any help.","f:\mathbb R\to\mathbb R x>0 f(x)=\lim_{n\to\infty}\left(e^{-nx}\cdot\sum_{k=0}^\infty\frac{(nx)^k}{k!}f\left(\frac{k}{n}\right)\right). x e^{-nx}\cdot\sum_{k=0}^\infty\frac{(nx)^k}{k!}f\left(\frac{k}{n}\right)=\frac{e^{-nx}}{(nx)!}\cdot\sum_{k=0}^\infty\binom{nx}{k}(nx-k)!(nx)^k. \sum \int \frac{e^{-nx}}{(nx)!}\int_0^\infty(nx+y)^{nx}e^{-y}\,dy. x","['real-analysis', 'calculus', 'sequences-and-series', 'limits', 'analysis']"
27,"If two continuous functions are equal almost everywhere on $[a,b]$, then they are equal everywhere on $[a,b]$","If two continuous functions are equal almost everywhere on , then they are equal everywhere on","[a,b] [a,b]","Suppose $f$ and $g$ are continuous functions on $[a,b]$. Show that if $f=g$ almost everywhere on $[a,b]$, then, in fact, $f=g$ on $[a,b]$. Is a similar assertion true if $[a,b]$ is replaced by a general measurable set $E$? I have known that the set $A=\{x \mid f(x) \neq g(x)\}$ has measure zero and we want to show that A is empty. Now let's assume $A$ is not empty. I am stuck in getting the contradiction. Thanks for your hints and answers.","Suppose $f$ and $g$ are continuous functions on $[a,b]$. Show that if $f=g$ almost everywhere on $[a,b]$, then, in fact, $f=g$ on $[a,b]$. Is a similar assertion true if $[a,b]$ is replaced by a general measurable set $E$? I have known that the set $A=\{x \mid f(x) \neq g(x)\}$ has measure zero and we want to show that A is empty. Now let's assume $A$ is not empty. I am stuck in getting the contradiction. Thanks for your hints and answers.",,['real-analysis']
28,Why don't we use closed covers to define compactness of metric space?,Why don't we use closed covers to define compactness of metric space?,,"I'm a beginner in metric space. So many books I've read, there is only the notion of open covers. I want to know why do we worry about open covers to define the compactness of metric spaces and why don't we use closed covers? What is the problem in defining closed cover of a set? Can we use the alternative definition of compactness: ""Every closed cover has a finite subcover""?","I'm a beginner in metric space. So many books I've read, there is only the notion of open covers. I want to know why do we worry about open covers to define the compactness of metric spaces and why don't we use closed covers? What is the problem in defining closed cover of a set? Can we use the alternative definition of compactness: ""Every closed cover has a finite subcover""?",,"['real-analysis', 'general-topology', 'metric-spaces', 'compactness']"
29,Proving that a Hölder space is a Banach space,Proving that a Hölder space is a Banach space,,"I am trying to show that the Hölder space $C^{k,\gamma}(\bar{U})$ is a Banach space. To do this, I successfully proved that the mapping $\| \quad \| : C^{k,\gamma}(\bar{U}) \to [0,\infty)$ is a norm, by proving its properties. But how do I show that the sequence $\{u_k\}_{k=1}^\infty \subset C^{k,\gamma}(\bar{U})$ converges to $u \in C^{k,\gamma}(\bar{U})$, that is, how do I show that $$\lim_{k \to \infty}\|u_k-u\|=0,$$ which would mean the normed linear space is complete , and hence a Banach space? Here are the following taken from PDE Evans, 2nd edition, page 255: Definition. The Hölder space $$C^{k,\gamma}(\bar{U})$$ consists of all functions $u \in C^k(\bar{U})$ for which the norm $$\|u\|_{C^{k,y}(\bar{U})}:= \sum_{|\alpha|\le k} \|D^\alpha u \|_{C(\bar{U})}+\sum_{|\alpha|=k} [D^\alpha u]_{C^{0,\gamma}(\bar{U})}$$ is finite. Also from page 254, Definitions. (i) If $u : U \to \mathbb{R}$ is bounded and continuous, we write $$\|u\|_{C(\bar{U})}:=\sup_{x\in U}|u(x)|.$$ (ii) The $\gamma$th-Hölder seminorm of $u : U \to \mathbb{R}$ is $$[u]_{C^{0,\gamma}(\bar{U})}:=\sup_{\substack{x,y\in U \\ x \neq y}} \left\{\frac{|u(x)-u(y)|}{|x-y|^\gamma} \right\},$$ and the $\gamma$th-Hölder norm is $$\|u\|_{C^{0,\gamma}(\bar{U})}:=\|u\|_{C(\bar{U})}+[u]_{C^{0,\gamma}(\bar{U})}.$$ This is all I got so far: \begin{align} \|u_k-u\|_{C^{k,\gamma}(\bar{U})}&=\sum_{|\alpha|\le k} \|D^\alpha u \|_{C(\bar{U})}+\sum_{|\alpha|=k} [D^\alpha u]_{C^{0,\gamma}(\bar{U})} \\ &= \sum_{|\alpha|\le k} \sup_{x\in U} |u_k(x)-u(x)|+ \sum_{|\alpha|=k} \sup_{\substack{x,y\in U \\ x \neq y}} \left\{\frac{|[u_k(x)-u(x)]-[u_k(y)-u(y)]|}{|x-y|^\gamma} \right\}. \end{align} Now, where can I go from here, to show that $\lim_{k \to \infty}\|u_k-u\|_{C^{k,\gamma}(\bar{U})}=0$? The sequence is Cauchy, and I have to use that fact somehow.","I am trying to show that the Hölder space $C^{k,\gamma}(\bar{U})$ is a Banach space. To do this, I successfully proved that the mapping $\| \quad \| : C^{k,\gamma}(\bar{U}) \to [0,\infty)$ is a norm, by proving its properties. But how do I show that the sequence $\{u_k\}_{k=1}^\infty \subset C^{k,\gamma}(\bar{U})$ converges to $u \in C^{k,\gamma}(\bar{U})$, that is, how do I show that $$\lim_{k \to \infty}\|u_k-u\|=0,$$ which would mean the normed linear space is complete , and hence a Banach space? Here are the following taken from PDE Evans, 2nd edition, page 255: Definition. The Hölder space $$C^{k,\gamma}(\bar{U})$$ consists of all functions $u \in C^k(\bar{U})$ for which the norm $$\|u\|_{C^{k,y}(\bar{U})}:= \sum_{|\alpha|\le k} \|D^\alpha u \|_{C(\bar{U})}+\sum_{|\alpha|=k} [D^\alpha u]_{C^{0,\gamma}(\bar{U})}$$ is finite. Also from page 254, Definitions. (i) If $u : U \to \mathbb{R}$ is bounded and continuous, we write $$\|u\|_{C(\bar{U})}:=\sup_{x\in U}|u(x)|.$$ (ii) The $\gamma$th-Hölder seminorm of $u : U \to \mathbb{R}$ is $$[u]_{C^{0,\gamma}(\bar{U})}:=\sup_{\substack{x,y\in U \\ x \neq y}} \left\{\frac{|u(x)-u(y)|}{|x-y|^\gamma} \right\},$$ and the $\gamma$th-Hölder norm is $$\|u\|_{C^{0,\gamma}(\bar{U})}:=\|u\|_{C(\bar{U})}+[u]_{C^{0,\gamma}(\bar{U})}.$$ This is all I got so far: \begin{align} \|u_k-u\|_{C^{k,\gamma}(\bar{U})}&=\sum_{|\alpha|\le k} \|D^\alpha u \|_{C(\bar{U})}+\sum_{|\alpha|=k} [D^\alpha u]_{C^{0,\gamma}(\bar{U})} \\ &= \sum_{|\alpha|\le k} \sup_{x\in U} |u_k(x)-u(x)|+ \sum_{|\alpha|=k} \sup_{\substack{x,y\in U \\ x \neq y}} \left\{\frac{|[u_k(x)-u(x)]-[u_k(y)-u(y)]|}{|x-y|^\gamma} \right\}. \end{align} Now, where can I go from here, to show that $\lim_{k \to \infty}\|u_k-u\|_{C^{k,\gamma}(\bar{U})}=0$? The sequence is Cauchy, and I have to use that fact somehow.",,"['real-analysis', 'functional-analysis', 'banach-spaces', 'holder-spaces']"
30,Does $f(x)$ is continuous and $f = 0$ a.e. imply $f=0$ everywhere?,Does  is continuous and  a.e. imply  everywhere?,f(x) f = 0 f=0,"I wanna prove that ""if $f: \mathbb{R}^n \to \mathbb{R}$ is continuous and satisfies $f=0$ almost everywhere (in the sense of Lebesgue measure), then, $f=0$ everywhere."" I am confident that the statement is true, but stuck with the proof. Also, is the statement true if the domain $\mathbb{R}^n$ is restricted to $\Omega \subseteq \mathbb{R}^n$ that contains a neighborhood of the origin ""$0$""?","I wanna prove that ""if $f: \mathbb{R}^n \to \mathbb{R}$ is continuous and satisfies $f=0$ almost everywhere (in the sense of Lebesgue measure), then, $f=0$ everywhere."" I am confident that the statement is true, but stuck with the proof. Also, is the statement true if the domain $\mathbb{R}^n$ is restricted to $\Omega \subseteq \mathbb{R}^n$ that contains a neighborhood of the origin ""$0$""?",,"['real-analysis', 'measure-theory']"
31,How to find $\lim_{n\to\infty}\Big(\sum_{k=1}^n\sqrt{\frac{1}{k}-\frac{1}{n}}-\frac{\pi}{2}\sqrt n\Big)$?,How to find ?,\lim_{n\to\infty}\Big(\sum_{k=1}^n\sqrt{\frac{1}{k}-\frac{1}{n}}-\frac{\pi}{2}\sqrt n\Big),"I'm looking for the asymptotic of the series $\displaystyle S(n)=\frac{1}{2}\sum_{k=1}^n\int_k^n\frac{dx}{x\sqrt{x(n-x)}}\,$ at $\,n\to\infty$ The first term can be found, for example, via switching to Riemann sums, changing the order of integration, and is equal to $\frac{\pi}{2}$ . To find the next term I performed integration and got $$S(n)=\frac{1}{n}\sum_{k=1}^{n-1}\sqrt{\frac{n}{k}-1}=\frac{1}{\sqrt n}\sum_{k=1}^{n-1}\sqrt{\frac{1}{k}-\frac{1}{n}}=\frac{1}{\sqrt n}\sum_{k=1}^{n-1}f(k)\tag{1}$$ To get the flavour of the next asymptotic term I used the Euler-Maclaurin formula $$\sum_{k=1}^{n-1}f(k)\sim\int_1^{n-1}f(k)dk+\frac{1}{2}\big(f(1)+f(n-1)\big)+\frac{1}{12}\big(f'(n-1)-f'(1)\big)+ ...\tag{2}$$ At $n\to\infty$ the first term (the integral) gives $\frac{\pi}{2}\sqrt n-2$ ; other terms in (2) give non-zero values at $k=1$ . Evaluating a couple of such terms, I got $$\sum_{k=1}^{n-1}f(k)\sim\frac{\pi}{2}\sqrt n-2+\frac{1}{2}+\frac{1}{24}=\frac{\pi}{2}\sqrt n-1.4583$$ The numeric evaluation at WolframAlpha for $n=1000$ gives $$ \sum_{k=1}^{n-1}f(k)-\frac{\pi}{2}\sqrt n\,\bigg|_{n=1000}=-1.46046$$ All this strongly resembles $\displaystyle \zeta\Big(\frac{1}{2}\Big)=-1.46035...\,\,$ ; at $\,n=10000\,$ we get even better agreement. Questions: How can we prove that $\,\,\displaystyle \lim_{n\to\infty}\Big(\sum_{k=1}^n\sqrt{\frac{1}{k}-\frac{1}{n}}-\frac{\pi}{2}\sqrt n\Big)=\zeta\Big(\frac{1}{2}\Big)\,\,$ ? Can we get next asymptotic terms (at least, several of them) analytically ?","I'm looking for the asymptotic of the series at The first term can be found, for example, via switching to Riemann sums, changing the order of integration, and is equal to . To find the next term I performed integration and got To get the flavour of the next asymptotic term I used the Euler-Maclaurin formula At the first term (the integral) gives ; other terms in (2) give non-zero values at . Evaluating a couple of such terms, I got The numeric evaluation at WolframAlpha for gives All this strongly resembles ; at we get even better agreement. Questions: How can we prove that ? Can we get next asymptotic terms (at least, several of them) analytically ?","\displaystyle S(n)=\frac{1}{2}\sum_{k=1}^n\int_k^n\frac{dx}{x\sqrt{x(n-x)}}\, \,n\to\infty \frac{\pi}{2} S(n)=\frac{1}{n}\sum_{k=1}^{n-1}\sqrt{\frac{n}{k}-1}=\frac{1}{\sqrt n}\sum_{k=1}^{n-1}\sqrt{\frac{1}{k}-\frac{1}{n}}=\frac{1}{\sqrt n}\sum_{k=1}^{n-1}f(k)\tag{1} \sum_{k=1}^{n-1}f(k)\sim\int_1^{n-1}f(k)dk+\frac{1}{2}\big(f(1)+f(n-1)\big)+\frac{1}{12}\big(f'(n-1)-f'(1)\big)+ ...\tag{2} n\to\infty \frac{\pi}{2}\sqrt n-2 k=1 \sum_{k=1}^{n-1}f(k)\sim\frac{\pi}{2}\sqrt n-2+\frac{1}{2}+\frac{1}{24}=\frac{\pi}{2}\sqrt n-1.4583 n=1000  \sum_{k=1}^{n-1}f(k)-\frac{\pi}{2}\sqrt n\,\bigg|_{n=1000}=-1.46046 \displaystyle \zeta\Big(\frac{1}{2}\Big)=-1.46035...\,\, \,n=10000\, \,\,\displaystyle \lim_{n\to\infty}\Big(\sum_{k=1}^n\sqrt{\frac{1}{k}-\frac{1}{n}}-\frac{\pi}{2}\sqrt n\Big)=\zeta\Big(\frac{1}{2}\Big)\,\,","['real-analysis', 'sequences-and-series', 'limits', 'asymptotics', 'zeta-functions']"
32,Why is the Monotone Convergence Theorem restricted to a nonnegative function sequence?,Why is the Monotone Convergence Theorem restricted to a nonnegative function sequence?,,"Monotone Convergence Theorem for general measure: Let $(X,\Sigma,\mu)$ be a measure space. Let $f_1, f_2, ...$ be a pointwise non-decreasing sequence of $[0, \infty]$ -valued $\Sigma-$ measurable functions, i.e. for every $k\ge 1$ and every $x$ in $X$ , $$0 \le f_k(x) \le f_{k+1}(x).$$ Next, set the pointwise limit of the sequence ${f_n}$ to be $f$ . That is, for every $x$ in $X$ , $$f(x) = \lim_{k\to \infty}f_k(x).$$ Then $f$ is $\Sigma-$ measurable and $$\lim_{k\to \infty}\int f_k d\mu = \int f d\mu.$$ I've noticed that when it comes to monotone convergence theorem (either Lebesgue or general measure), usually its definition restricts the monotone function sequences to be nonnegative. I'm not sure why the 'nonnegative' is necessary.","Monotone Convergence Theorem for general measure: Let be a measure space. Let be a pointwise non-decreasing sequence of -valued measurable functions, i.e. for every and every in , Next, set the pointwise limit of the sequence to be . That is, for every in , Then is measurable and I've noticed that when it comes to monotone convergence theorem (either Lebesgue or general measure), usually its definition restricts the monotone function sequences to be nonnegative. I'm not sure why the 'nonnegative' is necessary.","(X,\Sigma,\mu) f_1, f_2, ... [0, \infty] \Sigma- k\ge 1 x X 0 \le f_k(x) \le f_{k+1}(x). {f_n} f x X f(x) = \lim_{k\to \infty}f_k(x). f \Sigma- \lim_{k\to \infty}\int f_k d\mu = \int f d\mu.","['real-analysis', 'integration', 'measure-theory', 'lebesgue-integral', 'lebesgue-measure']"
33,Why do we need to check for more than $\frac{\infty}{\infty}$ or $\frac{0}{0}$ when applying L'Hospital?,Why do we need to check for more than  or  when applying L'Hospital?,\frac{\infty}{\infty} \frac{0}{0},With regard to this comment I wanted to ask (and provide an answer): what else do we need to assume and check for before we can apply L'Hospital's rule? And why do we have to do this e.g. does it really matter if we don't check them?,With regard to this comment I wanted to ask (and provide an answer): what else do we need to assume and check for before we can apply L'Hospital's rule? And why do we have to do this e.g. does it really matter if we don't check them?,,"['calculus', 'real-analysis', 'limits', 'limits-without-lhopital']"
34,"The 3 Integral $\int_0^\infty {x\,{\rm d}x\over \sqrt[3]{\,\left(e^{3x}-1\right)^2\,}}=\frac{\pi}{3\sqrt 3}\big(\log 3-\frac{\pi}{3\sqrt 3} \big)$",The 3 Integral,"\int_0^\infty {x\,{\rm d}x\over \sqrt[3]{\,\left(e^{3x}-1\right)^2\,}}=\frac{\pi}{3\sqrt 3}\big(\log 3-\frac{\pi}{3\sqrt 3} \big)","Hi I am trying evaluate this integral and obtain the closed form:$$ I:=\int_0^\infty \frac{x\,dx}{\sqrt[\large 3]{(e^{3x}-1)^2}}=\frac{\pi}{3\sqrt 3}\left(\log 3-\frac{\pi}{3\sqrt 3}		\right). $$ The integral and result has all 3's everywhere.   I am not sure how to approach this on.  The denominator seems to be a problem. If $\displaystyle x=\frac{2in\pi}{3}$ we have a singularity but I am not sure how to use complex methods. We will have  a branch cut because of the root function singularity. Differentiating under the integral sign did not help either.  I tried partial integration with $v=(e^{3x}-1)^{\frac{2}{3}}$ but this did not simplify since I get a power $x^n, \ (n>1)$ in the new integral. Thanks, how can we evaluate the integral I?","Hi I am trying evaluate this integral and obtain the closed form:$$ I:=\int_0^\infty \frac{x\,dx}{\sqrt[\large 3]{(e^{3x}-1)^2}}=\frac{\pi}{3\sqrt 3}\left(\log 3-\frac{\pi}{3\sqrt 3}		\right). $$ The integral and result has all 3's everywhere.   I am not sure how to approach this on.  The denominator seems to be a problem. If $\displaystyle x=\frac{2in\pi}{3}$ we have a singularity but I am not sure how to use complex methods. We will have  a branch cut because of the root function singularity. Differentiating under the integral sign did not help either.  I tried partial integration with $v=(e^{3x}-1)^{\frac{2}{3}}$ but this did not simplify since I get a power $x^n, \ (n>1)$ in the new integral. Thanks, how can we evaluate the integral I?",,"['calculus', 'real-analysis', 'integration', 'complex-analysis', 'definite-integrals']"
35,"If $A$ is positive definite, then $\int_{\mathbb{R}^n}\mathrm{e}^{-\langle Ax,x\rangle}\text{d}x=\left|\det\left({\pi}^{-1}A\right)\right|^{-1/2}$","If  is positive definite, then","A \int_{\mathbb{R}^n}\mathrm{e}^{-\langle Ax,x\rangle}\text{d}x=\left|\det\left({\pi}^{-1}A\right)\right|^{-1/2}","Let $A$ be a positive definite real $n\times n$ matrix. How can I prove that $$ \int_{\mathbb{R}^n}\mathrm{e}^{-\langle Ax,x\rangle}\text{d}x=\left|\,\det\left(\pi^{-1}{A}\right)\right|^{-1/2}=\pi^{n/2}\lvert\,\det A\rvert^{-1/2}\!, $$ where $\langle\cdot,\cdot\rangle$ denotes the inner product in $\mathbb R^n$, i.e. $\langle x,y\rangle =x^Ty$.","Let $A$ be a positive definite real $n\times n$ matrix. How can I prove that $$ \int_{\mathbb{R}^n}\mathrm{e}^{-\langle Ax,x\rangle}\text{d}x=\left|\,\det\left(\pi^{-1}{A}\right)\right|^{-1/2}=\pi^{n/2}\lvert\,\det A\rvert^{-1/2}\!, $$ where $\langle\cdot,\cdot\rangle$ denotes the inner product in $\mathbb R^n$, i.e. $\langle x,y\rangle =x^Ty$.",,"['real-analysis', 'integration', 'matrices', 'normal-distribution', 'determinant']"
36,"How do I prove that $f(x)f(y)=f(x+y)$ implies that $f(x)=e^{cx}$, assuming f is continuous and not zero?","How do I prove that  implies that , assuming f is continuous and not zero?",f(x)f(y)=f(x+y) f(x)=e^{cx},"This is part of a homework assignment for a real analysis course taught out of ""Baby Rudin.""  Just looking for a push in the right direction, not a full-blown solution.  We are to suppose that $f(x)f(y)=f(x+y)$ for all real x and y, and that f is continuous and not zero.  The first part of this question let me assume differentiability as well, and I was able to compose it with the natural log and take the derivative to prove that $f(x)=e^{cx}$ where c is a real constant.  I'm having a little more trouble only assuming continuity; I'm currently trying to prove that f is differentiable at zero, and hence all real numbers.  Is this an approach worth taking?","This is part of a homework assignment for a real analysis course taught out of ""Baby Rudin.""  Just looking for a push in the right direction, not a full-blown solution.  We are to suppose that $f(x)f(y)=f(x+y)$ for all real x and y, and that f is continuous and not zero.  The first part of this question let me assume differentiability as well, and I was able to compose it with the natural log and take the derivative to prove that $f(x)=e^{cx}$ where c is a real constant.  I'm having a little more trouble only assuming continuity; I'm currently trying to prove that f is differentiable at zero, and hence all real numbers.  Is this an approach worth taking?",,"['real-analysis', 'functional-equations']"
37,Subsets of the Cantor set,Subsets of the Cantor set,,"Does anyone know of any interesting subsets of the Cantor set? When I first started thinking about this, I thought that since the Cantor set $C$ is the intersection of disjoint unions of closed sets, those particular closed sets would be subsets of $C$. Those sets, however, happen to be intervals! The Cantor set contains no isolated points nor (open) intervals, so then what subsets does it contain besides $\varnothing$ and itself?","Does anyone know of any interesting subsets of the Cantor set? When I first started thinking about this, I thought that since the Cantor set $C$ is the intersection of disjoint unions of closed sets, those particular closed sets would be subsets of $C$. Those sets, however, happen to be intervals! The Cantor set contains no isolated points nor (open) intervals, so then what subsets does it contain besides $\varnothing$ and itself?",,"['real-analysis', 'general-topology', 'descriptive-set-theory', 'cantor-set']"
38,Infinite Series $\sum\limits_{n=0}^\infty\frac{(-1)^n}{(2n+1)^{2m+1}}$,Infinite Series,\sum\limits_{n=0}^\infty\frac{(-1)^n}{(2n+1)^{2m+1}},"I'm looking for a way to prove $$\sum_{n=0}^\infty\frac{(-1)^n}{(2n+1)^{2m+1}}=\frac{(-1)^m E_{2m}\pi^{2m+1}}{4^{m+1}(2m)!}$$ I know that $$\sum_{n=0}^\infty\frac{(-1)^n}{(2n+1)^{2m+1}}=\frac{1}{4^{2m+1}}\left(\zeta\left(2m+1,\frac14\right)-\zeta\left(2m+1,\frac34\right)\right)$$ so maybe I could simplify the above more?","I'm looking for a way to prove $$\sum_{n=0}^\infty\frac{(-1)^n}{(2n+1)^{2m+1}}=\frac{(-1)^m E_{2m}\pi^{2m+1}}{4^{m+1}(2m)!}$$ I know that $$\sum_{n=0}^\infty\frac{(-1)^n}{(2n+1)^{2m+1}}=\frac{1}{4^{2m+1}}\left(\zeta\left(2m+1,\frac14\right)-\zeta\left(2m+1,\frac34\right)\right)$$ so maybe I could simplify the above more?",,"['calculus', 'real-analysis']"
39,Transfinite derivatives,Transfinite derivatives,,"I don't know if this is exactly research level, as I am only starting college. But I feel like this is the best place to ask the question. We all know of 1st, 2nd, 3rd, nth derivatives. Is there a way of extending it to the transfinite ordinals? More specifically, can anyone show me a link to a webpage or paper that discuss this concept? Even if this question gets closed, I would still like a link to one or a few papers about this concept. I have searched google, but I can't find anything.","I don't know if this is exactly research level, as I am only starting college. But I feel like this is the best place to ask the question. We all know of 1st, 2nd, 3rd, nth derivatives. Is there a way of extending it to the transfinite ordinals? More specifically, can anyone show me a link to a webpage or paper that discuss this concept? Even if this question gets closed, I would still like a link to one or a few papers about this concept. I have searched google, but I can't find anything.",,['real-analysis']
40,Show that the operator norm is submultiplicative,Show that the operator norm is submultiplicative,,"We had in our lecture on numerical analysis the following: Let $\mathrm{Lin}(X,Y)$ be the set of all linear maps $X\rightarrow Y$ . Let $A\in\mathrm{Lin}(\mathbb R^l,\mathbb R^n)$ and $B\in\mathrm{Lin}(\mathbb R^n,\mathbb R^m)$ and $\|C\|_{op}:=\max_{\|x\|\leq1}\|C(x)\|$ . Then our lecturer followed $\|A\circ B\|_{op}\leq\|A\|_{op}\cdot\|B\|_{op}$ . So he didn't prove it and so I've tried it by my own. My attempt: $$\begin{aligned} \|A\circ B\|_{\mathrm{op}} &= \max_{\|x\| < 1}\|(A \circ B)(x)\| \\ & \leq \max_{\|x\| < 1} \|A\|_{\mathrm{op}} \|B(x)\| \\ &= \|A\|_{\mathrm{op}} \max_{\|x\|\leq1}\|B(x)\|\\ &= \|A\|_{\mathrm{op}}\|B\|_{\mathrm{op}} \end{aligned}$$ But this seems too easy. I am really interested in a nice proof so anybody could help? Thanks a lot!",We had in our lecture on numerical analysis the following: Let be the set of all linear maps . Let and and . Then our lecturer followed . So he didn't prove it and so I've tried it by my own. My attempt: But this seems too easy. I am really interested in a nice proof so anybody could help? Thanks a lot!,"\mathrm{Lin}(X,Y) X\rightarrow Y A\in\mathrm{Lin}(\mathbb R^l,\mathbb R^n) B\in\mathrm{Lin}(\mathbb R^n,\mathbb R^m) \|C\|_{op}:=\max_{\|x\|\leq1}\|C(x)\| \|A\circ B\|_{op}\leq\|A\|_{op}\cdot\|B\|_{op} \begin{aligned} \|A\circ B\|_{\mathrm{op}} &= \max_{\|x\| < 1}\|(A \circ B)(x)\| \\ & \leq \max_{\|x\| < 1} \|A\|_{\mathrm{op}} \|B(x)\| \\ &= \|A\|_{\mathrm{op}} \max_{\|x\|\leq1}\|B(x)\|\\ &= \|A\|_{\mathrm{op}}\|B\|_{\mathrm{op}} \end{aligned}","['real-analysis', 'inequality', 'linear-transformations', 'normed-spaces', 'matrix-norms']"
41,Clarification of L'Hopital Proof Pugh,Clarification of L'Hopital Proof Pugh,,"I am self-studying Real Analysis right now via Pugh's Real Mathematical Analysis but am having trouble understanding a step of the author's proof of L'Hopital's rule. The theorem is stated as: If $f$ and $g$ are differentiable functions defined on an intveral $(a,b)$ , both of which tend to $0$ at $b$ , ad if the ratio of their derivatives $f'(x)/g'(x)$ tends to a finite limit $L$ at $b$ then $f(x)/g(x)$ also tends to $L$ at $b$ , where $g(x),g'(x) \neq 0.$ His proof reads as follows: Given $\epsilon > 0$ we must find a $\delta > 0$ such that if $|x-b| < \delta$ then $|f(x)/g(x) - L|< \epsilon.$ Since $f'(x)/g'(x)$ tends to $L$ as $x$ tends to $b$ there does exist a $\delta > 0$ such that if $x \in (b-\delta, b)$ then $$\left\vert \frac{f'(x)}{g'(x)}-L \right\vert < \frac \epsilon 2.$$ For each $x \in (b-\delta, b)$ determine a point $t \in (b-\delta, b)$ which is so near to $b$ that \begin{align}|f(t)+g(t)| &< \frac{g(x)^2\epsilon}{4(|f(x)|+|g(x)|)} \\ |g(t)| &< \frac{|g(x)|}{2}.\end{align} Since $f(t)$ and $g(t)$ tend to $0$ as $t$ tends to $b$ , and since $g(x) \neq 0$ such a $t$ exists. It depends on $x$ , of course. By this choice of $t$ and the Ratio Mean Value Theorem we have, for some $\theta \in (x,t),$ \begin{align*}\left\vert \frac{f'(x)}{g'(x)}-L \right\vert &= \left\vert \frac{f(x)}{g(x)}-\frac{f(x)-f(t)}{g(x)-g(t)}+\frac{f(x)-f(t)}{g(x)-g(t)} - L \right\vert \\ &\le \left\vert \frac{g(x)f(t)-f(x)g(t)}{g(x)(g(x)-g(t))} \right\vert + \left\vert \frac{f'(\theta)}{g'(\theta)}-L \right\vert < \epsilon, \end{align*} which completes the proof that $f(x)/g(x) \to L$ as $x \to b.$ The part I didn't get was the last inequality $$\left\vert \frac{g(x)f(t)-f(x)g(t)}{g(x)(g(x)-g(t))} \right\vert + \left\vert \frac{f'(\theta)}{g'(\theta)}-L \right\vert < \epsilon,$$ which I'm sure relates to his constraints on $|f(t) + g(t)|$ and $g(t)$ . I understood his general point about $f(t)/f(x), g(t)/g(x)$ getting arbitrarily small so that $$\frac{f(x)}{g(x)} \approx \frac{f(x)-f(t)}{g(x)-g(t)}$$ but don't really understand the finer details of the proof. Any help is greatly appreciated. :)","I am self-studying Real Analysis right now via Pugh's Real Mathematical Analysis but am having trouble understanding a step of the author's proof of L'Hopital's rule. The theorem is stated as: If and are differentiable functions defined on an intveral , both of which tend to at , ad if the ratio of their derivatives tends to a finite limit at then also tends to at , where His proof reads as follows: Given we must find a such that if then Since tends to as tends to there does exist a such that if then For each determine a point which is so near to that Since and tend to as tends to , and since such a exists. It depends on , of course. By this choice of and the Ratio Mean Value Theorem we have, for some which completes the proof that as The part I didn't get was the last inequality which I'm sure relates to his constraints on and . I understood his general point about getting arbitrarily small so that but don't really understand the finer details of the proof. Any help is greatly appreciated. :)","f g (a,b) 0 b f'(x)/g'(x) L b f(x)/g(x) L b g(x),g'(x) \neq 0. \epsilon > 0 \delta > 0 |x-b| < \delta |f(x)/g(x) - L|< \epsilon. f'(x)/g'(x) L x b \delta > 0 x \in (b-\delta, b) \left\vert \frac{f'(x)}{g'(x)}-L \right\vert < \frac \epsilon 2. x \in (b-\delta, b) t \in (b-\delta, b) b \begin{align}|f(t)+g(t)| &< \frac{g(x)^2\epsilon}{4(|f(x)|+|g(x)|)} \\ |g(t)| &< \frac{|g(x)|}{2}.\end{align} f(t) g(t) 0 t b g(x) \neq 0 t x t \theta \in (x,t), \begin{align*}\left\vert \frac{f'(x)}{g'(x)}-L \right\vert &= \left\vert \frac{f(x)}{g(x)}-\frac{f(x)-f(t)}{g(x)-g(t)}+\frac{f(x)-f(t)}{g(x)-g(t)} - L \right\vert \\ &\le \left\vert \frac{g(x)f(t)-f(x)g(t)}{g(x)(g(x)-g(t))} \right\vert + \left\vert \frac{f'(\theta)}{g'(\theta)}-L \right\vert < \epsilon, \end{align*} f(x)/g(x) \to L x \to b. \left\vert \frac{g(x)f(t)-f(x)g(t)}{g(x)(g(x)-g(t))} \right\vert + \left\vert \frac{f'(\theta)}{g'(\theta)}-L \right\vert < \epsilon, |f(t) + g(t)| g(t) f(t)/f(x), g(t)/g(x) \frac{f(x)}{g(x)} \approx \frac{f(x)-f(t)}{g(x)-g(t)}",['real-analysis']
42,Difference between limsup and sup,Difference between limsup and sup,,I'm having some difficulty visualising the difference between the limit supremum and supremum (and for limit infimum/infimum) for bounded sequences. Would it be possible for some to provide a brief explanation and maybe some examples?,I'm having some difficulty visualising the difference between the limit supremum and supremum (and for limit infimum/infimum) for bounded sequences. Would it be possible for some to provide a brief explanation and maybe some examples?,,['real-analysis']
43,Almost Everywhere Convergence versus Convergence in Measure,Almost Everywhere Convergence versus Convergence in Measure,,"I am having some conceptual difficulties with almost everywhere (a.e.) convergence  versus convergence in measure. Let $f_{n} : X \to Y$. In my mind, a sequence of measurable functions $\{ f_{n} \}$ converges a.e. to the function $f$ if $f_{n} \to f$ everywhere on $X$ except for maybe on some set of measure zero. However, the definition of convergence in measure is that $f_{n} \to f$ in measure if for every $\varepsilon>0$ \begin{equation} \mu(\{x:|f_{n}(x)-f(x)|\geq\varepsilon\})\to0 \text{ as } n\to \infty \end{equation} Heuristically, it seems to me that convergence in measure is just saying that the subset of $X$ for which $f_{n}$ does not converge to $f$ must have a measure of zero. I cannot see the difference between a.e. convergence and convergence in measure. If anyone can point out my conceptual error it would be much appreciated.","I am having some conceptual difficulties with almost everywhere (a.e.) convergence  versus convergence in measure. Let $f_{n} : X \to Y$. In my mind, a sequence of measurable functions $\{ f_{n} \}$ converges a.e. to the function $f$ if $f_{n} \to f$ everywhere on $X$ except for maybe on some set of measure zero. However, the definition of convergence in measure is that $f_{n} \to f$ in measure if for every $\varepsilon>0$ \begin{equation} \mu(\{x:|f_{n}(x)-f(x)|\geq\varepsilon\})\to0 \text{ as } n\to \infty \end{equation} Heuristically, it seems to me that convergence in measure is just saying that the subset of $X$ for which $f_{n}$ does not converge to $f$ must have a measure of zero. I cannot see the difference between a.e. convergence and convergence in measure. If anyone can point out my conceptual error it would be much appreciated.",,"['real-analysis', 'measure-theory', 'convergence-divergence', 'almost-everywhere']"
44,Interior of a convex set is convex [duplicate],Interior of a convex set is convex [duplicate],,"This question already has answers here : Show that the interior of a convex set is convex (2 answers) Closed 10 years ago . A set $S$ in $\mathbb{R}^n$ is convex if for every pair of points $x,y$ in $S$ and every real $\theta$ where $0 < \theta < 1$, we have $\theta x + (1- \theta) y \in S$. I'm trying to show that the interior of a convex set is convex. If $x, y \in$ int $S$, then I know there exists open balls such that $B(x) \subseteq S$ and $B(y) \subseteq S$. I need to show that there exists a ball $B(\theta x + (1- \theta) y) \subseteq S$. Other then writing down the definitions, I don't really see how to proceed. Could someone give me a hint?","This question already has answers here : Show that the interior of a convex set is convex (2 answers) Closed 10 years ago . A set $S$ in $\mathbb{R}^n$ is convex if for every pair of points $x,y$ in $S$ and every real $\theta$ where $0 < \theta < 1$, we have $\theta x + (1- \theta) y \in S$. I'm trying to show that the interior of a convex set is convex. If $x, y \in$ int $S$, then I know there exists open balls such that $B(x) \subseteq S$ and $B(y) \subseteq S$. I need to show that there exists a ball $B(\theta x + (1- \theta) y) \subseteq S$. Other then writing down the definitions, I don't really see how to proceed. Could someone give me a hint?",,"['real-analysis', 'metric-spaces', 'topological-vector-spaces']"
45,Extension of the Lebesgue measurable sets,Extension of the Lebesgue measurable sets,,"My question is the following : is there a $\sigma$-algebra $\mathcal{T}$ (of subsets of $\mathbb{R^n}$) that contains strictly the $\sigma$-algebra $\mathcal{L}$ of  Lebesgue measurable sets (in $\mathbb{R}^n$), and such that there is a measure on $\mathcal{T}$ that extends the usual Lebesgue measure on $\mathcal{L}$ ? I guess not, but I did not find a reference.","My question is the following : is there a $\sigma$-algebra $\mathcal{T}$ (of subsets of $\mathbb{R^n}$) that contains strictly the $\sigma$-algebra $\mathcal{L}$ of  Lebesgue measurable sets (in $\mathbb{R}^n$), and such that there is a measure on $\mathcal{T}$ that extends the usual Lebesgue measure on $\mathcal{L}$ ? I guess not, but I did not find a reference.",,"['real-analysis', 'measure-theory']"
46,How can I prove $\sqrt{2}^{\sqrt{2}^{\sqrt{2}^{\sqrt{2}^{\sqrt{2}...}}}}=2$ [duplicate],How can I prove  [duplicate],\sqrt{2}^{\sqrt{2}^{\sqrt{2}^{\sqrt{2}^{\sqrt{2}...}}}}=2,This question already has answers here : Are these solutions of $2 = x^{x^{x^{\:\cdot^{\:\cdot^{\:\cdot}}}}}$ correct? (4 answers) Closed 9 years ago . How can I prove $$\sqrt{2}^{\sqrt{2}^{\sqrt{2}^{\sqrt{2}^{\sqrt{2}...}}}}=2$$ I don't know which method can be used for this?,This question already has answers here : Are these solutions of $2 = x^{x^{x^{\:\cdot^{\:\cdot^{\:\cdot}}}}}$ correct? (4 answers) Closed 9 years ago . How can I prove $$\sqrt{2}^{\sqrt{2}^{\sqrt{2}^{\sqrt{2}^{\sqrt{2}...}}}}=2$$ I don't know which method can be used for this?,,"['real-analysis', 'algebra-precalculus', 'roots', 'exponentiation', 'tetration']"
47,Complex differentiability vs differentiability in $\mathbb{R}^2$,Complex differentiability vs differentiability in,\mathbb{R}^2,"I was wondering about the following for quite a while: The euclidean topology on $\mathbb{R}^2$ is homeomorphic with the topology induced by the modulus function on $\mathbb{C}$. Hence limit behavior is the same in both. So, doesn't that mean that the limit which defines the complex derivative converges if and only it converges in $\mathbb{R}^2$? Clearly, I am missing something, since not every differentiable function $\mathbb{R}^2 \rightarrow \mathbb{R}^2$ is smooth, but over $\mathbb{C}$ this is true. P.S.: I read the question: Is Complex Analysis equivalent Real Analysis with $f:\mathbb R^2 \to \mathbb R^2$? , it does however not clear up my confusion.","I was wondering about the following for quite a while: The euclidean topology on $\mathbb{R}^2$ is homeomorphic with the topology induced by the modulus function on $\mathbb{C}$. Hence limit behavior is the same in both. So, doesn't that mean that the limit which defines the complex derivative converges if and only it converges in $\mathbb{R}^2$? Clearly, I am missing something, since not every differentiable function $\mathbb{R}^2 \rightarrow \mathbb{R}^2$ is smooth, but over $\mathbb{C}$ this is true. P.S.: I read the question: Is Complex Analysis equivalent Real Analysis with $f:\mathbb R^2 \to \mathbb R^2$? , it does however not clear up my confusion.",,"['real-analysis', 'complex-analysis']"
48,Prove $\sup(f+g) \le \sup f + \sup g$,Prove,\sup(f+g) \le \sup f + \sup g,"Suppose $D$ is a nonempty bounded subset of reals. Let $f:D \to \mathbb R$ and $g:D \to \mathbb R$ . Define $(f+g)(x)=f(x)+g(x)$ . Prove $\sup(f+g)(D) \le \sup f(D) + \sup g(D)$ (also prove that $\sup (f+g)$ exists). I understand why this is the case, just not how to prove it. Left side is pretty much $\sup  (f(x)+g(x))$ and right side is $\sup (f(x) + g(y))$ for $x,\,y \in D$ . Basically $f+g$ has to use the same variable and $f(D)+g(D)$ use different ones. But I don't know how to go about proving this. The second part of the question is to find a specific example where strict inequality holds. Let $D=\{a,b\}$ and $f: a \to 1,\, b\to 0, \,g: a \to 0,\, b\to 1$ . $\sup f(D) = 1,\, \sup g(D) = 1,\, \sup f(D) + \sup g(D) = 2.$ $\sup (f+g)(D) = 1$ (if we choose a, $f+g = 1+0,\, b,\, f+g=0+1$ ).","Suppose is a nonempty bounded subset of reals. Let and . Define . Prove (also prove that exists). I understand why this is the case, just not how to prove it. Left side is pretty much and right side is for . Basically has to use the same variable and use different ones. But I don't know how to go about proving this. The second part of the question is to find a specific example where strict inequality holds. Let and . (if we choose a, ).","D f:D \to \mathbb R g:D \to \mathbb R (f+g)(x)=f(x)+g(x) \sup(f+g)(D) \le \sup f(D) + \sup g(D) \sup (f+g) \sup  (f(x)+g(x)) \sup (f(x) + g(y)) x,\,y \in D f+g f(D)+g(D) D=\{a,b\} f: a \to 1,\, b\to 0, \,g: a \to 0,\, b\to 1 \sup f(D) = 1,\, \sup g(D) = 1,\, \sup f(D) + \sup g(D) = 2. \sup (f+g)(D) = 1 f+g = 1+0,\, b,\, f+g=0+1",['real-analysis']
49,Dini's Theorem - Proof.,Dini's Theorem - Proof.,,"I'd like to prove the following: If $f_j$ are continuous functions on a compact set $K$, and $f_{1}(x) \leq f_{2}(x) \leq \dots$ for all $x \in K$, and the $f_j$ converge pointwise to a continuous function $f$ on $K$ then in fact the $f_j$ converge uniformly to $f$ on $K$. Attempt: Let $g_{j}(x) = f(x) - f_{j}(x)$ for all $j$. Then, since $f_j \rightarrow f$ pointwise, we see $g_j \rightarrow 0$ pointwise. Now, let $\varepsilon > 0$ . And examine { $x \in K : g_{j}(x) < \varepsilon$ }. I've been told that the next step should be to  show that { $x \in K : g_{j}(x) < \varepsilon$ } is equal to the intersection of $K$ with some open set $U_j$. But I'm not certain why this is true? Advice? Insight?","I'd like to prove the following: If $f_j$ are continuous functions on a compact set $K$, and $f_{1}(x) \leq f_{2}(x) \leq \dots$ for all $x \in K$, and the $f_j$ converge pointwise to a continuous function $f$ on $K$ then in fact the $f_j$ converge uniformly to $f$ on $K$. Attempt: Let $g_{j}(x) = f(x) - f_{j}(x)$ for all $j$. Then, since $f_j \rightarrow f$ pointwise, we see $g_j \rightarrow 0$ pointwise. Now, let $\varepsilon > 0$ . And examine { $x \in K : g_{j}(x) < \varepsilon$ }. I've been told that the next step should be to  show that { $x \in K : g_{j}(x) < \varepsilon$ } is equal to the intersection of $K$ with some open set $U_j$. But I'm not certain why this is true? Advice? Insight?",,"['real-analysis', 'sequences-and-series']"
50,"Is there a continuous function from $[0,1]$ to $\mathbb R$ that satisfies",Is there a continuous function from  to  that satisfies,"[0,1] \mathbb R","Is there a continuous function $f:[0,1] \to \mathbb R$ such that $f(x) = 0$ uncountably often and, for every $x$ such that $f(x) = 0$, in any neighbourhood of $x$ there are $a$ and $b$ such that $f(a) > 0$ and $f(b) < 0$?","Is there a continuous function $f:[0,1] \to \mathbb R$ such that $f(x) = 0$ uncountably often and, for every $x$ such that $f(x) = 0$, in any neighbourhood of $x$ there are $a$ and $b$ such that $f(a) > 0$ and $f(b) < 0$?",,"['real-analysis', 'continuity']"
51,Definition of the product $\sigma$-algebra,Definition of the product -algebra,\sigma,"The following is the definition of the product $\sigma$-algebra given in Gerald Folland's Real Analysis: Modern Techniques and Their Applications (pg. 22) (note that $\mathcal{M}(X)$ denotes the smallest $\sigma$-algebra generated by the set $X$): ""Let $\{X_{\alpha}\}_{\alpha \in A}$ be an indexed collection of nonempty sets, $X=\prod_{\alpha \in A} X_{\alpha}$, and $\pi_{\alpha}: X \to X_{\alpha}$ the coordinate maps. If $\mathcal{M}_{\alpha}$ is a $\sigma$-algebra on $X_{\alpha}$ for each $\alpha$, the product $\sigma$-algebra on $X$ is the $\sigma$-algebra generated by  \begin{equation} \{\pi_{\alpha}^{-1}(E_{\alpha}): E_{\alpha} \in \mathcal{M}_{\alpha}, \alpha \in A \} \end{equation} We denote this $\sigma$-algebra by $\otimes_{\alpha \in A} \mathcal{M}_{\alpha}$."" With $A=\{1,2\}$, the wikipedia definition says that the product $\sigma$-algebra is given by: \begin{equation} \mathcal{M}_{1} \times \mathcal{M}_{2} = \mathcal{M}\left(\{E_{1} \times E_{2} : E_{1} \in \mathcal{M}_{1}, E_{2} \in \mathcal{M}_{2} \} \right) \end{equation} I understand the Wikipedia definition. However, I am new to measure theory and am having trouble reconciling Folland's definition with the Wikipedia definition. Assuming  $A=\{1,2\}$ for simplicity, can someone show me why the two definitions of the product $\sigma$-algebra are the same?","The following is the definition of the product $\sigma$-algebra given in Gerald Folland's Real Analysis: Modern Techniques and Their Applications (pg. 22) (note that $\mathcal{M}(X)$ denotes the smallest $\sigma$-algebra generated by the set $X$): ""Let $\{X_{\alpha}\}_{\alpha \in A}$ be an indexed collection of nonempty sets, $X=\prod_{\alpha \in A} X_{\alpha}$, and $\pi_{\alpha}: X \to X_{\alpha}$ the coordinate maps. If $\mathcal{M}_{\alpha}$ is a $\sigma$-algebra on $X_{\alpha}$ for each $\alpha$, the product $\sigma$-algebra on $X$ is the $\sigma$-algebra generated by  \begin{equation} \{\pi_{\alpha}^{-1}(E_{\alpha}): E_{\alpha} \in \mathcal{M}_{\alpha}, \alpha \in A \} \end{equation} We denote this $\sigma$-algebra by $\otimes_{\alpha \in A} \mathcal{M}_{\alpha}$."" With $A=\{1,2\}$, the wikipedia definition says that the product $\sigma$-algebra is given by: \begin{equation} \mathcal{M}_{1} \times \mathcal{M}_{2} = \mathcal{M}\left(\{E_{1} \times E_{2} : E_{1} \in \mathcal{M}_{1}, E_{2} \in \mathcal{M}_{2} \} \right) \end{equation} I understand the Wikipedia definition. However, I am new to measure theory and am having trouble reconciling Folland's definition with the Wikipedia definition. Assuming  $A=\{1,2\}$ for simplicity, can someone show me why the two definitions of the product $\sigma$-algebra are the same?",,"['real-analysis', 'measure-theory']"
52,Why is zero the only infinitesimal real number?,Why is zero the only infinitesimal real number?,,I am currently  reading Elementary Calculus: An Infinitesimal Approach by H. Jerome  Keisler and was wondering if someone could help me with an aspect treated in the book. On page 24 he says a number $\varepsilon$ is said to be infinitely small or infinitesimal if $$-a< \varepsilon < a$$ for every positive real number $a$. He then says the only real number that is infinitesimal is zero. I really don't get that. What I understand is that in order for a number to be considered infinitely small it has to be bigger then $-a$ and smaller then $a$. Well if I take $a$ to be $-2$ that means that $-1$ would be infinitesimal since it is bigger than $-2$ but smaller then $2$. So then how can zero be the only real number that satisfies that condition?,I am currently  reading Elementary Calculus: An Infinitesimal Approach by H. Jerome  Keisler and was wondering if someone could help me with an aspect treated in the book. On page 24 he says a number $\varepsilon$ is said to be infinitely small or infinitesimal if $$-a< \varepsilon < a$$ for every positive real number $a$. He then says the only real number that is infinitesimal is zero. I really don't get that. What I understand is that in order for a number to be considered infinitely small it has to be bigger then $-a$ and smaller then $a$. Well if I take $a$ to be $-2$ that means that $-1$ would be infinitesimal since it is bigger than $-2$ but smaller then $2$. So then how can zero be the only real number that satisfies that condition?,,"['real-analysis', 'infinitesimals']"
53,Prove that $\int_0^1\frac{\ln(1-x)\ln^2x}{x-1}dx=\frac{\pi^4}{180}$,Prove that,\int_0^1\frac{\ln(1-x)\ln^2x}{x-1}dx=\frac{\pi^4}{180},Prove that ( please ) $$\int_0^1\frac{\ln(1-x)\ln^2x}{x-1}dx=\frac{\pi^4}{180}$$ I've tried using Taylor series and I ended up with $$-\sum_{m=0}^\infty\sum_{n=1}^\infty\frac{2}{n(m+n+1)^3}$$ I am stuck there and I couldn't continue it using partial fraction to get any familiar sum of series. Could anyone here please help me to prove the integral preferably ( if possible ) with elementary ways (high school methods)? Any help would be greatly appreciated. Thank you.,Prove that ( please ) $$\int_0^1\frac{\ln(1-x)\ln^2x}{x-1}dx=\frac{\pi^4}{180}$$ I've tried using Taylor series and I ended up with $$-\sum_{m=0}^\infty\sum_{n=1}^\infty\frac{2}{n(m+n+1)^3}$$ I am stuck there and I couldn't continue it using partial fraction to get any familiar sum of series. Could anyone here please help me to prove the integral preferably ( if possible ) with elementary ways (high school methods)? Any help would be greatly appreciated. Thank you.,,"['real-analysis', 'integration', 'definite-integrals', 'improper-integrals', 'harmonic-numbers']"
54,Is a power series uniformly convergent in its interval of convergence?,Is a power series uniformly convergent in its interval of convergence?,,"Let $R>0$ be the radius of convergence of a power series $Σa_nx^n$. Is it not uniformly convergent in $(-R,R)$? My book goes out of its way to say that if $[a,b]⊂(-R,R)$, then the power series converges uniformly in $[a,b]$. Can't we just say that it is uniformly convergent in $(-R,R)$?","Let $R>0$ be the radius of convergence of a power series $Σa_nx^n$. Is it not uniformly convergent in $(-R,R)$? My book goes out of its way to say that if $[a,b]⊂(-R,R)$, then the power series converges uniformly in $[a,b]$. Can't we just say that it is uniformly convergent in $(-R,R)$?",,"['calculus', 'real-analysis', 'power-series', 'uniform-convergence']"
55,"What does ""bounded away from zero"" actually mean?","What does ""bounded away from zero"" actually mean?",,"For example, is $f(z) = 1/z$, on the set $0<z<1$ ""bounded away from zero""?","For example, is $f(z) = 1/z$, on the set $0<z<1$ ""bounded away from zero""?",,"['real-analysis', 'complex-analysis']"
56,Baby Rudin Theorem 1.20 (b) Proof,Baby Rudin Theorem 1.20 (b) Proof,,"I have a question about Rudin's proof of Theorem 1.20 (b) in his book Principles of Mathematical Analysis .  Theorem 1.20 is stated as follows: (a) If $x\in R, y\in R$, and $x>0$, then there is a positive integer $n$ such that $$nx>y.$$   (b) If $x\in R, y\in R$, and $x<y$, then there exists a $p\in Q$ such that $x<p<y$. I understand Rudin's proof of (a).  The beginning of Rudin's proof of (b) is given below: Since $x<y$, we have $y-x>0$, and (a) furnishes a positive integer $n$ such that $$n(y-x)>1.$$  Apply (a) again, to obtain positive integers $m_1$ and $m_2$ such that $m_1>nx$, $m_2>-nx$.  Then $$-m_2<nx<m_1.$$  Hence there is an integer $m$ (with $-m_2\leq m\leq m_1$) such that $$m-1\leq nx<m.$$ I don't understand the justification for this last sentence beginning ""Hence....""  How is $m$ found, and why are $m_1$ and $m_2$ needed to find $m$?","I have a question about Rudin's proof of Theorem 1.20 (b) in his book Principles of Mathematical Analysis .  Theorem 1.20 is stated as follows: (a) If $x\in R, y\in R$, and $x>0$, then there is a positive integer $n$ such that $$nx>y.$$   (b) If $x\in R, y\in R$, and $x<y$, then there exists a $p\in Q$ such that $x<p<y$. I understand Rudin's proof of (a).  The beginning of Rudin's proof of (b) is given below: Since $x<y$, we have $y-x>0$, and (a) furnishes a positive integer $n$ such that $$n(y-x)>1.$$  Apply (a) again, to obtain positive integers $m_1$ and $m_2$ such that $m_1>nx$, $m_2>-nx$.  Then $$-m_2<nx<m_1.$$  Hence there is an integer $m$ (with $-m_2\leq m\leq m_1$) such that $$m-1\leq nx<m.$$ I don't understand the justification for this last sentence beginning ""Hence....""  How is $m$ found, and why are $m_1$ and $m_2$ needed to find $m$?",,['real-analysis']
57,Prove that the sum of the infima is smaller than the infimum of the sum,Prove that the sum of the infima is smaller than the infimum of the sum,,"I'm trying to prove the following inequality: Let $f$ and $g$ be bounded real-valued functions with the same domain. Prove the following: $$\inf(f) + \inf(g) \leqslant \inf(f+g).$$ I thought I had proved it, but I made the erroneous assumption that $\inf(f+g)$ can always be expressed in the form $(f+g)(x_1)$ for some $x_1$ , which is not necessarily true.","I'm trying to prove the following inequality: Let and be bounded real-valued functions with the same domain. Prove the following: I thought I had proved it, but I made the erroneous assumption that can always be expressed in the form for some , which is not necessarily true.",f g \inf(f) + \inf(g) \leqslant \inf(f+g). \inf(f+g) (f+g)(x_1) x_1,['real-analysis']
58,Integral vanishes on all intervals implies the function is a.e. zero [duplicate],Integral vanishes on all intervals implies the function is a.e. zero [duplicate],,"This question already has answers here : If $\int_0^x f \ dm$ is zero everywhere then $f$ is zero almost everywhere (6 answers) Closed 4 years ago . I am having trouble with the following problem: $f:\mathbb{R}\to \mathbb{R}$ is a measurable function such that for all $a$:   $$\int_{[0,a]}f\,dm=0.$$   Prove that $f=0$ for $m$ almost every $x$ (here $m$ is the Lebesgue measure). I have no problem proving this for $f$ non-negative, or under the assumption that $f$ is integrable. But the question only assumes that $f$ is measurable and no more. My idea was the usual thing; we look at the set of points where $f$ is positive and negative and assume one of these has measure greater than zero. Then I wanted to estimate one of these by an open set, look at the integral on the open set and show that it had to be greater than zero, a contradiction. But a key part of this attack is the assumption of the absolute continuity of the integral, which only holds in the case where $f$ is integrable. Alternatively, if it were integrable one could simply estimate $f$ by a continuous function, where the result is quite obvious. Ultimately we are going to show that $f$ is integrable, but it is not clear to me how to show this before showing it is zero a.e. So there must be a simpler way. Does anyone have suggestions?","This question already has answers here : If $\int_0^x f \ dm$ is zero everywhere then $f$ is zero almost everywhere (6 answers) Closed 4 years ago . I am having trouble with the following problem: $f:\mathbb{R}\to \mathbb{R}$ is a measurable function such that for all $a$:   $$\int_{[0,a]}f\,dm=0.$$   Prove that $f=0$ for $m$ almost every $x$ (here $m$ is the Lebesgue measure). I have no problem proving this for $f$ non-negative, or under the assumption that $f$ is integrable. But the question only assumes that $f$ is measurable and no more. My idea was the usual thing; we look at the set of points where $f$ is positive and negative and assume one of these has measure greater than zero. Then I wanted to estimate one of these by an open set, look at the integral on the open set and show that it had to be greater than zero, a contradiction. But a key part of this attack is the assumption of the absolute continuity of the integral, which only holds in the case where $f$ is integrable. Alternatively, if it were integrable one could simply estimate $f$ by a continuous function, where the result is quite obvious. Ultimately we are going to show that $f$ is integrable, but it is not clear to me how to show this before showing it is zero a.e. So there must be a simpler way. Does anyone have suggestions?",,"['real-analysis', 'measure-theory', 'lebesgue-integral']"
59,How to calculate the limit $\lim_{n\to\infty}\left(\frac{1}{\sqrt{n^2+n+1}}+\frac{1}{\sqrt{n^2+n+2}}+\cdots+\frac{1}{\sqrt{n^2+n+n}}\right)^n$,How to calculate the limit,\lim_{n\to\infty}\left(\frac{1}{\sqrt{n^2+n+1}}+\frac{1}{\sqrt{n^2+n+2}}+\cdots+\frac{1}{\sqrt{n^2+n+n}}\right)^n,"$$I=\lim_{n\to\infty}\left(\frac{1}{\sqrt{n^2+n+1}}+\frac{1}{\sqrt{n^2+n+2}}+\cdots+\frac{1}{\sqrt{n^2+n+n}}\right)^n$$ I tried $$\frac{n}{\sqrt{n^2+n+n}}\leq\frac{1}{\sqrt{n^2+n+1}}+\frac{1}{\sqrt{n^2+n+2}}+\cdots+\frac{1}{\sqrt{n^2+n+n}}\leq\frac{n}{\sqrt{n^2+n+1}}$$ But $$\lim_{n\to\infty}\left(\frac{n}{\sqrt{n^2+n+n}}\right)^n=\lim_{n\to\infty}\left(1+\frac2n\right)^{\displaystyle-\frac{n}{2}}=e^{\displaystyle \lim_{n\to\infty}-\frac n2\ln(1+\frac2n)}=\frac1e$$ And in the same way,i got $$\lim_{n\to\infty}\left(\frac{n}{\sqrt{n^2+n+1}}\right)^n=\lim_{n\to\infty}\left(1+\frac1n+\frac{1}{n^2}\right)^{\displaystyle-\frac{n}{2}}=e^{\displaystyle \lim_{n\to\infty}-\frac n2\ln(1+\frac1n+\frac{1}{n^2})}=\frac{1}{\sqrt e}$$ So i only got $$\frac1e\leq I\leq\frac{1}{\sqrt e}$$ Could someone help me get the value of $I$ . Thanks!","I tried But And in the same way,i got So i only got Could someone help me get the value of . Thanks!",I=\lim_{n\to\infty}\left(\frac{1}{\sqrt{n^2+n+1}}+\frac{1}{\sqrt{n^2+n+2}}+\cdots+\frac{1}{\sqrt{n^2+n+n}}\right)^n \frac{n}{\sqrt{n^2+n+n}}\leq\frac{1}{\sqrt{n^2+n+1}}+\frac{1}{\sqrt{n^2+n+2}}+\cdots+\frac{1}{\sqrt{n^2+n+n}}\leq\frac{n}{\sqrt{n^2+n+1}} \lim_{n\to\infty}\left(\frac{n}{\sqrt{n^2+n+n}}\right)^n=\lim_{n\to\infty}\left(1+\frac2n\right)^{\displaystyle-\frac{n}{2}}=e^{\displaystyle \lim_{n\to\infty}-\frac n2\ln(1+\frac2n)}=\frac1e \lim_{n\to\infty}\left(\frac{n}{\sqrt{n^2+n+1}}\right)^n=\lim_{n\to\infty}\left(1+\frac1n+\frac{1}{n^2}\right)^{\displaystyle-\frac{n}{2}}=e^{\displaystyle \lim_{n\to\infty}-\frac n2\ln(1+\frac1n+\frac{1}{n^2})}=\frac{1}{\sqrt e} \frac1e\leq I\leq\frac{1}{\sqrt e} I,"['real-analysis', 'limits', 'analysis']"
60,Real analysis for a non-mathematician.,Real analysis for a non-mathematician.,,"I'm currently in an engineering program, so most of my mathematical education has been applied in nature (multivariable calculus, ODEs, PDEs, probability). The only real ""theory""-based courses I've taken have been abstract algebra$^1$ and a proof-based differential equations class. I'm looking to expand my mathematical horizons before graduate school, and I figured the two places that might be interesting for me (as well as useful!) are real analysis and differential geometry. I don't really have the space in my course schedule to take either of these, and the former is listed as a prerequisite for the latter at my university. I know that Rudin's Principles of Mathematical Analysis is the crème de la crème for real analysis texts, but I've started reading a pdf of it and it's not only extremely dense (which I'm not quite used to), but I also don't think I have the mathematical maturity to grasp it. I was wondering if anyone knows of any good texts where I can learn real analysis without presupposing a great deal of mathematical maturity. My foundations in calculus are quite strong, but my knowledge is that real analysis is only tangentially related. As a side question, I also want to ask if real analysis is really a prerequisite for differential geometry (I'm skeptical). $^1$We covered what you would normally find in an undergraduate algebra class and also touched on a bit of Galois theory, but to be completely honest I wasn't all that comfortable with the few Galois theory lectures we had, anyway.","I'm currently in an engineering program, so most of my mathematical education has been applied in nature (multivariable calculus, ODEs, PDEs, probability). The only real ""theory""-based courses I've taken have been abstract algebra$^1$ and a proof-based differential equations class. I'm looking to expand my mathematical horizons before graduate school, and I figured the two places that might be interesting for me (as well as useful!) are real analysis and differential geometry. I don't really have the space in my course schedule to take either of these, and the former is listed as a prerequisite for the latter at my university. I know that Rudin's Principles of Mathematical Analysis is the crème de la crème for real analysis texts, but I've started reading a pdf of it and it's not only extremely dense (which I'm not quite used to), but I also don't think I have the mathematical maturity to grasp it. I was wondering if anyone knows of any good texts where I can learn real analysis without presupposing a great deal of mathematical maturity. My foundations in calculus are quite strong, but my knowledge is that real analysis is only tangentially related. As a side question, I also want to ask if real analysis is really a prerequisite for differential geometry (I'm skeptical). $^1$We covered what you would normally find in an undergraduate algebra class and also touched on a bit of Galois theory, but to be completely honest I wasn't all that comfortable with the few Galois theory lectures we had, anyway.",,"['real-analysis', 'reference-request', 'book-recommendation']"
61,Supremum of Infimum and Infimum of Supremum,Supremum of Infimum and Infimum of Supremum,,"Let $X$ and $Y$ be nonempty sets, and let $f:X\times Y \rightarrow \mathbb{R}$ be a bounded function. a) Prove that $$\sup_{y\in Y}\left(\inf_{x\in X}f(x,y)\right)\leq \inf_{x\in X}\left(\sup_{y\in Y}f(x,y)\right)$$ b) Give an example (with proof) where the inequality is strict. I am stumped. First of all, I am having a hard time understanding the notation. The way I understand it, in the most basic terms, we have that the least upperbound of the greatest lower bound of the function is less than or equal to the greatest lower bond of the least upper bound of the function. But does this even make sense? sup and inf are unique, so wouldn't $$\sup_{y\in Y}\left(\inf_{x\in X}f(x,y)\right) = \inf_{x\in X}f(x,y)?? $$ and likewise for the inf of the sup...am I missing something? Plus I'm not sure what implications the subscripts $x\in X$ and $y\in Y$ carry. Please help me get an understanding of what the notation means and let me know if my reasoning is off.","Let $X$ and $Y$ be nonempty sets, and let $f:X\times Y \rightarrow \mathbb{R}$ be a bounded function. a) Prove that $$\sup_{y\in Y}\left(\inf_{x\in X}f(x,y)\right)\leq \inf_{x\in X}\left(\sup_{y\in Y}f(x,y)\right)$$ b) Give an example (with proof) where the inequality is strict. I am stumped. First of all, I am having a hard time understanding the notation. The way I understand it, in the most basic terms, we have that the least upperbound of the greatest lower bound of the function is less than or equal to the greatest lower bond of the least upper bound of the function. But does this even make sense? sup and inf are unique, so wouldn't $$\sup_{y\in Y}\left(\inf_{x\in X}f(x,y)\right) = \inf_{x\in X}f(x,y)?? $$ and likewise for the inf of the sup...am I missing something? Plus I'm not sure what implications the subscripts $x\in X$ and $y\in Y$ carry. Please help me get an understanding of what the notation means and let me know if my reasoning is off.",,['real-analysis']
62,Is every subset of a metric space a metric subspace?,Is every subset of a metric space a metric subspace?,,"Is every subset of a metric space a metric subspace? A simple proof does justify that all are subspaces, still, wanted to know if I missed something.","Is every subset of a metric space a metric subspace? A simple proof does justify that all are subspaces, still, wanted to know if I missed something.",,['real-analysis']
63,If $\sigma_n=\frac{s_1+s_2+\cdots+s_n}{n}$ then $\operatorname{{lim sup}}\sigma_n \leq \operatorname{lim sup} s_n$,If  then,\sigma_n=\frac{s_1+s_2+\cdots+s_n}{n} \operatorname{{lim sup}}\sigma_n \leq \operatorname{lim sup} s_n,This is a question from the book Methods of Real Analysis by R. R. Goldberg. If $(s_n)$ is a sequence of real numbers and if $$\sigma_n=\frac{s_1+s_2+\cdots+s_n}{n}$$ then prove that: $\operatorname{{lim sup}}\sigma_n \leq \operatorname{lim sup} s_n$. I don't have any idea how to start working on this problem. Please help. Thanks.,This is a question from the book Methods of Real Analysis by R. R. Goldberg. If $(s_n)$ is a sequence of real numbers and if $$\sigma_n=\frac{s_1+s_2+\cdots+s_n}{n}$$ then prove that: $\operatorname{{lim sup}}\sigma_n \leq \operatorname{lim sup} s_n$. I don't have any idea how to start working on this problem. Please help. Thanks.,,"['real-analysis', 'sequences-and-series', 'inequality', 'limsup-and-liminf']"
64,Compute $\lim\limits_{n\to\infty} \left(1+\frac{1}{n}\right)\left(1+\frac{2}{n}\right)^{{1}/{2}}\cdots\left(1+\frac{n}{n}\right)^{{1}/{n}}$,Compute,\lim\limits_{n\to\infty} \left(1+\frac{1}{n}\right)\left(1+\frac{2}{n}\right)^{{1}/{2}}\cdots\left(1+\frac{n}{n}\right)^{{1}/{n}},Compute the limit: $$\lim_{n\to\infty} \left(1+\frac{1}{n}\right)\left(1+\frac{2}{n}\right)^{\frac{1}{2}}\cdots\left(1+\frac{n}{n}\right)^{\frac{1}{n}}$$,Compute the limit: $$\lim_{n\to\infty} \left(1+\frac{1}{n}\right)\left(1+\frac{2}{n}\right)^{\frac{1}{2}}\cdots\left(1+\frac{n}{n}\right)^{\frac{1}{n}}$$,,"['calculus', 'real-analysis', 'limits']"
65,The Lebesgue measure of zero set of a polynomial function is zero,The Lebesgue measure of zero set of a polynomial function is zero,,Suppose $f :\mathbb R^n \to \mathbb R$ be a non zero polynomial(more generally smooth) function.Suppose $Z(f)=\{ x \in \mathbb R^n \mid f(x)=0 \}$ . Show that  Lebesgue measure of $Z(f)$ is zero. I am trying to use induction on $n$ .The result holds obviously if $n=1$ .Could someone give me some idea to prove the inductive step.The proof without induction on $n$ is also appreciated.,Suppose be a non zero polynomial(more generally smooth) function.Suppose . Show that  Lebesgue measure of is zero. I am trying to use induction on .The result holds obviously if .Could someone give me some idea to prove the inductive step.The proof without induction on is also appreciated.,f :\mathbb R^n \to \mathbb R Z(f)=\{ x \in \mathbb R^n \mid f(x)=0 \} Z(f) n n=1 n,"['real-analysis', 'polynomials', 'lebesgue-measure', 'analyticity', 'analytic-functions']"
66,"Definite integral $\int_0^1 \frac{\arctan x}{x\,\sqrt{1-x^2}}\,\text{d}x$",Definite integral,"\int_0^1 \frac{\arctan x}{x\,\sqrt{1-x^2}}\,\text{d}x","Wanting to calculate the integral $\int_0^1 \frac{\arctan x}{x\,\sqrt{1-x^2}}\,\text{d}x$ it will certainly already known to many of you that an interesting way to attack it is to refer to the method of integration and differentiation with respect to a parameter , getting $\frac{\pi}{2}\,\log\left(1+\sqrt{2}\right)$. Instead, what it does not at all clear is how software such as Wolfram Mathematica can calculate that result in an exact manner and not only approximate. Can someone enlighten me? Thanks!","Wanting to calculate the integral $\int_0^1 \frac{\arctan x}{x\,\sqrt{1-x^2}}\,\text{d}x$ it will certainly already known to many of you that an interesting way to attack it is to refer to the method of integration and differentiation with respect to a parameter , getting $\frac{\pi}{2}\,\log\left(1+\sqrt{2}\right)$. Instead, what it does not at all clear is how software such as Wolfram Mathematica can calculate that result in an exact manner and not only approximate. Can someone enlighten me? Thanks!",,"['real-analysis', 'definite-integrals']"
67,Limit of series involving ratio of two factorials,Limit of series involving ratio of two factorials,,"$$   \sum^{\infty}_{j=0} \frac{(j!)^2}{(2j)!} = \frac{2 \pi \sqrt{3}}{27}+\frac{4}{3} $$ The above series is in a homework sheet. We're not expected to find the limit, just prove its convergence. That's easy, but since we were given the limit, it got me thinking about how to find such a limit. If anyone could point me in the right direction, I'd be happy to discover it on my own, but after a few hours of searching, I don't feel much closer.","$$   \sum^{\infty}_{j=0} \frac{(j!)^2}{(2j)!} = \frac{2 \pi \sqrt{3}}{27}+\frac{4}{3} $$ The above series is in a homework sheet. We're not expected to find the limit, just prove its convergence. That's easy, but since we were given the limit, it got me thinking about how to find such a limit. If anyone could point me in the right direction, I'd be happy to discover it on my own, but after a few hours of searching, I don't feel much closer.",,"['real-analysis', 'sequences-and-series', 'limits', 'factorial']"
68,Find the infinite sum $\sum_{n=1}^{\infty}\frac{1}{2^n-1}$,Find the infinite sum,\sum_{n=1}^{\infty}\frac{1}{2^n-1},How to evaluate this infinite sum?  $$\sum_{n=1}^{\infty}\frac{1}{2^n-1}$$,How to evaluate this infinite sum?  $$\sum_{n=1}^{\infty}\frac{1}{2^n-1}$$,,"['calculus', 'real-analysis', 'sequences-and-series']"
69,How to find $\operatorname{P.V.}\int_0^1 \frac{1}{x (1-x)}\arctan \left(\frac{8 x^2-4 x^3+14 x-8}{2 x^4-3 x^3-11 x^2+16 x+16}\right) \textrm{d}x$?,How to find ?,\operatorname{P.V.}\int_0^1 \frac{1}{x (1-x)}\arctan \left(\frac{8 x^2-4 x^3+14 x-8}{2 x^4-3 x^3-11 x^2+16 x+16}\right) \textrm{d}x,The following problem is proposed by Cornel Valean: $$\operatorname{P.V.} \int_0^1 \frac{1}{x (1-x)}\arctan \left(\frac{8 x^2-4 x^3+14 x-8}{2 x^4-3 x^3-11 x^2+16 x+16}\right) \textrm{d}x=\log \left(\frac{5}{4}\right) \arctan\left(\frac{1}{2}\right).$$ The question is how can we prove this equality without using the main result in this paper and without the use of complex numbers?,The following problem is proposed by Cornel Valean: The question is how can we prove this equality without using the main result in this paper and without the use of complex numbers?,\operatorname{P.V.} \int_0^1 \frac{1}{x (1-x)}\arctan \left(\frac{8 x^2-4 x^3+14 x-8}{2 x^4-3 x^3-11 x^2+16 x+16}\right) \textrm{d}x=\log \left(\frac{5}{4}\right) \arctan\left(\frac{1}{2}\right).,"['real-analysis', 'calculus', 'integration', 'alternative-proof', 'cauchy-principal-value']"
70,Find functions family satisfying $ \lim_{n\to\infty} n \int_0^1 x^n f(x) = f(1)$,Find functions family satisfying, \lim_{n\to\infty} n \int_0^1 x^n f(x) = f(1),I wonder what kind of functions satisfy $$ \lim_{n\to\infty} n \int_0^1 x^n f(x) = f(1)$$ I suppose all functions must be continuous.,I wonder what kind of functions satisfy $$ \lim_{n\to\infty} n \int_0^1 x^n f(x) = f(1)$$ I suppose all functions must be continuous.,,"['calculus', 'real-analysis', 'integration', 'limits']"
71,An interesting sum to infinity,An interesting sum to infinity,,Is there any simple way of computing the following sum? $$\sum_{k=1}^\infty \frac1{k\space k!}$$,Is there any simple way of computing the following sum? $$\sum_{k=1}^\infty \frac1{k\space k!}$$,,"['calculus', 'real-analysis', 'sequences-and-series', 'limits', 'special-functions']"
72,An interesting limit: $\lim_\limits{n\to\infty}\frac{\sin 1\sin\sqrt{1}+\sin 2\sin\sqrt{2}+\sin 3\sin\sqrt{3}+\cdots+\sin n\sin\sqrt{n}}{n}$,An interesting limit:,\lim_\limits{n\to\infty}\frac{\sin 1\sin\sqrt{1}+\sin 2\sin\sqrt{2}+\sin 3\sin\sqrt{3}+\cdots+\sin n\sin\sqrt{n}}{n},"I would like to prove that $$\lim_\limits{n\to\infty}\frac{\sin 1\sin\sqrt{1}+\sin 2\sin\sqrt{2}+\sin 3\sin\sqrt{3}+\cdots+\sin n\sin\sqrt{n}}{n}=0$$ but I am stuck. I tried to solve it by using Euler-Maclaurin formula , but I could not to. Euler-Maclaurin formula applied to the function $f(x)=\sin x \sin\sqrt{x}\;\;$ is the following: $$\sum_{h=1}^n\sin h\sin\sqrt{h}=\int_\limits{0}^n\left[\sin x\sin\sqrt{x}+\left(x-\lfloor x\rfloor\right)\left(\cos x\sin\sqrt{x}+\frac{\sin x\cos\sqrt{x}}{2\sqrt{x}}\right)\right] \, dx$$ but I could not manage to prove that $$\frac{1}{n}\int_\limits{0}^n\left(x-\lfloor x\rfloor\right)\left(\cos x \sin\sqrt{x} \right) \, dx\rightarrow 0 \text{ as } n\to\infty.$$ Moreover I tried to write the limit as a limit of a Riemann sum, but I did not manage to. Furthermore I tried to prove the following inequality: $$\left|\sin 1\sin\sqrt{1}+\sin 2\sin\sqrt{2}+\cdots+\sin n \sin\sqrt{n} \right|\le\sqrt[4]{n^3}\\\text{for all }\;n\in\mathbb{N},$$ but it was not successful. I managed to prove that $$\lim_{n\to\infty}\frac{\sin 1+\sin 2 +\sin 3+\ldots+\sin n}{n}=0$$ and $$\lim_{n\to\infty}\frac{\sin\sqrt{1}+\sin\sqrt{2}+\sin\sqrt{3}+\cdots+\sin\sqrt{n}}{n}=0.$$ Is it possible to use these last two limits in order to prove that $$\lim_{n\to\infty}\frac{\sin 1\sin\sqrt{1}+\sin 2\sin\sqrt{2}+\sin 3 \sin\sqrt{3}+\cdots+\sin n\sin\sqrt{n}}{n}=0\text{ ?}$$ I tried to use Cauchy-Schwartz inequality, but I got $$\lim_{n\to\infty}\frac{\sin^21+\sin^22+\cdots+\sin^2n}{n}$$ and $$\lim_{n\to\infty}\frac{\sin^2\sqrt{1}+\sin^2\sqrt{2}+\cdots+\sin^2\sqrt{n}}{n}$$ and these last two limits are not zero in fact there are both $\frac{1}{2}$ .","I would like to prove that but I am stuck. I tried to solve it by using Euler-Maclaurin formula , but I could not to. Euler-Maclaurin formula applied to the function is the following: but I could not manage to prove that Moreover I tried to write the limit as a limit of a Riemann sum, but I did not manage to. Furthermore I tried to prove the following inequality: but it was not successful. I managed to prove that and Is it possible to use these last two limits in order to prove that I tried to use Cauchy-Schwartz inequality, but I got and and these last two limits are not zero in fact there are both .","\lim_\limits{n\to\infty}\frac{\sin 1\sin\sqrt{1}+\sin 2\sin\sqrt{2}+\sin 3\sin\sqrt{3}+\cdots+\sin n\sin\sqrt{n}}{n}=0 f(x)=\sin x \sin\sqrt{x}\;\; \sum_{h=1}^n\sin h\sin\sqrt{h}=\int_\limits{0}^n\left[\sin x\sin\sqrt{x}+\left(x-\lfloor x\rfloor\right)\left(\cos x\sin\sqrt{x}+\frac{\sin x\cos\sqrt{x}}{2\sqrt{x}}\right)\right] \, dx \frac{1}{n}\int_\limits{0}^n\left(x-\lfloor x\rfloor\right)\left(\cos x \sin\sqrt{x} \right) \, dx\rightarrow 0 \text{ as } n\to\infty. \left|\sin 1\sin\sqrt{1}+\sin 2\sin\sqrt{2}+\cdots+\sin n \sin\sqrt{n} \right|\le\sqrt[4]{n^3}\\\text{for all }\;n\in\mathbb{N}, \lim_{n\to\infty}\frac{\sin 1+\sin 2 +\sin 3+\ldots+\sin n}{n}=0 \lim_{n\to\infty}\frac{\sin\sqrt{1}+\sin\sqrt{2}+\sin\sqrt{3}+\cdots+\sin\sqrt{n}}{n}=0. \lim_{n\to\infty}\frac{\sin 1\sin\sqrt{1}+\sin 2\sin\sqrt{2}+\sin 3 \sin\sqrt{3}+\cdots+\sin n\sin\sqrt{n}}{n}=0\text{ ?} \lim_{n\to\infty}\frac{\sin^21+\sin^22+\cdots+\sin^2n}{n} \lim_{n\to\infty}\frac{\sin^2\sqrt{1}+\sin^2\sqrt{2}+\cdots+\sin^2\sqrt{n}}{n} \frac{1}{2}","['real-analysis', 'sequences-and-series', 'limits']"
73,Proving $(λ^d + (1-λ^d)e^{(d-1)s})^{\frac{1}{1-d}}\leq\sum\limits_{n=0}^\infty\frac1{n!}λ^{\frac{(d^n-1)d}{d-1}+n}s^ne^{-λs}$,Proving,(λ^d + (1-λ^d)e^{(d-1)s})^{\frac{1}{1-d}}\leq\sum\limits_{n=0}^\infty\frac1{n!}λ^{\frac{(d^n-1)d}{d-1}+n}s^ne^{-λs},"Question Let $\lambda \in (0,1), s \in (0,\infty), d \in \{2,3,\dots\}$ and show that in this case the following inequality holds: $$(\lambda^d + (1-\lambda^d) e^{(d-1)s})^{\frac{1}{1-d}} \leq \sum_{n=0}^\infty \frac{\lambda^{\frac{(d^n-1)d}{d-1}+n}s^n}{n!} e^{-\lambda s},$$ Thoughts My first intuition was to try and write the left hand side as some taylor series but this seems like a bad idea as the only part to write as a Taylor series is $e^{(d-1)s}$ and if we ""ignore"" the $\lambda^d$ this part is what gives $e^{-s}$ which is bounded by $e^{-\lambda s}$. Maybe there is some formula for a convex expression of the form $\lambda^d a + (1-\lambda^d) b$ which can be used here? Applying convexity I can write the right hand side as $e^{-(1-\lambda^d)s}$ which, unfortunately, is not smaller than the right hand side (for large values of $s$ the inequality fails, which is logical as convecity inequality is increasingly inaccurate as $s$ increases). I might be able to show it using the Taylor expansion of $f(x) := \frac{1}{(1+x)^{1/n}}$ as I can write the right hand side as: $$ (1 + (1-\lambda^d) (e^{(d-1)s}-1))^{1/(1-d)}, $$ for this I can take the taylor series at $0$ for $|x|=|(1-\lambda^d) (e^{(d-1)s}-1)| < 1$ and at $\infty$ for $|x| > 1$. It seems to me that I need something like the binomial approximation but for large values of $x$ rather than small values of $x$. Promising technique I can write the left hand side as  $$e^{-s} (1 + \lambda^d (e^{(1-d)s} - 1))^{1/(1-d)}$$ as $|\lambda^d (1-e^{(1-d)s})| < 1$ we can use the Taylor expansion of $1/(1+x)$ at $0$. This allows us to reduce the inequality to showing: $$ e^{-s} \sum_{n=0}^{\infty} \frac{1}{n!} \frac{\Gamma(1 - 1/(d-1))}{\Gamma(1 - 1/(d-1) - n)} \cdot \lambda^{nd} (e^{(1-d)s}-1)^n \leq  \sum_{n=0}^{\infty} \frac{\lambda^{((d^n-1)d)/(d-1)+n}s^n}{n!}e^{-\lambda s}. $$ The separate terms in the sum on the right hand side drops to zero much faster than the sum on the left hand side, but the first couple of terms of the right hand side are a lot larger than those on the left hand side. It seems to me that this corresponds to the fact that the Taylor series on the left hand side converges very slowly to the function, maybe we can take a better Taylor series which converges faster.","Question Let $\lambda \in (0,1), s \in (0,\infty), d \in \{2,3,\dots\}$ and show that in this case the following inequality holds: $$(\lambda^d + (1-\lambda^d) e^{(d-1)s})^{\frac{1}{1-d}} \leq \sum_{n=0}^\infty \frac{\lambda^{\frac{(d^n-1)d}{d-1}+n}s^n}{n!} e^{-\lambda s},$$ Thoughts My first intuition was to try and write the left hand side as some taylor series but this seems like a bad idea as the only part to write as a Taylor series is $e^{(d-1)s}$ and if we ""ignore"" the $\lambda^d$ this part is what gives $e^{-s}$ which is bounded by $e^{-\lambda s}$. Maybe there is some formula for a convex expression of the form $\lambda^d a + (1-\lambda^d) b$ which can be used here? Applying convexity I can write the right hand side as $e^{-(1-\lambda^d)s}$ which, unfortunately, is not smaller than the right hand side (for large values of $s$ the inequality fails, which is logical as convecity inequality is increasingly inaccurate as $s$ increases). I might be able to show it using the Taylor expansion of $f(x) := \frac{1}{(1+x)^{1/n}}$ as I can write the right hand side as: $$ (1 + (1-\lambda^d) (e^{(d-1)s}-1))^{1/(1-d)}, $$ for this I can take the taylor series at $0$ for $|x|=|(1-\lambda^d) (e^{(d-1)s}-1)| < 1$ and at $\infty$ for $|x| > 1$. It seems to me that I need something like the binomial approximation but for large values of $x$ rather than small values of $x$. Promising technique I can write the left hand side as  $$e^{-s} (1 + \lambda^d (e^{(1-d)s} - 1))^{1/(1-d)}$$ as $|\lambda^d (1-e^{(1-d)s})| < 1$ we can use the Taylor expansion of $1/(1+x)$ at $0$. This allows us to reduce the inequality to showing: $$ e^{-s} \sum_{n=0}^{\infty} \frac{1}{n!} \frac{\Gamma(1 - 1/(d-1))}{\Gamma(1 - 1/(d-1) - n)} \cdot \lambda^{nd} (e^{(1-d)s}-1)^n \leq  \sum_{n=0}^{\infty} \frac{\lambda^{((d^n-1)d)/(d-1)+n}s^n}{n!}e^{-\lambda s}. $$ The separate terms in the sum on the right hand side drops to zero much faster than the sum on the left hand side, but the first couple of terms of the right hand side are a lot larger than those on the left hand side. It seems to me that this corresponds to the fact that the Taylor series on the left hand side converges very slowly to the function, maybe we can take a better Taylor series which converges faster.",,"['calculus', 'real-analysis', 'inequality', 'summation', 'upper-lower-bounds']"
74,"Closed form of $\int_0^1(\ln(1-x)\ln(1+x)\ln(x))^2\,dx$",Closed form of,"\int_0^1(\ln(1-x)\ln(1+x)\ln(x))^2\,dx","I remember that some time ago I was asking this question Evaluate $\int_0^1\ln(1-x)\ln x\ln(1+x) \mathrm{dx}$ , and now, while I was making a review, I asked myself if we can get the closed form of $$ \int_{0}^{1}\left[\vphantom{\Large A}\ln\left(1 - x\right)\ln\left(1 + x\right)\ln\left(x\right)\right]^{\,2}\,\mathrm{d}x $$ by using the similar tools as in that proof. The problem is that we have to cope with some series that seem far more complicated. In case you have some fruitful ideas here ...","I remember that some time ago I was asking this question Evaluate $\int_0^1\ln(1-x)\ln x\ln(1+x) \mathrm{dx}$ , and now, while I was making a review, I asked myself if we can get the closed form of $$ \int_{0}^{1}\left[\vphantom{\Large A}\ln\left(1 - x\right)\ln\left(1 + x\right)\ln\left(x\right)\right]^{\,2}\,\mathrm{d}x $$ by using the similar tools as in that proof. The problem is that we have to cope with some series that seem far more complicated. In case you have some fruitful ideas here ...",,"['calculus', 'real-analysis', 'integration', 'sequences-and-series', 'definite-integrals']"
75,How do I evaluate the Lebesgue measure of a ball?,How do I evaluate the Lebesgue measure of a ball?,,"I remember I saw a post related to this question somewhere here, but I cannot find this now. How do I evaluate the Lebesgue measure of a ball? What I only remember is that the proof used the gamma function.","I remember I saw a post related to this question somewhere here, but I cannot find this now. How do I evaluate the Lebesgue measure of a ball? What I only remember is that the proof used the gamma function.",,"['real-analysis', 'measure-theory']"
76,Explaining $\cos^\infty$ [duplicate],Explaining  [duplicate],\cos^\infty,This question already has answers here : What is the solution of $\cos(x)=x$? (13 answers) Closed 4 years ago . I noticed something odd while messing around on my calculator. $$\lim_{n\to \infty} \cos^n(c)=0.7390851332$$ Where $c$ is a real constant. With $$\cos^n(c) =\underbrace{\cos \circ\cos \circ\cos \circ \cdots \circ \cos \circ \cos}_{n \text{ times}}(c)$$ My calculator is in radians and I got this number by simply taking the cosine of many numbers over and over again. No matter what number I use I always end up with that number. Why does this happen and where does this number come from?,This question already has answers here : What is the solution of $\cos(x)=x$? (13 answers) Closed 4 years ago . I noticed something odd while messing around on my calculator. $$\lim_{n\to \infty} \cos^n(c)=0.7390851332$$ Where $c$ is a real constant. With $$\cos^n(c) =\underbrace{\cos \circ\cos \circ\cos \circ \cdots \circ \cos \circ \cos}_{n \text{ times}}(c)$$ My calculator is in radians and I got this number by simply taking the cosine of many numbers over and over again. No matter what number I use I always end up with that number. Why does this happen and where does this number come from?,,"['calculus', 'real-analysis', 'limits', 'trigonometry']"
77,Clarification for proof of $\mathbb{Q}$ being dense in $\mathbb{R}$ (Rudin's PMA),Clarification for proof of  being dense in  (Rudin's PMA),\mathbb{Q} \mathbb{R},"Theorem 1.20(b) on page 9 of Rudin's ""Principles of Mathematical Analysis,"" 3rd edition. For those without the text handy: 1.20 Theorem (a) If $x \in \mathbb{R}$ and $x > 0$ , then there is a positive integer $n$ such that $nx > y$ . (b) If $x \in \mathbb{R}$ , $y \in \mathbb{R}$ , and $x < y$ , then there exists a $p \in \mathbb{Q}$ such that $x < p < y$ . Also a picture to the page in question: https://i.sstatic.net/U8hgO.png We are given that $x < y$ , thus $y - x > 0$ is obvious to me. I quickly lose Rudin after this step. I understand that the archimedean property $(a)$ is being used for the next line, where it says $n(y - x) > 1$ , however I have no clue where the number "" $1$ "" came from. Furthermore, he says to apply $(a)$ again, but I have no idea what it means to ""apply"" a theorem arbitrarily. He doesn't say what to apply it to, and if he meant to apply it on $n(y - x) > 1$ , then I am even more confused with the following step. He ""applies"" $(a)$ to obtain positive integers $m_1$ and $m_2$ such that $m_1 > nx$ and $m_2 > -nx$ . As far as I understand, the archimedean property says that for $x > 0$ , there is a positive integer $n$ such that $nx > y$ . I don't understand how $m_1 > nx$ and $m_2 > -nx$ follow this property. In $m_1 > nx$ , the equality sign is reversed from the archimedean property definition, and for $m_2 > -nx$ , there is a negative sign. And then the next line, he says ""hence"" there is an integer $m$ (with $-m_2 \leq m \leq m_1$ ) such that $m - 1 \leq nx \leq m$ . I don't understand how you can deduce from the previous lines ( $m_1 > nx$ and $m_2 > -nx$ ) to arrive at this one. Where are all these $m$ 's coming from? The only connection I see is that number $1$ from $n(y - x) > 1$ from the first step. I have no clue where these $m$ 's appeared out of nowhere.","Theorem 1.20(b) on page 9 of Rudin's ""Principles of Mathematical Analysis,"" 3rd edition. For those without the text handy: 1.20 Theorem (a) If and , then there is a positive integer such that . (b) If , , and , then there exists a such that . Also a picture to the page in question: https://i.sstatic.net/U8hgO.png We are given that , thus is obvious to me. I quickly lose Rudin after this step. I understand that the archimedean property is being used for the next line, where it says , however I have no clue where the number "" "" came from. Furthermore, he says to apply again, but I have no idea what it means to ""apply"" a theorem arbitrarily. He doesn't say what to apply it to, and if he meant to apply it on , then I am even more confused with the following step. He ""applies"" to obtain positive integers and such that and . As far as I understand, the archimedean property says that for , there is a positive integer such that . I don't understand how and follow this property. In , the equality sign is reversed from the archimedean property definition, and for , there is a negative sign. And then the next line, he says ""hence"" there is an integer (with ) such that . I don't understand how you can deduce from the previous lines ( and ) to arrive at this one. Where are all these 's coming from? The only connection I see is that number from from the first step. I have no clue where these 's appeared out of nowhere.",x \in \mathbb{R} x > 0 n nx > y x \in \mathbb{R} y \in \mathbb{R} x < y p \in \mathbb{Q} x < p < y x < y y - x > 0 (a) n(y - x) > 1 1 (a) n(y - x) > 1 (a) m_1 m_2 m_1 > nx m_2 > -nx x > 0 n nx > y m_1 > nx m_2 > -nx m_1 > nx m_2 > -nx m -m_2 \leq m \leq m_1 m - 1 \leq nx \leq m m_1 > nx m_2 > -nx m 1 n(y - x) > 1 m,['real-analysis']
78,The sum: $\sum_{n=1}^\infty (-1)^{n+1}\frac{1}{n}=\ln(2)$ using Riemann Integral and other methods,The sum:  using Riemann Integral and other methods,\sum_{n=1}^\infty (-1)^{n+1}\frac{1}{n}=\ln(2),"I need to prove the following: $$1-\frac{1}{2}+\frac{1}{3}-\frac{1}{4}+\cdots+(-1)^{n+1}\frac{1}{n}+\cdots=\sum_{n=1}^\infty (-1)^{n+1}\frac{1}{n}=\ln(2)$$ Method 1: The series $\sum_{n=1}^\infty (-1)^{n+1}\frac{1}{n}$ is an alternating series, thus it is convergent, say to $l$ . Therefore, both $s_{2n}$ and $s_n$ are convergent to the same limit $l$ . $$ \begin{align} s_{2n}=1-\frac{1}{2}+\frac{1}{3}-\frac{1}{4}+\cdots-\frac{1}{2n} & =\left(1+\frac{1}{2}+\frac{1}{3}+\frac{1}{4}+\cdots+\frac{1}{2n}\right)-2\left(\frac{1}{2}+\frac{1}{4}+\cdots+\frac{1}{2n}\right) \\[10pt] & =\frac{1}{n+1}+\frac{1}{n+2}+\cdots+\frac{1}{2n} \end{align} $$ It is an easy exercise to prove that $$\lim_{n \to \infty }s_{2n}=\lim_{n \to \infty }s_n =\lim_{n \to \infty }\left [ \frac{1}{n+1}+\frac{1}{n+2}+\cdots+\frac{1}{2n} \right ]=\ln(2)$$ which implies that the given alternating series converges to $l=\ln 2$ . However, I am interested to see proof of this problem using the definition of the Riemann Integral as a sum of infinitely many rectangles of widths tending to zero. I tried to come up with proof for this, but I couldn't. Can anyone share, please? Also, I am interested to see other methods of solving this problem (other than my method and the Riemann method.) If anyone of you is aware of any other methods, please share.","I need to prove the following: Method 1: The series is an alternating series, thus it is convergent, say to . Therefore, both and are convergent to the same limit . It is an easy exercise to prove that which implies that the given alternating series converges to . However, I am interested to see proof of this problem using the definition of the Riemann Integral as a sum of infinitely many rectangles of widths tending to zero. I tried to come up with proof for this, but I couldn't. Can anyone share, please? Also, I am interested to see other methods of solving this problem (other than my method and the Riemann method.) If anyone of you is aware of any other methods, please share.","1-\frac{1}{2}+\frac{1}{3}-\frac{1}{4}+\cdots+(-1)^{n+1}\frac{1}{n}+\cdots=\sum_{n=1}^\infty (-1)^{n+1}\frac{1}{n}=\ln(2) \sum_{n=1}^\infty (-1)^{n+1}\frac{1}{n} l s_{2n} s_n l 
\begin{align}
s_{2n}=1-\frac{1}{2}+\frac{1}{3}-\frac{1}{4}+\cdots-\frac{1}{2n} & =\left(1+\frac{1}{2}+\frac{1}{3}+\frac{1}{4}+\cdots+\frac{1}{2n}\right)-2\left(\frac{1}{2}+\frac{1}{4}+\cdots+\frac{1}{2n}\right) \\[10pt]
& =\frac{1}{n+1}+\frac{1}{n+2}+\cdots+\frac{1}{2n}
\end{align}
 \lim_{n \to \infty }s_{2n}=\lim_{n \to \infty }s_n =\lim_{n \to \infty }\left [ \frac{1}{n+1}+\frac{1}{n+2}+\cdots+\frac{1}{2n} \right ]=\ln(2) l=\ln 2","['calculus', 'real-analysis', 'sequences-and-series', 'analysis', 'integration']"
79,Prove Borel Measurable Set A with the following property has measure 0.,Prove Borel Measurable Set A with the following property has measure 0.,,"This question is exercise 4.10 of Richard F. Bass's Real Analysis for Graduate Students, 2nd edition. Let $\epsilon \in (0,1)$, let $m$ be Lebesgue measure, and suppose $A$ is Borel Measurable subset of $\mathbb R$. Prove that if $$ m(A\cap I)\leq (1-\epsilon)m(I)$$ for every interval $I$, then $m(A)=0$. Collection of my thoughts 1. Try to prove by contradiction, suppose $m(A)=a>0$ Borel Measurable Set satisfies the condition given. A property of any Borel Measurable Set on $\mathbb R$ is $A$ contains a closed set $F$ and is contained in open set $O$, and the measure of $F$ and $O$ can be arbitrarily close to $a$. 2. Obviously $A$ contains no interval, but it does not lead to contradiction, because we can find an example of $m(A)>0$ that contains no interval(fat Cantor Set(remove middle 1/4 each time) has measure $1/2$ and contains no interval). 2'. For fat Cantor Set $C$ and given $\epsilon>0$ how could we construct an interval I satisfies $m(C \cap I)>(1-\epsilon)m(I) ?$ Maybe try to write Borel measurable set as countable union/intersection of intervals. I'm stucked here and will appreciate any help, some examples/special case will be great too.","This question is exercise 4.10 of Richard F. Bass's Real Analysis for Graduate Students, 2nd edition. Let $\epsilon \in (0,1)$, let $m$ be Lebesgue measure, and suppose $A$ is Borel Measurable subset of $\mathbb R$. Prove that if $$ m(A\cap I)\leq (1-\epsilon)m(I)$$ for every interval $I$, then $m(A)=0$. Collection of my thoughts 1. Try to prove by contradiction, suppose $m(A)=a>0$ Borel Measurable Set satisfies the condition given. A property of any Borel Measurable Set on $\mathbb R$ is $A$ contains a closed set $F$ and is contained in open set $O$, and the measure of $F$ and $O$ can be arbitrarily close to $a$. 2. Obviously $A$ contains no interval, but it does not lead to contradiction, because we can find an example of $m(A)>0$ that contains no interval(fat Cantor Set(remove middle 1/4 each time) has measure $1/2$ and contains no interval). 2'. For fat Cantor Set $C$ and given $\epsilon>0$ how could we construct an interval I satisfies $m(C \cap I)>(1-\epsilon)m(I) ?$ Maybe try to write Borel measurable set as countable union/intersection of intervals. I'm stucked here and will appreciate any help, some examples/special case will be great too.",,['real-analysis']
80,Cardinality of set of discontinuities of cadlag functions,Cardinality of set of discontinuities of cadlag functions,,"I know that non-decreasing cadlag functions (functions that are right continuous with left limits) on $[0,\infty)$ have at most a countable number of discontinuities. Does the same result hold for more general (not necessarily non-decreasing) cadlag functions?","I know that non-decreasing cadlag functions (functions that are right continuous with left limits) on $[0,\infty)$ have at most a countable number of discontinuities. Does the same result hold for more general (not necessarily non-decreasing) cadlag functions?",,"['real-analysis', 'general-topology', 'set-theory']"
81,about addition and multiplications of real number,about addition and multiplications of real number,,"We all know that for every two real number $x,y$, the operation of addition $x+y$ satisfies the following conditions: \begin{gather} x+y=y+x;\\ x+0=x;\\ (x+y)+z=x+(y+z);\\ x+(-x)=0. \end{gather} Now, take a function $f\colon\Bbb R\times\Bbb R\to\Bbb R$ satisfying the following four conditions: $\forall x,y\in \Bbb R, f(x,y)=f(y,x);$ $\forall x\in \Bbb R, f(x,0)=x;$ $\forall x,y,z\in \Bbb R, f(f(x,y),z)=f(x,f(y,z));$ $\forall x\in \Bbb R, f(x,-x)=0.$ Can we conclude that $f(x,y)=x+y$ for all $x,y\in \Bbb R$? edit:  Thanks to Jared, the answer is ""NO"". think  a function $g\colon\Bbb R\times\Bbb R\to\Bbb R$ satisfying the following four conditions: a). $\forall x,y\in \Bbb R, g(x,y)=g(y,x);$ b). $\forall x\in \Bbb R, g(x,1)=x;$ c). $\forall x,y,z\in \Bbb R, g(g(x,y),z)=g(x,g(y,z));$ d). $\forall x\in \Bbb R,x\ne0,  g(x,\frac1x)=1.$ Can we conclude that $g(x,y)=xy$ for all $x,y\in \Bbb R$? again, two functions $f,g\colon\Bbb R\times\Bbb R\to\Bbb R$ satisfying 1,2,3,4, a),b),c),d),and e). $\forall x,y,z\in \Bbb R, g(x,f(y,z))=f(g(x,y),g(x,z))$ Can we conclude that $f(x,y)=x+y,g(x,y)=xy$ for all $x,y\in \Bbb R$? It seems that the answer is NO, $f(x,y)=\sqrt[3]{x^3+y^3},g(x,y)=xy$ works","We all know that for every two real number $x,y$, the operation of addition $x+y$ satisfies the following conditions: \begin{gather} x+y=y+x;\\ x+0=x;\\ (x+y)+z=x+(y+z);\\ x+(-x)=0. \end{gather} Now, take a function $f\colon\Bbb R\times\Bbb R\to\Bbb R$ satisfying the following four conditions: $\forall x,y\in \Bbb R, f(x,y)=f(y,x);$ $\forall x\in \Bbb R, f(x,0)=x;$ $\forall x,y,z\in \Bbb R, f(f(x,y),z)=f(x,f(y,z));$ $\forall x\in \Bbb R, f(x,-x)=0.$ Can we conclude that $f(x,y)=x+y$ for all $x,y\in \Bbb R$? edit:  Thanks to Jared, the answer is ""NO"". think  a function $g\colon\Bbb R\times\Bbb R\to\Bbb R$ satisfying the following four conditions: a). $\forall x,y\in \Bbb R, g(x,y)=g(y,x);$ b). $\forall x\in \Bbb R, g(x,1)=x;$ c). $\forall x,y,z\in \Bbb R, g(g(x,y),z)=g(x,g(y,z));$ d). $\forall x\in \Bbb R,x\ne0,  g(x,\frac1x)=1.$ Can we conclude that $g(x,y)=xy$ for all $x,y\in \Bbb R$? again, two functions $f,g\colon\Bbb R\times\Bbb R\to\Bbb R$ satisfying 1,2,3,4, a),b),c),d),and e). $\forall x,y,z\in \Bbb R, g(x,f(y,z))=f(g(x,y),g(x,z))$ Can we conclude that $f(x,y)=x+y,g(x,y)=xy$ for all $x,y\in \Bbb R$? It seems that the answer is NO, $f(x,y)=\sqrt[3]{x^3+y^3},g(x,y)=xy$ works",,"['real-analysis', 'analysis']"
82,Criteria for swapping integration and summation order,Criteria for swapping integration and summation order,,"I have a function (a potential from an electrostatic potential via a Fourier series) in the form of $$V(x, y, z)=\sum_n\sum_m \ a(x, n, m) b(y, n) c(z, m) \int\int f(u, v) d(u,n) e(v,m)  du\, dv$$ Here functions $a(), b(), c(), d(), e()$ are known, well behaved (mostly Fourier terms, so smooth sines or expoentials).  $f()$ is unknown. I can certainly safely move all of the terms into the integral: $$V(x, y, z)=\sum_n\sum_m \ \int\int a(x, n, m) b(y, n) c(z, m)  f(u, v) d(u,n) e(v,m)  du\, dv$$ This form is not yet useful to me. I want to switch it around to be in the form  $$V(x, y, z) = \int\int f(u, v) w(u,v) du\, dv $$  where all the summations get combined into a weight function $w(u, v)$ with all the summation nastiness hidden inside. (My final goal is to evaluate this weight function numerically.) I'm sure this is possible... it's a method that's used to compute Greens functions in various applications. But in my derivation steps, I get to the step where the summation and integration $\sum\sum\int\int$ must be swapped to $\int\int\sum\sum$ and I can't figure out when this is ""legal"" and when it isn't. It's a trick that many derivations simply seem to leave as given: they just say ""we swap the order"" in papers like this and lectures like this. Or I find dense theory (not practice) about convergence in complex poles with finite cuts and bruises, Fubini the Great's magic trick, superMeasuringTape theory, yadda yadda, including here. So I understand there's a wealth of fancy theoretical math on this question, and I understand that some lazy physicists just swap the order and don't even try to justify it. I am hoping someone can give me some intermediate balance between these extremes, something which says when the swap is justified and when it's not, assuming really smooth, constantly differentiable, terms like I have in my first equation above. Is the swap always safe for my example above, for any smooth functions $a() b() c() d()$ and $e()$? Thanks!","I have a function (a potential from an electrostatic potential via a Fourier series) in the form of $$V(x, y, z)=\sum_n\sum_m \ a(x, n, m) b(y, n) c(z, m) \int\int f(u, v) d(u,n) e(v,m)  du\, dv$$ Here functions $a(), b(), c(), d(), e()$ are known, well behaved (mostly Fourier terms, so smooth sines or expoentials).  $f()$ is unknown. I can certainly safely move all of the terms into the integral: $$V(x, y, z)=\sum_n\sum_m \ \int\int a(x, n, m) b(y, n) c(z, m)  f(u, v) d(u,n) e(v,m)  du\, dv$$ This form is not yet useful to me. I want to switch it around to be in the form  $$V(x, y, z) = \int\int f(u, v) w(u,v) du\, dv $$  where all the summations get combined into a weight function $w(u, v)$ with all the summation nastiness hidden inside. (My final goal is to evaluate this weight function numerically.) I'm sure this is possible... it's a method that's used to compute Greens functions in various applications. But in my derivation steps, I get to the step where the summation and integration $\sum\sum\int\int$ must be swapped to $\int\int\sum\sum$ and I can't figure out when this is ""legal"" and when it isn't. It's a trick that many derivations simply seem to leave as given: they just say ""we swap the order"" in papers like this and lectures like this. Or I find dense theory (not practice) about convergence in complex poles with finite cuts and bruises, Fubini the Great's magic trick, superMeasuringTape theory, yadda yadda, including here. So I understand there's a wealth of fancy theoretical math on this question, and I understand that some lazy physicists just swap the order and don't even try to justify it. I am hoping someone can give me some intermediate balance between these extremes, something which says when the swap is justified and when it's not, assuming really smooth, constantly differentiable, terms like I have in my first equation above. Is the swap always safe for my example above, for any smooth functions $a() b() c() d()$ and $e()$? Thanks!",,"['real-analysis', 'fourier-analysis', 'harmonic-analysis', 'integration', 'sequences-and-series']"
83,Proving Hölder's Inequality,Proving Hölder's Inequality,,"Let $f,g,\alpha:[a,b]\rightarrow \mathbb{R}$ with $\alpha$ increasing and $f,g \in \mathscr{R}(\alpha)$, and $p,q>0$ with $\frac{1}{p}+\frac{1}{q}=1$. Prove that $$\left|\int_a^b f(x)g(x)d\alpha\right|\leq \left(\int_a^b \left|f(x)\right|^p d\alpha \right)^{1/p} \left(\int_a^b \left|g(x)\right|^q d\alpha \right)^{1/q}$$ I am using Young's inequality, which states that for $a,b>0$, $uv\leq \frac{1}{p}u^{p}+\frac{1}{q}v^{q}$. This gets me as far as showing that $$\left|\int_a^b f(x)g(x)d\alpha\right|\leq \int\left( \frac {1}{p}|f(x)|^p +\frac{1}{q}|g(x)|^q\right)d\alpha$$ But here I'm stuck. I'm vaguely thinking that I could use the fact that $\frac {1}{p}|f(x)|^p +\frac{1}{q}|g(x)|^q$ is a convex combination and so if I do some Jensen's inequality type thing, but I can't figure out a way to make it work out.","Let $f,g,\alpha:[a,b]\rightarrow \mathbb{R}$ with $\alpha$ increasing and $f,g \in \mathscr{R}(\alpha)$, and $p,q>0$ with $\frac{1}{p}+\frac{1}{q}=1$. Prove that $$\left|\int_a^b f(x)g(x)d\alpha\right|\leq \left(\int_a^b \left|f(x)\right|^p d\alpha \right)^{1/p} \left(\int_a^b \left|g(x)\right|^q d\alpha \right)^{1/q}$$ I am using Young's inequality, which states that for $a,b>0$, $uv\leq \frac{1}{p}u^{p}+\frac{1}{q}v^{q}$. This gets me as far as showing that $$\left|\int_a^b f(x)g(x)d\alpha\right|\leq \int\left( \frac {1}{p}|f(x)|^p +\frac{1}{q}|g(x)|^q\right)d\alpha$$ But here I'm stuck. I'm vaguely thinking that I could use the fact that $\frac {1}{p}|f(x)|^p +\frac{1}{q}|g(x)|^q$ is a convex combination and so if I do some Jensen's inequality type thing, but I can't figure out a way to make it work out.",,"['real-analysis', 'integration', 'inequality']"
84,"Constructing a bijection from (0,1) to the irrationals in (0,1)","Constructing a bijection from (0,1) to the irrationals in (0,1)",,"How does one construct a bijection from (0,1) to the irrationals in (0,1)? Or if I am getting my notation right, can you provide an explicit function $f:(0,1)\rightarrow(0,1)\backslash\mathbb{Q}$ such that $f$ is a bijection?","How does one construct a bijection from (0,1) to the irrationals in (0,1)? Or if I am getting my notation right, can you provide an explicit function $f:(0,1)\rightarrow(0,1)\backslash\mathbb{Q}$ such that $f$ is a bijection?",,"['real-analysis', 'elementary-set-theory']"
85,"Does $\sum{\frac{\sin{(nx)}}{n}}$ converge uniformly for all $x$ in $[0,2\pi]$",Does  converge uniformly for all  in,"\sum{\frac{\sin{(nx)}}{n}} x [0,2\pi]","This question arises because of a problem I was doing (Bartle 3rd edition, section 9.4 problem 3). It was like this. Given $a_n$ a decreasing sequence of positive numbers and suppose that $$\sum_{n=0}^{\infty}{a_n \sin{(nx)}}$$ Converge uniformly (It doesn't specify the domain, so I guess is for every x). Prove that $n a_n \to 0$. Clearly $\frac{1}{n}$ fits the description of $a_n$, and $n \frac{1}{n} \to 1 \neq 0$, so this would prove that there is a mistake in the problem if $\sum{\frac{\sin{(nx)}}{n}}$ converge uniformly for all $x$. So my question is if $\sum{\frac{\sin{(nx)}}{n}}$ converge uniformly for every $x$. (I know that the series converge uniformly for every x in $[\delta, 2\pi - \delta]$, for $0 < \delta <2\pi$ by using the Dirichlet criterion.)","This question arises because of a problem I was doing (Bartle 3rd edition, section 9.4 problem 3). It was like this. Given $a_n$ a decreasing sequence of positive numbers and suppose that $$\sum_{n=0}^{\infty}{a_n \sin{(nx)}}$$ Converge uniformly (It doesn't specify the domain, so I guess is for every x). Prove that $n a_n \to 0$. Clearly $\frac{1}{n}$ fits the description of $a_n$, and $n \frac{1}{n} \to 1 \neq 0$, so this would prove that there is a mistake in the problem if $\sum{\frac{\sin{(nx)}}{n}}$ converge uniformly for all $x$. So my question is if $\sum{\frac{\sin{(nx)}}{n}}$ converge uniformly for every $x$. (I know that the series converge uniformly for every x in $[\delta, 2\pi - \delta]$, for $0 < \delta <2\pi$ by using the Dirichlet criterion.)",,"['real-analysis', 'sequences-and-series', 'convergence-divergence']"
86,Necessity of Differential Forms,Necessity of Differential Forms,,"All the undergraduate and graduate texts on analysis introduce Differential and integral calculus (I will assume this introduction of basic calculus/analysis). Among them, some books also introduce differential forms . I then understand that knowledge of differential forms is not too necessary in (Real/complex) analysis. On the other hand, the books, where differential forms are included, do not give any motivation for their consideration in the subsection in which they introduce it. They start the Definition like ... ...... this expression is called $1$-form; ......this expression is called 2 form ..... It is not mentioned in any book, for what purpose it is getting introduced? This really bothers me and gives a feeling like it is memorizing or copy-pasting from some old books, the definitions of differential forms and bombarding it on readers brains! Even in many lectures, I heard that the concept of Differential forms is introduced just as a memorized definition and start games with it! No book explains what is their necessity in analysis ? I believe that almost all the mathematical concepts and especially differential forms have been introduced concerning at least some elementary problem or I feel using differential forms one can interpret some mathematical contexts in a better frame . My question  is For the study of which elementary problems in  analysis, differential forms are necessary?","All the undergraduate and graduate texts on analysis introduce Differential and integral calculus (I will assume this introduction of basic calculus/analysis). Among them, some books also introduce differential forms . I then understand that knowledge of differential forms is not too necessary in (Real/complex) analysis. On the other hand, the books, where differential forms are included, do not give any motivation for their consideration in the subsection in which they introduce it. They start the Definition like ... ...... this expression is called $1$-form; ......this expression is called 2 form ..... It is not mentioned in any book, for what purpose it is getting introduced? This really bothers me and gives a feeling like it is memorizing or copy-pasting from some old books, the definitions of differential forms and bombarding it on readers brains! Even in many lectures, I heard that the concept of Differential forms is introduced just as a memorized definition and start games with it! No book explains what is their necessity in analysis ? I believe that almost all the mathematical concepts and especially differential forms have been introduced concerning at least some elementary problem or I feel using differential forms one can interpret some mathematical contexts in a better frame . My question  is For the study of which elementary problems in  analysis, differential forms are necessary?",,"['real-analysis', 'complex-analysis', 'analysis', 'soft-question']"
87,Sub-dimensional linear subspaces of $\mathbb{R}^{n}$ have measure zero.,Sub-dimensional linear subspaces of  have measure zero.,\mathbb{R}^{n},"I would appreciate it if someone could refer me to a proof (or simply give one here) for the statement in the title. That is: If $k<n$, then every $k-$dimensional subspace of $\mathbb{R}^{n}$ has $n-$dimensional Lebesgue measure zero. I've seen some proofs that use Sard's lemma but I'm not really familiar with that subject and I've never seen a proof of said lemma so I'd appreciate a proof that doesn't use it if possible. Thanks in advance!","I would appreciate it if someone could refer me to a proof (or simply give one here) for the statement in the title. That is: If $k<n$, then every $k-$dimensional subspace of $\mathbb{R}^{n}$ has $n-$dimensional Lebesgue measure zero. I've seen some proofs that use Sard's lemma but I'm not really familiar with that subject and I've never seen a proof of said lemma so I'd appreciate a proof that doesn't use it if possible. Thanks in advance!",,"['real-analysis', 'measure-theory', 'vector-spaces', 'lebesgue-measure', 'geometric-measure-theory']"
88,Uniform convergence of geometric series,Uniform convergence of geometric series,,"How do I show that the geometric series $\sum_{k=0}^\infty x^k$ converges uniformly on any interval $[a,b]$ for $-1 < a < b < 1$? The Cauchy test says that $\sum_{k=0}^\infty x^k$ converges uniformly if, for every $\varepsilon>0$, there exists a natural number $N$ so that for any $m,n>N$ and all $x\in[a,b]$, $|\sum_{k=0}^m x^k - \sum_{k=0}^n x^k|<\varepsilon$. The rightmost condition simplifies to $|\sum_{k=m}^n x^k|<\varepsilon$, but I don't see where to go from there. I realize this is probably a straightforward application of definitions, but I'm really lost here.","How do I show that the geometric series $\sum_{k=0}^\infty x^k$ converges uniformly on any interval $[a,b]$ for $-1 < a < b < 1$? The Cauchy test says that $\sum_{k=0}^\infty x^k$ converges uniformly if, for every $\varepsilon>0$, there exists a natural number $N$ so that for any $m,n>N$ and all $x\in[a,b]$, $|\sum_{k=0}^m x^k - \sum_{k=0}^n x^k|<\varepsilon$. The rightmost condition simplifies to $|\sum_{k=m}^n x^k|<\varepsilon$, but I don't see where to go from there. I realize this is probably a straightforward application of definitions, but I'm really lost here.",,['real-analysis']
89,Proving that $(b_n) \to b$ implies $\left(\frac{1}{b_n}\right) \to \frac{1}{b}$,Proving that  implies,(b_n) \to b \left(\frac{1}{b_n}\right) \to \frac{1}{b},"In my textbook (S. Abbott. Understanding Analysis 1 ed. pp 47 Theorem 2.3.3.iv), the author proves $b_n \to b$ implies $\frac{1}{b_n} \to \frac{1}{b}$ the following way: $$\left|\frac{1}{b_n}-\frac{1}{b}\right|=\frac{|b-b_n|}{|b||b_n|}$$ therefore, since we can make the numerator as small as we like by definition, we just need to find a ""worst-case estimate"" for $\frac{1}{|b||b_n|}$ by considering $|b_n| \geq \lambda > 0$. He then argues that we need to look far into the sequence so that  the terms are closer to $b$ than to $0$. So, considering $\epsilon_0$=$|b|/2$, we can use $|b_n-b|<|b|/2$ for all $n \geq N_1$. Thus, $|b_n| \geq |b|/2$. Then we choose $N_2$ s.t. $n \geq N_2$ implies $|b_n-b|<\frac{\epsilon |b|^{2}}{2}$. Finally, if $N$ = max {$N_1, N_2$}, then $n \geq N$ implies  $$\frac{|b-b_n|}{|b||b_n|}<\frac{\epsilon |b|^{2}}{2} \frac{1}{|b|\frac{|b|}{2}}=\epsilon.$$  What confuses me is why we need to pick elements closer to $b$ than to 0. What makes picking $\epsilon_0$=$|b|/3$ and $|b_n-b|<\frac{\epsilon |b|^{2}}{3}$ wrong for the proof? Thanks.","In my textbook (S. Abbott. Understanding Analysis 1 ed. pp 47 Theorem 2.3.3.iv), the author proves $b_n \to b$ implies $\frac{1}{b_n} \to \frac{1}{b}$ the following way: $$\left|\frac{1}{b_n}-\frac{1}{b}\right|=\frac{|b-b_n|}{|b||b_n|}$$ therefore, since we can make the numerator as small as we like by definition, we just need to find a ""worst-case estimate"" for $\frac{1}{|b||b_n|}$ by considering $|b_n| \geq \lambda > 0$. He then argues that we need to look far into the sequence so that  the terms are closer to $b$ than to $0$. So, considering $\epsilon_0$=$|b|/2$, we can use $|b_n-b|<|b|/2$ for all $n \geq N_1$. Thus, $|b_n| \geq |b|/2$. Then we choose $N_2$ s.t. $n \geq N_2$ implies $|b_n-b|<\frac{\epsilon |b|^{2}}{2}$. Finally, if $N$ = max {$N_1, N_2$}, then $n \geq N$ implies  $$\frac{|b-b_n|}{|b||b_n|}<\frac{\epsilon |b|^{2}}{2} \frac{1}{|b|\frac{|b|}{2}}=\epsilon.$$  What confuses me is why we need to pick elements closer to $b$ than to 0. What makes picking $\epsilon_0$=$|b|/3$ and $|b_n-b|<\frac{\epsilon |b|^{2}}{3}$ wrong for the proof? Thanks.",,['real-analysis']
90,"Let $g$ be a Riemann integrable function on $[a,b]$, and $f$ is a continuous. Prove that $f(g(x))$ is Riemann integrable for all $x\in[a,b]$.","Let  be a Riemann integrable function on , and  is a continuous. Prove that  is Riemann integrable for all .","g [a,b] f f(g(x)) x\in[a,b]","Let $g$ be a Riemann integrable function on $[a,b]$, and assume $f$ is a continuous function defined on $g(x)$, for all $x\in[a,b]$. Prove that $f(g(x))$ is Riemann integrable for all $x\in[a,b]$. What I have so far: I know that if $f(g)$ is continuous then certainly Riemann, but this requires $g$ to be continuous. So consider the definition of Riemann integrable: for all $\epsilon>0$ there exists a partition $P$ such that the difference of upper sum and lower sum is bounded by this $\epsilon$. Since $g$ is Riemann integrable, there exists a partition such that $U(P,g)-L(P,g)<\epsilon$. Since $f$ is continuous, there exists $\delta>0$ such that the difference of upper sum and lower sum over any interval of length $\delta$ is bounded by $\epsilon$. I'm having trouble of combining these two together...","Let $g$ be a Riemann integrable function on $[a,b]$, and assume $f$ is a continuous function defined on $g(x)$, for all $x\in[a,b]$. Prove that $f(g(x))$ is Riemann integrable for all $x\in[a,b]$. What I have so far: I know that if $f(g)$ is continuous then certainly Riemann, but this requires $g$ to be continuous. So consider the definition of Riemann integrable: for all $\epsilon>0$ there exists a partition $P$ such that the difference of upper sum and lower sum is bounded by this $\epsilon$. Since $g$ is Riemann integrable, there exists a partition such that $U(P,g)-L(P,g)<\epsilon$. Since $f$ is continuous, there exists $\delta>0$ such that the difference of upper sum and lower sum over any interval of length $\delta$ is bounded by $\epsilon$. I'm having trouble of combining these two together...",,"['real-analysis', 'integration', 'riemann-integration']"
91,(Elegant) proof of : $x \log_2\frac{1}{x}+(1-x) \log_2\frac{1}{1-x} \geq 1- (1-\frac{x}{1-x})^2$,(Elegant) proof of :,x \log_2\frac{1}{x}+(1-x) \log_2\frac{1}{1-x} \geq 1- (1-\frac{x}{1-x})^2,"I am looking for the most concise and elegant proof of the following inequality: $$ h(x) \geq 1- \left(1-\frac{x}{1-x}\right)^2, \qquad \forall x\in(0,1) $$ where $h(x) = x \log_2\frac{1}{x}+(1-x) \log_2\frac{1}{1-x}$ is the binary entropy function . Below is a graph of the two functions. Of course, an option would be to differentiate $1,2,\dots,k$ times, and study the function this way — it may very well work, but is not only computationally cumbersome, it also feels utterly inelegant. (For my purposes, I could go this way, but I'd rather not.) I am looking for a clever or neat argument involving concavity, Taylor expansion around $1/2$, or anything — an approach that would qualify as ""proof from the Book.""","I am looking for the most concise and elegant proof of the following inequality: $$ h(x) \geq 1- \left(1-\frac{x}{1-x}\right)^2, \qquad \forall x\in(0,1) $$ where $h(x) = x \log_2\frac{1}{x}+(1-x) \log_2\frac{1}{1-x}$ is the binary entropy function . Below is a graph of the two functions. Of course, an option would be to differentiate $1,2,\dots,k$ times, and study the function this way — it may very well work, but is not only computationally cumbersome, it also feels utterly inelegant. (For my purposes, I could go this way, but I'd rather not.) I am looking for a clever or neat argument involving concavity, Taylor expansion around $1/2$, or anything — an approach that would qualify as ""proof from the Book.""",,"['real-analysis', 'inequality', 'information-theory', 'entropy']"
92,"Arithmetic rules for big O notation, little o notation and so on...","Arithmetic rules for big O notation, little o notation and so on...",,"There are many asymptotic notations like the big O notation: big Omega notation, little o notation, ... Thus there are many arithmetic rules for them. For example Donald Knuth states in Concrete Mathemtics (p. 436) the following rules (without a proof): $f(n)=O(f(n))$ $c O(f(n)) = O(f(n))$, if $c$ is constant $O(O(f(n))) = O(f(n))$ ... My Question: Can you recommend a reference where all arithmetic rules of the asymptotic notations are stated and proved? It would be great if also the connections between the asymptotic notations are formulated and shown, e.g. $O(o(f(n))=o(f(n))$. My research results so far: The question What are the rules for equals signs with big-O and little-o? investigates the interpretation rules for the asymptotic notations. The Wikipedia article ""Big O notation"" states some rules but without a proof. (As already mentioned) there are some rules in Concrete Mathematics page 436 without proof. The article ""Some Rules for Big-Oh Notation"" lists some rules (without a proof). Reason for my question: I write my thesis which heavily bases on asymptotic notations. I want to prove all the arithmetic rules I used which are a lot... (I also use other notations like the big Delta notation ). A list of already proved arithmetic rules - which I can cite - would be great here ;-) Update: I had an idea to minimize the number of needed arithmetic rules via generalizing the concept of asymptotic notations. I describe this idea in the MO thread Generalization of asymptotic notations like big O or little o notation .","There are many asymptotic notations like the big O notation: big Omega notation, little o notation, ... Thus there are many arithmetic rules for them. For example Donald Knuth states in Concrete Mathemtics (p. 436) the following rules (without a proof): $f(n)=O(f(n))$ $c O(f(n)) = O(f(n))$, if $c$ is constant $O(O(f(n))) = O(f(n))$ ... My Question: Can you recommend a reference where all arithmetic rules of the asymptotic notations are stated and proved? It would be great if also the connections between the asymptotic notations are formulated and shown, e.g. $O(o(f(n))=o(f(n))$. My research results so far: The question What are the rules for equals signs with big-O and little-o? investigates the interpretation rules for the asymptotic notations. The Wikipedia article ""Big O notation"" states some rules but without a proof. (As already mentioned) there are some rules in Concrete Mathematics page 436 without proof. The article ""Some Rules for Big-Oh Notation"" lists some rules (without a proof). Reason for my question: I write my thesis which heavily bases on asymptotic notations. I want to prove all the arithmetic rules I used which are a lot... (I also use other notations like the big Delta notation ). A list of already proved arithmetic rules - which I can cite - would be great here ;-) Update: I had an idea to minimize the number of needed arithmetic rules via generalizing the concept of asymptotic notations. I describe this idea in the MO thread Generalization of asymptotic notations like big O or little o notation .",,"['calculus', 'real-analysis', 'reference-request', 'notation', 'asymptotics']"
93,Log Trig $\int_0^{\pi/2}\log^4 \tan \frac{x}{2}dx=\frac{5\pi^5}{32}$,Log Trig,\int_0^{\pi/2}\log^4 \tan \frac{x}{2}dx=\frac{5\pi^5}{32},"Hi I am trying to integrate a log trigonometric integral given by $$ I:=\int_0^{\pi/2}\log^4 \tan \frac{x}{2}dx=\frac{5\pi^5}{32}. $$ This is very similar to a previous integral posted except the power of the logarithm. Note this integral is also equal to $$ \int_0^{\pi/2}\log^4 \tan x \, dx=\frac{5\pi^5}{32}. $$ I have wrote I as $$ I=\int_0^{\pi/2} \left(\log \sin \frac{x}{2}-\log \cos \frac{x}{2}\right)^4 dx $$ but got stuck here since factoring this out seems like a mess.  Having seen how David H solved a similar integral I posted, I tried another method starting with I and using $t=\tan x/2$, and obtained $$ I=2\int_0^{1}\log^4 t \frac{dt}{1+t^2}. $$ Following this I tried $u=-\log t$ but got stuck after this.  Thanks, it would be nice to see a solution that doesn't reduce the integral to a difficult sum to evaluate","Hi I am trying to integrate a log trigonometric integral given by $$ I:=\int_0^{\pi/2}\log^4 \tan \frac{x}{2}dx=\frac{5\pi^5}{32}. $$ This is very similar to a previous integral posted except the power of the logarithm. Note this integral is also equal to $$ \int_0^{\pi/2}\log^4 \tan x \, dx=\frac{5\pi^5}{32}. $$ I have wrote I as $$ I=\int_0^{\pi/2} \left(\log \sin \frac{x}{2}-\log \cos \frac{x}{2}\right)^4 dx $$ but got stuck here since factoring this out seems like a mess.  Having seen how David H solved a similar integral I posted, I tried another method starting with I and using $t=\tan x/2$, and obtained $$ I=2\int_0^{1}\log^4 t \frac{dt}{1+t^2}. $$ Following this I tried $u=-\log t$ but got stuck after this.  Thanks, it would be nice to see a solution that doesn't reduce the integral to a difficult sum to evaluate",,"['calculus', 'real-analysis', 'integration', 'complex-analysis', 'definite-integrals']"
94,"Constructing a number not in $\bigcup\limits_{k=1}^{\infty} (q_k-\frac{\epsilon}{2^k},q_k+\frac{\epsilon}{2^k})$",Constructing a number not in,"\bigcup\limits_{k=1}^{\infty} (q_k-\frac{\epsilon}{2^k},q_k+\frac{\epsilon}{2^k})","I have couple of questions related to the properties of real numbers. My first question is as follows. Let $S_{\epsilon} = \displaystyle \bigcup_{k=1}^{\infty} \left( q_k-\frac{\epsilon}{2^{k+1}},q_k+\frac{\epsilon}{2^{k+1}} \right)$, where all the rationals are listed as $\{q_1,q_2,\ldots,q_n,\ldots\}$. The length of this set is bounded by $\epsilon$. This means there are a lot of irrationals not in the set. How do I go about explicitly constructing an irrational number not in the set? The irrational number will depend on the way I list the rationals but once the list is given I should be able to construct an irrational number not in the set. My second question is motivated from this question . I came to know that the set of rationals is not a $G_{\delta}$ set. However let us consider this. Let $$S_n = \bigcup_{k=1}^{\infty} \left(q_k - \frac{\epsilon}{2^{k+n+1}},q_k + \frac{\epsilon}{2^{k+n+1}} \right).$$ Clearly, $S_n$ is an open set and the length of $S_n$ is bounded by $\displaystyle \frac{\epsilon}{2^{n}}$. Let $$S = \bigcap_{n=1}^{\infty} S_n.$$ $S$ is a $G_{\delta}$ set and the length of $S$ is zero. Further, $\mathbb{Q} \subseteq S$. What other numbers are in $S$? How do I explicitly construct a number in $S \backslash \mathbb{Q}$? If there are no other numbers i.e. if $\mathbb{Q} = S$, then doesn't it imply that $\mathbb{Q}$ is a $G_{\delta}$ set? Thanks, Adhvaitha","I have couple of questions related to the properties of real numbers. My first question is as follows. Let $S_{\epsilon} = \displaystyle \bigcup_{k=1}^{\infty} \left( q_k-\frac{\epsilon}{2^{k+1}},q_k+\frac{\epsilon}{2^{k+1}} \right)$, where all the rationals are listed as $\{q_1,q_2,\ldots,q_n,\ldots\}$. The length of this set is bounded by $\epsilon$. This means there are a lot of irrationals not in the set. How do I go about explicitly constructing an irrational number not in the set? The irrational number will depend on the way I list the rationals but once the list is given I should be able to construct an irrational number not in the set. My second question is motivated from this question . I came to know that the set of rationals is not a $G_{\delta}$ set. However let us consider this. Let $$S_n = \bigcup_{k=1}^{\infty} \left(q_k - \frac{\epsilon}{2^{k+n+1}},q_k + \frac{\epsilon}{2^{k+n+1}} \right).$$ Clearly, $S_n$ is an open set and the length of $S_n$ is bounded by $\displaystyle \frac{\epsilon}{2^{n}}$. Let $$S = \bigcap_{n=1}^{\infty} S_n.$$ $S$ is a $G_{\delta}$ set and the length of $S$ is zero. Further, $\mathbb{Q} \subseteq S$. What other numbers are in $S$? How do I explicitly construct a number in $S \backslash \mathbb{Q}$? If there are no other numbers i.e. if $\mathbb{Q} = S$, then doesn't it imply that $\mathbb{Q}$ is a $G_{\delta}$ set? Thanks, Adhvaitha",,['real-analysis']
95,"Differences among Cauchy, Lagrange, and Schlömilch remainder in Taylor's formula: why is generalization useful?","Differences among Cauchy, Lagrange, and Schlömilch remainder in Taylor's formula: why is generalization useful?",,"I would like to know what really are the main differences (in terms of ""usefulness"") among Cauchy , Lagrange , and Schlömilch 's forms of the remainder in Taylor's formula. Could you provide examples of situations where one form ""works better"" than another? Also, what are the actual benefits of the new generalizations proposed for example in the following articles? BLUMENTHAL, L. M., Concerning the Remainder Term in Taylor's Formula . Amer. Math. Monthly 33 , pp. 424-426, 1926. BEESACK, P. R., A General Form of the Remainder in Taylor's Theorem . Amer. Math. Monthly 73 , pp. 64-67, 1966","I would like to know what really are the main differences (in terms of ""usefulness"") among Cauchy , Lagrange , and Schlömilch 's forms of the remainder in Taylor's formula. Could you provide examples of situations where one form ""works better"" than another? Also, what are the actual benefits of the new generalizations proposed for example in the following articles? BLUMENTHAL, L. M., Concerning the Remainder Term in Taylor's Formula . Amer. Math. Monthly 33 , pp. 424-426, 1926. BEESACK, P. R., A General Form of the Remainder in Taylor's Theorem . Amer. Math. Monthly 73 , pp. 64-67, 1966",,"['calculus', 'real-analysis', 'analysis', 'taylor-expansion', 'approximation']"
96,Average Distance Between Zeroes of $\sin(x)+\sin(x\sqrt{2})+\sin(x\sqrt{3})$,Average Distance Between Zeroes of,\sin(x)+\sin(x\sqrt{2})+\sin(x\sqrt{3}),"QUESTION: What is the average distance between the consecutive real zeroes of the function $$f(x)=\sin(x)+\sin(x\sqrt{2})+\sin(x\sqrt{3})$$ or, more specifically, if $z(x)$ is defined as the number of zeroes $\zeta$ satisfying $|\zeta|<x$ , what is the value of $$\lim_{x\to\infty} \frac{2x}{z(x)}=?$$ Here’s some context. I’ve been studying sums of sinusoids with “mutually irrational” periods, such that the sum of the sinusoids is not actually a periodic function. For example, the function $$\sin(x)+\sin(x\sqrt{2})$$ is not periodic, because $\sqrt{2}$ is irrational. In particular, I’ve been looking at the asymptotic distribution of solutions $x$ to equations in the form $$\sin(x)+\sin(\tau x)=\alpha$$ where $\tau \notin \mathbb Q$ and $|\alpha|<2$ . I’ve actually come up with a formula for the average distance between the solutions to the above equation along the real line, but it’s messy so I won’t type it out unless someone cares enough to ask for it.  The case of $\alpha = 0$ is almost trivial though, and can be figured out with an easy trig identity. However, when dealing with three summed sinusoids the case of $\alpha = 0$ is no longer trivial. For two sinusoids, $$\sin(x)+\sin(\tau x)=2\sin\bigg(\frac{\tau+1}{2}x\bigg)\cos\bigg(\frac{\tau - 1}{2}x\bigg)$$ so we can easily calculate the actual explicit values of the zeroes. But for three sinusoids with mutually irrational periods so that $\tau_1, \tau_2, \tau_1/\tau_2 \notin\mathbb Q$ , $$\sin(x)+\sin(\tau_1 x)+\sin(\tau_2 x)$$ I haven’t been able to come up with any explicit formulas for zeroes, or even an asymptotic density of/average distance between zeroes. Can anyone figure out how to work out this problem for the specific case of $\tau_1 = \sqrt{2}$ , $\tau_2 =\sqrt{3}$ ?","QUESTION: What is the average distance between the consecutive real zeroes of the function or, more specifically, if is defined as the number of zeroes satisfying , what is the value of Here’s some context. I’ve been studying sums of sinusoids with “mutually irrational” periods, such that the sum of the sinusoids is not actually a periodic function. For example, the function is not periodic, because is irrational. In particular, I’ve been looking at the asymptotic distribution of solutions to equations in the form where and . I’ve actually come up with a formula for the average distance between the solutions to the above equation along the real line, but it’s messy so I won’t type it out unless someone cares enough to ask for it.  The case of is almost trivial though, and can be figured out with an easy trig identity. However, when dealing with three summed sinusoids the case of is no longer trivial. For two sinusoids, so we can easily calculate the actual explicit values of the zeroes. But for three sinusoids with mutually irrational periods so that , I haven’t been able to come up with any explicit formulas for zeroes, or even an asymptotic density of/average distance between zeroes. Can anyone figure out how to work out this problem for the specific case of , ?","f(x)=\sin(x)+\sin(x\sqrt{2})+\sin(x\sqrt{3}) z(x) \zeta |\zeta|<x \lim_{x\to\infty} \frac{2x}{z(x)}=? \sin(x)+\sin(x\sqrt{2}) \sqrt{2} x \sin(x)+\sin(\tau x)=\alpha \tau \notin \mathbb Q |\alpha|<2 \alpha = 0 \alpha = 0 \sin(x)+\sin(\tau x)=2\sin\bigg(\frac{\tau+1}{2}x\bigg)\cos\bigg(\frac{\tau - 1}{2}x\bigg) \tau_1, \tau_2, \tau_1/\tau_2 \notin\mathbb Q \sin(x)+\sin(\tau_1 x)+\sin(\tau_2 x) \tau_1 = \sqrt{2} \tau_2 =\sqrt{3}","['real-analysis', 'periodic-functions', 'almost-periodic-functions']"
97,Uniform convergence of sequence of convex  functions,Uniform convergence of sequence of convex  functions,,"Let $\{f_n\}$ converge point-wise to $f$, where each $f_n:[a,b]\rightarrow \mathbb{R}$, and each $f_n$ is a continuous convex function. Furthermore, assume that $f$ is continuous. Prove that the convergence is uniform. I was trying to do something like: Consider $a=a_0<a_i<...<a_k=b$, where $a_{i+1}-a_i<\delta$, where the $\delta $ is such that $|x-y|<\delta$ imply that $|f(x)-f(y)|<\epsilon$. Then letting $N_i$ be such that $f_n(a_i)$ is $\epsilon$-close to $f(x_i)$, let $N$ be the max of $N_1,...,N_k$, and hence$$|f(x)-f_n(x)|\leq |f(x)-f(x_i)|+|f(x_i)-f_n(x_i)|+|f_n(x_i)-f_n(x)|$$I can make the first summand small since $f$ is continuos, and the second summand small by letting $n\geq N$. I am having trouble using the fact that they are convex.","Let $\{f_n\}$ converge point-wise to $f$, where each $f_n:[a,b]\rightarrow \mathbb{R}$, and each $f_n$ is a continuous convex function. Furthermore, assume that $f$ is continuous. Prove that the convergence is uniform. I was trying to do something like: Consider $a=a_0<a_i<...<a_k=b$, where $a_{i+1}-a_i<\delta$, where the $\delta $ is such that $|x-y|<\delta$ imply that $|f(x)-f(y)|<\epsilon$. Then letting $N_i$ be such that $f_n(a_i)$ is $\epsilon$-close to $f(x_i)$, let $N$ be the max of $N_1,...,N_k$, and hence$$|f(x)-f_n(x)|\leq |f(x)-f(x_i)|+|f(x_i)-f_n(x_i)|+|f_n(x_i)-f_n(x)|$$I can make the first summand small since $f$ is continuos, and the second summand small by letting $n\geq N$. I am having trouble using the fact that they are convex.",,['real-analysis']
98,Derivatives of a series of monotone functions,Derivatives of a series of monotone functions,,"Suppose that $(f_n)$ is a sequence of monotone non-decreasing functions on $[a,b]$ such that $f(x) = \sum_{n=1} ^\infty f_n (x)$ is finite for each $x\in [a,b]$. By Lebesgue's theorem on monotone functions, each $f_n$ is differentiable almost everywhere, and it is clear that $f$ is monotone as well, so $f$ is also differentiable a.e. Must it be the case that $\sum_{n=1} ^\infty f_n' = f'$ almost everywhere? I know that for each $N$ and for each $h \in \mathbb R$, $ \frac{1}{h}\sum_{n=1} ^N [f_n(x+h) - f(x)] \leq \frac{1}{h}\sum_{n=1} ^\infty [f_n(x+h) - f(x)]  $ and taking the limit as $h \rightarrow 0$ gives $\sum_{n=1} ^N f_n '\leq f'$, and so $\sum_{n=1} ^\infty f_n ' \leq f'$. Is the reverse inequality also true?","Suppose that $(f_n)$ is a sequence of monotone non-decreasing functions on $[a,b]$ such that $f(x) = \sum_{n=1} ^\infty f_n (x)$ is finite for each $x\in [a,b]$. By Lebesgue's theorem on monotone functions, each $f_n$ is differentiable almost everywhere, and it is clear that $f$ is monotone as well, so $f$ is also differentiable a.e. Must it be the case that $\sum_{n=1} ^\infty f_n' = f'$ almost everywhere? I know that for each $N$ and for each $h \in \mathbb R$, $ \frac{1}{h}\sum_{n=1} ^N [f_n(x+h) - f(x)] \leq \frac{1}{h}\sum_{n=1} ^\infty [f_n(x+h) - f(x)]  $ and taking the limit as $h \rightarrow 0$ gives $\sum_{n=1} ^N f_n '\leq f'$, and so $\sum_{n=1} ^\infty f_n ' \leq f'$. Is the reverse inequality also true?",,"['real-analysis', 'measure-theory']"
99,Show that $\int_0^1 h(t)dt\geq\left(\int_0^1 f(t)dt \right)^a\left(\int_0^1 g(t)dt \right)^{1-a}$,Show that,\int_0^1 h(t)dt\geq\left(\int_0^1 f(t)dt \right)^a\left(\int_0^1 g(t)dt \right)^{1-a},"Let $ a \in (0,1)$ and $f,g,h :[0,1]\to (0,\infty)$ be continuous functions satisfying $h(ax+(1-a)y)\geq f(x)^ag(y)^{1-a}$ for all $x,y \in [0,1]$ . Prove that $$ \int_0^1 h(t)dt\geq\left(\int_0^1 f(t)dt  \right)^a\left(\int_0^1 g(t)dt  \right)^{1-a} $$ I tried integrating left side with respect to $x$ and $y$ separately, but it didn't work. Edit: Number ' $a$ ' is chosen only once, the condition does not have to hold for values of ' $a$ ' other then the one we chose.","Let and be continuous functions satisfying for all . Prove that I tried integrating left side with respect to and separately, but it didn't work. Edit: Number ' ' is chosen only once, the condition does not have to hold for values of ' ' other then the one we chose."," a \in (0,1) f,g,h :[0,1]\to (0,\infty) h(ax+(1-a)y)\geq f(x)^ag(y)^{1-a} x,y \in [0,1]  \int_0^1 h(t)dt\geq\left(\int_0^1 f(t)dt  \right)^a\left(\int_0^1 g(t)dt  \right)^{1-a}  x y a a","['real-analysis', 'inequality']"
