,title_raw,title_text,title_latex,body_raw,body_text,body_latex,tags
0,"Keep the value of an 8-sided die roll, or gamble by taking a 12-sided die roll. What's the best strategy?","Keep the value of an 8-sided die roll, or gamble by taking a 12-sided die roll. What's the best strategy?",,"Consider a dice game in which you try to obtain the largest number ( e.g. you win a prize based on the final dice roll). You roll an 8-sided die, with numbers 1–8 on the sides. You may either keep the value you rolled, or choose to roll a 12-sided die, with numbers 1–12 on the sides. What's the best strategy for choosing what to do in step #2? I know the 8-sided die has expected payoff of 4.5, and the 12-sided die has expected payoff of 6.5. So I think relying on the 12-sided die is better — but how do I show the probability of this?","Consider a dice game in which you try to obtain the largest number ( e.g. you win a prize based on the final dice roll). You roll an 8-sided die, with numbers 1–8 on the sides. You may either keep the value you rolled, or choose to roll a 12-sided die, with numbers 1–12 on the sides. What's the best strategy for choosing what to do in step #2? I know the 8-sided die has expected payoff of 4.5, and the 12-sided die has expected payoff of 6.5. So I think relying on the 12-sided die is better — but how do I show the probability of this?",,"['probability', 'dice']"
1,How does Elo rating scale with games played?,How does Elo rating scale with games played?,,I only take two players starting out at 0 Elo and have them play against each other with one player winning all games. Also I consider a pure version of the Elo system without artificially introduced cut-offs (like FIDE's 400 point rule). The winner is constantly gaining rating while the other player is losing Elo rating. The gain is decreasing because the rating difference of the players increases. How does (mathematically) the Elo rating scale with the number of games ( n ) played for large n ?,I only take two players starting out at 0 Elo and have them play against each other with one player winning all games. Also I consider a pure version of the Elo system without artificially introduced cut-offs (like FIDE's 400 point rule). The winner is constantly gaining rating while the other player is losing Elo rating. The gain is decreasing because the rating difference of the players increases. How does (mathematically) the Elo rating scale with the number of games ( n ) played for large n ?,,"['probability', 'mathematical-modeling']"
2,Is tossing of a coin deterministic experimemt?,Is tossing of a coin deterministic experimemt?,,"This is a question that I practically encountered while I was playing a game: Is tossing a coin a deterministic experiment? It might seem silly to ask but I had some thought over it. By the definition of the term and apparent look, it is not. But I have a different notion. Deterministic experiments have the same conditions (it may be physical or regarding apparatus) while conducting an experiment.  But while tossing a coin do we always apply the same force at the same point of the coin. We toss with the same side of the coin upwards. If we do so, then then the outcome would be same. A 10 kg block would move with 10 m/s² acceleration when a force of 10 N is applied. Similarly, if we toss a coin applying the same torque at the same point, then the number of rotations in the air would be the same and give a consistent result. I was scolded by my teacher on asking this question.","This is a question that I practically encountered while I was playing a game: Is tossing a coin a deterministic experiment? It might seem silly to ask but I had some thought over it. By the definition of the term and apparent look, it is not. But I have a different notion. Deterministic experiments have the same conditions (it may be physical or regarding apparatus) while conducting an experiment.  But while tossing a coin do we always apply the same force at the same point of the coin. We toss with the same side of the coin upwards. If we do so, then then the outcome would be same. A 10 kg block would move with 10 m/s² acceleration when a force of 10 N is applied. Similarly, if we toss a coin applying the same torque at the same point, then the number of rotations in the air would be the same and give a consistent result. I was scolded by my teacher on asking this question.",,"['probability', 'physics', 'chaos-theory']"
3,Connecting noodles probability question,Connecting noodles probability question,,"I don't know how to solve this. You have 100 noodles in your soup bowl. Being blindfolded, you are told to take two ends of some noodles (each end of any noodle has the same probability of being chosen) in your bowl and connect them. You continue until there are no free ends. The number of loops formed by the noodles this way is stochastic. Calculate the expected number of circles.","I don't know how to solve this. You have 100 noodles in your soup bowl. Being blindfolded, you are told to take two ends of some noodles (each end of any noodle has the same probability of being chosen) in your bowl and connect them. You continue until there are no free ends. The number of loops formed by the noodles this way is stochastic. Calculate the expected number of circles.",,"['probability', 'harmonic-numbers']"
4,Proof for the calculation of mean in negative binomial distribution,Proof for the calculation of mean in negative binomial distribution,,"I am trying to figure out the mean for negative binomial distribution but have run into mistakes. I know there are other posts on deriving the mean bu I am attempting to derive it in my own way. I wonder if any of you can point out where my mistake is: In negative binomial distribution, the probability is: $$ p(X=x) = \frac{(x-1)!}{(r-1)!(x-r)!}p^r(1-p)^{x-r}, $$ where $X$ is a random variable for the number of trials required, $x$ is the number of trials, p is the probability of success, and r is the number of success until $x$th trial. Therefore, to calculate expectation: $$ E(x) = \sum_{x=r}^{\infty}xp(x)=x\sum_{x=r}^{\infty}\frac{(x-1)!}{(r-1)!(x-r)!}p^r(1-p)^{x-r}=\sum_{x=r}^{\infty}\frac{x!}{(r-1)!(x-r)!}p^r(1-p)^{x-r} $$ Let $k=x-r$, then the formula becomes: $$ E(x)=\sum_{k=0}^{\infty}\frac{(k+r)!}{(r-1)!k!}p^r(1-p)^k= \sum_{k=0}^{\infty}\frac{(k+r)!}{(r-1)!k!}p^r(1-p)^k= r\sum_{k=0}^{\infty}\frac{(k+r)!}{r!k!}p^r(1-p)^k $$ By binomial theorem, $\sum_{k=0}^{\infty}\frac{(k+r)!}{r!k!}p^r(1-p)^k$ becomes $[p+(1-p)]^{k+r} = 1$, and thus $E(x) = r$, which is obviously wrong. I cannot figure out what is wrong with my proof, and thus any help will be appreciated. For reference, someone else has done a similar proof here, but I still have trouble understanding the mistake(s) in my proof: Deriving Mean for Negative Binomial Distribution.","I am trying to figure out the mean for negative binomial distribution but have run into mistakes. I know there are other posts on deriving the mean bu I am attempting to derive it in my own way. I wonder if any of you can point out where my mistake is: In negative binomial distribution, the probability is: $$ p(X=x) = \frac{(x-1)!}{(r-1)!(x-r)!}p^r(1-p)^{x-r}, $$ where $X$ is a random variable for the number of trials required, $x$ is the number of trials, p is the probability of success, and r is the number of success until $x$th trial. Therefore, to calculate expectation: $$ E(x) = \sum_{x=r}^{\infty}xp(x)=x\sum_{x=r}^{\infty}\frac{(x-1)!}{(r-1)!(x-r)!}p^r(1-p)^{x-r}=\sum_{x=r}^{\infty}\frac{x!}{(r-1)!(x-r)!}p^r(1-p)^{x-r} $$ Let $k=x-r$, then the formula becomes: $$ E(x)=\sum_{k=0}^{\infty}\frac{(k+r)!}{(r-1)!k!}p^r(1-p)^k= \sum_{k=0}^{\infty}\frac{(k+r)!}{(r-1)!k!}p^r(1-p)^k= r\sum_{k=0}^{\infty}\frac{(k+r)!}{r!k!}p^r(1-p)^k $$ By binomial theorem, $\sum_{k=0}^{\infty}\frac{(k+r)!}{r!k!}p^r(1-p)^k$ becomes $[p+(1-p)]^{k+r} = 1$, and thus $E(x) = r$, which is obviously wrong. I cannot figure out what is wrong with my proof, and thus any help will be appreciated. For reference, someone else has done a similar proof here, but I still have trouble understanding the mistake(s) in my proof: Deriving Mean for Negative Binomial Distribution.",,"['probability', 'statistics']"
5,Expected number of cycles in permutation [duplicate],Expected number of cycles in permutation [duplicate],,"This question already has answers here : Name Drawing Puzzle (3 answers) Closed 7 years ago . Consider a random permutation of $1,2,\ldots,n$. What is the expected number of cycles in it? I thought about using linearity of expectation, but here it's not clear how we can break down the main random variable into different ones.","This question already has answers here : Name Drawing Puzzle (3 answers) Closed 7 years ago . Consider a random permutation of $1,2,\ldots,n$. What is the expected number of cycles in it? I thought about using linearity of expectation, but here it's not clear how we can break down the main random variable into different ones.",,"['probability', 'combinatorics', 'expectation']"
6,"Given two randomly chosen natural numbers, what is the probability that the second is greater than the first?","Given two randomly chosen natural numbers, what is the probability that the second is greater than the first?",,"Suppose that there exists some apparatus that, when prompted, displays a random natural number (i.e. it picks an integer uniformly from the range $[1, \infty)$). A man writes down a number generated by this apparatus, denotes it as the first number, and afterwards prompts the apparatus for another number, writes it down, and denotes it as the second number. What is the probability that the second number will be greater than the first number? I think the answer is $\frac{1}{2}$, because in the general case, the probability of the second number being greater than the first given two integers selected from the range $[1, n]$ is $\frac{n - 1}{2n}$. If you take the limit of this as $n$ approaches infinity, you get $\frac{1}{2}$. I have a friend who says that the answer is $1$, since the first number is guaranteed to be finite. He argues that if you have some finite number and then choose a random number from the set $[1, n]$, the resulting number, with probability $1$, will be greater than the originally chosen finite number. Which one of us, if not both, is wrong? Or is the problem poorly defined and thus impossible to answer?","Suppose that there exists some apparatus that, when prompted, displays a random natural number (i.e. it picks an integer uniformly from the range $[1, \infty)$). A man writes down a number generated by this apparatus, denotes it as the first number, and afterwards prompts the apparatus for another number, writes it down, and denotes it as the second number. What is the probability that the second number will be greater than the first number? I think the answer is $\frac{1}{2}$, because in the general case, the probability of the second number being greater than the first given two integers selected from the range $[1, n]$ is $\frac{n - 1}{2n}$. If you take the limit of this as $n$ approaches infinity, you get $\frac{1}{2}$. I have a friend who says that the answer is $1$, since the first number is guaranteed to be finite. He argues that if you have some finite number and then choose a random number from the set $[1, n]$, the resulting number, with probability $1$, will be greater than the originally chosen finite number. Which one of us, if not both, is wrong? Or is the problem poorly defined and thus impossible to answer?",,['probability']
7,Is this a paradox about probability of a fair coin at very large numbers of flips?,Is this a paradox about probability of a fair coin at very large numbers of flips?,,"The binomial formula for the probability of x heads on n flips with a probability of 0.5 is: $$\displaystyle \frac{\frac{n!}{x!(n-x)!)}}{2^{n}}$$ For the probability of getting exactly n/2 heads, this becomes: $$\displaystyle\frac{\frac{n!}{(\frac{n}{2})!(\frac{n}{2})!}}{2^{n}}$$ I don't know how to evaluate this in the limit that n approaches infinity, but I used a spreadsheet's binomdist function (x,n,0.5,false) to calculate the probability of exactly 50% heads on n flips (n = 2x), where x ranges from ten to a billion, with steps increasing by powers of 10.  Up to 1 billion heads on 2 billion flips, the probability of the 50% heads decreases per the table below. Heads Flips Probability 10 20 0.176197052 100 200 0.05634847901 1000 2000 0.01783901115 10000 20000 0.005641825312 100000 200000 0.001784121886 1000000 2000000 0.000564189513 10000000 20000000 0.0001784124094 100000000 200000000 0.00005641895828 1000000000 2000000000 0.00001784124116 It seems that the probability of an outcome with exactly 50% heads decreases as the number of flips increases.  This makes sense to me because the number of categories of outcomes that are near (but not equal to) 50% heads increases with increasing numbers of flips, so some of the most central probability would be reapportioned to the near-neighbor categories.  I also know that the likelihood of getting extreme results decreases as n increases, so that more of the probability is centralized around the 50% heads rather than the tails. However, from another point of view, it seems surprising.  If the trend continues in the limit that n approaches infinity, then the probability of getting exactly 50% heads is zero when the coin is flipped an infinite number of times.  This contradicts how I had thought expectation values work.  I had thought that by flipping a coin an infinite number of times, the limiting-case behavior equates to the expectation value.  The expectation value of heads for the coin is 0.5, but the binomial function suggests that we never reach it. I recognize that the probability distribution becomes increasingly centralized about 50% in a symmetrical way.  However it seems that it never actually lands there, even after an infinite number of flips, which seems paradoxical. Am I understanding this correctly?","The binomial formula for the probability of x heads on n flips with a probability of 0.5 is: For the probability of getting exactly n/2 heads, this becomes: I don't know how to evaluate this in the limit that n approaches infinity, but I used a spreadsheet's binomdist function (x,n,0.5,false) to calculate the probability of exactly 50% heads on n flips (n = 2x), where x ranges from ten to a billion, with steps increasing by powers of 10.  Up to 1 billion heads on 2 billion flips, the probability of the 50% heads decreases per the table below. Heads Flips Probability 10 20 0.176197052 100 200 0.05634847901 1000 2000 0.01783901115 10000 20000 0.005641825312 100000 200000 0.001784121886 1000000 2000000 0.000564189513 10000000 20000000 0.0001784124094 100000000 200000000 0.00005641895828 1000000000 2000000000 0.00001784124116 It seems that the probability of an outcome with exactly 50% heads decreases as the number of flips increases.  This makes sense to me because the number of categories of outcomes that are near (but not equal to) 50% heads increases with increasing numbers of flips, so some of the most central probability would be reapportioned to the near-neighbor categories.  I also know that the likelihood of getting extreme results decreases as n increases, so that more of the probability is centralized around the 50% heads rather than the tails. However, from another point of view, it seems surprising.  If the trend continues in the limit that n approaches infinity, then the probability of getting exactly 50% heads is zero when the coin is flipped an infinite number of times.  This contradicts how I had thought expectation values work.  I had thought that by flipping a coin an infinite number of times, the limiting-case behavior equates to the expectation value.  The expectation value of heads for the coin is 0.5, but the binomial function suggests that we never reach it. I recognize that the probability distribution becomes increasingly centralized about 50% in a symmetrical way.  However it seems that it never actually lands there, even after an infinite number of flips, which seems paradoxical. Am I understanding this correctly?",\displaystyle \frac{\frac{n!}{x!(n-x)!)}}{2^{n}} \displaystyle\frac{\frac{n!}{(\frac{n}{2})!(\frac{n}{2})!}}{2^{n}},"['probability', 'probability-distributions', 'probability-limit-theorems']"
8,How calculate the probability density function of $Z = X_1/X_2$,How calculate the probability density function of,Z = X_1/X_2,"Let $X_1$ and $X_2$ be two continuous r.v., my question is: what is the p.d.f of $Z=X_1/X_2$?","Let $X_1$ and $X_2$ be two continuous r.v., my question is: what is the p.d.f of $Z=X_1/X_2$?",,['probability']
9,Why does $ \operatorname{Var}(X) = E[X^2] - (E[X])^2 $,Why does, \operatorname{Var}(X) = E[X^2] - (E[X])^2 ,"$ \operatorname{Var}(X) = E[X^2] - (E[X])^2 $ I have seen and understand (mathematically) the proof for this. What I want to understand is: intuitively, why is this true? What does this formula tell us? From the formula, we see that if we subtract the square of expected value of x from the expected value of $ x^2 $ , we get a measure of dispersion in the data (or in the case of standard deviation, the root of this value gets us a measure of dispersion in the data). So it seems that there is some linkage between the expected value of $ x^2 $ and $ x $ . How do I make sense of this formula? For example, the formula $$ \sigma^2 = \frac 1n \sum_{i = 1}^n (x_i - \bar{x})^2 $$ makes perfect intuitive sense. It simply gives us the average of squares of deviations from the mean. What does the other formula tell us?","I have seen and understand (mathematically) the proof for this. What I want to understand is: intuitively, why is this true? What does this formula tell us? From the formula, we see that if we subtract the square of expected value of x from the expected value of , we get a measure of dispersion in the data (or in the case of standard deviation, the root of this value gets us a measure of dispersion in the data). So it seems that there is some linkage between the expected value of and . How do I make sense of this formula? For example, the formula makes perfect intuitive sense. It simply gives us the average of squares of deviations from the mean. What does the other formula tell us?", \operatorname{Var}(X) = E[X^2] - (E[X])^2   x^2   x^2   x   \sigma^2 = \frac 1n \sum_{i = 1}^n (x_i - \bar{x})^2 ,"['probability', 'statistics', 'variance']"
10,Difference between Real Analysis and Probability Theory?,Difference between Real Analysis and Probability Theory?,,"I do not really see a big difference between the two subjects. I was wondering if somebody can explain what the big difference between them is. Let us compare the superficial differences: In real analysis our subsets are called ""measurable sets"", in probability our subsets are called ""events"". The measure of a set in analysis is called the ""measure"", while in probability it is called ""probability"". In real analysis we deal with ""measurable functions"", in probability theory we deal with ""random variables"". In probability theory random variables induce ""distributions"", while in real analysis they are more naturally called ""push-forwards"". In analysis we ""integrate"" with respect to the measure, in probability we compute the ""expected value"". In analysis we say ""almost everywhere"" in almost every theorem, and in probability we say ""almost surely"" in almost every theorem. There is one major difference: Probability theory assumes that we have a finite measure normalized to be equal to 1. Other than that last part everything else seems to be essentially the same. It is the ""finite measure assumption"" which makes probability theory ""work"". The only difference that I see is that, analysis is more general than probability theory. In mathematics we often require more generality with a compromise of some of its theorems. Is there something more?","I do not really see a big difference between the two subjects. I was wondering if somebody can explain what the big difference between them is. Let us compare the superficial differences: In real analysis our subsets are called ""measurable sets"", in probability our subsets are called ""events"". The measure of a set in analysis is called the ""measure"", while in probability it is called ""probability"". In real analysis we deal with ""measurable functions"", in probability theory we deal with ""random variables"". In probability theory random variables induce ""distributions"", while in real analysis they are more naturally called ""push-forwards"". In analysis we ""integrate"" with respect to the measure, in probability we compute the ""expected value"". In analysis we say ""almost everywhere"" in almost every theorem, and in probability we say ""almost surely"" in almost every theorem. There is one major difference: Probability theory assumes that we have a finite measure normalized to be equal to 1. Other than that last part everything else seems to be essentially the same. It is the ""finite measure assumption"" which makes probability theory ""work"". The only difference that I see is that, analysis is more general than probability theory. In mathematics we often require more generality with a compromise of some of its theorems. Is there something more?",,['probability']
11,The two envelopes problem,The two envelopes problem,,"Suppose you're given two envelopes. Both envelopes have money in them, and you're told that one envelope has twice as much money as the other. Suppose you pick one of the envelopes. Should you switch to the other one? Intuitively, you don't know anything about either envelope, so it'd be ridiculous to say that you should switch to the other envelope to maximize your expected money. However, consider this argument. Let $x$ be the amount of money in the envelope you picked. If $y$ is the amount of money in the other envelope, then the expected value equals $$E(y) = \frac{1}{2}\left(\frac{1}{2}x\right) + \frac{1}{2}\left(2x\right) = \frac{5}{4} x$$ But $5x/4 > x$, so you should switch! The Wikipedia article says that $x$ stands for two different things, so this reasoning doesn't work. I say this is not a valid resolution. Consider opening up the envelope that you pick, and finding $\$10$ inside. Then you can run the expected value calculation to get $$E(y) = \frac{1}{2} \cdot \$5+\frac{1}{2} \cdot \$20 = \$12.50$$ This means that if you open one of the envelopes and find $\$10$, you should switch to the other envelope. The $\$10$ doesn't stand for two different things, it literally just means $\$10$. But you don't have to open up the envelope to run this calculation, you can just imagine what's inside, and run the calculation based on that. This is what ""Let $x$ be the amount in the envelope"" means. The problem with the argument is not that $x$ stands for two different things. So what is the problem? Previous questions on stack exchange have given the resolution that I just said I wasn't satisfied by, so please don't mark this as a duplicate . I want a different resolution, or a more satisfying explanation of why $x$ does stand for two different things. Apparently there is still research being published about this problem - maybe it isn't so obvious? I think there's something subtle wrong with the premise. Because there's no uniform probability distribution on $\mathbb{R}$, statements like ""random real number"" are not well-defined. Likewise, I think ""one envelope has twice as much money as the other"" assumes some probability distribution on $\mathbb{R}$, and perhaps our expected value calculation assumes that this distribution is uniform, which it cannot be ...","Suppose you're given two envelopes. Both envelopes have money in them, and you're told that one envelope has twice as much money as the other. Suppose you pick one of the envelopes. Should you switch to the other one? Intuitively, you don't know anything about either envelope, so it'd be ridiculous to say that you should switch to the other envelope to maximize your expected money. However, consider this argument. Let $x$ be the amount of money in the envelope you picked. If $y$ is the amount of money in the other envelope, then the expected value equals $$E(y) = \frac{1}{2}\left(\frac{1}{2}x\right) + \frac{1}{2}\left(2x\right) = \frac{5}{4} x$$ But $5x/4 > x$, so you should switch! The Wikipedia article says that $x$ stands for two different things, so this reasoning doesn't work. I say this is not a valid resolution. Consider opening up the envelope that you pick, and finding $\$10$ inside. Then you can run the expected value calculation to get $$E(y) = \frac{1}{2} \cdot \$5+\frac{1}{2} \cdot \$20 = \$12.50$$ This means that if you open one of the envelopes and find $\$10$, you should switch to the other envelope. The $\$10$ doesn't stand for two different things, it literally just means $\$10$. But you don't have to open up the envelope to run this calculation, you can just imagine what's inside, and run the calculation based on that. This is what ""Let $x$ be the amount in the envelope"" means. The problem with the argument is not that $x$ stands for two different things. So what is the problem? Previous questions on stack exchange have given the resolution that I just said I wasn't satisfied by, so please don't mark this as a duplicate . I want a different resolution, or a more satisfying explanation of why $x$ does stand for two different things. Apparently there is still research being published about this problem - maybe it isn't so obvious? I think there's something subtle wrong with the premise. Because there's no uniform probability distribution on $\mathbb{R}$, statements like ""random real number"" are not well-defined. Likewise, I think ""one envelope has twice as much money as the other"" assumes some probability distribution on $\mathbb{R}$, and perhaps our expected value calculation assumes that this distribution is uniform, which it cannot be ...",,"['probability', 'paradoxes']"
12,Proof of $E(X)=a$ when $a$ is a point of symmetry,Proof of  when  is a point of symmetry,E(X)=a a,"I am trying to develop a proof of the following: Given a random variable $X$ with symmetric probability density function $f(x)$, prove that $E(X)=a$ where $a$ is the point of symmetry. A couple of thoughts: It's easy to think of examples where this applies (e.g. normal) and doesn't (e.g. Cauchy). I am comfortable with calculating expectation, and even handling showing how $\int_{-\infty}^af(x)dx=\int_{a}^{\infty}f(x)dx$ But if the specific form of the pdf is not specified, is it possible to prove this in general? Also, how would one go about showing the existence of the expectation in general?","I am trying to develop a proof of the following: Given a random variable $X$ with symmetric probability density function $f(x)$, prove that $E(X)=a$ where $a$ is the point of symmetry. A couple of thoughts: It's easy to think of examples where this applies (e.g. normal) and doesn't (e.g. Cauchy). I am comfortable with calculating expectation, and even handling showing how $\int_{-\infty}^af(x)dx=\int_{a}^{\infty}f(x)dx$ But if the specific form of the pdf is not specified, is it possible to prove this in general? Also, how would one go about showing the existence of the expectation in general?",,"['probability', 'statistics', 'random-variables']"
13,Estimation of $\pi$ using dice,Estimation of  using dice,\pi,I have been asked in a Quant interview to estimate the value of $\pi$ using dice. I don't know even how to start with. Any help would be appreciated. Thanks.,I have been asked in a Quant interview to estimate the value of $\pi$ using dice. I don't know even how to start with. Any help would be appreciated. Thanks.,,['probability']
14,Probability of Head in coin flip when coin is flipped two times,Probability of Head in coin flip when coin is flipped two times,,"Probability of getting a head in coin flip is $1/2$.  If the coin is flipped two times what is the probability of getting a head in either of those attempts? I think both the coin flips are mutually exclusive events, so the probability would be getting head in attempt $1$ or attempt $2$ which is: $$P(\text{attempt $1$}) + P(\text{attempt $2$}) =  1/2 + 1/2 = 1$$ $100\%$ probability sounds wrong? What am I doing wrong. If I apply the same logic then probability of getting at least $1$ head in $3$ attempt will be $1/2+1/2+1/2 = 3/2 = 1.5$ which I know for sure is wrong. What do I have mixed up?","Probability of getting a head in coin flip is $1/2$.  If the coin is flipped two times what is the probability of getting a head in either of those attempts? I think both the coin flips are mutually exclusive events, so the probability would be getting head in attempt $1$ or attempt $2$ which is: $$P(\text{attempt $1$}) + P(\text{attempt $2$}) =  1/2 + 1/2 = 1$$ $100\%$ probability sounds wrong? What am I doing wrong. If I apply the same logic then probability of getting at least $1$ head in $3$ attempt will be $1/2+1/2+1/2 = 3/2 = 1.5$ which I know for sure is wrong. What do I have mixed up?",,"['probability', 'combinatorics', 'probability-theory']"
15,Group of $r$ people at least three people have the same birthday?,Group of  people at least three people have the same birthday?,r,"What is the probability that in a randomly chosen group of $r$ people at least three people have the same birthday? $\displaystyle 1- \frac{365\cdot364 \cdots(365-r+1)}{365^r}$ $\displaystyle \frac{365\cdot364         \cdots(365-r+1)}{365^r} +{r\choose 2}\cdot \frac{365\cdot364\cdot363 \cdots         (364-(r-2) +1)}{364^{r-2}}$ $\displaystyle 1- \frac{365\cdot364             \cdots(365-r+1)}{365^r} +{r\choose 2}\cdot \frac{365\cdot364\cdot363 \cdots (364-(r-2) +1)}{364^{r-2}}$ $\displaystyle\frac{365\cdot364 \cdots(365-r+1)}{365^r}                 $ My attempt : (May be typo in option $(3)$ of question !) $$P(\text{at least 3 persons have same birthday})$$ $$= 1 - \{P\text{(no one has same birthday) + P(any 2 have same birthday)\}}$$ So, option $(3)$ is true. Can you explain it, please? It asked here before,  but I'm not satisfied by explanation.","What is the probability that in a randomly chosen group of $r$ people at least three people have the same birthday? $\displaystyle 1- \frac{365\cdot364 \cdots(365-r+1)}{365^r}$ $\displaystyle \frac{365\cdot364         \cdots(365-r+1)}{365^r} +{r\choose 2}\cdot \frac{365\cdot364\cdot363 \cdots         (364-(r-2) +1)}{364^{r-2}}$ $\displaystyle 1- \frac{365\cdot364             \cdots(365-r+1)}{365^r} +{r\choose 2}\cdot \frac{365\cdot364\cdot363 \cdots (364-(r-2) +1)}{364^{r-2}}$ $\displaystyle\frac{365\cdot364 \cdots(365-r+1)}{365^r}                 $ My attempt : (May be typo in option $(3)$ of question !) $$P(\text{at least 3 persons have same birthday})$$ $$= 1 - \{P\text{(no one has same birthday) + P(any 2 have same birthday)\}}$$ So, option $(3)$ is true. Can you explain it, please? It asked here before,  but I'm not satisfied by explanation.",,"['probability', 'combinatorics', 'permutations', 'birthday']"
16,Proving the Law of the Unconscious Statistician,Proving the Law of the Unconscious Statistician,,"The proof seems a little too easy.  I am wondering if I misunderstood something. Let $Y = g(X)$. Prove the $$\mathbb E(Y) = \sum_x g(x)f_x(x)$$   provided that the sum converges absolutely. By definition: $\mathbb E(Y) = \sum_y yf_y(y)$. Since $g^{-1}(y) = \{x_1, x_2,\dots\}$ $$f_y(y) = \mathbb P(Y=y) = \sum_i \mathbb P(X=x_i)$$ where $g(x_i) = y$. Hence $$\mathbb E(Y) = \sum_y y \sum_i \mathbb P(x_i) = \sum_y \sum_i g(x_i) \mathbb P(x_i) = \sum_i g(x_i) \mathbb P(x_i) = \sum_x g(x)f_x(x)$$ My understanding is that you need to capture every single $x_i \in g^{-1}(y)$. Repeat the process for every $y$ and then add up the terms.","The proof seems a little too easy.  I am wondering if I misunderstood something. Let $Y = g(X)$. Prove the $$\mathbb E(Y) = \sum_x g(x)f_x(x)$$   provided that the sum converges absolutely. By definition: $\mathbb E(Y) = \sum_y yf_y(y)$. Since $g^{-1}(y) = \{x_1, x_2,\dots\}$ $$f_y(y) = \mathbb P(Y=y) = \sum_i \mathbb P(X=x_i)$$ where $g(x_i) = y$. Hence $$\mathbb E(Y) = \sum_y y \sum_i \mathbb P(x_i) = \sum_y \sum_i g(x_i) \mathbb P(x_i) = \sum_i g(x_i) \mathbb P(x_i) = \sum_x g(x)f_x(x)$$ My understanding is that you need to capture every single $x_i \in g^{-1}(y)$. Repeat the process for every $y$ and then add up the terms.",,"['probability', 'proof-verification']"
17,Estimate probabilities from its moments,Estimate probabilities from its moments,,"I want to estimate probability $Pr(X \leq a)$, where $X$ is a continuous random variable and $a$ is given, only based on some moments of $X$ (e.g., the first four moments, but without knowing its distribution type).","I want to estimate probability $Pr(X \leq a)$, where $X$ is a continuous random variable and $a$ is given, only based on some moments of $X$ (e.g., the first four moments, but without knowing its distribution type).",,"['probability', 'statistics', 'probability-distributions']"
18,Gaussian distribution on a $2$-sphere,Gaussian distribution on a -sphere,2,"I am wondering if there is a probability distribution function that emulates a Gaussian like distribution on a sphere. The mean $\mu$ would correspond to one single point on the sphere and $\sigma$ is a number that gives the standard deviation. I would guess that the pdf should be such that if $\sigma \rightarrow \infty$, then the pdf converges to a uniform distribution and if $\sigma \rightarrow 0$, then the pdf converges to a delta function on the sphere concentrated at the point $\mu$. Is there a well-known function of this type? If there is none, I would appreciate any hints towards obtaining such a function. Thank you all for your help.","I am wondering if there is a probability distribution function that emulates a Gaussian like distribution on a sphere. The mean $\mu$ would correspond to one single point on the sphere and $\sigma$ is a number that gives the standard deviation. I would guess that the pdf should be such that if $\sigma \rightarrow \infty$, then the pdf converges to a uniform distribution and if $\sigma \rightarrow 0$, then the pdf converges to a delta function on the sphere concentrated at the point $\mu$. Is there a well-known function of this type? If there is none, I would appreciate any hints towards obtaining such a function. Thank you all for your help.",,"['probability', 'differential-geometry']"
19,What are the sample spaces when talking about continuous random variables?,What are the sample spaces when talking about continuous random variables?,,"When talking about continuous random variables, with a particular probability distribution, what are the underlying sample spaces? Additionally, why these sample spaces are omitted oftentimes, and one simply says r.v. $X$ follows a uniform distribution on the interval $[0,1]$ ? Isn't the sample space critically important?","When talking about continuous random variables, with a particular probability distribution, what are the underlying sample spaces? Additionally, why these sample spaces are omitted oftentimes, and one simply says r.v. follows a uniform distribution on the interval ? Isn't the sample space critically important?","X [0,1]","['probability', 'probability-theory']"
20,Couple Probability,Couple Probability,,"The problem states that there are 12 boys and 12 girls. Each boy chooses a girl at random and each girl chooses a boy at random. If a boy and a girl choose each other, they form a couple. It then asks to find the probability that no couple is formed. How should we approach such problems? I get a hint of derangements, but I am not able to apply it. Please help me out. Thank you.","The problem states that there are 12 boys and 12 girls. Each boy chooses a girl at random and each girl chooses a boy at random. If a boy and a girl choose each other, they form a couple. It then asks to find the probability that no couple is formed. How should we approach such problems? I get a hint of derangements, but I am not able to apply it. Please help me out. Thank you.",,"['probability', 'combinatorics']"
21,Expected time to fuse a $D_{10}$ dragon,Expected time to fuse a  dragon,D_{10},"I play a game called DragonSky merge and idle (or something like this). The basic premise of the early game is that dragons will spawn, and will be fused in pairs. You continue to fuse these until you have a level $10$ dragon. Let me be precise. Let $\{D_1,D_2,\dots, D_{10}\}$ denote the set of types of dragons. Then, the following occurs: Every $.9$ seconds a dragon will spawn of type $D_1$ with probability $.8$ , and of type $D_2$ with probability $.2$ . For each $i\in\Bbb N$ such that $1\leq i\leq 8$ , two dragons of type $D_i$ will be fused to form a dragon of type $D_{i+1}$ with probability $P_1=.85$ or a dragon of type $D_{i+2}$ with probability $P_2=.15$ . For $i=9$ , they will always fuse to a dragon of type $D_{10}$ . This merging will occur until there is no pair of dragons of the same type. These two steps will continuously repeat. As a small example of $6$ time steps, let us denote a collection of $k$ dragons of type $D_i$ by $d_{i}^1,\dots,d_i^k$ (of course, after fusion $k\in\{0,1\}$ ). Then we could have the following sequence of sets of dragons, where $\overset{1}{\to}$ means rule $1$ was applied (a dragon spawned), and $\overset{2}{\to}$ means rule $2$ was applied (a pair of dragons was fused). $$\emptyset\overset{1}{\to}\{d_1^1\}\overset{1}{\to}\{d_1^1,d_1^2\}\overset{2}{\to}\{d_2^1\}\overset{1}{\to}\{d_2^1,d_2^2\}\overset{2}{\to}\{d_3^1\}\overset{1}{\to}\{d_2^1,d_3^1\}\overset{1}{\to}\{d_1^1,d_2^1,d_3^1\}\overset{1}{\to}\{d_1^1,d_1^2,d_2^1,d_3^1\}\overset{2}{\to}\{d_2^1,d_2^2,d_3^1\}\overset{2}{\to}\{d_3^1,d_3^2\}\overset{2}{\to}\{d_4^1\},$$ (This sequence might fully exhibit the behaviour I describe, noting that there are two steps where a $D_2$ was spawned, and $4$ where a $D_1$ was spawned.) I am trying to determine the number of seconds it takes on average to form a dragon of type $D_{10}$ assuming that we initially start with no dragons. I have very little experience with probability theory, so my first approach was to simplify this by taking $P_1=1$ and $P_2=0$ , but what I compute is definitely not correct. My approach there was to consider: $$E_n = \{(x,y)\mid x+y=n, x+2y\geq 2^{10}, x,y\in\Bbb Z_{\geq 0}\},$$ where $(x,y)\in E_n$ corresponds to a valid sequence of $n$ spawns that yields a $D_{10}$ dragon, such that there were $x$ spawns of $D_1$ and $y$ spawns of $D_2$ . Then I thought I would only need to take $$S_n=\sum_{(x,y)\in E_n}(.2)^i,$$ for the probability that we have a $D_{10}$ in precisely $n$ steps, and I am then looking for $k$ such that $$\sum_{i=1}^k S_i\approx .5.$$ This led me to make a mistake (after many calculations), and also doesn't deal with the proper fusion rates. A second thought I had would be to set this up in terms of Markov chains, where we simply enumerate all possible sequences $(n_1^t,\dots,n_{10}^t)$ of numbers of dragons $n_i^t$ of type $D_i$ at time step $t$ , and edges corresponding to merging and spawning, but I had trouble setting this up precisely, and even doing so, it seemed that I (personally) can't calculate the resulting probability . Can someone help me solve this problem?","I play a game called DragonSky merge and idle (or something like this). The basic premise of the early game is that dragons will spawn, and will be fused in pairs. You continue to fuse these until you have a level dragon. Let me be precise. Let denote the set of types of dragons. Then, the following occurs: Every seconds a dragon will spawn of type with probability , and of type with probability . For each such that , two dragons of type will be fused to form a dragon of type with probability or a dragon of type with probability . For , they will always fuse to a dragon of type . This merging will occur until there is no pair of dragons of the same type. These two steps will continuously repeat. As a small example of time steps, let us denote a collection of dragons of type by (of course, after fusion ). Then we could have the following sequence of sets of dragons, where means rule was applied (a dragon spawned), and means rule was applied (a pair of dragons was fused). (This sequence might fully exhibit the behaviour I describe, noting that there are two steps where a was spawned, and where a was spawned.) I am trying to determine the number of seconds it takes on average to form a dragon of type assuming that we initially start with no dragons. I have very little experience with probability theory, so my first approach was to simplify this by taking and , but what I compute is definitely not correct. My approach there was to consider: where corresponds to a valid sequence of spawns that yields a dragon, such that there were spawns of and spawns of . Then I thought I would only need to take for the probability that we have a in precisely steps, and I am then looking for such that This led me to make a mistake (after many calculations), and also doesn't deal with the proper fusion rates. A second thought I had would be to set this up in terms of Markov chains, where we simply enumerate all possible sequences of numbers of dragons of type at time step , and edges corresponding to merging and spawning, but I had trouble setting this up precisely, and even doing so, it seemed that I (personally) can't calculate the resulting probability . Can someone help me solve this problem?","10 \{D_1,D_2,\dots, D_{10}\} .9 D_1 .8 D_2 .2 i\in\Bbb N 1\leq i\leq 8 D_i D_{i+1} P_1=.85 D_{i+2} P_2=.15 i=9 D_{10} 6 k D_i d_{i}^1,\dots,d_i^k k\in\{0,1\} \overset{1}{\to} 1 \overset{2}{\to} 2 \emptyset\overset{1}{\to}\{d_1^1\}\overset{1}{\to}\{d_1^1,d_1^2\}\overset{2}{\to}\{d_2^1\}\overset{1}{\to}\{d_2^1,d_2^2\}\overset{2}{\to}\{d_3^1\}\overset{1}{\to}\{d_2^1,d_3^1\}\overset{1}{\to}\{d_1^1,d_2^1,d_3^1\}\overset{1}{\to}\{d_1^1,d_1^2,d_2^1,d_3^1\}\overset{2}{\to}\{d_2^1,d_2^2,d_3^1\}\overset{2}{\to}\{d_3^1,d_3^2\}\overset{2}{\to}\{d_4^1\}, D_2 4 D_1 D_{10} P_1=1 P_2=0 E_n = \{(x,y)\mid x+y=n, x+2y\geq 2^{10}, x,y\in\Bbb Z_{\geq 0}\}, (x,y)\in E_n n D_{10} x D_1 y D_2 S_n=\sum_{(x,y)\in E_n}(.2)^i, D_{10} n k \sum_{i=1}^k S_i\approx .5. (n_1^t,\dots,n_{10}^t) n_i^t D_i t","['probability', 'recreational-mathematics']"
22,Chebyshev's versus Markov's inequality,Chebyshev's versus Markov's inequality,,More important than knowing inequalities by hearts is knowing when and how to apply them and what they express. Regarding Chebyshev's and Markov's inequality. What is the relation (if any) between them? Which one is more strict (and in which situation)? Is there an easy way to understand what they express (kind of like drawing a triangle for the triangle inequality)? What is a typical application in probability theory? What are applications outside of probability theory?,More important than knowing inequalities by hearts is knowing when and how to apply them and what they express. Regarding Chebyshev's and Markov's inequality. What is the relation (if any) between them? Which one is more strict (and in which situation)? Is there an easy way to understand what they express (kind of like drawing a triangle for the triangle inequality)? What is a typical application in probability theory? What are applications outside of probability theory?,,"['probability', 'probability-theory', 'inequality']"
23,Applying law of total probability to conditional probability,Applying law of total probability to conditional probability,,"I was solving problems based on Bayes theorem from the book ""A First Course in Probability by Sheldon Ross"". The problem reads as follows: An insurance company believes that there are two types of people: accident prone and not accident prone. Company statistics states that accident prone person have an accident in any given year with probability $0.4$, whereas the probability is $0.2$ for not-accident prone person. If we assume $30\%$ of population is accident prone, what is the conditional probability that a new policyholder will have an accident in his or her second year of policy ownership, given that the policyholder has had an accident in the first year? The solution given is as follows: Book Solution $$ \begin{align} P(A)=0.3 & & (given)\\   \therefore P(A^c)=1-P(A)=0.7 & & \\ P(A_1|A)=P(A_2|AA_1)=0.4  & &(given)\\ P(A_1|A^c)=P(A_2|A^cA_1)=0.2 & & (given) \end{align} $$   $$ P(A_1)=P(A_1|A)P(A)+P(A_1|A^c)P(A^c) =(.4)(.3)+(.2)(.7)=.26  \\ P(A|A_1)=\frac{(.4)(.3)}{.26}=\frac{6}{13} \\ P(A^c|A_1)=1-P(A|A_1)=\frac{7}{13}  $$   $$ \begin{align} P(A_2|A_1)& =P(A_2|AA_1)P(A|A_1)+P(A_2|A^cA_1)P(A^c|A_1) &&...(I)\\ &=(.4)\frac{6}{13}+(.2)\frac{7}{13}\approx .29\\ \end{align} $$ I dont understand the statement $(I)$. My Solution Shouldnt it be like this:   $$P(A_2|A_1)=P(A_2|AA_1)P(AA_1)+P(A_2|A^cA_1)P(A^cA_1)$$   Continuing further: $$ \begin{align} P(A_2|A_1)&=P(A_2|AA_1)P(A_1|A)P(A)+P(A_2|A^cA_1)P(A_1|A^c)P(A^c)\\ &=(.4)(.4)(.3)+(.2)(.2)(.7)=0.076 \end{align} $$ Am I wrong? If yes, where did I go wrong? Added Later After going through comments and thinking more, it seems that I am struggling to apply law of total probability (and my above solution is very well wrong). The basic form of law of total probability, which I came across till now, is as follows:  $$P(A)=P(A|\color{red}{B})P(\color{red}{B})+P(A|\color{magenta}{B^c})P(\color{magenta}{B^c})$$ I am first time facing application of this law for conditional probability, as done book solution: $$P(A_2|A_1)=P(A_2|AA_1)P(A|A_1)+P(A_2|A^cA_1)P(A_c|A_1)$$  as it involves three events ($A,A_1,A_2$). Book did not explained this. Though in current problem, it looks ""somewhat"" intuitive, can someone generalize it, so as to make my understanding more clear? Say for $n$ events? Also, in $P(A_2|A_1)=P(A_2|\color{red}{AA_1})P(\color{red}{A|A_1})+P(A_2|\color{magenta}{A^cA_1})P(\color{magenta}{A^c|A_1})$, I feel red colored stuff should be same and pink colored stuff should be same, as in case of simple form law of total probability. I felt it should be $P(A_2|\color{red}{(A_1|A)})P(\color{red}{A_1|A})+P(A_2|\color{magenta}{(A_1|A^c)})P(\color{magenta}{A_1|A^c})$. Am I absolutely stupid here? For a moment I felt its related to:$P(E_1E_2E_2...E_n)=P(E_1)P(E_2|E_1)P(E_3|E_1E_2)...P(E_n|E_1...E_{n-1})$. Is it so? I am now screwed at my ability to apply law of total probability. Please enlighten me.","I was solving problems based on Bayes theorem from the book ""A First Course in Probability by Sheldon Ross"". The problem reads as follows: An insurance company believes that there are two types of people: accident prone and not accident prone. Company statistics states that accident prone person have an accident in any given year with probability $0.4$, whereas the probability is $0.2$ for not-accident prone person. If we assume $30\%$ of population is accident prone, what is the conditional probability that a new policyholder will have an accident in his or her second year of policy ownership, given that the policyholder has had an accident in the first year? The solution given is as follows: Book Solution $$ \begin{align} P(A)=0.3 & & (given)\\   \therefore P(A^c)=1-P(A)=0.7 & & \\ P(A_1|A)=P(A_2|AA_1)=0.4  & &(given)\\ P(A_1|A^c)=P(A_2|A^cA_1)=0.2 & & (given) \end{align} $$   $$ P(A_1)=P(A_1|A)P(A)+P(A_1|A^c)P(A^c) =(.4)(.3)+(.2)(.7)=.26  \\ P(A|A_1)=\frac{(.4)(.3)}{.26}=\frac{6}{13} \\ P(A^c|A_1)=1-P(A|A_1)=\frac{7}{13}  $$   $$ \begin{align} P(A_2|A_1)& =P(A_2|AA_1)P(A|A_1)+P(A_2|A^cA_1)P(A^c|A_1) &&...(I)\\ &=(.4)\frac{6}{13}+(.2)\frac{7}{13}\approx .29\\ \end{align} $$ I dont understand the statement $(I)$. My Solution Shouldnt it be like this:   $$P(A_2|A_1)=P(A_2|AA_1)P(AA_1)+P(A_2|A^cA_1)P(A^cA_1)$$   Continuing further: $$ \begin{align} P(A_2|A_1)&=P(A_2|AA_1)P(A_1|A)P(A)+P(A_2|A^cA_1)P(A_1|A^c)P(A^c)\\ &=(.4)(.4)(.3)+(.2)(.2)(.7)=0.076 \end{align} $$ Am I wrong? If yes, where did I go wrong? Added Later After going through comments and thinking more, it seems that I am struggling to apply law of total probability (and my above solution is very well wrong). The basic form of law of total probability, which I came across till now, is as follows:  $$P(A)=P(A|\color{red}{B})P(\color{red}{B})+P(A|\color{magenta}{B^c})P(\color{magenta}{B^c})$$ I am first time facing application of this law for conditional probability, as done book solution: $$P(A_2|A_1)=P(A_2|AA_1)P(A|A_1)+P(A_2|A^cA_1)P(A_c|A_1)$$  as it involves three events ($A,A_1,A_2$). Book did not explained this. Though in current problem, it looks ""somewhat"" intuitive, can someone generalize it, so as to make my understanding more clear? Say for $n$ events? Also, in $P(A_2|A_1)=P(A_2|\color{red}{AA_1})P(\color{red}{A|A_1})+P(A_2|\color{magenta}{A^cA_1})P(\color{magenta}{A^c|A_1})$, I feel red colored stuff should be same and pink colored stuff should be same, as in case of simple form law of total probability. I felt it should be $P(A_2|\color{red}{(A_1|A)})P(\color{red}{A_1|A})+P(A_2|\color{magenta}{(A_1|A^c)})P(\color{magenta}{A_1|A^c})$. Am I absolutely stupid here? For a moment I felt its related to:$P(E_1E_2E_2...E_n)=P(E_1)P(E_2|E_1)P(E_3|E_1E_2)...P(E_n|E_1...E_{n-1})$. Is it so? I am now screwed at my ability to apply law of total probability. Please enlighten me.",,['probability']
24,Summing over conditional probabilities,Summing over conditional probabilities,,"I've seen the following formula being used in various places: $P(a|b) = \sum_z P(a|z) \times P(z|b)$ So essentially they sum over all the variables z to get the original conditional probability. Is this correct, and does it make any strong assumptions? I am confused by the following example: $P(good\ weather | month) = \\ \sum_{z \in \{T,F\}} P(good\ weather| universe\ exists = z) * P(universe\ exists = z| month) = \\ P(good\ weather|universe\ exists = T) $ It starts off with the probability of having good weather, given the current month. We then introduce an additional variable (does the universe exist), and sum over all values (T,F). Let's say that the universe exists with probability 1.0, then: $P(universe\ exists = T| month) = 1.0$ $P(universe\ exists = F| month) = 0.0$ Therefore, we end up with: $P(good\ weather | month) = P(good\ weather | universe\ exists = T)$ Now, somewhere I have clearly lost some important information, because I'm showing that the conditional probability of good weather, depending on the month, is equal to the probability of good weather, given that the universe exists. What am I missing?","I've seen the following formula being used in various places: $P(a|b) = \sum_z P(a|z) \times P(z|b)$ So essentially they sum over all the variables z to get the original conditional probability. Is this correct, and does it make any strong assumptions? I am confused by the following example: $P(good\ weather | month) = \\ \sum_{z \in \{T,F\}} P(good\ weather| universe\ exists = z) * P(universe\ exists = z| month) = \\ P(good\ weather|universe\ exists = T) $ It starts off with the probability of having good weather, given the current month. We then introduce an additional variable (does the universe exist), and sum over all values (T,F). Let's say that the universe exists with probability 1.0, then: $P(universe\ exists = T| month) = 1.0$ $P(universe\ exists = F| month) = 0.0$ Therefore, we end up with: $P(good\ weather | month) = P(good\ weather | universe\ exists = T)$ Now, somewhere I have clearly lost some important information, because I'm showing that the conditional probability of good weather, depending on the month, is equal to the probability of good weather, given that the universe exists. What am I missing?",,['probability']
25,"Expected value of $1+1+1+\cdots+1$ (with $n$ ""$1$""s) if each '$+$' is independently deleted with probability $1/2$","Expected value of  (with  """"s) if each '' is independently deleted with probability",1+1+1+\cdots+1 n 1 + 1/2,"Problem from an old exam: We have a sequence of symbols $1+1+1+1+1+\cdots+1+1$ , consisting of $n$ ones and $n-1$ plus signs. Each of the plus signs is independently deleted with a probability of $\frac{1}{2}$ . Let $X$ be the value of the resulting expression. For example, if $n=5$ and we delete all the plus signs except the second one, then $X=11+111=122$ . (a) Calculate $E(X)$ . (b) Provide a formula for $\operatorname{Var}(X)$ . It suffices to provide a formula that is a sum of a polynomial number of terms in compact form (such a sum does not need to be calculated). For $n = 2$ , the expected value should be $ \frac{11+2}{2} = 6.5 $ , for $n = 3$ it should be $\frac{3+12+12+111}{4} = 34.5 $ , for $n = 4$ it should be $\frac{4+13+13+13+22+112+112+1111}{8} = 175 $ . The only thing that comes to my mind is something like $\mathbb{E}[X] = \sum_{i=1}^{n} \mathbb{E}[X_i],$ where $\mathbb{E}[X_i] = 1 + \sum_{j=1}^{i-1}10^j\cdot\frac{1}{2^j}.$ But this obviously gives results that are too large. How can I approach it?","Problem from an old exam: We have a sequence of symbols , consisting of ones and plus signs. Each of the plus signs is independently deleted with a probability of . Let be the value of the resulting expression. For example, if and we delete all the plus signs except the second one, then . (a) Calculate . (b) Provide a formula for . It suffices to provide a formula that is a sum of a polynomial number of terms in compact form (such a sum does not need to be calculated). For , the expected value should be , for it should be , for it should be . The only thing that comes to my mind is something like where But this obviously gives results that are too large. How can I approach it?","1+1+1+1+1+\cdots+1+1 n n-1 \frac{1}{2} X n=5 X=11+111=122 E(X) \operatorname{Var}(X) n = 2  \frac{11+2}{2} = 6.5  n = 3 \frac{3+12+12+111}{4} = 34.5  n = 4 \frac{4+13+13+13+22+112+112+1111}{8} = 175  \mathbb{E}[X] = \sum_{i=1}^{n} \mathbb{E}[X_i], \mathbb{E}[X_i] = 1 + \sum_{j=1}^{i-1}10^j\cdot\frac{1}{2^j}.","['probability', 'combinatorics']"
26,Probability vs Confidence,Probability vs Confidence,,"My notes on confidence give this question: An investigator is interested in the amount of time internet users spend watching TV a week. He assumes $\sigma = 3.5$ hours and samples $n=50$ users and takes the sample mean to estimate the population mean $\mu$ Since $n=50$ is large we know that $\frac{\bar{X}-\mu}{\sigma/\sqrt{n}}$ approximates the Standard Normal. So, with probability $\alpha = 0.99$ , the maximum error of estimate is $E = z_{\alpha/2} \times \frac{\sigma}{\sqrt{n}} \approx 1.27$ hours. The investigator collects that data and obtain $\bar{X}=11.5$ hours. Can he still assert with 99% probability that the error is at most 1.27 hours? With the answer that: No he cannot, because the probability describes the method/estimator, not the result . We say that ""we conclude with 99% confidence that the error does not exceed 1.27 hours."" I am confused. What is this difference between probability and confidence? Is it related to confidence intervals? Is there an intuitive explanation for the difference?","My notes on confidence give this question: An investigator is interested in the amount of time internet users spend watching TV a week. He assumes hours and samples users and takes the sample mean to estimate the population mean Since is large we know that approximates the Standard Normal. So, with probability , the maximum error of estimate is hours. The investigator collects that data and obtain hours. Can he still assert with 99% probability that the error is at most 1.27 hours? With the answer that: No he cannot, because the probability describes the method/estimator, not the result . We say that ""we conclude with 99% confidence that the error does not exceed 1.27 hours."" I am confused. What is this difference between probability and confidence? Is it related to confidence intervals? Is there an intuitive explanation for the difference?",\sigma = 3.5 n=50 \mu n=50 \frac{\bar{X}-\mu}{\sigma/\sqrt{n}} \alpha = 0.99 E = z_{\alpha/2} \times \frac{\sigma}{\sqrt{n}} \approx 1.27 \bar{X}=11.5,"['probability', 'statistics', 'statistical-inference', 'estimation', 'confidence-interval']"
27,What is the difference between a probability distribution on events and random variables?,What is the difference between a probability distribution on events and random variables?,,"For the purpose of simplicity, assume everything below is only in the discrete domain. A $\text{probability space}$ is usually defined as a triple $(\Omega , 2^\Omega , P)$ where $\Omega := \text{set of outcomes}$ $2^\Omega:= \text{set of events (for simplicity)}$ $P:= \text{mapping from } 2^\Omega \mapsto [0,1] \text{ which assigns a probability to each event}$ My first question is whether $P$ is a probability distribution or a probability measure , and what exactly is the difference between these two ideas. Moreover, I am trying to understand where the concept of a $\text{random variable}$ fits in with all this. From my reading a random variable (over the probability space above) $X:= \text{mapping from } \Omega \mapsto \mathbb{R}$. When we say $P(X=a)$, is this $P$ referring to the same probability distribution of the space? What exactly is the object $X=a$? I would think it refers to the set of all outcomes in $\Omega$ that map to $a$ but my reading does not elucidate this. What is the ultimate confusion is the statement: $\text{we will use the notation} P(X) \text{ to denote the distribution of the random variable X}$ Again, what exactly is meant by the probability distribution $P$ here? Does it also refer to the $P$ of the probability space that $X$ is defined over? How is this distribution different?","For the purpose of simplicity, assume everything below is only in the discrete domain. A $\text{probability space}$ is usually defined as a triple $(\Omega , 2^\Omega , P)$ where $\Omega := \text{set of outcomes}$ $2^\Omega:= \text{set of events (for simplicity)}$ $P:= \text{mapping from } 2^\Omega \mapsto [0,1] \text{ which assigns a probability to each event}$ My first question is whether $P$ is a probability distribution or a probability measure , and what exactly is the difference between these two ideas. Moreover, I am trying to understand where the concept of a $\text{random variable}$ fits in with all this. From my reading a random variable (over the probability space above) $X:= \text{mapping from } \Omega \mapsto \mathbb{R}$. When we say $P(X=a)$, is this $P$ referring to the same probability distribution of the space? What exactly is the object $X=a$? I would think it refers to the set of all outcomes in $\Omega$ that map to $a$ but my reading does not elucidate this. What is the ultimate confusion is the statement: $\text{we will use the notation} P(X) \text{ to denote the distribution of the random variable X}$ Again, what exactly is meant by the probability distribution $P$ here? Does it also refer to the $P$ of the probability space that $X$ is defined over? How is this distribution different?",,"['probability', 'probability-distributions', 'notation']"
28,Bell numbers and moments of the Poisson distribution,Bell numbers and moments of the Poisson distribution,,"Using generating functions one can see that the $n^{th}$ Bell number, i.e., the number of all possible partitions of a set of $n$ elements, is equal to $E(X^n)$ where $X$ is a Poisson random variable with mean 1. Is there a way to explain this connection intuitively?","Using generating functions one can see that the $n^{th}$ Bell number, i.e., the number of all possible partitions of a set of $n$ elements, is equal to $E(X^n)$ where $X$ is a Poisson random variable with mean 1. Is there a way to explain this connection intuitively?",,"['combinatorics', 'probability']"
29,What is the difference between sample space and event space?,What is the difference between sample space and event space?,,"I am a little confused about the difference between sample space and event space. After reading some information, I want to take an example. If I am wrong, please correct me. Sample space: all possible outcomes Event: a subset of the sample space Event space: all events For a fair die: Sample space: ${(1, 2, 3, 4, 5, 6)}$ Event: $(1)$ or $(2)$ or $(3)$ or $(4)$ or $(5)$ or $(6)$ or $(1,2)$ or $(1,3)$ or $(1,4)$ or $(1,5)$ or $(1,6)$ or $(2,3)$ or $(2,4)$ or $(2,5)$ or $(2,6)$ or $(3,4)$ or $(3,5)$ or $(3,6)$ or $(4,5)$ or $(4,6)$ or $(5,6)$ or $(1,2,3)$ or $(1,2,4)$ or $(1,2,5)$ or $(1,2,6)$ ,etc. Event space: all events Although the event is the subset of sample space, and event space is the all events. The sample space is actually not the same as event space, right?","I am a little confused about the difference between sample space and event space. After reading some information, I want to take an example. If I am wrong, please correct me. Sample space: all possible outcomes Event: a subset of the sample space Event space: all events For a fair die: Sample space: Event: or or or or or or or or or or or or or or or or or or or or or or or or ,etc. Event space: all events Although the event is the subset of sample space, and event space is the all events. The sample space is actually not the same as event space, right?","{(1, 2, 3, 4, 5, 6)} (1) (2) (3) (4) (5) (6) (1,2) (1,3) (1,4) (1,5) (1,6) (2,3) (2,4) (2,5) (2,6) (3,4) (3,5) (3,6) (4,5) (4,6) (5,6) (1,2,3) (1,2,4) (1,2,5) (1,2,6)","['probability', 'probability-theory']"
30,Proof for convergence in distribution implying convergence in probability for constants,Proof for convergence in distribution implying convergence in probability for constants,,"I'm trying to understand this proof (also in the image below) that proves if $X_{n}$ converges to some constant $c$ in distribution, then this implies it converges in probability too. Specifically, my questions about the proof are: How are they getting $\lim_{n \to \infty} F_{X_{n}}(c+\frac{\epsilon}{2}) = 1$ ? Why do they state the conclusion at the end in this way? They're basically saying that knowing $\lim_{n \to \infty}P(|X_{n} - c| \geq \epsilon) \geq 0$ allow you to conclude that $\lim_{n \to \infty}P(|X_{n} - c| \geq \epsilon) = 0$ but the real reason we can conclude this is because of the whole body of the proof above, right?","I'm trying to understand this proof (also in the image below) that proves if converges to some constant in distribution, then this implies it converges in probability too. Specifically, my questions about the proof are: How are they getting ? Why do they state the conclusion at the end in this way? They're basically saying that knowing allow you to conclude that but the real reason we can conclude this is because of the whole body of the proof above, right?",X_{n} c \lim_{n \to \infty} F_{X_{n}}(c+\frac{\epsilon}{2}) = 1 \lim_{n \to \infty}P(|X_{n} - c| \geq \epsilon) \geq 0 \lim_{n \to \infty}P(|X_{n} - c| \geq \epsilon) = 0,"['probability', 'weak-convergence']"
31,Magic 8 Ball Problem,Magic 8 Ball Problem,,"This problem is probably simple enough to have an analogous problem, I just don't know the name so I'm going to describe it and hopefully somebody can point me in the right direction. The problem is this: estimating the number of sides of a Magic 8 Ball . Let's say you perform the following process: Shake the ""Magic 8 Ball"" and mark down the result If the result has not previously been seen, add it to the set of possible results and label the trial ""N"" (for ""new"") If the result has been previously seen, label the trial ""O"" (for ""old"") Repeat all steps I imagine the following kind of sequence occurring: NNNNONNOONOONOOONOOOONONNOONOOONNOONOOOOOONOOONOOOOOOONOOOOOOOONOOOOOOOO... Now imagine we don't know there are twenty sides to a Magic 8 Ball. Or imagine that you have a Magic 8 Ball with 1000 sides. As the number of ""new"" trials approaches the actual number of sides, we'll get increasingly more ""old"" trials showing up in the mix. Once we've seen all of the possible results, we'll always get ""old"" results. But we're never 100% sure we've seen every single possible result. So here are the questions I'm interested in: As we proceed with trials, can we estimate the total number of ""sides"" of the Magic 8 ball based on the number of ""new"" and ""old"" trials up to this point? Can we calculate a probability that our current estimate is correct, or a probability that some bounded estimate is correct?","This problem is probably simple enough to have an analogous problem, I just don't know the name so I'm going to describe it and hopefully somebody can point me in the right direction. The problem is this: estimating the number of sides of a Magic 8 Ball . Let's say you perform the following process: Shake the ""Magic 8 Ball"" and mark down the result If the result has not previously been seen, add it to the set of possible results and label the trial ""N"" (for ""new"") If the result has been previously seen, label the trial ""O"" (for ""old"") Repeat all steps I imagine the following kind of sequence occurring: NNNNONNOONOONOOONOOOONONNOONOOONNOONOOOOOONOOONOOOOOOONOOOOOOOONOOOOOOOO... Now imagine we don't know there are twenty sides to a Magic 8 Ball. Or imagine that you have a Magic 8 Ball with 1000 sides. As the number of ""new"" trials approaches the actual number of sides, we'll get increasingly more ""old"" trials showing up in the mix. Once we've seen all of the possible results, we'll always get ""old"" results. But we're never 100% sure we've seen every single possible result. So here are the questions I'm interested in: As we proceed with trials, can we estimate the total number of ""sides"" of the Magic 8 ball based on the number of ""new"" and ""old"" trials up to this point? Can we calculate a probability that our current estimate is correct, or a probability that some bounded estimate is correct?",,['probability']
32,Log likelihood of a realization of a Poisson process?,Log likelihood of a realization of a Poisson process?,,"For an inhomogeneous Poisson process with instantaneous rate $\lambda(t)$, the log likelihood of observing events at times $t_1,\ldots,t_n$ in the time interval $[0,T)$ is given by $ \sum_i \mathrm{log}\lambda(t_i) - \int_0^T \lambda(t) dt$ I am told this can be derived by taking the limit of the discrete-time case as the bin width $\Delta t$ goes to $0$: $ \sum_i \mathrm{log}(\lambda(t_i)\Delta t) + \sum_{t\notin \{t_1,\ldots, t_n\}} \mathrm{log}(1-\lambda(t) \Delta t)$ For the second term, it is clear how this limit works if we take the Taylor expansion: $\mathrm{log}(1-\epsilon) \approx -\epsilon$ However for the first term, it is not clear to me how one goes from $\sum_i \mathrm{log}\lambda(t_i) + \mathrm{log} \Delta t$ to $\sum_i\mathrm{log}\lambda(t_i)$ as $\Delta t \rightarrow 0$.  Shouldn't the $\mathrm{log}\Delta t$ terms go to $-\infty$? Edit I think I understand now where my confusion was coming from. If we look at the probability, instead of log probability, then as $\Delta t$ becomes small the probability approaches: $ \Delta t^n \prod_{i=1}^n \lambda(t_i) \mathrm{exp}\left(-\int_0^T\lambda(t)dt\right)$ The probability of an event happening in the interval: $[t_1 - \Delta t/2, t_1 + \Delta t/2] \times [t_2 - \Delta t/2, t_2 + \Delta t/2] \times \ldots \times [t_n - \Delta t/2, t_n + \Delta t/2]$ should scale as $\Delta t^n$ for small $\Delta t$, so while the probability of events happening at exactly those times goes to zero (as it must), the density does not.","For an inhomogeneous Poisson process with instantaneous rate $\lambda(t)$, the log likelihood of observing events at times $t_1,\ldots,t_n$ in the time interval $[0,T)$ is given by $ \sum_i \mathrm{log}\lambda(t_i) - \int_0^T \lambda(t) dt$ I am told this can be derived by taking the limit of the discrete-time case as the bin width $\Delta t$ goes to $0$: $ \sum_i \mathrm{log}(\lambda(t_i)\Delta t) + \sum_{t\notin \{t_1,\ldots, t_n\}} \mathrm{log}(1-\lambda(t) \Delta t)$ For the second term, it is clear how this limit works if we take the Taylor expansion: $\mathrm{log}(1-\epsilon) \approx -\epsilon$ However for the first term, it is not clear to me how one goes from $\sum_i \mathrm{log}\lambda(t_i) + \mathrm{log} \Delta t$ to $\sum_i\mathrm{log}\lambda(t_i)$ as $\Delta t \rightarrow 0$.  Shouldn't the $\mathrm{log}\Delta t$ terms go to $-\infty$? Edit I think I understand now where my confusion was coming from. If we look at the probability, instead of log probability, then as $\Delta t$ becomes small the probability approaches: $ \Delta t^n \prod_{i=1}^n \lambda(t_i) \mathrm{exp}\left(-\int_0^T\lambda(t)dt\right)$ The probability of an event happening in the interval: $[t_1 - \Delta t/2, t_1 + \Delta t/2] \times [t_2 - \Delta t/2, t_2 + \Delta t/2] \times \ldots \times [t_n - \Delta t/2, t_n + \Delta t/2]$ should scale as $\Delta t^n$ for small $\Delta t$, so while the probability of events happening at exactly those times goes to zero (as it must), the density does not.",,"['probability', 'stochastic-processes']"
33,Characteristic function of exponential and geometric distributions,Characteristic function of exponential and geometric distributions,,"I'm trying to derive the characteristic function for exponential distribution and geometric distribution. Can you guide me on getting them? Here is my solution so far: Characteristic function of Exponential distribution: $\phi(t) = E[e^{itX}]$ where $X$ has exponential distribution , then by definition, expectation can be written as: $$ \int_0^{\infty} e^{itx}  \lambda e^{-\lambda x } dx = \frac{\lambda}{it - \lambda} e^{(it - \lambda)x}\bigg|_0^{\infty} = \frac{\lambda}{ \lambda- it}$$ For the geometric random variables, assume $\Pr(X = 0) = P$ , how can we find the characteristic function of $X$ ? Here, since $X$ is a  discrete random variable, we have: $\phi(t) =  E[e^{itX}] = \sum_{j = 0}^{\infty} e^{itj} (1 - P)^j P$ my issue is I don't know how to get a closed form for the characteristic function? Thanks for your help.","I'm trying to derive the characteristic function for exponential distribution and geometric distribution. Can you guide me on getting them? Here is my solution so far: Characteristic function of Exponential distribution: where has exponential distribution , then by definition, expectation can be written as: For the geometric random variables, assume , how can we find the characteristic function of ? Here, since is a  discrete random variable, we have: my issue is I don't know how to get a closed form for the characteristic function? Thanks for your help.",\phi(t) = E[e^{itX}] X  \int_0^{\infty} e^{itx}  \lambda e^{-\lambda x } dx = \frac{\lambda}{it - \lambda} e^{(it - \lambda)x}\bigg|_0^{\infty} = \frac{\lambda}{ \lambda- it} \Pr(X = 0) = P X X \phi(t) =  E[e^{itX}] = \sum_{j = 0}^{\infty} e^{itj} (1 - P)^j P,['probability']
34,The Probabilistic Pigeon Hole Principle,The Probabilistic Pigeon Hole Principle,,"Many people are aware of the Pigeonhole Principle: If we distribute $n+1$ pigeons into $n$ pigeonholes, at least one hole will contain at least two pigeons. However, much fewer are aware of the $\bf{Probabilistic}$ Pigeonhole Principle, which answers the question, 'How many do we usually need?' Those familiar with the famous birthday problem might have good intuition for this. Let's recall the birthday problem: How many (randomly chosen) people do you need in a room for the probability that some two share a birthday to be more than 1/2? Wikipedia has a nice article on this here . (If this is the first time you've heard the question, it's a great one to think about.) For most  people, the answer is quite a surprise, as it is only 23. Returning to the general problem, if we randomly distribute $n^{1/2}$ pigeons into $n$ pigeonholes, the probability of some overlap approaches 1, as $n \to \infty$. Let's state that more precisely: Take any $\epsilon > 0.$ Randomly place $\lfloor n^{1/2 + \epsilon} \rfloor$ pigeons into $n$ holes. (Each is placed independently of the others and with uniform distribution.) Let $\Pr{(n,\epsilon)}$ be the probability that at least two pigeons are placed in the same hole. Then we have,  $$\lim_{n \to \infty} \Pr{(n,\epsilon)} = 1.$$  I'm having some trouble proving this, and I also haven't found a proof online. In any case, the proof itself should not be difficult. Here is what I have so far: Let $m = \lfloor n^{1/2 + \epsilon} \rfloor$. Then we have, $$\Pr{(n,\epsilon)} = 1 - \frac{(n)_m}{n^m},$$ where $(n)_m$ is the falling factorial. i.e. $(n)_m = n(n-1)\times \ldots \times (n-(m-1))$. I tried applying Stirling's formula but after doing some algebraic manipulatoin, I couldn't quite finish the proof. (I also crunched some numbers with Python, providing some numerical evidence.) So, what would be a proof of this? Here is an extra note on my background to the question. I first became acquainted with it at a math conference when two people, (Piotr Przytycki and another person, whose name I don't recall), gave a short introduction to random (finitely presented) groups. The above principle is used to prove part of a basic result. You can read it in the literature on the bottom of page 31 of the excellent 2005 survey article (on random groups) which Yann Ollivier wrote.","Many people are aware of the Pigeonhole Principle: If we distribute $n+1$ pigeons into $n$ pigeonholes, at least one hole will contain at least two pigeons. However, much fewer are aware of the $\bf{Probabilistic}$ Pigeonhole Principle, which answers the question, 'How many do we usually need?' Those familiar with the famous birthday problem might have good intuition for this. Let's recall the birthday problem: How many (randomly chosen) people do you need in a room for the probability that some two share a birthday to be more than 1/2? Wikipedia has a nice article on this here . (If this is the first time you've heard the question, it's a great one to think about.) For most  people, the answer is quite a surprise, as it is only 23. Returning to the general problem, if we randomly distribute $n^{1/2}$ pigeons into $n$ pigeonholes, the probability of some overlap approaches 1, as $n \to \infty$. Let's state that more precisely: Take any $\epsilon > 0.$ Randomly place $\lfloor n^{1/2 + \epsilon} \rfloor$ pigeons into $n$ holes. (Each is placed independently of the others and with uniform distribution.) Let $\Pr{(n,\epsilon)}$ be the probability that at least two pigeons are placed in the same hole. Then we have,  $$\lim_{n \to \infty} \Pr{(n,\epsilon)} = 1.$$  I'm having some trouble proving this, and I also haven't found a proof online. In any case, the proof itself should not be difficult. Here is what I have so far: Let $m = \lfloor n^{1/2 + \epsilon} \rfloor$. Then we have, $$\Pr{(n,\epsilon)} = 1 - \frac{(n)_m}{n^m},$$ where $(n)_m$ is the falling factorial. i.e. $(n)_m = n(n-1)\times \ldots \times (n-(m-1))$. I tried applying Stirling's formula but after doing some algebraic manipulatoin, I couldn't quite finish the proof. (I also crunched some numbers with Python, providing some numerical evidence.) So, what would be a proof of this? Here is an extra note on my background to the question. I first became acquainted with it at a math conference when two people, (Piotr Przytycki and another person, whose name I don't recall), gave a short introduction to random (finitely presented) groups. The above principle is used to prove part of a basic result. You can read it in the literature on the bottom of page 31 of the excellent 2005 survey article (on random groups) which Yann Ollivier wrote.",,"['probability', 'limits', 'pigeonhole-principle']"
35,What is the difference between a random vector and a stochastic process?,What is the difference between a random vector and a stochastic process?,,"I am a little confused about random vectors and stochastic processes. I read their definitions in  Wikipedia ( random vector , stochastic process ) and I cannot understand their differences . I would appreciate your help. Thanks.","I am a little confused about random vectors and stochastic processes. I read their definitions in  Wikipedia ( random vector , stochastic process ) and I cannot understand their differences . I would appreciate your help. Thanks.",,"['probability', 'statistics', 'probability-theory', 'stochastic-processes', 'random-variables']"
36,Maximum likelihood estimate of hypergeometric distribution parameter,Maximum likelihood estimate of hypergeometric distribution parameter,,"Using the notation in the Wikipedia article on the hypergeometric distribution , I'm curious how one would obtain the maximum likelihood estimate for parameter $m$ , the number of white marbles, given $T$ trials from the same urn. For convenience, I'll copy/paste the notation from the article: Suppose you are to draw $n$ marbles without replacement from an urn containing $N$ marbles in total, $m$ of which are white. The hypergeometric distribution describes the distribution of the number of white marbles drawn from the urn, $k$ . Again, assuming I conduct $T$ trials, at each trial, I take $n$ balls from the urn, and $k_i$ is the number of white balls at trial $i$ . Define $K = (k_1,\ldots,k_T)$ . Then the likelihood function $L$ : $$L(m; K, N, n) = \prod_i^T \frac{\binom{m}{k_i}\binom{N-m}{n-k_i}}{\binom{N}{n}}$$ Taking a hint from this post , I first tried to solve the inequality: $$L(m;K,N,n) \geq L(m-1;K,N,n)$$ when $T=1$ . From this I obtained $$m \leq \frac{Nk+k}{n}$$ so the MLE should be $$m = \left\lfloor \frac{Nk+k}{n} \right\rfloor$$ Now, I'm stuck when I try to generalize to $T \geq 2$ . I first tried doing the same as above and I ended up with the following unwieldy inequality: $$\prod_i^T \frac{m}{m-k_i} \geq \prod_i^T \frac{N-m+1}{N-m-n+k_i+1}$$ which I'm not sure how to solve. Then I tried to take the log of the likelihood and differentiate as if $m$ were defined over positive reals and I ended up with an equally unwieldy equation to solve: $$\sum_i^T \left(\Psi(m+1) - \Psi(m-k_i+1) - \Psi(N-m+1) + \Psi(N-m-n+k_i+1)\right) = 0$$ where $\Psi$ is the digamma function (i.e. the derivative of the log-gamma function). My intuition tells me the solution to either of the above would look something like this: $$m = \left\lfloor \frac{(N+1)\sum_i^T k_i}{Tn} \right\rfloor$$ but I have no idea how to get here. The motivation for this problem is pure curiosity, since I've never seen a MLE for the hypergeometric distribution in terms of $m$ .","Using the notation in the Wikipedia article on the hypergeometric distribution , I'm curious how one would obtain the maximum likelihood estimate for parameter , the number of white marbles, given trials from the same urn. For convenience, I'll copy/paste the notation from the article: Suppose you are to draw marbles without replacement from an urn containing marbles in total, of which are white. The hypergeometric distribution describes the distribution of the number of white marbles drawn from the urn, . Again, assuming I conduct trials, at each trial, I take balls from the urn, and is the number of white balls at trial . Define . Then the likelihood function : Taking a hint from this post , I first tried to solve the inequality: when . From this I obtained so the MLE should be Now, I'm stuck when I try to generalize to . I first tried doing the same as above and I ended up with the following unwieldy inequality: which I'm not sure how to solve. Then I tried to take the log of the likelihood and differentiate as if were defined over positive reals and I ended up with an equally unwieldy equation to solve: where is the digamma function (i.e. the derivative of the log-gamma function). My intuition tells me the solution to either of the above would look something like this: but I have no idea how to get here. The motivation for this problem is pure curiosity, since I've never seen a MLE for the hypergeometric distribution in terms of .","m T n N m k T n k_i i K = (k_1,\ldots,k_T) L L(m; K, N, n) = \prod_i^T \frac{\binom{m}{k_i}\binom{N-m}{n-k_i}}{\binom{N}{n}} L(m;K,N,n) \geq L(m-1;K,N,n) T=1 m \leq \frac{Nk+k}{n} m = \left\lfloor \frac{Nk+k}{n} \right\rfloor T \geq 2 \prod_i^T \frac{m}{m-k_i} \geq \prod_i^T \frac{N-m+1}{N-m-n+k_i+1} m \sum_i^T \left(\Psi(m+1) - \Psi(m-k_i+1) - \Psi(N-m+1) + \Psi(N-m-n+k_i+1)\right) = 0 \Psi m = \left\lfloor \frac{(N+1)\sum_i^T k_i}{Tn} \right\rfloor m","['probability', 'probability-distributions']"
37,Probability distribution of the maximum of random variables,Probability distribution of the maximum of random variables,,"Given $N$ random iid variables, $X_1, \ldots, X_N$, with a uniform probability distribution on $[0, 1)$ what is the distribution of $\displaystyle \max_{i = 1 \ldots N}(X_i)$?","Given $N$ random iid variables, $X_1, \ldots, X_N$, with a uniform probability distribution on $[0, 1)$ what is the distribution of $\displaystyle \max_{i = 1 \ldots N}(X_i)$?",,['probability']
38,Probability that THHT occurs in a sequence of 10 coin tosses,Probability that THHT occurs in a sequence of 10 coin tosses,,"Assume we have a fair coin, and we throw the coin $10$ times in a row. I want to calculate the probability that the sequence 'tail, head, head, tail' occurs. So I think I can interpret this event as a binary number with $10$ digits. So $1$ means tail, $0$ means head. Therefore we have $2^{10} = 1024$ different outcomes of the $10$ throws. The sequence  'tail, head, head, tail' can start at $7$ different positions and so there are $7\cdot2^6 = 448$ different outcomes of the $10$ throws with the sequence 'tail, head, head, tail'. So the probability would be $\frac{448}{1024} = 0.4375$ . But I have a feeling there's something wrong?","Assume we have a fair coin, and we throw the coin times in a row. I want to calculate the probability that the sequence 'tail, head, head, tail' occurs. So I think I can interpret this event as a binary number with digits. So means tail, means head. Therefore we have different outcomes of the throws. The sequence  'tail, head, head, tail' can start at different positions and so there are different outcomes of the throws with the sequence 'tail, head, head, tail'. So the probability would be . But I have a feeling there's something wrong?",10 10 1 0 2^{10} = 1024 10 7 7\cdot2^6 = 448 10 \frac{448}{1024} = 0.4375,"['probability', 'combinatorics']"
39,Probability that a random graph is planar,Probability that a random graph is planar,,"I've been attempting to solve the following challenge problem from a combinatorics class but am getting absolutely nowhere. Prove: For sufficiently large $n$, the probability a random graph $G=(V,E)$ with $n$ vertices is planar is less than $2^{-0.49n^2}$. I've ruled out using Kuratowski's characterization of planarity for proving this (since I know it's hard to determine whether or not a subgraph $G' \subset G$ is a topological $K_{3,3}$ or $K_5$. The other characterization I'm aware of is if a graph is planar, then $\exists v \in V$ such that $deg[v] \leq 5$. So the probability a random graph is planar is equivalent to $1-\mathbb{P}$ ,where $\mathbb{P} = $ the probability that a random graph has no vertices of degree less than or equal to 5. However, I can't see how to proceed from this observation. Solutions/hints would be appreciated.","I've been attempting to solve the following challenge problem from a combinatorics class but am getting absolutely nowhere. Prove: For sufficiently large $n$, the probability a random graph $G=(V,E)$ with $n$ vertices is planar is less than $2^{-0.49n^2}$. I've ruled out using Kuratowski's characterization of planarity for proving this (since I know it's hard to determine whether or not a subgraph $G' \subset G$ is a topological $K_{3,3}$ or $K_5$. The other characterization I'm aware of is if a graph is planar, then $\exists v \in V$ such that $deg[v] \leq 5$. So the probability a random graph is planar is equivalent to $1-\mathbb{P}$ ,where $\mathbb{P} = $ the probability that a random graph has no vertices of degree less than or equal to 5. However, I can't see how to proceed from this observation. Solutions/hints would be appreciated.",,"['probability', 'combinatorics', 'graph-theory', 'random-graphs']"
40,"Make $2$ cubes out of $1729$ unit cubes, expected number of times you have to paint","Make  cubes out of  unit cubes, expected number of times you have to paint",2 1729,"I'm trying to solve question 6 from the PuMac 2007 Combinatorics A competition: Joe has $1729$ randomly oriented randomly arranged unit cubes, which are initially unpainted. He makes two cubes of sidelengths $9$ and $10$ or of sidelengths $1$ and $12$ (randomly chosen). These cubes are dipped into white paint. Then two more cubes of sidelengths $1$ and $12$ or $9$ and $10$ are formed from the same unit cubes, again randomly oriented and randomly arranged, and dipped into paint. Joe continues this process until every side of every unit cube is painted. After how many times of doing this is the expected number of painted faces closest to half of the total? Here's what I got so far: ${1\over2}$ chance of this happening: If you make two cubes of side lengths $9$ and $10$ , then $16$ cubes will have $3$ faces sharing a vertex painted, $12(8 + 7) = 180$ cubes will have $2$ faces sharing an edge painted, $6(8^2 + 7^2) = 678$ cubes will have $1$ face painted, and the remaining $7^3 + 8^3 = 855$ cubes will have no faces painted. ${1\over2}$ chance of this happening: If you make two cubes of side lengths $1$ and $12$ , then $1$ cube will have all $6$ faces painted, $8$ cubes will have $3$ faces sharing a vertex painted, $12(10) = 120$ cubes will have $2$ faces sharing an edge painted, $6(10^2) = 600$ cubes will have $1$ face painted, and the remaining $10^3 = 1000$ cubes will have no faces painted. But I'm stuck as this point, and don't know what to do next. Any help would be well-appreciated.","I'm trying to solve question 6 from the PuMac 2007 Combinatorics A competition: Joe has randomly oriented randomly arranged unit cubes, which are initially unpainted. He makes two cubes of sidelengths and or of sidelengths and (randomly chosen). These cubes are dipped into white paint. Then two more cubes of sidelengths and or and are formed from the same unit cubes, again randomly oriented and randomly arranged, and dipped into paint. Joe continues this process until every side of every unit cube is painted. After how many times of doing this is the expected number of painted faces closest to half of the total? Here's what I got so far: chance of this happening: If you make two cubes of side lengths and , then cubes will have faces sharing a vertex painted, cubes will have faces sharing an edge painted, cubes will have face painted, and the remaining cubes will have no faces painted. chance of this happening: If you make two cubes of side lengths and , then cube will have all faces painted, cubes will have faces sharing a vertex painted, cubes will have faces sharing an edge painted, cubes will have face painted, and the remaining cubes will have no faces painted. But I'm stuck as this point, and don't know what to do next. Any help would be well-appreciated.",1729 9 10 1 12 1 12 9 10 {1\over2} 9 10 16 3 12(8 + 7) = 180 2 6(8^2 + 7^2) = 678 1 7^3 + 8^3 = 855 {1\over2} 1 12 1 6 8 3 12(10) = 120 2 6(10^2) = 600 1 10^3 = 1000,"['probability', 'combinatorics', 'algebra-precalculus', 'contest-math', 'expected-value']"
41,"The ""true"" domain of random variables","The ""true"" domain of random variables",,"Typically in applied probabilistic or statical literature we work with random variables whose domain we don't specify. We just care about the set in which the random variables takes values. For example, the number of aces in hand at a certain cardgame, the height of a population or the income of a company in a certain year are all random variables (the last two examples come from statistics). But in all of these examples, the domain is never given. While we could always construct any number of artificial probability spaces, that would serve as domain,  I'm interested in what a  ̶""̶t̶r̶u̶e̶""  compelling probability space domain could be that really models (the underlying the experiment of) these three examples? EDIT To prevent unclarity with what I mean by ""compelling"". Let me be more precise by giving an example: Consider the random variable that counts the number of heads when flipping a coin $n$ times. Thus it takes values from $0,1,\ldots,n$. But which experiment would most likely be perform in order to lead to these values? The most complelling space $\Omega$ would be $\Omega=\{ H,T\}^n$, the space of sequences of $n$ coin flipping, since this is what actually happens. But one could just as well define this random variable on the set $\{0,1,…,n\}$, in which case the random variable would be the identity function. This space I would call artificial, not ""compelling"", because it doesn't give an accurate representation of the underlying experiment any more. In particular I'm interested in the underlying space for the statistical examples. P.S. See also this other question of mine, which also has a bounty running.","Typically in applied probabilistic or statical literature we work with random variables whose domain we don't specify. We just care about the set in which the random variables takes values. For example, the number of aces in hand at a certain cardgame, the height of a population or the income of a company in a certain year are all random variables (the last two examples come from statistics). But in all of these examples, the domain is never given. While we could always construct any number of artificial probability spaces, that would serve as domain,  I'm interested in what a  ̶""̶t̶r̶u̶e̶""  compelling probability space domain could be that really models (the underlying the experiment of) these three examples? EDIT To prevent unclarity with what I mean by ""compelling"". Let me be more precise by giving an example: Consider the random variable that counts the number of heads when flipping a coin $n$ times. Thus it takes values from $0,1,\ldots,n$. But which experiment would most likely be perform in order to lead to these values? The most complelling space $\Omega$ would be $\Omega=\{ H,T\}^n$, the space of sequences of $n$ coin flipping, since this is what actually happens. But one could just as well define this random variable on the set $\{0,1,…,n\}$, in which case the random variable would be the identity function. This space I would call artificial, not ""compelling"", because it doesn't give an accurate representation of the underlying experiment any more. In particular I'm interested in the underlying space for the statistical examples. P.S. See also this other question of mine, which also has a bounty running.",,"['probability', 'probability-theory', 'random-variables', 'examples-counterexamples', 'mathematical-modeling']"
42,Expected number of calls for bingo win,Expected number of calls for bingo win,,"Before I begin, I did a search through math.stackexchange and came across two previous attempts to get people to solve probability problems involving bingo.  Neither produced a response. So what makes me think I'll be any luckier?  Maybe some new guy/gal has some insight. The game of bingo is played with a bingo card having 25 squares, arranged in 5 columns of 5 squares.  The first column has numbers between 1-15, the 2nd column has numbers between 16-30, and so on (the 5th column (!) has numbers between 61-75).  Someone randomly draws a number from 1-75 and announces it.  To make things a little interesting, the middle square is labeled ""free"".  If a number called matches one on your card, you mark it.  The goal is to have a complete row, column, or diagonal of 5 marked off first. Here's my question: what is the expected number of random draws when there is a winner among $N$ players? I was thinking about this because I got involved in such a game this evening with my kids, and it seemed to take an awfully long time for a winner to surface. My thoughts: this seems to me to be an extremely tough problem.  I consider myself better than average in computing expected values, yet I found myself completely stuck on even how to approach the problem.  I suppose I could have searched the literature, but I figured I needed to pose a decent question here; I owe it to the users who have posed so many interesting questions here for me to answer. I'll look for any insight that might move the discussion forward; I do not expect a complete answer for you to post.","Before I begin, I did a search through math.stackexchange and came across two previous attempts to get people to solve probability problems involving bingo.  Neither produced a response. So what makes me think I'll be any luckier?  Maybe some new guy/gal has some insight. The game of bingo is played with a bingo card having 25 squares, arranged in 5 columns of 5 squares.  The first column has numbers between 1-15, the 2nd column has numbers between 16-30, and so on (the 5th column (!) has numbers between 61-75).  Someone randomly draws a number from 1-75 and announces it.  To make things a little interesting, the middle square is labeled ""free"".  If a number called matches one on your card, you mark it.  The goal is to have a complete row, column, or diagonal of 5 marked off first. Here's my question: what is the expected number of random draws when there is a winner among $N$ players? I was thinking about this because I got involved in such a game this evening with my kids, and it seemed to take an awfully long time for a winner to surface. My thoughts: this seems to me to be an extremely tough problem.  I consider myself better than average in computing expected values, yet I found myself completely stuck on even how to approach the problem.  I suppose I could have searched the literature, but I figured I needed to pose a decent question here; I owe it to the users who have posed so many interesting questions here for me to answer. I'll look for any insight that might move the discussion forward; I do not expect a complete answer for you to post.",,"['probability', 'recreational-mathematics']"
43,"Odds of guessing suit from a deck of cards, with perfect memory","Odds of guessing suit from a deck of cards, with perfect memory",,"While teaching my daughter why drawing to an inside straight is almost always a bad idea, we stumbled upon what I think is a far more difficult problem: You have a standard 52-card deck with 4 suits and I ask you to guess the suit of the top card. The odds of guessing the correct suit are obviously 1 in 4. You then guess again, but the first card is not returned to the deck. You guess a suit other than the first drawn and the odds are 13/51, somewhat better than 1 in 4. Continuing through the deck your odds continually change (never worse than 1 in 4, definitely 100% for the last card) what are your overall odds for any given draw over the course of 52 picks? Can this be calculated? Or do you need to devise a strategy and write a computer program to determine the answer? Do these type of problems have a name? Dad and to a much less extent daughter, await your thoughts!","While teaching my daughter why drawing to an inside straight is almost always a bad idea, we stumbled upon what I think is a far more difficult problem: You have a standard 52-card deck with 4 suits and I ask you to guess the suit of the top card. The odds of guessing the correct suit are obviously 1 in 4. You then guess again, but the first card is not returned to the deck. You guess a suit other than the first drawn and the odds are 13/51, somewhat better than 1 in 4. Continuing through the deck your odds continually change (never worse than 1 in 4, definitely 100% for the last card) what are your overall odds for any given draw over the course of 52 picks? Can this be calculated? Or do you need to devise a strategy and write a computer program to determine the answer? Do these type of problems have a name? Dad and to a much less extent daughter, await your thoughts!",,"['probability', 'statistics']"
44,"The vertices of a hexagon are random points on a unit circle; $a,b,c$ are the lengths of three random sides. Conjecture: $P(ab<c)=\frac35$.",The vertices of a hexagon are random points on a unit circle;  are the lengths of three random sides. Conjecture: .,"a,b,c P(ab<c)=\frac35","The vertices of a hexagon are uniformly random points on a unit circle; $a,b,c$ are the lengths of three distinct random sides. A simulation with $10^7$ such random hexagons yielded a proportion of $0.60008$ of them satisfying $ab<c$ . Is the following conjecture true: $P(ab<c)=\dfrac35$ . Probabilities involving the side lengths of random polygons inscribed in a circle are often simple rational numbers; for example here , here and here . Those examples involve triangles, which are determined by only two parameters (two random central angles, with the third angle determined by the others), so it was easy to set up an integral. But this question involves a hexagon, which involves five parameters (five random angles, with the sixth angle determined by the others), and I do not know how to set up an integral. Sometimes these kinds of questions can be answered without integration, using pure geometry, for example here . But I have been unable to find a purely geometric proof for this question. $P(ab<c)$ seems to be a simple rational number only for triangles and hexagons, among $n$ -gons for $3\le n\le 12$ , based on simulations. Distribution of $a$ Here I derive $f(x)$ , the density function of $a$ , which is the same as the density function of $b$ and $c$ . However, I do not know if this is helpful, because $a,b,c$ are not independent. Suppose the first chosen random point on the circle is $O$ , and $a$ is the length of the side immediately anticlockwise from $O$ . $a>x$ if all the other chosen points are on the major arc subtended by the red line segment, and the other chosen points are not all on the minor arc subtended by the green line segment. $\theta=\arcsin \frac{x}{2}$ $\begin{align} P(a<x)&=1-P(a>x)\\ &=1-\left(\left(1-\frac{1}{\pi}\arcsin\frac{x}{2}\right)^5-\left(\frac{1}{\pi}\arcsin\frac{x}{2}\right)^5\right) \end{align}$ $\begin{align} f(x)&=\frac{d}{dx}P(a<x)\\ &=\frac{5}{\pi\sqrt{4-x^2}}\left(\left(1-\frac{1}{\pi}\arcsin\frac{x}{2}\right)^4+\left(\frac{1}{\pi}\arcsin\frac{x}{2}\right)^4\right)\\ \end{align}$ I checked that $\int_0^2f(x)\mathrm dx=1$ .","The vertices of a hexagon are uniformly random points on a unit circle; are the lengths of three distinct random sides. A simulation with such random hexagons yielded a proportion of of them satisfying . Is the following conjecture true: . Probabilities involving the side lengths of random polygons inscribed in a circle are often simple rational numbers; for example here , here and here . Those examples involve triangles, which are determined by only two parameters (two random central angles, with the third angle determined by the others), so it was easy to set up an integral. But this question involves a hexagon, which involves five parameters (five random angles, with the sixth angle determined by the others), and I do not know how to set up an integral. Sometimes these kinds of questions can be answered without integration, using pure geometry, for example here . But I have been unable to find a purely geometric proof for this question. seems to be a simple rational number only for triangles and hexagons, among -gons for , based on simulations. Distribution of Here I derive , the density function of , which is the same as the density function of and . However, I do not know if this is helpful, because are not independent. Suppose the first chosen random point on the circle is , and is the length of the side immediately anticlockwise from . if all the other chosen points are on the major arc subtended by the red line segment, and the other chosen points are not all on the minor arc subtended by the green line segment. I checked that .","a,b,c 10^7 0.60008 ab<c P(ab<c)=\dfrac35 P(ab<c) n 3\le n\le 12 a f(x) a b c a,b,c O a O a>x \theta=\arcsin \frac{x}{2} \begin{align}
P(a<x)&=1-P(a>x)\\
&=1-\left(\left(1-\frac{1}{\pi}\arcsin\frac{x}{2}\right)^5-\left(\frac{1}{\pi}\arcsin\frac{x}{2}\right)^5\right)
\end{align} \begin{align}
f(x)&=\frac{d}{dx}P(a<x)\\
&=\frac{5}{\pi\sqrt{4-x^2}}\left(\left(1-\frac{1}{\pi}\arcsin\frac{x}{2}\right)^4+\left(\frac{1}{\pi}\arcsin\frac{x}{2}\right)^4\right)\\
\end{align} \int_0^2f(x)\mathrm dx=1","['probability', 'integration', 'circles', 'conjectures', 'geometric-probability']"
45,"Is there any significance to this ""doubly stochastic matrix"" with both a discrete and continuous index?","Is there any significance to this ""doubly stochastic matrix"" with both a discrete and continuous index?",,"This is just idle curiosity. Consider the function $(\lambda, n) \mapsto e^{-\lambda} \frac{\lambda^n}{n!}$ , where $\lambda \in \mathbb{R}_{\ge 0}$ is a nonnegative real parameter and $n \in \mathbb{Z}_{\ge 0}$ is a nonnegative integer parameter. This function has the very funny property of being a ""doubly stochastic matrix"" in the following sense: we have both $$\int_0^{\infty} e^{-\lambda} \frac{\lambda^n}{n!} \, d \lambda = 1$$ (the integrand being, for fixed $n$ , the probability density function of a sum of $n + 1$ exponential random variables $\text{Exp}(1)$ , or an Erlang random variable $\text{Erlang}(n+1, 1)$ ) and $$\sum_{n \ge 0} e^{-\lambda} \frac{\lambda^n}{n!} = 1$$ (the summand being, for fixed $\lambda$ , the probability density function of a Poisson random variable $\text{Pois}(\lambda)$ ). Question: What significance, if any, does this observation have? What this means concretely is that $e^{-\lambda} \frac{\lambda^n}{n!}$ can be used as a ""kernel"" that converts between probability distributions on $\mathbb{Z}_{\ge 0}$ and probability distributions on $\mathbb{R}_{\ge 0}$ , in either direction. The two descriptions of this function above also have the funny implication that for large $n$ as a function of $\lambda$ we have a Gaussian approximation, and the same for large $\lambda$ as a function of $n$ , as a result of applying the central limit theorem first to a sum of exponential random variables and then to a sum of Poisson random variables.","This is just idle curiosity. Consider the function , where is a nonnegative real parameter and is a nonnegative integer parameter. This function has the very funny property of being a ""doubly stochastic matrix"" in the following sense: we have both (the integrand being, for fixed , the probability density function of a sum of exponential random variables , or an Erlang random variable ) and (the summand being, for fixed , the probability density function of a Poisson random variable ). Question: What significance, if any, does this observation have? What this means concretely is that can be used as a ""kernel"" that converts between probability distributions on and probability distributions on , in either direction. The two descriptions of this function above also have the funny implication that for large as a function of we have a Gaussian approximation, and the same for large as a function of , as a result of applying the central limit theorem first to a sum of exponential random variables and then to a sum of Poisson random variables.","(\lambda, n) \mapsto e^{-\lambda} \frac{\lambda^n}{n!} \lambda \in \mathbb{R}_{\ge 0} n \in \mathbb{Z}_{\ge 0} \int_0^{\infty} e^{-\lambda} \frac{\lambda^n}{n!} \, d \lambda = 1 n n + 1 \text{Exp}(1) \text{Erlang}(n+1, 1) \sum_{n \ge 0} e^{-\lambda} \frac{\lambda^n}{n!} = 1 \lambda \text{Pois}(\lambda) e^{-\lambda} \frac{\lambda^n}{n!} \mathbb{Z}_{\ge 0} \mathbb{R}_{\ge 0} n \lambda \lambda n","['probability', 'probability-distributions', 'stochastic-matrices']"
46,Expected area of the intersection of two circles,Expected area of the intersection of two circles,,"If we pick randomly two points inside a circle centred at $O$ with radius $R$, and draw two circles centred at the two points with radius equal to the distance between them, what is the expected area of the intersection of the two cirlces that contain the origin $O$.","If we pick randomly two points inside a circle centred at $O$ with radius $R$, and draw two circles centred at the two points with radius equal to the distance between them, what is the expected area of the intersection of the two cirlces that contain the origin $O$.",,"['probability', 'geometry', 'geometric-probability']"
47,Expectancy value for the percentage of points lying in the Convex Hull (3D),Expectancy value for the percentage of points lying in the Convex Hull (3D),,"Suppose I chose n uniformly distributed random points in a 3D cube. What is the expected value for the percentage of points lying on the convex hull as a function of n? Just as a reference, I made the following experiment in Mathematica 8: Needs[""TetGenLink`""]; Show[  DiscretePlot[   1/k Mean[Length /@ Union /@ (Flatten /@ (TetGenConvexHull /@            RandomReal[{0, 1}, {500, k, 3}]))], {k, 4, 200, 3},    AxesOrigin -> {0, 0}, Joined -> True], Plot[.5, {x, 1, 200}]] Edit Again as a reference, if we plot the mean number of points in the convex hull (not the percentage) as a function of the total number of points we get: Edit 2 The second plot in Log and Log-Log forms: Edit 3 As noted by @Raskolnikov in the comments below, and confirmed by the ""experimental"" result, the case n=5 can be though as the Cube Tetrahedron Picking Formula wich is basically the probability of the fifth point being inside the tetrahedron determined by the other four points. As noted by @Steven Stadnicki  that is not completely obvious because you are choosing the four points beforehand and some permutation could have been left aside ... but the experiments confirm the @Raskolnicov reasoning. Edit 4 I fitted the data using Eureqa , a nice package from Cornell for guessing fitting functions, and got as a probable fit for the number of points in the convex hull: f[x_] := 1.4723399 Log@x Log@(3.0543704 + x) which gives: In line with Raskolnicov's answer about the asymptotic behavior. I wasn't able to read the cited paper, though (restricted access)","Suppose I chose n uniformly distributed random points in a 3D cube. What is the expected value for the percentage of points lying on the convex hull as a function of n? Just as a reference, I made the following experiment in Mathematica 8: Needs[""TetGenLink`""]; Show[  DiscretePlot[   1/k Mean[Length /@ Union /@ (Flatten /@ (TetGenConvexHull /@            RandomReal[{0, 1}, {500, k, 3}]))], {k, 4, 200, 3},    AxesOrigin -> {0, 0}, Joined -> True], Plot[.5, {x, 1, 200}]] Edit Again as a reference, if we plot the mean number of points in the convex hull (not the percentage) as a function of the total number of points we get: Edit 2 The second plot in Log and Log-Log forms: Edit 3 As noted by @Raskolnikov in the comments below, and confirmed by the ""experimental"" result, the case n=5 can be though as the Cube Tetrahedron Picking Formula wich is basically the probability of the fifth point being inside the tetrahedron determined by the other four points. As noted by @Steven Stadnicki  that is not completely obvious because you are choosing the four points beforehand and some permutation could have been left aside ... but the experiments confirm the @Raskolnicov reasoning. Edit 4 I fitted the data using Eureqa , a nice package from Cornell for guessing fitting functions, and got as a probable fit for the number of points in the convex hull: f[x_] := 1.4723399 Log@x Log@(3.0543704 + x) which gives: In line with Raskolnicov's answer about the asymptotic behavior. I wasn't able to read the cited paper, though (restricted access)",,"['probability', 'geometry', 'statistics', 'geometric-probability']"
48,What is the probability that every pair of students studies together at some point?,What is the probability that every pair of students studies together at some point?,,"A cohort in a school consists of 75 students who study for 6 years.  Each year, the students are randomly distributed into 3 classrooms of 25 students each.  What is the probability that, after 6 years, each student has at some point been in a classroom with every other student? More generally:  Starting with an edgeless (undirected) graph on cn vertices, a round consists of first randomly partitioning the vertices into c disjoint sets of n vertices each, then adding an edge between every pair of not-yet-joined vertices that lie in the same set.  What is the probability that, after y rounds, the result is a complete graph on cn vertices? I have estimates and solutions to special cases, and it's straightforward to find the probability that a single given student sees all the others, but I don't know how to tackle the question in general.  (I do have a very pretty but completely useless expression for the exact answer, which I can supply if there's interest.)  In the case c =3, n =25, y =6 it's clear that the answer is ""so close to zero that nobody can tell the difference"" but I was hoping for a more precise result.  Any guidance appreciated.","A cohort in a school consists of 75 students who study for 6 years.  Each year, the students are randomly distributed into 3 classrooms of 25 students each.  What is the probability that, after 6 years, each student has at some point been in a classroom with every other student? More generally:  Starting with an edgeless (undirected) graph on cn vertices, a round consists of first randomly partitioning the vertices into c disjoint sets of n vertices each, then adding an edge between every pair of not-yet-joined vertices that lie in the same set.  What is the probability that, after y rounds, the result is a complete graph on cn vertices? I have estimates and solutions to special cases, and it's straightforward to find the probability that a single given student sees all the others, but I don't know how to tackle the question in general.  (I do have a very pretty but completely useless expression for the exact answer, which I can supply if there's interest.)  In the case c =3, n =25, y =6 it's clear that the answer is ""so close to zero that nobody can tell the difference"" but I was hoping for a more precise result.  Any guidance appreciated.",,"['combinatorics', 'probability', 'graph-theory']"
49,A game with $n$ players - II,A game with  players - II,n,"Consider $n$ player numbered $1,2,\ldots,n$. If player $i$ fights against $j$ then $i$ wins with probability $i/(i+j)$. There are no ties. A player $i_1$ is extracted at random. Then, a second different player $i_2$ is extracted at random. They fight against each other. Then, we extract another player $i_3$ ($\neq i_1,i_2$). The winner of the latter round fights against $i_3$.The fights continues until all players have been extracted, hence $n-1$ fights in total. Now, given $i\le j$, I think that player $i$ wins the game with probability at most $i/j$ times the probability that player $j$ wins the game. (I can prove it manually for $n\le 4$.) Question. Is it possible to prove it for all $n$? Ps. Another property of the same game has been asked here . Ps2. Is it a ""known game"" ?","Consider $n$ player numbered $1,2,\ldots,n$. If player $i$ fights against $j$ then $i$ wins with probability $i/(i+j)$. There are no ties. A player $i_1$ is extracted at random. Then, a second different player $i_2$ is extracted at random. They fight against each other. Then, we extract another player $i_3$ ($\neq i_1,i_2$). The winner of the latter round fights against $i_3$.The fights continues until all players have been extracted, hence $n-1$ fights in total. Now, given $i\le j$, I think that player $i$ wins the game with probability at most $i/j$ times the probability that player $j$ wins the game. (I can prove it manually for $n\le 4$.) Question. Is it possible to prove it for all $n$? Ps. Another property of the same game has been asked here . Ps2. Is it a ""known game"" ?",,[]
50,Discover to which batch a coin belongs,Discover to which batch a coin belongs,,"The following question is taken from a book, in a chapter on probabilty: You have two batches of unbalanced coins. One has coins which turn up head with probability $p_1$, and the other has coins that turn up head with probability $p_2$. All you know about $p1$ and $p2$ is that $|p_1-p_2| \ge 0.01$. A coin was accidentaly dropped from one of the batches. Design an experiment that will determine which batch the coin belongs to, with error probability of no more than $10^{-12}$. You should use a reasonable number of coin tosses. As far as I can tell, any experiment will essentially boil down to something like this: Take a coin from each batch and toss it $n$ times. Let $X$ and $Y$ be the sample means of the tosses of the coins from the first and second batch, respectively. Then toss the coin in question $n$ times, and let $Z$ be the sample mean of the tosses of that coin. Then find $n$ for which: $$ \mathbb{P}[\,|X-Z|<|Y-Z|\,] \le10^{-12} $$ when the coin in question is from the second batch. I tried to play around with the inquality $|X-Z|<|Y-Z|$ to get something to which I may apply the Chernoff bound. But the best I got required $n\approx 3\cdot10^6$, which does not seem reasonable at all. Any help will be appreciated.","The following question is taken from a book, in a chapter on probabilty: You have two batches of unbalanced coins. One has coins which turn up head with probability $p_1$, and the other has coins that turn up head with probability $p_2$. All you know about $p1$ and $p2$ is that $|p_1-p_2| \ge 0.01$. A coin was accidentaly dropped from one of the batches. Design an experiment that will determine which batch the coin belongs to, with error probability of no more than $10^{-12}$. You should use a reasonable number of coin tosses. As far as I can tell, any experiment will essentially boil down to something like this: Take a coin from each batch and toss it $n$ times. Let $X$ and $Y$ be the sample means of the tosses of the coins from the first and second batch, respectively. Then toss the coin in question $n$ times, and let $Z$ be the sample mean of the tosses of that coin. Then find $n$ for which: $$ \mathbb{P}[\,|X-Z|<|Y-Z|\,] \le10^{-12} $$ when the coin in question is from the second batch. I tried to play around with the inquality $|X-Z|<|Y-Z|$ to get something to which I may apply the Chernoff bound. But the best I got required $n\approx 3\cdot10^6$, which does not seem reasonable at all. Any help will be appreciated.",,['probability']
51,Expected Number of Flips for a Sequence of 4 to Repeat,Expected Number of Flips for a Sequence of 4 to Repeat,,"I recently had this question in an interview: You are flipping a fair coin until a sequence of four flips repeats itself. The sequences are allowed to overlap. What is the expected number of flips? For example, if you flip HTHTHT, the sequence HTHT appears in flips 1-4 and 3-6. In this case, we are done after 6 flips. As another example, if you flip HHTTHHTT, the sequence HHTT repeats in flips 1-4 and 5-8, and we are done after 8 flips. What is the expected number of flips? I've been thinking about this question for a few days now, and I haven't come to an answer. I've tried the simpler problem where we consider a sequence of two flips instead of four flips, but it is still rather difficult. I suspect that there is a nice recursive way to solve this problem, but I can't figure it out. I am also interested in generalizations of this problem. For example, what is the expected number of flips for a sequence of $n$ to repeat itself? What happens if the coin isn't fair? I have another interview in a few days, so I would very much like to see how to solve this problem in advance. Any help is appreciated. Edit: Based on computational evidence (assuming I didn't make a mistake in the code), it appears that the expected number of flips is about 9.81. I would like to know the exact answer, as well as an analytic solution to this problem. Edit 2: Another piece of information that may be of use: I had 30 seconds to answer the question. This makes me believe that there is some ""easy"" way to get the answer, or they were looking for an approximate answer. Edit 3: @r.e.s. has kindly provided exact solutions for $n=1,2,3,4,5$. For $n=6$, numerical computation seems to indicate that the answer is around $18.977$ or $18.978$. I will be updating periodically with approximations for other values of $n$. Edit 4: $$ \begin{array}{|c|c|} \hline n & E(L_{n}) \\ \hline 1 & \frac{5}{2}\\ \hline 2 & \frac{35}{8}\\ \hline 3 & \frac{435}{64}\\ \hline 4 & \frac{2513}{256}\\ \hline 5 & \frac{57922047}{4194304}\\ \hline 6 & \approx 18.9775\\ \hline 7 & \approx 25.928\\ \hline 8 & \approx 35.288\\ \hline \end{array} $$","I recently had this question in an interview: You are flipping a fair coin until a sequence of four flips repeats itself. The sequences are allowed to overlap. What is the expected number of flips? For example, if you flip HTHTHT, the sequence HTHT appears in flips 1-4 and 3-6. In this case, we are done after 6 flips. As another example, if you flip HHTTHHTT, the sequence HHTT repeats in flips 1-4 and 5-8, and we are done after 8 flips. What is the expected number of flips? I've been thinking about this question for a few days now, and I haven't come to an answer. I've tried the simpler problem where we consider a sequence of two flips instead of four flips, but it is still rather difficult. I suspect that there is a nice recursive way to solve this problem, but I can't figure it out. I am also interested in generalizations of this problem. For example, what is the expected number of flips for a sequence of $n$ to repeat itself? What happens if the coin isn't fair? I have another interview in a few days, so I would very much like to see how to solve this problem in advance. Any help is appreciated. Edit: Based on computational evidence (assuming I didn't make a mistake in the code), it appears that the expected number of flips is about 9.81. I would like to know the exact answer, as well as an analytic solution to this problem. Edit 2: Another piece of information that may be of use: I had 30 seconds to answer the question. This makes me believe that there is some ""easy"" way to get the answer, or they were looking for an approximate answer. Edit 3: @r.e.s. has kindly provided exact solutions for $n=1,2,3,4,5$. For $n=6$, numerical computation seems to indicate that the answer is around $18.977$ or $18.978$. I will be updating periodically with approximations for other values of $n$. Edit 4: $$ \begin{array}{|c|c|} \hline n & E(L_{n}) \\ \hline 1 & \frac{5}{2}\\ \hline 2 & \frac{35}{8}\\ \hline 3 & \frac{435}{64}\\ \hline 4 & \frac{2513}{256}\\ \hline 5 & \frac{57922047}{4194304}\\ \hline 6 & \approx 18.9775\\ \hline 7 & \approx 25.928\\ \hline 8 & \approx 35.288\\ \hline \end{array} $$",,"['probability', 'expectation']"
52,"How was ""Number of ways of arranging n chords on a circle with k simple intersections"" solved?","How was ""Number of ways of arranging n chords on a circle with k simple intersections"" solved?",,"The problem whose solution is based on the solution to the problem in the title came up as I was trying to find a simpler variant of my needle problem . I we were to uniformly, randomly and independently set $2n$ points on   a circle, and then randomly connect them in a way such that each point   has its own pair, what would be the odds of finding $k$ intersections? Based on the maximum number of intersections we see that if $k \gt \frac{n(n-1)}{2}$, $P=0$. Otherwise we have some $P>0$. When connecting the points, all that matters is the ordering of points. Data Analysis I can write $P(n,k) = a / b$. Then $b$ is the number of ways to connect the points uniquely, and $a$ is the number of cases with $k$ intersections for $n$ lines. There are $b = (2n-1)!!$ ways of connecting the points uniquely. I wrote a piece of code in java to try to brute-force solutions of $a$, for $n$ up to $10$. I wrote them out in a spreadsheet as a image. Here is the raw data as text. After closely analyzing the values of $a$, OEIS provided me with a sequence. Looks like someone already calculated $a$ which actually is the number of ways of arranging n chords on a circle with k simple intersections . But the given formula is not correct given as it is. Thanks to Paul for fixing up a valid formula , since the OEIS one I stumbled upon seems to be wrong. The formula for $P(n,k)$ then is: $$ \frac{\displaystyle \sum_{j=1}^{\left\lfloor \tfrac12 +  \tfrac12\sqrt{1+8k} \right\rfloor} (-1)^j \cdot  \binom{n+k-1-\binom{j}{2}}{n-1} \cdot  \left( \binom{2n}{n+j} -  \binom{2n}{n+j-1} \right)}{(2n-1)!!} $$ Which solves my initial problem. But I'm still curious to know how someone came up with this in the   first place, starting out with just a circle and some cords? Regarding the title; How was "" Number of ways of arranging n chords on a circle with k simple intersections "" solved to produce the expression in the numerator?","The problem whose solution is based on the solution to the problem in the title came up as I was trying to find a simpler variant of my needle problem . I we were to uniformly, randomly and independently set $2n$ points on   a circle, and then randomly connect them in a way such that each point   has its own pair, what would be the odds of finding $k$ intersections? Based on the maximum number of intersections we see that if $k \gt \frac{n(n-1)}{2}$, $P=0$. Otherwise we have some $P>0$. When connecting the points, all that matters is the ordering of points. Data Analysis I can write $P(n,k) = a / b$. Then $b$ is the number of ways to connect the points uniquely, and $a$ is the number of cases with $k$ intersections for $n$ lines. There are $b = (2n-1)!!$ ways of connecting the points uniquely. I wrote a piece of code in java to try to brute-force solutions of $a$, for $n$ up to $10$. I wrote them out in a spreadsheet as a image. Here is the raw data as text. After closely analyzing the values of $a$, OEIS provided me with a sequence. Looks like someone already calculated $a$ which actually is the number of ways of arranging n chords on a circle with k simple intersections . But the given formula is not correct given as it is. Thanks to Paul for fixing up a valid formula , since the OEIS one I stumbled upon seems to be wrong. The formula for $P(n,k)$ then is: $$ \frac{\displaystyle \sum_{j=1}^{\left\lfloor \tfrac12 +  \tfrac12\sqrt{1+8k} \right\rfloor} (-1)^j \cdot  \binom{n+k-1-\binom{j}{2}}{n-1} \cdot  \left( \binom{2n}{n+j} -  \binom{2n}{n+j-1} \right)}{(2n-1)!!} $$ Which solves my initial problem. But I'm still curious to know how someone came up with this in the   first place, starting out with just a circle and some cords? Regarding the title; How was "" Number of ways of arranging n chords on a circle with k simple intersections "" solved to produce the expression in the numerator?",,"['probability', 'combinatorics', 'recreational-mathematics', 'geometric-probability']"
53,Variational formulations in group theory?,Variational formulations in group theory?,,"I apologise if this is a naïve question. Are there any known / widely applicable / important variational formulations in (finite) group theory? That is, a relationship of the form $$\alpha(G) = \sup\{\phi(g) : g \in G\}, $$ where $\alpha$ is a number (or set) determined by some property of the group $G$ and $\phi$ is a real-valued (or set-valued) function on $G$. For a silly example, take $\phi(g)$ to be the order of an element and $\alpha$ the order of the group; the group is cyclic iff $\alpha(G) = \#G = \sup_{g \in G}\phi(g).$ I would also be interested in results on the probability distributions induced on groups or reflected in their structure in some way. For instance, suppose a finite group $G$ acts on a finite set $A$, and let $\phi : G \rightarrow \mathbb{N}_{\geq 1}$ be a function such that $\phi(g) = \#\{a \in A : g\cdot a = a\}$. In other words, $\phi$ assigns to each $g \in G$ the number of fixed points it has in $A$. Now let $X$ be a random variable uniformly distributed on the finite set $G$. Then Burnside's lemma states that the expectation $\mathbb{E}[\phi(X)]$ gives us the number of orbits of the action: $$|A/G| = \mathbb{E}[\phi(X)] \stackrel{\cdot}{=} \sum_{g \in G} \frac{\phi(g)}{|G|}.$$ I'd appreciate any key words / things to Google. I assume many things come from number theory.","I apologise if this is a naïve question. Are there any known / widely applicable / important variational formulations in (finite) group theory? That is, a relationship of the form $$\alpha(G) = \sup\{\phi(g) : g \in G\}, $$ where $\alpha$ is a number (or set) determined by some property of the group $G$ and $\phi$ is a real-valued (or set-valued) function on $G$. For a silly example, take $\phi(g)$ to be the order of an element and $\alpha$ the order of the group; the group is cyclic iff $\alpha(G) = \#G = \sup_{g \in G}\phi(g).$ I would also be interested in results on the probability distributions induced on groups or reflected in their structure in some way. For instance, suppose a finite group $G$ acts on a finite set $A$, and let $\phi : G \rightarrow \mathbb{N}_{\geq 1}$ be a function such that $\phi(g) = \#\{a \in A : g\cdot a = a\}$. In other words, $\phi$ assigns to each $g \in G$ the number of fixed points it has in $A$. Now let $X$ be a random variable uniformly distributed on the finite set $G$. Then Burnside's lemma states that the expectation $\mathbb{E}[\phi(X)]$ gives us the number of orbits of the action: $$|A/G| = \mathbb{E}[\phi(X)] \stackrel{\cdot}{=} \sum_{g \in G} \frac{\phi(g)}{|G|}.$$ I'd appreciate any key words / things to Google. I assume many things come from number theory.",,"['probability', 'group-theory', 'finite-groups', 'random-variables', 'big-list']"
54,perfect play in 1-dimensional Minesweeper,perfect play in 1-dimensional Minesweeper,,"In 1-dimensional Minesweeper with a known number of mines (that are distributed uniformly), is there a known somewhat-simple strategy for perfect play? When there are n cells and [0 or n-1 or n] mines, the strategy is utterly trivial. When there is 1 mine, the strategy is to choose an end cell and then every cell other than the obvious mine that was revealed. $\:$ While it is plausible that ""start at one end and go to the other, skipping obvious mines"" is always optimal, I certainly do not have a proof either way for that.","In 1-dimensional Minesweeper with a known number of mines (that are distributed uniformly), is there a known somewhat-simple strategy for perfect play? When there are n cells and [0 or n-1 or n] mines, the strategy is utterly trivial. When there is 1 mine, the strategy is to choose an end cell and then every cell other than the obvious mine that was revealed. $\:$ While it is plausible that ""start at one end and go to the other, skipping obvious mines"" is always optimal, I certainly do not have a proof either way for that.",,"['probability', 'algorithms']"
55,How to compute $\mathbb{E}(\exp(\int_0^t W_s ds)|W_t)$?,How to compute ?,\mathbb{E}(\exp(\int_0^t W_s ds)|W_t),"I am trying to compute the conditional expectation $$\mathbb{E}\left[\exp\left(\int_0^t W_s ds\right)\middle|\, W_t\right]$$ where $W$ is a standard Wiener process and where $s\le t$. To initially simplify the problem, I have started with the calculations of $\mathbb{E}[W_s|W_t]$ and $\mathbb{E}\left[\int_0^t W_s \,ds\middle|\,W_t\right]$. On the one hand, since $W_t$ and $W_s- \frac{s}{t}W_t$ are independent (having zero covariance and using a gaussian vector argument), we can see that: $$\mathbb{E}\left[W_s\middle | W_t\right]=\mathbb{E}\left[W_s-\frac{s}{t}W_t\middle |\, W_t\right]+\frac{s}{t}W_t=\frac{s}{t}W_t$$ On the other hand, by independence of $W_t$ and $\int_0^t (W_s- \frac{s}{t}W_t)ds$: \begin{align}\mathbb{E}\left[\int_0^t W_s ds\middle|\,W_t\right]&=\mathbb{E}\left[\int_0^t \left(W_s-\frac{s}{t}W_t\right) ds\,\middle |\, W_t\right]+\frac{t}{2}W_t\\[0.3cm]&=\int_0^t \mathbb{E}\left[W_s-\frac{s}{t}W_t\right]ds+\frac{t}{2}W_t=\frac{t}{2}W_t\end{align} Coming back to our initial problem, we thus have: $$\mathbb{E}\left[\exp\left(\int_0^t W_s ds\right)\,\middle|\,W_t\right]=\exp\left(\frac{t}{2}W_t\right)\mathbb{E}\left[\exp\left(\int_0^t \left(W_s-\frac{s}{t}W_t\right) ds\right)\,\middle|\,W_t\right]$$ We also know that $\int_0^t \left(W_s-\frac{s}{t}W_t\right)ds$ is normally distributed with zero mean (easy to see) and variance given by: $$\mathbb{E}\left[\int_0^t\int_0^t \left(W_s- \frac{s}{t}W_t\right)\left(W_u- \frac{u}{t}W_t\right)dsdu\right]=\int_0^t\int_0^t\left(\min(s,u)-\frac{su}{t}\right)dsdu=\frac{t^3}{12}$$ By independence of $W_t$ and $\exp\left(\int_0^t \left(W_s- \frac{s}{t}W_t\right)ds\right)$, we finally obtain ($Z$ being a standard unit normal variable): $$\mathbb{E}\left[\exp\left(\int_0^t W_s ds\right)\,\middle|\,W_t\right]=\exp\left(\frac{t}{2}W_t\right)\mathbb{E}\left[\exp\left(Z\sqrt{\frac{t^3}{12}}\right)\right] =\exp\left(\frac{t}{2}W_t+\frac{t^3}{24}\right)$$ However, I am not sure if this answer and the arguments I have used are correct? Any ideas or comments would be greatly appreciated.","I am trying to compute the conditional expectation $$\mathbb{E}\left[\exp\left(\int_0^t W_s ds\right)\middle|\, W_t\right]$$ where $W$ is a standard Wiener process and where $s\le t$. To initially simplify the problem, I have started with the calculations of $\mathbb{E}[W_s|W_t]$ and $\mathbb{E}\left[\int_0^t W_s \,ds\middle|\,W_t\right]$. On the one hand, since $W_t$ and $W_s- \frac{s}{t}W_t$ are independent (having zero covariance and using a gaussian vector argument), we can see that: $$\mathbb{E}\left[W_s\middle | W_t\right]=\mathbb{E}\left[W_s-\frac{s}{t}W_t\middle |\, W_t\right]+\frac{s}{t}W_t=\frac{s}{t}W_t$$ On the other hand, by independence of $W_t$ and $\int_0^t (W_s- \frac{s}{t}W_t)ds$: \begin{align}\mathbb{E}\left[\int_0^t W_s ds\middle|\,W_t\right]&=\mathbb{E}\left[\int_0^t \left(W_s-\frac{s}{t}W_t\right) ds\,\middle |\, W_t\right]+\frac{t}{2}W_t\\[0.3cm]&=\int_0^t \mathbb{E}\left[W_s-\frac{s}{t}W_t\right]ds+\frac{t}{2}W_t=\frac{t}{2}W_t\end{align} Coming back to our initial problem, we thus have: $$\mathbb{E}\left[\exp\left(\int_0^t W_s ds\right)\,\middle|\,W_t\right]=\exp\left(\frac{t}{2}W_t\right)\mathbb{E}\left[\exp\left(\int_0^t \left(W_s-\frac{s}{t}W_t\right) ds\right)\,\middle|\,W_t\right]$$ We also know that $\int_0^t \left(W_s-\frac{s}{t}W_t\right)ds$ is normally distributed with zero mean (easy to see) and variance given by: $$\mathbb{E}\left[\int_0^t\int_0^t \left(W_s- \frac{s}{t}W_t\right)\left(W_u- \frac{u}{t}W_t\right)dsdu\right]=\int_0^t\int_0^t\left(\min(s,u)-\frac{su}{t}\right)dsdu=\frac{t^3}{12}$$ By independence of $W_t$ and $\exp\left(\int_0^t \left(W_s- \frac{s}{t}W_t\right)ds\right)$, we finally obtain ($Z$ being a standard unit normal variable): $$\mathbb{E}\left[\exp\left(\int_0^t W_s ds\right)\,\middle|\,W_t\right]=\exp\left(\frac{t}{2}W_t\right)\mathbb{E}\left[\exp\left(Z\sqrt{\frac{t^3}{12}}\right)\right] =\exp\left(\frac{t}{2}W_t+\frac{t^3}{24}\right)$$ However, I am not sure if this answer and the arguments I have used are correct? Any ideas or comments would be greatly appreciated.",,"['probability', 'probability-theory', 'stochastic-processes', 'stochastic-calculus', 'brownian-motion']"
56,Strategy in a waiting time game: fisherman's dilemma,Strategy in a waiting time game: fisherman's dilemma,,"Suppose there is a fisherman out on the lake, who repeatedly casts his line looking for fish. Let $X_k$ denote the time it takes him to catch a fish on his $k$th cast, $k \in \mathbb{N}$, where the $X_k$ are iid, sampled from some distribution function $F$, and so that that $X_k > 0$ and $\mathbb{E}X < \infty$. The fisherman knows $F$, and has control over one thing: he can choose to re-cast his line at any time. What is his optimal strategy to catch the most fish in the long run? There are situations where it is advantageous to re-cast: if $F$ is bi-modal, with the two modes very far apart, then the fisherman will wait for the first mode to pass, and if he hasn't caught a fish, re-cast the line rather than waiting to hit the next mode. I can characterize the distribution functions where it is sometimes advantageous to re-cast via a direct computation, though I am curious if there is a more elegant condition. If the fisherman hasn't seen a fish up to time $t$, then his expected waiting time to see a fish is \begin{equation} \mathbb{E}[X - t | X > t] = \int_{t}^\infty \frac{F(s)}{F(t)} ds, \end{equation} where I'm using $F(x) = \mathbb{P}(X > x)$. On the other hand, the unconditional expected waiting time is \begin{equation} \mu = \mathbb{E}[X] = \int_0^\infty F(s) ds. \end{equation} Thus, it is advantageous to re-cast our line at time $t$ if the conditional expected waiting time is larger than the unconditional waiting time, i.e. any time $t > 0$ satisfying \begin{equation} F(t) \int_0^\infty F(s) ds < \int_t^\infty F(s) ds. \end{equation} $\Big($ EDIT 2 : @DaneiWeissman pointed out that this condition isn't always the right one to figure out the optimal strategy. Assuming that the right strategy is to pick a time $t$ and always re-cast at that time, the waiting time for that strategy is \begin{equation} \mathbb{E}[X|X \leq t] + \sum_{j \geq 0} F(t)^j(1-F(t)) jt = \frac{1}{1-F(t)} \int_0^t (F(s) - F(t)) ds + \frac{t F(t)}{1-F(t)} \end{equation} \begin{equation} =\frac{1}{1-F(t)}\int_0^t F(s)ds. \end{equation} This is because we wait a geometrically-distributed number of times with failure probability $F(t)$ before there is a fish in $[0,t]$, and once this does occur, it takes time $\mathbb{E}[X|X \leq t]$ to catch it on average. So we should be instead be trying to solve \begin{equation} \frac{1}{F(t)} \int_t^\infty F(s) ds = \frac{1}{1-F(t)} \int_0^t F(s) ds, \hspace{5pt} (\star \star)\end{equation} or perhaps just minimize the RHS over $t \in \mathbb{R}$. $\Big)$ For example, if these expressions are equal for all $t$, then differentiating with respect to $t$ yields \begin{equation} \mu F'(t) = -F(t), \end{equation} and $F(t) = e^{-t/\mu}$ is the only distribution function on $[0,\infty)$ satisfying this ODE. So if the $X$'s are exponential then re-casting at any time never hurts or helps us. ( This shouldn't surprise anyone familiar with Poisson processes, and the memory-less property of the exponential distribution! ) One can also compute directly with $F_\alpha(x) = (1+x)^{-\alpha}$, for $\alpha > 1$. This gives \begin{equation} \mathbb{E}[X - t | X > t] = \frac{t+1}{\alpha - 1}, \end{equation} while \begin{equation} \mathbb{E}X = \frac{1}{\alpha - 1}. \end{equation} So the fisherman should recast his line at every positive time in this case! I am interested in the times $t$ where we have equality, namely times $t$ when \begin{equation} \mathbb{E}[X-t|X > t] = \mathbb{E}[X] \hspace{10pt} (\star) \end{equation} My questions are: 1) What possible sets can occur as solution sets to $(\star)$ or $(\star \star)$? For example, can there be a single unique solution? $N$ solutions for some $N \in \mathbb{N}$? Can it be satisfied for all $t$ in some interval? 2) Is there a nicer condition to determine whether or not there is some solution to $(\star$) or $(\star \star)$? (I wonder if there is some way to compare $F$ to an exponential distribution that would help.) 3) Can the integral equations $(\star)$ or $(\star \star)$ be reduced to a more tractable form? 4) Are there any other nice classes of distribution functions for which they can be solved explicitly? 5) How would one make sense of the case where $\mathbb{E}X = \infty$? The equations $(\star)$ and $(\star \star)$ don't make sense anymore. Should the fisherman ever re-cast in this case? Also, I am curious if this question has been studied in any other context: it seems like a very natural setup to me! P.S. There are many simple possible variants on the original question. I wonder what would happen if one imposed a time-cost on re-casting the line, or if the fisherman doesn't know how long has passed since he cast his line. What would the optimal strategy be? EDIT 1: Another distribution one can prove something about is $F(t) = e^{-t^2}$, which decays faster than exponentially. We have $\mathbb{E}X = \frac{\sqrt{\pi}}{2}$, while \begin{equation} \mathbb{E}[X-t | X > t] = e^{t^2} \int_{t}^\infty e^{-s^2} ds. \end{equation} It is straightforward to check that this function is strictly decreasing, and \begin{equation} \lim_{t \to 0^+} \mathbb{E}[X-t | X > t] = \mathbb{E}X, \end{equation} which holds for any distribution function $F$ (since $F$ has right limits and $F(0) = 1$). Thus, in this case the fisherman should never recast his line.","Suppose there is a fisherman out on the lake, who repeatedly casts his line looking for fish. Let $X_k$ denote the time it takes him to catch a fish on his $k$th cast, $k \in \mathbb{N}$, where the $X_k$ are iid, sampled from some distribution function $F$, and so that that $X_k > 0$ and $\mathbb{E}X < \infty$. The fisherman knows $F$, and has control over one thing: he can choose to re-cast his line at any time. What is his optimal strategy to catch the most fish in the long run? There are situations where it is advantageous to re-cast: if $F$ is bi-modal, with the two modes very far apart, then the fisherman will wait for the first mode to pass, and if he hasn't caught a fish, re-cast the line rather than waiting to hit the next mode. I can characterize the distribution functions where it is sometimes advantageous to re-cast via a direct computation, though I am curious if there is a more elegant condition. If the fisherman hasn't seen a fish up to time $t$, then his expected waiting time to see a fish is \begin{equation} \mathbb{E}[X - t | X > t] = \int_{t}^\infty \frac{F(s)}{F(t)} ds, \end{equation} where I'm using $F(x) = \mathbb{P}(X > x)$. On the other hand, the unconditional expected waiting time is \begin{equation} \mu = \mathbb{E}[X] = \int_0^\infty F(s) ds. \end{equation} Thus, it is advantageous to re-cast our line at time $t$ if the conditional expected waiting time is larger than the unconditional waiting time, i.e. any time $t > 0$ satisfying \begin{equation} F(t) \int_0^\infty F(s) ds < \int_t^\infty F(s) ds. \end{equation} $\Big($ EDIT 2 : @DaneiWeissman pointed out that this condition isn't always the right one to figure out the optimal strategy. Assuming that the right strategy is to pick a time $t$ and always re-cast at that time, the waiting time for that strategy is \begin{equation} \mathbb{E}[X|X \leq t] + \sum_{j \geq 0} F(t)^j(1-F(t)) jt = \frac{1}{1-F(t)} \int_0^t (F(s) - F(t)) ds + \frac{t F(t)}{1-F(t)} \end{equation} \begin{equation} =\frac{1}{1-F(t)}\int_0^t F(s)ds. \end{equation} This is because we wait a geometrically-distributed number of times with failure probability $F(t)$ before there is a fish in $[0,t]$, and once this does occur, it takes time $\mathbb{E}[X|X \leq t]$ to catch it on average. So we should be instead be trying to solve \begin{equation} \frac{1}{F(t)} \int_t^\infty F(s) ds = \frac{1}{1-F(t)} \int_0^t F(s) ds, \hspace{5pt} (\star \star)\end{equation} or perhaps just minimize the RHS over $t \in \mathbb{R}$. $\Big)$ For example, if these expressions are equal for all $t$, then differentiating with respect to $t$ yields \begin{equation} \mu F'(t) = -F(t), \end{equation} and $F(t) = e^{-t/\mu}$ is the only distribution function on $[0,\infty)$ satisfying this ODE. So if the $X$'s are exponential then re-casting at any time never hurts or helps us. ( This shouldn't surprise anyone familiar with Poisson processes, and the memory-less property of the exponential distribution! ) One can also compute directly with $F_\alpha(x) = (1+x)^{-\alpha}$, for $\alpha > 1$. This gives \begin{equation} \mathbb{E}[X - t | X > t] = \frac{t+1}{\alpha - 1}, \end{equation} while \begin{equation} \mathbb{E}X = \frac{1}{\alpha - 1}. \end{equation} So the fisherman should recast his line at every positive time in this case! I am interested in the times $t$ where we have equality, namely times $t$ when \begin{equation} \mathbb{E}[X-t|X > t] = \mathbb{E}[X] \hspace{10pt} (\star) \end{equation} My questions are: 1) What possible sets can occur as solution sets to $(\star)$ or $(\star \star)$? For example, can there be a single unique solution? $N$ solutions for some $N \in \mathbb{N}$? Can it be satisfied for all $t$ in some interval? 2) Is there a nicer condition to determine whether or not there is some solution to $(\star$) or $(\star \star)$? (I wonder if there is some way to compare $F$ to an exponential distribution that would help.) 3) Can the integral equations $(\star)$ or $(\star \star)$ be reduced to a more tractable form? 4) Are there any other nice classes of distribution functions for which they can be solved explicitly? 5) How would one make sense of the case where $\mathbb{E}X = \infty$? The equations $(\star)$ and $(\star \star)$ don't make sense anymore. Should the fisherman ever re-cast in this case? Also, I am curious if this question has been studied in any other context: it seems like a very natural setup to me! P.S. There are many simple possible variants on the original question. I wonder what would happen if one imposed a time-cost on re-casting the line, or if the fisherman doesn't know how long has passed since he cast his line. What would the optimal strategy be? EDIT 1: Another distribution one can prove something about is $F(t) = e^{-t^2}$, which decays faster than exponentially. We have $\mathbb{E}X = \frac{\sqrt{\pi}}{2}$, while \begin{equation} \mathbb{E}[X-t | X > t] = e^{t^2} \int_{t}^\infty e^{-s^2} ds. \end{equation} It is straightforward to check that this function is strictly decreasing, and \begin{equation} \lim_{t \to 0^+} \mathbb{E}[X-t | X > t] = \mathbb{E}X, \end{equation} which holds for any distribution function $F$ (since $F$ has right limits and $F(0) = 1$). Thus, in this case the fisherman should never recast his line.",,"['probability', 'analysis', 'statistics']"
57,What is the Probability of Transmission Between Two Nodes in a Neural Network?,What is the Probability of Transmission Between Two Nodes in a Neural Network?,,"I have a network which is an Erdős–Rényi graph. It is a simple neural network with degree 0.7N where N is the number of nodes. Each weight between neurons is $\frac{1}{N}$ , meaning that if node n has fired the probability that any connected node k will fire in the next time step is $\frac{1}{N}$ (there is no temporal integration of inputs to any neuron). My question is as follows. If node n fires at t=0 what is the probability that a specific node m will fire at t=d ( d time steps later)? I know that if the weighted adjacency matrix W was a stochastic matrix then the probability would be ${W}_{mn}^{d}$ . However the matrix is not stochastic (since the rows do not sum to 1). Furthermore this calculation fails after direct experimentation. This problem is related to the question: What is the probability of any path of length n between the two nodes in a random graph where the existence of any edge has probability $\frac{0.7}{N}$ ? It is most useful if I could do this in terms of W . I think the answer may be related to Schur decomposition, as this is alluded to in "" Networks: An Introduction "" by M.E.J. Newman. Thank you for your help.","I have a network which is an Erdős–Rényi graph. It is a simple neural network with degree 0.7N where N is the number of nodes. Each weight between neurons is , meaning that if node n has fired the probability that any connected node k will fire in the next time step is (there is no temporal integration of inputs to any neuron). My question is as follows. If node n fires at t=0 what is the probability that a specific node m will fire at t=d ( d time steps later)? I know that if the weighted adjacency matrix W was a stochastic matrix then the probability would be . However the matrix is not stochastic (since the rows do not sum to 1). Furthermore this calculation fails after direct experimentation. This problem is related to the question: What is the probability of any path of length n between the two nodes in a random graph where the existence of any edge has probability ? It is most useful if I could do this in terms of W . I think the answer may be related to Schur decomposition, as this is alluded to in "" Networks: An Introduction "" by M.E.J. Newman. Thank you for your help.",\frac{1}{N} \frac{1}{N} {W}_{mn}^{d} \frac{0.7}{N},"['probability', 'graph-theory', 'random-graphs', 'network', 'neural-networks']"
58,"How many edges does an Erdős-Rényi graph have to have, to almost surely have a component with multiple cycles?","How many edges does an Erdős-Rényi graph have to have, to almost surely have a component with multiple cycles?",,"An Erdős-Rényi graph is a random graph, selected according to the distribution obtained one where we have some number $n$ of nodes, and some probability $p$ of each potential edge being present. One often asks questions of this distribution when $p$ is taken as a function of $n$, and we take the limit as $n \to \infty$. Classic results on properties of the graph which hold almost surely, from Erdős and Rényi's original paper on random graphs , are as follows: In the case that $p = (1 - \Omega(1)) \cdot \tfrac1n$, the graph consists almost entirely of trees of size $O(\log n)$, and a finite number of components which contain a single cycle. In the case that $p = (1 + \Omega(1)) \cdot \tfrac1n$, the graph consists of a single ""giant"" component which contains a positive fraction of all the vertices and has multiple cycles, together with a finite number of components which contain a single cycle and trees of size $O(\log n)$. It is clear from this that between $p = n^{-1} \pm \Theta(n^{-1})$, the probability that the graph contains a component which has multiple cycles goes from 0 to 1. Furthermore, for $p = \tfrac1n$ precisely, we expect the number of vertices which belong to a component which has a single cycle also to scale as $\Theta(n^{2/3})$: connecting any two of these vertices (even in the same component) will result in a component with two cycles, so for $n^{-1} + \omega(n^{-4/3})$, we should already expect to see components with multiple cycles. What I'm interested is: what is known about the growth of functions $f(n)$ for which $p = n^{-1} + \Theta(f(n))$ implies that there is almost surely a component with multiple cycles? How much better can we do than $f(n) \in \omega(n^{-4/3})$? For instance: by Theorem 8a of Erdős and Rényi's original paper on the evolution of random graphs, we would expect an increasing probability, bounded only by zero and one, of having a single cycle with at least one diagonal edge, for $p = \tfrac{1}{n}(1 + c\tfrac{1}{\sqrt n})$, for any constant $c$; this would show that any $f(n) \in \omega(n^{-3/2})$ would suffice. However, a problem with this Theorem was discovered by Łuczak and Wierman, so such a threshold does not seem to hold. It would also be unnecessarily restrictive even if true, as it would describe the special case of a component which has two cycles which overlap on an edge: one could hope to show the existence (for instance) of components with two cycles which may be attached to distant leaves of a large tree, with a smaller $f$. How small a function $f$ can we find a transition from every component being a tree or unicyclic, to there being components with multiple cycles?","An Erdős-Rényi graph is a random graph, selected according to the distribution obtained one where we have some number $n$ of nodes, and some probability $p$ of each potential edge being present. One often asks questions of this distribution when $p$ is taken as a function of $n$, and we take the limit as $n \to \infty$. Classic results on properties of the graph which hold almost surely, from Erdős and Rényi's original paper on random graphs , are as follows: In the case that $p = (1 - \Omega(1)) \cdot \tfrac1n$, the graph consists almost entirely of trees of size $O(\log n)$, and a finite number of components which contain a single cycle. In the case that $p = (1 + \Omega(1)) \cdot \tfrac1n$, the graph consists of a single ""giant"" component which contains a positive fraction of all the vertices and has multiple cycles, together with a finite number of components which contain a single cycle and trees of size $O(\log n)$. It is clear from this that between $p = n^{-1} \pm \Theta(n^{-1})$, the probability that the graph contains a component which has multiple cycles goes from 0 to 1. Furthermore, for $p = \tfrac1n$ precisely, we expect the number of vertices which belong to a component which has a single cycle also to scale as $\Theta(n^{2/3})$: connecting any two of these vertices (even in the same component) will result in a component with two cycles, so for $n^{-1} + \omega(n^{-4/3})$, we should already expect to see components with multiple cycles. What I'm interested is: what is known about the growth of functions $f(n)$ for which $p = n^{-1} + \Theta(f(n))$ implies that there is almost surely a component with multiple cycles? How much better can we do than $f(n) \in \omega(n^{-4/3})$? For instance: by Theorem 8a of Erdős and Rényi's original paper on the evolution of random graphs, we would expect an increasing probability, bounded only by zero and one, of having a single cycle with at least one diagonal edge, for $p = \tfrac{1}{n}(1 + c\tfrac{1}{\sqrt n})$, for any constant $c$; this would show that any $f(n) \in \omega(n^{-3/2})$ would suffice. However, a problem with this Theorem was discovered by Łuczak and Wierman, so such a threshold does not seem to hold. It would also be unnecessarily restrictive even if true, as it would describe the special case of a component which has two cycles which overlap on an edge: one could hope to show the existence (for instance) of components with two cycles which may be attached to distant leaves of a large tree, with a smaller $f$. How small a function $f$ can we find a transition from every component being a tree or unicyclic, to there being components with multiple cycles?",,"['probability', 'graph-theory', 'asymptotics', 'random-graphs']"
59,$4$ dogs have been born in the same week. What is the probability that they were born on different days?,dogs have been born in the same week. What is the probability that they were born on different days?,4,"$4$ dogs have been born at a dog kennel in the same week. What is the probability that   they were born on different days? I did: $$\frac{^7C_4}{7^4}$$ But my book says the solution is: $\frac{120}{7^3}$ What did I do wrong? EDIT: I copied the problem exactly as it is in my book. If it is missing information, poorly thought or doesn't make any sense, that's not my fault. Typos, mistakes and low quality abound in these schoolbooks.","$4$ dogs have been born at a dog kennel in the same week. What is the probability that   they were born on different days? I did: $$\frac{^7C_4}{7^4}$$ But my book says the solution is: $\frac{120}{7^3}$ What did I do wrong? EDIT: I copied the problem exactly as it is in my book. If it is missing information, poorly thought or doesn't make any sense, that's not my fault. Typos, mistakes and low quality abound in these schoolbooks.",,"['probability', 'combinatorics']"
60,"Formally, why does a logical contradiction have probability zero?","Formally, why does a logical contradiction have probability zero?",,"In terms of formal probability theory, why does an event representing a logical contradiction (such as $A \wedge \neg A$) always have probability zero? I understand intuitively why this is the case, but I don't see anything in the axioms of probability that directly explains why $A \wedge \neg A$ must have probability zero (or, equivalently, why a logical tautology such as $A \vee \neg A$ must have probability 1).","In terms of formal probability theory, why does an event representing a logical contradiction (such as $A \wedge \neg A$) always have probability zero? I understand intuitively why this is the case, but I don't see anything in the axioms of probability that directly explains why $A \wedge \neg A$ must have probability zero (or, equivalently, why a logical tautology such as $A \vee \neg A$ must have probability 1).",,"['probability', 'probability-theory']"
61,"If you roll two fair six-sided dice, what is the probability that the sum is 4 or higher?","If you roll two fair six-sided dice, what is the probability that the sum is 4 or higher?",,"If you roll two fair six-sided dice, what is the probability that the sum is $4$ or higher? The answer is $\frac{33}{36}$ or $\frac{11}{12}$. I understand how to arrive at this answer. What I don't understand is why the answer isn't $\frac{9}{11}$? When summing the results after rolling two fair six sided dice, there are $11$ equally possible outcomes: $2$ through $12$. Two of these outcomes are below four, meaning $9$ are greater than or equal to four which is how I arrived at $\frac{9}{11}$. Can someone help explain why that is wrong?","If you roll two fair six-sided dice, what is the probability that the sum is $4$ or higher? The answer is $\frac{33}{36}$ or $\frac{11}{12}$. I understand how to arrive at this answer. What I don't understand is why the answer isn't $\frac{9}{11}$? When summing the results after rolling two fair six sided dice, there are $11$ equally possible outcomes: $2$ through $12$. Two of these outcomes are below four, meaning $9$ are greater than or equal to four which is how I arrived at $\frac{9}{11}$. Can someone help explain why that is wrong?",,"['probability', 'dice']"
62,On the probability of getting the same number for three dice,On the probability of getting the same number for three dice,,"I found the probability of having the same number when throwing 3 dice to be $1\times\left(\frac16\right)^2$. In addition, I don't understand how do people get the equation $\left(\frac16\right)^3\times6=\frac1{36}$, like why do we have to multiply $\left(\frac16\right)^3$ by six?","I found the probability of having the same number when throwing 3 dice to be $1\times\left(\frac16\right)^2$. In addition, I don't understand how do people get the equation $\left(\frac16\right)^3\times6=\frac1{36}$, like why do we have to multiply $\left(\frac16\right)^3$ by six?",,"['probability', 'dice']"
63,How to reasonably estimate the probability of your father being exactly 12222 days older than you?,How to reasonably estimate the probability of your father being exactly 12222 days older than you?,,"On 2024/05/03, my brother is 10,000 days of age, while my father will be 22,222 days old. To tell them as a fun-fact, I would like to grab a feel of how special this is. I have no access to datasets that could provide me a distribution of the likelihood of becoming a parent at n days of age. It doesn't matter whether this child is the father's first child. (Off-topic: I asked chat gpt for inspiration on how to celebrate this event, and to my surprise it was quite creative as long as it didn't have to organize it itself.) Kind regards from Belgium","On 2024/05/03, my brother is 10,000 days of age, while my father will be 22,222 days old. To tell them as a fun-fact, I would like to grab a feel of how special this is. I have no access to datasets that could provide me a distribution of the likelihood of becoming a parent at n days of age. It doesn't matter whether this child is the father's first child. (Off-topic: I asked chat gpt for inspiration on how to celebrate this event, and to my surprise it was quite creative as long as it didn't have to organize it itself.) Kind regards from Belgium",,['probability']
64,Proving $|P(A\cap B)-P(A)P(B)|\leq \frac{1}4$,Proving,|P(A\cap B)-P(A)P(B)|\leq \frac{1}4,"Let $A$ and $B$ be two events of a probability space. Prove that  $\displaystyle|P(A\cap B)-P(A)P(B)|\leq \frac{1}4$ I think it's a very challenging problem, and I've made no progress so far ... Can someone give me a hint ?","Let $A$ and $B$ be two events of a probability space. Prove that  $\displaystyle|P(A\cap B)-P(A)P(B)|\leq \frac{1}4$ I think it's a very challenging problem, and I've made no progress so far ... Can someone give me a hint ?",,"['probability', 'inequality']"
65,What's the expected number of times I have to roll two die until they both sum $7$?,What's the expected number of times I have to roll two die until they both sum ?,7,"Here is my guess: the probability of summing $7$ on two rolls is $\frac 16$. This means if I repeat the experiment many times I'll roll $7$ one sixth of them (approximately). Hence, $$N \cdot \bigg(\cfrac 16\bigg) \cdot 7 = 7$$ where $N$ is the total number of rolls. That gives me a total number of $6$ rolls on average to sum $7$. I'm not quite sure so I'm all open to suggestions! Thanks in advance.","Here is my guess: the probability of summing $7$ on two rolls is $\frac 16$. This means if I repeat the experiment many times I'll roll $7$ one sixth of them (approximately). Hence, $$N \cdot \bigg(\cfrac 16\bigg) \cdot 7 = 7$$ where $N$ is the total number of rolls. That gives me a total number of $6$ rolls on average to sum $7$. I'm not quite sure so I'm all open to suggestions! Thanks in advance.",,"['probability', 'expectation', 'dice']"
66,What does a probability of $1$ mean?,What does a probability of  mean?,1,"From a textbook on probability on the Law of Large Numbers: Theorem 3-19 (Law of Large Numbers): Let $X_1,X_2, \ldots , X_n$ be mutually independent random variables (discrete or continuous), each having finite mean  and variance. Then   if $S_n = X_1 + X_2 +\dots+ X_n$, $$ \lim_{n \to\infty} P\left(\left|\frac{S_n}{n} - \mu\right| \geq \varepsilon\right) = 0 $$ Since $S_n$ is the arithmetic mean of $X_1,X_2, \ldots , X_n$, this theorem states that the probability of the arithmetic mean $\frac{S_n}{n}$ differing from its expected value $\mu$ by more than $\varepsilon$ approaches zero as $n \to \infty$. A stronger result, which we might expect to be true, is that $ \lim_{n \to\infty} \frac{S_n}{n} = \mu $  but this is actually false. However, we can prove that $ \lim_{n \to\infty} \frac{S_n}{n} = \mu $ with probability one. The only difference between the last sentence and the one before that is the phrase 'with probability one'. What does probability one mean here ? The usual definition is that the event occurs with 100% certainty. If that is the case, why is the original assertion $ \lim_{n \to\infty} \frac{S_n}{n} = \mu $ false ?","From a textbook on probability on the Law of Large Numbers: Theorem 3-19 (Law of Large Numbers): Let $X_1,X_2, \ldots , X_n$ be mutually independent random variables (discrete or continuous), each having finite mean  and variance. Then   if $S_n = X_1 + X_2 +\dots+ X_n$, $$ \lim_{n \to\infty} P\left(\left|\frac{S_n}{n} - \mu\right| \geq \varepsilon\right) = 0 $$ Since $S_n$ is the arithmetic mean of $X_1,X_2, \ldots , X_n$, this theorem states that the probability of the arithmetic mean $\frac{S_n}{n}$ differing from its expected value $\mu$ by more than $\varepsilon$ approaches zero as $n \to \infty$. A stronger result, which we might expect to be true, is that $ \lim_{n \to\infty} \frac{S_n}{n} = \mu $  but this is actually false. However, we can prove that $ \lim_{n \to\infty} \frac{S_n}{n} = \mu $ with probability one. The only difference between the last sentence and the one before that is the phrase 'with probability one'. What does probability one mean here ? The usual definition is that the event occurs with 100% certainty. If that is the case, why is the original assertion $ \lim_{n \to\infty} \frac{S_n}{n} = \mu $ false ?",,"['probability', 'law-of-large-numbers']"
67,Are irrational numbers completely random?,Are irrational numbers completely random?,,As far as I know the decimal numbers in any irrational appear randomly showing no pattern. Hence it may not be possible to predict the $n^{th}$ decimal point without any calculations. So I was wondering if there is a chance that somewhere down the line in the infinite list of decimal numbers in an irrational could reveal something like our date of birth in order (eg: 19901225) or a even a paragraph in binary that would reveal something meaningful. Since this a infinite sequence of random numbers ; Is it possible to calculate the probability of a birthday (say 19901225) appearing in order inside the sequence? Does the probability approach to 1 since this is an infinite series. Any discussions and debate will be welcomed.,As far as I know the decimal numbers in any irrational appear randomly showing no pattern. Hence it may not be possible to predict the $n^{th}$ decimal point without any calculations. So I was wondering if there is a chance that somewhere down the line in the infinite list of decimal numbers in an irrational could reveal something like our date of birth in order (eg: 19901225) or a even a paragraph in binary that would reveal something meaningful. Since this a infinite sequence of random numbers ; Is it possible to calculate the probability of a birthday (say 19901225) appearing in order inside the sequence? Does the probability approach to 1 since this is an infinite series. Any discussions and debate will be welcomed.,,"['probability', 'sequences-and-series', 'random', 'infinity', 'irrational-numbers']"
68,Independence intuition,Independence intuition,,"Toss two fair dice. There are $36$ outcomes in the sample space $\Omega$, each with probability $\frac{1}{36}$. Let: $A$ be the event '$4$ on first die'. $B$ be the event 'sum of numbers is $7$'. $C$ be the event 'sum of numbers is $8$'. It says here $A$ and $B$ are independent. I don't understand why this is the case. What is the intuition behind this? Can someone offer an explanation to me that doesn't involve using the definition of $\mathbb{P}(A \cap B) = \mathbb{P}(A)\mathbb{P}(B)$? My understanding is informally, an event is independent if the occurrence of one does not affect the probability of the other and vise versa. So if $A$ occurs, wouldn't that affect the probability of $B$? Since if I were to roll a $4$ on the first die, the sample space will be reduced and hence the probability of 'sum of numbers is $7$' will be affected? It also says $A$ and $C$ are not independent and $B$ and $C$ are not independent. Why? I think this is because I'm confusing independence with conditional probability?","Toss two fair dice. There are $36$ outcomes in the sample space $\Omega$, each with probability $\frac{1}{36}$. Let: $A$ be the event '$4$ on first die'. $B$ be the event 'sum of numbers is $7$'. $C$ be the event 'sum of numbers is $8$'. It says here $A$ and $B$ are independent. I don't understand why this is the case. What is the intuition behind this? Can someone offer an explanation to me that doesn't involve using the definition of $\mathbb{P}(A \cap B) = \mathbb{P}(A)\mathbb{P}(B)$? My understanding is informally, an event is independent if the occurrence of one does not affect the probability of the other and vise versa. So if $A$ occurs, wouldn't that affect the probability of $B$? Since if I were to roll a $4$ on the first die, the sample space will be reduced and hence the probability of 'sum of numbers is $7$' will be affected? It also says $A$ and $C$ are not independent and $B$ and $C$ are not independent. Why? I think this is because I'm confusing independence with conditional probability?",,"['probability', 'dice', 'independence']"
69,Most efficient strategy for guessing outcome of (fair) dice roll?,Most efficient strategy for guessing outcome of (fair) dice roll?,,"I'm starting to learn about information theory and I'm a bit stuck on this one. here's what I have so far: 1 possible strategy is to simply ask 'did outcome 1 occur?' if yes then we have our answer, if no we ask again 'did outcome 2 occur' etc. for a dice, the maximum number of questions with this strategy would be 5, since if for 1 - 5 the answer is no, that must mean that the outcome 6 must be positive. Based on my calculation, the average number of questions using this strategy is 2.5 (sum from 1 to 5 of $Q/6$ , where Q is number of questions and 1/6 is the probabilty of rolling any of the faces on the dice). Another strategy I thought of would be to split the probabilties – i.e. is the outcome even? if yes, we could ask 'is it greater than 3?' and then we either have the outcome (6) or we ask 'is it 4?' and from here we have a definitive answer. Likewise for the case of the first answer being no i.e. the number is odd. I struggled to calculate the average number of questions for this. My logic was we must ask at least 2 questions for the answer to be fully determined. So, we must ask 1 question and then the outcome of the second question is subject to probabilty. Hence the average number of questions is: $1+(2/6)+(3/6)=1.83333...$ Is this right? Is my logic correct? Are there any other strategies worth looking at? I'm really enjoying imformation theory and am really keen to learn more more more!","I'm starting to learn about information theory and I'm a bit stuck on this one. here's what I have so far: 1 possible strategy is to simply ask 'did outcome 1 occur?' if yes then we have our answer, if no we ask again 'did outcome 2 occur' etc. for a dice, the maximum number of questions with this strategy would be 5, since if for 1 - 5 the answer is no, that must mean that the outcome 6 must be positive. Based on my calculation, the average number of questions using this strategy is 2.5 (sum from 1 to 5 of , where Q is number of questions and 1/6 is the probabilty of rolling any of the faces on the dice). Another strategy I thought of would be to split the probabilties – i.e. is the outcome even? if yes, we could ask 'is it greater than 3?' and then we either have the outcome (6) or we ask 'is it 4?' and from here we have a definitive answer. Likewise for the case of the first answer being no i.e. the number is odd. I struggled to calculate the average number of questions for this. My logic was we must ask at least 2 questions for the answer to be fully determined. So, we must ask 1 question and then the outcome of the second question is subject to probabilty. Hence the average number of questions is: Is this right? Is my logic correct? Are there any other strategies worth looking at? I'm really enjoying imformation theory and am really keen to learn more more more!",Q/6 1+(2/6)+(3/6)=1.83333...,"['probability', 'information-theory', 'dice']"
70,Why is my intuition wrong that one of two archers win in a tournament?,Why is my intuition wrong that one of two archers win in a tournament?,,"The probability of Robin and Tuck hitting a target respectively are $0.45$ and $0.38$.  For each round of the tournament, each archer must shoot simultaneously at their targets. A player wins the tournament if in a round, one archer hits the target while the other does not. If both or neither archers hit the target, then the tournament is continued with another round and this process is repeated. Intuitively, I feel that the probability that Robin wins the tournament is $p=\frac{0.45}{0.38+0.45}\approx0.5421686%$, but to be on the safe side, I did the below: Consider the two cases in which Robin wins the tournament:  Either a) Robin wins on his first shot, or b) Robin ties on his first shot but wins on the $n^{\text{th}}$ round, $n > 1$. Let $p$ denote the probability that robin wins. Case a: If Robin wins on his first shot, that means he hit the target and Tuck missed the target. The probability of this is $(0.45)(1-0.38)=0.279$ Case b: The probability that no one wins on the first round is the probability that no one hits the target plus the probability that both hit the target, which is $(1-0.45)(1-0.38)+(0.45)(0.38)=0.512$. After the first round, when the second round starts, it is as if the entire tournament has just started. Therefore, after a tie in the $1^{\text{st}}$ round, Robin still has the same probability $p$ of winning on the $n^{\text{th}}$ round where $n>1$. Thus the probability that Robin ties the 1st round and wins some round after that is $0.512p$. Adding cases a and b gets that $p = 0.279 + 0.512p$. Solving for $p$ yields around  $0.5717213115$. What accounts for the discrepancy between my intuition and my reasoning above? Is my reasoning flawed?","The probability of Robin and Tuck hitting a target respectively are $0.45$ and $0.38$.  For each round of the tournament, each archer must shoot simultaneously at their targets. A player wins the tournament if in a round, one archer hits the target while the other does not. If both or neither archers hit the target, then the tournament is continued with another round and this process is repeated. Intuitively, I feel that the probability that Robin wins the tournament is $p=\frac{0.45}{0.38+0.45}\approx0.5421686%$, but to be on the safe side, I did the below: Consider the two cases in which Robin wins the tournament:  Either a) Robin wins on his first shot, or b) Robin ties on his first shot but wins on the $n^{\text{th}}$ round, $n > 1$. Let $p$ denote the probability that robin wins. Case a: If Robin wins on his first shot, that means he hit the target and Tuck missed the target. The probability of this is $(0.45)(1-0.38)=0.279$ Case b: The probability that no one wins on the first round is the probability that no one hits the target plus the probability that both hit the target, which is $(1-0.45)(1-0.38)+(0.45)(0.38)=0.512$. After the first round, when the second round starts, it is as if the entire tournament has just started. Therefore, after a tie in the $1^{\text{st}}$ round, Robin still has the same probability $p$ of winning on the $n^{\text{th}}$ round where $n>1$. Thus the probability that Robin ties the 1st round and wins some round after that is $0.512p$. Adding cases a and b gets that $p = 0.279 + 0.512p$. Solving for $p$ yields around  $0.5717213115$. What accounts for the discrepancy between my intuition and my reasoning above? Is my reasoning flawed?",,['probability']
71,A fair coin is tossed $n$ times by two people. What is the probability that they get same number of heads?,A fair coin is tossed  times by two people. What is the probability that they get same number of heads?,n,"Say we have Tom and John, each tosses a fair coin $n$ times. What is the probability that they get same number of heads? I tried to do it this way: individually, the probability of getting $k$ heads for each is equal to $$\binom{n}{k} \Big(\frac12\Big)^n.$$ So, we can do $$\sum^{n}_{k=0} \left( \binom{n}{k} \Big(\frac12\Big)^n \cdot \binom{n}{k}\Big(\frac12\Big)^n \right)$$ which results into something very ugly. This ugly thing is equal to the 'simple' answer in the back of the book: $\binom{2n}{n}\left(\frac12\right)^{2n},$ but the equality was verified by WolframAlpha -- it's not obvious when you look at it. So I think there's a much easier way to solve this, can someone point it out? Thanks.","Say we have Tom and John, each tosses a fair coin $n$ times. What is the probability that they get same number of heads? I tried to do it this way: individually, the probability of getting $k$ heads for each is equal to $$\binom{n}{k} \Big(\frac12\Big)^n.$$ So, we can do $$\sum^{n}_{k=0} \left( \binom{n}{k} \Big(\frac12\Big)^n \cdot \binom{n}{k}\Big(\frac12\Big)^n \right)$$ which results into something very ugly. This ugly thing is equal to the 'simple' answer in the back of the book: $\binom{2n}{n}\left(\frac12\right)^{2n},$ but the equality was verified by WolframAlpha -- it's not obvious when you look at it. So I think there's a much easier way to solve this, can someone point it out? Thanks.",,"['probability', 'binomial-coefficients']"
72,Prove that the sample median is an unbiased estimator,Prove that the sample median is an unbiased estimator,,"My book says that sample median of a normal distribution is an unbiased estimator of its mean, by virtue of the symmetry of normal distribution. Please advice how can this be proved.","My book says that sample median of a normal distribution is an unbiased estimator of its mean, by virtue of the symmetry of normal distribution. Please advice how can this be proved.",,"['probability', 'statistics', 'order-statistics', 'median']"
73,Is correlation (in some sense) transitive?,Is correlation (in some sense) transitive?,,"If we know that A has some correlation with B ($\rho_{AB}$), and that B has some with C ($\rho_{BC}$), is there something we know to say about the correlation between A and C ($\rho_{AC}$)? Thanks.","If we know that A has some correlation with B ($\rho_{AB}$), and that B has some with C ($\rho_{BC}$), is there something we know to say about the correlation between A and C ($\rho_{AC}$)? Thanks.",,"['probability', 'correlation']"
74,Random walk on a cube,Random walk on a cube,,"Start a random walk on a vertex of a cube, with equal probability going along the three edges that you can see (to another vertex). what is the expected number of steps to reach the opposite vertex that you start with?","Start a random walk on a vertex of a cube, with equal probability going along the three edges that you can see (to another vertex). what is the expected number of steps to reach the opposite vertex that you start with?",,"['probability', 'expectation', 'mathematical-modeling', 'random-walk', 'negative-binomial']"
75,What is the insight behind the Lebesgue integral?,What is the insight behind the Lebesgue integral?,,"Edit 3: OK, I had an insight, inspired in part by Ben-Blum Smith's comment, and the post he linked to.  (I have no idea if this insight is right; it's barely a hunch, and that's why I'm not submitting it as an answer to my original question [see below], but I'll throw it out there for criticism.)  I vaguely remember a theorem on the rearrangement the terms of infinite series showing that, at least for some series, for any $L$, it was possible to find a rearrangement of the terms that would cause the rearranged series to ""converge"" to $L$.  IIRC, this fact was somehow related to the such series were somewhat difficult to work with.  In any case, allowing any rearrangement of the terms, it seems, causes problems down the line, but if we were able to agree upon some reasonable canonicalization of the rearrangement procedure, or to put it differently, if we could find a reasonable way to define admissible rearrangements , this could be the basis for a new definition of an infinite series such that more series would converge (i.e. such that there would be a more robust definition of the series' limiting value).  One such canonicalization would go like this: divide the range of the series by some standard mesh (e.g. by numbers of the form $m/2^n$, for some fixed integer $n$, and all integers $m$ in some suitable interval).  Now rearrange the series by collecting together all the terms of the original series whose values lie within the same cell of the mesh, and ordering these ""grouped terms"" in decreasing order of their cell's value.  Then the series could be approximated by a weighted sum of these cell values.  This canonicalized sum would be immune to the pathologies caused by arbitrary rearrangements.  One would then need to show that this procedure behaves reasonably as one lets the size of the mesh go to zero. Could something like this lead to the Lebesgue integral? Edit 4: One point I did not make sufficiently clear in ""Edit 3"" is that one could rationalize the concern with rearrangements by noting that one property that would be desirable in a definition of an integral is that it remains invariant relative to order in which the sum is performed.  The standard definition of the Riemann integral, as a sum that is ordered along the domain of integration, is susceptible to all the problems that one runs into when reordering infinite series (as alluded to above), which is unfortunate, considering that, when one works with sufficiently general functions, the ordering of the domain of integration should be irrelevant to the value of the integral: it should be possible to rearrange the domain of integration in fairly crazy ways and end up with the exact same integral.  In contrast, the procedure described in Edit 3 eliminates this dependence on any particular ordering of the domain of integration, and replaces it with a focus on the ordering of the range, which at least has a meaningful relationship to the value of the integral .  Also, it is clear why this alternative viewpoint shifts the focus to the problem of defining ""measures"" on sets, since these ""measures"" are precisely the weights assigned to the grouped terms in the new summation procedure. [Original post] I understand the definitions of the Lebesgue integral and of the Riemann integral, but it is not obvious to me from these definitions that the former would be the more suitable of the two for, say, probability theory, or for defining the Fourier transform, or for defining inner products in Hilbert space, etc. What was exactly the insight that led to the Lebesgue integral as a superior alternative to the Riemann integral? Thanks! Edit 1: after looking at the comments/answers so far (with one exception) I realized that the leading wording of my question was quite inept.  I've highlighted the part of my original question that is least ineptly put.  I know that Lebesgue integrals have all the advantages cited, but it is utterly mystifying to me how anyone could have seen ahead of time that this had to be the case.  What made this alternative approach (of the Lebesgue integral) look promising?  I know that it's perfectly possible that there was no clue that this approach would be fruitful: someone (Lebesgue, I suppose) just tried it, played with it for a while, and discovered all these unsuspected benefits, and had the latter not happened, the new approach would have been quietly forgotten.  But, just in case there is some clue (even a ""clue-in-hindsight"") of the alternative POV's superiority, I would love to see it.  (The ""one exception"" I mentioned above is Christian Blatter's answer, which goes in the direction I was trying to get at, but I'd like to give my question a second shot.) Edit 2: To be fair to the commenters, I have not finished digesting all the previous posts linked to in the comments; they may very well be what I was looking for.  (In particular, the one with the metaphor featuring a shop owner, etc., may be just the thing.)","Edit 3: OK, I had an insight, inspired in part by Ben-Blum Smith's comment, and the post he linked to.  (I have no idea if this insight is right; it's barely a hunch, and that's why I'm not submitting it as an answer to my original question [see below], but I'll throw it out there for criticism.)  I vaguely remember a theorem on the rearrangement the terms of infinite series showing that, at least for some series, for any $L$, it was possible to find a rearrangement of the terms that would cause the rearranged series to ""converge"" to $L$.  IIRC, this fact was somehow related to the such series were somewhat difficult to work with.  In any case, allowing any rearrangement of the terms, it seems, causes problems down the line, but if we were able to agree upon some reasonable canonicalization of the rearrangement procedure, or to put it differently, if we could find a reasonable way to define admissible rearrangements , this could be the basis for a new definition of an infinite series such that more series would converge (i.e. such that there would be a more robust definition of the series' limiting value).  One such canonicalization would go like this: divide the range of the series by some standard mesh (e.g. by numbers of the form $m/2^n$, for some fixed integer $n$, and all integers $m$ in some suitable interval).  Now rearrange the series by collecting together all the terms of the original series whose values lie within the same cell of the mesh, and ordering these ""grouped terms"" in decreasing order of their cell's value.  Then the series could be approximated by a weighted sum of these cell values.  This canonicalized sum would be immune to the pathologies caused by arbitrary rearrangements.  One would then need to show that this procedure behaves reasonably as one lets the size of the mesh go to zero. Could something like this lead to the Lebesgue integral? Edit 4: One point I did not make sufficiently clear in ""Edit 3"" is that one could rationalize the concern with rearrangements by noting that one property that would be desirable in a definition of an integral is that it remains invariant relative to order in which the sum is performed.  The standard definition of the Riemann integral, as a sum that is ordered along the domain of integration, is susceptible to all the problems that one runs into when reordering infinite series (as alluded to above), which is unfortunate, considering that, when one works with sufficiently general functions, the ordering of the domain of integration should be irrelevant to the value of the integral: it should be possible to rearrange the domain of integration in fairly crazy ways and end up with the exact same integral.  In contrast, the procedure described in Edit 3 eliminates this dependence on any particular ordering of the domain of integration, and replaces it with a focus on the ordering of the range, which at least has a meaningful relationship to the value of the integral .  Also, it is clear why this alternative viewpoint shifts the focus to the problem of defining ""measures"" on sets, since these ""measures"" are precisely the weights assigned to the grouped terms in the new summation procedure. [Original post] I understand the definitions of the Lebesgue integral and of the Riemann integral, but it is not obvious to me from these definitions that the former would be the more suitable of the two for, say, probability theory, or for defining the Fourier transform, or for defining inner products in Hilbert space, etc. What was exactly the insight that led to the Lebesgue integral as a superior alternative to the Riemann integral? Thanks! Edit 1: after looking at the comments/answers so far (with one exception) I realized that the leading wording of my question was quite inept.  I've highlighted the part of my original question that is least ineptly put.  I know that Lebesgue integrals have all the advantages cited, but it is utterly mystifying to me how anyone could have seen ahead of time that this had to be the case.  What made this alternative approach (of the Lebesgue integral) look promising?  I know that it's perfectly possible that there was no clue that this approach would be fruitful: someone (Lebesgue, I suppose) just tried it, played with it for a while, and discovered all these unsuspected benefits, and had the latter not happened, the new approach would have been quietly forgotten.  But, just in case there is some clue (even a ""clue-in-hindsight"") of the alternative POV's superiority, I would love to see it.  (The ""one exception"" I mentioned above is Christian Blatter's answer, which goes in the direction I was trying to get at, but I'd like to give my question a second shot.) Edit 2: To be fair to the commenters, I have not finished digesting all the previous posts linked to in the comments; they may very well be what I was looking for.  (In particular, the one with the metaphor featuring a shop owner, etc., may be just the thing.)",,"['probability', 'analysis', 'integration', 'measure-theory', 'functional-analysis']"
76,"$P(X>0,Y>0)$ for a bivariate normal distribution with correlation $\rho$",for a bivariate normal distribution with correlation,"P(X>0,Y>0) \rho","$X$ and $Y$ have a bivariate normal distribution with $\rho$ as covariance. $X$ and $Y$ are standard normal variables. I showed that $X$ and $Z= \dfrac{Y-\rho X}{\sqrt{1-\rho^2}}$ are independent standard normal variables. Using this I need to show that $$P(X >0,Y>0) = \frac14 + \frac{1}{2\pi} \cdot\arcsin(\rho).$$",and have a bivariate normal distribution with as covariance. and are standard normal variables. I showed that and are independent standard normal variables. Using this I need to show that,"X Y \rho X Y X Z= \dfrac{Y-\rho X}{\sqrt{1-\rho^2}} P(X >0,Y>0) = \frac14 + \frac{1}{2\pi} \cdot\arcsin(\rho).","['probability', 'probability-distributions', 'normal-distribution', 'bivariate-distributions']"
77,"Expected number of dice rolls before rolling ""1,2,3,4,5,6""","Expected number of dice rolls before rolling ""1,2,3,4,5,6""",,"QUESTION: I roll a single six-sided die repeatedly, recording the outcomes in a string of digits. I stop as soon as the string contains "" $123456$ "". What is the expected length of the string? My answer so far: My initial approach is to try and find the probability mass function. If we let the random variable $X$ be the length of the string, then we can easily calculate for $x\in\{6,\ldots,11\}$ , $$\mathbb{P}(X=x) = \left(\frac{1}{6}\right)^6$$ and zero for $x<6$ . As soon as we reach $x\ge12$ , we need to consider the probability that the final six rolls are "" $123456$ "" but that sequence isn't contained in the string before that. I believe the result for $x\in\{12,\ldots,17\}$ becomes $$\mathbb{P}(X=x) = \left(\frac{1}{6}\right)^6 - \left(\frac{1}{6}\right)^{12}(x-11).$$ Now for $x\ge18$ , we will need an extra term to discount the cases when two instances of "" $123456$ "" are contained before the final six rolls. And indeed every time we reach another multiple of six, we need to consider the number of ways of having so many instances of the string before the final six rolls. I've messed around with this counting problem but I'm getting bogged down in the calculations. Any input is appreciated to help shed some light on this. Thanks!","QUESTION: I roll a single six-sided die repeatedly, recording the outcomes in a string of digits. I stop as soon as the string contains "" "". What is the expected length of the string? My answer so far: My initial approach is to try and find the probability mass function. If we let the random variable be the length of the string, then we can easily calculate for , and zero for . As soon as we reach , we need to consider the probability that the final six rolls are "" "" but that sequence isn't contained in the string before that. I believe the result for becomes Now for , we will need an extra term to discount the cases when two instances of "" "" are contained before the final six rolls. And indeed every time we reach another multiple of six, we need to consider the number of ways of having so many instances of the string before the final six rolls. I've messed around with this counting problem but I'm getting bogged down in the calculations. Any input is appreciated to help shed some light on this. Thanks!","123456 X x\in\{6,\ldots,11\} \mathbb{P}(X=x) = \left(\frac{1}{6}\right)^6 x<6 x\ge12 123456 x\in\{12,\ldots,17\} \mathbb{P}(X=x) = \left(\frac{1}{6}\right)^6 - \left(\frac{1}{6}\right)^{12}(x-11). x\ge18 123456","['probability', 'combinatorics', 'probability-theory', 'expected-value', 'dice']"
78,Probability of a number rolled of a 20 sided dice being greater than the sum of the numbers rolled on 3 six sided die.,Probability of a number rolled of a 20 sided dice being greater than the sum of the numbers rolled on 3 six sided die.,,"Bob rolls $3$ six-sided die and sums the numbers facing up. Bill rolls a single $20$ sided dice and records the number. What is the probability that Bob's number is greater than Bill's number. I started the problem trying to come up with an equation, but that didn't work, so I resorted to creating $6$ six by six tables with all of the possible sums for Bob's die. Then, I counted the number of each number and created a chart and calculated the probability of each of those numbers occurring. Then, I multiplied the probability of each number occurring by the probability that Bill's number is greater than that number. Finally, I added them all up. Obviously, this was very tedious and time-consuming. Is there a more elegant/less tedious way to do this problem. PS: Is there away to do a $n$ die vs $m$ dice problem without listing them all out? Is there a general formula? Up until now, that's what I've been doing.","Bob rolls $3$ six-sided die and sums the numbers facing up. Bill rolls a single $20$ sided dice and records the number. What is the probability that Bob's number is greater than Bill's number. I started the problem trying to come up with an equation, but that didn't work, so I resorted to creating $6$ six by six tables with all of the possible sums for Bob's die. Then, I counted the number of each number and created a chart and calculated the probability of each of those numbers occurring. Then, I multiplied the probability of each number occurring by the probability that Bill's number is greater than that number. Finally, I added them all up. Obviously, this was very tedious and time-consuming. Is there a more elegant/less tedious way to do this problem. PS: Is there away to do a $n$ die vs $m$ dice problem without listing them all out? Is there a general formula? Up until now, that's what I've been doing.",,"['probability', 'combinatorics', 'dice']"
79,Break a stick at two random points. The probability that the longest piece is at least twice as long as each of the other pieces is $1/2$. Why?,Break a stick at two random points. The probability that the longest piece is at least twice as long as each of the other pieces is . Why?,1/2,"Choose two independent uniformly random points on a stick, and break the stick at that those points. The probability that the longest piece is at least twice as long as each of the other pieces is $1/2$ . My proof below is fairly technical. Given the simplicity of the final answer, I am looking for  a more intuitive explanation that does not involve so much calculation, possibly based on symmetry. ( Here is another question of mine about a breaking a stick at random points, that has an intuitive explanation. And here is one with a simple answer but perhaps no intuitive explanation.) My non-intuitive proof: Assume the stick has length $1$ . Hold the stick horizontally. $x=$ first chosen point's distance from left end. $y=$ second chosen point's distance from left end. In the graph, the blue regions contain combinations that meet the condition in the question. The red region contains combinations that do not meet the condition. The regions can be reflected across the diagonal, by symmetry. I worked out the lines by considering $x$ -values one by one, starting from $x=0$ and going up in small increments. Critical points emerged: $(0,\frac13),(0,\frac23),(\frac14,\frac12),(\frac14,\frac34),(\frac13,\frac13),(\frac13,1),(\frac12,\frac34),(\frac23,\frac23),(\frac23,1)$ . The areas of the blue and red regions are equal, so the probability that the condition is met is $1/2$ . Related fact: The probability that the longest piece is at least twice as long as the shortest piece is exactly $90\text{%}$ (an amusing geometrical probability).","Choose two independent uniformly random points on a stick, and break the stick at that those points. The probability that the longest piece is at least twice as long as each of the other pieces is . My proof below is fairly technical. Given the simplicity of the final answer, I am looking for  a more intuitive explanation that does not involve so much calculation, possibly based on symmetry. ( Here is another question of mine about a breaking a stick at random points, that has an intuitive explanation. And here is one with a simple answer but perhaps no intuitive explanation.) My non-intuitive proof: Assume the stick has length . Hold the stick horizontally. first chosen point's distance from left end. second chosen point's distance from left end. In the graph, the blue regions contain combinations that meet the condition in the question. The red region contains combinations that do not meet the condition. The regions can be reflected across the diagonal, by symmetry. I worked out the lines by considering -values one by one, starting from and going up in small increments. Critical points emerged: . The areas of the blue and red regions are equal, so the probability that the condition is met is . Related fact: The probability that the longest piece is at least twice as long as the shortest piece is exactly (an amusing geometrical probability).","1/2 1 x= y= x x=0 (0,\frac13),(0,\frac23),(\frac14,\frac12),(\frac14,\frac34),(\frac13,\frac13),(\frac13,1),(\frac12,\frac34),(\frac23,\frac23),(\frac23,1) 1/2 90\text{%}","['probability', 'geometry', 'intuition', 'geometric-probability']"
80,A coin is flipped ten times. What is the probability that the first three are heads if an equal number of heads and tails are flipped?,A coin is flipped ten times. What is the probability that the first three are heads if an equal number of heads and tails are flipped?,,"I understand the question but I am not sure how to solve it. For example, if we flip HHHTTTTT then the next three must be heads because of the question. This however seems counterintuitive. I believe that there are $2^{10}$ possible strings, but I am unsure of how to count all possible strings that begin with HHH.","I understand the question but I am not sure how to solve it. For example, if we flip HHHTTTTT then the next three must be heads because of the question. This however seems counterintuitive. I believe that there are $2^{10}$ possible strings, but I am unsure of how to count all possible strings that begin with HHH.",,['probability']
81,How do you prove the almost sure convergence is not (in general) metrizable?,How do you prove the almost sure convergence is not (in general) metrizable?,,How do you prove the almost sure convergence is not (in general) metrizable? Many thanks for your help.,How do you prove the almost sure convergence is not (in general) metrizable? Many thanks for your help.,,"['probability', 'probability-theory', 'convergence-divergence']"
82,Average number of times it takes for something to happen given a chance,Average number of times it takes for something to happen given a chance,,"Given a chance between 0% and 100% of getting something to happen, how would you determine the average amount of tries it will take for that something to happen? I was thinking that $\int_0^\infty \! (1-p)^x \, \mathrm{d} x$ where $p$ is the chance would give the answer, but doesn't that also put in non-integer tries?","Given a chance between 0% and 100% of getting something to happen, how would you determine the average amount of tries it will take for that something to happen? I was thinking that $\int_0^\infty \! (1-p)^x \, \mathrm{d} x$ where $p$ is the chance would give the answer, but doesn't that also put in non-integer tries?",,"['probability', 'sequences-and-series']"
83,Standardizing A Random Variable That is Normally Distributed,Standardizing A Random Variable That is Normally Distributed,,"To standardize a random variable that is normally distributed, it makes absolute sense to subtract the expected value $\mu$ , from each value that the random variable can assume--it shifts all of the values such that the expected value is centered at the origin. But how does dividing by the standard deviation play a role in the standardization of a random variable? That part is not as intuitive to me as is subtracting $\mu$.","To standardize a random variable that is normally distributed, it makes absolute sense to subtract the expected value $\mu$ , from each value that the random variable can assume--it shifts all of the values such that the expected value is centered at the origin. But how does dividing by the standard deviation play a role in the standardization of a random variable? That part is not as intuitive to me as is subtracting $\mu$.",,"['probability', 'statistics', 'normal-distribution', 'random-variables']"
84,Upper bound of expected maximum of weighted sub-gaussian r.v.s,Upper bound of expected maximum of weighted sub-gaussian r.v.s,,"Let $X_1, X_2, \ldots$ be an infinite sequence of sub-Gaussian random variables which are not necessarily independent. My question is how to prove \begin{eqnarray} \mathbb{E}\max_i \frac{|X_i|}{\sqrt{1+\log i}} \leq C K, \end{eqnarray} where $K=\max_i \|X_i\|_{\psi_2}$ . Note that $\|\cdot\|_{\psi_2}$ is the Orlicz norm for sub-Gaussian random variable. Here is my thought that confuses me.... Consider the finite case with $i\leq N$ , we have \begin{eqnarray} \mathbb{E}\max_{i\leq N} \frac{|X_i|}{\sqrt{1+\log i}} &=& \int_0^\infty \mathbb{P}\left(\max_{i\leq N} \frac{|X_i|}{\sqrt{1+\log i}} > t \right) dt \\ &\leq& \int_0^\infty \sum_{i=1}^N\mathbb{P}\left( \frac{|X_i|}{\sqrt{1+\log i}} > t \right) dt \\ &\leq& \sum_{i=1}^N \frac{2}{\sqrt{1+\log i}} \int_0^\infty e^{-cs^2/K^2}ds \\ &=&  K\sqrt{\frac{\pi}{c}} \sum_{i=1}^N \frac{1}{\sqrt{1+\log i}} \end{eqnarray} where the first inequality holds by a simple union bound and the second inequality holds by sub-Gaussianity of $X_i$ (i.e. we have $\mathbb{P}\{|X_i|\geq t\} \leq 2 e^{-ct^2/\|X_i\|_{\psi_2}^2}$ and $c$ is an absolute constant) and a simple trick of change-of-variable (i.e. let $s := t\sqrt{1+\log i}$ ). However, the problem of my proof above is that the sum $\sum_{i=1}^N \frac{1}{\sqrt{1+\log i}}\to\infty$ as $N\to\infty$ . Intuitively, I think the inequalities I used here are not very sharp. But what is the right inequality to use in this case??? This question comes from Exercise 2.5.10 of Prof. Roman Vershynin's book titled as ""High-Dimensional Probability"". The electric version of this book is downloadable from his personal webpage.","Let be an infinite sequence of sub-Gaussian random variables which are not necessarily independent. My question is how to prove where . Note that is the Orlicz norm for sub-Gaussian random variable. Here is my thought that confuses me.... Consider the finite case with , we have where the first inequality holds by a simple union bound and the second inequality holds by sub-Gaussianity of (i.e. we have and is an absolute constant) and a simple trick of change-of-variable (i.e. let ). However, the problem of my proof above is that the sum as . Intuitively, I think the inequalities I used here are not very sharp. But what is the right inequality to use in this case??? This question comes from Exercise 2.5.10 of Prof. Roman Vershynin's book titled as ""High-Dimensional Probability"". The electric version of this book is downloadable from his personal webpage.","X_1, X_2, \ldots \begin{eqnarray}
\mathbb{E}\max_i \frac{|X_i|}{\sqrt{1+\log i}} \leq C K,
\end{eqnarray} K=\max_i \|X_i\|_{\psi_2} \|\cdot\|_{\psi_2} i\leq N \begin{eqnarray}
\mathbb{E}\max_{i\leq N} \frac{|X_i|}{\sqrt{1+\log i}} &=& \int_0^\infty \mathbb{P}\left(\max_{i\leq N} \frac{|X_i|}{\sqrt{1+\log i}} > t \right) dt \\
&\leq& \int_0^\infty \sum_{i=1}^N\mathbb{P}\left( \frac{|X_i|}{\sqrt{1+\log i}} > t \right) dt \\
&\leq& \sum_{i=1}^N \frac{2}{\sqrt{1+\log i}} \int_0^\infty e^{-cs^2/K^2}ds \\
&=&  K\sqrt{\frac{\pi}{c}} \sum_{i=1}^N \frac{1}{\sqrt{1+\log i}}
\end{eqnarray} X_i \mathbb{P}\{|X_i|\geq t\} \leq 2 e^{-ct^2/\|X_i\|_{\psi_2}^2} c s := t\sqrt{1+\log i} \sum_{i=1}^N \frac{1}{\sqrt{1+\log i}}\to\infty N\to\infty","['probability', 'probability-distributions', 'order-statistics']"
85,History: Probability Theory,History: Probability Theory,,"Of course they're both major oversimplifications, but which of (1) and (2) is closer to the truth? Lebesgue invents measure theory and then Kolmogorov notices that measure theory can be used to axiomatize probability theory. Lebesgue invents measure theory, Kolmogorov gives an axiomatization of probability theory, then someone notices the connection.","Of course they're both major oversimplifications, but which of (1) and (2) is closer to the truth? Lebesgue invents measure theory and then Kolmogorov notices that measure theory can be used to axiomatize probability theory. Lebesgue invents measure theory, Kolmogorov gives an axiomatization of probability theory, then someone notices the connection.",,"['probability', 'math-history']"
86,Probability Problem with $n$ keys,Probability Problem with  keys,n,"A woman has $n$ keys, one of which will open a door. a)If she tries the keys at random, discarding those that do not work, what is the probability that she will open the door on her $k^{\mathrm{th}}$ try? Attempt: On her first try, she will have the correct key with probability $\frac1n$. If this does not work, she will throw it away and on her second attempt, she will have the correct key with probability $\frac1{(n-1)}$. So on her $k^{\mathrm{th}}$ try, the probability is $\frac1{(n-(k-1))}$ This does not agree with my solutions. b)The same as above but this time she does not discard the keys if they do not work. Attempt: We want the probability on her $k^{\mathrm{th}}$ try. So we want to consider the probability that she must fail on her $k-1$ attempts. Since she keeps all her keys, the correct one is chosen with probability $\frac1n$ for each trial. So the desired probability is $(1-\frac{1}{n})^{k-1} (\frac1n)^k$. Again, does not agree with solutions. I can't really see any mistake in my logic. Can anyone offer any advice? Many thanks","A woman has $n$ keys, one of which will open a door. a)If she tries the keys at random, discarding those that do not work, what is the probability that she will open the door on her $k^{\mathrm{th}}$ try? Attempt: On her first try, she will have the correct key with probability $\frac1n$. If this does not work, she will throw it away and on her second attempt, she will have the correct key with probability $\frac1{(n-1)}$. So on her $k^{\mathrm{th}}$ try, the probability is $\frac1{(n-(k-1))}$ This does not agree with my solutions. b)The same as above but this time she does not discard the keys if they do not work. Attempt: We want the probability on her $k^{\mathrm{th}}$ try. So we want to consider the probability that she must fail on her $k-1$ attempts. Since she keeps all her keys, the correct one is chosen with probability $\frac1n$ for each trial. So the desired probability is $(1-\frac{1}{n})^{k-1} (\frac1n)^k$. Again, does not agree with solutions. I can't really see any mistake in my logic. Can anyone offer any advice? Many thanks",,['probability']
87,Why is intersection of two independent set probability a multiplication process?,Why is intersection of two independent set probability a multiplication process?,,"Why is the probability of intersection of two independent sets $A$ and $B$, a multiplication of their respective probabilities i.e. Why is $$\mathbb{P}(A \cap B) = \mathbb{P}(A) \cdot \mathbb{P}(B)?$$ this question is about the intuition behind the definition of independence of sets in a probability space","Why is the probability of intersection of two independent sets $A$ and $B$, a multiplication of their respective probabilities i.e. Why is $$\mathbb{P}(A \cap B) = \mathbb{P}(A) \cdot \mathbb{P}(B)?$$ this question is about the intuition behind the definition of independence of sets in a probability space",,['probability']
88,"If n balls are thrown into k bins, what is the probability that every bin gets at least one ball?","If n balls are thrown into k bins, what is the probability that every bin gets at least one ball?",,"If $n$ balls are thrown into $k$ bins (uniformly at random and independently), what is the probability that every bin gets at least one ball?  i.e. If we write $X$ for the number of empty bins, what is $P(X=0)$ ? I was able to calculate the $E(X)$ and thus bound with Markov's inequality $P(X \geq 1) \le E(X)$ but I don't how to work out an exact answer. http://www.inference.phy.cam.ac.uk/mackay/itprnn/ps/588.596.pdf","If balls are thrown into bins (uniformly at random and independently), what is the probability that every bin gets at least one ball?  i.e. If we write for the number of empty bins, what is ? I was able to calculate the and thus bound with Markov's inequality but I don't how to work out an exact answer. http://www.inference.phy.cam.ac.uk/mackay/itprnn/ps/588.596.pdf",n k X P(X=0) E(X) P(X \geq 1) \le E(X),"['probability', 'balls-in-bins']"
89,Must probability density be continuous?,Must probability density be continuous?,,"From other materials that I've read, the probability density of a continuous random variable must itself be continuous. Is this correct? If it is, I don't understand why that would be so, why can't the probability change abruptly?","From other materials that I've read, the probability density of a continuous random variable must itself be continuous. Is this correct? If it is, I don't understand why that would be so, why can't the probability change abruptly?",,['probability']
90,What is average distance from center of square to some point?,What is average distance from center of square to some point?,,How can I calculate average distance from center of a square to points inside the square?,How can I calculate average distance from center of a square to points inside the square?,,['probability']
91,Inverse of binary entropy function for $0 \le x \le \frac{1}{2}$,Inverse of binary entropy function for,0 \le x \le \frac{1}{2},"I'm trying to find the inverse of $H_2(x) = -x \log_2 x - (1-x) \log_2 (1-x)$ [1] subject to $0 \le x \le \frac{1}{2}$. This is for a computation, so an approximation is good enough. My approach was to take the Taylor series at $x=\frac{1}{4}$, cut it off as a quadratic, then find the inverse of that. That yields $$H_2^{-1}(x) \approx -\frac{1}{16} \, \sqrt{-96 \, x \log\left(2\right) + 9 \, \log\left(3\right)^{2} - 72 \, \log\left(3\right) + 96 \, \log\left(4\right)} + \frac{3}{16} \, \log\left(3\right) + \frac{1}{4}$$ Unfortunately, that's a pretty bad approximation and it's complex at $H_2^{-1}(1)$. What other approaches can I take? [1] I originally forgot to write the base 2 subscript (I added that in a later edit)","I'm trying to find the inverse of $H_2(x) = -x \log_2 x - (1-x) \log_2 (1-x)$ [1] subject to $0 \le x \le \frac{1}{2}$. This is for a computation, so an approximation is good enough. My approach was to take the Taylor series at $x=\frac{1}{4}$, cut it off as a quadratic, then find the inverse of that. That yields $$H_2^{-1}(x) \approx -\frac{1}{16} \, \sqrt{-96 \, x \log\left(2\right) + 9 \, \log\left(3\right)^{2} - 72 \, \log\left(3\right) + 96 \, \log\left(4\right)} + \frac{3}{16} \, \log\left(3\right) + \frac{1}{4}$$ Unfortunately, that's a pretty bad approximation and it's complex at $H_2^{-1}(1)$. What other approaches can I take? [1] I originally forgot to write the base 2 subscript (I added that in a later edit)",,"['probability', 'information-theory', 'entropy']"
92,What does it mean to do MLE with a continuous variable,What does it mean to do MLE with a continuous variable,,"I am struggling with the semantics of continuous random variables. For example, we do maximum likelihood estimation, in which we try to find the parameter $\theta$ which, for some observed data $D$, maximizes the likelihood $P(\theta|D)$. But my understanding of this is $$P(\theta = x) = P(x\leq\theta\leq x) = \int_x^xp(t)dt = 0$$ so I am not sure how any $\theta$ can result in a non-zero probability. Intuitively I understand what it means to find the ""most probable"" $\theta$, but I am having trouble uniting it with the formal definition. EDIT: In my class we defined $L(\theta:D)=P(D|\theta)=\prod_i P(D_i|\theta)$ (assuming i.i.d, where $D_i$ are the observations). Then we want to find $\text{argmax}_\theta \prod_i P(D_i|\theta)$. I was incorrect above about finding $P(\theta)$, but it seems to me we're still trying to find the maximal probability, where all probabilities are zero. Some answerers suggested that we're actually trying to find the max probability density but I don't understand why this is true.","I am struggling with the semantics of continuous random variables. For example, we do maximum likelihood estimation, in which we try to find the parameter $\theta$ which, for some observed data $D$, maximizes the likelihood $P(\theta|D)$. But my understanding of this is $$P(\theta = x) = P(x\leq\theta\leq x) = \int_x^xp(t)dt = 0$$ so I am not sure how any $\theta$ can result in a non-zero probability. Intuitively I understand what it means to find the ""most probable"" $\theta$, but I am having trouble uniting it with the formal definition. EDIT: In my class we defined $L(\theta:D)=P(D|\theta)=\prod_i P(D_i|\theta)$ (assuming i.i.d, where $D_i$ are the observations). Then we want to find $\text{argmax}_\theta \prod_i P(D_i|\theta)$. I was incorrect above about finding $P(\theta)$, but it seems to me we're still trying to find the maximal probability, where all probabilities are zero. Some answerers suggested that we're actually trying to find the max probability density but I don't understand why this is true.",,"['probability', 'statistics']"
93,Showing that ${\rm E}[X]=\sum_{k=0}^\infty P(X>k)$ for a discrete random variable,Showing that  for a discrete random variable,{\rm E}[X]=\sum_{k=0}^\infty P(X>k),"Let $X$ be a discrete random variable whose range is $0,1,2,3,\ldots$. Prove that $$ {\rm E}[X]=\sum_{k=0}^\infty P(X>k). $$ How to prove this? I tried a bit but unable to post due to formatting issue. Can anyone help?","Let $X$ be a discrete random variable whose range is $0,1,2,3,\ldots$. Prove that $$ {\rm E}[X]=\sum_{k=0}^\infty P(X>k). $$ How to prove this? I tried a bit but unable to post due to formatting issue. Can anyone help?",,"['probability', 'probability-theory', 'probability-distributions']"
94,Probability zero vs impossible,Probability zero vs impossible,,"I understand that probability $0$ does not mean 'impossible' - because if we look for instance at a uniform distribution over $[0, 1]$ then while each of the singleton events $\{r\}$ for $0\leq r \leq 1$ has probability $0$ , if we carried out the experiment then we would get exactly one of the numbers in $[0,1]$ , and so these events are not actually impossible, even though they have probability $0$ . However, if we look at a distribution defined by a density function which is zero on $[0,\frac{1}{2}]$ and non-zero on $(\frac{1}{2},1]$ (let's say with a continuous transition between them), then we know that for any $0\leq r\leq \frac{1}{2}$ the event $\{r\}$ is impossible, and for $\frac{1}{2}<r\leq 1$ the event is possible, yet still has probability $0$ . Both have probability $0$ , but one of them is possible, and the other is not. Is there a definition that captures this distinction between the two cases?","I understand that probability does not mean 'impossible' - because if we look for instance at a uniform distribution over then while each of the singleton events for has probability , if we carried out the experiment then we would get exactly one of the numbers in , and so these events are not actually impossible, even though they have probability . However, if we look at a distribution defined by a density function which is zero on and non-zero on (let's say with a continuous transition between them), then we know that for any the event is impossible, and for the event is possible, yet still has probability . Both have probability , but one of them is possible, and the other is not. Is there a definition that captures this distinction between the two cases?","0 [0, 1] \{r\} 0\leq r \leq 1 0 [0,1] 0 [0,\frac{1}{2}] (\frac{1}{2},1] 0\leq r\leq \frac{1}{2} \{r\} \frac{1}{2}<r\leq 1 0 0","['probability', 'probability-theory', 'probability-distributions']"
95,"Each member of a population dies with probability $\frac12$ each day, what is the probability that there will be exactly $1$ person alive?","Each member of a population dies with probability  each day, what is the probability that there will be exactly  person alive?",\frac12 1,"Suppose that there are $n$ people alive in a population. Due to a deadly disease, each person dies with probability $\frac12$ each day (and there are no births). What is the probability that there will be exactly one person alive at some time? Thoughts: Let $p_k$ be the probability that the population reaches exactly $1$ person given that there are currently $k$ people alive. Then $p_0 = 0$ and $p_1 = 1$. The probability of going from $k$ people alive to $k - j$ being alive (where $0 \leq j \leq k$) is the probability that $j$ die: $$ \binom{k}{j} \left(\frac{1}{2}\right)^j \left(\frac{1}{2}\right)^{k - j} = \binom{k}{j} \left(\frac{1}{2}\right)^k $$ And using conditional probability we have the recursion: $$ p_k = \frac{1}{2^k} \binom{k}{0} p_k + \frac{1}{2^k} \binom{k}{1} p_{k - 1} + \cdots + \frac{1}{2^k} \binom{k}{k - 1} p_1 + \frac{1}{2^k} \binom{k}{k} p_0, $$ or $$ (2^k - 1)p_k = \binom{k}{1} p_{k - 1} + \cdots + \binom{k}{k - 1} p_1. $$ Is it possible to solve a recursion like this? Is there a better way to solve the puzzle?","Suppose that there are $n$ people alive in a population. Due to a deadly disease, each person dies with probability $\frac12$ each day (and there are no births). What is the probability that there will be exactly one person alive at some time? Thoughts: Let $p_k$ be the probability that the population reaches exactly $1$ person given that there are currently $k$ people alive. Then $p_0 = 0$ and $p_1 = 1$. The probability of going from $k$ people alive to $k - j$ being alive (where $0 \leq j \leq k$) is the probability that $j$ die: $$ \binom{k}{j} \left(\frac{1}{2}\right)^j \left(\frac{1}{2}\right)^{k - j} = \binom{k}{j} \left(\frac{1}{2}\right)^k $$ And using conditional probability we have the recursion: $$ p_k = \frac{1}{2^k} \binom{k}{0} p_k + \frac{1}{2^k} \binom{k}{1} p_{k - 1} + \cdots + \frac{1}{2^k} \binom{k}{k - 1} p_1 + \frac{1}{2^k} \binom{k}{k} p_0, $$ or $$ (2^k - 1)p_k = \binom{k}{1} p_{k - 1} + \cdots + \binom{k}{k - 1} p_1. $$ Is it possible to solve a recursion like this? Is there a better way to solve the puzzle?",,"['probability', 'recursion']"
96,An ant on an infinite chessboard,An ant on an infinite chessboard,,"There is an infinite chessboard, and an ant $A$ is in the middle of one of the squares. The ant can move in any of the eight directions, from the center of one square to another. If it moves 1 square north, south, east or west; it requires $1$ unit energy. If it moves to one of its diagonal neighbor (NE, NW, SE, SW); it requires $\sqrt 2$ units of energy. It is equally likely to move in any of the eight directions. If it initially has $20$ units of energy, find the probability that, after using the maximum possible energy, the ant will be $2$ units away from its initial position. Assumption If in case it doesn't have enough energy to move in a particular set of directions, it will move in any of the other directions with equal probability. I approached this problem, considering that the case that it finally ends up $2$ units to the east (we can multiply by four to get all the cases). If it ends up $2$ units to the east, then $\text{Total steps to right}=2+\text{Total steps to left}$ . We will somehow balance these steps, considering that the ant has a total of $20$ units of energy at the start. I don't know how to effectively calculate the sample space either. If the ant takes a total of $n$ steps, such that while taking all $n$ steps it is equally likely to move in any of the eight directions, then the sample space would be $8^n$ . But here we do not know $n$ . Further, if the energy left after the second-last step is less than $\sqrt 2$ and more than $1$ , then the ant will not be able to move diagonally. I wasn't able to think of much after this. Help is appreciated.","There is an infinite chessboard, and an ant is in the middle of one of the squares. The ant can move in any of the eight directions, from the center of one square to another. If it moves 1 square north, south, east or west; it requires unit energy. If it moves to one of its diagonal neighbor (NE, NW, SE, SW); it requires units of energy. It is equally likely to move in any of the eight directions. If it initially has units of energy, find the probability that, after using the maximum possible energy, the ant will be units away from its initial position. Assumption If in case it doesn't have enough energy to move in a particular set of directions, it will move in any of the other directions with equal probability. I approached this problem, considering that the case that it finally ends up units to the east (we can multiply by four to get all the cases). If it ends up units to the east, then . We will somehow balance these steps, considering that the ant has a total of units of energy at the start. I don't know how to effectively calculate the sample space either. If the ant takes a total of steps, such that while taking all steps it is equally likely to move in any of the eight directions, then the sample space would be . But here we do not know . Further, if the energy left after the second-last step is less than and more than , then the ant will not be able to move diagonally. I wasn't able to think of much after this. Help is appreciated.",A 1 \sqrt 2 20 2 2 2 \text{Total steps to right}=2+\text{Total steps to left} 20 n n 8^n n \sqrt 2 1,"['probability', 'combinatorics']"
97,Average bus waiting time,Average bus waiting time,,"My friends and I were ""thinking"" yesterday in the pub about the following: if a person is standing on a bus stop that is served by a single bus which comes every p minutes, we would expect the average waiting time to be p/2 (which may or may not be correct). But we had no idea how to calculate the average waiting time if there is more than one bus. So let's assume there is n many buses serving the stop, and each comes once in m1, m2 ... mn minutes. How would we go about calculating the average time a person has to wait for a bus? What is the theory behind it? Thank you","My friends and I were ""thinking"" yesterday in the pub about the following: if a person is standing on a bus stop that is served by a single bus which comes every p minutes, we would expect the average waiting time to be p/2 (which may or may not be correct). But we had no idea how to calculate the average waiting time if there is more than one bus. So let's assume there is n many buses serving the stop, and each comes once in m1, m2 ... mn minutes. How would we go about calculating the average time a person has to wait for a bus? What is the theory behind it? Thank you",,"['probability', 'average']"
98,"Player rolls dice: for every “1” he gets 1 point, “11” - 5 points, “111” - 10 pts, so on. What is the mean score after 100 rolls?","Player rolls dice: for every “1” he gets 1 point, “11” - 5 points, “111” - 10 pts, so on. What is the mean score after 100 rolls?",,"For $x \neq 1$ : every $“\cdots x1x\cdots“$ gives +1 pt every $“\cdots x11x\cdots“$ gives +5 pts. every $“\cdots x111x\cdots“$ gives +10 pts. And so on: $n$ consecutive 1’s gives us $(n-1)5$ points. To make it clear, the usual 6-sided dice is rolled 100 times, so for example if player rolls dice 1 time - there’s 1/6 chance of getting 1 point; rolls two times - then there’s $\frac{1}{6}\frac{5}{6}2$ chance of 1 point(“1x” or “x1”) and $\left(\frac{1}{6}\right)^2$ of getting 5 points(only if “11” rolled out). The question: what is the mean score after rolling dice 100 times? The problem is: how do we calculate the mean when the number of rolls is so huge? It’s clear that using definition of the mean directly is not an option because number of different configurations for getting any score is immense(only if that score is not, say, 99*5 which require all the 1’s). I tried to use induction, but it didn’t worked out, for 3-4 rolls it already gets complicated. Moreover, how am I suppose to use it? If I know mean for &n& rolls and then I add $(n+1)$ th roll - it will add 0, 1 or 5 points depending on which number rolled in $n$ th place. Seems like knowing mean for $n$ rolls won’t be much of a help because after one more roll chance of getting any score is different. Another idea given to me by roommate is to fix number of ones that we get in entire 100-length sequence(so probability is fixed as well), and see what number of points we can possibly get with that number of 1’s - to know that these numbers will appear in formula for mean with known probability factor. But I’m not sure about that also because the amount of combinations is still insane. I ran out of ideas for now. Feels like there must be some efficient, less bloody way to calculate all that because our teacher gave us only 40 minutes for that problem (and another one), which completely freaked me out. All I wanted to say - I really appreciate any of your help since I absolutely have to figure this out. One more question: could anyone recommend some book with hard combinatorial problems in probability? Or some good textbook which could explain how to solve problems of that kind. That would be very helpful as well, thank you.","For : every gives +1 pt every gives +5 pts. every gives +10 pts. And so on: consecutive 1’s gives us points. To make it clear, the usual 6-sided dice is rolled 100 times, so for example if player rolls dice 1 time - there’s 1/6 chance of getting 1 point; rolls two times - then there’s chance of 1 point(“1x” or “x1”) and of getting 5 points(only if “11” rolled out). The question: what is the mean score after rolling dice 100 times? The problem is: how do we calculate the mean when the number of rolls is so huge? It’s clear that using definition of the mean directly is not an option because number of different configurations for getting any score is immense(only if that score is not, say, 99*5 which require all the 1’s). I tried to use induction, but it didn’t worked out, for 3-4 rolls it already gets complicated. Moreover, how am I suppose to use it? If I know mean for &n& rolls and then I add th roll - it will add 0, 1 or 5 points depending on which number rolled in th place. Seems like knowing mean for rolls won’t be much of a help because after one more roll chance of getting any score is different. Another idea given to me by roommate is to fix number of ones that we get in entire 100-length sequence(so probability is fixed as well), and see what number of points we can possibly get with that number of 1’s - to know that these numbers will appear in formula for mean with known probability factor. But I’m not sure about that also because the amount of combinations is still insane. I ran out of ideas for now. Feels like there must be some efficient, less bloody way to calculate all that because our teacher gave us only 40 minutes for that problem (and another one), which completely freaked me out. All I wanted to say - I really appreciate any of your help since I absolutely have to figure this out. One more question: could anyone recommend some book with hard combinatorial problems in probability? Or some good textbook which could explain how to solve problems of that kind. That would be very helpful as well, thank you.",x \neq 1 “\cdots x1x\cdots“ “\cdots x11x\cdots“ “\cdots x111x\cdots“ n (n-1)5 \frac{1}{6}\frac{5}{6}2 \left(\frac{1}{6}\right)^2 (n+1) n n,"['probability', 'combinatorics', 'reference-request', 'expected-value', 'book-recommendation']"
99,Does taking expected value of two sides of inequality maintain the inequality?,Does taking expected value of two sides of inequality maintain the inequality?,,"As some operations are not necessarily valid for maintaining an inequality (such as raising both sides of $-2 < 2$ to an even power), I wondered if taking the expected value of both sides is actually fine. So, let's say $X$ is a random variable and $E[X^2] < \infty$ holds. Now, since $|x| < 1 + x^2$ holds for all $x \in \Bbb{R}$ (easy proof), then $E[|X|] < E[1 + X^2]$ Thanks.","As some operations are not necessarily valid for maintaining an inequality (such as raising both sides of $-2 < 2$ to an even power), I wondered if taking the expected value of both sides is actually fine. So, let's say $X$ is a random variable and $E[X^2] < \infty$ holds. Now, since $|x| < 1 + x^2$ holds for all $x \in \Bbb{R}$ (easy proof), then $E[|X|] < E[1 + X^2]$ Thanks.",,"['probability', 'inequality', 'random-variables', 'expectation']"
