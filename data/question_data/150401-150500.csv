,title_raw,title_text,title_latex,body_raw,body_text,body_latex,tags
0,Find Heisenberg evolution (matrix products with complex numbers),Find Heisenberg evolution (matrix products with complex numbers),,"We will study the time-evolution of a finite dimensional quantum system. To this end, let us consider a quantum mechanical system with the Hilbert space $\mathbb{C}^2$ . We denote by $\left . \left |   0  \right \rangle\right .$ and $\left . \left |   1  \right \rangle\right .$ the standard basis elements $(1,0)^T$ and $(0,1)^T$ . Let the Hamiltonian of the system in this basis be given by $$ H=\omega\begin{pmatrix} 0 &1 \\  1 &0  \end{pmatrix}=\begin{pmatrix} 0 &-i \\  -i &0  \end{pmatrix} $$ and assume that for $t=0$ the state of the system is just given by $\psi(t=0)=\left . \left |   0  \right \rangle\right .$ . In the following, we also assume natural units in which $\hbar=1$ . I have found that $\psi(t)=\begin{pmatrix} \frac{1}{2}e^{t}+\frac{1}{2}e^{-t}\\  -\frac{1}{2}e^{t}+\frac{1}{2}e^{-t} \end{pmatrix}.$ And I have computed  the expectation value of $\left \langle \sigma_z \right \rangle_{\psi(t)}=\left \langle \psi(t)\mid \sigma_z\psi(t) \right \rangle$ where $ \sigma_z=\begin{pmatrix} 1 &0 \\  0 &-1  \end{pmatrix}$ to 1. Now I have to determine the time-evolved observable $\sigma_z=e^{iHt}\sigma_ze^{-iHt}$ (the Heisenberg evolution of $\sigma_z$ ) and I have to determine the resulting expectation value $\langle \sigma_z(t)\rangle_{\psi_0}=\langle\psi_0|\sigma_z(t)\psi_0\rangle$ . But I'm not sure how to deal with it? Is the first one just a matrix product I have to solve? But what is $e^{iHt}$ then? I hope anyone can help me?","We will study the time-evolution of a finite dimensional quantum system. To this end, let us consider a quantum mechanical system with the Hilbert space . We denote by and the standard basis elements and . Let the Hamiltonian of the system in this basis be given by and assume that for the state of the system is just given by . In the following, we also assume natural units in which . I have found that And I have computed  the expectation value of where to 1. Now I have to determine the time-evolved observable (the Heisenberg evolution of ) and I have to determine the resulting expectation value . But I'm not sure how to deal with it? Is the first one just a matrix product I have to solve? But what is then? I hope anyone can help me?","\mathbb{C}^2 \left . \left |   0  \right \rangle\right . \left . \left |   1  \right \rangle\right . (1,0)^T (0,1)^T 
H=\omega\begin{pmatrix}
0 &1 \\ 
1 &0 
\end{pmatrix}=\begin{pmatrix}
0 &-i \\ 
-i &0 
\end{pmatrix}
 t=0 \psi(t=0)=\left . \left |   0  \right \rangle\right . \hbar=1 \psi(t)=\begin{pmatrix}
\frac{1}{2}e^{t}+\frac{1}{2}e^{-t}\\ 
-\frac{1}{2}e^{t}+\frac{1}{2}e^{-t}
\end{pmatrix}. \left \langle \sigma_z \right \rangle_{\psi(t)}=\left \langle \psi(t)\mid \sigma_z\psi(t) \right \rangle 
\sigma_z=\begin{pmatrix}
1 &0 \\ 
0 &-1 
\end{pmatrix} \sigma_z=e^{iHt}\sigma_ze^{-iHt} \sigma_z \langle \sigma_z(t)\rangle_{\psi_0}=\langle\psi_0|\sigma_z(t)\psi_0\rangle e^{iHt}","['linear-algebra', 'analysis', 'hilbert-spaces', 'mathematical-physics', 'quantum-mechanics']"
1,How wild are weakly continuous curves in a Banach space?,How wild are weakly continuous curves in a Banach space?,,"For one reason or the other I work in a reflexive infinite dimensional Banach space $X$ and I am interested in curves $$ \gamma:[0,1]\to X $$ that are only weakly continuous. I have no idea how to visualise their behaviour. My best understanding of how bad things can go is that we have sequences on the unit sphere that converge to any point inside the unit ball. From this observation it is clear that I can always build this behaviour on one point, but how much can I push it? Even a countable set seems doable, but I would still get a locally strongly continuous curve. In general that means that I could expect curves to wildly jump inside and outside of some fixed ball. I would be interested in one such example if there is one. In such constructions one things tends to happen, and it seems that the derivative ought to explode. We know that the weak topology in a reflexive Banach space $X$ can be metrized locally when the space $X$ is separable, and thus we obtain a notion of Lipschitz continuous curve and absolutely continuous curve when considered with respect this metric. So my question are: Can you show an example of a weakly continuous curve that is not strongly continuous in at least a dense set if not more, and/or Are there examples of weakly continuous curves in separable reflexive Banach spaces that are not strongly continuous but are absolutely continuous or Lipschitz with respect to the metrization of some ball?","For one reason or the other I work in a reflexive infinite dimensional Banach space and I am interested in curves that are only weakly continuous. I have no idea how to visualise their behaviour. My best understanding of how bad things can go is that we have sequences on the unit sphere that converge to any point inside the unit ball. From this observation it is clear that I can always build this behaviour on one point, but how much can I push it? Even a countable set seems doable, but I would still get a locally strongly continuous curve. In general that means that I could expect curves to wildly jump inside and outside of some fixed ball. I would be interested in one such example if there is one. In such constructions one things tends to happen, and it seems that the derivative ought to explode. We know that the weak topology in a reflexive Banach space can be metrized locally when the space is separable, and thus we obtain a notion of Lipschitz continuous curve and absolutely continuous curve when considered with respect this metric. So my question are: Can you show an example of a weakly continuous curve that is not strongly continuous in at least a dense set if not more, and/or Are there examples of weakly continuous curves in separable reflexive Banach spaces that are not strongly continuous but are absolutely continuous or Lipschitz with respect to the metrization of some ball?","X 
\gamma:[0,1]\to X
 X X","['analysis', 'banach-spaces', 'weak-convergence', 'weak-topology']"
2,Proving an implication of the form: $\lim_{x \to \infty}$ of derivative/function implies $\lim_{x \to \infty}$ function/derivative?,Proving an implication of the form:  of derivative/function implies  function/derivative?,\lim_{x \to \infty} \lim_{x \to \infty},"(Assume $?$ is some function that is continous and differentiable everywhere) For example, if $\lim_{x \to \infty} ?' = 7$ implies $\lim_{x \to \infty} ? = \infty$ or $\lim_{x \to \infty} ? = 5$ implies $\lim_{x \to \infty} ?' = something$ Basically, how do you (or how would you ) start writing a proof that is something along the lines of: $\lim_{x \to \infty} ?' = something$ implies $\lim_{x \to \infty} ? = something$ $\lim_{x \to \infty} ? = something$ implies $\lim_{x \to \infty} ?' = something$ TL;DR How would you start to think about this type of problem (i.e. limit of derivative/function implies limit function/derivative) and formulate a plan and prove the question? I cannot think of a good starting point to snowball into some kind of solution since the definition of limit doesn't get me anywhere. I have thought about using L'Hopital's rule shown here , but you would have to show $\lim_{x \to \infty} \frac{f(x)}{x} = a$ implies $f(x)→\infty$ for $x→\infty$ to be rigourous, but that seems like an equally difficult problem. If $a = 0$ it does not hold (as it says a, a must be greater than zero), but under what circumstances can you assume $\lim_{x \to \infty} \frac{f(x)}{x} = a$ and that $a$ must be greater than zero? Some other proofs of similar questions use MVT but also Cauchy's subsequence and a lot of different messy variables and inequalities, which is much more complicated then these problems should require (for a 'simple' analysis question) I think (or maybe not, I'm not sure). The last proof here only uses MVT but the conclusion doesn't make much sense to me ( $f(y)>M$ ), and the post is so old that commenting is probably pointless. To me it seems like using the definition of limit is the best place to start, such as trying to show $\forall \varepsilon > 0$ , $\exists M>0$ such that $x>M$ implies $\mid f(x) - L \mid < \varepsilon$ for some limit as $x$ approaches infinity and equals some number $L$ . If $L = \infty$ then perhaps showing $\forall M > 0$ , $\exists N>0$ such that $f(x)>M,  \forall x>N$ would work as well. But that is the hard part that I get stuck on. My problem is that I do not see how MVT or L'Hopital's Theorem (The most useful theorems I could think of to solve the problem) or the given assumption that $\lim_{x \to \infty} ? = something$ or $\lim_{x \to \infty} ?' = something$ (depending on the problem, since usually the if part is used to prove the then part) could be used to show that $f(x)>M$ , $\forall x>N$ or $\mid f(x) - L \mid < \varepsilon$ etc depending on what $something$ is equal to and what $?$ is. To me it seems impossible to prove with the limited amount of information given (which it is not, but it seems like it), which leaves me stuck.","(Assume is some function that is continous and differentiable everywhere) For example, if implies or implies Basically, how do you (or how would you ) start writing a proof that is something along the lines of: implies implies TL;DR How would you start to think about this type of problem (i.e. limit of derivative/function implies limit function/derivative) and formulate a plan and prove the question? I cannot think of a good starting point to snowball into some kind of solution since the definition of limit doesn't get me anywhere. I have thought about using L'Hopital's rule shown here , but you would have to show implies for to be rigourous, but that seems like an equally difficult problem. If it does not hold (as it says a, a must be greater than zero), but under what circumstances can you assume and that must be greater than zero? Some other proofs of similar questions use MVT but also Cauchy's subsequence and a lot of different messy variables and inequalities, which is much more complicated then these problems should require (for a 'simple' analysis question) I think (or maybe not, I'm not sure). The last proof here only uses MVT but the conclusion doesn't make much sense to me ( ), and the post is so old that commenting is probably pointless. To me it seems like using the definition of limit is the best place to start, such as trying to show , such that implies for some limit as approaches infinity and equals some number . If then perhaps showing , such that would work as well. But that is the hard part that I get stuck on. My problem is that I do not see how MVT or L'Hopital's Theorem (The most useful theorems I could think of to solve the problem) or the given assumption that or (depending on the problem, since usually the if part is used to prove the then part) could be used to show that , or etc depending on what is equal to and what is. To me it seems impossible to prove with the limited amount of information given (which it is not, but it seems like it), which leaves me stuck.","? \lim_{x \to \infty} ?' = 7 \lim_{x \to \infty} ? = \infty \lim_{x \to \infty} ? = 5 \lim_{x \to \infty} ?' = something \lim_{x \to \infty} ?' = something \lim_{x \to \infty} ? = something \lim_{x \to \infty} ? = something \lim_{x \to \infty} ?' = something \lim_{x \to \infty} \frac{f(x)}{x} = a f(x)→\infty x→\infty a = 0 \lim_{x \to \infty} \frac{f(x)}{x} = a a f(y)>M \forall \varepsilon > 0 \exists M>0 x>M \mid f(x) - L \mid < \varepsilon x L L = \infty \forall M > 0 \exists N>0 f(x)>M,  \forall x>N \lim_{x \to \infty} ? = something \lim_{x \to \infty} ?' = something f(x)>M \forall x>N \mid f(x) - L \mid < \varepsilon something ?","['real-analysis', 'analysis', 'proof-writing', 'proof-explanation']"
3,"Evaluating $\int_{0}^{\infty} \frac{4\pi}{16\pi^2 + x^2} \left(\frac{1}{x}+\frac{1}{e^{-x}-1}\right) \, dx$",Evaluating,"\int_{0}^{\infty} \frac{4\pi}{16\pi^2 + x^2} \left(\frac{1}{x}+\frac{1}{e^{-x}-1}\right) \, dx","I’m trying to evaluate the following integral: $$\int_{0}^{\infty} \frac{4\pi}{16\pi^2 + x^2} \left(\frac{1}{x}+\frac{1}{e^{-x}-1}\right) \, dx$$ I’ve tried using contour integration by using a quarter-circle contour and going around the pole at $z=4\pi i$ with a semi-circular arc, however, I wasn’t able to evaluate the integrals along the imaginary axis. I wasn’t able to come up with a real or complex method for evaluating this integral, so any help would be appreciated. I’m not sure if a closed-form exists.","I’m trying to evaluate the following integral: I’ve tried using contour integration by using a quarter-circle contour and going around the pole at with a semi-circular arc, however, I wasn’t able to evaluate the integrals along the imaginary axis. I wasn’t able to come up with a real or complex method for evaluating this integral, so any help would be appreciated. I’m not sure if a closed-form exists.","\int_{0}^{\infty} \frac{4\pi}{16\pi^2 + x^2} \left(\frac{1}{x}+\frac{1}{e^{-x}-1}\right) \, dx z=4\pi i","['calculus', 'integration', 'analysis', 'complex-integration']"
4,How to calculate this improper integral: $\int_0^{\infty}{\frac{1}{\theta}e^{\cos\theta}\sin(\sin\theta){d\theta}}$?,How to calculate this improper integral: ?,\int_0^{\infty}{\frac{1}{\theta}e^{\cos\theta}\sin(\sin\theta){d\theta}},"Calculate the improper integral $$\displaystyle\int_0^{\infty}{\frac{1}{\theta}e^{\cos\theta}\sin(\sin\theta){d\theta}}$$ My try: We know that for any $a\in\mathbb{C}$ the integral $$\displaystyle\int_0^{\infty}e^{-ax^2}=\frac{1}{2}\sqrt{\frac{\pi}{a}}$$ Let $a=\cos\theta+i\sin\theta$ we know $$\displaystyle\int_0^{\infty}{e^{x^2\cos\theta}\sin(x^2\sin\theta){dx}}=\frac{\sqrt{\pi}}{2}\sin\frac{\theta}{2}$$ then $$\displaystyle\int_0^{\infty}{\frac{1}{\theta}e^{x^2\cos\theta}\sin(x^2\sin\theta){dx}}=\frac{\sqrt{\pi}}{2}\frac{\sin\frac{\theta}{2}}{\theta}$$ $$\displaystyle\int_0^{\infty}d\theta\displaystyle\int_0^{\infty}{\frac{1}{\theta}e^{x^2\cos\theta}\sin(x^2\sin\theta){dx}}=\displaystyle\int_0^{\infty}\frac{\sqrt{\pi}}{2}\frac{\sin\frac{\theta}{2}}{\theta}d\theta$$ Let $F(x)=\displaystyle\int_0^{\infty}{\frac{1}{\theta}e^{x^2\cos\theta}\sin(x^2\sin\theta){d\theta}}$ , then the result equals to $F(1)$ ,But I don't know what to do next.","Calculate the improper integral My try: We know that for any the integral Let we know then Let , then the result equals to ,But I don't know what to do next.",\displaystyle\int_0^{\infty}{\frac{1}{\theta}e^{\cos\theta}\sin(\sin\theta){d\theta}} a\in\mathbb{C} \displaystyle\int_0^{\infty}e^{-ax^2}=\frac{1}{2}\sqrt{\frac{\pi}{a}} a=\cos\theta+i\sin\theta \displaystyle\int_0^{\infty}{e^{x^2\cos\theta}\sin(x^2\sin\theta){dx}}=\frac{\sqrt{\pi}}{2}\sin\frac{\theta}{2} \displaystyle\int_0^{\infty}{\frac{1}{\theta}e^{x^2\cos\theta}\sin(x^2\sin\theta){dx}}=\frac{\sqrt{\pi}}{2}\frac{\sin\frac{\theta}{2}}{\theta} \displaystyle\int_0^{\infty}d\theta\displaystyle\int_0^{\infty}{\frac{1}{\theta}e^{x^2\cos\theta}\sin(x^2\sin\theta){dx}}=\displaystyle\int_0^{\infty}\frac{\sqrt{\pi}}{2}\frac{\sin\frac{\theta}{2}}{\theta}d\theta F(x)=\displaystyle\int_0^{\infty}{\frac{1}{\theta}e^{x^2\cos\theta}\sin(x^2\sin\theta){d\theta}} F(1),"['real-analysis', 'calculus', 'analysis', 'improper-integrals']"
5,Multiplicity and set of zeros.,Multiplicity and set of zeros.,,"exercise: Let us assume that the function f has derivatives of all orders. Suppose that all zeros of $f$ have finite multiplicity. Let $a$ and $b$ be points of $A$ , such that $a<b$ and neither point is a zero. Show that $f$ has at most finitely many zeros in $] a, b[$ (We say that a point $c$ is a root of $f(x)=0$ with multiplicity $m$ , if $f^{(k)}(c)=0$ for $k=0, \ldots, m-1$ and $f^{(m)}(c) \neq 0 .$ As usual $f^{(0)}$ denotes $f$ .) lemma: A zero of finite multiplicity is an isolated point of the set of zeros proof: Considering Taylor polynomial $E(h)=f(c+h)-\left(f(c)+\frac{1}{1 !} f^{\prime}(c) h+\frac{1}{2 !} f^{\prime \prime}(c) h^{2}+\cdots+\frac{1}{m !} f^{(m)}(c) h^{m}\right)$ and using L’Hopital’s Rule we can show that $\lim\limits_{h \rightarrow 0} \frac{E(h)}{h^{m}}=0$ . Because of the multiplicity and for $h \neq 0$ we can write $$ \frac{f(c+h)-f(c)}{h^{m}}=\frac{1}{m !} f^{(m)}(c)+\frac{E(h)}{h^{m}} $$ and from this we can deduce the proof of the lemma. So if we know if all zeros of $f$ have finite multiplicity then all zeros are isolated. Meanwhile we have bounded interval we can use Bolzano–Weierstrass theorem to show that if f have infinitely many zeros then at least one of the zeros should be limit point which contradicts to be isolated. But why I do need $f(a)$ , $f(b)$ not being zero which was declared in the exercise. I am suspicious about using intermediate value theorem and taylor polynomial with remainder. But I don't know how to do it?? Remark: ( May be it can be some kind of hint which I cannot figure out: next exercise asks ::: In the previous exercise, if f (a) and f (b) have the same sign, show that the number of zeros in ]a, b[, counted by multiplicity, is even. If $f(a)$ and $f(b)$ have opposite signs, show that the number of zeros in ]a, b[, counted by multiplicity, is odd)","exercise: Let us assume that the function f has derivatives of all orders. Suppose that all zeros of have finite multiplicity. Let and be points of , such that and neither point is a zero. Show that has at most finitely many zeros in (We say that a point is a root of with multiplicity , if for and As usual denotes .) lemma: A zero of finite multiplicity is an isolated point of the set of zeros proof: Considering Taylor polynomial and using L’Hopital’s Rule we can show that . Because of the multiplicity and for we can write and from this we can deduce the proof of the lemma. So if we know if all zeros of have finite multiplicity then all zeros are isolated. Meanwhile we have bounded interval we can use Bolzano–Weierstrass theorem to show that if f have infinitely many zeros then at least one of the zeros should be limit point which contradicts to be isolated. But why I do need , not being zero which was declared in the exercise. I am suspicious about using intermediate value theorem and taylor polynomial with remainder. But I don't know how to do it?? Remark: ( May be it can be some kind of hint which I cannot figure out: next exercise asks ::: In the previous exercise, if f (a) and f (b) have the same sign, show that the number of zeros in ]a, b[, counted by multiplicity, is even. If and have opposite signs, show that the number of zeros in ]a, b[, counted by multiplicity, is odd)","f a b A a<b f ] a, b[ c f(x)=0 m f^{(k)}(c)=0 k=0, \ldots, m-1 f^{(m)}(c) \neq 0 . f^{(0)} f E(h)=f(c+h)-\left(f(c)+\frac{1}{1 !} f^{\prime}(c) h+\frac{1}{2 !} f^{\prime \prime}(c) h^{2}+\cdots+\frac{1}{m !} f^{(m)}(c) h^{m}\right) \lim\limits_{h \rightarrow 0} \frac{E(h)}{h^{m}}=0 h \neq 0 
\frac{f(c+h)-f(c)}{h^{m}}=\frac{1}{m !} f^{(m)}(c)+\frac{E(h)}{h^{m}}
 f f(a) f(b) f(a) f(b)","['real-analysis', 'calculus', 'analysis', 'functions', 'derivatives']"
6,$f$ don't have local minimum or local maximum in $c$,don't have local minimum or local maximum in,f c,"if $f$ is a function, with derivate of order $n$ on $(a,b)$ , and $c \in (a,b)$ such that $c$ is a critical point, where: \begin{eqnarray}  f'(c)=0, f''(c)=0, \dots, f^{(n-1)}(c)=0, f^{(n)} (c) \neq 0 \end{eqnarray} then for odd order, $f$ don't have a local minimum and local maximum in $c$ . I've tried by definition of limits and analyse the signs of some $x$ in the interval $(c-\delta, c+\delta)$ for $\delta >0$ on $2n-1$ derivate, but I'm stuck, Do you know some way to do this? Can I continue by this way?","if is a function, with derivate of order on , and such that is a critical point, where: then for odd order, don't have a local minimum and local maximum in . I've tried by definition of limits and analyse the signs of some in the interval for on derivate, but I'm stuck, Do you know some way to do this? Can I continue by this way?","f n (a,b) c \in (a,b) c \begin{eqnarray}
 f'(c)=0, f''(c)=0, \dots, f^{(n-1)}(c)=0, f^{(n)} (c) \neq 0
\end{eqnarray} f c x (c-\delta, c+\delta) \delta >0 2n-1","['real-analysis', 'analysis', 'derivatives']"
7,"is there example that inequality $\sup\{\inf\{f(x,y) : x \in X\}: y \in Y\} \le \inf\{\sup\{f(x,y) : y \in Y\}: x \in X\}$ be strict?",is there example that inequality  be strict?,"\sup\{\inf\{f(x,y) : x \in X\}: y \in Y\} \le \inf\{\sup\{f(x,y) : y \in Y\}: x \in X\}","I know it's similar to this and I proved this inequality, but I'm stuck at finding examples that this inequality is strict. thanks. Edit: range is bounded.","I know it's similar to this and I proved this inequality, but I'm stuck at finding examples that this inequality is strict. thanks. Edit: range is bounded.",,"['real-analysis', 'analysis', 'supremum-and-infimum']"
8,Prove certain integral equals $0$,Prove certain integral equals,0,"I've been trying to solve some problems from my Partial Differential Equations course. I got the idea to solve this one but there's one step at the end I'm not sure about. It goes like this: Given $$G(x)=\int_{x_0}^xg^*(y)dy,$$ where $g^*$ is a $2l$ -periodic odd function, and $x_0$ is fixed. Prove $G$ is $2l$ -periodic. The question was a bit more complex in context (about wave equation) but I've simplified it so anyone can understand it (making it just a real analysis problem). So what I need to prove is that $G(x)=G(x+2l)$ for any $x\in\mathbb R$ . I've seen that $$G(x+2l)=\int_{x_0}^{x+2l}g^*(y)dy = \int_{x_0}^xg^*(y)dy + \int_x^{x+2l}g^*(y)dy = G(x) + \int_x^{x+2l}g^*(y)dy,$$ so proving $G(x+2l)=G(x)$ would be equivalent to proving that $$\int_x^{x+2l}g^*(y)dy=0, \text{ }\forall x\in\mathbb R.$$ To prove this, I obviously need to use the fact that $g^*$ is both odd and $2l$ -periodic. I've tried doing: $$\int_x^{x+2l}g^*(y)dy=\int_{-l}^{l}g^*(y+x+l)dy,$$ what would be equal to zero if $g^*(y+x+l)$ was still an odd function (because it's integral limits are $-l$ and $l$ after that change). I'm not sure if changing what's inside the function $g^*$ makes it stop being odd or if it just does not matter. Is it still odd? If it is, is it trivial or do I need to prove it? If it's not, how can I see then that the integral equals zero? Another way to prove it that came to my mind was using that the antiderivative of a periodic function is also a periodic function of the same period, but I'm not sure if it's true (I know the other way it's true, derivative of periodic is periodic, but my intuition tells me that antiderivative of periodic may not necessarily be periodic of the same period, don't know why). Any help, hint or comment will be appreciated, thanks in advance.","I've been trying to solve some problems from my Partial Differential Equations course. I got the idea to solve this one but there's one step at the end I'm not sure about. It goes like this: Given where is a -periodic odd function, and is fixed. Prove is -periodic. The question was a bit more complex in context (about wave equation) but I've simplified it so anyone can understand it (making it just a real analysis problem). So what I need to prove is that for any . I've seen that so proving would be equivalent to proving that To prove this, I obviously need to use the fact that is both odd and -periodic. I've tried doing: what would be equal to zero if was still an odd function (because it's integral limits are and after that change). I'm not sure if changing what's inside the function makes it stop being odd or if it just does not matter. Is it still odd? If it is, is it trivial or do I need to prove it? If it's not, how can I see then that the integral equals zero? Another way to prove it that came to my mind was using that the antiderivative of a periodic function is also a periodic function of the same period, but I'm not sure if it's true (I know the other way it's true, derivative of periodic is periodic, but my intuition tells me that antiderivative of periodic may not necessarily be periodic of the same period, don't know why). Any help, hint or comment will be appreciated, thanks in advance.","G(x)=\int_{x_0}^xg^*(y)dy, g^* 2l x_0 G 2l G(x)=G(x+2l) x\in\mathbb R G(x+2l)=\int_{x_0}^{x+2l}g^*(y)dy = \int_{x_0}^xg^*(y)dy + \int_x^{x+2l}g^*(y)dy = G(x) + \int_x^{x+2l}g^*(y)dy, G(x+2l)=G(x) \int_x^{x+2l}g^*(y)dy=0, \text{ }\forall x\in\mathbb R. g^* 2l \int_x^{x+2l}g^*(y)dy=\int_{-l}^{l}g^*(y+x+l)dy, g^*(y+x+l) -l l g^*","['real-analysis', 'integration', 'analysis', 'periodic-functions', 'even-and-odd-functions']"
9,Find all $c\in \mathbb{R}$ such that $f^{-1}(c)$ is a submanifold(Solution verification),Find all  such that  is a submanifold(Solution verification),c\in \mathbb{R} f^{-1}(c),"Consider the following problem from my course notes on manifolds: Question:Define $f: \mathbb{R}^2 \to \mathbb{R}$ by $f(x,y)= x^3 -6xy+y^2$ . Find all values $c\in \mathbb{R}$ such that $f^{-1}(c)\in M$ is an embedded submanifold of $\mathbb{R}^2$ . So, $f$ is an onto map (just take $y=0$ , $x^3 $ is onto) and $f^{-1}(c) = $ { $(x,y): f(x,y)=c$ }. $\mathbb{R}^3$ is a manifold and there always exists a chart $(U,\phi)$ , $\phi$ is homeo. So, I think that using onto property of $f$ I think for all $c\in \mathbb{R}$ , $f^{-1}(c)$ is a submanifold as $\phi( U \cap f^{-1} (c)) $ will always be in $\mathbb{R}^3$ and hence satisfying definition of submanifold. Am I right?","Consider the following problem from my course notes on manifolds: Question:Define by . Find all values such that is an embedded submanifold of . So, is an onto map (just take , is onto) and { }. is a manifold and there always exists a chart , is homeo. So, I think that using onto property of I think for all , is a submanifold as will always be in and hence satisfying definition of submanifold. Am I right?","f: \mathbb{R}^2 \to \mathbb{R} f(x,y)= x^3 -6xy+y^2 c\in \mathbb{R} f^{-1}(c)\in M \mathbb{R}^2 f y=0 x^3  f^{-1}(c) =  (x,y): f(x,y)=c \mathbb{R}^3 (U,\phi) \phi f c\in \mathbb{R} f^{-1}(c) \phi( U \cap f^{-1} (c))  \mathbb{R}^3","['analysis', 'differential-geometry']"
10,The convex cone of a compact set not including the origin is always closed?,The convex cone of a compact set not including the origin is always closed?,,"Based on the examples given in https://en.wikipedia.org/wiki/Conical_combination , we know that the convex cone generated by a set containing the origin point does not necessarily a closed set, even if the convex cone is generated by convex compact set. In addition, we also have the following known result. S is a non-empty convex compact set which does not contain the origin, the convex conical hull of S is a closed set. I am wondering if we relax the condition of convexity, is there a case such that the convex conical hull of compact set in $\mathbb{R}^n$ not including the origin is not closed.","Based on the examples given in https://en.wikipedia.org/wiki/Conical_combination , we know that the convex cone generated by a set containing the origin point does not necessarily a closed set, even if the convex cone is generated by convex compact set. In addition, we also have the following known result. S is a non-empty convex compact set which does not contain the origin, the convex conical hull of S is a closed set. I am wondering if we relax the condition of convexity, is there a case such that the convex conical hull of compact set in not including the origin is not closed.",\mathbb{R}^n,"['general-topology', 'analysis', 'convex-analysis', 'compactness', 'convex-geometry']"
11,Understanding $\log p/q = \sum_{k=1}^{\infty} \frac{1}{2k-1}(\frac{p-q}{p+q})^{2k-1}$,Understanding,\log p/q = \sum_{k=1}^{\infty} \frac{1}{2k-1}(\frac{p-q}{p+q})^{2k-1},"Problem This problem comes straight from the Taylor Formula Chapter of Edwin Wilson's Advanced Calculus Textbook : The part that I am concerned with is part $(\gamma)$ . What I need: Either a hint pointing me in the right direction or a guide to a resources that will hellp illustrate why this relationship is true. Personal Work and Ideas: The problem comes down to proving that: $$\log\frac{p}{q} = 2\bigg[\sum\limits_{k=1}^{\infty}\frac{1}{2k-1}(\frac{p-q}{p+q})^{2k-1}\bigg]$$ which on its face seems to be some manipulation of the series $$\text{ (1)   }\log x = \log a + \sum\limits_{k=1}^\infty \frac{(-1)^{k-1}}{k}  (\frac{x-a}{a})^k$$ Origin of This formula of course comes from taylor expansion of $\log x$ and is derived from these applying these two pieces of information: $$f^{(k)}(x) = \begin{cases}        \log x & k=0 \\       \frac{(-1)^{k-1}}{(x)^k}(k-1)!& k>0\\     \end{cases} $$ and $$f(x) = f(a) + \sum\limits_{k=1}^{\infty} f(a)\frac{(x-a)^k}{k!}$$ Some ideas I have tried are the following ( Caution this mostly reads like scratch work. ): IDEA 1 Idea : Set $x=p$ and $a=q$ Motivation: As a first attempt I am trying to see how far simple substitution takes me. So I am going to plug in $x=p$ and $a=q$ to see how far it goes.. Attempt Assume that we plug $x=p$ and $a=q$ in to (1) then we obtain: $$\log (\frac{p}{q}) = \sum\limits_{k=1}^{\infty}\frac{(-1)^{k-1}}{k}(\frac{p-q}{q})^{k}$$ Problems The biggest problems with this approach are two fold. No introduction to $p+q$ as a denominator term. The k-values in parity with what the solution is giving us (all k-values of the solution are odd). IDEA 2 Idea : Sum pairs of summands Motivation: The issue with the parity of the $k$ values seems to be addressed if we sum pairs of items. This possibly might introduce $p+q$ to the denominator. Attempt Let $b_k =\frac{(-1)^{k-1}}{k}(\frac{p-q}{q})^{k} $ then $$b_{2k}+b_{2k+1} = \frac{(-1)^{2k-1}}{2k}(\frac{p-q}{q})^{2k} +\frac{(-1)^{2k}}{2k+1}(\frac{p-q}{q})^{2k+1}   $$ $$=(\frac{p-q}{q})^{2k}\bigg[-\frac{1}{2k} +\frac{\frac{p-q}{q}}{2k+1}\bigg]$$ $$=(\frac{p-q}{q})^{2k}\bigg[\frac{-q(2k+1) +(p-q)(2k)}{(2qk)(2k+1)}\bigg]$$ Problems: There doesn't seem to be a good way that this turns to the denominator to $p+q$ . IDEA 3 Idea: Take the difference of to taylor series. Namely $\log p$ and $\log q$ with the center being at p+q. Motivation The goal of this is to introduce the $p+q$ term to the denominator. Attempt $$\log p = \log(p+q) + \sum\limits_{k=1}^{\infty} \frac{(-1)^{k-1}}{k}  (\frac{p-(p+q)}{p+q})^k$$ or $$\log p = =\log(p+q) + \sum\limits_{k=1}^{\infty} \frac{(-1)^{2k-1}}{k}  (\frac{q}{p+q})^k$$ $$\log q=\log(p+q) + \sum\limits_{k=1}^{\infty} \frac{(-1)^{2k-1}}{k}  (\frac{p}{p+q})^k$$ which after subtraction gives us this: $$\log \frac{p}{q}= \sum\limits_{k=1}^{\infty} \frac{(-1)^{2k-1}}{k}\bigg[  (\frac{q}{p+q})^k-(\frac{p}{p+q})^k\bigg]= \sum\limits_{k=1}^{\infty} \frac{(-1)^{2k-1}}{k}(\frac{1}{p+q})^k\bigg[  q^k-p^k\bigg]$$ Notes: Since this is my current idea and I am muddling through, I will rename the problem section notes. I am not sure if this route will work or not. Might end up having a problem with the coeffients. I am still thinking through possible next steps.","Problem This problem comes straight from the Taylor Formula Chapter of Edwin Wilson's Advanced Calculus Textbook : The part that I am concerned with is part . What I need: Either a hint pointing me in the right direction or a guide to a resources that will hellp illustrate why this relationship is true. Personal Work and Ideas: The problem comes down to proving that: which on its face seems to be some manipulation of the series Origin of This formula of course comes from taylor expansion of and is derived from these applying these two pieces of information: and Some ideas I have tried are the following ( Caution this mostly reads like scratch work. ): IDEA 1 Idea : Set and Motivation: As a first attempt I am trying to see how far simple substitution takes me. So I am going to plug in and to see how far it goes.. Attempt Assume that we plug and in to (1) then we obtain: Problems The biggest problems with this approach are two fold. No introduction to as a denominator term. The k-values in parity with what the solution is giving us (all k-values of the solution are odd). IDEA 2 Idea : Sum pairs of summands Motivation: The issue with the parity of the values seems to be addressed if we sum pairs of items. This possibly might introduce to the denominator. Attempt Let then Problems: There doesn't seem to be a good way that this turns to the denominator to . IDEA 3 Idea: Take the difference of to taylor series. Namely and with the center being at p+q. Motivation The goal of this is to introduce the term to the denominator. Attempt or which after subtraction gives us this: Notes: Since this is my current idea and I am muddling through, I will rename the problem section notes. I am not sure if this route will work or not. Might end up having a problem with the coeffients. I am still thinking through possible next steps.","(\gamma) \log\frac{p}{q} = 2\bigg[\sum\limits_{k=1}^{\infty}\frac{1}{2k-1}(\frac{p-q}{p+q})^{2k-1}\bigg] \text{ (1)   }\log x = \log a + \sum\limits_{k=1}^\infty \frac{(-1)^{k-1}}{k}  (\frac{x-a}{a})^k \log x f^{(k)}(x) = \begin{cases} 
      \log x & k=0 \\
      \frac{(-1)^{k-1}}{(x)^k}(k-1)!& k>0\\ 
   \end{cases}  f(x) = f(a) + \sum\limits_{k=1}^{\infty} f(a)\frac{(x-a)^k}{k!} x=p a=q x=p a=q x=p a=q \log (\frac{p}{q}) = \sum\limits_{k=1}^{\infty}\frac{(-1)^{k-1}}{k}(\frac{p-q}{q})^{k} p+q k p+q b_k =\frac{(-1)^{k-1}}{k}(\frac{p-q}{q})^{k}  b_{2k}+b_{2k+1} = \frac{(-1)^{2k-1}}{2k}(\frac{p-q}{q})^{2k} +\frac{(-1)^{2k}}{2k+1}(\frac{p-q}{q})^{2k+1}    =(\frac{p-q}{q})^{2k}\bigg[-\frac{1}{2k} +\frac{\frac{p-q}{q}}{2k+1}\bigg] =(\frac{p-q}{q})^{2k}\bigg[\frac{-q(2k+1) +(p-q)(2k)}{(2qk)(2k+1)}\bigg] p+q \log p \log q p+q \log p = \log(p+q) + \sum\limits_{k=1}^{\infty} \frac{(-1)^{k-1}}{k}  (\frac{p-(p+q)}{p+q})^k \log p = =\log(p+q) + \sum\limits_{k=1}^{\infty} \frac{(-1)^{2k-1}}{k}  (\frac{q}{p+q})^k \log q=\log(p+q) + \sum\limits_{k=1}^{\infty} \frac{(-1)^{2k-1}}{k}  (\frac{p}{p+q})^k \log \frac{p}{q}= \sum\limits_{k=1}^{\infty} \frac{(-1)^{2k-1}}{k}\bigg[  (\frac{q}{p+q})^k-(\frac{p}{p+q})^k\bigg]= \sum\limits_{k=1}^{\infty} \frac{(-1)^{2k-1}}{k}(\frac{1}{p+q})^k\bigg[  q^k-p^k\bigg]","['real-analysis', 'analysis']"
12,"$[0,1]$ not compact with the discrete metric",not compact with the discrete metric,"[0,1]","I am trying to show that $[0,1]$ is not compact when $\mathbb{R}$ is equipped with the discrete metric. Consider $\mathbb{R}$ equipped with the discrete metric. Since every singleton set in $\mathbb{R}$ is both open and closed, under the discrete metric, every subset of $\mathbb{R}$ is both open and closed. Thus $[0,1]$ can be expressed as the union of arbitrarily many singleton sets (the union of arbitrarily many open sets is itself open). The collection of such sets constitutes an open cover of $[0,1]$ (call this cover $G$ ). Yet, if we remove at most one of the singleton sets from $G$ , $G$ no longer covers $[0,1]$ . Thus, $G$ is a cover, of $[0,1]$ , that does not admit a finite sub cover. Thank you.","I am trying to show that is not compact when is equipped with the discrete metric. Consider equipped with the discrete metric. Since every singleton set in is both open and closed, under the discrete metric, every subset of is both open and closed. Thus can be expressed as the union of arbitrarily many singleton sets (the union of arbitrarily many open sets is itself open). The collection of such sets constitutes an open cover of (call this cover ). Yet, if we remove at most one of the singleton sets from , no longer covers . Thus, is a cover, of , that does not admit a finite sub cover. Thank you.","[0,1] \mathbb{R} \mathbb{R} \mathbb{R} \mathbb{R} [0,1] [0,1] G G G [0,1] G [0,1]","['general-topology', 'analysis']"
13,"Inequality for a decreasing, concave function on [0,1]","Inequality for a decreasing, concave function on [0,1]",,"Suppose I have a function $f:[0,1]\rightarrow \mathbb{R}_{\geq0}$ that is decreasing and concave. Let $a,b,c,d\in(0,1)$ , with $a>b$ and $a+b>1$ . I am trying to prove the following inequality: $$  f\Bigg(\frac{(1-a)\cdot c}{(1-a)\cdot c + b\cdot d }\Bigg)\cdot \big( (1-a)\cdot c + b\cdot d \big) +  f\Bigg(\frac{a\cdot c}{a\cdot c + (1-b)\cdot d }\Bigg)\cdot \big( a\cdot c + (1-b)\cdot d \big) \leq \\ f\Bigg(\frac{(1-b)\cdot c}{(1-b)\cdot c + a\cdot d }\Bigg)\cdot \big( (1-b)\cdot c + a\cdot d \big) +  f\Bigg(\frac{b\cdot c}{b\cdot c + (1-a)\cdot d }\Bigg)\cdot \big( b\cdot c + (1-a)\cdot d \big)  $$ My approach was the following. Since $f$ is also subadditive (it's concave and $f(0)\geq0$ ), you can show that $$ f\Bigg(\frac{(1-a)\cdot c}{(1-a)\cdot c + b\cdot d }\Bigg)\cdot \big( (1-a)\cdot c + b\cdot d \big) \leq f\big( (1-a)\cdot c \big),  $$ and $$ f\Bigg(\frac{a\cdot c}{a\cdot c + (1-b)\cdot d }\Bigg)\cdot \big( a\cdot c + (1-b)\cdot d \big) \leq f\big(a\cdot c \big), $$ after which I got stuck.. Does anyone know how to proceed, or perhaps know a better way to approach this? EDIT: Similarly, (using the subadditivity), you can also show that $$ f\Bigg(\frac{(1-b)\cdot c}{(1-b)\cdot c + a\cdot d }\Bigg)\cdot \big( (1-b)\cdot c + a\cdot d \big)\geq f(1) \cdot (1-b)\cdot c, $$ and $$ f\Bigg(\frac{b\cdot c}{b\cdot c + (1-a)\cdot d }\Bigg)\cdot \big( b\cdot c + (1-a)\cdot d \big) \geq f(1) \cdot b\cdot c. $$ If you combine these inequalities you will get the sufficient condition: $$f\big( (1-a)\cdot c \big) + f\big(a\cdot c \big) \leq f(1)\cdot c,$$ which is not satisfied because of the decreasing property of $f$ . So this condition is too ""loose"", probably because the inequalities are too loose..","Suppose I have a function that is decreasing and concave. Let , with and . I am trying to prove the following inequality: My approach was the following. Since is also subadditive (it's concave and ), you can show that and after which I got stuck.. Does anyone know how to proceed, or perhaps know a better way to approach this? EDIT: Similarly, (using the subadditivity), you can also show that and If you combine these inequalities you will get the sufficient condition: which is not satisfied because of the decreasing property of . So this condition is too ""loose"", probably because the inequalities are too loose..","f:[0,1]\rightarrow \mathbb{R}_{\geq0} a,b,c,d\in(0,1) a>b a+b>1  
f\Bigg(\frac{(1-a)\cdot c}{(1-a)\cdot c + b\cdot d }\Bigg)\cdot \big( (1-a)\cdot c + b\cdot d \big)
+ 
f\Bigg(\frac{a\cdot c}{a\cdot c + (1-b)\cdot d }\Bigg)\cdot \big( a\cdot c + (1-b)\cdot d \big) \leq \\
f\Bigg(\frac{(1-b)\cdot c}{(1-b)\cdot c + a\cdot d }\Bigg)\cdot \big( (1-b)\cdot c + a\cdot d \big)
+ 
f\Bigg(\frac{b\cdot c}{b\cdot c + (1-a)\cdot d }\Bigg)\cdot \big( b\cdot c + (1-a)\cdot d \big) 
 f f(0)\geq0 
f\Bigg(\frac{(1-a)\cdot c}{(1-a)\cdot c + b\cdot d }\Bigg)\cdot \big( (1-a)\cdot c + b\cdot d \big) \leq f\big( (1-a)\cdot c \big), 
 
f\Bigg(\frac{a\cdot c}{a\cdot c + (1-b)\cdot d }\Bigg)\cdot \big( a\cdot c + (1-b)\cdot d \big) \leq f\big(a\cdot c \big),
 
f\Bigg(\frac{(1-b)\cdot c}{(1-b)\cdot c + a\cdot d }\Bigg)\cdot \big( (1-b)\cdot c + a\cdot d \big)\geq f(1) \cdot (1-b)\cdot c,
 
f\Bigg(\frac{b\cdot c}{b\cdot c + (1-a)\cdot d }\Bigg)\cdot \big( b\cdot c + (1-a)\cdot d \big) \geq f(1) \cdot b\cdot c.
 f\big( (1-a)\cdot c \big) + f\big(a\cdot c \big) \leq f(1)\cdot c, f","['real-analysis', 'analysis', 'inequality']"
14,Counter example that a convex set is not necessarily an interval or ray .,Counter example that a convex set is not necessarily an interval or ray .,,"The example I am trying to frame is like this : I am considering the dictionary order on $\mathbb{R}^2$ , now I know that the set $S=\{x^2+y^2<1\}$ is a convex set(I am confused that it is with respect to which order).It doesn't look like that $S$ is an interval or ray. I need some help to frame this question properly to give an example.","The example I am trying to frame is like this : I am considering the dictionary order on , now I know that the set is a convex set(I am confused that it is with respect to which order).It doesn't look like that is an interval or ray. I need some help to frame this question properly to give an example.",\mathbb{R}^2 S=\{x^2+y^2<1\} S,"['general-topology', 'analysis', 'examples-counterexamples']"
15,Convergence of the product of a convergent and absolutely convergent series' elements.,Convergence of the product of a convergent and absolutely convergent series' elements.,,"Prove that the series $\sum_{n=1}^{\infty} a_n b_n$ converges if the following conditions are met: series $\sum_{n=1}^{\infty} b_n$ converges, series $\sum_{n=1}^{\infty} (a_n - a_{n+1})$ absolutely converges. I was thinking applying Abel's test, proving that if: $\sum_{n=1}^{\infty} b_n$ is a convergent series (given), { $a_n$ } is a monotone sequence, and { $a_n$ } is bounded. Then the $\sum_{n=1}^{\infty} a_n b_n$ converges. To prove the second one statement, we've to get following inequality: $a_{n+1} \leq a_n$ . I've no idea how do we do that. Third one, I think, is obtained by the fact, that the $\sum_{n=1}^{\infty} (a_n - a_{n+1})$ series converges, so $\lim_{n\to\infty} (a_n - a_{n+1}) = 0$ . Is it correct?","Prove that the series converges if the following conditions are met: series converges, series absolutely converges. I was thinking applying Abel's test, proving that if: is a convergent series (given), { } is a monotone sequence, and { } is bounded. Then the converges. To prove the second one statement, we've to get following inequality: . I've no idea how do we do that. Third one, I think, is obtained by the fact, that the series converges, so . Is it correct?",\sum_{n=1}^{\infty} a_n b_n \sum_{n=1}^{\infty} b_n \sum_{n=1}^{\infty} (a_n - a_{n+1}) \sum_{n=1}^{\infty} b_n a_n a_n \sum_{n=1}^{\infty} a_n b_n a_{n+1} \leq a_n \sum_{n=1}^{\infty} (a_n - a_{n+1}) \lim_{n\to\infty} (a_n - a_{n+1}) = 0,"['calculus', 'sequences-and-series', 'analysis', 'convergence-divergence', 'absolute-convergence']"
16,Locally Euclidean topology - but not Hausdorff,Locally Euclidean topology - but not Hausdorff,,"We consider the set $X=\mathbb{R}\cup \{\star\}$ , i.e. $X$ consists of $\mathbb{R}$ and an additional point $\star$ . We say that $U\subset X$ is open if: (a) For each point $x\in U\cap \mathbb{R}$ there exists an $\epsilon>0$ such that $(x-\epsilon, x+\epsilon)\subset U$ . (b) If $\star \in U$ then there is an $\epsilon>0$ such that $(-\epsilon , 0)\cup (0, \epsilon)\subset U$ . $$$$ Show that this defines a topology in $X$ . Show that $X$ with this topology is locally Euclidean but not Hausdorff. $$$$ For (1) we have to show that $X$ and $\emptyset$ are open, the union of two open sets is open and the intersection of two open sets is open. First we show that $X$ is open : (a) For each point $x\in X\cap \mathbb{R}=\mathbb{R}$ there exists an $\epsilon>0$ such that $(x-\epsilon, x+\epsilon)\subset X=\mathbb{R}\cup \{\star\}$ . This is true since every neighboorhood of $x$ is contained. (b) If $\star \in X$ then there is an $\epsilon>0$ such that $(-\epsilon , 0)\cup (0, \epsilon)\subset X=\mathbb{R}\cup \{\star\}$ . Thisis true since the union of the intervals is a subspace of the real line. Is that correct? The emptyset is per definition open, or not? We don’t have to apply the given definition, do we? Let $M_1$ and $M_2$ be two open sets. We consider the union $M_1\cup M_2$ . For each point $x\in M_1\cup M_2$ it is either $x\in M_1$ or $x\in M_2$ (or both) so statement (a) follows from the fact that $M_1$ and/or $M_2$ are open. The same holds also for statement (b). Let $M_1$ and $M_2$ be two open sets. We consider the intersection $M_1\cap M_2$ . For each point $x\in M_1\cap M_2$ it is $x\in M_1$ and $x\in M_2$ so statement (a) follows from the fact that $M_1$ and $M_2$ are open. The same holds also for statement (b). Therefore we get that the above defines a topology in $X$ . Is that correct and complete? Could you give a hint for (2) ?","We consider the set , i.e. consists of and an additional point . We say that is open if: (a) For each point there exists an such that . (b) If then there is an such that . Show that this defines a topology in . Show that with this topology is locally Euclidean but not Hausdorff. For (1) we have to show that and are open, the union of two open sets is open and the intersection of two open sets is open. First we show that is open : (a) For each point there exists an such that . This is true since every neighboorhood of is contained. (b) If then there is an such that . Thisis true since the union of the intervals is a subspace of the real line. Is that correct? The emptyset is per definition open, or not? We don’t have to apply the given definition, do we? Let and be two open sets. We consider the union . For each point it is either or (or both) so statement (a) follows from the fact that and/or are open. The same holds also for statement (b). Let and be two open sets. We consider the intersection . For each point it is and so statement (a) follows from the fact that and are open. The same holds also for statement (b). Therefore we get that the above defines a topology in . Is that correct and complete? Could you give a hint for (2) ?","X=\mathbb{R}\cup \{\star\} X \mathbb{R} \star U\subset X x\in U\cap \mathbb{R} \epsilon>0 (x-\epsilon, x+\epsilon)\subset U \star \in U \epsilon>0 (-\epsilon , 0)\cup (0, \epsilon)\subset U  X X  X \emptyset X x\in X\cap \mathbb{R}=\mathbb{R} \epsilon>0 (x-\epsilon, x+\epsilon)\subset X=\mathbb{R}\cup \{\star\} x \star \in X \epsilon>0 (-\epsilon , 0)\cup (0, \epsilon)\subset X=\mathbb{R}\cup \{\star\} M_1 M_2 M_1\cup M_2 x\in M_1\cup M_2 x\in M_1 x\in M_2 M_1 M_2 M_1 M_2 M_1\cap M_2 x\in M_1\cap M_2 x\in M_1 x\in M_2 M_1 M_2 X","['general-topology', 'analysis']"
17,What does this property of the basis try to generalize?,What does this property of the basis try to generalize?,,Given a topological space $X$ and $x  \subset B_1 \cap B_2$ where $B_1$ and $B_2$ are basis elements then there exists a basis $B_3$ such that $ x \in B_3 \subset B_1 \cap B_2$ .Is it trying to generalize the connectedness property of a topological space ?,Given a topological space and where and are basis elements then there exists a basis such that .Is it trying to generalize the connectedness property of a topological space ?,X x  \subset B_1 \cap B_2 B_1 B_2 B_3  x \in B_3 \subset B_1 \cap B_2,"['general-topology', 'analysis']"
18,"Surface area of revolution $y=\sin{x}$ for $x\in[0,\pi]$ , I always get half of the correct answer","Surface area of revolution  for  , I always get half of the correct answer","y=\sin{x} x\in[0,\pi]","So we know the formula for the surface area of revolution: $$\left|S\right|=2\pi\displaystyle\int\limits_a^b\left|f(x)\right|\sqrt{1+\left(f'(x)\right)^2}  \space dx$$ For this question: $$\left|S\right|=2\pi\displaystyle\int\limits_0^{\pi}\left(\sin{x}\right)\sqrt{1+\cos^2{x}}  \space dx$$ So I decided to use substitution method for $\underline{\cos{x}=a}$ : $$-2\pi\displaystyle\int\sqrt{1+a^2}\space da$$ After this part, it gets more complicated. Now I know about hyperbolic trigonometric functions, however, I must solve this without using them. Hence I searched and found this and this videos where the integration is solved the supposed way, the answer being $\dfrac{1}{2}\sqrt{1+x^2}\cdot x+\dfrac{1}{2}\ln{\left|\sqrt{1+x^2}+x\right|}+C$ , which for me, is: $$-\dfrac{\pi}{2}\left[\sqrt{1+a^2}\cdot a+\ln{\left|\sqrt{1+a^2}+a\right|}\right]+C=-\dfrac{\pi}{2}\left[\sqrt{1+\cos^2{x}}\cdot\cos{x}+\ln{\left|\sqrt{1+\cos^2{x}}+\cos{x}\right|}\right]\Bigg|_0^{\pi}$$ When we calculate, we get: $-\dfrac{\pi}{2}\left[-2\sqrt{2}+\ln{\left(\dfrac{\sqrt{2}-1}{\sqrt{2}+1}\right)}\right]\approx7.21$ The problem is, when I calculate this problem on WolframAlpha , it gives me about $14.42$ , which is twice my answer. I am unsure where did I make the mistake.","So we know the formula for the surface area of revolution: For this question: So I decided to use substitution method for : After this part, it gets more complicated. Now I know about hyperbolic trigonometric functions, however, I must solve this without using them. Hence I searched and found this and this videos where the integration is solved the supposed way, the answer being , which for me, is: When we calculate, we get: The problem is, when I calculate this problem on WolframAlpha , it gives me about , which is twice my answer. I am unsure where did I make the mistake.","\left|S\right|=2\pi\displaystyle\int\limits_a^b\left|f(x)\right|\sqrt{1+\left(f'(x)\right)^2} 
\space dx \left|S\right|=2\pi\displaystyle\int\limits_0^{\pi}\left(\sin{x}\right)\sqrt{1+\cos^2{x}} 
\space dx \underline{\cos{x}=a} -2\pi\displaystyle\int\sqrt{1+a^2}\space da \dfrac{1}{2}\sqrt{1+x^2}\cdot x+\dfrac{1}{2}\ln{\left|\sqrt{1+x^2}+x\right|}+C -\dfrac{\pi}{2}\left[\sqrt{1+a^2}\cdot a+\ln{\left|\sqrt{1+a^2}+a\right|}\right]+C=-\dfrac{\pi}{2}\left[\sqrt{1+\cos^2{x}}\cdot\cos{x}+\ln{\left|\sqrt{1+\cos^2{x}}+\cos{x}\right|}\right]\Bigg|_0^{\pi} -\dfrac{\pi}{2}\left[-2\sqrt{2}+\ln{\left(\dfrac{\sqrt{2}-1}{\sqrt{2}+1}\right)}\right]\approx7.21 14.42","['calculus', 'integration', 'analysis', 'definite-integrals', 'surface-integrals']"
19,Folland proof of proposition 4.1,Folland proof of proposition 4.1,,"screenshot of proposition from folland here I am having trouble understanding the last part of the proof. Since $x \not\in A \cup acc(A) $ , then there exists a open $U$ containing $x$ such that $U \cap A = \emptyset$ . I do not understand how this implies $\overline{A} \subset U^c$ . My question arises since $A \subset \overline{A}$ , how can we know that $\overline{A} \cap U = \emptyset$ ? EDIT: I have been thinking about this a bit more and I think I have the solution: Let $x \in \overline{A}\backslash A$ and assume towards contradiction that $x\not\in acc(A)$ . Then there exists an open set $U$ such that $U \cap A = \emptyset$ . Then $\overline{A} \cap U^c$ is a closed set containing $A$ . Since $\overline{A}$ is the smallest set containing $A$ , we have a contradiction unless $\overline{A}\subset U^c$ which requires $\overline{A}\cap U = \emptyset$ . Is this correct? Also it seems to be a lot to leave out in a proof that doesn't seem to be a sketch. Is there a more concise way of thinking about this?","screenshot of proposition from folland here I am having trouble understanding the last part of the proof. Since , then there exists a open containing such that . I do not understand how this implies . My question arises since , how can we know that ? EDIT: I have been thinking about this a bit more and I think I have the solution: Let and assume towards contradiction that . Then there exists an open set such that . Then is a closed set containing . Since is the smallest set containing , we have a contradiction unless which requires . Is this correct? Also it seems to be a lot to leave out in a proof that doesn't seem to be a sketch. Is there a more concise way of thinking about this?",x \not\in A \cup acc(A)  U x U \cap A = \emptyset \overline{A} \subset U^c A \subset \overline{A} \overline{A} \cap U = \emptyset x \in \overline{A}\backslash A x\not\in acc(A) U U \cap A = \emptyset \overline{A} \cap U^c A \overline{A} A \overline{A}\subset U^c \overline{A}\cap U = \emptyset,"['general-topology', 'analysis']"
20,To prove it is a Schauder basis?,To prove it is a Schauder basis?,,"In Classical Banach Spaces I and II by Lindenstrauss and Tzafriri proposition 1.a.3 Let $\{x_n\}_{n=1}^\infty$ be a sequence of vectors in $X.$ Then $\{x_n\}_{n=1}^\infty$ is a Schauder basis of $X$ if and only if the following three conditions hold. $x_n \neq 0$ for all $n.$ There is a constant $K$ so that, for every choice of scalars $\{a_i\}_{i=1}^\infty$ and integers $n <m,$ we have $$\|\sum_{i=1}^n a_i x_i\| \leq K \|\sum_{i=1}^m a_i x_i\|.$$ The closed linear span of $\{x_n\}_{n=1}^\infty$ is all of $X.$ Then in the rest of the book to show that say $\{x_i\}_{i=1}^\infty$ is a Schauder basis of a Banach space X, he always say by proposition 1.a.3 we have to show that the operators $\{P_n\}_{n=1}^\infty,$ defined by $P_n x = \sum_{i=1}^n x_i^*(x) x_i,$ are uniformly bounded Q1 How proposition 1.a.3 enables us to do that? Any help will be appreciated","In Classical Banach Spaces I and II by Lindenstrauss and Tzafriri proposition 1.a.3 Let be a sequence of vectors in Then is a Schauder basis of if and only if the following three conditions hold. for all There is a constant so that, for every choice of scalars and integers we have The closed linear span of is all of Then in the rest of the book to show that say is a Schauder basis of a Banach space X, he always say by proposition 1.a.3 we have to show that the operators defined by are uniformly bounded Q1 How proposition 1.a.3 enables us to do that? Any help will be appreciated","\{x_n\}_{n=1}^\infty X. \{x_n\}_{n=1}^\infty X x_n \neq 0 n. K \{a_i\}_{i=1}^\infty n <m, \|\sum_{i=1}^n a_i x_i\| \leq K \|\sum_{i=1}^m a_i x_i\|. \{x_n\}_{n=1}^\infty X. \{x_i\}_{i=1}^\infty \{P_n\}_{n=1}^\infty, P_n x = \sum_{i=1}^n x_i^*(x) x_i,","['real-analysis', 'analysis', 'schauder-basis']"
21,How to prove the following integral equation? $\int_{0}^{c}x^2f(x)=0$,How to prove the following integral equation?,\int_{0}^{c}x^2f(x)=0,"Let $f:\mathbb{R}\to \mathbb{R}$ be a continuous function such that $\int_{0}^{1}f(x)(1-x)dx=0$ . Prove that there exists $c\in(0, 1)$ such that $$\int_{0}^{c}x^2f(x)=0$$ Didn't try anything because I don't know how to approach it.",Let be a continuous function such that . Prove that there exists such that Didn't try anything because I don't know how to approach it.,"f:\mathbb{R}\to \mathbb{R} \int_{0}^{1}f(x)(1-x)dx=0 c\in(0, 1) \int_{0}^{c}x^2f(x)=0","['calculus', 'integration', 'analysis']"
22,"Prob. 23, Chap. 2, in Royden's REAL ANALYSIS: How is $m^{***}(A)=\sup\{m^*(F) | F \subset A, F \mbox{ closed} \}$ related to the outer measure?","Prob. 23, Chap. 2, in Royden's REAL ANALYSIS: How is  related to the outer measure?","m^{***}(A)=\sup\{m^*(F) | F \subset A, F \mbox{ closed} \}","Here is Prob. 23, Chap. 2, in the book Real Analysis by H.L. Royden and P.M. Fitzpatrick, 4th edition: For any set $A$ , define $m^{***}(A) \in [0, \infty]$ by $$ m^{***}(A) = \sup \left\{ m^*(F) \, | \, F \subseteq  A, F \mbox{ closed} \right\}. $$ How is this set function $m^{***}$ related to $m^*$ ? My Attempt: If $F$ is any (closed) set such that $F \subseteq A$ , then by the monotonicity of the outer measure, we must have $m^*(F) \leq m^*(A)$ , and thus $m^*(A)$ is an upper bound for the set $$ \left\{ m^*(F) \, | \, F \subseteq  A, F \mbox{ closed} \right\}. \tag{1}  $$ Therefore we must have $$ m^{***}(A) \leq m^*(A). \tag{2}  $$ How to establish the reverse inequality? Here our set can either be measurable or not, and also either $m^*(A) = \infty$ or $m^*(A) < \infty$ . How to show in each one of the above four cases that $$ m^*(A) \leq m^{***}(A)?  $$ PS: Here is my attempted proof of the reverse inequality of (2) above. If $m^{***}(A) = \infty$ , then we trivially have $m^*(A) \leq m^{***}(A)$ . So let us assume that $m^{***}(A) < \infty$ . Then, for any closed set $F$ such that $F \subseteq A$ , we must have $m^*(F) \leq m^{***}(A) < \infty$ . Thus we have $m^*(F) < \infty$ for any closed set $F$ contained in $A$ . If $A$ is measurable, then by Theorem 11 (iii), Chap. 2, in Royden, for any real number $\epsilon > 0$ , there exists a closed set $F_\epsilon$ contained in $A$ for which $$ m^* \left( A \setminus F_\epsilon \right) < \epsilon. \tag{3}  $$ But as $F_\epsilon$ is a closed set, so $F_\epsilon$ is measurable, and as $F_\epsilon \subseteq A$ , so we also have $m^*\left( F_\epsilon \right) < \infty$ . Therefore (3) together with the excision property yields $$ m^*(A) - m^* \left( F_\epsilon \right) = m^* \left( A \setminus F_\epsilon \right) < \epsilon,  $$ which implies $$ m^*(A) < m^* \left( F_\epsilon \right) + \epsilon \leq m^{***}(A) + \epsilon, $$ which in turn implies $$ m^*(A) \leq m^{***}(A) + \epsilon  $$ for any real number $\epsilon > 0$ , and therefore we can conclude that $$ m^*(A) \leq m^{***}(A). \tag{4}  $$ If $A$ is not measurable, then by Prob. Prob. 17, Chap. 2, in Royden, there exists a real number $\epsilon_0 > 0$ such that, for any open set $O$ and for any closed set $F$ such that $F \subseteq A \subset O$ , we have $$ m^*(O \setminus F ) \geq \epsilon_0. \tag{5}  $$ But any closed set is measurable, and in particular any closed set $F$ contained in $A$ is measurable with finite outer measure; we therefore have $$ m^*(O \setminus F) = m^*(O) - m^*(F), $$ by the excision property, and the last identity together with (5) gives $$ m^*(O) - m*(F) \geq \epsilon_0, $$ and thus we have $$ m^*(O) \geq m^*(F) + \epsilon_0. \tag{5}  $$ What next? How to proceed from here and prove that (4) above holds in the case when $A$ is not measurable?","Here is Prob. 23, Chap. 2, in the book Real Analysis by H.L. Royden and P.M. Fitzpatrick, 4th edition: For any set , define by How is this set function related to ? My Attempt: If is any (closed) set such that , then by the monotonicity of the outer measure, we must have , and thus is an upper bound for the set Therefore we must have How to establish the reverse inequality? Here our set can either be measurable or not, and also either or . How to show in each one of the above four cases that PS: Here is my attempted proof of the reverse inequality of (2) above. If , then we trivially have . So let us assume that . Then, for any closed set such that , we must have . Thus we have for any closed set contained in . If is measurable, then by Theorem 11 (iii), Chap. 2, in Royden, for any real number , there exists a closed set contained in for which But as is a closed set, so is measurable, and as , so we also have . Therefore (3) together with the excision property yields which implies which in turn implies for any real number , and therefore we can conclude that If is not measurable, then by Prob. Prob. 17, Chap. 2, in Royden, there exists a real number such that, for any open set and for any closed set such that , we have But any closed set is measurable, and in particular any closed set contained in is measurable with finite outer measure; we therefore have by the excision property, and the last identity together with (5) gives and thus we have What next? How to proceed from here and prove that (4) above holds in the case when is not measurable?","A m^{***}(A) \in [0, \infty] 
m^{***}(A) = \sup \left\{ m^*(F) \, | \, F \subseteq  A, F \mbox{ closed} \right\}.
 m^{***} m^* F F \subseteq A m^*(F) \leq m^*(A) m^*(A) 
\left\{ m^*(F) \, | \, F \subseteq  A, F \mbox{ closed} \right\}. \tag{1} 
 
m^{***}(A) \leq m^*(A). \tag{2} 
 m^*(A) = \infty m^*(A) < \infty 
m^*(A) \leq m^{***}(A)? 
 m^{***}(A) = \infty m^*(A) \leq m^{***}(A) m^{***}(A) < \infty F F \subseteq A m^*(F) \leq m^{***}(A) < \infty m^*(F) < \infty F A A \epsilon > 0 F_\epsilon A 
m^* \left( A \setminus F_\epsilon \right) < \epsilon. \tag{3} 
 F_\epsilon F_\epsilon F_\epsilon \subseteq A m^*\left( F_\epsilon \right) < \infty 
m^*(A) - m^* \left( F_\epsilon \right) = m^* \left( A \setminus F_\epsilon \right) < \epsilon, 
 
m^*(A) < m^* \left( F_\epsilon \right) + \epsilon \leq m^{***}(A) + \epsilon,
 
m^*(A) \leq m^{***}(A) + \epsilon 
 \epsilon > 0 
m^*(A) \leq m^{***}(A). \tag{4} 
 A \epsilon_0 > 0 O F F \subseteq A \subset O 
m^*(O \setminus F ) \geq \epsilon_0. \tag{5} 
 F A 
m^*(O \setminus F) = m^*(O) - m^*(F),
 
m^*(O) - m*(F) \geq \epsilon_0,
 
m^*(O) \geq m^*(F) + \epsilon_0. \tag{5} 
 A","['real-analysis', 'analysis', 'measure-theory', 'outer-measure']"
23,Is the limit function continuous?,Is the limit function continuous?,,Is this function continuous or not? $$y\ =\ \lim _{a\to +\infty }\bigg[\frac{\ln \left(1+e^{\alpha \cdot x}\right)}{\ln \left(1+e^{\alpha }\right)}\bigg]$$ I computed that limit of function as $a\to+\infty$ is $x$ . How I proceed from here for proving the continuous part?,Is this function continuous or not? I computed that limit of function as is . How I proceed from here for proving the continuous part?,y\ =\ \lim _{a\to +\infty }\bigg[\frac{\ln \left(1+e^{\alpha \cdot x}\right)}{\ln \left(1+e^{\alpha }\right)}\bigg] a\to+\infty x,"['real-analysis', 'calculus', 'analysis', 'functions', 'continuity']"
24,$\int_{\Omega} f \psi = 0$ for every $\psi$ which is zero at $\partial \Omega$ imples $f=0$,for every  which is zero at  imples,\int_{\Omega} f \psi = 0 \psi \partial \Omega f=0,"This is a generalization of my previous question . Let $I=[t_{0},t_{1}]\subset \mathbb{R}$ be fixed and $\Omega \subset \mathbb{R}^{n}$ . Let $\psi$ and $f$ be sufficiently differentiable (we can assume it to be smooth) and let us assume that $\psi(t_{0},x)= \psi(t_{1},x) = 0$ for every $x \in \mathbb{R}^{n}$ and $\psi(t,x)|_{\partial \Omega} = 0$ for every $t \in I$ . If: $$\int_{I}\int_{\Omega}f(t,x)\psi(t,x)dxdt = 0$$ for every $\psi$ satisfying the above conditions, is it true that $f(t,x) \equiv 0$ ?","This is a generalization of my previous question . Let be fixed and . Let and be sufficiently differentiable (we can assume it to be smooth) and let us assume that for every and for every . If: for every satisfying the above conditions, is it true that ?","I=[t_{0},t_{1}]\subset \mathbb{R} \Omega \subset \mathbb{R}^{n} \psi f \psi(t_{0},x)= \psi(t_{1},x) = 0 x \in \mathbb{R}^{n} \psi(t,x)|_{\partial \Omega} = 0 t \in I \int_{I}\int_{\Omega}f(t,x)\psi(t,x)dxdt = 0 \psi f(t,x) \equiv 0","['integration', 'analysis', 'multivariable-calculus']"
25,Bounded subsets (supremum and infimum),Bounded subsets (supremum and infimum),,"The problem is as follows: Let $S$ and $T$ be nonempty, bounded subsets of the real numbers. Prove that if $T$ contains $S$ , then $\inf T\leq\inf S\leq \sup S\leq\sup T.$ My problem is that I don’t understand why the parent set has the greatest lower bound and the least upper bound. Help. Please and thank you.","The problem is as follows: Let and be nonempty, bounded subsets of the real numbers. Prove that if contains , then My problem is that I don’t understand why the parent set has the greatest lower bound and the least upper bound. Help. Please and thank you.",S T T S \inf T\leq\inf S\leq \sup S\leq\sup T.,"['real-analysis', 'analysis', 'supremum-and-infimum']"
26,Proving $ 1+2f'(x)+\frac{2}{x(1+x^2)}\left(\frac{3x}{2}+f(x) \right)\ge \frac{6x^2}{1+8x^2} $.,Proving ., 1+2f'(x)+\frac{2}{x(1+x^2)}\left(\frac{3x}{2}+f(x) \right)\ge \frac{6x^2}{1+8x^2} ,"Put \begin{align*} f(x)=\left( -\frac{x}{2} +\sqrt{\frac{1}{27}+\frac{x^2}{4}}  \right)^{1/3}-\left( \frac{x}{2} +\sqrt{\frac{1}{27}+\frac{x^2}{4}}  \right)^{1/3} \end{align*} Prove that $$ g(x):=1+2f'(x)+\frac{2}{x(1+x^2)}\left(\frac{3x}{2}+f(x) \right)\ge \frac{6x^2}{1+8x^2} $$ My attempt I put \begin{align*} A=\left( -\frac{x}{2} +\sqrt{\frac{1}{27}+\frac{x^2}{4}}  \right)^{1/3}\quad B=\left( \frac{x}{2} +\sqrt{\frac{1}{27}+\frac{x^2}{4}}  \right)^{1/3} \end{align*} and then \begin{align*} f'(x)=-\frac{1}{3}\frac{1}{A^2-AB+B^2} \end{align*} where $AB=\frac13$ . But I don't know how to continue. I know that $g(x)$ is an even function. Via mathematica I find that $$\left[(1+x^2)g(x)\right]'\ge 0\quad \forall \,x>0$$ But I also can't prove this. Any hints? Thanks in advance!",Put Prove that My attempt I put and then where . But I don't know how to continue. I know that is an even function. Via mathematica I find that But I also can't prove this. Any hints? Thanks in advance!,"\begin{align*}
f(x)=\left( -\frac{x}{2} +\sqrt{\frac{1}{27}+\frac{x^2}{4}}  \right)^{1/3}-\left( \frac{x}{2} +\sqrt{\frac{1}{27}+\frac{x^2}{4}}  \right)^{1/3}
\end{align*} 
g(x):=1+2f'(x)+\frac{2}{x(1+x^2)}\left(\frac{3x}{2}+f(x) \right)\ge \frac{6x^2}{1+8x^2}
 \begin{align*}
A=\left( -\frac{x}{2} +\sqrt{\frac{1}{27}+\frac{x^2}{4}}  \right)^{1/3}\quad
B=\left( \frac{x}{2} +\sqrt{\frac{1}{27}+\frac{x^2}{4}}  \right)^{1/3}
\end{align*} \begin{align*}
f'(x)=-\frac{1}{3}\frac{1}{A^2-AB+B^2}
\end{align*} AB=\frac13 g(x) \left[(1+x^2)g(x)\right]'\ge 0\quad \forall \,x>0","['analysis', 'derivatives', 'inequality', 'cubics']"
27,Increasing function $f: \mathbb{N} \to \mathbb{R}^{\ge 0}$ such that $f(3^n) = 2^n$ and $\forall n \exists c : f(n+c)+f(n-c) \ge 2f(n).$,Increasing function  such that  and,f: \mathbb{N} \to \mathbb{R}^{\ge 0} f(3^n) = 2^n \forall n \exists c : f(n+c)+f(n-c) \ge 2f(n).,"Can we build $f: \mathbb{N} \to \mathbb{R}^{\ge 0}$ which is increasing and satisfies the following criterions? $f(3^n) = 2^n$ for all $n.$ There exists $N$ such that for all $n \ge N,$ there exists some $c$ (which may depend on $n$ ) such that $f(n+c)+f(n-c) \ge 2f(n)$ I introduced the first criterion to make it easier to come up with the function on paper. I made a table using graph paper listing $n$ and $f(n)$ for $n = 1, \dots, 33$ and then put $1,2,4,8,\dots$ under the powers of $3.$ Now I'm having difficulties filling out this table so that the second criterion is satisfied Remark: Even if this task is possible, $N$ might be insanely large. However, trying to beg for a larger $N$ is just delaying the inevitable. In any case, I can just let the first $N$ terms be something like $\epsilon, 2\epsilon \dots, \epsilon N,$ but the real challenge begins after I've put down those terms because from then on, there is no violation of the second criterion. With that being said, you might as well try to see if a low value of $N$ works. 1|2|3|4|5|6|7|8|9|10|11|12|13|14|15|16|17|18|19|20|21|22|23|24|25|26|27|28|29|30|31|32|33 1|?|2|?|?|?|?|?|4|?|?|?|?|?|?|?|?|?|?|?|?|?|?|?|?|?|8|?|?|?|?|?|?| Update: An arithmetic progression between the powers of two almost works. It only  fails right on the powers of $2.$ So maybe there's some way to modify this idea that works.","Can we build which is increasing and satisfies the following criterions? for all There exists such that for all there exists some (which may depend on ) such that I introduced the first criterion to make it easier to come up with the function on paper. I made a table using graph paper listing and for and then put under the powers of Now I'm having difficulties filling out this table so that the second criterion is satisfied Remark: Even if this task is possible, might be insanely large. However, trying to beg for a larger is just delaying the inevitable. In any case, I can just let the first terms be something like but the real challenge begins after I've put down those terms because from then on, there is no violation of the second criterion. With that being said, you might as well try to see if a low value of works. 1|2|3|4|5|6|7|8|9|10|11|12|13|14|15|16|17|18|19|20|21|22|23|24|25|26|27|28|29|30|31|32|33 1|?|2|?|?|?|?|?|4|?|?|?|?|?|?|?|?|?|?|?|?|?|?|?|?|?|8|?|?|?|?|?|?| Update: An arithmetic progression between the powers of two almost works. It only  fails right on the powers of So maybe there's some way to modify this idea that works.","f: \mathbb{N} \to \mathbb{R}^{\ge 0} f(3^n) = 2^n n. N n \ge N, c n f(n+c)+f(n-c) \ge 2f(n) n f(n) n = 1, \dots, 33 1,2,4,8,\dots 3. N N N \epsilon, 2\epsilon \dots, \epsilon N, N 2.","['analysis', 'functions', 'functional-inequalities']"
28,"Conjecture If f is surjective then there exists x $\in$ (a, b) such that $|f'(x)| = 1$","Conjecture If f is surjective then there exists x  (a, b) such that",\in |f'(x)| = 1,"Conjecture Let $f$ be a continuous function from [a, b] to [a, b], and is differentiable on (a, b). If f is surjective then there exists x $\in$ (a, b) such that $|f'(x)| = 1$ Any counter example for this conjecture ? **Addition after Kavi Rama Murthy'answer **, we can improve the problem by: If $f(a)\leq f(b)$ and f is surjective then there exists x $\in$ (a, b) such that $f'(x)= 1$","Conjecture Let be a continuous function from [a, b] to [a, b], and is differentiable on (a, b). If f is surjective then there exists x (a, b) such that Any counter example for this conjecture ? **Addition after Kavi Rama Murthy'answer **, we can improve the problem by: If and f is surjective then there exists x (a, b) such that",f \in |f'(x)| = 1 f(a)\leq f(b) \in f'(x)= 1,['analysis']
29,The sign of the second order derivative sign,The sign of the second order derivative sign,,"Suppose that we know the sign of $\frac{\partial f(x,y)}{\partial x}$ and $ \frac{\partial f(x,y)}{\partial y}$ on a specific domain. Can we say that the sign of $ \frac{\partial^2 f(x,y)}{\partial x \partial y}$ is basically the product of the signs of the first order derivatives? Example: $f(x,y)=\frac{1}{x y}$ : $\frac{\partial f(x,y)}{\partial x}\leq 0$ and $ \frac{\partial f(x,y)}{\partial y}\leq 0$ and $ \frac{\partial^2 f(x,y)}{\partial x \partial y}\geq 0$ for all $x\geq 0$ and $y\geq 0$ . If this is correct, do you have a source? My search engine attempts didn't give me something :-( Thanks a lot","Suppose that we know the sign of and on a specific domain. Can we say that the sign of is basically the product of the signs of the first order derivatives? Example: : and and for all and . If this is correct, do you have a source? My search engine attempts didn't give me something :-( Thanks a lot","\frac{\partial f(x,y)}{\partial x}  \frac{\partial f(x,y)}{\partial y}  \frac{\partial^2 f(x,y)}{\partial x \partial y} f(x,y)=\frac{1}{x y} \frac{\partial f(x,y)}{\partial x}\leq 0  \frac{\partial f(x,y)}{\partial y}\leq 0  \frac{\partial^2 f(x,y)}{\partial x \partial y}\geq 0 x\geq 0 y\geq 0","['analysis', 'derivatives']"
30,"Showing that if $\forall n \ge 0: \int_{-1}^{1} f(t)t^{2n+1} dt = 0$ and $f$ is continuous, then $f$ must be even","Showing that if  and  is continuous, then  must be even",\forall n \ge 0: \int_{-1}^{1} f(t)t^{2n+1} dt = 0 f f,"I am trying to prove the following problem: Suppose that $f:[-1,1] \to \mathbb{R}$ is continuous and for sufficiently large $n$ , we have $$ \int_{-1}^{1} f(t)t^{2n+1} dt =0$$ Show that $f$ is even. I believe I have seen a similar problem before, and the solution inovolved some manipulations like I do above. However, I'm unable to come up with the complete solution. I do not remember clearly, but the solution may or may not involve the Stone-Weierstrass Theorem. My Attempt: Instead of ""sufficiently large $n$ "", we can assume $n \ge 0$ . It is easy to generalize later. Using the substitution $t \mapsto -t$ , we get that $$\int_{-1}^1 f(-t) t^{2n+1}.$$ Subtracting the two integrals gives $$\int_{-1}^1[f(t)-f(-t)] t^{2n+1} dt=0$$ Let $g(t) =  f(t)-f(-t)$ . This function $g$ is in fact odd. However, I'm not sure what to do after this point.","I am trying to prove the following problem: Suppose that is continuous and for sufficiently large , we have Show that is even. I believe I have seen a similar problem before, and the solution inovolved some manipulations like I do above. However, I'm unable to come up with the complete solution. I do not remember clearly, but the solution may or may not involve the Stone-Weierstrass Theorem. My Attempt: Instead of ""sufficiently large "", we can assume . It is easy to generalize later. Using the substitution , we get that Subtracting the two integrals gives Let . This function is in fact odd. However, I'm not sure what to do after this point.","f:[-1,1] \to \mathbb{R} n  \int_{-1}^{1} f(t)t^{2n+1} dt =0 f n n \ge 0 t \mapsto -t \int_{-1}^1 f(-t) t^{2n+1}. \int_{-1}^1[f(t)-f(-t)] t^{2n+1} dt=0 g(t) =  f(t)-f(-t) g","['real-analysis', 'calculus', 'integration', 'analysis']"
31,Find the sum $\sum _{n=1}^{\infty}a_1a_2a_3...a_n $ where $a_{n+1}=\ln\frac{e^{a_n}-1}{a_n}$.,Find the sum  where .,\sum _{n=1}^{\infty}a_1a_2a_3...a_n  a_{n+1}=\ln\frac{e^{a_n}-1}{a_n},"Let $\{a_n\} $ be defined as follows: $a_1 \gt 0$ and $$a_{n+1}=\ln\frac{e^{a_n}-1}{a_n}$$ for $n\ge 1.$ Then the sum $$\sum_{n=1}^{\infty}a_1a_2...a_n $$ is _____________ My attempt:(By using hint ) By Given condition $a_{n+1}-a_n=\ln\frac{e^{a_n}-1}{a_n}-lne^{a_n} $ $\Rightarrow a_{n+1}-a_n=\ln\frac{1-e^{-a_n}}{a_n}        \quad    (1) $ Now $e^x \gt 1+x \space \forall x \in R \quad (2)$ $\Rightarrow  e^{-a_n} \gt 1-a_n $ From (1) $ a_{n+1}-a_n \lt \ln\frac{1-1+a_n}{a_n}=0  $ Also $a_{n+1} \gt \ln \frac{a_n}{a_n}=0 $ using (2) for $x=a_n$ So $\{a_n\} $ is a nonotone decreasing sequence bounded below. Let $lim_{n\rightarrow \infty}a_n=l $ . Then by the given recurrence relation $l=\ln \frac{e^l-1}{l} $ $\Rightarrow e^l(l-1)+1=0 $ is satisfied by $l=0$ Hence $ \exists k\in N$ s.t $ \forall n\ge k$ ,we have $a_n \lt 1$ Let $a_1a_2...a_{k-1}=p $ Then $\sum_{n=k}^{\infty}a_1a_2...a_n=p\{a_k+a_ka_{k+1}+...\} $ $\Rightarrow \sum_{n=k}^{\infty}a_1a_2...a_n \lt p\{a_k+a_k^2+...\}$ since $a_n$ is monotone decreasing . $\Rightarrow \sum_{n=k}^{\infty}a_1a_2...a_n \lt p \frac{a_k}{1-a_k} $ So $\sum_{n=1}^{\infty}a_1a_2...a_n \lt S+p\frac{a_k}{1-a_k} $ where $ S=\sum_{n=1}^{k-1}a_1a_2...a_n $ So the given sereis is convergent. I would like to know if my workings are correct.I don't know how to find the actual sum.Also I have seen an answer here  ( What is $\sum_{n=1}^{\infty}a_1 a_2...a_n$? ) but unfortunately I can't understand it.Please help me in finding the sum.Thanks in advance.","Let be defined as follows: and for Then the sum is _____________ My attempt:(By using hint ) By Given condition Now From (1) Also using (2) for So is a nonotone decreasing sequence bounded below. Let . Then by the given recurrence relation is satisfied by Hence s.t ,we have Let Then since is monotone decreasing . So where So the given sereis is convergent. I would like to know if my workings are correct.I don't know how to find the actual sum.Also I have seen an answer here  ( What is $\sum_{n=1}^{\infty}a_1 a_2...a_n$? ) but unfortunately I can't understand it.Please help me in finding the sum.Thanks in advance.","\{a_n\}  a_1 \gt 0 a_{n+1}=\ln\frac{e^{a_n}-1}{a_n} n\ge 1. \sum_{n=1}^{\infty}a_1a_2...a_n  a_{n+1}-a_n=\ln\frac{e^{a_n}-1}{a_n}-lne^{a_n}  \Rightarrow a_{n+1}-a_n=\ln\frac{1-e^{-a_n}}{a_n}       
\quad    (1)  e^x \gt 1+x \space \forall x \in R \quad (2) \Rightarrow  e^{-a_n} \gt 1-a_n   a_{n+1}-a_n \lt \ln\frac{1-1+a_n}{a_n}=0   a_{n+1} \gt \ln \frac{a_n}{a_n}=0  x=a_n \{a_n\}  lim_{n\rightarrow \infty}a_n=l  l=\ln \frac{e^l-1}{l}  \Rightarrow e^l(l-1)+1=0  l=0  \exists k\in N  \forall n\ge k a_n \lt 1 a_1a_2...a_{k-1}=p  \sum_{n=k}^{\infty}a_1a_2...a_n=p\{a_k+a_ka_{k+1}+...\}  \Rightarrow \sum_{n=k}^{\infty}a_1a_2...a_n \lt p\{a_k+a_k^2+...\} a_n \Rightarrow \sum_{n=k}^{\infty}a_1a_2...a_n \lt p \frac{a_k}{1-a_k}  \sum_{n=1}^{\infty}a_1a_2...a_n \lt S+p\frac{a_k}{1-a_k}   S=\sum_{n=1}^{k-1}a_1a_2...a_n ","['sequences-and-series', 'analysis', 'convergence-divergence']"
32,Proving existence of a limit,Proving existence of a limit,,"I try to prove that the folowing limit does not exist. $$\displaystyle\lim_{(x,y)\to(0,0)} \frac{x\sin (ax^2+by^2)}{\sqrt{x^2+y^2}}, a,b>0, a\neq b$$ I make the assumption that this limit exist. So, I try to write the limit $$\displaystyle\lim_{(x,y)\to(0,0)} \frac{x}{\sqrt{x^2+y^2}}$$ as a limit of the function $ \dfrac{x\sin (ax^2+by^2)}{\sqrt{x^2+y^2}}$ and another function, that the limits of those functions exist, and thus from the limits function algebra, the $\displaystyle\lim_{(x,y)\to(0,0)} \frac{x}{\sqrt{x^2+y^2}}$ exists which is a contradiction, because this limit obviously does not exist. Maybe the limit $\displaystyle\lim_{(x,y)\to (0,0)} \dfrac{\sin (ax^2+by^2)}{ax^2+by^2}=1$ can help in someway. Any Ideas?? Thank you","I try to prove that the folowing limit does not exist. I make the assumption that this limit exist. So, I try to write the limit as a limit of the function and another function, that the limits of those functions exist, and thus from the limits function algebra, the exists which is a contradiction, because this limit obviously does not exist. Maybe the limit can help in someway. Any Ideas?? Thank you","\displaystyle\lim_{(x,y)\to(0,0)} \frac{x\sin (ax^2+by^2)}{\sqrt{x^2+y^2}}, a,b>0, a\neq b \displaystyle\lim_{(x,y)\to(0,0)} \frac{x}{\sqrt{x^2+y^2}}  \dfrac{x\sin (ax^2+by^2)}{\sqrt{x^2+y^2}} \displaystyle\lim_{(x,y)\to(0,0)} \frac{x}{\sqrt{x^2+y^2}} \displaystyle\lim_{(x,y)\to (0,0)} \dfrac{\sin (ax^2+by^2)}{ax^2+by^2}=1","['real-analysis', 'calculus', 'analysis']"
33,Surjective vector field on $\mathbb{R}^n$,Surjective vector field on,\mathbb{R}^n,"Let $V: \mathbb{R}^n \rightarrow \mathbb{R}^n$ be continuous with the property $$\frac{\langle V(x), \, x\rangle}{|x|} \, \to \infty \quad \text{as} \quad |x| \to \infty  \qquad \qquad (1)$$ where $\langle \cdot \, , \cdot \rangle$ denote the standard inner product and $| \cdot | = \sqrt{\langle \cdot \, , \cdot \rangle}$ is the Euclidian norm on $\mathbb{R}^n$ . I have to show that $V$ is surjective. My attempt: Take $z \in \mathbb{R}^n$ and define $\varphi: \mathbb{R}^n \rightarrow \mathbb{R}^n, \, \varphi(x): = V(x) - z$ . The aim is to show that $\varphi$ has a zero. So assume by contradiction that $\varphi(x)\neq 0 \, \, \, \forall \, x \in \mathbb{R}^n$ . Let $R > 0$ . We define an auxiliary function $\psi : \mathbb{R}^n \rightarrow \mathbb{R}^n, \, \psi(x): = R \cdot \frac{\varphi(x)}{|\varphi(x)|}$ . Then, $\psi$ is a continuous self-mapping $\psi: \overline{B}_R(0) \rightarrow \overline{B}_R(0)$ with $\text{im}(\psi) \subset \partial B_R(0)$ . Using Schauder's fixed point theorem, there exists $x_0 \in \overline{B}_R(0)$ s.t. $\psi(x_0) = x_0$ . In particular, $x_0 \in \partial B_R(0)$ . Now, I tried to get a contradiction with the assumption $(1)$ taking $R \to \infty$ without success. (Maybe it's not the right thing to do) Any suggestions? Thanks in advance!","Let be continuous with the property where denote the standard inner product and is the Euclidian norm on . I have to show that is surjective. My attempt: Take and define . The aim is to show that has a zero. So assume by contradiction that . Let . We define an auxiliary function . Then, is a continuous self-mapping with . Using Schauder's fixed point theorem, there exists s.t. . In particular, . Now, I tried to get a contradiction with the assumption taking without success. (Maybe it's not the right thing to do) Any suggestions? Thanks in advance!","V: \mathbb{R}^n \rightarrow \mathbb{R}^n \frac{\langle V(x), \, x\rangle}{|x|} \, \to \infty \quad \text{as} \quad |x| \to \infty  \qquad \qquad (1) \langle \cdot \, , \cdot \rangle | \cdot | = \sqrt{\langle \cdot \, , \cdot \rangle} \mathbb{R}^n V z \in \mathbb{R}^n \varphi: \mathbb{R}^n \rightarrow \mathbb{R}^n, \, \varphi(x): = V(x) - z \varphi \varphi(x)\neq 0 \, \, \, \forall \, x \in \mathbb{R}^n R > 0 \psi : \mathbb{R}^n \rightarrow \mathbb{R}^n, \, \psi(x): = R \cdot \frac{\varphi(x)}{|\varphi(x)|} \psi \psi: \overline{B}_R(0) \rightarrow \overline{B}_R(0) \text{im}(\psi) \subset \partial B_R(0) x_0 \in \overline{B}_R(0) \psi(x_0) = x_0 x_0 \in \partial B_R(0) (1) R \to \infty","['real-analysis', 'analysis', 'functions', 'vector-fields', 'fixed-point-theorems']"
34,"A notion reverse to continuity, does it have a name?","A notion reverse to continuity, does it have a name?",,"Does the following notion (in some sense reverse to continuity) have a name? What are interesting properties of this concept? $f$ is ""anticontinuous"" in $x$ if $$\forall \epsilon>0 \exists \delta>0:f[(x-\epsilon;x+\epsilon)]\supseteq (f(x)-\delta;f(x)+\delta).$$ (Here $f[X]$ is the image of a set $X$ under a map $f$ .)","Does the following notion (in some sense reverse to continuity) have a name? What are interesting properties of this concept? is ""anticontinuous"" in if (Here is the image of a set under a map .)",f x \forall \epsilon>0 \exists \delta>0:f[(x-\epsilon;x+\epsilon)]\supseteq (f(x)-\delta;f(x)+\delta). f[X] X f,"['real-analysis', 'analysis', 'continuity', 'terminology']"
35,Is $\ a + b$ defined by $\ (a + b)(x) = a(x) + b(x) $ a step function where $\ a $ and $\ b $ are step functions,Is  defined by  a step function where  and  are step functions,\ a + b \ (a + b)(x) = a(x) + b(x)  \ a  \ b ,"Note: x ∈ X $ is a step function where $ \ X ⊂ R^n $ is a finite union of boxes and $ \ a, b : X → [0,∞)$ are step functions. But I have only just started looking at step functions and I am struggling to come to terms with how this is possible. So I know that a function a is a step function if a takes only finitely many values and if there exists a finite union of boxes $\ a^{-1} ([R_i, ∞)) = $ { $\ x ∈ X | a(x)∈[R_i, ∞) $ } But I don't know how I proceed knowing this. In an example I have done, I took values from a graph and plugged them into $\ a^{-1} (R_i, ∞)) $ for some $\ R_1 $ and $\ R_2 $ to show they are both boxes... but I am unsure how we use this to show the product is a step function. Apologies if this is a juvenile question, but like I say, I am new to this topic so I am still trying to understand it.","Note: x ∈ X \ X ⊂ R^n \ a, b : X → [0,∞)$ are step functions. But I have only just started looking at step functions and I am struggling to come to terms with how this is possible. So I know that a function a is a step function if a takes only finitely many values and if there exists a finite union of boxes { } But I don't know how I proceed knowing this. In an example I have done, I took values from a graph and plugged them into for some and to show they are both boxes... but I am unsure how we use this to show the product is a step function. Apologies if this is a juvenile question, but like I say, I am new to this topic so I am still trying to understand it."," is a step function where   is a finite union of boxes and  \ a^{-1} ([R_i, ∞)) =  \ x ∈ X | a(x)∈[R_i, ∞)  \ a^{-1} (R_i, ∞))  \ R_1  \ R_2 ",['analysis']
36,Derivatives of convolution in $ \mathbb{R}^n $,Derivatives of convolution in, \mathbb{R}^n ,"I am trying to prove that if $f \in C_c^k(\mathbb{R}^n)$ and $g$ is Lebesgue integrable on $\mathbb{R}^n$ , then the derivatives of $f * g$ equal $$D^\alpha(f * g)(x) = \int_{\mathbb{R}^n}(D^\alpha f)(x-y)g(y)dy$$ and are continuous for all multi-indexes of order up to $k$ . I know the version for $\mathbb{R}$ , however I am interested in this $n$ -dimensional generalization.","I am trying to prove that if and is Lebesgue integrable on , then the derivatives of equal and are continuous for all multi-indexes of order up to . I know the version for , however I am interested in this -dimensional generalization.",f \in C_c^k(\mathbb{R}^n) g \mathbb{R}^n f * g D^\alpha(f * g)(x) = \int_{\mathbb{R}^n}(D^\alpha f)(x-y)g(y)dy k \mathbb{R} n,"['analysis', 'measure-theory', 'convolution']"
37,"How to evaluate $\iint_A \frac{1}{x^2+y^2}\,\mathrm dx\,\mathrm dy,$ where $A=[\frac{1}{a},a]\times[0,1]$ [duplicate]",How to evaluate  where  [duplicate],"\iint_A \frac{1}{x^2+y^2}\,\mathrm dx\,\mathrm dy, A=[\frac{1}{a},a]\times[0,1]","This question already has answers here : Evaluation of $\int_{\frac{1}{2014}}^{2014} \frac{\tan^{-1}x}{x} dx$ (2 answers) Closed 4 years ago . I want to compute $$\iint_A \frac{1}{x^2+y^2}\,\mathrm dx\,\mathrm dy$$ where $A:=\left[\frac{1}{a},a\right]\times[0,1]$ . I got that this is equal to $\int_{1/a} ^a \frac{1}{x}\arctan \Big(\frac{1}{x}\Big) \mathrm dx\ $ and I don't know what to do from here. Can somebody help me, please?","This question already has answers here : Evaluation of $\int_{\frac{1}{2014}}^{2014} \frac{\tan^{-1}x}{x} dx$ (2 answers) Closed 4 years ago . I want to compute where . I got that this is equal to and I don't know what to do from here. Can somebody help me, please?","\iint_A \frac{1}{x^2+y^2}\,\mathrm dx\,\mathrm dy A:=\left[\frac{1}{a},a\right]\times[0,1] \int_{1/a} ^a \frac{1}{x}\arctan \Big(\frac{1}{x}\Big) \mathrm dx\ ","['integration', 'analysis', 'definite-integrals']"
38,Rudin RCA Theorem 7.1,Rudin RCA Theorem 7.1,,"In Rudin's Real and Complex Analysis, Theorem 7.1 is provided without a proof. Unfortunately, I have a difficulty in proving it by myself. It says: Suppose $\mu$ is a complex Borel measure in $R^1$ and \begin{equation*} f(x) = \mu((-\infty,x)) \quad\quad (x\in R^1). \end{equation*} If $x\in R^1$ and $A$ is a complex number, each of the following two statements implies the other: (a) $f$ is differentiable at $x$ and $f'(x) = A$ . (b) To every $\epsilon>0$ corresponds a $\delta>0$ such that \begin{equation*} \left|\frac{\mu(I)}{m(I)} - A \right| < \epsilon \end{equation*} for every open segment $I$ that contains $x$ and whose length is less than $\delta$ .     Here $m$ denotes Lebesgue measure on $R^1$ . Here is my trial to prove that (a) imples (b). Assume (a) and let $\epsilon>0$ . Then, there exists $\delta>0$ satisfying \begin{equation*} \frac{|f(x') - f(x) - A(x'-x)|}{|x'-x|} < \frac{\epsilon}{2} \end{equation*} for all $0 < |x'-x| < \delta$ .     Let $x-\delta < \alpha < x < \beta < x+\delta$ such that $\beta-\alpha < \delta$ .     Then, \begin{equation*} \frac{|f(\alpha) - f(x) - A(\alpha - x)|}{|\alpha - x|} = \left| \frac{\mu([\alpha,x))}{x - \alpha} - A \right| < \frac{\epsilon}{2} \end{equation*} and \begin{equation*} \frac{|f(\beta) - f(x) - A(\beta - x)|}{|\beta - x|} = \left| \frac{\mu([x,\beta))}{\beta-x} - A \right| < \frac{\epsilon}{2}. \end{equation*} Thus, \begin{equation*} \left|\frac{\mu([\alpha,\beta))}{\beta-\alpha} - A\right| \le \left| \frac{\mu([\alpha,x))}{x - \alpha} - A \right| + \left| \frac{\mu([x,\beta))}{\beta-x} - A \right| < \epsilon. \end{equation*} Here is the point I stuck. I could not make $\mu([\alpha,\beta))$ to $\mu((\alpha,\beta))$ . I guess that either $\mu(\{\alpha\}) = 0$ or $\mu([\alpha_i,\beta)) \to \mu((\alpha,\beta))$ as $\alpha_i\to\alpha$ is necessary. Any help will be appreciated.","In Rudin's Real and Complex Analysis, Theorem 7.1 is provided without a proof. Unfortunately, I have a difficulty in proving it by myself. It says: Suppose is a complex Borel measure in and If and is a complex number, each of the following two statements implies the other: (a) is differentiable at and . (b) To every corresponds a such that for every open segment that contains and whose length is less than .     Here denotes Lebesgue measure on . Here is my trial to prove that (a) imples (b). Assume (a) and let . Then, there exists satisfying for all .     Let such that .     Then, and Thus, Here is the point I stuck. I could not make to . I guess that either or as is necessary. Any help will be appreciated.","\mu R^1 \begin{equation*}
f(x) = \mu((-\infty,x)) \quad\quad (x\in R^1).
\end{equation*} x\in R^1 A f x f'(x) = A \epsilon>0 \delta>0 \begin{equation*}
\left|\frac{\mu(I)}{m(I)} - A \right| < \epsilon
\end{equation*} I x \delta m R^1 \epsilon>0 \delta>0 \begin{equation*}
\frac{|f(x') - f(x) - A(x'-x)|}{|x'-x|} < \frac{\epsilon}{2}
\end{equation*} 0 < |x'-x| < \delta x-\delta < \alpha < x < \beta < x+\delta \beta-\alpha < \delta \begin{equation*}
\frac{|f(\alpha) - f(x) - A(\alpha - x)|}{|\alpha - x|} = \left| \frac{\mu([\alpha,x))}{x - \alpha} - A \right| < \frac{\epsilon}{2}
\end{equation*} \begin{equation*}
\frac{|f(\beta) - f(x) - A(\beta - x)|}{|\beta - x|} = \left| \frac{\mu([x,\beta))}{\beta-x} - A \right| < \frac{\epsilon}{2}.
\end{equation*} \begin{equation*}
\left|\frac{\mu([\alpha,\beta))}{\beta-\alpha} - A\right| \le \left| \frac{\mu([\alpha,x))}{x - \alpha} - A \right| + \left| \frac{\mu([x,\beta))}{\beta-x} - A \right| < \epsilon.
\end{equation*} \mu([\alpha,\beta)) \mu((\alpha,\beta)) \mu(\{\alpha\}) = 0 \mu([\alpha_i,\beta)) \to \mu((\alpha,\beta)) \alpha_i\to\alpha","['analysis', 'measure-theory']"
39,"extracting finite value from a non convergent integral, where am I wrong?","extracting finite value from a non convergent integral, where am I wrong?",,"This following integral is not convergent $$ \int_0^\infty dx \, x^{ia} e^{i\omega x} $$ but I know for example that $$ \int_0^\infty dx \, x^c e^{-b x} = b^{-1-c}\,  \Gamma(1+c) $$ where $\Gamma$ is the Euler Gamma function. Therefore calling $c=ia$ and $i\omega = -b$ one gets $$ \int_0^\infty dx \, x^{ia} e^{i\omega x} = (-i \omega)^{1-ia} \Gamma(1+ia) = e^{-i \pi/2} \omega^{1-ia} \, \Gamma(1+ia) $$ which looks finite to me, so can you tell me where I've made a mistake or an abuse?","This following integral is not convergent but I know for example that where is the Euler Gamma function. Therefore calling and one gets which looks finite to me, so can you tell me where I've made a mistake or an abuse?","
\int_0^\infty dx \, x^{ia} e^{i\omega x}
 
\int_0^\infty dx \, x^c e^{-b x} = b^{-1-c}\,  \Gamma(1+c)
 \Gamma c=ia i\omega = -b 
\int_0^\infty dx \, x^{ia} e^{i\omega x} = (-i \omega)^{1-ia} \Gamma(1+ia) = e^{-i \pi/2} \omega^{1-ia} \, \Gamma(1+ia)
","['calculus', 'integration', 'analysis', 'improper-integrals', 'gamma-function']"
40,uniform convergent,uniform convergent,,"Given f is a differentiable function. Define $$f_n(x)=n\left(f\left(x+\frac{1}{n}\right)-f(x)\right)$$ , prove that $f_n$ is uniformly converge to $f'$ I have tried to make these equations: $n(f(x+\frac{1}{n})-f(x))=\frac{f(x+\frac{1}{n})-f(x)}{1/n}$ and taking limits as n $\to\infty$ but i got stuck to prove its uniform convergent","Given f is a differentiable function. Define , prove that is uniformly converge to I have tried to make these equations: and taking limits as n but i got stuck to prove its uniform convergent",f_n(x)=n\left(f\left(x+\frac{1}{n}\right)-f(x)\right) f_n f' n(f(x+\frac{1}{n})-f(x))=\frac{f(x+\frac{1}{n})-f(x)}{1/n} \to\infty,"['real-analysis', 'analysis', 'functions']"
41,"Prove $\int\limits_0^{\frac{\pi}{2}}\sin(2n x)\,\cot x\,\mathrm{d}x- \int\limits_0^{\frac{\pi}{2}}\frac{\sin(2n x)}{x}\,\mathrm{d}x\to 0$",Prove,"\int\limits_0^{\frac{\pi}{2}}\sin(2n x)\,\cot x\,\mathrm{d}x- \int\limits_0^{\frac{\pi}{2}}\frac{\sin(2n x)}{x}\,\mathrm{d}x\to 0","Let $$a_n=\int\limits_0^{\frac{\pi}{2}}\sin(2n x)\,\cot x\,\mathrm{d}x ~~~\textrm{ and} ~~~   b_n=\int\limits_0^{\frac{\pi}{2}}\frac{\sin(2n x)}{x}\,\mathrm{d}x.$$ Prove that $a_n-b_n \to 0.$ Attempt. We have that $$a_n-b_n  = \int\limits_0^{\frac{\pi}{2}}\sin(2n x)\,\left(\cot x-\frac{1}{x}\right)\,\mathrm{d}x.$$ Integration by parts would give: $$a_n-b_n=\left[\log\left(\frac{\sin x}{x}\right)\sin(2nx)\right]_0^{\frac{\pi}{2}}-2n\int\limits_0^{\frac{\pi}{2}}\cos(2n x)\,\log\left(\frac{\sin x}{x}\right)\,\mathrm{d}x$$ $$=-2n\int\limits_0^{\frac{\pi}{2}}\cos(2n x)\,\log\left(\frac{\sin x}{x}\right)\,\mathrm{d}x$$ but this doesn't seem to go any further. On the other hand, from Integral $\int_0^\pi \cot(x/2)\sin(nx)\,dx$ we have $a_n=\frac{\pi}{2}$ , so: $$b_n-a_n=\int\limits_0^{\frac{\pi}{2}}\left(\frac{\sin(2n x)}{x}-1\right)\,\mathrm{d}x,$$ so: $$|b_n-a_n|\leqslant \int\limits_0^{\frac{\pi}{2}}\left|\frac{\sin(2n x)}{x}-1\right|\,\mathrm{d}x,$$ but I also didn't manage to get this any further. Thanks in advance for the help. Edit. Consequence of the above limit is the evaluation of the Dirichlet integral: $$\int\limits_0^{+\infty}\frac{\sin x}{x}\,\mathrm{d}x= \lim_{n \to +\infty}b_n=\lim_{n \to +\infty}a_n=\frac{\pi}{2}.$$","Let Prove that Attempt. We have that Integration by parts would give: but this doesn't seem to go any further. On the other hand, from Integral $\int_0^\pi \cot(x/2)\sin(nx)\,dx$ we have , so: so: but I also didn't manage to get this any further. Thanks in advance for the help. Edit. Consequence of the above limit is the evaluation of the Dirichlet integral:","a_n=\int\limits_0^{\frac{\pi}{2}}\sin(2n x)\,\cot x\,\mathrm{d}x ~~~\textrm{ and} ~~~   b_n=\int\limits_0^{\frac{\pi}{2}}\frac{\sin(2n x)}{x}\,\mathrm{d}x. a_n-b_n \to 0. a_n-b_n  = \int\limits_0^{\frac{\pi}{2}}\sin(2n x)\,\left(\cot x-\frac{1}{x}\right)\,\mathrm{d}x. a_n-b_n=\left[\log\left(\frac{\sin x}{x}\right)\sin(2nx)\right]_0^{\frac{\pi}{2}}-2n\int\limits_0^{\frac{\pi}{2}}\cos(2n x)\,\log\left(\frac{\sin x}{x}\right)\,\mathrm{d}x =-2n\int\limits_0^{\frac{\pi}{2}}\cos(2n x)\,\log\left(\frac{\sin x}{x}\right)\,\mathrm{d}x a_n=\frac{\pi}{2} b_n-a_n=\int\limits_0^{\frac{\pi}{2}}\left(\frac{\sin(2n x)}{x}-1\right)\,\mathrm{d}x, |b_n-a_n|\leqslant \int\limits_0^{\frac{\pi}{2}}\left|\frac{\sin(2n x)}{x}-1\right|\,\mathrm{d}x, \int\limits_0^{+\infty}\frac{\sin x}{x}\,\mathrm{d}x=
\lim_{n \to +\infty}b_n=\lim_{n \to +\infty}a_n=\frac{\pi}{2}.","['real-analysis', 'calculus', 'integration', 'analysis']"
42,$\sum_{n=1}^{\infty} 1/\sqrt[n]{n}$ converge,converge,\sum_{n=1}^{\infty} 1/\sqrt[n]{n},Does the series: $$ \sum_{n=1}^{\infty} \frac{1}{\sqrt[n]{n}} $$ converge or diverge? I'm unsure where to start with this question. I know that $n$ th root of $n$ converges to $1$ but not sure about its reciprocal.,Does the series: converge or diverge? I'm unsure where to start with this question. I know that th root of converges to but not sure about its reciprocal.,"
\sum_{n=1}^{\infty} \frac{1}{\sqrt[n]{n}}
 n n 1","['calculus', 'sequences-and-series', 'analysis', 'convergence-divergence']"
43,Polynomial vs power series vs formal power series?,Polynomial vs power series vs formal power series?,,"Wikipedia states: In mathematics, a formal power series is a generalization of a polynomial , where the number of terms is allowed to be infinite; this implies giving up the possibility of replacing the variable in the polynomial with an arbitrary number. Thus a formal power series differs from a polynomial in that it may have infinitely many terms, and differs from a power series , whose variables can take on numerical values. What I am getting from this is that in both polynomials and formal power series, the variables ""don't represent numbers"". But I'm not exactly sure what this means, or what they do represent. Also it seems to be inconsistent with how I've been using polynomials, which is very much as ""variables representing numbers"". So basically I'm conceptually confused about what this means, and can't really understand how they're being used.","Wikipedia states: In mathematics, a formal power series is a generalization of a polynomial , where the number of terms is allowed to be infinite; this implies giving up the possibility of replacing the variable in the polynomial with an arbitrary number. Thus a formal power series differs from a polynomial in that it may have infinitely many terms, and differs from a power series , whose variables can take on numerical values. What I am getting from this is that in both polynomials and formal power series, the variables ""don't represent numbers"". But I'm not exactly sure what this means, or what they do represent. Also it seems to be inconsistent with how I've been using polynomials, which is very much as ""variables representing numbers"". So basically I'm conceptually confused about what this means, and can't really understand how they're being used.",,"['analysis', 'polynomials', 'power-series', 'formal-power-series']"
44,Proof of the fact that the decreasing limit of continuous increasing functions is right continuous.,Proof of the fact that the decreasing limit of continuous increasing functions is right continuous.,,"As the title suggests, I'm looking for a proof of the following statement: Let $(f_n)_{n \in \mathbb{N}}$ be a sequence of continuous and increasing functions. If $f_n(x) \searrow f(x)$ , as $n \rightarrow +\infty$ , then $f(x)$ is right continuous. My attempt to prove it: Suppose that $f(x)$ is not right continuous; i.e., $\lim_{\delta \rightarrow 0^+} f(x + \delta) \neq f(x)$ . Then, $\exists \epsilon > 0$ , such that, $\forall \delta > 0$ , $|f(x + \delta) - f(x)| > \epsilon$ . It follows that: $$ \begin{align} \epsilon & < |f(x + \delta) - f(x) + f_n(x) - f_n(x) + f_n(x + \delta) - f_n(x + \delta)| \leq \\  &\leq |f(x + \delta) - f_n(x + \delta)| + |f_n(x) - f(x)| + |f_n(x + \delta) - f_n(x)|. \end{align} $$ The first two terms can be controlled by choosing a sufficiently large $n$ s.t. $|f(x + \delta) - f_n(x + \delta)|$ and $|f_n(x) - f(x)|$ are $< \frac{\epsilon}{3}$ , while the third term can be controlled by choosing a $\delta$ s.t. $|f_n(x + \delta) - f_n(x)| = \frac{\epsilon}{3}$ . Implying that $\epsilon < \frac{3 \epsilon}{3} = \epsilon$ , which is an absurdity, completing the proof. Since I'm not very convinced if I made something wrong, my question is: Is this a valid proof? If it is not, what should I do in order to correctly prove the described statement? EDIT: as pointed out by @TheoreticalEconomist, it is necessary an inequality (instead of a "" $=$ "", as it was written before) for controlling the first two terms. But even though, I'm not sure if the presented proof is correct: the fact that $f_n$ decreases to $f$ , as $n \rightarrow +\infty$ , is being used at the ""two terms"" part; however, the third term is still a problem $-$ somehow it should be justified by the $(f_n)$ characteristics. EDIT 2: I think it is correct because: 1) chosen $n = \max(n_0, n_1)$ , s.t. $|f(x + \delta) - f_n(x + \delta)|$ and $|f_n(x) - f(x)|$ are $< \frac{\epsilon}{3}$ , it is possible to choose a convenient $\delta$ s.t. $|f_n(x + \delta) - f_n(x)| = \frac{\epsilon}{3}$ (notice that, before that point, $\delta$ was not chose yet); and 2) the hyphothesis of $(f_n)$ being a sequence of increasing function (implying that $f$ is a increasing function as well) is indeed needed $-$ recall that $|f(x + \delta) - f(x)| > \epsilon$ has to be satisfied (otherwise, it would not be true). Which, if I'm not wrong, eliminates all potential problems. FINAL EDIT: for a proper proof, please refer to the Tom's answer.","As the title suggests, I'm looking for a proof of the following statement: Let be a sequence of continuous and increasing functions. If , as , then is right continuous. My attempt to prove it: Suppose that is not right continuous; i.e., . Then, , such that, , . It follows that: The first two terms can be controlled by choosing a sufficiently large s.t. and are , while the third term can be controlled by choosing a s.t. . Implying that , which is an absurdity, completing the proof. Since I'm not very convinced if I made something wrong, my question is: Is this a valid proof? If it is not, what should I do in order to correctly prove the described statement? EDIT: as pointed out by @TheoreticalEconomist, it is necessary an inequality (instead of a "" "", as it was written before) for controlling the first two terms. But even though, I'm not sure if the presented proof is correct: the fact that decreases to , as , is being used at the ""two terms"" part; however, the third term is still a problem somehow it should be justified by the characteristics. EDIT 2: I think it is correct because: 1) chosen , s.t. and are , it is possible to choose a convenient s.t. (notice that, before that point, was not chose yet); and 2) the hyphothesis of being a sequence of increasing function (implying that is a increasing function as well) is indeed needed recall that has to be satisfied (otherwise, it would not be true). Which, if I'm not wrong, eliminates all potential problems. FINAL EDIT: for a proper proof, please refer to the Tom's answer.","(f_n)_{n \in \mathbb{N}} f_n(x) \searrow f(x) n \rightarrow +\infty f(x) f(x) \lim_{\delta \rightarrow 0^+} f(x + \delta) \neq f(x) \exists \epsilon > 0 \forall \delta > 0 |f(x + \delta) - f(x)| > \epsilon 
\begin{align}
\epsilon & < |f(x + \delta) - f(x) + f_n(x) - f_n(x) + f_n(x + \delta) - f_n(x + \delta)| \leq \\ 
&\leq |f(x + \delta) - f_n(x + \delta)| + |f_n(x) - f(x)| + |f_n(x + \delta) - f_n(x)|.
\end{align}
 n |f(x + \delta) - f_n(x + \delta)| |f_n(x) - f(x)| < \frac{\epsilon}{3} \delta |f_n(x + \delta) - f_n(x)| = \frac{\epsilon}{3} \epsilon < \frac{3 \epsilon}{3} = \epsilon = f_n f n \rightarrow +\infty - (f_n) n = \max(n_0, n_1) |f(x + \delta) - f_n(x + \delta)| |f_n(x) - f(x)| < \frac{\epsilon}{3} \delta |f_n(x + \delta) - f_n(x)| = \frac{\epsilon}{3} \delta (f_n) f - |f(x + \delta) - f(x)| > \epsilon","['real-analysis', 'analysis']"
45,Prove that a total measure |$\mu$| is the smallest measure for the following,Prove that a total measure || is the smallest measure for the following,\mu,"Assume that we have a signed measure $\mu$ on a measurable space $(X,\Sigma)$ and a Hahn decomposition $(P,N)$ for $\mu$ , then we define $|\mu| = \mu^{+} + \mu^{-}$ where $\mu^{+}(B) = \mu(B \cap P)$ and $\mu^{-}(B) = \mu(B \cap N)$ for each $B \in \Sigma$ . I have managed to show that $|\mu(B)| \leq |\mu|(B)$ for each $B \in \Sigma$ , but would also like to show that $|\mu|$ is the smallest unsigned measure on $(X,\Sigma)$ that makes this possible. So if I pick an unsigned measure $m$ on $(X,\Sigma)$ such that $|\mu(B)| \leq m(B)$ and need to show that $|\mu|(B) \leq m(B)$ . Since $m(B \cup P) \subseteq m(B)$ and $|\mu|(B) = \mu(B \cap P) - \mu(B \cap N)$ , I guess I have to make use of this but I am not sure how to go further. Any help would be greatly appreciated.","Assume that we have a signed measure on a measurable space and a Hahn decomposition for , then we define where and for each . I have managed to show that for each , but would also like to show that is the smallest unsigned measure on that makes this possible. So if I pick an unsigned measure on such that and need to show that . Since and , I guess I have to make use of this but I am not sure how to go further. Any help would be greatly appreciated.","\mu (X,\Sigma) (P,N) \mu |\mu| = \mu^{+} + \mu^{-} \mu^{+}(B) = \mu(B \cap P) \mu^{-}(B) = \mu(B \cap N) B \in \Sigma |\mu(B)| \leq |\mu|(B) B \in \Sigma |\mu| (X,\Sigma) m (X,\Sigma) |\mu(B)| \leq m(B) |\mu|(B) \leq m(B) m(B \cup P) \subseteq m(B) |\mu|(B) = \mu(B \cap P) - \mu(B \cap N)","['real-analysis', 'analysis', 'measure-theory']"
46,How Does Ramanujan’s Master Theorem Work?,How Does Ramanujan’s Master Theorem Work?,,"I am trying to understand Ramanujan' Master theorem, but I really can't seem to wrap my head around it. So the theorem states that if you have a function $F$ such that you can write it as $$F(x)=\sum_{k=0}^\infty \frac {w(k)(-x)^k}{k!}$$ for some analytic or integrable function $w$ , then the Mellin transform of $F$ is $$I=\int_0^\infty x^{n-1}F(x)dx= \Gamma(n)w(-n).$$ However, there are some related parts that confuse me: The definition of $w$ by the series in the first expression just requires $w$ to take certain values for $k=0,1,\ldots,\infty$ , but not for any values in $\mathbb{R}\setminus\mathbb{N}_0$ . Is this really enough to uniquely define $w$ ? If so why? A typical example would be $w(k)=1$ for all $k$ . In that case, we get $F(x)=e^{-x}$ and $I=\Gamma(n)$ . However, if I instead use $w(k)=cos(2\pi k)$ , then I still get $F(x)=e^{-x}$ . However, for this $w$ I get $I(n)=\Gamma(n)cos(-2\pi n)$ , which agrees on $n\in \mathbb{Z}$ but not anywhere else. Obviously, these can't both be correct. Both $w$ are integrable, so why is one of the choices of $w$ incorrect? (this is likely related to the first) Why do we require the minus in the expression? If we define some function $\hat{w}$ such that $\hat{w}(k)=(-1)^k w(k)$ for $k\in\mathbb{N}_0$ , then writing the series in terms of $\hat{w}$ would give the more familiar Maclauren series. As an example take $w(k)=cos(2\pi k)$ . No if I choose $\hat{w}(k)=(cos(\pi k))^2$ (note the factor $2$ is missing), then $$F(x)=\sum_{k=0}^\infty \frac {\hat{w}(k)(x)^k}{k!}$$ is a Maclauren series for $e^{-x}$ . Why can't we get a similar result for the Maclauren series?","I am trying to understand Ramanujan' Master theorem, but I really can't seem to wrap my head around it. So the theorem states that if you have a function such that you can write it as for some analytic or integrable function , then the Mellin transform of is However, there are some related parts that confuse me: The definition of by the series in the first expression just requires to take certain values for , but not for any values in . Is this really enough to uniquely define ? If so why? A typical example would be for all . In that case, we get and . However, if I instead use , then I still get . However, for this I get , which agrees on but not anywhere else. Obviously, these can't both be correct. Both are integrable, so why is one of the choices of incorrect? (this is likely related to the first) Why do we require the minus in the expression? If we define some function such that for , then writing the series in terms of would give the more familiar Maclauren series. As an example take . No if I choose (note the factor is missing), then is a Maclauren series for . Why can't we get a similar result for the Maclauren series?","F F(x)=\sum_{k=0}^\infty \frac {w(k)(-x)^k}{k!} w F I=\int_0^\infty x^{n-1}F(x)dx= \Gamma(n)w(-n). w w k=0,1,\ldots,\infty \mathbb{R}\setminus\mathbb{N}_0 w w(k)=1 k F(x)=e^{-x} I=\Gamma(n) w(k)=cos(2\pi k) F(x)=e^{-x} w I(n)=\Gamma(n)cos(-2\pi n) n\in \mathbb{Z} w w \hat{w} \hat{w}(k)=(-1)^k w(k) k\in\mathbb{N}_0 \hat{w} w(k)=cos(2\pi k) \hat{w}(k)=(cos(\pi k))^2 2 F(x)=\sum_{k=0}^\infty \frac {\hat{w}(k)(x)^k}{k!} e^{-x}","['analysis', 'mellin-transform']"
47,Prove that $SU(n)$ is simply-connected (using Van Kampen),Prove that  is simply-connected (using Van Kampen),SU(n),"I have some problems to prove that $SU(n)$ is simply-connected, for $U(n) = \{ M \in GL_n(C) \; | \; ^{t}\overline{M}M = Id \;\}$ . In fact, there is some indications (to follow if it's possible). First, let's define $N=(0, \dots, 1)$ the North pole of $S^{2n-1} \subset \mathbb{C}$ . We suppose $n \geq 2$ . Then : Let $e_N : SU(n) \rightarrow \mathbb{S}^{2n-1}$ the application which evaluates the matrix at the $N$ , i.e : $e_N(A) = A \cdot N$ . Prove that it exists a continuous section : $s : \mathbb{S}^{2n-1} -{N} \rightarrow SU(n)$ of $s$ , i.e $e_N \circ s = Id$ . Then, show by induction that $SU(n)$ is simply connected, using Van Kampen. For the first point, so to find the section, let's $z \in \mathbb{S}^{2n-1} -N$ . Then, we want to say that $s(z)(N) = z$ and $s(z) = Id$ over $Vect(N,z)^{\perp}$ . Then, we have defined $s$ over $N$ and $Vect(N, z)$ , and as we should send an orthonormal basis on an orthonormal basis, and as $dim(Vect(N, z)^{\perp} + Vect(N))=2n-1$ , finally $s$ is totally determined. Then, I don't succeed to do the second part, i.e showing that $SU(n)$ is simply connected. Actually, let's suppose the result is true for $SU(n-1)$ . We can identify $SU(n-1)$ with $SU(Vect(N)^{\perp}) \subset SU(n)$ (by the ""natural"" inclusion). Then, I would like to consider : $S^{2n-1} - N \times SU(n-1)$ and $S^{2n-1} - S \times SU(n-1)$ (actually, the matrix associated, the first column would be the vector of $S^{2n-1} - N$ and the others columns would be the one of the element of $SU(n-1)$ ) because each factor are simply connected, but it's not a open cover of $SU(n)$ , as it's not even elements of $SU(n)$ . So, it should be here that we have to used the section. But I don't figure out how to do it. Someone could help me ? Thank you very much !","I have some problems to prove that is simply-connected, for . In fact, there is some indications (to follow if it's possible). First, let's define the North pole of . We suppose . Then : Let the application which evaluates the matrix at the , i.e : . Prove that it exists a continuous section : of , i.e . Then, show by induction that is simply connected, using Van Kampen. For the first point, so to find the section, let's . Then, we want to say that and over . Then, we have defined over and , and as we should send an orthonormal basis on an orthonormal basis, and as , finally is totally determined. Then, I don't succeed to do the second part, i.e showing that is simply connected. Actually, let's suppose the result is true for . We can identify with (by the ""natural"" inclusion). Then, I would like to consider : and (actually, the matrix associated, the first column would be the vector of and the others columns would be the one of the element of ) because each factor are simply connected, but it's not a open cover of , as it's not even elements of . So, it should be here that we have to used the section. But I don't figure out how to do it. Someone could help me ? Thank you very much !","SU(n) U(n) = \{ M \in GL_n(C) \; | \; ^{t}\overline{M}M = Id \;\} N=(0, \dots, 1) S^{2n-1} \subset \mathbb{C} n \geq 2 e_N : SU(n) \rightarrow \mathbb{S}^{2n-1} N e_N(A) = A \cdot N s : \mathbb{S}^{2n-1} -{N} \rightarrow SU(n) s e_N \circ s = Id SU(n) z \in \mathbb{S}^{2n-1} -N s(z)(N) = z s(z) = Id Vect(N,z)^{\perp} s N Vect(N, z) dim(Vect(N, z)^{\perp} + Vect(N))=2n-1 s SU(n) SU(n-1) SU(n-1) SU(Vect(N)^{\perp}) \subset SU(n) S^{2n-1} - N \times SU(n-1) S^{2n-1} - S \times SU(n-1) S^{2n-1} - N SU(n-1) SU(n) SU(n)","['analysis', 'algebraic-topology']"
48,If $f(a)=f(b)=0$ and $|f''(x)|\le M$ prove $|\int_a^bf(x)\mathrm{d}x| \le \frac{M}{12}(b-a)^3$,If  and  prove,f(a)=f(b)=0 |f''(x)|\le M |\int_a^bf(x)\mathrm{d}x| \le \frac{M}{12}(b-a)^3,"If $f(a)=f(b)=0$ and $|f''(x)|\le M$ . Prove $$|\int_a^bf(x)\mathrm{d}x| \le \frac{M}{12}(b-a)^3$$ I have thought about that since $f(a) = f(b) = 0 $ there is $\xi$ such that $f'(\xi) = 0$ . Then when $x \le \xi$ , $|f'(x)| \le (\xi - x)M$ and when $x \ge \xi$ , $|f'(x)| \le (x-\xi)M$ . After that $|f(x)| \le \frac{\xi^2 - (\xi -x)^2}{2}$ when $x \le \xi$ and $|f(x)| \le \frac{(b-\xi)^2 - (x - \xi)^2}{2}$ when $x \ge \xi$ . Therefore $$|\int_a^b f(x)\mathrm{d}x| \le \int_a^\xi |f(x)| + \int_\xi^b |f(x)| = \frac{\xi^3}{3} + \frac{(b-\xi)^3}{3}$$ If $x = \frac{a+b}{2}$ , we have $$\frac{x^3}{3}+\frac{(b-x)^3}{3} = \frac{(b-a)^3}{12}$$ But in this situation $\frac{x^3}{3}+\frac{(b-x)^3}{3}$ is the minimal value. So I can't go on.","If and . Prove I have thought about that since there is such that . Then when , and when , . After that when and when . Therefore If , we have But in this situation is the minimal value. So I can't go on.",f(a)=f(b)=0 |f''(x)|\le M |\int_a^bf(x)\mathrm{d}x| \le \frac{M}{12}(b-a)^3 f(a) = f(b) = 0  \xi f'(\xi) = 0 x \le \xi |f'(x)| \le (\xi - x)M x \ge \xi |f'(x)| \le (x-\xi)M |f(x)| \le \frac{\xi^2 - (\xi -x)^2}{2} x \le \xi |f(x)| \le \frac{(b-\xi)^2 - (x - \xi)^2}{2} x \ge \xi |\int_a^b f(x)\mathrm{d}x| \le \int_a^\xi |f(x)| + \int_\xi^b |f(x)| = \frac{\xi^3}{3} + \frac{(b-\xi)^3}{3} x = \frac{a+b}{2} \frac{x^3}{3}+\frac{(b-x)^3}{3} = \frac{(b-a)^3}{12} \frac{x^3}{3}+\frac{(b-x)^3}{3},"['analysis', 'inequality', 'integral-inequality']"
49,Boundedness requirement Theorem 7.29 Baby Rudin,Boundedness requirement Theorem 7.29 Baby Rudin,,"7.29   Theorem - Let $\mathscr B $ be the uniform closure of an algebra $\mathscr A$ of bounded functions. Then $\mathscr B$ is a uniformly closed algebra. An algebra $\mathscr A$ is a family of complex functions defined on a set $E$ which is closed under addition of two functions, multiplication of two functions, and multiplication of a function by a constant. Definition of uniform closure: Let $\mathscr B $ be the set of all functions which are limits of uniformly convergent sequences of members of an algebra $\mathscr A$ . Then $\mathscr B $ is called the uniform closure of $\mathscr A$ . Definition of uniform closed: If $\mathscr A$ has the property that $f \in \mathscr A$ whenever $f_n  \in \mathscr A (n = 1,2,3,...)$ and $f_n \to f$ uniformly on $E$ , then $\mathscr A$ is said to be uniformly closed . My question is: why do we require the functions to be bounded in theorem 7.29? For example, in proving $$f_n +g_n \to f + n$$ whenever $f_n \to f, g_n \to g $ uniformly, we can just appeal to theorem 3.3a (which states $\lim_{n \to \infty} (s_n + t_n) = s + t$ whenever $\lim_{n \to \infty} s_n = s$ , $\lim_{n \to \infty} t_n = t$ ) to see that $f_n +g_n \to f + n$ uniformly. I don't see how boundedness is required here, nor anywhere else in proving the theorem. So are the functions in $\mathscr A $ required to be bounded?","7.29   Theorem - Let be the uniform closure of an algebra of bounded functions. Then is a uniformly closed algebra. An algebra is a family of complex functions defined on a set which is closed under addition of two functions, multiplication of two functions, and multiplication of a function by a constant. Definition of uniform closure: Let be the set of all functions which are limits of uniformly convergent sequences of members of an algebra . Then is called the uniform closure of . Definition of uniform closed: If has the property that whenever and uniformly on , then is said to be uniformly closed . My question is: why do we require the functions to be bounded in theorem 7.29? For example, in proving whenever uniformly, we can just appeal to theorem 3.3a (which states whenever , ) to see that uniformly. I don't see how boundedness is required here, nor anywhere else in proving the theorem. So are the functions in required to be bounded?","\mathscr B  \mathscr A \mathscr B \mathscr A E \mathscr B  \mathscr A \mathscr B  \mathscr A \mathscr A f \in \mathscr A f_n  \in \mathscr A (n = 1,2,3,...) f_n \to f E \mathscr A f_n +g_n \to f + n f_n \to f, g_n \to g  \lim_{n \to \infty} (s_n + t_n) = s + t \lim_{n \to \infty} s_n = s \lim_{n \to \infty} t_n = t f_n +g_n \to f + n \mathscr A ","['general-topology', 'analysis']"
50,Formal way of writing the Binomial Expansion for Derivatives,Formal way of writing the Binomial Expansion for Derivatives,,"I'm currently in an Advanced Calculus class where we are currently talking about derivatives of functions of several variables, and I came across the following question: Find the tenth differential, $d^{10}f$ , of $f(x,y)=\ln(x+y)$ . After finding a few of the early differentials, I noticed that they followed a similar pattern of the Binomial Theorem for polynomials, e.g., $$d^4f=\frac{-(3!)}{(x+y)^4}(dx^4+4\,dx^3dy+6\,dx^2dy^2+4dx\,dy^3+dy^4),\tag{$\star$}$$ where everything in the parentheses is the fourth row of Pascal's Triangle. So my question is: $\textbf{Is there a formal shorthand for writing such differentials?}$ Am I okay to write my fourth differential as I did in $(\star)$ ? Could I write an arbitrary $n$ th differential as $\frac{(-1)^{n+1}(n-1)!}{(x+y)^n} \sum\limits_{k=0}^n\!{n \choose k}dx^{n-k}\,dy^{k}$ ? (E.g., $(\star)$ could be written $\frac{(-1)^{5}(3)!}{(x+y)^4} \sum\limits_{k=0}^4\!{4 \choose k}dx^{4-k}\,dy^{k}$ ) Or are both just informal nonsense? Note: [I am assuming that $dx\,dy=dy\,dx$ since $x+y=0$ is excluded in the domain of $f$ , which means all partial derivatives will be continuous on the same domain.]","I'm currently in an Advanced Calculus class where we are currently talking about derivatives of functions of several variables, and I came across the following question: Find the tenth differential, , of . After finding a few of the early differentials, I noticed that they followed a similar pattern of the Binomial Theorem for polynomials, e.g., where everything in the parentheses is the fourth row of Pascal's Triangle. So my question is: Am I okay to write my fourth differential as I did in ? Could I write an arbitrary th differential as ? (E.g., could be written ) Or are both just informal nonsense? Note: [I am assuming that since is excluded in the domain of , which means all partial derivatives will be continuous on the same domain.]","d^{10}f f(x,y)=\ln(x+y) d^4f=\frac{-(3!)}{(x+y)^4}(dx^4+4\,dx^3dy+6\,dx^2dy^2+4dx\,dy^3+dy^4),\tag{\star} \textbf{Is there a formal shorthand for writing such differentials?} (\star) n \frac{(-1)^{n+1}(n-1)!}{(x+y)^n} \sum\limits_{k=0}^n\!{n \choose k}dx^{n-k}\,dy^{k} (\star) \frac{(-1)^{5}(3)!}{(x+y)^4} \sum\limits_{k=0}^4\!{4 \choose k}dx^{4-k}\,dy^{k} dx\,dy=dy\,dx x+y=0 f","['calculus', 'combinatorics', 'analysis', 'derivatives', 'partial-derivative']"
51,"Prove that $\sup S=\sup R,$ if $S\subseteq\Bbb{R}$ is bounded from above and $R\subseteq S$.",Prove that  if  is bounded from above and .,"\sup S=\sup R, S\subseteq\Bbb{R} R\subseteq S","Please, is this correct? Let $S$ be a subset of the real numbers bounded from above and let $R\subseteq S$ satisfy the following condition: $\forall\;x\in S,\;\exists\; r\in R\;\text{such that}\;x\leq r$ . I want to prove that \begin{align} \sup S=\sup R.\end{align} PROOF Let $x\in S$ be arbitrary, then $\exists\; r\in R\;\text{such that}\;x\leq r.$ The number, $r$ , is an upper bound for the set $S.$ So, $\sup S\leq r,\;\text{for some} \; r\in R.$ By definition of $\sup,\;\;y\leq\sup R, \forall\; y\in R.$ In particular, $y=r,\;r\leq\sup R.$ Thus, \begin{align}\tag{1} \sup S\leq r\leq\sup R.\end{align} Since $R\subseteq S$ , we have \begin{align}\tag{2} \sup R\leq \sup S.\end{align} Thus, \begin{align} \sup S=\sup R.\end{align}","Please, is this correct? Let be a subset of the real numbers bounded from above and let satisfy the following condition: . I want to prove that PROOF Let be arbitrary, then The number, , is an upper bound for the set So, By definition of In particular, Thus, Since , we have Thus,","S R\subseteq S \forall\;x\in S,\;\exists\; r\in R\;\text{such that}\;x\leq r \begin{align} \sup S=\sup R.\end{align} x\in S \exists\; r\in R\;\text{such that}\;x\leq r. r S. \sup S\leq r,\;\text{for some} \; r\in R. \sup,\;\;y\leq\sup R, \forall\; y\in R. y=r,\;r\leq\sup R. \begin{align}\tag{1} \sup S\leq r\leq\sup R.\end{align} R\subseteq S \begin{align}\tag{2} \sup R\leq \sup S.\end{align} \begin{align} \sup S=\sup R.\end{align}","['real-analysis', 'analysis']"
52,What's the geometric explanation of Lie Derivative and Covariant Derivative?,What's the geometric explanation of Lie Derivative and Covariant Derivative?,,"What's the geometric explanation of Lie Derivative and Covariant Derivative? For my understanding, covariant derivative somehow explain/ provided a way for determine the invariance or for coordinate transformation. While Lie Derivative somehow showed the changes caused by the metric. But it's still a little bit fuzzy to see how to explain them in a more intuitive way. Notation Check: I was reading Hamilton's Ricci Flow by AMS GSM series Volume 77, where on page 11 it stated that $L_X f=Xf $ . Just to check that this was a typo and $L_X f = X \bigtriangledown f$ , right?","What's the geometric explanation of Lie Derivative and Covariant Derivative? For my understanding, covariant derivative somehow explain/ provided a way for determine the invariance or for coordinate transformation. While Lie Derivative somehow showed the changes caused by the metric. But it's still a little bit fuzzy to see how to explain them in a more intuitive way. Notation Check: I was reading Hamilton's Ricci Flow by AMS GSM series Volume 77, where on page 11 it stated that . Just to check that this was a typo and , right?",L_X f=Xf  L_X f = X \bigtriangledown f,"['analysis', 'differential-geometry', 'tensors']"
53,"Find $p>1$ that ${\int\limits^p_1}\frac{1}{x}\,\mathrm{d}x={\int\limits^p_1}\ln\left(x\right)\,\mathrm{d}x$",Find  that,"p>1 {\int\limits^p_1}\frac{1}{x}\,\mathrm{d}x={\int\limits^p_1}\ln\left(x\right)\,\mathrm{d}x","Find $p>1$ that $${\displaystyle\int\limits^p_1}\dfrac{1}{x}\,\mathrm{d}x={\displaystyle\int\limits^p_1}\ln\left(x\right)\,\mathrm{d}x$$ \begin{align*}     &{\displaystyle\int}\dfrac{1}{x}\,\mathrm{d}x=\ln\left(\mid x \mid \right) && \vert \ \text{general integral} \end{align*} $F_1(x)=\ln\left({\mid x \mid} \right)+C$ \begin{align*}     &{\displaystyle\int}1\cdot\ln\left(x\right )\,\mathrm{d}x && \vert \ 2. \text{ with } f'=1, g=\ln(x)\\     &=x\ln\left(x\right)-{\displaystyle\int}1\,\mathrm{d}x\\     &=x\ln\left(x\right)-x \end{align*} $F_2(x)=x\ln\left(x\right)-x+C$ \begin{align*}     \left[\ln\left(\mid{x}\mid\right)+C\right]^p_1&=\left[\ln({\mid p \mid})+C\right]-\left[\ln({\mid 1 \mid})+C\right]\\     &=\ln\left(p\right) \end{align*} \begin{align*}     &\left[x\ln\left(x \right)-x+C\right]^p_1=\left[p\ln\left(p \right)-p+C\right]-\left[1\ln\left(1\right)-1+C\right] \end{align*} It was already mentioned, that I had a typo. I corrected everything and contributed my own solution.","Find that It was already mentioned, that I had a typo. I corrected everything and contributed my own solution.","p>1 {\displaystyle\int\limits^p_1}\dfrac{1}{x}\,\mathrm{d}x={\displaystyle\int\limits^p_1}\ln\left(x\right)\,\mathrm{d}x \begin{align*}
    &{\displaystyle\int}\dfrac{1}{x}\,\mathrm{d}x=\ln\left(\mid x \mid \right) && \vert \ \text{general integral}
\end{align*} F_1(x)=\ln\left({\mid x \mid} \right)+C \begin{align*}
    &{\displaystyle\int}1\cdot\ln\left(x\right )\,\mathrm{d}x && \vert \ 2. \text{ with } f'=1, g=\ln(x)\\
    &=x\ln\left(x\right)-{\displaystyle\int}1\,\mathrm{d}x\\
    &=x\ln\left(x\right)-x
\end{align*} F_2(x)=x\ln\left(x\right)-x+C \begin{align*}
    \left[\ln\left(\mid{x}\mid\right)+C\right]^p_1&=\left[\ln({\mid p \mid})+C\right]-\left[\ln({\mid 1 \mid})+C\right]\\
    &=\ln\left(p\right)
\end{align*} \begin{align*}
    &\left[x\ln\left(x \right)-x+C\right]^p_1=\left[p\ln\left(p \right)-p+C\right]-\left[1\ln\left(1\right)-1+C\right]
\end{align*}","['calculus', 'analysis', 'definite-integrals']"
54,"About homeomorphisms on $[0,1]$",About homeomorphisms on,"[0,1]","I need some help with the following: Suppose that $T:[0,1] \rightarrow [0,1]$ is an homemorphism, which satisfies that $T(0)=0$ . It is really intuitive that $$\int_0^1 |T(y) - y| dy = \int_0^1 |T^{-1}(y) - y| dy$$ since $T$ and $T^{-1}$ are symmetric with respect to the identity, and so the area between $T$ and identity will be the same as $T^{-1}$ and identity, which is the equality posted above. For me, it's intuitive but I can't get a proof of that. Also, what happend if we change the absolute value by square (change the $L_1$ norm for $L_2$ norm). Thanks a lot!","I need some help with the following: Suppose that is an homemorphism, which satisfies that . It is really intuitive that since and are symmetric with respect to the identity, and so the area between and identity will be the same as and identity, which is the equality posted above. For me, it's intuitive but I can't get a proof of that. Also, what happend if we change the absolute value by square (change the norm for norm). Thanks a lot!","T:[0,1] \rightarrow [0,1] T(0)=0 \int_0^1 |T(y) - y| dy = \int_0^1 |T^{-1}(y) - y| dy T T^{-1} T T^{-1} L_1 L_2","['calculus', 'real-analysis', 'analysis', 'measure-theory', 'lebesgue-integral']"
55,How to make it formally correct?,How to make it formally correct?,,"Can someone help me formalizing this statement: $$ z= x^0 +ix^1 $$ And therefore $$ \frac{\partial}{\partial z} = \frac{\partial}{\partial (x^0 +ix^1)} = \frac{\partial}{\partial x^0} + \frac{1}{i} \frac{\partial}{\partial x^1} $$ My problem is with the last equality, I see it's right, but I'm not sure I'm allowed to do it in that way. Is there a way to procede more formally?","Can someone help me formalizing this statement: And therefore My problem is with the last equality, I see it's right, but I'm not sure I'm allowed to do it in that way. Is there a way to procede more formally?","
z= x^0 +ix^1
 
\frac{\partial}{\partial z} = \frac{\partial}{\partial (x^0 +ix^1)} = \frac{\partial}{\partial x^0} + \frac{1}{i} \frac{\partial}{\partial x^1}
","['calculus', 'analysis', 'multivariable-calculus', 'partial-derivative']"
56,Riemann Sums of Improper Integrals on Unbounded Domain,Riemann Sums of Improper Integrals on Unbounded Domain,,"Assume that $f:[0,\infty)\rightarrow[0,\infty)$ is decreasing (nonincreasing), continuous, and in $L^{1}[0,\infty)$ , it is not hard to see that the following holds: \begin{align*} \lim_{n\rightarrow\infty}\dfrac{1}{n}\sum_{i=1}^{\infty}f\left(\dfrac{i}{n}\right)=\int_{0}^{\infty}f(t)dt. \end{align*} The problem that I have is that, if we let $\{0=x_{0}^{N}<x_{1}^{N}<\cdots\}$ be a partition of $[0,\infty)$ such that $x_{i}^{N}-x_{i-1}^{N}<1/N$ , $i=1,2,...$ , here $N=1,2,...$ is fixed and we consider the (infinite) Riemann sum $S_{N}$ at the stage $N$ of the form \begin{align*} S_{N}=\sum_{i=1}^{\infty}f(x_{i}^{N})(x_{i}^{N}-x_{i-1}^{N}), \end{align*} must $S_{N}\rightarrow\displaystyle\int_{0}^{\infty}f(t)dt$ as $N\rightarrow\infty$ ? Obviously $S_{N}\leq\displaystyle\int_{0}^{\infty}f(t)dt$ , but how to proceed further then? For the special case that all $x_{i}^{N}$ are equal distributed with length $1/n$ , we can easily get the lower bound as $\displaystyle\int_{1/n}^{\infty}f(t)dt$ and then Squeeze Theorem just finishes the job. But now I have no idea how to estimate if $x_{i}^{N}$ are not necessarily equal distributed. Further thoughts: Writing that \begin{align*} S_{N}(x)=\sum_{i=1}^{\infty}f(x_{i}^{N})\chi_{[x_{i-1}^{N},x_{i}^{N})}(x), \end{align*} we have $S_{N}(x)\leq f(x)$ and it seems that $S_{N}(x)\rightarrow f(x)$ because of the continuity, if so, then we are ready to conclude. Edit: The continuity implies the result. Now the question is, what if continuity is not given?","Assume that is decreasing (nonincreasing), continuous, and in , it is not hard to see that the following holds: The problem that I have is that, if we let be a partition of such that , , here is fixed and we consider the (infinite) Riemann sum at the stage of the form must as ? Obviously , but how to proceed further then? For the special case that all are equal distributed with length , we can easily get the lower bound as and then Squeeze Theorem just finishes the job. But now I have no idea how to estimate if are not necessarily equal distributed. Further thoughts: Writing that we have and it seems that because of the continuity, if so, then we are ready to conclude. Edit: The continuity implies the result. Now the question is, what if continuity is not given?","f:[0,\infty)\rightarrow[0,\infty) L^{1}[0,\infty) \begin{align*}
\lim_{n\rightarrow\infty}\dfrac{1}{n}\sum_{i=1}^{\infty}f\left(\dfrac{i}{n}\right)=\int_{0}^{\infty}f(t)dt.
\end{align*} \{0=x_{0}^{N}<x_{1}^{N}<\cdots\} [0,\infty) x_{i}^{N}-x_{i-1}^{N}<1/N i=1,2,... N=1,2,... S_{N} N \begin{align*}
S_{N}=\sum_{i=1}^{\infty}f(x_{i}^{N})(x_{i}^{N}-x_{i-1}^{N}),
\end{align*} S_{N}\rightarrow\displaystyle\int_{0}^{\infty}f(t)dt N\rightarrow\infty S_{N}\leq\displaystyle\int_{0}^{\infty}f(t)dt x_{i}^{N} 1/n \displaystyle\int_{1/n}^{\infty}f(t)dt x_{i}^{N} \begin{align*}
S_{N}(x)=\sum_{i=1}^{\infty}f(x_{i}^{N})\chi_{[x_{i-1}^{N},x_{i}^{N})}(x),
\end{align*} S_{N}(x)\leq f(x) S_{N}(x)\rightarrow f(x)","['real-analysis', 'analysis', 'measure-theory']"
57,Integral of Hermite Polynomials.,Integral of Hermite Polynomials.,,"Let $f \in L^2(\mathbb{R}, (1/\sqrt{2 \pi})\exp(-x^2/2))$ be such that $0 \leq f(x) \leq 1$ . We know that the (normalized) Hermite polynomials are a complete orthonormal basis for this space.  Therefore we let \begin{equation} f_n(x) = \sum_{i=0}^n \langle f, H_i\rangle\ H_i(x) = \sum_{i=0}^n c_i H_i(x), \end{equation} where \begin{equation} c_i = \langle f, H_i \rangle = \int H_i(x) f(x) \frac{1}{\sqrt{2 \pi}} \exp(-x^2/2)\ \mathrm{d} x =  \int H_i(x) f(x)\ \phi(x) \mathrm{d} x, \end{equation} where $\phi(x) = (1/\sqrt{2 \pi}) \exp(-x^2/2)$ . Consider now the integral \begin{equation} d_i = \int H_i(x) f(x)\ \phi(x-\mu) \mathrm{d} x \end{equation} $d_i$ is the Hermite coefficient with respect to a translated Gaussian. Question:   Is there an upper bound for $|c_i - d_i|$ ? Ideally it would be something in    the form $|c_i - d_i| \leq A c_i$ . My guess would be that  there should exist some fixed constant $A$ such that \begin{equation} |c_i - d_i| \leq A |c_i| \exp(\mu^2). \end{equation} I know that this question could be formulated only in terms of integrals but I thought that providing more context could be helpful. Any pointers to relevant literature are welcome.",Let be such that . We know that the (normalized) Hermite polynomials are a complete orthonormal basis for this space.  Therefore we let where where . Consider now the integral is the Hermite coefficient with respect to a translated Gaussian. Question:   Is there an upper bound for ? Ideally it would be something in    the form . My guess would be that  there should exist some fixed constant such that I know that this question could be formulated only in terms of integrals but I thought that providing more context could be helpful. Any pointers to relevant literature are welcome.,"f \in L^2(\mathbb{R}, (1/\sqrt{2 \pi})\exp(-x^2/2)) 0 \leq f(x) \leq 1 \begin{equation}
f_n(x) = \sum_{i=0}^n \langle f, H_i\rangle\ H_i(x) = \sum_{i=0}^n c_i H_i(x),
\end{equation} \begin{equation}
c_i = \langle f, H_i \rangle = \int H_i(x) f(x) \frac{1}{\sqrt{2 \pi}} \exp(-x^2/2)\ \mathrm{d} x = 
\int H_i(x) f(x)\ \phi(x) \mathrm{d} x,
\end{equation} \phi(x) = (1/\sqrt{2 \pi}) \exp(-x^2/2) \begin{equation}
d_i = \int H_i(x) f(x)\ \phi(x-\mu) \mathrm{d} x
\end{equation} d_i |c_i - d_i| |c_i - d_i| \leq A c_i A \begin{equation}
|c_i - d_i| \leq A |c_i| \exp(\mu^2).
\end{equation}","['real-analysis', 'analysis', 'reference-request', 'fourier-analysis', 'hermite-polynomials']"
58,"For what values of $x$ in $(-3,17)$ does the series $\sum\limits^{\infty}_{n=1}\frac{(-1)^n x^n}{n[\log (n+1)]^2}$ converge?",For what values of  in  does the series  converge?,"x (-3,17) \sum\limits^{\infty}_{n=1}\frac{(-1)^n x^n}{n[\log (n+1)]^2}","For what values of $x$ in the following series, does the series converge? \begin{align}\sum^{\infty}_{n=1}\dfrac{(-1)^n x^n}{n[\log (n+1)]^2},\;\;-3<x<17 \end{align} MY TRIAL \begin{align}\lim\limits_{n\to \infty}\left|\dfrac{(-1)^{n+1} x^{n+1}}{(n+1)[\log (n+2)]^2}\cdot\dfrac{n[\log (n+1)]^2}{(-1)^n x^n}\right|&=|x|\lim\limits_{n\to \infty}\left|\dfrac{n}{n+1}\cdot\left[\dfrac{\log (n+1)}{\log (n+2)}\right]^2\right|\\&=|x|\lim\limits_{n\to \infty}\left(\dfrac{n}{n+1}\right)\cdot\lim\limits_{n\to \infty}\left[\dfrac{\log (n+1)}{\log (n+2)}\right]^2\\&=|x|\lim\limits_{n\to \infty}\left(\dfrac{n}{n+1}\right)\cdot\left[\lim\limits_{n\to \infty}\dfrac{\log (n+1)}{\log (n+2)}\right]^2\\&=|x|\left[\lim\limits_{n\to \infty}\dfrac{1}{n+1}\cdot n+2\right]^2\\&=|x|\end{align} Hence, the series converges absolutely for $|x|<1$ and diverges when $|x|>1$. When $x=1,$ \begin{align}\sum^{\infty}_{n=1}\dfrac{(-1)^n }{n[\log (n+1)]^2}<\infty\;\;\text{By Alternating series test}\end{align} When $x=-1,$ \begin{align}\sum^{\infty}_{n=1}\dfrac{1}{n[\log (n+1)]^2}<\infty\;\;\text{By Direct comparison test}\end{align} Hence, the values of $x$ for which the series converges, is $-1\leq x\leq 1.$ I'm I right? Constructive criticisms will be highly welcome! Thanks!","For what values of $x$ in the following series, does the series converge? \begin{align}\sum^{\infty}_{n=1}\dfrac{(-1)^n x^n}{n[\log (n+1)]^2},\;\;-3<x<17 \end{align} MY TRIAL \begin{align}\lim\limits_{n\to \infty}\left|\dfrac{(-1)^{n+1} x^{n+1}}{(n+1)[\log (n+2)]^2}\cdot\dfrac{n[\log (n+1)]^2}{(-1)^n x^n}\right|&=|x|\lim\limits_{n\to \infty}\left|\dfrac{n}{n+1}\cdot\left[\dfrac{\log (n+1)}{\log (n+2)}\right]^2\right|\\&=|x|\lim\limits_{n\to \infty}\left(\dfrac{n}{n+1}\right)\cdot\lim\limits_{n\to \infty}\left[\dfrac{\log (n+1)}{\log (n+2)}\right]^2\\&=|x|\lim\limits_{n\to \infty}\left(\dfrac{n}{n+1}\right)\cdot\left[\lim\limits_{n\to \infty}\dfrac{\log (n+1)}{\log (n+2)}\right]^2\\&=|x|\left[\lim\limits_{n\to \infty}\dfrac{1}{n+1}\cdot n+2\right]^2\\&=|x|\end{align} Hence, the series converges absolutely for $|x|<1$ and diverges when $|x|>1$. When $x=1,$ \begin{align}\sum^{\infty}_{n=1}\dfrac{(-1)^n }{n[\log (n+1)]^2}<\infty\;\;\text{By Alternating series test}\end{align} When $x=-1,$ \begin{align}\sum^{\infty}_{n=1}\dfrac{1}{n[\log (n+1)]^2}<\infty\;\;\text{By Direct comparison test}\end{align} Hence, the values of $x$ for which the series converges, is $-1\leq x\leq 1.$ I'm I right? Constructive criticisms will be highly welcome! Thanks!",,"['real-analysis', 'sequences-and-series', 'analysis', 'convergence-divergence', 'power-series']"
59,Implicit function theorem problem from Munkres Analysis on Manifolds text,Implicit function theorem problem from Munkres Analysis on Manifolds text,,"For the problem below, which is from Munkres' Analysis on Manifolds section on the implicit function theorem, I'm getting the following solution $\frac{\partial u}{\partial y} \neq \frac{18}{5}$, but I saw a solution giving the answer as $\frac{\partial u}{\partial y} \neq \frac{2}{5}$ My attempt: I began by checking the conditions for both $G$, and $H$, where $det\big(\frac{\partial G(2,-1,1)}{\partial y} \big) \neq 0$ and the same for $H$ (1) $\det \big(\frac{\partial H(2,-1,1)}{\partial y} \big)=2\frac{\partial u}{\partial y}+9+3\frac{\partial u}{\partial y}=0$ (2) $\det \big(\frac{\partial G(2,-1,1)}{\partial y} \big)=f'(x,y)\frac{\partial u}{\partial y}+2u\frac{\partial u}{\partial y}=0$ and substituting (1) into (2) gives $f'(x,y)\frac{\partial u}{\partial y}=\frac{18}{5}$ My thinking is that I was supposed to do something more with $f'(x,y)$ so that I could isolate $\frac{\partial u}{\partial y}$? Thanks","For the problem below, which is from Munkres' Analysis on Manifolds section on the implicit function theorem, I'm getting the following solution $\frac{\partial u}{\partial y} \neq \frac{18}{5}$, but I saw a solution giving the answer as $\frac{\partial u}{\partial y} \neq \frac{2}{5}$ My attempt: I began by checking the conditions for both $G$, and $H$, where $det\big(\frac{\partial G(2,-1,1)}{\partial y} \big) \neq 0$ and the same for $H$ (1) $\det \big(\frac{\partial H(2,-1,1)}{\partial y} \big)=2\frac{\partial u}{\partial y}+9+3\frac{\partial u}{\partial y}=0$ (2) $\det \big(\frac{\partial G(2,-1,1)}{\partial y} \big)=f'(x,y)\frac{\partial u}{\partial y}+2u\frac{\partial u}{\partial y}=0$ and substituting (1) into (2) gives $f'(x,y)\frac{\partial u}{\partial y}=\frac{18}{5}$ My thinking is that I was supposed to do something more with $f'(x,y)$ so that I could isolate $\frac{\partial u}{\partial y}$? Thanks",,"['real-analysis', 'analysis', 'multivariable-calculus', 'proof-writing']"
60,Proof Verification: Finding A Ball Strictly Contained In An Open Set Of A Metric Space,Proof Verification: Finding A Ball Strictly Contained In An Open Set Of A Metric Space,,"Problem: Let $X$ be a metric space and let $A$ be an open set of $X$ containing a point $x \in X$. Prove that there exists an $\epsilon > 0$ such that $B_{\epsilon}(x)$ is strictly contained in $A$. Proof Attempt: Case 1: $\partial A = \emptyset$ Since $X$ is a metric space, this implies that $A$ is clopen. The only clopen sets of a metric space are $\emptyset$ and the entire space. $A$ contains $x$, so it cannot be empty and thus $A = X$, so any $\epsilon > 0$ will suffice. Case 2: $\partial A \neq \emptyset$ Let $\displaystyle \epsilon = \frac{1}{2}\inf_{p \in \partial A}{d(p,x)}$, where $d$ is the metric of $X$. Note that $\epsilon \neq 0$ or else this would imply that $x \in \partial A$, which contradicts the hypothesis that $A$ contains $x$ and that $A$ is an open set. So $\epsilon > 0$. Then $B_{\epsilon}(x)$ is strictly contained in $A$ (I'm not sure how to justify this part). $\blacksquare$ Is this proof correct? How do I finish the proof? Thanks.","Problem: Let $X$ be a metric space and let $A$ be an open set of $X$ containing a point $x \in X$. Prove that there exists an $\epsilon > 0$ such that $B_{\epsilon}(x)$ is strictly contained in $A$. Proof Attempt: Case 1: $\partial A = \emptyset$ Since $X$ is a metric space, this implies that $A$ is clopen. The only clopen sets of a metric space are $\emptyset$ and the entire space. $A$ contains $x$, so it cannot be empty and thus $A = X$, so any $\epsilon > 0$ will suffice. Case 2: $\partial A \neq \emptyset$ Let $\displaystyle \epsilon = \frac{1}{2}\inf_{p \in \partial A}{d(p,x)}$, where $d$ is the metric of $X$. Note that $\epsilon \neq 0$ or else this would imply that $x \in \partial A$, which contradicts the hypothesis that $A$ contains $x$ and that $A$ is an open set. So $\epsilon > 0$. Then $B_{\epsilon}(x)$ is strictly contained in $A$ (I'm not sure how to justify this part). $\blacksquare$ Is this proof correct? How do I finish the proof? Thanks.",,"['general-topology', 'analysis', 'metric-spaces']"
61,Proving a function isn't injective by considering inverse,Proving a function isn't injective by considering inverse,,"I'm proving that a $C^r$ function $f:\mathbb{R^2}\rightarrow \mathbb{R}$ is not injective.  I've found a function $g:\mathbb{R^2}\rightarrow \mathbb{R^2}$ defined as $g(x,y)=\bigg(f(x,y),y\bigg)$, and proven that it's $C^r$.  To prove my main problem, I'm assuming $f$ is injective, and that allows me to apply the inverse function theorem to $g$, in such a way that gives me an open map from an open set in the domain of $g$ to the open image of $g$, such that $g$ has a differentiable inverse $g^{-1}:g(A)\rightarrow A$. Now I want to prove a contradiction in the injectivity of $f$, by showing that $g^{-1}$ is not defined at certain points, specifically a line.  Consider $f(x,y)=b$ then $g(x,y)=(b,y)$, Now I should be able to find that the line that $g^{-1}$ isn't defined on $\forall (b,z)$, $z\neq y$, since that would imply that   $f(x,z)=b$. I don't quite see how $g^{-1}$ being defined on $(b,z)$ implies that $f(x,z)=b$? Edit Here is the original problem.  It is problem $2$-$37$.  I've seen other solutions suggest to make $g$ satisfy the conditions of $f$ from $2$-$36$, which is why I left it in. Thanks!","I'm proving that a $C^r$ function $f:\mathbb{R^2}\rightarrow \mathbb{R}$ is not injective.  I've found a function $g:\mathbb{R^2}\rightarrow \mathbb{R^2}$ defined as $g(x,y)=\bigg(f(x,y),y\bigg)$, and proven that it's $C^r$.  To prove my main problem, I'm assuming $f$ is injective, and that allows me to apply the inverse function theorem to $g$, in such a way that gives me an open map from an open set in the domain of $g$ to the open image of $g$, such that $g$ has a differentiable inverse $g^{-1}:g(A)\rightarrow A$. Now I want to prove a contradiction in the injectivity of $f$, by showing that $g^{-1}$ is not defined at certain points, specifically a line.  Consider $f(x,y)=b$ then $g(x,y)=(b,y)$, Now I should be able to find that the line that $g^{-1}$ isn't defined on $\forall (b,z)$, $z\neq y$, since that would imply that   $f(x,z)=b$. I don't quite see how $g^{-1}$ being defined on $(b,z)$ implies that $f(x,z)=b$? Edit Here is the original problem.  It is problem $2$-$37$.  I've seen other solutions suggest to make $g$ satisfy the conditions of $f$ from $2$-$36$, which is why I left it in. Thanks!",,"['calculus', 'real-analysis', 'analysis', 'multivariable-calculus', 'proof-writing']"
62,On the $p$-adic logarithm,On the -adic logarithm,p,"It is well known that $p$-adic logarithm define an isomorphism from $B(1,p^{-1/p-1})$ to $B(0,p^{-1/p-1})$. I would like to know if is it possible to extend somehow this logarithm to the set of $x$ such that $|x|_p=1$. My idea would be to find a minmal $n$ such that $x^n \in B(1,p^{-1/p-1})$ and define $\log_p(x)=\frac{\log_p(x^n)}{n}$. But I am not able to find such a $n$. So I would like to know if it is indeed possible to extend $\log_p$ to this set.","It is well known that $p$-adic logarithm define an isomorphism from $B(1,p^{-1/p-1})$ to $B(0,p^{-1/p-1})$. I would like to know if is it possible to extend somehow this logarithm to the set of $x$ such that $|x|_p=1$. My idea would be to find a minmal $n$ such that $x^n \in B(1,p^{-1/p-1})$ and define $\log_p(x)=\frac{\log_p(x^n)}{n}$. But I am not able to find such a $n$. So I would like to know if it is indeed possible to extend $\log_p$ to this set.",,['analysis']
63,Can we split/decompose a multivariable function into several single-variable functions?,Can we split/decompose a multivariable function into several single-variable functions?,,"If I have a multivariable function, can I split/decompose it into several single-variable functions? For instance: Given $f:\mathbb R^2 \rightarrow \mathbb R$, I introduce the functions $g:\mathbb R\rightarrow \mathbb R$ and $h:\mathbb R\rightarrow \mathbb R$ so $$ f(x,y)=g(x)h(y) $$ Or $$ f(x,y)=g(x)+h(y) $$ Is this mathematically correct? Is function composition the right name? Ex. 1: The function $f:\mathbb R^2\rightarrow \mathbb R$ is given by $f(x,y)=2xy$. Introduce the functions $g,h:\mathbb R\rightarrow \mathbb R$ and write $$ f(x,y)=2xy=g(x)h(y) $$ where $g(x)=2x$ and $h(y)=y$. Ex. 2: Or if $f(x,y)=2x+y$, we write $$ f(x,y)=2x+y=g(x)+h(y) $$ where $g(x)=2x$ and $h(y)=y$.","If I have a multivariable function, can I split/decompose it into several single-variable functions? For instance: Given $f:\mathbb R^2 \rightarrow \mathbb R$, I introduce the functions $g:\mathbb R\rightarrow \mathbb R$ and $h:\mathbb R\rightarrow \mathbb R$ so $$ f(x,y)=g(x)h(y) $$ Or $$ f(x,y)=g(x)+h(y) $$ Is this mathematically correct? Is function composition the right name? Ex. 1: The function $f:\mathbb R^2\rightarrow \mathbb R$ is given by $f(x,y)=2xy$. Introduce the functions $g,h:\mathbb R\rightarrow \mathbb R$ and write $$ f(x,y)=2xy=g(x)h(y) $$ where $g(x)=2x$ and $h(y)=y$. Ex. 2: Or if $f(x,y)=2x+y$, we write $$ f(x,y)=2x+y=g(x)+h(y) $$ where $g(x)=2x$ and $h(y)=y$.",,"['calculus', 'real-analysis', 'analysis', 'multivariable-calculus']"
64,"Continuous $f:[0,1]\to\mathbb{R}$ is differentiable on $(0,1]$ and $\lim_{x\to 0^+} f'(x)$ is finite. Prove that f has right hand derivative at x=0 [duplicate]",Continuous  is differentiable on  and  is finite. Prove that f has right hand derivative at x=0 [duplicate],"f:[0,1]\to\mathbb{R} (0,1] \lim_{x\to 0^+} f'(x)","This question already has answers here : Prove that $f'(a)=\lim_{x\rightarrow a}f'(x)$. (6 answers) Closed 6 years ago . Suppose $f:[0,1]\to\mathbb{R}$ is differentiable on $(0,1]$  and $\lim_{x\to 0^+} f'(x)$ exists and it is finite. Prove that $f$ has a right hand derivative at $x=0$. What I know is that there exists some c such that  $$\lim_{x\to 0^+}\left(\lim_{x\to y} \frac{f(x)-f(y)}{x-y}\right)=c$$ and $$\lim_{x\to a} f(x)=f(a)$$ I really don't see how I should start from here, any hints welcome.","This question already has answers here : Prove that $f'(a)=\lim_{x\rightarrow a}f'(x)$. (6 answers) Closed 6 years ago . Suppose $f:[0,1]\to\mathbb{R}$ is differentiable on $(0,1]$  and $\lim_{x\to 0^+} f'(x)$ exists and it is finite. Prove that $f$ has a right hand derivative at $x=0$. What I know is that there exists some c such that  $$\lim_{x\to 0^+}\left(\lim_{x\to y} \frac{f(x)-f(y)}{x-y}\right)=c$$ and $$\lim_{x\to a} f(x)=f(a)$$ I really don't see how I should start from here, any hints welcome.",,"['calculus', 'real-analysis', 'analysis']"
65,Continuity of Un-plottable Piece-wise Defined Function,Continuity of Un-plottable Piece-wise Defined Function,,"I'm working on the following problem: Consider the function $$f(x) =  \begin{cases} x + x^2 & x \in \mathbb{Q} \\ x - x^2 & x \notin \mathbb{Q} \\ \end{cases} $$ Prove that $f$ is nowhere continuous except at $x=0$. My attempt: First, we show that $f$ is continuous at $x=0$. Accordingly, fix $\epsilon > 0$, and choose $\delta < \frac{-1 + \sqrt{1 + 4 \epsilon}}{2}$ (note that since the quantity on the left hand side is strictly positive, the Archimedean Principle guarantees the existence of such a $\delta$).  Then, if  $$ |x - 0| = |x| < \delta$$ then $$ |f(x) - f(0)| = |f(x)| =   \begin{cases} |x + x^2| & x \in \mathbb{Q} \\ |x - x^2| & x \notin \mathbb{Q} \\ \end{cases} $$ In either case, the triangle inequality gives that \begin{align*} |f(x)| &\leq |x| + |x|^2 \\ &< \delta + \delta^2 \\ &= \frac{-2 + 2\sqrt{1+4 \epsilon} + 1 - 2\sqrt{1 + 4 \epsilon} + 1 + 4 \epsilon}{4} \\ &= \frac{4\epsilon}{4} \\ &= \epsilon \end{align*} Thus, for arbitrary $\epsilon>0$, $\exists \delta > 0$ s.t. $$ |x - 0| < \delta $$ implies $$ |f(x) - f(0)| < \epsilon,$$  so $f$ is continuous at $x=0$. Now, consider some real $x_0 \neq 0$. It follows from the Sequential Density of Irrationals/Rationals that there exists some sequence $u_n$ of rational numbers that converges to $x_0$, and there exists some sequence $v_n$ of irrational numbers that also converges to $x_0$. Now, since the limits $\lim_{n \rightarrow \infty} u_n = x_0$ and  $\lim_{n \rightarrow \infty} v_n = x_0$ exist, we conclude that $$\lim_{n \rightarrow \infty} f(u_n) = \lim_{n \rightarrow \infty} u_n + u_n^2 = x_0 + x_0^2$$  and  $$\lim_{n \rightarrow \infty} f(v_n) = \lim_{n \rightarrow \infty} v_n + v_n^2 = x_0 - x_0^2.$$ Since these limits are distinct for $x_0 \neq 0$, it follows that either the image of $u_n$ or the image of $v_n$ under $f$ does not converge to $f(x_0)$. Thus, $f$ is not continuous for any $x_0 \neq 0$. Is this a valid proof? I'm concerned I jumped in a bit too eagerly - is it necessary to include the former proof of continuity? Or does it suffice to say the images limit to the same value only when $x_0$ is identically 0?","I'm working on the following problem: Consider the function $$f(x) =  \begin{cases} x + x^2 & x \in \mathbb{Q} \\ x - x^2 & x \notin \mathbb{Q} \\ \end{cases} $$ Prove that $f$ is nowhere continuous except at $x=0$. My attempt: First, we show that $f$ is continuous at $x=0$. Accordingly, fix $\epsilon > 0$, and choose $\delta < \frac{-1 + \sqrt{1 + 4 \epsilon}}{2}$ (note that since the quantity on the left hand side is strictly positive, the Archimedean Principle guarantees the existence of such a $\delta$).  Then, if  $$ |x - 0| = |x| < \delta$$ then $$ |f(x) - f(0)| = |f(x)| =   \begin{cases} |x + x^2| & x \in \mathbb{Q} \\ |x - x^2| & x \notin \mathbb{Q} \\ \end{cases} $$ In either case, the triangle inequality gives that \begin{align*} |f(x)| &\leq |x| + |x|^2 \\ &< \delta + \delta^2 \\ &= \frac{-2 + 2\sqrt{1+4 \epsilon} + 1 - 2\sqrt{1 + 4 \epsilon} + 1 + 4 \epsilon}{4} \\ &= \frac{4\epsilon}{4} \\ &= \epsilon \end{align*} Thus, for arbitrary $\epsilon>0$, $\exists \delta > 0$ s.t. $$ |x - 0| < \delta $$ implies $$ |f(x) - f(0)| < \epsilon,$$  so $f$ is continuous at $x=0$. Now, consider some real $x_0 \neq 0$. It follows from the Sequential Density of Irrationals/Rationals that there exists some sequence $u_n$ of rational numbers that converges to $x_0$, and there exists some sequence $v_n$ of irrational numbers that also converges to $x_0$. Now, since the limits $\lim_{n \rightarrow \infty} u_n = x_0$ and  $\lim_{n \rightarrow \infty} v_n = x_0$ exist, we conclude that $$\lim_{n \rightarrow \infty} f(u_n) = \lim_{n \rightarrow \infty} u_n + u_n^2 = x_0 + x_0^2$$  and  $$\lim_{n \rightarrow \infty} f(v_n) = \lim_{n \rightarrow \infty} v_n + v_n^2 = x_0 - x_0^2.$$ Since these limits are distinct for $x_0 \neq 0$, it follows that either the image of $u_n$ or the image of $v_n$ under $f$ does not converge to $f(x_0)$. Thus, $f$ is not continuous for any $x_0 \neq 0$. Is this a valid proof? I'm concerned I jumped in a bit too eagerly - is it necessary to include the former proof of continuity? Or does it suffice to say the images limit to the same value only when $x_0$ is identically 0?",,"['analysis', 'continuity', 'piecewise-continuity']"
66,Non-existence of weak solution in one dimension,Non-existence of weak solution in one dimension,,"Let $\Omega=(1,\infty).$ Then for any given $f\in L^2(\Omega),$ the equation  $$ -u''=f\,\,\text{in}\,\,\Omega, $$ does not admit any weak solution in $W_{0}^{1,2}(\Omega).$ I tried the solution by contradictory argument. Indeed, suppose such a weak solution exists, say $u.$ Then for every $\phi\in H_0^{1}(\Omega),$ we have $$ \int_{\Omega}\,u'\phi'\,dx=\int_{\Omega}f\phi\,dx $$ I tried to construct $\phi_n\in C_c^{\infty}(\Omega)$ such that the above inequality become false. I think this idea will work, but I am still unable to construct such $\phi.$ If anyone can help me regarding this or with some other idea on solving this problem, it will be very grateful for me. Thanks.","Let $\Omega=(1,\infty).$ Then for any given $f\in L^2(\Omega),$ the equation  $$ -u''=f\,\,\text{in}\,\,\Omega, $$ does not admit any weak solution in $W_{0}^{1,2}(\Omega).$ I tried the solution by contradictory argument. Indeed, suppose such a weak solution exists, say $u.$ Then for every $\phi\in H_0^{1}(\Omega),$ we have $$ \int_{\Omega}\,u'\phi'\,dx=\int_{\Omega}f\phi\,dx $$ I tried to construct $\phi_n\in C_c^{\infty}(\Omega)$ such that the above inequality become false. I think this idea will work, but I am still unable to construct such $\phi.$ If anyone can help me regarding this or with some other idea on solving this problem, it will be very grateful for me. Thanks.",,"['analysis', 'partial-differential-equations', 'regularity-theory-of-pdes', 'linear-pde']"
67,Understanding $\sum_{n=1}^{\infty} \frac{1}{n^4} = \frac{\pi^4}{90}$,Understanding,\sum_{n=1}^{\infty} \frac{1}{n^4} = \frac{\pi^4}{90},"In the answer, of this question Nice proofs of $\zeta(4) = \frac{\pi^4}{90}$? , given by Christian Blatter, Why $|c_{\pm k}|^2={1\over4}a_k^2={4\over k^4}$ $\ (k\geq1)$ ? According to my calculations $|c_{\pm k}|^2=\vert1/2\pi\int_{-\pi}^\pi t^2e^{-i(\pm k)x}dx\vert^2=\vert t^2\sin(\pi(\pm k))/\pi(\pm k)\vert^2=0$ And how did he get $\frac{\pi^4}{90}?$ Consider the function $f(t):=t^2\ \ (-\pi\leq t\leq \pi)$ , extended to all of ${\mathbb R}$ periodically with period $2\pi$ . Developping $f$ into a  Fourier series we get $$t^2 ={\pi^2\over3}+\sum_{k=1}^\infty {4(-1)^k\over k^2}\cos(kt)\qquad(-\pi\leq t\leq \pi).$$ If we put $t:=\pi$ here we easily find $\zeta(2)={\pi^2\over6}$ . For $\zeta(4)$ we use Parseval's formula $$\|f\|^2=\sum_{k=-\infty}^\infty |c_k|^2\ .$$ Here $$\|f\|^2={1\over2\pi}\int_{-\pi}^\pi t^4\>dt={\pi^4\over5}$$ and the $c_k$ are the complex Fourier coefficients of $f$ . Therefore $c_0={\pi^2\over3}$ and $|c_{\pm k}|^2={1\over4}a_k^2={4\over k^4}$ $\ (k\geq1)$ . Putting it all together gives $\zeta(4)={\pi^4\over 90}$ .","In the answer, of this question Nice proofs of $\zeta(4) = \frac{\pi^4}{90}$? , given by Christian Blatter, Why ? According to my calculations And how did he get Consider the function , extended to all of periodically with period . Developping into a  Fourier series we get If we put here we easily find . For we use Parseval's formula Here and the are the complex Fourier coefficients of . Therefore and . Putting it all together gives .",|c_{\pm k}|^2={1\over4}a_k^2={4\over k^4} \ (k\geq1) |c_{\pm k}|^2=\vert1/2\pi\int_{-\pi}^\pi t^2e^{-i(\pm k)x}dx\vert^2=\vert t^2\sin(\pi(\pm k))/\pi(\pm k)\vert^2=0 \frac{\pi^4}{90}? f(t):=t^2\ \ (-\pi\leq t\leq \pi) {\mathbb R} 2\pi f t^2 ={\pi^2\over3}+\sum_{k=1}^\infty {4(-1)^k\over k^2}\cos(kt)\qquad(-\pi\leq t\leq \pi). t:=\pi \zeta(2)={\pi^2\over6} \zeta(4) \|f\|^2=\sum_{k=-\infty}^\infty |c_k|^2\ . \|f\|^2={1\over2\pi}\int_{-\pi}^\pi t^4\>dt={\pi^4\over5} c_k f c_0={\pi^2\over3} |c_{\pm k}|^2={1\over4}a_k^2={4\over k^4} \ (k\geq1) \zeta(4)={\pi^4\over 90},"['real-analysis', 'sequences-and-series', 'analysis', 'fourier-analysis', 'proof-explanation']"
68,Make sense of taking 'differential' on both sides of a matrix equation,Make sense of taking 'differential' on both sides of a matrix equation,,"This is related to a question I posted. Here is the statement for the question In control theory, the discrete Lyapunov equation is defined as     \begin{align*} A^T X A + Q  = X, \end{align*}      where $A \in \mathcal{M}(n \times n; \mathbb R)$ and $Q \in \mathbb {S}_{++}$ ( positive definite matrices). There is a theorem stating if the spectral radius of $A$ satisfies $\rho(A) < 1$ and for fixed $Q > 0$, there exists a unique $X \in \mathbb {S}_{++}$ which solves above equation.     Let $D = \{A \in \mathcal{M}(n \times n; \mathbb R): \rho(A) < 1\}$ and fix $Q$. Suppose we define some scalar valued function $f$ over $X$ which are solutions of Lyapunov equation over $D$. To make it more concrete, let us define this scalar valued function to be $f(X) = \text{tr}(X)$. This function can be also viewed as a function $g$ over $D$, i.e., it is a composition     \begin{align*} g \colon A \xrightarrow{h} X \xrightarrow{f} \text{tr}(X). \end{align*}      Now I would like to differentiate $g$ with respect to $A$. There are some very good answers to this question. The answers posted by @greg and @lynn are very interesting. But in the answers, they kind of freely take 'differential' of both sides and applying product rule $$dA^T X A + A^T d X A + A^T X dA = dX.$$ I am a little uncomfortable with using the symbols $dX, dA$ before assigning some mathematical meaning. I know we can intuitively think them as infinitesimal change in the entries. But I would like to know some rigorous way to understand it.  The only place I know they have a meaning is in differential geometry, i.e., differential forms. In this situation, how do we make sense of this step?","This is related to a question I posted. Here is the statement for the question In control theory, the discrete Lyapunov equation is defined as     \begin{align*} A^T X A + Q  = X, \end{align*}      where $A \in \mathcal{M}(n \times n; \mathbb R)$ and $Q \in \mathbb {S}_{++}$ ( positive definite matrices). There is a theorem stating if the spectral radius of $A$ satisfies $\rho(A) < 1$ and for fixed $Q > 0$, there exists a unique $X \in \mathbb {S}_{++}$ which solves above equation.     Let $D = \{A \in \mathcal{M}(n \times n; \mathbb R): \rho(A) < 1\}$ and fix $Q$. Suppose we define some scalar valued function $f$ over $X$ which are solutions of Lyapunov equation over $D$. To make it more concrete, let us define this scalar valued function to be $f(X) = \text{tr}(X)$. This function can be also viewed as a function $g$ over $D$, i.e., it is a composition     \begin{align*} g \colon A \xrightarrow{h} X \xrightarrow{f} \text{tr}(X). \end{align*}      Now I would like to differentiate $g$ with respect to $A$. There are some very good answers to this question. The answers posted by @greg and @lynn are very interesting. But in the answers, they kind of freely take 'differential' of both sides and applying product rule $$dA^T X A + A^T d X A + A^T X dA = dX.$$ I am a little uncomfortable with using the symbols $dX, dA$ before assigning some mathematical meaning. I know we can intuitively think them as infinitesimal change in the entries. But I would like to know some rigorous way to understand it.  The only place I know they have a meaning is in differential geometry, i.e., differential forms. In this situation, how do we make sense of this step?",,"['analysis', 'differential-geometry', 'matrix-equations', 'differential-forms', 'differential']"
69,Sum of infinitely many integrals over a bounded interval,Sum of infinitely many integrals over a bounded interval,,"Recalling from my days in Calc BC that if we have some integrable function $\ f\colon \mathbb R \to \mathbb R, x \mapsto f(x)$, three numbers $\ a,b,c \in \mathbb R,a \lt b \lt c$ and we know $\int_a^c f(x)\, dx=A$, then it follows that $\int_a^b f(x)\, dx +\int_b^c f(x)\, dx =A$, too. This always bothered me, because it seems like we're double counting at b. To take the above property to its extreme, I wondered what would happen if we start dividing up the interval $[a,c]$ into smaller and smaller chunks and then summing up the integrals over said smaller and smaller chunks. In mathematical notation, I believe the above would be expressable as follows – define some $N \in \mathbb N$ and let $c-a=N\epsilon$, and take $$\displaystyle\lim_{N \to \infty} \displaystyle \sum_{j=0}^{N-1} \int_{a+j\epsilon}^{a+(j+1)\epsilon} f(x)\, dx.$$ My question is, can this above expression ever $\ne A$? If not, why does double counting a point on an integral an infinite number of times not affect its net value? As a follow up, what could happen if we let the various limits of integration overlap on some small interval larger than a point? Thanks!","Recalling from my days in Calc BC that if we have some integrable function $\ f\colon \mathbb R \to \mathbb R, x \mapsto f(x)$, three numbers $\ a,b,c \in \mathbb R,a \lt b \lt c$ and we know $\int_a^c f(x)\, dx=A$, then it follows that $\int_a^b f(x)\, dx +\int_b^c f(x)\, dx =A$, too. This always bothered me, because it seems like we're double counting at b. To take the above property to its extreme, I wondered what would happen if we start dividing up the interval $[a,c]$ into smaller and smaller chunks and then summing up the integrals over said smaller and smaller chunks. In mathematical notation, I believe the above would be expressable as follows – define some $N \in \mathbb N$ and let $c-a=N\epsilon$, and take $$\displaystyle\lim_{N \to \infty} \displaystyle \sum_{j=0}^{N-1} \int_{a+j\epsilon}^{a+(j+1)\epsilon} f(x)\, dx.$$ My question is, can this above expression ever $\ne A$? If not, why does double counting a point on an integral an infinite number of times not affect its net value? As a follow up, what could happen if we let the various limits of integration overlap on some small interval larger than a point? Thanks!",,"['calculus', 'real-analysis', 'analysis', 'measure-theory']"
70,Variation on Harmonic Series,Variation on Harmonic Series,,"I'm trying to establish the convergence or divergence of the following variant of the harmonic series: $$\frac{1}{1}+\frac{1}{2}-\frac{1}{3}-\frac{1}{4}-\frac{1}{5}+\frac{1}{6}+\frac{1}{7}-\frac{1}{8}-\frac{1}{9}-\frac{1}{10}+\frac{1}{11}\cdots$$ Where the sign pattern has period 5, ie, it looks like this: ++---++---++---.... My thought has been to find a regrouping that diverges, since I only need to find one in order to show the series is divergent. I tried to bound this series below by increasing the denominator on the positive terms, and decreasing it on the negative terms to yield a series like $$\left(\frac{1}{2}+\frac{1}{2}\right)-\left(\frac{1}{3}+\frac{1}{3}+\frac{1}{3}\right)+\left(\frac{1}{7}+\frac{1}{7}\right)+ \cdots$$ but I'm pretty sure this will converge. I don't really know what approach to take next. A hint would be greatly appreciated! Thanks!","I'm trying to establish the convergence or divergence of the following variant of the harmonic series: $$\frac{1}{1}+\frac{1}{2}-\frac{1}{3}-\frac{1}{4}-\frac{1}{5}+\frac{1}{6}+\frac{1}{7}-\frac{1}{8}-\frac{1}{9}-\frac{1}{10}+\frac{1}{11}\cdots$$ Where the sign pattern has period 5, ie, it looks like this: ++---++---++---.... My thought has been to find a regrouping that diverges, since I only need to find one in order to show the series is divergent. I tried to bound this series below by increasing the denominator on the positive terms, and decreasing it on the negative terms to yield a series like $$\left(\frac{1}{2}+\frac{1}{2}\right)-\left(\frac{1}{3}+\frac{1}{3}+\frac{1}{3}\right)+\left(\frac{1}{7}+\frac{1}{7}\right)+ \cdots$$ but I'm pretty sure this will converge. I don't really know what approach to take next. A hint would be greatly appreciated! Thanks!",,"['sequences-and-series', 'analysis', 'convergence-divergence']"
71,Completeness in Levi-Civita field,Completeness in Levi-Civita field,,"I've been wondering for quite a time about Levi-Civita field (you can read it simply in https://en.wikipedia.org/wiki/Levi-Civita_field ). I remember that I've read somewhere that Levi-Civita field is the smallest complete Non-Archimedean field. I am not too sure what it means by ""complete"" there. Does it mean that every number is in that field? I was thinking, for example, $\exp^{\pi}$ won't be there as the power is not rational. Is that right though? Is every number included in LC field? Cheers!","I've been wondering for quite a time about Levi-Civita field (you can read it simply in https://en.wikipedia.org/wiki/Levi-Civita_field ). I remember that I've read somewhere that Levi-Civita field is the smallest complete Non-Archimedean field. I am not too sure what it means by ""complete"" there. Does it mean that every number is in that field? I was thinking, for example, $\exp^{\pi}$ won't be there as the power is not rational. Is that right though? Is every number included in LC field? Cheers!",,"['analysis', 'field-theory', 'nonstandard-analysis']"
72,Ways to determine how fast a sequence diverge/converge [closed],Ways to determine how fast a sequence diverge/converge [closed],,"Closed . This question needs to be more focused . It is not currently accepting answers. Want to improve this question? Update the question so it focuses on one problem only by editing this post . Closed 6 years ago . Improve this question I have recently read about divergent/convergent sequences, and I wonder what methods are normally used to show how fast a sequence grow. Can I simply find the difference between two adjacent terms? And how can I do this for sequences like harmonic series?  Thank you! Here is an example, for a sequence that can be written as an iterated function $$ x_{n+1}=\frac{1}{\sqrt{x_n^2+1}} $$ How can I determine how fast this series grow?","Closed . This question needs to be more focused . It is not currently accepting answers. Want to improve this question? Update the question so it focuses on one problem only by editing this post . Closed 6 years ago . Improve this question I have recently read about divergent/convergent sequences, and I wonder what methods are normally used to show how fast a sequence grow. Can I simply find the difference between two adjacent terms? And how can I do this for sequences like harmonic series?  Thank you! Here is an example, for a sequence that can be written as an iterated function $$ x_{n+1}=\frac{1}{\sqrt{x_n^2+1}} $$ How can I determine how fast this series grow?",,"['sequences-and-series', 'analysis', 'convergence-divergence', 'divergent-series']"
73,Newton's Binomial Series and p-adic roots of unity,Newton's Binomial Series and p-adic roots of unity,,"I've been playing around with $p$  -adic exponentiation via Newton's binomial series: $$\left(1+t\right)^{x}=\sum_{k=0}^{\infty}t^{k}\binom{x}{k},\textrm{ }x\in\mathbb{Z}_{p}$$  where $t\in\mathbb{C}_{p}$   with $\left|t\right|_{p}<1$  . I should mention right away that I am most concerned with the case where $p=3$  , but answers for the case of an arbitrary prime $p$   would also be much appreciated. Throughout this, $x$   is an indeterminate/variable in $\mathbb{Z}_{p}$. Now, for any integer $a\geq2$, let $\xi_{a}$ denote a primitive $a$th root of unity. Moreover, let's assume that we have chosen a specific embedding/labeling of $\xi_{a}$ in $\mathbb{C}_{p}$, so that $\xi_{a}$ denotes a single element of $\mathbb{C}_{p}$. My hope is to try to use the Newton series formula to make sense of the function $x\in\mathbb{Z}_{p}\mapsto\xi_{a}^{x}\in\mathbb{C}_{p}$. As an example, for $p=3$, note that:$$\frac{x^{3}-1}{x-1}=x^{2}+x+1=\left(x-\xi_{3}\right)\left(x-\xi_{3}^{2}\right)$$  Setting $x=1$, I obtain:$$3=\left(1-\xi_{3}\right)\left(1-\xi_{3}^{2}\right)=\frac{1}{\xi_{3}}\left(1-\xi_{3}\right)\left(\xi_{3}-1\right)$$  and so:$$\frac{1}{3}=\left|-3\xi_{3}\right|_{3}=\left|\xi_{3}-1\right|_{3}^{2}$$  from which I deduce that: $$\left|\xi_{3}-1\right|_{3}=\frac{1}{\sqrt{3}}<1$$  and hence, that: $$\xi_{3}^{x}=\left(1+\xi_{3}-1\right)^{x}=\sum_{k=0}^{\infty}\left(\xi_{3}-1\right)^{k}\binom{x}{k}$$  is a continuous, (strictly) differentiable (and analytic?) function from $\mathbb{Z}_{3}\rightarrow\mathbb{C}_{3}$  . Obviously, this construction depends on both the values of $a$   and $p$. This leads me to my questions: 1) there a general, closed-form expression for $\left|\xi_{a}-1\right|_{p}$? For $\left|\xi_{a}-1\right|_{3}$? Equivalently: given $p$  , for what $a\geq2$   does: $$\xi_{a}^{x}=\left(1+\xi_{a}-1\right)^{x}=\sum_{k=0}^{\infty}\left(\xi_{a}-1\right)^{k}\binom{x}{k}$$   define a strictly differentiable (analytic?) function from $\mathbb{Z}_{p}$   to $\mathbb{C}_{p}$? 2) On a related note, if $a$   is even, is $\xi_{a}^{a/2}=-1$   in $\mathbb{C}_{p}$? That is to say, when $a$   is even, does there exist a choice of a specific embedding of $\xi_{a}$   into $\mathbb{C}_{p}$   so that $\xi_{a}$   satisfies $\xi_{a}^{a/2}=-1$?","I've been playing around with $p$  -adic exponentiation via Newton's binomial series: $$\left(1+t\right)^{x}=\sum_{k=0}^{\infty}t^{k}\binom{x}{k},\textrm{ }x\in\mathbb{Z}_{p}$$  where $t\in\mathbb{C}_{p}$   with $\left|t\right|_{p}<1$  . I should mention right away that I am most concerned with the case where $p=3$  , but answers for the case of an arbitrary prime $p$   would also be much appreciated. Throughout this, $x$   is an indeterminate/variable in $\mathbb{Z}_{p}$. Now, for any integer $a\geq2$, let $\xi_{a}$ denote a primitive $a$th root of unity. Moreover, let's assume that we have chosen a specific embedding/labeling of $\xi_{a}$ in $\mathbb{C}_{p}$, so that $\xi_{a}$ denotes a single element of $\mathbb{C}_{p}$. My hope is to try to use the Newton series formula to make sense of the function $x\in\mathbb{Z}_{p}\mapsto\xi_{a}^{x}\in\mathbb{C}_{p}$. As an example, for $p=3$, note that:$$\frac{x^{3}-1}{x-1}=x^{2}+x+1=\left(x-\xi_{3}\right)\left(x-\xi_{3}^{2}\right)$$  Setting $x=1$, I obtain:$$3=\left(1-\xi_{3}\right)\left(1-\xi_{3}^{2}\right)=\frac{1}{\xi_{3}}\left(1-\xi_{3}\right)\left(\xi_{3}-1\right)$$  and so:$$\frac{1}{3}=\left|-3\xi_{3}\right|_{3}=\left|\xi_{3}-1\right|_{3}^{2}$$  from which I deduce that: $$\left|\xi_{3}-1\right|_{3}=\frac{1}{\sqrt{3}}<1$$  and hence, that: $$\xi_{3}^{x}=\left(1+\xi_{3}-1\right)^{x}=\sum_{k=0}^{\infty}\left(\xi_{3}-1\right)^{k}\binom{x}{k}$$  is a continuous, (strictly) differentiable (and analytic?) function from $\mathbb{Z}_{3}\rightarrow\mathbb{C}_{3}$  . Obviously, this construction depends on both the values of $a$   and $p$. This leads me to my questions: 1) there a general, closed-form expression for $\left|\xi_{a}-1\right|_{p}$? For $\left|\xi_{a}-1\right|_{3}$? Equivalently: given $p$  , for what $a\geq2$   does: $$\xi_{a}^{x}=\left(1+\xi_{a}-1\right)^{x}=\sum_{k=0}^{\infty}\left(\xi_{a}-1\right)^{k}\binom{x}{k}$$   define a strictly differentiable (analytic?) function from $\mathbb{Z}_{p}$   to $\mathbb{C}_{p}$? 2) On a related note, if $a$   is even, is $\xi_{a}^{a/2}=-1$   in $\mathbb{C}_{p}$? That is to say, when $a$   is even, does there exist a choice of a specific embedding of $\xi_{a}$   into $\mathbb{C}_{p}$   so that $\xi_{a}$   satisfies $\xi_{a}^{a/2}=-1$?",,"['analysis', 'p-adic-number-theory', 'roots-of-unity']"
74,Prove $f(x) =0$ for all $x$,Prove  for all,f(x) =0 x,"Let $f:[0,1]→\Bbb R$ be continuous with $f(0)=f(1)=0$. Suppose that for every $x ∈ (0,1)$ there exists $δ > 0$ such that both $x−δ$ and $x+δ$ belong to $(0,1)$ and $f(x) = \frac{1}{2}(f(x−δ)+f(x+δ))$ . Prove that $f(x) = 0$ for all $x ∈ [0, 1]$. I have considered several approaches but not had much success with any of them. First, I tried to prove that $f'(x)=0$ for all x, because as we're given $f(0)=f(1)=0$, I think that is sufficient? But I ran into trouble proving that the limit exists for some general $a$ in the domain. So I decided to try and use continuity, specifically around zero: my intuition was that for $x$ sufficiently close to 0, $f(x)$ is approximately zero and I tried to build up that the images of the two perturbations would also be zero but again without much success. Please could I have some help? Ideally, a discussion of the right kind of intuition as well as a sketch solution. Comments on whether my approaches were at all in the right direction would also be appreciated. Edit: Please could this be solved with using connectedness and compactness. Thank you.","Let $f:[0,1]→\Bbb R$ be continuous with $f(0)=f(1)=0$. Suppose that for every $x ∈ (0,1)$ there exists $δ > 0$ such that both $x−δ$ and $x+δ$ belong to $(0,1)$ and $f(x) = \frac{1}{2}(f(x−δ)+f(x+δ))$ . Prove that $f(x) = 0$ for all $x ∈ [0, 1]$. I have considered several approaches but not had much success with any of them. First, I tried to prove that $f'(x)=0$ for all x, because as we're given $f(0)=f(1)=0$, I think that is sufficient? But I ran into trouble proving that the limit exists for some general $a$ in the domain. So I decided to try and use continuity, specifically around zero: my intuition was that for $x$ sufficiently close to 0, $f(x)$ is approximately zero and I tried to build up that the images of the two perturbations would also be zero but again without much success. Please could I have some help? Ideally, a discussion of the right kind of intuition as well as a sketch solution. Comments on whether my approaches were at all in the right direction would also be appreciated. Edit: Please could this be solved with using connectedness and compactness. Thank you.",,"['real-analysis', 'analysis', 'continuity']"
75,Show that the digit $d$($0<d<10$) occurs as a first digit in power of 2 with frequency $log_{10} \frac{d+1}{d}$.,Show that the digit () occurs as a first digit in power of 2 with frequency .,d 0<d<10 log_{10} \frac{d+1}{d},Show that the digit $d$($0<d<10$) occurs as a first digit in power of $2 $ with frequency $log_{10} \frac{d+1}{d}$. I think that this can be proved using Birkhoff Ergodic theorem. Any help?,Show that the digit $d$($0<d<10$) occurs as a first digit in power of $2 $ with frequency $log_{10} \frac{d+1}{d}$. I think that this can be proved using Birkhoff Ergodic theorem. Any help?,,"['probability', 'analysis']"
76,"Let $f$ a function with bounded partial derivatives. Find the greatest value that $f$ can achieve at point $(1, 2)$",Let  a function with bounded partial derivatives. Find the greatest value that  can achieve at point,"f f (1, 2)","Let $f: \mathbb{R}^2 \rightarrow \mathbb{R}$ a function partial differentiable with \begin{align*}  \left| \frac{\partial f}{\partial x} \,  (x, y) \right| \leq 10 \, \, \, \,  \text{and} \, \, \, \left| \frac{\partial f}{\partial y} \, (x, y) \right| \leq 20 \end{align*} for all $(x, y) \in \mathbb{R}^2$ and with $f(0, 0) = 0$. Find the greatest value that $f$ can achieve at the point $(1, 2)$. Give an example of a function $f$ satisfying these assumptions and such that $f(1, 2)$ achieve this value. I have really no idea to solve this problem (maybe with Taylor's theorem but I don't know how). I only know that a function $f$ satisfying these assumptions is continuous and $L$-lipschitz with $L:=10 \sqrt5$ but I don't see anything else. Can someone help me please? Thanks in advance!","Let $f: \mathbb{R}^2 \rightarrow \mathbb{R}$ a function partial differentiable with \begin{align*}  \left| \frac{\partial f}{\partial x} \,  (x, y) \right| \leq 10 \, \, \, \,  \text{and} \, \, \, \left| \frac{\partial f}{\partial y} \, (x, y) \right| \leq 20 \end{align*} for all $(x, y) \in \mathbb{R}^2$ and with $f(0, 0) = 0$. Find the greatest value that $f$ can achieve at the point $(1, 2)$. Give an example of a function $f$ satisfying these assumptions and such that $f(1, 2)$ achieve this value. I have really no idea to solve this problem (maybe with Taylor's theorem but I don't know how). I only know that a function $f$ satisfying these assumptions is continuous and $L$-lipschitz with $L:=10 \sqrt5$ but I don't see anything else. Can someone help me please? Thanks in advance!",,"['real-analysis', 'analysis', 'multivariable-calculus', 'taylor-expansion', 'partial-derivative']"
77,How to prove differentiability and continuity for piecewise function,How to prove differentiability and continuity for piecewise function,,"Given the following piecewise function: $f(x)$ \begin{cases}        x \sin x & x\ \in \mathbb Q \\       0 & x \in \mathbb {R/Q}     \end{cases} Prove/Disprove: a) $f(x)$ is continuous b) $f(x)$ is differentiable I am having trouble using the Epsilon-Delta definition given that there is always an irrational number between rational numbers, so I'm assumung that I need to construct a sequence that converges to an x $\in \mathbb {Q}$ but these sequences  $(x_{n})_n$ could also theoretically be $\in \mathbb {R/Q}$?","Given the following piecewise function: $f(x)$ \begin{cases}        x \sin x & x\ \in \mathbb Q \\       0 & x \in \mathbb {R/Q}     \end{cases} Prove/Disprove: a) $f(x)$ is continuous b) $f(x)$ is differentiable I am having trouble using the Epsilon-Delta definition given that there is always an irrational number between rational numbers, so I'm assumung that I need to construct a sequence that converges to an x $\in \mathbb {Q}$ but these sequences  $(x_{n})_n$ could also theoretically be $\in \mathbb {R/Q}$?",,"['analysis', 'derivatives', 'continuity']"
78,"Show that there is a unique polynomial of degree at most $2n+1$ such that $q^{[k]}(x_1)=a_k,$ $q^{[k]}(x_2)=b_k$ for $k=0, \dots, n$.",Show that there is a unique polynomial of degree at most  such that   for .,"2n+1 q^{[k]}(x_1)=a_k, q^{[k]}(x_2)=b_k k=0, \dots, n","Let $x_1, x_2 \in \mathbb R$ and let $(a_0, a_1, \dots, a_n), (b_0, b_1, \dots, b_n)$ be $(n+1)$-tuples of real numbers. Show that there is a unique polynomial of degree at most $2n+1$ such that $$q^{[k]}(x_1)=a_k,$$$$q^{[k]}(x_2)=b_k$$ for $k=0, \dots, n$. Any hints on how to get started with this exercise?","Let $x_1, x_2 \in \mathbb R$ and let $(a_0, a_1, \dots, a_n), (b_0, b_1, \dots, b_n)$ be $(n+1)$-tuples of real numbers. Show that there is a unique polynomial of degree at most $2n+1$ such that $$q^{[k]}(x_1)=a_k,$$$$q^{[k]}(x_2)=b_k$$ for $k=0, \dots, n$. Any hints on how to get started with this exercise?",,"['calculus', 'real-analysis', 'analysis', 'polynomials', 'interpolation']"
79,Conditions on a Lipschitz function $f:U\subset \Bbb R^m\to \Bbb R^n$ which guarantees differentiability at a point $a\in U$,Conditions on a Lipschitz function  which guarantees differentiability at a point,f:U\subset \Bbb R^m\to \Bbb R^n a\in U,"Let $f:U\to \Bbb R^n$ be Lipschitz in the open $U\subset \Bbb R^m$. Given $a\in U$, suppose that, for all $v\in \Bbb R^m$, there exists the directional derivative $\dfrac{\partial f}{\partial v}(a)$ and it depends linearly on $v$. Prove that, for all path $g:(-\epsilon,\epsilon)\to U$, with $g(0)=a$, differentiable in $t=0$, there exists the velocity vector $(f\circ g)'(0)$. Conclude that $f$ is differentiable in the point $a$. I was able to show the existence of such $(f\circ g)'(0)$'s (in the proof I didn't use the linearity of the directional derivatives), however, I couldn't do the final conclusion. I can define $T:\Bbb R^m\to \Bbb R^n$ as $$T(v)=\dfrac{\partial f}{\partial v}(a)$$ which is linear, by hypothesis. I wished that $T=f'(a)$, that is, I need to show that $$\lim_{h\to 0} \dfrac{r(h)}{|h|}=0$$ with $r(h)=f(a+h)-f(a)-T\cdot h=f(a+h)-f(a)-\dfrac{\partial f}{\partial h}(a)$, but I don't know how to work with this $\dfrac{}{\partial h}$ when $h\to 0$...","Let $f:U\to \Bbb R^n$ be Lipschitz in the open $U\subset \Bbb R^m$. Given $a\in U$, suppose that, for all $v\in \Bbb R^m$, there exists the directional derivative $\dfrac{\partial f}{\partial v}(a)$ and it depends linearly on $v$. Prove that, for all path $g:(-\epsilon,\epsilon)\to U$, with $g(0)=a$, differentiable in $t=0$, there exists the velocity vector $(f\circ g)'(0)$. Conclude that $f$ is differentiable in the point $a$. I was able to show the existence of such $(f\circ g)'(0)$'s (in the proof I didn't use the linearity of the directional derivatives), however, I couldn't do the final conclusion. I can define $T:\Bbb R^m\to \Bbb R^n$ as $$T(v)=\dfrac{\partial f}{\partial v}(a)$$ which is linear, by hypothesis. I wished that $T=f'(a)$, that is, I need to show that $$\lim_{h\to 0} \dfrac{r(h)}{|h|}=0$$ with $r(h)=f(a+h)-f(a)-T\cdot h=f(a+h)-f(a)-\dfrac{\partial f}{\partial h}(a)$, but I don't know how to work with this $\dfrac{}{\partial h}$ when $h\to 0$...",,"['calculus', 'analysis', 'multivariable-calculus', 'derivatives']"
80,Derivative of $e^{2x^4-x^2-1}$ with limit definition of derivative,Derivative of  with limit definition of derivative,e^{2x^4-x^2-1},Let $f:\mathbb{R}\to\mathbb{R}$ be defined as $f(x) = e^{2x^4-x^2-1}$. I have to find the derivative using the defintion: $$f'(x) = \lim_{h \to 0} \frac{f(x+h)-f(x)}{h}$$ My approach: $$ \begin{align} &\lim_{h \to 0} \frac{\exp({2(x+h)^4-(x+h)^2-1})-\exp({2x^4-x^2-1})}{h}\\ &=\lim_{h \to 0} \frac{\exp(2 h^4 + 8 h^3 x + h^2 (12 x^2 - 1) + h (8 x^3 - 2 x) + 2 x^4 - x^2 - 1) - e^{2 x^4 - x^2 - 1}}{h} \end{align}$$ How can I remove $h $from denominator?,Let $f:\mathbb{R}\to\mathbb{R}$ be defined as $f(x) = e^{2x^4-x^2-1}$. I have to find the derivative using the defintion: $$f'(x) = \lim_{h \to 0} \frac{f(x+h)-f(x)}{h}$$ My approach: $$ \begin{align} &\lim_{h \to 0} \frac{\exp({2(x+h)^4-(x+h)^2-1})-\exp({2x^4-x^2-1})}{h}\\ &=\lim_{h \to 0} \frac{\exp(2 h^4 + 8 h^3 x + h^2 (12 x^2 - 1) + h (8 x^3 - 2 x) + 2 x^4 - x^2 - 1) - e^{2 x^4 - x^2 - 1}}{h} \end{align}$$ How can I remove $h $from denominator?,,"['analysis', 'derivatives']"
81,Solution of the functional equation $f(x)+f\left(\frac 1 x\right) = f(x)f\left(\frac 1 x\right)$,Solution of the functional equation,f(x)+f\left(\frac 1 x\right) = f(x)f\left(\frac 1 x\right),"If $f(x)+f\left(\frac 1 x\right) = f(x)f\left(\frac 1 x\right)$ , where $f$ is infinitely differentiable function, will only solution of this functional equations be $f(x)=x^n+1$ or $f(x)=-x^n+1$ for some natural number $n \in \mathbb{N}$ . If yes, why?","If , where is infinitely differentiable function, will only solution of this functional equations be or for some natural number . If yes, why?",f(x)+f\left(\frac 1 x\right) = f(x)f\left(\frac 1 x\right) f f(x)=x^n+1 f(x)=-x^n+1 n \in \mathbb{N},"['calculus', 'analysis', 'functional-equations']"
82,"Let $f : \mathbb{R}^2\to \mathbb{R}^2$ be defined by the equation $f(r,\theta) = (r \cos \theta, r \sin \theta)$.",Let  be defined by the equation .,"f : \mathbb{R}^2\to \mathbb{R}^2 f(r,\theta) = (r \cos \theta, r \sin \theta)","Let $f : \mathbb{R}^2\to \mathbb{R}^2$ be defined by the equation $f(r,\theta) = (r \cos \theta, r \sin \theta)$. It is called the polar coordinate transformation. (a) Calculate $Df$ and $\text{det} Df$. (b) Sketch the image under $f$ of the set $S = [1 , 2] \times [0, \pi]$. [Hint: Find the images under $f$ of the line segments that bound $S$ .] I have calculated that $Df(r,θ)$ is the matrix $$\begin{bmatrix} \cos\theta &-r \sin\theta \\ \sin\theta & r\cos\theta \end{bmatrix}$$ so its determinant is $r$. I do not know how I can solve (b), could someone help me please? Thank you very much. Edit: I think that in (b), the image is this:","Let $f : \mathbb{R}^2\to \mathbb{R}^2$ be defined by the equation $f(r,\theta) = (r \cos \theta, r \sin \theta)$. It is called the polar coordinate transformation. (a) Calculate $Df$ and $\text{det} Df$. (b) Sketch the image under $f$ of the set $S = [1 , 2] \times [0, \pi]$. [Hint: Find the images under $f$ of the line segments that bound $S$ .] I have calculated that $Df(r,θ)$ is the matrix $$\begin{bmatrix} \cos\theta &-r \sin\theta \\ \sin\theta & r\cos\theta \end{bmatrix}$$ so its determinant is $r$. I do not know how I can solve (b), could someone help me please? Thank you very much. Edit: I think that in (b), the image is this:",,"['calculus', 'real-analysis', 'analysis', 'multivariable-calculus', 'vector-analysis']"
83,Differentiating under the integral sign and $C^1$ class,Differentiating under the integral sign and  class,C^1,"Suppose that $f\in C^1(\mathbb{R}^n; \mathbb{R})$ and $g(x)=\int_{0}^{1}f(x+ty)dt$, where $y \in \mathbb{R}^n$. How to rigorously show that the function $g(x)$ is continuously differentiable? Since the function $f(x+ty)$ is continuously differentiable for both $x \in \mathbb{R}^n$ and $t \in ]0,1[$ that result seems very reasonable, but I don't know what is the true mathematical reason behind that result.","Suppose that $f\in C^1(\mathbb{R}^n; \mathbb{R})$ and $g(x)=\int_{0}^{1}f(x+ty)dt$, where $y \in \mathbb{R}^n$. How to rigorously show that the function $g(x)$ is continuously differentiable? Since the function $f(x+ty)$ is continuously differentiable for both $x \in \mathbb{R}^n$ and $t \in ]0,1[$ that result seems very reasonable, but I don't know what is the true mathematical reason behind that result.",,"['calculus', 'real-analysis', 'analysis', 'multivariable-calculus']"
84,Show that if $Z\cup E$ is measurable then so is $E.$,Show that if  is measurable then so is,Z\cup E E.,"Problem: Let $E,Z \in\mathbb{R}^{n}$ and $Z$ be a set of measure zero. Show that if $Z\cup E$ is measurable then so is $E.$ What I have done: Since $Z\cup E$ is measurable then there exists    $G$    such that $Z\cup E\subset G$ consequently $ E\subset G$. Also,   $\vert G-(Z\cup E) \vert_{e}<\epsilon $   But    $Z$ is also zero measure so there exists $G'$ such that   $\vert G'-(Z) \vert_{e}<\epsilon $ and $Z\subset G'$   But I can't say that    $\vert G-E\vert<\epsilon'$   Actually, I have to find an open set like $G$ whcih cantains $E$(which I have done) and $\vert G-E\vert<\epsilon'$ I need just a hint, not the whole solution. Maybe my approach is wrong entirely.","Problem: Let $E,Z \in\mathbb{R}^{n}$ and $Z$ be a set of measure zero. Show that if $Z\cup E$ is measurable then so is $E.$ What I have done: Since $Z\cup E$ is measurable then there exists    $G$    such that $Z\cup E\subset G$ consequently $ E\subset G$. Also,   $\vert G-(Z\cup E) \vert_{e}<\epsilon $   But    $Z$ is also zero measure so there exists $G'$ such that   $\vert G'-(Z) \vert_{e}<\epsilon $ and $Z\subset G'$   But I can't say that    $\vert G-E\vert<\epsilon'$   Actually, I have to find an open set like $G$ whcih cantains $E$(which I have done) and $\vert G-E\vert<\epsilon'$ I need just a hint, not the whole solution. Maybe my approach is wrong entirely.",,"['real-analysis', 'analysis', 'measure-theory', 'lebesgue-measure', 'outer-measure']"
85,Determining the countability of a set with a particular property [duplicate],Determining the countability of a set with a particular property [duplicate],,"This question already has answers here : Uncountable set with exactly one limit point (3 answers) Closed 6 years ago . Let $E \subset \mathbb{C}$ be a set with the following property: for any sequence of elements $(e_n)_{n \in \mathbb{N}}$ with $e_n \neq e_m$ for $m \neq n$, $e_n \to 0$. Is $E$ necessarily countable? (Also convergence is in norm, of course). It seems like this problem should yield to contrapositive, that is, given an uncountable set, I can always find at least one nonrepeating sequence that does not converge to $0$. My idea is that for some $\epsilon >0$, if $E$ is uncountable, there must exist infinitely many distinct elements in the complement of $B_\epsilon (0)$. Then we can construct a sequence not converging to $0$ from that (so the statement is true). Is there a direct proof for the above?","This question already has answers here : Uncountable set with exactly one limit point (3 answers) Closed 6 years ago . Let $E \subset \mathbb{C}$ be a set with the following property: for any sequence of elements $(e_n)_{n \in \mathbb{N}}$ with $e_n \neq e_m$ for $m \neq n$, $e_n \to 0$. Is $E$ necessarily countable? (Also convergence is in norm, of course). It seems like this problem should yield to contrapositive, that is, given an uncountable set, I can always find at least one nonrepeating sequence that does not converge to $0$. My idea is that for some $\epsilon >0$, if $E$ is uncountable, there must exist infinitely many distinct elements in the complement of $B_\epsilon (0)$. Then we can construct a sequence not converging to $0$ from that (so the statement is true). Is there a direct proof for the above?",,"['analysis', 'elementary-set-theory']"
86,Analysis of a function: Showing that Quantum Mechanics violates relativity,Analysis of a function: Showing that Quantum Mechanics violates relativity,,"Consider the Hilbert space $L^2(\mathbb{R}^d)$ and a self-adjoint, positive operator $H$ (Hamiltonian). Let $\psi_t$ be a solution to the Schrödinger equation (with $\psi_0$ the initial condition), then consider the function $$t \mapsto \int_{\mathbb{R}} \overline{f(\boldsymbol{x})} e^{-iHt} \psi_0(\boldsymbol{x}) \ d^dx.$$ Assume that $\psi_o$ is compactly supported in some ball of radius $r>0$ centred at the origin. I am supposed to be observing how quantum mechanics violates special relativity by showing that the support of $\psi_t$ grows faster than the speed of light, but I'm lost as to where to go with this. How to proceed? Thanks.","Consider the Hilbert space and a self-adjoint, positive operator (Hamiltonian). Let be a solution to the Schrödinger equation (with the initial condition), then consider the function Assume that is compactly supported in some ball of radius centred at the origin. I am supposed to be observing how quantum mechanics violates special relativity by showing that the support of grows faster than the speed of light, but I'm lost as to where to go with this. How to proceed? Thanks.",L^2(\mathbb{R}^d) H \psi_t \psi_0 t \mapsto \int_{\mathbb{R}} \overline{f(\boldsymbol{x})} e^{-iHt} \psi_0(\boldsymbol{x}) \ d^dx. \psi_o r>0 \psi_t,"['analysis', 'partial-differential-equations']"
87,Triangle inequality for the distance between two sets,Triangle inequality for the distance between two sets,,"I am looking for a reference for the proof of this fact : Let $A$ and $B$ two nonempty closed subsets of a metric space $X$ and   let $x\in X$ . Then: dist $(A,B)\leq$ dist $_A(x)$ + dist $_B(x)$ I am sure that it has already been proved, but I don't find any book or old article where it is. Note: I know how to prove it. I am just looking for a reference about it.","I am looking for a reference for the proof of this fact : Let and two nonempty closed subsets of a metric space and   let . Then: dist dist + dist I am sure that it has already been proved, but I don't find any book or old article where it is. Note: I know how to prove it. I am just looking for a reference about it.","A B X x\in X (A,B)\leq _A(x) _B(x)","['general-topology', 'analysis', 'reference-request', 'metric-spaces']"
88,"Prove if $p + \sqrt{2} \cdot q = 0$, then $p = q = 0$","Prove if , then",p + \sqrt{2} \cdot q = 0 p = q = 0,"Let K be the set $ K := \{p + \sqrt{2} . q : p, q \in \mathbb{Q}\} $ and let addition and multiplication be defined on $\mathbb{R}$ Prove if $p + \sqrt{2} \cdot q = 0$, then $p = q = 0$ Hint: use that $ \sqrt{2}$ is irrational Logically, if $p + \sqrt{2} . q = 0$ this implies $ \sqrt{2} = \frac{-p}{q}$ which is not possible as $ \sqrt{2}$ is irrational. How do I write this as a proof though?","Let K be the set $ K := \{p + \sqrt{2} . q : p, q \in \mathbb{Q}\} $ and let addition and multiplication be defined on $\mathbb{R}$ Prove if $p + \sqrt{2} \cdot q = 0$, then $p = q = 0$ Hint: use that $ \sqrt{2}$ is irrational Logically, if $p + \sqrt{2} . q = 0$ this implies $ \sqrt{2} = \frac{-p}{q}$ which is not possible as $ \sqrt{2}$ is irrational. How do I write this as a proof though?",,['real-analysis']
89,"Find set with derived set $A=\{1/n\,:\,n\in\mathbb{Z^+}\}$",Find set with derived set,"A=\{1/n\,:\,n\in\mathbb{Z^+}\}","I have some doubts about an exercise.. My teacher proposed find the set $B$ such that its derived set $B´=A$ where $A=\{\frac{1}{n},n\in \mathbb{Z^+}\}$.. But the derived set must be closed in $\mathbb{R}$, ( I remembered this now). I proposed this set  $$B=\{\frac{1}{k-\frac{1}{n}} . k,n \in \mathbb Z^+, k*n\neq 1 \}$$ but if fixed $n=1$ then, $$lim_{n \to \infty} {\frac{1}{k-1}}=0$$ then $0 $ is accumulation point of $B$. Is it correct?? $0$ must also be a limit point of B?? why? Perhaps the $0$ should always be a point of accumulation of any set ? and this set, $$B=\cup_{m\in \mathbb{N}}\{\frac{n+1}{mn},n\in N\}$$ fixed $n=1$, $0$ is accumularion point for $B$. -How would you prove that there are no more accumulation points in that set?","I have some doubts about an exercise.. My teacher proposed find the set $B$ such that its derived set $B´=A$ where $A=\{\frac{1}{n},n\in \mathbb{Z^+}\}$.. But the derived set must be closed in $\mathbb{R}$, ( I remembered this now). I proposed this set  $$B=\{\frac{1}{k-\frac{1}{n}} . k,n \in \mathbb Z^+, k*n\neq 1 \}$$ but if fixed $n=1$ then, $$lim_{n \to \infty} {\frac{1}{k-1}}=0$$ then $0 $ is accumulation point of $B$. Is it correct?? $0$ must also be a limit point of B?? why? Perhaps the $0$ should always be a point of accumulation of any set ? and this set, $$B=\cup_{m\in \mathbb{N}}\{\frac{n+1}{mn},n\in N\}$$ fixed $n=1$, $0$ is accumularion point for $B$. -How would you prove that there are no more accumulation points in that set?",,"['real-analysis', 'general-topology', 'analysis']"
90,Let $R\backslash A$,Let,R\backslash A,Let $A\subseteq \mathbb{R}$ be finite and let $U\subseteq\mathbb{R}$ be open. Then $U\backslash A$ is open. Note that $U\backslash A=U\cap \left( \mathbb{R} \backslash A\right)$. We know that $U$ and $ \mathbb{R} \backslash A$ are open. We need to show that $U\cap \left( \mathbb{R} \backslash A\right)$ is open. How can I show the statement?,Let $A\subseteq \mathbb{R}$ be finite and let $U\subseteq\mathbb{R}$ be open. Then $U\backslash A$ is open. Note that $U\backslash A=U\cap \left( \mathbb{R} \backslash A\right)$. We know that $U$ and $ \mathbb{R} \backslash A$ are open. We need to show that $U\cap \left( \mathbb{R} \backslash A\right)$ is open. How can I show the statement?,,['general-topology']
91,"How to calculate $\iint_D{} (x^2 + y^2) \, dxdy$ with $D = \{(\frac{x}{2})^2 + (\frac{y}{3})^2 \leq 1\}$?",How to calculate  with ?,"\iint_D{} (x^2 + y^2) \, dxdy D = \{(\frac{x}{2})^2 + (\frac{y}{3})^2 \leq 1\}","I'm asked to calculate    $$\iint_D{} (x^2 + y^2)\, dxdy,\quad D = \left\{\left(\frac{x}{2}\right)^2 + \left(\frac{y}{3}\right)^2 \leq 1\right\}.$$ My attempt: Set  $$\frac{x}{2} = r\cos{\theta},\quad\frac{y}{3} = r\sin{\theta}$$ which nets the functional matrix i $6r$. $$\iint_D{(4r^2\cos^2{\theta} + 9r^2\sin^2{\theta})6r \, drd\theta}$$ $$\iint_D{24r^3 + 30r^3\sin^2{\theta} \, drd\theta}$$ $$\int^{2\pi}_0\int^1_0{24r^3 + 30r^3\sin^2{\theta} \,drd\theta}$$ $$\int^{2\pi}_0{6 + 10\sin^2{\theta} \, d\theta}$$ $$\left[6\theta + \frac{10}{2}(\theta - \sin{\theta}\cos{\theta})\right]^{2\pi}_0$$ which equals to $22\pi$. However, the answer is supposed to be $\frac{39\pi}{2}$. What am I doing wrong / missing?","I'm asked to calculate    $$\iint_D{} (x^2 + y^2)\, dxdy,\quad D = \left\{\left(\frac{x}{2}\right)^2 + \left(\frac{y}{3}\right)^2 \leq 1\right\}.$$ My attempt: Set  $$\frac{x}{2} = r\cos{\theta},\quad\frac{y}{3} = r\sin{\theta}$$ which nets the functional matrix i $6r$. $$\iint_D{(4r^2\cos^2{\theta} + 9r^2\sin^2{\theta})6r \, drd\theta}$$ $$\iint_D{24r^3 + 30r^3\sin^2{\theta} \, drd\theta}$$ $$\int^{2\pi}_0\int^1_0{24r^3 + 30r^3\sin^2{\theta} \,drd\theta}$$ $$\int^{2\pi}_0{6 + 10\sin^2{\theta} \, d\theta}$$ $$\left[6\theta + \frac{10}{2}(\theta - \sin{\theta}\cos{\theta})\right]^{2\pi}_0$$ which equals to $22\pi$. However, the answer is supposed to be $\frac{39\pi}{2}$. What am I doing wrong / missing?",,"['analysis', 'multivariable-calculus', 'definite-integrals']"
92,Prove that a subset of $L^1$ is closed under pointwise convergence.,Prove that a subset of  is closed under pointwise convergence.,L^1,"I'm currently studying for an analysis written qual and came across the following question: Let $m$ denote the Lebesgue measure on $\mathbb R$. Prove that the subset $A$ of $L^1(m)$ defined by $A := \{f \in L^1(m)\ :\ \int_{\mathbb R} |f|dm \leq 1\}$ is closed under pointwise convergence. I took a topological approach, using the fact that $L^1$ is a complete metric space, under the metric $\rho(f, g) = \int |f - g|dm$. As $A$ is the closed ball of radius 1 centered at $f = 0$, $A$ is also complete. That plus total boundedness implies $A$ is compact. So if $\{f_n\}_{n=1}^\infty$ is a sequence in $A$ that converges pointwise to $f$, it has a subsequence converging in $A$ which implies $f \in A$. My questions are: 1) is this solution valid? and 2) is there a more measure theoretic approach that works? My first instinct was to use Lebesgue dominated convergence but I couldn't come up with a dominating function.","I'm currently studying for an analysis written qual and came across the following question: Let $m$ denote the Lebesgue measure on $\mathbb R$. Prove that the subset $A$ of $L^1(m)$ defined by $A := \{f \in L^1(m)\ :\ \int_{\mathbb R} |f|dm \leq 1\}$ is closed under pointwise convergence. I took a topological approach, using the fact that $L^1$ is a complete metric space, under the metric $\rho(f, g) = \int |f - g|dm$. As $A$ is the closed ball of radius 1 centered at $f = 0$, $A$ is also complete. That plus total boundedness implies $A$ is compact. So if $\{f_n\}_{n=1}^\infty$ is a sequence in $A$ that converges pointwise to $f$, it has a subsequence converging in $A$ which implies $f \in A$. My questions are: 1) is this solution valid? and 2) is there a more measure theoretic approach that works? My first instinct was to use Lebesgue dominated convergence but I couldn't come up with a dominating function.",,"['real-analysis', 'analysis', 'measure-theory', 'lebesgue-integral']"
93,Show uniqueness to wave equation with finite speed $c(x).$,Show uniqueness to wave equation with finite speed,c(x).,"Suppose that $|c(x)| < C$ for all $x \in \mathbb{R},$ and that $\phi$ and $\psi$ are smooth functions.  Show that there exists at most one solution to   $$ u_{tt} - (c(x))^2 u_{xx} = -u_t$$   where $u(x,0) = \phi$ and $u_t(x,0) = \psi.$ The only way that comes to mind is energy methods, letting $w$ be the difference of two solutions to the PDE, and then letting  $$ E(t) := \frac{1}{2} \int w_t^2 + c(x)^2 w_x^2 \; dx.$$ Noting that we have $E(0) = 0,$ due to everything being linear, we can differentiate with respect to time and integrating by parts.  After doing so and using the PDE, one gets, $$ E'(t) = \int -w_t^2 - 2w_tw_xc(x)c'(x) \; dx \leq 2||c'(x)||_\infty E(t),$$ where it has been assumed that either $w_x$ and/or $w_t$ vanish as $|x| \to \infty,$ so no surface term appears when integrating by parts.  The last inequality comes from the fact that $ab \leq \frac{1}{2} (a^2 + b^2)$ and $\int -w_t^2$ is nonpositive. From here, its easy to see that $E(t) \leq 0$ for all time. The problem with this solution is, we aren't given any information about $c'(x)$ or the behavior of $w_x$ and $w_t$ as $|x| \to \infty.$  Also, no where did I use the fact that $|c|$ is bounded.  Does anyone see how to get around this, perhaps by using an entirely different method to show uniqueness?  Thanks in advance for any suggestions/proofs!","Suppose that $|c(x)| < C$ for all $x \in \mathbb{R},$ and that $\phi$ and $\psi$ are smooth functions.  Show that there exists at most one solution to   $$ u_{tt} - (c(x))^2 u_{xx} = -u_t$$   where $u(x,0) = \phi$ and $u_t(x,0) = \psi.$ The only way that comes to mind is energy methods, letting $w$ be the difference of two solutions to the PDE, and then letting  $$ E(t) := \frac{1}{2} \int w_t^2 + c(x)^2 w_x^2 \; dx.$$ Noting that we have $E(0) = 0,$ due to everything being linear, we can differentiate with respect to time and integrating by parts.  After doing so and using the PDE, one gets, $$ E'(t) = \int -w_t^2 - 2w_tw_xc(x)c'(x) \; dx \leq 2||c'(x)||_\infty E(t),$$ where it has been assumed that either $w_x$ and/or $w_t$ vanish as $|x| \to \infty,$ so no surface term appears when integrating by parts.  The last inequality comes from the fact that $ab \leq \frac{1}{2} (a^2 + b^2)$ and $\int -w_t^2$ is nonpositive. From here, its easy to see that $E(t) \leq 0$ for all time. The problem with this solution is, we aren't given any information about $c'(x)$ or the behavior of $w_x$ and $w_t$ as $|x| \to \infty.$  Also, no where did I use the fact that $|c|$ is bounded.  Does anyone see how to get around this, perhaps by using an entirely different method to show uniqueness?  Thanks in advance for any suggestions/proofs!",,"['analysis', 'partial-differential-equations', 'wave-equation']"
94,Does a continuous function map a closed set to closed set?,Does a continuous function map a closed set to closed set?,,"True or False?: If D is closed and f is continuous, then f(D) is closed. The answer is false. However, i can prove that it is true and i can't find what i did wrong. Here's my proof: Whenever $x_k$ is a sequence in D with $x_k \rightarrow x$, we have that $x \in D$ since D is closed. Also, since f is continuous on D, it follows that $f(x_k) \rightarrow f(x)$ and $f(x) \in f(D)$ because $x \in D$. which implies that f(D) is closed. Which step is wrong? question added: If D is open, then f(D) is open. true or false?","True or False?: If D is closed and f is continuous, then f(D) is closed. The answer is false. However, i can prove that it is true and i can't find what i did wrong. Here's my proof: Whenever $x_k$ is a sequence in D with $x_k \rightarrow x$, we have that $x \in D$ since D is closed. Also, since f is continuous on D, it follows that $f(x_k) \rightarrow f(x)$ and $f(x) \in f(D)$ because $x \in D$. which implies that f(D) is closed. Which step is wrong? question added: If D is open, then f(D) is open. true or false?",,"['calculus', 'analysis']"
95,Proof of multiplicative commutativity for all real numbers,Proof of multiplicative commutativity for all real numbers,,"I have seen proofs for commutativity for all integers, and these can be extended to rationals easily because a rational number is just the ratio of two integers. However, I have yet to see a proof that multiplication of real numbers is commutative. How would you prove this one?","I have seen proofs for commutativity for all integers, and these can be extended to rationals easily because a rational number is just the ratio of two integers. However, I have yet to see a proof that multiplication of real numbers is commutative. How would you prove this one?",,['analysis']
96,What is a geometric characterization of the smooth real-valued functions $f$ on an open set $U \subset \Bbb R^n$ satisfying $df \wedge dx^1 = 0$?,What is a geometric characterization of the smooth real-valued functions  on an open set  satisfying ?,f U \subset \Bbb R^n df \wedge dx^1 = 0,"Let $U \subset \Bbb R^n$ be open. What is a geometric characterization of the smooth real-valued functions $f$ on $U$ satisfying the condition $$df \wedge dx^1 = 0 ?$$ I've expanded the expression according to the definition of $df$, and am not sure my solution is correct. $f$ is defined on an arbitrary open subset $U$ of $R^n$. $$df = \sum_{i =1} ^{n} \frac{\partial f}{\partial x^i}dx^i$$ Thus: $$\begin{align}df \wedge dx^1 &= \sum_{i =1} ^{n} \frac{\partial f}{\partial x^i}dx^i \wedge dx^1\\&= \sum_{i =2} ^{n} \frac{\partial f}{\partial x^i}dx^i \wedge dx^1 .\end{align}$$ Thus for our given expression to be true, we require $\frac{\partial f}{\partial x^i}dx^i \wedge dx^1 = 0$ for all $i \neq 1$. Now, for these $i,$ we know that $dx^i \wedge dx^1 \neq 0.$ Thus these must be smooth functions that are constant along each $x^{i \neq 1}$ axis. Am I wrong anywhere?","Let $U \subset \Bbb R^n$ be open. What is a geometric characterization of the smooth real-valued functions $f$ on $U$ satisfying the condition $$df \wedge dx^1 = 0 ?$$ I've expanded the expression according to the definition of $df$, and am not sure my solution is correct. $f$ is defined on an arbitrary open subset $U$ of $R^n$. $$df = \sum_{i =1} ^{n} \frac{\partial f}{\partial x^i}dx^i$$ Thus: $$\begin{align}df \wedge dx^1 &= \sum_{i =1} ^{n} \frac{\partial f}{\partial x^i}dx^i \wedge dx^1\\&= \sum_{i =2} ^{n} \frac{\partial f}{\partial x^i}dx^i \wedge dx^1 .\end{align}$$ Thus for our given expression to be true, we require $\frac{\partial f}{\partial x^i}dx^i \wedge dx^1 = 0$ for all $i \neq 1$. Now, for these $i,$ we know that $dx^i \wedge dx^1 \neq 0.$ Thus these must be smooth functions that are constant along each $x^{i \neq 1}$ axis. Am I wrong anywhere?",,"['calculus', 'real-analysis', 'analysis', 'differential-geometry', 'differential-forms']"
97,Theoretical Advantages of Lebesgue Integration,Theoretical Advantages of Lebesgue Integration,,"This wiki article opens with the two sentences In measure theory, Lebesgue's dominated convergence theorem provides sufficient conditions under which almost everywhere convergence of a sequence of functions implies convergence in the $L_1$ norm. Its power and utility are two of the primary theoretical advantages ( my boldface ) of Lebesgue integration over Riemann integration. I thought the term ""theoretical advantages"" was interesting. I only know enough about Lebesgue integration to be able to understand terms like ""Lebesgue measurable"" when they crop up, and always thought of Lebesgue integration as a way of making a wider class of functions integrable than would be for the Riemann integral alone. Is there a generally agreed upon set of ""theoretical advantages"" like this for Lebesgue integration?","This wiki article opens with the two sentences In measure theory, Lebesgue's dominated convergence theorem provides sufficient conditions under which almost everywhere convergence of a sequence of functions implies convergence in the $L_1$ norm. Its power and utility are two of the primary theoretical advantages ( my boldface ) of Lebesgue integration over Riemann integration. I thought the term ""theoretical advantages"" was interesting. I only know enough about Lebesgue integration to be able to understand terms like ""Lebesgue measurable"" when they crop up, and always thought of Lebesgue integration as a way of making a wider class of functions integrable than would be for the Riemann integral alone. Is there a generally agreed upon set of ""theoretical advantages"" like this for Lebesgue integration?",,"['real-analysis', 'integration', 'analysis', 'measure-theory', 'lebesgue-integral']"
98,Epsilon-delta proof that $ \lim_{x\to 0} {1\over x^2}$ does not exist,Epsilon-delta proof that  does not exist, \lim_{x\to 0} {1\over x^2},"I'd like to see an epsilon delta proof that the $\lim: \lim_{x\to 0} {1\over x^2}$ does not exist and an explanation of the exact reason it does not exist, because I am not so sure I believe that the limit does not, in fact, exist, so I need to be proved wrong. What is the relationship between a limit existing, and the function in question having a least upper bound?  Because it seems to me that the only explanation I can find as to why the limit does not exist is that the function is unbounded. I'm not sure why this is relevant because it seems to me that when $x$ approaches $0$ then ${1\over x^2}$ gets infinitely close to the y-axis which suggests to me that there does exist, in fact, an epsilon infinitely close to zero such that if $|x - a| < \delta$ then $|f(x)-L| < \epsilon$ where $\delta$ is infinitely close to zero and $\epsilon$ is infinitely close to zero. Obviously, my understanding of calculus hinges on this question, so I really need to be convinced with a bulletproof explanation, otherwise I'll continue to doubt the truth (I don't believe anything unless I fully understand it myself, for better or worse, I ignore other's authority and rely only on proof and logical understanding -- I'm sorry if this attitude offends anyone)!  Thanks in advance!","I'd like to see an epsilon delta proof that the $\lim: \lim_{x\to 0} {1\over x^2}$ does not exist and an explanation of the exact reason it does not exist, because I am not so sure I believe that the limit does not, in fact, exist, so I need to be proved wrong. What is the relationship between a limit existing, and the function in question having a least upper bound?  Because it seems to me that the only explanation I can find as to why the limit does not exist is that the function is unbounded. I'm not sure why this is relevant because it seems to me that when $x$ approaches $0$ then ${1\over x^2}$ gets infinitely close to the y-axis which suggests to me that there does exist, in fact, an epsilon infinitely close to zero such that if $|x - a| < \delta$ then $|f(x)-L| < \epsilon$ where $\delta$ is infinitely close to zero and $\epsilon$ is infinitely close to zero. Obviously, my understanding of calculus hinges on this question, so I really need to be convinced with a bulletproof explanation, otherwise I'll continue to doubt the truth (I don't believe anything unless I fully understand it myself, for better or worse, I ignore other's authority and rely only on proof and logical understanding -- I'm sorry if this attitude offends anyone)!  Thanks in advance!",,"['calculus', 'analysis', 'infinity']"
99,"Prob. 4, Chap. 4 in Baby Rudin: A continuous image of a dense subset is dense in the range.","Prob. 4, Chap. 4 in Baby Rudin: A continuous image of a dense subset is dense in the range.",,"Here is Prob. 4, Chap. 4 in the book Principles of Mathematical Analysis by Walter Rudin, 3rd edition: Let $f$ and $g$ be continuous mappings of a metric space $X$ into a metric space $Y$, and let $E$ be a dense subset of $X$. Prove that $f(E)$ is dense in $f(X)$. If $g(p) = f(p)$ for all $p \in E$, prove that $g(p) = f(p)$ for all $p \in X$. (In other words, a continuous mapping is determined by its values on a dense subset of its domain.) I think I can prove both of these two facts. Now my question is, is either of the above two results still valid if $X$ and / or $Y$ be replaced by general topological spaces? My effort: Suppose $X$ and $Y$ are topological spaces, $E$ is a dense subset of $X$, and $f$ is a continuous mapping of $X$ into $Y$. Let $q$ be any point of $f(X)$. Then this point $q = f(p)$ for some point $p$ of $X$. Let $V$ be any open set in $Y$ containing $q$. Then $f^{-1}(V)$ is an open set in $X$ containing $p$. So there is a point $a \in E$ such that $a \in f^{-1}(V)$, which implies that $f(a) \in f(E) \cap V$, from which it follows that $f(E)$ is dense in $f(X)$. Am I right? Now for the second result: Suppose $X$ and $Y$ are topological spaces, $E$ is a dense subset of $X$, $f$ and $g$ are continuous mappings of $X$ into $Y$, and $g(x) = f(x)$ for all $x \in E$. Let $p \in X$. What next?","Here is Prob. 4, Chap. 4 in the book Principles of Mathematical Analysis by Walter Rudin, 3rd edition: Let $f$ and $g$ be continuous mappings of a metric space $X$ into a metric space $Y$, and let $E$ be a dense subset of $X$. Prove that $f(E)$ is dense in $f(X)$. If $g(p) = f(p)$ for all $p \in E$, prove that $g(p) = f(p)$ for all $p \in X$. (In other words, a continuous mapping is determined by its values on a dense subset of its domain.) I think I can prove both of these two facts. Now my question is, is either of the above two results still valid if $X$ and / or $Y$ be replaced by general topological spaces? My effort: Suppose $X$ and $Y$ are topological spaces, $E$ is a dense subset of $X$, and $f$ is a continuous mapping of $X$ into $Y$. Let $q$ be any point of $f(X)$. Then this point $q = f(p)$ for some point $p$ of $X$. Let $V$ be any open set in $Y$ containing $q$. Then $f^{-1}(V)$ is an open set in $X$ containing $p$. So there is a point $a \in E$ such that $a \in f^{-1}(V)$, which implies that $f(a) \in f(E) \cap V$, from which it follows that $f(E)$ is dense in $f(X)$. Am I right? Now for the second result: Suppose $X$ and $Y$ are topological spaces, $E$ is a dense subset of $X$, $f$ and $g$ are continuous mappings of $X$ into $Y$, and $g(x) = f(x)$ for all $x \in E$. Let $p \in X$. What next?",,"['real-analysis', 'general-topology', 'analysis', 'metric-spaces', 'continuity']"
