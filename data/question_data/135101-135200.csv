,title_raw,title_text,title_latex,body_raw,body_text,body_latex,tags
0,"Inverse Laplace transform of $\operatorname{arccot}(s)$, $\arctan(s)$","Inverse Laplace transform of ,",\operatorname{arccot}(s) \arctan(s),How would one find inverse Laplace transforms of  $\operatorname{arccot}(s)$ or of  $\arctan(s)$  without knowing in advance that this is related to  $\dfrac{\sin x}{x}$?,How would one find inverse Laplace transforms of  $\operatorname{arccot}(s)$ or of  $\arctan(s)$  without knowing in advance that this is related to  $\dfrac{\sin x}{x}$?,,"['calculus', 'ordinary-differential-equations', 'inverse', 'laplace-transform']"
1,Laplace Transforms of Step Functions,Laplace Transforms of Step Functions,,"The problem asks to find the Laplace transform of the given function: $$ f(t) = \begin{cases} 0,  & t<2 \\ (t-2)^2, & t \ge 2 \end{cases} $$ Here's how I worked out the solution: $$\mathcal{L}[f(t)]=\mathcal{L}[0]+\mathcal{L}[u_2(t)(t-2)^2]=0+e^{-2s}\mathcal{L}[(t-2)^2]=e^{-2s}\mathcal{L}[t^2-4t+4]=2e^{-2s}\left(\frac{1}{s^3}-\frac{2}{s^2}+\frac{2}{s} \right)$$ However, the solution in the back of the book is simply: $ 2e^{-s}s^{-3} $ What did I do wrong?","The problem asks to find the Laplace transform of the given function: $$ f(t) = \begin{cases} 0,  & t<2 \\ (t-2)^2, & t \ge 2 \end{cases} $$ Here's how I worked out the solution: $$\mathcal{L}[f(t)]=\mathcal{L}[0]+\mathcal{L}[u_2(t)(t-2)^2]=0+e^{-2s}\mathcal{L}[(t-2)^2]=e^{-2s}\mathcal{L}[t^2-4t+4]=2e^{-2s}\left(\frac{1}{s^3}-\frac{2}{s^2}+\frac{2}{s} \right)$$ However, the solution in the back of the book is simply: $ 2e^{-s}s^{-3} $ What did I do wrong?",,"['calculus', 'ordinary-differential-equations', 'laplace-transform']"
2,"Solve the differential equation. $\frac{dy}{dx} + 2y = f(x),$ where $f(x) = 1,$ if $ 0 \leq x \leq 1;$ $ f(x) = 0, x > 1, y(0) = 0.$",Solve the differential equation.  where  if,"\frac{dy}{dx} + 2y = f(x), f(x) = 1,  0 \leq x \leq 1;  f(x) = 0, x > 1, y(0) = 0.","Solve the differential equation. $\frac{dy}{dx} + 2y = f(x),$ where $f(x) = 1,$   if $ 0 \leq x \leq 1;$ $ f(x) = 0, x > 1, y(0) = 0.$ Find $f(\frac{3}{2}).$ I am confused whether to use the concept as follows: An integrating factor is $\mathrm e^{\int 2 dx}.$ Then after multiplication, it makes the original equation exact. Is it okay? Please tell. Thanks in advance...","Solve the differential equation. $\frac{dy}{dx} + 2y = f(x),$ where $f(x) = 1,$   if $ 0 \leq x \leq 1;$ $ f(x) = 0, x > 1, y(0) = 0.$ Find $f(\frac{3}{2}).$ I am confused whether to use the concept as follows: An integrating factor is $\mathrm e^{\int 2 dx}.$ Then after multiplication, it makes the original equation exact. Is it okay? Please tell. Thanks in advance...",,['ordinary-differential-equations']
3,Get the known Laplace's equation,Get the known Laplace's equation,,"Let $u(x,y), x^2+y^2 \leq 1$, a solution of $$u_{xx}(x,y)+2u_{yy}(x,y)+e^{u(x,y)}=0, x^2+y^2\leq 1$$ Show that $\min_{x^2+y^2 \leq 1} u(x,y)= \min_{x^2+y^2=1} u(x,y) $. We suppose that $\min_{x^2+y^2 \leq 1} u(x,y) \neq  \min_{x^2+y^2=1} u(x,y) $. How can we find a function $U(X,Y)$ so that from $u_{xx}(x,y)+2u_{yy}(x,y)+e^{u(x,y)}=0$ we get the known Laplace's equation ?","Let $u(x,y), x^2+y^2 \leq 1$, a solution of $$u_{xx}(x,y)+2u_{yy}(x,y)+e^{u(x,y)}=0, x^2+y^2\leq 1$$ Show that $\min_{x^2+y^2 \leq 1} u(x,y)= \min_{x^2+y^2=1} u(x,y) $. We suppose that $\min_{x^2+y^2 \leq 1} u(x,y) \neq  \min_{x^2+y^2=1} u(x,y) $. How can we find a function $U(X,Y)$ so that from $u_{xx}(x,y)+2u_{yy}(x,y)+e^{u(x,y)}=0$ we get the known Laplace's equation ?",,"['ordinary-differential-equations', 'transformation']"
4,How to determine generalized eigenvectors of $\begin {bmatrix} 2 & 1 & 0 & 0 \\ 0 & 2 & 0 & 0 \\ 0 & 0 & 2 &1 \\ 0 & 0 & 0 & 2 \end{bmatrix}$,How to determine generalized eigenvectors of,\begin {bmatrix} 2 & 1 & 0 & 0 \\ 0 & 2 & 0 & 0 \\ 0 & 0 & 2 &1 \\ 0 & 0 & 0 & 2 \end{bmatrix},"I want to calculate the general solution of this DE-system: $$ \frac{d \vec x}{d t}= A \vec x,\text{ with }A = \begin {bmatrix} 2 & 1 & 0 & 0 \\ 0 & 2 & 0 & 0 \\ 0 & 0 & 2 &1 \\ 0 & 0 & 0 & 2 \end{bmatrix}$$ $\lambda=2$ is eigenvalue with algebraic multiplicity $4$. Calculating Eigenvectors: $$\begin {bmatrix} 2-2 & 1 & 0 & 0 \\ 0 & 2-2 & 0 & 0 \\ 0 & 0 & 2-2 &1 \\ 0 & 0 & 0 & 2-2 \end{bmatrix} = \begin {bmatrix} 0 & 1 & 0 & 0 \\ 0 & 0 & 0 & 0 \\ 0 & 0 & 0 &1 \\ 0 & 0 & 0 & 0 \end{bmatrix}$$ So eigenvectors are obviously $$\vec v_1 = \begin {bmatrix} 1  \\ 0 \\ 0 \\ 0 \end{bmatrix}\text{ and }\vec v_2= \begin {bmatrix} 0  \\ 0 \\ 1 \\ 0 \end{bmatrix}.$$ My question is, how to calculate generalized eigenvectors $\vec v_3$ and $\vec v_4$  in this case. Would it be correct, to solve the following to two linear system? (1) $(A-\lambda I)\vec v_3 = \vec v_1$ (2) $(A-\lambda I)\vec v_4 = \vec v_2$","I want to calculate the general solution of this DE-system: $$ \frac{d \vec x}{d t}= A \vec x,\text{ with }A = \begin {bmatrix} 2 & 1 & 0 & 0 \\ 0 & 2 & 0 & 0 \\ 0 & 0 & 2 &1 \\ 0 & 0 & 0 & 2 \end{bmatrix}$$ $\lambda=2$ is eigenvalue with algebraic multiplicity $4$. Calculating Eigenvectors: $$\begin {bmatrix} 2-2 & 1 & 0 & 0 \\ 0 & 2-2 & 0 & 0 \\ 0 & 0 & 2-2 &1 \\ 0 & 0 & 0 & 2-2 \end{bmatrix} = \begin {bmatrix} 0 & 1 & 0 & 0 \\ 0 & 0 & 0 & 0 \\ 0 & 0 & 0 &1 \\ 0 & 0 & 0 & 0 \end{bmatrix}$$ So eigenvectors are obviously $$\vec v_1 = \begin {bmatrix} 1  \\ 0 \\ 0 \\ 0 \end{bmatrix}\text{ and }\vec v_2= \begin {bmatrix} 0  \\ 0 \\ 1 \\ 0 \end{bmatrix}.$$ My question is, how to calculate generalized eigenvectors $\vec v_3$ and $\vec v_4$  in this case. Would it be correct, to solve the following to two linear system? (1) $(A-\lambda I)\vec v_3 = \vec v_1$ (2) $(A-\lambda I)\vec v_4 = \vec v_2$",,"['ordinary-differential-equations', 'eigenvalues-eigenvectors']"
5,Problems with a Simple Differential Equation,Problems with a Simple Differential Equation,,"I am trying to solve the following: $y' = (y-5)(y+5)$ if $y(4) = 0$. So far, I have tried separating the variables and then use partial fractions and have followed these steps: (1) $A(y+5) + B (y-5) =1$. (2) For $y = -5$ we have $B = \dfrac{-1}{10}$. (3) For $ y= 5$, we have $A = \dfrac{1}{10}$. (4) so we have $\dfrac{1}{10}[ \ln|y^2 - 25|] = x + C$ (5) After a chain of algebraic operations, we have $y = \pm \sqrt{e^{10x + 10C} + 25}$. However, if I plug in my initial condition, then I get: $0 = e^{40 + 10C} + 25$ which implies $\ln(-25) = 40 + C$ which gets into complex numbers. Any help anyone can provide on where my reasoning went wrong is appreciated.","I am trying to solve the following: $y' = (y-5)(y+5)$ if $y(4) = 0$. So far, I have tried separating the variables and then use partial fractions and have followed these steps: (1) $A(y+5) + B (y-5) =1$. (2) For $y = -5$ we have $B = \dfrac{-1}{10}$. (3) For $ y= 5$, we have $A = \dfrac{1}{10}$. (4) so we have $\dfrac{1}{10}[ \ln|y^2 - 25|] = x + C$ (5) After a chain of algebraic operations, we have $y = \pm \sqrt{e^{10x + 10C} + 25}$. However, if I plug in my initial condition, then I get: $0 = e^{40 + 10C} + 25$ which implies $\ln(-25) = 40 + C$ which gets into complex numbers. Any help anyone can provide on where my reasoning went wrong is appreciated.",,['ordinary-differential-equations']
6,Advice on second order non-linear ordinary differential equation,Advice on second order non-linear ordinary differential equation,,I'm currently working on some problems concerning the calculus of variations and I have come up with the following differential equation that I now want to solve: $$1 + y'(x)^2 - y''(x)(y(x)-\lambda) = 0.$$ I'm only used to solving linear differential equations so I would appreciate some advice on where to start.,I'm currently working on some problems concerning the calculus of variations and I have come up with the following differential equation that I now want to solve: $$1 + y'(x)^2 - y''(x)(y(x)-\lambda) = 0.$$ I'm only used to solving linear differential equations so I would appreciate some advice on where to start.,,"['ordinary-differential-equations', 'calculus-of-variations']"
7,2nd order h ODE with non-constant coefficient,2nd order h ODE with non-constant coefficient,,"I have $$0 = F''(x) + p(x) F'(x) + cF(x)\\ p(x) = ab(1-x)$$ where $a$, $b$, $c$ are non-zero constants. I'm not very strong in the theories of 2nd order ODE, so I google'ed some solution methods. Turns out that there's a lot for constant coefficients, but many standard/introductory textbooks completely skip solution methods for the part with non-constant coefficients. I am aware of the principle of superposition, but in order to use that, I first need to find two independent solutions. How can I proceed here (or in general?)","I have $$0 = F''(x) + p(x) F'(x) + cF(x)\\ p(x) = ab(1-x)$$ where $a$, $b$, $c$ are non-zero constants. I'm not very strong in the theories of 2nd order ODE, so I google'ed some solution methods. Turns out that there's a lot for constant coefficients, but many standard/introductory textbooks completely skip solution methods for the part with non-constant coefficients. I am aware of the principle of superposition, but in order to use that, I first need to find two independent solutions. How can I proceed here (or in general?)",,['ordinary-differential-equations']
8,How to solve the differential equation $(y^2-1)+2(x-y(1+y)^2)y'=0$?,How to solve the differential equation ?,(y^2-1)+2(x-y(1+y)^2)y'=0,I have to find the solution to the differential equation $$(y^2-1)+2(x-y(1+y)^2)y'=0$$ So far I've only learned how to solve equations of the form $$y'+p(x)y=q(x)$$ And second order equations with constant coefficients. This equation that I have to solve right now does not seem to be of any of these $2$ forms so I don't understand how I can solve this. I know I'm supposed to show some attempted solutions I tried but frankly I just have no idea how to even start here.,I have to find the solution to the differential equation $$(y^2-1)+2(x-y(1+y)^2)y'=0$$ So far I've only learned how to solve equations of the form $$y'+p(x)y=q(x)$$ And second order equations with constant coefficients. This equation that I have to solve right now does not seem to be of any of these $2$ forms so I don't understand how I can solve this. I know I'm supposed to show some attempted solutions I tried but frankly I just have no idea how to even start here.,,['ordinary-differential-equations']
9,Dynamical System transformation,Dynamical System transformation,,"How can the system $$\frac{dx}{dt}=-y+\epsilon x(x^2+y^2)$$$$\frac{dy}{ dt}=x+\epsilon y(x^2+y^2)$$ be transformed into $$\frac{dr}{dt}=\epsilon r^3$$ $$\frac{d\theta}{dt}=1$$ via polar coordinates? I sub in $x=r\cos(\theta)$ and $y=r\sin(\theta)$ into both of the original equations, but there are $\frac{dr}{dt}$ and $\frac{d\theta}{dt}$ terms in both equations then. And adding/subtracting the equations doesn't seem to produce anything 'nice'.","How can the system $$\frac{dx}{dt}=-y+\epsilon x(x^2+y^2)$$$$\frac{dy}{ dt}=x+\epsilon y(x^2+y^2)$$ be transformed into $$\frac{dr}{dt}=\epsilon r^3$$ $$\frac{d\theta}{dt}=1$$ via polar coordinates? I sub in $x=r\cos(\theta)$ and $y=r\sin(\theta)$ into both of the original equations, but there are $\frac{dr}{dt}$ and $\frac{d\theta}{dt}$ terms in both equations then. And adding/subtracting the equations doesn't seem to produce anything 'nice'.",,"['ordinary-differential-equations', 'dynamical-systems', 'transformation', 'polar-coordinates']"
10,Conditions for Deriving $R_0$ for SIR Model Using Survival Function Method,Conditions for Deriving  for SIR Model Using Survival Function Method,R_0,"I'm taking a look at the SIR model given by the system of differential equations \begin{align} \frac{dS}{dt} & = -  \beta S I \\ \frac{dI}{dt} & =  \beta S I - \gamma I \\ \frac{dR}{dt}& = \gamma I \end{align} where $S$, $I$, and $R$ represent the number of susceptible, infectious, and removed individuals in a population, and we have that $S + I + R = 1$. I am trying to justify the derivation of $R_0$, the basic reproductive number, as being equal to $\frac{\beta}{\gamma}$, using its definition as the expected number of secondary infections caused by a single index case in a population where all other individuals are susceptible. To do this, I am trying to apply the survival function method outlined on page two of this article http://mysite.science.uottawa.ca/rsmith43/R0Review.pdf . This method, as far as I understand it, computes the value of $R_0$ as $$R_0 = \displaystyle\int_0^{\infty} F(\tau) b(\tau) d\tau$$ where $F(\tau)$ is the probability that the initially infectious individual is still infectious at time $\tau$, and $b(\tau)$ is the rate at which this infectious individual infects susceptibles at time $\tau$. My first guess for an approach is based on the fact that the initially infectious individual will deterministically remain infectious for $\frac{1}{\gamma}$ time in the SIR model. I would take $$F(\tau)\ = \left\{      \begin{array}{ll}        1 & : \tau <= \frac{1}{\gamma}\\        0 & : \tau > \frac{1}{\gamma}      \end{array}    \right. $$ And then to interpret $b(\tau) = \beta$ for all values of $\tau$, which will give us $$R_0 = \displaystyle\int_0^{\infty} \beta F(\tau) d\tau = \displaystyle\int_0^{\frac{1}{\gamma}} \beta d\tau = \frac{\beta}{\gamma}$$ This produces the desired result, but I feel that one step might be unjustified. When I assume that $b(\tau) = \beta$, I feel like I am implicitly assuming that, for the entire infectious period of the index case, the proportion of individuals infected by that index case have almost no impact on the availability of susceptibles to be infected. I guess I feel that I would like conditions on $\beta$, $\gamma$, and the population size such that the primary index case does not infect a substantial enough fraction of the population to slow down its own infection rate. For example, if $\beta$ was very large, it could be possible for the index case to have already depleted a large portion of the susceptibles before exiting its own infectious period. Do you guys know of any such conditions? Additionally, is it overkill to assume that such an issue could be possible in a somewhat realistic epidemic process on a large population?","I'm taking a look at the SIR model given by the system of differential equations \begin{align} \frac{dS}{dt} & = -  \beta S I \\ \frac{dI}{dt} & =  \beta S I - \gamma I \\ \frac{dR}{dt}& = \gamma I \end{align} where $S$, $I$, and $R$ represent the number of susceptible, infectious, and removed individuals in a population, and we have that $S + I + R = 1$. I am trying to justify the derivation of $R_0$, the basic reproductive number, as being equal to $\frac{\beta}{\gamma}$, using its definition as the expected number of secondary infections caused by a single index case in a population where all other individuals are susceptible. To do this, I am trying to apply the survival function method outlined on page two of this article http://mysite.science.uottawa.ca/rsmith43/R0Review.pdf . This method, as far as I understand it, computes the value of $R_0$ as $$R_0 = \displaystyle\int_0^{\infty} F(\tau) b(\tau) d\tau$$ where $F(\tau)$ is the probability that the initially infectious individual is still infectious at time $\tau$, and $b(\tau)$ is the rate at which this infectious individual infects susceptibles at time $\tau$. My first guess for an approach is based on the fact that the initially infectious individual will deterministically remain infectious for $\frac{1}{\gamma}$ time in the SIR model. I would take $$F(\tau)\ = \left\{      \begin{array}{ll}        1 & : \tau <= \frac{1}{\gamma}\\        0 & : \tau > \frac{1}{\gamma}      \end{array}    \right. $$ And then to interpret $b(\tau) = \beta$ for all values of $\tau$, which will give us $$R_0 = \displaystyle\int_0^{\infty} \beta F(\tau) d\tau = \displaystyle\int_0^{\frac{1}{\gamma}} \beta d\tau = \frac{\beta}{\gamma}$$ This produces the desired result, but I feel that one step might be unjustified. When I assume that $b(\tau) = \beta$, I feel like I am implicitly assuming that, for the entire infectious period of the index case, the proportion of individuals infected by that index case have almost no impact on the availability of susceptibles to be infected. I guess I feel that I would like conditions on $\beta$, $\gamma$, and the population size such that the primary index case does not infect a substantial enough fraction of the population to slow down its own infection rate. For example, if $\beta$ was very large, it could be possible for the index case to have already depleted a large portion of the susceptibles before exiting its own infectious period. Do you guys know of any such conditions? Additionally, is it overkill to assume that such an issue could be possible in a somewhat realistic epidemic process on a large population?",,"['ordinary-differential-equations', 'dynamical-systems', 'biology']"
11,Lyapunov functions and basins of attraction,Lyapunov functions and basins of attraction,,"\begin{align}     x' &= -x^3 + x^5 + (x^4)(y^5)\\[.7em]     y' &= -8y^3 + y^5 - 10(y^4)(x^5) \end{align} $(0,0)$ is obviously a critical point of the system, and we are given that it is asymptotically stable, but have to show it. I have tried to make a Lyapunov function $V(x,y) = ax^2 + cy^2$ , with $a,c > 0$ but I am having trouble to prove that $\frac{d}{dt} V(x,y)$ is negative definite. I get some complicated polynomial I can't use logic to finalize. How can I change the Lyapunov to come up with a meaningful conclusion? \begin{align}     \frac{d}{dt}V(x,y) = 2ax(-3x^2 + 5x^4 + 4x^3y^5) + 2cy(-24y^2+5y^4-40y^3x^5)\\[.7em] \end{align}","is obviously a critical point of the system, and we are given that it is asymptotically stable, but have to show it. I have tried to make a Lyapunov function , with but I am having trouble to prove that is negative definite. I get some complicated polynomial I can't use logic to finalize. How can I change the Lyapunov to come up with a meaningful conclusion?","\begin{align}
    x' &= -x^3 + x^5 + (x^4)(y^5)\\[.7em]
    y' &= -8y^3 + y^5 - 10(y^4)(x^5)
\end{align} (0,0) V(x,y) = ax^2 + cy^2 a,c > 0 \frac{d}{dt} V(x,y) \begin{align}
    \frac{d}{dt}V(x,y) = 2ax(-3x^2 + 5x^4 + 4x^3y^5) + 2cy(-24y^2+5y^4-40y^3x^5)\\[.7em]
\end{align}","['ordinary-differential-equations', 'stability-in-odes', 'lyapunov-functions']"
12,Write the piecewise function in terms of unit step functions,Write the piecewise function in terms of unit step functions,,"Write the piecewise function $f(t) = \begin{cases}        2t, &   0\leq t < 3 \\       6,  &   3 \le t < 5 \\       2t, &   t \ge 5 \\    \end{cases}  $ in terms of unit step functions. So here is what i;ve got just guessing , I don't think i'm correct. I really need some help. But I got: $f(t) = 2t[u(t-0) - u(t-3)] + 6[u(t-3) - u(t-5)] + 2t[u(t-5) - u(t - \infty)]$ Which becomes $f(t) = 2t[u(t) - u(t-3)] + 6[u(t-3) - u(t-5)] + 2t[u(t-5)]$","Write the piecewise function $f(t) = \begin{cases}        2t, &   0\leq t < 3 \\       6,  &   3 \le t < 5 \\       2t, &   t \ge 5 \\    \end{cases}  $ in terms of unit step functions. So here is what i;ve got just guessing , I don't think i'm correct. I really need some help. But I got: $f(t) = 2t[u(t-0) - u(t-3)] + 6[u(t-3) - u(t-5)] + 2t[u(t-5) - u(t - \infty)]$ Which becomes $f(t) = 2t[u(t) - u(t-3)] + 6[u(t-3) - u(t-5)] + 2t[u(t-5)]$",,['ordinary-differential-equations']
13,"Solving a linear, inhomogeneous, ordinary differential equation with constant coefficients","Solving a linear, inhomogeneous, ordinary differential equation with constant coefficients",,"How to solve this? $\newcommand{\d}[0]{{\rm d}}$ $$a\frac{\d^2y}{\d x^2}+b\frac{\d y}{\d x}+cy=d\\ a,b,c\in \mathbb R;\qquad y\equiv y(x);\qquad d\equiv d(x)$$ I know how to solve when $d=0$ by assuming a solution $y=e^{\alpha x}$ and then finding $\alpha$, for simplicity you can let $f(x)=ax^2+bx+c$ and $f(a_1)=f(a_2)=0$ where $a_1,a_2$ be $f$'s two roots.Therefore in the case of $d=0$, $y=c_1e^{a_1x}+c_2e^{a_2x}$ Actually I'm trying to solve: (from Alternating Current(AC) through a Capacitor, Resistor and Inductor) $$L\frac{\d^2q}{\d t^2}+R\frac{\d q}{\d t}+\frac qc=v_0\sin \omega t$$ and: (from Simple Harmonic Motion with a Spring, Damping and External agent's force) $$ m\frac{\d^2x}{\d t^2} +b \frac{\d x}{\d t}+k x =F_0\cos \omega_dt$$ Where c:Capacitance, R:Resistance, L:Inductance, $v_0$:Maximum Voltage, $\omega$:AC frequency  and t:time. Also m:mass, b:damping constant, k:spring constant, $F_0$:maximum external force, $\omega_d$:external agent's frequency and t:time. The analogy is in $L\leftrightarrow m,R\leftrightarrow b, 1/c\leftrightarrow k, F_0\leftrightarrow v_0,\omega\leftrightarrow \omega_d$. Also some useful shorthand notations are: $$X_L=L\omega\qquad X_c=\frac1{c\omega}\qquad Z=\sqrt{R^2+(X_c-X_L)^2}\qquad \phi=\arctan\frac{X_c-X_L}R$$ My book uses the assumption $q=q_0\sin(\omega t+\theta)$ but I clearly see it's just working backwards from the result. I wonder how physicists arrived at the result for the first time? Mysteriously another method involves vectors/phasers/complex numbers and Pythagoreas theorem!","How to solve this? $\newcommand{\d}[0]{{\rm d}}$ $$a\frac{\d^2y}{\d x^2}+b\frac{\d y}{\d x}+cy=d\\ a,b,c\in \mathbb R;\qquad y\equiv y(x);\qquad d\equiv d(x)$$ I know how to solve when $d=0$ by assuming a solution $y=e^{\alpha x}$ and then finding $\alpha$, for simplicity you can let $f(x)=ax^2+bx+c$ and $f(a_1)=f(a_2)=0$ where $a_1,a_2$ be $f$'s two roots.Therefore in the case of $d=0$, $y=c_1e^{a_1x}+c_2e^{a_2x}$ Actually I'm trying to solve: (from Alternating Current(AC) through a Capacitor, Resistor and Inductor) $$L\frac{\d^2q}{\d t^2}+R\frac{\d q}{\d t}+\frac qc=v_0\sin \omega t$$ and: (from Simple Harmonic Motion with a Spring, Damping and External agent's force) $$ m\frac{\d^2x}{\d t^2} +b \frac{\d x}{\d t}+k x =F_0\cos \omega_dt$$ Where c:Capacitance, R:Resistance, L:Inductance, $v_0$:Maximum Voltage, $\omega$:AC frequency  and t:time. Also m:mass, b:damping constant, k:spring constant, $F_0$:maximum external force, $\omega_d$:external agent's frequency and t:time. The analogy is in $L\leftrightarrow m,R\leftrightarrow b, 1/c\leftrightarrow k, F_0\leftrightarrow v_0,\omega\leftrightarrow \omega_d$. Also some useful shorthand notations are: $$X_L=L\omega\qquad X_c=\frac1{c\omega}\qquad Z=\sqrt{R^2+(X_c-X_L)^2}\qquad \phi=\arctan\frac{X_c-X_L}R$$ My book uses the assumption $q=q_0\sin(\omega t+\theta)$ but I clearly see it's just working backwards from the result. I wonder how physicists arrived at the result for the first time? Mysteriously another method involves vectors/phasers/complex numbers and Pythagoreas theorem!",,['ordinary-differential-equations']
14,How to solve $x'=x^2$?,How to solve ?,x'=x^2,"I need this in order to solve a PDE. How does one solve $X'(s)=X(s)^2?$ where $X(0)=x_0$. Does it involve cosines/sin? I tried use to $e$, but I don't think it works.","I need this in order to solve a PDE. How does one solve $X'(s)=X(s)^2?$ where $X(0)=x_0$. Does it involve cosines/sin? I tried use to $e$, but I don't think it works.",,['ordinary-differential-equations']
15,Suppose that the Wronskian of any 2 solutions of $\frac{d^2y}{dt^2}+p(t)\frac{dy}{dt}+q(t)y=0$ Prove that P(t)=0.,Suppose that the Wronskian of any 2 solutions of  Prove that P(t)=0.,\frac{d^2y}{dt^2}+p(t)\frac{dy}{dt}+q(t)y=0,"Suppose that the Wronskian of any 2 solutions is constant of $\frac{d^2y}{dt^2}+p(t)\frac{dy}{dt}+q(t)y=0$ Prove that P(t)=0. So my attempt: $$W(t)=y_1y_2'-y_1'y_2$$ So what I thought I would do is set W(t) = C, some constant: $$C=y_1y_2'-y_1'y_2$$ $\frac{C+y_1'y_2}{y_2'}=y_1$ and ${y_1y_2'-C}{y_2}=y_1'$ Then I thought I can substitute these into the original equation and somehow prove p(t)=0 $\frac{d^2y}{dt^2}+p(t)[{y_1y_2'-C}{y_2}]+q(t)\frac{C+y_1'y_2}{y_2'}=0$ I don't think this is quite right though...any thoughts?","Suppose that the Wronskian of any 2 solutions is constant of $\frac{d^2y}{dt^2}+p(t)\frac{dy}{dt}+q(t)y=0$ Prove that P(t)=0. So my attempt: $$W(t)=y_1y_2'-y_1'y_2$$ So what I thought I would do is set W(t) = C, some constant: $$C=y_1y_2'-y_1'y_2$$ $\frac{C+y_1'y_2}{y_2'}=y_1$ and ${y_1y_2'-C}{y_2}=y_1'$ Then I thought I can substitute these into the original equation and somehow prove p(t)=0 $\frac{d^2y}{dt^2}+p(t)[{y_1y_2'-C}{y_2}]+q(t)\frac{C+y_1'y_2}{y_2'}=0$ I don't think this is quite right though...any thoughts?",,"['ordinary-differential-equations', 'wronskian']"
16,General solution of partial differential equations.,General solution of partial differential equations.,,"Find the general solution of the given differential equation $$ \frac{y^2}{2}-2ye^t+(y-e^t)\frac{dy}{dt}=0$$ So here is my process: Let: $$\frac{\partial \phi}{\partial t}=\frac{y^2}{2}-2ye^t$$ where: $$ \phi(y,t)=\frac{y^2}{2}t-2ye^t+h(y)$$ and let : $$\frac{\partial \phi}{\partial y}=y-e^t$$ where:  $$ \phi(y,t)=\frac{y^2}{2}-ye^t+k(t)$$ Now I need to find k(t) and h(y) to make the two $\phi(y,t)$ equal. so this is where i'm stuck, if anyone can help, it be most appreciated.","Find the general solution of the given differential equation $$ \frac{y^2}{2}-2ye^t+(y-e^t)\frac{dy}{dt}=0$$ So here is my process: Let: $$\frac{\partial \phi}{\partial t}=\frac{y^2}{2}-2ye^t$$ where: $$ \phi(y,t)=\frac{y^2}{2}t-2ye^t+h(y)$$ and let : $$\frac{\partial \phi}{\partial y}=y-e^t$$ where:  $$ \phi(y,t)=\frac{y^2}{2}-ye^t+k(t)$$ Now I need to find k(t) and h(y) to make the two $\phi(y,t)$ equal. so this is where i'm stuck, if anyone can help, it be most appreciated.",,"['calculus', 'integration', 'ordinary-differential-equations']"
17,To find an extremal of the functional $\int_0^1 [(y')^2 + 12 xy] dx$,To find an extremal of the functional,\int_0^1 [(y')^2 + 12 xy] dx,"I have to find extremal of following : $\int_0^1 [(y')^2 + 12 xy] dx$ with $y(0) = 0$ and $y(1) = 1$. I applied the Euler's equation $\frac{\partial F}{\partial y} - \frac{d}{dx}(\frac{\partial F}{\partial y'}) = 0$ and I got $6x - y'' = 0$ and after solving this I get a solution $y = x^3 +cx + d$. Now applying the condition $y(0) = 0$ gives $d=0$ and thus $y = x^3 +cx$. Again applying the condition $y(1) = 1$, I get  $c=0$ which finally yields the answer $y=x^3$. Is my solution right? As I just started this topic, I am not sure about my answer. Thanks for giving time.","I have to find extremal of following : $\int_0^1 [(y')^2 + 12 xy] dx$ with $y(0) = 0$ and $y(1) = 1$. I applied the Euler's equation $\frac{\partial F}{\partial y} - \frac{d}{dx}(\frac{\partial F}{\partial y'}) = 0$ and I got $6x - y'' = 0$ and after solving this I get a solution $y = x^3 +cx + d$. Now applying the condition $y(0) = 0$ gives $d=0$ and thus $y = x^3 +cx$. Again applying the condition $y(1) = 1$, I get  $c=0$ which finally yields the answer $y=x^3$. Is my solution right? As I just started this topic, I am not sure about my answer. Thanks for giving time.",,"['calculus', 'ordinary-differential-equations', 'partial-derivative', 'functional-equations', 'calculus-of-variations']"
18,Solving the ODE $\frac{dy}{dx} -5y = e^x$ with $y(0)=-1$ using integrating factors.,Solving the ODE  with  using integrating factors.,\frac{dy}{dx} -5y = e^x y(0)=-1,"Solve  $$\frac{dy}{dx} -5y = e^x$$ Using initial condition $y(0)=-1$. I calculate that $c=-0.75$, which gives me$$y=-0.25e^x - \frac{0.75}{e^{-5x}}$$ I'm just asking for verification of answer. thanks to the people for providing the steps and explaining the process despite me only asking for my answer to be verified","Solve  $$\frac{dy}{dx} -5y = e^x$$ Using initial condition $y(0)=-1$. I calculate that $c=-0.75$, which gives me$$y=-0.25e^x - \frac{0.75}{e^{-5x}}$$ I'm just asking for verification of answer. thanks to the people for providing the steps and explaining the process despite me only asking for my answer to be verified",,"['calculus', 'ordinary-differential-equations', 'solution-verification']"
19,Linear ODE and Fourier Series,Linear ODE and Fourier Series,,"Let $m,k_0,k$ be positive real numbers and $x_1$, $x_2$ be real-valued functions of time. Suppose we have following system of two coupled ODEs ( motivated by a coupled oscillator with two masses and three springs )    : $m(x_1)'' = -k_0x_1 - k(x_1-x_2)      $ $m(x_2)'' = -k_0x_2 + k( x_1 - x_2)$ By doing some algebraic manipulations i ended up finding out that the solution to both $x_1(t)$  and $x_2(t)$ results in a linear combination of two cosine terms, of two different frequencies. More precisely : $$x_1(t) = c_1\cos(w_0t + \phi_1) + c_2\cos(w_1t + \phi_2)$$ with  $$w_0=\sqrt{\frac{k}{m}}$$ and  $$w_1=\sqrt{\frac{k_0 +2k}{m}} $$ I was intrigued with the fact that possible highly chaotic solutions $x_1(t), x_2(t)$ ( the motions of each mass after arbitrary initial conditions on the coupled oscillator ) could be described so nicely as a linear combination of two simple solutions. I wanted to understand why there exists such a nice simplification. After asking in physics.stackexchange, a suggestion was to check Fourier series. Since the solutions $x_1(t),x_2(t)$ are periodic continuous functions ( no damping in our oscillator) , it's possible to decompose it into a fourier series, i was told. For the past day, i've been reading and understanding Fourier Series but i still lack to see the exact connection in this case. For instance, decomposing a periodic function into a fourier series might get us infinitely many cosine terms ( infinitely many coefficients ) ... Why do we have only two in that case ? Secondly, the frequency of the cosine terms in a fourier series are multiples of the lowest frequency ( fundamental frequency ). Why the frequency of the fastest cosine term is not a multiple of the frequency of the slowest cosine term, but rather a function of $k_0$. In summary, i want to understand how such chaotic motions of each mass after an arbitrary set of initial conditions could be so nicely described as a linear combination of two simple harmonic motions. I can see how this might have to do with Fourier series, but i still lack the accurate understanding of such connection. Thanks a lot in advance.","Let $m,k_0,k$ be positive real numbers and $x_1$, $x_2$ be real-valued functions of time. Suppose we have following system of two coupled ODEs ( motivated by a coupled oscillator with two masses and three springs )    : $m(x_1)'' = -k_0x_1 - k(x_1-x_2)      $ $m(x_2)'' = -k_0x_2 + k( x_1 - x_2)$ By doing some algebraic manipulations i ended up finding out that the solution to both $x_1(t)$  and $x_2(t)$ results in a linear combination of two cosine terms, of two different frequencies. More precisely : $$x_1(t) = c_1\cos(w_0t + \phi_1) + c_2\cos(w_1t + \phi_2)$$ with  $$w_0=\sqrt{\frac{k}{m}}$$ and  $$w_1=\sqrt{\frac{k_0 +2k}{m}} $$ I was intrigued with the fact that possible highly chaotic solutions $x_1(t), x_2(t)$ ( the motions of each mass after arbitrary initial conditions on the coupled oscillator ) could be described so nicely as a linear combination of two simple solutions. I wanted to understand why there exists such a nice simplification. After asking in physics.stackexchange, a suggestion was to check Fourier series. Since the solutions $x_1(t),x_2(t)$ are periodic continuous functions ( no damping in our oscillator) , it's possible to decompose it into a fourier series, i was told. For the past day, i've been reading and understanding Fourier Series but i still lack to see the exact connection in this case. For instance, decomposing a periodic function into a fourier series might get us infinitely many cosine terms ( infinitely many coefficients ) ... Why do we have only two in that case ? Secondly, the frequency of the cosine terms in a fourier series are multiples of the lowest frequency ( fundamental frequency ). Why the frequency of the fastest cosine term is not a multiple of the frequency of the slowest cosine term, but rather a function of $k_0$. In summary, i want to understand how such chaotic motions of each mass after an arbitrary set of initial conditions could be so nicely described as a linear combination of two simple harmonic motions. I can see how this might have to do with Fourier series, but i still lack the accurate understanding of such connection. Thanks a lot in advance.",,"['linear-algebra', 'ordinary-differential-equations', 'fourier-series']"
20,Solving $y'(x)\left(4-3y(x)x^2\right)=4x$,Solving,y'(x)\left(4-3y(x)x^2\right)=4x,Solve the differential equation   $$y'(x)\left(4-3y(x)x^2\right)=4x$$ I would appreciate some help with this problem.,Solve the differential equation   $$y'(x)\left(4-3y(x)x^2\right)=4x$$ I would appreciate some help with this problem.,,['ordinary-differential-equations']
21,Inverse Laplace transform shifting error,Inverse Laplace transform shifting error,,"I am doing the inverse Laplace transform of the function: $\frac{e^{-s}}{s-1}$. I am solving and receiving the answer: $e^t\mathcal{U}(t-1)$, however the correct answer is $e^{t-1}\mathcal{U}(t-1).$ Im failing to see where the $-1$ in the exponential is coming from, and I have attempted several times. I am using the equation $\mathcal{L}^{-1}[e^{-as}F(s)] = \mathcal{U}(t-a)f(t-a)$;","I am doing the inverse Laplace transform of the function: $\frac{e^{-s}}{s-1}$. I am solving and receiving the answer: $e^t\mathcal{U}(t-1)$, however the correct answer is $e^{t-1}\mathcal{U}(t-1).$ Im failing to see where the $-1$ in the exponential is coming from, and I have attempted several times. I am using the equation $\mathcal{L}^{-1}[e^{-as}F(s)] = \mathcal{U}(t-a)f(t-a)$;",,"['calculus', 'ordinary-differential-equations', 'laplace-transform']"
22,Recurrence relation for a differential equation,Recurrence relation for a differential equation,,"I am reading a book that talks about series solutions of differential equations, and I couldn't seem to understand the following question: Consider the differential equation and use the assumption that then find the recurrence relation for the infinite series to be a solution. I am not entirely sure what the question means, especially about the recurrence relation. The only thing that I could get from the book is that the original equation is the  Legendre differential equation. How should I approach this problem? Thanks.","I am reading a book that talks about series solutions of differential equations, and I couldn't seem to understand the following question: Consider the differential equation and use the assumption that then find the recurrence relation for the infinite series to be a solution. I am not entirely sure what the question means, especially about the recurrence relation. The only thing that I could get from the book is that the original equation is the  Legendre differential equation. How should I approach this problem? Thanks.",,"['ordinary-differential-equations', 'recurrence-relations', 'legendre-polynomials']"
23,"Suppose that the ODE $x'=f(x)$ on $\mathbb{R}$ is bounded, $|f(x)| \leq M$ for all x","Suppose that the ODE  on  is bounded,  for all x",x'=f(x) \mathbb{R} |f(x)| \leq M,"Prove that no solution of the ODE escapes to infinity in finite time. What I've gotten so far is: $x' = \frac{dx}{dt} = f(x)$. And, $-M \leq \frac{dx}{dt} \leq M$. Thus, by integrating, $|x(t)| \leq Mt$. $x(t)$ will go to infinity as $t$ goes to infinity, but is a correct proof? Any help would be greatly appreciated!","Prove that no solution of the ODE escapes to infinity in finite time. What I've gotten so far is: $x' = \frac{dx}{dt} = f(x)$. And, $-M \leq \frac{dx}{dt} \leq M$. Thus, by integrating, $|x(t)| \leq Mt$. $x(t)$ will go to infinity as $t$ goes to infinity, but is a correct proof? Any help would be greatly appreciated!",,"['real-analysis', 'ordinary-differential-equations']"
24,$(2x+y)\frac{dy}{dx} + x = 0$,,(2x+y)\frac{dy}{dx} + x = 0,"Solve:   $(2x+y)\frac{dy}{dx} + x = 0$ . Only solving seperable equations and use of an integrating factor have been covered so far, and I can't see how to get it into a form such that it is solvable by these methods.","Solve:   $(2x+y)\frac{dy}{dx} + x = 0$ . Only solving seperable equations and use of an integrating factor have been covered so far, and I can't see how to get it into a form such that it is solvable by these methods.",,['ordinary-differential-equations']
25,Can a function that satisfies the condition $f''(x) = a \cos f(x) $ and $(f'(x))^2 = b \sin f(x)$ be found?,Can a function that satisfies the condition  and  be found?,f''(x) = a \cos f(x)  (f'(x))^2 = b \sin f(x),"Can I find a function that satisfies the following conditions? $$\begin{eqnarray} f''(x)    &=&  a \cos f(x) \tag{1}   \\   (f'(x))^2 &=&  b \sin f(x) \tag{2}   \\ f(x)      &>& 0            \tag{3} \end{eqnarray}$$ That's what I have tried so far: (I will denote the function with $y = f(x)$ and its derivatives as $y'$, $y''$. Also, for simplicity purposes we will let $a=b=1$) So: Squaring both and adding because I noted that $y''$ and $(y')^2$ are on the unit circle therefore resulting in: $$(y'')^2 + (y')^4 = 1$$ By using the chain rule to get  $$\frac{d^2y}{dx^2} = \frac{d(y')}{dy} \frac{dy}{dx} \\ \iff y'  \frac {d}{dy} (\sqrt {\sin {y}}) = \cos y \\ \iff y' \frac{1}{2 \sqrt{\sin y}} \cos y = \cos y \\ \iff \int \frac{1}{2 \sqrt{\sin y}} dy = x + c $$ Dividing $(1)$ and $(2)$ where $y' \neq 0$: $$- \frac{y''}{(y')^2} = - \cot y \iff \bigg (\frac{1}{y'} \bigg )' = - \cot y$$ Combine the new equations that were derived to get $(y'')^2 + (4 \sin y)^2 = 1$ and $\frac {d} {dx} (\frac{1}{2 \sqrt { \sin y}}) = - \cot y$","Can I find a function that satisfies the following conditions? $$\begin{eqnarray} f''(x)    &=&  a \cos f(x) \tag{1}   \\   (f'(x))^2 &=&  b \sin f(x) \tag{2}   \\ f(x)      &>& 0            \tag{3} \end{eqnarray}$$ That's what I have tried so far: (I will denote the function with $y = f(x)$ and its derivatives as $y'$, $y''$. Also, for simplicity purposes we will let $a=b=1$) So: Squaring both and adding because I noted that $y''$ and $(y')^2$ are on the unit circle therefore resulting in: $$(y'')^2 + (y')^4 = 1$$ By using the chain rule to get  $$\frac{d^2y}{dx^2} = \frac{d(y')}{dy} \frac{dy}{dx} \\ \iff y'  \frac {d}{dy} (\sqrt {\sin {y}}) = \cos y \\ \iff y' \frac{1}{2 \sqrt{\sin y}} \cos y = \cos y \\ \iff \int \frac{1}{2 \sqrt{\sin y}} dy = x + c $$ Dividing $(1)$ and $(2)$ where $y' \neq 0$: $$- \frac{y''}{(y')^2} = - \cot y \iff \bigg (\frac{1}{y'} \bigg )' = - \cot y$$ Combine the new equations that were derived to get $(y'')^2 + (4 \sin y)^2 = 1$ and $\frac {d} {dx} (\frac{1}{2 \sqrt { \sin y}}) = - \cot y$",,['ordinary-differential-equations']
26,How do I solve this differential equation ? It looks like just an integration problem.,How do I solve this differential equation ? It looks like just an integration problem.,,"$$\dfrac{dy}{dx} = \dfrac{xy}{x +y}$$ I am finding this very hard to understand how this is a differential equation requiring solving, wouldn't you just integrate the function ? Thanks.","$$\dfrac{dy}{dx} = \dfrac{xy}{x +y}$$ I am finding this very hard to understand how this is a differential equation requiring solving, wouldn't you just integrate the function ? Thanks.",,['ordinary-differential-equations']
27,Solving the differential equation $9x(1-x)y''-12y'+4y=0$,Solving the differential equation,9x(1-x)y''-12y'+4y=0,Solve in series the following ODE: $$9x(1-x)y''-12y'+4y=0$$ expanding $y(x)$ about $x_0=0$. My guess: I think it is by Frobenius series since it is not an ordinary point.,Solve in series the following ODE: $$9x(1-x)y''-12y'+4y=0$$ expanding $y(x)$ about $x_0=0$. My guess: I think it is by Frobenius series since it is not an ordinary point.,,['ordinary-differential-equations']
28,Finding a solution basis,Finding a solution basis,,"Find a real solution basis of $$y'=\left( \begin{matrix}-1&-2&0\\0&2&0\\-1&-3&2\\ \end{matrix} \right)y.$$ The characteristic equation of this matrix is $$P(t) = (1-t)(2-t)^2.$$ So next I calculated eigenvectors for the eigenvalues $1$ and $2$, which are $$u\overset{def}=(1,0,1) \text{ and }v\overset{def}=(0,0,1) \text{ respectively}.$$ The eigenvalue $2$ has algebraic multiplicity $2$ but it only has one eigenvector. So if I'm correct we need a principal vector. I computed this and it is $$v_p\overset{def}=(2,-1,0).$$ Now the solution basis is $$B=\Big\{t\mapsto u e^t, t\mapsto ve^{2t}, ?? \Big\}.$$ My question is, what is the third function? What solution does the principal vector I have computed correspond to? Thank you.","Find a real solution basis of $$y'=\left( \begin{matrix}-1&-2&0\\0&2&0\\-1&-3&2\\ \end{matrix} \right)y.$$ The characteristic equation of this matrix is $$P(t) = (1-t)(2-t)^2.$$ So next I calculated eigenvectors for the eigenvalues $1$ and $2$, which are $$u\overset{def}=(1,0,1) \text{ and }v\overset{def}=(0,0,1) \text{ respectively}.$$ The eigenvalue $2$ has algebraic multiplicity $2$ but it only has one eigenvector. So if I'm correct we need a principal vector. I computed this and it is $$v_p\overset{def}=(2,-1,0).$$ Now the solution basis is $$B=\Big\{t\mapsto u e^t, t\mapsto ve^{2t}, ?? \Big\}.$$ My question is, what is the third function? What solution does the principal vector I have computed correspond to? Thank you.",,"['calculus', 'ordinary-differential-equations']"
29,Does local existence in every point imply global existence for an ODE?,Does local existence in every point imply global existence for an ODE?,,"Consider the following first order ODE: $y' = f(t,y)$ subject to $y(t_{0}) = y_{0}$. I would like to show that there exists a unique function $y(\cdot)$ that passes through ($t_{0}, y_{0}$) The thing is $f$ is neither globally Lipschitz nor continuous. Therefore, I cannot apply standard theorems to prove the global existence. Instead, $f$ is locally Lipschitz, which implies the local existence at each point: For each point ($t_{0}, y_{0}$), there exists a unique function $y(\cdot)$ that satisfies $y' = f(t,y)$  for $t \in [t_{0} - \epsilon, t_{0}+\epsilon$] and the initial condition. My question is by pasting the local solutions, can we get a global solution? What are the necessary and sufficient conditions that make pasting a legitimate action for obtaining the global solution? I would appreciate your comments about this.","Consider the following first order ODE: $y' = f(t,y)$ subject to $y(t_{0}) = y_{0}$. I would like to show that there exists a unique function $y(\cdot)$ that passes through ($t_{0}, y_{0}$) The thing is $f$ is neither globally Lipschitz nor continuous. Therefore, I cannot apply standard theorems to prove the global existence. Instead, $f$ is locally Lipschitz, which implies the local existence at each point: For each point ($t_{0}, y_{0}$), there exists a unique function $y(\cdot)$ that satisfies $y' = f(t,y)$  for $t \in [t_{0} - \epsilon, t_{0}+\epsilon$] and the initial condition. My question is by pasting the local solutions, can we get a global solution? What are the necessary and sufficient conditions that make pasting a legitimate action for obtaining the global solution? I would appreciate your comments about this.",,['ordinary-differential-equations']
30,Need help with simple system of differential equations,Need help with simple system of differential equations,,"thanks to your help I advanced in computing differential equations, but now I encountered another problem I need help with - this time it is a system of differential equations: $$x_1'=-x_2$$    $$x_2'=x_1$$ I know that the answer should contain trigonometric functions, (sine and cosine) but I have no idea how to start. I tried to divide first equation/second equation and I got something like: $$\frac{x_1'}{x_2'}=-\frac{x2}{x1}$$ Then I rewrited x1' as $$\frac{dx1}{dt}$$ and did the same with x2. I got rid of dt this way and got a: $$x_1dx_1=-x_2dx_2$$ Which lead me to result: $$x_1=\sqrt(const-x_1^2)$$ After inserting x1 to the $$x_2'=x_1$$ equation I got some results, but neither of them contains sine or cosine. Could you point me what am I doing wrong?","thanks to your help I advanced in computing differential equations, but now I encountered another problem I need help with - this time it is a system of differential equations: $$x_1'=-x_2$$    $$x_2'=x_1$$ I know that the answer should contain trigonometric functions, (sine and cosine) but I have no idea how to start. I tried to divide first equation/second equation and I got something like: $$\frac{x_1'}{x_2'}=-\frac{x2}{x1}$$ Then I rewrited x1' as $$\frac{dx1}{dt}$$ and did the same with x2. I got rid of dt this way and got a: $$x_1dx_1=-x_2dx_2$$ Which lead me to result: $$x_1=\sqrt(const-x_1^2)$$ After inserting x1 to the $$x_2'=x_1$$ equation I got some results, but neither of them contains sine or cosine. Could you point me what am I doing wrong?",,"['ordinary-differential-equations', 'trigonometry', 'systems-of-equations']"
31,Separation of variables method,Separation of variables method,,Hi I am trying to solve the following $$\frac{dy}{dx}=\frac{2xy}{x^2-1}$$ with boundary condition $y=1$ at $x=0$. I use the method of separation of variables: $$\frac{dy}{y}=\frac{2x}{x^2-1}dx$$ $$\ln (y)=\ln(x^2-1)+c$$ Then I got stuck because the solution says that $y=1-x^2$. Can anyone give me a hand with this? Many Thanks,Hi I am trying to solve the following $$\frac{dy}{dx}=\frac{2xy}{x^2-1}$$ with boundary condition $y=1$ at $x=0$. I use the method of separation of variables: $$\frac{dy}{y}=\frac{2x}{x^2-1}dx$$ $$\ln (y)=\ln(x^2-1)+c$$ Then I got stuck because the solution says that $y=1-x^2$. Can anyone give me a hand with this? Many Thanks,,"['integration', 'ordinary-differential-equations']"
32,Solve the equation $\frac{dy}{dx}+ky=a\sin(mx)$,Solve the equation,\frac{dy}{dx}+ky=a\sin(mx),Solve the equation $$\frac{dy}{dx}+ky=a\sin(mx)$$ I've tried using an integrating factor which then got me to the equation $$\frac{d}{dx}(e^{kx}y)=e^{kx}a\sin(mx)$$ I'm not sure if this is right or not and if it is I'm not sure how to evaluate the integral.,Solve the equation $$\frac{dy}{dx}+ky=a\sin(mx)$$ I've tried using an integrating factor which then got me to the equation $$\frac{d}{dx}(e^{kx}y)=e^{kx}a\sin(mx)$$ I'm not sure if this is right or not and if it is I'm not sure how to evaluate the integral.,,['ordinary-differential-equations']
33,Differential equation $y''=e^y $,Differential equation,y''=e^y ,"Is there a quick way of finding $y(t)$ which satisfies the following equation:$$y''=e^y \ ?$$ Usually when given equation $ax''+bx'+cx=0$ I looked for roots of characteristic polynomial, but in this case I'm not sure how to proceed.","Is there a quick way of finding $y(t)$ which satisfies the following equation:$$y''=e^y \ ?$$ Usually when given equation $ax''+bx'+cx=0$ I looked for roots of characteristic polynomial, but in this case I'm not sure how to proceed.",,['ordinary-differential-equations']
34,Proving a differential equation is linear,Proving a differential equation is linear,,Prove that the following differential equation is linear: $$y(t)\frac{df(t)}{dt} - 3f(t)x(t)= 0.$$ I thought it was linear looking at it. However is there any way I can prove it? Any help would be much appreciated.,Prove that the following differential equation is linear: $$y(t)\frac{df(t)}{dt} - 3f(t)x(t)= 0.$$ I thought it was linear looking at it. However is there any way I can prove it? Any help would be much appreciated.,,['ordinary-differential-equations']
35,Solving of the first-order nonlinear differential equation,Solving of the first-order nonlinear differential equation,,Good day. Can you give me advice about solution of the equation $(x+1)(y'+y^2)=-y$? I guess it is Riccati's equation.,Good day. Can you give me advice about solution of the equation $(x+1)(y'+y^2)=-y$? I guess it is Riccati's equation.,,['ordinary-differential-equations']
36,annihilator method confusion,annihilator method confusion,,"I have a final in the morning and I am extremely confused on the annihilator method. I have been googling different explanations all night and I just dont get it at all. I am looking at an example: $$\ddot{y}+6\dot{y}+y=e^{(3x)}-\sin(x)$$ now I get that the annihilator of the $e$ term is $(D-3)$ but the answer is $(D-3)(D+1)(D^2 +6D +8)$ can someone explain the second part and if you are feeling generous how to do other annihilators maybe with examples in really simple language. I get so lost with these explanations that use ""math language"" also is there a list or something I can study for what annihilates what? i have found one that I understand but it's really limited. A lot of them are written in extremely complicated language. thanks for your help","I have a final in the morning and I am extremely confused on the annihilator method. I have been googling different explanations all night and I just dont get it at all. I am looking at an example: now I get that the annihilator of the term is but the answer is can someone explain the second part and if you are feeling generous how to do other annihilators maybe with examples in really simple language. I get so lost with these explanations that use ""math language"" also is there a list or something I can study for what annihilates what? i have found one that I understand but it's really limited. A lot of them are written in extremely complicated language. thanks for your help",\ddot{y}+6\dot{y}+y=e^{(3x)}-\sin(x) e (D-3) (D-3)(D+1)(D^2 +6D +8),"['ordinary-differential-equations', 'higher-order-logic']"
37,Nonlinear first order system of ODEs,Nonlinear first order system of ODEs,,"While solving some physical problem, I have obtained the following system of differential equations with boundary conditions: $$\left\{\begin{matrix} \frac{d\phi_1}{dz}=\frac{m^2}{\lambda}- \lambda\phi_1^2-\alpha\phi_2^2 \\ \frac{d\phi_2}{dz}=-2\alpha\phi_1\phi_2 \\ \phi_2(\pm\infty)=0 \\ \phi_1(\pm\infty)=\pm\frac{m}{\lambda} \end{matrix}\right.$$ where $\phi_1(z),\phi_2(z)$ are just functions of real variables, $m,\lambda,\alpha\in \mathbb{R}$ As far as I know, to solve this problem I should solve the system of differential equations and after that use boundary conditions. I see that it's easy to solve this system when $\lambda=\alpha$ : Just sum this two equations to obtain  $$\frac{d(\phi_1+\phi_2)}{dz}=\frac{m^2}{\lambda}-\lambda(\phi_1+\phi_2)$$ This DE easily solved  $$(\phi_1+\phi_2)=\frac{m}{\lambda}\tanh(mz-m\lambda C_1)$$ where $C_1$ is an integration constant.  After that I can find exact solutions for $\phi_1$ and $\phi_2$. And boundary conditions are satisfied automaticaly. But I would like to obtain the solution for any $\alpha$ and $\lambda$ at least by quadrature. My attempt was to reproduce the aproach as in case $\lambda=\alpha$: I've obtained $$\frac{1}{\sqrt{\alpha}}\frac{(\sqrt{\alpha}\phi_1+\lambda\phi_2)}{dz}=\frac{m^2}{\lambda}-(\sqrt{\lambda}\phi_1+\sqrt{\alpha}\phi_2)^2$$ So, this attempt was a fail. Also I have noticed that these two equations look like as Riccati equation And I have no any ideas how to solve it. Any help will be appreciated.","While solving some physical problem, I have obtained the following system of differential equations with boundary conditions: $$\left\{\begin{matrix} \frac{d\phi_1}{dz}=\frac{m^2}{\lambda}- \lambda\phi_1^2-\alpha\phi_2^2 \\ \frac{d\phi_2}{dz}=-2\alpha\phi_1\phi_2 \\ \phi_2(\pm\infty)=0 \\ \phi_1(\pm\infty)=\pm\frac{m}{\lambda} \end{matrix}\right.$$ where $\phi_1(z),\phi_2(z)$ are just functions of real variables, $m,\lambda,\alpha\in \mathbb{R}$ As far as I know, to solve this problem I should solve the system of differential equations and after that use boundary conditions. I see that it's easy to solve this system when $\lambda=\alpha$ : Just sum this two equations to obtain  $$\frac{d(\phi_1+\phi_2)}{dz}=\frac{m^2}{\lambda}-\lambda(\phi_1+\phi_2)$$ This DE easily solved  $$(\phi_1+\phi_2)=\frac{m}{\lambda}\tanh(mz-m\lambda C_1)$$ where $C_1$ is an integration constant.  After that I can find exact solutions for $\phi_1$ and $\phi_2$. And boundary conditions are satisfied automaticaly. But I would like to obtain the solution for any $\alpha$ and $\lambda$ at least by quadrature. My attempt was to reproduce the aproach as in case $\lambda=\alpha$: I've obtained $$\frac{1}{\sqrt{\alpha}}\frac{(\sqrt{\alpha}\phi_1+\lambda\phi_2)}{dz}=\frac{m^2}{\lambda}-(\sqrt{\lambda}\phi_1+\sqrt{\alpha}\phi_2)^2$$ So, this attempt was a fail. Also I have noticed that these two equations look like as Riccati equation And I have no any ideas how to solve it. Any help will be appreciated.",,"['ordinary-differential-equations', 'dynamical-systems', 'nonlinear-system', 'boundary-value-problem']"
38,How find this equation $y''+(y')^2\cdot e^x=0$,How find this equation,y''+(y')^2\cdot e^x=0,"if the ODE  $$y''+(y')^2\cdot e^x=0$$ such $$y(0)=1,y'(0)=1$$ Find the $y(x)=?$ my  ugly methods: let $$y'=p,y''(x)=p'(x)$$ so $$p'(x)+p^2\cdot e^x=0$$ $$\dfrac{dp}{dx}=-p^2e^x$$ $$\dfrac{dp}{-p^2}=e^xdx$$ $$\Longrightarrow \dfrac{1}{p}=e^x+c\Longrightarrow p=e^{-x}(p(0)=1)$$ so $$y(x)=2-e^{-x}$$ My question: This ODE have other methods,? such I want use $$(y'\cdot e^x)'=y''e^x+e^xy'$$ Thank you","if the ODE  $$y''+(y')^2\cdot e^x=0$$ such $$y(0)=1,y'(0)=1$$ Find the $y(x)=?$ my  ugly methods: let $$y'=p,y''(x)=p'(x)$$ so $$p'(x)+p^2\cdot e^x=0$$ $$\dfrac{dp}{dx}=-p^2e^x$$ $$\dfrac{dp}{-p^2}=e^xdx$$ $$\Longrightarrow \dfrac{1}{p}=e^x+c\Longrightarrow p=e^{-x}(p(0)=1)$$ so $$y(x)=2-e^{-x}$$ My question: This ODE have other methods,? such I want use $$(y'\cdot e^x)'=y''e^x+e^xy'$$ Thank you",,"['calculus', 'integration', 'ordinary-differential-equations', 'indefinite-integrals']"
39,Help solving an ODE,Help solving an ODE,,"This is an example in my book.  It is for the following system: \begin{align*} x'&=y+x(1-x^2-y^2)\\ y'&=-x+y(1-x^2-y^2) \end{align*} So using polar coordinates we get the following system \begin{align*} r'&=r(1-r^2)\\ \theta'&=-1, \end{align*} and the solutions are  $$r(t)=(1+ce^{-2t})^{\frac{-1}{2}}, \,\,\,\,\,\,\theta(t)=-(t-\alpha).$$ So my question is how did they solve $r'$.  It's been a while since I've take differential equations and I need help how to solve it.","This is an example in my book.  It is for the following system: \begin{align*} x'&=y+x(1-x^2-y^2)\\ y'&=-x+y(1-x^2-y^2) \end{align*} So using polar coordinates we get the following system \begin{align*} r'&=r(1-r^2)\\ \theta'&=-1, \end{align*} and the solutions are  $$r(t)=(1+ce^{-2t})^{\frac{-1}{2}}, \,\,\,\,\,\,\theta(t)=-(t-\alpha).$$ So my question is how did they solve $r'$.  It's been a while since I've take differential equations and I need help how to solve it.",,"['ordinary-differential-equations', 'polar-coordinates']"
40,Systems of Linear Differential Equations - population models,Systems of Linear Differential Equations - population models,,"I have to solve the following first-order linear system, $x(t)$ represents one population and the $y(t)$ represents another population that lives in the same ecosystem: (Note: $'$ denotes prime) \begin{align} x' = -5x - 20y &  \text{(Equation 1)} \\ y' = 5x + 7y  &   \text{(Equation 2)} \end{align} I start off with finding the derivative of (Equation 1) which gives me: $$x'' = -5x' - 20 y'$$ and then substitute $y'$ using the equations above (Equation 2): $$x'' = -5x' - 20 (5x + 7y)$$ foil expansion: $$x'' = -5x' - 100x - 140y$$ so I insert values for $y$ through reordering an existing equation: $$y = -\frac{1}{20}(x'+5x)$$ thus  \begin{align} x'' = -5x'-100x-140\bigg(-\frac{1}{20}x'-\frac{1}{4}x\bigg) \\ x'' = -5x'-100x+7x'+35x \\ x''+ 5x'+100x-7x'-35x=0 \\ x''-2x'+65x=0 \end{align} Then put into auxillary form: $r^2-2r+65=0$ thus $r = 1 \pm 8i$ ..etc. Eventually I get $x(t)$ and $y(t)$. I have to predict what will happen to the population densities over a long time. How can I do this? Note: The book describes using equilibrium points and determining their stability to do this. Upvotes to whoever shows this specific method. I appreciate any additional explanations however. Edit: Here is my answer for $x(t)$ and $y(t)$ for clarification purposes. ($C_1$ and $C_2$ are constants) $x(t) = C_1e^{t} \sin 8t + C_2e^{t} \cos 8t$ $y(t) = \frac{1}{10}e^{t} (4C_2 \sin 8t-4C_1 \cos 8t-3C_1 \sin 8t-3C_2 \cos 8t)$","I have to solve the following first-order linear system, $x(t)$ represents one population and the $y(t)$ represents another population that lives in the same ecosystem: (Note: $'$ denotes prime) \begin{align} x' = -5x - 20y &  \text{(Equation 1)} \\ y' = 5x + 7y  &   \text{(Equation 2)} \end{align} I start off with finding the derivative of (Equation 1) which gives me: $$x'' = -5x' - 20 y'$$ and then substitute $y'$ using the equations above (Equation 2): $$x'' = -5x' - 20 (5x + 7y)$$ foil expansion: $$x'' = -5x' - 100x - 140y$$ so I insert values for $y$ through reordering an existing equation: $$y = -\frac{1}{20}(x'+5x)$$ thus  \begin{align} x'' = -5x'-100x-140\bigg(-\frac{1}{20}x'-\frac{1}{4}x\bigg) \\ x'' = -5x'-100x+7x'+35x \\ x''+ 5x'+100x-7x'-35x=0 \\ x''-2x'+65x=0 \end{align} Then put into auxillary form: $r^2-2r+65=0$ thus $r = 1 \pm 8i$ ..etc. Eventually I get $x(t)$ and $y(t)$. I have to predict what will happen to the population densities over a long time. How can I do this? Note: The book describes using equilibrium points and determining their stability to do this. Upvotes to whoever shows this specific method. I appreciate any additional explanations however. Edit: Here is my answer for $x(t)$ and $y(t)$ for clarification purposes. ($C_1$ and $C_2$ are constants) $x(t) = C_1e^{t} \sin 8t + C_2e^{t} \cos 8t$ $y(t) = \frac{1}{10}e^{t} (4C_2 \sin 8t-4C_1 \cos 8t-3C_1 \sin 8t-3C_2 \cos 8t)$",,"['linear-algebra', 'ordinary-differential-equations', 'dynamical-systems']"
41,Implicit solution to second-order non-linear differential equation,Implicit solution to second-order non-linear differential equation,,"I am trying to solve the following differential equation: $$ \frac{8}{9}y(t) - 2y(t)^2 + y(t)^3 -y''(t) =0. $$ I think it is impossible to solve it for $y(t)$, but apparently it is possible to solve it for $t(y)$. I really do not have a clue how to solve this one. I guess I need to do some kind of transformation, but I don't really know which one. Can anyone help me?","I am trying to solve the following differential equation: $$ \frac{8}{9}y(t) - 2y(t)^2 + y(t)^3 -y''(t) =0. $$ I think it is impossible to solve it for $y(t)$, but apparently it is possible to solve it for $t(y)$. I really do not have a clue how to solve this one. I guess I need to do some kind of transformation, but I don't really know which one. Can anyone help me?",,['ordinary-differential-equations']
42,Solve differential equation with variation of parameters?,Solve differential equation with variation of parameters?,,$$(1-x)y'' +xy' - y = 1-x$$ i) Check if $y_1(x)=e^x$ and $y_2(x) = x$ is a solution to the differential equation (homogeneous). ii) Use variation of parameters to find a general solution to the inhomogeneous equation.  :) Any tips/solution on this one? :D,$$(1-x)y'' +xy' - y = 1-x$$ i) Check if $y_1(x)=e^x$ and $y_2(x) = x$ is a solution to the differential equation (homogeneous). ii) Use variation of parameters to find a general solution to the inhomogeneous equation.  :) Any tips/solution on this one? :D,,['ordinary-differential-equations']
43,Normal form of differential equation $y'=y-y^2$,Normal form of differential equation,y'=y-y^2,"I've been  told that I can transform any equation of form $y'=ay+g(y)$, where $g(y)$ is polynomial, to form $y'=y$ (Poincaré-Dulac theorem if I'm not mistaken). So I want to find the normal form of differential equation $y'=y-y^2$. What I've done: Let $y=\tilde{y}+a\tilde{y}^2$. LHS: $\tilde{y}'+2a\tilde{y}'\tilde{y}=\tilde{y}'(1+2a\tilde{y})$ RHS: $\tilde{y}+a\tilde{y}^2-\tilde{y}^2-2a\tilde{y}^3-a^2\tilde{y}=(1+2a\tilde{y})\tilde{y}-(1+a)\tilde{y}^2-2a\tilde{y}^3-a^2\tilde{y}^4$ So $$\tilde{y}'(1+2a\tilde{y})=(1+2a\tilde{y})\tilde{y}-(1+a)\tilde{y}^2-2a\tilde{y}^3-a^2\tilde{y}^4$$ We can divide both sides by $(1+2a\tilde{y}) $: $$ \tilde{y}'=\tilde{y}-\frac{1+a}{1+2a\tilde{y}}\tilde{y}^2-\frac{2a}{1+2a\tilde{y}}\tilde{y}^3-\frac{a^2}{1+2a\tilde{y}}\tilde{y}^4$$ So if $a=-1$ we have: $$\tilde{y}'=\tilde{y}+\frac{2}{1-2\tilde{y}}\tilde{y}^3-\frac{1}{1-2\tilde{y}}\tilde{y}^4$$ Then we can expand fractions in Taylor series and use the same algorithm to ""kill"" the third degree etc. The question is: Can I obtain the explicit form of the change of variables for any order? Does this series converge?","I've been  told that I can transform any equation of form $y'=ay+g(y)$, where $g(y)$ is polynomial, to form $y'=y$ (Poincaré-Dulac theorem if I'm not mistaken). So I want to find the normal form of differential equation $y'=y-y^2$. What I've done: Let $y=\tilde{y}+a\tilde{y}^2$. LHS: $\tilde{y}'+2a\tilde{y}'\tilde{y}=\tilde{y}'(1+2a\tilde{y})$ RHS: $\tilde{y}+a\tilde{y}^2-\tilde{y}^2-2a\tilde{y}^3-a^2\tilde{y}=(1+2a\tilde{y})\tilde{y}-(1+a)\tilde{y}^2-2a\tilde{y}^3-a^2\tilde{y}^4$ So $$\tilde{y}'(1+2a\tilde{y})=(1+2a\tilde{y})\tilde{y}-(1+a)\tilde{y}^2-2a\tilde{y}^3-a^2\tilde{y}^4$$ We can divide both sides by $(1+2a\tilde{y}) $: $$ \tilde{y}'=\tilde{y}-\frac{1+a}{1+2a\tilde{y}}\tilde{y}^2-\frac{2a}{1+2a\tilde{y}}\tilde{y}^3-\frac{a^2}{1+2a\tilde{y}}\tilde{y}^4$$ So if $a=-1$ we have: $$\tilde{y}'=\tilde{y}+\frac{2}{1-2\tilde{y}}\tilde{y}^3-\frac{1}{1-2\tilde{y}}\tilde{y}^4$$ Then we can expand fractions in Taylor series and use the same algorithm to ""kill"" the third degree etc. The question is: Can I obtain the explicit form of the change of variables for any order? Does this series converge?",,"['ordinary-differential-equations', 'dynamical-systems']"
44,Solution of $y''=\frac{K}{y^2}$ with $K$ a constant.,Solution of  with  a constant.,y''=\frac{K}{y^2} K,"Solve  $$ y''=\frac{K}{y^2} $$ where $K$ is a non-zero constant. My attempt : Lagrange method i.e.  $z=\frac{y'}{y}$ but it's look harder. Multiplying both sides by $y'$, then integration but it doesn't look better. I tried to let $y=y(0)(cos(z))^s$ but is ineffective Moreover, the equation is nonlinear, we know nothing about the finiteness or countability of the dimension of the solution space, Thank you in advance for you help,","Solve  $$ y''=\frac{K}{y^2} $$ where $K$ is a non-zero constant. My attempt : Lagrange method i.e.  $z=\frac{y'}{y}$ but it's look harder. Multiplying both sides by $y'$, then integration but it doesn't look better. I tried to let $y=y(0)(cos(z))^s$ but is ineffective Moreover, the equation is nonlinear, we know nothing about the finiteness or countability of the dimension of the solution space, Thank you in advance for you help,",,[]
45,"Second order linear ODE, self adjoint (Sturm-Liouville) form. Orthogonality of solutions - confused about the weight factor.","Second order linear ODE, self adjoint (Sturm-Liouville) form. Orthogonality of solutions - confused about the weight factor.",,"If I have an ODE of the form $$a(x)y''+b(x)y'+c(x)y= \lambda y$$ Such that $b=a'$, then it is equivalent to: $$(a(x)y')'+c(x)y= \lambda y$$ So the solutions corresponding to two different eigenvalues (suppose they are indexed by integers) are orthogonal (suppose $x\in[0,1]$): $$\int_0^1 y_n(x)y_m(x)dx=\delta_{nm}||y_n||.$$ But now if $b \neq a'$, I want to multiply the equation through a weight $w(x)$ so that $wb =(wa)'$, therefore $$w(x)=\frac{1}{a} \exp{\int \frac{b}{a}}.$$ After this, the solutions are orthogonal, this time with the weight factor added in: $$\int_0^1 y_n(x)y_m(x) w(x)dx=\delta_{nm}||y_n||.$$ Is the above correct? If so, consider the Bessel equation: $$x^2y''+xy'+(x^2-n^2)y$$ The weight is $w(x)=\frac{1}{x}$, so why on earth does it say here that: $$\int_0^1 xJ_\alpha (xu_{\alpha,n})J_\alpha (xu_{\alpha,m})dx = \delta_{nm}(\text{stuff})$$ Are those 'scaled' Bessel functions solution to some different differential equation with weight $x$?","If I have an ODE of the form $$a(x)y''+b(x)y'+c(x)y= \lambda y$$ Such that $b=a'$, then it is equivalent to: $$(a(x)y')'+c(x)y= \lambda y$$ So the solutions corresponding to two different eigenvalues (suppose they are indexed by integers) are orthogonal (suppose $x\in[0,1]$): $$\int_0^1 y_n(x)y_m(x)dx=\delta_{nm}||y_n||.$$ But now if $b \neq a'$, I want to multiply the equation through a weight $w(x)$ so that $wb =(wa)'$, therefore $$w(x)=\frac{1}{a} \exp{\int \frac{b}{a}}.$$ After this, the solutions are orthogonal, this time with the weight factor added in: $$\int_0^1 y_n(x)y_m(x) w(x)dx=\delta_{nm}||y_n||.$$ Is the above correct? If so, consider the Bessel equation: $$x^2y''+xy'+(x^2-n^2)y$$ The weight is $w(x)=\frac{1}{x}$, so why on earth does it say here that: $$\int_0^1 xJ_\alpha (xu_{\alpha,n})J_\alpha (xu_{\alpha,m})dx = \delta_{nm}(\text{stuff})$$ Are those 'scaled' Bessel functions solution to some different differential equation with weight $x$?",,"['ordinary-differential-equations', 'special-functions', 'bessel-functions']"
46,"Solve $ \vec{x^{'}} = \begin{bmatrix}1 & 2 \\ 2 & 1\end{bmatrix}\vec{x}$, $ \vec{x} (0) = \begin{bmatrix}x_0 \\ y_0\end{bmatrix}$","Solve ,", \vec{x^{'}} = \begin{bmatrix}1 & 2 \\ 2 & 1\end{bmatrix}\vec{x}  \vec{x} (0) = \begin{bmatrix}x_0 \\ y_0\end{bmatrix},"Solve $$ \vec{x^{'}} = \begin{bmatrix}1 & 2 \\ 2 & 1\end{bmatrix}\vec{x},$$ $$ \vec{x} (0) = \begin{bmatrix}x_0 \\ y_0\end{bmatrix}$$ The question is the encode the solution as a flow $\phi: $${\mathbb R} \times {\mathbb R}^2 \rightarrow {\mathbb R}^2 $ My approach is  when I first separate the matrix into two  $$ A = \begin{bmatrix}1 & 0 \\ 0 & 1\end{bmatrix},$$ $$ B = \begin{bmatrix}0 & 2 \\ 2 & 0\end{bmatrix}$$ For B I found that for even matrix such as $B^2, B^4...B^{2n}$ have entries $2^{2n}$ on top left and bottom right and for odd matrix such as $B^1, B^3...B^{2n-1}$ have entries $2^{2n-1}$ on top right and bottom left ...however...I do not know how to represent them...could someone please help with that?","Solve $$ \vec{x^{'}} = \begin{bmatrix}1 & 2 \\ 2 & 1\end{bmatrix}\vec{x},$$ $$ \vec{x} (0) = \begin{bmatrix}x_0 \\ y_0\end{bmatrix}$$ The question is the encode the solution as a flow $\phi: $${\mathbb R} \times {\mathbb R}^2 \rightarrow {\mathbb R}^2 $ My approach is  when I first separate the matrix into two  $$ A = \begin{bmatrix}1 & 0 \\ 0 & 1\end{bmatrix},$$ $$ B = \begin{bmatrix}0 & 2 \\ 2 & 0\end{bmatrix}$$ For B I found that for even matrix such as $B^2, B^4...B^{2n}$ have entries $2^{2n}$ on top left and bottom right and for odd matrix such as $B^1, B^3...B^{2n-1}$ have entries $2^{2n-1}$ on top right and bottom left ...however...I do not know how to represent them...could someone please help with that?",,['ordinary-differential-equations']
47,1st Order Differential Equation - Existence and Uniqueness Theorem,1st Order Differential Equation - Existence and Uniqueness Theorem,,"I have the following 1st order differential equation: $$y'=(x-4y)^{-2}$$ Which isn't continuous when $y=\frac{x}{4}$. Nevertheless, solutions exist and are unique on all the plane. How's that?","I have the following 1st order differential equation: $$y'=(x-4y)^{-2}$$ Which isn't continuous when $y=\frac{x}{4}$. Nevertheless, solutions exist and are unique on all the plane. How's that?",,['ordinary-differential-equations']
48,Hopf Bifurcation of Reaction-Diffusion System,Hopf Bifurcation of Reaction-Diffusion System,,"I'm considering the following reaction-diffusion system: $ \frac{\partial u}{\partial t} = f(u,v)+ D_1 \frac{d^2 u}{dx^2} $ $ \frac{\partial v}{\partial t} = g(u,v)+ D_2 \frac{d^2 v}{dx^2} $ where f and g describe the reaction kinetics, $ D_1 $ and $ D_2 $ are positive constants. I have been looking at the conditions for diffusion-driven instability which are as follows: $ f_u + g_v < 0 $ $ f_u g_v - f_v g_u > 0 $ $ D_2 f_u - D_1 g_v > 0 $ $ (D_2 f_u - D_1 g_v)^2 > 4D_1 D_2 (f_u g_v - f_v g_u) $ My notes then say that under these conditions bifurcation to solutions oscillating in time as well as space (called Hopf bifurcation) is not possible. I don't really understand this and really appreciate someone explaining why this is the case / proving it. Thanks","I'm considering the following reaction-diffusion system: $ \frac{\partial u}{\partial t} = f(u,v)+ D_1 \frac{d^2 u}{dx^2} $ $ \frac{\partial v}{\partial t} = g(u,v)+ D_2 \frac{d^2 v}{dx^2} $ where f and g describe the reaction kinetics, $ D_1 $ and $ D_2 $ are positive constants. I have been looking at the conditions for diffusion-driven instability which are as follows: $ f_u + g_v < 0 $ $ f_u g_v - f_v g_u > 0 $ $ D_2 f_u - D_1 g_v > 0 $ $ (D_2 f_u - D_1 g_v)^2 > 4D_1 D_2 (f_u g_v - f_v g_u) $ My notes then say that under these conditions bifurcation to solutions oscillating in time as well as space (called Hopf bifurcation) is not possible. I don't really understand this and really appreciate someone explaining why this is the case / proving it. Thanks",,"['ordinary-differential-equations', 'partial-differential-equations', 'dynamical-systems', 'bifurcation']"
49,What is the integrating factor of $2xy'+x^{2}e^{1-x^2}y=2$,What is the integrating factor of,2xy'+x^{2}e^{1-x^2}y=2,"I know how to start solving it, its dividing everything by $2x$, but I can't solve $$\int \dfrac{x^2e^{1-x^2}}{2x}\,dx$$","I know how to start solving it, its dividing everything by $2x$, but I can't solve $$\int \dfrac{x^2e^{1-x^2}}{2x}\,dx$$",,"['calculus', 'ordinary-differential-equations']"
50,Solve the differential equation $y''-10y'+21y=15e^{4t}$ using Laplace transform,Solve the differential equation  using Laplace transform,y''-10y'+21y=15e^{4t},"I need to solve the following with Laplace transform: $$y''-10y'+21y=15e^{4t}$$ $$y(0)=3,y'(0)=0$$ After Laplace transform I got: $$L(y)=\frac{15}{(S-4)(S^2-13S+21)}+\frac{30}{(S^2-13S+21)}$$ and now, what do I need to do?","I need to solve the following with Laplace transform: $$y''-10y'+21y=15e^{4t}$$ $$y(0)=3,y'(0)=0$$ After Laplace transform I got: $$L(y)=\frac{15}{(S-4)(S^2-13S+21)}+\frac{30}{(S^2-13S+21)}$$ and now, what do I need to do?",,"['ordinary-differential-equations', 'laplace-transform']"
51,Nonhomogeneous Linear ODE,Nonhomogeneous Linear ODE,,"$$ x' =\left(\begin{array}{rr}4 & 8 \\ -2 & -4\end{array}\right)x + \left(\begin{array}{rr}t^{-3} \\ -t^{-2}\end{array}\right), t>0 $$ To find the general solution of the given system above, my solution is: $det(\left(\begin{array}{rr}4-r & 8 \\ -2 & -4-r\end{array}\right)) = r^2$ , eigenvalues are $r_{1,2}=0$ $\left(\begin{array}{rr}4 & 8 \\ -2 & -4\end{array}\right)\left(\begin{array}{rr}v_{11} \\ v_{12}\end{array}\right) = \left(\begin{array}{rr}0 \\ 0\end{array}\right)$ Therefore $v_{1} =\left(\begin{array}{rr}-2 \\ 1\end{array}\right)$ $\left(\begin{array}{rr}4 & 8 \\ -2 & -4\end{array}\right)\left(\begin{array}{rr}v_{21} \\ v_{22}\end{array}\right) = \left(\begin{array}{rr}-2 \\ 1\end{array}\right)$ From there, I have and equation $2v_{21}+4v_{22}=-1$ What should I do now? Thanks.","$$ x' =\left(\begin{array}{rr}4 & 8 \\ -2 & -4\end{array}\right)x + \left(\begin{array}{rr}t^{-3} \\ -t^{-2}\end{array}\right), t>0 $$ To find the general solution of the given system above, my solution is: $det(\left(\begin{array}{rr}4-r & 8 \\ -2 & -4-r\end{array}\right)) = r^2$ , eigenvalues are $r_{1,2}=0$ $\left(\begin{array}{rr}4 & 8 \\ -2 & -4\end{array}\right)\left(\begin{array}{rr}v_{11} \\ v_{12}\end{array}\right) = \left(\begin{array}{rr}0 \\ 0\end{array}\right)$ Therefore $v_{1} =\left(\begin{array}{rr}-2 \\ 1\end{array}\right)$ $\left(\begin{array}{rr}4 & 8 \\ -2 & -4\end{array}\right)\left(\begin{array}{rr}v_{21} \\ v_{22}\end{array}\right) = \left(\begin{array}{rr}-2 \\ 1\end{array}\right)$ From there, I have and equation $2v_{21}+4v_{22}=-1$ What should I do now? Thanks.",,"['ordinary-differential-equations', 'eigenvalues-eigenvectors']"
52,Try to solve the following differential equation: $2y''=e^y$,Try to solve the following differential equation:,2y''=e^y,"I am trying to solve this equation: $$2y''=e^y$$ No $x$ in equation so:  $$y''=P'P , y'=p \\ \implies 2P'P=e^y$$  After the integrating on both sides I got: $$P^2=e^y$$ and back to $y$: $$y'2=e^y \\ \implies y'=\sqrt[]{e^y}$$ and now how I solve the integral? I hope I didn't mistake in my way thanks.","I am trying to solve this equation: $$2y''=e^y$$ No $x$ in equation so:  $$y''=P'P , y'=p \\ \implies 2P'P=e^y$$  After the integrating on both sides I got: $$P^2=e^y$$ and back to $y$: $$y'2=e^y \\ \implies y'=\sqrt[]{e^y}$$ and now how I solve the integral? I hope I didn't mistake in my way thanks.",,"['integration', 'ordinary-differential-equations']"
53,Finding Green's Functions,Finding Green's Functions,,"Well, I'm trying to understand how to find Green's functions for differential operators, and I'm working on the problem $Lu=-u''$ with BC's $u'(0)=u(1)=0$.  Can someone tell me if I'm doing this correctly?  In the end, I am getting that the Green's function is defined as $g(x)=(1-t)H(t-x)+(1-x)H(x-t)$ where $H(x-t)$ is the heaviside function.  Basically, the main idea is to make the function continuous at x = t and satisfy the derivative jump discontinuity, correct?","Well, I'm trying to understand how to find Green's functions for differential operators, and I'm working on the problem $Lu=-u''$ with BC's $u'(0)=u(1)=0$.  Can someone tell me if I'm doing this correctly?  In the end, I am getting that the Green's function is defined as $g(x)=(1-t)H(t-x)+(1-x)H(x-t)$ where $H(x-t)$ is the heaviside function.  Basically, the main idea is to make the function continuous at x = t and satisfy the derivative jump discontinuity, correct?",,['ordinary-differential-equations']
54,Choice of the First Term in Legendre Polynomials,Choice of the First Term in Legendre Polynomials,,"The two solutions of the Legendre's Differential Equation obtained by series solution method are : and Now according to my textbook, for the useful polynomial for n equal to a positive integer, the constant $a_{0}$ in the first case is chosen as $a_{0}={\frac{1.3.5...(2n-1)}{n!}}$ and the solution is then called Legendre's polyinomial or coffecient or Zonal Harmonics of the first kind. For n being the negative integer the constant $a_{0}$ in the second case is chosen as $a_{0}={\frac{n!}{1.3.5...(2n+1)}}$ and the corresponding polynomial is defined as Legendre's function of the second kind. Now the choice of $a_{0}$ seems rather arbitrary and exactly how is such a choice useful? P.S.: While solving the Hermite Equation we choose $a_{0}$ in a similar (and apparently) arbitrary manner. Is the reason behind the choice somehow related? The complete solution of Legendre's equation using series solution method can be found here","The two solutions of the Legendre's Differential Equation obtained by series solution method are : and Now according to my textbook, for the useful polynomial for n equal to a positive integer, the constant $a_{0}$ in the first case is chosen as $a_{0}={\frac{1.3.5...(2n-1)}{n!}}$ and the solution is then called Legendre's polyinomial or coffecient or Zonal Harmonics of the first kind. For n being the negative integer the constant $a_{0}$ in the second case is chosen as $a_{0}={\frac{n!}{1.3.5...(2n+1)}}$ and the corresponding polynomial is defined as Legendre's function of the second kind. Now the choice of $a_{0}$ seems rather arbitrary and exactly how is such a choice useful? P.S.: While solving the Hermite Equation we choose $a_{0}$ in a similar (and apparently) arbitrary manner. Is the reason behind the choice somehow related? The complete solution of Legendre's equation using series solution method can be found here",,"['ordinary-differential-equations', 'power-series', 'mathematical-physics', 'legendre-polynomials']"
55,undetermined coefficients. What am I doing wrong?,undetermined coefficients. What am I doing wrong?,,"I am having some trouble to solve the following differential equation for the undetermined coefficient: $$ y''+2y'+y=xe^{-x} $$ I have been watching some videos on youtube and done some reading but still do not fully understand what must be done to solve it. I was able to solve the left side by getting the common roots: $$ (x+1)^{2} = r^{2}+r+1 $$ roots are -1, so $$ y=(A+Bx)e^{-x} $$ Now after I found the left side I started to determine $$g(x)$$ so $$g(x)=xe^{-x}$$ $$yp = e^{-x}Ax+e^{-x}B$$ derivative of yp is $$y'p=-e^{-x}Ax+e^{-x}A-e^{-x}B$$ double derivative of yp is $$y''p=e^{-x}Ax-2Ae^{-x}+Be^{-x}$$ I then subbed these into the original equation: $$ (e^{-x}Ax-2Ae^{-x}+Be^{-x}) + 2(-e^{-x}Ax+e^{-x}A-e^{-x}B) + (e^{-x}Ax+e^{-x}B)=xe^{-1} $$ Now after factoring everything out I got that $$A=\frac{1}{2}e^{-x}$$ Which would bring me to my final answer of $$y=Ae^{-x}+Bxe^{-x}+\frac{1}{2}e^{-x}$$ However this is wrong as the final answer is actually  $$y=Ae^{-x}+Bxe^{-x}+\frac{1}{6}x^{3}e^{-x}$$ What is it that I am doing wrong? I am not sure what else to do :( Thank you :)","I am having some trouble to solve the following differential equation for the undetermined coefficient: $$ y''+2y'+y=xe^{-x} $$ I have been watching some videos on youtube and done some reading but still do not fully understand what must be done to solve it. I was able to solve the left side by getting the common roots: $$ (x+1)^{2} = r^{2}+r+1 $$ roots are -1, so $$ y=(A+Bx)e^{-x} $$ Now after I found the left side I started to determine $$g(x)$$ so $$g(x)=xe^{-x}$$ $$yp = e^{-x}Ax+e^{-x}B$$ derivative of yp is $$y'p=-e^{-x}Ax+e^{-x}A-e^{-x}B$$ double derivative of yp is $$y''p=e^{-x}Ax-2Ae^{-x}+Be^{-x}$$ I then subbed these into the original equation: $$ (e^{-x}Ax-2Ae^{-x}+Be^{-x}) + 2(-e^{-x}Ax+e^{-x}A-e^{-x}B) + (e^{-x}Ax+e^{-x}B)=xe^{-1} $$ Now after factoring everything out I got that $$A=\frac{1}{2}e^{-x}$$ Which would bring me to my final answer of $$y=Ae^{-x}+Bxe^{-x}+\frac{1}{2}e^{-x}$$ However this is wrong as the final answer is actually  $$y=Ae^{-x}+Bxe^{-x}+\frac{1}{6}x^{3}e^{-x}$$ What is it that I am doing wrong? I am not sure what else to do :( Thank you :)",,"['ordinary-differential-equations', 'binomial-coefficients']"
56,"Differentiation of $f(x_1, x_2) = \frac{x_1^2 + x_2^2}{x_2 - x_1 + 2}$",Differentiation of,"f(x_1, x_2) = \frac{x_1^2 + x_2^2}{x_2 - x_1 + 2}","this question might sound stupid to you, but I am having problems right now to differentiate this function: $$f(x_1, x_2) = \frac{x_1^2 + x_2^2}{x_2 - x_1 + 2}$$ I know the solution, from wolfram alpha, however I do not know how to come up with it by my own. I would appreciate your answer, if you could show me how to differentiate by $x_1$?","this question might sound stupid to you, but I am having problems right now to differentiate this function: $$f(x_1, x_2) = \frac{x_1^2 + x_2^2}{x_2 - x_1 + 2}$$ I know the solution, from wolfram alpha, however I do not know how to come up with it by my own. I would appreciate your answer, if you could show me how to differentiate by $x_1$?",,['ordinary-differential-equations']
57,Difference Equations and displacement operator,Difference Equations and displacement operator,,"For a Prep exam Exercise from the book: Numerical analysis of scientific computing. Section 1.3-3 Let $p$ be a polynomial of degree $m$, with $p(0) \neq 0$. If a sequence $x$ contains $m$ consecutive zeros and $p(E)x = 0$, then $x=0$ Where: $E$ denotes the displacement operator, $Ex=[x_2, x_3, \dots]$ where $x = [x_1, x_2, \dots]$ $L: V \to V$ $L=P(E) = \sum_{i=0}^nc_i E^i$ where $P(\lambda) = \sum_{i=0}^m c_i \lambda^i$ I've tried: I have studied the definitions of from Kincaid's book and I have found two theorems that says that: $\textbf{Theorem 1:}$ Let $p$ be a polynomial satisfying $p(0) \neq 0$. Then a basis for the null space of $p(E)$ is obtained as follows. With each zero $\lambda$ of $p$ having multiplicity $k$, associate the $k$ basic solutions $x(\lambda), x^{'}(\lambda), \dots, x^{k-1}(\lambda)$, where $x(\lambda) = [\lambda, \lambda^2, \lambda^3, \dots]$. $\textbf{Theorem 2:}$ For a polynomial p satisfying $p(0) \neq 0$, these properties are equivalent: (i) The difference equation $p(E)x = 0$ is stable (ii) All zeros of $p$ satisfy $|z| \leq 1$, and all multiple zeros satisfy $|z|< 1$ but I don't know how to start, at the beginingI was thinking to show that $P(E)=\sum_{i=0}^n c_iE^i /neq 0$, but I am not sure if this is the way, any suggestions? Thank u","For a Prep exam Exercise from the book: Numerical analysis of scientific computing. Section 1.3-3 Let $p$ be a polynomial of degree $m$, with $p(0) \neq 0$. If a sequence $x$ contains $m$ consecutive zeros and $p(E)x = 0$, then $x=0$ Where: $E$ denotes the displacement operator, $Ex=[x_2, x_3, \dots]$ where $x = [x_1, x_2, \dots]$ $L: V \to V$ $L=P(E) = \sum_{i=0}^nc_i E^i$ where $P(\lambda) = \sum_{i=0}^m c_i \lambda^i$ I've tried: I have studied the definitions of from Kincaid's book and I have found two theorems that says that: $\textbf{Theorem 1:}$ Let $p$ be a polynomial satisfying $p(0) \neq 0$. Then a basis for the null space of $p(E)$ is obtained as follows. With each zero $\lambda$ of $p$ having multiplicity $k$, associate the $k$ basic solutions $x(\lambda), x^{'}(\lambda), \dots, x^{k-1}(\lambda)$, where $x(\lambda) = [\lambda, \lambda^2, \lambda^3, \dots]$. $\textbf{Theorem 2:}$ For a polynomial p satisfying $p(0) \neq 0$, these properties are equivalent: (i) The difference equation $p(E)x = 0$ is stable (ii) All zeros of $p$ satisfy $|z| \leq 1$, and all multiple zeros satisfy $|z|< 1$ but I don't know how to start, at the beginingI was thinking to show that $P(E)=\sum_{i=0}^n c_iE^i /neq 0$, but I am not sure if this is the way, any suggestions? Thank u",,"['linear-algebra', 'ordinary-differential-equations', 'numerical-methods']"
58,showing existence and uniqueness of solution of $y'(t)=\frac1{1+|y(t)|}$,showing existence and uniqueness of solution of,y'(t)=\frac1{1+|y(t)|},"Given \begin{align*} y'(t)&=\frac1{1+|y(t)|},&y(0)=y_0&&\textrm{for }t\in[a,b]  \end{align*} I want to show that this IVP has a unique solution My attempt: We get $f(t,x)=\frac1{1+|x|}$. If $f$ is continuous there exists a solution on $[a,b]$ by Peano. If $f$ is Lipschitz-continuous, the solution is unique. Since fractions and $|\cdot|$ are continous and $|\cdot|\geq0$, $f$ is continuous and so there exists a solution by Peano, right? Now showing Lipschitz-continuity: $$|f(t,x)-f(t,y)|=\left|\frac1{1+|x|}+\frac1{1+|y|}\right|=\left|\frac{1+|y|+1+|x|}{(1+|x|)(1+|y|)}\right|$$ but now I am stuck. How do you get $\leq L|x-y|$ ?","Given \begin{align*} y'(t)&=\frac1{1+|y(t)|},&y(0)=y_0&&\textrm{for }t\in[a,b]  \end{align*} I want to show that this IVP has a unique solution My attempt: We get $f(t,x)=\frac1{1+|x|}$. If $f$ is continuous there exists a solution on $[a,b]$ by Peano. If $f$ is Lipschitz-continuous, the solution is unique. Since fractions and $|\cdot|$ are continous and $|\cdot|\geq0$, $f$ is continuous and so there exists a solution by Peano, right? Now showing Lipschitz-continuity: $$|f(t,x)-f(t,y)|=\left|\frac1{1+|x|}+\frac1{1+|y|}\right|=\left|\frac{1+|y|+1+|x|}{(1+|x|)(1+|y|)}\right|$$ but now I am stuck. How do you get $\leq L|x-y|$ ?",,['real-analysis']
59,Show that there exists a $k>o$ such that solutions of this system of differential equations never cross the line $y = kx$.,Show that there exists a  such that solutions of this system of differential equations never cross the line .,k>o y = kx,"For the system: $\frac{d x}{d t} = -y$ $\frac{d y}{d t} = x(1-x) - Ay$ Where $A \geq 2$.  I want to show that there exists a $k > 0$ such that $(x(t), y(t))$ cannot cross through the line $\{ y = k x, x > 0 \}$ when we start below this line. I started with showing that for the line $y = k x$ we have that $\frac{d y}{d t} = k \frac{d x}{ d t} = -k y$, and now I wanted to show that if we start below this line, we have a lower $\frac{dy}{dt}$ so we can never cross the line. But I have been unsuccesful. Any ideas? Thanks.","For the system: $\frac{d x}{d t} = -y$ $\frac{d y}{d t} = x(1-x) - Ay$ Where $A \geq 2$.  I want to show that there exists a $k > 0$ such that $(x(t), y(t))$ cannot cross through the line $\{ y = k x, x > 0 \}$ when we start below this line. I started with showing that for the line $y = k x$ we have that $\frac{d y}{d t} = k \frac{d x}{ d t} = -k y$, and now I wanted to show that if we start below this line, we have a lower $\frac{dy}{dt}$ so we can never cross the line. But I have been unsuccesful. Any ideas? Thanks.",,['ordinary-differential-equations']
60,Use Laplace transform to solve the following initial–value problems.,Use Laplace transform to solve the following initial–value problems.,,"Use Laplace transform to solve the following initial–value problem. $y′′′′ + 2y′′ + y = 0, y(0) = 1, y′(0) = −1, y′′(0) = 0, y′′′(0) = 2$ Answer $s^4 L(s) - s^3y(0) -s^2 y'(0) - s y''(0) - y'''(0) +2[s^2L(s)-sy(0)-y'(0)] +L(s) \\\\$ I get the partial fraction part and got stuck, need help! $L(s) =\frac{s^3 - s^2 +2s}{s^4 +2s^2 +1}= \frac{s-1}{s^2 +1}+\frac{s+1}{(s^2+1)^2}  \:\:$Factorising the denominator I get: $(s^2+1)^2$ Please some let me know if Im heading in the wrong direction here.","Use Laplace transform to solve the following initial–value problem. $y′′′′ + 2y′′ + y = 0, y(0) = 1, y′(0) = −1, y′′(0) = 0, y′′′(0) = 2$ Answer $s^4 L(s) - s^3y(0) -s^2 y'(0) - s y''(0) - y'''(0) +2[s^2L(s)-sy(0)-y'(0)] +L(s) \\\\$ I get the partial fraction part and got stuck, need help! $L(s) =\frac{s^3 - s^2 +2s}{s^4 +2s^2 +1}= \frac{s-1}{s^2 +1}+\frac{s+1}{(s^2+1)^2}  \:\:$Factorising the denominator I get: $(s^2+1)^2$ Please some let me know if Im heading in the wrong direction here.",,"['ordinary-differential-equations', 'laplace-transform']"
61,"Can I conclude that, if $v=0$ somewhere, then $v=0$ everywhere?","Can I conclude that, if  somewhere, then  everywhere?",v=0 v=0,"In solving a homework problem, I have encountered the following DE ($v$ is a function of $x$). $$v' = \frac{1}{x}\frac{1 + x^2}{1-x^2} v, \quad x \in (0,1)$$ I'd like to split the problem into two cases. $v=0$ somewhere $v=0$ nowhere For Case 2, I can solve the problem by taking $v$ to the LHS and choosing an appropriate substitution. However, Case 1 is giving me a bit of grief. I want to show that, assuming the hypothesis of Case 1, the only solution is the zero function. Question A. Can I conclude that, if $v=0$ somewhere, then $v=0$ on the entire interval? Question B. Is there a general principle that works for all DE's of the form $$v'(x) = f(x)v(x),\; x \in I$$ where $f$ is locally Lipschitz continuous and $I \subseteq \mathbb{R}$ is an open interval, allowing us to conclude that if $v=0$ somewhere, then $v=0$ everywhere? Please keep answers as non-technical as possible; I have received answers to similar questions in the past, and had a lot of trouble understanding them.","In solving a homework problem, I have encountered the following DE ($v$ is a function of $x$). $$v' = \frac{1}{x}\frac{1 + x^2}{1-x^2} v, \quad x \in (0,1)$$ I'd like to split the problem into two cases. $v=0$ somewhere $v=0$ nowhere For Case 2, I can solve the problem by taking $v$ to the LHS and choosing an appropriate substitution. However, Case 1 is giving me a bit of grief. I want to show that, assuming the hypothesis of Case 1, the only solution is the zero function. Question A. Can I conclude that, if $v=0$ somewhere, then $v=0$ on the entire interval? Question B. Is there a general principle that works for all DE's of the form $$v'(x) = f(x)v(x),\; x \in I$$ where $f$ is locally Lipschitz continuous and $I \subseteq \mathbb{R}$ is an open interval, allowing us to conclude that if $v=0$ somewhere, then $v=0$ everywhere? Please keep answers as non-technical as possible; I have received answers to similar questions in the past, and had a lot of trouble understanding them.",,['ordinary-differential-equations']
62,Finding a Hopf Bifucation with eigenvalues,Finding a Hopf Bifucation with eigenvalues,,"I am trying to show that the following 2D system has a Hopf bifurcation at $\lambda=0$: \begin{align}                                                                                                                                                                 x' =& y + \lambda x \\                                                                                                                                                        y' =& -x + \lambda y - x^2y                                                                                                                                                   \end{align} I know that I could easily plot the system with a CAS but I wish to analytical methods. So, I took the Jacobian: \begin{equation}                                                                                                                                                                J = \begin{pmatrix} \lambda&1\\-1-2xy&\lambda-x^2\end{pmatrix}                                                                                                              \end{equation} My book says I should look at the eigenvalues of the Jacobian and find where the real part of the eigenvalue switches from $-$ to $+$. This would correspond to where the system changes stability. So I took the $\det(J)$: \begin{align}                                                                                                                                                                   \det(J) =& -\lambda x^2 + 2xy + \lambda^2 + 1 = 0                                                                                                                           \end{align} I am stuck here with algebra and am not quite sure how to find out where the eigenvalues switch from negative real part to positive real part. I would like to use the quadratic formula but the $2xy$ term throws me off. How do I proceed? Thanks for all the help!","I am trying to show that the following 2D system has a Hopf bifurcation at $\lambda=0$: \begin{align}                                                                                                                                                                 x' =& y + \lambda x \\                                                                                                                                                        y' =& -x + \lambda y - x^2y                                                                                                                                                   \end{align} I know that I could easily plot the system with a CAS but I wish to analytical methods. So, I took the Jacobian: \begin{equation}                                                                                                                                                                J = \begin{pmatrix} \lambda&1\\-1-2xy&\lambda-x^2\end{pmatrix}                                                                                                              \end{equation} My book says I should look at the eigenvalues of the Jacobian and find where the real part of the eigenvalue switches from $-$ to $+$. This would correspond to where the system changes stability. So I took the $\det(J)$: \begin{align}                                                                                                                                                                   \det(J) =& -\lambda x^2 + 2xy + \lambda^2 + 1 = 0                                                                                                                           \end{align} I am stuck here with algebra and am not quite sure how to find out where the eigenvalues switch from negative real part to positive real part. I would like to use the quadratic formula but the $2xy$ term throws me off. How do I proceed? Thanks for all the help!",,"['ordinary-differential-equations', 'eigenvalues-eigenvectors', 'bifurcation']"
63,Problem checking that a fuction verifies the Laplace equation.,Problem checking that a fuction verifies the Laplace equation.,,"I'm having trouble solving an excersive from a past final that goes like this: Prove that if $u(x,y) \in \mathbb{C}^2$ satisfies $u_{xx} + u_{yy} = 0$ in $\mathbb{R}² - \{(0,0)\} $ then $v(x,y) = u( \frac{x}{x² + y²} ; \frac{y}{x²+y²})$ also satisfies it. Here's what I did: Let $ \alpha (x,y) = ( \frac{x}{x² + y²} , \frac{y}{x²+y²}) $, so $ \frac{\partial v}{\partial x} = \frac{\partial u}{\partial x} ({\alpha(x,y)}) \frac{\partial \alpha_1}{\partial x} + \frac{\partial u}{\partial y} ({\alpha(x,y)}) \frac{\partial \alpha_2}{\partial x}$ $ \frac{\partial² v}{\partial x²} = \frac{\partial^2 u}{\partial x²} ({\alpha(x,y)}) \frac{\partial \alpha_1}{\partial x} + \frac{\partial u}{\partial x} ({\alpha(x,y)}) \frac{\partial² \alpha_1}{\partial x²} + \frac{\partial^2 u}{\partial xy} ({\alpha(x,y)}) \frac{\partial \alpha_2}{\partial x} + \frac{\partial u}{\partial y} ({\alpha(x,y)}) \frac{\partial² \alpha_2}{\partial x²}$ Likewise for $y$. I did all the operations and went for $ \frac{\partial² v}{\partial x²} + \frac{\partial² v}{\partial y²} $ but couldn't actually make it equal to $0$, so I'm wondering if this is actually the right way to go. Thanks in advance for any help.","I'm having trouble solving an excersive from a past final that goes like this: Prove that if $u(x,y) \in \mathbb{C}^2$ satisfies $u_{xx} + u_{yy} = 0$ in $\mathbb{R}² - \{(0,0)\} $ then $v(x,y) = u( \frac{x}{x² + y²} ; \frac{y}{x²+y²})$ also satisfies it. Here's what I did: Let $ \alpha (x,y) = ( \frac{x}{x² + y²} , \frac{y}{x²+y²}) $, so $ \frac{\partial v}{\partial x} = \frac{\partial u}{\partial x} ({\alpha(x,y)}) \frac{\partial \alpha_1}{\partial x} + \frac{\partial u}{\partial y} ({\alpha(x,y)}) \frac{\partial \alpha_2}{\partial x}$ $ \frac{\partial² v}{\partial x²} = \frac{\partial^2 u}{\partial x²} ({\alpha(x,y)}) \frac{\partial \alpha_1}{\partial x} + \frac{\partial u}{\partial x} ({\alpha(x,y)}) \frac{\partial² \alpha_1}{\partial x²} + \frac{\partial^2 u}{\partial xy} ({\alpha(x,y)}) \frac{\partial \alpha_2}{\partial x} + \frac{\partial u}{\partial y} ({\alpha(x,y)}) \frac{\partial² \alpha_2}{\partial x²}$ Likewise for $y$. I did all the operations and went for $ \frac{\partial² v}{\partial x²} + \frac{\partial² v}{\partial y²} $ but couldn't actually make it equal to $0$, so I'm wondering if this is actually the right way to go. Thanks in advance for any help.",,"['calculus', 'ordinary-differential-equations', 'multivariable-calculus']"
64,Finding a Liapunov function. Is my analysis correct?,Finding a Liapunov function. Is my analysis correct?,,"Consider the following system of differential equations $x'=f(x)$: \begin{equation} x_1' = -x_1 -x_2^3 \\ x_2' = x_1-x_2 \end{equation} Here is the Liapunov function I wish to use: $V(x_1, x_2) = \alpha x_1^2 + \beta x_2^4$. So I need to find $\alpha$ and $\beta$ that satisfy the following 3 Liapunov conditions: The conditions to be a Liapunov funtion w.r.t the origin $x_0$: 1) $V(0, 0) = 0$ Condition 1) is clearly true. 2) $V(x) > 0$ for all $x\neq x_0$ This is clearly true for $\alpha \geq 0$ and $\beta \geq 0$ but not both equal to $0$. 3) $\nabla V\cdot f \leq 0$ for all $x$: \begin{align} \nabla V\cdot f &= 2\alpha x_1(-x_1-x_2^3) + 4\beta x_2 (x_1-x_2)\\                 &= -2\alpha x_1^2 -2\alpha x_1x_2^3 + 4\beta x_1x_2 - 4\beta x_2^2\\                 &= -4\beta x_2^2 -2\alpha x_1^2 -2\alpha x_1x_2^3 +4\beta x_1x_2\\                 &= -4\beta x_2^2 -2\alpha + x_1x_2(-2\alpha x_2^2+4\beta) \end{align} In order for condition 3) to hold, that is $\nabla V(x)\cdot f \leq 0$ for all $x$, then the third term must equal $0$. That is we need: \begin{equation} -2\alpha x_2^2+4\beta =0 \end{equation} If this is the case, all solutions depend on $x_2$ unless both $\alpha$ and $\beta$ equal $0$ which cannot happen as condition 2) would not be satisfied. This is where I am stuck. Please help!","Consider the following system of differential equations $x'=f(x)$: \begin{equation} x_1' = -x_1 -x_2^3 \\ x_2' = x_1-x_2 \end{equation} Here is the Liapunov function I wish to use: $V(x_1, x_2) = \alpha x_1^2 + \beta x_2^4$. So I need to find $\alpha$ and $\beta$ that satisfy the following 3 Liapunov conditions: The conditions to be a Liapunov funtion w.r.t the origin $x_0$: 1) $V(0, 0) = 0$ Condition 1) is clearly true. 2) $V(x) > 0$ for all $x\neq x_0$ This is clearly true for $\alpha \geq 0$ and $\beta \geq 0$ but not both equal to $0$. 3) $\nabla V\cdot f \leq 0$ for all $x$: \begin{align} \nabla V\cdot f &= 2\alpha x_1(-x_1-x_2^3) + 4\beta x_2 (x_1-x_2)\\                 &= -2\alpha x_1^2 -2\alpha x_1x_2^3 + 4\beta x_1x_2 - 4\beta x_2^2\\                 &= -4\beta x_2^2 -2\alpha x_1^2 -2\alpha x_1x_2^3 +4\beta x_1x_2\\                 &= -4\beta x_2^2 -2\alpha + x_1x_2(-2\alpha x_2^2+4\beta) \end{align} In order for condition 3) to hold, that is $\nabla V(x)\cdot f \leq 0$ for all $x$, then the third term must equal $0$. That is we need: \begin{equation} -2\alpha x_2^2+4\beta =0 \end{equation} If this is the case, all solutions depend on $x_2$ unless both $\alpha$ and $\beta$ equal $0$ which cannot happen as condition 2) would not be satisfied. This is where I am stuck. Please help!",,"['ordinary-differential-equations', 'lyapunov-functions']"
65,Centre of mass moves with constant velocity,Centre of mass moves with constant velocity,,"The centre of mass of the Newton $n$-body problem is given by $$S=\frac{1}{M} \sum m_ix_i$$ with $M=\sum m_i$. Show that it moves with contant speed and hence has no acceleration. I don't understand as if I differentiate, I'll surely just get $$S'=\frac{1}{M} \sum m_ix'_i$$ which is not constant...is it?","The centre of mass of the Newton $n$-body problem is given by $$S=\frac{1}{M} \sum m_ix_i$$ with $M=\sum m_i$. Show that it moves with contant speed and hence has no acceleration. I don't understand as if I differentiate, I'll surely just get $$S'=\frac{1}{M} \sum m_ix'_i$$ which is not constant...is it?",,"['ordinary-differential-equations', 'summation', 'classical-mechanics']"
66,Solution to $y'' - 2y = 2\tan^3x$,Solution to,y'' - 2y = 2\tan^3x,"I'm struggling with this nonhomogeneous second order differential equation $$y'' - 2y = 2\tan^3x$$ I assumed that the form of the solution would be $A\tan^3x$ where A was some constant, but this results in a mess when solving. The back of the book reports that the solution is simply $y(x) = \tan x$ . Can someone explain why they chose the form $A\tan x$ instead of $A\tan^3x$ ? Thanks in advance.","I'm struggling with this nonhomogeneous second order differential equation I assumed that the form of the solution would be where A was some constant, but this results in a mess when solving. The back of the book reports that the solution is simply . Can someone explain why they chose the form instead of ? Thanks in advance.",y'' - 2y = 2\tan^3x A\tan^3x y(x) = \tan x A\tan x A\tan^3x,['ordinary-differential-equations']
67,Question about linear dependence and independence by using Wronskian,Question about linear dependence and independence by using Wronskian,,"Here is the theorem I use: Two solutions $\phi_1$, $\phi_2$ of $L(y)=y''+a_1y'+a_2y=0$, where $a_1$ and $a_2$ are constants, are linearly independent on an interval $I$ if, and only if, the Wronskain $W(\phi_1,\phi_2)\ne0$ for all $x\in I.$ Then, consider the functions: $$\phi_1(x)=x,\phi_2(x)=|x|, x\in(-\infty,\infty). $$Are they linearly dependent ot independent? My answer is that they are linearly dependent since when $x\ge0$, $\phi_1(x)=x=\phi_2(x)$ and I plug them and the corresponding derivatives into the Wronskian, $W=0$ for all $x\ge0$; and also check when $x\lt0$, the Wronskian also equals to 0, thus $\phi_1$ and $\phi_2$ here are linear dependent. But the answer in the back of the book says they are linearly independent. Where went wrong? A similar example is $$\phi_1(x)=x^2, \phi_2(x)=x|x|, x\in(-\infty,\infty).$$The answer is also linear independence, but I think they are linear dependence. Thanks for your answer.","Here is the theorem I use: Two solutions $\phi_1$, $\phi_2$ of $L(y)=y''+a_1y'+a_2y=0$, where $a_1$ and $a_2$ are constants, are linearly independent on an interval $I$ if, and only if, the Wronskain $W(\phi_1,\phi_2)\ne0$ for all $x\in I.$ Then, consider the functions: $$\phi_1(x)=x,\phi_2(x)=|x|, x\in(-\infty,\infty). $$Are they linearly dependent ot independent? My answer is that they are linearly dependent since when $x\ge0$, $\phi_1(x)=x=\phi_2(x)$ and I plug them and the corresponding derivatives into the Wronskian, $W=0$ for all $x\ge0$; and also check when $x\lt0$, the Wronskian also equals to 0, thus $\phi_1$ and $\phi_2$ here are linear dependent. But the answer in the back of the book says they are linearly independent. Where went wrong? A similar example is $$\phi_1(x)=x^2, \phi_2(x)=x|x|, x\in(-\infty,\infty).$$The answer is also linear independence, but I think they are linear dependence. Thanks for your answer.",,"['linear-algebra', 'ordinary-differential-equations']"
68,"Velocity, Wave Equation, Differential Equations","Velocity, Wave Equation, Differential Equations",,Suppose you have a differential equation of the form: $$ \frac{\partial^2 u}{\partial z^2} = C \frac{\partial^2 u}{\partial t^2} + D \frac{\partial u}{\partial t}$$ Is it possible to find the velocity from this? For a normal wave equation: $$ \frac{\partial^2 u}{\partial z^2} = C \frac{\partial^2 u}{\partial t^2} $$ The velocity would be given by: $$\frac{1}{\sqrt{C}}$$ Is it possible to identify the velocity in the former case?,Suppose you have a differential equation of the form: $$ \frac{\partial^2 u}{\partial z^2} = C \frac{\partial^2 u}{\partial t^2} + D \frac{\partial u}{\partial t}$$ Is it possible to find the velocity from this? For a normal wave equation: $$ \frac{\partial^2 u}{\partial z^2} = C \frac{\partial^2 u}{\partial t^2} $$ The velocity would be given by: $$\frac{1}{\sqrt{C}}$$ Is it possible to identify the velocity in the former case?,,"['ordinary-differential-equations', 'partial-differential-equations']"
69,Problem related with solving ODE,Problem related with solving ODE,,"I was solving old exam papers and am stuck on the following problem: Consider the system of ODE $\frac {d}{dx}Y=AY,Y(0)=\begin{pmatrix} 2\\  -1 \end{pmatrix}$ where $A=\begin{pmatrix} 1 &2 \\   0&-1  \end{pmatrix},Y=\begin{pmatrix} y_1(x)\\  y_2(x) \end{pmatrix}$.  Then I have to determine which of the following options hold good. $y_1(x) \to \infty,y_2(x) \to 0\,\,$ as $\,\,x \to \infty$ $y_1(x) \to 0,y_2(x) \to 0\,\,$ as $\,\,x \to \infty$ $y_1(x) \to \infty,y_2(x) \to -\infty \,\,$ as $\,\,x \to -\infty$ $y_1(x),y_2(x) \to -\infty \,\,$ as $\,\,x \to -\infty$ . My Attempt: Using the eigenvalues of $A$,I get the ODE of the form: $y(x)=c_1e^x+c_2e^{-x},$ where $c_1,c_2$ being arbitrary constants to be determined .  Here,I am stuck.I could not use the fact that $Y(0)=\begin{pmatrix} 2\\  -1 \end{pmatrix}$ for finding $c_1,c_2$. Can someone point me in the right direction with some explanation? Thanks in advance for your time.","I was solving old exam papers and am stuck on the following problem: Consider the system of ODE $\frac {d}{dx}Y=AY,Y(0)=\begin{pmatrix} 2\\  -1 \end{pmatrix}$ where $A=\begin{pmatrix} 1 &2 \\   0&-1  \end{pmatrix},Y=\begin{pmatrix} y_1(x)\\  y_2(x) \end{pmatrix}$.  Then I have to determine which of the following options hold good. $y_1(x) \to \infty,y_2(x) \to 0\,\,$ as $\,\,x \to \infty$ $y_1(x) \to 0,y_2(x) \to 0\,\,$ as $\,\,x \to \infty$ $y_1(x) \to \infty,y_2(x) \to -\infty \,\,$ as $\,\,x \to -\infty$ $y_1(x),y_2(x) \to -\infty \,\,$ as $\,\,x \to -\infty$ . My Attempt: Using the eigenvalues of $A$,I get the ODE of the form: $y(x)=c_1e^x+c_2e^{-x},$ where $c_1,c_2$ being arbitrary constants to be determined .  Here,I am stuck.I could not use the fact that $Y(0)=\begin{pmatrix} 2\\  -1 \end{pmatrix}$ for finding $c_1,c_2$. Can someone point me in the right direction with some explanation? Thanks in advance for your time.",,['ordinary-differential-equations']
70,stability and asymptotic stability: unstable but asymptotically convergent solution of nonlinear system,stability and asymptotic stability: unstable but asymptotically convergent solution of nonlinear system,,"Consider nonlinear systems of the form $X(t)'=F(X(t))$, where $F$ is smooth (assume $C^\infty$). Is it possible to construct such a system (preferably planar system) so that $X_0$ is an unstable equilibrium, but all nearby solution curves tend to $X_0$ as $t \to \infty$? If so, how? A conceptual construction is enough. What will the phase portrait look like?","Consider nonlinear systems of the form $X(t)'=F(X(t))$, where $F$ is smooth (assume $C^\infty$). Is it possible to construct such a system (preferably planar system) so that $X_0$ is an unstable equilibrium, but all nearby solution curves tend to $X_0$ as $t \to \infty$? If so, how? A conceptual construction is enough. What will the phase portrait look like?",,['ordinary-differential-equations']
71,Method of Averaging an ODE,Method of Averaging an ODE,,"A few weeks ago, I was asked the following in a homework assignment Study the system $\dot{x}(t)=-\epsilon x(t)\cos(t)$ by the method of averaging and compare this to the exact solution.  My exact solution is $$x(t)=x(0)\exp{[-\epsilon \sin(t)}]$$ This is very straight forward to show (this is a separable ODE).  But I'm not too sure about the method of averaging. There is a section in Arnold's Mechanics on this..... which I don't understand.  Any ideas greatly appreciated. A partial result is that the solution should look like the following: $$x(t) \approx x(0)(1-\epsilon t)$$ This can be obtained by Taylor expanding the exact solution of the exponential, throwing out terms squared or higher, and approximating $\sin(t)$ by t. Here is what Arnold's book says: Let $I, \varphi$ be action-angle variables in an integrable ('nonperturbed') system with Hamiltonian function $H_0(I)$: $$\dot{I}=0 ;\dot{\varphi}=\omega(I);\omega(I)=\frac{\partial H_0}{\partial I}$$ As the nearby ""perturbed"" system we take the system $$\dot{\varphi}=\omega(I)+\epsilon f(I,\varphi);  I=\epsilon g(I,\varphi) : \epsilon<<1$$ The $ averaging$ $principle$  $for$  $the$  $system$ consists of its replacement by another system, called the averaged system: $$\dot{J}=\epsilon \hat{g}(J); \hat{g}(J)=(2\pi)^{-k} \int_0^{2k}\cdots\int_0^{2k} g(J,\varphi) d\varphi _1,... d\varphi _k$$ I have a little bit of an idea how to do this... but not too clear.","A few weeks ago, I was asked the following in a homework assignment Study the system $\dot{x}(t)=-\epsilon x(t)\cos(t)$ by the method of averaging and compare this to the exact solution.  My exact solution is $$x(t)=x(0)\exp{[-\epsilon \sin(t)}]$$ This is very straight forward to show (this is a separable ODE).  But I'm not too sure about the method of averaging. There is a section in Arnold's Mechanics on this..... which I don't understand.  Any ideas greatly appreciated. A partial result is that the solution should look like the following: $$x(t) \approx x(0)(1-\epsilon t)$$ This can be obtained by Taylor expanding the exact solution of the exponential, throwing out terms squared or higher, and approximating $\sin(t)$ by t. Here is what Arnold's book says: Let $I, \varphi$ be action-angle variables in an integrable ('nonperturbed') system with Hamiltonian function $H_0(I)$: $$\dot{I}=0 ;\dot{\varphi}=\omega(I);\omega(I)=\frac{\partial H_0}{\partial I}$$ As the nearby ""perturbed"" system we take the system $$\dot{\varphi}=\omega(I)+\epsilon f(I,\varphi);  I=\epsilon g(I,\varphi) : \epsilon<<1$$ The $ averaging$ $principle$  $for$  $the$  $system$ consists of its replacement by another system, called the averaged system: $$\dot{J}=\epsilon \hat{g}(J); \hat{g}(J)=(2\pi)^{-k} \int_0^{2k}\cdots\int_0^{2k} g(J,\varphi) d\varphi _1,... d\varphi _k$$ I have a little bit of an idea how to do this... but not too clear.",,"['ordinary-differential-equations', 'classical-mechanics']"
72,General solution of $y'' + \frac{\sin x}{1 + \cos x}y' + \frac{1}{1 + \cos x}y =0$,General solution of,y'' + \frac{\sin x}{1 + \cos x}y' + \frac{1}{1 + \cos x}y =0,I was hoping to get some help with this question. How do you find the general solution of  $$ y'' + \frac{\sin x}{1 + \cos x}y' + \frac{1}{1 + \cos x}y =0. $$  I'm not used to dealing with complex trig functions. Your help will be much appreciated.,I was hoping to get some help with this question. How do you find the general solution of  $$ y'' + \frac{\sin x}{1 + \cos x}y' + \frac{1}{1 + \cos x}y =0. $$  I'm not used to dealing with complex trig functions. Your help will be much appreciated.,,['ordinary-differential-equations']
73,Power Series Solution for $e^xy''+xy=0$,Power Series Solution for,e^xy''+xy=0,"$$e^xy''+xy=0$$ How do I find the power series solution to this equation, or rather, how should I go about dealing with the $e^x$? Thanks!","$$e^xy''+xy=0$$ How do I find the power series solution to this equation, or rather, how should I go about dealing with the $e^x$? Thanks!",,['ordinary-differential-equations']
74,Picard iteration (general),Picard iteration (general),,"This a general question about Picard iterations and is as follows. Let A be a $n\times n$ matrix. show that the Picard method for solving $X^{'}=AX$, $X(0)=X_{0}$ gives the solution $e^{tA}X_{0}$ I really don't even have a clue where to start? i have used it to do some very simple problems but im totally stumped on this one.","This a general question about Picard iterations and is as follows. Let A be a $n\times n$ matrix. show that the Picard method for solving $X^{'}=AX$, $X(0)=X_{0}$ gives the solution $e^{tA}X_{0}$ I really don't even have a clue where to start? i have used it to do some very simple problems but im totally stumped on this one.",,"['ordinary-differential-equations', 'dynamical-systems']"
75,Solution to the second order differential equation,Solution to the second order differential equation,,"Hello i have read in a book that second order diferential equation of this form ($\psi$ is a function of $x$): $$ \frac{d^2 \psi}{dx^2} = - k^2\, \psi $$ describes a simple harmonic oscilator and the solution to this second order differential equation is of form: $$ \psi = A \sin(k x) + B \cos(kx) $$ This solution is generaly known, but i want to know the background on how it is calculated out of the first equation.","Hello i have read in a book that second order diferential equation of this form ($\psi$ is a function of $x$): $$ \frac{d^2 \psi}{dx^2} = - k^2\, \psi $$ describes a simple harmonic oscilator and the solution to this second order differential equation is of form: $$ \psi = A \sin(k x) + B \cos(kx) $$ This solution is generaly known, but i want to know the background on how it is calculated out of the first equation.",,['ordinary-differential-equations']
76,Damped Harmonic Oscillator [ Math not physics question],Damped Harmonic Oscillator [ Math not physics question],,"Question Consider, The motion of a a damped harmonic oscillator is described by $x^{''}$ + $bx^{'}$ + $kx = 0$, where b is greater than or equal to 0 A) Rewrite as a two dimensional linear system. $ x^{'} = y$ $y^{'}$ = - $(by + kx)$ B) Sketch the analogue of the trace-determinant plane in the bk-plane. That is, identify the regions in the relevant portion of the bk-plane where the corresponding system has similar phase portraits. I have cut out most of my answer etc as it seems to of merely confused people, how does one answer this question?","Question Consider, The motion of a a damped harmonic oscillator is described by $x^{''}$ + $bx^{'}$ + $kx = 0$, where b is greater than or equal to 0 A) Rewrite as a two dimensional linear system. $ x^{'} = y$ $y^{'}$ = - $(by + kx)$ B) Sketch the analogue of the trace-determinant plane in the bk-plane. That is, identify the regions in the relevant portion of the bk-plane where the corresponding system has similar phase portraits. I have cut out most of my answer etc as it seems to of merely confused people, how does one answer this question?",,"['ordinary-differential-equations', 'dynamical-systems']"
77,general solution of the differential equation $\frac{dy}{dx} + \frac{x+y+a}{x+y+b}=0$,general solution of the differential equation,\frac{dy}{dx} + \frac{x+y+a}{x+y+b}=0,"I'm trying to find the general solution of the differential equation $\frac{dy}{dx} + \frac{x+y+a}{x+y+b}=0$ where a and b are constants. I have tried puttting z=x+y thus, $\frac{dz}{dx} = 1 + \frac{dy}{dx}$. I subbed this into the equation to get $\frac{dz}{dx} = 1 -(\frac{z+a}{z+b})$ which I simplified to $\frac{b-a}{z+b}$ and I separated variables and integrated giving: $$\frac{z^2}{2} +bz = (b-a)x +c$$ I then multiplied through by 2 and subbed z=x+y back however this did not give me the correct answer which is $(x+y+b)^2 = 2(b-a)(x+c)$. Does anyone know how to get to this answer?","I'm trying to find the general solution of the differential equation $\frac{dy}{dx} + \frac{x+y+a}{x+y+b}=0$ where a and b are constants. I have tried puttting z=x+y thus, $\frac{dz}{dx} = 1 + \frac{dy}{dx}$. I subbed this into the equation to get $\frac{dz}{dx} = 1 -(\frac{z+a}{z+b})$ which I simplified to $\frac{b-a}{z+b}$ and I separated variables and integrated giving: $$\frac{z^2}{2} +bz = (b-a)x +c$$ I then multiplied through by 2 and subbed z=x+y back however this did not give me the correct answer which is $(x+y+b)^2 = 2(b-a)(x+c)$. Does anyone know how to get to this answer?",,['ordinary-differential-equations']
78,Please help me understand this. $\frac{dx}{dt} = S x (a-x)$. What does it mean for some constant $S$? How to find $x$ for fastest/slowest growth?,Please help me understand this. . What does it mean for some constant ? How to find  for fastest/slowest growth?,\frac{dx}{dt} = S x (a-x) S x,"I am having some trouble understanding this problem. There is this function that calculates reaction rate of a substance for some constant positive $S$. $a$ = original amount of the first substance  $x$ = some amount of substance First question, what does $\frac{dx}{dt} = S x (a-x)$ mean? Does this mean that the rate of change of $x$ in the equation $(S x (a-x))$ is affected by the change in time? So if there was no '$x$' in the equation, than change in time would not affect the equation right? *This is the first time I am encountering '$dt$' in my derivative assignments. When finding derivatives for simple equations, its mostly been of d/dx notation. When I graphed $S x (a-x)$, I substituted random numbers for $S$ and $a$. Does this tell me anything about the function for rate of increase and decrease? Should I have solved for $a$? I notice the function increases and then decreases as $x$ moves away from $0$. I also know that when the derivative crosses the $x$ axis, the original function (which I don't know) will start to decrease. What do I need to do to determine the fastest/slowest growth rate using the derivative?  Thanks","I am having some trouble understanding this problem. There is this function that calculates reaction rate of a substance for some constant positive $S$. $a$ = original amount of the first substance  $x$ = some amount of substance First question, what does $\frac{dx}{dt} = S x (a-x)$ mean? Does this mean that the rate of change of $x$ in the equation $(S x (a-x))$ is affected by the change in time? So if there was no '$x$' in the equation, than change in time would not affect the equation right? *This is the first time I am encountering '$dt$' in my derivative assignments. When finding derivatives for simple equations, its mostly been of d/dx notation. When I graphed $S x (a-x)$, I substituted random numbers for $S$ and $a$. Does this tell me anything about the function for rate of increase and decrease? Should I have solved for $a$? I notice the function increases and then decreases as $x$ moves away from $0$. I also know that when the derivative crosses the $x$ axis, the original function (which I don't know) will start to decrease. What do I need to do to determine the fastest/slowest growth rate using the derivative?  Thanks",,"['calculus', 'ordinary-differential-equations', 'derivatives']"
79,"If Wronskian is zero at some point in an interval, then the Wronskian is zero at all points in the interval","If Wronskian is zero at some point in an interval, then the Wronskian is zero at all points in the interval",,"Suppose that $y_1$ and $y_2$ are solutions to the homogeneous DE $$y^{\prime \prime}+p(x)y^{\prime}+q(x)y=0$$ on $I$ and assume that for some $x_0 \in I$ , we have $$W(y_1,y_2)(x_0)>0~.$$ Show that $$W(y_1,y_2)(x)>0\qquad \forall ~~x \in I~$$ I have no idea on how to start. Anyone can help ?","Suppose that and are solutions to the homogeneous DE on and assume that for some , we have Show that I have no idea on how to start. Anyone can help ?","y_1 y_2 y^{\prime \prime}+p(x)y^{\prime}+q(x)y=0 I x_0 \in I W(y_1,y_2)(x_0)>0~. W(y_1,y_2)(x)>0\qquad \forall ~~x \in I~","['ordinary-differential-equations', 'wronskian']"
80,How to differentiate discrete probabilities?,How to differentiate discrete probabilities?,,"Assuming that I have a function  $$f(p(x),p(c),p(x,c)) = \ln (p(x)p(c)) + \ln (p(x,c))$$ where $p(\cdot)$ are discrete probabilities, $x \in X, c \in C$ are random variables. So $p(x)=p(X=x)$ denotes probability of $x$ occurring, $p(x,c)=p(X=x,C=c)$ denotes probability of $x,c$ occurring together. If I differentiate this function with respect to $p(x,c)$, how do I differentiate the term $ln (p(x)p(c))$ wrt $p(x,c)$? Do I have to do partial differentiation  $$\frac{\partial \ln (p(x)p(c))}{\partial p(x)}\frac{\partial (p(x))}{\partial p(x,c)}+\frac{\partial \ln (p(x)p(c))}{\partial p(c)}\frac{\partial (p(c))}{\partial p(x,c)}$$ or differentiation of this term wrt $p(x,c)$ is zero?","Assuming that I have a function  $$f(p(x),p(c),p(x,c)) = \ln (p(x)p(c)) + \ln (p(x,c))$$ where $p(\cdot)$ are discrete probabilities, $x \in X, c \in C$ are random variables. So $p(x)=p(X=x)$ denotes probability of $x$ occurring, $p(x,c)=p(X=x,C=c)$ denotes probability of $x,c$ occurring together. If I differentiate this function with respect to $p(x,c)$, how do I differentiate the term $ln (p(x)p(c))$ wrt $p(x,c)$? Do I have to do partial differentiation  $$\frac{\partial \ln (p(x)p(c))}{\partial p(x)}\frac{\partial (p(x))}{\partial p(x,c)}+\frac{\partial \ln (p(x)p(c))}{\partial p(c)}\frac{\partial (p(c))}{\partial p(x,c)}$$ or differentiation of this term wrt $p(x,c)$ is zero?",,"['probability', 'ordinary-differential-equations']"
81,Classification of Differential Equations,Classification of Differential Equations,,"It's been quite a while since I last dealed with DE's. I'd appreciate if you could help me with the official, or usual, classification of the next DE's and/or if there are some definite methods to solve them. Hints will also be welcome: $$(1)\;\;\;\;\;\;\;\;y'=\frac{(y+2x-1)^2}{(4y+8x-6)(2y+4x-1)}\cdot\frac{1}{\sin\left(\frac{4y+8x-3}{y+2x-1}\right)}-2$$ $$(2)\;\;\;\;\;\;\;\;y'x+y\left(\ln^2x+\ln^2y-2\ln x\ln y\right)=0\;\;,\;x,y>0$$ I'm guessing here one could write $$\ln^2x+\ln^2y-2\ln x\ln y=\left(\ln x-\ln y\right)^2=\ln^2\frac{x}{y}$$ Thanks.","It's been quite a while since I last dealed with DE's. I'd appreciate if you could help me with the official, or usual, classification of the next DE's and/or if there are some definite methods to solve them. Hints will also be welcome: $$(1)\;\;\;\;\;\;\;\;y'=\frac{(y+2x-1)^2}{(4y+8x-6)(2y+4x-1)}\cdot\frac{1}{\sin\left(\frac{4y+8x-3}{y+2x-1}\right)}-2$$ $$(2)\;\;\;\;\;\;\;\;y'x+y\left(\ln^2x+\ln^2y-2\ln x\ln y\right)=0\;\;,\;x,y>0$$ I'm guessing here one could write $$\ln^2x+\ln^2y-2\ln x\ln y=\left(\ln x-\ln y\right)^2=\ln^2\frac{x}{y}$$ Thanks.",,['ordinary-differential-equations']
82,Dimension of kernel of differential operator,Dimension of kernel of differential operator,,"Consider the following differential equation. For $ y(t)\in C^\infty([0,1]) $, $$y''(t) - 5y'(t) + 6y(t) =0 $$ Here the solutions are $y(t) =Ae^{2t}$ or $A e^{3t}$ where A is a constant. I want to know the number of independent solutions of the above differential equation. In fact we already know the number. The number is two. This question can be reduced to the question about dimension of kernel of differential operator : $$ \frac{d^2}{dt^2} :  C^\infty([0,1]) \rightarrow C^\infty([0,1])$$ has the kernel of dimension 2. It is clear since $t^n$ is sended to $nt^{n-2}$. From the above argument we already know that the dimension of kernel of the following operator is at least 2 :  $$ \frac{d^2}{dt^2} -5 \frac{d}{dt} + 6 I:  C^\infty([0,1]) \rightarrow C^\infty([0,1])$$  where $I$ is an identity map. But how can we show that the dimension of kernel of the operator is two ?","Consider the following differential equation. For $ y(t)\in C^\infty([0,1]) $, $$y''(t) - 5y'(t) + 6y(t) =0 $$ Here the solutions are $y(t) =Ae^{2t}$ or $A e^{3t}$ where A is a constant. I want to know the number of independent solutions of the above differential equation. In fact we already know the number. The number is two. This question can be reduced to the question about dimension of kernel of differential operator : $$ \frac{d^2}{dt^2} :  C^\infty([0,1]) \rightarrow C^\infty([0,1])$$ has the kernel of dimension 2. It is clear since $t^n$ is sended to $nt^{n-2}$. From the above argument we already know that the dimension of kernel of the following operator is at least 2 :  $$ \frac{d^2}{dt^2} -5 \frac{d}{dt} + 6 I:  C^\infty([0,1]) \rightarrow C^\infty([0,1])$$  where $I$ is an identity map. But how can we show that the dimension of kernel of the operator is two ?",,['ordinary-differential-equations']
83,Transform the system into a single fourth-order ODE in either x or y,Transform the system into a single fourth-order ODE in either x or y,,"$2x'' = y' +y$ $y'' = -x'+x+3t^2$ I've always learned how to transform this to a system of first-order ODEs (downwards) but never up to higher differentials.  How can I solve this, and how does ""either x or y"" effect what the solution may be?","$2x'' = y' +y$ $y'' = -x'+x+3t^2$ I've always learned how to transform this to a system of first-order ODEs (downwards) but never up to higher differentials.  How can I solve this, and how does ""either x or y"" effect what the solution may be?",,['ordinary-differential-equations']
84,Implicit solution to an ODE,Implicit solution to an ODE,,"I have come accross a problem which leads to the following ODE: $\frac{dy}{dx}= \frac{y}{x}-\frac{1}{h}\frac{\sqrt{x^2+y^2}}{x}$, where $h>0$ is a parameter. I am not able to solve it, however maple gives an implicit  solution: $\frac{x^{1/h}y}{x}+\frac{x^{1/h}\sqrt{x^2+y^2}}{x}=$constant. I want to understand, how to $\textit{find}$ this implicit solution. (Of course one can differentiate it and find back the ODE.)","I have come accross a problem which leads to the following ODE: $\frac{dy}{dx}= \frac{y}{x}-\frac{1}{h}\frac{\sqrt{x^2+y^2}}{x}$, where $h>0$ is a parameter. I am not able to solve it, however maple gives an implicit  solution: $\frac{x^{1/h}y}{x}+\frac{x^{1/h}\sqrt{x^2+y^2}}{x}=$constant. I want to understand, how to $\textit{find}$ this implicit solution. (Of course one can differentiate it and find back the ODE.)",,"['real-analysis', 'ordinary-differential-equations']"
85,Comparable Graduate ODE Text Suggestions,Comparable Graduate ODE Text Suggestions,,"First off, I'm very sorry if this sort of question is not allowed here.  I've seen a couple similar questions on the OverFlow site, but I think discussions of basic material should be kept to this site. Anyway, in my ODE course we are using the introductory ODE text written by Jack Hale: http://www.amazon.com/Ordinary-Differential-Equations-Dover-Mathematics/dp/0486472116/ref=sr_1_1?ie=UTF8&qid=1350512056&sr=8-1&keywords=jack+Hale Actually, it's only introductory in the sense that it is self-contained, but it is rather advanced (think of those Dover reprints in size 8 font, half of which is written in Greek letters).  The table of contents and first pages give a hint of the level and material covered (see below). The proofs encountered in ODE seem to me VERY VERY unintuitive, in the sense that one (without experience) would be very unlikely to ever reproduce such proofs on their own.  Often they begin by establishing (without any motivation) estimates which are then used to prove things like ""now we see such and such is a nondecreasing,"" and only once three fourths through the proof does one see why it was even necessary to establish any of the prerequisite facts. I'm hoping this is just the style of the author, but perhaps this is just the specific taste of ODE theory. In any case, I would like some recommendations for texts that cover similar material on ODE theory that people here have found useful in the past .  There are many ODE texts, and they cover different parts of ODE theory.  This text (and our course) is aimed specifically at establishing the general theoretical framework for ODE theory (e.g. preliminaries on fixed point theorems/Banach spaces, Peano existence, Picard uniqueness, continuation of solutions, continuous dependence of parameters, differential estimates, further theory on linear systems, etc.), and then going straight to stability analysis (e.g. analysis of linear systems, perturbations of non-linear systems, Poincare-Bendixson theory, and Liapunov methods) and finally perturbation methods (e.g. asymptotic expansions, averaging, multiple scales, etc.).  In other words, this is not a course on elementary solution methods encountered in undergraduate courses, nor is it a course on advanced analytic topics such as Sturm-Louiville theory and eigenfunction expansions.  It is very much an ""applied"" course. Thank you in advance, and again I apologize if this is not strictly a ""Math Stackexchange"" question. EDIT (1): I know that several people have used Strogatz' non-linear dynamics text, which covers the ladder two topics discussed (actually, it covers very little on perturbation methods).  However, this text is extremely non-rigorous, and almost nothing is proved (it has the flavor of a catalog of various methods and corresponding examples).  So it is not the companion text I am looking for.","First off, I'm very sorry if this sort of question is not allowed here.  I've seen a couple similar questions on the OverFlow site, but I think discussions of basic material should be kept to this site. Anyway, in my ODE course we are using the introductory ODE text written by Jack Hale: http://www.amazon.com/Ordinary-Differential-Equations-Dover-Mathematics/dp/0486472116/ref=sr_1_1?ie=UTF8&qid=1350512056&sr=8-1&keywords=jack+Hale Actually, it's only introductory in the sense that it is self-contained, but it is rather advanced (think of those Dover reprints in size 8 font, half of which is written in Greek letters).  The table of contents and first pages give a hint of the level and material covered (see below). The proofs encountered in ODE seem to me VERY VERY unintuitive, in the sense that one (without experience) would be very unlikely to ever reproduce such proofs on their own.  Often they begin by establishing (without any motivation) estimates which are then used to prove things like ""now we see such and such is a nondecreasing,"" and only once three fourths through the proof does one see why it was even necessary to establish any of the prerequisite facts. I'm hoping this is just the style of the author, but perhaps this is just the specific taste of ODE theory. In any case, I would like some recommendations for texts that cover similar material on ODE theory that people here have found useful in the past .  There are many ODE texts, and they cover different parts of ODE theory.  This text (and our course) is aimed specifically at establishing the general theoretical framework for ODE theory (e.g. preliminaries on fixed point theorems/Banach spaces, Peano existence, Picard uniqueness, continuation of solutions, continuous dependence of parameters, differential estimates, further theory on linear systems, etc.), and then going straight to stability analysis (e.g. analysis of linear systems, perturbations of non-linear systems, Poincare-Bendixson theory, and Liapunov methods) and finally perturbation methods (e.g. asymptotic expansions, averaging, multiple scales, etc.).  In other words, this is not a course on elementary solution methods encountered in undergraduate courses, nor is it a course on advanced analytic topics such as Sturm-Louiville theory and eigenfunction expansions.  It is very much an ""applied"" course. Thank you in advance, and again I apologize if this is not strictly a ""Math Stackexchange"" question. EDIT (1): I know that several people have used Strogatz' non-linear dynamics text, which covers the ladder two topics discussed (actually, it covers very little on perturbation methods).  However, this text is extremely non-rigorous, and almost nothing is proved (it has the flavor of a catalog of various methods and corresponding examples).  So it is not the companion text I am looking for.",,['ordinary-differential-equations']
86,First-order nonlinear ordinary differential equation,First-order nonlinear ordinary differential equation,,How to solve this differential equation: $$x\frac{dy}{dx} = y + x\frac{e^x}{e^y}?$$ I tried to rearrange the equation to the form $f\left(\frac{y}{x}\right)$ but I couldn't thus I couldn't use $v = \frac{y}{x}$ to solve it.,How to solve this differential equation: $$x\frac{dy}{dx} = y + x\frac{e^x}{e^y}?$$ I tried to rearrange the equation to the form $f\left(\frac{y}{x}\right)$ but I couldn't thus I couldn't use $v = \frac{y}{x}$ to solve it.,,"['ordinary-differential-equations', 'integration']"
87,Approximated solution to differential equation in the form $f(u)u'^2+(u-u_0)^2=k$,Approximated solution to differential equation in the form,f(u)u'^2+(u-u_0)^2=k,"I'm trying to solve the following differential equation, that arises from conservation of energy in a physical problem. $R,k$ are constants. $$(1+R^4u^4)u'^2+(u-u_0)^2=k$$ Now, according to my book I should find ""approximate solutions"" around the point of equilibrium $u_0$. I started my reasoning with simpler equations and I have some questions: Is there a general good method to solve equations of the form $1/2k_1u'^2+1/2k_2u^2=k_3$? What I usually do is to resort to the physical problem of an harmonic oscillator with mass $k_1$, elastic constant $k_2$ and energy $k_3$ to find a cosinusoidal solution. I can find the amplitude but not the phase difference. In alternative I take the derivative of both sides of the equation and solve the second order linear differential equation that arises. I can find the amplitude of the oscillations by substitution of $u=A\cos t$ in the original equation. Are there more direct approaches? In the case of an equation of the form $1/2k_1u'^2+1/2k_2(u-u_0)^2=k_3$ I solve them either by inspection or by sobstituting $\xi = u-u_0,\ \xi'=u'$. Is that procedure correct? Now the original problem $(1+R^4u^4)u'^2+(u-u_0)^2=k$. My book gives what I think is a wrong answer : $u_0+(\sqrt{k}/u_0)\cos(\sqrt{1+R^4u_0^2})$. How to obtain the correct solution? I thought to substitute $u^4 = u_0^4$ in the leftmost term. Am I allowed to do this or it would be too rough an approximation? If I approximate even to first order $u^4$ the differential equation becomes too difficult for me","I'm trying to solve the following differential equation, that arises from conservation of energy in a physical problem. $R,k$ are constants. $$(1+R^4u^4)u'^2+(u-u_0)^2=k$$ Now, according to my book I should find ""approximate solutions"" around the point of equilibrium $u_0$. I started my reasoning with simpler equations and I have some questions: Is there a general good method to solve equations of the form $1/2k_1u'^2+1/2k_2u^2=k_3$? What I usually do is to resort to the physical problem of an harmonic oscillator with mass $k_1$, elastic constant $k_2$ and energy $k_3$ to find a cosinusoidal solution. I can find the amplitude but not the phase difference. In alternative I take the derivative of both sides of the equation and solve the second order linear differential equation that arises. I can find the amplitude of the oscillations by substitution of $u=A\cos t$ in the original equation. Are there more direct approaches? In the case of an equation of the form $1/2k_1u'^2+1/2k_2(u-u_0)^2=k_3$ I solve them either by inspection or by sobstituting $\xi = u-u_0,\ \xi'=u'$. Is that procedure correct? Now the original problem $(1+R^4u^4)u'^2+(u-u_0)^2=k$. My book gives what I think is a wrong answer : $u_0+(\sqrt{k}/u_0)\cos(\sqrt{1+R^4u_0^2})$. How to obtain the correct solution? I thought to substitute $u^4 = u_0^4$ in the leftmost term. Am I allowed to do this or it would be too rough an approximation? If I approximate even to first order $u^4$ the differential equation becomes too difficult for me",,"['ordinary-differential-equations', 'physics']"
88,Failure of uniqueness for linear ODE,Failure of uniqueness for linear ODE,,"Let's say we have a linear ODE  with polynomial coefficients $p_j(x)$: $$ p_n(x) y^{(n)}(x)+\dots+p_1(x)y'(x)+p_0(x)y=0 $$ and let's say $x_0$ is a root of $p_n(x)$. What can be said about uniqueness of the IVP for this ODE at $x_0$? In particular, if I succeeded to prove that an analytic solution $y_0(x)$ satisfies $y_0(x_0)=y'(x_0)=\dots=y^{(n-1)}(x_0)=0$, does it necessarily follow that $y_0(x)\equiv 0$?","Let's say we have a linear ODE  with polynomial coefficients $p_j(x)$: $$ p_n(x) y^{(n)}(x)+\dots+p_1(x)y'(x)+p_0(x)y=0 $$ and let's say $x_0$ is a root of $p_n(x)$. What can be said about uniqueness of the IVP for this ODE at $x_0$? In particular, if I succeeded to prove that an analytic solution $y_0(x)$ satisfies $y_0(x_0)=y'(x_0)=\dots=y^{(n-1)}(x_0)=0$, does it necessarily follow that $y_0(x)\equiv 0$?",,['ordinary-differential-equations']
89,Singular points of ODE,Singular points of ODE,,"My friend and I have conflicting answers and since his phone is off, I can't get his full solution and I don't understand his argument. Consider this ODE $$(x+1)y''+\frac{1}{x}y' + (x+3)y= 0$$ Basically what I did was divide out that $(x+1)$ on $y''$ and got $$y''+\frac{1}{x(x+1)}y' + \frac{(x+3)}{(x+1)}y= 0$$ The singularities are x = -1, and 0 (both are regular) My friend said we had to get our ODE in the form of $$a(x-x_0)^2y'' + b(x-x_0)y' + (x-x_0)y =0$$ otherwise we cant' do anything. and he got x = 3 as an irregular singular point which i have no idea how even got this. Sorry if this is too vague, but my first source was also as vague. what was my friend doing and who's right?","My friend and I have conflicting answers and since his phone is off, I can't get his full solution and I don't understand his argument. Consider this ODE $$(x+1)y''+\frac{1}{x}y' + (x+3)y= 0$$ Basically what I did was divide out that $(x+1)$ on $y''$ and got $$y''+\frac{1}{x(x+1)}y' + \frac{(x+3)}{(x+1)}y= 0$$ The singularities are x = -1, and 0 (both are regular) My friend said we had to get our ODE in the form of $$a(x-x_0)^2y'' + b(x-x_0)y' + (x-x_0)y =0$$ otherwise we cant' do anything. and he got x = 3 as an irregular singular point which i have no idea how even got this. Sorry if this is too vague, but my first source was also as vague. what was my friend doing and who's right?",,"['ordinary-differential-equations', 'singularity-theory']"
90,A nonlinear ordinary differential equation,A nonlinear ordinary differential equation,,"I've  got a ordinary differential equation basically on this form: $$ \frac{(f'(x))^2 x^2}{\sinh^2\left(2\,q\,f(x)\right)}+K^2\sinh^2\left(2\,q\,f(x)\right)=\frac{a}{q^2} $$ where $q>0$, $f(x)>0 $, and everything here is real. I would like a solution valid for all $x$, not a particular $x$. Does someone have a clue as to how I can proceed with this? Thanks for any help! Best regards, Jakob","I've  got a ordinary differential equation basically on this form: $$ \frac{(f'(x))^2 x^2}{\sinh^2\left(2\,q\,f(x)\right)}+K^2\sinh^2\left(2\,q\,f(x)\right)=\frac{a}{q^2} $$ where $q>0$, $f(x)>0 $, and everything here is real. I would like a solution valid for all $x$, not a particular $x$. Does someone have a clue as to how I can proceed with this? Thanks for any help! Best regards, Jakob",,['ordinary-differential-equations']
91,The theorem on ordinary differential equations stated in the appendix of Kobayashi-Nomizu,The theorem on ordinary differential equations stated in the appendix of Kobayashi-Nomizu,,"The following theorem is stated in the appendix I of Foundation of differential geometry by Kobayashi-Nomizu. They say the proof will be found in various text books on differential equations. I checked several books, but could not find it. Theorem Let $E$ and $F$ be finite dimensional normed spaces over $\mathbb R$. Let $U$ and $V$ be non-empty open subsets of $E$ and $F$ respectively. Let $J$ be an open interval of $\mathbb R$ containing $0$. Let $f:J×U×V → E$ be a map. Suppose $f$ is differentiable of class $C^p, 0 ≦ p ≦ ω$ in $J$ and of class $C^q, 1 ≦ q ≦ ω$ in $U$ and $V$. Then there exist open subinterval $J_0$ of $J$ containing $0$, non-empty open connected subsets $U_0, V_0$ of $U, V$ respectively and a unique map $g:J_0×U_0×V_0 → U$ which satisfy the following properties. (1) $g$ is differentiable of class $C^{p+1}$ in $J_0$ and of class $C^q$ in $U_0$ and $V_0$. (2) $D_tg(t, x, s) = f(t, g(t, x, s)$, s) for all $(t, x, s) ∈ J_0×U_0×V_0$ (3) $g(0, x, s) = x$ for all $(x, s) ∈ U_0×V_0$","The following theorem is stated in the appendix I of Foundation of differential geometry by Kobayashi-Nomizu. They say the proof will be found in various text books on differential equations. I checked several books, but could not find it. Theorem Let $E$ and $F$ be finite dimensional normed spaces over $\mathbb R$. Let $U$ and $V$ be non-empty open subsets of $E$ and $F$ respectively. Let $J$ be an open interval of $\mathbb R$ containing $0$. Let $f:J×U×V → E$ be a map. Suppose $f$ is differentiable of class $C^p, 0 ≦ p ≦ ω$ in $J$ and of class $C^q, 1 ≦ q ≦ ω$ in $U$ and $V$. Then there exist open subinterval $J_0$ of $J$ containing $0$, non-empty open connected subsets $U_0, V_0$ of $U, V$ respectively and a unique map $g:J_0×U_0×V_0 → U$ which satisfy the following properties. (1) $g$ is differentiable of class $C^{p+1}$ in $J_0$ and of class $C^q$ in $U_0$ and $V_0$. (2) $D_tg(t, x, s) = f(t, g(t, x, s)$, s) for all $(t, x, s) ∈ J_0×U_0×V_0$ (3) $g(0, x, s) = x$ for all $(x, s) ∈ U_0×V_0$",,['ordinary-differential-equations']
92,Solving ODE with substitution,Solving ODE with substitution,,"I have this as homework: $$(xy^2+y)dx+(x^2y-x)dy=0$$ I tried to solve it by substituting $z=xy+1$, but got the answer like $y=Cxe^{xy}$, which, I guess, is wrong. I tried to solve it couple of times, but with no success. P.S. I'm new on math.stackexchange.com. Do I need to show the whole process?","I have this as homework: $$(xy^2+y)dx+(x^2y-x)dy=0$$ I tried to solve it by substituting $z=xy+1$, but got the answer like $y=Cxe^{xy}$, which, I guess, is wrong. I tried to solve it couple of times, but with no success. P.S. I'm new on math.stackexchange.com. Do I need to show the whole process?",,['ordinary-differential-equations']
93,"Initial value problem, $dy/dt=1/y$, $y(0)=0$","Initial value problem, ,",dy/dt=1/y y(0)=0,"i am curious why following initial-value problem $$ \frac{dy}{dt}=\frac{1}{y},\quad   y(0)=0$$  has no solution if we solve  it by method of seperation of variables, we get that $$y(t)=\pm\sqrt{2t\  {}} $$ we  have assumption that our function has form  $f(t,y)$; book  from which i have taken this example,says that ,it has not solution  because of  it does not contain $t$ variable (or at book  language,does not include $t$ axis) i need to understand  it  well, as if i met  such type of  problem, i   won't to  mixes  and say  that,it has solution, thanks a lot of,as a additional fact, in book there    is written,if   change  $y(0)=1$, then   $y(t)=\sqrt{2t+1}$, it is defined on this interval $(-1/2,\infty)$, does it have solution here?if yes  than, $2t$ would be defined  on  $[0,\infty]$ right? thanks","i am curious why following initial-value problem $$ \frac{dy}{dt}=\frac{1}{y},\quad   y(0)=0$$  has no solution if we solve  it by method of seperation of variables, we get that $$y(t)=\pm\sqrt{2t\  {}} $$ we  have assumption that our function has form  $f(t,y)$; book  from which i have taken this example,says that ,it has not solution  because of  it does not contain $t$ variable (or at book  language,does not include $t$ axis) i need to understand  it  well, as if i met  such type of  problem, i   won't to  mixes  and say  that,it has solution, thanks a lot of,as a additional fact, in book there    is written,if   change  $y(0)=1$, then   $y(t)=\sqrt{2t+1}$, it is defined on this interval $(-1/2,\infty)$, does it have solution here?if yes  than, $2t$ would be defined  on  $[0,\infty]$ right? thanks",,['ordinary-differential-equations']
94,Ignoring absolute values for integration,Ignoring absolute values for integration,,"Consider the differential equation with $\frac{dx}{dt}=1-x$ and $x=0$ when $t=0$. The answer uses the result that $\int \frac{dx}{1-x}=\ln (1-x)$, hence getting the solution $x=1-e^{-t}$. However I use $\int \frac{dx}{1-x}=\ln \left|1-x\right|$ instead, which gets me $x=1-e^{-t} \text{ or }1+e^{-t}$. Am I right, or is it standard to not use the absolute signs?","Consider the differential equation with $\frac{dx}{dt}=1-x$ and $x=0$ when $t=0$. The answer uses the result that $\int \frac{dx}{1-x}=\ln (1-x)$, hence getting the solution $x=1-e^{-t}$. However I use $\int \frac{dx}{1-x}=\ln \left|1-x\right|$ instead, which gets me $x=1-e^{-t} \text{ or }1+e^{-t}$. Am I right, or is it standard to not use the absolute signs?",,['ordinary-differential-equations']
95,"Generating unitary matrices numerically - ""close"" to the identity element","Generating unitary matrices numerically - ""close"" to the identity element",,"EDIT: broke this into two parts - for these were two different questions. For numerically obtaining the stabilities of a matricial equation, i need to generate an ensemble of matrices that are members of SU(n), and strategies for uniform random generation of those are outlined here . But I also need these unitary matrices to be clustered close to the SU(n) identity operation, at some fixed ""distance"". Thus my question distills to - What might be a good metric for SU(n) matrices?  EDIT: one option has been provided by @Jim_Belk downstairs (:D) - certainly feel free  to add more! How does one generate SU(n) matrices close to the Identity? EDIT: are there any alternatives other than computing the generators first and then exponentiating them? If you find the questions framed improperly or dubious, do tell!","EDIT: broke this into two parts - for these were two different questions. For numerically obtaining the stabilities of a matricial equation, i need to generate an ensemble of matrices that are members of SU(n), and strategies for uniform random generation of those are outlined here . But I also need these unitary matrices to be clustered close to the SU(n) identity operation, at some fixed ""distance"". Thus my question distills to - What might be a good metric for SU(n) matrices?  EDIT: one option has been provided by @Jim_Belk downstairs (:D) - certainly feel free  to add more! How does one generate SU(n) matrices close to the Identity? EDIT: are there any alternatives other than computing the generators first and then exponentiating them? If you find the questions framed improperly or dubious, do tell!",,"['group-theory', 'ordinary-differential-equations', 'numerical-linear-algebra']"
96,Complementary Solution = Homogenous solution?,Complementary Solution = Homogenous solution?,,"I have calculated solutions to homogenous equations but is the complementary solution mentioned here the same as the homogenous solution? Let's take example $y''-3y'+2y=\cos(wx)$ and now the homogenous solution is $$y_{hom}=C_{1}e^{2x}+C_{2}e^{x}_{|\text{Characteristic eq. =}(r-1)(r-2)}$$ which can be showed with Wronk's determinant to be valid (cannot yet understand it but go on). Now to find out the general solution there are multiple ways apparently: Method of Undetermined Coefficient Variation of constant I have not practised them yet enough (because cannot understand the terms yet) so cannot ask much about them but I am trying to, could someone help me with the terminology here about the complementary solution ?","I have calculated solutions to homogenous equations but is the complementary solution mentioned here the same as the homogenous solution? Let's take example $y''-3y'+2y=\cos(wx)$ and now the homogenous solution is $$y_{hom}=C_{1}e^{2x}+C_{2}e^{x}_{|\text{Characteristic eq. =}(r-1)(r-2)}$$ which can be showed with Wronk's determinant to be valid (cannot yet understand it but go on). Now to find out the general solution there are multiple ways apparently: Method of Undetermined Coefficient Variation of constant I have not practised them yet enough (because cannot understand the terms yet) so cannot ask much about them but I am trying to, could someone help me with the terminology here about the complementary solution ?",,"['ordinary-differential-equations', 'terminology']"
97,Sturm-Liouville systems and the Wronskian,Sturm-Liouville systems and the Wronskian,,"I'm currently reading through Gohberg and Goldberg's treatment of Sturm-Liouville systems in Basic Operator Theory. Define a Sturm-Liouville system to be a differential equation of the form $$\frac{d}{dx}(p(x) \frac{dy}{dx}) + q(x) y = f(x)$$ with boundary conditions $$a_1 y(a) + a_2 y'(a) = 0$$ $$b_1 y(b) + b_2 y'(b) = 0$$ where $a_i, b_i$ are real numbers with $a_1^2 + a_2^2 \neq 0$, $b_1^2 + b_2^2 \neq 0$. Suppose the only solution where $f = 0$ is $y=0$. Then there exist real valued nonzero functions $y_1, y_2$ that are solutions, such that $y_1$ satisfies the first boundary condition and $y_2$ the second. Then they claim that a ""straightforward computation"" verifies that $(pW)' = 0$, where $W$ denotes the Wronskian. I do not see this computation. What I end up with is $$(pW)' = p'y_1 y_2' - p' y_2 y_1' + py_1' y_2' + py_1 y_2'' - py_2' y_1' - py_2 y_1''$$ and I don't see why it's zero.","I'm currently reading through Gohberg and Goldberg's treatment of Sturm-Liouville systems in Basic Operator Theory. Define a Sturm-Liouville system to be a differential equation of the form $$\frac{d}{dx}(p(x) \frac{dy}{dx}) + q(x) y = f(x)$$ with boundary conditions $$a_1 y(a) + a_2 y'(a) = 0$$ $$b_1 y(b) + b_2 y'(b) = 0$$ where $a_i, b_i$ are real numbers with $a_1^2 + a_2^2 \neq 0$, $b_1^2 + b_2^2 \neq 0$. Suppose the only solution where $f = 0$ is $y=0$. Then there exist real valued nonzero functions $y_1, y_2$ that are solutions, such that $y_1$ satisfies the first boundary condition and $y_2$ the second. Then they claim that a ""straightforward computation"" verifies that $(pW)' = 0$, where $W$ denotes the Wronskian. I do not see this computation. What I end up with is $$(pW)' = p'y_1 y_2' - p' y_2 y_1' + py_1' y_2' + py_1 y_2'' - py_2' y_1' - py_2 y_1''$$ and I don't see why it's zero.",,['ordinary-differential-equations']
98,Nonlinear ODE with integral,Nonlinear ODE with integral,,"My question is to find the function $f(t)$ such that $$\frac{df}{dt} = -2f(t)\int_{0}^{t}f(s)\, ds$$ with $f(0) = 1$. My idea is to divide both sides by $-2f$ and differentiate both sides, and then let $g = \frac{df}{dt}$ and consider $g$ as a function of $f$ which would reduce the order of the differential equation, but this doesn't seem to be working. Is this the right way? Is there another way to do this?","My question is to find the function $f(t)$ such that $$\frac{df}{dt} = -2f(t)\int_{0}^{t}f(s)\, ds$$ with $f(0) = 1$. My idea is to divide both sides by $-2f$ and differentiate both sides, and then let $g = \frac{df}{dt}$ and consider $g$ as a function of $f$ which would reduce the order of the differential equation, but this doesn't seem to be working. Is this the right way? Is there another way to do this?",,['ordinary-differential-equations']
99,General Process to Solve a Differential Equation,General Process to Solve a Differential Equation,,"Lately in my Physics C class we have been doing differential equations, which my teacher has explained more than once to me. Yet for some reason I can still not grasp the concept, I think my problem lies in not understanding the process of solving a differential equation. So my question then becomes what is the general process for solving a differential equation of the first order? I am a high school senior and a physics major by heart, so I really need to understand this process before I go to college. I am in Calculus AB and any answers are appreciated.","Lately in my Physics C class we have been doing differential equations, which my teacher has explained more than once to me. Yet for some reason I can still not grasp the concept, I think my problem lies in not understanding the process of solving a differential equation. So my question then becomes what is the general process for solving a differential equation of the first order? I am a high school senior and a physics major by heart, so I really need to understand this process before I go to college. I am in Calculus AB and any answers are appreciated.",,"['calculus', 'ordinary-differential-equations']"
