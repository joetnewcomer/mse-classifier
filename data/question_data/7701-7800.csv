,title_raw,title_text,title_latex,body_raw,body_text,body_latex,tags
0,Can someone help me finish this: evaluate $S_n = \frac{x}{1-x^2}+\frac{x^2}{1-x^4}+ ... + \frac{x^{2^{n-1}}}{1-x^{2^{n}}}$ [duplicate],Can someone help me finish this: evaluate  [duplicate],S_n = \frac{x}{1-x^2}+\frac{x^2}{1-x^4}+ ... + \frac{x^{2^{n-1}}}{1-x^{2^{n}}},"This question already has answers here : Two different expansions of $\frac{z}{1-z}$ (3 answers) Closed 5 years ago . I am asked to find the closed form solution for the below. $$S_n = \frac{x}{1-x^2}+\frac{x^2}{1-x^4}+ ... + \frac{x^{2^{n-1}}}{1-x^{2^{n}}}$$ Just writing out the $S_1, S_2, S_3$ , I have managed to find a pattern, which is: $$S_n = \frac{S_{n-1}}{1-x^{2^n}} + \frac{x^{2^n}}{1-x^{2^{n+1}}}$$ I am not sure how to proceed onwards to solve this recurrence relation. Is there a clever trick I can do to solve it?","This question already has answers here : Two different expansions of $\frac{z}{1-z}$ (3 answers) Closed 5 years ago . I am asked to find the closed form solution for the below. Just writing out the , I have managed to find a pattern, which is: I am not sure how to proceed onwards to solve this recurrence relation. Is there a clever trick I can do to solve it?","S_n = \frac{x}{1-x^2}+\frac{x^2}{1-x^4}+ ... + \frac{x^{2^{n-1}}}{1-x^{2^{n}}} S_1, S_2, S_3 S_n = \frac{S_{n-1}}{1-x^{2^n}} + \frac{x^{2^n}}{1-x^{2^{n+1}}}","['real-analysis', 'sequences-and-series', 'convergence-divergence', 'summation']"
1,Let $\lim\limits_{n\to\infty}a_{n+1}-a_{n}=\alpha$. Show that $\lim\limits_{n\to\infty} \frac{a_n}{n}=\alpha$. [duplicate],Let . Show that . [duplicate],\lim\limits_{n\to\infty}a_{n+1}-a_{n}=\alpha \lim\limits_{n\to\infty} \frac{a_n}{n}=\alpha,"This question already has an answer here : Limit of $b_n$ when $b_n=\frac{a_n}{n}$ and $\lim\limits_{n\to\infty}(a_{n+1}-a_n)=l$ [duplicate] (1 answer) Closed 5 years ago . Let $\lim\limits_{n\to\infty}a_{n+1}-a_{n}=\alpha$. Show that $\lim\limits_{n\to\infty} \frac{a_n}{n}=\alpha$. Since $\lim\limits_{n\to\infty}a_{n+1}-a_{n}=\alpha$, we have that for all $\epsilon>0$ there exists $N\in\mathbb{N}$ such that  $$|(a_{n+1}-a_n) - \alpha| < \epsilon \quad \forall n>N.$$ We wish to show that for all $\epsilon>0$ there exists $M\in\mathbb{N}$ such that  $$\left|\frac{a_n}{n} - \alpha \right| < \epsilon \quad \forall n>M.$$ So, \begin{align*} \left|\frac{a_n}{n} - \alpha \right| &= \left|\frac{a_n}{n} - \frac{a_{n+1}}{n} + \frac{a_{n+1}}{n} - \alpha \right| \\ &= \left| \frac{1}{n}(a_n - a_{n+1} - \alpha) + \frac{a_{n+1}}{n} + \sum_{j=1}^{n+1} \frac{\alpha}{n} \right| \\ \end{align*} I tried continuing with this pattern, but I did not get anywhere.","This question already has an answer here : Limit of $b_n$ when $b_n=\frac{a_n}{n}$ and $\lim\limits_{n\to\infty}(a_{n+1}-a_n)=l$ [duplicate] (1 answer) Closed 5 years ago . Let $\lim\limits_{n\to\infty}a_{n+1}-a_{n}=\alpha$. Show that $\lim\limits_{n\to\infty} \frac{a_n}{n}=\alpha$. Since $\lim\limits_{n\to\infty}a_{n+1}-a_{n}=\alpha$, we have that for all $\epsilon>0$ there exists $N\in\mathbb{N}$ such that  $$|(a_{n+1}-a_n) - \alpha| < \epsilon \quad \forall n>N.$$ We wish to show that for all $\epsilon>0$ there exists $M\in\mathbb{N}$ such that  $$\left|\frac{a_n}{n} - \alpha \right| < \epsilon \quad \forall n>M.$$ So, \begin{align*} \left|\frac{a_n}{n} - \alpha \right| &= \left|\frac{a_n}{n} - \frac{a_{n+1}}{n} + \frac{a_{n+1}}{n} - \alpha \right| \\ &= \left| \frac{1}{n}(a_n - a_{n+1} - \alpha) + \frac{a_{n+1}}{n} + \sum_{j=1}^{n+1} \frac{\alpha}{n} \right| \\ \end{align*} I tried continuing with this pattern, but I did not get anywhere.",,"['real-analysis', 'sequences-and-series', 'analysis']"
2,Prove that $\sum_{k=1}^{n} \frac{1}{n+k} < \ln 2$,Prove that,\sum_{k=1}^{n} \frac{1}{n+k} < \ln 2,I have a loose upper bound: $$ \sum_{k=1}^{n} \frac{1}{n+k}  < \frac{1}{n} \cdot  n = 1 $$ Clearly this is very loose compared to the upper bound $\ln 2 \approx 0.693$,I have a loose upper bound: $$ \sum_{k=1}^{n} \frac{1}{n+k}  < \frac{1}{n} \cdot  n = 1 $$ Clearly this is very loose compared to the upper bound $\ln 2 \approx 0.693$,,['real-analysis']
3,"can I find the closest rational to any given real, if I assume that the denominator is not larger than some fixed n","can I find the closest rational to any given real, if I assume that the denominator is not larger than some fixed n",,"Given $n\in\mathbb{N}$ and $r\in\mathbb{R}$(or $r\in\mathbb{Q}$), is that possible to find a rational $\frac{a}{b}$such that,  $b<n$ and $\frac{a}{b}$ is the ""closest"" rational to r? With ""closest"" I mean when $\frac{a}{b}>r$ it is the smallest rational with $b<n$ and likewise for $\frac{a}{b}<r$ it is the largest rational with $b<n$ Approach: I can see there is infinite many real/rational that close to $r$ by Archimedean property, but will that be the case such that given any $\epsilon>0$, for $|r-\frac{a}{b}|<\epsilon$, the set $\{\frac{a}{b}: b<n\}$ is bounded?","Given $n\in\mathbb{N}$ and $r\in\mathbb{R}$(or $r\in\mathbb{Q}$), is that possible to find a rational $\frac{a}{b}$such that,  $b<n$ and $\frac{a}{b}$ is the ""closest"" rational to r? With ""closest"" I mean when $\frac{a}{b}>r$ it is the smallest rational with $b<n$ and likewise for $\frac{a}{b}<r$ it is the largest rational with $b<n$ Approach: I can see there is infinite many real/rational that close to $r$ by Archimedean property, but will that be the case such that given any $\epsilon>0$, for $|r-\frac{a}{b}|<\epsilon$, the set $\{\frac{a}{b}: b<n\}$ is bounded?",,"['real-analysis', 'number-theory', 'analysis', 'elementary-number-theory', 'real-numbers']"
4,Calculate $\int_{-\pi}^\pi\bigg(\sum_{n=1}^\infty\frac{\sin(nx)}{2^n}\bigg)^2dx$,Calculate,\int_{-\pi}^\pi\bigg(\sum_{n=1}^\infty\frac{\sin(nx)}{2^n}\bigg)^2dx,"Calculate:$$\int_{-\pi}^\pi\bigg(\sum_{n=1}^\infty\frac{\sin(nx)}{2^n}\bigg)^2dx$$ One can prove $\sum_{n=1}^\infty\frac{\sin(nx)}{2^n}$ converges uniformly by Dirichlet's test, integrate term-by-term, and since $\int_{-\pi}^\pi\frac{\sin(nx)}{2^n}dx=0,$ we get series of $0$'s and the final result would be $0.$ Thing is I'm not sure how to deal with the square. Any help appreciated.","Calculate:$$\int_{-\pi}^\pi\bigg(\sum_{n=1}^\infty\frac{\sin(nx)}{2^n}\bigg)^2dx$$ One can prove $\sum_{n=1}^\infty\frac{\sin(nx)}{2^n}$ converges uniformly by Dirichlet's test, integrate term-by-term, and since $\int_{-\pi}^\pi\frac{\sin(nx)}{2^n}dx=0,$ we get series of $0$'s and the final result would be $0.$ Thing is I'm not sure how to deal with the square. Any help appreciated.",,"['calculus', 'real-analysis', 'integration', 'sequences-and-series', 'analysis']"
5,How to define the differential $df$ of a function $f$ using limits?,How to define the differential  of a function  using limits?,df f,"Let $f:A\subseteq\mathbb{R} \longrightarrow \mathbb{R}$ be a differentiable function of $x\in A$. Then its derivative $\displaystyle {df \over dx}: A\subseteq\mathbb{R} \longrightarrow \mathbb{R}$ is also a function of $x\in A$ and it's defined as: $$\begin{align} {df \over dx} : A\subseteq\mathbb{R} & \longrightarrow \mathbb{R}\\ x & \longmapsto \lim_{\delta\to 0} {f(x+\delta) - f(x) \over (x + \delta) -  x}, \\ \end{align}$$ meaning it maps each $x \in A$ to the above limit. (That limit exists by hypothesis.) There's another function associated to $f$, namely, the differential $df$ of $f$, with domain $A\subseteq\mathbb{R}$ (and codomain unknown to me), which I cannot define (since I don't know what it is in the first place). I can give some examples, though. (I do know that if $f$ is a differentiable map between differentiable manifolds, then the differential $df_x$ at $x\in \textbf{Dom }f$ is a certain linear map from the tangent space $T_x$ to the tangent space $T_{f(x)}$, but I don't see how this definition uses limits ,  and I don't know if it's equivalent to the one we (implicitly) use for the examples below.) Example 0 . Let $f:x\longmapsto x^2$. Now $$df: x \longmapsto 2xdx$$ and $$ {df \over dx } : x\longmapsto 2x.$$ Example 1 . Let $f:(x,y)\longmapsto x^4 + 3y^2 + 8$. Now $$df: x \longmapsto 4x^3dx + 6ydy + 0$$ and $${df \over dx}: x \longmapsto 4x^3 + 6y{dy \over dx}$$ and $${df \over dy}: x \longmapsto 4x^3{dx \over dy} + 6y.$$ What limit is $df$? How do we define $df$ using limits? What is the codomain of the map $df$? What limit is $dx$? How do we define $dx$ using limits? What is the codomain of the map $dx$?","Let $f:A\subseteq\mathbb{R} \longrightarrow \mathbb{R}$ be a differentiable function of $x\in A$. Then its derivative $\displaystyle {df \over dx}: A\subseteq\mathbb{R} \longrightarrow \mathbb{R}$ is also a function of $x\in A$ and it's defined as: $$\begin{align} {df \over dx} : A\subseteq\mathbb{R} & \longrightarrow \mathbb{R}\\ x & \longmapsto \lim_{\delta\to 0} {f(x+\delta) - f(x) \over (x + \delta) -  x}, \\ \end{align}$$ meaning it maps each $x \in A$ to the above limit. (That limit exists by hypothesis.) There's another function associated to $f$, namely, the differential $df$ of $f$, with domain $A\subseteq\mathbb{R}$ (and codomain unknown to me), which I cannot define (since I don't know what it is in the first place). I can give some examples, though. (I do know that if $f$ is a differentiable map between differentiable manifolds, then the differential $df_x$ at $x\in \textbf{Dom }f$ is a certain linear map from the tangent space $T_x$ to the tangent space $T_{f(x)}$, but I don't see how this definition uses limits ,  and I don't know if it's equivalent to the one we (implicitly) use for the examples below.) Example 0 . Let $f:x\longmapsto x^2$. Now $$df: x \longmapsto 2xdx$$ and $$ {df \over dx } : x\longmapsto 2x.$$ Example 1 . Let $f:(x,y)\longmapsto x^4 + 3y^2 + 8$. Now $$df: x \longmapsto 4x^3dx + 6ydy + 0$$ and $${df \over dx}: x \longmapsto 4x^3 + 6y{dy \over dx}$$ and $${df \over dy}: x \longmapsto 4x^3{dx \over dy} + 6y.$$ What limit is $df$? How do we define $df$ using limits? What is the codomain of the map $df$? What limit is $dx$? How do we define $dx$ using limits? What is the codomain of the map $dx$?",,"['calculus', 'real-analysis', 'analysis', 'multivariable-calculus']"
6,"If an infinite sequence has no last element, why do we say the $\sqrt2$ is irrational?","If an infinite sequence has no last element, why do we say the  is irrational?",\sqrt2,"The arguments I've seen for certain square roots being irrational (like $\sqrt2$) boil down to proving by contradiction that if we assume p/q to be fully reduced we'll find that both p and q must be even (in the case of $\sqrt2$) and :boom: contradiction. But if I allow p to have an infinite number of digits (a valid member of the integers) then it has no last digit and cannot be classified as even or odd. Right? The proof by infinite descent, for example, seems to assume we can't descend infinitely but why not? I must be missing something important here about the integers. Similarly, why not define pi as the fraction 314159.../100000...? I believe both are valid integers. I've often heard people loosely describe rational numbers as the set of numbers whose decimal representations terminate or repeat, but I don't see what limits the construction of an arbitrarily large integer by an infinite process. Lets for example attempt to construct the numerator in the quotient above: Step 1: Construct the number 3 (1+1+1 = 3) Step 2: Construct 31 (3+1+1...+1 = 28) Step 3: Construct 314 (you get the idea) It seems I could continue the process above indefinitely and produce a valid integer at every step. Because it is an infinite process any one step has a finite integer, but the number this process describes is unlimited. Is the number this process describes then not an integer? Update As commenters and answerers of this question have pointed out, my question assumes that a number with an infinite number of digits could be called an integer. That underlying assumption has confused some readers of my question so I've edited the above in an attempt to make the meat of my question clearer. To that end, I'd like to point out that the essence of my question is either very similar to or a duplicate of A ""number"" with an infinite number of digits is a natural number? .","The arguments I've seen for certain square roots being irrational (like $\sqrt2$) boil down to proving by contradiction that if we assume p/q to be fully reduced we'll find that both p and q must be even (in the case of $\sqrt2$) and :boom: contradiction. But if I allow p to have an infinite number of digits (a valid member of the integers) then it has no last digit and cannot be classified as even or odd. Right? The proof by infinite descent, for example, seems to assume we can't descend infinitely but why not? I must be missing something important here about the integers. Similarly, why not define pi as the fraction 314159.../100000...? I believe both are valid integers. I've often heard people loosely describe rational numbers as the set of numbers whose decimal representations terminate or repeat, but I don't see what limits the construction of an arbitrarily large integer by an infinite process. Lets for example attempt to construct the numerator in the quotient above: Step 1: Construct the number 3 (1+1+1 = 3) Step 2: Construct 31 (3+1+1...+1 = 28) Step 3: Construct 314 (you get the idea) It seems I could continue the process above indefinitely and produce a valid integer at every step. Because it is an infinite process any one step has a finite integer, but the number this process describes is unlimited. Is the number this process describes then not an integer? Update As commenters and answerers of this question have pointed out, my question assumes that a number with an infinite number of digits could be called an integer. That underlying assumption has confused some readers of my question so I've edited the above in an attempt to make the meat of my question clearer. To that end, I'd like to point out that the essence of my question is either very similar to or a duplicate of A ""number"" with an infinite number of digits is a natural number? .",,"['real-analysis', 'sequences-and-series', 'irrational-numbers']"
7,Absolute value of a measurable function is measurable,Absolute value of a measurable function is measurable,,"Let $(X , \mathscr{M}, \mu)$ be a measure space. Prove that if $f$ is measurable, then $|f|$ is measurable. A function $f$ is measurable if $f^{-1}((a , \infty)) = \{x : f(x) > a\} \in \mathscr{M}$ for every $a \in \mathbb{R}$. Ok so this is what I have so far: Consider two cases: $a < 0$ and $a \geq 0$. If $a < 0$, then we have $$|f|^{-1}((a , \infty)) = \{x : |f(x)| > a \} = X \in \mathscr{M}$$ since $\mathscr{M}$ is a $\sigma$-algebra. I am confused on how to prove the case when $a \geq 0$. If $a \geq 0$, we have $$|f|^{-1}((a , \infty)) = \{x : |f(x)| > a \} = ?$$ I am assuming we will have to use that fact that $f$ is measurable.  Any help will be appreciated!","Let $(X , \mathscr{M}, \mu)$ be a measure space. Prove that if $f$ is measurable, then $|f|$ is measurable. A function $f$ is measurable if $f^{-1}((a , \infty)) = \{x : f(x) > a\} \in \mathscr{M}$ for every $a \in \mathbb{R}$. Ok so this is what I have so far: Consider two cases: $a < 0$ and $a \geq 0$. If $a < 0$, then we have $$|f|^{-1}((a , \infty)) = \{x : |f(x)| > a \} = X \in \mathscr{M}$$ since $\mathscr{M}$ is a $\sigma$-algebra. I am confused on how to prove the case when $a \geq 0$. If $a \geq 0$, we have $$|f|^{-1}((a , \infty)) = \{x : |f(x)| > a \} = ?$$ I am assuming we will have to use that fact that $f$ is measurable.  Any help will be appreciated!",,"['real-analysis', 'measure-theory']"
8,"$\mathbb{R}$ is uncountable, Abbott's proof","is uncountable, Abbott's proof",\mathbb{R},"There is this proof in Abbott's book (Understanding Analysis) on page 25, that I am failing to understand. Theorem is that $\mathbb{R}$ is uncountable. And here is how the author proceeds to prove it (I know there is an easier proof by Cantor): To show that a set is uncountable, we need to show that there is no 1-1,onto function of the form: $f:\mathbb{N} \rightarrow \mathbb{R}$. The author uses proof by contradiction, therefore assume there is a 1-1, onto function. This implies that no two ""input"" values from $\mathbb{N}$ will map us to the same value in $\mathbb{R}$ (1-1) and also that every element maps onto a unique value in $\mathbb{R}$, i.e. $x_1=f(1)$,$x_2=f(2)$ and so on, so we can write: $$\mathbb{R} = \{x_1,x_2,x_3, ... \}$$ Would be the set of all the reals. Now the author proceeds to use the Nested Interval Property (Theorem 1.4.1 in the book) to produce a real that is not in the list. Let $I_1$ be a closed interval that does not contain $x_1$ and $I_2$ be a closed interval that is contained in $I_1$ and which does not contain $x_2$. (1) My first understanding hurdle : ""Certainly $I_1$ contains two smaller disjoint closed intervals"", I cannot see which two disjoint intervals he has in mind . So I am starting to struggle at this point. He then proceeds to say that $x_{n_0}$ is some real number from the list, then $x_{n_0} \notin I_{n_0}$ (fair enough, this is due to the way we constructed the intervals) and he then proceeds saying that: $$x_{n_0} \notin \bigcap_{n=1}^{\infty}I_n$$ I do not see where the $I_{n_0}$ is in the above... The above has an intersection of the following intervals: $I_1,I_2,I_3,..$ and there is no $I_{n_0}$. (2) Finally , by construction there is a real that is not in the interval, $x_n \notin I_n$ is not in there at all. So by definition, we are omitting a real number from the interval, even though we are meant to prove it (I realize that this is an interval in the first place; My thinking was that if we prove it on the interval then we can extend the conclusions from that interval to the whole of the real line; But if we are omitting a real from the number line, then how can we make any conclusions at all).","There is this proof in Abbott's book (Understanding Analysis) on page 25, that I am failing to understand. Theorem is that $\mathbb{R}$ is uncountable. And here is how the author proceeds to prove it (I know there is an easier proof by Cantor): To show that a set is uncountable, we need to show that there is no 1-1,onto function of the form: $f:\mathbb{N} \rightarrow \mathbb{R}$. The author uses proof by contradiction, therefore assume there is a 1-1, onto function. This implies that no two ""input"" values from $\mathbb{N}$ will map us to the same value in $\mathbb{R}$ (1-1) and also that every element maps onto a unique value in $\mathbb{R}$, i.e. $x_1=f(1)$,$x_2=f(2)$ and so on, so we can write: $$\mathbb{R} = \{x_1,x_2,x_3, ... \}$$ Would be the set of all the reals. Now the author proceeds to use the Nested Interval Property (Theorem 1.4.1 in the book) to produce a real that is not in the list. Let $I_1$ be a closed interval that does not contain $x_1$ and $I_2$ be a closed interval that is contained in $I_1$ and which does not contain $x_2$. (1) My first understanding hurdle : ""Certainly $I_1$ contains two smaller disjoint closed intervals"", I cannot see which two disjoint intervals he has in mind . So I am starting to struggle at this point. He then proceeds to say that $x_{n_0}$ is some real number from the list, then $x_{n_0} \notin I_{n_0}$ (fair enough, this is due to the way we constructed the intervals) and he then proceeds saying that: $$x_{n_0} \notin \bigcap_{n=1}^{\infty}I_n$$ I do not see where the $I_{n_0}$ is in the above... The above has an intersection of the following intervals: $I_1,I_2,I_3,..$ and there is no $I_{n_0}$. (2) Finally , by construction there is a real that is not in the interval, $x_n \notin I_n$ is not in there at all. So by definition, we are omitting a real number from the interval, even though we are meant to prove it (I realize that this is an interval in the first place; My thinking was that if we prove it on the interval then we can extend the conclusions from that interval to the whole of the real line; But if we are omitting a real from the number line, then how can we make any conclusions at all).",,['real-analysis']
9,"What is the relation between Neighbourhood of a point,Interior point and open set?","What is the relation between Neighbourhood of a point,Interior point and open set?",,"I want to know what is the relation between of Neighbourhood of a point,Interior point and open set? Definition: A set $N \subset \mathbb{R}$ is called the $\textbf{neighbourhood}$ of a point a, if there  exists an interval I containing a and contained in N, i.e ,$$a\in I \subset N $$. Definition: A point x is an interior point of a set S if S is a nbd of x. In other words, x is an interior point of S if $\exists $ an open interval $(a,b)$ containing x and contained in S , i.e., $$x\in(a,b)\subseteq S$$. Definition: A set S is said to be open if it is a nbd of each of its point, i.e, $x\in S$, there exists an open interval $I_x$ such that $$mx \in I_x \subseteq S $$.","I want to know what is the relation between of Neighbourhood of a point,Interior point and open set? Definition: A set $N \subset \mathbb{R}$ is called the $\textbf{neighbourhood}$ of a point a, if there  exists an interval I containing a and contained in N, i.e ,$$a\in I \subset N $$. Definition: A point x is an interior point of a set S if S is a nbd of x. In other words, x is an interior point of S if $\exists $ an open interval $(a,b)$ containing x and contained in S , i.e., $$x\in(a,b)\subseteq S$$. Definition: A set S is said to be open if it is a nbd of each of its point, i.e, $x\in S$, there exists an open interval $I_x$ such that $$mx \in I_x \subseteq S $$.",,"['real-analysis', 'general-topology']"
10,"Contour integral for finding $\int_0^\infty \frac{\ln x}{(x+a)^2+b^2} \, dx$",Contour integral for finding,"\int_0^\infty \frac{\ln x}{(x+a)^2+b^2} \, dx","I can't prove the following result: $$\displaystyle\int_0^\infty \frac{\ln x}{(x+a)^2+b^2} \, dx=\frac{\ln \sqrt{a^2+b^2}}{b}\arctan\frac{b}{a} \text{ for all } a,b \in \mathbb{R}.$$ Well, I consider $\displaystyle\int_C \frac{\operatorname{Ln} z}{(z+a)^2+b^2} \, dz$ where $C$ is the usual contour for this kind of integral with logarithm (actually, I don't know if I'm right). So, assuming there is a simple pole inside the interior $C$ , I apply the residue theorem as: $$\begin{align}\int_{C}\frac{\operatorname{Ln} z}{(z+a)^2+b^2} \, dz&= 2\pi i\left(\lim_{z\to-a+ib}\dfrac{(z+a-ib) \operatorname{Ln} z}{(z+a-ib)(z+a+ib)}\right)\\ \\ &=\frac{\pi}{b} \operatorname{Ln} (-a+ib)=\frac{\pi}{b}(\ln (\sqrt{a^2+b^2})+i\arg (-a+ib))\end{align}$$ Now, I must consider estimations of integrals on each semicircle, however I have not idea how I can reach it. Thanks for any hint.","I can't prove the following result: Well, I consider where is the usual contour for this kind of integral with logarithm (actually, I don't know if I'm right). So, assuming there is a simple pole inside the interior , I apply the residue theorem as: Now, I must consider estimations of integrals on each semicircle, however I have not idea how I can reach it. Thanks for any hint.","\displaystyle\int_0^\infty \frac{\ln x}{(x+a)^2+b^2} \, dx=\frac{\ln \sqrt{a^2+b^2}}{b}\arctan\frac{b}{a} \text{ for all } a,b \in \mathbb{R}. \displaystyle\int_C \frac{\operatorname{Ln} z}{(z+a)^2+b^2} \, dz C C \begin{align}\int_{C}\frac{\operatorname{Ln} z}{(z+a)^2+b^2} \, dz&=
2\pi i\left(\lim_{z\to-a+ib}\dfrac{(z+a-ib) \operatorname{Ln} z}{(z+a-ib)(z+a+ib)}\right)\\
\\
&=\frac{\pi}{b} \operatorname{Ln} (-a+ib)=\frac{\pi}{b}(\ln (\sqrt{a^2+b^2})+i\arg (-a+ib))\end{align}","['real-analysis', 'integration', 'complex-analysis', 'improper-integrals', 'contour-integration']"
11,The sum of integrals of a function and its inverse: $\int_{0}^{a}f+\int_{f(0)}^{f(a)}f^{-1}=af(a)$,The sum of integrals of a function and its inverse:,\int_{0}^{a}f+\int_{f(0)}^{f(a)}f^{-1}=af(a),"Regarding real numbers, the following appears to be true, or at least true with some modifications. Could you help me for the proof? $$\int_0^af(x)dx+\int_{f(0)}^{f(a)}f^{-1}(x)dx=af(a)$$","Regarding real numbers, the following appears to be true, or at least true with some modifications. Could you help me for the proof? $$\int_0^af(x)dx+\int_{f(0)}^{f(a)}f^{-1}(x)dx=af(a)$$",,"['real-analysis', 'calculus']"
12,Subspace of a separable space is separable,Subspace of a separable space is separable,,"Let $(X,d)$ be a separable space and $Y \subset X$. Show that $(Y,d)$ is also separable. My approach is as follows: Let $(X,d)$ be a separable space and $Y \subset X$. Since $X$ is separable, by definition there exists a countable dense subset in $X$, call it $K$. We want to show that $K \subset Y$. Is this a valid approach to take to the proof? P.S. This is an analysis course, not strictly a topology course.","Let $(X,d)$ be a separable space and $Y \subset X$. Show that $(Y,d)$ is also separable. My approach is as follows: Let $(X,d)$ be a separable space and $Y \subset X$. Since $X$ is separable, by definition there exists a countable dense subset in $X$, call it $K$. We want to show that $K \subset Y$. Is this a valid approach to take to the proof? P.S. This is an analysis course, not strictly a topology course.",,"['real-analysis', 'metric-spaces']"
13,Continuity of outer measure induced by measure from below,Continuity of outer measure induced by measure from below,,"This question comes from Bass, Ex 4.15. Given a finite measure space $(X, \mathcal{A}, \mu)$, we can define an outer measure $\mu^*$ as $$   \mu^*(A) := \inf\{ \mu(B) : A \subset B , B \in \mathcal{A} \} \:. $$ (One can check that this is an outer measure and that $\mu^*$ agrees with $\mu$ on $\mathcal{A}$). I want to show now that if $\{A_n\}_{n \geq 1}$ is a sequence of increasing sets (not necessarily in $\mathcal{A}$) such that $A_n \nearrow A$, then $\mu^*(A_n) \nearrow \mu^*(A)$. I have this so far. By monotonicity of outer measure, $\mu^*(A_n) \leq \mu^*(A)$ for all $n$, hence $\lim_{n \rightarrow \infty} \mu^*(A_n) \leq \mu^*(A)$. It remains to show that $\mu^*(A) \leq \lim_{n \rightarrow \infty} \mu^*(A_n)$. My rough idea is as follows. Fix any $\epsilon > 0$. We can pick a sequence of sets $\{G_i\}_{i \geq 1}$ in $\mathcal{A}$ such that $\mu^*(A_i) \geq \mu(G_i) - \epsilon/2^{i}$. If the $\{G_i\}$ were an increasing set then we'd be done by using the continuity of measure from below. However, they are not. But, we can define $H_n := \bigcup_{i=1}^{n} G_i$ which is an increasing sequence by definition. Since $A \subset \bigcup_{n \geq 1} H_n$, we have that $$ \mu^*(A) \leq \mu( \bigcup_{n \geq 1} H_n ) = \lim_{n \rightarrow \infty} \mu(H_n) \:. $$ Now I am not quite sure how to finish off. I want to say something to the effect of $\mu(H_n) \leq \mu^*(A_n) + \epsilon$. However, since the $A_n$'s are not measurable, I am having trouble making such a comparison.","This question comes from Bass, Ex 4.15. Given a finite measure space $(X, \mathcal{A}, \mu)$, we can define an outer measure $\mu^*$ as $$   \mu^*(A) := \inf\{ \mu(B) : A \subset B , B \in \mathcal{A} \} \:. $$ (One can check that this is an outer measure and that $\mu^*$ agrees with $\mu$ on $\mathcal{A}$). I want to show now that if $\{A_n\}_{n \geq 1}$ is a sequence of increasing sets (not necessarily in $\mathcal{A}$) such that $A_n \nearrow A$, then $\mu^*(A_n) \nearrow \mu^*(A)$. I have this so far. By monotonicity of outer measure, $\mu^*(A_n) \leq \mu^*(A)$ for all $n$, hence $\lim_{n \rightarrow \infty} \mu^*(A_n) \leq \mu^*(A)$. It remains to show that $\mu^*(A) \leq \lim_{n \rightarrow \infty} \mu^*(A_n)$. My rough idea is as follows. Fix any $\epsilon > 0$. We can pick a sequence of sets $\{G_i\}_{i \geq 1}$ in $\mathcal{A}$ such that $\mu^*(A_i) \geq \mu(G_i) - \epsilon/2^{i}$. If the $\{G_i\}$ were an increasing set then we'd be done by using the continuity of measure from below. However, they are not. But, we can define $H_n := \bigcup_{i=1}^{n} G_i$ which is an increasing sequence by definition. Since $A \subset \bigcup_{n \geq 1} H_n$, we have that $$ \mu^*(A) \leq \mu( \bigcup_{n \geq 1} H_n ) = \lim_{n \rightarrow \infty} \mu(H_n) \:. $$ Now I am not quite sure how to finish off. I want to say something to the effect of $\mu(H_n) \leq \mu^*(A_n) + \epsilon$. However, since the $A_n$'s are not measurable, I am having trouble making such a comparison.",,"['real-analysis', 'measure-theory']"
14,limit of $n \left(e-\sum_{k=0}^{n-1} \frac{1}{k!}\right) = ?$,limit of,n \left(e-\sum_{k=0}^{n-1} \frac{1}{k!}\right) = ?,"As in the title: $$ \lim_{n\to\infty} n \left(e-\sum_{k=0}^{n-1} \frac{1}{k!}\right) = ? $$ Numerically, it seems 0, but how to prove/disprove it? I tried to show that the speed of convergence of the sum to e is faster than $1/n$ but with no success.","As in the title: $$ \lim_{n\to\infty} n \left(e-\sum_{k=0}^{n-1} \frac{1}{k!}\right) = ? $$ Numerically, it seems 0, but how to prove/disprove it? I tried to show that the speed of convergence of the sum to e is faster than $1/n$ but with no success.",,"['real-analysis', 'sequences-and-series', 'limits']"
15,Every open set in $\mathbb{R}$ is the countable union of rational open intervals,Every open set in  is the countable union of rational open intervals,\mathbb{R},How would I prove that every open set in $\mathbb{R}$ is the countable  union of open intervals with rational endpoints? Context: I am trying to see whether the set of rational open subsets of $\mathbb{R}$ is countable.,How would I prove that every open set in $\mathbb{R}$ is the countable  union of open intervals with rational endpoints? Context: I am trying to see whether the set of rational open subsets of $\mathbb{R}$ is countable.,,"['real-analysis', 'real-numbers']"
16,"Help explain why ""$f$ be finite a.e"" is necessary in a theorem.","Help explain why "" be finite a.e"" is necessary in a theorem.",f,"A theorem states Let $ϕ$ be continuous on $\Bbb{R}$, let $f$ be finite on $Ω$ a.e., then $ϕ∘f$ is measurable if $f$ is measurable. The following is the proof from my textbook. Since $ϕ$ is defined on $\Bbb{R}$ and is continuous, then $ϕ^{-1} (G)$ is an open set if $G$ is open, by Theorem 24. Now given any open set $G$, $(ϕ∘f)^{-1} (G)=f^{-1} (ϕ^{-1} (G))$ is measurable since $f$ is measurable and $ϕ^{-1} (G)$ is open, which gives $ϕ∘f$ is measurable. My question is why ""$f$ be finite on $Ω$ a.e."" is necessary? The proof seems not using this condition. What happens if $f$ is infinite on a non-zero-measure subset of $\Omega$? Thank you!","A theorem states Let $ϕ$ be continuous on $\Bbb{R}$, let $f$ be finite on $Ω$ a.e., then $ϕ∘f$ is measurable if $f$ is measurable. The following is the proof from my textbook. Since $ϕ$ is defined on $\Bbb{R}$ and is continuous, then $ϕ^{-1} (G)$ is an open set if $G$ is open, by Theorem 24. Now given any open set $G$, $(ϕ∘f)^{-1} (G)=f^{-1} (ϕ^{-1} (G))$ is measurable since $f$ is measurable and $ϕ^{-1} (G)$ is open, which gives $ϕ∘f$ is measurable. My question is why ""$f$ be finite on $Ω$ a.e."" is necessary? The proof seems not using this condition. What happens if $f$ is infinite on a non-zero-measure subset of $\Omega$? Thank you!",,"['real-analysis', 'measure-theory']"
17,"How exactly does one define the ""spectral measure"" of an operator?","How exactly does one define the ""spectral measure"" of an operator?",,"I am seeing kind of different definitions of ""spectral measure"" at different places and its not clear to me as to what is the universal idea. It would be great to get some ""standard"" definition. In my zone of studies now, I am seeing this occur frequently : that given a self-adjoint (possibly unbounded) operator $A$ on a separable Hilbert space and a $\phi \in \cal{H}_{-1}(A)$ then the ""spectral measure"" associated to possibly this pair $(A,\phi)$ is that unique $\mu$ such that for all $\lambda$ (may be one wants to restrict to $\lambda \in \mathbb{C} \backslash \mathbb{R}$ for some reason) the following holds, $<(A -\lambda I  )^{-1}\phi,\phi > = \int_{\mathbb{R}} \frac{ d\mu(x)}{x - \lambda }$ (0) Does this have some standard name which I can look up in some textbook? (1) I am not sure what is the innerproduct used in the LHS. (2) Can someone give an example of how this might actually be calculated? (3) Can one use $(A-\lambda I)^{-2}$ here to define something analogous?","I am seeing kind of different definitions of ""spectral measure"" at different places and its not clear to me as to what is the universal idea. It would be great to get some ""standard"" definition. In my zone of studies now, I am seeing this occur frequently : that given a self-adjoint (possibly unbounded) operator $A$ on a separable Hilbert space and a $\phi \in \cal{H}_{-1}(A)$ then the ""spectral measure"" associated to possibly this pair $(A,\phi)$ is that unique $\mu$ such that for all $\lambda$ (may be one wants to restrict to $\lambda \in \mathbb{C} \backslash \mathbb{R}$ for some reason) the following holds, $<(A -\lambda I  )^{-1}\phi,\phi > = \int_{\mathbb{R}} \frac{ d\mu(x)}{x - \lambda }$ (0) Does this have some standard name which I can look up in some textbook? (1) I am not sure what is the innerproduct used in the LHS. (2) Can someone give an example of how this might actually be calculated? (3) Can one use $(A-\lambda I)^{-2}$ here to define something analogous?",,"['real-analysis', 'functional-analysis', 'operator-theory', 'spectral-theory']"
18,Show 1/x is Lipschitz continuous,Show 1/x is Lipschitz continuous,,"Show that $\space f:(c,∞)\rightarrow\mathbb{R}$ for some $c>0$, and defined by $f(x)= \frac{1}{x}$, is Lipschitz continuous. I'm not quite certain how to go about this.","Show that $\space f:(c,∞)\rightarrow\mathbb{R}$ for some $c>0$, and defined by $f(x)= \frac{1}{x}$, is Lipschitz continuous. I'm not quite certain how to go about this.",,"['real-analysis', 'lipschitz-functions']"
19,Curvature of curve not parametrized by arclength,Curvature of curve not parametrized by arclength,,"If I have a curve that is not parametrized by arclength, is the curvature still $||\gamma''(t)||$? I am not so sure about this, cause then we don't know that $\gamma'' \perp \gamma'$ holds, so the concept of curvature might not be transferable to this situation. So is this only defined for curves with constant speed?","If I have a curve that is not parametrized by arclength, is the curvature still $||\gamma''(t)||$? I am not so sure about this, cause then we don't know that $\gamma'' \perp \gamma'$ holds, so the concept of curvature might not be transferable to this situation. So is this only defined for curves with constant speed?",,"['real-analysis', 'analysis']"
20,composition of Lebesgue measurable functions is not Lebesgue mesurable,composition of Lebesgue measurable functions is not Lebesgue mesurable,,"Let $f,g:\mathbb{R}\longrightarrow \mathbb{R}$ be Lebesgue measurable. If $f$ is Borel measurable, then $f\circ g$ is Lebesgue mesuarable. In general, $f\circ g$ is not necessarily Lebesgue measurable. Is there any counterexample?","Let $f,g:\mathbb{R}\longrightarrow \mathbb{R}$ be Lebesgue measurable. If $f$ is Borel measurable, then $f\circ g$ is Lebesgue mesuarable. In general, $f\circ g$ is not necessarily Lebesgue measurable. Is there any counterexample?",,"['real-analysis', 'analysis', 'measure-theory']"
21,Integral $\int_0^1 \log x \frac{(1+x^2)x^{c-2}}{1-x^{2c}}dx=-\left(\frac{\pi}{2c}\right)^2\sec ^2 \frac{\pi}{2c}$,Integral,\int_0^1 \log x \frac{(1+x^2)x^{c-2}}{1-x^{2c}}dx=-\left(\frac{\pi}{2c}\right)^2\sec ^2 \frac{\pi}{2c},"Hi I am trying to prove this result $$ I:=\int_0^1 \log x \frac{(1+x^2)x^{c-2}}{1-x^{2c}}dx=-\left(\frac{\pi}{2c}\right)^2\sec ^2 \frac{\pi}{2c},\quad c>1. $$ Thanks.  Since $x\in[0,1] $ we can write$$ I=\sum_{n=0}^\infty \int_0^1 \log x (1+x^2) x^{c-2+2cn}dx. $$ since $\sum_{n=0}^\infty x^n= (1-x)^{-1} , |x| < 1.$  Simplifying $$ I=\sum_n \int_0^1 \log x\, x^{c-2+2n} dx+\sum_n \int_0^1 \log x \, x^{c+2cn}\, dx. $$ Now we can write $$ I=-\frac{1}{4}\psi_1 \left(\frac{c+1}{2}\right)-\frac{1}{4} \psi_1\left( \frac{3+c}{2}\right) $$ where I summed the results of the integrals using general result of $\int_0^1 \log x \, x^n \, dx=-\frac{1}{(n+1)^2}$.  Howeever this is not the result $\sec^2...$.  Thanks The function $\psi$ is the polygamma function which is defined in general by $$ \psi_m(z)=\frac{d^{m+1}}{dz^{m+1}} \log \Gamma(z) $$ and $\Gamma(z)=(z-1)!$, for this case $m=1$.","Hi I am trying to prove this result $$ I:=\int_0^1 \log x \frac{(1+x^2)x^{c-2}}{1-x^{2c}}dx=-\left(\frac{\pi}{2c}\right)^2\sec ^2 \frac{\pi}{2c},\quad c>1. $$ Thanks.  Since $x\in[0,1] $ we can write$$ I=\sum_{n=0}^\infty \int_0^1 \log x (1+x^2) x^{c-2+2cn}dx. $$ since $\sum_{n=0}^\infty x^n= (1-x)^{-1} , |x| < 1.$  Simplifying $$ I=\sum_n \int_0^1 \log x\, x^{c-2+2n} dx+\sum_n \int_0^1 \log x \, x^{c+2cn}\, dx. $$ Now we can write $$ I=-\frac{1}{4}\psi_1 \left(\frac{c+1}{2}\right)-\frac{1}{4} \psi_1\left( \frac{3+c}{2}\right) $$ where I summed the results of the integrals using general result of $\int_0^1 \log x \, x^n \, dx=-\frac{1}{(n+1)^2}$.  Howeever this is not the result $\sec^2...$.  Thanks The function $\psi$ is the polygamma function which is defined in general by $$ \psi_m(z)=\frac{d^{m+1}}{dz^{m+1}} \log \Gamma(z) $$ and $\Gamma(z)=(z-1)!$, for this case $m=1$.",,"['calculus', 'real-analysis', 'integration', 'complex-analysis', 'definite-integrals']"
22,$f: \mathbb N \to \mathbb N $ is injective $ \implies $ $\lim\Big(f(n)\Big)=+ \infty $?,is injective  ?,f: \mathbb N \to \mathbb N   \implies  \lim\Big(f(n)\Big)=+ \infty ,"If $f: \mathbb N \to \mathbb N $ is an injective function , then for the sequence $\Big(f(n)\Big)$ do we have $\lim\Big(f(n)\Big)=+ \infty  $ ?","If $f: \mathbb N \to \mathbb N $ is an injective function , then for the sequence $\Big(f(n)\Big)$ do we have $\lim\Big(f(n)\Big)=+ \infty  $ ?",,['real-analysis']
23,Behavior of $f'(x)$ as $x \to \infty$ given the behavior of $f(x)$ and $f''(x)$ [duplicate],Behavior of  as  given the behavior of  and  [duplicate],f'(x) x \to \infty f(x) f''(x),"This question already has answers here : If $f(x)\to 0$ as $x\to\infty$ and $f''$ is bounded, show that $f'(x)\to0$ as $x\to\infty$ (4 answers) Closed 10 years ago . Suppose $f$ is twice differentiable on $(a, \infty)$ and let $\lim_{x \to \infty}f(x) = 0$. Also assume that $f''(x)$ is bounded for all $x \in (a, \infty)$. Prove that $f'(x) \to 0$ as $x \to \infty$. Note that if $f'(x)$ tends to a limit as $x \to \infty$ then this limit must be $0$ (because of the relation $f(x) - f(x/2) = (x/2)f'(c)$). What we need to establish now is that $f'(x)$ does tend to a limit. To show that we need to somehow use the fact that $f''(x)$ is bounded. Let $b > 0$ be any arbitrary but fixed number then we have relation $f(x + b) - f(x) = bf'(c)$ and this again implies that for all sufficiently large $x$ the derivative $f'$ takes a small value in the interval $(x, x + b)$. What we need to show that eventually all the values of $f'$ are small in such intervals. Now we can take $b$ as small as we please, we can therefore find a value $N > 0$  such that $f'$ takes very small value for at least one point in $(x, x + b)$ for all $x  > N$. If $p, q$ are two points of this interval $(x, x + b)$ then $|f'(p) - f'(q)| = |(p - q)f''(\xi)| \leq b|f''(\xi)|$ and since $f''$ is bounded it follows that $f'(p) - f'(q)$ is small so that effectively all values of $f'$ in $(x, x + b)$ are small. The above is a non-rigorous argument which I have not been able to make rigorous by using $\epsilon, \delta$ in proper manner. Maybe the approach used above can't be made rigorous and I am wrong path (or may be not!). Please help me in making above approach rigorous or suggest some alternative method. Further there is another generalization available: If $f(x) $ tends to a finite limit as $x \to \infty$ and $f^{(n + 1)}(x)$ is bounded then show that $f^{(n)}(x) \to 0$ as $x \to \infty$. I have not tried to solve this general problem and any hint would be great.","This question already has answers here : If $f(x)\to 0$ as $x\to\infty$ and $f''$ is bounded, show that $f'(x)\to0$ as $x\to\infty$ (4 answers) Closed 10 years ago . Suppose $f$ is twice differentiable on $(a, \infty)$ and let $\lim_{x \to \infty}f(x) = 0$. Also assume that $f''(x)$ is bounded for all $x \in (a, \infty)$. Prove that $f'(x) \to 0$ as $x \to \infty$. Note that if $f'(x)$ tends to a limit as $x \to \infty$ then this limit must be $0$ (because of the relation $f(x) - f(x/2) = (x/2)f'(c)$). What we need to establish now is that $f'(x)$ does tend to a limit. To show that we need to somehow use the fact that $f''(x)$ is bounded. Let $b > 0$ be any arbitrary but fixed number then we have relation $f(x + b) - f(x) = bf'(c)$ and this again implies that for all sufficiently large $x$ the derivative $f'$ takes a small value in the interval $(x, x + b)$. What we need to show that eventually all the values of $f'$ are small in such intervals. Now we can take $b$ as small as we please, we can therefore find a value $N > 0$  such that $f'$ takes very small value for at least one point in $(x, x + b)$ for all $x  > N$. If $p, q$ are two points of this interval $(x, x + b)$ then $|f'(p) - f'(q)| = |(p - q)f''(\xi)| \leq b|f''(\xi)|$ and since $f''$ is bounded it follows that $f'(p) - f'(q)$ is small so that effectively all values of $f'$ in $(x, x + b)$ are small. The above is a non-rigorous argument which I have not been able to make rigorous by using $\epsilon, \delta$ in proper manner. Maybe the approach used above can't be made rigorous and I am wrong path (or may be not!). Please help me in making above approach rigorous or suggest some alternative method. Further there is another generalization available: If $f(x) $ tends to a finite limit as $x \to \infty$ and $f^{(n + 1)}(x)$ is bounded then show that $f^{(n)}(x) \to 0$ as $x \to \infty$. I have not tried to solve this general problem and any hint would be great.",,"['calculus', 'real-analysis']"
24,Integral $ \int_0^1 \frac{\ln \ln (1/x)}{1+x^{2p}} dx$...Definite Integral,Integral ...Definite Integral, \int_0^1 \frac{\ln \ln (1/x)}{1+x^{2p}} dx,"Calculate $$ I_1:=\int_0^1 \frac{\ln \ln (1/x)}{1+x^{2p}} dx, \ p \geq 1. $$ I am trying to solve this integral $I_1$.  I know how to solve a related integral $I_2$ $$ I_2:=\int_0^1 \frac{\ln \ln (1/x)}{1+x^2} dx=\frac{\pi}{4}\bigg(2\ln 2 +3\ln \pi-4\ln\Gamma\big(\frac{1}{4}\big) \bigg) $$ but I am not sure how to use that result here. In this case I just use the substitution $x=e^{-\xi}$ and than use a series expansion.  The result is $$ I_2=\int_0^\infty \frac{\xi^s e^{-\xi}}{1+e^{-2\xi}} d\xi=\sum_{n=0}^\infty (-1)^n \frac{\Gamma(s+1)}{(2n+1)^{s+1}}=\Gamma(s+1)L(s+1,\chi_4) $$ where L is the Dirichlet L-Function where $\chi_4$ is the unique non-principal character.  This result is further simplified but takes some work.  I am interested in the general case above, $I_1$ Thanks","Calculate $$ I_1:=\int_0^1 \frac{\ln \ln (1/x)}{1+x^{2p}} dx, \ p \geq 1. $$ I am trying to solve this integral $I_1$.  I know how to solve a related integral $I_2$ $$ I_2:=\int_0^1 \frac{\ln \ln (1/x)}{1+x^2} dx=\frac{\pi}{4}\bigg(2\ln 2 +3\ln \pi-4\ln\Gamma\big(\frac{1}{4}\big) \bigg) $$ but I am not sure how to use that result here. In this case I just use the substitution $x=e^{-\xi}$ and than use a series expansion.  The result is $$ I_2=\int_0^\infty \frac{\xi^s e^{-\xi}}{1+e^{-2\xi}} d\xi=\sum_{n=0}^\infty (-1)^n \frac{\Gamma(s+1)}{(2n+1)^{s+1}}=\Gamma(s+1)L(s+1,\chi_4) $$ where L is the Dirichlet L-Function where $\chi_4$ is the unique non-principal character.  This result is further simplified but takes some work.  I am interested in the general case above, $I_1$ Thanks",,"['real-analysis', 'integration', 'definite-integrals', 'special-functions', 'contour-integration']"
25,How to show that the set of all Lipschitz functions on a compact set X is dense in C(X)?,How to show that the set of all Lipschitz functions on a compact set X is dense in C(X)?,,"Im reading Chapter12 of Carothers' Real Analysis, 1ed. Here is a reading material of Lip(X) which denotes the set of all Lipschitz functions on a compact set X, How to show that the set of all Lipschitz functions on a compact set X is dense in C(X)? I want to show $Ball_ε$(g) ∩ Lip(X) ≠ empty for every continuous function g ∈ $C$(X) and every ε>0. But I got stuck here cos I need to find a function f in Lip(X) such that $||f - g||_∞$<ε.","Im reading Chapter12 of Carothers' Real Analysis, 1ed. Here is a reading material of Lip(X) which denotes the set of all Lipschitz functions on a compact set X, How to show that the set of all Lipschitz functions on a compact set X is dense in C(X)? I want to show $Ball_ε$(g) ∩ Lip(X) ≠ empty for every continuous function g ∈ $C$(X) and every ε>0. But I got stuck here cos I need to find a function f in Lip(X) such that $||f - g||_∞$<ε.",,"['real-analysis', 'analysis']"
26,Minimizing $\int_0^1|f'(t)|^2dt$ subject to $f(0)=0$ and $f(1)=1$.,Minimizing  subject to  and .,\int_0^1|f'(t)|^2dt f(0)=0 f(1)=1,"Say you want to minimize  $$ \int_0^1 \!|f'(t)|^2 \,dt, $$ subject to $f(0)=0$ and $f(1)=1$, amongst infinitely differentiable functions. Is this even possible? My guess is the minimizing function $f(x)=x$. My line of thinking... A function $f$ minimizes $\int_0^1|f'(t)|^2\,dt$ iff it minimizes $\int_0^1 (1+|f'(t)|^2)\, dt$, iff it minimizes $\int_0^1\sqrt{1+|f'(t)|^2}\,dt$. Since this last expression is the arc length formula, the question is equivalent to finding the shortest continuous function from $(0,0)$ to $(1,1)$, which is just $f(x)=x$, and this is the unique function which minimizes the integral. Is this line of reasoning sound? Maybe I overlooked some subtlety.","Say you want to minimize  $$ \int_0^1 \!|f'(t)|^2 \,dt, $$ subject to $f(0)=0$ and $f(1)=1$, amongst infinitely differentiable functions. Is this even possible? My guess is the minimizing function $f(x)=x$. My line of thinking... A function $f$ minimizes $\int_0^1|f'(t)|^2\,dt$ iff it minimizes $\int_0^1 (1+|f'(t)|^2)\, dt$, iff it minimizes $\int_0^1\sqrt{1+|f'(t)|^2}\,dt$. Since this last expression is the arc length formula, the question is equivalent to finding the shortest continuous function from $(0,0)$ to $(1,1)$, which is just $f(x)=x$, and this is the unique function which minimizes the integral. Is this line of reasoning sound? Maybe I overlooked some subtlety.",,"['real-analysis', 'integration', 'optimization']"
27,Series $\sum \frac{\sin(n)}{n} \cdot \left(1+\cdots +\frac{1}{n}\right)$ convergence question,Series  convergence question,\sum \frac{\sin(n)}{n} \cdot \left(1+\cdots +\frac{1}{n}\right),"I want to test the convergence of the series.  $$ \sum_{n=1}^{\infty} \frac{\sin(n)}{n} \cdot \left(1+\frac{1}{2} + \cdots + \frac{1}{n}\right)$$ My guess is this should diverge, and the below I provide the details Note that $\displaystyle 1+\frac{1}{2} + \cdots + \frac{1}{n} \geq 1 +\frac{1}{2} + \frac{1}{4} + \cdots + \frac{1}{2^{n-1}} = 2 \cdot \left(1-\frac{1}{2^{n}}\right)$ So from above I conclude that $$\sum_{n=1}^{\infty} \frac{\sin(n)}{n} \cdot \left(1+\frac{1}{2}+\cdots + \frac{1}{n}\right) \geq  2 \cdot \sum_{n=1}^{\infty} \frac{\sin(n)}{n} - \cdot \underbrace{\sum_{n=1}^{\infty} \frac{\sin(n)}{n \cdot 2^{n}}}_{A}$$ Now $A$ converges and I think $\displaystyle \sum \frac{\sin(n)}{n}$ diverges hence my given series should diverge. Am I correct in my reasoning?","I want to test the convergence of the series.  $$ \sum_{n=1}^{\infty} \frac{\sin(n)}{n} \cdot \left(1+\frac{1}{2} + \cdots + \frac{1}{n}\right)$$ My guess is this should diverge, and the below I provide the details Note that $\displaystyle 1+\frac{1}{2} + \cdots + \frac{1}{n} \geq 1 +\frac{1}{2} + \frac{1}{4} + \cdots + \frac{1}{2^{n-1}} = 2 \cdot \left(1-\frac{1}{2^{n}}\right)$ So from above I conclude that $$\sum_{n=1}^{\infty} \frac{\sin(n)}{n} \cdot \left(1+\frac{1}{2}+\cdots + \frac{1}{n}\right) \geq  2 \cdot \sum_{n=1}^{\infty} \frac{\sin(n)}{n} - \cdot \underbrace{\sum_{n=1}^{\infty} \frac{\sin(n)}{n \cdot 2^{n}}}_{A}$$ Now $A$ converges and I think $\displaystyle \sum \frac{\sin(n)}{n}$ diverges hence my given series should diverge. Am I correct in my reasoning?",,"['real-analysis', 'sequences-and-series']"
28,Is there a problem in studying analysis before calculus? [closed],Is there a problem in studying analysis before calculus? [closed],,"Closed . This question is opinion-based . It is not currently accepting answers. Want to improve this question? Update the question so it can be answered with facts and citations by editing this post . Closed 7 years ago . Improve this question Is there a problem in studying analysis before calculus? Most people say that analysis is rigorous calculus, the university I'm studying teaches calculus first because they believe it's better for the student to have a intuitive background (which is obtained with calculus) and the go to analysis but I've seen some other universities that teach analysis first, for undegraduate levels. I decided to study analysis first and I'm being able to understand it, I'm just not sure if any loss is made by not studying calculus. What could also be a nice alternative would be to study some Springer books on calculus and analysis, what do you think? EDIT: One of the doubts I also have is if I'll be able to use calculus if studying only analysis.","Closed . This question is opinion-based . It is not currently accepting answers. Want to improve this question? Update the question so it can be answered with facts and citations by editing this post . Closed 7 years ago . Improve this question Is there a problem in studying analysis before calculus? Most people say that analysis is rigorous calculus, the university I'm studying teaches calculus first because they believe it's better for the student to have a intuitive background (which is obtained with calculus) and the go to analysis but I've seen some other universities that teach analysis first, for undegraduate levels. I decided to study analysis first and I'm being able to understand it, I'm just not sure if any loss is made by not studying calculus. What could also be a nice alternative would be to study some Springer books on calculus and analysis, what do you think? EDIT: One of the doubts I also have is if I'll be able to use calculus if studying only analysis.",,"['calculus', 'real-analysis', 'self-learning']"
29,Show that $\lim_{n\to\infty}\frac{\lfloor nx \rfloor}{n}=x$,Show that,\lim_{n\to\infty}\frac{\lfloor nx \rfloor}{n}=x,How do I prove that $\lim_{n\to\infty}\frac{\lfloor nx \rfloor}{n}=x$ for $x\in\mathbb{R}$? I see that $\lfloor nx\rfloor = n\lfloor x \rfloor + \lfloor n(x-\lfloor x \rfloor )\rfloor + O(1)$ but I'm not sure how to deal with the middle term.,How do I prove that $\lim_{n\to\infty}\frac{\lfloor nx \rfloor}{n}=x$ for $x\in\mathbb{R}$? I see that $\lfloor nx\rfloor = n\lfloor x \rfloor + \lfloor n(x-\lfloor x \rfloor )\rfloor + O(1)$ but I'm not sure how to deal with the middle term.,,['real-analysis']
30,Rate of convergence of modified Newton's method for multiple roots,Rate of convergence of modified Newton's method for multiple roots,,"I've got a problem with a modified Newton's method. We've got a function $f \in C^{(k+1)}$ and $r$ which is it's multiple root of multiciplity $k$. Also $f^{(k)}(r) \neq 0$ and $f'(x) \neq 0 $ in the neighbourhood of $r$. The modified Newton's method is $$x_{n+1} = x_n - k\frac{f(x_n)}{f'(x_n)}$$ How to prove that $x_n$ converges to $r$ quadratically? I found one method using a function $G(x) = (x-r) f'(x) - kf(x)$ but then it's said that $ G^{(k+1)}(r) \neq 0$ but it comes from the fact that $f^{(k+1)}(r) \neq 0$, but it doesn't necessarily have to be true, am I right? We know that $f^{(k)}(r) \neq 0$, but we have no information about the $k+1$st derivative. I would appreciate if somebody explained to me precisely, why it quadratically converges.","I've got a problem with a modified Newton's method. We've got a function $f \in C^{(k+1)}$ and $r$ which is it's multiple root of multiciplity $k$. Also $f^{(k)}(r) \neq 0$ and $f'(x) \neq 0 $ in the neighbourhood of $r$. The modified Newton's method is $$x_{n+1} = x_n - k\frac{f(x_n)}{f'(x_n)}$$ How to prove that $x_n$ converges to $r$ quadratically? I found one method using a function $G(x) = (x-r) f'(x) - kf(x)$ but then it's said that $ G^{(k+1)}(r) \neq 0$ but it comes from the fact that $f^{(k+1)}(r) \neq 0$, but it doesn't necessarily have to be true, am I right? We know that $f^{(k)}(r) \neq 0$, but we have no information about the $k+1$st derivative. I would appreciate if somebody explained to me precisely, why it quadratically converges.",,"['real-analysis', 'numerical-methods']"
31,Another simple series convergence question: $\sum\limits_{n=3}^\infty \frac1{n (\ln n)\ln(\ln n)}$,Another simple series convergence question:,\sum\limits_{n=3}^\infty \frac1{n (\ln n)\ln(\ln n)},"I'm being asked to determine if $\displaystyle\sum\limits_{n=3}^\infty \frac1{n (\ln n)\ln(\ln n)}$ converges. So, using Cauchy's Condensation Test, I reduced the problem to one of determining the convergence of $\displaystyle\sum\limits_{n=3}^\infty\frac 1{n\ln (n\ln 2)}$. Am I on the right path, and how do I proceed from here?","I'm being asked to determine if $\displaystyle\sum\limits_{n=3}^\infty \frac1{n (\ln n)\ln(\ln n)}$ converges. So, using Cauchy's Condensation Test, I reduced the problem to one of determining the convergence of $\displaystyle\sum\limits_{n=3}^\infty\frac 1{n\ln (n\ln 2)}$. Am I on the right path, and how do I proceed from here?",,"['real-analysis', 'sequences-and-series', 'convergence-divergence', 'divergent-series']"
32,Lie group homomorphism from $\mathbb{R}\rightarrow S^1$,Lie group homomorphism from,\mathbb{R}\rightarrow S^1,"I need to prove that every Lie group homomorphism from $\mathbb{R}\rightarrow S^1$ is of the form $x\mapsto e^{iax}$ for some $a\in\mathbb{R}$. Here is my attempt: As it is group homomorphism so it must satisfies $\phi(x+y)=\phi(x).\phi(y),\forall x,y\in\mathbb{R}$, I know one result if some continous function satisfies this rule, then it is of the form $e^x$, is this the same trick here we need to apply?","I need to prove that every Lie group homomorphism from $\mathbb{R}\rightarrow S^1$ is of the form $x\mapsto e^{iax}$ for some $a\in\mathbb{R}$. Here is my attempt: As it is group homomorphism so it must satisfies $\phi(x+y)=\phi(x).\phi(y),\forall x,y\in\mathbb{R}$, I know one result if some continous function satisfies this rule, then it is of the form $e^x$, is this the same trick here we need to apply?",,"['real-analysis', 'group-theory', 'lie-groups']"
33,Absolute value of Lebesgue integrable function,Absolute value of Lebesgue integrable function,,I want to prove that a measurable function $f$ is Lebesgue integrable iff $|f|$ is. I've proved the first part  but how can I show if $|f|$ is Lebesgue integrable then $f$ is ?,I want to prove that a measurable function $f$ is Lebesgue integrable iff $|f|$ is. I've proved the first part  but how can I show if $|f|$ is Lebesgue integrable then $f$ is ?,,['real-analysis']
34,Formal definition of *not* uniformly continuous,Formal definition of *not* uniformly continuous,,"The definition for uniform continuity for a function $f : X \to Y$ is that for all $\epsilon > 0$, there exists a $\delta > 0$ such that $d_Y(f(p),f(q)) < \epsilon$ for all $p,q \in X$ such that $d_X(p,q) < \delta$. Mathematically, we can write this definition as $\forall \epsilon > 0, \exists \delta > 0 \textrm{ such that } d_Y(f(p),f(q) < \epsilon\ \forall p,q \in X \textrm{ for which } d_X(p,q) < \delta.$ I have learned that when negating a statement, one switches the quantifiers, and reverses any equality/inequality statements in the conclusion. Following these rules, the definition for not uniformly continuous would be $\exists \epsilon > 0\ \forall \delta > 0 \textrm{ such that } d_Y(f(p),f(q)) \ge \epsilon\ \exists p,q \in X \textrm{ for which } d_X(p,q) < \delta.$ This, however makes little sense as written. I believe that I would be better served writing this as $\exists p,q \in X \textrm{ such that } \forall \delta > 0 \textrm { for which } d_X(p,q) < \delta, \exists \epsilon > 0 \textrm{ such that } d_Y(f(p),f(q)) \ge \epsilon.$ My question is: does my latter define criteria for which a function is not uniformly continuous? In other words, is this a proper negation of the definition? I believe that it should provide a criteria for a function not being uniformly continuous, but is this the same thing as the wholesale negation of the definition?","The definition for uniform continuity for a function $f : X \to Y$ is that for all $\epsilon > 0$, there exists a $\delta > 0$ such that $d_Y(f(p),f(q)) < \epsilon$ for all $p,q \in X$ such that $d_X(p,q) < \delta$. Mathematically, we can write this definition as $\forall \epsilon > 0, \exists \delta > 0 \textrm{ such that } d_Y(f(p),f(q) < \epsilon\ \forall p,q \in X \textrm{ for which } d_X(p,q) < \delta.$ I have learned that when negating a statement, one switches the quantifiers, and reverses any equality/inequality statements in the conclusion. Following these rules, the definition for not uniformly continuous would be $\exists \epsilon > 0\ \forall \delta > 0 \textrm{ such that } d_Y(f(p),f(q)) \ge \epsilon\ \exists p,q \in X \textrm{ for which } d_X(p,q) < \delta.$ This, however makes little sense as written. I believe that I would be better served writing this as $\exists p,q \in X \textrm{ such that } \forall \delta > 0 \textrm { for which } d_X(p,q) < \delta, \exists \epsilon > 0 \textrm{ such that } d_Y(f(p),f(q)) \ge \epsilon.$ My question is: does my latter define criteria for which a function is not uniformly continuous? In other words, is this a proper negation of the definition? I believe that it should provide a criteria for a function not being uniformly continuous, but is this the same thing as the wholesale negation of the definition?",,"['real-analysis', 'continuity']"
35,Compute $\lim_{n\to\infty} \int_{0}^{\pi/4}\tan^n x \ dx$,Compute,\lim_{n\to\infty} \int_{0}^{\pi/4}\tan^n x \ dx,I'm trying to find some nice proofs for the following limit $$\lim_{n\to\infty} \int_{0}^{\pi/4}\tan^n x \ dx$$ One way is to use the integration by parts. What else can we do here? Are there some fast ways? All answers will have my upvote. Thanks!,I'm trying to find some nice proofs for the following limit $$\lim_{n\to\infty} \int_{0}^{\pi/4}\tan^n x \ dx$$ One way is to use the integration by parts. What else can we do here? Are there some fast ways? All answers will have my upvote. Thanks!,,"['calculus', 'real-analysis', 'integration', 'limits', 'definite-integrals']"
36,Existence of Limit of a Sequence in $\mathbb{R}$,Existence of Limit of a Sequence in,\mathbb{R},"Let $x_n$ be a bounded sequence such that $x_{n+1}\leq x_n + 1/n$ for all $n \in \mathbb{N}$. Then prove or disprove that $\{x_n\}_n$ always converges . I think that it is not necessarily convergent, but I could not manage to find a counter example . Thanks for any help .","Let $x_n$ be a bounded sequence such that $x_{n+1}\leq x_n + 1/n$ for all $n \in \mathbb{N}$. Then prove or disprove that $\{x_n\}_n$ always converges . I think that it is not necessarily convergent, but I could not manage to find a counter example . Thanks for any help .",,['real-analysis']
37,Smooth closed real plane curve intersecting itself at infinitely many points,Smooth closed real plane curve intersecting itself at infinitely many points,,"Can a smooth closed real plane curve intersect itself at infinitely many points? It seems intuitively obvious that the answer should be no, yet I have no idea how to prove this or construct a counter-example. Here by smooth I mean $C^1$. If the answer is no, to which $C^k$ do we have to move for this geometric condition to be satisfied? Edit: Here is an attempt to formalize the above: Let $C$ be a closed curve and $P$ a point at its image. We say that $C$ intersects itself at P, if for all parametrizations $f: [a,b] \to C$ (which are of the same $C^k$ class as C), the equation $f(x)=P$ has at least two solutions in $[a,b]$. I think this would work for what I had in mind posing this question. By the way, I have no idea if this is the same with the transversal intersection definition proposed below.","Can a smooth closed real plane curve intersect itself at infinitely many points? It seems intuitively obvious that the answer should be no, yet I have no idea how to prove this or construct a counter-example. Here by smooth I mean $C^1$. If the answer is no, to which $C^k$ do we have to move for this geometric condition to be satisfied? Edit: Here is an attempt to formalize the above: Let $C$ be a closed curve and $P$ a point at its image. We say that $C$ intersects itself at P, if for all parametrizations $f: [a,b] \to C$ (which are of the same $C^k$ class as C), the equation $f(x)=P$ has at least two solutions in $[a,b]$. I think this would work for what I had in mind posing this question. By the way, I have no idea if this is the same with the transversal intersection definition proposed below.",,"['real-analysis', 'differential-geometry', 'differential-topology']"
38,Is there a Lebesgue measurable choice function?,Is there a Lebesgue measurable choice function?,,"A mapping $f$ from $\mathbb R$ to $\mathbb R$ is called a choice function if, for any $x, y \ {\rm in}\ \mathbb R$, $f(x)-x \in\mathbb Q$ and $f(x)=f(y)$ whenever $x-y$ is rational. My questions is:  Is there a Lebesgue-measurable choice function? Note: Here I use the equivalence relation for the construction of Vitali sets: x and y are in the same equivalence class iff x-y is rational. So choice functions pick up one element from each equivalence class as its representative value.","A mapping $f$ from $\mathbb R$ to $\mathbb R$ is called a choice function if, for any $x, y \ {\rm in}\ \mathbb R$, $f(x)-x \in\mathbb Q$ and $f(x)=f(y)$ whenever $x-y$ is rational. My questions is:  Is there a Lebesgue-measurable choice function? Note: Here I use the equivalence relation for the construction of Vitali sets: x and y are in the same equivalence class iff x-y is rational. So choice functions pick up one element from each equivalence class as its representative value.",,"['real-analysis', 'measure-theory', 'axiom-of-choice']"
39,Least Upper Bound Property $\implies$ Complete,Least Upper Bound Property  Complete,\implies,"I want to show that if an ordered field $X$ has the least upper bound property (meaning, every nonempty set $E$ which is bounded above has $\sup E \in X$), then it is complete (meaning, every Cauchy sequence converges in $X$). I know the converse is not true, but how do I prove this direction?","I want to show that if an ordered field $X$ has the least upper bound property (meaning, every nonempty set $E$ which is bounded above has $\sup E \in X$), then it is complete (meaning, every Cauchy sequence converges in $X$). I know the converse is not true, but how do I prove this direction?",,"['real-analysis', 'ordered-fields']"
40,"A function which changes sign infinitely many times, but for which L'Hôpital's rule works","A function which changes sign infinitely many times, but for which L'Hôpital's rule works",,"I found a visual proof of the L'Hôpital's rule by Giorgio Goldoni which uses an additional hypothesis: $g'(x)$ (i.e. the function which is at the denominator) cannot change its sign. My question is this: is this additional hypothesis reductive? Does it exist a function whose first derivative changes sign infinitely many times, but for which the De l'Hospital rule works?","I found a visual proof of the L'Hôpital's rule by Giorgio Goldoni which uses an additional hypothesis: $g'(x)$ (i.e. the function which is at the denominator) cannot change its sign. My question is this: is this additional hypothesis reductive? Does it exist a function whose first derivative changes sign infinitely many times, but for which the De l'Hospital rule works?",,"['real-analysis', 'functions', 'limits']"
41,Proof of exponential from homomorphism property,Proof of exponential from homomorphism property,,"I am going back through a bunch of calculus I learned in high school and proving the stuff that they just told us was true.  Along the way, I found I had to prove that if $f(x+y)=f(x)f(y)$ then $f$ is an exponential function. I managed to do it by induction on the integers, then generalizing to the rationals by the fundamental theorem of arithmetic, then to the reals by continuity.  It involved some rather ugly analysis for $f(1/2)$ because there are two possible values for the square root, which I didn't have to worry about for the other primes because, over $R$, odd powers are invertible.  But this prevents my argument from being generalized to the complex numbers. There has to be a more elegant way to get this result.  I'm hoping you can point me in the right direction toward a better proof (preferably without giving the whole thing away -- the point, after all, is to develop my ingenuity).","I am going back through a bunch of calculus I learned in high school and proving the stuff that they just told us was true.  Along the way, I found I had to prove that if $f(x+y)=f(x)f(y)$ then $f$ is an exponential function. I managed to do it by induction on the integers, then generalizing to the rationals by the fundamental theorem of arithmetic, then to the reals by continuity.  It involved some rather ugly analysis for $f(1/2)$ because there are two possible values for the square root, which I didn't have to worry about for the other primes because, over $R$, odd powers are invertible.  But this prevents my argument from being generalized to the complex numbers. There has to be a more elegant way to get this result.  I'm hoping you can point me in the right direction toward a better proof (preferably without giving the whole thing away -- the point, after all, is to develop my ingenuity).",,"['real-analysis', 'exponentiation']"
42,Continuous representatives in Sobolev Spaces,Continuous representatives in Sobolev Spaces,,"My question arise from the study of the possible extensions of Rademacher's Theorem to the Sobolev Space $W^{1,p}(\Omega)$, with $\Omega\subset \mathbb{R}^n$. In specific I'm studying the proof of the fact that any element of $W^{1,p}(\Omega)$ is differentiable almost everywhere if $p>n$. A key result in the proof is that any element in $W^{1,p}(\Omega)$, if $p>n$, has a continuous representative. Unfortunately I did not found any reference for this result. Moreover, looking on Wikipedia's page on Sobolev inequalities , I discovered that in my setup (which is part of the general case $k<\frac{p}{n}$) every element of $W^{1,p}(\Omega)$, if $p>n$, should be an Holder's continuous function. I'm puzzled by that, because I always thought to the element of a Soboloev Space as class of functions, and seems to me unrealistic that any element of any class of those Sobolev spaces is Holder continuous (which, if I'm not wrong, is stated as a property that holds everywhere). So my questions are: 1) Do you have a reference which explain why any class of functions in $W^{1,p}(\Omega)$ (for $p>n$) has a continuous representative? 2) Is it correct the result stated by Wikipedia on Holder continuity? If the answer is yes, where am I wrong in thinking Sobolev functions? Thank you very much for your time!","My question arise from the study of the possible extensions of Rademacher's Theorem to the Sobolev Space $W^{1,p}(\Omega)$, with $\Omega\subset \mathbb{R}^n$. In specific I'm studying the proof of the fact that any element of $W^{1,p}(\Omega)$ is differentiable almost everywhere if $p>n$. A key result in the proof is that any element in $W^{1,p}(\Omega)$, if $p>n$, has a continuous representative. Unfortunately I did not found any reference for this result. Moreover, looking on Wikipedia's page on Sobolev inequalities , I discovered that in my setup (which is part of the general case $k<\frac{p}{n}$) every element of $W^{1,p}(\Omega)$, if $p>n$, should be an Holder's continuous function. I'm puzzled by that, because I always thought to the element of a Soboloev Space as class of functions, and seems to me unrealistic that any element of any class of those Sobolev spaces is Holder continuous (which, if I'm not wrong, is stated as a property that holds everywhere). So my questions are: 1) Do you have a reference which explain why any class of functions in $W^{1,p}(\Omega)$ (for $p>n$) has a continuous representative? 2) Is it correct the result stated by Wikipedia on Holder continuity? If the answer is yes, where am I wrong in thinking Sobolev functions? Thank you very much for your time!",,"['real-analysis', 'analysis', 'sobolev-spaces']"
43,Prof gave us wrong definition of convexity?,Prof gave us wrong definition of convexity?,,"My professor gave us this exercise: Let $f : \mathbb{R} \rightarrow \mathbb{R}$ be a differentiable function. Given the following two definitions of convexity of $f$ , prove that (i) implies (ii): (i) $\forall x, y \in \mathbb{R} : f(x) \ge f(y) + f'(y)(x - y)$ (ii) $\forall x, y \in \mathbb{R}, \forall \lambda \in [0, 1] : f(\lambda x + (1 - \lambda)y) \le \lambda f(x) + (1 - \lambda)f(y)$ I think definition (i) is incorrect. I think it would be correct, if we would assume $x,y$ arbitrary but we must have $x \geq y$ . Since (i) is differently written: $$\frac{f(x)-f(y)}{x-y} \ge f'(y)$$ So the gradient/slope from $y$ to $x$ is greater than the slope of $y$ . That makes sense because in a convex function the slope increases. But if we choose $y$ to be greater than $x$ , then it's wrong. Let me give this example: We have $f(x)=x^2$ and $x,y=\pm 1$ . The slope from $x$ to $y$ (or $y$ to $x$ ) is $0$ . It's the black line in the plot. The slope at $-1$ is $-2$ , that's the green line. So we indeed have $-2 \le 0$ . But if we pick $x$ and $y$ to be $x=-1$ and $y=1$ , then the statement (i) is false, since it would state that $0 \ge 2$ . So to me it seems, the correct definition would be: (i) $\forall x, y \in \mathbb{R}$ with $x \ge y : f(x) \ge f(y) + f'(y)(x - y)$","My professor gave us this exercise: Let be a differentiable function. Given the following two definitions of convexity of , prove that (i) implies (ii): (i) (ii) I think definition (i) is incorrect. I think it would be correct, if we would assume arbitrary but we must have . Since (i) is differently written: So the gradient/slope from to is greater than the slope of . That makes sense because in a convex function the slope increases. But if we choose to be greater than , then it's wrong. Let me give this example: We have and . The slope from to (or to ) is . It's the black line in the plot. The slope at is , that's the green line. So we indeed have . But if we pick and to be and , then the statement (i) is false, since it would state that . So to me it seems, the correct definition would be: (i) with","f : \mathbb{R} \rightarrow \mathbb{R} f \forall x, y \in \mathbb{R} : f(x) \ge f(y) + f'(y)(x - y) \forall x, y \in \mathbb{R}, \forall \lambda \in [0, 1] : f(\lambda x + (1 - \lambda)y) \le \lambda f(x) + (1 - \lambda)f(y) x,y x \geq y \frac{f(x)-f(y)}{x-y} \ge f'(y) y x y y x f(x)=x^2 x,y=\pm 1 x y y x 0 -1 -2 -2 \le 0 x y x=-1 y=1 0 \ge 2 \forall x, y \in \mathbb{R} x \ge y : f(x) \ge f(y) + f'(y)(x - y)","['real-analysis', 'analysis', 'convex-analysis']"
44,Limit of lacunar power series in $1^-$.,Limit of lacunar power series in .,1^-,"Let $\sigma:\mathbb{N}\longrightarrow\mathbb{N}$ be strictly increasing, and consider the power series $$ S_{\sigma}(x)=\sum_{n=0}^{+\infty}(-1)^nx^{\sigma(n)}. $$ Can any real number in $[0,1]$ be obtained as the limit $\lim\limits_{x\rightarrow 1^-}S_{\sigma}(x)$ for some $\sigma$ ? According to this answer, the limit always is $\frac{1}{2}$ when $\sigma$ is a polynomial, WolframAlpha suggests that the limit is also $\frac{1}{2}$ with $\sigma(n)=n\log n$ (think of $\sigma(n)$ as the $n$ -th prime number). Therefore my question can also be : Is the limit $\lim\limits_{x\rightarrow 1^-}S_{\sigma}(x)$ always $\frac{1}{2}$ ? if not, can any rational number in $[0,1]$ be obtained this way for some $\sigma$ ?","Let be strictly increasing, and consider the power series Can any real number in be obtained as the limit for some ? According to this answer, the limit always is when is a polynomial, WolframAlpha suggests that the limit is also with (think of as the -th prime number). Therefore my question can also be : Is the limit always ? if not, can any rational number in be obtained this way for some ?","\sigma:\mathbb{N}\longrightarrow\mathbb{N}  S_{\sigma}(x)=\sum_{n=0}^{+\infty}(-1)^nx^{\sigma(n)}.  [0,1] \lim\limits_{x\rightarrow 1^-}S_{\sigma}(x) \sigma \frac{1}{2} \sigma \frac{1}{2} \sigma(n)=n\log n \sigma(n) n \lim\limits_{x\rightarrow 1^-}S_{\sigma}(x) \frac{1}{2} [0,1] \sigma","['real-analysis', 'limits', 'power-series', 'analytic-number-theory', 'lacunary-series']"
45,How did Feynman produce solutions in under a minute?,How did Feynman produce solutions in under a minute?,,"As detailed here and elsewhere, Feynman and others at Los Alamos could calculate many problems to 10% accuracy in minutes: When I was at Los Alamos I found out that Hans Bethe was absolutely topnotch at calculating... A few minutes later we need to take the cube root of $2 \frac 1 2$ ... and he says, ""It's about $1.35$ .""... I had a lot of fun trying to do arithmetic fast, by tricks, with Hans... I announced, ""I can work out in sixty seconds the answer to any problem that anybody can state in ten seconds, to 10 percent!"" ... People started giving me problems they thought were difficult, such as integrating a function like $\frac 1 {1 + x^4}$ , which hardly changed over the range they gave me. The hardest one somebody gave me was the binomial coefficient of $x^{10}$ in $(1 + x)^{20}$ ; I got that just in time.... [Paul Olum] says, $\tan 10^{100}$ . I was sunk: you have to divide by $\pi$ to $100$ decimal places! What methods do Feynman, Bethe, Olum, use to do these? Or, since we can't really know the answer to that: What methods can we use to easily approximate calculations within 10% error? Now, one might simply respond: Wolfram Alpha .  But we do this not for lack of a calculator!  For example, Sanjay Mahajan requires his students to show ""number sense"" by Without a calculator, estimate $\sqrt{1.3}, \sqrt[3]{1.6}, \sin 7,$ and $1.01^{100}$ . What methods can we use to do this?  I'll post my collection as an answer below.  Surprisingly, while I've found good methods for estimating logs, exponents, and trig, approximate long division (in less steps than the real thing) proves to be the hardest!","As detailed here and elsewhere, Feynman and others at Los Alamos could calculate many problems to 10% accuracy in minutes: When I was at Los Alamos I found out that Hans Bethe was absolutely topnotch at calculating... A few minutes later we need to take the cube root of ... and he says, ""It's about .""... I had a lot of fun trying to do arithmetic fast, by tricks, with Hans... I announced, ""I can work out in sixty seconds the answer to any problem that anybody can state in ten seconds, to 10 percent!"" ... People started giving me problems they thought were difficult, such as integrating a function like , which hardly changed over the range they gave me. The hardest one somebody gave me was the binomial coefficient of in ; I got that just in time.... [Paul Olum] says, . I was sunk: you have to divide by to decimal places! What methods do Feynman, Bethe, Olum, use to do these? Or, since we can't really know the answer to that: What methods can we use to easily approximate calculations within 10% error? Now, one might simply respond: Wolfram Alpha .  But we do this not for lack of a calculator!  For example, Sanjay Mahajan requires his students to show ""number sense"" by Without a calculator, estimate and . What methods can we use to do this?  I'll post my collection as an answer below.  Surprisingly, while I've found good methods for estimating logs, exponents, and trig, approximate long division (in less steps than the real thing) proves to be the hardest!","2 \frac 1 2 1.35 \frac 1 {1 + x^4} x^{10} (1 + x)^{20} \tan 10^{100} \pi 100 \sqrt{1.3}, \sqrt[3]{1.6}, \sin 7, 1.01^{100}","['real-analysis', 'calculus', 'numerical-methods', 'taylor-expansion', 'approximation']"
46,Show that $ I = \int_0^1 \frac{dx}{\sqrt{\sin(2x) + \sin(x) + 3}} < \frac{1}{2} $?,Show that ?, I = \int_0^1 \frac{dx}{\sqrt{\sin(2x) + \sin(x) + 3}} < \frac{1}{2} ,"How to efficiently show that $$ I = \int_0^1 \frac{dx}{\sqrt{\sin(2x) + \sin(x) + 3}} < \frac{1}{2} $$ ? What I tried : $$\sin(2x) + \sin(x) + 3 = 2 \cos(x)\sin(x) + \sin(x) + 3 =(2 \cos(x) + 1)\sin(x) + 3$$ and that is about $$(2(1 - x^2/2 + x^4/24 - O(x^6) ) + 1)(x - x^3/6 + x^5/120 - O(x^7)) + 3$$ and more importantly, it is above $$(2(1 - x^2/2 + x^4/24 -1/720 ) + 1)(x - x^3/6 + x^5/120 -1/5040) + 3$$ and therefore we compute (in closed form or numerical method) $$ \int_0^1 \frac{dx}{((2(1 - x^2/2 + x^4/24 -1/720 ) + 1)(x - x^3/6 + x^5/120 -1/5040) + 3} $$ what is about $0.24..$ Now including the $\sqrt *$ should be about twice that, what makes about $0.49..$ . (edit : to be more precise : Jensen's inequality gives here $\int \sqrt{f(x)} dx < \sqrt \int f(x) dx$ , so $\sqrt 0.24.. = 0.49... < \frac{1}{2}$ see : https://en.wikipedia.org/wiki/Jensen%27s_inequality ) But this gets complicated, I want an efficient and convincing method. But lets continue by truncating further : $$ \int_0^1 \frac{dx}{\sqrt{3x + 3}} = \frac{2(\sqrt 2 - 1)}{\sqrt 3}$$ this is about $0.478$ , however this requires $\sqrt 2,\sqrt 3$ and is in fact a lower bound, not an upper bound. I tried a truncated product as well, but also without any simple ways to success. Then I gave up. Edit : I explained how I used https://en.wikipedia.org/wiki/Jensen%27s_inequality and thus the answer of Zarrax is most appealing to me for now.","How to efficiently show that ? What I tried : and that is about and more importantly, it is above and therefore we compute (in closed form or numerical method) what is about Now including the should be about twice that, what makes about . (edit : to be more precise : Jensen's inequality gives here , so see : https://en.wikipedia.org/wiki/Jensen%27s_inequality ) But this gets complicated, I want an efficient and convincing method. But lets continue by truncating further : this is about , however this requires and is in fact a lower bound, not an upper bound. I tried a truncated product as well, but also without any simple ways to success. Then I gave up. Edit : I explained how I used https://en.wikipedia.org/wiki/Jensen%27s_inequality and thus the answer of Zarrax is most appealing to me for now."," I = \int_0^1 \frac{dx}{\sqrt{\sin(2x) + \sin(x) + 3}} < \frac{1}{2}  \sin(2x) + \sin(x) + 3 =
2 \cos(x)\sin(x) + \sin(x) + 3 =(2 \cos(x) + 1)\sin(x) + 3 (2(1 - x^2/2 + x^4/24 - O(x^6) ) + 1)(x - x^3/6 + x^5/120 - O(x^7)) + 3 (2(1 - x^2/2 + x^4/24 -1/720 ) + 1)(x - x^3/6 + x^5/120 -1/5040) + 3  \int_0^1 \frac{dx}{((2(1 - x^2/2 + x^4/24 -1/720 ) + 1)(x - x^3/6 + x^5/120 -1/5040) + 3}  0.24.. \sqrt * 0.49.. \int \sqrt{f(x)} dx < \sqrt \int f(x) dx \sqrt 0.24.. = 0.49... < \frac{1}{2}  \int_0^1 \frac{dx}{\sqrt{3x + 3}} = \frac{2(\sqrt 2 - 1)}{\sqrt 3} 0.478 \sqrt 2,\sqrt 3","['real-analysis', 'calculus', 'inequality', 'definite-integrals', 'numerical-methods']"
47,Riff on Sequential Criterion for Continuity,Riff on Sequential Criterion for Continuity,,"A function $f:X\rightarrow\mathbb{R}$ on a metric space $(X,d)$ is continuous at $p\in X$ if and only if whenever we have a sequence $\{x_n\}\subseteq X\setminus\{p\}$ such that $x_n\rightarrow p$ , $f(x_n)\rightarrow f(p)$ . For my own curiosity, I asked myself whether a function $f:[a,b]\rightarrow\mathbb{R}$ is continuous provided it satisfies the following weaker condition: whenever we have a sequence $\{x_n\}\subseteq ([a,b]\setminus\{p\})\cap\mathbb{Q}$ such that $x_n\rightarrow p$ , $f(x_n)\rightarrow f(p)$ . After playing with examples like Thomas’s function and it’s variants, I have been unable to find a counterexample as the point $p$ is not required to be a rational number. At the same time, when I try to prove $f$ is continuous, I am unable to do so. Does anyone have any hints?","A function on a metric space is continuous at if and only if whenever we have a sequence such that , . For my own curiosity, I asked myself whether a function is continuous provided it satisfies the following weaker condition: whenever we have a sequence such that , . After playing with examples like Thomas’s function and it’s variants, I have been unable to find a counterexample as the point is not required to be a rational number. At the same time, when I try to prove is continuous, I am unable to do so. Does anyone have any hints?","f:X\rightarrow\mathbb{R} (X,d) p\in X \{x_n\}\subseteq X\setminus\{p\} x_n\rightarrow p f(x_n)\rightarrow f(p) f:[a,b]\rightarrow\mathbb{R} \{x_n\}\subseteq ([a,b]\setminus\{p\})\cap\mathbb{Q} x_n\rightarrow p f(x_n)\rightarrow f(p) p f","['real-analysis', 'sequences-and-series', 'continuity', 'metric-spaces']"
48,Proving the Alternate Series Test,Proving the Alternate Series Test,,"I am self-learning Real Analysis from Understanding Analysis by Stephen Abbott. I am finding it difficult to come up with a proof for the Alternating Series Test for an infinite series. I only have some initial ideas, so I'll put those down. I'd like to ask if somebody could help, by providing any hints (please do not write the complete proof), on how to think about this. Exercise 2.7.1 Proving the Alternating Series Test amounts to showing that the sequence of partial sums \begin{align*} 	s_n = a_1 - a_2 + a_3 - \ldots \pm a_n \end{align*} converges. Different characterizations of completeness lead to different proofs. (a) Produce the Alternating Series Test by showing that $(s_n)$ is a Cauchy sequence. (b) Supply another proof for this result using the Nested Interval Property. (c) Consider the sequences $(s_{2n})$ and $(s_{2n+1})$ , and show how the Monotone Convergence Theorem leads to a third proof for the Alternating Series Test. Proof. (a)Let $(a_n)$ be a sequence satisfying (i) $a_1 \ge a_2 \ge a_3 \ge \ldots \ge a_n \ge a_{n+1} \ge$ (ii) $(a_n) \to 0$ Our claim is that, the alternating series $\sum_{n=1}^{\infty}(-1)^n a_n$ converges. Let $(s_n)$ be sequence of the partial sums, where \begin{align*} s_n = a_1 - a_2 + a_3 - a_4 + a_5 - a_6 + \ldots + (-1)^{n+1}a_n \end{align*} We are interested to show that $(s_n)$ is a Cauchy sequence. Let's see if can find a simple expression for $\lvert s_n - s_m \vert$ . If $n > m$ , \begin{align*} \lvert (s_n - s_m) \rvert &= \lvert (-1)^{m+2}a_{m+1} + (-1)^{m+3}a_{m+2} + (-1)^{n+1}a_{n} \rvert\\ &= \lvert {a_{m+1} - a_{m+2} + a_{m+3} - \ldots + (-1)^{n-m+1}a_n} \rvert \end{align*} A consequence of condition(i) is that $(a_n)$ is a decreasing sequence and has a lower bound zero, so $a_n \ge 0$ for all $n \in \mathbf{N}$ . Since $a_{m+2} \ge 0$ , $a_{m+4} \ge 0$ , $a_{m+6} \ge 0, \ldots$ we can write, \begin{align*} \lvert (s_n - s_m) \rvert &= \lvert {a_{m+1} - a_{m+2} + a_{m+3} - \ldots + (-1)^{n-m+1}a_n} \rvert\\ &\le \lvert {a_{m+1} + a_{m+3} + a_{m+5} + \ldots + a_{n}} \rvert \\ &\le \vert {a_{m+1} + a_{m+1} + a_{m+1} + \ldots + a_{m+1}} \rvert \end{align*} But this gives me a variable number of terms (whereas I need to prove that the distance $\lvert s_n - s_m \rvert$ becomes smaller than fixed $\epsilon$ or $k\epsilon$ . I am not quite sure, where to go from here. I know that the original sequence $(a_n)$ converges, so given any $\epsilon > 0$ , $\exists N \in \mathbf{N}$ , such that $\lvert a_n \rvert < \epsilon$ for all $n \ge N$ . (b) I am thinking of constructing a sequence of closed nested intervals $I_1 \supseteq I_2 \supseteq I_3 \supseteq \ldots \supseteq I_n \supseteq I_{n+1} \supseteq \ldots$ , in such a way that, $I_1=\{s_1,s_2,s_3,\ldots\}$ , $I_2 = I_1 - \{s_1\}$ and in general $I_{k+1} = I_k - \{s_k\}$ , as I am interested in the tail of the sequence of partial sums. Then, I can apply the nested interval property to show that $\bigcup_{k\ge 1}I_k$ is not empty. The one thing I don't know, is whether $(s_n)$ is bounded. If and only if this set is bounded, one may construct a closed interval. I think this is important for the construction. (c) Again, I need some bounds for $(s_n)$ , I guess.","I am self-learning Real Analysis from Understanding Analysis by Stephen Abbott. I am finding it difficult to come up with a proof for the Alternating Series Test for an infinite series. I only have some initial ideas, so I'll put those down. I'd like to ask if somebody could help, by providing any hints (please do not write the complete proof), on how to think about this. Exercise 2.7.1 Proving the Alternating Series Test amounts to showing that the sequence of partial sums converges. Different characterizations of completeness lead to different proofs. (a) Produce the Alternating Series Test by showing that is a Cauchy sequence. (b) Supply another proof for this result using the Nested Interval Property. (c) Consider the sequences and , and show how the Monotone Convergence Theorem leads to a third proof for the Alternating Series Test. Proof. (a)Let be a sequence satisfying (i) (ii) Our claim is that, the alternating series converges. Let be sequence of the partial sums, where We are interested to show that is a Cauchy sequence. Let's see if can find a simple expression for . If , A consequence of condition(i) is that is a decreasing sequence and has a lower bound zero, so for all . Since , , we can write, But this gives me a variable number of terms (whereas I need to prove that the distance becomes smaller than fixed or . I am not quite sure, where to go from here. I know that the original sequence converges, so given any , , such that for all . (b) I am thinking of constructing a sequence of closed nested intervals , in such a way that, , and in general , as I am interested in the tail of the sequence of partial sums. Then, I can apply the nested interval property to show that is not empty. The one thing I don't know, is whether is bounded. If and only if this set is bounded, one may construct a closed interval. I think this is important for the construction. (c) Again, I need some bounds for , I guess.","\begin{align*}
	s_n = a_1 - a_2 + a_3 - \ldots \pm a_n
\end{align*} (s_n) (s_{2n}) (s_{2n+1}) (a_n) a_1 \ge a_2 \ge a_3 \ge \ldots \ge a_n \ge a_{n+1} \ge (a_n) \to 0 \sum_{n=1}^{\infty}(-1)^n a_n (s_n) \begin{align*}
s_n = a_1 - a_2 + a_3 - a_4 + a_5 - a_6 + \ldots + (-1)^{n+1}a_n
\end{align*} (s_n) \lvert s_n - s_m \vert n > m \begin{align*}
\lvert (s_n - s_m) \rvert &= \lvert (-1)^{m+2}a_{m+1} + (-1)^{m+3}a_{m+2} + (-1)^{n+1}a_{n} \rvert\\
&= \lvert {a_{m+1} - a_{m+2} + a_{m+3} - \ldots + (-1)^{n-m+1}a_n} \rvert
\end{align*} (a_n) a_n \ge 0 n \in \mathbf{N} a_{m+2} \ge 0 a_{m+4} \ge 0 a_{m+6} \ge 0, \ldots \begin{align*}
\lvert (s_n - s_m) \rvert &= \lvert {a_{m+1} - a_{m+2} + a_{m+3} - \ldots + (-1)^{n-m+1}a_n} \rvert\\
&\le \lvert {a_{m+1} + a_{m+3} + a_{m+5} + \ldots + a_{n}} \rvert \\
&\le \vert {a_{m+1} + a_{m+1} + a_{m+1} + \ldots + a_{m+1}} \rvert
\end{align*} \lvert s_n - s_m \rvert \epsilon k\epsilon (a_n) \epsilon > 0 \exists N \in \mathbf{N} \lvert a_n \rvert < \epsilon n \ge N I_1 \supseteq I_2 \supseteq I_3 \supseteq \ldots \supseteq I_n \supseteq I_{n+1} \supseteq \ldots I_1=\{s_1,s_2,s_3,\ldots\} I_2 = I_1 - \{s_1\} I_{k+1} = I_k - \{s_k\} \bigcup_{k\ge 1}I_k (s_n) (s_n)","['real-analysis', 'sequences-and-series']"
49,"Gödel: If a statement of existence is neither provable nor refutable, doesn‘t that imply that the statement is false? [duplicate]","Gödel: If a statement of existence is neither provable nor refutable, doesn‘t that imply that the statement is false? [duplicate]",,"This question already has answers here : True vs. Provable (8 answers) Closed 3 years ago . I‘m not very versed in Gödel‘s incompleteness theorem but in a naive way: If a statement of existence is not provable, there you cannot find an example which fulfills the statement (otherwise the statement would be provable with this example). But when there is no element which fulfills the statement, doesn’t that imply that the statement ist false? I thought about that one in the context of the measure problem - because the statement $$\exists \text{ measure function } \mu: 2^{\mathbb R} \to [0,\infty] \, \forall I = [a,b] \subseteq \bar{\mathbb R}: \mu(I) = b - a$$ is neither provable nor refutable. But if I cannot prove there is a measure function, I cannot find a $\mu$ for which the statement is true. Because finding such a $\mu$ would prove the statement. But when there is no such $\mu$ , the statement of existence is false, isn‘t it? Where is my mistake in thinking?","This question already has answers here : True vs. Provable (8 answers) Closed 3 years ago . I‘m not very versed in Gödel‘s incompleteness theorem but in a naive way: If a statement of existence is not provable, there you cannot find an example which fulfills the statement (otherwise the statement would be provable with this example). But when there is no element which fulfills the statement, doesn’t that imply that the statement ist false? I thought about that one in the context of the measure problem - because the statement is neither provable nor refutable. But if I cannot prove there is a measure function, I cannot find a for which the statement is true. Because finding such a would prove the statement. But when there is no such , the statement of existence is false, isn‘t it? Where is my mistake in thinking?","\exists \text{ measure function } \mu: 2^{\mathbb R} \to [0,\infty] \, \forall I = [a,b] \subseteq \bar{\mathbb R}: \mu(I) = b - a \mu \mu \mu","['real-analysis', 'logic', 'incompleteness']"
50,L'Hopital's rule conditions,L'Hopital's rule conditions,,"I have seen easy geometrical argument why L'Hopital's rule ( $\lim_{x \to a} \frac{f(x)}{g(x)} = \lim_{x \to a} \frac{f'(x)}{g'(x)}$ ) works (local linearization). But, I still don't understand this: why is rule defined just when limit is in form $\frac{0}{0}$ or $\pm \frac{\infty}{\infty}$ ? Why must be $f(a) = g(a) = 0$ ? why must be $g'(a) \neq 0$ ? Counterexample for 1: $\lim_{x \to 0} \frac{e^x}{x^2 + x + 1} = \frac{1}{1}$ but is also $\lim_{x \to 0} \frac{(e^x)'}{(x^2 + x + 1)'} = \lim_{x \to 0} \frac{e^x}{2x + 1} = \frac{1}{1}.$ So, L'Hopital's rule works here but $\frac{1}{1} \neq \frac{0}{0}$ ! Also, I read that there is another condition: 4. $\frac{f'(x)}{g'(x)}$ must exist does condition 3 implies this? can you give example when original limit exist but $\frac{f'}{g'}$ does not and how is this possible if functions $f, g$ are differentiable?","I have seen easy geometrical argument why L'Hopital's rule ( ) works (local linearization). But, I still don't understand this: why is rule defined just when limit is in form or ? Why must be ? why must be ? Counterexample for 1: but is also So, L'Hopital's rule works here but ! Also, I read that there is another condition: 4. must exist does condition 3 implies this? can you give example when original limit exist but does not and how is this possible if functions are differentiable?","\lim_{x \to a} \frac{f(x)}{g(x)} = \lim_{x \to a} \frac{f'(x)}{g'(x)} \frac{0}{0} \pm \frac{\infty}{\infty} f(a) = g(a) = 0 g'(a) \neq 0 \lim_{x \to 0} \frac{e^x}{x^2 + x + 1} = \frac{1}{1} \lim_{x \to 0} \frac{(e^x)'}{(x^2 + x + 1)'} = \lim_{x \to 0} \frac{e^x}{2x + 1} = \frac{1}{1}. \frac{1}{1} \neq \frac{0}{0} \frac{f'(x)}{g'(x)} \frac{f'}{g'} f, g","['real-analysis', 'calculus', 'geometry', 'limits']"
51,"$\int_0^1f(x) dx =0$, $\int_0^1xf(x) dx =0$. How to show that f has at least two zeros?",", . How to show that f has at least two zeros?",\int_0^1f(x) dx =0 \int_0^1xf(x) dx =0,"$f:[0,1]\to \mathbb{R}$ is a countinous function. $$\int_0^1f(x) dx =0 \qquad \mbox{ and } \qquad \int_0^1xf(x) dx =0. $$ If $f \ge 0$ ( $f\le0$ ) were true then $\int_0^1f(x) dx \ge0$ ( $\int_0^1f(x) dx \le0$ ). This is a contradiction, we can conclude that $f$ changes sign. By intermediate value property there exist a point $c$ such that $f(c)=0$ . This is the first zero. By using mean value theorems for integrals I can also show that a zero does exist. I can't show that these zeroes are different from each other. How to show a second zero exists?","is a countinous function. If ( ) were true then ( ). This is a contradiction, we can conclude that changes sign. By intermediate value property there exist a point such that . This is the first zero. By using mean value theorems for integrals I can also show that a zero does exist. I can't show that these zeroes are different from each other. How to show a second zero exists?","f:[0,1]\to \mathbb{R} \int_0^1f(x) dx =0 \qquad \mbox{ and } \qquad \int_0^1xf(x) dx =0.  f \ge 0 f\le0 \int_0^1f(x) dx \ge0 \int_0^1f(x) dx \le0 f c f(c)=0","['real-analysis', 'calculus', 'definite-integrals']"
52,"For any sequence of real numbers, one can always find a subsequence that is monotone","For any sequence of real numbers, one can always find a subsequence that is monotone",,"Homework Exercise: Let $(x_n)$ be ${\bf any}$ sequence of real numbers. ${\bf carefully}$ , that is, from first principles, prove that there exists a subsequence that is monotone. My sol: Let $x \in \mathbb{R}$ . Then, $(x_n)$ either converges to $x$ or not. So, we can do cases. ${\bf Case 1.}$ If $x_n \to x$ , then for any $\epsilon > 0$ one can take $N$ so that for all $n > N$ (in particular, for $n=n_1$ ) we have $|x_{n_1} - x|  < \epsilon $ Applying the definition again with $\epsilon = |x_{n_1} - x| > 0$ and taking $n = n_2 > n_1 > N$ we observe that $|x_{n_2} - x| < |x_{n_1} - x| $ Now, choose $\epsilon = |x_{n_2} - x| > 0$ and take $N > 0$ so that for all $n_3 > n_2 > n_1 > N$ one has $|x_{n_3} - x | < |x_{n_2} - x | $ If we continue in this fashion, we observe that for $n_k > n_{k-1} > ... > n_1$ we have $x_{n_1} < x_{n_2} < .... < x_{n_k} $ . In particular $(x_{n_k})$ is a monotone subsequence of $(x_n)$ ${\bf Case2.}$ Suppose $x_n$ not converges to $x$ . We know $\exists $ some $\epsilon > 0$ and some subsequence $(x_{n_k})$ so that $|x_{n_k}-x| \geq \epsilon$ $\forall k \in \mathbb{N}$ So, notice that $x_{n_k} - x \geq \epsilon \implies x_{n_k} \geq x + \epsilon $ . Also, $x_{n_{k+1} } - x < - \epsilon \implies -x_{n_{k+1}} >-x+\epsilon $ So that $x_{n_k} - x_{n_{k-1}} \geq 2 \epsilon > 0 $ so that $x_{n_k} > x_{n_{k+1}} $ and thus the subsequence is monotone. QED Is this a correct and 'careful' proof?","Homework Exercise: Let be sequence of real numbers. , that is, from first principles, prove that there exists a subsequence that is monotone. My sol: Let . Then, either converges to or not. So, we can do cases. If , then for any one can take so that for all (in particular, for ) we have Applying the definition again with and taking we observe that Now, choose and take so that for all one has If we continue in this fashion, we observe that for we have . In particular is a monotone subsequence of Suppose not converges to . We know some and some subsequence so that So, notice that . Also, So that so that and thus the subsequence is monotone. QED Is this a correct and 'careful' proof?",(x_n) {\bf any} {\bf carefully} x \in \mathbb{R} (x_n) x {\bf Case 1.} x_n \to x \epsilon > 0 N n > N n=n_1 |x_{n_1} - x|  < \epsilon  \epsilon = |x_{n_1} - x| > 0 n = n_2 > n_1 > N |x_{n_2} - x| < |x_{n_1} - x|  \epsilon = |x_{n_2} - x| > 0 N > 0 n_3 > n_2 > n_1 > N |x_{n_3} - x | < |x_{n_2} - x |  n_k > n_{k-1} > ... > n_1 x_{n_1} < x_{n_2} < .... < x_{n_k}  (x_{n_k}) (x_n) {\bf Case2.} x_n x \exists  \epsilon > 0 (x_{n_k}) |x_{n_k}-x| \geq \epsilon \forall k \in \mathbb{N} x_{n_k} - x \geq \epsilon \implies x_{n_k} \geq x + \epsilon  x_{n_{k+1} } - x < - \epsilon \implies -x_{n_{k+1}} >-x+\epsilon  x_{n_k} - x_{n_{k-1}} \geq 2 \epsilon > 0  x_{n_k} > x_{n_{k+1}} ,"['real-analysis', 'calculus', 'sequences-and-series', 'solution-verification']"
53,A particularly tricky integral: $ \lim_{n \to \infty} n^{\frac{3}{2}} \int_0^1 \frac{x^2}{(1+x^2)^n} dx. $,A particularly tricky integral:, \lim_{n \to \infty} n^{\frac{3}{2}} \int_0^1 \frac{x^2}{(1+x^2)^n} dx. ,"I encountered the following, deceptively simple looking integral - $$ \lim_{n \to \infty} n^{\frac{3}{2}} \int_0^1 \frac{x^2}{(1+x^2)^n} dx. $$ My goal was to hopefully somehow use the Lebesgue dominated convergence theorem; I've tried every elementary bound that I can think of to no avail. One approach suggested to me is to write $$ \int_0^1 \frac{x^2}{(1+x^2)^{n+1}}dx = \int_0^1 \frac{1}{(1+x^2)^n} - \int_0^1 \frac{1}{(1+x^2)^{n+1}}dx, $$ then try and use an induction and partial fraction decomposition argument. Doing this, I didn't find anything useful. When I tried plugging into wolfram/mathematica I got roughly the behavior (this could be wrong, so take it with a grain of salt) $$\int_0^1 \frac{x^2}{(1+x^2)^n} dx =  {}_2 F_1\left(\frac{1}{2}, n, \frac{3}{2}, -1\right). $$ Other calculations by trying to play with Gaussian bounds led to the result $ \frac{\sqrt{\pi}}{4} $ but this was done on a computer, not with an explicit method. There's also been some hints from mathematica that $$ \lim_{n \to \infty} n^{\frac{3}{2}} \int_0^1 \frac{x^2}{(1+x^2)^n} dx $$ is somehow related to $$ \sqrt{\pi}\frac{\Gamma\left(n-\frac{1}{2}\right)}{2\Gamma(n)}. $$ I believe that there is an elementary argument and that I just haven't found it. Any tips would be appreciated.","I encountered the following, deceptively simple looking integral - My goal was to hopefully somehow use the Lebesgue dominated convergence theorem; I've tried every elementary bound that I can think of to no avail. One approach suggested to me is to write then try and use an induction and partial fraction decomposition argument. Doing this, I didn't find anything useful. When I tried plugging into wolfram/mathematica I got roughly the behavior (this could be wrong, so take it with a grain of salt) Other calculations by trying to play with Gaussian bounds led to the result but this was done on a computer, not with an explicit method. There's also been some hints from mathematica that is somehow related to I believe that there is an elementary argument and that I just haven't found it. Any tips would be appreciated."," \lim_{n \to \infty} n^{\frac{3}{2}} \int_0^1 \frac{x^2}{(1+x^2)^n} dx.   \int_0^1 \frac{x^2}{(1+x^2)^{n+1}}dx = \int_0^1 \frac{1}{(1+x^2)^n} - \int_0^1 \frac{1}{(1+x^2)^{n+1}}dx,  \int_0^1 \frac{x^2}{(1+x^2)^n} dx =  {}_2 F_1\left(\frac{1}{2}, n, \frac{3}{2}, -1\right).   \frac{\sqrt{\pi}}{4}   \lim_{n \to \infty} n^{\frac{3}{2}} \int_0^1 \frac{x^2}{(1+x^2)^n} dx   \sqrt{\pi}\frac{\Gamma\left(n-\frac{1}{2}\right)}{2\Gamma(n)}. ","['real-analysis', 'calculus', 'analysis', 'definite-integrals']"
54,Intuition for non-convergence of Cauchy sequence in $\mathbb{Q}$,Intuition for non-convergence of Cauchy sequence in,\mathbb{Q},"Suppose we were standing on the rational line at the point 3. Then we took a step to the point 3.1, then to 3.14, etc. (Cauchy sequence of decimal approximations of $\pi$ ). Suppose, also, that it takes us $\frac{1}{2^n}$ seconds to take the n'th step (so that we'll have ""completed"" the process after 1 second). Given that this sequence obviously doesn't converge in $\mathbb{Q}$ , where would we be after continuing this process for 1 second? I get this is kind of a weird question to ask, but I feel like it's hard for me to intuitively grasp what it means for a Cauchy sequence to not converge (since it feels like it is going somewhere).","Suppose we were standing on the rational line at the point 3. Then we took a step to the point 3.1, then to 3.14, etc. (Cauchy sequence of decimal approximations of ). Suppose, also, that it takes us seconds to take the n'th step (so that we'll have ""completed"" the process after 1 second). Given that this sequence obviously doesn't converge in , where would we be after continuing this process for 1 second? I get this is kind of a weird question to ask, but I feel like it's hard for me to intuitively grasp what it means for a Cauchy sequence to not converge (since it feels like it is going somewhere).",\pi \frac{1}{2^n} \mathbb{Q},"['real-analysis', 'general-topology', 'cauchy-sequences', 'rational-numbers']"
55,Evaluate $\sum_{n=1}^\infty\frac{(-1)^{n-1}H_n}{(2n+1)^3}$,Evaluate,\sum_{n=1}^\infty\frac{(-1)^{n-1}H_n}{(2n+1)^3},How to prove that $$\sum_{n=1}^\infty\frac{(-1)^{n-1}H_n}{(2n+1)^3}=\frac{7\pi}{16}\zeta(3)+\frac{\pi^3}{16}\ln2+\frac{\pi^4}{32}-\frac1{256}\psi^{(3)}\left(\frac14\right)$$ where $H_n=1+\frac1{2}+\frac1{3}+...+\frac1{n}$ is the $n$ th harmonic number. This sum was proposed by Cornel and I solved it using integration but can we solve it using series manipulation? The integral representation of the sum is $\ \displaystyle\frac12\int_0^1\frac{\ln^2x\ln(1+x^2)}{1+x^2}\ dx$ in case it is needed.,How to prove that where is the th harmonic number. This sum was proposed by Cornel and I solved it using integration but can we solve it using series manipulation? The integral representation of the sum is in case it is needed.,\sum_{n=1}^\infty\frac{(-1)^{n-1}H_n}{(2n+1)^3}=\frac{7\pi}{16}\zeta(3)+\frac{\pi^3}{16}\ln2+\frac{\pi^4}{32}-\frac1{256}\psi^{(3)}\left(\frac14\right) H_n=1+\frac1{2}+\frac1{3}+...+\frac1{n} n \ \displaystyle\frac12\int_0^1\frac{\ln^2x\ln(1+x^2)}{1+x^2}\ dx,"['real-analysis', 'calculus', 'integration', 'harmonic-numbers', 'polygamma']"
56,Distance of two points in $\mathbb{R}^N$ $\leq$ length of curve,Distance of two points in   length of curve,\mathbb{R}^N \leq,"Let $\gamma$ be a $C^1$ -curve and $x,y \in \mathbb{R}^N$ with $\gamma: [0,1] \longrightarrow \mathbb{R}^N, \gamma(0)=x, \gamma(1)=y$ . My intuition tells me that $||x-y|| \leq \ell(\gamma)$ , where $\ell$ is the length of a curve. Is this true and how to show it?","Let be a -curve and with . My intuition tells me that , where is the length of a curve. Is this true and how to show it?","\gamma C^1 x,y \in \mathbb{R}^N \gamma: [0,1] \longrightarrow \mathbb{R}^N, \gamma(0)=x, \gamma(1)=y ||x-y|| \leq \ell(\gamma) \ell",['real-analysis']
57,The Hausdorff dimension of the zero set of a real analytic function,The Hausdorff dimension of the zero set of a real analytic function,,"Let $n>1$ , and let $f:\mathbb{R}^n \to \mathbb{R}$ be a real-analytic function which is not identically zero. Does $\dim_{\mathcal H}(f^{-1}(0)) \le n-1$ ? here $\dim_{\mathcal H}$ refers to the Hausdorff dimension. (I have read this claim in a paper, but there was no reference). I know that $f^{-1}(0)$ has Lebesgue measure zero. If this is false, is it true then that $\dim_{\mathcal H}(f^{-1}(0)) < n$ ? Any reference would be appreciated.","Let , and let be a real-analytic function which is not identically zero. Does ? here refers to the Hausdorff dimension. (I have read this claim in a paper, but there was no reference). I know that has Lebesgue measure zero. If this is false, is it true then that ? Any reference would be appreciated.",n>1 f:\mathbb{R}^n \to \mathbb{R} \dim_{\mathcal H}(f^{-1}(0)) \le n-1 \dim_{\mathcal H} f^{-1}(0) \dim_{\mathcal H}(f^{-1}(0)) < n,"['real-analysis', 'reference-request', 'geometric-measure-theory', 'analytic-functions', 'dimension-theory-analysis']"
58,$\int_{0}^{\infty} \frac{1}{1 + x^r}\:dx = \frac{1}{r}\Gamma\left( \frac{r - 1}{r}\right)\Gamma\left( \frac{1}{r}\right)$ [duplicate],[duplicate],\int_{0}^{\infty} \frac{1}{1 + x^r}\:dx = \frac{1}{r}\Gamma\left( \frac{r - 1}{r}\right)\Gamma\left( \frac{1}{r}\right),"This question already has answers here : Prove $\int_0^{\infty}\! \frac{\mathbb{d}x}{1+x^n}=\frac{\pi}{n \sin\frac{\pi}{n}}$ using real analysis techniques only (5 answers) Closed 5 years ago . As part of a recent question I posted, I decided to try and generalise for a power of $2$ to any $r \in \mathbb{R}$ . As part of the method I took, I had to solve the following integral: \begin{equation}  I = \int_{0}^{\infty} \frac{1}{1 + x^r}\:dx \end{equation} I believe what I've done is correct, but I'm concerned that I may missed something (in particular whether it holds for all $r \neq 0$ ). So, here I have two questions (1) Is what I've done correct? and (2) What other methods can be employed that doesn't rely on complex analysis? Here is the method I took: First make the substitution $u = x^{\frac{1}{r}}$ to arrive at \begin{equation}  I = \frac{1}{n} \int_{0}^{\infty} \frac{1}{1 + u} \cdot u^{1 -\frac{1}{r}}\:du \end{equation} We now substitute $t = \frac{1}{1 + u}$ to arrive at: \begin{align}  I &= \frac{1}{r} \int_{1}^{0} t \cdot \left(\frac{1 - t}{t}\right)^{\frac{1}{r} -1}\frac{1}{t^2}\:dt = \frac{1}{r}\int_{0}^{1}t^{-\frac{1}{r}}\left(1 - t\right)^{ \frac{1}{r} - 1}\:dt \\ &= \frac{1}{r}B\left(1 - \frac{1}{n}, 1 + \frac{1}{r} - 1\right) = \frac{1}{r} B\left(\frac{r - 1}{r},\frac{1}{r}\right) \\ &= \frac{1}{r} B\left(\frac{r - 1}{r},\frac{1}{r}\right) \end{align} Wheer $B(a,b)$ is the Beta function. Using the relationship between the Beta and Gamma function we arrive at: \begin{equation} I = \frac{1}{r} \frac{\Gamma\left( \frac{r - 1}{r}\right)\Gamma\left( \frac{1}{r}\right)}{\Gamma\left(\frac{r - 1}{r} + \frac{1}{r}\right)} = \frac{1}{r}\Gamma\left( \frac{r - 1}{r}\right)\Gamma\left( \frac{1}{r}\right) \end{equation} And so, we arrive at: \begin{equation}  I = \int_{0}^{\infty} \frac{1}{1 + x^r}\:dx = \frac{1}{r}\Gamma\left( \frac{r - 1}{r}\right)\Gamma\left( \frac{1}{r}\right) \end{equation} for $r > 1$ As per KemonoChen 's comment and others, we can employ Euler's Reflection Formula to position this result for $\frac{1}{r} \not \in \mathbb{Z}$ Here, as $r \in \mathbb{R}, r > 1 \rightarrow \frac{1}{r} \not \in \mathbb{Z}$ and so our formula holds. \begin{equation}  I = \int_{0}^{\infty} \frac{1}{1 + x^r}\:dx = \frac{1}{r}\Gamma\left( \frac{r - 1}{r}\right)\Gamma\left( \frac{1}{r}\right) = \frac{\pi}{r\sin\left(\frac{\pi}{r} \right)} \end{equation} Thank you also to Winther , Jjagmath , and MrTaurho 's for their comments and corrections/clarifications.","This question already has answers here : Prove $\int_0^{\infty}\! \frac{\mathbb{d}x}{1+x^n}=\frac{\pi}{n \sin\frac{\pi}{n}}$ using real analysis techniques only (5 answers) Closed 5 years ago . As part of a recent question I posted, I decided to try and generalise for a power of to any . As part of the method I took, I had to solve the following integral: I believe what I've done is correct, but I'm concerned that I may missed something (in particular whether it holds for all ). So, here I have two questions (1) Is what I've done correct? and (2) What other methods can be employed that doesn't rely on complex analysis? Here is the method I took: First make the substitution to arrive at We now substitute to arrive at: Wheer is the Beta function. Using the relationship between the Beta and Gamma function we arrive at: And so, we arrive at: for As per KemonoChen 's comment and others, we can employ Euler's Reflection Formula to position this result for Here, as and so our formula holds. Thank you also to Winther , Jjagmath , and MrTaurho 's for their comments and corrections/clarifications.","2 r \in \mathbb{R} \begin{equation}
 I = \int_{0}^{\infty} \frac{1}{1 + x^r}\:dx
\end{equation} r \neq 0 u = x^{\frac{1}{r}} \begin{equation}
 I = \frac{1}{n} \int_{0}^{\infty} \frac{1}{1 + u} \cdot u^{1 -\frac{1}{r}}\:du
\end{equation} t = \frac{1}{1 + u} \begin{align}
 I &= \frac{1}{r} \int_{1}^{0} t \cdot \left(\frac{1 - t}{t}\right)^{\frac{1}{r} -1}\frac{1}{t^2}\:dt = \frac{1}{r}\int_{0}^{1}t^{-\frac{1}{r}}\left(1 - t\right)^{ \frac{1}{r} - 1}\:dt \\
&= \frac{1}{r}B\left(1 - \frac{1}{n}, 1 + \frac{1}{r} - 1\right) = \frac{1}{r} B\left(\frac{r - 1}{r},\frac{1}{r}\right) \\
&= \frac{1}{r} B\left(\frac{r - 1}{r},\frac{1}{r}\right)
\end{align} B(a,b) \begin{equation}
I = \frac{1}{r} \frac{\Gamma\left( \frac{r - 1}{r}\right)\Gamma\left( \frac{1}{r}\right)}{\Gamma\left(\frac{r - 1}{r} + \frac{1}{r}\right)} = \frac{1}{r}\Gamma\left( \frac{r - 1}{r}\right)\Gamma\left( \frac{1}{r}\right)
\end{equation} \begin{equation}
 I = \int_{0}^{\infty} \frac{1}{1 + x^r}\:dx = \frac{1}{r}\Gamma\left( \frac{r - 1}{r}\right)\Gamma\left( \frac{1}{r}\right)
\end{equation} r > 1 \frac{1}{r} \not \in \mathbb{Z} r \in \mathbb{R}, r > 1 \rightarrow \frac{1}{r} \not \in \mathbb{Z} \begin{equation}
 I = \int_{0}^{\infty} \frac{1}{1 + x^r}\:dx = \frac{1}{r}\Gamma\left( \frac{r - 1}{r}\right)\Gamma\left( \frac{1}{r}\right) = \frac{\pi}{r\sin\left(\frac{\pi}{r} \right)}
\end{equation}","['real-analysis', 'integration']"
59,"Convergence of series $\sum\limits_{n=1}^\infty\int\limits_{1}^{+\infty}e^{-x^n}\,dx$",Convergence of series,"\sum\limits_{n=1}^\infty\int\limits_{1}^{+\infty}e^{-x^n}\,dx","Determine if the series $\sum\limits_{n=1}^\infty \alpha_n$ converge, where $$\alpha_n=\int\limits_{1}^{+\infty}e^{-x^n}\,dx.$$ Attempt. Ι am pretty sure the inequalities $$e^{-x^n}\leq \frac{1}{1+x^n}$$ and $e^{-x^n}\geq 1-x^n$ will be useful (the first one I believe more, which gives convergence for the series though, which I am not sure if it is correct). Thanks for the help.","Determine if the series converge, where Attempt. Ι am pretty sure the inequalities and will be useful (the first one I believe more, which gives convergence for the series though, which I am not sure if it is correct). Thanks for the help.","\sum\limits_{n=1}^\infty \alpha_n \alpha_n=\int\limits_{1}^{+\infty}e^{-x^n}\,dx. e^{-x^n}\leq \frac{1}{1+x^n} e^{-x^n}\geq 1-x^n","['calculus', 'real-analysis', 'integration', 'sequences-and-series', 'convergence-divergence']"
60,The asymptotic expansion of an integral of an exponential function,The asymptotic expansion of an integral of an exponential function,,"What is the asymptotic expansion of $f(x) := \int_0^1 e^{-x(1-u^2)}du$? The integrand steeply declines near $u=1$. I tried to transform $u$ into something that is suitable for the method of steepest descent, but have not found an appropriate transformation. Integration by parts has not yield a satisfactory result, perhaps due to I having not found the right components to integrate.","What is the asymptotic expansion of $f(x) := \int_0^1 e^{-x(1-u^2)}du$? The integrand steeply declines near $u=1$. I tried to transform $u$ into something that is suitable for the method of steepest descent, but have not found an appropriate transformation. Integration by parts has not yield a satisfactory result, perhaps due to I having not found the right components to integrate.",,"['real-analysis', 'asymptotics']"
61,Continuous $f$ such that $f(x)=f(x^2)$ is constant? [duplicate],Continuous  such that  is constant? [duplicate],f f(x)=f(x^2),"This question already has an answer here : Proof that a continuous function with $f(x) = f(x^2)$ is constant. [closed] (1 answer) Closed 6 years ago . Let $f:\mathbb{R}\to\mathbb{R}$ be a continuous function such that $f(x)=f(x^2)$ for all $x\in\mathbb{R}$. I've proven that also $f\left(y^{(2^{-n})}\right)=f(y)$ for all $n\in\mathbb{N}, y\geq0$. How can I deduce that $f$ is constant?","This question already has an answer here : Proof that a continuous function with $f(x) = f(x^2)$ is constant. [closed] (1 answer) Closed 6 years ago . Let $f:\mathbb{R}\to\mathbb{R}$ be a continuous function such that $f(x)=f(x^2)$ for all $x\in\mathbb{R}$. I've proven that also $f\left(y^{(2^{-n})}\right)=f(y)$ for all $n\in\mathbb{N}, y\geq0$. How can I deduce that $f$ is constant?",,"['real-analysis', 'analysis', 'functions', 'continuity', 'functional-equations']"
62,How to show the Jungle River Metric is Complete,How to show the Jungle River Metric is Complete,,"Currently, I'm struggling to prove that the Jungle-River Metric is complete over $R^2$. More generally, however, I'm having trouble grasping the approach that one should take when prooving completeness. Specific Question: Prove the Jungle River Metric, defined as $$d((x,y),(x',y')) = |y|+|y'|+|x-x'|$$ if $x \neq x'$. and $$d((x,y),(x',y')) =|y-y'|$$ if  $x = x'$. My approach so far: So, I know that a metric space is complete if every Cauchy space in the sequence converges. From what I understand, I want to define a general Cauchy Sequence $(x_n,y_n)_{n \in N}$ and then prove that the limit of this Cauchy sequence is inside of my metric space. However, I'm not too sure how to proceed from there. I've tried drawing open balls in my metric space in hopes of some inspiration. I found that the open balls in this metric space are diamonds centered around the x-axis, with possible vertical lines extending upwards and downwards depending on the radius of the ball. However, I don't see how this can help me. I feel like, please let me know if I'm wrong, but that I understand what it means to be complete, but I don't understand how specifically to begin proving that what I know needs to be true, is true. Any tips or advice would be greatly appreciated. Thank you!","Currently, I'm struggling to prove that the Jungle-River Metric is complete over $R^2$. More generally, however, I'm having trouble grasping the approach that one should take when prooving completeness. Specific Question: Prove the Jungle River Metric, defined as $$d((x,y),(x',y')) = |y|+|y'|+|x-x'|$$ if $x \neq x'$. and $$d((x,y),(x',y')) =|y-y'|$$ if  $x = x'$. My approach so far: So, I know that a metric space is complete if every Cauchy space in the sequence converges. From what I understand, I want to define a general Cauchy Sequence $(x_n,y_n)_{n \in N}$ and then prove that the limit of this Cauchy sequence is inside of my metric space. However, I'm not too sure how to proceed from there. I've tried drawing open balls in my metric space in hopes of some inspiration. I found that the open balls in this metric space are diamonds centered around the x-axis, with possible vertical lines extending upwards and downwards depending on the radius of the ball. However, I don't see how this can help me. I feel like, please let me know if I'm wrong, but that I understand what it means to be complete, but I don't understand how specifically to begin proving that what I know needs to be true, is true. Any tips or advice would be greatly appreciated. Thank you!",,"['real-analysis', 'metric-spaces', 'complete-spaces']"
63,Closed form for a series of functions,Closed form for a series of functions,,"Let $f_1:\mathbb{R}\to \mathbb{R}$ be a locally integrable function (that is $f_1\in L^1_{loc}(\mathbb{R})$). Let us define $f_{n+1}:=\int_0^x f_n(t)\,dt$ for all $n\ge1$. We consider the series $S(x)=\sum_{n=1}^{+\infty} f_n(x)$. The problem asks: prove that $S$ pointwise converges for all $x\in\mathbb{R}$ and find a closed form for the sum. For the part about the pointwise convegence I think one can consider: $|f_n(x)|=|\int_0^x\int_0^{t_1}...\int_0^{t_{n-2}} f_1(t_{n-1})\,dt_{n-1}\,dt_{n-2}\,...\,dt_1   |\le||f_1||_{L^1(0,x)}\frac{x^{n-2}}{(n-2)!}=C\frac{x^{n-2}}{(n-2)!}$ with $C\ge 0$ depending on $x$. By Stirling approximation $\frac{x^{n}}{(n)!}$ is asymptotic to $\frac{1}{\sqrt{2\pi n}}\big(\frac{xe}{n}\big)^n$ which gives a converging series. But now what about the closed form for the series? How can we proceed? I am not even sure about what one means by ""closed form"" in this case. Thank you all!","Let $f_1:\mathbb{R}\to \mathbb{R}$ be a locally integrable function (that is $f_1\in L^1_{loc}(\mathbb{R})$). Let us define $f_{n+1}:=\int_0^x f_n(t)\,dt$ for all $n\ge1$. We consider the series $S(x)=\sum_{n=1}^{+\infty} f_n(x)$. The problem asks: prove that $S$ pointwise converges for all $x\in\mathbb{R}$ and find a closed form for the sum. For the part about the pointwise convegence I think one can consider: $|f_n(x)|=|\int_0^x\int_0^{t_1}...\int_0^{t_{n-2}} f_1(t_{n-1})\,dt_{n-1}\,dt_{n-2}\,...\,dt_1   |\le||f_1||_{L^1(0,x)}\frac{x^{n-2}}{(n-2)!}=C\frac{x^{n-2}}{(n-2)!}$ with $C\ge 0$ depending on $x$. By Stirling approximation $\frac{x^{n}}{(n)!}$ is asymptotic to $\frac{1}{\sqrt{2\pi n}}\big(\frac{xe}{n}\big)^n$ which gives a converging series. But now what about the closed form for the series? How can we proceed? I am not even sure about what one means by ""closed form"" in this case. Thank you all!",,"['calculus', 'real-analysis', 'sequences-and-series', 'analysis', 'closed-form']"
64,"If twice-differiantiable $f$ satisfies $f(x)f''(x)=0$ for all $x\in\mathbb R$, then $f$ is a polynomial of degree at most $1$","If twice-differiantiable  satisfies  for all , then  is a polynomial of degree at most",f f(x)f''(x)=0 x\in\mathbb R f 1,"Let $f: \mathbb R \to \mathbb R$ be a twice differentiable function such that$$f(x)f''(x)=0.\quad \forall x \in \mathbb R$$Then is it true that $f$ is a polynomial of degree at most $1$? I could not find any other function satisfying the condition, but could not prove that there are no other functions. Please help.","Let $f: \mathbb R \to \mathbb R$ be a twice differentiable function such that$$f(x)f''(x)=0.\quad \forall x \in \mathbb R$$Then is it true that $f$ is a polynomial of degree at most $1$? I could not find any other function satisfying the condition, but could not prove that there are no other functions. Please help.",,"['real-analysis', 'derivatives', 'continuity']"
65,"If $(f_n)\to0$ in $L^2$ and the derivatives converge in $L^2$, must the limit be 0?","If  in  and the derivatives converge in , must the limit be 0?",(f_n)\to0 L^2 L^2,"Question Let $f_n:[0,1]\to\mathbb R$ be infinitely differentiable functions such that $$\lim_{n\to\infty}\int_0^1\!f_n^2\,dx=0,$$ and for all $\varepsilon>0$ there exists $N\in\mathbb N$ such that for all $n,m\geq N$ , $$\int_0^1\!(f'_n-f'_m)^2\,dx<\varepsilon.$$ Must it be true that $$\lim_{n\to\infty}\int_0^1\!{f'_n}^2\,dx=0?$$ Motivation The following is an exercise for a first course in functional analysis: Let $\Omega\in\mathbb R^n$ be compact. Prove that for all $f\in C(\Omega)$ there exists $u\in H^1(\Omega)$ such that $$\int_\Omega\!\nabla u\cdot\nabla\phi\,dx=\int_\Omega\!f\phi\,dx\quad\text{for all }\phi\in C^\infty(\Omega),$$ where $H^1$ is the completion of $C^\infty$ under the inner product $\langle f,g\rangle=\int_\Omega(fg+\nabla f\cdot\nabla g)\,dx$ . I can solve the question provided that $[f,g]=\int_\Omega fg\,dx$ in $C^\infty$ extends to an inner product on $H^1$ . If $(f_n),(g_n)$ are sequences in $C^\infty$ which are Cauchy (with respect to $\langle\cdot,\cdot\rangle$ ), it is not hard to show that $\lim_{n\to\infty}[f_n,g_n]$ exists and is independent of choice of representative Cauchy sequences, so $[\cdot,\cdot]$ is well-defined in $H^1$ . Most of the axioms of an inner product space for $(H^1,[\cdot,\cdot])$ can be recovered from those in $(C^\infty,[\cdot,\cdot])$ , except $[f,f]=0\implies f=0$ . This is equivalent to the following statement about Cauchy sequences: Let $f_n\in C^\infty(\Omega)$ such that $[f_n,f_n]=\int_\Omega\!f_n^2\to0$ , and $\forall\varepsilon>0\,\exists N\in\mathbb N\,\forall n,m\geq N$ $$\langle f_n-f_m,f_n-f_m\rangle\to0,\quad\text{ie. }\int_\Omega\!\nabla(f_n-f_m)\cdot\nabla(f_n-f_m)\,dx\to0.$$ Show that $\langle f_n,f_n\rangle\to0$ , ie. $\int_\Omega\nabla f_n\cdot\nabla f_n\,dx\to0$ . Taking the simplest case, $\Omega=[0,1]$ , gives the question stated above. Edit I am looking for solutions with preferably minimal use of tools from functional analysis. In particular, the concept of weak convergence is only covered later in the course, and weak derivatives are not covered at all, so I suppose we are not expected to be familiar with them for this question. Right now it appears to me that the question is about a sequence of functions in $C^\infty$ , or even $L^2$ , so there should be a direct solution using tools from real analysis or measure theory. If this is incorrect, it would be very helpful if someone can answer the following (admittedly much more vague) question instead: Question' : why does the above question ""live"" in $H^1$ , instead of $C^\infty$ or $L^2$ ?","Question Let be infinitely differentiable functions such that and for all there exists such that for all , Must it be true that Motivation The following is an exercise for a first course in functional analysis: Let be compact. Prove that for all there exists such that where is the completion of under the inner product . I can solve the question provided that in extends to an inner product on . If are sequences in which are Cauchy (with respect to ), it is not hard to show that exists and is independent of choice of representative Cauchy sequences, so is well-defined in . Most of the axioms of an inner product space for can be recovered from those in , except . This is equivalent to the following statement about Cauchy sequences: Let such that , and Show that , ie. . Taking the simplest case, , gives the question stated above. Edit I am looking for solutions with preferably minimal use of tools from functional analysis. In particular, the concept of weak convergence is only covered later in the course, and weak derivatives are not covered at all, so I suppose we are not expected to be familiar with them for this question. Right now it appears to me that the question is about a sequence of functions in , or even , so there should be a direct solution using tools from real analysis or measure theory. If this is incorrect, it would be very helpful if someone can answer the following (admittedly much more vague) question instead: Question' : why does the above question ""live"" in , instead of or ?","f_n:[0,1]\to\mathbb R \lim_{n\to\infty}\int_0^1\!f_n^2\,dx=0, \varepsilon>0 N\in\mathbb N n,m\geq N \int_0^1\!(f'_n-f'_m)^2\,dx<\varepsilon. \lim_{n\to\infty}\int_0^1\!{f'_n}^2\,dx=0? \Omega\in\mathbb R^n f\in C(\Omega) u\in H^1(\Omega) \int_\Omega\!\nabla u\cdot\nabla\phi\,dx=\int_\Omega\!f\phi\,dx\quad\text{for all }\phi\in C^\infty(\Omega), H^1 C^\infty \langle f,g\rangle=\int_\Omega(fg+\nabla f\cdot\nabla g)\,dx [f,g]=\int_\Omega fg\,dx C^\infty H^1 (f_n),(g_n) C^\infty \langle\cdot,\cdot\rangle \lim_{n\to\infty}[f_n,g_n] [\cdot,\cdot] H^1 (H^1,[\cdot,\cdot]) (C^\infty,[\cdot,\cdot]) [f,f]=0\implies f=0 f_n\in C^\infty(\Omega) [f_n,f_n]=\int_\Omega\!f_n^2\to0 \forall\varepsilon>0\,\exists N\in\mathbb N\,\forall n,m\geq N \langle f_n-f_m,f_n-f_m\rangle\to0,\quad\text{ie. }\int_\Omega\!\nabla(f_n-f_m)\cdot\nabla(f_n-f_m)\,dx\to0. \langle f_n,f_n\rangle\to0 \int_\Omega\nabla f_n\cdot\nabla f_n\,dx\to0 \Omega=[0,1] C^\infty L^2 H^1 C^\infty L^2","['real-analysis', 'functional-analysis', 'measure-theory', 'lp-spaces']"
66,Let $f$ be an entire function and $L$ a line in $\mathbb{C}$ such that $f(\mathbb{C})\cap L=\emptyset$. Show that $f$ is constant function.,Let  be an entire function and  a line in  such that . Show that  is constant function.,f L \mathbb{C} f(\mathbb{C})\cap L=\emptyset f,"Let $f$ be an entire function and $L$ a line in $\mathbb{C}$ such that $f(\mathbb{C})\cap L=\emptyset$. Show that $f$ is constant function. If $f$ is not constant  then $f(\mathbb{C})$ is dense set in $\mathbb{C}$, but how can I use that line does not intersect the image set?","Let $f$ be an entire function and $L$ a line in $\mathbb{C}$ such that $f(\mathbb{C})\cap L=\emptyset$. Show that $f$ is constant function. If $f$ is not constant  then $f(\mathbb{C})$ is dense set in $\mathbb{C}$, but how can I use that line does not intersect the image set?",,"['real-analysis', 'complex-analysis']"
67,$ \lim_{n \to \infty} \frac{\sqrt{n}(\sqrt{1} + \sqrt{2} + ... + \sqrt{n})}{n^2} $,, \lim_{n \to \infty} \frac{\sqrt{n}(\sqrt{1} + \sqrt{2} + ... + \sqrt{n})}{n^2} ,"How do I find the following limit? $$ \lim_{n \to \infty} \frac{\sqrt{n}(\sqrt{1} + \sqrt{2} + ... + \sqrt{n})}{n^2} $$ Can limit be find by Riemann sums? $$\lim_{n\to \infty}\sum_{k=1}^{n}f(C_k)\Delta{x} = \int_{a}^{b}f(x)\,dx$$ I'm not sure what $f(C_k)$ is.","How do I find the following limit? $$ \lim_{n \to \infty} \frac{\sqrt{n}(\sqrt{1} + \sqrt{2} + ... + \sqrt{n})}{n^2} $$ Can limit be find by Riemann sums? $$\lim_{n\to \infty}\sum_{k=1}^{n}f(C_k)\Delta{x} = \int_{a}^{b}f(x)\,dx$$ I'm not sure what $f(C_k)$ is.",,"['real-analysis', 'limits', 'definite-integrals', 'radicals', 'limits-without-lhopital']"
68,$ \lim_{n \rightarrow \infty} n \left( \frac{1}{(n+1)^2} + \frac{1}{(n+2)^2} + \cdots + \frac{1}{(2n)^2} \right)$ as Riemann sum?,as Riemann sum?, \lim_{n \rightarrow \infty} n \left( \frac{1}{(n+1)^2} + \frac{1}{(n+2)^2} + \cdots + \frac{1}{(2n)^2} \right),"I am trying to evaluate the limit $$ \lim_{n \rightarrow \infty} n \left( \frac{1}{(n+1)^2} + \frac{1}{(n+2)^2} + \cdots +  \frac{1}{(2n)^2} \right).$$ I have been trying to convert it to the Riemann sum of some integral, but have been unable to recongize what the integral should be. How should I go about solving this problem?","I am trying to evaluate the limit $$ \lim_{n \rightarrow \infty} n \left( \frac{1}{(n+1)^2} + \frac{1}{(n+2)^2} + \cdots +  \frac{1}{(2n)^2} \right).$$ I have been trying to convert it to the Riemann sum of some integral, but have been unable to recongize what the integral should be. How should I go about solving this problem?",,"['calculus', 'real-analysis']"
69,Prove $\sum_1^{\infty} a_i^2$ convergent,Prove  convergent,\sum_1^{\infty} a_i^2,"Let$\{a_i\}$ be a decreasing sequence of positive number such than$\sum_{j=1}^{\infty}\frac{a_j}{\sqrt{j}}$ is convergent. Prove $\sum_1^{\infty} a_i^2$ is convergent. My attemption is using Abel test , clearly $\frac{a_j}{\sqrt{j}}$ is a decreasing sequence, and also $\sum_{j=1}^{\infty}\frac{a_j}{\sqrt{j}}$ is convergent, so $\{\frac{a_j}{\sqrt{j}}\}$ is bound, in order to use Abel test, we only need to prove $\sum \sqrt{j}a_i$ is convergent, but I do not know how to do it.","Let$\{a_i\}$ be a decreasing sequence of positive number such than$\sum_{j=1}^{\infty}\frac{a_j}{\sqrt{j}}$ is convergent. Prove $\sum_1^{\infty} a_i^2$ is convergent. My attemption is using Abel test , clearly $\frac{a_j}{\sqrt{j}}$ is a decreasing sequence, and also $\sum_{j=1}^{\infty}\frac{a_j}{\sqrt{j}}$ is convergent, so $\{\frac{a_j}{\sqrt{j}}\}$ is bound, in order to use Abel test, we only need to prove $\sum \sqrt{j}a_i$ is convergent, but I do not know how to do it.",,"['real-analysis', 'sequences-and-series']"
70,Can you replace $x$ with $x^2$ for any Maclaurin series?,Can you replace  with  for any Maclaurin series?,x x^2,"Let's say I have a Maclaurin series for some function $f(x)$, and to find Maclaurin series $f(x^2)$ can I just substitute $x^2$ for each term Maclaurin series for $f(x)$?","Let's say I have a Maclaurin series for some function $f(x)$, and to find Maclaurin series $f(x^2)$ can I just substitute $x^2$ for each term Maclaurin series for $f(x)$?",,[]
71,Does $\sum_{n = 2}^{\infty} [\zeta(n) - 1]$ converge?,Does  converge?,\sum_{n = 2}^{\infty} [\zeta(n) - 1],"Question in the title. Does $\sum_{n = 2}^{\infty} [\zeta(n) - 1]$ converge? If not, how about $\sum_{n = 1}^{\infty} [\zeta(2n) - 1]$?","Question in the title. Does $\sum_{n = 2}^{\infty} [\zeta(n) - 1]$ converge? If not, how about $\sum_{n = 1}^{\infty} [\zeta(2n) - 1]$?",,"['real-analysis', 'complex-analysis', 'analytic-number-theory', 'riemann-zeta']"
72,Riemann Integral as a limit of sum,Riemann Integral as a limit of sum,,"One of the standard definitions of Riemann Integral is as follows: Let $f$ be bounded on $[a, b]$. For any partition $P = \{x_{0}, x_{1}, x_{2}, \ldots, x_{n}\}$ of $[a, b]$ and any choice of points $t_{k} \in [x_{k - 1}, x_{k}]$ the sum $$S(P, f) = \sum_{k = 1}^{n}f(t_{k})(x_{k} - x_{k - 1})$$ is called a Riemann sum for $f$ over $P$ and tags $t_{k}$. The norm of $P$ denoted by $||P||$ is defined as $||P|| = \max_{k = 1}^{n}(x_{k} - x_{k - 1})$. A number $I$ is said to be Riemann integral of $f$ over $[a, b]$ if for any arbitrary $\epsilon > 0$ there exists a $\delta > 0$ such that $$|S(P, f) - I| < \epsilon$$ for all Riemann sums $f$ over any partition $P$ with $||P|| < \delta$. When such a number $I$ exists we say that $f$ is Riemann integrable over $[a, b]$ and we write $$I = \int_{a}^{b}f(x)\,dx$$ Note that if $f$ is Riemann integrable over $[a, b]$ then we can choose partition $P$ with points $x_{k} = a + k(b - a)/n$ and $t_{k} = x_{k}$ or $t_{k} = x_{k - 1}$. Thus if $I = \int_{a}^{b}f(x)\,dx$ exists then by definition we have $$\int_{a}^{b}f(x)\,dx = \lim_{n \to \infty}\frac{b-a}{n}\sum_{k = 1}^{n}f\left(a + \frac{k(b - a)}{n}\right)\tag{1}$$ My question is: does the converse hold? If the limit in $(1)$ exists for a certain function $f$ bounded on $[a, b]$ does it mean that $f$ is Riemann integrable over $[a, b]$ according to the definition of Riemann integrability mentioned above? The reason I ask this question is that many introductory calculus textbooks try to be smart and sort of put $(1)$ as the definition of $\int_{a}^{b}f(x)\,dx$ and hope that they have given a much better definition of integral compared to $F(b) - F(a)$ where $F$ is anti-derivative of $f$. Update : As can be seen from the answer by user mrf, the converse does not hold and the equation $(1)$ can not be used as a definition of Riemann integral for a bounded. Now I update my question with a pedagogic bent. Why do many introductory calculus textbook try to define Riemann integral as a limit of sum as mentioned in $(1)$? Does it add any value in terms of pedagogy to teach something which is totally wrong?","One of the standard definitions of Riemann Integral is as follows: Let $f$ be bounded on $[a, b]$. For any partition $P = \{x_{0}, x_{1}, x_{2}, \ldots, x_{n}\}$ of $[a, b]$ and any choice of points $t_{k} \in [x_{k - 1}, x_{k}]$ the sum $$S(P, f) = \sum_{k = 1}^{n}f(t_{k})(x_{k} - x_{k - 1})$$ is called a Riemann sum for $f$ over $P$ and tags $t_{k}$. The norm of $P$ denoted by $||P||$ is defined as $||P|| = \max_{k = 1}^{n}(x_{k} - x_{k - 1})$. A number $I$ is said to be Riemann integral of $f$ over $[a, b]$ if for any arbitrary $\epsilon > 0$ there exists a $\delta > 0$ such that $$|S(P, f) - I| < \epsilon$$ for all Riemann sums $f$ over any partition $P$ with $||P|| < \delta$. When such a number $I$ exists we say that $f$ is Riemann integrable over $[a, b]$ and we write $$I = \int_{a}^{b}f(x)\,dx$$ Note that if $f$ is Riemann integrable over $[a, b]$ then we can choose partition $P$ with points $x_{k} = a + k(b - a)/n$ and $t_{k} = x_{k}$ or $t_{k} = x_{k - 1}$. Thus if $I = \int_{a}^{b}f(x)\,dx$ exists then by definition we have $$\int_{a}^{b}f(x)\,dx = \lim_{n \to \infty}\frac{b-a}{n}\sum_{k = 1}^{n}f\left(a + \frac{k(b - a)}{n}\right)\tag{1}$$ My question is: does the converse hold? If the limit in $(1)$ exists for a certain function $f$ bounded on $[a, b]$ does it mean that $f$ is Riemann integrable over $[a, b]$ according to the definition of Riemann integrability mentioned above? The reason I ask this question is that many introductory calculus textbooks try to be smart and sort of put $(1)$ as the definition of $\int_{a}^{b}f(x)\,dx$ and hope that they have given a much better definition of integral compared to $F(b) - F(a)$ where $F$ is anti-derivative of $f$. Update : As can be seen from the answer by user mrf, the converse does not hold and the equation $(1)$ can not be used as a definition of Riemann integral for a bounded. Now I update my question with a pedagogic bent. Why do many introductory calculus textbook try to define Riemann integral as a limit of sum as mentioned in $(1)$? Does it add any value in terms of pedagogy to teach something which is totally wrong?",,"['calculus', 'real-analysis', 'soft-question']"
73,"If $K$ is compact and $C$ is closed in $\mathbb{R}^k$, prove that $K + C$ is closed using a ""direct"" proof","If  is compact and  is closed in , prove that  is closed using a ""direct"" proof",K C \mathbb{R}^k K + C,"Rudin Exercise 4.25(a) reads: If $K$ is compact and $C$ is closed in $\mathbb{R}^k$, prove that $K + C$ is closed. The hints in the problem suggest a proof by proving that the complement of $K + C$ is open, a path which I was able to follow into a successful proof.  However, I want to prove it ""directly"", by showing that any limit point of $K + C$ must be within $K + C$, however I run into the following problem: My Attempt: Suppose $z$ is a limit point of $K + C$.  Then there is a sequence $\{z_n\} \to z$ in $K + C$.  Since each $z_i$ is an element of $K + C$, we can write $z_i = k_i + c_i$ for sequences $\{k_i\}$, $\{c_i\}$ in $K$ and $C$ respectively. Now, we simply must show that $\{k_i\} \to k \in K$ and $\{c_i\} \to c \in C$ to be done. However, I noticed that $\{k_n\}$ and $\{c_n\}$ do not necessarily converge when their sum does. As an example, in $\mathbb{R}$, take $k_i = (-1)^i$ and $c_i = (-1)^{i+1}$.  Then neither $\{k_n\}$ nor $\{c_n\}$ converge, but their sum does. Are there any suggestions on how to get around this problem and complete this more ""direct"" proof? Thanks!","Rudin Exercise 4.25(a) reads: If $K$ is compact and $C$ is closed in $\mathbb{R}^k$, prove that $K + C$ is closed. The hints in the problem suggest a proof by proving that the complement of $K + C$ is open, a path which I was able to follow into a successful proof.  However, I want to prove it ""directly"", by showing that any limit point of $K + C$ must be within $K + C$, however I run into the following problem: My Attempt: Suppose $z$ is a limit point of $K + C$.  Then there is a sequence $\{z_n\} \to z$ in $K + C$.  Since each $z_i$ is an element of $K + C$, we can write $z_i = k_i + c_i$ for sequences $\{k_i\}$, $\{c_i\}$ in $K$ and $C$ respectively. Now, we simply must show that $\{k_i\} \to k \in K$ and $\{c_i\} \to c \in C$ to be done. However, I noticed that $\{k_n\}$ and $\{c_n\}$ do not necessarily converge when their sum does. As an example, in $\mathbb{R}$, take $k_i = (-1)^i$ and $c_i = (-1)^{i+1}$.  Then neither $\{k_n\}$ nor $\{c_n\}$ converge, but their sum does. Are there any suggestions on how to get around this problem and complete this more ""direct"" proof? Thanks!",,"['real-analysis', 'general-topology', 'analysis', 'limits', 'convergence-divergence']"
74,"$f:\mathbb{R}\to\mathbb{R}$, continuous, such that $xf(x)>0$ when $x\neq 0$. Show that $f(0)=0$",", continuous, such that  when . Show that",f:\mathbb{R}\to\mathbb{R} xf(x)>0 x\neq 0 f(0)=0,"I need to prove: $f:\mathbb{R}\to\mathbb{R}$, continuous, such that $xf(x)>0$ when $x\neq 0$. Show that $f(0)=0$. Show that if we remove the continuity this result will fail. Give an example. For an example, I thought of $f(x) = x$. This is functinuous, and $xf(x) = x^2>0$ when $x\neq 0$, but I can't make a non continuous version of this function to try. Maybe $f(x) = \frac{x^2-2x}{x-2}$? This functions is not continuous at $x=0$ but it's equal to $x$ everywhere except at $x=0$, so $xf(x)>0$ stills valid, but instead of having $f(0)\neq 0$ we don't even have a definition for $x=0$. Maybe if I define it with any number for $x=0$ it works? Also, how to prove such result? I tried considering: $$g(x) = f(x)-x$$ Somehow if I assume $xf(x)>0$ I need to prove $g(0) = 0$, maybe using the intermediate value theorem. Any ideas? I can't see how $xf(x)>0$ helps.","I need to prove: $f:\mathbb{R}\to\mathbb{R}$, continuous, such that $xf(x)>0$ when $x\neq 0$. Show that $f(0)=0$. Show that if we remove the continuity this result will fail. Give an example. For an example, I thought of $f(x) = x$. This is functinuous, and $xf(x) = x^2>0$ when $x\neq 0$, but I can't make a non continuous version of this function to try. Maybe $f(x) = \frac{x^2-2x}{x-2}$? This functions is not continuous at $x=0$ but it's equal to $x$ everywhere except at $x=0$, so $xf(x)>0$ stills valid, but instead of having $f(0)\neq 0$ we don't even have a definition for $x=0$. Maybe if I define it with any number for $x=0$ it works? Also, how to prove such result? I tried considering: $$g(x) = f(x)-x$$ Somehow if I assume $xf(x)>0$ I need to prove $g(0) = 0$, maybe using the intermediate value theorem. Any ideas? I can't see how $xf(x)>0$ helps.",,"['calculus', 'real-analysis', 'functions']"
75,Continuous function rational domain.,Continuous function rational domain.,,Is the function defined by $$\begin{align} \mathbb{Q} &\rightarrow \mathbb{R}\\ q &\mapsto \frac{1}{q-e} \end{align}$$ a continuous function?,Is the function defined by $$\begin{align} \mathbb{Q} &\rightarrow \mathbb{R}\\ q &\mapsto \frac{1}{q-e} \end{align}$$ a continuous function?,,"['real-analysis', 'continuity']"
76,Understanding theorem $9.21$ from Rudin -- Partial Derivatives.,Understanding theorem  from Rudin -- Partial Derivatives.,9.21,"Let's say that $f: E \subset \mathbb{R}^n \to \mathbb{R}^m$, where $E$ is an open set,  is continuously differentiable if $f'$ is a continuous mapping of $E$ into $L(\mathbb{R}^n,\mathbb{R}^m)$, and denote the set of such functions by $\mathcal{C}'(E)$. Now, consider the following theorem: Suppose $\mathbf{f}$ maps an open set $E \subset \mathbb{R}^n$ into   $\mathbb{R}^m$. Then $f \in \mathcal{C}'(E)$ if and only if the   partial derivatives $(D_jf_i)$ exist and are continuous on $E$ for $1 > \leq i \leq m$ and $1 \leq j \leq n$. $f \in \mathcal{C}'(E)$ means   class of continuous differentiable functions with domain $E$. This is theorem $9.21$ of Rudin 3rd edition (page $219$). I am having trouble understanding the proof. For the $\implies $ side of the proof, I do not understand where they use the assumption that $f \in \mathcal{C}'(E)$ as well as how they form the first inequality. For the $\Leftarrow$ implication, I definitely do not follow the argument given. Can anyone break down what is going? I would definitely appreciate help in understand how the proof of this theorem works.","Let's say that $f: E \subset \mathbb{R}^n \to \mathbb{R}^m$, where $E$ is an open set,  is continuously differentiable if $f'$ is a continuous mapping of $E$ into $L(\mathbb{R}^n,\mathbb{R}^m)$, and denote the set of such functions by $\mathcal{C}'(E)$. Now, consider the following theorem: Suppose $\mathbf{f}$ maps an open set $E \subset \mathbb{R}^n$ into   $\mathbb{R}^m$. Then $f \in \mathcal{C}'(E)$ if and only if the   partial derivatives $(D_jf_i)$ exist and are continuous on $E$ for $1 > \leq i \leq m$ and $1 \leq j \leq n$. $f \in \mathcal{C}'(E)$ means   class of continuous differentiable functions with domain $E$. This is theorem $9.21$ of Rudin 3rd edition (page $219$). I am having trouble understanding the proof. For the $\implies $ side of the proof, I do not understand where they use the assumption that $f \in \mathcal{C}'(E)$ as well as how they form the first inequality. For the $\Leftarrow$ implication, I definitely do not follow the argument given. Can anyone break down what is going? I would definitely appreciate help in understand how the proof of this theorem works.",,"['calculus', 'real-analysis']"
77,Proof Check Lemma 2.2.10 in Tao,Proof Check Lemma 2.2.10 in Tao,,"I aim to prove the following. Lemma 2.2.10 . Let $a$ be a positive number. Then there exists exactly one natural number $b$ such that $b{+\!+} = a$ . I use the following. Definition 2.2.1 (Addition of natural numbers). Let $m$ be a natural number. To add zero to $m$ , we define $0 + m := m$ . Now suppose inductively that we have defined how to add $n$ to $m$ . Then we can add $n{+\!+}$ to $m$ by defining $(n{+\!+}) + m := (n + m){+\!+}$ . Proposition 2.2.6 (Cancellation law). Let $a, b, c$ be natural numbers such that $a + b = a + c$ . Then we have $b = c$ . Axiom 2.4 . Different natural numbers must have different successors; i.e., if $n, m$ are natural numbers and $n \neq m$ , then $n{+\!+} \neq m{+\!+}$ . Equivalently, if $n{+\!+} = m{+\!+}$ , then we must have $n = m$ . Tao suggested the use of induction, so I am doubting the validity of my proof. Proof: Proceed by contradiction. Let us assume we have $2$ differing natural numbers $b$ and $c$ , such that $b{+\!+} = a$ and $c{+\!+} = a$ .  Then we have $b{+\!+} = 0 + b{+\!+}$ and $c{+\!+} = 0 + c{+\!+}$ (definition of addition). So we then have that $0 + b{+\!+} = 0 + c{+\!+}$ , but then $b{+\!+} = c{+\!+}$ (Cancellation Law). This is a contradiction due to Axiom 2.4 . I am self-studying real analysis, so I want to ensure that I am proceeding correctly.","I aim to prove the following. Lemma 2.2.10 . Let be a positive number. Then there exists exactly one natural number such that . I use the following. Definition 2.2.1 (Addition of natural numbers). Let be a natural number. To add zero to , we define . Now suppose inductively that we have defined how to add to . Then we can add to by defining . Proposition 2.2.6 (Cancellation law). Let be natural numbers such that . Then we have . Axiom 2.4 . Different natural numbers must have different successors; i.e., if are natural numbers and , then . Equivalently, if , then we must have . Tao suggested the use of induction, so I am doubting the validity of my proof. Proof: Proceed by contradiction. Let us assume we have differing natural numbers and , such that and .  Then we have and (definition of addition). So we then have that , but then (Cancellation Law). This is a contradiction due to Axiom 2.4 . I am self-studying real analysis, so I want to ensure that I am proceeding correctly.","a b b{+\!+} = a m m 0 + m := m n m n{+\!+} m (n{+\!+}) + m := (n + m){+\!+} a, b, c a + b = a + c b = c n, m n \neq m n{+\!+} \neq m{+\!+} n{+\!+} = m{+\!+} n = m 2 b c b{+\!+} = a c{+\!+} = a b{+\!+} = 0 + b{+\!+} c{+\!+} = 0 + c{+\!+} 0 + b{+\!+} = 0 + c{+\!+} b{+\!+} = c{+\!+}",['real-analysis']
78,How to show that $e^{-x}$ tends to $0$ when $x\to \infty$ if $e^{-x}$ is defined as the power series.,How to show that  tends to  when  if  is defined as the power series.,e^{-x} 0 x\to \infty e^{-x},With only the formal definition of $$f(x) = \exp(-x)= \sum \frac{(-x)^n}{n!}$$ how can we show that $$\lim_{x\to  \infty} f(x)=0?$$ I am looking for a proof that would not use the identity $\exp(x)\exp(-x)=1$ in order to find a strategy for similar problems (i.e. limits of functions defined as power series $\sum a_n x^n$ where $a_n$ does not have a fixed sign beginning at any $n_0$).,With only the formal definition of $$f(x) = \exp(-x)= \sum \frac{(-x)^n}{n!}$$ how can we show that $$\lim_{x\to  \infty} f(x)=0?$$ I am looking for a proof that would not use the identity $\exp(x)\exp(-x)=1$ in order to find a strategy for similar problems (i.e. limits of functions defined as power series $\sum a_n x^n$ where $a_n$ does not have a fixed sign beginning at any $n_0$).,,"['real-analysis', 'sequences-and-series', 'power-series']"
79,Improper integral $\int_0^\infty \frac{\sin(x)}{x}dx$ - Showing convergence.,Improper integral  - Showing convergence.,\int_0^\infty \frac{\sin(x)}{x}dx,"1)Show that for all $n\in\mathbb{N}$ the following is true: $$\int_{\pi}^{n\pi}|\frac{\sin(x)}{x}|dx\geq C\cdot \sum_{k=1}^{n-1}\frac{1}{k+1}$$ for a constant $C>0$ and conclude that the improper integral $\int_0^\infty \frac{\sin(x)}{x}dx$ isn't absolutely convergent. 2)Show that the improper integral $\int_0^\infty \frac{1-\cos(x)}{x^2}dx$ is absolutely convergent. (The integrand is to be expanded continuous at $x=0$.). 3)Using 2), show that the improper integral $\int_0^\infty \frac{\sin(x)}{x}dx$ is convergent. We started discussing improper integrals in class and our prof showed us how some can be solved and some can't. Anyways, Here were my ideas so far: 1) I thought about to do the integral and seeing if what I get out of it gives me any idea to show the inequality. But I couldn't even solve the integral (not by hand nor with the help of an integral calculator). So I don't know what to do next. 2)To be hoenst I'm totally lost here. No idea how to approach it. 3)Well, since I didn't solve 2). Sorry for my lack of work here, but this topic just doesn't want to stick with me.","1)Show that for all $n\in\mathbb{N}$ the following is true: $$\int_{\pi}^{n\pi}|\frac{\sin(x)}{x}|dx\geq C\cdot \sum_{k=1}^{n-1}\frac{1}{k+1}$$ for a constant $C>0$ and conclude that the improper integral $\int_0^\infty \frac{\sin(x)}{x}dx$ isn't absolutely convergent. 2)Show that the improper integral $\int_0^\infty \frac{1-\cos(x)}{x^2}dx$ is absolutely convergent. (The integrand is to be expanded continuous at $x=0$.). 3)Using 2), show that the improper integral $\int_0^\infty \frac{\sin(x)}{x}dx$ is convergent. We started discussing improper integrals in class and our prof showed us how some can be solved and some can't. Anyways, Here were my ideas so far: 1) I thought about to do the integral and seeing if what I get out of it gives me any idea to show the inequality. But I couldn't even solve the integral (not by hand nor with the help of an integral calculator). So I don't know what to do next. 2)To be hoenst I'm totally lost here. No idea how to approach it. 3)Well, since I didn't solve 2). Sorry for my lack of work here, but this topic just doesn't want to stick with me.",,"['real-analysis', 'integration', 'convergence-divergence', 'improper-integrals', 'absolute-convergence']"
80,Proving that the characteristic function is uniformly continous.,Proving that the characteristic function is uniformly continous.,,I am trying to prove that the characteristic function is uniformly continuous. I understand how to get to this bound: And I would like to find the $\delta$ as a function of $\epsilon$  but I am used to having $|f(x) -f(c)|$ instead of $|f(x+h) - f(x)|$ when doing uniform continuous proofs. Also $X$ is a random variable and I am unsure how to proceed. Could I see how one would terminate this proof by finding the correct $\delta$?,I am trying to prove that the characteristic function is uniformly continuous. I understand how to get to this bound: And I would like to find the $\delta$ as a function of $\epsilon$  but I am used to having $|f(x) -f(c)|$ instead of $|f(x+h) - f(x)|$ when doing uniform continuous proofs. Also $X$ is a random variable and I am unsure how to proceed. Could I see how one would terminate this proof by finding the correct $\delta$?,,"['real-analysis', 'probability-theory', 'uniform-continuity']"
81,How to show that $f\equiv 0$? [closed],How to show that ? [closed],f\equiv 0,"Closed. This question is off-topic . It is not currently accepting answers. This question is missing context or other details : Please improve the question by providing additional context, which ideally includes your thoughts on the problem and any attempts you have made to solve it. This information helps others identify where you have difficulties and helps them write answers appropriate to your experience level. Closed 9 years ago . Improve this question Let $f:[0;1]\to \mathbb{R}$ be a continuous function satisfying $$f\left(\frac{x}{2}\right) + f\left(\frac{x+1}{2}\right)=3f(x).$$ How to show that $f\equiv0$?","Closed. This question is off-topic . It is not currently accepting answers. This question is missing context or other details : Please improve the question by providing additional context, which ideally includes your thoughts on the problem and any attempts you have made to solve it. This information helps others identify where you have difficulties and helps them write answers appropriate to your experience level. Closed 9 years ago . Improve this question Let $f:[0;1]\to \mathbb{R}$ be a continuous function satisfying $$f\left(\frac{x}{2}\right) + f\left(\frac{x+1}{2}\right)=3f(x).$$ How to show that $f\equiv0$?",,"['real-analysis', 'continuity']"
82,$ \sum_{n=1}^{\infty}a_{n} $ diverges but $ \sum_{n=1}^{\infty}\frac{a_{n}}{1+a_{n}^{2}} $ sometimes converges and sometime diverges.,diverges but  sometimes converges and sometime diverges., \sum_{n=1}^{\infty}a_{n}   \sum_{n=1}^{\infty}\frac{a_{n}}{1+a_{n}^{2}} ,"Let $ \lbrace a_{n}\rbrace $ be a sequence of positive terms such that $ \sum \limits_{n=1}^{\infty}a_{n} $ diverges. I am going to show that the series $$ \sum \limits_{n=1}^{\infty}\dfrac{a_{n}}{1+a_{n}^{2}} $$ sometimes converges and sometime diverges. My Attempt: For the divergence of $ \sum \limits_{n=1}^{\infty}\dfrac{a_{n}}{1+a_{n}^{2}} $, I defined the sequence $ \lbrace a_{n}\rbrace $ as follows. For each $ n\in \mathbb{N} $, $ a_{n}=1 $. Then $ \sum \limits_{n=1}^{\infty}a_{n}=\sum \limits_{n=1}^{\infty}1 $  and  $ \sum \limits_{n=1}^{\infty}\dfrac{a_{n}}{1+a_{n}^{2}}=\sum \limits_{n=1}^{\infty}\dfrac{1}{2} $. Hence both  $ \sum \limits_{n=1}^{\infty}a_{n} $ and $ \sum \limits_{n=1}^{\infty}\dfrac{a_{n}}{1+a_{n}^{2}} $ are divergent. But I am having trouble finding an example for the convergence of $ \sum \limits_{n=1}^{\infty}\dfrac{a_{n}}{1+a_{n}^{2}} $. Can any one please give me a hint or an idea?","Let $ \lbrace a_{n}\rbrace $ be a sequence of positive terms such that $ \sum \limits_{n=1}^{\infty}a_{n} $ diverges. I am going to show that the series $$ \sum \limits_{n=1}^{\infty}\dfrac{a_{n}}{1+a_{n}^{2}} $$ sometimes converges and sometime diverges. My Attempt: For the divergence of $ \sum \limits_{n=1}^{\infty}\dfrac{a_{n}}{1+a_{n}^{2}} $, I defined the sequence $ \lbrace a_{n}\rbrace $ as follows. For each $ n\in \mathbb{N} $, $ a_{n}=1 $. Then $ \sum \limits_{n=1}^{\infty}a_{n}=\sum \limits_{n=1}^{\infty}1 $  and  $ \sum \limits_{n=1}^{\infty}\dfrac{a_{n}}{1+a_{n}^{2}}=\sum \limits_{n=1}^{\infty}\dfrac{1}{2} $. Hence both  $ \sum \limits_{n=1}^{\infty}a_{n} $ and $ \sum \limits_{n=1}^{\infty}\dfrac{a_{n}}{1+a_{n}^{2}} $ are divergent. But I am having trouble finding an example for the convergence of $ \sum \limits_{n=1}^{\infty}\dfrac{a_{n}}{1+a_{n}^{2}} $. Can any one please give me a hint or an idea?",,"['calculus', 'real-analysis', 'sequences-and-series', 'analysis', 'convergence-divergence']"
83,Isometry of a complete normed space is also complete.,Isometry of a complete normed space is also complete.,,"Let $X$ be a complete normed space and assume the normed space $Y$ is isometric to $X$. Show that $Y$ is complete. I tried: Since X is complete $||x_n-x_m||<\epsilon, \forall n,m>N$ and since $Y$ is isometric to $X$ there exists an isometry $f:X\to Y$ such that  $$||f(x_n)-f(x_m)||=||x_n-x_m||<\epsilon, \forall n,m>N.$$ I stuck at this step.","Let $X$ be a complete normed space and assume the normed space $Y$ is isometric to $X$. Show that $Y$ is complete. I tried: Since X is complete $||x_n-x_m||<\epsilon, \forall n,m>N$ and since $Y$ is isometric to $X$ there exists an isometry $f:X\to Y$ such that  $$||f(x_n)-f(x_m)||=||x_n-x_m||<\epsilon, \forall n,m>N.$$ I stuck at this step.",,"['real-analysis', 'functional-analysis', 'normed-spaces']"
84,Example 3.53 in Baby Rudin,Example 3.53 in Baby Rudin,,"Here's Example 3.53 in the book Principles of Mathematical Analysis by Walter Rudin, third edition. Consider the convergent series $$1 - \frac{1}{2} + \frac{1}{3} - \frac{1}{4} + \frac{1}{5} - \frac{1}{6} + \ldots$$ and one of its rearrangements $$1 + \frac{1}{3} - \frac{1}{2} + \frac{1}{5} + \frac{1}{7} - \frac{1}{4} + \frac{1}{9} + \frac{1}{11} - \frac{1}{6} + \ldots$$ in which two positive terms are always followed by one negative. If $s$ is the sum of the original series, then $$s < 1 - \frac{1}{2} + \frac{1}{3} = \frac{5}{6}.$$ Since $$\frac{1}{4k-3} + \frac{1}{4k-1} - \frac{1}{2k} = \frac{8k-4}{(4k-1)(4k-3)} - \frac{1}{2k} = \frac{2k(8k-4) - (4k-1)(4k-3)}{2k(4k-1)(4k-3)} = \frac{8k-3}{2k(4k-1)(4k-3)} > 0$$ for $k \geq 1$, we see that $$s^\prime_3 < s^\prime_6 < s^\prime_9 < \ldots,$$ where $s^\prime_n$ is the $n$th partial sum of the series after the rearrangement. Hence $$\lim_{n\to\infty}\sup s^\prime_n > s^\prime_3 = \frac{5}{6},$$ so that the rearranged series certainly does not converge to $s$. Now here's my question: How to determine, using the machinery developed by Rudin upto this point in the book, if the new (or rearranged) series converges at all? Rudin leaves it to the reader to check that the new series does converge. How to prove this convergence? I would like to have answers that use only the results that Rudin has discussed so far in the book.","Here's Example 3.53 in the book Principles of Mathematical Analysis by Walter Rudin, third edition. Consider the convergent series $$1 - \frac{1}{2} + \frac{1}{3} - \frac{1}{4} + \frac{1}{5} - \frac{1}{6} + \ldots$$ and one of its rearrangements $$1 + \frac{1}{3} - \frac{1}{2} + \frac{1}{5} + \frac{1}{7} - \frac{1}{4} + \frac{1}{9} + \frac{1}{11} - \frac{1}{6} + \ldots$$ in which two positive terms are always followed by one negative. If $s$ is the sum of the original series, then $$s < 1 - \frac{1}{2} + \frac{1}{3} = \frac{5}{6}.$$ Since $$\frac{1}{4k-3} + \frac{1}{4k-1} - \frac{1}{2k} = \frac{8k-4}{(4k-1)(4k-3)} - \frac{1}{2k} = \frac{2k(8k-4) - (4k-1)(4k-3)}{2k(4k-1)(4k-3)} = \frac{8k-3}{2k(4k-1)(4k-3)} > 0$$ for $k \geq 1$, we see that $$s^\prime_3 < s^\prime_6 < s^\prime_9 < \ldots,$$ where $s^\prime_n$ is the $n$th partial sum of the series after the rearrangement. Hence $$\lim_{n\to\infty}\sup s^\prime_n > s^\prime_3 = \frac{5}{6},$$ so that the rearranged series certainly does not converge to $s$. Now here's my question: How to determine, using the machinery developed by Rudin upto this point in the book, if the new (or rearranged) series converges at all? Rudin leaves it to the reader to check that the new series does converge. How to prove this convergence? I would like to have answers that use only the results that Rudin has discussed so far in the book.",,"['real-analysis', 'sequences-and-series', 'analysis', 'convergence-divergence']"
85,Is Dirac's delta function well-defined at Lebesgue points?,Is Dirac's delta function well-defined at Lebesgue points?,,"Usually in textbooks,  $$\int_{\mathbb{R}^d} \delta(\mathbf{x}-\mathbf{y})f(\mathbf{x}) = f(\mathbf{y})$$ holds given $f$ is continuous. On the other hand, the definition of Lebesugue point $\mathbf{y}$ is that the following limit exists $$ \lim_{\epsilon\rightarrow0}\frac{1}{|B_{\epsilon}|}\int_{B_{\epsilon}}f(\mathbf{x}), $$ where $B_{\epsilon}$ is the ball of radius $\epsilon$ centered around $\mathbf{y}.$ Lebesgure differentiation theorem says for $L^1(\mathbb{R}^d)$ function almost every point is a Lebesgue point. All these facts let me think the delta function is also well-defined almost everywhere for $f\in L^1(\mathbb{R}^d)$ because the definition of Lebesgure point is exactly an example of approximation of the delta function. More precisely, can the definition of the delta functional be extended to the domain $L^1$ in this way? For delta function, we take this definition: The delta function is a generalized function that can be defined as the limit of a class of delta sequences. My question is to extend this functional to $L^1$ such that the extended delta function is a linear bounded functional on $L^1.$","Usually in textbooks,  $$\int_{\mathbb{R}^d} \delta(\mathbf{x}-\mathbf{y})f(\mathbf{x}) = f(\mathbf{y})$$ holds given $f$ is continuous. On the other hand, the definition of Lebesugue point $\mathbf{y}$ is that the following limit exists $$ \lim_{\epsilon\rightarrow0}\frac{1}{|B_{\epsilon}|}\int_{B_{\epsilon}}f(\mathbf{x}), $$ where $B_{\epsilon}$ is the ball of radius $\epsilon$ centered around $\mathbf{y}.$ Lebesgure differentiation theorem says for $L^1(\mathbb{R}^d)$ function almost every point is a Lebesgue point. All these facts let me think the delta function is also well-defined almost everywhere for $f\in L^1(\mathbb{R}^d)$ because the definition of Lebesgure point is exactly an example of approximation of the delta function. More precisely, can the definition of the delta functional be extended to the domain $L^1$ in this way? For delta function, we take this definition: The delta function is a generalized function that can be defined as the limit of a class of delta sequences. My question is to extend this functional to $L^1$ such that the extended delta function is a linear bounded functional on $L^1.$",,"['real-analysis', 'functional-analysis', 'sobolev-spaces', 'lebesgue-integral']"
86,"Does there exist a function$f: [0,1]\rightarrow \mathbb{R}$ almost everywhere $0$ but whose range is equal to $\mathbb{R}$",Does there exist a function almost everywhere  but whose range is equal to,"f: [0,1]\rightarrow \mathbb{R} 0 \mathbb{R}","Does there exist a function$f: [0,1]\rightarrow \mathbb{R}, f=0,a.e$  but whose range is equal to $\mathbb{R}$? I can't image what this kind function looks like.","Does there exist a function$f: [0,1]\rightarrow \mathbb{R}, f=0,a.e$  but whose range is equal to $\mathbb{R}$? I can't image what this kind function looks like.",,['real-analysis']
87,Prove that integrable implies bounded [duplicate],Prove that integrable implies bounded [duplicate],,"This question already has answers here : If a function $f(x)$ is Riemann integrable on $[a,b]$, is $f(x)$ bounded on $[a,b]$? (4 answers) Closed 10 years ago . If function $f$ is integrable at $[a,b]$ prove that $f$ is bounded at $[a,b]$.  Could you explain without using rieaman integrable? I have did so far :  if $f$ is integrable then it is continuous at $[a,b].$ let $a\in I=[a,b]$ so for any $\epsilon>0$ there is $\delta$  such as $$|x-a|\leq\delta\Rightarrow|f(x)-f(a)|\leq\epsilon$$ assume $|t-a|\leq\delta$ then $-\epsilon\leq f(t)-f(a)\leq \epsilon$ so $\int_a^x-\epsilon\leq \int_a^xf(t)-f(a)\leq \int_a^x \epsilon$ $\int_a^x|f(t)-f(a)|\leq\epsilon|x-a| $","This question already has answers here : If a function $f(x)$ is Riemann integrable on $[a,b]$, is $f(x)$ bounded on $[a,b]$? (4 answers) Closed 10 years ago . If function $f$ is integrable at $[a,b]$ prove that $f$ is bounded at $[a,b]$.  Could you explain without using rieaman integrable? I have did so far :  if $f$ is integrable then it is continuous at $[a,b].$ let $a\in I=[a,b]$ so for any $\epsilon>0$ there is $\delta$  such as $$|x-a|\leq\delta\Rightarrow|f(x)-f(a)|\leq\epsilon$$ assume $|t-a|\leq\delta$ then $-\epsilon\leq f(t)-f(a)\leq \epsilon$ so $\int_a^x-\epsilon\leq \int_a^xf(t)-f(a)\leq \int_a^x \epsilon$ $\int_a^x|f(t)-f(a)|\leq\epsilon|x-a| $",,"['calculus', 'real-analysis']"
88,"Show that if $\sum_na_n=\infty$ and $a_n\downarrow 0$ then $\sum\limits_n\min(a_{n},\frac{1}{n})=\infty$",Show that if  and  then,"\sum_na_n=\infty a_n\downarrow 0 \sum\limits_n\min(a_{n},\frac{1}{n})=\infty","Let $a_n$ be a sequence decreasing to $0$, and $\sum {{a_n} = \infty } $. Show that:$$\sum {\min \left( {{a_n},{1 \over n}} \right)}  = \infty $$ If there's $N_0$ such that $\forall n>N_0: \min(a_n, {1\over n}) = {1\over n}$  or $\forall n>N_0: \min(a_n, {1\over n}) = a_n$ then, the problem is trivial. Otherwise, let us define $b_n = \min(a_n, {1\over n})$ and two subsequences: $$b_{n_k} = {1 \over n_k},\quad b_{n_l} = a_{n_l}$$ $$\sum {{b_n} = } \sum {{b_{{n_k}}}}  + \sum {{b_{{n_l}}}}  = \infty  + \infty $$ Is that right?","Let $a_n$ be a sequence decreasing to $0$, and $\sum {{a_n} = \infty } $. Show that:$$\sum {\min \left( {{a_n},{1 \over n}} \right)}  = \infty $$ If there's $N_0$ such that $\forall n>N_0: \min(a_n, {1\over n}) = {1\over n}$  or $\forall n>N_0: \min(a_n, {1\over n}) = a_n$ then, the problem is trivial. Otherwise, let us define $b_n = \min(a_n, {1\over n})$ and two subsequences: $$b_{n_k} = {1 \over n_k},\quad b_{n_l} = a_{n_l}$$ $$\sum {{b_n} = } \sum {{b_{{n_k}}}}  + \sum {{b_{{n_l}}}}  = \infty  + \infty $$ Is that right?",,"['real-analysis', 'sequences-and-series']"
89,"Is $f$ surjective, where $|f(x)-x| \leq 2$?","Is  surjective, where ?",f |f(x)-x| \leq 2,Let $f$ be continuous and $f: \mathbb{R}^2 \rightarrow \mathbb{R}^2$. Suppose  $|f(x)-x| \leq 2$ holds for all $x$. Is $f$ surjective?,Let $f$ be continuous and $f: \mathbb{R}^2 \rightarrow \mathbb{R}^2$. Suppose  $|f(x)-x| \leq 2$ holds for all $x$. Is $f$ surjective?,,"['real-analysis', 'analysis']"
90,Continuity of Fixed Point,Continuity of Fixed Point,,"For all $a \in \mathbb{R}$, let $f_a: \mathbb{R}^n \rightarrow \mathbb{R}^n$ be continuous and contractive, that is, there exists $\epsilon \in (0,1)$ such that $\left\| f_a(x)-f_a(y) \right\| \leq (1-\epsilon) \left\| x-y\right\|$ for all $x,y \in \mathbb{R}^n$. Assume that for all $x \in \mathbb{R}^n$, the mapping $a \mapsto f_a(x)$ is continuous. Now let $x_0 \in \mathbb{R}^n$ be fixed. For all $a \in \mathbb{R}$, define the sequence $\{ x_a^0, x_a^1, x_a^2, \cdots \} := \{ x_0, \ f_a(x_0), \ f_a( f_a(x_0) ), \ \cdots \}$, that is, $x_a^k := f_a^{(k)}(x_0)$ for all $k \geq 0$. Define $ \bar{x}_a := \lim_{k \rightarrow \infty} x_a^k$. Under what additional conditions the mapping $a \mapsto \bar{x}_a$ is continuous?","For all $a \in \mathbb{R}$, let $f_a: \mathbb{R}^n \rightarrow \mathbb{R}^n$ be continuous and contractive, that is, there exists $\epsilon \in (0,1)$ such that $\left\| f_a(x)-f_a(y) \right\| \leq (1-\epsilon) \left\| x-y\right\|$ for all $x,y \in \mathbb{R}^n$. Assume that for all $x \in \mathbb{R}^n$, the mapping $a \mapsto f_a(x)$ is continuous. Now let $x_0 \in \mathbb{R}^n$ be fixed. For all $a \in \mathbb{R}$, define the sequence $\{ x_a^0, x_a^1, x_a^2, \cdots \} := \{ x_0, \ f_a(x_0), \ f_a( f_a(x_0) ), \ \cdots \}$, that is, $x_a^k := f_a^{(k)}(x_0)$ for all $k \geq 0$. Define $ \bar{x}_a := \lim_{k \rightarrow \infty} x_a^k$. Under what additional conditions the mapping $a \mapsto \bar{x}_a$ is continuous?",,"['real-analysis', 'sequences-and-series', 'analysis', 'functional-analysis', 'functions']"
91,How would I prove $|x + y| \le |x| + |y|$?,How would I prove ?,|x + y| \le |x| + |y|,"How would I write a detailed structured proof for: for all real numbers $x$ and $y$, $|x + y| \le |x| + |y|$ I'm planning on breaking it up into four cases, where both $x,y < 0$, $x \ge 0$ and $y<0$, $x<0$ and $y \ge0$, and $x,y \ge 0$. But I'm not sure how I'd go about writing it formally. Thanks!","How would I write a detailed structured proof for: for all real numbers $x$ and $y$, $|x + y| \le |x| + |y|$ I'm planning on breaking it up into four cases, where both $x,y < 0$, $x \ge 0$ and $y<0$, $x<0$ and $y \ge0$, and $x,y \ge 0$. But I'm not sure how I'd go about writing it formally. Thanks!",,"['real-analysis', 'inequality', 'proof-writing', 'absolute-value']"
92,Calculate $a^3+b^3+c^3+d^3$ for the real roots of $x^4+2x^3-3x^2-3x+2$,Calculate  for the real roots of,a^3+b^3+c^3+d^3 x^4+2x^3-3x^2-3x+2,"$a,b,c,d \in \mathbb{R}$ are the real roots of $x^4+2x^3-3x^2-3x+2$. Calculate $a^3+b^3+c^3+d^3$. With approximation i found out, that $a^3+b^3+c^3+d^3 = -17$, but how can I proof that without calculating the roots exactly? Cheers","$a,b,c,d \in \mathbb{R}$ are the real roots of $x^4+2x^3-3x^2-3x+2$. Calculate $a^3+b^3+c^3+d^3$. With approximation i found out, that $a^3+b^3+c^3+d^3 = -17$, but how can I proof that without calculating the roots exactly? Cheers",,"['real-analysis', 'polynomials', 'roots', 'symmetric-polynomials']"
93,The Value of a series,The Value of a series,,What is the value of the following series $\sum_{n=1}^\infty\sum_{m=1}^\infty\sum_{k=1}^\infty \frac{1}{mnk(m+n+k+1)}$,What is the value of the following series $\sum_{n=1}^\infty\sum_{m=1}^\infty\sum_{k=1}^\infty \frac{1}{mnk(m+n+k+1)}$,,"['real-analysis', 'sequences-and-series', 'education']"
94,Interior Points,Interior Points,,"I'm learning analysis using the Rudin's book, and sometimes the definitions make me wonder and leave me quite puzzled... So, interior points: a set is open if all the points in the set are interior points. However, if a set has a point inside it, surely it will always have a neighborhood (or a small ball) that will be contained in the set. So, what keeps all the points from being interior points? (points inside the set I mean) Also, second question: is a limit point an interior point? Thank you!","I'm learning analysis using the Rudin's book, and sometimes the definitions make me wonder and leave me quite puzzled... So, interior points: a set is open if all the points in the set are interior points. However, if a set has a point inside it, surely it will always have a neighborhood (or a small ball) that will be contained in the set. So, what keeps all the points from being interior points? (points inside the set I mean) Also, second question: is a limit point an interior point? Thank you!",,"['real-analysis', 'analysis', 'limits']"
95,How to calculate $\lim\limits_{n\to \infty} n^{\frac {\log n}{n^2}}$?,How to calculate ?,\lim\limits_{n\to \infty} n^{\frac {\log n}{n^2}},"I need help to calculate this limit $$\large\lim_{n\to \infty} n^{\frac {\log n}{n^2}}.$$ I know  the limit of the exponent goes to zero, but the limit of the base goes to infinity, I think the limit goes to 1, am I right? Thanks a lot","I need help to calculate this limit $$\large\lim_{n\to \infty} n^{\frac {\log n}{n^2}}.$$ I know  the limit of the exponent goes to zero, but the limit of the base goes to infinity, I think the limit goes to 1, am I right? Thanks a lot",,"['real-analysis', 'limits']"
96,comparison test of $\sum\frac{\sqrt{n+1}-\sqrt{n}}{n}$,comparison test of,\sum\frac{\sqrt{n+1}-\sqrt{n}}{n},I want to know if the sum $$\sum_{n=1}^\infty\frac{\sqrt{n+1}-\sqrt{n}}{n}$$ converges or not. I've tried the ratio and root test but they don't fit. So wolframalpha says that the sum converges by the comparison test. So I've tried to find a convergent majorizing sum (I've tried e.g. $\sum\frac1{n^\alpha}$ with $\alpha>1$ etc.) but I can't find one. does anybody know one?,I want to know if the sum $$\sum_{n=1}^\infty\frac{\sqrt{n+1}-\sqrt{n}}{n}$$ converges or not. I've tried the ratio and root test but they don't fit. So wolframalpha says that the sum converges by the comparison test. So I've tried to find a convergent majorizing sum (I've tried e.g. $\sum\frac1{n^\alpha}$ with $\alpha>1$ etc.) but I can't find one. does anybody know one?,,['real-analysis']
97,Proofs with limit superior and limit inferior: $\liminf a_n \leq \limsup a_n$,Proofs with limit superior and limit inferior:,\liminf a_n \leq \limsup a_n,"I am stuck on proofs with subsequences. I do not really have a strategy or starting point with subsequences. NOTE: subsequential limits are limits of subsequences Prove: $a_n$ is bounded $\implies \liminf a_n \leq \limsup a_n$ Proof: Let $a_n$ be a bounded sequence. That is, $\forall_n(a_n \leq A)$. If $a_n$ converges then $\liminf a_n = \lim a_n = \limsup a_n$ and we are done. Otherwise $a_n$ has a set of subsequential limits we need to show $\liminf a_n \leq \limsup a_n$: This is where I am stuck...","I am stuck on proofs with subsequences. I do not really have a strategy or starting point with subsequences. NOTE: subsequential limits are limits of subsequences Prove: $a_n$ is bounded $\implies \liminf a_n \leq \limsup a_n$ Proof: Let $a_n$ be a bounded sequence. That is, $\forall_n(a_n \leq A)$. If $a_n$ converges then $\liminf a_n = \lim a_n = \limsup a_n$ and we are done. Otherwise $a_n$ has a set of subsequential limits we need to show $\liminf a_n \leq \limsup a_n$: This is where I am stuck...",,"['real-analysis', 'sequences-and-series', 'limits', 'inequality', 'limsup-and-liminf']"
98,What's the sum of $\sum_{k=1}^{\infty} e^{-k(x-k)^{2}}$?,What's the sum of ?,\sum_{k=1}^{\infty} e^{-k(x-k)^{2}},Absolute convergence and uniform convergence are easy to determine for this power series. What could be a possible approach to find the sum of this series  $\sum_{k=1}^{\infty} e^{-k(x-k)^{2}}$ (the sum or an estimate of the sum)?,Absolute convergence and uniform convergence are easy to determine for this power series. What could be a possible approach to find the sum of this series  $\sum_{k=1}^{\infty} e^{-k(x-k)^{2}}$ (the sum or an estimate of the sum)?,,"['calculus', 'real-analysis', 'sequences-and-series']"
99,Can one prove $\int f > 0$ for $f > 0$ without Lebesgue integration?,Can one prove  for  without Lebesgue integration?,\int f > 0 f > 0,"In an exercise session of an analysis course (which covers Riemann integration and differentiation in one dimension rigorously) the following question came up: Suppose $f$ is strictly positive and integrable (on some compact interval $[a,b]$ on the real line). Can we show that $\int_a^b f > 0$? The proof by using measure theory and Lebesgue integration is easy, but also beyond the students at this point. So can one do without machinery such as sets of zero measure? Tools in use: Mean value theorems, fundamental theorem of calculus, Riemann's condition for integrability (upper and lower integral within every epsilon of each other implies integrability), Riemann-Darboux integral, usual integration and differentiation techniques, as well as elementary real analysis in the epsilon-delta style.","In an exercise session of an analysis course (which covers Riemann integration and differentiation in one dimension rigorously) the following question came up: Suppose $f$ is strictly positive and integrable (on some compact interval $[a,b]$ on the real line). Can we show that $\int_a^b f > 0$? The proof by using measure theory and Lebesgue integration is easy, but also beyond the students at this point. So can one do without machinery such as sets of zero measure? Tools in use: Mean value theorems, fundamental theorem of calculus, Riemann's condition for integrability (upper and lower integral within every epsilon of each other implies integrability), Riemann-Darboux integral, usual integration and differentiation techniques, as well as elementary real analysis in the epsilon-delta style.",,"['real-analysis', 'integration']"
