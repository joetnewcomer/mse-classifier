,title_raw,title_text,title_latex,body_raw,body_text,body_latex,tags
0,Jordan measure and Riemann integral,Jordan measure and Riemann integral,,"I am studying Riemann integral, and I haven't understood what is the relation between the concepts of Riemann integral and Jordan measure. Is Riemann integral a special case of Jordan Measure? What is the precise definition of Jordan Measure? Thank so much to who will help me!","I am studying Riemann integral, and I haven't understood what is the relation between the concepts of Riemann integral and Jordan measure. Is Riemann integral a special case of Jordan Measure? What is the precise definition of Jordan Measure? Thank so much to who will help me!",,"['real-analysis', 'analysis', 'measure-theory']"
1,Check if the function is differentiable or not,Check if the function is differentiable or not,,"Consider $f: \mathbb{R^2} \to \mathbb{R}$ , $$f(x, y) = \begin{cases}\frac{xy^2}{x^2 + 2y^2} & (x, y) \ne (0,0)\\0 & (x, y) = (0,0)\end{cases}$$ Where is $f(x, y)$ differentiable over its domain? I am considering $(0, 0)$ as a point, but I am not sure how to go about proving it (or disproving)","Consider , Where is differentiable over its domain? I am considering as a point, but I am not sure how to go about proving it (or disproving)","f: \mathbb{R^2} \to \mathbb{R} f(x, y) = \begin{cases}\frac{xy^2}{x^2 + 2y^2} & (x, y) \ne (0,0)\\0 & (x, y) = (0,0)\end{cases} f(x, y) (0, 0)","['calculus', 'analysis', 'multivariable-calculus']"
2,How prove this $\int_{a}^{b}f(x)dx=\frac{1}{2}(b-a)[f(a)+f(b)]-\frac{1}{12}(b-a)^3f''(\xi)$,How prove this,\int_{a}^{b}f(x)dx=\frac{1}{2}(b-a)[f(a)+f(b)]-\frac{1}{12}(b-a)^3f''(\xi),"Let $f(x)$  be a twice-differentiable function on $(a,b)$,show that there exsit $\xi\in(a,b)$ ,such $$\int_{a}^{b}f(x)dx=\dfrac{1}{2}(b-a)[f(a)+f(b)]-\dfrac{1}{12}(b-a)^3f''(\xi)$$ if this problem condition is Amuss that $f(x)$ is a three-differentiable function, $$f(x)=f(a)+f'(a)(x-a)+\frac{f''(\xi_{1})\cdot (x-a)^2}{2}$$ $$f(x)=f(b)+f'(b)(x-b)+\frac{f''(\xi_{2})\cdot (x-b)^2}{2}$$ so $(1)+(2)$ $$\Longrightarrow 2f(x)=f(a)+f(b)+f'(a)(x-a)+f'(b)(x-b)+\dfrac{1}{2}[f''(\xi_{1})+f''(\xi_{2})][(x-a)^2+(x-b)^2]$$ since $f(x)$ is a three-differentiable function,so $f''(x)$ is  continuous,so there exsit $\xi\in(\xi_{1},\xi_{2})$,such  $$\dfrac{1}{2}[f''(\xi_{1})+f''(\xi_{2})]=f''(\xi)$$ $$\int_{a}^{b}f(x)dx=\dfrac{1}{2}(b-a)[f(a)+f(b)]-\dfrac{1}{12}(b-a)^3f''(\xi)$$ But if $f(x)$ have twice-differentiable,this methods is not usefull Then I use this methods can't prove it.","Let $f(x)$  be a twice-differentiable function on $(a,b)$,show that there exsit $\xi\in(a,b)$ ,such $$\int_{a}^{b}f(x)dx=\dfrac{1}{2}(b-a)[f(a)+f(b)]-\dfrac{1}{12}(b-a)^3f''(\xi)$$ if this problem condition is Amuss that $f(x)$ is a three-differentiable function, $$f(x)=f(a)+f'(a)(x-a)+\frac{f''(\xi_{1})\cdot (x-a)^2}{2}$$ $$f(x)=f(b)+f'(b)(x-b)+\frac{f''(\xi_{2})\cdot (x-b)^2}{2}$$ so $(1)+(2)$ $$\Longrightarrow 2f(x)=f(a)+f(b)+f'(a)(x-a)+f'(b)(x-b)+\dfrac{1}{2}[f''(\xi_{1})+f''(\xi_{2})][(x-a)^2+(x-b)^2]$$ since $f(x)$ is a three-differentiable function,so $f''(x)$ is  continuous,so there exsit $\xi\in(\xi_{1},\xi_{2})$,such  $$\dfrac{1}{2}[f''(\xi_{1})+f''(\xi_{2})]=f''(\xi)$$ $$\int_{a}^{b}f(x)dx=\dfrac{1}{2}(b-a)[f(a)+f(b)]-\dfrac{1}{12}(b-a)^3f''(\xi)$$ But if $f(x)$ have twice-differentiable,this methods is not usefull Then I use this methods can't prove it.",,['analysis']
3,Can a function be uniformly continuous on an open interval?,Can a function be uniformly continuous on an open interval?,,"I am learning analysis and all the uniformly continuous functions I have seen are over a closed interval. So, can a uniformly continuous function be defined on an open interval?","I am learning analysis and all the uniformly continuous functions I have seen are over a closed interval. So, can a uniformly continuous function be defined on an open interval?",,['analysis']
4,Is $f$ constant if every point is local maximum or local minimum of $f$?,Is  constant if every point is local maximum or local minimum of ?,f f,"Suppose $f: I \to \mathbb{R}$ where $I = (a,b)$ or $I = \mathbb{R}$, etc. Suppose that every $x$ is either a local maximum of $f$ or a local minimum of $f$. Does it follow that $f$ is a constant function? I think it's probably easy if you know $f$ is continuous, but assume for the moment that it isn't.","Suppose $f: I \to \mathbb{R}$ where $I = (a,b)$ or $I = \mathbb{R}$, etc. Suppose that every $x$ is either a local maximum of $f$ or a local minimum of $f$. Does it follow that $f$ is a constant function? I think it's probably easy if you know $f$ is continuous, but assume for the moment that it isn't.",,['analysis']
5,"Prove that $1/f$ is Riemann integrable on $[a,b]$",Prove that  is Riemann integrable on,"1/f [a,b]","Prove that if $f$ is Riemann integrable on $[a,b]$ and $0<\rho \le f(x)$ for all $x \in [a,b]$ then $1/f$ is Riemann integrable on $[a,b]$.","Prove that if $f$ is Riemann integrable on $[a,b]$ and $0<\rho \le f(x)$ for all $x \in [a,b]$ then $1/f$ is Riemann integrable on $[a,b]$.",,['analysis']
6,The primitive of a discontinuous function?,The primitive of a discontinuous function?,,"I'm thinking about the primitive of an arbitrary function. As we all know, every continuous function has a primitive according to the Newton-Leibniz formula, $\int_{x_0}^{x} f(x) dx$ is the primitive of function $f(x)$. However, if $f(x)$ is discontinuous, say has a point of discontinuity of the first kind, the the integral has both left and right derivative, but they don't coincide. So the question is whether we are able to find a primitive for those kind of functions. Or more generally, what kind of function has a primitive?","I'm thinking about the primitive of an arbitrary function. As we all know, every continuous function has a primitive according to the Newton-Leibniz formula, $\int_{x_0}^{x} f(x) dx$ is the primitive of function $f(x)$. However, if $f(x)$ is discontinuous, say has a point of discontinuity of the first kind, the the integral has both left and right derivative, but they don't coincide. So the question is whether we are able to find a primitive for those kind of functions. Or more generally, what kind of function has a primitive?",,"['calculus', 'real-analysis', 'analysis']"
7,Lebesgue Density Theorem,Lebesgue Density Theorem,,"Problem from Folland : based on Lebesgue Density Theorem: Let $D_{E}(x) = \lim_{r\to 0}\frac{\mu(E\cap B(r,x))}{\mu(B(r,x))}$ whenever it exists. Find examples of $E$ and $x$ such that $D_{E}(x)$ is a given number $\alpha \in (0,1)$ , or such that $D_{E}(x)$ does not exist. ($X = \mathbb{R}^n$,$\mu$ is Lebesgue measure)","Problem from Folland : based on Lebesgue Density Theorem: Let $D_{E}(x) = \lim_{r\to 0}\frac{\mu(E\cap B(r,x))}{\mu(B(r,x))}$ whenever it exists. Find examples of $E$ and $x$ such that $D_{E}(x)$ is a given number $\alpha \in (0,1)$ , or such that $D_{E}(x)$ does not exist. ($X = \mathbb{R}^n$,$\mu$ is Lebesgue measure)",,['analysis']
8,How can I solve this infinite sum?,How can I solve this infinite sum?,,"I calculated (with the help of Maple) that the following infinite sum is equal to the fraction on the right side. $$ \sum_{i=1}^\infty \frac{i}{\vartheta^{i}}=\frac{\vartheta}{(\vartheta-1)^2} $$ However I don't understand how to derive it correctly. I've tried numerous approaches but none of them have worked out so far. Could someone please give me a hint on how to evaluate the infinite sum above and understand the derivation? Thanks. :)","I calculated (with the help of Maple) that the following infinite sum is equal to the fraction on the right side. $$ \sum_{i=1}^\infty \frac{i}{\vartheta^{i}}=\frac{\vartheta}{(\vartheta-1)^2} $$ However I don't understand how to derive it correctly. I've tried numerous approaches but none of them have worked out so far. Could someone please give me a hint on how to evaluate the infinite sum above and understand the derivation? Thanks. :)",,['analysis']
9,"Let $|f(x)-f(y)| \leqslant (x-y)^2$ for all $x,y\in \mathbb{R}.$ Show that $f$ is a constant.",Let  for all  Show that  is a constant.,"|f(x)-f(y)| \leqslant (x-y)^2 x,y\in \mathbb{R}. f","Let $|f(x)-f(y)| \leqslant (x-y)^2$ for all $x,y\in \mathbb{R}.$ Show that $f$ is a constant. This seemed quite straightforward just using the definition of the derivative, but I've ran into some weird issue. I have that $$|f(x)-f(y)| \leqslant (x-y)(x+y) \Longrightarrow \frac{|f(x)-f(y)|}{x-y} \leqslant x+y$$ from where I have that $\lim_{x\to y} x+y = 2y$ . If the problem assignment would instead had $|f(x)-f(y)| \leqslant |(x-y)^2|$ I would have gotten $$|f(x)-f(y)| \leqslant |x-y||x+y| \Longrightarrow \frac{|f(x)-f(y)|}{x-y} \leqslant |x-y|$$ from where $\lim_{x\to y} |x-y| = 0$ . And $f$ would be a constant. What might I be missing here?","Let for all Show that is a constant. This seemed quite straightforward just using the definition of the derivative, but I've ran into some weird issue. I have that from where I have that . If the problem assignment would instead had I would have gotten from where . And would be a constant. What might I be missing here?","|f(x)-f(y)| \leqslant (x-y)^2 x,y\in \mathbb{R}. f |f(x)-f(y)| \leqslant (x-y)(x+y) \Longrightarrow \frac{|f(x)-f(y)|}{x-y} \leqslant x+y \lim_{x\to y} x+y = 2y |f(x)-f(y)| \leqslant |(x-y)^2| |f(x)-f(y)| \leqslant |x-y||x+y| \Longrightarrow \frac{|f(x)-f(y)|}{x-y} \leqslant |x-y| \lim_{x\to y} |x-y| = 0 f",['calculus']
10,Right continuous and monotone function must exist right derivative?,Right continuous and monotone function must exist right derivative?,,"Right continuous and monotone function must exist right derivative? Suppose $f:R\rightarrow R$ is a right continuous and monotone function, i.e. $f(x+)=f(x),\forall x\in R$ and $f(x)$ is monotone, say non-decreasing. Does the limit exist $\lim_{\delta \searrow 0 }\frac{f(x+\delta)-f(x)}{\delta}$ everywhere ?","Right continuous and monotone function must exist right derivative? Suppose $f:R\rightarrow R$ is a right continuous and monotone function, i.e. $f(x+)=f(x),\forall x\in R$ and $f(x)$ is monotone, say non-decreasing. Does the limit exist $\lim_{\delta \searrow 0 }\frac{f(x+\delta)-f(x)}{\delta}$ everywhere ?",,"['real-analysis', 'analysis', 'derivatives', 'continuity']"
11,Roots of $f(x)=x^3-x^2-2x+1$,Roots of,f(x)=x^3-x^2-2x+1,"We can prove using a monotony study that the function $f(x)=x^3-x^2-2x+1$ has three real roots. However, when I solve the equation $f(x)=0$ using Mathematica, I get $$x_1=\frac{1}{3}+\frac{7^{2/3}}{3 \left(\frac{1}{2} \left(-1+3 i \sqrt{3}\right)\right)^{1/3}}+\frac{1}{3} \left(\frac{7}{2} \left(-1+3 i \sqrt{3}\right)\right)^{1/3}$$ $$x_2=\frac{1}{3}-\frac{\left(\frac{7}{2}\right)^{2/3} \left(1+i \sqrt{3}\right)}{3 \left(-1+3 i \sqrt{3}\right)^{1/3}}-\frac{1}{6} \left(1-i \sqrt{3}\right) \left(\frac{7}{2} \left(-1+3 i \sqrt{3}\right)\right)^{1/3}$$ $$x_3=\frac{1}{3}-\frac{\left(\frac{7}{2}\right)^{2/3} \left(1-i \sqrt{3}\right)}{3 \left(-1+3 i \sqrt{3}\right)^{1/3}}-\frac{1}{6} \left(1+i \sqrt{3}\right) \left(\frac{7}{2} \left(-1+3 i \sqrt{3}\right)\right)^{1/3}$$ Since those roots are real, then maybe there's a way to write them without using the complex number $i$?","We can prove using a monotony study that the function $f(x)=x^3-x^2-2x+1$ has three real roots. However, when I solve the equation $f(x)=0$ using Mathematica, I get $$x_1=\frac{1}{3}+\frac{7^{2/3}}{3 \left(\frac{1}{2} \left(-1+3 i \sqrt{3}\right)\right)^{1/3}}+\frac{1}{3} \left(\frac{7}{2} \left(-1+3 i \sqrt{3}\right)\right)^{1/3}$$ $$x_2=\frac{1}{3}-\frac{\left(\frac{7}{2}\right)^{2/3} \left(1+i \sqrt{3}\right)}{3 \left(-1+3 i \sqrt{3}\right)^{1/3}}-\frac{1}{6} \left(1-i \sqrt{3}\right) \left(\frac{7}{2} \left(-1+3 i \sqrt{3}\right)\right)^{1/3}$$ $$x_3=\frac{1}{3}-\frac{\left(\frac{7}{2}\right)^{2/3} \left(1-i \sqrt{3}\right)}{3 \left(-1+3 i \sqrt{3}\right)^{1/3}}-\frac{1}{6} \left(1+i \sqrt{3}\right) \left(\frac{7}{2} \left(-1+3 i \sqrt{3}\right)\right)^{1/3}$$ Since those roots are real, then maybe there's a way to write them without using the complex number $i$?",,"['analysis', 'polynomials', 'complex-numbers', 'roots']"
12,"A function on [0,1] that satisfies the following conditions","A function on [0,1] that satisfies the following conditions",,"I am struggling to come up with a function $F(x)$ with $x \in [0,1]$ that satisfy the following conditions (some sort of variants of Inada conditions) continuously differentiable strictly increasing convex $F(0) = 0$ the first order derivative approaches $0$ when $x \rightarrow 0$ the first order derivative approaches $+\infty$ when $x \rightarrow 1$ Any thought will be appreciated. Thanks for your help in advance! Addition: Also, would it be possible to parameterize this function, say with a parameter $a$, so that $\partial F(x)/\partial x$ is monotonic in $a$? Edits: As correctly pointed out, indeed, what I meant is $F(x)$ continuous on $[0,1]$ and $F'(x)$ continous on $(0,1)$. Thank you all for the wonderful help!","I am struggling to come up with a function $F(x)$ with $x \in [0,1]$ that satisfy the following conditions (some sort of variants of Inada conditions) continuously differentiable strictly increasing convex $F(0) = 0$ the first order derivative approaches $0$ when $x \rightarrow 0$ the first order derivative approaches $+\infty$ when $x \rightarrow 1$ Any thought will be appreciated. Thanks for your help in advance! Addition: Also, would it be possible to parameterize this function, say with a parameter $a$, so that $\partial F(x)/\partial x$ is monotonic in $a$? Edits: As correctly pointed out, indeed, what I meant is $F(x)$ continuous on $[0,1]$ and $F'(x)$ continous on $(0,1)$. Thank you all for the wonderful help!",,"['calculus', 'analysis']"
13,Is it possible / allowed to use L'Hôpitals rule for products?,Is it possible / allowed to use L'Hôpitals rule for products?,,"In our readings, we had L'Hôpitals rule and defined it like that: $\lim_{x\rightarrow x_{0}}\frac{f'(x)}{g'(x)}$ Because we had it in our readings, we are allowed to use this to find limit of functions. Now my question is, is it possible to use this rule for products? If yes, do you think I would be allowed to do it (since we have dicussed this rule in our reading...)? Actually, a fraction is a product at the same time, isn't it? Because we can also write: $\lim_{x\rightarrow x_{0}}f'(x)*\frac{1}{g'(x)}$ and it would be called product, or am I totally wrong here? How would you use L'Hôpitals rule for products? Possible at all? I could imagine it has something to do with fraction and reciprocal. But not sure about that.","In our readings, we had L'Hôpitals rule and defined it like that: $\lim_{x\rightarrow x_{0}}\frac{f'(x)}{g'(x)}$ Because we had it in our readings, we are allowed to use this to find limit of functions. Now my question is, is it possible to use this rule for products? If yes, do you think I would be allowed to do it (since we have dicussed this rule in our reading...)? Actually, a fraction is a product at the same time, isn't it? Because we can also write: $\lim_{x\rightarrow x_{0}}f'(x)*\frac{1}{g'(x)}$ and it would be called product, or am I totally wrong here? How would you use L'Hôpitals rule for products? Possible at all? I could imagine it has something to do with fraction and reciprocal. But not sure about that.",,"['calculus', 'analysis', 'functions']"
14,Inequality from Analysis Qual [closed],Inequality from Analysis Qual [closed],,"Closed. This question is off-topic . It is not currently accepting answers. This question is missing context or other details : Please improve the question by providing additional context, which ideally includes your thoughts on the problem and any attempts you have made to solve it. This information helps others identify where you have difficulties and helps them write answers appropriate to your experience level. Closed 8 years ago . Improve this question Let $\{a_j\}_{j=1}^N$ be a finite set of positive real numbers. Suppose $$\sum_{j=1}^{N} a_j = A,$$ prove $$\sum_{j=1}^{N} \frac{1}{a_j} \geq \frac{N^2}{A}.$$ Hints on how to proceed?","Closed. This question is off-topic . It is not currently accepting answers. This question is missing context or other details : Please improve the question by providing additional context, which ideally includes your thoughts on the problem and any attempts you have made to solve it. This information helps others identify where you have difficulties and helps them write answers appropriate to your experience level. Closed 8 years ago . Improve this question Let $\{a_j\}_{j=1}^N$ be a finite set of positive real numbers. Suppose $$\sum_{j=1}^{N} a_j = A,$$ prove $$\sum_{j=1}^{N} \frac{1}{a_j} \geq \frac{N^2}{A}.$$ Hints on how to proceed?",,"['real-analysis', 'analysis', 'measure-theory', 'inequality']"
15,If $ \lim_{h\to 0}\frac{f(x+h)-f(x-h)}{h}$ exists then $f$ is differentiable,If  exists then  is differentiable, \lim_{h\to 0}\frac{f(x+h)-f(x-h)}{h} f,"TRUE or FALSE : If $f:\mathbb R\to \mathbb R$ be such that $\displaystyle \lim_{h\to 0}\frac{f(x+h)-f(x-h)}{h}$ exists for all $x\in \mathbb R$ then $f$ is differentiable. Let , $l=\displaystyle \lim_{h\to 0}\frac{f(x+h)-f(x-h)}{h}=\lim_{h\to 0}\frac{f(x+h)-f(x)}{h}+\lim_{h\to 0}\frac{f(x)-f(x-h)}{h}=f'(x)+\lim_{h\to 0}\frac{f(x)-f(x-h)}{h}$. Then ..??","TRUE or FALSE : If $f:\mathbb R\to \mathbb R$ be such that $\displaystyle \lim_{h\to 0}\frac{f(x+h)-f(x-h)}{h}$ exists for all $x\in \mathbb R$ then $f$ is differentiable. Let , $l=\displaystyle \lim_{h\to 0}\frac{f(x+h)-f(x-h)}{h}=\lim_{h\to 0}\frac{f(x+h)-f(x)}{h}+\lim_{h\to 0}\frac{f(x)-f(x-h)}{h}=f'(x)+\lim_{h\to 0}\frac{f(x)-f(x-h)}{h}$. Then ..??",,['calculus']
16,How to show a set is closed.,How to show a set is closed.,,"Show $A = \{f \in C([0,1], R) \ | \max f(x) \le 1 \}$ is closed. I tried to show $A$ is compact, which is not compact; and I tried to show by definition of closed set but failed. I am thinking of showing the limit is exist in the set but I am not sure how to show the limit exist first and then show it is in the set.","Show $A = \{f \in C([0,1], R) \ | \max f(x) \le 1 \}$ is closed. I tried to show $A$ is compact, which is not compact; and I tried to show by definition of closed set but failed. I am thinking of showing the limit is exist in the set but I am not sure how to show the limit exist first and then show it is in the set.",,"['real-analysis', 'analysis']"
17,Is inverse use of mean value theorem right?,Is inverse use of mean value theorem right?,,"If we have $f$ is differentiable on $(a,b)$, and continuous on $[a,b]$, then for any $x\in (a,b)$, exists $y, z \in [a,b]$, such that $f '(x)=\dfrac{f(z)-f(y)}{z-y}$ Is this right?","If we have $f$ is differentiable on $(a,b)$, and continuous on $[a,b]$, then for any $x\in (a,b)$, exists $y, z \in [a,b]$, such that $f '(x)=\dfrac{f(z)-f(y)}{z-y}$ Is this right?",,"['analysis', 'derivatives']"
18,A better approximation for $2(p+1)x^p - px - 2 = 0$,A better approximation for,2(p+1)x^p - px - 2 = 0,"I would like to approximate the positive root of the following equation $$ 2(p+1)x^p - px - 2 = 0 $$  where $p$ is an integer. We could use the formula $(1 - y)^p \approx 1 - py$ for $y$ small to get an approximation of root $x_0 \approx \frac{1}{2p+1}$. However, I believe that we can make a stronger approximation. Could you please suggest some ideas?","I would like to approximate the positive root of the following equation $$ 2(p+1)x^p - px - 2 = 0 $$  where $p$ is an integer. We could use the formula $(1 - y)^p \approx 1 - py$ for $y$ small to get an approximation of root $x_0 \approx \frac{1}{2p+1}$. However, I believe that we can make a stronger approximation. Could you please suggest some ideas?",,"['analysis', 'roots']"
19,Why discrete set must be countable?,Why discrete set must be countable?,,"I want to show a set, which every point of it is an isolated point. Then this set must be countable. How to show it? I find this from Wikipedia's article Isolated point , but I don't understand: A set that is made up only of isolated points is called a discrete set (see also discrete space). Any discrete subset S of Euclidean space must be countable, since the isolation of each of its points together with the fact that rationals are dense in the reals means that the points of S may be mapped into a set of points with rational coordinates, of which there are only countably many.","I want to show a set, which every point of it is an isolated point. Then this set must be countable. How to show it? I find this from Wikipedia's article Isolated point , but I don't understand: A set that is made up only of isolated points is called a discrete set (see also discrete space). Any discrete subset S of Euclidean space must be countable, since the isolation of each of its points together with the fact that rationals are dense in the reals means that the points of S may be mapped into a set of points with rational coordinates, of which there are only countably many.",,"['real-analysis', 'analysis']"
20,Are all open sets open intervals?,Are all open sets open intervals?,,"Are all open sets open intervals? If not, provide an example. I think that all open intervals are open sets but not the other way around, but couldn't find an example.","Are all open sets open intervals? If not, provide an example. I think that all open intervals are open sets but not the other way around, but couldn't find an example.",,"['real-analysis', 'calculus']"
21,"Prove that if $A$ has a maximum, then the maximum must be the supremum","Prove that if  has a maximum, then the maximum must be the supremum",A,"Prove that given $A\subseteq\mathbb{R}$, if has a maximum $a_0$, then $a_0$ must be the supremum. In order to prove this, I have written the following: Let $A\subseteq \mathbb{R}$, nonempty set because $a_{0}$ $\in A$ for hypotesis. Then, since $a_{0}$ $\in A$ is the largest element of the set $A$   (i.e. for each element $A$ of A we have that $ a\leq a_{0}$) $A$ is bounded above. Then, A has a supremum. Let $s$ be the least upper bound of $A,$ so we have that for each element $a$ of $A$, $a\leq s$, for being $s$ an upper bound. In particular, $a_0\leq s$, which implies that $s=a_0$. Then, we have that $\sup(A)=\max(A)=a_0$, which is exactly what we wanted to prove. Observation: this proof is true if $s \in A$ I'm not sure if my proof is correct. I will be thankful if someone tells me.","Prove that given $A\subseteq\mathbb{R}$, if has a maximum $a_0$, then $a_0$ must be the supremum. In order to prove this, I have written the following: Let $A\subseteq \mathbb{R}$, nonempty set because $a_{0}$ $\in A$ for hypotesis. Then, since $a_{0}$ $\in A$ is the largest element of the set $A$   (i.e. for each element $A$ of A we have that $ a\leq a_{0}$) $A$ is bounded above. Then, A has a supremum. Let $s$ be the least upper bound of $A,$ so we have that for each element $a$ of $A$, $a\leq s$, for being $s$ an upper bound. In particular, $a_0\leq s$, which implies that $s=a_0$. Then, we have that $\sup(A)=\max(A)=a_0$, which is exactly what we wanted to prove. Observation: this proof is true if $s \in A$ I'm not sure if my proof is correct. I will be thankful if someone tells me.",,"['real-analysis', 'analysis', 'proof-verification', 'supremum-and-infimum']"
22,How to prove $\sin (x)+ \sin(y) = 2 \sin \left(\frac{x+y}{2}\right) \cos\left(\frac{x-y}{2}\right)$ using addition theorems?,How to prove  using addition theorems?,\sin (x)+ \sin(y) = 2 \sin \left(\frac{x+y}{2}\right) \cos\left(\frac{x-y}{2}\right),"I am stuck at one task, it's to proof the following equation using addition theorems. From a draft I understood that the equation is correct or true. But that's it. What's a good way to explain it mathematically? $$\sin (x)+ \sin(y) = 2 \sin \left(\frac{x+y}{2}\right) \cos\left(\frac{x-y}{2}\right)$$ As usual, any help is upvoted immediately.","I am stuck at one task, it's to proof the following equation using addition theorems. From a draft I understood that the equation is correct or true. But that's it. What's a good way to explain it mathematically? $$\sin (x)+ \sin(y) = 2 \sin \left(\frac{x+y}{2}\right) \cos\left(\frac{x-y}{2}\right)$$ As usual, any help is upvoted immediately.",,"['analysis', 'trigonometry']"
23,Short proof for $1-x\leq \exp(-x)$,Short proof for,1-x\leq \exp(-x),"I'm trying to find a short way to show that: $$ 1-x\leq \exp(-x),\forall x\in\mathbb{R} $$ Does anyone know a really short solution for this problem?","I'm trying to find a short way to show that: $$ 1-x\leq \exp(-x),\forall x\in\mathbb{R} $$ Does anyone know a really short solution for this problem?",,"['calculus', 'real-analysis', 'analysis', 'inequality']"
24,"Find $\lim_{x\to\infty}\frac{f^{-1}(x)}{\ln(x)}$, where $f(x)=e^x+x^3-x^2+x$, without L'Hospital","Find , where , without L'Hospital",\lim_{x\to\infty}\frac{f^{-1}(x)}{\ln(x)} f(x)=e^x+x^3-x^2+x,"We have $f:\mathbb{R}\to\mathbb{R},f(x)=e^x+x^3-x^2+x$ and we need to evaluate $$\lim_{x\to\infty}\frac{f^{-1}(x)}{\ln(x)}$$ There is an elegant way to solve this problem ? Here is all my steps: My first ideea was to use squeeze theorem such that: $$\alpha\leq f^{-1}(x)\leq\beta$$ $f(\ln(x))=x+\ln^3(x)-\ln^2(x)+ln(x)\ge x\Rightarrow\beta=\ln(x),\forall x>1$ How can I find $\alpha$ such that $f( ? )\leq x,\forall x\in V$ where $V\subset\mathbb{R}$ such that $(\exists)$ an open interval I such that $x\in I\subset V$ ? P.S. : I know the method with L'Hospital's rule and absolutely is the easiest way to solve my problem, but I don't consider an ""elegant way"". I forgot to say that. What I want to prove in my demonstration is that $\frac{\alpha}{\ln(x)}\leq\frac{f^{-1}(x)}{\ln(x)}\leq\frac{\beta}{\ln(x)}$ and if we see the upper bound $\frac{\beta}{\ln(x)}\to 1$ as $x\to\infty$ So what I have to do is to find $\alpha$ such that $\frac{\alpha}{\ln(x)}\to 1$","We have and we need to evaluate There is an elegant way to solve this problem ? Here is all my steps: My first ideea was to use squeeze theorem such that: How can I find such that where such that an open interval I such that ? P.S. : I know the method with L'Hospital's rule and absolutely is the easiest way to solve my problem, but I don't consider an ""elegant way"". I forgot to say that. What I want to prove in my demonstration is that and if we see the upper bound as So what I have to do is to find such that","f:\mathbb{R}\to\mathbb{R},f(x)=e^x+x^3-x^2+x \lim_{x\to\infty}\frac{f^{-1}(x)}{\ln(x)} \alpha\leq f^{-1}(x)\leq\beta f(\ln(x))=x+\ln^3(x)-\ln^2(x)+ln(x)\ge x\Rightarrow\beta=\ln(x),\forall x>1 \alpha f( ? )\leq x,\forall x\in V V\subset\mathbb{R} (\exists) x\in I\subset V \frac{\alpha}{\ln(x)}\leq\frac{f^{-1}(x)}{\ln(x)}\leq\frac{\beta}{\ln(x)} \frac{\beta}{\ln(x)}\to 1 x\to\infty \alpha \frac{\alpha}{\ln(x)}\to 1","['calculus', 'real-analysis', 'analysis']"
25,Why is there a 'missing' $1$ in the Euler–Mascheroni constant?,Why is there a 'missing'  in the Euler–Mascheroni constant?,1,"It is easy to show that : $$ \sum_{k=1}^n \frac{1}{k} > \ln(n+1), $$ but the Euler–Mascheroni constant is defined as: $$ \gamma = \lim_{n \to \infty} \left(  \sum_{k=1}^n \frac{1}{k} - \ln(n) \right). $$ My question is, why was $\gamma$ defined using $\ln(n)$ and not  $\ln(n+1$)? Are the two definitions identical, or does it simply turn out to be more convenient for other applications to define $\gamma$ using $\ln(n)$?","It is easy to show that : $$ \sum_{k=1}^n \frac{1}{k} > \ln(n+1), $$ but the Euler–Mascheroni constant is defined as: $$ \gamma = \lim_{n \to \infty} \left(  \sum_{k=1}^n \frac{1}{k} - \ln(n) \right). $$ My question is, why was $\gamma$ defined using $\ln(n)$ and not  $\ln(n+1$)? Are the two definitions identical, or does it simply turn out to be more convenient for other applications to define $\gamma$ using $\ln(n)$?",,"['real-analysis', 'analysis', 'elementary-number-theory', 'definition', 'analytic-number-theory']"
26,"Prove that $\exp(x)=3x$ has at least one solution for $x\in [0,1]$",Prove that  has at least one solution for,"\exp(x)=3x x\in [0,1]","Prove that $\exp(x)=3x$ has at least one solution $x \in [0,1]$. $$e^x=3x$$ $$\Leftrightarrow e^x-3x=0$$ Let $$f(x) = e^x - 3x$$ $$f(0)=e^0 - 3 \cdot 0 = 1 > 0$$ $$f(1)= e^1-3 \cdot 1 = e - 3 < 0$$ Thus, since $f(1) < 0 < f(0) $, by the IVT:  $$\exists \zeta \in [0,1]\text{ such that }f(\zeta)=0 \Leftrightarrow e^\zeta = 3\zeta$$ $\Box$ Is that correct? Is there something I can improve?","Prove that $\exp(x)=3x$ has at least one solution $x \in [0,1]$. $$e^x=3x$$ $$\Leftrightarrow e^x-3x=0$$ Let $$f(x) = e^x - 3x$$ $$f(0)=e^0 - 3 \cdot 0 = 1 > 0$$ $$f(1)= e^1-3 \cdot 1 = e - 3 < 0$$ Thus, since $f(1) < 0 < f(0) $, by the IVT:  $$\exists \zeta \in [0,1]\text{ such that }f(\zeta)=0 \Leftrightarrow e^\zeta = 3\zeta$$ $\Box$ Is that correct? Is there something I can improve?",,"['real-analysis', 'analysis']"
27,Uniform Continuity of $1/x^3$ on compact intervals.,Uniform Continuity of  on compact intervals.,1/x^3,"So, I have the function $f : (0,\infty) \to \Bbb R$. With $f(x) = \frac{1}{x^2}$ and I have to show that $f(x)$ is uniformly continuous on the set $[1,\infty)$. Is the following proof correct? $[1,\infty) = \cup_{n=1}^{\infty} [n,n+1]$. And each of those sets is compact, and $f(x)$ is continuous so by Heine-Cantor it is UC on every compact set, and thus it is UC on the whole union of them. Is this coherent/consistent?","So, I have the function $f : (0,\infty) \to \Bbb R$. With $f(x) = \frac{1}{x^2}$ and I have to show that $f(x)$ is uniformly continuous on the set $[1,\infty)$. Is the following proof correct? $[1,\infty) = \cup_{n=1}^{\infty} [n,n+1]$. And each of those sets is compact, and $f(x)$ is continuous so by Heine-Cantor it is UC on every compact set, and thus it is UC on the whole union of them. Is this coherent/consistent?",,"['real-analysis', 'analysis']"
28,Show that the set is closed,Show that the set is closed,,"Let $(E, d)$ be a metric space, $x$ element of $E$. Show that the set \begin{equation}  A = \{y \in\ E : d(x, y) \geq 5 \} \end{equation} is closed. Generally, how would you go about this? I have an exam soon and I want to learn this.","Let $(E, d)$ be a metric space, $x$ element of $E$. Show that the set \begin{equation}  A = \{y \in\ E : d(x, y) \geq 5 \} \end{equation} is closed. Generally, how would you go about this? I have an exam soon and I want to learn this.",,"['analysis', 'metric-spaces']"
29,"Can any continous,bounded function have a fourier series?","Can any continous,bounded function have a fourier series?",,"In particular, can an oscillatory function with some decay term ( i.e $e^{-t} \cos(kt)$) have a fourier series representation? All the articles I read said that the function has to be periodic,but this one is not.","In particular, can an oscillatory function with some decay term ( i.e $e^{-t} \cos(kt)$) have a fourier series representation? All the articles I read said that the function has to be periodic,but this one is not.",,"['analysis', 'fourier-series']"
30,definition of the exterior derivative,definition of the exterior derivative,,"I have a question concerning the definition of $d^*$. It is usually defined to be the (formally) adjoint of $d$? what is the meaning of formally?, is not just the adjoint of $d$? thanks","I have a question concerning the definition of $d^*$. It is usually defined to be the (formally) adjoint of $d$? what is the meaning of formally?, is not just the adjoint of $d$? thanks",,"['analysis', 'differential-geometry']"
31,Different ways to represent functions other than Laurent and Fourier series?,Different ways to represent functions other than Laurent and Fourier series?,,"In the book ""A Course of modern analysis"", examples of expanding functions in terms of inverse factorials was given, I am not sure in today's math what subject would that come under but besides the followings : power series ( Taylor Series, Laurent Series ), expansions in terms of theta functions, expanding a function in terms of another function (powers of, inverse factorial etc.), Fourier series, infinite Products (Complex analysis) and partial fractions (Weisenstein series), what other ways of representing functions have been studied? is there a comprehensive list of representation of functions  and the motivation behind each method? For example , power series are relatively easy to work with and establish the domain of convergence e.g. for $ \sin , e^x \text {etc.}$ but infinite product representation makes it trivial to see all the zeroes of $\sin, \cos \text etc. $ Also if anyone can point out the subject that they are studied under would be great. Thank you","In the book ""A Course of modern analysis"", examples of expanding functions in terms of inverse factorials was given, I am not sure in today's math what subject would that come under but besides the followings : power series ( Taylor Series, Laurent Series ), expansions in terms of theta functions, expanding a function in terms of another function (powers of, inverse factorial etc.), Fourier series, infinite Products (Complex analysis) and partial fractions (Weisenstein series), what other ways of representing functions have been studied? is there a comprehensive list of representation of functions  and the motivation behind each method? For example , power series are relatively easy to work with and establish the domain of convergence e.g. for $ \sin , e^x \text {etc.}$ but infinite product representation makes it trivial to see all the zeroes of $\sin, \cos \text etc. $ Also if anyone can point out the subject that they are studied under would be great. Thank you",,['analysis']
32,Is there a proof that there is no general method to solve transcendental equations?,Is there a proof that there is no general method to solve transcendental equations?,,"Being motivated by this post , I was wondering if there is a proof (analogous to the case of Diophantine equations) that there is no general method for solving transcendental equations ?  It seems pretty clear, intuitively, that there can be no general method; but the only reason I feel strongly about that is because I can't conceive that transcendental equations have a general method, but Diophantine equations don't.  I was never able to understand the proof for the case of Diophantine equations, so I am not in a position to even know where to begin thinking about this.  Has any work been done on this problem?","Being motivated by this post , I was wondering if there is a proof (analogous to the case of Diophantine equations) that there is no general method for solving transcendental equations ?  It seems pretty clear, intuitively, that there can be no general method; but the only reason I feel strongly about that is because I can't conceive that transcendental equations have a general method, but Diophantine equations don't.  I was never able to understand the proof for the case of Diophantine equations, so I am not in a position to even know where to begin thinking about this.  Has any work been done on this problem?",,"['analysis', 'transcendence-theory']"
33,"Visual intuition for the definition of ""asymptotically equivalent""","Visual intuition for the definition of ""asymptotically equivalent""",,"I'm trying to intuitively grasp the following definition: The real-valued functions $f$ and $g$ are asymptotically equivalent as $x \to \infty$ if $$\lim_{x \to \infty} \dfrac{f(x)}{g(x)}=1.$$ We write this as $f \sim g$ . My question is: how do we visually interpret this in terms of the graphs of $f$ and $g$ ? Does this mean that the graphs of $f$ and $g$ get closer to each other as $x$ gets larger and larger? My only intuition for this comes from the following example: we know that $\sin x \sim x$ as $x \to 0$ (since $\lim_{x \to 0} (\sin x)/x = 1$ ). And as we can see below, the graphs of $\sin x$ (the green line) and $x$ (the black line) get closer and closer as $x$ goes to $0$ . But this intuition does not seem to hold for functions asymptotically equivalent at $\infty$ . I graphed $x^2 + x$ (black line) and $x^2$ (green line) and their graphs do not appear to be getting closer at all! In fact, it looks like there's a ""gap"" between the two graphs. This leads me to believe that I'm not interpreting ""asymptotically equivalent"" in the right way. I've come across the idea that $f \sim g$ means that $f$ and $g$ have the ""same rate of growth"", but that feels very unintuitive for me. Is there are a way to see that in the graphs? Any guidance would be greatly appreciated! Thanks.","I'm trying to intuitively grasp the following definition: The real-valued functions and are asymptotically equivalent as if We write this as . My question is: how do we visually interpret this in terms of the graphs of and ? Does this mean that the graphs of and get closer to each other as gets larger and larger? My only intuition for this comes from the following example: we know that as (since ). And as we can see below, the graphs of (the green line) and (the black line) get closer and closer as goes to . But this intuition does not seem to hold for functions asymptotically equivalent at . I graphed (black line) and (green line) and their graphs do not appear to be getting closer at all! In fact, it looks like there's a ""gap"" between the two graphs. This leads me to believe that I'm not interpreting ""asymptotically equivalent"" in the right way. I've come across the idea that means that and have the ""same rate of growth"", but that feels very unintuitive for me. Is there are a way to see that in the graphs? Any guidance would be greatly appreciated! Thanks.",f g x \to \infty \lim_{x \to \infty} \dfrac{f(x)}{g(x)}=1. f \sim g f g f g x \sin x \sim x x \to 0 \lim_{x \to 0} (\sin x)/x = 1 \sin x x x 0 \infty x^2 + x x^2 f \sim g f g,"['real-analysis', 'analysis', 'asymptotics']"
34,Linearity of the directional derivative,Linearity of the directional derivative,,"I'm confused by the highlighted remark in the text quoted below. Isn't $df_x$ linear by the very definition? At least Rudin and Apostol define derivative to be a linear map. Actually, as I write this question I perhaps came up with an answer, but since I'm not sure whether I'm right, let me state my version here and see whether it is correct. I think what is meant in the text below is that their definition of the derivative at $x$ evaluated at $h$ does not assume linearity. Whereas the definitions of Rudin and Apostol assume linearity, and then they (at least Apostol) prove (see Theorem 12.3 here Expressing directional derivative in terms of partial derivatives ) that the derivative of $f$ at $x$ evaluated at $h$, $df_x(h)$, actually equals the directional derivative of $f$ at $x$ in the direction of $h$. Am I interpreting what is going on correctly? Also, if we assume the definition given in the text below, how to prove that $df_x$ is linear? I don't see how it follows after playing with the quotient inside the $\lim$ sign. (I don't have Spivak at hand.)","I'm confused by the highlighted remark in the text quoted below. Isn't $df_x$ linear by the very definition? At least Rudin and Apostol define derivative to be a linear map. Actually, as I write this question I perhaps came up with an answer, but since I'm not sure whether I'm right, let me state my version here and see whether it is correct. I think what is meant in the text below is that their definition of the derivative at $x$ evaluated at $h$ does not assume linearity. Whereas the definitions of Rudin and Apostol assume linearity, and then they (at least Apostol) prove (see Theorem 12.3 here Expressing directional derivative in terms of partial derivatives ) that the derivative of $f$ at $x$ evaluated at $h$, $df_x(h)$, actually equals the directional derivative of $f$ at $x$ in the direction of $h$. Am I interpreting what is going on correctly? Also, if we assume the definition given in the text below, how to prove that $df_x$ is linear? I don't see how it follows after playing with the quotient inside the $\lim$ sign. (I don't have Spivak at hand.)",,"['real-analysis', 'analysis', 'multivariable-calculus', 'derivatives']"
35,Proof by induction: $2^{n} < n!$ [duplicate],Proof by induction:  [duplicate],2^{n} < n!,"This question already has answers here : Prove that: $2^n < n!$ Using Induction [duplicate] (3 answers) Closed 7 years ago . Prove that $2^{n} < n!$ $\forall$ n > 4 $n=5:$ $$2^{5}<5!$$ $$32 < 120$$ This is true. Now, after knowing it worked for $n$ we need to show it works for every other, so $n+1$: $$2^{n+1} < (n+1)!$$ $$2^{n} < \frac{(n+1)!}{2}$$ We know from beginning that $2^{n} < n!$ So we replace it with it here and then show: $$n! < \frac{(n+1)!}{2}$$ $$n! < \frac{n!\cdot(n+1)}{2}$$ $$1<\frac{n+1}{2}$$ Task say for all $n > 4$ so the thing on the right side will really be greater than $1$. I hope everything is ok? Edit: The possible-duplicate-link didn't help me because I'm not really looking for a solution to the task. I'm rather interested in knowing if MY proof is correct.","This question already has answers here : Prove that: $2^n < n!$ Using Induction [duplicate] (3 answers) Closed 7 years ago . Prove that $2^{n} < n!$ $\forall$ n > 4 $n=5:$ $$2^{5}<5!$$ $$32 < 120$$ This is true. Now, after knowing it worked for $n$ we need to show it works for every other, so $n+1$: $$2^{n+1} < (n+1)!$$ $$2^{n} < \frac{(n+1)!}{2}$$ We know from beginning that $2^{n} < n!$ So we replace it with it here and then show: $$n! < \frac{(n+1)!}{2}$$ $$n! < \frac{n!\cdot(n+1)}{2}$$ $$1<\frac{n+1}{2}$$ Task say for all $n > 4$ so the thing on the right side will really be greater than $1$. I hope everything is ok? Edit: The possible-duplicate-link didn't help me because I'm not really looking for a solution to the task. I'm rather interested in knowing if MY proof is correct.",,"['calculus', 'analysis', 'inequality', 'solution-verification', 'induction']"
36,"If the derivative of a function is zero, is the function a constant function?","If the derivative of a function is zero, is the function a constant function?",,"If the derivative of a function is zero, is the function then  a constant function ? I think it is not true, because if f in the sub interval be constant function then derivative of $f$ is zero, is it true?","If the derivative of a function is zero, is the function then  a constant function ? I think it is not true, because if f in the sub interval be constant function then derivative of $f$ is zero, is it true?",,"['real-analysis', 'analysis']"
37,"Convergence in $L^1$ norm, but not point-wise a.e.","Convergence in  norm, but not point-wise a.e.",L^1,"As part of a course assignment, I'm asked to find a sequence of functions that converges in $L^1(\Omega \subset \mathbb{R})$, yet does not converge point-wise a.e. My thought is that such a sequence would have to be some sort of sliding function that moves along, say, $[0,1]$, so that each point has a constant height infinitely often. I've tried to illustrate what I mean below: Iteratively, Though, I'm not quite certain if this is the right idea. I can't seem to think of anything else that would behave in this manner, converging to $0$ in integral, but not point-wise. My issue currently, if this is the correct idea, is that I have absolutely no idea how to explicitly write such functions down. The one's I have depicted would be some sort of shifting of the bump function involving $e^x$, yet I don't know what shifts would be correct, along with what scaling.","As part of a course assignment, I'm asked to find a sequence of functions that converges in $L^1(\Omega \subset \mathbb{R})$, yet does not converge point-wise a.e. My thought is that such a sequence would have to be some sort of sliding function that moves along, say, $[0,1]$, so that each point has a constant height infinitely often. I've tried to illustrate what I mean below: Iteratively, Though, I'm not quite certain if this is the right idea. I can't seem to think of anything else that would behave in this manner, converging to $0$ in integral, but not point-wise. My issue currently, if this is the correct idea, is that I have absolutely no idea how to explicitly write such functions down. The one's I have depicted would be some sort of shifting of the bump function involving $e^x$, yet I don't know what shifts would be correct, along with what scaling.",,"['real-analysis', 'analysis', 'measure-theory', 'examples-counterexamples', 'lp-spaces']"
38,Prove that $f $ is constant,Prove that  is constant,f ,"Let $f:\mathbb R \to \mathbb R $ be a continuous function such that for all $x \in \mathbb R$,  $f(x)=f(x^2) $ prove that $f$ is constant. ""please give me hints not answer. thanks a lot. :):):):):)""","Let $f:\mathbb R \to \mathbb R $ be a continuous function such that for all $x \in \mathbb R$,  $f(x)=f(x^2) $ prove that $f$ is constant. ""please give me hints not answer. thanks a lot. :):):):):)""",,"['real-analysis', 'analysis', 'continuity']"
39,Sequence that converges to 0 but its function diverges,Sequence that converges to 0 but its function diverges,,Let f(x)=-2x+1 if x<0 and f(x)=$x^2$+x if x>0. Give a sequence {$x_n$} in $\mathbb R$\ {0} such that {$x_n$} converges to zero but {f($x_n$)} diverges. I've tried with the sequence {1/n} but it does not seem to work. I can't think of another sequence that would converge to zero. Can anyone help me out please? Thanks.,Let f(x)=-2x+1 if x<0 and f(x)=$x^2$+x if x>0. Give a sequence {$x_n$} in $\mathbb R$\ {0} such that {$x_n$} converges to zero but {f($x_n$)} diverges. I've tried with the sequence {1/n} but it does not seem to work. I can't think of another sequence that would converge to zero. Can anyone help me out please? Thanks.,,"['real-analysis', 'analysis']"
40,Derivative of matrix inversion function?,Derivative of matrix inversion function?,,Let's say I have a function $f$ which maps any invertible $n\times n$ matrix to its inverse. How do I calculate the derivative of this function?,Let's say I have a function $f$ which maps any invertible $n\times n$ matrix to its inverse. How do I calculate the derivative of this function?,,['analysis']
41,"Suppose that $f \in C[0,1]$ . Show that $\lim_{n\to \infty}\int_{0}^{1}(n+1)x^{n}f(x)dx=f(1)$",Suppose that  . Show that,"f \in C[0,1] \lim_{n\to \infty}\int_{0}^{1}(n+1)x^{n}f(x)dx=f(1)","Suppose that $f \in C[0,1]$ . Show that $\lim_{n\to \infty}\int_{0}^{1}(n+1)x^{n}f(x)dx=f(1)$. This is how I proceeded: Suppose $x^{n+1}=t$ . Then $(n+1)x^ndx=dt$. Now the integral becomes $$\int_{0}^{1}dtf(t^{\frac{1}{n+1}})$$. Now If i take $g_n(t)=f(t^{\frac{1}{n+1}})$, then $\lim_{n\to \infty}g_{n}(t)=f(1)$. Now I know that $g_{n}(t)$ $\to$ $f(1)$ pointwise . All i need to show is that the convergence is uniform and then I am through. How do i show that the convergence is uniform?? Thanks for the help!!","Suppose that $f \in C[0,1]$ . Show that $\lim_{n\to \infty}\int_{0}^{1}(n+1)x^{n}f(x)dx=f(1)$. This is how I proceeded: Suppose $x^{n+1}=t$ . Then $(n+1)x^ndx=dt$. Now the integral becomes $$\int_{0}^{1}dtf(t^{\frac{1}{n+1}})$$. Now If i take $g_n(t)=f(t^{\frac{1}{n+1}})$, then $\lim_{n\to \infty}g_{n}(t)=f(1)$. Now I know that $g_{n}(t)$ $\to$ $f(1)$ pointwise . All i need to show is that the convergence is uniform and then I am through. How do i show that the convergence is uniform?? Thanks for the help!!",,"['real-analysis', 'analysis', 'convergence-divergence']"
42,Prove that the logarithmic mean is less than the power mean.,Prove that the logarithmic mean is less than the power mean.,,"Prove that the logarithmic mean is less than the power mean. $$L(a,b)=\frac{a-b}{\ln(a)-\ln(b)} < M_p(a,b) = \left(\frac{a^p+b^p}{2}\right)^{\frac{1}{p}}$$ such that $$p\geq \frac{1}{3}$$ That is the $\frac{1}{p}$ root of the power mean.","Prove that the logarithmic mean is less than the power mean. $$L(a,b)=\frac{a-b}{\ln(a)-\ln(b)} < M_p(a,b) = \left(\frac{a^p+b^p}{2}\right)^{\frac{1}{p}}$$ such that $$p\geq \frac{1}{3}$$ That is the $\frac{1}{p}$ root of the power mean.",,"['analysis', 'inequality']"
43,Solution of a partial differential equation.,Solution of a partial differential equation.,,"Find $u=u(x,y)$ satisfying $$\dfrac{\partial^2 u}{\partial x^2} = 6xy, \,\,\,u(0,y) = y, \,\,\,\dfrac{\partial u}{\partial x}(1,y)=0.$$ I have tried by laplace transformation $$\displaystyle s^2\bar{u}(s,y)-su(0,y)-\frac{\partial u}{\partial x}(0,y) = \frac{6y}{s^2} \,\,\,\Longrightarrow\,\,\, \bar{u}(s,y)=sy +\frac{ 6y}{s^2}+\frac{\partial u}{\partial x}(0,y).$$ Please tell me how to proceed.","Find $u=u(x,y)$ satisfying $$\dfrac{\partial^2 u}{\partial x^2} = 6xy, \,\,\,u(0,y) = y, \,\,\,\dfrac{\partial u}{\partial x}(1,y)=0.$$ I have tried by laplace transformation $$\displaystyle s^2\bar{u}(s,y)-su(0,y)-\frac{\partial u}{\partial x}(0,y) = \frac{6y}{s^2} \,\,\,\Longrightarrow\,\,\, \bar{u}(s,y)=sy +\frac{ 6y}{s^2}+\frac{\partial u}{\partial x}(0,y).$$ Please tell me how to proceed.",,"['calculus', 'analysis', 'multivariable-calculus', 'partial-differential-equations', 'partial-derivative']"
44,"If $\lim_{h\to 0} \frac{f(x_0 + h) - f(x_0 - h)}{2h} = f'(x_0)$ exists, is f differentiable at $x_0$?","If  exists, is f differentiable at ?",\lim_{h\to 0} \frac{f(x_0 + h) - f(x_0 - h)}{2h} = f'(x_0) x_0,"I'm not too sure where to start. Any help will be appreciated. If $\lim_{h\to 0} \frac{f(x_0 + h) - f(x_0 - h)}{2h} = f'(x_0)$ exists, is f differentiable at $x_0$?","I'm not too sure where to start. Any help will be appreciated. If $\lim_{h\to 0} \frac{f(x_0 + h) - f(x_0 - h)}{2h} = f'(x_0)$ exists, is f differentiable at $x_0$?",,"['real-analysis', 'analysis', 'derivatives']"
45,Continuous function over the reals which maps closed sets to a not closed set,Continuous function over the reals which maps closed sets to a not closed set,,"My main question here may be notation but here is the actual problem: ""Give an example of a function $f : \mathbb{R} \rightarrow \mathbb{R}$ which is continuous and a set $E \subset \mathbb{R}$ closed but $f(E)$ is not closed"" Here are my issues: 1) I'm assuming the function has to be continuous over all of the reals so no sets that blow up are allowed. 2) This is the big one for me. I'm not sure if it was intended to be a proper subset or subset with possible equality. Because $e^x$ would work  since the Reals are technically closed, also open but logically I don't think I'm cheating, and then the image would be the open set $(0,\infty)$. Thoughts? And if you think it is proper does anyone have a good function that is continuous over the reals and maps a proper closed subset to one that isn't closed?","My main question here may be notation but here is the actual problem: ""Give an example of a function $f : \mathbb{R} \rightarrow \mathbb{R}$ which is continuous and a set $E \subset \mathbb{R}$ closed but $f(E)$ is not closed"" Here are my issues: 1) I'm assuming the function has to be continuous over all of the reals so no sets that blow up are allowed. 2) This is the big one for me. I'm not sure if it was intended to be a proper subset or subset with possible equality. Because $e^x$ would work  since the Reals are technically closed, also open but logically I don't think I'm cheating, and then the image would be the open set $(0,\infty)$. Thoughts? And if you think it is proper does anyone have a good function that is continuous over the reals and maps a proper closed subset to one that isn't closed?",,"['analysis', 'continuity']"
46,How to use the inverse function theorem?,How to use the inverse function theorem?,,"I have a function $F (x, y) = (x^2+y^2, xy)$ and I need to show that it has an inverse. How do I find the inverse of this function using the inverse function theorem? I have not learned this before in all of my math classes and I just need some brushing up on how to use it. Thank you for your assistance. edit: On the set $\{(x, y) : −x < y < x\}$ .",I have a function and I need to show that it has an inverse. How do I find the inverse of this function using the inverse function theorem? I have not learned this before in all of my math classes and I just need some brushing up on how to use it. Thank you for your assistance. edit: On the set .,"F (x, y) = (x^2+y^2, xy) \{(x, y) : −x < y < x\}","['calculus', 'real-analysis', 'analysis', 'multivariable-calculus']"
47,Do all angles occur in Hilbert spaces?,Do all angles occur in Hilbert spaces?,,"Let $X$ be a Hilbert space with scalar product $(\cdot,\cdot)$. Then for two vectors $v,w$ of norm $1$, we can interpret $(v,w)$ as an angle, so that $(v,w)=\cos(\varphi)$ for a unique angle $\varphi\in[0,\pi)$. My question is the following: Let $\varphi'\in[0,\pi)$ and $v'\in X$ with $\|v'\|=1$ (norm induced by the scalar product) be given. Is there a vector $w'\in X$ with $(v',w')=\cos(\varphi')$? Thank you in advance!","Let $X$ be a Hilbert space with scalar product $(\cdot,\cdot)$. Then for two vectors $v,w$ of norm $1$, we can interpret $(v,w)$ as an angle, so that $(v,w)=\cos(\varphi)$ for a unique angle $\varphi\in[0,\pi)$. My question is the following: Let $\varphi'\in[0,\pi)$ and $v'\in X$ with $\|v'\|=1$ (norm induced by the scalar product) be given. Is there a vector $w'\in X$ with $(v',w')=\cos(\varphi')$? Thank you in advance!",,"['analysis', 'hilbert-spaces']"
48,Laplacian in polar coordinates,Laplacian in polar coordinates,,"I am stuck with an exercise that requires me to find the Laplacian $\Delta u=(D_x^2u+D_y^2u)$ of a 2d-function $u$ in polar coordinates (in the standard Euclidean plane). I found the following article on the net, and tried to follow its logic, but I could not understand two steps: http://www.sci.brooklyn.cuny.edu/~mate/misc/laplacian_polarcoord_higherdim.pdf at first, the representation of $D_x$ and $D_y$ in terms of $r, \theta$, at the bottom of page 2: $D_y=\sin\theta D_r+\frac{\cos\theta}{r}D_\theta$ and $D_x=\cos\theta D_r-\frac{\sin\theta}{r}D_\theta$ . When I draw a sketch of the plane with a circle and all the coordinates, I get that it should be $D_y=\sin\theta D_r+r\ \cos\theta D_\theta$, because the larger the radius is, the greater will be the impact of a change in $\theta$ on a change in $y$. What am I making wrong here? And then, secondly, what looks like an easy multiplication, namely taking the square of the above terms (on the top of page 3 in the link): $D_y^2=(\sin\theta D_r+\frac{\cos\theta}{r}D_\theta)(\sin\theta D_r+\frac{\cos\theta}{r}D_\theta)$ $=\sin^2\theta D_r^2+\frac{2\sin\theta \cos\theta}{r}D_\theta D_r+\frac{\sin^2\theta}{r^2}D_\theta^2 \\ -\frac{\cos\theta}{r^2}D_\theta + \frac{\cos^2\theta}{r}D_r - \frac{\cos\theta \sin\theta}{r^2}D_\theta$ and similarly for $D_x$. I don't see from where the last three summands come, what I see is: $(a+b)(a+b)=a^2+2ab+b^2+c+d+e$ and I cannot see the context of $c,d,e.$ It would be great if you could explain those issues to me!","I am stuck with an exercise that requires me to find the Laplacian $\Delta u=(D_x^2u+D_y^2u)$ of a 2d-function $u$ in polar coordinates (in the standard Euclidean plane). I found the following article on the net, and tried to follow its logic, but I could not understand two steps: http://www.sci.brooklyn.cuny.edu/~mate/misc/laplacian_polarcoord_higherdim.pdf at first, the representation of $D_x$ and $D_y$ in terms of $r, \theta$, at the bottom of page 2: $D_y=\sin\theta D_r+\frac{\cos\theta}{r}D_\theta$ and $D_x=\cos\theta D_r-\frac{\sin\theta}{r}D_\theta$ . When I draw a sketch of the plane with a circle and all the coordinates, I get that it should be $D_y=\sin\theta D_r+r\ \cos\theta D_\theta$, because the larger the radius is, the greater will be the impact of a change in $\theta$ on a change in $y$. What am I making wrong here? And then, secondly, what looks like an easy multiplication, namely taking the square of the above terms (on the top of page 3 in the link): $D_y^2=(\sin\theta D_r+\frac{\cos\theta}{r}D_\theta)(\sin\theta D_r+\frac{\cos\theta}{r}D_\theta)$ $=\sin^2\theta D_r^2+\frac{2\sin\theta \cos\theta}{r}D_\theta D_r+\frac{\sin^2\theta}{r^2}D_\theta^2 \\ -\frac{\cos\theta}{r^2}D_\theta + \frac{\cos^2\theta}{r}D_r - \frac{\cos\theta \sin\theta}{r^2}D_\theta$ and similarly for $D_x$. I don't see from where the last three summands come, what I see is: $(a+b)(a+b)=a^2+2ab+b^2+c+d+e$ and I cannot see the context of $c,d,e.$ It would be great if you could explain those issues to me!",,"['analysis', 'polar-coordinates']"
49,$\sum_\limits{n\ge 2}\frac{1}{\log n}\sin nx$ is not the Fourier series of a Riemann integrable function.,is not the Fourier series of a Riemann integrable function.,\sum_\limits{n\ge 2}\frac{1}{\log n}\sin nx,"Show that $$\sum_\limits{n\ge 2}\frac{1}{\log n}\sin nx$$ is not the Fourier series of a Riemann integrable function. I think this comes from the low increasing speed of $\log$ function. Analogously, $$\sum_\limits{n\ge 1}\frac{1}{n^\alpha}\sin nx$$ is not the Fourier series of a Riemann integrable function when $0<\alpha<1$ . And for the boundary situation, $$\sum_\limits{n\ge 1}\frac{1}{n}\sin nx$$ is a Fourier series of a Riemann integrable function, the sawtooth function.","Show that is not the Fourier series of a Riemann integrable function. I think this comes from the low increasing speed of function. Analogously, is not the Fourier series of a Riemann integrable function when . And for the boundary situation, is a Fourier series of a Riemann integrable function, the sawtooth function.",\sum_\limits{n\ge 2}\frac{1}{\log n}\sin nx \log \sum_\limits{n\ge 1}\frac{1}{n^\alpha}\sin nx 0<\alpha<1 \sum_\limits{n\ge 1}\frac{1}{n}\sin nx,"['analysis', 'fourier-analysis', 'fourier-series']"
50,AM>HM Problem $\frac{1}{n+1}+...+\frac{1}{3n+1}>1$,AM>HM Problem,\frac{1}{n+1}+...+\frac{1}{3n+1}>1,"I am having difficulty solving one of the problems from ""Problems in Mathematical Analysis I"" -  W. J. Kaczor;M. T. Nowak . It's a problem 1.2.5 b), and it goes like this: 1.2.5. For $n \in \mathbb{N}$, verify the following claims: $$\tag{b} \qquad \dfrac{1}{n + 1} + \dfrac{1}{n + 2} + \dfrac{1}{n + 3} + \ldots + \dfrac{1}{3n + 1} \, > \, 1$$ In solutions it says: ""Use the arithmetic-harmonic mean inequality!"" I tried to apply it on whole inequality but got: \begin{align} & \dfrac{\frac{1}{n+1}+\ldots+\frac{1}{3n+1}}{2n}\, >\, \dfrac{2n}{n+1+\ldots+3n+1} \\  \implies & \frac{1}{n+1}+\ldots+\frac{1}{3n+1}>\frac{8n^2}{2n(n+1+3n+1)} \\ \implies & \frac{1}{n+1}+\ldots+\frac{1}{3n+1}>\frac{2n}{2n+1} \end{align}","I am having difficulty solving one of the problems from ""Problems in Mathematical Analysis I"" -  W. J. Kaczor;M. T. Nowak . It's a problem 1.2.5 b), and it goes like this: 1.2.5. For $n \in \mathbb{N}$, verify the following claims: $$\tag{b} \qquad \dfrac{1}{n + 1} + \dfrac{1}{n + 2} + \dfrac{1}{n + 3} + \ldots + \dfrac{1}{3n + 1} \, > \, 1$$ In solutions it says: ""Use the arithmetic-harmonic mean inequality!"" I tried to apply it on whole inequality but got: \begin{align} & \dfrac{\frac{1}{n+1}+\ldots+\frac{1}{3n+1}}{2n}\, >\, \dfrac{2n}{n+1+\ldots+3n+1} \\  \implies & \frac{1}{n+1}+\ldots+\frac{1}{3n+1}>\frac{8n^2}{2n(n+1+3n+1)} \\ \implies & \frac{1}{n+1}+\ldots+\frac{1}{3n+1}>\frac{2n}{2n+1} \end{align}",,"['analysis', 'inequality', 'cauchy-schwarz-inequality']"
51,Prove $A= \mathbb{R} $,Prove,A= \mathbb{R} ,"It can be seen easy but i'm really stack.I don't know how to start. Let $A  \neq  \emptyset   $ is a subset of $  \mathbb{R}  $.If for every real $x$,$y$ sum $x+y$ belongs to $A$,then $xy$ also belongs to the set $A$.Then prove that $A= \mathbb{R} $ $\mathbb{R}$- here  is the set of real numbers. Thanks for help in advance!","It can be seen easy but i'm really stack.I don't know how to start. Let $A  \neq  \emptyset   $ is a subset of $  \mathbb{R}  $.If for every real $x$,$y$ sum $x+y$ belongs to $A$,then $xy$ also belongs to the set $A$.Then prove that $A= \mathbb{R} $ $\mathbb{R}$- here  is the set of real numbers. Thanks for help in advance!",,['analysis']
52,Proof that Right hand and Left hand derivatives always exist for convex functions.,Proof that Right hand and Left hand derivatives always exist for convex functions.,,"Definition A function $f$ is convex on an interval if for $a,x, \text{and} \;b$ in the interval with $a\lt x\lt b$, we have $$\frac{f(x)-f(a)}{x-a}\lt \frac{f(b)-f(a)}{b-a}.$$ While reading the proof that if $f$ is convex on some open interval containing $a$, then $f_+'(a)$ and $f_{-}'(a)$ always exist. The proof first shows that $[f(a+h)-f(a)]/h$ is decreasing as $h\to 0^+$, so  $$f_+'(a)=\lim_{h\to 0^+}\frac{f(a+h)-f(a)}{h}=\operatorname{inf}\{\frac{f(a+h)-f(a)}{h}:h\gt 0\}.$$ The proof states that this inf exists because each quotient $\frac{f(a+h)-f(a)}{h}$ for $h\gt 0$ is greater than any one such quotient for $h'\lt 0$. However, I'm struggling to show this inequality, that for any $h\gt 0$ and $h'\lt 0$, we have $$\frac{f(a+h')-f(a)}{h'}\lt \frac{f(a+h)-f(a)}{h}.$$ I would greatly appreciate it if anyone can help me prove this inequality.","Definition A function $f$ is convex on an interval if for $a,x, \text{and} \;b$ in the interval with $a\lt x\lt b$, we have $$\frac{f(x)-f(a)}{x-a}\lt \frac{f(b)-f(a)}{b-a}.$$ While reading the proof that if $f$ is convex on some open interval containing $a$, then $f_+'(a)$ and $f_{-}'(a)$ always exist. The proof first shows that $[f(a+h)-f(a)]/h$ is decreasing as $h\to 0^+$, so  $$f_+'(a)=\lim_{h\to 0^+}\frac{f(a+h)-f(a)}{h}=\operatorname{inf}\{\frac{f(a+h)-f(a)}{h}:h\gt 0\}.$$ The proof states that this inf exists because each quotient $\frac{f(a+h)-f(a)}{h}$ for $h\gt 0$ is greater than any one such quotient for $h'\lt 0$. However, I'm struggling to show this inequality, that for any $h\gt 0$ and $h'\lt 0$, we have $$\frac{f(a+h')-f(a)}{h'}\lt \frac{f(a+h)-f(a)}{h}.$$ I would greatly appreciate it if anyone can help me prove this inequality.",,"['calculus', 'real-analysis', 'analysis', 'inequality', 'convex-analysis']"
53,Is it possible to turn infinite sums into infinite products?,Is it possible to turn infinite sums into infinite products?,,"I am working on studying infinite products. I know that it is possible to convert an infinite product to an infinite sum using logarithms, where $$\log \prod s_n = \sum \log s_n$$ My question is, it always possible go from sum to product with $e$, where $$\exp \sum s_n = \prod e^{s_n}$$ Also, are there any other methods to convert sums into products? Thanks in advance.","I am working on studying infinite products. I know that it is possible to convert an infinite product to an infinite sum using logarithms, where $$\log \prod s_n = \sum \log s_n$$ My question is, it always possible go from sum to product with $e$, where $$\exp \sum s_n = \prod e^{s_n}$$ Also, are there any other methods to convert sums into products? Thanks in advance.",,"['real-analysis', 'analysis', 'infinite-product']"
54,Proving measurable function: Real versus rational number [duplicate],Proving measurable function: Real versus rational number [duplicate],,"This question already has an answer here : Let $E$ be measurable and define $f:E\rightarrow\mathbb{R}$ such that $\{x\in E : f(x)>c\}$ is measurable for all $c\in\mathbb{Q}$, is $f$ measurable? (1 answer) Closed 9 years ago . I am working on this problem$^{(1)}$ on measurable function like this: Show that $f$ is measurable if $(X, \mathcal A)$ is measurable space, $f$ is real-value function and $\{x \mid f(x) > r \} \in \mathcal A$ for each rational number $r.$ Earlier on the text gives this definition$^{{2}}$ of measurable function: A function $f : X \to \mathbb R$ is measurable or $\mathcal A$- measurable if $\{x \mid f(x) > a \} \in \mathcal A$ for all $a \in \mathbb R.$ To an untrained eyes, the problem with this problem is that there seems to be no problem at all, except, perhaps, that the exercise says ""$\forall r \in \mathbb Q$"", whereas the definition says ""$\forall a \in \mathbb R.$"" Am I on the right track here? Please help me going forward if this is the right issue to tackle. Thank you for your time and help. Footnotes: (1) Richard F. Bass' Real Analysis , 2nd. edition, chapter 5: Measurable Functions, Exercise 5.1, page 44. (2) Definition 5.1, page 37.","This question already has an answer here : Let $E$ be measurable and define $f:E\rightarrow\mathbb{R}$ such that $\{x\in E : f(x)>c\}$ is measurable for all $c\in\mathbb{Q}$, is $f$ measurable? (1 answer) Closed 9 years ago . I am working on this problem$^{(1)}$ on measurable function like this: Show that $f$ is measurable if $(X, \mathcal A)$ is measurable space, $f$ is real-value function and $\{x \mid f(x) > r \} \in \mathcal A$ for each rational number $r.$ Earlier on the text gives this definition$^{{2}}$ of measurable function: A function $f : X \to \mathbb R$ is measurable or $\mathcal A$- measurable if $\{x \mid f(x) > a \} \in \mathcal A$ for all $a \in \mathbb R.$ To an untrained eyes, the problem with this problem is that there seems to be no problem at all, except, perhaps, that the exercise says ""$\forall r \in \mathbb Q$"", whereas the definition says ""$\forall a \in \mathbb R.$"" Am I on the right track here? Please help me going forward if this is the right issue to tackle. Thank you for your time and help. Footnotes: (1) Richard F. Bass' Real Analysis , 2nd. edition, chapter 5: Measurable Functions, Exercise 5.1, page 44. (2) Definition 5.1, page 37.",,"['real-analysis', 'analysis', 'measure-theory', 'lebesgue-measure']"
55,Composition of a function with a metric,Composition of a function with a metric,,"Which of the following functions, $f: [0,\infty) \rightarrow [0,\infty)$, can be composed with a metric $d$ to get a new metric $f \circ d$: a)$\;f(x) = \begin{cases}0 & \text{if $x=0$} \\x+1 & \text{if $x >0$}\end{cases}$ b)$f(x) = x^2, x \ge 0$ c)$f(x) = \arctan(x), x \ge 0$ In order for $f \circ d$ to be a metric we need $f$ to be monotonic and subadditive (and of course evaluate to 0 only when the input is 0) so that we can have the triangle inequality property satisfied. This is what I have so far: a) $f$ is monotonic (looking at the graph, it is always increasing over $[0,\infty)$) $\;\;\;f$ is subadditive since $1+(x+y) \le (1+x) + (1+y)$, for all $x,y$ b)$f$ is monotonic(looking at the graph, it is always increasing over $[0,\infty)$) $\;\;\;f$ is not subadditive since $(x+y)^2 = x^2 + 2xy + y^2 \not\le x^2 + y^2$ c)$f$ is monotonic(looking at the graph, it is always increasing over $[0,\infty)$) $\;\;\;f$ is or is not subadditive ?? I'm thinking that $\arctan(x)$ is subadditive since I can't think of any case where $\arctan(x+y) \le \arctan(x) + \arctan(y)$ doesn't hold over $[0,\infty)$. Feedback on what I have so far is appreciated.","Which of the following functions, $f: [0,\infty) \rightarrow [0,\infty)$, can be composed with a metric $d$ to get a new metric $f \circ d$: a)$\;f(x) = \begin{cases}0 & \text{if $x=0$} \\x+1 & \text{if $x >0$}\end{cases}$ b)$f(x) = x^2, x \ge 0$ c)$f(x) = \arctan(x), x \ge 0$ In order for $f \circ d$ to be a metric we need $f$ to be monotonic and subadditive (and of course evaluate to 0 only when the input is 0) so that we can have the triangle inequality property satisfied. This is what I have so far: a) $f$ is monotonic (looking at the graph, it is always increasing over $[0,\infty)$) $\;\;\;f$ is subadditive since $1+(x+y) \le (1+x) + (1+y)$, for all $x,y$ b)$f$ is monotonic(looking at the graph, it is always increasing over $[0,\infty)$) $\;\;\;f$ is not subadditive since $(x+y)^2 = x^2 + 2xy + y^2 \not\le x^2 + y^2$ c)$f$ is monotonic(looking at the graph, it is always increasing over $[0,\infty)$) $\;\;\;f$ is or is not subadditive ?? I'm thinking that $\arctan(x)$ is subadditive since I can't think of any case where $\arctan(x+y) \le \arctan(x) + \arctan(y)$ doesn't hold over $[0,\infty)$. Feedback on what I have so far is appreciated.",,"['real-analysis', 'analysis', 'proof-verification']"
56,Convergence of $\sum{a_kb_k}$ if $\sum{a_k}$ converges and $\sum{b_k}$ absolutely converges.,Convergence of  if  converges and  absolutely converges.,\sum{a_kb_k} \sum{a_k} \sum{b_k},"Convergence of $\sum{a_kb_k}$ if $\sum{a_k}$ converges and $\sum{b_k}$ absolutely converges. I tried to think that Since $\sum |b_k|$ is bounded I thought that $\sum a_k b_k$ $<$ $S\sum a_k$. Is it a correct inequality? If both sequences are just conditionally converge, then do $\sum{a_k b_k}$ converge?","Convergence of $\sum{a_kb_k}$ if $\sum{a_k}$ converges and $\sum{b_k}$ absolutely converges. I tried to think that Since $\sum |b_k|$ is bounded I thought that $\sum a_k b_k$ $<$ $S\sum a_k$. Is it a correct inequality? If both sequences are just conditionally converge, then do $\sum{a_k b_k}$ converge?",,"['real-analysis', 'analysis']"
57,What is $\frac{d^n}{dx^n} \frac{e^{\lambda x}}{x}$?,What is ?,\frac{d^n}{dx^n} \frac{e^{\lambda x}}{x},"I was wondering whether there is an explicit way to say what the derivative of $\dfrac{d^n}{dx^n} \dfrac{e^{\lambda x}}{x}$ for $n \in \mathbb{N}_0$is, where we assume that $\lambda \neq 0$.","I was wondering whether there is an explicit way to say what the derivative of $\dfrac{d^n}{dx^n} \dfrac{e^{\lambda x}}{x}$ for $n \in \mathbb{N}_0$is, where we assume that $\lambda \neq 0$.",,"['calculus', 'real-analysis', 'analysis', 'derivatives']"
58,Why is the domain of convergence of a power series a perfect disk?,Why is the domain of convergence of a power series a perfect disk?,,"I've been going over power series in my Differential Equations class for approximating solutions, and one thing that's been fascinating me is the statement that there is a radius of convergence, around the point for which the series converges. Now, I understand that when a function has an asymptote, like $f(x)=\frac1x$, this is naturally going to ""obstruct"" convergence, and so the series has to stop converging for values too close to the asymptote. But why should this hinder convergence on the other side of the point? Furthermore, this implies that when dealing with a complex power series, the region of convergence is a perfect disk . Why can't we have more irregular regions of convergence? What keeps convergence about the point perfectly symmetric?","I've been going over power series in my Differential Equations class for approximating solutions, and one thing that's been fascinating me is the statement that there is a radius of convergence, around the point for which the series converges. Now, I understand that when a function has an asymptote, like $f(x)=\frac1x$, this is naturally going to ""obstruct"" convergence, and so the series has to stop converging for values too close to the asymptote. But why should this hinder convergence on the other side of the point? Furthermore, this implies that when dealing with a complex power series, the region of convergence is a perfect disk . Why can't we have more irregular regions of convergence? What keeps convergence about the point perfectly symmetric?",,"['calculus', 'analysis', 'power-series']"
59,How can infinity be an accumulation point?,How can infinity be an accumulation point?,,"Let $(a_n)_{n∈\mathbb N}$ be a set in $\mathbb R$ . For every $r ∈ \mathbb R$ exits an accumulation point $b ∈ R \cup \{{−\infty} \}$ of $(a_n)_{n∈\mathbb N}$ with $ b < r$ .  ( $\pm \infty$ are allowed as accumulation points) Show, that $-\infty$ is an accumulation point of $(a_n)_{n∈\mathbb N}$ . I can't wrap my head around this: An accumulation point is a point of a set, which in every yet so small neighborhood (of itself) contains infinitely many points of the set, right? If (in my case) $-\infty$ is an accumulation point, then we could find a point of the set, that is finitely distant from infintity?!","Let be a set in . For every exits an accumulation point of with .  ( are allowed as accumulation points) Show, that is an accumulation point of . I can't wrap my head around this: An accumulation point is a point of a set, which in every yet so small neighborhood (of itself) contains infinitely many points of the set, right? If (in my case) is an accumulation point, then we could find a point of the set, that is finitely distant from infintity?!",(a_n)_{n∈\mathbb N} \mathbb R r ∈ \mathbb R b ∈ R \cup \{{−\infty} \} (a_n)_{n∈\mathbb N}  b < r \pm \infty -\infty (a_n)_{n∈\mathbb N} -\infty,"['analysis', 'infinity']"
60,"$x^2 f^{''}(x)+4xf^{'}(x)+2f(x)\geq 0$, prove $f(x)\leq 0$",", prove",x^2 f^{''}(x)+4xf^{'}(x)+2f(x)\geq 0 f(x)\leq 0,"More specifically, suppose $f$ is continuous on $[a,b]$ with $f(a)=f(b)=0$ and $x^2f^{\prime \prime}(x)+4xf^{\prime}(x)+2f(x)\geq 0$ for $x\in (a,b)$. Prove that $f(x)\leq 0$ for $x\in [a,b]$. I'm not sure how to approach this question. Any help would be greatly appreciated.","More specifically, suppose $f$ is continuous on $[a,b]$ with $f(a)=f(b)=0$ and $x^2f^{\prime \prime}(x)+4xf^{\prime}(x)+2f(x)\geq 0$ for $x\in (a,b)$. Prove that $f(x)\leq 0$ for $x\in [a,b]$. I'm not sure how to approach this question. Any help would be greatly appreciated.",,['analysis']
61,Prove approximation given by the physicist Max Born,Prove approximation given by the physicist Max Born,,"In an old book about optics, I have found a nice approximation, that for large l one has: $$P_l(\cos(\theta))  \sim \sqrt{\frac{2}{l \pi \sin(\theta)}} \sin \left((l+\frac{1}{2}) \theta + \frac{\pi}{4} \right)$$, where $P_l$ is the l-th Legendre polynomial. Unfortunately, I do not have any clue how to prove this, but maybe somebody here has an idea and an error approximation for this approximation? A reference would be sufficient too, of course.","In an old book about optics, I have found a nice approximation, that for large l one has: $$P_l(\cos(\theta))  \sim \sqrt{\frac{2}{l \pi \sin(\theta)}} \sin \left((l+\frac{1}{2}) \theta + \frac{\pi}{4} \right)$$, where $P_l$ is the l-th Legendre polynomial. Unfortunately, I do not have any clue how to prove this, but maybe somebody here has an idea and an error approximation for this approximation? A reference would be sufficient too, of course.",,"['calculus', 'real-analysis']"
62,When is the infimum of the sum of two sets equal to the sum of their infima?,When is the infimum of the sum of two sets equal to the sum of their infima?,,When is the following true? $A$ and $B$ are subsets of real numbers. I don't say that $A$ and/or $B$ are closed: $$\inf (A + B) = \inf (A) + \inf(B)$$ When is there a strict inequality in between? Could you give an example? When is the equality for what sufficient condition? Could you give an example? Thanks a lot!,When is the following true? $A$ and $B$ are subsets of real numbers. I don't say that $A$ and/or $B$ are closed: $$\inf (A + B) = \inf (A) + \inf(B)$$ When is there a strict inequality in between? Could you give an example? When is the equality for what sufficient condition? Could you give an example? Thanks a lot!,,"['calculus', 'real-analysis', 'analysis']"
63,"Find nth derivative of $\frac{x^{n}}{(1-x)^{2}}$, please?","Find nth derivative of , please?",\frac{x^{n}}{(1-x)^{2}},"I need to find the nth derivative of $\frac{x^{n}}{(1-x)^{2}}$ for $0<x<1$ So far, I tried the same method used for $\frac{x^{n}}{1-x}$ and here's what I got: \begin{equation} \frac{x^{n}}{(1-x)^{2}}=x^{n}(1+2x+3x^{2}+4x^{3}+....)=(x^{n}+2x^{n+1}+3x^{n+2}+...) \end{equation} Take nth derivative to get: \begin{align} \frac{\partial^n }{\partial x^n} \frac{x^{n}}{(1-x)^{2}} \notag\\ =  & (n!+2(n+1)!x+3\frac{(n+2)!}{2!}x^{2}+...) \notag\\ = & n!(1+2(n+1)x+3\frac{(n+2)(n+1)}{2!}x^{2}+4\frac{(n+3)(n+2)(n+1)}{3!}x^{3}+..)  \end{align} Next would be finding a a function whose taylor expansion is the series in the parenthesis but I couldn't think of one. Any ideas for a function or for another method to do this? Thanks!","I need to find the nth derivative of $\frac{x^{n}}{(1-x)^{2}}$ for $0<x<1$ So far, I tried the same method used for $\frac{x^{n}}{1-x}$ and here's what I got: \begin{equation} \frac{x^{n}}{(1-x)^{2}}=x^{n}(1+2x+3x^{2}+4x^{3}+....)=(x^{n}+2x^{n+1}+3x^{n+2}+...) \end{equation} Take nth derivative to get: \begin{align} \frac{\partial^n }{\partial x^n} \frac{x^{n}}{(1-x)^{2}} \notag\\ =  & (n!+2(n+1)!x+3\frac{(n+2)!}{2!}x^{2}+...) \notag\\ = & n!(1+2(n+1)x+3\frac{(n+2)(n+1)}{2!}x^{2}+4\frac{(n+3)(n+2)(n+1)}{3!}x^{3}+..)  \end{align} Next would be finding a a function whose taylor expansion is the series in the parenthesis but I couldn't think of one. Any ideas for a function or for another method to do this? Thanks!",,"['calculus', 'analysis', 'derivatives', 'taylor-expansion']"
64,Uniform continuous maps that converge uniformly to a function f. What can we say about f?,Uniform continuous maps that converge uniformly to a function f. What can we say about f?,,"Here is the question: Let $X$ and $Y$ be metric spaces.Suppose a sequence of uniformly continuous maps $f_n : X \rightarrow Y$ converges uniformly to a map $f: X \rightarrow Y$.Does that imply that f is continuous ? Uniformly continuous ($X$ is not necessarily compact)? I proved (hopefully right) that $f:X \rightarrow Y$ is continuous , even without using uniform continuity of $f_n$ but just assuming $f_n$ are continuous.  I can't prove or disprove that $f$ is uniformly continuous  and whether adding the compactness condition would change the answer.","Here is the question: Let $X$ and $Y$ be metric spaces.Suppose a sequence of uniformly continuous maps $f_n : X \rightarrow Y$ converges uniformly to a map $f: X \rightarrow Y$.Does that imply that f is continuous ? Uniformly continuous ($X$ is not necessarily compact)? I proved (hopefully right) that $f:X \rightarrow Y$ is continuous , even without using uniform continuity of $f_n$ but just assuming $f_n$ are continuous.  I can't prove or disprove that $f$ is uniformly continuous  and whether adding the compactness condition would change the answer.",,"['analysis', 'uniform-convergence', 'uniform-continuity']"
65,Determine value of harmonic function at origin given boundary values on a regular hexagon,Determine value of harmonic function at origin given boundary values on a regular hexagon,,"Let $E$ be a regular hexagon centered at the origin of $\mathbb{R}^2$. Let $f$ be the harmonic function in $E$ with boundary value 1 on one of the sides of $E$ and boundary value $0$ on each of the remaining sides. What is the value of $f$ at the origin? This question has shown up on an old PDE qual I am studying. This problem is causing me a lot of concern, because $f$ seems to be discontinuous on the boundary of $E$. But, given that $f$ is harmonic (and thus continuous) in $E$, shouldn't the boundary values of $f$ also define a continuous function? Hints or explanations are greatly appreciated!","Let $E$ be a regular hexagon centered at the origin of $\mathbb{R}^2$. Let $f$ be the harmonic function in $E$ with boundary value 1 on one of the sides of $E$ and boundary value $0$ on each of the remaining sides. What is the value of $f$ at the origin? This question has shown up on an old PDE qual I am studying. This problem is causing me a lot of concern, because $f$ seems to be discontinuous on the boundary of $E$. But, given that $f$ is harmonic (and thus continuous) in $E$, shouldn't the boundary values of $f$ also define a continuous function? Hints or explanations are greatly appreciated!",,"['analysis', 'partial-differential-equations']"
66,Bound of a function $f_n=\frac{x^2}{x^2+(1-nx)^2}$,Bound of a function,f_n=\frac{x^2}{x^2+(1-nx)^2},"Let $\displaystyle f_n=\frac{x^2}{x^2+(1-nx)^2}$ where ($0\le x\le 1,\; n=1,2,3,...$) Then $|f_n(x)| \le M$. Find this $M$. The answer is $1$. Without any restriction of n, how can we find that bound?","Let $\displaystyle f_n=\frac{x^2}{x^2+(1-nx)^2}$ where ($0\le x\le 1,\; n=1,2,3,...$) Then $|f_n(x)| \le M$. Find this $M$. The answer is $1$. Without any restriction of n, how can we find that bound?",,['analysis']
67,Power series of a function,Power series of a function,,"I am wondering if there are any functions $f(x)$ such that it cannot be expressed as a power series of $x$? This might turn out to be a silly question, but I can't think of one at the moment. Thanks!","I am wondering if there are any functions $f(x)$ such that it cannot be expressed as a power series of $x$? This might turn out to be a silly question, but I can't think of one at the moment. Thanks!",,['analysis']
68,Could you give a application of a special function on number theory or analysis?,Could you give a application of a special function on number theory or analysis?,,"With the best effort i have ever taken, i couldn't find a application of a special function on number theory or analysis on the internet. By the way, why is the applications of special functions in pure math almost never mention in books","With the best effort i have ever taken, i couldn't find a application of a special function on number theory or analysis on the internet. By the way, why is the applications of special functions in pure math almost never mention in books",,"['number-theory', 'analysis', 'special-functions']"
69,Convergence of integral $\int_{1}^{\infty}\frac{\sqrt{x}+2+\cos{x}}{x+2}dx$,Convergence of integral,\int_{1}^{\infty}\frac{\sqrt{x}+2+\cos{x}}{x+2}dx,"Is $$\frac{1}{x^{\frac{3}{4}}}<\frac{\sqrt{x}+2+\cos{x}}{x+2},\forall x>x_0$$ enough to prove that $$\int_{1}^{\infty}\frac{\sqrt{x}+2+\cos{x}}{x+2}dx$$ diverges? What are the standard techniques which can be used in these type of problems?","Is $$\frac{1}{x^{\frac{3}{4}}}<\frac{\sqrt{x}+2+\cos{x}}{x+2},\forall x>x_0$$ enough to prove that $$\int_{1}^{\infty}\frac{\sqrt{x}+2+\cos{x}}{x+2}dx$$ diverges? What are the standard techniques which can be used in these type of problems?",,['analysis']
70,Completeness of Upper Half Plane,Completeness of Upper Half Plane,,"I am trying to prove that the upper half plane, defined as $\mathbb{H} = \{z \in \mathbb{C} : \Im(z)>0 \}$, is complete with respect to the hyperbolic metric. First I note that if I have some closed and bounded subset $X$ of $\mathbb{H}$, it is complete. However, when dealing with $\mathbb{H}$, I would like to use the nice property that if I am in $\mathbb{R}^n$, and have some Cauchy sequence $x_n$ in a closed and bounded subset of $\mathbb{R}^n$, then it has a subsequence say $x_{n_k}$ which converges in my closed and bounded in the euclidean metric, viz. if I am in $\mathbb{R}^2$ it is just $|\mathbf{x} - \mathbf{y}|$, where $\mathbf{x}, \mathbf{y}$ some points in my set. How do I deal with the fact that at the boundary, my euclidean metric remains bounded but the hyperbolic metric defined as $$d(z_1,z_2) = \ln \left[ \frac{|z_1 - \bar{z_2}| + |z_1  - z_2 |  }{|z_1 - \bar{z_2}| - |z_1  - z_2 | }\right]$$ goes to infinity? In addition, the upper half of the complex plane is not closed, so how can I use nice properties like convergence of subsequences and stuff to prove that it is complete? Thanks.","I am trying to prove that the upper half plane, defined as $\mathbb{H} = \{z \in \mathbb{C} : \Im(z)>0 \}$, is complete with respect to the hyperbolic metric. First I note that if I have some closed and bounded subset $X$ of $\mathbb{H}$, it is complete. However, when dealing with $\mathbb{H}$, I would like to use the nice property that if I am in $\mathbb{R}^n$, and have some Cauchy sequence $x_n$ in a closed and bounded subset of $\mathbb{R}^n$, then it has a subsequence say $x_{n_k}$ which converges in my closed and bounded in the euclidean metric, viz. if I am in $\mathbb{R}^2$ it is just $|\mathbf{x} - \mathbf{y}|$, where $\mathbf{x}, \mathbf{y}$ some points in my set. How do I deal with the fact that at the boundary, my euclidean metric remains bounded but the hyperbolic metric defined as $$d(z_1,z_2) = \ln \left[ \frac{|z_1 - \bar{z_2}| + |z_1  - z_2 |  }{|z_1 - \bar{z_2}| - |z_1  - z_2 | }\right]$$ goes to infinity? In addition, the upper half of the complex plane is not closed, so how can I use nice properties like convergence of subsequences and stuff to prove that it is complete? Thanks.",,['analysis']
71,directional derivative in a manifold,directional derivative in a manifold,,"Let us assume that directional derivative of a function $f$ exists at a point $p$ (i.e.,$ D_v(f)$) for all vectors $v \in \mathbb{R}^{n}$. Does it imply that the function is differentiable?","Let us assume that directional derivative of a function $f$ exists at a point $p$ (i.e.,$ D_v(f)$) for all vectors $v \in \mathbb{R}^{n}$. Does it imply that the function is differentiable?",,"['calculus', 'analysis', 'differential-geometry']"
72,Archimedean property of $\mathbb{R}$,Archimedean property of,\mathbb{R},"Theorem: If $x,y \in \mathbb{R}$ and $x > 0$, $\exists$ a positive integer $n$ such that $nx > y$ I read the proof by Rudin and understood it. I think it is very elegant and uses the LUB property of $\mathbb{R}$. But the way I tried to prove it is the following: If $x > y$, then $n = 1$. If $x < y$, then $y > 0$ and it makes sense to define $m = \frac{y}{x}$. $m \in \mathbb{R}$ using one of the field axioms. Since the set of natural numbers is unbounded $\exists n \in \mathbb{N}$ such that $n > m$. This implies $nx > y$. This proves the Archimedean property. Is there something wrong with the proof or am I assuming something that is not obvious? This is something I frequently wonder while I am trying to learn analysis. I really like the way Rudin proves the theorems in his book. But I find it really difficult to adapt that kind of thinking and rigor. I think a big reason why I find this difficult is because I have an engineering background and I am never used to being so rigorous with proofs. Can anyone suggest some tips (or your experience) that deal with improving the ability to write rigorous proofs (and the ability to find logical gaps in proofs).","Theorem: If $x,y \in \mathbb{R}$ and $x > 0$, $\exists$ a positive integer $n$ such that $nx > y$ I read the proof by Rudin and understood it. I think it is very elegant and uses the LUB property of $\mathbb{R}$. But the way I tried to prove it is the following: If $x > y$, then $n = 1$. If $x < y$, then $y > 0$ and it makes sense to define $m = \frac{y}{x}$. $m \in \mathbb{R}$ using one of the field axioms. Since the set of natural numbers is unbounded $\exists n \in \mathbb{N}$ such that $n > m$. This implies $nx > y$. This proves the Archimedean property. Is there something wrong with the proof or am I assuming something that is not obvious? This is something I frequently wonder while I am trying to learn analysis. I really like the way Rudin proves the theorems in his book. But I find it really difficult to adapt that kind of thinking and rigor. I think a big reason why I find this difficult is because I have an engineering background and I am never used to being so rigorous with proofs. Can anyone suggest some tips (or your experience) that deal with improving the ability to write rigorous proofs (and the ability to find logical gaps in proofs).",,"['analysis', 'soft-question']"
73,"What are some examples of non smooth continuous functions without ""kinks""?","What are some examples of non smooth continuous functions without ""kinks""?",,"In a recent post I asked what are the ways a continuous function cannot be smooth.  Apparently my question was not well posed, but since I am still thinking about these types of functions I will try one more time to ask my question in a proper way. I define a ""kink"" in the graph of a function as an abrupt, discontinuous change in the first derivative.  For example, the function $f(x) = |x|$ has what I call a kink at $x=0$.  I apologize if this is not totally rigorous, but I think the idea of a ""kink"" is clear enough that someone could make it rigorous with little effort. Besides $\sin(1/x)$ and the Weirstrauss function, what are some examples of non-smooth continuous functions without kinks, either in the graph of the function or the graph of any of it's derivatives? (i.e. I am not looking for any function for which the graph of one of the derivatives has a kink) In case anyone is interested, I edited the linked post yesterday to respond to Qiaochu's comment.","In a recent post I asked what are the ways a continuous function cannot be smooth.  Apparently my question was not well posed, but since I am still thinking about these types of functions I will try one more time to ask my question in a proper way. I define a ""kink"" in the graph of a function as an abrupt, discontinuous change in the first derivative.  For example, the function $f(x) = |x|$ has what I call a kink at $x=0$.  I apologize if this is not totally rigorous, but I think the idea of a ""kink"" is clear enough that someone could make it rigorous with little effort. Besides $\sin(1/x)$ and the Weirstrauss function, what are some examples of non-smooth continuous functions without kinks, either in the graph of the function or the graph of any of it's derivatives? (i.e. I am not looking for any function for which the graph of one of the derivatives has a kink) In case anyone is interested, I edited the linked post yesterday to respond to Qiaochu's comment.",,['analysis']
74,Tao Analysis 1 Exercise 6.1.8.,Tao Analysis 1 Exercise 6.1.8.,,"Let $(b_n)_{n=m}^\infty$ be a sequence of real numbers and let $L$ be a real number such that $\lim_{n\to\infty}b_n = L$ . Let also $b_n \neq 0$ for all n and $L \neq 0$ . Show that \begin{equation*} \lim_{n\to\infty}\frac{1}{b_n} = \frac{1}{L}. \end{equation*} I do not understand the hint behind the exercise. The hint states that one should prove that the sequence is bounded away from zero. However, if $b_n \neq 0$ for all $n$ why do I need to prove such a thing? In particular, can someone point out where the mistake is in my reasoning? My proof: Since $\lim_{n\to\infty} b_n = L$ the sequence is bounded; i.e., there exists a $B>0$ such that $|b_n|\leq B$ for all $n$ . Observe that \begin{align} |\frac{1}{b_n} - \frac{1}{L}| &= |\frac{L-b_n}{b_nL}|\\ &= \frac{|b_n - L|}{|b_n||L|}\\ &\leq \frac{|b_n - L|}{B|L|}. \end{align} Choosing now an appropriate $\epsilon$ for $|b_n - L|$ like $\epsilon B|L|$ should be sufficient to conclude the reasoning.","Let be a sequence of real numbers and let be a real number such that . Let also for all n and . Show that I do not understand the hint behind the exercise. The hint states that one should prove that the sequence is bounded away from zero. However, if for all why do I need to prove such a thing? In particular, can someone point out where the mistake is in my reasoning? My proof: Since the sequence is bounded; i.e., there exists a such that for all . Observe that Choosing now an appropriate for like should be sufficient to conclude the reasoning.","(b_n)_{n=m}^\infty L \lim_{n\to\infty}b_n = L b_n \neq 0 L \neq 0 \begin{equation*}
\lim_{n\to\infty}\frac{1}{b_n} = \frac{1}{L}.
\end{equation*} b_n \neq 0 n \lim_{n\to\infty} b_n = L B>0 |b_n|\leq B n \begin{align}
|\frac{1}{b_n} - \frac{1}{L}| &= |\frac{L-b_n}{b_nL}|\\
&= \frac{|b_n - L|}{|b_n||L|}\\
&\leq \frac{|b_n - L|}{B|L|}.
\end{align} \epsilon |b_n - L| \epsilon B|L|","['real-analysis', 'analysis']"
75,"$\int_a^bf'=f(b)-f(a)$ if $f'$ is integrable, but not continuous?","if  is integrable, but not continuous?",\int_a^bf'=f(b)-f(a) f',"Let $a<b$ and $f:(a,b)\to\mathbb R$ be differentiable (i.e. $f'$ exists, but is NOT necessarily continuous). Assuming that $f'$ is Lebesgue integrable , does it still hold that $$\int_a^bf'=f(b)-f(a)?\tag1$$ The claim is clearly true when $f'$ is continuous, since then $(1)$ is simply the second fundamental theorem of calculus. I wasn't able to come up with a couterexample, but I didn't found a reference of the claim either.","Let and be differentiable (i.e. exists, but is NOT necessarily continuous). Assuming that is Lebesgue integrable , does it still hold that The claim is clearly true when is continuous, since then is simply the second fundamental theorem of calculus. I wasn't able to come up with a couterexample, but I didn't found a reference of the claim either.","a<b f:(a,b)\to\mathbb R f' f' \int_a^bf'=f(b)-f(a)?\tag1 f' (1)","['real-analysis', 'calculus', 'analysis', 'derivatives', 'lebesgue-integral']"
76,a mean value theorem question [closed],a mean value theorem question [closed],,"Closed. This question does not meet Mathematics Stack Exchange guidelines . It is not currently accepting answers. Please provide additional context , which ideally explains why the question is relevant to you and our  community. Some forms of context include: background and motivation, relevant definitions, source, possible strategies, your current progress, why the question is interesting or important, etc. Closed 3 years ago . Improve this question if $f(x)$ can be differentiated to any degree for $x\in (0,+\infty)$ and $f'(x)>0,f''(x)<0$ ，if $0<a<b$ ，acordding to mean value theorem we have $\displaystyle\exists \xi\in(a,b),st.\frac{f(b)-f(a)}{b-a}=f'(\xi)$ , prove: $\displaystyle \xi<\frac{a+b}{2}$ And I think that's pretty obvious by looking at the graph of the function,but i don't know how to prove it in math words.","Closed. This question does not meet Mathematics Stack Exchange guidelines . It is not currently accepting answers. Please provide additional context , which ideally explains why the question is relevant to you and our  community. Some forms of context include: background and motivation, relevant definitions, source, possible strategies, your current progress, why the question is interesting or important, etc. Closed 3 years ago . Improve this question if can be differentiated to any degree for and ，if ，acordding to mean value theorem we have , prove: And I think that's pretty obvious by looking at the graph of the function,but i don't know how to prove it in math words.","f(x) x\in (0,+\infty) f'(x)>0,f''(x)<0 0<a<b \displaystyle\exists \xi\in(a,b),st.\frac{f(b)-f(a)}{b-a}=f'(\xi) \displaystyle \xi<\frac{a+b}{2}","['real-analysis', 'analysis', 'functions']"
77,"Is $x\log x$ Hölder continuous on $[0,1]$?",Is  Hölder continuous on ?,"x\log x [0,1]","Consider the function $f:[0,1]\rightarrow \mathbb{R}$ given by $$f(x)= x\log(x).$$ This function is not Lipschitz continuous at zero, but apparently Hölder continuous for any $\alpha<1$ , i.e. there is a constant $C_{\alpha}$ such that $$\vert f(x)-f(y)\vert \le C_{\alpha} \vert x-y \vert^{\alpha},$$ but how can one show the Hölder continuity? Please let me know if you have any questions or remarks.","Consider the function given by This function is not Lipschitz continuous at zero, but apparently Hölder continuous for any , i.e. there is a constant such that but how can one show the Hölder continuity? Please let me know if you have any questions or remarks.","f:[0,1]\rightarrow \mathbb{R} f(x)= x\log(x). \alpha<1 C_{\alpha} \vert f(x)-f(y)\vert \le C_{\alpha} \vert x-y \vert^{\alpha},","['real-analysis', 'analysis', 'continuity', 'holder-spaces']"
78,Show that $\int_0^a \frac{\sin x}{x} dx \leq 1+\log a$ for all $a \geq 1$,Show that  for all,\int_0^a \frac{\sin x}{x} dx \leq 1+\log a a \geq 1,"Show that $\int_0^a \frac{\sin x}{x} dx \leq 1+\log a$ for all $a \geq 1$ . My attempt: define $f(a)=1+\log a-\int_0^a \frac{\sin x}{x}dx$ . Then the inequality is equivalent to show that $f(a) \geq 0$ for all $a \geq 1$ . $f$ is differentiable for all $a \geq 1$ , since it a sum of differentiable functions (a constant, a logarithm and an integral of a continuous function that is differentiable in the interval $0 < a \leq 1$ for the fundamental theorem of calculus) in the interval $a\in[1,\infty)$ . SO it has sense to evaluate $f'$ , and it is $$f'(a)=\frac{1}{a}-\frac{\sin a}{a}=\frac{1}{a}\left(1-\sin a\right) \geq 0 \ \ \ \ \ \forall a \geq1$$ So $f$ is increasing for all $a \geq 1$ , this means that if $f(1) \geq 0$ then $f(a) \geq 0$ for all $a \geq 1$ for monotonicity. It is $$f(1)=1-\int_0^1 \frac{\sin x}{x} dx$$ So it is $$f(1) \geq 0 \iff \int_0^1 \frac{\sin x}{x} dx \leq 1$$ Since $\sin t \leq t$ for all $t \geq 0$ and since $x\in[0,1]$ it is $\sin x \leq x \iff \frac{\sin x}{x} \leq 1$ , so for the monotonicity of the integral it is $\int_0^1 \frac{\sin x}{x} dx \leq \int_0^1 1 dx=1$ as well; this shows that the inequality is true for all $a \geq 1$ . Is my approach correct? If there is mistakes or imprecisions let me know please. If it is correct I would be glad to see other way to prove this. Thanks.","Show that for all . My attempt: define . Then the inequality is equivalent to show that for all . is differentiable for all , since it a sum of differentiable functions (a constant, a logarithm and an integral of a continuous function that is differentiable in the interval for the fundamental theorem of calculus) in the interval . SO it has sense to evaluate , and it is So is increasing for all , this means that if then for all for monotonicity. It is So it is Since for all and since it is , so for the monotonicity of the integral it is as well; this shows that the inequality is true for all . Is my approach correct? If there is mistakes or imprecisions let me know please. If it is correct I would be glad to see other way to prove this. Thanks.","\int_0^a \frac{\sin x}{x} dx \leq 1+\log a a \geq 1 f(a)=1+\log a-\int_0^a \frac{\sin x}{x}dx f(a) \geq 0 a \geq 1 f a \geq 1 0 < a \leq 1 a\in[1,\infty) f' f'(a)=\frac{1}{a}-\frac{\sin a}{a}=\frac{1}{a}\left(1-\sin a\right) \geq 0 \ \ \ \ \ \forall a \geq1 f a \geq 1 f(1) \geq 0 f(a) \geq 0 a \geq 1 f(1)=1-\int_0^1 \frac{\sin x}{x} dx f(1) \geq 0 \iff \int_0^1 \frac{\sin x}{x} dx \leq 1 \sin t \leq t t \geq 0 x\in[0,1] \sin x \leq x \iff \frac{\sin x}{x} \leq 1 \int_0^1 \frac{\sin x}{x} dx \leq \int_0^1 1 dx=1 a \geq 1","['analysis', 'inequality', 'definite-integrals']"
79,Find all functions $f$ :- $\mathbb{N}$ $\to$ $\mathbb{N}$ such that :- $xf(y) + yf(x) = (x + y)f(x^2 + y^2)$,Find all functions  :-    such that :-,f \mathbb{N} \to \mathbb{N} xf(y) + yf(x) = (x + y)f(x^2 + y^2),"So here is the Question :- Find all functions $f$ :- $\mathbb{N}$ $\to$ $\mathbb{N}$ such that :- $$xf(y) + yf(x) = (x + y)f(x^2 + y^2)$$ I tried substituting values for $x$ and $y$ , but I couldn't reach to a possible clue to the solution. Any hints or suggestions will be greatly appreciated!","So here is the Question :- Find all functions :- such that :- I tried substituting values for and , but I couldn't reach to a possible clue to the solution. Any hints or suggestions will be greatly appreciated!",f \mathbb{N} \to \mathbb{N} xf(y) + yf(x) = (x + y)f(x^2 + y^2) x y,"['analysis', 'functions', 'functional-equations']"
80,Little-o notation: limit in the definition,Little-o notation: limit in the definition,,"In Apostol, little-o $f(x) = o(g(x))$ is defined as $\lim_{x \rightarrow 0} \frac{f(x)}{g(x)}$ . However, in Wiki it is defined as $\lim_{x \rightarrow \infty} \frac{f(x)}{g(x)}$ . How do we determine the limit point from the notation? Is it possible that $f(x) = o(g(x))$ if $x \rightarrow 0$ , while $f(x) \neq o(g(x))$ , if $x \rightarrow \infty$","In Apostol, little-o is defined as . However, in Wiki it is defined as . How do we determine the limit point from the notation? Is it possible that if , while , if",f(x) = o(g(x)) \lim_{x \rightarrow 0} \frac{f(x)}{g(x)} \lim_{x \rightarrow \infty} \frac{f(x)}{g(x)} f(x) = o(g(x)) x \rightarrow 0 f(x) \neq o(g(x)) x \rightarrow \infty,"['calculus', 'analysis', 'asymptotics']"
81,Question related to Darboux's theorem,Question related to Darboux's theorem,,"Darboux's theorem says that $f'$ has intermediate value property. More precisely, ( Darboux's theorem) If $f$ is differentiable on $[a,b]$ , and if r is any number for which $f'(a)<r<f'(b)$ then $\exists$ c in (a,b) such that $f'(r)=c$ . Thus Darboux's theorem implies that $f'$ cannot have any simple discontinuities on $[a,b].$ I have a question as follows: Is there a function $f$ satisfying $\displaystyle \lim_{x\to d} f'(x)=\infty$ for some $d \in (a,b)$ under the assumptions in Darboux's theorem? If so, I can conclude that for each $x \in (a,b),$ either $(i)$ $f'$ is continuous at $x$ or $(ii)$ $f'$ oscillates near $x.$ It seems that there is no differentiable function $f$ on $[a,b]$ satisfying function $\displaystyle \lim_{x\to d} f'(x)=\infty$ and $\exists f'(d)$ for some $d \in (a,b)$ . Please let me know if you have any idea or comment for my question. Thanks in advance!","Darboux's theorem says that has intermediate value property. More precisely, ( Darboux's theorem) If is differentiable on , and if r is any number for which then c in (a,b) such that . Thus Darboux's theorem implies that cannot have any simple discontinuities on I have a question as follows: Is there a function satisfying for some under the assumptions in Darboux's theorem? If so, I can conclude that for each either is continuous at or oscillates near It seems that there is no differentiable function on satisfying function and for some . Please let me know if you have any idea or comment for my question. Thanks in advance!","f' f [a,b] f'(a)<r<f'(b) \exists f'(r)=c f' [a,b]. f \displaystyle \lim_{x\to d} f'(x)=\infty d \in (a,b) x \in (a,b), (i) f' x (ii) f' x. f [a,b] \displaystyle \lim_{x\to d} f'(x)=\infty \exists f'(d) d \in (a,b)","['real-analysis', 'calculus', 'analysis']"
82,Improper integral convergence: $\int_{0}^{1}{\frac{\mid{\log(x)}\mid^a}{\sqrt{1-x^2}}}dx$,Improper integral convergence:,\int_{0}^{1}{\frac{\mid{\log(x)}\mid^a}{\sqrt{1-x^2}}}dx,"As its told on the title I want to check the convergence/divergence of the improper integral when $a\in \mathbb{R}$: \begin{equation} \int_{0}^{1}{\frac{\mid{\log(x)}\mid^a}{\sqrt{1-x^2}}}dx \end{equation} So, it's improper at $x=0$ and $x=1$, so I split the integral in : \begin{equation} \int_{0}^{\frac{1}{2}}{\frac{\mid{\log(x)}\mid^a}{\sqrt{1-x^2}}}dx + \int_{\frac{1}{2}}^{1}{\frac{\mid{\log(x)}\mid^a}{\sqrt{1-x^2}}}dx  \end{equation} I see, that on the first one the integrals it's like $\int\mid{\log(x)}\mid^a dx $ by the comparison limit test, but I don't know how to prove that $\int\mid{\log(x)}\mid^a dx $ converges. Hopefully you can help me. Much thanks!","As its told on the title I want to check the convergence/divergence of the improper integral when $a\in \mathbb{R}$: \begin{equation} \int_{0}^{1}{\frac{\mid{\log(x)}\mid^a}{\sqrt{1-x^2}}}dx \end{equation} So, it's improper at $x=0$ and $x=1$, so I split the integral in : \begin{equation} \int_{0}^{\frac{1}{2}}{\frac{\mid{\log(x)}\mid^a}{\sqrt{1-x^2}}}dx + \int_{\frac{1}{2}}^{1}{\frac{\mid{\log(x)}\mid^a}{\sqrt{1-x^2}}}dx  \end{equation} I see, that on the first one the integrals it's like $\int\mid{\log(x)}\mid^a dx $ by the comparison limit test, but I don't know how to prove that $\int\mid{\log(x)}\mid^a dx $ converges. Hopefully you can help me. Much thanks!",,"['analysis', 'improper-integrals']"
83,The value of $\prod_2^\infty \left(1-2/(n(n+1))\right)$,The value of,\prod_2^\infty \left(1-2/(n(n+1))\right),"I'm trying to find the value of $\prod_2^\infty \left(1-2/(n(n+1))\right)$. So far I have the following. \begin{align*} \prod_2^\infty \left(1-2/(n(n+1))\right) &= (1-2/6)*(1-2/12)*(1-2/20)*(1-2/30)... \\&= (1-1/3)*(1-1/6)*(1-1/10)*(1-1/15)... \\&= (2/3)*(5/6)*(9/10)*(14/15)... \\&= (2/3) * ((1*5)/(2*3)) * ((3*3)/(5*2)) * ((2*7)/(3*5))... \end{align*} The partial product with just the first four terms equals $(1/3)*(7/5)$ because almost everything cancels out. I claim that the term which $(1/3)$ gets multiplied by goes to zero since it goes from $5/3$ to $3/2$ to $7/5$... (making the value $1/3$), but I don't know the explicit formula for each factor of each term so I can't prove that. Does anyone know the explicit formula for the factors?","I'm trying to find the value of $\prod_2^\infty \left(1-2/(n(n+1))\right)$. So far I have the following. \begin{align*} \prod_2^\infty \left(1-2/(n(n+1))\right) &= (1-2/6)*(1-2/12)*(1-2/20)*(1-2/30)... \\&= (1-1/3)*(1-1/6)*(1-1/10)*(1-1/15)... \\&= (2/3)*(5/6)*(9/10)*(14/15)... \\&= (2/3) * ((1*5)/(2*3)) * ((3*3)/(5*2)) * ((2*7)/(3*5))... \end{align*} The partial product with just the first four terms equals $(1/3)*(7/5)$ because almost everything cancels out. I claim that the term which $(1/3)$ gets multiplied by goes to zero since it goes from $5/3$ to $3/2$ to $7/5$... (making the value $1/3$), but I don't know the explicit formula for each factor of each term so I can't prove that. Does anyone know the explicit formula for the factors?",,"['real-analysis', 'analysis']"
84,Inequality : $\sum_{k=1}^n x_k\cdot \sum_{k=1}^n \frac{1}{x_k} \geq n^2$,Inequality :,\sum_{k=1}^n x_k\cdot \sum_{k=1}^n \frac{1}{x_k} \geq n^2,"I have to show the inequality of $$\left(\sum_{i=1}^n x_i\right)*\left(\sum_{i=1}^n \frac{1}{x_i}\right) \geq n^2.$$For $x_1, ... x_n \in \mathbb{R_{>0}}$ and $ n \geq 1$. I wanted to show this inequality by induction. The basis is clear, but I am not sure how to do the inductive step. Can someone help me with that?","I have to show the inequality of $$\left(\sum_{i=1}^n x_i\right)*\left(\sum_{i=1}^n \frac{1}{x_i}\right) \geq n^2.$$For $x_1, ... x_n \in \mathbb{R_{>0}}$ and $ n \geq 1$. I wanted to show this inequality by induction. The basis is clear, but I am not sure how to do the inductive step. Can someone help me with that?",,"['analysis', 'inequality', 'induction']"
85,"Let $\lVert\cdot\rVert_1,\lVert\cdot\rVert_2$ be norms on vector space $X$. Prove that they generate the same topology iff they are equivalent. [duplicate]",Let  be norms on vector space . Prove that they generate the same topology iff they are equivalent. [duplicate],"\lVert\cdot\rVert_1,\lVert\cdot\rVert_2 X","This question already has answers here : Are two norms equivalent if they induce the same topology on a vector space? (2 answers) Closed 8 years ago . Note that by ""generate the same topology"" we mean that any set that is open with respect to $\lVert\cdot\rVert_1$ is also open with respect to $\lVert\cdot\rVert_2$ and vice versa. By ""equivalent"" we mean that $\exists m,M>0$ such that $m\lVert x \rVert_2\leq \lVert x \rVert_1 \leq M\lVert x \rVert_2$. This is part of an old preliminary exam in Analysis I'm using to prep for my own prelim. I have already proven that equivalent $\implies$ same topology, but the other direction is causing me trouble. I've been attempting a reductio, supposing that for some open $A$ we have $\inf_{x\in A} \{m>0: m\lVert x \rVert_2 \leq \lVert x \rVert_1\}=0$. I don't see any contradiction here, though.","This question already has answers here : Are two norms equivalent if they induce the same topology on a vector space? (2 answers) Closed 8 years ago . Note that by ""generate the same topology"" we mean that any set that is open with respect to $\lVert\cdot\rVert_1$ is also open with respect to $\lVert\cdot\rVert_2$ and vice versa. By ""equivalent"" we mean that $\exists m,M>0$ such that $m\lVert x \rVert_2\leq \lVert x \rVert_1 \leq M\lVert x \rVert_2$. This is part of an old preliminary exam in Analysis I'm using to prep for my own prelim. I have already proven that equivalent $\implies$ same topology, but the other direction is causing me trouble. I've been attempting a reductio, supposing that for some open $A$ we have $\inf_{x\in A} \{m>0: m\lVert x \rVert_2 \leq \lVert x \rVert_1\}=0$. I don't see any contradiction here, though.",,"['real-analysis', 'analysis', 'normed-spaces']"
86,$\lim a_n = 0$ if $\left|\frac{a_{n +1}}{a_n}\right|\to L < 1$,if,\lim a_n = 0 \left|\frac{a_{n +1}}{a_n}\right|\to L < 1,"Suppose that $a_n \neq 0$, for every n and that $L = \lim |\frac{a_{n +1}}{a_n}|$ exists. Show that if L < 1, then $\lim a_n = 0$. What I did so far: If L < 1 and $L = \lim |\frac{a_{n +1}}{a_n}|$, there exists $n_0$ such that for $n \geq n_0$, $0 < |a_{n+1}| < |a_n|$. That means that the sequence $(|a_n|)_{n \geq n_0}$ is decreasing. Consider the set $S = \{ |a_0|, ..., |a_{n_0} \}$. S is finite. Let $\beta = \max_{0 \leq i \leq n_0} |a_ i|$. Thus, $(|a_n|)$ is limited (because, for every n, $0 \leq |a_n| \leq \beta).$ Now I have that $(|a_n|)$ is limited and, throwing away a finite number of terms (the $n_0$ firsts) I can assume that it is decreasing. So I know that $(|a_n|) converges. How can I prove that it converges to $0$? I also know that if I prove that $ \lim |a_n| = 0$, then I have that $ \lim a_n = 0$.","Suppose that $a_n \neq 0$, for every n and that $L = \lim |\frac{a_{n +1}}{a_n}|$ exists. Show that if L < 1, then $\lim a_n = 0$. What I did so far: If L < 1 and $L = \lim |\frac{a_{n +1}}{a_n}|$, there exists $n_0$ such that for $n \geq n_0$, $0 < |a_{n+1}| < |a_n|$. That means that the sequence $(|a_n|)_{n \geq n_0}$ is decreasing. Consider the set $S = \{ |a_0|, ..., |a_{n_0} \}$. S is finite. Let $\beta = \max_{0 \leq i \leq n_0} |a_ i|$. Thus, $(|a_n|)$ is limited (because, for every n, $0 \leq |a_n| \leq \beta).$ Now I have that $(|a_n|)$ is limited and, throwing away a finite number of terms (the $n_0$ firsts) I can assume that it is decreasing. So I know that $(|a_n|) converges. How can I prove that it converges to $0$? I also know that if I prove that $ \lim |a_n| = 0$, then I have that $ \lim a_n = 0$.",,"['real-analysis', 'analysis']"
87,"Cauchy sequence in $L^p$, existence of a set with finite measure, and integral is less than epsilon over the complement","Cauchy sequence in , existence of a set with finite measure, and integral is less than epsilon over the complement",L^p,"The setting is as follows, for each $f\in L^p$, define a set function $\lambda_f$ on $\mathcal{A}$ by $$\lambda_f(E)=\left(\int_E |f|^p d\mu\right)^{1/p}=\|f\cdot I_E\|_p.$$ Let $(f_n)$ be a Cauchy sequence in $L^p$. Show that for all $\epsilon >0$, there exists a set $E\in\mathcal{A}$ (depending on $\epsilon$) with $\mu (E)<\infty$ such that for all $F\in\mathcal{A}$ with $F\subseteq E^c$, we have $\lambda_{f_n} (F)<\epsilon$ for all $n\in\mathbb{N}$. I tried proving by contradiction: Suppose to the contrary for all $E\in\mathcal{A}$ with $\mu(E)<\infty$, $\exists F\in\mathcal{A}$ with $F\subseteq E^c$ but $\lambda_{f_n}(F)\geq\epsilon$ for all $n\in\mathbb{N}$. Intuitively, I see that that may be a problem as $\mu(E)$ gets larger, $\lambda_{f_n}(F)$ should tend towards zero? However I don't know how to prove it rigorously. Thanks for any help. Note: I set a bounty to attract more answers, so that I can choose one that I can understand. Having some problems with understanding this one.","The setting is as follows, for each $f\in L^p$, define a set function $\lambda_f$ on $\mathcal{A}$ by $$\lambda_f(E)=\left(\int_E |f|^p d\mu\right)^{1/p}=\|f\cdot I_E\|_p.$$ Let $(f_n)$ be a Cauchy sequence in $L^p$. Show that for all $\epsilon >0$, there exists a set $E\in\mathcal{A}$ (depending on $\epsilon$) with $\mu (E)<\infty$ such that for all $F\in\mathcal{A}$ with $F\subseteq E^c$, we have $\lambda_{f_n} (F)<\epsilon$ for all $n\in\mathbb{N}$. I tried proving by contradiction: Suppose to the contrary for all $E\in\mathcal{A}$ with $\mu(E)<\infty$, $\exists F\in\mathcal{A}$ with $F\subseteq E^c$ but $\lambda_{f_n}(F)\geq\epsilon$ for all $n\in\mathbb{N}$. Intuitively, I see that that may be a problem as $\mu(E)$ gets larger, $\lambda_{f_n}(F)$ should tend towards zero? However I don't know how to prove it rigorously. Thanks for any help. Note: I set a bounty to attract more answers, so that I can choose one that I can understand. Having some problems with understanding this one.",,"['real-analysis', 'analysis', 'measure-theory']"
88,$\int_a^b |f|=0\implies f=0$,,\int_a^b |f|=0\implies f=0,"Let $f:[a,b]\to\mathbb R$ continuous. I want to show that $\int_a^b |f|=0\implies f=0$. By contradiction, suppose $f\neq 0$ and denote $A=\{x\mid f(x)\neq 0\}$. We have that $$0=\int_{[a,b]}|f|=\int_{[a,b]\backslash A}|f|+\int_A|f|\implies \underbrace{\int_{[a,b]\backslash A}|f|}_{\geq 0}=\underbrace{-\int_A|f|}_{\leq 0},$$ which is a contradiction. Therefore $|f|=0$ and thus $f=0$. Is it correct ? (the problem is that I didn't use the continuity, and I know that for a non-continuous function, we can have $\int_a^b|f|=0$ and $f\neq 0$) . Do you have other proofs ?","Let $f:[a,b]\to\mathbb R$ continuous. I want to show that $\int_a^b |f|=0\implies f=0$. By contradiction, suppose $f\neq 0$ and denote $A=\{x\mid f(x)\neq 0\}$. We have that $$0=\int_{[a,b]}|f|=\int_{[a,b]\backslash A}|f|+\int_A|f|\implies \underbrace{\int_{[a,b]\backslash A}|f|}_{\geq 0}=\underbrace{-\int_A|f|}_{\leq 0},$$ which is a contradiction. Therefore $|f|=0$ and thus $f=0$. Is it correct ? (the problem is that I didn't use the continuity, and I know that for a non-continuous function, we can have $\int_a^b|f|=0$ and $f\neq 0$) . Do you have other proofs ?",,['analysis']
89,porous sets: why measure zero?,porous sets: why measure zero?,,"We call a measurable set $E\subset\mathbb R^N$ porous if every ball $B_r(x)$ contains a smaller ball $B_{cr}(y)$ for some $c\in(0,1)$ such that $$ B_{cr}(y)\subset B_r(x)\setminus E. $$ So I've read in my book that all porous sets have Lebesgue-measure zero. Why does this hold? Add: I do know $\limsup_{r\to0}\frac{|E\cap B_r(z)|}{|B_r|}<1$ for some $z\in E$.","We call a measurable set $E\subset\mathbb R^N$ porous if every ball $B_r(x)$ contains a smaller ball $B_{cr}(y)$ for some $c\in(0,1)$ such that $$ B_{cr}(y)\subset B_r(x)\setminus E. $$ So I've read in my book that all porous sets have Lebesgue-measure zero. Why does this hold? Add: I do know $\limsup_{r\to0}\frac{|E\cap B_r(z)|}{|B_r|}<1$ for some $z\in E$.",,['real-analysis']
90,Find all continuous functions $f(x)^2=x^2$,Find all continuous functions,f(x)^2=x^2,"Find all functions $f$ which are continuous on $\mathbb R$ and which satisfy the equation $f(x)^2=x^2$ for all $x \in \mathbb R$. Clearly $f(x)=x, -x, |x|, -|x|$ all satisfy the condition. However, how can I show that these must be the only possible choices? The condition guarantees that $|f(x)|=|x|$, for all $x$ so I think it's quite obvious that these four choices are the only possibilities. But I don't see why continuity is necessary. If $f$ does not need to be continuous, are there other possibilities? Then how can I use continuity to guarantee that these are the only choices?","Find all functions $f$ which are continuous on $\mathbb R$ and which satisfy the equation $f(x)^2=x^2$ for all $x \in \mathbb R$. Clearly $f(x)=x, -x, |x|, -|x|$ all satisfy the condition. However, how can I show that these must be the only possible choices? The condition guarantees that $|f(x)|=|x|$, for all $x$ so I think it's quite obvious that these four choices are the only possibilities. But I don't see why continuity is necessary. If $f$ does not need to be continuous, are there other possibilities? Then how can I use continuity to guarantee that these are the only choices?",,"['calculus', 'real-analysis', 'analysis', 'continuity']"
91,"Suppose $\int_{[a,b]}f=0,\text{ then }f(x)=0 \forall x\in[a,b]$",Suppose,"\int_{[a,b]}f=0,\text{ then }f(x)=0 \forall x\in[a,b]","Let $a<b$ be real numbers. Let $f:[a,b]\to\mathbb{R}$ be a continuous non-negative function. Suppose $\int_{[a,b]}f=0,\text{ then }f(x)=0 \forall x\in[a,b]$ Proof: Suppose for the sake of contradiction $\exists x_0\in[a,b] \text{ such that } f(x_0)= D> 0$ Since the function is continuous at $x_0$, we have $$\forall \epsilon>0, \exists \delta>0 \text{ such that } |f(x)-f(x_0)|\leq\epsilon\text{ whenever } x\in[a,b]\cap(x_0-\delta,x_0+\delta)$$ choose $\delta=\delta_1$ such that $\epsilon=D/2$, then we can say: $$-D/2 \leq f(x)-f(x_0)\leq D/2$$ $$-D/2 \leq f(x) - D$$ $$f(x)\geq D/2>0$$ This implies that $\int_{[a,b]\cap(x_0-\delta_1,x_0+\delta_1)}f >0$ Now since $\int_{[a,b]} f = 0$. we have $$\int_{[a,b]}f=\int_{[a,x_0-\delta_1]}f+ \int_{(x_0-\delta_1, x_0+\delta_1)} f + \int_{[x_0+\delta_1,b]}f=0$$ $$\int_{[a,x_0-\delta_1]}f+\int_{[x_0+\delta_1,b]}f<-D/2,$$ but this is a contradiction since $f(x)\geq 0$ and hence $\int_{[a,x_0-\delta_1]}f+\int_{[x_0+\delta_1,b]}f\geq 0$ Is my proof correct?","Let $a<b$ be real numbers. Let $f:[a,b]\to\mathbb{R}$ be a continuous non-negative function. Suppose $\int_{[a,b]}f=0,\text{ then }f(x)=0 \forall x\in[a,b]$ Proof: Suppose for the sake of contradiction $\exists x_0\in[a,b] \text{ such that } f(x_0)= D> 0$ Since the function is continuous at $x_0$, we have $$\forall \epsilon>0, \exists \delta>0 \text{ such that } |f(x)-f(x_0)|\leq\epsilon\text{ whenever } x\in[a,b]\cap(x_0-\delta,x_0+\delta)$$ choose $\delta=\delta_1$ such that $\epsilon=D/2$, then we can say: $$-D/2 \leq f(x)-f(x_0)\leq D/2$$ $$-D/2 \leq f(x) - D$$ $$f(x)\geq D/2>0$$ This implies that $\int_{[a,b]\cap(x_0-\delta_1,x_0+\delta_1)}f >0$ Now since $\int_{[a,b]} f = 0$. we have $$\int_{[a,b]}f=\int_{[a,x_0-\delta_1]}f+ \int_{(x_0-\delta_1, x_0+\delta_1)} f + \int_{[x_0+\delta_1,b]}f=0$$ $$\int_{[a,x_0-\delta_1]}f+\int_{[x_0+\delta_1,b]}f<-D/2,$$ but this is a contradiction since $f(x)\geq 0$ and hence $\int_{[a,x_0-\delta_1]}f+\int_{[x_0+\delta_1,b]}f\geq 0$ Is my proof correct?",,"['calculus', 'real-analysis', 'analysis', 'proof-verification', 'epsilon-delta']"
92,Taylor series of $\arctan(x+2)$ at $x=\infty$,Taylor series of  at,\arctan(x+2) x=\infty,"The simple question is: what is the correct way to calculate the series expansion of $\arctan(x+2)$ at $x=\infty$ without strange (and maybe wrong) tricks? Read further only if you want more details. The first (obvious) problem is that infinite is not a number, thus $\sum_{k=0}^{\infty}\frac{f^{(k)}(\infty)}{k!}(x-\infty)^k$ doesn't make sense. So I've tried to solve $\lim_{x_0\to \infty} ({\sum_{k=0}^{\infty}\frac{f^{(k)}(x_0)}{k!}(x-x_0)^k})$ one term at a time. The first term (with $k=0$) can be easily calculated and the second term is: $$\lim_{x_0\to \infty} (\frac{x-x_0}{1+(x_0+4x_0+4)} \sim -\frac{x_0}{x_0^2}=\frac{-1}{x_0}) = 0$$ The asymptotic function $\frac{-1}{x_0}$ is similar to what I should get as the second term (which is $\frac{-1}{x}$). But to get the correct result I should do some dangerous stuff: I should replace $x_0$ with $x$ (why?) and do not calculate the limit (otherwise I cannot get the desired precision of the series expansion). I've also tried to solve $\arctan(1/t+2)$ with $t=0$, but the argument of arctg is still an infinite and the difficulties I encounter are the same. Is there any way to calculate the series expansion of $\arctan(x+2)$ at $x=\infty$ in a cleaner way without all these problems?","The simple question is: what is the correct way to calculate the series expansion of $\arctan(x+2)$ at $x=\infty$ without strange (and maybe wrong) tricks? Read further only if you want more details. The first (obvious) problem is that infinite is not a number, thus $\sum_{k=0}^{\infty}\frac{f^{(k)}(\infty)}{k!}(x-\infty)^k$ doesn't make sense. So I've tried to solve $\lim_{x_0\to \infty} ({\sum_{k=0}^{\infty}\frac{f^{(k)}(x_0)}{k!}(x-x_0)^k})$ one term at a time. The first term (with $k=0$) can be easily calculated and the second term is: $$\lim_{x_0\to \infty} (\frac{x-x_0}{1+(x_0+4x_0+4)} \sim -\frac{x_0}{x_0^2}=\frac{-1}{x_0}) = 0$$ The asymptotic function $\frac{-1}{x_0}$ is similar to what I should get as the second term (which is $\frac{-1}{x}$). But to get the correct result I should do some dangerous stuff: I should replace $x_0$ with $x$ (why?) and do not calculate the limit (otherwise I cannot get the desired precision of the series expansion). I've also tried to solve $\arctan(1/t+2)$ with $t=0$, but the argument of arctg is still an infinite and the difficulties I encounter are the same. Is there any way to calculate the series expansion of $\arctan(x+2)$ at $x=\infty$ in a cleaner way without all these problems?",,"['analysis', 'asymptotics', 'taylor-expansion']"
93,"Give an example of a continuous function $f : [0, ∞) \to [0, ∞)$ such that $\int_{0}^{\infty}f(x)dx$ exists but $f$ is unbounded.",Give an example of a continuous function  such that  exists but  is unbounded.,"f : [0, ∞) \to [0, ∞) \int_{0}^{\infty}f(x)dx f","Give an example of a continuous function $f : [0,  ∞) \to [0, ∞)$ such that $\int_{0}^{\infty}f(x)dx$ exists but $f$ is unbounded. I have been thinking about this. And I have come to the conclusion that I will need to construct a function, $f$ , such that $f$ is a sequence of triangles of increasing height, but decreasing base. I obviously need $f$ such that both the height of the triangles and the sum of the bases tend to infinity. But I also need that the $\sum (\text{height} \times \text{base}) \leq \infty $","Give an example of a continuous function such that exists but is unbounded. I have been thinking about this. And I have come to the conclusion that I will need to construct a function, , such that is a sequence of triangles of increasing height, but decreasing base. I obviously need such that both the height of the triangles and the sum of the bases tend to infinity. But I also need that the","f : [0,  ∞) \to [0, ∞) \int_{0}^{\infty}f(x)dx f f f f \sum (\text{height} \times \text{base}) \leq \infty ","['real-analysis', 'analysis']"
94,"Why they use the word ""abstract"" for naming mathematical fields of study?","Why they use the word ""abstract"" for naming mathematical fields of study?",,"I've found some books with titles such as abstract analysis - but I don't understand why they choose such word. For me it seems quite vague and perhaps misleading - consider the examples: Real Analysis Complex Analysis In this case, it seems to be clear what they are dealing with: Real and Complex numbers and also some structures that emerges from them. I've already seen books that were called abstract analysis and covered real and complex analysis and also other books that  had the same name and covered measure theory. It's the same for abstract algebra but in this case, it seems to be fixed to concepts such as rings, fields, monoids, etc. This question may be an invite to a discussion, but it is not: I am really trying to understand what's the usage of the word abstract - I'm not really sure if it's really employed in this vague broad sense or if there's something I still don't know.","I've found some books with titles such as abstract analysis - but I don't understand why they choose such word. For me it seems quite vague and perhaps misleading - consider the examples: Real Analysis Complex Analysis In this case, it seems to be clear what they are dealing with: Real and Complex numbers and also some structures that emerges from them. I've already seen books that were called abstract analysis and covered real and complex analysis and also other books that  had the same name and covered measure theory. It's the same for abstract algebra but in this case, it seems to be fixed to concepts such as rings, fields, monoids, etc. This question may be an invite to a discussion, but it is not: I am really trying to understand what's the usage of the word abstract - I'm not really sure if it's really employed in this vague broad sense or if there's something I still don't know.",,"['abstract-algebra', 'analysis', 'terminology']"
95,Mathematical Analysis advice,Mathematical Analysis advice,,"Claim: Let $\delta>0, n\in N.  $ Then $\lim_{n\rightarrow\infty} I_{n} $exists, where $ I_{n}=\int_{0}^{\delta} \frac{\sin\ nx}{x}  dx  $ Proof: $f(x) =\frac{\sin\ nx}{x}$ has a removable discontinuity at $x=0$ and so we let $f(0) =n$ $x = \frac{t}{n}$ is continuous and monotone on $t\in[0,n\delta]$, hence, $ I_{n}=\int_{0}^{n\delta} \frac{\sin\ t}{t}  dt  $ For all  $p\geq 1$, given any $\epsilon \gneq 0,  \exists n_{0} \gneq \frac{\epsilon}{2\delta}$ such that $\forall n\gneq n_{0}$,   $|I_{n+p}-I_{n}| =  \left\lvert\int_{n\delta}^{(n+p)\delta} \frac{\sin\ t}{t}  dt \right\lvert  \leq \int_{n\delta}^{(n+p)\delta} \frac{\left\lvert\sin t\ \right\lvert}{t}  dt = \frac{1} {n\delta}\int_{n\delta}^{M} \left\lvert\sin t\right\lvert dt \ \leq \frac{2} {n\delta} \lneq \epsilon$ ,  for some $M \in [n\delta, (n+p)\delta]$, by Bonnet's  theorem Is my proof valid? Thank you.","Claim: Let $\delta>0, n\in N.  $ Then $\lim_{n\rightarrow\infty} I_{n} $exists, where $ I_{n}=\int_{0}^{\delta} \frac{\sin\ nx}{x}  dx  $ Proof: $f(x) =\frac{\sin\ nx}{x}$ has a removable discontinuity at $x=0$ and so we let $f(0) =n$ $x = \frac{t}{n}$ is continuous and monotone on $t\in[0,n\delta]$, hence, $ I_{n}=\int_{0}^{n\delta} \frac{\sin\ t}{t}  dt  $ For all  $p\geq 1$, given any $\epsilon \gneq 0,  \exists n_{0} \gneq \frac{\epsilon}{2\delta}$ such that $\forall n\gneq n_{0}$,   $|I_{n+p}-I_{n}| =  \left\lvert\int_{n\delta}^{(n+p)\delta} \frac{\sin\ t}{t}  dt \right\lvert  \leq \int_{n\delta}^{(n+p)\delta} \frac{\left\lvert\sin t\ \right\lvert}{t}  dt = \frac{1} {n\delta}\int_{n\delta}^{M} \left\lvert\sin t\right\lvert dt \ \leq \frac{2} {n\delta} \lneq \epsilon$ ,  for some $M \in [n\delta, (n+p)\delta]$, by Bonnet's  theorem Is my proof valid? Thank you.",,"['analysis', 'proof-verification']"
96,"Proof on showing if F(x,y,z)=0 then product of partial derivatives (evaluated at an assigned coordinate) is -1","Proof on showing if F(x,y,z)=0 then product of partial derivatives (evaluated at an assigned coordinate) is -1",,"The task is as follows: Given: $$F(x,y,z) = 0$$ Goal: Show    $\frac{\partial  z}{\partial y}|_x \frac{\partial  y}{\partial x}|_z \frac{\partial x}{\partial z} |_y = -1$ Here is my work so far: (1) Differentiate with respect to y, I get: $0 + F_2 + F_3  \frac{\partial z}{\partial y} = 0$ So $ F_3  \frac{\partial z}{\partial y} = - F_2$ (2) Differentiate with respect to x, I get: $F_1 + F_2 \frac{\partial y}{\partial x}  + 0 = 0$ So $ F_2  \frac{\partial y}{\partial x} = - F_1$ (3) Differentiate with respect to z, I get: $F_1  \frac{\partial x}{\partial z}    + 0  + F_3 = 0$ So $ F_1  \frac{\partial x}{\partial z} = - F_3$ (4) After some manipulations with the $F_i$, I get to the conclusion that $\frac{\partial z}{\partial y}* \frac{\partial y}{\partial x} * \frac{\partial x}{\partial z}  = -1$, so when evaluated with x, z, y respectively, conclusion is still true My questions are: (1) Is my proof correct? (2) For example, when I differentiate with respect to y, I ""let"" $F_1$ be 0 and find partials for other coordinates. I had a hard time trying to explain to my friend on the reason(s) why I can do such ""let be 0"" thing.  Although I think if I can't do that, then there is no way that I can reach the conclusion, but I somehow feel confused about the fact too.  Since my book is doing it that way, my understanding is that I can do such ""let be 0"" thing based on the independece of x with respect to y, when I differentiate with respect to y.  But is my thought ok? Would someone please help me on this question? Thank you very much ^_^","The task is as follows: Given: $$F(x,y,z) = 0$$ Goal: Show    $\frac{\partial  z}{\partial y}|_x \frac{\partial  y}{\partial x}|_z \frac{\partial x}{\partial z} |_y = -1$ Here is my work so far: (1) Differentiate with respect to y, I get: $0 + F_2 + F_3  \frac{\partial z}{\partial y} = 0$ So $ F_3  \frac{\partial z}{\partial y} = - F_2$ (2) Differentiate with respect to x, I get: $F_1 + F_2 \frac{\partial y}{\partial x}  + 0 = 0$ So $ F_2  \frac{\partial y}{\partial x} = - F_1$ (3) Differentiate with respect to z, I get: $F_1  \frac{\partial x}{\partial z}    + 0  + F_3 = 0$ So $ F_1  \frac{\partial x}{\partial z} = - F_3$ (4) After some manipulations with the $F_i$, I get to the conclusion that $\frac{\partial z}{\partial y}* \frac{\partial y}{\partial x} * \frac{\partial x}{\partial z}  = -1$, so when evaluated with x, z, y respectively, conclusion is still true My questions are: (1) Is my proof correct? (2) For example, when I differentiate with respect to y, I ""let"" $F_1$ be 0 and find partials for other coordinates. I had a hard time trying to explain to my friend on the reason(s) why I can do such ""let be 0"" thing.  Although I think if I can't do that, then there is no way that I can reach the conclusion, but I somehow feel confused about the fact too.  Since my book is doing it that way, my understanding is that I can do such ""let be 0"" thing based on the independece of x with respect to y, when I differentiate with respect to y.  But is my thought ok? Would someone please help me on this question? Thank you very much ^_^",,"['analysis', 'multivariable-calculus']"
97,A counterexample on the existence of some sequence in Hilbert space,A counterexample on the existence of some sequence in Hilbert space,,"I want to find a uniformly bounded sequence $\{x_n\}$ in $l^2(\mathbb{C})$ such that $x_n$ does not converge to zero in weak topology, i.e., $\exists ~y\in l^2(\mathbb{C}),$ such that $\langle y, x_n\rangle\not\to 0$, but $\{x_n\}$ satisfies the following condition: $$\lim_m\lim_n\langle x_{n+m},x_n\rangle=0$$ or the stronger condition: $$\lim_n\langle x_{n+m},x_n\rangle=0, \forall m\geq 1.$$ Thanks in advance! Remarks: 1, Jacob Schlather has solved it for the case $\{x_n\}$ is not uniformly bounded, I have added the assumption that $\{x_n\}$ is uniformly bounded, which I forgot to add before. 2, This is one ''remark'' in page 85 of the book-- H.Furstenberg, Recurrence in ergodic theory and combinatorial number theory, Princeton Univ. Press, unless I misunderstand the meaning in the book. It says: ""It should be noted that the analogous result for ordinary convergence does not hold"". Lemma 4.9. Let $\{x_n\}$ be a bounded sequence of vectors in Hilbert space and suppose that $$D-\lim_m(D-\lim_n\langle x_{n+m}, x_n\rangle)=0$$ Then with respect to the weak topology, $$D-\lim_nx_n=0$$","I want to find a uniformly bounded sequence $\{x_n\}$ in $l^2(\mathbb{C})$ such that $x_n$ does not converge to zero in weak topology, i.e., $\exists ~y\in l^2(\mathbb{C}),$ such that $\langle y, x_n\rangle\not\to 0$, but $\{x_n\}$ satisfies the following condition: $$\lim_m\lim_n\langle x_{n+m},x_n\rangle=0$$ or the stronger condition: $$\lim_n\langle x_{n+m},x_n\rangle=0, \forall m\geq 1.$$ Thanks in advance! Remarks: 1, Jacob Schlather has solved it for the case $\{x_n\}$ is not uniformly bounded, I have added the assumption that $\{x_n\}$ is uniformly bounded, which I forgot to add before. 2, This is one ''remark'' in page 85 of the book-- H.Furstenberg, Recurrence in ergodic theory and combinatorial number theory, Princeton Univ. Press, unless I misunderstand the meaning in the book. It says: ""It should be noted that the analogous result for ordinary convergence does not hold"". Lemma 4.9. Let $\{x_n\}$ be a bounded sequence of vectors in Hilbert space and suppose that $$D-\lim_m(D-\lim_n\langle x_{n+m}, x_n\rangle)=0$$ Then with respect to the weak topology, $$D-\lim_nx_n=0$$",,"['analysis', 'hilbert-spaces']"
98,How can I prove the monotonicity of a function?,How can I prove the monotonicity of a function?,,Given is the function $$f(x) =\frac{x}{\sqrt{1-x^2}}$$ How can I prove that this function is monotonic and thus injective?,Given is the function $$f(x) =\frac{x}{\sqrt{1-x^2}}$$ How can I prove that this function is monotonic and thus injective?,,['analysis']
99,inequality with roots of unity,inequality with roots of unity,,"Do you know proofs or references for the following inequality: There exists a positive constant $C>0$ such that for any complex numbers $a_1,\ldots,a_n$ $$ |a_1|+\cdots+|a_n| \leq C\sup_{z_1^3=1,\ldots,z_n^3=1 } |a_1z_1+\cdots + a_n z_n| $$ where the supremum is taken over the complex numbers $z_1,\ldots,z_n$ such that $z_1^3=1,\ldots,z_n^3=1$?","Do you know proofs or references for the following inequality: There exists a positive constant $C>0$ such that for any complex numbers $a_1,\ldots,a_n$ $$ |a_1|+\cdots+|a_n| \leq C\sup_{z_1^3=1,\ldots,z_n^3=1 } |a_1z_1+\cdots + a_n z_n| $$ where the supremum is taken over the complex numbers $z_1,\ldots,z_n$ such that $z_1^3=1,\ldots,z_n^3=1$?",,"['analysis', 'reference-request', 'inequality', 'fourier-analysis', 'complex-numbers']"
