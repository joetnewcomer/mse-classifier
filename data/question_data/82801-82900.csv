,title_raw,title_text,title_latex,body_raw,body_text,body_latex,tags
0,identity of $(I-z^nT^n)^{-1} =\frac{1}{n}[(I-zT)^{-1}+(I-wzT)^{-1}+...+(I-w^{n-1}zT)^{-1}]$,identity of,(I-z^nT^n)^{-1} =\frac{1}{n}[(I-zT)^{-1}+(I-wzT)^{-1}+...+(I-w^{n-1}zT)^{-1}],"I am trying to understand the identity  $$(I-z^nT^n)^{-1} =\frac{1}{n}[(I-zT)^{-1}+(I-wzT)^{-1}+...+(I-w^{n-1}zT)^{-1}] \quad (*),$$ where $T \in \mathbb{C}^{n\times n},z\in \mathbb{C}$ and the spectral radius of $T$, $\rho(T)=\max\{|\lambda|: \exists v, Tv=\lambda v\}\leq 1$ and $|z|<1$ and $w$ is a primitive $n$th root of 1,i.e. $w =e^{i2\pi/n}$. I have tried using the identity   $$[I-A]^{-1} = I+A+A^2+A^3+... $$ which holds when $A^n$ becomes zero matrix. This  works as  $\rho(T)=\max\{|\lambda|: \exists v, Tv=\lambda v\}\leq 1$ and $|z|<1$ implies the convergence of $A^n$. So I expand both sides of $(*)$ and then compared the coefficient of the two power series. But this approach seems to be a bit tedious, I wonder whether there is a more clean and simpler proof.","I am trying to understand the identity  $$(I-z^nT^n)^{-1} =\frac{1}{n}[(I-zT)^{-1}+(I-wzT)^{-1}+...+(I-w^{n-1}zT)^{-1}] \quad (*),$$ where $T \in \mathbb{C}^{n\times n},z\in \mathbb{C}$ and the spectral radius of $T$, $\rho(T)=\max\{|\lambda|: \exists v, Tv=\lambda v\}\leq 1$ and $|z|<1$ and $w$ is a primitive $n$th root of 1,i.e. $w =e^{i2\pi/n}$. I have tried using the identity   $$[I-A]^{-1} = I+A+A^2+A^3+... $$ which holds when $A^n$ becomes zero matrix. This  works as  $\rho(T)=\max\{|\lambda|: \exists v, Tv=\lambda v\}\leq 1$ and $|z|<1$ implies the convergence of $A^n$. So I expand both sides of $(*)$ and then compared the coefficient of the two power series. But this approach seems to be a bit tedious, I wonder whether there is a more clean and simpler proof.",,"['complex-analysis', 'matrices', 'complex-numbers', 'matrix-equations']"
1,Matrix equation $aX^{3} + bX^{2} = I$.,Matrix equation .,aX^{3} + bX^{2} = I,"I want to solve the matrix equation for $X$ $$aX^{3} + bX^{2} = I,$$ where $a,b \in \mathbb{R}$ and $X \in \mathbb{R}^{n\times n}$. My thoughts: If $a = 0$ or $b = 0$, the solution is easy. If $a, b \neq 0$ I have tried to find matrices $U$ and $V$ such that $X^3 =UD_3V$ and  $X^2 =UD_2V$, where $D_2$ and $D_3$ are diagonal matrices, but I did not have success until now. Maybe this is not possible. Do you have any idea? Thanks in advance!","I want to solve the matrix equation for $X$ $$aX^{3} + bX^{2} = I,$$ where $a,b \in \mathbb{R}$ and $X \in \mathbb{R}^{n\times n}$. My thoughts: If $a = 0$ or $b = 0$, the solution is easy. If $a, b \neq 0$ I have tried to find matrices $U$ and $V$ such that $X^3 =UD_3V$ and  $X^2 =UD_2V$, where $D_2$ and $D_3$ are diagonal matrices, but I did not have success until now. Maybe this is not possible. Do you have any idea? Thanks in advance!",,"['matrices', 'matrix-equations']"
2,Connected components $0-1$ matrices,Connected components  matrices,0-1,"Let $M$ be a $0-1$ matrix. Here a matrix has one component means we can traverse from a matrix entry $(i,j)$ which is $1$ to any other one by moving step of $(i\pm1,j),(i,j\pm1),(i\pm1,j\pm1)$ where each step you take you step on another $1$. Can every $0-1$ be converted to a matrix of one component by permutations of rows and columns? What if you only have steps $(i\pm1,j),(i,j\pm1)$? What classes of matrices cannot have one component? also posted: https://mathoverflow.net/questions/190981/connected-components-0-1-matrices","Let $M$ be a $0-1$ matrix. Here a matrix has one component means we can traverse from a matrix entry $(i,j)$ which is $1$ to any other one by moving step of $(i\pm1,j),(i,j\pm1),(i\pm1,j\pm1)$ where each step you take you step on another $1$. Can every $0-1$ be converted to a matrix of one component by permutations of rows and columns? What if you only have steps $(i\pm1,j),(i,j\pm1)$? What classes of matrices cannot have one component? also posted: https://mathoverflow.net/questions/190981/connected-components-0-1-matrices",,"['matrices', 'algebraic-topology']"
3,Rank(AB) and Rank(BA)?,Rank(AB) and Rank(BA)?,,$A$ is a matrix of size $n\times r$. $B$ is a matrix of size $r\times n$. The rank of $A$ and $B$ are both equal to $r$.  Assuming $r < n$. My question is: $\def\rank{\operatorname{rank}}\rank(AB) = \rank(BA)$? PS: It's easy to prove that $\rank(AB)=r$.,$A$ is a matrix of size $n\times r$. $B$ is a matrix of size $r\times n$. The rank of $A$ and $B$ are both equal to $r$.  Assuming $r < n$. My question is: $\def\rank{\operatorname{rank}}\rank(AB) = \rank(BA)$? PS: It's easy to prove that $\rank(AB)=r$.,,"['linear-algebra', 'matrices', 'matrix-rank']"
4,Is there always a mapping from invertible $A$ to any $B \in M_n(\Bbb R)$?,Is there always a mapping from invertible  to any ?,A B \in M_n(\Bbb R),"Let $A, B$ be $n\times n$ matrices, then $1)$ If $A$ is invertible then for every $B$ exists a matrix $X \in M_n(\Bbb R)$ such that $AX = B$. $2)$ If for every $B$ there exists a matrix $X \in M_n(\Bbb R)$ such that $AX = B$ then $A$ is invertible. For $1)$ I started with: $AX=B \implies X=A^{-1}B$ But I'm not sure I can do this: $$A(A^{-1}B)=B$$ $$(AA^{-1})B=B$$ --> not sure about this $$ IB = B $$ Am I right about the first part? How should I prove the second part? thanks","Let $A, B$ be $n\times n$ matrices, then $1)$ If $A$ is invertible then for every $B$ exists a matrix $X \in M_n(\Bbb R)$ such that $AX = B$. $2)$ If for every $B$ there exists a matrix $X \in M_n(\Bbb R)$ such that $AX = B$ then $A$ is invertible. For $1)$ I started with: $AX=B \implies X=A^{-1}B$ But I'm not sure I can do this: $$A(A^{-1}B)=B$$ $$(AA^{-1})B=B$$ --> not sure about this $$ IB = B $$ Am I right about the first part? How should I prove the second part? thanks",,"['linear-algebra', 'matrices']"
5,How many permutations do we need before we're in $SU\left( n\right)$?,How many permutations do we need before we're in ?,SU\left( n\right),"Let $\mathcal{L}\subseteq \mathfrak{su}\left( n\right)$ be a Lie algebra for $n \geq 2$ with Lie group $G = e^{\mathcal L}$, and let $X \in G$ be represented by an $n\times n$ matrix (I prefer fixing a basis to work with something tangible). Suppose $P_1, P_2, \ldots , P_k \in G$ are permutation matrices. What are the necessary conditions on $\left\{ P_k\right\}$ to guarantee that $G = SU\left( n\right)$? Do such conditions even exist? This question arises in the context of a larger control problem as follows: Let $n = 4$ and $G \subsetneq SU\left( n\right)$ by assumption. Let the group action on $X_1, X_2 \in G$ be the matrix product $X_1 X_2$. Suppose we can generate any element of a group $G_1 \subset G$: $${G_1} = SU\left( 3 \right) \oplus 1 = \left\{ {X = \left[ {\begin{array}{*{20}{c}}   U&{} \\    {}&1  \end{array}} \right],\,\,U \in SU\left( 3 \right)} \right\}$$ and let us consider the group $G_2 \subset G$: $${G_2} = \left\{ {Y = \left[ {\begin{array}{*{20}{c}}   {{y_{11}}}&{{y_{12}}}&{}&{{y_{13}}} \\    {{y_{21}}}&{{y_{22}}}&{}&{{y_{23}}} \\    {}&{}&1&{} \\    {{y_{31}}}&{{y_{32}}}&{}&{{y_{33}}}  \end{array}} \right],\,\,\left[ {\begin{array}{*{20}{c}}   {{y_{11}}}&{{y_{12}}}&{{y_{13}}} \\    {{y_{21}}}&{{y_{22}}}&{{y_{23}}} \\    {{y_{31}}}&{{y_{32}}}&{{y_{33}}}  \end{array}} \right] \in V} \right\},\,\,V \subseteq SU\left( 3 \right)$$ It is easily seen that if we can generate the permutation matrix $${P_{\left( {3,4} \right)}} = \left[ {\begin{array}{*{20}{c}}   1&{}&{}&{} \\    {}&1&{}&{} \\    {}&{}&{}&1 \\    {}&{}&1&{}  \end{array}} \right]$$ then we can generate elements of $G_2$ such that $V = SU\left( 3\right)$. In general, the converse doesn't hold. That is, $V = SU\left( 3\right) \not\Rightarrow P_{\left( {3,4} \right)} \in G$. We see that $G_1$ ""comprises"" rows $1, 2, 3$, and $G_2$ comprises rows $1,2,4$. This concepts extends to general $n$, for groups comprising $2 \leq m < n$ rows, where we call such groups ""$m$-subgroups"" of $G$. An identical derivation shows that if any such $m$-subgroup $G_a \sim SU\left( m\right)$ can be generated for row tuple $a = \left( a_1, a_2, \ldots , a_m\right)$, along with the permutation matrix $P_{a\rightarrow b}$, this implies that $m$-subgroup $G_b$ for row tuple $b = \left( b_1, b_2, \ldots , b_m\right)$ is also isomorphic to $SU\left( m\right)$. In this way, the $SU$-isomorphism of an $m$-subgroup extends to all $m$-subgroups we can reach via the set of permutations we can generate in $G$. Thus if we know the group $P$ of permutations that can be generated in $G$ (or in particular, a set of permutations $\left\{ P_k\right\}$ that generates this group), this saves us a considerable bit of work in checking each $m$-subgroup for isomorphism. In the process of systematically determining $\left\{ P_k\right\}$, the issue then arises that we've assumed $G \subsetneq SU\left( n\right)$, hence if the set of $P_1, P_2, \ldots , P_k \in G$ implies $G = SU\left( n\right)$ at any point during the construction, we've violated our assumption. In the context of this control problem, this means that determining $SU$-isomorphism of the $m$-groups is moot and we needn't bother checking.","Let $\mathcal{L}\subseteq \mathfrak{su}\left( n\right)$ be a Lie algebra for $n \geq 2$ with Lie group $G = e^{\mathcal L}$, and let $X \in G$ be represented by an $n\times n$ matrix (I prefer fixing a basis to work with something tangible). Suppose $P_1, P_2, \ldots , P_k \in G$ are permutation matrices. What are the necessary conditions on $\left\{ P_k\right\}$ to guarantee that $G = SU\left( n\right)$? Do such conditions even exist? This question arises in the context of a larger control problem as follows: Let $n = 4$ and $G \subsetneq SU\left( n\right)$ by assumption. Let the group action on $X_1, X_2 \in G$ be the matrix product $X_1 X_2$. Suppose we can generate any element of a group $G_1 \subset G$: $${G_1} = SU\left( 3 \right) \oplus 1 = \left\{ {X = \left[ {\begin{array}{*{20}{c}}   U&{} \\    {}&1  \end{array}} \right],\,\,U \in SU\left( 3 \right)} \right\}$$ and let us consider the group $G_2 \subset G$: $${G_2} = \left\{ {Y = \left[ {\begin{array}{*{20}{c}}   {{y_{11}}}&{{y_{12}}}&{}&{{y_{13}}} \\    {{y_{21}}}&{{y_{22}}}&{}&{{y_{23}}} \\    {}&{}&1&{} \\    {{y_{31}}}&{{y_{32}}}&{}&{{y_{33}}}  \end{array}} \right],\,\,\left[ {\begin{array}{*{20}{c}}   {{y_{11}}}&{{y_{12}}}&{{y_{13}}} \\    {{y_{21}}}&{{y_{22}}}&{{y_{23}}} \\    {{y_{31}}}&{{y_{32}}}&{{y_{33}}}  \end{array}} \right] \in V} \right\},\,\,V \subseteq SU\left( 3 \right)$$ It is easily seen that if we can generate the permutation matrix $${P_{\left( {3,4} \right)}} = \left[ {\begin{array}{*{20}{c}}   1&{}&{}&{} \\    {}&1&{}&{} \\    {}&{}&{}&1 \\    {}&{}&1&{}  \end{array}} \right]$$ then we can generate elements of $G_2$ such that $V = SU\left( 3\right)$. In general, the converse doesn't hold. That is, $V = SU\left( 3\right) \not\Rightarrow P_{\left( {3,4} \right)} \in G$. We see that $G_1$ ""comprises"" rows $1, 2, 3$, and $G_2$ comprises rows $1,2,4$. This concepts extends to general $n$, for groups comprising $2 \leq m < n$ rows, where we call such groups ""$m$-subgroups"" of $G$. An identical derivation shows that if any such $m$-subgroup $G_a \sim SU\left( m\right)$ can be generated for row tuple $a = \left( a_1, a_2, \ldots , a_m\right)$, along with the permutation matrix $P_{a\rightarrow b}$, this implies that $m$-subgroup $G_b$ for row tuple $b = \left( b_1, b_2, \ldots , b_m\right)$ is also isomorphic to $SU\left( m\right)$. In this way, the $SU$-isomorphism of an $m$-subgroup extends to all $m$-subgroups we can reach via the set of permutations we can generate in $G$. Thus if we know the group $P$ of permutations that can be generated in $G$ (or in particular, a set of permutations $\left\{ P_k\right\}$ that generates this group), this saves us a considerable bit of work in checking each $m$-subgroup for isomorphism. In the process of systematically determining $\left\{ P_k\right\}$, the issue then arises that we've assumed $G \subsetneq SU\left( n\right)$, hence if the set of $P_1, P_2, \ldots , P_k \in G$ implies $G = SU\left( n\right)$ at any point during the construction, we've violated our assumption. In the context of this control problem, this means that determining $SU$-isomorphism of the $m$-groups is moot and we needn't bother checking.",,"['group-theory', 'matrices', 'permutations', 'lie-groups']"
6,What is the Moore-Penrose pseudoinverse for a hermitian block-matrix with one zero block?,What is the Moore-Penrose pseudoinverse for a hermitian block-matrix with one zero block?,,"Given a block matrix of the form \begin{pmatrix}   A & B^* \\ B & 0 \end{pmatrix} where $A$ is singular (otherwise one could simply use the well-known block matrix inverse), is there a simpler formula for the Moore-Penrose inverse than the general one ?","Given a block matrix of the form \begin{pmatrix}   A & B^* \\ B & 0 \end{pmatrix} where $A$ is singular (otherwise one could simply use the well-known block matrix inverse), is there a simpler formula for the Moore-Penrose inverse than the general one ?",,"['matrices', 'inverse', 'pseudoinverse']"
7,$A^TCA \leq B^TCB \Rightarrow A^TA \leq B^TB$?,?,A^TCA \leq B^TCB \Rightarrow A^TA \leq B^TB,"Let $A$,$B \in \mathbb{R}^n$, $C\in\mathbb{R}^{n\times n}$, and $C=D^TD$ where $D$ is a $n\times n$ psd matrix, is it guaranteed that $A^TCA \leq B^TCB \Rightarrow A^TA \leq B^TB$?","Let $A$,$B \in \mathbb{R}^n$, $C\in\mathbb{R}^{n\times n}$, and $C=D^TD$ where $D$ is a $n\times n$ psd matrix, is it guaranteed that $A^TCA \leq B^TCB \Rightarrow A^TA \leq B^TB$?",,['matrices']
8,Characteristic polynomial of 10x10 matrix,Characteristic polynomial of 10x10 matrix,,Consider the matrix A = $\begin{bmatrix}1&0&1&0&1&0&1&0&1&0&\\0&2&0&2&0&2&0&2&0&2&\end{bmatrix}$. What is the characteristic polynomial of the 10×10 matrix $A^T A$?,Consider the matrix A = $\begin{bmatrix}1&0&1&0&1&0&1&0&1&0&\\0&2&0&2&0&2&0&2&0&2&\end{bmatrix}$. What is the characteristic polynomial of the 10×10 matrix $A^T A$?,,"['linear-algebra', 'matrices']"
9,An annoying Pell-like equation related to a binary quadratic form problem,An annoying Pell-like equation related to a binary quadratic form problem,,"Let $A,B,C,D$ be integers such that $AD-BC= 1 $ and $ A+D = -1 $. Show by elementary means that the Diophantine equation   $$\bigl[2Bx + (D-A) y\bigr] ^ 2 + 3y^2 = 4|B|$$ has an integer solution (that is, a solution $(x,y)\in\mathbb Z^2$). If possible, find an explicit solution (involving $A,B,C,D$, of course). Motivation: I arrived at this equation after trying to find explicitly the matrix $g$ suggested by Will Jagy on his answer to this question of mine . Concretely, if $\gamma=\binom{A\ \ B}{C\ \ D}$, then $\gamma$ has order $3$ in $\operatorname{SL_2}(\mathbb Z)$. By indirect methods it can be shown that $\gamma$ is conjugated in $\operatorname{SL_2}(\mathbb Z)$ to one of the matrices $P$ or $P^{-1}$, being $P=\binom{\ \ \,0\quad1}{-1\ \ -1}$ (see studiosus' answer to the same question.). Unfortunately this argument is rather sophisticated to my knowledge, and besides I think that a direct argument is possible. Because of this I tried to find a explicit matrix $g=\binom{x\ \ y}{z\ \ w}\in\operatorname{SL_2}(\mathbb Z)$ such that $gP=\gamma g$ or $gP^{-1}=\gamma g$. The matricial equalities lead to a system of $4$ linear equations in the unknowns $x,y,z,w$ , which can be easily solved. Plugging these solutions $(x,y,z,w)$ (recall that we are considering the two possibilities of conjugation, to $P$ or $P^{-1}$) into the equation $xy-zw=1$ yields $Bx^2+(D-A)xy+(-C)y^2=\pm1$. Completing the square and using the equalities $AD-BC=1$ and $A+D=-1$ we obtain the required equation. I tried to solve it explicitly, with no success.","Let $A,B,C,D$ be integers such that $AD-BC= 1 $ and $ A+D = -1 $. Show by elementary means that the Diophantine equation   $$\bigl[2Bx + (D-A) y\bigr] ^ 2 + 3y^2 = 4|B|$$ has an integer solution (that is, a solution $(x,y)\in\mathbb Z^2$). If possible, find an explicit solution (involving $A,B,C,D$, of course). Motivation: I arrived at this equation after trying to find explicitly the matrix $g$ suggested by Will Jagy on his answer to this question of mine . Concretely, if $\gamma=\binom{A\ \ B}{C\ \ D}$, then $\gamma$ has order $3$ in $\operatorname{SL_2}(\mathbb Z)$. By indirect methods it can be shown that $\gamma$ is conjugated in $\operatorname{SL_2}(\mathbb Z)$ to one of the matrices $P$ or $P^{-1}$, being $P=\binom{\ \ \,0\quad1}{-1\ \ -1}$ (see studiosus' answer to the same question.). Unfortunately this argument is rather sophisticated to my knowledge, and besides I think that a direct argument is possible. Because of this I tried to find a explicit matrix $g=\binom{x\ \ y}{z\ \ w}\in\operatorname{SL_2}(\mathbb Z)$ such that $gP=\gamma g$ or $gP^{-1}=\gamma g$. The matricial equalities lead to a system of $4$ linear equations in the unknowns $x,y,z,w$ , which can be easily solved. Plugging these solutions $(x,y,z,w)$ (recall that we are considering the two possibilities of conjugation, to $P$ or $P^{-1}$) into the equation $xy-zw=1$ yields $Bx^2+(D-A)xy+(-C)y^2=\pm1$. Completing the square and using the equalities $AD-BC=1$ and $A+D=-1$ we obtain the required equation. I tried to solve it explicitly, with no success.",,"['matrices', 'diophantine-equations', 'quadratic-forms']"
10,"Is it true that $\|A+PBP\|\le\|A+B\|$ for every projection $P$ and positive operators $A,B$?",Is it true that  for every projection  and positive operators ?,"\|A+PBP\|\le\|A+B\| P A,B",Let A and B be positive operators on and let P be a projection. Is the inequality $$\|A+PBP\|\le\|A+B\|$$ true? Here $\|.\|$ stands for the operator norm.,Let A and B be positive operators on and let P be a projection. Is the inequality $$\|A+PBP\|\le\|A+B\|$$ true? Here $\|.\|$ stands for the operator norm.,,"['matrices', 'functional-analysis', 'operator-theory']"
11,Determinant of specific infinite matrix,Determinant of specific infinite matrix,,"What is the limit, as n approaches infinity, of the determinant of an n x n matrix where each cell has the value $\cos(n * row + column)$? My friend and I believe the answer to be 0, but can't figure out a method of finding the solution without n! steps. We are interested in how this can be solved as well as the answer.","What is the limit, as n approaches infinity, of the determinant of an n x n matrix where each cell has the value $\cos(n * row + column)$? My friend and I believe the answer to be 0, but can't figure out a method of finding the solution without n! steps. We are interested in how this can be solved as well as the answer.",,"['matrices', 'limits', 'determinant']"
12,Is this argument on positive definite matrices correct?,Is this argument on positive definite matrices correct?,,"Let $A$ be a $N\times N$ positive definite matrix. Then, there exists a $N\times 1$ gaussian random vector $a$ such that $A=E[aa^T]$ where $E[.]$ denotes expectation. Then for any given vector $x$, $x^TAx=x^TE[aa^T]x=E[|a^Tx|^2]>0$. Since it is independent of choice of $x$, $x^TAx>0$ for all $x$. What is wrong with the above sort of argument? How will one justify the interchange of expectation and inner product in the equality $x^TE[aa^T]x=E[|a^Tx|^2]$?","Let $A$ be a $N\times N$ positive definite matrix. Then, there exists a $N\times 1$ gaussian random vector $a$ such that $A=E[aa^T]$ where $E[.]$ denotes expectation. Then for any given vector $x$, $x^TAx=x^TE[aa^T]x=E[|a^Tx|^2]>0$. Since it is independent of choice of $x$, $x^TAx>0$ for all $x$. What is wrong with the above sort of argument? How will one justify the interchange of expectation and inner product in the equality $x^TE[aa^T]x=E[|a^Tx|^2]$?",,"['linear-algebra', 'probability', 'matrices']"
13,Eigenvalues of Hankel matrices,Eigenvalues of Hankel matrices,,"Let $\mathbf{A}$ be a $4-$ dimensional symmetric matrix with real entries, whose elements are given as \begin{equation} \mathbf{A} = \left( \begin{array}{cccc} a & b & c & d \\ b & c & d & e \\ c & d & e & f \\ d & e & f & g  \end{array} \right) \end{equation} Let $\mathbf{B}$ be another $4-$ dimensional matrix whose elements are given as \begin{equation} \mathbf{B} = \left( \begin{array}{cccc} b & c & d & e \\ c & d & e & f \\ d & e & f & g \\ e & f & g & h  \end{array} \right) \end{equation} One can see that the elements of $\mathbf{B}$ are shifted by one with respect to $\mathbf{A}$. $\mathbf{A}$ and $\mathbf{B}$ are also called Hankel Matrices. My query is: ""Does any relationship exist between the eigenvalues of $\mathbf{A}$ and those of $\mathbf{B}$?""","Let $\mathbf{A}$ be a $4-$ dimensional symmetric matrix with real entries, whose elements are given as \begin{equation} \mathbf{A} = \left( \begin{array}{cccc} a & b & c & d \\ b & c & d & e \\ c & d & e & f \\ d & e & f & g  \end{array} \right) \end{equation} Let $\mathbf{B}$ be another $4-$ dimensional matrix whose elements are given as \begin{equation} \mathbf{B} = \left( \begin{array}{cccc} b & c & d & e \\ c & d & e & f \\ d & e & f & g \\ e & f & g & h  \end{array} \right) \end{equation} One can see that the elements of $\mathbf{B}$ are shifted by one with respect to $\mathbf{A}$. $\mathbf{A}$ and $\mathbf{B}$ are also called Hankel Matrices. My query is: ""Does any relationship exist between the eigenvalues of $\mathbf{A}$ and those of $\mathbf{B}$?""",,"['matrices', 'eigenvalues-eigenvectors', 'spectral-theory']"
14,How do you solve a general second order matrix differential equation?,How do you solve a general second order matrix differential equation?,,"How do you find solutions to the equation of the form : \begin{equation}A\frac{d^2X}{dt^2} + B\frac{dX}{dt} + CX = 0 \end{equation} where A,B,C are 3X3 positive definite and symmetric matrices with constant values, and  $X = \begin{bmatrix}X1 \\ X2 \\ X3\end{bmatrix}$.","How do you find solutions to the equation of the form : \begin{equation}A\frac{d^2X}{dt^2} + B\frac{dX}{dt} + CX = 0 \end{equation} where A,B,C are 3X3 positive definite and symmetric matrices with constant values, and  $X = \begin{bmatrix}X1 \\ X2 \\ X3\end{bmatrix}$.",,"['matrices', 'differential']"
15,Find the value(s) of $k$ such that the given vectors do not span $\mathbb{R}^3$,Find the value(s) of  such that the given vectors do not span,k \mathbb{R}^3,"I'm currently attempting to solve the following problem: Find the value(s) of $k$ such that the vectors $\{\vec{a}_1, \vec{a}_2, \vec{a}_3\}$ do not span $\mathbb{R}^3$, where: $$ a_1 = \begin{bmatrix}-1\\k\\7\end{bmatrix}, \quad a_2 = \begin{bmatrix}4\\-2\\5\end{bmatrix}, \quad a_3 = \begin{bmatrix}1\\-7\\2\end{bmatrix} $$ To try and solve this, I first tried putting these vectors into an augmented matrix and attempted to reduce it: $$ \begin{bmatrix}     -1 & 4 & 1 \\     k & -2 & -7 \\     7 & 5 & 2 \\ \end{bmatrix} \to \begin{bmatrix}     -1 & 4 & 1 \\     k & -2 & -7 \\     0 & 33 & 9 \\ \end{bmatrix} \to \begin{bmatrix}     -1 & 4 & 1 \\     0 & -2 + 4k & -7 + k \\     0 & 33 & 9 \\ \end{bmatrix} $$ From here, I thought that perhaps what I needed to do was add up the elements in the second row, and set it equal to zero (solve for $-2 + 4k -7 + k = 0$) so that I could find the values for $k$ where the second row is equal to zero and so could not span $\mathbb{R}^3$. I then arrived at the solution $k = \frac{9}{5}$, but that did not turn out to be the answer. I then wondered if I needed to account for the 'variables' -- if the matrix can be expressed as $\vec{a}_1 x_1 + \vec{a}_2 x_2 + \vec{a}_3 x_3 = \vec{b}$, then perhaps I need to do $(-2 + 4k)x_2 + (-7 + k)x_3 = 0$. If so, then I thought that the solutions might be $k=\frac{2x_2+7x_3}{4x_2+x_3}$, but that also was not the solution (and in any case, I think I'm expected to provide actual numbers, not a formula). I did look at this problem , which is attempting to solve a similar solution except in $\mathbb{R}^2$. In that posted question, it's very obvious that $h$ must be any value other then $3$, but I'm not seeing any obvious values for $k$ in my problem. What am I doing wrong?","I'm currently attempting to solve the following problem: Find the value(s) of $k$ such that the vectors $\{\vec{a}_1, \vec{a}_2, \vec{a}_3\}$ do not span $\mathbb{R}^3$, where: $$ a_1 = \begin{bmatrix}-1\\k\\7\end{bmatrix}, \quad a_2 = \begin{bmatrix}4\\-2\\5\end{bmatrix}, \quad a_3 = \begin{bmatrix}1\\-7\\2\end{bmatrix} $$ To try and solve this, I first tried putting these vectors into an augmented matrix and attempted to reduce it: $$ \begin{bmatrix}     -1 & 4 & 1 \\     k & -2 & -7 \\     7 & 5 & 2 \\ \end{bmatrix} \to \begin{bmatrix}     -1 & 4 & 1 \\     k & -2 & -7 \\     0 & 33 & 9 \\ \end{bmatrix} \to \begin{bmatrix}     -1 & 4 & 1 \\     0 & -2 + 4k & -7 + k \\     0 & 33 & 9 \\ \end{bmatrix} $$ From here, I thought that perhaps what I needed to do was add up the elements in the second row, and set it equal to zero (solve for $-2 + 4k -7 + k = 0$) so that I could find the values for $k$ where the second row is equal to zero and so could not span $\mathbb{R}^3$. I then arrived at the solution $k = \frac{9}{5}$, but that did not turn out to be the answer. I then wondered if I needed to account for the 'variables' -- if the matrix can be expressed as $\vec{a}_1 x_1 + \vec{a}_2 x_2 + \vec{a}_3 x_3 = \vec{b}$, then perhaps I need to do $(-2 + 4k)x_2 + (-7 + k)x_3 = 0$. If so, then I thought that the solutions might be $k=\frac{2x_2+7x_3}{4x_2+x_3}$, but that also was not the solution (and in any case, I think I'm expected to provide actual numbers, not a formula). I did look at this problem , which is attempting to solve a similar solution except in $\mathbb{R}^2$. In that posted question, it's very obvious that $h$ must be any value other then $3$, but I'm not seeing any obvious values for $k$ in my problem. What am I doing wrong?",,"['linear-algebra', 'matrices', 'vector-spaces', 'span']"
16,Determinant of the matrix $\binom{m_i}{j-1}$,Determinant of the matrix,\binom{m_i}{j-1},"Let $m_1,\dots,m_n$ be real numbers $\ge n-1$. How can I find the determinant of the matrix $A$ defined by $(a_{i,j})=\binom{m_i}{j-1}$, for $1\le i\le n$ and $1 \le j \le n$ ? This all looks pretty VanDerMonde-ish, but I haven't found a good way to solve it.","Let $m_1,\dots,m_n$ be real numbers $\ge n-1$. How can I find the determinant of the matrix $A$ defined by $(a_{i,j})=\binom{m_i}{j-1}$, for $1\le i\le n$ and $1 \le j \le n$ ? This all looks pretty VanDerMonde-ish, but I haven't found a good way to solve it.",,"['matrices', 'determinant']"
17,Bases for the image and the kernel of a linear map,Bases for the image and the kernel of a linear map,,"Let the linear map $T: \mathbb{R}^3  \to \mathbb{R}^4$ be given by $$T(x,y,z)=(x+y+z,x+2y-3z,2x+3y-2z,3x+4y-z)$$ Find the basis for the image and kernel of T. HINT: First find the matrix for T in the usual basis. I understand how to find the kernel and image from a matrix.  What I do not understand is how to find the matrix for T.  My only idea so far is that $$(x+y+z)e_1+(x+2y-3z)e_2+(2x+3y-2z)e_3+(3x+4y-z)e_4$$ gives me the matrix for T in the usual basis.","Let the linear map $T: \mathbb{R}^3  \to \mathbb{R}^4$ be given by $$T(x,y,z)=(x+y+z,x+2y-3z,2x+3y-2z,3x+4y-z)$$ Find the basis for the image and kernel of T. HINT: First find the matrix for T in the usual basis. I understand how to find the kernel and image from a matrix.  What I do not understand is how to find the matrix for T.  My only idea so far is that $$(x+y+z)e_1+(x+2y-3z)e_2+(2x+3y-2z)e_3+(3x+4y-z)e_4$$ gives me the matrix for T in the usual basis.",,"['linear-algebra', 'matrices']"
18,Matrix exponentiation intuition.,Matrix exponentiation intuition.,,"What does $x^A$ intuitively mean if $x \in \mathbb{C}$ and $A$ is any matrix? Also, what if we had $x$ being a matrix too? Last but not least, what happens if we have a complex $x$ raised to a quaternion(answering the first question probably generalizes this, but I include it in case only this case makes sense)? What does $x^a, x \in \mathbb{C}, a \in \mathbb{H}$ intuitively mean? What if $x \in \mathbb{H}$ too? I know that matrix exponentiation with base $e$ is $e^A=\displaystyle \sum_{k=0}^{\infty} \frac{1}{k!} A^k$ but apart from symbols in a ""paper"", I see nothing else.","What does $x^A$ intuitively mean if $x \in \mathbb{C}$ and $A$ is any matrix? Also, what if we had $x$ being a matrix too? Last but not least, what happens if we have a complex $x$ raised to a quaternion(answering the first question probably generalizes this, but I include it in case only this case makes sense)? What does $x^a, x \in \mathbb{C}, a \in \mathbb{H}$ intuitively mean? What if $x \in \mathbb{H}$ too? I know that matrix exponentiation with base $e$ is $e^A=\displaystyle \sum_{k=0}^{\infty} \frac{1}{k!} A^k$ but apart from symbols in a ""paper"", I see nothing else.",,"['matrices', 'soft-question', 'intuition', 'quaternions']"
19,Generalized inverse/Pseudo Inverse,Generalized inverse/Pseudo Inverse,,"Let $A_{m. n}$ be a matrix with rank $p$ where $p\leq m$ and $p\leq n$. First Question: We need to show that $A$ can be decomposed as a product of two matrices $A=BC$ where $B$ is an $m$ by $p$ and $C$ is an $p$ by $n$ and both $B$ and $C$ have rank $p$. Second question: We define the generalized inverse of $A$ by $A^{+}=C^{T}(CC^{T})^{-1}(B^{T}B)^{-1}B^{T}$. Show that the solution to the equations: $Ax=b$ (in case it exists) is given by: $x=A^{+}b+e$ where $Ae=0$ and that the solution that has the smallest norm is given by: $x=A^{+}b$. For the first question, I tried to decompose $A$ using the generalized QR decomposition where $Q$ has $p$ linearly independent columns and $R$ has all entries below the main diagonal equal to $0$. The issue is I don't know how to prove such decomposition in the case of this problem because not all columns of $A$ are linearly independent. If anyone has another way to decompose $A$ like required by the statement of the problem, and that makes solving part 2 easy, please let me know. For part 2, I have no clue how to do it, so any help is appreciated. Thanks!","Let $A_{m. n}$ be a matrix with rank $p$ where $p\leq m$ and $p\leq n$. First Question: We need to show that $A$ can be decomposed as a product of two matrices $A=BC$ where $B$ is an $m$ by $p$ and $C$ is an $p$ by $n$ and both $B$ and $C$ have rank $p$. Second question: We define the generalized inverse of $A$ by $A^{+}=C^{T}(CC^{T})^{-1}(B^{T}B)^{-1}B^{T}$. Show that the solution to the equations: $Ax=b$ (in case it exists) is given by: $x=A^{+}b+e$ where $Ae=0$ and that the solution that has the smallest norm is given by: $x=A^{+}b$. For the first question, I tried to decompose $A$ using the generalized QR decomposition where $Q$ has $p$ linearly independent columns and $R$ has all entries below the main diagonal equal to $0$. The issue is I don't know how to prove such decomposition in the case of this problem because not all columns of $A$ are linearly independent. If anyone has another way to decompose $A$ like required by the statement of the problem, and that makes solving part 2 easy, please let me know. For part 2, I have no clue how to do it, so any help is appreciated. Thanks!",,"['linear-algebra', 'matrices', 'numerical-linear-algebra', 'least-squares']"
20,Upper bound for the determinant of matrix sum,Upper bound for the determinant of matrix sum,,"I wonder is there any way to express the ""upper bound"" (not lower bound) of $\det(A+B)$ in terms of $\det(A)$ and $\det(B)$ ?  Thank you!","I wonder is there any way to express the ""upper bound"" (not lower bound) of $\det(A+B)$ in terms of $\det(A)$ and $\det(B)$ ?  Thank you!",,['matrices']
21,Condition for linear minimal polynomials,Condition for linear minimal polynomials,,"I'm just wondering that there is a necessary and sufficient condition for minimal polynomials for in which cases are them linear. Let $A$ be a square matrix. I think that $A$ has a linear minimal polynomial if and only if $A=cI$ for some $c$ constant. Furthermore, the minimal polynomial is $x-c$. Is that true? In that special case, if $c=0$, then $A$ is zero matrix and the minimal polynomial of $A$ is $x$.","I'm just wondering that there is a necessary and sufficient condition for minimal polynomials for in which cases are them linear. Let $A$ be a square matrix. I think that $A$ has a linear minimal polynomial if and only if $A=cI$ for some $c$ constant. Furthermore, the minimal polynomial is $x-c$. Is that true? In that special case, if $c=0$, then $A$ is zero matrix and the minimal polynomial of $A$ is $x$.",,"['linear-algebra', 'matrices', 'polynomials']"
22,a linear algebra problem arising in geometry,a linear algebra problem arising in geometry,,"This is a matrix problem. Assume that $A$ and $B$ are real $n\times n$ matrices. Denote $\Lambda=A+iB$,  $$ M=\left (\begin{array}{cc} A &-B\\ B & A \end{array} \right ) $$ I would like to prove  $$ \det M=\lvert\det\Lambda\rvert^2 $$ Thank you!","This is a matrix problem. Assume that $A$ and $B$ are real $n\times n$ matrices. Denote $\Lambda=A+iB$,  $$ M=\left (\begin{array}{cc} A &-B\\ B & A \end{array} \right ) $$ I would like to prove  $$ \det M=\lvert\det\Lambda\rvert^2 $$ Thank you!",,"['linear-algebra', 'matrices', 'geometry']"
23,45 degree rotation of the line $y=-3x+1$?,45 degree rotation of the line ?,y=-3x+1,"Currently working on problems in a textbook for Senior Maths (Year 11 Maths C, named 'Maths Quest - Maths C for Queensland), however I'm currently at a problem where my answer, despite attempting it multiple times, is incorrect to the textbook's result. The question is, what the image of the line $y=-3x+1$ would be under the rotation of 45 degrees. It sounds like I am making a simple mistake somewhere, but I'm not sure where, therefore can't identify my mistake. $$\begin{bmatrix} x' \\ y' \end{bmatrix} = R_{45^\circ} \begin{bmatrix} x \\ y \end{bmatrix}$$$$\begin{bmatrix} x \\ y \\ \end{bmatrix}=R_{45^\circ}^{-1}\begin{bmatrix} x' \\ y' \end{bmatrix}$$$$R_\theta=\begin{bmatrix} \cos\theta & -\sin\theta \\ \sin\theta & \cos\theta \\ \end{bmatrix}$$$$R_{45^\circ}=\begin{bmatrix} \cos45 & -\sin45 \\ \sin45 & \cos45 \\ \end{bmatrix}$$$$=\begin{bmatrix} \frac{1}{\sqrt{2}} & -\frac{1}{\sqrt{2}} \\ \frac{1}{\sqrt{2}} & \frac{1}{\sqrt{2}}\end{bmatrix}$$ $$_\text{The error occurred here, I didn't use the inverse}$$$$\begin{bmatrix} x \\ y\end{bmatrix}=\begin{bmatrix} \frac{1}{\sqrt{2}} & -\frac{1}{\sqrt{2}} \\ \frac{1}{\sqrt{2}} & \frac{1}{\sqrt{2}}\end{bmatrix}\begin{bmatrix}x'\\y'\end{bmatrix}$$$$\therefore x=\frac1{\sqrt2}x' - \frac1{\sqrt2}y'$$$$ \space y=\frac1{\sqrt2}x' + \frac1{\sqrt2}y'$$$$\text{Sub x and y into original equation...}$$$$\frac1{\sqrt2}x' + \frac1{\sqrt2}y'=-3(\frac1{\sqrt2}x' - \frac1{\sqrt2}y')+1$$$$=\frac{-3}{\sqrt2}x'+\frac{3}{\sqrt2}+1$$$$\frac1{\sqrt2}y'=-\frac4{\sqrt2}x'+\frac3{\sqrt2}y'+1$$$$\frac{-2}{\sqrt2}y'=\frac{-4}{\sqrt2}x'+1$$$$\frac2{\sqrt2}y'=\frac4{\sqrt2}x'-1$$$$y'=\frac{4\sqrt2}{2\sqrt2}-\frac{\sqrt2}{2}$$$$y'=2x'-\frac{\sqrt2}2$$ All of the above methods used are simply multiplication, division, addition, and subtraction. I have tried this in a variety of orders, getting similar (once the same, however with $-\frac{\sqrt2}{2}$ instead of $\frac{\sqrt2}{2}$), however the textbook declares that the answer should be $y'=\frac{x'}2+\frac{\sqrt2}{4}$. Where have I gone wrong? While keeping it roughly as simple as my own working out, any help is appreciated, especially other working out.","Currently working on problems in a textbook for Senior Maths (Year 11 Maths C, named 'Maths Quest - Maths C for Queensland), however I'm currently at a problem where my answer, despite attempting it multiple times, is incorrect to the textbook's result. The question is, what the image of the line $y=-3x+1$ would be under the rotation of 45 degrees. It sounds like I am making a simple mistake somewhere, but I'm not sure where, therefore can't identify my mistake. $$\begin{bmatrix} x' \\ y' \end{bmatrix} = R_{45^\circ} \begin{bmatrix} x \\ y \end{bmatrix}$$$$\begin{bmatrix} x \\ y \\ \end{bmatrix}=R_{45^\circ}^{-1}\begin{bmatrix} x' \\ y' \end{bmatrix}$$$$R_\theta=\begin{bmatrix} \cos\theta & -\sin\theta \\ \sin\theta & \cos\theta \\ \end{bmatrix}$$$$R_{45^\circ}=\begin{bmatrix} \cos45 & -\sin45 \\ \sin45 & \cos45 \\ \end{bmatrix}$$$$=\begin{bmatrix} \frac{1}{\sqrt{2}} & -\frac{1}{\sqrt{2}} \\ \frac{1}{\sqrt{2}} & \frac{1}{\sqrt{2}}\end{bmatrix}$$ $$_\text{The error occurred here, I didn't use the inverse}$$$$\begin{bmatrix} x \\ y\end{bmatrix}=\begin{bmatrix} \frac{1}{\sqrt{2}} & -\frac{1}{\sqrt{2}} \\ \frac{1}{\sqrt{2}} & \frac{1}{\sqrt{2}}\end{bmatrix}\begin{bmatrix}x'\\y'\end{bmatrix}$$$$\therefore x=\frac1{\sqrt2}x' - \frac1{\sqrt2}y'$$$$ \space y=\frac1{\sqrt2}x' + \frac1{\sqrt2}y'$$$$\text{Sub x and y into original equation...}$$$$\frac1{\sqrt2}x' + \frac1{\sqrt2}y'=-3(\frac1{\sqrt2}x' - \frac1{\sqrt2}y')+1$$$$=\frac{-3}{\sqrt2}x'+\frac{3}{\sqrt2}+1$$$$\frac1{\sqrt2}y'=-\frac4{\sqrt2}x'+\frac3{\sqrt2}y'+1$$$$\frac{-2}{\sqrt2}y'=\frac{-4}{\sqrt2}x'+1$$$$\frac2{\sqrt2}y'=\frac4{\sqrt2}x'-1$$$$y'=\frac{4\sqrt2}{2\sqrt2}-\frac{\sqrt2}{2}$$$$y'=2x'-\frac{\sqrt2}2$$ All of the above methods used are simply multiplication, division, addition, and subtraction. I have tried this in a variety of orders, getting similar (once the same, however with $-\frac{\sqrt2}{2}$ instead of $\frac{\sqrt2}{2}$), however the textbook declares that the answer should be $y'=\frac{x'}2+\frac{\sqrt2}{4}$. Where have I gone wrong? While keeping it roughly as simple as my own working out, any help is appreciated, especially other working out.",,['matrices']
24,Perron–Frobenius theorem,Perron–Frobenius theorem,,"What exactly is the Perron–Frobenius theorem? In different books and papers I read different statements, and I don't know what the truth is. In Wikipedia there are also a lot of statements under this label. And if somebody can characterize the statement for me, then I need also a proof. I read that the proof needs Brouwer's fixed point theorem , but is there a quite easy way to prove it?","What exactly is the Perron–Frobenius theorem? In different books and papers I read different statements, and I don't know what the truth is. In Wikipedia there are also a lot of statements under this label. And if somebody can characterize the statement for me, then I need also a proof. I read that the proof needs Brouwer's fixed point theorem , but is there a quite easy way to prove it?",,"['linear-algebra', 'matrices', 'eigenvalues-eigenvectors', 'positive-matrices']"
25,A subset that is closed under multiplication but not addition? [duplicate],A subset that is closed under multiplication but not addition? [duplicate],,"This question already has an answer here : About closure under + (1 answer) Closed 9 years ago . I can't get my head around subspaces despite having studied on them quite a lot. Here goes: The problem statement, all given variables and data Give an example of a non-empty subset U of R^2 such that U is closed under scalar multiplication but is not a subspace of R^2. Attempt at a solution So a set such as [x1, x2] | x1 >= 0 is NOT closed under scalar multiplication but is closed under addition. I get this but I cannot find an answer in reverse. A wild guess is x1 + x2 + 2 = 0. I could multiply it and 2x1 + 2x2 + 4 = 0 would still hold true. However, if I wish to perform an addition on it, I don't exactly understand what I add to what equation exactly.. it's all so bogged up. Help would be appreciated. :-)","This question already has an answer here : About closure under + (1 answer) Closed 9 years ago . I can't get my head around subspaces despite having studied on them quite a lot. Here goes: The problem statement, all given variables and data Give an example of a non-empty subset U of R^2 such that U is closed under scalar multiplication but is not a subspace of R^2. Attempt at a solution So a set such as [x1, x2] | x1 >= 0 is NOT closed under scalar multiplication but is closed under addition. I get this but I cannot find an answer in reverse. A wild guess is x1 + x2 + 2 = 0. I could multiply it and 2x1 + 2x2 + 4 = 0 would still hold true. However, if I wish to perform an addition on it, I don't exactly understand what I add to what equation exactly.. it's all so bogged up. Help would be appreciated. :-)",,"['linear-algebra', 'matrices', 'vector-spaces']"
26,All permutation matrices that convert one Hadamard matrix into another Hadamard matrix.,All permutation matrices that convert one Hadamard matrix into another Hadamard matrix.,,"Given a Hadamard matrix $H$, I know that applying row and column permutations, along with multiplying a row or a column with a -1 results in another Hadamard matrix $H^{'}$ equivalent to the first.  Given $H$ and $H^{'}$, ( assume we know that they're equivalent) , is it possible to find all pairs of permutation matrices $(P,Q)$ that preserve Hadamard equivalence, such that, \begin{equation} H^{'}=PHQ \end{equation} ? For example, take 2 Hadamard matrices of order 4, $H,H^{'}$. We know that there is only one equivalence class of Hadamard matrices of order 4, so the 2 matrices are equivalent. $H = \left( \begin{matrix} 1 & 1 & - & 1 \\ - & 1 & - & -\\ 1 & 1 & 1 & -\\ - & 1 & 1 & 1 \\ \end{matrix} \right) $,  $H^{'} = \left( \begin{matrix} 1 & 1 & 1 & - \\ - & 1 & 1 & 1\\ - & - & 1 & -\\ 1 & - & 1 & 1 \\ \end{matrix} \right) $ Now, $ \left( \begin{matrix} 1 & 1 & 1 & - \\ - & 1 & 1 & 1\\ - & - & 1 & -\\ 1 & - & 1 & 1 \\ \end{matrix} \right) =$ $ \left( \begin{matrix} 1 & 0 & 0 & 0 \\ 0 & 1 & 0 & 0\\ 0 & 0 & 1 & 0\\ 0 & 0 & 0 & 1 \\ \end{matrix} \right) *$ $ \left( \begin{matrix} 1 & 1 & - & 1 \\ - & 1 & - & -\\ 1 & 1 & 1 & -\\ - & 1 & 1 & 1 \\ \end{matrix} \right) *$ $ \left( \begin{matrix} 0 & 0 & 0 & - \\ 0 & 0 & 1 & 0\\ 0 & - & 0 & 0\\ 1 & 0 & 0 & 0 \\ \end{matrix} \right) $ So here, $P = \left( \begin{matrix} 1 & 0 & 0 & 0 \\ 0 & 1 & 0 & 0\\ 0 & 0 & 1 & 0\\ 0 & 0 & 0 & 1 \\ \end{matrix} \right) $ , $Q = \left( \begin{matrix} 0 & 0 & 0 & - \\ 0 & 0 & 1 & 0\\ 0 & - & 0 & 0\\ 1 & 0 & 0 & 0 \\ \end{matrix} \right) $ This is one example of a pair $(P,Q)$. How do you list all such pairs? (I'll make the question more specific and add that I'm looking for permutation matrices for 4x4 Hadamard matrices only)","Given a Hadamard matrix $H$, I know that applying row and column permutations, along with multiplying a row or a column with a -1 results in another Hadamard matrix $H^{'}$ equivalent to the first.  Given $H$ and $H^{'}$, ( assume we know that they're equivalent) , is it possible to find all pairs of permutation matrices $(P,Q)$ that preserve Hadamard equivalence, such that, \begin{equation} H^{'}=PHQ \end{equation} ? For example, take 2 Hadamard matrices of order 4, $H,H^{'}$. We know that there is only one equivalence class of Hadamard matrices of order 4, so the 2 matrices are equivalent. $H = \left( \begin{matrix} 1 & 1 & - & 1 \\ - & 1 & - & -\\ 1 & 1 & 1 & -\\ - & 1 & 1 & 1 \\ \end{matrix} \right) $,  $H^{'} = \left( \begin{matrix} 1 & 1 & 1 & - \\ - & 1 & 1 & 1\\ - & - & 1 & -\\ 1 & - & 1 & 1 \\ \end{matrix} \right) $ Now, $ \left( \begin{matrix} 1 & 1 & 1 & - \\ - & 1 & 1 & 1\\ - & - & 1 & -\\ 1 & - & 1 & 1 \\ \end{matrix} \right) =$ $ \left( \begin{matrix} 1 & 0 & 0 & 0 \\ 0 & 1 & 0 & 0\\ 0 & 0 & 1 & 0\\ 0 & 0 & 0 & 1 \\ \end{matrix} \right) *$ $ \left( \begin{matrix} 1 & 1 & - & 1 \\ - & 1 & - & -\\ 1 & 1 & 1 & -\\ - & 1 & 1 & 1 \\ \end{matrix} \right) *$ $ \left( \begin{matrix} 0 & 0 & 0 & - \\ 0 & 0 & 1 & 0\\ 0 & - & 0 & 0\\ 1 & 0 & 0 & 0 \\ \end{matrix} \right) $ So here, $P = \left( \begin{matrix} 1 & 0 & 0 & 0 \\ 0 & 1 & 0 & 0\\ 0 & 0 & 1 & 0\\ 0 & 0 & 0 & 1 \\ \end{matrix} \right) $ , $Q = \left( \begin{matrix} 0 & 0 & 0 & - \\ 0 & 0 & 1 & 0\\ 0 & - & 0 & 0\\ 1 & 0 & 0 & 0 \\ \end{matrix} \right) $ This is one example of a pair $(P,Q)$. How do you list all such pairs? (I'll make the question more specific and add that I'm looking for permutation matrices for 4x4 Hadamard matrices only)",,"['matrices', 'permutations', 'hadamard-product']"
27,Effective way of checking if all eigenvalues of a matrix are integers,Effective way of checking if all eigenvalues of a matrix are integers,,"Given A matrix with integer entries, it should be checked if all its eigenvalues  are integers. Of course, the characteristic polynomial could be calculated, but is there any  faster (or easier) method ? Another possibility would be : Search a bound for the eigenvalues and check for  each possible integer n, if $A-nI$ is invertible. But this is also cumbersome and another problem is that multiple eigenvalues may not be detected.","Given A matrix with integer entries, it should be checked if all its eigenvalues  are integers. Of course, the characteristic polynomial could be calculated, but is there any  faster (or easier) method ? Another possibility would be : Search a bound for the eigenvalues and check for  each possible integer n, if $A-nI$ is invertible. But this is also cumbersome and another problem is that multiple eigenvalues may not be detected.",,"['matrices', 'eigenvalues-eigenvectors']"
28,Square root of a $3\times3$ matrix,Square root of a  matrix,3\times3,"Here is  $3\times3$ matrix$$\begin{pmatrix}   0&    0&    1\\     0  & -1 &   0\\     1&    0  &  0\end{pmatrix}$$ How can I solve this  by using Cayley-Hamilton? I know how to use Cayley-Hamilton for a $2$-dimensional matrix. How can it help in finding the square root of a $3\times3$ matrix? for 2 dimensional matrix we can solve this equation A^2−(trA)A+(detA)I=0 we have A and I, we can compute det(A^2) so we have det A, and  we can find A. for 2 dimensional matrix using above equation we can compute square root. for example we have this matrix: A^2= $$\begin{pmatrix}   4&    2\\         2&    2\end{pmatrix}$$ det A^2= 4  det A=2     4 2             1 0 2 2  +6 *A+ 2*  0 1 =0 by solving above equation we can find A.","Here is  $3\times3$ matrix$$\begin{pmatrix}   0&    0&    1\\     0  & -1 &   0\\     1&    0  &  0\end{pmatrix}$$ How can I solve this  by using Cayley-Hamilton? I know how to use Cayley-Hamilton for a $2$-dimensional matrix. How can it help in finding the square root of a $3\times3$ matrix? for 2 dimensional matrix we can solve this equation A^2−(trA)A+(detA)I=0 we have A and I, we can compute det(A^2) so we have det A, and  we can find A. for 2 dimensional matrix using above equation we can compute square root. for example we have this matrix: A^2= $$\begin{pmatrix}   4&    2\\         2&    2\end{pmatrix}$$ det A^2= 4  det A=2     4 2             1 0 2 2  +6 *A+ 2*  0 1 =0 by solving above equation we can find A.",,['matrices']
29,Why not $SL_n (\mathbb R)$ in this exercise,Why not  in this exercise,SL_n (\mathbb R),"I just solved the following exercise: Let $SL_2(\mathbb Z)$ denote the set of $2\times2$ matrices with integer entries and determinant $1$. Prove that $SL_2(\mathbb Z)$ is a subgroup of $GL_2(\mathbb R)$. Is $SL_n(\mathbb Z)$ a subgroup of $GL_n(\mathbb R)$? It's clear that if $A,B \in SL_2(\mathbb Z)$ then also $AB \in SL_2(\mathbb Z)$ and that $A^{-1} \in SL_2 (\mathbb Z)$ follows from the Cayley-Hamilton formula for the inverse (it is clear the entries are integers). Similarly the answer to the question is clearly affirmative. What I don't quite get is why it is $\mathbb Z$ for the subgroup and $\mathbb R$ for the group. I believe that it is also true that $SL_n(\mathbb Z)$ is a subgroup of $GL_n(\mathbb Z)$ and $SL_n(\mathbb R)$ is a subgroup of $GL_n(\mathbb R)$. So why is it integers for the subgroup and reals for the group in the exercise? Does this setting have any special properties?","I just solved the following exercise: Let $SL_2(\mathbb Z)$ denote the set of $2\times2$ matrices with integer entries and determinant $1$. Prove that $SL_2(\mathbb Z)$ is a subgroup of $GL_2(\mathbb R)$. Is $SL_n(\mathbb Z)$ a subgroup of $GL_n(\mathbb R)$? It's clear that if $A,B \in SL_2(\mathbb Z)$ then also $AB \in SL_2(\mathbb Z)$ and that $A^{-1} \in SL_2 (\mathbb Z)$ follows from the Cayley-Hamilton formula for the inverse (it is clear the entries are integers). Similarly the answer to the question is clearly affirmative. What I don't quite get is why it is $\mathbb Z$ for the subgroup and $\mathbb R$ for the group. I believe that it is also true that $SL_n(\mathbb Z)$ is a subgroup of $GL_n(\mathbb Z)$ and $SL_n(\mathbb R)$ is a subgroup of $GL_n(\mathbb R)$. So why is it integers for the subgroup and reals for the group in the exercise? Does this setting have any special properties?",,"['linear-algebra', 'matrices', 'lie-groups']"
30,Why is $L_A$ not $\mathbb K$ linear (I can prove that it is),Why is  not  linear (I can prove that it is),L_A \mathbb K,Let $\mathbb K$ denote the skew field of quaternions and $A \in M^{n \times n}(\mathbb K)$ and $X\in M^{1\times n}(\mathbb K)$. Let $L_A : \mathbb K^n \to \mathbb K^n$ be defined as $L_A(X) = (AX^T)^T$. $L_A$ is supposedly not $\mathbb K$ linear. But I can prove that it is. Like so: $$ \begin{align}L_A(aX + bY) &= (A(aX + bY)^T)^T =(A(aX^T + bY^T))^T\\ &=(aX^T + bY^T)^TA^T = (aX + bY)A^T \\ &= aXA^T + bYA^T = a(AX^T)^T + b(AY^T)^T \\ &= aL_A(X) + bL_A(Y) \end{align}$$ Why is this proof wrong?,Let $\mathbb K$ denote the skew field of quaternions and $A \in M^{n \times n}(\mathbb K)$ and $X\in M^{1\times n}(\mathbb K)$. Let $L_A : \mathbb K^n \to \mathbb K^n$ be defined as $L_A(X) = (AX^T)^T$. $L_A$ is supposedly not $\mathbb K$ linear. But I can prove that it is. Like so: $$ \begin{align}L_A(aX + bY) &= (A(aX + bY)^T)^T =(A(aX^T + bY^T))^T\\ &=(aX^T + bY^T)^TA^T = (aX + bY)A^T \\ &= aXA^T + bYA^T = a(AX^T)^T + b(AY^T)^T \\ &= aL_A(X) + bL_A(Y) \end{align}$$ Why is this proof wrong?,,"['linear-algebra', 'matrices']"
31,Find all $3\times3$ square matrices which commute with any $3\times3$ upper triangular matrix. [duplicate],Find all  square matrices which commute with any  upper triangular matrix. [duplicate],3\times3 3\times3,"This question already has answers here : A linear operator commuting with all such operators is a scalar multiple of the identity. (10 answers) Closed 6 years ago . I'm not sure how to proceed. Let us find all possible solutions for the matrix $A$ which commutes with any other matrix $X$. In other words: $$AX=XA$$ Stating the matrix multiplication explicitly we can conclude that for any $i,j\in\{1,2,3\}$: $$\sum_{n=1}^3 A_{i,n} X_{n,j} = \sum_{n=1}^3 X_{i,n} A_{n,j}$$ At this point I do not know what to do anymore. How do I proceed?","This question already has answers here : A linear operator commuting with all such operators is a scalar multiple of the identity. (10 answers) Closed 6 years ago . I'm not sure how to proceed. Let us find all possible solutions for the matrix $A$ which commutes with any other matrix $X$. In other words: $$AX=XA$$ Stating the matrix multiplication explicitly we can conclude that for any $i,j\in\{1,2,3\}$: $$\sum_{n=1}^3 A_{i,n} X_{n,j} = \sum_{n=1}^3 X_{i,n} A_{n,j}$$ At this point I do not know what to do anymore. How do I proceed?",,"['linear-algebra', 'matrices']"
32,Projection matrix to project a point in a plane,Projection matrix to project a point in a plane,,"How to determinate the 4x4 S matrix so that the P gets projected into Q, on the XZ (Y=0) plane? Q = S P","How to determinate the 4x4 S matrix so that the P gets projected into Q, on the XZ (Y=0) plane? Q = S P",,"['matrices', 'geometry', 'projective-geometry']"
33,the representation of a free group,the representation of a free group,,"A group $G$ is generated by $\begin{pmatrix}1&n\\0&1\end{pmatrix}$ and $\begin{pmatrix}1&0\\n&1\end{pmatrix}$, then we know $G\cong \mathbb{F}_2$ which is a free group generated by two elements. Now I consider the representation: $G\to GL(2,\mathbb{R})$, it is necessary that the image of $\begin{pmatrix}1&n\\0&1\end{pmatrix}$ is a triangular matrix under conjugation? Thanks in advance.","A group $G$ is generated by $\begin{pmatrix}1&n\\0&1\end{pmatrix}$ and $\begin{pmatrix}1&0\\n&1\end{pmatrix}$, then we know $G\cong \mathbb{F}_2$ which is a free group generated by two elements. Now I consider the representation: $G\to GL(2,\mathbb{R})$, it is necessary that the image of $\begin{pmatrix}1&n\\0&1\end{pmatrix}$ is a triangular matrix under conjugation? Thanks in advance.",,"['group-theory', 'matrices', 'representation-theory']"
34,"$X$, find $A$ such that $A^m=X$",", find  such that",X A A^m=X,"I encountered a problem as folows: Show a $3\times 3$ real matrx $A$ , such that $$A^4=\left(\begin{array}{ccc}3&0&0\\0&3&1\\0&0&0\end{array}\right)$$ well, this problem is not difficult, one can first find $B=\left(\begin{array}{ccc}\sqrt3&0&0\\0&\sqrt3&x\\0&0&0\end{array}\right)$ such that $B^2=\left(\begin{array}{ccc}3&0&0\\0&3&1\\0&0&0\end{array}\right)$ . My problem is: Let $m,n$ be two positive integers. then, for what $n\times n$ real  matrix $X$ , there exist real matrix $A$ such that $A^m=X$ ? Is there a general method or theorem to calculate all the matrices $X$ and $A$ ? Maybe, there does not exist a  general answer. then, How about $n=3$ or $4$ ? Thanks a lot!","I encountered a problem as folows: Show a real matrx , such that well, this problem is not difficult, one can first find such that . My problem is: Let be two positive integers. then, for what real  matrix , there exist real matrix such that ? Is there a general method or theorem to calculate all the matrices and ? Maybe, there does not exist a  general answer. then, How about or ? Thanks a lot!","3\times 3 A A^4=\left(\begin{array}{ccc}3&0&0\\0&3&1\\0&0&0\end{array}\right) B=\left(\begin{array}{ccc}\sqrt3&0&0\\0&\sqrt3&x\\0&0&0\end{array}\right) B^2=\left(\begin{array}{ccc}3&0&0\\0&3&1\\0&0&0\end{array}\right) m,n n\times n X A A^m=X X A n=3 4","['linear-algebra', 'matrices']"
35,Suppose $A^n = 0$ matrix for some $n > 1$. Find an inverse for $I - A$.,Suppose  matrix for some . Find an inverse for .,A^n = 0 n > 1 I - A,"Source: Linear Algebra by Lay (4 edn 2011). p. 160. Chapter 2 Supplementary Exercise 4. Solution: From p. 160 Supplementary Exercise 3, the inverse of $I-A$ is probably $I+A+A^{2}+...+A^{n-1}$. To verify this, compute   $ (I \color{orangered}{-A} )\color{forestgreen}{(I+A+\cdots+A^{n-1})}=I+A+\cdots+A^{n-1} \quad \color{orangered}{-A}(I+A+\cdots+A^{n-1})=I \color{orangered}{-A} A^{n-1}=I-A^{n}. \;[...] \; \blacksquare$ How can you divine that the inverse of $I-A$ is  $\color{forestgreen}{\sum_{0 \le i \le n - 1} A^{i}}$ ? The solution doesn't explain. I already understand that $ (I \color{orangered}{-A} )\color{forestgreen}{(I+A+\cdots+A^{n-1})}= ... =I-A^{n}. $","Source: Linear Algebra by Lay (4 edn 2011). p. 160. Chapter 2 Supplementary Exercise 4. Solution: From p. 160 Supplementary Exercise 3, the inverse of $I-A$ is probably $I+A+A^{2}+...+A^{n-1}$. To verify this, compute   $ (I \color{orangered}{-A} )\color{forestgreen}{(I+A+\cdots+A^{n-1})}=I+A+\cdots+A^{n-1} \quad \color{orangered}{-A}(I+A+\cdots+A^{n-1})=I \color{orangered}{-A} A^{n-1}=I-A^{n}. \;[...] \; \blacksquare$ How can you divine that the inverse of $I-A$ is  $\color{forestgreen}{\sum_{0 \le i \le n - 1} A^{i}}$ ? The solution doesn't explain. I already understand that $ (I \color{orangered}{-A} )\color{forestgreen}{(I+A+\cdots+A^{n-1})}= ... =I-A^{n}. $",,['linear-algebra']
36,Determinants of matrices over the finite field $\Bbb{Z}_q$,Determinants of matrices over the finite field,\Bbb{Z}_q,"Let $\Bbb{Z}_q$ be a finite field, where $q$ is prime number. Let $A_1$ be a set of $n \times n$ matrices such that $\det(M) = 1$ , for any matrix $M \in A_1$ $A_2$ be set of $n \times n$ matrices such that $\det(M) = 2$ , for any matrix $M \in A_2$ and so forth. Is $|A_1| = |A_2|= \cdots =|A_{q-1}|$ ? Need proof or counterexample. I know that $|A_0|<>|A_1| $","Let be a finite field, where is prime number. Let be a set of matrices such that , for any matrix be set of matrices such that , for any matrix and so forth. Is ? Need proof or counterexample. I know that",\Bbb{Z}_q q A_1 n \times n \det(M) = 1 M \in A_1 A_2 n \times n \det(M) = 2 M \in A_2 |A_1| = |A_2|= \cdots =|A_{q-1}| |A_0|<>|A_1| ,"['matrices', 'determinant', 'finite-fields']"
37,"Find all $3 \times 3$ matrices X, such that $X^2+E=0$","Find all  matrices X, such that",3 \times 3 X^2+E=0,"Task is to find all $3 \times 3$ matrices X, $x_{ij} \in R$, such that $X^2+E=0$ I used suggestions from this question, though I stuck anyway. $X^2=Y=-E$ Then $det(Y-\lambda E)=0$, which results $\lambda=-1$ and that gives only trivial solutions for eigenvectors. So, would you be so kind to point a direction for me?","Task is to find all $3 \times 3$ matrices X, $x_{ij} \in R$, such that $X^2+E=0$ I used suggestions from this question, though I stuck anyway. $X^2=Y=-E$ Then $det(Y-\lambda E)=0$, which results $\lambda=-1$ and that gives only trivial solutions for eigenvectors. So, would you be so kind to point a direction for me?",,"['linear-algebra', 'matrices']"
38,Using a matrix method to find the square root of a number. How does it work?,Using a matrix method to find the square root of a number. How does it work?,,"This is the matrix that is used to find the square root of a number (M). p and q is an estimate of the root of M in a fraction form (5=10/2 or 5/1) and a and b is a new fraction of a closer approximate of M. Repeating this improve accuracy where the a and b of the previous iteration are the new p and q (a=p and b=q). What I am asking is how does this method work? Any guidance as to finding how this method work will be appreciated, and if any further information is required, just let me know. My teacher suggests that deep algebraic equations are not necessary, but if they are required, I will try my best to understand and interpret them.","This is the matrix that is used to find the square root of a number (M). p and q is an estimate of the root of M in a fraction form (5=10/2 or 5/1) and a and b is a new fraction of a closer approximate of M. Repeating this improve accuracy where the a and b of the previous iteration are the new p and q (a=p and b=q). What I am asking is how does this method work? Any guidance as to finding how this method work will be appreciated, and if any further information is required, just let me know. My teacher suggests that deep algebraic equations are not necessary, but if they are required, I will try my best to understand and interpret them.",,['matrices']
39,Eigenvalues of product of symmetric positive-definite matrix,Eigenvalues of product of symmetric positive-definite matrix,,Let $M$ be a symmetric positive-definite matrix and $$A = (I+M)^{-1}(I-M)$$ we know that eigenvalues of matrices $I+M$ and $I-M$ are as $1+\mu_i$ where $\mu_i$ is eigenvalue of $M$. Who we can determine eigenvalues of matrix $A$?,Let $M$ be a symmetric positive-definite matrix and $$A = (I+M)^{-1}(I-M)$$ we know that eigenvalues of matrices $I+M$ and $I-M$ are as $1+\mu_i$ where $\mu_i$ is eigenvalue of $M$. Who we can determine eigenvalues of matrix $A$?,,"['linear-algebra', 'matrices']"
40,Determinant of a matrix with symmetric positive definite block,Determinant of a matrix with symmetric positive definite block,,"In reviewing linear algebra for an exam, I encountered the following problem: Let $A \in \mathbb{R}^{n\times n}$ be symmetric positive definite. If $x$ is any nonzero vector, show that   $$ \det\begin{pmatrix} a_{11} & \cdots & a_{1n} & x_1 \\ \vdots & \ddots & \vdots & \vdots \\ a_{n1} & \cdots & a_{nn} & x_n \\ x_1 & \cdots & x_n & 0\end{pmatrix} < 0$$ I solved the problem using this identity : $$ \begin{pmatrix}A& B\\ C& D\end{pmatrix} = \begin{pmatrix}A& 0\\ C& I\end{pmatrix} \begin{pmatrix}I& A^{-1} B\\ 0& D - C A^{-1} B\end{pmatrix} $$ Applied to the case above, the original determinant is equivalent to $\det(A)\det(-x^T A^{-1} x)$; since $A$ is symmetric positive definite, $A^{-1}$ is as well, so $x^T A^{-1} x$ is a positive number, and it follows that the expression is negative. Now to my question: is there another (better) way of solving this problem? My first thought was to approach the problem by induction, but that seemed to be a dead end. I had to look up the above identity which won't be an option for an exam.","In reviewing linear algebra for an exam, I encountered the following problem: Let $A \in \mathbb{R}^{n\times n}$ be symmetric positive definite. If $x$ is any nonzero vector, show that   $$ \det\begin{pmatrix} a_{11} & \cdots & a_{1n} & x_1 \\ \vdots & \ddots & \vdots & \vdots \\ a_{n1} & \cdots & a_{nn} & x_n \\ x_1 & \cdots & x_n & 0\end{pmatrix} < 0$$ I solved the problem using this identity : $$ \begin{pmatrix}A& B\\ C& D\end{pmatrix} = \begin{pmatrix}A& 0\\ C& I\end{pmatrix} \begin{pmatrix}I& A^{-1} B\\ 0& D - C A^{-1} B\end{pmatrix} $$ Applied to the case above, the original determinant is equivalent to $\det(A)\det(-x^T A^{-1} x)$; since $A$ is symmetric positive definite, $A^{-1}$ is as well, so $x^T A^{-1} x$ is a positive number, and it follows that the expression is negative. Now to my question: is there another (better) way of solving this problem? My first thought was to approach the problem by induction, but that seemed to be a dead end. I had to look up the above identity which won't be an option for an exam.",,"['linear-algebra', 'matrices', 'determinant', 'alternative-proof']"
41,Using QR decomposition to solve a system of equations with a singular matrix,Using QR decomposition to solve a system of equations with a singular matrix,,"If $A\in\mathbb{R}^{n\times n}$ is singular and $x,b\in\mathbb{R}^{n}$ are such that $Ax=b$, am I right in thinking that the upper triangular matrix $R$ of $A$'s $QR$ decomposition must have at least one diagonal entry because otherwise saying $Rx=Q^{T}b$ and using backward substitution would give one definite answer for $x$. But, because $A$ is singular, there cannot be just one solution to the system?","If $A\in\mathbb{R}^{n\times n}$ is singular and $x,b\in\mathbb{R}^{n}$ are such that $Ax=b$, am I right in thinking that the upper triangular matrix $R$ of $A$'s $QR$ decomposition must have at least one diagonal entry because otherwise saying $Rx=Q^{T}b$ and using backward substitution would give one definite answer for $x$. But, because $A$ is singular, there cannot be just one solution to the system?",,"['matrices', 'systems-of-equations']"
42,Derivative of a Matrix to a Power,Derivative of a Matrix to a Power,,"Fix a positive interger $k$ and let $F: \mathbb{R}^{n \times n} \rightarrow \mathbb{R}^{n \times n}$ be the map on $n \times n$ matrices defined by $F(A)= A^k$. Show that $F$ is differentiable at every point of $\mathbb{R}^{n \times n}$, and find $(DF)(A)$. So my first guess was the obvious one $(DF)(A)= k A^{k-1}$. So I looked at $$\frac{F(X) - F(A) - DF(X-A)}{|X-A|} = \frac{X^k - A^k - k (X-A)^{k-1}}{|X-A|}$$ And now  I don't know how to proceed. I not even sure if this is the right linear transformation for $DF$...","Fix a positive interger $k$ and let $F: \mathbb{R}^{n \times n} \rightarrow \mathbb{R}^{n \times n}$ be the map on $n \times n$ matrices defined by $F(A)= A^k$. Show that $F$ is differentiable at every point of $\mathbb{R}^{n \times n}$, and find $(DF)(A)$. So my first guess was the obvious one $(DF)(A)= k A^{k-1}$. So I looked at $$\frac{F(X) - F(A) - DF(X-A)}{|X-A|} = \frac{X^k - A^k - k (X-A)^{k-1}}{|X-A|}$$ And now  I don't know how to proceed. I not even sure if this is the right linear transformation for $DF$...",,"['analysis', 'matrices', 'derivatives']"
43,Matrices rank problem,Matrices rank problem,,"$X\in \text{Mat}_n (\mathbb{R} )$ and $|X|\neq 0$. $X$ has column vectors $X_1,X_2,\ldots ,X_n$. $Y$ is a matrix that consists of column vectors $X_2,X_3,\ldots ,X_n,0$. Let $A=YX^{-1}$ and $B=X^{-1}Y$. Find rank $A$ and rank $B$. It's clear from $|AB|=|A||B|$ that $A$ and $B$ rank must be $<n$. How could I get definite answer here?","$X\in \text{Mat}_n (\mathbb{R} )$ and $|X|\neq 0$. $X$ has column vectors $X_1,X_2,\ldots ,X_n$. $Y$ is a matrix that consists of column vectors $X_2,X_3,\ldots ,X_n,0$. Let $A=YX^{-1}$ and $B=X^{-1}Y$. Find rank $A$ and rank $B$. It's clear from $|AB|=|A||B|$ that $A$ and $B$ rank must be $<n$. How could I get definite answer here?",,"['linear-algebra', 'matrices', 'matrix-rank']"
44,Calculate rotation/translation matrix to match measurement points to nominal points,Calculate rotation/translation matrix to match measurement points to nominal points,,"I have two matrices, one containing 3D coordinates that are nominal positions per a CAD model and the other containing 3D coordinates of actual measured positions using a CMM. Every nominal point has a corresponding measurement, or in other words the two matrices are of equal length and width. I'm not sure what the best way is to fit the measured points to the nominal points. I need a way of calculating the translation and rotation to apply to all of the measured points that produce the minimum distance between each nominal/measured pair of points while not exceeding allowed tolerances on maximum distance at any other point. This is similar to Registration of point clouds but different in that each pair of nominal/measured points has a unique tolerance/limit on how far apart they are allowed to be. That limit is higher for some pairs and lower for others. I'm programming in .Net and have looked into Point Cloud Library (PCL), OpenCV, Excel, and basic matrix operations as possible approaches. This is a sample of the data X Nom    Y Nom  Z Nom   X Meas  Y Meas  Z Meas  Upper Tol   Lower Tol 118.81  2.24    -14.14  118.68  2.24    -14.14  1.00    -0.50 118.72  1.71    -17.19  118.52  1.70    -17.16  1.00    -0.50 115.36  1.53    -24.19  115.14  1.52    -23.98  0.50    -0.50 108.73  1.20    -27.75  108.66  1.20    -27.41  0.20    -0.20 Below is the type of matrix I need to calculate in order to best fit the measured points to the nominal points. I will multiply it by the measured point matrix to best fit to the nominal point matrix. Transformation   0.999897324 -0.000587540    0.014317661 0.000632725 0.999994834 -0.003151567 -0.014315736    0.003160302 0.999892530 -0.000990993    0.001672040 0.001672040","I have two matrices, one containing 3D coordinates that are nominal positions per a CAD model and the other containing 3D coordinates of actual measured positions using a CMM. Every nominal point has a corresponding measurement, or in other words the two matrices are of equal length and width. I'm not sure what the best way is to fit the measured points to the nominal points. I need a way of calculating the translation and rotation to apply to all of the measured points that produce the minimum distance between each nominal/measured pair of points while not exceeding allowed tolerances on maximum distance at any other point. This is similar to Registration of point clouds but different in that each pair of nominal/measured points has a unique tolerance/limit on how far apart they are allowed to be. That limit is higher for some pairs and lower for others. I'm programming in .Net and have looked into Point Cloud Library (PCL), OpenCV, Excel, and basic matrix operations as possible approaches. This is a sample of the data X Nom    Y Nom  Z Nom   X Meas  Y Meas  Z Meas  Upper Tol   Lower Tol 118.81  2.24    -14.14  118.68  2.24    -14.14  1.00    -0.50 118.72  1.71    -17.19  118.52  1.70    -17.16  1.00    -0.50 115.36  1.53    -24.19  115.14  1.52    -23.98  0.50    -0.50 108.73  1.20    -27.75  108.66  1.20    -27.41  0.20    -0.20 Below is the type of matrix I need to calculate in order to best fit the measured points to the nominal points. I will multiply it by the measured point matrix to best fit to the nominal point matrix. Transformation   0.999897324 -0.000587540    0.014317661 0.000632725 0.999994834 -0.003151567 -0.014315736    0.003160302 0.999892530 -0.000990993    0.001672040 0.001672040",,"['matrices', 'least-squares', 'rigid-transformation']"
45,let $A$ be an $n\times n$ matrix. Show that $\det(A^{-1}) = \frac{1}{\det(A)}$,let  be an  matrix. Show that,A n\times n \det(A^{-1}) = \frac{1}{\det(A)},"Let $A$ be a $n \times n$ matrix , and then show that $$\det(A^{-1}) = \frac{1}{\det(A)}.$$ Any tips on this one? basically I don't have a clue.","Let $A$ be a $n \times n$ matrix , and then show that $$\det(A^{-1}) = \frac{1}{\det(A)}.$$ Any tips on this one? basically I don't have a clue.",,"['linear-algebra', 'matrices', 'determinant', 'inverse']"
46,Upper Unitriangular Matrices,Upper Unitriangular Matrices,,Let $U$ be the group of the upper unitriangular matrices $n$-$n$ over the field of rationals $\mathbb{Q}$. I know that $U$ is nilpotent and torsion-free. It is also radicable? How it can be proved in an elementary way? Deifnition A gorup $G$ is said to be a radicable group iff every element of $G$ has an $n$th root in $G$ for all positive numbers $n$.,Let $U$ be the group of the upper unitriangular matrices $n$-$n$ over the field of rationals $\mathbb{Q}$. I know that $U$ is nilpotent and torsion-free. It is also radicable? How it can be proved in an elementary way? Deifnition A gorup $G$ is said to be a radicable group iff every element of $G$ has an $n$th root in $G$ for all positive numbers $n$.,,"['abstract-algebra', 'group-theory', 'matrices']"
47,Can the identity matrix be negative?,Can the identity matrix be negative?,,"I got the following question: Find, if possible, the inverse of the matrix: $\begin{bmatrix}3, -1\\2, -2\end{bmatrix} $ and I did the following: $\begin{bmatrix}3, -1\\2, -2\end{bmatrix}^{-1} = \frac{1}{ad-bc}\begin{bmatrix}-2, 1\\-2, 3\end{bmatrix} = \frac{1}{4} \begin{bmatrix}-2, 1\\-2, 3\end{bmatrix}$ Test: $( A * A^{-1} = I)$ $\begin{bmatrix}3, -1\\2, -2\end{bmatrix} *\begin{bmatrix}-0.5,0.25\\-0.5, 0.75\end{bmatrix} =  \begin{bmatrix}-1, 0\\0, -1\end{bmatrix}$ Is this correct?","I got the following question: Find, if possible, the inverse of the matrix: $\begin{bmatrix}3, -1\\2, -2\end{bmatrix} $ and I did the following: $\begin{bmatrix}3, -1\\2, -2\end{bmatrix}^{-1} = \frac{1}{ad-bc}\begin{bmatrix}-2, 1\\-2, 3\end{bmatrix} = \frac{1}{4} \begin{bmatrix}-2, 1\\-2, 3\end{bmatrix}$ Test: $( A * A^{-1} = I)$ $\begin{bmatrix}3, -1\\2, -2\end{bmatrix} *\begin{bmatrix}-0.5,0.25\\-0.5, 0.75\end{bmatrix} =  \begin{bmatrix}-1, 0\\0, -1\end{bmatrix}$ Is this correct?",,[]
48,Finding limits of diagonalised matrix,Finding limits of diagonalised matrix,,"I diagonalised  $A=$ $\left[\begin{array}[c]{rr} 0.6 & 0.9\\ 0.4 & 0.1\end{array}\right]$ and got $SAS^-$$^1$$= V = (-1/13)$ $\left[\begin{array}[c]{rr} -1 & -1\\ -4 & 9\end{array}\right]$  $\left[\begin{array}[c]{rr} 1 & 0\\ 0 & -0.3\end{array}\right]$  $\left[\begin{array}[c]{rr} 9 & 1\\ 4 & -1\end{array}\right]$ now I am supposed to somehow deduce what matrix $V^k$ approaches as $k$ approaches $∞$. How am I supposed to figure that out? Also, I am supposed to find the limit for $SV^kS^-$$^1$ when $k$ approaches $∞$ and specify what the columns of this matrix portray. What came to mind is that when all $|λ|<1$ then $A^k$ approaches $0$, and $λ_1=1$ and $λ_2 = -0.3$. I'm not really sure if that's of any use though. I'm somewhat lost and any help is much appreciated!","I diagonalised  $A=$ $\left[\begin{array}[c]{rr} 0.6 & 0.9\\ 0.4 & 0.1\end{array}\right]$ and got $SAS^-$$^1$$= V = (-1/13)$ $\left[\begin{array}[c]{rr} -1 & -1\\ -4 & 9\end{array}\right]$  $\left[\begin{array}[c]{rr} 1 & 0\\ 0 & -0.3\end{array}\right]$  $\left[\begin{array}[c]{rr} 9 & 1\\ 4 & -1\end{array}\right]$ now I am supposed to somehow deduce what matrix $V^k$ approaches as $k$ approaches $∞$. How am I supposed to figure that out? Also, I am supposed to find the limit for $SV^kS^-$$^1$ when $k$ approaches $∞$ and specify what the columns of this matrix portray. What came to mind is that when all $|λ|<1$ then $A^k$ approaches $0$, and $λ_1=1$ and $λ_2 = -0.3$. I'm not really sure if that's of any use though. I'm somewhat lost and any help is much appreciated!",,"['linear-algebra', 'matrices']"
49,Derivative of a matrix: Outer product chain rule,Derivative of a matrix: Outer product chain rule,,"I ran into a seemingly simple matrix calculus question that I can't seem to find the solution to. Suppose I have the following matrices: $X_{(t \times n)}, V_{(n \times m)}$, and $\Phi_{(t\times m)} = f(XV)$ for some differentiable function $f$, which is applied element-wise to the argument $XV$. I would like to calculate $\frac{\partial}{\partial V} \|1^T\Phi\|_2^2$, which I expanded to the outer product (hopefully correctly) as $\frac{\partial}{\partial V} 1^T \Phi\Phi^T 1 = \frac{\partial}{\partial V} 1^T f(XV) f(XV)^T 1^T$. The Matrix Cookbook states that $\frac{d}{dx} \|x\|_2^2 = \frac{d}{dx} \|x^Tx\|_2 = 2x$. However, I'm not 100% certain I can use this in my case. So far I have that $\frac{\partial}{\partial V} 1^T f(XV) f(XV)^T 1 = 2X^T[f(XV) \circ f^\prime(XV)]$ but my gradient checker (gradest in Matlab) is saying this is incorrect. I've been stuck on this all day, can anyone help? I'm trying to figure out a vectorized solution (not involving for loop summations) since this piece of code will be called iteratively for optimization. Edit : I've confirmed that $\frac{d}{d\Phi} \|1^T \Phi \|_2^2 = 2 \cdot 1 1^T \Phi$.","I ran into a seemingly simple matrix calculus question that I can't seem to find the solution to. Suppose I have the following matrices: $X_{(t \times n)}, V_{(n \times m)}$, and $\Phi_{(t\times m)} = f(XV)$ for some differentiable function $f$, which is applied element-wise to the argument $XV$. I would like to calculate $\frac{\partial}{\partial V} \|1^T\Phi\|_2^2$, which I expanded to the outer product (hopefully correctly) as $\frac{\partial}{\partial V} 1^T \Phi\Phi^T 1 = \frac{\partial}{\partial V} 1^T f(XV) f(XV)^T 1^T$. The Matrix Cookbook states that $\frac{d}{dx} \|x\|_2^2 = \frac{d}{dx} \|x^Tx\|_2 = 2x$. However, I'm not 100% certain I can use this in my case. So far I have that $\frac{\partial}{\partial V} 1^T f(XV) f(XV)^T 1 = 2X^T[f(XV) \circ f^\prime(XV)]$ but my gradient checker (gradest in Matlab) is saying this is incorrect. I've been stuck on this all day, can anyone help? I'm trying to figure out a vectorized solution (not involving for loop summations) since this piece of code will be called iteratively for optimization. Edit : I've confirmed that $\frac{d}{d\Phi} \|1^T \Phi \|_2^2 = 2 \cdot 1 1^T \Phi$.",,"['calculus', 'matrices', 'normed-spaces']"
50,Prove that $W_\lambda$ is a subspace of $R^n$,Prove that  is a subspace of,W_\lambda R^n,"Let $A$ be an $n \times n$ matrix, and let $\lambda$ be an eigenvalue for $A$. The eigenspace of $A$ corresponding to $\lambda$ is the set $$W_\lambda = \{x \in R^n : Ax = \lambda x\}$$ Prove that $W_\lambda$ is a subspace of $R^n$. I am not sure how to prove that. Any known theorems I can make use of?","Let $A$ be an $n \times n$ matrix, and let $\lambda$ be an eigenvalue for $A$. The eigenspace of $A$ corresponding to $\lambda$ is the set $$W_\lambda = \{x \in R^n : Ax = \lambda x\}$$ Prove that $W_\lambda$ is a subspace of $R^n$. I am not sure how to prove that. Any known theorems I can make use of?",,"['linear-algebra', 'matrices']"
51,How prove this matrix limit is $\lim_{m\to\infty}A^mx=\left(\dfrac{e}{n}\right)$,How prove this matrix limit is,\lim_{m\to\infty}A^mx=\left(\dfrac{e}{n}\right),"Question: let $A$ is Doubly stochastic matrix,and the eigenvalue such   $$\lambda_{1}=1,|\lambda_{j}|<1,(j=2,3,\cdots,n)$$   and the $$e=(1,1,1,\cdots,1)^T$$   show that : for any    vector $$x=(a_{1},a_{2},\cdots,a_{n})^T,a_{1}+a_{2}+\cdots +a_{n}=1,a_{i}\ge 0$$, have   $$\lim_{m\to\infty}A^mx=\left(\dfrac{e}{n}\right)$$ where Doubly stochastic matrix some  properties and some result   can see this My idea: since $$|A|=\lambda_{2}\lambda_{3}\cdots\lambda_{n}<1$$ then I can't find this limit. and can't solve this problem,","Question: let $A$ is Doubly stochastic matrix,and the eigenvalue such   $$\lambda_{1}=1,|\lambda_{j}|<1,(j=2,3,\cdots,n)$$   and the $$e=(1,1,1,\cdots,1)^T$$   show that : for any    vector $$x=(a_{1},a_{2},\cdots,a_{n})^T,a_{1}+a_{2}+\cdots +a_{n}=1,a_{i}\ge 0$$, have   $$\lim_{m\to\infty}A^mx=\left(\dfrac{e}{n}\right)$$ where Doubly stochastic matrix some  properties and some result   can see this My idea: since $$|A|=\lambda_{2}\lambda_{3}\cdots\lambda_{n}<1$$ then I can't find this limit. and can't solve this problem,",,['linear-algebra']
52,Find 2D affine transform matrix given a pair of points,Find 2D affine transform matrix given a pair of points,,"I have the coordinates of two points in an initial 2d coordinate system and the corresponding coordinates in a target system. Is is possible to determine the affine transform matrix from these values? Since there are 6 unknowns, I would assume 2 points are not enough. $ \begin{bmatrix}x' & y' & 1\end{bmatrix} =  \begin{bmatrix}x & y & 1\end{bmatrix} \begin{bmatrix}a & b & 0 \\ c & d & 0 \\ t_x & t_y & 1\end{bmatrix} $ But intuitively I don't quite understand why 2 points would not be enough. Consider the 2 finger ""pinch to zoom"" gesture you can use on touch screens. Isn't that exactly this case? You have the coordinates of the fingers when they first touched the screen and you have their current coordinates. And seemingly it's possible to transform the content in a way that always maps the originally touched locations to the current finger positions and scales, translates, rotates the rest accordingly.","I have the coordinates of two points in an initial 2d coordinate system and the corresponding coordinates in a target system. Is is possible to determine the affine transform matrix from these values? Since there are 6 unknowns, I would assume 2 points are not enough. $ \begin{bmatrix}x' & y' & 1\end{bmatrix} =  \begin{bmatrix}x & y & 1\end{bmatrix} \begin{bmatrix}a & b & 0 \\ c & d & 0 \\ t_x & t_y & 1\end{bmatrix} $ But intuitively I don't quite understand why 2 points would not be enough. Consider the 2 finger ""pinch to zoom"" gesture you can use on touch screens. Isn't that exactly this case? You have the coordinates of the fingers when they first touched the screen and you have their current coordinates. And seemingly it's possible to transform the content in a way that always maps the originally touched locations to the current finger positions and scales, translates, rotates the rest accordingly.",,"['linear-algebra', 'matrices', 'geometry', 'transformation', 'affine-geometry']"
53,The exponention map of the matrix,The exponention map of the matrix,,"Let $sl(n)$ denote the set of all $n\times n$ real matrices with trace equal to zero and let $SL(n)$ be the set of all $n\times n$ matrices with determinant equal to one. Let $\varphi(z)$ be a real analytic function defined in a neighborhood of $z=0$ of the complex plane $\mathbb{C}$ satisfying the conditions $\varphi(0)=1$ and $\varphi'(0)=1$. (1) If $\varphi$ maps any near zero matrix in $sl(n)$ into $SL(n)$ for some $n\geq 3$, show that $\varphi(z)=\exp(z)$. (2) Is the conclusion of (1) still true in the case $n=2$? If it is true, prove it. If not, give a counterexample. On (1), for near zero $A$ with $tr(A)=0$, then $det(\varphi(A))=1$. How can we deduce that $\varphi(z)=\exp(z)$.","Let $sl(n)$ denote the set of all $n\times n$ real matrices with trace equal to zero and let $SL(n)$ be the set of all $n\times n$ matrices with determinant equal to one. Let $\varphi(z)$ be a real analytic function defined in a neighborhood of $z=0$ of the complex plane $\mathbb{C}$ satisfying the conditions $\varphi(0)=1$ and $\varphi'(0)=1$. (1) If $\varphi$ maps any near zero matrix in $sl(n)$ into $SL(n)$ for some $n\geq 3$, show that $\varphi(z)=\exp(z)$. (2) Is the conclusion of (1) still true in the case $n=2$? If it is true, prove it. If not, give a counterexample. On (1), for near zero $A$ with $tr(A)=0$, then $det(\varphi(A))=1$. How can we deduce that $\varphi(z)=\exp(z)$.",,"['matrices', 'functions']"
54,Name for multiples of orthogonal matrices,Name for multiples of orthogonal matrices,,"Is there a name for a matrix which is a multiple of an orthogonal matrix? I.e. a square matrix $A$ which satisfies the condition $$A^TA = AA^T = \lambda I$$ where $\lambda$ is some scalar (which should perhaps be required to be non-zero) and $I$ is the identity matrix. Or in other words, a matrix whose rows and columns are orthogonal vectors of equal length $\sqrt\lambda$, but not neccessarily unit length, so not orthonormal. I've thought about these objects twice in different contexts recently, and it feels like a concept that should have a name. So far I haven't been able to find such a name, though. Do you know an established name for these matrices?","Is there a name for a matrix which is a multiple of an orthogonal matrix? I.e. a square matrix $A$ which satisfies the condition $$A^TA = AA^T = \lambda I$$ where $\lambda$ is some scalar (which should perhaps be required to be non-zero) and $I$ is the identity matrix. Or in other words, a matrix whose rows and columns are orthogonal vectors of equal length $\sqrt\lambda$, but not neccessarily unit length, so not orthonormal. I've thought about these objects twice in different contexts recently, and it feels like a concept that should have a name. So far I haven't been able to find such a name, though. Do you know an established name for these matrices?",,"['linear-algebra', 'matrices', 'terminology', 'orthogonal-matrices']"
55,Minimal and Characteristic Polynomials of Matrix Multiplication Transformation,Minimal and Characteristic Polynomials of Matrix Multiplication Transformation,,"Fix a matrix $A \in M_n(F)$ where $F$ is a field, and consider the following linear transformation $\phi_A: M_n(F) \to M_n(F)$ given by $\phi(B) = AB$. Prove that the minimal polynomials of $\phi$ and $A$ are equal. Are their characteristic polynomials equal?","Fix a matrix $A \in M_n(F)$ where $F$ is a field, and consider the following linear transformation $\phi_A: M_n(F) \to M_n(F)$ given by $\phi(B) = AB$. Prove that the minimal polynomials of $\phi$ and $A$ are equal. Are their characteristic polynomials equal?",,"['linear-algebra', 'matrices', 'polynomials']"
56,"How to show whether a $3\times 4$ matrix has no solution, a unique solution or infinitely many solutions?","How to show whether a  matrix has no solution, a unique solution or infinitely many solutions?",3\times 4,"The system is : $$ \begin{matrix} 1 & -4 & 6 & a & | & 0 \\ -2 & 5 & -4 & -1 & | & b \\ 1 & -10 & 22 & 8 & | & c \end{matrix} $$ After Gaussian elimination, I found that $$ \begin{array}{cccc|cc} 1 & -4 & 6 & a &  & 0 \\ 0 & 1 & -\tfrac{8}{3} & - \left( 2a- \tfrac{1}{3} \right) & & - \tfrac{1}{3}b \\ 0 & 0 & 0 & 10-5a & & c-2b \end{array} $$ Is it correct and I can continue to determine whether there is no solution, a unique solution or infinitely many solutions? Here are the operations: $$ \begin{matrix} 1 & -4 & 6 & a & | & 0 \\ -2 & 5 & -4 & -1 & | & b \\ 1 & -10 & 22 & 8 & | & c \end{matrix} $$ $R_2+2R_1\rightarrow R_2$ $$ \begin{matrix} 1 & -4 & 6 & a & | & 0 \\ 0 & -3 & 8 & 2a-1 & | & b \\ 1 & -10 & 22 & 8 & | & c \end{matrix} $$ $R_3-R_1\rightarrow R_3$ $$ \begin{matrix} 1 & -4 & 6 & a & | & 0 \\ 0 & -3 & 8 & 2a-1 & | & b \\ 0 & -6 & 16 & 8-a & | & c \end{matrix} $$ $R_3-2R_2\rightarrow R_3$ $$ \begin{matrix} 1 & -4 & 6 & a & | & 0 \\ 0 & -3 & 8 & 2a-1 & | & b \\ 0 & 0 & 0 & 10-5a & | & c-2b \end{matrix} $$ $-\frac 13(R_2)\rightarrow R_2$ $$ \begin{matrix} 1 & -4 & 6 & a & | & 0 \\ 0 & 1 & -\frac 83 & -\tfrac{2a-1}{3} & | & -\frac 13b \\ 0 & 0 & 0 & 10-5a & | & c-2b \end{matrix} $$","The system is : $$ \begin{matrix} 1 & -4 & 6 & a & | & 0 \\ -2 & 5 & -4 & -1 & | & b \\ 1 & -10 & 22 & 8 & | & c \end{matrix} $$ After Gaussian elimination, I found that $$ \begin{array}{cccc|cc} 1 & -4 & 6 & a &  & 0 \\ 0 & 1 & -\tfrac{8}{3} & - \left( 2a- \tfrac{1}{3} \right) & & - \tfrac{1}{3}b \\ 0 & 0 & 0 & 10-5a & & c-2b \end{array} $$ Is it correct and I can continue to determine whether there is no solution, a unique solution or infinitely many solutions? Here are the operations: $$ \begin{matrix} 1 & -4 & 6 & a & | & 0 \\ -2 & 5 & -4 & -1 & | & b \\ 1 & -10 & 22 & 8 & | & c \end{matrix} $$ $R_2+2R_1\rightarrow R_2$ $$ \begin{matrix} 1 & -4 & 6 & a & | & 0 \\ 0 & -3 & 8 & 2a-1 & | & b \\ 1 & -10 & 22 & 8 & | & c \end{matrix} $$ $R_3-R_1\rightarrow R_3$ $$ \begin{matrix} 1 & -4 & 6 & a & | & 0 \\ 0 & -3 & 8 & 2a-1 & | & b \\ 0 & -6 & 16 & 8-a & | & c \end{matrix} $$ $R_3-2R_2\rightarrow R_3$ $$ \begin{matrix} 1 & -4 & 6 & a & | & 0 \\ 0 & -3 & 8 & 2a-1 & | & b \\ 0 & 0 & 0 & 10-5a & | & c-2b \end{matrix} $$ $-\frac 13(R_2)\rightarrow R_2$ $$ \begin{matrix} 1 & -4 & 6 & a & | & 0 \\ 0 & 1 & -\frac 83 & -\tfrac{2a-1}{3} & | & -\frac 13b \\ 0 & 0 & 0 & 10-5a & | & c-2b \end{matrix} $$",,"['linear-algebra', 'matrices', 'gaussian-elimination']"
57,arrow structure matrices and Sherman-Morrison-Woodbury,arrow structure matrices and Sherman-Morrison-Woodbury,,"I have two questions regarding ""arrow structured"" matrices and I'll be grateful if you can give more insights about them: 1- If $A \in \mathbb{R}^{n \times n}$ is SPD and has the arrow structure, e.g. $$A=\begin{bmatrix}x& x& x& x\\x& x& 0& 0\\x &0 &x &0\\x &0& 0 & x\end{bmatrix}$$ then using Sherman-Morrison-Woodbury how can we solve $Ax=b$ with $O(n)$ flops? 2- How to define a permutation matrix $P$ i.e. $PAP' = GG'$ so that the Cholesky factorization can be computed with $O(n)$ flops? Thanks","I have two questions regarding ""arrow structured"" matrices and I'll be grateful if you can give more insights about them: 1- If $A \in \mathbb{R}^{n \times n}$ is SPD and has the arrow structure, e.g. $$A=\begin{bmatrix}x& x& x& x\\x& x& 0& 0\\x &0 &x &0\\x &0& 0 & x\end{bmatrix}$$ then using Sherman-Morrison-Woodbury how can we solve $Ax=b$ with $O(n)$ flops? 2- How to define a permutation matrix $P$ i.e. $PAP' = GG'$ so that the Cholesky factorization can be computed with $O(n)$ flops? Thanks",,"['matrices', 'inverse']"
58,Rank of a Matrix and Echelon Form to determine ranks.,Rank of a Matrix and Echelon Form to determine ranks.,,"What is the meaning rank of a matrix in terms of vectors, and how does Echelon form work in determining the rank of a matrix?","What is the meaning rank of a matrix in terms of vectors, and how does Echelon form work in determining the rank of a matrix?",,"['linear-algebra', 'matrices', 'vectors']"
59,I have a 2x2 positive-semidefinite matrix. I am trying to find the equation of its elements.,I have a 2x2 positive-semidefinite matrix. I am trying to find the equation of its elements.,,"So long story short. I have a matrix $A \in S^2_+$, that is, a symmetric, positive semi-definite 2x2 matrix. Here it is: $A = \begin{bmatrix} x & y \\y & z \end{bmatrix}$. Here is what it 'looks like' apparently. I have two problems: The first one is that I would like to find out the constraints of the $x,y$, and $z$. For example, that $x \geq0$, $z \geq0$, and the same for $y$. I do not know how to find this. I have tried multiple things like using the characteristic polynomial etc, but no dice. I know that the eigenvalues must be $\geq0$ etc, but... Once I know what the x,y and z are, I think I will be able to interpret this diagram. That is all, I appreciate any help. Thanks!","So long story short. I have a matrix $A \in S^2_+$, that is, a symmetric, positive semi-definite 2x2 matrix. Here it is: $A = \begin{bmatrix} x & y \\y & z \end{bmatrix}$. Here is what it 'looks like' apparently. I have two problems: The first one is that I would like to find out the constraints of the $x,y$, and $z$. For example, that $x \geq0$, $z \geq0$, and the same for $y$. I do not know how to find this. I have tried multiple things like using the characteristic polynomial etc, but no dice. I know that the eigenvalues must be $\geq0$ etc, but... Once I know what the x,y and z are, I think I will be able to interpret this diagram. That is all, I appreciate any help. Thanks!",,"['matrices', 'eigenvalues-eigenvectors', 'determinant', 'symmetry']"
60,"$A+A^T=I$, $\lambda$ is an eigenvalue of $A$, show that $\lambda=\frac{1}{2}+\alpha i$",",  is an eigenvalue of , show that",A+A^T=I \lambda A \lambda=\frac{1}{2}+\alpha i,"I tried to solve it but I got $\lambda =\frac{1}{2}$ without the complex part, I'd like to know where my logic is flawed. Assume $v$ is the eigenvector associated with lambda, then: $(A+A^T)v=Iv$ which quickly implies that $2\lambda v=v$ and so $(2\lambda -1)v=0$. since $v$ isn't the zero vector (zero can't be an eigenvector by definiotion), we get $2\lambda =1 $ and so $\lambda=\frac{1}{2}$. I don't see where the imaginary part comes in.","I tried to solve it but I got $\lambda =\frac{1}{2}$ without the complex part, I'd like to know where my logic is flawed. Assume $v$ is the eigenvector associated with lambda, then: $(A+A^T)v=Iv$ which quickly implies that $2\lambda v=v$ and so $(2\lambda -1)v=0$. since $v$ isn't the zero vector (zero can't be an eigenvector by definiotion), we get $2\lambda =1 $ and so $\lambda=\frac{1}{2}$. I don't see where the imaginary part comes in.",,"['linear-algebra', 'matrices', 'complex-numbers', 'eigenvalues-eigenvectors']"
61,"Given the degrees to rotate around axis, how do you come up with rotation matrix?","Given the degrees to rotate around axis, how do you come up with rotation matrix?",,"Given angles (in degrees) to rotate around, $x$ -, $y$ -, $z$ -axis how does one come up with the rotation matrix? For example if you have a point $p$ represented by a vector, how do you rotate it by multiplying it with a matrix $A$ so $p = Ap$ . From Wikipedia : A basic rotation (also called elemental rotation) is a rotation about one of the axes of a coordinate system. The following three basic rotation matrices rotate vectors by an angle $\theta$ about the $x$ , $y$ , or $z$ axis, in three dimensions: $$R_{x}(\theta)=\left[\begin{array}{ccc}1 & 0 & 0 \\ 0 & \cos \theta & -\sin \theta \\ 0 & \sin \theta & \cos \theta\end{array}\right]$$ $$R_{y}(\theta)=\left[\begin{array}{ccc}\cos \theta & 0 & \sin \theta \\ 0 & 1 & 0 \\ -\sin \theta & 0 & \cos \theta\end{array}\right]$$ $$R_{z}(\theta)=\left[\begin{array}{cc}\cos \theta & -\sin \theta & 0 \\ \sin \theta & \cos \theta & 0 \\ 0 & 0 & 1\end{array}\right]$$ I'm a little unclear. So you need a different rotation matrix for each one of the axis? Is there a way just to get one rotation matrix that covers the rotations done to each axis? Am I even reading this right, so $R_x(\theta)$ is the matrix used to perform the rotation about the $x$ -axis for $\theta$ degrees?","Given angles (in degrees) to rotate around, -, -, -axis how does one come up with the rotation matrix? For example if you have a point represented by a vector, how do you rotate it by multiplying it with a matrix so . From Wikipedia : A basic rotation (also called elemental rotation) is a rotation about one of the axes of a coordinate system. The following three basic rotation matrices rotate vectors by an angle about the , , or axis, in three dimensions: I'm a little unclear. So you need a different rotation matrix for each one of the axis? Is there a way just to get one rotation matrix that covers the rotations done to each axis? Am I even reading this right, so is the matrix used to perform the rotation about the -axis for degrees?",x y z p A p = Ap \theta x y z R_{x}(\theta)=\left[\begin{array}{ccc}1 & 0 & 0 \\ 0 & \cos \theta & -\sin \theta \\ 0 & \sin \theta & \cos \theta\end{array}\right] R_{y}(\theta)=\left[\begin{array}{ccc}\cos \theta & 0 & \sin \theta \\ 0 & 1 & 0 \\ -\sin \theta & 0 & \cos \theta\end{array}\right] R_{z}(\theta)=\left[\begin{array}{cc}\cos \theta & -\sin \theta & 0 \\ \sin \theta & \cos \theta & 0 \\ 0 & 0 & 1\end{array}\right] R_x(\theta) x \theta,"['matrices', 'rotations']"
62,Is $2A^2 \geq AB+BA$ when $A\geq B\geq 0$ ? Always true?,Is  when  ? Always true?,2A^2 \geq AB+BA A\geq B\geq 0,"Let $A$ and $B$ be real (symmetric) and positive definite. It follows that $AB+BA$ is not necessarily positive definite (it can be indefinite, negative definite or positive definite). But now suppose $A\geq B$. Can one always say $2A^2 \geq AB+BA$? Since this ordering implies $2A^2-AB-BA\geq0$ and adding indefinite/neg-def/pos-def to a pos-def matrix may still be pos-def I am supposing the question is well-defined.","Let $A$ and $B$ be real (symmetric) and positive definite. It follows that $AB+BA$ is not necessarily positive definite (it can be indefinite, negative definite or positive definite). But now suppose $A\geq B$. Can one always say $2A^2 \geq AB+BA$? Since this ordering implies $2A^2-AB-BA\geq0$ and adding indefinite/neg-def/pos-def to a pos-def matrix may still be pos-def I am supposing the question is well-defined.",,['matrices']
63,Show a matrix is normal - check my proof,Show a matrix is normal - check my proof,,"Short easy question, I just want someone to double check what I did. We are given that $T$ is an invertible, normal matrix. We are asked to show that $T^{-1}$ is also normal, and find it's unitary diagonlization. What I did: $T$ is normal if and only if there is a unitary matrix $U$ and diagonal matrix $D$ such that $T=UDU^{-1}$ if $T=UDU^{-1}$ then $T^{-1}=(UDU^{-1})^{-1} = U^{-1}D^{-1}U$ $U$ and $U^{-1}$ are still the same (unitary) and $D^{-1}$ is still diagonal. So $T^{-1}$ is unitary diagonlizable, so it is normal, and as stated above, the unitary decomposition is $T^{-1}=U^{-1}D^{-1}U$","Short easy question, I just want someone to double check what I did. We are given that $T$ is an invertible, normal matrix. We are asked to show that $T^{-1}$ is also normal, and find it's unitary diagonlization. What I did: $T$ is normal if and only if there is a unitary matrix $U$ and diagonal matrix $D$ such that $T=UDU^{-1}$ if $T=UDU^{-1}$ then $T^{-1}=(UDU^{-1})^{-1} = U^{-1}D^{-1}U$ $U$ and $U^{-1}$ are still the same (unitary) and $D^{-1}$ is still diagonal. So $T^{-1}$ is unitary diagonlizable, so it is normal, and as stated above, the unitary decomposition is $T^{-1}=U^{-1}D^{-1}U$",,"['linear-algebra', 'matrices', 'proof-verification', 'diagonalization']"
64,Why the largest eigenvalue is the bound?,Why the largest eigenvalue is the bound?,,"I am new to Linear Algebra, say introduction level (know the definition and some basic properties, theories about it but not really familiar with doing math on it). Please explain to me: If A is an inverse of a positive definite matrix, then why all of its eigenvalues are positive and why $\frac{x^TAAx}{1+x^TAx}$ is bounded above by the largest eigenvalue of A? Can you recommend some books that offer a higher level than introduction that can help me get over these kind of problems? I'm learning Machine Learning by the way. Thank you.","I am new to Linear Algebra, say introduction level (know the definition and some basic properties, theories about it but not really familiar with doing math on it). Please explain to me: If A is an inverse of a positive definite matrix, then why all of its eigenvalues are positive and why $\frac{x^TAAx}{1+x^TAx}$ is bounded above by the largest eigenvalue of A? Can you recommend some books that offer a higher level than introduction that can help me get over these kind of problems? I'm learning Machine Learning by the way. Thank you.",,"['linear-algebra', 'matrices', 'eigenvalues-eigenvectors', 'machine-learning', 'vectors']"
65,Divisibility of determinants and matrix rows,Divisibility of determinants and matrix rows,,"I have stumbled upon an exercise that is giving me nighmares. I've found that it is quite common in older exercise books, but I haven't even heard about it in class or seen it in any lecture in all my years in college, even if it seems to be a basic and simple problem. Naturally, as it happens, these promblem books don't have a solution for it (They just skip it in the solution pages). Here it is: Given the matrix A=$ \left( \begin{array}{ccc} a_{11} & a_{12} & ... &  a_{1n} \\ a_{21} & a_{22} & ... &  a_{2n} \\ \vdots & \vdots & \ddots & \vdots & \\ a_{n1} & a_{n2} & ... &  a_{nn} \end{array} \right)  \in M_{n}(N) / (a_{11}, ..., a_{1n})$ are single-digit natural positive numbers, prove that the determinant of the matrix A is divisible by the GCD of the numbers formed by the rows of the matrix. I have tried applying determinant proprieties, solving the actual determinant and juggling with everything I know, but I just have no idea about how to do this. It is very unlikely that they ask us something like this at all, but I am very cusious and I want to know how can this be explained. Edit: Since this is confusing to write, here is another exercise I found using this: The numbers 20604, 53227, 25755, 20927 and 78421 are all divisible by 17. Prove that the determinant $\left| \begin{array}{ccc} 2&0&6&0&4 \\ 5&3&2&2&7 \\ 2&5&7&5&5 \\ 2&0&9&2&7 \\ 7&8&4&2&1 \end{array} \right|.$ is also divisible by 17 It's the same concept and it can be seen more clearly.","I have stumbled upon an exercise that is giving me nighmares. I've found that it is quite common in older exercise books, but I haven't even heard about it in class or seen it in any lecture in all my years in college, even if it seems to be a basic and simple problem. Naturally, as it happens, these promblem books don't have a solution for it (They just skip it in the solution pages). Here it is: Given the matrix A=$ \left( \begin{array}{ccc} a_{11} & a_{12} & ... &  a_{1n} \\ a_{21} & a_{22} & ... &  a_{2n} \\ \vdots & \vdots & \ddots & \vdots & \\ a_{n1} & a_{n2} & ... &  a_{nn} \end{array} \right)  \in M_{n}(N) / (a_{11}, ..., a_{1n})$ are single-digit natural positive numbers, prove that the determinant of the matrix A is divisible by the GCD of the numbers formed by the rows of the matrix. I have tried applying determinant proprieties, solving the actual determinant and juggling with everything I know, but I just have no idea about how to do this. It is very unlikely that they ask us something like this at all, but I am very cusious and I want to know how can this be explained. Edit: Since this is confusing to write, here is another exercise I found using this: The numbers 20604, 53227, 25755, 20927 and 78421 are all divisible by 17. Prove that the determinant $\left| \begin{array}{ccc} 2&0&6&0&4 \\ 5&3&2&2&7 \\ 2&5&7&5&5 \\ 2&0&9&2&7 \\ 7&8&4&2&1 \end{array} \right|.$ is also divisible by 17 It's the same concept and it can be seen more clearly.",,['linear-algebra']
66,Is a Lyapunov equation always solvable when matrix $A$ is negative definite?,Is a Lyapunov equation always solvable when matrix  is negative definite?,A,"Given a negative definite matrix $A$ , is the following Lyapunov equation in $P$ $$P A + A^T P = -I$$ always solvable? What kind of form does the solution have? I would appreciate if examples could be given.","Given a negative definite matrix , is the following Lyapunov equation in always solvable? What kind of form does the solution have? I would appreciate if examples could be given.",A P P A + A^T P = -I,"['matrices', 'systems-of-equations', 'matrix-equations', 'control-theory', 'linear-control']"
67,"Show that T is a linear transformation and find a, b, c","Show that T is a linear transformation and find a, b, c",,"I'm having trouble understanding this question and the proper way to solve it. I don't understand the solution given and why this was the right way to answer it. Problem: For the vector space $P_3$ of polynomials of degree less than or equal to 3, let $T:P_3 \rightarrow R$ be the function $$T(p)=p(2)+p(3)$$ Show that $T$ is a linear transformation, and find numbers a, b, and c so that $$T(x+a)=T(x^2+b)=T(x^3+c)=0$$ Solution: \begin{align*} T(p+q) &= (p+q)(2)+(p+q)(3) \\ &= \left(p(2)+q(2)\right)+\left(p(3)+q(3)\right)\\ &= \left(p(2)+p(3)\right)+\left(q(2)+q(3)\right)\\ &= T(p)+T(q) \\\\ T(cp) &= (cp)(2) +(cp)(3) \\ &= c\left(p(2)\right) + c\left(p(3)\right) \\ &= c\left(p(2)+p(3)\right) \\ &= cT(p) \end{align*} Because of these proofs $T$ is a linear transformation. \begin{align*} T(x+a)&=(2+a)+(3+a)\\ &=2a+5\\ &=0 \\\\ a&=-\frac{5}{2}\\\\ &...\\\\ b&=-\frac{13}{2}\\\\ c&=-\frac{35}{2} \end{align*} My Confusion: What is ""$T:P_3\rightarrow R$"" in words? What is $P_3$ and how does it relate to the problem? What does the lowercase $p$ in $T(p)=p(2)+p(3)$ represent and why can you replace it? How does $\left(p(2)+p(3)\right)+\left(q(2)+q(3)\right)= T(p)+T(q)$?  or $c\left(p(2)+p(3)\right)= cT(p)$? Why isn't it intuitive that $T(p)=p(2)+p(3)$ is a transformation?","I'm having trouble understanding this question and the proper way to solve it. I don't understand the solution given and why this was the right way to answer it. Problem: For the vector space $P_3$ of polynomials of degree less than or equal to 3, let $T:P_3 \rightarrow R$ be the function $$T(p)=p(2)+p(3)$$ Show that $T$ is a linear transformation, and find numbers a, b, and c so that $$T(x+a)=T(x^2+b)=T(x^3+c)=0$$ Solution: \begin{align*} T(p+q) &= (p+q)(2)+(p+q)(3) \\ &= \left(p(2)+q(2)\right)+\left(p(3)+q(3)\right)\\ &= \left(p(2)+p(3)\right)+\left(q(2)+q(3)\right)\\ &= T(p)+T(q) \\\\ T(cp) &= (cp)(2) +(cp)(3) \\ &= c\left(p(2)\right) + c\left(p(3)\right) \\ &= c\left(p(2)+p(3)\right) \\ &= cT(p) \end{align*} Because of these proofs $T$ is a linear transformation. \begin{align*} T(x+a)&=(2+a)+(3+a)\\ &=2a+5\\ &=0 \\\\ a&=-\frac{5}{2}\\\\ &...\\\\ b&=-\frac{13}{2}\\\\ c&=-\frac{35}{2} \end{align*} My Confusion: What is ""$T:P_3\rightarrow R$"" in words? What is $P_3$ and how does it relate to the problem? What does the lowercase $p$ in $T(p)=p(2)+p(3)$ represent and why can you replace it? How does $\left(p(2)+p(3)\right)+\left(q(2)+q(3)\right)= T(p)+T(q)$?  or $c\left(p(2)+p(3)\right)= cT(p)$? Why isn't it intuitive that $T(p)=p(2)+p(3)$ is a transformation?",,"['linear-algebra', 'matrices', 'functions', 'polynomials', 'transformation']"
68,Check if matrix determinant is zero,Check if matrix determinant is zero,,"What's the simplest way to check if a NxN Matrix determinant is zero ? Using Gauss Jordan to calculate the determinant first is to complicated (took N^3 calculation), is there any way to know it in at most (N^2 calculation). Anyway the matrix is always in form of this (the first row is known value) : 2x2 matrix  $\begin{bmatrix} a & b \\ b & a \\ \end{bmatrix}$ 3x3 matrix  $\begin{bmatrix} a & b & c \\ b & c & a \\ c & a & b \end{bmatrix}$ 4x4 matrix  $\begin{bmatrix} a & b & c & d\\ b & c & d & a\\ c & d & a & b\\ d & a & b & c \end{bmatrix}$","What's the simplest way to check if a NxN Matrix determinant is zero ? Using Gauss Jordan to calculate the determinant first is to complicated (took N^3 calculation), is there any way to know it in at most (N^2 calculation). Anyway the matrix is always in form of this (the first row is known value) : 2x2 matrix  $\begin{bmatrix} a & b \\ b & a \\ \end{bmatrix}$ 3x3 matrix  $\begin{bmatrix} a & b & c \\ b & c & a \\ c & a & b \end{bmatrix}$ 4x4 matrix  $\begin{bmatrix} a & b & c & d\\ b & c & d & a\\ c & d & a & b\\ d & a & b & c \end{bmatrix}$",,"['matrices', 'determinant']"
69,Calculating a basis of vector space $U \cap V$,Calculating a basis of vector space,U \cap V,"So I have two vector spaces: $ U := \langle(1,2,1,2), (1,2,3,3), (1,2,2,3)\rangle $ and $ V := \langle(2,0,2,1), (3,2,3,2), (0,4,0,1)\rangle $ I was able to calculate the base of both $U$ and $V$: $ B_U = \langle(1,2,1,2), (1,2,3,3), (1,2,2,3)\rangle $ since the vectors linearly independent. $ B_V = \langle(2,0,2,1), (3,2,3,2))\rangle $ since you can write $(0,4,0,1)$ as $2*(3,2,3,2) - 3*(2,0,2,1)$. However, I have no clue for to do it for $U \cap V$. Could you please point out how to go about doing that and/or giving me an example? Thanks in advance.","So I have two vector spaces: $ U := \langle(1,2,1,2), (1,2,3,3), (1,2,2,3)\rangle $ and $ V := \langle(2,0,2,1), (3,2,3,2), (0,4,0,1)\rangle $ I was able to calculate the base of both $U$ and $V$: $ B_U = \langle(1,2,1,2), (1,2,3,3), (1,2,2,3)\rangle $ since the vectors linearly independent. $ B_V = \langle(2,0,2,1), (3,2,3,2))\rangle $ since you can write $(0,4,0,1)$ as $2*(3,2,3,2) - 3*(2,0,2,1)$. However, I have no clue for to do it for $U \cap V$. Could you please point out how to go about doing that and/or giving me an example? Thanks in advance.",,"['linear-algebra', 'matrices', 'vector-spaces']"
70,Rank of a given matrix,Rank of a given matrix,,"Going through many articles, questions on MSE and my book; I now know how by looking at a given matrix we can tell whether it is of full rank or not: its columns must be independent to be  of full rank. My question is, suppose matrix $A$ is not  full rank, then what can be said about its rank in general? Thank you.","Going through many articles, questions on MSE and my book; I now know how by looking at a given matrix we can tell whether it is of full rank or not: its columns must be independent to be  of full rank. My question is, suppose matrix $A$ is not  full rank, then what can be said about its rank in general? Thank you.",,"['linear-algebra', 'matrices']"
71,How find this invertible matrix $C=\left[\begin{smallmatrix} A&B\\ B^T&0 \end{smallmatrix}\right]$,How find this invertible matrix,C=\left[\begin{smallmatrix} A&B\\ B^T&0 \end{smallmatrix}\right],"let matrix $A_{n\times n}$,and $\det(A)>0$, and the matrix $B_{n\times m}$,and such $rank(B)=m$,and let $$C=\begin{bmatrix} A&B\\ B^T&0 \end{bmatrix}$$ Find this Invertible matrix  $C^{-1}$ my try: I found this matrix Invertible matrix $C$,it must find $B^TAB$ Invertible matrix.But I can't Thank you for your help","let matrix $A_{n\times n}$,and $\det(A)>0$, and the matrix $B_{n\times m}$,and such $rank(B)=m$,and let $$C=\begin{bmatrix} A&B\\ B^T&0 \end{bmatrix}$$ Find this Invertible matrix  $C^{-1}$ my try: I found this matrix Invertible matrix $C$,it must find $B^TAB$ Invertible matrix.But I can't Thank you for your help",,"['linear-algebra', 'matrices']"
72,"name of matrix of inner products $\langle f_i, f_j\rangle$",name of matrix of inner products,"\langle f_i, f_j\rangle","Given a Hilbert space $H$ and a number of elements $\phi_i\in H$, does the matrix $M$ with $$ M_{i,j} := \langle\phi_i, \phi_j\rangle $$ have any particular name?","Given a Hilbert space $H$ and a number of elements $\phi_i\in H$, does the matrix $M$ with $$ M_{i,j} := \langle\phi_i, \phi_j\rangle $$ have any particular name?",,"['matrices', 'hilbert-spaces', 'inner-products']"
73,Correspondence between eigenvalues and eigenvectors in ellipsoids,Correspondence between eigenvalues and eigenvectors in ellipsoids,,"think of an ellipsoid in the n-dimensional space defined by $$(x-\mu)'A(x-\mu)=1.$$ I was calculating the volumes of n-dimensional ellipsoids like the one from above for a while, which is straightforward once the eigenvalues of matrix $A$ are retrieved. The volume $V$ is then given by (using the log scale so that it will not create numerical problems for very large $n$) $$ V_{0} = log(1),\\ V_{1} = log(2), $$ and then for any $i>1$ $$ V_{i} = log(\frac{2\pi}{i-1}) + V_{i-2},$$ and finally $$V = V_{n} + \sum\limits_{i=1}^{n} log(\lambda_{i}^{-\frac{1}{2}}),$$ where each $\lambda_{i}$ refers to one of the $n$ eigenvalues of $A$. Now we know that each eigenvector of $A$ is the orientation of an axis of the ellipsoid and each $\lambda^{-\frac{1}{2}}$ refers to the half length of one of these axes. Nevertheless, most (all that I know) mathematical routines that calculate the eigenvalues of $A$ will give the result in a meaningless order. Thus, I will know all half length of the axes of the ellipsoid, but I do not know which eigenvalue corresponds to which eigenvector. I can find this out (I guess) when testing for each eigenvector $v_{k}$ if $$ Av_{k}=\lambda_{i}v_{k}$$ and the $\lambda_{i}$ for which the above holds, will then be the eigenvalue corresponding to $v_{k}$. Nevertheless, my software (R) return eigenvectors normalized to length 1, so the above identity never exists. My question would be if someone could help me to find out how to sort the eigenvalues anyway so that the first eigenvalue corresponds to the first eigenvector and so on... Every help is highly appreciated, J PS: my post is somewhat related to this post and this post , but finally could not figure out how to answer my question with these posts.","think of an ellipsoid in the n-dimensional space defined by $$(x-\mu)'A(x-\mu)=1.$$ I was calculating the volumes of n-dimensional ellipsoids like the one from above for a while, which is straightforward once the eigenvalues of matrix $A$ are retrieved. The volume $V$ is then given by (using the log scale so that it will not create numerical problems for very large $n$) $$ V_{0} = log(1),\\ V_{1} = log(2), $$ and then for any $i>1$ $$ V_{i} = log(\frac{2\pi}{i-1}) + V_{i-2},$$ and finally $$V = V_{n} + \sum\limits_{i=1}^{n} log(\lambda_{i}^{-\frac{1}{2}}),$$ where each $\lambda_{i}$ refers to one of the $n$ eigenvalues of $A$. Now we know that each eigenvector of $A$ is the orientation of an axis of the ellipsoid and each $\lambda^{-\frac{1}{2}}$ refers to the half length of one of these axes. Nevertheless, most (all that I know) mathematical routines that calculate the eigenvalues of $A$ will give the result in a meaningless order. Thus, I will know all half length of the axes of the ellipsoid, but I do not know which eigenvalue corresponds to which eigenvector. I can find this out (I guess) when testing for each eigenvector $v_{k}$ if $$ Av_{k}=\lambda_{i}v_{k}$$ and the $\lambda_{i}$ for which the above holds, will then be the eigenvalue corresponding to $v_{k}$. Nevertheless, my software (R) return eigenvectors normalized to length 1, so the above identity never exists. My question would be if someone could help me to find out how to sort the eigenvalues anyway so that the first eigenvalue corresponds to the first eigenvector and so on... Every help is highly appreciated, J PS: my post is somewhat related to this post and this post , but finally could not figure out how to answer my question with these posts.",,"['linear-algebra', 'matrices', 'geometry', 'eigenvalues-eigenvectors', 'ellipsoids']"
74,Divergence-free vector field from skew-symmetric matrix,Divergence-free vector field from skew-symmetric matrix,,"Let $[a_{i,j}(x_1,\ldots,x_n)]$ be a skew-symmetric $n\times n$ matrix of functions $a_{i,j}\in C^\infty(\mathbb{R}^n)$. Show that the vector field $$v=\sum\left(\dfrac{\partial}{\partial x_i}a_{i,j}\right)\dfrac{\partial}{\partial x_j}$$ is divergence-free. It looks like there's a typo somewhere in this problem. For one thing, it looks like there should be a double summation, one over $i$ and the other one over $j$.  Should it be $$\sum_{j=1}^n\left(\sum_{i=1}^n\dfrac{\partial}{\partial x_i}a_{i,j}\right)\dfrac{\partial}{\partial x_j}?$$ The definition of a vector field $v$ to be divergence-free is that the divergence $\sum_{j=1}^n\dfrac{\partial v_j}{\partial x_j}=0$.","Let $[a_{i,j}(x_1,\ldots,x_n)]$ be a skew-symmetric $n\times n$ matrix of functions $a_{i,j}\in C^\infty(\mathbb{R}^n)$. Show that the vector field $$v=\sum\left(\dfrac{\partial}{\partial x_i}a_{i,j}\right)\dfrac{\partial}{\partial x_j}$$ is divergence-free. It looks like there's a typo somewhere in this problem. For one thing, it looks like there should be a double summation, one over $i$ and the other one over $j$.  Should it be $$\sum_{j=1}^n\left(\sum_{i=1}^n\dfrac{\partial}{\partial x_i}a_{i,j}\right)\dfrac{\partial}{\partial x_j}?$$ The definition of a vector field $v$ to be divergence-free is that the divergence $\sum_{j=1}^n\dfrac{\partial v_j}{\partial x_j}=0$.",,"['real-analysis', 'matrices', 'vector-fields']"
75,"Prove $X=0$, if $\det(I+pX)=1$ and $(I+pX)^n=I$,","Prove , if  and ,",X=0 \det(I+pX)=1 (I+pX)^n=I,"Question: Let the matrix $X=(a_{ij})_{2\times 2},a_{ij}\in Z,$, and $p> 2,p\neq 4, p\in \mathbb{Z}$, $(1):$   such that $\det(I+pX)=1$. $(2):$Suppose there exists positive integer $n$ such $(I+pX)^n=I$, Show that: $X=0$. My try: let $$X=\begin{bmatrix} a&b\\ c&d \end{bmatrix} $$ where $a,b,c,d\in Z$ since $X\in M_{2}(Z)$,and $\det(I+pX)=1$,  then we have $$\begin{vmatrix} pa+1&pb\\ pc&pd+1 \end{vmatrix}=1 $$ then $$(pa+1)(pd+1)-p^2bc=1$$ $$p^2ad+pa+pd+1=p^2bc+1 \Longrightarrow pad+a+d-pbc=0$$ $$\Longrightarrow p(ad-bc)+a+d=0$$ and since there exist $n$ such that $$(I+pX)^n=I$$   $$\Longrightarrow \begin{bmatrix} pa+1&pb\\ pc&pd+1 \end{bmatrix}^n=\begin{bmatrix} 1&0\\ 0&1\\ \end{bmatrix}$$   and Then I can't prove $a=b=c=d=0$. maybe this problem use other methods? Thank you  Salman post his solution,But this is wrong,because  $$I + p\binom{n}{2}X + \ldots + p^{n-1}\binom{n}{n-1}X^{n-1} + p^n X^n = I.$$ we can't have this Taking determinants of our equation, we get: $$1 + \binom{n}{2}(a + d) + \ldots + \binom{n}{n-1}(a+d)^{n-1} + (a + d)^n = 1.$$ because  $$det(A_{1}+A_{2}+\cdots+A_{n})\neq det(A_{1})+det(A_{2})+\cdots+det(A_{n})$$ Thank you very much!","Question: Let the matrix $X=(a_{ij})_{2\times 2},a_{ij}\in Z,$, and $p> 2,p\neq 4, p\in \mathbb{Z}$, $(1):$   such that $\det(I+pX)=1$. $(2):$Suppose there exists positive integer $n$ such $(I+pX)^n=I$, Show that: $X=0$. My try: let $$X=\begin{bmatrix} a&b\\ c&d \end{bmatrix} $$ where $a,b,c,d\in Z$ since $X\in M_{2}(Z)$,and $\det(I+pX)=1$,  then we have $$\begin{vmatrix} pa+1&pb\\ pc&pd+1 \end{vmatrix}=1 $$ then $$(pa+1)(pd+1)-p^2bc=1$$ $$p^2ad+pa+pd+1=p^2bc+1 \Longrightarrow pad+a+d-pbc=0$$ $$\Longrightarrow p(ad-bc)+a+d=0$$ and since there exist $n$ such that $$(I+pX)^n=I$$   $$\Longrightarrow \begin{bmatrix} pa+1&pb\\ pc&pd+1 \end{bmatrix}^n=\begin{bmatrix} 1&0\\ 0&1\\ \end{bmatrix}$$   and Then I can't prove $a=b=c=d=0$. maybe this problem use other methods? Thank you  Salman post his solution,But this is wrong,because  $$I + p\binom{n}{2}X + \ldots + p^{n-1}\binom{n}{n-1}X^{n-1} + p^n X^n = I.$$ we can't have this Taking determinants of our equation, we get: $$1 + \binom{n}{2}(a + d) + \ldots + \binom{n}{n-1}(a+d)^{n-1} + (a + d)^n = 1.$$ because  $$det(A_{1}+A_{2}+\cdots+A_{n})\neq det(A_{1})+det(A_{2})+\cdots+det(A_{n})$$ Thank you very much!",,"['linear-algebra', 'matrices']"
76,$A$ is similar to $B$ if $A\oplus A$ is similar to $B\oplus B$,is similar to  if  is similar to,A B A\oplus A B\oplus B,"Question: If the matrix $\begin{pmatrix} A & 0 \\ 0& A \end{pmatrix}$ is similar to $\begin{pmatrix} B & 0 \\ 0 & B \end{pmatrix}$ show that: the matrix $A$ is similar the matrix $B$ My try: since  the matrix diag $(A,A)$  is  similar  matrix diag $(B,B)$; and  I want use this elementary divisor,But there is not in The plural number field then I can't.Thank you someone help me,Thank you  very much!","Question: If the matrix $\begin{pmatrix} A & 0 \\ 0& A \end{pmatrix}$ is similar to $\begin{pmatrix} B & 0 \\ 0 & B \end{pmatrix}$ show that: the matrix $A$ is similar the matrix $B$ My try: since  the matrix diag $(A,A)$  is  similar  matrix diag $(B,B)$; and  I want use this elementary divisor,But there is not in The plural number field then I can't.Thank you someone help me,Thank you  very much!",,"['linear-algebra', 'matrices']"
77,"If A is invertible, prove that $\lambda \neq 0$, and $\vec{v}$ is also an eigenvector for $A^{-1}$, what is the corresponding eigenvalue?","If A is invertible, prove that , and  is also an eigenvector for , what is the corresponding eigenvalue?",\lambda \neq 0 \vec{v} A^{-1},"If A is invertible, prove that $\lambda \neq 0$, and $\vec{v}$ is also an eigenvector for $A^{-1}$, what is the corresponding eigenvalue? I don't really know where to start with this one. I know that $p(0)=det(0*I_{n}-A)=det(-A)=(-1)^{n}*det(A)$, thus if both $p(0)$ and $det(0) = 0$ then $0$ is an eigenvalue of A and A is not invertible. If neither are $0$, then $0$ is not an eigenvalue of A and thus A is invertible. I'm unsure of how to use this information to prove $\vec{v}$ is also an eigenvector for $A^{-1}$ and how to find a corresponding eigenvalue.","If A is invertible, prove that $\lambda \neq 0$, and $\vec{v}$ is also an eigenvector for $A^{-1}$, what is the corresponding eigenvalue? I don't really know where to start with this one. I know that $p(0)=det(0*I_{n}-A)=det(-A)=(-1)^{n}*det(A)$, thus if both $p(0)$ and $det(0) = 0$ then $0$ is an eigenvalue of A and A is not invertible. If neither are $0$, then $0$ is not an eigenvalue of A and thus A is invertible. I'm unsure of how to use this information to prove $\vec{v}$ is also an eigenvector for $A^{-1}$ and how to find a corresponding eigenvalue.",,"['linear-algebra', 'matrices', 'eigenvalues-eigenvectors', 'determinant']"
78,QR-factorisation using Givens-rotation. Find upper triangular matrix using Givens-rotation.,QR-factorisation using Givens-rotation. Find upper triangular matrix using Givens-rotation.,,"I'm looking into QR-factorisation using Givens-rotations and I want to transform matrices into their upper triangular matrices. I know how to do this for matrix $ B \in \mathbb{R}^{m\times m}$ but how do you do this for a matrix $ A \in \mathbb{R}^{m\times n}$? My problem is that the Givens-rotation matrix is by definition a $ m \times m $ matrix and that I think I have to do something special to matrix $A$ or to the Givens-rotation matrix. So how do we find for example $G^{(1)^{T}}_{34}A$ if $$ A = \begin{bmatrix}         1 & -2 & 3 \\         1 & -3 & 5 \\         2 & 3 & 1 \\        -3 & 4 & 2\\      \end{bmatrix} $$ The solution should be $$  G^{(1)^{T}}_{34}A =      \begin{bmatrix}         1 & -2 & 3 \\         1 & -3 & 5 \\         3.6056 & -1.6641 & -1.1094 \\         0 & 4.7150 & 1.9415\\      \end{bmatrix} $$ but I don't know how to calculate this. Can someone tell me where I go wrong in the following calculation? Step 1: $$  G^{(1)}_{34} =      \begin{bmatrix}         1 & 0 & 0 & 0\\         0 & 1 & 0 & 0\\         0 & 0 & c & -s \\         0 & 0 & s & c\\      \end{bmatrix} $$ Step 2: We add a zero-vector to $A$ because  $ A \in \mathbb{R}^{m\times n}$ and the matrix A must have form $m \times m$ to use Givens-rotations. We create $$  A' =      \begin{bmatrix}         1 & -2 & 3 & 0\\         1 & -3 & 5 & 0\\         2 & 3 & 1 & 0\\        -3 & 4 & 2 & 0\\      \end{bmatrix} $$ Step 3: We calculate c and s. $$ c = 1 / (\sqrt{1^2 + 2^2}) $$ $$ s = 2 / (\sqrt{1^2 + 2^2}) $$ Step 4: We fill in the values $c$ and $s$ in matrix $G^{(1)}_{34}$ and calculate the transpose $G^{(1)^{T}}_{34}$. Step 5: We multiply $G^{(1)^{T}}_{34}$ and $A'$. It doesn't return a matrix with $A_{1,4} = 0$ when I use the previous steps so where do I go wrong?","I'm looking into QR-factorisation using Givens-rotations and I want to transform matrices into their upper triangular matrices. I know how to do this for matrix $ B \in \mathbb{R}^{m\times m}$ but how do you do this for a matrix $ A \in \mathbb{R}^{m\times n}$? My problem is that the Givens-rotation matrix is by definition a $ m \times m $ matrix and that I think I have to do something special to matrix $A$ or to the Givens-rotation matrix. So how do we find for example $G^{(1)^{T}}_{34}A$ if $$ A = \begin{bmatrix}         1 & -2 & 3 \\         1 & -3 & 5 \\         2 & 3 & 1 \\        -3 & 4 & 2\\      \end{bmatrix} $$ The solution should be $$  G^{(1)^{T}}_{34}A =      \begin{bmatrix}         1 & -2 & 3 \\         1 & -3 & 5 \\         3.6056 & -1.6641 & -1.1094 \\         0 & 4.7150 & 1.9415\\      \end{bmatrix} $$ but I don't know how to calculate this. Can someone tell me where I go wrong in the following calculation? Step 1: $$  G^{(1)}_{34} =      \begin{bmatrix}         1 & 0 & 0 & 0\\         0 & 1 & 0 & 0\\         0 & 0 & c & -s \\         0 & 0 & s & c\\      \end{bmatrix} $$ Step 2: We add a zero-vector to $A$ because  $ A \in \mathbb{R}^{m\times n}$ and the matrix A must have form $m \times m$ to use Givens-rotations. We create $$  A' =      \begin{bmatrix}         1 & -2 & 3 & 0\\         1 & -3 & 5 & 0\\         2 & 3 & 1 & 0\\        -3 & 4 & 2 & 0\\      \end{bmatrix} $$ Step 3: We calculate c and s. $$ c = 1 / (\sqrt{1^2 + 2^2}) $$ $$ s = 2 / (\sqrt{1^2 + 2^2}) $$ Step 4: We fill in the values $c$ and $s$ in matrix $G^{(1)}_{34}$ and calculate the transpose $G^{(1)^{T}}_{34}$. Step 5: We multiply $G^{(1)^{T}}_{34}$ and $A'$. It doesn't return a matrix with $A_{1,4} = 0$ when I use the previous steps so where do I go wrong?",,"['linear-algebra', 'matrices']"
79,"Given $A, B\in R^{n\times n}$ diagonal matrices, there exist $p,q \in R[x]$ and $X\in R^{n\times n}$ such that $A = p(X),B=q(X)$","Given  diagonal matrices, there exist  and  such that","A, B\in R^{n\times n} p,q \in R[x] X\in R^{n\times n} A = p(X),B=q(X)","(1) We are given $A,B \in R^{n\times n}$ diagonal matrices of n rows and n columns with real values. Show that there are $X \in R^{n\times n}$ and polynomials $q$ and $p$ such that: $$q(X)=B \text{ and }p(X)=A$$ What I've tried doing: I've tried a different combinations of X and p,q, for example: If we say that $X=A$ and we define $q(X) = X-A+B$ and $p(X)=X$ we indeed solve the problem BUT those p and q are not polynomials, because if i use a scalar as an input for those polynomials instead of a matrix, it doesn't work. There is no scalar version of the matrix A (Unlike for example the unit matrix, which the scalar version of which is 1) So that isn't the way to solve the question (2) Is this correct for any 2 matrices or just for diagonal matrices? Hint: What basic difference is there between the polynomial ring and the matrix ring? (Haven't started working on this yet cause didn't solve question 1, but it probably involves that the matrix multiplication isnt commutative)","(1) We are given diagonal matrices of n rows and n columns with real values. Show that there are and polynomials and such that: What I've tried doing: I've tried a different combinations of X and p,q, for example: If we say that and we define and we indeed solve the problem BUT those p and q are not polynomials, because if i use a scalar as an input for those polynomials instead of a matrix, it doesn't work. There is no scalar version of the matrix A (Unlike for example the unit matrix, which the scalar version of which is 1) So that isn't the way to solve the question (2) Is this correct for any 2 matrices or just for diagonal matrices? Hint: What basic difference is there between the polynomial ring and the matrix ring? (Haven't started working on this yet cause didn't solve question 1, but it probably involves that the matrix multiplication isnt commutative)","A,B \in R^{n\times n} X \in R^{n\times n} q p q(X)=B \text{ and }p(X)=A X=A q(X) = X-A+B p(X)=X","['matrices', 'polynomials']"
80,"Given $A$ and $B$ positive-definite matrices and $Q$ unitary matrix, prove that if $A = BQ$, then $A=B$.","Given  and  positive-definite matrices and  unitary matrix, prove that if , then .",A B Q A = BQ A=B,"Given $A$ and $B$ positive-definite matrices and $Q$ unitary matrix, prove that if $A = BQ$ , then $A=B$ . $Q$ is unitary, so $QQ^*=I$ If $A$ and $B$ are positive-definite, than $A=A^*$ and $B=B^*$ . $A^*=(BQ)^*=Q^*B^*$ $A^2=AA=AA^*=(BQ)(Q^*B^*)=B(QQ^*)B^*=BIB^*=B^2$ $A^2=B^2$ . I don't know how to use the fact that $A$ and $B$ are positive-definite to finish the proof.","Given and positive-definite matrices and unitary matrix, prove that if , then . is unitary, so If and are positive-definite, than and . . I don't know how to use the fact that and are positive-definite to finish the proof.",A B Q A = BQ A=B Q QQ^*=I A B A=A^* B=B^* A^*=(BQ)^*=Q^*B^* A^2=AA=AA^*=(BQ)(Q^*B^*)=B(QQ^*)B^*=BIB^*=B^2 A^2=B^2 A B,"['linear-algebra', 'matrices']"
81,"I've seen ""hyperbolic rotation"" - from this: generalization to multisection rotation: is this possible?","I've seen ""hyperbolic rotation"" - from this: generalization to multisection rotation: is this possible?",,"This question is more in recreational mathematics area By accident I came across the concept of ""hyperbolic rotation"" where we use a matrix containing $\cosh$ and $\sinh$ instead of the trigonometric $\cos$ and $\sin$ for the rotation by matrix-multiplication such that we have: $$ \tag{trigonometric} \qquad T_t(\varphi) = \begin{bmatrix} \cos \varphi &  \sin \varphi \\  -\sin \varphi &  \cos \varphi  \end{bmatrix} $$ $$ \tag{hyperbolic} \qquad T_h(\varphi) = \begin{bmatrix} \cosh \varphi &  \sinh \varphi \\  \sinh \varphi &  \cosh \varphi  \end{bmatrix} $$ A key-feature is surely, that both rotation-matrices have a determinant of $1$ ( because $$\small \cos^2(\varphi) + \sin^2(\varphi) = 1 \tag{trigonometric}  $$ and $$\small \cosh^2(\varphi) - \sinh^2(\varphi) = 1 \tag{hyperbolic} $$ ) . Now I toyed a bit to extend this to the case of 3-multisection series of the exponential; an example for what I mean is my older question ; let's call that three functions just $f(x),g(x),h(x)$ such that $f(x)+g(x)+h(x) = \exp(x)$ and the analogon to the square-formulae which equal $1$ is: $$ f(x)^3  + g(x)^3 + h(x)^3 - 3f(x)g(x)h(x) = 1 \tag{3-multisection} $$ At least, such a ""rotation""-matrix must have size of $3 \times 3$ but possibly even more - if it is constructable at all. Qu1: Is such a generalization to higher multisections (here order 3) possible? Qu2: and if, how could such a ""rotation""- matrix be contructed? [Update]: I've just found, that  $$ \tag{3-multisection} \qquad T_{3m}(\varphi) = \begin{bmatrix} f(\varphi) & h(\varphi) & g(\varphi)\\  g(\varphi)&f(\varphi)&h(\varphi)\\ h(\varphi) & g(\varphi) &f(\varphi)  \end{bmatrix}$$ has determinant $1$ and could be a candidate model. But I didn't find nice properties so far. Perhaps it is even better to not to stick to the determinant 1-condition, but allow determinant $-1$ here; the ""rotation""-matrix could then be a simple circulant one, like in the hyperbolic case. The log of that matrix looks like $$ \log(T_{3m}(\varphi))=\small \begin{bmatrix}   0 & 0 & \varphi \\   \varphi & 0 & 0 \\   0 & \varphi & 0  \end{bmatrix}$$ and I think it is a good hint, that this is equivalent to the rotation-matrices in the trigonometric/hyperbolic-cases, where the form of the matrix-log comes out to be much similar.","This question is more in recreational mathematics area By accident I came across the concept of ""hyperbolic rotation"" where we use a matrix containing $\cosh$ and $\sinh$ instead of the trigonometric $\cos$ and $\sin$ for the rotation by matrix-multiplication such that we have: $$ \tag{trigonometric} \qquad T_t(\varphi) = \begin{bmatrix} \cos \varphi &  \sin \varphi \\  -\sin \varphi &  \cos \varphi  \end{bmatrix} $$ $$ \tag{hyperbolic} \qquad T_h(\varphi) = \begin{bmatrix} \cosh \varphi &  \sinh \varphi \\  \sinh \varphi &  \cosh \varphi  \end{bmatrix} $$ A key-feature is surely, that both rotation-matrices have a determinant of $1$ ( because $$\small \cos^2(\varphi) + \sin^2(\varphi) = 1 \tag{trigonometric}  $$ and $$\small \cosh^2(\varphi) - \sinh^2(\varphi) = 1 \tag{hyperbolic} $$ ) . Now I toyed a bit to extend this to the case of 3-multisection series of the exponential; an example for what I mean is my older question ; let's call that three functions just $f(x),g(x),h(x)$ such that $f(x)+g(x)+h(x) = \exp(x)$ and the analogon to the square-formulae which equal $1$ is: $$ f(x)^3  + g(x)^3 + h(x)^3 - 3f(x)g(x)h(x) = 1 \tag{3-multisection} $$ At least, such a ""rotation""-matrix must have size of $3 \times 3$ but possibly even more - if it is constructable at all. Qu1: Is such a generalization to higher multisections (here order 3) possible? Qu2: and if, how could such a ""rotation""- matrix be contructed? [Update]: I've just found, that  $$ \tag{3-multisection} \qquad T_{3m}(\varphi) = \begin{bmatrix} f(\varphi) & h(\varphi) & g(\varphi)\\  g(\varphi)&f(\varphi)&h(\varphi)\\ h(\varphi) & g(\varphi) &f(\varphi)  \end{bmatrix}$$ has determinant $1$ and could be a candidate model. But I didn't find nice properties so far. Perhaps it is even better to not to stick to the determinant 1-condition, but allow determinant $-1$ here; the ""rotation""-matrix could then be a simple circulant one, like in the hyperbolic case. The log of that matrix looks like $$ \log(T_{3m}(\varphi))=\small \begin{bmatrix}   0 & 0 & \varphi \\   \varphi & 0 & 0 \\   0 & \varphi & 0  \end{bmatrix}$$ and I think it is a good hint, that this is equivalent to the rotation-matrices in the trigonometric/hyperbolic-cases, where the form of the matrix-log comes out to be much similar.",,"['matrices', 'trigonometry', 'special-functions', 'rotations']"
82,Finding the eigenvalues of a $3N \times 3N$ block matrix,Finding the eigenvalues of a  block matrix,3N \times 3N,"I have a block matrix of size $3N \times 3N$ of the form $$B = \begin{bmatrix} A & C & \ldots & C\\ C & A & \ldots & C\\ \vdots & \vdots & \ddots & \vdots\\ C & C & \ldots & A\\ \end{bmatrix}$$ where $A$ and $C$ are $3 \times 3$ matrices. Specifically, $C$ is given by $$C = \begin{bmatrix} 0 & 0 & 0 \\ 0 & 0 & 0 \\ 0 & 0 & \gamma\\ \end{bmatrix}$$ I would like to find the eigenvalues of the matrix $B$ . I have a paper that roughly states that the way to do this is as follows: First, note that $B = I_N \otimes A + M_N \otimes C$ where $\otimes$ is the Kronecker product, $I_N$ is the $N \times N$ identity, and $M_N$ is the matrix $$M_N = \begin{bmatrix} 0 & 1 & \ldots & 1 & 1\\ 1 & 0 & \ldots & 1 & 1\\ \vdots & \vdots & \ddots & \vdots & \vdots\\ 1 & 1 & \dots & 0 & 1\\ 1 & 1 & \dots & 1 & 0\\ \end{bmatrix}$$ I understand this part. However, the argument proceeds as follows: Let the eigenvalues of $M_N$ be $\mu_1, \dots, \mu_N$ . To find the eigenvalues $\lambda$ of $B$ we must use the characteristic equation $\det(B - \lambda) = 0$ . Once again, I understand this. However, I'm confused about the next bit of the argument. The paper states that we can diagonalize $M_N$ and that since this transformation does not affect the identity $I_N$ , the characteristic equation for the determinant can be transformed into $N$ equations given by $$\det(A + \mu_k C - \lambda) = 0, \qquad{} k = 1, \dots, N$$ I don't understand this final transformation, which involves diagonalizing $M_N$ . Could someone explain this for me?","I have a block matrix of size of the form where and are matrices. Specifically, is given by I would like to find the eigenvalues of the matrix . I have a paper that roughly states that the way to do this is as follows: First, note that where is the Kronecker product, is the identity, and is the matrix I understand this part. However, the argument proceeds as follows: Let the eigenvalues of be . To find the eigenvalues of we must use the characteristic equation . Once again, I understand this. However, I'm confused about the next bit of the argument. The paper states that we can diagonalize and that since this transformation does not affect the identity , the characteristic equation for the determinant can be transformed into equations given by I don't understand this final transformation, which involves diagonalizing . Could someone explain this for me?","3N \times 3N B = \begin{bmatrix}
A & C & \ldots & C\\
C & A & \ldots & C\\
\vdots & \vdots & \ddots & \vdots\\
C & C & \ldots & A\\
\end{bmatrix} A C 3 \times 3 C C = \begin{bmatrix}
0 & 0 & 0 \\
0 & 0 & 0 \\
0 & 0 & \gamma\\
\end{bmatrix} B B = I_N \otimes A + M_N \otimes C \otimes I_N N \times N M_N M_N = \begin{bmatrix}
0 & 1 & \ldots & 1 & 1\\
1 & 0 & \ldots & 1 & 1\\
\vdots & \vdots & \ddots & \vdots & \vdots\\
1 & 1 & \dots & 0 & 1\\
1 & 1 & \dots & 1 & 0\\
\end{bmatrix} M_N \mu_1, \dots, \mu_N \lambda B \det(B - \lambda) = 0 M_N I_N N \det(A + \mu_k C - \lambda) = 0, \qquad{} k = 1, \dots, N M_N","['linear-algebra', 'matrices', 'eigenvalues-eigenvectors', 'determinant', 'block-matrices']"
83,How to determine if system Ax = b has a solution for all choices of b?,How to determine if system Ax = b has a solution for all choices of b?,,"I'm supposed to determine if system Ax = b (where x and b have appropriate number of components) has a solution for all choices of b. For example, for these matrices: a)$$A=\left( \begin{array}{c} 3 & -4 \\ 4 & 2 \end{array}\right)$$ b)$$A=\left( \begin{array}{c} -3 & 2 & 1 \\ 1 & -1 & -1 \\ 5 & -4 & -3\end{array}\right)$$ In my textbook, I'm only given the answer, but not the steps on how the answer came about. a) For every choice of b there is a solution of Ax + b b) There is a choice of b where there is no solution to Ax = b I've used Gaussian elimination on the matrix, but I'm not sure what to do from there. Also, how do you determine if columns of a given matrix spans R^3? Given this matrix: $$\left( \begin{array}{c} 2 & 1 & -3 & 5 \\ 1 & 4 & 2 & 6 \\ 0 & 3 & 3 & 3\end{array}\right)$$ I've reduced it to  $$\left( \begin{array}{c} 1 & 4 & 2 & 6 \\ 0 & -7 & -7 & -7 \\ 0 & 0 & 0 & 0\end{array}\right)$$ All I know is that if 0 = 0, there are infinitely many solutions and if 0 equals a non-zero, then the system has no solutions. Since the bottom row of the matrix is all zero, wouldn't the columns span R^3? However, the answer in the textbook is that the columns do not span R^3. Not sure if it's a typo or I'm just not getting it. Probably the latter! Thanks for your help!","I'm supposed to determine if system Ax = b (where x and b have appropriate number of components) has a solution for all choices of b. For example, for these matrices: a)$$A=\left( \begin{array}{c} 3 & -4 \\ 4 & 2 \end{array}\right)$$ b)$$A=\left( \begin{array}{c} -3 & 2 & 1 \\ 1 & -1 & -1 \\ 5 & -4 & -3\end{array}\right)$$ In my textbook, I'm only given the answer, but not the steps on how the answer came about. a) For every choice of b there is a solution of Ax + b b) There is a choice of b where there is no solution to Ax = b I've used Gaussian elimination on the matrix, but I'm not sure what to do from there. Also, how do you determine if columns of a given matrix spans R^3? Given this matrix: $$\left( \begin{array}{c} 2 & 1 & -3 & 5 \\ 1 & 4 & 2 & 6 \\ 0 & 3 & 3 & 3\end{array}\right)$$ I've reduced it to  $$\left( \begin{array}{c} 1 & 4 & 2 & 6 \\ 0 & -7 & -7 & -7 \\ 0 & 0 & 0 & 0\end{array}\right)$$ All I know is that if 0 = 0, there are infinitely many solutions and if 0 equals a non-zero, then the system has no solutions. Since the bottom row of the matrix is all zero, wouldn't the columns span R^3? However, the answer in the textbook is that the columns do not span R^3. Not sure if it's a typo or I'm just not getting it. Probably the latter! Thanks for your help!",,"['linear-algebra', 'matrices']"
84,Unique of eigenbasis of self-adjoint operator.,Unique of eigenbasis of self-adjoint operator.,,"Today I am reading an article: Eigenvalues and sums of Hermitian matrices there is an exercise copy from that article: Exercise 1 Suppose that the eigenvalues $\lambda_1(A)>\cdots>\lambda_n(A)$ of an $n\times n$ Hermitian matrix are distinct. Show that the associated eigenbasis $u_1(A),\cdots,u_n(A)$ is unique up to rotating each individual eigenvector $u_j(A)$ by a complex phase $e^{i\theta_j}$. In particular, the spectral projections $P_j(A):=u_j(A)^*u_j(A)$ are unique. what happens when there is eigenvalue multiplicity? Assume $A$ is the matrix of self-adjoint operator $\mathcal{A}$ under some property basis of $V_{\Bbb{C}}$. I don't know why eigenbasis $u_1(A),\cdots,u_n(A)$ is unique up to rotating. If we multiply a complex number $c_k \not=0$ respectively, then $c_1u_1(A),\cdots,c_nu_n(A)$ is also a basis of $V_{\Bbb{C}}$ consist of $\mathcal{A}$'s eigenvectors,under this basis, $\mathcal{A}$ correspond to a diagonal matrix consist of eigenvalues: $\text{diag}\{\lambda_1,\cdots,\lambda_n\}$. I think it is unique up to multiply a complex number $c_k$ respectively. what's wrong?","Today I am reading an article: Eigenvalues and sums of Hermitian matrices there is an exercise copy from that article: Exercise 1 Suppose that the eigenvalues $\lambda_1(A)>\cdots>\lambda_n(A)$ of an $n\times n$ Hermitian matrix are distinct. Show that the associated eigenbasis $u_1(A),\cdots,u_n(A)$ is unique up to rotating each individual eigenvector $u_j(A)$ by a complex phase $e^{i\theta_j}$. In particular, the spectral projections $P_j(A):=u_j(A)^*u_j(A)$ are unique. what happens when there is eigenvalue multiplicity? Assume $A$ is the matrix of self-adjoint operator $\mathcal{A}$ under some property basis of $V_{\Bbb{C}}$. I don't know why eigenbasis $u_1(A),\cdots,u_n(A)$ is unique up to rotating. If we multiply a complex number $c_k \not=0$ respectively, then $c_1u_1(A),\cdots,c_nu_n(A)$ is also a basis of $V_{\Bbb{C}}$ consist of $\mathcal{A}$'s eigenvectors,under this basis, $\mathcal{A}$ correspond to a diagonal matrix consist of eigenvalues: $\text{diag}\{\lambda_1,\cdots,\lambda_n\}$. I think it is unique up to multiply a complex number $c_k$ respectively. what's wrong?",,"['linear-algebra', 'matrices']"
85,Determine the matrices that represent the following rotations of $\mathbb{R}^3$,Determine the matrices that represent the following rotations of,\mathbb{R}^3,"I need to determine the matrix that represents the following rotation of $\mathbb{R}^3$. (a) angle $\theta$, the axis $e_2$ (b) angle $2\pi/3$, axis contains the vector $(1,1,1)^t$ (c) angle $\pi/2$, axis contains the vector $(1,1,0)^t$ Now, I would like to check if I got the right answers because this problem has been quite difficult for me. Any help is greatly appreciated. Please forgive me for skipping the work because formatting matrices is a real pain. Especially when I have a lot of them. For part $(a)$, I got that $(e_2,e_3,e_1)$ is an orthonormal basis of $\mathbb{R}^3$. Then after simplification, the matrix is $$\left(         \begin{matrix}         \cos\theta & 0 & \sin\theta \\         0 & 1 & 0 \\         -\sin\theta & 0 & \cos\theta \\         \end{matrix}\right) $$ For part $(b)$, I got an orthonormal basis as $\{\left[1/\sqrt{3}, 1/\sqrt{3}, 1/\sqrt{3}\right]^t, \left[1/\sqrt{2},-1/\sqrt{2},0\right]^t,\left[1/\sqrt{6},1/\sqrt{6},-2/\sqrt{6}\right]^t\}$. Then after simplification, the matrix is $$\left(         \begin{matrix}         -\frac{\sqrt{3}}{2} & 0 & -\frac12 \\         0 & 1 & 0 \\         \frac12 & 0 & -\frac{\sqrt{3}}{2} \\         \end{matrix}\right) $$ Is what I have done so far correct such that I can proceed with part $(c)$?","I need to determine the matrix that represents the following rotation of $\mathbb{R}^3$. (a) angle $\theta$, the axis $e_2$ (b) angle $2\pi/3$, axis contains the vector $(1,1,1)^t$ (c) angle $\pi/2$, axis contains the vector $(1,1,0)^t$ Now, I would like to check if I got the right answers because this problem has been quite difficult for me. Any help is greatly appreciated. Please forgive me for skipping the work because formatting matrices is a real pain. Especially when I have a lot of them. For part $(a)$, I got that $(e_2,e_3,e_1)$ is an orthonormal basis of $\mathbb{R}^3$. Then after simplification, the matrix is $$\left(         \begin{matrix}         \cos\theta & 0 & \sin\theta \\         0 & 1 & 0 \\         -\sin\theta & 0 & \cos\theta \\         \end{matrix}\right) $$ For part $(b)$, I got an orthonormal basis as $\{\left[1/\sqrt{3}, 1/\sqrt{3}, 1/\sqrt{3}\right]^t, \left[1/\sqrt{2},-1/\sqrt{2},0\right]^t,\left[1/\sqrt{6},1/\sqrt{6},-2/\sqrt{6}\right]^t\}$. Then after simplification, the matrix is $$\left(         \begin{matrix}         -\frac{\sqrt{3}}{2} & 0 & -\frac12 \\         0 & 1 & 0 \\         \frac12 & 0 & -\frac{\sqrt{3}}{2} \\         \end{matrix}\right) $$ Is what I have done so far correct such that I can proceed with part $(c)$?",,"['linear-algebra', 'matrices']"
86,Maximum of two positive operators,Maximum of two positive operators,,"Let $A,B$ be two positive operators in $B(H)$. Does there exist, in general, an operator $C$ such that for each $T$, if $A \leq T$ and $B \leq T$, then $$A\leq C \leq T\quad  \text{and}\quad B\leq C\leq T?$$ That is, does there exist the maximum operator of any two positive operators?","Let $A,B$ be two positive operators in $B(H)$. Does there exist, in general, an operator $C$ such that for each $T$, if $A \leq T$ and $B \leq T$, then $$A\leq C \leq T\quad  \text{and}\quad B\leq C\leq T?$$ That is, does there exist the maximum operator of any two positive operators?",,"['matrices', 'functional-analysis', 'operator-theory']"
87,"Monotonicity of $\log \det R(d_i, d_j)$",Monotonicity of,"\log \det R(d_i, d_j)","$R(d_i, d_j)=\log\det(\mathbf{I}+(d_{i}^{-\alpha}\mathbf{H}_{i}\mathbf{v}_{i}\mathbf{v}_{i}^{*}\mathbf{H}_{i}^{*})(d_{j}^{-\alpha}\mathbf{H}_{j}\mathbf{v}_{j}\mathbf{v}_{j}^{*}\mathbf{H}_{j}^{*}+\sigma^{2}\mathbf{I})^{-1}).$ I want to claim that this is decreasing in $d_{i}$ and  increasing in $d_{j}$. I think it is monotonic in these variables. I tried with simulations and it works. I also can prove this for scalar case but I would like  to prove it for more general case where $\mathbf{H_i}$, $\mathbf{H_j}$ are complex valued constant matrices. Then $\mathbf{v_i}$, $\mathbf{v_j}$ are complex valued constant column vectors. $\alpha$ is a real positive constant and $d_i$, $d_j$ are non negative real valued scalar variables, $\mathbf{I}$ is the identity matrix. The $()^*$ is the Hermitian (conjugate transpose) Both $\mathbf{v_i}$,$\mathbf{v_j}$ exclude the all zero vector. Similarly both  $\mathbf{H_i}$, $\mathbf{H_j}$ exclude all zero matrix. And $\sigma^2>0$ a real constant. May be it may require other assumptions on these vectors and matrices, but I don't know what those may be and any suggestion is most appreciated. I already tried it for scalar case and it holds. $$\log\left\{1+\frac{\frac{a}{x}}{\frac{b}{y}+\sigma^{2}}\right\}$$ for $a,$ $b$ positive real scalars. Thank you.","$R(d_i, d_j)=\log\det(\mathbf{I}+(d_{i}^{-\alpha}\mathbf{H}_{i}\mathbf{v}_{i}\mathbf{v}_{i}^{*}\mathbf{H}_{i}^{*})(d_{j}^{-\alpha}\mathbf{H}_{j}\mathbf{v}_{j}\mathbf{v}_{j}^{*}\mathbf{H}_{j}^{*}+\sigma^{2}\mathbf{I})^{-1}).$ I want to claim that this is decreasing in $d_{i}$ and  increasing in $d_{j}$. I think it is monotonic in these variables. I tried with simulations and it works. I also can prove this for scalar case but I would like  to prove it for more general case where $\mathbf{H_i}$, $\mathbf{H_j}$ are complex valued constant matrices. Then $\mathbf{v_i}$, $\mathbf{v_j}$ are complex valued constant column vectors. $\alpha$ is a real positive constant and $d_i$, $d_j$ are non negative real valued scalar variables, $\mathbf{I}$ is the identity matrix. The $()^*$ is the Hermitian (conjugate transpose) Both $\mathbf{v_i}$,$\mathbf{v_j}$ exclude the all zero vector. Similarly both  $\mathbf{H_i}$, $\mathbf{H_j}$ exclude all zero matrix. And $\sigma^2>0$ a real constant. May be it may require other assumptions on these vectors and matrices, but I don't know what those may be and any suggestion is most appreciated. I already tried it for scalar case and it holds. $$\log\left\{1+\frac{\frac{a}{x}}{\frac{b}{y}+\sigma^{2}}\right\}$$ for $a,$ $b$ positive real scalars. Thank you.",,"['real-analysis', 'complex-analysis', 'matrices', 'analysis', 'functional-analysis']"
88,Why does $\operatorname{tr}(A^k)=\operatorname{tr}(B^k)$ imply $\operatorname{Spec}(A)=\operatorname{Spec}(B)$?,Why does  imply ?,\operatorname{tr}(A^k)=\operatorname{tr}(B^k) \operatorname{Spec}(A)=\operatorname{Spec}(B),"Suppose $A$ and $B$ are two square $n\times n$ matrices over some field. Why do the $n$ equations $$\operatorname{tr}(A^k)=\operatorname{tr}(B^k)\text{ for } 1\leq k\leq n$$ imply that $A$ and $B$ have the same spectra? If $\operatorname{Spec}(A)=\{\lambda_1,\dots,\lambda_p\}$ and $\operatorname{Spec}(B)=\{\mu_1,\dots,\mu_r\}$, then I know $\operatorname{Spec}(A^k)=\{\lambda_1^k,\dots,\lambda_p^k\}$ and $\operatorname{Spec}(B^k)=\{\mu_1^k,\dots,\mu_r^k\}$, (some of these new eigenvalues may not be distinct). Since the trace is the sum of the eigenvalues, this would give a new set of equations $$ \sum \lambda_i^k=\sum \mu_i^k $$ but since these aren't linear I don't think it'll be possible to solve these and show the set of eigenvalues for $A$ and $B$ are the same.","Suppose $A$ and $B$ are two square $n\times n$ matrices over some field. Why do the $n$ equations $$\operatorname{tr}(A^k)=\operatorname{tr}(B^k)\text{ for } 1\leq k\leq n$$ imply that $A$ and $B$ have the same spectra? If $\operatorname{Spec}(A)=\{\lambda_1,\dots,\lambda_p\}$ and $\operatorname{Spec}(B)=\{\mu_1,\dots,\mu_r\}$, then I know $\operatorname{Spec}(A^k)=\{\lambda_1^k,\dots,\lambda_p^k\}$ and $\operatorname{Spec}(B^k)=\{\mu_1^k,\dots,\mu_r^k\}$, (some of these new eigenvalues may not be distinct). Since the trace is the sum of the eigenvalues, this would give a new set of equations $$ \sum \lambda_i^k=\sum \mu_i^k $$ but since these aren't linear I don't think it'll be possible to solve these and show the set of eigenvalues for $A$ and $B$ are the same.",,"['matrices', 'eigenvalues-eigenvectors']"
89,"action of $O(n,\mathbb{R})$ on ${S}^{n-1}$",action of  on,"O(n,\mathbb{R}) {S}^{n-1}","Is the action of $O(n,\mathbb{R})$ on ${S}^{n-1}$ transitive? I think this is true as orthogonal matrices are supposed to rotate and keep the length fixed, but how do I prove this? EDIT: Based on Alexander's comment, if $x = (x_{1}, \ldots, x_{n}) \in S^{n-1}$, then I can construct a matrix $A$ whose first row is the vector $x$ and whose other rows can be obtained by completing it to an orthonormal basis. But does it ensure that columns are also orthogonal?","Is the action of $O(n,\mathbb{R})$ on ${S}^{n-1}$ transitive? I think this is true as orthogonal matrices are supposed to rotate and keep the length fixed, but how do I prove this? EDIT: Based on Alexander's comment, if $x = (x_{1}, \ldots, x_{n}) \in S^{n-1}$, then I can construct a matrix $A$ whose first row is the vector $x$ and whose other rows can be obtained by completing it to an orthonormal basis. But does it ensure that columns are also orthogonal?",,"['linear-algebra', 'group-theory', 'matrices', 'lie-groups', 'group-actions']"
90,A question about the index of the matrix $A$,A question about the index of the matrix,A,"Let $A\in \mathbb{C}^{m\times n}$ . We say the nonnegative integer number $k$ to be the index of matrix $A$, if $k$ is the smallest nonnegative integer number such that $rank(A^{k+1}) = rank(A^{k})$......$(1)$ Based on the definition $(1)$ could we conclude that $R(A^{k+1}) =R(A^{k})$, where $R(A)$, denote the range space of the matrix $A$.? Thanks for the help.","Let $A\in \mathbb{C}^{m\times n}$ . We say the nonnegative integer number $k$ to be the index of matrix $A$, if $k$ is the smallest nonnegative integer number such that $rank(A^{k+1}) = rank(A^{k})$......$(1)$ Based on the definition $(1)$ could we conclude that $R(A^{k+1}) =R(A^{k})$, where $R(A)$, denote the range space of the matrix $A$.? Thanks for the help.",,"['linear-algebra', 'matrices']"
91,Factorization of a linear combination of matrices,Factorization of a linear combination of matrices,,I'm trying to understand the determinant from Axler Sheldon's paper and there is one point in the very beginning that I don't understand :S (Link below to the paper) http://www.cs.berkeley.edu/~wkahan/MathH110/DownDets.pdf I have attached a picture of the point I don't understand: I have higlighted the area I don't understand with a red rectangle. Could someone clarify why can we do the factorization and the last remark (about the injectivity) made by the author? Thank you for any help :),I'm trying to understand the determinant from Axler Sheldon's paper and there is one point in the very beginning that I don't understand :S (Link below to the paper) http://www.cs.berkeley.edu/~wkahan/MathH110/DownDets.pdf I have attached a picture of the point I don't understand: I have higlighted the area I don't understand with a red rectangle. Could someone clarify why can we do the factorization and the last remark (about the injectivity) made by the author? Thank you for any help :),,"['linear-algebra', 'matrices', 'determinant', 'factoring']"
92,How to generate algebraic span of a set of matrices (how many multiplications?),How to generate algebraic span of a set of matrices (how many multiplications?),,"I've got a question about matrices and matrix algebras that offhand seems difficult, I'm wondering there is any sharp solution? Or perhaps it's known to not have any solution at all? Suppose you have some set of n complex DxD matrices {M1,...,Mn}, and consider the algebra which they generate (under finite multiplications and additions with coefficients over the complex numbers), in particular, consider the case where that algebra is NOT the full matrix algebra M_D(C). EDIT: First question, I suppose, is to ask, given the generators (and supposing that they're linearly independent), what's the best way to figure out the sub-algebra (or even just its dimension) of the full matrix algebra that you can generate? Second, It seems clear that you should be able to generate any ""basis"" element (in the vector space sense, a matrix with a one in exactly one entry and zero everywhere else) which exists in your algebra after some finite number of multiplications. My question is, is there a good way to figure out exactly what the ""maximum"" number of multiplications you'd need to generate any of the basis elements is? Certainly it should be bounded, but has anyone figured out bounds on this, perhaps different bounds given different conditions on your set of generators? If anyone recognizes this as something other people have thought about/figured out and could point me in the right direction I would be very grateful!","I've got a question about matrices and matrix algebras that offhand seems difficult, I'm wondering there is any sharp solution? Or perhaps it's known to not have any solution at all? Suppose you have some set of n complex DxD matrices {M1,...,Mn}, and consider the algebra which they generate (under finite multiplications and additions with coefficients over the complex numbers), in particular, consider the case where that algebra is NOT the full matrix algebra M_D(C). EDIT: First question, I suppose, is to ask, given the generators (and supposing that they're linearly independent), what's the best way to figure out the sub-algebra (or even just its dimension) of the full matrix algebra that you can generate? Second, It seems clear that you should be able to generate any ""basis"" element (in the vector space sense, a matrix with a one in exactly one entry and zero everywhere else) which exists in your algebra after some finite number of multiplications. My question is, is there a good way to figure out exactly what the ""maximum"" number of multiplications you'd need to generate any of the basis elements is? Certainly it should be bounded, but has anyone figured out bounds on this, perhaps different bounds given different conditions on your set of generators? If anyone recognizes this as something other people have thought about/figured out and could point me in the right direction I would be very grateful!",,"['matrices', 'operator-algebras']"
93,Gaussian Elimination with Scaled Row Pivoting for numerical methods,Gaussian Elimination with Scaled Row Pivoting for numerical methods,,"I am solving a system first with basic Gaussian Elimination, and then Gaussian Elimination with scaled row pivoting (used in numerical methods) Basic Gaussian Elimination on the system $Ax=b$ : $$                                                                                                                                                            \begin{pmatrix}-1& 1& -4 \\                                                                                                                                                                  2& 2& 0 \\                                                                                                                                                                   3& 3& 2 \end{pmatrix}                                                                                                                                        \begin{pmatrix}x_1\\x_2\\x_3\end{pmatrix} =                                                                                                                                  \begin{pmatrix}0\\1\\\frac{1}{2}\end{pmatrix}                                                                                                                                $$ Let $A_i$ denote the $i^\text{th}$ row of matrix $A$ and let $A^{(1)} A^{(2)}\ldots$ denote the matrix after the first, second and so forth elementary row operations. Note that $A^{0} =A$ . Compute the following elementary row operations: \begin{align}                                                                                                                                                                A^{(1)}_2 &= A^{(0)}_2 - (-2)A^{(0)}_1 \\                                                                                                                                    A^{(1)}_3 &= A^{(0)}_3 - (-3)A^{(0)}_1                                                                                                                                        \end{align} This yields: $$                                                                                                                                                                                                                                                                                             \begin{pmatrix}-1& 1& -4 \\                                                                                                                                                                                                                                                                                                   0& 4& -8 \\                                                                                                                                                                                                                                                                                                   0& 6& -10 \end{pmatrix}                                                                                                                                                                                                                                                                       \begin{pmatrix}x_1\\x_2\\x_3\end{pmatrix} =                                                                                                                                                                                                                                                                   \begin{pmatrix}0\\1\\-1\end{pmatrix}                                                                                                                                                                                                                                                                          $$ Compute: $$ A^{(2)}_3 = A^{(1)}_3 - \tfrac{3}{2}A^{(1)}_2 $$ This yields: $$                                                                                                                                                                                                                                                                                              \begin{pmatrix}-1& 1& -4\\                                                                                                                                                                                                                                                                                                    0& 4& -8\\                                                                                                                                                                                                                                                                                                    0& 0& 2\end{pmatrix}                                                                                                                                                                                                                                                                          \begin{pmatrix}x_1\\x_2\\x_3\end{pmatrix}=                                                                                                                                                                                                                                                                    \begin{pmatrix}0\\1\\-1\end{pmatrix}                                                                                                                                                                                                                                                                          $$ Thus we have: $$ x=\begin{pmatrix}\frac{5}{4}\\                                                                                                                                                                                                                                                                                             \frac{-3}{4}\\                                                                                                                                                                                                                                                                                               \frac{-1}{2}\end{pmatrix}$$ Now I will solve the same system with Scaled Row Pivoting. The $i^\text{th}$ element of the list $S$ will denote the maximum element in row $i$ in matrix $A$ . $P$ will denote the order of the rows. Initially we have: $$                                                                                                                                                                                                                                                                                             S = (4, 2, 3) \\                                                                                                                                                                                                                                                                                              P = (2, 1, 3)                                                                                                                                                                                                                                                                                                 $$ Swap rows $1$ and $2$ since row $2$ has the maximum pivot relative to its row: $$                                                                                                                                                                                                                                                                                              \begin{pmatrix}2&2&0\\                                                                                                                                                                                                                                                                                                        -1&1&-4\\                                                                                                                                                                                                                                                                                                     3& 3& 2\end{pmatrix}                                                                                                                                                                                                                                                                          \begin{pmatrix}x_1\\x_2\\x_3\end{pmatrix} =                                                                                                                                                                                                                                                                   \begin{pmatrix}1\\0\\\frac{1}{2}\end{pmatrix}                                                                                                                                                                                                                                                                 $$ Now compute the following elementary row operations w.r.t. the ordering given by $p$ : \begin{align}                                                                                                                                                                                                                                                                                                   A^{(1)}_1 &= A^{(0)}_1 - \tfrac{-1}{2}A^{(0)}_2 \\                                                                                                                                                                                                                                                           A^{(1)}_3 &= A^{(0)}_3 - \tfrac{3}{2}A^{(0)}_2                                                                                                                                                                                                                                                             \end{align} This yields: $$                                                                                                                                                                                                                                                                                            \begin{pmatrix}2&2&0\\                                                                                                                                                                                                                                                                                                       0&2&-4\\                                                                                                                                                                                                                                                                                                      0&0&2\end{pmatrix}                                                                                                                                                                                                                                                                             \begin{pmatrix}x_1\\x_2\\x_3\end{pmatrix}=                                                                                                                                                                                                                                                                    \begin{pmatrix}1\\\frac{1}{2}\\-1                                                                                                                                                                                                                                                                             \end{pmatrix}                                                                                                                                                                                                                                                                                                 $$ Now using back substitution to solve for $x$ we get: $$                                                                                                                                                                                                                                                                                             x=\begin{pmatrix}\frac{-1}{4}\\\frac{3}{4}\\\frac{-1}{2}\end{pmatrix}                                                                                                                                                                                                                                         $$ Clearly, I must have made a mistake along the way since the solutions for both methods are not the same! I know that the scaled pivoting is incorrect as I checked my solution in a C.A.S. and it matched the solution for the Basic Method. Please show me what I have done wrong in the scaled pivoting algorithm.","I am solving a system first with basic Gaussian Elimination, and then Gaussian Elimination with scaled row pivoting (used in numerical methods) Basic Gaussian Elimination on the system : Let denote the row of matrix and let denote the matrix after the first, second and so forth elementary row operations. Note that . Compute the following elementary row operations: This yields: Compute: This yields: Thus we have: Now I will solve the same system with Scaled Row Pivoting. The element of the list will denote the maximum element in row in matrix . will denote the order of the rows. Initially we have: Swap rows and since row has the maximum pivot relative to its row: Now compute the following elementary row operations w.r.t. the ordering given by : This yields: Now using back substitution to solve for we get: Clearly, I must have made a mistake along the way since the solutions for both methods are not the same! I know that the scaled pivoting is incorrect as I checked my solution in a C.A.S. and it matched the solution for the Basic Method. Please show me what I have done wrong in the scaled pivoting algorithm.","Ax=b                                                                                                                                                            
\begin{pmatrix}-1& 1& -4 \\                                                                                                                                                 
                2& 2& 0 \\                                                                                                                                                  
                3& 3& 2 \end{pmatrix}                                                                                                                                       
\begin{pmatrix}x_1\\x_2\\x_3\end{pmatrix} =                                                                                                                                 
\begin{pmatrix}0\\1\\\frac{1}{2}\end{pmatrix}                                                                                                                               
 A_i i^\text{th} A A^{(1)} A^{(2)}\ldots A^{0} =A \begin{align}                                                                                                                                                               
A^{(1)}_2 &= A^{(0)}_2 - (-2)A^{(0)}_1 \\                                                                                                                                   
A^{(1)}_3 &= A^{(0)}_3 - (-3)A^{(0)}_1                                                                                                                                       
\end{align}                                                                                                                                                                                                                                                                                             
\begin{pmatrix}-1& 1& -4 \\                                                                                                                                                                                                                                                                                  
                0& 4& -8 \\                                                                                                                                                                                                                                                                                  
                0& 6& -10 \end{pmatrix}                                                                                                                                                                                                                                                                      
\begin{pmatrix}x_1\\x_2\\x_3\end{pmatrix} =                                                                                                                                                                                                                                                                  
\begin{pmatrix}0\\1\\-1\end{pmatrix}                                                                                                                                                                                                                                                                         
  A^{(2)}_3 = A^{(1)}_3 - \tfrac{3}{2}A^{(1)}_2                                                                                                                                                                                                                                                                                               
\begin{pmatrix}-1& 1& -4\\                                                                                                                                                                                                                                                                                   
                0& 4& -8\\                                                                                                                                                                                                                                                                                   
                0& 0& 2\end{pmatrix}                                                                                                                                                                                                                                                                         
\begin{pmatrix}x_1\\x_2\\x_3\end{pmatrix}=                                                                                                                                                                                                                                                                   
\begin{pmatrix}0\\1\\-1\end{pmatrix}                                                                                                                                                                                                                                                                         
  x=\begin{pmatrix}\frac{5}{4}\\                                                                                                                                                                                                                                                                                             \frac{-3}{4}\\                                                                                                                                                                                                                                                                                               \frac{-1}{2}\end{pmatrix} i^\text{th} S i A P                                                                                                                                                                                                                                                                                             
S = (4, 2, 3) \\                                                                                                                                                                                                                                                                                             
P = (2, 1, 3)                                                                                                                                                                                                                                                                                                
 1 2 2                                                                                                                                                                                                                                                                                              
\begin{pmatrix}2&2&0\\                                                                                                                                                                                                                                                                                       
                -1&1&-4\\                                                                                                                                                                                                                                                                                    
                3& 3& 2\end{pmatrix}                                                                                                                                                                                                                                                                         
\begin{pmatrix}x_1\\x_2\\x_3\end{pmatrix} =                                                                                                                                                                                                                                                                  
\begin{pmatrix}1\\0\\\frac{1}{2}\end{pmatrix}                                                                                                                                                                                                                                                                
 p \begin{align}                                                                                                                                                                                                                                                                                                
  A^{(1)}_1 &= A^{(0)}_1 - \tfrac{-1}{2}A^{(0)}_2 \\                                                                                                                                                                                                                                                        
  A^{(1)}_3 &= A^{(0)}_3 - \tfrac{3}{2}A^{(0)}_2                                                                                                                                                                                                                                                            
\end{align}                                                                                                                                                                                                                                                                                            
\begin{pmatrix}2&2&0\\                                                                                                                                                                                                                                                                                       
               0&2&-4\\                                                                                                                                                                                                                                                                                      
               0&0&2\end{pmatrix}                                                                                                                                                                                                                                                                            
\begin{pmatrix}x_1\\x_2\\x_3\end{pmatrix}=                                                                                                                                                                                                                                                                   
\begin{pmatrix}1\\\frac{1}{2}\\-1                                                                                                                                                                                                                                                                            
\end{pmatrix}                                                                                                                                                                                                                                                                                                
 x                                                                                                                                                                                                                                                                                             
x=\begin{pmatrix}\frac{-1}{4}\\\frac{3}{4}\\\frac{-1}{2}\end{pmatrix}                                                                                                                                                                                                                                        
","['matrices', 'numerical-linear-algebra']"
94,Multiples of determinant are elements in a matrix,Multiples of determinant are elements in a matrix,,"Suppose we have an $n \times n$ square matrix, $A$. Let the determinant be $|A|.$ We also constrain the elements of $A$ such that each element of  $A$ is an integer multiple of $|A|.$ Is there an equation to generate such a matrix?","Suppose we have an $n \times n$ square matrix, $A$. Let the determinant be $|A|.$ We also constrain the elements of $A$ such that each element of  $A$ is an integer multiple of $|A|.$ Is there an equation to generate such a matrix?",,"['linear-algebra', 'matrices']"
95,Maximum matrix simplification,Maximum matrix simplification,,"What is the most that a matrix can be simplified if row or column operations are both allowed? Intuitively, I am guessing that everything is 0 except the diagonal entries, which are a mix of 0's and 1's. However, I'm unable to conjecture how many of the diagonal entries are 0's or 1's. I am lacking a solid proof of my understanding... I haven't learned about rank, or null, or dimension, or kernels yet. I've just learned about invertibility, multiplication by elementary matrices, REF, RREF, row reduction, and block matrices. Any help?","What is the most that a matrix can be simplified if row or column operations are both allowed? Intuitively, I am guessing that everything is 0 except the diagonal entries, which are a mix of 0's and 1's. However, I'm unable to conjecture how many of the diagonal entries are 0's or 1's. I am lacking a solid proof of my understanding... I haven't learned about rank, or null, or dimension, or kernels yet. I've just learned about invertibility, multiplication by elementary matrices, REF, RREF, row reduction, and block matrices. Any help?",,[]
96,Determining whether a set of vectors forms a basis using Gaussian elimination,Determining whether a set of vectors forms a basis using Gaussian elimination,,"Given the following three vectors: $$ b_1 = \begin{pmatrix} 1 \\ 3 \\5 \end{pmatrix},\space b_2 = \begin{pmatrix} 2 \\ 1 \\ 7 \end{pmatrix},\space b_3 = \begin{pmatrix} 4 \\ 2 \\3 \end{pmatrix}, \space\space b_1, b_2, b_3 \in \mathbb{R}^3 $$ Does $B = \{ b_1, b_2, b_3 \}$ form a basis of $\mathbb{R}^3$? I know I can write the vectors as rows of a matrix and then use Gaussian elimination to check for linear independence: $$ \left[ \begin{array}{ccc|c} 1 &  3 & 5 & 0 \\ 2 & 1 & 7 & 0\\ 4 &  2 & 3 & 0 \\ \end{array} \right] $$ The elimination algorithm itself is clear to me, as is the correct solution. However, I just can't wrap my head around why the vectors are written line-wise into the matrix (and thus form rows). Intuitively, I would've written the vectors as columns of the matrix. Can anybody give me a brief explanation why they go in line-wise instead of column-wise?","Given the following three vectors: $$ b_1 = \begin{pmatrix} 1 \\ 3 \\5 \end{pmatrix},\space b_2 = \begin{pmatrix} 2 \\ 1 \\ 7 \end{pmatrix},\space b_3 = \begin{pmatrix} 4 \\ 2 \\3 \end{pmatrix}, \space\space b_1, b_2, b_3 \in \mathbb{R}^3 $$ Does $B = \{ b_1, b_2, b_3 \}$ form a basis of $\mathbb{R}^3$? I know I can write the vectors as rows of a matrix and then use Gaussian elimination to check for linear independence: $$ \left[ \begin{array}{ccc|c} 1 &  3 & 5 & 0 \\ 2 & 1 & 7 & 0\\ 4 &  2 & 3 & 0 \\ \end{array} \right] $$ The elimination algorithm itself is clear to me, as is the correct solution. However, I just can't wrap my head around why the vectors are written line-wise into the matrix (and thus form rows). Intuitively, I would've written the vectors as columns of the matrix. Can anybody give me a brief explanation why they go in line-wise instead of column-wise?",,"['linear-algebra', 'matrices']"
97,Matrix determinant $\neq 0$,Matrix determinant,\neq 0,"I have problem with this task: Given a square matrix $A = [a_{ij}]^n_{i,j=1} \in M_{n \times n}(\mathbb{R})$ and $t \geq 0$ satisfies conditions: $\forall i \neq j : a_{ij} = t,$ $\forall i : a_{ii} > t$. How to prove that $\det A \neq 0$ ? I have tried with brute force, but it isn't a good way to solve it.","I have problem with this task: Given a square matrix $A = [a_{ij}]^n_{i,j=1} \in M_{n \times n}(\mathbb{R})$ and $t \geq 0$ satisfies conditions: $\forall i \neq j : a_{ij} = t,$ $\forall i : a_{ii} > t$. How to prove that $\det A \neq 0$ ? I have tried with brute force, but it isn't a good way to solve it.",,"['linear-algebra', 'matrices']"
98,A variation of Cauchy's determinant,A variation of Cauchy's determinant,,"Prove the following identity: $$\det_{_{1\leq i,j \leq n}}\left(\frac{1}{(x_i+y_j)^2}\right)=\det_{_{1\leq i,j \leq n}}\left(\frac{1}{x_i+y_j}\right)\text{perm}_{_{1\leq i,j \leq n}}\left(\left(\frac{1}{x_i+y_j}\right)\right)$$ where $\text{perm}(A)$ is the permanent of a matrix $A$ defined as: $$\text{perm}(A)=\sum_{\sigma\in S_n}\prod_{i=1}^{n}a_{i,\sigma(i)}$$ the sum here extends all elements of the symmetric group $S_n$, i.e. over all permutations of the numbers $1,2,\cdots, n$. here is some reference: Permanent Symmetric_group thanks very much","Prove the following identity: $$\det_{_{1\leq i,j \leq n}}\left(\frac{1}{(x_i+y_j)^2}\right)=\det_{_{1\leq i,j \leq n}}\left(\frac{1}{x_i+y_j}\right)\text{perm}_{_{1\leq i,j \leq n}}\left(\left(\frac{1}{x_i+y_j}\right)\right)$$ where $\text{perm}(A)$ is the permanent of a matrix $A$ defined as: $$\text{perm}(A)=\sum_{\sigma\in S_n}\prod_{i=1}^{n}a_{i,\sigma(i)}$$ the sum here extends all elements of the symmetric group $S_n$, i.e. over all permutations of the numbers $1,2,\cdots, n$. here is some reference: Permanent Symmetric_group thanks very much",,"['linear-algebra', 'combinatorics', 'matrices', 'determinant']"
99,Singularity in matrix when inverting in Matlab,Singularity in matrix when inverting in Matlab,,"As data I get a matrix A but in my algorithm I need to work on its inverse. What I do is: C = inv(A) + B; Then in another line I update A. In the next cycles I also need (updated) A inverse, again for this algorithm. And so on. In the later cycles I get this: Matrix is close to singular or badly scaled. Results may be inaccurate. RCOND = 1.425117e-019 or this: Warning: Matrix is singular to working precision. or this: Warning: Matrix is singular, close to singular or badly scaled. Results may be inaccurate. RCOND = NaN. Can you help me how to avoid such singularity? Matrix is squared always.","As data I get a matrix A but in my algorithm I need to work on its inverse. What I do is: C = inv(A) + B; Then in another line I update A. In the next cycles I also need (updated) A inverse, again for this algorithm. And so on. In the later cycles I get this: Matrix is close to singular or badly scaled. Results may be inaccurate. RCOND = 1.425117e-019 or this: Warning: Matrix is singular to working precision. or this: Warning: Matrix is singular, close to singular or badly scaled. Results may be inaccurate. RCOND = NaN. Can you help me how to avoid such singularity? Matrix is squared always.",,"['matrices', 'matlab', 'inverse']"
