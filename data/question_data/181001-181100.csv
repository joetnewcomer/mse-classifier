,title_raw,title_text,title_latex,body_raw,body_text,body_latex,tags
0,Two Intuition Questions: Conservative VF and Dot Product,Two Intuition Questions: Conservative VF and Dot Product,,"All, In doing multivariable calculus, I have encountered line integrals. I have no problem calculating these, but I do have a question about the intuition behind two ideas (one of which, dot product, is early on in MV calculus). First, the role of gradients in a conservative vector field. I do understand how to determine if a vector field is conservative: it is the gradient of a potential function. And I can do the math from there. But what I don't understand is the intuition behind this. What does the gradient have to do with the fact that I don't have to calculate a curve in the conservative vector field? I'm having a hard time understanding the WHY even though I get the HOW. Second, on dot products. These are easy to do, and I understand that you need to dot product the force and the dr before taking the integral. But, again, why? The dot product, I guess, shows the relationship between two vectors - if it is zero, the vectors are orthogonal; and the dot product increases the closer the vectors get to parallel. But what is the intuition behind why the dot product can be used to determine work, i.e., w = F * d. Again, I can solve this, but I don't understand the intuition. Thanks.","All, In doing multivariable calculus, I have encountered line integrals. I have no problem calculating these, but I do have a question about the intuition behind two ideas (one of which, dot product, is early on in MV calculus). First, the role of gradients in a conservative vector field. I do understand how to determine if a vector field is conservative: it is the gradient of a potential function. And I can do the math from there. But what I don't understand is the intuition behind this. What does the gradient have to do with the fact that I don't have to calculate a curve in the conservative vector field? I'm having a hard time understanding the WHY even though I get the HOW. Second, on dot products. These are easy to do, and I understand that you need to dot product the force and the dr before taking the integral. But, again, why? The dot product, I guess, shows the relationship between two vectors - if it is zero, the vectors are orthogonal; and the dot product increases the closer the vectors get to parallel. But what is the intuition behind why the dot product can be used to determine work, i.e., w = F * d. Again, I can solve this, but I don't understand the intuition. Thanks.",,"['integration', 'multivariable-calculus', 'intuition']"
1,Finding integrating factor for non exact differential equation,Finding integrating factor for non exact differential equation,,"I am trying to solve non exact differential equation but I am unable to find the integration factor. I solve the following question $$-3y\frac{dy}{dx}+2x=0 $$ and I get the integrationg factor $x^{-5/2}$ and I use the proper way to find the integrating factor, but in our text book the integrating facot for above differential equation is finded and that is $y/x^4$ . Now I am completely confused that why please help me out with this.","I am trying to solve non exact differential equation but I am unable to find the integration factor. I solve the following question and I get the integrationg factor and I use the proper way to find the integrating factor, but in our text book the integrating facot for above differential equation is finded and that is . Now I am completely confused that why please help me out with this.",-3y\frac{dy}{dx}+2x=0  x^{-5/2} y/x^4,"['calculus', 'integration', 'algebra-precalculus', 'ordinary-differential-equations', 'multivariable-calculus']"
2,"How can the center of mass of a region with density $d[x, y]$ be understood as as a kind of (average x-value, average y-value)?","How can the center of mass of a region with density  be understood as as a kind of (average x-value, average y-value)?","d[x, y]","For a region of uniform density, the center of mass is equal to centroid, which is equal to: $$(\text{average x-value}, \text{average y-value}) = \left(\frac{\int\int_R{x}\,dxdy}{ \int\int_R{1}\,dxdy}, \frac{\int\int_R{y}\,dxdy}{ \int\int_R{1}\,dxdy}\right)$$ For density $d[x, y]$ , the center of mass is instead: $$\left(\frac{\int\int_R{x d[x, y]}\,dxdy}{ \int\int_R{d[x, y]}\,dxdy}, \frac{\int\int_R{y d[x, y]}\,dxdy}{ \int\int_R{d[x, y]}\,dxdy}\right)$$ Is there a way to understand the center of mass with variable density using this same idea of an average value? The uniform density center of mass is analogous to finding the mean value in 1D ( $\frac{\int_a^b f[x]\,dx}{b - a} = \frac{F[b] - F[a]}{b -a}$ ); is there a way a similar way to understand center of mass for variable density? When I look online (such as this and this ), they use moment and mass, which I don't really grasp. So, preferably, I would like an explanation that doesn't use these concepts; or if they are necessary, an explanation that explains what they are and why they are necessary. What is the connection between $\left(\frac{M_y}{m}, \frac{M_x}{m}\right)$ , and $\left(\frac{\int\int_R{x}\,dxdy}{ \int\int_R{1}\,dxdy}, \frac{\int\int_R{y}\,dxdy}{ \int\int_R{1}\,dxdy}\right)$ and $\left(\frac{\int\int_R{x d[x, y]}\,dxdy}{ \int\int_R{d[x, y]}\,dxdy}, \frac{\int\int_R{y d[x, y]}\,dxdy}{ \int\int_R{d[x, y]}\,dxdy}\right)$ ? (One thought I had was: the reason the uniform density center of mass works is because all surrounding areas around the point balance out. (If you draw a straight line through the point, two mass of the two sides are the same. Or maybe another way of thinking of it is, if you take the integral of both sides created by the line, those two masses are equal). Perhaps the center of mass with variable density can be derived similarly?) Sorry if this is very rambling.","For a region of uniform density, the center of mass is equal to centroid, which is equal to: For density , the center of mass is instead: Is there a way to understand the center of mass with variable density using this same idea of an average value? The uniform density center of mass is analogous to finding the mean value in 1D ( ); is there a way a similar way to understand center of mass for variable density? When I look online (such as this and this ), they use moment and mass, which I don't really grasp. So, preferably, I would like an explanation that doesn't use these concepts; or if they are necessary, an explanation that explains what they are and why they are necessary. What is the connection between , and and ? (One thought I had was: the reason the uniform density center of mass works is because all surrounding areas around the point balance out. (If you draw a straight line through the point, two mass of the two sides are the same. Or maybe another way of thinking of it is, if you take the integral of both sides created by the line, those two masses are equal). Perhaps the center of mass with variable density can be derived similarly?) Sorry if this is very rambling.","(\text{average x-value}, \text{average y-value}) = \left(\frac{\int\int_R{x}\,dxdy}{ \int\int_R{1}\,dxdy}, \frac{\int\int_R{y}\,dxdy}{ \int\int_R{1}\,dxdy}\right) d[x, y] \left(\frac{\int\int_R{x d[x, y]}\,dxdy}{ \int\int_R{d[x, y]}\,dxdy}, \frac{\int\int_R{y d[x, y]}\,dxdy}{ \int\int_R{d[x, y]}\,dxdy}\right) \frac{\int_a^b f[x]\,dx}{b - a} = \frac{F[b] - F[a]}{b -a} \left(\frac{M_y}{m}, \frac{M_x}{m}\right) \left(\frac{\int\int_R{x}\,dxdy}{ \int\int_R{1}\,dxdy}, \frac{\int\int_R{y}\,dxdy}{ \int\int_R{1}\,dxdy}\right) \left(\frac{\int\int_R{x d[x, y]}\,dxdy}{ \int\int_R{d[x, y]}\,dxdy}, \frac{\int\int_R{y d[x, y]}\,dxdy}{ \int\int_R{d[x, y]}\,dxdy}\right)","['integration', 'multivariable-calculus', 'physics']"
3,Lagrange Multiplier In Proving AM-GM inequality [duplicate],Lagrange Multiplier In Proving AM-GM inequality [duplicate],,"This question already has an answer here : Proving the AM-GM Inequality with Lagrange Multipliers (1 answer) Closed 3 years ago . I am dealing with a question in Stewart Calculus Book which states the following: Using Lagrange multiplier, find the maximum of the function $$f(x_1,x_2,...,x_n)=(x_1x_2...x_n)^{1/n}$$ subject to the constraint $$\sum_{i=1}^{n} x_i=c$$ ,where $x_i$ 's are all positive, and use this to prove the AM-GM inequality. Actually in AM-GM inequality, it holds for all nonnegative numbers, but I think the author wants to get rid of the case where $x_i$ 's are all zero, where Lagrange multiplier can no longer be applied. And the result I obtained is: $f$ will have its maximum/minimum (the arithmetic mean), or even saddle point at $(\frac{c}{n},\frac{c}{n},...,\frac{c}{n})$ , since Lagrange method gives no definiteness on the behaviour of the critical points found (whether max, min, or saddle). And my question is: In this case, how do we show that the point I found gives the maximum value of the function? My teacher gave me an argument that said ""the minimum value would be some place very near to $0$ , so the arithmetic mean will not be the minimum value"". But I think the argument is not persuasive enough to explain why the arithmetic mean is the maximum value. So, I hope that there will someone who are willing to give their opinions on this. Thanks.","This question already has an answer here : Proving the AM-GM Inequality with Lagrange Multipliers (1 answer) Closed 3 years ago . I am dealing with a question in Stewart Calculus Book which states the following: Using Lagrange multiplier, find the maximum of the function subject to the constraint ,where 's are all positive, and use this to prove the AM-GM inequality. Actually in AM-GM inequality, it holds for all nonnegative numbers, but I think the author wants to get rid of the case where 's are all zero, where Lagrange multiplier can no longer be applied. And the result I obtained is: will have its maximum/minimum (the arithmetic mean), or even saddle point at , since Lagrange method gives no definiteness on the behaviour of the critical points found (whether max, min, or saddle). And my question is: In this case, how do we show that the point I found gives the maximum value of the function? My teacher gave me an argument that said ""the minimum value would be some place very near to , so the arithmetic mean will not be the minimum value"". But I think the argument is not persuasive enough to explain why the arithmetic mean is the maximum value. So, I hope that there will someone who are willing to give their opinions on this. Thanks.","f(x_1,x_2,...,x_n)=(x_1x_2...x_n)^{1/n} \sum_{i=1}^{n} x_i=c x_i x_i f (\frac{c}{n},\frac{c}{n},...,\frac{c}{n}) 0","['multivariable-calculus', 'a.m.-g.m.-inequality']"
4,"Differentiating and integrating $r(t)=\langle 8t^4+\frac{5}{t} ,\sec^2(\frac{t\pi}{6},3e^t-2t\ln(t))\rangle $",Differentiating and integrating,"r(t)=\langle 8t^4+\frac{5}{t} ,\sec^2(\frac{t\pi}{6},3e^t-2t\ln(t))\rangle ","I am asked to differentiate and integrate $$r(t)=\langle 8t^4+\frac{5}{t} ,\sec^2(\frac{t\pi}{6},3e^t-2t\ln(t))\rangle $$ Am I supposed to differentiate/integrate each term independently? For example, I have that $r'(t)$ of $-8t^4 + 5/t$ is $-32t^3 - 5/x^2$ .","I am asked to differentiate and integrate Am I supposed to differentiate/integrate each term independently? For example, I have that of is .","r(t)=\langle 8t^4+\frac{5}{t} ,\sec^2(\frac{t\pi}{6},3e^t-2t\ln(t))\rangle  r'(t) -8t^4 + 5/t -32t^3 - 5/x^2","['linear-algebra', 'multivariable-calculus']"
5,vector identities in solving wave equations with different speeds of propagation,vector identities in solving wave equations with different speeds of propagation,,"Assume that $u = (u^1, u^2, u^3)$ solves the evolution equations of linear elasticity: $$u_{tt}-µ \Delta u − (λ + µ) D (\nabla\cdot u) = 0$$ in $\mathbf{R}^3 × (0, ∞)$ . Show that $w := \nabla \cdot u $ and $w := \nabla \times u$ each solve wave equations, but with differing speeds of propagation. This is problem 21 in chapter 2 of Evan's PDE. I am able to do this problem when $ w := \nabla \times u$ to obtain $w_{tt} = \mu \Delta w$ . For $ w:= \nabla \cdot u$ , I am not recognizing how to proceed from $$ w_{tt} = \mu \Delta w + (\lambda + \mu) \nabla (\nabla \cdot w) $$ to $$w_{tt} = \mu (\Delta w) + (\lambda + \mu)(\Delta w)$$ Since $ w = \nabla \cdot u$ is a scalar, I am not sure how the divergence of $w$ is defined here. The identity $\Delta w = \nabla(\nabla \cdot w) - \nabla \times \nabla \times w $ would be useful here but I can't see why the curl of curl of $w$ would be zero in this case or even defined when $w$ is a scalar. Maybe I am misunderstanding something in the statement of the problem? Any help will be appreciated.","Assume that solves the evolution equations of linear elasticity: in . Show that and each solve wave equations, but with differing speeds of propagation. This is problem 21 in chapter 2 of Evan's PDE. I am able to do this problem when to obtain . For , I am not recognizing how to proceed from to Since is a scalar, I am not sure how the divergence of is defined here. The identity would be useful here but I can't see why the curl of curl of would be zero in this case or even defined when is a scalar. Maybe I am misunderstanding something in the statement of the problem? Any help will be appreciated.","u = (u^1, u^2, u^3) u_{tt}-µ \Delta u − (λ + µ) D (\nabla\cdot u) = 0 \mathbf{R}^3 × (0, ∞) w := \nabla \cdot u  w := \nabla \times u  w := \nabla \times u w_{tt} = \mu \Delta w  w:= \nabla \cdot u  w_{tt} = \mu \Delta w + (\lambda + \mu) \nabla (\nabla \cdot w)  w_{tt} = \mu (\Delta w) + (\lambda + \mu)(\Delta w)  w = \nabla \cdot u w \Delta w = \nabla(\nabla \cdot w) - \nabla \times \nabla \times w  w w","['multivariable-calculus', 'partial-differential-equations', 'vector-fields', 'divergence-operator', 'curl']"
6,Riemann integral in two variables,Riemann integral in two variables,,"Let $f:[0,1]\times[0,1]\to \mathbb{R}$ , be defined by: \begin{equation}     f(x,y)=\begin{cases} 0 & \text{ if  }\; 0\leq x<\frac{1}{2},\\  1 & \text{ if  }\; \frac{1}{2}\leq x \leq 1.  \end{cases} \end{equation} Show that $f$ is integrable and $\displaystyle\int_{[0,1]\times [0,1]}f=\frac{1}{2}$ . Attempt. Let $P$ a partition of $[0,1]\times[0,1]$ , such that: $\mathcal{P}=(\mathcal{S}_{1},\mathcal{S}_{2})$ , where : \begin{align}     \begin{split}         \mathcal{S}_{1}&=\left[0,\frac{1}{2}\right]\times \left[0,1\right]\\         \mathcal{S}_{2}&=\left[\frac{1}{2},1\right]\times \left[0,1\right]\\     \end{split} \end{align} Define \begin{align}    \begin{split}      m_{\mathcal{S}_{1}}(f)&=\inf_{x\in \mathcal{S}_{1}}f(x)=0\\    m_{\mathcal{S}_{2}}(f)&=\inf_{x\in \mathcal{S}_{2}}f(x)=1\\       \end{split} \end{align} and \begin{align}       \begin{split}    M_{\mathcal{S}_{1}}(f)&=\sup_{x\in \mathcal{S}_{1}}f(x)=0\\    M_{\mathcal{S}_{2}}(f)&=\sup_{x\in \mathcal{S}_{2}}f(x)=1\\       \end{split}   \end{align} The lower and upper sums: \begin{align*}     L(f,\mathcal{P})& =\sum_{\mathcal{S}_{1}}m_{\mathcal{S}_{1}}(f).\upsilon(\mathcal{S}_{1})+\sum_{\mathcal{S}_{2}}m_{\mathcal{S}_{2}}(f).\upsilon(\mathcal{S}_{2})\\     &=\sum_{\mathcal{S}_{1}}0\cdot\frac{1}{2}+\sum_{\mathcal{S}_{2}}\frac{1}{2}\\     &=\frac{1}{2} \end{align*} and \begin{align*}     U(f,\mathcal{P})& =\sum_{\mathcal{S}_{1}}M_{\mathcal{S}_{1}}(f).\upsilon(\mathcal{S}_{1})+\sum_{\mathcal{S}_{2}}M_{\mathcal{S}_{2}}(f).\upsilon(\mathcal{S}_{2})\\     &=\sum_{\mathcal{S}_{1}}0\cdot\frac{1}{2}+\sum_{\mathcal{S}_{2}}\frac{1}{2}\\     &=\frac{1}{2} \end{align*} Then \begin{align*} \sup{L(f,\mathcal{P})}=\frac{1}{2}=\inf{U(f,\mathcal{P})}. \end{align*} Hence $f$ is integrable on $[0,1]\times [0,1]$ and $\displaystyle\int_{[0,1]\times [0,1]}f=\frac{1}{2}$ . I have doubts about my solution since my instructor told me that the conclusion is not correct since I calculate the lowest of the upper sums on a particular partition, and by definition of the upper integral it must be calculated on the set of all partitions. He suggested using the definition of infimum $ \varepsilon $ . I appreciate if someone could help me or give an indication.","Let , be defined by: Show that is integrable and . Attempt. Let a partition of , such that: , where : Define and The lower and upper sums: and Then Hence is integrable on and . I have doubts about my solution since my instructor told me that the conclusion is not correct since I calculate the lowest of the upper sums on a particular partition, and by definition of the upper integral it must be calculated on the set of all partitions. He suggested using the definition of infimum . I appreciate if someone could help me or give an indication.","f:[0,1]\times[0,1]\to \mathbb{R} \begin{equation}
    f(x,y)=\begin{cases}
0 & \text{ if  }\; 0\leq x<\frac{1}{2},\\ 
1 & \text{ if  }\; \frac{1}{2}\leq x \leq 1. 
\end{cases}
\end{equation} f \displaystyle\int_{[0,1]\times [0,1]}f=\frac{1}{2} P [0,1]\times[0,1] \mathcal{P}=(\mathcal{S}_{1},\mathcal{S}_{2}) \begin{align}
    \begin{split}
        \mathcal{S}_{1}&=\left[0,\frac{1}{2}\right]\times \left[0,1\right]\\
        \mathcal{S}_{2}&=\left[\frac{1}{2},1\right]\times \left[0,1\right]\\
    \end{split}
\end{align} \begin{align}
   \begin{split}
     m_{\mathcal{S}_{1}}(f)&=\inf_{x\in \mathcal{S}_{1}}f(x)=0\\
   m_{\mathcal{S}_{2}}(f)&=\inf_{x\in \mathcal{S}_{2}}f(x)=1\\   
   \end{split}
\end{align} \begin{align}
      \begin{split}
   M_{\mathcal{S}_{1}}(f)&=\sup_{x\in \mathcal{S}_{1}}f(x)=0\\
   M_{\mathcal{S}_{2}}(f)&=\sup_{x\in \mathcal{S}_{2}}f(x)=1\\
      \end{split}
  \end{align} \begin{align*}
    L(f,\mathcal{P})& =\sum_{\mathcal{S}_{1}}m_{\mathcal{S}_{1}}(f).\upsilon(\mathcal{S}_{1})+\sum_{\mathcal{S}_{2}}m_{\mathcal{S}_{2}}(f).\upsilon(\mathcal{S}_{2})\\
    &=\sum_{\mathcal{S}_{1}}0\cdot\frac{1}{2}+\sum_{\mathcal{S}_{2}}\frac{1}{2}\\
    &=\frac{1}{2}
\end{align*} \begin{align*}
    U(f,\mathcal{P})& =\sum_{\mathcal{S}_{1}}M_{\mathcal{S}_{1}}(f).\upsilon(\mathcal{S}_{1})+\sum_{\mathcal{S}_{2}}M_{\mathcal{S}_{2}}(f).\upsilon(\mathcal{S}_{2})\\
    &=\sum_{\mathcal{S}_{1}}0\cdot\frac{1}{2}+\sum_{\mathcal{S}_{2}}\frac{1}{2}\\
    &=\frac{1}{2}
\end{align*} \begin{align*}
\sup{L(f,\mathcal{P})}=\frac{1}{2}=\inf{U(f,\mathcal{P})}.
\end{align*} f [0,1]\times [0,1] \displaystyle\int_{[0,1]\times [0,1]}f=\frac{1}{2}  \varepsilon ","['real-analysis', 'multivariable-calculus', 'vector-analysis', 'alternative-proof']"
7,"Homogenous PDE, changing of variable","Homogenous PDE, changing of variable",,"I have an PDE $\dfrac{df}{d \xi}-\xi\dfrac{d x_1}{d \xi}=0$ homogeneous for $\xi$ , where $f:\mathbb{R}^n\to\mathbb{R}$ is function of $x_1,\cdots,x_n:\mathbb{R}\to\mathbb{R}$ , which in turn are functions of $\xi$ (so, $\dfrac{df}{d \xi}$ is a total derivative). I also have $\xi=y/z\in\mathbb{R}$ , where $y,z\in\mathbb{R}$ , and one said that once the PDE is homogeneous in $\xi$ I have $\dfrac{df}{d y}-\xi\dfrac{d x_1}{d y}=0$ . I missed this gap. I imagine that I can multiply the first ODE by $\dfrac{d\xi}{d y}=\dfrac{1}{z}$ , so $$\dfrac{df}{d \xi}\dfrac{d\xi}{d y}-\xi\dfrac{d x_1}{d \xi}\dfrac{d\xi}{d y}=\sum_{i=1}^n \dfrac{\partial f}{\partial x_i}\dfrac{d x_i}{d\xi}\dfrac{d\xi}{d y}-\xi\dfrac{d x_1}{d y}=\sum_{i=1}^n \dfrac{\partial f}{\partial x_i}\dfrac{d x_i}{d y}-\xi\dfrac{d x_1}{d y}=\dfrac{d f}{d y}-\xi\dfrac{d x_1}{d y}=0.$$ But I am not sure. Indeed, I'd like to know how state this only from the fact of homogeneous. Thanks so much. Related: Equality between two total derivatives","I have an PDE homogeneous for , where is function of , which in turn are functions of (so, is a total derivative). I also have , where , and one said that once the PDE is homogeneous in I have . I missed this gap. I imagine that I can multiply the first ODE by , so But I am not sure. Indeed, I'd like to know how state this only from the fact of homogeneous. Thanks so much. Related: Equality between two total derivatives","\dfrac{df}{d \xi}-\xi\dfrac{d x_1}{d \xi}=0 \xi f:\mathbb{R}^n\to\mathbb{R} x_1,\cdots,x_n:\mathbb{R}\to\mathbb{R} \xi \dfrac{df}{d \xi} \xi=y/z\in\mathbb{R} y,z\in\mathbb{R} \xi \dfrac{df}{d y}-\xi\dfrac{d x_1}{d y}=0 \dfrac{d\xi}{d y}=\dfrac{1}{z} \dfrac{df}{d \xi}\dfrac{d\xi}{d y}-\xi\dfrac{d x_1}{d \xi}\dfrac{d\xi}{d y}=\sum_{i=1}^n \dfrac{\partial f}{\partial x_i}\dfrac{d x_i}{d\xi}\dfrac{d\xi}{d y}-\xi\dfrac{d x_1}{d y}=\sum_{i=1}^n \dfrac{\partial f}{\partial x_i}\dfrac{d x_i}{d y}-\xi\dfrac{d x_1}{d y}=\dfrac{d f}{d y}-\xi\dfrac{d x_1}{d y}=0.","['functional-analysis', 'ordinary-differential-equations', 'multivariable-calculus', 'partial-differential-equations', 'homogeneous-equation']"
8,Local analysis two-variable functions with absolute value,Local analysis two-variable functions with absolute value,,"I have a problem with the local analysis of two-variable functions which have absolute value. For example, how can I study nature of point $(0,0)$ in $f:\mathbb{R}^2 \to \mathbb{R}, f(x,y)=sin(xy+|xy|)$ ? Or in the function $f:\mathbb{R}^2 \to \mathbb{R}, f(x,y)=e^{sin(|xy|)}$ ? I think that in both cases $(0,0)$ is a local minimum, but I don't know how to prove it. Typically, I use the Hessian matrix to do the analysis, however in this case I don't think it will help me...","I have a problem with the local analysis of two-variable functions which have absolute value. For example, how can I study nature of point in ? Or in the function ? I think that in both cases is a local minimum, but I don't know how to prove it. Typically, I use the Hessian matrix to do the analysis, however in this case I don't think it will help me...","(0,0) f:\mathbb{R}^2 \to \mathbb{R}, f(x,y)=sin(xy+|xy|) f:\mathbb{R}^2 \to \mathbb{R}, f(x,y)=e^{sin(|xy|)} (0,0)","['analysis', 'multivariable-calculus', 'hessian-matrix']"
9,Is there anything interesting about the limit $\lim_{h\rightarrow 0}\Big|\frac{f(a+h)-f(a)}{|h|}\Big|$?,Is there anything interesting about the limit ?,\lim_{h\rightarrow 0}\Big|\frac{f(a+h)-f(a)}{|h|}\Big|,"I was wondering if there is anything interesting to say about the following limit: Let $f:\mathbb{R}^n\rightarrow \mathbb{R}^m$ , and $a\in \mathbb{R}^n$ . Consider $$\lim_{h\rightarrow 0}\Big|\frac{f(a+h)-f(a)}{|h|}\Big|$$ If $f$ is differentiable I don't think the limit needs to converge. Suppose $f$ is differentiable at $a$ , then $$\lim_{h\rightarrow 0}\Big|\frac{f(a+h)-f(a)}{|h|}\Big|=\lim_{h\rightarrow 0}\Big|\frac{f(a+h)-f(a)-T(h)}{|h|}-\frac{|h|T(u)}{|h|}\Big|=\Big|T(u)\Big|$$ where $u$ is the unit vector in the direction of $h$ . So the direction at which the limit is taken matters.","I was wondering if there is anything interesting to say about the following limit: Let , and . Consider If is differentiable I don't think the limit needs to converge. Suppose is differentiable at , then where is the unit vector in the direction of . So the direction at which the limit is taken matters.",f:\mathbb{R}^n\rightarrow \mathbb{R}^m a\in \mathbb{R}^n \lim_{h\rightarrow 0}\Big|\frac{f(a+h)-f(a)}{|h|}\Big| f f a \lim_{h\rightarrow 0}\Big|\frac{f(a+h)-f(a)}{|h|}\Big|=\lim_{h\rightarrow 0}\Big|\frac{f(a+h)-f(a)-T(h)}{|h|}-\frac{|h|T(u)}{|h|}\Big|=\Big|T(u)\Big| u h,"['real-analysis', 'limits', 'multivariable-calculus', 'derivatives']"
10,"Find a closed curve where $F(x,y,z)=(y,x,x)$ closed line integral isn't 0.",Find a closed curve where  closed line integral isn't 0.,"F(x,y,z)=(y,x,x)","Given the function $F(x,y,z)=(y,x,x)$ , I have to show it isn't conservative and find a close curve where it isn't equal to 0. It was easy to show that $D_1F_3\neq D_3F_1$ , but I can't find an easy curve that isn't 0. I tried unit circle but it equals to zero. Watching the plot I can't figure it out. In general, I don't know how to solve this when I'm in $\mathbb{R^3}.$","Given the function , I have to show it isn't conservative and find a close curve where it isn't equal to 0. It was easy to show that , but I can't find an easy curve that isn't 0. I tried unit circle but it equals to zero. Watching the plot I can't figure it out. In general, I don't know how to solve this when I'm in","F(x,y,z)=(y,x,x) D_1F_3\neq D_3F_1 \mathbb{R^3}.","['multivariable-calculus', 'line-integrals']"
11,A proof on the Jacobian and error to prove differentiability,A proof on the Jacobian and error to prove differentiability,,"Let $F : \mathbb{R}^n \to \mathbb{R}^m$ . If there exists an $m \times n$ matrix $A$ and $\rho : \mathbb{R}^n \to \mathbb{R}^m$ such that $\forall h \in \mathbb{R}^n$ , $F(a+h) = F(a) + Ah + ||h||\rho(h)$ , where $\rho(h) \to 0$ as $h \to 0$ , then $F$ is differentiable at a. My attempt: Rearranging yields $\dfrac{F(a+h) - F(a) - Ah}{||h||} = \rho(h)$ , and $\rho(h) \to 0$ means the left hand goes to $0$ , and that's the definition of differentiability (if $A$ is the Jacobian). This almost seems too easy but I feel like I'm missing many details: could someone help me out in filling the missing assumptions?","Let . If there exists an matrix and such that , , where as , then is differentiable at a. My attempt: Rearranging yields , and means the left hand goes to , and that's the definition of differentiability (if is the Jacobian). This almost seems too easy but I feel like I'm missing many details: could someone help me out in filling the missing assumptions?",F : \mathbb{R}^n \to \mathbb{R}^m m \times n A \rho : \mathbb{R}^n \to \mathbb{R}^m \forall h \in \mathbb{R}^n F(a+h) = F(a) + Ah + ||h||\rho(h) \rho(h) \to 0 h \to 0 F \dfrac{F(a+h) - F(a) - Ah}{||h||} = \rho(h) \rho(h) \to 0 0 A,"['multivariable-calculus', 'proof-writing', 'solution-verification', 'jacobian']"
12,Chain Rule for multivariable functions to one variable functions,Chain Rule for multivariable functions to one variable functions,,"I'm trying to understand some questions about the chain rule of multivariable and how it relates with one variable functions. Ok, so, imagine that I have two functions $f: U \subset E \rightarrow \mathbb{R}$ and $\alpha: (-\epsilon, \epsilon) \subset \mathbb{R} \rightarrow E$ , where $E$ is a normed vector space. Then, by the chain rule, the derivative of $f \circ \alpha$ in some point $a \in U$ is: $(f \circ \alpha)'(a) = Df_{\alpha(a)} \cdot D\alpha_a$ . The point that I don't get is that $(f \circ \alpha)'(a)$ is a number, but $Df_{\alpha(a)} \cdot D\alpha_a$ . is a linear application. So, I'm a little bit confused, can the number $(f \circ \alpha)'(a)$ be interpreted in some way by a linear application? Thanks!","I'm trying to understand some questions about the chain rule of multivariable and how it relates with one variable functions. Ok, so, imagine that I have two functions and , where is a normed vector space. Then, by the chain rule, the derivative of in some point is: . The point that I don't get is that is a number, but . is a linear application. So, I'm a little bit confused, can the number be interpreted in some way by a linear application? Thanks!","f: U \subset E \rightarrow \mathbb{R} \alpha: (-\epsilon, \epsilon) \subset \mathbb{R} \rightarrow E E f \circ \alpha a \in U (f \circ \alpha)'(a) = Df_{\alpha(a)} \cdot D\alpha_a (f \circ \alpha)'(a) Df_{\alpha(a)} \cdot D\alpha_a (f \circ \alpha)'(a)","['multivariable-calculus', 'derivatives', 'chain-rule']"
13,Limits of integration to find $\int_0^tB_s^2\text{d}s$,Limits of integration to find,\int_0^tB_s^2\text{d}s,"I am currently attempting to find the variance of $\int_0^tB_s^2\text{d}s$ . In particular, I am confused about the integral of $\mathbb{E}\left(\int_0^tB_s^2\text{d}s\right)^2$ . I know that repeated applications of Fubini's theorem leads us to $\mathbb{E}\left(\int_0^tB_s^2\text{d}s\right)^2=\int_0^t\int_0^t\mathbb{E}(B_r^2B_s^2)^2\text{d}r\text{d}s$ . I also know that $\mathbb{E}(B_r^2B_s^2)^2=2s^2+rs$ . What I don't understand is why we cannot integrate this directly, i.e. $$\int_0^t\int_0^t\mathbb{E}(B_r^2B_s^2)^2\text{d}r\text{d}s=\int_0^t\int_0^t2s^2+rs\text{d}r\text{d}s=\int_0^t\frac{2t^3}3+\frac{rt^2}2\text{d}r=\frac{11t^4}{12}.$$ I have read from these links here , here and here that I should split the integral when $r<s$ and $s<r$ , and use the symmetry of the integrand to obtain $$\int_0^t\int_0^t\mathbb{E}(B_r^2B_s^2)^2\text{d}r\text{d}s=2\int_0^t\int_0^s2s^2+rs\text{d}r\text{d}s=\frac{7t^4}{12}.$$ Perhaps I have overlooked something from my elementary multivariable calculus, but I really can't think of any reason why. Help is appreciated, thanks!","I am currently attempting to find the variance of . In particular, I am confused about the integral of . I know that repeated applications of Fubini's theorem leads us to . I also know that . What I don't understand is why we cannot integrate this directly, i.e. I have read from these links here , here and here that I should split the integral when and , and use the symmetry of the integrand to obtain Perhaps I have overlooked something from my elementary multivariable calculus, but I really can't think of any reason why. Help is appreciated, thanks!",\int_0^tB_s^2\text{d}s \mathbb{E}\left(\int_0^tB_s^2\text{d}s\right)^2 \mathbb{E}\left(\int_0^tB_s^2\text{d}s\right)^2=\int_0^t\int_0^t\mathbb{E}(B_r^2B_s^2)^2\text{d}r\text{d}s \mathbb{E}(B_r^2B_s^2)^2=2s^2+rs \int_0^t\int_0^t\mathbb{E}(B_r^2B_s^2)^2\text{d}r\text{d}s=\int_0^t\int_0^t2s^2+rs\text{d}r\text{d}s=\int_0^t\frac{2t^3}3+\frac{rt^2}2\text{d}r=\frac{11t^4}{12}. r<s s<r \int_0^t\int_0^t\mathbb{E}(B_r^2B_s^2)^2\text{d}r\text{d}s=2\int_0^t\int_0^s2s^2+rs\text{d}r\text{d}s=\frac{7t^4}{12}.,"['multivariable-calculus', 'stochastic-processes', 'brownian-motion', 'multiple-integral', 'fubini-tonelli-theorems']"
14,"Find the extreme values of $f(x,y)=x^2y$ in $D=\{x^2+8y^2\leq24\}$",Find the extreme values of  in,"f(x,y)=x^2y D=\{x^2+8y^2\leq24\}","Find the extreme values of $f(x,y)=x^2y$ in $D=\{x^2+8y^2\leq24\}$ It was easy to find using Lagrange multipliers the local extreme values on $\partial{D}$ since we have the condition $x^2+8y^2=24.$ I found that $(-4,1),(-4,-1),(4,1)$ are critical points. Now I have to find the extreme values inside $D$ . $\nabla f(x,y)=(2xy,x^2)=(0,0)$ gives me that every $(0,y)$ is a critical point. However, I'm not sure how to get extremes from here, since I cannot use the determinant test. Would be a possibility to use the monotony of $x^2$ depending the sign of $y$ ?","Find the extreme values of in It was easy to find using Lagrange multipliers the local extreme values on since we have the condition I found that are critical points. Now I have to find the extreme values inside . gives me that every is a critical point. However, I'm not sure how to get extremes from here, since I cannot use the determinant test. Would be a possibility to use the monotony of depending the sign of ?","f(x,y)=x^2y D=\{x^2+8y^2\leq24\} \partial{D} x^2+8y^2=24. (-4,1),(-4,-1),(4,1) D \nabla f(x,y)=(2xy,x^2)=(0,0) (0,y) x^2 y","['real-analysis', 'multivariable-calculus']"
15,How to get the jacobian of this change of variables?,How to get the jacobian of this change of variables?,,"I am reading an article where we have $$\iint_{S^2\times\mathbb{R}^3}f(w,v)\,dw\,dv$$ where $w\in S^2$ and $v\in\mathbb{R}^3$ are variable vectors. A change of variable was done $(w,v)\mapsto(p,q)$ where $$p=(v\cdot w)w$$ and $$q=v-p=v-(v\cdot w)w.$$ That is, $p$ is the projection of $v$ on $w$ , and $q$ is the projection of $p$ on $w^\perp$ , where $w^\perp$ is the rotation of $w$ in the plane $\{w,v\}$ by $\pi/2$ . Then $p\in\mathbb{R}^3$ and $q\in\{p\}^\perp$ . They computed the Jacobian to be $$\,dw\,dv=\frac{2}{p^2\sin(p,p+q)}\,dp\,dq.$$ Does anyone have an idea how to compute this Jacobian explicitly? The article can be found here The part I am asking about is in page 12 proposition 5. It has to be related to spherical coordinate, but since $\omega$ is a vector I don't understand how they compute the Jacobian.","I am reading an article where we have where and are variable vectors. A change of variable was done where and That is, is the projection of on , and is the projection of on , where is the rotation of in the plane by . Then and . They computed the Jacobian to be Does anyone have an idea how to compute this Jacobian explicitly? The article can be found here The part I am asking about is in page 12 proposition 5. It has to be related to spherical coordinate, but since is a vector I don't understand how they compute the Jacobian.","\iint_{S^2\times\mathbb{R}^3}f(w,v)\,dw\,dv w\in S^2 v\in\mathbb{R}^3 (w,v)\mapsto(p,q) p=(v\cdot w)w q=v-p=v-(v\cdot w)w. p v w q p w^\perp w^\perp w \{w,v\} \pi/2 p\in\mathbb{R}^3 q\in\{p\}^\perp \,dw\,dv=\frac{2}{p^2\sin(p,p+q)}\,dp\,dq. \omega","['multivariable-calculus', 'multiple-integral', 'jacobian', 'change-of-variable']"
16,"Is the limit $\lim_{t \to 0} \left(t^2\ln t , t, \ln(t^3)\right) $ defined?",Is the limit  defined?,"\lim_{t \to 0} \left(t^2\ln t , t, \ln(t^3)\right) ","Find the limit or prove that it doesn't exist $$\lim_{t \to 0} \left(t^2\ln t , t,  \ln(t^3)\right) $$ I got $$(0,\space 0, \space -\infty)$$ does this mean the limit is defined ? Thanks in advance",Find the limit or prove that it doesn't exist I got does this mean the limit is defined ? Thanks in advance,"\lim_{t \to 0} \left(t^2\ln t , t,  \ln(t^3)\right)  (0,\space 0, \space -\infty)","['limits', 'multivariable-calculus']"
17,Derivative of a function $f({\bf X})$ w.r.t. matrix ${\bf X}$ when ${\bf X}^\prime {\bf X}$ is a diagonal matrix,Derivative of a function  w.r.t. matrix  when  is a diagonal matrix,f({\bf X}) {\bf X} {\bf X}^\prime {\bf X},"I wish to compute the derivative and the Hessian of the function ${\bf f}(X)$ where $$ {\bf f}({\bf X}) = {\bf X} \, {\bf a}, $$ ${\bf X}$ is an $(m \times n)$ matrix and ${\bf a}$ is a vector of constants of size $n$ . From ""Matrix Differential Calculus"" by Magnus and Neudecker, it is relatively straightforward to obtain the derivative and the Hessian of ${\bf f}({\bf X})$ w.r.t. ${\bf X}$ . For example, the first differential is \begin{align} \partial {\bf f}({\bf X}) &= (\partial {\bf X}) {\bf a} = {\rm vec} (\partial {\bf X}) {\bf a} = ({\bf a}^\prime \otimes {\bf I}_m) \,\partial {\rm vec}  {\bf X} \\ \end{align} In my case however the matrix ${\bf X}$ has a special structure. Specifically, I know that that ${\bf X}^\prime {\bf X}$ is a diagonal matrix, say, ${\bf X}^\prime {\bf X} = {\rm diag}(d_1, \ldots, d_n)$ , which is not necessarily equal to the identity matrix. In other words, I know that the columns of ${\bf X}$ are mutually orthogonal, but each column vector can be of any length. How do I compute the derivative and the Hessian of $f({\bf X})$ w.r.t. ${\bf X}$ to take this structure into account? Any references on how to proceed would be greatly appreciated. I could not find a similar question posted before. The closest I found was Hessian of $f(X)$ when $X$ is a symmetric matrix , but I was not able to apply it to my problem.","I wish to compute the derivative and the Hessian of the function where is an matrix and is a vector of constants of size . From ""Matrix Differential Calculus"" by Magnus and Neudecker, it is relatively straightforward to obtain the derivative and the Hessian of w.r.t. . For example, the first differential is In my case however the matrix has a special structure. Specifically, I know that that is a diagonal matrix, say, , which is not necessarily equal to the identity matrix. In other words, I know that the columns of are mutually orthogonal, but each column vector can be of any length. How do I compute the derivative and the Hessian of w.r.t. to take this structure into account? Any references on how to proceed would be greatly appreciated. I could not find a similar question posted before. The closest I found was Hessian of $f(X)$ when $X$ is a symmetric matrix , but I was not able to apply it to my problem.","{\bf f}(X)  {\bf f}({\bf X}) = {\bf X} \, {\bf a},  {\bf X} (m \times n) {\bf a} n {\bf f}({\bf X}) {\bf X} \begin{align}
\partial {\bf f}({\bf X}) &= (\partial {\bf X}) {\bf a} = {\rm vec} (\partial {\bf X}) {\bf a} = ({\bf a}^\prime \otimes {\bf I}_m) \,\partial {\rm vec}  {\bf X} \\
\end{align} {\bf X} {\bf X}^\prime {\bf X} {\bf X}^\prime {\bf X} = {\rm diag}(d_1, \ldots, d_n) {\bf X} f({\bf X}) {\bf X}","['multivariable-calculus', 'matrix-calculus', 'hessian-matrix']"
18,Why can't set of discontinous points with volume zero intersect with the boundary of a rectangle?,Why can't set of discontinous points with volume zero intersect with the boundary of a rectangle?,,"In Multivariable Mathematics by Dr. Shifrin, Prop. 1.8 in chapter 7 says Suppose $f:R\to \mathbb{R}$ is a bounded function and the set $X=\{\mathbf{x}\in R:f \text{ is not continuous at }\mathbf{x}\}$ has volume zero. Then $f$ is integrable on $R$ . Here $R\subset\mathbb{R}^n$ is a closed rectangle. The proof then covers $X$ by finitely many closed rectangles $R_1', \dotsc, R_s'$ due to volume zero and the critical line where I'm stuck is ""We can also ensure that no point of $X$ is a frontier point of the union of these rectangles"". My question is why can't a point of discontinuity of $f$ occur on the frontier of the rectangle $R$ and thus necessarily on the frontier of the union of $R_1', \dotsc, R_s' \subset R$ ?","In Multivariable Mathematics by Dr. Shifrin, Prop. 1.8 in chapter 7 says Suppose is a bounded function and the set has volume zero. Then is integrable on . Here is a closed rectangle. The proof then covers by finitely many closed rectangles due to volume zero and the critical line where I'm stuck is ""We can also ensure that no point of is a frontier point of the union of these rectangles"". My question is why can't a point of discontinuity of occur on the frontier of the rectangle and thus necessarily on the frontier of the union of ?","f:R\to \mathbb{R} X=\{\mathbf{x}\in R:f \text{ is not continuous at }\mathbf{x}\} f R R\subset\mathbb{R}^n X R_1', \dotsc, R_s' X f R R_1', \dotsc, R_s' \subset R","['integration', 'multivariable-calculus', 'continuity', 'volume']"
19,Implicit partial derivatives,Implicit partial derivatives,,"Say I have a function $F(P,V,T) = 0 $ . Using the chain rule: $$dF = \frac{\partial F}{\partial P}dP + \frac{\partial F}{\partial V}dV + \frac{\partial F}{\partial T}dT$$ Now, if P is constant, we have $dP = 0$ and $dF = 0$ of course, because $F$ is a constant (0). We get: $$dF = \frac{\partial F}{\partial V}dV + \frac{\partial F}{\partial T}dT = 0 \implies (\frac{dV}{dT})_P = - \frac{\frac{\partial F}{\partial T}} {\frac{\partial F}{\partial V}}$$ And that is true generally? Correct me if i'm mistaken please. Now, if we have an explicit expression for $P(T,V)$ we can also say for constant P: $$ (\frac{dV}{dT})_P = - \frac{\frac{\partial P}{\partial T}} {\frac{\partial P}{\partial V}}$$ BUT the results aren't always the same, why is that? Do P,V and T have to be independent of each other for the first derivation to be correct? I'm really confused and will be glad for explanation without getting too rigorous. Edit: Example (from physics): Say: $$F(P,V,T)=PV-PNb+\frac{aN^{2}}{V}-\frac{aN^{3}b}{V^{2}}-Nk_{B}T=0$$ We can solve for P: $$P = N\left(\frac{K_{B}T}{V-Nb}-\frac{aN}{V^{2}}\right)$$ We now get: $$- \frac{\frac{\partial F}{\partial T}} {\frac{\partial F}{\partial V}} \neq - \frac{\frac{\partial P}{\partial T}} {\frac{\partial P}{\partial V}} $$","Say I have a function . Using the chain rule: Now, if P is constant, we have and of course, because is a constant (0). We get: And that is true generally? Correct me if i'm mistaken please. Now, if we have an explicit expression for we can also say for constant P: BUT the results aren't always the same, why is that? Do P,V and T have to be independent of each other for the first derivation to be correct? I'm really confused and will be glad for explanation without getting too rigorous. Edit: Example (from physics): Say: We can solve for P: We now get:","F(P,V,T) = 0  dF = \frac{\partial F}{\partial P}dP + \frac{\partial F}{\partial V}dV + \frac{\partial F}{\partial T}dT dP = 0 dF = 0 F dF = \frac{\partial F}{\partial V}dV + \frac{\partial F}{\partial T}dT = 0 \implies (\frac{dV}{dT})_P = - \frac{\frac{\partial F}{\partial T}} {\frac{\partial F}{\partial V}} P(T,V)  (\frac{dV}{dT})_P = - \frac{\frac{\partial P}{\partial T}} {\frac{\partial P}{\partial V}} F(P,V,T)=PV-PNb+\frac{aN^{2}}{V}-\frac{aN^{3}b}{V^{2}}-Nk_{B}T=0 P = N\left(\frac{K_{B}T}{V-Nb}-\frac{aN}{V^{2}}\right) - \frac{\frac{\partial F}{\partial T}} {\frac{\partial F}{\partial V}} \neq - \frac{\frac{\partial P}{\partial T}} {\frac{\partial P}{\partial V}} ","['calculus', 'multivariable-calculus', 'derivatives']"
20,$f$ have finitely many critical points in $\Omega$,have finitely many critical points in,f \Omega,"Assume that $\Omega$ is an bounded open set in $R^m, f\in C^2(\overline{\Omega},R^m)$ . If $f$ does not have any critical point in $\partial \Omega$ , and all the critical points of $f$ in $\Omega$ are non-degenerate, prove that $f$ has finitely many critical points in $\Omega$ . Note that $a$ is a critical point of $f$ if $\nabla f(a)=0$ , and a critical point $a$ is said to be non-degenerate if $\det H_f(a) \neq0$ . Here $H_f$ denotes the Hessian matrix of $f$ . A few observations: $\overline{\Omega}$ and $\partial \Omega$ are both compact. Hence $f$ obtains its extreme value in $\overline{\Omega}$ , and the restriction of $f$ i.e. $f|_{\partial\Omega}$ also obtains its extreme value. And, take maximum value for example, $\max_{x\in \partial\Omega} f(x) \leq \max_{x\in \overline{\Omega}} f(x)$ . Need help.","Assume that is an bounded open set in . If does not have any critical point in , and all the critical points of in are non-degenerate, prove that has finitely many critical points in . Note that is a critical point of if , and a critical point is said to be non-degenerate if . Here denotes the Hessian matrix of . A few observations: and are both compact. Hence obtains its extreme value in , and the restriction of i.e. also obtains its extreme value. And, take maximum value for example, . Need help.","\Omega R^m, f\in C^2(\overline{\Omega},R^m) f \partial \Omega f \Omega f \Omega a f \nabla f(a)=0 a \det H_f(a) \neq0 H_f f \overline{\Omega} \partial \Omega f \overline{\Omega} f f|_{\partial\Omega} \max_{x\in \partial\Omega} f(x) \leq \max_{x\in \overline{\Omega}} f(x)","['multivariable-calculus', 'derivatives', 'stationary-point']"
21,Double Integral with absolute value.,Double Integral with absolute value.,,"I'm trying to get $$\int_0^2\int_0^{\sqrt{2x}}y^2|x-y|dydx.$$ I'm struggling with that absolute value. I'm not sure how to ""divide"" the integral into two. I know that $$|x-y|=\begin{cases} x-y &\text{ if }x\geq y, \\ y-x &\text{ if }x\leq y.\end{cases}$$ My problem is I have that $y=\sqrt{2x}$ in the top integral extreme.","I'm trying to get I'm struggling with that absolute value. I'm not sure how to ""divide"" the integral into two. I know that My problem is I have that in the top integral extreme.","\int_0^2\int_0^{\sqrt{2x}}y^2|x-y|dydx. |x-y|=\begin{cases} x-y &\text{ if }x\geq y, \\
y-x &\text{ if }x\leq y.\end{cases} y=\sqrt{2x}","['multivariable-calculus', 'absolute-value', 'multiple-integral']"
22,Finding the smooth inverse of a function,Finding the smooth inverse of a function,,"Problem: The mapping $\phi: S^2 \longrightarrow S^2 $ by $$\phi(x,y,z)=(x\cos z+y\sin z,x\sin z-y \cos z,z)$$ is a diffeomorphism.  Where $S^2$ is a unit sphere in $\mathbb{R}^3$ . I've already shown that $\phi$ is smooth and bijective. The only thing I can't find is $\phi^{-1}$ that is smooth. Any help would be much appreciated!",Problem: The mapping by is a diffeomorphism.  Where is a unit sphere in . I've already shown that is smooth and bijective. The only thing I can't find is that is smooth. Any help would be much appreciated!,"\phi: S^2 \longrightarrow S^2  \phi(x,y,z)=(x\cos z+y\sin z,x\sin z-y \cos z,z) S^2 \mathbb{R}^3 \phi \phi^{-1}","['multivariable-calculus', 'differential-geometry', 'smooth-functions']"
23,Continuous game payoff,Continuous game payoff,,"I'm new in game theory. The lecturer said that is something wrong with a sentence that is highlighted in yellow, but I can't spot any inaccuracy in it. Please share your opinion, what is wrong in that sentence.","I'm new in game theory. The lecturer said that is something wrong with a sentence that is highlighted in yellow, but I can't spot any inaccuracy in it. Please share your opinion, what is wrong in that sentence.",,"['multivariable-calculus', 'partial-derivative', 'game-theory']"
24,The directional derivative is the Jacobian,The directional derivative is the Jacobian,,"Let $ f:\mathbb{R}^m \rightarrow \mathbb{R}^n$ differentiable at $x$ and $v$ an unit vector in $\mathbb{R}^m$ . I've always used the formula that the directional derivative in $x$ by $v$ is simply the Jacobian times the $v$ . However, I could never figure it out how to prove it. $$\frac{\partial f}{\partial v}(x) = Df(x) v$$ In this question , the answer simply assumes that $$\lim_{t\to0}\frac{\bigl\lVert f(a+tv)-f(a)-Df(a)(tv)\bigr\rVert}{\lVert tv\rVert}=\lim_{t\to0}\frac{f(a+tv)-f(a)-tDf(a)(v)}t$$ But it apparently ignores the triangular inequality in the given norm. What am I not seeing here? How can I prove it?","Let differentiable at and an unit vector in . I've always used the formula that the directional derivative in by is simply the Jacobian times the . However, I could never figure it out how to prove it. In this question , the answer simply assumes that But it apparently ignores the triangular inequality in the given norm. What am I not seeing here? How can I prove it?", f:\mathbb{R}^m \rightarrow \mathbb{R}^n x v \mathbb{R}^m x v v \frac{\partial f}{\partial v}(x) = Df(x) v \lim_{t\to0}\frac{\bigl\lVert f(a+tv)-f(a)-Df(a)(tv)\bigr\rVert}{\lVert tv\rVert}=\lim_{t\to0}\frac{f(a+tv)-f(a)-tDf(a)(v)}t,"['multivariable-calculus', 'partial-derivative', 'jacobian']"
25,How to find range of a function $f : \mathbb{R}^2\to \mathbb{R}^2 $,How to find range of a function,f : \mathbb{R}^2\to \mathbb{R}^2 ,"How to find range of a function $f : \mathbb{R}^2\to \mathbb{R}^2 $ defined by $f(x,y)=(x^2-y^2, 2xy)$ I am new to calculus of several variables and I have no idea on how to solve such questions. I tried the following way: We set $f(x,y) = (x^2-y^2, 2xy) = (h,k)$ but it is getting very dirty solving for $x$ and $y$ from this. However, is this the correct method?","How to find range of a function defined by I am new to calculus of several variables and I have no idea on how to solve such questions. I tried the following way: We set but it is getting very dirty solving for and from this. However, is this the correct method?","f : \mathbb{R}^2\to \mathbb{R}^2  f(x,y)=(x^2-y^2, 2xy) f(x,y) = (x^2-y^2, 2xy) = (h,k) x y","['multivariable-calculus', 'implicit-function-theorem', 'inverse-function-theorem']"
26,"Limit $\lim_{(x,y)\to\infty} e^{-e^{xy}}$ with polar coordinates",Limit  with polar coordinates,"\lim_{(x,y)\to\infty} e^{-e^{xy}}","can i use polar to solve this limit? $$\lim_{(x,y)\to\infty} e^{-e^{xy}}=$$ $$\frac{1}{e^{e^{r^2\cos\theta\sin \theta}}}=$$ but i'm quite stuck here i think the denominator goes to infinity but should i show that? or can i just write $0$ as a solution after the step above like $$\frac{1}{e^{e^{r^2\cos\theta\sin \theta}}}=0$$ is it correct? any help or suggestion would be very helpful Thank's in advance",can i use polar to solve this limit? but i'm quite stuck here i think the denominator goes to infinity but should i show that? or can i just write as a solution after the step above like is it correct? any help or suggestion would be very helpful Thank's in advance,"\lim_{(x,y)\to\infty} e^{-e^{xy}}= \frac{1}{e^{e^{r^2\cos\theta\sin \theta}}}= 0 \frac{1}{e^{e^{r^2\cos\theta\sin \theta}}}=0","['calculus', 'limits', 'multivariable-calculus', 'polar-coordinates', 'limits-without-lhopital']"
27,How to use the chain rule (multivariable) to find an expression for the second derivative,How to use the chain rule (multivariable) to find an expression for the second derivative,,"More specifically, I’m thinking about how to find a nice expression for $\frac{\partial^2 f}{\partial x^2}$ where we have an $f$ of the form $f(r(x,y), \varphi(x,y))$ . If anyone could help me with this it would be greatly appreciated.","More specifically, I’m thinking about how to find a nice expression for where we have an of the form . If anyone could help me with this it would be greatly appreciated.","\frac{\partial^2 f}{\partial x^2} f f(r(x,y), \varphi(x,y))","['multivariable-calculus', 'partial-derivative', 'chain-rule']"
28,Existence of multivariable function whos limit exists agrees for all paths that are functions but fails otherwise.,Existence of multivariable function whos limit exists agrees for all paths that are functions but fails otherwise.,,"Consider $\lim_{(x, y) \to (a, b)} f(x, y)$ . We know the limit for this function exists if for all paths, $\lim_{(x, y) \to (a, b)} f(x, y) = L$ . In addition, if we want to show that $\lim_{(x, y) \to (a, b)} f(x, y)$ does not exist, we can take $y=g(x)$ and y= $m(x)$ s.t. $\lim_{(x, y) \to (a, b)} f(x, g(x)) = T$ and $\lim_{(x, y) \to (a, b)} f(x, m(x)) = S$ with $S \neq T$ to conclude that the limit does not exist. My questions is if for all functions $y=g(x)$ , the limit $\lim_{(x, y) \to (a, b)} f(x, g(x)) = L$ agrees and is equal to $L$ , does that allow us to conclude that $\lim_{(x, y) \to (a, b)} f(x, y) = L$ ? I would imagine that it is not enough since some paths do not need to be functions, but I can't imagine a pathological counter example that could have its limit agree for every well defined function but fail otherwise.","Consider . We know the limit for this function exists if for all paths, . In addition, if we want to show that does not exist, we can take and y= s.t. and with to conclude that the limit does not exist. My questions is if for all functions , the limit agrees and is equal to , does that allow us to conclude that ? I would imagine that it is not enough since some paths do not need to be functions, but I can't imagine a pathological counter example that could have its limit agree for every well defined function but fail otherwise.","\lim_{(x, y) \to (a, b)} f(x, y) \lim_{(x, y) \to (a, b)} f(x, y) = L \lim_{(x, y) \to (a, b)} f(x, y) y=g(x) m(x) \lim_{(x, y) \to (a, b)} f(x, g(x)) = T \lim_{(x, y) \to (a, b)} f(x, m(x)) = S S \neq T y=g(x) \lim_{(x, y) \to (a, b)} f(x, g(x)) = L L \lim_{(x, y) \to (a, b)} f(x, y) = L","['limits', 'multivariable-calculus']"
29,How to determine if a field is a gradient field?,How to determine if a field is a gradient field?,,"What is a gradient field? If I have a scalar function : $$f(x,y)$$ and the derivative of the scalar function   i.e. the gradient of the function is : $$ \nabla f $$ Is it true that if $ \nabla f $ somehow equals the vector field only then can I call it a gradeint vector field? I'm considerably new to this topic..so forgive me if this seems like a stupid question. If I have a gradient that looks like $$<xy,y^2>$$ what would the original function look like? How do I know if the field is a gradient field. Is it necessary that the fundamental theorem of line integral calculus : $$\int_C \overrightarrow {\nabla f}. \overrightarrow {dr} = f(P_1)-f(P_0)$$ where $P_1$ and $P_0$ are the final and initial points on the curve satisfies only if f is a gradient field.",What is a gradient field? If I have a scalar function : and the derivative of the scalar function   i.e. the gradient of the function is : Is it true that if somehow equals the vector field only then can I call it a gradeint vector field? I'm considerably new to this topic..so forgive me if this seems like a stupid question. If I have a gradient that looks like what would the original function look like? How do I know if the field is a gradient field. Is it necessary that the fundamental theorem of line integral calculus : where and are the final and initial points on the curve satisfies only if f is a gradient field.,"f(x,y)  \nabla f   \nabla f  <xy,y^2> \int_C \overrightarrow {\nabla f}. \overrightarrow {dr} = f(P_1)-f(P_0) P_1 P_0",['multivariable-calculus']
30,Find angle a path makes with the horizontal given magnitude and angle made with other paths.,Find angle a path makes with the horizontal given magnitude and angle made with other paths.,,"Given this information: ""The elevation along a straight path up the side of a hill increases at a rate of 1/3 meter per horizontal meter. At point P, a new path at an angle of 𝜋/6 with the original path goes off more steeply uphill. The steepest uphill direction at P makes an angle of 𝜋/3 with the original path."" ""What angle does the new path make with the horizontal?"" It seems to be a standard directional derivative and gradient question, but I can't seem to get any definitive answer, because no direction of the original path or the new path/steepest path is given. For example, it seems valid to say the original path is along x-axis, in which case the horizontal angle would be 𝜋/6 , or I could have the original path be have an angle 5𝜋/6 with the x-axis, in which case it seems my new path would have a horizontal angle of 𝜋/2. I'm either misunderstanding the question and visualizing it wrong, or I need more information. Anyone have an idea about this?","Given this information: ""The elevation along a straight path up the side of a hill increases at a rate of 1/3 meter per horizontal meter. At point P, a new path at an angle of 𝜋/6 with the original path goes off more steeply uphill. The steepest uphill direction at P makes an angle of 𝜋/3 with the original path."" ""What angle does the new path make with the horizontal?"" It seems to be a standard directional derivative and gradient question, but I can't seem to get any definitive answer, because no direction of the original path or the new path/steepest path is given. For example, it seems valid to say the original path is along x-axis, in which case the horizontal angle would be 𝜋/6 , or I could have the original path be have an angle 5𝜋/6 with the x-axis, in which case it seems my new path would have a horizontal angle of 𝜋/2. I'm either misunderstanding the question and visualizing it wrong, or I need more information. Anyone have an idea about this?",,"['multivariable-calculus', 'vectors', 'partial-derivative']"
31,When is a multivariable limit path independent?,When is a multivariable limit path independent?,,"When I had calculus I was taught that the limit of a multivariable limit can be path-dependent. So In order to check if a limit exists, you should, in theory, check every possible path, which is infinitely many. So how do I actually calculate a multivariable limit? Just because I have checked one path, it doesn't necessarily mean the limit would be the same at every path? Is there an easy way to know whether a limit is path independent, or when a multivariable limit might be path dependent? Consider the limit: $$\lim _{(x, y) \rightarrow(2,3)} 2x^3-y^{2}=16-9=7$$ How do I know that I can just put in the values in this case? $$\lim _{(x, y) \rightarrow(0,0)} \frac{x^{2} y}{x^{4}+y^{2}}$$ I know this limit does not exist, because if you go along the path $y=mx$ the limit is 0. But if you go along the parabola $y=\pm x^2$ the limit is $\pm \frac{1}{2}$ . How are these two cases different. I mean how can you immediately see that the first case is path independent, but the second case may not be?","When I had calculus I was taught that the limit of a multivariable limit can be path-dependent. So In order to check if a limit exists, you should, in theory, check every possible path, which is infinitely many. So how do I actually calculate a multivariable limit? Just because I have checked one path, it doesn't necessarily mean the limit would be the same at every path? Is there an easy way to know whether a limit is path independent, or when a multivariable limit might be path dependent? Consider the limit: How do I know that I can just put in the values in this case? I know this limit does not exist, because if you go along the path the limit is 0. But if you go along the parabola the limit is . How are these two cases different. I mean how can you immediately see that the first case is path independent, but the second case may not be?","\lim _{(x, y) \rightarrow(2,3)} 2x^3-y^{2}=16-9=7 \lim _{(x, y) \rightarrow(0,0)} \frac{x^{2} y}{x^{4}+y^{2}} y=mx y=\pm x^2 \pm \frac{1}{2}",['multivariable-calculus']
32,"Clarification requested: Surface integrals, functions defined on surfaces and dimension","Clarification requested: Surface integrals, functions defined on surfaces and dimension",,"Surface integrals were always confusing for me, mainly because I never studied them carefully and in depth. After some research on several sources (not sufficient enough) I came across with the following questions/statements which I would highly appreciate if someone could explain/verify. Let's consider the unit sphere $\mathbb S^2=\{(x,y,z):\;x^2+y^2+z^2=1\}$ . A function $f$ defined on $\mathbb S^2$ is, in general, a function of $3-$ variables. The surface integral $\int_{\mathbb S^2} f\;dS$ though, can be calculated with an appropriate parametrization as a double integral (with respect to $2-$ coordinates). Is this correct? If yes, then how is this justified? Assume now that $f$ is a function defined on $\mathbb S^2$ as before but depends only on variable $x$ . If I want to integrate $f$ over the set $\{x>x_0\}$ , how would this integral look like? I've seen this formula $\int_{\{x>x_0\}} f\;dS=2\pi \int_{x_0}^1 f(x)\;dx$ which implies (if not mistaken) that there was some extra integration from $0$ to $2\pi$ that resulted in the $2\pi$ factor but I can't understand how this came up. I tried to use cylindrical coordinates but I don't obtain the desired formula. I'm having a really hard time getting my head around these notions and arguments. Any help, hint or comments are very much welcome. Thanks a lot in advance!","Surface integrals were always confusing for me, mainly because I never studied them carefully and in depth. After some research on several sources (not sufficient enough) I came across with the following questions/statements which I would highly appreciate if someone could explain/verify. Let's consider the unit sphere . A function defined on is, in general, a function of variables. The surface integral though, can be calculated with an appropriate parametrization as a double integral (with respect to coordinates). Is this correct? If yes, then how is this justified? Assume now that is a function defined on as before but depends only on variable . If I want to integrate over the set , how would this integral look like? I've seen this formula which implies (if not mistaken) that there was some extra integration from to that resulted in the factor but I can't understand how this came up. I tried to use cylindrical coordinates but I don't obtain the desired formula. I'm having a really hard time getting my head around these notions and arguments. Any help, hint or comments are very much welcome. Thanks a lot in advance!","\mathbb S^2=\{(x,y,z):\;x^2+y^2+z^2=1\} f \mathbb S^2 3- \int_{\mathbb S^2} f\;dS 2- f \mathbb S^2 x f \{x>x_0\} \int_{\{x>x_0\}} f\;dS=2\pi \int_{x_0}^1 f(x)\;dx 0 2\pi 2\pi","['calculus', 'multivariable-calculus', 'surface-integrals']"
33,Prove that $f$ is the identically zero function.,Prove that  is the identically zero function.,f,"Let $f : [0,1] \times [0,1] \longrightarrow [0,\infty)$ be a continuous function. Suppose that $$\int_{0}^{1} \left ( \int_{0}^{1} f(x,y)\ dy \right ) dx = 0.$$ Prove that $f$ is the identically zero function. My attempt $:$ $f$ is a non-negative measurable function on $[0,1] \times [0,1].$ So by Tonelli's theorem we have $$\iint_{[0,1] \times [0,1]} f(x,y)\ dx\ dy = \int_{0}^{1} \left ( \int_{0}^{1} f(x,y)\ dy \right ) dx = 0.$$ So if there exists $\textbf {x}_0= (x_0,y_0) \in [0,1] \times [0,1]$ such that $f(\textbf {x}_0) > 0,$ by continuity of $f$ at $\textbf {x}_0$ there exists an open ball $B( \textbf {x}_0, \delta)$ of some radius $\delta > 0$ surrounding $\textbf {x}_0$ with $B(\textbf {x}_0, \delta) \subseteq [0,1] \times [0,1]$ such that for any $\textbf {x} = (x,y) \in B(\textbf {x}_0, \delta)$ we have $$f(\textbf {x}) \gt \frac {f(\textbf{x}_0)} {2 \pi {\delta}^2}.$$ Since $f$ is a non-negative function so we must have $$\frac {f(\textbf {x}_0 )} {2} \lt \iint_{B(\textbf {x}_0, \delta)} f(x,y)\ dx\ dy \leq \iint_{[0,1] \times [0,1]} f(x,y)\ dx\ dy = 0$$ a contradiction. This shows that $f \equiv 0$ on $[0,1] \times [0,1].$ Can anybody plaese check my proof to ascertain whether it holds good or not? Thanks in advance.",Let be a continuous function. Suppose that Prove that is the identically zero function. My attempt is a non-negative measurable function on So by Tonelli's theorem we have So if there exists such that by continuity of at there exists an open ball of some radius surrounding with such that for any we have Since is a non-negative function so we must have a contradiction. This shows that on Can anybody plaese check my proof to ascertain whether it holds good or not? Thanks in advance.,"f : [0,1] \times [0,1] \longrightarrow [0,\infty) \int_{0}^{1} \left ( \int_{0}^{1} f(x,y)\ dy \right ) dx = 0. f : f [0,1] \times [0,1]. \iint_{[0,1] \times [0,1]} f(x,y)\ dx\ dy = \int_{0}^{1} \left ( \int_{0}^{1} f(x,y)\ dy \right ) dx = 0. \textbf {x}_0= (x_0,y_0) \in [0,1] \times [0,1] f(\textbf {x}_0) > 0, f \textbf {x}_0 B( \textbf {x}_0, \delta) \delta > 0 \textbf {x}_0 B(\textbf {x}_0, \delta) \subseteq [0,1] \times [0,1] \textbf {x} = (x,y) \in B(\textbf {x}_0, \delta) f(\textbf {x}) \gt \frac {f(\textbf{x}_0)} {2 \pi {\delta}^2}. f \frac {f(\textbf {x}_0 )} {2} \lt \iint_{B(\textbf {x}_0, \delta)} f(x,y)\ dx\ dy \leq \iint_{[0,1] \times [0,1]} f(x,y)\ dx\ dy = 0 f \equiv 0 [0,1] \times [0,1].","['multivariable-calculus', 'continuity', 'solution-verification', 'multiple-integral', 'fubini-tonelli-theorems']"
34,Mixed directional derivative of second order and the Hessian matrix,Mixed directional derivative of second order and the Hessian matrix,,"First order directional derivative in 2 dimensional $xy$ -coorinate system is $$D_{\bf{u}}=u_1\frac{\partial}{\partial x}+u_2\frac{\partial}{\partial y}$$ I thought to go further and analyse second order directional derivatives. The formula for general second order mixed directional derivative is $$ D^2_{\bf{u}\bf{v}}=D_{\bf{v}}\left[u_1\frac{\partial}{\partial x}+u_2\frac{\partial}{\partial y}\right]=u_1D_{\bf{v}}\frac{\partial}{\partial x}+u_2D_{\bf{v}}\frac{\partial}{\partial y}=$$ $$=u_1(v_1\frac{\partial^2}{\partial x^2}+v_2\frac{\partial^2}{\partial y\partial x})+u_2(v_1\frac{\partial^2}{\partial x\partial y}+v_2\frac{\partial^2}{\partial y^2})=$$ $$=u_1v_1\frac{\partial^2}{\partial x^2}+u_1v_2\frac{\partial^2}{\partial y\partial x}+u_2v_1\frac{\partial^2}{\partial x\partial y}+u_2v_2\frac{\partial^2}{\partial y^2}$$ I see the formula we get has definitely something to do with the following two matrices: $$U=\left[\matrix{u_1v_1 && u_1v_2\\ u_2v_1 && u_2v_2}\right]$$ $$H=\left[\matrix{\frac{\partial^2}{\partial x^2} && \frac{\partial^2}{\partial x\partial y} \\ \frac{\partial^2}{\partial y\partial x} && \frac{\partial^2}{\partial y^2}}\right]$$ Where the second of them is known as the Hessian operator in the 2-dimensional $xy$ -plane. Are here any matrix algebra masters, who will find how to express $D_{\bf{u}\bf{v}}$ in terms of $U$ and $H$ ? If we had defined an operation like $$\left[\matrix{a_{11} && a_{12} \\ a_{21} && a_{22}}\right]* \left[\matrix{b_{11} && b_{12} \\ b_{21} && b_{22}}\right]=a_{11}b_{11}+a_{12}b_{12}+a_{21}b_{21}+a_{22}b_{22}$$ $$A*B=\sum_{i, j} A_{ij}B_{ij},$$ then the formula for mixed directional 2nd order derivative would look like $$D_{\bf{u}\bf{v}}=U^T*H$$ Any ideas how to express the $*$ operation using standard operations on matrices?","First order directional derivative in 2 dimensional -coorinate system is I thought to go further and analyse second order directional derivatives. The formula for general second order mixed directional derivative is I see the formula we get has definitely something to do with the following two matrices: Where the second of them is known as the Hessian operator in the 2-dimensional -plane. Are here any matrix algebra masters, who will find how to express in terms of and ? If we had defined an operation like then the formula for mixed directional 2nd order derivative would look like Any ideas how to express the operation using standard operations on matrices?","xy D_{\bf{u}}=u_1\frac{\partial}{\partial x}+u_2\frac{\partial}{\partial y}  D^2_{\bf{u}\bf{v}}=D_{\bf{v}}\left[u_1\frac{\partial}{\partial x}+u_2\frac{\partial}{\partial y}\right]=u_1D_{\bf{v}}\frac{\partial}{\partial x}+u_2D_{\bf{v}}\frac{\partial}{\partial y}= =u_1(v_1\frac{\partial^2}{\partial x^2}+v_2\frac{\partial^2}{\partial y\partial x})+u_2(v_1\frac{\partial^2}{\partial x\partial y}+v_2\frac{\partial^2}{\partial y^2})= =u_1v_1\frac{\partial^2}{\partial x^2}+u_1v_2\frac{\partial^2}{\partial y\partial x}+u_2v_1\frac{\partial^2}{\partial x\partial y}+u_2v_2\frac{\partial^2}{\partial y^2} U=\left[\matrix{u_1v_1 && u_1v_2\\ u_2v_1 && u_2v_2}\right] H=\left[\matrix{\frac{\partial^2}{\partial x^2} && \frac{\partial^2}{\partial x\partial y} \\ \frac{\partial^2}{\partial y\partial x} && \frac{\partial^2}{\partial y^2}}\right] xy D_{\bf{u}\bf{v}} U H \left[\matrix{a_{11} && a_{12} \\ a_{21} && a_{22}}\right]* \left[\matrix{b_{11} && b_{12} \\ b_{21} && b_{22}}\right]=a_{11}b_{11}+a_{12}b_{12}+a_{21}b_{21}+a_{22}b_{22} A*B=\sum_{i, j} A_{ij}B_{ij}, D_{\bf{u}\bf{v}}=U^T*H *","['multivariable-calculus', 'partial-derivative', 'hessian-matrix']"
35,"If $f:A→\Bbb R^n$ is differentiable at $a$ then there exist $δ>0$ such that $\Biggl|\frac{f(a+tu)-f(a)-B\cdot tu}{|t|}\Biggl|<ε$ for any $t\in(-δ,δ)$",If  is differentiable at  then there exist  such that  for any,"f:A→\Bbb R^n a δ>0 \Biggl|\frac{f(a+tu)-f(a)-B\cdot tu}{|t|}\Biggl|<ε t\in(-δ,δ)","Definition Let $A\subset\Bbb R^m$ and let $f:A\rightarrow\Bbb R^n$ a function and we suppose that $A$ contains a neighborhood of $a$ . So given $u\in\Bbb R^m$ with $u\neq 0$ we define the directional derivative of $f$ at $a$ with respect to the vector $u$ the quantity $$ f'(a;u):=\lim_{t\rightarrow 0}\frac{f(a+tu)-f(a)}t $$ provided the limit exists. Definition Let $A\subset\Bbb R^m$ and let $f:A\rightarrow\Bbb R^n$ a function and we suppose $A$ contains a neighborhood of $a$ . So we say that $f$ is differentiable at $a$ if there is a $n$ by $m$ matrix $B$ such that $$ \frac{f(a+h)-f(a)-B\cdot h}{|h|}\rightarrow0\,\,\,\text{as}\,\,\,h\rightarrow0 $$ The matrix $B$ , which is unique, is called the derivative of $f$ at $a$ ; t is denoted $Df(a)$ . Theorem Let $A\subset\Bbb R^m$ and let $f:A\rightarrow\Bbb R^n$ a function. So if $f$ is differentiable at $a$ then all the directionalderivatives of $f$ at $a$ exists and $$ f'(a;u)=Df(a)\cdot u $$ Proof . See the theorem $5.1$ of the text Analysis on Manifolds by James Munkres. So clearly with the previous definition if $f:A\rightarrow\Bbb R^n$ is derivable at the point $a$ in the direction $u\in\Bbb R^n$ then for any $\epsilon>0$ there exist $\delta_{\epsilon,u}$ such that $$ \Biggl|\frac{f(a+tu)-f(a)}t\Biggl|<\epsilon $$ for any $t\in(-\delta_{\epsilon,u},\delta_{\epsilon,u})$ and so for the completness theorem given $\epsilon>0$ the quantity $$ \delta_\epsilon:=\inf\{\delta_{\epsilon,u}\in\Bbb R^n: u\,\text{is a direction of}\,\Bbb R^n\} $$ is well defined and it is non negative. So I ask if in the case where $f$ is differentiable at $a$ necessarly it must be $\delta_\epsilon>0$ for any $\epsilon>0$ So could someone help me, please?","Definition Let and let a function and we suppose that contains a neighborhood of . So given with we define the directional derivative of at with respect to the vector the quantity provided the limit exists. Definition Let and let a function and we suppose contains a neighborhood of . So we say that is differentiable at if there is a by matrix such that The matrix , which is unique, is called the derivative of at ; t is denoted . Theorem Let and let a function. So if is differentiable at then all the directionalderivatives of at exists and Proof . See the theorem of the text Analysis on Manifolds by James Munkres. So clearly with the previous definition if is derivable at the point in the direction then for any there exist such that for any and so for the completness theorem given the quantity is well defined and it is non negative. So I ask if in the case where is differentiable at necessarly it must be for any So could someone help me, please?","A\subset\Bbb R^m f:A\rightarrow\Bbb R^n A a u\in\Bbb R^m u\neq 0 f a u 
f'(a;u):=\lim_{t\rightarrow 0}\frac{f(a+tu)-f(a)}t
 A\subset\Bbb R^m f:A\rightarrow\Bbb R^n A a f a n m B 
\frac{f(a+h)-f(a)-B\cdot h}{|h|}\rightarrow0\,\,\,\text{as}\,\,\,h\rightarrow0
 B f a Df(a) A\subset\Bbb R^m f:A\rightarrow\Bbb R^n f a f a 
f'(a;u)=Df(a)\cdot u
 5.1 f:A\rightarrow\Bbb R^n a u\in\Bbb R^n \epsilon>0 \delta_{\epsilon,u} 
\Biggl|\frac{f(a+tu)-f(a)}t\Biggl|<\epsilon
 t\in(-\delta_{\epsilon,u},\delta_{\epsilon,u}) \epsilon>0 
\delta_\epsilon:=\inf\{\delta_{\epsilon,u}\in\Bbb R^n: u\,\text{is a direction of}\,\Bbb R^n\}
 f a \delta_\epsilon>0 \epsilon>0","['real-analysis', 'calculus', 'multivariable-calculus', 'derivatives', 'differential-geometry']"
36,Use Stokes' theorem to show this integral relation.,Use Stokes' theorem to show this integral relation.,,"By applying Stokes’ theorem to the vector field $a×F$ , where $a$ is an arbitrary constant vector and $F(r)$ is a vector field, show that $$ \int_{C}dr\times F(r)=\int_{S}(dS\times \nabla)\times F $$ where the curve $C$ bounds the open surface $S$ . Stokes's theorem gives $\int_{S}(\nabla\times (a\times F))\cdot dS=\int_{C}(a\times F)\cdot dr$ . The right hand side is (being $a$ constant, and by permuting triple product) $a\cdot \int_{C}dr\times F(r)$ , while the left hand side, being $a$ constant, is $\int_{S}(a(\nabla\cdot F)-(a\cdot \nabla)F)\cdot dS=a\cdot \int_{S}(\nabla\cdot F)\cdot dS-\int_{S}dS\cdot (a\cdot \nabla)F$ . I wonder if I can isolate the $a\cdot$ on this side like I did on the right hand side to obtain a relation without $a$ . Or if I should use a particular value for $a$ .","By applying Stokes’ theorem to the vector field , where is an arbitrary constant vector and is a vector field, show that where the curve bounds the open surface . Stokes's theorem gives . The right hand side is (being constant, and by permuting triple product) , while the left hand side, being constant, is . I wonder if I can isolate the on this side like I did on the right hand side to obtain a relation without . Or if I should use a particular value for .","a×F a F(r) 
\int_{C}dr\times F(r)=\int_{S}(dS\times \nabla)\times F
 C S \int_{S}(\nabla\times (a\times F))\cdot dS=\int_{C}(a\times F)\cdot dr a a\cdot \int_{C}dr\times F(r) a \int_{S}(a(\nabla\cdot F)-(a\cdot \nabla)F)\cdot dS=a\cdot \int_{S}(\nabla\cdot F)\cdot dS-\int_{S}dS\cdot (a\cdot \nabla)F a\cdot a a","['multivariable-calculus', 'vector-analysis', 'stokes-theorem']"
37,"If $Df(a)$ is non singular and $f \in \mathscr{C}^{1}$, so $f$ is locally one-to-one.","If  is non singular and , so  is locally one-to-one.",Df(a) f \in \mathscr{C}^{1} f,"Let $A$ be a open in $\mathbb{R}^{n}$ . Let $f: A \to \mathbb{R}^{n}$ be of class $\mathscr{C}^{1}$ . If $Df(a)$ is non-singular, there exists an $\alpha>0$ s.t the inequality $$||f(x)-f(y)||\geq \alpha ||x-y||$$ holds for all $x,y$ in some open cube $C(a;\epsilon)$ in some open cube $C(a;\epsilon)$ centered at $a$ . It follows that $f$ is one-to-one on this open cube. My attempt: Let $E=Df(a)$ , so since $\det(Df(a))\not=0$ , we have that there exists one matrix $E^{-1}$ , such that $E^{-1}E=EE^{-1}=I_{n}$ . Let $T: \mathbb{R}^{n}\to \mathbb{R}^{n}$ by $T(x)=Ex$ , so we have that $\forall x,y \in \mathbb{R}^{n}$ , $$ ||x-y||=||E^{-1}(Ex-Ey)||\leq n ||E^{-1}||\cdot ||Ex-Ey||,n \in \mathbb{N} \iff \frac{1}{n||E^{-1}||}||x-y||\leq ||Ex-Ey||$$ Now, since $||E^{-1}||> 0$ and $n\in \mathbb{N}$ , so $2\beta:=\frac{1}{n||E^{-1}||}>0$ . Therefore, $\forall x, y \in \mathbb{R}^{n}$ , we have that $$||Ex-Ey||\geq 2\beta ||x-y||$$ Let $H: A\subset \mathbb{R}^{n} \to \mathbb{R}^{n}$ by $$H(x)=f(x)-Ex$$ since $f \in \mathscr{C}^{1}$ and $Ex \in \mathscr{C}^{1}$ ,  so $H\in \mathscr{C}^{1}$ . Therefore $\forall x \in A: DH(x)=Df(x)-D(Ex)=Df(x)-E$ . In particular, $DH(a)=Df(a)-E=E-E=0 \implies DH(a)=0$ . Claim: Since $H \in \mathscr{C}^{1}$ , so we can choose a $\epsilon>0$ such that $\left|DH(x)\right|<\beta$ for $x \in C=C(a;\epsilon)$ Question: How can I prove claim ? Thanks @ nicomezi and @peek-a-boo for the explain about my claim . EDIT: Since that $H \in \mathscr{C}^{1}$ , we can choose $\epsilon>0$ s.t $|DH(x)|<\beta$ for $x \in C(a;\epsilon)$ . Now, the TVM applied to the $i^{th}$ component function $H$ , tells us gives $x,y \in C(a;\epsilon)$ , there's a $c \in C(a;\epsilon)$ s.t: $$|H_{i}(x)-H_{i}(y)|\leq ||DH(c)||\cdot ||x-y||= \beta ||x-y||$$ Then, $\forall x, y \in C(a;\epsilon)$ , we have $$\begin{align*} \beta||x-y||&\geq & ||H(x)-H(y)||\\ &=&||f(x)-Ex-f(y)+Ey||\\ &\geq& ||Ex-Ey||-||f(x)-f(y)||\\ &\geq& 2\beta ||x-y||-||f(x)-f(y)||\end{align*}$$ Now, let $\beta:=\alpha>0$ , so we have $$||f(x)-f(y)||\geq 2\alpha ||x-y||-\alpha||x-y||=||x-y||$$ So, there exists an $\alpha>0$ s.t $$||f(x)-f(y)||\geq \alpha||x-y||$$ holds for all $x,y \in C(a;\epsilon)$ . Is my prove correct, now? Any suggests? Thanks so much.","Let be a open in . Let be of class . If is non-singular, there exists an s.t the inequality holds for all in some open cube in some open cube centered at . It follows that is one-to-one on this open cube. My attempt: Let , so since , we have that there exists one matrix , such that . Let by , so we have that , Now, since and , so . Therefore, , we have that Let by since and ,  so . Therefore . In particular, . Claim: Since , so we can choose a such that for Question: How can I prove claim ? Thanks @ nicomezi and @peek-a-boo for the explain about my claim . EDIT: Since that , we can choose s.t for . Now, the TVM applied to the component function , tells us gives , there's a s.t: Then, , we have Now, let , so we have So, there exists an s.t holds for all . Is my prove correct, now? Any suggests? Thanks so much.","A \mathbb{R}^{n} f: A \to \mathbb{R}^{n} \mathscr{C}^{1} Df(a) \alpha>0 ||f(x)-f(y)||\geq \alpha ||x-y|| x,y C(a;\epsilon) C(a;\epsilon) a f E=Df(a) \det(Df(a))\not=0 E^{-1} E^{-1}E=EE^{-1}=I_{n} T: \mathbb{R}^{n}\to \mathbb{R}^{n} T(x)=Ex \forall x,y \in \mathbb{R}^{n}  ||x-y||=||E^{-1}(Ex-Ey)||\leq n ||E^{-1}||\cdot ||Ex-Ey||,n \in \mathbb{N} \iff \frac{1}{n||E^{-1}||}||x-y||\leq ||Ex-Ey|| ||E^{-1}||> 0 n\in \mathbb{N} 2\beta:=\frac{1}{n||E^{-1}||}>0 \forall x, y \in \mathbb{R}^{n} ||Ex-Ey||\geq 2\beta ||x-y|| H: A\subset \mathbb{R}^{n} \to \mathbb{R}^{n} H(x)=f(x)-Ex f \in \mathscr{C}^{1} Ex \in \mathscr{C}^{1} H\in \mathscr{C}^{1} \forall x \in A: DH(x)=Df(x)-D(Ex)=Df(x)-E DH(a)=Df(a)-E=E-E=0 \implies DH(a)=0 H \in \mathscr{C}^{1} \epsilon>0 \left|DH(x)\right|<\beta x \in C=C(a;\epsilon) H \in \mathscr{C}^{1} \epsilon>0 |DH(x)|<\beta x \in C(a;\epsilon) i^{th} H x,y \in C(a;\epsilon) c \in C(a;\epsilon) |H_{i}(x)-H_{i}(y)|\leq ||DH(c)||\cdot ||x-y||= \beta ||x-y|| \forall x, y \in C(a;\epsilon) \begin{align*}
\beta||x-y||&\geq & ||H(x)-H(y)||\\
&=&||f(x)-Ex-f(y)+Ey||\\
&\geq& ||Ex-Ey||-||f(x)-f(y)||\\
&\geq& 2\beta ||x-y||-||f(x)-f(y)||\end{align*} \beta:=\alpha>0 ||f(x)-f(y)||\geq 2\alpha ||x-y||-\alpha||x-y||=||x-y|| \alpha>0 ||f(x)-f(y)||\geq \alpha||x-y|| x,y \in C(a;\epsilon)",['real-analysis']
38,"If $f$ is twice differentiable at $a$, prove $f''(a) vw= \lim_{k \rightarrow \infty} \frac{f(a+t_k(v_k+w_k))-f(a+t_kv_k)-f(a+t_kw_k)+f(a)}{t_k^2}$","If  is twice differentiable at , prove",f a f''(a) vw= \lim_{k \rightarrow \infty} \frac{f(a+t_k(v_k+w_k))-f(a+t_kv_k)-f(a+t_kw_k)+f(a)}{t_k^2},"Let $f: U \subset \mathbb{R}^m \rightarrow \mathbb{R}^n$ twice differentiable at the point $a \in U$ .  suppose that $\lim_{k \rightarrow \infty} t_k=0$ in $\mathbb{R}$ and that $\lim_{k \rightarrow \infty} v_k=v$ ; $\lim_{k \rightarrow \infty} w_k=w$ prove that $$f''(a) vw= \lim_{k \rightarrow \infty} \frac{f(a+t_k(v_k+w_k))-f(a+t_kv_k)-f(a+t_kw_k)+f(a)}{t_k^2}$$ We know that since $f$ is differentiable there is a linear transformation $$T_a: \mathbb{R}^m \rightarrow \mathbb{R }^n$$ such that $f(a+h)-f(a)=T_a h + r(h)$ where $\lim_{h \rightarrow \infty} \frac{\|r(h)\|}{\|h\|}=0$ , I don't know how to proceed, any idea please.","Let twice differentiable at the point .  suppose that in and that ; prove that We know that since is differentiable there is a linear transformation such that where , I don't know how to proceed, any idea please.",f: U \subset \mathbb{R}^m \rightarrow \mathbb{R}^n a \in U \lim_{k \rightarrow \infty} t_k=0 \mathbb{R} \lim_{k \rightarrow \infty} v_k=v \lim_{k \rightarrow \infty} w_k=w f''(a) vw= \lim_{k \rightarrow \infty} \frac{f(a+t_k(v_k+w_k))-f(a+t_kv_k)-f(a+t_kw_k)+f(a)}{t_k^2} f T_a: \mathbb{R}^m \rightarrow \mathbb{R }^n f(a+h)-f(a)=T_a h + r(h) \lim_{h \rightarrow \infty} \frac{\|r(h)\|}{\|h\|}=0,"['real-analysis', 'multivariable-calculus', 'derivatives', 'hessian-matrix']"
39,Finding bounds in double integral,Finding bounds in double integral,,"Consider the unit circle $x^2 + y^2 = 1$ and the line $y = 3x$ . I want to compute the area of the portion of the circle enclosed above the line $y = 0$ but to the left of the line $y = 3x$ . In other words, I require $y > 3x$ and $y > 0$ to hold. NOTE: Although the shaded blue area in the image above goes outside of the circle, I only want the area of the portion enclosed inside of the circle. Is there any nice geometric way to solve this problem? I tried to solve it with a double integral, but I can't quite figure out the bounds. I'm happy with any solution. I know that the entire second quadrant has area $\pi/4$ , but figuring out the last bit is difficult. I thought about finding the angle from the line $x = 0$ to $y = 3x$ by drawing a right triangle with side-lengths $3$ and $1$ . This means that the angle is $\theta = \arctan(1/3)$ , but how can I figure out the area from here?","Consider the unit circle and the line . I want to compute the area of the portion of the circle enclosed above the line but to the left of the line . In other words, I require and to hold. NOTE: Although the shaded blue area in the image above goes outside of the circle, I only want the area of the portion enclosed inside of the circle. Is there any nice geometric way to solve this problem? I tried to solve it with a double integral, but I can't quite figure out the bounds. I'm happy with any solution. I know that the entire second quadrant has area , but figuring out the last bit is difficult. I thought about finding the angle from the line to by drawing a right triangle with side-lengths and . This means that the angle is , but how can I figure out the area from here?",x^2 + y^2 = 1 y = 3x y = 0 y = 3x y > 3x y > 0 \pi/4 x = 0 y = 3x 3 1 \theta = \arctan(1/3),"['geometry', 'multivariable-calculus']"
40,Why $\boldsymbol\nabla\times(\boldsymbol\nabla\times\mathbf{u})\neq\boldsymbol\nabla(\boldsymbol\nabla\cdot\mathbf{u})-\nabla^2\mathbf{u}$?,Why ?,\boldsymbol\nabla\times(\boldsymbol\nabla\times\mathbf{u})\neq\boldsymbol\nabla(\boldsymbol\nabla\cdot\mathbf{u})-\nabla^2\mathbf{u},"It is well known that for some vector field $\mathbf{u}$ the following holds: $$ \boldsymbol\nabla\times(\boldsymbol\nabla\times\mathbf{u})=\boldsymbol\nabla(\boldsymbol\nabla\cdot\mathbf{u})-\nabla^2\mathbf{u}.$$ Let's consider the following vector field in cylindrical coordinates with the unit vectors $\hat{\mathbf{r}}, \hat{\boldsymbol\phi}, \hat{\mathbf{z}}$ : $$\mathbf{A}=0\hat{\mathbf{r}}+1\hat{\boldsymbol\phi}+0\hat{\mathbf{z}}.$$ For cylindrical coordinate frame we know that divergence, curl, and Laplacian are written as respectively: $$\boldsymbol\nabla\cdot\mathbf{A} = \frac{1}{r}\frac{\partial(rA_r)}{\partial r} + \frac{1}{r}\frac{\partial A_\phi}{\partial \phi} + \frac{\partial A_z}{\partial z},$$ $$\boldsymbol\nabla\times\mathbf{A}=\left(\frac{1}{r}\frac{\partial A_z}{\partial\phi}-\frac{\partial A_\phi}{\partial z}\right)\hat{\mathbf{r}}+\left(\frac{\partial A_r}{\partial z}-\frac{\partial A_z}{\partial r}\right)\hat{\boldsymbol\phi}+\left(\frac{1}{r}\frac{\partial(rA_\phi)}{\partial r}-\frac{1}{r}\frac{\partial A_r}{\partial \phi}\right)\hat{\mathbf{z}},$$ $$\nabla^2 =\frac{1}{r}\frac{\partial}{\partial r}\left(r\frac{\partial}{\partial r}\right)+\frac{1}{r^2}\frac{\partial^2}{\partial\phi^2}+\frac{\partial^2}{\partial z^2}.$$ On the one hand, for the left side using these formulae we have: $$\boldsymbol\nabla\times\hat{\boldsymbol{\phi}} = \frac{\hat{\mathbf{z}}}{r},$$ and $$\boldsymbol\nabla\times\frac{\hat{\mathbf{z}}}{r}=\frac{\hat{\boldsymbol\phi}}{r^2}.$$ However, on the other hand, for the right side we shall have zeros, because all partials of $1$ are equal to zero. Where am I wrong?","It is well known that for some vector field the following holds: Let's consider the following vector field in cylindrical coordinates with the unit vectors : For cylindrical coordinate frame we know that divergence, curl, and Laplacian are written as respectively: On the one hand, for the left side using these formulae we have: and However, on the other hand, for the right side we shall have zeros, because all partials of are equal to zero. Where am I wrong?","\mathbf{u}  \boldsymbol\nabla\times(\boldsymbol\nabla\times\mathbf{u})=\boldsymbol\nabla(\boldsymbol\nabla\cdot\mathbf{u})-\nabla^2\mathbf{u}. \hat{\mathbf{r}}, \hat{\boldsymbol\phi}, \hat{\mathbf{z}} \mathbf{A}=0\hat{\mathbf{r}}+1\hat{\boldsymbol\phi}+0\hat{\mathbf{z}}. \boldsymbol\nabla\cdot\mathbf{A} = \frac{1}{r}\frac{\partial(rA_r)}{\partial r} + \frac{1}{r}\frac{\partial A_\phi}{\partial \phi} + \frac{\partial A_z}{\partial z}, \boldsymbol\nabla\times\mathbf{A}=\left(\frac{1}{r}\frac{\partial A_z}{\partial\phi}-\frac{\partial A_\phi}{\partial z}\right)\hat{\mathbf{r}}+\left(\frac{\partial A_r}{\partial z}-\frac{\partial A_z}{\partial r}\right)\hat{\boldsymbol\phi}+\left(\frac{1}{r}\frac{\partial(rA_\phi)}{\partial r}-\frac{1}{r}\frac{\partial A_r}{\partial \phi}\right)\hat{\mathbf{z}}, \nabla^2 =\frac{1}{r}\frac{\partial}{\partial r}\left(r\frac{\partial}{\partial r}\right)+\frac{1}{r^2}\frac{\partial^2}{\partial\phi^2}+\frac{\partial^2}{\partial z^2}. \boldsymbol\nabla\times\hat{\boldsymbol{\phi}} = \frac{\hat{\mathbf{z}}}{r}, \boldsymbol\nabla\times\frac{\hat{\mathbf{z}}}{r}=\frac{\hat{\boldsymbol\phi}}{r^2}. 1","['multivariable-calculus', 'vector-analysis']"
41,Calculating the volume under the surface $z = x^2 + y^2$ and above $D$,Calculating the volume under the surface  and above,z = x^2 + y^2 D,"Given: $$D = \left\{(x,y) | 1 \leq x^2 +y^2 \leq 100, ~~ \frac{x \sqrt{3}}{3} \leq y \leq x \sqrt{3} \right\}$$ Calculate the Volume under the surface $$z = x^2 + y^2$$ And above $D$ . My try: $$I = \iint_D x^2 + y^2 \,dx\,dy$$ We can write $D$ as: $$D = \left\{(x,y) | 1 \leq x^2 +y^2 \leq 100, ~~ \frac{\sqrt{3}}{3} \leq \frac{y}{x} \leq  \sqrt{3} \right\}$$ And change to $u,v$ as the following: $$\left\{\begin{matrix}  u = x^2 + y^2 \\ v = \frac{y}{x} \end{matrix}\right.$$ So the set $D$ would be: $$D = \left\{(u,v) | 1 \leq u \leq 100, ~~ \frac{1}{\sqrt{3}} \leq v \leq  \sqrt{3} \right\}$$ Now, because we changed the variables, we need to calculate the Jacobian: $$ J = \frac{ D(x,y)}{D(u,v)} = \frac{1}{\frac{D(u,v)}{D(x,y)}} = \frac{1}{\begin{vmatrix} u_x & u_y\\  v_x & v_y \end{vmatrix}} = \frac{1}{\begin{vmatrix} 2x & 2y\\  -\frac{y}{x^2} & \frac1x \end{vmatrix}} = \frac{1}{2+ 2 (\frac{y}{x})^2}$$ $$J = \frac{1}{2(1+v^2)}$$ And we can calculate the integral as so: $$\iint_{D_{uv}} u \cdot \frac{1}{2} \cdot \frac{1}{1+v^2} \,du\,dv = \frac{1}{2} \int_1^{100} ( \int_{ \frac{1}{\sqrt{3}}}^{\sqrt{3}} \frac{u}{1+v^2} du)dv = \frac{1}{2} \int_1^{100} \frac{1}{1+v^2} ( \frac{ (\sqrt{3})^2 - (\frac{1}{ \sqrt{3}})^2}{2})dv = \frac23 \int_1^{100} \frac{1}{1+v^2} dv = \frac23 ( \arctan{v} |_1^{100}) \approx 0.5169 \dots$$ Some people I talked with got an answer above $10,000$ and my answer is not close at all! So I ask for your help, if you can review my work, I would be so thankful! Thanks for helping!","Given: Calculate the Volume under the surface And above . My try: We can write as: And change to as the following: So the set would be: Now, because we changed the variables, we need to calculate the Jacobian: And we can calculate the integral as so: Some people I talked with got an answer above and my answer is not close at all! So I ask for your help, if you can review my work, I would be so thankful! Thanks for helping!","D = \left\{(x,y) | 1 \leq x^2 +y^2 \leq 100, ~~ \frac{x \sqrt{3}}{3} \leq y \leq x \sqrt{3} \right\} z = x^2 + y^2 D I = \iint_D x^2 + y^2 \,dx\,dy D D = \left\{(x,y) | 1 \leq x^2 +y^2 \leq 100, ~~ \frac{\sqrt{3}}{3} \leq \frac{y}{x} \leq  \sqrt{3} \right\} u,v \left\{\begin{matrix}
 u = x^2 + y^2 \\ v = \frac{y}{x}
\end{matrix}\right. D D = \left\{(u,v) | 1 \leq u \leq 100, ~~ \frac{1}{\sqrt{3}} \leq v \leq  \sqrt{3} \right\}  J = \frac{ D(x,y)}{D(u,v)} = \frac{1}{\frac{D(u,v)}{D(x,y)}} = \frac{1}{\begin{vmatrix}
u_x & u_y\\ 
v_x & v_y
\end{vmatrix}} = \frac{1}{\begin{vmatrix}
2x & 2y\\ 
-\frac{y}{x^2} & \frac1x
\end{vmatrix}} = \frac{1}{2+ 2 (\frac{y}{x})^2} J = \frac{1}{2(1+v^2)} \iint_{D_{uv}} u \cdot \frac{1}{2} \cdot \frac{1}{1+v^2} \,du\,dv = \frac{1}{2} \int_1^{100} ( \int_{ \frac{1}{\sqrt{3}}}^{\sqrt{3}} \frac{u}{1+v^2} du)dv = \frac{1}{2} \int_1^{100} \frac{1}{1+v^2} ( \frac{ (\sqrt{3})^2 - (\frac{1}{ \sqrt{3}})^2}{2})dv = \frac23 \int_1^{100} \frac{1}{1+v^2} dv = \frac23 ( \arctan{v} |_1^{100}) \approx 0.5169 \dots 10,000","['integration', 'multivariable-calculus', 'solution-verification', 'volume', 'multiple-integral']"
42,"Volume of the solid where it is enclosed from $2x + y + z = 4$ and the planes $x = 0$, $y = 0$, $z = 0$","Volume of the solid where it is enclosed from  and the planes , ,",2x + y + z = 4 x = 0 y = 0 z = 0,"Calculate the volume of the solid where it is enclosed from $2x + y + z = 4$ and the planes $x = 0$ , $y = 0$ , $z = 0$ . My approach: For $z = 0$ : \begin{equation*} 2x + y = 4 \implies y = -2x + 4 \end{equation*} and for $y = 0$ : \begin{equation*} 2x = 4 \implies x = 2 \end{equation*} So the integral for the volume are: \begin{equation*}   \int_0^2 \int_0^{-2x +4} \left(4 - 2x - y\right)\,dy \,dx       = \frac{16}{3} \end{equation*} Is my approach correct? If not, can you provide the correct answer?","Calculate the volume of the solid where it is enclosed from and the planes , , . My approach: For : and for : So the integral for the volume are: Is my approach correct? If not, can you provide the correct answer?","2x + y + z = 4 x = 0 y = 0 z = 0 z = 0 \begin{equation*}
2x + y = 4 \implies y = -2x + 4
\end{equation*} y = 0 \begin{equation*}
2x = 4 \implies x = 2
\end{equation*} \begin{equation*}
  \int_0^2 \int_0^{-2x +4} \left(4 - 2x - y\right)\,dy \,dx
      = \frac{16}{3}
\end{equation*}","['integration', 'multivariable-calculus', 'solution-verification', 'volume']"
43,Primal and Dual Solution not same,Primal and Dual Solution not same,,"Suppose, we have following convex optimization problem: $$\ \min_x\psi(x) \  \ s.t. \ \phi(x) \leq 0 $$ We can write the primal problem as: $$\ \min_x\max_{\lambda\geq 0} \psi(x) + \lambda\phi(x) $$ and the dual problem as: $$\ \max_{\lambda\geq 0}\min_x \psi(x) + \lambda\phi(x) $$ Let's say $(\bar x, \bar \lambda)$ is the solution to primal problem and $(x^*, \lambda^*)$ solution to dual problem. Assuming $\psi$ and $\phi$ are not strictly convex, primal solution need not be the same as dual solution i.e. $x^* \neq \bar x$ and $\lambda^* \neq \bar\lambda$ . However strong duality tell us that, $\mathcal L (\bar x, \bar \lambda) = \mathcal L(x^*, \lambda^*)$ . Can you point me to some interesting example for the same or give an intuition? I understand the problem does not have unique saddle point and hence this issue. I still have conceptual doubts, will $(x^*, \lambda^*)$ not satisfy the K.K.T conditions for the original constrained optimization problem? Edit: I am confused because often people talk about methods for getting primal solution from the dual solution. Paper Page 56 (Applying Minimax theorem line) tells that the approximate solution is in the convex hull of dual iterates. Well my question is why not take the last iterate of dual?","Suppose, we have following convex optimization problem: We can write the primal problem as: and the dual problem as: Let's say is the solution to primal problem and solution to dual problem. Assuming and are not strictly convex, primal solution need not be the same as dual solution i.e. and . However strong duality tell us that, . Can you point me to some interesting example for the same or give an intuition? I understand the problem does not have unique saddle point and hence this issue. I still have conceptual doubts, will not satisfy the K.K.T conditions for the original constrained optimization problem? Edit: I am confused because often people talk about methods for getting primal solution from the dual solution. Paper Page 56 (Applying Minimax theorem line) tells that the approximate solution is in the convex hull of dual iterates. Well my question is why not take the last iterate of dual?","\
\min_x\psi(x) \  \ s.t. \ \phi(x) \leq 0
 \
\min_x\max_{\lambda\geq 0} \psi(x) + \lambda\phi(x)
 \
\max_{\lambda\geq 0}\min_x \psi(x) + \lambda\phi(x)
 (\bar x, \bar \lambda) (x^*, \lambda^*) \psi \phi x^* \neq \bar x \lambda^* \neq \bar\lambda \mathcal L (\bar x, \bar \lambda) = \mathcal L(x^*, \lambda^*) (x^*, \lambda^*)","['multivariable-calculus', 'optimization', 'convex-analysis', 'convex-optimization', 'lagrange-multiplier']"
44,Infinite group acting differentiably on continuous space,Infinite group acting differentiably on continuous space,,"I am interested in infinite groups $G$ that act on $\mathbb{R}^n$ , with some informal properties: For each $g \in G$ , the map $x \mapsto gx$ is smooth and differentiable with respect to $x$ . For each $x \in \mathbb{R}^n$ , the map $g \mapsto gx$ is smooth and differentiable with respect to $g$ . Is there a standard concept that is a reasonable match to this or way to formalize this? Should I perhaps be looking at Lie groups?  I confess I've tried learning about them and the examples look like a match but even understanding the definition of a Lie group requires machinery that I am not familiar with. I realize this is specified in a somewhat vague and imprecise way.  I have the sense that my situation falls within some well-studied mathematical concept, but I'm not entirely sure what.","I am interested in infinite groups that act on , with some informal properties: For each , the map is smooth and differentiable with respect to . For each , the map is smooth and differentiable with respect to . Is there a standard concept that is a reasonable match to this or way to formalize this? Should I perhaps be looking at Lie groups?  I confess I've tried learning about them and the examples look like a match but even understanding the definition of a Lie group requires machinery that I am not familiar with. I realize this is specified in a somewhat vague and imprecise way.  I have the sense that my situation falls within some well-studied mathematical concept, but I'm not entirely sure what.",G \mathbb{R}^n g \in G x \mapsto gx x x \in \mathbb{R}^n g \mapsto gx g,"['group-theory', 'multivariable-calculus', 'differential-geometry', 'lie-groups', 'mathematical-modeling']"
45,Derivatives of Polygamma Functions,Derivatives of Polygamma Functions,,"I would like to know if there's a quicker way to verify: $$\partial_z^{n-1}\psi(tz)t^n = \partial_z^n\ln[\Gamma(tz)], \,\,\\ n \in \mathbb{N}^+, t \in \mathbb{C}_+\tag{1}\label{1}$$ That's true for $t=1$ . But what about a complex $t$ ? For complex $t$ , $(1)$ holds for $n=1,2,3$ . These computations were done by hand. I searched any information that would help me on DLMF ( https://dlmf.nist.gov/5.15 ), but nothing relevant was found. Thanks","I would like to know if there's a quicker way to verify: That's true for . But what about a complex ? For complex , holds for . These computations were done by hand. I searched any information that would help me on DLMF ( https://dlmf.nist.gov/5.15 ), but nothing relevant was found. Thanks","\partial_z^{n-1}\psi(tz)t^n = \partial_z^n\ln[\Gamma(tz)], \,\,\\ n \in \mathbb{N}^+, t \in \mathbb{C}_+\tag{1}\label{1} t=1 t t (1) n=1,2,3","['multivariable-calculus', 'special-functions', 'digamma-function', 'polygamma']"
46,"Limit of ${ \lim_{(x,y)\to(0,0)} {(\left| x \right| + \left| y \right|) \ln{(x^2 + y^4)} }}$",Limit of,"{ \lim_{(x,y)\to(0,0)} {(\left| x \right| + \left| y \right|) \ln{(x^2 + y^4)} }}","This question comes from Hubbard's textbook ""Vector Calculus, Linear Algebra, and Differential Forms: A Unified Approach"" (5th edition, exercise 1.5.16b) I wrote a $\epsilon -\delta$ proof to show ${ \lim_{(x,y)\to(0,0)} {(\left| x \right| + \left| y \right|) \ln{(x^2 + y^4)} }} = 0$ , but wanted to ask to check if what I wrote was correct: Suppose we have $\epsilon > 0$ . First, it can be shown (with L'Hoptial for example) that $\lim_{x\to 0} |x| \ln(x^k) = 0$ for even positive integers $k$ . Thus there exists $\delta_{1;k} > 0$ such that $|x| < \delta_{1;k}$ implies $\left| |x| \ln(x^k) \right| = \left| k |x| \ln(x) \right| < \frac{k\epsilon}{6}$ . Therefore: $|x| \ln(x) > - \frac{\epsilon}{6}$ . Secondly, clearly we have $\lim_{x \to 0} {|x| \ln(x^2 + c)} = \lim_{y \to 0} {|y| \ln(y^4 + c)} = 0$ for any $c > 0$ . So there exists $\delta_{2; c}, \delta_{3;c} > 0$ such that: $|x| < \delta_{2;c} \implies \left| |x| \ln(x^2 + c) \right| < \frac{\epsilon}{2}$ . $|y| < \delta_{3;c} \implies \left| |y| \ln(y^4 + c) \right| < \frac{\epsilon}{2}$ . Let $\delta = \min(\epsilon, \delta_{1;2}, \delta_{1;4}, \delta_{2;\epsilon}, \delta_{3;\epsilon})$ . So if $\sqrt{x^2 + y^2} < \delta$ , note that this implies $|x| < \delta$ and $|y| < \delta$ . Then: $$\begin{align} (|x| + |y|) \ln(x^2 + y^4) &= |x| \ln(x^2 + y^4) + |y| \ln(x^2 + y^4) \\ &\leq |x| \ln(x^2 + \epsilon^4) + |y| \ln(\epsilon^2 + y^4) \\ &< \frac{\epsilon}{2} + \frac{\epsilon}{2} = \epsilon \end{align}$$ As $x^2 + y^4 < \epsilon^2 + y^4$ (as $|x| < \delta \leq \epsilon$ ) and $\ln(x)$ is a strictly increasing implies $\ln(x^2 + y^4) < \ln(\epsilon^2 + y^4)$ . Since $|y| \geq 0$ then $|y| \ln(x^2 + y^4) \leq |y| \ln(\epsilon^2 + y^4)$ . Moreover: $$\begin{align} (|x| + |y|) \ln(x^2 + y^4) &= |x| \ln(x^2 + y^4) + |y| \ln(x^2 + y^4) \\ &\geq |x| \ln(x^2) + |y| \ln(y^4) \\ &= 2|x|\ln(x) + 4|y| \ln(y) \\ &> -\frac{2\epsilon}{6} - \frac{4\epsilon}{6} = -\epsilon \end{align}$$ Putting these together we have $\left| (|x| + |y|) \ln(x^2 + y^4) \right| < \epsilon$ . Therefore ${ \lim_{(x,y)\to(0,0)} {(\left| x \right| + \left| y \right|) \ln{(x^2 + y^4)} }} = 0$ .","This question comes from Hubbard's textbook ""Vector Calculus, Linear Algebra, and Differential Forms: A Unified Approach"" (5th edition, exercise 1.5.16b) I wrote a proof to show , but wanted to ask to check if what I wrote was correct: Suppose we have . First, it can be shown (with L'Hoptial for example) that for even positive integers . Thus there exists such that implies . Therefore: . Secondly, clearly we have for any . So there exists such that: . . Let . So if , note that this implies and . Then: As (as ) and is a strictly increasing implies . Since then . Moreover: Putting these together we have . Therefore .","\epsilon -\delta { \lim_{(x,y)\to(0,0)} {(\left| x \right| + \left| y \right|) \ln{(x^2 + y^4)} }} = 0 \epsilon > 0 \lim_{x\to 0} |x| \ln(x^k) = 0 k \delta_{1;k} > 0 |x| < \delta_{1;k} \left| |x| \ln(x^k) \right| = \left| k |x| \ln(x) \right| < \frac{k\epsilon}{6} |x| \ln(x) > - \frac{\epsilon}{6} \lim_{x \to 0} {|x| \ln(x^2 + c)} = \lim_{y \to 0} {|y| \ln(y^4 + c)} = 0 c > 0 \delta_{2; c}, \delta_{3;c} > 0 |x| < \delta_{2;c} \implies \left| |x| \ln(x^2 + c) \right| < \frac{\epsilon}{2} |y| < \delta_{3;c} \implies \left| |y| \ln(y^4 + c) \right| < \frac{\epsilon}{2} \delta = \min(\epsilon, \delta_{1;2}, \delta_{1;4}, \delta_{2;\epsilon}, \delta_{3;\epsilon}) \sqrt{x^2 + y^2} < \delta |x| < \delta |y| < \delta \begin{align} (|x| + |y|) \ln(x^2 + y^4) &= |x| \ln(x^2 + y^4) + |y| \ln(x^2 + y^4) \\ &\leq |x| \ln(x^2 + \epsilon^4) + |y| \ln(\epsilon^2 + y^4) \\ &< \frac{\epsilon}{2} + \frac{\epsilon}{2} = \epsilon \end{align} x^2 + y^4 < \epsilon^2 + y^4 |x| < \delta \leq \epsilon \ln(x) \ln(x^2 + y^4) < \ln(\epsilon^2 + y^4) |y| \geq 0 |y| \ln(x^2 + y^4) \leq |y| \ln(\epsilon^2 + y^4) \begin{align} (|x| + |y|) \ln(x^2 + y^4) &= |x| \ln(x^2 + y^4) + |y| \ln(x^2 + y^4)
\\ &\geq |x| \ln(x^2) + |y| \ln(y^4) \\ &= 2|x|\ln(x) + 4|y| \ln(y) \\ &> -\frac{2\epsilon}{6} - \frac{4\epsilon}{6} = -\epsilon \end{align} \left| (|x| + |y|) \ln(x^2 + y^4) \right| < \epsilon { \lim_{(x,y)\to(0,0)} {(\left| x \right| + \left| y \right|) \ln{(x^2 + y^4)} }} = 0","['limits', 'multivariable-calculus', 'solution-verification']"
47,"Derive a field from gradient expression that is linear in the field variable, $\textrm{grad}\,F \propto aF$","Derive a field from gradient expression that is linear in the field variable,","\textrm{grad}\,F \propto aF","Not a mathematician. I have got a problem of finding by integration a field $F$ from the expression of its gradient. It is a physical problem in continuous space $x,y$ and time $t$ and with real-valued smooth functions. I first show the vanilla case I coped with; please do correct where applicable. The gradient relationship has the simple form $$ \textrm{grad} F =a \{\cos (\omega t), \sin(\omega t)\}   $$ I consider the differential form for the spatial variation: $$ dF = \frac{\partial F}{\partial x} dx + \frac{\partial F}{\partial y} dy $$ Since the gradient components are independent of space, and of $F$ , in my case the formula above becomes the relationship $$ dF = a \cos(\omega t)\, dx + a \sin(\omega t)\,dy $$ I integrate both sides along an arbitrary line on the plane $(x,y)$ and the result is $$ F -F_0 = a \cos(\omega t)\, (x-x_0) + a \sin(\omega t)\,(y-y_0) $$ where $F_0 = F(x_0,y_0)$ , which I can set. The surface of $F$ is a rotating plane. The case that I found difficult is when the gradient depends on $F$ , say as simply as this form $$ \textrm{grad} F =aF \{\cos (\omega t), \sin(\omega t)\}   $$ The same differential form as above becomes $$ dF = aF \cos(\omega t)\, dx + aF \sin(\omega t)\,dy $$ If I tread the same steps as in the vanilla case above, I do not know how to derive a closed form for $F$ any longer. Would someone please show and explain the way to derive $F$ from this second gradient expression?","Not a mathematician. I have got a problem of finding by integration a field from the expression of its gradient. It is a physical problem in continuous space and time and with real-valued smooth functions. I first show the vanilla case I coped with; please do correct where applicable. The gradient relationship has the simple form I consider the differential form for the spatial variation: Since the gradient components are independent of space, and of , in my case the formula above becomes the relationship I integrate both sides along an arbitrary line on the plane and the result is where , which I can set. The surface of is a rotating plane. The case that I found difficult is when the gradient depends on , say as simply as this form The same differential form as above becomes If I tread the same steps as in the vanilla case above, I do not know how to derive a closed form for any longer. Would someone please show and explain the way to derive from this second gradient expression?","F x,y t 
\textrm{grad} F =a \{\cos (\omega t), \sin(\omega t)\}  
 
dF = \frac{\partial F}{\partial x} dx + \frac{\partial F}{\partial y} dy
 F 
dF = a \cos(\omega t)\, dx + a \sin(\omega t)\,dy
 (x,y) 
F -F_0 = a \cos(\omega t)\, (x-x_0) + a \sin(\omega t)\,(y-y_0)
 F_0 = F(x_0,y_0) F F 
\textrm{grad} F =aF \{\cos (\omega t), \sin(\omega t)\}  
 
dF = aF \cos(\omega t)\, dx + aF \sin(\omega t)\,dy
 F F","['multivariable-calculus', 'partial-differential-equations', 'vector-analysis']"
48,"Volume Bound by $z+x^2=1; z+y^2=1, x=0;y=0;z=0$",Volume Bound by,"z+x^2=1; z+y^2=1, x=0;y=0;z=0","I am trying to produce a triple integral that finds the volume underneath the surraces $x^2+z=1$ , $y^2+z=1$ , and is bound by the coordinate planes. On the graph, this looks like two parabaloids orthogonal to each other intersecting along the line $x=y$ . The bounds I choose are $0<z<1-y^2$ , $0<y<x$ , $0<x<1$ . I then solve this integral and multiply it by two (because of symmetry) and get $5/6$ but apparently the answer is $1/2$ according to the book I'm using. I assume I have my bounds incorrect, could someone explain to me why this is?","I am trying to produce a triple integral that finds the volume underneath the surraces , , and is bound by the coordinate planes. On the graph, this looks like two parabaloids orthogonal to each other intersecting along the line . The bounds I choose are , , . I then solve this integral and multiply it by two (because of symmetry) and get but apparently the answer is according to the book I'm using. I assume I have my bounds incorrect, could someone explain to me why this is?",x^2+z=1 y^2+z=1 x=y 0<z<1-y^2 0<y<x 0<x<1 5/6 1/2,"['integration', 'multivariable-calculus', 'volume', 'multiple-integral']"
49,Changing order of integration using chart technique error?,Changing order of integration using chart technique error?,,Consider: $$ \int_0^3 \int_4^{\sqrt{25-z^2}} \int_{-\sqrt{25-y^2-z^2}}^{\sqrt{25-y^2-z^2}} dxdydz$$ i. Clearly sketch the graph of the solid whose volume this triple integral determines. ii. Present all other triple integrals in rectangular coordinates equivalent to the given triple integral but each of a different order than the others.<br> Here is my graph and chart for changing order of integration any help at all in confirming my answers would be great! I also added my work if that helps as well.,Consider: i. Clearly sketch the graph of the solid whose volume this triple integral determines. ii. Present all other triple integrals in rectangular coordinates equivalent to the given triple integral but each of a different order than the others.<br> Here is my graph and chart for changing order of integration any help at all in confirming my answers would be great! I also added my work if that helps as well., \int_0^3 \int_4^{\sqrt{25-z^2}} \int_{-\sqrt{25-y^2-z^2}}^{\sqrt{25-y^2-z^2}} dxdydz,"['integration', 'multivariable-calculus', 'multiple-integral', 'order-of-integration']"
50,Double integral of two periodic functions,Double integral of two periodic functions,,"I'm quite stuck with the following practice question. Suppose $f,g: R \to R$ are continuous $2\pi$ -periodic functions. Let $h(s) =\int_{0}^{2\pi} f(s-t)g(t)  \ dt$ . Prove that $$\int_{0}^{2\pi} h(s)  \ ds = \left( \int_{0}^{2\pi} f(t)\ dt \right) \left( \int_{0}^{2\pi} g(t) dt \right)$$ My Attempt $\begin{equation} \int_{0}^{2\pi} h(s)  \ ds  = \int_{0}^{2\pi}\int_{0}^{2\pi} f(s-t)g(t) \ dt \ ds  \\ = \int_{0}^{2\pi}g(t) [\int_{0}^{2\pi} f(s-t)\ ds] \ dt \end{equation}$ Could someone point out the right direction to go from here?",I'm quite stuck with the following practice question. Suppose are continuous -periodic functions. Let . Prove that My Attempt Could someone point out the right direction to go from here?,"f,g: R \to R 2\pi h(s) =\int_{0}^{2\pi} f(s-t)g(t)  \ dt \int_{0}^{2\pi} h(s)  \ ds = \left( \int_{0}^{2\pi} f(t)\ dt \right) \left( \int_{0}^{2\pi} g(t) dt \right) \begin{equation}
\int_{0}^{2\pi} h(s)  \ ds  = \int_{0}^{2\pi}\int_{0}^{2\pi} f(s-t)g(t) \ dt \ ds  \\
= \int_{0}^{2\pi}g(t) [\int_{0}^{2\pi} f(s-t)\ ds] \ dt
\end{equation}",[]
51,Proving Kepler's 1st Law,Proving Kepler's 1st Law,,"I am currently taking a Multivariable Calculus class, and my professor says there are going to be proofs on the next exam. He says that proving one of the three Kepler's law is going to be on it, but I do not know how to prove Kepler's 1st Law. Kepler's First Law states that When orbiting, the orbited object (i.e. the Sun) is at one of the focus of the elliptical orbit. This is what I started with. $\vec{a}\times\vec{h}=\frac{-GM}{r^2}\vec{u}\times\left(r^2\ \vec{u}\times\vec{u'}\right)=-GM\left[\left(\vec{u}\cdot\vec{u'}\right)\vec{u}-(\vec{u}\cdot\vec{u})\vec{u'}\right]=GM\vec{u'}$ because $\vec{u}\cdot\vec{u'}=0$ and $\vec{u}\cdot\vec{u}=1$ because $\vec{u}$ is a unit vector. I have gotten this far, but I do not know how to continue from here. Can anyone steer me along the right direction and tell me if I have the correct idea?","I am currently taking a Multivariable Calculus class, and my professor says there are going to be proofs on the next exam. He says that proving one of the three Kepler's law is going to be on it, but I do not know how to prove Kepler's 1st Law. Kepler's First Law states that When orbiting, the orbited object (i.e. the Sun) is at one of the focus of the elliptical orbit. This is what I started with. because and because is a unit vector. I have gotten this far, but I do not know how to continue from here. Can anyone steer me along the right direction and tell me if I have the correct idea?",\vec{a}\times\vec{h}=\frac{-GM}{r^2}\vec{u}\times\left(r^2\ \vec{u}\times\vec{u'}\right)=-GM\left[\left(\vec{u}\cdot\vec{u'}\right)\vec{u}-(\vec{u}\cdot\vec{u})\vec{u'}\right]=GM\vec{u'} \vec{u}\cdot\vec{u'}=0 \vec{u}\cdot\vec{u}=1 \vec{u},['multivariable-calculus']
52,LHS where the argument of the function isn't explicit stated (vector equation),LHS where the argument of the function isn't explicit stated (vector equation),,"The Lorentz force is given as $$ \mathbf  F= q\left[\mathbf E(\mathbf r(t),t)+\mathbf v(t)\times \mathbf B(\mathbf r(t),t)\right] \tag 1 $$ where $\mathbf E, \mathbf B:\mathbb R^4\to\mathbb R^3$ are vector fields and $\mathbf r, \mathbf v:\mathbb R\to \mathbb R^3$ are vector-valued functions of one variable, $t\in \mathbb R$ . And $q$ is a constant. Question: In books the argument of $\mathbf F$ is not explicit given, but why? Does it mean it is a constant vector $\mathbf F\in \mathbb R^3$ , i.e. $$ \mathbf F=(F_x,F_y,F_z) \quad ? \tag 2 $$ Or, based on the right hand side, is $\mathbf F$ a vector-valued function, $\mathbf F: \mathbb R\to \mathbb R^3$ , i.e. $$ \mathbf F(t)=\big(F_x(t),F_y(t),F_z(t) \big) \quad ? \tag 3 $$ Or is it maybe a vector field, $\mathbf F: \mathbb R^4\to \mathbb R^3$ , i.e. $$ \mathbf F(\mathbf r(t),t)=\big(F_x(r(t),t)),F_y(r(t),t)),F_z(r(t),t)) \big ) \quad ? \tag 4 $$","The Lorentz force is given as where are vector fields and are vector-valued functions of one variable, . And is a constant. Question: In books the argument of is not explicit given, but why? Does it mean it is a constant vector , i.e. Or, based on the right hand side, is a vector-valued function, , i.e. Or is it maybe a vector field, , i.e.","
\mathbf  F= q\left[\mathbf E(\mathbf r(t),t)+\mathbf v(t)\times \mathbf B(\mathbf r(t),t)\right] \tag 1
 \mathbf E, \mathbf B:\mathbb R^4\to\mathbb R^3 \mathbf r, \mathbf v:\mathbb R\to \mathbb R^3 t\in \mathbb R q \mathbf F \mathbf F\in \mathbb R^3 
\mathbf F=(F_x,F_y,F_z) \quad ? \tag 2
 \mathbf F \mathbf F: \mathbb R\to \mathbb R^3 
\mathbf F(t)=\big(F_x(t),F_y(t),F_z(t) \big) \quad ? \tag 3
 \mathbf F: \mathbb R^4\to \mathbb R^3 
\mathbf F(\mathbf r(t),t)=\big(F_x(r(t),t)),F_y(r(t),t)),F_z(r(t),t)) \big ) \quad ? \tag 4
","['multivariable-calculus', 'functions', 'notation', 'vector-analysis', 'mathematical-physics']"
53,How to minimize this quadratic form?,How to minimize this quadratic form?,,"For real numbers $x_1,\dots, x_{2n}$ , satisfying $\sum_{i=1}^{2n}x_i = 0$ and $\sum_{i=1}^{2n}x_i^2 = 1$ , how do I show that $$ \sum_{1\le i < j\le n}(x_i-x_j)^2 + \sum_{n+1\le i < j\le 2n}(x_i-x_j)^2 + \sum_{i=1}^n\sum_{j=0}^{m-1} (x_i-x_{n+1+(i+j-1)})^2 \ge 2m$$ where the $(i+j-1)$ in the subscript is taken modulo $n$ and $m\le n$ ? A case of equality would be $x_i = \begin{cases}\frac{1}{\sqrt{2n}} & 1\le i\le n \\ -\frac{1}{\sqrt{2n}} & n+1\le i\le 2n\end{cases}$ .","For real numbers , satisfying and , how do I show that where the in the subscript is taken modulo and ? A case of equality would be .","x_1,\dots, x_{2n} \sum_{i=1}^{2n}x_i = 0 \sum_{i=1}^{2n}x_i^2 = 1  \sum_{1\le i < j\le n}(x_i-x_j)^2 + \sum_{n+1\le i < j\le 2n}(x_i-x_j)^2 + \sum_{i=1}^n\sum_{j=0}^{m-1} (x_i-x_{n+1+(i+j-1)})^2 \ge 2m (i+j-1) n m\le n x_i = \begin{cases}\frac{1}{\sqrt{2n}} & 1\le i\le n \\ -\frac{1}{\sqrt{2n}} & n+1\le i\le 2n\end{cases}","['multivariable-calculus', 'inequality', 'optimization', 'quadratic-forms']"
54,A problem about differential of a smooth map,A problem about differential of a smooth map,,"Problem: We have a map $ f: \mathbb{R}^3\rightarrow M(2,R)$ $$f(x,y,z)=exp \begin{pmatrix}     x & y+z \\     y-z & -x \\     \end{pmatrix}$$ Here, we denote the set of all 2-dimensional real square matrices as $M(2,R)$ . Find the points where the differential of $f$ is not injective and draw the graph of this set. What I have done: (You can ignore the following part) Let's denote $\begin{pmatrix} x & y+z \\ y-z & -x \\ \end{pmatrix}$ as $A$ . Then $A^2 = \begin{pmatrix} x^2+y^2-z^2 & 0 \\ 0 & x^2+y^2-z^2 \\ \end{pmatrix} = (x^2+y^2-z^2)I$ Then, we can get the value of $A^n:$ $$  \begin{array}{c|c|c}   &\\\hline   A^0&I&\\   A^1&&A\\   A^2&(x^2+y^2-z^2)I&\\   A^3&&(x^2+y^2-z^2)A\\   A^4&(x^2+y^2-z^2)^2I&\\   A^5&&(x^2+y^2-z^2)^2A\\ \cdots&\cdots&\cdots \end{array}  $$ It is easy for us to calculate: $$\begin{align}exp A  & = I+ \color{red}{A}+\frac{A^2}{2!}+\color{red}{\frac{A^3}{3!}}+\frac{A^4}{4!}+\color{red}{\frac{A^5}{5!}}+\cdots \\   & = I (1 + \frac{(x^2+y^2-z^2)}{2!} + \frac{(x^2+y^2-z^4)^2}{4!} +\cdots )+ \color{red}{A} ( \color{red}{1}+ \color{red}{\frac{(x^2+y^2-z^2)}{3!}} + \color{red}{\frac{(x^2+y^2-z^2)^2}{5!}} +\cdots ) \\  & = I (1 + \frac{(x^2+y^2-z^2)}{2!} + \frac{(x^2+y^2-z^4)^2}{4!} +\cdots )\\ & + \color{red}{\frac{A}{\sqrt{x^2+y^2-z^2}}} ( \color{red}{\sqrt{x^2+y^2-z^2}}+ \color{red}{\frac{(x^2+y^2-z^2)^\frac{3}{2}}{3!}} + \color{red}{\frac{(x^2+y^2-z^2)^\frac{5}{2}}{5!}} +\cdots )  \end{align}$$ When $x^2+y^2-z^2>0$ , $$\begin{align}exp A  & = cosh\sqrt{x^2+y^2-z^2}I + \color{red}{\frac{A}{\sqrt{x^2+y^2-z^2}} sinh \sqrt{x^2+y^2-z^2}} \\ & = \begin{pmatrix}     cosh\sqrt{x^2+y^2-z^2} + \color{red}{\frac{x}{\sqrt{x^2+y^2-z^2}} sinh \sqrt{x^2+y^2-z^2}} & \color{red}{\frac{y+z}{\sqrt{x^2+y^2-z^2}} sinh \sqrt{x^2+y^2-z^2}}  \\     \color{red}{\frac{y-z}{\sqrt{x^2+y^2-z^2}} sinh \sqrt{x^2+y^2-z^2}} & cosh\sqrt{x^2+y^2-z^2} - \color{red}{\frac{x}{\sqrt{x^2+y^2-z^2}} sinh \sqrt{x^2+y^2-z^2}}  \\     \end{pmatrix} \end{align} \tag{1}$$ Similarly, We can calculate that when $x^2+y^2-z^2<0$ , $$exp A= \begin{pmatrix}     cos\sqrt{z^2-(x^2+y^2)} + \color{red}{\frac{x}{\sqrt{z^2-(x^2+y^2)}} sin \sqrt{z^2-(x^2+y^2)}} & \color{red}{\frac{y+z}{\sqrt{z^2-(x^2+y^2)}} sin \sqrt{z^2-(x^2+y^2)}}  \\     \color{red}{\frac{y-z}{\sqrt{z^2-(x^2+y^2)}} sin \sqrt{z^2-(x^2+y^2)}} & cos\sqrt{z^2-(x^2+y^2)} - \color{red}{\frac{x}{\sqrt{z^2-(x^2+y^2)}} sin \sqrt{z^2-(x^2+y^2)}}  \\     \end{pmatrix} \tag{2}$$ When $x^2+y^2-z^2=0$ , $ exp A = \begin{pmatrix} 1+x & y+z\\ y-z & 1-x\end{pmatrix} \tag{3}$ The standard way to solve the problem may be to identify $M(2,R)$ as $\mathbb{R}^4:$ $$\begin{pmatrix} x & z \\ y & w\end{pmatrix} \mapsto (x,y,z,w)$$ then calculate the differential for (1) (2) and (3) and decide when their rank $<3$ . But I found it is hard to calculate. So is to find under which condition rank $<3$ . Any one can give me some ideas or which books deal with related topics?","Problem: We have a map Here, we denote the set of all 2-dimensional real square matrices as . Find the points where the differential of is not injective and draw the graph of this set. What I have done: (You can ignore the following part) Let's denote as . Then Then, we can get the value of It is easy for us to calculate: When , Similarly, We can calculate that when , When , The standard way to solve the problem may be to identify as then calculate the differential for (1) (2) and (3) and decide when their rank . But I found it is hard to calculate. So is to find under which condition rank . Any one can give me some ideas or which books deal with related topics?"," f: \mathbb{R}^3\rightarrow M(2,R) f(x,y,z)=exp \begin{pmatrix}
    x & y+z \\
    y-z & -x \\
    \end{pmatrix} M(2,R) f \begin{pmatrix} x & y+z \\ y-z & -x \\ \end{pmatrix} A A^2 = \begin{pmatrix} x^2+y^2-z^2 & 0 \\ 0 & x^2+y^2-z^2 \\ \end{pmatrix} = (x^2+y^2-z^2)I A^n:  
\begin{array}{c|c|c}
  &\\\hline
  A^0&I&\\
  A^1&&A\\
  A^2&(x^2+y^2-z^2)I&\\
  A^3&&(x^2+y^2-z^2)A\\
  A^4&(x^2+y^2-z^2)^2I&\\
  A^5&&(x^2+y^2-z^2)^2A\\
\cdots&\cdots&\cdots
\end{array}
  \begin{align}exp A 
& = I+ \color{red}{A}+\frac{A^2}{2!}+\color{red}{\frac{A^3}{3!}}+\frac{A^4}{4!}+\color{red}{\frac{A^5}{5!}}+\cdots \\  
& = I (1 + \frac{(x^2+y^2-z^2)}{2!} + \frac{(x^2+y^2-z^4)^2}{4!} +\cdots )+ \color{red}{A} ( \color{red}{1}+ \color{red}{\frac{(x^2+y^2-z^2)}{3!}} + \color{red}{\frac{(x^2+y^2-z^2)^2}{5!}} +\cdots ) \\ 
& = I (1 + \frac{(x^2+y^2-z^2)}{2!} + \frac{(x^2+y^2-z^4)^2}{4!} +\cdots )\\
& + \color{red}{\frac{A}{\sqrt{x^2+y^2-z^2}}} ( \color{red}{\sqrt{x^2+y^2-z^2}}+ \color{red}{\frac{(x^2+y^2-z^2)^\frac{3}{2}}{3!}} + \color{red}{\frac{(x^2+y^2-z^2)^\frac{5}{2}}{5!}} +\cdots ) 
\end{align} x^2+y^2-z^2>0 \begin{align}exp A 
& = cosh\sqrt{x^2+y^2-z^2}I + \color{red}{\frac{A}{\sqrt{x^2+y^2-z^2}} sinh \sqrt{x^2+y^2-z^2}} \\
& = \begin{pmatrix}
    cosh\sqrt{x^2+y^2-z^2} + \color{red}{\frac{x}{\sqrt{x^2+y^2-z^2}} sinh \sqrt{x^2+y^2-z^2}} & \color{red}{\frac{y+z}{\sqrt{x^2+y^2-z^2}} sinh \sqrt{x^2+y^2-z^2}}  \\
    \color{red}{\frac{y-z}{\sqrt{x^2+y^2-z^2}} sinh \sqrt{x^2+y^2-z^2}} & cosh\sqrt{x^2+y^2-z^2} - \color{red}{\frac{x}{\sqrt{x^2+y^2-z^2}} sinh \sqrt{x^2+y^2-z^2}}  \\
    \end{pmatrix}
\end{align} \tag{1} x^2+y^2-z^2<0 exp A= \begin{pmatrix}
    cos\sqrt{z^2-(x^2+y^2)} + \color{red}{\frac{x}{\sqrt{z^2-(x^2+y^2)}} sin \sqrt{z^2-(x^2+y^2)}} & \color{red}{\frac{y+z}{\sqrt{z^2-(x^2+y^2)}} sin \sqrt{z^2-(x^2+y^2)}}  \\
    \color{red}{\frac{y-z}{\sqrt{z^2-(x^2+y^2)}} sin \sqrt{z^2-(x^2+y^2)}} & cos\sqrt{z^2-(x^2+y^2)} - \color{red}{\frac{x}{\sqrt{z^2-(x^2+y^2)}} sin \sqrt{z^2-(x^2+y^2)}}  \\
    \end{pmatrix} \tag{2} x^2+y^2-z^2=0  exp A = \begin{pmatrix} 1+x & y+z\\ y-z & 1-x\end{pmatrix} \tag{3} M(2,R) \mathbb{R}^4: \begin{pmatrix} x & z \\ y & w\end{pmatrix} \mapsto (x,y,z,w) <3 <3","['matrices', 'multivariable-calculus', 'manifolds', 'exponential-function']"
55,Find the volume between $z=\sqrt{x^{2}+y^{2}}$ and $x^2+y^2+z^2=2$ in spherical cordinates,Find the volume between  and  in spherical cordinates,z=\sqrt{x^{2}+y^{2}} x^2+y^2+z^2=2,"I am asking to find the volume of the volume trap above the cone $z=\sqrt{x^{2}+y^{2}}$ and below the sphere $x^2+y^2+z^2=2$ When I checked the solution I noticed that it was writen as $$V=\int_{0}^{2 \pi} \int_{0}^{\frac{\pi}{4}} \int_{0}^{\sqrt{2}} r^{2} \sin \theta \,d r \,d \theta \,d \varphi$$ and my question is why the boundries of $\theta$ is between $0$ to $\frac{\pi}{4}$ and not $0$ to $\pi$ . why $0$ to $\pi$ is wrong? I just can't imagine the scenerio in my head",I am asking to find the volume of the volume trap above the cone and below the sphere When I checked the solution I noticed that it was writen as and my question is why the boundries of is between to and not to . why to is wrong? I just can't imagine the scenerio in my head,"z=\sqrt{x^{2}+y^{2}} x^2+y^2+z^2=2 V=\int_{0}^{2 \pi} \int_{0}^{\frac{\pi}{4}} \int_{0}^{\sqrt{2}} r^{2} \sin \theta \,d r \,d \theta \,d \varphi \theta 0 \frac{\pi}{4} 0 \pi 0 \pi","['integration', 'multivariable-calculus', 'volume', 'spherical-coordinates', 'multiple-integral']"
56,"Double integral on a 2D rotated area : $\iint_D (x+y)^3 (x-y)^2 \,\mathrm{d}x\,\mathrm{d}y$",Double integral on a 2D rotated area :,"\iint_D (x+y)^3 (x-y)^2 \,\mathrm{d}x\,\mathrm{d}y","I was assigned an exercise in which I have to calculate a double integral of a given function, in an area which is made from four lines. $D$ is defined by the relations: $x+y=1$ , $x-y=1$ , $x+y=3$ , $x-y=-1$ . The integral is $$\iint\limits_D (x+y)^3 \cdot (x-y)^2 \,\mathrm{d}x\,\mathrm{d}y$$ I came to the conclusion that the lines form a rotated square, with edges the points $A(0,1)$ $B(1,2)$ $C(2,1)$ $D(1,0)$ . I guess my last step is to calculate the double integral in that area. My problem is, can I split the rotated square in 2 isosceles' triangles with points $ABC$ , $ADC$ and sum those double integrals together, for more ease? If so, do I have to mention a theorem of some sort that I can't remember? Is there another way to calculate the double integral on that area? Thanks in advance for your help!","I was assigned an exercise in which I have to calculate a double integral of a given function, in an area which is made from four lines. is defined by the relations: , , , . The integral is I came to the conclusion that the lines form a rotated square, with edges the points . I guess my last step is to calculate the double integral in that area. My problem is, can I split the rotated square in 2 isosceles' triangles with points , and sum those double integrals together, for more ease? If so, do I have to mention a theorem of some sort that I can't remember? Is there another way to calculate the double integral on that area? Thanks in advance for your help!","D x+y=1 x-y=1 x+y=3 x-y=-1 \iint\limits_D (x+y)^3 \cdot (x-y)^2 \,\mathrm{d}x\,\mathrm{d}y A(0,1) B(1,2) C(2,1) D(1,0) ABC ADC","['integration', 'multivariable-calculus', 'definite-integrals', 'multiple-integral']"
57,Differential of Gauss map,Differential of Gauss map,,"Let $S$ we a regular surface, differential of Gauss map is $\mathrm{d} \mathrm{N}_{\mathrm{p}}: \mathrm{T}_{\mathrm{p}}(\mathrm{S}) \rightarrow \mathrm{T}_{\mathrm{p}}(\mathrm{S})$ . To evaluate the differential at some point, we can choose a curve $\alpha(t) = x(u(t),v(t))$ on $S$ as a image of plane curve $(u(t),v(t))$ under the chart map $\textbf{x}$ . So we have : $$\begin{aligned} d N_{p}\left(\alpha^{\prime}(0)\right) &=d N_{p}\left(\mathbf{x}_{u} u^{\prime}(0)+\mathbf{x}_{v} v^{\prime}(0)\right) \\ &=\left.\frac{d}{d t} N(u(t), v(t))\right|_{t=0} \\ &=N_{u} u^{\prime}(0)+N_{v} v^{\prime}(0) \end{aligned}$$ The first equality and the third one is easy, what I don't understand is the second one, we know in general $df_p(v) = \frac{d}{dt}_{t=0}(f\alpha(t))$ so for here shouldn't it be $\left.\frac{d}{d t} N(x(u(t), v(t)))\right|_{t=0}$ . This is the result from Do Carmo's differential geometry textbook page 142-143. The second question is  the book says in particular $d    N_{p}\left(\mathbf{x}_{u}\right)=N_{u} \text { and } d    N_{p}\left(\mathbf{x}_{v}\right)=N_{v}$ . I don't know how to get it. My idea is to choose plane curve $(u(t),v(t))$ vary only along the u-axis so $N_u =dN_p(x_u)$ in this case.Is my interpretation right?","Let we a regular surface, differential of Gauss map is . To evaluate the differential at some point, we can choose a curve on as a image of plane curve under the chart map . So we have : The first equality and the third one is easy, what I don't understand is the second one, we know in general so for here shouldn't it be . This is the result from Do Carmo's differential geometry textbook page 142-143. The second question is  the book says in particular . I don't know how to get it. My idea is to choose plane curve vary only along the u-axis so in this case.Is my interpretation right?","S \mathrm{d} \mathrm{N}_{\mathrm{p}}: \mathrm{T}_{\mathrm{p}}(\mathrm{S}) \rightarrow \mathrm{T}_{\mathrm{p}}(\mathrm{S}) \alpha(t) = x(u(t),v(t)) S (u(t),v(t)) \textbf{x} \begin{aligned}
d N_{p}\left(\alpha^{\prime}(0)\right) &=d N_{p}\left(\mathbf{x}_{u} u^{\prime}(0)+\mathbf{x}_{v} v^{\prime}(0)\right) \\
&=\left.\frac{d}{d t} N(u(t), v(t))\right|_{t=0} \\
&=N_{u} u^{\prime}(0)+N_{v} v^{\prime}(0)
\end{aligned} df_p(v) = \frac{d}{dt}_{t=0}(f\alpha(t)) \left.\frac{d}{d t} N(x(u(t), v(t)))\right|_{t=0} d
   N_{p}\left(\mathbf{x}_{u}\right)=N_{u} \text { and } d
   N_{p}\left(\mathbf{x}_{v}\right)=N_{v} (u(t),v(t)) N_u =dN_p(x_u)","['real-analysis', 'multivariable-calculus', 'differential-geometry', 'surfaces']"
58,how to use the chain rule in a multivariable function?,how to use the chain rule in a multivariable function?,,"Problem: Let the function $f(x,y)=(x^2+y^2)\sin(x)$ where $x=r^2e^s$ and $y=rs$ Using the chain rule compute $\frac{\partial f}{\partial r}$ and $\frac{\partial f}{\partial s}$ and then compute $\frac{\partial^2 f}{\partial r^2}$ , $\frac{\partial^2 f}{\partial s^2}$ , $\frac{\partial^2 f}{\partial r \partial s}$ and $\frac{\partial^2 f}{\partial s \partial r}$ I do this: Using the chain rule $$\frac{\partial f}{\partial r}=\frac{\partial f}{\partial x}\frac{\partial x}{\partial r}+\frac{\partial f}{\partial y}\frac{\partial f}{\partial r}$$ $$\frac{\partial f}{\partial r}=[(x^2+y^2)\cos(x)+2x\sin(x)]2re^s+2y\sin(x)s$$ $$\frac{\partial f}{\partial s}=\frac{\partial f}{\partial x}\frac{\partial x}{\partial s}+\frac{\partial f}{\partial y}\frac{\partial f}{\partial s}$$ $$\frac{\partial f}{\partial s}= [(x^2+y^2)\cos(x)+2x\sin(x)]e^sr^2+2y\sin(x)r$$ is this right? and I don't know how to compute the second-order partial derivatives, I need to use the chain rule?","Problem: Let the function where and Using the chain rule compute and and then compute , , and I do this: Using the chain rule is this right? and I don't know how to compute the second-order partial derivatives, I need to use the chain rule?","f(x,y)=(x^2+y^2)\sin(x) x=r^2e^s y=rs \frac{\partial f}{\partial r} \frac{\partial f}{\partial s} \frac{\partial^2 f}{\partial r^2} \frac{\partial^2 f}{\partial s^2} \frac{\partial^2 f}{\partial r \partial s} \frac{\partial^2 f}{\partial s \partial r} \frac{\partial f}{\partial r}=\frac{\partial f}{\partial x}\frac{\partial x}{\partial r}+\frac{\partial f}{\partial y}\frac{\partial f}{\partial r} \frac{\partial f}{\partial r}=[(x^2+y^2)\cos(x)+2x\sin(x)]2re^s+2y\sin(x)s \frac{\partial f}{\partial s}=\frac{\partial f}{\partial x}\frac{\partial x}{\partial s}+\frac{\partial f}{\partial y}\frac{\partial f}{\partial s} \frac{\partial f}{\partial s}= [(x^2+y^2)\cos(x)+2x\sin(x)]e^sr^2+2y\sin(x)r","['calculus', 'multivariable-calculus', 'partial-derivative', 'chain-rule']"
59,total differentiability of $\frac{x^3z^4}{(x^2+y^2)(y^2+z^2)}$ if the denominator equals $0$,total differentiability of  if the denominator equals,\frac{x^3z^4}{(x^2+y^2)(y^2+z^2)} 0,"Let $f:\mathbb{R}^3\to\mathbb{R}$ be given by $f(x,y,z)=\frac{x^3z^4}{(x^2+y^2)(y^2+z^2)}$ if the denominator is not equal to $0$ and otherwise by $f(x,y,z)=0$ . Find all the points where $f$ is differentiable. $f$ is surely differentiable when the denominator is not equal to zero so we only need to consider the case that $(x^2+y^2)(y^2+z^2)=0$ . However, even if we can show that the partial derivatives of $f$ are not continuous at those points, that still doesn't imply that $f$ is not differentiable there. How do I continue from here?","Let be given by if the denominator is not equal to and otherwise by . Find all the points where is differentiable. is surely differentiable when the denominator is not equal to zero so we only need to consider the case that . However, even if we can show that the partial derivatives of are not continuous at those points, that still doesn't imply that is not differentiable there. How do I continue from here?","f:\mathbb{R}^3\to\mathbb{R} f(x,y,z)=\frac{x^3z^4}{(x^2+y^2)(y^2+z^2)} 0 f(x,y,z)=0 f f (x^2+y^2)(y^2+z^2)=0 f f",['real-analysis']
60,Solution to quadratic program,Solution to quadratic program,,"Let $ (a_{ij}) $ and $ (b_{ij}) $ be two sequences of real numbers. I'm trying to solve the quadratic program $$ \min_{(a_{ij})} \sum_i \Big(\sum_j a_{ij} \Big)^2 \quad \text{s.t.} \quad \sum_i\sum_j a_{ij}b_{ij} = 1 $$ using Lagrange multipliers, but can't seem to get anywhere. Does anyone know any references for these types of programs? Edit: With $ a = (a_{ij}) $ , the Lagrangian is $$ \mathcal{L}(a, \lambda) = \sum_i \Big(\sum_j a_{ij} \Big)^2 - \lambda \sum_i\sum_j a_{ij}b_{ij} $$ and its partial derivatives are $$ \frac{\partial}{\partial a_{ij}}\mathcal{L}(a, \lambda) = 2\sum_{j'} a_{ij'} - \lambda b_{ij}. $$","Let and be two sequences of real numbers. I'm trying to solve the quadratic program using Lagrange multipliers, but can't seem to get anywhere. Does anyone know any references for these types of programs? Edit: With , the Lagrangian is and its partial derivatives are"," (a_{ij})   (b_{ij})  
\min_{(a_{ij})} \sum_i \Big(\sum_j a_{ij} \Big)^2 \quad \text{s.t.} \quad \sum_i\sum_j a_{ij}b_{ij} = 1
  a = (a_{ij})  
\mathcal{L}(a, \lambda) = \sum_i \Big(\sum_j a_{ij} \Big)^2 - \lambda \sum_i\sum_j a_{ij}b_{ij}
 
\frac{\partial}{\partial a_{ij}}\mathcal{L}(a, \lambda) = 2\sum_{j'} a_{ij'} - \lambda b_{ij}.
","['multivariable-calculus', 'optimization', 'lagrange-multiplier', 'quadratic-programming']"
61,"A specific change of variable, similar to spherical coordinates","A specific change of variable, similar to spherical coordinates",,"Is it possible to get an explicitly formula for the following change of variable (formula for the Jacobian or for the inverse. I would even accept results from mathematica or other software, which I never use and I'm not proficient in). Let me first introduce the following function $\omega : \left\lbrace \begin{aligned} \mathbb{R}^3 & \longrightarrow \ \mathbb{R}\\ \mathbf{k}\; & \longmapsto  \sqrt{\mathbf{k}^2 + m^2} \end{aligned} \right. $ . I'm now considering $$ \int_{\mathbb{R}^{3\times 3}} \frac{ \varphi\big(\omega(\mathbf{k}_1) + \omega(\mathbf{k}_2) +\omega(\mathbf{k}_3), - \mathbf{k}_1 - \mathbf{k}_2 - \mathbf{k}_3 \big)}{ \omega(\mathbf{k}_1)\hspace{1pt} \omega(\mathbf{k}_2) \hspace{1pt} \omega(\mathbf{k}_3)\, \big[ \big(\omega(\mathbf{k}_1) + \omega(\mathbf{k}_2) + \omega(\mathbf{k}_3) \big)^2 - \omega(\mathbf{k}_1+ \mathbf{k}_2 + \mathbf{k}_3)^2 \big]^2}\, d\mathbf{k}_1\,d\mathbf{k}_2\, d\mathbf{k}_3 $$ where $\varphi$ is a function of 4 variables. It seems natural to consider $$ \Phi: \left\lbrace \begin{aligned} 	U \subset \mathbb{R}^9  		& \longrightarrow V \subset \mathbb{R}^3 \times [m,+\infty[^3 \times ]0,\pi[^3 \\ 	\begin{pmatrix} 		\mathbf{k}_1 \\ \mathbf{k}_2 \\ \mathbf{k}_3 	\end{pmatrix}\  		& \longmapsto  	\begin{pmatrix} 		\mathbf{k} \\ \omega_{\mathbf{k}_1} \\ \omega_{\mathbf{k}_2} \\ \omega_{\mathbf{k}_3}\\ \theta_{12} \\ \theta_{23} \\ \theta_{31} 	\end{pmatrix}  	:= \begin{pmatrix} 		\mathbf{k}_1 + \mathbf{k}_2 + \mathbf{k}_3\\ 		\omega(\mathbf{k}_1) \\ \omega(\mathbf{k}_2) \\ \omega(\mathbf{k}_3)\\ 		\arccos \big( \mathbf{k}_1 \cdot \mathbf{k}_2 \big/ \left\lVert\mathbf{k}_1 \right\rVert \left\lVert\mathbf{k}_2\right\rVert \big)\\ 		\arccos\big( \mathbf{k}_2 \cdot \mathbf{k}_3 \big/ \left\lVert\mathbf{k}_2\right\rVert\left\lVert\mathbf{k}_3\right\rVert \big)\\ 		\arccos\big( \mathbf{k}_3 \cdot \mathbf{k}_1 \big/ \left\lVert\mathbf{k}_3\right\rVert \left\lVert\mathbf{k}_1\right\rVert \big) 	\end{pmatrix} 	\end{aligned} \right. $$ where $U:= \left\lbrace (\mathbf{k}_1 , \mathbf{k}_2 , \mathbf{k}_3)\in \mathbb{R}^9,\ \operatorname{det}(\mathbf{k}_1 , \mathbf{k}_2 , \mathbf{k}_3) >0 \right\rbrace$ is just half the space and the image $V$ a little complicated. I'm not entirely sure this a $\mathcal{C}^1$ -diffeomorphism. The Jacobian looks like $$ \begin{vmatrix} 	1 & 0 & 0 & 1 & 0 & 0 & 1 & 0 & 0 \\ 	0 & 1 & 0 & 0 & 1 & 0 & 0 & 1 & 0 \\ 	0 & 0 & 1 & 0 & 0 & 1 & 0 & 0 & 1 \\ 	k_1^1 / \omega_{\mathbf{k}_1} & k_1^2 / \omega_{\mathbf{k}_1} & k_1^3 / \omega_{\mathbf{k}_1} & 0 & 0 & 0 & 0 & 0 & 0\\	 	0 & 0 & 0 & k_2^1 / \omega_{\mathbf{k}_2} & k_2^2 / \omega_{\mathbf{k}_2} & k_2^3 / \omega_{\mathbf{k}_2} & 0 & 0 & 0\\ 	0 & 0 & 0 & 0 & 0 & 0 & k_3^1 / \omega_{\mathbf{k}_3} & k_3^2 / \omega_{\mathbf{k}_3} & k_3^3 / \omega_{\mathbf{k}_3} \\ 	\frac{\cos \theta_{12}\hspace{.8pt} \frac{k_1^1}{\lVert\mathbf{k}_1\rVert}- \frac{k_2^1}{\lVert\mathbf{k}_2\rVert}}{\lVert\mathbf{k}_1\rVert\, \sin \theta_{12}} & '' 	%\frac{\cos \theta_{12}\, \frac{k_1^2}{\norm{\mathbf{k}_1}}- \frac{k_2^2}{\norm{\mathbf{k}_2}}}{\norm{\mathbf{k}_1} \sin \theta_{12}}   	& '' 	% \frac{\cos \theta_{12}\, \frac{k_1^3}{\norm{\mathbf{k}_1}}- \frac{k_2^3}{\norm{\mathbf{k}_2}}}{\norm{\mathbf{k}_1} \sin \theta_{12}}  	& 0 & 0 & 0 & 0 & 0 & 0\\ 	0 & 0 & 0 &	\frac{\cos \theta_{23}\hspace{.8pt} \frac{k_2^1}{\lVert\mathbf{k}_2\rVert}- \frac{k_3^1}{\lVert\mathbf{k}_3\rVert}}{\lVert\mathbf{k}_2\rVert\, \sin \theta_{23}} & '' & '' & 0 & 0 & 0\\ 	0 & 0 & 0 & 0 & 0 & 0 & \frac{\cos \theta_{31}\hspace{.8pt} \frac{k_3^1}{\lVert\mathbf{k}_3\rVert}- \frac{k_1^1}{\lVert\mathbf{k}_1\rVert}}{\lVert\mathbf{k}_3\rVert\, \sin \theta_{31}} & '' & '' 	\end{vmatrix}$$ where $''$ stands for similar and not identical and where I used the convention from physics that $k^i_j$ is the $i^{\text{th}}$ -component of $\mathbf{k}_j$ and not something to the power $i$ .","Is it possible to get an explicitly formula for the following change of variable (formula for the Jacobian or for the inverse. I would even accept results from mathematica or other software, which I never use and I'm not proficient in). Let me first introduce the following function . I'm now considering where is a function of 4 variables. It seems natural to consider where is just half the space and the image a little complicated. I'm not entirely sure this a -diffeomorphism. The Jacobian looks like where stands for similar and not identical and where I used the convention from physics that is the -component of and not something to the power .","\omega : \left\lbrace \begin{aligned} \mathbb{R}^3 & \longrightarrow \ \mathbb{R}\\ \mathbf{k}\; & \longmapsto  \sqrt{\mathbf{k}^2 + m^2} \end{aligned} \right.   \int_{\mathbb{R}^{3\times 3}} \frac{ \varphi\big(\omega(\mathbf{k}_1) + \omega(\mathbf{k}_2) +\omega(\mathbf{k}_3), - \mathbf{k}_1 - \mathbf{k}_2 - \mathbf{k}_3 \big)}{ \omega(\mathbf{k}_1)\hspace{1pt} \omega(\mathbf{k}_2) \hspace{1pt} \omega(\mathbf{k}_3)\, \big[ \big(\omega(\mathbf{k}_1) + \omega(\mathbf{k}_2) + \omega(\mathbf{k}_3) \big)^2 - \omega(\mathbf{k}_1+ \mathbf{k}_2 + \mathbf{k}_3)^2 \big]^2}\, d\mathbf{k}_1\,d\mathbf{k}_2\, d\mathbf{k}_3  \varphi  \Phi: \left\lbrace \begin{aligned}
	U \subset \mathbb{R}^9 
		& \longrightarrow V \subset \mathbb{R}^3 \times [m,+\infty[^3 \times ]0,\pi[^3 \\
	\begin{pmatrix}
		\mathbf{k}_1 \\ \mathbf{k}_2 \\ \mathbf{k}_3
	\end{pmatrix}\ 
		& \longmapsto 
	\begin{pmatrix}
		\mathbf{k} \\ \omega_{\mathbf{k}_1} \\ \omega_{\mathbf{k}_2} \\ \omega_{\mathbf{k}_3}\\ \theta_{12} \\ \theta_{23} \\ \theta_{31}
	\end{pmatrix} 
	:= \begin{pmatrix}
		\mathbf{k}_1 + \mathbf{k}_2 + \mathbf{k}_3\\
		\omega(\mathbf{k}_1) \\ \omega(\mathbf{k}_2) \\ \omega(\mathbf{k}_3)\\
		\arccos \big( \mathbf{k}_1 \cdot \mathbf{k}_2 \big/ \left\lVert\mathbf{k}_1 \right\rVert \left\lVert\mathbf{k}_2\right\rVert \big)\\
		\arccos\big( \mathbf{k}_2 \cdot \mathbf{k}_3 \big/ \left\lVert\mathbf{k}_2\right\rVert\left\lVert\mathbf{k}_3\right\rVert \big)\\
		\arccos\big( \mathbf{k}_3 \cdot \mathbf{k}_1 \big/ \left\lVert\mathbf{k}_3\right\rVert \left\lVert\mathbf{k}_1\right\rVert \big)
	\end{pmatrix}
	\end{aligned} \right.  U:= \left\lbrace (\mathbf{k}_1 , \mathbf{k}_2 , \mathbf{k}_3)\in \mathbb{R}^9,\ \operatorname{det}(\mathbf{k}_1 , \mathbf{k}_2 , \mathbf{k}_3) >0 \right\rbrace V \mathcal{C}^1  \begin{vmatrix}
	1 & 0 & 0 & 1 & 0 & 0 & 1 & 0 & 0 \\
	0 & 1 & 0 & 0 & 1 & 0 & 0 & 1 & 0 \\
	0 & 0 & 1 & 0 & 0 & 1 & 0 & 0 & 1 \\
	k_1^1 / \omega_{\mathbf{k}_1} & k_1^2 / \omega_{\mathbf{k}_1} & k_1^3 / \omega_{\mathbf{k}_1} & 0 & 0 & 0 & 0 & 0 & 0\\	
	0 & 0 & 0 & k_2^1 / \omega_{\mathbf{k}_2} & k_2^2 / \omega_{\mathbf{k}_2} & k_2^3 / \omega_{\mathbf{k}_2} & 0 & 0 & 0\\
	0 & 0 & 0 & 0 & 0 & 0 & k_3^1 / \omega_{\mathbf{k}_3} & k_3^2 / \omega_{\mathbf{k}_3} & k_3^3 / \omega_{\mathbf{k}_3} \\
	\frac{\cos \theta_{12}\hspace{.8pt} \frac{k_1^1}{\lVert\mathbf{k}_1\rVert}- \frac{k_2^1}{\lVert\mathbf{k}_2\rVert}}{\lVert\mathbf{k}_1\rVert\, \sin \theta_{12}} & ''
	%\frac{\cos \theta_{12}\, \frac{k_1^2}{\norm{\mathbf{k}_1}}- \frac{k_2^2}{\norm{\mathbf{k}_2}}}{\norm{\mathbf{k}_1} \sin \theta_{12}}  
	& ''
	% \frac{\cos \theta_{12}\, \frac{k_1^3}{\norm{\mathbf{k}_1}}- \frac{k_2^3}{\norm{\mathbf{k}_2}}}{\norm{\mathbf{k}_1} \sin \theta_{12}} 
	& 0 & 0 & 0 & 0 & 0 & 0\\
	0 & 0 & 0 &	\frac{\cos \theta_{23}\hspace{.8pt} \frac{k_2^1}{\lVert\mathbf{k}_2\rVert}- \frac{k_3^1}{\lVert\mathbf{k}_3\rVert}}{\lVert\mathbf{k}_2\rVert\, \sin \theta_{23}} & '' & '' & 0 & 0 & 0\\
	0 & 0 & 0 & 0 & 0 & 0 & \frac{\cos \theta_{31}\hspace{.8pt} \frac{k_3^1}{\lVert\mathbf{k}_3\rVert}- \frac{k_1^1}{\lVert\mathbf{k}_1\rVert}}{\lVert\mathbf{k}_3\rVert\, \sin \theta_{31}} & '' & ''
	\end{vmatrix} '' k^i_j i^{\text{th}} \mathbf{k}_j i","['calculus', 'multivariable-calculus', 'determinant', 'change-of-variable']"
62,Can I find an inverse Laplace operator?,Can I find an inverse Laplace operator?,,"I am working on a problem, and I ran into the following equation $$\nabla^2f =f_{xx} + f_{yy} + f_{zz} = \frac{12 xy(x^2-y^2)}{(x^2 +y^2)^2}$$ I want to know if there exists a scalar field $f:\mathbb{R}^3 \to \mathbb{R}$ such that the relation above holds, and if so, if there's a way that I can find $f$ explicitly. Intuitively I know that if $f$ exists it isn't unique, since adding a constant to it would also result in a correct solution, but for this case, I'm just interested in finding one example that works. I've basically only tried guessing and checking. I noticed that I could write the expression as $\frac{12 x^5 y}{(x^2 + y^2)^3} - \frac{12 x y^5}{(x^2 + y^2)^3}$ , and I tried integrating each of the fractions separately with respect to $y$ and $x$ respectively to see if I got something similar, but that rapidly turned out to be impractical. I'm not sure if there's a method to inverse a Laplace operator to find and explicit function. Could anyone tell me if there's a way to solve this? Thank you!","I am working on a problem, and I ran into the following equation I want to know if there exists a scalar field such that the relation above holds, and if so, if there's a way that I can find explicitly. Intuitively I know that if exists it isn't unique, since adding a constant to it would also result in a correct solution, but for this case, I'm just interested in finding one example that works. I've basically only tried guessing and checking. I noticed that I could write the expression as , and I tried integrating each of the fractions separately with respect to and respectively to see if I got something similar, but that rapidly turned out to be impractical. I'm not sure if there's a method to inverse a Laplace operator to find and explicit function. Could anyone tell me if there's a way to solve this? Thank you!",\nabla^2f =f_{xx} + f_{yy} + f_{zz} = \frac{12 xy(x^2-y^2)}{(x^2 +y^2)^2} f:\mathbb{R}^3 \to \mathbb{R} f f \frac{12 x^5 y}{(x^2 + y^2)^3} - \frac{12 x y^5}{(x^2 + y^2)^3} y x,"['multivariable-calculus', 'vector-analysis', 'laplacian']"
63,Computing a line integral,Computing a line integral,,"Let $C$ be the curve which obtains from the intersection of the plane $z=x$ and the cylinder $x^{2}+y^{2}=1$ , oriented counterclockwise. If $F$ is a vector field $F=(x,z,2y) \in \mathbb{R}^{3}$ , compute $$I= \oint_{C} F  \cdot dr $$ Hint: Use Stoke's Theorem. I have that, $\nabla \times F= i$ , then $$I= \int_{S} (\nabla \times F) \cdot dS= \int \int_{D} 1dA $$ where $D= \{(x,y)| \quad x^{2} + y^{2} \le 1 \}$ . Is this fine?, I don't know if I am using correctly the theorem.","Let be the curve which obtains from the intersection of the plane and the cylinder , oriented counterclockwise. If is a vector field , compute Hint: Use Stoke's Theorem. I have that, , then where . Is this fine?, I don't know if I am using correctly the theorem.","C z=x x^{2}+y^{2}=1 F F=(x,z,2y) \in \mathbb{R}^{3} I= \oint_{C} F  \cdot dr  \nabla \times F= i I= \int_{S} (\nabla \times F) \cdot dS= \int \int_{D} 1dA  D= \{(x,y)| \quad x^{2} + y^{2} \le 1 \}","['calculus', 'integration', 'multivariable-calculus', 'stokes-theorem']"
64,Applying Stokes Theorem Without Vector Field,Applying Stokes Theorem Without Vector Field,,"Q: Evaluate $\oint_S 4x dx + 9y dy + 3(x^2 +y^2) dz$ where $S$ is the boundary of the surface $z=4-x^2-y^2$ where $x,y,z \ge 0$ , oriented counterclockwise as viewed from above. I am a bit confused with how to do this question since I'm not given a vector field. Do I not apply Stoke's theorem? Any push in the right direction would be greatly appreciated. Edit: I now know that I can express this as a vector field F $=4x$ i $+9y$ j $+3(x^2+y^2)$ k . Using the cross product, I have found curl F $=6y$ i $-6x$ j , and want to evaluate $\oint_S 4x dx + 9y dy + 3(x^2 +y^2) dz$ using Stokes' theorem ( $\iint_S$ curl F $\cdot$ n $dA$ ). However, I am unsure how to calculate n , the unit normal of $S$ which in this case would be the surface $z=4-x^2-y^2$ restricted to $x,y,z \ge 0$ . Does anyone have any pointers?","Q: Evaluate where is the boundary of the surface where , oriented counterclockwise as viewed from above. I am a bit confused with how to do this question since I'm not given a vector field. Do I not apply Stoke's theorem? Any push in the right direction would be greatly appreciated. Edit: I now know that I can express this as a vector field F i j k . Using the cross product, I have found curl F i j , and want to evaluate using Stokes' theorem ( curl F n ). However, I am unsure how to calculate n , the unit normal of which in this case would be the surface restricted to . Does anyone have any pointers?","\oint_S 4x dx + 9y dy + 3(x^2 +y^2) dz S z=4-x^2-y^2 x,y,z \ge 0 =4x +9y +3(x^2+y^2) =6y -6x \oint_S 4x dx + 9y dy + 3(x^2 +y^2) dz \iint_S \cdot dA S z=4-x^2-y^2 x,y,z \ge 0","['multivariable-calculus', 'vector-analysis', 'stokes-theorem']"
65,Chebyshev formula question,Chebyshev formula question,,"I'm stuck with this problem, I want to show that if $f$ is an integrable function then $$\lim_{t\rightarrow \infty}t\mu\{x\in \mathbb{R}^d:|f(x)|>t\}=0.$$ I have the feeling that I should use the Chebyshev's inequality, but I don't know how to use it in this case. Any ideas? Thanks in advance!!","I'm stuck with this problem, I want to show that if is an integrable function then I have the feeling that I should use the Chebyshev's inequality, but I don't know how to use it in this case. Any ideas? Thanks in advance!!",f \lim_{t\rightarrow \infty}t\mu\{x\in \mathbb{R}^d:|f(x)|>t\}=0.,"['calculus', 'multivariable-calculus']"
66,How to prove that the product of two $C^\infty$ functions is also a $C^\infty$ function rigorously?,How to prove that the product of two  functions is also a  function rigorously?,C^\infty C^\infty,"How to prove that the product of two $C^\infty$ functions is also a $C^\infty$ function? I guess it is easy to prove the above fact by induction, but I cannot prove that. Let $f, g$ be $C^\infty$ functions from an open set $U \subset \mathbb{R}^n \to \mathbb{R}$ . Prove that $f g$ is also a $C^\infty$ function from an open set $U \subset \mathbb{R}^n \to \mathbb{R}$ . My attemp is here: $D_i f$ is also a $C^\infty$ function for any $i \in \{1, 2, \cdots, n\}$ . $D_i g$ is also a $C^\infty$ function for any $i \in \{1, 2, \cdots, n\}$ . $D_i (f g) = D_i(f) g + f D_i(g)$ and $D_i f, D_i g$ are $C^\infty$ functions. It is enough to prove that $D_i(f) g$ and $f D_i(g)$ are $C^\infty$ functions. But to prove that $D_i(f) g$ and $f D_i(g)$ are $C^\infty$ functions, we need to prove the product of two $C^\infty$ functions is also a $C^\infty$ function. A circular argument?","How to prove that the product of two functions is also a function? I guess it is easy to prove the above fact by induction, but I cannot prove that. Let be functions from an open set . Prove that is also a function from an open set . My attemp is here: is also a function for any . is also a function for any . and are functions. It is enough to prove that and are functions. But to prove that and are functions, we need to prove the product of two functions is also a function. A circular argument?","C^\infty C^\infty f, g C^\infty U \subset \mathbb{R}^n \to \mathbb{R} f g C^\infty U \subset \mathbb{R}^n \to \mathbb{R} D_i f C^\infty i \in \{1, 2, \cdots, n\} D_i g C^\infty i \in \{1, 2, \cdots, n\} D_i (f g) = D_i(f) g + f D_i(g) D_i f, D_i g C^\infty D_i(f) g f D_i(g) C^\infty D_i(f) g f D_i(g) C^\infty C^\infty C^\infty","['multivariable-calculus', 'derivatives', 'continuity']"
67,Intermediate value theorem for $\mathbb{R}^2$,Intermediate value theorem for,\mathbb{R}^2,"Let's consider the continuous function $$ f:\mathbb{R}\times [a,b]\to\mathbb{R}$$ Such that $f(x,a)>0$ for all $x$ and there exists $x_b$ with $f(x_b,b)\leq 0$ . Then there exists $c\in [a,b]$ such that $f(x,c)\geq 0$ for all $x$ and a real value $x_c$ with $f(x_c,c)=0$ . Intuitively this seems true, but I woludn't know how to prove it. Any hint would be appreciated.","Let's consider the continuous function Such that for all and there exists with . Then there exists such that for all and a real value with . Intuitively this seems true, but I woludn't know how to prove it. Any hint would be appreciated."," f:\mathbb{R}\times [a,b]\to\mathbb{R} f(x,a)>0 x x_b f(x_b,b)\leq 0 c\in [a,b] f(x,c)\geq 0 x x_c f(x_c,c)=0","['real-analysis', 'calculus', 'multivariable-calculus', 'continuity']"
68,"Problem in finding $\delta$ to prove that $\lim_{x\to a}[\lim_{y\to b}f(x,y)]=\lim_{y\to b}[\lim_{x\to a}f(x,y)]=L$",Problem in finding  to prove that,"\delta \lim_{x\to a}[\lim_{y\to b}f(x,y)]=\lim_{y\to b}[\lim_{x\to a}f(x,y)]=L","Theorem: Let $f$ be a vector field. If $\lim_{(x,y)\to(a,b)}f(x,y)=L$ and $1$ D limits $\lim_{x\to a}f(x,y)$ and $\lim_{y\to b}f(x,y)$ exist then, $\lim_{x\to a}[\lim_{y\to b}f(x,y)]=\lim_{y\to b}[\lim_{x\to a}f(x,y)]=L$ . I tried to prove it as follows: Geometrically, let $\lim_{x\to a}f(x,y)=g(y)$ , whence it follows that we fix some $y$ arbitrarily and then approach $a$ along $x$ -axis to get $\lim_{x\to a}f(x,y)$ . Similarly for the other $1$ D limit.  Then since the limit $\lim_{(x,y)\to(a,b)}f(x,y)=L$ exists, it doesn't matter how we approach $(a,b)$ and hence the result is verified. Is my interpretation correct? Now coming to the proof: Let $\lim_{y\to b}f(x,y)=h(x)$ . Since $\lim_{(x,y)\to(a,b)}f(x,y)=L$ , it follows that $\forall \epsilon_1 \gt 0, \exists \delta_1 \gt 0$ such that if $0\lt \sqrt{(x-a)^2+(y-b)^2}\lt \delta_1$ , then $||f(x,y)-L||\lt \epsilon_1 \tag{1}$ Since $\lim_{x\to a}f(x,y)=g(y)$ , it follows that $\forall \epsilon _2\gt 0, \exists \delta_2\gt 0$ such that if $0\lt |x-a|\lt \delta_2$ , then $||f(x,y)-g(y)||\lt \epsilon_2 \tag{2}$ Similarly, $\forall \epsilon _3\gt 0, \exists \delta_3\gt 0$ such that if $0\lt |y-b|\lt \delta_3$ , then $||f(x,y)-h(x)||\lt \epsilon_3\tag {3}$ From (2)& (3), it follows that $\require{enclose}      \enclose{horizontalstrike}{||g(y)-h(x)||\le ||f(x,y)-g(y)||+||f(x,y)-h(x)||\lt\epsilon_2+\epsilon_3}$ , whenever $\require{enclose}      \enclose{horizontalstrike}{0\lt |y-b|\lt \delta_3}$ and $\require{enclose}      \enclose{horizontalstrike}{0\lt |x-a|\lt \delta_2}$ That is by taking $\require{enclose}      \enclose{horizontalstrike}{\epsilon_2=\epsilon_3=\epsilon/2}$ , we have $\require{enclose}      \enclose{horizontalstrike}{||g(y)-h(x)||\lt\epsilon}$ , whenever $\require{enclose}      \enclose{horizontalstrike}{0\lt \sqrt{(x-a)^2+(y-b)^2} \lt  \sqrt{\delta_2^2+\delta_3^2}}$ Since $\lim_{x\to a}[\lim_{y\to b}f(x,y)]=\lim_{x\to a}h(x)$ , consider $||h(x)-L||\lt||h(x)-f(x,y)||+||f(x,y)-L|\lt \epsilon_3+\epsilon_1$ , whenever $0\lt |y-b|\lt \delta_3$ and $0\lt \sqrt{(x-a)^2+(y-b)^2}\lt \delta_1$ whence by taking $\epsilon_3=\epsilon_1=\epsilon/2$ , it would follow that $||h(x)-L||\lt \epsilon$ whenever $0\lt \sqrt{(x-a)^2+(y-b)^2}\lt \delta$ and similarly for $g(y)$ . But the problem is how to get this $\delta$ from $(1)$ and $(3)$ ? I tried the following here: since $0\lt |y-b|\lt \sqrt{(x-a)^2+(y-b)^2}$ , taking $\delta=\min\{\delta_1,\delta_3\}$ will cure the problem. Is this correct? Please help. Thanks for your time.","Theorem: Let be a vector field. If and D limits and exist then, . I tried to prove it as follows: Geometrically, let , whence it follows that we fix some arbitrarily and then approach along -axis to get . Similarly for the other D limit.  Then since the limit exists, it doesn't matter how we approach and hence the result is verified. Is my interpretation correct? Now coming to the proof: Let . Since , it follows that such that if , then Since , it follows that such that if , then Similarly, such that if , then From (2)& (3), it follows that , whenever and That is by taking , we have , whenever Since , consider , whenever and whence by taking , it would follow that whenever and similarly for . But the problem is how to get this from and ? I tried the following here: since , taking will cure the problem. Is this correct? Please help. Thanks for your time.","f \lim_{(x,y)\to(a,b)}f(x,y)=L 1 \lim_{x\to a}f(x,y) \lim_{y\to b}f(x,y) \lim_{x\to a}[\lim_{y\to b}f(x,y)]=\lim_{y\to b}[\lim_{x\to a}f(x,y)]=L \lim_{x\to a}f(x,y)=g(y) y a x \lim_{x\to a}f(x,y) 1 \lim_{(x,y)\to(a,b)}f(x,y)=L (a,b) \lim_{y\to b}f(x,y)=h(x) \lim_{(x,y)\to(a,b)}f(x,y)=L \forall \epsilon_1 \gt 0, \exists \delta_1 \gt 0 0\lt \sqrt{(x-a)^2+(y-b)^2}\lt \delta_1 ||f(x,y)-L||\lt \epsilon_1 \tag{1} \lim_{x\to a}f(x,y)=g(y) \forall \epsilon _2\gt 0, \exists \delta_2\gt 0 0\lt |x-a|\lt \delta_2 ||f(x,y)-g(y)||\lt \epsilon_2 \tag{2} \forall \epsilon _3\gt 0, \exists \delta_3\gt 0 0\lt |y-b|\lt \delta_3 ||f(x,y)-h(x)||\lt \epsilon_3\tag {3} \require{enclose}
     \enclose{horizontalstrike}{||g(y)-h(x)||\le ||f(x,y)-g(y)||+||f(x,y)-h(x)||\lt\epsilon_2+\epsilon_3} \require{enclose}
     \enclose{horizontalstrike}{0\lt |y-b|\lt \delta_3} \require{enclose}
     \enclose{horizontalstrike}{0\lt |x-a|\lt \delta_2} \require{enclose}
     \enclose{horizontalstrike}{\epsilon_2=\epsilon_3=\epsilon/2} \require{enclose}
     \enclose{horizontalstrike}{||g(y)-h(x)||\lt\epsilon} \require{enclose}
     \enclose{horizontalstrike}{0\lt \sqrt{(x-a)^2+(y-b)^2} \lt  \sqrt{\delta_2^2+\delta_3^2}} \lim_{x\to a}[\lim_{y\to b}f(x,y)]=\lim_{x\to a}h(x) ||h(x)-L||\lt||h(x)-f(x,y)||+||f(x,y)-L|\lt \epsilon_3+\epsilon_1 0\lt |y-b|\lt \delta_3 0\lt \sqrt{(x-a)^2+(y-b)^2}\lt \delta_1 \epsilon_3=\epsilon_1=\epsilon/2 ||h(x)-L||\lt \epsilon 0\lt \sqrt{(x-a)^2+(y-b)^2}\lt \delta g(y) \delta (1) (3) 0\lt |y-b|\lt \sqrt{(x-a)^2+(y-b)^2} \delta=\min\{\delta_1,\delta_3\}","['real-analysis', 'calculus', 'limits', 'multivariable-calculus', 'solution-verification']"
69,Is an infinitesimally small portion of a surface essentially a 2-D area? (Surface Integrals),Is an infinitesimally small portion of a surface essentially a 2-D area? (Surface Integrals),,"I'm trying to derive the equation for surface flux $$\iint_{S}\vec F\cdot \hat n \;dS $$ So far, I understand that if we consider the vector field going through smooth surface. Then the flux on a small piece of the surface, or surface element, $\Delta S$ , is  given by the contribution of $\vec F$ in the direction of the unit normal $\hat n$ , times the piece $\Delta S$ $$\vec F\cdot \hat n \Delta s$$ Stewart's Calculus then suggests that this portion $\Delta S$ is essentially $\Delta A$ , a 2-D area. My question is if an infinitesimally small portion of a surface is essentially a 2-D area somewhat like a plane? Thanks.","I'm trying to derive the equation for surface flux So far, I understand that if we consider the vector field going through smooth surface. Then the flux on a small piece of the surface, or surface element, , is  given by the contribution of in the direction of the unit normal , times the piece Stewart's Calculus then suggests that this portion is essentially , a 2-D area. My question is if an infinitesimally small portion of a surface is essentially a 2-D area somewhat like a plane? Thanks.",\iint_{S}\vec F\cdot \hat n \;dS  \Delta S \vec F \hat n \Delta S \vec F\cdot \hat n \Delta s \Delta S \Delta A,"['calculus', 'integration', 'multivariable-calculus']"
70,Surface-Curl integral questions,Surface-Curl integral questions,,"Good morning, I have some questions about a surface integral with curl. The exercise is the following: Be $(\Sigma, \omega)$ an oriented surface with boundary where $$\Sigma = \{(x, y, z): x^2 + y^2 = z^2+1 ,\ -1 \leq z \leq 3\}$$ Calculate $$ \int\int_{\Sigma}\langle \text{rot}F, \omega\rangle \text{d}\sigma$$ where $$ F(x, y, z) = -\dfrac{1}{3}(y, x, z)$$ and $$\omega(0, 1, 0) = (0, 1, 0)$$ Then he asks to verify the result also by applying Stokes (later). Some details on the procedure Well first of all it's not a big deal to find out that $$\text{rot}F = \dfrac{1}{3}(1, 1, 1)$$ Then we have $$\int\int_{\Sigma} \langle \text{rot}F, \omega\rangle \text{d}\sigma = \dfrac{1}{3}\int\int_{\Sigma} \sum_{i = 1}^3 \omega_i \text{d}\sigma$$ Where $\omega = (\omega_1, \omega_2, \omega_3)$ . A parametrisation for $\Sigma$ is given by $$\phi:[0, 2\pi) \times [-1, 3] \to \Sigma$$ where $$\phi(\theta, z) = (\sqrt{z^2+1}\cos\theta, \sqrt{z^2+1}\sin\theta, z)$$ In particular we find that the Jacobian is $$ \begin{pmatrix} -\sqrt{z^2+1}\sin\theta & \dfrac{z}{\sqrt{z^2+1}}\cos\theta \\ \sqrt{z^2+1}\cos\theta & \dfrac{z}{\sqrt{z^2+1}}\sin\theta \\ 0 & 1 \end{pmatrix} $$ And its rank is two. Thence: $$\dfrac{\partial \phi}{\partial \theta} \wedge \dfrac{\partial \phi}{\partial z} = (\sqrt{z^2+1}\cos\theta, \sqrt{z^2+1}\sin\theta, -z)$$ Now: $$\omega(0, 1, 0) = \omega(\phi(\pi/2, 0)) = \dfrac{\dfrac{\partial \phi}{\partial \theta} \wedge \dfrac{\partial \phi(\pi/2, 0)}{\partial z}}{||\dfrac{\partial \phi}{\partial \theta} \wedge \dfrac{\partial \phi(\pi/2, 0)}{\partial z}||} = (0, 1, 0)$$ He then now says that $\phi$ is compatible with $\omega$ hence the integral is $$\dfrac{1}{3}\int \int_{[0, 2\pi]\times [-1, 3]}\left( \sqrt{z^2+1}\cos\theta + \sqrt{z^2+1}\sin\theta - z)\right) \text{d}\theta\text{d}z = -\dfrac{1}{3}\int\int_{\ldots}z \text{d}\theta\text{d}z = -\dfrac{8\pi}{3}$$ Now my questions It's all clear until we need to calculate the norm of the cult of $\phi$ , then its blackout. 1) I found NOWHERE that the compatibility between $\omega$ and $\omega(\phi)$ has to be verified through the ration between the cult of $\phi$ and its norm. So why do we have to do this? 2) Once we verified the compatibility... then what? I mean I do not need to know that $\omega(0, 1, 0) = \omega(\phi(\pi/2, 0)) = (0, 1, 0)$ do I? So why do I have to do this? Thank you very much for your time, those are really critical points for me to understand... Updates I understood that the proof of the compatibility is irrelevant for the exercise. So it remains the first question: why does that method tell me that they are compatible?","Good morning, I have some questions about a surface integral with curl. The exercise is the following: Be an oriented surface with boundary where Calculate where and Then he asks to verify the result also by applying Stokes (later). Some details on the procedure Well first of all it's not a big deal to find out that Then we have Where . A parametrisation for is given by where In particular we find that the Jacobian is And its rank is two. Thence: Now: He then now says that is compatible with hence the integral is Now my questions It's all clear until we need to calculate the norm of the cult of , then its blackout. 1) I found NOWHERE that the compatibility between and has to be verified through the ration between the cult of and its norm. So why do we have to do this? 2) Once we verified the compatibility... then what? I mean I do not need to know that do I? So why do I have to do this? Thank you very much for your time, those are really critical points for me to understand... Updates I understood that the proof of the compatibility is irrelevant for the exercise. So it remains the first question: why does that method tell me that they are compatible?","(\Sigma, \omega) \Sigma = \{(x, y, z): x^2 + y^2 = z^2+1 ,\ -1 \leq z \leq 3\}  \int\int_{\Sigma}\langle \text{rot}F, \omega\rangle \text{d}\sigma  F(x, y, z) = -\dfrac{1}{3}(y, x, z) \omega(0, 1, 0) = (0, 1, 0) \text{rot}F = \dfrac{1}{3}(1, 1, 1) \int\int_{\Sigma} \langle \text{rot}F, \omega\rangle \text{d}\sigma = \dfrac{1}{3}\int\int_{\Sigma} \sum_{i = 1}^3 \omega_i \text{d}\sigma \omega = (\omega_1, \omega_2, \omega_3) \Sigma \phi:[0, 2\pi) \times [-1, 3] \to \Sigma \phi(\theta, z) = (\sqrt{z^2+1}\cos\theta, \sqrt{z^2+1}\sin\theta, z) 
\begin{pmatrix}
-\sqrt{z^2+1}\sin\theta & \dfrac{z}{\sqrt{z^2+1}}\cos\theta \\
\sqrt{z^2+1}\cos\theta & \dfrac{z}{\sqrt{z^2+1}}\sin\theta \\
0 & 1
\end{pmatrix}
 \dfrac{\partial \phi}{\partial \theta} \wedge \dfrac{\partial \phi}{\partial z} = (\sqrt{z^2+1}\cos\theta, \sqrt{z^2+1}\sin\theta, -z) \omega(0, 1, 0) = \omega(\phi(\pi/2, 0)) = \dfrac{\dfrac{\partial \phi}{\partial \theta} \wedge \dfrac{\partial \phi(\pi/2, 0)}{\partial z}}{||\dfrac{\partial \phi}{\partial \theta} \wedge \dfrac{\partial \phi(\pi/2, 0)}{\partial z}||} = (0, 1, 0) \phi \omega \dfrac{1}{3}\int \int_{[0, 2\pi]\times [-1, 3]}\left( \sqrt{z^2+1}\cos\theta + \sqrt{z^2+1}\sin\theta - z)\right) \text{d}\theta\text{d}z = -\dfrac{1}{3}\int\int_{\ldots}z \text{d}\theta\text{d}z = -\dfrac{8\pi}{3} \phi \omega \omega(\phi) \phi \omega(0, 1, 0) = \omega(\phi(\pi/2, 0)) = (0, 1, 0)","['integration', 'multivariable-calculus', 'surface-integrals', 'parametrization', 'curl']"
71,Visualizing the total differential,Visualizing the total differential,,"I'm trying to convince myself of the fact that $d z=\frac{\partial f}{\partial x} d x+\frac{\partial f}{\partial y} d y$ by looking at this picture (i.e. $dz$ should equal the sum of the two segments in red), but can't seem to do it. I guess I can go the difficult road, trying to compute $dz$ by using the Pythagorean formula on the diagonal of the square on top of the cube and the diagonal of the yellow parallelogram. But is there an easier way?","I'm trying to convince myself of the fact that by looking at this picture (i.e. should equal the sum of the two segments in red), but can't seem to do it. I guess I can go the difficult road, trying to compute by using the Pythagorean formula on the diagonal of the square on top of the cube and the diagonal of the yellow parallelogram. But is there an easier way?",d z=\frac{\partial f}{\partial x} d x+\frac{\partial f}{\partial y} d y dz dz,"['calculus', 'multivariable-calculus', 'derivatives']"
72,"What happens to differentials like $dA$, $dV$, $ds$, and $dS$ under a change of variables?","What happens to differentials like , , , and  under a change of variables?",dA dV ds dS,"Motivating example I have noticed that the derivation of the moment of inertia of a solid ellipsoid uses a change of variables to transform it into a unit sphere; then the unit sphere's MoI gets scaled by the Jacobian determinant. I wondered if I could use a similar technique to get the perimeter of an ellipse. (My ultimate goal is to compute the moment of inertia of a hollow ellipsoid, and I reason that if I can do an arc integral along an ellipse, then I can introduce another parameter to do a surface integral over the ellipsoid.) The ellipse is defined by $$\frac{x^2}{a^2} + \frac{y^2}{b^2} = 1$$ and its arc length is $$\int_C ds$$ where $C$ is the ellipse itself. Now, the ellipse can be transformed into the unit circle if we assert $$x = au \\ y = vb \\ \implies\\ u^2 + v^2 = 1$$ This transformation has $$\left\vert J\right\vert = \left\vert \begin{matrix} a & 0 \\ 0 & b \\ \end{matrix}\right\vert = ab$$ And under this transformation, $$\int_C ds = \int_{C'} \left\vert J\right\vert \,ds' = ab \int_{C'} ds'$$ But $C'$ is just the transformed ellipse, which is the unit circle! So this is just $ab$ times the circumference of the unit circle, and the perimeter is simply $$2\pi a b$$ I know this is wrong , so I thought that I need to transform $ds$ as well. I tried $$ds = \sqrt{dx^2 + dy^2} = \sqrt{(a\,du)^2 +(b\,dv)^2}$$ which doesn't really get me anywhere, as I expected, given that there isn't a simple formula for the perimeter of an ellipse. Nonetheless, is this kind of manipulation correct in principle? If so, why isn't it possible to derive a closed-form expression for the perimeter of an ellipse from this? How come transforming $ds$ doesn't work here, but transforming $dV$ using the Jacobian in the above ellipsoid example works just fine? Is it because the moment of inertia problem considers a hollow ellipsoid, which would be analogous to integrating over the area of an ellipse (not its perimeter)? The 3D analogue of the perimeter would be the surface area, and a surface integral over the ellipsoid is required to find the moment of inertia of a hollow ellipsoid—I assume this latter quantity has no closed-form expression, either? My central question In general, what happens to ""geometric"" differentials like $dA$ , $dV$ , $ds$ , and $dS$ under a change of variables?","Motivating example I have noticed that the derivation of the moment of inertia of a solid ellipsoid uses a change of variables to transform it into a unit sphere; then the unit sphere's MoI gets scaled by the Jacobian determinant. I wondered if I could use a similar technique to get the perimeter of an ellipse. (My ultimate goal is to compute the moment of inertia of a hollow ellipsoid, and I reason that if I can do an arc integral along an ellipse, then I can introduce another parameter to do a surface integral over the ellipsoid.) The ellipse is defined by and its arc length is where is the ellipse itself. Now, the ellipse can be transformed into the unit circle if we assert This transformation has And under this transformation, But is just the transformed ellipse, which is the unit circle! So this is just times the circumference of the unit circle, and the perimeter is simply I know this is wrong , so I thought that I need to transform as well. I tried which doesn't really get me anywhere, as I expected, given that there isn't a simple formula for the perimeter of an ellipse. Nonetheless, is this kind of manipulation correct in principle? If so, why isn't it possible to derive a closed-form expression for the perimeter of an ellipse from this? How come transforming doesn't work here, but transforming using the Jacobian in the above ellipsoid example works just fine? Is it because the moment of inertia problem considers a hollow ellipsoid, which would be analogous to integrating over the area of an ellipse (not its perimeter)? The 3D analogue of the perimeter would be the surface area, and a surface integral over the ellipsoid is required to find the moment of inertia of a hollow ellipsoid—I assume this latter quantity has no closed-form expression, either? My central question In general, what happens to ""geometric"" differentials like , , , and under a change of variables?","\frac{x^2}{a^2} + \frac{y^2}{b^2} = 1 \int_C ds C x = au \\ y = vb \\ \implies\\ u^2 + v^2 = 1 \left\vert J\right\vert =
\left\vert
\begin{matrix}
a & 0 \\ 0 & b \\
\end{matrix}\right\vert
= ab \int_C ds = \int_{C'} \left\vert J\right\vert \,ds' = ab \int_{C'} ds' C' ab 2\pi a b ds ds = \sqrt{dx^2 + dy^2} = \sqrt{(a\,du)^2 +(b\,dv)^2} ds dV dA dV ds dS",['multivariable-calculus']
73,"Let $F=\langle xy^2, 3z-xy^2, 4y-x^2y\rangle$ Find the maximum value of the line integral of $F$ over a simply closed curve C in the plane $x+y+z=1$",Let  Find the maximum value of the line integral of  over a simply closed curve C in the plane,"F=\langle xy^2, 3z-xy^2, 4y-x^2y\rangle F x+y+z=1","Let $F=(xy^2, 3z-xy^2, 4y-x^2y)$ . Find the maximum value of the line integral of F over a simply closed curve $C$ in the plane $x+y+z=1$ . What is the curve that maximises it? I am a bit confused on how to approach this question. I tried parameterising $C$ which clearly didn't help as I needed two parameters to do so. I then tried using Stokes theorem. I took out the dot product of the curl and the normal vector which came out to be $1-x^2+2x-y^2-2xy $ but I don't know how to proceed further. What substitution would we make?",Let . Find the maximum value of the line integral of F over a simply closed curve in the plane . What is the curve that maximises it? I am a bit confused on how to approach this question. I tried parameterising which clearly didn't help as I needed two parameters to do so. I then tried using Stokes theorem. I took out the dot product of the curl and the normal vector which came out to be but I don't know how to proceed further. What substitution would we make?,"F=(xy^2, 3z-xy^2, 4y-x^2y) C x+y+z=1 C 1-x^2+2x-y^2-2xy ","['multivariable-calculus', 'line-integrals', 'stokes-theorem']"
74,Improper integral with two variables,Improper integral with two variables,,"I've got the following problem: Proof or disproof: there is a g $\in C(\mathbb{R^2})$ so that a $h(y) := \int_{-\infty}^{\infty}g(x,y)dx$ for every $y \in \mathbb{R}$ exists, but is not continuous. A hint suggests I should consider that for every $f \in C(\mathbb{R})$ that is improperly integrable and $y>0$ it applies that: $\int_{-\infty}^{\infty}yf(yx)dx = \int_{-\infty}^{\infty}f(x)dx$ I honestly don't quiet know how to even start solving this problem. I think I don't have the right properties for improper integrals with two variables in mind to work with them. Does anyone have an Idea?","I've got the following problem: Proof or disproof: there is a g so that a for every exists, but is not continuous. A hint suggests I should consider that for every that is improperly integrable and it applies that: I honestly don't quiet know how to even start solving this problem. I think I don't have the right properties for improper integrals with two variables in mind to work with them. Does anyone have an Idea?","\in C(\mathbb{R^2}) h(y) := \int_{-\infty}^{\infty}g(x,y)dx y \in \mathbb{R} f \in C(\mathbb{R}) y>0 \int_{-\infty}^{\infty}yf(yx)dx = \int_{-\infty}^{\infty}f(x)dx","['multivariable-calculus', 'continuity', 'improper-integrals']"
75,local extremum of $x\mapsto \sum_{j=1}^{\mu}g(x-x_j)^2$,local extremum of,x\mapsto \sum_{j=1}^{\mu}g(x-x_j)^2,"Let $x_1,x_2,\ldots,x_{\mu}\in\mathbb{R}^n$ and let $\phi:\mathbb{R}^n\to\mathbb{R}$ be defined by $x\mapsto \sum_{j=1}^{\mu}g(x-x_j)^2$ , where $g$ denotes the euclidean norm on $\mathbb{R}^n$ . Problem: Find all local extrema of $\phi$ on $\mathbb{R}^n$ , if there are any. My problem is that $\phi$ doesn't seem to be differentiable (at least I haven't been able to find a derivative) and so we have to manually find local extrema or show that there aren't any.        In order to show the latter, I've been trying different approaches for a while now but the problem seems to be that if we let $x$ be ""further away"" from one of the $x_j$ 's, we can't really take into account the effect this has on the ""distance"" from $x$ to the others. I would very much appreciate help with this.","Let and let be defined by , where denotes the euclidean norm on . Problem: Find all local extrema of on , if there are any. My problem is that doesn't seem to be differentiable (at least I haven't been able to find a derivative) and so we have to manually find local extrema or show that there aren't any.        In order to show the latter, I've been trying different approaches for a while now but the problem seems to be that if we let be ""further away"" from one of the 's, we can't really take into account the effect this has on the ""distance"" from to the others. I would very much appreciate help with this.","x_1,x_2,\ldots,x_{\mu}\in\mathbb{R}^n \phi:\mathbb{R}^n\to\mathbb{R} x\mapsto \sum_{j=1}^{\mu}g(x-x_j)^2 g \mathbb{R}^n \phi \mathbb{R}^n \phi x x_j x","['real-analysis', 'calculus']"
76,Is it possible to evaluate $\iiint \frac{2x^2+z^2}{x^2+z^2} dxdydz$ using cylindrical coordinates instead of spherical?,Is it possible to evaluate  using cylindrical coordinates instead of spherical?,\iiint \frac{2x^2+z^2}{x^2+z^2} dxdydz,"I know that this integral is way easier with spherical coordinates, but I would like to understand my mistakes; evaluate $$\iiint_D \frac{2x^2+z^2}{x^2+z^2} dxdydz$$ Where $D=\{(x,y,z)\in\mathbb{R}^3 \ \text{s.t.} \ 1 \leq x^2+y^2+z^2 \leq 4, \ x^2-y^2+z^2 \leq 0\}$ . Letting $x=\rho \cos \theta$ , $y=y$ and $z=\rho \sin \theta$ it follows that $$\iiint_E (2\cos^2 \theta+\sin^2 \theta)\rho d\rho dyd\theta=\iiint_E (1+\cos^2 \theta)\rho d\rho dyd\theta$$ Where $E=\{(\rho,y,\theta)\in\mathbb{R}^3 \ \text{s.t.} \ 1 \leq \rho^2+y^2 \leq 4, \rho^2 \leq y^2\, \rho \geq 0, 0 \leq \theta < 2\pi\}$ . The point is that now I have a lot of conditions on $y$ , because $\sqrt{1-y^2} \leq \rho \leq \sqrt{4-y^2}$ , $-y\leq\rho\leq y$ and $\rho \geq 0$ . From the existence conditions of the roots we get $-1 \leq y \leq 1$ and $-2 \leq y \leq 2$ , so it follows that $-1 \leq y \leq 1$ . So it remains to discuss the cases of $\max\left\{\sqrt{1-y^2},-y\right\} \leq \rho$ and $\rho \leq \min\left\{y,\sqrt{4-y^2}\right\}$ ; it is $y \leq \sqrt{4-y^2}$ for $-1 \leq y \leq 1$ and it is always $\sqrt{1-y^2} \leq \sqrt{4-y^2}$ , we have that $$\max\left\{\sqrt{1-y^2},-y\right\}=\begin{cases} -y, \ \text{if} -1 \leq y \leq -\frac{1}{\sqrt2} \\ \sqrt{4-y^2}, \ \text{if} \ -\frac{1}{\sqrt2} \leq y \leq 1  \end{cases}$$ So I end up with $$\iiint_E (1+\cos^2 \theta)\rho d\rho dyd\theta=\int_0^{2\pi} \left(\int_{-1}^{-\frac{1}{\sqrt2}} \left(\int_{-y}^{\sqrt{4-y^2}} (1+\cos^2 \theta)\rho d\rho\right)dy \right)d\theta+$$ $$+\int_0^{2\pi} \left(\int_{-\frac{1}{\sqrt2}}^{1} \left(\int_{\sqrt{1-y^2}}^{\sqrt{4-y^2}} (1+\cos^2 \theta)\rho d\rho\right)dy \right)d\theta$$ But I get the wrong answer, am I missing some more conditions (maybe the discussion of $\rho \geq 0$ too) or am I making other mistakes? Thanks.","I know that this integral is way easier with spherical coordinates, but I would like to understand my mistakes; evaluate Where . Letting , and it follows that Where . The point is that now I have a lot of conditions on , because , and . From the existence conditions of the roots we get and , so it follows that . So it remains to discuss the cases of and ; it is for and it is always , we have that So I end up with But I get the wrong answer, am I missing some more conditions (maybe the discussion of too) or am I making other mistakes? Thanks.","\iiint_D \frac{2x^2+z^2}{x^2+z^2} dxdydz D=\{(x,y,z)\in\mathbb{R}^3 \ \text{s.t.} \ 1 \leq x^2+y^2+z^2 \leq 4, \ x^2-y^2+z^2 \leq 0\} x=\rho \cos \theta y=y z=\rho \sin \theta \iiint_E (2\cos^2 \theta+\sin^2 \theta)\rho d\rho dyd\theta=\iiint_E (1+\cos^2 \theta)\rho d\rho dyd\theta E=\{(\rho,y,\theta)\in\mathbb{R}^3 \ \text{s.t.} \ 1 \leq \rho^2+y^2 \leq 4, \rho^2 \leq y^2\, \rho \geq 0, 0 \leq \theta < 2\pi\} y \sqrt{1-y^2} \leq \rho \leq \sqrt{4-y^2} -y\leq\rho\leq y \rho \geq 0 -1 \leq y \leq 1 -2 \leq y \leq 2 -1 \leq y \leq 1 \max\left\{\sqrt{1-y^2},-y\right\} \leq \rho \rho \leq \min\left\{y,\sqrt{4-y^2}\right\} y \leq \sqrt{4-y^2} -1 \leq y \leq 1 \sqrt{1-y^2} \leq \sqrt{4-y^2} \max\left\{\sqrt{1-y^2},-y\right\}=\begin{cases} -y, \ \text{if} -1 \leq y \leq -\frac{1}{\sqrt2} \\ \sqrt{4-y^2}, \ \text{if} \ -\frac{1}{\sqrt2} \leq y \leq 1  \end{cases} \iiint_E (1+\cos^2 \theta)\rho d\rho dyd\theta=\int_0^{2\pi} \left(\int_{-1}^{-\frac{1}{\sqrt2}} \left(\int_{-y}^{\sqrt{4-y^2}} (1+\cos^2 \theta)\rho d\rho\right)dy \right)d\theta+ +\int_0^{2\pi} \left(\int_{-\frac{1}{\sqrt2}}^{1} \left(\int_{\sqrt{1-y^2}}^{\sqrt{4-y^2}} (1+\cos^2 \theta)\rho d\rho\right)dy \right)d\theta \rho \geq 0","['integration', 'multivariable-calculus', 'solution-verification']"
77,"Show that the tangent plane of the cone $z^2=x^2+y^2$ at (a,b,c)$\ne$0 intersects the cone in a line","Show that the tangent plane of the cone  at (a,b,c)0 intersects the cone in a line",z^2=x^2+y^2 \ne,"My attempt: Let $f(x,y,z)=x^2+y^2-z^2$ and calculate the total derivative $Df(a,b,c)=(2a,2b,-2c)$ . The tangent plane is $$g(x,y,z)=f(a,b,c)+Df(a,b,c)(x-a,y-b,z-c)=2ax+2by-2cz-a^2-b^2+c^2$$ Then let $g(x,y,z)=f(x,y,z)$ to find the intersction got $$(x-a)^2+(y-b)^2-(z-c)^2=0$$ Does this equation means the tangent plane intersects the cone in a line?",My attempt: Let and calculate the total derivative . The tangent plane is Then let to find the intersction got Does this equation means the tangent plane intersects the cone in a line?,"f(x,y,z)=x^2+y^2-z^2 Df(a,b,c)=(2a,2b,-2c) g(x,y,z)=f(a,b,c)+Df(a,b,c)(x-a,y-b,z-c)=2ax+2by-2cz-a^2-b^2+c^2 g(x,y,z)=f(x,y,z) (x-a)^2+(y-b)^2-(z-c)^2=0",['multivariable-calculus']
78,"Given $\phi$ a mapping. Prove that for each $\mathit{i}$, $\sum_{j=1}^n \partial_{x_j}(\mathbf{cof} \mathit{D} \phi)_{ji} \equiv 0$","Given  a mapping. Prove that for each ,",\phi \mathit{i} \sum_{j=1}^n \partial_{x_j}(\mathbf{cof} \mathit{D} \phi)_{ji} \equiv 0,"Let $\phi \in \mathit{C}^2 (\mathbb{R}^n , \mathbb{R}^n)$ . Let $\mathbf{cof} \mathit{D} \phi$ be the cofactor of $\mathit{D} \phi$ (the Jacobian matrix of $\phi$ ). i.e. $$(\mathbf{cof} \mathit{D} \phi)_{ij} =(-1)^{i+j}*(\mathit{D} \phi)_{ji}$$ where $(\mathit{D} \phi)_{ji}$ is the determinant of the submatrix of $\mathit{D} \phi$ obtained by deleting the $\mathit{j}$ th row and $\mathit{i}$ th column. Prove that for each $\mathit{i}$ , $$\sum_{j=1}^n \partial_{x_j}(\mathbf{cof} \mathit{D} \phi)_{ji} \equiv 0$$ I tried many ways, but all failed. And I think this problem may have something to do with closed differential forms, but I can't give a proof.","Let . Let be the cofactor of (the Jacobian matrix of ). i.e. where is the determinant of the submatrix of obtained by deleting the th row and th column. Prove that for each , I tried many ways, but all failed. And I think this problem may have something to do with closed differential forms, but I can't give a proof.","\phi \in \mathit{C}^2 (\mathbb{R}^n , \mathbb{R}^n) \mathbf{cof} \mathit{D} \phi \mathit{D} \phi \phi (\mathbf{cof} \mathit{D} \phi)_{ij} =(-1)^{i+j}*(\mathit{D} \phi)_{ji} (\mathit{D} \phi)_{ji} \mathit{D} \phi \mathit{j} \mathit{i} \mathit{i} \sum_{j=1}^n \partial_{x_j}(\mathbf{cof} \mathit{D} \phi)_{ji} \equiv 0","['multivariable-calculus', 'partial-derivative', 'differential-forms', 'jacobian']"
79,Is the partial derivative of a constant always zero?,Is the partial derivative of a constant always zero?,,"I'm trying to get my head round using the multivariable chain rule to find exact derivatives. For example I want to find the exact derivative(using partial derivatives) of, $$r^2=x^2+y^2$$ Where r is initially a constant. I now assign $f(x,y)=r^2$ , then it follows $$\frac{\partial f}{\partial x}=2x, \frac{\partial f}{\partial y}=2y$$ Then from the multivariable chain rule, $$\frac{df}{dx}=\frac{\partial f}{\partial x}*1 + \frac{\partial f}{\partial y} \frac{dy}{dx}$$ Now I substitute in what is known and can simplify to reach the answer, $$0=2x+2y\frac{dy}{dx} \ \ \ \ \ (*)$$ This leads to the correct answer, but I'm certain that my reasoning must be wrong. In $(*)$ , I asserted that $\frac{dr^2}{dx}=0$ , which is obvious. However, does this not also mean that the partial derivatives must be 0 instead of $2x$ , $2y$ ? It does not make any intuitive sense to me how the partial derivative of a constant can be non-zero, and I am certain this cannot be the case.","I'm trying to get my head round using the multivariable chain rule to find exact derivatives. For example I want to find the exact derivative(using partial derivatives) of, Where r is initially a constant. I now assign , then it follows Then from the multivariable chain rule, Now I substitute in what is known and can simplify to reach the answer, This leads to the correct answer, but I'm certain that my reasoning must be wrong. In , I asserted that , which is obvious. However, does this not also mean that the partial derivatives must be 0 instead of , ? It does not make any intuitive sense to me how the partial derivative of a constant can be non-zero, and I am certain this cannot be the case.","r^2=x^2+y^2 f(x,y)=r^2 \frac{\partial f}{\partial x}=2x, \frac{\partial f}{\partial y}=2y \frac{df}{dx}=\frac{\partial f}{\partial x}*1 + \frac{\partial f}{\partial y} \frac{dy}{dx} 0=2x+2y\frac{dy}{dx} \ \ \ \ \ (*) (*) \frac{dr^2}{dx}=0 2x 2y","['multivariable-calculus', 'partial-derivative']"
80,Deducing function of two variables from its partial derivatives,Deducing function of two variables from its partial derivatives,,"I am fighting with a very simple system of linear PDEs, which solution I think it is straightforward, however, I cannot write it down in a formal way. $$ \left\{ \begin{array}{c} \frac{\partial X\left(x_{1},x_{2}\right)}{\partial x_{1}}=f\left(x_{1},x_{2}\right)\\ \frac{\partial X\left(x_{1},x_{2}\right)}{\partial x_{2}}=g\left(x_{1},x_{2}\right) \end{array}\right. $$ I would like to write the solution as an integral of $f$ and $g$ . In this way I can also work out the boundary conditions. Can I write it as some sort of $X\left(x_{1},x_{2}\right)=\int\ ?\ dx_{1}\ dx_{2}+ Constant$ ? Just to give an idea, at the end, a boundary condition could be e.g. $X(x_1=0,x_2) = 0$ but I'd like to be general.","I am fighting with a very simple system of linear PDEs, which solution I think it is straightforward, however, I cannot write it down in a formal way. I would like to write the solution as an integral of and . In this way I can also work out the boundary conditions. Can I write it as some sort of ? Just to give an idea, at the end, a boundary condition could be e.g. but I'd like to be general."," \left\{ \begin{array}{c}
\frac{\partial X\left(x_{1},x_{2}\right)}{\partial x_{1}}=f\left(x_{1},x_{2}\right)\\
\frac{\partial X\left(x_{1},x_{2}\right)}{\partial x_{2}}=g\left(x_{1},x_{2}\right)
\end{array}\right.  f g X\left(x_{1},x_{2}\right)=\int\ ?\ dx_{1}\ dx_{2}+ Constant X(x_1=0,x_2) = 0","['multivariable-calculus', 'partial-differential-equations', 'systems-of-equations', 'linear-pde']"
81,How to find a function whose limit points identify $\mathbb{C}$ with a cylinder?,How to find a function whose limit points identify  with a cylinder?,\mathbb{C},"I was considering the set of mobius transformations $$ \frac{a+bx}{c+dx} $$ A property they have is that if you consider any line $x(t) = e^{i\theta} t $ then: $$ \lim_{t \rightarrow \infty} \frac{a + bx(t)}{c+dx(t)} =  \frac{a+be^{i\theta}t}{c+de^{i\theta t}} = \frac{b}{d}  $$ Where curiously $\frac{b}{d}$ is COMPLETELY independent of $e^{i\theta}$ . In some sense the ""limit"" points of the mobius transformation, identify every direction with the same complex value, and you might even go as to say the ""limit points"" identify $\mathbb{C}$ as a sphere. All this obvious, but now suppose we turn the question around a bit, How do we find a complex function whose set of limit points identify something other than a sphere. The Question: I wanted to start with something simple like a cylinder. In this case we are trying to find a complex function $f$ such that if $\frac{\pi}{4} < s < \frac{3\pi}{4} $ Then: $$  \lim_{t \rightarrow \infty} f(e^{is}t) = f(e^{-is}t) $$ However we don't want over identification, that is for distinct $\frac{\pi}{4} < s < \frac{3\pi}{4} $ it should be that $\lim_{t \rightarrow \infty} f(e^{is}t)$ is distinct Moreover it also must be the case that any $s \in (-\frac{\pi}{4}, \frac{\pi}{4}) \cup (\frac{3\pi}{4}, \frac{5\pi}{4}) $ it should be that $  \lim_{t \rightarrow \infty} f(e^{is}t)  $ takes on unique values. A strategy: So, if we forget that we are working in $\mathbb{C}$ we can just consider the unit disk in $\mathbb{R}^2$ and from here find a function $f: \mathbb{R}^2 \rightarrow \mathbb{R}^2$ which behaves in the expected way on the unit disk [identifying it with a cylinder]. Now the next step would be to find another function $g: \mathbb{R}^2 \rightarrow \mathbb{R}^2$ whose limit points in each direction are the coordinates the unit circle corresponding to that direction. Then $f(g)$ is a function $\mathbb{R}^2 \rightarrow \mathbb{R}^2$ that behaves how we want and we just try our luck in checking if $f(g)$ obeys the Cauchy-Riemann Equations to give rise to a complex function.","I was considering the set of mobius transformations A property they have is that if you consider any line then: Where curiously is COMPLETELY independent of . In some sense the ""limit"" points of the mobius transformation, identify every direction with the same complex value, and you might even go as to say the ""limit points"" identify as a sphere. All this obvious, but now suppose we turn the question around a bit, How do we find a complex function whose set of limit points identify something other than a sphere. The Question: I wanted to start with something simple like a cylinder. In this case we are trying to find a complex function such that if Then: However we don't want over identification, that is for distinct it should be that is distinct Moreover it also must be the case that any it should be that takes on unique values. A strategy: So, if we forget that we are working in we can just consider the unit disk in and from here find a function which behaves in the expected way on the unit disk [identifying it with a cylinder]. Now the next step would be to find another function whose limit points in each direction are the coordinates the unit circle corresponding to that direction. Then is a function that behaves how we want and we just try our luck in checking if obeys the Cauchy-Riemann Equations to give rise to a complex function."," \frac{a+bx}{c+dx}  x(t) = e^{i\theta} t   \lim_{t \rightarrow \infty} \frac{a + bx(t)}{c+dx(t)} =  \frac{a+be^{i\theta}t}{c+de^{i\theta t}} = \frac{b}{d}   \frac{b}{d} e^{i\theta} \mathbb{C} f \frac{\pi}{4} < s < \frac{3\pi}{4}    \lim_{t \rightarrow \infty} f(e^{is}t) = f(e^{-is}t)  \frac{\pi}{4} < s < \frac{3\pi}{4}  \lim_{t \rightarrow \infty} f(e^{is}t) s \in (-\frac{\pi}{4}, \frac{\pi}{4}) \cup (\frac{3\pi}{4}, \frac{5\pi}{4})    \lim_{t \rightarrow \infty} f(e^{is}t)   \mathbb{C} \mathbb{R}^2 f: \mathbb{R}^2 \rightarrow \mathbb{R}^2 g: \mathbb{R}^2 \rightarrow \mathbb{R}^2 f(g) \mathbb{R}^2 \rightarrow \mathbb{R}^2 f(g)","['complex-analysis', 'multivariable-calculus']"
82,What exactly is the way to find Projection of a surface on to a Plane,What exactly is the way to find Projection of a surface on to a Plane,,"I was trying to find the surface integral of the surface defined by equation of plane passing through the points $(0,0,2)$ , $(0,1,0)$ and $(2,1,0)$ over the vector field $\vec{F}=x\vec{i}+y^3\vec{j}+\vec{k}$ My try: The equation of the plane is $2y+z=2$ whose unit outward normal being: $$\hat{n}=\frac{2\vec{j}+\vec{k}}{\sqrt{5}}$$ So the surface integral is now: $$\int\int_{S}\vec{F}.\hat{n}dS=\int\int_{S}\frac{2y^3+1}{\sqrt{5}}dS$$ Now how to start the procedure of projection? the book has projected the surface on $x-z$ plane, but i am not clear the concept behind this projection","I was trying to find the surface integral of the surface defined by equation of plane passing through the points , and over the vector field My try: The equation of the plane is whose unit outward normal being: So the surface integral is now: Now how to start the procedure of projection? the book has projected the surface on plane, but i am not clear the concept behind this projection","(0,0,2) (0,1,0) (2,1,0) \vec{F}=x\vec{i}+y^3\vec{j}+\vec{k} 2y+z=2 \hat{n}=\frac{2\vec{j}+\vec{k}}{\sqrt{5}} \int\int_{S}\vec{F}.\hat{n}dS=\int\int_{S}\frac{2y^3+1}{\sqrt{5}}dS x-z","['multivariable-calculus', 'surface-integrals', 'multiple-integral']"
83,Triple integral $\iiint_{D}\cos(x+2y+3z) dV $ over a sphere of radius 1,Triple integral  over a sphere of radius 1,\iiint_{D}\cos(x+2y+3z) dV ,"My math problem is to evaluate the integral $$\iiint_{D}cos(x+2y+3z) dV  $$ where $D=\left \{ (x,y,z):x^2+y^2+z^2\leq 1  \right \}$ I tried approaching it with spherical coordinate by substituting $x=\rho \space \sin\phi \space \cos\theta$ , $y=\rho \space \sin\phi \space \sin\theta$ , $z=\rho\space  \cos\phi $ , but it doesn't help much since the function inside the integral ends up being more complicated. (cosine and sine functions inside cosine) I also tried using Cartesian coordinate, and changing the integral into $$8\int_{0}^{1}\int_{0}^{\sqrt{1-x^2}}\int_{0}^{\sqrt{1-x^2-y^2}} \cos(x+2y+3z) \space \space dzdydx $$ $$=8\int_{0}^{1}\int_{0}^{\sqrt{1-x^2}} \frac{\sin(x+2y+3\sqrt{1-x^2-y^2})-\sin(x+2y)}3 \space \space dydx$$ But I have no idea how to evaluate it, and I don't have any more ideas on this problem. I see someone else in another post saying we can make use of spherical symmetry and ""replace the integrand by $\cos(hz)$ , where $h=|(1,2,3)|$ . Then, integrate in slices perpendicular to the $z$ -axis."", but I have no idea if it would work or why it would work. (and I don't have enough reputation points to comment on that post :( ) Any help is appreciated. Thanks!","My math problem is to evaluate the integral where I tried approaching it with spherical coordinate by substituting , , , but it doesn't help much since the function inside the integral ends up being more complicated. (cosine and sine functions inside cosine) I also tried using Cartesian coordinate, and changing the integral into But I have no idea how to evaluate it, and I don't have any more ideas on this problem. I see someone else in another post saying we can make use of spherical symmetry and ""replace the integrand by , where . Then, integrate in slices perpendicular to the -axis."", but I have no idea if it would work or why it would work. (and I don't have enough reputation points to comment on that post :( ) Any help is appreciated. Thanks!","\iiint_{D}cos(x+2y+3z) dV   D=\left \{ (x,y,z):x^2+y^2+z^2\leq 1  \right \} x=\rho \space \sin\phi \space \cos\theta y=\rho \space \sin\phi \space \sin\theta z=\rho\space  \cos\phi  8\int_{0}^{1}\int_{0}^{\sqrt{1-x^2}}\int_{0}^{\sqrt{1-x^2-y^2}} \cos(x+2y+3z) \space \space dzdydx  =8\int_{0}^{1}\int_{0}^{\sqrt{1-x^2}} \frac{\sin(x+2y+3\sqrt{1-x^2-y^2})-\sin(x+2y)}3 \space \space dydx \cos(hz) h=|(1,2,3)| z","['integration', 'multivariable-calculus']"
84,How to find the intervals on which $f$ is concave up? $f(x) = \arctan(\sin x )$,How to find the intervals on which  is concave up?,f f(x) = \arctan(\sin x ),\begin{align*}f(x) &= \arctan(\sin x )\\[4pt] f'(x) &= \frac{\cos x }{1+\sin^2 x }\\[4pt] f''(x) &= -\frac{(\sin x )(2+\cos^2 x )}{(1+ \sin^2 x )^2} \end{align*} So I need to use these three equations to find the intervals on which $f$ is concave up. Can you reason why it is?,So I need to use these three equations to find the intervals on which is concave up. Can you reason why it is?,"\begin{align*}f(x) &= \arctan(\sin x )\\[4pt]
f'(x) &= \frac{\cos x }{1+\sin^2 x }\\[4pt]
f''(x) &= -\frac{(\sin x )(2+\cos^2 x )}{(1+ \sin^2 x )^2}
\end{align*} f","['calculus', 'multivariable-calculus', 'derivatives', 'trigonometry']"
85,Existence of limit of $f(x)=\frac{x_1}{\Vert x\Vert_2}$ at ${0 \choose 0}$,Existence of limit of  at,f(x)=\frac{x_1}{\Vert x\Vert_2} {0 \choose 0},"We are given the function: $f: M\subset\mathbb{R}^2 \to \mathbb{R}$ , where $f(x)=\frac{x_1}{\Vert x\Vert_2}$ and $M:=\{x={x_1 \choose x_2}\in\mathbb{R}^2~:~x_1>\sqrt{|x_2|}\}$ . Show that the limit at ${0 \choose 0}$ exists. I already figured out that the limit must be $1$ . As the domain is restricted to specific points I could not properly use zero-sequences to show the existence of the limit. So I tried to apply th $\epsilon$ - $\delta$ -criterion for limits. However, I could not find an upper boundary for $\left|\frac{x_1}{\Vert x\Vert_2}-1\right|$ such that for all $x \in M$ with $\Vert x - 0\Vert <\delta$ we get: $\left|\frac{x_1}{\Vert x\Vert_2}-1\right|\leq....<\epsilon$ . At the begining I was optimistic to get such an upper boundary if I incorporate the condition of $M$ but it didn't get me anywhere... Maybe there is some secret trick... As this is homework I would appreciate if you just provide me a little hint and not the full solution unless you hide it.","We are given the function: , where and . Show that the limit at exists. I already figured out that the limit must be . As the domain is restricted to specific points I could not properly use zero-sequences to show the existence of the limit. So I tried to apply th - -criterion for limits. However, I could not find an upper boundary for such that for all with we get: . At the begining I was optimistic to get such an upper boundary if I incorporate the condition of but it didn't get me anywhere... Maybe there is some secret trick... As this is homework I would appreciate if you just provide me a little hint and not the full solution unless you hide it.",f: M\subset\mathbb{R}^2 \to \mathbb{R} f(x)=\frac{x_1}{\Vert x\Vert_2} M:=\{x={x_1 \choose x_2}\in\mathbb{R}^2~:~x_1>\sqrt{|x_2|}\} {0 \choose 0} 1 \epsilon \delta \left|\frac{x_1}{\Vert x\Vert_2}-1\right| x \in M \Vert x - 0\Vert <\delta \left|\frac{x_1}{\Vert x\Vert_2}-1\right|\leq....<\epsilon M,"['limits', 'analysis', 'multivariable-calculus']"
86,Derivative of $g(t) = Df(x + t(y-x))(y - x)$?,Derivative of ?,g(t) = Df(x + t(y-x))(y - x),$f \in C^2(\mathbb{R}^2)$ and $x$ represents the column vector while $x^T$ represents horizontal vector. I was told that $g'(t) = (y - x)^T D^2f(x + t(y - x))(y - x)$ . Why is that?,and represents the column vector while represents horizontal vector. I was told that . Why is that?,f \in C^2(\mathbb{R}^2) x x^T g'(t) = (y - x)^T D^2f(x + t(y - x))(y - x),"['analysis', 'multivariable-calculus', 'derivatives']"
87,Prove the convexity for f: $\mathbb{R}^3$ $\to$ $\mathbb{R}$,Prove the convexity for f:,\mathbb{R}^3 \to \mathbb{R},"The problem below is one of my homework for the Convex Geometry course.  I have tried to prove by using the Hessian matrix or the gradient but cannot figure out the right answers. Show that $$ f(x_1, x_2, x_3) = \cfrac1{x_1-\cfrac{1}{x_2-\cfrac1{x_3}}} $$ is convex on $\left(\Bbb R_*^+\right)^3$ . Can you help me on this, please?","The problem below is one of my homework for the Convex Geometry course.  I have tried to prove by using the Hessian matrix or the gradient but cannot figure out the right answers. Show that is convex on . Can you help me on this, please?","
f(x_1, x_2, x_3) = \cfrac1{x_1-\cfrac{1}{x_2-\cfrac1{x_3}}}
 \left(\Bbb R_*^+\right)^3","['multivariable-calculus', 'convex-geometry']"
88,Triple integral evaluation problem,Triple integral evaluation problem,,"I am asked to evaluate the triple integral, $$\iiint_E (12x^2-2yx-5) \ dV$$ Where $E$ is the region enclosed by the surfaces $$z=x^2-1, \ z=1-x^2, \ y=0, \ y=2$$ Am I right in saying that we can define the region; $$E=\{(x,y,z)\ | \ -1\leq x\leq 1, \ 0\leq y \leq 2, \ x^2-1\leq z \leq1-x^2\}$$","I am asked to evaluate the triple integral, Where is the region enclosed by the surfaces Am I right in saying that we can define the region;","\iiint_E (12x^2-2yx-5) \ dV E z=x^2-1, \ z=1-x^2, \ y=0, \ y=2 E=\{(x,y,z)\ | \ -1\leq x\leq 1, \ 0\leq y \leq 2, \ x^2-1\leq z \leq1-x^2\}","['calculus', 'integration', 'multivariable-calculus', 'definite-integrals']"
89,"Evaluate $\iiint_V \sqrt{x^2+y^2+z^2}\, dV$ where $V: x^2 + y^2 + z^2 \leq 2z$",Evaluate  where,"\iiint_V \sqrt{x^2+y^2+z^2}\, dV V: x^2 + y^2 + z^2 \leq 2z","Evaluate $$I=\iiint_V \sqrt{x^2+y^2+z^2}\, dV\,,$$ where $V: x^2 + y^2 + z^2 \leq 2z$ . I tried using the cylindrical coordinates to arrive at: $$I = \int\int_R\int_{z=1-\sqrt{1-r^2}}^{1+\sqrt{1-r^2}}r\cdot\sqrt{r^2+z^2}\,dz\,dr\,d\theta$$ where $R = r \leq 1$ But this is very tough to evaluate. Is there a better way to do this?",Evaluate where . I tried using the cylindrical coordinates to arrive at: where But this is very tough to evaluate. Is there a better way to do this?,"I=\iiint_V \sqrt{x^2+y^2+z^2}\, dV\,, V: x^2 + y^2 +
z^2 \leq 2z I = \int\int_R\int_{z=1-\sqrt{1-r^2}}^{1+\sqrt{1-r^2}}r\cdot\sqrt{r^2+z^2}\,dz\,dr\,d\theta R = r \leq 1","['multivariable-calculus', 'multiple-integral', 'stokes-theorem']"
90,"Find all values of $a$ that satisfy the inequality for all $(x,y)$",Find all values of  that satisfy the inequality for all,"a (x,y)","I need to determine all possible values for $a\in [0,2[$ such that the following holds for all $(x,y)\in\mathbb{R}^2$ : $$(2x+ay)(-x+6y)+(2y+ax)(-20y)\leq 0$$ or equivalent $$-2x^2-40y^2-21axy+12xy+a6y^2\leq 0$$ I've plotted the function and observed that it holds for all $a\in[0,\alpha[$ for some $\alpha$ between 1.3 and 1.4 and it doesn't hold for greater values. But I don't know how to prove it nor find such $\alpha$ .",I need to determine all possible values for such that the following holds for all : or equivalent I've plotted the function and observed that it holds for all for some between 1.3 and 1.4 and it doesn't hold for greater values. But I don't know how to prove it nor find such .,"a\in [0,2[ (x,y)\in\mathbb{R}^2 (2x+ay)(-x+6y)+(2y+ax)(-20y)\leq 0 -2x^2-40y^2-21axy+12xy+a6y^2\leq 0 a\in[0,\alpha[ \alpha \alpha","['multivariable-calculus', 'inequality', 'polynomials', 'optimization', 'quadratics']"
91,Is the optimum of this constraint optimization problem smooth everywhere except at one jump?,Is the optimum of this constraint optimization problem smooth everywhere except at one jump?,,"Let $f:\mathbb R^+ \to \mathbb R$ be a smooth function, satisfying $f(1)=0$ , and suppose that $\,|f(x)|$ is strictly increasing when $x \ge 1$ , and strictly decreasing when $x \le 1$ . For any $s>0$ , define $$ F(s)=\min_{xy=s,x,y>0} f^2(x)+ f^2(y).  $$ If I am not mistaken, the map $s \to F(s)$ is continuous. Question: Does there exist an $s^* >0$ such that $F|_{(0,s^*)},F|_{(s^*,\infty)}$ are smooth? I ask if $F(s)$ is piecewise smooth, with at most one ""jump"" point. Can there be more than one? Here is an example which shows that we must allow for at least one point of non-regularity: Linear penalization: Take $f(x)=x-1$ . In that case $$F(s) = \begin{cases} 2(\sqrt{s}-1)^2,  & \text{ if  }\, s \ge \frac{1}{4} \\ 1-2s, & \text{ if  }\, s \le \frac{1}{4} \end{cases} $$ $F$ is $C^1$ but not $C^2$ . (for a more involved example, see here ). Of course, $F(s)$ can be smooth, e.g. when $f(x)=\log x$ . In that case $ F(s)=2f^2(\sqrt s)=\frac{1}{2}(\log s)^2.$","Let be a smooth function, satisfying , and suppose that is strictly increasing when , and strictly decreasing when . For any , define If I am not mistaken, the map is continuous. Question: Does there exist an such that are smooth? I ask if is piecewise smooth, with at most one ""jump"" point. Can there be more than one? Here is an example which shows that we must allow for at least one point of non-regularity: Linear penalization: Take . In that case is but not . (for a more involved example, see here ). Of course, can be smooth, e.g. when . In that case","f:\mathbb R^+ \to \mathbb R f(1)=0 \,|f(x)| x \ge 1 x \le 1 s>0 
F(s)=\min_{xy=s,x,y>0} f^2(x)+ f^2(y). 
 s \to F(s) s^* >0 F|_{(0,s^*)},F|_{(s^*,\infty)} F(s) f(x)=x-1 F(s) =
\begin{cases}
2(\sqrt{s}-1)^2,  & \text{ if  }\, s \ge \frac{1}{4} \\
1-2s, & \text{ if  }\, s \le \frac{1}{4}
\end{cases}
 F C^1 C^2 F(s) f(x)=\log x  F(s)=2f^2(\sqrt s)=\frac{1}{2}(\log s)^2.","['real-analysis', 'multivariable-calculus', 'optimization', 'nonlinear-optimization', 'smooth-functions']"
92,Relative Topology in $\mathbb{R}^n$,Relative Topology in,\mathbb{R}^n,"I'm studying relative topology for the first time and I've come across a proposition which states if $V \subset \mathbb{R}^n$ is non empty and $A \subseteq V$ $$ \partial_v A = \partial A \cap V.$$ The proposition is clearly false if you take $A=V$ and $V$ closed then, $\partial_v V \subseteq \text{cl}_V(V - V)=\emptyset.$ Whereas, $\partial V$ is certainly not empty. The following is a proof on someone elses question Boundary of set on relative topology in $R^n$ . This doesn't work either, I've proven it for the case that $V$ is open, is that the only case that it holds for all $A \subset V$ (I know it is for $A \subseteq V$ )?","I'm studying relative topology for the first time and I've come across a proposition which states if is non empty and The proposition is clearly false if you take and closed then, Whereas, is certainly not empty. The following is a proof on someone elses question Boundary of set on relative topology in $R^n$ . This doesn't work either, I've proven it for the case that is open, is that the only case that it holds for all (I know it is for )?",V \subset \mathbb{R}^n A \subseteq V  \partial_v A = \partial A \cap V. A=V V \partial_v V \subseteq \text{cl}_V(V - V)=\emptyset. \partial V V A \subset V A \subseteq V,"['real-analysis', 'general-topology', 'multivariable-calculus']"
93,Compute Curl Integral of Vector Field and Ellipse,Compute Curl Integral of Vector Field and Ellipse,,"Consider a vector field : ${\bf F}=P(x){\bf i}+ Q(y){\bf j}$ , for some functions : $P(x), Q(y)$ which have continuous partial derivatives everywhere. Let: $C$ stand for the ellipse : $\{ 4x^2+9y^2=1\}$ . Then : $\int_C {\bf F}\cdot d{\bf r}$ equals: I can't determine an answer with this data but the options are: $P(\frac{1}{2}) - Q(\frac{1}{2}), 0, P(\frac{1}{2}),P(\frac{1}{2}) + Q(\frac{1}{2})$ Which is the right answer?","Consider a vector field : , for some functions : which have continuous partial derivatives everywhere. Let: stand for the ellipse : . Then : equals: I can't determine an answer with this data but the options are: Which is the right answer?","{\bf F}=P(x){\bf i}+ Q(y){\bf j} P(x), Q(y) C \{ 4x^2+9y^2=1\} \int_C {\bf F}\cdot d{\bf r} P(\frac{1}{2}) - Q(\frac{1}{2}), 0, P(\frac{1}{2}),P(\frac{1}{2}) + Q(\frac{1}{2})",['multivariable-calculus']
94,Total derivative of $\psi(x) = \frac{1}{\left \| x \right \|^p}Ax$,Total derivative of,\psi(x) = \frac{1}{\left \| x \right \|^p}Ax,"I have been trying to find the Fréchet derivative of the following function: $\psi(x) = \frac{1}{\left \| x \right \|^p}Ax$ $(x \in \mathbb{R}^n, A \in\mathbb{R^{m \times n}})$ . One possibility would be to use the derivative of continuous and bilinear operators on Banach Spaces, so in this case if we could set $\psi(x)=B(f(x),g(x))$ whereas $f:\mathbb{R^n} \rightarrow \mathbb{R}, x \mapsto\frac{1}{\left \| x \right \|^p}$ and $g:\mathbb{R^n} \rightarrow \mathbb{R^m}, x \mapsto Ax $ . And $B:(\mathbb{R} \times \mathbb{R}^m) \simeq \mathbb{R}^{m+1} \rightarrow \mathbb{R}^m ,B(x,y)=xy$ $$$$ The formula I got to is the following ( $Df$ is the total derivative of $f$ ): $$D\psi(x)=g(x)Df(x)+f(x)Dg(x)$$ But then the wikipedia article on the product rule states that: $$D(f \cdot g)=Df \cdot g+f \cdot Dg(x) (""."" \text{represents scalar multiplication})$$ which is different as $Df$ and $g$ do not commute. I checked the dimensions of the objects I'm using to see where the difference lies, and I actually discovered that using the Wikipedia article the matrix multiplication dimensions do not match. Does anyone see whether there is a mistake in my reasoning, and if so then where? By the way, I am looking for an answer that does NOT use partial derivatives. I can of course solve this problem using them, but I'm looking for a more general approach that doesn't just rely on heavy calculations.","I have been trying to find the Fréchet derivative of the following function: . One possibility would be to use the derivative of continuous and bilinear operators on Banach Spaces, so in this case if we could set whereas and . And The formula I got to is the following ( is the total derivative of ): But then the wikipedia article on the product rule states that: which is different as and do not commute. I checked the dimensions of the objects I'm using to see where the difference lies, and I actually discovered that using the Wikipedia article the matrix multiplication dimensions do not match. Does anyone see whether there is a mistake in my reasoning, and if so then where? By the way, I am looking for an answer that does NOT use partial derivatives. I can of course solve this problem using them, but I'm looking for a more general approach that doesn't just rely on heavy calculations.","\psi(x) = \frac{1}{\left \| x \right \|^p}Ax (x \in \mathbb{R}^n, A \in\mathbb{R^{m \times n}}) \psi(x)=B(f(x),g(x)) f:\mathbb{R^n} \rightarrow \mathbb{R}, x \mapsto\frac{1}{\left \| x \right \|^p} g:\mathbb{R^n} \rightarrow \mathbb{R^m}, x \mapsto Ax  B:(\mathbb{R} \times \mathbb{R}^m) \simeq \mathbb{R}^{m+1} \rightarrow \mathbb{R}^m ,B(x,y)=xy  Df f D\psi(x)=g(x)Df(x)+f(x)Dg(x) D(f \cdot g)=Df \cdot g+f \cdot Dg(x) (""."" \text{represents scalar multiplication}) Df g","['multivariable-calculus', 'derivatives', 'bilinear-form', 'frechet-derivative']"
95,"Finding the critical points of $f(x,y) = \sin(x)\sin(y)$",Finding the critical points of,"f(x,y) = \sin(x)\sin(y)","I am attempting to find the critical points of a function for local max min purposes and have gotten stuck. The function is $$f(x,y) = \sin(x)\sin(y)$$ Bounded by $-\pi < x < \pi$ and $-\pi < y < \pi$ . I have the partial derivative wrt to $x$ as: \begin{equation} \frac{\partial f}{\partial x} = \cos(x)\sin(y) \end{equation} and the partial derivate wrt to $y$ as: \begin{equation} \frac{\partial f}{\partial y} = \cos(y)\sin(x) \end{equation} I now want $$\cos(y)\sin(x) = 0 = \sin(x)\cos(y)$$ For \begin{equation} \frac{\partial f}{\partial x} = 0 \end{equation} Then $\sin(x) = 0$ when $x = 0$ , OR $\cos(y) = 0$ when $y = -\pi/2$ and $\pi/2$ . For \begin{equation} \frac{\partial f}{\partial y} = 0 \end{equation} then $\sin(y) = 0$ when $y = 0$ , OR $\cos(x) = 0$ when $x = -\pi/2$ and $\pi/2$ . My question is how do I actually arrange these values into critical points properly?","I am attempting to find the critical points of a function for local max min purposes and have gotten stuck. The function is Bounded by and . I have the partial derivative wrt to as: and the partial derivate wrt to as: I now want For Then when , OR when and . For then when , OR when and . My question is how do I actually arrange these values into critical points properly?","f(x,y) = \sin(x)\sin(y) -\pi < x < \pi -\pi < y < \pi x \begin{equation}
\frac{\partial f}{\partial x} = \cos(x)\sin(y)
\end{equation} y \begin{equation}
\frac{\partial f}{\partial y} = \cos(y)\sin(x)
\end{equation} \cos(y)\sin(x) = 0 = \sin(x)\cos(y) \begin{equation}
\frac{\partial f}{\partial x} = 0
\end{equation} \sin(x) = 0 x = 0 \cos(y) = 0 y = -\pi/2 \pi/2 \begin{equation}
\frac{\partial f}{\partial y} = 0
\end{equation} \sin(y) = 0 y = 0 \cos(x) = 0 x = -\pi/2 \pi/2","['multivariable-calculus', 'trigonometry', 'maxima-minima']"
96,"Suggestions for $ \lim_{(x,y)\to (0,0)} \frac{x-\sqrt{xy}}{x^2-y^2} $?",Suggestions for ?," \lim_{(x,y)\to (0,0)} \frac{x-\sqrt{xy}}{x^2-y^2} ","I'm trying to evaluate $$  \lim_{(x,y)\to (0,0)} \frac{x-\sqrt{xy}}{x^2-y^2} $$ over the domain $x>0$ , $y>0$ . ============ My attempt: $f(x,x^2)\to +\infty$ ; so if the limit exists it must be $+\infty$ . I tried to evaluate the limits ""near"" $(x,x)$ where, I thought, there's may be some problems: $f(x, x-x^2)\to +\infty$ . Then I convinced myself the limit could be $+\infty$ : since $f(x,y)>0$ over the domain, I had to find such $g(x,y)$ that: 1. $f(x,y) \ge g(x,y)$ 2. $ \lim_{(x,y)\to (0,0)} g(x,y)\to +\infty $ $$ f(x,y)=\frac{x-\sqrt{xy}}{x^2-y^2}=\frac{x-\sqrt{xy}+y-y}{x^2-y^2}=\frac{x-\sqrt{xy}+y}{x^2-y^2}-\frac{y}{x^2-y^2}=\frac{\sqrt{\left(x-\sqrt{xy}+y\right)^2}}{x^2-y^2}-\frac{y}{x^2-y^2} $$ Where the last step follows by $(x-\sqrt{xy}+y) \ge 0$ with $x>0$ , $y>0$ . $$ \frac{\sqrt{\left(x-\sqrt{xy}+y\right)^2}}{x^2-y^2}-\frac{y}{x^2-y^2} = \frac{\sqrt{3\left(x-\sqrt{xy}+y\right)^2}}{\sqrt{3}(x^2-y^2)}-\frac{y}{x^2-y^2}. $$ From $\left[3\left(x-\sqrt{xy}+y\right)^2\right] \ge \left[x+xy+y^2\right]$ , for every $(x,y)$ with $(x>y)$ : $$ \frac{\sqrt{3\left(x-\sqrt{xy}+y\right)^2}}{\sqrt{3}(x^2-y^2)}-\frac{y}{x^2-y^2} \ge \frac{\sqrt{x^2+xy+y^2}}{\sqrt{3}(x^2-y^2)}+\frac{y}{y^2-x^2}. $$ From here I observated that $ \left[\lim_{(x,y)\to (0,0)} g(x,y)\to +\infty \right]$ and eventually $ \left[\lim_{(x,y)\to (0,0)} f(x,y)\to +\infty \right]$ for $(x>y)$ . I thought that for $(y>x)$ , the inequality was formally equivalent when I replace $(x)$ with $(y)$ and viceversa: $$ \frac{\sqrt{3\left(x-\sqrt{xy}+y\right)^2}}{\sqrt{3}(x^2-y^2)}-\frac{y}{x^2-y^2} \ge \frac{\sqrt{x^2+xy+y^2}}{\sqrt{3}(y^2-x^2)}+\frac{x}{x^2-y^2}. $$ However I could see, through an online grapher, that it is false!! So I remained without any chance to conclude the limit. ============ Is there anybody who knows why the last inequality isn't correct? And also, has anybody some hints to evaluate the limit?","I'm trying to evaluate over the domain , . ============ My attempt: ; so if the limit exists it must be . I tried to evaluate the limits ""near"" where, I thought, there's may be some problems: . Then I convinced myself the limit could be : since over the domain, I had to find such that: 1. 2. Where the last step follows by with , . From , for every with : From here I observated that and eventually for . I thought that for , the inequality was formally equivalent when I replace with and viceversa: However I could see, through an online grapher, that it is false!! So I remained without any chance to conclude the limit. ============ Is there anybody who knows why the last inequality isn't correct? And also, has anybody some hints to evaluate the limit?"," 
\lim_{(x,y)\to (0,0)} \frac{x-\sqrt{xy}}{x^2-y^2}
 x>0 y>0 f(x,x^2)\to +\infty +\infty (x,x) f(x, x-x^2)\to +\infty +\infty f(x,y)>0 g(x,y) f(x,y) \ge g(x,y)  \lim_{(x,y)\to (0,0)} g(x,y)\to +\infty  
f(x,y)=\frac{x-\sqrt{xy}}{x^2-y^2}=\frac{x-\sqrt{xy}+y-y}{x^2-y^2}=\frac{x-\sqrt{xy}+y}{x^2-y^2}-\frac{y}{x^2-y^2}=\frac{\sqrt{\left(x-\sqrt{xy}+y\right)^2}}{x^2-y^2}-\frac{y}{x^2-y^2}
 (x-\sqrt{xy}+y) \ge 0 x>0 y>0 
\frac{\sqrt{\left(x-\sqrt{xy}+y\right)^2}}{x^2-y^2}-\frac{y}{x^2-y^2} = \frac{\sqrt{3\left(x-\sqrt{xy}+y\right)^2}}{\sqrt{3}(x^2-y^2)}-\frac{y}{x^2-y^2}.
 \left[3\left(x-\sqrt{xy}+y\right)^2\right] \ge \left[x+xy+y^2\right] (x,y) (x>y) 
\frac{\sqrt{3\left(x-\sqrt{xy}+y\right)^2}}{\sqrt{3}(x^2-y^2)}-\frac{y}{x^2-y^2} \ge \frac{\sqrt{x^2+xy+y^2}}{\sqrt{3}(x^2-y^2)}+\frac{y}{y^2-x^2}.
  \left[\lim_{(x,y)\to (0,0)} g(x,y)\to +\infty \right]  \left[\lim_{(x,y)\to (0,0)} f(x,y)\to +\infty \right] (x>y) (y>x) (x) (y) 
\frac{\sqrt{3\left(x-\sqrt{xy}+y\right)^2}}{\sqrt{3}(x^2-y^2)}-\frac{y}{x^2-y^2} \ge \frac{\sqrt{x^2+xy+y^2}}{\sqrt{3}(y^2-x^2)}+\frac{x}{x^2-y^2}.
","['real-analysis', 'calculus', 'limits', 'multivariable-calculus']"
97,"Show $(\vec x\cdot\nabla)\vec F=t \frac{\partial F}{\partial t}$ where $\vec F(\vec x,t)=\vec B(t \vec x)$",Show  where,"(\vec x\cdot\nabla)\vec F=t \frac{\partial F}{\partial t} \vec F(\vec x,t)=\vec B(t \vec x)","I keep missing the factor of $t$ . My working: $$(\vec x\cdot\nabla)\vec F=(x_i \vec e_i\cdot\vec e_j\frac{\partial}{\partial x_j})F_k\vec e_k=x_j\frac{\partial F_k}{\partial x_j}\vec e_k$$ and $$\frac{\partial\vec F}{\partial t}=\frac{\partial}{\partial t}\vec B(t\vec x)=(\vec x\cdot\nabla B_i(t\vec x))\vec e_i=(x_j\vec e_j\cdot\frac{\partial B_i(t\vec x)}{\partial x_k}\vec e_k)\vec e_i=x_k\frac{\partial B_i(t\vec x)}{\partial x_k}\vec e_i=x_k\frac{F_i}{x_k}\vec e_i$$ which would seem to suggest that the two lines are equal, when there should be a factor of $t$ between them. I'm not sure I have differentiated correctly on the second line, but I can't see a better way. What is the correct way to partial differentiate a time dependent vector field with respect to time?","I keep missing the factor of . My working: and which would seem to suggest that the two lines are equal, when there should be a factor of between them. I'm not sure I have differentiated correctly on the second line, but I can't see a better way. What is the correct way to partial differentiate a time dependent vector field with respect to time?",t (\vec x\cdot\nabla)\vec F=(x_i \vec e_i\cdot\vec e_j\frac{\partial}{\partial x_j})F_k\vec e_k=x_j\frac{\partial F_k}{\partial x_j}\vec e_k \frac{\partial\vec F}{\partial t}=\frac{\partial}{\partial t}\vec B(t\vec x)=(\vec x\cdot\nabla B_i(t\vec x))\vec e_i=(x_j\vec e_j\cdot\frac{\partial B_i(t\vec x)}{\partial x_k}\vec e_k)\vec e_i=x_k\frac{\partial B_i(t\vec x)}{\partial x_k}\vec e_i=x_k\frac{F_i}{x_k}\vec e_i t,"['multivariable-calculus', 'vector-analysis', 'vector-fields']"
98,Questions about a proof of Stokes' theorem in my calculus 2 lecture notes,Questions about a proof of Stokes' theorem in my calculus 2 lecture notes,,"My lecture notes look to prove Stokes' theorem for the special case where a surface can be represented as the graph of some function, so $z=f(x,y)$ . The surface $S$ is parametrized as $r(x,y)=(x,y,f(x,y))$ , where $(x,y)$ is in the region $U$ in the $xy$ plane.  Now assume $U$ has a boundary curve $C_u$ and $S$ has a boundary curve $C_s$ . My first question is where it is said that the line integral of the vector field $v=(v_1,v_2,v_3)$ over the curve $C_s$ is equal to the line integral of the same vector field over the curve $C_u$ . Why is this the case? Lecture notes for my first question My second question is I believe about a total derivative but I'm not sure. My lecturer has written that since $z=f(x,y)$ , $dz=(f_x)dx+(f_y)dy$ where $f_x$ and $f_y$ denote the partial derivatives of $f$ with respect to $x$ and $y$ respectively and $dx, dy, dz$ denote normal differentials, not partial ones. Can someone also explain this equality to me, please? Lecture notes for my second question","My lecture notes look to prove Stokes' theorem for the special case where a surface can be represented as the graph of some function, so . The surface is parametrized as , where is in the region in the plane.  Now assume has a boundary curve and has a boundary curve . My first question is where it is said that the line integral of the vector field over the curve is equal to the line integral of the same vector field over the curve . Why is this the case? Lecture notes for my first question My second question is I believe about a total derivative but I'm not sure. My lecturer has written that since , where and denote the partial derivatives of with respect to and respectively and denote normal differentials, not partial ones. Can someone also explain this equality to me, please? Lecture notes for my second question","z=f(x,y) S r(x,y)=(x,y,f(x,y)) (x,y) U xy U C_u S C_s v=(v_1,v_2,v_3) C_s C_u z=f(x,y) dz=(f_x)dx+(f_y)dy f_x f_y f x y dx, dy, dz","['multivariable-calculus', 'stokes-theorem']"
99,Maximum value of $\frac{a}{1+bc} + \frac b{1+ac} + \frac{c}{1+ab}$ given $a^2 + b^2 + c^2 = 1$,Maximum value of  given,\frac{a}{1+bc} + \frac b{1+ac} + \frac{c}{1+ab} a^2 + b^2 + c^2 = 1,"Given that the constraint of $a, b, c$ , for which $a, b, c$ are non-negative real numbers, is $a^2+b^2+c^2=1,$ find the maximum value of $$\frac{a}{(1+bc)}+\frac b{(1+ac)}+\frac{c}{(1+ab)}.$$ For this question I have tried using this geometric method, hopefully it can be logically correct Do you all have actually better method(s) to solve this problem?","Given that the constraint of , for which are non-negative real numbers, is find the maximum value of For this question I have tried using this geometric method, hopefully it can be logically correct Do you all have actually better method(s) to solve this problem?","a, b, c a, b, c a^2+b^2+c^2=1, \frac{a}{(1+bc)}+\frac b{(1+ac)}+\frac{c}{(1+ab)}.","['calculus', 'multivariable-calculus', 'optimization', 'maxima-minima']"
