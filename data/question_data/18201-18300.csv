,title_raw,title_text,title_latex,body_raw,body_text,body_latex,tags
0,Why are real symmetric matrices diagonalizable?,Why are real symmetric matrices diagonalizable?,,"A matrix is diagonalizable iff it has a basis of eigenvectors. Now, why is this satisfied in case of a real symmetric matrix ?","A matrix is diagonalizable iff it has a basis of eigenvectors. Now, why is this satisfied in case of a real symmetric matrix ?",,"['linear-algebra', 'vector-spaces']"
1,Cauchy's integral formula for Cayley-Hamilton Theorem,Cauchy's integral formula for Cayley-Hamilton Theorem,,"I'm just working through Conway's book on complex analysis and I stumbled across this lovely exercise: Use Cauchy's Integral Formula to prove the Cayley-Hamilton Theorem: If $A$ is an $n \times n$ matrix over $\mathbb C$ and $f(z)  = \det(z-A)$ is the characteristic polynomial of $A$ then $f(A) = 0$. (This exercise was taken from a paper by C. A. McCarthy, Amer. Math. Monthly , 82 (1975), 390-391) Unfortunately, I was not able to find said paper. I'm completely lost with this exercise. I can't even start to imagine how one could possibly make use of Cauchy here... Thanks for any hints. Regards, S.L.","I'm just working through Conway's book on complex analysis and I stumbled across this lovely exercise: Use Cauchy's Integral Formula to prove the Cayley-Hamilton Theorem: If $A$ is an $n \times n$ matrix over $\mathbb C$ and $f(z)  = \det(z-A)$ is the characteristic polynomial of $A$ then $f(A) = 0$. (This exercise was taken from a paper by C. A. McCarthy, Amer. Math. Monthly , 82 (1975), 390-391) Unfortunately, I was not able to find said paper. I'm completely lost with this exercise. I can't even start to imagine how one could possibly make use of Cauchy here... Thanks for any hints. Regards, S.L.",,"['linear-algebra', 'complex-analysis', 'cauchy-integral-formula', 'functional-calculus', 'cayley-hamilton']"
2,Non-zero eigenvalues of $AA^T$ and $A^TA$,Non-zero eigenvalues of  and,AA^T A^TA,"As a part of an exercise I have to prove the following: Let $A$ be an $(n \times m)$ matrix. Let $A^T$ be the transposed matrix of $A$. Then $AA^T$ is an $(n \times n)$ matrix and $A^TA$ is an $(m \times m)$ matrix. $AA^T$ then has a total of $n$ eigenvalues and $A^TA$ has a total of $m$ eigenvalues. What I need to prove is the following: $AA^T$ has an eigenvalue $\mu \not = 0$ $\Longleftrightarrow$ $A^TA$ has an eigenvalue $\mu \not = 0$ In other words, they have the same non-zero eigenvalues, and if one has more eigenvalues than the other, then these are all equal to $0$. How can I prove this? Thanks and regards.","As a part of an exercise I have to prove the following: Let $A$ be an $(n \times m)$ matrix. Let $A^T$ be the transposed matrix of $A$. Then $AA^T$ is an $(n \times n)$ matrix and $A^TA$ is an $(m \times m)$ matrix. $AA^T$ then has a total of $n$ eigenvalues and $A^TA$ has a total of $m$ eigenvalues. What I need to prove is the following: $AA^T$ has an eigenvalue $\mu \not = 0$ $\Longleftrightarrow$ $A^TA$ has an eigenvalue $\mu \not = 0$ In other words, they have the same non-zero eigenvalues, and if one has more eigenvalues than the other, then these are all equal to $0$. How can I prove this? Thanks and regards.",,['linear-algebra']
3,The inverse of a positive definite matrix is also positive definite,The inverse of a positive definite matrix is also positive definite,,"Let $K$ be nonsingular symmetric matrix, prove that if $K$ is positive definite so is $K^{-1}$ . My attempt: I have that $K = K^T$ so $x^TKx = x^TK^Tx = (xK)^Tx = (xIK)^Tx$ and then I don't know what to do next.","Let be nonsingular symmetric matrix, prove that if is positive definite so is . My attempt: I have that so and then I don't know what to do next.",K K K^{-1} K = K^T x^TKx = x^TK^Tx = (xK)^Tx = (xIK)^Tx,"['linear-algebra', 'matrices', 'inverse', 'symmetric-matrices', 'positive-definite']"
4,Insights into linear algebra from abstract algebra,Insights into linear algebra from abstract algebra,,"I use linear algebra quite a lot in applications, but I do not have a very strong abstract algebra background (i.e. around the level of an intro course, just knowing the basics of rings, groups, ideals, the first isomorphism theorem). Of course, the latter is far more general, so I was interested in how it can given insight into the former. For instance, I thought it was interesting to look at the set of rotations as the group $SO(3)$. So, essentially I'm curious as to what insights one can glean from ""viewing linear algebra though an abstract algebra lens"". Not necessarily practical tools, but rather more important are ones that aid in understanding or intuition (i.e. provide something new). As a particular example, what are the relations of vector spaces to these abstract structures, and is viewing them from that point of view helpful?","I use linear algebra quite a lot in applications, but I do not have a very strong abstract algebra background (i.e. around the level of an intro course, just knowing the basics of rings, groups, ideals, the first isomorphism theorem). Of course, the latter is far more general, so I was interested in how it can given insight into the former. For instance, I thought it was interesting to look at the set of rotations as the group $SO(3)$. So, essentially I'm curious as to what insights one can glean from ""viewing linear algebra though an abstract algebra lens"". Not necessarily practical tools, but rather more important are ones that aid in understanding or intuition (i.e. provide something new). As a particular example, what are the relations of vector spaces to these abstract structures, and is viewing them from that point of view helpful?",,"['linear-algebra', 'abstract-algebra', 'soft-question']"
5,"Basis-free, field-independent definition of determinants?","Basis-free, field-independent definition of determinants?",,"Let $T$ be a linear operator on a finite-dimensional vector space $V$ over the field $K$ , with $\dim V=n$ . Is there a definition of the determinant of $T$ that (1) does not make reference to a particular basis of $V$ , and (2) does not require $K$ to be a particular field? As motivation, if $K=\mathbb C$ , I know of three ways to define $\det(T)$ , two of which refer to a choice of basis, and the other of which relies on $\mathbb C$ being algebraically closed: Choose an ordered basis $B$ of $T$ , and let $\mathcal M(T)$ denote the matrix of $T$ with respect to this basis. Then apply any of the formulas/algorithms for calculating a determinant to $\mathcal M(T).$ [This works for any field, but requires choosing a basis to express $\mathcal M(T)$ .] Choose an ordered basis $B$ of $T$ , and let $\det_n$ be an alternating multilinear map from $V^n\to K$ . Then the determinant of $T$ can be defined as $\det_n(TB)/\det_n(B)$ . [This works for any field, but requires choosing a basis to extract ""column vectors of the matrix of $T$ .""] Define $\det(T)$ as the product of eigenvalues of $T$ , repeated according to their algebraic multiplicity. [This makes no reference to a basis, but only works because $\mathbb C$ is algebraically closed.]","Let be a linear operator on a finite-dimensional vector space over the field , with . Is there a definition of the determinant of that (1) does not make reference to a particular basis of , and (2) does not require to be a particular field? As motivation, if , I know of three ways to define , two of which refer to a choice of basis, and the other of which relies on being algebraically closed: Choose an ordered basis of , and let denote the matrix of with respect to this basis. Then apply any of the formulas/algorithms for calculating a determinant to [This works for any field, but requires choosing a basis to express .] Choose an ordered basis of , and let be an alternating multilinear map from . Then the determinant of can be defined as . [This works for any field, but requires choosing a basis to extract ""column vectors of the matrix of .""] Define as the product of eigenvalues of , repeated according to their algebraic multiplicity. [This makes no reference to a basis, but only works because is algebraically closed.]",T V K \dim V=n T V K K=\mathbb C \det(T) \mathbb C B T \mathcal M(T) T \mathcal M(T). \mathcal M(T) B T \det_n V^n\to K T \det_n(TB)/\det_n(B) T \det(T) T \mathbb C,"['linear-algebra', 'matrices', 'linear-transformations', 'determinant']"
6,Max dimension of a subspace of singular $n\times n$ matrices,Max dimension of a subspace of singular  matrices,n\times n,"I am sure the answer to this is (kind of) well known. I've searched the web and the site for a proof and found nothing, and if this is a duplicate, I'm sorry. The following question was given in a contest I took part. I had an approach but it didn't solve the problem. Consider $V$ a linear subspace of the real vector space $\mathcal{M}_n(\Bbb{R})$ ($n\times n$ real entries matrices) such that $V$ contains only singular matrices (i.e matrices with determinant equal to $0$). What is the maximal dimension of $V$? A quick guess would be $n^2-n$ since if we consider $W$ the set of $n\times n$ real matrices with last line equal to $0$ then this space has dimension $n^2-n$ and it is a linear space of singular matrices. Now the only thing there is to prove is that if $V$ is a subspace of $\mathcal{M}_n(\Bbb{R})$ of dimension $k > n^2-n$ then $V$ contains a non-singular matrix. The official proof was unsatisfactory for me, because it was a combinatorial one, and seemed to have few things in common with linear algebra. I was hoping for a pure linear algebra proof. My approach was to search for a permutation matrix in $V$, but I used some 'false theorem' in between, which I am ashamed to post here.","I am sure the answer to this is (kind of) well known. I've searched the web and the site for a proof and found nothing, and if this is a duplicate, I'm sorry. The following question was given in a contest I took part. I had an approach but it didn't solve the problem. Consider $V$ a linear subspace of the real vector space $\mathcal{M}_n(\Bbb{R})$ ($n\times n$ real entries matrices) such that $V$ contains only singular matrices (i.e matrices with determinant equal to $0$). What is the maximal dimension of $V$? A quick guess would be $n^2-n$ since if we consider $W$ the set of $n\times n$ real matrices with last line equal to $0$ then this space has dimension $n^2-n$ and it is a linear space of singular matrices. Now the only thing there is to prove is that if $V$ is a subspace of $\mathcal{M}_n(\Bbb{R})$ of dimension $k > n^2-n$ then $V$ contains a non-singular matrix. The official proof was unsatisfactory for me, because it was a combinatorial one, and seemed to have few things in common with linear algebra. I was hoping for a pure linear algebra proof. My approach was to search for a permutation matrix in $V$, but I used some 'false theorem' in between, which I am ashamed to post here.",,"['linear-algebra', 'matrices']"
7,Product of inverse matrices $ (AB)^{-1}$,Product of inverse matrices, (AB)^{-1},I am unsure how to go about doing this inverse product problem: The question says to find the value of each matrix expression where A and B are the invertible 3 x 3 matrices such that  $$A^{-1} = \left(\begin{array}{ccc}1& 2& 3\\ 2& 0& 1\\ 1& 1& -1\end{array}\right) $$ and  $$B^{-1}=\left(\begin{array}{ccc}2 &-1 &3\\ 0& 0 &4\\ 3& -2 & 1\end{array}\right) $$ The actual question is to find $ (AB)^{-1}$. $ (AB)^{-1}$ is just $ A^{-1}B^{-1}$ and we already know matrices $ A^{-1}$ and $ B^{-1}$ so taking the product should give us the matrix  $$\left(\begin{array}{ccc}11 &-7 &14\\ 7& -4 &7\\ -1& 1 & 6\end{array}\right) $$ yet the answer is  $$ \left(\begin{array}{ccc} 3 &7 &2 \\ 4& 4 &-4\\ 0 & 7 & 6 \end{array}\right) $$ What am I not understanding about the problem or what am I doing wrong? Isn't this just matrix multiplication?,I am unsure how to go about doing this inverse product problem: The question says to find the value of each matrix expression where A and B are the invertible 3 x 3 matrices such that  $$A^{-1} = \left(\begin{array}{ccc}1& 2& 3\\ 2& 0& 1\\ 1& 1& -1\end{array}\right) $$ and  $$B^{-1}=\left(\begin{array}{ccc}2 &-1 &3\\ 0& 0 &4\\ 3& -2 & 1\end{array}\right) $$ The actual question is to find $ (AB)^{-1}$. $ (AB)^{-1}$ is just $ A^{-1}B^{-1}$ and we already know matrices $ A^{-1}$ and $ B^{-1}$ so taking the product should give us the matrix  $$\left(\begin{array}{ccc}11 &-7 &14\\ 7& -4 &7\\ -1& 1 & 6\end{array}\right) $$ yet the answer is  $$ \left(\begin{array}{ccc} 3 &7 &2 \\ 4& 4 &-4\\ 0 & 7 & 6 \end{array}\right) $$ What am I not understanding about the problem or what am I doing wrong? Isn't this just matrix multiplication?,,"['linear-algebra', 'matrices', 'inverse', 'matrix-equations']"
8,"To prove Cayley-Hamilton theorem, why can't we substitute $A$ for $\lambda$ in $p(\lambda) = \det(\lambda I - A)$?","To prove Cayley-Hamilton theorem, why can't we substitute  for  in ?",A \lambda p(\lambda) = \det(\lambda I - A),"The Cayley-Hamilton Theorem states that if $A$ is an $n \times n$ matrix over the field $F$ then $p(A) = 0,$ where $p(x)$ is the characteristic polynomial of $A,$ now considered as defined over an appropriate matrix algebra. We note that $p(\lambda) = \det(\lambda I - A)$ . Hence, why can't we just replace $\lambda$ with $A$ and directly prove the Cayley-Hamilton Theorem by saying that $p(A) = \det(AI - A) = 0$ ?","The Cayley-Hamilton Theorem states that if is an matrix over the field then where is the characteristic polynomial of now considered as defined over an appropriate matrix algebra. We note that . Hence, why can't we just replace with and directly prove the Cayley-Hamilton Theorem by saying that ?","A n \times n F p(A) = 0, p(x) A, p(\lambda) = \det(\lambda I - A) \lambda A p(A) = \det(AI - A) = 0","['linear-algebra', 'matrices']"
9,Frobenius norm of product of matrix,Frobenius norm of product of matrix,,"The Frobenius norm of a $m \times n$ matrix $F$ is defined as $$\| F \|_F^2 := \sum_{i=1}^m\sum_{j=1}^n |f_{i,j}|^2$$ If I have $FG$, where $G$ is a $n \times p$ matrix, can we say the following? $$\| F G \|_F^2 = \|F\|_F^2 \|G\|_F^2$$ Also, what does Frobenius norm mean? Is it analogous to the magnitude of a vector, but for matrix?","The Frobenius norm of a $m \times n$ matrix $F$ is defined as $$\| F \|_F^2 := \sum_{i=1}^m\sum_{j=1}^n |f_{i,j}|^2$$ If I have $FG$, where $G$ is a $n \times p$ matrix, can we say the following? $$\| F G \|_F^2 = \|F\|_F^2 \|G\|_F^2$$ Also, what does Frobenius norm mean? Is it analogous to the magnitude of a vector, but for matrix?",,"['linear-algebra', 'matrices', 'normed-spaces', 'matrix-norms']"
10,Do positive semidefinite matrices have to be symmetric?,Do positive semidefinite matrices have to be symmetric?,,"Do positive semidefinite matrices have to be symmetric? Can you have a non-symmetric matrix that is positive definite? I can't seem to figure out why you wouldn't be able to have such a matrix, but all my notes specify positive definite matrices as ""symmetric $n \times n$ matrices."" Can anyone help me with an example of a non-symmetric positive definite matrix, or some insight into a proof for why it would need to be symmetric should that be the case? Thanks!","Do positive semidefinite matrices have to be symmetric? Can you have a non-symmetric matrix that is positive definite? I can't seem to figure out why you wouldn't be able to have such a matrix, but all my notes specify positive definite matrices as ""symmetric matrices."" Can anyone help me with an example of a non-symmetric positive definite matrix, or some insight into a proof for why it would need to be symmetric should that be the case? Thanks!",n \times n,"['linear-algebra', 'matrices', 'symmetric-matrices', 'positive-definite', 'positive-semidefinite']"
11,Slick proof the determinant is an irreducible polynomial,Slick proof the determinant is an irreducible polynomial,,"A polynomial $p$ over a field $k$ is called irreducible if $p=fg$ for polynomials $f,g$ implies $f$ or $g$ are constant. One can consider the determinant of an $n\times n$ matrix to be a polynomial in $n^2$ variables. Does anyone know of a slick way to prove this polynomial is irreducible? It feels like this should follow quite easily from basic properties of the determinant or an induction argument, but I cannot think of a nice proof. One consequence of this fact is that $GL_n$ is the complement of a hypersurface in $M_{n}$. Thanks.","A polynomial $p$ over a field $k$ is called irreducible if $p=fg$ for polynomials $f,g$ implies $f$ or $g$ are constant. One can consider the determinant of an $n\times n$ matrix to be a polynomial in $n^2$ variables. Does anyone know of a slick way to prove this polynomial is irreducible? It feels like this should follow quite easily from basic properties of the determinant or an induction argument, but I cannot think of a nice proof. One consequence of this fact is that $GL_n$ is the complement of a hypersurface in $M_{n}$. Thanks.",,"['linear-algebra', 'algebraic-geometry', 'determinant']"
12,Wedge product and cross product - any difference?,Wedge product and cross product - any difference?,,"I'm taking a course in differential geometry, and have here been introduced to the wedge product of two vectors defined (in Differential Geometry of Curves and Surfaces by Manfredo Perdigão do Carmo ) by: Let $\mathbf{u}$, $\mathbf{v}$ be in $\mathbb{R}^3$. $\mathbf{u}\wedge\mathbf{v}$ in $\mathbb{R}^3$ is the unique vector that satisfies: $(\mathbf{u}\wedge\mathbf{v})\cdot\mathbf{w} = \det\;(\mathbf{u}\;\mathbf{v}\;\mathbf{w})$ for all $\mathbf{w}$ in $\mathbb{R}^3$ And to clarify, $(\mathbf{u}\;\mathbf{v}\;\mathbf{w})$ is the 3×3 matrix with $\mathbf{u}$, $\mathbf{v}$ and $\mathbf{w}$ as its columns, in that order. My question: is there any difference between this and the regular cross product or vector product of two vectors, as long as we stay in $\mathbb{R}^3$? And if there is no difference, then why introduce the wedge? Cheers!","I'm taking a course in differential geometry, and have here been introduced to the wedge product of two vectors defined (in Differential Geometry of Curves and Surfaces by Manfredo Perdigão do Carmo ) by: Let $\mathbf{u}$, $\mathbf{v}$ be in $\mathbb{R}^3$. $\mathbf{u}\wedge\mathbf{v}$ in $\mathbb{R}^3$ is the unique vector that satisfies: $(\mathbf{u}\wedge\mathbf{v})\cdot\mathbf{w} = \det\;(\mathbf{u}\;\mathbf{v}\;\mathbf{w})$ for all $\mathbf{w}$ in $\mathbb{R}^3$ And to clarify, $(\mathbf{u}\;\mathbf{v}\;\mathbf{w})$ is the 3×3 matrix with $\mathbf{u}$, $\mathbf{v}$ and $\mathbf{w}$ as its columns, in that order. My question: is there any difference between this and the regular cross product or vector product of two vectors, as long as we stay in $\mathbb{R}^3$? And if there is no difference, then why introduce the wedge? Cheers!",,"['linear-algebra', 'terminology', 'differential-geometry', 'cross-product', 'exterior-algebra']"
13,Integral of matrix exponential,Integral of matrix exponential,,"Let $A$ be an $n \times n$ matrix. Then the solution of the initial value problem \begin{align*}   \dot{x}(t) = A x(t), \quad x(0) = x_0 \end{align*} is given by $x(t) = \mathrm{e}^{At} x_0$. I am interested in the following matrix \begin{align*}    \int_{0}^T \mathrm{e}^{At}\, dt \end{align*} for some $T>0$. Can one write down a general solution to this without distinguishing cases (e.g. $A$ nonsingular)? Is this matrix always invertible?","Let $A$ be an $n \times n$ matrix. Then the solution of the initial value problem \begin{align*}   \dot{x}(t) = A x(t), \quad x(0) = x_0 \end{align*} is given by $x(t) = \mathrm{e}^{At} x_0$. I am interested in the following matrix \begin{align*}    \int_{0}^T \mathrm{e}^{At}\, dt \end{align*} for some $T>0$. Can one write down a general solution to this without distinguishing cases (e.g. $A$ nonsingular)? Is this matrix always invertible?",,"['linear-algebra', 'matrices', 'definite-integrals', 'matrix-calculus', 'matrix-exponential']"
14,Generalized rotation matrix in N dimensional space around N-2 unit vector,Generalized rotation matrix in N dimensional space around N-2 unit vector,,"There is a 2d rotation matrix around point $(0, 0)$ with angle $\theta$. $$ \left[ \begin{array}{ccc} \cos(\theta) & -\sin(\theta) \\ \sin(\theta) & \cos(\theta) \end{array} \right] $$ Next, there is a 3d rotation matrix around point $(0, 0, 0)$ and unit axis $(u_x, u_y, u_z)$ with angle $\theta$ ( Rodrigues' Rotation Formula ). \begin{bmatrix} \cos \theta +u_x^2 \left(1-\cos \theta\right) & u_x u_y \left(1-\cos \theta\right) - u_z \sin \theta & u_x u_z \left(1-\cos \theta\right) + u_y \sin \theta \\ u_y u_x \left(1-\cos \theta\right) + u_z \sin \theta & \cos \theta + u_y^2\left(1-\cos \theta\right) & u_y u_z \left(1-\cos \theta\right) - u_x \sin \theta \\ u_z u_x \left(1-\cos \theta\right) - u_y \sin \theta & u_z u_y \left(1-\cos \theta\right) + u_x \sin \theta & \cos \theta + u_z^2\left(1-\cos \theta\right)  \end{bmatrix} How it is possible to generalize rotation matrix on $N$ dimension around zero point and $N-2$ dimensional unit axis with angle $\theta$?","There is a 2d rotation matrix around point $(0, 0)$ with angle $\theta$. $$ \left[ \begin{array}{ccc} \cos(\theta) & -\sin(\theta) \\ \sin(\theta) & \cos(\theta) \end{array} \right] $$ Next, there is a 3d rotation matrix around point $(0, 0, 0)$ and unit axis $(u_x, u_y, u_z)$ with angle $\theta$ ( Rodrigues' Rotation Formula ). \begin{bmatrix} \cos \theta +u_x^2 \left(1-\cos \theta\right) & u_x u_y \left(1-\cos \theta\right) - u_z \sin \theta & u_x u_z \left(1-\cos \theta\right) + u_y \sin \theta \\ u_y u_x \left(1-\cos \theta\right) + u_z \sin \theta & \cos \theta + u_y^2\left(1-\cos \theta\right) & u_y u_z \left(1-\cos \theta\right) - u_x \sin \theta \\ u_z u_x \left(1-\cos \theta\right) - u_y \sin \theta & u_z u_y \left(1-\cos \theta\right) + u_x \sin \theta & \cos \theta + u_z^2\left(1-\cos \theta\right)  \end{bmatrix} How it is possible to generalize rotation matrix on $N$ dimension around zero point and $N-2$ dimensional unit axis with angle $\theta$?",,"['linear-algebra', 'matrices', 'rotations']"
15,Meaning of the identity $\det(A+B)+\text{tr}(AB) = \det(A)+\det(B) + \text{tr}(A)\text{tr}(B)$ (in dimension $2$),Meaning of the identity  (in dimension ),\det(A+B)+\text{tr}(AB) = \det(A)+\det(B) + \text{tr}(A)\text{tr}(B) 2,"Throughout, $A$ and $B$ denote $n \times n$ matrices over $\mathbb{C}$. Everyone knows that the determinant is multiplicative, and the trace is additive (actually linear). \begin{align*} \det(AB) = \det(A)\det(B) && \mathrm{tr}(A+B)= \mathrm{tr}(A) + \mathrm{tr}(B). \end{align*} On the other hand, the opposite equations \begin{align} \det(A+B) = \det(A)+\det(B) && \mathrm{tr}(AB)= \mathrm{tr}(A)\mathrm{tr}(B) \tag{1}. \end{align} don't hold for all $A,B$ unless $n=1$. For instance, taking $A=B=I$ in (1), we get \begin{align*} \det(A+B) = 2^n && \det(A)+\det(B) = 2 && \mathrm{tr}(AB)= n && \mathrm{tr}(A)\mathrm{tr}(B) = n^2. \end{align*} When $n=2$ a very curious thing happens, which is that, even though the equations (1) are typically false, their sum is actually valid. That is, \begin{align} \det(A+B) +  \mathrm{tr}(AB)= \det(A)+\det(B)+  \mathrm{tr}(A)\mathrm{tr}(B) \tag{2} \end{align} for all $A$ and $B$ when $n=2$. However, (2) does not hold when $n>2$. Indeed, specializing to $B=I$ in (2), we get \begin{align} \det(A+I) +  \mathrm{tr}(A)= \det(A)+1+  n \cdot \mathrm{tr}(A) \end{align} or, equivalently, \begin{align} \det(A+I) = \det(A)+(n-1) \cdot \mathrm{tr}(A) +1 \tag{3}. \end{align} If $n \geq 2$ and $A$ is projection onto the first coordinate, we have \begin{align} \det(A+I) = 2 && \det(A)+(n-1)\cdot \mathrm{tr}(A) +1 = n, \end{align} so (3) is only an identity for $n=2$ (or, trivially, when $n=1$). Question: Is there any special significance to the equation   \begin{align} \det(A+B) +  \mathrm{tr}(AB)= \det(A)+\det(B)+  \mathrm{tr}(A)\mathrm{tr}(B) , \end{align}   which is valid for all $2 \times 2$ matrices? It seems very strange to me that the sum of two obviously false equations should turn out true. Are there any nice applications of this identity?","Throughout, $A$ and $B$ denote $n \times n$ matrices over $\mathbb{C}$. Everyone knows that the determinant is multiplicative, and the trace is additive (actually linear). \begin{align*} \det(AB) = \det(A)\det(B) && \mathrm{tr}(A+B)= \mathrm{tr}(A) + \mathrm{tr}(B). \end{align*} On the other hand, the opposite equations \begin{align} \det(A+B) = \det(A)+\det(B) && \mathrm{tr}(AB)= \mathrm{tr}(A)\mathrm{tr}(B) \tag{1}. \end{align} don't hold for all $A,B$ unless $n=1$. For instance, taking $A=B=I$ in (1), we get \begin{align*} \det(A+B) = 2^n && \det(A)+\det(B) = 2 && \mathrm{tr}(AB)= n && \mathrm{tr}(A)\mathrm{tr}(B) = n^2. \end{align*} When $n=2$ a very curious thing happens, which is that, even though the equations (1) are typically false, their sum is actually valid. That is, \begin{align} \det(A+B) +  \mathrm{tr}(AB)= \det(A)+\det(B)+  \mathrm{tr}(A)\mathrm{tr}(B) \tag{2} \end{align} for all $A$ and $B$ when $n=2$. However, (2) does not hold when $n>2$. Indeed, specializing to $B=I$ in (2), we get \begin{align} \det(A+I) +  \mathrm{tr}(A)= \det(A)+1+  n \cdot \mathrm{tr}(A) \end{align} or, equivalently, \begin{align} \det(A+I) = \det(A)+(n-1) \cdot \mathrm{tr}(A) +1 \tag{3}. \end{align} If $n \geq 2$ and $A$ is projection onto the first coordinate, we have \begin{align} \det(A+I) = 2 && \det(A)+(n-1)\cdot \mathrm{tr}(A) +1 = n, \end{align} so (3) is only an identity for $n=2$ (or, trivially, when $n=1$). Question: Is there any special significance to the equation   \begin{align} \det(A+B) +  \mathrm{tr}(AB)= \det(A)+\det(B)+  \mathrm{tr}(A)\mathrm{tr}(B) , \end{align}   which is valid for all $2 \times 2$ matrices? It seems very strange to me that the sum of two obviously false equations should turn out true. Are there any nice applications of this identity?",,"['linear-algebra', 'reference-request', 'determinant', 'quadratic-forms', 'trace']"
16,Which polynomials are characteristic polynomials of a symmetric matrix?,Which polynomials are characteristic polynomials of a symmetric matrix?,,"Let $f(x)$ be a polynomial of degree $n$ with coefficients in $\mathbb{Q}$.  There are well-known ways to construct a $n \times n$ matrix $A$ with entries in $\mathbb{Q}$ whose characteristic polynomial is $f$.  My question is: when is it possible to choose $A$ symmetric? An obvious necessary condition is that the roots of $f$ are all real, but it is not clear to me even in the case $n = 2$ that this is sufficient.  In degree $2$ this comes down to determining whether or not every pair $(p,q)$ which satisfies $p^2 > 4q$ (the condition that $x^2 + px + q$ has real roots) can be expressed in the form $$p = -(a + c)$$ $$q = ac - b^2$$ where $a$, $b$, and $c$ are rational.  I have some partial results from just fumbling around and checking cases, but it seems clear that a more conceptual argument would be required to handle larger degrees.","Let $f(x)$ be a polynomial of degree $n$ with coefficients in $\mathbb{Q}$.  There are well-known ways to construct a $n \times n$ matrix $A$ with entries in $\mathbb{Q}$ whose characteristic polynomial is $f$.  My question is: when is it possible to choose $A$ symmetric? An obvious necessary condition is that the roots of $f$ are all real, but it is not clear to me even in the case $n = 2$ that this is sufficient.  In degree $2$ this comes down to determining whether or not every pair $(p,q)$ which satisfies $p^2 > 4q$ (the condition that $x^2 + px + q$ has real roots) can be expressed in the form $$p = -(a + c)$$ $$q = ac - b^2$$ where $a$, $b$, and $c$ are rational.  I have some partial results from just fumbling around and checking cases, but it seems clear that a more conceptual argument would be required to handle larger degrees.",,"['linear-algebra', 'matrices', 'polynomials', 'symmetric-matrices', 'characteristic-polynomial']"
17,"How to efficiently use a calculator in a linear algebra exam, if allowed","How to efficiently use a calculator in a linear algebra exam, if allowed",,"We are allowed to use a calculator in our linear algebra exam. Luckily, my calculator can also do matrix calculations. Let's say there is a task like this: Calculate the rank of this matrix: $$M =\begin{pmatrix} 5 & 6 & 7\\  12 &4  &9 \\  1 & 7 & 4 \end{pmatrix}$$ The problem with this matrix is we cannot use the trick with multiples, we cannot see multiples on first glance and thus cannot say whether the vectors rows / columns are linearly in/dependent. Using Gauss is also very time consuming (especially in case we don't get a zero line and keep trying harder). Enough said, I took my calculator because we are allowed to use it and it gives me following results: $$M =\begin{pmatrix} 1 & 0{,}3333 & 0{,}75\\  0 &1  &0{,}75 \\  0 & 0 & 1 \end{pmatrix}$$ I quickly see that $\text{rank(M)} = 3$ since there is no row full of zeroes. Now my question is, how can I convince the teacher that I calculated it? If the task says ""calculate"" and I just write down the result, I don't think I will get all the points. What would you do? And please give me some advice, this is really time consuming in an exam.","We are allowed to use a calculator in our linear algebra exam. Luckily, my calculator can also do matrix calculations. Let's say there is a task like this: Calculate the rank of this matrix: $$M =\begin{pmatrix} 5 & 6 & 7\\  12 &4  &9 \\  1 & 7 & 4 \end{pmatrix}$$ The problem with this matrix is we cannot use the trick with multiples, we cannot see multiples on first glance and thus cannot say whether the vectors rows / columns are linearly in/dependent. Using Gauss is also very time consuming (especially in case we don't get a zero line and keep trying harder). Enough said, I took my calculator because we are allowed to use it and it gives me following results: $$M =\begin{pmatrix} 1 & 0{,}3333 & 0{,}75\\  0 &1  &0{,}75 \\  0 & 0 & 1 \end{pmatrix}$$ I quickly see that $\text{rank(M)} = 3$ since there is no row full of zeroes. Now my question is, how can I convince the teacher that I calculated it? If the task says ""calculate"" and I just write down the result, I don't think I will get all the points. What would you do? And please give me some advice, this is really time consuming in an exam.",,"['linear-algebra', 'matrices', 'vector-spaces', 'proof-writing']"
18,$2\times2$ matrices are not big enough,matrices are not big enough,2\times2,"Olga Tausky-Todd had once said that ""If an assertion about matrices is false, there is usually a 2x2 matrix that reveals this."" There are, however, assertions about matrices that are true for $2\times2$ matrices but not for the larger ones. I came across one nice little example yesterday. Actually, every student who has studied first-year linear algebra should know that there are even assertions that are true for $3\times3$ matrices, but false for larger ones --- the rule of Sarrus is one obvious example; a question I answered last year provides another. So, here is my question. What is your favourite assertion that is true for small matrices but not for larger ones? Here, $1\times1$ matrices are ignored because they form special cases too easily (otherwise, Tausky-Todd would have not made the above comment). The assertions are preferrably simple enough to understand, but their disproofs for larger matrices can be advanced or difficult.","Olga Tausky-Todd had once said that ""If an assertion about matrices is false, there is usually a 2x2 matrix that reveals this."" There are, however, assertions about matrices that are true for $2\times2$ matrices but not for the larger ones. I came across one nice little example yesterday. Actually, every student who has studied first-year linear algebra should know that there are even assertions that are true for $3\times3$ matrices, but false for larger ones --- the rule of Sarrus is one obvious example; a question I answered last year provides another. So, here is my question. What is your favourite assertion that is true for small matrices but not for larger ones? Here, $1\times1$ matrices are ignored because they form special cases too easily (otherwise, Tausky-Todd would have not made the above comment). The assertions are preferrably simple enough to understand, but their disproofs for larger matrices can be advanced or difficult.",,"['linear-algebra', 'matrices', 'soft-question', 'education']"
19,why don't we define vector multiplication component-wise?,why don't we define vector multiplication component-wise?,,"I was just wondering why we don't ever define multiplication of vectors as individual component multiplication.  That is, why doesn't anybody ever define $\langle a_1,b_1 \rangle \cdot \langle a_2,b_2 \rangle$ to be $\langle a_1a_2, b_1b_2 \rangle$?  Is the resulting vector just not geometrically interesting?","I was just wondering why we don't ever define multiplication of vectors as individual component multiplication.  That is, why doesn't anybody ever define $\langle a_1,b_1 \rangle \cdot \langle a_2,b_2 \rangle$ to be $\langle a_1a_2, b_1b_2 \rangle$?  Is the resulting vector just not geometrically interesting?",,['linear-algebra']
20,Determinant of a matrix with diagonal entries $a$ and off-diagonal entries $b$ [duplicate],Determinant of a matrix with diagonal entries  and off-diagonal entries  [duplicate],a b,"This question already has answers here : Determinant of a rank $1$ update of a scalar matrix, or characteristic polynomial of a rank $1$ matrix (2 answers) Closed 3 years ago . The community reviewed whether to reopen this question 1 year ago and left it closed: Original close reason(s) were not resolved I have the following $n\times n$ matrix: $$A=\begin{bmatrix} a & b & \ldots & b\\ b & a & \ldots & b\\ \vdots & \vdots & \ddots & \vdots\\ b & b & \ldots & a\end{bmatrix}$$ where $0 < b < a$ . I am interested in the expression for the determinant $\det[A]$ in terms of $a$ , $b$ and $n$ .  This seems like a trivial problem, as the matrix $A$ has such a nice structure, but my linear algebra skills are pretty rusty and I can't figure it out.  Any help would be appreciated.","This question already has answers here : Determinant of a rank $1$ update of a scalar matrix, or characteristic polynomial of a rank $1$ matrix (2 answers) Closed 3 years ago . The community reviewed whether to reopen this question 1 year ago and left it closed: Original close reason(s) were not resolved I have the following matrix: where . I am interested in the expression for the determinant in terms of , and .  This seems like a trivial problem, as the matrix has such a nice structure, but my linear algebra skills are pretty rusty and I can't figure it out.  Any help would be appreciated.",n\times n A=\begin{bmatrix} a & b & \ldots & b\\ b & a & \ldots & b\\ \vdots & \vdots & \ddots & \vdots\\ b & b & \ldots & a\end{bmatrix} 0 < b < a \det[A] a b n A,"['linear-algebra', 'matrices', 'determinant']"
21,Is there a matrix of every size with all its submatrices invertible?,Is there a matrix of every size with all its submatrices invertible?,,"Let's call a real matrix of size $m \times n$ totally invertible if for every $k$ rows and $k$ columns that we choose, we get an invertible matrix. I am curious about the following: Is there a totally invertible matrix for all sizes $m \times n$? By taking the transpose, we can assume $m \leq n$. For $m=1$ we can take any vector in $\Bbb R^m$ without a zero entry. For $m=2$ we can take $\begin{pmatrix} 1 &2  & 3 & ... &n \\  2 & 3 & 4 & ... & n+1 \end{pmatrix}$. Indeed, $\det\begin{pmatrix} a &b \\  a+1 & b+1 \end{pmatrix}=a-b$ is nonzero when $a \neq b$. This does not generalize, since $a_{ij}=i+j-1$ fabulously fails for all submatrices of size $\geq 3$. I also tried taking the rows to be $m$ of the cyclic shifts of the vector $(1,2, ... ,n)$. This does not work either because I found a $k=3, m=n=5$ counterexample. I do feel that the answer should be positive, however. Is it?","Let's call a real matrix of size $m \times n$ totally invertible if for every $k$ rows and $k$ columns that we choose, we get an invertible matrix. I am curious about the following: Is there a totally invertible matrix for all sizes $m \times n$? By taking the transpose, we can assume $m \leq n$. For $m=1$ we can take any vector in $\Bbb R^m$ without a zero entry. For $m=2$ we can take $\begin{pmatrix} 1 &2  & 3 & ... &n \\  2 & 3 & 4 & ... & n+1 \end{pmatrix}$. Indeed, $\det\begin{pmatrix} a &b \\  a+1 & b+1 \end{pmatrix}=a-b$ is nonzero when $a \neq b$. This does not generalize, since $a_{ij}=i+j-1$ fabulously fails for all submatrices of size $\geq 3$. I also tried taking the rows to be $m$ of the cyclic shifts of the vector $(1,2, ... ,n)$. This does not work either because I found a $k=3, m=n=5$ counterexample. I do feel that the answer should be positive, however. Is it?",,"['linear-algebra', 'matrices', 'inverse']"
22,"If $A,B$ have eigenvalues $\pm1$, what can I say about the eigenvalues of $AB,BA$?","If  have eigenvalues , what can I say about the eigenvalues of ?","A,B \pm1 AB,BA","If $n\times n$ matrix $A$ has eigenvalues $1,-1$ and $n\times n$ matrix $B$ also has eigenvalues $1,-1$, can I then say something about eigenvalues of $AB$ and $BA$?","If $n\times n$ matrix $A$ has eigenvalues $1,-1$ and $n\times n$ matrix $B$ also has eigenvalues $1,-1$, can I then say something about eigenvalues of $AB$ and $BA$?",,"['linear-algebra', 'eigenvalues-eigenvectors']"
23,Explain proof that any positive definite matrix is invertible,Explain proof that any positive definite matrix is invertible,,"If an $n \times n$ symmetric A is positive definite, then all of its eigenvalues are positive, so $0$ is not an eigenvalue of $A$. Therefore, the system of equations $A\mathbf{x}=\mathbf{0}$ has no non-trivial solution, and so A is invertible. I don't get how knowing that $0$ is not an eigenvalue of $A$ enables us to conclude that $A\mathbf{x}=\mathbf{0}$ has the trivial solution only. In other words, how do we exclude the possibility that for all $\mathbf{x}$ that is not an eigenvector of $A$, $A\mathbf{x}=\mathbf{0}$ will not have a non-trivial solution?","If an $n \times n$ symmetric A is positive definite, then all of its eigenvalues are positive, so $0$ is not an eigenvalue of $A$. Therefore, the system of equations $A\mathbf{x}=\mathbf{0}$ has no non-trivial solution, and so A is invertible. I don't get how knowing that $0$ is not an eigenvalue of $A$ enables us to conclude that $A\mathbf{x}=\mathbf{0}$ has the trivial solution only. In other words, how do we exclude the possibility that for all $\mathbf{x}$ that is not an eigenvector of $A$, $A\mathbf{x}=\mathbf{0}$ will not have a non-trivial solution?",,"['linear-algebra', 'proof-writing', 'eigenvalues-eigenvectors']"
24,Fun Linear Algebra Problems,Fun Linear Algebra Problems,,"I'm teaching a linear algebra course this term (using Lay's book) and would like some fun ""challenge problems"" to give the students. The problems that I am looking for should be be easy to state and have a solution that Requires only knowledge that an average matrix algebra student would be expected to have (i.e. calculus and linear algebra, but not necessarily abstract algebra). Has a solution that requires cleverness, but not so much cleverness that only students with contest math backgrounds will be able to solve the problem. An example of the type of problem that I have in mind is: Fix an integer $n>1$. Let $S$ be the set of all $n \times n$ matrices whose entries are only zero and one. Show that the average determinant of a matrix in $S$ is zero.","I'm teaching a linear algebra course this term (using Lay's book) and would like some fun ""challenge problems"" to give the students. The problems that I am looking for should be be easy to state and have a solution that Requires only knowledge that an average matrix algebra student would be expected to have (i.e. calculus and linear algebra, but not necessarily abstract algebra). Has a solution that requires cleverness, but not so much cleverness that only students with contest math backgrounds will be able to solve the problem. An example of the type of problem that I have in mind is: Fix an integer $n>1$. Let $S$ be the set of all $n \times n$ matrices whose entries are only zero and one. Show that the average determinant of a matrix in $S$ is zero.",,['linear-algebra']
25,Divergence as transpose of gradient?,Divergence as transpose of gradient?,,"In his online lectures on Computational Science, Prof. Gilbert Strang often interprets divergence as the ""transpose"" of the gradient, for example here (at 32:30), however he does not explain the reason. How is it that the divergence can be interpreted as the transpose of the gradient?","In his online lectures on Computational Science, Prof. Gilbert Strang often interprets divergence as the ""transpose"" of the gradient, for example here (at 32:30), however he does not explain the reason. How is it that the divergence can be interpreted as the transpose of the gradient?",,"['linear-algebra', 'multivariable-calculus', 'divergence-operator']"
26,Real world uses of Quaternions?,Real world uses of Quaternions?,,"I've recently started reading about Quaternions, and I keep reading that for example they're used in computer graphics and mechanics calculations to calculate movement and rotation, but without real explanations of the benefits of using them. I'm wondering what exactly can be done with Quaternions that can't be done as easily (or easier) using more tradition approaches, such as with Vectors?","I've recently started reading about Quaternions, and I keep reading that for example they're used in computer graphics and mechanics calculations to calculate movement and rotation, but without real explanations of the benefits of using them. I'm wondering what exactly can be done with Quaternions that can't be done as easily (or easier) using more tradition approaches, such as with Vectors?",,"['soft-question', 'big-list', 'linear-algebra', 'applications', 'quaternions']"
27,Series expansion of the determinant for a matrix near the identity,Series expansion of the determinant for a matrix near the identity,,"The problem is to find the second order term in the series expansion of the expression $\mathrm{det}( I + \epsilon A)$ as a power series in $\epsilon$ for a diagonalizable matrix $A$. Formally, we will write the series as follows $$ \det( I + \epsilon A ) = f_0(A) + \epsilon f_1(A) + \epsilon^2 f_2(A) + \cdots +\epsilon^N f_N(A), $$ In particular, we are looking for an expression in terms of the trace of the matrix $A$.","The problem is to find the second order term in the series expansion of the expression $\mathrm{det}( I + \epsilon A)$ as a power series in $\epsilon$ for a diagonalizable matrix $A$. Formally, we will write the series as follows $$ \det( I + \epsilon A ) = f_0(A) + \epsilon f_1(A) + \epsilon^2 f_2(A) + \cdots +\epsilon^N f_N(A), $$ In particular, we are looking for an expression in terms of the trace of the matrix $A$.",,"['linear-algebra', 'polynomials', 'determinant', 'approximation-theory', 'perturbation-theory']"
28,Is it misleading to think of rank-2 tensors as matrices?,Is it misleading to think of rank-2 tensors as matrices?,,"Having picked up a rudimentary understanding of tensors from reading mechanics papers and Wikipedia, I tend to think of rank-2 tensors simply as square matrices (along with appropriate transformation rules). Certainly, if the distinction between vectors and dual vectors is ignored, a rank 2 tensor $T$ seems to be simply a multilinear map $V \times V \rightarrow \mathbb{R}$, and (I think) any such map can be represented by a matrix $\mathbf{A}$ using the mapping $(\mathbf{v},\mathbf{w}) \mapsto \mathbf{v}^T\mathbf{Aw}$. My question is this: Is this a reasonable way of thinking about things, at least as long as you're working in $\mathbb{R}^n$? Are there any obvious problems or subtle misunderstandings that this naive approach can cause? Does it break down when you deal with something other than $\mathbb{R}^n$? In short, is it ""morally wrong""?","Having picked up a rudimentary understanding of tensors from reading mechanics papers and Wikipedia, I tend to think of rank-2 tensors simply as square matrices (along with appropriate transformation rules). Certainly, if the distinction between vectors and dual vectors is ignored, a rank 2 tensor $T$ seems to be simply a multilinear map $V \times V \rightarrow \mathbb{R}$, and (I think) any such map can be represented by a matrix $\mathbf{A}$ using the mapping $(\mathbf{v},\mathbf{w}) \mapsto \mathbf{v}^T\mathbf{Aw}$. My question is this: Is this a reasonable way of thinking about things, at least as long as you're working in $\mathbb{R}^n$? Are there any obvious problems or subtle misunderstandings that this naive approach can cause? Does it break down when you deal with something other than $\mathbb{R}^n$? In short, is it ""morally wrong""?",,['linear-algebra']
29,Are most matrices diagonalizable?,Are most matrices diagonalizable?,,"More precisely, does the set of non-diagonalizable (over $\mathbb C$) matrices have Lebesgue measure zero in $\mathbb R^{n\times n}$ or $\mathbb C^{n\times n}$? Intuitively, I would think yes, since in order for a matrix to be non-diagonalizable its characteristic polynomial would have to have a multiple root. But most monic polynomials of degree $n$ have distinct roots. Can this argument be formalized?","More precisely, does the set of non-diagonalizable (over $\mathbb C$) matrices have Lebesgue measure zero in $\mathbb R^{n\times n}$ or $\mathbb C^{n\times n}$? Intuitively, I would think yes, since in order for a matrix to be non-diagonalizable its characteristic polynomial would have to have a multiple root. But most monic polynomials of degree $n$ have distinct roots. Can this argument be formalized?",,"['linear-algebra', 'matrices', 'measure-theory', 'lebesgue-measure', 'diagonalization']"
30,What does the space of non-diagonalizable matrices look like?,What does the space of non-diagonalizable matrices look like?,,"Let $k$ be a field  $\mathbb C$. Consider the action of $G=GL_n(k)$ by conjugation on the set of $n\times n$ matrices over $k$. The collection $X$ of matrices with repeated eigenvalues over $\overline k$ is a subvariety (as it is the zero set of the discriminant of the characteristic polynomial), and moreover it is preserved by $G$. If we let $k^n\subset X$ be the diagonal matrices with repeated roots, then $Y=X\setminus G(D)$ is the set of non-diagonalizable matrices, and also has an action of $G$. If $k=\overline k$, then every $G$-orbit contains an element in Jordan normal form, and by scaling the off-diagonal entries, we remain in the same conjugacy class, and so we see the corresponding diagonal matrix is in the closure of the orbit. Therefore $Y$ is dense in $X$.  This allows one to compute the dimension of $Y$ (I think). However, I'm not really sure what else to say in describing $Y$. What does $Y$ look like?  I know this is a little vague, but I'm not really sure what a reasonable reformulation would be. Are there good decompositions of $Y$ that help in understanding its structure?  Is it smooth? Is it a manifold?  Can we calculate useful invariants of $Y$, such as the cohomology?  Are we better off understanding the individual orbits? Are there other group actions on $Y$ which elucidate its structure?","Let $k$ be a field  $\mathbb C$. Consider the action of $G=GL_n(k)$ by conjugation on the set of $n\times n$ matrices over $k$. The collection $X$ of matrices with repeated eigenvalues over $\overline k$ is a subvariety (as it is the zero set of the discriminant of the characteristic polynomial), and moreover it is preserved by $G$. If we let $k^n\subset X$ be the diagonal matrices with repeated roots, then $Y=X\setminus G(D)$ is the set of non-diagonalizable matrices, and also has an action of $G$. If $k=\overline k$, then every $G$-orbit contains an element in Jordan normal form, and by scaling the off-diagonal entries, we remain in the same conjugacy class, and so we see the corresponding diagonal matrix is in the closure of the orbit. Therefore $Y$ is dense in $X$.  This allows one to compute the dimension of $Y$ (I think). However, I'm not really sure what else to say in describing $Y$. What does $Y$ look like?  I know this is a little vague, but I'm not really sure what a reasonable reformulation would be. Are there good decompositions of $Y$ that help in understanding its structure?  Is it smooth? Is it a manifold?  Can we calculate useful invariants of $Y$, such as the cohomology?  Are we better off understanding the individual orbits? Are there other group actions on $Y$ which elucidate its structure?",,"['linear-algebra', 'matrices', 'algebraic-geometry']"
31,How does Cramer's rule work?,How does Cramer's rule work?,,I know Cramer's rule works for 3 linear equations. I know all steps to get solutions. But I don't know why (how) Cramer's rule gives us solutions? Why do we get $x=\frac{\Delta_1}\Delta$ and $y$ and $z$ in the same way? I want to know how these steps give us solutions?,I know Cramer's rule works for 3 linear equations. I know all steps to get solutions. But I don't know why (how) Cramer's rule gives us solutions? Why do we get $x=\frac{\Delta_1}\Delta$ and $y$ and $z$ in the same way? I want to know how these steps give us solutions?,,"['linear-algebra', 'matrices', 'systems-of-equations', 'determinant']"
32,Traces of all positive powers of a matrix are zero implies it is nilpotent,Traces of all positive powers of a matrix are zero implies it is nilpotent,,"Let $A$ be an $n\times n$ complex nilpotent matrix. Then we know that because all eigenvalues of $A$ must be $0$, it follows that $\text{tr}(A^n)=0$ for all positive integers $n$. What I would like to show is the converse, that is, if $\text{tr}(A^n)=0$ for all positive integers $n$, then $A$ is nilpotent. I tried to show that $0$ must be an eigenvalue of $A$, then try to show that all other eigenvalues must be equal to 0. However, I am stuck at the point where I need to show that $\det(A)=0$. May I know of the approach to show that $A$ is nilpotent?","Let $A$ be an $n\times n$ complex nilpotent matrix. Then we know that because all eigenvalues of $A$ must be $0$, it follows that $\text{tr}(A^n)=0$ for all positive integers $n$. What I would like to show is the converse, that is, if $\text{tr}(A^n)=0$ for all positive integers $n$, then $A$ is nilpotent. I tried to show that $0$ must be an eigenvalue of $A$, then try to show that all other eigenvalues must be equal to 0. However, I am stuck at the point where I need to show that $\det(A)=0$. May I know of the approach to show that $A$ is nilpotent?",,"['linear-algebra', 'matrices']"
33,distance from a point to a hyperplane,distance from a point to a hyperplane,,"I have an n-dimensional hyperplane: $w'x + b = 0$ and a point $x_0$. The shortest distance from this point to a hyperplane is $d = \frac{|w \cdot x_0+ b|}{||w||}$. I have no problem to prove this for 2 and 3 dimension space using algebraic manipulations, but fail to do this for an n-dimensional space. Can someone show a nice explanation for it?","I have an n-dimensional hyperplane: $w'x + b = 0$ and a point $x_0$. The shortest distance from this point to a hyperplane is $d = \frac{|w \cdot x_0+ b|}{||w||}$. I have no problem to prove this for 2 and 3 dimension space using algebraic manipulations, but fail to do this for an n-dimensional space. Can someone show a nice explanation for it?",,"['linear-algebra', 'geometry']"
34,Column Vectors orthogonal implies Row Vectors also orthogonal?,Column Vectors orthogonal implies Row Vectors also orthogonal?,,"If the column vectors of a matrix $A$ are all orthogonal and $A$ is a square matrix, can I say that the row vectors of matrix $A$ are also orthogonal to each other? From the equation $Q \cdot Q^{T}=I$ if $Q$ is orthogonal and square matrix, it seems that this is true but I still find it hard to believe. I have a feeling that I may still be wrong because those column vectors that are perpendicular are vectors within the column space. Taking the rows vectors give a totally different direction from the column vectors in the row space and so how could they always happen to be perpendicular? Thanks for any help.","If the column vectors of a matrix $A$ are all orthogonal and $A$ is a square matrix, can I say that the row vectors of matrix $A$ are also orthogonal to each other? From the equation $Q \cdot Q^{T}=I$ if $Q$ is orthogonal and square matrix, it seems that this is true but I still find it hard to believe. I have a feeling that I may still be wrong because those column vectors that are perpendicular are vectors within the column space. Taking the rows vectors give a totally different direction from the column vectors in the row space and so how could they always happen to be perpendicular? Thanks for any help.",,"['linear-algebra', 'matrices']"
35,What is an Inner Product Space?,What is an Inner Product Space?,,I've learned that the dot product is just one of many possible inner product spaces . Can someone explain this concept? When is it useful to define it as something other than the dot product ?,I've learned that the dot product is just one of many possible inner product spaces . Can someone explain this concept? When is it useful to define it as something other than the dot product ?,,"['linear-algebra', 'vector-spaces', 'inner-products']"
36,necessary and sufficient condition for trivial kernel of a matrix over a commutative ring,necessary and sufficient condition for trivial kernel of a matrix over a commutative ring,,"In answering Do these matrix rings have non-zero elements that are neither units nor zero divisors? I was surprised how hard it was to find anything on the Web about the generalization of the following fact to commutative rings: A square matrix over a field has trivial kernel if and only if its determinant is non-zero. As Bill demonstrated in the above question, a related fact about fields generalizes directly to commutative rings: A square matrix over a commutative ring is invertible if and only if its   determinant is invertible. However, the kernel being trivial and the matrix being invertible are not equivalent for general rings, so the question arises what the proper generalization of the first fact is. Since it took me quite a lot of searching to find the answer to this rather basic question, and it's excplicitly encouraged to write a question and answer it to document something that might be useful to others, I thought I'd write this up here in an accessible form. So my questions are: What is the relationship between the determinant of a square matrix over a commutative ring and the triviality of its kernel? Can the simple relationship that holds for fields be generalized? And (generalizing with a view to the answer) what is a necessary and sufficient condition for a (not necessarily square) matrix over a commutative ring to have trivial kernel?","In answering Do these matrix rings have non-zero elements that are neither units nor zero divisors? I was surprised how hard it was to find anything on the Web about the generalization of the following fact to commutative rings: A square matrix over a field has trivial kernel if and only if its determinant is non-zero. As Bill demonstrated in the above question, a related fact about fields generalizes directly to commutative rings: A square matrix over a commutative ring is invertible if and only if its   determinant is invertible. However, the kernel being trivial and the matrix being invertible are not equivalent for general rings, so the question arises what the proper generalization of the first fact is. Since it took me quite a lot of searching to find the answer to this rather basic question, and it's excplicitly encouraged to write a question and answer it to document something that might be useful to others, I thought I'd write this up here in an accessible form. So my questions are: What is the relationship between the determinant of a square matrix over a commutative ring and the triviality of its kernel? Can the simple relationship that holds for fields be generalized? And (generalizing with a view to the answer) what is a necessary and sufficient condition for a (not necessarily square) matrix over a commutative ring to have trivial kernel?",,"['linear-algebra', 'abstract-algebra', 'matrices', 'commutative-algebra']"
37,How to tell if some power of my integer matrix is the identity?,How to tell if some power of my integer matrix is the identity?,,"Given an $n\times n$-matrix $A$ with integer entries, I would like to decide whether there is some $m\in\mathbb N$ such that $A^m$ is the identity matrix. I can solve this by regarding $A$ as a complex matrix and computing its Jordan normal form; equivalently, I can compute the eigenvalues and check whether they are roots of $1$ and whether their geometric and algebraic multiplicities coincide. Are there other ways to solve this problem, perhaps exploiting the fact that $A$ has integer entries? Edit: I am interested in conditions which are easy to verify for families of matrices in a proof. Edit: Thanks to everyone for this wealth of answers. It will take me some time to read all of them carefully.","Given an $n\times n$-matrix $A$ with integer entries, I would like to decide whether there is some $m\in\mathbb N$ such that $A^m$ is the identity matrix. I can solve this by regarding $A$ as a complex matrix and computing its Jordan normal form; equivalently, I can compute the eigenvalues and check whether they are roots of $1$ and whether their geometric and algebraic multiplicities coincide. Are there other ways to solve this problem, perhaps exploiting the fact that $A$ has integer entries? Edit: I am interested in conditions which are easy to verify for families of matrices in a proof. Edit: Thanks to everyone for this wealth of answers. It will take me some time to read all of them carefully.",,"['linear-algebra', 'abstract-algebra', 'matrices']"
38,Relationship between eigendecomposition and singular value decomposition,Relationship between eigendecomposition and singular value decomposition,,"Let $A \in \mathbb{R}^{n\times n}$ be a real symmetric matrix. Please help me clear up some confusion about the relationship between the singular value decomposition of $A$ and the eigen-decomposition of $A$. Let $A = U\Sigma V^T$ be the SVD of $A$. Since $A = A^T$, we have $AA^T = A^TA = A^2$ and: $$A^2 = AA^T = U\Sigma V^T V \Sigma U^T = U\Sigma^2 U^T$$ $$A^2 = A^TA = V\Sigma U^T U\Sigma V^T = V\Sigma^2 V^T$$ Both of these are eigen-decompositions of $A^2$. Now consider some eigen-decomposition of $A$ $$A = W\Lambda W^T$$ Then $$A^2 = W\Lambda W^T W\Lambda W^T = W\Lambda^2 W^T$$ So $W$ also can be used to perform an eigen-decomposition of $A^2$. So now my confusion: It seems that $A = W\Lambda W^T$ is also a singular value decomposition of A. But singular values are always non-negative, and eigenvalues can be negative, so something must be wrong. What is going on?","Let $A \in \mathbb{R}^{n\times n}$ be a real symmetric matrix. Please help me clear up some confusion about the relationship between the singular value decomposition of $A$ and the eigen-decomposition of $A$. Let $A = U\Sigma V^T$ be the SVD of $A$. Since $A = A^T$, we have $AA^T = A^TA = A^2$ and: $$A^2 = AA^T = U\Sigma V^T V \Sigma U^T = U\Sigma^2 U^T$$ $$A^2 = A^TA = V\Sigma U^T U\Sigma V^T = V\Sigma^2 V^T$$ Both of these are eigen-decompositions of $A^2$. Now consider some eigen-decomposition of $A$ $$A = W\Lambda W^T$$ Then $$A^2 = W\Lambda W^T W\Lambda W^T = W\Lambda^2 W^T$$ So $W$ also can be used to perform an eigen-decomposition of $A^2$. So now my confusion: It seems that $A = W\Lambda W^T$ is also a singular value decomposition of A. But singular values are always non-negative, and eigenvalues can be negative, so something must be wrong. What is going on?",,"['linear-algebra', 'matrices']"
39,Coordinate free proof that $\operatorname{trace}(A)=0\:\Longrightarrow\:A=BC-CB$,Coordinate free proof that,\operatorname{trace}(A)=0\:\Longrightarrow\:A=BC-CB,"As you probably know, the trace function on square matrices has the property that $$\operatorname{trace}(AB-BA)=0\,.$$ You might also know that the converse is true: $$\operatorname{trace}(A)=0\;\text{ implies } A=BC-CB\:\text{ for some matrices } B\text{ and }C.$$ This is in fact true of linear operators on a vector space, it's a coordinate free fact. BUT all proofs I'm aware of fix a basis and give a proof using coordinates. So the question is, does anybody know a basis-free proof that does not use coordinates? Thanks for any information - even if the information is that everything I've said here is wrong and I'm a complete idiot.","As you probably know, the trace function on square matrices has the property that You might also know that the converse is true: This is in fact true of linear operators on a vector space, it's a coordinate free fact. BUT all proofs I'm aware of fix a basis and give a proof using coordinates. So the question is, does anybody know a basis-free proof that does not use coordinates? Thanks for any information - even if the information is that everything I've said here is wrong and I'm a complete idiot.","\operatorname{trace}(AB-BA)=0\,. \operatorname{trace}(A)=0\;\text{ implies } A=BC-CB\:\text{ for some matrices } B\text{ and }C.","['linear-algebra', 'linear-transformations', 'operator-theory', 'trace']"
40,The logarithm is non-linear! Or isn't it?,The logarithm is non-linear! Or isn't it?,,"The logarithm is non-linear Almost unexceptionally, I hear people say that the logarithm was a non-linear function. If asked to prove this, they often do something like this: We have   $$ \ln(x + y) \neq \ln(x) + \ln(y) \quad\text{and}\quad \ln(\lambda \cdot x) = \ln(\lambda) + \ln(x) \neq \lambda \cdot \ln(x), $$   and therefore $\ln$ is not linear. And indeed, the literature is abundant with the claim that... ... a function $f : V \to W$ is linear, if and only if   $$ f(x + y) = f(x) + f(y) \quad\text{and}\quad f(\lambda \cdot x) = \lambda \cdot f(x) $$   for all $x,y$ and all scalars $\lambda$. Often, there is no hint that the symbols $+$ and $\cdot$ on the left belong to $V$, whereas the symbols $+$ and the $\cdot$ on the right belong to $W$. The logarithm is linear My proof that the logarithm is a linear function goes like this: $$\ln(x \cdot y) = \ln(x) + \ln(y) \quad\text{and}\quad \ln(x^\lambda) = \lambda \cdot \ln(x).$$ The rationale for this is that $\ln : \mathbb{R}_{>0} \to \mathbb{R}$, i.e., the logarithm is a function from the $\mathbb{R}$-vector space $\mathbb{R}_{>0}$ (the positive-real numbers), to the $\mathbb{R}$-vector space $\mathbb{R}$ (the real numbers). Vector addition in $\mathbb{R}_{>0}$ is, however, not usual addition, but multiplication. Likewise, scalar multiplication in $\mathbb{R}_{>0}$ is not usual multiplication, but potentiation. In fact, the linear-algebra definition of linearity is (e.g. Ricardo, 2009 ; Bowen and Wang, 1976 ): A function $f : V \to W$ from a vectors space $(V,\oplus,\odot)$ over a field $F$ to a vector space $(W,\boxplus,\boxdot)$ over $F$ is linear if and only if it satisfies   $$ f(x \oplus y) = f(x) \boxplus f(y) \quad\text{and}\quad f(\lambda \odot x) = \lambda \boxdot f(x) $$   for all $x,y \in V$ and $\lambda \in F$. Another proof goes as follows: The logarithm is an isomorphism between the vector space of positive-real numbers to the vector space of real numbers. And as every isomorphism is a linear function, so is the logarithm. Question We have two conflicting statements here: The logarithm is non-linear. The logarithm is linear. Can both statements be correct simultaneously, depending on something I cannot imagine now? But wouldn't this also imply that two conflicting concepts of linearity exist? Or is this a case of sloppy notation, e.g., abuse of the same symbol $+$ for vector addition or $\cdot$ for scalar multiplication even though two different vector spaces are involved? Update The solutions given to rescue the first statement haven't convinced me yet, because they are inconsistent: Using usual addition and multiplication on $\mathbb{R}_{>0}$ implies that $(\mathbb{R}_{>0},+,\cdot)$ is not a vector space anymore. But a precondition of the linearity proof is that the domain and the range of $f$ are vector spaces. Allowing the domain of $\ln$ to be $\mathbb{R}$ with usual addition and multiplication instead of $\mathbb{R}_{>0}$ doesn't work, because then the image of $\ln$ is the set of complex numbers. A mathematically consistent definition of ""linearity"" for subsets (but not subspaces) of a vector space was given in a comment by @Alex G. Let $S$ be an arbitrary subset of a real vector space $V$, and let $W$ be a real vector space. A function $f : S \to W$ is called ""linear"" if for all $x,y \in S$ such that $x+y \in S$, then $f(x+y) = f(x)+f(y)$, and for all $x \in S$, $k \in \mathbb{R}$ such that $kx \in S$, then $f(kx)=k⋅f(x)$. However, this definition is not what is meant by the concept of linearity coming from linear algebra. One would actually need to use another term for ""linearity"" here.","The logarithm is non-linear Almost unexceptionally, I hear people say that the logarithm was a non-linear function. If asked to prove this, they often do something like this: We have   $$ \ln(x + y) \neq \ln(x) + \ln(y) \quad\text{and}\quad \ln(\lambda \cdot x) = \ln(\lambda) + \ln(x) \neq \lambda \cdot \ln(x), $$   and therefore $\ln$ is not linear. And indeed, the literature is abundant with the claim that... ... a function $f : V \to W$ is linear, if and only if   $$ f(x + y) = f(x) + f(y) \quad\text{and}\quad f(\lambda \cdot x) = \lambda \cdot f(x) $$   for all $x,y$ and all scalars $\lambda$. Often, there is no hint that the symbols $+$ and $\cdot$ on the left belong to $V$, whereas the symbols $+$ and the $\cdot$ on the right belong to $W$. The logarithm is linear My proof that the logarithm is a linear function goes like this: $$\ln(x \cdot y) = \ln(x) + \ln(y) \quad\text{and}\quad \ln(x^\lambda) = \lambda \cdot \ln(x).$$ The rationale for this is that $\ln : \mathbb{R}_{>0} \to \mathbb{R}$, i.e., the logarithm is a function from the $\mathbb{R}$-vector space $\mathbb{R}_{>0}$ (the positive-real numbers), to the $\mathbb{R}$-vector space $\mathbb{R}$ (the real numbers). Vector addition in $\mathbb{R}_{>0}$ is, however, not usual addition, but multiplication. Likewise, scalar multiplication in $\mathbb{R}_{>0}$ is not usual multiplication, but potentiation. In fact, the linear-algebra definition of linearity is (e.g. Ricardo, 2009 ; Bowen and Wang, 1976 ): A function $f : V \to W$ from a vectors space $(V,\oplus,\odot)$ over a field $F$ to a vector space $(W,\boxplus,\boxdot)$ over $F$ is linear if and only if it satisfies   $$ f(x \oplus y) = f(x) \boxplus f(y) \quad\text{and}\quad f(\lambda \odot x) = \lambda \boxdot f(x) $$   for all $x,y \in V$ and $\lambda \in F$. Another proof goes as follows: The logarithm is an isomorphism between the vector space of positive-real numbers to the vector space of real numbers. And as every isomorphism is a linear function, so is the logarithm. Question We have two conflicting statements here: The logarithm is non-linear. The logarithm is linear. Can both statements be correct simultaneously, depending on something I cannot imagine now? But wouldn't this also imply that two conflicting concepts of linearity exist? Or is this a case of sloppy notation, e.g., abuse of the same symbol $+$ for vector addition or $\cdot$ for scalar multiplication even though two different vector spaces are involved? Update The solutions given to rescue the first statement haven't convinced me yet, because they are inconsistent: Using usual addition and multiplication on $\mathbb{R}_{>0}$ implies that $(\mathbb{R}_{>0},+,\cdot)$ is not a vector space anymore. But a precondition of the linearity proof is that the domain and the range of $f$ are vector spaces. Allowing the domain of $\ln$ to be $\mathbb{R}$ with usual addition and multiplication instead of $\mathbb{R}_{>0}$ doesn't work, because then the image of $\ln$ is the set of complex numbers. A mathematically consistent definition of ""linearity"" for subsets (but not subspaces) of a vector space was given in a comment by @Alex G. Let $S$ be an arbitrary subset of a real vector space $V$, and let $W$ be a real vector space. A function $f : S \to W$ is called ""linear"" if for all $x,y \in S$ such that $x+y \in S$, then $f(x+y) = f(x)+f(y)$, and for all $x \in S$, $k \in \mathbb{R}$ such that $kx \in S$, then $f(kx)=k⋅f(x)$. However, this definition is not what is meant by the concept of linearity coming from linear algebra. One would actually need to use another term for ""linearity"" here.",,"['linear-algebra', 'notation', 'logarithms', 'terminology', 'linear-transformations']"
41,"How to prove $\text{Rank}(AB)\leq \min(\text{Rank}(A), \text{Rank}(B))$? [duplicate]",How to prove ? [duplicate],"\text{Rank}(AB)\leq \min(\text{Rank}(A), \text{Rank}(B))","This question already has answers here : How to prove and interpret $\operatorname{rank}(AB) \leq \operatorname{min}(\operatorname{rank}(A), \operatorname{rank}(B))$? (9 answers) Closed 9 years ago . How to prove $\text{Rank}(AB)\leq \min(\text{Rank}(A), \text{Rank}(B))$?","This question already has answers here : How to prove and interpret $\operatorname{rank}(AB) \leq \operatorname{min}(\operatorname{rank}(A), \operatorname{rank}(B))$? (9 answers) Closed 9 years ago . How to prove $\text{Rank}(AB)\leq \min(\text{Rank}(A), \text{Rank}(B))$?",,"['linear-algebra', 'matrices', 'inequality', 'matrix-rank']"
42,How did mathematicians decide on the axioms of linear algebra,How did mathematicians decide on the axioms of linear algebra,,"So we vector spaces, linear transformations, inner products etc all have their own axioms that they have to satisfy in order to be considered to be what they are. But how did we come to decide to include those axioms and not include others? For example, why does  this rule hold in inner product spaces $c\langle u,v\rangle=\langle cu,v\rangle$, when my intuition says that it should be $\langle cu,cv\rangle$? And how did we decide that it was  scalar multiplication and additive were sufficient criteria for something to be a linear map?","So we vector spaces, linear transformations, inner products etc all have their own axioms that they have to satisfy in order to be considered to be what they are. But how did we come to decide to include those axioms and not include others? For example, why does  this rule hold in inner product spaces $c\langle u,v\rangle=\langle cu,v\rangle$, when my intuition says that it should be $\langle cu,cv\rangle$? And how did we decide that it was  scalar multiplication and additive were sufficient criteria for something to be a linear map?",,['linear-algebra']
43,Cross product in higher dimensions,Cross product in higher dimensions,,"Suppose we have a vector $(a,b)$ in $2$ -space. Then the vector $(-b,a)$ is orthogonal to the one we started with. Furthermore, the function $$(a,b) \mapsto (-b,a)$$ is linear. Suppose instead we have two vectors $x$ and $y$ in $3$ -space. Then the cross product gives us a new vector $x \times y$ that's orthogonal to the first two. Furthermore, cross products are bilinear. Question. Can we do this in higher dimensions? For example, is there a way of turning three vectors in $4$ -space into a fourth vector, orthogonal to the others, in a trilinear way?","Suppose we have a vector in -space. Then the vector is orthogonal to the one we started with. Furthermore, the function is linear. Suppose instead we have two vectors and in -space. Then the cross product gives us a new vector that's orthogonal to the first two. Furthermore, cross products are bilinear. Question. Can we do this in higher dimensions? For example, is there a way of turning three vectors in -space into a fourth vector, orthogonal to the others, in a trilinear way?","(a,b) 2 (-b,a) (a,b) \mapsto (-b,a) x y 3 x \times y 4","['linear-algebra', 'vectors', 'orthogonality', 'multilinear-algebra', 'cross-product']"
44,Name for diagonals of a matrix,Name for diagonals of a matrix,,"I am looking for the terms to use for particular types of diagonals in two dimensional matrices.  I have heard the longest diagonal, from top-left element and in the direction down-right often called the ""leading diagonal"". -What about the 'other' diagonal, from the top right going down-left?  Does it have a common name? -Any general name to use for any diagonal going top-left to bottom-right direction, not necessarily just the longest one(s)?  I am searching for a term to distinguish between these types of diagonals and the ones in the 'other' direction.  Preferably the term should not be restricted to square matrices.","I am looking for the terms to use for particular types of diagonals in two dimensional matrices.  I have heard the longest diagonal, from top-left element and in the direction down-right often called the ""leading diagonal"". -What about the 'other' diagonal, from the top right going down-left?  Does it have a common name? -Any general name to use for any diagonal going top-left to bottom-right direction, not necessarily just the longest one(s)?  I am searching for a term to distinguish between these types of diagonals and the ones in the 'other' direction.  Preferably the term should not be restricted to square matrices.",,"['linear-algebra', 'matrices', 'terminology']"
45,How can you explain the Singular Value Decomposition to non-specialists?,How can you explain the Singular Value Decomposition to non-specialists?,,"In two days, I am giving a presentation about a search engine I have been making the past summer. My research involved the use of singular value decompositions, i.e., $A = U \Sigma V^T$ . I took a high school course on Linear Algebra last year, but the course was not very thorough, and though I know how to find the SVD of a matrix, I don't know how to explain what I have in my hands after the matrix has been decomposed. To someone who has taken linear algebra, I can say that I can decompose a matrix $A$ into matrix $\Sigma$ , whose diagonal holds the singular values, and matrices $U$ and $V$ whose columns represent the left and right singular vectors of matrix $A$ . I am not sure how to explain what a singular value or what left/right singular vectors are. I can still be satisfied if there is no easy way to explain what this decomposition means, but I always prefer keeping the audience as informed as possible.","In two days, I am giving a presentation about a search engine I have been making the past summer. My research involved the use of singular value decompositions, i.e., . I took a high school course on Linear Algebra last year, but the course was not very thorough, and though I know how to find the SVD of a matrix, I don't know how to explain what I have in my hands after the matrix has been decomposed. To someone who has taken linear algebra, I can say that I can decompose a matrix into matrix , whose diagonal holds the singular values, and matrices and whose columns represent the left and right singular vectors of matrix . I am not sure how to explain what a singular value or what left/right singular vectors are. I can still be satisfied if there is no easy way to explain what this decomposition means, but I always prefer keeping the audience as informed as possible.",A = U \Sigma V^T A \Sigma U V A,"['linear-algebra', 'matrices', 'matrix-decomposition', 'svd', 'singular-values']"
46,what does ∇ (upside down triangle) symbol mean in this problem,what does ∇ (upside down triangle) symbol mean in this problem,,"Given $f(x) = \frac{1}{2}x^TAx + b^Tx + \alpha $ where A is an nxn symmetric matrix, b is an n-dimensional vector, and alpha a scalar. Show that $\bigtriangledown _{x}f(x) = Ax + b$ and $H = \bigtriangledown ^{2}_{x}f(x) = A$ Is this simply a matter of taking a derivative with respect to X, how would you attack this one?","Given $f(x) = \frac{1}{2}x^TAx + b^Tx + \alpha $ where A is an nxn symmetric matrix, b is an n-dimensional vector, and alpha a scalar. Show that $\bigtriangledown _{x}f(x) = Ax + b$ and $H = \bigtriangledown ^{2}_{x}f(x) = A$ Is this simply a matter of taking a derivative with respect to X, how would you attack this one?",,"['linear-algebra', 'notation']"
47,Understanding rotation matrices,Understanding rotation matrices,,"How does $ {\sqrt 2 \over 2} = \cos (45^\circ)$ ? Is my graph (the one underneath the original) accurate with how I've depicted the representation of the triangle that the trig function represent? What I mean is, the blue triangle is the pre-rotated block, the green is the post-rotated block, and the purple is the rotated change ( $45^\circ$ ) between them. How do these trig functions in this matrix represent a clockwise rotation? (Like, why does "" $-\sin \theta $ "" in the bottom left mean clockwise rotation... and "" $- \sin \theta $ "" in the upper right mean counter clockwise? Why not "" $-\cos \theta $ ""? $$\begin{bmatrix}\cos \theta &\sin \theta \\-\sin \theta & \cos \theta \end{bmatrix}$$ Any help in understanding the trig representations of a rotation would be extremely helpful! Thanks","How does ? Is my graph (the one underneath the original) accurate with how I've depicted the representation of the triangle that the trig function represent? What I mean is, the blue triangle is the pre-rotated block, the green is the post-rotated block, and the purple is the rotated change ( ) between them. How do these trig functions in this matrix represent a clockwise rotation? (Like, why does "" "" in the bottom left mean clockwise rotation... and "" "" in the upper right mean counter clockwise? Why not "" ""? Any help in understanding the trig representations of a rotation would be extremely helpful! Thanks", {\sqrt 2 \over 2} = \cos (45^\circ) 45^\circ -\sin \theta  - \sin \theta  -\cos \theta  \begin{bmatrix}\cos \theta &\sin \theta \\-\sin \theta & \cos \theta \end{bmatrix},"['linear-algebra', 'matrices', 'rotations']"
48,Relation between rank and number of distinct eigenvalues of a matrix,Relation between rank and number of distinct eigenvalues of a matrix,,"Let $T : V\to V$ be a linear transformation such that $\dim\operatorname{Range}(T)=k\leq n$ , where $n=\dim V$ . Show that $T$ can have at most $k+1$ distinct eigenvalues. I can realize that the rank  will correspond to the number of non-zero eigenvalues (counted up to multiplicity) and the nullity will correspond to the 0 eigenvalue (counted up to multiplicity), but I cannot design an analytical proof of this. Thanks for any help .","Let be a linear transformation such that , where . Show that can have at most distinct eigenvalues. I can realize that the rank  will correspond to the number of non-zero eigenvalues (counted up to multiplicity) and the nullity will correspond to the 0 eigenvalue (counted up to multiplicity), but I cannot design an analytical proof of this. Thanks for any help .",T : V\to V \dim\operatorname{Range}(T)=k\leq n n=\dim V T k+1,"['linear-algebra', 'vector-spaces']"
49,Why it is important for isomorphism between vector space and its double dual space to be natural?,Why it is important for isomorphism between vector space and its double dual space to be natural?,,"I'm reading the book (by A. Kostrikin) on linear algebra and I feel like I'm really missing something about this idea. I understand the formal proofs of: a) isomorphism between vector space $V$ and its dual space $V^*$ b) natural isomorphism between $V$ and $V^{**}$. What I don't understand is the idea behind it: 1) Why is it so important for isomorphism to be ""natural""? Does it make it  better, more usable is some way, or?.. 2) It is said later in the book, that established natural isomorphism between $V$ and $V^{**}$ makes them ""totally equal"" (my translation of Russian ""совершенно равноправные""). But wait, isn't it enough to establish any kind of isomorphism between two sets to make them ""equal""? What is it in natural isomorphism that makes the sets more equal than with any other isomorphism? 3) Is there a way to strictly define natural isomorphism (at least for finite vector spaces) without using category theory? Thank you!","I'm reading the book (by A. Kostrikin) on linear algebra and I feel like I'm really missing something about this idea. I understand the formal proofs of: a) isomorphism between vector space $V$ and its dual space $V^*$ b) natural isomorphism between $V$ and $V^{**}$. What I don't understand is the idea behind it: 1) Why is it so important for isomorphism to be ""natural""? Does it make it  better, more usable is some way, or?.. 2) It is said later in the book, that established natural isomorphism between $V$ and $V^{**}$ makes them ""totally equal"" (my translation of Russian ""совершенно равноправные""). But wait, isn't it enough to establish any kind of isomorphism between two sets to make them ""equal""? What is it in natural isomorphism that makes the sets more equal than with any other isomorphism? 3) Is there a way to strictly define natural isomorphism (at least for finite vector spaces) without using category theory? Thank you!",,"['linear-algebra', 'vector-spaces']"
50,Prove that if V is finite dimensional then V is even dimensional?,Prove that if V is finite dimensional then V is even dimensional?,,"Let $f:V \to V$ be a linear map such that $(f\circ f)(v) = -v$. Prove that if $V$ is a finite dimensional vector space over $\mathbb R$, $V$ is even dimensional. From what I can figure out for myself, if $V$ is finite dimensional, then every basis of $V$ is finite, i.e. a linearly independently subset of $V$ has a finite number of vectors. And I figure that if $V$ is even dimensional, then every basis of $V$ is even, i.e. a linearly independently subset of $V$ has an even number of vectors. But I'm not sure how to connect these two points.","Let $f:V \to V$ be a linear map such that $(f\circ f)(v) = -v$. Prove that if $V$ is a finite dimensional vector space over $\mathbb R$, $V$ is even dimensional. From what I can figure out for myself, if $V$ is finite dimensional, then every basis of $V$ is finite, i.e. a linearly independently subset of $V$ has a finite number of vectors. And I figure that if $V$ is even dimensional, then every basis of $V$ is even, i.e. a linearly independently subset of $V$ has an even number of vectors. But I'm not sure how to connect these two points.",,"['linear-algebra', 'linear-transformations']"
51,Linear independence of $\sin(x)$ and $\cos(x)$,Linear independence of  and,\sin(x) \cos(x),"In the vector space of $f:\mathbb R \to \mathbb R$, how do I prove that functions $\sin(x)$ and $\cos(x)$ are linearly independent. By def., two elements of a vector space are linearly independent if $0 = a\cos(x) + b\sin(x)$ implies that $a=b=0$, but how can I formalize that? Giving $x$ different values? Thanks in advance.","In the vector space of $f:\mathbb R \to \mathbb R$, how do I prove that functions $\sin(x)$ and $\cos(x)$ are linearly independent. By def., two elements of a vector space are linearly independent if $0 = a\cos(x) + b\sin(x)$ implies that $a=b=0$, but how can I formalize that? Giving $x$ different values? Thanks in advance.",,['linear-algebra']
52,Determinant of a block lower triangular matrix,Determinant of a block lower triangular matrix,,"I'm trying to prove the following: Let $A$ be a  $k\times k$ matrix, let $D$ have size $n\times n$, and $C$ have size $n\times k$. Then, $$\det\left(\begin{array}{cc} A&0\\ C&D \end{array}\right) = \det(A)\det(D).$$ Can I just say that $AD - 0C = AD$, and I'm done?","I'm trying to prove the following: Let $A$ be a  $k\times k$ matrix, let $D$ have size $n\times n$, and $C$ have size $n\times k$. Then, $$\det\left(\begin{array}{cc} A&0\\ C&D \end{array}\right) = \det(A)\det(D).$$ Can I just say that $AD - 0C = AD$, and I'm done?",,"['linear-algebra', 'matrices', 'determinant', 'block-matrices']"
53,Why is a linear transformation called linear? [duplicate],Why is a linear transformation called linear? [duplicate],,"This question already has answers here : Why are linear functions linear? (2 answers) Closed 8 years ago . $T(av_1 + bv_2) = aT(v_1) + bT(v_2)$ Why is this called linear? $f(x) =ax + b$, the simplest linear equation does not satisfy $f(x_1 + x_2) = f(x_1) + f(x_2)$. Thank you.","This question already has answers here : Why are linear functions linear? (2 answers) Closed 8 years ago . $T(av_1 + bv_2) = aT(v_1) + bT(v_2)$ Why is this called linear? $f(x) =ax + b$, the simplest linear equation does not satisfy $f(x_1 + x_2) = f(x_1) + f(x_2)$. Thank you.",,"['linear-algebra', 'terminology']"
54,Why does positive definite matrix have strictly positive eigenvalue?,Why does positive definite matrix have strictly positive eigenvalue?,,We say $A$ is a positive definite matrix if and only if $x^T A x > 0$ for all nonzero vectors $x$. Then why does every positive definite matrix have strictly positive eigenvalues?,We say $A$ is a positive definite matrix if and only if $x^T A x > 0$ for all nonzero vectors $x$. Then why does every positive definite matrix have strictly positive eigenvalues?,,"['linear-algebra', 'matrices', 'eigenvalues-eigenvectors']"
55,Is the sum of positive definite matrices still positive definite?,Is the sum of positive definite matrices still positive definite?,,"I have two symmetric positive definite (SPD) matrices. I would like to prove that the sum of these two matrices is still SPD. Symmetry is obvious, but what about PD-ness? Any clues, please?","I have two symmetric positive definite (SPD) matrices. I would like to prove that the sum of these two matrices is still SPD. Symmetry is obvious, but what about PD-ness? Any clues, please?",,"['linear-algebra', 'matrices', 'symmetric-matrices', 'positive-definite']"
56,Minimal polynomials and characteristic polynomials [duplicate],Minimal polynomials and characteristic polynomials [duplicate],,This question already has answers here : When are minimal and characteristic polynomials the same? (4 answers) Closed 6 years ago . I am trying to understand the similarities and differences between the minimal polynomial and characteristic polynomial of Matrices. When are the minimal polynomial and characteristic polynomial the same When are they different What conditions (eigenvalues/eigenvectors/...) would imply 1 or 2 Please tell me anything else about these two polynomials that is essential in comparing them.,This question already has answers here : When are minimal and characteristic polynomials the same? (4 answers) Closed 6 years ago . I am trying to understand the similarities and differences between the minimal polynomial and characteristic polynomial of Matrices. When are the minimal polynomial and characteristic polynomial the same When are they different What conditions (eigenvalues/eigenvectors/...) would imply 1 or 2 Please tell me anything else about these two polynomials that is essential in comparing them.,,"['linear-algebra', 'matrices', 'eigenvalues-eigenvectors', 'minimal-polynomials', 'characteristic-polynomial']"
57,Showing that $l_2$ norm is smaller than $l_1$,Showing that  norm is smaller than,l_2 l_1,"How can I show that $L_2\le L_1$ $||x||_1\ge ||x||_2$ and also we have that $\|x\|_2\leq \sqrt m\|x\|_{\infty}$ regarding the first part, can I say that: $$ \sqrt{\sum\limits_{i=1}^n x^2 } \leq {\sum\limits_{i=1}^n {\sqrt x}^2 } $$","How can I show that $L_2\le L_1$ $||x||_1\ge ||x||_2$ and also we have that $\|x\|_2\leq \sqrt m\|x\|_{\infty}$ regarding the first part, can I say that: $$ \sqrt{\sum\limits_{i=1}^n x^2 } \leq {\sum\limits_{i=1}^n {\sqrt x}^2 } $$",,"['linear-algebra', 'inequality', 'normed-spaces']"
58,Why is the SVM margin equal to $\frac{2}{\|\mathbf{w}\|}$?,Why is the SVM margin equal to ?,\frac{2}{\|\mathbf{w}\|},"I am reading the Wikipedia article about Support Vector Machine and I don't understand how they compute the distance between two hyperplanes. In the article, By using geometry, we find the distance between these two hyperplanes   is $\frac{2}{\|\mathbf{w}\|}$ I don't understand how the find that result. What I tried I tried setting up an example in two dimensions with an hyperplane having the equation $y = -2x+5$ and separating some points $A(2,0)$, $B(3,0)$ and $C(0,4)$, $D(0,6)$ . If I take a vector $\mathbf{w}(-2,-1)$ normal to that hyperplane and compute the margin with $\frac{2}{\|\mathbf{w}\|}$ I get $\frac{2}{\sqrt{5}}$ when in my example the margin is equal to 2 (distance between $C$ and $D$). How did they come up with $\frac{2}{\|\mathbf{w}\|}$ ?","I am reading the Wikipedia article about Support Vector Machine and I don't understand how they compute the distance between two hyperplanes. In the article, By using geometry, we find the distance between these two hyperplanes   is $\frac{2}{\|\mathbf{w}\|}$ I don't understand how the find that result. What I tried I tried setting up an example in two dimensions with an hyperplane having the equation $y = -2x+5$ and separating some points $A(2,0)$, $B(3,0)$ and $C(0,4)$, $D(0,6)$ . If I take a vector $\mathbf{w}(-2,-1)$ normal to that hyperplane and compute the margin with $\frac{2}{\|\mathbf{w}\|}$ I get $\frac{2}{\sqrt{5}}$ when in my example the margin is equal to 2 (distance between $C$ and $D$). How did they come up with $\frac{2}{\|\mathbf{w}\|}$ ?",,"['linear-algebra', 'geometry', 'vectors']"
59,Is a map that preserves lines and fixes the origin necessarily linear?,Is a map that preserves lines and fixes the origin necessarily linear?,,Let $V$ and $W$ be vector spaces over a field $\mathbb{F}$ with $\text{dim }V \ge 2$. A line is a set of the form $\{ \mathbf{u} + t\mathbf{v} : t \in \mathbb{F} \}$. A map $f: V \to W$ preserves lines if the image of every line in $V$ is a line in $W$. A map fixes the origin if $f(0) = 0$. Is a function $f: V\to W$ that preserves lines and fixes the origin necessarily linear?,Let $V$ and $W$ be vector spaces over a field $\mathbb{F}$ with $\text{dim }V \ge 2$. A line is a set of the form $\{ \mathbf{u} + t\mathbf{v} : t \in \mathbb{F} \}$. A map $f: V \to W$ preserves lines if the image of every line in $V$ is a line in $W$. A map fixes the origin if $f(0) = 0$. Is a function $f: V\to W$ that preserves lines and fixes the origin necessarily linear?,,"['linear-algebra', 'linear-transformations']"
60,A practical way to check if a matrix is positive-definite,A practical way to check if a matrix is positive-definite,,"Let $A$ be a symmetric $n\times n$ matrix. I found a method on the web to check if $A$ is positive definite : $A$ is positive-definite if  all the diagonal entries are positive, and   each diagonal entry is greater than the sum of the absolute values of all other entries in the corresponding row/column. I couldn't find a proof for this statement. I also couldn't find a reference in my linear algebra books. I've a few questions. How do we prove the above statement? Is the following slightly weaker statement true? A symmetric matrix $A$ is positive-definite if all the diagonal entries are positive, each diagonal entry is greater than or equal to the sum of the absolute values of all other entries in the corresponding row/column, and there exists one diagonal entry which is strictly greater than the sum of the absolute values of all other entries in the corresponding row/column.","Let $A$ be a symmetric $n\times n$ matrix. I found a method on the web to check if $A$ is positive definite : $A$ is positive-definite if  all the diagonal entries are positive, and   each diagonal entry is greater than the sum of the absolute values of all other entries in the corresponding row/column. I couldn't find a proof for this statement. I also couldn't find a reference in my linear algebra books. I've a few questions. How do we prove the above statement? Is the following slightly weaker statement true? A symmetric matrix $A$ is positive-definite if all the diagonal entries are positive, each diagonal entry is greater than or equal to the sum of the absolute values of all other entries in the corresponding row/column, and there exists one diagonal entry which is strictly greater than the sum of the absolute values of all other entries in the corresponding row/column.",,"['linear-algebra', 'matrices', 'numerical-methods', 'proof-writing', 'positive-definite']"
61,Is this determinant always non-negative?,Is this determinant always non-negative?,,"For any $(a_1,a_2,\cdots,a_n)\in\mathbb{R}^n$ , a matrix $A$ is defined by $$A_{ij}=\frac1{1+|a_i-a_j|}$$ Is $\det(A)$ always non-negative? I did some numerical test and it seems to be true, but I have no idea how to prove it. Thanks!","For any , a matrix is defined by Is always non-negative? I did some numerical test and it seems to be true, but I have no idea how to prove it. Thanks!","(a_1,a_2,\cdots,a_n)\in\mathbb{R}^n A A_{ij}=\frac1{1+|a_i-a_j|} \det(A)","['linear-algebra', 'matrices', 'determinant']"
62,What do zero eigenvalues mean?,What do zero eigenvalues mean?,,"What is the geometric meaning of a 3x3 matrix having all three eigenvalues as zero? I have interpretations in mind for 0, 1, and 2 eigenvalues being zero, but what about all of them?","What is the geometric meaning of a 3x3 matrix having all three eigenvalues as zero? I have interpretations in mind for 0, 1, and 2 eigenvalues being zero, but what about all of them?",,"['linear-algebra', 'eigenvalues-eigenvectors']"
63,Geometric meaning of the determinant of a matrix,Geometric meaning of the determinant of a matrix,,"What is the geometric meaning of the determinant of a matrix? I know that ""The determinant of a matrix represents the area of ​​a rectangle."" Perhaps this phrase is imprecise, but I would like to know something more, please. Thank you very much.","What is the geometric meaning of the determinant of a matrix? I know that ""The determinant of a matrix represents the area of ​​a rectangle."" Perhaps this phrase is imprecise, but I would like to know something more, please. Thank you very much.",,"['linear-algebra', 'geometry', 'determinant', 'intuition']"
64,Can I solve an integral (or other tough problem) by playing with knots?,Can I solve an integral (or other tough problem) by playing with knots?,,"I've seen that in calculating things in knot theory that involves a lot of hard looking integrals and matrices, even though the knots themselves appear fairly simple. So is there some way in which this works backwards, where I might have a really large determinant or complicated integral that can be represented as a knot, and by tying, untying, or rearranging the knot I can solve the problem on that ""knot domain"" and then transfer it back into the lingo of integrals and matrices, kind of like you might imagine using a laplace transform?","I've seen that in calculating things in knot theory that involves a lot of hard looking integrals and matrices, even though the knots themselves appear fairly simple. So is there some way in which this works backwards, where I might have a really large determinant or complicated integral that can be represented as a knot, and by tying, untying, or rearranging the knot I can solve the problem on that ""knot domain"" and then transfer it back into the lingo of integrals and matrices, kind of like you might imagine using a laplace transform?",,"['linear-algebra', 'integration', 'knot-theory', 'knot-invariants']"
65,What is that thing that keeps showing in papers on different fields?,What is that thing that keeps showing in papers on different fields?,,"A few months ago, when I was studying strategies for the evaluation of functional programs, I found that the optimal algorithm uses something called Interaction Combinators, a graph system based on a few nodes and rewrite rules . I've implemented and got some surprising results with it. I tried learning more, only to realize there are very few papers and almost nobody talking about it. Another day, by sheer luck, I stumble with this paper about chemical computers that would be ""the future of computation"". Midway through my read, I see this: The similarity to Interaction Nets in striking. The nodes, the rules, the principal ports - on its core, the system is mostly the same. I tried looking at references to find more about ""it"", but didn't find anything very relevant, so I gave up. Another day, by sheer luck once again, I sumble with this blog post about some kind of graphical linear algebra that ""can divide by zero"". Midway through the read, I see this: Once again, the same ""thing"" can be seen. There are some minor differences but, on its core, it is the same. What is that thing in common with those systems? How is it called, what is its importance, where is it studied and, most importantly, why it keeps showing in completely different fields?","A few months ago, when I was studying strategies for the evaluation of functional programs, I found that the optimal algorithm uses something called Interaction Combinators, a graph system based on a few nodes and rewrite rules . I've implemented and got some surprising results with it. I tried learning more, only to realize there are very few papers and almost nobody talking about it. Another day, by sheer luck, I stumble with this paper about chemical computers that would be ""the future of computation"". Midway through my read, I see this: The similarity to Interaction Nets in striking. The nodes, the rules, the principal ports - on its core, the system is mostly the same. I tried looking at references to find more about ""it"", but didn't find anything very relevant, so I gave up. Another day, by sheer luck once again, I sumble with this blog post about some kind of graphical linear algebra that ""can divide by zero"". Midway through the read, I see this: Once again, the same ""thing"" can be seen. There are some minor differences but, on its core, it is the same. What is that thing in common with those systems? How is it called, what is its importance, where is it studied and, most importantly, why it keeps showing in completely different fields?",,"['linear-algebra', 'functions', 'graph-theory']"
66,What is the idea behind a projection operator? What does it do?,What is the idea behind a projection operator? What does it do?,,"I know what a projection operator is, but I am unable to explain it in words without using mathematical symbols. Can anyone help me? I don't need examples or the definition - I want to know why and how its need arose, and what is the idea behind it?","I know what a projection operator is, but I am unable to explain it in words without using mathematical symbols. Can anyone help me? I don't need examples or the definition - I want to know why and how its need arose, and what is the idea behind it?",,"['linear-algebra', 'functional-analysis', 'intuition']"
67,Can you completely permute the elements of a matrix by applying permutation matrices?,Can you completely permute the elements of a matrix by applying permutation matrices?,,"Suppose I have a $n\times n$ matrix $A$. Can I, by using only pre- and post-multiplication by permutation matrices, permute all the elements of $A$? That is, there should be no binding conditions, like $a_{11}$ will always be to the left of $a_{n1}$, etc. This seems to be intuitively obvious. What I think is that I can write the matrix as an $n^2$-dimensional vector, then I can permute all entries by multiplying by a suitable permutation matrix, and then re-form a matrix with the permuted vector.","Suppose I have a $n\times n$ matrix $A$. Can I, by using only pre- and post-multiplication by permutation matrices, permute all the elements of $A$? That is, there should be no binding conditions, like $a_{11}$ will always be to the left of $a_{n1}$, etc. This seems to be intuitively obvious. What I think is that I can write the matrix as an $n^2$-dimensional vector, then I can permute all entries by multiplying by a suitable permutation matrix, and then re-form a matrix with the permuted vector.",,"['linear-algebra', 'matrices', 'permutations', 'permutation-matrices']"
68,How to prove $\det \left(e^A\right) = e^{\operatorname{tr}(A)}$?,How to prove ?,\det \left(e^A\right) = e^{\operatorname{tr}(A)},Prove $$\det \left( e^A \right) = e^{\operatorname{tr}(A)}$$ for all matrices $A \in \mathbb{C}^{n \times n}$ .,Prove for all matrices .,\det \left( e^A \right) = e^{\operatorname{tr}(A)} A \in \mathbb{C}^{n \times n},"['linear-algebra', 'matrices', 'determinant', 'trace', 'matrix-exponential']"
69,Rank of product of a matrix and its transpose [duplicate],Rank of product of a matrix and its transpose [duplicate],,This question already has answers here : Prove $\operatorname{rank}A^TA=\operatorname{rank}A$ for any $A\in M_{m \times n}$ (4 answers) Closed 9 years ago . How do we prove that $\operatorname{rank}(A) = \operatorname{rank}(AA^T) = \operatorname{rank}(A^TA)$ ? Is it always true?,This question already has answers here : Prove $\operatorname{rank}A^TA=\operatorname{rank}A$ for any $A\in M_{m \times n}$ (4 answers) Closed 9 years ago . How do we prove that ? Is it always true?,\operatorname{rank}(A) = \operatorname{rank}(AA^T) = \operatorname{rank}(A^TA),"['linear-algebra', 'matrices']"
70,How to prove $\operatorname{Tr}(AB) = \operatorname{Tr}(BA)$?,How to prove ?,\operatorname{Tr}(AB) = \operatorname{Tr}(BA),"there is a similar thread here Coordinate-free proof of $\operatorname{Tr}(AB)=\operatorname{Tr}(BA)$? , but I'm only looking for a simple linear algebra proof.","there is a similar thread here Coordinate-free proof of $\operatorname{Tr}(AB)=\operatorname{Tr}(BA)$? , but I'm only looking for a simple linear algebra proof.",,"['linear-algebra', 'matrices', 'trace']"
71,Dimensions of symmetric and skew-symmetric matrices,Dimensions of symmetric and skew-symmetric matrices,,"Let $\textbf A$ denote the space of symmetric $(n\times n)$ matrices over the field $\mathbb K$, and $\textbf B$ the space of skew-symmetric $(n\times n)$ matrices over the field $\mathbb K$. Then $\dim (\textbf A)=n(n+1)/2$ and $\dim (\textbf B)=n(n-1)/2$. Short question: is there any short explanation (maybe with combinatorics) why this statement is true? EDIT : $\dim$ refers to linear spaces.","Let $\textbf A$ denote the space of symmetric $(n\times n)$ matrices over the field $\mathbb K$, and $\textbf B$ the space of skew-symmetric $(n\times n)$ matrices over the field $\mathbb K$. Then $\dim (\textbf A)=n(n+1)/2$ and $\dim (\textbf B)=n(n-1)/2$. Short question: is there any short explanation (maybe with combinatorics) why this statement is true? EDIT : $\dim$ refers to linear spaces.",,"['linear-algebra', 'matrices', 'symmetric-matrices', 'skew-symmetric-matrices']"
72,Square matrices satisfying certain relations must have dimension divisible by $3$,Square matrices satisfying certain relations must have dimension divisible by,3,"I saw this tucked away in a MathOverflow comment and am asking this question to preserve (and advertise?) it. It's a nice problem! Problem: Suppose $A$ and $B$ are real $n\times n$ matrices with $A^2+B^2=AB$. If $AB-BA$ is invertible, prove $n$ is a multiple of $3$.","I saw this tucked away in a MathOverflow comment and am asking this question to preserve (and advertise?) it. It's a nice problem! Problem: Suppose $A$ and $B$ are real $n\times n$ matrices with $A^2+B^2=AB$. If $AB-BA$ is invertible, prove $n$ is a multiple of $3$.",,"['linear-algebra', 'matrices', 'contest-math']"
73,How can one intuitively think about quaternions?,How can one intuitively think about quaternions?,,"Quaternions came up while I was interning not too long ago and it seemed like no one really know how they worked. While eventually certain people were tracked down and were able to help with the issue, it piqued my interest in quaternions. After reading many articles and a couple books on them, I began to know the formulas associated with them, but still have no clue how they work (why they allow rotations in 3D space to be specific). I back-tracked a little bit and looked at normal complex numbers with just one imaginary component and asked myself if I even understood how they allow rotations in 2D space. After a couple awesome moments of understanding, I understood it for imaginary numbers, but I'm still having trouble extending the thoughts to quaternions. How can someone intuitively think about quaternions and how they allow for rotations in 3D space?","Quaternions came up while I was interning not too long ago and it seemed like no one really know how they worked. While eventually certain people were tracked down and were able to help with the issue, it piqued my interest in quaternions. After reading many articles and a couple books on them, I began to know the formulas associated with them, but still have no clue how they work (why they allow rotations in 3D space to be specific). I back-tracked a little bit and looked at normal complex numbers with just one imaginary component and asked myself if I even understood how they allow rotations in 2D space. After a couple awesome moments of understanding, I understood it for imaginary numbers, but I'm still having trouble extending the thoughts to quaternions. How can someone intuitively think about quaternions and how they allow for rotations in 3D space?",,"['linear-algebra', 'complex-numbers', 'intuition', 'quaternions']"
74,Symmetric matrix is always diagonalizable?,Symmetric matrix is always diagonalizable?,,"I'm reading my linear algebra textbook and there are two sentences that make me confused. (1) Symmetric matrix $A$ can be factored into $A=Q\lambda Q^{T}$ where $Q$ is orthogonal matrix : Diagonalizable ($Q$ has eigenvectors of $A$ in its columns, and $\lambda$ is diagonal matrix which has eigenvalues of $A$) (2) Any symmetric matrix has a complete set of orthonormal eigenvectors whether its eigenvalues are distinct or not. It's a contradiction, right? Diagonalizable means the matrix has n distinct eigenvectors (for $n$ by $n$ matrix). If symmetric matrix can be factored into $A=Q\lambda Q^{T}$, it means that symmetric matrix has n distinct eigenvalues. Then why the phrase ""whether its eigenvalues are distinct or not"" is added in (2)? After reading eigenvalue and eigenvector part of textbook, I conclude that every symmetric matrix is diagonalizable. Is that true?","I'm reading my linear algebra textbook and there are two sentences that make me confused. (1) Symmetric matrix $A$ can be factored into $A=Q\lambda Q^{T}$ where $Q$ is orthogonal matrix : Diagonalizable ($Q$ has eigenvectors of $A$ in its columns, and $\lambda$ is diagonal matrix which has eigenvalues of $A$) (2) Any symmetric matrix has a complete set of orthonormal eigenvectors whether its eigenvalues are distinct or not. It's a contradiction, right? Diagonalizable means the matrix has n distinct eigenvectors (for $n$ by $n$ matrix). If symmetric matrix can be factored into $A=Q\lambda Q^{T}$, it means that symmetric matrix has n distinct eigenvalues. Then why the phrase ""whether its eigenvalues are distinct or not"" is added in (2)? After reading eigenvalue and eigenvector part of textbook, I conclude that every symmetric matrix is diagonalizable. Is that true?",,"['linear-algebra', 'matrices', 'diagonalization', 'orthogonality']"
75,Does a positive definite matrix have positive determinant?,Does a positive definite matrix have positive determinant?,,Let $A$ be a positive-definite real matrix in the sense that $x^T A x > 0$ for every nonzero real vector $x$ . I don't require $A$ to be symmetric. Does it follow that $\mathrm{det}(A) > 0$ ?,Let be a positive-definite real matrix in the sense that for every nonzero real vector . I don't require to be symmetric. Does it follow that ?,A x^T A x > 0 x A \mathrm{det}(A) > 0,"['linear-algebra', 'matrices', 'positive-definite']"
76,Use of determinants,Use of determinants,,"I have been teaching myself maths (primarily calculus) throughout this and last year, and was stumped with the use of determinants.  In the math textbooks I have, they simply show how to compute a determinant and some properties about them (i.e. Cramer's Rule), but not why they are used and how they work. So my question is, how do they work and why/when would I know to use them to help solve something?","I have been teaching myself maths (primarily calculus) throughout this and last year, and was stumped with the use of determinants.  In the math textbooks I have, they simply show how to compute a determinant and some properties about them (i.e. Cramer's Rule), but not why they are used and how they work. So my question is, how do they work and why/when would I know to use them to help solve something?",,"['linear-algebra', 'matrices', 'self-learning', 'determinant']"
77,Why are all nonzero eigenvalues of the skew-symmetric real matrices pure imaginary?,Why are all nonzero eigenvalues of the skew-symmetric real matrices pure imaginary?,,"Assume that $A$ is an $n\times n$ skew-symmetric real matrix, i.e. $$A^T=-A.$$ Since $\det(A-\lambda I)=\det(A^T-\lambda I)$, $A$ and $A^T$ have the same eigenvalues. On the other hand, $A^T$ and $-A$ also have the same eigenvalues. Thus if $\lambda$ is an eigenvalue of $A$, so is $-\lambda$. If $n$ is odd, $\lambda = 0 $ is an eigenvalue. A curious search in Google returns that the nonzero eigenvalues of $A$ are all pure imaginary and thus are of the form $iλ_1, −iλ_1, iλ_2, −iλ_2,$ … where each of the $λ_k$ are real. Here is my question : How can I prove the fact that ""the nonzero eigenvalues of $A$ are all pure imaginary""?","Assume that $A$ is an $n\times n$ skew-symmetric real matrix, i.e. $$A^T=-A.$$ Since $\det(A-\lambda I)=\det(A^T-\lambda I)$, $A$ and $A^T$ have the same eigenvalues. On the other hand, $A^T$ and $-A$ also have the same eigenvalues. Thus if $\lambda$ is an eigenvalue of $A$, so is $-\lambda$. If $n$ is odd, $\lambda = 0 $ is an eigenvalue. A curious search in Google returns that the nonzero eigenvalues of $A$ are all pure imaginary and thus are of the form $iλ_1, −iλ_1, iλ_2, −iλ_2,$ … where each of the $λ_k$ are real. Here is my question : How can I prove the fact that ""the nonzero eigenvalues of $A$ are all pure imaginary""?",,"['linear-algebra', 'matrices']"
78,Proof that two bases of a vector space have the same cardinality in the infinite-dimensional case.,Proof that two bases of a vector space have the same cardinality in the infinite-dimensional case.,,"I am having a difficulty setting up the proof of the fact that two bases of a vector space have the same cardinality for the infinite-dimensional case. In particular, let $V$ be a vector space over a field $K$ and let $\left\{v_i\right\}_{i \in I}$ be a basis where $I$ is infinite countable. Let $\left\{u_j\right\}_{j \in J}$ be another basis. Then $J$ must be infinite countable as well. Any ideas on how to approach the proof?","I am having a difficulty setting up the proof of the fact that two bases of a vector space have the same cardinality for the infinite-dimensional case. In particular, let be a vector space over a field and let be a basis where is infinite countable. Let be another basis. Then must be infinite countable as well. Any ideas on how to approach the proof?",V K \left\{v_i\right\}_{i \in I} I \left\{u_j\right\}_{j \in J} J,"['linear-algebra', 'vector-spaces']"
79,Prove: symmetric positive definite matrix,Prove: symmetric positive definite matrix,,"I'm studying for my exam of linear algebra.. I want to prove the following corollary: If $A$ is a symmetric positive definite matrix then each entry $a_{ii}> 0$, ie all the elements of the diagonal of the matrix are positive. My teacher gave a suggestion to consider the unit vector ""$e_i$"", but I see that is using it. $a_{ii} >0$ for each $i = 1, 2, \ldots, n$. For any $i$, define $x = (x_j)$ by $x_i =1$ and by $x_j =0$, if $j\neq i$, since $x \neq 0$, then: $0< x^TAx = a_{ii}$ But my teacher says my proof is ambiguous. How I can use the unit vector $e_1$ for the demonstration?","I'm studying for my exam of linear algebra.. I want to prove the following corollary: If $A$ is a symmetric positive definite matrix then each entry $a_{ii}> 0$, ie all the elements of the diagonal of the matrix are positive. My teacher gave a suggestion to consider the unit vector ""$e_i$"", but I see that is using it. $a_{ii} >0$ for each $i = 1, 2, \ldots, n$. For any $i$, define $x = (x_j)$ by $x_i =1$ and by $x_j =0$, if $j\neq i$, since $x \neq 0$, then: $0< x^TAx = a_{ii}$ But my teacher says my proof is ambiguous. How I can use the unit vector $e_1$ for the demonstration?",,"['linear-algebra', 'matrices']"
80,Proof of when is $A=X^TX$ invertible?,Proof of when is  invertible?,A=X^TX,"Say we have an $n\times m$ matrix $X$. What are the specific properties that $X$ must have so that $A=X^TX$ invertible? I know that when the rows and columns are independent, then matrix $A$ (which is square) would be invertible and would have a non-zero determinant. However, what confuses me is, what sort of conditions must we have on each row of $X$ such that $A$ would be invertible. It would be very nice to have a solution of the form: when $n > m$ then $X$ must have... when $n < m$ then $X$ must have... when $n = m$ then $X$ must have... I think in the 3rd case we just need $X$ to be invertible but I was unsure of the other two cases.","Say we have an $n\times m$ matrix $X$. What are the specific properties that $X$ must have so that $A=X^TX$ invertible? I know that when the rows and columns are independent, then matrix $A$ (which is square) would be invertible and would have a non-zero determinant. However, what confuses me is, what sort of conditions must we have on each row of $X$ such that $A$ would be invertible. It would be very nice to have a solution of the form: when $n > m$ then $X$ must have... when $n < m$ then $X$ must have... when $n = m$ then $X$ must have... I think in the 3rd case we just need $X$ to be invertible but I was unsure of the other two cases.",,['linear-algebra']
81,How to find the determinant of this $3 \times 3$ Hankel matrix?,How to find the determinant of this  Hankel matrix?,3 \times 3,"Today, at my linear algebra exam, there was this question that I couldn't solve. Prove that $$\det \begin{bmatrix}  n^{2} & (n+1)^{2} &(n+2)^{2} \\  (n+1)^{2} &(n+2)^{2}  & (n+3)^{2}\\  (n+2)^{2} & (n+3)^{2} & (n+4)^{2} \end{bmatrix} = -8$$ Clearly, calculating the determinant, with the matrix as it is, wasn't the right way. The calculations went on and on. But I couldn't think of any other way to solve it. Is there any way to simplify $A$ , so as to calculate the determinant?","Today, at my linear algebra exam, there was this question that I couldn't solve. Prove that Clearly, calculating the determinant, with the matrix as it is, wasn't the right way. The calculations went on and on. But I couldn't think of any other way to solve it. Is there any way to simplify , so as to calculate the determinant?","\det \begin{bmatrix} 
n^{2} & (n+1)^{2} &(n+2)^{2} \\ 
(n+1)^{2} &(n+2)^{2}  & (n+3)^{2}\\ 
(n+2)^{2} & (n+3)^{2} & (n+4)^{2}
\end{bmatrix} = -8 A","['linear-algebra', 'matrices', 'determinant', 'hankel-matrices']"
82,Why is the matrix product of 2 orthogonal matrices also an orthogonal matrix?,Why is the matrix product of 2 orthogonal matrices also an orthogonal matrix?,,"I've seen the statement ""The matrix product of two orthogonal matrices is another orthogonal matrix. "" on Wolfram's website but haven't seen any proof online as to why this is true. By orthogonal matrix, I mean an $n \times n$ matrix with orthonormal columns. I was working on a problem to show whether $Q^3$ is an orthogonal matrix (where $Q$ is orthogonal matrix), but I think understanding this general case would probably solve that.","I've seen the statement ""The matrix product of two orthogonal matrices is another orthogonal matrix. "" on Wolfram's website but haven't seen any proof online as to why this is true. By orthogonal matrix, I mean an $n \times n$ matrix with orthonormal columns. I was working on a problem to show whether $Q^3$ is an orthogonal matrix (where $Q$ is orthogonal matrix), but I think understanding this general case would probably solve that.",,"['linear-algebra', 'matrices', 'orthogonality', 'orthogonal-matrices']"
83,Find the square root of a matrix,Find the square root of a matrix,,"Let $A$ be the matrix $$A = \left(\begin{array}{cc} 41 & 12\\ 12 & 34 \end{array}\right).$$ I want to decompose it into the form of $B^2$. I tried diagonalization , but can not move one step further. Any thought on this? Thanks a lot! ONE STEP FURTHER: How to find a upper triangular $U$ such that $A = U^T U$?","Let $A$ be the matrix $$A = \left(\begin{array}{cc} 41 & 12\\ 12 & 34 \end{array}\right).$$ I want to decompose it into the form of $B^2$. I tried diagonalization , but can not move one step further. Any thought on this? Thanks a lot! ONE STEP FURTHER: How to find a upper triangular $U$ such that $A = U^T U$?",,['linear-algebra']
84,What do I use to find the image and kernel of a given matrix?,What do I use to find the image and kernel of a given matrix?,,"I had a couple of questions about a matrix problem. What I'm given is: Consider a linear transformation $T: \mathbb R^5 \to \mathbb R^4$ defined by $T( \vec{x} )=A\vec{x}$, where   $$A = \left(\begin{array}{crc}  1 &  2 &  2 & -5 &  6\\ -1 & -2 & -1 &  1 & -1\\  4 &  8 &  5 & -8 &  9\\  3 &  6 &  1 &  5 & -7 \end{array}\right)$$ Find $\mathrm{im}(T)$ Find $\ker(T)$ My questions are: What do they mean by the transformation ? What do I use to actually find the image and kernel , and how do I do that?","I had a couple of questions about a matrix problem. What I'm given is: Consider a linear transformation $T: \mathbb R^5 \to \mathbb R^4$ defined by $T( \vec{x} )=A\vec{x}$, where   $$A = \left(\begin{array}{crc}  1 &  2 &  2 & -5 &  6\\ -1 & -2 & -1 &  1 & -1\\  4 &  8 &  5 & -8 &  9\\  3 &  6 &  1 &  5 & -7 \end{array}\right)$$ Find $\mathrm{im}(T)$ Find $\ker(T)$ My questions are: What do they mean by the transformation ? What do I use to actually find the image and kernel , and how do I do that?",,"['linear-algebra', 'matrices', 'transformation']"
85,Why is a projection matrix symmetric?,Why is a projection matrix symmetric?,,I am looking for an intuitive reason for a projection matrix of an orthogonal projection to be symmetric. The algebraic proof is straightforward yet somewhat unsatisfactory. Take for example another property: $P=P^2$. It's clear that applying the projection one more time shouldn't change anything and hence the equality. So what's the reason behind $P^T=P$?,I am looking for an intuitive reason for a projection matrix of an orthogonal projection to be symmetric. The algebraic proof is straightforward yet somewhat unsatisfactory. Take for example another property: $P=P^2$. It's clear that applying the projection one more time shouldn't change anything and hence the equality. So what's the reason behind $P^T=P$?,,"['linear-algebra', 'matrices', 'intuition', 'projection-matrices']"
86,Why is minimizing the nuclear norm of a matrix a good surrogate for minimizing the rank?,Why is minimizing the nuclear norm of a matrix a good surrogate for minimizing the rank?,,"A method called ""Robust PCA"" solves the matrix decomposition problem $$L^*, S^* = \arg \min_{L, S} \|L\|_* + \|S\|_1 \quad \text{s.t. } L + S = X$$ as a surrogate for the actual problem $$L^*, S^* = \arg \min_{L, S} rank(L) + \|S\|_0 \quad \text{s.t. } L + S = X,$$ i.e. the actual goal is to decompose the data matrix $X$ into a low-rank signal matrix $L$ and a sparse noise matrix $S$. In this context: why is the nuclear norm a good approximation for the rank of a matrix? I can think of matrices with low nuclear norm but high rank and vice-versa. Is there any intuition one can appeal to?","A method called ""Robust PCA"" solves the matrix decomposition problem $$L^*, S^* = \arg \min_{L, S} \|L\|_* + \|S\|_1 \quad \text{s.t. } L + S = X$$ as a surrogate for the actual problem $$L^*, S^* = \arg \min_{L, S} rank(L) + \|S\|_0 \quad \text{s.t. } L + S = X,$$ i.e. the actual goal is to decompose the data matrix $X$ into a low-rank signal matrix $L$ and a sparse noise matrix $S$. In this context: why is the nuclear norm a good approximation for the rank of a matrix? I can think of matrices with low nuclear norm but high rank and vice-versa. Is there any intuition one can appeal to?",,"['linear-algebra', 'matrices', 'normed-spaces', 'matrix-rank', 'nuclear-norm']"
87,Are one-by-one matrices equivalent to scalars?,Are one-by-one matrices equivalent to scalars?,,"I am a programmer, so to me $[x] \neq x$—a scalar in some sort of container is not equal to the scalar. However, I just read in a math book that for $1 \times 1$ matrices, the brackets are often dropped. This strikes me as very sloppy notation if $1 \times 1$ matrices are not at least functionally equivalent to scalars. As I began to think about the matrix operations I am familiar with, I could not think of any (tho I am weak on matrices) in which a $1 \times 1$ matrix would not act the same way as a scalar would when the corresponding scalar operations were applied to it. So, is $[x]$ functionally equivalent to $x$? And can we then say $[x] = x$? (And are those two different questions, or are entities in mathematics ""duck typed"" as we would say in the coding world?)","I am a programmer, so to me $[x] \neq x$—a scalar in some sort of container is not equal to the scalar. However, I just read in a math book that for $1 \times 1$ matrices, the brackets are often dropped. This strikes me as very sloppy notation if $1 \times 1$ matrices are not at least functionally equivalent to scalars. As I began to think about the matrix operations I am familiar with, I could not think of any (tho I am weak on matrices) in which a $1 \times 1$ matrix would not act the same way as a scalar would when the corresponding scalar operations were applied to it. So, is $[x]$ functionally equivalent to $x$? And can we then say $[x] = x$? (And are those two different questions, or are entities in mathematics ""duck typed"" as we would say in the coding world?)",,"['linear-algebra', 'matrices', 'soft-question']"
88,Change of basis matrix to convert standard basis to another basis,Change of basis matrix to convert standard basis to another basis,,"Consider the basis $B=\left\{\begin{pmatrix} -1  \\ 1 \\0 \end{pmatrix}\begin{pmatrix} -1  \\ 0 \\1 \end{pmatrix}\begin{pmatrix} 1  \\ 1 \\1 \end{pmatrix} \right\}$ for $\mathbb{R}^3$ . A) Find the change of basis matrix for converting from the standard basis to the basis B. I have never done anything like this and the only examples I can find online basically tell me how to do the change of basis for ""change-of-coordinates matrix from B to C"". B) Write the vector $\begin{pmatrix} 1  \\ 0 \\0 \end{pmatrix}$ in B-coordinates. Obviously I can't do this if I can't complete part A. Can someone either give me a hint, or preferably guide me towards an example of this type of problem? The absolute only thing I can think to do is take an augmented matrix $[B E]$ (note - E in this case is the standard basis, because I don't know the correct notation) and row reduce until B is now the standard matrix.  This is basically finding the inverse, so I doubt this is correct.","Consider the basis for . A) Find the change of basis matrix for converting from the standard basis to the basis B. I have never done anything like this and the only examples I can find online basically tell me how to do the change of basis for ""change-of-coordinates matrix from B to C"". B) Write the vector in B-coordinates. Obviously I can't do this if I can't complete part A. Can someone either give me a hint, or preferably guide me towards an example of this type of problem? The absolute only thing I can think to do is take an augmented matrix (note - E in this case is the standard basis, because I don't know the correct notation) and row reduce until B is now the standard matrix.  This is basically finding the inverse, so I doubt this is correct.",B=\left\{\begin{pmatrix} -1  \\ 1 \\0 \end{pmatrix}\begin{pmatrix} -1  \\ 0 \\1 \end{pmatrix}\begin{pmatrix} 1  \\ 1 \\1 \end{pmatrix} \right\} \mathbb{R}^3 \begin{pmatrix} 1  \\ 0 \\0 \end{pmatrix} [B E],"['linear-algebra', 'vector-spaces', 'change-of-basis']"
89,"If $(A-B)^2=AB$, prove that $\det(AB-BA)=0$.","If , prove that .",(A-B)^2=AB \det(AB-BA)=0,"Let $A,B\in M_{n}(\mathbb{Q})$. If $(A-B)^2=AB$, prove that $\det(AB-BA)=0$. I considered the function $f:\mathbb{Q}\rightarrow \mathbb{Q}$, $f(x)=\det(A^2+B^2-BA-xAB)$ and I obtained that: $$f(0)=\det(A^2+B^2-BA)=\det(2AB)=2^n\det(AB)$$ $$f(1)=\det(A^2+B^2-BA-AB)=\det((A-B)^2)=\det(AB)$$ $$f(2)=\det(A^2+B^2-BA-2AB)=\det((A-B)^2-AB)=\det(AB-AB)=0$$ I don't have any other idea.","Let $A,B\in M_{n}(\mathbb{Q})$. If $(A-B)^2=AB$, prove that $\det(AB-BA)=0$. I considered the function $f:\mathbb{Q}\rightarrow \mathbb{Q}$, $f(x)=\det(A^2+B^2-BA-xAB)$ and I obtained that: $$f(0)=\det(A^2+B^2-BA)=\det(2AB)=2^n\det(AB)$$ $$f(1)=\det(A^2+B^2-BA-AB)=\det((A-B)^2)=\det(AB)$$ $$f(2)=\det(A^2+B^2-BA-2AB)=\det((A-B)^2-AB)=\det(AB-AB)=0$$ I don't have any other idea.",,"['linear-algebra', 'matrices', 'determinant']"
90,Intersection of kernels and linear dependence of functionals,Intersection of kernels and linear dependence of functionals,,"I am trying to prove the following. I have seen it alluded to in other places of the internet (this site included) but without proof. Let $L,L_1\ldots L_n$ be linear functionals on a vector space $X$ . If $\bigcap_{i=1}^n ker(L_i) \subset ker(L)$ then there exists $t_i$ for $i=1\ldots n \in \mathbb{R}$ such that $L = \sum_{i=1}^n t_i L_i$ . In other words, if the intersection of kernels of linear functionals is contained by the kernel of another linear functional then they are linearly dependent. Related: Intersection of kernels and linear dependence of linear maps Linear dependence of linear functionals","I am trying to prove the following. I have seen it alluded to in other places of the internet (this site included) but without proof. Let be linear functionals on a vector space . If then there exists for such that . In other words, if the intersection of kernels of linear functionals is contained by the kernel of another linear functional then they are linearly dependent. Related: Intersection of kernels and linear dependence of linear maps Linear dependence of linear functionals","L,L_1\ldots L_n X \bigcap_{i=1}^n ker(L_i) \subset ker(L) t_i i=1\ldots n \in \mathbb{R} L = \sum_{i=1}^n t_i L_i","['linear-algebra', 'functional-analysis']"
91,What is duality?,What is duality?,,"I have seen some examples of duality. Sometimes applied to theorems, as for example Desargues theorem and Pappus theorem. Sometimes applied to spaces, for example the dual space of a vector space. Sometimes even applied to a method like simplex and dual simplex methods in linear programming. My question is what is the general meaning behind the term duality and what is its relevance to mathematics. Do we mean the same always when we use the term? Or the examples I have put have no connection whatsoever? Thanks a lot","I have seen some examples of duality. Sometimes applied to theorems, as for example Desargues theorem and Pappus theorem. Sometimes applied to spaces, for example the dual space of a vector space. Sometimes even applied to a method like simplex and dual simplex methods in linear programming. My question is what is the general meaning behind the term duality and what is its relevance to mathematics. Do we mean the same always when we use the term? Or the examples I have put have no connection whatsoever? Thanks a lot",,"['linear-algebra', 'geometry', 'intuition', 'duality-theorems']"
92,Does Nakayama Lemma imply Cayley-Hamilton Theorem?,Does Nakayama Lemma imply Cayley-Hamilton Theorem?,,"Consider the Cayley-Hamilton Theorem in the following form: CH: Let $A$ be a commutative ring, $\mathfrak{a}$ an ideal of $A$, $M$ a finitely generated $A$-module, $\phi$ an $A$-module endomorphism of $M$ such that $\phi(M)\subseteq\mathfrak{a}M$. Then there are coefficients $a_i\in\mathfrak{a}$ such that $\phi^n+a_1\phi^{n-1}+\dots+a_n=0$. This theorem can be proved by using elementary linear algebra in the context of rings. As a corollary, we find the following two versions of Nakayama's Lemma: NAK1: Let $A$ be a commutative ring, $M$ a finitely generated $A$-module and $\mathfrak{a}\subseteq A$ an Ideal such that $\mathfrak{a}M=M$. Then there is an $x=1\mod\mathfrak{a}$ such that $xM=0$. Proof. One just sets $\phi=\operatorname{id}$ and plugs in $x=1+a_1+\dots+a_n$. It follows: NAK2: Let $M$ be a finitely generated $A$-module, $\mathfrak{a}$ an ideal contained in the Jacobson radical of $A$. Then $\mathfrak{a}M=M$ implies $M=0$. Proof. Indeed, $xM=0$ for an element $x\in 1+\mathfrak{a}\subseteq 1+J(A)$, which is a unit, hence $M=0$. However, one can prove Nakayama's Lemma avoiding linear algebra: Alternative proof of NAK2 : Let $u_1,\dots,u_n$ be a generating system of $M$. $u_n\in M=\mathfrak{a}M$, so $u_n=a_1u_1+\dots+a_nu_n$. Subtracting, $(1-a_n)u_n=a_1u_1+\dots+a_{n-1}u_{n-1}$. But $1-a_n$ is a unit, since $a_n\in J(A)$, hence $u_n\in\langle u_1,\dots,u_{n-1}\rangle$. Iterating, we see that all $u_i$ have been zero. Alternative proof of Nak1 : Let $S=1+\mathfrak{a}$. Then $S^{-1}\mathfrak{a}\subseteq J(S^{-1}A)$. If $M=\mathfrak{a}M$, then $S^{-1}M=S^{-1}(\mathfrak{a}M)=(S^{-1}\mathfrak{a})(S^{-1}M)$, thus Nak2 implies $S^{-1}M=0$. As $M$ is finitely generated, there is an $x\in S$ such that $xM=0$. Now for my question: Can CH be deduced from Nakayama's Lemma, avoiding linear algebra, in particular the theory of determinants? By the way, the arguments are taken from Atiyah-Macdonald, I did not find them myself.","Consider the Cayley-Hamilton Theorem in the following form: CH: Let $A$ be a commutative ring, $\mathfrak{a}$ an ideal of $A$, $M$ a finitely generated $A$-module, $\phi$ an $A$-module endomorphism of $M$ such that $\phi(M)\subseteq\mathfrak{a}M$. Then there are coefficients $a_i\in\mathfrak{a}$ such that $\phi^n+a_1\phi^{n-1}+\dots+a_n=0$. This theorem can be proved by using elementary linear algebra in the context of rings. As a corollary, we find the following two versions of Nakayama's Lemma: NAK1: Let $A$ be a commutative ring, $M$ a finitely generated $A$-module and $\mathfrak{a}\subseteq A$ an Ideal such that $\mathfrak{a}M=M$. Then there is an $x=1\mod\mathfrak{a}$ such that $xM=0$. Proof. One just sets $\phi=\operatorname{id}$ and plugs in $x=1+a_1+\dots+a_n$. It follows: NAK2: Let $M$ be a finitely generated $A$-module, $\mathfrak{a}$ an ideal contained in the Jacobson radical of $A$. Then $\mathfrak{a}M=M$ implies $M=0$. Proof. Indeed, $xM=0$ for an element $x\in 1+\mathfrak{a}\subseteq 1+J(A)$, which is a unit, hence $M=0$. However, one can prove Nakayama's Lemma avoiding linear algebra: Alternative proof of NAK2 : Let $u_1,\dots,u_n$ be a generating system of $M$. $u_n\in M=\mathfrak{a}M$, so $u_n=a_1u_1+\dots+a_nu_n$. Subtracting, $(1-a_n)u_n=a_1u_1+\dots+a_{n-1}u_{n-1}$. But $1-a_n$ is a unit, since $a_n\in J(A)$, hence $u_n\in\langle u_1,\dots,u_{n-1}\rangle$. Iterating, we see that all $u_i$ have been zero. Alternative proof of Nak1 : Let $S=1+\mathfrak{a}$. Then $S^{-1}\mathfrak{a}\subseteq J(S^{-1}A)$. If $M=\mathfrak{a}M$, then $S^{-1}M=S^{-1}(\mathfrak{a}M)=(S^{-1}\mathfrak{a})(S^{-1}M)$, thus Nak2 implies $S^{-1}M=0$. As $M$ is finitely generated, there is an $x\in S$ such that $xM=0$. Now for my question: Can CH be deduced from Nakayama's Lemma, avoiding linear algebra, in particular the theory of determinants? By the way, the arguments are taken from Atiyah-Macdonald, I did not find them myself.",,"['linear-algebra', 'abstract-algebra']"
93,Can some proof that $\det(A) \ne 0$ be checked faster than matrix multiplication?,Can some proof that  be checked faster than matrix multiplication?,\det(A) \ne 0,"We can compute a determinant of an $n \times n$ matrix in $O(n^3)$ operations in several ways, for example by LU decomposition. It's also known (see, e.g., Wikipedia ) that if we can multiply two $n \times n$ matrices in $M(n)$ steps, then we can compute the determinant in $O(M(n))$ steps as well. However (and this is the motivating observation here), as in this question , if $\det(A) = 0$, then I can find a vector $\mathbf x$ such that $A \mathbf x = \mathbf 0$, and tell you: ""$A$ is a singular matrix. Here is a vector $\mathbf x$ such that $A \mathbf x = \mathbf 0$"". I might have done lots of work to find $\mathbf x$, but you can check my work in only $O(n^2)$ steps by computing $A \mathbf x$: faster than you could compute $\det(A)$ without help. Is it possible, in a similar way, for me to take a matrix $A$ with $\det(A) \ne 0$, and write a proof of this fact which you can also check faster than computing $\det(A)$? (A perfect solution would check the proof in $O(n^2)$ steps; this is best possible, since we need that many steps to even read $A$.) Observations: A probabilistic argument exists based on Freivalds's algorithm : I give you $A^{-1}$, and leave you to check that $AA^{-1} = I$. As far as we know, this still needs $O(M(n))$ time to do deterministically, but a probabilistic algorithm can take $O(n^2)$ steps to achieve a one-sided error rate of $\frac12$: if $A^{-1}$ is correct, it will always say ""yes"", and if $A^{-1}$ is wrong, it will say ""no"" with probability at most $\frac12$. As a result, you can take $O(n^2\log n)$ steps to achieve  one-sided error rate of $n^{-k}$ for any $k$. More generally, we could ask for a proof that $\det(A) = x$ for any specific nonzero value of $x$. This was the original question, but there's no hope of solving that for general $x$ if we can't even solve the $\det(A) \ne 0$ case. (After all, a proof that $\det(A)$ has a specific nonzero value $x$ is in particular a proof that $\det(A)$ has some nonzero value.)","We can compute a determinant of an $n \times n$ matrix in $O(n^3)$ operations in several ways, for example by LU decomposition. It's also known (see, e.g., Wikipedia ) that if we can multiply two $n \times n$ matrices in $M(n)$ steps, then we can compute the determinant in $O(M(n))$ steps as well. However (and this is the motivating observation here), as in this question , if $\det(A) = 0$, then I can find a vector $\mathbf x$ such that $A \mathbf x = \mathbf 0$, and tell you: ""$A$ is a singular matrix. Here is a vector $\mathbf x$ such that $A \mathbf x = \mathbf 0$"". I might have done lots of work to find $\mathbf x$, but you can check my work in only $O(n^2)$ steps by computing $A \mathbf x$: faster than you could compute $\det(A)$ without help. Is it possible, in a similar way, for me to take a matrix $A$ with $\det(A) \ne 0$, and write a proof of this fact which you can also check faster than computing $\det(A)$? (A perfect solution would check the proof in $O(n^2)$ steps; this is best possible, since we need that many steps to even read $A$.) Observations: A probabilistic argument exists based on Freivalds's algorithm : I give you $A^{-1}$, and leave you to check that $AA^{-1} = I$. As far as we know, this still needs $O(M(n))$ time to do deterministically, but a probabilistic algorithm can take $O(n^2)$ steps to achieve a one-sided error rate of $\frac12$: if $A^{-1}$ is correct, it will always say ""yes"", and if $A^{-1}$ is wrong, it will say ""no"" with probability at most $\frac12$. As a result, you can take $O(n^2\log n)$ steps to achieve  one-sided error rate of $n^{-k}$ for any $k$. More generally, we could ask for a proof that $\det(A) = x$ for any specific nonzero value of $x$. This was the original question, but there's no hope of solving that for general $x$ if we can't even solve the $\det(A) \ne 0$ case. (After all, a proof that $\det(A)$ has a specific nonzero value $x$ is in particular a proof that $\det(A)$ has some nonzero value.)",,"['linear-algebra', 'matrices', 'determinant', 'computational-complexity']"
94,What are all the generalizations needed to pass from finite dimensional linear algebra with matrices to fourier series and pdes?,What are all the generalizations needed to pass from finite dimensional linear algebra with matrices to fourier series and pdes?,,"I've studied Linear Algebra on finite dimensions and now I'm studying fourier series, sturm-liouville problems, pdes etc. However none of our lecturers made any connection between linear algebra an this. I think this is a very big mistake because I see many questions here where everyone just talks about these topics as generalizations of simple linear algebra. Hence what are the generalizations? For example I think a matrix becomes a function in infinite dimension, but that's about it.","I've studied Linear Algebra on finite dimensions and now I'm studying fourier series, sturm-liouville problems, pdes etc. However none of our lecturers made any connection between linear algebra an this. I think this is a very big mistake because I see many questions here where everyone just talks about these topics as generalizations of simple linear algebra. Hence what are the generalizations? For example I think a matrix becomes a function in infinite dimension, but that's about it.",,"['linear-algebra', 'ordinary-differential-equations', 'partial-differential-equations', 'fourier-analysis', 'sturm-liouville']"
95,Why is the norm of a matrix larger than its eigenvalue?,Why is the norm of a matrix larger than its eigenvalue?,,"I know there are different definitions of Matrix Norm, but I want to use the definition on WolframMathWorld , and Wikipedia also gives a similar definition. The definition states as below: Given a square complex or real $n\times n$ matrix $A$ , a matrix norm $\|A\|$ is a nonnegative number associated with $A$ having the properties 1. $\|A\|>0$ when $A\neq0$ and $\|A\|=0$ iff $A=0$ , 2. $\|kA\|=|k|\|A\|$ for any scalar $k$ , 3. $\|A+B\|\leq\|A\|+\|B\|$ , for $n \times n$ matrix $B$ 4. $\|AB\|\leq\|A\|\|B\|$ . Then, as the website states, we have $\|A\|\geq|\lambda|$ , here $\lambda$ is an eigenvalue of $A$ . I don't know how to prove it, by using just these four properties.","I know there are different definitions of Matrix Norm, but I want to use the definition on WolframMathWorld , and Wikipedia also gives a similar definition. The definition states as below: Given a square complex or real matrix , a matrix norm is a nonnegative number associated with having the properties 1. when and iff , 2. for any scalar , 3. , for matrix 4. . Then, as the website states, we have , here is an eigenvalue of . I don't know how to prove it, by using just these four properties.",n\times n A \|A\| A \|A\|>0 A\neq0 \|A\|=0 A=0 \|kA\|=|k|\|A\| k \|A+B\|\leq\|A\|+\|B\| n \times n B \|AB\|\leq\|A\|\|B\| \|A\|\geq|\lambda| \lambda A,"['linear-algebra', 'matrices', 'eigenvalues-eigenvectors', 'normed-spaces']"
96,Prove that the set of $n$-by-$n$ real matrices with positive determinant is connected,Prove that the set of -by- real matrices with positive determinant is connected,n n,"Math people: In the fourth edition of Strang's ""Linear Algebra and its Applications"", page 230, he poses the following problem (I have changed his wording): show that if $A \in \mathbf{R}^{n \times n}$ with $\det(A) >0$, then there exists a continuous function $f:[0,1] \to \mathbf{R}^{n \times n}$ with $f(0) = I$, $f(1) = A$, and $\det(f(t)) > 0$ for all $t \in [0,1]$.  He says ""the problem is not so easy, and solutions are welcomed by the author"" (direct quote from the book).  Since the author welcomes solutions, I am not 100% sure even he has solved it.  Does anyone know if this is true or false, and may I have a solution or a hint?  I assigned this problem to a graduate student in a linear algebra class, because I wanted to give him a challenge.  Neither one of us has solved it.  I have a lot of other things to do, but I'd like to give him a hint. STEFAN (Stack Exchange FAN)","Math people: In the fourth edition of Strang's ""Linear Algebra and its Applications"", page 230, he poses the following problem (I have changed his wording): show that if $A \in \mathbf{R}^{n \times n}$ with $\det(A) >0$, then there exists a continuous function $f:[0,1] \to \mathbf{R}^{n \times n}$ with $f(0) = I$, $f(1) = A$, and $\det(f(t)) > 0$ for all $t \in [0,1]$.  He says ""the problem is not so easy, and solutions are welcomed by the author"" (direct quote from the book).  Since the author welcomes solutions, I am not 100% sure even he has solved it.  Does anyone know if this is true or false, and may I have a solution or a hint?  I assigned this problem to a graduate student in a linear algebra class, because I wanted to give him a challenge.  Neither one of us has solved it.  I have a lot of other things to do, but I'd like to give him a hint. STEFAN (Stack Exchange FAN)",,"['linear-algebra', 'general-topology', 'matrices', 'determinant', 'connectedness']"
97,High-level linear algebra book,High-level linear algebra book,,"Please, recommend high-level and modern books on linear algebra (not for first reading). Like Kostrikin, Manin ""Linear algebra and geometry"" or respective chapters of Lang ""Algebra"".","Please, recommend high-level and modern books on linear algebra (not for first reading). Like Kostrikin, Manin ""Linear algebra and geometry"" or respective chapters of Lang ""Algebra"".",,"['linear-algebra', 'reference-request', 'book-recommendation']"
98,How to check if a set of vectors is a basis,How to check if a set of vectors is a basis,,"OK, I am having a real problem with this and I am desperate. I have a set of vectors $\{(1,0,-1), (2,5,1), (0,-4,3)\}$ . How do I check is this is a basis for $\mathbb{R}^3?$ My text says a basis $B$ for a vector space $V$ is a linearly independent subset of $V$ that generates $V$ . OK then. I need to see if these vectors are linearly independent, yes? If that is so, then for these to be linearly independent the following must be true: $a_1v_1 + a_2v_2 + ... + a_nv_n \neq 0$ for any scalars $a_i$ Is this the case or not? If it is, then I just have to see if $a_1(1,0.-1)+ a_2(2,5,1)+ a_3(0,-4,3) = 0$ or $a_1 + 2a_2 + 0a_3 = 0$ $0a_1 + 5a_2 - 4a_3 = 0$ $-a_1 + a_2  + 3a_3 = 0$ has a solution. Adding these equations up I get $8a_2 - a_3 = 0$ or $a_3 = 8a_2$ so $5a_2 - 32a_2 = 0$ which gets me $a_2 = 0$ and that implies $a_1 = 0$ and $a_3=0$ as well. So they are all linearly dependent and thus they are not a basis for $\mathbb{R}^3$ . Something tells me that this is wrong. But I am having a hell of a time figuring this stuff out. Please someone help, and I ask: pretend I am the dumbest student you ever met.","OK, I am having a real problem with this and I am desperate. I have a set of vectors . How do I check is this is a basis for My text says a basis for a vector space is a linearly independent subset of that generates . OK then. I need to see if these vectors are linearly independent, yes? If that is so, then for these to be linearly independent the following must be true: for any scalars Is this the case or not? If it is, then I just have to see if or has a solution. Adding these equations up I get or so which gets me and that implies and as well. So they are all linearly dependent and thus they are not a basis for . Something tells me that this is wrong. But I am having a hell of a time figuring this stuff out. Please someone help, and I ask: pretend I am the dumbest student you ever met.","\{(1,0,-1), (2,5,1), (0,-4,3)\} \mathbb{R}^3? B V V V a_1v_1 + a_2v_2 + ... + a_nv_n \neq 0 a_i a_1(1,0.-1)+ a_2(2,5,1)+ a_3(0,-4,3) = 0 a_1 + 2a_2 + 0a_3 = 0 0a_1 + 5a_2 - 4a_3 = 0 -a_1 + a_2  + 3a_3 = 0 8a_2 - a_3 = 0 a_3 = 8a_2 5a_2 - 32a_2 = 0 a_2 = 0 a_1 = 0 a_3=0 \mathbb{R}^3","['linear-algebra', 'vector-spaces']"
99,What is the intuitive meaning of the basis of a vector space and the span?,What is the intuitive meaning of the basis of a vector space and the span?,,"The formal definition of basis is: A basis of a vector space $V$ is defined as a subset  $v_1, v_2, . . . , v_n$ of vectors in  that are linearly independent and span vector space $V$. The definition of spanning is: A set of vectors spans a space if their linear combinations fill the space. But what is the intuitive meaning of this, and the idea of a vector span? All I know how to do is the process of solving by putting a matrix into reduced row-echelon form. Separately, I""m not sure if I should put this in a new question, but could someone relate this to an intuitive explanation for the row space and column space? So a column space is all the linear combinations of each column of matrix $A$. So what? What does this imply? And a row space is, is it a linear combination of all the rows of $A$, because the book just says its the column space of $A^T$, which I hope means the same thing. So, sure, that's what the definitions of the row space and column space are, but how do all these concepts relate? I'm getting especially confused getting to the fundamental theorems of linear algebra part where we talk about row space, column space, and nullspaces all together.","The formal definition of basis is: A basis of a vector space $V$ is defined as a subset  $v_1, v_2, . . . , v_n$ of vectors in  that are linearly independent and span vector space $V$. The definition of spanning is: A set of vectors spans a space if their linear combinations fill the space. But what is the intuitive meaning of this, and the idea of a vector span? All I know how to do is the process of solving by putting a matrix into reduced row-echelon form. Separately, I""m not sure if I should put this in a new question, but could someone relate this to an intuitive explanation for the row space and column space? So a column space is all the linear combinations of each column of matrix $A$. So what? What does this imply? And a row space is, is it a linear combination of all the rows of $A$, because the book just says its the column space of $A^T$, which I hope means the same thing. So, sure, that's what the definitions of the row space and column space are, but how do all these concepts relate? I'm getting especially confused getting to the fundamental theorems of linear algebra part where we talk about row space, column space, and nullspaces all together.",,"['linear-algebra', 'intuition']"
