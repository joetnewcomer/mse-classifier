,title_raw,title_text,title_latex,body_raw,body_text,body_latex,tags
0,Linear algebra machinery for differentiation of families of functions.,Linear algebra machinery for differentiation of families of functions.,,"So I know that since differentiation is linear, for many types of functions we can represent it using linear algebra. Famous examples include polynomials, if we represent them with their coefficients stored in vectors. Such a matrix performing differentiation on 4th order polynomial could look like: $$D(4) = \left[\begin{array}{ccccc} 0&1&0&0&0\\ 0&0&2&0&0\\ 0&0&0&3&0\\ 0&0&0&0&4 \end{array}\right] \text{assuming order of coefficients stored as  }  \left[\begin{array}{ccccc} x^0\\ x^1\\ x^2\\ x^3\\ x^4 \end{array}\right]$$ Other families of functions have other differential operators, the trigonometric functions are well known for this: $$D_{sintrig} = \left[\begin{array}{rr} 0&1\\ -1&0 \end{array}\right] \text{assuming coefficients stored like} \left[\begin{array}{rr} \sin\\ \cos \end{array}\right]$$ and the hyperbolic trigs: $$D_{hyptrig} = \left[\begin{array}{rr} 0&1\\ 1&0 \end{array}\right] \text{assuming coefficients stored like} \left[\begin{array}{rr} \sinh\\ \cosh \end{array}\right]$$ Then we can go one step further, investigating what happens when we use chain rule. A famous example for this is the Hermite polynomials which are the polynomials resulting in iteratively differentiating the function $e^{-x^2}$ which is of the form $\exp(P(x))$. This also gives linear representations, although more complicated. So finally, to my question... do there exist systematic approaches to build such differential linear algebra ""machinery"" for even more complicated families of functions?","So I know that since differentiation is linear, for many types of functions we can represent it using linear algebra. Famous examples include polynomials, if we represent them with their coefficients stored in vectors. Such a matrix performing differentiation on 4th order polynomial could look like: $$D(4) = \left[\begin{array}{ccccc} 0&1&0&0&0\\ 0&0&2&0&0\\ 0&0&0&3&0\\ 0&0&0&0&4 \end{array}\right] \text{assuming order of coefficients stored as  }  \left[\begin{array}{ccccc} x^0\\ x^1\\ x^2\\ x^3\\ x^4 \end{array}\right]$$ Other families of functions have other differential operators, the trigonometric functions are well known for this: $$D_{sintrig} = \left[\begin{array}{rr} 0&1\\ -1&0 \end{array}\right] \text{assuming coefficients stored like} \left[\begin{array}{rr} \sin\\ \cos \end{array}\right]$$ and the hyperbolic trigs: $$D_{hyptrig} = \left[\begin{array}{rr} 0&1\\ 1&0 \end{array}\right] \text{assuming coefficients stored like} \left[\begin{array}{rr} \sinh\\ \cosh \end{array}\right]$$ Then we can go one step further, investigating what happens when we use chain rule. A famous example for this is the Hermite polynomials which are the polynomials resulting in iteratively differentiating the function $e^{-x^2}$ which is of the form $\exp(P(x))$. This also gives linear representations, although more complicated. So finally, to my question... do there exist systematic approaches to build such differential linear algebra ""machinery"" for even more complicated families of functions?",,"['linear-algebra', 'ordinary-differential-equations', 'derivatives', 'representation-theory', 'symbolic-computation']"
1,Differential Equations: Jordan Form of a Matrix,Differential Equations: Jordan Form of a Matrix,,"I am using Lawrence Perko's book Differential Equations and Dynamical Systems , for my Differential Equations course. At the moment we are going over Jordan Forms of a linear system $x^{'}(t) = Ax$, where A is an $n\times n$ matrix. The solution of the system is $x(t) = e^{At}x_0 = Pe^{Bt}P^{-1}x_0$. For the purpose of this problem, we are considering two elementary Jordan blocks. \begin{align} B_1 & = \begin{bmatrix}           \lambda & 1 & 0 & \dots & 0\\           0 & \lambda & 1 & \dots & 0\\           \vdots & & \ddots\\           0 & \dots & & \lambda & 1\\           0 & \dots & & 0 & \lambda          \end{bmatrix}  &  B_2 & =  \begin{bmatrix} D & I_2 & 0 & \dots & 0\\           0 & D& I_2 & \dots & 0\\           \vdots & & \ddots\\           0 & \dots & & D& I_2\\           0 & \dots & & 0 & D          \end{bmatrix}  \end{align} 1st Elementary case is where we have $\lambda$ to be one of the real eigenvalues of some matrix $A$. \ 2nd Elementary case is where consider $\lambda$ is a complex number $a+ib$ with \begin{align} D & = \begin{bmatrix}           a & -b\\           b & a          \end{bmatrix}  &  I & =  \begin{bmatrix} 1 & 0\\ 0 & 1          \end{bmatrix}  & \text{and }& & 0 & = \begin{bmatrix} 0 & 0\\ 0 & 0  \end{bmatrix} \end{align} Here is my question: Suppose that the elementary blocks $B$ in the Jordan form of the matrix $A$, given by $B_1$ or $B_2$, have no ones or $I_2$ blocks off the diagonal respectively. Show that if all of the eigenvalues of $A$ have nonpositive real parts, then for each $x_0\in\mathbb{R}^n$ there is a positive constant $M$ such that $|x(t)|\leq M$ for all $t\geq 0$. Now since we know for a fact that the real parts of all eigenvalues $\lambda_i$ are negative or zero, then every $y_i = e^{\lambda t}\rightarrow 0$ or $y_i = e^{\lambda t}\rightarrow 1$ as $t\rightarrow \infty$ (implying the system is stable or centered). Since we also don't have the ones and $I_2$ (for each respective case), we have matrix $A$ to be semisimple. Focusing on the first case. Since we are looking to prove the norm of our solution is less than some constant $M$ for all $t\geq 0$. We have by the properties of norm operators on a linear transformation \begin{align} |x(t)| = &|e^{At}x_0|\leq ||e^{At}||\cdot |x_{0}| &=& ||Pe^{Bt}P^{-1}||\cdot|x_{0}|\\ & & \leq & ||P||\cdot ||e^{Bt}||\cdot||P^{-1}||\cdot |x_{0}|\\ & & \leq & ||P||\cdot ||e^{Bt}||\cdot\frac{1}{||P||}\cdot |x_{0}|\\ & & \leq & ||e^{Bt}||\cdot|x_{0}|\\ & & \leq & e^{||B||\cdot|t|}\cdot|x_{0}|\\ \end{align} Since the initial condition $x_0$ is a constant vector, bounded by some value $M$, and $||B||=\max_{|x|\leq 1}|T(x)| = 0$. Thus $e^{||B|||t|}x_0\leq e^{0}\cdot M = $ M. Am I on the right track? I would assume The second case follows a similar procedure. Is there anything I may have incorrectly? Please let me know. Thank You for you time. I appreciate any feedback, comments, or suggestions you may have. Thank you in advance and have a wonderful day.","I am using Lawrence Perko's book Differential Equations and Dynamical Systems , for my Differential Equations course. At the moment we are going over Jordan Forms of a linear system $x^{'}(t) = Ax$, where A is an $n\times n$ matrix. The solution of the system is $x(t) = e^{At}x_0 = Pe^{Bt}P^{-1}x_0$. For the purpose of this problem, we are considering two elementary Jordan blocks. \begin{align} B_1 & = \begin{bmatrix}           \lambda & 1 & 0 & \dots & 0\\           0 & \lambda & 1 & \dots & 0\\           \vdots & & \ddots\\           0 & \dots & & \lambda & 1\\           0 & \dots & & 0 & \lambda          \end{bmatrix}  &  B_2 & =  \begin{bmatrix} D & I_2 & 0 & \dots & 0\\           0 & D& I_2 & \dots & 0\\           \vdots & & \ddots\\           0 & \dots & & D& I_2\\           0 & \dots & & 0 & D          \end{bmatrix}  \end{align} 1st Elementary case is where we have $\lambda$ to be one of the real eigenvalues of some matrix $A$. \ 2nd Elementary case is where consider $\lambda$ is a complex number $a+ib$ with \begin{align} D & = \begin{bmatrix}           a & -b\\           b & a          \end{bmatrix}  &  I & =  \begin{bmatrix} 1 & 0\\ 0 & 1          \end{bmatrix}  & \text{and }& & 0 & = \begin{bmatrix} 0 & 0\\ 0 & 0  \end{bmatrix} \end{align} Here is my question: Suppose that the elementary blocks $B$ in the Jordan form of the matrix $A$, given by $B_1$ or $B_2$, have no ones or $I_2$ blocks off the diagonal respectively. Show that if all of the eigenvalues of $A$ have nonpositive real parts, then for each $x_0\in\mathbb{R}^n$ there is a positive constant $M$ such that $|x(t)|\leq M$ for all $t\geq 0$. Now since we know for a fact that the real parts of all eigenvalues $\lambda_i$ are negative or zero, then every $y_i = e^{\lambda t}\rightarrow 0$ or $y_i = e^{\lambda t}\rightarrow 1$ as $t\rightarrow \infty$ (implying the system is stable or centered). Since we also don't have the ones and $I_2$ (for each respective case), we have matrix $A$ to be semisimple. Focusing on the first case. Since we are looking to prove the norm of our solution is less than some constant $M$ for all $t\geq 0$. We have by the properties of norm operators on a linear transformation \begin{align} |x(t)| = &|e^{At}x_0|\leq ||e^{At}||\cdot |x_{0}| &=& ||Pe^{Bt}P^{-1}||\cdot|x_{0}|\\ & & \leq & ||P||\cdot ||e^{Bt}||\cdot||P^{-1}||\cdot |x_{0}|\\ & & \leq & ||P||\cdot ||e^{Bt}||\cdot\frac{1}{||P||}\cdot |x_{0}|\\ & & \leq & ||e^{Bt}||\cdot|x_{0}|\\ & & \leq & e^{||B||\cdot|t|}\cdot|x_{0}|\\ \end{align} Since the initial condition $x_0$ is a constant vector, bounded by some value $M$, and $||B||=\max_{|x|\leq 1}|T(x)| = 0$. Thus $e^{||B|||t|}x_0\leq e^{0}\cdot M = $ M. Am I on the right track? I would assume The second case follows a similar procedure. Is there anything I may have incorrectly? Please let me know. Thank You for you time. I appreciate any feedback, comments, or suggestions you may have. Thank you in advance and have a wonderful day.",,"['linear-algebra', 'ordinary-differential-equations', 'normed-spaces', 'jordan-normal-form']"
2,Boundedness of the solution of class of ODEs,Boundedness of the solution of class of ODEs,,"Let's have linear $$ \tag 1 y''(t) + b(t)y(t) = 0, \quad t \in (t_{0}, \infty ) $$ Here $|b(t)|$ is monotonically decreased function of time, and $\lim_{t\to \infty }b(t) = \frac{a}{t} \to 0$. How to prove that the solution of Eq. $(1)$ is bounded from above? An edit. I've made the mistake. The correct question is about boundedness of $\frac{y(t)}{t^{\frac{1}{4}}}$. Instead of Eq. $(1)$ the following equation arises: $$ \tag 2 y''(t) + (k^{2} - ab(t))y(t) = 0, \quad b(t) \to \frac{f(t)}{\sqrt{t}}, $$ where $f(t)$ is bounded periodic function. Numerical simulations show that $$ |y(t)| \leqslant \text{exp}\left[\alpha \times \frac{a}{\sqrt{k}}\right], \quad \alpha = O(1) $$ It seems that the proof of boundedness can be performed through manipulations with integral representation $$ y(t) = cos(kt)-a\int \limits_{t_{0}}^{t}sin(s - t)b(s)y(s)ds, $$ which is equal to Eq.$(2)$. But I can show that Eq. $(2)$ has bounded solutions only in case of $|b(s)| \leqslant \frac{1}{t^{d + 1}}, d > 0$, while in this case $b(s) \to \frac{f(t)}{\sqrt{t}}$.","Let's have linear $$ \tag 1 y''(t) + b(t)y(t) = 0, \quad t \in (t_{0}, \infty ) $$ Here $|b(t)|$ is monotonically decreased function of time, and $\lim_{t\to \infty }b(t) = \frac{a}{t} \to 0$. How to prove that the solution of Eq. $(1)$ is bounded from above? An edit. I've made the mistake. The correct question is about boundedness of $\frac{y(t)}{t^{\frac{1}{4}}}$. Instead of Eq. $(1)$ the following equation arises: $$ \tag 2 y''(t) + (k^{2} - ab(t))y(t) = 0, \quad b(t) \to \frac{f(t)}{\sqrt{t}}, $$ where $f(t)$ is bounded periodic function. Numerical simulations show that $$ |y(t)| \leqslant \text{exp}\left[\alpha \times \frac{a}{\sqrt{k}}\right], \quad \alpha = O(1) $$ It seems that the proof of boundedness can be performed through manipulations with integral representation $$ y(t) = cos(kt)-a\int \limits_{t_{0}}^{t}sin(s - t)b(s)y(s)ds, $$ which is equal to Eq.$(2)$. But I can show that Eq. $(2)$ has bounded solutions only in case of $|b(s)| \leqslant \frac{1}{t^{d + 1}}, d > 0$, while in this case $b(s) \to \frac{f(t)}{\sqrt{t}}$.",,"['ordinary-differential-equations', 'asymptotics', 'stability-in-odes']"
3,"$(\partial_{tt}-\nabla^2+\partial_t)f=g,\quad (\partial_t-\nabla^2+b)g=\partial_t f$",,"(\partial_{tt}-\nabla^2+\partial_t)f=g,\quad (\partial_t-\nabla^2+b)g=\partial_t f","Hi I am looking for complete solutions for $f(r,t),g(r,t)$ given in the coupled linear partial differential equations below: $$ (\partial_{tt}-a\nabla^2+b\partial_t)f(r,t)=bg(r,t) $$ $$ (\partial_t-c \nabla^2+b)g(r,t)=b\partial_t f(r,t) $$ Initial and boundary conditions are given by $$ \partial_t f(0,t)=0,\ \partial_t f(R,t)=d\cos(\omega t) $$ $$ g(0,t)=0, \  g(R,t)=d\cos (\omega t) $$ where $a,b,c,d,R,\omega>0.$  Note $$ \nabla^2\equiv \frac{1}{r}\partial_r(r\partial_r)-\frac{1}{r^2}=\partial_{rr}+\frac{1}{r}\partial_r -\frac{1}{r^2}. $$ Thank you!  Some comments: If the right hand side of both equations are zero, (equations become homogenous), then the kernel of both linear operators are known and are in terms of Bessel functions $J_1$ (we're in a cylindrical geometry hence the Laplacian like term) times oscillating functions of time: $\sin \omega t,\cos \omega t$.","Hi I am looking for complete solutions for $f(r,t),g(r,t)$ given in the coupled linear partial differential equations below: $$ (\partial_{tt}-a\nabla^2+b\partial_t)f(r,t)=bg(r,t) $$ $$ (\partial_t-c \nabla^2+b)g(r,t)=b\partial_t f(r,t) $$ Initial and boundary conditions are given by $$ \partial_t f(0,t)=0,\ \partial_t f(R,t)=d\cos(\omega t) $$ $$ g(0,t)=0, \  g(R,t)=d\cos (\omega t) $$ where $a,b,c,d,R,\omega>0.$  Note $$ \nabla^2\equiv \frac{1}{r}\partial_r(r\partial_r)-\frac{1}{r^2}=\partial_{rr}+\frac{1}{r}\partial_r -\frac{1}{r^2}. $$ Thank you!  Some comments: If the right hand side of both equations are zero, (equations become homogenous), then the kernel of both linear operators are known and are in terms of Bessel functions $J_1$ (we're in a cylindrical geometry hence the Laplacian like term) times oscillating functions of time: $\sin \omega t,\cos \omega t$.",,"['calculus', 'real-analysis', 'ordinary-differential-equations', 'partial-differential-equations', 'special-functions']"
4,Tough NL Diff Eq.,Tough NL Diff Eq.,,"I'm trying to explore $$ \left( y'' + (1/x) \, y' \right)(1-y) \,  –  \, (1/x)\left(y'\right)^4 = 0 $$ with the initial conditions $y(0) = 0$ and $y'(0) = 1$. By substitution I can show that an asymptotic solution exists of the form $y = x - (1/4)x^2 - (1/12)x^3$ and it seems to have a decent radius of convergence, but when I try to run the Taylor expansion further I run into problems in which the coefficients become overconstrained causing attempts to reach higher order in the series to fail. Any ideas how to approach learning what the solution function looks like at other places along $x$? WGH","I'm trying to explore $$ \left( y'' + (1/x) \, y' \right)(1-y) \,  –  \, (1/x)\left(y'\right)^4 = 0 $$ with the initial conditions $y(0) = 0$ and $y'(0) = 1$. By substitution I can show that an asymptotic solution exists of the form $y = x - (1/4)x^2 - (1/12)x^3$ and it seems to have a decent radius of convergence, but when I try to run the Taylor expansion further I run into problems in which the coefficients become overconstrained causing attempts to reach higher order in the series to fail. Any ideas how to approach learning what the solution function looks like at other places along $x$? WGH",,['ordinary-differential-equations']
5,Difficulty understanding Floquet multipliers wrt Mathieu equation,Difficulty understanding Floquet multipliers wrt Mathieu equation,,"We have the system $$\begin{pmatrix}y\\z\end{pmatrix}' = \begin{pmatrix}0 & 1 \\ a-2\epsilon \cos t & 0 \end{pmatrix}\begin{pmatrix}y\\z\end{pmatrix}$$ and from Abel's formula we have that $\det \Phi(T) = 1$ if we choose $\Phi(0) = I$ We also know that the characteristic multipliers solve $$\mu^2 - \text{tr}(\Phi(T))\mu + \det \Phi(T) = 0$$ Defining $2\phi := \text{tr}(\Phi(T))$, we get $\mu^2 - 2\phi \mu + 1 = 0$ and $$\mu = \phi \pm \sqrt{\phi^2-1}$$ Starting with different values of $\phi$, we can then investigate stability. What I'm confused about is how we go from the initial parameters $a$ and $\epsilon$ to the value of $\phi = \text{tr}(\Phi(t))$. We kind of sped past Floquet theory in this class, so I've clearly missed something important. How do we find $\Phi(t)$ here?","We have the system $$\begin{pmatrix}y\\z\end{pmatrix}' = \begin{pmatrix}0 & 1 \\ a-2\epsilon \cos t & 0 \end{pmatrix}\begin{pmatrix}y\\z\end{pmatrix}$$ and from Abel's formula we have that $\det \Phi(T) = 1$ if we choose $\Phi(0) = I$ We also know that the characteristic multipliers solve $$\mu^2 - \text{tr}(\Phi(T))\mu + \det \Phi(T) = 0$$ Defining $2\phi := \text{tr}(\Phi(T))$, we get $\mu^2 - 2\phi \mu + 1 = 0$ and $$\mu = \phi \pm \sqrt{\phi^2-1}$$ Starting with different values of $\phi$, we can then investigate stability. What I'm confused about is how we go from the initial parameters $a$ and $\epsilon$ to the value of $\phi = \text{tr}(\Phi(t))$. We kind of sped past Floquet theory in this class, so I've clearly missed something important. How do we find $\Phi(t)$ here?",,['ordinary-differential-equations']
6,Constructing a metric for the tautological line bundle of $\mathbb C P^2$,Constructing a metric for the tautological line bundle of,\mathbb C P^2,"I'm doing some independent reading in differential geometry, and the following is my attempt to work out the details of the construction of the tautological bundle on $\mathbb CP^2$ and the induced metric. I'm posting this as a sort of sanity check. Any corrections or nitpicks are very welcome. Let me first consider $CP^2$ with the standard charts, just to fix notation. That is , $U_2=\{[x:y:z] : z\neq 0\}$ with the map $\phi_0([x:y:z])=(x/z, y/z)$ and similarly for $U_0$ and $U_1$. The transition map from $U_2$ to $U_1$, for example, is  $(x,y) \rightarrow [x:y:1]\rightarrow (x/y, 1/y)$. We let $E\subset \mathbb CP^2 \times \mathbb C^3$ be the set of points $([s], ws)$ with $s$ nonzero and $w\in \mathbb C$. I claim this is a line bundle. It suffices to give trivializations and transition functions. Over $U_2$, we give $([x:y:z], w(x,y,z))\rightarrow (x/y, y/z, w)$ and similarly for the other $U_i$. Let us compute the transition function from part of the bundle over $U_2$ to the part over $U_1$. $$(x,y, t)\rightarrow ([x:y:1], t(x,y,1))=([x/y:1:1/y], ty(x/y,1,1/y))\rightarrow (x/y, 1/y, ty)$$ We get similar transitions for the other two intersections. If $([z], tz)$ and $([z], sz)$ are two elements of the tangent space over $[z]$, we can introduce the metric $\langle tz,sz\rangle $, where the brackets indicate the standard Hermitian metric on $\mathbb C^3$. In coordinates over $U_2$, we look at the points $(x,y,t)$ and $(x,y,s)$ and define the inner product here to be $\langle t(x,y,1), s(x,y,1)\rangle$. Is there anything else I should be checking here?","I'm doing some independent reading in differential geometry, and the following is my attempt to work out the details of the construction of the tautological bundle on $\mathbb CP^2$ and the induced metric. I'm posting this as a sort of sanity check. Any corrections or nitpicks are very welcome. Let me first consider $CP^2$ with the standard charts, just to fix notation. That is , $U_2=\{[x:y:z] : z\neq 0\}$ with the map $\phi_0([x:y:z])=(x/z, y/z)$ and similarly for $U_0$ and $U_1$. The transition map from $U_2$ to $U_1$, for example, is  $(x,y) \rightarrow [x:y:1]\rightarrow (x/y, 1/y)$. We let $E\subset \mathbb CP^2 \times \mathbb C^3$ be the set of points $([s], ws)$ with $s$ nonzero and $w\in \mathbb C$. I claim this is a line bundle. It suffices to give trivializations and transition functions. Over $U_2$, we give $([x:y:z], w(x,y,z))\rightarrow (x/y, y/z, w)$ and similarly for the other $U_i$. Let us compute the transition function from part of the bundle over $U_2$ to the part over $U_1$. $$(x,y, t)\rightarrow ([x:y:1], t(x,y,1))=([x/y:1:1/y], ty(x/y,1,1/y))\rightarrow (x/y, 1/y, ty)$$ We get similar transitions for the other two intersections. If $([z], tz)$ and $([z], sz)$ are two elements of the tangent space over $[z]$, we can introduce the metric $\langle tz,sz\rangle $, where the brackets indicate the standard Hermitian metric on $\mathbb C^3$. In coordinates over $U_2$, we look at the points $(x,y,t)$ and $(x,y,s)$ and define the inner product here to be $\langle t(x,y,1), s(x,y,1)\rangle$. Is there anything else I should be checking here?",,"['ordinary-differential-equations', 'riemannian-geometry']"
7,Effect of adding a constant to the torsion of a 3D curve,Effect of adding a constant to the torsion of a 3D curve,,"Let $\gamma$ be an arc-length parametrized curve in $\mathbb{R}^3$. Let say I add a constant to the torsion of $\gamma$ and let $\widetilde{\gamma}$ be the curve associated to the curvature of $\gamma$ and this new torsion. Is there an easy way to characterize $\widetilde{\gamma}$ in terms of $\gamma$? In other words, is there a map that links $\gamma$ to $\widetilde{\gamma}$?","Let $\gamma$ be an arc-length parametrized curve in $\mathbb{R}^3$. Let say I add a constant to the torsion of $\gamma$ and let $\widetilde{\gamma}$ be the curve associated to the curvature of $\gamma$ and this new torsion. Is there an easy way to characterize $\widetilde{\gamma}$ in terms of $\gamma$? In other words, is there a map that links $\gamma$ to $\widetilde{\gamma}$?",,"['calculus', 'ordinary-differential-equations', 'differential-geometry', 'curves']"
8,Two variable perturbation analysis of differential equations,Two variable perturbation analysis of differential equations,,"I have following set of equations, $\frac{dy(t)}{dt}=k z(t) - 3 k y(t) - y(t)^2 + \epsilon_1 (M-z(t))^2$ $\epsilon_2 \frac{dz(t)}{dt}=Mz(t) - z(t) y(t) - 2 \epsilon_2 y(t) + 2 \epsilon_1 \epsilon_2 (M-z(t))^2$ From above, I can't even solve the unperturbed problem if the perturbation is around only one of the $\epsilon_1$ or $\epsilon_2$ . Now my question is: Is it possible to do perturbation theory for two small parameters and not just one? One more thing to note is: the equations are singular for $\epsilon_2$ but regular for $\epsilon_1$ . Any suggestion, reference or help on how to go about solving these equations will be of great help.","I have following set of equations, From above, I can't even solve the unperturbed problem if the perturbation is around only one of the or . Now my question is: Is it possible to do perturbation theory for two small parameters and not just one? One more thing to note is: the equations are singular for but regular for . Any suggestion, reference or help on how to go about solving these equations will be of great help.",\frac{dy(t)}{dt}=k z(t) - 3 k y(t) - y(t)^2 + \epsilon_1 (M-z(t))^2 \epsilon_2 \frac{dz(t)}{dt}=Mz(t) - z(t) y(t) - 2 \epsilon_2 y(t) + 2 \epsilon_1 \epsilon_2 (M-z(t))^2 \epsilon_1 \epsilon_2 \epsilon_2 \epsilon_1,"['ordinary-differential-equations', 'nonlinear-system', 'perturbation-theory']"
9,Using a Fourier Series to Solve Differential Equation,Using a Fourier Series to Solve Differential Equation,,"The problem states to use the fourier series of the function f(t) defined as follows: $f(t)= t+1 , -1<t<0  $ $f(t)=1-t  , 0<t<1$ to solve the differential equation: x''+4x=f(t), x(0)=1, x'(0)=0 I have found the fourier series of f(t) to be: $\frac1 2 + \sum_{n=odd}^\infty \frac{cos(\pi n t)}{n^2}$ the complementary solution for the homogeneous equation is: $x_c= C_1 cos(2t) + C_2 sin(2t)$ The particular solution corresponding to the constant 1/2 is 1/8 I then proceed to find the coefficient of each n-th particular solution corresponding to the n-th $cos(n \pi t)$ this turns out to be $\frac{1}{n^2(4-n^2 \pi^2)}$ The solution should be a sum of the complementary and particular solutions, thus: $x=C_1 cos(2t) + C_2 sin(2t)+\frac{1}{8} + \sum_{n=odd}^\infty \frac{cos(\pi n t)}{n^2(4-n^2 \pi^2)}$ differentiating this termwise yields: $x'=-2C_1 sin(2t) + 2C_2 cos(2t) + \sum_{n=odd}^\infty \frac{-\pi n sin(\pi n t)}{n^2(4-n^2 \pi^2)}$ using the initial condition x'(0)=0 it is apperent $C_2$=0 thus: $x=C_1 cos(2t) +\frac{1}{8} + \sum_{n=odd}^\infty \frac{cos(\pi n t)}{n^2(4-n^2 \pi^2)}$ This is where I am lost, how do I find the remaining coefficient? My intuition tells me to use the initial condition x(0)=1 which yields: $C_1 + \frac{1}{8} + \sum_{n=odd}^\infty \frac{1}{n^2(4-n^2 \pi^2)}$=1 but this implies: $C_1 = \frac{-7}{8} - \sum_{n=odd}^\infty \frac{1}{n^2(4-n^2 \pi^2)}$ is such a solution allowed or is there a different way of doing this? I appreciate all the help in advance, I have a big test on this and really need to know!","The problem states to use the fourier series of the function f(t) defined as follows: $f(t)= t+1 , -1<t<0  $ $f(t)=1-t  , 0<t<1$ to solve the differential equation: x''+4x=f(t), x(0)=1, x'(0)=0 I have found the fourier series of f(t) to be: $\frac1 2 + \sum_{n=odd}^\infty \frac{cos(\pi n t)}{n^2}$ the complementary solution for the homogeneous equation is: $x_c= C_1 cos(2t) + C_2 sin(2t)$ The particular solution corresponding to the constant 1/2 is 1/8 I then proceed to find the coefficient of each n-th particular solution corresponding to the n-th $cos(n \pi t)$ this turns out to be $\frac{1}{n^2(4-n^2 \pi^2)}$ The solution should be a sum of the complementary and particular solutions, thus: $x=C_1 cos(2t) + C_2 sin(2t)+\frac{1}{8} + \sum_{n=odd}^\infty \frac{cos(\pi n t)}{n^2(4-n^2 \pi^2)}$ differentiating this termwise yields: $x'=-2C_1 sin(2t) + 2C_2 cos(2t) + \sum_{n=odd}^\infty \frac{-\pi n sin(\pi n t)}{n^2(4-n^2 \pi^2)}$ using the initial condition x'(0)=0 it is apperent $C_2$=0 thus: $x=C_1 cos(2t) +\frac{1}{8} + \sum_{n=odd}^\infty \frac{cos(\pi n t)}{n^2(4-n^2 \pi^2)}$ This is where I am lost, how do I find the remaining coefficient? My intuition tells me to use the initial condition x(0)=1 which yields: $C_1 + \frac{1}{8} + \sum_{n=odd}^\infty \frac{1}{n^2(4-n^2 \pi^2)}$=1 but this implies: $C_1 = \frac{-7}{8} - \sum_{n=odd}^\infty \frac{1}{n^2(4-n^2 \pi^2)}$ is such a solution allowed or is there a different way of doing this? I appreciate all the help in advance, I have a big test on this and really need to know!",,"['sequences-and-series', 'ordinary-differential-equations', 'fourier-series']"
10,Show that any closed disk in $\mathbb{R²}$ containing a limited semi-orbit of $x' = f(x)$ necessarily contains one equilibrium point.,Show that any closed disk in  containing a limited semi-orbit of  necessarily contains one equilibrium point.,\mathbb{R²} x' = f(x),"Show that any closed disk in $\mathbb{R²}$ containing a limited semi-orbit of $x' = f(x)$ necessarily contains one equilibrium point. Sorry for the mistakes in the translation, I am Brazilian and thanks for the help.","Show that any closed disk in $\mathbb{R²}$ containing a limited semi-orbit of $x' = f(x)$ necessarily contains one equilibrium point. Sorry for the mistakes in the translation, I am Brazilian and thanks for the help.",,"['analysis', 'ordinary-differential-equations']"
11,clarification of a doubt over a defined result in ODE,clarification of a doubt over a defined result in ODE,,"I was going through the topic of Wronskian in ODE came up with the following result: I have a little doubt. Can we say the same if we interchange $y_1$  and $y_2$ i.e. between consecutive zeroes of $y_2$ there is exactly one zero of $y_1$. Or, between successive zeroes of $y_1~(or~y_2)$ there is exactly one zero of $y_2~(or~y_1$) Kindly help.","I was going through the topic of Wronskian in ODE came up with the following result: I have a little doubt. Can we say the same if we interchange $y_1$  and $y_2$ i.e. between consecutive zeroes of $y_2$ there is exactly one zero of $y_1$. Or, between successive zeroes of $y_1~(or~y_2)$ there is exactly one zero of $y_2~(or~y_1$) Kindly help.",,['ordinary-differential-equations']
12,Find extremum of functional,Find extremum of functional,,"I want to find the extremum of $$J(y)= \int_1^2 \frac{\sqrt{1+y'^2}}{x}dx, \ y(1)=0, \ \ y(2)=1$$ I thought to use the following theorem: If $y$ is a local extremum for the functional $J(y)= \int_a^b L(x,y,y') dx$ with $y \in C^2([a,b]), \ y(a)=y_0, \ y(b)=y_1$ then the extremum $y$ satifies the ordinary differential equation of second order: $$L_y(x,y,y')- \frac{d}{dx} L_{y'}(x,y,y')=0 (\text{ Euler's equation})$$ That's what I have tried so far: $$L(x,y,y')= \frac{\sqrt{1+y'^2}}{x}$$ $$L_y(x,y,y')=0$$ $$L_{y'}(x,y,y')= \frac{2y'}{2x \sqrt{1+y'^2}}$$ So Euler's equation for this problem is: $$- \frac{d}{dx} \left( \frac{2y'}{2x \sqrt{1+y'^2}} \right)=0 \Rightarrow \frac{y'}{x \sqrt{1+y'^2}}=c \Rightarrow y'=cx \sqrt{1+y'^2} \\ \Rightarrow y'^2=c^2 x^2 (1+y'^2) \Rightarrow \left( \frac{dy}{dx}\right)^2=c^2x^2+ c^2 x^2 \left( \frac{dy}{dx} \right)^2 \\ \Rightarrow \left( 1-c^2x^2 \right) \left( \frac{dy}{dx}\right)^2=c^2 x^2 \Rightarrow \left( \frac{dy}{dx} \right)^2=\frac{c^2 x^2}{1-c^2 x^2} \\ \Rightarrow \frac{dy}{dx}= \pm  c \frac{x}{\sqrt{1-c^2 x^2}}$$ We set $\pm c= C$ Then: $\frac{dy}{dx}= \frac{Cx}{\sqrt{1-C^2 x^2}} \Rightarrow dy= \frac{Cx}{\sqrt{1-C^2 x^2}} dx \Rightarrow y(x)=- \frac{1}{c} \sqrt{1-c^2x^2}+k$ $$y(1)=0 \Rightarrow - \frac{1}{c} \sqrt{1-c^2}+k=0 \ (1)$$ $$y(2)=1 \Rightarrow -\frac{1}{c} \sqrt{1-4c^2}+k=1 \ (2)$$ Is it right so far? How could we continue? EDIT : $$(2)-(1) \Rightarrow \frac{1}{c} \sqrt{1-c^2}- \frac{1}{c} \sqrt{1-4c^2}=1 \Rightarrow \sqrt{1-c^2}-  \sqrt{1-4c^2}=c \Rightarrow (\sqrt{1-c^2}-  \sqrt{1-4c^2})^2= c^2 \Rightarrow \dots \Rightarrow 1-6c^2+9c^4=1-4c^2-c^2+4c^4 \Rightarrow 5c^4=5c^2 \Rightarrow c^2(c^2-1)=0 \Rightarrow c=0 \text{ or } c= \pm 1$$ EDIT 2 : We reject $c=0$ since we dicide by $c$. But from the relation $-\frac{1}{c} \sqrt{1-4c^2}+k=1$ we have also the restriction $1-4c^2 \geq 0 \Rightarrow 1 \geq 4c^2 \Rightarrow c^2 \leq \frac{1}{4} \Rightarrow - \frac{1}{2} \leq c \leq \frac{1}{2}$. So $c$ can't take the values $\pm 1$. So does this mean that there is no extremum? Or have I done something wrong? EDIT 3 : I must have done something wrong with the calculations. I retried them and I got $c= \frac{1}{\sqrt{5}}$. So we get $k=\frac{1}{c} \sqrt{1-c^2}=\sqrt{5}\sqrt{1-\frac{1}{5}}=\sqrt{5} \sqrt{\frac{4}{5}}=\sqrt{4}=2$ So we deduce that $y(x)= - \sqrt{5} \sqrt{1-\frac{x^2}{5}}+2$ is a local minimum. Or am I wrong?","I want to find the extremum of $$J(y)= \int_1^2 \frac{\sqrt{1+y'^2}}{x}dx, \ y(1)=0, \ \ y(2)=1$$ I thought to use the following theorem: If $y$ is a local extremum for the functional $J(y)= \int_a^b L(x,y,y') dx$ with $y \in C^2([a,b]), \ y(a)=y_0, \ y(b)=y_1$ then the extremum $y$ satifies the ordinary differential equation of second order: $$L_y(x,y,y')- \frac{d}{dx} L_{y'}(x,y,y')=0 (\text{ Euler's equation})$$ That's what I have tried so far: $$L(x,y,y')= \frac{\sqrt{1+y'^2}}{x}$$ $$L_y(x,y,y')=0$$ $$L_{y'}(x,y,y')= \frac{2y'}{2x \sqrt{1+y'^2}}$$ So Euler's equation for this problem is: $$- \frac{d}{dx} \left( \frac{2y'}{2x \sqrt{1+y'^2}} \right)=0 \Rightarrow \frac{y'}{x \sqrt{1+y'^2}}=c \Rightarrow y'=cx \sqrt{1+y'^2} \\ \Rightarrow y'^2=c^2 x^2 (1+y'^2) \Rightarrow \left( \frac{dy}{dx}\right)^2=c^2x^2+ c^2 x^2 \left( \frac{dy}{dx} \right)^2 \\ \Rightarrow \left( 1-c^2x^2 \right) \left( \frac{dy}{dx}\right)^2=c^2 x^2 \Rightarrow \left( \frac{dy}{dx} \right)^2=\frac{c^2 x^2}{1-c^2 x^2} \\ \Rightarrow \frac{dy}{dx}= \pm  c \frac{x}{\sqrt{1-c^2 x^2}}$$ We set $\pm c= C$ Then: $\frac{dy}{dx}= \frac{Cx}{\sqrt{1-C^2 x^2}} \Rightarrow dy= \frac{Cx}{\sqrt{1-C^2 x^2}} dx \Rightarrow y(x)=- \frac{1}{c} \sqrt{1-c^2x^2}+k$ $$y(1)=0 \Rightarrow - \frac{1}{c} \sqrt{1-c^2}+k=0 \ (1)$$ $$y(2)=1 \Rightarrow -\frac{1}{c} \sqrt{1-4c^2}+k=1 \ (2)$$ Is it right so far? How could we continue? EDIT : $$(2)-(1) \Rightarrow \frac{1}{c} \sqrt{1-c^2}- \frac{1}{c} \sqrt{1-4c^2}=1 \Rightarrow \sqrt{1-c^2}-  \sqrt{1-4c^2}=c \Rightarrow (\sqrt{1-c^2}-  \sqrt{1-4c^2})^2= c^2 \Rightarrow \dots \Rightarrow 1-6c^2+9c^4=1-4c^2-c^2+4c^4 \Rightarrow 5c^4=5c^2 \Rightarrow c^2(c^2-1)=0 \Rightarrow c=0 \text{ or } c= \pm 1$$ EDIT 2 : We reject $c=0$ since we dicide by $c$. But from the relation $-\frac{1}{c} \sqrt{1-4c^2}+k=1$ we have also the restriction $1-4c^2 \geq 0 \Rightarrow 1 \geq 4c^2 \Rightarrow c^2 \leq \frac{1}{4} \Rightarrow - \frac{1}{2} \leq c \leq \frac{1}{2}$. So $c$ can't take the values $\pm 1$. So does this mean that there is no extremum? Or have I done something wrong? EDIT 3 : I must have done something wrong with the calculations. I retried them and I got $c= \frac{1}{\sqrt{5}}$. So we get $k=\frac{1}{c} \sqrt{1-c^2}=\sqrt{5}\sqrt{1-\frac{1}{5}}=\sqrt{5} \sqrt{\frac{4}{5}}=\sqrt{4}=2$ So we deduce that $y(x)= - \sqrt{5} \sqrt{1-\frac{x^2}{5}}+2$ is a local minimum. Or am I wrong?",,"['ordinary-differential-equations', 'optimization', 'calculus-of-variations']"
13,Differential Equation (Non linear to linear differential equation),Differential Equation (Non linear to linear differential equation),,"Show that the substitution $u=\frac{1}{y}$ transform the non-linear differential equation $$\frac{dy}{dx}+\frac{y}{x}=y^2\ln (x)$$ into the linear differential equation $$\frac{du}{dx}-\frac{u}{x}=-\ln (x)$$. Solve this linear differential equation, and hence obtain $y$ in terms of $x$, given that $y=\frac{1}{2}$ when $x=1$ My attempt, $\frac{dy}{dx}+\frac{y}{x}=y^2\ln (x)$ $\frac{-\frac{dy}{dx}}{y^2}-\frac{1}{xy}=-\ln (x)$ Let $u=\frac{1}{y}$ $\frac{du}{dx}=\frac{-\frac{dy}{dx}}{y^2}$ $\frac{du}{dx}-\frac{y}{x}=-\ln (x)$ $e^{\int -\frac{1}{x}dx}=\frac{1}{x}$ $\frac{\frac{du}{dx}}{x}-\frac{u}{x^2}=-\frac{\ln (x)}{x}$ $\frac{\frac{du}{dx}}{x}+\frac{d}{dx}(\frac{1}{x})u=-\frac{\ln (x)}{x}$ $\frac{d}{dx}(\frac{u}{x})=\frac{-\ln (x)}{x}$ $\int \frac{d}{dx}(\frac{u}{x})dx=\int \frac{-\ln (x)}{x}dx$ $\frac{u}{x}=-\frac{1}{2}\ln ^2(x)+c_1$ $u=x(-\frac{1}{2}\ln ^2(x)+c_1)$ $y=\frac{1}{u}=\frac{2}{-x\ln ^2(x)+2c_1x}$ $y=\frac{2}{-x\ln ^2(x)+c_1x}$ When $x=1$, $y=\frac{1}{2}$ I got $c_1=4$ Therefore, $y=\frac{2}{-x\ln ^2(x)+4x}$ Is this working right? Is there another method to solve this question?","Show that the substitution $u=\frac{1}{y}$ transform the non-linear differential equation $$\frac{dy}{dx}+\frac{y}{x}=y^2\ln (x)$$ into the linear differential equation $$\frac{du}{dx}-\frac{u}{x}=-\ln (x)$$. Solve this linear differential equation, and hence obtain $y$ in terms of $x$, given that $y=\frac{1}{2}$ when $x=1$ My attempt, $\frac{dy}{dx}+\frac{y}{x}=y^2\ln (x)$ $\frac{-\frac{dy}{dx}}{y^2}-\frac{1}{xy}=-\ln (x)$ Let $u=\frac{1}{y}$ $\frac{du}{dx}=\frac{-\frac{dy}{dx}}{y^2}$ $\frac{du}{dx}-\frac{y}{x}=-\ln (x)$ $e^{\int -\frac{1}{x}dx}=\frac{1}{x}$ $\frac{\frac{du}{dx}}{x}-\frac{u}{x^2}=-\frac{\ln (x)}{x}$ $\frac{\frac{du}{dx}}{x}+\frac{d}{dx}(\frac{1}{x})u=-\frac{\ln (x)}{x}$ $\frac{d}{dx}(\frac{u}{x})=\frac{-\ln (x)}{x}$ $\int \frac{d}{dx}(\frac{u}{x})dx=\int \frac{-\ln (x)}{x}dx$ $\frac{u}{x}=-\frac{1}{2}\ln ^2(x)+c_1$ $u=x(-\frac{1}{2}\ln ^2(x)+c_1)$ $y=\frac{1}{u}=\frac{2}{-x\ln ^2(x)+2c_1x}$ $y=\frac{2}{-x\ln ^2(x)+c_1x}$ When $x=1$, $y=\frac{1}{2}$ I got $c_1=4$ Therefore, $y=\frac{2}{-x\ln ^2(x)+4x}$ Is this working right? Is there another method to solve this question?",,"['calculus', 'integration', 'ordinary-differential-equations']"
14,Solving differential equation,Solving differential equation,,"I want to solve the following differential equation with initial conditions: $$\frac{\mathrm{d}^2 y}{\mathrm{d} x^2}=\frac{x \, y(x)}{\sqrt{1-x}}$$   But do not know how to actually solve it. Any suggestion?","I want to solve the following differential equation with initial conditions: $$\frac{\mathrm{d}^2 y}{\mathrm{d} x^2}=\frac{x \, y(x)}{\sqrt{1-x}}$$   But do not know how to actually solve it. Any suggestion?",,['ordinary-differential-equations']
15,What does affine invariance mean in the context of the Newton's method?,What does affine invariance mean in the context of the Newton's method?,,"The textbook Numerical Solution of Boundary Value Problems for Ordinary Differential Equations (by Ascher, Mattheij, and Russell) states on page 329 : [W]e observe  that Newton's method  is affine  invariant ; i. e., if $$\hat{f}(s)=Bf(s)$$ where $B$  is a constant,  nonsingular matrix, then Newton's  method  yields  precisely  the same  sequence  of  iterates  for  the  two  functions  $f$ and  $\hat{f}$.  (Note  that  $f$ and  $\hat{f}$  have the same  zeroes.) And I've seen pretty much the same definition of affine invariance in other texts. I get the invariance part: $$\hat{f'}(s)^{-1}\hat{f}(s)=(Bf'(s))^{-1}Bf(s)=f(s)^{-1}f(s).$$ But if $B$ is any non-singular matrix, what is so affine about it? E.g. if we compose $f$ with a translation, then it doesn't seem to work anymore. Why do we call the invariance affine ?","The textbook Numerical Solution of Boundary Value Problems for Ordinary Differential Equations (by Ascher, Mattheij, and Russell) states on page 329 : [W]e observe  that Newton's method  is affine  invariant ; i. e., if $$\hat{f}(s)=Bf(s)$$ where $B$  is a constant,  nonsingular matrix, then Newton's  method  yields  precisely  the same  sequence  of  iterates  for  the  two  functions  $f$ and  $\hat{f}$.  (Note  that  $f$ and  $\hat{f}$  have the same  zeroes.) And I've seen pretty much the same definition of affine invariance in other texts. I get the invariance part: $$\hat{f'}(s)^{-1}\hat{f}(s)=(Bf'(s))^{-1}Bf(s)=f(s)^{-1}f(s).$$ But if $B$ is any non-singular matrix, what is so affine about it? E.g. if we compose $f$ with a translation, then it doesn't seem to work anymore. Why do we call the invariance affine ?",,"['linear-algebra', 'ordinary-differential-equations', 'numerical-methods', 'numerical-linear-algebra', 'affine-geometry']"
16,Difficult exercise on unicity of solutions for an IVP,Difficult exercise on unicity of solutions for an IVP,,"Suppose $f$ and $g$ are continuous and $g$ is odd and  strictly increasing function. I have to prove that the IVP $$y'=f(x)g(y)$$ $$y(0)=1$$ has a unique solution if and only if $$\lim \limits_{u \to 0} \left [ \int_{u}^{1} \frac {1}{g(y)} dy \right] = + \infty$$ Does someone have a hint on what I could use? I have absolutely no idea from where the result follows. As $g$ is odd (and so $g(0)=0$), I would expect the limit to be always infinite. I know that any solution of the ODE will satisfy $$\int \frac {1}{g(y)} dy = \int f(x) dx$$ but I don't know how I could use this result...","Suppose $f$ and $g$ are continuous and $g$ is odd and  strictly increasing function. I have to prove that the IVP $$y'=f(x)g(y)$$ $$y(0)=1$$ has a unique solution if and only if $$\lim \limits_{u \to 0} \left [ \int_{u}^{1} \frac {1}{g(y)} dy \right] = + \infty$$ Does someone have a hint on what I could use? I have absolutely no idea from where the result follows. As $g$ is odd (and so $g(0)=0$), I would expect the limit to be always infinite. I know that any solution of the ODE will satisfy $$\int \frac {1}{g(y)} dy = \int f(x) dx$$ but I don't know how I could use this result...",,"['calculus', 'real-analysis', 'ordinary-differential-equations']"
17,Is this differential equation separable?,Is this differential equation separable?,,"$$x\frac{dy}{dx}-y^2 = \frac{dy}{dx}+5$$ I have found that this equation is differentiable as shown in the following. $$x\frac{dy}{dx}-\frac{dy}{dx} = y^2+5$$ $$dy(\frac{x}{dx}-\frac{1}{dx}) = y^2+5$$ $$\frac{x-1}{dx} = \frac{y^2+5}{dy}$$ $$\frac{dx}{x-1} = \frac{dy}{y^2+5}$$ $$dx(\frac{1}{x-1}) = dy(\frac{1}{y^2+5})$$ Is this method valid? Or to be more specific, is the following equation valid? $$x\frac{dy}{dx}-\frac{dy}{dx} =dy(\frac{x}{dx}-\frac{1}{dx})  $$","$$x\frac{dy}{dx}-y^2 = \frac{dy}{dx}+5$$ I have found that this equation is differentiable as shown in the following. $$x\frac{dy}{dx}-\frac{dy}{dx} = y^2+5$$ $$dy(\frac{x}{dx}-\frac{1}{dx}) = y^2+5$$ $$\frac{x-1}{dx} = \frac{y^2+5}{dy}$$ $$\frac{dx}{x-1} = \frac{dy}{y^2+5}$$ $$dx(\frac{1}{x-1}) = dy(\frac{1}{y^2+5})$$ Is this method valid? Or to be more specific, is the following equation valid? $$x\frac{dy}{dx}-\frac{dy}{dx} =dy(\frac{x}{dx}-\frac{1}{dx})  $$",,"['calculus', 'ordinary-differential-equations']"
18,How can one prove the existence and uniqueness of solutions to linear differential equations?,How can one prove the existence and uniqueness of solutions to linear differential equations?,,"It is a theorem (I think) that the equation: $$\mathbf{x}'(t) = A(t)\mathbf{x}(t) + \mathbf{b}(t); \qquad \qquad \mathbf{x}(t_0) = \mathbf{x}_0$$ Has a unique global solution for any matrix $A(t)$ and vector $\mathbf{b}(t)$ whose entries are continuous functions of $t$. My question is: Is there a relatively simple proof of existence and uniqueness? (By relatively simple, I mean not applying Picard-Lindelof). In the case that $A$ is a constant matrix, one has a fairly easy proof using the matrix exponential. In the general case, perhaps one could use a time-ordered matrix exponential? Or perhaps there is a nice inductive proof based on the size of $A$?","It is a theorem (I think) that the equation: $$\mathbf{x}'(t) = A(t)\mathbf{x}(t) + \mathbf{b}(t); \qquad \qquad \mathbf{x}(t_0) = \mathbf{x}_0$$ Has a unique global solution for any matrix $A(t)$ and vector $\mathbf{b}(t)$ whose entries are continuous functions of $t$. My question is: Is there a relatively simple proof of existence and uniqueness? (By relatively simple, I mean not applying Picard-Lindelof). In the case that $A$ is a constant matrix, one has a fairly easy proof using the matrix exponential. In the general case, perhaps one could use a time-ordered matrix exponential? Or perhaps there is a nice inductive proof based on the size of $A$?",,"['linear-algebra', 'ordinary-differential-equations', 'alternative-proof']"
19,Polar representation of conic sections $r(\theta)=\frac1{1 + e \cos\theta}$,Polar representation of conic sections,r(\theta)=\frac1{1 + e \cos\theta},"Consider a curve given in polar coordinates by $r(\theta) = \dfrac1{1 + e \cos\theta}$, where $e\ge0$. a) Show that the distance of each point on this curve to the line $x=\frac1e$ is a constant multiple of $r(\theta)$. b) When $e>1$, show that the curve approaches two asymptotes, find them and sketch the curve. Hint: If the critical angles are $\pm\theta_0$, compute the vertical distance of the point of the curve at angle $\theta = \theta_0+h$ to the line $\theta=\theta_0$, and take a limit using Taylor approximations. c) Observe that the curve is bounded if and only if $e < 1$. Show that the curve is an ellipse as follows: Let $a$ be the midpoint between the two points intersecting the $x$-axis. Show that $(1-e^2)(x-a)^2 + y^2$ is constant. d) What happens when $e=1$? I'm not really sure where to begin with this question.  I know that the curve is an ellipse if $e<1$, a parabola if $e=1$ and a hyperbola if $e>1$. I converted to parametric form, $x=\frac{\cos(\theta)}{1+e\cos(\theta)}$, $y=\frac{\sin(\theta)}{1+e\cos(\theta)}$ but I'm not sure this helps. I'm also not sure why this is on a differential equation assignment. Any input is greatly appreciated. EDIT: Okay, so I've got part a) down. Using the Cartesian coordinates, $\left|\frac{1}{e}-\frac{\cos\theta}{1+e\cos\theta}\right|=\left|1+e\cos\theta-e\cos\theta\right|=1$ which is constant.","Consider a curve given in polar coordinates by $r(\theta) = \dfrac1{1 + e \cos\theta}$, where $e\ge0$. a) Show that the distance of each point on this curve to the line $x=\frac1e$ is a constant multiple of $r(\theta)$. b) When $e>1$, show that the curve approaches two asymptotes, find them and sketch the curve. Hint: If the critical angles are $\pm\theta_0$, compute the vertical distance of the point of the curve at angle $\theta = \theta_0+h$ to the line $\theta=\theta_0$, and take a limit using Taylor approximations. c) Observe that the curve is bounded if and only if $e < 1$. Show that the curve is an ellipse as follows: Let $a$ be the midpoint between the two points intersecting the $x$-axis. Show that $(1-e^2)(x-a)^2 + y^2$ is constant. d) What happens when $e=1$? I'm not really sure where to begin with this question.  I know that the curve is an ellipse if $e<1$, a parabola if $e=1$ and a hyperbola if $e>1$. I converted to parametric form, $x=\frac{\cos(\theta)}{1+e\cos(\theta)}$, $y=\frac{\sin(\theta)}{1+e\cos(\theta)}$ but I'm not sure this helps. I'm also not sure why this is on a differential equation assignment. Any input is greatly appreciated. EDIT: Okay, so I've got part a) down. Using the Cartesian coordinates, $\left|\frac{1}{e}-\frac{\cos\theta}{1+e\cos\theta}\right|=\left|1+e\cos\theta-e\cos\theta\right|=1$ which is constant.",,"['calculus', 'analysis', 'ordinary-differential-equations', 'conic-sections', 'polar-coordinates']"
20,Using Green's Function to Find Particular Solution,Using Green's Function to Find Particular Solution,,"We have the non-homogeneous differential equation $x^3y'''-3x^2y''+6xy'-6y=4x^2$ with conditions $y(1)=1, y'(1)=1, y''(1)=0$, and I have been tasked with finding its particular solution using Green's function. First, I found the homogeneous solution, $$y_h(x)=c_1x+c_2x^2+c_3x^3$$ Then, $y_1=x, y_2=x^2, y_3=x^3$. Then, after calculations, I obtain that $G(x,\xi)=\frac{x(\xi-x)^2}{2\xi}$. I know that $y_p(x)=\int_{x_0}^{x}G(x,\xi)f(\xi)d\xi$, and plugging in $1$ as the $x_0$ value, I obtain $y_p(x)=\frac13(x-1)^3x$. Finding the general solution after this is straight-forward; my question concerns the $x_0$ parameter. Is it supposed to always be $x_0=0$, or does it depend on the initial values provided; I used $x_0=1$ because of this, but when I checked the general solution with WolframAlpha, it found $y_p(x)=\frac{x^4}{3}$. Is this because they do not take into account initial values, or set a default of $0$? My notes on this subject are a bit confusing; any help would be appreciated.","We have the non-homogeneous differential equation $x^3y'''-3x^2y''+6xy'-6y=4x^2$ with conditions $y(1)=1, y'(1)=1, y''(1)=0$, and I have been tasked with finding its particular solution using Green's function. First, I found the homogeneous solution, $$y_h(x)=c_1x+c_2x^2+c_3x^3$$ Then, $y_1=x, y_2=x^2, y_3=x^3$. Then, after calculations, I obtain that $G(x,\xi)=\frac{x(\xi-x)^2}{2\xi}$. I know that $y_p(x)=\int_{x_0}^{x}G(x,\xi)f(\xi)d\xi$, and plugging in $1$ as the $x_0$ value, I obtain $y_p(x)=\frac13(x-1)^3x$. Finding the general solution after this is straight-forward; my question concerns the $x_0$ parameter. Is it supposed to always be $x_0=0$, or does it depend on the initial values provided; I used $x_0=1$ because of this, but when I checked the general solution with WolframAlpha, it found $y_p(x)=\frac{x^4}{3}$. Is this because they do not take into account initial values, or set a default of $0$? My notes on this subject are a bit confusing; any help would be appreciated.",,['calculus']
21,Analytic solution of: ${u}''+\frac{1}{x}{u}'=-\delta e^{u}$,Analytic solution of:,{u}''+\frac{1}{x}{u}'=-\delta e^{u},I am trying to find the analytic solution of  $${u}''+\frac{1}{x}{u}'=-\delta e^{u}$$ given the homogeneous mixed boundary conditions  $${u'(0)}=0$$ $$u(1)=0$$ How would one attack such a problem? I have been given that the analytic solution is  $$u=ln\left ( \frac{8a/\delta }{(ax^2+1)^2} \right )$$  where $a$ solves $8a=\delta (a+1)^2$. My approach (EDITED): I was given a hint: use the relation $x=e^{-y}$. I proceeded with that and used the chain-rule as follows $$(u\circ y)'(x)=\frac{d}{dx}u(y(x))=u'(y(x))y'(x)=u'x=-u'e^{y}$$ $$(u\circ y)''(x)=\frac{d}{dx}[u'(y(x))y'(x)]=u''(y(x))[y'(x)]^2+u'(y(x))y''(x)=u''e^{2y}+u'e^{2y}$$ substituting these relations into the ODE yields $$e^{2y}u''=-\delta e^{u}$$ or in an alternative form $$u''=-\delta e^{-2y} e^{u}$$ which is separable ( thanks for the help ).,I am trying to find the analytic solution of  $${u}''+\frac{1}{x}{u}'=-\delta e^{u}$$ given the homogeneous mixed boundary conditions  $${u'(0)}=0$$ $$u(1)=0$$ How would one attack such a problem? I have been given that the analytic solution is  $$u=ln\left ( \frac{8a/\delta }{(ax^2+1)^2} \right )$$  where $a$ solves $8a=\delta (a+1)^2$. My approach (EDITED): I was given a hint: use the relation $x=e^{-y}$. I proceeded with that and used the chain-rule as follows $$(u\circ y)'(x)=\frac{d}{dx}u(y(x))=u'(y(x))y'(x)=u'x=-u'e^{y}$$ $$(u\circ y)''(x)=\frac{d}{dx}[u'(y(x))y'(x)]=u''(y(x))[y'(x)]^2+u'(y(x))y''(x)=u''e^{2y}+u'e^{2y}$$ substituting these relations into the ODE yields $$e^{2y}u''=-\delta e^{u}$$ or in an alternative form $$u''=-\delta e^{-2y} e^{u}$$ which is separable ( thanks for the help ).,,"['ordinary-differential-equations', 'solution-verification']"
22,To show a given function is not the viscosity solution.,To show a given function is not the viscosity solution.,,"For the equation $ F(x,u,u',u'') = -au''-1 =0$ for $ x\in (0,2)$ with $ u(0) = 0 = u(2) $  and $a(x)$ is $1$ for $x\in (0,1)$ and $2$ for $x\in [1,2)$. Need to show that the function $$ u(x) = \begin{cases} -x^2/2 + 5x/6 &\mbox{if } x \in (0,1] \\ -x^2/4 + 5x/12+1/6 & \mbox{if } x\in(1,2) \end{cases} $$ is not a viscosity solution. Here we observe $ u(1) = 1/3 $ from both sides, thus I need to show there exists $ \phi \in C^2(0,2)$ such that $ \phi(x) < u(x) $ for $ x\neq 1$ and $\phi(1) = 1/3 = u(1)$ with $ F(1,\phi(1),\phi'(1))<0$. I can not seem to be able to find such a $ \phi$. Any hints or solutions would be extremely helpful. Thank you.","For the equation $ F(x,u,u',u'') = -au''-1 =0$ for $ x\in (0,2)$ with $ u(0) = 0 = u(2) $  and $a(x)$ is $1$ for $x\in (0,1)$ and $2$ for $x\in [1,2)$. Need to show that the function $$ u(x) = \begin{cases} -x^2/2 + 5x/6 &\mbox{if } x \in (0,1] \\ -x^2/4 + 5x/12+1/6 & \mbox{if } x\in(1,2) \end{cases} $$ is not a viscosity solution. Here we observe $ u(1) = 1/3 $ from both sides, thus I need to show there exists $ \phi \in C^2(0,2)$ such that $ \phi(x) < u(x) $ for $ x\neq 1$ and $\phi(1) = 1/3 = u(1)$ with $ F(1,\phi(1),\phi'(1))<0$. I can not seem to be able to find such a $ \phi$. Any hints or solutions would be extremely helpful. Thank you.",,"['analysis', 'ordinary-differential-equations', 'partial-differential-equations', 'viscosity-solutions']"
23,Solve $x^2f''(x)+f(x)=0$ check my answer,Solve  check my answer,x^2f''(x)+f(x)=0,"I'd just like someone else to review my answer, I'm  preparing for an exam and I saw this question but a solution was not included with it, and the result is...somewhat unpleasant, It's not feasible to derive it twice and check that it is indeed a solution. Ok, so the question is to solve the ODE $x^2f''(x)+f(x)=0$, it's not with constant coefficients so there is no set method, but we do have a hint: define $t=\ln{x}$ Ok, so $e^{2t}f''(e^t)+f(e^t)=0$ is what we want to solve. Let's look at the function $g(t)=f(e^t)$: $g'(t)=e^tf'(e^t)$, and $g''(t)=e^tf'(e^t)+e^{2t}f''(e^t)$ We can see that $g''(t)-g'(t)+g(t)=e^{2t}f''(t)+f(e^t)$. So solving $g''(t)-g'(t)+g(t)=0$ is equivalent to solving our original question. Using the method of finding the roots of the characteristic polynomial, the solutions are $g_1(t)=e^{\frac{1}{2}t}\cos(\frac{\sqrt{3}}{2}t)$ and $g_2(t)=e^{\frac{1}{2}t}\sin({\frac{\sqrt{3}}{2}t})$ And the most general solution is $g(t)=f(e^t)=c_1e^{\frac{1}{2}t}\cos(\frac{\sqrt{3}}{2}t)+c_2e^{\frac{1}{2}t}\sin({\frac{\sqrt{3}}{2}t}) $ But since our original question was in the sense of $x$, not $t$, our final answer should be $f(x)=c_1e^{\frac{1}{2}\ln x}\cos(\frac{\sqrt{3}}{2}\ln x)+c_2e^{\frac{1}{2}\ln x}\sin({\frac{\sqrt{3}}{2}\ln x})$ Is this result correct?","I'd just like someone else to review my answer, I'm  preparing for an exam and I saw this question but a solution was not included with it, and the result is...somewhat unpleasant, It's not feasible to derive it twice and check that it is indeed a solution. Ok, so the question is to solve the ODE $x^2f''(x)+f(x)=0$, it's not with constant coefficients so there is no set method, but we do have a hint: define $t=\ln{x}$ Ok, so $e^{2t}f''(e^t)+f(e^t)=0$ is what we want to solve. Let's look at the function $g(t)=f(e^t)$: $g'(t)=e^tf'(e^t)$, and $g''(t)=e^tf'(e^t)+e^{2t}f''(e^t)$ We can see that $g''(t)-g'(t)+g(t)=e^{2t}f''(t)+f(e^t)$. So solving $g''(t)-g'(t)+g(t)=0$ is equivalent to solving our original question. Using the method of finding the roots of the characteristic polynomial, the solutions are $g_1(t)=e^{\frac{1}{2}t}\cos(\frac{\sqrt{3}}{2}t)$ and $g_2(t)=e^{\frac{1}{2}t}\sin({\frac{\sqrt{3}}{2}t})$ And the most general solution is $g(t)=f(e^t)=c_1e^{\frac{1}{2}t}\cos(\frac{\sqrt{3}}{2}t)+c_2e^{\frac{1}{2}t}\sin({\frac{\sqrt{3}}{2}t}) $ But since our original question was in the sense of $x$, not $t$, our final answer should be $f(x)=c_1e^{\frac{1}{2}\ln x}\cos(\frac{\sqrt{3}}{2}\ln x)+c_2e^{\frac{1}{2}\ln x}\sin({\frac{\sqrt{3}}{2}\ln x})$ Is this result correct?",,"['calculus', 'ordinary-differential-equations', 'derivatives', 'proof-verification']"
24,Green's function in a moving frame for a constant heat source,Green's function in a moving frame for a constant heat source,,"I am looking for the Green's function of the problem in two dimensions $r =(x,z)$, \begin{equation}  \nabla^2g + \frac{v}{D}\frac{\partial g}{\partial z} = -\delta (r-r_0) \end{equation} Which corresponds to finding temperature distribution in a moving frame (moving at constant velocity $v$ in $z$ direction and diffusion coefficient of the medium is $D$) because of the constant source and $|r-r_0| = \sqrt{(x-x_0)^2+(z-z_0)^2}$ research paper gives solution as, \begin{eqnarray} g(r,r_0) = \frac{1}{2\pi}\exp\left(-\frac{z-z_1}{l} \right)K_0\left(\frac{|r-r_0|}{l} \right) \end{eqnarray} Where, $K_0$ is a modified Bessel function of second kind and $l = \frac{2D}{v}$. How can one arrive at this solution? I tried to varify the solution but I could not go anywhere close. \begin{eqnarray} \frac{\partial g(r,r_0)}{\partial z} &=& \exp\left(-\frac{z-z_1}{l} \right)\left[- \frac{1}{l}K_0 + \frac{\partial K_0}{\partial z} \right]\\ \frac{\partial^2 g(r,r_0)}{\partial z^2} &=& \exp\left(-\frac{z-z_1}{l} \right)\left[ \frac{1}{l^2}K_0 -\frac{2}{l} \frac{\partial K_0}{\partial z} + \frac{\partial^2 K_0}{\partial z^2}\right] \\ \frac{\partial^2 g(r,r_0)}{\partial x^2} &=& \exp\left(-\frac{z-z_1}{l} \right)\left[  \frac{\partial^2 K_0}{\partial x^2}\right] \end{eqnarray} then putting the terms together, \begin{eqnarray} \nabla^2g + \frac{2}{l}\frac{\partial g}{\partial z} &=& \exp\left(-\frac{z-z_1}{l} \right)\left[\frac{\partial^2 K_0}{\partial x^2}+ \frac{\partial^2 K_0}{\partial z^2} -\frac{1}{l^2}K_0 \right] \end{eqnarray} rhs of the equation should be zero unless $r=r_0$. possible definition of $K_0$ is \begin{eqnarray} K_0([r-r_0]/l) &=& \int^{\infty}_{0}\frac{\cos([r-r_0] \frac{u}{l})}{\sqrt{1+u^2}} du \\ &=& \int^{\infty}_{0}\frac{\cos(\sqrt{(x-x_0)^2+(z-z_0)^2} \frac{u}{l})}{\sqrt{1+u^2}} du \end{eqnarray} rhs is then, \begin{eqnarray} rhs &=& -\exp\left(-\frac{z-z_1}{l} \right)\left[ \int^{\infty}_{0}\frac{\frac{1+u^2}{l^2}\cos([r-r_0] \frac{u}{l}) + \frac{u}{l(r-r_0)}\sin([r-r_0] \frac{u}{l})}{\sqrt{1+u^2}} du \right] \end{eqnarray} Which I do not see how is equal to $\delta(r-r_0)$. What am I missing here?","I am looking for the Green's function of the problem in two dimensions $r =(x,z)$, \begin{equation}  \nabla^2g + \frac{v}{D}\frac{\partial g}{\partial z} = -\delta (r-r_0) \end{equation} Which corresponds to finding temperature distribution in a moving frame (moving at constant velocity $v$ in $z$ direction and diffusion coefficient of the medium is $D$) because of the constant source and $|r-r_0| = \sqrt{(x-x_0)^2+(z-z_0)^2}$ research paper gives solution as, \begin{eqnarray} g(r,r_0) = \frac{1}{2\pi}\exp\left(-\frac{z-z_1}{l} \right)K_0\left(\frac{|r-r_0|}{l} \right) \end{eqnarray} Where, $K_0$ is a modified Bessel function of second kind and $l = \frac{2D}{v}$. How can one arrive at this solution? I tried to varify the solution but I could not go anywhere close. \begin{eqnarray} \frac{\partial g(r,r_0)}{\partial z} &=& \exp\left(-\frac{z-z_1}{l} \right)\left[- \frac{1}{l}K_0 + \frac{\partial K_0}{\partial z} \right]\\ \frac{\partial^2 g(r,r_0)}{\partial z^2} &=& \exp\left(-\frac{z-z_1}{l} \right)\left[ \frac{1}{l^2}K_0 -\frac{2}{l} \frac{\partial K_0}{\partial z} + \frac{\partial^2 K_0}{\partial z^2}\right] \\ \frac{\partial^2 g(r,r_0)}{\partial x^2} &=& \exp\left(-\frac{z-z_1}{l} \right)\left[  \frac{\partial^2 K_0}{\partial x^2}\right] \end{eqnarray} then putting the terms together, \begin{eqnarray} \nabla^2g + \frac{2}{l}\frac{\partial g}{\partial z} &=& \exp\left(-\frac{z-z_1}{l} \right)\left[\frac{\partial^2 K_0}{\partial x^2}+ \frac{\partial^2 K_0}{\partial z^2} -\frac{1}{l^2}K_0 \right] \end{eqnarray} rhs of the equation should be zero unless $r=r_0$. possible definition of $K_0$ is \begin{eqnarray} K_0([r-r_0]/l) &=& \int^{\infty}_{0}\frac{\cos([r-r_0] \frac{u}{l})}{\sqrt{1+u^2}} du \\ &=& \int^{\infty}_{0}\frac{\cos(\sqrt{(x-x_0)^2+(z-z_0)^2} \frac{u}{l})}{\sqrt{1+u^2}} du \end{eqnarray} rhs is then, \begin{eqnarray} rhs &=& -\exp\left(-\frac{z-z_1}{l} \right)\left[ \int^{\infty}_{0}\frac{\frac{1+u^2}{l^2}\cos([r-r_0] \frac{u}{l}) + \frac{u}{l(r-r_0)}\sin([r-r_0] \frac{u}{l})}{\sqrt{1+u^2}} du \right] \end{eqnarray} Which I do not see how is equal to $\delta(r-r_0)$. What am I missing here?",,"['ordinary-differential-equations', 'mathematical-physics', 'bessel-functions']"
25,Solution techniques for f'(x)=f(g(x)),Solution techniques for f'(x)=f(g(x)),,"I stumbled over this seemingly natural question and was surprised, that I couldn't find a satisfying answer.  Differential equations of the type $f'(x)=g(f(x))$ are studied for all kind of classes of g, but what is there to say about solutions to the opposite case where $f'(x)=f(g(x))$? I even failed to find solutions for the very simple case $f'(x)=f(1-x)$, say on domain $[0,1]$.  Ultimately I am looking for solutions to  $ f'(x)=\frac{1-f(x)}{f(1-x)-1+x} $. Thank you for any helpful comments.","I stumbled over this seemingly natural question and was surprised, that I couldn't find a satisfying answer.  Differential equations of the type $f'(x)=g(f(x))$ are studied for all kind of classes of g, but what is there to say about solutions to the opposite case where $f'(x)=f(g(x))$? I even failed to find solutions for the very simple case $f'(x)=f(1-x)$, say on domain $[0,1]$.  Ultimately I am looking for solutions to  $ f'(x)=\frac{1-f(x)}{f(1-x)-1+x} $. Thank you for any helpful comments.",,"['ordinary-differential-equations', 'functional-equations']"
26,How to show that a leaf is topologically a cone.,How to show that a leaf is topologically a cone.,,"I am trying to understand the topological behaviour of foliations around irreducible singularities, specially in the case of singularities in the Poincaré domain. I am using the  third chapter of this text: Singular Points of Analytic Differential Equations . My question is about a consequence of Proposition (1.1) oh Chapter III, which says that, given $\omega_0=\lambda xdy-\mu ydx$ with $\lambda \mu \ne 0$, $\lambda / \mu \notin \mathbb{R}^-$, then 1) The leaves of $\omega_0=0$ are transverse to the centered spheres. 2) There exists a real linear flow whose trajectories are contained in the leaves of $\omega_0=0$ and for which $(0,0)$ is an attractor. A consequence of this result is the following: (*) Every leaf is topologically a cone with vertex $(0,0)$ and whose directrix is its intersection with the unit sphere. This fact is not difficult to visualize thinking the problem in 3 dimensions, but I am not sure of the correct way to show it, so my question is how to write the proof of (*) formally.","I am trying to understand the topological behaviour of foliations around irreducible singularities, specially in the case of singularities in the Poincaré domain. I am using the  third chapter of this text: Singular Points of Analytic Differential Equations . My question is about a consequence of Proposition (1.1) oh Chapter III, which says that, given $\omega_0=\lambda xdy-\mu ydx$ with $\lambda \mu \ne 0$, $\lambda / \mu \notin \mathbb{R}^-$, then 1) The leaves of $\omega_0=0$ are transverse to the centered spheres. 2) There exists a real linear flow whose trajectories are contained in the leaves of $\omega_0=0$ and for which $(0,0)$ is an attractor. A consequence of this result is the following: (*) Every leaf is topologically a cone with vertex $(0,0)$ and whose directrix is its intersection with the unit sphere. This fact is not difficult to visualize thinking the problem in 3 dimensions, but I am not sure of the correct way to show it, so my question is how to write the proof of (*) formally.",,"['ordinary-differential-equations', 'algebraic-geometry', 'algebraic-topology']"
27,"A line integral equation popped up when trying to derive Exact ODE integrating factor, can it be solved analytically?","A line integral equation popped up when trying to derive Exact ODE integrating factor, can it be solved analytically?",,"(For convenience, for any functions, only its first instance the x,y dependence will be written out, all subsequent instance the x,y will be suppressed) I have an ODE $$M(x,y)+N(x,y)\frac{dy}{dx}=0$$ I understood the ODE is inexact when $$\frac{\partial M}{\partial y} \neq \frac{\partial N}{\partial x}$$ I also learnt that for some inexact ODE, it can be made exact if the integrating factor $\mu$ is a function of x or y only, which is determined if the first (resp second) of these is satisfied $$\frac{\frac{\partial M}{\partial y}-\frac{\partial N}{\partial x}}{M},\frac{\frac{\partial M}{\partial y}-\frac{\partial N}{\partial x}}{N}$$ So that a function $$H(x,y)=C$$ can be found as the solution to the ODE However when I tried to do it as a general $\mu (x,y)$ I got something interesting Start with $$M(x,y)+N(x,y)\frac{dy}{dx}=0$$ ODE is exact with integrating factor $\mu$ iff $$\frac{\partial (\mu M)}{\partial y} = \frac{\partial (\mu N)}{\partial x}$$ $$\mu\frac{\partial M}{\partial y}+\frac{\partial \mu}{\partial y}M = \mu\frac{\partial N}{\partial x}+\frac{\partial \mu}{\partial x}N$$ Rearrange $$\mu\left(\frac{\partial M}{\partial y}-\frac{\partial N}{\partial x}\right) =\frac{\partial \mu}{\partial x}N-\frac{\partial \mu}{\partial y}M$$ Rewrite with del operators $$\mu \nabla \cdot \begin{pmatrix}  M \\ -N \end{pmatrix} =\nabla\mu \cdot \begin{pmatrix}  N  \\ -M \end{pmatrix}$$ Move everything to the LHS and move the - sign into the vectors $$\mu \nabla \cdot \begin{pmatrix}  M  \\ -N \end{pmatrix} + \nabla\mu \cdot \begin{pmatrix}  -N  \\ M \end{pmatrix}= 0$$ This thing $$\mu \nabla \cdot \begin{pmatrix}  M  \\ -N \end{pmatrix} + \nabla\mu \cdot \begin{pmatrix}  -N  \\ M \end{pmatrix}= 0$$ looks deceptively similar to the divergence product rule, is it actually possible to do something on it and solve it analytically for $\mu$? ================================================================== UPDATE KittyL have pointed out a mistake I made in the 1st version. It is now corrected as follows $$\mu \nabla \cdot \begin{pmatrix}  \color{red}{-N} \\ \color{red}{M} \end{pmatrix} =\nabla\mu \cdot \begin{pmatrix}  N  \\ -M \end{pmatrix}$$ which rearranges to $$\mu \nabla \cdot \begin{pmatrix}  N \\ -M \end{pmatrix} +\nabla\mu \cdot \begin{pmatrix}  N  \\ -M \end{pmatrix}=0$$ Using the divergence product rule backwards $$ \nabla \cdot \left(\mu\begin{pmatrix}  N \\ -M \end{pmatrix}\right)=0$$ Since the starting functions M,N,$\mu$ are all functions of two variables (e.g. x,y) we can say this problem lives in $\mathbb{R}^2$ Applying Divergence theorem in $\mathbb{R}^2$ $$ \iint_A\nabla \cdot \left(\mu\begin{pmatrix}  N \\ -M \end{pmatrix}\right)dA=\iint_A 0 dA$$ $$ \oint \mu\begin{pmatrix}  N \\ -M \end{pmatrix}\cdot d\vec{l}=0$$ Because divergence theorem holds for all closed surfaces, let's choose our surface to be a circle $x^2+y^2=1$ Parametrising in terms of $\theta$, we obtained $$ \int_0^{2\pi} \mu(r,\theta)\begin{pmatrix}  N(r,\theta) \\ -M(r,\theta) \end{pmatrix}\cdot \frac{\nabla{r^2}}{||\nabla{r^2}||}d\theta=0$$ $$ \int_0^{2\pi} \mu(r,\theta)\begin{pmatrix}  N(r,\theta) \\ -M(r,\theta) \end{pmatrix}\cdot \frac{1}{\sqrt{2r}}\begin{pmatrix}  \cos\theta \\ \sin\theta \end{pmatrix}d\theta=0$$ $$ \int_0^{2\pi} \frac{\mu(r,\theta)}{\sqrt{2r}}\left( N(r,\theta)\cos\theta-M(r,\theta)\sin\theta\right)d\theta=0$$ Is there a non trivial solution to $\mu$ for this integral? If so, can $\mu$ be solved analytically/in a closed form? If no analytic solutions exists in general, what is the most numerically stable and efficient way to approximate $\mu$? $$ \int_0^{2\pi} \frac{\mu(r,\theta)}{\sqrt{2r}}\left( N(r,\theta)\cos\theta-M(r,\theta)\sin\theta\right)d\theta=0$$","(For convenience, for any functions, only its first instance the x,y dependence will be written out, all subsequent instance the x,y will be suppressed) I have an ODE $$M(x,y)+N(x,y)\frac{dy}{dx}=0$$ I understood the ODE is inexact when $$\frac{\partial M}{\partial y} \neq \frac{\partial N}{\partial x}$$ I also learnt that for some inexact ODE, it can be made exact if the integrating factor $\mu$ is a function of x or y only, which is determined if the first (resp second) of these is satisfied $$\frac{\frac{\partial M}{\partial y}-\frac{\partial N}{\partial x}}{M},\frac{\frac{\partial M}{\partial y}-\frac{\partial N}{\partial x}}{N}$$ So that a function $$H(x,y)=C$$ can be found as the solution to the ODE However when I tried to do it as a general $\mu (x,y)$ I got something interesting Start with $$M(x,y)+N(x,y)\frac{dy}{dx}=0$$ ODE is exact with integrating factor $\mu$ iff $$\frac{\partial (\mu M)}{\partial y} = \frac{\partial (\mu N)}{\partial x}$$ $$\mu\frac{\partial M}{\partial y}+\frac{\partial \mu}{\partial y}M = \mu\frac{\partial N}{\partial x}+\frac{\partial \mu}{\partial x}N$$ Rearrange $$\mu\left(\frac{\partial M}{\partial y}-\frac{\partial N}{\partial x}\right) =\frac{\partial \mu}{\partial x}N-\frac{\partial \mu}{\partial y}M$$ Rewrite with del operators $$\mu \nabla \cdot \begin{pmatrix}  M \\ -N \end{pmatrix} =\nabla\mu \cdot \begin{pmatrix}  N  \\ -M \end{pmatrix}$$ Move everything to the LHS and move the - sign into the vectors $$\mu \nabla \cdot \begin{pmatrix}  M  \\ -N \end{pmatrix} + \nabla\mu \cdot \begin{pmatrix}  -N  \\ M \end{pmatrix}= 0$$ This thing $$\mu \nabla \cdot \begin{pmatrix}  M  \\ -N \end{pmatrix} + \nabla\mu \cdot \begin{pmatrix}  -N  \\ M \end{pmatrix}= 0$$ looks deceptively similar to the divergence product rule, is it actually possible to do something on it and solve it analytically for $\mu$? ================================================================== UPDATE KittyL have pointed out a mistake I made in the 1st version. It is now corrected as follows $$\mu \nabla \cdot \begin{pmatrix}  \color{red}{-N} \\ \color{red}{M} \end{pmatrix} =\nabla\mu \cdot \begin{pmatrix}  N  \\ -M \end{pmatrix}$$ which rearranges to $$\mu \nabla \cdot \begin{pmatrix}  N \\ -M \end{pmatrix} +\nabla\mu \cdot \begin{pmatrix}  N  \\ -M \end{pmatrix}=0$$ Using the divergence product rule backwards $$ \nabla \cdot \left(\mu\begin{pmatrix}  N \\ -M \end{pmatrix}\right)=0$$ Since the starting functions M,N,$\mu$ are all functions of two variables (e.g. x,y) we can say this problem lives in $\mathbb{R}^2$ Applying Divergence theorem in $\mathbb{R}^2$ $$ \iint_A\nabla \cdot \left(\mu\begin{pmatrix}  N \\ -M \end{pmatrix}\right)dA=\iint_A 0 dA$$ $$ \oint \mu\begin{pmatrix}  N \\ -M \end{pmatrix}\cdot d\vec{l}=0$$ Because divergence theorem holds for all closed surfaces, let's choose our surface to be a circle $x^2+y^2=1$ Parametrising in terms of $\theta$, we obtained $$ \int_0^{2\pi} \mu(r,\theta)\begin{pmatrix}  N(r,\theta) \\ -M(r,\theta) \end{pmatrix}\cdot \frac{\nabla{r^2}}{||\nabla{r^2}||}d\theta=0$$ $$ \int_0^{2\pi} \mu(r,\theta)\begin{pmatrix}  N(r,\theta) \\ -M(r,\theta) \end{pmatrix}\cdot \frac{1}{\sqrt{2r}}\begin{pmatrix}  \cos\theta \\ \sin\theta \end{pmatrix}d\theta=0$$ $$ \int_0^{2\pi} \frac{\mu(r,\theta)}{\sqrt{2r}}\left( N(r,\theta)\cos\theta-M(r,\theta)\sin\theta\right)d\theta=0$$ Is there a non trivial solution to $\mu$ for this integral? If so, can $\mu$ be solved analytically/in a closed form? If no analytic solutions exists in general, what is the most numerically stable and efficient way to approximate $\mu$? $$ \int_0^{2\pi} \frac{\mu(r,\theta)}{\sqrt{2r}}\left( N(r,\theta)\cos\theta-M(r,\theta)\sin\theta\right)d\theta=0$$",,"['ordinary-differential-equations', 'vector-analysis']"
28,Coefficients of spherical solution to Laplace's equation with difficult Robin boundary conditions,Coefficients of spherical solution to Laplace's equation with difficult Robin boundary conditions,,"I'm trying to solve Laplace's equation in an (axisymmetric) external spherical domain.  The controlling equation is: $$\nabla^2 f = 0$$ $f$ must dissappear at infinity, and at the surface of the sphere ($r=1$) we have: $$\frac{\partial f}{\partial r} = \sigma h(f+1)$$ Here $\sigma$ is a positive parameter, and $h$ is an angular ""step"" function: $$h=\begin{cases} 1 & \mu \leq \tau \\ 0 & \mu \gt \tau  \end{cases}$$ Here $\mu$ is simply a transformed angular coordinate ($\mu = \cos{\theta}$) and $\tau$ is a parameter.  The most important case is $\tau = 0$, but it can take any value from $-1$ to $1$. The solution to Laplace's equation in spherical coordiantes for this situation is: $$ f(r,\mu) = \sum_0^\infty A_nr^{-(n+1)}P_n({\mu})$$ Here $P_n$ are the Legendre Polynomials and  the $A_n$ are the coefficients that define the solution. Inserting this into the boundary condition at the surface ($r=1$) we get: $$ \sum_0^\infty -(n+1)A_nP_n(\mu)= \sigma h\left(1 + \sum_0^\infty A_nP_n(\mu)\right) $$ The next step here is to multiply by a generic Legendre Polynomial (here $P_m$), and to integrate from $-1$ to $1$, to take advantage of orthogonality. Unfortunately, the factor $h$ ruins orthogonality on the RHS, so we end up with: $$ \frac{-2(m+1)}{2m+1}A_m = \sigma \left[ \int_{-1}^\tau P_m \, d\mu +\sum_{n=0}^\infty A_n\left( \int_{-1}^\tau P_m(\mu) P_n(\mu) \, d\mu \right)\right] $$ So we end up with an infinite linear system of equations for the $A_n$ (note than in the above the $A_n$ are the same as the $A_m$, just different counters). For now what I've been doing is truncating the series (which basically forces all $A_n$ above a certain $n$ to be $0$) and solving the now finite system computationally.  However, choosing a maximum $n$ has been somewhat arbitrary, and for non-small values of $\sigma$ this procedure seems to converge incorrectly. Is there a smart to way to solve this system?  It's be best to have procedures that work for all $\sigma$ and $\tau$, but I'd be satisfied with a procedure that handles $\tau = 0$ and\or limiting cases of $\sigma$ ($\sigma \to \infty$ or $\sigma \to 0$).","I'm trying to solve Laplace's equation in an (axisymmetric) external spherical domain.  The controlling equation is: $$\nabla^2 f = 0$$ $f$ must dissappear at infinity, and at the surface of the sphere ($r=1$) we have: $$\frac{\partial f}{\partial r} = \sigma h(f+1)$$ Here $\sigma$ is a positive parameter, and $h$ is an angular ""step"" function: $$h=\begin{cases} 1 & \mu \leq \tau \\ 0 & \mu \gt \tau  \end{cases}$$ Here $\mu$ is simply a transformed angular coordinate ($\mu = \cos{\theta}$) and $\tau$ is a parameter.  The most important case is $\tau = 0$, but it can take any value from $-1$ to $1$. The solution to Laplace's equation in spherical coordiantes for this situation is: $$ f(r,\mu) = \sum_0^\infty A_nr^{-(n+1)}P_n({\mu})$$ Here $P_n$ are the Legendre Polynomials and  the $A_n$ are the coefficients that define the solution. Inserting this into the boundary condition at the surface ($r=1$) we get: $$ \sum_0^\infty -(n+1)A_nP_n(\mu)= \sigma h\left(1 + \sum_0^\infty A_nP_n(\mu)\right) $$ The next step here is to multiply by a generic Legendre Polynomial (here $P_m$), and to integrate from $-1$ to $1$, to take advantage of orthogonality. Unfortunately, the factor $h$ ruins orthogonality on the RHS, so we end up with: $$ \frac{-2(m+1)}{2m+1}A_m = \sigma \left[ \int_{-1}^\tau P_m \, d\mu +\sum_{n=0}^\infty A_n\left( \int_{-1}^\tau P_m(\mu) P_n(\mu) \, d\mu \right)\right] $$ So we end up with an infinite linear system of equations for the $A_n$ (note than in the above the $A_n$ are the same as the $A_m$, just different counters). For now what I've been doing is truncating the series (which basically forces all $A_n$ above a certain $n$ to be $0$) and solving the now finite system computationally.  However, choosing a maximum $n$ has been somewhat arbitrary, and for non-small values of $\sigma$ this procedure seems to converge incorrectly. Is there a smart to way to solve this system?  It's be best to have procedures that work for all $\sigma$ and $\tau$, but I'd be satisfied with a procedure that handles $\tau = 0$ and\or limiting cases of $\sigma$ ($\sigma \to \infty$ or $\sigma \to 0$).",,"['linear-algebra', 'ordinary-differential-equations', 'partial-differential-equations']"
29,Solution to Laguerre differential equation using generating function,Solution to Laguerre differential equation using generating function,,"This is an exercise in Modern Quantum Mechanics by Sakurai and Napolitano. Follow these steps to show that solutions to Kummer's equation (7.46) can be written in terms of Laguerre polynomials $L_n(x)$, which are defined according to a generating function as   $$g(x, t) = \frac{e^{-xt/(1-t)}}{1-t} = \sum_{n=0}^\infty L_n(x) \frac{t^n}{n!}$$   where $0 < t < 1$. The discussion on generating functions for Hermite polynomials will be helpful. (a) Prove that $L_n(0) = n!$ and $L_0(x) = 1$. (b) Differentiate $g(x, t)$ with respect to $x$, show that   $$L'_n(x) - nL'_{n-1}(x) = -nL_{n-1}(x),$$   and find the first few Laguerre polynomials. (c) Differentiate $g(x, t)$ with respect to $t$ and show that   $$L_{n+1}(x) - (2n+1-x)L_n(x) + n^2L_{n-1}(x) = 0.$$ (d) Now show that Kummer's Equation is solved by deriving   $$xL''_n(x) + (1-x)L'_n(x) + nL_n(x) = 0,$$   and associate $n$ with the principal quantum number for the hydrogen atom. I've done (a), (b), and (c), but I'm having trouble with (d). The difficulty seems to be that using the result in (d) to shift $n$ invariably introduces two new $n$ values, so I can never get it to be of use in simplifying what I get from (b). No matter what I do, I can't get an equation that only contains one $n$ value. But obviously I just haven't tried the right thing. How can (d) be solved using (b) and (c)?","This is an exercise in Modern Quantum Mechanics by Sakurai and Napolitano. Follow these steps to show that solutions to Kummer's equation (7.46) can be written in terms of Laguerre polynomials $L_n(x)$, which are defined according to a generating function as   $$g(x, t) = \frac{e^{-xt/(1-t)}}{1-t} = \sum_{n=0}^\infty L_n(x) \frac{t^n}{n!}$$   where $0 < t < 1$. The discussion on generating functions for Hermite polynomials will be helpful. (a) Prove that $L_n(0) = n!$ and $L_0(x) = 1$. (b) Differentiate $g(x, t)$ with respect to $x$, show that   $$L'_n(x) - nL'_{n-1}(x) = -nL_{n-1}(x),$$   and find the first few Laguerre polynomials. (c) Differentiate $g(x, t)$ with respect to $t$ and show that   $$L_{n+1}(x) - (2n+1-x)L_n(x) + n^2L_{n-1}(x) = 0.$$ (d) Now show that Kummer's Equation is solved by deriving   $$xL''_n(x) + (1-x)L'_n(x) + nL_n(x) = 0,$$   and associate $n$ with the principal quantum number for the hydrogen atom. I've done (a), (b), and (c), but I'm having trouble with (d). The difficulty seems to be that using the result in (d) to shift $n$ invariably introduces two new $n$ values, so I can never get it to be of use in simplifying what I get from (b). No matter what I do, I can't get an equation that only contains one $n$ value. But obviously I just haven't tried the right thing. How can (d) be solved using (b) and (c)?",,"['ordinary-differential-equations', 'generating-functions']"
30,A special differential equation,A special differential equation,,"I want to know that how we can solve a differential equation of the general form $$y'(x)= y(u(x))+g(x). $$ For example $y'(x)= y(\sin(x))+ x,$ or $y'(x)= y(\sin(x)).$ How we prove  existence or uniqueness of the solution?","I want to know that how we can solve a differential equation of the general form $$y'(x)= y(u(x))+g(x). $$ For example $y'(x)= y(\sin(x))+ x,$ or $y'(x)= y(\sin(x)).$ How we prove  existence or uniqueness of the solution?",,['ordinary-differential-equations']
31,Fourier Transform of Partial Derivative w.r.t x of [ x*f(x) ],Fourier Transform of Partial Derivative w.r.t x of [ x*f(x) ],,"Can someone please help with the Fourier Transform of : Thank you in advance! ::Edit:: This is what I am trying to solve: $\frac{\partial(p(x, t))}{\partial t} = -A\frac{\partial(xp(x, t))}{\partial x}+\frac{B}{2}\frac{\partial^{2}(xp(x, t))}{\partial x^{2}}$ Where:[{A, B} = Constants] Define: $FT\{p(x, t)\}(\omega) = \int_{-\infty }^{\infty }p(x, t)e^{-2\pi ix\omega }\,dx$ and $FT^{-1}\{\bar{p}(\omega , t)\}(x) = \int_{-\infty }^{\infty }p(\omega , t)e^{2\pi ix\omega }\,d\omega$ The next step is to convert each term so I can reduce the order but I started reading about Fourier Transforms two days ago, so I do not know all the tricks. I did use the properties below to get rid of the derivative - but I have no idea how to convolve x with p(x,t) - since p(x,t) is unknown. p(x,t) is a density function - so it goes to zero in the infinities (if this is important) Thank you again!","Can someone please help with the Fourier Transform of : Thank you in advance! ::Edit:: This is what I am trying to solve: $\frac{\partial(p(x, t))}{\partial t} = -A\frac{\partial(xp(x, t))}{\partial x}+\frac{B}{2}\frac{\partial^{2}(xp(x, t))}{\partial x^{2}}$ Where:[{A, B} = Constants] Define: $FT\{p(x, t)\}(\omega) = \int_{-\infty }^{\infty }p(x, t)e^{-2\pi ix\omega }\,dx$ and $FT^{-1}\{\bar{p}(\omega , t)\}(x) = \int_{-\infty }^{\infty }p(\omega , t)e^{2\pi ix\omega }\,d\omega$ The next step is to convert each term so I can reduce the order but I started reading about Fourier Transforms two days ago, so I do not know all the tricks. I did use the properties below to get rid of the derivative - but I have no idea how to convolve x with p(x,t) - since p(x,t) is unknown. p(x,t) is a density function - so it goes to zero in the infinities (if this is important) Thank you again!",,"['calculus', 'ordinary-differential-equations', 'partial-differential-equations', 'fourier-analysis']"
32,Is this a correct application of Poincaré-Bendixson?,Is this a correct application of Poincaré-Bendixson?,,"Consider a non-vanishing $C^1$ vector field $f$ on a neighbourhood of the annulus with radii $1$ and $2$ in $\mathbb{R}^2$. The vector field is transversal to the boundary of the annulus and points inward. Prove that there is a closed orbit solving $\dot x = f(x)$. Prove also that if exactly $7$ closed orbits exist, then there exist associated orbits that spiral towards one of the closed orbits from both sides. Here is my work so far: Because $f$ is transversal and points inwards, the orbits inside the annulus do not leave the annulus (since if they did, the vector field would have to point outward from the annulus). The Poincaré-Bendixson theorem states that any compact $\omega$-limit set of an orbit must be a periodic orbit (since no fixed points exist), so for the orbits inside the annulus there should be two possibilities: either they are periodic orbits, or their limit sets are periodic orbits. So clearly at least one periodic orbit has to exist, and if any non-periodic orbits exist, they must spiral towards a periodic orbit. To prove that non-periodic orbits exist, use the fact that exactly $7$ periodic orbits exist. Each of these periodic orbits is a loop inside the annulus, but finitely many loops cannot cover the entire annulus. So there exist points that do not belong to any periodic orbit, and therefore have a non-periodic orbit. These arguments feel a bit shaky. Can anyone tell me what errors I've made?","Consider a non-vanishing $C^1$ vector field $f$ on a neighbourhood of the annulus with radii $1$ and $2$ in $\mathbb{R}^2$. The vector field is transversal to the boundary of the annulus and points inward. Prove that there is a closed orbit solving $\dot x = f(x)$. Prove also that if exactly $7$ closed orbits exist, then there exist associated orbits that spiral towards one of the closed orbits from both sides. Here is my work so far: Because $f$ is transversal and points inwards, the orbits inside the annulus do not leave the annulus (since if they did, the vector field would have to point outward from the annulus). The Poincaré-Bendixson theorem states that any compact $\omega$-limit set of an orbit must be a periodic orbit (since no fixed points exist), so for the orbits inside the annulus there should be two possibilities: either they are periodic orbits, or their limit sets are periodic orbits. So clearly at least one periodic orbit has to exist, and if any non-periodic orbits exist, they must spiral towards a periodic orbit. To prove that non-periodic orbits exist, use the fact that exactly $7$ periodic orbits exist. Each of these periodic orbits is a loop inside the annulus, but finitely many loops cannot cover the entire annulus. So there exist points that do not belong to any periodic orbit, and therefore have a non-periodic orbit. These arguments feel a bit shaky. Can anyone tell me what errors I've made?",,"['ordinary-differential-equations', 'dynamical-systems']"
33,Can the Heat Equation be Averaged Over a Region?,Can the Heat Equation be Averaged Over a Region?,,"I am doing a project for my partial differential equations class in which I am motivating the definition of a weak solution. To get started, I assumed that $T$ was a solution to $\nabla^2 T = \partial T/\partial t$, and then I considered the averaged version $$\hat T_D(x,t) = \frac{1}{m(D)}\int_{x+D}T(\xi,t)\,d\xi$$ where $D$ is a nice region in the space containing $x$, and $x+D$ is a translation. Question: Does $\hat T_D$ satisfy $\nabla^2\hat T_D = \partial \hat T_D/\partial T$ ? It seems physically reasonable to me that $\hat T_D$ should satisfy the same differential equation, and I know this is true in one dimension: \begin{align} \frac{\partial \hat T_D}{\partial t}  &= \frac{1}{m(D)}\int_{x-a}^{x+b} \frac{\partial T}{\partial t}\!(\xi,t)\,d\xi \\ &= \frac{1}{m(D)}\int_{x-a}^{x+b} \frac{\partial^2 T}{\partial \xi^2}\!(\xi,t)\,d\xi \\ &= \frac{1}{m(D)}\left(\frac{\partial T}{\partial x}(x+b,t) - \frac{\partial T}{\partial x}\!(x-a,t) \right) \\ &= \frac{1}{m(D)}\frac{\partial}{\partial x}\left(\frac{\partial}{\partial x}\left[\int_0^{x+b}T(\xi,t) - \int_0^{x-a}T(\xi,t)\right]\right) \\ &= \frac{\partial^2 \hat T_D}{\partial x^2}  \end{align} The beginning of this argument extends well into higher dimensions by replacing FToC by Stokes: $$m(D)\frac{\partial \hat T_D}{\partial t}  = \int_D \nabla^2T(\xi,t)\,d\xi = \int_{\partial D} \nabla T(\xi,t)\,d\xi $$ However, to do the next part I used the fact that derivatives are linear to interchange the derivative and the difference; it's not clear if this is still possible in higher dimensions. Certainly the naive method of just pulling the nabla out of the integral does not work for simple dimensions considerations, but I'm not aware of a more sophisticated technique either.","I am doing a project for my partial differential equations class in which I am motivating the definition of a weak solution. To get started, I assumed that $T$ was a solution to $\nabla^2 T = \partial T/\partial t$, and then I considered the averaged version $$\hat T_D(x,t) = \frac{1}{m(D)}\int_{x+D}T(\xi,t)\,d\xi$$ where $D$ is a nice region in the space containing $x$, and $x+D$ is a translation. Question: Does $\hat T_D$ satisfy $\nabla^2\hat T_D = \partial \hat T_D/\partial T$ ? It seems physically reasonable to me that $\hat T_D$ should satisfy the same differential equation, and I know this is true in one dimension: \begin{align} \frac{\partial \hat T_D}{\partial t}  &= \frac{1}{m(D)}\int_{x-a}^{x+b} \frac{\partial T}{\partial t}\!(\xi,t)\,d\xi \\ &= \frac{1}{m(D)}\int_{x-a}^{x+b} \frac{\partial^2 T}{\partial \xi^2}\!(\xi,t)\,d\xi \\ &= \frac{1}{m(D)}\left(\frac{\partial T}{\partial x}(x+b,t) - \frac{\partial T}{\partial x}\!(x-a,t) \right) \\ &= \frac{1}{m(D)}\frac{\partial}{\partial x}\left(\frac{\partial}{\partial x}\left[\int_0^{x+b}T(\xi,t) - \int_0^{x-a}T(\xi,t)\right]\right) \\ &= \frac{\partial^2 \hat T_D}{\partial x^2}  \end{align} The beginning of this argument extends well into higher dimensions by replacing FToC by Stokes: $$m(D)\frac{\partial \hat T_D}{\partial t}  = \int_D \nabla^2T(\xi,t)\,d\xi = \int_{\partial D} \nabla T(\xi,t)\,d\xi $$ However, to do the next part I used the fact that derivatives are linear to interchange the derivative and the difference; it's not clear if this is still possible in higher dimensions. Certainly the naive method of just pulling the nabla out of the integral does not work for simple dimensions considerations, but I'm not aware of a more sophisticated technique either.",,"['ordinary-differential-equations', 'multivariable-calculus']"
34,Solving $y^{(n)}(t)=f(t); t>0$ with initial conditions,Solving  with initial conditions,y^{(n)}(t)=f(t); t>0,"I will use the notation $\frac{d^n y}{dt^n} \equiv y^{(n)}$. How do I solve this ODE? $$y^{(n)}(t)=f(t); t>0;\\ y(0)=y_0, y'(0)=y_1, ..., y^{(n-1)}(0)=y_{n-1}$$ What I did: The ODE is in the form $F(t,y^{(n)})=0$ so I should look for parametric solution $\begin{cases} t=\phi(\tau)\\ y^{(n)}=\psi(\tau) \end{cases}$ so $y^{(n-1)}=\int\psi(\tau)\phi'(\tau)d\tau$ and the order is reduced by one. In my problem, let's choose $\phi(\tau)=\tau; \psi_n=f(\tau)$ and $y^{(k-1)}=\psi_{k-1}(\tau)=\int \psi_k(\tau)d\tau$ where $k=1,2,...,n$ How do I apply the initial conditions here? There is also an answer which reads $$y(t)=\frac{1}{(n-1)!}\int_0^t (t-\tau)^{n-1}f(\tau)d\tau + \sum_{i=0}^{n-1} \frac{y_i t^i}{i!}$$ How do I get this answer?","I will use the notation $\frac{d^n y}{dt^n} \equiv y^{(n)}$. How do I solve this ODE? $$y^{(n)}(t)=f(t); t>0;\\ y(0)=y_0, y'(0)=y_1, ..., y^{(n-1)}(0)=y_{n-1}$$ What I did: The ODE is in the form $F(t,y^{(n)})=0$ so I should look for parametric solution $\begin{cases} t=\phi(\tau)\\ y^{(n)}=\psi(\tau) \end{cases}$ so $y^{(n-1)}=\int\psi(\tau)\phi'(\tau)d\tau$ and the order is reduced by one. In my problem, let's choose $\phi(\tau)=\tau; \psi_n=f(\tau)$ and $y^{(k-1)}=\psi_{k-1}(\tau)=\int \psi_k(\tau)d\tau$ where $k=1,2,...,n$ How do I apply the initial conditions here? There is also an answer which reads $$y(t)=\frac{1}{(n-1)!}\int_0^t (t-\tau)^{n-1}f(\tau)d\tau + \sum_{i=0}^{n-1} \frac{y_i t^i}{i!}$$ How do I get this answer?",,['ordinary-differential-equations']
35,Can you recommend some books on elliptic function?,Can you recommend some books on elliptic function?,,I plan to study elliptic function.  Can you recommend  some  books? What is the relationship between elliptic function and elliptic curve？Many thanks in advance!,I plan to study elliptic function.  Can you recommend  some  books? What is the relationship between elliptic function and elliptic curve？Many thanks in advance!,,"['reference-request', 'special-functions', 'elliptic-curves', 'elliptic-functions']"
36,Question about solutions of $x''+(1+r(t))x=0$ when $\int_1^\infty |r(t)| dx <\infty$ .,Question about solutions of  when  .,x''+(1+r(t))x=0 \int_1^\infty |r(t)| dx <\infty,Let $x''+(1+r(t))x=0$ where $r(t)$ is continous and $\int_1^\infty |r(t)| dx <\infty$  show that the equation has solutions $\phi_1$ and $\phi_2$ such that $$\lim_{t\to\infty} [(\phi_1(t)-e^{it})=0$$ and $$\lim_{t\to\infty} [(\phi_1'(t)-ie^{it})=0$$ and $$\lim_{t\to\infty} [(\phi_2(t)-e^{-it})=0$$ and $$\lim_{t\to\infty} [(\phi_2'(t)+ie^{-it})=0$$ I don't know how to start to solve. I need some hint. thanks.,Let $x''+(1+r(t))x=0$ where $r(t)$ is continous and $\int_1^\infty |r(t)| dx <\infty$  show that the equation has solutions $\phi_1$ and $\phi_2$ such that $$\lim_{t\to\infty} [(\phi_1(t)-e^{it})=0$$ and $$\lim_{t\to\infty} [(\phi_1'(t)-ie^{it})=0$$ and $$\lim_{t\to\infty} [(\phi_2(t)-e^{-it})=0$$ and $$\lim_{t\to\infty} [(\phi_2'(t)+ie^{-it})=0$$ I don't know how to start to solve. I need some hint. thanks.,,"['analysis', 'ordinary-differential-equations', 'dynamical-systems']"
37,Solving $2x(t)(x'(t))^3-(x'(t))^2=C$,Solving,2x(t)(x'(t))^3-(x'(t))^2=C,"I have differential equation: $$2x(t)(x'(t))^3-(x'(t))^2=C$$ where $C$ is some constant and $x(0)=x(1)=0$. I've solved this equation for $C=0$, then $x(t)=0$, but what about other $C$?","I have differential equation: $$2x(t)(x'(t))^3-(x'(t))^2=C$$ where $C$ is some constant and $x(0)=x(1)=0$. I've solved this equation for $C=0$, then $x(t)=0$, but what about other $C$?",,['ordinary-differential-equations']
38,Applying the Fourier transform to solve an ODE.,Applying the Fourier transform to solve an ODE.,,"We are learning about fourier transfrms in class and I was wondering about solving the following ODE using this method. So, I want to solve the equation $u''(x)+u(x)=0$. Now, it is clear that a solution is of the form $u(x)=Acos(x)+Bsin(x)$ for all $x\in R$ where A and B are constants. So: $$u''(x)+u(x)=0$$ So applying fourier transform $$\mathscr F (u'')+\mathscr F(u)=0$$ $$(i \omega )^2\mathscr F (u)+\mathscr F(u)=0$$ $$-\omega^2\mathscr F (u)+\mathscr F(u)=0$$ $$(1-\omega^2)\mathscr F (u)=0$$ $$\Rightarrow \mathscr F (u)=0$$ $$\Rightarrow u=0$$ Now although this is a solution, I dont understand why this method unable to produce the more general solution: $u=Acos(x)+Bsin(x)$","We are learning about fourier transfrms in class and I was wondering about solving the following ODE using this method. So, I want to solve the equation $u''(x)+u(x)=0$. Now, it is clear that a solution is of the form $u(x)=Acos(x)+Bsin(x)$ for all $x\in R$ where A and B are constants. So: $$u''(x)+u(x)=0$$ So applying fourier transform $$\mathscr F (u'')+\mathscr F(u)=0$$ $$(i \omega )^2\mathscr F (u)+\mathscr F(u)=0$$ $$-\omega^2\mathscr F (u)+\mathscr F(u)=0$$ $$(1-\omega^2)\mathscr F (u)=0$$ $$\Rightarrow \mathscr F (u)=0$$ $$\Rightarrow u=0$$ Now although this is a solution, I dont understand why this method unable to produce the more general solution: $u=Acos(x)+Bsin(x)$",,"['ordinary-differential-equations', 'fourier-analysis']"
39,Find K values that make a differential equation solution stable,Find K values that make a differential equation solution stable,,"Given some differential equations, ie. ""a"", or ""b"": a. $$Y'''+Y''+2Y'+KY=0$$ b. $$Y'''+KY''+3KY'+2Y=0$$ How do I get the $K$ values that make the solution stable? I know that for ""a"", it should be $0 < K <2$ and that for ""b"" the solution is $K>(\frac2{3})^{1/2}$ but I didn't figure that out by myself.","Given some differential equations, ie. ""a"", or ""b"": a. $$Y'''+Y''+2Y'+KY=0$$ b. $$Y'''+KY''+3KY'+2Y=0$$ How do I get the $K$ values that make the solution stable? I know that for ""a"", it should be $0 < K <2$ and that for ""b"" the solution is $K>(\frac2{3})^{1/2}$ but I didn't figure that out by myself.",,"['ordinary-differential-equations', 'multivariable-calculus']"
40,"How to adapt the discrete-time to continuous, $(A) \Rightarrow (B)$?","How to adapt the discrete-time to continuous, ?",(A) \Rightarrow (B),"in class  was proved oseledets theorem for discrete time, following guidelines Ricardo Mañe book. Theorem discrete Oseledets (A) : Let $ M ^ n $ be a Riemannian manifold, $ f: M \rightarrow M $ be a diffeomorphism of class $ C^1$ and $\mu$ invariant probability measure for $f$. For $\mu$ a.e  $ x \in M $ there are real numbers $\lambda_1(x)>\ldots >\lambda_r(x)$ and decomposition $T_xM=E_1(x)\oplus \ldots \oplus E_r(x)$ such that  $$\displaystyle{\lim_{ k \rightarrow \pm \infty }\frac{1}{k}\log\Vert Df^k(x)u\Vert=\lambda_j(x)}$$  for all $u\in E_j(x)\backslash \lbrace 0\rbrace$ and  $1\leq j\leq r$. I am curious if this result can be adapted to flows. Let me know if my reasoning is correct: Theorem continuous Oseledets (B) [I think this should be]: Let $ \phi: \mathbb{R}\times M \rightarrow M $ a flows and $d\phi_x^t: T_xM \rightarrow T_{\phi^t(x)}M$ where $ \phi^t=\phi(t,.):  M \rightarrow M $ then $\mu$ a.e  $ x \in M $ there are real numbers $\lambda_1(x)>\ldots >\lambda_r(x)$ and decomposition $T_xM=E_1(x)\oplus \ldots \oplus E_r(x)$ such that  $$\displaystyle{\lim_{ t \rightarrow \pm \infty }\frac{1}{t}\log\Vert d\phi_x^tu\Vert=\lambda_j(x)} \ \ \ (*)$$  for all $u\in E_j(x)\backslash \lbrace 0\rbrace$ and  $1\leq j\leq r$. Also appreciate any suggestions to prove this assertion (B), in particular the existence of limit (*) using the result (A).","in class  was proved oseledets theorem for discrete time, following guidelines Ricardo Mañe book. Theorem discrete Oseledets (A) : Let $ M ^ n $ be a Riemannian manifold, $ f: M \rightarrow M $ be a diffeomorphism of class $ C^1$ and $\mu$ invariant probability measure for $f$. For $\mu$ a.e  $ x \in M $ there are real numbers $\lambda_1(x)>\ldots >\lambda_r(x)$ and decomposition $T_xM=E_1(x)\oplus \ldots \oplus E_r(x)$ such that  $$\displaystyle{\lim_{ k \rightarrow \pm \infty }\frac{1}{k}\log\Vert Df^k(x)u\Vert=\lambda_j(x)}$$  for all $u\in E_j(x)\backslash \lbrace 0\rbrace$ and  $1\leq j\leq r$. I am curious if this result can be adapted to flows. Let me know if my reasoning is correct: Theorem continuous Oseledets (B) [I think this should be]: Let $ \phi: \mathbb{R}\times M \rightarrow M $ a flows and $d\phi_x^t: T_xM \rightarrow T_{\phi^t(x)}M$ where $ \phi^t=\phi(t,.):  M \rightarrow M $ then $\mu$ a.e  $ x \in M $ there are real numbers $\lambda_1(x)>\ldots >\lambda_r(x)$ and decomposition $T_xM=E_1(x)\oplus \ldots \oplus E_r(x)$ such that  $$\displaystyle{\lim_{ t \rightarrow \pm \infty }\frac{1}{t}\log\Vert d\phi_x^tu\Vert=\lambda_j(x)} \ \ \ (*)$$  for all $u\in E_j(x)\backslash \lbrace 0\rbrace$ and  $1\leq j\leq r$. Also appreciate any suggestions to prove this assertion (B), in particular the existence of limit (*) using the result (A).",,"['ordinary-differential-equations', 'dynamical-systems', 'riemannian-geometry', 'ergodic-theory']"
41,What allows us to break up $dy/dx$ in solving a separable differential equation?,What allows us to break up  in solving a separable differential equation?,dy/dx,"Suppose you had a separable first order differential equation that can be written as $$ \frac{dy}{dx}=f(x,y)=g(x)h(y) $$ Rigourously, what allows us to rearrange this as $$ \frac{1}{h(y)}dy=g(x)dx? $$ I'm familiar with differential geometry, and differential $k$-forms, but is there a rigourous explanation of why treating the derivative as a fraction ends up working?","Suppose you had a separable first order differential equation that can be written as $$ \frac{dy}{dx}=f(x,y)=g(x)h(y) $$ Rigourously, what allows us to rearrange this as $$ \frac{1}{h(y)}dy=g(x)dx? $$ I'm familiar with differential geometry, and differential $k$-forms, but is there a rigourous explanation of why treating the derivative as a fraction ends up working?",,['ordinary-differential-equations']
42,Second order DE question,Second order DE question,,"I am looking for tips for this equation: $ 4xy''+y'+xy'+\frac{3}{2}y=0 $. I am solving with the substitution y=a(x)b(x), but it is getting messy..","I am looking for tips for this equation: $ 4xy''+y'+xy'+\frac{3}{2}y=0 $. I am solving with the substitution y=a(x)b(x), but it is getting messy..",,['ordinary-differential-equations']
43,How do I approximate $f''(x)+(E-U(x))f(x)=0$ for a piecewise $U$ and find $E$?,How do I approximate  for a piecewise  and find ?,f''(x)+(E-U(x))f(x)=0 U E,"I am trying to approximate the solution to the equation $f''(x)+(E-U(x))f(x)=0$ where  $U(x) = \begin{cases} \frac{U_0}{m}x-U_0 & \text{for $-m<x<0$} \\ \frac{-U_0}{m}x-U_0 & \text{for $0<x<m$} \\ 0 & \text{otherwise} \end{cases}$ using the WKB method. Assuming that the solution is bounded everywhere, I would then like to find the possible negative values of E. My attempt (and plans after I got stuck): When $E>U(x)$, WKB gives $f\approx \frac{A}{(E-U)^\frac{1}{4}}cos(\int_a^x\sqrt {E-U(x_1)}dx_1-\phi)$ for arbitrary $a$ and constants $A$ and $\phi$,  and when $E<U(x)$, WKB gives $f\approx A\frac{1}{(E-U)^\frac{1}{4}}e^{\int_a^x\sqrt {E-U(x_1)}dx_1}+B\frac{1}{(E-U)^\frac{1}{4}}e^{-\int_a^x\sqrt {E-U(x_1)}dx_1}$ for arbitrary $a$ and constants $A$ and $B$. I want negative E so when $x<-m$ or $x>m$, then $E<U$ and so the first formula applies but when $-m<x<m$, I'm not sure which formula applies as depending on the value of $E$, $E$ could be more or less than $U$ (??). Anyway, I'm trying to find a formula for $f$ for each of $x<-m$, $-m<x<0$, $0<x<m$ and $x>m$. Then, how do I combine these? Can $f$ be approximated by a piecewise function? Or do I have to look at continuity between the sections? Then, for the values of E: I apply the restriction that $f$ is bounded everywhere and hopefully that will lead to a formula for E. Any ideas or critiques of my ideas?","I am trying to approximate the solution to the equation $f''(x)+(E-U(x))f(x)=0$ where  $U(x) = \begin{cases} \frac{U_0}{m}x-U_0 & \text{for $-m<x<0$} \\ \frac{-U_0}{m}x-U_0 & \text{for $0<x<m$} \\ 0 & \text{otherwise} \end{cases}$ using the WKB method. Assuming that the solution is bounded everywhere, I would then like to find the possible negative values of E. My attempt (and plans after I got stuck): When $E>U(x)$, WKB gives $f\approx \frac{A}{(E-U)^\frac{1}{4}}cos(\int_a^x\sqrt {E-U(x_1)}dx_1-\phi)$ for arbitrary $a$ and constants $A$ and $\phi$,  and when $E<U(x)$, WKB gives $f\approx A\frac{1}{(E-U)^\frac{1}{4}}e^{\int_a^x\sqrt {E-U(x_1)}dx_1}+B\frac{1}{(E-U)^\frac{1}{4}}e^{-\int_a^x\sqrt {E-U(x_1)}dx_1}$ for arbitrary $a$ and constants $A$ and $B$. I want negative E so when $x<-m$ or $x>m$, then $E<U$ and so the first formula applies but when $-m<x<m$, I'm not sure which formula applies as depending on the value of $E$, $E$ could be more or less than $U$ (??). Anyway, I'm trying to find a formula for $f$ for each of $x<-m$, $-m<x<0$, $0<x<m$ and $x>m$. Then, how do I combine these? Can $f$ be approximated by a piecewise function? Or do I have to look at continuity between the sections? Then, for the values of E: I apply the restriction that $f$ is bounded everywhere and hopefully that will lead to a formula for E. Any ideas or critiques of my ideas?",,"['ordinary-differential-equations', 'approximation-theory']"
44,A second order differential equation,A second order differential equation,,"How does one solve the following differential equation $y^{""}+xy^{'}+(1-x^2)y=y\sin x$? I don't know how to proceed?","How does one solve the following differential equation $y^{""}+xy^{'}+(1-x^2)y=y\sin x$? I don't know how to proceed?",,['ordinary-differential-equations']
45,Derivation of Lagrange-Charpit Equations [duplicate],Derivation of Lagrange-Charpit Equations [duplicate],,"This question already has an answer here : How to take this exterior derivative of the expression $du - \sum_i p_i dx_i$? (1 answer) Closed 6 years ago . I am working through the derivation of the Lagrange-Charpit equations presented in this Wikipedia article: http://en.wikipedia.org/wiki/Method_of_characteristics#Fully_nonlinear_case I am interested in the ""fully"" nonlinear case. Where we have: $$F(x_1,...,x_n,u,p_1,...,p_n)=0  $$ And $$ p_i=\frac{\partial u}{\partial x_i} $$ I am fine with the derivation up until the point where they say that (Also, $\dot{x_i}=dx_i/ds$): $$\sum_i(\dot{x}_idp_i-\dot{p_i}dx_i)=0 $$ Follows from taking the exterior derivative of: $$ du-\sum_ip_idx_i=0  $$ From the little I know about exterior derivatives, it seems like doing this would give (for the two dimensional case): $$0=\left(\frac{\partial p_2}{\partial x_1}-\frac{\partial p_1}{\partial x_2}\right)dx_1\wedge dx_2 $$ Because $ddu=0$ and the anti-symmetry of the wedge product.  I don't see how this result leads to the one given.  Could someone help me out? I feel like there is something very simple that I am missing.","This question already has an answer here : How to take this exterior derivative of the expression $du - \sum_i p_i dx_i$? (1 answer) Closed 6 years ago . I am working through the derivation of the Lagrange-Charpit equations presented in this Wikipedia article: http://en.wikipedia.org/wiki/Method_of_characteristics#Fully_nonlinear_case I am interested in the ""fully"" nonlinear case. Where we have: $$F(x_1,...,x_n,u,p_1,...,p_n)=0  $$ And $$ p_i=\frac{\partial u}{\partial x_i} $$ I am fine with the derivation up until the point where they say that (Also, $\dot{x_i}=dx_i/ds$): $$\sum_i(\dot{x}_idp_i-\dot{p_i}dx_i)=0 $$ Follows from taking the exterior derivative of: $$ du-\sum_ip_idx_i=0  $$ From the little I know about exterior derivatives, it seems like doing this would give (for the two dimensional case): $$0=\left(\frac{\partial p_2}{\partial x_1}-\frac{\partial p_1}{\partial x_2}\right)dx_1\wedge dx_2 $$ Because $ddu=0$ and the anti-symmetry of the wedge product.  I don't see how this result leads to the one given.  Could someone help me out? I feel like there is something very simple that I am missing.",,"['ordinary-differential-equations', 'partial-differential-equations']"
46,Solution for $\frac{a}{x} = \int_0^1 \frac{f(z)}{\left(f(x)+f(z)\right)^2} dz$,Solution for,\frac{a}{x} = \int_0^1 \frac{f(z)}{\left(f(x)+f(z)\right)^2} dz,I am looking for the function $f(x)$ that solves $\frac{a}{x}  = \int_0^1 \frac{f(z)}{\left(f(x)+f(z)\right)^2} dz$ such that $f(0)=0$. Even hints how to approach to this question would be very helpful.,I am looking for the function $f(x)$ that solves $\frac{a}{x}  = \int_0^1 \frac{f(z)}{\left(f(x)+f(z)\right)^2} dz$ such that $f(0)=0$. Even hints how to approach to this question would be very helpful.,,['ordinary-differential-equations']
47,What is the solution to the system $\frac{df_n}{dt} = kf_{n-1}-(k+l)f_n+lf_{n+1}$?,What is the solution to the system ?,\frac{df_n}{dt} = kf_{n-1}-(k+l)f_n+lf_{n+1},"I'm trying to solve the system $$ \begin{matrix} & \frac{df_1}{dt} = kf_1+lf_2 \\ & \vdots \\ & \frac{df_n}{dt} = kf_{n-1}-(k+l)f_n+lf_{n+1} \\ & \vdots \\ & \frac{df_N}{dt} = kf_{N-1}-lf_N \end{matrix} \tag{1}$$ with $f_1 (0) = f_0$ and $\forall n \neq 1 \ [f_n(0)=0]$, where $k$, $l$ and $f_0$ are real positive constants. The system may also be represented diagrammatically as $\fbox{1}\leftrightarrow \fbox{2}\leftrightarrow \fbox{3}\leftrightarrow \cdots\leftrightarrow \fbox{N}$ if this is desired. What is the solution to that system? I would like to study the cases where $N$ is finite and arbitrarily large. The standard general method (substitute exponentials, solve the resulting linear system) when followed directly becomes untractable for large N; I believe that a method that takes advantage of particular features of the problem besides linearity will be fruitful. There are two methods that I have considered, but I have been unable to carry out the calculation in either of them to completion. First, a direct method, using Laplace transforms; a strategy that renders the solution of the case $l=0$ particularly easy. Setting $t\rightarrow lt$ and defining $r = \tfrac{k}{l}$ eliminates one parameter, so that for some $n \neq 1,N$ $$ \frac{df_n}{dt} = rf_{n-1}-(r+1)f_n+f_{n+1} $$ Taking the laplace transform ($f(t) \mapsto \tilde{f}(s)$) and rewriting the system in matrix form leads to the linear system $$ \begin{pmatrix}  s+r   &   -1   &   0    &    0    &    0    &        & \cdots & \cdots &  0  \\  -r    &  s+r+1 &  -1    &    0    &    0    &        & \cdots & \cdots &  0  \\   0    &   -r   & s+r+1  &   -1    &    0    &        & \cdots & \cdots &  0  \\   0    &    0   &  -r    &  s+r+1  &   -1    &        & \cdots & \cdots &  0  \\ \vdots & \vdots & \vdots & \vdots  &         & \ddots & \ddots & \ddots &\vdots\\    0   &    0   &    0   &    0    & \cdots  & 0      &   -r   & s+r+1  &  -1 \\    0   &    0   &    0   &    0    & \cdots  & 0      &    0   &  -r    & s+1 \\ \end{pmatrix} \mathbf{f}_s = f_0 \begin{pmatrix} 1 \\ 0 \\ 0 \\ 0 \\ \vdots\\ 0 \\ 0 \\ \end{pmatrix} \Leftrightarrow \mathcal{T}_{N,s} \mathbf{f}_s =  f_0  \begin{pmatrix} 1\\ 0\\ \vdots \end{pmatrix} $$ I am not sure how to proceed with solving this system, however I do believe that a recursive relation can be found without too much effort. I also suspect that the fact that $\mathcal{T}$ is a tridiagonal 'perturbed' Toeplitz matrix should help in the calculation. However, since this solution leads to rational functions with an N-th degree polynomial in the denominator, for which the inverse Laplace transform must be calculated and in general factorising a polynomial of arbitrary degree cannot be done analytically, I abandon the calculation at this stage (I have worked out solutions for N=3 but even then the calculations are very long and tedious when done directly). There may be some shortcut to estimating the solution using this method - but I fail to see it. The second method that I have tried (which in reality it is more of an outline than a complete calculation) is to notice that the original system can be seen as a sum of all possible (and terminating) 'histories', such as $\fbox{1} \fbox{2} \fbox{3} \fbox{2} \fbox{3} \fbox{4} \cdots$ and $\fbox{1} \fbox{2} \fbox{3} \fbox{4} \fbox{5} \fbox{4} \fbox{3} \fbox{2} \cdots$. The solution for individual 'histories' (for the intermediate states) will be of the form $$ \sum_{m} { \left[ a_m \frac{(-kt)^m}{m!} e^{-kt} + b_m \frac{(-lt)^m}{m!} e^{-lt} \right] } \tag{2} $$ implying that the final solution of (1) must have a representation of the form $$\sum_{Histories} {\sum_{m} { \left[ a_m \frac{(-kt)^m}{m!} e^{-kt} + b_m \frac{(-lt)^m}{m!} e^{-lt} \right] }}$$ so that perhaps a solution like (2) can be used as an ansatz for the original equation. But I am not certain if this process is along the right path either. I have also thought of follwoing the usual method for linear systems of ODEs, i.e. calculating the spectrum of $\mathcal{T}_{r,1}$ $$ \mathcal{T}_{r,1} = \begin{pmatrix}   -r   &    1    &    0   &    0    & \cdots & \cdots & \cdots & \cdots \\    r   &   -r-1  &    1   &    0    & \cdots & \cdots & \cdots & \cdots \\    0   &    r    &  -r-1  &    1    & \cdots & \cdots & \cdots & \cdots \\ \vdots & \vdots  & \vdots & \vdots  & \ddots &        &        &        \\        &         &        &         &        &    r   &  -r-1  &   -1   \\        &         &        &         & \cdots &    0   &    r   &   -1   \\ \end{pmatrix} $$ which contains $-r-1$ alsong the diagonal (apart of positions {1,1} and {N,N}, which are $-r$ and $-1$ respecively) $r$ in the subdiagonal and $-1$ in the superdiagonal. I am not certain how to calculate this either for arbitrary N. Help will be greatly appreciated!","I'm trying to solve the system $$ \begin{matrix} & \frac{df_1}{dt} = kf_1+lf_2 \\ & \vdots \\ & \frac{df_n}{dt} = kf_{n-1}-(k+l)f_n+lf_{n+1} \\ & \vdots \\ & \frac{df_N}{dt} = kf_{N-1}-lf_N \end{matrix} \tag{1}$$ with $f_1 (0) = f_0$ and $\forall n \neq 1 \ [f_n(0)=0]$, where $k$, $l$ and $f_0$ are real positive constants. The system may also be represented diagrammatically as $\fbox{1}\leftrightarrow \fbox{2}\leftrightarrow \fbox{3}\leftrightarrow \cdots\leftrightarrow \fbox{N}$ if this is desired. What is the solution to that system? I would like to study the cases where $N$ is finite and arbitrarily large. The standard general method (substitute exponentials, solve the resulting linear system) when followed directly becomes untractable for large N; I believe that a method that takes advantage of particular features of the problem besides linearity will be fruitful. There are two methods that I have considered, but I have been unable to carry out the calculation in either of them to completion. First, a direct method, using Laplace transforms; a strategy that renders the solution of the case $l=0$ particularly easy. Setting $t\rightarrow lt$ and defining $r = \tfrac{k}{l}$ eliminates one parameter, so that for some $n \neq 1,N$ $$ \frac{df_n}{dt} = rf_{n-1}-(r+1)f_n+f_{n+1} $$ Taking the laplace transform ($f(t) \mapsto \tilde{f}(s)$) and rewriting the system in matrix form leads to the linear system $$ \begin{pmatrix}  s+r   &   -1   &   0    &    0    &    0    &        & \cdots & \cdots &  0  \\  -r    &  s+r+1 &  -1    &    0    &    0    &        & \cdots & \cdots &  0  \\   0    &   -r   & s+r+1  &   -1    &    0    &        & \cdots & \cdots &  0  \\   0    &    0   &  -r    &  s+r+1  &   -1    &        & \cdots & \cdots &  0  \\ \vdots & \vdots & \vdots & \vdots  &         & \ddots & \ddots & \ddots &\vdots\\    0   &    0   &    0   &    0    & \cdots  & 0      &   -r   & s+r+1  &  -1 \\    0   &    0   &    0   &    0    & \cdots  & 0      &    0   &  -r    & s+1 \\ \end{pmatrix} \mathbf{f}_s = f_0 \begin{pmatrix} 1 \\ 0 \\ 0 \\ 0 \\ \vdots\\ 0 \\ 0 \\ \end{pmatrix} \Leftrightarrow \mathcal{T}_{N,s} \mathbf{f}_s =  f_0  \begin{pmatrix} 1\\ 0\\ \vdots \end{pmatrix} $$ I am not sure how to proceed with solving this system, however I do believe that a recursive relation can be found without too much effort. I also suspect that the fact that $\mathcal{T}$ is a tridiagonal 'perturbed' Toeplitz matrix should help in the calculation. However, since this solution leads to rational functions with an N-th degree polynomial in the denominator, for which the inverse Laplace transform must be calculated and in general factorising a polynomial of arbitrary degree cannot be done analytically, I abandon the calculation at this stage (I have worked out solutions for N=3 but even then the calculations are very long and tedious when done directly). There may be some shortcut to estimating the solution using this method - but I fail to see it. The second method that I have tried (which in reality it is more of an outline than a complete calculation) is to notice that the original system can be seen as a sum of all possible (and terminating) 'histories', such as $\fbox{1} \fbox{2} \fbox{3} \fbox{2} \fbox{3} \fbox{4} \cdots$ and $\fbox{1} \fbox{2} \fbox{3} \fbox{4} \fbox{5} \fbox{4} \fbox{3} \fbox{2} \cdots$. The solution for individual 'histories' (for the intermediate states) will be of the form $$ \sum_{m} { \left[ a_m \frac{(-kt)^m}{m!} e^{-kt} + b_m \frac{(-lt)^m}{m!} e^{-lt} \right] } \tag{2} $$ implying that the final solution of (1) must have a representation of the form $$\sum_{Histories} {\sum_{m} { \left[ a_m \frac{(-kt)^m}{m!} e^{-kt} + b_m \frac{(-lt)^m}{m!} e^{-lt} \right] }}$$ so that perhaps a solution like (2) can be used as an ansatz for the original equation. But I am not certain if this process is along the right path either. I have also thought of follwoing the usual method for linear systems of ODEs, i.e. calculating the spectrum of $\mathcal{T}_{r,1}$ $$ \mathcal{T}_{r,1} = \begin{pmatrix}   -r   &    1    &    0   &    0    & \cdots & \cdots & \cdots & \cdots \\    r   &   -r-1  &    1   &    0    & \cdots & \cdots & \cdots & \cdots \\    0   &    r    &  -r-1  &    1    & \cdots & \cdots & \cdots & \cdots \\ \vdots & \vdots  & \vdots & \vdots  & \ddots &        &        &        \\        &         &        &         &        &    r   &  -r-1  &   -1   \\        &         &        &         & \cdots &    0   &    r   &   -1   \\ \end{pmatrix} $$ which contains $-r-1$ alsong the diagonal (apart of positions {1,1} and {N,N}, which are $-r$ and $-1$ respecively) $r$ in the subdiagonal and $-1$ in the superdiagonal. I am not certain how to calculate this either for arbitrary N. Help will be greatly appreciated!",,"['linear-algebra', 'ordinary-differential-equations', 'dynamical-systems', 'systems-of-equations']"
48,Non-Linear Ordinary Differential Equation in Fluid Dynamics,Non-Linear Ordinary Differential Equation in Fluid Dynamics,,"So while trying to model the physics of a rocket shot from the ground through the atmosphere, I came up with a second-order Non-Linear ODE of the form: $$ \ddot y + \dot y^2 e^y = f(t) $$ This is simplified, of course, but I have been searching and cannot determine a solution or if there even IS an analytic solution. Of course, maybe from my short description of the problem you already see that I am taking the wrong approach entirely; if so, please direct me to the right line of thinking. So in summary: Is there an analytic solution to this equation? If so, what is it? If not, how should I approach this problem given certain initial values to get a numeric solution? Thanks very much in advance.","So while trying to model the physics of a rocket shot from the ground through the atmosphere, I came up with a second-order Non-Linear ODE of the form: $$ \ddot y + \dot y^2 e^y = f(t) $$ This is simplified, of course, but I have been searching and cannot determine a solution or if there even IS an analytic solution. Of course, maybe from my short description of the problem you already see that I am taking the wrong approach entirely; if so, please direct me to the right line of thinking. So in summary: Is there an analytic solution to this equation? If so, what is it? If not, how should I approach this problem given certain initial values to get a numeric solution? Thanks very much in advance.",,"['ordinary-differential-equations', 'nonlinear-system', 'fluid-dynamics']"
49,Differential Equations.Is it possible to solve?,Differential Equations.Is it possible to solve?,,"$$ e^y(1+x^2)dy-2x(1+e^y)dx =0 $$ $$ e^y(1+x^2)dy=2x(1+e^y)dx $$ multiplies both sides by $$ \frac{1}{(1+x^2)(1+e^y)} $$ $$ $$ $$\frac{e^y dy}{1+e^y}=\frac{2xdx}{1+x^2} $$ introduced under the integral sign $$\int\frac{e^y dy}{1+e^y}=\int\frac{2xdx}{1+x^2} $$ After the conversion was $$Ln|1+e^y|=Ln|1+x^2|+C $$ $$ 1+e^y=(1+x^2)*C $$ $$ e^y= (1+x^2)*C-1$$ $$$$ And here is the main question , if I can make both sides of the logarithm to express Y $$ ln (e^y)=Ln ((1+x^2)*C-1)$$ $$ y*Ln(e)=Ln ((1+x^2)*C-1) $$ $$ y=Ln ((1+x^2)*C-1)$$ Can I do that?","$$ e^y(1+x^2)dy-2x(1+e^y)dx =0 $$ $$ e^y(1+x^2)dy=2x(1+e^y)dx $$ multiplies both sides by $$ \frac{1}{(1+x^2)(1+e^y)} $$ $$ $$ $$\frac{e^y dy}{1+e^y}=\frac{2xdx}{1+x^2} $$ introduced under the integral sign $$\int\frac{e^y dy}{1+e^y}=\int\frac{2xdx}{1+x^2} $$ After the conversion was $$Ln|1+e^y|=Ln|1+x^2|+C $$ $$ 1+e^y=(1+x^2)*C $$ $$ e^y= (1+x^2)*C-1$$ $$$$ And here is the main question , if I can make both sides of the logarithm to express Y $$ ln (e^y)=Ln ((1+x^2)*C-1)$$ $$ y*Ln(e)=Ln ((1+x^2)*C-1) $$ $$ y=Ln ((1+x^2)*C-1)$$ Can I do that?",,['ordinary-differential-equations']
50,Application of Picard-Lindelöf to determine uniqueness of a solution to an IVP,Application of Picard-Lindelöf to determine uniqueness of a solution to an IVP,,"I am still struggling quite a lot with the Picard-Lindelöf-Theorem (also known as the Cauchy-Lipschitz-Theorem ). Problem : Consider the following IVP with $\alpha \neq 1$  $$\begin{cases} y'&= t|y|^\alpha \\ y(0)&=1 \end{cases} $$   And show that there exists a unique solution $f_\alpha$ on an Interval $I_\alpha$ with $0 \in I_\alpha$, also show that $f_\alpha (t) > 0$ for small $t$ So to use Picard-Lindelöf I want to show that $f(t,y)=t|y|^\alpha$ is Lipschitz-continuous with respect to its second variable. I could do this by computing the partial derivative with respect to $y$ and find an interval on which this expression is bound, then on said hypothetical interval the solution would exist and be unique. My approach : $$\frac{\partial f}{\partial y}= t\alpha|y|^{\alpha-1} \cdot \frac{y}{\sqrt{y^2}} $$ At which point I already run into trouble. I have made use of the fact that $|x|'= x/\sqrt{x^2}$ because it gives correct results in regard of the direction (when approaching from the left and the right). But the above expression is not defined at $y=0$. So I thought if I could come up with an argument that shows why $y \neq 0$, then at least this problem would be out of the way. Assume that $y=0$ then $y'=0=f(t,y) \implies \frac{\partial f}{\partial y}=0$ which is bound on $\mathbb{R} \implies f$ is Lipschitz continuous with respect to $y$ but we have $y(0)=1$ and thus $$y'=0 \implies y=c $$ So $y$ is a constant function and $y(0)=1$ means that $c=1 \implies y=1 \neq 0$ which shows that $y$ can not be the 'zero' function (if that exists). Questions : Is the above argumentation correct? How can I find the desired interval $I_\alpha$ with $0 \in I_\alpha$? Update : I tried to continue a bit on my own and ignore the Picard-Lindelöf part and just focus on solving the IVP, apparently the step I have taken above was necessary so I can divide by $|y|^\alpha$, however I will land at $$\frac{dy}{|y|^\alpha}=tdx $$ So it seems like I have to come up with a condition that guarantees me that $y$ is positive in some Intervall.","I am still struggling quite a lot with the Picard-Lindelöf-Theorem (also known as the Cauchy-Lipschitz-Theorem ). Problem : Consider the following IVP with $\alpha \neq 1$  $$\begin{cases} y'&= t|y|^\alpha \\ y(0)&=1 \end{cases} $$   And show that there exists a unique solution $f_\alpha$ on an Interval $I_\alpha$ with $0 \in I_\alpha$, also show that $f_\alpha (t) > 0$ for small $t$ So to use Picard-Lindelöf I want to show that $f(t,y)=t|y|^\alpha$ is Lipschitz-continuous with respect to its second variable. I could do this by computing the partial derivative with respect to $y$ and find an interval on which this expression is bound, then on said hypothetical interval the solution would exist and be unique. My approach : $$\frac{\partial f}{\partial y}= t\alpha|y|^{\alpha-1} \cdot \frac{y}{\sqrt{y^2}} $$ At which point I already run into trouble. I have made use of the fact that $|x|'= x/\sqrt{x^2}$ because it gives correct results in regard of the direction (when approaching from the left and the right). But the above expression is not defined at $y=0$. So I thought if I could come up with an argument that shows why $y \neq 0$, then at least this problem would be out of the way. Assume that $y=0$ then $y'=0=f(t,y) \implies \frac{\partial f}{\partial y}=0$ which is bound on $\mathbb{R} \implies f$ is Lipschitz continuous with respect to $y$ but we have $y(0)=1$ and thus $$y'=0 \implies y=c $$ So $y$ is a constant function and $y(0)=1$ means that $c=1 \implies y=1 \neq 0$ which shows that $y$ can not be the 'zero' function (if that exists). Questions : Is the above argumentation correct? How can I find the desired interval $I_\alpha$ with $0 \in I_\alpha$? Update : I tried to continue a bit on my own and ignore the Picard-Lindelöf part and just focus on solving the IVP, apparently the step I have taken above was necessary so I can divide by $|y|^\alpha$, however I will land at $$\frac{dy}{|y|^\alpha}=tdx $$ So it seems like I have to come up with a condition that guarantees me that $y$ is positive in some Intervall.",,"['calculus', 'real-analysis', 'ordinary-differential-equations', 'definition', 'self-learning']"
51,Solving second order nonlinear ODE,Solving second order nonlinear ODE,,"Having the following second order ordinary differential equation: $$ \ddot{x} = a \cos(x) $$ where, $a$ is a constant. What's an approach to solve this kind of equation?","Having the following second order ordinary differential equation: $$ \ddot{x} = a \cos(x) $$ where, $a$ is a constant. What's an approach to solve this kind of equation?",,"['ordinary-differential-equations', 'nonlinear-system']"
52,$y=0$ is singular solution of $\frac{dy}{dx}=E(y)$ iff the improper integral $\int_{0}^{1}\frac{dy}{E(y)}$ is convergent,is singular solution of  iff the improper integral  is convergent,y=0 \frac{dy}{dx}=E(y) \int_{0}^{1}\frac{dy}{E(y)},"Assume that a continuous function $E(y)$ is such that $$E(0)=0,~~~~E(y)\neq 0,~~~~0<y\le 1.$$ Then $y=0$ is the singular solution of the differential equation $$ \dfrac{dy}{dx}=E(y), $$ if and only if the improper integral $$ \int_{0}^{1}\dfrac{dy}{E(y)}$$ is  convergent. This problem is from china ODE problem (can see : http://item.jd.com/1048628556.html , page 111. problem 4, and I don't see any book have solution by this problem. Thank you","Assume that a continuous function is such that Then is the singular solution of the differential equation if and only if the improper integral is  convergent. This problem is from china ODE problem (can see : http://item.jd.com/1048628556.html , page 111. problem 4, and I don't see any book have solution by this problem. Thank you","E(y) E(0)=0,~~~~E(y)\neq 0,~~~~0<y\le 1. y=0 
\dfrac{dy}{dx}=E(y),
 
\int_{0}^{1}\dfrac{dy}{E(y)}","['ordinary-differential-equations', 'singular-solution']"
53,Fundamental Solution of a Nonlinear ODE (using Riccati Transformation & Wronskian),Fundamental Solution of a Nonlinear ODE (using Riccati Transformation & Wronskian),,"I am given the differential equation: \begin{equation*} y^{\prime}(t) = y(t)^{2} + 2\sin(t)\cos(t) - \sin^{4}(t) \end{equation*} and one solution $y_{1}(t) = \sin^{2}(t)$. I wish to find a second solution, and I am told this second solution should also be periodic. To find the second solution $y_{2}$, I use the following steps: 1.) Convert the nonlinear differential equation to a second order system of differential equations using the Riccati transformation: $y = -z^{\prime}/z$. This gives me the form: $z^{\prime}(t) = A(t)z$ with  \begin{equation*} A(t) =  \left[ \begin{matrix} 0 & 1 \\ -2\sin(t)\cos(t)-\sin^{4}(t) & 0 \end{matrix} \right] \end{equation*} and solving $-y_{1}(t) = -\sin^{2}(t) = z^{\prime}/z$, \begin{alignat*}{2} z(t) = ce^{-\int_{0}^{t}\sin^{2}(s)ds}, \\ z^{\prime}(t) = -c\sin^{2}(t)e^{-\int_{0}^{t}\sin^{2}(s)ds} \end{alignat*} (I dispense with the common constant $c$ since I'm using a general solution matrix in the next step). 2.) Form the Wronskian matrix with known $z(t)$ and unknown $\phi(t)$, and use Abel's formula to solve for $\phi(t)$, i.e. \begin{alignat*}{2} W(t) &= \left| \begin{matrix} z(t) & \phi(t) \\ z^{\prime}(t) & \phi^{\prime}(t) \end{matrix} \right| \\ &= z(t)\phi^{\prime}(t) - z^{\prime}(t)\phi(t) \\ &= W_{0}e^{-\int_{0}^{t}\text{tr}(A(s))ds} \\ &= W_{0}e^{-\int_{0}^{t}0ds} \\ &= W_{0} \end{alignat*} 3.) Solving the resulting differential equation for $\phi$.  Thus, \begin{alignat*}{2} z(t)\phi^{\prime}(t) - z^{\prime}(t)\phi(t) &= W_{0} &&\Rightarrow \\ z(t)\phi^{\prime}(t) + \sin^{2}(t)z(t)\phi(t) &= W_{0} &&\Rightarrow \\ \phi^{\prime}(t) + \sin^{2}(t)\phi(t) &= W_{0}z^{-1}(t) &&\Rightarrow \end{alignat*} Let $\mu(t) = z^{-1}(t)$, and multiply both sides by $\mu$ \begin{alignat*}{2} \mu(t)\phi^{\prime}(t) + \sin^{2}(t)\mu(t)\phi(t) &= W_{0}\mu^{2}(t) &&\Rightarrow \\ \mu(t)\phi^{\prime}(t) + \mu^{\prime}(t)\phi(t) &= W_{0}\mu^{2}(t)&&\Rightarrow \\ \frac{\partial}{\partial t}\left(\mu(t)\phi(t)\right) &= W_{0}\mu^{2} \\ \mu(t)\phi(t) &= W_{0}\int_{0}^{t}\mu^{2}(s)ds &&\Rightarrow \\ \phi(t) &= W_{0}z(t)\int_{0}^{t}\mu^{2}(s)ds &&\Rightarrow \\ \phi^{\prime}(t) &= W_{0}z^{\prime}(t)\int_{0}^{t}\mu^{2}(s)ds +W_{0}z(t)\mu^{2}(t) \end{alignat*} 4.) Convert back using the same Riccati transformation: \begin{alignat*}{2} y_{2}(t) &= -\frac{\phi^{\prime}(t)}{\phi(t)} \\ &= -y_{1}(t) - \frac{\mu^{2}(t)}{\int_{0}^{t}\mu^{2}(s)ds}.   \end{alignat*} However, this result does not look like it is going to solve the original nonlinear differential equation nor does it seem periodic.  I would like to determine if I have made a calculation error or if there is a flaw in my understanding of this material or a combination of both.","I am given the differential equation: \begin{equation*} y^{\prime}(t) = y(t)^{2} + 2\sin(t)\cos(t) - \sin^{4}(t) \end{equation*} and one solution $y_{1}(t) = \sin^{2}(t)$. I wish to find a second solution, and I am told this second solution should also be periodic. To find the second solution $y_{2}$, I use the following steps: 1.) Convert the nonlinear differential equation to a second order system of differential equations using the Riccati transformation: $y = -z^{\prime}/z$. This gives me the form: $z^{\prime}(t) = A(t)z$ with  \begin{equation*} A(t) =  \left[ \begin{matrix} 0 & 1 \\ -2\sin(t)\cos(t)-\sin^{4}(t) & 0 \end{matrix} \right] \end{equation*} and solving $-y_{1}(t) = -\sin^{2}(t) = z^{\prime}/z$, \begin{alignat*}{2} z(t) = ce^{-\int_{0}^{t}\sin^{2}(s)ds}, \\ z^{\prime}(t) = -c\sin^{2}(t)e^{-\int_{0}^{t}\sin^{2}(s)ds} \end{alignat*} (I dispense with the common constant $c$ since I'm using a general solution matrix in the next step). 2.) Form the Wronskian matrix with known $z(t)$ and unknown $\phi(t)$, and use Abel's formula to solve for $\phi(t)$, i.e. \begin{alignat*}{2} W(t) &= \left| \begin{matrix} z(t) & \phi(t) \\ z^{\prime}(t) & \phi^{\prime}(t) \end{matrix} \right| \\ &= z(t)\phi^{\prime}(t) - z^{\prime}(t)\phi(t) \\ &= W_{0}e^{-\int_{0}^{t}\text{tr}(A(s))ds} \\ &= W_{0}e^{-\int_{0}^{t}0ds} \\ &= W_{0} \end{alignat*} 3.) Solving the resulting differential equation for $\phi$.  Thus, \begin{alignat*}{2} z(t)\phi^{\prime}(t) - z^{\prime}(t)\phi(t) &= W_{0} &&\Rightarrow \\ z(t)\phi^{\prime}(t) + \sin^{2}(t)z(t)\phi(t) &= W_{0} &&\Rightarrow \\ \phi^{\prime}(t) + \sin^{2}(t)\phi(t) &= W_{0}z^{-1}(t) &&\Rightarrow \end{alignat*} Let $\mu(t) = z^{-1}(t)$, and multiply both sides by $\mu$ \begin{alignat*}{2} \mu(t)\phi^{\prime}(t) + \sin^{2}(t)\mu(t)\phi(t) &= W_{0}\mu^{2}(t) &&\Rightarrow \\ \mu(t)\phi^{\prime}(t) + \mu^{\prime}(t)\phi(t) &= W_{0}\mu^{2}(t)&&\Rightarrow \\ \frac{\partial}{\partial t}\left(\mu(t)\phi(t)\right) &= W_{0}\mu^{2} \\ \mu(t)\phi(t) &= W_{0}\int_{0}^{t}\mu^{2}(s)ds &&\Rightarrow \\ \phi(t) &= W_{0}z(t)\int_{0}^{t}\mu^{2}(s)ds &&\Rightarrow \\ \phi^{\prime}(t) &= W_{0}z^{\prime}(t)\int_{0}^{t}\mu^{2}(s)ds +W_{0}z(t)\mu^{2}(t) \end{alignat*} 4.) Convert back using the same Riccati transformation: \begin{alignat*}{2} y_{2}(t) &= -\frac{\phi^{\prime}(t)}{\phi(t)} \\ &= -y_{1}(t) - \frac{\mu^{2}(t)}{\int_{0}^{t}\mu^{2}(s)ds}.   \end{alignat*} However, this result does not look like it is going to solve the original nonlinear differential equation nor does it seem periodic.  I would like to determine if I have made a calculation error or if there is a flaw in my understanding of this material or a combination of both.",,['ordinary-differential-equations']
54,Showing that solutions to a differential equation are non-negative for all time,Showing that solutions to a differential equation are non-negative for all time,,"We've been given the following question: Let $a$ and $p$ be real-valued piecewise continuous functions defined on $[t_0,t_1]$ and let $E \subset [t_0,t_1]$ be the (finite) set of points at which $a$ or $p$ fail to be continuous. Assume that $x:[t_0,t_1] \rightarrow \mathbb{R}$ is continuous and satisfies $$ x(t) = x(t_0) + \int_{t_0}^t a(s)x(s)+p(s) ds$$ Show that if $x(t_0) \ge 0$ and $p(t) \ge 0$ for all $t \in [t_0,t_1]$, then $x(t) \ge 0$ for all $t \in [t_0,t_1]$. I've little idea of how to go about showing this. I've tried going about it by contradiction, assuming that $x(t)$ was negative at some point (and so in some small interval by continuity) but this got me nowhere. I'd appreciate a pointer in the right direction.","We've been given the following question: Let $a$ and $p$ be real-valued piecewise continuous functions defined on $[t_0,t_1]$ and let $E \subset [t_0,t_1]$ be the (finite) set of points at which $a$ or $p$ fail to be continuous. Assume that $x:[t_0,t_1] \rightarrow \mathbb{R}$ is continuous and satisfies $$ x(t) = x(t_0) + \int_{t_0}^t a(s)x(s)+p(s) ds$$ Show that if $x(t_0) \ge 0$ and $p(t) \ge 0$ for all $t \in [t_0,t_1]$, then $x(t) \ge 0$ for all $t \in [t_0,t_1]$. I've little idea of how to go about showing this. I've tried going about it by contradiction, assuming that $x(t)$ was negative at some point (and so in some small interval by continuity) but this got me nowhere. I'd appreciate a pointer in the right direction.",,['ordinary-differential-equations']
55,Scalar Autonomous Differential Equation?,Scalar Autonomous Differential Equation?,,"What precisely is a scalar autonomous differential equation? I'm confused about what this precisely means, more so because we did not discuss this in any lectures nor is it, as far as I can tell, defined in the notes. I've tried looking it up as well, but I couldn't find anything, only specific examples. It occurs in the following question: Show that every scalar autonomous differential equation with $f\in\mathcal{C}^1$ is of gradient-type. Furthermore, show that the matrix of a linear gradient equation is symmetric. Here gradient-type means that for a (system of) autonomous differential equation(s) $\dot{y}=f(y)$, there exists $V:\mathbb{R}^n\supset\!\to\mathbb{R}$ such that $f=\operatorname{grad}V$ with $V\in\mathcal{C}^2$. Furthermore, what is a (linear) gradient equation? Do they simply mean a (linear) differential equation of gradient-type? The notes are not clear about it at all. Any help would be appreciated. Thank you!","What precisely is a scalar autonomous differential equation? I'm confused about what this precisely means, more so because we did not discuss this in any lectures nor is it, as far as I can tell, defined in the notes. I've tried looking it up as well, but I couldn't find anything, only specific examples. It occurs in the following question: Show that every scalar autonomous differential equation with $f\in\mathcal{C}^1$ is of gradient-type. Furthermore, show that the matrix of a linear gradient equation is symmetric. Here gradient-type means that for a (system of) autonomous differential equation(s) $\dot{y}=f(y)$, there exists $V:\mathbb{R}^n\supset\!\to\mathbb{R}$ such that $f=\operatorname{grad}V$ with $V\in\mathcal{C}^2$. Furthermore, what is a (linear) gradient equation? Do they simply mean a (linear) differential equation of gradient-type? The notes are not clear about it at all. Any help would be appreciated. Thank you!",,['ordinary-differential-equations']
56,Determining whether the origin is an attracting fixed point for a scalar system,Determining whether the origin is an attracting fixed point for a scalar system,,"I have been asked to determine and prove the attraction properties of a continuous-time dynamical system, generated by the ODE  \begin{equation} \frac{dx}{dt} =-x \end{equation} which gives the system  \begin{equation} S_tx=x_0e^{-t}.  \end{equation} For the compact invariant set $I=\{0\}$ I need to work out whether it attracts points in a neighbourhood of $I$, attracts points in the set of all real numbers, attracts all compact sets in $\Re$, or attracts all sets in $\Re$. How would I go about doing this? Any help would be greatly appreciated.","I have been asked to determine and prove the attraction properties of a continuous-time dynamical system, generated by the ODE  \begin{equation} \frac{dx}{dt} =-x \end{equation} which gives the system  \begin{equation} S_tx=x_0e^{-t}.  \end{equation} For the compact invariant set $I=\{0\}$ I need to work out whether it attracts points in a neighbourhood of $I$, attracts points in the set of all real numbers, attracts all compact sets in $\Re$, or attracts all sets in $\Re$. How would I go about doing this? Any help would be greatly appreciated.",,"['ordinary-differential-equations', 'dynamical-systems']"
57,Measurability of points regular,Measurability of points regular,,"I'm reviewing the proof of the theorem of oseledet the book  Mañe: Let $M$ a compact metric space and $f:M \rightarrow M$ a homeomorphism, $\pi: F \rightarrow M$ a finite-dimensional continuos vector bundle over $M$, endowed with a continuous Riemannian metric. Let $L: F \rightarrow F$ vector bundles covering $f$ (i.e. $\pi\circ L= f\circ \pi$) such that both $L$ and $L^{-1}$ have bounded normas. Denote by $L_n$ the n-th iterate of $L$  $$L_n(x)=L(f^{n-1}(x))\circ \ldots \circ L(f(x))\circ L(x) \ \ \ \mbox{if} \ \ n>0,$$ $$L_n(x)=L^{-1}(f^{-n+1}(x))\circ \ldots \circ L^{-1}(f^{-1}(x))\circ L^{-1}(x)\ \ \ \mbox{if} \ \ n<0.$$ For $n_1,\ldots, n_l\geq 1$ fixed. Let $A_k$ the set of 2l-uples of rational numbers $\alpha_1>\beta_1\ldots \alpha_l>\beta_l$ with $(\alpha_i-\beta_i)<\frac{1}{k}$ for $1\leq i\leq l.$ For $m\geq 1$ and ($\alpha_1,\ldots,\beta_l)\in A_k$, let $\Lambda(m,\alpha_1,\ldots,\beta_l)$ be the set of point $x\in M$ for which there is a splitting $F_x=F_1(x)\oplus \ldots \oplus F_l(x)$ with $dimF_j=n_j$ and $$exp(n\alpha_j)\Vert u\Vert \geq \Vert L_n(x)u\Vert \geq exp(n\beta_j)\Vert u\Vert $$  $$exp(-n\alpha_j)\Vert u\Vert \leq \Vert L_{-n}(x)u\Vert \leq exp(-n\beta_j)\Vert u\Vert $$ for all $n\geq m$, $1\leq j\leq l$ and $u\in F_j$. Then the set $\Lambda(m,\alpha_1,\ldots,\beta_l)$ is closed and $\Lambda(m,\alpha_1,\ldots,\beta_l) \ni x \rightarrow F_j(x)$ is contionuos for every $1\leq j\leq l$. This is where I have difficulty Let $(x_n)\subset \Lambda(m,\alpha_1,\ldots,\beta_l)$ such that $x_n \rightarrow y\in M$, then there $F_{x_n}=F_1(x_n)\oplus \ldots \oplus F_l(x_n)$ with $dimF_j(x_n)=n_j$ but as I can guarantee that $F_{x_n} \rightarrow F_{y}$? how to make the convergence of subspaces? thanks for any suggestions","I'm reviewing the proof of the theorem of oseledet the book  Mañe: Let $M$ a compact metric space and $f:M \rightarrow M$ a homeomorphism, $\pi: F \rightarrow M$ a finite-dimensional continuos vector bundle over $M$, endowed with a continuous Riemannian metric. Let $L: F \rightarrow F$ vector bundles covering $f$ (i.e. $\pi\circ L= f\circ \pi$) such that both $L$ and $L^{-1}$ have bounded normas. Denote by $L_n$ the n-th iterate of $L$  $$L_n(x)=L(f^{n-1}(x))\circ \ldots \circ L(f(x))\circ L(x) \ \ \ \mbox{if} \ \ n>0,$$ $$L_n(x)=L^{-1}(f^{-n+1}(x))\circ \ldots \circ L^{-1}(f^{-1}(x))\circ L^{-1}(x)\ \ \ \mbox{if} \ \ n<0.$$ For $n_1,\ldots, n_l\geq 1$ fixed. Let $A_k$ the set of 2l-uples of rational numbers $\alpha_1>\beta_1\ldots \alpha_l>\beta_l$ with $(\alpha_i-\beta_i)<\frac{1}{k}$ for $1\leq i\leq l.$ For $m\geq 1$ and ($\alpha_1,\ldots,\beta_l)\in A_k$, let $\Lambda(m,\alpha_1,\ldots,\beta_l)$ be the set of point $x\in M$ for which there is a splitting $F_x=F_1(x)\oplus \ldots \oplus F_l(x)$ with $dimF_j=n_j$ and $$exp(n\alpha_j)\Vert u\Vert \geq \Vert L_n(x)u\Vert \geq exp(n\beta_j)\Vert u\Vert $$  $$exp(-n\alpha_j)\Vert u\Vert \leq \Vert L_{-n}(x)u\Vert \leq exp(-n\beta_j)\Vert u\Vert $$ for all $n\geq m$, $1\leq j\leq l$ and $u\in F_j$. Then the set $\Lambda(m,\alpha_1,\ldots,\beta_l)$ is closed and $\Lambda(m,\alpha_1,\ldots,\beta_l) \ni x \rightarrow F_j(x)$ is contionuos for every $1\leq j\leq l$. This is where I have difficulty Let $(x_n)\subset \Lambda(m,\alpha_1,\ldots,\beta_l)$ such that $x_n \rightarrow y\in M$, then there $F_{x_n}=F_1(x_n)\oplus \ldots \oplus F_l(x_n)$ with $dimF_j(x_n)=n_j$ but as I can guarantee that $F_{x_n} \rightarrow F_{y}$? how to make the convergence of subspaces? thanks for any suggestions",,"['ordinary-differential-equations', 'ergodic-theory', 'geometric-measure-theory', 'compact-manifolds']"
58,Why don't power series methods work for linear ODE's with singularities?,Why don't power series methods work for linear ODE's with singularities?,,"My math class tells me power series methods don't work for equations of the form $$f'' +p(x)f' +q(x)f = 0$$ if the functions $p(x)$ or $q(x)$ have singularities at the point about which you're trying to expand. This creates a motivation for the Frobenius method, for equations of the form $$f'' + \frac{p(x)}{x}f' +\frac{q(x)}{x^2}f = 0$$ or rearranged: $$x^2f'' + xp(x) +q(x) = 0$$ Why won't regular power series methods work for equations of this form? If you generalize to a Laurent series, will the power series method work without using the Frobenius method? Any general answers about expanding about singular points would also be helpful.","My math class tells me power series methods don't work for equations of the form $$f'' +p(x)f' +q(x)f = 0$$ if the functions $p(x)$ or $q(x)$ have singularities at the point about which you're trying to expand. This creates a motivation for the Frobenius method, for equations of the form $$f'' + \frac{p(x)}{x}f' +\frac{q(x)}{x^2}f = 0$$ or rearranged: $$x^2f'' + xp(x) +q(x) = 0$$ Why won't regular power series methods work for equations of this form? If you generalize to a Laurent series, will the power series method work without using the Frobenius method? Any general answers about expanding about singular points would also be helpful.",,"['ordinary-differential-equations', 'power-series']"
59,Quadratic system of ODEs,Quadratic system of ODEs,,"I have a quadratic ODE system that looks like this:  $\dot{x}=Ax+diag(x)Nx$ where $x \in R^n$ and $A,N \in R^{n \times n}$ and $diag(x) \in R^{n \times n}$ is a diagonal matrix in which $x$ is its main diagonal.  Is there a known solution to this system? Any hints, articles, papers or books about this kind of systems would be appreciated. Thanks.","I have a quadratic ODE system that looks like this:  $\dot{x}=Ax+diag(x)Nx$ where $x \in R^n$ and $A,N \in R^{n \times n}$ and $diag(x) \in R^{n \times n}$ is a diagonal matrix in which $x$ is its main diagonal.  Is there a known solution to this system? Any hints, articles, papers or books about this kind of systems would be appreciated. Thanks.",,"['ordinary-differential-equations', 'nonlinear-system', 'quadratics']"
60,Solve the differential equation: $y''+(1-2x)y'+2x(e^{x^{2}}-1)y=xe^{x^{2}}$,Solve the differential equation:,y''+(1-2x)y'+2x(e^{x^{2}}-1)y=xe^{x^{2}},"I have an exercise of solving the equation above using MATLAB, with a hint to set $ v(x)=(y'+y)e^{-x^{2}} $. I tried to do with my hand and get the following: $ v'+2xy=x $, but I don't know what to do next, I'm stuck there. My teacher asked to not using the ""dsolve"" function, so I'm stuck with MATLAB too. Anyone can do this problem? Thank you so much.","I have an exercise of solving the equation above using MATLAB, with a hint to set $ v(x)=(y'+y)e^{-x^{2}} $. I tried to do with my hand and get the following: $ v'+2xy=x $, but I don't know what to do next, I'm stuck there. My teacher asked to not using the ""dsolve"" function, so I'm stuck with MATLAB too. Anyone can do this problem? Thank you so much.",,"['ordinary-differential-equations', 'matlab']"
61,One-parameter group for spheres,One-parameter group for spheres,,"Let $S^1$ be the unit sphere $x_1^2+x_2^2=1$ in $\mathbb{R}^2$ and let $X=S^1\times S^1\in\mathbb{R}^4$ with defining equations $f_1=x_1^2+x_2^2-1=0, f_2=x_3^2+x_4^2-1=0$. The vector field $$w=x_1\frac\partial{\partial x_2}-x_2\frac\partial{\partial x_1}+\lambda\left(x_4\frac\partial{\partial x_3}-x_3\frac\partial{\partial x_4}\right)$$ ($\lambda\in\mathbb{R}$) is tangent to $X$ and hence defines by restriction a vector field $v$ on $X$. What is the one-parameter group of diffeomorphisms that $v$ generates? The definition of a one-parameter group of diffeomorphisms that I'm using is the following: Let $U$ be an open subset of $\mathbb{R}^n$ and $F : U \times \mathbb{R} \rightarrow U$ a $C^{\infty}$ mapping. The family of mappings $f_t: U \rightarrow U$ , $f_t(x) = F(x, t)$   is said to be a one-parameter group of diffeomorphisms of $U$ if $f_0$ is the identity   map and $f_s \cdot f_t = f_{s+t}$   for all s and t. I solved to get that the integral curve is given by $$\gamma(t)=(\cos(t-a),\sin(t-a),\cos(b-\lambda t),\sin(b-\lambda t))$$ for arbitrary constants $a,b$. Given an arbitrary point $x=(\cos a,\sin a,\cos b,\sin b)\in S^1\times S^1$, I want to say that the one-parameter group is $$f_t(x)=(\cos(t-a),\sin(t-a),\cos(b-\lambda t),\sin(b-\lambda t))$$ But there is a problem: $f_0(x)=(\cos(-a),\sin(-a),\cos b,\sin b)=(\cos a,-\sin a,\cos b,\sin b)\neq x.$ What can I do here?","Let $S^1$ be the unit sphere $x_1^2+x_2^2=1$ in $\mathbb{R}^2$ and let $X=S^1\times S^1\in\mathbb{R}^4$ with defining equations $f_1=x_1^2+x_2^2-1=0, f_2=x_3^2+x_4^2-1=0$. The vector field $$w=x_1\frac\partial{\partial x_2}-x_2\frac\partial{\partial x_1}+\lambda\left(x_4\frac\partial{\partial x_3}-x_3\frac\partial{\partial x_4}\right)$$ ($\lambda\in\mathbb{R}$) is tangent to $X$ and hence defines by restriction a vector field $v$ on $X$. What is the one-parameter group of diffeomorphisms that $v$ generates? The definition of a one-parameter group of diffeomorphisms that I'm using is the following: Let $U$ be an open subset of $\mathbb{R}^n$ and $F : U \times \mathbb{R} \rightarrow U$ a $C^{\infty}$ mapping. The family of mappings $f_t: U \rightarrow U$ , $f_t(x) = F(x, t)$   is said to be a one-parameter group of diffeomorphisms of $U$ if $f_0$ is the identity   map and $f_s \cdot f_t = f_{s+t}$   for all s and t. I solved to get that the integral curve is given by $$\gamma(t)=(\cos(t-a),\sin(t-a),\cos(b-\lambda t),\sin(b-\lambda t))$$ for arbitrary constants $a,b$. Given an arbitrary point $x=(\cos a,\sin a,\cos b,\sin b)\in S^1\times S^1$, I want to say that the one-parameter group is $$f_t(x)=(\cos(t-a),\sin(t-a),\cos(b-\lambda t),\sin(b-\lambda t))$$ But there is a problem: $f_0(x)=(\cos(-a),\sin(-a),\cos b,\sin b)=(\cos a,-\sin a,\cos b,\sin b)\neq x.$ What can I do here?",,"['real-analysis', 'ordinary-differential-equations', 'dynamical-systems']"
62,Second order linear differential equation,Second order linear differential equation,,I have to teach the following methods to my juniors at college to solve differential equations: 1) partial fractions 2) reduction of order 3) variation of parameter 4) power series 5) green's function I was thinking of taking a non-trivial second order linear differential equation that can be solved by all the above methods. Please help me with some examples.,I have to teach the following methods to my juniors at college to solve differential equations: 1) partial fractions 2) reduction of order 3) variation of parameter 4) power series 5) green's function I was thinking of taking a non-trivial second order linear differential equation that can be solved by all the above methods. Please help me with some examples.,,"['ordinary-differential-equations', 'big-list']"
63,solving second order differential equation,solving second order differential equation,,"Bonsoir je cherche les solutions de l'équation differentielle de type $$x^3y''(x)+(ax^3+bx^2+cx+d)y(x) =0$$ Merci d'avance Good evening, I'm searching solutions of a differential equation of the type: $x^3y''(x) + (ax^3+bx^2+cx+d)y(x) = 0.$ Thanks is advance.","Bonsoir je cherche les solutions de l'équation differentielle de type $$x^3y''(x)+(ax^3+bx^2+cx+d)y(x) =0$$ Merci d'avance Good evening, I'm searching solutions of a differential equation of the type: $x^3y''(x) + (ax^3+bx^2+cx+d)y(x) = 0.$ Thanks is advance.",,['ordinary-differential-equations']
64,What Happens At An Equilibrium Point For An Autonomous First-Order Differential Equation.,What Happens At An Equilibrium Point For An Autonomous First-Order Differential Equation.,,"Let $\frac{dx}{dt} =f(x)$ be an autonomous first-order differential equation with equilibrium point at $x_0$. a) Suppose $f'(x_0) = 0$. What can you say about the behaviour of the solution near $x_0$. Give examples. b) Suppose $f'(x_0) = 0$ and $f''(x_0)\neq 0$. What can you say now? I attempted to do a. It looks like the behaviour at point $x_0$ could either be a sink/source at least from the examples I tried using. i.e. I used $f(x) = x^2, f(x) = x^3$ . Otherwise, I can't really think of a way to attempt this. Any help?","Let $\frac{dx}{dt} =f(x)$ be an autonomous first-order differential equation with equilibrium point at $x_0$. a) Suppose $f'(x_0) = 0$. What can you say about the behaviour of the solution near $x_0$. Give examples. b) Suppose $f'(x_0) = 0$ and $f''(x_0)\neq 0$. What can you say now? I attempted to do a. It looks like the behaviour at point $x_0$ could either be a sink/source at least from the examples I tried using. i.e. I used $f(x) = x^2, f(x) = x^3$ . Otherwise, I can't really think of a way to attempt this. Any help?",,['ordinary-differential-equations']
65,"Prove that all the solutions of (2): $\frac{dy}{dt}=A(t)y+f(t)$ are bounded in $ \left[t_0,+\infty \right )$",Prove that all the solutions of (2):  are bounded in,"\frac{dy}{dt}=A(t)y+f(t)  \left[t_0,+\infty \right )","I have a problem: Assume that system (1): $$\dfrac{dx}{dt}=A(t)x$$ is stable, where   $A(t) \in C\left [t_0,+\infty  \right )$, when $t \to \infty$ and $$\begin{cases}  & \mathrm{  } \lim \inf_{t \to \infty}\int_{t_0}^{t}Tr(A(t_1)dt_1> - \infty;(1') \\ \\  & \mathrm{  } \int_{t_0}^{\infty}\left \|f(t_1)  \right \|dt_1<+\infty; (2'). \end{cases}$$ Prove that all the solutions of (2): $$\frac{dy}{dt}=A(t)y+f(t)$$ are   bounded in $ \left[t_0,+\infty  \right )$. ........................................................................... Ok, Here's my solution: (I'm not sure, but I still post it). Assume that $X(t)$ be a fundamental matrix of (1), $X(t_0)=E$. Then $$y(t)=X(t)X^{-1}(t)x(t_0)+X(t)\int_{t_0}^{t}X^{-1}(s)f(s)\mathrm{ds}$$ Because system (1) is stable, so $X(t)$ is bounded Applying Ostrogradski - Liouville, we have:$$\det X(t)=\det E \cdot \exp \left[\int_{t_0}^{t} TrA(s)\mathrm{ds}\right]=\exp \left[\int_{t_0}^{t} TrA(s)\mathrm{ds}\right]$$ whence  $$\lim \inf_{t \to \infty}\int_{t_0}^{t}Tr(A(s)\mathrm{d}s=\lim \inf_{t \to \infty}\ln\det X(t)> - \infty$$ We see that, provided (1') is satisfied, $X^{-1}(t)$ is bounded as $t \to \infty$. Let $c_1=\max\{\sup_{t \ge t_0}\|X(t)\|;\sup_{t \ge t_0}\|X^{-1}(t)\|;\sup\|y(t_0)\|\}$. Therefore, $\|y(t)\| \le c_{1}^{3}+c_{1}^{2}\int_{t_0}^{t}\|f(s)\|\mathrm{d}s$. Since (2'): $\int_{t_0}^{\infty}\left \|f(t_1)  \right \|dt_1<+\infty$, we're done! I've made several errors with my solution, may be! :(. Anyone can check it help me? Thanks!","I have a problem: Assume that system (1): $$\dfrac{dx}{dt}=A(t)x$$ is stable, where   $A(t) \in C\left [t_0,+\infty  \right )$, when $t \to \infty$ and $$\begin{cases}  & \mathrm{  } \lim \inf_{t \to \infty}\int_{t_0}^{t}Tr(A(t_1)dt_1> - \infty;(1') \\ \\  & \mathrm{  } \int_{t_0}^{\infty}\left \|f(t_1)  \right \|dt_1<+\infty; (2'). \end{cases}$$ Prove that all the solutions of (2): $$\frac{dy}{dt}=A(t)y+f(t)$$ are   bounded in $ \left[t_0,+\infty  \right )$. ........................................................................... Ok, Here's my solution: (I'm not sure, but I still post it). Assume that $X(t)$ be a fundamental matrix of (1), $X(t_0)=E$. Then $$y(t)=X(t)X^{-1}(t)x(t_0)+X(t)\int_{t_0}^{t}X^{-1}(s)f(s)\mathrm{ds}$$ Because system (1) is stable, so $X(t)$ is bounded Applying Ostrogradski - Liouville, we have:$$\det X(t)=\det E \cdot \exp \left[\int_{t_0}^{t} TrA(s)\mathrm{ds}\right]=\exp \left[\int_{t_0}^{t} TrA(s)\mathrm{ds}\right]$$ whence  $$\lim \inf_{t \to \infty}\int_{t_0}^{t}Tr(A(s)\mathrm{d}s=\lim \inf_{t \to \infty}\ln\det X(t)> - \infty$$ We see that, provided (1') is satisfied, $X^{-1}(t)$ is bounded as $t \to \infty$. Let $c_1=\max\{\sup_{t \ge t_0}\|X(t)\|;\sup_{t \ge t_0}\|X^{-1}(t)\|;\sup\|y(t_0)\|\}$. Therefore, $\|y(t)\| \le c_{1}^{3}+c_{1}^{2}\int_{t_0}^{t}\|f(s)\|\mathrm{d}s$. Since (2'): $\int_{t_0}^{\infty}\left \|f(t_1)  \right \|dt_1<+\infty$, we're done! I've made several errors with my solution, may be! :(. Anyone can check it help me? Thanks!",,"['ordinary-differential-equations', 'control-theory', 'trace']"
66,Can Fredholm integral equation of the first type be represented as a differential equation?,Can Fredholm integral equation of the first type be represented as a differential equation?,,"Can Fredholm integral equation of the first type be represented as a differential equation? In other words, given a Fredholm integral equation of the second type does there exist a differential equation that has the same solution. More practically, is there some kind of transformation that would make this integral equation a differential equation?","Can Fredholm integral equation of the first type be represented as a differential equation? In other words, given a Fredholm integral equation of the second type does there exist a differential equation that has the same solution. More practically, is there some kind of transformation that would make this integral equation a differential equation?",,"['integration', 'ordinary-differential-equations']"
67,Singular solution to $(x+2y)y'=1$,Singular solution to,(x+2y)y'=1,"I have a problem and I got most of the solution, but don't understand how to proceed. The problem is to solve: $$(x+2y)y'=1, \qquad y(0)=-1.$$ Here is my reasoning: Substitute $z = x+2y$. Then $z'=1+2y'$. So $y'=\frac{z'-1}{2}$. So $\frac{z'-1}{2} = \frac{1}{z}$. So $z' = \frac{2+z}{z}$. So $\frac{dz}{dx}$ ($x$ taken without loss of generality) = $\frac{2+z}{z}$. So $\frac{zdz}{2+z} = dx$. So $z - 2\ln|z+2| = x+C$. Therefore,  $$(x+2y) - 2\ln|x+2y+2| = x+C.$$ Am I correct that there is no member of this one-parameter family of solutions that satisfies the initial condition $y(0)=-1$, because plugging in does not work? Also, is there a singular solution, and how to find it?? Thanks.","I have a problem and I got most of the solution, but don't understand how to proceed. The problem is to solve: $$(x+2y)y'=1, \qquad y(0)=-1.$$ Here is my reasoning: Substitute $z = x+2y$. Then $z'=1+2y'$. So $y'=\frac{z'-1}{2}$. So $\frac{z'-1}{2} = \frac{1}{z}$. So $z' = \frac{2+z}{z}$. So $\frac{dz}{dx}$ ($x$ taken without loss of generality) = $\frac{2+z}{z}$. So $\frac{zdz}{2+z} = dx$. So $z - 2\ln|z+2| = x+C$. Therefore,  $$(x+2y) - 2\ln|x+2y+2| = x+C.$$ Am I correct that there is no member of this one-parameter family of solutions that satisfies the initial condition $y(0)=-1$, because plugging in does not work? Also, is there a singular solution, and how to find it?? Thanks.",,['ordinary-differential-equations']
68,"Given independent solutions $f(x)$ and $g(x)$ of a linear homogeneous DE on $(a,b)$, which must also be solutions?","Given independent solutions  and  of a linear homogeneous DE on , which must also be solutions?","f(x) g(x) (a,b)","Given that $f(x)$ and $g(x)$ are independent solutions of a linear homogeneous differential equation on $(a, b)$, which of the following must also be solutions? A) 0 B) $2f(x)-3g(x)$ C) $f(x)g(x)$ D) both (A) and (B) E) both (B) and (C) My answer would be D, since any linear combination of independent solutions should also be a solution to the given equation, but the answer key states that only A, that is, the zero solution, is the correct one. Am I overlooking a subtlety in the wording of the question, or is D indeed the correct answer? I namely can't see why B wouldn't also be a solution. This question is straight from a GRE preparation book, by the way, and the answer key is from UCLA's GRE preparation course website.","Given that $f(x)$ and $g(x)$ are independent solutions of a linear homogeneous differential equation on $(a, b)$, which of the following must also be solutions? A) 0 B) $2f(x)-3g(x)$ C) $f(x)g(x)$ D) both (A) and (B) E) both (B) and (C) My answer would be D, since any linear combination of independent solutions should also be a solution to the given equation, but the answer key states that only A, that is, the zero solution, is the correct one. Am I overlooking a subtlety in the wording of the question, or is D indeed the correct answer? I namely can't see why B wouldn't also be a solution. This question is straight from a GRE preparation book, by the way, and the answer key is from UCLA's GRE preparation course website.",,['ordinary-differential-equations']
69,Simplest Schrödinger equation with both continuous and residual spectrum,Simplest Schrödinger equation with both continuous and residual spectrum,,"Consider a Schrödinger equation: $$-\frac{\text{d}^2}{\text{d}x^2}f(x)+U(x)f(x)=Ef(x),$$ I need a $U(x)$ satisfying the following: The Schrödinger equation with it must be solvable purely analytically, without need for any numerics (but using special functions, integrals or series is acceptable — the main point is that they must be explicit, not yet another equation to solve) $\displaystyle \lim_{x\to\infty} U(\pm x)=0$ $\exists a,b: U(x)<0\;\forall x\in[a,b]$ I.e. $U(x)$ should represent some potential well, which would have both free and bound states. Boundary conditions are imposed at some points $q$, $r$. Changing locations of these points shouldn't affect analytical solvability of the BVP. Are there any such $U(x)$? If yes, what are examples? Examples of what does not answer the question are: finite square potential well, because to solve it one has to solve transcendental equations, which need numerics $\delta$-shaped potential well, since despite it can be solved analytically for infinite space, it still results in transcendental equation when $x\in[q,r]$","Consider a Schrödinger equation: $$-\frac{\text{d}^2}{\text{d}x^2}f(x)+U(x)f(x)=Ef(x),$$ I need a $U(x)$ satisfying the following: The Schrödinger equation with it must be solvable purely analytically, without need for any numerics (but using special functions, integrals or series is acceptable — the main point is that they must be explicit, not yet another equation to solve) $\displaystyle \lim_{x\to\infty} U(\pm x)=0$ $\exists a,b: U(x)<0\;\forall x\in[a,b]$ I.e. $U(x)$ should represent some potential well, which would have both free and bound states. Boundary conditions are imposed at some points $q$, $r$. Changing locations of these points shouldn't affect analytical solvability of the BVP. Are there any such $U(x)$? If yes, what are examples? Examples of what does not answer the question are: finite square potential well, because to solve it one has to solve transcendental equations, which need numerics $\delta$-shaped potential well, since despite it can be solved analytically for infinite space, it still results in transcendental equation when $x\in[q,r]$",,"['ordinary-differential-equations', 'boundary-value-problem', 'eigenfunctions']"
70,Help me understand this differential equation solution,Help me understand this differential equation solution,,"I found a differential equation in an old paper, where the solution is a bit hard to understand. Given this equation: $$\frac{1}{2} r^2 \left(\frac{d \phi}{dr}\right)^2 + c^2 \left(r \frac{d\phi}{dr} + \phi\right) = 0$$ Multiplication with $\frac{8}{c^4}$ and substitution with $\xi := 2 \frac{\phi}{c^2}$ leads to: $$r^2 \left(\frac{d \xi}{d r}\right)^2 + 4 \left(r \frac{d \xi}{dr} + \xi\right)$$ Then he says with the new argument $\ln r$ and $\xi' = \frac{d \xi}{d(\ln r)}$ (I interpreted this as $\frac{d \xi}{d(\ln r)} = \frac{d \xi}{dr} \frac{d r}{d(\ln r)} = \frac{d \xi}{dr} r$) follows: $$\xi'^2 + 4(\xi' + \xi) = 0$$ And then he solves this quadratic equation to $$\xi' = -2\left(1 \pm \sqrt{1 - \xi}\right)$$ Now the part that where I am stuck. He says transformation with $Q' = \left(1 - \sqrt{1 + \xi}\right)$ results in $$d(Q' e^{-Q'}) = -d \ln r,$$ or integrated $$r Q' e^{-Q'} = C$$ And then he starts resubstituting $Q = ...$ and so on. But what kind of ""transformation"" is this? Can you add some intermediate steps?","I found a differential equation in an old paper, where the solution is a bit hard to understand. Given this equation: $$\frac{1}{2} r^2 \left(\frac{d \phi}{dr}\right)^2 + c^2 \left(r \frac{d\phi}{dr} + \phi\right) = 0$$ Multiplication with $\frac{8}{c^4}$ and substitution with $\xi := 2 \frac{\phi}{c^2}$ leads to: $$r^2 \left(\frac{d \xi}{d r}\right)^2 + 4 \left(r \frac{d \xi}{dr} + \xi\right)$$ Then he says with the new argument $\ln r$ and $\xi' = \frac{d \xi}{d(\ln r)}$ (I interpreted this as $\frac{d \xi}{d(\ln r)} = \frac{d \xi}{dr} \frac{d r}{d(\ln r)} = \frac{d \xi}{dr} r$) follows: $$\xi'^2 + 4(\xi' + \xi) = 0$$ And then he solves this quadratic equation to $$\xi' = -2\left(1 \pm \sqrt{1 - \xi}\right)$$ Now the part that where I am stuck. He says transformation with $Q' = \left(1 - \sqrt{1 + \xi}\right)$ results in $$d(Q' e^{-Q'}) = -d \ln r,$$ or integrated $$r Q' e^{-Q'} = C$$ And then he starts resubstituting $Q = ...$ and so on. But what kind of ""transformation"" is this? Can you add some intermediate steps?",,['ordinary-differential-equations']
71,Ordinary Differential Equations - Poincaré Bendixson,Ordinary Differential Equations - Poincaré Bendixson,,I would like to solve next problem: Let $X$ be a $C^1$ vector field in $\mathbb{R^2}$ end $\gamma$ an orbit of the $X$. Prove  that if  $\gamma$  is not a periodic orbit or a singularity then $\omega(\gamma)\cap\alpha(\gamma)=\emptyset$ or  $\omega(\gamma)\cap\alpha(\gamma)$ is a singular point.,I would like to solve next problem: Let $X$ be a $C^1$ vector field in $\mathbb{R^2}$ end $\gamma$ an orbit of the $X$. Prove  that if  $\gamma$  is not a periodic orbit or a singularity then $\omega(\gamma)\cap\alpha(\gamma)=\emptyset$ or  $\omega(\gamma)\cap\alpha(\gamma)$ is a singular point.,,['ordinary-differential-equations']
72,Perturbation Theory on Finite Domains,Perturbation Theory on Finite Domains,,"In this video (from 27.00 - 50.00, which you don't need to watch!) a guy shows how you can solve the general second order ode $y'' + P(x)y = 0$ using perturbation theory. However he points out that the domain must be finite in order for this to work, I'm wondering how you would phrase a question like this or how you would know when you are working with an infinite domain etc...? If I was just given $y'' + x^2y = 0$ & asked to solve it using perturbation theory I wouldn't know if I was getting into trouble about finite or infinite domains, how do I re-phrase the question so that it makes sense? Thanks for your time.","In this video (from 27.00 - 50.00, which you don't need to watch!) a guy shows how you can solve the general second order ode $y'' + P(x)y = 0$ using perturbation theory. However he points out that the domain must be finite in order for this to work, I'm wondering how you would phrase a question like this or how you would know when you are working with an infinite domain etc...? If I was just given $y'' + x^2y = 0$ & asked to solve it using perturbation theory I wouldn't know if I was getting into trouble about finite or infinite domains, how do I re-phrase the question so that it makes sense? Thanks for your time.",,['ordinary-differential-equations']
73,Nonlinear equation (oscillon) comparison,Nonlinear equation (oscillon) comparison,,"Lagrangian for a spherically-symmetric, real scalar field in d spatial dimensions, $$L=c_d \int r^{d-1}dr\left[ \frac{1}{2} \dot\phi^2 - \frac{1}{2}  \left(\frac{\partial \phi}{\partial r} \right)^2 -V(\phi)\right] \tag{1}$$ where $$v= m^2\phi^2$$,  $$c_d = 2π^{d/2} /Γ(d/2)$$ is the unit- sphere volume in d dimensions. The solution of $\phi$ is,  $$\phi(r,t) = A(t)P(r,R)= A(t)e^{\frac{−r^2} {R2}}\tag{2}$$ By applying these conditions the author in the article equations 11,14 shows how oscillons formed , but If I write a oscillon equation from the article equation 5 $$\phi= \sum_{k=1}^{\infty}\varepsilon^k \phi_k \tag{3}$$ , what is the fundamental difference between the two nonlinear equations for \phi. Can we get the solution like (3) from equation (2)?","Lagrangian for a spherically-symmetric, real scalar field in d spatial dimensions, $$L=c_d \int r^{d-1}dr\left[ \frac{1}{2} \dot\phi^2 - \frac{1}{2}  \left(\frac{\partial \phi}{\partial r} \right)^2 -V(\phi)\right] \tag{1}$$ where $$v= m^2\phi^2$$,  $$c_d = 2π^{d/2} /Γ(d/2)$$ is the unit- sphere volume in d dimensions. The solution of $\phi$ is,  $$\phi(r,t) = A(t)P(r,R)= A(t)e^{\frac{−r^2} {R2}}\tag{2}$$ By applying these conditions the author in the article equations 11,14 shows how oscillons formed , but If I write a oscillon equation from the article equation 5 $$\phi= \sum_{k=1}^{\infty}\varepsilon^k \phi_k \tag{3}$$ , what is the fundamental difference between the two nonlinear equations for \phi. Can we get the solution like (3) from equation (2)?",,['ordinary-differential-equations']
74,"Sturm-liouville problem, first eigenvalue","Sturm-liouville problem, first eigenvalue",,"Any idea to solve the Sturm-Liouville Problem  $$ -\cos^{2}(t)g''+n\sin(t)\cos(t)g'-(n+1)\cos^{2}(t)g=(\delta)g, $$ with $t\in[\epsilon,0]$, and boundary conditions $g(\epsilon)=g(0)=0$? We may rewrite the equation as $$ (\cos^{n}(t)g')'+(n+1)\cos^{n}(t)g+(\delta)\cos^{n-2}(t)g=0. $$ I wanna know the first eigenvalue. Thanks!","Any idea to solve the Sturm-Liouville Problem  $$ -\cos^{2}(t)g''+n\sin(t)\cos(t)g'-(n+1)\cos^{2}(t)g=(\delta)g, $$ with $t\in[\epsilon,0]$, and boundary conditions $g(\epsilon)=g(0)=0$? We may rewrite the equation as $$ (\cos^{n}(t)g')'+(n+1)\cos^{n}(t)g+(\delta)\cos^{n-2}(t)g=0. $$ I wanna know the first eigenvalue. Thanks!",,['ordinary-differential-equations']
75,Stability of limit cycle,Stability of limit cycle,,"What can be said about the stability of the limit cycle for $r=1$ of the equation   $$\dot{r}=(r^2-1)\cdot (2 r \cos(\phi) - 1), \dot{\phi}=1?$$ This is a problem posed in Arnol'd's book on ODEs. Does anybody have a good tool/matlab profram for visualizing solutions of this? Thanks!","What can be said about the stability of the limit cycle for $r=1$ of the equation   $$\dot{r}=(r^2-1)\cdot (2 r \cos(\phi) - 1), \dot{\phi}=1?$$ This is a problem posed in Arnol'd's book on ODEs. Does anybody have a good tool/matlab profram for visualizing solutions of this? Thanks!",,['ordinary-differential-equations']
76,Show that this orbit has a zero Lyapunov exponent,Show that this orbit has a zero Lyapunov exponent,,"I'm using J.Meiss -Differential dynamical systems, and have some trouble to understand a proof about Lyapunov exponents. We have a dynamical system  $$ \dot{x} = f(x), $$ with the corresponding flow $ \varphi_t $. Now suppose $ \varphi_t(x)  $ is a bounded orbit, and that the flow $ \varphi$ is not forward asymptotic to an equilibrium. Want to show that then it has a zero Lyapunov exponent, i.e $ \mu(x,v) = \limsup_{t \rightarrow \infty} \frac{1}{t} \ln |D_x\varphi_t(x)v| =  0 $. So this is what they do: Consider the vector $v(t) = f( \varphi_t(x)) $. Now $$ \frac{d}{dt}v(t) = \frac{d}{dt}f( \varphi_t(x)) = Df( \varphi_t(x))\frac{d}{dt} \varphi_t(x) = Df( \varphi_t(x)) f( \varphi_t(x)) =  Df( \varphi_t(x))v(t), $$ hence  $$ \dot{v} =  Df( \varphi_t(x)) v.$$  Thus $v$ is a solution to the linearization  of the system about this orbit with initial condition $f(x)$. Now they do some argument that I do not uderstand: First they say that since $ \varphi $ is bounded, $v$ is also bounded. Why is that?  Further they say that since $ \varphi_t$ is not asymptotic to an equilibrium $ \limsup |v(t)| > 0$. Why is that? And Therefore $ \mu(x,v) = 0$. Thank you for your help.","I'm using J.Meiss -Differential dynamical systems, and have some trouble to understand a proof about Lyapunov exponents. We have a dynamical system  $$ \dot{x} = f(x), $$ with the corresponding flow $ \varphi_t $. Now suppose $ \varphi_t(x)  $ is a bounded orbit, and that the flow $ \varphi$ is not forward asymptotic to an equilibrium. Want to show that then it has a zero Lyapunov exponent, i.e $ \mu(x,v) = \limsup_{t \rightarrow \infty} \frac{1}{t} \ln |D_x\varphi_t(x)v| =  0 $. So this is what they do: Consider the vector $v(t) = f( \varphi_t(x)) $. Now $$ \frac{d}{dt}v(t) = \frac{d}{dt}f( \varphi_t(x)) = Df( \varphi_t(x))\frac{d}{dt} \varphi_t(x) = Df( \varphi_t(x)) f( \varphi_t(x)) =  Df( \varphi_t(x))v(t), $$ hence  $$ \dot{v} =  Df( \varphi_t(x)) v.$$  Thus $v$ is a solution to the linearization  of the system about this orbit with initial condition $f(x)$. Now they do some argument that I do not uderstand: First they say that since $ \varphi $ is bounded, $v$ is also bounded. Why is that?  Further they say that since $ \varphi_t$ is not asymptotic to an equilibrium $ \limsup |v(t)| > 0$. Why is that? And Therefore $ \mu(x,v) = 0$. Thank you for your help.",,"['ordinary-differential-equations', 'dynamical-systems']"
77,Hodograph transformation and implicit solution of a non-linear PDE,Hodograph transformation and implicit solution of a non-linear PDE,,"I am trying to understand how can one apply the Hodograph transformation to a non-linear PDE. I read that this transformation implies the representation of the solution in the implicit form . So, if I have a function $f(x,t)$ which is defined by a certain non-linear PDE, by applying the above mentioned transformation, my equation will become an equation for the function $x(f,t)$ and the solution of this equation will represent the implicit solution of my original equation. I don't really understand the concept of implicit solution and why would I want such a solution , when I'm interested in finding  $f(x,t)$ and not   $x(f,t)$. Can one get from the solution for  $x(f,t)$ to the solution for $f(x,t)$? I have the feeling this is a very silly question, but if someone could help me with this problem I would really appreciate it.","I am trying to understand how can one apply the Hodograph transformation to a non-linear PDE. I read that this transformation implies the representation of the solution in the implicit form . So, if I have a function $f(x,t)$ which is defined by a certain non-linear PDE, by applying the above mentioned transformation, my equation will become an equation for the function $x(f,t)$ and the solution of this equation will represent the implicit solution of my original equation. I don't really understand the concept of implicit solution and why would I want such a solution , when I'm interested in finding  $f(x,t)$ and not   $x(f,t)$. Can one get from the solution for  $x(f,t)$ to the solution for $f(x,t)$? I have the feeling this is a very silly question, but if someone could help me with this problem I would really appreciate it.",,"['ordinary-differential-equations', 'partial-differential-equations', 'transformation']"
78,Analog of Picard's theorem for Fractional Differential equations.,Analog of Picard's theorem for Fractional Differential equations.,,I need an analog of Picard's theorem of existence and uniqueness of solutions. The theorem is to be applied to linear fractional order differential equations with constants coefficients.  I don't want the more general result because I need a 'simple' proof of the theorem. I have not found such result. Any idea?  Any type of help would be very appreciated.,I need an analog of Picard's theorem of existence and uniqueness of solutions. The theorem is to be applied to linear fractional order differential equations with constants coefficients.  I don't want the more general result because I need a 'simple' proof of the theorem. I have not found such result. Any idea?  Any type of help would be very appreciated.,,"['ordinary-differential-equations', 'fractional-calculus']"
79,Continuity of the inverse Laplace Transform,Continuity of the inverse Laplace Transform,,"If I know $Y(s)$, can I predict when $\mathscr{L}^{-1}[Y(s)]=y(t)$ will be continuous or continuously differentiable or even stronger conditions? For example; I'm solving an ODE with the Laplace Tranform method so it turns out that the solution is $\mathscr{L}^{-1}[Y(s)]=y(t)$. How do I know that that function will be countinuously differentiable up to the order of the ODE?","If I know $Y(s)$, can I predict when $\mathscr{L}^{-1}[Y(s)]=y(t)$ will be continuous or continuously differentiable or even stronger conditions? For example; I'm solving an ODE with the Laplace Tranform method so it turns out that the solution is $\mathscr{L}^{-1}[Y(s)]=y(t)$. How do I know that that function will be countinuously differentiable up to the order of the ODE?",,"['calculus', 'ordinary-differential-equations', 'continuity', 'laplace-transform', 'integral-transforms']"
80,Bifurcation in 3 dimensions (simple),Bifurcation in 3 dimensions (simple),,"I am Doing a project i have a toy system that describes a bifurcation in 3 dimensions i am posting this in part because i can no longer understand what i have written down ( its been awhile) i have drawn a horrible picture attached below of what the stage's looks like. Now onto the interesting bit. Some translation of variables that may help in no particular order as well as some useful information about the system. (r,0,0) on $\sum_{1}$ corresponds to the homo-clinic orbit. The surface $\sum_{1}$ is defined in such a way that each ode will only intersect this surface once and that it must intersect this surface at least once. $\nu_{n+1}/\nu_{n} = -e^{-np/w}$ An Aproximation of the flow between the 2 surfaces $\sum_{1} \to \sum_{0}$ is governed by the following differential equations. $x^{'}=-px+wy$ $y^{'}=-py-wx$ $z^{'}=\lambda z$ $\theta^{'}=w$ $r^{'}=-p$ The basic idea was to fiddle with some parameters $\sum_{0} \to \sum_{1}$ an look at bifurcations. $\begin{pmatrix} x \\ y\\  \end{pmatrix}^{'}= A \begin{pmatrix} x \\ y\\  \end{pmatrix} + \begin{pmatrix} r \\ 0\\  \end{pmatrix} + \begin{pmatrix} a\nu \\ b\nu\\  \end{pmatrix}$ Where $\nu$ is our bifurcation parameter. First we wanted to build a mapping matrix A. Staring by setting out intial conditions we have $\theta_{0}=0$ and $r_{0}=x_{0}$ and $z_{0}=z_{0}$ $z(t)=z_{0} e^{\lambda t}$ $r(t)=x_{0} e^{-p t}$ $\theta(t)=wt$ $z(T)=h$ $ T= 1/(\lambda) \log (h/z_{0})$ $x=x_{0}e^{-p \log(h/z_{0})/ \lambda} \cos(w \log(h/z_{0})/ \lambda)$ $y=x_{0}e^{-p \log(h/z_{0})/ \lambda} \sin(w \log(h/z_{0})/ \lambda)$ $x=x_{0}(z_{0}/h)^{p/ \lambda} \cos(w T)$ $y=x_{0}(z_{0}/h)^{p/ \lambda} \sin(w T)$ $z_{new}=F(z_{old})$ $\begin{pmatrix} x \\ z\\  \end{pmatrix}_{new} = C_{1} \begin{pmatrix} a&b \\ c&d\\  \end{pmatrix} \begin{pmatrix} \cos(w T) \\ \sin(w T)\\  \end{pmatrix} + \begin{pmatrix} r \\ 0\\  \end{pmatrix} + \begin{pmatrix} a\nu \\ b\nu\\  \end{pmatrix}$ Where $C_{1}=x_{0}(z_{0}/h)^{\delta}$ and $\delta=(p/\lambda)$ This step im a bit lost on i believe that  the K is just some correctional value for the c,d and coupled with the phase shift identity to put this as all one equation which is fine but i need to know exactly what it is to run iterations. $z_{new} = K z_{0}^{\delta} \cos(wT- \alpha) +b \nu$ i dont think i expanded this right but... $z_{new}=x_{0}(z_{0}/h)^{p/\lambda} (c(\cos((wlog(h/z_{0})/\lambda)) +d(\sin((wlog(h/z_{0})/\lambda) +b\nu$ but i dont know what c and d are so i moved back to $z_{new} = x_{0}(z_{0}/h)^{p/\lambda} z_{0}^{p/\lambda} \cos(wT- \alpha) +b \nu$ i think i am looking at (p/\lamda) >1 which if we assume from here it was very easy to understand the periodic values when this equaled 0 I have never seen anything in 3d and only one example of a bifurcation in 2-Dimensions but my intuition tells me that the important value of this bifurcation in the way its setup are going to be when $b\nu=0$ i believe without proof of course that this will be where all the fun happens  im not sure where i defined it but in case i hadn't but ill look at $b>0$ and where $nu \in R$  Intuition tells me that when $\nu>0$ we will get many fp in fact as this appear to be like a 1 dimensional analog i think we will get 2 bifurcation points generated on certain values of $n\pi/2$ and that each of these will be saddle and node bifurcations. again i also believe that when $\nu<0$ we will have only 1 fp in the system ( anything starting at z=0 will end up at (0,0,0) ) this will be the most boring case when the unstable manifold on the z axis has too much strength and will merely tangent all solutions off to +/- infinity  depending on whether u have + or - z values of course. As shown in the picture below. i believe the homo clinic orbit corresponds to $z_{new}=0 $ clearly if we pick $x_{0}=0$ ( $y_{0}=0$ already ) any flow of the solution $z_{new}=x_{0}(z_{0}/h)^{p/\lambda} (c(\cos((wlog(h/z_{0})/\lambda)) +d(\sin((wlog(h/z_{0})/\lambda) +b\nu$ when  yields $z_{new}= 0 +b\nu$ and this is true for all initial values $z_{0}$ remove $\lambda=0$ i believe that when $\nu=0$ as well we will have our homo clinic orbit. i) My question is Can someone help verify that this is whats going on? ii) help me translate where im at to the point that i can put this mapping into a computer and run some iterations. I would like to find a stable and unstable fp in the $b>0$ and $\nu>0$ case  and verify the homo-clinical orbit and lastly if possible find a closed orbit inside of this region. I have very little understanding of computers so not only do i need a means of solving for the ode but i would also like to know what software i should put it into that's as simple as possible to use. much of this could be completely wrong my prof explained it to me in about 30 minutes.  Any comments or confusion on the notation is welcome but please be as specific and obvious as possible i am physics undergrad and am liable to not understand anything you say at any time. EDIT: For those that are looking at this with 2 heads its kind of something like $x^{'}= y $ $y^{'}=-x -(y/3)$ $z^{'}=z - (x^{2}+y^{2})$ This Matrix A would be the linearization of our non linear system and would corresponding to a spiral sink in the x-y plane and an unstable manifold on the z axis you then add some parameter on z so that as z gets large all of a sudden z gets overwhelmed by the spiral sink. $A=\begin{pmatrix} 0&1&0 \\ -1&-(1/3)&0\\ 0&0&1\\ \end{pmatrix}$ this may attain an orbit like the one described below the one used and explained is toy system with several assumptions to make it easier to work with.","I am Doing a project i have a toy system that describes a bifurcation in 3 dimensions i am posting this in part because i can no longer understand what i have written down ( its been awhile) i have drawn a horrible picture attached below of what the stage's looks like. Now onto the interesting bit. Some translation of variables that may help in no particular order as well as some useful information about the system. (r,0,0) on $\sum_{1}$ corresponds to the homo-clinic orbit. The surface $\sum_{1}$ is defined in such a way that each ode will only intersect this surface once and that it must intersect this surface at least once. $\nu_{n+1}/\nu_{n} = -e^{-np/w}$ An Aproximation of the flow between the 2 surfaces $\sum_{1} \to \sum_{0}$ is governed by the following differential equations. $x^{'}=-px+wy$ $y^{'}=-py-wx$ $z^{'}=\lambda z$ $\theta^{'}=w$ $r^{'}=-p$ The basic idea was to fiddle with some parameters $\sum_{0} \to \sum_{1}$ an look at bifurcations. $\begin{pmatrix} x \\ y\\  \end{pmatrix}^{'}= A \begin{pmatrix} x \\ y\\  \end{pmatrix} + \begin{pmatrix} r \\ 0\\  \end{pmatrix} + \begin{pmatrix} a\nu \\ b\nu\\  \end{pmatrix}$ Where $\nu$ is our bifurcation parameter. First we wanted to build a mapping matrix A. Staring by setting out intial conditions we have $\theta_{0}=0$ and $r_{0}=x_{0}$ and $z_{0}=z_{0}$ $z(t)=z_{0} e^{\lambda t}$ $r(t)=x_{0} e^{-p t}$ $\theta(t)=wt$ $z(T)=h$ $ T= 1/(\lambda) \log (h/z_{0})$ $x=x_{0}e^{-p \log(h/z_{0})/ \lambda} \cos(w \log(h/z_{0})/ \lambda)$ $y=x_{0}e^{-p \log(h/z_{0})/ \lambda} \sin(w \log(h/z_{0})/ \lambda)$ $x=x_{0}(z_{0}/h)^{p/ \lambda} \cos(w T)$ $y=x_{0}(z_{0}/h)^{p/ \lambda} \sin(w T)$ $z_{new}=F(z_{old})$ $\begin{pmatrix} x \\ z\\  \end{pmatrix}_{new} = C_{1} \begin{pmatrix} a&b \\ c&d\\  \end{pmatrix} \begin{pmatrix} \cos(w T) \\ \sin(w T)\\  \end{pmatrix} + \begin{pmatrix} r \\ 0\\  \end{pmatrix} + \begin{pmatrix} a\nu \\ b\nu\\  \end{pmatrix}$ Where $C_{1}=x_{0}(z_{0}/h)^{\delta}$ and $\delta=(p/\lambda)$ This step im a bit lost on i believe that  the K is just some correctional value for the c,d and coupled with the phase shift identity to put this as all one equation which is fine but i need to know exactly what it is to run iterations. $z_{new} = K z_{0}^{\delta} \cos(wT- \alpha) +b \nu$ i dont think i expanded this right but... $z_{new}=x_{0}(z_{0}/h)^{p/\lambda} (c(\cos((wlog(h/z_{0})/\lambda)) +d(\sin((wlog(h/z_{0})/\lambda) +b\nu$ but i dont know what c and d are so i moved back to $z_{new} = x_{0}(z_{0}/h)^{p/\lambda} z_{0}^{p/\lambda} \cos(wT- \alpha) +b \nu$ i think i am looking at (p/\lamda) >1 which if we assume from here it was very easy to understand the periodic values when this equaled 0 I have never seen anything in 3d and only one example of a bifurcation in 2-Dimensions but my intuition tells me that the important value of this bifurcation in the way its setup are going to be when $b\nu=0$ i believe without proof of course that this will be where all the fun happens  im not sure where i defined it but in case i hadn't but ill look at $b>0$ and where $nu \in R$  Intuition tells me that when $\nu>0$ we will get many fp in fact as this appear to be like a 1 dimensional analog i think we will get 2 bifurcation points generated on certain values of $n\pi/2$ and that each of these will be saddle and node bifurcations. again i also believe that when $\nu<0$ we will have only 1 fp in the system ( anything starting at z=0 will end up at (0,0,0) ) this will be the most boring case when the unstable manifold on the z axis has too much strength and will merely tangent all solutions off to +/- infinity  depending on whether u have + or - z values of course. As shown in the picture below. i believe the homo clinic orbit corresponds to $z_{new}=0 $ clearly if we pick $x_{0}=0$ ( $y_{0}=0$ already ) any flow of the solution $z_{new}=x_{0}(z_{0}/h)^{p/\lambda} (c(\cos((wlog(h/z_{0})/\lambda)) +d(\sin((wlog(h/z_{0})/\lambda) +b\nu$ when  yields $z_{new}= 0 +b\nu$ and this is true for all initial values $z_{0}$ remove $\lambda=0$ i believe that when $\nu=0$ as well we will have our homo clinic orbit. i) My question is Can someone help verify that this is whats going on? ii) help me translate where im at to the point that i can put this mapping into a computer and run some iterations. I would like to find a stable and unstable fp in the $b>0$ and $\nu>0$ case  and verify the homo-clinical orbit and lastly if possible find a closed orbit inside of this region. I have very little understanding of computers so not only do i need a means of solving for the ode but i would also like to know what software i should put it into that's as simple as possible to use. much of this could be completely wrong my prof explained it to me in about 30 minutes.  Any comments or confusion on the notation is welcome but please be as specific and obvious as possible i am physics undergrad and am liable to not understand anything you say at any time. EDIT: For those that are looking at this with 2 heads its kind of something like $x^{'}= y $ $y^{'}=-x -(y/3)$ $z^{'}=z - (x^{2}+y^{2})$ This Matrix A would be the linearization of our non linear system and would corresponding to a spiral sink in the x-y plane and an unstable manifold on the z axis you then add some parameter on z so that as z gets large all of a sudden z gets overwhelmed by the spiral sink. $A=\begin{pmatrix} 0&1&0 \\ -1&-(1/3)&0\\ 0&0&1\\ \end{pmatrix}$ this may attain an orbit like the one described below the one used and explained is toy system with several assumptions to make it easier to work with.",,"['ordinary-differential-equations', 'dynamical-systems']"
81,Existence Theorem for Geodesics,Existence Theorem for Geodesics,,"The text I am reading now defined geodesics to be those curves that satisfy the following differential equation: $\ddot{\gamma}^k(t)+\dot{\gamma}^i(t)\dot{\gamma}^j(t)\Gamma^k_{ij}(\gamma(t)) = 0$ where the $\Gamma^k_{ij}$ are the Christoffel symbols. The text says that through any fixed point in the manifold we can find a unique geodesic with a given velocity vector at that point. The text appeals to the existence and uniqueness theorem for ODEs to support this fact. I get that the usual approach would be to make a substitution like $\nu = \dot{\gamma}$ and then apply the theorem, but this technique doesn't seem to get me anywhere since $\Gamma_{ij}^k(\gamma(t))$ depends on $\gamma$. Can someone help me clear up this confusion?","The text I am reading now defined geodesics to be those curves that satisfy the following differential equation: $\ddot{\gamma}^k(t)+\dot{\gamma}^i(t)\dot{\gamma}^j(t)\Gamma^k_{ij}(\gamma(t)) = 0$ where the $\Gamma^k_{ij}$ are the Christoffel symbols. The text says that through any fixed point in the manifold we can find a unique geodesic with a given velocity vector at that point. The text appeals to the existence and uniqueness theorem for ODEs to support this fact. I get that the usual approach would be to make a substitution like $\nu = \dot{\gamma}$ and then apply the theorem, but this technique doesn't seem to get me anywhere since $\Gamma_{ij}^k(\gamma(t))$ depends on $\gamma$. Can someone help me clear up this confusion?",,"['ordinary-differential-equations', 'differential-geometry']"
82,Method of isoclines,Method of isoclines,,"I have this exercise and I do not know how to solve it. By using the method of isoclines represent the integrals of equation corbes nonautonomous $x'=x^2-t$. There are some indications: Let $P = I_0$, the parabola $x^2 = t$. Show that $P^-$ is a trapping region ($P = P^+\cup P^-$). How can we prove it?","I have this exercise and I do not know how to solve it. By using the method of isoclines represent the integrals of equation corbes nonautonomous $x'=x^2-t$. There are some indications: Let $P = I_0$, the parabola $x^2 = t$. Show that $P^-$ is a trapping region ($P = P^+\cup P^-$). How can we prove it?",,"['real-analysis', 'analysis', 'ordinary-differential-equations', 'dynamical-systems']"
83,Solving inhomogenous bessel equation,Solving inhomogenous bessel equation,,I have the following differential equation to be solved $\dfrac{d^2\psi}{dr^2}+\dfrac{d\psi}{rdr}+4\left(\omega^2-k_0^2-\dfrac{n^2}{r^2}\right)\psi=AJ_n^2(kr)+\dfrac{k}{r}J_n(kr)J_{n+1}(kr)-\omega k^2J_{n+1}^2(kr)$ where $A$ is a constant and $k=\sqrt{\omega^2-k_0^2}$. Can you please let me know the approach to the problem?,I have the following differential equation to be solved $\dfrac{d^2\psi}{dr^2}+\dfrac{d\psi}{rdr}+4\left(\omega^2-k_0^2-\dfrac{n^2}{r^2}\right)\psi=AJ_n^2(kr)+\dfrac{k}{r}J_n(kr)J_{n+1}(kr)-\omega k^2J_{n+1}^2(kr)$ where $A$ is a constant and $k=\sqrt{\omega^2-k_0^2}$. Can you please let me know the approach to the problem?,,"['ordinary-differential-equations', 'special-functions', 'mathematical-physics']"
84,IVP Perturbation With Small Non-Linear Term,IVP Perturbation With Small Non-Linear Term,,"EDIT:  Sorry to bump this without having anything extra to add, but I still cannot reconcile my solution with what was asked (in (2)).  Could someone with expertise in this subject take a look?  I would definitely appreciate it.  In particular, either my approximation derived in the majority of my solution is incorrect, or, in the last paragraph or so, my methodology for determining the center of oscillation is flawed.  I've triple checked everything though, and I'm not sure what the issue is.  Perhaps it was a typo in the original problem statement from the text? (BACKGROUND) I'm asked to do two things with the IVP $\ddot{x}+x+\epsilon x^{2}=0$ where $0<\epsilon\ll 1$ and $x(0)=a>0$ and $\dot{x}(0)=0$. (1) Find an approximate solution of accuracy $O(\epsilon)$, valid for all time. (2) Show the center of oscillation is approximately $\epsilon\frac{a^2}{2}$. I first observe that if a simple $O(\epsilon)$ approximation is to be valid for all time, then the only real possibility for this to happen is if the solution to be approximated is periodic.  To that end, I use the Poincaré-Leindstedt perturbation method, since if the system has very strict requirements on initial conditions for periodic solutions, the two parameters given by this method (as opposed to one given by regular perturbations) will allow me to obtain them (i.e. when it comes time to eliminate resonance terms). EDIT: It turns out that there is no restriction on $a$ for periodic solutions, but this is only discoverable after proceeding with the method. (ACTUAL QUESTION) I am pretty confident that my approximation is correct (i.e. that I correctly applied the method).  But I have doubts, primarily because in (2) I get a different (though similar) answer for the center of oscillation.  So either my methodology for determining it is wrong, or my approximation is wrong.  So I included both stages here. (SOLUTION ATTEMPT) Our equation is $\ddot{x}+x+\epsilon x^{2}=0$ subject to initial conditions $x(0)=a$ and $\dot{x}(0)=0$.  Note that because the system is autonomous, the initial condition on $x'(t)$ is completely general assuming the solution oscillates indefinitely (which are the solution(s) we obtain by letting $a$ be yet undertermined), for in this case $\dot{x}(t)=0$ for infinitely many $t$, and being autonomous, the solution is unchanged by the translation $t\mapsto t-t_{0}$.  Introduce a yet to be determined time dilation (or equivalently, frequency) $$ \tau=\omega t. $$ Expanding $x$ and $\omega$ asymptotically gives \begin{eqnarray*} x(\tau)=x_{0}+\epsilon x_{1}+O(\epsilon^{2})\\ \omega=\omega_{0}+\epsilon\omega_{1}+O(\epsilon^{2}). \end{eqnarray*} Substituting the asymptotic expansion for $x$ into the equation and immediately discarding $O(\epsilon^{2})$ and higher order terms yields $$ \omega^{2}x_{0}''+\epsilon\omega^{2}x_{1}''+x_{0}+\epsilon x_{1}+\epsilon x_{0}^{2}=0, $$ where $'$ denotes differentiation with respect to $\tau$.  Substituting the asymptotic expansion for $\omega$ into this expression and ignoring $O(\epsilon^{2})$ and smaller terms then gives $$ \omega_{0}^{2}x_{0}''+2\omega_{0}\omega_{1}\epsilon x_{0}''+\epsilon\omega_{0}^{2}x_{1}''+x_{0}+\epsilon x_{1}+\epsilon x_{0}^{2}=0. $$ Collecting like-order terms forms the system of equations \begin{eqnarray*} (1)\;\;\omega_{0}^{2}x_{0}''+x_{0}=0,\\ (2)\;\;\omega_{0}^{2}x_{1}''+2\omega_{0}\omega_{1}x_{0}''+x_{0}^{2}+x_{1}=0. \end{eqnarray*} (1) implies $\omega_{0}=1$ and $x_{0}=A\cos\tau+B\sin\tau$.  The initial conditions on $x$ imply $x_{0}(0)=a$ and $x_{k}(0)=x_{k}'(0)=0$.  Hence, $$ x_{0}=a\cos\tau. $$ Substituting $\omega_{0}$ and $x_{0}$ into (2) and rearranging gives $$ x_{1}''+x_{1}=2\omega_{1}a\cos\tau-a^{2}\cos^{2}\tau=2\omega_{1}a\cos\tau-\frac{a^{2}}{2}-\frac{a^{2}}{2}\cos2\tau=f(\tau). $$ To eliminate resonance terms and obtain a periodic solution, we only require $$ 2\omega_{1}a=0, $$ and this implies $\omega_{1}=0$.  This means that $a$ may be regarded as a free parameter for the system, and so there are infinitely many initial conditions which lead to periodic solutions (note that we have only computed up to $O(\epsilon)$ order terms; presumably, eliminating resonance terms from $x_{k}$ ($k>1$) would require some dependence on $a$ for $w_{k}$).  At any rate, our equation for $x_{1}$ then becomes $$ x_{1}''+x_{1}=-a^{2}\cos2\tau. $$ Solving this equation (using variation of parameters or some other method to obtain a particular solution) with homogeneous initial conditions gives $$ x_{1}(\tau)=-\frac{2}{3}a^{2}\sin^{2}\left(\frac{\tau}{2}\right)\left(2+\cos\tau\right). $$ We then form our first order approximation to $x(t)$ by substituting these solutions into our asymptotic expansion for $x$: \begin{eqnarray*} x(t)&=&a\cos\omega t-\epsilon\frac{2}{3}a^{2}\sin^{2}\left(\frac{\omega t}{2}\right)\left(2+\cos\omega t\right)+O(\epsilon^{2})\\ &=&a\cos\omega t +\epsilon\frac{a^{2}}{6}(3-2\cos\omega t-\cos2\omega t)+O(\epsilon^{2}), \end{eqnarray*} where $\omega=1+O(\epsilon^{2})$ and the simplifying identity $\sin^{2}(\frac{\tau}{2})=\frac{1}{2}(1-\cos\tau)$ was used.  As for the center of oscillation, we expect it to occur at approximately $\frac{T}{4}$, where $T$ is the period of oscillation, since the oscillation begins at maximal amplitude for $t=0$ (see initial conditions).  The period of oscillation is $$ T=\frac{2\pi}{\omega} $$ so that evaluating our asymptotic expansion at $\frac{T}{4}=\frac{\pi}{2\omega}$ gives \begin{eqnarray*} x\left(\frac{\pi}{2\omega}\right) &=&a\cos\frac{\pi}{2}+\epsilon\frac{a^{2}}{6}\left(3-2\cos\frac{\pi}{2}-\cos\pi\right)\\ &=& 0+\epsilon\frac{a^2}{6}\left(3-0-(-1)\right)\\ &=& \epsilon\frac{2a^{2}}{3} \end{eqnarray*} which is accurate up to $O(\epsilon^{2})$.","EDIT:  Sorry to bump this without having anything extra to add, but I still cannot reconcile my solution with what was asked (in (2)).  Could someone with expertise in this subject take a look?  I would definitely appreciate it.  In particular, either my approximation derived in the majority of my solution is incorrect, or, in the last paragraph or so, my methodology for determining the center of oscillation is flawed.  I've triple checked everything though, and I'm not sure what the issue is.  Perhaps it was a typo in the original problem statement from the text? (BACKGROUND) I'm asked to do two things with the IVP $\ddot{x}+x+\epsilon x^{2}=0$ where $0<\epsilon\ll 1$ and $x(0)=a>0$ and $\dot{x}(0)=0$. (1) Find an approximate solution of accuracy $O(\epsilon)$, valid for all time. (2) Show the center of oscillation is approximately $\epsilon\frac{a^2}{2}$. I first observe that if a simple $O(\epsilon)$ approximation is to be valid for all time, then the only real possibility for this to happen is if the solution to be approximated is periodic.  To that end, I use the Poincaré-Leindstedt perturbation method, since if the system has very strict requirements on initial conditions for periodic solutions, the two parameters given by this method (as opposed to one given by regular perturbations) will allow me to obtain them (i.e. when it comes time to eliminate resonance terms). EDIT: It turns out that there is no restriction on $a$ for periodic solutions, but this is only discoverable after proceeding with the method. (ACTUAL QUESTION) I am pretty confident that my approximation is correct (i.e. that I correctly applied the method).  But I have doubts, primarily because in (2) I get a different (though similar) answer for the center of oscillation.  So either my methodology for determining it is wrong, or my approximation is wrong.  So I included both stages here. (SOLUTION ATTEMPT) Our equation is $\ddot{x}+x+\epsilon x^{2}=0$ subject to initial conditions $x(0)=a$ and $\dot{x}(0)=0$.  Note that because the system is autonomous, the initial condition on $x'(t)$ is completely general assuming the solution oscillates indefinitely (which are the solution(s) we obtain by letting $a$ be yet undertermined), for in this case $\dot{x}(t)=0$ for infinitely many $t$, and being autonomous, the solution is unchanged by the translation $t\mapsto t-t_{0}$.  Introduce a yet to be determined time dilation (or equivalently, frequency) $$ \tau=\omega t. $$ Expanding $x$ and $\omega$ asymptotically gives \begin{eqnarray*} x(\tau)=x_{0}+\epsilon x_{1}+O(\epsilon^{2})\\ \omega=\omega_{0}+\epsilon\omega_{1}+O(\epsilon^{2}). \end{eqnarray*} Substituting the asymptotic expansion for $x$ into the equation and immediately discarding $O(\epsilon^{2})$ and higher order terms yields $$ \omega^{2}x_{0}''+\epsilon\omega^{2}x_{1}''+x_{0}+\epsilon x_{1}+\epsilon x_{0}^{2}=0, $$ where $'$ denotes differentiation with respect to $\tau$.  Substituting the asymptotic expansion for $\omega$ into this expression and ignoring $O(\epsilon^{2})$ and smaller terms then gives $$ \omega_{0}^{2}x_{0}''+2\omega_{0}\omega_{1}\epsilon x_{0}''+\epsilon\omega_{0}^{2}x_{1}''+x_{0}+\epsilon x_{1}+\epsilon x_{0}^{2}=0. $$ Collecting like-order terms forms the system of equations \begin{eqnarray*} (1)\;\;\omega_{0}^{2}x_{0}''+x_{0}=0,\\ (2)\;\;\omega_{0}^{2}x_{1}''+2\omega_{0}\omega_{1}x_{0}''+x_{0}^{2}+x_{1}=0. \end{eqnarray*} (1) implies $\omega_{0}=1$ and $x_{0}=A\cos\tau+B\sin\tau$.  The initial conditions on $x$ imply $x_{0}(0)=a$ and $x_{k}(0)=x_{k}'(0)=0$.  Hence, $$ x_{0}=a\cos\tau. $$ Substituting $\omega_{0}$ and $x_{0}$ into (2) and rearranging gives $$ x_{1}''+x_{1}=2\omega_{1}a\cos\tau-a^{2}\cos^{2}\tau=2\omega_{1}a\cos\tau-\frac{a^{2}}{2}-\frac{a^{2}}{2}\cos2\tau=f(\tau). $$ To eliminate resonance terms and obtain a periodic solution, we only require $$ 2\omega_{1}a=0, $$ and this implies $\omega_{1}=0$.  This means that $a$ may be regarded as a free parameter for the system, and so there are infinitely many initial conditions which lead to periodic solutions (note that we have only computed up to $O(\epsilon)$ order terms; presumably, eliminating resonance terms from $x_{k}$ ($k>1$) would require some dependence on $a$ for $w_{k}$).  At any rate, our equation for $x_{1}$ then becomes $$ x_{1}''+x_{1}=-a^{2}\cos2\tau. $$ Solving this equation (using variation of parameters or some other method to obtain a particular solution) with homogeneous initial conditions gives $$ x_{1}(\tau)=-\frac{2}{3}a^{2}\sin^{2}\left(\frac{\tau}{2}\right)\left(2+\cos\tau\right). $$ We then form our first order approximation to $x(t)$ by substituting these solutions into our asymptotic expansion for $x$: \begin{eqnarray*} x(t)&=&a\cos\omega t-\epsilon\frac{2}{3}a^{2}\sin^{2}\left(\frac{\omega t}{2}\right)\left(2+\cos\omega t\right)+O(\epsilon^{2})\\ &=&a\cos\omega t +\epsilon\frac{a^{2}}{6}(3-2\cos\omega t-\cos2\omega t)+O(\epsilon^{2}), \end{eqnarray*} where $\omega=1+O(\epsilon^{2})$ and the simplifying identity $\sin^{2}(\frac{\tau}{2})=\frac{1}{2}(1-\cos\tau)$ was used.  As for the center of oscillation, we expect it to occur at approximately $\frac{T}{4}$, where $T$ is the period of oscillation, since the oscillation begins at maximal amplitude for $t=0$ (see initial conditions).  The period of oscillation is $$ T=\frac{2\pi}{\omega} $$ so that evaluating our asymptotic expansion at $\frac{T}{4}=\frac{\pi}{2\omega}$ gives \begin{eqnarray*} x\left(\frac{\pi}{2\omega}\right) &=&a\cos\frac{\pi}{2}+\epsilon\frac{a^{2}}{6}\left(3-2\cos\frac{\pi}{2}-\cos\pi\right)\\ &=& 0+\epsilon\frac{a^2}{6}\left(3-0-(-1)\right)\\ &=& \epsilon\frac{2a^{2}}{3} \end{eqnarray*} which is accurate up to $O(\epsilon^{2})$.",,"['ordinary-differential-equations', 'perturbation-theory']"
85,Does multiplying by $dt$ have any meaning?,Does multiplying by  have any meaning?,dt,"Consider, for example, the equation $x'=x$, then it is usually solved by writing $\frac{dx}{dt}=x\implies\frac{dx}{x}=dt\implies\int\frac{dx}{x}=\int dt$ ... I know that there is a theorem in ODE that justify $x'=x\implies\int\frac{dx}{x}=\int dt$ ,but my question is about the intermediate step: $x'$ at some point $x_{0}$ is defined via a limit. $\frac{dx}{dt}$ is, as far as I understand, a notation for the function $x'$ - so we can not multiply by $dt$ since it has no meaning, it is a part of the notation. My question is as follows: Although the last step is indeed correct and can be justified, does the intermediate step (multiplying by $dt$) have any meaning, or is it just an easy way to remember and get to the last step ?","Consider, for example, the equation $x'=x$, then it is usually solved by writing $\frac{dx}{dt}=x\implies\frac{dx}{x}=dt\implies\int\frac{dx}{x}=\int dt$ ... I know that there is a theorem in ODE that justify $x'=x\implies\int\frac{dx}{x}=\int dt$ ,but my question is about the intermediate step: $x'$ at some point $x_{0}$ is defined via a limit. $\frac{dx}{dt}$ is, as far as I understand, a notation for the function $x'$ - so we can not multiply by $dt$ since it has no meaning, it is a part of the notation. My question is as follows: Although the last step is indeed correct and can be justified, does the intermediate step (multiplying by $dt$) have any meaning, or is it just an easy way to remember and get to the last step ?",,"['calculus', 'ordinary-differential-equations']"
86,$u''+\frac{4}{x+1}u'+\frac{2}{\left(x+1\right)^{2}}u=0$ variational solution,variational solution,u''+\frac{4}{x+1}u'+\frac{2}{\left(x+1\right)^{2}}u=0,"This is a concept solution scheme derived from a particular example that I have not been able to generalise sufficiently. The objective is to find a particular solution to a certain second-order equation. Rewrite the equation in the self-adjoint form: $$2\left(\left(x+1\right)^4u'\right)'+4(x+1)^2u=0$$ $$\left(Pu'\right)'-Qu=0 \qquad (1)$$ where $$P(x)=2\left(x+1\right)^4$$ $$Q(x)=4\left(x+1\right)^2.$$ Now (1) can be interpreted as the Jacobi equation for a certain variational problem $$S[y]=\int_a^b F\left(x,y,y'\right)dx \to \min, \qquad y(a)=A, y(b)=B$$ so that $$P(x) = \left. \frac{\partial^2 F}{\partial y'^2} \right|_{y=y_0(x)}$$ $$Q(x) = \left. \frac{\partial^2 F}{\partial y^2}\right|_{y=y_0(x)}-\left.\frac{\partial^2 F}{\partial y\partial y'}\right|_{y=y_0(x)}$$ where $y_0(x)$ is a solution of the Euler-Lagrange equation for the same functional. Suppose we have figured out that for the given example: $$F=\frac{y^2}{y'^4}.$$ Consider the following problem $$S[y]=\int_0^1\frac{y^2}{y'^4}dx, \qquad y(0)=1,y(1)=\frac{1}{2}$$ for which the given equation will be a Jacobi one. Now it is easy to write out the Euler-Lagrange equation. In fact we can write the first integral straightaway, since $F$ does not depend on $x$: $$y'F_{y'}-F=C$$ $$y'\frac{2y'}{y^{4}}-\frac{y'^{2}}{y^{4}}=C^{2}$$ $$\frac{y'}{y^{2}}=C$$ $$-\frac{1}{y}=Cx+C_{0}$$ Applying boundary condition $y(0)=1$ we obtain a family of solutions: $$y(x,C)=\frac{1}{Cx+1}$$ Now a solution of the Jacobi equation is really an envelope of the family of solutions of Euler-Lagrange equation and can be found by differentiating with respect to parameter $C$: $$u(x)=\frac{\partial y(x,C)}{\partial C}=-\frac{x}{\left(Cx+1\right)^{2}}$$ Finally setting $C=1$ by virtue of the boundary conditions: $$u(x)=-\frac{x}{\left(x+1\right)^{2}}$$ It can be verified by direct substitution that the above function is a solution of the subject equation. Since the equation is of the second order, the process of constructing a general solution from here is straightforward. Now my concern is: does there exist a way to work backwards from the expressions of $P$ and $Q$ to the function $F$? If this is accomplished then this might result in another indirect method of solving a certain class of second-order ODE. If however, this requires too much pre-knowledge about the solutions of E-L, then this example is bound to remain a case of reverse-engineering.","This is a concept solution scheme derived from a particular example that I have not been able to generalise sufficiently. The objective is to find a particular solution to a certain second-order equation. Rewrite the equation in the self-adjoint form: $$2\left(\left(x+1\right)^4u'\right)'+4(x+1)^2u=0$$ $$\left(Pu'\right)'-Qu=0 \qquad (1)$$ where $$P(x)=2\left(x+1\right)^4$$ $$Q(x)=4\left(x+1\right)^2.$$ Now (1) can be interpreted as the Jacobi equation for a certain variational problem $$S[y]=\int_a^b F\left(x,y,y'\right)dx \to \min, \qquad y(a)=A, y(b)=B$$ so that $$P(x) = \left. \frac{\partial^2 F}{\partial y'^2} \right|_{y=y_0(x)}$$ $$Q(x) = \left. \frac{\partial^2 F}{\partial y^2}\right|_{y=y_0(x)}-\left.\frac{\partial^2 F}{\partial y\partial y'}\right|_{y=y_0(x)}$$ where $y_0(x)$ is a solution of the Euler-Lagrange equation for the same functional. Suppose we have figured out that for the given example: $$F=\frac{y^2}{y'^4}.$$ Consider the following problem $$S[y]=\int_0^1\frac{y^2}{y'^4}dx, \qquad y(0)=1,y(1)=\frac{1}{2}$$ for which the given equation will be a Jacobi one. Now it is easy to write out the Euler-Lagrange equation. In fact we can write the first integral straightaway, since $F$ does not depend on $x$: $$y'F_{y'}-F=C$$ $$y'\frac{2y'}{y^{4}}-\frac{y'^{2}}{y^{4}}=C^{2}$$ $$\frac{y'}{y^{2}}=C$$ $$-\frac{1}{y}=Cx+C_{0}$$ Applying boundary condition $y(0)=1$ we obtain a family of solutions: $$y(x,C)=\frac{1}{Cx+1}$$ Now a solution of the Jacobi equation is really an envelope of the family of solutions of Euler-Lagrange equation and can be found by differentiating with respect to parameter $C$: $$u(x)=\frac{\partial y(x,C)}{\partial C}=-\frac{x}{\left(Cx+1\right)^{2}}$$ Finally setting $C=1$ by virtue of the boundary conditions: $$u(x)=-\frac{x}{\left(x+1\right)^{2}}$$ It can be verified by direct substitution that the above function is a solution of the subject equation. Since the equation is of the second order, the process of constructing a general solution from here is straightforward. Now my concern is: does there exist a way to work backwards from the expressions of $P$ and $Q$ to the function $F$? If this is accomplished then this might result in another indirect method of solving a certain class of second-order ODE. If however, this requires too much pre-knowledge about the solutions of E-L, then this example is bound to remain a case of reverse-engineering.",,"['ordinary-differential-equations', 'calculus-of-variations']"
87,Prove there are at least two periodic solutions,Prove there are at least two periodic solutions,,"Could anyone comment on the following ODE problem? Thank you. Given a 2-d system in polar coordinates: $$\dot{r}=r+r^{5}-r^{3}(1+\sin^{2}\theta)$$ $$\dot{\theta}=1$$ Prove that there are at least two nonconstant periodic solutions to this system. It's easy to prove that there is a noncostant periodic solution using Poincare-Bendixson theorem, but I don't know how to prove the existantce of two nonconstant periodic solutions.","Could anyone comment on the following ODE problem? Thank you. Given a 2-d system in polar coordinates: $$\dot{r}=r+r^{5}-r^{3}(1+\sin^{2}\theta)$$ $$\dot{\theta}=1$$ Prove that there are at least two nonconstant periodic solutions to this system. It's easy to prove that there is a noncostant periodic solution using Poincare-Bendixson theorem, but I don't know how to prove the existantce of two nonconstant periodic solutions.",,['ordinary-differential-equations']
88,Solving geodesic problems with Euler-Lagrange equation,Solving geodesic problems with Euler-Lagrange equation,,"This is the question: Problem B.1 Two cities - Tel-Aviv, Israel and SanDiego, CA - have the same latitude 32 ◦ N, but, diﬀerent longitudes: Tel-Aviv is 34 ◦ E and San-Diego is 117 ◦ W. What is the maximal distance between the great circle arc and the 32 ◦ latitude line? By how much the path along the great circle (geodesic) arc will be shorter than the path along the latitude line? Hint: Spherical triangles (on a sphere with radius 1) satisfy a spherical law of cosines $$\cos(c)=\cos(a)\cos(b)+\sin(a)\sin(b)\cos(C)$$. I don't want the solution. My main problem is I don't know where to begin and where I want to get to. Could you help me with figuring out how to get through such problems? Euler-Lagrange equation: $$\frac{d}{dx}L_{y'}-L_y=0$$ while $$I[y(x)]=\int_{x_0}^{x_1}L(y',y,x)dx$$ Full description (as our lecturer assures us) is right here , page 2.","This is the question: Problem B.1 Two cities - Tel-Aviv, Israel and SanDiego, CA - have the same latitude 32 ◦ N, but, diﬀerent longitudes: Tel-Aviv is 34 ◦ E and San-Diego is 117 ◦ W. What is the maximal distance between the great circle arc and the 32 ◦ latitude line? By how much the path along the great circle (geodesic) arc will be shorter than the path along the latitude line? Hint: Spherical triangles (on a sphere with radius 1) satisfy a spherical law of cosines $$\cos(c)=\cos(a)\cos(b)+\sin(a)\sin(b)\cos(C)$$. I don't want the solution. My main problem is I don't know where to begin and where I want to get to. Could you help me with figuring out how to get through such problems? Euler-Lagrange equation: $$\frac{d}{dx}L_{y'}-L_y=0$$ while $$I[y(x)]=\int_{x_0}^{x_1}L(y',y,x)dx$$ Full description (as our lecturer assures us) is right here , page 2.",,"['ordinary-differential-equations', 'geodesy']"
89,Satisfying a Differential Equation and complex Laguerre,Satisfying a Differential Equation and complex Laguerre,,"I have the following problem Show that $$L_n(x)=\frac{e^x}{2 \pi i}\oint \frac{t^n e^{-t}}{(t-x)^{n+1}}dt$$ satisfies $$x\, L_n^{\prime\prime}+(1-x)L_n^\prime+n\, L_n=0$$ where the contour is counterclockwise around the origin. I have busted through with the x-derivatives, but I'm not sure where to go from there... or if that is even the way to tackle this problem...  There was talk about Taylor expanding the integrand and using the properties of contour integration to whittle down the terms to a finite number of contributions inside the integrals.  Is this nonsense, or legit? any help would be great, Thanks.","I have the following problem Show that $$L_n(x)=\frac{e^x}{2 \pi i}\oint \frac{t^n e^{-t}}{(t-x)^{n+1}}dt$$ satisfies $$x\, L_n^{\prime\prime}+(1-x)L_n^\prime+n\, L_n=0$$ where the contour is counterclockwise around the origin. I have busted through with the x-derivatives, but I'm not sure where to go from there... or if that is even the way to tackle this problem...  There was talk about Taylor expanding the integrand and using the properties of contour integration to whittle down the terms to a finite number of contributions inside the integrals.  Is this nonsense, or legit? any help would be great, Thanks.",,"['ordinary-differential-equations', 'special-functions', 'contour-integration', 'orthogonal-polynomials']"
90,A solution of $-y'' + q(x)y= \lambda y$,A solution of,-y'' + q(x)y= \lambda y,"Could you help me with the following problem (from Poschel and Trubowitz)? I am looking for a solution of the differential equation $-y'' + q(x)y= \lambda y$, for $0 \leq x \leq  1$ with $\lambda \in \mathbb{C}$ and $q \in L_{\mathbb{C}}^2([0,1])$ subject to the initial conditions $y(0)=1$ and $y'(0)=0$. Let $y(x,\lambda,q)$ is given as a power series in $q$, i.e. $$y(x,\lambda,q)= C_0(x,\lambda) + \sum_{n \geq 1} C_n(x,\lambda,q),$$ where $C_n(x,\lambda,q)=C_n(x,\lambda,q_1,\cdots, q_n)|_{q_1=\cdots=q_n=q} $ and $C_n(x,\lambda,q_1,\cdots, q_n)$ is a bounded, multi-linear symmetric form on $L_{\mathbb{C}}^2\times \cdots\times L_{\mathbb{C}}^2$ for each $x$ and $\lambda$. We already know that $C_n(x,\lambda,q)= \int_0^x \frac{\sin(\sqrt{\lambda}(x-t))}{\sqrt{\lambda}} q(t) C_{n-1}(t,\lambda,q) \,dt$, $n\geq 1$ is a solution with $C_0(x,\lambda)=\cos{\sqrt{\lambda}x}$. How to show that 1) $y(x,\lambda,q)=y(x,0,q-\lambda)$. 2) $C_n(x,\lambda,q)= \int_{0\leq t_1 \leq \cdots \leq t_{n+1}= x} \prod_{i=1}^n (t_{i+1}-t_i)(q(t_i)-\lambda) \, dt_1 \cdots dt_n$. Thank you!","Could you help me with the following problem (from Poschel and Trubowitz)? I am looking for a solution of the differential equation $-y'' + q(x)y= \lambda y$, for $0 \leq x \leq  1$ with $\lambda \in \mathbb{C}$ and $q \in L_{\mathbb{C}}^2([0,1])$ subject to the initial conditions $y(0)=1$ and $y'(0)=0$. Let $y(x,\lambda,q)$ is given as a power series in $q$, i.e. $$y(x,\lambda,q)= C_0(x,\lambda) + \sum_{n \geq 1} C_n(x,\lambda,q),$$ where $C_n(x,\lambda,q)=C_n(x,\lambda,q_1,\cdots, q_n)|_{q_1=\cdots=q_n=q} $ and $C_n(x,\lambda,q_1,\cdots, q_n)$ is a bounded, multi-linear symmetric form on $L_{\mathbb{C}}^2\times \cdots\times L_{\mathbb{C}}^2$ for each $x$ and $\lambda$. We already know that $C_n(x,\lambda,q)= \int_0^x \frac{\sin(\sqrt{\lambda}(x-t))}{\sqrt{\lambda}} q(t) C_{n-1}(t,\lambda,q) \,dt$, $n\geq 1$ is a solution with $C_0(x,\lambda)=\cos{\sqrt{\lambda}x}$. How to show that 1) $y(x,\lambda,q)=y(x,0,q-\lambda)$. 2) $C_n(x,\lambda,q)= \int_{0\leq t_1 \leq \cdots \leq t_{n+1}= x} \prod_{i=1}^n (t_{i+1}-t_i)(q(t_i)-\lambda) \, dt_1 \cdots dt_n$. Thank you!",,"['ordinary-differential-equations', 'spectral-theory']"
91,Steady-state of `degenerate' delayed differential equation,Steady-state of `degenerate' delayed differential equation,,"Consider the simple delayed differential equation: $$X'(t) = -a X(t) + a X(t - d)$$ where $d$ and $a$ are positive constants. I'm interested in the possible steady-state (stationary) solutions of this and other similar equations as $t\to\infty$. The steady-state seems to vary depending on the initial conditions, for example, I am interested in the case with initial data given by $X(t) = 0$ for $t \in [-d, 0)$ and $X(0) = 1$. Numerical integration with Matlab's dde23 routine gives the steady-state in the case at around $0.33$. My question is whether there is another way (other than numerical integration for a long time period) to find such steady-states analogous to finding fixed points of ODEs. If you try and do that here, by assuming that the terms $X(t)$ and $X(t - d)$ become equal as $t\to\infty$, and that $X'(t) = 0$, you get the useless relation: $$0 = -a X^* + a X^*$$ which, of course, tells you nothing about possible fixed points $X^*$. Any ideas? Thanks!","Consider the simple delayed differential equation: $$X'(t) = -a X(t) + a X(t - d)$$ where $d$ and $a$ are positive constants. I'm interested in the possible steady-state (stationary) solutions of this and other similar equations as $t\to\infty$. The steady-state seems to vary depending on the initial conditions, for example, I am interested in the case with initial data given by $X(t) = 0$ for $t \in [-d, 0)$ and $X(0) = 1$. Numerical integration with Matlab's dde23 routine gives the steady-state in the case at around $0.33$. My question is whether there is another way (other than numerical integration for a long time period) to find such steady-states analogous to finding fixed points of ODEs. If you try and do that here, by assuming that the terms $X(t)$ and $X(t - d)$ become equal as $t\to\infty$, and that $X'(t) = 0$, you get the useless relation: $$0 = -a X^* + a X^*$$ which, of course, tells you nothing about possible fixed points $X^*$. Any ideas? Thanks!",,"['ordinary-differential-equations', 'delay-differential-equations']"
92,One question on 1st-order PDE,One question on 1st-order PDE,,"Given a smooth vector field $\mathbf{b}$ on $\mathbb{R}^n$, let   $\mathbf{x}(s)=\mathbf{x}(s,x,t)$ solve the ODE   $$\dot{\mathbf{x}}=\mathbf{b}(\mathbf{x}) (s\in\mathbb{R}), x(t)=x.$$ (a) Define the Jacobian   $$J(s,x,t):=\det D_x\mathbf{x}(s,x,t)$$   and derive the Euler formula   $$J_s=\operatorname{div} \mathbf{b}(\mathbf{x})J.$$ (b) Set $$u(x,t):=g(\mathbf{x}(0,x,t))J(0,x,t).$$ Show that $u(x,t)$   solves the equation   $$u_t+\operatorname{div}(u\mathbf{b}(x))=0, \mbox{ in } \mathbb{R}^n\times\mathbb{R}_+,$$   with the initial value $u(x,0)=g(x)$. This is an exercise in L. Evans' classic textbook ""Partial differential equations (2nd Edition)"", page 162-163. We have solved (a). Now we are focusing on (b). Evans gave a hint on (b): one should show $\frac{\partial}{\partial s}(u(\mathbf{x},s)J)=0$ firstly. We have tried the characteristic method and some other methods, however, we are not able to work it out. Thanks for all of your help!","Given a smooth vector field $\mathbf{b}$ on $\mathbb{R}^n$, let   $\mathbf{x}(s)=\mathbf{x}(s,x,t)$ solve the ODE   $$\dot{\mathbf{x}}=\mathbf{b}(\mathbf{x}) (s\in\mathbb{R}), x(t)=x.$$ (a) Define the Jacobian   $$J(s,x,t):=\det D_x\mathbf{x}(s,x,t)$$   and derive the Euler formula   $$J_s=\operatorname{div} \mathbf{b}(\mathbf{x})J.$$ (b) Set $$u(x,t):=g(\mathbf{x}(0,x,t))J(0,x,t).$$ Show that $u(x,t)$   solves the equation   $$u_t+\operatorname{div}(u\mathbf{b}(x))=0, \mbox{ in } \mathbb{R}^n\times\mathbb{R}_+,$$   with the initial value $u(x,0)=g(x)$. This is an exercise in L. Evans' classic textbook ""Partial differential equations (2nd Edition)"", page 162-163. We have solved (a). Now we are focusing on (b). Evans gave a hint on (b): one should show $\frac{\partial}{\partial s}(u(\mathbf{x},s)J)=0$ firstly. We have tried the characteristic method and some other methods, however, we are not able to work it out. Thanks for all of your help!",,"['ordinary-differential-equations', 'partial-differential-equations']"
93,Approximating a system of differential equations as a Bézier curve,Approximating a system of differential equations as a Bézier curve,,"I am looking for a general transform to approximate the solution to an n-dimensional system of differential equations and initial conditions as a cubic or quadratic Bézier curve. Sorry if my terminology is a bit off, most of my maths is self-taught and I am acutely aware I might be asking a question that reveals some fundamental misunderstandings. The reason I want a Bézier curve is that they can be rendered efficiently on modern graphics hardware. As an example of what I am looking for, I might have a system of differential equations with respect to time: $\begin{align*}\frac{dx}{dt} &= -2x\\ \frac{dy}{dt} &= -2y\\ \frac{dz}{dt} &= -2z\end{align*}$ with initial conditions $x=y=z=1$ at $t=0$ Solved with a time-step method (in practice the systems may not be algebraically solvable) for small increments of $t$ and plotted in 3D space the resulting line traces a circular path around the origin. If I wanted to render this on a computer screen, I could collect a series of points and play 'connect the dots' to draw a crude line which would require recalculation with every adjustment to level of detail. If I could approximate the path as a Bézier curve I would only need to calculate it once to get a line which is smooth at any level of detail. The curve will always be plotted in three dimensions, but the system may be higher dimensional. Is there any way to generate such an approximation algebraically? Should I be looking at generating some points numerically and just fitting a simpler curve to them step by step? Any recommendations on an approach for that?","I am looking for a general transform to approximate the solution to an n-dimensional system of differential equations and initial conditions as a cubic or quadratic Bézier curve. Sorry if my terminology is a bit off, most of my maths is self-taught and I am acutely aware I might be asking a question that reveals some fundamental misunderstandings. The reason I want a Bézier curve is that they can be rendered efficiently on modern graphics hardware. As an example of what I am looking for, I might have a system of differential equations with respect to time: $\begin{align*}\frac{dx}{dt} &= -2x\\ \frac{dy}{dt} &= -2y\\ \frac{dz}{dt} &= -2z\end{align*}$ with initial conditions $x=y=z=1$ at $t=0$ Solved with a time-step method (in practice the systems may not be algebraically solvable) for small increments of $t$ and plotted in 3D space the resulting line traces a circular path around the origin. If I wanted to render this on a computer screen, I could collect a series of points and play 'connect the dots' to draw a crude line which would require recalculation with every adjustment to level of detail. If I could approximate the path as a Bézier curve I would only need to calculate it once to get a line which is smooth at any level of detail. The curve will always be plotted in three dimensions, but the system may be higher dimensional. Is there any way to generate such an approximation algebraically? Should I be looking at generating some points numerically and just fitting a simpler curve to them step by step? Any recommendations on an approach for that?",,"['ordinary-differential-equations', 'numerical-methods', 'approximation', 'bezier-curve']"
94,How to analysis the stability of these ODE?,How to analysis the stability of these ODE?,,"Study whether the null solution of the system: $$\begin{cases}  \frac{dx_1}{dt}=x_2(t)\\  \frac{dx_2}{dt}=-w(t)^2 x_1(t)\\  \end{cases} $$ is Lyapunov stable, where $$ w(t)=  \begin{cases} 0.4 & \text{for } 2k\pi \leq t < (2k + 1)\pi\\  0.6 & \text{for } (2k-1)\pi \leq t < 2k\pi  \end{cases} $$ Carry out the stability analysis of $$\frac{d^2x(t)}{dt^2}+1-\cos(x(t))=0$$","Study whether the null solution of the system: $$\begin{cases}  \frac{dx_1}{dt}=x_2(t)\\  \frac{dx_2}{dt}=-w(t)^2 x_1(t)\\  \end{cases} $$ is Lyapunov stable, where $$ w(t)=  \begin{cases} 0.4 & \text{for } 2k\pi \leq t < (2k + 1)\pi\\  0.6 & \text{for } (2k-1)\pi \leq t < 2k\pi  \end{cases} $$ Carry out the stability analysis of $$\frac{d^2x(t)}{dt^2}+1-\cos(x(t))=0$$",,['ordinary-differential-equations']
95,Solving differential equations with Fourier Transformation,Solving differential equations with Fourier Transformation,,"I'm looking for some documentation about solving DEs using Fourier Transformation (not Fourier Series). In particular, I have this one DE that I solved using another technique. Somebody mentioned that it can be solved using FT as well: $$\varepsilon - \frac{1}{2}l^2 \frac{d^2 \varepsilon}{dx^2} = \delta(x)$$ Where $\varepsilon$ is a function of $x$ only, $l$ is a scalar constant and $\delta(x)$ is the Dirac ""function"". The solution for the homogeneous case is $$\varepsilon = A \; exp \left[ -\frac{|x|\sqrt2}{l} \right]$$ If I remember correctly, taking some derivatives resulted in the final solution of $A = \dfrac{1}{l \sqrt{2}}$. Ok, now with FT. I found this site , the example looks a little like my DE -- with in my case $g(t) = \delta(x)$, which is convenient considering the convolution in the end. Without the constant $\frac{1}{2}l^2$ I get $$Y(f) - (2\pi i f)^2 Y(f) = \left( 1 - (2 \pi i f)^2 \right)\;Y(f) = G(f)$$ So $$Y(f) = \frac{G(f)}{1+4 \pi^2 f^2}$$. According to the site (would like to know how to compute it myself), $$F^{-1}\left( \frac{1}{1+4 \pi^2 f^2} \right) = \frac{e^{-|t|}}{2}$$ Taking the convolution with the Dirac ""function"" is again the same function, $\dfrac{e^{-|t|}}{2} = exp \left[ \dfrac{-|t|}{2} \right]$. So to conclude: Any documentation about solving DEs using FT is more than welcome How to do the same, now with the constant $\frac{1}{2}l^2$ How to compute the inverse Fourier Transform of expressions like above (I know it involves an integral to infinity, but I never learned how to solve integrals like that)","I'm looking for some documentation about solving DEs using Fourier Transformation (not Fourier Series). In particular, I have this one DE that I solved using another technique. Somebody mentioned that it can be solved using FT as well: $$\varepsilon - \frac{1}{2}l^2 \frac{d^2 \varepsilon}{dx^2} = \delta(x)$$ Where $\varepsilon$ is a function of $x$ only, $l$ is a scalar constant and $\delta(x)$ is the Dirac ""function"". The solution for the homogeneous case is $$\varepsilon = A \; exp \left[ -\frac{|x|\sqrt2}{l} \right]$$ If I remember correctly, taking some derivatives resulted in the final solution of $A = \dfrac{1}{l \sqrt{2}}$. Ok, now with FT. I found this site , the example looks a little like my DE -- with in my case $g(t) = \delta(x)$, which is convenient considering the convolution in the end. Without the constant $\frac{1}{2}l^2$ I get $$Y(f) - (2\pi i f)^2 Y(f) = \left( 1 - (2 \pi i f)^2 \right)\;Y(f) = G(f)$$ So $$Y(f) = \frac{G(f)}{1+4 \pi^2 f^2}$$. According to the site (would like to know how to compute it myself), $$F^{-1}\left( \frac{1}{1+4 \pi^2 f^2} \right) = \frac{e^{-|t|}}{2}$$ Taking the convolution with the Dirac ""function"" is again the same function, $\dfrac{e^{-|t|}}{2} = exp \left[ \dfrac{-|t|}{2} \right]$. So to conclude: Any documentation about solving DEs using FT is more than welcome How to do the same, now with the constant $\frac{1}{2}l^2$ How to compute the inverse Fourier Transform of expressions like above (I know it involves an integral to infinity, but I never learned how to solve integrals like that)",,"['ordinary-differential-equations', 'fourier-analysis']"
96,Inequality of ODE solutions,Inequality of ODE solutions,,"Says I have two (scalar) ODE: $u' = f(u,t)$ and $v' = g(v,t)$ where Both $f$ and $g$ are piecewise-continuous and locally Lipschitz, for existence & uniqueness of solutions $u(t)$ and $v(t)$. $f(x,t) \leq g(x,t)$ for all $x$ and $t$. I believe that if $u(0) \leq v(0)$ then $u(t) \leq v(t)$ for all $t \geq 0$. But I don't know if there is such theorem, or if not, how to prove it.","Says I have two (scalar) ODE: $u' = f(u,t)$ and $v' = g(v,t)$ where Both $f$ and $g$ are piecewise-continuous and locally Lipschitz, for existence & uniqueness of solutions $u(t)$ and $v(t)$. $f(x,t) \leq g(x,t)$ for all $x$ and $t$. I believe that if $u(0) \leq v(0)$ then $u(t) \leq v(t)$ for all $t \geq 0$. But I don't know if there is such theorem, or if not, how to prove it.",,"['ordinary-differential-equations', 'inequality']"
97,"Efficiently solving a large, sparse linear system $M(s)ab(s)=c(s)$ (determined by smooth functions) over some range of $s$","Efficiently solving a large, sparse linear system  (determined by smooth functions) over some range of",M(s)ab(s)=c(s) s,"I'm looking at a differential equation on the edges of a graph (the application is neuroscience), and the Laplace transform of the solution on most of the edges has a general solution more-or-less of the form $(\mathcal{L}v)(x,s)= a(s)e^{-x/\sqrt{s}} + b(s)e^{x/\sqrt{s}}$, where $x$ is in the interval assigned to the edge and $s$ is the Laplace variable. One of the edges has another term in its solution which has no unknowns and can be evaluated easily. The functions $a(s)$ and $b(s)$ are unknown, but can be found by putting together the boundary conditions for each vertex of my graph. This means substituting $x=0$ or $x=L$ (depending on the direction of the edge) for each edge incident on each vertex, and substituting a value of $s$ ($L$ is the length of an edge). This results in a system of $2m$ independent linear equations in $2m$ variables (where $m$ is the number of edges). We can write this system as $M(s)ab(s)=c(s)$, where $M(s)$ is a sparse matrix whose nonzero entries are smooth functions of $s$, $c(s)$ is a vector mostly consisting of zeros whose entries are smooth functions of $s$, and the unknown $ab(s)$ contains the values of $a(s)$ and $b(s)$ for all edges. The zero entries are zero for every value of $s$. I plan to get the solution $v(x,t)$ for a single value of $x$ on an edge (in the time domain) by finding $a(s)$ and $b(s)$ for that edge, for some range of $s$ (probably a large finite number of discrete values), substituting $a(s)$ and $b(s)$ into the formula for $(\mathcal{L}v)(x,s)$, and feeding the resulting Laplace-domain solution into a numerical inverse Laplace transform ( using a fast Fourier transform ) to get a time series of $v(x,t)$ (for a single value of $x$ and a range of $t$-values). An obvious way to do this is to apply a sparse linear system solver to the system $S$ times, where $S$ is the number of values of $s$ required for the NILT. This could be very time-consuming, and I would like to be able to do this for values of $m$ as large as possible. My question(s) are: Is there a name for this kind of problem? I would be surprised if no-one has encountered anything similar before, but I've no idea what it would be called. EDIT: These are called parametrised matrix equations . Most of the material about them that I can find is by Paul Constantine. Can the smoothness of the functions defining $M(s)$ and $c(s)$ be exploited to find the array of values of $a(s)$ and $b(s)$ without solving the whole system $S$ times? For instance, is it possible to specify any the circumstances under which interpolation could be used to get intermediate values of $a(s)$ and $b(s)$ without ruining the accuracy of the NILT at the end? Would it be practical to solve this symbolically using a computer algebra system? All of the nonzero matrix entries have a form something like $Ke^{L/\sqrt{s}}$, where $K$ and $L$ are known numbers (and $L$ may be zero). Am I going completely the wrong way about this?","I'm looking at a differential equation on the edges of a graph (the application is neuroscience), and the Laplace transform of the solution on most of the edges has a general solution more-or-less of the form $(\mathcal{L}v)(x,s)= a(s)e^{-x/\sqrt{s}} + b(s)e^{x/\sqrt{s}}$, where $x$ is in the interval assigned to the edge and $s$ is the Laplace variable. One of the edges has another term in its solution which has no unknowns and can be evaluated easily. The functions $a(s)$ and $b(s)$ are unknown, but can be found by putting together the boundary conditions for each vertex of my graph. This means substituting $x=0$ or $x=L$ (depending on the direction of the edge) for each edge incident on each vertex, and substituting a value of $s$ ($L$ is the length of an edge). This results in a system of $2m$ independent linear equations in $2m$ variables (where $m$ is the number of edges). We can write this system as $M(s)ab(s)=c(s)$, where $M(s)$ is a sparse matrix whose nonzero entries are smooth functions of $s$, $c(s)$ is a vector mostly consisting of zeros whose entries are smooth functions of $s$, and the unknown $ab(s)$ contains the values of $a(s)$ and $b(s)$ for all edges. The zero entries are zero for every value of $s$. I plan to get the solution $v(x,t)$ for a single value of $x$ on an edge (in the time domain) by finding $a(s)$ and $b(s)$ for that edge, for some range of $s$ (probably a large finite number of discrete values), substituting $a(s)$ and $b(s)$ into the formula for $(\mathcal{L}v)(x,s)$, and feeding the resulting Laplace-domain solution into a numerical inverse Laplace transform ( using a fast Fourier transform ) to get a time series of $v(x,t)$ (for a single value of $x$ and a range of $t$-values). An obvious way to do this is to apply a sparse linear system solver to the system $S$ times, where $S$ is the number of values of $s$ required for the NILT. This could be very time-consuming, and I would like to be able to do this for values of $m$ as large as possible. My question(s) are: Is there a name for this kind of problem? I would be surprised if no-one has encountered anything similar before, but I've no idea what it would be called. EDIT: These are called parametrised matrix equations . Most of the material about them that I can find is by Paul Constantine. Can the smoothness of the functions defining $M(s)$ and $c(s)$ be exploited to find the array of values of $a(s)$ and $b(s)$ without solving the whole system $S$ times? For instance, is it possible to specify any the circumstances under which interpolation could be used to get intermediate values of $a(s)$ and $b(s)$ without ruining the accuracy of the NILT at the end? Would it be practical to solve this symbolically using a computer algebra system? All of the nonzero matrix entries have a form something like $Ke^{L/\sqrt{s}}$, where $K$ and $L$ are known numbers (and $L$ may be zero). Am I going completely the wrong way about this?",,"['linear-algebra', 'ordinary-differential-equations', 'numerical-methods']"
98,Series of nested double integrals,Series of nested double integrals,,"This is kind of a follow-up of my previous question . I'm investigating the following infinite series of nested two-dimensional integrals $$\sigma(t,t^\prime) = 1 - \int_{t^\prime}^t\mathrm dt_1 \int_{t^\prime}^{t_1}\mathrm dt_2 \, \Delta(t_1,t_2) + \int_{t^\prime}^t\mathrm dt_1 \int_{t^\prime}^{t_1}\mathrm dt_2 \int_{t^\prime}^{t_2}\mathrm dt_3 \int_{t^\prime}^{t_3}\mathrm dt_4 \, \Delta(t_1,t_2) \, \Delta(t_3,t_4) + \cdots \;,$$ with $\Delta(t_1,t_2)$ a complex function obeying the relation $$\Delta(t_1,t_2) = \Delta^*(t_2,t_1) \;.$$ I'm looking for a representation of $\sigma(t,t^\prime)$ that can be calculated in completeness (the above infinite sum obviously isn't very useful in a concrete application). Treating $t^\prime$ as constant and taking the derivative of $\sigma(t,t^\prime)$ with respect to $t$ I obtain $$\frac{\mathrm d \sigma(t,t^\prime)}{\mathrm d t} = - \int_{t^\prime}^t\mathrm dt_2 \, \Delta(t,t_2) \, \sigma(t_2,t^\prime) \;,$$ which after re-integrating and using the boundary condition $\sigma(t^\prime,t^\prime) = 1$ yields the recursion relation $$\sigma(t,t^\prime) = 1 - \int_{t^\prime}^t\mathrm dt_1 \int_{t^\prime}^{t_1}\mathrm dt_2 \, \Delta(t_1,t_2) \, \sigma(t_2,t^\prime) \;.$$ However, I don't know how to solve either of these two equations for $\sigma(t,t^\prime)$. In view of my previous question my first guess was to inspect $$\sigma_1(t,t^\prime) = \exp \left[ -\int_{t^\prime}^t\mathrm dt_1 \int_{t^\prime}^{t_1}\mathrm dt_2 \, \Delta(t_1,t_2) \right] \;.$$ Obviously $\sigma_1(t^\prime,t^\prime) = 1$, but taking the derivative of $\sigma_1(t,t^\prime)$ with respect to $t$ at constant $t^\prime$ yields $$\frac{\mathrm d \sigma_1(t,t^\prime)}{\mathrm d t} = - \int_{t^\prime}^t\mathrm dt_2 \, \Delta(t,t_2) \, \sigma(t,t^\prime) \;,$$ which is different from the derivative of the original series $\sigma(t,t^\prime)$ (see above). Still, to first order in $\Delta(t_1,t_2)$ the derivatives are equal and thus $$\sigma(t,t^\prime) = \sigma_1(t,t^\prime) + \exp\left[\mathcal{O}\left(\Delta^2 \right) \right] \;,$$ which is actually quite acceptable to me, since in my case $\Delta(t_1,t_2)$ is a small perturbation. Nevertheless, I'm wondering if one can obtain a better solution to the problem. Edit 1 I discovered the following propagator property of $\sigma(t,t^\prime)$ $$\sigma(t,t_1) \sigma(t_1,t^\prime) \stackrel{!}{=} \sigma(t,t^\prime) \;,$$ which follows from the fact that $\sigma(t,t^\prime)$ represents the interaction part of a Keldysh Green's function, i.e. $$G(t,t^\prime) = G_0(t,t^\prime) \sigma(t,t^\prime) \;,$$ with $G_0(t,t^\prime)$ and $G(t,t^\prime)$ obeying the propagator relation $$iG_{(0)}(t,t_1) iG_{(0)}(t_1,t^\prime) = iG_{(0)}(t,t^\prime)\;.$$ Edit 2 As pointed out by Didier Piau the above propagator property cannot hold, since $$\frac{\mathrm d}{\mathrm d t} \left( \sigma(t,\bar t) \sigma(\bar t,t^\prime) \right) = - \int_{\bar t}^t\mathrm dt_2 \, \Delta(t,t_2) \, \sigma(t_2,\bar t) \sigma(\bar t, t^\prime) \;""=""\; - \int_{\bar t}^t\mathrm dt_2 \, \Delta(t,t_2) \, \sigma(t_2, t^\prime) \;,$$ which is obviously different from $\frac{\mathrm d \sigma(t,t^\prime)}{\mathrm d t}$ (see above). Edit 3 I verified that the above propagator property does in fact only hold for the unperturbed Green's functions $G_0(t,t^\prime)$, but in general not for the full ones $G(t,t^\prime)$ and thus not for $\sigma(t,t^\prime)$.","This is kind of a follow-up of my previous question . I'm investigating the following infinite series of nested two-dimensional integrals $$\sigma(t,t^\prime) = 1 - \int_{t^\prime}^t\mathrm dt_1 \int_{t^\prime}^{t_1}\mathrm dt_2 \, \Delta(t_1,t_2) + \int_{t^\prime}^t\mathrm dt_1 \int_{t^\prime}^{t_1}\mathrm dt_2 \int_{t^\prime}^{t_2}\mathrm dt_3 \int_{t^\prime}^{t_3}\mathrm dt_4 \, \Delta(t_1,t_2) \, \Delta(t_3,t_4) + \cdots \;,$$ with $\Delta(t_1,t_2)$ a complex function obeying the relation $$\Delta(t_1,t_2) = \Delta^*(t_2,t_1) \;.$$ I'm looking for a representation of $\sigma(t,t^\prime)$ that can be calculated in completeness (the above infinite sum obviously isn't very useful in a concrete application). Treating $t^\prime$ as constant and taking the derivative of $\sigma(t,t^\prime)$ with respect to $t$ I obtain $$\frac{\mathrm d \sigma(t,t^\prime)}{\mathrm d t} = - \int_{t^\prime}^t\mathrm dt_2 \, \Delta(t,t_2) \, \sigma(t_2,t^\prime) \;,$$ which after re-integrating and using the boundary condition $\sigma(t^\prime,t^\prime) = 1$ yields the recursion relation $$\sigma(t,t^\prime) = 1 - \int_{t^\prime}^t\mathrm dt_1 \int_{t^\prime}^{t_1}\mathrm dt_2 \, \Delta(t_1,t_2) \, \sigma(t_2,t^\prime) \;.$$ However, I don't know how to solve either of these two equations for $\sigma(t,t^\prime)$. In view of my previous question my first guess was to inspect $$\sigma_1(t,t^\prime) = \exp \left[ -\int_{t^\prime}^t\mathrm dt_1 \int_{t^\prime}^{t_1}\mathrm dt_2 \, \Delta(t_1,t_2) \right] \;.$$ Obviously $\sigma_1(t^\prime,t^\prime) = 1$, but taking the derivative of $\sigma_1(t,t^\prime)$ with respect to $t$ at constant $t^\prime$ yields $$\frac{\mathrm d \sigma_1(t,t^\prime)}{\mathrm d t} = - \int_{t^\prime}^t\mathrm dt_2 \, \Delta(t,t_2) \, \sigma(t,t^\prime) \;,$$ which is different from the derivative of the original series $\sigma(t,t^\prime)$ (see above). Still, to first order in $\Delta(t_1,t_2)$ the derivatives are equal and thus $$\sigma(t,t^\prime) = \sigma_1(t,t^\prime) + \exp\left[\mathcal{O}\left(\Delta^2 \right) \right] \;,$$ which is actually quite acceptable to me, since in my case $\Delta(t_1,t_2)$ is a small perturbation. Nevertheless, I'm wondering if one can obtain a better solution to the problem. Edit 1 I discovered the following propagator property of $\sigma(t,t^\prime)$ $$\sigma(t,t_1) \sigma(t_1,t^\prime) \stackrel{!}{=} \sigma(t,t^\prime) \;,$$ which follows from the fact that $\sigma(t,t^\prime)$ represents the interaction part of a Keldysh Green's function, i.e. $$G(t,t^\prime) = G_0(t,t^\prime) \sigma(t,t^\prime) \;,$$ with $G_0(t,t^\prime)$ and $G(t,t^\prime)$ obeying the propagator relation $$iG_{(0)}(t,t_1) iG_{(0)}(t_1,t^\prime) = iG_{(0)}(t,t^\prime)\;.$$ Edit 2 As pointed out by Didier Piau the above propagator property cannot hold, since $$\frac{\mathrm d}{\mathrm d t} \left( \sigma(t,\bar t) \sigma(\bar t,t^\prime) \right) = - \int_{\bar t}^t\mathrm dt_2 \, \Delta(t,t_2) \, \sigma(t_2,\bar t) \sigma(\bar t, t^\prime) \;""=""\; - \int_{\bar t}^t\mathrm dt_2 \, \Delta(t,t_2) \, \sigma(t_2, t^\prime) \;,$$ which is obviously different from $\frac{\mathrm d \sigma(t,t^\prime)}{\mathrm d t}$ (see above). Edit 3 I verified that the above propagator property does in fact only hold for the unperturbed Green's functions $G_0(t,t^\prime)$, but in general not for the full ones $G(t,t^\prime)$ and thus not for $\sigma(t,t^\prime)$.",,"['integration', 'ordinary-differential-equations', 'power-series', 'integral-transforms']"
99,Finding the modified Green function for the Helmholtz equation,Finding the modified Green function for the Helmholtz equation,,"I've been wrestling with this question for quite some time now, and the result was like 20 leaves of paper packed with scribbling...anyway, here's the question: I need to find the solution to the following source excited Helmholtz equation: $$(\nabla^2 + k^2)G_1(r,a) = -\delta(r-a)$$ With the boundary condition: $$\left. G_1(r,a)\right|_{z=0} = 0$$ Now, the solution is given by the integral: \begin{multline} G_1(r,a)=\iiint G(r,r') \delta(r'-a) dV' \\ + \iint [G(r,r')\nabla'G_1(r',a)-G_1(r',a)\nabla'G(r,r')]\cdot \hat{n} dS' \end{multline} (The surface integral is on a closed surface, for some reason MathJaX doesn't understand what \oiint is) The second term in the surface integral drops since it's $0$ on all the boundaries of the volume that interests us (which is $z \geq 0$). HOWEVER, I do not know what the gradient of $G_1$ is on $z=0$, and I understand that I'm free to chose it however I see fit as long as it only affects the $z<0$ region. Even simplifying the problem to a one dimensional problem I still can't solve it. I do know what the solution is, because I've been thought many years ago in introduction to Electromagnetics that when encountering a boundary over which the electrical potential is $0$, I need to mirror all my sources and give them a negative sign, and if I plug that solution back into the equation is does satisfy it and the boundary condition. But how THAT solution is supposed to be inferred from the mathematical problem is beyond me. Any help/hints/insights would be most welcome. (Sorry for the paucity of tags, I wanted to use ""Helmholtz"" and ""Green"" but I'm still a newbie so I can't create new tags...)","I've been wrestling with this question for quite some time now, and the result was like 20 leaves of paper packed with scribbling...anyway, here's the question: I need to find the solution to the following source excited Helmholtz equation: $$(\nabla^2 + k^2)G_1(r,a) = -\delta(r-a)$$ With the boundary condition: $$\left. G_1(r,a)\right|_{z=0} = 0$$ Now, the solution is given by the integral: \begin{multline} G_1(r,a)=\iiint G(r,r') \delta(r'-a) dV' \\ + \iint [G(r,r')\nabla'G_1(r',a)-G_1(r',a)\nabla'G(r,r')]\cdot \hat{n} dS' \end{multline} (The surface integral is on a closed surface, for some reason MathJaX doesn't understand what \oiint is) The second term in the surface integral drops since it's $0$ on all the boundaries of the volume that interests us (which is $z \geq 0$). HOWEVER, I do not know what the gradient of $G_1$ is on $z=0$, and I understand that I'm free to chose it however I see fit as long as it only affects the $z<0$ region. Even simplifying the problem to a one dimensional problem I still can't solve it. I do know what the solution is, because I've been thought many years ago in introduction to Electromagnetics that when encountering a boundary over which the electrical potential is $0$, I need to mirror all my sources and give them a negative sign, and if I plug that solution back into the equation is does satisfy it and the boundary condition. But how THAT solution is supposed to be inferred from the mathematical problem is beyond me. Any help/hints/insights would be most welcome. (Sorry for the paucity of tags, I wanted to use ""Helmholtz"" and ""Green"" but I'm still a newbie so I can't create new tags...)",,"['ordinary-differential-equations', 'partial-differential-equations']"
