,title_raw,title_text,title_latex,body_raw,body_text,body_latex,tags
0,"Maximal Ideals in $C((0 ,1))$",Maximal Ideals in,"C((0 ,1))","For a set $S\subset\mathbb R$ let $C(S)$ denote the continuous real-valued functions on $S$ . Describe the maximal ideals in $C((0,1))$ .  For $C([0,1])$ we know that maximal ideals are points in $[0, 1]$ and these are the only maximal ideals, all whose elements vanish at a single common point. The proof relies on the compactness of $[0,1]$ . I want to know what happens when the compact interval is replaced by the open interval $(0, 1)$ . Thanks for your help.","For a set let denote the continuous real-valued functions on . Describe the maximal ideals in .  For we know that maximal ideals are points in and these are the only maximal ideals, all whose elements vanish at a single common point. The proof relies on the compactness of . I want to know what happens when the compact interval is replaced by the open interval . Thanks for your help.","S\subset\mathbb R C(S) S C((0,1)) C([0,1]) [0, 1] [0,1] (0, 1)",['linear-algebra']
1,"One to one correspondence between all $F[x]$-module $V$ and all linear transformations $T\colon V\to V$, $V$ being a vector space over $F$.","One to one correspondence between all -module  and all linear transformations ,  being a vector space over .",F[x] V T\colon V\to V V F,"I am fairly a beginner in module theory. While discussing the $F[x]$ -modules, my text book (Dummit & Foote) describes: $\left\{     V \text{ an } F[x] \text{-module} \right\}\longleftrightarrow \left\{ \begin{aligned}     V \text{ is a vector space }\\     \text{               and               }\\ T\colon V\to V \text{ a linear transformation }\end{aligned} \right\}\tag*{}$ given by $\text{ the element } x \text{ acts on } V \text{ as the linear transformation } T\tag*{}$ This is saying that we can't find any $F[x]$ -module without specifying the linear transformation $T$ . But how can we exclude the possibility of having some other $F[x]$ -module which can be obtained without the help of the linear transformation $T\colon V\to V$ ? Is it the case that if $x$ acts on the vectors $v\in V$ , then it must be a linear transformation? If it is the case, how can I prove it?","I am fairly a beginner in module theory. While discussing the -modules, my text book (Dummit & Foote) describes: given by This is saying that we can't find any -module without specifying the linear transformation . But how can we exclude the possibility of having some other -module which can be obtained without the help of the linear transformation ? Is it the case that if acts on the vectors , then it must be a linear transformation? If it is the case, how can I prove it?","F[x] \left\{
    V \text{ an } F[x] \text{-module}
\right\}\longleftrightarrow \left\{
\begin{aligned}
    V \text{ is a vector space }\\
    \text{               and               }\\
T\colon V\to V \text{ a linear transformation }\end{aligned}
\right\}\tag*{} \text{ the element } x \text{ acts on } V \text{ as the linear transformation } T\tag*{} F[x] T F[x] T\colon V\to V x v\in V","['linear-algebra', 'abstract-algebra', 'linear-transformations', 'modules']"
2,Shilov or Axler for Linear Algebra?,Shilov or Axler for Linear Algebra?,,I'm looking for a book on linear algebra. I found that Axler's and Shilov's books have a good reputation. Which of them is better? Which is more complete and suitable for theoretical study?,I'm looking for a book on linear algebra. I found that Axler's and Shilov's books have a good reputation. Which of them is better? Which is more complete and suitable for theoretical study?,,"['linear-algebra', 'reference-request', 'book-recommendation']"
3,Jordan canonical form over $\mathbb{R}$,Jordan canonical form over,\mathbb{R},"The theorem about Jordan canonical form states that for any operator $f:V\to V$ where $V$ is a vector space over $\mathbb{C}$ there is a basis such that the matrix of this operator in this basis is a union of Jordan blocks. And in my lecture notes there is a remark that the theorem is not true for vector spaces over $\mathbb{R}$ . After some time I came up with an example: indeed let's take an operator $f:\mathbb{R}^2\to \mathbb{R}^2$ given by matrix $$A_f=\begin{bmatrix} 0 & -1 \\ 1 & 0 \end{bmatrix}.$$ Suppose it is true and $J$ is a Jordan canonical form then there is a matrix $C$ such that $\det C\neq 0$ with $J=C^{-1}A_fC$ or $CJ=A_fC$ . But $J$ can be one of the following options: $\begin{bmatrix} \lambda & 0 \\ 0 & \lambda \end{bmatrix},\begin{bmatrix} \lambda & 1 \\ 0 & \lambda \end{bmatrix}, \begin{bmatrix} \lambda & 0 \\ 0 & \mu \end{bmatrix} (\lambda\neq \mu)$ . In other words, we can have two block $J_1(\lambda)$ , one block $J_2(\lambda)$ to two blocks with different diagonal elements. If $C=\begin{bmatrix} a & b \\ c & d \end{bmatrix}$ then comparing elements of the first column in matrix equation $CJ=A_fC$ we have: $$a\lambda=-c \quad \text{and} \quad c\lambda=a.$$ Since $\det C\neq 0$ , i.e. $ad\neq bc$ then it is easy to claim that $a\neq 0$ . Then $\lambda=-\frac{c}{a}$ and using the second equality we have $\dfrac{-c^2}{a}=a$ or $a^2+c^2=0$ . And since $a,c$ are reals then $a=c=0$ which is contradiction. Is this reasoning correct? Would be very grateful for any remarks.","The theorem about Jordan canonical form states that for any operator where is a vector space over there is a basis such that the matrix of this operator in this basis is a union of Jordan blocks. And in my lecture notes there is a remark that the theorem is not true for vector spaces over . After some time I came up with an example: indeed let's take an operator given by matrix Suppose it is true and is a Jordan canonical form then there is a matrix such that with or . But can be one of the following options: . In other words, we can have two block , one block to two blocks with different diagonal elements. If then comparing elements of the first column in matrix equation we have: Since , i.e. then it is easy to claim that . Then and using the second equality we have or . And since are reals then which is contradiction. Is this reasoning correct? Would be very grateful for any remarks.","f:V\to V V \mathbb{C} \mathbb{R} f:\mathbb{R}^2\to \mathbb{R}^2 A_f=\begin{bmatrix}
0 & -1 \\
1 & 0
\end{bmatrix}. J C \det C\neq 0 J=C^{-1}A_fC CJ=A_fC J \begin{bmatrix}
\lambda & 0 \\
0 & \lambda
\end{bmatrix},\begin{bmatrix}
\lambda & 1 \\
0 & \lambda
\end{bmatrix},
\begin{bmatrix}
\lambda & 0 \\
0 & \mu
\end{bmatrix} (\lambda\neq \mu) J_1(\lambda) J_2(\lambda) C=\begin{bmatrix}
a & b \\
c & d
\end{bmatrix} CJ=A_fC a\lambda=-c \quad \text{and} \quad c\lambda=a. \det C\neq 0 ad\neq bc a\neq 0 \lambda=-\frac{c}{a} \dfrac{-c^2}{a}=a a^2+c^2=0 a,c a=c=0",['linear-algebra']
4,Show that $SO_{3}$ contains the free group of rank 2 as a subgroup.,Show that  contains the free group of rank 2 as a subgroup.,SO_{3},"I'm trying to prove that $SO_{3}$ contains the free group of rank 2, $F_{2}$ , as a subgroup, by showing there are two rotations in $SO_{3}$ that are independent and hence generate $F_{2}$ . I came across the following proof of this online: $$ A(x,y,z) := (\frac{3}{5}x + \frac{4}{5} y, -\frac{4}{5} x + \frac{3}{5} y, z ); \quad B(x,y,z) := (x, \frac{3}{5}y + \frac{4}{5} z, -\frac{4}{5} y + \frac{3}{5} z ).$$ These are easily seen to be rotation matrices with inverses $$ A^{-1}(x,y,z) := (\frac{3}{5}x - \frac{4}{5} y, \frac{4}{5} x + \frac{3}{5} y, z ); \quad B^{-1}(x,y,z) := (x, \frac{3}{5}y - \frac{4}{5} z, \frac{4}{5} y + \frac{3}{5} z ).$$ Now we claim that no non-trivial composition of $A, B, A^{-1}, B^{-1}$ gives the identity.  It suffices to show that no non-trivial composition   of the operators $5A$ , $5B$ , $5A^{-1}$ , $5B^{-1}$ gives a linear operator whose coefficients are all divisible by 5.   We now work in the finite field geometry $F_5^3$ , where $F_5 = \mathbb{Z}/5\mathbb{Z}$ is the field of order 5.  Then we have $$ 5A(x,y,z) := (3x + 4y, -4x + 3y, 0); \quad 5B(x,y,z) := (0, 3y + 4z, -4y + 3z)$$ and $$ 5A^{-1}(x,y,z) := (3x - 4y, 4x + 3y, 0); \quad 5B^{-1}(x,y,z) := (0, 3y - 4z, 4y + 3z).$$ Each of these operators are rank one operators in $F_5^3$ : \begin{align*} \text{range}(5A) &= \text{span}( (3,-4,0) ) = \ker(5A^{-1})^\perp\\ \text{range}(5A^{-1}) &= \text{span}( (3,4,0) ) =  \ker(5A)^\perp\\ \text{range}(5B) &= \text{span}( (0,3,-4) ) = \ker(5B^{-1})^\perp\\ \text{range}(5B^{-1}) &= \text{span}( (0,3,4) ) = \ker(5B)^\perp. \end{align*} From this we see that any non-trivial combination of $5A$ , $5A^{-1}$ , $5B$ , $5B^{-1}$ (in which $5A$ and $5A^{-1}$ are never   adjacent, and $5B$ and $5B^{-1}$ are never adjacent) will always be a non-zero operator, as desired, because the ranges and kernels are skew. I can't quite understand the proof from 'Each of these operators are rank one operators in $F_5^3$ ' onwards. I would really appreciate it if someone could explain the proof in a way that is relatively easy to understand. Thanks for any help.","I'm trying to prove that contains the free group of rank 2, , as a subgroup, by showing there are two rotations in that are independent and hence generate . I came across the following proof of this online: These are easily seen to be rotation matrices with inverses Now we claim that no non-trivial composition of gives the identity.  It suffices to show that no non-trivial composition   of the operators , , , gives a linear operator whose coefficients are all divisible by 5.   We now work in the finite field geometry , where is the field of order 5.  Then we have and Each of these operators are rank one operators in : From this we see that any non-trivial combination of , , , (in which and are never   adjacent, and and are never adjacent) will always be a non-zero operator, as desired, because the ranges and kernels are skew. I can't quite understand the proof from 'Each of these operators are rank one operators in ' onwards. I would really appreciate it if someone could explain the proof in a way that is relatively easy to understand. Thanks for any help.","SO_{3} F_{2} SO_{3} F_{2}  A(x,y,z) := (\frac{3}{5}x + \frac{4}{5} y, -\frac{4}{5} x + \frac{3}{5} y, z ); \quad
B(x,y,z) := (x, \frac{3}{5}y + \frac{4}{5} z, -\frac{4}{5} y + \frac{3}{5} z ).  A^{-1}(x,y,z) := (\frac{3}{5}x - \frac{4}{5} y, \frac{4}{5} x + \frac{3}{5} y, z ); \quad
B^{-1}(x,y,z) := (x, \frac{3}{5}y - \frac{4}{5} z, \frac{4}{5} y + \frac{3}{5} z ). A, B, A^{-1}, B^{-1} 5A 5B 5A^{-1} 5B^{-1} F_5^3 F_5 = \mathbb{Z}/5\mathbb{Z}  5A(x,y,z) := (3x + 4y, -4x + 3y, 0); \quad 5B(x,y,z) := (0, 3y + 4z, -4y + 3z)  5A^{-1}(x,y,z) := (3x - 4y, 4x + 3y, 0); \quad 5B^{-1}(x,y,z) := (0, 3y - 4z, 4y + 3z). F_5^3 \begin{align*}
\text{range}(5A) &= \text{span}( (3,-4,0) ) = \ker(5A^{-1})^\perp\\
\text{range}(5A^{-1}) &= \text{span}( (3,4,0) ) =  \ker(5A)^\perp\\
\text{range}(5B) &= \text{span}( (0,3,-4) ) = \ker(5B^{-1})^\perp\\
\text{range}(5B^{-1}) &= \text{span}( (0,3,4) ) = \ker(5B)^\perp.
\end{align*} 5A 5A^{-1} 5B 5B^{-1} 5A 5A^{-1} 5B 5B^{-1} F_5^3","['linear-algebra', 'matrices', 'group-theory', 'proof-explanation', 'rotations']"
5,Methodical way to form a basis,Methodical way to form a basis,,"If I have a basis $(3,1,0,0,0),(0,0,7,1,0),(0,0,0,0,1)$ for a subspace of $\mathbb{R}^{5}$ and want to extend this collection of vectors to a basis for $\mathbb{R}^{5}$ is there any methodical, or algorithmic way to do this besides logically picking two linearly independent vectors to extend the set to a basis?","If I have a basis for a subspace of and want to extend this collection of vectors to a basis for is there any methodical, or algorithmic way to do this besides logically picking two linearly independent vectors to extend the set to a basis?","(3,1,0,0,0),(0,0,7,1,0),(0,0,0,0,1) \mathbb{R}^{5} \mathbb{R}^{5}",[]
6,"If $V = \text{null}(\textsf{T}-\lambda\textsf{I}) \oplus \text{range}(\textsf{T}-\lambda\textsf{I})$, then prove that $\textsf{T}$ is diagonalizable","If , then prove that  is diagonalizable",V = \text{null}(\textsf{T}-\lambda\textsf{I}) \oplus \text{range}(\textsf{T}-\lambda\textsf{I}) \textsf{T},"If $\textsf{V}$ is a finite-dimensional complex vector space, $\textsf{T}\in\mathcal{L}(\textsf{V})^1$ , $\lambda$ is arbitrary in $\mathbb{C}$ and if $$\textsf{V} = \text{null}(\textsf{T}-\lambda\textsf{I})  \oplus \text{range}(\textsf{T}-\lambda\textsf{I})$$ then prove that $\textsf{T}$ is diagonalizable. Attempt : I am solving Axler's 3 edition book in Exercise $5c.$ The book hasn't introduced the Jordan normal form or the generalized eigenvectors. Could someone please give a direction to move ahead. Thanks a lot for your help. $^1$ $\mathcal{L}(\textsf{V})$ is the set of all linear maps from $\textsf{V}$ to itself.","If is a finite-dimensional complex vector space, , is arbitrary in and if then prove that is diagonalizable. Attempt : I am solving Axler's 3 edition book in Exercise The book hasn't introduced the Jordan normal form or the generalized eigenvectors. Could someone please give a direction to move ahead. Thanks a lot for your help. is the set of all linear maps from to itself.","\textsf{V} \textsf{T}\in\mathcal{L}(\textsf{V})^1 \lambda \mathbb{C} \textsf{V} = \text{null}(\textsf{T}-\lambda\textsf{I}) 
\oplus \text{range}(\textsf{T}-\lambda\textsf{I}) \textsf{T} 5c. ^1 \mathcal{L}(\textsf{V}) \textsf{V}","['linear-algebra', 'eigenvalues-eigenvectors', 'linear-transformations', 'diagonalization']"
7,"If $f_1,...,f_p$ are linear functionals on an $n$-dimensional vector space, show that there is $x \neq 0$ such that $f_{1}(x) = \cdots =f_{p}(x) = 0$","If  are linear functionals on an -dimensional vector space, show that there is  such that","f_1,...,f_p n x \neq 0 f_{1}(x) = \cdots =f_{p}(x) = 0","Here is the question : If $f_{1},\dots,f_{p}$ are linear functionals on an $n$ -dimensional vector space, $X$ , where $p < n$ , show that there is a vector $x \neq 0$ in $X$ such that $f_{1}(x) = \cdots =f_{p}(x) = 0$ . I really want to prove this by contradiction. I want to consider this function $x \mapsto (f_{1}(x),..., f_{p}(x))$ . I know for the dual basis, $X^*$ , there are Kroneker delta's but I do not know how to apply them properly. Is this the way to go? Thank you very much!","Here is the question : If are linear functionals on an -dimensional vector space, , where , show that there is a vector in such that . I really want to prove this by contradiction. I want to consider this function . I know for the dual basis, , there are Kroneker delta's but I do not know how to apply them properly. Is this the way to go? Thank you very much!","f_{1},\dots,f_{p} n X p < n x \neq 0 X f_{1}(x) = \cdots =f_{p}(x) = 0 x \mapsto (f_{1}(x),..., f_{p}(x)) X^*","['linear-algebra', 'functional-analysis', 'linear-transformations', 'dual-spaces']"
8,Show that for any subset $S \subset V$ that the span(S) is the smallest subspace of $V$ containing $S$.,Show that for any subset  that the span(S) is the smallest subspace of  containing .,S \subset V V S,"Here is my proof. After I show that the span(S) is a subspace of $V$ , I am wondering if my proof is rigorous enough and doesn't have any holes on it. I feel okay about my reasoning, but not fully confident. Did I skip a step or assume too much in my leaps of logic? Let the span(S) of any subset $S\subset V$ be denoted as span $(v_1, \dots v_m)$ for $v_i \in S$ . First we need to check if the span(S) is a subspace of $V$ . 0-vector: $0 = 0v_1 + \dots + 0v_m$ is in $S$ Closure under addition: $$(a_1v_1 + \dots + a_mv_m) +  (c_1v_1 + \dots + c_mv_m)\\ = (a_1+c_1)v_1 + \dots + (a_m+c_m)v_m$$ Closure under scalar multiplication for $\lambda \in \mathbb{F}$ : $$\lambda (a_1v_1 + \dots a_mv_m) = \lambda a_1v_1 + \dots + \lambda a_mv_m$$ Now by the given subspace properties of closure under addition and scalar multiplication, each $v_j$ is a linear combination of the span $(v_1, \dots , v_j)$ . Let $W$ be a subspace in $V$ that only contains a linear combination $x=a_1v_1 + \dots + a_mv_m$ for $v_i \in S$ . Then $W$ is the smallest subspace of $V$ that contains $S$ because every vector $v_i \in S$ can be created from the linear combination.","Here is my proof. After I show that the span(S) is a subspace of , I am wondering if my proof is rigorous enough and doesn't have any holes on it. I feel okay about my reasoning, but not fully confident. Did I skip a step or assume too much in my leaps of logic? Let the span(S) of any subset be denoted as span for . First we need to check if the span(S) is a subspace of . 0-vector: is in Closure under addition: Closure under scalar multiplication for : Now by the given subspace properties of closure under addition and scalar multiplication, each is a linear combination of the span . Let be a subspace in that only contains a linear combination for . Then is the smallest subspace of that contains because every vector can be created from the linear combination.","V S\subset V (v_1, \dots v_m) v_i \in S V 0 = 0v_1 + \dots + 0v_m S (a_1v_1 + \dots + a_mv_m) +  (c_1v_1 + \dots + c_mv_m)\\ = (a_1+c_1)v_1 + \dots + (a_m+c_m)v_m \lambda \in \mathbb{F} \lambda (a_1v_1 + \dots a_mv_m) = \lambda a_1v_1 + \dots + \lambda a_mv_m v_j (v_1, \dots , v_j) W V x=a_1v_1 + \dots + a_mv_m v_i \in S W V S v_i \in S","['linear-algebra', 'proof-verification']"
9,Matrix exponential bound,Matrix exponential bound,,"I am looking to find analogues for products of matrices of the scalar inequalities $$|1+x|\leq e^x,\qquad \big|\frac{1}{1+x}\big|\leq e^{-x+x^2},$$ which hold for $|x|\leq 1/2$ . Take $n,d\geq 1$ , $A_1,\ldots,A_n\in\mathbb{R}^{d\times d}$ such that $\|A_i\|\leq 1/2$ , where $\|\cdot\|$ stands for the operator norm. Are the following inequalities true? $$ \big\|\prod_{i=1}^n(I+A_i)\big\|\leq \big\|\exp\big(\sum_{i=1}^nA_i\big)\big\| $$ $$ \big\|\prod_{i=1}^n(I+A_i)^{-1}\big\|\leq\big\|\exp\big(\sum_{i=1}^n-A_i+A_i^2\big)\big\| $$","I am looking to find analogues for products of matrices of the scalar inequalities which hold for . Take , such that , where stands for the operator norm. Are the following inequalities true?","|1+x|\leq e^x,\qquad \big|\frac{1}{1+x}\big|\leq e^{-x+x^2}, |x|\leq 1/2 n,d\geq 1 A_1,\ldots,A_n\in\mathbb{R}^{d\times d} \|A_i\|\leq 1/2 \|\cdot\| 
\big\|\prod_{i=1}^n(I+A_i)\big\|\leq \big\|\exp\big(\sum_{i=1}^nA_i\big)\big\|
 
\big\|\prod_{i=1}^n(I+A_i)^{-1}\big\|\leq\big\|\exp\big(\sum_{i=1}^n-A_i+A_i^2\big)\big\|
","['linear-algebra', 'matrices', 'upper-lower-bounds', 'matrix-exponential']"
10,Sylvester's Criterion for tensors,Sylvester's Criterion for tensors,,"We know from linear algebra that if $A = (a_{ij})_{i,j=1}^n$ is a symmetric matrix, then $A$ is positive-definite if and only if all principal minors of $A$ are positive. If I say that a symmetric (covariant) $k$ -tensor $(T_{i_1\cdots i_k})_{i_1,\ldots, i_k=1}^n$ is positive-definite if for any non-zero vector $v = (v^1,\ldots, v^n)$ , we have that $$\sum T_{i_1\cdots i_k}v^{i_1}\cdots v^{i_k} > 0,$$ is there a Sylvester-like criterion for that? I imagine so, but it should be ugly. I know no references, though.","We know from linear algebra that if is a symmetric matrix, then is positive-definite if and only if all principal minors of are positive. If I say that a symmetric (covariant) -tensor is positive-definite if for any non-zero vector , we have that is there a Sylvester-like criterion for that? I imagine so, but it should be ugly. I know no references, though.","A = (a_{ij})_{i,j=1}^n A A k (T_{i_1\cdots i_k})_{i_1,\ldots, i_k=1}^n v = (v^1,\ldots, v^n) \sum T_{i_1\cdots i_k}v^{i_1}\cdots v^{i_k} > 0,","['linear-algebra', 'tensors', 'positive-definite']"
11,Which vectors are stretched the most in a transform?,Which vectors are stretched the most in a transform?,,I thought that it would be obvious that in a transform the eigenvectors are the ones that are stretched the most as those are the directions in which the matrix acts. But according to this short video and what others have said: https://youtube.com/watch?v=vs2sRvSzA3o that does not seem to be the case. I thought so beacuse repeated application of a transform on any vectors makes that vector converge to its largest eigenvalue vector. I'm having a hard time digesting that there are other vectors stretched more. Can someone explain me why it makes sense? Consider the classic case of a 2D sphere being deformed to an ellipse by a transform. How would I find the vectors then which would become the major and minor axis? [Please dont use SVD but solve more fundamentally through calculus or basic linear algebra],I thought that it would be obvious that in a transform the eigenvectors are the ones that are stretched the most as those are the directions in which the matrix acts. But according to this short video and what others have said: https://youtube.com/watch?v=vs2sRvSzA3o that does not seem to be the case. I thought so beacuse repeated application of a transform on any vectors makes that vector converge to its largest eigenvalue vector. I'm having a hard time digesting that there are other vectors stretched more. Can someone explain me why it makes sense? Consider the classic case of a 2D sphere being deformed to an ellipse by a transform. How would I find the vectors then which would become the major and minor axis? [Please dont use SVD but solve more fundamentally through calculus or basic linear algebra],,"['linear-algebra', 'abstract-algebra', 'matrices']"
12,Can $AB = \gamma BA$ for matrices $A$ and $B$,Can  for matrices  and,AB = \gamma BA A B,"For what values of $\gamma \in \mathbb{C}$ do there exist non-singular matrices $A , B \in \mathbb{C}^{n \times n}$ such that $$AB = \gamma BA \,?$$ So far what I have done shown that $\gamma$ must be an nth root of unity, by considering the determinant. $$det(AB) = det(\gamma BA)$$ $$det(A)det(B) = \gamma ^n det(B)det(A)$$ Now since both $A$ and $B$ are non singular we have $det(A) \neq 0$ and $det(B) \neq 0$ So: $$\gamma ^n =1$$ . I also know that $$tr(AB - \gamma BA)=0$$ $$tr(AB) - \gamma tr(BA)=0$$ $$tr(AB)\big(1-\gamma \big) = 0$$ Clearly we can assume $\gamma \neq 1$ since surely we can find $A$ and $B$ such that they commute so we conclude that $tr(AB) = tr(BA) = 0$ Now i'm thinking that we can find matrices $A$ and $B$ for any $\gamma = e^{\frac{2 \pi i}{n}}$ such that $AB = \gamma BA$ but I cannot think of a way of constructing them. Does anyone have any ideas? thanks in  advance!","For what values of do there exist non-singular matrices such that So far what I have done shown that must be an nth root of unity, by considering the determinant. Now since both and are non singular we have and So: . I also know that Clearly we can assume since surely we can find and such that they commute so we conclude that Now i'm thinking that we can find matrices and for any such that but I cannot think of a way of constructing them. Does anyone have any ideas? thanks in  advance!","\gamma \in \mathbb{C} A , B \in \mathbb{C}^{n \times n} AB = \gamma BA \,? \gamma det(AB) = det(\gamma BA) det(A)det(B) = \gamma ^n det(B)det(A) A B det(A) \neq 0 det(B) \neq 0 \gamma ^n =1 tr(AB - \gamma BA)=0 tr(AB) - \gamma tr(BA)=0 tr(AB)\big(1-\gamma \big) = 0 \gamma \neq 1 A B tr(AB) = tr(BA) = 0 A B \gamma = e^{\frac{2 \pi i}{n}} AB = \gamma BA","['linear-algebra', 'matrices']"
13,Write a homogeneous polynomial of degree $d$ as a sum of $d$-th power of linear polynomials,Write a homogeneous polynomial of degree  as a sum of -th power of linear polynomials,d d,"I learned that the Warning rank of a homogeneous polynomial $h\in \mathbb{C}[x_1, \cdots, x_n]_d$ of degree $d$ is defined by the smallest number of summands such that $h$ can be expressed as a sum of $d$ -th powers of linear polynomials. For example, $XY=(\frac{X}{2}+\frac{Y}{2})^2+(i\frac{X}{2}-i\frac{Y}{2})^2$ so the Warning rank of $XY$ is $2$ . But how do we know that any homogeneous polynomial of degree $d$ can be writte as a sum of $d$ -th powers of linear polynomials? I read a proof for the case of elementary symmetric polynomials, but I have no idea how to get the generalized result for arbitrary homogeneous polynomials. I know that the dimension of $\mathbb{C}[x_1, \cdots, x_n]_d$ is $\binom{n+d-1}{d}$ . I tried to find $\binom{n+d-1}{d}$ linearly independent polynomials each of which is a $d$ -th power of a linear polynomial, but there is no progress so far.","I learned that the Warning rank of a homogeneous polynomial of degree is defined by the smallest number of summands such that can be expressed as a sum of -th powers of linear polynomials. For example, so the Warning rank of is . But how do we know that any homogeneous polynomial of degree can be writte as a sum of -th powers of linear polynomials? I read a proof for the case of elementary symmetric polynomials, but I have no idea how to get the generalized result for arbitrary homogeneous polynomials. I know that the dimension of is . I tried to find linearly independent polynomials each of which is a -th power of a linear polynomial, but there is no progress so far.","h\in \mathbb{C}[x_1, \cdots, x_n]_d d h d XY=(\frac{X}{2}+\frac{Y}{2})^2+(i\frac{X}{2}-i\frac{Y}{2})^2 XY 2 d d \mathbb{C}[x_1, \cdots, x_n]_d \binom{n+d-1}{d} \binom{n+d-1}{d} d","['linear-algebra', 'combinatorics', 'polynomials', 'symmetric-polynomials']"
14,The eigenvalue of Lie bracket,The eigenvalue of Lie bracket,,"Set $R = M_n(\mathbb{C})$ and let $f_A : R \to R$ be a $\mathbb{C}$ -linear map such that $$ f_A(X) = [X,A] = XA - AX. $$ Obvious fact: Note that there are $a_{ij} \in \mathbb{C}$ such that $$ f_A^n (X) = \sum_{i+j=n} a_{ij} A^i X A^j. $$ Thus if $A$ is nilpotent then $f_A$ is nilpotent, obviously. But the assumption "" $A$ is nilpotent"" is too strong. I want to extend this fact. My prediction: Assume that $f_A$ has a non-zero eigenvalue $\lambda \in \mathbb{C}$ . Then there are $\beta,\gamma \in \mathbb{C}$ which are eigenvalues of $A$ such that $\beta - \gamma = \lambda$ My effort Assume that $n=2$ . Let $X$ be a non-zero element of $E(\lambda,f_A)$ . Then $X^k \in E(k\lambda, f_A)$ . So we get $X$ is nilpotent. So there is a $P \in GL_2(\mathbb{C})$ such that $$ \Lambda := PXP^{-1} =  \begin{pmatrix} 0 &1 \\ 0& 0 \end{pmatrix}. $$ Set $$ B=PAP^{-1} = \begin{pmatrix} a & b \\ c & d \end{pmatrix}. $$ Then $[\Lambda, B] = \lambda \Lambda$ implies that $c=0$ and $d-a = \lambda$ . So we obtain $$ \Phi_A(x) = \Phi_B(x) = (x-a)(x-a-\lambda). $$ My question: What is a proper extension of the ""obvious fact""? Is my prediction true? If so, how to prove ?","Set and let be a -linear map such that Obvious fact: Note that there are such that Thus if is nilpotent then is nilpotent, obviously. But the assumption "" is nilpotent"" is too strong. I want to extend this fact. My prediction: Assume that has a non-zero eigenvalue . Then there are which are eigenvalues of such that My effort Assume that . Let be a non-zero element of . Then . So we get is nilpotent. So there is a such that Set Then implies that and . So we obtain My question: What is a proper extension of the ""obvious fact""? Is my prediction true? If so, how to prove ?","R = M_n(\mathbb{C}) f_A : R \to R \mathbb{C} 
f_A(X) = [X,A] = XA - AX.
 a_{ij} \in \mathbb{C} 
f_A^n (X) = \sum_{i+j=n} a_{ij} A^i X A^j.
 A f_A A f_A \lambda \in \mathbb{C} \beta,\gamma \in \mathbb{C} A \beta - \gamma = \lambda n=2 X E(\lambda,f_A) X^k \in E(k\lambda, f_A) X P \in GL_2(\mathbb{C}) 
\Lambda := PXP^{-1} =  \begin{pmatrix} 0 &1 \\ 0& 0 \end{pmatrix}.
 
B=PAP^{-1} = \begin{pmatrix} a & b \\ c & d \end{pmatrix}.
 [\Lambda, B] = \lambda \Lambda c=0 d-a = \lambda 
\Phi_A(x) = \Phi_B(x) = (x-a)(x-a-\lambda).
","['linear-algebra', 'proof-writing', 'eigenvalues-eigenvectors']"
15,"Prove $v_1,v_2,\cdots,v_m $ are linear independent.",Prove  are linear independent.,"v_1,v_2,\cdots,v_m ","$\Bbb{E}^n$ is an Euclidean space with dimension $n$ . $v_0,v_1,\cdots,v_m\in \Bbb{E}^n,m\le n $ , and $(v_i,v_j)\lt0$ for $0\le i\ne j\le m$ . Prove $v_1,v_2,\cdots,v_m $ are linear independent. My try: I tried to make an argument like this, if $v_1,v_2$ are linearly dependent, then there exists $a_1$ such that $v_2=a_1v_1$ , then $(v_2,v_2)=a_1(v_2,v_1)$ . At first I thought that the left hand $\ge 0$ , the right hand $\lt0$ . But I notice that the constant $a_1$ could also be negative. So this contradiction failed. Any  hints would be helpful.","is an Euclidean space with dimension . , and for . Prove are linear independent. My try: I tried to make an argument like this, if are linearly dependent, then there exists such that , then . At first I thought that the left hand , the right hand . But I notice that the constant could also be negative. So this contradiction failed. Any  hints would be helpful.","\Bbb{E}^n n v_0,v_1,\cdots,v_m\in \Bbb{E}^n,m\le n  (v_i,v_j)\lt0 0\le i\ne j\le m v_1,v_2,\cdots,v_m  v_1,v_2 a_1 v_2=a_1v_1 (v_2,v_2)=a_1(v_2,v_1) \ge 0 \lt0 a_1",['linear-algebra']
16,Strang's Perron–Frobenius Proof,Strang's Perron–Frobenius Proof,,"I'm stuck on the first paragraph in Strang's proof of (part of) the Perron–Frobenius theorem, from his Introduction to Linear Algebra . Why can we assume $t_{\text{max}}$ exists, what guarantees that a maximum of the $t$ 's ""is attained""? For a given nonnegative $\mathbf{x}$ , I see why there must be a maximum $t$ such that $A \mathbf{x} \geq t \mathbf{x}$ . We just start with $t = 0$ and increase it until $t x_i = (Ax)_i$ for some $i$ . But there might be many $\mathbf{x}$ that satisfy $A \mathbf{x} \geq t\mathbf{x}$ for some $t$ . And if there are infinitely many, there may be infinitely many maximal $t$ 's to choose from, one for each $\mathbf{x}$ . In which case I don't see how to guarantee that there's one $t_{\text{max}}$ to rule them all.","I'm stuck on the first paragraph in Strang's proof of (part of) the Perron–Frobenius theorem, from his Introduction to Linear Algebra . Why can we assume exists, what guarantees that a maximum of the 's ""is attained""? For a given nonnegative , I see why there must be a maximum such that . We just start with and increase it until for some . But there might be many that satisfy for some . And if there are infinitely many, there may be infinitely many maximal 's to choose from, one for each . In which case I don't see how to guarantee that there's one to rule them all.",t_{\text{max}} t \mathbf{x} t A \mathbf{x} \geq t \mathbf{x} t = 0 t x_i = (Ax)_i i \mathbf{x} A \mathbf{x} \geq t\mathbf{x} t t \mathbf{x} t_{\text{max}},"['linear-algebra', 'matrices', 'eigenvalues-eigenvectors']"
17,Do isometries of regular convex polytopes generate everything?,Do isometries of regular convex polytopes generate everything?,,"Let $D$ be a regular convex $d$ -dimensional polytope that is symmetric $(D=-D$ ) in $\mathbb R^d$ centred at the origin. Let $G$ be the group of all isometries of $D$ , that is, linear maps/matrices $T\in M_d$ such that $T[D]=D$ . Does the linear span of $G$ in $M_d$ generate everything, that is, ${\rm span}\, G = M_d$ , the space of all $d\times d$ -matrices? It is the case for $d$ -dimensional cubes and octahedrons but is it true in general?","Let be a regular convex -dimensional polytope that is symmetric ) in centred at the origin. Let be the group of all isometries of , that is, linear maps/matrices such that . Does the linear span of in generate everything, that is, , the space of all -matrices? It is the case for -dimensional cubes and octahedrons but is it true in general?","D d (D=-D \mathbb R^d G D T\in M_d T[D]=D G M_d {\rm span}\, G = M_d d\times d d","['linear-algebra', 'matrices', 'polyhedra', 'polytopes', 'linear-groups']"
18,Can we reduce finding matrix roots to finding roots of Jordan blocks?,Can we reduce finding matrix roots to finding roots of Jordan blocks?,,"I just found some interesting question about matrix square roots and I came to think of one way to find them, or at least reduce them to a set of simpler problems. Assume we have a matrix $\bf A$ and it can be put on some Jordan form: $${\bf A} = {\bf SJS}^{-1}$$ Where $\bf J$ is block-diagonal consisting of famous Jordan blocks of the shape: $${\bf J_{k}} = \begin{bmatrix} \lambda_k&1&0&\cdots&0\\0&\ddots&\ddots& & 0\\\vdots&\ddots&\ddots&\ddots&\vdots\\0&\cdots&0&\lambda_k&1\\0&\cdots&\cdots&0&\lambda_k\end{bmatrix}$$ In other words main diagonal full with eigenvalue $\lambda_k$ and first off-diagonal filled with ones. The problem of finding some n'th root to $\bf A$ can now be written $${\bf SJ}^{1/n}{\bf S}^{-1}$$ (Why?). So If I am correct so far .. we have reduced down to find some way of calculating square root of such Jordan blocks $\bf J_k$ , and in simplest case blocks of dimensionality 1, finding some root over our scalar field (for the eigenvalues themselves). Firstly, is this reasoning correct so far? Secondly, how can we approach finding square root to matrices of the form $\bf J_k$ . Is there some simplification or short-cut that can be done?","I just found some interesting question about matrix square roots and I came to think of one way to find them, or at least reduce them to a set of simpler problems. Assume we have a matrix and it can be put on some Jordan form: Where is block-diagonal consisting of famous Jordan blocks of the shape: In other words main diagonal full with eigenvalue and first off-diagonal filled with ones. The problem of finding some n'th root to can now be written (Why?). So If I am correct so far .. we have reduced down to find some way of calculating square root of such Jordan blocks , and in simplest case blocks of dimensionality 1, finding some root over our scalar field (for the eigenvalues themselves). Firstly, is this reasoning correct so far? Secondly, how can we approach finding square root to matrices of the form . Is there some simplification or short-cut that can be done?",\bf A {\bf A} = {\bf SJS}^{-1} \bf J {\bf J_{k}} = \begin{bmatrix} \lambda_k&1&0&\cdots&0\\0&\ddots&\ddots& & 0\\\vdots&\ddots&\ddots&\ddots&\vdots\\0&\cdots&0&\lambda_k&1\\0&\cdots&\cdots&0&\lambda_k\end{bmatrix} \lambda_k \bf A {\bf SJ}^{1/n}{\bf S}^{-1} \bf J_k \bf J_k,"['linear-algebra', 'matrices', 'polynomials', 'jordan-normal-form']"
19,Write a bivector as the exterior product of two vectors,Write a bivector as the exterior product of two vectors,,"The Wikipedia article https://en.wikipedia.org/wiki/Bivector#Simple_bivectors states that ""A bivector that can be written as the exterior product of two vectors is simple. In two and three dimensions all bivectors are simple."" This implies that if we have a bivector in 3D given by $$B :=\alpha(e_1\wedge e_2) + \beta(e_2\wedge e_3) + \gamma(e_3\wedge e_1)$$ then there are the two vectors $v, w$ such that $B = v \wedge w$ . I am wondering how we can construct such $v, w$ from $\alpha, \beta, \gamma$ ? I understand that there is not a unique choice of $v$ and $w$ however is there a 'nicest' choice? One that is the most symmetric? I attempted the following manipulation on the definition of $B$ : $$\begin{eqnarray}B &=&\alpha(e_1\wedge e_2) + \beta(e_2\wedge e_3) + \gamma(e_3\wedge e_1) \\ &=&\alpha(e_1\wedge e_2) - \beta(e_3\wedge e_2) - \gamma(e_1\wedge e_3) \\ &=&  (\alpha e_1 - \beta e_3)\wedge e_2 - \left(\frac{\gamma}{\alpha}\right)(\alpha e_1 \wedge e_3) +  \frac{\beta\gamma}{\alpha}(e_3\wedge e_3)\\ &=& (\alpha e_1 - \beta e_3)\wedge e_2 - (\alpha e_1) \wedge\left(\frac{\gamma}{\alpha}e_3\right) +  (\beta e_3)\wedge\left(\frac{\gamma}{\alpha} e_3\right) \\ &=& (\alpha e_1 - \beta e_3)\wedge e_2 - (\alpha e_1 - \beta e_3) \wedge\left(\frac{\gamma}{\alpha}e_3\right) \\ &=& (\alpha e_1 - \beta e_3)\wedge \left(e_2 - \frac{\gamma}{\alpha}e_3\right) \end{eqnarray}$$ This shows that the decomposition of a 3D bivector into the wedge product of two vectors is possible (and thus that every 3D bivector is simple) however it is not a very satisfying end result in that it is not symmetric and doesn't offer any insight into the nature of the decomposition. For example, a nicer decomposition would be one of the form $$(ae_1 + be_2 + ce_3)\wedge (a'e_1 + b'e_2 + c'e_3)$$ in which there is some symmetry in the coefficients $a, b, c, a', b', c'$ .","The Wikipedia article https://en.wikipedia.org/wiki/Bivector#Simple_bivectors states that ""A bivector that can be written as the exterior product of two vectors is simple. In two and three dimensions all bivectors are simple."" This implies that if we have a bivector in 3D given by then there are the two vectors such that . I am wondering how we can construct such from ? I understand that there is not a unique choice of and however is there a 'nicest' choice? One that is the most symmetric? I attempted the following manipulation on the definition of : This shows that the decomposition of a 3D bivector into the wedge product of two vectors is possible (and thus that every 3D bivector is simple) however it is not a very satisfying end result in that it is not symmetric and doesn't offer any insight into the nature of the decomposition. For example, a nicer decomposition would be one of the form in which there is some symmetry in the coefficients .","B :=\alpha(e_1\wedge e_2) + \beta(e_2\wedge e_3) + \gamma(e_3\wedge e_1) v, w B = v \wedge w v, w \alpha, \beta, \gamma v w B \begin{eqnarray}B &=&\alpha(e_1\wedge e_2) + \beta(e_2\wedge e_3) + \gamma(e_3\wedge e_1) \\ &=&\alpha(e_1\wedge e_2) - \beta(e_3\wedge e_2) - \gamma(e_1\wedge e_3) \\ &=&  (\alpha e_1 - \beta e_3)\wedge e_2 - \left(\frac{\gamma}{\alpha}\right)(\alpha e_1 \wedge e_3) +  \frac{\beta\gamma}{\alpha}(e_3\wedge e_3)\\ &=& (\alpha e_1 - \beta e_3)\wedge e_2 - (\alpha e_1) \wedge\left(\frac{\gamma}{\alpha}e_3\right) +  (\beta e_3)\wedge\left(\frac{\gamma}{\alpha} e_3\right) \\ &=& (\alpha e_1 - \beta e_3)\wedge e_2 - (\alpha e_1 - \beta e_3) \wedge\left(\frac{\gamma}{\alpha}e_3\right) \\ &=& (\alpha e_1 - \beta e_3)\wedge \left(e_2 - \frac{\gamma}{\alpha}e_3\right)
\end{eqnarray} (ae_1 + be_2 + ce_3)\wedge (a'e_1 + b'e_2 + c'e_3) a, b, c, a', b', c'","['linear-algebra', 'exterior-algebra', 'clifford-algebras', 'geometric-algebras', 'outer-product']"
20,Levi-Civita & Kronecker delta identity,Levi-Civita & Kronecker delta identity,,"I am trying to prove the following identity: $\epsilon^{ijk}\epsilon_{pqk}=\delta_p^i\delta_q^j-\delta_p^j\delta_q^i$ Starting from the following identity: $\epsilon^{ijk}\epsilon_{pqr}=\begin{vmatrix} \delta^i_p & \delta^i_q & \delta^i_r \\ \delta^j_p & \delta^j_q & \delta^j_r \\ \delta^k_p & \delta^k_q & \delta^k_r \\ \end{vmatrix}$ But, when I expand out the matrix, I end up with: $\epsilon^{ijk}\epsilon_{pqr}=\delta_p^i(\delta_q^j\delta_r^k-\delta_q^k\delta_r^j)-\delta_q^i(\delta_p^j\delta_r^k-\delta_p^k\delta_r^j)+\delta_r^i(\delta_p^j\delta_q^k-\delta_p^k\delta_q^j)$ Contracting by setting r=k, I obtained: $\epsilon^{ijk}\epsilon_{pqr}=\delta_p^i(\delta_q^j\delta_k^k-\delta_q^k\delta_k^j)-\delta_q^i(\delta_p^j\delta_k^k-\delta_p^k\delta_k^j)+\delta_k^i(\delta_p^j\delta_q^k-\delta_p^k\delta_q^j)$ Then: $\epsilon^{ijk}\epsilon_{pqr}=\delta_p^i(\delta_q^j-\delta_q^j)-\delta_q^i(\delta_p^j-\delta_p^j)+(\delta_p^j\delta_q^i-\delta_p^i\delta_q^j)$ So finally: $\epsilon^{ijk}\epsilon_{pqr}=\delta_p^j\delta_q^i-\delta_p^i\delta_q^j$ Which is the identity I'm trying to prove, except that the right side is multiplied by -1 for some reason. I must be doing something wrong along the way, but I haven't been able to find it. Any ideas? EDIT: A similar question has been answered elsewhere , but it is only when expanding the determinant that I arrive at this problem. For that reason, I decided to make a separate post about this. EDIT 2: Added intermediate steps. FINAL EDIT: As Travis pointed out, I incorrectly replaced $\delta_k^k$ by $1$ instead of $3$ . After correcting this, I arrived at the correct solution: $\epsilon^{ijk}\epsilon_{pqr}=\delta_p^i(3\delta_q^j-\delta_q^j)-\delta_q^i(3\delta_p^j-\delta_p^j)+(\delta_p^j\delta_q^i-\delta_p^i\delta_q^j)=\delta_p^i\delta_q^j-\delta_p^j\delta_q^i$","I am trying to prove the following identity: Starting from the following identity: But, when I expand out the matrix, I end up with: Contracting by setting r=k, I obtained: Then: So finally: Which is the identity I'm trying to prove, except that the right side is multiplied by -1 for some reason. I must be doing something wrong along the way, but I haven't been able to find it. Any ideas? EDIT: A similar question has been answered elsewhere , but it is only when expanding the determinant that I arrive at this problem. For that reason, I decided to make a separate post about this. EDIT 2: Added intermediate steps. FINAL EDIT: As Travis pointed out, I incorrectly replaced by instead of . After correcting this, I arrived at the correct solution:","\epsilon^{ijk}\epsilon_{pqk}=\delta_p^i\delta_q^j-\delta_p^j\delta_q^i \epsilon^{ijk}\epsilon_{pqr}=\begin{vmatrix}
\delta^i_p & \delta^i_q & \delta^i_r \\
\delta^j_p & \delta^j_q & \delta^j_r \\
\delta^k_p & \delta^k_q & \delta^k_r \\
\end{vmatrix} \epsilon^{ijk}\epsilon_{pqr}=\delta_p^i(\delta_q^j\delta_r^k-\delta_q^k\delta_r^j)-\delta_q^i(\delta_p^j\delta_r^k-\delta_p^k\delta_r^j)+\delta_r^i(\delta_p^j\delta_q^k-\delta_p^k\delta_q^j) \epsilon^{ijk}\epsilon_{pqr}=\delta_p^i(\delta_q^j\delta_k^k-\delta_q^k\delta_k^j)-\delta_q^i(\delta_p^j\delta_k^k-\delta_p^k\delta_k^j)+\delta_k^i(\delta_p^j\delta_q^k-\delta_p^k\delta_q^j) \epsilon^{ijk}\epsilon_{pqr}=\delta_p^i(\delta_q^j-\delta_q^j)-\delta_q^i(\delta_p^j-\delta_p^j)+(\delta_p^j\delta_q^i-\delta_p^i\delta_q^j) \epsilon^{ijk}\epsilon_{pqr}=\delta_p^j\delta_q^i-\delta_p^i\delta_q^j \delta_k^k 1 3 \epsilon^{ijk}\epsilon_{pqr}=\delta_p^i(3\delta_q^j-\delta_q^j)-\delta_q^i(3\delta_p^j-\delta_p^j)+(\delta_p^j\delta_q^i-\delta_p^i\delta_q^j)=\delta_p^i\delta_q^j-\delta_p^j\delta_q^i","['linear-algebra', 'tensors']"
21,Is this subset of $M_4(\mathbb R)$ connected?,Is this subset of  connected?,M_4(\mathbb R),"Let us consider an affine structure $\star$ of $M_4(\mathbb R)$ which has following form \begin{align*} \begin{pmatrix} 0 & * & 0 & * \\ 1 & * & 0 & * \\ 0 & * & 0 & * \\ 0 & * & 1 & * \end{pmatrix}, \end{align*} where $*$ can assume any real number. It is also clear for any monic $4^{th}$ degree real polynomial, we can at least find one realization in $\star$ since the upper left block and lower right block can be considered as in companion form. We further concern this set \begin{align*} \mathcal E = \{ A \in \star: \max_i \left( \lambda_i(A) \right) < 0 \}. \end{align*} In other words, all elements in $\mathcal E$ has above defined structure and all eigenvalues on the left open half plane. I am trying to determine whether the set $\mathcal E$ is connected. My first try was to determine for a fixed monic polynomial, whether all realizations in $\star$ is connected. If this is true, for any $A_1, A_2 \in \mathcal E$ , we may first connect them to a companion form in $\star$ (by making the $32$ entry to be $1$ and other entries in the second column to be $0$ ) without changing the eigenvalues, and then the companion forms are connected as a consequence of property of polynomials. But as the question I asked a while ago, this is only true if it has at least one real eigenvalue. I strongly believe the set is connected. The question is related but I am not sure it is that related. Because the condition there is much more strict. I was asking to connect all realizations in $\star$ yielding the same characteristic equation without changing its eigenvalues. Here we allow this to vary inside $\mathcal E$ .","Let us consider an affine structure of which has following form where can assume any real number. It is also clear for any monic degree real polynomial, we can at least find one realization in since the upper left block and lower right block can be considered as in companion form. We further concern this set In other words, all elements in has above defined structure and all eigenvalues on the left open half plane. I am trying to determine whether the set is connected. My first try was to determine for a fixed monic polynomial, whether all realizations in is connected. If this is true, for any , we may first connect them to a companion form in (by making the entry to be and other entries in the second column to be ) without changing the eigenvalues, and then the companion forms are connected as a consequence of property of polynomials. But as the question I asked a while ago, this is only true if it has at least one real eigenvalue. I strongly believe the set is connected. The question is related but I am not sure it is that related. Because the condition there is much more strict. I was asking to connect all realizations in yielding the same characteristic equation without changing its eigenvalues. Here we allow this to vary inside .","\star M_4(\mathbb R) \begin{align*}
\begin{pmatrix}
0 & * & 0 & * \\
1 & * & 0 & * \\
0 & * & 0 & * \\
0 & * & 1 & *
\end{pmatrix},
\end{align*} * 4^{th} \star \begin{align*}
\mathcal E = \{ A \in \star: \max_i \left( \lambda_i(A) \right) < 0 \}.
\end{align*} \mathcal E \mathcal E \star A_1, A_2 \in \mathcal E \star 32 1 0 \star \mathcal E","['linear-algebra', 'general-topology', 'path-connected']"
22,Number of possible zero entries in orthogonal matrices,Number of possible zero entries in orthogonal matrices,,"It's easy to check that in an orthogonal  matrix $Q$ dimension $2 \times 2$ if there is entry $0$ in the matrix then necessary one additional zero must be present and the total number of zeros is $2$ . In an orthogonal matrix  dim. $3 \times 3$ number of zeros can be (if they are present) , I suppose from observations, only $4$ or $6$ - once again we obtain an even  number of possible zeros. Examples: $ \begin{bmatrix}    0.6 & -0.8  & 0 \\ 0.8 & 0.6 & 0 \\ 0 & 0 & 1 \\  \end{bmatrix} \ \ $ , $ \ \ \begin{bmatrix}   0 & 0 & 1 \\ 1 & 0  & 0 \\ 0 & 1 & 0 \\ \end{bmatrix}$ Can this observation be extended for other orthogonal matrices of greater dimensions?  The number of zeros is always even? How to prove this? Maybe, it is known the explicit formula for the  number of possible zeros in orthogonal matrices of any dimension?","It's easy to check that in an orthogonal  matrix dimension if there is entry in the matrix then necessary one additional zero must be present and the total number of zeros is . In an orthogonal matrix  dim. number of zeros can be (if they are present) , I suppose from observations, only or - once again we obtain an even  number of possible zeros. Examples: , Can this observation be extended for other orthogonal matrices of greater dimensions?  The number of zeros is always even? How to prove this? Maybe, it is known the explicit formula for the  number of possible zeros in orthogonal matrices of any dimension?","Q 2 \times 2 0 2 3 \times 3 4 6  \begin{bmatrix}   
0.6 & -0.8  & 0 \\ 0.8 & 0.6 & 0 \\ 0 & 0 & 1 \\ 
\end{bmatrix} \ \   \ \ \begin{bmatrix}   0 & 0 & 1 \\
1 & 0  & 0 \\ 0 & 1 & 0 \\
\end{bmatrix}","['linear-algebra', 'matrices', 'orthogonal-matrices']"
23,"Prove that $\langle\mathbf{A}, \mathbf{C}\rangle \leq \delta$ equals with $\|\mathbf{A}\|_*\leq\delta$ [closed]",Prove that  equals with  [closed],"\langle\mathbf{A}, \mathbf{C}\rangle \leq \delta \|\mathbf{A}\|_*\leq\delta","Closed. This question does not meet Mathematics Stack Exchange guidelines . It is not currently accepting answers. Please provide additional context , which ideally explains why the question is relevant to you and our  community. Some forms of context include: background and motivation, relevant definitions, source, possible strategies, your current progress, why the question is interesting or important, etc. Closed 5 years ago . Improve this question Given an arbitrary matrix $\mathbf{A}\in R^{n\times n}$ and the basis matrix set $\mathbb{S}=\{\mathbf{C}\in R^{n\times n}: \mathbf{C}^T\mathbf{C}=\mathbf{I}_n\}$ .","Closed. This question does not meet Mathematics Stack Exchange guidelines . It is not currently accepting answers. Please provide additional context , which ideally explains why the question is relevant to you and our  community. Some forms of context include: background and motivation, relevant definitions, source, possible strategies, your current progress, why the question is interesting or important, etc. Closed 5 years ago . Improve this question Given an arbitrary matrix and the basis matrix set .",\mathbf{A}\in R^{n\times n} \mathbb{S}=\{\mathbf{C}\in R^{n\times n}: \mathbf{C}^T\mathbf{C}=\mathbf{I}_n\},"['calculus', 'linear-algebra', 'matrices']"
24,Geometric multiplicity of the largest eigenvalue,Geometric multiplicity of the largest eigenvalue,,"Let $$ A= \begin{bmatrix}  a  &  2f  &  0  \\  2f &  b   & 3f  \\   0 &  3f  &  c   \end{bmatrix}, $$ where $a$ , $b$ , $c$ , $f$ are real numbers and $f\neq 0$ . Find the geometric multiplicity of the largest eigenvalue of $A$ . I don't think I have to use the characteristic equation. Or do I?","Let where , , , are real numbers and . Find the geometric multiplicity of the largest eigenvalue of . I don't think I have to use the characteristic equation. Or do I?","
A= \begin{bmatrix}
 a  &  2f  &  0  \\
 2f &  b   & 3f  \\
  0 &  3f  &  c  
\end{bmatrix},
 a b c f f\neq 0 A","['linear-algebra', 'eigenvalues-eigenvectors']"
25,A curious observation regarding eigenvectors of $3 \times 3$ matrices - Hoffman and Kunze's *Linear Algebra*,A curious observation regarding eigenvectors of  matrices - Hoffman and Kunze's *Linear Algebra*,3 \times 3,"I am reading Hoffman and Kunze's Linear Algebra , 2nd ed., and I made a curious observation in a couple of the examples relating to computing eigenvalues and eigenvectors in Chapter 6. In Example 2 on pages 184-185, we have the (real) $3 \times 3$ matrix $$ A = \begin{bmatrix} 3 & 1 & -1\\ 2 & 2 & -1\\ 2 & 2 & \phantom{-}0 \end{bmatrix}. $$ The characteristic polynomial for $A$ is $(x-1)(x-2)^2$ . Thus, the characteristic values of $A$ are $1$ and $2$ . We have $$ \begin{align} A - I &=  \begin{bmatrix} 2 & 1 & -1\\ 2 & 1 & -1\\ 2 & 2 & -1 \end{bmatrix}\\\\ A - 2I &=  \begin{bmatrix} 1 & 1 & -1\\ 2 & 0 & -1\\ 2 & 2 & -2 \end{bmatrix}. \end{align} $$ The characteristic spaces associated to each characteristic value is one-dimensional in this case. The vector $\alpha_1 = (1,0,2)$ spans the null space of $T - I$ and the vector $\alpha_2 = (1,1,2)$ spans the null space of $T - 2I$ . Here, my observation is that $\alpha_1$ is the middle column vector of $A - 2I$ , and $\alpha_2$ is the middle column vector of $A - I$ . A similar thing happens in Example 3 (pages 187-188): $T$ is the linear operator on $\Bbb{R}^3$ which is represented in the standard ordered basis by the matrix $$ A = \begin{bmatrix} \phantom{-}5 & -6 & -6 \\ -1 & \phantom{-}4 & \phantom{-}2 \\ \phantom{-}3 & -6 & -4 \end{bmatrix}. $$ The characteristic polynomial is computed to be $(x-2)^2(x-1)$ . Then, we have $$ \begin{align} A - I &= \begin{bmatrix} \phantom{-}4 & -6 & -6 \\ -1 & \phantom{-}3 & \phantom{-}2 \\ \phantom{-}3 & -6 & -5 \end{bmatrix}\\\\ A - 2I &= \begin{bmatrix} \phantom{-}3 & -6 & -6 \\ -1 & \phantom{-}2 & \phantom{-}2 \\ \phantom{-}3 & -6 & -6 \end{bmatrix}. \end{align} $$ The null space of $T-I$ is one-dimensional and the null space of $T-2I$ is two-dimensional. The vector $\alpha_1 = (3,-1,3)$ spans the null space of $T-I$ . The null space of $T-2I$ consists of the vectors $(x_1,x_2,x_3)$ with $x_1 = 2x_2 + 2x_3$ , so the authors give an example of a basis of the null space of $T-2I$ as $$\begin{align}\alpha_2 &= (2,1,0)\\ \alpha_3 &= (2,0,1).\end{align}$$ However, we can also take $$\begin{align}\alpha_2 &= (-6,3,-6)\\ \alpha_3 &= (-6,2,-5)\end{align}$$ and we see again that $\alpha_1$ is the first column of $A - 2I$ and $\alpha_2,\alpha_3$ are the second and third columns of $A - I$ . I find this quite curious, more so since the authors don't mention this observation at all. Is there a simple explanation for why this is happening, and can this observation be used to quickly find eigenvectors of linear transformations?","I am reading Hoffman and Kunze's Linear Algebra , 2nd ed., and I made a curious observation in a couple of the examples relating to computing eigenvalues and eigenvectors in Chapter 6. In Example 2 on pages 184-185, we have the (real) matrix The characteristic polynomial for is . Thus, the characteristic values of are and . We have The characteristic spaces associated to each characteristic value is one-dimensional in this case. The vector spans the null space of and the vector spans the null space of . Here, my observation is that is the middle column vector of , and is the middle column vector of . A similar thing happens in Example 3 (pages 187-188): is the linear operator on which is represented in the standard ordered basis by the matrix The characteristic polynomial is computed to be . Then, we have The null space of is one-dimensional and the null space of is two-dimensional. The vector spans the null space of . The null space of consists of the vectors with , so the authors give an example of a basis of the null space of as However, we can also take and we see again that is the first column of and are the second and third columns of . I find this quite curious, more so since the authors don't mention this observation at all. Is there a simple explanation for why this is happening, and can this observation be used to quickly find eigenvectors of linear transformations?","3 \times 3 
A = \begin{bmatrix}
3 & 1 & -1\\
2 & 2 & -1\\
2 & 2 & \phantom{-}0
\end{bmatrix}.
 A (x-1)(x-2)^2 A 1 2 
\begin{align}
A - I &= 
\begin{bmatrix}
2 & 1 & -1\\
2 & 1 & -1\\
2 & 2 & -1
\end{bmatrix}\\\\
A - 2I &= 
\begin{bmatrix}
1 & 1 & -1\\
2 & 0 & -1\\
2 & 2 & -2
\end{bmatrix}.
\end{align}
 \alpha_1 = (1,0,2) T - I \alpha_2 = (1,1,2) T - 2I \alpha_1 A - 2I \alpha_2 A - I T \Bbb{R}^3 
A =
\begin{bmatrix}
\phantom{-}5 & -6 & -6 \\
-1 & \phantom{-}4 & \phantom{-}2 \\
\phantom{-}3 & -6 & -4
\end{bmatrix}.
 (x-2)^2(x-1) 
\begin{align}
A - I &=
\begin{bmatrix}
\phantom{-}4 & -6 & -6 \\
-1 & \phantom{-}3 & \phantom{-}2 \\
\phantom{-}3 & -6 & -5
\end{bmatrix}\\\\
A - 2I &=
\begin{bmatrix}
\phantom{-}3 & -6 & -6 \\
-1 & \phantom{-}2 & \phantom{-}2 \\
\phantom{-}3 & -6 & -6
\end{bmatrix}.
\end{align}
 T-I T-2I \alpha_1 = (3,-1,3) T-I T-2I (x_1,x_2,x_3) x_1 = 2x_2 + 2x_3 T-2I \begin{align}\alpha_2 &= (2,1,0)\\ \alpha_3 &= (2,0,1).\end{align} \begin{align}\alpha_2 &= (-6,3,-6)\\ \alpha_3 &= (-6,2,-5)\end{align} \alpha_1 A - 2I \alpha_2,\alpha_3 A - I","['linear-algebra', 'matrices']"
26,The product of two zero-row-sum matrices,The product of two zero-row-sum matrices,,"Suppose $A\in Mat_{m\times n}(\mathbb{R}), B\in Mat_{n\times r}(\mathbb{R}).$ and their row sums are $0$ , i.e. $\displaystyle\sum_jA_{ij}=\sum_j B_{ij}=0.$ Suppose $AB=C$ . Show that the row sum of $C$ is still $0$ . My attempt: Suppose $A\in Mat_{m\times n}(\mathbb{R})$ is an arbitrary matrix and $B\in Mat_{n\times r}(\mathbb{R})$ is the matrix that the row sum of $B$ is $0$ . The $ith$ row sum is $$\begin{align}\displaystyle\sum_{j=1}^r (AB)_{ij} &=\sum_{j=1}^r\sum_{k=1}^n a_{ik}b_{kj} \\&=\sum_{k=1}^n \sum_{j=1}^r a_{ik}b_{kj} \\&=\sum_k^na_{ik}(\sum_j^r b_{kj}) \\& =\sum_k^n a_{ik}*0,~~~~\text{since the row sum of $B$ is $0$}.\end{align}$$ So, the row sum of product $AB$ is $0$ . So, if the row sums of both two matrices are $0$ , then the row sum of their product $C$ is also $0$ (because we can always have a zero-row-sum matrix on the right). Is my proof correct? Can someone check it for me? Thank you very much.","Suppose and their row sums are , i.e. Suppose . Show that the row sum of is still . My attempt: Suppose is an arbitrary matrix and is the matrix that the row sum of is . The row sum is So, the row sum of product is . So, if the row sums of both two matrices are , then the row sum of their product is also (because we can always have a zero-row-sum matrix on the right). Is my proof correct? Can someone check it for me? Thank you very much.","A\in Mat_{m\times n}(\mathbb{R}), B\in Mat_{n\times r}(\mathbb{R}). 0 \displaystyle\sum_jA_{ij}=\sum_j B_{ij}=0. AB=C C 0 A\in Mat_{m\times n}(\mathbb{R}) B\in Mat_{n\times r}(\mathbb{R}) B 0 ith \begin{align}\displaystyle\sum_{j=1}^r (AB)_{ij} &=\sum_{j=1}^r\sum_{k=1}^n a_{ik}b_{kj} \\&=\sum_{k=1}^n \sum_{j=1}^r a_{ik}b_{kj} \\&=\sum_k^na_{ik}(\sum_j^r b_{kj}) \\& =\sum_k^n a_{ik}*0,~~~~\text{since the row sum of B is 0}.\end{align} AB 0 0 C 0","['linear-algebra', 'matrices']"
27,Simple Property of Determinant Functions,Simple Property of Determinant Functions,,"Okay, so I am in a linear algebra course and we are going though the derivation of the determinant function for matrices. I am struggling with some of the properties of determinant functions i.e. functions which are n-linear, alternating, and give a 1 for the identity matrix. The question I am currently stuck on is as follows. Let $K$ be a commutative ring with identity and $D$ an $n$ -linear function on $n\times n$ matrices over $K$ . Show that $D(B)=D(A)$ , if $B$ is obtained from $A$ by adding a scalar multiple of one row of $A$ to another. What I have so far is as follows. Let $A$ be an $n\times n$ matrix. Let $A=(\alpha_1, \cdots,\alpha_i,\cdots,\alpha_j,\cdots, \alpha_n)$ where $a_k$ , $k=1,...,n$ denote the rows of $A$ . Let $B=(\alpha_1,\cdots,a\alpha_i+\alpha_j,\cdots,\alpha_n)$ for $a\in K$ . That is let $B$ be the matrix formed from $A$ where the $i^{th}$ row of $A$ is replaced by $a$ times the $i^{th}$ of $A$ row plus the $j^{th}$ row of $A$ . Then $D(B)=D(\alpha_1,\cdots,a\alpha_i+\alpha_j,\cdots,\alpha_n)=aD(\alpha_1,\cdots,\alpha_i,\cdots,\alpha_j,\cdots,\alpha_n)+D(\alpha_1,\cdots,\alpha_j,\cdots,\alpha_j,\cdots,\alpha_n)\text{($n$-linearity) }=aD(A)+0=aD(A).$ Therefore $D(B)=aD(A)$ I am clearly missing something obvious because the $a$ should not be there according to the question but for the life of me I can't see what is going on. I am assuming it is something obvious and easy but I have been stuck for far too long. Thanks.","Okay, so I am in a linear algebra course and we are going though the derivation of the determinant function for matrices. I am struggling with some of the properties of determinant functions i.e. functions which are n-linear, alternating, and give a 1 for the identity matrix. The question I am currently stuck on is as follows. Let be a commutative ring with identity and an -linear function on matrices over . Show that , if is obtained from by adding a scalar multiple of one row of to another. What I have so far is as follows. Let be an matrix. Let where , denote the rows of . Let for . That is let be the matrix formed from where the row of is replaced by times the of row plus the row of . Then Therefore I am clearly missing something obvious because the should not be there according to the question but for the life of me I can't see what is going on. I am assuming it is something obvious and easy but I have been stuck for far too long. Thanks.","K D n n\times n K D(B)=D(A) B A A A n\times n A=(\alpha_1, \cdots,\alpha_i,\cdots,\alpha_j,\cdots, \alpha_n) a_k k=1,...,n A B=(\alpha_1,\cdots,a\alpha_i+\alpha_j,\cdots,\alpha_n) a\in K B A i^{th} A a i^{th} A j^{th} A D(B)=D(\alpha_1,\cdots,a\alpha_i+\alpha_j,\cdots,\alpha_n)=aD(\alpha_1,\cdots,\alpha_i,\cdots,\alpha_j,\cdots,\alpha_n)+D(\alpha_1,\cdots,\alpha_j,\cdots,\alpha_j,\cdots,\alpha_n)\text{(n-linearity) }=aD(A)+0=aD(A). D(B)=aD(A) a","['linear-algebra', 'determinant']"
28,Writing cycles of a graph as a linear combination of fundamental cycles,Writing cycles of a graph as a linear combination of fundamental cycles,,"It is folklore that the fundamental cycles (corresponding to aparticular spanning tree) of a graph constitute a basis for its cycle space, while the proof uses the linear indepence of fundamental cycles as well as some orthogonality arguements (see chapter 1 of Diestel’s Text book for more context). But I could not find any constructive way for writing an arbitrary cycle as a linear combination of fundamental cycles in the litrature, and my personal effort also did not get to anywhere. Is there any such way of writing the precise linear combination, preferably a purely combinatorial one? All answers and comments are appreciated.","It is folklore that the fundamental cycles (corresponding to aparticular spanning tree) of a graph constitute a basis for its cycle space, while the proof uses the linear indepence of fundamental cycles as well as some orthogonality arguements (see chapter 1 of Diestel’s Text book for more context). But I could not find any constructive way for writing an arbitrary cycle as a linear combination of fundamental cycles in the litrature, and my personal effort also did not get to anywhere. Is there any such way of writing the precise linear combination, preferably a purely combinatorial one? All answers and comments are appreciated.",,"['linear-algebra', 'graph-theory', 'algebraic-graph-theory']"
29,Explanation of spectral theorem for reals to high schoolers who have only done computational single-variable calculus.,Explanation of spectral theorem for reals to high schoolers who have only done computational single-variable calculus.,,"This is a shot in the dark and a pretty tall order, but I am wondering if anybody could give a good explanation of the spectral theorem for the reals to high schoolers who have only seen computational calculus of one variable and have not taken a linear algebra course before? An overview of the proof, why does one care about the result, what is the result, how to visualize it, etc. I've tried myself to explain to high school students this result but failed, so maybe there is someone out there who is a better teacher who can do this.","This is a shot in the dark and a pretty tall order, but I am wondering if anybody could give a good explanation of the spectral theorem for the reals to high schoolers who have only seen computational calculus of one variable and have not taken a linear algebra course before? An overview of the proof, why does one care about the result, what is the result, how to visualize it, etc. I've tried myself to explain to high school students this result but failed, so maybe there is someone out there who is a better teacher who can do this.",,"['calculus', 'linear-algebra', 'vector-spaces', 'proof-explanation', 'intuition']"
30,"Proof for the dimension of a subspace of M(n,R)","Proof for the dimension of a subspace of M(n,R)",,"First I will state the exercise and then show what I could get. Let $A_{1},...,A_{k} \in M(n,R)$ such that for all $ 1 \le i \neq j \le k $:  $ A_{i}^{2}=I$ and $A_{i}A_{j}+A_{j}A_{i}=0$ show that $k \le \frac{n(n+1)}{2}$ Ok, first of all we can see that the condition $ A_{i}^{2}=I$ means that all the matrices in this form are diagonalizable with eigenvalues $\in  \big\{1,-1\big\}  $, so we can state that $A_{i}$ are all invertible matrix and $A_{i}^{-1}=A_{i}$. Now if I multiply the relation between the matrices for $A_{i}$ i get that $A_{i}A_{j}A_{i}=-A_{j}$ wich means that $A_{j}$ and $-A_{j}$ are similar and because they need to share determinant it has to be 0 if the dimension of the space is odd. I also know that they commute. I feel like I should be able to find some other relation (maybe about the Ker of the linear application) from $A_{i}A_{j}+A_{j}A_{i}=0$ but I don't know how. Thanks in advance","First I will state the exercise and then show what I could get. Let $A_{1},...,A_{k} \in M(n,R)$ such that for all $ 1 \le i \neq j \le k $:  $ A_{i}^{2}=I$ and $A_{i}A_{j}+A_{j}A_{i}=0$ show that $k \le \frac{n(n+1)}{2}$ Ok, first of all we can see that the condition $ A_{i}^{2}=I$ means that all the matrices in this form are diagonalizable with eigenvalues $\in  \big\{1,-1\big\}  $, so we can state that $A_{i}$ are all invertible matrix and $A_{i}^{-1}=A_{i}$. Now if I multiply the relation between the matrices for $A_{i}$ i get that $A_{i}A_{j}A_{i}=-A_{j}$ wich means that $A_{j}$ and $-A_{j}$ are similar and because they need to share determinant it has to be 0 if the dimension of the space is odd. I also know that they commute. I feel like I should be able to find some other relation (maybe about the Ker of the linear application) from $A_{i}A_{j}+A_{j}A_{i}=0$ but I don't know how. Thanks in advance",,['linear-algebra']
31,Conditions for a matrix to have non-repeated eigenvalues,Conditions for a matrix to have non-repeated eigenvalues,,"I am wondering if anybody knows any reference/idea that can be used to adress the following seemingly simple question ""Is there any set of conditions so that all the eigenvalues of a real positive definite matrix are different?"" Motivation Duality in principal component analysis","I am wondering if anybody knows any reference/idea that can be used to adress the following seemingly simple question ""Is there any set of conditions so that all the eigenvalues of a real positive definite matrix are different?"" Motivation Duality in principal component analysis",,"['linear-algebra', 'matrices']"
32,If $p_A(x) = x^4 (x+3)^2 (x-4)$ then $A$ is diagonalizable iff $\operatorname{Rank}(A) + \operatorname{Rank}(-3I-A) = 8$,If  then  is diagonalizable iff,p_A(x) = x^4 (x+3)^2 (x-4) A \operatorname{Rank}(A) + \operatorname{Rank}(-3I-A) = 8,"Given the Characteristic polynomial of a matrix $A$ is   $$   p(x) = x^4 (x+3)^2 (x-4), $$   show that $A$ is diagonalizable if and only if   $$   \operatorname{Rank}(A) + \operatorname{Rank}(-3I-A) = 8. $$ Given: $A$ is diagonalizable Prove: $\operatorname{Rank}(A) + \operatorname{Rank}(-3I-A) = 8$ (Help is needed in the other way around) The geometric multiplicity equals the algebraic multiplicity, hence the diagonal matrix $D$ will look like $$     D   = \begin{pmatrix}       -3  &     &   &   &   &   &   \\           & -3  &   &   &   &   &   \\           &     & 4 &   &   &   &   \\           &     &   & 0 &   &   &   \\           &     &   &   & 0 &   &   \\           &     &   &   &   & 0 &   \\           &     &   &   &   &   & 0      \end{pmatrix} $$ Since $D$ and $A$ are similar matrices, their rank must be the same: $$  \operatorname{Rank}(A) = \operatorname{Rank}(D) = 3. $$ Also $$     -3I - A   = \begin{pmatrix}       0 &   &     &   &     &     &     \\         & 0 &     &   &     &     &     \\         &   & -7  &   &     &     &     \\         &   &     & 0 &     &     &     \\         &   &     &   & -3  &     &     \\         &   &     &   &     & -3  &     \\         &   &     &   &     &     & -3      \end{pmatrix} $$ and therefore $$     \operatorname{Rank}(A) + \operatorname{Rank}(-3I-A)   = 3 + 5   = 8. $$ Perfect. (Is it okay to place $D$ instead of $A$ in $\operatorname{Rank}(-3I-A)$? The rank will be the same, is it not?) Given: $\operatorname{Rank}(A) + \operatorname{Rank}(-3I-A) = 8$ Prove: $A$ is diagonalizable No clue really… I was thinking since geometric multiplicity is less or equal to algebraic multiplicity, I was thinking of finding all possible $D$'s and show that the one that makes the statement $\operatorname{Rank}(A) + \operatorname{Rank}(-3I-A) = 8$ hold makes $A$ diagonal. Any hint is appreciated.","Given the Characteristic polynomial of a matrix $A$ is   $$   p(x) = x^4 (x+3)^2 (x-4), $$   show that $A$ is diagonalizable if and only if   $$   \operatorname{Rank}(A) + \operatorname{Rank}(-3I-A) = 8. $$ Given: $A$ is diagonalizable Prove: $\operatorname{Rank}(A) + \operatorname{Rank}(-3I-A) = 8$ (Help is needed in the other way around) The geometric multiplicity equals the algebraic multiplicity, hence the diagonal matrix $D$ will look like $$     D   = \begin{pmatrix}       -3  &     &   &   &   &   &   \\           & -3  &   &   &   &   &   \\           &     & 4 &   &   &   &   \\           &     &   & 0 &   &   &   \\           &     &   &   & 0 &   &   \\           &     &   &   &   & 0 &   \\           &     &   &   &   &   & 0      \end{pmatrix} $$ Since $D$ and $A$ are similar matrices, their rank must be the same: $$  \operatorname{Rank}(A) = \operatorname{Rank}(D) = 3. $$ Also $$     -3I - A   = \begin{pmatrix}       0 &   &     &   &     &     &     \\         & 0 &     &   &     &     &     \\         &   & -7  &   &     &     &     \\         &   &     & 0 &     &     &     \\         &   &     &   & -3  &     &     \\         &   &     &   &     & -3  &     \\         &   &     &   &     &     & -3      \end{pmatrix} $$ and therefore $$     \operatorname{Rank}(A) + \operatorname{Rank}(-3I-A)   = 3 + 5   = 8. $$ Perfect. (Is it okay to place $D$ instead of $A$ in $\operatorname{Rank}(-3I-A)$? The rank will be the same, is it not?) Given: $\operatorname{Rank}(A) + \operatorname{Rank}(-3I-A) = 8$ Prove: $A$ is diagonalizable No clue really… I was thinking since geometric multiplicity is less or equal to algebraic multiplicity, I was thinking of finding all possible $D$'s and show that the one that makes the statement $\operatorname{Rank}(A) + \operatorname{Rank}(-3I-A) = 8$ hold makes $A$ diagonal. Any hint is appreciated.",,"['linear-algebra', 'matrices', 'diagonalization']"
33,Prove that two operators have a common eigenvector,Prove that two operators have a common eigenvector,,"Suppose $S,T:\mathbb C^3\to\mathbb C^3$ are linear operators. The   degree of the minimal polynomial of each of the operators is at most   2. Show that they share a common eigenvector. I tried to exploit the condition on the degree. I obtained that there are two possible Jordan canonical forms for $S,T$. The first possibility is that the JCF is diagonal of the form $(a,a,b)$. The second possibility is that the JCF has one block of size 2 with eigenvalue $a$ and 1 block of size 1 with eigenvalue $a$. For the other operator the possibilities for the JCF are the same except that the eigenvalues may be different. But I don't know how to proceed from this point.","Suppose $S,T:\mathbb C^3\to\mathbb C^3$ are linear operators. The   degree of the minimal polynomial of each of the operators is at most   2. Show that they share a common eigenvector. I tried to exploit the condition on the degree. I obtained that there are two possible Jordan canonical forms for $S,T$. The first possibility is that the JCF is diagonal of the form $(a,a,b)$. The second possibility is that the JCF has one block of size 2 with eigenvalue $a$ and 1 block of size 1 with eigenvalue $a$. For the other operator the possibilities for the JCF are the same except that the eigenvalues may be different. But I don't know how to proceed from this point.",,"['linear-algebra', 'eigenvalues-eigenvectors', 'linear-transformations', 'jordan-normal-form']"
34,How to arrive at $ \frac{\partial }{\partial X} \log(\mathrm{det}(I + A X B^\top )) = A^\top (I +B X^\top A^\top )^{-1} B$?,How to arrive at ?, \frac{\partial }{\partial X} \log(\mathrm{det}(I + A X B^\top )) = A^\top (I +B X^\top A^\top )^{-1} B,"Dear Matrix Calculus experts, please do enlighten me. How to arrive at the following matrix derivative? $$\frac{\partial }{\partial X} \log( \mathrm{det}(I + A X B^\top ) ) = A^\top (I + B X^\top A^\top )^{-1} B$$ I am in a confused state with matrix calculus because I am learning and not fully grasping the concept, I must admit. In Wikipedia , it says the following: It is often easier to work in differential form and then convert back to normal derivatives. My source of confusion begins from here. So, the differential is $$\mathrm d \log(\det(X)) = \mbox{tr} \left( X^{-1} \mathrm d X \right)$$ now do we ""convert"" it to normal derivative such that I get my above answer? :/ I hope I will get this matrix calculus someday. Thank you so much in advance.","Dear Matrix Calculus experts, please do enlighten me. How to arrive at the following matrix derivative? $$\frac{\partial }{\partial X} \log( \mathrm{det}(I + A X B^\top ) ) = A^\top (I + B X^\top A^\top )^{-1} B$$ I am in a confused state with matrix calculus because I am learning and not fully grasping the concept, I must admit. In Wikipedia , it says the following: It is often easier to work in differential form and then convert back to normal derivatives. My source of confusion begins from here. So, the differential is $$\mathrm d \log(\det(X)) = \mbox{tr} \left( X^{-1} \mathrm d X \right)$$ now do we ""convert"" it to normal derivative such that I get my above answer? :/ I hope I will get this matrix calculus someday. Thank you so much in advance.",,"['linear-algebra', 'matrices', 'derivatives', 'matrix-calculus']"
35,Is the map $A \to \bigwedge^{k}A $ an immersion?,Is the map  an immersion?,A \to \bigwedge^{k}A ,"$\newcommand{\Cof}{\operatorname{cof}} \newcommand{\id}{\operatorname{Id}}$ Let $V$ be a real oriented $d$-dimensional vector space ($d>2$). Let $2 \le k \le d-1$ be fixed. Consider the following map: $$\psi:\text{GL}^+(V) \to \text{GL}(\bigwedge^{k}V,\bigwedge^{k}V) \, \,, \, \, \psi(A)=\bigwedge^{k}A,$$ where $\bigwedge^{k} V$ is the $k$-th exterior power of $V$. Is $\psi$ an immersion? Note that $\psi$ is injective and smooth. My motivation is connected to this question . Edit: Here are some observations: First, using the multiplicative nature of $\psi$, we can reduce everything to the identity. (Since $d\psi_A(B)=\psi(A) \circ d\psi_{\id}(A^{-1}B)$). Let's consider for a moment the case $k=2$: In that case $$ \big(d\psi_{\id}(B)\big)(e_i \wedge e_j)=e_i \wedge Be_j+Be_i \wedge e_j. \tag{1}$$ So, we have reduced the question into showing that if equation $(1)$ holds for every two vectors $e_i,e_j$, then $B=0$. (Of course, it suffices to take $e_i,e_j$ to be part of a given basis for $V$). I have also proved that $\text{trace} B=0$ (In general $d\psi_A(B)=0 \Rightarrow \langle \Cof A,B \rangle=0$ and $\Cof A=\id$). Since the answer is positive for $k=d-1$ (see explanation below), we see that the first non-obvious case is $k=2,d=4$. A proof the answer is positive for $k=d-1$: In that case $\bigwedge^{d-1}A$ is essentially the cofactor matrix of $A$, $\Cof A$. Then we have the following formula for its derivative : $$d(\Cof)_A(B)   = (A^{T})^{-1}\big(\langle \Cof A , B\rangle \cdot \id - B^T \circ \Cof A  \big),$$ so $d(\Cof)_A(B)=0$ implies $\langle \Cof A , B\rangle \cdot \id = B^T \circ \Cof A$. Taking traces we get $$d \langle \Cof A , B\rangle = \langle \Cof A , B\rangle \Rightarrow \langle \Cof A , B\rangle =0,$$ which in turn implies $$ B^T \circ \Cof A  =0 \Rightarrow  B=0.$$","$\newcommand{\Cof}{\operatorname{cof}} \newcommand{\id}{\operatorname{Id}}$ Let $V$ be a real oriented $d$-dimensional vector space ($d>2$). Let $2 \le k \le d-1$ be fixed. Consider the following map: $$\psi:\text{GL}^+(V) \to \text{GL}(\bigwedge^{k}V,\bigwedge^{k}V) \, \,, \, \, \psi(A)=\bigwedge^{k}A,$$ where $\bigwedge^{k} V$ is the $k$-th exterior power of $V$. Is $\psi$ an immersion? Note that $\psi$ is injective and smooth. My motivation is connected to this question . Edit: Here are some observations: First, using the multiplicative nature of $\psi$, we can reduce everything to the identity. (Since $d\psi_A(B)=\psi(A) \circ d\psi_{\id}(A^{-1}B)$). Let's consider for a moment the case $k=2$: In that case $$ \big(d\psi_{\id}(B)\big)(e_i \wedge e_j)=e_i \wedge Be_j+Be_i \wedge e_j. \tag{1}$$ So, we have reduced the question into showing that if equation $(1)$ holds for every two vectors $e_i,e_j$, then $B=0$. (Of course, it suffices to take $e_i,e_j$ to be part of a given basis for $V$). I have also proved that $\text{trace} B=0$ (In general $d\psi_A(B)=0 \Rightarrow \langle \Cof A,B \rangle=0$ and $\Cof A=\id$). Since the answer is positive for $k=d-1$ (see explanation below), we see that the first non-obvious case is $k=2,d=4$. A proof the answer is positive for $k=d-1$: In that case $\bigwedge^{d-1}A$ is essentially the cofactor matrix of $A$, $\Cof A$. Then we have the following formula for its derivative : $$d(\Cof)_A(B)   = (A^{T})^{-1}\big(\langle \Cof A , B\rangle \cdot \id - B^T \circ \Cof A  \big),$$ so $d(\Cof)_A(B)=0$ implies $\langle \Cof A , B\rangle \cdot \id = B^T \circ \Cof A$. Taking traces we get $$d \langle \Cof A , B\rangle = \langle \Cof A , B\rangle \Rightarrow \langle \Cof A , B\rangle =0,$$ which in turn implies $$ B^T \circ \Cof A  =0 \Rightarrow  B=0.$$",,"['linear-algebra', 'differential-geometry', 'smooth-manifolds', 'exterior-algebra']"
36,How to prove for a PSD matrix $ \lambda _{\min} (A)\operatorname{tr}(B)\le \operatorname{tr}(AB) \le \lambda _{\max} (A)\operatorname{tr}(B)$,How to prove for a PSD matrix, \lambda _{\min} (A)\operatorname{tr}(B)\le \operatorname{tr}(AB) \le \lambda _{\max} (A)\operatorname{tr}(B),"As the title says, for $A,B\in \mathbb{S}^n$, $B\ge 0$ (PSD), how to prove that $$ \lambda_{\min} (A)\operatorname{tr}(B)\le \operatorname{tr}(AB) \le \lambda_{\max} (A)\operatorname{tr}(B). $$ My thinking Is it possible to use eigendecomposition to prove this?","As the title says, for $A,B\in \mathbb{S}^n$, $B\ge 0$ (PSD), how to prove that $$ \lambda_{\min} (A)\operatorname{tr}(B)\le \operatorname{tr}(AB) \le \lambda_{\max} (A)\operatorname{tr}(B). $$ My thinking Is it possible to use eigendecomposition to prove this?",,"['linear-algebra', 'matrices', 'matrix-equations', 'matrix-decomposition', 'matrix-rank']"
37,The nonexistence of a Jordan normal form over a finite field,The nonexistence of a Jordan normal form over a finite field,,"Since finite fields are not algebraically closed, this suggests to me that there may be matrices over finite fields whose characteristic polynomials don't split over that finite field, and thus do not have Jordan normal forms. As a simple example, is it the case that there exists a $3 \times 3$ matrix with entries from $\mathbb{F}_3$ that does not have a Jordan normal form? I'm wondering if I can just find a polynomial $f \in \mathbb{F}_3[x]$ that doesn't split in $\mathbb{F}_3[x]$ and then try to build a matrix whose characteristic polynomial is $f$, but I'm not sure that this is a feasible approach to constructing such a matrix.","Since finite fields are not algebraically closed, this suggests to me that there may be matrices over finite fields whose characteristic polynomials don't split over that finite field, and thus do not have Jordan normal forms. As a simple example, is it the case that there exists a $3 \times 3$ matrix with entries from $\mathbb{F}_3$ that does not have a Jordan normal form? I'm wondering if I can just find a polynomial $f \in \mathbb{F}_3[x]$ that doesn't split in $\mathbb{F}_3[x]$ and then try to build a matrix whose characteristic polynomial is $f$, but I'm not sure that this is a feasible approach to constructing such a matrix.",,"['linear-algebra', 'matrices', 'finite-fields', 'jordan-normal-form']"
38,why symmetric matrix is always diagonalizable even when it has repeated eigenvalues?,why symmetric matrix is always diagonalizable even when it has repeated eigenvalues?,,"I am studying linear algebra and I have a very basic question. Why symmetric matrix is always diagonalizable even if it has repeated eigenvalues? I've seen that sufficient orthonormal eigenvectors can be generated by applying Gram-Schmidt process to the eigenspace of repeated eigenvalue. I know what this means. I have solved bunch of exercise problems and I've been always able to generate full set of orthonormal basis of eigenspace of repeated eigenvalue. But what I want to know is this: The dimension of eigenspace of repeated eigenvalue with multiplicity of ""k"" is always ""k""? Is it impossible that eigenspace of repeated eigenvalue of symmetric matrix is a 1-dimensional line? Thank you.","I am studying linear algebra and I have a very basic question. Why symmetric matrix is always diagonalizable even if it has repeated eigenvalues? I've seen that sufficient orthonormal eigenvectors can be generated by applying Gram-Schmidt process to the eigenspace of repeated eigenvalue. I know what this means. I have solved bunch of exercise problems and I've been always able to generate full set of orthonormal basis of eigenspace of repeated eigenvalue. But what I want to know is this: The dimension of eigenspace of repeated eigenvalue with multiplicity of ""k"" is always ""k""? Is it impossible that eigenspace of repeated eigenvalue of symmetric matrix is a 1-dimensional line? Thank you.",,"['linear-algebra', 'eigenvalues-eigenvectors', 'diagonalization']"
39,Isomorphisms between $O_n$ and the direct product of $SO_n$ with another group,Isomorphisms between  and the direct product of  with another group,O_n SO_n,"Let $O_n(\mathbb{R})$ be the group of real orthogonal $n\times n$ matrices and $SO_n(\mathbb{R})$ be the group of real orthogonal matrices with determinant $1$. (i) Show that $O_n(\mathbb{R}) = SO_n(\mathbb{R}) × \{\pm I_n\}$ if and only if $n$ is odd. (ii) Show that if $n$ is even, then $O_n(\mathbb{R})$ is not the direct product of $SO_n(\mathbb{R})$ with any normal subgroup. Here is the progress I have made so far: (i) If $n$ is odd then consider the map $\phi: SO_n(\mathbb{R}) × \{\pm I_n\} \to O_n : (A,B) \to AB$. This is a homomorphism as all the elements of $\{\pm I_n\}$ commute with the elements of $SO_n(\mathbb{R})$. Furthermore it is injective as is $AB = CD$ then since $A,C$ have determinant $1$ we get that $B,D$ have the same determinant so $B,D$ are the same matrix so $A,C$ are the same as well. It is also surjective as if $E \in O_n(\mathbb{r})$ then either $E$ or $-E \in SO_n(\mathbb{R})$ and so either $(E,I_n)$ or $(-E,-I_n)$ maps to $E$. Hence we have an isomorphism and they are the same. If $n$ is even then note that $O_n(\mathbb{R})$ has center of order $2$ while $ SO_n(\mathbb{R}) × \{\pm I_n\}$ has center of order $4$ so they are not isomorphic. (ii) I am having trouble with this bit. I can't even manage to show $O_n(\mathbb{R})$ is not isomorphic to $SO_n(\mathbb{R})$ for even $n$. Any help is much appreciated.","Let $O_n(\mathbb{R})$ be the group of real orthogonal $n\times n$ matrices and $SO_n(\mathbb{R})$ be the group of real orthogonal matrices with determinant $1$. (i) Show that $O_n(\mathbb{R}) = SO_n(\mathbb{R}) × \{\pm I_n\}$ if and only if $n$ is odd. (ii) Show that if $n$ is even, then $O_n(\mathbb{R})$ is not the direct product of $SO_n(\mathbb{R})$ with any normal subgroup. Here is the progress I have made so far: (i) If $n$ is odd then consider the map $\phi: SO_n(\mathbb{R}) × \{\pm I_n\} \to O_n : (A,B) \to AB$. This is a homomorphism as all the elements of $\{\pm I_n\}$ commute with the elements of $SO_n(\mathbb{R})$. Furthermore it is injective as is $AB = CD$ then since $A,C$ have determinant $1$ we get that $B,D$ have the same determinant so $B,D$ are the same matrix so $A,C$ are the same as well. It is also surjective as if $E \in O_n(\mathbb{r})$ then either $E$ or $-E \in SO_n(\mathbb{R})$ and so either $(E,I_n)$ or $(-E,-I_n)$ maps to $E$. Hence we have an isomorphism and they are the same. If $n$ is even then note that $O_n(\mathbb{R})$ has center of order $2$ while $ SO_n(\mathbb{R}) × \{\pm I_n\}$ has center of order $4$ so they are not isomorphic. (ii) I am having trouble with this bit. I can't even manage to show $O_n(\mathbb{R})$ is not isomorphic to $SO_n(\mathbb{R})$ for even $n$. Any help is much appreciated.",,"['linear-algebra', 'group-theory']"
40,Vector space of matrices of rank $\le r$,Vector space of matrices of rank,\le r,"Let $F$ be a field, and $M_{n\times n}(F)$ be the set of $n\times n$ matrices with entries in $F$. Let $V$ be a subspace of $M_{n\times n}(F)$ where every matrix in $V$ has rank at most $r$. Prove that $\dim V\le nr$. This is a puzzle I found on the internet which I am having trouble solving. I suppose I am not even sure if the result is correct, but it seems so. It is easy to come up with subspaces of dimension $nr$, like the set of matrices where all but $r$ columns are zero, but these examples are all tight. I have been able to find literature on subspaces of matrices of rank $\ge r$, but not of rank $\le r$. I would appreciate any hints, references, or even solutions which require $F=\mathbb R$ or $\mathbb C$. Here is a failed attempt that might serve for inspiration: Let $t=\dim V$, and assume $t=nr+1$. By using Gaussian elimination, we can find a basis $A^1,\dots,A^t$ for $V$ such that for each $1\le k\le t$, there exists a position $(i(k),j(k))$ where $A^k_{i(t),j(t)}=1$, while $A^\ell_{i(t),j(t)}=0$ for all $\ell\neq k$. These positions are the ""leading ones"" found by Gaussian elimination, and when choosing a matrix in $V$, the entries in these positions can be chosen independently of each other. Since there are $nr+1$ leading ones, there exists $r+1$ leading ones which are in pairwise different rows and columns (this follows from a pigeonhole argument). I was hoping to leverage this to show how to that the $(r+1)\times(r+1)$ sub-matrix where these rows and columns intersect could be chosen to have rank $r+1$ by an appropriate choice of the leading ones. This fails, because there is no control over the other entries in the sub-matrix, and they can get in the way. Many thanks!","Let $F$ be a field, and $M_{n\times n}(F)$ be the set of $n\times n$ matrices with entries in $F$. Let $V$ be a subspace of $M_{n\times n}(F)$ where every matrix in $V$ has rank at most $r$. Prove that $\dim V\le nr$. This is a puzzle I found on the internet which I am having trouble solving. I suppose I am not even sure if the result is correct, but it seems so. It is easy to come up with subspaces of dimension $nr$, like the set of matrices where all but $r$ columns are zero, but these examples are all tight. I have been able to find literature on subspaces of matrices of rank $\ge r$, but not of rank $\le r$. I would appreciate any hints, references, or even solutions which require $F=\mathbb R$ or $\mathbb C$. Here is a failed attempt that might serve for inspiration: Let $t=\dim V$, and assume $t=nr+1$. By using Gaussian elimination, we can find a basis $A^1,\dots,A^t$ for $V$ such that for each $1\le k\le t$, there exists a position $(i(k),j(k))$ where $A^k_{i(t),j(t)}=1$, while $A^\ell_{i(t),j(t)}=0$ for all $\ell\neq k$. These positions are the ""leading ones"" found by Gaussian elimination, and when choosing a matrix in $V$, the entries in these positions can be chosen independently of each other. Since there are $nr+1$ leading ones, there exists $r+1$ leading ones which are in pairwise different rows and columns (this follows from a pigeonhole argument). I was hoping to leverage this to show how to that the $(r+1)\times(r+1)$ sub-matrix where these rows and columns intersect could be chosen to have rank $r+1$ by an appropriate choice of the leading ones. This fails, because there is no control over the other entries in the sub-matrix, and they can get in the way. Many thanks!",,"['linear-algebra', 'matrix-rank']"
41,"What are the ""fancy linear-algebra methods"" that would allow me to solve this physics problem without Kirchhoff's Rule?","What are the ""fancy linear-algebra methods"" that would allow me to solve this physics problem without Kirchhoff's Rule?",,"This might be better suited for the Physics Stack Exchange, if that's the case feel free to migrate it there. I'm also not terribly familiar with the tagging here on the Mathematics SE so if I tagged incorrectly, I'd appreciate any improvements. I was studying for an upcoming undergraduate Physics exam when I found this slidedeck from another university. I don't attend Rochester, so I obviously wasn't at that lecture, but there's an interesting bit on Page 5 that notes that the system of equations used to solve the problem can be converted into a matrices to be solved using linear algebra. Now while my physics skills are pretty mediocre, I really enjoy linear algebra, so I was curious as to what the ""fancy linear-algebra methods"" mentioned on Slide 7 might be. The slides say that it would involve finding the inverse of the 6x6 matrix, which I assume would entail putting it in an augmented matrix with the 6x6 Identity Matrix and row reducing, and then left multiplying both sides. That doesn't sound too fancy to me, although it may have simply been sarcasm on the professor's part. Is there something I'm missing or is it just row reduction and left-multiplying?","This might be better suited for the Physics Stack Exchange, if that's the case feel free to migrate it there. I'm also not terribly familiar with the tagging here on the Mathematics SE so if I tagged incorrectly, I'd appreciate any improvements. I was studying for an upcoming undergraduate Physics exam when I found this slidedeck from another university. I don't attend Rochester, so I obviously wasn't at that lecture, but there's an interesting bit on Page 5 that notes that the system of equations used to solve the problem can be converted into a matrices to be solved using linear algebra. Now while my physics skills are pretty mediocre, I really enjoy linear algebra, so I was curious as to what the ""fancy linear-algebra methods"" mentioned on Slide 7 might be. The slides say that it would involve finding the inverse of the 6x6 matrix, which I assume would entail putting it in an augmented matrix with the 6x6 Identity Matrix and row reducing, and then left multiplying both sides. That doesn't sound too fancy to me, although it may have simply been sarcasm on the professor's part. Is there something I'm missing or is it just row reduction and left-multiplying?",,"['linear-algebra', 'matrices', 'physics']"
42,"Knowing that $A^2+B^2=\sqrt{2+\sqrt{2}}\cdot AB$, prove that $n$ is a multiple of $16$","Knowing that , prove that  is a multiple of",A^2+B^2=\sqrt{2+\sqrt{2}}\cdot AB n 16,"Let $A,B \in \mathcal{M}_{n}(\mathbb{R})$ such that $$A^2+B^2=\sqrt{2+\sqrt{2}}\cdot AB$$   Knowing that $\det(AB-BA)>0$, prove that $n$ is multiple of $16$. I know that for this type of problems, one usually uses some identities such as $$(A+iB)(A-iB)=c(AB-BA)$$ and its conjugate, where $c$ is a complex number and since $A,B$ are real matrices, their determinants will be positive real numbers. Since $\det(AB-BA)>0$, that will lead to $c^n=0$ and with the help of some trigonometry it would follow that $n$ is the multiple of something. Here, however, I couldn't obatain the identity I described. That square root points, though, exactly to this method and some trigonometry.","Let $A,B \in \mathcal{M}_{n}(\mathbb{R})$ such that $$A^2+B^2=\sqrt{2+\sqrt{2}}\cdot AB$$   Knowing that $\det(AB-BA)>0$, prove that $n$ is multiple of $16$. I know that for this type of problems, one usually uses some identities such as $$(A+iB)(A-iB)=c(AB-BA)$$ and its conjugate, where $c$ is a complex number and since $A,B$ are real matrices, their determinants will be positive real numbers. Since $\det(AB-BA)>0$, that will lead to $c^n=0$ and with the help of some trigonometry it would follow that $n$ is the multiple of something. Here, however, I couldn't obatain the identity I described. That square root points, though, exactly to this method and some trigonometry.",,"['linear-algebra', 'matrices', 'determinant']"
43,Can any complex $n\times n$ matrix be in a maximal commuting set of $\lfloor n^2/4\rfloor + 1$ matrices?,Can any complex  matrix be in a maximal commuting set of  matrices?,n\times n \lfloor n^2/4\rfloor + 1,"The maximum number of mutually commuting linearly independent complex matrices of order $n$ is equal to $\lfloor n^2/4\rfloor + 1$ ($\lfloor \cdot \rfloor$ denotes the integer part of $\cdot$). This result is attributed to Schur and a short proof can be found here . In an answer to this question , an explicit set $\mathcal{M}$ of linearly independent and mutually commuting matrices is written for $2n\times 2n$ complex matrices: $$ \begin{pmatrix}a\cdot\text{Id}_n & M_{n\times n} \\ 0_{n\times n} &a\cdot \text{Id}_n\end{pmatrix}, $$ where $a$ is a complex number, $\text{Id}_n$ is the order $n$ identity matrix and $M$ is an arbitrary complex matrix. Now my question: can any nonzero $n\times n$ complex matrix be part of a subspace of ($\lfloor n^2/4\rfloor +1$) linearly independent mutually commuting matrices? If the answer is negative, what would be necessary? To avoid misunderstanding, let me add some redundancy and rephrase my question as: given an arbitrary nonzero $n\times n$ complex matrix $A_0$, is it possible to find $\lfloor n^2/4\rfloor$ linearly independent $A_i$, $i=1,\ldots,\lfloor n^2/4\rfloor$, such that $A_iA_j-A_jA_i=0$ for $i,j=0,\ldots,\lfloor n^2/4\rfloor$? Update The answer to my main question - if any nonzero matrix can be in the maximal subspace of mutually commuting matrices - is negative in general as Ewan Delanoy pointed out with the example of diagonalizable matrices with distinct eigenvalues. The secondary question - what is necessary to be in the maximal commuting set - seems more complicated. A set of mutually commuting matrices can be simultaneously triangularized , then only upper (lower) triangular matrices need be analyzed. For $2n\times 2n$ matrices an explicit set $\mathcal{M}$ of solutions is provided above and for $(2n+1)\times(2n+1)$ matrices the following set, also called $\mathcal{M}$, is a maximal commuting subspace: $$ \begin{pmatrix}a\cdot\text{Id}_n & M_{n\times n+1} \\ 0_{n+1\times n} &a\cdot \text{Id}_{n+1}\end{pmatrix}. $$ This set of matrices is also suggested in the paper by M. Mirzakhani, "" A simple proof of a theorem of Schur "", already cited above. Now I would like to change my secondary question for a more specific one: must any maximal commuting subspace have the form of $\mathcal{M}$ ? I think the answer is affirmative and, if that is the case, it will answer this question in MO: "" How many commuting nilpotent matrices are there? "" because the set $\mathcal{M}$ would be the maximal abelian nilpotent subalgebra plus (a multiple of) the identity matrix.","The maximum number of mutually commuting linearly independent complex matrices of order $n$ is equal to $\lfloor n^2/4\rfloor + 1$ ($\lfloor \cdot \rfloor$ denotes the integer part of $\cdot$). This result is attributed to Schur and a short proof can be found here . In an answer to this question , an explicit set $\mathcal{M}$ of linearly independent and mutually commuting matrices is written for $2n\times 2n$ complex matrices: $$ \begin{pmatrix}a\cdot\text{Id}_n & M_{n\times n} \\ 0_{n\times n} &a\cdot \text{Id}_n\end{pmatrix}, $$ where $a$ is a complex number, $\text{Id}_n$ is the order $n$ identity matrix and $M$ is an arbitrary complex matrix. Now my question: can any nonzero $n\times n$ complex matrix be part of a subspace of ($\lfloor n^2/4\rfloor +1$) linearly independent mutually commuting matrices? If the answer is negative, what would be necessary? To avoid misunderstanding, let me add some redundancy and rephrase my question as: given an arbitrary nonzero $n\times n$ complex matrix $A_0$, is it possible to find $\lfloor n^2/4\rfloor$ linearly independent $A_i$, $i=1,\ldots,\lfloor n^2/4\rfloor$, such that $A_iA_j-A_jA_i=0$ for $i,j=0,\ldots,\lfloor n^2/4\rfloor$? Update The answer to my main question - if any nonzero matrix can be in the maximal subspace of mutually commuting matrices - is negative in general as Ewan Delanoy pointed out with the example of diagonalizable matrices with distinct eigenvalues. The secondary question - what is necessary to be in the maximal commuting set - seems more complicated. A set of mutually commuting matrices can be simultaneously triangularized , then only upper (lower) triangular matrices need be analyzed. For $2n\times 2n$ matrices an explicit set $\mathcal{M}$ of solutions is provided above and for $(2n+1)\times(2n+1)$ matrices the following set, also called $\mathcal{M}$, is a maximal commuting subspace: $$ \begin{pmatrix}a\cdot\text{Id}_n & M_{n\times n+1} \\ 0_{n+1\times n} &a\cdot \text{Id}_{n+1}\end{pmatrix}. $$ This set of matrices is also suggested in the paper by M. Mirzakhani, "" A simple proof of a theorem of Schur "", already cited above. Now I would like to change my secondary question for a more specific one: must any maximal commuting subspace have the form of $\mathcal{M}$ ? I think the answer is affirmative and, if that is the case, it will answer this question in MO: "" How many commuting nilpotent matrices are there? "" because the set $\mathcal{M}$ would be the maximal abelian nilpotent subalgebra plus (a multiple of) the identity matrix.",,"['linear-algebra', 'matrices', 'vector-spaces']"
44,Eigenvalues and eigenvectors - uniqueness,Eigenvalues and eigenvectors - uniqueness,,"Suppose I have a square $n\times n$ matrix A with $n$ linearly independent eigenvectors. Clearly more than one matrix can share the same eigenvectors and eigenvalues. However, I also know that I can write this matrix A in the form D = P $^{-1}$ AP , where D is a diagonal matrix with diagonal entries equal to the eigenvalues and the columns of P are the eigenvectors of A . However, in the other direction, if I know the eigenvalues and eigenvectors of a matrix A , then I can form the matrices P and D using the above. However, this seems to suggest that given eigenvalues and eigenvectors I can find a single matrix that corresponds to these values... So when are eigenvalues and eigenvetors unique and not unique? What am I missing?","Suppose I have a square $n\times n$ matrix A with $n$ linearly independent eigenvectors. Clearly more than one matrix can share the same eigenvectors and eigenvalues. However, I also know that I can write this matrix A in the form D = P $^{-1}$ AP , where D is a diagonal matrix with diagonal entries equal to the eigenvalues and the columns of P are the eigenvectors of A . However, in the other direction, if I know the eigenvalues and eigenvectors of a matrix A , then I can form the matrices P and D using the above. However, this seems to suggest that given eigenvalues and eigenvectors I can find a single matrix that corresponds to these values... So when are eigenvalues and eigenvetors unique and not unique? What am I missing?",,"['linear-algebra', 'eigenvalues-eigenvectors']"
45,What is the geometric intuition behind algebraic multiplicity?,What is the geometric intuition behind algebraic multiplicity?,,"The algebraic multiplicity of an eigenvalue $\lambda$ is the number of times $\lambda$ appears as a root of the characteristic polynomial. The geometric multiplicity of an eigenvalue $\lambda$ is dimension of the eigenspace of the eigenvalue $\lambda$. Let us consider the linear transformation $T:\Bbb R^3 \to \Bbb R^3$ for simplicity. Suppose the characteristic polynomial of $T$ has the eigenvalue $\lambda$ as a repeated root, $2$ times. For example, if the eigenspace of the eigenvalue $\lambda$ were a line (one-dimensional), we could visualize $T$ as the transformation squishing or stretching all vectors on that line by an amount $\lambda$. But what is the geometric significance of the algebraic multiplicity $2$, in this case? Is there any underlying geometric intuition?","The algebraic multiplicity of an eigenvalue $\lambda$ is the number of times $\lambda$ appears as a root of the characteristic polynomial. The geometric multiplicity of an eigenvalue $\lambda$ is dimension of the eigenspace of the eigenvalue $\lambda$. Let us consider the linear transformation $T:\Bbb R^3 \to \Bbb R^3$ for simplicity. Suppose the characteristic polynomial of $T$ has the eigenvalue $\lambda$ as a repeated root, $2$ times. For example, if the eigenspace of the eigenvalue $\lambda$ were a line (one-dimensional), we could visualize $T$ as the transformation squishing or stretching all vectors on that line by an amount $\lambda$. But what is the geometric significance of the algebraic multiplicity $2$, in this case? Is there any underlying geometric intuition?",,['linear-algebra']
46,Puzzled for a (supposed) way to show that $\det(e^A)=e^{{\rm tr}(A)}$,Puzzled for a (supposed) way to show that,\det(e^A)=e^{{\rm tr}(A)},"I know how to show that $\det(e^A)=e^{{\rm tr}(A)}$ by elementary methods using the eigenvalues of $e^A$ (that is, if $\lambda$ is an eigenvalue of $A$ then $e^\lambda$ is an eigenvalue of $e^A$). However I found an exercise that want that the identity be proved in a very different way (more analytical). It says: Let $f(t):=\det(e^{tA})-e^{{\rm tr}(tA)}$. Now, using the derivative of the determinant, show that $f'=0$. The derivative of the determinant is stated as: Let $a_1,\ldots,a_m\in C^1(X,\Bbb K^m)$, where $X\subset\Bbb K$ is open, then $\det[a_1,\ldots,a_m]\in C^1(X,\Bbb K)$ and $$(\det[a_1,\ldots,a_m])'=\sum_{k=1}^m\det[a_1,\ldots,a_{k-1},a_k',a_{k+1},\ldots,a_m]$$ Here $\Bbb K\in\{\Bbb R,\Bbb C\}$ and the derivative is clear as the derivative of a $m$-linear function for the $a_k$ as vector columns (or rows) of a $m\times m$ matrix. Trying to use this I can see that $$f'(t)=\partial\det(e^{tA})e^{tA}A-{\rm tr}(A)e^{t\,{\rm tr}(A)}$$ But it is far to be clear how the identity of the question can be proved from here, that is, how to prove that $f'=0$. Anyone knows how to handle this proof? Thank you.","I know how to show that $\det(e^A)=e^{{\rm tr}(A)}$ by elementary methods using the eigenvalues of $e^A$ (that is, if $\lambda$ is an eigenvalue of $A$ then $e^\lambda$ is an eigenvalue of $e^A$). However I found an exercise that want that the identity be proved in a very different way (more analytical). It says: Let $f(t):=\det(e^{tA})-e^{{\rm tr}(tA)}$. Now, using the derivative of the determinant, show that $f'=0$. The derivative of the determinant is stated as: Let $a_1,\ldots,a_m\in C^1(X,\Bbb K^m)$, where $X\subset\Bbb K$ is open, then $\det[a_1,\ldots,a_m]\in C^1(X,\Bbb K)$ and $$(\det[a_1,\ldots,a_m])'=\sum_{k=1}^m\det[a_1,\ldots,a_{k-1},a_k',a_{k+1},\ldots,a_m]$$ Here $\Bbb K\in\{\Bbb R,\Bbb C\}$ and the derivative is clear as the derivative of a $m$-linear function for the $a_k$ as vector columns (or rows) of a $m\times m$ matrix. Trying to use this I can see that $$f'(t)=\partial\det(e^{tA})e^{tA}A-{\rm tr}(A)e^{t\,{\rm tr}(A)}$$ But it is far to be clear how the identity of the question can be proved from here, that is, how to prove that $f'=0$. Anyone knows how to handle this proof? Thank you.",,"['linear-algebra', 'analysis']"
47,Trace of a matrix product,Trace of a matrix product,,"For an arbitrary matrix $A$ and a given matrix $B$, is is possible to generate a matrix $C$ such that $\text{Trace}(ABC) = \text{Trace}(AB)$ barring the trivial identity case? If not in the general case, is it possible if $B$ is symmetric? Entries in all the matrices are real. Any tips are appreciated","For an arbitrary matrix $A$ and a given matrix $B$, is is possible to generate a matrix $C$ such that $\text{Trace}(ABC) = \text{Trace}(AB)$ barring the trivial identity case? If not in the general case, is it possible if $B$ is symmetric? Entries in all the matrices are real. Any tips are appreciated",,"['linear-algebra', 'matrices', 'trace']"
48,Milnor's octahedron,Milnor's octahedron,,"If $V$ is a vector space, then a curvaturelike tensor in $V$ is a quadrilinear map $F:V \times V \times V \times V \to \Bbb R$ satisfying 1) $F(x,y,z,w) = -F(y,x,z,w) = -F(x,y,w,z)$; 2) $F(x,y,z,w) = F(z,w,x,y)$; 3) $F(x,y,z,w) + F(y,z,x,w) + F(z,x,y,w) = 0$, for all $x,y,z,w \in V$. It turns out that the Bianchi identity (item 3) actually implies item 2, assuming 1. This implication is called "" Milnor's octahedron argument "". I'd like to know where this appeared for the first time. Thanks!","If $V$ is a vector space, then a curvaturelike tensor in $V$ is a quadrilinear map $F:V \times V \times V \times V \to \Bbb R$ satisfying 1) $F(x,y,z,w) = -F(y,x,z,w) = -F(x,y,w,z)$; 2) $F(x,y,z,w) = F(z,w,x,y)$; 3) $F(x,y,z,w) + F(y,z,x,w) + F(z,x,y,w) = 0$, for all $x,y,z,w \in V$. It turns out that the Bianchi identity (item 3) actually implies item 2, assuming 1. This implication is called "" Milnor's octahedron argument "". I'd like to know where this appeared for the first time. Thanks!",,"['linear-algebra', 'reference-request', 'riemannian-geometry', 'math-history', 'tensors']"
49,Find all functions $f: \mathbb{R} \rightarrow \mathbb{R}$ such that $f(xy - 1) + f(x)f(y) = 2xy - 1$,Find all functions  such that,f: \mathbb{R} \rightarrow \mathbb{R} f(xy - 1) + f(x)f(y) = 2xy - 1,"Using induction, I proved that $f(x) = x$ and $f(x) = -x^2$ work, but only for rational numbers. How can I prove them for all real numbers?","Using induction, I proved that $f(x) = x$ and $f(x) = -x^2$ work, but only for rational numbers. How can I prove them for all real numbers?",,['linear-algebra']
50,Prove no field exists if $b+d\neq 0$,Prove no field exists if,b+d\neq 0,"I need to prove no field $K$ exists (with minimum 3 elements) which holds: $\forall a,c\in K, b,d\in K\setminus\{0\}: b+d\neq 0 \implies\frac a b + \frac c d = \frac{a+c}{b+d}$ I know this can't be true, but I don't know where to start to prove it. Probably my problem is with proving things false in general. Thanks in advance.","I need to prove no field $K$ exists (with minimum 3 elements) which holds: $\forall a,c\in K, b,d\in K\setminus\{0\}: b+d\neq 0 \implies\frac a b + \frac c d = \frac{a+c}{b+d}$ I know this can't be true, but I don't know where to start to prove it. Probably my problem is with proving things false in general. Thanks in advance.",,"['linear-algebra', 'field-theory']"
51,Solve $A^2=B$ where $B$ is the $3\times3$ matrix whose only nonzero entry is the top right entry,Solve  where  is the  matrix whose only nonzero entry is the top right entry,A^2=B B 3\times3,"Find all the matrices $A$ such that  $$A^2= \left( \begin {array}{ccc} 0&0&1\\ 0&0&0 \\ 0&0&0\end {array} \right) $$  where $A$ is a $3\times 3$ matrix. $A= \left( \begin {array}{ccc} 0&1&1\\ 0&0&1 \\ 0&0&0\end {array} \right) $  and  $A= \left( \begin {array}{ccc} 0&1&0\\ 0&0&1 \\ 0&0&0\end {array} \right) $ work, but how can I find all the matrices?","Find all the matrices $A$ such that  $$A^2= \left( \begin {array}{ccc} 0&0&1\\ 0&0&0 \\ 0&0&0\end {array} \right) $$  where $A$ is a $3\times 3$ matrix. $A= \left( \begin {array}{ccc} 0&1&1\\ 0&0&1 \\ 0&0&0\end {array} \right) $  and  $A= \left( \begin {array}{ccc} 0&1&0\\ 0&0&1 \\ 0&0&0\end {array} \right) $ work, but how can I find all the matrices?",,['linear-algebra']
52,How to take a partial derivative of $\|y - Xw\|^2$ with respect to w?,How to take a partial derivative of  with respect to w?,\|y - Xw\|^2,So I've tried to solve this problem where we are asked to solve the partial derivative of function $\sum(y - Xw)^2$ or $\|y-Xw\|^2$ and  then minimize it. I've never done any linear algebra aside some really basic stuff and I can't seem to find any information how to take partial derivative of such a function. I know how to take partial derivative of simple function but not functions with $\||x||^2$ notation. In this case it would probably help to denote some variable e.g $\ z=y-Xw $ that way we get $\|z\|^2 $. Is this even a right approach? I have no clue what to do after this.,So I've tried to solve this problem where we are asked to solve the partial derivative of function $\sum(y - Xw)^2$ or $\|y-Xw\|^2$ and  then minimize it. I've never done any linear algebra aside some really basic stuff and I can't seem to find any information how to take partial derivative of such a function. I know how to take partial derivative of simple function but not functions with $\||x||^2$ notation. In this case it would probably help to denote some variable e.g $\ z=y-Xw $ that way we get $\|z\|^2 $. Is this even a right approach? I have no clue what to do after this.,,"['calculus', 'linear-algebra']"
53,Rank of sum of matrix and arbitrarily small matrix,Rank of sum of matrix and arbitrarily small matrix,,Suppose I have an $n\times n$ matrix $A$ with full rank. Let $B$ be another $n\times n$ matrix and let $c>0$ be a constant. Can we always find a constant $c>0$ sufficiently small such that $$ A + cB $$ also has full rank?,Suppose I have an $n\times n$ matrix $A$ with full rank. Let $B$ be another $n\times n$ matrix and let $c>0$ be a constant. Can we always find a constant $c>0$ sufficiently small such that $$ A + cB $$ also has full rank?,,"['linear-algebra', 'matrices', 'matrix-rank']"
54,Proving $A$ is not invertible if $AB=A^2B^2-(AB)^2$ and $\det(B)=2$,Proving  is not invertible if  and,A AB=A^2B^2-(AB)^2 \det(B)=2,"Let $A$ and $B$ be two $2\times 2$ matrices with real element such that $$AB=A^2B^2-(AB)^2 \qquad\text{and}\qquad  \det(B)=2.$$ Show that $A=0$. My Attempt: \begin{align} AB=A^2B^2-(AB)^2 &\implies A=A^2B-ABA &&\text{(since $|B|\neq0$)} \\ &\implies A = A(AB) - A(BA) \\ &\implies \operatorname{tr}(A) = 0 \end{align} Also, by Cayley Hamilton theorem I could get $$A^2=-|A|I$$ This is all I could gather.","Let $A$ and $B$ be two $2\times 2$ matrices with real element such that $$AB=A^2B^2-(AB)^2 \qquad\text{and}\qquad  \det(B)=2.$$ Show that $A=0$. My Attempt: \begin{align} AB=A^2B^2-(AB)^2 &\implies A=A^2B-ABA &&\text{(since $|B|\neq0$)} \\ &\implies A = A(AB) - A(BA) \\ &\implies \operatorname{tr}(A) = 0 \end{align} Also, by Cayley Hamilton theorem I could get $$A^2=-|A|I$$ This is all I could gather.",,"['linear-algebra', 'matrices']"
55,Trying to formalise intuition into a proof that symmetric (hermitian) matrices are diagonalisable,Trying to formalise intuition into a proof that symmetric (hermitian) matrices are diagonalisable,,"The other day I stumbled upon LittleO's answer in this question . To make reading it easier I did a straight copy-and-paste here: Here's some intuition (but not a rigorous proof). If $A$ is hermitian (with entries in $\mathbb C$), you can easily show that the eigenvalues of $A$ are real and that eigenvectors corresponding to distinct eigenvalues are orthogonal. Typically, all the eigenvalues of $A$ are distinct.  (It is in some sense a huge coincidence if two eigenvalues turn out to be equal.) So, typically $A$ has an orthonormal basis of eigenvectors. Even if $A$ has some repeated eigenvalues, perturbing $A$ slightly will probably cause the eigenvalues to become distinct, in which case there is an orthonormal basis of eigenvectors.  By thinking of $A$ as a limit of slight perturbations of $A$, each of which has an ON basis of eigenvectors, it seems plausible that $A$ also has an ON basis of eigenvectors. While this idea struck me as very ingenious and intuitive, I'm really having difficulty formalising it into a real proof. The major obstacles are: 1). How to perturb an hermitian $A$ while keeping its eigenvalues distinct? In particular, how to find an hermitian sequence $A_n$ that have distinct eigenvalues and converge to $A$ in some norm? 2). Based on 1), how to continuously associate the eigenvector family $V_n:=[v_{n,j},j=1,\cdots,d]$ (corresponding to $A_n$, assuming $A_n$ are $d$ by $d$) to $A_n$? Having solved this I would have $V:=[v_j,j=1,\cdots,d]$ (corresponding to $A$) are the limits of $\{v_{n,j},j=1,\cdots,d\}$ respectively and hence $V^HV = \lim_{n\to\infty} V_n^HV_n = I$ and we are done.","The other day I stumbled upon LittleO's answer in this question . To make reading it easier I did a straight copy-and-paste here: Here's some intuition (but not a rigorous proof). If $A$ is hermitian (with entries in $\mathbb C$), you can easily show that the eigenvalues of $A$ are real and that eigenvectors corresponding to distinct eigenvalues are orthogonal. Typically, all the eigenvalues of $A$ are distinct.  (It is in some sense a huge coincidence if two eigenvalues turn out to be equal.) So, typically $A$ has an orthonormal basis of eigenvectors. Even if $A$ has some repeated eigenvalues, perturbing $A$ slightly will probably cause the eigenvalues to become distinct, in which case there is an orthonormal basis of eigenvectors.  By thinking of $A$ as a limit of slight perturbations of $A$, each of which has an ON basis of eigenvectors, it seems plausible that $A$ also has an ON basis of eigenvectors. While this idea struck me as very ingenious and intuitive, I'm really having difficulty formalising it into a real proof. The major obstacles are: 1). How to perturb an hermitian $A$ while keeping its eigenvalues distinct? In particular, how to find an hermitian sequence $A_n$ that have distinct eigenvalues and converge to $A$ in some norm? 2). Based on 1), how to continuously associate the eigenvector family $V_n:=[v_{n,j},j=1,\cdots,d]$ (corresponding to $A_n$, assuming $A_n$ are $d$ by $d$) to $A_n$? Having solved this I would have $V:=[v_j,j=1,\cdots,d]$ (corresponding to $A$) are the limits of $\{v_{n,j},j=1,\cdots,d\}$ respectively and hence $V^HV = \lim_{n\to\infty} V_n^HV_n = I$ and we are done.",,"['linear-algebra', 'eigenvalues-eigenvectors']"
56,angle between two vector when one vector is zero,angle between two vector when one vector is zero,,"I am trying to find angle between two vector . I know the theory . If u and v are two vectors then the angle between these two vector is defined as the following theorem : $$\theta=\arccos\left(\frac{\operatorname{Re}(u\cdot v)}{\|u\|\|v\|}\right)$$ where the inner product u⋅vu⋅v is defined to be $$u\cdot v=\sum_{k=0}^{n-1} u_k\overline{v_k}$$ But when when one of the two vectors is zero , then what will be  the angle between the two vector ? Suppose what will be angle of the following two vector : $$\vec{u} = 5\hat{i} + 2\hat{j}+3\hat{k}$$ $$\vec{v} = 0\hat{i} + 0\hat{j}+0\hat{k}$$ I cant figure out the angle between these two vectors . Please help me .","I am trying to find angle between two vector . I know the theory . If u and v are two vectors then the angle between these two vector is defined as the following theorem : $$\theta=\arccos\left(\frac{\operatorname{Re}(u\cdot v)}{\|u\|\|v\|}\right)$$ where the inner product u⋅vu⋅v is defined to be $$u\cdot v=\sum_{k=0}^{n-1} u_k\overline{v_k}$$ But when when one of the two vectors is zero , then what will be  the angle between the two vector ? Suppose what will be angle of the following two vector : $$\vec{u} = 5\hat{i} + 2\hat{j}+3\hat{k}$$ $$\vec{v} = 0\hat{i} + 0\hat{j}+0\hat{k}$$ I cant figure out the angle between these two vectors . Please help me .",,"['linear-algebra', 'vector-spaces']"
57,Understanding some properties $U(2)$,Understanding some properties,U(2),"I have read that the unitary group $U(2)$ can be naturally embedded as a subgroup of $SO(4)$. What exactly does this map look like? In particular, can we identify $U(2)$ as the set of matrices of  $SO(4)$ that are $\textit{complex}$-linear? Moreover, it seems that we still identify $\mathbb C^2$ with $\mathbb R^4$ here (correct?). Thus, if $U(2)$ is a subgroup of $SO(4)$, then wouldn't every unitary matrix be an orientation-preserving isometry of $\mathbb R^4$? But the problem with this is that this matrix could have determinant $-1$. How do we resolve this? Finally (and a bit vaguely), if every unitary matrix is an isometry of $\mathbb R^4$, then how can we interpret geometrically   the fact that this matrix is complex-linear? That is, can we view a unitary matrix as an isometry of $\mathbb C^2$ as a two-dimensional vector space over $\mathbb C$? (However, we would need a different norm in this case.)","I have read that the unitary group $U(2)$ can be naturally embedded as a subgroup of $SO(4)$. What exactly does this map look like? In particular, can we identify $U(2)$ as the set of matrices of  $SO(4)$ that are $\textit{complex}$-linear? Moreover, it seems that we still identify $\mathbb C^2$ with $\mathbb R^4$ here (correct?). Thus, if $U(2)$ is a subgroup of $SO(4)$, then wouldn't every unitary matrix be an orientation-preserving isometry of $\mathbb R^4$? But the problem with this is that this matrix could have determinant $-1$. How do we resolve this? Finally (and a bit vaguely), if every unitary matrix is an isometry of $\mathbb R^4$, then how can we interpret geometrically   the fact that this matrix is complex-linear? That is, can we view a unitary matrix as an isometry of $\mathbb C^2$ as a two-dimensional vector space over $\mathbb C$? (However, we would need a different norm in this case.)",,"['linear-algebra', 'abstract-algebra', 'group-theory', 'geometry']"
58,"Problem of rank, trace, determinant and eigenvalue","Problem of rank, trace, determinant and eigenvalue",,"Here is a problem and its solution that I translated from Korean (so it might contain some errors): Problem: For real n-by-n matrices $A,B$, suppose $xA+yB=I$ for non-zero real $x,y$ and $AB=0$.   Prove $$det(A+B) = \frac{1}{x^{\text{rank}(A)}y^{\text{rank}(B)}}.$$ Solution: Let $A'=xA$ and $B'=yB$. Then $A'+B'=I$ and $A'B'=0$. Then it follows that $A'=A'^2$ and $B'=B'^2$. So the minimal polynomials of $A'$ and $B'$ divides $x^2-x$.   Thus the eigenvalues are $0$ or $1$. Therefore $A'$ and $B'$ are diagonalizable.   Let $V,W$ be the eigenspace of $A'$, $B'$ respectively corresponding to the eigenvalue $1$.   Then $\text{trace}(A')=\text{dim}(V)$ and $\text{trace}(B')=\text{dim}(W)$. (*) Also $\text{trace}(A')+\text{trace}(B')=n$.   Thus $\text{dim}(V \cap W)=0$ and $R^n=V \oplus W$ (direct sum). (**) Thus    $$\text{det}(A+B)= \frac{1}{x}^{\text{dim}(V)} \frac{1}{y}^{\text{dim}(W)}=\frac{1}{x}^{\text{trace}(A')} \frac{1}{y}^{\text{trace}(B')}=\frac{1}{x^{\text{rank}(A)}y^{\text{rank}(B)}}$$ My question: 1) Why (*) holds? 2) Why (**) holds? I know that the sum of eigenvalues equals $\text{trace}(A)$ and the multiple equals $\text{det}(A)$. Thank you.","Here is a problem and its solution that I translated from Korean (so it might contain some errors): Problem: For real n-by-n matrices $A,B$, suppose $xA+yB=I$ for non-zero real $x,y$ and $AB=0$.   Prove $$det(A+B) = \frac{1}{x^{\text{rank}(A)}y^{\text{rank}(B)}}.$$ Solution: Let $A'=xA$ and $B'=yB$. Then $A'+B'=I$ and $A'B'=0$. Then it follows that $A'=A'^2$ and $B'=B'^2$. So the minimal polynomials of $A'$ and $B'$ divides $x^2-x$.   Thus the eigenvalues are $0$ or $1$. Therefore $A'$ and $B'$ are diagonalizable.   Let $V,W$ be the eigenspace of $A'$, $B'$ respectively corresponding to the eigenvalue $1$.   Then $\text{trace}(A')=\text{dim}(V)$ and $\text{trace}(B')=\text{dim}(W)$. (*) Also $\text{trace}(A')+\text{trace}(B')=n$.   Thus $\text{dim}(V \cap W)=0$ and $R^n=V \oplus W$ (direct sum). (**) Thus    $$\text{det}(A+B)= \frac{1}{x}^{\text{dim}(V)} \frac{1}{y}^{\text{dim}(W)}=\frac{1}{x}^{\text{trace}(A')} \frac{1}{y}^{\text{trace}(B')}=\frac{1}{x^{\text{rank}(A)}y^{\text{rank}(B)}}$$ My question: 1) Why (*) holds? 2) Why (**) holds? I know that the sum of eigenvalues equals $\text{trace}(A)$ and the multiple equals $\text{det}(A)$. Thank you.",,"['linear-algebra', 'eigenvalues-eigenvectors']"
59,Block diagonalize skew symmetric matrix with orthogonal transformation,Block diagonalize skew symmetric matrix with orthogonal transformation,,"I have a real-valued, antisymmetric matrix (A and B real): $$ H=\begin{bmatrix}     0 & A & 0 \\     -A & 0 & B \\     0 & -B & 0 \\ \end{bmatrix} $$ which can be diagonalized with a unitary transformation: $$ S= \frac{1}{\sqrt{A^2+B^2}} \begin{bmatrix}     B & \frac{-A}{\sqrt{2}} & \frac{-A}{\sqrt{2}} \\     0 & \frac{-i}{\sqrt{2}}\sqrt{A^2+B^2} & \frac{i}{\sqrt{2}}\sqrt{A^2+B^2} \\     A & \frac{B}{\sqrt{2}} & \frac{B}{\sqrt{2}} \\ \end{bmatrix} $$ such that $$ D=S^\dagger H S =  \begin{bmatrix}     0 & 0 & 0 \\     0 & i\lambda & 0 \\     0 & 0 & -i\lambda \\ \end{bmatrix} $$ where $\lambda=\sqrt{A^2+B^2}$ It is a known property of skew symmetric matrices that they can be brought to the block diagonal form containing the same eigenvalues as follows: $$ \Sigma=Q^T H Q =  \begin{bmatrix}     0 & 0 & 0 \\     0 & 0 & \lambda \\     0 & -\lambda & 0 \\ \end{bmatrix} $$ where $Q$ is a real orthogonal matrix. How do you generally construct the matrix $Q$? I have found two unitary transformations $$ W_{1} =  \begin{bmatrix}     1 & 0 & 0 \\     0 & \frac{-i}{\sqrt{2}} & \frac{1}{\sqrt{2}} \\     0 & \frac{1}{\sqrt{2}} & \frac{-i}{\sqrt{2}} \\ \end{bmatrix} $$ and $$ W_{2} =  \begin{bmatrix}     1 & 0 & 0 \\     0 & \frac{1}{\sqrt{2}} & \frac{-i}{\sqrt{2}} \\     0 & \frac{i}{\sqrt{2}} & \frac{-1}{\sqrt{2}} \\ \end{bmatrix} $$ which transform $D$ to $\Sigma$ as $\Sigma=W_{j}DW_{j}^\dagger$ for $j=1,2$. Then we have $H=TDT^\dagger$ where $T=SW_{j}^\dagger$. I would expect $T$ to be the real orthogonal matrix $Q$ I am looking for (or its transpose), but instead it is a complex-valued matrix (unitary by construction). How do I construct the real orthogonal matrix $Q$ in this example (and in general)? Similar questions on stackexchange that I couldn't figure out the answer from are: A unitary matrix taking a real matrix to another real matrix, is it an orthogonal matrix? Constructing a Darboux basis for a real, skew-symmetric matrix Eigenvectors of a skew-symmetric matrix","I have a real-valued, antisymmetric matrix (A and B real): $$ H=\begin{bmatrix}     0 & A & 0 \\     -A & 0 & B \\     0 & -B & 0 \\ \end{bmatrix} $$ which can be diagonalized with a unitary transformation: $$ S= \frac{1}{\sqrt{A^2+B^2}} \begin{bmatrix}     B & \frac{-A}{\sqrt{2}} & \frac{-A}{\sqrt{2}} \\     0 & \frac{-i}{\sqrt{2}}\sqrt{A^2+B^2} & \frac{i}{\sqrt{2}}\sqrt{A^2+B^2} \\     A & \frac{B}{\sqrt{2}} & \frac{B}{\sqrt{2}} \\ \end{bmatrix} $$ such that $$ D=S^\dagger H S =  \begin{bmatrix}     0 & 0 & 0 \\     0 & i\lambda & 0 \\     0 & 0 & -i\lambda \\ \end{bmatrix} $$ where $\lambda=\sqrt{A^2+B^2}$ It is a known property of skew symmetric matrices that they can be brought to the block diagonal form containing the same eigenvalues as follows: $$ \Sigma=Q^T H Q =  \begin{bmatrix}     0 & 0 & 0 \\     0 & 0 & \lambda \\     0 & -\lambda & 0 \\ \end{bmatrix} $$ where $Q$ is a real orthogonal matrix. How do you generally construct the matrix $Q$? I have found two unitary transformations $$ W_{1} =  \begin{bmatrix}     1 & 0 & 0 \\     0 & \frac{-i}{\sqrt{2}} & \frac{1}{\sqrt{2}} \\     0 & \frac{1}{\sqrt{2}} & \frac{-i}{\sqrt{2}} \\ \end{bmatrix} $$ and $$ W_{2} =  \begin{bmatrix}     1 & 0 & 0 \\     0 & \frac{1}{\sqrt{2}} & \frac{-i}{\sqrt{2}} \\     0 & \frac{i}{\sqrt{2}} & \frac{-1}{\sqrt{2}} \\ \end{bmatrix} $$ which transform $D$ to $\Sigma$ as $\Sigma=W_{j}DW_{j}^\dagger$ for $j=1,2$. Then we have $H=TDT^\dagger$ where $T=SW_{j}^\dagger$. I would expect $T$ to be the real orthogonal matrix $Q$ I am looking for (or its transpose), but instead it is a complex-valued matrix (unitary by construction). How do I construct the real orthogonal matrix $Q$ in this example (and in general)? Similar questions on stackexchange that I couldn't figure out the answer from are: A unitary matrix taking a real matrix to another real matrix, is it an orthogonal matrix? Constructing a Darboux basis for a real, skew-symmetric matrix Eigenvectors of a skew-symmetric matrix",,"['linear-algebra', 'matrices', 'eigenvalues-eigenvectors', 'diagonalization', 'unitary-matrices']"
60,"Hoffman Kunze, Lemma concerning triangulable operators","Hoffman Kunze, Lemma concerning triangulable operators",,"I am currently studying Linear Algebra by myself and recently I have stumbled across an important Lemma in Hoffman Kunze's book: Lemma .    Let $V$ be a finite-dimensional vector space over the field $F$. Let $T$ be linear operator on $V$ such that the minimal polynomial for $T$ is a product of linear factors $$p = (x-c_{1})^{r_{1}}...(x-c_{k})^{r_{k}}, c_{i} \in F.$$ Let $W$ be a proper subspace of $V$ which is invariant under $T$. there  exists a vector $\alpha$ in $V$ such that a) $\alpha\notin W$; b) $(T-cI)\alpha\in W$, for some characteristic value $c$ of the operator $T$. I must admit that I'm a little confused at this point. If I understand correctly, by using this Lemma we can produce a sequence $\{0\} = W_{0} \subset W_{1} \subset \ldots \subset W_{n} = V$ of invariant subspaces for $T$ s.t. $\dim{W_{i}}=i$, which would give me a basis in which the matrix of the operator $T$ is triangular. But this is not clear to me. Can someone explain to me the exact meaning of the condition $(b)$? Why this $\alpha$, together with the old basis, spans an invariant subspace? How does it all relates to the invariant hyperplanes - are those exactly what $W_{i}$'s are? The book says that $(a)$ and $(b)$ state that the $T$-conductor of $\alpha$ into $W$ (i.e. the unique monic generator of the ideal $S(\alpha ; W)$) is a linear polynomial - that is also a little confusing to me and I would appreciate some explanation of this statement. Thanks in advance for any help.","I am currently studying Linear Algebra by myself and recently I have stumbled across an important Lemma in Hoffman Kunze's book: Lemma .    Let $V$ be a finite-dimensional vector space over the field $F$. Let $T$ be linear operator on $V$ such that the minimal polynomial for $T$ is a product of linear factors $$p = (x-c_{1})^{r_{1}}...(x-c_{k})^{r_{k}}, c_{i} \in F.$$ Let $W$ be a proper subspace of $V$ which is invariant under $T$. there  exists a vector $\alpha$ in $V$ such that a) $\alpha\notin W$; b) $(T-cI)\alpha\in W$, for some characteristic value $c$ of the operator $T$. I must admit that I'm a little confused at this point. If I understand correctly, by using this Lemma we can produce a sequence $\{0\} = W_{0} \subset W_{1} \subset \ldots \subset W_{n} = V$ of invariant subspaces for $T$ s.t. $\dim{W_{i}}=i$, which would give me a basis in which the matrix of the operator $T$ is triangular. But this is not clear to me. Can someone explain to me the exact meaning of the condition $(b)$? Why this $\alpha$, together with the old basis, spans an invariant subspace? How does it all relates to the invariant hyperplanes - are those exactly what $W_{i}$'s are? The book says that $(a)$ and $(b)$ state that the $T$-conductor of $\alpha$ into $W$ (i.e. the unique monic generator of the ideal $S(\alpha ; W)$) is a linear polynomial - that is also a little confusing to me and I would appreciate some explanation of this statement. Thanks in advance for any help.",,"['linear-algebra', 'invariant-subspace']"
61,Proof that eigenvalue of matrix product smaller than 1,Proof that eigenvalue of matrix product smaller than 1,,"Suppose that we are given an $M\times N$ complex matrix  $\mathbf{A}$ and an $N\times N$ real diagonal matrix $\mathbf{D}$ with non-negative entries on the diagonal. Through numerical simulations, I found that the eigenvalues of $\mathbf{B}$, which is defined as $$\mathbf{B}=\mathbf{A}(\mathbf{A}^{\mathrm{H}}\mathbf{A} + \mathbf{D})^{-1}\mathbf{A}^{\mathrm{H}},$$ are no larger than $1$, where $(\cdot)^\mathrm{H}$ denotes matrix conjugate transpose. How can I prove that such an observation holds theoretically? Or is there any counter-example to show that this observation does not always hold? I notice that $\mathbf{A}(\mathbf{A}^{\mathrm{H}}\mathbf{A} + \mathbf{D})^{-1}\mathbf{A}^{\mathrm{H}}$ shares the same non-zero eigenvalues as  $(\mathbf{A}^{\mathrm{H}}\mathbf{A} + \mathbf{D})^{-1}\mathbf{A}^{\mathrm{H}}\mathbf{A}$. This motivates me to consider if I could approach this proof through an upper bound for the largest eigenvalue of matrix product, i.e., the product of $(\mathbf{A}^{\mathrm{H}}\mathbf{A} + \mathbf{D})^{-1}$ and $\mathbf{A}^{\mathrm{H}}\mathbf{A}$. However, so far I have gone nowhere. Any suggestion would be greatly appreciated.","Suppose that we are given an $M\times N$ complex matrix  $\mathbf{A}$ and an $N\times N$ real diagonal matrix $\mathbf{D}$ with non-negative entries on the diagonal. Through numerical simulations, I found that the eigenvalues of $\mathbf{B}$, which is defined as $$\mathbf{B}=\mathbf{A}(\mathbf{A}^{\mathrm{H}}\mathbf{A} + \mathbf{D})^{-1}\mathbf{A}^{\mathrm{H}},$$ are no larger than $1$, where $(\cdot)^\mathrm{H}$ denotes matrix conjugate transpose. How can I prove that such an observation holds theoretically? Or is there any counter-example to show that this observation does not always hold? I notice that $\mathbf{A}(\mathbf{A}^{\mathrm{H}}\mathbf{A} + \mathbf{D})^{-1}\mathbf{A}^{\mathrm{H}}$ shares the same non-zero eigenvalues as  $(\mathbf{A}^{\mathrm{H}}\mathbf{A} + \mathbf{D})^{-1}\mathbf{A}^{\mathrm{H}}\mathbf{A}$. This motivates me to consider if I could approach this proof through an upper bound for the largest eigenvalue of matrix product, i.e., the product of $(\mathbf{A}^{\mathrm{H}}\mathbf{A} + \mathbf{D})^{-1}$ and $\mathbf{A}^{\mathrm{H}}\mathbf{A}$. However, so far I have gone nowhere. Any suggestion would be greatly appreciated.",,"['linear-algebra', 'eigenvalues-eigenvectors']"
62,Komlós Conjecture Upper Bound,Komlós Conjecture Upper Bound,,"I was reading lecture notes for MIT 18.S096 and came across the following open problem in discrepancy theory known as the Komlós Conjecture: Given $n$, let $K(n)$ denote the infimum over all real numbers such that: for all sets of $n$ vectors $u_1,\ldots,u_n\in\mathbb{R}^n$ satisfying $\| u_i\|_2\leq 1$, there exists signs $\epsilon_i=\pm 1$ such that  \begin{equation} \| \epsilon_1u_1+\epsilon_2u_2\ldots+\epsilon_nu_n\|_{\infty}\leq K(n). \end{equation} There exists a universal constant $K$ such that $K(n)\leq K$ for all $n$. The author asserts that it is not too difficult to show the simpler claim that $K(n)\leq \sqrt{n}$; however, I'm having some difficulty showing this. The $\sqrt{n}$ is fairly suggestive of using Cauchy-Schwartz, but it's not immediately clear to me how one could use it; once can write the above problem in matrix form as $\min_{x\in \{-1,1\}^n} \|Ux\|_{\infty}$, where the columns of $U$ are the $u_i$, but the fundamental problem is that bounds on the row norms don't seem helpful. Any help would be appreciated; I feel like I'm missing something obvious. Thanks!","I was reading lecture notes for MIT 18.S096 and came across the following open problem in discrepancy theory known as the Komlós Conjecture: Given $n$, let $K(n)$ denote the infimum over all real numbers such that: for all sets of $n$ vectors $u_1,\ldots,u_n\in\mathbb{R}^n$ satisfying $\| u_i\|_2\leq 1$, there exists signs $\epsilon_i=\pm 1$ such that  \begin{equation} \| \epsilon_1u_1+\epsilon_2u_2\ldots+\epsilon_nu_n\|_{\infty}\leq K(n). \end{equation} There exists a universal constant $K$ such that $K(n)\leq K$ for all $n$. The author asserts that it is not too difficult to show the simpler claim that $K(n)\leq \sqrt{n}$; however, I'm having some difficulty showing this. The $\sqrt{n}$ is fairly suggestive of using Cauchy-Schwartz, but it's not immediately clear to me how one could use it; once can write the above problem in matrix form as $\min_{x\in \{-1,1\}^n} \|Ux\|_{\infty}$, where the columns of $U$ are the $u_i$, but the fundamental problem is that bounds on the row norms don't seem helpful. Any help would be appreciated; I feel like I'm missing something obvious. Thanks!",,"['linear-algebra', 'functional-analysis']"
63,Help understand equivalence of two optimization problems,Help understand equivalence of two optimization problems,,"The problem is the circled part. I don't fully understand why they are equivalent. In their notation, two vectors ${\bf x} \ge {\bf y}$ means every component of $\bf x$ is larger or equal to the corresponding component of vector $\bf y$. My attempt and more explanation (might contain error): By my understanding, $\bf x$ here should be treated as fixed. The first optimization is varying ${\bf a}_i$, and the second optimization is varying ${\bf p}_i$. I think the two optimizations are equivalent in the sense that $\max_{{\bf a}_i} {\bf a}_i^T{\bf x} =\min_{{\bf p}_i} {\bf p}^T_i{\bf d}_i$. It is easy to see ${\bf D}_i{\bf a}_i \le {\bf d}_i$ is equivalent to ${\bf p}_i^T{\bf D}_i{\bf a}_i \le {\bf p}_i^T{\bf d}_i, \forall {\bf p}_i \ge {\bf 0}$. Now if there exists ${\bf p}_i$ s.t. ${\bf p}_i^T{\bf D}_i = {\bf x}^T$, then we have $${{\bf{x}}^T}{{\bf{a}}_i} = {\bf{p}}_i^T{{\bf{D}}_i}{{\bf{a}}_i} \leqslant {\bf{p}}_i^T{{\bf{d}}_i},\forall {{\bf{p}}_i} \geqslant {\bf{0}},{{\bf{p}_i}^T}{{\bf{D}}_i} = {\bf{x}}_i^T$$ Thus the maximum value of ${{\bf{x}}^T}{{\bf{a}}_i}$ is at most $\inf \{ {\bf{p}}_i^T{{\bf{d}}_i}:{{\bf{p}}_i} \geqslant {\bf{0}},{{\bf{p}}_i^T}{{\bf{d}}_i} = {\bf{x}}_i^T\} $. Here comes the question : First , The equivalence in the circled part of the screen shot means $$\max \{ {{\bf{x}}^T}{{\bf{a}}_i}:{\bf{p}}_i^T{{\bf{D}}_i}{{\bf{a}}_i} \leqslant {\bf{p}}_i^T{{\bf{d}}_i},\forall {{\bf{p}}_i} \geqslant {\bf{0}}\}  = \inf \{ {\bf{p}}_i^T{{\bf{d}}_i}:{{\bf{p}}_i} \geqslant {\bf{0}},{{\bf{x}}^T}{{\bf{a}}_i} = {\bf{p}}_i^T\} $$ but I can only arrive at ""$\le$"" rather tahn $=$ from above discussion.   $$\max \{ {{\bf{x}}^T}{{\bf{a}}_i}:{\bf{p}}_i^T{{\bf{D}}_i}{{\bf{a}}_i} \leqslant {\bf{p}}_i^T{{\bf{d}}_i},\forall {{\bf{p}}_i} \geqslant {\bf{0}}\}  \leqslant \inf \{ {\bf{p}}_i^T{{\bf{d}}_i}:{{\bf{p}}_i} \geqslant {\bf{0}},{{\bf{x}}^T}{{\bf{a}}_i} = {\bf{p}}_i^T\} $$ I am not able to see the full equivalence. Secondly , What happens if there does not exist ${\bf p}_i$ s.t. ${\bf p}_i^T{\bf D}_i = {\bf x}^T$? Paper: https://faculty.fuqua.duke.edu/~dbbrown/bio/papers/bertsimas_brown_caramanis_11.pdf","The problem is the circled part. I don't fully understand why they are equivalent. In their notation, two vectors ${\bf x} \ge {\bf y}$ means every component of $\bf x$ is larger or equal to the corresponding component of vector $\bf y$. My attempt and more explanation (might contain error): By my understanding, $\bf x$ here should be treated as fixed. The first optimization is varying ${\bf a}_i$, and the second optimization is varying ${\bf p}_i$. I think the two optimizations are equivalent in the sense that $\max_{{\bf a}_i} {\bf a}_i^T{\bf x} =\min_{{\bf p}_i} {\bf p}^T_i{\bf d}_i$. It is easy to see ${\bf D}_i{\bf a}_i \le {\bf d}_i$ is equivalent to ${\bf p}_i^T{\bf D}_i{\bf a}_i \le {\bf p}_i^T{\bf d}_i, \forall {\bf p}_i \ge {\bf 0}$. Now if there exists ${\bf p}_i$ s.t. ${\bf p}_i^T{\bf D}_i = {\bf x}^T$, then we have $${{\bf{x}}^T}{{\bf{a}}_i} = {\bf{p}}_i^T{{\bf{D}}_i}{{\bf{a}}_i} \leqslant {\bf{p}}_i^T{{\bf{d}}_i},\forall {{\bf{p}}_i} \geqslant {\bf{0}},{{\bf{p}_i}^T}{{\bf{D}}_i} = {\bf{x}}_i^T$$ Thus the maximum value of ${{\bf{x}}^T}{{\bf{a}}_i}$ is at most $\inf \{ {\bf{p}}_i^T{{\bf{d}}_i}:{{\bf{p}}_i} \geqslant {\bf{0}},{{\bf{p}}_i^T}{{\bf{d}}_i} = {\bf{x}}_i^T\} $. Here comes the question : First , The equivalence in the circled part of the screen shot means $$\max \{ {{\bf{x}}^T}{{\bf{a}}_i}:{\bf{p}}_i^T{{\bf{D}}_i}{{\bf{a}}_i} \leqslant {\bf{p}}_i^T{{\bf{d}}_i},\forall {{\bf{p}}_i} \geqslant {\bf{0}}\}  = \inf \{ {\bf{p}}_i^T{{\bf{d}}_i}:{{\bf{p}}_i} \geqslant {\bf{0}},{{\bf{x}}^T}{{\bf{a}}_i} = {\bf{p}}_i^T\} $$ but I can only arrive at ""$\le$"" rather tahn $=$ from above discussion.   $$\max \{ {{\bf{x}}^T}{{\bf{a}}_i}:{\bf{p}}_i^T{{\bf{D}}_i}{{\bf{a}}_i} \leqslant {\bf{p}}_i^T{{\bf{d}}_i},\forall {{\bf{p}}_i} \geqslant {\bf{0}}\}  \leqslant \inf \{ {\bf{p}}_i^T{{\bf{d}}_i}:{{\bf{p}}_i} \geqslant {\bf{0}},{{\bf{x}}^T}{{\bf{a}}_i} = {\bf{p}}_i^T\} $$ I am not able to see the full equivalence. Secondly , What happens if there does not exist ${\bf p}_i$ s.t. ${\bf p}_i^T{\bf D}_i = {\bf x}^T$? Paper: https://faculty.fuqua.duke.edu/~dbbrown/bio/papers/bertsimas_brown_caramanis_11.pdf",,"['linear-algebra', 'optimization']"
64,Dimension of $W=\{p(x) : p(x)=p(1-x)\}$.,Dimension of .,W=\{p(x) : p(x)=p(1-x)\},"Find the dimension of the subspace $W$ of $P_n(x)$ , space of all polynomials ; where $\displaystyle W=\{p(x) : p(x)=p(1-x)\}$. I just found that the polynomials satisfies the condition $p(x)=p(1-x)$ are of the types $p(x)=x^n(1-x)^n$ for every positive integer $x$. But I don't know whether there are more than this type of polynomials or not and how I can find the dimension.","Find the dimension of the subspace $W$ of $P_n(x)$ , space of all polynomials ; where $\displaystyle W=\{p(x) : p(x)=p(1-x)\}$. I just found that the polynomials satisfies the condition $p(x)=p(1-x)$ are of the types $p(x)=x^n(1-x)^n$ for every positive integer $x$. But I don't know whether there are more than this type of polynomials or not and how I can find the dimension.",,"['linear-algebra', 'algebra-precalculus', 'polynomials', 'vector-spaces']"
65,Determinants and $2\times 2$ minors,Determinants and  minors,2\times 2,"I just proved that $$0=\left|\begin{array}{cccc} 	a_0 & a_1 & a_2 & a_3 \\ 	b_0 & b_1 & b_2 & b_3 \\ 	a_0 & a_1 & a_2 & a_3 \\ 	b_0 & b_1 & b_2 & b_3 \end{array}\right|=2(\Delta_{01}\Delta_{23}-\Delta_{02}\Delta_{13}+\Delta_{03}\Delta_{12})$$ where  $$\Delta_{ij}=\left|\begin{array}{cc} a_i & a_j \\ b_i & b_j \end{array}\right|$$ Is there any way of expressing the following determinant in a similar way? That is, is there any way of expanding the following determinant as a sum of products of minors of order $2\times 2$ of the form $\Delta_{ij}$? $$0=\left|\begin{array}{cccc} 	a_0 & a_1 & \cdots & a_n \\ 	b_0 & b_1 & \cdots & b_n \\ 	\vdots & \vdots & \cdots & \vdots \\ 	a_0 & a_1 & \cdots & a_n \\ 	b_0 & b_1 & \cdots & b_n  \end{array}\right|$$ (of course, one must take into account the parity of $n$). I need it in order to find the set of zeroes that defines a projective algebraic variety.","I just proved that $$0=\left|\begin{array}{cccc} 	a_0 & a_1 & a_2 & a_3 \\ 	b_0 & b_1 & b_2 & b_3 \\ 	a_0 & a_1 & a_2 & a_3 \\ 	b_0 & b_1 & b_2 & b_3 \end{array}\right|=2(\Delta_{01}\Delta_{23}-\Delta_{02}\Delta_{13}+\Delta_{03}\Delta_{12})$$ where  $$\Delta_{ij}=\left|\begin{array}{cc} a_i & a_j \\ b_i & b_j \end{array}\right|$$ Is there any way of expressing the following determinant in a similar way? That is, is there any way of expanding the following determinant as a sum of products of minors of order $2\times 2$ of the form $\Delta_{ij}$? $$0=\left|\begin{array}{cccc} 	a_0 & a_1 & \cdots & a_n \\ 	b_0 & b_1 & \cdots & b_n \\ 	\vdots & \vdots & \cdots & \vdots \\ 	a_0 & a_1 & \cdots & a_n \\ 	b_0 & b_1 & \cdots & b_n  \end{array}\right|$$ (of course, one must take into account the parity of $n$). I need it in order to find the set of zeroes that defines a projective algebraic variety.",,"['linear-algebra', 'determinant']"
66,"If elements of matrix are bounded, its determinant is bounded","If elements of matrix are bounded, its determinant is bounded",,"If absolute values of all elements of $n \times n$ matrix A are less than 1,   $|\det(A)| \leq n ^ {n/2}$ It's proved by induction by n. Base is obvious, and transition is the following: consider the $n \times n$ matrix A'. Let the elements from the first row be $a_0, ..., a_{n-1}$. Then $\det(A') = \Sigma_{i=0}^{n-1} (-1)^{i} a_i \det(A_i)$, where $A_i$ are the matrices obtained from $A'$ by deleting the $i$th column and the first row. $\forall A_i \det(A_i) \leq (n-1)^{(n-1)/2}$. So  $\det(A') = \Sigma_{i=0}^{n-1} (-1)^{i} a_i \det(A_i) \leq \Sigma_{i=0}^{n-1} a_i (n-1)^{(n-1)/2} \leq n(n-1)^{(n-1)/2}.$ The last step is to prove that $\forall n \geq 2$ it's true that $n(n-1)^{(n-1)/2} \leq n^{n/2} $. Here I am stuck. I've plotted it and it is true, but I have no idea how to prove it strictly. And maybe a more elegant solution exists? Induction is quite boring.","If absolute values of all elements of $n \times n$ matrix A are less than 1,   $|\det(A)| \leq n ^ {n/2}$ It's proved by induction by n. Base is obvious, and transition is the following: consider the $n \times n$ matrix A'. Let the elements from the first row be $a_0, ..., a_{n-1}$. Then $\det(A') = \Sigma_{i=0}^{n-1} (-1)^{i} a_i \det(A_i)$, where $A_i$ are the matrices obtained from $A'$ by deleting the $i$th column and the first row. $\forall A_i \det(A_i) \leq (n-1)^{(n-1)/2}$. So  $\det(A') = \Sigma_{i=0}^{n-1} (-1)^{i} a_i \det(A_i) \leq \Sigma_{i=0}^{n-1} a_i (n-1)^{(n-1)/2} \leq n(n-1)^{(n-1)/2}.$ The last step is to prove that $\forall n \geq 2$ it's true that $n(n-1)^{(n-1)/2} \leq n^{n/2} $. Here I am stuck. I've plotted it and it is true, but I have no idea how to prove it strictly. And maybe a more elegant solution exists? Induction is quite boring.",,"['linear-algebra', 'matrices', 'induction']"
67,Visualizing transpose of matrices,Visualizing transpose of matrices,,"I am a new linear-algebra student. I find linear algebra really difficult ,however, visualizing concepts really helps me out. So I stumbled upon rotation matrices and the book stated that the transpose of B times B must equal to the identity matrix. So are there any sources I could use to maybe view transposes for myself and kind of figure it out. And if not, I would greatly appreciate any help. Thanks!","I am a new linear-algebra student. I find linear algebra really difficult ,however, visualizing concepts really helps me out. So I stumbled upon rotation matrices and the book stated that the transpose of B times B must equal to the identity matrix. So are there any sources I could use to maybe view transposes for myself and kind of figure it out. And if not, I would greatly appreciate any help. Thanks!",,"['linear-algebra', 'matrices', 'transpose']"
68,"Determinant of matrix that is diagonal, but for last row/column","Determinant of matrix that is diagonal, but for last row/column",,"I would like to compute the determinant of a symmetric $(K+1)\times (K+1)$ matrix in which the upper left $K \times K$ matrix is diagonal but the $(K+1)$th row and column are complete. E.g. $$ X = \begin{bmatrix}  x_1 & 0 & \dots & 0 & y_1 \\ 0 & x_2 & \dots & 0 & y_2 \\ \vdots & \vdots & \ddots & \vdots & \vdots \\ 0 & 0 & \dots & x_K & y_K \\ y_1 & y_2 & \dots & y_K & y_{K+1} \end{bmatrix} $$  Is there a simple way to compute $\det(X)$? Note, a similar question was asked for a matrix with the same form but with more constraints on the entries: Determinant of an almost-diagonal matrix I can't see that the answer to this question helps here though.","I would like to compute the determinant of a symmetric $(K+1)\times (K+1)$ matrix in which the upper left $K \times K$ matrix is diagonal but the $(K+1)$th row and column are complete. E.g. $$ X = \begin{bmatrix}  x_1 & 0 & \dots & 0 & y_1 \\ 0 & x_2 & \dots & 0 & y_2 \\ \vdots & \vdots & \ddots & \vdots & \vdots \\ 0 & 0 & \dots & x_K & y_K \\ y_1 & y_2 & \dots & y_K & y_{K+1} \end{bmatrix} $$  Is there a simple way to compute $\det(X)$? Note, a similar question was asked for a matrix with the same form but with more constraints on the entries: Determinant of an almost-diagonal matrix I can't see that the answer to this question helps here though.",,"['linear-algebra', 'matrices', 'determinant']"
69,How to know if equation adds more info to a set of equations?,How to know if equation adds more info to a set of equations?,,"I'm looking for a formalized way to be able to know whether adding an equation to a set of linear equations adds new information or not, does anyone know of a way? For a really simple case, let's say I have these equations: $A = 3\\ B+C = 4\\ D = 8 $ Obviously, the equation below doesn't add any new information: $-A = -3$ But it gets less obvious when adding an equation like this: $B + D = 6$ Or: $C = 2$ When the list of equations is longer, it gets a lot less obvious to me as well: $ A=3\\ B+C=4\\ D=8\\ C=4\\ F=3\\ E=12\\ F+G=4\\ H=1 $ If I want to add a new equation: $D+E=7$ I can see that $D$ and $E$ are already fully specified, so this doesn't really add any new information, but a more complex expression would be harder for me to rule out. Is there a formalized way to see whether or not adding a new equation will add more information?  Possibly by looking at the matrix form of the equations? Here is the last group of equations: $ \begin{bmatrix} 1 & 0 & 0 & 0 & 0 & 0 & 0 & 0 \\ 0 & 1 & 1 & 0 & 0 & 0 & 0 & 0 \\ 0 & 0 & 0 & 1 & 0 & 0 & 0 & 0 \\ 0 & 0 & 1 & 0 & 0 & 0 & 0 & 0 \\ 0 & 0 & 0 & 0 & 0 & 1 & 0 & 0 \\ 0 & 0 & 0 & 0 & 1 & 0 & 0 & 0 \\ 0 & 0 & 0 & 0 & 0 & 1 & 1 & 0 \\ 0 & 0 & 0 & 0 & 0 & 0 & 0 & 1 \\ \end{bmatrix} *  \begin{bmatrix} A \\ B \\ C \\ D \\ E \\ F \\ G \\ H \\ \end{bmatrix} = \begin{bmatrix} 3 \\ 4 \\ 8 \\ 4 \\ 3 \\ 12 \\ 4 \\ 1 \\ \end{bmatrix} $ And here is the equation I wanted to add to the set: $ \begin{bmatrix} 0 & 0 & 0 & 1 & 1 & 0 & 0 & 0 \\ \end{bmatrix} *  \begin{bmatrix} A \\ B \\ C \\ D \\ E \\ F \\ G \\ H \\ \end{bmatrix} = 7 $ Thanks for any help you can provide!","I'm looking for a formalized way to be able to know whether adding an equation to a set of linear equations adds new information or not, does anyone know of a way? For a really simple case, let's say I have these equations: $A = 3\\ B+C = 4\\ D = 8 $ Obviously, the equation below doesn't add any new information: $-A = -3$ But it gets less obvious when adding an equation like this: $B + D = 6$ Or: $C = 2$ When the list of equations is longer, it gets a lot less obvious to me as well: $ A=3\\ B+C=4\\ D=8\\ C=4\\ F=3\\ E=12\\ F+G=4\\ H=1 $ If I want to add a new equation: $D+E=7$ I can see that $D$ and $E$ are already fully specified, so this doesn't really add any new information, but a more complex expression would be harder for me to rule out. Is there a formalized way to see whether or not adding a new equation will add more information?  Possibly by looking at the matrix form of the equations? Here is the last group of equations: $ \begin{bmatrix} 1 & 0 & 0 & 0 & 0 & 0 & 0 & 0 \\ 0 & 1 & 1 & 0 & 0 & 0 & 0 & 0 \\ 0 & 0 & 0 & 1 & 0 & 0 & 0 & 0 \\ 0 & 0 & 1 & 0 & 0 & 0 & 0 & 0 \\ 0 & 0 & 0 & 0 & 0 & 1 & 0 & 0 \\ 0 & 0 & 0 & 0 & 1 & 0 & 0 & 0 \\ 0 & 0 & 0 & 0 & 0 & 1 & 1 & 0 \\ 0 & 0 & 0 & 0 & 0 & 0 & 0 & 1 \\ \end{bmatrix} *  \begin{bmatrix} A \\ B \\ C \\ D \\ E \\ F \\ G \\ H \\ \end{bmatrix} = \begin{bmatrix} 3 \\ 4 \\ 8 \\ 4 \\ 3 \\ 12 \\ 4 \\ 1 \\ \end{bmatrix} $ And here is the equation I wanted to add to the set: $ \begin{bmatrix} 0 & 0 & 0 & 1 & 1 & 0 & 0 & 0 \\ \end{bmatrix} *  \begin{bmatrix} A \\ B \\ C \\ D \\ E \\ F \\ G \\ H \\ \end{bmatrix} = 7 $ Thanks for any help you can provide!",,"['linear-algebra', 'systems-of-equations']"
70,"Null space, column space and rank with projection matrix","Null space, column space and rank with projection matrix",,"If I have a projection matrix L in $\mathbb {R^4}$ , I'm just wondering how L would transform vectors in the nullspace of $[L]$ and the column space. I'm also trying to figure out how these pieces of information allow me to find the rank and nullity of $[L]$ without elementary row operations. For context, here is the question: $\text{The linear transformation of $L:\mathbb {R^4}\rightarrow \mathbb {R^4}$ projects $\mathbb {R^4}$ orthogonally}$ $\text{onto the subspace $V=\text{span}\{a,b\}$, with}:$ $a=(1,1,1,1)$ $b=(4,2,1,2)$ $\text { (a) How does L transform vectors transform vectors in the null space of [L]?}$ $\text { (b) How does L transform vectors transform vectors in the Column space of [L]?}$ $\text {(c)Explain how the answers to parts (a) and (b) }$ $\text{enable you to find the rank and the nullity of [L] without row reduction.}$ I'm looking at notes here , but I'm having a hard time coming up with some reasoning. I can see that the null space and the column space are orthogonal to each other but I am not really sure how that would explain the transformation or help with part $(c)$ in any way. If someone could nudge me in the right direction that would be great! I mean I feel like null space comes into play somehow because I am doing projections and since I want the matrix to be orthogonal, the dot product has to be $0$ so I am trying to see if I can relate that somehow. The fact I am writing the vectors as columns I feel like has to do with something in the column space but I'm not entirely sure about that... For part $(c)$, I feel like I have to use the rank nullity theorem somehow but I am not sure about this...","If I have a projection matrix L in $\mathbb {R^4}$ , I'm just wondering how L would transform vectors in the nullspace of $[L]$ and the column space. I'm also trying to figure out how these pieces of information allow me to find the rank and nullity of $[L]$ without elementary row operations. For context, here is the question: $\text{The linear transformation of $L:\mathbb {R^4}\rightarrow \mathbb {R^4}$ projects $\mathbb {R^4}$ orthogonally}$ $\text{onto the subspace $V=\text{span}\{a,b\}$, with}:$ $a=(1,1,1,1)$ $b=(4,2,1,2)$ $\text { (a) How does L transform vectors transform vectors in the null space of [L]?}$ $\text { (b) How does L transform vectors transform vectors in the Column space of [L]?}$ $\text {(c)Explain how the answers to parts (a) and (b) }$ $\text{enable you to find the rank and the nullity of [L] without row reduction.}$ I'm looking at notes here , but I'm having a hard time coming up with some reasoning. I can see that the null space and the column space are orthogonal to each other but I am not really sure how that would explain the transformation or help with part $(c)$ in any way. If someone could nudge me in the right direction that would be great! I mean I feel like null space comes into play somehow because I am doing projections and since I want the matrix to be orthogonal, the dot product has to be $0$ so I am trying to see if I can relate that somehow. The fact I am writing the vectors as columns I feel like has to do with something in the column space but I'm not entirely sure about that... For part $(c)$, I feel like I have to use the rank nullity theorem somehow but I am not sure about this...",,"['linear-algebra', 'linear-transformations']"
71,Find a matrix with a given null space,Find a matrix with a given null space,,"I’m trying to solve the following question: Find a $3\times 3$ matrix $A$ such that $\operatorname{Null}(A)=\operatorname{span}\left\lbrace \begin{bmatrix}1 \\1 \\1 \\\end{bmatrix},\begin{bmatrix}1\\2\\3\end{bmatrix}\right\rbrace$ . My attempt was this. Let $A=\begin{bmatrix}a_{11}&a_{12}&a_{13}\\a_{21}&a_{22}&a_{23}\\a_{31}&a_{32}&a_{33} \end{bmatrix}$ . Then I solved $A\begin{bmatrix}1 \\1 \\1 \\\end{bmatrix}=\begin{bmatrix}0\\0\\0\end{bmatrix}$ and $A\begin{bmatrix}1\\2\\3\end{bmatrix}=\begin{bmatrix}0\\0\\0\end{bmatrix}$ which gave me the following conditions $a_{12}+2a_{13}$ =0 , $a_{22}+2a_{23}=0$ and $a_{32}+2a_{33}=0$ . Solving these I can get a matrix that look like this $A=\begin{bmatrix}1&-2&1\\1&-2&1\\1&-2&1\\\end{bmatrix}$ . Is my approach correct? I’m asking because I wanted to check my answer and calculated the null space for the matrix I found above and the null space was spanned by $\left\lbrace\begin{bmatrix}-1\\0\\1\end{bmatrix},\begin{bmatrix}2\\1\\0\end{bmatrix}\right\rbrace$ .","I’m trying to solve the following question: Find a matrix such that . My attempt was this. Let . Then I solved and which gave me the following conditions =0 , and . Solving these I can get a matrix that look like this . Is my approach correct? I’m asking because I wanted to check my answer and calculated the null space for the matrix I found above and the null space was spanned by .","3\times 3 A \operatorname{Null}(A)=\operatorname{span}\left\lbrace \begin{bmatrix}1 \\1 \\1 \\\end{bmatrix},\begin{bmatrix}1\\2\\3\end{bmatrix}\right\rbrace A=\begin{bmatrix}a_{11}&a_{12}&a_{13}\\a_{21}&a_{22}&a_{23}\\a_{31}&a_{32}&a_{33} \end{bmatrix} A\begin{bmatrix}1 \\1 \\1 \\\end{bmatrix}=\begin{bmatrix}0\\0\\0\end{bmatrix} A\begin{bmatrix}1\\2\\3\end{bmatrix}=\begin{bmatrix}0\\0\\0\end{bmatrix} a_{12}+2a_{13} a_{22}+2a_{23}=0 a_{32}+2a_{33}=0 A=\begin{bmatrix}1&-2&1\\1&-2&1\\1&-2&1\\\end{bmatrix} \left\lbrace\begin{bmatrix}-1\\0\\1\end{bmatrix},\begin{bmatrix}2\\1\\0\end{bmatrix}\right\rbrace","['linear-algebra', 'matrices', 'eigenvalues-eigenvectors', 'matrix-rank']"
72,Eigenvalue decomposition of non symmetric matrix,Eigenvalue decomposition of non symmetric matrix,,"Often in examples, eigenvalue decomposition $A=U\Lambda U^T$, $A$ is usually assumed to be a symmetric matrix. I am wondering what are the differences and implications when $A$ is a non-symmetric (still positive values if that helps). What can we say about the eigenvalues and eigenvectors of such decomposition?","Often in examples, eigenvalue decomposition $A=U\Lambda U^T$, $A$ is usually assumed to be a symmetric matrix. I am wondering what are the differences and implications when $A$ is a non-symmetric (still positive values if that helps). What can we say about the eigenvalues and eigenvectors of such decomposition?",,"['linear-algebra', 'eigenvalues-eigenvectors', 'spectral-theory', 'matrix-decomposition']"
73,When is the union of a family of subspaces of a vector space also a subspace?,When is the union of a family of subspaces of a vector space also a subspace?,,"It is not difficult to prove that the union of a chain (or, more generally, a directed family) of subspaces of a vector space $V$ is a subspace of $V$. Given a family $\mathcal{F}$ of subspaces of a vector space $V$ such that the union of $\mathcal{F}$ is a subspace of $V$, is it true that $\mathcal{F}$ is a directed family? If not, is there a ""nice"" characterization of families of subspaces whose union is a subspace?","It is not difficult to prove that the union of a chain (or, more generally, a directed family) of subspaces of a vector space $V$ is a subspace of $V$. Given a family $\mathcal{F}$ of subspaces of a vector space $V$ such that the union of $\mathcal{F}$ is a subspace of $V$, is it true that $\mathcal{F}$ is a directed family? If not, is there a ""nice"" characterization of families of subspaces whose union is a subspace?",,['linear-algebra']
74,From Hermitian matrix operator to differential equations,From Hermitian matrix operator to differential equations,,"I understand that if my operator $A$ is a Hermitian, then suppose it has the following eigen vectors $$A|v_1\rangle=\lambda|v_1\rangle:(A).(V_1)=\lambda(V_1)\\ A|v_2\rangle=\beta|v_2\rangle:(A).(V_2)=\beta(V_2)\\ \langle v_2|A|v_1\rangle=\lambda\langle v_2|v_1\rangle:(V_2)^{\dagger}.(A).(V_1)=\lambda(V_2)^{\dagger}.(V_1)\\ \text{since}(A).(V_2)=\beta(V_2)\quad\text{hence}\quad (V2)^{\dagger}.(A)^{\dagger}=\beta(V_2)^{\dagger}$$ Now, if $A$ is a hermitian operator, then $A=A^{\dagger}$, hence $$(V_2)^{\dagger}.(A)^{\dagger}=(V_2)^{\dagger}.(A)=\beta(V_2)^{\dagger}\\ \text{Hence}\quad (V_2)^{\dagger}.(A).(V_1)=\beta(V_2)^{\dagger}(V_1)\\ \text{implies} \quad\langle v_2|A|v_1\rangle =\lambda\langle v_2|v_1\rangle = \beta\langle v_2|v_1\rangle:\lambda\neq\beta\\ \text{Hence}\quad \langle v_2|v_1\rangle=0$$ So, the vectors must be orthogonal to each other and their inner product must be zero, but how do I know in a differential equation if my operator is a hermitian or not and whether I will have quantized solutions whose inner products will be zero? For example the schrodinger wave equation leads to quantized orthogonal solutions. Or rather, how do I extend a matrix based Hermitian operator to a differential equation?","I understand that if my operator $A$ is a Hermitian, then suppose it has the following eigen vectors $$A|v_1\rangle=\lambda|v_1\rangle:(A).(V_1)=\lambda(V_1)\\ A|v_2\rangle=\beta|v_2\rangle:(A).(V_2)=\beta(V_2)\\ \langle v_2|A|v_1\rangle=\lambda\langle v_2|v_1\rangle:(V_2)^{\dagger}.(A).(V_1)=\lambda(V_2)^{\dagger}.(V_1)\\ \text{since}(A).(V_2)=\beta(V_2)\quad\text{hence}\quad (V2)^{\dagger}.(A)^{\dagger}=\beta(V_2)^{\dagger}$$ Now, if $A$ is a hermitian operator, then $A=A^{\dagger}$, hence $$(V_2)^{\dagger}.(A)^{\dagger}=(V_2)^{\dagger}.(A)=\beta(V_2)^{\dagger}\\ \text{Hence}\quad (V_2)^{\dagger}.(A).(V_1)=\beta(V_2)^{\dagger}(V_1)\\ \text{implies} \quad\langle v_2|A|v_1\rangle =\lambda\langle v_2|v_1\rangle = \beta\langle v_2|v_1\rangle:\lambda\neq\beta\\ \text{Hence}\quad \langle v_2|v_1\rangle=0$$ So, the vectors must be orthogonal to each other and their inner product must be zero, but how do I know in a differential equation if my operator is a hermitian or not and whether I will have quantized solutions whose inner products will be zero? For example the schrodinger wave equation leads to quantized orthogonal solutions. Or rather, how do I extend a matrix based Hermitian operator to a differential equation?",,"['linear-algebra', 'functional-analysis', 'partial-differential-equations', 'vector-spaces', 'mathematical-physics']"
75,How to find limits using change of variables?,How to find limits using change of variables?,,"In my country education system, in Math we don't have the right to use L'Hopital's rule to solve indeterminate forms limits for this year. Instead we use differnt techniques, such as expanding expressions and recently we used change of variable in order to compute limits for different functions that have $ln$ or $e^x$ in them . For $\ln$ we know the following limits, we don't need to prove them : $$\lim_{x\to\infty} \ln x/x = 0^+$$ $$\lim_{x\to 0}\ln (x+1)/x = 1 $$ $$\lim_{x\to0} \ln (x)x = 0^-$$ Then, whenever we have a limit with $ln$ that yield an indeterminate form we use change of variable in order to get a limit that is similar to one of these 3 limits. Example : For example we need to compute the following limit : $$\lim_{x\to-\infty} \frac{x}{\ln|x|}$$ For this one we get an indeterminate form, I need to use the change of variable to try to get a limit that is similar to one of the previous 3 limits, let : $t = -x <-> x = -t$ then we would have the following : $$\lim_{t\to\infty} \frac{-t}{\ln(t)}$$ $$\lim_{t\to\infty} \frac{-1}{\frac{\ln(t)}{t}} = -\infty$$ As you can see in the denominator we got the expression as the first limit expression, then I can easily solve it. The Problem : My problem, is that I'm really bad at changing variable, I can't find which one would suite the problem as I have very weak algebra skills . Here are some limits that I couldn't find a way to solve using this method : $$\lim_{x\to0^+} \frac{\ln(x+1)}{\sqrt{x}} $$ $$\lim_{x\to\infty} \frac{\ln(x)^2}{x}$$ So How Can I solve such limits using the method I described ? How Can I Find the right way to change a variable for a given complicated limit ? Is there a way to follow? Note: As noted previously, I can't use L'Hospital's Rule for this or any other technique except changing variables .","In my country education system, in Math we don't have the right to use L'Hopital's rule to solve indeterminate forms limits for this year. Instead we use differnt techniques, such as expanding expressions and recently we used change of variable in order to compute limits for different functions that have or in them . For we know the following limits, we don't need to prove them : Then, whenever we have a limit with that yield an indeterminate form we use change of variable in order to get a limit that is similar to one of these 3 limits. Example : For example we need to compute the following limit : For this one we get an indeterminate form, I need to use the change of variable to try to get a limit that is similar to one of the previous 3 limits, let : then we would have the following : As you can see in the denominator we got the expression as the first limit expression, then I can easily solve it. The Problem : My problem, is that I'm really bad at changing variable, I can't find which one would suite the problem as I have very weak algebra skills . Here are some limits that I couldn't find a way to solve using this method : So How Can I solve such limits using the method I described ? How Can I Find the right way to change a variable for a given complicated limit ? Is there a way to follow? Note: As noted previously, I can't use L'Hospital's Rule for this or any other technique except changing variables .",ln e^x \ln \lim_{x\to\infty} \ln x/x = 0^+ \lim_{x\to 0}\ln (x+1)/x = 1  \lim_{x\to0} \ln (x)x = 0^- ln \lim_{x\to-\infty} \frac{x}{\ln|x|} t = -x <-> x = -t \lim_{t\to\infty} \frac{-t}{\ln(t)} \lim_{t\to\infty} \frac{-1}{\frac{\ln(t)}{t}} = -\infty \lim_{x\to0^+} \frac{\ln(x+1)}{\sqrt{x}}  \lim_{x\to\infty} \frac{\ln(x)^2}{x},"['calculus', 'linear-algebra', 'limits', 'logarithms']"
76,Counting $T$-invariant subspaces over finite field,Counting -invariant subspaces over finite field,T,"I have some explicit questions regarding the following problems. Additional critiques/suggestions are more than welcome. Let $F$ be a finite field with $p$ elements, let $V$ be a $3$-dimensional vector space over $F$ and let $T\colon V\to V$ be a linear operator that has minimal polynomial $x^2.$ How many $1$-dimensional $T$-invariant subspaces does $V$ have? Let's view $V$ as an $F[x]$-module with $x\alpha = T\alpha.$ Since $p(x) = x^2$ is the minimal polynomial and $\dim V = 3,$ we have that $f(x) = x^3$ is the characteristic polynomial. Thus $$V\cong \frac{F[x]}{(x)}\oplus\frac{F[x]}{(x^2)},$$ and there exits $\alpha_1,\alpha_2\in V$ such that $\{\alpha_1,\alpha_2,x\alpha_2\}$ is a basis for $V$. If we are looking for a $1$-dimensional subspace generated by $\beta = c_1\alpha_1+c_2\alpha_2+c_3x\alpha_2,$ then $x\beta = 0,$ which implies $c_2=0.$ Since $|F|=p,$ there are $p^2-1$ choices for $c_1,c_3$ since both can't be $0$. I think we also want to divide this by $p-1$ to give $p+1$ $1$-dimensional $T$-invariant subspaces. I know it has to do with the number of generators and that there are $p-1$ numbers relatively prime to $p$, but I'm not sure how to say that precisely. Also, how does this guarantee that the subspace is $T$-invariant? How many $1$-dimensional $T$-invariant subspaces $W$ of $V$ are direct summands of $V,$ i.e., are such that $V = W\oplus W',$ where $W'$ is a $T$-invariant subspace of $V$? These have to come from the first factor, so there are $p$ such subspaces since there is a unique $1$-dimensional subspace of $\frac{F[x]}{(x^2)},$ namely $$\frac{xF[x]}{(x^2)}.$$ How many $2$-dimensional $T$-invariant subspaces does $V$ have? I have the same question about $T$-invariant-ness, but here we want $x\beta\ne0,$ but $x^2\beta=0$ (where $\beta = c_1\alpha_1+c_2\alpha_2+c_3x\alpha_2$). Then necessarily $c_2\ne0$. Then there are $p^2(p-1)$ vectors $\beta$ such that $x^2\beta=0$ (and $x\beta\ne0$). If the subspace is cyclic then for $\gamma\in\left<\beta\right>,$ $\gamma = d_1\beta+d_2x\beta$ and $\left<\gamma\right> = \left<\beta\right>$ if and only if $d_1\ne 0$. So there are $p(p-1)$ generators. Thus there are  $$\frac{p^2(p-1)}{p(p-1)}=p$$ $2$-dimensional cyclic subspaces. But we could also come from  $$\frac{F[x]}{(x)}\oplus\frac{xF[x]}{(x^2)}.$$ But how many would that be? How many $2$-dimensional $T$-invariant subspaces are direct summands of $V$? I believe it should just be $p$, the number of cyclic $2$-dimensional subspaces.","I have some explicit questions regarding the following problems. Additional critiques/suggestions are more than welcome. Let $F$ be a finite field with $p$ elements, let $V$ be a $3$-dimensional vector space over $F$ and let $T\colon V\to V$ be a linear operator that has minimal polynomial $x^2.$ How many $1$-dimensional $T$-invariant subspaces does $V$ have? Let's view $V$ as an $F[x]$-module with $x\alpha = T\alpha.$ Since $p(x) = x^2$ is the minimal polynomial and $\dim V = 3,$ we have that $f(x) = x^3$ is the characteristic polynomial. Thus $$V\cong \frac{F[x]}{(x)}\oplus\frac{F[x]}{(x^2)},$$ and there exits $\alpha_1,\alpha_2\in V$ such that $\{\alpha_1,\alpha_2,x\alpha_2\}$ is a basis for $V$. If we are looking for a $1$-dimensional subspace generated by $\beta = c_1\alpha_1+c_2\alpha_2+c_3x\alpha_2,$ then $x\beta = 0,$ which implies $c_2=0.$ Since $|F|=p,$ there are $p^2-1$ choices for $c_1,c_3$ since both can't be $0$. I think we also want to divide this by $p-1$ to give $p+1$ $1$-dimensional $T$-invariant subspaces. I know it has to do with the number of generators and that there are $p-1$ numbers relatively prime to $p$, but I'm not sure how to say that precisely. Also, how does this guarantee that the subspace is $T$-invariant? How many $1$-dimensional $T$-invariant subspaces $W$ of $V$ are direct summands of $V,$ i.e., are such that $V = W\oplus W',$ where $W'$ is a $T$-invariant subspace of $V$? These have to come from the first factor, so there are $p$ such subspaces since there is a unique $1$-dimensional subspace of $\frac{F[x]}{(x^2)},$ namely $$\frac{xF[x]}{(x^2)}.$$ How many $2$-dimensional $T$-invariant subspaces does $V$ have? I have the same question about $T$-invariant-ness, but here we want $x\beta\ne0,$ but $x^2\beta=0$ (where $\beta = c_1\alpha_1+c_2\alpha_2+c_3x\alpha_2$). Then necessarily $c_2\ne0$. Then there are $p^2(p-1)$ vectors $\beta$ such that $x^2\beta=0$ (and $x\beta\ne0$). If the subspace is cyclic then for $\gamma\in\left<\beta\right>,$ $\gamma = d_1\beta+d_2x\beta$ and $\left<\gamma\right> = \left<\beta\right>$ if and only if $d_1\ne 0$. So there are $p(p-1)$ generators. Thus there are  $$\frac{p^2(p-1)}{p(p-1)}=p$$ $2$-dimensional cyclic subspaces. But we could also come from  $$\frac{F[x]}{(x)}\oplus\frac{xF[x]}{(x^2)}.$$ But how many would that be? How many $2$-dimensional $T$-invariant subspaces are direct summands of $V$? I believe it should just be $p$, the number of cyclic $2$-dimensional subspaces.",,['linear-algebra']
77,Basic Understanding of Linear Combinations Geometrically,Basic Understanding of Linear Combinations Geometrically,,"I am currently working through MIT's Introduction to Linear Algebra by Gilbert Strang, with no previous matrix experience.  In the first lecture, we are given the following linear equations: $$2x -  y = 0\\ -x + 2y = 3$$ The solution to the system of equations is $(1,2)$. Following this Professor Strang rewrites the system of linear equations in a column picture: x [ 2 ] + y [-1] = [ 0 ] [-1] +  [2] = [ 3 ] In the following steps, the vectors $[2, -1]$ and $[-1, 2]$ are plotted to show that the solution $(0, 3)$ can be found by geometrically by multiplying the ""$x$-vector"" by $1$ and the ""$y$-vector"" by $2$, and adding the two results. My confusion is as follows, looking at the column vectors: In the first vector from the $x$ coefficients we get $[2,-1]$.   What property allows the two $x$ coefficients to be drawn as a vector in $x$ and $y$ on an $xy$ plot?  The result of this is that both coefficients from equation $1$ give magnitude only in the $x$-axis and the coefficients from equation $2$ are on the $y$-axis? Which confuses me if this is an $xy$ plot I am new to this so I am not articulating this very well, so apologies and thanks in advance for your time. Diagrams and formal class notes on P1 & 2: https://ocw.mit.edu/courses/mathematics/18-06sc-linear-algebra-fall-2011/ax-b-and-the-four-subspaces/the-geometry-of-linear-equations/MIT18_06SCF11_Ses1.1sum.pdf","I am currently working through MIT's Introduction to Linear Algebra by Gilbert Strang, with no previous matrix experience.  In the first lecture, we are given the following linear equations: $$2x -  y = 0\\ -x + 2y = 3$$ The solution to the system of equations is $(1,2)$. Following this Professor Strang rewrites the system of linear equations in a column picture: x [ 2 ] + y [-1] = [ 0 ] [-1] +  [2] = [ 3 ] In the following steps, the vectors $[2, -1]$ and $[-1, 2]$ are plotted to show that the solution $(0, 3)$ can be found by geometrically by multiplying the ""$x$-vector"" by $1$ and the ""$y$-vector"" by $2$, and adding the two results. My confusion is as follows, looking at the column vectors: In the first vector from the $x$ coefficients we get $[2,-1]$.   What property allows the two $x$ coefficients to be drawn as a vector in $x$ and $y$ on an $xy$ plot?  The result of this is that both coefficients from equation $1$ give magnitude only in the $x$-axis and the coefficients from equation $2$ are on the $y$-axis? Which confuses me if this is an $xy$ plot I am new to this so I am not articulating this very well, so apologies and thanks in advance for your time. Diagrams and formal class notes on P1 & 2: https://ocw.mit.edu/courses/mathematics/18-06sc-linear-algebra-fall-2011/ax-b-and-the-four-subspaces/the-geometry-of-linear-equations/MIT18_06SCF11_Ses1.1sum.pdf",,['linear-algebra']
78,Representation of quiver $1 \to 2 \to 3$,Representation of quiver,1 \to 2 \to 3,"Let $Q$ be a quiver $1 \to 2 \to 3$. How to determine whether the following two representations are isomorphic, in other words, how to find invertible matrices $Q_1$, $Q_2$, $Q_3$ such that the following diagram commutes: \begin{matrix} & K^n & \overset{A}{\longrightarrow} & K^m &\overset{B}{\longrightarrow} & K^r \\ & \downarrow_{Q_3} &  & \downarrow_{Q_2} & & \downarrow_{Q_1} \\ & K^n & \overset{I_A}{\longrightarrow} & K^m & \overset{I_B}{\longrightarrow} &   K^r \end{matrix} where $I_A$ is a diagonal matrix whose upper left $r(A)\times r(A)$ ($r(A)$ denotes the rank of a matrix $A$) block is the identity matrix and all other entries are zero, $n,m,r\in \mathbb{Z}_{\geq 1}$. For an example: \begin{matrix} & K^3 & \overset{A}{\longrightarrow} & K^2 &\overset{B}{\longrightarrow} & K \\ & \downarrow_{Q_3} &  & \downarrow_{Q_2} & & \downarrow_{Q_1} \\ & K^3 & \overset{I_A}{\longrightarrow} & K^2 & \overset{I_B}{\longrightarrow} &   K \end{matrix} Find $Q_1 \in K\backslash\{0\}$, an $2\times 2$ invertible matrix $Q_2$, an $3\times 3$ invertible matrix $Q_3$, such that  \begin{align*} Q_1 B =I_B Q_2, \quad Q_2 A = I_A Q_3.  \end{align*} My idea is as follows: there exist $Q_1 \in K\backslash\{0\}$ and an $2\times 2$ invertible matrix $Q_2$ such that $Q_1BQ^{-1}_2$ is equivalent to $I_B$. So  \begin{matrix} & K^3 & \overset{A}{\longrightarrow} & K^2 &\overset{B}{\longrightarrow} & K \\ & \downarrow_{Q_3} &  & \downarrow_{Q_2} & & \downarrow_{Q_1} \\ & K^3 & \overset{Q_2 A Q^{-1}_3}{\longrightarrow} & K^2 & \overset{I_B}{\longrightarrow} &   K \end{matrix} I am stuck whether there exists an invertible matrix $Q_3$ such that  \begin{align*} Q_2 A Q^{-1}_3 = I_A.  \end{align*} Who can give me a hint？  Thanks in advance.","Let $Q$ be a quiver $1 \to 2 \to 3$. How to determine whether the following two representations are isomorphic, in other words, how to find invertible matrices $Q_1$, $Q_2$, $Q_3$ such that the following diagram commutes: \begin{matrix} & K^n & \overset{A}{\longrightarrow} & K^m &\overset{B}{\longrightarrow} & K^r \\ & \downarrow_{Q_3} &  & \downarrow_{Q_2} & & \downarrow_{Q_1} \\ & K^n & \overset{I_A}{\longrightarrow} & K^m & \overset{I_B}{\longrightarrow} &   K^r \end{matrix} where $I_A$ is a diagonal matrix whose upper left $r(A)\times r(A)$ ($r(A)$ denotes the rank of a matrix $A$) block is the identity matrix and all other entries are zero, $n,m,r\in \mathbb{Z}_{\geq 1}$. For an example: \begin{matrix} & K^3 & \overset{A}{\longrightarrow} & K^2 &\overset{B}{\longrightarrow} & K \\ & \downarrow_{Q_3} &  & \downarrow_{Q_2} & & \downarrow_{Q_1} \\ & K^3 & \overset{I_A}{\longrightarrow} & K^2 & \overset{I_B}{\longrightarrow} &   K \end{matrix} Find $Q_1 \in K\backslash\{0\}$, an $2\times 2$ invertible matrix $Q_2$, an $3\times 3$ invertible matrix $Q_3$, such that  \begin{align*} Q_1 B =I_B Q_2, \quad Q_2 A = I_A Q_3.  \end{align*} My idea is as follows: there exist $Q_1 \in K\backslash\{0\}$ and an $2\times 2$ invertible matrix $Q_2$ such that $Q_1BQ^{-1}_2$ is equivalent to $I_B$. So  \begin{matrix} & K^3 & \overset{A}{\longrightarrow} & K^2 &\overset{B}{\longrightarrow} & K \\ & \downarrow_{Q_3} &  & \downarrow_{Q_2} & & \downarrow_{Q_1} \\ & K^3 & \overset{Q_2 A Q^{-1}_3}{\longrightarrow} & K^2 & \overset{I_B}{\longrightarrow} &   K \end{matrix} I am stuck whether there exists an invertible matrix $Q_3$ such that  \begin{align*} Q_2 A Q^{-1}_3 = I_A.  \end{align*} Who can give me a hint？  Thanks in advance.",,"['linear-algebra', 'matrices', 'reference-request', 'representation-theory', 'quiver']"
79,Why isn't there a field of mathematics that specifically studies nonlinear systems?,Why isn't there a field of mathematics that specifically studies nonlinear systems?,,"There is linear algebra that is partially devoted to studying linear systems, vectors etc., but why isn't there such a developed field which focuses on nonlinear systems? I've managed to find this publication on the topic, but in the abstract it says ""relatively new field"". What are the greatest difficulties for development of that stream of mathematics?","There is linear algebra that is partially devoted to studying linear systems, vectors etc., but why isn't there such a developed field which focuses on nonlinear systems? I've managed to find this publication on the topic, but in the abstract it says ""relatively new field"". What are the greatest difficulties for development of that stream of mathematics?",,"['linear-algebra', 'polynomials', 'systems-of-equations', 'nonlinear-system']"
80,Showing that positive definite symmetric matrices have a smooth square root,Showing that positive definite symmetric matrices have a smooth square root,,"Show that a map in the vector space of positive-definite symmetric $n \times n$ matrices $$f:S_n^+(\mathbb{R}) \rightarrow S_n^+(\mathbb{R})$$   $$f(s) = s^2$$   is a smooth diffeomorphism. My thought is that if I can show that the derivative $f'(a)(b)$ is invertible for all $s \in S_n^+(\mathbb{R})$, then I will have shown that there is a positive definite square root that varies smoothly with $s$. Since $s$ is a symmetric positive definite matrix, we know that it has $n$ eigenvectors with positive eigenvalues. Further, we know that  $$f'(a)(b) = ab + ba$$ $$ ab + ba = 0 $$ $$ abb^T + bab^T = 0 $$ Suppose that $b$ has non-zero entries. We know that $a, bb^T$ are both positive definite matrices, and we know that diagonal entries of $bab^T$ are positive, since they are equal to $b_iab_i^T$, which is positive because $a$ is positive definite. How do I show that the product $abb^T$ has non-negative entries on the diagonal? I want to show that this results in a contradiction.","Show that a map in the vector space of positive-definite symmetric $n \times n$ matrices $$f:S_n^+(\mathbb{R}) \rightarrow S_n^+(\mathbb{R})$$   $$f(s) = s^2$$   is a smooth diffeomorphism. My thought is that if I can show that the derivative $f'(a)(b)$ is invertible for all $s \in S_n^+(\mathbb{R})$, then I will have shown that there is a positive definite square root that varies smoothly with $s$. Since $s$ is a symmetric positive definite matrix, we know that it has $n$ eigenvectors with positive eigenvalues. Further, we know that  $$f'(a)(b) = ab + ba$$ $$ ab + ba = 0 $$ $$ abb^T + bab^T = 0 $$ Suppose that $b$ has non-zero entries. We know that $a, bb^T$ are both positive definite matrices, and we know that diagonal entries of $bab^T$ are positive, since they are equal to $b_iab_i^T$, which is positive because $a$ is positive definite. How do I show that the product $abb^T$ has non-negative entries on the diagonal? I want to show that this results in a contradiction.",,"['linear-algebra', 'differential-geometry', 'lie-groups', 'positive-definite']"
81,tricky System of equations word problem,tricky System of equations word problem,,"At a county fair, adults' tickets sold for $\$5.50$, senior citizens' tickets for $\$4.00$, and children's tickets for $\$1.50$. On the opening day, the number of children's and senior's tickets sold was 30 more than half the number of adults' tickets sold. The number of senior citizens' tickets sold was $5$ more than four times the number of children's tickets. How many of each type of ticket were sold if the total receipts from the ticket sales were $\$14,970$? This is how I solved it, but I did something wrong and don't know what. $$c+s=\frac 12a+30$$ $$s=4c+5$$ $$5.5a+4s+1.5c=14970$$ $$5.5a+16c+20+1.5c=14970$$ $$5.5a+1.75a+87.5+20=14970$$ $$7.25a=1486.25$$ $$a=205$$ $$c=25.5, s=107$$","At a county fair, adults' tickets sold for $\$5.50$, senior citizens' tickets for $\$4.00$, and children's tickets for $\$1.50$. On the opening day, the number of children's and senior's tickets sold was 30 more than half the number of adults' tickets sold. The number of senior citizens' tickets sold was $5$ more than four times the number of children's tickets. How many of each type of ticket were sold if the total receipts from the ticket sales were $\$14,970$? This is how I solved it, but I did something wrong and don't know what. $$c+s=\frac 12a+30$$ $$s=4c+5$$ $$5.5a+4s+1.5c=14970$$ $$5.5a+16c+20+1.5c=14970$$ $$5.5a+1.75a+87.5+20=14970$$ $$7.25a=1486.25$$ $$a=205$$ $$c=25.5, s=107$$",,"['linear-algebra', 'systems-of-equations']"
82,To prove that the algebraic multiplicity equals the dimension of the generalized eigenspace,To prove that the algebraic multiplicity equals the dimension of the generalized eigenspace,,"I am stuck at one step of the proof. Let $G_{T}(\lambda)$ be the dimension of the generalized eigenspace of an upper triangular matrix $T$ with eigenvalue $\lambda$, and let $\#_T(\lambda)$ be the number of times $\lambda$ appears on the diagonal. The author has already proved that $$G_T(\lambda) \ge \#_T(\lambda)$$ and so the equality can be established by showing that $$\#_T(\lambda) \ge G_T(\lambda)$$ The author uses MI on the dimensional $m$ of $T$, of which the base case $m=1$ is trivial. Then assume the inequality is true for all $1$ to $m-1$, and then consider an upper triangular matrix $T$ with dimension $m$. Suppose $\{v_1,v_2,\cdots,v_m\}$ is a basis for the vector space $V$ with diagonal entries $\lambda_1,\lambda_2,\cdots,\lambda_m$. Then $U=\langle\{v_1,v_2,\cdots,v_{m-1}\}\rangle$ is a subspace of $V$ that is invariant relative to $T$. Then the restriction $T_U:U\to U$ with basis $\{v_1,v_2,\cdots,v_{m-1}\}$ has an upper triangular representation with diagonal elements $\lambda_1,\lambda_2,\cdots,\lambda_{m-1}$, for which one can apply the induction assumption. Now, the author's induction step starts by this: Suppose that $\lambda$ is any eigenvalue of $T$. Then suppose that $v\in Ker((T-\lambda I_V)^m)$. As an element of $V$, we can write $v$ as a linear combination of the basis elements of $\{v_1,v_2,\cdots,v_m\}$, or more compactly, there is a vector $u \in U$ and a scalar $\beta$ such that $v=u+\beta v_m$. Then, $$\beta(\lambda_m - \lambda)^m v_m =\beta(T-\lambda I_V)^m(v_m)$$ $$=-(T-\lambda I_V)^m (u) + (T-\lambda I_V)^m (u) + \beta(T-\lambda I_V)^m(v_m)$$ $$=-(T-\lambda I_V)^m (u) + (T-\lambda I_V)^m (u+\beta v_m)$$ $$=-(T-\lambda I_V)^m (u) + (T-\lambda I_V)^m (v)$$ $$=-(T-\lambda I_V)^m (u)$$ The final expression is an element of $U$ because $U$ is invariant relative to both $T$ and $I_V$. Hence $$\beta(\lambda_m - \lambda)^m v_m=0$$ $$\cdots$$ I am stuck at the first step. The author says this step uses the theorem that if $\lambda$ is an eigenvalue of a matrix $A$, then for integer $s \ge 0$, $\lambda^s$ is an eigenvalue of $A^s$. But $$(T-\lambda I_V)v_m \ne (\lambda_m-\lambda)v_m$$ Thanks in advance for any help! Regards!","I am stuck at one step of the proof. Let $G_{T}(\lambda)$ be the dimension of the generalized eigenspace of an upper triangular matrix $T$ with eigenvalue $\lambda$, and let $\#_T(\lambda)$ be the number of times $\lambda$ appears on the diagonal. The author has already proved that $$G_T(\lambda) \ge \#_T(\lambda)$$ and so the equality can be established by showing that $$\#_T(\lambda) \ge G_T(\lambda)$$ The author uses MI on the dimensional $m$ of $T$, of which the base case $m=1$ is trivial. Then assume the inequality is true for all $1$ to $m-1$, and then consider an upper triangular matrix $T$ with dimension $m$. Suppose $\{v_1,v_2,\cdots,v_m\}$ is a basis for the vector space $V$ with diagonal entries $\lambda_1,\lambda_2,\cdots,\lambda_m$. Then $U=\langle\{v_1,v_2,\cdots,v_{m-1}\}\rangle$ is a subspace of $V$ that is invariant relative to $T$. Then the restriction $T_U:U\to U$ with basis $\{v_1,v_2,\cdots,v_{m-1}\}$ has an upper triangular representation with diagonal elements $\lambda_1,\lambda_2,\cdots,\lambda_{m-1}$, for which one can apply the induction assumption. Now, the author's induction step starts by this: Suppose that $\lambda$ is any eigenvalue of $T$. Then suppose that $v\in Ker((T-\lambda I_V)^m)$. As an element of $V$, we can write $v$ as a linear combination of the basis elements of $\{v_1,v_2,\cdots,v_m\}$, or more compactly, there is a vector $u \in U$ and a scalar $\beta$ such that $v=u+\beta v_m$. Then, $$\beta(\lambda_m - \lambda)^m v_m =\beta(T-\lambda I_V)^m(v_m)$$ $$=-(T-\lambda I_V)^m (u) + (T-\lambda I_V)^m (u) + \beta(T-\lambda I_V)^m(v_m)$$ $$=-(T-\lambda I_V)^m (u) + (T-\lambda I_V)^m (u+\beta v_m)$$ $$=-(T-\lambda I_V)^m (u) + (T-\lambda I_V)^m (v)$$ $$=-(T-\lambda I_V)^m (u)$$ The final expression is an element of $U$ because $U$ is invariant relative to both $T$ and $I_V$. Hence $$\beta(\lambda_m - \lambda)^m v_m=0$$ $$\cdots$$ I am stuck at the first step. The author says this step uses the theorem that if $\lambda$ is an eigenvalue of a matrix $A$, then for integer $s \ge 0$, $\lambda^s$ is an eigenvalue of $A^s$. But $$(T-\lambda I_V)v_m \ne (\lambda_m-\lambda)v_m$$ Thanks in advance for any help! Regards!",,"['linear-algebra', 'eigenvalues-eigenvectors']"
83,Find closest point between 2D Point and 2D Area,Find closest point between 2D Point and 2D Area,,"This is a mathematics problem with the application in computer science. Basically I have a rotated 2D box (black) and I need to find the closest point (red) on its' bounds towards a point (green) in 2D Space. The box is defined by center (x/y) size (x,y) and rotation (y euler angles). Approximations which are performance efficient are welcome as well.","This is a mathematics problem with the application in computer science. Basically I have a rotated 2D box (black) and I need to find the closest point (red) on its' bounds towards a point (green) in 2D Space. The box is defined by center (x/y) size (x,y) and rotation (y euler angles). Approximations which are performance efficient are welcome as well.",,"['linear-algebra', 'computer-science', 'area']"
84,Rotation by Householder matrices,Rotation by Householder matrices,,"I have two vectors, let's say $u, v \in \mathbb{R}^n$ with the same norm $\|u\| = \|v\| = 1$. I want to map first vector to second using 2 Householder reflections $(I - 2pp^T), \|p\| = 1$. Is it always possible and what is the formula for such reflections? For me it seems that it is true, because we need the rotation, so every rotation can be represented as a composition of 2 reflections. But I can't find exactly formula. Thanks for the help!","I have two vectors, let's say $u, v \in \mathbb{R}^n$ with the same norm $\|u\| = \|v\| = 1$. I want to map first vector to second using 2 Householder reflections $(I - 2pp^T), \|p\| = 1$. Is it always possible and what is the formula for such reflections? For me it seems that it is true, because we need the rotation, so every rotation can be represented as a composition of 2 reflections. But I can't find exactly formula. Thanks for the help!",,"['linear-algebra', 'matrices']"
85,Jordan Normal Form - Number of Ones on Superdiagonal,Jordan Normal Form - Number of Ones on Superdiagonal,,"I was reading notes on Jordan Normal Form and it says that for a given matrix $A$, the number of ones on the super-diagonal of its associated Jordan matrix is equal to $n-d$, however they seem to assume an implied meaning of $n$ and $d$ which I can't seem to figure out. If you scroll to the bottom, where they mention it, they use $d$ once saying that the Jordan Matrix is of the form $$\left(\begin{array}{cccc} J_1 & 0 & \cdots & 0 \\ 0 & J_2 & \cdots & 0 \\ \vdots & \vdots &  \ddots&\vdots \\ 0 & 0 & \cdots & J_d \\ \end{array}\right)$$ appearing to refer to the number of Jordan blocks. They also use $n$ on the bullet above referring to the number of distinct eigenvalues of $A$. Is this interpretation of $n$ and $d$ most likely what they meant (i.e. does $n-d$ give the number of ones on the superdiagonal)? If so can anyone explain why?","I was reading notes on Jordan Normal Form and it says that for a given matrix $A$, the number of ones on the super-diagonal of its associated Jordan matrix is equal to $n-d$, however they seem to assume an implied meaning of $n$ and $d$ which I can't seem to figure out. If you scroll to the bottom, where they mention it, they use $d$ once saying that the Jordan Matrix is of the form $$\left(\begin{array}{cccc} J_1 & 0 & \cdots & 0 \\ 0 & J_2 & \cdots & 0 \\ \vdots & \vdots &  \ddots&\vdots \\ 0 & 0 & \cdots & J_d \\ \end{array}\right)$$ appearing to refer to the number of Jordan blocks. They also use $n$ on the bullet above referring to the number of distinct eigenvalues of $A$. Is this interpretation of $n$ and $d$ most likely what they meant (i.e. does $n-d$ give the number of ones on the superdiagonal)? If so can anyone explain why?",,"['linear-algebra', 'matrices', 'jordan-normal-form']"
86,Is it possible to define the tensor product of two vectors with respect to a bilinear form?,Is it possible to define the tensor product of two vectors with respect to a bilinear form?,,"Given two vectors $\vec{v},\vec{w} \in \mathbb{R}^n$, and a bilinear form $\mathcal{B}$ represented by an $n \times n$ matrix $B$, we can define the inner product of $\vec{v}$ and $\vec{w}$ with respect to $\mathcal{B}$ as follows: $$\langle \vec{v} , \vec{w} \rangle_{\mathcal{B}} = \vec{v}^T B \vec{w} = \vec{v} (B\vec{w}) = (B^T\vec{v})^T \vec{w}$$ The tensor product of $\vec{v}$ and $\vec{w}$ can be defined as follows: $$\vec{v}\otimes \vec{w} = \vec{v}\vec{w}^T$$ (see, for example, here: https://en.wikipedia.org/wiki/Dyadics#Definitions_and_terminology ). Is there a sensible and consistent way to define a tensor product with respect to $\mathcal{B}$ when $\mathcal{B}$is not the identity bilinear form? If we let $I_n$ be the identity $n \times n$ matrix, and thus the matrix representation of the identity bilinear form, the above equation suggests multiple possible ways to attempt to generalize, although none stand out as being especially sensible to me. $$v \otimes w = I_n \vec{v} \vec{w}^T = \vec{v} I_n \vec{w}^T = \vec{v}\vec{w}^T I_n = I_n \vec{v} \vec{w}^T I_n=\dots$$ I was thinking maybe $$\vec{v}(B\vec{w})^T=\vec{v}  \vec{w}^T B^T \quad \text{or} \quad (B^T \vec{v})\vec{w}^T = B^T \vec{v}\vec{w}^T$$ since they seem like the closest analogies to both definitions above. I would greatly appreciate your thoughts.","Given two vectors $\vec{v},\vec{w} \in \mathbb{R}^n$, and a bilinear form $\mathcal{B}$ represented by an $n \times n$ matrix $B$, we can define the inner product of $\vec{v}$ and $\vec{w}$ with respect to $\mathcal{B}$ as follows: $$\langle \vec{v} , \vec{w} \rangle_{\mathcal{B}} = \vec{v}^T B \vec{w} = \vec{v} (B\vec{w}) = (B^T\vec{v})^T \vec{w}$$ The tensor product of $\vec{v}$ and $\vec{w}$ can be defined as follows: $$\vec{v}\otimes \vec{w} = \vec{v}\vec{w}^T$$ (see, for example, here: https://en.wikipedia.org/wiki/Dyadics#Definitions_and_terminology ). Is there a sensible and consistent way to define a tensor product with respect to $\mathcal{B}$ when $\mathcal{B}$is not the identity bilinear form? If we let $I_n$ be the identity $n \times n$ matrix, and thus the matrix representation of the identity bilinear form, the above equation suggests multiple possible ways to attempt to generalize, although none stand out as being especially sensible to me. $$v \otimes w = I_n \vec{v} \vec{w}^T = \vec{v} I_n \vec{w}^T = \vec{v}\vec{w}^T I_n = I_n \vec{v} \vec{w}^T I_n=\dots$$ I was thinking maybe $$\vec{v}(B\vec{w})^T=\vec{v}  \vec{w}^T B^T \quad \text{or} \quad (B^T \vec{v})\vec{w}^T = B^T \vec{v}\vec{w}^T$$ since they seem like the closest analogies to both definitions above. I would greatly appreciate your thoughts.",,"['linear-algebra', 'tensor-products', 'multilinear-algebra', 'bilinear-form']"
87,What is the expected distortion of a linear transformation?,What is the expected distortion of a linear transformation?,,"Let $A: \mathbb{R}^n \to \mathbb{R}^n$. I am interested in the ""average distortion"" caused by the action of $A$ on vectors. (i.e stretching or contraction of the norm). Consider for instance the uniform distribution on $\mathbb{S}^{n-1}$, and the random variable $X:\mathbb{S}^{n-1} \to \mathbb{R}$ defined by $X(x)=(\|A(x)\|_2)^2$. What is the expectation of $X$? Using SVD, it is easy to check that the problem reduces to $A$ being a diagonal matrix with non-negative entries. So, the question amounts to calculating $$\int_{\mathbb{S}^{n-1}} \sum_{i=1}^n (\sigma_ix_i)^2 $$ (and dividing by the volume of $\mathbb{S}^{n-1}$). Is there a closed formula for this integral? Also, one could take the expected value of the norm, and not its square (I thought this should be easier if there are no sqaure roots involved...)","Let $A: \mathbb{R}^n \to \mathbb{R}^n$. I am interested in the ""average distortion"" caused by the action of $A$ on vectors. (i.e stretching or contraction of the norm). Consider for instance the uniform distribution on $\mathbb{S}^{n-1}$, and the random variable $X:\mathbb{S}^{n-1} \to \mathbb{R}$ defined by $X(x)=(\|A(x)\|_2)^2$. What is the expectation of $X$? Using SVD, it is easy to check that the problem reduces to $A$ being a diagonal matrix with non-negative entries. So, the question amounts to calculating $$\int_{\mathbb{S}^{n-1}} \sum_{i=1}^n (\sigma_ix_i)^2 $$ (and dividing by the volume of $\mathbb{S}^{n-1}$). Is there a closed formula for this integral? Also, one could take the expected value of the norm, and not its square (I thought this should be easier if there are no sqaure roots involved...)",,"['linear-algebra', 'probability', 'matrices', 'random-variables']"
88,"On the difference between $\textbf{R}^{\{1,2,...,n\}}$, $\textbf{R}^{\{1,2,...,n+1\}}$, $\textbf{R}^{[0, 1]}$, and $\textbf{R}^\infty$","On the difference between , , , and","\textbf{R}^{\{1,2,...,n\}} \textbf{R}^{\{1,2,...,n+1\}} \textbf{R}^{[0, 1]} \textbf{R}^\infty","I'm working my way through Axler's ""Linear Algebra Done Right"" (3rd ed.), and I'm getting stuck on section 1.23, which says: If $S$ is a set, then $\textbf{F}^S$ denotes the set of functions from $S$ to $\textbf{F}$. For $f, g \in \textbf{F}^S$, the sum $f + g \in \textbf{F}^S$ is the function defined by   $$(f + g)(x) = f(x) + g(x)$$   for all $x \in S$. For $\lambda \in \textbf{F}$ and $f \in \textbf{F}^S$, the product $\lambda f \in \textbf{F}^S$ is the function defined by   $$(\lambda f)(x) = \lambda f(x)$$   for all $x \in S$. As an example of the notation above, if $S$ is the interval [0,1] and $\textbf{F} = \textbf{R}$, then $\textbf{R}^{[0,1]}$ is the set of real-valued functions on the interval [0,1]. In the next paragraph, the author goes on to assert the following: Our previous examples of vector spaces, $\textbf{F}^n$ and $\textbf{F}^\infty$, are special cases of the vector space $\textbf{F}^S$ because a list of length $n$ of numbers in $\textbf{F}$ can be thought of as a function from {1, 2, ..., $n$} to $\textbf{F}$ and a sequence of numbers in $\textbf{F}$ can be thought of as a function from the set of positive integers to $\textbf{F}$.  In other words, we can think of $\textbf{F}^n$ as $\textbf{F}^{\{1,2,...,n\}}$ and we can think of $\textbf{F}^\infty$ as $\textbf{F}^{\{1,2,...\}}$. It's at this point I get confused, due mostly to the example which relies on $\textbf{R}^{[0,1]}$.  It seems to me that then number of elements in $\textbf{R}^{[0,1]}$ should be uncountably infinite and that I should be able to generate any value between $-\infty$ and $\infty$ from [0,1] using some member of $\textbf{R}^{[0,1]}$, which feels a whole lot like generating a point in $\textbf{R}^\infty$. If that's the case, then what's the difference between a tuple generated from $\textbf{R}^{\{1,2,...,n\}}$ and one generated from $\textbf{R}^{\{1,2,...,n+1\}}$? I think my difficulty lies in not understanding implicit restrictions on the notation. The answer to this specific sub-question may be a shortcut to understanding: Suppose I'm trying to think of a particular point in $(x,y,z)\epsilon\textbf{R}^3$ as $\textbf{R}^{\{1,2,3\}}$ where $f,g,h\epsilon\textbf{R}^{\{1,2,3\}}$.  Must I think of that point in $\textbf{R}^3$ as $(f(1)=x, f(2)=y, f(3)=z)$, or can I think of it as $(f(1)=x, g(2)=y, h(3)=z)$?","I'm working my way through Axler's ""Linear Algebra Done Right"" (3rd ed.), and I'm getting stuck on section 1.23, which says: If $S$ is a set, then $\textbf{F}^S$ denotes the set of functions from $S$ to $\textbf{F}$. For $f, g \in \textbf{F}^S$, the sum $f + g \in \textbf{F}^S$ is the function defined by   $$(f + g)(x) = f(x) + g(x)$$   for all $x \in S$. For $\lambda \in \textbf{F}$ and $f \in \textbf{F}^S$, the product $\lambda f \in \textbf{F}^S$ is the function defined by   $$(\lambda f)(x) = \lambda f(x)$$   for all $x \in S$. As an example of the notation above, if $S$ is the interval [0,1] and $\textbf{F} = \textbf{R}$, then $\textbf{R}^{[0,1]}$ is the set of real-valued functions on the interval [0,1]. In the next paragraph, the author goes on to assert the following: Our previous examples of vector spaces, $\textbf{F}^n$ and $\textbf{F}^\infty$, are special cases of the vector space $\textbf{F}^S$ because a list of length $n$ of numbers in $\textbf{F}$ can be thought of as a function from {1, 2, ..., $n$} to $\textbf{F}$ and a sequence of numbers in $\textbf{F}$ can be thought of as a function from the set of positive integers to $\textbf{F}$.  In other words, we can think of $\textbf{F}^n$ as $\textbf{F}^{\{1,2,...,n\}}$ and we can think of $\textbf{F}^\infty$ as $\textbf{F}^{\{1,2,...\}}$. It's at this point I get confused, due mostly to the example which relies on $\textbf{R}^{[0,1]}$.  It seems to me that then number of elements in $\textbf{R}^{[0,1]}$ should be uncountably infinite and that I should be able to generate any value between $-\infty$ and $\infty$ from [0,1] using some member of $\textbf{R}^{[0,1]}$, which feels a whole lot like generating a point in $\textbf{R}^\infty$. If that's the case, then what's the difference between a tuple generated from $\textbf{R}^{\{1,2,...,n\}}$ and one generated from $\textbf{R}^{\{1,2,...,n+1\}}$? I think my difficulty lies in not understanding implicit restrictions on the notation. The answer to this specific sub-question may be a shortcut to understanding: Suppose I'm trying to think of a particular point in $(x,y,z)\epsilon\textbf{R}^3$ as $\textbf{R}^{\{1,2,3\}}$ where $f,g,h\epsilon\textbf{R}^{\{1,2,3\}}$.  Must I think of that point in $\textbf{R}^3$ as $(f(1)=x, f(2)=y, f(3)=z)$, or can I think of it as $(f(1)=x, g(2)=y, h(3)=z)$?",,"['linear-algebra', 'elementary-set-theory', 'vector-spaces']"
89,Converse of Schur's Lemma in finite dimensional vector spaces,Converse of Schur's Lemma in finite dimensional vector spaces,,"I am trying to prove (or disprove) the converse of Schur's Lemma in finite dimensional vector spaces. I am not sure if it holds in this case, but I have tried to apply the idea that proves it in representation theory (see for example Theorem 4.3 here that uses Maschke's theorem or in questions here and here ). The converse of Schur's Lemma in finite dimensional vector spaces is: Let $V$ be a finite dimensional vector space over the complex numbers. Let $S$ be a set of endomorphisms of $V$ and assume that every endomorphism $A$ of $V$ such that $$AB=BA\text{ for all }B\in S$$ is of the form $\lambda I, \ \lambda\in\mathbb{C}$. Then $V$ is a simple $S$-space. (*an endomorphism of $V$ is a linear operator from $V$ to $V$ *$V$ is a simple $S$-space if the only $S$-invariant subspaces of $V$ are $V$ itself and the zero subspace) What I've tried so far: Assume to the contrary that $V$ is not a simple $S-$space. Then, there exists a subspace $W$ of $V$ such that $W\not =\{0\}$, $W\not= V$ and $W$ is $S$-invariant. Let $W'=V\setminus W$. Then, since $V$ is finite, we can easily prove that $V=W\oplus W'$ (this is my attempt to translate Maschke's theorem in vector spaces). Consequently, for every $v\in V$, there exists unique $w\in W$ and $w'\in W'$, such that $v=w+w'$. Define the projection $P:V\rightarrow V$ by $Pv=w$ for every $v\in V$. What is left to prove is that $PB=BP$ for all $B\in S$. Then $P$ is clearly not a scalar and thus we have the contradiction we are looking for. There is a difficulty in showing that $PB=BP$ for all $B\in S$, because $W'$ might be $S$-invariant or not. More precisely: Let $v\in V=W\oplus W'$ and $v\not =0_V$. If $v\in W$ then $Bv\in W$ because $W$ is $S$-invariant, therefore: $PBv=Bv$ and $BPv=Bv$. If $v\in W'$ then $Pv=0$, hence $BPv=B\cdot 0=0$, and (i) if $Bv\in W'$ then $PBv=0$. (ii) if $Bv\in W$ then $PBv=Bv$ and this is where the problem occurs. Any hints, ideas or counterexamples would be very helpful.","I am trying to prove (or disprove) the converse of Schur's Lemma in finite dimensional vector spaces. I am not sure if it holds in this case, but I have tried to apply the idea that proves it in representation theory (see for example Theorem 4.3 here that uses Maschke's theorem or in questions here and here ). The converse of Schur's Lemma in finite dimensional vector spaces is: Let $V$ be a finite dimensional vector space over the complex numbers. Let $S$ be a set of endomorphisms of $V$ and assume that every endomorphism $A$ of $V$ such that $$AB=BA\text{ for all }B\in S$$ is of the form $\lambda I, \ \lambda\in\mathbb{C}$. Then $V$ is a simple $S$-space. (*an endomorphism of $V$ is a linear operator from $V$ to $V$ *$V$ is a simple $S$-space if the only $S$-invariant subspaces of $V$ are $V$ itself and the zero subspace) What I've tried so far: Assume to the contrary that $V$ is not a simple $S-$space. Then, there exists a subspace $W$ of $V$ such that $W\not =\{0\}$, $W\not= V$ and $W$ is $S$-invariant. Let $W'=V\setminus W$. Then, since $V$ is finite, we can easily prove that $V=W\oplus W'$ (this is my attempt to translate Maschke's theorem in vector spaces). Consequently, for every $v\in V$, there exists unique $w\in W$ and $w'\in W'$, such that $v=w+w'$. Define the projection $P:V\rightarrow V$ by $Pv=w$ for every $v\in V$. What is left to prove is that $PB=BP$ for all $B\in S$. Then $P$ is clearly not a scalar and thus we have the contradiction we are looking for. There is a difficulty in showing that $PB=BP$ for all $B\in S$, because $W'$ might be $S$-invariant or not. More precisely: Let $v\in V=W\oplus W'$ and $v\not =0_V$. If $v\in W$ then $Bv\in W$ because $W$ is $S$-invariant, therefore: $PBv=Bv$ and $BPv=Bv$. If $v\in W'$ then $Pv=0$, hence $BPv=B\cdot 0=0$, and (i) if $Bv\in W'$ then $PBv=0$. (ii) if $Bv\in W$ then $PBv=Bv$ and this is where the problem occurs. Any hints, ideas or counterexamples would be very helpful.",,"['linear-algebra', 'vector-spaces']"
90,About transpose matrix transformation problem.,About transpose matrix transformation problem.,,I have this problem that I don't understand so I can't solve. I wish someone could explain me it or solve it. Let $M_2(\mathbb{R})$ the vector space generated by all the square   matrices of $2\times 2$. Consider the linear transformation $T\colon  M_2(\mathbb{R})\longrightarrow M_2(\mathbb{R})$ given by $T(A)=A^T$   (where $A^T$ is the transpose of $A$). Calculate a basis for   $M_2(\mathbb{R})$ such that the transformation $T$ is represented by a   diagonal matrix. Which are the possible values for the diagonal?,I have this problem that I don't understand so I can't solve. I wish someone could explain me it or solve it. Let $M_2(\mathbb{R})$ the vector space generated by all the square   matrices of $2\times 2$. Consider the linear transformation $T\colon  M_2(\mathbb{R})\longrightarrow M_2(\mathbb{R})$ given by $T(A)=A^T$   (where $A^T$ is the transpose of $A$). Calculate a basis for   $M_2(\mathbb{R})$ such that the transformation $T$ is represented by a   diagonal matrix. Which are the possible values for the diagonal?,,"['linear-algebra', 'matrices', 'linear-transformations']"
91,"Prove if $U$ unitary, $U^k$ has convergent subsequence to $I$","Prove if  unitary,  has convergent subsequence to",U U^k I,Let $U$ be a unitary matrix in $M_{n}(\mathbb{C})$. Prove $\{U^{k}\}_{k \in\Bbb N}$ has a subsequence that converges to $I$ (identity matrix). How to prove that? I tried using the Spectral theorem .,Let $U$ be a unitary matrix in $M_{n}(\mathbb{C})$. Prove $\{U^{k}\}_{k \in\Bbb N}$ has a subsequence that converges to $I$ (identity matrix). How to prove that? I tried using the Spectral theorem .,,"['real-analysis', 'linear-algebra', 'matrices', 'analysis']"
92,Question regarding on powers of a certain matrix,Question regarding on powers of a certain matrix,,"I am a high school student who is interested in mathematics, and I'm stuck on a problem while doing a small investigation (too small to be called ""research""). Problem: what is the sum of all elements of $n \times n$ matrix $M^m$, where $M_{ij} = \begin{cases} 0, & j > i + 1 \\ 1, & j \le i + 1 \end{cases} ?$ Although answers in terms of $m, n$ are better, other forms would help. Thanks in advance!","I am a high school student who is interested in mathematics, and I'm stuck on a problem while doing a small investigation (too small to be called ""research""). Problem: what is the sum of all elements of $n \times n$ matrix $M^m$, where $M_{ij} = \begin{cases} 0, & j > i + 1 \\ 1, & j \le i + 1 \end{cases} ?$ Although answers in terms of $m, n$ are better, other forms would help. Thanks in advance!",,"['linear-algebra', 'matrices', 'exponentiation']"
93,Find 10 commuting $2\times 2$ matrices of the same order,Find 10 commuting  matrices of the same order,2\times 2,"Prove that there exists 10 distinct real $2\times 2$ matrices which are pairwise commuting and all of the same finite order. Here, the order of matrix A is the smallest integer $k > 0$ such that $A^k = I.$  Also by 'pairwise commuting', I mean that if $\{A_1, \cdots, A_{10}\}$ are the ten matrices, then $A_i A_j = A_j A_i$ for any $i,j = 1,2,\cdots, 10$ and $i\neq j.$ Linear algebra is not my forte yet, I haven't got a clue where to begin.  The matrices commute so they all have the same eigenvectors... so letting $A_1$ be a matrix of just these eigenvectors, $A_2 = A_1 + I$ would have the same eigenvectors, thus $A_2A_1 = A_1A_2.$  However I am not sure how to deal with making them the same order.","Prove that there exists 10 distinct real $2\times 2$ matrices which are pairwise commuting and all of the same finite order. Here, the order of matrix A is the smallest integer $k > 0$ such that $A^k = I.$  Also by 'pairwise commuting', I mean that if $\{A_1, \cdots, A_{10}\}$ are the ten matrices, then $A_i A_j = A_j A_i$ for any $i,j = 1,2,\cdots, 10$ and $i\neq j.$ Linear algebra is not my forte yet, I haven't got a clue where to begin.  The matrices commute so they all have the same eigenvectors... so letting $A_1$ be a matrix of just these eigenvectors, $A_2 = A_1 + I$ would have the same eigenvectors, thus $A_2A_1 = A_1A_2.$  However I am not sure how to deal with making them the same order.",,['linear-algebra']
94,What is the most general/abstract way to think about Tensors,What is the most general/abstract way to think about Tensors,,"In their most general and abstract definitions as Mathematical Objects : A Scalar is an element of a field used to define Vector Spaces A Vector is an element of a Vector Space. Since a Scalar is a Tensor of rank-0 and a Vector is a Tensor of rank-1, then what Space are Tensors an element of? Can you even think of Tensors abstractly as elements of a Mathematical Space?","In their most general and abstract definitions as Mathematical Objects : A Scalar is an element of a field used to define Vector Spaces A Vector is an element of a Vector Space. Since a Scalar is a Tensor of rank-0 and a Vector is a Tensor of rank-1, then what Space are Tensors an element of? Can you even think of Tensors abstractly as elements of a Mathematical Space?",,"['linear-algebra', 'abstract-algebra', 'vector-spaces', 'vector-analysis', 'tensors']"
95,"Perfect Pairing, non-degeneracy and dimension.","Perfect Pairing, non-degeneracy and dimension.",,"On this wikipedia entry https://en.wikipedia.org/wiki/Bilinear_form#Different_spaces it tells us that if $B: V \times W \to K$ is a bilinear map, then In finite dimensions, [a perfect pairing] is equivalent to the pairing being   nondegenerate (the spaces necessarily having the same dimensions). My question is why does non-degeneracy imply that the induced linear map from $V$ to $W^*$ is an isomoprhism? And, why do $V$ and $W$ necessarily have the same dimension?","On this wikipedia entry https://en.wikipedia.org/wiki/Bilinear_form#Different_spaces it tells us that if $B: V \times W \to K$ is a bilinear map, then In finite dimensions, [a perfect pairing] is equivalent to the pairing being   nondegenerate (the spaces necessarily having the same dimensions). My question is why does non-degeneracy imply that the induced linear map from $V$ to $W^*$ is an isomoprhism? And, why do $V$ and $W$ necessarily have the same dimension?",,"['linear-algebra', 'duality-theorems', 'bilinear-form']"
96,Orthonormal Basis Question: Linear Algebra,Orthonormal Basis Question: Linear Algebra,,"I've been staring at this proof for a long time so any suggestions would be of great help! Prove that for any $m\times n$ matrix $A$ there is an orthonormal Basis $B =\{ v_1,\ldots,v_n\}$ of $\mathbb R^n$ such that the vectors $A v_1,\ldots,A v_n$ are orthogonal. Note that some of the vectors $A v_i$ may be zero.","I've been staring at this proof for a long time so any suggestions would be of great help! Prove that for any $m\times n$ matrix $A$ there is an orthonormal Basis $B =\{ v_1,\ldots,v_n\}$ of $\mathbb R^n$ such that the vectors $A v_1,\ldots,A v_n$ are orthogonal. Note that some of the vectors $A v_i$ may be zero.",,"['linear-algebra', 'matrices', 'proof-explanation', 'orthonormal']"
97,"Equality in the Cauchy-Schwarz inequality implies $\overrightarrow v$,$\overrightarrow w$ linearly dependent","Equality in the Cauchy-Schwarz inequality implies , linearly dependent",\overrightarrow v \overrightarrow w,"Show that one gets equality in the Schwarz inequality if and only if $\overrightarrow v$,$\overrightarrow w$ are linearly dependent. (I am supposing they want me to prove it in an inner product space we call V) The $\Leftarrow$ part of the proof was pretty straightforward but I don't know how to go about the $\Rightarrow$ part because linear dependence requires us to show $\overrightarrow v$=$\lambda \overrightarrow w$ for some $\lambda \in \mathbb{R}$ but the Cauchy-Schwarz inequality only involves norms\lengths of vectors and not relations between the vectors themselves. I would appreciate tips on starting points.","Show that one gets equality in the Schwarz inequality if and only if $\overrightarrow v$,$\overrightarrow w$ are linearly dependent. (I am supposing they want me to prove it in an inner product space we call V) The $\Leftarrow$ part of the proof was pretty straightforward but I don't know how to go about the $\Rightarrow$ part because linear dependence requires us to show $\overrightarrow v$=$\lambda \overrightarrow w$ for some $\lambda \in \mathbb{R}$ but the Cauchy-Schwarz inequality only involves norms\lengths of vectors and not relations between the vectors themselves. I would appreciate tips on starting points.",,"['linear-algebra', 'inner-products']"
98,Find a matrix of order $p$ in a subgroup of $\operatorname{GL}_n(\mathbb Z_p)$,Find a matrix of order  in a subgroup of,p \operatorname{GL}_n(\mathbb Z_p),"Let $p$ be a fixed prime and $\mathbb Z_p$ the ring of $p$-adic integers. Consider the subgroup $G_n\subseteq \operatorname{GL}_n(\mathbb Z_p)$ given by all matrices $(a_{ij})_{ij}$ such that $$ \begin{align*} v(a_{ij}) &\ge 1 &&\text{if $i>j$}\\ v(a_{ii}-1)&\ge1  &&\text{for all $1\le i\le n$}, \end{align*} $$ where $v$ denotes the usual $p$-adic valuation of $\mathbb Z_p$, i. e. $v(x) = r$ if $x\in p^r\mathbb Z_p\setminus p^{r+1} \mathbb Z_p$ and $v(0) = \infty$. If $n\ge p-1$, find a matrix of order $p$ in $G_n$. This problem appears as an exercise in Peter Schneider’s book on $p$-adic Lie groups, example 23.3, which shows that $G_n$ carries a $p$-valuation if and only if $n<p-1$. The condition $n\ge p-1$ is necessary, since a matrix $A$ of order $p$ has only $p$-th roots of unity as its eigenvalues and hence $f(X) = X^{p-1}+X^{p-2}+\dotsb+ X+1$ has to divide the characteristic polynomial of $A$. Hence, the companion matrix of $f(X)\cdot (X-1)^{n-p+1}= (X^p-1)(X-1)^{n-p}$ gives a matrix of order $p$ in $\operatorname{GL}_n(\mathbb Z_p)$ (which even has entries in $\mathbb Z$). This is, where I got stuck. For $p=3$, I found the matrix $\begin{pmatrix}-2 & -1\\ 3 & 1\end{pmatrix}$ by making the ansatz $\begin{pmatrix}1+3a & b\\ 3c & 1+3d\end{pmatrix}$ and deriving from the characteristic polynomial $X^2+X+1$ the conditions $$ \begin{align*} a+d = -1\\ bc = 3ad-1, \end{align*} $$ which has four solutions in $\mathbb Z$; in $\mathbb Z_3$, $(a,b)$ can be freely chosen from $\mathbb Z_3\times \mathbb Z_3^\times$.","Let $p$ be a fixed prime and $\mathbb Z_p$ the ring of $p$-adic integers. Consider the subgroup $G_n\subseteq \operatorname{GL}_n(\mathbb Z_p)$ given by all matrices $(a_{ij})_{ij}$ such that $$ \begin{align*} v(a_{ij}) &\ge 1 &&\text{if $i>j$}\\ v(a_{ii}-1)&\ge1  &&\text{for all $1\le i\le n$}, \end{align*} $$ where $v$ denotes the usual $p$-adic valuation of $\mathbb Z_p$, i. e. $v(x) = r$ if $x\in p^r\mathbb Z_p\setminus p^{r+1} \mathbb Z_p$ and $v(0) = \infty$. If $n\ge p-1$, find a matrix of order $p$ in $G_n$. This problem appears as an exercise in Peter Schneider’s book on $p$-adic Lie groups, example 23.3, which shows that $G_n$ carries a $p$-valuation if and only if $n<p-1$. The condition $n\ge p-1$ is necessary, since a matrix $A$ of order $p$ has only $p$-th roots of unity as its eigenvalues and hence $f(X) = X^{p-1}+X^{p-2}+\dotsb+ X+1$ has to divide the characteristic polynomial of $A$. Hence, the companion matrix of $f(X)\cdot (X-1)^{n-p+1}= (X^p-1)(X-1)^{n-p}$ gives a matrix of order $p$ in $\operatorname{GL}_n(\mathbb Z_p)$ (which even has entries in $\mathbb Z$). This is, where I got stuck. For $p=3$, I found the matrix $\begin{pmatrix}-2 & -1\\ 3 & 1\end{pmatrix}$ by making the ansatz $\begin{pmatrix}1+3a & b\\ 3c & 1+3d\end{pmatrix}$ and deriving from the characteristic polynomial $X^2+X+1$ the conditions $$ \begin{align*} a+d = -1\\ bc = 3ad-1, \end{align*} $$ which has four solutions in $\mathbb Z$; in $\mathbb Z_3$, $(a,b)$ can be freely chosen from $\mathbb Z_3\times \mathbb Z_3^\times$.",,"['linear-algebra', 'matrices', 'p-adic-number-theory']"
99,Is there a name for this matrix product?,Is there a name for this matrix product?,,"Define $A \square B = A \otimes I + I \otimes B$. Is there a name for the operation $\square$, when considered as a type of matrix product? It's a generalisation of the Cartesian product for graphs (also denoted $\square$), but I'm more interested in what can be said about it from a linear algebra point of view. Searching for ""matrix Cartesian product"" didn't turn up anything. Clearly this is quite nicely behaved in terms of eigendecomposition, since if $\mu$ and $\nu$ are eigenvalues of $A$ and $B$ then $\mu + \nu$ is an eigenvalue of $A\square B$, and the corresponding eigenvectors are also related in a simple way. Is there more that can be said about its properties than this? In addition, I am also interested in matrices that have the same pattern of zero and non-zero entries as a matrix of the form $A\square B$, but which cannot be factored into that form. Is there anything useful that can be said about the spectrum of such matrices? (If necessary, assume that the non-zero elements are all positive.) Edit: I know the answer now. (See my own answer below, which I can't accept yet for some reason.) I've asked the part about matrices with the same sign pattern as a separate question .","Define $A \square B = A \otimes I + I \otimes B$. Is there a name for the operation $\square$, when considered as a type of matrix product? It's a generalisation of the Cartesian product for graphs (also denoted $\square$), but I'm more interested in what can be said about it from a linear algebra point of view. Searching for ""matrix Cartesian product"" didn't turn up anything. Clearly this is quite nicely behaved in terms of eigendecomposition, since if $\mu$ and $\nu$ are eigenvalues of $A$ and $B$ then $\mu + \nu$ is an eigenvalue of $A\square B$, and the corresponding eigenvectors are also related in a simple way. Is there more that can be said about its properties than this? In addition, I am also interested in matrices that have the same pattern of zero and non-zero entries as a matrix of the form $A\square B$, but which cannot be factored into that form. Is there anything useful that can be said about the spectrum of such matrices? (If necessary, assume that the non-zero elements are all positive.) Edit: I know the answer now. (See my own answer below, which I can't accept yet for some reason.) I've asked the part about matrices with the same sign pattern as a separate question .",,"['linear-algebra', 'matrices', 'eigenvalues-eigenvectors']"
