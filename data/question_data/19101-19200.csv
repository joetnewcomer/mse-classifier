,title_raw,title_text,title_latex,body_raw,body_text,body_latex,tags
0,Closest point on a line to another point,Closest point on a line to another point,,"The problem asks to find the nearest to $P(2,2)$, on the line: $$-x -2y +3 =0$$ This is what I've tried, the normal vector, $$n = (-1,-2)$$ found an arbitrary point on the given line by setting $y = 0$, which results in $$P_1 = (3,0)$$ calculated distance from $P$ to $P_1$ (call it the vector u): $$u = (-1,2)$$ then finally I simply applied the projection formula and got that the point on the line should be (3/5,6/5) which unfortunately is the wrong answer. any ideas where I've gone wrong?","The problem asks to find the nearest to $P(2,2)$, on the line: $$-x -2y +3 =0$$ This is what I've tried, the normal vector, $$n = (-1,-2)$$ found an arbitrary point on the given line by setting $y = 0$, which results in $$P_1 = (3,0)$$ calculated distance from $P$ to $P_1$ (call it the vector u): $$u = (-1,2)$$ then finally I simply applied the projection formula and got that the point on the line should be (3/5,6/5) which unfortunately is the wrong answer. any ideas where I've gone wrong?",,['linear-algebra']
1,"Show that if $T_1$, $T_2$ are normal operators that commute then $T_1+T_2$ and $T_1T_2$ are normal.","Show that if ,  are normal operators that commute then  and  are normal.",T_1 T_2 T_1+T_2 T_1T_2,"Let $V$ be a finite dimensional inner-product space, and suppose that $T_1$, $T_2$ are normal operators on $V$ that commute. How to show that $T_1+T_2$ and $T_1T_2$ are then normal? It is clear if $T_1$ commutes with $T_2^*$ and $T_2$ with $T_1^*$, but is it true? If yes how to prove it? Edit: It is an exercise in Schaum's Outlines of Linear Algebra.","Let $V$ be a finite dimensional inner-product space, and suppose that $T_1$, $T_2$ are normal operators on $V$ that commute. How to show that $T_1+T_2$ and $T_1T_2$ are then normal? It is clear if $T_1$ commutes with $T_2^*$ and $T_2$ with $T_1^*$, but is it true? If yes how to prove it? Edit: It is an exercise in Schaum's Outlines of Linear Algebra.",,"['linear-algebra', 'inner-products']"
2,Does anyone know any resources for Quaternions for truly understanding them?,Does anyone know any resources for Quaternions for truly understanding them?,,"I've been studying Quaternions for a week, on my own. I've learned various facts about them but I still don't understand them. My goal is to understand rotation quaternions specifically. I don't want to just memorize the formulas for using rotation quaternions, I want to truly understand them. It's the most difficult thing I've ever tried to learn. Does anyone know any good resources for understanding quaternions?","I've been studying Quaternions for a week, on my own. I've learned various facts about them but I still don't understand them. My goal is to understand rotation quaternions specifically. I don't want to just memorize the formulas for using rotation quaternions, I want to truly understand them. It's the most difficult thing I've ever tried to learn. Does anyone know any good resources for understanding quaternions?",,"['linear-algebra', 'matrices', 'vector-spaces', 'complex-numbers', 'quaternions']"
3,"What is the dimension of $\{X\in M_{n,n}(F); AX=XA=0\}$?",What is the dimension of ?,"\{X\in M_{n,n}(F); AX=XA=0\}","Let $A$ be a fixed $n\times n$ matrix over a field $F$ . We can look at the subspace $$W=\{X\in M_{n,n}(F); AX=XA=0\}$$ of the matrices which fulfill both $AX=0$ and $XA=0$ . Looking a these equations we get that all columns of $X$ have to fulfill the equation $A\vec c=\vec 0$ . (Let us say we're working with column vectors.) Similarly we get for the rows $\vec r^T A=\vec 0^T$ . This tells us that if we are looking at the possible choices for columns/rows of the matrix $X$ , they have to be in a subspace of dimension $n-\operatorname{rank}A$ (in the right/left null space of $A$ ). At least in some cases it is almost immediately possible to find $W$ or at least $\dim W$ . Obviously, if $A$ is invertible, then $W=\{0\}$ and $\dim W=0$ . Another trivial case is when $A=0$ , which gives us $W=M_{n,n}$ and $\dim W=n^2$ . Slightly less trivial but still simple case is when $\operatorname{rank} A=n-1$ . In this case the condition on rows/columns give us one-dimensional spaces, so there are non-zero vectors $\vec r$ , $\vec c$ such that each row has to be multiple of $\vec r^T$ and each column has to be a multiple of $\vec c$ . Up to a scalar multiple, there is only one way how to get such a matrix and we get that $W$ is generated by the matrix $\vec c\vec r^T$ and $\dim W=1$ . The general case seems to be a bit more complicated. If we denote $k=n-\operatorname{rank}A$ , we can use the same argument to see that there are $k$ linearly independent vectors $\vec c_1,\dots,\vec c_k$ such that the columns have to be linear combinations of these vectors. Similarly, row can be chosen only from the span of the linearly independent vectors $\vec r_1,\dots,\vec r_k$ . (This is again just a direct consequence of $A\vec c=\vec 0$ and $\vec r^TA=\vec 0^T$ .) Using these vectors we can get $k^2$ matrices $$A_{ij}=\vec c_i \vec r_j^T$$ for $i,j\in\{1,2,\dots,k\}$ . Unless I missed something, it seems that showing that these matrices are linearly independent is not too difficult. So we should get that $$\dim W \ge k^2 = (n-\operatorname{rank}A)^2.$$ It is not obvious to me whether these vectors actually generate $W$ . (And perhaps something can be said about the dimension of $W$ without exhibiting a basis.) You may notice that in the three trivial examples above (with $k=0,1,n$ ) we got the equality $\dim W=(n-\operatorname{rank}A)^2$ . Another possible way to look at this problem could be to use the linear function $$f\colon X\to(AX,XA)$$ $f\colon M_{n,n} \to M_{n,n}\oplus M_{n,n}$ , then we have $W=\operatorname{Ker} f$ , so we are basically asking for the dimension of the kernel of this map. So to find $\dim W$ it would be sufficient to find $\dim\operatorname{Im} f$ . However, this does not seem to be easier than the original formulation of the problem. It is also possible to see this as a system of $n^2$ linear equations with $n^2$ unknowns $x_{11}, x_{12}, \dots, x_{nn}$ . If we try to use this line of thinking, the difficult part seems to be determining how many of those equations are linearly dependent. Question: What can be said about the dimension of the subspace $W$ ? Is it equal to $(n-\operatorname{rank}A)^2$ ? Is it determined just by the rank of $A$ ? If not, what are best possible bounds we can get, if we know only the rank of $A$ and have no further information about $A$ ? Motivation for this question was working on an exercise which asked for calculating dimensions of spaces $W_1$ , $W_2$ , $W_1\cap W_2$ and $W_1+W_2$ , where the spaces $W_1$ and $W_2$ were determined by the conditions $AX=0$ and $XA=0$ , respectively. Since the matrix $A$ was given, in this exercise it was possible to find a basis of $W_1\cap W_2$ explicitly. (And the exercise was probably intended just to make the students accustomed to some basic computations such as finding basis, using Grassmann's formula, etc.) Still, I was wondering how much we can say just from knowing the rank of $A$ , without going through all the computations.","Let be a fixed matrix over a field . We can look at the subspace of the matrices which fulfill both and . Looking a these equations we get that all columns of have to fulfill the equation . (Let us say we're working with column vectors.) Similarly we get for the rows . This tells us that if we are looking at the possible choices for columns/rows of the matrix , they have to be in a subspace of dimension (in the right/left null space of ). At least in some cases it is almost immediately possible to find or at least . Obviously, if is invertible, then and . Another trivial case is when , which gives us and . Slightly less trivial but still simple case is when . In this case the condition on rows/columns give us one-dimensional spaces, so there are non-zero vectors , such that each row has to be multiple of and each column has to be a multiple of . Up to a scalar multiple, there is only one way how to get such a matrix and we get that is generated by the matrix and . The general case seems to be a bit more complicated. If we denote , we can use the same argument to see that there are linearly independent vectors such that the columns have to be linear combinations of these vectors. Similarly, row can be chosen only from the span of the linearly independent vectors . (This is again just a direct consequence of and .) Using these vectors we can get matrices for . Unless I missed something, it seems that showing that these matrices are linearly independent is not too difficult. So we should get that It is not obvious to me whether these vectors actually generate . (And perhaps something can be said about the dimension of without exhibiting a basis.) You may notice that in the three trivial examples above (with ) we got the equality . Another possible way to look at this problem could be to use the linear function , then we have , so we are basically asking for the dimension of the kernel of this map. So to find it would be sufficient to find . However, this does not seem to be easier than the original formulation of the problem. It is also possible to see this as a system of linear equations with unknowns . If we try to use this line of thinking, the difficult part seems to be determining how many of those equations are linearly dependent. Question: What can be said about the dimension of the subspace ? Is it equal to ? Is it determined just by the rank of ? If not, what are best possible bounds we can get, if we know only the rank of and have no further information about ? Motivation for this question was working on an exercise which asked for calculating dimensions of spaces , , and , where the spaces and were determined by the conditions and , respectively. Since the matrix was given, in this exercise it was possible to find a basis of explicitly. (And the exercise was probably intended just to make the students accustomed to some basic computations such as finding basis, using Grassmann's formula, etc.) Still, I was wondering how much we can say just from knowing the rank of , without going through all the computations.","A n\times n F W=\{X\in M_{n,n}(F); AX=XA=0\} AX=0 XA=0 X A\vec c=\vec 0 \vec r^T A=\vec 0^T X n-\operatorname{rank}A A W \dim W A W=\{0\} \dim W=0 A=0 W=M_{n,n} \dim W=n^2 \operatorname{rank} A=n-1 \vec r \vec c \vec r^T \vec c W \vec c\vec r^T \dim W=1 k=n-\operatorname{rank}A k \vec c_1,\dots,\vec c_k \vec r_1,\dots,\vec r_k A\vec c=\vec 0 \vec r^TA=\vec 0^T k^2 A_{ij}=\vec c_i \vec r_j^T i,j\in\{1,2,\dots,k\} \dim W \ge k^2 = (n-\operatorname{rank}A)^2. W W k=0,1,n \dim W=(n-\operatorname{rank}A)^2 f\colon X\to(AX,XA) f\colon M_{n,n} \to M_{n,n}\oplus M_{n,n} W=\operatorname{Ker} f \dim W \dim\operatorname{Im} f n^2 n^2 x_{11}, x_{12}, \dots, x_{nn} W (n-\operatorname{rank}A)^2 A A A W_1 W_2 W_1\cap W_2 W_1+W_2 W_1 W_2 AX=0 XA=0 A W_1\cap W_2 A","['linear-algebra', 'matrices', 'vector-spaces', 'linear-transformations', 'matrix-equations']"
4,Formal proof that mean minimize squared error function,Formal proof that mean minimize squared error function,,"On an important book of Machine Learning, I've found this proof. We want to minimize the cost function $J_0(X_0)$ defined by the formula $$J_0(x_0) = \sum_{k=1}^n \|x_0 - x_k \|^2.$$ The solution to this problem is given by $x_0=m$, where $m$ is the sample mean $m = \frac{1}{n}\sum_{k=1}^nx_k$. Proof. $$ \begin{array}{rcl} J_0(x_0) & = & \sum_{k=1}^n \|(x_0 - m)-(x_k - m) \|^2 \\  & = & \sum_{k=1}^n \|x_0 - m \|^2 -2(x_0-m)^T\sum_{k=1}^n(x_k-m) + \sum_{k=1}^n \|x_k - m \|^2 \\  & = & \sum_{k=1}^n \|x_0 - m \|^2 + \sum_{k=1}^n \|x_k - m \|^2. \end{array}$$ Since $ \sum_{k=1}^n \|x_k - m \|^2  $ is independent of $x_0$, this expression is obviously minimized by $x_0=m$. I cannot understand this proof. What does assure that $ \sum_{k=1}^n \|x_k - m \|^2  $ is minimized?","On an important book of Machine Learning, I've found this proof. We want to minimize the cost function $J_0(X_0)$ defined by the formula $$J_0(x_0) = \sum_{k=1}^n \|x_0 - x_k \|^2.$$ The solution to this problem is given by $x_0=m$, where $m$ is the sample mean $m = \frac{1}{n}\sum_{k=1}^nx_k$. Proof. $$ \begin{array}{rcl} J_0(x_0) & = & \sum_{k=1}^n \|(x_0 - m)-(x_k - m) \|^2 \\  & = & \sum_{k=1}^n \|x_0 - m \|^2 -2(x_0-m)^T\sum_{k=1}^n(x_k-m) + \sum_{k=1}^n \|x_k - m \|^2 \\  & = & \sum_{k=1}^n \|x_0 - m \|^2 + \sum_{k=1}^n \|x_k - m \|^2. \end{array}$$ Since $ \sum_{k=1}^n \|x_k - m \|^2  $ is independent of $x_0$, this expression is obviously minimized by $x_0=m$. I cannot understand this proof. What does assure that $ \sum_{k=1}^n \|x_k - m \|^2  $ is minimized?",,"['linear-algebra', 'statistics', 'machine-learning']"
5,Why is Householder computationally more stable than modified Gram-Schmidt?,Why is Householder computationally more stable than modified Gram-Schmidt?,,I'm having trouble nailing down why using Householder transformations yields a more stable/accurate result than the modified Gram-Schmidt method when computing the QR decomposition of a matrix. Can anyone explain? Thanks,I'm having trouble nailing down why using Householder transformations yields a more stable/accurate result than the modified Gram-Schmidt method when computing the QR decomposition of a matrix. Can anyone explain? Thanks,,"['linear-algebra', 'computational-mathematics']"
6,a 2 distance set has an upper bound for number of its elements,a 2 distance set has an upper bound for number of its elements,,"Can anyone help me with this problem? We call a set $A\subseteq \mathbb R^n$ a $2$-distance set if for each $v_i,v_j$ in $A$, $i\neq j$, $|v_i-v_j|=r$ or $s$. Find an upper bound for the number of the elements of $A$. Our Teacher proved that $|A|\le \frac{n^2+5n+4}{2}$ using linear algebra, but I read somewhere that $|A|\le \frac{n^2+3n+4}{2}$. Any good upper bound is welcomed!","Can anyone help me with this problem? We call a set $A\subseteq \mathbb R^n$ a $2$-distance set if for each $v_i,v_j$ in $A$, $i\neq j$, $|v_i-v_j|=r$ or $s$. Find an upper bound for the number of the elements of $A$. Our Teacher proved that $|A|\le \frac{n^2+5n+4}{2}$ using linear algebra, but I read somewhere that $|A|\le \frac{n^2+3n+4}{2}$. Any good upper bound is welcomed!",,['linear-algebra']
7,"Does there exist a Hamel basis $\mathcal B$ for $\mathbb R$ over $\mathbb Q$ such that $a,b \in \mathcal B \implies \dfrac ab \in \mathcal B$?",Does there exist a Hamel basis  for  over  such that ?,"\mathcal B \mathbb R \mathbb Q a,b \in \mathcal B \implies \dfrac ab \in \mathcal B","This is part of an attempt to understand what multiplicative structure a Hamel basis of the reals over the rationals can have. Does there exist a Hamel basis $\mathcal B$ for $\mathbb R$ over $\mathbb Q$ such that $a,b \in \mathcal B \implies \dfrac ab \in \mathcal B$ ? Additionally, as proposed by Noah Schwerber, if the answer to the above is negative, what if the restriction that $a \neq b$ is imposed, that is: Does there exist a Hamel basis $\mathcal B$ for $\mathbb R$ over $\mathbb Q$ such that $a,b \in \mathcal B $ distinct $ \implies \dfrac ab \in \mathcal B$ ? The following earlier question showing that such a Hamel basis cannot be closed under multiplication by a (non-trivial) constant could be helpful Hamel basis for $\mathbb{R}$ over $\mathbb{Q}$ cannot be closed under scalar multiplication by $a \ne 0,1$ A recent related but distinct question Hamel basis for the vector space of real numbers over rational numbers and closedness of the basis under inversion focuses on whether a Hamel basis can be closed under taking inverses.","This is part of an attempt to understand what multiplicative structure a Hamel basis of the reals over the rationals can have. Does there exist a Hamel basis $\mathcal B$ for $\mathbb R$ over $\mathbb Q$ such that $a,b \in \mathcal B \implies \dfrac ab \in \mathcal B$ ? Additionally, as proposed by Noah Schwerber, if the answer to the above is negative, what if the restriction that $a \neq b$ is imposed, that is: Does there exist a Hamel basis $\mathcal B$ for $\mathbb R$ over $\mathbb Q$ such that $a,b \in \mathcal B $ distinct $ \implies \dfrac ab \in \mathcal B$ ? The following earlier question showing that such a Hamel basis cannot be closed under multiplication by a (non-trivial) constant could be helpful Hamel basis for $\mathbb{R}$ over $\mathbb{Q}$ cannot be closed under scalar multiplication by $a \ne 0,1$ A recent related but distinct question Hamel basis for the vector space of real numbers over rational numbers and closedness of the basis under inversion focuses on whether a Hamel basis can be closed under taking inverses.",,"['linear-algebra', 'abstract-algebra']"
8,Lower bound on smallest eigenvalue of (symmetric positive-definite) matrix,Lower bound on smallest eigenvalue of (symmetric positive-definite) matrix,,"If $M$ is a symmetric positive-definite matrix, is it possible to get a positive lower bound on the smallest eigenvalue of $M$ in terms of a matrix norm of $M$ or elements of $M$ ? E.g., I want $$\lambda_{\text{min}} \geq f(\lVert M \rVert)$$ or something like that. $M$ is a Gram matrix, if that helps.","If is a symmetric positive-definite matrix, is it possible to get a positive lower bound on the smallest eigenvalue of in terms of a matrix norm of or elements of ? E.g., I want or something like that. is a Gram matrix, if that helps.",M M M M \lambda_{\text{min}} \geq f(\lVert M \rVert) M,"['linear-algebra', 'matrices', 'eigenvalues-eigenvectors', 'symmetric-matrices', 'positive-definite']"
9,Intuitive interpretation of the adjacency matrix as a linear operator.,Intuitive interpretation of the adjacency matrix as a linear operator.,,"Naturally we can describe graphs via tables of ""yes there is an edge"" or ""no there is not"" between each pair of vertices, so the definition of an adjacency matrix is easily understood.  Thinking of these tables as matrices , however, adds structure - specifically, an interpretation as a linear operator.  Why do we look at them in this light?  Is it just for application - for example, efficiently obtaining a lot of data about a graph by computing its spectrum?  Or is there also an intuitive geometric (or algebraic) motivation behind the adjacency matrix? For example, the $2$-path has adjacency matrix  $$\mathcal{A}(P_2)=\left(\begin{array}{cc} 0 & 1\\1 & 0\end{array}\right)$$ which acts on a $2$-dimensional vector space by flipping the coordinates, $(x,y)\mapsto (y,x)$.  Can we somehow intuitively connect this action to the $2$-path?  What about for other simple graphs?","Naturally we can describe graphs via tables of ""yes there is an edge"" or ""no there is not"" between each pair of vertices, so the definition of an adjacency matrix is easily understood.  Thinking of these tables as matrices , however, adds structure - specifically, an interpretation as a linear operator.  Why do we look at them in this light?  Is it just for application - for example, efficiently obtaining a lot of data about a graph by computing its spectrum?  Or is there also an intuitive geometric (or algebraic) motivation behind the adjacency matrix? For example, the $2$-path has adjacency matrix  $$\mathcal{A}(P_2)=\left(\begin{array}{cc} 0 & 1\\1 & 0\end{array}\right)$$ which acts on a $2$-dimensional vector space by flipping the coordinates, $(x,y)\mapsto (y,x)$.  Can we somehow intuitively connect this action to the $2$-path?  What about for other simple graphs?",,"['linear-algebra', 'abstract-algebra', 'graph-theory', 'intuition', 'spectral-graph-theory']"
10,Determinant of a real skew-symmetric matrix is square of an integer,Determinant of a real skew-symmetric matrix is square of an integer,,"Let $A$ be a real skew-symmetric matrix with integer entries. Show that $\operatorname{det}{A}$ is square of an integer. Here is my idea: If $A$ is skew-symmetric matrix of odd order, then $\operatorname{det}{A}$ is zero. So, take $A$ to be of even order and non-singular. Since all the eigenvalues of $A$ are of the form $ia$ and its conjugate (where $a$ is real number), we see that $\operatorname{det}{A}$ is square of a real number. But I am not getting how to show it is square of an integer.","Let $A$ be a real skew-symmetric matrix with integer entries. Show that $\operatorname{det}{A}$ is square of an integer. Here is my idea: If $A$ is skew-symmetric matrix of odd order, then $\operatorname{det}{A}$ is zero. So, take $A$ to be of even order and non-singular. Since all the eigenvalues of $A$ are of the form $ia$ and its conjugate (where $a$ is real number), we see that $\operatorname{det}{A}$ is square of a real number. But I am not getting how to show it is square of an integer.",,"['linear-algebra', 'matrices']"
11,Is there a name for a matrix where the column space equals the row space? Does it have any interesting properties?,Is there a name for a matrix where the column space equals the row space? Does it have any interesting properties?,,"I sat down to write a linear algebra take-home exam problem where I would give a $4\times4$ matrix $A$ and ask for bases for these six spaces: $\mathrm{Col}\,A$, $\mathrm{Row}\,A$, $\mathrm{Nul}\,A$, $\mathrm{Col}\,A^t$, $\mathrm{Row}\,A^t$, $\mathrm{Nul}\,A^t$. Trivially, a basis for $\mathrm{Col}\,A$ will work for $\mathrm{Row}\,A^t$, and likewise with $\mathrm{Row}\,A$ and $\mathrm{Col}\,A^t$; that was meant to be part of the test. I expected $\mathrm{Nul}\,A$ and $\mathrm{Nul}\,A^t$ to be different. I worked on the first $4\times4$ matrix that came to mind: $\begin{bmatrix}1&2&3&4\\5&6&7&8\\9&10&11&12\\13&14&15&16\end{bmatrix}$. To my surprise, the standard method for finding the basis of the null space led me to see that in this case, $\mathrm{Nul}\,A=\mathrm{Nul}\,A^t$. (And since I do not want students to think this will always be the case, I have to try a different matrix.) As a consequence of the orthogonality of row and null spaces, $\mathrm{Row}\,A=\mathrm{Row}\,A^t$. So we have a matrix where: $$\mathrm{Col}\,A^t=\mathrm{Row}\,A=\mathrm{Row}\,A^t=\mathrm{Col}\,A$$ $$\mathrm{Nul}\; A=\mathrm{Nul}\;A^t$$ all of which is equivalent to just knowing $$\mathrm{Row}\, A=\mathrm{Col}\,A$$ or to just knowing $$\mathrm{Nul}\, A=\mathrm{Nul}\,A^t$$And these are equivalent to the existence of a matrix $B$ such that $AB=A^t$, since the rows of $A$ (columns of $A^t$) are linear combinations of the columns of $A$. Now any invertible matrix would automatically have these properties, since the null space of an invertible matrix is $\{\vec0\}$. And any symmetric matrix will automatically have these properties, since $A=A^t$. But it seems like a noninvertible, asymmetric square matrix will rarely have these properties. (In the $2\times 2$ case, there are none.) With general $n\times n$ matrices again, I would like to know Is there a name for this kind of matrix ($\mathrm{Row}\,A=\mathrm{Row}\,A^t$)? Perhaps additionally demanding noninvertible asymmetric specimens? Does this kind of matrix have any interesting applications? If we fix $\mathrm{rank}\, A$, do matrices of this form make an interesting surface in $M_{n\times n}$? If so, what is the dimension of this surface?","I sat down to write a linear algebra take-home exam problem where I would give a $4\times4$ matrix $A$ and ask for bases for these six spaces: $\mathrm{Col}\,A$, $\mathrm{Row}\,A$, $\mathrm{Nul}\,A$, $\mathrm{Col}\,A^t$, $\mathrm{Row}\,A^t$, $\mathrm{Nul}\,A^t$. Trivially, a basis for $\mathrm{Col}\,A$ will work for $\mathrm{Row}\,A^t$, and likewise with $\mathrm{Row}\,A$ and $\mathrm{Col}\,A^t$; that was meant to be part of the test. I expected $\mathrm{Nul}\,A$ and $\mathrm{Nul}\,A^t$ to be different. I worked on the first $4\times4$ matrix that came to mind: $\begin{bmatrix}1&2&3&4\\5&6&7&8\\9&10&11&12\\13&14&15&16\end{bmatrix}$. To my surprise, the standard method for finding the basis of the null space led me to see that in this case, $\mathrm{Nul}\,A=\mathrm{Nul}\,A^t$. (And since I do not want students to think this will always be the case, I have to try a different matrix.) As a consequence of the orthogonality of row and null spaces, $\mathrm{Row}\,A=\mathrm{Row}\,A^t$. So we have a matrix where: $$\mathrm{Col}\,A^t=\mathrm{Row}\,A=\mathrm{Row}\,A^t=\mathrm{Col}\,A$$ $$\mathrm{Nul}\; A=\mathrm{Nul}\;A^t$$ all of which is equivalent to just knowing $$\mathrm{Row}\, A=\mathrm{Col}\,A$$ or to just knowing $$\mathrm{Nul}\, A=\mathrm{Nul}\,A^t$$And these are equivalent to the existence of a matrix $B$ such that $AB=A^t$, since the rows of $A$ (columns of $A^t$) are linear combinations of the columns of $A$. Now any invertible matrix would automatically have these properties, since the null space of an invertible matrix is $\{\vec0\}$. And any symmetric matrix will automatically have these properties, since $A=A^t$. But it seems like a noninvertible, asymmetric square matrix will rarely have these properties. (In the $2\times 2$ case, there are none.) With general $n\times n$ matrices again, I would like to know Is there a name for this kind of matrix ($\mathrm{Row}\,A=\mathrm{Row}\,A^t$)? Perhaps additionally demanding noninvertible asymmetric specimens? Does this kind of matrix have any interesting applications? If we fix $\mathrm{rank}\, A$, do matrices of this form make an interesting surface in $M_{n\times n}$? If so, what is the dimension of this surface?",,['linear-algebra']
12,Does the product rule imply the chain rule?,Does the product rule imply the chain rule?,,"Let $\mathbb{F}$ be a field, and consider $\mathbb{F}^\mathbb{F}$ as an algebra over $\mathbb{F}$ with the standard function multiplication. Let $D$ be a derivation on a subalgebra of $\mathbb{F}^\mathbb{F}$ closed under function composition that takes the identity function to the function that sends everything to $1,$ as in the answers to these two questions. Does $D$ necessarily satisfy the chain rule for arbitrary $\mathbb{F}$ ?","Let be a field, and consider as an algebra over with the standard function multiplication. Let be a derivation on a subalgebra of closed under function composition that takes the identity function to the function that sends everything to as in the answers to these two questions. Does necessarily satisfy the chain rule for arbitrary ?","\mathbb{F} \mathbb{F}^\mathbb{F} \mathbb{F} D \mathbb{F}^\mathbb{F} 1, D \mathbb{F}","['linear-algebra', 'abstract-algebra', 'vector-spaces', 'field-theory', 'algebras']"
13,When is a symmetric matrix invertible?,When is a symmetric matrix invertible?,,"My professor always writes on the board: $A$ is $m \times n$, assuming that the vectors of $A$ form a basis, then $A^TA$ is always invertible. one thing I know is that $A^TA$ is always symmetric, but I'm not sure about the conditions on a symmetric matrix needed to ensure that it is invertible?","My professor always writes on the board: $A$ is $m \times n$, assuming that the vectors of $A$ form a basis, then $A^TA$ is always invertible. one thing I know is that $A^TA$ is always symmetric, but I'm not sure about the conditions on a symmetric matrix needed to ensure that it is invertible?",,"['linear-algebra', 'matrices', 'inverse', 'symmetric-matrices']"
14,LU decomposition; do permutation matrices commute?,LU decomposition; do permutation matrices commute?,,"I have an assignment for my Numerical Methods class to write a function that finds the PA=LU  decomposition for a given matrix A and returns P, L, and U. Nevermind the coding problems for a moment; there is a major mathematical problem I'm having that my professor seems incapable of addressing, and after searching for hours (perhaps inefficiently) I could find no accounting of it. How do we extricate the permutation matrices from the row elimination matrices? Essentially the idea, if I understand it correctly, is that we perform a series of transformations on a matrix $A$ by applying successive lower triangular matrices that eliminate single elements, thus $L_nL_{n-1}...L_2L_1A = U$. In my understanding, this is computationally useful because lower triangular atomic matrices can be inverted by changing the sign of the off-diagonal element, so $A = L_1^{-1}L_2^{-1}...U$. That's all fine (assuming I'm correct), but the introduction of pivot matrices between each $L_j$ seems to make the problem intractable. In every accounting I've seen some sorcery occurs that looks like this:  $$L_nP_nL_{n-1}...P_3L_2P_2L_1P_1A = U \Rightarrow P_nP_{n-1}...P_2P_1L_nL_{n-1}...L_2L_1A = U$$ And no one bothers to explain how this happens or in fact even states it explicitly. If possible I would like to know a) Is this operationally acceptable? b) What properties of these respective classes of matrices make this kind of willy-nilly commutation legal? c) Is my understanding of the method and its advantages accurate?","I have an assignment for my Numerical Methods class to write a function that finds the PA=LU  decomposition for a given matrix A and returns P, L, and U. Nevermind the coding problems for a moment; there is a major mathematical problem I'm having that my professor seems incapable of addressing, and after searching for hours (perhaps inefficiently) I could find no accounting of it. How do we extricate the permutation matrices from the row elimination matrices? Essentially the idea, if I understand it correctly, is that we perform a series of transformations on a matrix $A$ by applying successive lower triangular matrices that eliminate single elements, thus $L_nL_{n-1}...L_2L_1A = U$. In my understanding, this is computationally useful because lower triangular atomic matrices can be inverted by changing the sign of the off-diagonal element, so $A = L_1^{-1}L_2^{-1}...U$. That's all fine (assuming I'm correct), but the introduction of pivot matrices between each $L_j$ seems to make the problem intractable. In every accounting I've seen some sorcery occurs that looks like this:  $$L_nP_nL_{n-1}...P_3L_2P_2L_1P_1A = U \Rightarrow P_nP_{n-1}...P_2P_1L_nL_{n-1}...L_2L_1A = U$$ And no one bothers to explain how this happens or in fact even states it explicitly. If possible I would like to know a) Is this operationally acceptable? b) What properties of these respective classes of matrices make this kind of willy-nilly commutation legal? c) Is my understanding of the method and its advantages accurate?",,"['linear-algebra', 'matrices', 'matrix-decomposition', 'gaussian-elimination', 'permutation-matrices']"
15,Square root is operator monotone,Square root is operator monotone,,"This is a fact I've used a lot, but how would one actually prove this statement? Paraphrased: given two positive operators $X, Y \geq 0$, how can you show that $X^2 \leq Y^2 \Rightarrow X \leq Y$ (or that $X \leq Y \Rightarrow \sqrt X \leq \sqrt Y$, but I feel like the first version would be easier to work with)? Note: $X$ and $Y$ don't have to commute , so $X^2 - Y^2$ is not necessarily $(X+Y)(X-Y)$.","This is a fact I've used a lot, but how would one actually prove this statement? Paraphrased: given two positive operators $X, Y \geq 0$, how can you show that $X^2 \leq Y^2 \Rightarrow X \leq Y$ (or that $X \leq Y \Rightarrow \sqrt X \leq \sqrt Y$, but I feel like the first version would be easier to work with)? Note: $X$ and $Y$ don't have to commute , so $X^2 - Y^2$ is not necessarily $(X+Y)(X-Y)$.",,"['linear-algebra', 'functional-analysis', 'operator-theory', 'c-star-algebras']"
16,Comparing two rotation matrices,Comparing two rotation matrices,,"Problem I want to compare two rotation matrices $R_A$ and $R_B$ both representing the orientation of the same point cloud in space, but computed from different methods. The idea is to have an estimation of the error between those two matrices. Method My idea was to do it as follows: Compute the rotation $R_{AB}$ between $R_A$ and $R_B$ as $R_{AB} = R_A^TR_B$ Compute the axis-angle ($\omega$, $\theta$) representation of $R_{AB}$ using the following formula: $$Tr(R_A) = 1 + 2cos(\theta)$$ Use the angle $\theta$ as the rotation error. In Python, I do: r_oa = import(R_A) // See R_A below, in ""Data"" r_ob = import(R_B) // See R_B below, in ""Data""  r_oa_t = np.transpose(r_oa) r_ab = r_oa_t * r_ob  np.rad2deg(np.arccos((np.trace(r_ab) - 1) / 2)) That seems quite straightforward to me, but I get $\theta = 23.86\unicode{xb0}$, which seems really unrealistic. This intuition is confirmed by the use of a software comparing the matrices, as described below in ""Verification"". Am I doing something wrong there? Is it just not the right way to compare my matrices? Verification In a processing software, I have the possibility to ""compare"" two matrices (each being a 4x4 matrix defining a position and an orientation: [R | t]). The documentation does not explain exactly how it works, but the output of this comparison is: 4 values for the rotation [deg]: Omega, Phi, Kappa, Total 4 values for the translation: $\Delta$x, $\Delta$y, $\Delta$z, Total. I assumed that the ""Total"" angle for the rotation is my $\theta$ (as described above). For the same matrices $R_A$ and $R_B$ as above, the software outputs: Total = 0.036477551. That seems much more realistic, but then I don't know why it is not what I get. Just to check my computation of $\theta$, I tried to compute it on $R_A$ and $R_B$ (instead of $R_{AB}$) and to compare that to the output of the software when running its comparison between $R_A$ and $I$, and also between $R_B$ and $I$. Even though I don't get the exact same $\theta$ as the software, my result is really close (to the 4th decimal). Assuming that the software loses precision somewhere, I believe that my computation of $\theta$ is correct. And my computation of $R_{AB}$ is quite straightforward, so I don't understand why I don't find the same $\theta$ for $R_{AB}$. Data My matrices are: $R_A = \begin{bmatrix} -0.956395958000000 & 0.292073230000000 & 0.000014880000000 \\ -0.292073218000000 & -0.956395931000000 & 0.000242173000000 \\ 0.000084963000000 & 0.000227268000000 & 0.999999971000000\end{bmatrix}$ $R_B = \begin{bmatrix}  -0.956227882000000 & 0.292623030000000 & -0.000013768000000 \\ -0.292623029000000 & -0.956227882000000 & -0.000029806000000 \\ -0.000021887000000 & -0.000024473000000 & 0.999999999000000 \end{bmatrix}$ The software outputs, for the comparison between $R_A$ and $R_B$: ""Total = 0.036477551"". Following my method, I get $\theta = 23.86$, which is completely different. Related questions ""Change in rotation"" matrix","Problem I want to compare two rotation matrices $R_A$ and $R_B$ both representing the orientation of the same point cloud in space, but computed from different methods. The idea is to have an estimation of the error between those two matrices. Method My idea was to do it as follows: Compute the rotation $R_{AB}$ between $R_A$ and $R_B$ as $R_{AB} = R_A^TR_B$ Compute the axis-angle ($\omega$, $\theta$) representation of $R_{AB}$ using the following formula: $$Tr(R_A) = 1 + 2cos(\theta)$$ Use the angle $\theta$ as the rotation error. In Python, I do: r_oa = import(R_A) // See R_A below, in ""Data"" r_ob = import(R_B) // See R_B below, in ""Data""  r_oa_t = np.transpose(r_oa) r_ab = r_oa_t * r_ob  np.rad2deg(np.arccos((np.trace(r_ab) - 1) / 2)) That seems quite straightforward to me, but I get $\theta = 23.86\unicode{xb0}$, which seems really unrealistic. This intuition is confirmed by the use of a software comparing the matrices, as described below in ""Verification"". Am I doing something wrong there? Is it just not the right way to compare my matrices? Verification In a processing software, I have the possibility to ""compare"" two matrices (each being a 4x4 matrix defining a position and an orientation: [R | t]). The documentation does not explain exactly how it works, but the output of this comparison is: 4 values for the rotation [deg]: Omega, Phi, Kappa, Total 4 values for the translation: $\Delta$x, $\Delta$y, $\Delta$z, Total. I assumed that the ""Total"" angle for the rotation is my $\theta$ (as described above). For the same matrices $R_A$ and $R_B$ as above, the software outputs: Total = 0.036477551. That seems much more realistic, but then I don't know why it is not what I get. Just to check my computation of $\theta$, I tried to compute it on $R_A$ and $R_B$ (instead of $R_{AB}$) and to compare that to the output of the software when running its comparison between $R_A$ and $I$, and also between $R_B$ and $I$. Even though I don't get the exact same $\theta$ as the software, my result is really close (to the 4th decimal). Assuming that the software loses precision somewhere, I believe that my computation of $\theta$ is correct. And my computation of $R_{AB}$ is quite straightforward, so I don't understand why I don't find the same $\theta$ for $R_{AB}$. Data My matrices are: $R_A = \begin{bmatrix} -0.956395958000000 & 0.292073230000000 & 0.000014880000000 \\ -0.292073218000000 & -0.956395931000000 & 0.000242173000000 \\ 0.000084963000000 & 0.000227268000000 & 0.999999971000000\end{bmatrix}$ $R_B = \begin{bmatrix}  -0.956227882000000 & 0.292623030000000 & -0.000013768000000 \\ -0.292623029000000 & -0.956227882000000 & -0.000029806000000 \\ -0.000021887000000 & -0.000024473000000 & 0.999999999000000 \end{bmatrix}$ The software outputs, for the comparison between $R_A$ and $R_B$: ""Total = 0.036477551"". Following my method, I get $\theta = 23.86$, which is completely different. Related questions ""Change in rotation"" matrix",,"['linear-algebra', 'rotations']"
17,"A relationship between matrices, bernoulli polynomials, and binomial coefficients","A relationship between matrices, bernoulli polynomials, and binomial coefficients",,"We define the following polynomials, for $n≥0$: $$P_n(x)=(x+1)^{n+1}-x^{n+1}=\sum_{k=0}^{n}{\binom{n+1}{k}x^k}$$ For $n=0,1,2,3$ this gives us, $$P_0(x)=1\enspace P_1(x)=2x+1\enspace P_2(x)=3x^2+3x+1\enspace P_3(x)=4x^3+6x^2+4x+1$$ We then define the set $P_{(3)}=\{P_0,P_1,P_2,P_3\}$. It can be easily shown that this set is a basis over the vector space of polynomials of degree $3$ and lower. We take $3$ for the sake of brevity. Taking the coefficients of these polynomials and turning them into column vectors, we can construct the matrix (coefficients from the lowest term to the highest term) $$\large{M_{P_{(3)}}}=\begin{pmatrix}  1 & 1 & 1 & 1 \\  0 & 2 & 3 & 4 \\  0 & 0 & 3 & 6 \\  0 & 0 & 0 & 4 \end{pmatrix}$$ We'll call this matrix the pascal in the context of this post, and the above polynomials as pascal polynomials.  The inverse of this matrix is the matrix, $$M_{P_{(3)}}^{-1}=\begin{pmatrix}  1 & -\frac{1}{2} & \frac{1}{6} & 0 \\  0 & \frac{1}{2} & -\frac{1}{2} & \frac{1}{4} \\  0 & 0 & \frac{1}{3} & -\frac{1}{2} \\  0 & 0 & 0 & \frac{1}{4} \end{pmatrix}$$ We'll factor this matrix into two matrices as follows: $$M_{P_{(3)}}^{-1}=\begin{pmatrix}  1 & 0 & 0 & 0 \\  0 & \frac{1}{2} & 0 & 0 \\  0 & 0 & \frac{1}{3} & 0 \\  0 & 0 & 0 & \frac{1}{4} \end{pmatrix}×\begin{pmatrix}  1 & -\frac{1}{2} & \frac{1}{6} & 0 \\  0 & 1 & -1 & \frac{1}{2} \\  0 & 0 & 1 & -\frac{3}{2} \\  0 & 0 & 0 & 1 \end{pmatrix}$$ We can see the Bernoulli numbers in the first row of the matrix. Every column is a coefficient vector of a Bernoulli polynomial The following are extended versions of these matrices: Pascal Matrix Inverse Pascal Matrix Last, Bernoulli Matrix Are there accepted names for these matrices and polynomials? What is the meaning of these relationships? In particular, is there some treatment of using these matrices as change of basis transformations between representations of polynomials? E.g. from a linear combination of pascal polynomials to a linear combination of monomial terms.","We define the following polynomials, for $n≥0$: $$P_n(x)=(x+1)^{n+1}-x^{n+1}=\sum_{k=0}^{n}{\binom{n+1}{k}x^k}$$ For $n=0,1,2,3$ this gives us, $$P_0(x)=1\enspace P_1(x)=2x+1\enspace P_2(x)=3x^2+3x+1\enspace P_3(x)=4x^3+6x^2+4x+1$$ We then define the set $P_{(3)}=\{P_0,P_1,P_2,P_3\}$. It can be easily shown that this set is a basis over the vector space of polynomials of degree $3$ and lower. We take $3$ for the sake of brevity. Taking the coefficients of these polynomials and turning them into column vectors, we can construct the matrix (coefficients from the lowest term to the highest term) $$\large{M_{P_{(3)}}}=\begin{pmatrix}  1 & 1 & 1 & 1 \\  0 & 2 & 3 & 4 \\  0 & 0 & 3 & 6 \\  0 & 0 & 0 & 4 \end{pmatrix}$$ We'll call this matrix the pascal in the context of this post, and the above polynomials as pascal polynomials.  The inverse of this matrix is the matrix, $$M_{P_{(3)}}^{-1}=\begin{pmatrix}  1 & -\frac{1}{2} & \frac{1}{6} & 0 \\  0 & \frac{1}{2} & -\frac{1}{2} & \frac{1}{4} \\  0 & 0 & \frac{1}{3} & -\frac{1}{2} \\  0 & 0 & 0 & \frac{1}{4} \end{pmatrix}$$ We'll factor this matrix into two matrices as follows: $$M_{P_{(3)}}^{-1}=\begin{pmatrix}  1 & 0 & 0 & 0 \\  0 & \frac{1}{2} & 0 & 0 \\  0 & 0 & \frac{1}{3} & 0 \\  0 & 0 & 0 & \frac{1}{4} \end{pmatrix}×\begin{pmatrix}  1 & -\frac{1}{2} & \frac{1}{6} & 0 \\  0 & 1 & -1 & \frac{1}{2} \\  0 & 0 & 1 & -\frac{3}{2} \\  0 & 0 & 0 & 1 \end{pmatrix}$$ We can see the Bernoulli numbers in the first row of the matrix. Every column is a coefficient vector of a Bernoulli polynomial The following are extended versions of these matrices: Pascal Matrix Inverse Pascal Matrix Last, Bernoulli Matrix Are there accepted names for these matrices and polynomials? What is the meaning of these relationships? In particular, is there some treatment of using these matrices as change of basis transformations between representations of polynomials? E.g. from a linear combination of pascal polynomials to a linear combination of monomial terms.",,"['linear-algebra', 'number-theory', 'binomial-coefficients', 'bernoulli-numbers']"
18,Does zero vector have zero dimension?,Does zero vector have zero dimension?,,"I know this sounds like a stupid question, but I just want to organize and clear what I studied. For an $n\times n$ matrix $A$ , it has independent columns when nullspace only has zero vector. And independent columns mean $A$ has rank $n$ , therefore by the rank theorem, nullspace has zero dimension. That is, zero vector is zero dimension, is that right? AND one more thing. I want to show that $\lbrace Av_1,...,Av_n \rbrace$ span $R^n$ when $\lbrace v_1,...,v_n \rbrace$ form a basis. Dimension theorem is used in here? If so, how can I show that $\lbrace Av_1,...,Av_n \rbrace$ span $R^n$ ?","I know this sounds like a stupid question, but I just want to organize and clear what I studied. For an matrix , it has independent columns when nullspace only has zero vector. And independent columns mean has rank , therefore by the rank theorem, nullspace has zero dimension. That is, zero vector is zero dimension, is that right? AND one more thing. I want to show that span when form a basis. Dimension theorem is used in here? If so, how can I show that span ?","n\times n A A n \lbrace Av_1,...,Av_n \rbrace R^n \lbrace v_1,...,v_n \rbrace \lbrace Av_1,...,Av_n \rbrace R^n",['linear-algebra']
19,"What's the ""geometry"" in ""geometric multiplicity""?","What's the ""geometry"" in ""geometric multiplicity""?",,"The geometric multiplicity of an eigenvalue is defined as the dimension of the associated eigenspace, i.e. number of linearly independent eigenvectors with that eigenvalue. Here are my questions : Where is the name ""geometric multiplicity"" from in math history? Is there any thing in geometry related to this concept? Does it only mean the number of linearly independent eigenvectors with that eigenvalue one can ""draw"" as the definition says?","The geometric multiplicity of an eigenvalue is defined as the dimension of the associated eigenspace, i.e. number of linearly independent eigenvectors with that eigenvalue. Here are my questions : Where is the name ""geometric multiplicity"" from in math history? Is there any thing in geometry related to this concept? Does it only mean the number of linearly independent eigenvectors with that eigenvalue one can ""draw"" as the definition says?",,['linear-algebra']
20,Questions about Spivak's proof that $\sqrt{2} + \sqrt[3]{2}$ is irrational,Questions about Spivak's proof that  is irrational,\sqrt{2} + \sqrt[3]{2},"I'm aware that there are already two questions about this specific proof (namely this one and this one ), but I'm specifically puzzled by Spivak's approach in his Answer Book . The basic idea is clear enough: one needs to find a polynomial expression of the form $x^6 + a_5 x^5 + a_4 x^4 + a_3 x^3 + a_2 x^2 + a_1 x + a_0$ (with $a_0, \dots, a_5$ integers) with $\sqrt{2} + \sqrt[3]{2}$ as a root; then, by Gauss's lemma, since $\sqrt{2} + \sqrt[3]{2}$ is not an integer, it is not rational either. The question thus reduces to finding the appropriate $a_0, \dots, a_5$. And here is where I got confused. Spivak's idea is the following: let $x = \sqrt{2} + \sqrt[3]{2}$ and $n = 2^{\frac{1}{6}}$. We can then write the first powers of $x$ in terms of $n$ as follows: $x^0 = n^0$ $x^1 = n^2 + n^3$ $x^2 = 2n^0 + n^4 + 2n^5$ $x^3 = 2n^0 + 6n + 6n^2 + 2n^3$ $x^4 = 4n^0 + 2n^2 + 8n^3 + 12n^4 + 8n^5$ $x^5 = 40n^0 + 40n + 20n^2 + 4n^3 + 2n^4 + 10n^5$ $x^6 = 12n^0 + 24n + 60n^2 + 80n^3 + 60n^4 + 24n^5$ This gives us the following table: $\begin{array}{l | c | r}     & n^0 & n^1 & n^2 & n^3 & n^4 & n^5 \\ x^0 & 1   & 0   & 0   & 0   & 0   & 0   \\ x^1 & 0   & 0   & 1   & 1   & 0   & 0   \\ x^2 & 2   & 0   & 0   & 0   & 1   & 2   \\ x^3 & 2   & 6   & 6   & 2   & 0   & 0   \\ x^4 & 4   & 0   & 2   & 8   & 12  & 8   \\ x^5 & 40  & 40  & 20  & 4   & 2   & 10  \\ x^6 & 12  & 24  & 60  & 80  & 60  & 24 \end{array}$ (Note that there's a missing $4$ in the second column of Spivak's own table on p. 14, with a corresponding missing term in the first equation below) This is the part I don't really understand. Spivak then claims that the integer coefficients $a_0, \dots, a_5$ are the integers satisfying the system of linear equations generated by the columns of the above table, i.e. satisfying: $a_0 + 2a_2 + 2a_3 + 4a_4 + 40a_5 + 12 =0$ $6a_3 + 40a_5 + 24=0$ $a_1 + 6a_3 + 2a_4 + 20a_5 + 60 = 0$ $a_1 + 2a_3 + 8a_4 + 4a_5 + 80 = 0$ $a_2 + 12a_4 +2a_5 + 60 =0$ $2a_2 + 8a_4 + 10a_5 + 24 = 0$ I checked and indeed the solution to this system of equations indeed gives us the right answer. But I don't understand why . That is, I don't understand (a) what is the heuristics behind the solution, i.e. how he came up with this idea and, more importantly, (b) why taking the columns in the above table as coefficients in a system of linear equations gives us the correct solution. If someone could explain this to me, I would be immensely grateful!","I'm aware that there are already two questions about this specific proof (namely this one and this one ), but I'm specifically puzzled by Spivak's approach in his Answer Book . The basic idea is clear enough: one needs to find a polynomial expression of the form $x^6 + a_5 x^5 + a_4 x^4 + a_3 x^3 + a_2 x^2 + a_1 x + a_0$ (with $a_0, \dots, a_5$ integers) with $\sqrt{2} + \sqrt[3]{2}$ as a root; then, by Gauss's lemma, since $\sqrt{2} + \sqrt[3]{2}$ is not an integer, it is not rational either. The question thus reduces to finding the appropriate $a_0, \dots, a_5$. And here is where I got confused. Spivak's idea is the following: let $x = \sqrt{2} + \sqrt[3]{2}$ and $n = 2^{\frac{1}{6}}$. We can then write the first powers of $x$ in terms of $n$ as follows: $x^0 = n^0$ $x^1 = n^2 + n^3$ $x^2 = 2n^0 + n^4 + 2n^5$ $x^3 = 2n^0 + 6n + 6n^2 + 2n^3$ $x^4 = 4n^0 + 2n^2 + 8n^3 + 12n^4 + 8n^5$ $x^5 = 40n^0 + 40n + 20n^2 + 4n^3 + 2n^4 + 10n^5$ $x^6 = 12n^0 + 24n + 60n^2 + 80n^3 + 60n^4 + 24n^5$ This gives us the following table: $\begin{array}{l | c | r}     & n^0 & n^1 & n^2 & n^3 & n^4 & n^5 \\ x^0 & 1   & 0   & 0   & 0   & 0   & 0   \\ x^1 & 0   & 0   & 1   & 1   & 0   & 0   \\ x^2 & 2   & 0   & 0   & 0   & 1   & 2   \\ x^3 & 2   & 6   & 6   & 2   & 0   & 0   \\ x^4 & 4   & 0   & 2   & 8   & 12  & 8   \\ x^5 & 40  & 40  & 20  & 4   & 2   & 10  \\ x^6 & 12  & 24  & 60  & 80  & 60  & 24 \end{array}$ (Note that there's a missing $4$ in the second column of Spivak's own table on p. 14, with a corresponding missing term in the first equation below) This is the part I don't really understand. Spivak then claims that the integer coefficients $a_0, \dots, a_5$ are the integers satisfying the system of linear equations generated by the columns of the above table, i.e. satisfying: $a_0 + 2a_2 + 2a_3 + 4a_4 + 40a_5 + 12 =0$ $6a_3 + 40a_5 + 24=0$ $a_1 + 6a_3 + 2a_4 + 20a_5 + 60 = 0$ $a_1 + 2a_3 + 8a_4 + 4a_5 + 80 = 0$ $a_2 + 12a_4 +2a_5 + 60 =0$ $2a_2 + 8a_4 + 10a_5 + 24 = 0$ I checked and indeed the solution to this system of equations indeed gives us the right answer. But I don't understand why . That is, I don't understand (a) what is the heuristics behind the solution, i.e. how he came up with this idea and, more importantly, (b) why taking the columns in the above table as coefficients in a system of linear equations gives us the correct solution. If someone could explain this to me, I would be immensely grateful!",,"['linear-algebra', 'polynomials', 'irrational-numbers']"
21,Generalizing Cauchy-Schwarz for more than two vectors,Generalizing Cauchy-Schwarz for more than two vectors,,"For a complex inner product space, $X$, Cauchy-Schwarz inequality states $$ | \langle x,y \rangle |^2 \leq \langle x,x\rangle \cdot \langle y, y\rangle , $$ for any $x,y \in X$.  Equality holds if and only if $x$ and $y$ are linearly dependent.  I noticed that this can be restated as: $$ \left|\begin{array}{cc}  \langle v_1, v_1 \rangle & \langle v_1, v_2\rangle \\ \langle v_2, v_1 \rangle & \langle v_2, v_2\rangle \\  \end{array}\right| \geq 0$$  with strict equality if $\{v_i \}$ is linearly independent.  Does this (somehow) generalize for $n$ vectors?  That is, does the following hold:  $$ \left|\begin{array}{cccc}  \langle v_1, v_1 \rangle & \langle v_1, v_2\rangle & \cdots &\langle v_1, v_n \rangle \\ \langle v_2, v_1 \rangle & \langle v_2, v_2\rangle & \cdots &\langle v_2, v_n \rangle \\ \vdots & \vdots & \ddots & \vdots \\ \langle v_n, v_1 \rangle & \langle v_n, v_2\rangle & \cdots &\langle v_n, v_n \rangle  \end{array}\right| \geq 0$$ At the very least, can we prove that the above determinant is non-zero if $\{v_i \}$ is linearly independent?  I came across this working on a functional analysis problem set, but this isn't a homework problem. EDIT: For those tagging it as a duplicate, I see this as different because this question specifically concerns inequality, and not just proving that the determinant is non-zero if they are linearly independent.  Additionally, this post specifically suggests a connection to Cauchy-Schwarz that isn't mentioned in the other post. As a commenter (Algebraic) pointed out, this matrix is called the Gram matrix of the vectors $\{v_i\}$; Wikipedia states that this matrix if positive semi-definite, and is positive definite in the case where they are linearly independent.  This proves that the determinant is indeed greater than or equal to zero for arbitrary $\{v_i\}$ and is strictly positive in the case where the $\{v_i\}$ are linearly independent.","For a complex inner product space, $X$, Cauchy-Schwarz inequality states $$ | \langle x,y \rangle |^2 \leq \langle x,x\rangle \cdot \langle y, y\rangle , $$ for any $x,y \in X$.  Equality holds if and only if $x$ and $y$ are linearly dependent.  I noticed that this can be restated as: $$ \left|\begin{array}{cc}  \langle v_1, v_1 \rangle & \langle v_1, v_2\rangle \\ \langle v_2, v_1 \rangle & \langle v_2, v_2\rangle \\  \end{array}\right| \geq 0$$  with strict equality if $\{v_i \}$ is linearly independent.  Does this (somehow) generalize for $n$ vectors?  That is, does the following hold:  $$ \left|\begin{array}{cccc}  \langle v_1, v_1 \rangle & \langle v_1, v_2\rangle & \cdots &\langle v_1, v_n \rangle \\ \langle v_2, v_1 \rangle & \langle v_2, v_2\rangle & \cdots &\langle v_2, v_n \rangle \\ \vdots & \vdots & \ddots & \vdots \\ \langle v_n, v_1 \rangle & \langle v_n, v_2\rangle & \cdots &\langle v_n, v_n \rangle  \end{array}\right| \geq 0$$ At the very least, can we prove that the above determinant is non-zero if $\{v_i \}$ is linearly independent?  I came across this working on a functional analysis problem set, but this isn't a homework problem. EDIT: For those tagging it as a duplicate, I see this as different because this question specifically concerns inequality, and not just proving that the determinant is non-zero if they are linearly independent.  Additionally, this post specifically suggests a connection to Cauchy-Schwarz that isn't mentioned in the other post. As a commenter (Algebraic) pointed out, this matrix is called the Gram matrix of the vectors $\{v_i\}$; Wikipedia states that this matrix if positive semi-definite, and is positive definite in the case where they are linearly independent.  This proves that the determinant is indeed greater than or equal to zero for arbitrary $\{v_i\}$ and is strictly positive in the case where the $\{v_i\}$ are linearly independent.",,"['linear-algebra', 'functional-analysis']"
22,Finding $\max |A|$ with $a_{ij}=\pm 1$,Finding  with,\max |A| a_{ij}=\pm 1,"$A$ is a matrix sized $n\times n$, all elements in $A$ are $\pm1$. Find $\max |A|$. My Attempt Denote $f(n)=\max |A_{n\times n}|$. $f(1)=1$. $f(2)=2$ is also obviously. If $n\ge2$, $|A|$ must be even. For $n=3$, $f(3)\ge4$ because $\left|\begin{array} r1&1&1\\1&-1&1\\1&-1&-1 \end{array}\right|=4$. Also, $|A|=A_{11} A_{22} A_{33}+A_{12} A_{23} A_{31}+A_{13} A_{21} A_{32}-A_{13} A_{22} A_{31}-A_{11} A_{23} A_{32}-A_{12} A_{21} A_{33}$ can not be $6$ since $A_{11} A_{22} A_{33}A_{12} A_{23} A_{31}A_{13} A_{21} A_{32}A_{13} A_{22} A_{31}A_{11} A_{23} A_{32}A_{12} A_{21} A_{33}$ must be $1$.  $1\ne 1\cdot1\cdot1\cdot(-1)\cdot(-1)\cdot(-1)$. Hence we have $f(3)\ne 6$. $f(3)=4$. For $n\ge4$, I have no idea where to start with $f(n)$. A trivial bound is $0<f(n)\le n!$. EDIT Related question: Maximum value of Determinant of $3 \times 3$ Matrix with entries $\pm 1$ It is not duplicated since I am discussing $n\times n$ determinants, not $3\times3$.","$A$ is a matrix sized $n\times n$, all elements in $A$ are $\pm1$. Find $\max |A|$. My Attempt Denote $f(n)=\max |A_{n\times n}|$. $f(1)=1$. $f(2)=2$ is also obviously. If $n\ge2$, $|A|$ must be even. For $n=3$, $f(3)\ge4$ because $\left|\begin{array} r1&1&1\\1&-1&1\\1&-1&-1 \end{array}\right|=4$. Also, $|A|=A_{11} A_{22} A_{33}+A_{12} A_{23} A_{31}+A_{13} A_{21} A_{32}-A_{13} A_{22} A_{31}-A_{11} A_{23} A_{32}-A_{12} A_{21} A_{33}$ can not be $6$ since $A_{11} A_{22} A_{33}A_{12} A_{23} A_{31}A_{13} A_{21} A_{32}A_{13} A_{22} A_{31}A_{11} A_{23} A_{32}A_{12} A_{21} A_{33}$ must be $1$.  $1\ne 1\cdot1\cdot1\cdot(-1)\cdot(-1)\cdot(-1)$. Hence we have $f(3)\ne 6$. $f(3)=4$. For $n\ge4$, I have no idea where to start with $f(n)$. A trivial bound is $0<f(n)\le n!$. EDIT Related question: Maximum value of Determinant of $3 \times 3$ Matrix with entries $\pm 1$ It is not duplicated since I am discussing $n\times n$ determinants, not $3\times3$.",,"['linear-algebra', 'determinant']"
23,Prove the inequality: $\prod_{j=1}^ka_{jj}\leq\left(\frac{1}{k}\sum_{j=1}^k\lambda_j\right)^k.$,Prove the inequality:,\prod_{j=1}^ka_{jj}\leq\left(\frac{1}{k}\sum_{j=1}^k\lambda_j\right)^k.,"To all those who are eagerly awaiting a new question, all those who love math, I give this challenge and I hope for you good moments of reflection. Let $A=(a_{ij})_n$ a real nonnegative symmetric matrix with eigenvalues $\lambda_1\geq\cdots\geq \lambda_n\geq 0$. How to prove that, for all $k\in\{1,\ldots,n\}$, we have $$\prod_{j=1}^ka_{jj}\leq\left(\frac{1}{k}\sum_{j=1}^k\lambda_j\right)^k.$$","To all those who are eagerly awaiting a new question, all those who love math, I give this challenge and I hope for you good moments of reflection. Let $A=(a_{ij})_n$ a real nonnegative symmetric matrix with eigenvalues $\lambda_1\geq\cdots\geq \lambda_n\geq 0$. How to prove that, for all $k\in\{1,\ldots,n\}$, we have $$\prod_{j=1}^ka_{jj}\leq\left(\frac{1}{k}\sum_{j=1}^k\lambda_j\right)^k.$$",,['linear-algebra']
24,Finite Subgroups of $GL_2(\mathbb Q)$,Finite Subgroups of,GL_2(\mathbb Q),"I want to prove that the only finite subgroups of $GL_2(\mathbb Q)$ are $C_1, C_2, C_3, C_4, C_6, V_4, D_6, D_8,$ and $D_{12}$. First, we determine all possible finite orders of elements. Now, an element of order $n$ will have a minimal polynomial that divides $x^n-1$, so its (complex) roots will be distinct, and so the matrix will be diagonalizable over $\mathbb C$. This implies that both eigenvalues are $n$th roots of unity, and that at least one is primitive, and so the minimal polynomial will be the $n$th cyclotomic polynomial. But since the minimal polynomial can only have degree $1$ or $2$, since we're dealing with $2\times 2$ matrices, the only possible orders are those $n$ for which $\phi(n)=1$ or $2$, so the only possible orders are $1, 2, 3, 4$, and $6$.  Thus, if $G$ is a finite subgroup of $GL_2(\mathbb Q)$, then $|G|=2^a3^b$. Now, since $G$ contains a Sylow-$3$ of order $3^b$, and any $3$-group contains subgroups of every possible order, once we show that $C_3\times C_3$ is not a subgroup of $GL_2(\mathbb Q)$, then we can conclude that $b=0$ or $1$, since we already saw that $C_9$ cannot be a subgroup. WLOG, let $g=\begin{bmatrix} 0&-1\\1&-1\end{bmatrix}$, which is the Rational Canonical Form for the minimal polynomial $x^2+x+1$, and so has order $3$. We seek to show that there is no matrix $h$ such that $h$ has order $3$, commutes with $g$, and isn't a power of $g$. So assume $h=\begin{bmatrix} a&b\\c&d\end{bmatrix}$. Then $gh=\begin{bmatrix} -c&-d\\a-c&b-d\end{bmatrix}$ and $hg=\begin{bmatrix} b&-(a+b)\\d&-(c+d)\end{bmatrix}$. Equating these, we get that $c=-b, d=a+b$, so $h=\begin{bmatrix} a&b\\-b&a+b\end{bmatrix}$. For $h$ to have order $3$, the trace must be $-1$, and the determinant must be $1$, just like in the Rational Canonical Form, so $2a+b=1$, and $a^2+ab+b^2=1$. The solutions are $a=-1, b=1$ and $a=0, b=-1$. The latter gives $h=g$ and the former gives $h=g^2$, and so there can be no subgroup isomorphic to $C_3\times C_3$, and thus $9$ does not divide the order of the group. Now, the next step would be to restrict the exponent of $2$, but I'm not quite sure how to do this. One part of the problem tells me to show that the only noncyclic abelian subgroup is $V_4$, the Klein-$4$ group. So if we let $g=\begin{bmatrix} 0&1\\1&0\end{bmatrix}$ be the RCF of $x+1$, then the only order $2$ matrices that commute with it are $-g$ and $-g^2=-I$, which form a Klein-$4$ group. This means that $C_2^3$ is not a subgroup. Also, if we get $g=\begin{bmatrix} 0&-1\\1&0\end{bmatrix}$ be the RCF of $x^2+1$, then the only order $2$ element that commutes with it is $-I=g^2$, so there is also no subgroup isomorphic to $C_4\times C_2$. In a previous exercise, I showed that $Q_8$ is not a subgroup of $GL_2(\mathbb R)$, and thus isn't a subgroup over $\mathbb Q$ either. But this doesn't seem to prevent subgroups of order $16$, since $D_8$ is in fact a subgroup. Checking all $14$ groups of order $16$, it seems that all of them have an order $8$ subgroup besides $D_8$, but is there a cleaner way to rule out groups of order $16$ without classifying them, since I'm apparently supposed to conclude that the order of $G$ divides $24=2^3\cdot 3$ from the fact that the Klein-$4$ group is the only noncyclic abelian subgroup. So now, if I assume as known that $24$ divides the order of $G$, then the possible orders for $G$ are $1, 2, 3, 4, 6, 8, 12$, and $24$. I can find $C_1, C_2, C_3, C_4, C_6, V_4, D_6, D_8,$ and $D_{12}$. This takes care of all the orders except $12$ and $24$. I can show that $C_{12}$ and $C_{6}\times C_2$ are not subgroups, but I'm not sure how to rule out the nonabelian groups of order 12, $A_4$ and $C_3\rtimes C_4$, or the groups of order $24$. So in summary, I'm a bit stuck in ruling out the nonabelian groups of order $12, 16,$ and $24$. Is there a more elegant way to do this than to look at the classifications of these groups and find subgroups which I have already shown to be impossible?","I want to prove that the only finite subgroups of $GL_2(\mathbb Q)$ are $C_1, C_2, C_3, C_4, C_6, V_4, D_6, D_8,$ and $D_{12}$. First, we determine all possible finite orders of elements. Now, an element of order $n$ will have a minimal polynomial that divides $x^n-1$, so its (complex) roots will be distinct, and so the matrix will be diagonalizable over $\mathbb C$. This implies that both eigenvalues are $n$th roots of unity, and that at least one is primitive, and so the minimal polynomial will be the $n$th cyclotomic polynomial. But since the minimal polynomial can only have degree $1$ or $2$, since we're dealing with $2\times 2$ matrices, the only possible orders are those $n$ for which $\phi(n)=1$ or $2$, so the only possible orders are $1, 2, 3, 4$, and $6$.  Thus, if $G$ is a finite subgroup of $GL_2(\mathbb Q)$, then $|G|=2^a3^b$. Now, since $G$ contains a Sylow-$3$ of order $3^b$, and any $3$-group contains subgroups of every possible order, once we show that $C_3\times C_3$ is not a subgroup of $GL_2(\mathbb Q)$, then we can conclude that $b=0$ or $1$, since we already saw that $C_9$ cannot be a subgroup. WLOG, let $g=\begin{bmatrix} 0&-1\\1&-1\end{bmatrix}$, which is the Rational Canonical Form for the minimal polynomial $x^2+x+1$, and so has order $3$. We seek to show that there is no matrix $h$ such that $h$ has order $3$, commutes with $g$, and isn't a power of $g$. So assume $h=\begin{bmatrix} a&b\\c&d\end{bmatrix}$. Then $gh=\begin{bmatrix} -c&-d\\a-c&b-d\end{bmatrix}$ and $hg=\begin{bmatrix} b&-(a+b)\\d&-(c+d)\end{bmatrix}$. Equating these, we get that $c=-b, d=a+b$, so $h=\begin{bmatrix} a&b\\-b&a+b\end{bmatrix}$. For $h$ to have order $3$, the trace must be $-1$, and the determinant must be $1$, just like in the Rational Canonical Form, so $2a+b=1$, and $a^2+ab+b^2=1$. The solutions are $a=-1, b=1$ and $a=0, b=-1$. The latter gives $h=g$ and the former gives $h=g^2$, and so there can be no subgroup isomorphic to $C_3\times C_3$, and thus $9$ does not divide the order of the group. Now, the next step would be to restrict the exponent of $2$, but I'm not quite sure how to do this. One part of the problem tells me to show that the only noncyclic abelian subgroup is $V_4$, the Klein-$4$ group. So if we let $g=\begin{bmatrix} 0&1\\1&0\end{bmatrix}$ be the RCF of $x+1$, then the only order $2$ matrices that commute with it are $-g$ and $-g^2=-I$, which form a Klein-$4$ group. This means that $C_2^3$ is not a subgroup. Also, if we get $g=\begin{bmatrix} 0&-1\\1&0\end{bmatrix}$ be the RCF of $x^2+1$, then the only order $2$ element that commutes with it is $-I=g^2$, so there is also no subgroup isomorphic to $C_4\times C_2$. In a previous exercise, I showed that $Q_8$ is not a subgroup of $GL_2(\mathbb R)$, and thus isn't a subgroup over $\mathbb Q$ either. But this doesn't seem to prevent subgroups of order $16$, since $D_8$ is in fact a subgroup. Checking all $14$ groups of order $16$, it seems that all of them have an order $8$ subgroup besides $D_8$, but is there a cleaner way to rule out groups of order $16$ without classifying them, since I'm apparently supposed to conclude that the order of $G$ divides $24=2^3\cdot 3$ from the fact that the Klein-$4$ group is the only noncyclic abelian subgroup. So now, if I assume as known that $24$ divides the order of $G$, then the possible orders for $G$ are $1, 2, 3, 4, 6, 8, 12$, and $24$. I can find $C_1, C_2, C_3, C_4, C_6, V_4, D_6, D_8,$ and $D_{12}$. This takes care of all the orders except $12$ and $24$. I can show that $C_{12}$ and $C_{6}\times C_2$ are not subgroups, but I'm not sure how to rule out the nonabelian groups of order 12, $A_4$ and $C_3\rtimes C_4$, or the groups of order $24$. So in summary, I'm a bit stuck in ruling out the nonabelian groups of order $12, 16,$ and $24$. Is there a more elegant way to do this than to look at the classifications of these groups and find subgroups which I have already shown to be impossible?",,"['linear-algebra', 'group-theory']"
25,Topology on the general linear group of a topological vector space,Topology on the general linear group of a topological vector space,,"Let $K$ be a topological field . Let $V$ be a topological vector space over $K$ (if it makes things convenient, you may assume it is finite dimensional). Naive Question: Is there a canonical way of defining a topology on $\text{GL}(V)$? Attempted Focusing of Naive Question: Let $\mathcal{C}$ be the subcategory of $\mathsf{TVect}_K$ with the same objects (topological $K$-vector spaces) but where the morphisms are just the isomorphisms from $\mathsf{TVect}_K$. Can we define a functor $A:\mathcal{C}\rightarrow\mathsf{TGrp}$ such  that $B(A(V))=\text{GL}(V)$ for all $V\in \mathcal{C}$, where $B:\mathsf{TGrp}\rightarrow\mathsf{Grp}$ is the forgetful functor Let $X=K^d$ with the standard (product) topology. Then $A(X)\cong\!\! \text{GL}_d(K)$, where $\text{GL}_d(K)$ is given the subspace topology from $K^{d^2}$ Let $Y=K^d$ with the trivial (indiscrete) topology. Then $A(Y)\cong\!\!\text{GL}_d(K)$, where $\text{GL}_d(K)$ is given the trivial topology In short, I want to avoid ""silly"" answers, like $A(V)=\text{GL}(V)$ with the trivial topology for all $V$. I'm not sure if my above conditions will sufficiently rule out that kind of thing, but if you see a ""silly"" answer, I encourage you to post it so that either I can hone my question better, or I can see why there is no well-defined question to ask here. Motivation: In my differential topology class today, there was a lot of debate about what the topology on the Grassmanian $\text{Gr}(r,V)$  was (for $V$ an $\mathbb{R}$-vector space). The professor ultimately gave what was, in my opinion, an unaesthetic answer that depended on choosing a basis for $V$ (which I avoid when possible) and, now that $V$ and $\mathbb{R}^n$ are identified via the choice of basis, using the inner product structure on $\mathbb{R}^n$ (which I also avoid when possible) to define a metric, and hence topology, on the set of $r$-dimensional subspaces. In particular, I saw no reason there should fail to be a topology on $\text{Gr}(r,V)$ in the absence of an inner product structure on $V$, so my motivation here is to define a canonical topology on $\text{Gr}(r,V)$ for any topological vector space $V$. Looking at the Wikipedia page on Grassmanians , it seems the natural way of going about this would just be to have a topology on $\text{GL}(V)$, and then put the quotient topology on $\text{Gr}(r,V)=\text{GL}(V)/H$ where $H=\text{Stab}(W)$ for some $r$-dimensional subspace $W\subset V$. This raised the question of what exactly the topology was on $\text{GL}(V)$, which is what I'm asking here now.","Let $K$ be a topological field . Let $V$ be a topological vector space over $K$ (if it makes things convenient, you may assume it is finite dimensional). Naive Question: Is there a canonical way of defining a topology on $\text{GL}(V)$? Attempted Focusing of Naive Question: Let $\mathcal{C}$ be the subcategory of $\mathsf{TVect}_K$ with the same objects (topological $K$-vector spaces) but where the morphisms are just the isomorphisms from $\mathsf{TVect}_K$. Can we define a functor $A:\mathcal{C}\rightarrow\mathsf{TGrp}$ such  that $B(A(V))=\text{GL}(V)$ for all $V\in \mathcal{C}$, where $B:\mathsf{TGrp}\rightarrow\mathsf{Grp}$ is the forgetful functor Let $X=K^d$ with the standard (product) topology. Then $A(X)\cong\!\! \text{GL}_d(K)$, where $\text{GL}_d(K)$ is given the subspace topology from $K^{d^2}$ Let $Y=K^d$ with the trivial (indiscrete) topology. Then $A(Y)\cong\!\!\text{GL}_d(K)$, where $\text{GL}_d(K)$ is given the trivial topology In short, I want to avoid ""silly"" answers, like $A(V)=\text{GL}(V)$ with the trivial topology for all $V$. I'm not sure if my above conditions will sufficiently rule out that kind of thing, but if you see a ""silly"" answer, I encourage you to post it so that either I can hone my question better, or I can see why there is no well-defined question to ask here. Motivation: In my differential topology class today, there was a lot of debate about what the topology on the Grassmanian $\text{Gr}(r,V)$  was (for $V$ an $\mathbb{R}$-vector space). The professor ultimately gave what was, in my opinion, an unaesthetic answer that depended on choosing a basis for $V$ (which I avoid when possible) and, now that $V$ and $\mathbb{R}^n$ are identified via the choice of basis, using the inner product structure on $\mathbb{R}^n$ (which I also avoid when possible) to define a metric, and hence topology, on the set of $r$-dimensional subspaces. In particular, I saw no reason there should fail to be a topology on $\text{Gr}(r,V)$ in the absence of an inner product structure on $V$, so my motivation here is to define a canonical topology on $\text{Gr}(r,V)$ for any topological vector space $V$. Looking at the Wikipedia page on Grassmanians , it seems the natural way of going about this would just be to have a topology on $\text{GL}(V)$, and then put the quotient topology on $\text{Gr}(r,V)=\text{GL}(V)/H$ where $H=\text{Stab}(W)$ for some $r$-dimensional subspace $W\subset V$. This raised the question of what exactly the topology was on $\text{GL}(V)$, which is what I'm asking here now.",,"['linear-algebra', 'topological-vector-spaces', 'topological-groups', 'grassmannian']"
26,Matrices with a certain property,Matrices with a certain property,,"Suppose $A$ is a $n\times n$ matrix having the following properties $A$ is invertible all the entries in $A$ are either $-1$ , $0$ , or $1$ if a column has more than one non-zero entry, then the non-zero entries are strictly below the diagonal Suppose $A^{-1}$ has the same properties. Does it follow that every row and column in $A$ must contain exactly one entry which is $1$ or $-1$ ?","Suppose is a matrix having the following properties is invertible all the entries in are either , , or if a column has more than one non-zero entry, then the non-zero entries are strictly below the diagonal Suppose has the same properties. Does it follow that every row and column in must contain exactly one entry which is or ?",A n\times n A A -1 0 1 A^{-1} A 1 -1,"['linear-algebra', 'matrices', 'functional-analysis']"
27,Dimension of the vector space of homogeneous polynomials,Dimension of the vector space of homogeneous polynomials,,"Let $k[X_0, X_1, \ldots, X_n]_d$, or briefly $k[X]_d$, be the $k$-vector space whose elements are the zero polynomial and homogeneous polynomials of degree $d\geq 1$. I found the following formula for the dimension $$dim(k[X]_d)= \binom{n+d} {n}$$ but in my book there is no justification for this equality. Could someone explain me, possibly in an intuitive way, why that binomial coefficient is the dimension of that vector space?","Let $k[X_0, X_1, \ldots, X_n]_d$, or briefly $k[X]_d$, be the $k$-vector space whose elements are the zero polynomial and homogeneous polynomials of degree $d\geq 1$. I found the following formula for the dimension $$dim(k[X]_d)= \binom{n+d} {n}$$ but in my book there is no justification for this equality. Could someone explain me, possibly in an intuitive way, why that binomial coefficient is the dimension of that vector space?",,"['linear-algebra', 'polynomials', 'vector-spaces']"
28,Is there a sense in which the Chi-squared distribution is an inner product?,Is there a sense in which the Chi-squared distribution is an inner product?,,"I have been self-studying statistics recently, and the apparent similarities between linear algebra (especially Hilbert spaces ) and statistics have been popping out to me. Linear independence gets replaced with statistical independence . The mean is like an origin to which random variables can be translated to change from an affine space into a vector space . The covariance is basically just an inner product . At first I thought that these similarities might be coincidental, but after a lot of research, it became clear that they are not. There appears to be quite a lot of literature on this topic, but I have not yet studied this relationship very closely and am in the process of finding a textbook to read. That said, based on the research that I have done so far, there is still one connection to vectors that has not been discussed anywhere I've looked: the Chi-Squared distribution . Given the analogies described above, it seems obvious that the analogy for independent , standard normal random variables of the form $$X=\frac{x-\mu_x}{\sigma_x}, \ \ Y=\frac{x-\mu_y}{\sigma_y}, \text{ etc.} $$ would be orthogonal unit vectors . With this understanding, one might construct a ""random variable vector"" ( $\chi$ ) of the form $${\large \chi}(a,b,c) = aX+bY+cZ= \pmatrix{a \\ b \\ c} $$ where $a$, $b$, and $c$ are constant, real-valued, and non-random. This vector is itself a normally distributed random variable with mean $\mu_\chi=a\mu_x+b\mu_y+c\mu_z=0$ and variance $\sigma_\chi^2=a^2\sigma_x^2+b^2\sigma_y^2+c^2\sigma_z^2=a^2+b^2+c^2$. So far, it is obeying all of the analogical rules established above. That would lead naturally to the Chi-squared distribution $${\large \chi^2} =\pmatrix{1 & 1 & 1} \pmatrix{1 \\ 1 \\ 1}=X^2+Y^2+Z^2 $$ which is not a ""vector"" (as expected). However, it is not clear to me how to interpret its meaning - especially its mean (in this case, $\mu_{\chi^2}=3$) or variance (in this case, $\sigma_{\chi^2}^2=6$). So I have 3 inter-related questions: 1) Does it make sense to interpret independent, standard normal random variables as being akin to ""orthogonal unit vectors""? 2) If it does, is there any meaning or utility in constructing a ""random variable vector"" as described above? If there is not, is there another way to construct a random variable vector that is meaningful? 3) Finally, is there any sense in which the Chi-squared distribution can be seen as an inner product between a ""random variable vector"" and itself? Can that inner product be generalized to be between different vectors? Can it be used to construct a ""norm"" on this space (say, with the Chi distribution )? If so, what exactly does that mean? Note: I know that this is a question from ignorance built on possibly tenuous analogies. Since I am self-studying, I can't help but draw connections between this new material and the sort of math that I am already comfortable with. If there is any way that I can improve this question, please let me know.","I have been self-studying statistics recently, and the apparent similarities between linear algebra (especially Hilbert spaces ) and statistics have been popping out to me. Linear independence gets replaced with statistical independence . The mean is like an origin to which random variables can be translated to change from an affine space into a vector space . The covariance is basically just an inner product . At first I thought that these similarities might be coincidental, but after a lot of research, it became clear that they are not. There appears to be quite a lot of literature on this topic, but I have not yet studied this relationship very closely and am in the process of finding a textbook to read. That said, based on the research that I have done so far, there is still one connection to vectors that has not been discussed anywhere I've looked: the Chi-Squared distribution . Given the analogies described above, it seems obvious that the analogy for independent , standard normal random variables of the form $$X=\frac{x-\mu_x}{\sigma_x}, \ \ Y=\frac{x-\mu_y}{\sigma_y}, \text{ etc.} $$ would be orthogonal unit vectors . With this understanding, one might construct a ""random variable vector"" ( $\chi$ ) of the form $${\large \chi}(a,b,c) = aX+bY+cZ= \pmatrix{a \\ b \\ c} $$ where $a$, $b$, and $c$ are constant, real-valued, and non-random. This vector is itself a normally distributed random variable with mean $\mu_\chi=a\mu_x+b\mu_y+c\mu_z=0$ and variance $\sigma_\chi^2=a^2\sigma_x^2+b^2\sigma_y^2+c^2\sigma_z^2=a^2+b^2+c^2$. So far, it is obeying all of the analogical rules established above. That would lead naturally to the Chi-squared distribution $${\large \chi^2} =\pmatrix{1 & 1 & 1} \pmatrix{1 \\ 1 \\ 1}=X^2+Y^2+Z^2 $$ which is not a ""vector"" (as expected). However, it is not clear to me how to interpret its meaning - especially its mean (in this case, $\mu_{\chi^2}=3$) or variance (in this case, $\sigma_{\chi^2}^2=6$). So I have 3 inter-related questions: 1) Does it make sense to interpret independent, standard normal random variables as being akin to ""orthogonal unit vectors""? 2) If it does, is there any meaning or utility in constructing a ""random variable vector"" as described above? If there is not, is there another way to construct a random variable vector that is meaningful? 3) Finally, is there any sense in which the Chi-squared distribution can be seen as an inner product between a ""random variable vector"" and itself? Can that inner product be generalized to be between different vectors? Can it be used to construct a ""norm"" on this space (say, with the Chi distribution )? If so, what exactly does that mean? Note: I know that this is a question from ignorance built on possibly tenuous analogies. Since I am self-studying, I can't help but draw connections between this new material and the sort of math that I am already comfortable with. If there is any way that I can improve this question, please let me know.",,"['linear-algebra', 'statistics', 'random-variables', 'hilbert-spaces', 'self-learning']"
29,Connection between the spectra of a family of matrices and a modelization of particles' scattering?,Connection between the spectra of a family of matrices and a modelization of particles' scattering?,,"In the excellent book ""Numerical Computing with MATLAB"" by Cleve B. Moler (SIAM 2004), [Moler is the ""father"" of Matlab], one finds, on pages 298-299, the following graphical representation (fig. 10.12 ; I have reconstructed it with minor changes ; Matlab program below) with the following explanations (personal adaptation of the text): Let $n$ be a fixed integer $>1$ . Let $t\in (0,1)$ ; let $A_t$ be the $n \times n$ matrix with entries $$A_{t,i,j}=\dfrac{1}{i-j+t}$$ Consider Fig. 1, gathering the spectra of matrices $A_t$ , for $n=11$ and $0.1 \leq t \leq 0.9$ with steps $\Delta t=0.005$ . Figure 1. Interpretation : the 11 rightmost points correspond to the spectrum of matrix $A_t$ for $t=0.1$ . There is a striking similarity with particles' scattering by a nucleus situated at the origin, with hyperbolic trajectories ; see for example this reference . My question is : How matrices $A_t$ can be connected to a model of particle scattering ? My attempts (mostly unsuccessful) : Consider $A_t$ as a Cauchy matrix ( $x_i=i$ , $y_j=j-t$ ) (notations of https://en.wikipedia.org/wiki/Cauchy_matrix )  with its different properties, in particular displacement equation. See as well the answers to this question . Connect $A_t$ with a quadratic form defined by its moment's matrix $\int_C z^{i}\bar{z}^{j}z^{t-1}dz$ , $z^{t-1}$ playing the rôle of a weight function. But for which curve $C$ ? Prove that the eigenvalues are situated on hyperbolas. Make many simulations (see figures below). Matlab program for Fig. 1 : n=11;  [I,J]=meshgrid(1:n,1:n);  E=[];  for t=0.1:0.005:0.9    A=1./(I-J+t);    E=[E,eig(A)];  end;  plot(E,'.k');axis equal Addendum (November 22, 2018) : Here is a second figure that provides a ""bigger"" view, with $n=20$ and $0.005 \leq t \leq 0.995$ . The eigenvalues corresponding to $t=0.005$ are grouped into the rightmost big red blob on the right, to $t=0.995$ are blue filled dots (quasi-circle with radius $\approx 110$ ). Figure 2 : [enlarged version of Fig. 1 ; case $n=15$ ] Everything happens as if a planar wave enters at $t=0$ from the right, is slowed down by  nucleus' repulsion, then scattered as a circular wave... Fig. 3 : Two cases are gathered here, both for $t=0.995$ : $n=51$ (empty circles) and $n=100$ (stars). One can note that the eigenvalues are very close to the $(n+1)$ -th roots of unity. For this value of $t$ , the  radii  are given by experimental formula : $R_n=200(1-12.4/n+212/n^2-3110/n^3)$ . I am grateful to @AmbretteOrrisey who did very interesting remarks, in particular by giving the following polar representation for the trajectory of particles : [citation follows ; more details can be found in his/her answer] ""The polar equation of the trajectory of a particle being deflected by a point charge is $$r=\frac{2a^2}{\sqrt{b^2+4a^2}\sin\theta -b}\tag{1}$$ where $a$ is the impact parameter which is the closest approach to the nucleus were the path undeviated ; $b$ is the closest approach of a head-on ( $a=0$ ) particle with repulsion operating."" Figure 4 displays a reconstruction result that takes into account the fact that (n+1)th roots of unity give asymptotic directions. Figure 4 : Case $n=201$ . An approximate reconstruction of $21$ among the $n$ trajectories (Matlab program below ; polar equation - adapted from (1) can be seen on line 5). Please note the ""ad hoc"" coefficient $3.0592$ ... n=201;  for k=1:20:n     d=pi*k/(2*(n+1));c=cos(d);     t=-d+0.01:0.01:d-0.01;     r=3.0592*(1-c)*exp(i*(t-d))./(cos(t)-c);      plot(r);plot(conj(r));  end; Slightly related : http://bdpi.usp.br/bitstream/handle/BDPI/35181/wos2012-3198.pdf I mention here a book gathering publications of Steve Moler [Milestones in Matrix Computation] ( https://global.oup.com/academic/product/milestones-in-matrix-computation-9780199206810?cc=fr&lang=en& ) and an article mentionning the analycity of the obtain curves : ( https://www.math.upenn.edu/~kazdan/504/eigenv.pdf )","In the excellent book ""Numerical Computing with MATLAB"" by Cleve B. Moler (SIAM 2004), [Moler is the ""father"" of Matlab], one finds, on pages 298-299, the following graphical representation (fig. 10.12 ; I have reconstructed it with minor changes ; Matlab program below) with the following explanations (personal adaptation of the text): Let be a fixed integer . Let ; let be the matrix with entries Consider Fig. 1, gathering the spectra of matrices , for and with steps . Figure 1. Interpretation : the 11 rightmost points correspond to the spectrum of matrix for . There is a striking similarity with particles' scattering by a nucleus situated at the origin, with hyperbolic trajectories ; see for example this reference . My question is : How matrices can be connected to a model of particle scattering ? My attempts (mostly unsuccessful) : Consider as a Cauchy matrix ( , ) (notations of https://en.wikipedia.org/wiki/Cauchy_matrix )  with its different properties, in particular displacement equation. See as well the answers to this question . Connect with a quadratic form defined by its moment's matrix , playing the rôle of a weight function. But for which curve ? Prove that the eigenvalues are situated on hyperbolas. Make many simulations (see figures below). Matlab program for Fig. 1 : n=11;  [I,J]=meshgrid(1:n,1:n);  E=[];  for t=0.1:0.005:0.9    A=1./(I-J+t);    E=[E,eig(A)];  end;  plot(E,'.k');axis equal Addendum (November 22, 2018) : Here is a second figure that provides a ""bigger"" view, with and . The eigenvalues corresponding to are grouped into the rightmost big red blob on the right, to are blue filled dots (quasi-circle with radius ). Figure 2 : [enlarged version of Fig. 1 ; case ] Everything happens as if a planar wave enters at from the right, is slowed down by  nucleus' repulsion, then scattered as a circular wave... Fig. 3 : Two cases are gathered here, both for : (empty circles) and (stars). One can note that the eigenvalues are very close to the -th roots of unity. For this value of , the  radii  are given by experimental formula : . I am grateful to @AmbretteOrrisey who did very interesting remarks, in particular by giving the following polar representation for the trajectory of particles : [citation follows ; more details can be found in his/her answer] ""The polar equation of the trajectory of a particle being deflected by a point charge is where is the impact parameter which is the closest approach to the nucleus were the path undeviated ; is the closest approach of a head-on ( ) particle with repulsion operating."" Figure 4 displays a reconstruction result that takes into account the fact that (n+1)th roots of unity give asymptotic directions. Figure 4 : Case . An approximate reconstruction of among the trajectories (Matlab program below ; polar equation - adapted from (1) can be seen on line 5). Please note the ""ad hoc"" coefficient ... n=201;  for k=1:20:n     d=pi*k/(2*(n+1));c=cos(d);     t=-d+0.01:0.01:d-0.01;     r=3.0592*(1-c)*exp(i*(t-d))./(cos(t)-c);      plot(r);plot(conj(r));  end; Slightly related : http://bdpi.usp.br/bitstream/handle/BDPI/35181/wos2012-3198.pdf I mention here a book gathering publications of Steve Moler [Milestones in Matrix Computation] ( https://global.oup.com/academic/product/milestones-in-matrix-computation-9780199206810?cc=fr&lang=en& ) and an article mentionning the analycity of the obtain curves : ( https://www.math.upenn.edu/~kazdan/504/eigenv.pdf )","n >1 t\in (0,1) A_t n \times n A_{t,i,j}=\dfrac{1}{i-j+t} A_t n=11 0.1 \leq t \leq 0.9 \Delta t=0.005 A_t t=0.1 A_t A_t x_i=i y_j=j-t A_t \int_C z^{i}\bar{z}^{j}z^{t-1}dz z^{t-1} C n=20 0.005 \leq t \leq 0.995 t=0.005 t=0.995 \approx 110 n=15 t=0 t=0.995 n=51 n=100 (n+1) t R_n=200(1-12.4/n+212/n^2-3110/n^3) r=\frac{2a^2}{\sqrt{b^2+4a^2}\sin\theta -b}\tag{1} a b a=0 n=201 21 n 3.0592","['linear-algebra', 'matrices', 'eigenvalues-eigenvectors', 'mathematical-physics', 'cauchy-matrices']"
30,Is the closure $\overline{ \{X \in \mathbb{R}^{m \times n} : \rho(M-NX) < 1\} }$ equal to $ \{X \in \mathbb{R}^{m \times n} : \rho(M-NX) \le 1\}$,Is the closure  equal to,\overline{ \{X \in \mathbb{R}^{m \times n} : \rho(M-NX) < 1\} }  \{X \in \mathbb{R}^{m \times n} : \rho(M-NX) \le 1\},"Suppose $M \in \mathcal M(n \times n; \mathbb R)$ and $N \in \mathcal M(n \times m; \mathbb R)$ are fixed with $N \neq 0$. Let \begin{align*}   E =  \{X \in \mathcal{M}(m \times n; \mathbb R) : \rho(M-NX) < 1\}, \end{align*} where $\rho(\cdot)$ denotes the spectral radius of a matrix. I want to know whether the closure $\bar{E}$ of $E$ is equal to $$F = \{X \in \mathcal{M}(m \times n; \mathbb R) : \rho(M-NX) \le 1\}.$$ We will assume $E$ is not empty. If we define following composition of continuous maps \begin{align*} f: X \mapsto M-NX \mapsto (\lambda_1(M-NX), \dots, \lambda_n(M-NX)) \mapsto (|\lambda_1(M-NX)|, \dots, |\lambda_n(M-NX)|)  \\ \mapsto \max( |\lambda_1(M-NX)|, \dots, |\lambda_n(M-NX)|). \end{align*} Then $f$ is continuous into $[0, \infty)$. We note $E = \{X: f^{-1}([0,1)\}$ and $F = \{X: f^{-1}([0,1])\}$. So $E$ is open and $F$ is closed. Clearly $\bar{E} \subset F$. But I could not show the other direction (or possibly $\bar{E}$ is a proper subset of $F$). I tried to construct a sequence $\{X_n\}$ converging to $X \in F \setminus E$ by multiplying a factor $1-\varepsilon$ to $X$ but apparently to conclude $\bar{E} = F$ we need some kind of sublinearatily of spectral radius which is not true in general.","Suppose $M \in \mathcal M(n \times n; \mathbb R)$ and $N \in \mathcal M(n \times m; \mathbb R)$ are fixed with $N \neq 0$. Let \begin{align*}   E =  \{X \in \mathcal{M}(m \times n; \mathbb R) : \rho(M-NX) < 1\}, \end{align*} where $\rho(\cdot)$ denotes the spectral radius of a matrix. I want to know whether the closure $\bar{E}$ of $E$ is equal to $$F = \{X \in \mathcal{M}(m \times n; \mathbb R) : \rho(M-NX) \le 1\}.$$ We will assume $E$ is not empty. If we define following composition of continuous maps \begin{align*} f: X \mapsto M-NX \mapsto (\lambda_1(M-NX), \dots, \lambda_n(M-NX)) \mapsto (|\lambda_1(M-NX)|, \dots, |\lambda_n(M-NX)|)  \\ \mapsto \max( |\lambda_1(M-NX)|, \dots, |\lambda_n(M-NX)|). \end{align*} Then $f$ is continuous into $[0, \infty)$. We note $E = \{X: f^{-1}([0,1)\}$ and $F = \{X: f^{-1}([0,1])\}$. So $E$ is open and $F$ is closed. Clearly $\bar{E} \subset F$. But I could not show the other direction (or possibly $\bar{E}$ is a proper subset of $F$). I tried to construct a sequence $\{X_n\}$ converging to $X \in F \setminus E$ by multiplying a factor $1-\varepsilon$ to $X$ but apparently to conclude $\bar{E} = F$ we need some kind of sublinearatily of spectral radius which is not true in general.",,"['linear-algebra', 'general-topology', 'spectral-radius']"
31,Reference suggestion: eigenvalues of tridiagonal matrices,Reference suggestion: eigenvalues of tridiagonal matrices,,"I would like to ask for a reference on the problem of computing the eigenvalues/eigenvectors of tridiagonal matrices (not necessarily with constant diagonals). I have seen authors use continued fractions and generating functions. However, I have thus far been unable to really grasp the foundations of this idea. From what I can see, the idea is really to reduce it to a difference equations. Then, perhaps, my request is for a good book on difference equations.  Moreover, is there any technique which is really of a broad scope; ie applicable to a broad range of problems. Thank you all in advance, Gabieel","I would like to ask for a reference on the problem of computing the eigenvalues/eigenvectors of tridiagonal matrices (not necessarily with constant diagonals). I have seen authors use continued fractions and generating functions. However, I have thus far been unable to really grasp the foundations of this idea. From what I can see, the idea is really to reduce it to a difference equations. Then, perhaps, my request is for a good book on difference equations.  Moreover, is there any technique which is really of a broad scope; ie applicable to a broad range of problems. Thank you all in advance, Gabieel",,"['linear-algebra', 'reference-request', 'soft-question', 'eigenvalues-eigenvectors']"
32,Is this matrix decomposition possible?,Is this matrix decomposition possible?,,"Given a $2\times2$ matrix $S$ with entries in $\mathbb{Z}$ or $\mathbb{Q}$ , when is it possible to write $S=\frac{1}{3}(ABC+CAB+BCA)$ such that $A+B+C=0$, where $A, B, C$ are matrices over the same ground ring as S. Always? How would I find $A, B, C$?","Given a $2\times2$ matrix $S$ with entries in $\mathbb{Z}$ or $\mathbb{Q}$ , when is it possible to write $S=\frac{1}{3}(ABC+CAB+BCA)$ such that $A+B+C=0$, where $A, B, C$ are matrices over the same ground ring as S. Always? How would I find $A, B, C$?",,"['linear-algebra', 'matrices', 'ring-theory']"
33,A puzzling KKT for LMI vs. scalar constraint,A puzzling KKT for LMI vs. scalar constraint,,"I am trying to understand the KKT conditions for LMI constraints in order to solve my original question in KKT conditions for $\max \log \det(X)$ with LMI constraints . In the meantime, I found a much simpler problem that does not go through when extending the KKT conditions from scalar case to the vector case. The problem is \begin{align} & \max_{X\succeq0} \log \det(I + B XB^T)\\ \\ & s.t.  \begin{pmatrix} AXA^T - X + Q & AXB^T \\ BXA^T& I + BXB^T \end{pmatrix}\succeq0, \end{align} and the goal is to show $(A - K(X^\ast)B)(A-K(X^\ast)B )^T\prec I$ where $K(X)\triangleq AXB^T(I + BXB^T)^{-1}$ . This is an important consequence in control theory since it implies that the optimal solution $X^*$ is the stabilizing solution for the corresponding system. From here, I elaborate on my modest progress. The constraints can be either written as a ""big LMI"": \begin{align} R(X)&=\begin{pmatrix} AXA^T - X + Q & AXB^T &0\\ BXA^T& I + BXB^T &0 \\ 0&0&X \end{pmatrix}\succeq 0 \end{align} or \begin{align} AXA^T-X+Q - AXB^T(I + BXB^T)^{-1}BXA^T\succeq0\\\ X\succeq0, \end{align} where we used the Schur complement along with $I+BXB^T\succ0$ . The scalar case (solution): By the KKT stationarity condition \begin{align}\label{eq:lagra_1} 0&= -B^2 -\lambda_1+ \lambda_2\{ 1 - A^2 + 2 A^2B^2X(I + BXB^T)^{-1} - A^2B^4X^2(I + BXB^T)^{-2}\} \end{align} for $\lambda_1,\lambda_2\ge0$ and correspond to the constraints. If $B\neq0$ , it follows that $\lambda_2>0$ and \begin{align} 0&<1 - A^2 + 2 A^2B^2X^\ast(I + BX^\ast B^T)^{-1} - A^2B^4X^{2\ast}(I + BX^\ast B^T)^{-2}\\   &= 1 - (A - K(X^\ast)B)^2 \end{align} where $K(X)$ is defined above. If $B=0$ , the stability condition holds only if $|A|<1$ . The combination of this conditions lead to the following known result: there exists a stabilizing solution $X$ iff $(A,B)$ is detectable. The vector case using the Schur complement constraint (unsolved): I could make the following progress, but I am not sure how to complete the proof. The Lagrangian in the vector case is: \begin{align} L(X,\Lambda_1,\Lambda_2)=  - \log \det(I + B XB^T) - \text{Tr}(X\Lambda_1) - \text{Tr}((AXA^T-X+Q - AXB^T(I + BXB^T)^{-1}BXA^T) \Lambda_2). \end{align} From the stationarity condition, (is the derivative correct?) \begin{align} 0&=\frac{\partial L(X,\Lambda_1,\Lambda_2)}{\partial X}\\ &= - (I+BX B^T)^{-1}B^TB - \Lambda_1 \\  &\ \ + (I - A^TA + A^TKB + B^TK^TA - B^TK^TKB) \Lambda_2 \end{align} with $\Lambda_1,\Lambda_2\succeq0$ . By the primal feasibility constraint $X\succeq0$ . Thus, first summand is positive semidefinite so that \begin{align} (I - (A-KB)^T(A-KB))\Lambda_2 \succeq 0 \end{align} Is the conclusion correct since the product may not be symmetric The complementary slackness conditions read as: \begin{align} 0&= \Lambda_1X\\ 0&= (AXA^T-X+Q - AXB^T(I + BXB^T)^{-1}BXA^T))\Lambda_2\\  &= [(A-KB)X(A-KB)^T - X + Q + KK^T ]\Lambda_2\\ \end{align} I do not know how to proceed from here, but I try to enlighten when I am aiming to arrive. A necessary condition for the existence of stabilizing solution is detectability, i.e., if $Ax = \lambda x$ for a vector $x$ and $|\lambda|\ge1$ then $Bx\neq0$ . Let's try to show this fact using contradiction: Assume there exists a vector $x\neq0$ such that $Ax = \lambda x$ and $Bx=0$ with $|\lambda|\ge1$ . We can now pre- and post-multiplying the stationarity condition with $x^T$ and $x$ and have \begin{align} 0&= -x^T\Lambda_1x - x^T(I - (A-KB)^T(A-KB))\Lambda_2x \end{align} The vector case using the big LMI: Without loss of generality, the dual variable is \begin{align} Z=\begin{pmatrix} S & U&0\\ U^T & T&0\\ 0&0&W \end{pmatrix}. \end{align} The Lagrangian in this case is: \begin{align} L(X,Z)&=  - \log \det(I + B XB^T) - \text{Tr}(R(X)Z). \end{align} The KKT stationarity condition gives: \begin{align} 0&= - \text{det}(I+BX B^T)\text{Tr}((I+BX B^T)^{-1}BB^T) \\& \ \ -\text{Tr}( ASA^T - S + B^TU^TA + A^TUB + B^TTB + W ) \end{align} and the complementary slackness condition $R(X)Z=0$ simplifies to: \begin{align} 0&= (AXA^T - X + Q)S + AXB^T U^T\\ 0&= (AXA^T - X + Q)U + AXB^T T\\ 0&= BXA^T S + (I+BXB^T)U^T\\ 0&= BXA^T U + (I+BXB^T)T\\ 0&= XW \end{align} If $B=I$ , it follows that $S\succ0$ (Assume $Sx=0$ for some $x$ and conclude that $x=0$ ). For the general case, $Sx=0$ implies $Bx=0  \& \& Wx=0$ .","I am trying to understand the KKT conditions for LMI constraints in order to solve my original question in KKT conditions for $\max \log \det(X)$ with LMI constraints . In the meantime, I found a much simpler problem that does not go through when extending the KKT conditions from scalar case to the vector case. The problem is and the goal is to show where . This is an important consequence in control theory since it implies that the optimal solution is the stabilizing solution for the corresponding system. From here, I elaborate on my modest progress. The constraints can be either written as a ""big LMI"": or where we used the Schur complement along with . The scalar case (solution): By the KKT stationarity condition for and correspond to the constraints. If , it follows that and where is defined above. If , the stability condition holds only if . The combination of this conditions lead to the following known result: there exists a stabilizing solution iff is detectable. The vector case using the Schur complement constraint (unsolved): I could make the following progress, but I am not sure how to complete the proof. The Lagrangian in the vector case is: From the stationarity condition, (is the derivative correct?) with . By the primal feasibility constraint . Thus, first summand is positive semidefinite so that Is the conclusion correct since the product may not be symmetric The complementary slackness conditions read as: I do not know how to proceed from here, but I try to enlighten when I am aiming to arrive. A necessary condition for the existence of stabilizing solution is detectability, i.e., if for a vector and then . Let's try to show this fact using contradiction: Assume there exists a vector such that and with . We can now pre- and post-multiplying the stationarity condition with and and have The vector case using the big LMI: Without loss of generality, the dual variable is The Lagrangian in this case is: The KKT stationarity condition gives: and the complementary slackness condition simplifies to: If , it follows that (Assume for some and conclude that ). For the general case, implies .","\begin{align}
& \max_{X\succeq0} \log \det(I + B XB^T)\\ \\
& s.t. 
\begin{pmatrix} AXA^T - X + Q & AXB^T \\
BXA^T& I + BXB^T
\end{pmatrix}\succeq0,
\end{align} (A - K(X^\ast)B)(A-K(X^\ast)B )^T\prec I K(X)\triangleq AXB^T(I + BXB^T)^{-1} X^* \begin{align}
R(X)&=\begin{pmatrix} AXA^T - X + Q & AXB^T &0\\
BXA^T& I + BXB^T &0 \\
0&0&X
\end{pmatrix}\succeq 0
\end{align} \begin{align}
AXA^T-X+Q - AXB^T(I + BXB^T)^{-1}BXA^T\succeq0\\\
X\succeq0,
\end{align} I+BXB^T\succ0 \begin{align}\label{eq:lagra_1}
0&= -B^2 -\lambda_1+ \lambda_2\{ 1 - A^2 + 2 A^2B^2X(I + BXB^T)^{-1} - A^2B^4X^2(I + BXB^T)^{-2}\}
\end{align} \lambda_1,\lambda_2\ge0 B\neq0 \lambda_2>0 \begin{align}
0&<1 - A^2 + 2 A^2B^2X^\ast(I + BX^\ast B^T)^{-1} - A^2B^4X^{2\ast}(I + BX^\ast B^T)^{-2}\\
  &= 1 - (A - K(X^\ast)B)^2
\end{align} K(X) B=0 |A|<1 X (A,B) \begin{align}
L(X,\Lambda_1,\Lambda_2)=  - \log \det(I + B XB^T) - \text{Tr}(X\Lambda_1) - \text{Tr}((AXA^T-X+Q - AXB^T(I + BXB^T)^{-1}BXA^T) \Lambda_2).
\end{align} \begin{align}
0&=\frac{\partial L(X,\Lambda_1,\Lambda_2)}{\partial X}\\
&= - (I+BX B^T)^{-1}B^TB - \Lambda_1 \\ 
&\ \ + (I - A^TA + A^TKB + B^TK^TA - B^TK^TKB) \Lambda_2
\end{align} \Lambda_1,\Lambda_2\succeq0 X\succeq0 \begin{align}
(I - (A-KB)^T(A-KB))\Lambda_2 \succeq 0
\end{align} \begin{align}
0&= \Lambda_1X\\
0&= (AXA^T-X+Q - AXB^T(I + BXB^T)^{-1}BXA^T))\Lambda_2\\
 &= [(A-KB)X(A-KB)^T - X + Q + KK^T ]\Lambda_2\\
\end{align} Ax = \lambda x x |\lambda|\ge1 Bx\neq0 x\neq0 Ax = \lambda x Bx=0 |\lambda|\ge1 x^T x \begin{align}
0&= -x^T\Lambda_1x - x^T(I - (A-KB)^T(A-KB))\Lambda_2x
\end{align} \begin{align}
Z=\begin{pmatrix}
S & U&0\\
U^T & T&0\\
0&0&W
\end{pmatrix}.
\end{align} \begin{align}
L(X,Z)&=  - \log \det(I + B XB^T) - \text{Tr}(R(X)Z).
\end{align} \begin{align}
0&= - \text{det}(I+BX B^T)\text{Tr}((I+BX B^T)^{-1}BB^T) \\&
\ \ -\text{Tr}( ASA^T - S + B^TU^TA + A^TUB + B^TTB + W )
\end{align} R(X)Z=0 \begin{align}
0&= (AXA^T - X + Q)S + AXB^T U^T\\
0&= (AXA^T - X + Q)U + AXB^T T\\
0&= BXA^T S + (I+BXB^T)U^T\\
0&= BXA^T U + (I+BXB^T)T\\
0&= XW
\end{align} B=I S\succ0 Sx=0 x x=0 Sx=0 Bx=0  \& \& Wx=0","['linear-algebra', 'optimization', 'convex-optimization', 'karush-kuhn-tucker', 'linear-matrix-inequality']"
34,Scalar triple product - why equivalent to determinant?,Scalar triple product - why equivalent to determinant?,,I'm looking at the scalar triple product and I'm wondering: is there any demonstration (possibly a simple one) that $$ \mathbf{a} \cdot \left(\mathbf{b} \times \mathbf{c} \right)=   \begin{bmatrix}     a_1 & a_2 & a_3 \\     b_1 & b_2 & b_3 \\     c_1 & c_2 & c_3   \end{bmatrix} $$ These two things seem totally unrelated to me.,I'm looking at the scalar triple product and I'm wondering: is there any demonstration (possibly a simple one) that $$ \mathbf{a} \cdot \left(\mathbf{b} \times \mathbf{c} \right)=   \begin{bmatrix}     a_1 & a_2 & a_3 \\     b_1 & b_2 & b_3 \\     c_1 & c_2 & c_3   \end{bmatrix} $$ These two things seem totally unrelated to me.,,"['linear-algebra', 'matrices', 'vectors', 'determinant', 'cross-product']"
35,If two matrices have the same characteristic polynomial then do they have the same determinant?,If two matrices have the same characteristic polynomial then do they have the same determinant?,,"There is a similar question here , but it's asking that if two matrices have the same characteristic polynomial then are they similar. If the answer were positive then the answer to my question will also be positive, but it's not. Now, if two matrices have the same characteristic polynomial then they are of the same order. Suppose $A$ and $B$ are matrices of order 2 and they both have a characteristic polynomial of $t^2+at+b$ . We know that $t^2+at+b=t^2-\text{trace}(A)t+\text{det}(A)=t^2-\text{trace}(B)t+\text{det}(B)$ , so in this case the answer to my question is affirmative. However, I don't think the answer is ""yes"" in general because then I probably will have a theorem which says this in my book. I looked for counterexamples but couldn't find any. Basically, I made up random matrices and never got a counterexample. So if the answer to the question is ""no"", can you please explain how you arrived at the counterexample.","There is a similar question here , but it's asking that if two matrices have the same characteristic polynomial then are they similar. If the answer were positive then the answer to my question will also be positive, but it's not. Now, if two matrices have the same characteristic polynomial then they are of the same order. Suppose and are matrices of order 2 and they both have a characteristic polynomial of . We know that , so in this case the answer to my question is affirmative. However, I don't think the answer is ""yes"" in general because then I probably will have a theorem which says this in my book. I looked for counterexamples but couldn't find any. Basically, I made up random matrices and never got a counterexample. So if the answer to the question is ""no"", can you please explain how you arrived at the counterexample.",A B t^2+at+b t^2+at+b=t^2-\text{trace}(A)t+\text{det}(A)=t^2-\text{trace}(B)t+\text{det}(B),"['linear-algebra', 'matrices', 'determinant', 'trace', 'characteristic-polynomial']"
36,Does cross product have an identity?,Does cross product have an identity?,,Does cross product have an identity?  I.e. Does there exist some  $\vec{id}\in \mathbb{R}^3$ such that  $$\vec{id} \times \vec{v} = \vec{v}\times \vec{id} = \vec{v} $$  for all $\vec{v}\in \mathbb{R}^3$?,Does cross product have an identity?  I.e. Does there exist some  $\vec{id}\in \mathbb{R}^3$ such that  $$\vec{id} \times \vec{v} = \vec{v}\times \vec{id} = \vec{v} $$  for all $\vec{v}\in \mathbb{R}^3$?,,"['linear-algebra', 'vectors']"
37,Why doesn't the definition of dependence require that one can expresses each vector in terms of the others?,Why doesn't the definition of dependence require that one can expresses each vector in terms of the others?,,"I was reviewing my foundations on linear algebra and realized that I am confused about independence and dependence. I understand that by definition independence means: A set of vectors $\{x_1,\ldots,x_k\}$ is independent if the only linear combination that gives the zero vector is the zero vector itself. i.e. if $[x_1, \ldots, x_k]c = Xc = 0$ iff $c=0$ I understand what the definition says but it sort of goes against my intuition of what the definition of dependence should be (and hence its negation independence ). In my head intuitively dependence means that the a set of vectors depends on each other. In other word one should always be able to express one vector as a linear combination of the others. Something like: $$ \forall x_i \in \{x_1,\ldots,x_k\}, \exists c \neq 0 : \sum_{j \neq i} c_j x_j = x_i$$ however with my definition above (which is wrong and is not what the standard definition is, I know but I am trying to come to terms why its wrong) implies that a set of independent vectors with the zero vector tacked on is not dependent (i.e. independent) which is the opposite of what is should be. i.e. tacking the zero vector and the set remains independent (this should be wrong cuz [0,...,0,1] is not the zero vector and only the zero vector should give 0). Consider for a simple example $ \{ x_1,x_2,0 \}$ where $x_1,x_1$ only give zero with the zero vector (standard definition of independence). With my definition of things its obvious that these vectors are independent. In reality they should be dependent because [0,0,1] is now in the nullspace but things are only independent if only the zero vector is in the nullspace. With my definition the vectors are independent because there is no way to express any of them in terms of each other. For example: $a x_1 + b x_2 = 0$ $c x_1 + d 0 = x_2$ $e x_2 + f 0 = x_1$ non of the above can be made true with non zero (non-trivial) linear combinations. Thus, the vectors are not dependent so they are independent. I know its sort of an ""edge case"" condition for the definition but it sort of flipped my world to find out that I've been thinking about such a fundamental concept like independence and dependence wrongly in linear algebra and I'm trying to come to terms with it. Why is my intuition incorrect? Why was the standard definition of independence as $Xc = 0 \iff c=0$ the accepted definition of independence? Whats wrong with my definition? Are they essentially the same definition except for this weird edge case? last footnote is about what the word dependence means with respect to the number and vector zero. I think what my last confusion boils down to is why $0x = \mathbf{0}$ is considered as $\mathbf{0}$ depending on $x$. I guess in my head saying that we don't need any of $x$ to express $\mathbf{0}$ seems to mean that $\mathbf{0}$ doesn't need $x$ (or any other vector). But the convention according to everything pointed out by everyone in these set of answers points out to the opposite. I don't understand why. Is it that just having an equation linking terms means dependence even if we specify with a zero that we don't actually need the term?","I was reviewing my foundations on linear algebra and realized that I am confused about independence and dependence. I understand that by definition independence means: A set of vectors $\{x_1,\ldots,x_k\}$ is independent if the only linear combination that gives the zero vector is the zero vector itself. i.e. if $[x_1, \ldots, x_k]c = Xc = 0$ iff $c=0$ I understand what the definition says but it sort of goes against my intuition of what the definition of dependence should be (and hence its negation independence ). In my head intuitively dependence means that the a set of vectors depends on each other. In other word one should always be able to express one vector as a linear combination of the others. Something like: $$ \forall x_i \in \{x_1,\ldots,x_k\}, \exists c \neq 0 : \sum_{j \neq i} c_j x_j = x_i$$ however with my definition above (which is wrong and is not what the standard definition is, I know but I am trying to come to terms why its wrong) implies that a set of independent vectors with the zero vector tacked on is not dependent (i.e. independent) which is the opposite of what is should be. i.e. tacking the zero vector and the set remains independent (this should be wrong cuz [0,...,0,1] is not the zero vector and only the zero vector should give 0). Consider for a simple example $ \{ x_1,x_2,0 \}$ where $x_1,x_1$ only give zero with the zero vector (standard definition of independence). With my definition of things its obvious that these vectors are independent. In reality they should be dependent because [0,0,1] is now in the nullspace but things are only independent if only the zero vector is in the nullspace. With my definition the vectors are independent because there is no way to express any of them in terms of each other. For example: $a x_1 + b x_2 = 0$ $c x_1 + d 0 = x_2$ $e x_2 + f 0 = x_1$ non of the above can be made true with non zero (non-trivial) linear combinations. Thus, the vectors are not dependent so they are independent. I know its sort of an ""edge case"" condition for the definition but it sort of flipped my world to find out that I've been thinking about such a fundamental concept like independence and dependence wrongly in linear algebra and I'm trying to come to terms with it. Why is my intuition incorrect? Why was the standard definition of independence as $Xc = 0 \iff c=0$ the accepted definition of independence? Whats wrong with my definition? Are they essentially the same definition except for this weird edge case? last footnote is about what the word dependence means with respect to the number and vector zero. I think what my last confusion boils down to is why $0x = \mathbf{0}$ is considered as $\mathbf{0}$ depending on $x$. I guess in my head saying that we don't need any of $x$ to express $\mathbf{0}$ seems to mean that $\mathbf{0}$ doesn't need $x$ (or any other vector). But the convention according to everything pointed out by everyone in these set of answers points out to the opposite. I don't understand why. Is it that just having an equation linking terms means dependence even if we specify with a zero that we don't actually need the term?",,"['linear-algebra', 'linear-transformations', 'intuition']"
38,$I-AB$ be invertible $\Leftrightarrow$ $I-BA$ is invertible [duplicate],be invertible   is invertible [duplicate],I-AB \Leftrightarrow I-BA,"This question already has answers here : $I_m - AB$ is invertible if and only if $I_n - BA$ is invertible. (4 answers) Closed 11 years ago . assume $A,B\in M_n(F)$ if  $I-AB$ be  invertible then  how  to prove $I-BA$ is invertible and how find inverse of $I-BA$ Thanks in advance","This question already has answers here : $I_m - AB$ is invertible if and only if $I_n - BA$ is invertible. (4 answers) Closed 11 years ago . assume $A,B\in M_n(F)$ if  $I-AB$ be  invertible then  how  to prove $I-BA$ is invertible and how find inverse of $I-BA$ Thanks in advance",,"['linear-algebra', 'matrices']"
39,Check if $\det(I + S) = 1 + \operatorname{trace}(S)$ holds ?,Check if  holds ?,\det(I + S) = 1 + \operatorname{trace}(S),"I saw the following statement in my homework and we are asked to make use of the statement: If $S$ is a symmetric matrix then $$\det(I + S ) = 1 + \operatorname{trace}(S).$$ However, I am not sure if this really holds. Please give me a hand here please","I saw the following statement in my homework and we are asked to make use of the statement: If $S$ is a symmetric matrix then $$\det(I + S ) = 1 + \operatorname{trace}(S).$$ However, I am not sure if this really holds. Please give me a hand here please",,"['linear-algebra', 'matrices', 'determinant', 'trace']"
40,"Counterexamples for ""every linear map on an infinite dimensional complex vector space has an eigenvalue""","Counterexamples for ""every linear map on an infinite dimensional complex vector space has an eigenvalue""",,"Every linear map on a finite dimensional complex vector space has an eigenvalue. Not so in the infinite case. I'm interested in nice counterexamples anyone might have. Here's one: Consider the vector space $\mathbb C^\infty$ of sequences and the right shift map $R$ defined by $$R(a_1, a_2, a_3, ...) = (0, a_1, a_2, a_3, ...)$$ $R$ has no eigenvalue (using the usual convention that there must be a non-trivial eigenvector).","Every linear map on a finite dimensional complex vector space has an eigenvalue. Not so in the infinite case. I'm interested in nice counterexamples anyone might have. Here's one: Consider the vector space $\mathbb C^\infty$ of sequences and the right shift map $R$ defined by $$R(a_1, a_2, a_3, ...) = (0, a_1, a_2, a_3, ...)$$ $R$ has no eigenvalue (using the usual convention that there must be a non-trivial eigenvector).",,"['linear-algebra', 'examples-counterexamples', 'big-list']"
41,Is adjoint of singular matrix singular? What would be its rank?,Is adjoint of singular matrix singular? What would be its rank?,,Let $A$ be a square and singular matrix of order $n$ . Is $\operatorname{adj}(A)$ necessarily singular? What would be the rank of $\operatorname{adj}(A)$ ?,Let be a square and singular matrix of order . Is necessarily singular? What would be the rank of ?,A n \operatorname{adj}(A) \operatorname{adj}(A),"['linear-algebra', 'matrices', 'determinant']"
42,Linear Algebra: Preserving the null space,Linear Algebra: Preserving the null space,,What does it mean when a book says that row operations preserve the null space? And why should that be true? I have read that row operations are equivalent to multiplying a vector on the left by an invertible elementary matrix. And I think I understand that the nullspace is the set of all vectors from $u \in U$ which get mapped to the zero vector in $V$ if $T:U\rightarrow V$ is linear. But I'm still not sure what this means.,What does it mean when a book says that row operations preserve the null space? And why should that be true? I have read that row operations are equivalent to multiplying a vector on the left by an invertible elementary matrix. And I think I understand that the nullspace is the set of all vectors from $u \in U$ which get mapped to the zero vector in $V$ if $T:U\rightarrow V$ is linear. But I'm still not sure what this means.,,['linear-algebra']
43,If $A$ is normal and upper triangular then it is diagonal,If  is normal and upper triangular then it is diagonal,A,"Let $A$ be a normal matrix in Mat$_{n\times n}(\mathbb C)$, if $A$ is upper triangular then it is diagonal (Normal means $AA^*=A^*A$, where $A^*$ is the conjugate transpose of $A$) If I consider the diagonal of $AA^*$,  let denote $(a_{ij})=A$ and $(â_{ij})_{i,j}=AA^*$ then, since $AA^*=A^*A$ $â_{ii}=\sum\limits_{k=1}^na_{ik}\overline{a}_{ik}=\sum\limits_{k=1}^n\overline{a_{ki}}{a}_{ki}$ $\implies\sum\limits_{k=1}^n|a_{ik}|^2=\sum\limits_{k=1}^n|a_{ki}|^2$. If I take $i=n$ then it follows that $a_{in}=0, \forall 1\le i\le n-1$ and continuing in this manner the upper diagonal entries are zero, Is this correct ? Can I show it in another way, because in a previous exercise I had to show that ''If A is normal and nilpotent then $A=0$'' so using this can I decompose $A$ into diagonal and nilpotent matrix, then show that the nilpotent part is zero ?","Let $A$ be a normal matrix in Mat$_{n\times n}(\mathbb C)$, if $A$ is upper triangular then it is diagonal (Normal means $AA^*=A^*A$, where $A^*$ is the conjugate transpose of $A$) If I consider the diagonal of $AA^*$,  let denote $(a_{ij})=A$ and $(â_{ij})_{i,j}=AA^*$ then, since $AA^*=A^*A$ $â_{ii}=\sum\limits_{k=1}^na_{ik}\overline{a}_{ik}=\sum\limits_{k=1}^n\overline{a_{ki}}{a}_{ki}$ $\implies\sum\limits_{k=1}^n|a_{ik}|^2=\sum\limits_{k=1}^n|a_{ki}|^2$. If I take $i=n$ then it follows that $a_{in}=0, \forall 1\le i\le n-1$ and continuing in this manner the upper diagonal entries are zero, Is this correct ? Can I show it in another way, because in a previous exercise I had to show that ''If A is normal and nilpotent then $A=0$'' so using this can I decompose $A$ into diagonal and nilpotent matrix, then show that the nilpotent part is zero ?",,"['linear-algebra', 'matrices']"
44,Other guises for the vector space $\mathbb{R}^n$?,Other guises for the vector space ?,\mathbb{R}^n,"One way the vector space $\mathbb{R}^n$ can come up is as the space of polynomials over $\mathbb{R}$ of degree at most $(n-1)$ .  Here we have the isomorphism: $$(a_0,a_1,\ldots,a_{n-1}) \leftrightarrow a_0+a_1x+a_2x^2+\cdots+a_{n-1}x^{n-1}.$$  The linear independence of the polynomials in $\{1,x,x^2,\ldots,x^{n-1}\}$ is why it is legitimate to equate the coefficients of polynomials. Question : What are other guises for the vector space $\mathbb{R}^n$? I.e., what are some other interesting vector spaces that are isomorphic to $\mathbb{R}^n$?","One way the vector space $\mathbb{R}^n$ can come up is as the space of polynomials over $\mathbb{R}$ of degree at most $(n-1)$ .  Here we have the isomorphism: $$(a_0,a_1,\ldots,a_{n-1}) \leftrightarrow a_0+a_1x+a_2x^2+\cdots+a_{n-1}x^{n-1}.$$  The linear independence of the polynomials in $\{1,x,x^2,\ldots,x^{n-1}\}$ is why it is legitimate to equate the coefficients of polynomials. Question : What are other guises for the vector space $\mathbb{R}^n$? I.e., what are some other interesting vector spaces that are isomorphic to $\mathbb{R}^n$?",,"['linear-algebra', 'vector-spaces', 'examples-counterexamples', 'vector-space-isomorphism']"
45,"Proving that $\det(A) \ne 0$ if $a_{i,i} = 0$ and $a_{i,j} = \pm 1$ for $i \neq j$",Proving that  if  and  for,"\det(A) \ne 0 a_{i,i} = 0 a_{i,j} = \pm 1 i \neq j","Let $A$ be an $n\times n$ matrix ($n=2k$, $k \in \Bbb N^*$) such that. $$a_{ij} = \begin{cases} \pm 1,  & \text{if $i \ne j$} \\ 0, & \text{if $i=j$} \end{cases}$$ Show that $\det (A) \ne 0$. P.S. $a_{ij}=\pm 1$ means that it can be $+1$ or $-1$ not necessarily the same for all $a_{ij}$. My approach: I've started with the definition of $\det A$ writing like a permutation sum but it became messy. I also tried Laplace's method but also didn't work. I also tried induction but once $\pm 1$ is aleatory it became tough to deal with.","Let $A$ be an $n\times n$ matrix ($n=2k$, $k \in \Bbb N^*$) such that. $$a_{ij} = \begin{cases} \pm 1,  & \text{if $i \ne j$} \\ 0, & \text{if $i=j$} \end{cases}$$ Show that $\det (A) \ne 0$. P.S. $a_{ij}=\pm 1$ means that it can be $+1$ or $-1$ not necessarily the same for all $a_{ij}$. My approach: I've started with the definition of $\det A$ writing like a permutation sum but it became messy. I also tried Laplace's method but also didn't work. I also tried induction but once $\pm 1$ is aleatory it became tough to deal with.",,"['linear-algebra', 'matrices', 'determinant']"
46,Determining a matrix from its characteristic polynomial,Determining a matrix from its characteristic polynomial,,"Let $A\in\mathcal{M}_{n}(K)$ , where $K$ is a field. Then, we can obtain the characteristic polynomial of $A$ by simply taking $p(\lambda)=\det(A-\lambda I_n)$ , which give us something like $$p(\lambda) = (-1)^n\lambda^n + (-1)^{n-1}(\text{tr } A)\lambda^{n-1} +  \cdots + \det A$$ Now, how can we obtain the matrix $A$ knowing the characteristic polynomial?","Let , where is a field. Then, we can obtain the characteristic polynomial of by simply taking , which give us something like Now, how can we obtain the matrix knowing the characteristic polynomial?",A\in\mathcal{M}_{n}(K) K A p(\lambda)=\det(A-\lambda I_n) p(\lambda) = (-1)^n\lambda^n + (-1)^{n-1}(\text{tr } A)\lambda^{n-1} +  \cdots + \det A A,"['linear-algebra', 'matrices']"
47,Derivative of Binary Cross Entropy - why are my signs not right?,Derivative of Binary Cross Entropy - why are my signs not right?,,"I'm trying to derive formulas used in backpropagation for a neural network that uses a binary cross entropy loss function.  When I perform the differentiation, however, my signs do not come out right: Binary cross entropy loss function: $$J(\hat y) = \frac{-1}{m}\sum_{i=1}^m y_i\log(\hat y_i)+(1-y_i)(\log(1-\hat y)$$ where $m = $ number of training examples $y = $ true y value $\hat y = $ predicted y value When I attempt to differentiate this for one training example, I do the following process: Product rule: $$ \frac{dJ}{d\hat y_i} = -1(\frac{d}{d\hat y_i}(y_i\log(\hat y_i)+(1-y_i)(\log(1-\hat y)))) $$ Sum rule: $$ = -1(\frac{d}{d\hat y_i}y_i\log(\hat y_i)+\frac{d}{d\hat y_i}(1-y_i)(\log(1-\hat y))) $$ Product rule, deriv of constant (treating $y$ as a constant) and deriv of natural log: $$ = -1(\frac{y_i}{\hat y_i} + \frac{1-y_i}{1 - \hat y_i})$$ However, this is different from the expected result: $$ \frac{dJ}{d\hat y_i} = -1(\frac{y_i}{\hat y_i} - \frac{1-y_i}{1 - \hat y_i}) $$ Not sure what's going wrong.  I'm sure I'm doing something incorrectly, but I can't figure out what it is.  Any help is appreciated!","I'm trying to derive formulas used in backpropagation for a neural network that uses a binary cross entropy loss function.  When I perform the differentiation, however, my signs do not come out right: Binary cross entropy loss function: $$J(\hat y) = \frac{-1}{m}\sum_{i=1}^m y_i\log(\hat y_i)+(1-y_i)(\log(1-\hat y)$$ where $m = $ number of training examples $y = $ true y value $\hat y = $ predicted y value When I attempt to differentiate this for one training example, I do the following process: Product rule: $$ \frac{dJ}{d\hat y_i} = -1(\frac{d}{d\hat y_i}(y_i\log(\hat y_i)+(1-y_i)(\log(1-\hat y)))) $$ Sum rule: $$ = -1(\frac{d}{d\hat y_i}y_i\log(\hat y_i)+\frac{d}{d\hat y_i}(1-y_i)(\log(1-\hat y))) $$ Product rule, deriv of constant (treating $y$ as a constant) and deriv of natural log: $$ = -1(\frac{y_i}{\hat y_i} + \frac{1-y_i}{1 - \hat y_i})$$ However, this is different from the expected result: $$ \frac{dJ}{d\hat y_i} = -1(\frac{y_i}{\hat y_i} - \frac{1-y_i}{1 - \hat y_i}) $$ Not sure what's going wrong.  I'm sure I'm doing something incorrectly, but I can't figure out what it is.  Any help is appreciated!",,"['linear-algebra', 'derivatives', 'machine-learning']"
48,Is $\mathbb{R}$ a vector space over $\mathbb{C}$?,Is  a vector space over ?,\mathbb{R} \mathbb{C},"Here is a problem so beautiful that I had to share it. I found it in Paul Halmos's autobiography. Everyone knows that $\mathbb{C}$ is a vector space over $\mathbb{R}$, but what about the other way around? Problem: Prove or disprove: $\mathbb{R}$ can be written as vector space over $\mathbb{C}$ Of course, we would like for $\mathbb{R}$ to retain its structure as an additive group.","Here is a problem so beautiful that I had to share it. I found it in Paul Halmos's autobiography. Everyone knows that $\mathbb{C}$ is a vector space over $\mathbb{R}$, but what about the other way around? Problem: Prove or disprove: $\mathbb{R}$ can be written as vector space over $\mathbb{C}$ Of course, we would like for $\mathbb{R}$ to retain its structure as an additive group.",,['linear-algebra']
49,Analyze the symmetric property of positive definite matrices [duplicate],Analyze the symmetric property of positive definite matrices [duplicate],,"This question already has answers here : Do positive semidefinite matrices have to be symmetric? (3 answers) Closed 6 years ago . In the definition, a positive definite matrix is usually referred to symmetric expressed in quadratic form. So I am confused about is it always symmetric? Why do they refer to the symmetric property in its definition? Please give me some examples and proof of this problem.","This question already has answers here : Do positive semidefinite matrices have to be symmetric? (3 answers) Closed 6 years ago . In the definition, a positive definite matrix is usually referred to symmetric expressed in quadratic form. So I am confused about is it always symmetric? Why do they refer to the symmetric property in its definition? Please give me some examples and proof of this problem.",,"['linear-algebra', 'matrices', 'positive-definite', 'symmetric-matrices']"
50,The definition of direct sums and subspaces?,The definition of direct sums and subspaces?,,"From what I have read the definition of the direct sum of the vector spaces $V_1$, $V_2$ is the set $V_1\times V_2$ with the operations of addition and scalar multiplication defined as follows (Knapp, 2006): \[(u_1,u_2)+(v_1,v_2)=(u_1+v_1,u_2+v_2)\] \[c(v_1,v_2)=(cv_1,cv_2)\] But then today I came across the fact that if $V$ has linear subspaces $V_1$, $V_2$ and every $v\in V$ can be written uniquly as: $$v=v_1+v_2$$ for $v_i \in V_i$ then $V=V_1\oplus V_2$. I cannot see how this makes sense given the definition above. Is the above definition wrong? If not how can $V$ have the form $(v_1,v_2)$ when $v_1,v_2 \in V$?","From what I have read the definition of the direct sum of the vector spaces $V_1$, $V_2$ is the set $V_1\times V_2$ with the operations of addition and scalar multiplication defined as follows (Knapp, 2006): \[(u_1,u_2)+(v_1,v_2)=(u_1+v_1,u_2+v_2)\] \[c(v_1,v_2)=(cv_1,cv_2)\] But then today I came across the fact that if $V$ has linear subspaces $V_1$, $V_2$ and every $v\in V$ can be written uniquly as: $$v=v_1+v_2$$ for $v_i \in V_i$ then $V=V_1\oplus V_2$. I cannot see how this makes sense given the definition above. Is the above definition wrong? If not how can $V$ have the form $(v_1,v_2)$ when $v_1,v_2 \in V$?",,"['linear-algebra', 'direct-sum']"
51,What is the inverse of the $\mbox{vec}$ operator?,What is the inverse of the  operator?,\mbox{vec},"There is a well known vectorization operator $\mbox{vec}$ in matrix analysis. I've vectorized my matrix equations, did some transformation of vectorized  equations and now I want to get back to the matrix form. Is there special operator for it?","There is a well known vectorization operator $\mbox{vec}$ in matrix analysis. I've vectorized my matrix equations, did some transformation of vectorized  equations and now I want to get back to the matrix form. Is there special operator for it?",,"['linear-algebra', 'matrices', 'terminology', 'matrix-equations', 'vectorization']"
52,Prove that the 23 people have the same weight.,Prove that the 23 people have the same weight.,,"I know that it must be an old question, and I have encountered it before, in MAT 1993.2 (Oxford admission test). But I dimly remember the solution. The question is Twenty-three people, each with integral weight, decide to play football, separating into two teams of 11 people each, plus a referee. To keep things fair, the teams chosen must have equal total weight. It turns out that no matter who is chosen to be the referee, this can always be done. Prove that the 23 people have the same weight.","I know that it must be an old question, and I have encountered it before, in MAT 1993.2 (Oxford admission test). But I dimly remember the solution. The question is Twenty-three people, each with integral weight, decide to play football, separating into two teams of 11 people each, plus a referee. To keep things fair, the teams chosen must have equal total weight. It turns out that no matter who is chosen to be the referee, this can always be done. Prove that the 23 people have the same weight.",,"['linear-algebra', 'combinatorics', 'pigeonhole-principle']"
53,Inverse of a symmetric positive definite matrix,Inverse of a symmetric positive definite matrix,,"If a matrix is symmetric and positive definite, determine if it is invertible and if its inverse matrix is symmetric and positive definite. I know that ""if a matrix is symmetric and positive definite, then its inverse matrix is also positive definite"", based on a theorem. But I am not sure how to prove that the matrix even is invertible or that its inverse matrix is also symmetric. It would really help if someone explained this a bit. Thanks","If a matrix is symmetric and positive definite, determine if it is invertible and if its inverse matrix is symmetric and positive definite. I know that ""if a matrix is symmetric and positive definite, then its inverse matrix is also positive definite"", based on a theorem. But I am not sure how to prove that the matrix even is invertible or that its inverse matrix is also symmetric. It would really help if someone explained this a bit. Thanks",,"['linear-algebra', 'matrices', 'positive-definite', 'symmetric-matrices']"
54,Showing that $n$ exponential functions are linearly independent.,Showing that  exponential functions are linearly independent.,n,"I have $n$ lambdas, which are all different real and positive numbers, where:  $\lambda_1 < \lambda_2 < \cdots < \lambda_n$. I then have to show that these functions are linearly independent: $$e^{\lambda_1 t}, e^{\lambda_2 t}, \ldots, e^{\lambda_n t}$$ So I guess what I want to show is that the only solution to this equations: $$c_1 e^{\lambda_1 t} + c_2 e^{\lambda_2 t}+ \ldots +c_n e^{\lambda_n t}=0, \quad \text{for all } t$$ is that all constants are zero. I am not entirely sure how to do this when it is for $n$ lambdas - I did it earlier for just 3 lambdas, where I differentiated the function, so I have tried to use the same approach. I wanted to get $n$ equations with $n$ unknown, so I differentiated the function $n-1$ times, and then I chose to look at the case where $t = 0$, so I have something looking like this: $$c_1+c_2+ \ldots +c_n = 0$$ $$c_1 \lambda_1+c_2 \lambda_2+ \ldots +c_n \lambda_n = 0$$ $$ \cdots $$ $$c_1 \lambda_1^{n-1}+c_2 \lambda_2^{n-1}+ \ldots +\lambda_n^{n-1} = 0$$ Then I put this into a matrix $Ac = 0$, and now I want to show that the $\det(A)$ doesn't equal zero, so that the only solution is c = 0, but I'm not quite sure how to do this, or whether there is an easier way to do it ?","I have $n$ lambdas, which are all different real and positive numbers, where:  $\lambda_1 < \lambda_2 < \cdots < \lambda_n$. I then have to show that these functions are linearly independent: $$e^{\lambda_1 t}, e^{\lambda_2 t}, \ldots, e^{\lambda_n t}$$ So I guess what I want to show is that the only solution to this equations: $$c_1 e^{\lambda_1 t} + c_2 e^{\lambda_2 t}+ \ldots +c_n e^{\lambda_n t}=0, \quad \text{for all } t$$ is that all constants are zero. I am not entirely sure how to do this when it is for $n$ lambdas - I did it earlier for just 3 lambdas, where I differentiated the function, so I have tried to use the same approach. I wanted to get $n$ equations with $n$ unknown, so I differentiated the function $n-1$ times, and then I chose to look at the case where $t = 0$, so I have something looking like this: $$c_1+c_2+ \ldots +c_n = 0$$ $$c_1 \lambda_1+c_2 \lambda_2+ \ldots +c_n \lambda_n = 0$$ $$ \cdots $$ $$c_1 \lambda_1^{n-1}+c_2 \lambda_2^{n-1}+ \ldots +\lambda_n^{n-1} = 0$$ Then I put this into a matrix $Ac = 0$, and now I want to show that the $\det(A)$ doesn't equal zero, so that the only solution is c = 0, but I'm not quite sure how to do this, or whether there is an easier way to do it ?",,"['linear-algebra', 'independence']"
55,Why does the Cholesky decomposition requires a positive definite matrix?,Why does the Cholesky decomposition requires a positive definite matrix?,,"Why does the Cholesky factorization requires the matrix A to be positive definite? What happens when we factorize non-positive definite matrix? Let's assume that we have a matrix A' that is not positive definite (so at least one leading principal minor is negative). Can one prove that there is no L such as A' = LL* ? If not, wouldn't the positive definite criteria remove some of the matrices that could be potentially decomposed? We could also put this question in the form of a demonstration for the next statement: For any square matrix L, the product LL* is a positive definite matrix.","Why does the Cholesky factorization requires the matrix A to be positive definite? What happens when we factorize non-positive definite matrix? Let's assume that we have a matrix A' that is not positive definite (so at least one leading principal minor is negative). Can one prove that there is no L such as A' = LL* ? If not, wouldn't the positive definite criteria remove some of the matrices that could be potentially decomposed? We could also put this question in the form of a demonstration for the next statement: For any square matrix L, the product LL* is a positive definite matrix.",,"['linear-algebra', 'matrices', 'matrix-decomposition', 'positive-definite', 'cholesky-decomposition']"
56,Derivative (or differential) of symmetric square root of a matrix,Derivative (or differential) of symmetric square root of a matrix,,"Let A be a square, symmetric, positive-definite matrix.  Let S be its symmetric square root found by a singular value decomposition.  Let vech() be the half-vectorization operator. Is there a convenient expression for the derivative (or differential) of vech(S) with respect to vech(A)? I know the expression for an inverse, which is sort of like a matrix version of the power rule.  Would this approach work for a symmetric square root as well (i.e., (1/2)S^(-1/2))?","Let A be a square, symmetric, positive-definite matrix.  Let S be its symmetric square root found by a singular value decomposition.  Let vech() be the half-vectorization operator. Is there a convenient expression for the derivative (or differential) of vech(S) with respect to vech(A)? I know the expression for an inverse, which is sort of like a matrix version of the power rule.  Would this approach work for a symmetric square root as well (i.e., (1/2)S^(-1/2))?",,"['linear-algebra', 'matrices', 'derivatives', 'svd']"
57,Best way of introducing determinants in a linear algebra course,Best way of introducing determinants in a linear algebra course,,What is the best way of introducing determinants in a linear algebra course? I want to give real life examples of where the determinant is applied. It should have a real impact.,What is the best way of introducing determinants in a linear algebra course? I want to give real life examples of where the determinant is applied. It should have a real impact.,,"['linear-algebra', 'determinant', 'education']"
58,Proof that the derivative is unique?,Proof that the derivative is unique?,,"Given a subset $\Omega$ of $\mathbb{R}^n$ and a function $\sigma: \Omega \to \mathbb{R}^n$ , we define its derivative at $x$ to be a linear operator $\sigma'(x)$ such that $$  \lim_{y \to 0} \frac{||\,\sigma(x + y) - \sigma(x) - \sigma'(x)y\,||}{||\,y\,||} = 0 $$ If this operator exists (i.e. if $\sigma$ is differentiable at $x$ ) then how do we prove that $\sigma'(x)$ is unique?","Given a subset of and a function , we define its derivative at to be a linear operator such that If this operator exists (i.e. if is differentiable at ) then how do we prove that is unique?","\Omega \mathbb{R}^n \sigma: \Omega \to \mathbb{R}^n x \sigma'(x)   \lim_{y \to 0} \frac{||\,\sigma(x + y) - \sigma(x) - \sigma'(x)y\,||}{||\,y\,||} = 0  \sigma x \sigma'(x)","['linear-algebra', 'multivariable-calculus', 'derivatives']"
59,"A normal, idempotent linear operator must be self-adjoint","A normal, idempotent linear operator must be self-adjoint",,"I have been trying to solve this problem for quite a while. I am still unsure of whether any of the avenues I have pursued have been of any use. Any advice will be much appreciated. Question: Let $V$ be a finite-dimensional inner product space, and let $E$ be an idempotent linear operator on $V$ . Prove that if $EE^* = E^*E$ , then $E$ is self-adjoint. (This is essentially exercise 5(a) in sec. 80 on p.162 of Paul R. Halmos, Finite-Dimensional Vector Spaces , but Halmos didn't assume that the dimension of $V$ is finite.)","I have been trying to solve this problem for quite a while. I am still unsure of whether any of the avenues I have pursued have been of any use. Any advice will be much appreciated. Question: Let be a finite-dimensional inner product space, and let be an idempotent linear operator on . Prove that if , then is self-adjoint. (This is essentially exercise 5(a) in sec. 80 on p.162 of Paul R. Halmos, Finite-Dimensional Vector Spaces , but Halmos didn't assume that the dimension of is finite.)",V E V EE^* = E^*E E V,['linear-algebra']
60,Express the following invertible matrix A as a product of elementary matrices,Express the following invertible matrix A as a product of elementary matrices,,"I've been at this for a while... I tried to the inverse method but it keeps on saying I'm getting it wrong... Can anyone show me a step-by-step solution? The matrix I have is a $3\times 3$ square one(sorry for formatting): $$ \begin{pmatrix} 6  & 6 & -2 \\ -1 & 0 & 0 \\ -1 & 1 & 0 \end{pmatrix} $$ I'm starting to go crazy, I honestly have a few pages of written work and the marker keeps on saying I got it wrong.. I guess it's too late to get my marks since I used all my attempts but I want to see how to do it for future reference.","I've been at this for a while... I tried to the inverse method but it keeps on saying I'm getting it wrong... Can anyone show me a step-by-step solution? The matrix I have is a $3\times 3$ square one(sorry for formatting): $$ \begin{pmatrix} 6  & 6 & -2 \\ -1 & 0 & 0 \\ -1 & 1 & 0 \end{pmatrix} $$ I'm starting to go crazy, I honestly have a few pages of written work and the marker keeps on saying I got it wrong.. I guess it's too late to get my marks since I used all my attempts but I want to see how to do it for future reference.",,['linear-algebra']
61,Matrix for rotation around a vector,Matrix for rotation around a vector,,"I'm trying to figure out the general form for the matrix (let's say in $\mathbb R^3$ for simplicity) of a rotation of $\theta$ around an arbitrary vector $v$ passing through the origin (look towards the origin and rotate counterclockwise). This is inspired by a similar problem which asked me to find the matrix for a rotation of $120^\circ$ around the vector $v=\begin{bmatrix}1&1&1\end{bmatrix}^\top$. However, in this case I was able to cheat a little since the transformation corresponds to a rotation of the vertices. So even though I found a solution, I'm not satisfied with my methodology. Is there a general form for rotation around an arbitrary vector in $\mathbb R^3$? A reference would be perfectly acceptable. Thanks.","I'm trying to figure out the general form for the matrix (let's say in $\mathbb R^3$ for simplicity) of a rotation of $\theta$ around an arbitrary vector $v$ passing through the origin (look towards the origin and rotate counterclockwise). This is inspired by a similar problem which asked me to find the matrix for a rotation of $120^\circ$ around the vector $v=\begin{bmatrix}1&1&1\end{bmatrix}^\top$. However, in this case I was able to cheat a little since the transformation corresponds to a rotation of the vertices. So even though I found a solution, I'm not satisfied with my methodology. Is there a general form for rotation around an arbitrary vector in $\mathbb R^3$? A reference would be perfectly acceptable. Thanks.",,"['linear-algebra', 'geometry', 'matrices', 'transformation']"
62,Do the algebraic and geometric multiplicities determine the minimal polynomial?,Do the algebraic and geometric multiplicities determine the minimal polynomial?,,"Let $T$ denote some linear transformation of a finite-dimensional space $V$ (say, over $\mathbb{C}$). Suppose we know the eigenvalues $\{\lambda_i\}_i$ and their associated algebraic multiplicities $\{d_i\}_i$ and geometric multiplicities $\{r_i\}_i$ of $T$, can we determine the minimal polynomial of $T$ via these informations? If the answer is no, is there a nice way to produce different linear transformations with same eigenvalues and associated algebraic and geometric multiplicities? Some backgraoud: It is well-known that for a given linear transformation, the minimal polynomial divides the characteristic polynomial: $m_T|p_T$. And I find in a paper proved that $$m_T|\prod_i(x-\lambda_i)^{d_i-r_i+1}\ ,\ \ \ \  p_T|m_T\prod_i(x-\lambda_i)^{r_i}$$  And then I want to know if there are any better results.","Let $T$ denote some linear transformation of a finite-dimensional space $V$ (say, over $\mathbb{C}$). Suppose we know the eigenvalues $\{\lambda_i\}_i$ and their associated algebraic multiplicities $\{d_i\}_i$ and geometric multiplicities $\{r_i\}_i$ of $T$, can we determine the minimal polynomial of $T$ via these informations? If the answer is no, is there a nice way to produce different linear transformations with same eigenvalues and associated algebraic and geometric multiplicities? Some backgraoud: It is well-known that for a given linear transformation, the minimal polynomial divides the characteristic polynomial: $m_T|p_T$. And I find in a paper proved that $$m_T|\prod_i(x-\lambda_i)^{d_i-r_i+1}\ ,\ \ \ \  p_T|m_T\prod_i(x-\lambda_i)^{r_i}$$  And then I want to know if there are any better results.",,['linear-algebra']
63,Why can a matrix without a full rank not be invertible?,Why can a matrix without a full rank not be invertible?,,"I know you could just say because the $\det = 0$. But during the introduction of determinants the professor said, obviously if two columns of the matrix are linearly dependent the matrix can't be inverted, therefore it is zero. He made it sound like it is an intuitive thing, a simple observation, but I always have to resort to the properties of determinants to show it. How does one trivially see that you can not invert a matrix without a full rank?","I know you could just say because the $\det = 0$. But during the introduction of determinants the professor said, obviously if two columns of the matrix are linearly dependent the matrix can't be inverted, therefore it is zero. He made it sound like it is an intuitive thing, a simple observation, but I always have to resort to the properties of determinants to show it. How does one trivially see that you can not invert a matrix without a full rank?",,"['linear-algebra', 'matrices', 'determinant']"
64,Are matrices always diagonalizable in the complex field?,Are matrices always diagonalizable in the complex field?,,"As the title says, considering the roots of the characteristic polynomial are the eigenvalues of the matrix, and in the complex field every polynomial has $n$ different roots where $n$ is the polynomial degree, this means every eigenvalue on the complex field is a simple eigenvalue, so every matrix in $\mathbb C$ will be diagonalizable. Am I wrong?","As the title says, considering the roots of the characteristic polynomial are the eigenvalues of the matrix, and in the complex field every polynomial has different roots where is the polynomial degree, this means every eigenvalue on the complex field is a simple eigenvalue, so every matrix in will be diagonalizable. Am I wrong?",n n \mathbb C,"['linear-algebra', 'polynomials', 'complex-numbers']"
65,Show that similar matrices have same trace,Show that similar matrices have same trace,,"If $A$ and $B$ are $n\times n$ matrices of a field $F$, then show that $\text{trace}(AB)=\text{trace}(BA)$. Hence show that similar matrices have the same trace. I've done the first part (proving that $AB$ and $BA$ have the same trace). I can show that here if you say so. But I'm stuck on the 'Hence show' part. Please give me some ideas.","If $A$ and $B$ are $n\times n$ matrices of a field $F$, then show that $\text{trace}(AB)=\text{trace}(BA)$. Hence show that similar matrices have the same trace. I've done the first part (proving that $AB$ and $BA$ have the same trace). I can show that here if you say so. But I'm stuck on the 'Hence show' part. Please give me some ideas.",,"['linear-algebra', 'matrices', 'trace']"
66,Prove that a symmetric matrix with a positive diagonal entry has at least one positive eigenvalue,Prove that a symmetric matrix with a positive diagonal entry has at least one positive eigenvalue,,"Let $A$ be a symmetric martix $n \times n$ such that there is some $i$ such that $a_{ii}>0$ . Prove that $A$ has a positive eigenvalue. I have a hint which I don't how to use/check: ""Check that $a_{ii}=e^t_i*A*e_i$ . Thanks, Alan","Let be a symmetric martix such that there is some such that . Prove that has a positive eigenvalue. I have a hint which I don't how to use/check: ""Check that . Thanks, Alan",A n \times n i a_{ii}>0 A a_{ii}=e^t_i*A*e_i,"['linear-algebra', 'matrices']"
67,Are two matrices having the same characteristic and minimal polynomial always similar?,Are two matrices having the same characteristic and minimal polynomial always similar?,,"If it is not true, can you provide a counter-example?","If it is not true, can you provide a counter-example?",,"['linear-algebra', 'matrices', 'minimal-polynomials']"
68,Proof of linear independence of $e^{at}$,Proof of linear independence of,e^{at},"Given $\left\{ a_{i}\right\} _{i=0}^{n}\subset\mathbb{R}$ which are distinct, show that $\left\{ e^{a_{i}t}\right\} \subset C^{0}\left(\mathbb{R},\mathbb{R}\right)$,  form a linearly independent set of functions. Any tips on how to go about this proof. I tried a working from the definition of an exponential and combining sums but that didn't seem to get me anywhere. I saw a tip on the internet that said write it in the form $\mu_{1}e^{a_{1}t}+\dots+\mu_{n}e^{a_{n}t}=0$ to try to show $\mu_{1}=\dots=\mu_{n}=0$ considering each term of the left hand side must be positive, but I can't get my head around that because while I understand $e^{x}>0\forall x\in\mathbb{R}$ I cannot see why $\mu_{i}$ must be positive in any case. I have thought about differentiating but that doesn't seem to help. The question did originally ask for a ""rigourous"" proof but I'll take any hints right now and the provided the solution of 'is obvious' is most unhelpful to me. Any input would be fantastic. Thank you.","Given $\left\{ a_{i}\right\} _{i=0}^{n}\subset\mathbb{R}$ which are distinct, show that $\left\{ e^{a_{i}t}\right\} \subset C^{0}\left(\mathbb{R},\mathbb{R}\right)$,  form a linearly independent set of functions. Any tips on how to go about this proof. I tried a working from the definition of an exponential and combining sums but that didn't seem to get me anywhere. I saw a tip on the internet that said write it in the form $\mu_{1}e^{a_{1}t}+\dots+\mu_{n}e^{a_{n}t}=0$ to try to show $\mu_{1}=\dots=\mu_{n}=0$ considering each term of the left hand side must be positive, but I can't get my head around that because while I understand $e^{x}>0\forall x\in\mathbb{R}$ I cannot see why $\mu_{i}$ must be positive in any case. I have thought about differentiating but that doesn't seem to help. The question did originally ask for a ""rigourous"" proof but I'll take any hints right now and the provided the solution of 'is obvious' is most unhelpful to me. Any input would be fantastic. Thank you.",,"['functions', 'linear-algebra']"
69,Signs in the natural map $\Lambda^k V \otimes \Lambda^k V^* \to \Bbbk$,Signs in the natural map,\Lambda^k V \otimes \Lambda^k V^* \to \Bbbk,"Let $V$ be a finite-dimensional vector space over a field $\Bbbk$.  Let $V^*$ denote its dual.  I strongly suspect that there is a natural map $$\Lambda^k V \otimes \Lambda^k V^* \to \Bbbk$$ that looks something like $$v_1 \wedge \dotsb \wedge v_k \otimes \alpha_1 \wedge \dotsb \wedge \alpha_k \mapsto \sum_{\sigma} {\operatorname{sgn} \, \sigma}\prod_i \alpha_i(v_{\sigma(i)}).$$ What does the correct, natural formula look like?  In particular, what is the correct sign convention?","Let $V$ be a finite-dimensional vector space over a field $\Bbbk$.  Let $V^*$ denote its dual.  I strongly suspect that there is a natural map $$\Lambda^k V \otimes \Lambda^k V^* \to \Bbbk$$ that looks something like $$v_1 \wedge \dotsb \wedge v_k \otimes \alpha_1 \wedge \dotsb \wedge \alpha_k \mapsto \sum_{\sigma} {\operatorname{sgn} \, \sigma}\prod_i \alpha_i(v_{\sigma(i)}).$$ What does the correct, natural formula look like?  In particular, what is the correct sign convention?",,"['linear-algebra', 'exterior-algebra', 'multilinear-algebra']"
70,If $A + A^t = 2I$ then $\det(A) \geq 1$,If  then,A + A^t = 2I \det(A) \geq 1,"Let $A$ be a $n \times n$ real matrix such that $$ A + A^t = 2I, $$ where $I$ is the $n \times n $ identity matrix. Prove that $\det(A) \geq 1$ . It is obvious that tr $(A) = n$ . Furthermore, we also have that $$A - 2I = -A^t, $$ so we get that $$\det(A-2I) = (-1)^n \cdot \det(A).$$ Now, we let $\lambda_1, \lambda_2, \cdots, \lambda_n \in \mathbb{C}$ be the  eigenvalues of $A$ , so we get that $$(\lambda_1 - 2)(\lambda_2 - 2) \cdots (\lambda_n - 2) = (-1)^n\lambda_1\lambda_2 \cdots \lambda_n, $$ but I couldn't derive anything about the product $\lambda_1\lambda_2 \cdots \lambda_n = \det(A)$ (the only known thing is $\lambda_1 + \cdots + \lambda_n = n$ ). Also, I tried the same approach for $2 \times 2$ and $3 \times 3 $ matrices, but it didn't lead to anything (especially since the eigenvalues can be complex numbers).","Let be a real matrix such that where is the identity matrix. Prove that . It is obvious that tr . Furthermore, we also have that so we get that Now, we let be the  eigenvalues of , so we get that but I couldn't derive anything about the product (the only known thing is ). Also, I tried the same approach for and matrices, but it didn't lead to anything (especially since the eigenvalues can be complex numbers).","A n \times n  A + A^t = 2I,  I n \times n  \det(A) \geq 1 (A) = n A - 2I = -A^t,  \det(A-2I) = (-1)^n \cdot \det(A). \lambda_1, \lambda_2, \cdots, \lambda_n \in \mathbb{C} A (\lambda_1 - 2)(\lambda_2 - 2) \cdots (\lambda_n - 2) = (-1)^n\lambda_1\lambda_2 \cdots \lambda_n,  \lambda_1\lambda_2 \cdots \lambda_n = \det(A) \lambda_1 + \cdots + \lambda_n = n 2 \times 2 3 \times 3 ","['linear-algebra', 'matrices', 'eigenvalues-eigenvectors']"
71,What is the difference between singular matrices and degenerate matrices?,What is the difference between singular matrices and degenerate matrices?,,"In my opinion, both are the matrices that the number of the directions of eigenvector is smaller than their size. So, I guess singular matrices and degenerate matrices refer to the same thing. Is that right?","In my opinion, both are the matrices that the number of the directions of eigenvector is smaller than their size. So, I guess singular matrices and degenerate matrices refer to the same thing. Is that right?",,"['linear-algebra', 'matrices']"
72,Vandermonde Determinant,Vandermonde Determinant,,"This is an exercise from Ian Stewart's Galois Theory , $3^{rd}$ edition: If $z_1,z_2,\ldots,z_n$ are distinct complex numbers, show that the determinant $$D=\left|\begin{array}[cccc]  11&1&\cdots&1\\ z_1&z_2&\cdots&z_n\\ z_1^2&z_2^2&\cdots&z_n^2\\ \vdots&\vdots&\ddots&\vdots\\ z_1^{n-1}&z_2^{n-1}&\cdots&z_n^{n-1} \end{array}\right|$$   is nonzero. Hint: Consider the $z_j$ as independent indeterminates over $\Bbb C$. Then $D$ is a polynomial in the $z_j$, of total degree $0+1+\cdots+(n-1)=\frac{1}{2}n(n-1)$. Moreover, $D$ vanishes whenever $z_j=z_k$ for $k\neq j$, as it then has two identical rows. Therefore $D$ is divisible by $z_j-z_k$ for all $j\neq k$, hence it is divisible by $\prod_{j<k}(z_j-z_k)$. Now compare degrees. My question is how to do this. I follow the hint and I'm thinking of $D$ as a cofactor expansion; is that the idea? Also, the fact that $D$ vanishes whenever $z_j=z_k$ for $k\neq j$ follows since it would have two identical columns, right? I don't see why $z_j=z_k$ would imply it has two identical rows, as the hint suggests. The fact that it is divisible by $(z_j-z_k)$ is easy since that would make $z_k$ a root for $k=j$, but I'm not quite sure I follow why the degrees would be different. I think the degree of the product would be $1+2+\cdots+(k-1)$, but then add these up for all $k<n$? I appreciate any clarification that you can provide!","This is an exercise from Ian Stewart's Galois Theory , $3^{rd}$ edition: If $z_1,z_2,\ldots,z_n$ are distinct complex numbers, show that the determinant $$D=\left|\begin{array}[cccc]  11&1&\cdots&1\\ z_1&z_2&\cdots&z_n\\ z_1^2&z_2^2&\cdots&z_n^2\\ \vdots&\vdots&\ddots&\vdots\\ z_1^{n-1}&z_2^{n-1}&\cdots&z_n^{n-1} \end{array}\right|$$   is nonzero. Hint: Consider the $z_j$ as independent indeterminates over $\Bbb C$. Then $D$ is a polynomial in the $z_j$, of total degree $0+1+\cdots+(n-1)=\frac{1}{2}n(n-1)$. Moreover, $D$ vanishes whenever $z_j=z_k$ for $k\neq j$, as it then has two identical rows. Therefore $D$ is divisible by $z_j-z_k$ for all $j\neq k$, hence it is divisible by $\prod_{j<k}(z_j-z_k)$. Now compare degrees. My question is how to do this. I follow the hint and I'm thinking of $D$ as a cofactor expansion; is that the idea? Also, the fact that $D$ vanishes whenever $z_j=z_k$ for $k\neq j$ follows since it would have two identical columns, right? I don't see why $z_j=z_k$ would imply it has two identical rows, as the hint suggests. The fact that it is divisible by $(z_j-z_k)$ is easy since that would make $z_k$ a root for $k=j$, but I'm not quite sure I follow why the degrees would be different. I think the degree of the product would be $1+2+\cdots+(k-1)$, but then add these up for all $k<n$? I appreciate any clarification that you can provide!",,"['linear-algebra', 'polynomials']"
73,Do four dimensional vectors have a cross product property? [duplicate],Do four dimensional vectors have a cross product property? [duplicate],,"This question already has answers here : Cross product in $\mathbb R^n$ (5 answers) Closed 10 years ago . We know how to make cross product of three dimensional vectors. $$ \vec A \times \vec B = \vec C$$ Where : $ \vec A = (A_i; A_j; A_k)$ $ \vec B = (B_i; B_j; B_k)$ $ \vec C = (C_i; C_j; C_k)$ $C_i = \left|\begin{matrix}A_j&A_k\\B_j&B_k\end{matrix}\right|$ $C_j = \left|\begin{matrix}A_k&A_i\\B_k&B_i\end{matrix}\right|$ $C_k = \left|\begin{matrix}A_i&A_j\\B_i&B_j\end{matrix}\right|$ But what about if we have four dimensional vectors? Is it possible to make cross product of four dimensional vectors? If it  is possible, then tell me when it can be possible? Let say we have two vectors: $ \vec A = (A_i; A_j; A_k; A_l)$ $ \vec B = (B_i; B_j; B_k; B_l)$ Then how to compute a cross product of this two vectors? Will it again vector? $$ \vec A \times \vec B = \vec C$$ $ \vec C = (C_i; C_j; C_k; C_l)$ Then how to compute those coordinates? We know that only square matrices have a determinant property! In this case it might not be correct if we will wright... $\color{red} {\text { $C_i = \left|\begin{matrix}A_j&A_k&A_l\\B_j&B_k&B_l\end{matrix}\right|$}    C_j = \left|\begin{matrix}A_k&A_i&A_l\\B_k&B_i&B_l\end{matrix}\right| C_k = \left|\begin{matrix}A_i&A_j&A_l\\B_i&B_j&B_l\end{matrix}\right| C_l = \left|\begin{matrix}A_i&A_j&A_k\\B_i&B_j&B_k\end{matrix}\right|}$ So tell me how to solve this problem?","This question already has answers here : Cross product in $\mathbb R^n$ (5 answers) Closed 10 years ago . We know how to make cross product of three dimensional vectors. $$ \vec A \times \vec B = \vec C$$ Where : $ \vec A = (A_i; A_j; A_k)$ $ \vec B = (B_i; B_j; B_k)$ $ \vec C = (C_i; C_j; C_k)$ $C_i = \left|\begin{matrix}A_j&A_k\\B_j&B_k\end{matrix}\right|$ $C_j = \left|\begin{matrix}A_k&A_i\\B_k&B_i\end{matrix}\right|$ $C_k = \left|\begin{matrix}A_i&A_j\\B_i&B_j\end{matrix}\right|$ But what about if we have four dimensional vectors? Is it possible to make cross product of four dimensional vectors? If it  is possible, then tell me when it can be possible? Let say we have two vectors: $ \vec A = (A_i; A_j; A_k; A_l)$ $ \vec B = (B_i; B_j; B_k; B_l)$ Then how to compute a cross product of this two vectors? Will it again vector? $$ \vec A \times \vec B = \vec C$$ $ \vec C = (C_i; C_j; C_k; C_l)$ Then how to compute those coordinates? We know that only square matrices have a determinant property! In this case it might not be correct if we will wright... $\color{red} {\text { $C_i = \left|\begin{matrix}A_j&A_k&A_l\\B_j&B_k&B_l\end{matrix}\right|$}    C_j = \left|\begin{matrix}A_k&A_i&A_l\\B_k&B_i&B_l\end{matrix}\right| C_k = \left|\begin{matrix}A_i&A_j&A_l\\B_i&B_j&B_l\end{matrix}\right| C_l = \left|\begin{matrix}A_i&A_j&A_k\\B_i&B_j&B_k\end{matrix}\right|}$ So tell me how to solve this problem?",,"['linear-algebra', 'geometry', 'vector-spaces', 'vectors', 'cross-product']"
74,Does every vector space contain a zero vector?,Does every vector space contain a zero vector?,,"If this is true, then every vector space must always have at least one subspace, the one consisting of only the zero vector, correct? Thanks!","If this is true, then every vector space must always have at least one subspace, the one consisting of only the zero vector, correct? Thanks!",,['linear-algebra']
75,Under which conditions is the spectral radius submultiplicative?,Under which conditions is the spectral radius submultiplicative?,,"Suppose $A,B \in M(n \times n, \mathbb{C})$ or $ A,B \in M(n \times n, \mathbb{R}) $ . Under which hypothesis can I state that: $\rho(AB) \leq \rho(A)\rho(B)$ ?",Suppose or . Under which hypothesis can I state that: ?,"A,B \in M(n \times n, \mathbb{C})  A,B \in M(n \times n, \mathbb{R})  \rho(AB) \leq \rho(A)\rho(B)","['linear-algebra', 'operator-theory', 'spectral-theory']"
76,What is the difference between vector components and its coordinates?,What is the difference between vector components and its coordinates?,,"Some mathematitians told me that vector components and coordinates are different things. They say that vector $F^n$ always has N components but coordinates depend on chosen basis and, therefore, it is meaningless to talk about coordinates when basis is unspecified. But don't components depend on basis? Why can you talk about components regardless of the basis? Do they just demonstrate their domination by being too picky and making the difference between the things, which are the same, and, thus, causing confusion? Later they mock my definition of vector space, where I say that it is a collection of vectors. They say that it is wrong since vector space an Abelian group whose elements can be scaled. But, I see no difference between these definitions. edit If this answer and Wikipedia are correct then $\psi_i = \langle \psi|i\rangle$ in $$\vec \psi = \begin{bmatrix}|1\rangle |2\rangle \cdots |n\rangle\end{bmatrix} \begin{bmatrix} \psi_1 \\ \psi_2 \\ \vdots \\ \psi_n \end{bmatrix}$$ must be coordinates in the basis $\begin{bmatrix}|1\rangle |2\rangle \cdots |n\rangle\end{bmatrix}$, which I believe is different from the standard basis. Why do quantum mechanics say that $\psi_i$ are components then rather than coordinates? Is it incorrect? Miami Operators and Matrices also say Now you see where the defining equation for operator components comes   in. Eq. (7.7) is   $$\sum_k u_k \vec e_k = \sum_i v_i \sum_k f_{ki} \vec e_k$$ Actually, I was motivated to ask this question when tried to represent these equations, which involve integration, in vector form and do not understand why everybody calls the coefficients $u_k$ in $\sum u_k \vec e_k$ components rather than coordinates. It seems to me that matematicians use these terms interchangably and troll the noobs with immaterial difference. PS Dec 2013 I see here that components , $a_i \psi_i$ are contrasted with coefficients $a_i$. Are components just another names for coordinates or we have a 3rd type of object?","Some mathematitians told me that vector components and coordinates are different things. They say that vector $F^n$ always has N components but coordinates depend on chosen basis and, therefore, it is meaningless to talk about coordinates when basis is unspecified. But don't components depend on basis? Why can you talk about components regardless of the basis? Do they just demonstrate their domination by being too picky and making the difference between the things, which are the same, and, thus, causing confusion? Later they mock my definition of vector space, where I say that it is a collection of vectors. They say that it is wrong since vector space an Abelian group whose elements can be scaled. But, I see no difference between these definitions. edit If this answer and Wikipedia are correct then $\psi_i = \langle \psi|i\rangle$ in $$\vec \psi = \begin{bmatrix}|1\rangle |2\rangle \cdots |n\rangle\end{bmatrix} \begin{bmatrix} \psi_1 \\ \psi_2 \\ \vdots \\ \psi_n \end{bmatrix}$$ must be coordinates in the basis $\begin{bmatrix}|1\rangle |2\rangle \cdots |n\rangle\end{bmatrix}$, which I believe is different from the standard basis. Why do quantum mechanics say that $\psi_i$ are components then rather than coordinates? Is it incorrect? Miami Operators and Matrices also say Now you see where the defining equation for operator components comes   in. Eq. (7.7) is   $$\sum_k u_k \vec e_k = \sum_i v_i \sum_k f_{ki} \vec e_k$$ Actually, I was motivated to ask this question when tried to represent these equations, which involve integration, in vector form and do not understand why everybody calls the coefficients $u_k$ in $\sum u_k \vec e_k$ components rather than coordinates. It seems to me that matematicians use these terms interchangably and troll the noobs with immaterial difference. PS Dec 2013 I see here that components , $a_i \psi_i$ are contrasted with coefficients $a_i$. Are components just another names for coordinates or we have a 3rd type of object?",,"['linear-algebra', 'terminology', 'analytic-geometry']"
77,What is an ordered basis?,What is an ordered basis?,,"To my understanding, $$e_1 = \{1,0,\ldots,0\},\quad e_2 = \{0,1,\ldots,0\}, \quad \ldots, \quad e_n = \{0,0,\ldots,1\}$$ is an ordered basis for a vector space of dimension $n$. But the group of basis, which implies no ordering is not ordered. For example: $$\{1,0,\ldots,0\},\quad \{0,1,\ldots,0\},\quad \ldots, \quad\{0,0,\ldots,1\}$$ is not ordered basis, but a basis, for a vector space of dimension $n$. Is this correct? Thanks!","To my understanding, $$e_1 = \{1,0,\ldots,0\},\quad e_2 = \{0,1,\ldots,0\}, \quad \ldots, \quad e_n = \{0,0,\ldots,1\}$$ is an ordered basis for a vector space of dimension $n$. But the group of basis, which implies no ordering is not ordered. For example: $$\{1,0,\ldots,0\},\quad \{0,1,\ldots,0\},\quad \ldots, \quad\{0,0,\ldots,1\}$$ is not ordered basis, but a basis, for a vector space of dimension $n$. Is this correct? Thanks!",,"['linear-algebra', 'terminology']"
78,Looking for insightful explanation as to why right inverse equals left inverse for square invertible  matrices,Looking for insightful explanation as to why right inverse equals left inverse for square invertible  matrices,,"The simple proof goes: Let B be the left inverse of A, C the right inverse. C = (BA)C = B(AC) = B This proof relies on associativity yet does not give any insight as to why this surprising fact about matrices is true. AC means a bunch of linear combinations of of columns of A. CA means a bunch of linear combinations of rows of A. Completely different numbers get multiplied in each case. The proof above is just a bunch of syntactical steps not having much to do with matrices directly, I cannot see how CA=AC=I. Can anyone shed some light on this?","The simple proof goes: Let B be the left inverse of A, C the right inverse. C = (BA)C = B(AC) = B This proof relies on associativity yet does not give any insight as to why this surprising fact about matrices is true. AC means a bunch of linear combinations of of columns of A. CA means a bunch of linear combinations of rows of A. Completely different numbers get multiplied in each case. The proof above is just a bunch of syntactical steps not having much to do with matrices directly, I cannot see how CA=AC=I. Can anyone shed some light on this?",,"['linear-algebra', 'matrices']"
79,Column Space and SVD,Column Space and SVD,,"I was reading Gilbert Strang's book and he says that if $A=USV'$ be the SVD of A ( assume square for the moment) then the nullspace of A is given by the last $n-r$ columns of V and the column space by the first $r$ columns of $U$. I understand the null space part. If  \begin{align} A&=USV'\\ AV&=US\\ [Av_1 \;Av_2\cdots Av_r\cdots Av_n]&=[Us_1 \;Us_2\cdots Us_r\cdots Us_n] \end{align} The columns $Us_{r+1}\cdots Us_n$ are $0$ vectors and correspond to (applicatoin of A on) linearly independent vectors $v_{r+1}\cdots v_n$. Thus, vectors $v_{r+1}\cdots v_n$ form the null space of A. How do I prove the statement on column space? I am not even able to start. This is not homework by the way. Just wanted to prove everything in Strang myself for fun. I feel it has to be inclusion proof where I show that any vector in the column space of A is in the column space of $\hat{U}$ and vice versa. ($\hat{U}$ is first r columns of $U$)","I was reading Gilbert Strang's book and he says that if $A=USV'$ be the SVD of A ( assume square for the moment) then the nullspace of A is given by the last $n-r$ columns of V and the column space by the first $r$ columns of $U$. I understand the null space part. If  \begin{align} A&=USV'\\ AV&=US\\ [Av_1 \;Av_2\cdots Av_r\cdots Av_n]&=[Us_1 \;Us_2\cdots Us_r\cdots Us_n] \end{align} The columns $Us_{r+1}\cdots Us_n$ are $0$ vectors and correspond to (applicatoin of A on) linearly independent vectors $v_{r+1}\cdots v_n$. Thus, vectors $v_{r+1}\cdots v_n$ form the null space of A. How do I prove the statement on column space? I am not even able to start. This is not homework by the way. Just wanted to prove everything in Strang myself for fun. I feel it has to be inclusion proof where I show that any vector in the column space of A is in the column space of $\hat{U}$ and vice versa. ($\hat{U}$ is first r columns of $U$)",,"['linear-algebra', 'matrices', 'svd']"
80,Is this matrix diagonalizable? Wolfram Alpha seems to contradict itself...,Is this matrix diagonalizable? Wolfram Alpha seems to contradict itself...,,"I have the matrix $\begin{bmatrix}0.45 & 0.40 \\ 0.55 & 0.60 \end{bmatrix}$. I believe $\begin{bmatrix}\frac{10}{17} \\ \frac{55}{68}\end{bmatrix}$ is an eigenvector for this matrix corresponding to the eigenvalue $1$, and that $\begin{bmatrix}-\sqrt{2} \\ \sqrt{2}\end{bmatrix}$ is an eigenvector for this matrix corresponding to the eigenvalue $0.05$. However, Wolfram Alpha tells me this matrix is, in fact, not diagonalizable (a.k.a. ""defective""): I'm really confused... which one is in fact defective -- Wolfram Alpha, or the matrix? Or is it my understanding of diagonalizability that's, uh, defective?","I have the matrix $\begin{bmatrix}0.45 & 0.40 \\ 0.55 & 0.60 \end{bmatrix}$. I believe $\begin{bmatrix}\frac{10}{17} \\ \frac{55}{68}\end{bmatrix}$ is an eigenvector for this matrix corresponding to the eigenvalue $1$, and that $\begin{bmatrix}-\sqrt{2} \\ \sqrt{2}\end{bmatrix}$ is an eigenvector for this matrix corresponding to the eigenvalue $0.05$. However, Wolfram Alpha tells me this matrix is, in fact, not diagonalizable (a.k.a. ""defective""): I'm really confused... which one is in fact defective -- Wolfram Alpha, or the matrix? Or is it my understanding of diagonalizability that's, uh, defective?",,"['linear-algebra', 'eigenvalues-eigenvectors', 'wolfram-alpha']"
81,What are some examples of vector spaces that aren't graded?,What are some examples of vector spaces that aren't graded?,,"From wikipedia : a vector space $V$ is graded if it decomposes into direct sum $ \oplus_{n \geq 0} V_n$ of vector spaces $V_n$. So as far as I understand things, any vector space with a countable basis is graded:  Let $V$ be a vector space over a field $k$ with basis $\{v_n\}_{n\in\mathbb{N}}$, then $V = \oplus_{n\geq 0} k\cdot v_n$.  Then the only vector spaces that I can think of that aren't obviously graded are things like $C(X)$, the space of continuous functions on some manifold $X$ Is this correct? are there any more? or do I not understand something? Thanks","From wikipedia : a vector space $V$ is graded if it decomposes into direct sum $ \oplus_{n \geq 0} V_n$ of vector spaces $V_n$. So as far as I understand things, any vector space with a countable basis is graded:  Let $V$ be a vector space over a field $k$ with basis $\{v_n\}_{n\in\mathbb{N}}$, then $V = \oplus_{n\geq 0} k\cdot v_n$.  Then the only vector spaces that I can think of that aren't obviously graded are things like $C(X)$, the space of continuous functions on some manifold $X$ Is this correct? are there any more? or do I not understand something? Thanks",,"['linear-algebra', 'ring-theory', 'vector-spaces']"
82,Distance between two hyperplanes,Distance between two hyperplanes,,"I have two parallel hyper planes $$a^Tx=b_1,a^Tx=b_2$$ where $a \in \mathbb{R}^n, x \in \mathbb{R}^n ,b \in \mathbb{R}$ and I want to find the distance between the two. I have read that the distance between the two hyperplanes is also the distance between the two points $x_1$ and $x_2$ where the hyperplane intersects the line through the origin and parallel to the normal vector $\vec a$. These points are given by $$x_1=\frac{b_1}{\|a\|^2_2}a$$ and $$x_2=\frac{b_2}{\|a\|^2_2}a$$ Then the distance is $|x_1-x_2|$ but I don't really understand how we got $x_1$ and $x_2$.","I have two parallel hyper planes $$a^Tx=b_1,a^Tx=b_2$$ where $a \in \mathbb{R}^n, x \in \mathbb{R}^n ,b \in \mathbb{R}$ and I want to find the distance between the two. I have read that the distance between the two hyperplanes is also the distance between the two points $x_1$ and $x_2$ where the hyperplane intersects the line through the origin and parallel to the normal vector $\vec a$. These points are given by $$x_1=\frac{b_1}{\|a\|^2_2}a$$ and $$x_2=\frac{b_2}{\|a\|^2_2}a$$ Then the distance is $|x_1-x_2|$ but I don't really understand how we got $x_1$ and $x_2$.",,['linear-algebra']
83,Why the unit circle in $\mathbf{R^2}$ has one dimension?,Why the unit circle in  has one dimension?,\mathbf{R^2},"When I was reading 'Convex Optimization, Stephen Boyd', I was wondering of following steps Consider the unit circle in $\mathbf{R^2}$, $i.e.$, $\{x\in\mathbf{R^2}|x^2_1+x^2_2=1\}$. Its affine hull is all of $\mathbf{R^2}$, so its affine dimension is two. By most definitions of dimension, however, the unit circle in $\mathbf{R^2}$ has dimension one. I understood the affine hull of unit circle has dimension two because the all of values of affine hull is in $\mathbf{R^2}$.  But still I don't understand why the unit circle in $\mathbf{R^2}$ has one dimension. Thanks.","When I was reading 'Convex Optimization, Stephen Boyd', I was wondering of following steps Consider the unit circle in $\mathbf{R^2}$, $i.e.$, $\{x\in\mathbf{R^2}|x^2_1+x^2_2=1\}$. Its affine hull is all of $\mathbf{R^2}$, so its affine dimension is two. By most definitions of dimension, however, the unit circle in $\mathbf{R^2}$ has dimension one. I understood the affine hull of unit circle has dimension two because the all of values of affine hull is in $\mathbf{R^2}$.  But still I don't understand why the unit circle in $\mathbf{R^2}$ has one dimension. Thanks.",,"['linear-algebra', 'circles', 'convex-optimization']"
84,How can a group of matrices form a manifold?,How can a group of matrices form a manifold?,,"So for example, $GL(n,\mathbb{R})$ group. It is said that this group can be considered as manifold - but I do not get how this is possible. How does one then assign a neighborhood of a matrix, and talk of compactness? (of course manifold can be disconnected, but.)","So for example, $GL(n,\mathbb{R})$ group. It is said that this group can be considered as manifold - but I do not get how this is possible. How does one then assign a neighborhood of a matrix, and talk of compactness? (of course manifold can be disconnected, but.)",,"['linear-algebra', 'abstract-algebra', 'group-theory', 'differential-geometry']"
85,Being isomorphic as representations of a group G,Being isomorphic as representations of a group G,,"Let $G$ be a finite group. What is meant by two finite dimensional vector spaces (over $\mathbb{C}$) $V$ and $W$ being ""isomorphic as representations of $G$""? To show that we have such an isomorphism, wouldn't it suffice to just show that $\dim V = \dim W$?","Let $G$ be a finite group. What is meant by two finite dimensional vector spaces (over $\mathbb{C}$) $V$ and $W$ being ""isomorphic as representations of $G$""? To show that we have such an isomorphism, wouldn't it suffice to just show that $\dim V = \dim W$?",,"['linear-algebra', 'terminology', 'representation-theory']"
86,"Minimal structure imposed on sets to define ""dimension""","Minimal structure imposed on sets to define ""dimension""",,"How do you formally define dimension of a set without using metrics, charts, an atlas, or a vector space structure? $\mathbb{R}$ and $\mathbb{R}^2$ are equinumerous, but $\dim \mathbb{R} = 1 \neq 2 = \dim \mathbb{R}^2$ . Is there no way to formally talk about this without using span or basis vectors? Really, I'm looking for the minimal structure imposed on sets to talk about dimensionality of a set.","How do you formally define dimension of a set without using metrics, charts, an atlas, or a vector space structure? and are equinumerous, but . Is there no way to formally talk about this without using span or basis vectors? Really, I'm looking for the minimal structure imposed on sets to talk about dimensionality of a set.",\mathbb{R} \mathbb{R}^2 \dim \mathbb{R} = 1 \neq 2 = \dim \mathbb{R}^2,"['linear-algebra', 'general-topology', 'dimension-theory-analysis']"
87,Definition of adjoint of a linear map,Definition of adjoint of a linear map,,"I am having a tough time understanding adjoint of a linear map. Consider a linear map between two vector spaces $\, f:V\rightarrow W,$ let us denote $f^*$ to denote its adjoint. Accroding to this video https://www.youtube.com/watch?v=SjCs_HyYtSo (around time 5:50) the author explains that adjoint of a linear map is a function from dual of $\,W$ (denoted by $\,W^*$) to the dual of $\,V$ (denoted by $\,V^*$). So this implies $\,f^*:W^*\rightarrow V^*.$ On the other hand in the pdf http://math.mit.edu/~trasched/18.700.f10/lect17-article.pdf , the adjoint of the linear map is defined as another linear map from $\,W$ to $\,V.$ So this implies  $\,f^*:W\rightarrow V.$ Can some body clarify this discrepancy?","I am having a tough time understanding adjoint of a linear map. Consider a linear map between two vector spaces $\, f:V\rightarrow W,$ let us denote $f^*$ to denote its adjoint. Accroding to this video https://www.youtube.com/watch?v=SjCs_HyYtSo (around time 5:50) the author explains that adjoint of a linear map is a function from dual of $\,W$ (denoted by $\,W^*$) to the dual of $\,V$ (denoted by $\,V^*$). So this implies $\,f^*:W^*\rightarrow V^*.$ On the other hand in the pdf http://math.mit.edu/~trasched/18.700.f10/lect17-article.pdf , the adjoint of the linear map is defined as another linear map from $\,W$ to $\,V.$ So this implies  $\,f^*:W\rightarrow V.$ Can some body clarify this discrepancy?",,"['linear-algebra', 'adjoint-operators']"
88,Why do elementary matrix operations not affect the row space of a given matrix?,Why do elementary matrix operations not affect the row space of a given matrix?,,"I have shown that two of the three elementary operations will not change the image of the row space of the matrix: given a row vector $\vec{v}$, $k\vec{v}$ will span the same (scalar multiplication), and switching the order of the rows won't change anything because the span of the row vectors will be the same (vectors will be in a different order). However, how would one show that adding/subtracting vectors does not change the row space?","I have shown that two of the three elementary operations will not change the image of the row space of the matrix: given a row vector $\vec{v}$, $k\vec{v}$ will span the same (scalar multiplication), and switching the order of the rows won't change anything because the span of the row vectors will be the same (vectors will be in a different order). However, how would one show that adding/subtracting vectors does not change the row space?",,"['linear-algebra', 'matrix-rank']"
89,Every $n\times n$ matrix is the sum of a diagonalizable matrix and a nilpotent matrix.,Every  matrix is the sum of a diagonalizable matrix and a nilpotent matrix.,n\times n,"I would like to prove that every $n\times n$ matrix is the sum of a diagonalizable matrix and a nilpotent matrix. How is this possible? I'm not sure where to begin really- I know that a nilpotent matrix is one of which some power is the zero matrix.  I also know that a matrix A can be written as $AP=PJ$ with $P$ invertible and $J$ of Jordan form.  I have proven that any strictly upper triangular matrix is nilpotent, so $J$ can be written as $D+N $, with D diagonal and $N$ nilpotent, but how can I change this for A? Thank you!","I would like to prove that every $n\times n$ matrix is the sum of a diagonalizable matrix and a nilpotent matrix. How is this possible? I'm not sure where to begin really- I know that a nilpotent matrix is one of which some power is the zero matrix.  I also know that a matrix A can be written as $AP=PJ$ with $P$ invertible and $J$ of Jordan form.  I have proven that any strictly upper triangular matrix is nilpotent, so $J$ can be written as $D+N $, with D diagonal and $N$ nilpotent, but how can I change this for A? Thank you!",,['linear-algebra']
90,Is every endomorphism of a vector space a linear combination of idempotents?,Is every endomorphism of a vector space a linear combination of idempotents?,,"Is every endomorphism of a $K$-vector space a $K$-linear combination of idempotents? This question was asked by Jonas Meyer in a comment to this question . To make sure I earn no points thanks to a question raised by somebody else, I shall put a 500 point bounty on this question (but I think I must wait two days). George Lowther told me in a comment to this answer that he had a positive answer to Jonas's question. I considered suggesting to George that he posts his answer to Jonas's question as an answer to this question , but I thought Jonas's question deserved to be asked separately.","Is every endomorphism of a $K$-vector space a $K$-linear combination of idempotents? This question was asked by Jonas Meyer in a comment to this question . To make sure I earn no points thanks to a question raised by somebody else, I shall put a 500 point bounty on this question (but I think I must wait two days). George Lowther told me in a comment to this answer that he had a positive answer to Jonas's question. I considered suggesting to George that he posts his answer to Jonas's question as an answer to this question , but I thought Jonas's question deserved to be asked separately.",,['linear-algebra']
91,"What shape do we get when we shear an ellipse? And more generally, do affine transformations always map conic sections to conic sections?","What shape do we get when we shear an ellipse? And more generally, do affine transformations always map conic sections to conic sections?",,"What shape do we get when we shear an ellipse? Is it another ellipse (or circle in special cases)? Or is it some other shape which isn’t a conic section? I was under the impression that applying any affine transformations to an ellipse would always yield an ellipse (or special ellipses like circles, line segments or points). Yet recently, I was doing an exercise in my Linear Algebra textbook, and in one of the answers it states that: “An ellipse has the form $Ax^2 + By^2 = C$ where $A ≠ B$.” Well clearly, I disagree with the condition A ≠ B for a start, as I implied above that I consider circles to be a subset of ellipses. Secondly, a rotated ellipse will not be of that form, either. This leads me to believe that this is a mistake in the book (the question/answers do not say that the ellipse has to have major and minor axis in the same orientation as the $x$/$y$ axes either). Is it, or am I missing something here? This textbook I’m using is the Further Maths HL textbook for IB, and although 1st edition, all of the content in all of the IB maths textbooks so far is very good, and the answers have always been (until now) trustworthy. This led me to a few other speculations as well… so in addition to my first question: What shape do we get when an ellipse is sheared? Could someone clarify these following ones too? When ellipses are affine transformed, is the image always an ellipse? When parabolas/hyperbolas are affine transformed, are the images always parabolas/hyperbolas? And hence, from the above 2 questions, does the affine transformation of a conic section always result in a conic section? The question in the book (although not necessarily relevant, as I am just challenging the statement above in the answers) is: Under what conditions does a linear transformation with matrix \begin{bmatrix} a&b\\ c&d \end{bmatrix} transform the unit circle into: a. an ellipse? b. a circle?","What shape do we get when we shear an ellipse? Is it another ellipse (or circle in special cases)? Or is it some other shape which isn’t a conic section? I was under the impression that applying any affine transformations to an ellipse would always yield an ellipse (or special ellipses like circles, line segments or points). Yet recently, I was doing an exercise in my Linear Algebra textbook, and in one of the answers it states that: “An ellipse has the form $Ax^2 + By^2 = C$ where $A ≠ B$.” Well clearly, I disagree with the condition A ≠ B for a start, as I implied above that I consider circles to be a subset of ellipses. Secondly, a rotated ellipse will not be of that form, either. This leads me to believe that this is a mistake in the book (the question/answers do not say that the ellipse has to have major and minor axis in the same orientation as the $x$/$y$ axes either). Is it, or am I missing something here? This textbook I’m using is the Further Maths HL textbook for IB, and although 1st edition, all of the content in all of the IB maths textbooks so far is very good, and the answers have always been (until now) trustworthy. This led me to a few other speculations as well… so in addition to my first question: What shape do we get when an ellipse is sheared? Could someone clarify these following ones too? When ellipses are affine transformed, is the image always an ellipse? When parabolas/hyperbolas are affine transformed, are the images always parabolas/hyperbolas? And hence, from the above 2 questions, does the affine transformation of a conic section always result in a conic section? The question in the book (although not necessarily relevant, as I am just challenging the statement above in the answers) is: Under what conditions does a linear transformation with matrix \begin{bmatrix} a&b\\ c&d \end{bmatrix} transform the unit circle into: a. an ellipse? b. a circle?",,"['linear-algebra', 'conic-sections', 'linear-transformations']"
92,How to find a basis of an image of a linear transformation?,How to find a basis of an image of a linear transformation?,,"I apologize for asking a question though there are pretty much questions on math.stackexchange with the same title, but the answers on them are still not clear for me. I have this linear operator: $$ Ax = (2x_1-x_2-x_3, x_1-2x_2+x_3, x_1+x_2-2x_3); $$ And I need to find the basis of the kernel and the basis of the image of this transformation. First, I wrote the matrix of this transformation, which is: $$ \begin{pmatrix} 2 & -1 & -1 \\ 1 & -2 & 1 \\ 1 & 1 & -2\end{pmatrix} $$ I found the basis of the kernel by solving a system of 3 linear equations: $$ \begin{pmatrix} 2 & -1 & -1 \\ 1 & -2 & 1 \\ 1 & 1 & -2\end{pmatrix}\begin{pmatrix} x_1 \\ x_2 \\ x_3\end{pmatrix} =  \begin{pmatrix} 0 \\ 0 \\ 0\end{pmatrix} $$ It is $$ kerA = (1,1,1) $$ But how can I find the basis of the image? What I have found so far is that I need to complement a basis of a kernel up to a basis of an original space. But I do not have an idea of how to do this correctly. I thought that I can use any two linear independent vectors for this purpose, like $$ imA = \{(1,0,0), (0,1,0)\} $$ because the image here is $\mathbb{R}^2$ But the correct answer from my textbook is: $$ imA = \{(2,1,1), (-1,2,1)\} $$ And by the way I cannot be sure that there is no error in the textbook's answer. So could anyone help me with this. I will be very grateful, thank you in advance.","I apologize for asking a question though there are pretty much questions on math.stackexchange with the same title, but the answers on them are still not clear for me. I have this linear operator: $$ Ax = (2x_1-x_2-x_3, x_1-2x_2+x_3, x_1+x_2-2x_3); $$ And I need to find the basis of the kernel and the basis of the image of this transformation. First, I wrote the matrix of this transformation, which is: $$ \begin{pmatrix} 2 & -1 & -1 \\ 1 & -2 & 1 \\ 1 & 1 & -2\end{pmatrix} $$ I found the basis of the kernel by solving a system of 3 linear equations: $$ \begin{pmatrix} 2 & -1 & -1 \\ 1 & -2 & 1 \\ 1 & 1 & -2\end{pmatrix}\begin{pmatrix} x_1 \\ x_2 \\ x_3\end{pmatrix} =  \begin{pmatrix} 0 \\ 0 \\ 0\end{pmatrix} $$ It is $$ kerA = (1,1,1) $$ But how can I find the basis of the image? What I have found so far is that I need to complement a basis of a kernel up to a basis of an original space. But I do not have an idea of how to do this correctly. I thought that I can use any two linear independent vectors for this purpose, like $$ imA = \{(1,0,0), (0,1,0)\} $$ because the image here is $\mathbb{R}^2$ But the correct answer from my textbook is: $$ imA = \{(2,1,1), (-1,2,1)\} $$ And by the way I cannot be sure that there is no error in the textbook's answer. So could anyone help me with this. I will be very grateful, thank you in advance.",,"['linear-algebra', 'abstract-algebra', 'matrices', 'self-learning', 'linear-transformations']"
93,Choosing an orthonormal basis in which a linear operator has a sparse matrix,Choosing an orthonormal basis in which a linear operator has a sparse matrix,,"Given a linear operator $T$ on an $n$-dimensional vector space $V$ (over $\mathbb R$), I want to find an orthonormal basis for $V$ in which the matrix of $T$ is sparse (has many zeros). How many zeros can I get? Equivalent formulation: what is the largest number $k=k(n)$ such that every real $n\times n$ matrix is orthogonally equivalent to a matrix with at least $k$ zero entries? Updated with partial results Since the orthogonal group $O(n)$ has dimension $\frac12 n(n-1)$, it follows that  $$k(n)\le \frac12 n(n-1) \tag{1}$$ Indeed, the set of all matrices with $k$ particular entries set to $0$ is an $(n^2-k)$-dimensional vector space, hence its orbit under $A\mapsto O^TAO$ ($O$ orthogonal) has at most $n^2-k+\frac12 n(n-1)$ dimensions. (Hausdorff dimension can be used here to make this rigorous.) On the other hand, the answer by Omnomnomnom gives a lower bound  $$k(n)\ge \left\lceil \frac12 n(n-2) \right\rceil \tag{2}$$ A gap remains here. It is easy to see that $k(2)=0$ (e.g.,  a generic rotation of the plane  has full matrix  in every orthonormal basis), so (2) is sharp in this case. For $n=3$, the inequalities give $2\le k(3)\le 3$; which one is   sharp? The same question could be asked for complex matrices and unitary equivalence. Let $k_{\mathbb C}(n)$ denote the analog of $k(n)$ for this case. Since the real dimension of $U(n)$ is $n^2$, it follows that  $$k_{\mathbb C}(n) \le \left\lfloor \frac12 n^2 \right\rfloor\tag{3}$$ In the opposite direction, Omnomnomnom pointed out the lower bound $$k_{\mathbb C}(n)\ge \frac12 n(n-1)   \tag{4}$$ There is a gap here too.","Given a linear operator $T$ on an $n$-dimensional vector space $V$ (over $\mathbb R$), I want to find an orthonormal basis for $V$ in which the matrix of $T$ is sparse (has many zeros). How many zeros can I get? Equivalent formulation: what is the largest number $k=k(n)$ such that every real $n\times n$ matrix is orthogonally equivalent to a matrix with at least $k$ zero entries? Updated with partial results Since the orthogonal group $O(n)$ has dimension $\frac12 n(n-1)$, it follows that  $$k(n)\le \frac12 n(n-1) \tag{1}$$ Indeed, the set of all matrices with $k$ particular entries set to $0$ is an $(n^2-k)$-dimensional vector space, hence its orbit under $A\mapsto O^TAO$ ($O$ orthogonal) has at most $n^2-k+\frac12 n(n-1)$ dimensions. (Hausdorff dimension can be used here to make this rigorous.) On the other hand, the answer by Omnomnomnom gives a lower bound  $$k(n)\ge \left\lceil \frac12 n(n-2) \right\rceil \tag{2}$$ A gap remains here. It is easy to see that $k(2)=0$ (e.g.,  a generic rotation of the plane  has full matrix  in every orthonormal basis), so (2) is sharp in this case. For $n=3$, the inequalities give $2\le k(3)\le 3$; which one is   sharp? The same question could be asked for complex matrices and unitary equivalence. Let $k_{\mathbb C}(n)$ denote the analog of $k(n)$ for this case. Since the real dimension of $U(n)$ is $n^2$, it follows that  $$k_{\mathbb C}(n) \le \left\lfloor \frac12 n^2 \right\rfloor\tag{3}$$ In the opposite direction, Omnomnomnom pointed out the lower bound $$k_{\mathbb C}(n)\ge \frac12 n(n-1)   \tag{4}$$ There is a gap here too.",,"['linear-algebra', 'matrices', 'orthonormal']"
94,Maximizing a sum of inner products,Maximizing a sum of inner products,,"Someone asked this question on a French maths forum here and it caught my attention. The question is the following: let $(E, \langle \cdot, \cdot \rangle)$ be a Euclidean vector space. Find the minimum and maximum possible values of the sum $$\langle u_1, u_2 \rangle + \langle u_2, u_3 \rangle + \dots + \langle u_{n-1}, u_n \rangle + \langle u_n, u_1 \rangle$$ when the $u_k$ are unit vectors whose sum is zero. $$ $$ $$ $$ Edit : Okay, since there is no answer so far, let me write here what I came up with, maybe someone will know how to follow. Of course, there could be a much better approach! Let's introduce a couple of notations. Let $E^n$ denote the $n$-fold space with its inner product derived from $E$ (*) and let $\varphi$ and $\psi$ denote the functions defined by: $$ \varphi: \left\{ \begin{array}{ccc} E^n & \rightarrow &\mathbb{R} \\ u = (u_1, \dots, u_n) &\mapsto & \langle u_1, u_2 \rangle + \dots + \langle u_n, u_1 \rangle \end{array} \right.$$ $$ \psi: \left\{ \begin{array}{ccc} E^n & \rightarrow &\mathbb{R}^n \times E \\ u = (u_1, \dots, u_n) &\mapsto & \left((||u_i||^2 - 1)_{k= 1 \dots n}\,,\, \sum_{k=1}^n u_k\right)\\ \end{array} \right.$$ So the question is: find the extrema of $\varphi$ on the level set $K := \{\psi = 0\}$. Note that $K$ is compact (NB: it looks like a ""slice"" of an $n$-fold product of hyperspheres, whatever) so $\varphi$ has a minimum and a maximum on $K$ indeed. What are their values? Let's see what differential calculus tells us (**). Let $u = (u_1, \dots, u_n)$ be a local extremum of $\varphi_{|K}$. The derivative of $\varphi$ at $u$ must kill tangent vectors to $K$, which amounts to say that $\mathrm{Ker}\, D_u \psi \subset \mathrm{Ker}\, D_u \varphi$. It is straightforward to compute these derivatives and their kernels, they are given by: $$ \mathrm{Ker}\, D_u \varphi = \{v\}^\perp$$ where $v = (u_n + u_2, u_1 + u_3, \dots, u_{n-1} + u_1) \in E^n$, and  $$ \mathrm{Ker}\, D_u \psi = ({L_1}^\perp \times \dots \times {L_n}^\perp) ~ \cap ~ \Delta^\perp$$ where $L_k$ denotes the line through $u_k$ in $E$ and $\Delta$ denotes the diagonal in $E^n$. The condition $\mathrm{Ker}\, D_u \psi \subset \mathrm{Ker}\, D_u \varphi$ then amounts to saying that $v \in ({L_1}\times \dots \times {L_n}) ~ + ~ \Delta$. In conclusion: if $u = (u_1, \dots, u_n)$ is a local extremum of $\varphi$ restricted to $K$, then there exists scalars $\lambda_1, \dots, \lambda_n$ and a vector $a \in E$ such that: $$\begin{align*} u_n + u_2 &= \lambda_1 u_1 + a \\ u_1 + u_3 &= \lambda_2 u_2 + a \\ & \cdots  \\ u_{n-1} + u_1 & =  \lambda_n u_n + a  \end{align*}$$ What can we derive from that? First, note that $a$ is given by  $0 = \sum_{k=1}^n \lambda_k u_k + na$ (sum all the equations). Also, the value of $\varphi$ at this point $u$ is given by $\sum_{k=1}^n \lambda_k / 2$. More importantly, it is easy to see inductively that all the $u_k$ lie in a same $3$-dimensional subspace of $E$. That's all that I could derive from these equations unfortunately. But I think it should be possible to make them confess more, maybe using a symmetry argument. Here is what I suspect: $a$ must be $0$ and all the $\lambda_k$ must be equal. It follows that all the $u_k$ are coplanar and that the angle between $u_k$ and $u_{k+2}$ is constant. Finally, in a nutshell, break into cases according to whether $n$ is even or odd. In both cases, the maximum is achieved when the $u_k$ lie like $n$-th roots of unity on the circle, and it is given by $n \cos (2\pi / n)$. When $n$ is even, the minimum is $-n$ (just take $u_1 = u_3 = \dots = u_{n-1} = -u_2 = -u_4 = \dots = -u_n$) and when $n$ is odd, $-n \cos(2\pi/n)$ (not totally sure about that last one). Wow, this is much longer than I expected, hope I didn't bore too many people to death. $$ $$ $$ $$ (*) It is given by $\langle u, v \rangle = \sum_{k=1}^n \langle u_k, v_k \rangle$. (**) in what follows, I recover some version of Lagrange's multiplier ""manually"". You may skip that part and jump to the set of linear equations if you don't like what you see :)","Someone asked this question on a French maths forum here and it caught my attention. The question is the following: let $(E, \langle \cdot, \cdot \rangle)$ be a Euclidean vector space. Find the minimum and maximum possible values of the sum $$\langle u_1, u_2 \rangle + \langle u_2, u_3 \rangle + \dots + \langle u_{n-1}, u_n \rangle + \langle u_n, u_1 \rangle$$ when the $u_k$ are unit vectors whose sum is zero. $$ $$ $$ $$ Edit : Okay, since there is no answer so far, let me write here what I came up with, maybe someone will know how to follow. Of course, there could be a much better approach! Let's introduce a couple of notations. Let $E^n$ denote the $n$-fold space with its inner product derived from $E$ (*) and let $\varphi$ and $\psi$ denote the functions defined by: $$ \varphi: \left\{ \begin{array}{ccc} E^n & \rightarrow &\mathbb{R} \\ u = (u_1, \dots, u_n) &\mapsto & \langle u_1, u_2 \rangle + \dots + \langle u_n, u_1 \rangle \end{array} \right.$$ $$ \psi: \left\{ \begin{array}{ccc} E^n & \rightarrow &\mathbb{R}^n \times E \\ u = (u_1, \dots, u_n) &\mapsto & \left((||u_i||^2 - 1)_{k= 1 \dots n}\,,\, \sum_{k=1}^n u_k\right)\\ \end{array} \right.$$ So the question is: find the extrema of $\varphi$ on the level set $K := \{\psi = 0\}$. Note that $K$ is compact (NB: it looks like a ""slice"" of an $n$-fold product of hyperspheres, whatever) so $\varphi$ has a minimum and a maximum on $K$ indeed. What are their values? Let's see what differential calculus tells us (**). Let $u = (u_1, \dots, u_n)$ be a local extremum of $\varphi_{|K}$. The derivative of $\varphi$ at $u$ must kill tangent vectors to $K$, which amounts to say that $\mathrm{Ker}\, D_u \psi \subset \mathrm{Ker}\, D_u \varphi$. It is straightforward to compute these derivatives and their kernels, they are given by: $$ \mathrm{Ker}\, D_u \varphi = \{v\}^\perp$$ where $v = (u_n + u_2, u_1 + u_3, \dots, u_{n-1} + u_1) \in E^n$, and  $$ \mathrm{Ker}\, D_u \psi = ({L_1}^\perp \times \dots \times {L_n}^\perp) ~ \cap ~ \Delta^\perp$$ where $L_k$ denotes the line through $u_k$ in $E$ and $\Delta$ denotes the diagonal in $E^n$. The condition $\mathrm{Ker}\, D_u \psi \subset \mathrm{Ker}\, D_u \varphi$ then amounts to saying that $v \in ({L_1}\times \dots \times {L_n}) ~ + ~ \Delta$. In conclusion: if $u = (u_1, \dots, u_n)$ is a local extremum of $\varphi$ restricted to $K$, then there exists scalars $\lambda_1, \dots, \lambda_n$ and a vector $a \in E$ such that: $$\begin{align*} u_n + u_2 &= \lambda_1 u_1 + a \\ u_1 + u_3 &= \lambda_2 u_2 + a \\ & \cdots  \\ u_{n-1} + u_1 & =  \lambda_n u_n + a  \end{align*}$$ What can we derive from that? First, note that $a$ is given by  $0 = \sum_{k=1}^n \lambda_k u_k + na$ (sum all the equations). Also, the value of $\varphi$ at this point $u$ is given by $\sum_{k=1}^n \lambda_k / 2$. More importantly, it is easy to see inductively that all the $u_k$ lie in a same $3$-dimensional subspace of $E$. That's all that I could derive from these equations unfortunately. But I think it should be possible to make them confess more, maybe using a symmetry argument. Here is what I suspect: $a$ must be $0$ and all the $\lambda_k$ must be equal. It follows that all the $u_k$ are coplanar and that the angle between $u_k$ and $u_{k+2}$ is constant. Finally, in a nutshell, break into cases according to whether $n$ is even or odd. In both cases, the maximum is achieved when the $u_k$ lie like $n$-th roots of unity on the circle, and it is given by $n \cos (2\pi / n)$. When $n$ is even, the minimum is $-n$ (just take $u_1 = u_3 = \dots = u_{n-1} = -u_2 = -u_4 = \dots = -u_n$) and when $n$ is odd, $-n \cos(2\pi/n)$ (not totally sure about that last one). Wow, this is much longer than I expected, hope I didn't bore too many people to death. $$ $$ $$ $$ (*) It is given by $\langle u, v \rangle = \sum_{k=1}^n \langle u_k, v_k \rangle$. (**) in what follows, I recover some version of Lagrange's multiplier ""manually"". You may skip that part and jump to the set of linear equations if you don't like what you see :)",,"['linear-algebra', 'multivariable-calculus', 'optimization', 'inner-products']"
95,Determinant of a special matrix,Determinant of a special matrix,,"The question is related to the eigenvalue problem. Using MAPLE, I calculated the following determinants: $$\begin{align} \begin{vmatrix}    -\lambda & 1 \\   1 & -\lambda \\ \end{vmatrix}&=\lambda^2-1\\ \begin{vmatrix}   -\lambda & 1 & 0 \\    1& -\lambda & 1 \\    0& 1 & -\lambda \\ \end{vmatrix}&=-\lambda^3+2\lambda\\ \begin{vmatrix}   -\lambda & 1 &0  &1  \\    1& -\lambda & 1 &0  \\    0&  1& -\lambda & 1 \\    1&  0&  1& -\lambda \\ \end{vmatrix}&=\lambda^4-4\lambda^2\\ \begin{vmatrix}   -\lambda &1  &0  &1  &0  \\    1& -\lambda &1  &0  &1  \\    0&  1& -\lambda &1  &0  \\    1&  0&  1& -\lambda &1  \\    0&  1&  0&  1& -\lambda \\ \end{vmatrix}&=-\lambda^5+6\lambda^3\\ \begin{vmatrix}   -\lambda &1  &0  &1  &0  &1  \\    1& -\lambda &1  &0  &1  &0  \\    0&  1& -\lambda &1  &0  &1  \\    1&  0&  1& -\lambda &1  &0  \\    0&  1&  0&  1& -\lambda &1  \\    1&  0&  1&  0&1 & -\lambda \\ \end{vmatrix}&=-9\lambda^4+\lambda^6\\ \end{align} $$ But I have no idea how to calculate the determinants quickly by hand. Here is my question : What is the determinant in the $n$ by $n$ case?","The question is related to the eigenvalue problem. Using MAPLE, I calculated the following determinants: $$\begin{align} \begin{vmatrix}    -\lambda & 1 \\   1 & -\lambda \\ \end{vmatrix}&=\lambda^2-1\\ \begin{vmatrix}   -\lambda & 1 & 0 \\    1& -\lambda & 1 \\    0& 1 & -\lambda \\ \end{vmatrix}&=-\lambda^3+2\lambda\\ \begin{vmatrix}   -\lambda & 1 &0  &1  \\    1& -\lambda & 1 &0  \\    0&  1& -\lambda & 1 \\    1&  0&  1& -\lambda \\ \end{vmatrix}&=\lambda^4-4\lambda^2\\ \begin{vmatrix}   -\lambda &1  &0  &1  &0  \\    1& -\lambda &1  &0  &1  \\    0&  1& -\lambda &1  &0  \\    1&  0&  1& -\lambda &1  \\    0&  1&  0&  1& -\lambda \\ \end{vmatrix}&=-\lambda^5+6\lambda^3\\ \begin{vmatrix}   -\lambda &1  &0  &1  &0  &1  \\    1& -\lambda &1  &0  &1  &0  \\    0&  1& -\lambda &1  &0  &1  \\    1&  0&  1& -\lambda &1  &0  \\    0&  1&  0&  1& -\lambda &1  \\    1&  0&  1&  0&1 & -\lambda \\ \end{vmatrix}&=-9\lambda^4+\lambda^6\\ \end{align} $$ But I have no idea how to calculate the determinants quickly by hand. Here is my question : What is the determinant in the $n$ by $n$ case?",,['linear-algebra']
96,Bases for exterior powers,Bases for exterior powers,,"I've seen the following claim several times: If $V$ is a vector space over $K$ with basis $\{e_1,\ldots,e_n\}$ then the basis to the kth exterior power of $V$ is given by the elements $$\{e_{i_1}\wedge e_{i_2}\wedge\cdots\wedge e_{i_k} \mid 1 \le  i_1 < i_2 < \cdots < i_k \le n\}$$ Now, given some basic facts on exterior powers, it's not hard to show that the above set actually spans the $k$th exterior power. On the other hand, I've never seen a complete proof of the linear independence of this set. So my question is: What is the simplest way to show that $\{e_{i_1}\wedge e_{i_2}\wedge\cdots \wedge e_{i_k} \mid 1 \le  i_1 < i_2 < \cdots < i_k \le n\}$ is linearly independent?","I've seen the following claim several times: If $V$ is a vector space over $K$ with basis $\{e_1,\ldots,e_n\}$ then the basis to the kth exterior power of $V$ is given by the elements $$\{e_{i_1}\wedge e_{i_2}\wedge\cdots\wedge e_{i_k} \mid 1 \le  i_1 < i_2 < \cdots < i_k \le n\}$$ Now, given some basic facts on exterior powers, it's not hard to show that the above set actually spans the $k$th exterior power. On the other hand, I've never seen a complete proof of the linear independence of this set. So my question is: What is the simplest way to show that $\{e_{i_1}\wedge e_{i_2}\wedge\cdots \wedge e_{i_k} \mid 1 \le  i_1 < i_2 < \cdots < i_k \le n\}$ is linearly independent?",,"['linear-algebra', 'abstract-algebra', 'multilinear-algebra']"
97,Matrices such that $M^2+M^T=I_n$ are invertible,Matrices such that  are invertible,M^2+M^T=I_n,"Let $M$ be an $n\times n$ real matrix such that  $M^2+M^T=I_n$. Prove that $M$ is invertible Here is my progress: Playing with determinant: one has $\det(M^2)=\det(I_n-M^T)$ hence $\det(M)^2=\det(I_n-M)$ and $\det(M^T)=\det(I_n-M^2)$, hence $\det(M)=\det(I_n-M)\det(I_n+M)$ Combining both equalities yield $$\det(I_n-M)(\det(I_n-M)\det(I_n+M)-1)=0$$ Playing with the original assumption: transposing  yields $(M^T)^2+M=I_n$, and combining gives $(M^2-I_n)^2=I_n-M$ that is $M^4-2M^2+M=0$. $M$ is therefore diagonalizable and its eigenvalues lie in the set $\{0,1,-\frac{1+\sqrt{5}}{2},\frac{1-\sqrt{5}}{2}\}$ Misc Multiplying $M^2+M^T=I_n$ by $M$ in two different ways, one has $MM^T=M^TM$ Looking for a contradiction ? Supposing $M$ is not invertible, there is some $X$ such that $MX=0$. This in turn implies $M^TX=X$... So what ?","Let $M$ be an $n\times n$ real matrix such that  $M^2+M^T=I_n$. Prove that $M$ is invertible Here is my progress: Playing with determinant: one has $\det(M^2)=\det(I_n-M^T)$ hence $\det(M)^2=\det(I_n-M)$ and $\det(M^T)=\det(I_n-M^2)$, hence $\det(M)=\det(I_n-M)\det(I_n+M)$ Combining both equalities yield $$\det(I_n-M)(\det(I_n-M)\det(I_n+M)-1)=0$$ Playing with the original assumption: transposing  yields $(M^T)^2+M=I_n$, and combining gives $(M^2-I_n)^2=I_n-M$ that is $M^4-2M^2+M=0$. $M$ is therefore diagonalizable and its eigenvalues lie in the set $\{0,1,-\frac{1+\sqrt{5}}{2},\frac{1-\sqrt{5}}{2}\}$ Misc Multiplying $M^2+M^T=I_n$ by $M$ in two different ways, one has $MM^T=M^TM$ Looking for a contradiction ? Supposing $M$ is not invertible, there is some $X$ such that $MX=0$. This in turn implies $M^TX=X$... So what ?",,"['linear-algebra', 'matrices']"
98,Why do physicists get away with thinking of the Dirac Delta functional as a function?,Why do physicists get away with thinking of the Dirac Delta functional as a function?,,"For instance they use it for finding solutions to things like Poisson's Equation, i.e. the method of Green's functions . Moreover in Quantum Mechanics, it's common practise to think of the delta functions $\delta_x$ as being a sort of standard basis for the vector space of square integrable functions but $\delta$ is obviously not a square integrable function itself so how can it form a basis for something that it's not a even an element of. (On a slightly separate issue, it also doesn't make sense to me why you can think of the functions $e^{ikx}$ (Fourier Basis) as a basis square integrable functions since $e^{ikx}$ is also not square integrable. ) Are physicists just really lucky that these things work out or is there some deeper underlying reason why it's okay to think in these terms? I've come across a lot of great books by a lot of great physicists who've simply assumed this to be the case. It's hard to believe that they were all naive enough to dismiss the mathematical fallacy in their arguments. Clearly I 'm missing something. My question is what is it? An elaborate answer would be much appreciated.","For instance they use it for finding solutions to things like Poisson's Equation, i.e. the method of Green's functions . Moreover in Quantum Mechanics, it's common practise to think of the delta functions $\delta_x$ as being a sort of standard basis for the vector space of square integrable functions but $\delta$ is obviously not a square integrable function itself so how can it form a basis for something that it's not a even an element of. (On a slightly separate issue, it also doesn't make sense to me why you can think of the functions $e^{ikx}$ (Fourier Basis) as a basis square integrable functions since $e^{ikx}$ is also not square integrable. ) Are physicists just really lucky that these things work out or is there some deeper underlying reason why it's okay to think in these terms? I've come across a lot of great books by a lot of great physicists who've simply assumed this to be the case. It's hard to believe that they were all naive enough to dismiss the mathematical fallacy in their arguments. Clearly I 'm missing something. My question is what is it? An elaborate answer would be much appreciated.",,"['linear-algebra', 'functional-analysis', 'vector-spaces', 'dirac-delta', 'quantum-mechanics']"
99,Can anyone please explain the the difference between a vector and a matrix?,Can anyone please explain the the difference between a vector and a matrix?,,"I just took Calculus 3 last semester at my University and got comfortable with the idea of vectors, vector-valued functions, and basic vector operations like the dot and cross products. This semester, I'm taking Differential Equations and we seem to be throwing around the terms ""vector"" and ""matrix"" as if they're interchangeable, especially now that we're studying systems of first-order differential equations. Additionally, there's mention of ""vector spaces"" which haven't been clearly explained to me. The last time I dealt with matrices was in Algebra II in the 9th grade about 4 years ago, so there's quite a disconnect here. I feel like if I had taken Linear Algebra, this course would have been easier since my professor keeps saying ""if you've taken Linear Algebra, then this should be familiar to you"" which isn't exactly helpful. Can anyone help me bridge these gaps in my understanding?","I just took Calculus 3 last semester at my University and got comfortable with the idea of vectors, vector-valued functions, and basic vector operations like the dot and cross products. This semester, I'm taking Differential Equations and we seem to be throwing around the terms ""vector"" and ""matrix"" as if they're interchangeable, especially now that we're studying systems of first-order differential equations. Additionally, there's mention of ""vector spaces"" which haven't been clearly explained to me. The last time I dealt with matrices was in Algebra II in the 9th grade about 4 years ago, so there's quite a disconnect here. I feel like if I had taken Linear Algebra, this course would have been easier since my professor keeps saying ""if you've taken Linear Algebra, then this should be familiar to you"" which isn't exactly helpful. Can anyone help me bridge these gaps in my understanding?",,"['linear-algebra', 'matrices']"
