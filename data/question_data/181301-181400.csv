,title_raw,title_text,title_latex,body_raw,body_text,body_latex,tags
0,Can the gradient of a non-constant scalar field be zero?,Can the gradient of a non-constant scalar field be zero?,,"Let $S$ be an open set in $\mathbb{R}^n$, and let $f \colon S \to \mathbb{R}$ be a scalar field such that all the partial derivatives $D_1 f (\mathbf{a}), \ldots, D_n f (\mathbf{a})$ exist for all points $\mathbf{a} \in S$. If the gradient $\nabla f(\mathbf{a})$ of $f$ is zero for all points $\mathbf{a} \in S$, then we can show that $f$ is constant on $S$. Am I right? If we know that the gradient of $f$ is zero on $S$, then we can even show that $f$ is constant on the closure of $S$. Am I right? Now my question is, if we know that the gradient of $f$ is zero on $S$, then can we conclude that $f$ is constant on some set in $\mathbb{R}^n$ that properly contains the closure of $S$? What is the most general statement of this sort that could be made?","Let $S$ be an open set in $\mathbb{R}^n$, and let $f \colon S \to \mathbb{R}$ be a scalar field such that all the partial derivatives $D_1 f (\mathbf{a}), \ldots, D_n f (\mathbf{a})$ exist for all points $\mathbf{a} \in S$. If the gradient $\nabla f(\mathbf{a})$ of $f$ is zero for all points $\mathbf{a} \in S$, then we can show that $f$ is constant on $S$. Am I right? If we know that the gradient of $f$ is zero on $S$, then we can even show that $f$ is constant on the closure of $S$. Am I right? Now my question is, if we know that the gradient of $f$ is zero on $S$, then can we conclude that $f$ is constant on some set in $\mathbb{R}^n$ that properly contains the closure of $S$? What is the most general statement of this sort that could be made?",,"['calculus', 'real-analysis', 'analysis', 'multivariable-calculus', 'derivatives']"
1,"Prob. 2 (b), Sec. 8.14, in Apostol's CALCULUS Vol 2: How to find the directional derivative of this scalar field?","Prob. 2 (b), Sec. 8.14, in Apostol's CALCULUS Vol 2: How to find the directional derivative of this scalar field?",,"Let $f$ be the scalar field defined by the formula  $$ f(x, y, z) = (x/y)^z, $$ for all points $(x, y, z) \in \mathbb{R}^3$ for which the formula makes sense. Then what is the directional derivative of $f$ at the point $(1, 1, 1)$ in the direction of $2 \mathbf{i} + \mathbf{j} - \mathbf{k}$? My Attempt: Let us put $\mathbf{a} \colon= (1, 1, 1)$ and $\mathbf{y} \colon= (2, 1, -1)$. Then $\mathbf{a} + h \mathbf{y} = ( 1 + 2h, 1 + h, 1 - h)$, and so    $$ \begin{align}  \frac{ f( \mathbf{a} + h \mathbf{y} ) - f( \mathbf{a} ) }{ h } &= \frac{ [ (1+2h) / ( 1+h ) ]^{1-h} - 1 }{ h } \\ &=  \end{align} $$ What next? How to find  $$ \lim_{ h \to 0 }  \frac{ f( \mathbf{a} + h \mathbf{y} ) - f( \mathbf{a} ) }{ h }? $$ Or, does this limit exist?","Let $f$ be the scalar field defined by the formula  $$ f(x, y, z) = (x/y)^z, $$ for all points $(x, y, z) \in \mathbb{R}^3$ for which the formula makes sense. Then what is the directional derivative of $f$ at the point $(1, 1, 1)$ in the direction of $2 \mathbf{i} + \mathbf{j} - \mathbf{k}$? My Attempt: Let us put $\mathbf{a} \colon= (1, 1, 1)$ and $\mathbf{y} \colon= (2, 1, -1)$. Then $\mathbf{a} + h \mathbf{y} = ( 1 + 2h, 1 + h, 1 - h)$, and so    $$ \begin{align}  \frac{ f( \mathbf{a} + h \mathbf{y} ) - f( \mathbf{a} ) }{ h } &= \frac{ [ (1+2h) / ( 1+h ) ]^{1-h} - 1 }{ h } \\ &=  \end{align} $$ What next? How to find  $$ \lim_{ h \to 0 }  \frac{ f( \mathbf{a} + h \mathbf{y} ) - f( \mathbf{a} ) }{ h }? $$ Or, does this limit exist?",,"['calculus', 'real-analysis', 'analysis', 'multivariable-calculus', 'derivatives']"
2,An inequality on the geometric mean of sines,An inequality on the geometric mean of sines,,"Let $n \in \mathbb{N}$,  $n \geq 2$. Let $x_1, \ldots,  x_n \in (0, \pi)$. Set $x = \frac{(x_1 + \cdots + x_n)}{n}$. Which of the following statements are true? (b) $\prod_{k=1}^n \sin x_k \leq \sin^n x$ Option b is correct. We proceed by induction. Take $n = 2$. Then, we have $$\sin x_1+\sin x_2 = \frac{1}{2} \cos(x_1-x_2)- \cos(x_1+x_2)) \leq \frac{1-\cos(x_1+x_2)}{2} = \sin^2 x$$ as $\left|\cos x\right| \leq 1$. How to prove further?","Let $n \in \mathbb{N}$,  $n \geq 2$. Let $x_1, \ldots,  x_n \in (0, \pi)$. Set $x = \frac{(x_1 + \cdots + x_n)}{n}$. Which of the following statements are true? (b) $\prod_{k=1}^n \sin x_k \leq \sin^n x$ Option b is correct. We proceed by induction. Take $n = 2$. Then, we have $$\sin x_1+\sin x_2 = \frac{1}{2} \cos(x_1-x_2)- \cos(x_1+x_2)) \leq \frac{1-\cos(x_1+x_2)}{2} = \sin^2 x$$ as $\left|\cos x\right| \leq 1$. How to prove further?",,"['multivariable-calculus', 'trigonometry', 'inequality', 'arithmetic', 'jensen-inequality']"
3,"Prob. 20 (a) & (b), Exercises 8.9, in Apostol's CALCULUS vol. 2: If $f^\prime(x;y)=0$ for every $x$ and for every $y$, . . .","Prob. 20 (a) & (b), Exercises 8.9, in Apostol's CALCULUS vol. 2: If  for every  and for every , . . .",f^\prime(x;y)=0 x y,"Here is Prob. 20, Exercises 8.9, in the book Calculus Vol II by Tom M. Apostol, 2nd edition: (a)  Assume that $f^\prime ( \mathbf{x}; \mathbf{y} ) = 0$ for every $\mathbf{x}$ in some $n$ -ball $B(\mathbf{a})$ and for every vector $\mathbf{y}$ . Use the mean-value theorem to prove that $f$ is constant on $B(\mathbf{a})$ . (b) Suppose that $f^\prime ( \mathbf{x}; \mathbf{y} ) = 0$ for a fixed vector $\mathbf{y}$ and for every $\mathbf{x}$ in $B(\mathbf{a})$ . What can you conclude about $f$ in this case? And, here is Theorem 8.4 (The Mean-Value Theorem For Derivatives Of Scalar Fields): Assume the derivative $f^\prime ( \mathbf{a} + t \mathbf{y}; \mathbf{y} )$ exists for each $t$ in the interval $0 \leq t \leq 1$ . Then for some real $\theta$ in the open interval $0 < \theta < 1$ we have $$ f ( \mathbf{a} + \mathbf{y} ) - f(  \mathbf{a} ) = f^\prime (  \mathbf{z};  \mathbf{y} ), \ \mbox{ where } \  \mathbf{z} =  \mathbf{a} + \theta  \mathbf{y}. $$ My Attempt: Part (a) Let $\mathbf{x}$ be any point in the $n$ -ball $B( \mathbf{a}  )$ . Let us put $$\mathbf{y} \colon=  \mathbf{x} -  \mathbf{a}$$ so that $$\mathbf{x} =  \mathbf{a} +  \mathbf{y}. $$ Then, for any real number $\theta$ in the interval $0 \leq \theta \leq 1$ , we find that the point $ \mathbf{a} + \theta \mathbf{y}  $ also lies in the $n$ -ball $B( \mathbf{a} )$ , and so we must have $$ f^\prime ( \mathbf{a} + \theta \mathbf{y} ; \mathbf{y} ) = 0. $$ Then, for some real number $\theta$ such that $0 < \theta < 1$ , we obtain $$ f(\mathbf{x}) - f(\mathbf{a}) = f(\mathbf{a} + \mathbf{y} ) - f( \mathbf{a}) = f^\prime ( \mathbf{a} + \theta \mathbf{y}; \mathbf{y} ) = 0, $$ and so $$ f(\mathbf{x}) = f(\mathbf{a}) $$ for every point $\mathbf{x}$ in that $n$ -ball. Hence $f$ is constant on the $n$ -ball. Is this proof correct? And if so, then is it clear enough too? Part (b) Since $f^\prime ( \mathbf{x}; \mathbf{y} ) = 0$ for a fixed vector $\mathbf{y}$ and for every $\mathbf{x}$ in $B( \mathbf{a} )$ , therefore if $t$ is a real number such that the point $\mathbf{a} + t \mathbf{y}$ is also in $B( \mathbf{a} )$ , then we see that, for some real number $\theta \in (0, 1)$ , we have $$ f( \mathbf{a} + t \mathbf{y} ) - f( \mathbf{a} ) = f^\prime ( \mathbf{a} + \theta t \mathbf{y}; t \mathbf{y} ) = f^\prime ( \mathbf{a} + \theta t \mathbf{y};  \mathbf{y} ) = 0, $$ and so $$ f( \mathbf{a} + t \mathbf{y} ) = f( \mathbf{a} ), $$ which shows that $f$ does not change along any line through the point $\mathbf{a}$ and parallel to the vector $\mathbf{y}$ . Is each and everything of what I have done (and stated) in Part (b) correct? Or, have I made an error? P.S.: Part (b) Continued: Now let $\mathbf{x}$ be any other point in $B(\mathbf{a})$ . Then using the same argument as above but with $\mathbf{a}$ replaced by $\mathbf{x}$ we can conclude that our scalar field $f$ doesn't change throughout $B(\mathbf{a})$ along any line parallel to the vector $\mathbf{y}$ . Am I right? Is my reasoning correct?","Here is Prob. 20, Exercises 8.9, in the book Calculus Vol II by Tom M. Apostol, 2nd edition: (a)  Assume that for every in some -ball and for every vector . Use the mean-value theorem to prove that is constant on . (b) Suppose that for a fixed vector and for every in . What can you conclude about in this case? And, here is Theorem 8.4 (The Mean-Value Theorem For Derivatives Of Scalar Fields): Assume the derivative exists for each in the interval . Then for some real in the open interval we have My Attempt: Part (a) Let be any point in the -ball . Let us put so that Then, for any real number in the interval , we find that the point also lies in the -ball , and so we must have Then, for some real number such that , we obtain and so for every point in that -ball. Hence is constant on the -ball. Is this proof correct? And if so, then is it clear enough too? Part (b) Since for a fixed vector and for every in , therefore if is a real number such that the point is also in , then we see that, for some real number , we have and so which shows that does not change along any line through the point and parallel to the vector . Is each and everything of what I have done (and stated) in Part (b) correct? Or, have I made an error? P.S.: Part (b) Continued: Now let be any other point in . Then using the same argument as above but with replaced by we can conclude that our scalar field doesn't change throughout along any line parallel to the vector . Am I right? Is my reasoning correct?","f^\prime ( \mathbf{x}; \mathbf{y} ) = 0 \mathbf{x} n B(\mathbf{a}) \mathbf{y} f B(\mathbf{a}) f^\prime ( \mathbf{x}; \mathbf{y} ) = 0 \mathbf{y} \mathbf{x} B(\mathbf{a}) f f^\prime ( \mathbf{a} + t \mathbf{y}; \mathbf{y} ) t 0 \leq t \leq 1 \theta 0 < \theta < 1  f ( \mathbf{a} + \mathbf{y} ) - f(  \mathbf{a} ) = f^\prime (  \mathbf{z};  \mathbf{y} ), \ \mbox{ where } \  \mathbf{z} =  \mathbf{a} + \theta  \mathbf{y}.  \mathbf{x} n B( \mathbf{a}  ) \mathbf{y} \colon=  \mathbf{x} -  \mathbf{a} \mathbf{x} =  \mathbf{a} +  \mathbf{y}.  \theta 0 \leq \theta \leq 1  \mathbf{a} + \theta \mathbf{y}   n B( \mathbf{a} )  f^\prime ( \mathbf{a} + \theta \mathbf{y} ; \mathbf{y} ) = 0.  \theta 0 < \theta < 1  f(\mathbf{x}) - f(\mathbf{a}) = f(\mathbf{a} + \mathbf{y} ) - f( \mathbf{a}) = f^\prime ( \mathbf{a} + \theta \mathbf{y}; \mathbf{y} ) = 0,   f(\mathbf{x}) = f(\mathbf{a})  \mathbf{x} n f n f^\prime ( \mathbf{x}; \mathbf{y} ) = 0 \mathbf{y} \mathbf{x} B( \mathbf{a} ) t \mathbf{a} + t \mathbf{y} B( \mathbf{a} ) \theta \in (0, 1)  f( \mathbf{a} + t \mathbf{y} ) - f( \mathbf{a} ) = f^\prime ( \mathbf{a} + \theta t \mathbf{y}; t \mathbf{y} ) = f^\prime ( \mathbf{a} + \theta t \mathbf{y};  \mathbf{y} ) = 0,   f( \mathbf{a} + t \mathbf{y} ) = f( \mathbf{a} ),  f \mathbf{a} \mathbf{y} \mathbf{x} B(\mathbf{a}) \mathbf{a} \mathbf{x} f B(\mathbf{a}) \mathbf{y}","['calculus', 'real-analysis', 'analysis', 'multivariable-calculus', 'derivatives']"
4,Divergence theorem and change of coordinates,Divergence theorem and change of coordinates,,"A friend and I were working on a Calculus (III) assignment and we both came to different results, but I can't figure out just why. It was something like the following: If $F(x,y,z)=(x^3,y^3,z)$ and $K$ is the sphere $x^2+y^2+z^2=1$, calculate the flux of $F$ through $K$ utilizing the divergence theorem. (Spherical coordinates could be useful). We are supposed to solve this by using Wolfram Mathematica. This is what we did: Me: I changed the coordinates system right away from cartesian to spherical, and got  $$ F'=(\rho^3\sin^3\theta\sin^3\phi,\rho^3\cos^3\theta\sin^3\phi,\rho\cos\phi).$$ I then calculated the divergence for spherical coordinates and got $$Div(F')=\rho^2\sin^2\phi\left(4\cos\phi\cos^3\theta+5\sin\phi\sin^3\theta\right). $$ Finally, the flux would be  $$ f_1=\int_0^\pi\int_0^{2\pi}\int_0^1 Div(F')\rho^2\sin\phi\,d\rho\,d\theta\,d\phi=0 $$ Them: First, calculating the divergence of $F$ for cartesian coordinates, they got $$Div(F)=1+3x^2+3y^2.$$ Then, changing the coordinates:  $$ Div(F')=1+\rho^2\left(3\cos^2\theta\sin^2\phi+3\sin^2\phi\sin^2\theta\right) $$ Finally, the flux would be $$f_2=\int_0^\pi\int_0^{2\pi}\int_0^1 Div(F')\rho^2\sin\phi\,d\rho\,d\theta\,d\phi=\frac{44\pi}{15}$$ Why are the results different? Why is there a difference in calculating the divergence before and after the change of coordinates? Note 1: I suspect I'm wrong here. Note 2: The deadline was yesterday, I just want to learn.","A friend and I were working on a Calculus (III) assignment and we both came to different results, but I can't figure out just why. It was something like the following: If $F(x,y,z)=(x^3,y^3,z)$ and $K$ is the sphere $x^2+y^2+z^2=1$, calculate the flux of $F$ through $K$ utilizing the divergence theorem. (Spherical coordinates could be useful). We are supposed to solve this by using Wolfram Mathematica. This is what we did: Me: I changed the coordinates system right away from cartesian to spherical, and got  $$ F'=(\rho^3\sin^3\theta\sin^3\phi,\rho^3\cos^3\theta\sin^3\phi,\rho\cos\phi).$$ I then calculated the divergence for spherical coordinates and got $$Div(F')=\rho^2\sin^2\phi\left(4\cos\phi\cos^3\theta+5\sin\phi\sin^3\theta\right). $$ Finally, the flux would be  $$ f_1=\int_0^\pi\int_0^{2\pi}\int_0^1 Div(F')\rho^2\sin\phi\,d\rho\,d\theta\,d\phi=0 $$ Them: First, calculating the divergence of $F$ for cartesian coordinates, they got $$Div(F)=1+3x^2+3y^2.$$ Then, changing the coordinates:  $$ Div(F')=1+\rho^2\left(3\cos^2\theta\sin^2\phi+3\sin^2\phi\sin^2\theta\right) $$ Finally, the flux would be $$f_2=\int_0^\pi\int_0^{2\pi}\int_0^1 Div(F')\rho^2\sin\phi\,d\rho\,d\theta\,d\phi=\frac{44\pi}{15}$$ Why are the results different? Why is there a difference in calculating the divergence before and after the change of coordinates? Note 1: I suspect I'm wrong here. Note 2: The deadline was yesterday, I just want to learn.",,"['multivariable-calculus', 'vector-analysis', 'coordinate-systems', 'spherical-coordinates']"
5,Find distance between 3d point and 3d curve,Find distance between 3d point and 3d curve,,I have a 3 dimensional curve given by the equation: $$\begin{bmatrix}x_0 \\ y_0 \\ z_0 \end{bmatrix} +t\begin{bmatrix}a \\ b \\ c \end{bmatrix}+t^2\begin{bmatrix}0 \\ 0 \\ g \end{bmatrix}$$ How do I find the smallest distance between this curve and a vector like$\begin{bmatrix}x \\ y \\ z \end{bmatrix}$. I tried to calculate distance in terms of t and take the derivative but it got very complex very quickly. Is there a simpler way of doing this or do I need to just keep plugging away at the derivative?,I have a 3 dimensional curve given by the equation: $$\begin{bmatrix}x_0 \\ y_0 \\ z_0 \end{bmatrix} +t\begin{bmatrix}a \\ b \\ c \end{bmatrix}+t^2\begin{bmatrix}0 \\ 0 \\ g \end{bmatrix}$$ How do I find the smallest distance between this curve and a vector like$\begin{bmatrix}x \\ y \\ z \end{bmatrix}$. I tried to calculate distance in terms of t and take the derivative but it got very complex very quickly. Is there a simpler way of doing this or do I need to just keep plugging away at the derivative?,,"['multivariable-calculus', 'vectors']"
6,"Find the following limit: $\lim \limits_{x,y \to 0,0} \frac{x+y-\frac{1}{2}y^2}{\sin\left(y\right)+\log\left(1+x\right)}$",Find the following limit:,"\lim \limits_{x,y \to 0,0} \frac{x+y-\frac{1}{2}y^2}{\sin\left(y\right)+\log\left(1+x\right)}","Recently I came upon a limit which confused me. The reason is that when I try to solve the following limit using polar coordinates I get a constant which I do not know if it gives me information. Let : $$\lim_{x,y \to 0} \frac{x+y-\frac{1}{2}y^2}{\sin\left(y\right)+\log\left(1+x\right)}$$ Using polar coordnates I get this:  $$\lim_{r \to 0} \frac{r\cos\left(\theta\right)+r\sin\left(\theta\right)-\frac{1}{2}r^2\sin^2\left(\theta\right)}{\sin\left(r\sin\left(\theta\right)\right)+\log\left(1+r\cos\left(\theta\right)\right)}$$ Which is equal to: $$\lim_{r \to 0} \frac{r\cos\left(\theta\right)+r\sin\left(\theta\right)-\frac{1}{2}r^2\sin^2\left(\theta\right)}{r\sin\left(\theta\right)+r\cos\left(\theta\right)}=1$$ I already know this limit does not exist. Actually it was quite difficult to find a path for which I get a different limit... My question is: If I use polar coordinates and the result is not something that depends on $r,\theta$ then what I get is basically useless information? (I know that if that limit goes to infinity the limit of the function does not exist)","Recently I came upon a limit which confused me. The reason is that when I try to solve the following limit using polar coordinates I get a constant which I do not know if it gives me information. Let : $$\lim_{x,y \to 0} \frac{x+y-\frac{1}{2}y^2}{\sin\left(y\right)+\log\left(1+x\right)}$$ Using polar coordnates I get this:  $$\lim_{r \to 0} \frac{r\cos\left(\theta\right)+r\sin\left(\theta\right)-\frac{1}{2}r^2\sin^2\left(\theta\right)}{\sin\left(r\sin\left(\theta\right)\right)+\log\left(1+r\cos\left(\theta\right)\right)}$$ Which is equal to: $$\lim_{r \to 0} \frac{r\cos\left(\theta\right)+r\sin\left(\theta\right)-\frac{1}{2}r^2\sin^2\left(\theta\right)}{r\sin\left(\theta\right)+r\cos\left(\theta\right)}=1$$ I already know this limit does not exist. Actually it was quite difficult to find a path for which I get a different limit... My question is: If I use polar coordinates and the result is not something that depends on $r,\theta$ then what I get is basically useless information? (I know that if that limit goes to infinity the limit of the function does not exist)",,"['limits', 'multivariable-calculus']"
7,continuity of a multivariable function2,continuity of a multivariable function2,,"I'm studying the continuity of the function $$ f(x,y) = \left\{     \begin{array}{l l}      \frac{x^2y^2}{x^2+y^2} & \quad , \quad(x,y)\neq(0,0)\\      0 & \quad , \quad(x,y)=(0,0)    \end{array} \right.$$ in the point $(x,y)=(0,0)$. It's clear to me that if a function is not continuous I have to find a case of discontinuity, but perhaps it's more difficult to prove the continuity in which I can't find a fault in the behaviour of the function. In this case how can I show the limit $ \lim_{(x,y)\rightarrow (0,0)}  f(x,y) = \lim_{(x,y)\rightarrow (0,0)}       \frac{x^2y^2}{x^2+y^2} =0$? It's simple to prove that on a line $y=mx$ through the origin $(0,0)$ but in the most general way?","I'm studying the continuity of the function $$ f(x,y) = \left\{     \begin{array}{l l}      \frac{x^2y^2}{x^2+y^2} & \quad , \quad(x,y)\neq(0,0)\\      0 & \quad , \quad(x,y)=(0,0)    \end{array} \right.$$ in the point $(x,y)=(0,0)$. It's clear to me that if a function is not continuous I have to find a case of discontinuity, but perhaps it's more difficult to prove the continuity in which I can't find a fault in the behaviour of the function. In this case how can I show the limit $ \lim_{(x,y)\rightarrow (0,0)}  f(x,y) = \lim_{(x,y)\rightarrow (0,0)}       \frac{x^2y^2}{x^2+y^2} =0$? It's simple to prove that on a line $y=mx$ through the origin $(0,0)$ but in the most general way?",,['multivariable-calculus']
8,Multivariable Limits Using Substitution,Multivariable Limits Using Substitution,,"Suppose  I want to calculate a multivariable limit using substitution. For example, $$ \lim _{(x,y)\to (0,0) } \frac{2^{xy}-1}{xy} $$ or  $$ \lim _{(x,y)\to (0,0) } \frac{\sin(xy) }{xy}. $$ Substituting $u=xy$ and then moving to a limit of $u\to 0$ gives a result. As far as I can understand, if the limit of $u\to 0 $ exists, then in particular, the above limits also exist (even though the case of $u\to 0$ can also correspond to e.g. $(x,y)\to(0,2) $). I am not sure if this is legitimate and if I am right. Is it possible that the substitution $u=xy$ and then $u\to 0 $ only corresponds to a specific path? Thanks in advance!","Suppose  I want to calculate a multivariable limit using substitution. For example, $$ \lim _{(x,y)\to (0,0) } \frac{2^{xy}-1}{xy} $$ or  $$ \lim _{(x,y)\to (0,0) } \frac{\sin(xy) }{xy}. $$ Substituting $u=xy$ and then moving to a limit of $u\to 0$ gives a result. As far as I can understand, if the limit of $u\to 0 $ exists, then in particular, the above limits also exist (even though the case of $u\to 0$ can also correspond to e.g. $(x,y)\to(0,2) $). I am not sure if this is legitimate and if I am right. Is it possible that the substitution $u=xy$ and then $u\to 0 $ only corresponds to a specific path? Thanks in advance!",,"['limits', 'multivariable-calculus']"
9,Solution to Dirichlet boundary value problem on upper halve plane using Green's function,Solution to Dirichlet boundary value problem on upper halve plane using Green's function,,"Currently I am studying for an exam about partial differential equations. While looking through some of the exercises concerning Green's function on the plane, I came across a rather impenetrable-seeming integral connected to a Dirichlet BVP. As the textbook we are using (Partial Differential Equations, Peter Olver) does not provide clear examples on this topic, I figured that the internet would be a fine next step towards a solution. I also think that some experienced mathematicians out there might enjoy an exercise, and explaining it. Let me be clear: this exercise will not be examined. The problem is as follows: Solve for $u$ where \begin{align} -\Delta u (x,y)= \frac{1}{1+y}\text{ on }\{(x,y):y>0\}\text{ and }u(x,0) = 0\text{ for all }x\in \mathbb{R}. \end{align} Now, we are asked to use the Green's function for the upper half plane to find a solution. Using the method of images, one finds such a function quite easily: \begin{align} G(x,y;\theta,\eta) = \frac{1}{4\pi}\log\left(\frac{(x-\theta)^2 + (y-\eta)^2}{(x-\theta)^2+(y+\eta)^2}\right). \end{align} By Green's representation formula, the solution is then given by \begin{align}u(x,y) = \frac{1}{4\pi}\int_{-\infty}^{\infty}\int_0^{\infty}\frac{1}{1+\eta}\log\left(\frac{(x-\theta)^2 + (y-\eta)^2}{(x-\theta)^2+(y+\eta)^2}\right)\,\mathrm{d}\eta\,\mathrm{d\theta}. \end{align} I am wondering, did the author perhaps choose this example problem because the resulting integral would be solvable? If so, could someone shed light on the problem? And if not, is there a better way to solve it? My thanks.","Currently I am studying for an exam about partial differential equations. While looking through some of the exercises concerning Green's function on the plane, I came across a rather impenetrable-seeming integral connected to a Dirichlet BVP. As the textbook we are using (Partial Differential Equations, Peter Olver) does not provide clear examples on this topic, I figured that the internet would be a fine next step towards a solution. I also think that some experienced mathematicians out there might enjoy an exercise, and explaining it. Let me be clear: this exercise will not be examined. The problem is as follows: Solve for $u$ where \begin{align} -\Delta u (x,y)= \frac{1}{1+y}\text{ on }\{(x,y):y>0\}\text{ and }u(x,0) = 0\text{ for all }x\in \mathbb{R}. \end{align} Now, we are asked to use the Green's function for the upper half plane to find a solution. Using the method of images, one finds such a function quite easily: \begin{align} G(x,y;\theta,\eta) = \frac{1}{4\pi}\log\left(\frac{(x-\theta)^2 + (y-\eta)^2}{(x-\theta)^2+(y+\eta)^2}\right). \end{align} By Green's representation formula, the solution is then given by \begin{align}u(x,y) = \frac{1}{4\pi}\int_{-\infty}^{\infty}\int_0^{\infty}\frac{1}{1+\eta}\log\left(\frac{(x-\theta)^2 + (y-\eta)^2}{(x-\theta)^2+(y+\eta)^2}\right)\,\mathrm{d}\eta\,\mathrm{d\theta}. \end{align} I am wondering, did the author perhaps choose this example problem because the resulting integral would be solvable? If so, could someone shed light on the problem? And if not, is there a better way to solve it? My thanks.",,"['integration', 'multivariable-calculus', 'partial-differential-equations', 'boundary-value-problem', 'greens-function']"
10,If $B \subset \mathbb R^m$ is compact and $x\in \mathbb R^n$ then $\{x\} \times B \subset \mathbb R^{n+m}$ is compact.,If  is compact and  then  is compact.,B \subset \mathbb R^m x\in \mathbb R^n \{x\} \times B \subset \mathbb R^{n+m},If $B \subset \mathbb R^m$ is compact and $x\in \mathbb R^n$ then $\{x\} \times B \subset \mathbb R^{n+m}$ is compact. If $ \mathcal O$ is open cover of  $\{x\} \times B \subset \mathbb R^{n+m}$ then as $B$ is compact there is finite cover $ \mathcal O'$ of $B$ and hence finite open sets $U$ s.t. $ \{x\} \times U' \subset U \in \mathcal O $ where $U' \in \mathcal O'$. Is this argument correct?,If $B \subset \mathbb R^m$ is compact and $x\in \mathbb R^n$ then $\{x\} \times B \subset \mathbb R^{n+m}$ is compact. If $ \mathcal O$ is open cover of  $\{x\} \times B \subset \mathbb R^{n+m}$ then as $B$ is compact there is finite cover $ \mathcal O'$ of $B$ and hence finite open sets $U$ s.t. $ \{x\} \times U' \subset U \in \mathcal O $ where $U' \in \mathcal O'$. Is this argument correct?,,"['real-analysis', 'multivariable-calculus', 'compactness']"
11,What is the difference between $(u \cdot \nabla)v$ and $u\cdot(\nabla v)$ when written in Einstein notation?,What is the difference between  and  when written in Einstein notation?,(u \cdot \nabla)v u\cdot(\nabla v),"What is the difference between $(u \cdot \nabla)v$ and $u\cdot(\nabla v)$ when written in Einstein notation? I understand that they are different, but I'm not quite sure how. I've proven that $u \cdot (\nabla u)=\frac{1}{2}\nabla (u\cdot u)-u \times (\nabla \times u)$ . I want to prove stuff with $(u \cdot \nabla)v$ but I don't know how to write it in Einstein notation. Both $u$ and $v$ are vector fields.","What is the difference between and when written in Einstein notation? I understand that they are different, but I'm not quite sure how. I've proven that . I want to prove stuff with but I don't know how to write it in Einstein notation. Both and are vector fields.",(u \cdot \nabla)v u\cdot(\nabla v) u \cdot (\nabla u)=\frac{1}{2}\nabla (u\cdot u)-u \times (\nabla \times u) (u \cdot \nabla)v u v,"['multivariable-calculus', 'index-notation']"
12,Calculus: substitution for multiple variables,Calculus: substitution for multiple variables,,"Problem I have a question about variable substitution for a multivariate integral. Let me first pose the question and then provide some context, since I think the context is not necessarily very important. Say we have ( for example ) the following integral  $$\int_0^1 \int_0^{1-\varepsilon_1} \int_0^{1-\varepsilon_1 -\varepsilon_2} 24 (1- \varepsilon_1 - \varepsilon_2 - \varepsilon_3) d\varepsilon_3 d\varepsilon_2 d\varepsilon_1= 1$$ Furthermore, let $z = \varepsilon_1 + \varepsilon_2 + \varepsilon_3$. I'm trying to find $f(z)$ satisfying $$\int_0^1 f(z) dz = 1$$ corresponding to the integral above. At first glance, this seems like an easy problem, but whatever I do, I cannot seem to eliminate all the epsilon! I suppose there are three cases: $$\text{substituting } \varepsilon_1 = z - \varepsilon_2 - \varepsilon_3: \qquad \int_{\varepsilon_2 + \varepsilon_3}^{1+\varepsilon_2 + \varepsilon_3} \int_0^{1-\varepsilon_1} \int_{0}^{1- \varepsilon_1 - \varepsilon_2} 24 (1- z) d\varepsilon_3 d\varepsilon_2 dz$$ $$\text{substituting } \varepsilon_2 = z - \varepsilon_1 - \varepsilon_3: \qquad \int_0^1 \int_{\varepsilon_1 + \varepsilon_3}^{1+\varepsilon_3} \int_{0}^{1- \varepsilon_1 - \varepsilon_2} 24 (1- z) d\varepsilon_3 dz d\varepsilon_1$$ $$\text{substituting } \varepsilon_3 = z - \varepsilon_1 - \varepsilon_2: \qquad \int_0^1 \int_0^{1-\varepsilon_1} \int_{\varepsilon_1 + \varepsilon_2}^{1} 24 (1- z) dz d\varepsilon_2 d\varepsilon_1$$ The first two integrals will actually not evaluate to a numerical value. Only the last one will, but $z$ is eliminated in the first step (which is not what I want of course). So my question is: can I partially solve this integral in a way that leaves me with a function $f(z)$? Context I am trying to compute the convolution of a linear combination of three dependent variables $\varepsilon_1$, $\varepsilon_2$ and $\varepsilon_3$. So I am trying to determine the distribution of $\alpha_1 \varepsilon_1 + \alpha_2 \varepsilon_2 + \alpha_3 \varepsilon_3 + \cdots + \alpha_n \varepsilon_n$. I was able to determine the joint distribution $f(\alpha_1 \varepsilon_1, \ldots, \alpha_n \varepsilon_n)$, so I really only have the integration problem left to solve.","Problem I have a question about variable substitution for a multivariate integral. Let me first pose the question and then provide some context, since I think the context is not necessarily very important. Say we have ( for example ) the following integral  $$\int_0^1 \int_0^{1-\varepsilon_1} \int_0^{1-\varepsilon_1 -\varepsilon_2} 24 (1- \varepsilon_1 - \varepsilon_2 - \varepsilon_3) d\varepsilon_3 d\varepsilon_2 d\varepsilon_1= 1$$ Furthermore, let $z = \varepsilon_1 + \varepsilon_2 + \varepsilon_3$. I'm trying to find $f(z)$ satisfying $$\int_0^1 f(z) dz = 1$$ corresponding to the integral above. At first glance, this seems like an easy problem, but whatever I do, I cannot seem to eliminate all the epsilon! I suppose there are three cases: $$\text{substituting } \varepsilon_1 = z - \varepsilon_2 - \varepsilon_3: \qquad \int_{\varepsilon_2 + \varepsilon_3}^{1+\varepsilon_2 + \varepsilon_3} \int_0^{1-\varepsilon_1} \int_{0}^{1- \varepsilon_1 - \varepsilon_2} 24 (1- z) d\varepsilon_3 d\varepsilon_2 dz$$ $$\text{substituting } \varepsilon_2 = z - \varepsilon_1 - \varepsilon_3: \qquad \int_0^1 \int_{\varepsilon_1 + \varepsilon_3}^{1+\varepsilon_3} \int_{0}^{1- \varepsilon_1 - \varepsilon_2} 24 (1- z) d\varepsilon_3 dz d\varepsilon_1$$ $$\text{substituting } \varepsilon_3 = z - \varepsilon_1 - \varepsilon_2: \qquad \int_0^1 \int_0^{1-\varepsilon_1} \int_{\varepsilon_1 + \varepsilon_2}^{1} 24 (1- z) dz d\varepsilon_2 d\varepsilon_1$$ The first two integrals will actually not evaluate to a numerical value. Only the last one will, but $z$ is eliminated in the first step (which is not what I want of course). So my question is: can I partially solve this integral in a way that leaves me with a function $f(z)$? Context I am trying to compute the convolution of a linear combination of three dependent variables $\varepsilon_1$, $\varepsilon_2$ and $\varepsilon_3$. So I am trying to determine the distribution of $\alpha_1 \varepsilon_1 + \alpha_2 \varepsilon_2 + \alpha_3 \varepsilon_3 + \cdots + \alpha_n \varepsilon_n$. I was able to determine the joint distribution $f(\alpha_1 \varepsilon_1, \ldots, \alpha_n \varepsilon_n)$, so I really only have the integration problem left to solve.",,"['integration', 'multivariable-calculus', 'convolution', 'substitution']"
13,Limit of multivariable function including absolute value,Limit of multivariable function including absolute value,,"I have some trouble with this limit: $$\lim_{(x,y)\to (0,0)} \frac{x^3+y^2}{x^2+|y|}.$$ My first attempt to solve it was with polar coordinates but I couldn't find an expression which was independent of $\varphi$. Now I'm trying to solve it with the triangle inequality so I can find an upper limit that squeezes my expression into $0$. My work so far: $$\left| \frac{x^3+y^2}{x^2+|y|} \right| \leq \frac{|x^3+y^2|}{|x^2+|y||} \leq \frac{|x^3|+|y^2|}{|x^2|+||y||} = \frac{|x^3|+y^2}{x^2+|y|}.$$ But from here I don't know how to continue. Thanks in advance!","I have some trouble with this limit: $$\lim_{(x,y)\to (0,0)} \frac{x^3+y^2}{x^2+|y|}.$$ My first attempt to solve it was with polar coordinates but I couldn't find an expression which was independent of $\varphi$. Now I'm trying to solve it with the triangle inequality so I can find an upper limit that squeezes my expression into $0$. My work so far: $$\left| \frac{x^3+y^2}{x^2+|y|} \right| \leq \frac{|x^3+y^2|}{|x^2+|y||} \leq \frac{|x^3|+|y^2|}{|x^2|+||y||} = \frac{|x^3|+y^2}{x^2+|y|}.$$ But from here I don't know how to continue. Thanks in advance!",,"['limits', 'multivariable-calculus', 'absolute-value']"
14,Cant understand how chain rule works,Cant understand how chain rule works,,"Let $w(x,y)$ be a function of class $C^2$ in the variables $x$ and $y$, and let $x=u+v$, $y=u-v$, show that: \begin{align} \frac{\partial^2 w}{\partial u \partial v} = \frac{\partial^2 f}{\partial x^2} -  \frac{\partial^2 f}{\partial y^2}\end{align} My attempt: What we are looking for is $\frac{\partial}{\partial u}(\frac{\partial w}{\partial v})$, so, by the chain rule: $$\frac{\partial w}{\partial v} = \frac{\partial f}{\partial x}\frac{\partial x}{\partial v} + \frac{\partial f}{\partial y}\frac{\partial y}{\partial v} = \frac{\partial f}{\partial x} - \frac{\partial f}{\partial y}$$ and similarly for $\frac{\partial w}{\partial u}$. Now, what we need is: $$\frac{\partial}{\partial u}(\frac{\partial w}{\partial v}) = \frac{\partial}{\partial u}(\frac{\partial f}{\partial x} - \frac{\partial f}{\partial y}) = \frac{\partial}{\partial u}(\frac{\partial f}{\partial x}) - \frac{\partial}{\partial u}(\frac{\partial f}{\partial y})$$ But I can’t seem to grasp what $\frac{\partial}{\partial u}(\frac{\partial f}{\partial x})$ is or how am I supposed to apply the chain rule again in this case.","Let $w(x,y)$ be a function of class $C^2$ in the variables $x$ and $y$, and let $x=u+v$, $y=u-v$, show that: \begin{align} \frac{\partial^2 w}{\partial u \partial v} = \frac{\partial^2 f}{\partial x^2} -  \frac{\partial^2 f}{\partial y^2}\end{align} My attempt: What we are looking for is $\frac{\partial}{\partial u}(\frac{\partial w}{\partial v})$, so, by the chain rule: $$\frac{\partial w}{\partial v} = \frac{\partial f}{\partial x}\frac{\partial x}{\partial v} + \frac{\partial f}{\partial y}\frac{\partial y}{\partial v} = \frac{\partial f}{\partial x} - \frac{\partial f}{\partial y}$$ and similarly for $\frac{\partial w}{\partial u}$. Now, what we need is: $$\frac{\partial}{\partial u}(\frac{\partial w}{\partial v}) = \frac{\partial}{\partial u}(\frac{\partial f}{\partial x} - \frac{\partial f}{\partial y}) = \frac{\partial}{\partial u}(\frac{\partial f}{\partial x}) - \frac{\partial}{\partial u}(\frac{\partial f}{\partial y})$$ But I can’t seem to grasp what $\frac{\partial}{\partial u}(\frac{\partial f}{\partial x})$ is or how am I supposed to apply the chain rule again in this case.",,"['multivariable-calculus', 'chain-rule']"
15,The shortest distance from a point to the graph of the function,The shortest distance from a point to the graph of the function,,"To compute the distance from the point (5,5) to the graph of xy=4. I choose an arbitrary point (u,v) on the graph of $xy=4$.  I get $d(u,v)=\sqrt{(u-5)^2+(v-5)^2}$ again $(u,v)$ satisfies equation of hyperbola so that $uv=4$. Now what shall i do next?","To compute the distance from the point (5,5) to the graph of xy=4. I choose an arbitrary point (u,v) on the graph of $xy=4$.  I get $d(u,v)=\sqrt{(u-5)^2+(v-5)^2}$ again $(u,v)$ satisfies equation of hyperbola so that $uv=4$. Now what shall i do next?",,['multivariable-calculus']
16,Sobolev norms related to affine maps.,Sobolev norms related to affine maps.,,"Goal I wish to prove that it is possible to relate the Sobolev norms on an arbitrary triangle $K$ to Sobolev norms on a reference triangle $\hat{K}$. Preliminaries To this end: Let $F \colon \hat{K} \to K$ be an invertible affine map given by $F(\hat{x}) = B\hat{x} + c$. For a function $\hat{v} \in C^2(\hat{K})$, we define the corresponding function $v \in C^2(K)$ by $$    v(x) = (\hat{v}\circ F^{-1})(x). $$ I am interested in giving a bound for the Sobolev semi-norm $|v|_{2, K}$ in terms of $|\hat{v}|_{2, \hat{K}}$ and the matrix $B$. I use the definitions $$ |v|_{2, K} = \left(\int_{K} \sum_{|\alpha| = 2} |\partial^\alpha v(x)|^2 dx \right)^{1/2} $$ where $\partial^\alpha v$ is the the mixed partial derivatives of order $|\alpha| = 2$. What I have tried I started by trying to bound $|\partial^\alpha v(x)|$ in terms of the directional derivatives $$    |\partial^\alpha v(x)| \leq \sup_{\|\xi_1\|, \|\xi_2\| \leq 1} |\nabla ((\nabla v(x))\cdot \xi_1)\cdot \xi_2)| $$ and then use that since $v(x) = \hat{v}(\hat{x})$ this equals $$    |\partial^\alpha v(x)| \leq \sup_{\|\xi_1\|, \|\xi_2\| \leq 1} |\nabla ((\nabla v(x))\cdot \xi_1)\cdot \xi_2)| \\  = \sup_{\|\xi_1\|, \|\xi_2\| \leq 1} |\nabla ((\nabla v(x))\cdot B^{-1}\xi_1)\cdot B^{-1}\xi_2)| \\  \leq \sup_{\|\xi_1\|, \|\xi_2\| \leq 1} |\nabla ((\nabla v(x))\cdot \xi_1)\cdot \xi_2)|\| B^{-1} \|^{2}, $$ however - I get lost in the notation. Am I on the right path, and is there any better notation for working with derivatives in this fashion?","Goal I wish to prove that it is possible to relate the Sobolev norms on an arbitrary triangle $K$ to Sobolev norms on a reference triangle $\hat{K}$. Preliminaries To this end: Let $F \colon \hat{K} \to K$ be an invertible affine map given by $F(\hat{x}) = B\hat{x} + c$. For a function $\hat{v} \in C^2(\hat{K})$, we define the corresponding function $v \in C^2(K)$ by $$    v(x) = (\hat{v}\circ F^{-1})(x). $$ I am interested in giving a bound for the Sobolev semi-norm $|v|_{2, K}$ in terms of $|\hat{v}|_{2, \hat{K}}$ and the matrix $B$. I use the definitions $$ |v|_{2, K} = \left(\int_{K} \sum_{|\alpha| = 2} |\partial^\alpha v(x)|^2 dx \right)^{1/2} $$ where $\partial^\alpha v$ is the the mixed partial derivatives of order $|\alpha| = 2$. What I have tried I started by trying to bound $|\partial^\alpha v(x)|$ in terms of the directional derivatives $$    |\partial^\alpha v(x)| \leq \sup_{\|\xi_1\|, \|\xi_2\| \leq 1} |\nabla ((\nabla v(x))\cdot \xi_1)\cdot \xi_2)| $$ and then use that since $v(x) = \hat{v}(\hat{x})$ this equals $$    |\partial^\alpha v(x)| \leq \sup_{\|\xi_1\|, \|\xi_2\| \leq 1} |\nabla ((\nabla v(x))\cdot \xi_1)\cdot \xi_2)| \\  = \sup_{\|\xi_1\|, \|\xi_2\| \leq 1} |\nabla ((\nabla v(x))\cdot B^{-1}\xi_1)\cdot B^{-1}\xi_2)| \\  \leq \sup_{\|\xi_1\|, \|\xi_2\| \leq 1} |\nabla ((\nabla v(x))\cdot \xi_1)\cdot \xi_2)|\| B^{-1} \|^{2}, $$ however - I get lost in the notation. Am I on the right path, and is there any better notation for working with derivatives in this fashion?",,"['multivariable-calculus', 'derivatives', 'notation', 'sobolev-spaces']"
17,Solving least squares problem using partial derivatives,Solving least squares problem using partial derivatives,,"Let's say we want to solve a linear regression problem by choosing the best slope and bias with the least squared errors. As example, let the points be $x=[1, 2, 3]$ and $y=[1,2,2]$ . This quadratic minimization problem can also be represented as: $||Ax-b||^2=||e||^2=e^2_1+e^2_2+e^2_3$ Linear Algebra We could solve this problem by utilizing linear algebraic methods. We could use projections. For projecting on the $0+$ dimensional subspaces. Projection equation $p = Ax = A(A^TA)^{-1}A^Tb$ could be utilized: $A^T(b-Ax)=0$ $A^TAx = A^Tb$ $x = (A^TA)^{-1} A^Tb$ We know the inner product of $A^T$ and $e=b-p=b-Ax$ is $0$ since they are orthogonal (or since $e$ is in the null space of $A^T$ ). Which is the reason why we got the equation above. Now we need to present the quadratic minimization problem in linear algebra $Ax=b$ : $\begin{bmatrix}1 & 1 \\ 1 & 2 \\ 1 & 3 \end{bmatrix}\begin{bmatrix}c \\m\end{bmatrix} = \begin{bmatrix}1 \\ 2 \\ 2 \end{bmatrix}$ where $c$ is bias and $m$ is slope. We can see that matrix $A$ is a basis for the column space, $c$ and $m$ are linear coefficients and $b$ represents range of the function. Considering that this equation doesn't have direct solution, then we are looking for projection of the vector $b$ on the column space of matrix $A$ . Solution : Let $Proj(x)$ be the projection function (where $x$ contains unknown coefficients that we are trying to find, in this case $[c, m]^T$ ): $Proj(x) = Proj\left(\begin{bmatrix}c \\ m \end{bmatrix}\right) = (A^TA)^{-1}A^Tb = \left(\begin{bmatrix}1 & 1 & 1 \\ 1 & 2 & 3\end{bmatrix}\begin{bmatrix}1 & 1 \\ 1 & 2 \\ 1 &  3\\ \end{bmatrix}\right)^{-1} \begin{bmatrix}1 & 1 & 1 \\ 1 & 2 & 3\end{bmatrix}\begin{bmatrix}1 \\ 2 \\  2\\ \end{bmatrix} = \left(\begin{bmatrix}3 & 6 \\ 6 & 14 \end{bmatrix}\right)^{-1}\begin{bmatrix}5 \\ 11 \end{bmatrix}=\left(\frac{1}{3(14)-6(6)}\begin{bmatrix}14 & -6 \\ -6 & 3  \end{bmatrix}\right)\begin{bmatrix}5 \\ 11 \end{bmatrix}=\begin{bmatrix}2.33333333 & -1 \\ -1 & 0.5  \end{bmatrix}\begin{bmatrix}5 \\ 11 \end{bmatrix} = \begin{bmatrix}0.66666667 \\ 0.5 \end{bmatrix}$ Multivariable Calculus At this point of the lecture Professor Strang presents the minimization problem as $A^TAx=A^Tb$ and shows the normal equations. Then he proceeds solving minimization problem using partial derivatives, although I couldn't quite understand how could partial differentiation be used to solve this problem. From what I know, partial derivatives can be used to find derivatives for the structures that are in higher dimensions. Since for example finding full derivative at certain point of a 3 dimensional object may not be possible since it can have infinite tangent lines. Although, by treating one variable as a constant can be utilized to solve the differentiation problem, and this process is called partial differentiation from my knowledge. Question How does partial differentiation solution exactly work? How can it be compared to the linear algebraic orthogonal projection solution? Thank you!","Let's say we want to solve a linear regression problem by choosing the best slope and bias with the least squared errors. As example, let the points be and . This quadratic minimization problem can also be represented as: Linear Algebra We could solve this problem by utilizing linear algebraic methods. We could use projections. For projecting on the dimensional subspaces. Projection equation could be utilized: We know the inner product of and is since they are orthogonal (or since is in the null space of ). Which is the reason why we got the equation above. Now we need to present the quadratic minimization problem in linear algebra : where is bias and is slope. We can see that matrix is a basis for the column space, and are linear coefficients and represents range of the function. Considering that this equation doesn't have direct solution, then we are looking for projection of the vector on the column space of matrix . Solution : Let be the projection function (where contains unknown coefficients that we are trying to find, in this case ): Multivariable Calculus At this point of the lecture Professor Strang presents the minimization problem as and shows the normal equations. Then he proceeds solving minimization problem using partial derivatives, although I couldn't quite understand how could partial differentiation be used to solve this problem. From what I know, partial derivatives can be used to find derivatives for the structures that are in higher dimensions. Since for example finding full derivative at certain point of a 3 dimensional object may not be possible since it can have infinite tangent lines. Although, by treating one variable as a constant can be utilized to solve the differentiation problem, and this process is called partial differentiation from my knowledge. Question How does partial differentiation solution exactly work? How can it be compared to the linear algebraic orthogonal projection solution? Thank you!","x=[1, 2, 3] y=[1,2,2] ||Ax-b||^2=||e||^2=e^2_1+e^2_2+e^2_3 0+ p = Ax = A(A^TA)^{-1}A^Tb A^T(b-Ax)=0 A^TAx = A^Tb x = (A^TA)^{-1} A^Tb A^T e=b-p=b-Ax 0 e A^T Ax=b \begin{bmatrix}1 & 1 \\ 1 & 2 \\ 1 & 3 \end{bmatrix}\begin{bmatrix}c \\m\end{bmatrix} = \begin{bmatrix}1 \\ 2 \\ 2 \end{bmatrix} c m A c m b b A Proj(x) x [c, m]^T Proj(x) = Proj\left(\begin{bmatrix}c \\ m \end{bmatrix}\right) = (A^TA)^{-1}A^Tb = \left(\begin{bmatrix}1 & 1 & 1 \\ 1 & 2 & 3\end{bmatrix}\begin{bmatrix}1 & 1 \\ 1 & 2 \\ 1 &  3\\ \end{bmatrix}\right)^{-1} \begin{bmatrix}1 & 1 & 1 \\ 1 & 2 & 3\end{bmatrix}\begin{bmatrix}1 \\ 2 \\  2\\ \end{bmatrix} = \left(\begin{bmatrix}3 & 6 \\ 6 & 14 \end{bmatrix}\right)^{-1}\begin{bmatrix}5 \\ 11 \end{bmatrix}=\left(\frac{1}{3(14)-6(6)}\begin{bmatrix}14 & -6 \\ -6 & 3  \end{bmatrix}\right)\begin{bmatrix}5 \\ 11 \end{bmatrix}=\begin{bmatrix}2.33333333 & -1 \\ -1 & 0.5  \end{bmatrix}\begin{bmatrix}5 \\ 11 \end{bmatrix} = \begin{bmatrix}0.66666667 \\ 0.5 \end{bmatrix} A^TAx=A^Tb","['linear-algebra', 'multivariable-calculus', 'partial-derivative', 'least-squares', 'projection']"
18,Some may explain this example: the implicit function theorem,Some may explain this example: the implicit function theorem,,"In the Munkres analysis on manifolds, after proving the implicit function theorem and saying that the choice of the last coordinates is given only for convenience, the following example: suppose $A$ open in $R^5$ and $f:A \rightarrow R^2$ is a function of class $C^r$. Suppose one wishes to ""solve"" the equation $f(x,y,z,u,v)=0$ for the two unknowns $y$ and $u$ in terms of the other three. In this case, the implict function theorem tell us that if $a$ is a point of $A$ such that $f(a)=0$ and $det\dfrac{\partial f}{\partial(y,u)}(a) \ne 0$, then one can solve for $y$ and $u$ locally near that point, say $y= \phi (x,z,v)$ and $y= \psi (x,z,v)$. Furthermore, the derivatives of $\phi$ and $\psi$ satisfy the formula $\dfrac{\partial (\phi,\psi)}{\partial(x,z,v)} = - \left[ \dfrac{\partial f}{\partial(y,u)} \right] ^{-1}.\dfrac{\partial f}{\partial(x,z,v)}$. I'm not understanding how to get into this final formula, I've already tried to call $H (x,z,v) = (\phi,\psi)$, but it does not work. I can not get into this equation. Thanks for any tips.","In the Munkres analysis on manifolds, after proving the implicit function theorem and saying that the choice of the last coordinates is given only for convenience, the following example: suppose $A$ open in $R^5$ and $f:A \rightarrow R^2$ is a function of class $C^r$. Suppose one wishes to ""solve"" the equation $f(x,y,z,u,v)=0$ for the two unknowns $y$ and $u$ in terms of the other three. In this case, the implict function theorem tell us that if $a$ is a point of $A$ such that $f(a)=0$ and $det\dfrac{\partial f}{\partial(y,u)}(a) \ne 0$, then one can solve for $y$ and $u$ locally near that point, say $y= \phi (x,z,v)$ and $y= \psi (x,z,v)$. Furthermore, the derivatives of $\phi$ and $\psi$ satisfy the formula $\dfrac{\partial (\phi,\psi)}{\partial(x,z,v)} = - \left[ \dfrac{\partial f}{\partial(y,u)} \right] ^{-1}.\dfrac{\partial f}{\partial(x,z,v)}$. I'm not understanding how to get into this final formula, I've already tried to call $H (x,z,v) = (\phi,\psi)$, but it does not work. I can not get into this equation. Thanks for any tips.",,"['real-analysis', 'multivariable-calculus']"
19,"Representation of a sphere as $\left\{\theta \in [0,2\pi], \phi \in [0,\pi], R\in [0, r]\right\}$",Representation of a sphere as,"\left\{\theta \in [0,2\pi], \phi \in [0,\pi], R\in [0, r]\right\}","In spherical coordinates, a sphere can be described as $S = \left\{\theta \in [0,2\pi], \phi \in [0,\pi], r\in [0, R]\right\}$ by letting  $x = r\sin \pi \cos \theta, ~ y= r \sin \phi \sin \theta,$ and $z= r\cos \phi$ in the equation $x^2+y^2+z^2 = R$. This apparently comes from the parametrisation after considering the top surface and bottom surface by drawing a picture. I've tried, but I'm honestly incapable of thinking geometrically. Question 1: Could someone explain how one can come to this representation algebraically? Question 2 : If we consider the region between the sphere $x^2+y^2+z^2 = R$ and the cone $z = \sqrt{x^2+y^2}$, the upper bound for $\phi$ changes and everything else stays the same:  we have $$S'=\left\{\theta \in [0,2\pi], \phi \in [0,\pi/4], r\in [0, R]\right\}$$ Why is that? I'm thinking because we're only considering a quarter of the sphere, but I'm not sure.","In spherical coordinates, a sphere can be described as $S = \left\{\theta \in [0,2\pi], \phi \in [0,\pi], r\in [0, R]\right\}$ by letting  $x = r\sin \pi \cos \theta, ~ y= r \sin \phi \sin \theta,$ and $z= r\cos \phi$ in the equation $x^2+y^2+z^2 = R$. This apparently comes from the parametrisation after considering the top surface and bottom surface by drawing a picture. I've tried, but I'm honestly incapable of thinking geometrically. Question 1: Could someone explain how one can come to this representation algebraically? Question 2 : If we consider the region between the sphere $x^2+y^2+z^2 = R$ and the cone $z = \sqrt{x^2+y^2}$, the upper bound for $\phi$ changes and everything else stays the same:  we have $$S'=\left\{\theta \in [0,2\pi], \phi \in [0,\pi/4], r\in [0, R]\right\}$$ Why is that? I'm thinking because we're only considering a quarter of the sphere, but I'm not sure.",,['multivariable-calculus']
20,A multiplier to make a couple of bounded functions continuous wrt one variable?,A multiplier to make a couple of bounded functions continuous wrt one variable?,,"Let $Q=(0,1)^2$ be a square, functions $f_1,f_2\in L_\infty(Q)$ and $f_1,f_2\ge c>0$ on $Q$. Does there exists for every pair of such $f_1,f_2$ a function $g>0$ on $Q$ s.t. function $f_1g$ is continuous wrt $x$ on $(0,1)$ for every $y\in(0,1)$ and $f_2g$ is continuous wrt $y$ for every $x\in(0,1)$?","Let $Q=(0,1)^2$ be a square, functions $f_1,f_2\in L_\infty(Q)$ and $f_1,f_2\ge c>0$ on $Q$. Does there exists for every pair of such $f_1,f_2$ a function $g>0$ on $Q$ s.t. function $f_1g$ is continuous wrt $x$ on $(0,1)$ for every $y\in(0,1)$ and $f_2g$ is continuous wrt $y$ for every $x\in(0,1)$?",,"['real-analysis', 'multivariable-calculus', 'continuity']"
21,Minimize modulus of three-variable function,Minimize modulus of three-variable function,,"Let $$ \begin{array}{l} f: [0,\frac{\pi}{2}]^3 \to {\mathbb R}^+, \\ (\theta_1,\theta_2,\theta_3) \mapsto |2+e^{i\theta_1}+e^{i\theta_2}+e^{i\theta_3}| \end{array} $$ Numerical values suggest that the minimum of $f$ is $\sqrt{13}$, and is attained at the following four points : the three permutations of $(0,\frac{\pi}{2},\frac{\pi}{2})$, and $(\frac{\pi}{2},\frac{\pi}{2},\frac{\pi}{2})$. But I was unable so far to show it rigorously, any help appreciated.","Let $$ \begin{array}{l} f: [0,\frac{\pi}{2}]^3 \to {\mathbb R}^+, \\ (\theta_1,\theta_2,\theta_3) \mapsto |2+e^{i\theta_1}+e^{i\theta_2}+e^{i\theta_3}| \end{array} $$ Numerical values suggest that the minimum of $f$ is $\sqrt{13}$, and is attained at the following four points : the three permutations of $(0,\frac{\pi}{2},\frac{\pi}{2})$, and $(\frac{\pi}{2},\frac{\pi}{2},\frac{\pi}{2})$. But I was unable so far to show it rigorously, any help appreciated.",,"['multivariable-calculus', 'complex-numbers']"
22,normal domains on the plane,normal domains on the plane,,"Let $R\subset\mathbb{R}^2$ be a normal domain, normal with respect to the x-axis ( the definition I'm using ) Let $\gamma:\mathbb{R}^2\to\mathbb{R}^2$ be a rotation. Then: A. $\gamma(R)$ is normal with respect to the x-axis B. $\gamma(R)$ can be normal with respect to the y-axis C. $\gamma(R)$ is normal with respect to the x-axis or to the y-axis D. $\gamma(R)$ can be normal with respect to the x-axis and to the y-axis even if $R$ was normal with respect to the x-axis only The half-annulus is a simple example showing that A is false and B is true. I would guess C-true and D-false, but I have no proof for my suspicions.","Let be a normal domain, normal with respect to the x-axis ( the definition I'm using ) Let be a rotation. Then: A. is normal with respect to the x-axis B. can be normal with respect to the y-axis C. is normal with respect to the x-axis or to the y-axis D. can be normal with respect to the x-axis and to the y-axis even if was normal with respect to the x-axis only The half-annulus is a simple example showing that A is false and B is true. I would guess C-true and D-false, but I have no proof for my suspicions.",R\subset\mathbb{R}^2 \gamma:\mathbb{R}^2\to\mathbb{R}^2 \gamma(R) \gamma(R) \gamma(R) \gamma(R) R,"['geometry', 'multivariable-calculus']"
23,what's $Df(A)(X)$ if $f(A) = \det(A)$?,what's  if ?,Df(A)(X) f(A) = \det(A),"let's begin with the simple case where $A$ is just a $2\times2$ matrix let $\begin{align} f :& \mathbb{R^{2\times2}} \to \mathbb{R} \\ & A \mapsto \det(A) \end{align}$ I want to find the differential of this mapping if $A$ is invertible. as a hint I was suggested to compute the following limit : $\lim_{t \to 0} \frac1t [\det(I+tX) -1]$ where $X \in \mathbb{R^{2\times2}}$ the limit turns out to be just the trace of $X$ and since $\det(I) = 1 $ we have that $Df(I)(X) = Tr(X)$, right ? so I guess now that if I want to find $Df(A)(X)$ for $A$ invertible I have to compute this limit : $\lim_{t \to 0} \frac1t [\det(A+tX) -\det(A)] = \det(A)\lim_{t \to 0} \frac1t [\det(I+tA^{-1}X) -1] $ so $Df(A)(X) = \det(A)Tr(A^{-1}X)$, right ? now in higher dimensions the last step wouldn't change and I guess that in the first limit the expression $[\det(I+tX) -1]$ would be something of the form $tTr(X) +t^2(\cdots) + t^3(\cdots)+\cdots$ so it's all cool but what if $A$ is not invertible ? the $\det$ being some sort of a polynomial would still be differentiable, right ? but how do you construct the differential in this case ? Edit : my bad if $A$ is not invertible then $\det(A) = 0$ so I guess $Df(A)(X) = 0$ ? can someone confirm this ? Edit 2 : at the end it all comes down to evaluating this : $$\lim_{t \to 0 } \frac1t \det(A+tX)$$ for $A$ non-invertible and $X \in \mathbb{R^{n\times n}}$","let's begin with the simple case where $A$ is just a $2\times2$ matrix let $\begin{align} f :& \mathbb{R^{2\times2}} \to \mathbb{R} \\ & A \mapsto \det(A) \end{align}$ I want to find the differential of this mapping if $A$ is invertible. as a hint I was suggested to compute the following limit : $\lim_{t \to 0} \frac1t [\det(I+tX) -1]$ where $X \in \mathbb{R^{2\times2}}$ the limit turns out to be just the trace of $X$ and since $\det(I) = 1 $ we have that $Df(I)(X) = Tr(X)$, right ? so I guess now that if I want to find $Df(A)(X)$ for $A$ invertible I have to compute this limit : $\lim_{t \to 0} \frac1t [\det(A+tX) -\det(A)] = \det(A)\lim_{t \to 0} \frac1t [\det(I+tA^{-1}X) -1] $ so $Df(A)(X) = \det(A)Tr(A^{-1}X)$, right ? now in higher dimensions the last step wouldn't change and I guess that in the first limit the expression $[\det(I+tX) -1]$ would be something of the form $tTr(X) +t^2(\cdots) + t^3(\cdots)+\cdots$ so it's all cool but what if $A$ is not invertible ? the $\det$ being some sort of a polynomial would still be differentiable, right ? but how do you construct the differential in this case ? Edit : my bad if $A$ is not invertible then $\det(A) = 0$ so I guess $Df(A)(X) = 0$ ? can someone confirm this ? Edit 2 : at the end it all comes down to evaluating this : $$\lim_{t \to 0 } \frac1t \det(A+tX)$$ for $A$ non-invertible and $X \in \mathbb{R^{n\times n}}$",,"['multivariable-calculus', 'differential-geometry']"
24,Simple L^2 bound for bivariate Sobolev function on a square,Simple L^2 bound for bivariate Sobolev function on a square,,"I have a rather basic question about Sobolev functions. I would need a reference or proof for the following inequality which seems to be well-known in approximation theory. Question: Let $\Omega=[x,x+h]\times[y,y+h]$ be a square of side-length $h$ and let $f\in H^s(\Omega)$ be a Sobolev funtion of regularity $s\in(1,2)$ such that $f=0$ on the vertices of $\Omega$. Does it hold that $$\|f\|^2_{L^2(\Omega)}\leq C h^{2s}\|f\|^2_{H^s(\Omega)}?$$ I would like to use such a bound to get the rate of the approximation error of a function on $[0,1]^2$ by its piecewise linearly interpolated counterpart on a grid of size $h$, which explains the assumptions of roots on the vertices of the grid. Here is my argument for the univariate case: Let $I=[x,x+h]$ and $g:I\to R$ such that $g(x)=g(x+h)=0$. Assume first that $g\in H^1(I)$. Then weak differentiability, $g(x)=0$ and Cauchy-Schwarz imply \begin{align}\|g\|^2_{L^2(I)}&=\int^{x+h}_xg(t)^2dt=\int^{x+h}_x\Big(g(x)+\int^t_xg'(s)ds\Big)^2dt\\ &\leq\int^{x+h}_x(t-x)\int^t_xg'(s)^2dsdt\leq\int^{x+h}_x(t-x)dt\int^{x+h}_xg'(s)^2ds\\ &=\frac{1}{2}h^2\|g'\|^2_{L^2(I)}\leq\frac{1}{2}h^2\|g\|^2_{H^1(I)}. \end{align} Assume now that $g\in H^2(I)$. Then additionally using that there is some $x_0\in I$ with $g'(x_0)=0$ (since $g$ has to have an extremum on $I$, by $g(x)=g(x+h)=0$) and applying Cauchy-Schwarz twice yields \begin{align}\|g\|^2_{L^2(I)}&=\int^{x+h}_xg(t)^2dt=\int^{x+h}_x\Big(g(x)+\int^t_xg'(s)ds\Big)^2dt\\ &=\int^{x+h}_x\Big(g(x)+\int^t_x\Big(g'(x_0)+\int^s_{x_0}g''(u)du\Big)ds\Big)^2dt\\ &=\int^{x+h}_x\Big(\int^t_x\Big(\int^s_{x_0}g''(u)du\Big)ds\Big)^2dt\\ &\leq\int^{x+h}_x(t-x)\int^t_x(s-x_0)\int^s_{x_0}g''(u)^2dudsdt\\ &\leq\int^{x+h}_x(t-x)\int^t_x(s-x)dsdt\int^{x+h}_xg''(u)^2du\\ &=\frac{1}{8}h^4\|g''\|^2_{L^2(I)}\leq\frac{1}{8}h^4\|g\|^2_{H^2(I)}. \end{align} Now an interpolation argument gives for $g\in H^s(I),s\in(1,2)$, the inequality $$\|f\|^2_{L^2(I)}\leq Ch^{2s}\|f\|^2_{H^s(I)}.$$ For the bivariate case I have a few problems. For instance if $f\in H^1(\Omega)$ the point evaluations are not necessarily well-defined since the approximated function might not be continuous. But even if I would assume that I do not have to worry about that the same approach would give me (using $f(x)=0$) $$\|f\|^2_{L^2(\Omega)}=\int_{\Omega}\Big(\int^1_0\langle\nabla f(t+u(t-x)),t-x\rangle\Big)^2dudt$$ and I am stuck at this point. For $f\in H^2(\Omega)$ I found a bound in this paper (by the proof of Lemma 1). Has anyone an idea or knows some helpful literature? Thank you!","I have a rather basic question about Sobolev functions. I would need a reference or proof for the following inequality which seems to be well-known in approximation theory. Question: Let $\Omega=[x,x+h]\times[y,y+h]$ be a square of side-length $h$ and let $f\in H^s(\Omega)$ be a Sobolev funtion of regularity $s\in(1,2)$ such that $f=0$ on the vertices of $\Omega$. Does it hold that $$\|f\|^2_{L^2(\Omega)}\leq C h^{2s}\|f\|^2_{H^s(\Omega)}?$$ I would like to use such a bound to get the rate of the approximation error of a function on $[0,1]^2$ by its piecewise linearly interpolated counterpart on a grid of size $h$, which explains the assumptions of roots on the vertices of the grid. Here is my argument for the univariate case: Let $I=[x,x+h]$ and $g:I\to R$ such that $g(x)=g(x+h)=0$. Assume first that $g\in H^1(I)$. Then weak differentiability, $g(x)=0$ and Cauchy-Schwarz imply \begin{align}\|g\|^2_{L^2(I)}&=\int^{x+h}_xg(t)^2dt=\int^{x+h}_x\Big(g(x)+\int^t_xg'(s)ds\Big)^2dt\\ &\leq\int^{x+h}_x(t-x)\int^t_xg'(s)^2dsdt\leq\int^{x+h}_x(t-x)dt\int^{x+h}_xg'(s)^2ds\\ &=\frac{1}{2}h^2\|g'\|^2_{L^2(I)}\leq\frac{1}{2}h^2\|g\|^2_{H^1(I)}. \end{align} Assume now that $g\in H^2(I)$. Then additionally using that there is some $x_0\in I$ with $g'(x_0)=0$ (since $g$ has to have an extremum on $I$, by $g(x)=g(x+h)=0$) and applying Cauchy-Schwarz twice yields \begin{align}\|g\|^2_{L^2(I)}&=\int^{x+h}_xg(t)^2dt=\int^{x+h}_x\Big(g(x)+\int^t_xg'(s)ds\Big)^2dt\\ &=\int^{x+h}_x\Big(g(x)+\int^t_x\Big(g'(x_0)+\int^s_{x_0}g''(u)du\Big)ds\Big)^2dt\\ &=\int^{x+h}_x\Big(\int^t_x\Big(\int^s_{x_0}g''(u)du\Big)ds\Big)^2dt\\ &\leq\int^{x+h}_x(t-x)\int^t_x(s-x_0)\int^s_{x_0}g''(u)^2dudsdt\\ &\leq\int^{x+h}_x(t-x)\int^t_x(s-x)dsdt\int^{x+h}_xg''(u)^2du\\ &=\frac{1}{8}h^4\|g''\|^2_{L^2(I)}\leq\frac{1}{8}h^4\|g\|^2_{H^2(I)}. \end{align} Now an interpolation argument gives for $g\in H^s(I),s\in(1,2)$, the inequality $$\|f\|^2_{L^2(I)}\leq Ch^{2s}\|f\|^2_{H^s(I)}.$$ For the bivariate case I have a few problems. For instance if $f\in H^1(\Omega)$ the point evaluations are not necessarily well-defined since the approximated function might not be continuous. But even if I would assume that I do not have to worry about that the same approach would give me (using $f(x)=0$) $$\|f\|^2_{L^2(\Omega)}=\int_{\Omega}\Big(\int^1_0\langle\nabla f(t+u(t-x)),t-x\rangle\Big)^2dudt$$ and I am stuck at this point. For $f\in H^2(\Omega)$ I found a bound in this paper (by the proof of Lemma 1). Has anyone an idea or knows some helpful literature? Thank you!",,"['multivariable-calculus', 'derivatives', 'sobolev-spaces', 'integral-inequality', 'approximation-theory']"
25,Working with limits of functions of the type $f: \mathbb{R}^2 \to \mathbb{R}$,Working with limits of functions of the type,f: \mathbb{R}^2 \to \mathbb{R},"I am trying to jump doing $\epsilon-\delta$ proofs of the limit of functions of one variable (i.e. $f:\mathbb{R} \to \mathbb{R}$), to functions of two variables (i.e. $g: \mathbb{R}^2 \to \mathbb{R}$) and I am struggling since some of the methods I learned for one variable do not immediately carry for two variables. Unfortunately, I find that my books are too eager to jump to nice theorems to calculate limits (limit of a sum is equal to the sum of the limits, etc.) but I insist on being able to find limits to the most elementary functions like $x+y,xy, x^2 + y^2, \frac{x}{y}, \frac{1}{x+y}$ using the $\epsilon-\delta$ definition (without more advanced notions like the topology of $\mathbb{R^n}$) of the limit until I gain some intuition to move on and prove more general theorems using this technique. In this post, I will show you one case I think I was able to work out correctly, and another where I am struggling. 1) Perharps the simplest case I could think was to show that $$\lim_{(x,y)\to(a,b)}(x+y) = a+b$$. In this case, the definition says that $f(x,y)=(x+y) \to (a+b)$ when $(x,y) \to (a,b)$ if $\forall \epsilon > 0$, $\exists \delta > 0$ such that for all $(x,y)$ in the domain of $f$, whenever $0<\sqrt{(x-a)^2+(y-b)^2}<\delta$, we have $|f(x,y) - (a+b)|<\epsilon$. For this, all I had to do was to realize that $\sqrt{(x-a)^2+(y-b)^2}<\delta$ implies that $|x-a|<\delta$ and $|y-b|<\delta$, therefore $|x + y - a -b| \le |x-a| + |y-b| < 2\delta$, therefore I should choose $\delta = \epsilon/2$. Is this line or reasoning correct? 2) Now, consider $$ \lim_{(x,y)\to(a,b)}xy = ab $$ working backwards, I want $$ \begin{align} |xy - ab| &< \epsilon\\ |(x-a+a)(y-b+b) -ab| &<\epsilon \\ |(x-a)(y-b) +b(x-a) +a(y-b)| &< \epsilon \qquad (*) \\ &\vdots\\ \sqrt{(x-a)^2+(y-b)^2}&<\delta \end{align} $$ (*) I got this idea from here . But that's it I am afraid. I don't know how to take it from there. I would appreciate any hint or guidance. Sorry if this post is too long. I wanted to provide context and background and would be happy to edit or split questions. Thank you in advance.","I am trying to jump doing $\epsilon-\delta$ proofs of the limit of functions of one variable (i.e. $f:\mathbb{R} \to \mathbb{R}$), to functions of two variables (i.e. $g: \mathbb{R}^2 \to \mathbb{R}$) and I am struggling since some of the methods I learned for one variable do not immediately carry for two variables. Unfortunately, I find that my books are too eager to jump to nice theorems to calculate limits (limit of a sum is equal to the sum of the limits, etc.) but I insist on being able to find limits to the most elementary functions like $x+y,xy, x^2 + y^2, \frac{x}{y}, \frac{1}{x+y}$ using the $\epsilon-\delta$ definition (without more advanced notions like the topology of $\mathbb{R^n}$) of the limit until I gain some intuition to move on and prove more general theorems using this technique. In this post, I will show you one case I think I was able to work out correctly, and another where I am struggling. 1) Perharps the simplest case I could think was to show that $$\lim_{(x,y)\to(a,b)}(x+y) = a+b$$. In this case, the definition says that $f(x,y)=(x+y) \to (a+b)$ when $(x,y) \to (a,b)$ if $\forall \epsilon > 0$, $\exists \delta > 0$ such that for all $(x,y)$ in the domain of $f$, whenever $0<\sqrt{(x-a)^2+(y-b)^2}<\delta$, we have $|f(x,y) - (a+b)|<\epsilon$. For this, all I had to do was to realize that $\sqrt{(x-a)^2+(y-b)^2}<\delta$ implies that $|x-a|<\delta$ and $|y-b|<\delta$, therefore $|x + y - a -b| \le |x-a| + |y-b| < 2\delta$, therefore I should choose $\delta = \epsilon/2$. Is this line or reasoning correct? 2) Now, consider $$ \lim_{(x,y)\to(a,b)}xy = ab $$ working backwards, I want $$ \begin{align} |xy - ab| &< \epsilon\\ |(x-a+a)(y-b+b) -ab| &<\epsilon \\ |(x-a)(y-b) +b(x-a) +a(y-b)| &< \epsilon \qquad (*) \\ &\vdots\\ \sqrt{(x-a)^2+(y-b)^2}&<\delta \end{align} $$ (*) I got this idea from here . But that's it I am afraid. I don't know how to take it from there. I would appreciate any hint or guidance. Sorry if this post is too long. I wanted to provide context and background and would be happy to edit or split questions. Thank you in advance.",,"['limits', 'multivariable-calculus']"
26,Trying to evaluate this triple integral?,Trying to evaluate this triple integral?,,"So I'm trying to evaluate the triple integral $$\displaystyle \iiint \limits_{R} \displaystyle \frac{1}{((x-a)^2+y^2+z^2)^{1/2}} \mathrm dV$$ for $a>1$ over the solid sphere $0 \leq x^2 + y^2 + z^2 \leq 1$. Apparently, there's an interpretation that I should be able to draw from this to. Not too sure what it is. So the first thing that came to mind when I saw the integral was to apply spherical coordinates, but this doesn't make the denominator of the integrand any less messy. Using spherical coordinates, the integrand becomes $$\displaystyle \iiint \limits_{R} \displaystyle\frac{1}{(\rho^2-2a\rho\sin\phi\cos\theta+a^2)^{1/2} } \rho^2\sin\phi \space\mathrm d\rho\mathrm d\phi\mathrm d\theta$$ (I haven't bothered to add the bounds yet), which doesn't look that much more friendly. Any support for this question would be appreciated.","So I'm trying to evaluate the triple integral $$\displaystyle \iiint \limits_{R} \displaystyle \frac{1}{((x-a)^2+y^2+z^2)^{1/2}} \mathrm dV$$ for $a>1$ over the solid sphere $0 \leq x^2 + y^2 + z^2 \leq 1$. Apparently, there's an interpretation that I should be able to draw from this to. Not too sure what it is. So the first thing that came to mind when I saw the integral was to apply spherical coordinates, but this doesn't make the denominator of the integrand any less messy. Using spherical coordinates, the integrand becomes $$\displaystyle \iiint \limits_{R} \displaystyle\frac{1}{(\rho^2-2a\rho\sin\phi\cos\theta+a^2)^{1/2} } \rho^2\sin\phi \space\mathrm d\rho\mathrm d\phi\mathrm d\theta$$ (I haven't bothered to add the bounds yet), which doesn't look that much more friendly. Any support for this question would be appreciated.",,"['multivariable-calculus', 'spherical-coordinates', 'multiple-integral', 'iterated-integrals']"
27,Can I perform 1D integration by substitution in multiple integrals?,Can I perform 1D integration by substitution in multiple integrals?,,"Suppose I have an integral of the form: $$\idotsint_D f({\bf{x}})dA = \int_{a_n}^{b_n}\ldots\int_{a_1}^{b_1}f({\bf{x}})dx_1\ldots dx_n$$ Can I perform a single variable calculus style integration by substition? If I fix all other variables in $f({\bf{x}})$ and consider $g(x_1) = f(x_1, c_2 \ldots, c_n)$, then I can make the substitution $u = h(x_1)$ and write $dx = \frac{1}{h'(x_1)}du$ to conclude: $$\int_{a_1}^{b_1}g(x_1)dx_1 = \int_{h(a_1)}^{h(b_1)}\hat{g}(u)du$$ Assuming the substitution is well chosen s.t. $g(x_1, u)\frac{1}{h'(x_1)} = \hat{g}(u)$ and the problem is more tractable. But if I try to carry this logic over to an iterated integral then I am transforming several variables, even if they are constant in this integral. Can I naively put these in the limits? I am not sure if I should start drawing pictures in the new system of coordinates to determine the limits I defined and if I should calculate Jacobians, or if I should just apply the formula from the 1 variable case. If it helps, the integral that confused me was: $$\int_{0}^{1} \int_{0}^{2}\frac{xy(x^2-y^2)}{(x^2+y^2)^3}dydx$$ With the substition $u = x^2 + y^2$. I'm not sure how I would even calculate a Jacobian for this, without trying to horribly force square roots to write $x$ as a $x(u)$?, for that I need enough relations for a $2\times2$ matrix of partial derivatives, this is just one.","Suppose I have an integral of the form: $$\idotsint_D f({\bf{x}})dA = \int_{a_n}^{b_n}\ldots\int_{a_1}^{b_1}f({\bf{x}})dx_1\ldots dx_n$$ Can I perform a single variable calculus style integration by substition? If I fix all other variables in $f({\bf{x}})$ and consider $g(x_1) = f(x_1, c_2 \ldots, c_n)$, then I can make the substitution $u = h(x_1)$ and write $dx = \frac{1}{h'(x_1)}du$ to conclude: $$\int_{a_1}^{b_1}g(x_1)dx_1 = \int_{h(a_1)}^{h(b_1)}\hat{g}(u)du$$ Assuming the substitution is well chosen s.t. $g(x_1, u)\frac{1}{h'(x_1)} = \hat{g}(u)$ and the problem is more tractable. But if I try to carry this logic over to an iterated integral then I am transforming several variables, even if they are constant in this integral. Can I naively put these in the limits? I am not sure if I should start drawing pictures in the new system of coordinates to determine the limits I defined and if I should calculate Jacobians, or if I should just apply the formula from the 1 variable case. If it helps, the integral that confused me was: $$\int_{0}^{1} \int_{0}^{2}\frac{xy(x^2-y^2)}{(x^2+y^2)^3}dydx$$ With the substition $u = x^2 + y^2$. I'm not sure how I would even calculate a Jacobian for this, without trying to horribly force square roots to write $x$ as a $x(u)$?, for that I need enough relations for a $2\times2$ matrix of partial derivatives, this is just one.",,"['multivariable-calculus', 'definite-integrals', 'coordinate-systems']"
28,Do I need to substitute in the expressions before calculating this partial derivative?,Do I need to substitute in the expressions before calculating this partial derivative?,,"Find $\frac{\partial f}{\partial x}$ when: $f = 3xy^2z^3$, $y = 3x^2 + 2$, $z = \sqrt{x-1}$ I would need to replace $y$ and $z$ in $f$ before calculating the partial of $f$, correct? (The alternative would be find the partial of $y$ and $z$, square and cube them respectively and multiply all that by 3, which I think is incorrect, but I'm not sure)","Find $\frac{\partial f}{\partial x}$ when: $f = 3xy^2z^3$, $y = 3x^2 + 2$, $z = \sqrt{x-1}$ I would need to replace $y$ and $z$ in $f$ before calculating the partial of $f$, correct? (The alternative would be find the partial of $y$ and $z$, square and cube them respectively and multiply all that by 3, which I think is incorrect, but I'm not sure)",,"['multivariable-calculus', 'partial-derivative']"
29,Non-vanishing Jacobian determinant is bounded below?,Non-vanishing Jacobian determinant is bounded below?,,"Let $F:\mathbb{R}^n \to \mathbb{R}^m$ ($m < n$) be a Lipschitz function whose Jacobian determinant $J F$ does not vanish on a compact set $A \subseteq  \mathbb{R^n}$. Assume $J F$ exists everywhere. Does this imply $J F(x)$ is bounded away from zero for all $x \in A$? I know this would follow from $x \mapsto J F (x)$ being a continuous map or even a closed map, but I don't see why either of those should be true. The Jacobian determinant of $F$ is defined as $$J F(x) = \sqrt{ \text{det}( DF(x) DF(x)^T )  },$$  where $D F$ is the $m \times n$ matrix of partial derivatives  of $F$.","Let $F:\mathbb{R}^n \to \mathbb{R}^m$ ($m < n$) be a Lipschitz function whose Jacobian determinant $J F$ does not vanish on a compact set $A \subseteq  \mathbb{R^n}$. Assume $J F$ exists everywhere. Does this imply $J F(x)$ is bounded away from zero for all $x \in A$? I know this would follow from $x \mapsto J F (x)$ being a continuous map or even a closed map, but I don't see why either of those should be true. The Jacobian determinant of $F$ is defined as $$J F(x) = \sqrt{ \text{det}( DF(x) DF(x)^T )  },$$  where $D F$ is the $m \times n$ matrix of partial derivatives  of $F$.",,"['real-analysis', 'multivariable-calculus', 'jacobian']"
30,"Limit of two-variable function: ${\lim_{(x,y) \to (0,0)} \frac{e^{x(y+1)} -x -1}{\|(x,y)\|}}$",Limit of two-variable function:,"{\lim_{(x,y) \to (0,0)} \frac{e^{x(y+1)} -x -1}{\|(x,y)\|}}","I am stuck with this limit, which according to Wolfram Alpha does not exist. $$ \begin{align} \lim_{(x,y) \to (0,0)} \frac{e^{x(y+1)} -x -1}{\|(x,y)\|} = \lim_{(x,y) \to (0,0)} \frac{e^{x(y+1)} -x -1}{\sqrt{x^2+y^2}} = \lim_{r \to 0\; \forall\theta} \frac{e^{r\cos{\theta}(r\sin{\theta}+1)} -r\cos{\theta} -1}{r} \end{align} $$ From there, I thought about using the first-degree Taylor expansion in order to get rid of the exponential (which I'm not even sure I can do). $$ \begin{split} \lim_{r \to 0\; \forall\theta} \frac{e^{r\cos{\theta}(r\sin{\theta}+1)} -r\cos{\theta} -1}{r} &= \lim_{r \to 0\; \forall\theta} \frac{1+r\cos{\theta}(r\sin{\theta}+1) -r\cos{\theta} -1}{r} \\ &=\lim_{r \to 0\; \forall\theta} \frac{1+ r^2\cos{\theta}\sin{\theta}+r\cos{\theta} -r\cos{\theta} -1}{r}\\ & = \lim_{r \to 0\; \forall\theta} \frac{r^2\cos{\theta}\sin{\theta}}{r} \\ &= \lim_{r \to 0\; \forall\theta} {r\cos{\theta}\sin{\theta}}  = 0 \end{split} $$ This is apparently wrong, but I cannot think of any other way to solve this problem, and through this method the result is clearly 0. Since I am pretty sure using Taylor's theorem here is not allowed, how else could you go about solving the limit?","I am stuck with this limit, which according to Wolfram Alpha does not exist. $$ \begin{align} \lim_{(x,y) \to (0,0)} \frac{e^{x(y+1)} -x -1}{\|(x,y)\|} = \lim_{(x,y) \to (0,0)} \frac{e^{x(y+1)} -x -1}{\sqrt{x^2+y^2}} = \lim_{r \to 0\; \forall\theta} \frac{e^{r\cos{\theta}(r\sin{\theta}+1)} -r\cos{\theta} -1}{r} \end{align} $$ From there, I thought about using the first-degree Taylor expansion in order to get rid of the exponential (which I'm not even sure I can do). $$ \begin{split} \lim_{r \to 0\; \forall\theta} \frac{e^{r\cos{\theta}(r\sin{\theta}+1)} -r\cos{\theta} -1}{r} &= \lim_{r \to 0\; \forall\theta} \frac{1+r\cos{\theta}(r\sin{\theta}+1) -r\cos{\theta} -1}{r} \\ &=\lim_{r \to 0\; \forall\theta} \frac{1+ r^2\cos{\theta}\sin{\theta}+r\cos{\theta} -r\cos{\theta} -1}{r}\\ & = \lim_{r \to 0\; \forall\theta} \frac{r^2\cos{\theta}\sin{\theta}}{r} \\ &= \lim_{r \to 0\; \forall\theta} {r\cos{\theta}\sin{\theta}}  = 0 \end{split} $$ This is apparently wrong, but I cannot think of any other way to solve this problem, and through this method the result is clearly 0. Since I am pretty sure using Taylor's theorem here is not allowed, how else could you go about solving the limit?",,"['limits', 'multivariable-calculus', 'limits-without-lhopital']"
31,Prove $\nabla^2 f(x) \preceq L I$ for convex $f$ with Lipschitz gradient,Prove  for convex  with Lipschitz gradient,\nabla^2 f(x) \preceq L I f,"I am given a convex and twice differentiable function $f$ whose gradient is Lipschitz with constant "" $L$ "". I am trying to prove $$ \nabla^2 f(x) \preceq L\,I. $$ I recognize that this is pretty easy, and suspect the proof will invoke the definition of the Hessian. From $f$ having Lipschitz gradient, we know $\|\nabla f(x) - \nabla f(y)\| \lt L\,\|x - y\|$ . And therefore $$ \lim_{x \to y} \frac{\|\nabla f(x) - \nabla f(y)\|}{\|x - y\|} \lt  \frac{L\,\|x - y\|}{\|x - y\|} ~~\Longrightarrow ~~\nabla^2 f(x) \preceq LI. $$ But this isn't quite right since the Hessian is a matrix and the quantities inside the limit to the left of the implication arrow are scalars. What is the proper way to prove this? Any help here would be helpful :-)","I am given a convex and twice differentiable function whose gradient is Lipschitz with constant "" "". I am trying to prove I recognize that this is pretty easy, and suspect the proof will invoke the definition of the Hessian. From having Lipschitz gradient, we know . And therefore But this isn't quite right since the Hessian is a matrix and the quantities inside the limit to the left of the implication arrow are scalars. What is the proper way to prove this? Any help here would be helpful :-)","f L 
\nabla^2 f(x) \preceq L\,I.
 f \|\nabla f(x) - \nabla f(y)\| \lt L\,\|x - y\| 
\lim_{x \to y} \frac{\|\nabla f(x) - \nabla f(y)\|}{\|x - y\|} \lt 
\frac{L\,\|x - y\|}{\|x - y\|}
~~\Longrightarrow ~~\nabla^2 f(x) \preceq LI.
","['real-analysis', 'multivariable-calculus', 'convex-analysis']"
32,How to prove $\textbf{n}d^2\sigma=\frac{\partial r}{\partial u} \times \frac{\partial r}{\partial v}dudv$,How to prove,\textbf{n}d^2\sigma=\frac{\partial r}{\partial u} \times \frac{\partial r}{\partial v}dudv,"How to prove $$\textbf{n}d^2\sigma=\frac{\partial r}{\partial u} \times \frac{\partial r}{\partial v}dudv$$ where $\sigma$ is an open surface that has a smooth parameterization $\textbf{r}(u,v)$ and $\textbf{n}$ is the unit vector normal to the oriented surface, using the notion of a Jacobian? I understand that the cross product gives a vector normal to the surface, but I am not sure how to justify this rigorously. On the internet, most sources use this fact as if it is obvious so I am finding it difficult to find a formal argument that justifies this claim. Perhaps someone has an idea how to prove this?","How to prove $$\textbf{n}d^2\sigma=\frac{\partial r}{\partial u} \times \frac{\partial r}{\partial v}dudv$$ where $\sigma$ is an open surface that has a smooth parameterization $\textbf{r}(u,v)$ and $\textbf{n}$ is the unit vector normal to the oriented surface, using the notion of a Jacobian? I understand that the cross product gives a vector normal to the surface, but I am not sure how to justify this rigorously. On the internet, most sources use this fact as if it is obvious so I am finding it difficult to find a formal argument that justifies this claim. Perhaps someone has an idea how to prove this?",,['multivariable-calculus']
33,Sobolev approximation lifts to $L^p$ convergence of the exterior powers,Sobolev approximation lifts to  convergence of the exterior powers,L^p,"I am reading the book ""Geometric Function Theory and Non-linear Analysis"", where the following claim is used: Let $\Omega \subseteq \mathbb{R}^n$ be a bounded open set. Let $f \in W^{1,s}(\Omega,\mathbb{R}^n)$ , and let $f_n \in C^{\infty}(\Omega,\mathbb{R}^n)$ converge to $f$ in $W^{1,s}$ . Suppose $s \ge k \in \mathbb{N}$ . Then $\bigwedge^k df_n $ converges to $\bigwedge^k df $ in $L^1$ , i.e $$ \int_{\Omega}|\bigwedge^k df_n -\bigwedge^k df | \to 0,$$ where the norm $|\cdot|$ is the standard Euclidean norm on linear maps between exterior powers $\Lambda_k(\mathbb{R}^n) \to \Lambda_k(\mathbb{R}^n)$ . Question: How to prove this claim? My naive approach was to guess that $$ |\bigwedge^k df_n -\bigwedge^k df | \le C |df_n-df|^k, \tag{1}$$ which would imply $$ ||\bigwedge^k df_n -\bigwedge^k df ||_ 1 \le C||df_n-df||_k^k \le \tilde C||df_n-df||_s^k \to 0.$$ However, estimate $(1)$ is false . Edit: I tried to prove this via induction on $k$ . However, I hit an obstacle .","I am reading the book ""Geometric Function Theory and Non-linear Analysis"", where the following claim is used: Let be a bounded open set. Let , and let converge to in . Suppose . Then converges to in , i.e where the norm is the standard Euclidean norm on linear maps between exterior powers . Question: How to prove this claim? My naive approach was to guess that which would imply However, estimate is false . Edit: I tried to prove this via induction on . However, I hit an obstacle .","\Omega \subseteq \mathbb{R}^n f \in W^{1,s}(\Omega,\mathbb{R}^n) f_n \in C^{\infty}(\Omega,\mathbb{R}^n) f W^{1,s} s \ge k \in \mathbb{N} \bigwedge^k df_n  \bigwedge^k df  L^1  \int_{\Omega}|\bigwedge^k df_n -\bigwedge^k df | \to 0, |\cdot| \Lambda_k(\mathbb{R}^n) \to \Lambda_k(\mathbb{R}^n)  |\bigwedge^k df_n -\bigwedge^k df | \le C |df_n-df|^k, \tag{1}  ||\bigwedge^k df_n -\bigwedge^k df ||_ 1 \le C||df_n-df||_k^k \le \tilde C||df_n-df||_s^k \to 0. (1) k","['multivariable-calculus', 'riemannian-geometry', 'sobolev-spaces', 'approximation-theory', 'exterior-algebra']"
34,"Evaluate $\lim\limits_{(x,y)\to (0,0)}\frac{\sin(xy^2)}{x}$",Evaluate,"\lim\limits_{(x,y)\to (0,0)}\frac{\sin(xy^2)}{x}","Evaluate $$\lim_{(x,y)\to (0,0)}\frac{\sin(xy^2)}{x}$$ $$\lim_{(x,y)\to (0,0)}\frac{\sin(xy^2)}{x}=\lim_{(x,y)\to (0,0)}\frac{\sin(xy^2)}{xy^2}y^2$$ now  $$\lim_{(x,y)\to (0,0)}\frac{\sin(xy^2)}{xy^2}\cdot \lim_{(x,y)\to (0,0)} y^2=1\cdot 0=0$$ Is there a problem with this calculation regarding the where the function is defined? In general if we look at $(x,0)\to(0,0)$ or $(0,y)\to(0,0)$ are those iterative limits?","Evaluate $$\lim_{(x,y)\to (0,0)}\frac{\sin(xy^2)}{x}$$ $$\lim_{(x,y)\to (0,0)}\frac{\sin(xy^2)}{x}=\lim_{(x,y)\to (0,0)}\frac{\sin(xy^2)}{xy^2}y^2$$ now  $$\lim_{(x,y)\to (0,0)}\frac{\sin(xy^2)}{xy^2}\cdot \lim_{(x,y)\to (0,0)} y^2=1\cdot 0=0$$ Is there a problem with this calculation regarding the where the function is defined? In general if we look at $(x,0)\to(0,0)$ or $(0,y)\to(0,0)$ are those iterative limits?",,['multivariable-calculus']
35,Is it possible to execute line integrals of non-conservative vector fields on curves defined by implicit relation such as $\sin(xy)=x+y$?,Is it possible to execute line integrals of non-conservative vector fields on curves defined by implicit relation such as ?,\sin(xy)=x+y,"I want to execute the line integral (analytically) of a vector field over the curve defined by implicit function $\sin(xy)=x+y$ for some $x=a$ & $y=c$ to $x=b$ & $y=d$. The difficulties I face executing this problem are: There can no explicit parametrization of $x$ and $y$ wrt to some parameter $t$. Ted Shifrin has pointed to me in chat that this problem has become unsolvable for non-conservative vector fields for this reason. But, for conservative vector fields , the problem becomes trivial to execute. Even if we consider some conservative vector field, this problem is problematic some values of $x$, $y$ has multiple corresponding values. See the pictures. Then, to use the fundamental theorem of line integral we need to divide the curve into several parts where the $\sin(xy)=x+y$ relation becomes a function. And last of all , is this problem really (analytically) unsolvable for non-conservative vector fields? I can always numerically execute the integral (of course, approximately). But, that's not my point here. Is there any novel technique present in the to-date literature to somehow overcome the barrier of the non-conservative vector fields? Any help would be appreciated.","I want to execute the line integral (analytically) of a vector field over the curve defined by implicit function $\sin(xy)=x+y$ for some $x=a$ & $y=c$ to $x=b$ & $y=d$. The difficulties I face executing this problem are: There can no explicit parametrization of $x$ and $y$ wrt to some parameter $t$. Ted Shifrin has pointed to me in chat that this problem has become unsolvable for non-conservative vector fields for this reason. But, for conservative vector fields , the problem becomes trivial to execute. Even if we consider some conservative vector field, this problem is problematic some values of $x$, $y$ has multiple corresponding values. See the pictures. Then, to use the fundamental theorem of line integral we need to divide the curve into several parts where the $\sin(xy)=x+y$ relation becomes a function. And last of all , is this problem really (analytically) unsolvable for non-conservative vector fields? I can always numerically execute the integral (of course, approximately). But, that's not my point here. Is there any novel technique present in the to-date literature to somehow overcome the barrier of the non-conservative vector fields? Any help would be appreciated.",,"['multivariable-calculus', 'vector-analysis', 'vector-fields', 'implicit-function']"
36,"What is the slope of the line tangent to the surface at that point $(2,-1)$ lying in the plane $y=-1$?",What is the slope of the line tangent to the surface at that point  lying in the plane ?,"(2,-1) y=-1","For the function $f(x,y)=x(x+y^5)$,What is the slope of the line   tangent to the surface at that point $(2,-1)$ lying in the plane   $y=-1$? I know how to find the linear approximation of any function,but i'm not getting any approach of finding the slope. Please suggest any method/formula....","For the function $f(x,y)=x(x+y^5)$,What is the slope of the line   tangent to the surface at that point $(2,-1)$ lying in the plane   $y=-1$? I know how to find the linear approximation of any function,but i'm not getting any approach of finding the slope. Please suggest any method/formula....",,"['real-analysis', 'multivariable-calculus', 'derivatives', '3d']"
37,Compute $\iint_D(-2x-y)\cos((x-y)(-2x-y))dxdy$,Compute,\iint_D(-2x-y)\cos((x-y)(-2x-y))dxdy,"Compute $$\iint_D(-2x-y)\cos((x-y)(-2x-y)) \ dxdy$$ Where $D$ has vertices at the points $$P_1=\left(-1 + \frac{\pi}{6} , 2+\frac{\pi}{6} \right)$$ $$P_2=\left(-2 + \frac{\pi}{6} , 4+\frac{\pi}{6} \right)$$ $$P_3=\left(-2 + \frac{\pi}{4} , 4+\frac{\pi}{4} \right)$$ $$P_4=\left(-1 + \frac{\pi}{4} , 2+\frac{\pi}{4} \right)$$ I think I'm very close, but for the life of me I can't figure why I get the incorrect answer. Here is what I've done: Plotting the points I get The short lines, from left to right are $y=x+6$ and $y=x-3$. The long lines are $y=-2x+\pi/2$ and $y=-2x+3\pi/4.$ So I have that \begin{array}{lcl} -6 \leq &x-y&  \leq 3 \\ -\frac{3\pi}{4} \leq &-2x-y&  \leq -\frac{\pi}{2} \end{array} Setting $u=x-y$ and $v=-2x-y$ and solving the system I get $$\left\{   \begin{array}{rcr}     x & = & \frac{1}{3}u-\frac{1}{3}v \\     y & = & -\frac{2}{3}u-\frac{1}{3}v \\   \end{array} \right.$$ $\implies |J(u,v)|=1/3.$ Thus: $$\iint_D(-2x-y)\cos((x-y)(-2x-y)) \ dxdy=\iint_Ev\cos{(uv)}|J(u,v)|\ du dv$$ $$= \frac{1}{3}\int_{-6}^{3}v\left(\int_{-3\pi/4}^{-\pi/2}\cos{(uv)} \ du\right) \ dv=\frac{2(3-\sqrt{2})}{9\pi}.$$ Where is the error?","Compute $$\iint_D(-2x-y)\cos((x-y)(-2x-y)) \ dxdy$$ Where $D$ has vertices at the points $$P_1=\left(-1 + \frac{\pi}{6} , 2+\frac{\pi}{6} \right)$$ $$P_2=\left(-2 + \frac{\pi}{6} , 4+\frac{\pi}{6} \right)$$ $$P_3=\left(-2 + \frac{\pi}{4} , 4+\frac{\pi}{4} \right)$$ $$P_4=\left(-1 + \frac{\pi}{4} , 2+\frac{\pi}{4} \right)$$ I think I'm very close, but for the life of me I can't figure why I get the incorrect answer. Here is what I've done: Plotting the points I get The short lines, from left to right are $y=x+6$ and $y=x-3$. The long lines are $y=-2x+\pi/2$ and $y=-2x+3\pi/4.$ So I have that \begin{array}{lcl} -6 \leq &x-y&  \leq 3 \\ -\frac{3\pi}{4} \leq &-2x-y&  \leq -\frac{\pi}{2} \end{array} Setting $u=x-y$ and $v=-2x-y$ and solving the system I get $$\left\{   \begin{array}{rcr}     x & = & \frac{1}{3}u-\frac{1}{3}v \\     y & = & -\frac{2}{3}u-\frac{1}{3}v \\   \end{array} \right.$$ $\implies |J(u,v)|=1/3.$ Thus: $$\iint_D(-2x-y)\cos((x-y)(-2x-y)) \ dxdy=\iint_Ev\cos{(uv)}|J(u,v)|\ du dv$$ $$= \frac{1}{3}\int_{-6}^{3}v\left(\int_{-3\pi/4}^{-\pi/2}\cos{(uv)} \ du\right) \ dv=\frac{2(3-\sqrt{2})}{9\pi}.$$ Where is the error?",,"['multivariable-calculus', 'proof-verification', 'multiple-integral']"
38,Show that limit of $e^{e^{-xy}}$ as $ x^2+y^2 \to \infty$ does not exist,Show that limit of  as  does not exist,e^{e^{-xy}}  x^2+y^2 \to \infty,"The problem consists of showing that $\lim\limits_{x^2+y^2 \to \infty}e^{e^{-xy}}$ does not exist. My initial approach was to set $x = t, y = 0$ and show that the function converges to different limits when I let $t \to \infty$ compared to when $t \to -\infty$. From that, I concluded that the function does not have a limit. However, I am not sure whether it is a valid approach to set $y=0$ in this case. Is this the right way to go about it, or should I try some other strategy? I tried using polar coordinates but that does not get me far. Thanks,","The problem consists of showing that $\lim\limits_{x^2+y^2 \to \infty}e^{e^{-xy}}$ does not exist. My initial approach was to set $x = t, y = 0$ and show that the function converges to different limits when I let $t \to \infty$ compared to when $t \to -\infty$. From that, I concluded that the function does not have a limit. However, I am not sure whether it is a valid approach to set $y=0$ in this case. Is this the right way to go about it, or should I try some other strategy? I tried using polar coordinates but that does not get me far. Thanks,",,"['limits', 'multivariable-calculus']"
39,"Check differentiability of $f(x,y)=y\sin\frac{1}{x}$ at $(0,0)$",Check differentiability of  at,"f(x,y)=y\sin\frac{1}{x} (0,0)","Given , $f(x,y)=y\sin\frac{1}{x}$ when $x\ne0$ and $f(0,0)=0$ . Investigate differentiability at $(0,0)$ . I've found that it is continuous at $(0,0)$ and the partial derivatives $f_x=0$  $\forall (x,y)$ and $f_y=\begin{cases} \sin\frac1x & \text{ if } x\ne0\\  1 & \text{ if } x= 0 \end{cases}.$ For differentiability it is sufficient to show that both partial derivatives exist and one of them(confused between ""both continuous"" Or ""one of them"") is continuous about some neighbourhood of $(0,0)$ . Now, I'm stuck. Please help how to think. EDIT $f(0,y)=y$","Given , $f(x,y)=y\sin\frac{1}{x}$ when $x\ne0$ and $f(0,0)=0$ . Investigate differentiability at $(0,0)$ . I've found that it is continuous at $(0,0)$ and the partial derivatives $f_x=0$  $\forall (x,y)$ and $f_y=\begin{cases} \sin\frac1x & \text{ if } x\ne0\\  1 & \text{ if } x= 0 \end{cases}.$ For differentiability it is sufficient to show that both partial derivatives exist and one of them(confused between ""both continuous"" Or ""one of them"") is continuous about some neighbourhood of $(0,0)$ . Now, I'm stuck. Please help how to think. EDIT $f(0,y)=y$",,['multivariable-calculus']
40,What is the difference between a trace and a contour in calculus?,What is the difference between a trace and a contour in calculus?,,"As far as I can tell they're exactly the same thing, but the notes here discuss them as if they are separate: The final topic in this section is that of traces.  In some ways these are similar to contours.  As noted above we can think of contours as the intersection of the surface given by $z=f(x,y)$ and the plane $z=k$.  Traces of surfaces are curves that represent the intersection of the surface and the plane given by $x=a$ or $y=b$. Is the only difference whether we're holding an ""input"" to the function constant as opposed to the ""output""? Generally the functions are defined by equations where any of the variables could be considered a function of the other two, so the distinction seems arbitrary? If there isn't a difference in denotation is there one of connotation?","As far as I can tell they're exactly the same thing, but the notes here discuss them as if they are separate: The final topic in this section is that of traces.  In some ways these are similar to contours.  As noted above we can think of contours as the intersection of the surface given by $z=f(x,y)$ and the plane $z=k$.  Traces of surfaces are curves that represent the intersection of the surface and the plane given by $x=a$ or $y=b$. Is the only difference whether we're holding an ""input"" to the function constant as opposed to the ""output""? Generally the functions are defined by equations where any of the variables could be considered a function of the other two, so the distinction seems arbitrary? If there isn't a difference in denotation is there one of connotation?",,"['calculus', 'multivariable-calculus', 'visualization']"
41,how to compute the vector derivative of this matrix equation,how to compute the vector derivative of this matrix equation,,"How to compute the derivative of the latent factor where $u_n$, $v_n$ are column vectors. How did he take derivative and obtain the below equation? I was reading probabilistic matrix factorization where I came across this equation. I don't know how to take vector derivative of another vector. To learn about vector derivatives in general can you point a source?","How to compute the derivative of the latent factor where $u_n$, $v_n$ are column vectors. How did he take derivative and obtain the below equation? I was reading probabilistic matrix factorization where I came across this equation. I don't know how to take vector derivative of another vector. To learn about vector derivatives in general can you point a source?",,"['calculus', 'probability', 'multivariable-calculus', 'vector-analysis']"
42,"Define all constant $a$ values in way that function $f(x,y)$ has critical point in $(0,0)$",Define all constant  values in way that function  has critical point in,"a f(x,y) (0,0)","Problem Define all constant $a$ values in way that function $f(x,y)$ has critical point in $(0,0)$ whe: $$ f(x,y)=(4x^2+axy+y^2)(a+x) $$ Attempt to solve I wan to find out all constant $a$ values in a way that this function has zero gradient in point $(0,0)$ $\nabla(0,0)=0$ with what constant $a$. Gradient for this function is: $$ \nabla f(x,y)=\begin{bmatrix} a^2y+2axy+8ax+12x^2+y^2 \\ a^2x+ax^2+2ay+2xy  \end{bmatrix} $$ $$ \nabla f(0,0)=\begin{bmatrix} a^2\cdot 0 + 2\cdot 0 \cdot 0 + 8 \cdot 0 \cdot 0 + 12 \cdot 0^2 + 0^2 \\ a^2\cdot 0 + a\cdot 0 ^2 + 2 \cdot 0 \cdot 0 + 2 \cdot 0 \cdot 0 \end{bmatrix}=\begin{bmatrix} 0 \\ 0 \end{bmatrix} $$ Since value of gradient at point $(0,0)$ is not dependent on value of constant $a$. This constant can be anything when $a \in \mathbb{R}$. I think this is correct solution to this problem but i still have some doubt that maybe there is flaw. It would be highly appreciated if someone can tell that does this seem to be correct or not ?","Problem Define all constant $a$ values in way that function $f(x,y)$ has critical point in $(0,0)$ whe: $$ f(x,y)=(4x^2+axy+y^2)(a+x) $$ Attempt to solve I wan to find out all constant $a$ values in a way that this function has zero gradient in point $(0,0)$ $\nabla(0,0)=0$ with what constant $a$. Gradient for this function is: $$ \nabla f(x,y)=\begin{bmatrix} a^2y+2axy+8ax+12x^2+y^2 \\ a^2x+ax^2+2ay+2xy  \end{bmatrix} $$ $$ \nabla f(0,0)=\begin{bmatrix} a^2\cdot 0 + 2\cdot 0 \cdot 0 + 8 \cdot 0 \cdot 0 + 12 \cdot 0^2 + 0^2 \\ a^2\cdot 0 + a\cdot 0 ^2 + 2 \cdot 0 \cdot 0 + 2 \cdot 0 \cdot 0 \end{bmatrix}=\begin{bmatrix} 0 \\ 0 \end{bmatrix} $$ Since value of gradient at point $(0,0)$ is not dependent on value of constant $a$. This constant can be anything when $a \in \mathbb{R}$. I think this is correct solution to this problem but i still have some doubt that maybe there is flaw. It would be highly appreciated if someone can tell that does this seem to be correct or not ?",,['multivariable-calculus']
43,"$x_1,x_2\in \mathbb R^2 $ or $(x_1,x_2)\in \mathbb R^2 $ for this function?",or  for this function?,"x_1,x_2\in \mathbb R^2  (x_1,x_2)\in \mathbb R^2 ","Say I have the function $f(x_1,x_2)=x_1+x_2$ and I want to use vector notation. Q1: I set $\mathbf x=x_1\hat e_1+x_2\hat e_2=(x_1,x_2)$, so $f(x_1,x_2)=f(\mathbf x)$ and I now have $f(\mathbf x)=x_1+x_2$. I guess this is correct? Q2 But should I write $$ f(\mathbf x)=x_1+x_2, \quad x_1,x_2\in \mathbb R^2 \quad \tag 1 $$  or  $$ f(\mathbf x)=x_1+x_2, \quad (x_1,x_2)\in \mathbb R^2 \quad \tag 2 $$","Say I have the function $f(x_1,x_2)=x_1+x_2$ and I want to use vector notation. Q1: I set $\mathbf x=x_1\hat e_1+x_2\hat e_2=(x_1,x_2)$, so $f(x_1,x_2)=f(\mathbf x)$ and I now have $f(\mathbf x)=x_1+x_2$. I guess this is correct? Q2 But should I write $$ f(\mathbf x)=x_1+x_2, \quad x_1,x_2\in \mathbb R^2 \quad \tag 1 $$  or  $$ f(\mathbf x)=x_1+x_2, \quad (x_1,x_2)\in \mathbb R^2 \quad \tag 2 $$",,"['multivariable-calculus', 'vectors']"
44,I cannot find boundaries in Divergence and Stokes's Theorems,I cannot find boundaries in Divergence and Stokes's Theorems,,"I have two questions from my exam. I'm sorry for not remembering what was F. 1) $F$ is 3-dimensional vector field. $\delta$ is a surface which is bounded by $z=x^2+y^2-3$ and $z=1$ and oriented in negative $z$ $$ \iint_{\delta}curlF.\hat N.ds $$ Should I do $z=x^2+y^2-3 = 1$ $\Rightarrow$ $z=x^2+y^2=4$ and use polar coordinates? And I know  we can write $\hat N.ds$ = $\pm \frac{\nabla G}{G_3}dxdy$ when $G(x,y,z)=0$ is equation of surface with (1-1) projection onto a domain in the $xy$-plane. How can I decide what is $\hat N.ds$ when we have two or more equations like $x^2+y^2+z^2=a^2$ and $x+y+z=0$ 2) It was more confusing me. I have no idea how can I determine boundaries. I have found $divF=3y$ $$ \iiint_D 3ydV $$ Equations : $z=1-x^2$ parabolic cylinder and $z=0$, $y=0$, $y+z=2$ What should I say for boundaries? As you can see I have some problems about these theorems. Can someone suggest some tutorials or books etc for catching them pratically? Thanks","I have two questions from my exam. I'm sorry for not remembering what was F. 1) $F$ is 3-dimensional vector field. $\delta$ is a surface which is bounded by $z=x^2+y^2-3$ and $z=1$ and oriented in negative $z$ $$ \iint_{\delta}curlF.\hat N.ds $$ Should I do $z=x^2+y^2-3 = 1$ $\Rightarrow$ $z=x^2+y^2=4$ and use polar coordinates? And I know  we can write $\hat N.ds$ = $\pm \frac{\nabla G}{G_3}dxdy$ when $G(x,y,z)=0$ is equation of surface with (1-1) projection onto a domain in the $xy$-plane. How can I decide what is $\hat N.ds$ when we have two or more equations like $x^2+y^2+z^2=a^2$ and $x+y+z=0$ 2) It was more confusing me. I have no idea how can I determine boundaries. I have found $divF=3y$ $$ \iiint_D 3ydV $$ Equations : $z=1-x^2$ parabolic cylinder and $z=0$, $y=0$, $y+z=2$ What should I say for boundaries? As you can see I have some problems about these theorems. Can someone suggest some tutorials or books etc for catching them pratically? Thanks",,"['calculus', 'integration', 'multivariable-calculus']"
45,Area of a spherical rectangle,Area of a spherical rectangle,,"I would like to know how one can calculate the area of a spherical rectangle which is defined by two longitudes and latitudes on the unit sphere. I am well aware of answers like this question here , but I would like to do it using multidimensional integration. My approach so far I know I can parameterize the points on a unit sphere  $$\partial\mathbb{S}^2 = \{(x,y,z) \in \mathbb{R}^3 : x^2 + y^2 + z^2 = 1\}$$ by using spherical coordinates: $[0,\pi]\times[0,2\pi]$ $$\Omega= \begin{bmatrix} \sin\theta\cos\phi \\ \sin\theta\sin\phi \\ \cos\theta \end{bmatrix}$$ If I integrated over all of the unit sphere's area, I would do the following: $$\int_F do = \int_0 ^\pi\int_0 ^{2\pi}|\partial_{\theta}\Omega\times \partial_{\phi}\Omega|d\phi d\theta$$ Now, however, I do not need to integrate over the whole unit sphere so I must change my area of integration. Furthermore, I believe that I would have to change my parametrization slightly. Let's say the rectangle is $b$ high (distance between two latitudes), and $c$ wide, (the distance between two latitudes) as well as $a$ above the equator. Since $\phi$ ""is symmetrical"" instead of integrating from $[0,2\pi]$ we can integrate from $[0,c]$, (right?), but how do I integrate over $\theta$, since not only the height of the rectangle is important but also how far away it is from the equator. Your help is greatly appreciated. (Sorry for the bad picture)","I would like to know how one can calculate the area of a spherical rectangle which is defined by two longitudes and latitudes on the unit sphere. I am well aware of answers like this question here , but I would like to do it using multidimensional integration. My approach so far I know I can parameterize the points on a unit sphere  $$\partial\mathbb{S}^2 = \{(x,y,z) \in \mathbb{R}^3 : x^2 + y^2 + z^2 = 1\}$$ by using spherical coordinates: $[0,\pi]\times[0,2\pi]$ $$\Omega= \begin{bmatrix} \sin\theta\cos\phi \\ \sin\theta\sin\phi \\ \cos\theta \end{bmatrix}$$ If I integrated over all of the unit sphere's area, I would do the following: $$\int_F do = \int_0 ^\pi\int_0 ^{2\pi}|\partial_{\theta}\Omega\times \partial_{\phi}\Omega|d\phi d\theta$$ Now, however, I do not need to integrate over the whole unit sphere so I must change my area of integration. Furthermore, I believe that I would have to change my parametrization slightly. Let's say the rectangle is $b$ high (distance between two latitudes), and $c$ wide, (the distance between two latitudes) as well as $a$ above the equator. Since $\phi$ ""is symmetrical"" instead of integrating from $[0,2\pi]$ we can integrate from $[0,c]$, (right?), but how do I integrate over $\theta$, since not only the height of the rectangle is important but also how far away it is from the equator. Your help is greatly appreciated. (Sorry for the bad picture)",,['integration']
46,Why is $\kappa$ for a vertical line in 2-space not undefined?,Why is  for a vertical line in 2-space not undefined?,\kappa,"By the definition of curvature (in terms of $t$),  $$\kappa(t)=\frac{\|r'(t)\times r''(t)\|}{\|r'(t)\|^3},$$ where $r(t)$ represents a linear vertical vector-valued function, such as $r(t)=<0,t>$. Since the derivative of a vertical line is undefined, (e.g. a vertical tangent) it would seem that that $r'(t)$ would similarly be undefined, meaning $\kappa(t)$ would be undefined. Yet at the same time, the textbook does point out that a linear line's curvature is constant (by nature of it not bending) -- that is, $$\left\|\frac{dT}{ds}\right\|,$$ is constant, or $$\kappa(s)=\left\|\frac{dT}{ds}\right\|=0.$$ Thus, my question is how I can reconcile the two answers, and see where I went wrong in my reasoning that $\kappa(t)$ was undefined.","By the definition of curvature (in terms of $t$),  $$\kappa(t)=\frac{\|r'(t)\times r''(t)\|}{\|r'(t)\|^3},$$ where $r(t)$ represents a linear vertical vector-valued function, such as $r(t)=<0,t>$. Since the derivative of a vertical line is undefined, (e.g. a vertical tangent) it would seem that that $r'(t)$ would similarly be undefined, meaning $\kappa(t)$ would be undefined. Yet at the same time, the textbook does point out that a linear line's curvature is constant (by nature of it not bending) -- that is, $$\left\|\frac{dT}{ds}\right\|,$$ is constant, or $$\kappa(s)=\left\|\frac{dT}{ds}\right\|=0.$$ Thus, my question is how I can reconcile the two answers, and see where I went wrong in my reasoning that $\kappa(t)$ was undefined.",,"['multivariable-calculus', 'curvature']"
47,Volume between a sphere and a cone,Volume between a sphere and a cone,,"Find the volume between $r=R\,$ (sphere) and $\theta=\alpha\,$ (cone) , for $\theta$ and $r $ constants such that -  $0 < \theta < \dfrac{\pi}{2}$ I am sorry for the question being basic, but i couldn't find similar questions on Math Exchange. I can't figure what my integration limits suppose to be in order to solve this in a spherical coordinate system - $\phi$ is obviously from $0$ to $2\pi$, $\theta$ from $0$ to $\alpha$ but how can i bound $r$ ? Thank you !","Find the volume between $r=R\,$ (sphere) and $\theta=\alpha\,$ (cone) , for $\theta$ and $r $ constants such that -  $0 < \theta < \dfrac{\pi}{2}$ I am sorry for the question being basic, but i couldn't find similar questions on Math Exchange. I can't figure what my integration limits suppose to be in order to solve this in a spherical coordinate system - $\phi$ is obviously from $0$ to $2\pi$, $\theta$ from $0$ to $\alpha$ but how can i bound $r$ ? Thank you !",,"['multivariable-calculus', 'spherical-coordinates']"
48,How to find the unit tangent vector of a curve in R^3,How to find the unit tangent vector of a curve in R^3,,"I have this curve defined by : \begin{align} x & =\int_0^t \frac{(1+\cosh^3 u)\cos u \,du}{\cosh^2 u}; \\[10pt] y & =\int_0^t \frac{(1+\cosh^3 u)\sin u \,du}{\cosh^2 u}; \\[10pt] z & =\int_0^t \frac{(1+\cosh^3 u)\sinh u \, du}{cosh^2 u} \end{align} I should find $\vec{T}$,the unit tangent vector to(C) in M,so I have to use t as a parameter,is there any easy way to integrate x,y,z?","I have this curve defined by : \begin{align} x & =\int_0^t \frac{(1+\cosh^3 u)\cos u \,du}{\cosh^2 u}; \\[10pt] y & =\int_0^t \frac{(1+\cosh^3 u)\sin u \,du}{\cosh^2 u}; \\[10pt] z & =\int_0^t \frac{(1+\cosh^3 u)\sinh u \, du}{cosh^2 u} \end{align} I should find $\vec{T}$,the unit tangent vector to(C) in M,so I have to use t as a parameter,is there any easy way to integrate x,y,z?",,"['integration', 'multivariable-calculus']"
49,Domain and range of a multivariable function,Domain and range of a multivariable function,,"I have this exercise $$z={2x\over y+5}$$, and I am supposed to obtain the domain and range. I understand that the domain is all the pair of $(x,y)$ except $y=-5$ , then the exercise said that the range is $z=R$ I dont understand why z accept all the values, suppose that you want to plot the point $(1,-5)$ you wont be able to plot that point because $y=-5$ its not accepted by the domain so it cant output a value for z (range) Please help me undestand this or how i get the range for rational functions Thank you!","I have this exercise $$z={2x\over y+5}$$, and I am supposed to obtain the domain and range. I understand that the domain is all the pair of $(x,y)$ except $y=-5$ , then the exercise said that the range is $z=R$ I dont understand why z accept all the values, suppose that you want to plot the point $(1,-5)$ you wont be able to plot that point because $y=-5$ its not accepted by the domain so it cant output a value for z (range) Please help me undestand this or how i get the range for rational functions Thank you!",,['multivariable-calculus']
50,Finding extrema of general function of n variables without constrained optimization,Finding extrema of general function of n variables without constrained optimization,,"In our calculus textbook we are given the following exercise: Given function $f(x_1, x_2, \dots, x_n) = \sum_{i = 1}^{n}\sin(x_i) +  \sin\left(\sum_{i=1}^{n}x_i\right)$ with $x_i \in (0, \pi)$ and   $\sum_{i = 1}^{n}x_i \in (0, \pi)$ find it's extrema points and determine their type using only gradient and Hessian matrix. Taking partial derivatives for two arbitrary variables $x_i$ and $x_j$ we get $$\partial_{i}f = \cos(x_i) + \cos\left(\sum_{i = 1}^{n} x_i\right) = 0,$$ $$\partial_{j}f = \cos(x_j) + \cos\left(\sum_{i = 1}^{n} x_i\right) = 0.$$ Subtracting them we get that $$\cos(x_i) = \cos(x_j),$$ and so all minima lie at point where all values will be the same. Now trying to find points of interest gives me the condition $$\cos(x_i) = \cos(nx_i).$$ From that point and on, I'm stuck. How can I proceed there?","In our calculus textbook we are given the following exercise: Given function $f(x_1, x_2, \dots, x_n) = \sum_{i = 1}^{n}\sin(x_i) +  \sin\left(\sum_{i=1}^{n}x_i\right)$ with $x_i \in (0, \pi)$ and   $\sum_{i = 1}^{n}x_i \in (0, \pi)$ find it's extrema points and determine their type using only gradient and Hessian matrix. Taking partial derivatives for two arbitrary variables $x_i$ and $x_j$ we get $$\partial_{i}f = \cos(x_i) + \cos\left(\sum_{i = 1}^{n} x_i\right) = 0,$$ $$\partial_{j}f = \cos(x_j) + \cos\left(\sum_{i = 1}^{n} x_i\right) = 0.$$ Subtracting them we get that $$\cos(x_i) = \cos(x_j),$$ and so all minima lie at point where all values will be the same. Now trying to find points of interest gives me the condition $$\cos(x_i) = \cos(nx_i).$$ From that point and on, I'm stuck. How can I proceed there?",,"['multivariable-calculus', 'derivatives', 'optimization', 'maxima-minima']"
51,How to find a vector normal to a circle?,How to find a vector normal to a circle?,,"Assume $S_1$  be the disk in the $ y = 1$  plane bounded by the circle $ x^2$  +$ z^2$  = $9$ . Prove that the rightward pointing unit normal to $ S_1$ is  the vector $ (0, 1, 0)$. I know that the gradient is orthogonal to level curves.","Assume $S_1$  be the disk in the $ y = 1$  plane bounded by the circle $ x^2$  +$ z^2$  = $9$ . Prove that the rightward pointing unit normal to $ S_1$ is  the vector $ (0, 1, 0)$. I know that the gradient is orthogonal to level curves.",,"['multivariable-calculus', 'vectors']"
52,Multi-metrics in metric space,Multi-metrics in metric space,,"If I have $\mathbb R^n$ an $p\in[ 1,\infty )$ and the distance between vectors v and w is defined as $\sqrt[p]{(|v_1-w_1|^p)}$ . What does $d_\infty(v, w)$ = max { $|v_1-w_1|,|v_2-w_2|,\ldots,|v_n-w_n|$ } mean? How would you vizualize it? If I have $S^2\subset \mathbb R^3$ and $S^2$ ={ $v\in$ $\mathbb R^3$ ,|v|=1} then we conclude that is metric space. An observation: Shouldn't be enough to write $v\in$ $\mathbb R^2$ ? I'm confused why it is written in my textbook $v\in$ $\mathbb R^3$ . Is it just random reason for example: It's ok, that third dimension of a vector doesn't matter or is a reason behind it? I somehow understand that it is possible to have more metrics in metric space- you just have two defined ways how to measure distance between elements. If I understand  correctly, then for example by line or by arc. But then our professor told us that you only take the shortest way of measuring distance and that would be of course a line.  Is this true? I would really appreciate an explanation.","If I have an and the distance between vectors v and w is defined as . What does = max { } mean? How would you vizualize it? If I have and ={ ,|v|=1} then we conclude that is metric space. An observation: Shouldn't be enough to write ? I'm confused why it is written in my textbook . Is it just random reason for example: It's ok, that third dimension of a vector doesn't matter or is a reason behind it? I somehow understand that it is possible to have more metrics in metric space- you just have two defined ways how to measure distance between elements. If I understand  correctly, then for example by line or by arc. But then our professor told us that you only take the shortest way of measuring distance and that would be of course a line.  Is this true? I would really appreciate an explanation.","\mathbb R^n p\in[ 1,\infty ) \sqrt[p]{(|v_1-w_1|^p)} d_\infty(v, w) |v_1-w_1|,|v_2-w_2|,\ldots,|v_n-w_n| S^2\subset \mathbb R^3 S^2 v\in \mathbb R^3 v\in \mathbb R^2 v\in \mathbb R^3","['multivariable-calculus', 'metric-spaces']"
53,Integration Order Change,Integration Order Change,,"I have this integration $$\int_{y=0}^{1} \int_{z=0}^{x^2+y^2}$$ and I want to use the order change method to get $dy dz$ instead. However, when I try that I obtain the following, $$\int_{z=0}^{1+x^2} \int_{y=z-x^2}^{1}$$ but alas they don't give the same result. When I plot the boundaries, I can see the function doesn't really go from $(z-x^2) \to 1$ but I can't really figure out how and why.","I have this integration $$\int_{y=0}^{1} \int_{z=0}^{x^2+y^2}$$ and I want to use the order change method to get $dy dz$ instead. However, when I try that I obtain the following, $$\int_{z=0}^{1+x^2} \int_{y=z-x^2}^{1}$$ but alas they don't give the same result. When I plot the boundaries, I can see the function doesn't really go from $(z-x^2) \to 1$ but I can't really figure out how and why.",,"['calculus', 'multivariable-calculus']"
54,"Verification: For what $s$ is $\frac{x^4+y^4-6x^2y^2}{(x^2+y^2)^s}$ continuous and differentiable at $(0,0)$",Verification: For what  is  continuous and differentiable at,"s \frac{x^4+y^4-6x^2y^2}{(x^2+y^2)^s} (0,0)","Question For what values of $s$ (where s is real and positive) is: $f(x,y)=\dfrac{x^4+y^4-6x^2y^2}{(x^2+y^2)^s}$ when $(x,y) \neq (0,0)$ and $f(x,y) = 0$ when $(x,y)=(0,0)$ , (a) Continuous at $(0,0)$ (b) Differentiable at $(0,0)$ . Attempt at answer Looking at the line $y=x$, I get: $$f(t,t)=\frac{-4t^4}{(2t^4)^s} = \frac{-4}{(2)^s(t^{4(s-1)})}$$ However for any positive $s$, $f(t,t)$ does not tend to zero as $t$ tends to zero. Therefore $f(x,y)$ is not continuous or differentiable at $(0,0)$ for any positive real $s$. My question is, does this prove the function is not differentiable at $(0,0)$ for any $s$?","Question For what values of $s$ (where s is real and positive) is: $f(x,y)=\dfrac{x^4+y^4-6x^2y^2}{(x^2+y^2)^s}$ when $(x,y) \neq (0,0)$ and $f(x,y) = 0$ when $(x,y)=(0,0)$ , (a) Continuous at $(0,0)$ (b) Differentiable at $(0,0)$ . Attempt at answer Looking at the line $y=x$, I get: $$f(t,t)=\frac{-4t^4}{(2t^4)^s} = \frac{-4}{(2)^s(t^{4(s-1)})}$$ However for any positive $s$, $f(t,t)$ does not tend to zero as $t$ tends to zero. Therefore $f(x,y)$ is not continuous or differentiable at $(0,0)$ for any positive real $s$. My question is, does this prove the function is not differentiable at $(0,0)$ for any $s$?",,"['real-analysis', 'multivariable-calculus']"
55,"Multivariable calculus, change of variables, expression of derivative operators","Multivariable calculus, change of variables, expression of derivative operators",,"I have the following relations that express a change of variables $$\begin{cases} x=ae^\theta\cos(\varphi) \\  y=ae^\theta\sin(\varphi) \end{cases} $$ And I want to use it to transform the operator: $$L=x^2 \frac {\partial^2}{\partial x^2}-2xy \frac{\partial^2}{\partial x\partial y}+y^2 \frac{\partial^2}{\partial y^2}$$ The derivatives become rather complicated and I also have trouble defining the order of regularity of the change of variables. I would love an explicit solution as to how $$\frac{\partial}{\partial x}, \frac{\partial}{\partial y}$$ are expressed.","I have the following relations that express a change of variables $$\begin{cases} x=ae^\theta\cos(\varphi) \\  y=ae^\theta\sin(\varphi) \end{cases} $$ And I want to use it to transform the operator: $$L=x^2 \frac {\partial^2}{\partial x^2}-2xy \frac{\partial^2}{\partial x\partial y}+y^2 \frac{\partial^2}{\partial y^2}$$ The derivatives become rather complicated and I also have trouble defining the order of regularity of the change of variables. I would love an explicit solution as to how $$\frac{\partial}{\partial x}, \frac{\partial}{\partial y}$$ are expressed.",,"['calculus', 'multivariable-calculus', 'derivatives', 'partial-derivative', 'change-of-variable']"
56,How can I solve this partial differential equation? Wolfram alpha can't interpret it right.,How can I solve this partial differential equation? Wolfram alpha can't interpret it right.,,"I stumbled upon a differential equation which I do not know how to solve but would love to know the answer. I tried plugging it in wolfram alpha but it didn't help. For some reason WA wasn't interpreting it right. $$ \frac{ \partial y}{\partial x} \bigg( { \frac{\partial^2 y}{\partial \epsilon \partial x}\bigg) } = 0 $$ I am looking for $y(x, \epsilon)$ with these conditions: $$\frac{\partial y}{\partial x} {(0, \epsilon)} = 0$$ $$y(x, \epsilon_0) = y(x, -\epsilon_0) = h$$ where $h \in \mathbb{R}_{>0}$ If not analytical, can someone at least give me a hint as to what the numerical solution would like so that I know, intuitively, if this model is right.","I stumbled upon a differential equation which I do not know how to solve but would love to know the answer. I tried plugging it in wolfram alpha but it didn't help. For some reason WA wasn't interpreting it right. $$ \frac{ \partial y}{\partial x} \bigg( { \frac{\partial^2 y}{\partial \epsilon \partial x}\bigg) } = 0 $$ I am looking for $y(x, \epsilon)$ with these conditions: $$\frac{\partial y}{\partial x} {(0, \epsilon)} = 0$$ $$y(x, \epsilon_0) = y(x, -\epsilon_0) = h$$ where $h \in \mathbb{R}_{>0}$ If not analytical, can someone at least give me a hint as to what the numerical solution would like so that I know, intuitively, if this model is right.",,"['multivariable-calculus', 'partial-differential-equations', 'partial-derivative']"
57,What is the conditional expected value?,What is the conditional expected value?,,"Consider two random variables, X and Y, with joint density function, \begin{align} f_{X,Y}(x,y)=8xy, \qquad(0<x<1,0<y<x) \end{align} Calculate $\mathbb{E}(X\mid Y=y)$. I've tried this, $$f_{X\mid Y=y}=\frac{f_{X,Y}(x,y)}{f_Y(y)}=\frac{8xy}{\int_0^18xy\;dx}=\frac{8xy}{8y\left[\frac{x^2}{2}\right]_0^1}=\frac{8xy}{4y}=2x$$ $$\mathbb{E}(X\mid Y=y) = \int_0^1 xf_{X\mid Y=y} \; dx = \int_0^1 2x^2 \; dx = 2\left[\frac{x^3}{3}\right]_0^1 =2\cdot\frac{1}{3}=\frac{2}{3}$$ but my book says it is: $$\frac{2(1-y^3)}{3(1-y^2)},\;(0<y<1)$$ What I'm doing wrong? Help please!","Consider two random variables, X and Y, with joint density function, \begin{align} f_{X,Y}(x,y)=8xy, \qquad(0<x<1,0<y<x) \end{align} Calculate $\mathbb{E}(X\mid Y=y)$. I've tried this, $$f_{X\mid Y=y}=\frac{f_{X,Y}(x,y)}{f_Y(y)}=\frac{8xy}{\int_0^18xy\;dx}=\frac{8xy}{8y\left[\frac{x^2}{2}\right]_0^1}=\frac{8xy}{4y}=2x$$ $$\mathbb{E}(X\mid Y=y) = \int_0^1 xf_{X\mid Y=y} \; dx = \int_0^1 2x^2 \; dx = 2\left[\frac{x^3}{3}\right]_0^1 =2\cdot\frac{1}{3}=\frac{2}{3}$$ but my book says it is: $$\frac{2(1-y^3)}{3(1-y^2)},\;(0<y<1)$$ What I'm doing wrong? Help please!",,"['probability', 'multivariable-calculus', 'random-variables']"
58,Order of integration for sign-function,Order of integration for sign-function,,"Evaluate $\int_{0}^{1}\left ( \int_{0}^{1}f\left ( x,y \right )dx \right )dy$ and $\int_{0}^{1}\left ( \int_{0}^{1}f\left ( x,y \right )dy \right )dx$ Given that  $f\left ( x,y \right )= \frac{{sgn}\left ( y-x \right )}{{{max}}\left ( y-x \right )^{2}}$ and is defined on the interval $ \left ( x,y \right )\in \left [ 0,1 \right ] \times \left [ 0,1 \right ]$ Attempt: We need to first justify the order of integration by ensuring that the function is uniformly convergent. But it is not, and thus we cannot invoke Dominated Convergence Theorem. Any alternative methods?","Evaluate $\int_{0}^{1}\left ( \int_{0}^{1}f\left ( x,y \right )dx \right )dy$ and $\int_{0}^{1}\left ( \int_{0}^{1}f\left ( x,y \right )dy \right )dx$ Given that  $f\left ( x,y \right )= \frac{{sgn}\left ( y-x \right )}{{{max}}\left ( y-x \right )^{2}}$ and is defined on the interval $ \left ( x,y \right )\in \left [ 0,1 \right ] \times \left [ 0,1 \right ]$ Attempt: We need to first justify the order of integration by ensuring that the function is uniformly convergent. But it is not, and thus we cannot invoke Dominated Convergence Theorem. Any alternative methods?",,"['real-analysis', 'functional-analysis', 'multivariable-calculus', 'fourier-analysis']"
59,Finding the general equation of a cross section of a roof and the position of each joist that makes up its surface,Finding the general equation of a cross section of a roof and the position of each joist that makes up its surface,,"Doing some revision and came across this question. Would rather step along my thought process first rather than just type down the questions at the start if that's okay. A roof is given by the function $z = f(x, y) = \frac{5}{2} + \frac{1}{200} (9x^2-4y^2)$ and is on the domain $[-6, 6] × [-6, 6]$ Nine timber joists will be cut, and positioned such that they lie on cross sections of the surface. Shadecloth will be laid on top of the joists to produce a curved roof effect. So I can see we have a hyperbola here and can understand where the 9 joists would be positioned. Here is what I think this function is looking like, I can see where each joint would go in this picture: Our vertical plane for cross sections is going to take the form $ax+by=c$ I think and taking x or y as constants give the cross sections. $z = f(x, y) = \frac{5}{2} + \frac{1}{200} (9x^2-4c^2)$ and $z = f(x, y) = \frac{5}{2} + \frac{1}{200} (9c^2-4y^2)$ These are parabolas so I don't really know what to do with these, seems kind of useless with the problem. I'm first asked this by the book Which values of $a, b, c$ can be chosen in my general equation such that the cross sections of our original equation are straight lines in three-dimenstional space? Okay, I'd think I'd have to determine the positions of the joists to answer this. That's why I'm scratching my head. I'll continue here with more info. Each joist $J_{k}$, $k = 1, ..., 9$ is to have one end attached to the position $P_{k}$ and the other end attached to the position $Q_{k}$ in three-dimensional space. $P_{k}$ is specified in the question. Determine the other endpoints $Q_{k}$, such that each joist $J_{k}$ lies on the surface $f(x, y)$ and the $x$ and $y$ coordinates of each $Q_{k}$ lie on the boundary of the domain $[−6, 6] × [−6, 6].$ I'm not going to list all 9 points because I'd eventually like to solve this myself but I'll give 4 $P_{k}$ as an example here. $P_{1} = (-6, 3, f(-6, 3))$ $P_{3} = (-6, -3, f(-6, -3))$ $P_{5} = (-4, -6, f(-4, -6))$ $P_{7} = (0, -6, f(0, -6))$ So once I find the general equation. Which I'll probably do through some sort of simultaneous method maybe? Then I have to find the point on a plane from a vector at each $P_{k}$ 9 different times? I think that's how I go about solving it but it seems so inefficient and I'm also not sure. Any help appreciated, I'll keep working with mucking with the general equation to find a b and c values in the meantime.","Doing some revision and came across this question. Would rather step along my thought process first rather than just type down the questions at the start if that's okay. A roof is given by the function $z = f(x, y) = \frac{5}{2} + \frac{1}{200} (9x^2-4y^2)$ and is on the domain $[-6, 6] × [-6, 6]$ Nine timber joists will be cut, and positioned such that they lie on cross sections of the surface. Shadecloth will be laid on top of the joists to produce a curved roof effect. So I can see we have a hyperbola here and can understand where the 9 joists would be positioned. Here is what I think this function is looking like, I can see where each joint would go in this picture: Our vertical plane for cross sections is going to take the form $ax+by=c$ I think and taking x or y as constants give the cross sections. $z = f(x, y) = \frac{5}{2} + \frac{1}{200} (9x^2-4c^2)$ and $z = f(x, y) = \frac{5}{2} + \frac{1}{200} (9c^2-4y^2)$ These are parabolas so I don't really know what to do with these, seems kind of useless with the problem. I'm first asked this by the book Which values of $a, b, c$ can be chosen in my general equation such that the cross sections of our original equation are straight lines in three-dimenstional space? Okay, I'd think I'd have to determine the positions of the joists to answer this. That's why I'm scratching my head. I'll continue here with more info. Each joist $J_{k}$, $k = 1, ..., 9$ is to have one end attached to the position $P_{k}$ and the other end attached to the position $Q_{k}$ in three-dimensional space. $P_{k}$ is specified in the question. Determine the other endpoints $Q_{k}$, such that each joist $J_{k}$ lies on the surface $f(x, y)$ and the $x$ and $y$ coordinates of each $Q_{k}$ lie on the boundary of the domain $[−6, 6] × [−6, 6].$ I'm not going to list all 9 points because I'd eventually like to solve this myself but I'll give 4 $P_{k}$ as an example here. $P_{1} = (-6, 3, f(-6, 3))$ $P_{3} = (-6, -3, f(-6, -3))$ $P_{5} = (-4, -6, f(-4, -6))$ $P_{7} = (0, -6, f(0, -6))$ So once I find the general equation. Which I'll probably do through some sort of simultaneous method maybe? Then I have to find the point on a plane from a vector at each $P_{k}$ 9 different times? I think that's how I go about solving it but it seems so inefficient and I'm also not sure. Any help appreciated, I'll keep working with mucking with the general equation to find a b and c values in the meantime.",,"['calculus', 'linear-algebra', 'multivariable-calculus']"
60,Spivak Calculus on Manifolds 2-37 Question,Spivak Calculus on Manifolds 2-37 Question,,"I was working on problem 2-37(a) of Spivak's Calculus on Manifolds and I found that my approach to solving it was probably quite different from what was intended. To be clear, I understand the argument Spivak probably intended the reader to make. Also, just to clarify, I'm not saying my approach is better. Indeed I think the intended approach is preferable as it utilizes the theorems of the section and anticipates some of the ideas used to prove the Implicit Function Theorem in the section ahead. I just wanted to see if there was something wrong with my approach, as it doesn't require the same assumptions that were given in the problem. We are asked to show that if $f:\mathbb{R}^2 \rightarrow \mathbb{R}$ is continuously differentiable then $f$ is not injective. We argue that, assuming $f$ is continuously differentiable (though we could use a weaker assumption than this) and injective, it follows that $f(x,y)$ is differentiable on $\mathbb{R}^2$ and therefore continuous on $\mathbb{R}^2$. So $f(x,0)$ and $f(0,y)$ are continuous and injective on the interval $[-1,1]\subset \mathbb{R}$. By connectedness of $[-1,1]$ and continuity of $f(x,0)$ and $f(0,y)$, the images of $[-1,1]$ by $f(x,0)$ and $f(0,y)$ are both connected. $f(0,0)$ being a maximum or minimum of either function on $[-1,1]$ contradicts with $f$ being injective. If $f(0,0)$ is not a maximum or a minimum, then it follows by connectedness that both images contain some open interval about $f(0,0)$, which, again, contradicts with injectivity of $f$. Thanks in advance.","I was working on problem 2-37(a) of Spivak's Calculus on Manifolds and I found that my approach to solving it was probably quite different from what was intended. To be clear, I understand the argument Spivak probably intended the reader to make. Also, just to clarify, I'm not saying my approach is better. Indeed I think the intended approach is preferable as it utilizes the theorems of the section and anticipates some of the ideas used to prove the Implicit Function Theorem in the section ahead. I just wanted to see if there was something wrong with my approach, as it doesn't require the same assumptions that were given in the problem. We are asked to show that if $f:\mathbb{R}^2 \rightarrow \mathbb{R}$ is continuously differentiable then $f$ is not injective. We argue that, assuming $f$ is continuously differentiable (though we could use a weaker assumption than this) and injective, it follows that $f(x,y)$ is differentiable on $\mathbb{R}^2$ and therefore continuous on $\mathbb{R}^2$. So $f(x,0)$ and $f(0,y)$ are continuous and injective on the interval $[-1,1]\subset \mathbb{R}$. By connectedness of $[-1,1]$ and continuity of $f(x,0)$ and $f(0,y)$, the images of $[-1,1]$ by $f(x,0)$ and $f(0,y)$ are both connected. $f(0,0)$ being a maximum or minimum of either function on $[-1,1]$ contradicts with $f$ being injective. If $f(0,0)$ is not a maximum or a minimum, then it follows by connectedness that both images contain some open interval about $f(0,0)$, which, again, contradicts with injectivity of $f$. Thanks in advance.",,"['multivariable-calculus', 'proof-verification', 'inverse-function-theorem']"
61,Find the rates at which the volume V and the surface area S are changing with respect to time,Find the rates at which the volume V and the surface area S are changing with respect to time,,"The dimensions of a rectangular box are linear functions of time, $l(t), w(t), h(t).$ If length and width are increasing at $2$ $in./sec$ and height is decreasing  at $3 \; in./sec$ . Find the rates at which the volume V and the surface area S are changing with respect to time. If $l(0)=10$ and $w(0)=8$ and $h(0)=20$ . Is V increasing or decreasing when $t=5$ ? What about $S$ at $t=5$ My attempt :- $V = l(t)w(t)h(t) \Rightarrow \frac{dV}{dt} = \frac{dV}{dl} \frac{dl}{dt} + \frac{dV}{dw} \frac{dw}{dt} +\frac{dV}{dh} \frac{dh}{dt}= 2l(t)w(t) + 2l(t)h (t) -3l(t)w(t)$ ----> Equation 1 Similarly I can find it for the surface area. My problem here is how do I get $l(5), w(5)$ and $h(5)$ using: $l(0)=10$ and $w(0)=8$ and $h(0)=20$ ? Show I solve like this ? $\frac{dl}{dt} = 2 \Rightarrow l(t) = 2t + C \Rightarrow C = 10  \Rightarrow l(t) = 2t + 10 \Rightarrow l(5) = 20?$ and plug it in equation $1$ to get rate of change of volume at $t=5$ ? Please help me.","The dimensions of a rectangular box are linear functions of time, If length and width are increasing at and height is decreasing  at . Find the rates at which the volume V and the surface area S are changing with respect to time. If and and . Is V increasing or decreasing when ? What about at My attempt :- ----> Equation 1 Similarly I can find it for the surface area. My problem here is how do I get and using: and and ? Show I solve like this ? and plug it in equation to get rate of change of volume at ? Please help me.","l(t), w(t), h(t). 2 in./sec 3 \; in./sec l(0)=10 w(0)=8 h(0)=20 t=5 S t=5 V = l(t)w(t)h(t) \Rightarrow \frac{dV}{dt} = \frac{dV}{dl} \frac{dl}{dt} + \frac{dV}{dw} \frac{dw}{dt} +\frac{dV}{dh} \frac{dh}{dt}= 2l(t)w(t) + 2l(t)h (t) -3l(t)w(t) l(5), w(5) h(5) l(0)=10 w(0)=8 h(0)=20 \frac{dl}{dt} = 2 \Rightarrow l(t) = 2t + C \Rightarrow C = 10  \Rightarrow l(t) = 2t + 10 \Rightarrow l(5) = 20? 1 t=5","['calculus', 'multivariable-calculus']"
62,Evaluate $\int_{1}^{4} \int_{-1}^{2z} \int_{0}^{\sqrt{3}x} \frac{x-y}{x^2 +y^2} \;dy \; dx\; dz$,Evaluate,\int_{1}^{4} \int_{-1}^{2z} \int_{0}^{\sqrt{3}x} \frac{x-y}{x^2 +y^2} \;dy \; dx\; dz,$\int_{1}^{4} \int_{-1}^{2z} \int_{0}^{\sqrt{3}x} \frac{x-y}{x^2 +y^2} \;dy \; dx\; dz$ Consider $ \int_{0}^{\sqrt{3}x} \frac{x-y}{x^2 +y^2} \;dy =  $ $\int_{0}^{\sqrt{3}x} \frac{x}{x^2 +y^2} - \frac{y}{x^2+y^2} \;dy$ $\int_{0}^{\sqrt{3}x} \frac{x}{x^2 +y^2} \; dy  = \arctan(\sqrt{3}) = \pi/3$ $x^2 + y^2 = t \Rightarrow y \; dy = dt/2$ $ \int_{0}^{\sqrt{3}x} \frac{y}{x^2 +y^2} \;dy = \int_{x^2}^{3+x^2} \frac{dt}{t} = \frac{\ln(3+x^2)}{2} - \ln(x)$ $ \int_{0}^{\sqrt{3}x} \frac{x-y}{x^2 +y^2} \;dy  = \pi/3 + \ln(x) - \frac{\ln(3+x^2)}{2}$ Now it will be very lengthy and  difficult to integrate this with respect to x and z.  So I am stuck here. Is there any easy method to solve such questions ?,$\int_{1}^{4} \int_{-1}^{2z} \int_{0}^{\sqrt{3}x} \frac{x-y}{x^2 +y^2} \;dy \; dx\; dz$ Consider $ \int_{0}^{\sqrt{3}x} \frac{x-y}{x^2 +y^2} \;dy =  $ $\int_{0}^{\sqrt{3}x} \frac{x}{x^2 +y^2} - \frac{y}{x^2+y^2} \;dy$ $\int_{0}^{\sqrt{3}x} \frac{x}{x^2 +y^2} \; dy  = \arctan(\sqrt{3}) = \pi/3$ $x^2 + y^2 = t \Rightarrow y \; dy = dt/2$ $ \int_{0}^{\sqrt{3}x} \frac{y}{x^2 +y^2} \;dy = \int_{x^2}^{3+x^2} \frac{dt}{t} = \frac{\ln(3+x^2)}{2} - \ln(x)$ $ \int_{0}^{\sqrt{3}x} \frac{x-y}{x^2 +y^2} \;dy  = \pi/3 + \ln(x) - \frac{\ln(3+x^2)}{2}$ Now it will be very lengthy and  difficult to integrate this with respect to x and z.  So I am stuck here. Is there any easy method to solve such questions ?,,['multivariable-calculus']
63,"A projectile is fired up from the surface of the earth with initial velocity $(u_0, v_0)$",A projectile is fired up from the surface of the earth with initial velocity,"(u_0, v_0)","A projectile is fired up from the surface of the earth with initial velocity $(u_0, v_0)$. Under the influence of constant vertical acceleration − g the projectile reaches height $h_{max}$ and then falls back to earth. Neglecting air resistance, show that the fraction of time during its trajectory that the projectile spends above height $h_1$ is $|v_1|/v_0$, where $(u_1, v_1)$ is the projectile’s velocity vector at height $h_1$. Assume that $0 ≤ h_1 ≤ h_{max}$ I am having trouble solving this because I can't figure out the time the projectile is in the air without a function being given. Or would I not even need that to determine the time it is above $h_1$? Thanks for the help!","A projectile is fired up from the surface of the earth with initial velocity $(u_0, v_0)$. Under the influence of constant vertical acceleration − g the projectile reaches height $h_{max}$ and then falls back to earth. Neglecting air resistance, show that the fraction of time during its trajectory that the projectile spends above height $h_1$ is $|v_1|/v_0$, where $(u_1, v_1)$ is the projectile’s velocity vector at height $h_1$. Assume that $0 ≤ h_1 ≤ h_{max}$ I am having trouble solving this because I can't figure out the time the projectile is in the air without a function being given. Or would I not even need that to determine the time it is above $h_1$? Thanks for the help!",,['multivariable-calculus']
64,Divergence Theorem with a vector field.,Divergence Theorem with a vector field.,,I have been stuck on the following question for quite a while and my professor have not been helpful at all. I'm not sure how to make delta(f) a scalar so I can apply the theorem. Any useful hints are appreciated!,I have been stuck on the following question for quite a while and my professor have not been helpful at all. I'm not sure how to make delta(f) a scalar so I can apply the theorem. Any useful hints are appreciated!,,[]
65,Show that $T : \mathbb{R}^3 → \mathbb{R}^3$ is a linear transformation.,Show that  is a linear transformation.,T : \mathbb{R}^3 → \mathbb{R}^3,"Consider the function $T : \mathbb{R}^3 \to \mathbb{R}^3$ given by $$T(a, b, c) = (\operatorname{Curl }\vec{F}_{(a,b,c)} )(1, 1, 1),$$ where $\vec{F}_{(a,b,c)}$ is the vector field given by $$\vec{F}_{(a,b,c)}(x, y, z) = (axy + bxz, bxy + cyz, cxz + ayz).$$ Is $T$ a linear transformation? If so, calculate its matrix. I know that I need to take the curl of $\vec{F}_{(a,b,c)}$, and then plug in the point $(1, 1, 1)$ for $(x, y, z)$. But how would I determine if $T$ is a linear transformation? Thank you for your help!","Consider the function $T : \mathbb{R}^3 \to \mathbb{R}^3$ given by $$T(a, b, c) = (\operatorname{Curl }\vec{F}_{(a,b,c)} )(1, 1, 1),$$ where $\vec{F}_{(a,b,c)}$ is the vector field given by $$\vec{F}_{(a,b,c)}(x, y, z) = (axy + bxz, bxy + cyz, cxz + ayz).$$ Is $T$ a linear transformation? If so, calculate its matrix. I know that I need to take the curl of $\vec{F}_{(a,b,c)}$, and then plug in the point $(1, 1, 1)$ for $(x, y, z)$. But how would I determine if $T$ is a linear transformation? Thank you for your help!",,"['linear-algebra', 'multivariable-calculus', 'linear-transformations', 'vector-analysis']"
66,Parametrising the surface enclosed by a parametric curve,Parametrising the surface enclosed by a parametric curve,,"I have a curve given by $(\cos t, \sin t, \sin 2t)$ with $0 \leq t \leq 2\pi$: I need to integrate a function over any of the infinitely many surfaces to which this curve is a boundary. How would I find a parametrised form of such a surface?","I have a curve given by $(\cos t, \sin t, \sin 2t)$ with $0 \leq t \leq 2\pi$: I need to integrate a function over any of the infinitely many surfaces to which this curve is a boundary. How would I find a parametrised form of such a surface?",,"['multivariable-calculus', 'surfaces', 'parametric', 'surface-integrals']"
67,To prove $\int_0^1dx\int_x^{1/x}\frac{ydy}{(1+xy)^2(1+y^2)}=\frac{\pi-1}{4}$,To prove,\int_0^1dx\int_x^{1/x}\frac{ydy}{(1+xy)^2(1+y^2)}=\frac{\pi-1}{4},"Prove by changing order of integration $$\int_0^1dx\int_x^{1/x}\frac{ydy}{(1+xy)^2(1+y^2)}=\frac{\pi-1}{4}$$ When I tried to draw the region of integration for this,I'm not getting a closed boundary.I double checked and I'm not getting where I went wrong","Prove by changing order of integration $$\int_0^1dx\int_x^{1/x}\frac{ydy}{(1+xy)^2(1+y^2)}=\frac{\pi-1}{4}$$ When I tried to draw the region of integration for this,I'm not getting a closed boundary.I double checked and I'm not getting where I went wrong",,"['multivariable-calculus', 'multiple-integral']"
68,Find a set of parametric equations for the tangent line,Find a set of parametric equations for the tangent line,,"Find a set of parametric equations for the tangent line to the curve of intersection of the surface $x^2 + z^2 = 2$ and the surface $x^2 + y^2 - z^2 = 1$ at the point $(1, 1, 1)$.","Find a set of parametric equations for the tangent line to the curve of intersection of the surface $x^2 + z^2 = 2$ and the surface $x^2 + y^2 - z^2 = 1$ at the point $(1, 1, 1)$.",,['multivariable-calculus']
69,"To check if $f_x$ and $f$ are bounded for $f(x,y)=\frac{x^2y}{x^2+y^2}$",To check if  and  are bounded for,"f_x f f(x,y)=\frac{x^2y}{x^2+y^2}","Let $$f(x,y)=\frac{x^2y}{x^2+y^2}$$ for$(x,y) \ne(0,0)$ I want to check if $f_x$ and $f$ are bounded I calculated $f_x=\frac{2xy^3}{(x^2+y^2)^2}$ To check if its bounded below or above,do I have to calculate limits? If so,at which points?","Let $$f(x,y)=\frac{x^2y}{x^2+y^2}$$ for$(x,y) \ne(0,0)$ I want to check if $f_x$ and $f$ are bounded I calculated $f_x=\frac{2xy^3}{(x^2+y^2)^2}$ To check if its bounded below or above,do I have to calculate limits? If so,at which points?",,"['limits', 'multivariable-calculus']"
70,Multivariate Calculus: continuous functions,Multivariate Calculus: continuous functions,,"I was reading about continuity and i saw this problem as a exercise but I cannot find a way to prove it. Suppose that $f$ is continuous on a region and $f$ is different of zero, show that $f$ has only one sign. Thanks","I was reading about continuity and i saw this problem as a exercise but I cannot find a way to prove it. Suppose that $f$ is continuous on a region and $f$ is different of zero, show that $f$ has only one sign. Thanks",,"['real-analysis', 'multivariable-calculus', 'continuity']"
71,"Evaluate line integral $\int_c x\,dx+ y\,dy + z\,dz$, where $C$ is the straight line from $(1,0,0)$ to $(0,1,\pi/2)$","Evaluate line integral , where  is the straight line from  to","\int_c x\,dx+ y\,dy + z\,dz C (1,0,0) (0,1,\pi/2)","$ \displaystyle \int_c x\,dx+ y\,dy + z\,dz$, where $C$ is the straight line from   $(1,0,0)$ to $(0,1,\pi/2)$ Parametric form of line will be: $$x=1 - t, \quad y= t, \quad z= \frac{\pi t} 2$$ The integral becomes $$\int_0^1 2t -1 + \frac{\pi^2t} 4  \, dt$$ but the final answer given in my book is $\pi^2/8$ which I am not getting by this. So where am I making the mistake ?","$ \displaystyle \int_c x\,dx+ y\,dy + z\,dz$, where $C$ is the straight line from   $(1,0,0)$ to $(0,1,\pi/2)$ Parametric form of line will be: $$x=1 - t, \quad y= t, \quad z= \frac{\pi t} 2$$ The integral becomes $$\int_0^1 2t -1 + \frac{\pi^2t} 4  \, dt$$ but the final answer given in my book is $\pi^2/8$ which I am not getting by this. So where am I making the mistake ?",,['multivariable-calculus']
72,I'm calculating exterior derivatives and my answer is off by a factor of -1. What am I doing wrong?,I'm calculating exterior derivatives and my answer is off by a factor of -1. What am I doing wrong?,,"For example. I want to find the exterior derivative of $$\omega=(-3x-2y-z)dx +(2x^2+3y^3+4z^4)dy + (x+2y+3z)dz$$ Here's what I did: $$ \begin{align}  d\omega &= \left[\frac{\partial}{\partial x}(-3x-2y-z)\ dx+\frac{\partial}{\partial x}(2x^2+3y^3+4z^4)\ dy+\frac{\partial}{\partial x}(x+2y+3z)\ dz\right]dx \\ &+ \left[\frac{\partial}{\partial y}(-3x-2y-z)\ dx+\frac{\partial}{\partial y}(2x^2+3y^3+4z^4)\ dy+\frac{\partial}{\partial y}(x+2y+3z)\ dz\right]dy \\ &+ \left[\frac{\partial}{\partial z}(-3x-2y-z)\ dx+\frac{\partial}{\partial z}(2x^2+3y^3+4z^4)\ dy+\frac{\partial}{\partial z}(x+2y+3z)\ dz\right]dz \\ &= (-3\ dx +4x\ dy +1\ dz)\ dx +(-2\ dx+9y^2\ dy+2\ dz)\ dy+(-1\ dx+16z^3\ dy+3\ dz)\ dz \\ &= 4x\ dy\ dx+ dz\ dx-2\ dx\ dy+2\ dz\ dy-1\ dx\ dz+16z^3\ dy\ dz \\ &= -4x\ dx\ dy-2\ dx\ dy-1\ dx\ dz-1\ dx\ dz-2\ dy\ dz+16z^3\ dy\ dz \\ &= (-4x-2)\ dx\ dy-2\ dx\ dz+(16z^3-2)\ dy\ dz \end{align} $$ However, Maple says this: Maple says I'm off by a factor of -1? What am I doing wrong? Thank you!","For example. I want to find the exterior derivative of $$\omega=(-3x-2y-z)dx +(2x^2+3y^3+4z^4)dy + (x+2y+3z)dz$$ Here's what I did: $$ \begin{align}  d\omega &= \left[\frac{\partial}{\partial x}(-3x-2y-z)\ dx+\frac{\partial}{\partial x}(2x^2+3y^3+4z^4)\ dy+\frac{\partial}{\partial x}(x+2y+3z)\ dz\right]dx \\ &+ \left[\frac{\partial}{\partial y}(-3x-2y-z)\ dx+\frac{\partial}{\partial y}(2x^2+3y^3+4z^4)\ dy+\frac{\partial}{\partial y}(x+2y+3z)\ dz\right]dy \\ &+ \left[\frac{\partial}{\partial z}(-3x-2y-z)\ dx+\frac{\partial}{\partial z}(2x^2+3y^3+4z^4)\ dy+\frac{\partial}{\partial z}(x+2y+3z)\ dz\right]dz \\ &= (-3\ dx +4x\ dy +1\ dz)\ dx +(-2\ dx+9y^2\ dy+2\ dz)\ dy+(-1\ dx+16z^3\ dy+3\ dz)\ dz \\ &= 4x\ dy\ dx+ dz\ dx-2\ dx\ dy+2\ dz\ dy-1\ dx\ dz+16z^3\ dy\ dz \\ &= -4x\ dx\ dy-2\ dx\ dy-1\ dx\ dz-1\ dx\ dz-2\ dy\ dz+16z^3\ dy\ dz \\ &= (-4x-2)\ dx\ dy-2\ dx\ dz+(16z^3-2)\ dy\ dz \end{align} $$ However, Maple says this: Maple says I'm off by a factor of -1? What am I doing wrong? Thank you!",,"['multivariable-calculus', 'differential-geometry', 'differential-forms', 'exterior-algebra']"
73,"Global minimum of $f(x)=\langle Ax,x \rangle +2 \langle x,b \rangle+c$",Global minimum of,"f(x)=\langle Ax,x \rangle +2 \langle x,b \rangle+c","Given that $A\in M_n(\Bbb R)$ is a symmetric positive-definite matrix, $b \in \Bbb R^n , c\in \Bbb R$ I need to find the global minimum of the function  $$ f(x) =\langle Ax,x \rangle +2 \langle x,b \rangle + c.$$ To solve this I first wanted to solve $D_f(a)=0$ ($D_f(a)$ is the differential of $f$ at point $a$). In order to find a local minimum, then show it is also global. But it seems like a lot of work if I define $A=(a)_{ij}$. Is there a shorter/better way?","Given that $A\in M_n(\Bbb R)$ is a symmetric positive-definite matrix, $b \in \Bbb R^n , c\in \Bbb R$ I need to find the global minimum of the function  $$ f(x) =\langle Ax,x \rangle +2 \langle x,b \rangle + c.$$ To solve this I first wanted to solve $D_f(a)=0$ ($D_f(a)$ is the differential of $f$ at point $a$). In order to find a local minimum, then show it is also global. But it seems like a lot of work if I define $A=(a)_{ij}$. Is there a shorter/better way?",,"['multivariable-calculus', 'matrix-equations', 'matrix-calculus', 'maxima-minima', 'positive-definite']"
74,Affine Curvature,Affine Curvature,,"I was reading a paper related to convex curves, and encountered the following quantity: Let $\gamma$ be a convex curve and $k(\cdot)$ be its curvature (with respect to arc length). The author is using the following highly ambiguous (at least to me) notation: $$\int_\gamma k(s)^\frac{1}{3} ds$$ and calling this quantity the affine curvature. My question is: $\textbf{What does this notation mean?}$ First of all, what exactly is $k(s)?$ Secondly, what does it mean to integrate $k(s)$ with respect to $s$ along the curve $\gamma$? Any help will be greatly appreciated!","I was reading a paper related to convex curves, and encountered the following quantity: Let $\gamma$ be a convex curve and $k(\cdot)$ be its curvature (with respect to arc length). The author is using the following highly ambiguous (at least to me) notation: $$\int_\gamma k(s)^\frac{1}{3} ds$$ and calling this quantity the affine curvature. My question is: $\textbf{What does this notation mean?}$ First of all, what exactly is $k(s)?$ Secondly, what does it mean to integrate $k(s)$ with respect to $s$ along the curve $\gamma$? Any help will be greatly appreciated!",,['multivariable-calculus']
75,Surface Element ($dS$) of a surface integral over a sphere,Surface Element () of a surface integral over a sphere,dS,"I am having difficulty with the following snippet from my multi-variable calculus textbook: From what I understand, if $f(x,y,z)$ is a vector field on $\Bbb R^3$, then the surface integral of $f$ over a sphere of radius $R$ (centered at the origin), parametrized with spherical co-ordinates with the function $g(\theta,\phi)$, is $\int \int_{S} f \cdot dS$, where $dS = \frac {\partial g}{\partial \theta} \times \frac {\partial g} {\partial \phi}  d\theta d\phi $ ( **  in the textbook above it is written $T_u , T_v$ which are equivalent to my notation of  $ \frac {\partial g}{\partial \theta},  \frac {\partial g} {\partial \phi} $ **). I know that $T_u \times T_v$ is equal to $rR sin \phi$, where $r (x,y,z) = xi + yj + zk$. Is there an error with this last line since it seems to me that second equality is a scalar, whereas the first, third and fourth are vectors. Also how can I relate my definition of $dS$ with the one described in the picture above. Thanks.","I am having difficulty with the following snippet from my multi-variable calculus textbook: From what I understand, if $f(x,y,z)$ is a vector field on $\Bbb R^3$, then the surface integral of $f$ over a sphere of radius $R$ (centered at the origin), parametrized with spherical co-ordinates with the function $g(\theta,\phi)$, is $\int \int_{S} f \cdot dS$, where $dS = \frac {\partial g}{\partial \theta} \times \frac {\partial g} {\partial \phi}  d\theta d\phi $ ( **  in the textbook above it is written $T_u , T_v$ which are equivalent to my notation of  $ \frac {\partial g}{\partial \theta},  \frac {\partial g} {\partial \phi} $ **). I know that $T_u \times T_v$ is equal to $rR sin \phi$, where $r (x,y,z) = xi + yj + zk$. Is there an error with this last line since it seems to me that second equality is a scalar, whereas the first, third and fourth are vectors. Also how can I relate my definition of $dS$ with the one described in the picture above. Thanks.",,['multivariable-calculus']
76,Integral of softmax (i.e multi-variate sigmoid) over hyper-cube,Integral of softmax (i.e multi-variate sigmoid) over hyper-cube,,"Let $n$ and $k$ be a positive integers and $\mathbf{b}_1,\ldots,\mathbf{b}_k \in  \mathbb R^n$ (with $\mathbf{b}_l \ne 0$ for at least one $l$), $c_1,\ldots,c_k \in \mathbb R$. What does the following integral $$I(\mathbf{b}_1,\ldots,\mathbf{b}_k,c_1,\ldots,c_k): =\int_{[0,1]^n}\frac{1}{1 + \sum_{l=1}^k\exp(\mathbf{x}^T\mathbf{b}_l + c_l)}d \mathbf{x}$$ evaluate to ? Observations The 1-dimensional binary case (i.e $n=k=1$) is trivial. Indeed, from $$\int \frac{1}{1 + \exp(-x)}dx = \ln (1 + \exp(x)) + \text{constant}, $$ one gets $$ \begin{split} I(b, c) &= \int_0^1 \frac{1}{1 + \exp(bx + c)}dx = -\frac{1}{b} \int_{-c}^{-b-c} \frac{1}{1 + \exp(-z)}dz \\ &= \frac{1}{b}\ln\left(\frac{1 + \exp(-c)}{1 + \exp(-b-c)}\right), \end{split} $$ where we've used the change of variable: $-z = bx + c$. In particular, one has $$ I(-1,0) = \int_0^1 \frac{1}{1 + \exp(-x)}dx = \ln\left(\frac{1 + e}{2}\right) \approx 0.62$$","Let $n$ and $k$ be a positive integers and $\mathbf{b}_1,\ldots,\mathbf{b}_k \in  \mathbb R^n$ (with $\mathbf{b}_l \ne 0$ for at least one $l$), $c_1,\ldots,c_k \in \mathbb R$. What does the following integral $$I(\mathbf{b}_1,\ldots,\mathbf{b}_k,c_1,\ldots,c_k): =\int_{[0,1]^n}\frac{1}{1 + \sum_{l=1}^k\exp(\mathbf{x}^T\mathbf{b}_l + c_l)}d \mathbf{x}$$ evaluate to ? Observations The 1-dimensional binary case (i.e $n=k=1$) is trivial. Indeed, from $$\int \frac{1}{1 + \exp(-x)}dx = \ln (1 + \exp(x)) + \text{constant}, $$ one gets $$ \begin{split} I(b, c) &= \int_0^1 \frac{1}{1 + \exp(bx + c)}dx = -\frac{1}{b} \int_{-c}^{-b-c} \frac{1}{1 + \exp(-z)}dz \\ &= \frac{1}{b}\ln\left(\frac{1 + \exp(-c)}{1 + \exp(-b-c)}\right), \end{split} $$ where we've used the change of variable: $-z = bx + c$. In particular, one has $$ I(-1,0) = \int_0^1 \frac{1}{1 + \exp(-x)}dx = \ln\left(\frac{1 + e}{2}\right) \approx 0.62$$",,"['integration', 'multivariable-calculus', 'sampling', 'jacobian', 'change-of-variable']"
77,Explanation regarding limit of function of two independent variables as sequences.,Explanation regarding limit of function of two independent variables as sequences.,,"Please help! If $f(x,y)$ is a real valued function of two variables which is continuous .Then $\lim_{n\to\infty}f(x_{n},y_{n})=f(\lim_{n\to\infty} x_{n},\lim_{n\to\infty}y_{n})$, where $x_{n}$ and $y_{n}$ are any sequences of real numbers whose limits exist. Is this true in general(Give short proof)?","Please help! If $f(x,y)$ is a real valued function of two variables which is continuous .Then $\lim_{n\to\infty}f(x_{n},y_{n})=f(\lim_{n\to\infty} x_{n},\lim_{n\to\infty}y_{n})$, where $x_{n}$ and $y_{n}$ are any sequences of real numbers whose limits exist. Is this true in general(Give short proof)?",,"['real-analysis', 'multivariable-calculus']"
78,Derivative of inner product of matrix-valued functions of matrices,Derivative of inner product of matrix-valued functions of matrices,,"I am working on an optimization problem where I have to find the derivative of $$\langle f(\alpha) , Wf(\alpha) \rangle$$ with respect to $\alpha$. Here $\langle \cdot , \cdot \rangle$ denotes the Frobenius inner product, $W$ is a constant matrix, and $f$ a function of a matrix whose output is also a matrix. If $W$ is an indentity matrix, I can finds its derivative, but I am not able to find the derivative when $W$ is not an identity matrix.","I am working on an optimization problem where I have to find the derivative of $$\langle f(\alpha) , Wf(\alpha) \rangle$$ with respect to $\alpha$. Here $\langle \cdot , \cdot \rangle$ denotes the Frobenius inner product, $W$ is a constant matrix, and $f$ a function of a matrix whose output is also a matrix. If $W$ is an indentity matrix, I can finds its derivative, but I am not able to find the derivative when $W$ is not an identity matrix.",,"['matrices', 'multivariable-calculus', 'derivatives', 'matrix-calculus']"
79,How to find the maximum value of $x^2+y^2+z^2$ after applying the technique of Lagrange multipliers (complicated set of equations)?,How to find the maximum value of  after applying the technique of Lagrange multipliers (complicated set of equations)?,x^2+y^2+z^2,"I need to show that the maximum or the minimum of $f(x,y,z)=x^2+y^2+z^2=r^2$ given that $$(x^2+y^2+z^2)^2=\frac{x^2}{a^2}+\frac{y^2}{b^2}+\frac{z^2}{c^2}\tag{1}$$ and $$\lambda x+ \mu y+ \gamma z=0\tag{2}$$ is given by the equation $$\frac{a^2\lambda^2}{1-a^2r^2}+\frac{b^2\mu^2}{1-b^2r^2}+\frac{c^2\gamma^2}{1-c^2r^2}=0$$ My Approach: Say $$g(x,y,z)=(x^2+y^2+z^2)^2-(\frac{x^2}{a^2}+\frac{y^2}{b^2}+\frac{z^2}{c^2})$$ and $$h(x,y,z)=\lambda x+ \mu y+ \gamma z$$ Using the method of Lagrange multipliers I get $f_x=Ag_x+Bh_x$ $f_y=Ag_y+Bh_y$ $f_z=Ag_z+Bh_z$ Where $A$ and $B$ are constants. Hence we get: $$ \begin{cases} 2x=A(2(x^2+y^2+z^2)(2x)-2x/a^2)+B\lambda,\\ 2y=A(2(x^2+y^2+z^2)(2y)-2y/b^2)+B\mu,\\ 2z=A(2(x^2+y^2+z^2)(2z)-2z/c^2)+B\gamma. \end{cases}\tag{*} $$ Somehow I need to eliminate $A$ and $B$, but the direct method is too lengthy. I'm not sure how to elegantly reach $$\frac{a^2\lambda^2}{1-a^2r^2}+\frac{b^2\mu^2}{1-b^2r^2}+\frac{c^2\gamma^2}{1-c^2r^2}=0$$ from here. Any suggestions?","I need to show that the maximum or the minimum of $f(x,y,z)=x^2+y^2+z^2=r^2$ given that $$(x^2+y^2+z^2)^2=\frac{x^2}{a^2}+\frac{y^2}{b^2}+\frac{z^2}{c^2}\tag{1}$$ and $$\lambda x+ \mu y+ \gamma z=0\tag{2}$$ is given by the equation $$\frac{a^2\lambda^2}{1-a^2r^2}+\frac{b^2\mu^2}{1-b^2r^2}+\frac{c^2\gamma^2}{1-c^2r^2}=0$$ My Approach: Say $$g(x,y,z)=(x^2+y^2+z^2)^2-(\frac{x^2}{a^2}+\frac{y^2}{b^2}+\frac{z^2}{c^2})$$ and $$h(x,y,z)=\lambda x+ \mu y+ \gamma z$$ Using the method of Lagrange multipliers I get $f_x=Ag_x+Bh_x$ $f_y=Ag_y+Bh_y$ $f_z=Ag_z+Bh_z$ Where $A$ and $B$ are constants. Hence we get: $$ \begin{cases} 2x=A(2(x^2+y^2+z^2)(2x)-2x/a^2)+B\lambda,\\ 2y=A(2(x^2+y^2+z^2)(2y)-2y/b^2)+B\mu,\\ 2z=A(2(x^2+y^2+z^2)(2z)-2z/c^2)+B\gamma. \end{cases}\tag{*} $$ Somehow I need to eliminate $A$ and $B$, but the direct method is too lengthy. I'm not sure how to elegantly reach $$\frac{a^2\lambda^2}{1-a^2r^2}+\frac{b^2\mu^2}{1-b^2r^2}+\frac{c^2\gamma^2}{1-c^2r^2}=0$$ from here. Any suggestions?",,['multivariable-calculus']
80,"Find the domain and range of the function, $f(x,y) = \sqrt{x+y}$ and sketch the domain in the xy-plane.","Find the domain and range of the function,  and sketch the domain in the xy-plane.","f(x,y) = \sqrt{x+y}","I have found the domain to be $y \geq -x$. I have found the range to be $z \geq 0$, or $[ 0, \infty )$. I'm not sure how to sketch the domain in the xy-plane. I figured it would be a straight line through $(0,0)$ with $-1$ slope, and the values in the domain would be everything above that line, but I seem to be wrong.","I have found the domain to be $y \geq -x$. I have found the range to be $z \geq 0$, or $[ 0, \infty )$. I'm not sure how to sketch the domain in the xy-plane. I figured it would be a straight line through $(0,0)$ with $-1$ slope, and the values in the domain would be everything above that line, but I seem to be wrong.",,"['calculus', 'multivariable-calculus']"
81,"Show that $f(x,y) = 3xe^y - x^3 - e^{3y}$ has a single critical point and determine whether or not it's an absolute maximum.",Show that  has a single critical point and determine whether or not it's an absolute maximum.,"f(x,y) = 3xe^y - x^3 - e^{3y}","Show that $f(x,y) = 3xe^y - x^3 - e^{3y}$ has a single critical point and determine whether or not it's an absolute maximum. Solving $f_{x} = 0$ , $f_{y} = 0$ gives $(e^y)^2 = x$ and $(e^y) = x^2$ and $y \neq \ln(x) $ so we presume x = 1. Therefore there's only a single critical point. It's a local maximum. However, I want to know if it's also an absolute maximum. So now I have two questions: a) Is my thought process right so far? b) From this stage onwards, how do I determine whether this point P(1,0,0) is a local or an absolute maximum?","Show that $f(x,y) = 3xe^y - x^3 - e^{3y}$ has a single critical point and determine whether or not it's an absolute maximum. Solving $f_{x} = 0$ , $f_{y} = 0$ gives $(e^y)^2 = x$ and $(e^y) = x^2$ and $y \neq \ln(x) $ so we presume x = 1. Therefore there's only a single critical point. It's a local maximum. However, I want to know if it's also an absolute maximum. So now I have two questions: a) Is my thought process right so far? b) From this stage onwards, how do I determine whether this point P(1,0,0) is a local or an absolute maximum?",,"['multivariable-calculus', 'hessian-matrix']"
82,"Prove that function is not C1 in every neighbourhood of (0,0)","Prove that function is not C1 in every neighbourhood of (0,0)",,"$f(x,y)=(x^2+y^2)\sin{\frac{1}{\sqrt{x^2+y^2}}}$ Wolfram partial derivative: $$\frac{\partial f}{\partial x}=\frac{-x\cos{\frac{1}{\sqrt{x^2 + y^2}}}}{\sqrt{x^2 + y^2}} + 2 x \sin{\frac{1}{\sqrt{x^2 + y^2}}} $$ I figured out its partial derivatives are actually continuos by trying to find the derivatives from definition. What is the correct way to approach this?","$f(x,y)=(x^2+y^2)\sin{\frac{1}{\sqrt{x^2+y^2}}}$ Wolfram partial derivative: $$\frac{\partial f}{\partial x}=\frac{-x\cos{\frac{1}{\sqrt{x^2 + y^2}}}}{\sqrt{x^2 + y^2}} + 2 x \sin{\frac{1}{\sqrt{x^2 + y^2}}} $$ I figured out its partial derivatives are actually continuos by trying to find the derivatives from definition. What is the correct way to approach this?",,"['limits', 'multivariable-calculus', 'partial-derivative']"
83,Line integrals and reparametrization,Line integrals and reparametrization,,"$C = (1,2,0)$, $B= (1,0,2)$. part B : I have that my parametric equations are $x = 1$, $y = 2\cos(t)$, and $z = 2\sin (t)$. part C : I don't know how to approach this. Do I start with the divergence of $G$ and then reparametrize with $\tau$?","$C = (1,2,0)$, $B= (1,0,2)$. part B : I have that my parametric equations are $x = 1$, $y = 2\cos(t)$, and $z = 2\sin (t)$. part C : I don't know how to approach this. Do I start with the divergence of $G$ and then reparametrize with $\tau$?",,"['integration', 'multivariable-calculus', 'vector-analysis']"
84,"$\frac{dz}{dt}$ where $z=f(x,y)$",where,"\frac{dz}{dt} z=f(x,y)","I'm trying to differentiate the function $$z = x^3-y^3$$ where $$ x = \frac{1}{1+t}, \ \ \ \  y = \frac{t}{t+1}$$ I remember there being a proper way to do this using partial differentiation, but I decided to take a stab at it by expressing $x$ and $y$ in terms of $t$ and differentiating it explicitly. Prepare for some messy maths. Hopefully I didn't make any mistakes here, but the point is whether my approach is a valid one: $$\frac{dz}{dt} = \frac{d}{dt} [(\frac{1}{1+t})^3] - \frac{d}{dt}[ (\frac{t}{t+1})^3]$$ $$\frac{dz}{dt} = [3(\frac{1}{1+t})^2 * \frac{d}{dt}[(1+t)^{-1}]] - [3(\frac{t}{t+1})^2*\frac{d}{dt}[\frac{t}{t+1}]]$$ $$\frac{dz}{dt} = [-3(\frac{1}{1+t})^2*(\frac{1}{1+t})^{2}] - [3 (\frac{t}{t+1})^2 * (\frac{1}{t+1})^2]$$ Erm.. hopefully this isn't too hideous but this is what I have. Is this approach invalid or a poor idea, and if so, why?","I'm trying to differentiate the function $$z = x^3-y^3$$ where $$ x = \frac{1}{1+t}, \ \ \ \  y = \frac{t}{t+1}$$ I remember there being a proper way to do this using partial differentiation, but I decided to take a stab at it by expressing $x$ and $y$ in terms of $t$ and differentiating it explicitly. Prepare for some messy maths. Hopefully I didn't make any mistakes here, but the point is whether my approach is a valid one: $$\frac{dz}{dt} = \frac{d}{dt} [(\frac{1}{1+t})^3] - \frac{d}{dt}[ (\frac{t}{t+1})^3]$$ $$\frac{dz}{dt} = [3(\frac{1}{1+t})^2 * \frac{d}{dt}[(1+t)^{-1}]] - [3(\frac{t}{t+1})^2*\frac{d}{dt}[\frac{t}{t+1}]]$$ $$\frac{dz}{dt} = [-3(\frac{1}{1+t})^2*(\frac{1}{1+t})^{2}] - [3 (\frac{t}{t+1})^2 * (\frac{1}{t+1})^2]$$ Erm.. hopefully this isn't too hideous but this is what I have. Is this approach invalid or a poor idea, and if so, why?",,"['multivariable-calculus', 'chain-rule']"
85,Area of surface $z=16-x^2-y^2$ on the first octant,Area of surface  on the first octant,z=16-x^2-y^2,"I am asked to find the area of surface $z=16-x^2-y^2$ on the first octant. I proceeded the following way: Reasoning $$A = \iint_D \left\vert \frac{\partial P}{\partial x} \times \frac{\partial P}{\partial y} \right\vert \ dA$$ Given $P = (x,y,16-x^2-y^2)$ we have $$\frac{\partial P}{\partial x} = (1,0,-2x) \quad \frac{\partial P}{\partial y} = (0,1,-2y)$$ so that $$\int_{0}^{\pi/2} \int_{0}^{4} r \sqrt{1+4r^2} \ drd\theta = \cdots = \frac{\pi}{24} \left( 65 \sqrt{65} -1 \right)$$ Is this correct? Thank you.","I am asked to find the area of surface $z=16-x^2-y^2$ on the first octant. I proceeded the following way: Reasoning $$A = \iint_D \left\vert \frac{\partial P}{\partial x} \times \frac{\partial P}{\partial y} \right\vert \ dA$$ Given $P = (x,y,16-x^2-y^2)$ we have $$\frac{\partial P}{\partial x} = (1,0,-2x) \quad \frac{\partial P}{\partial y} = (0,1,-2y)$$ so that $$\int_{0}^{\pi/2} \int_{0}^{4} r \sqrt{1+4r^2} \ drd\theta = \cdots = \frac{\pi}{24} \left( 65 \sqrt{65} -1 \right)$$ Is this correct? Thank you.",,"['multivariable-calculus', 'surfaces']"
86,Does a double integral calculate an area or a volume?,Does a double integral calculate an area or a volume?,,This takes a little explanation.  I realize that double integrals can be used to calculate both an area or a volume but should I assume that in the case of calculating the area I am really calculating the volume and multiplying it by a height of 1 which just gives you the area? Or are they really two different techniques that depend solely on the context of the assigned problem ?,This takes a little explanation.  I realize that double integrals can be used to calculate both an area or a volume but should I assume that in the case of calculating the area I am really calculating the volume and multiplying it by a height of 1 which just gives you the area? Or are they really two different techniques that depend solely on the context of the assigned problem ?,,['multivariable-calculus']
87,Partial derivative of a function of a function,Partial derivative of a function of a function,,"Suppose $z=f(x+y)$. Taking partial derivatives w.r.t. $x$ on either side: $\partial z/\partial x = \partial f(x+y)/\partial x$. Let us denote $x+y$ by $u$. So $\partial z/\partial x = \partial f(u(x,y))/\partial x$. How do I evaluate the R.H.S. The chain rule I know is for finding total derivatives and not partial derivatives.","Suppose $z=f(x+y)$. Taking partial derivatives w.r.t. $x$ on either side: $\partial z/\partial x = \partial f(x+y)/\partial x$. Let us denote $x+y$ by $u$. So $\partial z/\partial x = \partial f(u(x,y))/\partial x$. How do I evaluate the R.H.S. The chain rule I know is for finding total derivatives and not partial derivatives.",,"['multivariable-calculus', 'partial-derivative']"
88,Minimize the power function with given constraint.,Minimize the power function with given constraint.,,"Let $x_1+x_2+\cdots+x_n=m$ then minimize the function $f(x_1,x_2,\cdots,x_n)=\sum_{i=1}^n (x_i)^{\alpha}$ where $x_i,m,n$ are positive integers and $\alpha>1$. My attempt: I applied the Lagrange's multiplier and found that minimum is obtained when all $x_i's$ are equal, but I am unable to prove that these $x_i's$ are integers. My observation: I think the minimum is obtained when all $x_i's$ are almost equal i.e. $|x_i-x_j|\le1, i,j=1,2,\cdots,n$.","Let $x_1+x_2+\cdots+x_n=m$ then minimize the function $f(x_1,x_2,\cdots,x_n)=\sum_{i=1}^n (x_i)^{\alpha}$ where $x_i,m,n$ are positive integers and $\alpha>1$. My attempt: I applied the Lagrange's multiplier and found that minimum is obtained when all $x_i's$ are equal, but I am unable to prove that these $x_i's$ are integers. My observation: I think the minimum is obtained when all $x_i's$ are almost equal i.e. $|x_i-x_j|\le1, i,j=1,2,\cdots,n$.",,"['real-analysis', 'multivariable-calculus', 'optimization', 'lagrange-multiplier', 'integer-programming']"
89,Triple Integral With Spherical Coordinates - Napkin Ring?,Triple Integral With Spherical Coordinates - Napkin Ring?,,"I wish to calculate the integral bellow, where $T$ is the region bounded by $x^2 + y^2 = 1$ and $x^2 + y^2 + z^2 = 4.$ It looks to me that it represents a Napkin ring. $$\iiint_T\bigl(x^2 + y^2\bigr)\,\text{d}V.$$ The answer is $\dfrac{\bigl(256 - 132\sqrt{3}\,\bigr)\pi}{15}.$","I wish to calculate the integral bellow, where $T$ is the region bounded by $x^2 + y^2 = 1$ and $x^2 + y^2 + z^2 = 4.$ It looks to me that it represents a Napkin ring. $$\iiint_T\bigl(x^2 + y^2\bigr)\,\text{d}V.$$ The answer is $\dfrac{\bigl(256 - 132\sqrt{3}\,\bigr)\pi}{15}.$",,"['integration', 'multivariable-calculus', 'definite-integrals', 'volume', 'spherical-coordinates']"
90,Can I get an example of a function that satisfies the finite dimensional MPT?,Can I get an example of a function that satisfies the finite dimensional MPT?,,"The finite dimensional Mountain Pass Theorem states: Theorem (Finite Dimensional MPT, Courant). Suppose that ϕ ∈ C1(Rn,R) is proper and possesses two distinct strict relative minima x1 and x2. Then ϕ possesses a third critical point x3 distinct from x1 and x2, characterized by ϕ(x3) = inf Σ∈Γmax x∈Σ ϕ(x) where Γ = {Σ ⊂ Rn;Σ is compact and connected and x1,x2 ∈ Σ} By proper Courant means the function is coercive. So I am trying to find a finite dimensional function that has two minima and is coercive for my thesis so I can study it and write an algorithm to find the saddle point. As a side question I don't particularly understand what the inf max means. I think it's the smallest of the maximums but I'm not really sure. Thank you for taking the time to read this question and taking the time to help me.","The finite dimensional Mountain Pass Theorem states: Theorem (Finite Dimensional MPT, Courant). Suppose that ϕ ∈ C1(Rn,R) is proper and possesses two distinct strict relative minima x1 and x2. Then ϕ possesses a third critical point x3 distinct from x1 and x2, characterized by ϕ(x3) = inf Σ∈Γmax x∈Σ ϕ(x) where Γ = {Σ ⊂ Rn;Σ is compact and connected and x1,x2 ∈ Σ} By proper Courant means the function is coercive. So I am trying to find a finite dimensional function that has two minima and is coercive for my thesis so I can study it and write an algorithm to find the saddle point. As a side question I don't particularly understand what the inf max means. I think it's the smallest of the maximums but I'm not really sure. Thank you for taking the time to read this question and taking the time to help me.",,"['functional-analysis', 'multivariable-calculus']"
91,2D Divergence Theorem: Question on the integral over the boundary curve,2D Divergence Theorem: Question on the integral over the boundary curve,,"Let $\;F=(F_1,F_2)\;$ be a two-dimensional vector field and   consider the rectangle $\;\mathcal R= PQRS\;$: If $\;\vec v\;$ is a function which gives outward-facing unit normal   vectors to $\;\partial \mathcal R\;$(=boundary of $\;\mathcal R\;$),   then by divergence theorem one can get: $\;\int_{\mathcal R} div(F_1,F_2) \;dx=\int_{SR} F_2\;dx_1  -\int_{PQ} F_2\;dx_1-\int_{SP} F_1\;dx_2+\int_{QR} F_1\;dx_2\;$ My Attempt: Searching on google, I found this: So, in my case it holds: $\;\int_{\mathcal R} div(F_1,F_2) \;dx=\;\int_{\partial \mathcal R} F_2 dx_1 - F_1 dx_2\;(1)$ In addition, $\;\partial \mathcal R=(SR)\cup(RQ)\cup(QP)\cup(PS)\;(2)$ Now, combining $\;(1),(2)\;$ and considering the counterclockwise direction , I get: $\;\int_{\partial \mathcal R} -F_2 dx_1 + F_1 dx_2=-\int_{SR} F_2 dx_1+\int_{PQ} F_2 dx_1-\int_{QR} F_2 dx_1+\int_{PS} F_2 dx_1+\int_{SR} F_1 dx_2-\int_{PQ} F_1 dx_2+\int_{QR} F_1 dx_2-\int_{PS} F_1 dx_2\;$ At this point, I've been stuck. Not only I find some additional terms such as $\;\int_{QR} F_2 dx_1\;$, but also some of the signs are wrong. I haven't seen boundary integrals since a very long time, so I'm $\;100\;$% sure there's something I'm missing here! Any help would be valuable. Thanks in advance!","Let $\;F=(F_1,F_2)\;$ be a two-dimensional vector field and   consider the rectangle $\;\mathcal R= PQRS\;$: If $\;\vec v\;$ is a function which gives outward-facing unit normal   vectors to $\;\partial \mathcal R\;$(=boundary of $\;\mathcal R\;$),   then by divergence theorem one can get: $\;\int_{\mathcal R} div(F_1,F_2) \;dx=\int_{SR} F_2\;dx_1  -\int_{PQ} F_2\;dx_1-\int_{SP} F_1\;dx_2+\int_{QR} F_1\;dx_2\;$ My Attempt: Searching on google, I found this: So, in my case it holds: $\;\int_{\mathcal R} div(F_1,F_2) \;dx=\;\int_{\partial \mathcal R} F_2 dx_1 - F_1 dx_2\;(1)$ In addition, $\;\partial \mathcal R=(SR)\cup(RQ)\cup(QP)\cup(PS)\;(2)$ Now, combining $\;(1),(2)\;$ and considering the counterclockwise direction , I get: $\;\int_{\partial \mathcal R} -F_2 dx_1 + F_1 dx_2=-\int_{SR} F_2 dx_1+\int_{PQ} F_2 dx_1-\int_{QR} F_2 dx_1+\int_{PS} F_2 dx_1+\int_{SR} F_1 dx_2-\int_{PQ} F_1 dx_2+\int_{QR} F_1 dx_2-\int_{PS} F_1 dx_2\;$ At this point, I've been stuck. Not only I find some additional terms such as $\;\int_{QR} F_2 dx_1\;$, but also some of the signs are wrong. I haven't seen boundary integrals since a very long time, so I'm $\;100\;$% sure there's something I'm missing here! Any help would be valuable. Thanks in advance!",,"['integration', 'multivariable-calculus', 'line-integrals', 'divergence-operator', 'greens-theorem']"
92,"Traveling between points on the surface $C(x,y)=e^{-2x^2-3y^2}$",Traveling between points on the surface,"C(x,y)=e^{-2x^2-3y^2}","We have a start point $(0,0)$ and an end point $(1,2)$, we would like to travel from the start to the end point. The surface is given by the equation $C(x,y)=e^{-2x^2-3y^2}$ and we would like to always travel in the direction of where the slope is the steepest (meaning that in each point we travel in the gradients direction). This means we will get some path between the points, and this path can be seen as the graph of the function $y=\phi (x)$. I want to determine what this function $\phi$ is. How do I approach this problem?","We have a start point $(0,0)$ and an end point $(1,2)$, we would like to travel from the start to the end point. The surface is given by the equation $C(x,y)=e^{-2x^2-3y^2}$ and we would like to always travel in the direction of where the slope is the steepest (meaning that in each point we travel in the gradients direction). This means we will get some path between the points, and this path can be seen as the graph of the function $y=\phi (x)$. I want to determine what this function $\phi$ is. How do I approach this problem?",,"['ordinary-differential-equations', 'multivariable-calculus', '3d']"
93,"Calculate surface area of $z = x^2, \quad 0 \leq y \leq x \leq 1$",Calculate surface area of,"z = x^2, \quad 0 \leq y \leq x \leq 1","Calculate surface area of $z = x^2, \quad 0 \leq y \leq x \leq 1$ My attempt : I parametrize the surface with  $$r(x,y) = (x,y,x^2),$$ where the domain E is given by:  $$0 \leq x\leq1,$$ $$0 \leq y \leq x.$$ I find $|r'_x \times r'_y|$: $$|r'_x \times r'_y|$$ $$|(1,0,2x) \times (0,1,0)|= |(-2x,0,1)| = \sqrt{4x^2+1}$$ The area is given by  $$\iint_E(\sqrt{4x^2+1})dxdy,$$ but I can't compute this integral and it doesn't seem to be computable with reasonable complexity according to wolfram alpha. What am I doing wrong? The answer is supposed to be: $\frac{5\sqrt{5}-1}{12}$","Calculate surface area of $z = x^2, \quad 0 \leq y \leq x \leq 1$ My attempt : I parametrize the surface with  $$r(x,y) = (x,y,x^2),$$ where the domain E is given by:  $$0 \leq x\leq1,$$ $$0 \leq y \leq x.$$ I find $|r'_x \times r'_y|$: $$|r'_x \times r'_y|$$ $$|(1,0,2x) \times (0,1,0)|= |(-2x,0,1)| = \sqrt{4x^2+1}$$ The area is given by  $$\iint_E(\sqrt{4x^2+1})dxdy,$$ but I can't compute this integral and it doesn't seem to be computable with reasonable complexity according to wolfram alpha. What am I doing wrong? The answer is supposed to be: $\frac{5\sqrt{5}-1}{12}$",,"['multivariable-calculus', 'surface-integrals']"
94,"How to prove that $f(x,y)=xy$ is differentiable at $(x_0,y_0)$?",How to prove that  is differentiable at ?,"f(x,y)=xy (x_0,y_0)","Prove that $f(x,y)=xy$ is differentiable at $(x_0,y_0)$ using the $\epsilon$ definition. We can use the definition of differentiability:  $$ f(x_0+\Delta x, y_0+\Delta y)-f(x_0,y_0)=f_x(x_0,y_0)\cdot\Delta x + f_y(x_0,y_0)\cdot \Delta y+\epsilon_1\cdot\Delta x+\epsilon_2\cdot\Delta y $$ According to the above formula we get: $$ x(y-y_0)-x_0(y-y_0)=(x-x_0)(y-y_0)=\epsilon_1\cdot(x-x_0)+\epsilon_2\cdot(y-y_0) $$ By comparing the coefficients we have that $\epsilon_2=x-x_0$ thus: $$ \lim_{(x,y)\to(x_0,y_0)} (x-x_0)=0 $$ But what about $\epsilon_1$? We can conclude that $\epsilon_1(x-x_0)=0$ but we can't know if $\epsilon_1$ necessarily goes to $0$.","Prove that $f(x,y)=xy$ is differentiable at $(x_0,y_0)$ using the $\epsilon$ definition. We can use the definition of differentiability:  $$ f(x_0+\Delta x, y_0+\Delta y)-f(x_0,y_0)=f_x(x_0,y_0)\cdot\Delta x + f_y(x_0,y_0)\cdot \Delta y+\epsilon_1\cdot\Delta x+\epsilon_2\cdot\Delta y $$ According to the above formula we get: $$ x(y-y_0)-x_0(y-y_0)=(x-x_0)(y-y_0)=\epsilon_1\cdot(x-x_0)+\epsilon_2\cdot(y-y_0) $$ By comparing the coefficients we have that $\epsilon_2=x-x_0$ thus: $$ \lim_{(x,y)\to(x_0,y_0)} (x-x_0)=0 $$ But what about $\epsilon_1$? We can conclude that $\epsilon_1(x-x_0)=0$ but we can't know if $\epsilon_1$ necessarily goes to $0$.",,"['multivariable-calculus', 'derivatives']"
95,How to interpret the curl and div geometrically? [duplicate],How to interpret the curl and div geometrically? [duplicate],,"This question already has answers here : Geometric intuition behind gradient, divergence and curl (3 answers) Closed 6 years ago . How to interpret the curl and div geometrically? My book said the vector derivative operator '$\nabla $' is considered as vector as follows : $$\left(\frac{\partial}{\partial x}, \frac{\partial}{\partial y}, \frac{\partial}{\partial z}\right)$$ Let $\mathbf F$ be  a vector field. First,  I know the fact that the $\operatorname{curl}$ is the tendency of rotation and the pivot is $\operatorname{curl} \mathbf F$ However, why the curl is the tendency of rotation geometrically? Easily, when calculating the outer product between two vector, we can interpret the result. The value is $\mathbf a\mathbf b\sin\theta$ and the direction is perpendicular to the two vectors. I want to know to draw $\nabla$ as vector. Second, the $\operatorname{div}$ is tendency of divergence or convergence. But which direction? If the result of $\operatorname{div} \mathbf F$ is plus, then the tendency is same direction to $\mathbf F$? (If minus, then opposite direction?)","This question already has answers here : Geometric intuition behind gradient, divergence and curl (3 answers) Closed 6 years ago . How to interpret the curl and div geometrically? My book said the vector derivative operator '$\nabla $' is considered as vector as follows : $$\left(\frac{\partial}{\partial x}, \frac{\partial}{\partial y}, \frac{\partial}{\partial z}\right)$$ Let $\mathbf F$ be  a vector field. First,  I know the fact that the $\operatorname{curl}$ is the tendency of rotation and the pivot is $\operatorname{curl} \mathbf F$ However, why the curl is the tendency of rotation geometrically? Easily, when calculating the outer product between two vector, we can interpret the result. The value is $\mathbf a\mathbf b\sin\theta$ and the direction is perpendicular to the two vectors. I want to know to draw $\nabla$ as vector. Second, the $\operatorname{div}$ is tendency of divergence or convergence. But which direction? If the result of $\operatorname{div} \mathbf F$ is plus, then the tendency is same direction to $\mathbf F$? (If minus, then opposite direction?)",,"['calculus', 'multivariable-calculus', 'derivatives', 'multiple-integral']"
96,Derive expression for the gradient of an arbitrary scalar function,Derive expression for the gradient of an arbitrary scalar function,,"I'm trying to do the following problem but I have no idea how to start and was wondering if someone could point me in the right direction? Start from the expression for the metric (the square of the infinitesimal line element) in an arbitrary orthogonal curvilinear 3-dimensional coor- dinate system, and obtain an expression for the gradient, grad$\phi$, of an arbitrary  differentiable  scalar  function $\phi$. The square of the infinitesimal line element I think is: $(ds)^2=h_1^2(dx_1)^2+h_2^2(dx_2)^2+h_3^2(dx_3)^2$ Any help would be greatly appreciated!","I'm trying to do the following problem but I have no idea how to start and was wondering if someone could point me in the right direction? Start from the expression for the metric (the square of the infinitesimal line element) in an arbitrary orthogonal curvilinear 3-dimensional coor- dinate system, and obtain an expression for the gradient, grad$\phi$, of an arbitrary  differentiable  scalar  function $\phi$. The square of the infinitesimal line element I think is: $(ds)^2=h_1^2(dx_1)^2+h_2^2(dx_2)^2+h_3^2(dx_3)^2$ Any help would be greatly appreciated!",,"['multivariable-calculus', 'differential-geometry']"
97,How that $\int_0^x\int_0^{\infty}\exp\left(-\frac{t^b+s^a}{2}\right) dtds \leq \int_0^x\int_{0}^{+\infty}\exp\left(-\frac{t^a+s^b}{2}\right)dtds$,How that,\int_0^x\int_0^{\infty}\exp\left(-\frac{t^b+s^a}{2}\right) dtds \leq \int_0^x\int_{0}^{+\infty}\exp\left(-\frac{t^a+s^b}{2}\right)dtds,"How to show the following inequality \begin{align} \int_{0}^{x}\int_{0}^{+\infty}\exp\left(-\frac{t^b+s^a}{2}\right)\,dt\,ds \leq \int_{0}^{x}\int_{0}^{+\infty}\exp\left(-\frac{t^a+s^b}{2}\right)\,dt\,ds \end{align} where $0<a<b$ and all $x>0$. I learned This is the inequality from this question . However, I was not able to follow the proof.","How to show the following inequality \begin{align} \int_{0}^{x}\int_{0}^{+\infty}\exp\left(-\frac{t^b+s^a}{2}\right)\,dt\,ds \leq \int_{0}^{x}\int_{0}^{+\infty}\exp\left(-\frac{t^a+s^b}{2}\right)\,dt\,ds \end{align} where $0<a<b$ and all $x>0$. I learned This is the inequality from this question . However, I was not able to follow the proof.",,"['calculus', 'real-analysis', 'multivariable-calculus']"
98,double integral integrand and region,double integral integrand and region,,"Find the volume of the cylinder with base as the  disk of unit radius i the $xy$ plane centred at $(1,1,0)$ and the top being the surface $z=\left[(x-1)^2+(y-1)^2\right]^{3/2}$. I set up the integral as $$\int\int_C\int_{0}^{\left[(x-1)^2+(y-1)^2\right]^{3/2}}dzdydz$$ $$\implies \int\int_C[(x-1)^2+(y-1)^2]^{3/2}dydx$$ $C=\{(x,y):(x-1)^2+(y-1)^2=1\}$ Hence the integral becomes $$\int_0^{2\pi}\int_0^1r^4drd\theta$$ Is this correct?","Find the volume of the cylinder with base as the  disk of unit radius i the $xy$ plane centred at $(1,1,0)$ and the top being the surface $z=\left[(x-1)^2+(y-1)^2\right]^{3/2}$. I set up the integral as $$\int\int_C\int_{0}^{\left[(x-1)^2+(y-1)^2\right]^{3/2}}dzdydz$$ $$\implies \int\int_C[(x-1)^2+(y-1)^2]^{3/2}dydx$$ $C=\{(x,y):(x-1)^2+(y-1)^2=1\}$ Hence the integral becomes $$\int_0^{2\pi}\int_0^1r^4drd\theta$$ Is this correct?",,"['multivariable-calculus', 'definite-integrals', 'polar-coordinates', 'change-of-variable']"
99,stokes theorem integral around a sphere and plane,stokes theorem integral around a sphere and plane,,"Evaluate using Stokes theorem $$\int_\gamma ydx+zdy+xdz$$ where $\gamma$ is the curve given by $(x-a)^2+(y-a)^2=z^2=2a^2$,$x+y=2a~~$, starting from $(2a,0,0)$ and then going below the z-plane. $$$$ What i tried was i calculate $curl(\vec{V})=-(i+j+k)~~$. Then i calculated the unit norma as $\hat{n}=\dfrac{(x-a)\hat{i}+(y-a)\hat{j}+z\hat{k}}{\sqrt{2}}$. Now $$\int\int_S(\nabla\times \vec{V})\cdot \hat{n}=\int\int_S(2a-x-y-z)dS=\int\int_R \sqrt{(x-a)^2+(y-a)^2}dxdy$$ where $R $ is the region bounded by the circle $(x-a)^2+(y-a)^2=2a^2$ $$$$  is this correct, if not please help. The problem with me using Stokes, Gauss theorems is that i am not able to handle the surfaces of integration, limits of integration. Please also suggest some books to make my understanding more clear","Evaluate using Stokes theorem $$\int_\gamma ydx+zdy+xdz$$ where $\gamma$ is the curve given by $(x-a)^2+(y-a)^2=z^2=2a^2$,$x+y=2a~~$, starting from $(2a,0,0)$ and then going below the z-plane. $$$$ What i tried was i calculate $curl(\vec{V})=-(i+j+k)~~$. Then i calculated the unit norma as $\hat{n}=\dfrac{(x-a)\hat{i}+(y-a)\hat{j}+z\hat{k}}{\sqrt{2}}$. Now $$\int\int_S(\nabla\times \vec{V})\cdot \hat{n}=\int\int_S(2a-x-y-z)dS=\int\int_R \sqrt{(x-a)^2+(y-a)^2}dxdy$$ where $R $ is the region bounded by the circle $(x-a)^2+(y-a)^2=2a^2$ $$$$  is this correct, if not please help. The problem with me using Stokes, Gauss theorems is that i am not able to handle the surfaces of integration, limits of integration. Please also suggest some books to make my understanding more clear",,"['multivariable-calculus', 'surface-integrals', 'stokes-theorem']"
