,title_raw,title_text,title_latex,body_raw,body_text,body_latex,tags
0,"Rationale behind MLE of $f_{\theta}(x) = \frac{1}{\theta} I_{\{1, \dots,\theta\}}(x)$",Rationale behind MLE of,"f_{\theta}(x) = \frac{1}{\theta} I_{\{1, \dots,\theta\}}(x)","Our probability density for $\theta \in \{1,\dots,\theta_0\}$ is $$f_{\theta}(x) = \frac{1}{\theta} I_{\{1, \dots,\theta\}}(x)$$ Let $X_{(n)}$ be the largest order statistic. Acooding to the solution, the likelihood function is $$\frac{1}{\theta^n} I_{\{X_{(n)}, \dots,\theta_0\}}(x)$$ I don't get why. Also, $X_{(n)}$ is supposed to be the MLE of $\theta$. Why?","Our probability density for $\theta \in \{1,\dots,\theta_0\}$ is $$f_{\theta}(x) = \frac{1}{\theta} I_{\{1, \dots,\theta\}}(x)$$ Let $X_{(n)}$ be the largest order statistic. Acooding to the solution, the likelihood function is $$\frac{1}{\theta^n} I_{\{X_{(n)}, \dots,\theta_0\}}(x)$$ I don't get why. Also, $X_{(n)}$ is supposed to be the MLE of $\theta$. Why?",,"['statistics', 'probability-distributions', 'uniform-distribution', 'maximum-likelihood']"
1,Composite Standard Deviations,Composite Standard Deviations,,"Normally, in AP Stats we were taught that to find the standard deviation of a new set of data, for example: to compute $stdv(A - B)$, you say $$stdv(A - B) = \sqrt{\text{var}(A) + \text{var}(B)}$$ This normally works all fine and dandy. For this set of data I'm dealing with for our final project, though, the calculator says the standard deviation of $84.1603$. The standard deviation of $A$ is $66.2304$, and of $B$ is $54.0266$. When I compute the standard deviation, I cannot get anything other than $85.4713$. Am I doing something wrong? This is crucial for the project, as this miscalculation makes the difference between this portion of our experiment being statistically significant or not. Please help.","Normally, in AP Stats we were taught that to find the standard deviation of a new set of data, for example: to compute $stdv(A - B)$, you say $$stdv(A - B) = \sqrt{\text{var}(A) + \text{var}(B)}$$ This normally works all fine and dandy. For this set of data I'm dealing with for our final project, though, the calculator says the standard deviation of $84.1603$. The standard deviation of $A$ is $66.2304$, and of $B$ is $54.0266$. When I compute the standard deviation, I cannot get anything other than $85.4713$. Am I doing something wrong? This is crucial for the project, as this miscalculation makes the difference between this portion of our experiment being statistically significant or not. Please help.",,"['statistics', 'standard-deviation']"
2,Likelihood Functon.,Likelihood Functon.,,"$n$ random variables or a random sample of size $n$ $\quad X_1,X_2,\ldots,X_n$  assume  a particular value $\quad x_1,x_2,\ldots,x_n$ . What does it mean? The set $\quad x_1,x_2,\ldots,x_n$ constitutes only a single value? or, $\quad x_1,x_2,\ldots,x_n$ are n values , that is, $X_1$ assumes the value $x_1$, $X_2$ assumes the value $x_2$,and so on? Why Likelihood Function is a function of parameter $\theta$? Why not is the function of random variables $\quad X_1,X_2,\ldots,X_n$? Let $n=1$ and $X_1$ has the normal density with mean, $\mu=6$ and variance $\sigma^2=1.$ Then the value of $X_1$ which is most likely to occur is $X_1=6.$  How it has been computed? I have thought in the way that since $X_1$ is a random variable it can assume the values  $x_1,\ldots,x_n$ [Then how $\quad X_1,X_2,\ldots,X_n$ can assume $\quad x_1,x_2,\ldots,x_n$?] and since the mean of the random variable is $6$ so The value which is most likely to occur is $6$. Is it the real process ?","$n$ random variables or a random sample of size $n$ $\quad X_1,X_2,\ldots,X_n$  assume  a particular value $\quad x_1,x_2,\ldots,x_n$ . What does it mean? The set $\quad x_1,x_2,\ldots,x_n$ constitutes only a single value? or, $\quad x_1,x_2,\ldots,x_n$ are n values , that is, $X_1$ assumes the value $x_1$, $X_2$ assumes the value $x_2$,and so on? Why Likelihood Function is a function of parameter $\theta$? Why not is the function of random variables $\quad X_1,X_2,\ldots,X_n$? Let $n=1$ and $X_1$ has the normal density with mean, $\mu=6$ and variance $\sigma^2=1.$ Then the value of $X_1$ which is most likely to occur is $X_1=6.$  How it has been computed? I have thought in the way that since $X_1$ is a random variable it can assume the values  $x_1,\ldots,x_n$ [Then how $\quad X_1,X_2,\ldots,X_n$ can assume $\quad x_1,x_2,\ldots,x_n$?] and since the mean of the random variable is $6$ so The value which is most likely to occur is $6$. Is it the real process ?",,"['probability', 'statistics', 'probability-theory', 'probability-distributions', 'statistical-inference']"
3,Maximum likelihood estimation and efficiency,Maximum likelihood estimation and efficiency,,"Let $X_1, ..., X_n$ be independent observations of a random variable $X$ that has with probability $p$ the distribution $\mathcal{U}[0,a]$ and with probability $1-p$ the distrbiution $\mathcal{U}[0,b]$ where $a$ and $b$ are known with $a<b$. What is the maximum-likelihood estimator for $p$? Is that estimator efficient? (This means, is the Cramér–Rao bound achieved?) I'm having quite some trouble with this task for several hours now. I think the density of $X$ is given by $$f_X(x) = p \frac{1}{a}1_{[0,a]}(x) + (1-p) \frac{1}{b}1_{[0,b]}(x).$$ For the trivial case $n=1$ this gives $\hat{p}=1_{[0,a]}(x_1)$ where the expected value of $X_1$ is $a/b$. So the estimator is in fact biased. So this doesn't really help with the second task. I can't even figure out the general likelihood function. I think it should be something like $L(p) = (p \frac{1}{a})^k ... + ((1-p) \frac{1}{b})^m ...$ where $k$ denotes the number of values from the sample $x_1, ..., x_n$ where $x_i\le a$ and $m=n-k$. But I don't understand how to deal with the indicator functions and if maybe I have to add a factor like with the binomial distribution. However, I took the raw form $L(p) = (p \frac{1}{a})^k + ((1-p) \frac{1}{b})^{(n-k)}$, $k$ like explained above and tried to maximize by using the log-likelihood. But from that I got $\hat p = \frac{an-bN}{an-bn}$ which doesn't strike me as correct as because of $a<b$ it's negative for small $N$. Nonetheless, I tried to calculate the Fisher information and compared it's inverse to the variance of $\hat p$ - I wasn't very surprised not to get an equality... Could be that I messed up some calculations, too - dealing with the derivatives of the log-likelihood was a little messy. Now I'm rather frustrated. Can anyone show me how to solve this?","Let $X_1, ..., X_n$ be independent observations of a random variable $X$ that has with probability $p$ the distribution $\mathcal{U}[0,a]$ and with probability $1-p$ the distrbiution $\mathcal{U}[0,b]$ where $a$ and $b$ are known with $a<b$. What is the maximum-likelihood estimator for $p$? Is that estimator efficient? (This means, is the Cramér–Rao bound achieved?) I'm having quite some trouble with this task for several hours now. I think the density of $X$ is given by $$f_X(x) = p \frac{1}{a}1_{[0,a]}(x) + (1-p) \frac{1}{b}1_{[0,b]}(x).$$ For the trivial case $n=1$ this gives $\hat{p}=1_{[0,a]}(x_1)$ where the expected value of $X_1$ is $a/b$. So the estimator is in fact biased. So this doesn't really help with the second task. I can't even figure out the general likelihood function. I think it should be something like $L(p) = (p \frac{1}{a})^k ... + ((1-p) \frac{1}{b})^m ...$ where $k$ denotes the number of values from the sample $x_1, ..., x_n$ where $x_i\le a$ and $m=n-k$. But I don't understand how to deal with the indicator functions and if maybe I have to add a factor like with the binomial distribution. However, I took the raw form $L(p) = (p \frac{1}{a})^k + ((1-p) \frac{1}{b})^{(n-k)}$, $k$ like explained above and tried to maximize by using the log-likelihood. But from that I got $\hat p = \frac{an-bN}{an-bn}$ which doesn't strike me as correct as because of $a<b$ it's negative for small $N$. Nonetheless, I tried to calculate the Fisher information and compared it's inverse to the variance of $\hat p$ - I wasn't very surprised not to get an equality... Could be that I messed up some calculations, too - dealing with the derivatives of the log-likelihood was a little messy. Now I'm rather frustrated. Can anyone show me how to solve this?",,['statistics']
4,summary of a summary,summary of a summary,,"At the printing company I work for, we have different materials that we print on; part numbers for those materials are assigned based on the unique supplier, material type, and material widths (so the same supplier could send us two different widths of the same material type, and they'd get different part numbers). With each part number, our supplier sends us a statistical summary of their quality control data - how many samples were taken (n), the average thickness of the samples, and the standard deviation. For the most common parts, we might get a few hundred different averages, Ns, and standard deviations. I'm trying to group those summaries together. Since the sample size is so high, I'm fine averaging the average thicknesses for each part number; the numbers are closely grouped enough that it shouldn't be a significant difference. My question is, how do I summarize the standard deviation? The quality tests are standardized and the instruments used to measure the thickness are constantly calibrated, so I have no reason to think that the measurements would be inconsistent from one shipment to the next.","At the printing company I work for, we have different materials that we print on; part numbers for those materials are assigned based on the unique supplier, material type, and material widths (so the same supplier could send us two different widths of the same material type, and they'd get different part numbers). With each part number, our supplier sends us a statistical summary of their quality control data - how many samples were taken (n), the average thickness of the samples, and the standard deviation. For the most common parts, we might get a few hundred different averages, Ns, and standard deviations. I'm trying to group those summaries together. Since the sample size is so high, I'm fine averaging the average thicknesses for each part number; the numbers are closely grouped enough that it shouldn't be a significant difference. My question is, how do I summarize the standard deviation? The quality tests are standardized and the instruments used to measure the thickness are constantly calibrated, so I have no reason to think that the measurements would be inconsistent from one shipment to the next.",,"['statistics', 'statistical-inference']"
5,Statistic question,Statistic question,,"1) A student took a chemistry exam where the exam scores were mound-shaped with a mean score of 90 and a standard deviation of 64. She also took a statistics exam where the scores were mound-shaped, the mean score was 70 and the standard deviation was 16. If the student's grades were 102 on the chemistry exam and 77 on the statistics exam, then: a. the student did relatively better on the chemistry exam than on the statistics exam, compared to the other students in each class b. the student's scores on both exams are comparable, when accounting for the scores of the other students in the two classes c. it is impossible to say which of the student's exam scores indicates the better performance d. the student did relatively better on the statistics exam than on the chemistry exam, compared to the other students in the two classes e. the student did relatively the same on both exams The correct answer is D but I don't know why.","1) A student took a chemistry exam where the exam scores were mound-shaped with a mean score of 90 and a standard deviation of 64. She also took a statistics exam where the scores were mound-shaped, the mean score was 70 and the standard deviation was 16. If the student's grades were 102 on the chemistry exam and 77 on the statistics exam, then: a. the student did relatively better on the chemistry exam than on the statistics exam, compared to the other students in each class b. the student's scores on both exams are comparable, when accounting for the scores of the other students in the two classes c. it is impossible to say which of the student's exam scores indicates the better performance d. the student did relatively better on the statistics exam than on the chemistry exam, compared to the other students in the two classes e. the student did relatively the same on both exams The correct answer is D but I don't know why.",,['statistics']
6,Sampling labeled items on a conveyor belts,Sampling labeled items on a conveyor belts,,"I have items on a moving conveyor belt. Every item has a label with a number that goes from $1$ to $N$; on the conveyor belt there are more than $N$ items. I have a camera above the items on the belt, the camera is in a fixed position; an example with $N=8$ (where * is the camera): * ...7 6 5 4 3 2 1 8 7 6 5 4 3 2 1 8 7 6 5 4 3 2... ------------------------------------------------- The camera knows when it can take a picture of the item because it is triggered by the presence of the item. The camera can't see the label on the items. It is assumed that the labels are ordered, there are no gaps, there are no duplicated labels in the wrong positions. For quality control purpose, the camera should take pictures of the items but it is not fast enough to take one picture for every item and so basically it can take the picture of one item, then skips the following $M$ items, then take the picture of one item, then skips the following $M$ items and so on; $M\gt0$ is a characteristic parameter of the camera in use. My goal is to find a sampling method in order to have pictures evenly distributed among the $N$ labels and to skip the least number of items. I have tried to solve the problem in this way: I compute a prime factorization of $N$; then I choose a prime $P$ that does not belong to the prime factorization of $N$ and $P\gt M+1$. Then the sampling goes in this way: the camera takes the picture of one item, then skips the following $P-1$ items, then takes the picture of one item, then skips the following $P-1$ items and so on. For example with $N=10$, $M=7$, my solution is $P=11$. I made some simulations of the above sampling method and it seems to meet my goal, but I would like to know whether it can be proved to be correct and/or optimized.","I have items on a moving conveyor belt. Every item has a label with a number that goes from $1$ to $N$; on the conveyor belt there are more than $N$ items. I have a camera above the items on the belt, the camera is in a fixed position; an example with $N=8$ (where * is the camera): * ...7 6 5 4 3 2 1 8 7 6 5 4 3 2 1 8 7 6 5 4 3 2... ------------------------------------------------- The camera knows when it can take a picture of the item because it is triggered by the presence of the item. The camera can't see the label on the items. It is assumed that the labels are ordered, there are no gaps, there are no duplicated labels in the wrong positions. For quality control purpose, the camera should take pictures of the items but it is not fast enough to take one picture for every item and so basically it can take the picture of one item, then skips the following $M$ items, then take the picture of one item, then skips the following $M$ items and so on; $M\gt0$ is a characteristic parameter of the camera in use. My goal is to find a sampling method in order to have pictures evenly distributed among the $N$ labels and to skip the least number of items. I have tried to solve the problem in this way: I compute a prime factorization of $N$; then I choose a prime $P$ that does not belong to the prime factorization of $N$ and $P\gt M+1$. Then the sampling goes in this way: the camera takes the picture of one item, then skips the following $P-1$ items, then takes the picture of one item, then skips the following $P-1$ items and so on. For example with $N=10$, $M=7$, my solution is $P=11$. I made some simulations of the above sampling method and it seems to meet my goal, but I would like to know whether it can be proved to be correct and/or optimized.",,"['statistics', 'prime-numbers', 'applications', 'sampling']"
7,"X,Y are independent RVs with known characteristic functions. Find P(X+Y=2).","X,Y are independent RVs with known characteristic functions. Find P(X+Y=2).",,"X,Y are independent random variables with the following characteristic functions: $ \phi_X(\theta) = \frac{1}{4}e^{i\theta}+\frac{3}{4}e^{i2\theta} \\   \phi_Y(\theta) = exp(e^{i\theta}-1)=e^{-1}\sum_{k=0}^\infty(\frac{e^{i\theta k}}{k!})$ Find: P(X+Y = 2) Taking the product of the characteristic functions (since the characteristic function of the convolution/sum of X + Y is the product of their char functions), we get: $\phi_{X+Y}(\theta)=\frac{1}{4}e^{-1}\sum_{K=0}^\infty(\frac{e^{i\theta (k+1)}}{k!})+\frac{3}{4}e^{-1}\sum_{K=0}^\infty(\frac{e^{i\theta (k+2)}}{k!}) \\$ Somehow, my lecturer goes from this line directly to: $\\$ $\therefore P(X+Y=2)=\frac{1}{4}e^{-1}\frac{1}{1!}+\frac{3}{4}e^{-1}\frac{1}{0!}=e^{-1}          $ Could someone please explain how my lecturer has made that step above. I can't seem the work it out. I thought maybe he was using some property of the Poisson distribution, since Y is Poisson distributed with $\lambda=1$. But, I couldn't see how he did that when the (K+1) and (K+2) components were in the indices, for the characteristic function of the sum.","X,Y are independent random variables with the following characteristic functions: $ \phi_X(\theta) = \frac{1}{4}e^{i\theta}+\frac{3}{4}e^{i2\theta} \\   \phi_Y(\theta) = exp(e^{i\theta}-1)=e^{-1}\sum_{k=0}^\infty(\frac{e^{i\theta k}}{k!})$ Find: P(X+Y = 2) Taking the product of the characteristic functions (since the characteristic function of the convolution/sum of X + Y is the product of their char functions), we get: $\phi_{X+Y}(\theta)=\frac{1}{4}e^{-1}\sum_{K=0}^\infty(\frac{e^{i\theta (k+1)}}{k!})+\frac{3}{4}e^{-1}\sum_{K=0}^\infty(\frac{e^{i\theta (k+2)}}{k!}) \\$ Somehow, my lecturer goes from this line directly to: $\\$ $\therefore P(X+Y=2)=\frac{1}{4}e^{-1}\frac{1}{1!}+\frac{3}{4}e^{-1}\frac{1}{0!}=e^{-1}          $ Could someone please explain how my lecturer has made that step above. I can't seem the work it out. I thought maybe he was using some property of the Poisson distribution, since Y is Poisson distributed with $\lambda=1$. But, I couldn't see how he did that when the (K+1) and (K+2) components were in the indices, for the characteristic function of the sum.",,"['statistics', 'probability-theory', 'probability-distributions', 'problem-solving', 'characteristic-functions']"
8,Estimating Poisson $\theta$ only from which percentage of intervals have events,Estimating Poisson  only from which percentage of intervals have events,\theta,"Radioactive particles are emitted randomly over time from a source at an average rate of per second. In $n$ time periods of varying lengths $t_1,t_2,\dots,t_n$ (seconds), the numbers of particles emitted (as determined by an automatic counter) were $y_1,y_1,\dots,y_n$ respectively. (b) Suppose that instead of knowing the $y_i$s, we know only whether or not there was one or more particles emitted in each time interval. Making a suitable assumption, give the likelihood function for $\theta$ based on these data, and describe how you can find the maximum likelihood estimate of $\theta$. What would be a ""suitable assumption""? The assumption I made was that each interval contains $1$ event if it has an event and no events if it does not have any event. This seems to be a trivializing and unsuitable assumption. What would the correct one be then?","Radioactive particles are emitted randomly over time from a source at an average rate of per second. In $n$ time periods of varying lengths $t_1,t_2,\dots,t_n$ (seconds), the numbers of particles emitted (as determined by an automatic counter) were $y_1,y_1,\dots,y_n$ respectively. (b) Suppose that instead of knowing the $y_i$s, we know only whether or not there was one or more particles emitted in each time interval. Making a suitable assumption, give the likelihood function for $\theta$ based on these data, and describe how you can find the maximum likelihood estimate of $\theta$. What would be a ""suitable assumption""? The assumption I made was that each interval contains $1$ event if it has an event and no events if it does not have any event. This seems to be a trivializing and unsuitable assumption. What would the correct one be then?",,"['probability', 'statistics', 'estimation']"
9,A question about Infinitesimal generator of Feller Process,A question about Infinitesimal generator of Feller Process,,"Let $S=% %TCIMACRO{\U{211d} }% %BeginExpansion \mathbb{R} %EndExpansion $, and consider the Feller process $\left( X_{t}\right) _{t\geq 0}$ with state space $S$ such that $X_{t}=t+X_{0}$ for all $t\geq 0$. Let $A$ be the infinitesimal generator of $\left( X_{t}\right) _{t\geq 0}.$ Show that $% D\left( A\right) =\left\{ f\in C\left( S\right) :f^{\prime }\in C\left( S\right) \right\} $ and $Af=f^{\prime }$ for all $f\in D\left( A\right) .$","Let $S=% %TCIMACRO{\U{211d} }% %BeginExpansion \mathbb{R} %EndExpansion $, and consider the Feller process $\left( X_{t}\right) _{t\geq 0}$ with state space $S$ such that $X_{t}=t+X_{0}$ for all $t\geq 0$. Let $A$ be the infinitesimal generator of $\left( X_{t}\right) _{t\geq 0}.$ Show that $% D\left( A\right) =\left\{ f\in C\left( S\right) :f^{\prime }\in C\left( S\right) \right\} $ and $Af=f^{\prime }$ for all $f\in D\left( A\right) .$",,"['probability', 'statistics', 'stochastic-processes', 'markov-process']"
10,MLE for a uniform distribution.,MLE for a uniform distribution.,,"Let $X_1,\dots,X_n$ be a sample of independent random variables with uniform distribution $(0,\theta)$. Find a $\hat{\theta}$ as an estimator for $\theta$ using the maximum likelihood method. the pdf is $\dfrac 1 \theta$ for $\theta \le x \le 2\theta$. I've had a go and found that the likelihood function is $θ^{-n}$, taken the log and differentiated it to find the max and got to $-\dfrac {n}{\theta}$. I've seen a similar answer posted here: maximum estimator method more known as MLE of a uniform distribution , but I don't understand what the order statistics reasoning is, and what bounds to impose (if any). I was also wondering how to check its a max when we look at the second order derivative?","Let $X_1,\dots,X_n$ be a sample of independent random variables with uniform distribution $(0,\theta)$. Find a $\hat{\theta}$ as an estimator for $\theta$ using the maximum likelihood method. the pdf is $\dfrac 1 \theta$ for $\theta \le x \le 2\theta$. I've had a go and found that the likelihood function is $θ^{-n}$, taken the log and differentiated it to find the max and got to $-\dfrac {n}{\theta}$. I've seen a similar answer posted here: maximum estimator method more known as MLE of a uniform distribution , but I don't understand what the order statistics reasoning is, and what bounds to impose (if any). I was also wondering how to check its a max when we look at the second order derivative?",,"['statistics', 'uniform-distribution', 'maximum-likelihood', 'parameter-estimation']"
11,Poker calculations on winrate,Poker calculations on winrate,,Just to learn I'm doing some calculations on my poker results. I'm trying to figure out the probability of given winrate. The available statistics I got in my database are: Hands: $30 000$ Winrate: $2$ bets per $100$ hands Expected winrate (what our winrate would be if we had gained all of our EV) : $8$ bets per $100$ hands StdDev: $8$ bets per $100$ hands What's the probability of our winrate per $100$ hands being less than $2$ given that our expected winrate is $8$ ? I'm thinking like this: Central limit theorem can be applied and therefore normalized Use this formula: $t = \dfrac{2-8}{8/(3k^{0.5})}$ And then look up $t$ in a $z$ -table? ( $t$ will be very close to $z$ because of the large sample). Am I doing this right?,Just to learn I'm doing some calculations on my poker results. I'm trying to figure out the probability of given winrate. The available statistics I got in my database are: Hands: Winrate: bets per hands Expected winrate (what our winrate would be if we had gained all of our EV) : bets per hands StdDev: bets per hands What's the probability of our winrate per hands being less than given that our expected winrate is ? I'm thinking like this: Central limit theorem can be applied and therefore normalized Use this formula: And then look up in a -table? ( will be very close to because of the large sample). Am I doing this right?,30 000 2 100 8 100 8 100 100 2 8 t = \dfrac{2-8}{8/(3k^{0.5})} t z t z,"['probability', 'statistics', 'poker']"
12,Random variable with density proportional to a function and finite in some points,Random variable with density proportional to a function and finite in some points,,"Let $X$ be a random variable on $[-1,3]$ with density $f(x) = k x^2$ (with $k \in \mathbb{R} $ to be determined) on $[-1,3]$ apart from some points s.t. $p(X=-1) = p(X=3) = \dfrac{1}{4} $ and $p(X=0) = \dfrac{1}{3}$. What is the cumulative distribution function of $X$? How much is $p(-1 \leq X < 3) $? To find $k$ I would just calculate $$ k = \dfrac{1}{\int_{-1}^3 x^2 \ dx} $$ and then the cumulative distribution should be $$F(x) = \int_{-1}^x f(t) \ dt $$ while $$p(-1 \leq X < 3) = 1 - p(X = 3) = \dfrac{3}{4}$$ Any suggestion would be appreciated.","Let $X$ be a random variable on $[-1,3]$ with density $f(x) = k x^2$ (with $k \in \mathbb{R} $ to be determined) on $[-1,3]$ apart from some points s.t. $p(X=-1) = p(X=3) = \dfrac{1}{4} $ and $p(X=0) = \dfrac{1}{3}$. What is the cumulative distribution function of $X$? How much is $p(-1 \leq X < 3) $? To find $k$ I would just calculate $$ k = \dfrac{1}{\int_{-1}^3 x^2 \ dx} $$ and then the cumulative distribution should be $$F(x) = \int_{-1}^x f(t) \ dt $$ while $$p(-1 \leq X < 3) = 1 - p(X = 3) = \dfrac{3}{4}$$ Any suggestion would be appreciated.",,"['probability', 'statistics', 'probability-distributions']"
13,Total probabilities of being admitted to any university,Total probabilities of being admitted to any university,,"Let's provide an hypothetical situation in which a student applies to 10 different universities whose number of applicants, admissions and admission rate you can see in the table below. ------------------------------------------------------ | Name      | Applicants | Admitted | Admission rate | ------------------------------------------------------ | Columbia  |  329281    |  22885   |     6.95%      | | Stanford  |  280915    |  19945   |     7.10%      | | MIT       |  111963    |  10894   |     9.73%      | | CalTech   |   17471    |   2231   |    12.77%      | | Cornell   |  117590    |  21131   |    17.97%      | | Berkeley  |  167324    |  36142   |    21.60%      | | UCLA      |  159635    |  40675   |    25.48%      | | Virginia  |   73030    |  24297   |    33.27%      | | Rochester |   30261    |  10319   |    34.10%      | | UCSD      |   80544    |  28593   |    35.50%      | ------------------------------------------------------ How can we estimate the total probabilities of being admitted in any of these 10 universities? We also have to take in account that some of the other applicants may have also applied to more than one university. I'd be grateful if you could redirect me to any similar question or provide me any useful formula to solve this problem. Thanks for your interest. Source: USNews Education","Let's provide an hypothetical situation in which a student applies to 10 different universities whose number of applicants, admissions and admission rate you can see in the table below. ------------------------------------------------------ | Name      | Applicants | Admitted | Admission rate | ------------------------------------------------------ | Columbia  |  329281    |  22885   |     6.95%      | | Stanford  |  280915    |  19945   |     7.10%      | | MIT       |  111963    |  10894   |     9.73%      | | CalTech   |   17471    |   2231   |    12.77%      | | Cornell   |  117590    |  21131   |    17.97%      | | Berkeley  |  167324    |  36142   |    21.60%      | | UCLA      |  159635    |  40675   |    25.48%      | | Virginia  |   73030    |  24297   |    33.27%      | | Rochester |   30261    |  10319   |    34.10%      | | UCSD      |   80544    |  28593   |    35.50%      | ------------------------------------------------------ How can we estimate the total probabilities of being admitted in any of these 10 universities? We also have to take in account that some of the other applicants may have also applied to more than one university. I'd be grateful if you could redirect me to any similar question or provide me any useful formula to solve this problem. Thanks for your interest. Source: USNews Education",,"['probability', 'statistics']"
14,help me with this regarding hypothesis using chi square distribution,help me with this regarding hypothesis using chi square distribution,,"The rope used in a lift produced by a certain manufacturer is known to have a mean tensile breaking strength of 1700 kg and standard deviation 10.5kg. A new component is added to the material which will, it is claimed, decrease the standard deviation without altering the tensile strength. Random samples of 20 pieces of the new rope are tested to destruction and the tensile strength of each piece is noted. The results are used to calculate unbiased estimates of the mean strength and standard deviation of the population of new rope. These were found to be 1724 kg and 8.5kg. (i) Test, at the 5% level, whether or not the variance has been reduced. (ii) What recommendation would you make to the manufacturer? my answer* i want to simply know when using hypothesis is it possible to use NULL HYPOTHESIS : population variance(sigma^2) = 10.5^2 ALTERNATE HYPOTHESIS : variance has reduced ==>population variance < 10.5^2 then test statistic =chi square = (n-1)s^2/(sigma)^2 = (20-1)8.5^2/10.5^ = 12.4512 degree of freedom =20-1=19 i have a problem with choosing the critical chi square value i have chosen chi square critical= chi square for .05 sl and df of 19 =10.1 Since test statistic doesnt fall in region of rejection we don't reject Null hypothesis. hence Variance has not been reduced. please help me with this problem","The rope used in a lift produced by a certain manufacturer is known to have a mean tensile breaking strength of 1700 kg and standard deviation 10.5kg. A new component is added to the material which will, it is claimed, decrease the standard deviation without altering the tensile strength. Random samples of 20 pieces of the new rope are tested to destruction and the tensile strength of each piece is noted. The results are used to calculate unbiased estimates of the mean strength and standard deviation of the population of new rope. These were found to be 1724 kg and 8.5kg. (i) Test, at the 5% level, whether or not the variance has been reduced. (ii) What recommendation would you make to the manufacturer? my answer* i want to simply know when using hypothesis is it possible to use NULL HYPOTHESIS : population variance(sigma^2) = 10.5^2 ALTERNATE HYPOTHESIS : variance has reduced ==>population variance < 10.5^2 then test statistic =chi square = (n-1)s^2/(sigma)^2 = (20-1)8.5^2/10.5^ = 12.4512 degree of freedom =20-1=19 i have a problem with choosing the critical chi square value i have chosen chi square critical= chi square for .05 sl and df of 19 =10.1 Since test statistic doesnt fall in region of rejection we don't reject Null hypothesis. hence Variance has not been reduced. please help me with this problem",,"['statistics', 'probability-distributions', 'statistical-inference']"
15,Continuous random variable question,Continuous random variable question,,$ X $ is a non-negative continuous random variable with density function $f$ and distribution function $F$. Use integration by parts to show that $ \int_0^{\infty}  ( 1- F(x)) dx =  \int_0^{\infty}xf(x)dx  $ I'm quite puzzled on how to even integrate $F(X)$ to get $f(x)$  :S,$ X $ is a non-negative continuous random variable with density function $f$ and distribution function $F$. Use integration by parts to show that $ \int_0^{\infty}  ( 1- F(x)) dx =  \int_0^{\infty}xf(x)dx  $ I'm quite puzzled on how to even integrate $F(X)$ to get $f(x)$  :S,,['statistics']
16,"Range of the distribution of $(1-X)$ when $X$ follows Beta distribution as $X\sim beta(p,q)$",Range of the distribution of  when  follows Beta distribution as,"(1-X) X X\sim beta(p,q)","if $X$ follows beta distribution with parameter $p$ and $q$  where $p>0\quad ,  q>0$ then $1-X$ follows beta distribution with parameters $q$ and $p$, that is if $X\sim beta(p,q)$ then $(1-X)\sim beta(q,p)$ i have come up with the result but the range of the distribution of $1-X$ is coming 1 to 0,that is, $1<(1-x)<0$ i computed the limit thus: if $x=0$ ,then , $(1-x)=(1-0)=1$ and if $x=1$ ,then , $(1-x)=(1-1)=0$ But the range should be $0<(1-x)<1$  otherwise i can't conclude  with the result that if $X\sim beta(p,q)$ then $(1-X)\sim beta(q,p)$ .","if $X$ follows beta distribution with parameter $p$ and $q$  where $p>0\quad ,  q>0$ then $1-X$ follows beta distribution with parameters $q$ and $p$, that is if $X\sim beta(p,q)$ then $(1-X)\sim beta(q,p)$ i have come up with the result but the range of the distribution of $1-X$ is coming 1 to 0,that is, $1<(1-x)<0$ i computed the limit thus: if $x=0$ ,then , $(1-x)=(1-0)=1$ and if $x=1$ ,then , $(1-x)=(1-1)=0$ But the range should be $0<(1-x)<1$  otherwise i can't conclude  with the result that if $X\sim beta(p,q)$ then $(1-X)\sim beta(q,p)$ .",,"['statistics', 'limits', 'probability-distributions']"
17,simplifying an asymptotic expression,simplifying an asymptotic expression,,"I have this expression in a statistics book, namely $nh(f(x) +o(1)+O_p(1/\sqrt{nh}))$. Where $f$ is a density function. Now, this expression is equal to $nhf(x)\{1+o_p(1)\}$. Note, that  $n\to \infty$, $h\to 0$, and $nh\to \infty$. I do appreciate any comment on how to get this final expression. Thanks in advance.","I have this expression in a statistics book, namely $nh(f(x) +o(1)+O_p(1/\sqrt{nh}))$. Where $f$ is a density function. Now, this expression is equal to $nhf(x)\{1+o_p(1)\}$. Note, that  $n\to \infty$, $h\to 0$, and $nh\to \infty$. I do appreciate any comment on how to get this final expression. Thanks in advance.",,"['statistics', 'asymptotics']"
18,A question regarding the Poisson Process,A question regarding the Poisson Process,,"I have the following question: Buses arrive at a city according to a Poisson process with a rate of 5 per hour. What is the probability that the fifth bus of the day arrives after midday given they start arriving at 9 a.m. What I think is the correct way to go about this question is that I calculate: $ P(N(0,3)\le4) $ The probability that less than or equal to four buses occur between 9am to midday. Because we have to reach the 4th bus in that time before the 5th bus can arrive. Provided I am correct with that notion the formula would look like: $   \sum_i_=_0^4 {15^i e^{-15}\over i!} $. Thank you","I have the following question: Buses arrive at a city according to a Poisson process with a rate of 5 per hour. What is the probability that the fifth bus of the day arrives after midday given they start arriving at 9 a.m. What I think is the correct way to go about this question is that I calculate: $ P(N(0,3)\le4) $ The probability that less than or equal to four buses occur between 9am to midday. Because we have to reach the 4th bus in that time before the 5th bus can arrive. Provided I am correct with that notion the formula would look like: $   \sum_i_=_0^4 {15^i e^{-15}\over i!} $. Thank you",,"['probability', 'statistics']"
19,Probability of finding a point on or in an $n$-dimensional unit sphere,Probability of finding a point on or in an -dimensional unit sphere,n,"If a point is chosen at random in an $N$-dimensional unit sphere, what is the probability of falling inside the sphere of radius $0.99999999$? What if $N=3$, $N=10^{23}$, or $N = \infty $? Okay, that is the question I have to address. I don't know how to bring the 2 concepts of Probability and $N$-dimensional hypersphere together to arrive at a solution. I found similar work 1 , and 2 . I still don't understand the underlying concept that would allow me to find $P(x,y)$.","If a point is chosen at random in an $N$-dimensional unit sphere, what is the probability of falling inside the sphere of radius $0.99999999$? What if $N=3$, $N=10^{23}$, or $N = \infty $? Okay, that is the question I have to address. I don't know how to bring the 2 concepts of Probability and $N$-dimensional hypersphere together to arrive at a solution. I found similar work 1 , and 2 . I still don't understand the underlying concept that would allow me to find $P(x,y)$.",,"['probability', 'statistics', 'geometric-probability']"
20,Which probability law?,Which probability law?,,"It may be a basic probability law in another form, but I cannot figure it out. Why can we say the following: $P(A∩B|C) = $$P(A|B∩C)P(B|C)$ Thank you.","It may be a basic probability law in another form, but I cannot figure it out. Why can we say the following: $P(A∩B|C) = $$P(A|B∩C)P(B|C)$ Thank you.",,"['probability', 'statistics', 'conditional-probability']"
21,Bayesian learning,Bayesian learning,,"Imagine we assume there are two different types of coins: Coin A: a fair coin, p(heads) = 0.5. Coin B: biased to heads at p(heads)=0.7. We then want to learn from samples which coin we are flipping. Assume a naive prior over the two coins, so we have a Beta distribution, $\beta_0(1,1)$. You flip the coin and see heads. Since you know the probability that coin A would generate heads is 0.5 and you know the probability that coin B would generate heads is 0.7, we update our distribution as: $$ \beta_1 = (1+\frac{0.5}{1.2},1+\frac{0.7}{1.2}) \approx (1.4167, 1.5833) $$ Is this the correct way to update the distribution or will it improperly bias the distribution in some way?","Imagine we assume there are two different types of coins: Coin A: a fair coin, p(heads) = 0.5. Coin B: biased to heads at p(heads)=0.7. We then want to learn from samples which coin we are flipping. Assume a naive prior over the two coins, so we have a Beta distribution, $\beta_0(1,1)$. You flip the coin and see heads. Since you know the probability that coin A would generate heads is 0.5 and you know the probability that coin B would generate heads is 0.7, we update our distribution as: $$ \beta_1 = (1+\frac{0.5}{1.2},1+\frac{0.7}{1.2}) \approx (1.4167, 1.5833) $$ Is this the correct way to update the distribution or will it improperly bias the distribution in some way?",,"['statistics', 'bayesian', 'statistical-inference']"
22,How can I mathematically show the similarity between these 3 plots?,How can I mathematically show the similarity between these 3 plots?,,I have 3 3D plots of field strength measured around an antenna. I want to calculate the mathematical similarity between the points of the field patterns. How can I do this? thanks,I have 3 3D plots of field strength measured around an antenna. I want to calculate the mathematical similarity between the points of the field patterns. How can I do this? thanks,,"['statistics', 'standard-deviation', 'correlation']"
23,Confidence intervals and different methods of sampling,Confidence intervals and different methods of sampling,,"I have two samples collected from the same population, where each sample is collected using a different method of selection, and both samples are large enough for the distribution of the sample distribution of the mean to be normal. Now, when calculating two 95% confidence intervals for the mean, using each of the two samples, I get two non-overlapping intervals, so I can be fairly confident that one of the two samples suggest a larger population mean than the other sample suggest, or in other words, they are suggesting two different population means even-though they are sampled from the same population. So, my question is now, is it valid to conclude that one, or both, of the sampling methods are flawed due to the discrepancy in the population means predicted by the two methods?","I have two samples collected from the same population, where each sample is collected using a different method of selection, and both samples are large enough for the distribution of the sample distribution of the mean to be normal. Now, when calculating two 95% confidence intervals for the mean, using each of the two samples, I get two non-overlapping intervals, so I can be fairly confident that one of the two samples suggest a larger population mean than the other sample suggest, or in other words, they are suggesting two different population means even-though they are sampled from the same population. So, my question is now, is it valid to conclude that one, or both, of the sampling methods are flawed due to the discrepancy in the population means predicted by the two methods?",,['statistics']
24,Is a parameterization defined to be surjective and/or injective?,Is a parameterization defined to be surjective and/or injective?,,"A parameterization is a mapping used in differential geometry for describing a manifold, and in statistics for describing a family of distributions, and may be used for other applications I don't know or list here. I was wondering if a parameterization is defined to be surjective? I guess yes, because, in statistics, it seems that if a parameterization is injective, we can then take its inverse. I am not sure if my understanding is correct, and if it is the same case in other areas than statistics. Is it not defined to be injective? I think yes, because for example, in statistics, there is another concept identifiability for an injective parameterization and unidentifiability for an noninjective parameterization. I am not sure if my understanding is correct, and if it is the same case in other areas than statistics. Thanks and regards!","A parameterization is a mapping used in differential geometry for describing a manifold, and in statistics for describing a family of distributions, and may be used for other applications I don't know or list here. I was wondering if a parameterization is defined to be surjective? I guess yes, because, in statistics, it seems that if a parameterization is injective, we can then take its inverse. I am not sure if my understanding is correct, and if it is the same case in other areas than statistics. Is it not defined to be injective? I think yes, because for example, in statistics, there is another concept identifiability for an injective parameterization and unidentifiability for an noninjective parameterization. I am not sure if my understanding is correct, and if it is the same case in other areas than statistics. Thanks and regards!",,"['statistics', 'differential-geometry', 'parametric']"
25,Find posterior mean,Find posterior mean,,"I have this problem, Let $X\sim U(0,\theta)$ with $\theta>0$.  Assume a signal random sample $X$, the squared error loss, and the prior $\pi(\theta) = \exp(1)$ i.e. $\pi(\theta) = \theta e^{-\theta}$ for $\theta>0$ (a) Find the posterior distribution of $\theta$. (b) Show that the posterior risk of an estimate of $\hat{\theta}$ is given by    $e^{x}\int_{x}^{\infty}(\hat{\theta}-\theta)^{2}e^{-\theta}\, d\theta$ (c) Find the posterior mean. (d) Show that the result in (c) is the minimizer of posterior risk. I've now completed parts (a) and (b), got that the prior distribution is $e^{x-\theta}$.  Now when I go to find the posterior mean I get $e^{x}$, but when I compute the integral in part (b) and take derivative with respect to $x$, set to 0 and solve for $x$ I get an expression that is in terms of $\theta$ and $\hat{\theta}$.  Obviously I'm getting radically different sorts of answers and so there's something very fundamental in what I'm doing wrong but I can't see it.","I have this problem, Let $X\sim U(0,\theta)$ with $\theta>0$.  Assume a signal random sample $X$, the squared error loss, and the prior $\pi(\theta) = \exp(1)$ i.e. $\pi(\theta) = \theta e^{-\theta}$ for $\theta>0$ (a) Find the posterior distribution of $\theta$. (b) Show that the posterior risk of an estimate of $\hat{\theta}$ is given by    $e^{x}\int_{x}^{\infty}(\hat{\theta}-\theta)^{2}e^{-\theta}\, d\theta$ (c) Find the posterior mean. (d) Show that the result in (c) is the minimizer of posterior risk. I've now completed parts (a) and (b), got that the prior distribution is $e^{x-\theta}$.  Now when I go to find the posterior mean I get $e^{x}$, but when I compute the integral in part (b) and take derivative with respect to $x$, set to 0 and solve for $x$ I get an expression that is in terms of $\theta$ and $\hat{\theta}$.  Obviously I'm getting radically different sorts of answers and so there's something very fundamental in what I'm doing wrong but I can't see it.",,"['statistics', 'bayesian']"
26,Getting P-value While Using Variance,Getting P-value While Using Variance,,"Suppose we observe a random sample of five measurements: 10, 13, 15, 15, 17, from a normal distribution with unknown mean $\mu_1$ and unknown variance $\sigma_1^2$, A second random sample from another normal population with unknown mean $\mu_2$ and unknown variance $\sigma_2^2$ yields the measurements: 13, 7, 9, 11. a) Test for evidence that $\sigma_1 > 1.0$. Complete the P-value for this test as accurately as possible. Draw a conclusion at $\alpha = 0.05$. Here's what I've done so far: Step#1: Calculate $\sigma_1$ $\sigma_1 = \sqrt \frac{(10-14)^2 + (13-14)^2 + (15-14)^2 + (15-14)^2 + (17-14)^2}{5} = \sqrt\frac{28}{5} = 2.366$ Step#2: Set up Hypothesis Test $H_0: \sigma_1 = 2.366$ $H_a: \sigma_1 > 1.0$ How do I proceed from here? Thanks. EDIT: Also have this question, and would appreciate some insight. b) Use the pivotal method(and a pivotal statistic with F distribution) to derive a 95% confidence interval for $\frac{\sigma_2}{\sigma_1}$. Work it out for these data. And test the null hypothesis that $\sigma_2 = \sigma_1$ at the 5% level of significance.","Suppose we observe a random sample of five measurements: 10, 13, 15, 15, 17, from a normal distribution with unknown mean $\mu_1$ and unknown variance $\sigma_1^2$, A second random sample from another normal population with unknown mean $\mu_2$ and unknown variance $\sigma_2^2$ yields the measurements: 13, 7, 9, 11. a) Test for evidence that $\sigma_1 > 1.0$. Complete the P-value for this test as accurately as possible. Draw a conclusion at $\alpha = 0.05$. Here's what I've done so far: Step#1: Calculate $\sigma_1$ $\sigma_1 = \sqrt \frac{(10-14)^2 + (13-14)^2 + (15-14)^2 + (15-14)^2 + (17-14)^2}{5} = \sqrt\frac{28}{5} = 2.366$ Step#2: Set up Hypothesis Test $H_0: \sigma_1 = 2.366$ $H_a: \sigma_1 > 1.0$ How do I proceed from here? Thanks. EDIT: Also have this question, and would appreciate some insight. b) Use the pivotal method(and a pivotal statistic with F distribution) to derive a 95% confidence interval for $\frac{\sigma_2}{\sigma_1}$. Work it out for these data. And test the null hypothesis that $\sigma_2 = \sigma_1$ at the 5% level of significance.",,['statistics']
27,Behaviour of Two Coupled Sequences Towards a Stable Distribution,Behaviour of Two Coupled Sequences Towards a Stable Distribution,,"The following question arises from research that I am doing in swarm intelligence. The relationships given come from geometric considerations which, I believe, should not be relevant for this problem. However, if anyone deems it necessary, I would be happy to provide more information. I have studied this problem using numerical simulations, and what I am after now are leads on how I can attack this problem formally, or re-formulate it such that it becomes more manageable, etc. I am not a mathematician, so apologies in advance for any shortcomings in formulation and/or notation; any sort of suggestion to improve the problem would be very welcome and appreciated. Firstly, we define the following two piecewise functions, where $R$ and $r$ are constants, and $R>r$: $\delta(*) = \begin{cases} 0 & \text{if $(*)<R-r$}\\ \sqrt{\frac{R}{r}((*)^2 - (R-r)^2)} & \text{if $R-r\leq(*)\leq R+r$} \\ \frac{2Rr}{\sqrt{(*)^2 - (R^2+2Rr)}} & \text{if $(*) > R+r$}\end{cases}$ $f(*)=\begin{cases} (*) & \text{if $(*)<R-r$} \\ R-r & \text{if $R-r\leq(*)\leq R+r$} \\ \sqrt{(*)^2-4Rr} & \text{if $(*) > R+r$}\end{cases}$ Now, consider an large number of points $(x,y)$, initially all occupying the same point in the $xy$-plane, $(x_0, y_0)$. The points transform themselves in epochs, as follows. At the beginning of an epoch, a random 'coin toss' decides whether the points will be $x$-updated or $y$-updated. For an $x$-update, each point undergoes the following transformation: $y \leftarrow  y + \zeta \delta(x)\\ x \leftarrow f(x)$ where $\zeta$ is a random number between $-1$ and $1$ generated differently for each point (with a uniform distribution). A $y$-update is the inverse of what is described above: $x \leftarrow  x + \zeta \delta(y)\\ y \leftarrow f(y)$ What is happening is that, after each epoch, the points are forming a new distribution in the $xy$-plane. This distribution becomes continuous if the number of points is allowed to tend towards infinity. From numerical results, it turns out that this process has a single distribution that is a stable attractor for any initial condition $(x_0, y_0)$. After many epochs, the distribution tends towards two lines: one line is horizontal, located at $y=R-r$, and ranges from $x = 0$ to $x = R-r$. The other line is vertical, located at $x=R-r$, and ranges from $y=0$ to $y=R-r$.","The following question arises from research that I am doing in swarm intelligence. The relationships given come from geometric considerations which, I believe, should not be relevant for this problem. However, if anyone deems it necessary, I would be happy to provide more information. I have studied this problem using numerical simulations, and what I am after now are leads on how I can attack this problem formally, or re-formulate it such that it becomes more manageable, etc. I am not a mathematician, so apologies in advance for any shortcomings in formulation and/or notation; any sort of suggestion to improve the problem would be very welcome and appreciated. Firstly, we define the following two piecewise functions, where $R$ and $r$ are constants, and $R>r$: $\delta(*) = \begin{cases} 0 & \text{if $(*)<R-r$}\\ \sqrt{\frac{R}{r}((*)^2 - (R-r)^2)} & \text{if $R-r\leq(*)\leq R+r$} \\ \frac{2Rr}{\sqrt{(*)^2 - (R^2+2Rr)}} & \text{if $(*) > R+r$}\end{cases}$ $f(*)=\begin{cases} (*) & \text{if $(*)<R-r$} \\ R-r & \text{if $R-r\leq(*)\leq R+r$} \\ \sqrt{(*)^2-4Rr} & \text{if $(*) > R+r$}\end{cases}$ Now, consider an large number of points $(x,y)$, initially all occupying the same point in the $xy$-plane, $(x_0, y_0)$. The points transform themselves in epochs, as follows. At the beginning of an epoch, a random 'coin toss' decides whether the points will be $x$-updated or $y$-updated. For an $x$-update, each point undergoes the following transformation: $y \leftarrow  y + \zeta \delta(x)\\ x \leftarrow f(x)$ where $\zeta$ is a random number between $-1$ and $1$ generated differently for each point (with a uniform distribution). A $y$-update is the inverse of what is described above: $x \leftarrow  x + \zeta \delta(y)\\ y \leftarrow f(y)$ What is happening is that, after each epoch, the points are forming a new distribution in the $xy$-plane. This distribution becomes continuous if the number of points is allowed to tend towards infinity. From numerical results, it turns out that this process has a single distribution that is a stable attractor for any initial condition $(x_0, y_0)$. After many epochs, the distribution tends towards two lines: one line is horizontal, located at $y=R-r$, and ranges from $x = 0$ to $x = R-r$. The other line is vertical, located at $x=R-r$, and ranges from $y=0$ to $y=R-r$.",,"['probability', 'sequences-and-series', 'statistics', 'probability-theory', 'probability-distributions']"
28,Latent Dirichlet allocation,Latent Dirichlet allocation,,"I am currently trying to understand Blei, Ng and Jordan 2003 JMLR paper ""latent Dirichlet allocation"". In section 3 page 997 I don't understand how to get to equation 3. The paper says ""integrating over theta and summing over z"". How come they push the summation over z after the product of the words? The sum of products and the product of the sums is not equal in general. What allows, in that case, to push the sum inside the product?","I am currently trying to understand Blei, Ng and Jordan 2003 JMLR paper ""latent Dirichlet allocation"". In section 3 page 997 I don't understand how to get to equation 3. The paper says ""integrating over theta and summing over z"". How come they push the summation over z after the product of the words? The sum of products and the product of the sums is not equal in general. What allows, in that case, to push the sum inside the product?",,"['statistics', 'computer-science']"
29,Generating random numbers with skewed distribution,Generating random numbers with skewed distribution,,I want to generate random numbers with skewed distribution. But I have only following information about distribution from the paper : skewed distribution where the value is 1 with probability 0.9 and 46 with probability 0.1. the distribution  has mean (5.5) I don't know how to generate random numbers with this information and I don't know what is the distribution function. I'm using jdistlib for random number generation. If anyone has experience using this library can help me with this library for skewed distribution random number generation?,I want to generate random numbers with skewed distribution. But I have only following information about distribution from the paper : skewed distribution where the value is 1 with probability 0.9 and 46 with probability 0.1. the distribution  has mean (5.5) I don't know how to generate random numbers with this information and I don't know what is the distribution function. I'm using jdistlib for random number generation. If anyone has experience using this library can help me with this library for skewed distribution random number generation?,,"['probability', 'statistics', 'probability-distributions', 'random']"
30,Biased coin hypothesis,Biased coin hypothesis,,"Let's assume, we threw a coin $110$ times and in $85$ tosses it was head. What is the probability that the coin is biased towards head? We can use chi squared test to test, whether the coin is biased, but using this test we only find out, that the coin is biased towards heads or tails and there seems to be no one tailed chi squared test. The same problem seems to appear when using z-test approach. What is the correct way to solve this problem?","Let's assume, we threw a coin $110$ times and in $85$ tosses it was head. What is the probability that the coin is biased towards head? We can use chi squared test to test, whether the coin is biased, but using this test we only find out, that the coin is biased towards heads or tails and there seems to be no one tailed chi squared test. The same problem seems to appear when using z-test approach. What is the correct way to solve this problem?",,['statistics']
31,Find standard deviation given standard deviation,Find standard deviation given standard deviation,,"How would I find the standard deviation of a value, V that is the average of other values, say heights of people, given that I have the standard deviation of the heights? I'm looking to improve my intuition and understanding of standard deviation.","How would I find the standard deviation of a value, V that is the average of other values, say heights of people, given that I have the standard deviation of the heights? I'm looking to improve my intuition and understanding of standard deviation.",,"['probability', 'statistics', 'random-variables', 'standard-deviation']"
32,Brownian Motion and the Functional CLT,Brownian Motion and the Functional CLT,,"Suppose we have a time series $(x_t\mid t\in \mathbb{Z})$ for which the partial sum process $X_T$ defined on the unit interval by $$ X_T(\xi)=\omega_T^{-1}\sum_{t=1}^{[T\xi]} (x_t-\mathbb{E}(x_t)),\quad \xi\in[0,1],$$ where $\omega_T^2=\mathrm{Var}(\sum_{t=1}^Tx_t)$, converges weakly to a standard Brownian motion $B$ as $T\rightarrow\infty$. Here is my question : Why does the above setup imply that $\omega^2_T\sim T\omega^2$ for some $\omega^2\in(0,\infty)$? I have read this statement in a paper, and the argument provided there is ""otherwise the limit process cannot have the Brownian property $\mathrm{E}((B(s)-B(r))^2)=s-r$ for $0\leq r < s \leq 1$."" However, I could not figure out a way to ""formalize"" this argument. Can anybody help me out there? I appreciate any help! Many thanks!!!","Suppose we have a time series $(x_t\mid t\in \mathbb{Z})$ for which the partial sum process $X_T$ defined on the unit interval by $$ X_T(\xi)=\omega_T^{-1}\sum_{t=1}^{[T\xi]} (x_t-\mathbb{E}(x_t)),\quad \xi\in[0,1],$$ where $\omega_T^2=\mathrm{Var}(\sum_{t=1}^Tx_t)$, converges weakly to a standard Brownian motion $B$ as $T\rightarrow\infty$. Here is my question : Why does the above setup imply that $\omega^2_T\sim T\omega^2$ for some $\omega^2\in(0,\infty)$? I have read this statement in a paper, and the argument provided there is ""otherwise the limit process cannot have the Brownian property $\mathrm{E}((B(s)-B(r))^2)=s-r$ for $0\leq r < s \leq 1$."" However, I could not figure out a way to ""formalize"" this argument. Can anybody help me out there? I appreciate any help! Many thanks!!!",,"['real-analysis', 'statistics', 'probability-theory', 'stochastic-processes', 'stochastic-calculus']"
33,"Maximum likelihood for $(\mu,\sigma)$ and other related questions",Maximum likelihood for  and other related questions,"(\mu,\sigma)","$$f(x)=\frac{1}{2\sigma}\exp\left(\frac{-|x-\mu|}{\sigma}\right)$$ $$\mu\in,\sigma>0$$ When trying to calculate the maximum likelihood for $(\mu,\sigma)$, I got as far as: $\log L(\mu,\sigma)=-n \log(2\sigma)-\frac{-\sum|x_i-\mu|}{\sigma}$ and I'm not really sure how am I supposed to calculate the derivative according to $\mu$ and according to $\sigma$. I also got stuck in the calculation of $E(x)$ and $E(x^2)$ and would really appreciate any assistance. I think I should recognize a PDF in the integral, but since the limits do not span the entire range (because of the abs..) I'm not sure how to do it. Thanks!!","$$f(x)=\frac{1}{2\sigma}\exp\left(\frac{-|x-\mu|}{\sigma}\right)$$ $$\mu\in,\sigma>0$$ When trying to calculate the maximum likelihood for $(\mu,\sigma)$, I got as far as: $\log L(\mu,\sigma)=-n \log(2\sigma)-\frac{-\sum|x_i-\mu|}{\sigma}$ and I'm not really sure how am I supposed to calculate the derivative according to $\mu$ and according to $\sigma$. I also got stuck in the calculation of $E(x)$ and $E(x^2)$ and would really appreciate any assistance. I think I should recognize a PDF in the integral, but since the limits do not span the entire range (because of the abs..) I'm not sure how to do it. Thanks!!",,"['probability', 'statistics', 'probability-theory', 'calculus-of-variations']"
34,Bias of Estimator with square root of a sum of squared random variables,Bias of Estimator with square root of a sum of squared random variables,,Got a distribution of $f_X(x;\theta) = (x/\theta^2) \exp(-x^2/2\theta^2)$ for $x \ge 0$ where the MLE is calculated as $\theta_{MLE} = \sqrt{(\sum_{i=1}^{n}x^2_i)/2n}$ So now need to find if it's unbiased by taking the expected value of the beast. How would this be approached?,Got a distribution of $f_X(x;\theta) = (x/\theta^2) \exp(-x^2/2\theta^2)$ for $x \ge 0$ where the MLE is calculated as $\theta_{MLE} = \sqrt{(\sum_{i=1}^{n}x^2_i)/2n}$ So now need to find if it's unbiased by taking the expected value of the beast. How would this be approached?,,['statistics']
35,Likelihood ratio test of $n$ iid $\operatorname{Poisson}(\theta)$ random variables.,Likelihood ratio test of  iid  random variables.,n \operatorname{Poisson}(\theta),"Let $X_1, X_2, ..., X_n$ be iid random variables, each with a Poisson distribution with parameter $\theta$. Find the form of the likelihood ratio test of $H_0: \theta = 1$ against $H_1:\theta=1.21$. By using the Central Limit Theorem to approximate the distribution of $\sum_i X_i$, show that the smallest value of $n$ required to make $\alpha=0.05$ and $\beta \leq0.1$ is approximately $212$.","Let $X_1, X_2, ..., X_n$ be iid random variables, each with a Poisson distribution with parameter $\theta$. Find the form of the likelihood ratio test of $H_0: \theta = 1$ against $H_1:\theta=1.21$. By using the Central Limit Theorem to approximate the distribution of $\sum_i X_i$, show that the smallest value of $n$ required to make $\alpha=0.05$ and $\beta \leq0.1$ is approximately $212$.",,['probability']
36,Obscure Probability Question,Obscure Probability Question,,"Suppose that blood chloride concentration (mmol/L) has a normal distribution with mean 104 and standard deviation 5 (information in the article “Mathematical Model of   Chloride Concentration in Human Blood,” J. of Med. Engr. and Tech., 2006: 25–30, including a normal probability plot as described in Section 4.6, supports this assumption). a.What is the probability that chloride concentration   equals 105? Is less than 105? Is at most 105? b.What is the probability that chloride concentration   differs from the mean by more than 1 standard deviation? Does this probability depend on the values of $\sigma$ and $\mu$ c. How would you characterize the most extreme .1% of chloride concentration values? I am having trouble with part c). I'm just not quite sure what it is asking.","Suppose that blood chloride concentration (mmol/L) has a normal distribution with mean 104 and standard deviation 5 (information in the article “Mathematical Model of   Chloride Concentration in Human Blood,” J. of Med. Engr. and Tech., 2006: 25–30, including a normal probability plot as described in Section 4.6, supports this assumption). a.What is the probability that chloride concentration   equals 105? Is less than 105? Is at most 105? b.What is the probability that chloride concentration   differs from the mean by more than 1 standard deviation? Does this probability depend on the values of $\sigma$ and $\mu$ c. How would you characterize the most extreme .1% of chloride concentration values? I am having trouble with part c). I'm just not quite sure what it is asking.",,"['probability', 'statistics']"
37,Marginal Density of X,Marginal Density of X,,"I need to find the marginal density of $x$ where $$f(x, y) = xe^{-x-y}$$ Where the inputs must abide by: $x, y > 0$ My approach to solving the problem was: $\int_0^\infty \int_0^\infty xe^{-x-y}dxdy$ $= \int_0^\infty \left[ (x+1)(-e^{-x-y})\right]_0^\infty dy$ $= \int_0^\infty 0 dy$ $= 0$ But that does not make sense at all. The density function should not and cannot be $0$.","I need to find the marginal density of $x$ where $$f(x, y) = xe^{-x-y}$$ Where the inputs must abide by: $x, y > 0$ My approach to solving the problem was: $\int_0^\infty \int_0^\infty xe^{-x-y}dxdy$ $= \int_0^\infty \left[ (x+1)(-e^{-x-y})\right]_0^\infty dy$ $= \int_0^\infty 0 dy$ $= 0$ But that does not make sense at all. The density function should not and cannot be $0$.",,"['statistics', 'integration']"
38,Is my answer to this combinatorics question correct? Counting the number of functions where $F(a) = F(b)$?,Is my answer to this combinatorics question correct? Counting the number of functions where ?,F(a) = F(b),"Let $A = \{a, b, c, d, e, f\}$ and $B = \{1, 2, 3, 4, 5\}$. How many functions $F$ from $A$ to $B$ are there such that $F(a) = F(b)$. I looked at it like this: If $F(a)$ and $F(b)$ are equal, that means a and b must map to the same value in $B$. So that basically means they're considered one entity, and $A = \{ab, c, d, e, f\}$ so the new size is $5$. Matching, there would be $5$ choices for $ab$, $5$ choices for $c$, for $d$, for $e$, for $f$, so $5^5$ is the answer. Is that right? (Also, why is it not $6^5$? For a function, could it not match with none of them? Would $c$ not have the option of matching with $1, 2, 3, 4, 5$ or none of them ($6$ options)?)","Let $A = \{a, b, c, d, e, f\}$ and $B = \{1, 2, 3, 4, 5\}$. How many functions $F$ from $A$ to $B$ are there such that $F(a) = F(b)$. I looked at it like this: If $F(a)$ and $F(b)$ are equal, that means a and b must map to the same value in $B$. So that basically means they're considered one entity, and $A = \{ab, c, d, e, f\}$ so the new size is $5$. Matching, there would be $5$ choices for $ab$, $5$ choices for $c$, for $d$, for $e$, for $f$, so $5^5$ is the answer. Is that right? (Also, why is it not $6^5$? For a function, could it not match with none of them? Would $c$ not have the option of matching with $1, 2, 3, 4, 5$ or none of them ($6$ options)?)",,"['statistics', 'discrete-mathematics', 'permutations', 'combinatorics']"
39,Probability with Chi-Square distribution,Probability with Chi-Square distribution,,"What is the difference, when calculating probabilities of Chi-Square distributions, between $<$ and $\leq$ or $>$ and $\geq$. For example, say you are asked to find P$(\chi_{5}^{2} \leq 1.145)$. I know that this is $=0.05$ from the table of Chi-Square distributions, but what if you were asked to find P$(\chi_{5}^{2} < 1.145)$? How would this be different?","What is the difference, when calculating probabilities of Chi-Square distributions, between $<$ and $\leq$ or $>$ and $\geq$. For example, say you are asked to find P$(\chi_{5}^{2} \leq 1.145)$. I know that this is $=0.05$ from the table of Chi-Square distributions, but what if you were asked to find P$(\chi_{5}^{2} < 1.145)$? How would this be different?",,"['probability', 'statistics', 'probability-theory']"
40,How do you find the mean and variance with new observations?,How do you find the mean and variance with new observations?,,"When given a mean and variance of a sample, without knowing the observations, how would you then find the new mean and variance given more observations? Any help with this would be much appreciated. Thank you in advance","When given a mean and variance of a sample, without knowing the observations, how would you then find the new mean and variance given more observations? Any help with this would be much appreciated. Thank you in advance",,['statistics']
41,Game Theory: determining the value of the foxhole game,Game Theory: determining the value of the foxhole game,,"""A soldier can hide in one of five foxholes, and a gunner can hide in four spots: A, B C, and D. The configuration looks like this: 1 (A) 2 (B) 3 (C) 4 (D) 5. If a shot is fired at a location and the soldier is in an adjacent foxhole (ex: shot is fired at B and soldier is in hole #2 or #3), the gunner received a reward of 1. Otherwise, the gunner receives a reward of 0. Assume that this is a zero-sum game We are given that the optimal strategy for the soldier is to hide 1/3 of the time in foxholes 1, 3 and 5. For the gunner, an optimal strategy is to shoot 1.3 of the time at A, 1/3 of the time at D, and 1/3 of the time at B or C. I have to determine the value of the game for the gunner. Honestly, I have no idea where to start. I know why the player should hide at holes one and five, but not three. How would I go about solving this>.","""A soldier can hide in one of five foxholes, and a gunner can hide in four spots: A, B C, and D. The configuration looks like this: 1 (A) 2 (B) 3 (C) 4 (D) 5. If a shot is fired at a location and the soldier is in an adjacent foxhole (ex: shot is fired at B and soldier is in hole #2 or #3), the gunner received a reward of 1. Otherwise, the gunner receives a reward of 0. Assume that this is a zero-sum game We are given that the optimal strategy for the soldier is to hide 1/3 of the time in foxholes 1, 3 and 5. For the gunner, an optimal strategy is to shoot 1.3 of the time at A, 1/3 of the time at D, and 1/3 of the time at B or C. I have to determine the value of the game for the gunner. Honestly, I have no idea where to start. I know why the player should hide at holes one and five, but not three. How would I go about solving this>.",,"['statistics', 'game-theory']"
42,Combinatorics Statistics Question,Combinatorics Statistics Question,,"The problem I am working on is: An academic department with five faculty members—Anderson, Box, Cox, Cramer, and Fisher—must select two of its members to serve on a personnel review committee. Because the work will be time-consuming, no one is anx-ious to serve, so it is decided that the representative will be selected by putting the names on identical pieces of paper and then randomly selecting two. a.What is the probability that both Anderson and Box will be selected? [Hint:List the equally likely outcomes.] b.What is the probability that at least one of the two members whose name begins with C is selected? c. If the five faculty members have taught for 3, 6, 7, 10, and 14 years, respectively, at the university, what is the probability that the two chosen representatives have a total of at least 15 years’ teaching experience there? For a), I figured that since probability of Anderson being chosen is $1/5$ and Box being chosen is $1/5$ the answer would simply be $2/5$. It isn't, though. It is $0.1$ How did they get that answer? I might need help with parts b) and c) as well.","The problem I am working on is: An academic department with five faculty members—Anderson, Box, Cox, Cramer, and Fisher—must select two of its members to serve on a personnel review committee. Because the work will be time-consuming, no one is anx-ious to serve, so it is decided that the representative will be selected by putting the names on identical pieces of paper and then randomly selecting two. a.What is the probability that both Anderson and Box will be selected? [Hint:List the equally likely outcomes.] b.What is the probability that at least one of the two members whose name begins with C is selected? c. If the five faculty members have taught for 3, 6, 7, 10, and 14 years, respectively, at the university, what is the probability that the two chosen representatives have a total of at least 15 years’ teaching experience there? For a), I figured that since probability of Anderson being chosen is $1/5$ and Box being chosen is $1/5$ the answer would simply be $2/5$. It isn't, though. It is $0.1$ How did they get that answer? I might need help with parts b) and c) as well.",,"['probability', 'statistics']"
43,What am I reinventing? RE: Linear regression modeling for frequency of discrete events,What am I reinventing? RE: Linear regression modeling for frequency of discrete events,,"I'm looking to model the frequency of events to quantify how much that frequency is increasing or decreasing.  For the sake of concreteness think of the events as web page hits for several low traffic web sites, and I would like to compare how much they are ""trending"" up or down relative to one another.  My main question is what am I reinventing? In broad strokes this is the what I'm thinking.  I have a set of events at a set of times $E = \{t_1,...t_N\}$ with $t_i < 0$.  From these I have an event distribution function which is the sum of Dirac delta functions. $$ \Phi(t) = K\sum_{i=1}^N\delta(t-t_i) $$ where K is some normalizing factor. I would like to model this like a linear regression with $L(t) = at + b$ by minimizing $ \Vert{L-\Phi}\Vert$.  Older events should be weighted less, so my inner product measure would be something like: $$ d\omega = e^{kt}dt $$ Before I started digging out the details of this (the normalization, the inner product measure, etc.) I got the nagging suspicion that this has been done before :)  So my question is -- where can I read about the established theory, practice, and terminology for this type of problem?  Any suggestions about how to either rephrase the title of this question or reformulate the problem are appreciated as well.","I'm looking to model the frequency of events to quantify how much that frequency is increasing or decreasing.  For the sake of concreteness think of the events as web page hits for several low traffic web sites, and I would like to compare how much they are ""trending"" up or down relative to one another.  My main question is what am I reinventing? In broad strokes this is the what I'm thinking.  I have a set of events at a set of times $E = \{t_1,...t_N\}$ with $t_i < 0$.  From these I have an event distribution function which is the sum of Dirac delta functions. $$ \Phi(t) = K\sum_{i=1}^N\delta(t-t_i) $$ where K is some normalizing factor. I would like to model this like a linear regression with $L(t) = at + b$ by minimizing $ \Vert{L-\Phi}\Vert$.  Older events should be weighted less, so my inner product measure would be something like: $$ d\omega = e^{kt}dt $$ Before I started digging out the details of this (the normalization, the inner product measure, etc.) I got the nagging suspicion that this has been done before :)  So my question is -- where can I read about the established theory, practice, and terminology for this type of problem?  Any suggestions about how to either rephrase the title of this question or reformulate the problem are appreciated as well.",,"['statistics', 'probability-distributions', 'regression']"
44,"Given a product of these two functions, can I recover the factorization?","Given a product of these two functions, can I recover the factorization?",,"Sorry for the vague question, I'm not sure how to make this more specific. Let $\sigma(x)$ denote the logistic function $\frac{1}{1 + e^{-x}}$, and $f(y)$ denotes the density of some random variable. I am capable of determining the following function:  $$ h(y) = \sigma(\alpha + \beta y) f(y). $$ That is, I know $h(y)$ for all $y$. I am interested in whether I am capable of recovering $(\alpha, \beta, f)$ from this information. My gut says ""no"" but I'm not strongly convinced either way. I think one approach is to ask if  $$ \sigma(\alpha + \beta y) f(y) = \sigma(a + by) g(y), $$ implies that $\alpha = a, \beta = b, f = g$. Since $g$ must integrate to $1$, what I need is $$ \int \frac{\sigma(\alpha + \beta y)}{\sigma(a + by)} f(y) \ dy = \int \frac{h(y)}{\sigma(a + by)} \ dy= 1, $$ to have multiple solutions to show that I can't recover $(\alpha, \beta, f)$; by assumption, I know it has at least one. Since this seems like it might depend on the form of $h(y)$, conditions under which this is possible are also desirable.","Sorry for the vague question, I'm not sure how to make this more specific. Let $\sigma(x)$ denote the logistic function $\frac{1}{1 + e^{-x}}$, and $f(y)$ denotes the density of some random variable. I am capable of determining the following function:  $$ h(y) = \sigma(\alpha + \beta y) f(y). $$ That is, I know $h(y)$ for all $y$. I am interested in whether I am capable of recovering $(\alpha, \beta, f)$ from this information. My gut says ""no"" but I'm not strongly convinced either way. I think one approach is to ask if  $$ \sigma(\alpha + \beta y) f(y) = \sigma(a + by) g(y), $$ implies that $\alpha = a, \beta = b, f = g$. Since $g$ must integrate to $1$, what I need is $$ \int \frac{\sigma(\alpha + \beta y)}{\sigma(a + by)} f(y) \ dy = \int \frac{h(y)}{\sigma(a + by)} \ dy= 1, $$ to have multiple solutions to show that I can't recover $(\alpha, \beta, f)$; by assumption, I know it has at least one. Since this seems like it might depend on the form of $h(y)$, conditions under which this is possible are also desirable.",,"['probability', 'statistics']"
45,Probability distribution..,Probability distribution..,,"If $Z$ represents number of times $6$ appeared in two independent throws of a die, what would be the probability of $A =${maximum of two throws was $2$ and $Z = 0$}. $Z = 0$ means $6$ appeared $0$ time. -My try- Probability of $6$ appears $ 0$ time = $\frac{25}{36}$ maximum of two throws would be $(1,2)$ or $(2,1)$ or $(2,2)$. So there are $3$ ways to get maximum of two throws was $2$. Therefore $P(A) = \frac3{36} \times \frac{25}{36}$ ???","If $Z$ represents number of times $6$ appeared in two independent throws of a die, what would be the probability of $A =${maximum of two throws was $2$ and $Z = 0$}. $Z = 0$ means $6$ appeared $0$ time. -My try- Probability of $6$ appears $ 0$ time = $\frac{25}{36}$ maximum of two throws would be $(1,2)$ or $(2,1)$ or $(2,2)$. So there are $3$ ways to get maximum of two throws was $2$. Therefore $P(A) = \frac3{36} \times \frac{25}{36}$ ???",,"['statistics', 'probability-distributions']"
46,How do I use student's-t distribution without the sample size?,How do I use student's-t distribution without the sample size?,,"Here is my question (homework obviously): A sample from a normal population produced variance 4.0. Find the size of the sample if the sample mean deviates from the population mean by no more than 2.0 with a probability of at least 0.95. So I'm trying to find $n$, the sample size, having only $\hat{\sigma}$, the sample variance, and a bound on the distance between $\bar{x}$ and $\mu$. My intuition was normally in this situation we need to use the t distribution since $\hat{\sigma}$ is an unbiased estimate for $\sigma$ (we did all the proofs in class).The problem is the t distribution changes depending on $n$, the sample size, so which distribution (how many degrees of freedom) should I consult when looking up the t-values containing 95% of the probability mass? I tried it for different values of $n$, and then squared the values to compare them to the d.f. of the t distribution - the closest I could get was 0.6 off. (I took the t-value at $\alpha = 0.025$ (right-tail) for 5 d.f., implying $n$ is 6, and squaring the t-value gave me 6.61, which is a discrepancy of 0.61 (isn't this large?). The reason I squared the t-values becomes apparent if you ""normalize"" the bound on the means into a t-statistic. Am I going about this correctly? This doesn't seem right...","Here is my question (homework obviously): A sample from a normal population produced variance 4.0. Find the size of the sample if the sample mean deviates from the population mean by no more than 2.0 with a probability of at least 0.95. So I'm trying to find $n$, the sample size, having only $\hat{\sigma}$, the sample variance, and a bound on the distance between $\bar{x}$ and $\mu$. My intuition was normally in this situation we need to use the t distribution since $\hat{\sigma}$ is an unbiased estimate for $\sigma$ (we did all the proofs in class).The problem is the t distribution changes depending on $n$, the sample size, so which distribution (how many degrees of freedom) should I consult when looking up the t-values containing 95% of the probability mass? I tried it for different values of $n$, and then squared the values to compare them to the d.f. of the t distribution - the closest I could get was 0.6 off. (I took the t-value at $\alpha = 0.025$ (right-tail) for 5 d.f., implying $n$ is 6, and squaring the t-value gave me 6.61, which is a discrepancy of 0.61 (isn't this large?). The reason I squared the t-values becomes apparent if you ""normalize"" the bound on the means into a t-statistic. Am I going about this correctly? This doesn't seem right...",,['statistics']
47,Stein's Paradox and James Stein Estimator,Stein's Paradox and James Stein Estimator,,I am trying to figure the intermediate steps in the proof of the Stein's paradox.  How does it go from the left to the right? $$\frac{\partial}{\partial Y_i} \left\{\frac{Y_i }{\sum_j Y_j^2}\right\} = \frac{\sum_j Y_j^2 - 2Y_i^2 }{(\sum_j Y_j^2)^2}$$ The part I have trouble with is: How would you differentiate this: $$ \frac{\partial}{\partial Y_i} \left\{ \frac{1}{\sum_j Y_j^2}\right\}$$,I am trying to figure the intermediate steps in the proof of the Stein's paradox.  How does it go from the left to the right? $$\frac{\partial}{\partial Y_i} \left\{\frac{Y_i }{\sum_j Y_j^2}\right\} = \frac{\sum_j Y_j^2 - 2Y_i^2 }{(\sum_j Y_j^2)^2}$$ The part I have trouble with is: How would you differentiate this: $$ \frac{\partial}{\partial Y_i} \left\{ \frac{1}{\sum_j Y_j^2}\right\}$$,,"['calculus', 'probability', 'statistics', 'multivariable-calculus', 'derivatives']"
48,"Optimization, Gradients, and Multivariate Data","Optimization, Gradients, and Multivariate Data",,"I would like to learn gradient based optimization for multivariate data. For example, assume the data I have is $X = (x_0, ..., x_n)$ where $x_i$ are some random variables and $f$ a function measuring (Pearson, if you like) correlation. Then, I would like to minimize the value of $f(X)$ i.e. make the variables $x_0, ..., x_n$ uncorrelated. How could this be achieved using gradient based methods? After I have learnt this, the next thing is that I would like to implement the procedure in MATLAB. If you have any tips for that, I would like to hear those as well.","I would like to learn gradient based optimization for multivariate data. For example, assume the data I have is $X = (x_0, ..., x_n)$ where $x_i$ are some random variables and $f$ a function measuring (Pearson, if you like) correlation. Then, I would like to minimize the value of $f(X)$ i.e. make the variables $x_0, ..., x_n$ uncorrelated. How could this be achieved using gradient based methods? After I have learnt this, the next thing is that I would like to implement the procedure in MATLAB. If you have any tips for that, I would like to hear those as well.",,"['statistics', 'multivariable-calculus', 'optimization', 'numerical-methods']"
49,Determining If 2D Collection of Points Are Random Or Not,Determining If 2D Collection of Points Are Random Or Not,,"Please forgive the lack of depth in respect of my mathematical vocabulary.  I have searched quite a bit in order to determine the answers for myself, however, if you don't know what mathematical/statistical terms to search for, it can be frustrating! In essence, I have a set of points in the plane and I would like to determine some ""properties"" of this set.  The best way for me to explain is to cite an example.  Say a sheet of paper has a target temporarily stuck on it somewhere and darts are thrown with the intent of hitting this target but most miss (dependent of the skill of the thrower, I suppose!).  The target is then removed and the sheet of paper with the points (dart holes in this instance) is analysed. How would you determine the likely position at which the target was located and how could you put a figure on the ""tightness"" of the grouping - in the example of this dart thrower, quantifying their accuracy? I'm not expecting perfect answers, but any indication of what terms to research would be most welcome. I can work out the ""centre of mass"" of the grouping, but I'm not sure how to discount ""outliers"" (actually, I'm not sure how to identify them) and I'm not convinced that this would be the best indication of where the target was likely to be anyway - although I may be wrong in that assumption.","Please forgive the lack of depth in respect of my mathematical vocabulary.  I have searched quite a bit in order to determine the answers for myself, however, if you don't know what mathematical/statistical terms to search for, it can be frustrating! In essence, I have a set of points in the plane and I would like to determine some ""properties"" of this set.  The best way for me to explain is to cite an example.  Say a sheet of paper has a target temporarily stuck on it somewhere and darts are thrown with the intent of hitting this target but most miss (dependent of the skill of the thrower, I suppose!).  The target is then removed and the sheet of paper with the points (dart holes in this instance) is analysed. How would you determine the likely position at which the target was located and how could you put a figure on the ""tightness"" of the grouping - in the example of this dart thrower, quantifying their accuracy? I'm not expecting perfect answers, but any indication of what terms to research would be most welcome. I can work out the ""centre of mass"" of the grouping, but I'm not sure how to discount ""outliers"" (actually, I'm not sure how to identify them) and I'm not convinced that this would be the best indication of where the target was likely to be anyway - although I may be wrong in that assumption.",,['statistics']
50,Why does this statistic have an approximate $\chi_{\nu}^{2}$ Distribution?,Why does this statistic have an approximate  Distribution?,\chi_{\nu}^{2},"I am currently studying goodness of fit tests and the $\chi^{2}$ distribution. To calculate the goodness of fit of a theoretical distribution, we compute the quantity $$X^{2}=\sum_{i=1}^{n}\frac{(O_{i}-E_{i})^{2}}{E_{i}}$$ Where $n$ is the number of outcomes, $O_{i}$ is the number of times the $i^{th}$ outcome is observed, and $E_{i}$ is the number of times we expect it to occur (given our distribution). My book then states $X^{2} \sim \chi^{2}_{\nu}$ approximately, where $\nu$ is an appropriate number of degrees of freedom for the data. It then defines the $\chi^{2}$ distribution as follows: If $Z_{i} \sim N(0,1)$ and $X=\sum_{i=1}^{\nu}Z_{i}^2$, then $X\sim \chi^{2}_{\nu}$. It also briefly proves that $X^{2}$ has an approximately $\chi^{2}$ distribution when $\nu=1$. My question is this: How do we prove that, for any $\nu$, $X^{2}$ has an approximately $\chi^{2}$ distribution? Intuitively I feel like this is the start of an induction proof, but I don't know how to show the inductive step. If this is the case, please show me how. NOTE: I have just noticed that there is an added condition: Each of the $E_{i}$ are greater than or equal to 5.","I am currently studying goodness of fit tests and the $\chi^{2}$ distribution. To calculate the goodness of fit of a theoretical distribution, we compute the quantity $$X^{2}=\sum_{i=1}^{n}\frac{(O_{i}-E_{i})^{2}}{E_{i}}$$ Where $n$ is the number of outcomes, $O_{i}$ is the number of times the $i^{th}$ outcome is observed, and $E_{i}$ is the number of times we expect it to occur (given our distribution). My book then states $X^{2} \sim \chi^{2}_{\nu}$ approximately, where $\nu$ is an appropriate number of degrees of freedom for the data. It then defines the $\chi^{2}$ distribution as follows: If $Z_{i} \sim N(0,1)$ and $X=\sum_{i=1}^{\nu}Z_{i}^2$, then $X\sim \chi^{2}_{\nu}$. It also briefly proves that $X^{2}$ has an approximately $\chi^{2}$ distribution when $\nu=1$. My question is this: How do we prove that, for any $\nu$, $X^{2}$ has an approximately $\chi^{2}$ distribution? Intuitively I feel like this is the start of an induction proof, but I don't know how to show the inductive step. If this is the case, please show me how. NOTE: I have just noticed that there is an added condition: Each of the $E_{i}$ are greater than or equal to 5.",,"['probability', 'statistics', 'probability-distributions']"
51,Expected value of a max,Expected value of a max,,"We have a roulette with the circumference $a$. We spin the roulette 10 times and we measure 10 distances, $x_1,\ldots,x_{10}$, from a predefined zero-point. We can assume that those distances are $U(0,a)$ distributed. An estimation of the circumference $a$ is given: $$a^* = \max(x_1,\ldots,x_{10})$$ To check whether it's biased or not I need to calculate: $$E(a^*) = E(\max(x_1,\ldots,x_{10}))$$ How do I proceed? I don't know any rules for calculating the estimate of a $\max$.","We have a roulette with the circumference $a$. We spin the roulette 10 times and we measure 10 distances, $x_1,\ldots,x_{10}$, from a predefined zero-point. We can assume that those distances are $U(0,a)$ distributed. An estimation of the circumference $a$ is given: $$a^* = \max(x_1,\ldots,x_{10})$$ To check whether it's biased or not I need to calculate: $$E(a^*) = E(\max(x_1,\ldots,x_{10}))$$ How do I proceed? I don't know any rules for calculating the estimate of a $\max$.",,"['statistics', 'parameter-estimation']"
52,Finding the mean of a uniform distribution?,Finding the mean of a uniform distribution?,,"I have a random set $\{a,b,c\}$ and a second set $\{e,d\}$ I draw one number first number and one from the second Letting $X_1$ denote the first number and $X_2$ the second number find, $E(X_1)$ and $E(X_2)$ and $E(X_1+X_2)$ Please please help, it might be obvious but I just can't work it out!","I have a random set $\{a,b,c\}$ and a second set $\{e,d\}$ I draw one number first number and one from the second Letting $X_1$ denote the first number and $X_2$ the second number find, $E(X_1)$ and $E(X_2)$ and $E(X_1+X_2)$ Please please help, it might be obvious but I just can't work it out!",,"['probability', 'statistics', 'probability-theory', 'uniform-distribution']"
53,What is a good distance-metric to compare 2 histograms of similarity?,What is a good distance-metric to compare 2 histograms of similarity?,,"in a project I want to compare 2 Histograms of similarity. My problem is to choose the right metric/ distance function. Can someone tell me, what's the difference between the Bhattacharyya distance and the Chi-square distance? My dataset in the best case is near of the normal distribution. In my opinion I saying that  Bhattacharyya distance is the right one for me (especialy the distance metric is bounded in [0,1}). But i can't find more arguments. Greetings","in a project I want to compare 2 Histograms of similarity. My problem is to choose the right metric/ distance function. Can someone tell me, what's the difference between the Bhattacharyya distance and the Chi-square distance? My dataset in the best case is near of the normal distribution. In my opinion I saying that  Bhattacharyya distance is the right one for me (especialy the distance metric is bounded in [0,1}). But i can't find more arguments. Greetings",,"['statistics', 'metric-spaces']"
54,How to find $E(X|S)$ where $S=X+Y$ and $X$ and $Y$ are independent random variables with some density function $f$,How to find  where  and  and  are independent random variables with some density function,E(X|S) S=X+Y X Y f,"So far I have this, but I am not sure if you are allowed to do this or if it is correct: I said since $X$ and $Y$ are independent and follow the same distribution, they must have the same expectation so: $$E(X|S=s)=E(X|X+Y=x+y)=E(Y|X+Y=x+y)=E(Y|S=s)$$ since $X$ and $Y$ are i.i.d. Then, can I say? $$E(X|S=s)+E(Y|S=s)=E(X+Y|X+Y=x+y)$$ and since $E(X|S=s)=E(Y|S=s)$;  $$E(X|S=s)+E(Y|S=s)=2E(X|S=s)=x+y,$$ then $E(X|S)=\frac{x+y}{2}$. This is my steps and logic, but I am not sure if this is correct, so any feedback on my work would be greatly appreciated. Thanks!","So far I have this, but I am not sure if you are allowed to do this or if it is correct: I said since $X$ and $Y$ are independent and follow the same distribution, they must have the same expectation so: $$E(X|S=s)=E(X|X+Y=x+y)=E(Y|X+Y=x+y)=E(Y|S=s)$$ since $X$ and $Y$ are i.i.d. Then, can I say? $$E(X|S=s)+E(Y|S=s)=E(X+Y|X+Y=x+y)$$ and since $E(X|S=s)=E(Y|S=s)$;  $$E(X|S=s)+E(Y|S=s)=2E(X|S=s)=x+y,$$ then $E(X|S)=\frac{x+y}{2}$. This is my steps and logic, but I am not sure if this is correct, so any feedback on my work would be greatly appreciated. Thanks!",,"['probability', 'statistics', 'probability-theory', 'probability-distributions']"
55,Sample variance of rolling a die 100 times,Sample variance of rolling a die 100 times,,"The textbook gives an example of testing a null hypothesis that rolling a die 100 times will give you a value of $6$, $\frac{1}{6}$ times. In the experiment, a die was rolled 100 times and 30 of them were $6$'s. The book obtains a $z$ score for this with the formula  $$\frac{\bar{x}- \mu}{ \sqrt{ \frac{p(1-p)}{100}}} = \frac{.30- .167}{ \sqrt{ \frac{.167(1-.167)}{100}}} $$. I understand that $\sqrt{ \frac{0.167(1-0.167)}{100}}$ must be the standard deviation of the sample mean (tell me if I'm wrong), but how did they get $0.167(1-0.167)$ as the variance? Where did that formula come from?","The textbook gives an example of testing a null hypothesis that rolling a die 100 times will give you a value of $6$, $\frac{1}{6}$ times. In the experiment, a die was rolled 100 times and 30 of them were $6$'s. The book obtains a $z$ score for this with the formula  $$\frac{\bar{x}- \mu}{ \sqrt{ \frac{p(1-p)}{100}}} = \frac{.30- .167}{ \sqrt{ \frac{.167(1-.167)}{100}}} $$. I understand that $\sqrt{ \frac{0.167(1-0.167)}{100}}$ must be the standard deviation of the sample mean (tell me if I'm wrong), but how did they get $0.167(1-0.167)$ as the variance? Where did that formula come from?",,['statistics']
56,Readings necessary to understand Ito Integrals?,Readings necessary to understand Ito Integrals?,,I searched for this question but couldn't find a direct answer. Basically I want to understand (and possibly compute some simple instances of) the Ito integral. I am coming from a physics background and have little experience with statistics. I encountered the idea in the topic of Quantum Path Integrals and I am now very curious. So basically I am looking for a list of somewhat informal resources that would build me to the point of understanding and being able to numerically/analytically compute some simple Ito Integrals and understand what is going on.,I searched for this question but couldn't find a direct answer. Basically I want to understand (and possibly compute some simple instances of) the Ito integral. I am coming from a physics background and have little experience with statistics. I encountered the idea in the topic of Quantum Path Integrals and I am now very curious. So basically I am looking for a list of somewhat informal resources that would build me to the point of understanding and being able to numerically/analytically compute some simple Ito Integrals and understand what is going on.,,"['statistics', 'stochastic-integrals', 'stochastic-calculus', 'stochastic-analysis']"
57,what the likely size of the chance error if a simple random sample?,what the likely size of the chance error if a simple random sample?,,"The National Assessment of Educational Progress periodically administers tests on different subjects to high school students. In 2000, the grade 12 students in the sample averaged 301 on the mathematics test; the SD was 30. Can you say what the likely size of the chance error in the 301 is if a simple random sample of 1000 students was tested?","The National Assessment of Educational Progress periodically administers tests on different subjects to high school students. In 2000, the grade 12 students in the sample averaged 301 on the mathematics test; the SD was 30. Can you say what the likely size of the chance error in the 301 is if a simple random sample of 1000 students was tested?",,"['probability', 'statistics']"
58,Choice of Principal Components in PCA,Choice of Principal Components in PCA,,"For computing PCA of $X$, do we use the eigenvectors of covariance matrix $X^TX$, or the eigenvectors of kernel matrix $XX^T$ as the principal components? I am really confused, because seen both used.","For computing PCA of $X$, do we use the eigenvectors of covariance matrix $X^TX$, or the eigenvectors of kernel matrix $XX^T$ as the principal components? I am really confused, because seen both used.",,"['linear-algebra', 'statistics']"
59,How to calculate percentile rank if all values are equal to zero,How to calculate percentile rank if all values are equal to zero,,"The formula for finding Percentile rank as per wikipedia is $$\frac{c_{\ell}+0.5 f_{i}}{N} \times 100 \%$$ One of the edge cases in my application is when all the values are zero. In this case the number of scores less than the score of interest is zero, the frequency is equal to the number of total values which is $N$ . This formula gives me a rank percentile of $50\%$ when all the scores are equal and zero. It seems counter intuitive to suggest that the rank percentile is $50\%$ even though there is not a single score less or more than the score of interest. Is this the correct value?","The formula for finding Percentile rank as per wikipedia is One of the edge cases in my application is when all the values are zero. In this case the number of scores less than the score of interest is zero, the frequency is equal to the number of total values which is . This formula gives me a rank percentile of when all the scores are equal and zero. It seems counter intuitive to suggest that the rank percentile is even though there is not a single score less or more than the score of interest. Is this the correct value?",\frac{c_{\ell}+0.5 f_{i}}{N} \times 100 \% N 50\% 50\%,['statistics']
60,absolute value of state space necessarily a markov chain?,absolute value of state space necessarily a markov chain?,,"Suppose $X_t$ is a first-order Markov Chain with state space $\{-1, 0, 1\}$, and transition matrix $P$, Is $|X_t|$ (absolute value) necessarily a Markov chain? Thanks!","Suppose $X_t$ is a first-order Markov Chain with state space $\{-1, 0, 1\}$, and transition matrix $P$, Is $|X_t|$ (absolute value) necessarily a Markov chain? Thanks!",,"['statistics', 'stochastic-processes', 'markov-chains']"
61,Help with probabilities on a game I am making,Help with probabilities on a game I am making,,"I asked over at the RPG stack exchange and they sent me here. I am working on making a RPG and am trying to understand the statistics of the core mechanic so I can determine how effective leveling and bonuses will be. Right now the core mechanic is a skill die based mechanic where you roll a d4, d6, d8 or d10 based on your skill opposed by either a DC set by the story teller (1-10) or the result of another die via an opposed roll. I am experimenting with rolling two dice and picking the higher of the two, however I don't know how to make a statistical model for this. So if I rolled a 5 and a 4 on two six sided dice I would pick the 5 and ignore the 4. I don't know how this would change the probabilities. I am finding this especially difficult due to the fact that I am planning on using exploding dice. (Exploding dice is a term for when you roll the highest number on a die and roll the die again, adding the number you previously rolled to the die. This can continue indefinitely.) I would very much appreciate some help on coming up with a formula for this. I want to set up an excel sheet to compare different bonus methods so I would really like something where I can put the formula in and tweak it instead of just probabilities listed.","I asked over at the RPG stack exchange and they sent me here. I am working on making a RPG and am trying to understand the statistics of the core mechanic so I can determine how effective leveling and bonuses will be. Right now the core mechanic is a skill die based mechanic where you roll a d4, d6, d8 or d10 based on your skill opposed by either a DC set by the story teller (1-10) or the result of another die via an opposed roll. I am experimenting with rolling two dice and picking the higher of the two, however I don't know how to make a statistical model for this. So if I rolled a 5 and a 4 on two six sided dice I would pick the 5 and ignore the 4. I don't know how this would change the probabilities. I am finding this especially difficult due to the fact that I am planning on using exploding dice. (Exploding dice is a term for when you roll the highest number on a die and roll the die again, adding the number you previously rolled to the die. This can continue indefinitely.) I would very much appreciate some help on coming up with a formula for this. I want to set up an excel sheet to compare different bonus methods so I would really like something where I can put the formula in and tweak it instead of just probabilities listed.",,"['probability', 'statistics', 'dice']"
62,Calculating the density of a joint distribution,Calculating the density of a joint distribution,,"For the joint density function $$P\big((X,Y) \in A\big) = \int_A f_{(X,Y)} (x,y) \, dx\,dy$$ how would you show that if $(X,Y)$ is a random vector in $\mathbb{R}^2$ with density $f_{(X,Y)}$ and $f_{(X,Y)}(x,y) = f(x)g(y)$ for a pair of non-negative functions $f$ and $g$ then $X$ has density $$\frac{f}{\int_\mathbb{R}f(t)\,dt}$$ and $Y$ has density $$\frac{f}{\int_\mathbb{R}g(t)\,dt}\, \, ?$$","For the joint density function $$P\big((X,Y) \in A\big) = \int_A f_{(X,Y)} (x,y) \, dx\,dy$$ how would you show that if $(X,Y)$ is a random vector in $\mathbb{R}^2$ with density $f_{(X,Y)}$ and $f_{(X,Y)}(x,y) = f(x)g(y)$ for a pair of non-negative functions $f$ and $g$ then $X$ has density $$\frac{f}{\int_\mathbb{R}f(t)\,dt}$$ and $Y$ has density $$\frac{f}{\int_\mathbb{R}g(t)\,dt}\, \, ?$$",,['probability']
63,Expectation of a multivariate Gaussian over a plane,Expectation of a multivariate Gaussian over a plane,,"For a vector $X$ which follows a multinomial Gaussian distribution $N(\vec{0},\Sigma)$, a given vector $b$, and a known scalar value $c$, I would like to calculate the expectation : $E[X|X^Tb = c]$ That is the expected value of the multivariate variable $X$ given that it will lie on the plane $ X^Tb = c$. I have tried by parametrizing $X$ as $X = \vec{a_0} + t_1 \vec{a_1} ... t_{n-1} \vec{a_{n-1}}$ and calculating the integral $\int_{-\infty}^{\infty} ... \int_{-\infty}^{\infty} xf(x) dt_1 ... dt_{n-1}$, where $f(x)$ is the pdf of the Gaussian, but I end up with an extremely messy formula even when trying to solve in the simple three-dimensional case. My question is whether there is a known closed form solution for the above expectation and/or if there is a specific parametrization I could use to simplify the solution.","For a vector $X$ which follows a multinomial Gaussian distribution $N(\vec{0},\Sigma)$, a given vector $b$, and a known scalar value $c$, I would like to calculate the expectation : $E[X|X^Tb = c]$ That is the expected value of the multivariate variable $X$ given that it will lie on the plane $ X^Tb = c$. I have tried by parametrizing $X$ as $X = \vec{a_0} + t_1 \vec{a_1} ... t_{n-1} \vec{a_{n-1}}$ and calculating the integral $\int_{-\infty}^{\infty} ... \int_{-\infty}^{\infty} xf(x) dt_1 ... dt_{n-1}$, where $f(x)$ is the pdf of the Gaussian, but I end up with an extremely messy formula even when trying to solve in the simple three-dimensional case. My question is whether there is a known closed form solution for the above expectation and/or if there is a specific parametrization I could use to simplify the solution.",,"['linear-algebra', 'probability', 'algebraic-geometry', 'statistics', 'integration']"
64,Sum of variances of experiments does not equal variance of joint experiment,Sum of variances of experiments does not equal variance of joint experiment,,"Say I make 3 independent experiments and these are the outputs: O/P of $1$st exp : $1,2,3$ O/P of $2$nd exp : $4,5,6$ O/P of $3$rd exp : $7,8,9$ In general ${\rm Var}(A+B+C) = {\rm Var}( A ) + {\rm Var}( B ) + {\rm Var}( C )$ In this case ${\rm Var}(\text{1st}) + {\rm Var}(\text{2nd}) + {\rm Var}(\text{3rd}) \ne {\rm Var}(1,2,3,4,5,6,7,8,9)$ Why ?","Say I make 3 independent experiments and these are the outputs: O/P of $1$st exp : $1,2,3$ O/P of $2$nd exp : $4,5,6$ O/P of $3$rd exp : $7,8,9$ In general ${\rm Var}(A+B+C) = {\rm Var}( A ) + {\rm Var}( B ) + {\rm Var}( C )$ In this case ${\rm Var}(\text{1st}) + {\rm Var}(\text{2nd}) + {\rm Var}(\text{3rd}) \ne {\rm Var}(1,2,3,4,5,6,7,8,9)$ Why ?",,"['probability', 'statistics']"
65,flares probability,flares probability,,There are 33 no of flares distributed in 2 hemisphere north and south. the no of flares in north hemisphere is 14 and no of flares in southern hemisphere is 19. then how to find the probability of distribution of dominant hemisphere (in this case southern) =0.189 for more detail please see the attachment how the probability is counted in this attachment?,There are 33 no of flares distributed in 2 hemisphere north and south. the no of flares in north hemisphere is 14 and no of flares in southern hemisphere is 19. then how to find the probability of distribution of dominant hemisphere (in this case southern) =0.189 for more detail please see the attachment how the probability is counted in this attachment?,,"['probability', 'combinatorics', 'statistics', 'standard-deviation']"
66,Find factor of sample elements given the median,Find factor of sample elements given the median,,"I have a sample $S(x)$ containing $n$ elements: $$S(x)=\{ s_1 x, s_2 x, \ldots, s_n x  \},\qquad s_i \in \mathbb{R}, x\in \mathbb{R^{+}}$$ Every element in the sample is multiplied by $x$. Now median of this sample is $$\tilde{S}(x)=y,\qquad y\in \mathbb{R^{+}}$$ When $y$ is given, how to find $x$? In other words: If I know median of a sample whose every element is multiplied by a certain factor, how to find this factor? The original sample elements $s_{i}$ are also known. I think it would be possible to find the value with a search algorithm (to some degree of precision), but maybe there is a simple closed solution. Note that there may be more that one $x$ satisfying the above equation, since $s_i$ come from $\mathbb{N}$. The solution will more likely be an interval of values.","I have a sample $S(x)$ containing $n$ elements: $$S(x)=\{ s_1 x, s_2 x, \ldots, s_n x  \},\qquad s_i \in \mathbb{R}, x\in \mathbb{R^{+}}$$ Every element in the sample is multiplied by $x$. Now median of this sample is $$\tilde{S}(x)=y,\qquad y\in \mathbb{R^{+}}$$ When $y$ is given, how to find $x$? In other words: If I know median of a sample whose every element is multiplied by a certain factor, how to find this factor? The original sample elements $s_{i}$ are also known. I think it would be possible to find the value with a search algorithm (to some degree of precision), but maybe there is a simple closed solution. Note that there may be more that one $x$ satisfying the above equation, since $s_i$ come from $\mathbb{N}$. The solution will more likely be an interval of values.",,"['statistics', 'algorithms', 'median']"
67,Trying to understand LMS algorithm,Trying to understand LMS algorithm,,"This is what's written: So, $h(x)$ is basically a linear function to predict values of some training set. I understand everything that's happening up to the point where we take the partial derivative of $h(x)$ in summation form (the second equation above the black line), but how do we get the partial derivative of $h(x) - y$ to just $x_j$? Also, why does the update in the equation under the black line change what's calculated before: $(h(x) - y)x_j$ to $(y - h(x))x_j$? I'm sorry if I'm not being clear enough. Please let me know if that's the case. Thank you!","This is what's written: So, $h(x)$ is basically a linear function to predict values of some training set. I understand everything that's happening up to the point where we take the partial derivative of $h(x)$ in summation form (the second equation above the black line), but how do we get the partial derivative of $h(x) - y$ to just $x_j$? Also, why does the update in the equation under the black line change what's calculated before: $(h(x) - y)x_j$ to $(y - h(x))x_j$? I'm sorry if I'm not being clear enough. Please let me know if that's the case. Thank you!",,"['statistics', 'derivatives', 'machine-learning']"
68,Finding $E(\bar{X^2}|\bar{X})$ [duplicate],Finding  [duplicate],E(\bar{X^2}|\bar{X}),"This question already has an answer here : Closed 11 years ago . Possible Duplicate: Finding $E\Bigl(\overline{Y^2}\Bigm|\overline{Y\vphantom{Y^2}}\Bigr)$ by Basu's theorem? Suppose $X_1,\ldots,X_n$ are a random sample of $N(\theta,1)$. if $\bar{X^2}=\displaystyle\frac{1}{n}\sum_{i=1}^n X_i^2$, how can I find $E(\bar{X^2}|\bar{X})$?","This question already has an answer here : Closed 11 years ago . Possible Duplicate: Finding $E\Bigl(\overline{Y^2}\Bigm|\overline{Y\vphantom{Y^2}}\Bigr)$ by Basu's theorem? Suppose $X_1,\ldots,X_n$ are a random sample of $N(\theta,1)$. if $\bar{X^2}=\displaystyle\frac{1}{n}\sum_{i=1}^n X_i^2$, how can I find $E(\bar{X^2}|\bar{X})$?",,['statistics']
69,Proving that the sum of Good-Turing estimators is $1$,Proving that the sum of Good-Turing estimators is,1,"I want to know how to go about proving that the Good-Turing estimator has a total probability of $1$. I have seen this proof (page 2) but I found unclear the first step: $$\sum_j \theta[j] = \sum_r \theta[r]N_r = \frac{1}{N}\sum \left[(r+1) \frac{N_{r+1}}{N_r}\right]N_r$$ $\theta[j]$ is the probability of having an $n$-gram, $\theta[r]$ is the probability of a $n$-gram occurring $r$ times and $N_r$ is the number of $n$-grams that occur $r$ times. Since $\sum_r(r+1)N_{r+1}$ = $\sum_r rN_r$, it's more or less straightforward that it actually sums $1$. However, as I said, I don't understand the first part: $\sum_j \theta[j] = \sum_r \theta[r]N_r$. What is going on there? Thanks in advance.","I want to know how to go about proving that the Good-Turing estimator has a total probability of $1$. I have seen this proof (page 2) but I found unclear the first step: $$\sum_j \theta[j] = \sum_r \theta[r]N_r = \frac{1}{N}\sum \left[(r+1) \frac{N_{r+1}}{N_r}\right]N_r$$ $\theta[j]$ is the probability of having an $n$-gram, $\theta[r]$ is the probability of a $n$-gram occurring $r$ times and $N_r$ is the number of $n$-grams that occur $r$ times. Since $\sum_r(r+1)N_{r+1}$ = $\sum_r rN_r$, it's more or less straightforward that it actually sums $1$. However, as I said, I don't understand the first part: $\sum_j \theta[j] = \sum_r \theta[r]N_r$. What is going on there? Thanks in advance.",,"['statistics', 'probability-theory', 'parameter-estimation']"
70,Expected Value of function of two random variable,Expected Value of function of two random variable,,"Assume $X_1$ is an Exponential random variable with unit mean ( i.e. $f_{X_1}(x) = e^{-x}$ ) and $X_2$ is an Erlang distribution with shape $N$ and unit rate ( i.e. $f_{X_2}(x) = \frac{x^{N-1}e^{-x}}{(N-1)!}$ ). I need to compute the expected value of  $X=\frac{X_1+X_1X_2}{c+\alpha X_1+\beta X_1X_2}$ where $c$, $\alpha$ and $\beta$ are larger than zero. I tried to find the expectation by calculating $f_X(x)$ and then $\int_0^\infty xf_X(x)dx$, but I could not calculate integral. I also tried $E(X)=\int_0^\infty P[X>x] dx$, but again failed calculating $E(X)$. I am not even sure if $E(X)$ is available in closed form at all or not. I would appreciate if one could help me.","Assume $X_1$ is an Exponential random variable with unit mean ( i.e. $f_{X_1}(x) = e^{-x}$ ) and $X_2$ is an Erlang distribution with shape $N$ and unit rate ( i.e. $f_{X_2}(x) = \frac{x^{N-1}e^{-x}}{(N-1)!}$ ). I need to compute the expected value of  $X=\frac{X_1+X_1X_2}{c+\alpha X_1+\beta X_1X_2}$ where $c$, $\alpha$ and $\beta$ are larger than zero. I tried to find the expectation by calculating $f_X(x)$ and then $\int_0^\infty xf_X(x)dx$, but I could not calculate integral. I also tried $E(X)=\int_0^\infty P[X>x] dx$, but again failed calculating $E(X)$. I am not even sure if $E(X)$ is available in closed form at all or not. I would appreciate if one could help me.",,"['probability', 'statistics', 'probability-theory', 'probability-distributions']"
71,$\chi^2$ test and sampling variance,test and sampling variance,\chi^2,"Let $f(x)$ denote the pdf of a $\chi^2$-distribution with $n\in\mathbb{N}$ degrees of freedom given by   $$f(x) = \frac{2^{-n/2}}{\Gamma(n/2)}\cdot x^{n/2-1}\cdot\mathrm e^{-1/2x}\cdot\textbf{1}_{[0,\infty)}(x),$$   where $\textbf{1}_A(x)=\begin{cases}1,&x\in A,\\0,&\text{else.}\end{cases}$ Furthermore we define $\Gamma(1/2)=\sqrt{\pi},\;\Gamma(1)=1$ and $\Gamma(r+1)=r\cdot\Gamma(r)$. Assume we have two independant random variables $X_1,X_2\sim\mathcal{N}(\mu,\sigma^2)$ with unknown $\mu$ and unknown $\sigma$ and their sampling variance $S_X^2=\frac{1}{n-1}\sum\limits_{i=1}^n(X_i-\overline{X})^2$ with $\overline{X}=\frac{1}{n}\sum\limits_{i=1}^nX_i$. Show that   $$\frac{1}{\sigma^2}S_X^2=\frac{(X_1-\overline{X})^2+(X_2-\overline{X})^2}{\sigma^2}$$   is $\chi^2$ distributed with one degree of freedom. To be honest, i have no idea at all how to start because this huge amount of information intimidates me. Can anyone explain me an appropriate ansatz to proove this?","Let $f(x)$ denote the pdf of a $\chi^2$-distribution with $n\in\mathbb{N}$ degrees of freedom given by   $$f(x) = \frac{2^{-n/2}}{\Gamma(n/2)}\cdot x^{n/2-1}\cdot\mathrm e^{-1/2x}\cdot\textbf{1}_{[0,\infty)}(x),$$   where $\textbf{1}_A(x)=\begin{cases}1,&x\in A,\\0,&\text{else.}\end{cases}$ Furthermore we define $\Gamma(1/2)=\sqrt{\pi},\;\Gamma(1)=1$ and $\Gamma(r+1)=r\cdot\Gamma(r)$. Assume we have two independant random variables $X_1,X_2\sim\mathcal{N}(\mu,\sigma^2)$ with unknown $\mu$ and unknown $\sigma$ and their sampling variance $S_X^2=\frac{1}{n-1}\sum\limits_{i=1}^n(X_i-\overline{X})^2$ with $\overline{X}=\frac{1}{n}\sum\limits_{i=1}^nX_i$. Show that   $$\frac{1}{\sigma^2}S_X^2=\frac{(X_1-\overline{X})^2+(X_2-\overline{X})^2}{\sigma^2}$$   is $\chi^2$ distributed with one degree of freedom. To be honest, i have no idea at all how to start because this huge amount of information intimidates me. Can anyone explain me an appropriate ansatz to proove this?",,"['statistics', 'random', 'normal-distribution', 'sampling']"
72,The limit of hazard rate $h(x)=A/(1-B)$ as $x$ approaches $\pm \infty$,The limit of hazard rate  as  approaches,h(x)=A/(1-B) x \pm \infty,"Can we tell what happens to the limit as $x$ approaches $\pm \infty$ of a hazard rate $h(x)$ defined for unspecified or generalized density as: $$ h(x)=A/(1-B) $$ where $A=f(x)$ is the density function, and $B=F(x)$ is the CDF.","Can we tell what happens to the limit as $x$ approaches $\pm \infty$ of a hazard rate $h(x)$ defined for unspecified or generalized density as: $$ h(x)=A/(1-B) $$ where $A=f(x)$ is the density function, and $B=F(x)$ is the CDF.",,"['calculus', 'probability', 'statistics', 'limits']"
73,Ranking probability task,Ranking probability task,,"Local football association has $10$ teams. Team A has $40\%$ chance to score a win in the game against better ranking opponent, and $75\%$ against worse ranking opponent. If team A is currently $4$-th on the ranking list, define probability that A will win in the next game. So I have done the following here $0.4 \cdot 0.3$ ($3$ out of $10$ are better ranking opponents)+ $0.75 \cdot 0.6$ ($6$ are worse ranking opponents) = $\frac{1}{12} + \frac{3}{24} = \frac{5}{24}$. I am not sure whether or not this is correct, but I plead you to elaborate this claim of mine.","Local football association has $10$ teams. Team A has $40\%$ chance to score a win in the game against better ranking opponent, and $75\%$ against worse ranking opponent. If team A is currently $4$-th on the ranking list, define probability that A will win in the next game. So I have done the following here $0.4 \cdot 0.3$ ($3$ out of $10$ are better ranking opponents)+ $0.75 \cdot 0.6$ ($6$ are worse ranking opponents) = $\frac{1}{12} + \frac{3}{24} = \frac{5}{24}$. I am not sure whether or not this is correct, but I plead you to elaborate this claim of mine.",,"['probability', 'statistics']"
74,Probability distribution explanation,Probability distribution explanation,,"What exactly is a probability distribution, and what are the two requirements for a probability distribution? I am not sure what this means or how to apply it? Any examples that can be given would be great!","What exactly is a probability distribution, and what are the two requirements for a probability distribution? I am not sure what this means or how to apply it? Any examples that can be given would be great!",,"['statistics', 'probability-distributions', 'terminology']"
75,Is the expectation $E[\xi U'(\xi)]$ finite?,Is the expectation  finite?,E[\xi U'(\xi)],"I encounter the following problem today. It seems a simple question. Let $U$ be a real function from $R^+\rightarrow \bar{R}$ satisfying the following conditions: (1) $U$ is concave, continuous, and strictly increasing, (2) $\limsup_{x\rightarrow +\infty}\dfrac{xU'(x)}{U(x)} <1.$ (3) $ U'(0+) = +\infty, \mbox{ and }U'(+\infty) = 0.$ Is the following statement true? $\bf Claim:$ For any non-negative random variable $\xi,$ if  $E[U(\xi)] < +\infty,$ then we have  $$E[\xi U'(\xi)] < +\infty.$$ $\bf Remark:$ If $U(0) >-\infty,$ it is trivial if one notices that $$0\leq \xi U'(\xi) \leq U(\xi) - U(0).$$ But in general, the property $U(0) >-\infty$ does NOT hold. For example $\ln(x).$ Any comment and suggestion are welcome. Thanks.","I encounter the following problem today. It seems a simple question. Let $U$ be a real function from $R^+\rightarrow \bar{R}$ satisfying the following conditions: (1) $U$ is concave, continuous, and strictly increasing, (2) $\limsup_{x\rightarrow +\infty}\dfrac{xU'(x)}{U(x)} <1.$ (3) $ U'(0+) = +\infty, \mbox{ and }U'(+\infty) = 0.$ Is the following statement true? $\bf Claim:$ For any non-negative random variable $\xi,$ if  $E[U(\xi)] < +\infty,$ then we have  $$E[\xi U'(\xi)] < +\infty.$$ $\bf Remark:$ If $U(0) >-\infty,$ it is trivial if one notices that $$0\leq \xi U'(\xi) \leq U(\xi) - U(0).$$ But in general, the property $U(0) >-\infty$ does NOT hold. For example $\ln(x).$ Any comment and suggestion are welcome. Thanks.",,"['probability', 'statistics', 'probability-theory', 'random', 'stochastic-analysis']"
76,local variance of Markov decision processes,local variance of Markov decision processes,,"Does anybody know the notion of ""local variance"" of Markov decision processes? Any reference would be appreciated. Thanks.","Does anybody know the notion of ""local variance"" of Markov decision processes? Any reference would be appreciated. Thanks.",,"['probability', 'statistics', 'markov-process']"
77,Normal distributions obey central limit theorem,Normal distributions obey central limit theorem,,"Let $X_1,\dots,X_n$ be independent random variables, each normally distributed as $X_k\sim N(m_k;\sigma^2_k)$. Let $S_n = \sum_{k=1}^n X_k - m_k$ and $T_n = \frac{S_n}{\sqrt{\operatorname{Var}(S_n)}}$. We wish to show that $\lim_{n\to\infty} T_n \sim N(0;1)$. I've laboriously proven that $T_n \sim N(m; \sigma^2)$ where $m=\sum_k m_k$ and $\sigma^2=\sum_k \sigma^2_k$. So now I'm wondering if I'm supposed to assume that the $\lim_{n\to\infty}m=0$ and similarly $\sigma^2\to 1$, which I don't really see a strong justification for. Is this what I'm supposed to do? What's the justification for it? (Note this is exercise 14.31.7 in Apostol's Calculus II. Looking online it seems like some other places don't define CLT as it approaches standard normal, just normal distribution of some mean and variance. So this could be an unusual definition he's using.)","Let $X_1,\dots,X_n$ be independent random variables, each normally distributed as $X_k\sim N(m_k;\sigma^2_k)$. Let $S_n = \sum_{k=1}^n X_k - m_k$ and $T_n = \frac{S_n}{\sqrt{\operatorname{Var}(S_n)}}$. We wish to show that $\lim_{n\to\infty} T_n \sim N(0;1)$. I've laboriously proven that $T_n \sim N(m; \sigma^2)$ where $m=\sum_k m_k$ and $\sigma^2=\sum_k \sigma^2_k$. So now I'm wondering if I'm supposed to assume that the $\lim_{n\to\infty}m=0$ and similarly $\sigma^2\to 1$, which I don't really see a strong justification for. Is this what I'm supposed to do? What's the justification for it? (Note this is exercise 14.31.7 in Apostol's Calculus II. Looking online it seems like some other places don't define CLT as it approaches standard normal, just normal distribution of some mean and variance. So this could be an unusual definition he's using.)",,"['calculus', 'statistics', 'probability-theory']"
78,Remove statistical outliers,Remove statistical outliers,,"I've analysed newspapers by counting the language distributions of the articles. The results look like that: Day 1               Day 2              Day 3  Economy             Economy            Economy language 1: 0,35    language 1: 0,30   language 1: 0,90 language 2: 0,11    language 2: 0,10   language 2: 0,00 language 3: 0,54    language 3: 0,60   language 3: 0,10  Sports              Sports             Sports language 1: 0,40    language 1: 0,30   language 1: 1.00 language 2: 0,20    language 2: 0,20   language 2: 0,00 language 3: 0,40    language 3: 0,50   language 3: 0,00 So for instance on day 1, 35 % of the Economy-articles are written in language 1, 11 % in language 2 and so on. Now I want to remove the outliers (like e.g. day 3) from my data. I was thinking about calculating the double standard deviation and remove all the values that are outside of it. Does that make sense? Is there a problem if my values don't have a normal distribution? Or is there another way to get rid off the outliers? In the end I want to calculate the average of all language 1, language 2 etc. values of each category over time and see how the values change. Any technique how to do that? Thanks in advance.","I've analysed newspapers by counting the language distributions of the articles. The results look like that: Day 1               Day 2              Day 3  Economy             Economy            Economy language 1: 0,35    language 1: 0,30   language 1: 0,90 language 2: 0,11    language 2: 0,10   language 2: 0,00 language 3: 0,54    language 3: 0,60   language 3: 0,10  Sports              Sports             Sports language 1: 0,40    language 1: 0,30   language 1: 1.00 language 2: 0,20    language 2: 0,20   language 2: 0,00 language 3: 0,40    language 3: 0,50   language 3: 0,00 So for instance on day 1, 35 % of the Economy-articles are written in language 1, 11 % in language 2 and so on. Now I want to remove the outliers (like e.g. day 3) from my data. I was thinking about calculating the double standard deviation and remove all the values that are outside of it. Does that make sense? Is there a problem if my values don't have a normal distribution? Or is there another way to get rid off the outliers? In the end I want to calculate the average of all language 1, language 2 etc. values of each category over time and see how the values change. Any technique how to do that? Thanks in advance.",,"['statistics', 'average', 'standard-deviation']"
79,"naive bayes, understanding the correctness of a model and computation","naive bayes, understanding the correctness of a model and computation",,"I implemented naive bayes algorithm to predict an emotion ( happy , sad ) for blogs using the formula provided by Manning's Information Retrieval book http://nlp.stanford.edu/IR-book/pdf/13bayes.pdf essentially for a given document, it comes down to comparing p(word = a,b,c | label = bad) p(label = bad) vs. p(word = a,b,c | label = good) p (label = good) now I wonder since we have all the counts of the words for each emotion, Can we reframe the problem such as this: p(label = bad | word = a) * p(label = bad | word = b) * p(label = bad | word = c) vs. p(label = good | word = a) * p(label = good | word = b) * p(label = good | word = c) Does this computation make sense? This is a more general question... how do you evaluate the correctness of this computation? If so, what is the implication of this model vs the formula found in the textbook? for example, are there different claims about independence? With naive bayes, the assumption is that the words are ""conditionally"" independent. It seems to me that the textbook model is where you compare 2 factories (happy, sad) and you compare the likelihood of making that string of words from the 2 factories as opposed to say rolling several dies and using each die's signal of being good or bad fyi I took one basic probability course","I implemented naive bayes algorithm to predict an emotion ( happy , sad ) for blogs using the formula provided by Manning's Information Retrieval book http://nlp.stanford.edu/IR-book/pdf/13bayes.pdf essentially for a given document, it comes down to comparing p(word = a,b,c | label = bad) p(label = bad) vs. p(word = a,b,c | label = good) p (label = good) now I wonder since we have all the counts of the words for each emotion, Can we reframe the problem such as this: p(label = bad | word = a) * p(label = bad | word = b) * p(label = bad | word = c) vs. p(label = good | word = a) * p(label = good | word = b) * p(label = good | word = c) Does this computation make sense? This is a more general question... how do you evaluate the correctness of this computation? If so, what is the implication of this model vs the formula found in the textbook? for example, are there different claims about independence? With naive bayes, the assumption is that the words are ""conditionally"" independent. It seems to me that the textbook model is where you compare 2 factories (happy, sad) and you compare the likelihood of making that string of words from the 2 factories as opposed to say rolling several dies and using each die's signal of being good or bad fyi I took one basic probability course",,"['probability', 'statistics']"
80,"Find all possible numbers with specific mean, median and range","Find all possible numbers with specific mean, median and range",,"Given that there are $20$ positive numbers and that the median is $42$, mean is $46$ and the range is $35$. An important condition, I missed: A number can appear at the most two times. This was indeed question asked in my sister's class (6th grade). She solved it by setting $$x_1, x_2, x_9 \dots 42, 42, y_1, y_2 \dots y_9$$ and found three solutions and got her assignment correct. The values she got was $$30, 30, 31, 31, 32, 32, 33, 33, 34, 42, 42, 43, 61, 62, 63, 63, 64, 64, 65, 65$$ $$30, 30, 31, 31, 32, 32, 33, 33, 34, 42, 42, 55, 55, 56, 63, 63, 64, 64, 65, 65$$ $$30, 30, 31, 31, 32, 32, 33, 33, 34, 42, 42, 51, 54, 62, 63, 63, 64, 64, 65, 65$$ I looked at this question and seemed interesting to ask what are all possible solutions? I am therefore seeking an approach to find all possible solutions. I attempted by listing a possibility (there could be more) $$x, x, x_1, x_2, x_3, x_4, x_5, x_6, x_7, 42, 42, y_7, y_6, y_5, y_4, y_3, y_2, x_1, x+3, x+35$$ where $x_i < 42, $ and $y_i > 42$ and $7 < x < 42$ By doing a little more work, I could come with the smallest value of $x=29$. Is that correct?","Given that there are $20$ positive numbers and that the median is $42$, mean is $46$ and the range is $35$. An important condition, I missed: A number can appear at the most two times. This was indeed question asked in my sister's class (6th grade). She solved it by setting $$x_1, x_2, x_9 \dots 42, 42, y_1, y_2 \dots y_9$$ and found three solutions and got her assignment correct. The values she got was $$30, 30, 31, 31, 32, 32, 33, 33, 34, 42, 42, 43, 61, 62, 63, 63, 64, 64, 65, 65$$ $$30, 30, 31, 31, 32, 32, 33, 33, 34, 42, 42, 55, 55, 56, 63, 63, 64, 64, 65, 65$$ $$30, 30, 31, 31, 32, 32, 33, 33, 34, 42, 42, 51, 54, 62, 63, 63, 64, 64, 65, 65$$ I looked at this question and seemed interesting to ask what are all possible solutions? I am therefore seeking an approach to find all possible solutions. I attempted by listing a possibility (there could be more) $$x, x, x_1, x_2, x_3, x_4, x_5, x_6, x_7, 42, 42, y_7, y_6, y_5, y_4, y_3, y_2, x_1, x+3, x+35$$ where $x_i < 42, $ and $y_i > 42$ and $7 < x < 42$ By doing a little more work, I could come with the smallest value of $x=29$. Is that correct?",,['statistics']
81,Find probabilities of winning in a modified version of Monty Hall Problem,Find probabilities of winning in a modified version of Monty Hall Problem,,"So I modified the original version of the Monty Hall problem and allow there to have 4 doors; 1 car and 3 goats behind the doors. I will choose one door, Monty, who knows where the car is, randomly reveals one of the other 3 doors to show a goat. In a case which I didn't switch doors, my chance of winning is clearly $\frac{1}{4}$. Suppose I select Door #1, Monty randomly reveals a goat from one of the remaining doors, I make a switch. In this case, I believe I can calculate my probabilities of winning this way: Let $S:\{$ Succeed if I switch$\}$, and let $D_j: \{$ Car behind the Door $j$, where $j=\{1,2,3,4\}$ $ \\} $ Since $P(D_j)=\frac{1}{4}$ , Then, $$\begin{align} P(S)&=P(S|D_{ 1 })\times \frac { 1 }{ 4 } +P(S|D_{ 2 })\times \frac { 1 }{ 4 } +P(S|D_{ 3 })\times \frac { 1 }{ 4 } +P(S|D_{ 4 })\times \frac { 1 }{ 4 } \\ \qquad &=0+(\frac { 1 }{ 2 } \cdot \frac { 1 }{ 4 } )+(\frac { 1 }{ 2 } \cdot \frac { 1 }{ 4 } )+(\frac { 1 }{ 2 } \cdot \frac { 1 }{ 4 } )\\ \qquad &=\frac { 3 }{ 8 } \end{align}$$ Now, in another strategy, say I do this: I select Door #1. Then, If Monty opens Door #2, I switch to Door #3. If Monty opens Door #3, I switch to Door #4. If Monty opens Door #4, I remain with my initial first choice of Door #1. Of course, all that Monty reveals are goats. I want to find out the probability of winning with this strategy. Let $A_j$ be the $j$th event in the above numbered list, where $j=\{1,2,3\}$. Then to find the probability of winning with this new strategy, I find: $$ P(S)=P(S|A_1)P(A_1)+P(S|A_2)P(A_2)+P(S|A_3)P(A_3) $$ I am not sure, but I rationalise the chance of happening for the events $P(A_1)=P(A_2)=P(A_3)=\frac{1}{3}$. With 4 doors and I have chosen 1 door, Monty left with 3 doors. So I thought it has to be $\frac{1}{3}$. After here, I'm very confuse with how I should proceed on. $$P(S|A_1) = P(S|A_2) =\frac { P(A_1\cap S)}{ P(A) } =\frac { P(A_2\cap S)}{ P(A) }= \frac { \frac { 1 }{ 3 } \times \frac { 3 }{ 8 } }{ \frac { 1 }{ 3 }  } =\frac { 3 }{ 8 }  $$ In this equation, I assumed independence between $A_j$ and $S$, even though I don't know if they really are. Also, I am reusing the probability of $P(S)$ from the earlier strategy , which I am still pondering if it even make sense to do this. Anyway, so if I proceed from above, I will get: $$\begin{align*} P(S)&=P(S|A_1)P(A_1)+P(S|A_2)P(A_2)+P(S|A_3)P(A_3) \\ &=\frac { 3 }{ 8 }\cdot\frac { 1 }{ 3 } +\frac { 3 }{ 8 }\cdot\frac { 1 }{ 3 } +\frac { 1 }{ 4 }\cdot\frac { 1 }{ 3 } =\frac { 1 }{ 3 } \end{align*}$$ Finally, my probability of winning with this new strategy would be $\frac{1}{3}$. But I am not certain at all if this is correct or not. Is what I have done and thought correct? What should I do to find out the probabilities of the different strategies?","So I modified the original version of the Monty Hall problem and allow there to have 4 doors; 1 car and 3 goats behind the doors. I will choose one door, Monty, who knows where the car is, randomly reveals one of the other 3 doors to show a goat. In a case which I didn't switch doors, my chance of winning is clearly $\frac{1}{4}$. Suppose I select Door #1, Monty randomly reveals a goat from one of the remaining doors, I make a switch. In this case, I believe I can calculate my probabilities of winning this way: Let $S:\{$ Succeed if I switch$\}$, and let $D_j: \{$ Car behind the Door $j$, where $j=\{1,2,3,4\}$ $ \\} $ Since $P(D_j)=\frac{1}{4}$ , Then, $$\begin{align} P(S)&=P(S|D_{ 1 })\times \frac { 1 }{ 4 } +P(S|D_{ 2 })\times \frac { 1 }{ 4 } +P(S|D_{ 3 })\times \frac { 1 }{ 4 } +P(S|D_{ 4 })\times \frac { 1 }{ 4 } \\ \qquad &=0+(\frac { 1 }{ 2 } \cdot \frac { 1 }{ 4 } )+(\frac { 1 }{ 2 } \cdot \frac { 1 }{ 4 } )+(\frac { 1 }{ 2 } \cdot \frac { 1 }{ 4 } )\\ \qquad &=\frac { 3 }{ 8 } \end{align}$$ Now, in another strategy, say I do this: I select Door #1. Then, If Monty opens Door #2, I switch to Door #3. If Monty opens Door #3, I switch to Door #4. If Monty opens Door #4, I remain with my initial first choice of Door #1. Of course, all that Monty reveals are goats. I want to find out the probability of winning with this strategy. Let $A_j$ be the $j$th event in the above numbered list, where $j=\{1,2,3\}$. Then to find the probability of winning with this new strategy, I find: $$ P(S)=P(S|A_1)P(A_1)+P(S|A_2)P(A_2)+P(S|A_3)P(A_3) $$ I am not sure, but I rationalise the chance of happening for the events $P(A_1)=P(A_2)=P(A_3)=\frac{1}{3}$. With 4 doors and I have chosen 1 door, Monty left with 3 doors. So I thought it has to be $\frac{1}{3}$. After here, I'm very confuse with how I should proceed on. $$P(S|A_1) = P(S|A_2) =\frac { P(A_1\cap S)}{ P(A) } =\frac { P(A_2\cap S)}{ P(A) }= \frac { \frac { 1 }{ 3 } \times \frac { 3 }{ 8 } }{ \frac { 1 }{ 3 }  } =\frac { 3 }{ 8 }  $$ In this equation, I assumed independence between $A_j$ and $S$, even though I don't know if they really are. Also, I am reusing the probability of $P(S)$ from the earlier strategy , which I am still pondering if it even make sense to do this. Anyway, so if I proceed from above, I will get: $$\begin{align*} P(S)&=P(S|A_1)P(A_1)+P(S|A_2)P(A_2)+P(S|A_3)P(A_3) \\ &=\frac { 3 }{ 8 }\cdot\frac { 1 }{ 3 } +\frac { 3 }{ 8 }\cdot\frac { 1 }{ 3 } +\frac { 1 }{ 4 }\cdot\frac { 1 }{ 3 } =\frac { 1 }{ 3 } \end{align*}$$ Finally, my probability of winning with this new strategy would be $\frac{1}{3}$. But I am not certain at all if this is correct or not. Is what I have done and thought correct? What should I do to find out the probabilities of the different strategies?",,"['probability', 'statistics']"
82,Closed form for a sequence related to divisibility,Closed form for a sequence related to divisibility,,"If we consider intervals of the form $[1,p_k !! ]$ in which $p_k!! := 2\cdot3\cdots p_k$ we can ask about distribution of primes, near-primes, etc., on such intervals. A naive approach might be to note first that any interval of length $2n$ contains $n$ even and $n$ odd numbers. We label the evens [[2]] and the odds [[2']]. For the interval  $[1,3!!] = [1, 6] $, labeling numbers divisible by 3 as [[3]] and those not as [[3']], we have that 1/6 are even and divisible by 3, 2/6 are even but not divisible by 3, 1/6 are odd and divisible by 3, 2/6 are odd and not divisible by 3. More economically: $$D_3  = \frac{1}{3}\frac{1}{2} [[2,3]]+\frac{2}{3}\frac{1}{2}[[2,3']]+\frac{1}{3}\frac{1}{2}[[2',3]] + \frac{2}{3}\frac{1}{2}[[2',3']]$$ or $$D_3 = \frac{1}{6} [[2,3]]+\frac{2}{6}[[2,3']]+\frac{1}{6}[[2',3]] + \frac{2}{6}[[2',3']] $$ If we group according to the number of primes by which a number is divided, we get $$D_3 = \frac{1}{6}(2) + \frac{3}{6}(1) + \frac{2}{6}(0)$$ In words, there is one number on [1,6] divisible by both 2 and 3, there are three numbers divisible by either 2 or 3, and there are two numbers divisible by neither 2 nor 3 (i.e, 1,5). Going through the same process with 5 and 7: $$D_5 = \frac{1}{30}[[2,3,5]] + \frac{4}{30}[[2,3,5']] + \frac{2}{30}[[2,3'5]]+ \frac{8}{30}[[2,3',5']] + \frac{1}{30}[[2',3,5]] + \frac{4}{30}[[2',3,5']] + \frac{2}{30}[[2',3',5]]+ \frac{8}{30}[[2'3',5']]$$ and grouping, $$D_5 = \frac{1}{30}(3) + \frac{7}{30}(2) + \frac{14}{30}(1) + \frac{8}{30}(0).$$ Barring typos, etc., $$D_7 = \frac{1}{210}(4 ) +\frac{13}{210}(3) + \frac{56}{210}(2) + \frac{92}{210}(1) + \frac{48}{210}(0) $$ There are problems built into this approximation of divisibility. A number divisible only by $2$ may have repetitions of that factor--$4$, for example.  On $[1,30]$, $22$ has two factors, one of which is not in the set $\{2,3,5\}$. Likewise, on $[1,210]$, $121 = 11^2$, but with respect to $\{2,3,5,7\}$, $121$ is a ""prime.""  So the process doesn't exactly sort according to the number of divisors with repetition. In particular, the class of numbers in $D_{p_k} $ on $ [1, p_k!!] $ mutually prime to $\{ 2,3,\ldots,p_k\}$ is I think identical to $ P_k =  \prod_{n = 1}^k(1 - \frac{1}{p_k} )$ , a gross overestimate of the proportion of primes on these intervals. So much for motivation. Acknowledging the question is of limited interest, can anyone suggest a closed form for the sequence (omitting denominators): 1, 1 1, 3,  2 1, 7, 14, 8 1, 13, 56, 92, 48 ...etc.? Thanks.","If we consider intervals of the form $[1,p_k !! ]$ in which $p_k!! := 2\cdot3\cdots p_k$ we can ask about distribution of primes, near-primes, etc., on such intervals. A naive approach might be to note first that any interval of length $2n$ contains $n$ even and $n$ odd numbers. We label the evens [[2]] and the odds [[2']]. For the interval  $[1,3!!] = [1, 6] $, labeling numbers divisible by 3 as [[3]] and those not as [[3']], we have that 1/6 are even and divisible by 3, 2/6 are even but not divisible by 3, 1/6 are odd and divisible by 3, 2/6 are odd and not divisible by 3. More economically: $$D_3  = \frac{1}{3}\frac{1}{2} [[2,3]]+\frac{2}{3}\frac{1}{2}[[2,3']]+\frac{1}{3}\frac{1}{2}[[2',3]] + \frac{2}{3}\frac{1}{2}[[2',3']]$$ or $$D_3 = \frac{1}{6} [[2,3]]+\frac{2}{6}[[2,3']]+\frac{1}{6}[[2',3]] + \frac{2}{6}[[2',3']] $$ If we group according to the number of primes by which a number is divided, we get $$D_3 = \frac{1}{6}(2) + \frac{3}{6}(1) + \frac{2}{6}(0)$$ In words, there is one number on [1,6] divisible by both 2 and 3, there are three numbers divisible by either 2 or 3, and there are two numbers divisible by neither 2 nor 3 (i.e, 1,5). Going through the same process with 5 and 7: $$D_5 = \frac{1}{30}[[2,3,5]] + \frac{4}{30}[[2,3,5']] + \frac{2}{30}[[2,3'5]]+ \frac{8}{30}[[2,3',5']] + \frac{1}{30}[[2',3,5]] + \frac{4}{30}[[2',3,5']] + \frac{2}{30}[[2',3',5]]+ \frac{8}{30}[[2'3',5']]$$ and grouping, $$D_5 = \frac{1}{30}(3) + \frac{7}{30}(2) + \frac{14}{30}(1) + \frac{8}{30}(0).$$ Barring typos, etc., $$D_7 = \frac{1}{210}(4 ) +\frac{13}{210}(3) + \frac{56}{210}(2) + \frac{92}{210}(1) + \frac{48}{210}(0) $$ There are problems built into this approximation of divisibility. A number divisible only by $2$ may have repetitions of that factor--$4$, for example.  On $[1,30]$, $22$ has two factors, one of which is not in the set $\{2,3,5\}$. Likewise, on $[1,210]$, $121 = 11^2$, but with respect to $\{2,3,5,7\}$, $121$ is a ""prime.""  So the process doesn't exactly sort according to the number of divisors with repetition. In particular, the class of numbers in $D_{p_k} $ on $ [1, p_k!!] $ mutually prime to $\{ 2,3,\ldots,p_k\}$ is I think identical to $ P_k =  \prod_{n = 1}^k(1 - \frac{1}{p_k} )$ , a gross overestimate of the proportion of primes on these intervals. So much for motivation. Acknowledging the question is of limited interest, can anyone suggest a closed form for the sequence (omitting denominators): 1, 1 1, 3,  2 1, 7, 14, 8 1, 13, 56, 92, 48 ...etc.? Thanks.",,"['probability', 'sequences-and-series', 'elementary-number-theory', 'statistics']"
83,Smallest order statistics [closed],Smallest order statistics [closed],,"Closed. This question does not meet Mathematics Stack Exchange guidelines . It is not currently accepting answers. Please provide additional context , which ideally explains why the question is relevant to you and our  community. Some forms of context include: background and motivation, relevant definitions, source, possible strategies, your current progress, why the question is interesting or important, etc. Closed 3 years ago . Improve this question Suppose $X_1$ has exponential distribution with mean $\frac{1}{\theta}$ and $X_2,\ldots,X_n$ have exponential distribution with mean $\frac{2}{\theta}$. also suppose $X_1,X_2,\ldots,X_n$ are independent.how likely that $X_1$ be smallest order statistics  in sample $X_1,X_2,\ldots,X_n$?","Closed. This question does not meet Mathematics Stack Exchange guidelines . It is not currently accepting answers. Please provide additional context , which ideally explains why the question is relevant to you and our  community. Some forms of context include: background and motivation, relevant definitions, source, possible strategies, your current progress, why the question is interesting or important, etc. Closed 3 years ago . Improve this question Suppose $X_1$ has exponential distribution with mean $\frac{1}{\theta}$ and $X_2,\ldots,X_n$ have exponential distribution with mean $\frac{2}{\theta}$. also suppose $X_1,X_2,\ldots,X_n$ are independent.how likely that $X_1$ be smallest order statistics  in sample $X_1,X_2,\ldots,X_n$?",,['statistics']
84,statistics conjecture,statistics conjecture,,"A engineer conjectures that the population mean of a certain component parts is 5.0 millimeters. An experiment is conducted in which 100 parts produced by the process are selected randomly and the diameter measured on each. It is known that the population standard deviation σ = 0.1. The experiment indicates a sample average diameter X = 5.027 millimeters. Does this sample information appear to support or refute the engineer’s conjecture? The solution goes by this here is what bothers me, why is that, he considered the area P(z>2.7). From my point of view, it should be set up like this P(z<2.7) (since we are finding the probability that the length should be between 0 and 0.027 mm. Then it yields 0.99 which means, 99% of the time, it falls with -0.027 - 0 - 0.027 and should support the engineers conjecture. Thanks Please illuminate me if Im wrong","A engineer conjectures that the population mean of a certain component parts is 5.0 millimeters. An experiment is conducted in which 100 parts produced by the process are selected randomly and the diameter measured on each. It is known that the population standard deviation σ = 0.1. The experiment indicates a sample average diameter X = 5.027 millimeters. Does this sample information appear to support or refute the engineer’s conjecture? The solution goes by this here is what bothers me, why is that, he considered the area P(z>2.7). From my point of view, it should be set up like this P(z<2.7) (since we are finding the probability that the length should be between 0 and 0.027 mm. Then it yields 0.99 which means, 99% of the time, it falls with -0.027 - 0 - 0.027 and should support the engineers conjecture. Thanks Please illuminate me if Im wrong",,['statistics']
85,Proving or refuting an inequality regarding the variance,Proving or refuting an inequality regarding the variance,,"I'm trying to prove, or find a counterexample, for the following problem: Let $Y = \{y_i\}_{i=1}^n$ be a set of data, where $y_i \ge 1$ for $i \in \{1,\ldots,n\}$, and let $\alpha$ be a natural number. Then the following holds between the variances of $Y$ and $Y^\alpha = \{y_i^\alpha\}_{i=1}^n$: $Var[Y] \le Var[Y^\alpha]$ Note that: $Var[Y] = \frac{1}{n}\sum_{i=1}^n(y_i - E[Y])^2$ $Var[Y^\alpha] = \frac{1}{n}\sum_{i=1}^n(y_i^\alpha - E[Y^\alpha])^2$ I tried a lot of examples, and a term-wise comparison always resulted in the following (for all $i \in {1,\ldots,n}$): $(y_i - E[Y])^2 \le (y_i^\alpha - E[Y^\alpha])^2$ I'm still not sure if that's the case, but since I couldn't find any counterexamples, I used the identity $A^\alpha - B^\alpha = (A-B)(A^{\alpha-1} + A^{\alpha-2}B + \cdots + B^{\alpha-1})$ to expand left-hand side of the above inequality: $(y_i^\alpha - E[Y^\alpha])^2 = (ny_i^\alpha - (y_1^\alpha+\cdots+y_n^\alpha))^2/n^2 = [\sum_{1\le j \le n}(y_i^\alpha - y_j ^\alpha)]^2/n^2 = [\sum_{1\le j \le n}(y_i-y_j)(y_i^{\alpha-1} + y_i^{\alpha-2}y_j + \cdots + y_j^{\alpha-1})]^2/n^2$ and the value $(y_i^{\alpha-1} + y_i^{\alpha-2}y_j + \cdots + y_j^{\alpha-1})$ is at least $\alpha$, since we assumed that $y_i$'s are at least 1. But that's pretty much it, and I couldn't go any further, since the value $(y_i-y_j)$ might be negative. Any help is appreciated. Please note that my approach might be totally wrong, so please feel free to use any suitable approach. Edit: I finally found an example which shows the term-wise approach does not work. Let $Y=\{2,7,9.7\}$, and consider $\alpha = 2$. Then, $E[Y] = 6.23$ and $E[Y^2] = 49.03$, and for $y_2$ (which is 7) we have: $(y_2 - E[Y])^2 = 0.59$, which is greater than $(y_2^2 - E[Y^2])^2 = 0.0009$. Still, note that the inequality in question holds; i.e. $Var[Y] \le Var[Y^\alpha]$.","I'm trying to prove, or find a counterexample, for the following problem: Let $Y = \{y_i\}_{i=1}^n$ be a set of data, where $y_i \ge 1$ for $i \in \{1,\ldots,n\}$, and let $\alpha$ be a natural number. Then the following holds between the variances of $Y$ and $Y^\alpha = \{y_i^\alpha\}_{i=1}^n$: $Var[Y] \le Var[Y^\alpha]$ Note that: $Var[Y] = \frac{1}{n}\sum_{i=1}^n(y_i - E[Y])^2$ $Var[Y^\alpha] = \frac{1}{n}\sum_{i=1}^n(y_i^\alpha - E[Y^\alpha])^2$ I tried a lot of examples, and a term-wise comparison always resulted in the following (for all $i \in {1,\ldots,n}$): $(y_i - E[Y])^2 \le (y_i^\alpha - E[Y^\alpha])^2$ I'm still not sure if that's the case, but since I couldn't find any counterexamples, I used the identity $A^\alpha - B^\alpha = (A-B)(A^{\alpha-1} + A^{\alpha-2}B + \cdots + B^{\alpha-1})$ to expand left-hand side of the above inequality: $(y_i^\alpha - E[Y^\alpha])^2 = (ny_i^\alpha - (y_1^\alpha+\cdots+y_n^\alpha))^2/n^2 = [\sum_{1\le j \le n}(y_i^\alpha - y_j ^\alpha)]^2/n^2 = [\sum_{1\le j \le n}(y_i-y_j)(y_i^{\alpha-1} + y_i^{\alpha-2}y_j + \cdots + y_j^{\alpha-1})]^2/n^2$ and the value $(y_i^{\alpha-1} + y_i^{\alpha-2}y_j + \cdots + y_j^{\alpha-1})$ is at least $\alpha$, since we assumed that $y_i$'s are at least 1. But that's pretty much it, and I couldn't go any further, since the value $(y_i-y_j)$ might be negative. Any help is appreciated. Please note that my approach might be totally wrong, so please feel free to use any suitable approach. Edit: I finally found an example which shows the term-wise approach does not work. Let $Y=\{2,7,9.7\}$, and consider $\alpha = 2$. Then, $E[Y] = 6.23$ and $E[Y^2] = 49.03$, and for $y_2$ (which is 7) we have: $(y_2 - E[Y])^2 = 0.59$, which is greater than $(y_2^2 - E[Y^2])^2 = 0.0009$. Still, note that the inequality in question holds; i.e. $Var[Y] \le Var[Y^\alpha]$.",,"['statistics', 'inequality']"
86,Partitioned Multivariate Gaussian,Partitioned Multivariate Gaussian,,"My question is on partitioned multivariate Gaussians which are shown as $$ f(x;\mu,\Sigma) =  \frac{ 1}{(2\pi)^{(p+q)/2} \det(\Sigma)^{1/2}}  \exp  \bigg\{  -\frac{ 1}{2} \left[\begin{array}{r} x_1 - \mu_1\\ x_2 - \mu_2 \end{array}\right]^T \left[\begin{array}{rr} \Sigma_{11} & \Sigma_{12}\\ \Sigma_{21} & \Sigma_{22} \end{array}\right]^{-1} \left[\begin{array}{r} x_1 - \mu_1\\ x_2 - \mu_2 \end{array}\right] \bigg\} $$ and $$ \Sigma =  \left[\begin{array}{rr} \Sigma_{11} & \Sigma_{12}\\ \Sigma_{21} & \Sigma_{22} \end{array}\right] $$ In order to derive marginal and conditional densities for pieces of this Gaussian, in book Introduction to Graphical Models chapter 12, Jordan shows These are the final steps of this algebra, right after Schur's complement is used which was used to obtain the pieces for the inverse. However in the last step, the transition from 2nd to 3rd equation, I am not able to follow the derivation, for example in the last equation $-\Sigma_{22}^{-1}\Sigma_{21}$ seems to have disappeared, but it is there in the second equation. Does anyone know how? Is it because transpose of $\Sigma_{12}$ is $\Sigma_{21}$, he had to pull out transpose operator to a larger group of symbols, therefore had to use $(\Sigma_{21}^T)^T$, and inside the paranthesis it is $\Sigma_{12}$.. maybe? Thanks,","My question is on partitioned multivariate Gaussians which are shown as $$ f(x;\mu,\Sigma) =  \frac{ 1}{(2\pi)^{(p+q)/2} \det(\Sigma)^{1/2}}  \exp  \bigg\{  -\frac{ 1}{2} \left[\begin{array}{r} x_1 - \mu_1\\ x_2 - \mu_2 \end{array}\right]^T \left[\begin{array}{rr} \Sigma_{11} & \Sigma_{12}\\ \Sigma_{21} & \Sigma_{22} \end{array}\right]^{-1} \left[\begin{array}{r} x_1 - \mu_1\\ x_2 - \mu_2 \end{array}\right] \bigg\} $$ and $$ \Sigma =  \left[\begin{array}{rr} \Sigma_{11} & \Sigma_{12}\\ \Sigma_{21} & \Sigma_{22} \end{array}\right] $$ In order to derive marginal and conditional densities for pieces of this Gaussian, in book Introduction to Graphical Models chapter 12, Jordan shows These are the final steps of this algebra, right after Schur's complement is used which was used to obtain the pieces for the inverse. However in the last step, the transition from 2nd to 3rd equation, I am not able to follow the derivation, for example in the last equation $-\Sigma_{22}^{-1}\Sigma_{21}$ seems to have disappeared, but it is there in the second equation. Does anyone know how? Is it because transpose of $\Sigma_{12}$ is $\Sigma_{21}$, he had to pull out transpose operator to a larger group of symbols, therefore had to use $(\Sigma_{21}^T)^T$, and inside the paranthesis it is $\Sigma_{12}$.. maybe? Thanks,",,"['linear-algebra', 'statistics']"
87,Definition of rate of convergence for a sequence of measurable mappings,Definition of rate of convergence for a sequence of measurable mappings,,"If a sequence of measurable mappings defined from a measure spaces to another measure space converges in different modes (see Wikipedia and John Cook's site ), I wonder if there are some concepts capturing their rates of convergence for different modes of convergence? You may restrict the discussion to probability theory. One example is in statistics. Suppose $\theta_n$ is an estimator for $\theta^*$ based on a sample with sample size $n$. I was wondering how the rate of convergence of $\theta_n$ is defined? I also appreciate if there can be some references offered. Thanks and regards!","If a sequence of measurable mappings defined from a measure spaces to another measure space converges in different modes (see Wikipedia and John Cook's site ), I wonder if there are some concepts capturing their rates of convergence for different modes of convergence? You may restrict the discussion to probability theory. One example is in statistics. Suppose $\theta_n$ is an estimator for $\theta^*$ based on a sample with sample size $n$. I was wondering how the rate of convergence of $\theta_n$ is defined? I also appreciate if there can be some references offered. Thanks and regards!",,"['probability', 'reference-request', 'statistics', 'measure-theory']"
88,A theorem about inductive inference,A theorem about inductive inference,,"In the book 'Introduction of the theory of Statistics' by Mood,Graybill,Boes (third edition)on page 220 (Chapter 6 on Sampling) you can read: 'Inductive inference is well known to be a hazardous process.In fact,it is a theorem of logic that in inductive inference uncertainty is present.One simply cannot make absolutely certain generalization.' What theorem of logic do they refer to? Can you give me please some reference to this fundamental result ?","In the book 'Introduction of the theory of Statistics' by Mood,Graybill,Boes (third edition)on page 220 (Chapter 6 on Sampling) you can read: 'Inductive inference is well known to be a hazardous process.In fact,it is a theorem of logic that in inductive inference uncertainty is present.One simply cannot make absolutely certain generalization.' What theorem of logic do they refer to? Can you give me please some reference to this fundamental result ?",,"['reference-request', 'statistics', 'soft-question', 'logic', 'meta-math']"
89,Distribution from quantile data or custom distribution,Distribution from quantile data or custom distribution,,"I'd like to fit a distribution (any you like) based on these requirements: Produces integer values (preferable but not required) Mean: $\mu=100$ Std $=114$ Quantiles $( 25\%, 50\%, 75\%)=(6,39,200)$ $\min=0$ ; $\max\approx300$ (but $\infty$ is acceptable); Poisson fits criteria 1,2 and nearly 3, but not 4 by far. Lognormal... maybe. EXTRA info: value $0$ is produced $8\%$ of the time and $300$ is produced $18\%$ of the time. Is it possible to do something like this?","I'd like to fit a distribution (any you like) based on these requirements: Produces integer values (preferable but not required) Mean: Std Quantiles ; (but is acceptable); Poisson fits criteria 1,2 and nearly 3, but not 4 by far. Lognormal... maybe. EXTRA info: value is produced of the time and is produced of the time. Is it possible to do something like this?","\mu=100 =114 ( 25\%, 50\%, 75\%)=(6,39,200) \min=0 \max\approx300 \infty 0 8\% 300 18\%","['statistics', 'probability-distributions']"
90,Statistics with Bayes' theorem,Statistics with Bayes' theorem,,"I am working on a maths exercise and came across this question: Given: A motorist which causes an accident, must submit to a blood test. Research shows that 1% of the drivers which cause crashes, driving under influence (alcohol, drugs). There is a chance of 75% that someone under influence is tested positive. However, there is a chance of 5% that someone who is sober is tested positive. Wanted: (a) What is the probability that someone is under influence if the test was positive? (b) What is the probability that someone is sober, while the test was negative? So i tried calculating these questions. a: ((0.75*0.01) /( 0.75 * 0.01 + 0.25 * 0.99)) = 0.029 or ((0.75*0.99) /( 0.75 * 0.99 + 0.25 * 0.01)) = 0.996 For b:  ((0.05*0.01) /( 0.05 * 0.01 + 0.95 * 0.99)) Are these answers correct? My maths skills are terrible any help is appreciated, Thanks, Jef Ps. My first language isn't english.","I am working on a maths exercise and came across this question: Given: A motorist which causes an accident, must submit to a blood test. Research shows that 1% of the drivers which cause crashes, driving under influence (alcohol, drugs). There is a chance of 75% that someone under influence is tested positive. However, there is a chance of 5% that someone who is sober is tested positive. Wanted: (a) What is the probability that someone is under influence if the test was positive? (b) What is the probability that someone is sober, while the test was negative? So i tried calculating these questions. a: ((0.75*0.01) /( 0.75 * 0.01 + 0.25 * 0.99)) = 0.029 or ((0.75*0.99) /( 0.75 * 0.99 + 0.25 * 0.01)) = 0.996 For b:  ((0.05*0.01) /( 0.05 * 0.01 + 0.95 * 0.99)) Are these answers correct? My maths skills are terrible any help is appreciated, Thanks, Jef Ps. My first language isn't english.",,"['probability', 'statistics']"
91,Statistics Probability,Statistics Probability,,"A survey of adults found that 34% say their favorite sport is professional football.  You randomly select 130 adults and ask them if their favorite sport is professional football. a) Find the probability that at most 66 people say their favorite sport is professional football. The steps I have taken to solve this include: $$n=130,\quad  p=.34,\quad q=.66$$ $$\text{mean} = np = 44.2;\quad \text{standard deviation} = \sqrt{ npq} = 5.4$$ So $$z ={ 66 - 44.2 \over 5.4} = 4.03.$$ This z-score is too high to use on my chart.  I don't know where to go from here. The next question I have is how I would find the probability that MORE than 31 people say their favorite sport is professional football.  I know I am doing something wrong, can anyone help?  Thank you!","A survey of adults found that 34% say their favorite sport is professional football.  You randomly select 130 adults and ask them if their favorite sport is professional football. a) Find the probability that at most 66 people say their favorite sport is professional football. The steps I have taken to solve this include: $$n=130,\quad  p=.34,\quad q=.66$$ $$\text{mean} = np = 44.2;\quad \text{standard deviation} = \sqrt{ npq} = 5.4$$ So $$z ={ 66 - 44.2 \over 5.4} = 4.03.$$ This z-score is too high to use on my chart.  I don't know where to go from here. The next question I have is how I would find the probability that MORE than 31 people say their favorite sport is professional football.  I know I am doing something wrong, can anyone help?  Thank you!",,"['probability', 'statistics']"
92,Conditions for convergence to symmetric stable distribution?,Conditions for convergence to symmetric stable distribution?,,"Under which conditions converges a sum of i.i.d. random variables $$ \frac{1}{a_N} \sum\limits_{n=1}^N X_n $$ to a symmetric stable distribution? Two examples of sufficient conditions are finite variance or symmetry of $X_n$. But can we say anything about the symmetry of the limit distribution without making any of these two assumptions? Thanks.","Under which conditions converges a sum of i.i.d. random variables $$ \frac{1}{a_N} \sum\limits_{n=1}^N X_n $$ to a symmetric stable distribution? Two examples of sufficient conditions are finite variance or symmetry of $X_n$. But can we say anything about the symmetry of the limit distribution without making any of these two assumptions? Thanks.",,"['sequences-and-series', 'statistics', 'probability-theory']"
93,Approximating a continuous distribution with discrete points,Approximating a continuous distribution with discrete points,,"I want to estimate the pdf distribution of a polynomial transformation of a known continuous random variable $X$: $Y = f(X) = a_nX^n + \cdots + a_1X + a_0$ Normally, the only numerical approach I know of is by using a simulation. What I tried to do: evaluate discrete points the pdf of $Y$ by solving a root finding problem: $P(a_nX^n + \cdots + a_1X + a_0 = c) = P(X = \alpha)$, where $\alpha$ is the root of  $a_nX^n + \cdots + a_1X + a_0 - c = 0$. I do this for varying $c$.  I then evaluate these $\alpha$'s in the continuous distribution of the known $X$, and use the found points to fit a curve as approximation to the pdf of $Y$. When I try this, the shape of the curve is right, however, it is not normalized. I believe I am making an error by evaluating discrete points in a continuous distribution (which should be 0 by definition), but why is it that the shape of the curve is still right?","I want to estimate the pdf distribution of a polynomial transformation of a known continuous random variable $X$: $Y = f(X) = a_nX^n + \cdots + a_1X + a_0$ Normally, the only numerical approach I know of is by using a simulation. What I tried to do: evaluate discrete points the pdf of $Y$ by solving a root finding problem: $P(a_nX^n + \cdots + a_1X + a_0 = c) = P(X = \alpha)$, where $\alpha$ is the root of  $a_nX^n + \cdots + a_1X + a_0 - c = 0$. I do this for varying $c$.  I then evaluate these $\alpha$'s in the continuous distribution of the known $X$, and use the found points to fit a curve as approximation to the pdf of $Y$. When I try this, the shape of the curve is right, however, it is not normalized. I believe I am making an error by evaluating discrete points in a continuous distribution (which should be 0 by definition), but why is it that the shape of the curve is still right?",,"['statistics', 'probability-distributions']"
94,What are the chances of getting $K$ identical outcomes in a row in a series of $Y$ independent events?,What are the chances of getting  identical outcomes in a row in a series of  independent events?,K Y,"Each event has two possible outcomes: a and b . Each event is independent. Outcome a has $X\%$ chance of happening. If we run $Y$ number of such events, what are the chances of getting $K$ outcomes a in a row, at least once? To put it in practical terms imagine a series of coin tosses. Each coin toss has two possible outcomes: a = heads and b = tails. Outcome a = heads has a chance of $50\%$. If we run $Y = 100$ number of coin tosses, what are the chances of getting $K = 10$ outcomes a =heads in a row, at least once? I'm using the practical example only so I can express the idea more clearly. I'm interested in finding out the general formulas and computations so that I can run the calculations for any $X$, $Y$, or $K$. I would also be very interested in knowing how the probabilities would change if the events would not be independent. What if the more outcomes a we have in a row the bigger the chance of getting an outcome a in the next event? What if it's the other way around, and the more outcomes a in a row we have the lower the chance of getting another a ?","Each event has two possible outcomes: a and b . Each event is independent. Outcome a has $X\%$ chance of happening. If we run $Y$ number of such events, what are the chances of getting $K$ outcomes a in a row, at least once? To put it in practical terms imagine a series of coin tosses. Each coin toss has two possible outcomes: a = heads and b = tails. Outcome a = heads has a chance of $50\%$. If we run $Y = 100$ number of coin tosses, what are the chances of getting $K = 10$ outcomes a =heads in a row, at least once? I'm using the practical example only so I can express the idea more clearly. I'm interested in finding out the general formulas and computations so that I can run the calculations for any $X$, $Y$, or $K$. I would also be very interested in knowing how the probabilities would change if the events would not be independent. What if the more outcomes a we have in a row the bigger the chance of getting an outcome a in the next event? What if it's the other way around, and the more outcomes a in a row we have the lower the chance of getting another a ?",,"['probability', 'statistics']"
95,Sample Mean & Variance,Sample Mean & Variance,,"Let $X, Y$ be IID $\sim N(\mu, \sigma^2)$. $$M = \frac12(X + Y),\qquad  V = (X - M)^2 + (Y - M)^2$$ Consider the joint moment generating function of $(M, X - M, Y - M)$, show that $M$ and $V$ are independent. We haven't learn about stuff like Cochran's Theorm or multivariable normal distribution. I'm confused in that how do you find the MGF of something whos pmf is not give? Or should I be able to work out the pmf? Any help appreciated, thanks!","Let $X, Y$ be IID $\sim N(\mu, \sigma^2)$. $$M = \frac12(X + Y),\qquad  V = (X - M)^2 + (Y - M)^2$$ Consider the joint moment generating function of $(M, X - M, Y - M)$, show that $M$ and $V$ are independent. We haven't learn about stuff like Cochran's Theorm or multivariable normal distribution. I'm confused in that how do you find the MGF of something whos pmf is not give? Or should I be able to work out the pmf? Any help appreciated, thanks!",,"['statistics', 'probability-distributions', 'normal-distribution']"
96,How does the backward/forward algorithm work if there is no end?,How does the backward/forward algorithm work if there is no end?,,"I'm using Jason Eisner's spreadsheet to understand HMM more better. There's a box at the top that have a transition matrix.  I see the Cold day and Hot day options, but don't understand why there's a stop option there(its 10% of occurring). I think its needed for the backwards part of the algorithm, but I'm not sure what I can replace it with if there is no end.  I want to feed data to train the model with no specific end date. I am trying to train a model based on this question , and all options are equally likely at first, so I'm trying to take them and give them all the same percent chance. Can anyone help me understand this?","I'm using Jason Eisner's spreadsheet to understand HMM more better. There's a box at the top that have a transition matrix.  I see the Cold day and Hot day options, but don't understand why there's a stop option there(its 10% of occurring). I think its needed for the backwards part of the algorithm, but I'm not sure what I can replace it with if there is no end.  I want to feed data to train the model with no specific end date. I am trying to train a model based on this question , and all options are equally likely at first, so I'm trying to take them and give them all the same percent chance. Can anyone help me understand this?",,"['statistics', 'algorithms', 'markov-chains', 'machine-learning']"
97,Maximum likelihood covariance estimation of Gaussian,Maximum likelihood covariance estimation of Gaussian,,I was reading these notes on matrix calculus http://research.microsoft.com/en-us/um/people/minka/papers/matrix/minka-matrix.pdf and I could not figure out how to go from equation (30) to (31). Any kind of help is appreciated. Edit: The oI notation is defined in equation (21).,I was reading these notes on matrix calculus http://research.microsoft.com/en-us/um/people/minka/papers/matrix/minka-matrix.pdf and I could not figure out how to go from equation (30) to (31). Any kind of help is appreciated. Edit: The oI notation is defined in equation (21).,,"['calculus', 'matrices', 'statistics', 'convex-optimization']"
98,How to represent uniformity of a surface?,How to represent uniformity of a surface?,,"My knowledge of math is very basic, my statistics knowledge is even less.  It was suggested to me that I try asking this question here, so here we go: I am developing a software application that will measure and analyze properties of an object's surface.  We want to determine how uniform the surface is and compare it to another object to determine which is ""more uniform"".  Due to confidentiality issues as well as simplicity, I'll use an analogy of a dirt field: Field ""A"" is level, however it has many small pits and little mounds.  All of the bumps and pits are pretty close to the same size, but they are everywhere on the field. Field ""B"" is also level and has much less pits and bumps than field A, however the pits and bumps it DOES have are quite large (and deep). Let's say we have this great machine that can measure the the depth and height of the field in a 6"" grid pattern - so we have this large set of data presenting an evenly spaced grid of points for each field. The first part of my problem was to determine uniformity and after asking around and doing some research it seems that ""variance"" is what I was after.  I am now determining variance by dividing the sum of the squared distances from the mean for each point, something like:  1. determine the mean  2. for each point, calculate the square of the distance from the mean  3. Divide the sum of squared distances by number of points. So { 1, 3, 3, 2, 1 } = 0.8 Honestly I'm not sure I'm doing it correctly, but let's assume I am (for now).  My next question is ""what is 0.8?""  It's my variance... OK, but how would I explain that to a layperson?  Our application is not intended for the scientifically minded person, we basically want to convey the message that ""Field A is smoother than field B"" or ""Field A, while it has more imperfections they are evenly spread out and not as intense as field B"" This is difficult to describe...  I basically need to know what 0.8 ""means""?  Percentages are easy, you can say that ""XYZ is 98% efficient"" (at something) and most people can process that information and understand it.  However I don't know how I can say ""XYZ's surface is 0.8 uniform.""  The person would say ""Well what does that mean?  Is that good?"" That's my first problem: How to express uniformity in simple terms or relative to some other value that makes it easily understandable.  If I plug some more severe numbers into my test program, say { 1,1,1,1,60 } I get a variance of 652.6875 - what!? Please keep in mind, I really don't know what I'm talking about Math wise.  What it boils down to is how to express uniformity of a set of values in a meaningful way to a layperson. I also wasn't sure what tags to use, so if I missed some relevant ones please let me know. Thanks for reading.","My knowledge of math is very basic, my statistics knowledge is even less.  It was suggested to me that I try asking this question here, so here we go: I am developing a software application that will measure and analyze properties of an object's surface.  We want to determine how uniform the surface is and compare it to another object to determine which is ""more uniform"".  Due to confidentiality issues as well as simplicity, I'll use an analogy of a dirt field: Field ""A"" is level, however it has many small pits and little mounds.  All of the bumps and pits are pretty close to the same size, but they are everywhere on the field. Field ""B"" is also level and has much less pits and bumps than field A, however the pits and bumps it DOES have are quite large (and deep). Let's say we have this great machine that can measure the the depth and height of the field in a 6"" grid pattern - so we have this large set of data presenting an evenly spaced grid of points for each field. The first part of my problem was to determine uniformity and after asking around and doing some research it seems that ""variance"" is what I was after.  I am now determining variance by dividing the sum of the squared distances from the mean for each point, something like:  1. determine the mean  2. for each point, calculate the square of the distance from the mean  3. Divide the sum of squared distances by number of points. So { 1, 3, 3, 2, 1 } = 0.8 Honestly I'm not sure I'm doing it correctly, but let's assume I am (for now).  My next question is ""what is 0.8?""  It's my variance... OK, but how would I explain that to a layperson?  Our application is not intended for the scientifically minded person, we basically want to convey the message that ""Field A is smoother than field B"" or ""Field A, while it has more imperfections they are evenly spread out and not as intense as field B"" This is difficult to describe...  I basically need to know what 0.8 ""means""?  Percentages are easy, you can say that ""XYZ is 98% efficient"" (at something) and most people can process that information and understand it.  However I don't know how I can say ""XYZ's surface is 0.8 uniform.""  The person would say ""Well what does that mean?  Is that good?"" That's my first problem: How to express uniformity in simple terms or relative to some other value that makes it easily understandable.  If I plug some more severe numbers into my test program, say { 1,1,1,1,60 } I get a variance of 652.6875 - what!? Please keep in mind, I really don't know what I'm talking about Math wise.  What it boils down to is how to express uniformity of a set of values in a meaningful way to a layperson. I also wasn't sure what tags to use, so if I missed some relevant ones please let me know. Thanks for reading.",,"['geometry', 'statistics']"
99,statistic to measure degree of stationary,statistic to measure degree of stationary,,"I want to claim the time series in figure 1 is more stationary than figure 2. However, [Augmented Dickey-Fuller test asserts http://en.wikipedia.org/wiki/Augmented_Dickey%E2%80%93Fuller_test asserts they are both stationary, at least at 90% confidence. But intuition, the distribution of figure 2 seems to be changing over time. Is there any statistic to measure the degree of stationary of a time series? Or any other way to support (or even refute) my claim? Thanks in advance.","I want to claim the time series in figure 1 is more stationary than figure 2. However, [Augmented Dickey-Fuller test asserts http://en.wikipedia.org/wiki/Augmented_Dickey%E2%80%93Fuller_test asserts they are both stationary, at least at 90% confidence. But intuition, the distribution of figure 2 seems to be changing over time. Is there any statistic to measure the degree of stationary of a time series? Or any other way to support (or even refute) my claim? Thanks in advance.",,['statistics']
