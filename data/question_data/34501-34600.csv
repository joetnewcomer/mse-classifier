,title_raw,title_text,title_latex,body_raw,body_text,body_latex,tags
0,Find the Method of Moments estimator of $\theta$ and derive its asymptotic distribution,Find the Method of Moments estimator of  and derive its asymptotic distribution,\theta,"Let $Y_1, \ldots , Y_n$ be a random sample from the uniform distribution on the interval $(0, \theta)$ with an unknown $\theta > 1$. Suppose we only observe for $i = 1, \ldots , n$ $$X_i= \begin{cases} Y_i & \text{if } Y_i \leq 1, \\ 1 & \text{if } Y_i > 1. \end{cases}$$ Find the Method of Moments estimator of $\theta$ and derive its asymptotic distribution So I found the expected value like usual when finding an MME, however I was unable to solve for $\theta.$ This is what I got: $$ \operatorname{E}(X_i) = \frac{\theta-1} 2 +\frac 1 \theta$$ is my $\operatorname{E}(X_i)$ wrong? Because I can't seem to solve for $\theta$ when equating it to the sample mean","Let $Y_1, \ldots , Y_n$ be a random sample from the uniform distribution on the interval $(0, \theta)$ with an unknown $\theta > 1$. Suppose we only observe for $i = 1, \ldots , n$ $$X_i= \begin{cases} Y_i & \text{if } Y_i \leq 1, \\ 1 & \text{if } Y_i > 1. \end{cases}$$ Find the Method of Moments estimator of $\theta$ and derive its asymptotic distribution So I found the expected value like usual when finding an MME, however I was unable to solve for $\theta.$ This is what I got: $$ \operatorname{E}(X_i) = \frac{\theta-1} 2 +\frac 1 \theta$$ is my $\operatorname{E}(X_i)$ wrong? Because I can't seem to solve for $\theta$ when equating it to the sample mean",,"['probability', 'statistics', 'statistical-inference', 'censoring']"
1,An Interesting Two Players' Game Involving Cumulative Sum of Uniform Distribution,An Interesting Two Players' Game Involving Cumulative Sum of Uniform Distribution,,"$A$ and $B$ are two players, each have exactly one turn. $A$ goes first. $A$ keeps on choosing a random number uniformly distributed over $(0,1)$ and add the values. If at one point it exceeds $1$, $A$ loses. If $A$ thinks his cumulative sum is very close to $1$, hence there is a risk of losing, he stops. Then $B$ starts the same process and add the values separately. If at one point $B$ exceeds $A$'s sum and still below $1$, he wins. What is the optimal strategy for $A$ to stop adding and what is the probability of winning in that case ($B$ knows the value $A$ stopped at)? From simulation It appears that the optimal threshold of $A$'s cumulative sum is approximately $0.5772$, which is very close to the Euler-Mascheroni constant $\gamma$.","$A$ and $B$ are two players, each have exactly one turn. $A$ goes first. $A$ keeps on choosing a random number uniformly distributed over $(0,1)$ and add the values. If at one point it exceeds $1$, $A$ loses. If $A$ thinks his cumulative sum is very close to $1$, hence there is a risk of losing, he stops. Then $B$ starts the same process and add the values separately. If at one point $B$ exceeds $A$'s sum and still below $1$, he wins. What is the optimal strategy for $A$ to stop adding and what is the probability of winning in that case ($B$ knows the value $A$ stopped at)? From simulation It appears that the optimal threshold of $A$'s cumulative sum is approximately $0.5772$, which is very close to the Euler-Mascheroni constant $\gamma$.",,"['probability', 'uniform-distribution']"
2,Why probability of unordered samples is not equally likely?,Why probability of unordered samples is not equally likely?,,"Consider a survey where a sample of size k is collected by choosing people from a population of size n one at a time, with replacement and with equal probabilities. Then the n^k ordered samples are equally likely, making the naive definition applicable, but the  ""n+k-1 choose k"" unordered samples (where all that matters is how many times each person was sampled) are not equally likely. [Source: Introduction to probability by joseph k. blitzstein] Could you please explain why is it like that?","Consider a survey where a sample of size k is collected by choosing people from a population of size n one at a time, with replacement and with equal probabilities. Then the n^k ordered samples are equally likely, making the naive definition applicable, but the  ""n+k-1 choose k"" unordered samples (where all that matters is how many times each person was sampled) are not equally likely. [Source: Introduction to probability by joseph k. blitzstein] Could you please explain why is it like that?",,['probability']
3,"Explanation of the formula for combinations without repetition, probability","Explanation of the formula for combinations without repetition, probability",,"The formula for combinations without the repetitions is as follows: $$ \frac{n!}{r!(n-r)!}$$ This is achieved by doing $$\frac{n!}{(n-r)!}*\frac{1}{r!}$$ What I don't understand where $\frac{1}{r!}$ comes from. I know that the first part is what you do when you have n things and want all the combinations of r of those things, in a way that order doesn't matter. How does this $\frac{1}{r!}$ remove repetitions taking in consideration the order? My guess is that $\frac{1}{r!}$ is the percentage of $\frac{n!}{(n-r)!}$ things that are repetitions, but how was this value discovered? Is it just a coincidence or property or is there some logic behind this? Thanks.","The formula for combinations without the repetitions is as follows: $$ \frac{n!}{r!(n-r)!}$$ This is achieved by doing $$\frac{n!}{(n-r)!}*\frac{1}{r!}$$ What I don't understand where $\frac{1}{r!}$ comes from. I know that the first part is what you do when you have n things and want all the combinations of r of those things, in a way that order doesn't matter. How does this $\frac{1}{r!}$ remove repetitions taking in consideration the order? My guess is that $\frac{1}{r!}$ is the percentage of $\frac{n!}{(n-r)!}$ things that are repetitions, but how was this value discovered? Is it just a coincidence or property or is there some logic behind this? Thanks.",,['probability']
4,"What is the probability that in a group of $n$ people, every month of the year has at least one birthday.","What is the probability that in a group of  people, every month of the year has at least one birthday.",n,"What is the probability that in a group of $n$ people, every month of the year has at least one birthday. This is my approach: We have 12 months, and the probability that a month has at least one birthday = $1-(11/12)^n$ . And to find the probability that every month has at least 1 birthday I am trying to use the inclusion exclusion formula. But I am not able to proceed for the probability that there is at least one birthday for each of 2 months. Is my approach correct?","What is the probability that in a group of people, every month of the year has at least one birthday. This is my approach: We have 12 months, and the probability that a month has at least one birthday = . And to find the probability that every month has at least 1 birthday I am trying to use the inclusion exclusion formula. But I am not able to proceed for the probability that there is at least one birthday for each of 2 months. Is my approach correct?",n 1-(11/12)^n,"['probability', 'probability-theory', 'coupon-collector', 'birthday']"
5,Find $f(X)$ that minimizes $E[(Y-f(X))^2|X]$,Find  that minimizes,f(X) E[(Y-f(X))^2|X],Let $X$ and $Y$ random variables with $E(Y)=\mu$ and $E(Y^2)<\infty$.   Deduce that the random variable $f(X)$ that minimizes   $E[(Y-f(X))^2|X]$ is $f(X)=E[Y|X]$. I just find the minimum with derivatives $$\frac{d}{d f(X)}E[(Y-f(X))^2|X]=-2E[Y-f(X)|X]$$ $$=-2E[Y|X]+2E[f(X)|X]=0$$ $$\Leftrightarrow E[Y|X]=E[f(X)|X]$$ $$\Leftrightarrow f(X)=E[Y|X]$$ Is this right? I founded this solution Is this wrong too?,Let $X$ and $Y$ random variables with $E(Y)=\mu$ and $E(Y^2)<\infty$.   Deduce that the random variable $f(X)$ that minimizes   $E[(Y-f(X))^2|X]$ is $f(X)=E[Y|X]$. I just find the minimum with derivatives $$\frac{d}{d f(X)}E[(Y-f(X))^2|X]=-2E[Y-f(X)|X]$$ $$=-2E[Y|X]+2E[f(X)|X]=0$$ $$\Leftrightarrow E[Y|X]=E[f(X)|X]$$ $$\Leftrightarrow f(X)=E[Y|X]$$ Is this right? I founded this solution Is this wrong too?,,"['probability', 'statistics', 'derivatives', 'expectation']"
6,Is $\{\frac1n\sum_{k=1}^n X_k\ \text{converges}\}$ a tail event?,Is  a tail event?,\{\frac1n\sum_{k=1}^n X_k\ \text{converges}\},"Suppose that $X_1,X_2,\dots$ is a sequence of random variables on some probability space. The tail $\sigma$-algebra $\mathcal{T}$ is defined as the intersection of $\sigma$-algebras $\mathcal{T}:=\bigcap_n\mathcal{F}_n$, where $\mathcal{F}_n=\sigma(X_n,X_{n+1},\dots)$ is the $\sigma$-algebra generated by $X_n,X_{n+1},\dots$. I know that  $\{\sum_{k=1}^n X_n\ \text{converges}\}$ is a tail event. But is $\{\frac1n\sum_{k=1}^n X_k\ \text{converges}\}$ a tail event?","Suppose that $X_1,X_2,\dots$ is a sequence of random variables on some probability space. The tail $\sigma$-algebra $\mathcal{T}$ is defined as the intersection of $\sigma$-algebras $\mathcal{T}:=\bigcap_n\mathcal{F}_n$, where $\mathcal{F}_n=\sigma(X_n,X_{n+1},\dots)$ is the $\sigma$-algebra generated by $X_n,X_{n+1},\dots$. I know that  $\{\sum_{k=1}^n X_n\ \text{converges}\}$ is a tail event. But is $\{\frac1n\sum_{k=1}^n X_k\ \text{converges}\}$ a tail event?",,['probability']
7,Showing 2 Distributions are the Same,Showing 2 Distributions are the Same,,"Let $X_1, X_2, \dots$ be i.i.d. exponentially distributed RVs. For $n = 1,2,\dots$ consider: $Y_n := \max(X_1, X_2, \dots, X_n)$ $U_n := \sum_{i=1}^{n}\frac{X_i}{i}$ Show that $Y_n$ and $U_n$ have the same distribution What I've tried: $P(Y_n<y)= P(X_i<y)^n = (1- e^{\lambda y})^n => P(Y_n=y) = n(1- e^{\lambda y})^{n-1}$ But I get stuck with $U_n$ . I tried an MGF, $U_n$ evaluates nicely but then $Y_n$ gets messy. Any thoughts?","Let be i.i.d. exponentially distributed RVs. For consider: Show that and have the same distribution What I've tried: But I get stuck with . I tried an MGF, evaluates nicely but then gets messy. Any thoughts?","X_1, X_2, \dots n = 1,2,\dots Y_n := \max(X_1, X_2, \dots, X_n) U_n := \sum_{i=1}^{n}\frac{X_i}{i} Y_n U_n P(Y_n<y)= P(X_i<y)^n = (1- e^{\lambda y})^n => P(Y_n=y) = n(1- e^{\lambda y})^{n-1} U_n U_n Y_n","['probability', 'probability-distributions']"
8,Explanation of the Sum of an Infinite Series Equation,Explanation of the Sum of an Infinite Series Equation,,"I've been presented with the following infinite sum (where $P$ is the probability of an event, and $1-P$ is therefore the probability of it not occurring. I was given the following equation as fact: $$\sum_{i=1}^\infty(iP^{i-1}(1-P))=\frac{1}{1-p}$$ Now I know from my basic education the sum of an infinite series for probability $P$ is $$ \sum_{i = 0}^\infty P^i = \frac{1}{1-P}$$ Assuming I haven't made a very basic mistake, these are therefore equivalent but I don't really see how. Normally, I would ask the person who proposed the first equation to justify their working, however, in this instance I can't (they have an obscure transform named after them and would take it very badly!). My question is, how is the first equation evaluated to such a simple result, and therefore equivalent to the second. Thanks!","I've been presented with the following infinite sum (where $P$ is the probability of an event, and $1-P$ is therefore the probability of it not occurring. I was given the following equation as fact: $$\sum_{i=1}^\infty(iP^{i-1}(1-P))=\frac{1}{1-p}$$ Now I know from my basic education the sum of an infinite series for probability $P$ is $$ \sum_{i = 0}^\infty P^i = \frac{1}{1-P}$$ Assuming I haven't made a very basic mistake, these are therefore equivalent but I don't really see how. Normally, I would ask the person who proposed the first equation to justify their working, however, in this instance I can't (they have an obscure transform named after them and would take it very badly!). My question is, how is the first equation evaluated to such a simple result, and therefore equivalent to the second. Thanks!",,"['probability', 'sequences-and-series', 'summation', 'power-series']"
9,Probability. Find the CDF of $Y = X^2 $,Probability. Find the CDF of,Y = X^2 ,"Let $X$ have the uniform distribution $U(−1, 3)$. Find the CDF of $Y = X^2$.  I thought this would be simply  $$G(y)= \int_{-\sqrt{(y)}}^{\sqrt{(y)}} \frac{1}{4}  dx$$ where $0\leq{y}<9$. Which is $G(y) = \frac{\sqrt{(y)}}{2}$. However, the answer says when $0\leq{y}<1$ , $G(y) = \frac{\sqrt{(y)}}{2}$ And when $1\leq{y}<9$, $G(y) = \int_{-1}^{\sqrt{(y)}} \frac{1}{4} dx$ = $\frac{\sqrt{(y)}+1}{4}$. And $G(y)$ is the piecewise function of the 2 cases combined. My question is, why are the cases considered separately?","Let $X$ have the uniform distribution $U(−1, 3)$. Find the CDF of $Y = X^2$.  I thought this would be simply  $$G(y)= \int_{-\sqrt{(y)}}^{\sqrt{(y)}} \frac{1}{4}  dx$$ where $0\leq{y}<9$. Which is $G(y) = \frac{\sqrt{(y)}}{2}$. However, the answer says when $0\leq{y}<1$ , $G(y) = \frac{\sqrt{(y)}}{2}$ And when $1\leq{y}<9$, $G(y) = \int_{-1}^{\sqrt{(y)}} \frac{1}{4} dx$ = $\frac{\sqrt{(y)}+1}{4}$. And $G(y)$ is the piecewise function of the 2 cases combined. My question is, why are the cases considered separately?",,"['probability', 'probability-distributions']"
10,Winning All Levels in a Game,Winning All Levels in a Game,,"There are $L$ levels in a game. In each turn of the game, you go through each level one by one and try to complete it. The goal is to complete all levels of the game. The probability of completing any one of the $L$ levels in  a single turn is $p$. If you complete a particular level at a previous turn then that progress is saved and you don't have to complete it in any successive turns. Even if you fail to complete any level at a particular turn, then the turn continues with the other levels(you don't go to a new turn). So in each turn, you try all the $L$ levels. On average, how many turns do you have to play the game to complete all the levels? This was my approach. Let $N_k$ be the average number of turns you need to play the game in order to complete any $k$ of those $L$ levels($0\le$ $k$ $\le$ $L$). I write the following recurrence relation(from which I can easily calculate $N_L$, the desired answer, since $N_0=0$). $(N_k+1)[1-(1-p)^{L-k}]+(N_{k+1}+1)(1-p)^{L-k}=N_{k+1}$ This is because if you win any one of the remaining $L-k$ levels in the next turn, you have taken $N_k+1$ turns to complete $k+1$ levels and if you lose all, you need $N_{k+1}+1$ turns to complete $k+1$ levels. Is this recurrence correct? Is there any loophole in my logic?","There are $L$ levels in a game. In each turn of the game, you go through each level one by one and try to complete it. The goal is to complete all levels of the game. The probability of completing any one of the $L$ levels in  a single turn is $p$. If you complete a particular level at a previous turn then that progress is saved and you don't have to complete it in any successive turns. Even if you fail to complete any level at a particular turn, then the turn continues with the other levels(you don't go to a new turn). So in each turn, you try all the $L$ levels. On average, how many turns do you have to play the game to complete all the levels? This was my approach. Let $N_k$ be the average number of turns you need to play the game in order to complete any $k$ of those $L$ levels($0\le$ $k$ $\le$ $L$). I write the following recurrence relation(from which I can easily calculate $N_L$, the desired answer, since $N_0=0$). $(N_k+1)[1-(1-p)^{L-k}]+(N_{k+1}+1)(1-p)^{L-k}=N_{k+1}$ This is because if you win any one of the remaining $L-k$ levels in the next turn, you have taken $N_k+1$ turns to complete $k+1$ levels and if you lose all, you need $N_{k+1}+1$ turns to complete $k+1$ levels. Is this recurrence correct? Is there any loophole in my logic?",,"['probability', 'combinatorics']"
11,Does weak convergence ($X_n\Rightarrow X$) imply weak convergence of the difference to zero ($X_n-X\Rightarrow 0$)?,Does weak convergence () imply weak convergence of the difference to zero ()?,X_n\Rightarrow X X_n-X\Rightarrow 0,"Let $(X_n)_{n\geq1}$ be a sequence of random variables weakly converging to $X$ ($X_n\Rightarrow X$) as $n\rightarrow \infty$. I am wondering if this implies that $X_n-X\Rightarrow 0$ as $n\rightarrow \infty$? I know the converse is true via continuous mapping, but I would like to know if the two statements are actually equivalent? I believe it should be true as we could use the Skorokhod representation theorem to get the almost sure convergence of the difference to $0$, and thus the weak convergence of the difference to $0$, but I am not sure if this is correct? Any ideas or comments would be greatly appreciated. edit: my reasoning using SRT was as follows: $X_n\Rightarrow X$ so there exist copies $Y_n$ and $Y$ with respective laws equal to those of $X_n$ and $X$ on another probability space (say $(\Omega,\mathcal{A}, \mathbb{Q}))$ such that $Y_n\rightarrow Y$ $\mathbb{Q}$-a.s. As $Y\rightarrow Y$ $\mathbb{Q}$-a.s, then $Y_n-Y\rightarrow 0$ $\mathbb{Q}$-a.s and therefore $Y_n-Y\Rightarrow 0$, and $X_n-Y\Rightarrow 0$. There must be a mistake here somewhere, but I can't seem to see where?","Let $(X_n)_{n\geq1}$ be a sequence of random variables weakly converging to $X$ ($X_n\Rightarrow X$) as $n\rightarrow \infty$. I am wondering if this implies that $X_n-X\Rightarrow 0$ as $n\rightarrow \infty$? I know the converse is true via continuous mapping, but I would like to know if the two statements are actually equivalent? I believe it should be true as we could use the Skorokhod representation theorem to get the almost sure convergence of the difference to $0$, and thus the weak convergence of the difference to $0$, but I am not sure if this is correct? Any ideas or comments would be greatly appreciated. edit: my reasoning using SRT was as follows: $X_n\Rightarrow X$ so there exist copies $Y_n$ and $Y$ with respective laws equal to those of $X_n$ and $X$ on another probability space (say $(\Omega,\mathcal{A}, \mathbb{Q}))$ such that $Y_n\rightarrow Y$ $\mathbb{Q}$-a.s. As $Y\rightarrow Y$ $\mathbb{Q}$-a.s, then $Y_n-Y\rightarrow 0$ $\mathbb{Q}$-a.s and therefore $Y_n-Y\Rightarrow 0$, and $X_n-Y\Rightarrow 0$. There must be a mistake here somewhere, but I can't seem to see where?",,"['probability', 'probability-theory', 'weak-convergence', 'probability-limit-theorems']"
12,Marginal independence v.s. joint independence,Marginal independence v.s. joint independence,,"Suppose that $X$ is independent with $Y$ and is also independent with $Z$. No further assumption is made about the joint distribution of $Y$ and $Z$. Does it follow that $X$ is independent with $(Y,Z)$? I know the reverse direction is true and I suspect the direction is above is not true but I don't have a counterexample.","Suppose that $X$ is independent with $Y$ and is also independent with $Z$. No further assumption is made about the joint distribution of $Y$ and $Z$. Does it follow that $X$ is independent with $(Y,Z)$? I know the reverse direction is true and I suspect the direction is above is not true but I don't have a counterexample.",,"['probability', 'probability-theory', 'independence']"
13,A sum of a random number of Poisson random variables,A sum of a random number of Poisson random variables,,"in my probability class I was given this question on which I am stuck concerning a sum of random number of Poisson random variables: Let us define the countable set of independent random variables $ X_i \sim \mathrm{Pois}(\lambda _i) $ and the random variable $N$ independent from the rest of the $ X_i $ which also has a Poisson distribution $ N \sim \mathrm{Pois}(\lambda) $. We are asked to check if the following sum $ Y = \sum_{i=1}^N X_i $ also has a Poisson distribution and if so, with what parameter? As a hint we are asked to look at the characteristic function of the variable to check. I know the deterministic finite sum of Poisson random variables is again a Poisson random variable with the sum of parameters, but I cannot solve it for a random number of summands that is also Poisson-ly distributed, I know the characteristic function of a Poisson distributed random variable $ \Phi(t) = e^{\lambda (e^{it}-1)} $. I thank all helpers who can show me a way out of this.","in my probability class I was given this question on which I am stuck concerning a sum of random number of Poisson random variables: Let us define the countable set of independent random variables $ X_i \sim \mathrm{Pois}(\lambda _i) $ and the random variable $N$ independent from the rest of the $ X_i $ which also has a Poisson distribution $ N \sim \mathrm{Pois}(\lambda) $. We are asked to check if the following sum $ Y = \sum_{i=1}^N X_i $ also has a Poisson distribution and if so, with what parameter? As a hint we are asked to look at the characteristic function of the variable to check. I know the deterministic finite sum of Poisson random variables is again a Poisson random variable with the sum of parameters, but I cannot solve it for a random number of summands that is also Poisson-ly distributed, I know the characteristic function of a Poisson distributed random variable $ \Phi(t) = e^{\lambda (e^{it}-1)} $. I thank all helpers who can show me a way out of this.",,"['probability', 'probability-theory', 'probability-distributions', 'poisson-distribution', 'characteristic-functions']"
14,1) In how many ways can letters of the word $\text{MAMMAL}$ be arranged in a line?,1) In how many ways can letters of the word  be arranged in a line?,\text{MAMMAL},1) In how many ways can letters of the word MAMMAL be arranged in a line? For this question I put $$\frac{6!}{3!2!} = 60$$ since there are $3$ 'M's and $2$ 'A's 2) The letters of the word PROBABILITY are arranged at random. Find the probability that the two 'I' are separated. Do I have to group the two I's to find the probability of them being separated ? Thank you.,1) In how many ways can letters of the word MAMMAL be arranged in a line? For this question I put $$\frac{6!}{3!2!} = 60$$ since there are $3$ 'M's and $2$ 'A's 2) The letters of the word PROBABILITY are arranged at random. Find the probability that the two 'I' are separated. Do I have to group the two I's to find the probability of them being separated ? Thank you.,,"['probability', 'combinatorics', 'permutations']"
15,How many ways can we deal a 13-card hand with at least one suit that does not appear?,How many ways can we deal a 13-card hand with at least one suit that does not appear?,,"In dealing a $13$-hand card that with at least $1$ suit that does not appear, I came up with this: We can choose $3$ of the $4$ suits, as in $3 \choose 4$, and then $13$ cards out of the $39$ cards (from the $3$ suits), which is $39 \choose 13$. Multiply them together to get $3 \choose 4$$39 \choose 13$, and there we have an answer. However, I don't think that is correct. I think that I am thinking too simple and missing a few calculations and steps. Can someone tell me what I am doing wrong?","In dealing a $13$-hand card that with at least $1$ suit that does not appear, I came up with this: We can choose $3$ of the $4$ suits, as in $3 \choose 4$, and then $13$ cards out of the $39$ cards (from the $3$ suits), which is $39 \choose 13$. Multiply them together to get $3 \choose 4$$39 \choose 13$, and there we have an answer. However, I don't think that is correct. I think that I am thinking too simple and missing a few calculations and steps. Can someone tell me what I am doing wrong?",,"['probability', 'combinations']"
16,"Show $ (\int_{-\infty}^\infty \sqrt{p}\sqrt{q}d\mu)^2\leq 2 \int_{-\infty}^\infty \min\{p,q\}d\mu $",Show," (\int_{-\infty}^\infty \sqrt{p}\sqrt{q}d\mu)^2\leq 2 \int_{-\infty}^\infty \min\{p,q\}d\mu ","Consider a random variable $X$ in $(\Omega, \mathcal{F}, \mathbb{P})$. Let $p,q$ be two densities with respect to a measure $\mu$ in $(\mathbb{R}, \mathcal{B}(\mathbb{R}))$ where $\mathcal{B}(\mathbb{R})$ is the Borel $\sigma$-algebra in $\mathbb{R}$. Could you help me to show that $$ \left(\int_{-\infty}^\infty \sqrt{p}\sqrt{q}d\mu\right)^2\le 2 \int_{-\infty}^\infty\min\{p,q\}d\mu $$ from van der Vaart ""Asymptotic Statistics"" proof of Lemma 14.31. My attempt : \begin{align}\left(\int_{-\infty}^\infty \sqrt{p}\sqrt{q}d\mu\right)^2&\le  \left(\int_{-\infty}^\infty \min\{\sqrt{p},\sqrt{q}\}\left(\sqrt{p}+\sqrt{q}\right)d\mu\right)^2\\[0.2cm]& \le \int_{-\infty}^{\infty}\left(\sqrt{p}+\sqrt{q}\right)^2d\mu \int_{-\infty}^{\infty}\min\left\{\sqrt{p},\sqrt{q}\right\}^2d\mu\\[0.2cm]&=\left(2+2\int_{-\infty}^{\infty}\sqrt{p}\sqrt{q}d\mu\right)\int_{-\infty}^{\infty}\min\{p,q\}d\mu=\dots?\end{align}","Consider a random variable $X$ in $(\Omega, \mathcal{F}, \mathbb{P})$. Let $p,q$ be two densities with respect to a measure $\mu$ in $(\mathbb{R}, \mathcal{B}(\mathbb{R}))$ where $\mathcal{B}(\mathbb{R})$ is the Borel $\sigma$-algebra in $\mathbb{R}$. Could you help me to show that $$ \left(\int_{-\infty}^\infty \sqrt{p}\sqrt{q}d\mu\right)^2\le 2 \int_{-\infty}^\infty\min\{p,q\}d\mu $$ from van der Vaart ""Asymptotic Statistics"" proof of Lemma 14.31. My attempt : \begin{align}\left(\int_{-\infty}^\infty \sqrt{p}\sqrt{q}d\mu\right)^2&\le  \left(\int_{-\infty}^\infty \min\{\sqrt{p},\sqrt{q}\}\left(\sqrt{p}+\sqrt{q}\right)d\mu\right)^2\\[0.2cm]& \le \int_{-\infty}^{\infty}\left(\sqrt{p}+\sqrt{q}\right)^2d\mu \int_{-\infty}^{\infty}\min\left\{\sqrt{p},\sqrt{q}\right\}^2d\mu\\[0.2cm]&=\left(2+2\int_{-\infty}^{\infty}\sqrt{p}\sqrt{q}d\mu\right)\int_{-\infty}^{\infty}\min\{p,q\}d\mu=\dots?\end{align}",,"['probability', 'integration']"
17,An example of a reversible but reducible Markov chain,An example of a reversible but reducible Markov chain,,"The reversibility of a Markov chain is defined in the following way with some basic propositions. Unfortunately all examples of reversible Markov chains shown in my textbook so far are irreducible, giving me an impression (I think it is false) that all reversible Markov chains are irreducible. I am curious about is there any example of reversible yet reducible Markov chains , so the initial distribution satisfying the detailed balance is not the only stationary distribution. Thank you!","The reversibility of a Markov chain is defined in the following way with some basic propositions. Unfortunately all examples of reversible Markov chains shown in my textbook so far are irreducible, giving me an impression (I think it is false) that all reversible Markov chains are irreducible. I am curious about is there any example of reversible yet reducible Markov chains , so the initial distribution satisfying the detailed balance is not the only stationary distribution. Thank you!",,"['probability', 'stochastic-processes', 'markov-chains']"
18,PDFs of Piecewise Transformations: why doesn't it apply in this case?,PDFs of Piecewise Transformations: why doesn't it apply in this case?,,"This is from Casella and Berger's Statistical Inference , although it is more of a probability question than a stats question. Theorem 2.1.8 Let $X$ have pdf $f_{X}$, let $Y = g(X)$. Define the sample space $$\mathcal{X} = \{x:f_{X}(x) > 0\}\text{.}$$ Suppose   there exists a partition $A_0, A_1, \dots, A_k$ of $\mathcal{X}$ such   that $\mathbb{P}\left(X \in A_0\right) = 0$ and $f_{X}$ is continuous   on each $A_i$. Furthermore, suppose there exist functions $g_1, \dots,  g_k$ defined on $A_1, \dots, A_k$, respectively, satisfying: $g(x) = g_i(x)$, for $x \in A_i$, $g_i$ is monotone on $A_i$, the set $\mathcal{Y} = \{y:y = g_i(x)\text{ for some }x \in A_i\}$ is the same for each $i = 1, \dots, k$, and $g_i^{-1}$ has a continuous derivative on $\mathcal{Y}$, for each $i, \dots, k$. Then, $$f_{Y}(y) = \begin{cases} \sum_{i=1}^{k}f_{X}\left(g^{-1}_{i}(y)\right)\left|\dfrac{\text{d}}{\text{d}y}g_i^{-1}(y)\right|\text{,  } & y \in \mathcal{Y} \\ 0\text{, } & \text{otherwise.} \end{cases}$$ Problem 2.7(a) in this book is the following: Let $X$ have pdf $f_{X}(x) = \dfrac{2}{9}(x+1)$, $-1 \leq x \leq 2$.   Find the pdf of $Y = X^2$. Note that Theorem 2.1.8 is not directly   applicable to this problem. Why isn't Theorem 2.1.8 applicable? Take $A_0 = \{0\}$, $A_1 = [-1, 0)$, and $A_2 = (0, 2]$. The transformation $Y = X^2$ is monotone over $A_1$ and $A_2$, so I don't see why this doesn't work. And then Problem 2.7(b) is even more confusing: Show that Theorem 2.1.8 remains valid if the sets $A_0, A_1, \dots, A_k$ contain $\mathcal{X}$, and apply the extension to solve part (a) using $A_0 = \varnothing$, $A_1 = (-1, 1)$, and $A_2 = (1, 2)$. This doesn't make sense to me since $X^2$ is not monotone over $A_1$ , although the extension does make some sense.","This is from Casella and Berger's Statistical Inference , although it is more of a probability question than a stats question. Theorem 2.1.8 Let $X$ have pdf $f_{X}$, let $Y = g(X)$. Define the sample space $$\mathcal{X} = \{x:f_{X}(x) > 0\}\text{.}$$ Suppose   there exists a partition $A_0, A_1, \dots, A_k$ of $\mathcal{X}$ such   that $\mathbb{P}\left(X \in A_0\right) = 0$ and $f_{X}$ is continuous   on each $A_i$. Furthermore, suppose there exist functions $g_1, \dots,  g_k$ defined on $A_1, \dots, A_k$, respectively, satisfying: $g(x) = g_i(x)$, for $x \in A_i$, $g_i$ is monotone on $A_i$, the set $\mathcal{Y} = \{y:y = g_i(x)\text{ for some }x \in A_i\}$ is the same for each $i = 1, \dots, k$, and $g_i^{-1}$ has a continuous derivative on $\mathcal{Y}$, for each $i, \dots, k$. Then, $$f_{Y}(y) = \begin{cases} \sum_{i=1}^{k}f_{X}\left(g^{-1}_{i}(y)\right)\left|\dfrac{\text{d}}{\text{d}y}g_i^{-1}(y)\right|\text{,  } & y \in \mathcal{Y} \\ 0\text{, } & \text{otherwise.} \end{cases}$$ Problem 2.7(a) in this book is the following: Let $X$ have pdf $f_{X}(x) = \dfrac{2}{9}(x+1)$, $-1 \leq x \leq 2$.   Find the pdf of $Y = X^2$. Note that Theorem 2.1.8 is not directly   applicable to this problem. Why isn't Theorem 2.1.8 applicable? Take $A_0 = \{0\}$, $A_1 = [-1, 0)$, and $A_2 = (0, 2]$. The transformation $Y = X^2$ is monotone over $A_1$ and $A_2$, so I don't see why this doesn't work. And then Problem 2.7(b) is even more confusing: Show that Theorem 2.1.8 remains valid if the sets $A_0, A_1, \dots, A_k$ contain $\mathcal{X}$, and apply the extension to solve part (a) using $A_0 = \varnothing$, $A_1 = (-1, 1)$, and $A_2 = (1, 2)$. This doesn't make sense to me since $X^2$ is not monotone over $A_1$ , although the extension does make some sense.",,"['probability', 'probability-distributions', 'transformation']"
19,Moments of minimum of random variables,Moments of minimum of random variables,,"Let $\mu$ be a non-atomic probability measure on $[0,\infty)$ and sample $X_1,X_2$ from $\mu$ independently. Does $\min(X_1,X_2)$ have twice as many moments as $X_1$? Is the quantity $$ \frac{\mathbb E\min(X_1,X_2)}{\left(\mathbb E \sqrt{X_1}\right)^2} $$ bounded away from $0$ and $\infty$? More generally, does $\min(X_1,\ldots,X_n)$ have $n$ times as many moments as $X_1$? Moreover is $$ \frac{\mathbb E\min(X_1,\ldots,X_n)}{\left(\mathbb E \sqrt[n]{X_1}\right)^n} $$ bounded away from $0$ and $\infty$? For nice distributions, the identity $\mathbb E X=\int \mathbb P(X>x)\; d\mu(x)$ allows us to reformulate the general versions as follows: $$ \|\mathbb P(X_1>t)\|_n\approx\|\mathbb P(X_1>t^n)\|_1, $$ where $\approx$ means bounded by constants.","Let $\mu$ be a non-atomic probability measure on $[0,\infty)$ and sample $X_1,X_2$ from $\mu$ independently. Does $\min(X_1,X_2)$ have twice as many moments as $X_1$? Is the quantity $$ \frac{\mathbb E\min(X_1,X_2)}{\left(\mathbb E \sqrt{X_1}\right)^2} $$ bounded away from $0$ and $\infty$? More generally, does $\min(X_1,\ldots,X_n)$ have $n$ times as many moments as $X_1$? Moreover is $$ \frac{\mathbb E\min(X_1,\ldots,X_n)}{\left(\mathbb E \sqrt[n]{X_1}\right)^n} $$ bounded away from $0$ and $\infty$? For nice distributions, the identity $\mathbb E X=\int \mathbb P(X>x)\; d\mu(x)$ allows us to reformulate the general versions as follows: $$ \|\mathbb P(X_1>t)\|_n\approx\|\mathbb P(X_1>t^n)\|_1, $$ where $\approx$ means bounded by constants.",,"['probability', 'random-variables', 'moment-problem']"
20,Markov Chain: flip 8 coins and get 3 consecutive heads,Markov Chain: flip 8 coins and get 3 consecutive heads,,"I was reading the material and I am confused at the following example. Q: In a sequence of independent flips of a fair coin, let N denote the number of flips until there is a run of three consecutive heads. Find $P(N = 8)$. ANS: Another way to determine P(N = 8) is to consider a Markov chain with states 0, 1, 2, 3, 4 where, as before, for i < 3 state i means that we currently are on a run of i consecutive heads, state 3 means that the first run of size 3 has just occurred, and state 4 that a run of size 3 occurred in the past . That is, this Markov chain has transition probability matrix $Q=\begin{matrix} 1/2&1/2&0&0&0\\1/2&0&1/2&0&0\\1/2&0&0&1/2&0\\0&0&0&0&1\\0&0&0&0&1 \end{matrix}$ P(N=8)=$Q^8_{0,3}$ *Confusion: I am wondering what is the state 4? Can I say state 4 is the probability of getting three consecutive heads when we flip the coin 8 times? (State 3 is the until we flip the coin 8 times?)","I was reading the material and I am confused at the following example. Q: In a sequence of independent flips of a fair coin, let N denote the number of flips until there is a run of three consecutive heads. Find $P(N = 8)$. ANS: Another way to determine P(N = 8) is to consider a Markov chain with states 0, 1, 2, 3, 4 where, as before, for i < 3 state i means that we currently are on a run of i consecutive heads, state 3 means that the first run of size 3 has just occurred, and state 4 that a run of size 3 occurred in the past . That is, this Markov chain has transition probability matrix $Q=\begin{matrix} 1/2&1/2&0&0&0\\1/2&0&1/2&0&0\\1/2&0&0&1/2&0\\0&0&0&0&1\\0&0&0&0&1 \end{matrix}$ P(N=8)=$Q^8_{0,3}$ *Confusion: I am wondering what is the state 4? Can I say state 4 is the probability of getting three consecutive heads when we flip the coin 8 times? (State 3 is the until we flip the coin 8 times?)",,"['probability', 'stochastic-processes', 'markov-chains', 'markov-process']"
21,"If I have that $X$ is a random variable satisfying $0\leq X \leq 1$, how can I show that $P\left(X \geq \frac{E(X)}{2}\right) \geq \frac{E(X)}{2}$?","If I have that  is a random variable satisfying , how can I show that ?",X 0\leq X \leq 1 P\left(X \geq \frac{E(X)}{2}\right) \geq \frac{E(X)}{2},"If I have that $X$ is a random variable satisfying $0\leq X \leq 1$, how can I show that $P\left(X \geq \frac{E(X)}{2}\right) \geq \frac{E(X)}{2}$? I saw a footnote which gave a hint to split up $E(X)$ as two integrals over $\left[X<\frac{E(X)}{2}\right]$ and $\left[X\geq\frac{E(X)}{2}\right]$. However, I am not quite sure how to bound this integral. Would anyone have any ideas?","If I have that $X$ is a random variable satisfying $0\leq X \leq 1$, how can I show that $P\left(X \geq \frac{E(X)}{2}\right) \geq \frac{E(X)}{2}$? I saw a footnote which gave a hint to split up $E(X)$ as two integrals over $\left[X<\frac{E(X)}{2}\right]$ and $\left[X\geq\frac{E(X)}{2}\right]$. However, I am not quite sure how to bound this integral. Would anyone have any ideas?",,"['probability', 'statistics']"
22,Does convergence in distribution of components imply convergence for the random vector too?,Does convergence in distribution of components imply convergence for the random vector too?,,"If $X_1,X_2,\ldots$ are $\mathbb{R}^d$-valued random variables such that along each component $j:1 \leq j \leq d,$ we have $\{(X_n)_j\} \xrightarrow{\mathcal{D}}U[0,1]$, where $U[0,1]$ denotes the uniform random variable on $[0,1].$ Can we say that $\{X_n\} \xrightarrow{\mathcal{D}}U[0,1]^d$ ? Does the converse hold true ? i.e, if $\{X_n\} \xrightarrow{\mathcal{D}}U[0,1]^d$ then does $\{(X_n)_j\} \xrightarrow{\mathcal{D}}U[0,1]$ hold true ?","If $X_1,X_2,\ldots$ are $\mathbb{R}^d$-valued random variables such that along each component $j:1 \leq j \leq d,$ we have $\{(X_n)_j\} \xrightarrow{\mathcal{D}}U[0,1]$, where $U[0,1]$ denotes the uniform random variable on $[0,1].$ Can we say that $\{X_n\} \xrightarrow{\mathcal{D}}U[0,1]^d$ ? Does the converse hold true ? i.e, if $\{X_n\} \xrightarrow{\mathcal{D}}U[0,1]^d$ then does $\{(X_n)_j\} \xrightarrow{\mathcal{D}}U[0,1]$ hold true ?",,"['probability', 'probability-theory', 'probability-distributions', 'weak-convergence']"
23,Fair die: Probability of rolling $2$ before rolling $3$ or $5$,Fair die: Probability of rolling  before rolling  or,2 3 5,"Independent trials consisting of rolling a fair die are performed, what is the probability that $2$ appears before $3$ or $5?$ There are $36$ cases if we take two trials like $11 12 13 14 15 16  ..21 22 23 24 25 26..31 32 33 34 35 36$ like this . But two has occurred before , so total $6$ cases , favourable just two$(23 25)$ so ${2\over 6} ={1\over 3}$ what is wrong in this approach , answer given is $3\over 8$ .","Independent trials consisting of rolling a fair die are performed, what is the probability that $2$ appears before $3$ or $5?$ There are $36$ cases if we take two trials like $11 12 13 14 15 16  ..21 22 23 24 25 26..31 32 33 34 35 36$ like this . But two has occurred before , so total $6$ cases , favourable just two$(23 25)$ so ${2\over 6} ={1\over 3}$ what is wrong in this approach , answer given is $3\over 8$ .",,['probability']
24,Probability that none of 3 tennis balls chosen at random have been used before,Probability that none of 3 tennis balls chosen at random have been used before,,"There are 15 tennis balls in a box, of which 9 have not previously been used. 3 of these balls are randomly chosen, played with and then returned to the box. later, another 3 balls are randomly chosen from the box. Find the probability that none of these balls has ever been used. I have decided to go simpler way and I have solved this problem another way: $$ (9/15)*(8/14)*(7/13)*(6/15)*(5/14)*(4/13)$$ and I got the answer 0.008114962 But book gives another answer - $0.083$. Did I solve the problem right? If not, tell me please where I have made a mistake.","There are 15 tennis balls in a box, of which 9 have not previously been used. 3 of these balls are randomly chosen, played with and then returned to the box. later, another 3 balls are randomly chosen from the box. Find the probability that none of these balls has ever been used. I have decided to go simpler way and I have solved this problem another way: $$ (9/15)*(8/14)*(7/13)*(6/15)*(5/14)*(4/13)$$ and I got the answer 0.008114962 But book gives another answer - $0.083$. Did I solve the problem right? If not, tell me please where I have made a mistake.",,"['probability', 'balls-in-bins']"
25,Another way of counting probability,Another way of counting probability,,"A set $S = \{1, 2, \cdots, k\}$ is given. Two persons independently choose some numbers from this set. I want to count the probability that the cardinality of intersection of the chosen sets by both persons is exactly one. Each person has to choose at least one number. A person can choose a number which is already chosen by another person. A number is being chosen independently and uniformly at random from the set. My approach : Fix one number from $1$ to $k$. Probability of both persons choosing that number is $\frac{1}{k^2}$. Remaining $k-1$ numbers can be chosen by either of both or by none of both. So, probability for that is $\left(1 - \frac{1}{k^2}\right)^{k-1}$. And that fixed number can be selected in ${k \choose 1}$ ways. So, probability of required event is: \begin{align} P(E) = {k \choose 1} \frac{1}{k^2} \left(1 - \frac{1}{k^2}\right)^{k-1} \end{align} My questions : Is this reasoning correct? Initially I started with counting all the possible ways of choosing numbers by each person. Can we count the required probability by counting ways technique? Is there any other way of counting the probability?","A set $S = \{1, 2, \cdots, k\}$ is given. Two persons independently choose some numbers from this set. I want to count the probability that the cardinality of intersection of the chosen sets by both persons is exactly one. Each person has to choose at least one number. A person can choose a number which is already chosen by another person. A number is being chosen independently and uniformly at random from the set. My approach : Fix one number from $1$ to $k$. Probability of both persons choosing that number is $\frac{1}{k^2}$. Remaining $k-1$ numbers can be chosen by either of both or by none of both. So, probability for that is $\left(1 - \frac{1}{k^2}\right)^{k-1}$. And that fixed number can be selected in ${k \choose 1}$ ways. So, probability of required event is: \begin{align} P(E) = {k \choose 1} \frac{1}{k^2} \left(1 - \frac{1}{k^2}\right)^{k-1} \end{align} My questions : Is this reasoning correct? Initially I started with counting all the possible ways of choosing numbers by each person. Can we count the required probability by counting ways technique? Is there any other way of counting the probability?",,"['probability', 'combinatorics']"
26,Can I always decompose a random variable in sum of iid random variables?,Can I always decompose a random variable in sum of iid random variables?,,"Let $Z$ be a random variable. Can I always find a number $n \in \mathbb N > 1$, weights $w_i \neq 0$, and iid random variables $X_i$ such that $$Z = w_1X_1 + \dots + w_n X_n$$? Conversely, if I have a certain combination of $w_i$ and $X_i$, can I always choose the distribution that the $X_i$ follow so to make $Z$ follow whatever distribution I like? Thanks! (Maybe I should ask the converse in another question?)","Let $Z$ be a random variable. Can I always find a number $n \in \mathbb N > 1$, weights $w_i \neq 0$, and iid random variables $X_i$ such that $$Z = w_1X_1 + \dots + w_n X_n$$? Conversely, if I have a certain combination of $w_i$ and $X_i$, can I always choose the distribution that the $X_i$ follow so to make $Z$ follow whatever distribution I like? Thanks! (Maybe I should ask the converse in another question?)",,"['probability', 'probability-theory', 'random-variables', 'characteristic-functions']"
27,Birthday problem: no unique birthday (all collide),Birthday problem: no unique birthday (all collide),,"I have another variation of a birthday problem to solve. Given $n$ people and $m$ birthdays (let's keep in generic...), what would be the probability that none of the people has a unique birthday date? I've already run out of ways to approach the problem...","I have another variation of a birthday problem to solve. Given $n$ people and $m$ birthdays (let's keep in generic...), what would be the probability that none of the people has a unique birthday date? I've already run out of ways to approach the problem...",,"['probability', 'combinatorics', 'birthday']"
28,"In a game of drawing straws, why are all turns equally good?","In a game of drawing straws, why are all turns equally good?",,"For example, there are 3 straws in a pile - 2 long and 1 short. The person who draws the shortest straw loses. When a straw is drawn, it is removed from the pile. Drawing first, second, or last all apparently result in the same probability for drawing the shortest straw. For me, this is counter-intuitive. I originally thought that drawing the straw later would be more beneficial because the previous drawers would have to not draw the short straw before you have a chance to. Is there a reason why drawing on all turns results in the same probability for drawing the short straw?","For example, there are 3 straws in a pile - 2 long and 1 short. The person who draws the shortest straw loses. When a straw is drawn, it is removed from the pile. Drawing first, second, or last all apparently result in the same probability for drawing the shortest straw. For me, this is counter-intuitive. I originally thought that drawing the straw later would be more beneficial because the previous drawers would have to not draw the short straw before you have a chance to. Is there a reason why drawing on all turns results in the same probability for drawing the short straw?",,"['probability', 'game-theory']"
29,Probability of Winning at least 7 times,Probability of Winning at least 7 times,,"Tried to model a popular game I was playing, but the probabilities seemed off. A game allows you to have up to 12 wins but only allows 3 losses. Each win/lose is independent from each other with a 50% probability and assuming we play until 12 wins are hit or 3 losses have occurred. What is the probability of having at least 7 wins? Attempt: Let X be number of wins. I know that if X = 7, then there   were 10 games total. X = 8, 11 games total. . . . For X = 12, there is   a special case of 12, 13, 14 total games. I assumed that X was a binomial random variable and summed up the   possibilities for X = 7, 8, 9, 10, 11, 12 and it comes out to be around   30.6% which seems plausible, but if I up the win chance to 75%, then the sum of the probabilities becomes greater than one which implies my original logic is flawed.","Tried to model a popular game I was playing, but the probabilities seemed off. A game allows you to have up to 12 wins but only allows 3 losses. Each win/lose is independent from each other with a 50% probability and assuming we play until 12 wins are hit or 3 losses have occurred. What is the probability of having at least 7 wins? Attempt: Let X be number of wins. I know that if X = 7, then there   were 10 games total. X = 8, 11 games total. . . . For X = 12, there is   a special case of 12, 13, 14 total games. I assumed that X was a binomial random variable and summed up the   possibilities for X = 7, 8, 9, 10, 11, 12 and it comes out to be around   30.6% which seems plausible, but if I up the win chance to 75%, then the sum of the probabilities becomes greater than one which implies my original logic is flawed.",,['probability']
30,Characteristic function with modulus 1 implies degenerate distribution,Characteristic function with modulus 1 implies degenerate distribution,,"Let $X$ be a random variable with characteristic function $\phi(\ )$ satisfying $|\phi(t)|=1$ for all $|t|\leq 1/T$ with some $T>0$. Show that $X$ is degenerate, i.e., there is $c$ such that $P(X=c)=1$. My try : $|\phi(t)|^2=1 \implies (\mathbb{E}(\cos tX))^2+(\mathbb{E}(\sin tX))^2=1=\mathbb{E}(\cos^2 tX+\sin^2 tX)=\mathbb{E}(\cos^2 tX)+\mathbb{E}(\sin^2 tX)$ so we can say that $\sin tX=\mathbb{E}(\sin tX), \cos tX=\mathbb{E}(\cos tX)$, that is $\phi(t)=\rm{e}^{\rm{i}tX}$ for $|t|\leq 1/T$. But I cannot go anywhere from here, can someone help me? Thanks. Edit :  I found out this fact. Let $\psi(t)=|\phi(t)|^2$ which is a characteristic function and its real, and $\psi(t)=1, |t|\leq 1/T$. Now employ the inequality $\Re(1-\psi(t))\leq 4\Re(1-\psi(t/2))$ now apply this $n$ times we get $(1-\psi(t))\leq 4^n\left(1-\psi\left(\dfrac{t}{2^n}\right)\right)$ now for any $t$ we get rhs goes to $0$. But since $\psi$ is real it is also $\le 1$ so $\psi(t)=1$ for all $t$. Now we have $|\phi(t)|=1$ for all $t$. I know this is pretty pointless, but I don't understand any of the proofs given below, if someone would clearly explain why the sets mentioned have only one element in common, I would be grateful.","Let $X$ be a random variable with characteristic function $\phi(\ )$ satisfying $|\phi(t)|=1$ for all $|t|\leq 1/T$ with some $T>0$. Show that $X$ is degenerate, i.e., there is $c$ such that $P(X=c)=1$. My try : $|\phi(t)|^2=1 \implies (\mathbb{E}(\cos tX))^2+(\mathbb{E}(\sin tX))^2=1=\mathbb{E}(\cos^2 tX+\sin^2 tX)=\mathbb{E}(\cos^2 tX)+\mathbb{E}(\sin^2 tX)$ so we can say that $\sin tX=\mathbb{E}(\sin tX), \cos tX=\mathbb{E}(\cos tX)$, that is $\phi(t)=\rm{e}^{\rm{i}tX}$ for $|t|\leq 1/T$. But I cannot go anywhere from here, can someone help me? Thanks. Edit :  I found out this fact. Let $\psi(t)=|\phi(t)|^2$ which is a characteristic function and its real, and $\psi(t)=1, |t|\leq 1/T$. Now employ the inequality $\Re(1-\psi(t))\leq 4\Re(1-\psi(t/2))$ now apply this $n$ times we get $(1-\psi(t))\leq 4^n\left(1-\psi\left(\dfrac{t}{2^n}\right)\right)$ now for any $t$ we get rhs goes to $0$. But since $\psi$ is real it is also $\le 1$ so $\psi(t)=1$ for all $t$. Now we have $|\phi(t)|=1$ for all $t$. I know this is pretty pointless, but I don't understand any of the proofs given below, if someone would clearly explain why the sets mentioned have only one element in common, I would be grateful.",,"['probability', 'characteristic-functions']"
31,Conditional expectation on Gaussian random variables,Conditional expectation on Gaussian random variables,,"If we suppose that the two independent random variables $X \sim \mathcal{N}(0,\sigma^2_x)$ and $N \sim \mathcal{N}(0,\sigma^2_n)$ and that $S = X + N$, how would I work out the conditional expectation $E[X\mid S=s]$?","If we suppose that the two independent random variables $X \sim \mathcal{N}(0,\sigma^2_x)$ and $N \sim \mathcal{N}(0,\sigma^2_n)$ and that $S = X + N$, how would I work out the conditional expectation $E[X\mid S=s]$?",,['probability']
32,Median of Poisson Distribution,Median of Poisson Distribution,,"I've just taken Probability and Statistics exam and here is the question that becomes a hot debate among my friends in the classroom: What is the median of a random variable that follows a Poisson distribution with parameter $\lambda=5$? $(\text{A})\,\,\,4\quad\quad(\text{B})\,\,\,5\quad\quad(\text{C})\,\,\,6\quad\quad(\text{D})\,\,\,7\quad\quad(\text{E})\,\,\,8$ Here is my attempt: Let $X$ be a random variable that follows a Poisson distribution with parameter $\lambda=5$, then we have $P(X\le4)\approx0.440493$ and $P(X\le5)\approx0.615961$. So, it's clearly the median $(x_{50})$ of $X$ is between $4$ and $5$, more precisely $x_{50}\approx4.32937026824559119408$ obtained by a computer program. Unfortunately, we are only allowed to use a simple scientific calculator so basically it's difficult to obtain that precise value using a simple scientific calculator by trial and error or Newton method due to time limit. I managed to obtain $x_{50}\approx4.33$ by trial and error method, so I answered $4$ because I rounded to the nearest integer. But some of friends argued and they answered $5$ because they used the nearest rank method, $x_{50}=\lceil4.33\rceil=5$. What exactly is the answer? Please explain. Thanks.","I've just taken Probability and Statistics exam and here is the question that becomes a hot debate among my friends in the classroom: What is the median of a random variable that follows a Poisson distribution with parameter $\lambda=5$? $(\text{A})\,\,\,4\quad\quad(\text{B})\,\,\,5\quad\quad(\text{C})\,\,\,6\quad\quad(\text{D})\,\,\,7\quad\quad(\text{E})\,\,\,8$ Here is my attempt: Let $X$ be a random variable that follows a Poisson distribution with parameter $\lambda=5$, then we have $P(X\le4)\approx0.440493$ and $P(X\le5)\approx0.615961$. So, it's clearly the median $(x_{50})$ of $X$ is between $4$ and $5$, more precisely $x_{50}\approx4.32937026824559119408$ obtained by a computer program. Unfortunately, we are only allowed to use a simple scientific calculator so basically it's difficult to obtain that precise value using a simple scientific calculator by trial and error or Newton method due to time limit. I managed to obtain $x_{50}\approx4.33$ by trial and error method, so I answered $4$ because I rounded to the nearest integer. But some of friends argued and they answered $5$ because they used the nearest rank method, $x_{50}=\lceil4.33\rceil=5$. What exactly is the answer? Please explain. Thanks.",,"['probability', 'statistics', 'probability-distributions', 'percentile']"
33,Distribution of Bernoulli and Uniform Random Variable,Distribution of Bernoulli and Uniform Random Variable,,"Here's a problem I am stuck on: Let $X$ and $Y$ be independent random variables such that $X$ is Bernoulli-distributed with $p=1/2$, and $Y$ is uniformly distributed on the interval $[0,1]$. Then: What is the CDF and PDF of $X+Y$ ? Does the PDF of $XY$ exist? What is the CDF of $XY$? I tried finding the CDF of $X+Y$ by conditioning on $X$ as per this answer , but could not get any further. Can anyone show me what to do, or how to do this?","Here's a problem I am stuck on: Let $X$ and $Y$ be independent random variables such that $X$ is Bernoulli-distributed with $p=1/2$, and $Y$ is uniformly distributed on the interval $[0,1]$. Then: What is the CDF and PDF of $X+Y$ ? Does the PDF of $XY$ exist? What is the CDF of $XY$? I tried finding the CDF of $X+Y$ by conditioning on $X$ as per this answer , but could not get any further. Can anyone show me what to do, or how to do this?",,"['probability', 'random-variables', 'uniform-distribution']"
34,Expected Value and Variance of transformed Random variable,Expected Value and Variance of transformed Random variable,,"I am trying to find the expected value and variance of $Y_i=\ln(X_i)$ for $X$ is uniformly distributed between $1$ and $3$. I believe that $E(Y_i)=(\ln3)/2$ and $\operatorname{Var}(x)=(\ln3)^2/12$. Could someone please confirm this to be true? Isn't $Y$ uniformly distributed on $[0,\ln(3)]$ now?","I am trying to find the expected value and variance of $Y_i=\ln(X_i)$ for $X$ is uniformly distributed between $1$ and $3$. I believe that $E(Y_i)=(\ln3)/2$ and $\operatorname{Var}(x)=(\ln3)^2/12$. Could someone please confirm this to be true? Isn't $Y$ uniformly distributed on $[0,\ln(3)]$ now?",,"['probability', 'probability-distributions', 'proof-verification']"
35,Deriving the MSE [mean squared error],Deriving the MSE [mean squared error],,"The above image is from wikipedia. I'm having troubles with the third line to the 4th: question 1): How did they get from $2E[(\hat{\theta} - E(\hat{\theta})(E(\hat{\theta}) - \theta)]$ to $2(E(\hat{\theta}) - \theta)E(\hat{\theta} - E(\hat{\theta}))$? I thought $E(XY) = E(X)E(Y)$ if and only if $X,Y$ are independent random variables? qeustion 2) Furthermore, why does $E(\hat{\theta} - E(\hat{\theta})) = E(\hat{\theta}) - E(\hat{\theta})$? question 3) from the 4th to the 5th line they have for the last term: $E[(E(\hat{\theta}) - \theta)^2]$ but that is not equal to $\text{Bias}(\hat{\theta},\theta)^2$. I have $\text{Bias}(\hat{\theta},\theta)^2 = (E(\hat{\theta}) - \theta)^2$?","The above image is from wikipedia. I'm having troubles with the third line to the 4th: question 1): How did they get from $2E[(\hat{\theta} - E(\hat{\theta})(E(\hat{\theta}) - \theta)]$ to $2(E(\hat{\theta}) - \theta)E(\hat{\theta} - E(\hat{\theta}))$? I thought $E(XY) = E(X)E(Y)$ if and only if $X,Y$ are independent random variables? qeustion 2) Furthermore, why does $E(\hat{\theta} - E(\hat{\theta})) = E(\hat{\theta}) - E(\hat{\theta})$? question 3) from the 4th to the 5th line they have for the last term: $E[(E(\hat{\theta}) - \theta)^2]$ but that is not equal to $\text{Bias}(\hat{\theta},\theta)^2$. I have $\text{Bias}(\hat{\theta},\theta)^2 = (E(\hat{\theta}) - \theta)^2$?",,['probability']
36,"Two numbers are chosen at random over the interval $ [0,1]$",Two numbers are chosen at random over the interval," [0,1]","Two real numbers, $x$ and $y$ are chosen at random over the interval $ [0,1]$. What is the probability that the closest integer to $\frac{x}{y}$ will be even? Floor functions don't play nicely with integrals and I'm fairly certain that's the wrong way to be going about this. Looking for some rigor in the answer, if possible.","Two real numbers, $x$ and $y$ are chosen at random over the interval $ [0,1]$. What is the probability that the closest integer to $\frac{x}{y}$ will be even? Floor functions don't play nicely with integrals and I'm fairly certain that's the wrong way to be going about this. Looking for some rigor in the answer, if possible.",,"['real-analysis', 'probability', 'integration']"
37,How to understand the product of two conditional probabilities?,How to understand the product of two conditional probabilities?,,"I am struggling a  little bit with making sense of the distribution of bigrams in an artificial language I randomly generated from english. Every word occurs with an equal probability but the syllables and phones that make up the word have a specific distribution such that the transitional probabilities within a word are much higher than between words. I am trying to show that the information entropy of a word (log2(1/#words)) is the summation of the IE of it's individual components.  I tried working it out on my own and got stuck, maybe someone could help get my gears going again? So I am aware of the product rule: p(A,B) = p(A|B) * p(B) Where A and B are considered independent events if p(A|B) = p(A,B)/p(B) = p(A) If A and B are considered independent events then we can define the probability of p(A,B) as p(A) * p(B). However what does it mean when you multiply two separate conditional probabilities in this fashion: p(B|A) * p(C|B)? Does this simplify to p(A, B, C)? Also does this generalize when we write the information entropy of A, B and C? I.e] H(A,B,C) = H(B|A) + H(C|B) Where H(x) = log2(1/p(x)) Note that I am interested in the case where A, B and C occur in that exact order.","I am struggling a  little bit with making sense of the distribution of bigrams in an artificial language I randomly generated from english. Every word occurs with an equal probability but the syllables and phones that make up the word have a specific distribution such that the transitional probabilities within a word are much higher than between words. I am trying to show that the information entropy of a word (log2(1/#words)) is the summation of the IE of it's individual components.  I tried working it out on my own and got stuck, maybe someone could help get my gears going again? So I am aware of the product rule: p(A,B) = p(A|B) * p(B) Where A and B are considered independent events if p(A|B) = p(A,B)/p(B) = p(A) If A and B are considered independent events then we can define the probability of p(A,B) as p(A) * p(B). However what does it mean when you multiply two separate conditional probabilities in this fashion: p(B|A) * p(C|B)? Does this simplify to p(A, B, C)? Also does this generalize when we write the information entropy of A, B and C? I.e] H(A,B,C) = H(B|A) + H(C|B) Where H(x) = log2(1/p(x)) Note that I am interested in the case where A, B and C occur in that exact order.",,"['probability', 'information-theory']"
38,"$\operatorname{Cov} \, (A,B)\geq 0$ and $\operatorname{Cov}(B,C)\geq 0\Rightarrow \operatorname{Cov}(A,C)\geq 0$?",and ?,"\operatorname{Cov} \, (A,B)\geq 0 \operatorname{Cov}(B,C)\geq 0\Rightarrow \operatorname{Cov}(A,C)\geq 0","Let's say we have three random variables $A$, $B$ and $C$. I know that $\DeclareMathOperator{cov}{Cov} \cov(A,B)\geq 0$ and $\cov(B,C)\geq 0$ . Then is it true that $\cov(A,C)\geq 0$?","Let's say we have three random variables $A$, $B$ and $C$. I know that $\DeclareMathOperator{cov}{Cov} \cov(A,B)\geq 0$ and $\cov(B,C)\geq 0$ . Then is it true that $\cov(A,C)\geq 0$?",,"['probability', 'covariance']"
39,Probability of cartesian product of events,Probability of cartesian product of events,,"Suppose we have $n$ experiments performed independently. Every $j$ -th experiment has a probability space $(\Omega_j, F_j, P_j)$ associated to it. The outcome of $n$ experiments is a sequence $(w_1, \ldots, w_n$ ) - $w_j$ is the outcome of $j$ -th experiment. Then our sample space (we conduct $n$ experiments) is $\Omega = \Omega_1 \times \Omega_2 \times \cdots \times \Omega_n$ . $$F=F_1\otimes \cdots \otimes F_n \tag{1}.$$ Then probability $P$ should satisfy the property that $P(A) = P_j(A_j)$ where $A_j \in F_j$ , i.e. \begin{align} P(A_1 \times A_2 \times \cdots \times A_n) &= P(A_1 \times \Omega_2 \times \cdots \times \Omega_n) \times \cdots \times P(\Omega_1 \times \cdots \times A_n) \\ &= {} P_1(A_1) \cdot P_2(A_2)\cdot \cdots \cdot P_n(A_n). \tag{2} \end{align} There exists only one $P$ , namely $$P=P_1\otimes P_2\otimes P_3 \otimes \cdots \otimes P_n. \tag{3}$$ Questions It makes sense to me that if we have several experiments with respective sample spaces, then conducting them all gives a sample space that is their Cartesian product. Could you explain what does it all mean? $1.$ What does $(1)$ mean and why is it true? $2.$ I understand the first term - the probability of Cartesian product of events. Why is it equal to the right side and how we obtain the second and third term in this equality (by term I mean the part after the 1st and 2nd '=' sign). For example, after 1st equality sign there is a Cartesian product of probabilities. How can we have Cartesian product of numbers? Isn't it a mistake? Anyway, this equality follows from the above statement 'Then probability $P$ should satisfy the property that $P(A) = P_j(A_j)$ , but I'm not sure what's the connection. $3.$ Again, what does it mean and why is that?","Suppose we have experiments performed independently. Every -th experiment has a probability space associated to it. The outcome of experiments is a sequence ) - is the outcome of -th experiment. Then our sample space (we conduct experiments) is . Then probability should satisfy the property that where , i.e. There exists only one , namely Questions It makes sense to me that if we have several experiments with respective sample spaces, then conducting them all gives a sample space that is their Cartesian product. Could you explain what does it all mean? What does mean and why is it true? I understand the first term - the probability of Cartesian product of events. Why is it equal to the right side and how we obtain the second and third term in this equality (by term I mean the part after the 1st and 2nd '=' sign). For example, after 1st equality sign there is a Cartesian product of probabilities. How can we have Cartesian product of numbers? Isn't it a mistake? Anyway, this equality follows from the above statement 'Then probability should satisfy the property that , but I'm not sure what's the connection. Again, what does it mean and why is that?","n j (\Omega_j, F_j, P_j) n (w_1, \ldots, w_n w_j j n \Omega = \Omega_1 \times \Omega_2 \times \cdots \times \Omega_n F=F_1\otimes \cdots \otimes F_n \tag{1}. P P(A) = P_j(A_j) A_j \in F_j \begin{align}
P(A_1 \times A_2 \times \cdots \times A_n) &= P(A_1 \times \Omega_2 \times \cdots \times \Omega_n) \times \cdots \times P(\Omega_1 \times \cdots \times A_n) \\
&= {} P_1(A_1) \cdot P_2(A_2)\cdot \cdots \cdot P_n(A_n).
\tag{2}
\end{align} P P=P_1\otimes P_2\otimes P_3 \otimes \cdots \otimes P_n. \tag{3} 1. (1) 2. P P(A) = P_j(A_j) 3.",['probability']
40,Understanding probability,Understanding probability,,"I'm stariting to study probability and some really interesting questions starting to bother me. Let's consider the unit circle $C$ and $D$ - the circle with radius $\frac{1}{2}$. I know that the probability of randomly picked point $p$ of C to be in $D$ too is $\frac{\text{area of } D}{\text{area of } C} = \frac{1}{4}$, while the probability $p$ to lie over any particular line through $C$ is $0$. People say that this is because the line does not have area, while the circle has. So I guess this is the case with curves as well, but if the curve is very thick (like $\sin \frac{1}{x}$ near $0$) is the probability of point to lie over such curve still $0$?","I'm stariting to study probability and some really interesting questions starting to bother me. Let's consider the unit circle $C$ and $D$ - the circle with radius $\frac{1}{2}$. I know that the probability of randomly picked point $p$ of C to be in $D$ too is $\frac{\text{area of } D}{\text{area of } C} = \frac{1}{4}$, while the probability $p$ to lie over any particular line through $C$ is $0$. People say that this is because the line does not have area, while the circle has. So I guess this is the case with curves as well, but if the curve is very thick (like $\sin \frac{1}{x}$ near $0$) is the probability of point to lie over such curve still $0$?",,"['probability', 'probability-theory', 'area']"
41,"If $X$, $Y$ are independent random variables and $E[X+Y]$ exists, then $E[X]$ and $E[Y]$ exist.","If ,  are independent random variables and  exists, then  and  exist.",X Y E[X+Y] E[X] E[Y],"I've been trying to show E$|X+Y|$ < $\infty$ $\Rightarrow$ E$|X|$ < $\infty$ by showing E$|X|$ $\leq$ E$|X+Y|$, but I'm stuck and cannot proceed from here. Someone can help me, please? -----[added]----- $$E|X+Y| = \int\int|x+y|f_X f_Y\mathsf dx\mathsf dy = \int E|X+y|f_Y\mathsf dy < \infty.$$ So, can I say that $E|X+y| < \infty$ for almost every $y$ including $y=0$?","I've been trying to show E$|X+Y|$ < $\infty$ $\Rightarrow$ E$|X|$ < $\infty$ by showing E$|X|$ $\leq$ E$|X+Y|$, but I'm stuck and cannot proceed from here. Someone can help me, please? -----[added]----- $$E|X+Y| = \int\int|x+y|f_X f_Y\mathsf dx\mathsf dy = \int E|X+y|f_Y\mathsf dy < \infty.$$ So, can I say that $E|X+y| < \infty$ for almost every $y$ including $y=0$?",,"['probability', 'expectation']"
42,Birthday problem with exactly 3 people,Birthday problem with exactly 3 people,,"I have been doing a bunch of birthday problem questions, however this one has thrown a mental block my way. The questions is: What is the probability that in a room of 10 people exactly 3 people will have the same birth month as each other, while the other 7 have different birth months from everyone else.","I have been doing a bunch of birthday problem questions, however this one has thrown a mental block my way. The questions is: What is the probability that in a room of 10 people exactly 3 people will have the same birth month as each other, while the other 7 have different birth months from everyone else.",,['probability']
43,"If $X_1,X_2$ are independent beta then showing $\sqrt{X_1X_2}$ is beta",If  are independent beta then showing  is beta,"X_1,X_2 \sqrt{X_1X_2}","Here is a problem that came in a semester exam in our university few years back which I am struggling to solve. If $X_1,X_2$ are independent $\beta$ random variables with densities $\beta(n_1,n_2)$ and $\beta(n_1+\dfrac{1}{2},n_2)$ respectively then show that $\sqrt{X_1X_2}$ follows $\beta(2n_1,2n_2)$. I used the Jacobian method to obtain that the density of $Y=\sqrt{X_1X_2}$ is as follows: $$f_Y(y)=\dfrac{4y^{2n_1}}{B(n_1,n_2)B(n_1+\dfrac{1}{2},n_2)}\int_y^1\dfrac{1}{x^2}(1-x^2)^{n_2-1}(1-\dfrac{y^2}{x^2})^{n_2-1}dx$$ I am lost at this point actually. Now, in the main paper, I found a hint had been supplied. I tried to use the hint but could not obtain the desired expressions. The hint is verbatim as follows: Hint: Derive a formula for the density of $Y=\sqrt{X_1X_2}$ in terms of the given densities of $X_1$ and $X_2$ and try to use a change of variable with $z=\dfrac{y^2}{x}$. So at this point, I try to make use of this hint by considering this change of variable. Hence I get, $$f_Y(y)=\dfrac{4y^{2n_1}}{B(n_1,n_2)B(n_1+\dfrac{1}{2},n_2)}\int_{y^2}^y\dfrac{z^2}{y^4}(1-\dfrac{y^4}{z^2})^{n_2-1}(1-y^2.\dfrac{z^2}{y^4})^{n_2-1}\dfrac{y^2}{z^2}dz$$which after simplification turns out to be (writing $x$ for $z$)$$f_Y(y)=\dfrac{4y^{2n_1}}{B(n_1,n_2)B(n_1+\dfrac{1}{2},n_2)}\int_{y^2}^y\dfrac{1}{y^2}(1-\dfrac{y^4}{x^2})^{n_2-1}(1-\dfrac{x^2}{y^2})^{n_2-1}dx$$ I do not really know how to proceed. I am not even sure that I am interpreting the hint properly. Anyway, here goes the rest of the hint: Observe that by using the change of variable $z=\dfrac{y^2}{x}$, the required density can be expressed in two ways to get by averaging $$f_Y(y)=constant.y^{2n_1-1}\int_{y^2}^1(1-\dfrac{y^2}{x})^{n_2-1}(1-x)^{n_2-1}(1+\dfrac{y}{x})\dfrac{1}{\sqrt{x}}dx$$Now divide the range of integration into $(y^2,y)$ and $(y,1)$ and write $(1-\dfrac{y^2}{x})(1-x)=(1-y)^2-(\dfrac{y}{\sqrt{x}}-\sqrt{x})^2$ and proceed with $u=\dfrac{y}{\sqrt{x}}-\sqrt{x}$. Well, honestly, I cannot understand how one can use these hints: it seems I am getting nowhere. Help is appreciated. Thanks in advance.","Here is a problem that came in a semester exam in our university few years back which I am struggling to solve. If $X_1,X_2$ are independent $\beta$ random variables with densities $\beta(n_1,n_2)$ and $\beta(n_1+\dfrac{1}{2},n_2)$ respectively then show that $\sqrt{X_1X_2}$ follows $\beta(2n_1,2n_2)$. I used the Jacobian method to obtain that the density of $Y=\sqrt{X_1X_2}$ is as follows: $$f_Y(y)=\dfrac{4y^{2n_1}}{B(n_1,n_2)B(n_1+\dfrac{1}{2},n_2)}\int_y^1\dfrac{1}{x^2}(1-x^2)^{n_2-1}(1-\dfrac{y^2}{x^2})^{n_2-1}dx$$ I am lost at this point actually. Now, in the main paper, I found a hint had been supplied. I tried to use the hint but could not obtain the desired expressions. The hint is verbatim as follows: Hint: Derive a formula for the density of $Y=\sqrt{X_1X_2}$ in terms of the given densities of $X_1$ and $X_2$ and try to use a change of variable with $z=\dfrac{y^2}{x}$. So at this point, I try to make use of this hint by considering this change of variable. Hence I get, $$f_Y(y)=\dfrac{4y^{2n_1}}{B(n_1,n_2)B(n_1+\dfrac{1}{2},n_2)}\int_{y^2}^y\dfrac{z^2}{y^4}(1-\dfrac{y^4}{z^2})^{n_2-1}(1-y^2.\dfrac{z^2}{y^4})^{n_2-1}\dfrac{y^2}{z^2}dz$$which after simplification turns out to be (writing $x$ for $z$)$$f_Y(y)=\dfrac{4y^{2n_1}}{B(n_1,n_2)B(n_1+\dfrac{1}{2},n_2)}\int_{y^2}^y\dfrac{1}{y^2}(1-\dfrac{y^4}{x^2})^{n_2-1}(1-\dfrac{x^2}{y^2})^{n_2-1}dx$$ I do not really know how to proceed. I am not even sure that I am interpreting the hint properly. Anyway, here goes the rest of the hint: Observe that by using the change of variable $z=\dfrac{y^2}{x}$, the required density can be expressed in two ways to get by averaging $$f_Y(y)=constant.y^{2n_1-1}\int_{y^2}^1(1-\dfrac{y^2}{x})^{n_2-1}(1-x)^{n_2-1}(1+\dfrac{y}{x})\dfrac{1}{\sqrt{x}}dx$$Now divide the range of integration into $(y^2,y)$ and $(y,1)$ and write $(1-\dfrac{y^2}{x})(1-x)=(1-y)^2-(\dfrac{y}{\sqrt{x}}-\sqrt{x})^2$ and proceed with $u=\dfrac{y}{\sqrt{x}}-\sqrt{x}$. Well, honestly, I cannot understand how one can use these hints: it seems I am getting nowhere. Help is appreciated. Thanks in advance.",,"['probability', 'probability-theory', 'probability-distributions', 'definite-integrals']"
44,Probability of a 2 pair in poker,Probability of a 2 pair in poker,,"What is wrong with my method of find the probability of a 2 pair? For the first card we have 52 choices, one this is picked( call it A) we have 3 choices(different suite than A but this same value) for the second card, ( lets call this card B). Now we have AB, to we have to throw away 2 more cards so we don't have 3 of a kind or a four of a kind. Therefore one our third card we have 48 choices and on our fourth card we have 3 choices again similar to above.  On our last card we only have 44 choices left. So the probablity of getting a 2 pair should be (52*3*48*3*44)/(52 choose 5). But this isn't correct. Note: I edited this to ""2 pair""; the original version was wrong.","What is wrong with my method of find the probability of a 2 pair? For the first card we have 52 choices, one this is picked( call it A) we have 3 choices(different suite than A but this same value) for the second card, ( lets call this card B). Now we have AB, to we have to throw away 2 more cards so we don't have 3 of a kind or a four of a kind. Therefore one our third card we have 48 choices and on our fourth card we have 3 choices again similar to above.  On our last card we only have 44 choices left. So the probablity of getting a 2 pair should be (52*3*48*3*44)/(52 choose 5). But this isn't correct. Note: I edited this to ""2 pair""; the original version was wrong.",,"['probability', 'combinatorics']"
45,Probability in network reliability,Probability in network reliability,,"The probability that a link is working fine is given by $2/3$. a)Find the probability that there exists a path from A to B along which no link has failed. (Give a numerical answer.) b) Given that exactly one link in the network has failed, find the probability that there exists a path from A to B along which no link has failed. The first part is pretty straight-forward and I have solved it correctly. But I am pretty confused with my approach in the second part. I wanted to do systematically. So here's is my approach:  $S =$ there is a part from $A$  to $B$, $F_1 =$ link $1$ has failed, $W_1 =$ link $1$ is working , $F =$  exactly one link has failed. So the probability that it is working is given by $P(S | F) = \frac{P(S \cap F)}{P(F)} $ So here $F = (F_1 \cap W_2 \cap W_3 \cap W_4 \cap W_5 ) \cup (W_1 \cap F_2 \cap W_3 \cap W_4 \cap W_5) \cup ... $ From here I am kind of lost. I don't understand where to go from here. Can anyone explain me further from here, how to proceed.","The probability that a link is working fine is given by $2/3$. a)Find the probability that there exists a path from A to B along which no link has failed. (Give a numerical answer.) b) Given that exactly one link in the network has failed, find the probability that there exists a path from A to B along which no link has failed. The first part is pretty straight-forward and I have solved it correctly. But I am pretty confused with my approach in the second part. I wanted to do systematically. So here's is my approach:  $S =$ there is a part from $A$  to $B$, $F_1 =$ link $1$ has failed, $W_1 =$ link $1$ is working , $F =$  exactly one link has failed. So the probability that it is working is given by $P(S | F) = \frac{P(S \cap F)}{P(F)} $ So here $F = (F_1 \cap W_2 \cap W_3 \cap W_4 \cap W_5 ) \cup (W_1 \cap F_2 \cap W_3 \cap W_4 \cap W_5) \cup ... $ From here I am kind of lost. I don't understand where to go from here. Can anyone explain me further from here, how to proceed.",,['probability']
46,Random walk on a tree,Random walk on a tree,,"Consider a Cayley tree with coordination number 3 ( http://en.wikipedia.org/wiki/Bethe_lattice ). Consider two sites, $x$ and $y$, having a distance $k$ one from another. What is the probability that random walk starting from $x$ will ever visit $y$? Jumps are distributed uniformly at random among the neighbours.","Consider a Cayley tree with coordination number 3 ( http://en.wikipedia.org/wiki/Bethe_lattice ). Consider two sites, $x$ and $y$, having a distance $k$ one from another. What is the probability that random walk starting from $x$ will ever visit $y$? Jumps are distributed uniformly at random among the neighbours.",,"['probability', 'graph-theory', 'markov-chains', 'random-walk', 'random-graphs']"
47,Convergence in distribution to derive the expectation convergence,Convergence in distribution to derive the expectation convergence,,"If $X_n\longrightarrow X$ in distribution, $\mathbb{E}(X)\lt\infty$, Do we have the following conclusion: $\mathbb{E}(X_n)\longrightarrow\mathbb{E}(X)$?","If $X_n\longrightarrow X$ in distribution, $\mathbb{E}(X)\lt\infty$, Do we have the following conclusion: $\mathbb{E}(X_n)\longrightarrow\mathbb{E}(X)$?",,"['probability', 'probability-theory', 'probability-distributions', 'expectation', 'weak-convergence']"
48,An intuitive understanding of the equation $Var(X)=E(X^2)-E(X)^2$,An intuitive understanding of the equation,Var(X)=E(X^2)-E(X)^2,"I know the equation $Var(X)=E(X^2)-E(X)^2$ and its proof. After reading the textbook of mine, I found that this equation has been used in a lot of place. I want to know whether there is an intuitive understanding of this equation?","I know the equation $Var(X)=E(X^2)-E(X)^2$ and its proof. After reading the textbook of mine, I found that this equation has been used in a lot of place. I want to know whether there is an intuitive understanding of this equation?",,['probability']
49,Covariance of uniform distribution and it's square,Covariance of uniform distribution and it's square,,"I have $X$ ~ $U(-1,1)$ and $Y = X^2$ random variables, I need to calculate their covariance. My calculations are: $$ Cov(X,Y) = Cov(X,X^2) = E((X-E(X))(X^2-E(X^2))) = E(X X^2) = E(X^3) = 0 $$ because $$ E(X) = E(X^2) = 0 $$ I'm not sure about the $X^3$ part, are my calculations correct?","I have $X$ ~ $U(-1,1)$ and $Y = X^2$ random variables, I need to calculate their covariance. My calculations are: $$ Cov(X,Y) = Cov(X,X^2) = E((X-E(X))(X^2-E(X^2))) = E(X X^2) = E(X^3) = 0 $$ because $$ E(X) = E(X^2) = 0 $$ I'm not sure about the $X^3$ part, are my calculations correct?",,"['probability', 'probability-theory', 'random-variables', 'covariance']"
50,Simple explanation for Hypergeometric distribution probability,Simple explanation for Hypergeometric distribution probability,,"I am following through the Hypergeometric distribution: The probability that we select a sample of size $n$ containing $r$ defective items from a population of $N$ items known to contain $M$ defective items is $P(X = r) = C(M,r) * C(N-M,n-r) / C(N,n)$ where C(P,Q) is the combination of P items taken Q at a time. Explanation on the above equation: (a) we may select $n$ items from a population of $N$ items in $C(N,n)$ ways - understood (b) we may select $r$ defective items from $M$ defective items in $C(M,r)$ ways - understood (c) we may select $n−r$ non-defective items from $N−M$ non-defective items in $C(N−M,n−r)$ ways -did not understand (d) hence we may select $n$ items containing $r$ defectives in $C(M,r) * C(N−M,n-r)$ ways -did not understand Why both (b) and (c) must be considered and those factors got multiplied in (d) Can anybody explain the hypergeometric distribution derivation in simple terms. The above material is taken from here : The Hypergeometric distribution","I am following through the Hypergeometric distribution: The probability that we select a sample of size $n$ containing $r$ defective items from a population of $N$ items known to contain $M$ defective items is $P(X = r) = C(M,r) * C(N-M,n-r) / C(N,n)$ where C(P,Q) is the combination of P items taken Q at a time. Explanation on the above equation: (a) we may select $n$ items from a population of $N$ items in $C(N,n)$ ways - understood (b) we may select $r$ defective items from $M$ defective items in $C(M,r)$ ways - understood (c) we may select $n−r$ non-defective items from $N−M$ non-defective items in $C(N−M,n−r)$ ways -did not understand (d) hence we may select $n$ items containing $r$ defectives in $C(M,r) * C(N−M,n-r)$ ways -did not understand Why both (b) and (c) must be considered and those factors got multiplied in (d) Can anybody explain the hypergeometric distribution derivation in simple terms. The above material is taken from here : The Hypergeometric distribution",,"['probability', 'hypergeometric-function']"
51,Probability of 3 people in a room of 7 having the same birthday?,Probability of 3 people in a room of 7 having the same birthday?,,What is the probability of 3 people in a room of 7 having the same birthday ?,What is the probability of 3 people in a room of 7 having the same birthday ?,,['probability']
52,What is the expected value of the payment if T has exponential distribution with mean 5?,What is the expected value of the payment if T has exponential distribution with mean 5?,,"The initial value of an appliance is $700, and its future value is given by: \begin{align}v(t)=100(2^{3-t}-1),&&0\leq t\leq 3.\end{align} If the appliance fails in the first 3 years, the warranty pays $v(t)$. what is the expected value of the payment on the warranty if $T$ has an exponential distribution with mean 5. I got  \begin{align}\int^3_0(100(2^{3-t}-1)\frac{1}{5}(e^{-t/5})\,dt.\end{align}  I'm not sure if that's right though. any help is appreciated!","The initial value of an appliance is $700, and its future value is given by: \begin{align}v(t)=100(2^{3-t}-1),&&0\leq t\leq 3.\end{align} If the appliance fails in the first 3 years, the warranty pays $v(t)$. what is the expected value of the payment on the warranty if $T$ has an exponential distribution with mean 5. I got  \begin{align}\int^3_0(100(2^{3-t}-1)\frac{1}{5}(e^{-t/5})\,dt.\end{align}  I'm not sure if that's right though. any help is appreciated!",,"['probability', 'probability-theory', 'probability-distributions']"
53,"If $X < a$, $EX < a$?","If , ?",X < a EX < a,"If a r.v. $X < a$, does it imply $EX < a$? If not, why is it different from what I know: If a r.v. $X \leq a$, it  implies $EX \leq a$, proved by replacing $X$ with $a$ as the integrand. Note that $a \in \mathbb R$. If it allows that $a= \infty$, the answer is no: if $X$ has a Pareto distribution with $α=1$, then $X < \infty$, but $EX = \infty$, from http://en.wikipedia.org/wiki/Pareto_distribution and https://stats.stackexchange.com/a/91515/1005 . How can we explain that difference? Thanks.","If a r.v. $X < a$, does it imply $EX < a$? If not, why is it different from what I know: If a r.v. $X \leq a$, it  implies $EX \leq a$, proved by replacing $X$ with $a$ as the integrand. Note that $a \in \mathbb R$. If it allows that $a= \infty$, the answer is no: if $X$ has a Pareto distribution with $α=1$, then $X < \infty$, but $EX = \infty$, from http://en.wikipedia.org/wiki/Pareto_distribution and https://stats.stackexchange.com/a/91515/1005 . How can we explain that difference? Thanks.",,"['real-analysis', 'probability', 'probability-theory']"
54,Generalization of the birthday problem with convergence in distribution.,Generalization of the birthday problem with convergence in distribution.,,"I have independent identically distributed random variables $Y_1, Y_2, ...$ that are uniformly distributed on the set $\lbrace1,2,...,n\rbrace$. I define $X^{(n)}=min\lbrace k:Y_k=Y_j$ for some $ j <k\rbrace$. I'd like to show that $X^{(n)}/\sqrt{n}$ converges in distribution (and to find the distribution function of the limit). Here is what I've done: Letting $F_n(x)$ be the distribution function of $X^{(n)}/\sqrt{n}$, we have that $$F_n(x)=\mathbb{P}(X^{(n)}/\sqrt{n}\leq x)=1-\mathbb{P}(X^{(n)}>\lfloor \sqrt{n}x\rfloor)=1-\prod_{k=1}^{\lfloor \sqrt{n}x\rfloor-1}(1-\frac{k}{n}).$$ But I'm stuck with the limit... Can you help me please, I feel like I've done it wrong!?","I have independent identically distributed random variables $Y_1, Y_2, ...$ that are uniformly distributed on the set $\lbrace1,2,...,n\rbrace$. I define $X^{(n)}=min\lbrace k:Y_k=Y_j$ for some $ j <k\rbrace$. I'd like to show that $X^{(n)}/\sqrt{n}$ converges in distribution (and to find the distribution function of the limit). Here is what I've done: Letting $F_n(x)$ be the distribution function of $X^{(n)}/\sqrt{n}$, we have that $$F_n(x)=\mathbb{P}(X^{(n)}/\sqrt{n}\leq x)=1-\mathbb{P}(X^{(n)}>\lfloor \sqrt{n}x\rfloor)=1-\prod_{k=1}^{\lfloor \sqrt{n}x\rfloor-1}(1-\frac{k}{n}).$$ But I'm stuck with the limit... Can you help me please, I feel like I've done it wrong!?",,"['probability', 'limits', 'probability-distributions', 'probability-limit-theorems']"
55,Probability of server cluster failure,Probability of server cluster failure,,"I have a problem where I'm trying to derive an expression for the availability of a server cluster. Suppose I have 100 servers, where each one of them individually has a probability of failure 0.05. Now, I'm trying to find an expression for the probability that at most 3 of the servers fail . Currently I have reasoned that: $Pr$(at most 3 of the servers fail) = $Pr$(0, 1, 2, or 3 of the servers fail) = $Pr$(0 fail) + $Pr$(1 fails) + $Pr$(2 fail) + $Pr$(3 fail) $Pr$(0 servers fail) = $0.95^{100} = 0.00592$ $Pr$(1 server fails) = $0.95^{99} \times 0.05^1 = 0.05623$ $Pr$(2 server fails) = $0.95^{98} \times 0.05^2 = 0.00906$ $Pr$(3 server fails) = $0.95^{97} \times 0.05^3 = 0.00702$ Thus, $Pr$(at most 3 of the servers fail) = $0.00592 + 0.05623 + 0.00906 + 0.00702 = 0.07823$. Is that correct? That probability of 7.8% seems like a really small number for having at most 3 servers fail.","I have a problem where I'm trying to derive an expression for the availability of a server cluster. Suppose I have 100 servers, where each one of them individually has a probability of failure 0.05. Now, I'm trying to find an expression for the probability that at most 3 of the servers fail . Currently I have reasoned that: $Pr$(at most 3 of the servers fail) = $Pr$(0, 1, 2, or 3 of the servers fail) = $Pr$(0 fail) + $Pr$(1 fails) + $Pr$(2 fail) + $Pr$(3 fail) $Pr$(0 servers fail) = $0.95^{100} = 0.00592$ $Pr$(1 server fails) = $0.95^{99} \times 0.05^1 = 0.05623$ $Pr$(2 server fails) = $0.95^{98} \times 0.05^2 = 0.00906$ $Pr$(3 server fails) = $0.95^{97} \times 0.05^3 = 0.00702$ Thus, $Pr$(at most 3 of the servers fail) = $0.00592 + 0.05623 + 0.00906 + 0.00702 = 0.07823$. Is that correct? That probability of 7.8% seems like a really small number for having at most 3 servers fail.",,"['probability', 'statistics']"
56,Probability of birthday in a group of N people,Probability of birthday in a group of N people,,What is the probability that on any chosen given day (e.g. today) there is at least one person (in a group of N) who is celebrating his birthday? I would say the answer is either $N/365$ because you get $\frac{1}{365}+\frac{1}{365}$..$N$ times or $1-(\frac{364}{365})^N$ because this is $1$ - the chances of all N people having birthday on a different day other than the chosen one. Which is the right one(if any is) and why?,What is the probability that on any chosen given day (e.g. today) there is at least one person (in a group of N) who is celebrating his birthday? I would say the answer is either $N/365$ because you get $\frac{1}{365}+\frac{1}{365}$..$N$ times or $1-(\frac{364}{365})^N$ because this is $1$ - the chances of all N people having birthday on a different day other than the chosen one. Which is the right one(if any is) and why?,,['probability']
57,What is the formula for the birthday problem?,What is the formula for the birthday problem?,,"What is the formula for the birthday problem? I can't seem to find JUST the formula. I don't care how it works at this point, I just want to know what to do. Can someone please just tell me? Most websites seem to be a convoluted mess (at least to me).","What is the formula for the birthday problem? I can't seem to find JUST the formula. I don't care how it works at this point, I just want to know what to do. Can someone please just tell me? Most websites seem to be a convoluted mess (at least to me).",,"['probability', 'statistics', 'birthday']"
58,A counter example of Brownian Motion,A counter example of Brownian Motion,,"Here is an example in my textbook to illustrate why we need the continuous sample path in the definition of Brownian motion. Let $(B_t)$ be a Brownian motion and $U$ be a uniform random variable on $[0,1]$. Define $(\tilde B_t)$ by $$ \tilde B_t = \begin{cases} B_t & \mbox{if } t\neq U, \\ 0 & \mbox{if } t = U. \end{cases}  $$ Then it is claimed that $\tilde B_t$ has the same finite-dimensional distribution as $B_t$ but with discontinuous sample path if $B(U)\neq 0$. I do not understand this example. To be more specific, What does it mean by $t \neq U$ or $t=U$, please? Here, $t$ is a deterministic time and $U$ is a random variable. How can we talk about whether the two are equal or not? Since I do not get the definition of $\tilde B_t$, I cannot see why it has the same finite dimensional distribution and discontinuous sample path. Could anyone explain this with more detail, please? Thank you!","Here is an example in my textbook to illustrate why we need the continuous sample path in the definition of Brownian motion. Let $(B_t)$ be a Brownian motion and $U$ be a uniform random variable on $[0,1]$. Define $(\tilde B_t)$ by $$ \tilde B_t = \begin{cases} B_t & \mbox{if } t\neq U, \\ 0 & \mbox{if } t = U. \end{cases}  $$ Then it is claimed that $\tilde B_t$ has the same finite-dimensional distribution as $B_t$ but with discontinuous sample path if $B(U)\neq 0$. I do not understand this example. To be more specific, What does it mean by $t \neq U$ or $t=U$, please? Here, $t$ is a deterministic time and $U$ is a random variable. How can we talk about whether the two are equal or not? Since I do not get the definition of $\tilde B_t$, I cannot see why it has the same finite dimensional distribution and discontinuous sample path. Could anyone explain this with more detail, please? Thank you!",,"['probability', 'measure-theory', 'probability-theory', 'self-learning', 'brownian-motion']"
59,If $\sum_{n \geq 1}X_n$ converges a.s. then $\forall a > 0: \sum P(|X_n|>a) < \infty$,If  converges a.s. then,\sum_{n \geq 1}X_n \forall a > 0: \sum P(|X_n|>a) < \infty,"I'd like to show that for $(X_n)_{n\geq 1}$ a sequence of real-valued and independent random variables, If $\sum_{n \geq 1}X_n$ converges a.s., then $\forall a > 0: \sum  P(|X_n|>a) < \infty$ I know that since $\sum X_n$ converges a.s. then it converges in probability. For $A_n = \{|X_n| >a\}$ if $P(\limsup A_n) \neq 1 \Rightarrow \sum P(A_n) < \infty$ by Borel-Cantelli as the $A_n$ are independent. I thought I might be able to show that the event $\limsup A_n$ is not in the tail sigma algebra and so by Kolmogorov's 0-1 law the question would be answered, but I got stuck at this point.","I'd like to show that for $(X_n)_{n\geq 1}$ a sequence of real-valued and independent random variables, If $\sum_{n \geq 1}X_n$ converges a.s., then $\forall a > 0: \sum  P(|X_n|>a) < \infty$ I know that since $\sum X_n$ converges a.s. then it converges in probability. For $A_n = \{|X_n| >a\}$ if $P(\limsup A_n) \neq 1 \Rightarrow \sum P(A_n) < \infty$ by Borel-Cantelli as the $A_n$ are independent. I thought I might be able to show that the event $\limsup A_n$ is not in the tail sigma algebra and so by Kolmogorov's 0-1 law the question would be answered, but I got stuck at this point.",,"['probability', 'convergence-divergence']"
60,In 30 boxes are 15 balls. Chance all balls in 10 or less boxes?,In 30 boxes are 15 balls. Chance all balls in 10 or less boxes?,,"Question1: I found 30 boxes. In 10 boxes i found 15 balls. In 20 boxes i found 0 balls. Afer i collected all 15 balls i put them randomly inside the boxes. How much is the chance that all balls are in only 10 boxes or less? Question2: I found 30 boxes. In 10 boxes i found 15 balls. In 20 boxes i found 0 balls. In two of the boxes i could find 3 balls. (So in one box has to be 2 balls and in the other seven boxes have to be 1 ball.) Afer i collected all 15 balls i put them randomly inside the boxes. How much is the chance that i find in only 2 boxes 6 balls or more? I wrote a c# programm and tried it 1 million times. My solution was: With a chance of 12,4694% all balls are in 10 boxes or less.","Question1: I found 30 boxes. In 10 boxes i found 15 balls. In 20 boxes i found 0 balls. Afer i collected all 15 balls i put them randomly inside the boxes. How much is the chance that all balls are in only 10 boxes or less? Question2: I found 30 boxes. In 10 boxes i found 15 balls. In 20 boxes i found 0 balls. In two of the boxes i could find 3 balls. (So in one box has to be 2 balls and in the other seven boxes have to be 1 ball.) Afer i collected all 15 balls i put them randomly inside the boxes. How much is the chance that i find in only 2 boxes 6 balls or more? I wrote a c# programm and tried it 1 million times. My solution was: With a chance of 12,4694% all balls are in 10 boxes or less.",,"['probability', 'statistics']"
61,Binomial dependent on a Poisson,Binomial dependent on a Poisson,,I have been working on a problem with a binomial rv dependent on a poisson rv and have worked through to this point: $P(X=x) = \sum_{n=x}^{\infty} \dfrac{n!}{x!(n-x)!} p^x(1−p)^{n−x} \dfrac{\lambda^n e^{-\lambda}}{n!}$ Might someone guide me through a next step?,I have been working on a problem with a binomial rv dependent on a poisson rv and have worked through to this point: $P(X=x) = \sum_{n=x}^{\infty} \dfrac{n!}{x!(n-x)!} p^x(1−p)^{n−x} \dfrac{\lambda^n e^{-\lambda}}{n!}$ Might someone guide me through a next step?,,"['probability', 'binomial-coefficients', 'poissons-equation']"
62,Probability of a year which is not a leap year,Probability of a year which is not a leap year,,"If a 4 digit year is choosen randomly, what is the probability that it is not a leap year ? This problem has come in my exam and i have written like this I know that the  number of four digit year is divisible by 4, so probability of a year which is not a leap year is 3/4. But When i have seen the answer key , answer is   probability of a year which is not a leap year is  $> 3/4$. Please clear my doubt, Thank you","If a 4 digit year is choosen randomly, what is the probability that it is not a leap year ? This problem has come in my exam and i have written like this I know that the  number of four digit year is divisible by 4, so probability of a year which is not a leap year is 3/4. But When i have seen the answer key , answer is   probability of a year which is not a leap year is  $> 3/4$. Please clear my doubt, Thank you",,"['probability', 'calendar-computations']"
63,Drunken sailor's Random Walking,Drunken sailor's Random Walking,,"A drunken walker is on $x=0$, and if $x<0$, he falls and he dies.(Once he gets position $x<0$, he dies permanently.) There is $0<p<1$ chance to move right ($x \rightarrow x+1$), and $1-p$ chance to move left  ($x \rightarrow x-1$). (1) After $n$th step, what is the probability he is still alive? (2) I have no idea to how to define probability he is still alive after infinite iteration, since there are infinite state and I can't convince this probability will be limit of (1) when $n$ goes $\infty$. So how to define that probability precisely and why it will be limit of (1)?","A drunken walker is on $x=0$, and if $x<0$, he falls and he dies.(Once he gets position $x<0$, he dies permanently.) There is $0<p<1$ chance to move right ($x \rightarrow x+1$), and $1-p$ chance to move left  ($x \rightarrow x-1$). (1) After $n$th step, what is the probability he is still alive? (2) I have no idea to how to define probability he is still alive after infinite iteration, since there are infinite state and I can't convince this probability will be limit of (1) when $n$ goes $\infty$. So how to define that probability precisely and why it will be limit of (1)?",,['probability']
64,What's the probability a random number is at least twice as big as another?,What's the probability a random number is at least twice as big as another?,,"Two numbers $m,n$ are chosen from a normal distribution, i.e. the chance that either number lies between $a$ and $b$ is $$\frac{1}{\sqrt{2\pi}}\int_a^be^{-x^2}dx$$ Edit: you could also say $e^{-x^2}/\sqrt{2\pi}$ is the probability density function. What's the probability that $n\ge 2m$?","Two numbers $m,n$ are chosen from a normal distribution, i.e. the chance that either number lies between $a$ and $b$ is $$\frac{1}{\sqrt{2\pi}}\int_a^be^{-x^2}dx$$ Edit: you could also say $e^{-x^2}/\sqrt{2\pi}$ is the probability density function. What's the probability that $n\ge 2m$?",,"['probability', 'probability-distributions']"
65,4 heads in 8 tosses,4 heads in 8 tosses,,"If someone asked me the odds of getting 4 heads in 8 flips of a fair coin. I would initially think to do something like this: $\dfrac{2^8 - \left( \binom{8}{0} + \binom{8}{1} + \binom{8}{2} + \binom{8}{3} \right)}{2^8} = \dfrac{163}{256} \simeq 63.67\%$ In that there's $2^8$ total different outcomes of 8 flips, and of those (in the numerator) I subtract out the outcomes that have 0 heads, 1 heads, 2 heads, or 3 heads.  (Or equivalently, I could have subtracted out those with 8, 7, 6, or 5 tails). However, I think this is more like asking ""getting at least 4 heads in 8 fair flips""?  I assume if you were tossing these as soon as you hit 4 heads you'd be in a ""win"" state, so it wouldn't matter if the rest of the flips would result in greater than 4 heads for that particular outcome? However, suppose that as soon as 4 heads were obtained, the flips would stop, so it would not be possible to get greater than 4 heads in any outcome. So wouldn't that mean the total outcomes would be less? Either 0 heads happen, 1 heads, 2 heads, 3 heads, or 4 heads: $\binom{8}{0} + \binom{8}{1} + \binom{8}{2} + \binom{8}{3} + \binom{8}{4} = 163 $ Then, I only care about those that have 4 heads: $\dfrac{\binom{8}{4}}{163} = \dfrac{70}{163} \simeq 42.94\%$ So why are the probabilities different? Shouldn't this be the same sort of question?","If someone asked me the odds of getting 4 heads in 8 flips of a fair coin. I would initially think to do something like this: $\dfrac{2^8 - \left( \binom{8}{0} + \binom{8}{1} + \binom{8}{2} + \binom{8}{3} \right)}{2^8} = \dfrac{163}{256} \simeq 63.67\%$ In that there's $2^8$ total different outcomes of 8 flips, and of those (in the numerator) I subtract out the outcomes that have 0 heads, 1 heads, 2 heads, or 3 heads.  (Or equivalently, I could have subtracted out those with 8, 7, 6, or 5 tails). However, I think this is more like asking ""getting at least 4 heads in 8 fair flips""?  I assume if you were tossing these as soon as you hit 4 heads you'd be in a ""win"" state, so it wouldn't matter if the rest of the flips would result in greater than 4 heads for that particular outcome? However, suppose that as soon as 4 heads were obtained, the flips would stop, so it would not be possible to get greater than 4 heads in any outcome. So wouldn't that mean the total outcomes would be less? Either 0 heads happen, 1 heads, 2 heads, 3 heads, or 4 heads: $\binom{8}{0} + \binom{8}{1} + \binom{8}{2} + \binom{8}{3} + \binom{8}{4} = 163 $ Then, I only care about those that have 4 heads: $\dfrac{\binom{8}{4}}{163} = \dfrac{70}{163} \simeq 42.94\%$ So why are the probabilities different? Shouldn't this be the same sort of question?",,"['probability', 'combinatorics']"
66,Non-geometric way to calculate expected value of breaks?,Non-geometric way to calculate expected value of breaks?,,"In ""50 Challenging Problems in Probability"", question #43 is the following: ""A bar is broken at random in two places. Find the average size of the smallest, of the middle-sized, and of the largest pieces."" The author gives what seems like a complicated geometric way of calculating the probabilities. He arrives at the solutions 1/9, 5/18, and 11/18. Is there a simpler, non-geometric way of calculating these probabilities?","In ""50 Challenging Problems in Probability"", question #43 is the following: ""A bar is broken at random in two places. Find the average size of the smallest, of the middle-sized, and of the largest pieces."" The author gives what seems like a complicated geometric way of calculating the probabilities. He arrives at the solutions 1/9, 5/18, and 11/18. Is there a simpler, non-geometric way of calculating these probabilities?",,"['probability', 'puzzle', 'expectation']"
67,Distribution related to brownian bridge,Distribution related to brownian bridge,,"Let $B(t)$ be a Brownian Bridge and $U$ is uniformly distributed on $(0,1)$. I wish to know the distribution function $B(U)$. Is it possible? As we know, $B(t)\sim N(0,t(1-t))$. But, I haven't a clue when $t$ is replaced by random variable $U$. Could anyone help me?","Let $B(t)$ be a Brownian Bridge and $U$ is uniformly distributed on $(0,1)$. I wish to know the distribution function $B(U)$. Is it possible? As we know, $B(t)\sim N(0,t(1-t))$. But, I haven't a clue when $t$ is replaced by random variable $U$. Could anyone help me?",,"['probability', 'probability-theory', 'probability-distributions', 'brownian-motion', 'uniform-distribution']"
68,Expected value of two successive heads or tails (stuck on computation),Expected value of two successive heads or tails (stuck on computation),,"Problem: This is problem 33 of section 2.6 (conditioning) in Bertsekas, Tsitsiklis intro to probability text. We are given that a coin has probability of heads equal to p and tails equal to q and it is tossed successively and independently until a head comes twice in a row or a tail comes twice in a row. What isthe expected value of the number of tosses? Solution: Let $X$ be a random value that tracks the number of tosses. In addition, let $H_k, T_k$ be the event that a head or tail comes up on the kth flip respectively. The solution I am looking at involves using total expectation theorem. I outline the following steps in the solution. We can partition the sample space into $H_1$ and $T_1$, allowing us to invoke the total expectation theorem. Hence, $E[X] = pE[X|H_1] + qE[X|T_1]$ Again by total expectation theorem, we can utilize another partition to obtain $E[X|H_1] = pE[X|H_1,H_2] + qE[X|H_1,T_2] = 2p + q(1 + E[X|T_1])$ $E[X|T_1] = qE[X|T_1,T_2] + pE[X|T_1,H_2] = 2q + p(1+E[X|H_1])$ Stuck: The solution follows up by saying that we can combine the above two relations and use the fact that p+q = 1 to obtain $E[X|T_1] = \frac{2+p^2}{1-pq} , E[X|H_1] = \frac{2+q^2}{1-pq}$ I have been staring at this for a while now, but I cannot seem to see the steps of the computation. I'd be very grateful if someone could fill in the details regarding the computation. Thank you.","Problem: This is problem 33 of section 2.6 (conditioning) in Bertsekas, Tsitsiklis intro to probability text. We are given that a coin has probability of heads equal to p and tails equal to q and it is tossed successively and independently until a head comes twice in a row or a tail comes twice in a row. What isthe expected value of the number of tosses? Solution: Let $X$ be a random value that tracks the number of tosses. In addition, let $H_k, T_k$ be the event that a head or tail comes up on the kth flip respectively. The solution I am looking at involves using total expectation theorem. I outline the following steps in the solution. We can partition the sample space into $H_1$ and $T_1$, allowing us to invoke the total expectation theorem. Hence, $E[X] = pE[X|H_1] + qE[X|T_1]$ Again by total expectation theorem, we can utilize another partition to obtain $E[X|H_1] = pE[X|H_1,H_2] + qE[X|H_1,T_2] = 2p + q(1 + E[X|T_1])$ $E[X|T_1] = qE[X|T_1,T_2] + pE[X|T_1,H_2] = 2q + p(1+E[X|H_1])$ Stuck: The solution follows up by saying that we can combine the above two relations and use the fact that p+q = 1 to obtain $E[X|T_1] = \frac{2+p^2}{1-pq} , E[X|H_1] = \frac{2+q^2}{1-pq}$ I have been staring at this for a while now, but I cannot seem to see the steps of the computation. I'd be very grateful if someone could fill in the details regarding the computation. Thank you.",,"['probability', 'conditional-probability']"
69,Sampling error with weighted mean,Sampling error with weighted mean,,I am studying statistics and I am wondering when it comes to standard error or a sampling if the calculation changes when there are weights added. I have a weighted mean: $$\mu_{w} = \dfrac{\sum_{i=1}^Nw_ix_i}{\sum_{i=1}^Nw_i}$$ and a weighted variance calculated by: $$s^2_{w}=\dfrac{\sum_{i=1}^Nw_i}{(\sum_{i=1}^Nw_i)^2-\sum_{i=1}^Nw_i^2}\cdot \sum_{i=1}^N(x_i-\mu)^2$$ is the sampling error still calculated as $$\text{SE}=\sqrt{\dfrac{s^2_{w}}{n}}$$,I am studying statistics and I am wondering when it comes to standard error or a sampling if the calculation changes when there are weights added. I have a weighted mean: $$\mu_{w} = \dfrac{\sum_{i=1}^Nw_ix_i}{\sum_{i=1}^Nw_i}$$ and a weighted variance calculated by: $$s^2_{w}=\dfrac{\sum_{i=1}^Nw_i}{(\sum_{i=1}^Nw_i)^2-\sum_{i=1}^Nw_i^2}\cdot \sum_{i=1}^N(x_i-\mu)^2$$ is the sampling error still calculated as $$\text{SE}=\sqrt{\dfrac{s^2_{w}}{n}}$$,,"['probability', 'statistics', 'statistical-inference', 'sampling']"
70,Show Wright-Fisher Model is a martingale,Show Wright-Fisher Model is a martingale,,"Consider successive generations of a population of genes of fixed population size M. Each gene can be just one of two alleles, type a or A. Let $Y_n \in S = \{0,1,\dots,M\}$ denote the number of type A alleles in generation n under the standard Wright-Fisher model. You may assume that $\{Y_n : n = 0,1,2,\dots\}$ is a Markov chain with finite sample space S and stationary transition probabilities $$p_{ij} = P(Y_{i + 1} = j|Y_n = i) = {M \choose j} \left( \frac{i}{M} \right) ^j \left( 1 - \frac{i}{M} \right) ^ {M - j}$$ I have shown $E(|Y_n|) < \infty$, and need help showing $E(Y_{n+1} | Y_n, \dots, Y_0) = Y_n$. I have attempted: $$E(Y_{n+1} | Y_n, \dots, Y_0) = \sum_{k=1}^M k{M \choose k} \left( \frac{Y_n}{M} \right) ^k \left( 1 - \frac{Y_n}{M} \right) ^ {M - k}$$ and messing around with it but haven't come out with the result. I have an exam (tomorrow actually) and am just doing some last minute revision. Any help will be appreciated. Thanks.","Consider successive generations of a population of genes of fixed population size M. Each gene can be just one of two alleles, type a or A. Let $Y_n \in S = \{0,1,\dots,M\}$ denote the number of type A alleles in generation n under the standard Wright-Fisher model. You may assume that $\{Y_n : n = 0,1,2,\dots\}$ is a Markov chain with finite sample space S and stationary transition probabilities $$p_{ij} = P(Y_{i + 1} = j|Y_n = i) = {M \choose j} \left( \frac{i}{M} \right) ^j \left( 1 - \frac{i}{M} \right) ^ {M - j}$$ I have shown $E(|Y_n|) < \infty$, and need help showing $E(Y_{n+1} | Y_n, \dots, Y_0) = Y_n$. I have attempted: $$E(Y_{n+1} | Y_n, \dots, Y_0) = \sum_{k=1}^M k{M \choose k} \left( \frac{Y_n}{M} \right) ^k \left( 1 - \frac{Y_n}{M} \right) ^ {M - k}$$ and messing around with it but haven't come out with the result. I have an exam (tomorrow actually) and am just doing some last minute revision. Any help will be appreciated. Thanks.",,['probability']
71,Probability question related to coin tosses,Probability question related to coin tosses,,"In an exam I gave recently, the following question was asked: A fair coin is tossed $10$ times and the outcomes are listed. let $H_i$ be the event that the $i^{th}$ outcome is a head and $A_m$ be the event that the list contains exactly m heads, then are $H_2$ and $A_5$ independent events ? The Official solution to this question was as follows: $$p(H_i) = \frac{1}{2},\qquad p(A_m)=\frac{^{10}C_m}{2^{10}}\\p(H_i\cap{A_m})=\frac{^9C_{m-1}}{2^{10}}.\\\text{For}\;H_i\;\text{and}\;A_m\;\text{to be independent},\;\frac{^9C_{m-1}}{2^{10}}=\frac{1}{2}.\frac{^{10}C_m}{2^{10}}\\ \Rightarrow1=\frac{1}{2}.\frac{10}{m}\Rightarrow m=5\\ \Rightarrow H_2\;\text{and}\;A_5\;\text{are independent events} $$ Now while I understand the mathematics behind this answer, I find it logically confusing that $p(A_5)$ does not get affected whether or not the $2^{nd}$ outcome is heads. Any ideas ?","In an exam I gave recently, the following question was asked: A fair coin is tossed $10$ times and the outcomes are listed. let $H_i$ be the event that the $i^{th}$ outcome is a head and $A_m$ be the event that the list contains exactly m heads, then are $H_2$ and $A_5$ independent events ? The Official solution to this question was as follows: $$p(H_i) = \frac{1}{2},\qquad p(A_m)=\frac{^{10}C_m}{2^{10}}\\p(H_i\cap{A_m})=\frac{^9C_{m-1}}{2^{10}}.\\\text{For}\;H_i\;\text{and}\;A_m\;\text{to be independent},\;\frac{^9C_{m-1}}{2^{10}}=\frac{1}{2}.\frac{^{10}C_m}{2^{10}}\\ \Rightarrow1=\frac{1}{2}.\frac{10}{m}\Rightarrow m=5\\ \Rightarrow H_2\;\text{and}\;A_5\;\text{are independent events} $$ Now while I understand the mathematics behind this answer, I find it logically confusing that $p(A_5)$ does not get affected whether or not the $2^{nd}$ outcome is heads. Any ideas ?",,['probability']
72,why generating function $A(z) = 1 + z + z^2 + \cdots$ can be denoted as $\frac{1}{1-z}$,why generating function  can be denoted as,A(z) = 1 + z + z^2 + \cdots \frac{1}{1-z},"It is easy to see that $1 + z + z^2 + \cdots$ is equal to $\frac{1}{1-z}$ when $1 > z > 0$ and for $z >= 1$, they are not equivalent. So I have thought $\frac{1}{1-z}$ is just a short for the more complex formula $1 + z + z^2 + \cdots$ and it has nothing to do with the value of the function. But later when I look at some materials about generating function (e.g. analytic combinatorics), I find that people also do some algebra operations on the generating function. For example, for the problem of how many trees are there with $N$ nodes, the generating function is $G(z) = z(1 + G(z) + G(z)^2 + \cdots)$ and thus $G(z)=\frac{z}{1-G(z)}$. I regard $\frac{1}{1-G(z)}$ is just a short of $1 + G(z) + G(z)^2 + \cdots$ here and all things look good. To get the closed form of $G(z)$, however, they transform $G(z)=\frac{z}{1-G(z)}$ to $G(z)-G(z)^2=z$, which is an algebra operation. We know that $1 + G(z) + G(z)^2 + \cdots$ and $\frac{1}{1-G(z)}$ are not necessarily the same. So is the solution after we solving $G(z)-G(z)^2=z$ exactly the $G(z)$ we expect?","It is easy to see that $1 + z + z^2 + \cdots$ is equal to $\frac{1}{1-z}$ when $1 > z > 0$ and for $z >= 1$, they are not equivalent. So I have thought $\frac{1}{1-z}$ is just a short for the more complex formula $1 + z + z^2 + \cdots$ and it has nothing to do with the value of the function. But later when I look at some materials about generating function (e.g. analytic combinatorics), I find that people also do some algebra operations on the generating function. For example, for the problem of how many trees are there with $N$ nodes, the generating function is $G(z) = z(1 + G(z) + G(z)^2 + \cdots)$ and thus $G(z)=\frac{z}{1-G(z)}$. I regard $\frac{1}{1-G(z)}$ is just a short of $1 + G(z) + G(z)^2 + \cdots$ here and all things look good. To get the closed form of $G(z)$, however, they transform $G(z)=\frac{z}{1-G(z)}$ to $G(z)-G(z)^2=z$, which is an algebra operation. We know that $1 + G(z) + G(z)^2 + \cdots$ and $\frac{1}{1-G(z)}$ are not necessarily the same. So is the solution after we solving $G(z)-G(z)^2=z$ exactly the $G(z)$ we expect?",,"['probability', 'sequences-and-series', 'combinatorics', 'number-theory', 'discrete-mathematics']"
73,"Green balls and Red balls, probability problem","Green balls and Red balls, probability problem",,"I'm studying for my exam and I came across the following draw without replacement  problem : $N$ boxes filled with red and green balls. The box $r$ contains $r-1$ red balls and $N-r$ green balls. We pick a box at random and we take $2$ random balls inside it, without putting them  back. 1) What is the probability that the second ball is green ? 2) What is a probability that the second ball is green, knowing the first one is green. I don't know where to start, all those dependance (to $r$ and $N$ ) are blowing my mind. I don't know if I should concider it as a Binomial law (with Bernoulli : ball = green, $n=2, p = ?$ ) or go with the formula $$p(X=k)=\frac{C_{m}^{k} C_{N-m}^{n-k}}{C_{N}^{n}}$$ or something else... Could someone advise me ?","I'm studying for my exam and I came across the following draw without replacement  problem : boxes filled with red and green balls. The box contains red balls and green balls. We pick a box at random and we take random balls inside it, without putting them  back. 1) What is the probability that the second ball is green ? 2) What is a probability that the second ball is green, knowing the first one is green. I don't know where to start, all those dependance (to and ) are blowing my mind. I don't know if I should concider it as a Binomial law (with Bernoulli : ball = green, ) or go with the formula or something else... Could someone advise me ?","N r r-1 N-r 2 r N n=2, p = ? p(X=k)=\frac{C_{m}^{k} C_{N-m}^{n-k}}{C_{N}^{n}}","['probability', 'combinatorics', 'statistics']"
74,Meaning of covariance,Meaning of covariance,,Can someone please give me an intuitive explanation for the meaning of covariance between two random variables? What does it measure?!,Can someone please give me an intuitive explanation for the meaning of covariance between two random variables? What does it measure?!,,"['probability', 'statistics', 'terminology']"
75,"Russian roulette, how many people left","Russian roulette, how many people left",,"I have a questian about a game similar to russian roulette. Suppose that we have n people in a room. Every round, everyone shoots a random person. So it can happen that everbody dies, or if everyone shoots the same person only two people die(the unlucky person and the person that hè shot).  I want to know what the expected number of surviving persons is after one round. I have no clue how to approach this problem.","I have a questian about a game similar to russian roulette. Suppose that we have n people in a room. Every round, everyone shoots a random person. So it can happen that everbody dies, or if everyone shoots the same person only two people die(the unlucky person and the person that hè shot).  I want to know what the expected number of surviving persons is after one round. I have no clue how to approach this problem.",,['probability']
76,Which of the two popular definitions of independent events is more primitive?,Which of the two popular definitions of independent events is more primitive?,,"I know there are two ways to say event $a$ and $b$ are independent: $P(a)P(b)=P(a\cap b)$ $P(a\mid b)=P(a)$ and I can derive one from the other with the Bayes Formula $P(a|b)=P(a\cap b)/P(b)$. My question is: Of the two equations above, which is the definition from which the other equation is proven?","I know there are two ways to say event $a$ and $b$ are independent: $P(a)P(b)=P(a\cap b)$ $P(a\mid b)=P(a)$ and I can derive one from the other with the Bayes Formula $P(a|b)=P(a\cap b)/P(b)$. My question is: Of the two equations above, which is the definition from which the other equation is proven?",,"['probability', 'bayesian']"
77,expected value of this markov chain,expected value of this markov chain,,"Question: A bag contains $3$ white chips and $3$ red chips. You repeatedly draw a chip at random from the bag. If it's white, you set it aside; if it's red, you put it back in the bag. After removing all $3$ white chips, you stop. What is the expected number of times you will draw from the bag? I believe this gives me the markov chain: [ $0.5$ , $0.5$ , $0$ , $0$ ] [ $0$ , $0.6$ , $0.4$ , $0$ ] [ $0$ , $0$ , $0.75$ , $0.25$ ] [ $0$ , $0$ , $0$ , $1$ ] But how am I supposed to find the expected number of times until getting to state $4$ ?","Question: A bag contains white chips and red chips. You repeatedly draw a chip at random from the bag. If it's white, you set it aside; if it's red, you put it back in the bag. After removing all white chips, you stop. What is the expected number of times you will draw from the bag? I believe this gives me the markov chain: [ , , , ] [ , , , ] [ , , , ] [ , , , ] But how am I supposed to find the expected number of times until getting to state ?",3 3 3 0.5 0.5 0 0 0 0.6 0.4 0 0 0 0.75 0.25 0 0 0 1 4,"['probability', 'expected-value', 'markov-chains']"
78,Solve the problem using Chebyshev inequality,Solve the problem using Chebyshev inequality,,The problem is the following: The symmetric coin is tossed 1600 times. What is the probability that the head will be shown up more than 1200 times? Attempt. Using the formula $\mathbb{P}(|X-MX|)>e)≤ DX/e^2$ I put the numbers in it $$\mathbb{P}(|X-800|>1200)\le 400/1200^2$$ But do not get the answer which is $\le 1/800$.,The problem is the following: The symmetric coin is tossed 1600 times. What is the probability that the head will be shown up more than 1200 times? Attempt. Using the formula $\mathbb{P}(|X-MX|)>e)≤ DX/e^2$ I put the numbers in it $$\mathbb{P}(|X-800|>1200)\le 400/1200^2$$ But do not get the answer which is $\le 1/800$.,,['probability']
79,Shop customers Poisson process,Shop customers Poisson process,,"People arrive at some shop as Poisson process of rate $N$. There are $N$ corridors in the shop and each customer chooses one at random, independently of the other customers. Now let $X_t^N$ be the proportion of corridors which remain empty at time $t$ and $T^N$ the time until half of the corridors have at least one customer, then it holds that $X_t^N\rightarrow e^{-t}, T^N\rightarrow \log 2$ for $N\rightarrow \infty$ in probability. Well I started step by step. Let $X$ be the RV that describes the number customers that arrived, then we know $P(X=k)=\frac{N^k e^{-N}}{N!}$. I denote each customer with a number so $P$(customer i chooses some corridor)$=\frac{1}{N}$. How can I continue?","People arrive at some shop as Poisson process of rate $N$. There are $N$ corridors in the shop and each customer chooses one at random, independently of the other customers. Now let $X_t^N$ be the proportion of corridors which remain empty at time $t$ and $T^N$ the time until half of the corridors have at least one customer, then it holds that $X_t^N\rightarrow e^{-t}, T^N\rightarrow \log 2$ for $N\rightarrow \infty$ in probability. Well I started step by step. Let $X$ be the RV that describes the number customers that arrived, then we know $P(X=k)=\frac{N^k e^{-N}}{N!}$. I denote each customer with a number so $P$(customer i chooses some corridor)$=\frac{1}{N}$. How can I continue?",,"['probability', 'probability-theory', 'probability-distributions']"
80,Probability question : meaning of the sentence,Probability question : meaning of the sentence,,"The following is the problem I am working on. The probability of a passing car being an import is defined as $p(i)=1/4$ and the probability of it being domestic is $p(d)=3/4$. Find the probability that there will be at least 2 imports passing before the 3rd domestic car. I am not quite understanding what they mean by the ""3rd"" domestic car. Opinion 1), The probability I am looking for is $$p(\text{2i's 2ds and last d})+p(\text{3i's 2d's and last d})+\cdots p(\text{n i's 2d's and last d}))+\cdots$$ Opinion 2), The ""3rd"" car is domestic $$p(\text{ddd})+p(\text{idd})+p(\text{did})+\cdots+p(()()d()()()\cdots)$$ But the book that I am working on explains as the only cases are $$iidd,idid,iddi,ddid,didi,ddii, iiid,iidi,idii, ddii, iiii$$ I am very confused because first of all, i don't see a ""3rd"" domestic car in any of these cases and why is it assuming that there are exactly four cars passing by ? Can someone explain me what is going on ?","The following is the problem I am working on. The probability of a passing car being an import is defined as $p(i)=1/4$ and the probability of it being domestic is $p(d)=3/4$. Find the probability that there will be at least 2 imports passing before the 3rd domestic car. I am not quite understanding what they mean by the ""3rd"" domestic car. Opinion 1), The probability I am looking for is $$p(\text{2i's 2ds and last d})+p(\text{3i's 2d's and last d})+\cdots p(\text{n i's 2d's and last d}))+\cdots$$ Opinion 2), The ""3rd"" car is domestic $$p(\text{ddd})+p(\text{idd})+p(\text{did})+\cdots+p(()()d()()()\cdots)$$ But the book that I am working on explains as the only cases are $$iidd,idid,iddi,ddid,didi,ddii, iiid,iidi,idii, ddii, iiii$$ I am very confused because first of all, i don't see a ""3rd"" domestic car in any of these cases and why is it assuming that there are exactly four cars passing by ? Can someone explain me what is going on ?",,"['probability', 'actuarial-science']"
81,Probability problem - fake and real diamonds -,Probability problem - fake and real diamonds -,,"A box contains 35 gems, of which 10 are real, 25 are fake. Gems are randomly taken out of the box, one at a time without replacement.  What is the probability that exactly 2 fakes are selected before the second real diamond is selected ? I was working on the above problem and I have 2 issues which I would appreciate if you could give me some input. 1), From looking at the problem, I was thinking that the following must be all the cases that I must work on. If I let $f=$ the event that the diamond is fake while $r=$ the event that the diamond is real, $rffr$ The first is real, the next two is fake and the second real shows up. $frffr$ The first is fake, the next is real, two fake shows and the second real shows up. ... $f...frffr$ The first 23 fake shows, a real shows, the rest of the two fake shows and the last one is inevitably real. I though that this is what it means to have ""exactly two fakes before the second real."" However, the solution to this problem describes it as ""exactly 2 fakes must be picked in the first 3 picks and the second real diamond must occur on the 4th pick"" and calculated $ffrr,frfr$ and $rffr$ as if the problem said to pick exactly 4 gems from the box. I would like to have a confirmation.  If the problem does not explicitly say that 4 gems were picked, was the way I approaching natural ? Or, do we normally assume how the solution tells us to do ? 2), Whether what I did was right or not, I still would like to know how to calculate it. I know that the probability must be, $$\frac{10*25*24*9}{35*34*33*32}+\frac{25*10*24*23*9}{35*34*33*32*31}+\cdots +\frac{25*24\cdots*310*2*1*9}{35*34*\cdots*9}$$ Which somewhat simplifies to $$90*\frac{25*24}{35*34*33*32}*\left(1+{23\over31}+\frac{23*22}{31*30}+\cdot+\frac{23*22*\cdot*1}{31*30*\cdot*9} \right)$$ I am not familiar with these type of series and it kind of looks like geometric but it is not.  Can someone guide me to where I can study this type of calculation ? What is it called ?","A box contains 35 gems, of which 10 are real, 25 are fake. Gems are randomly taken out of the box, one at a time without replacement.  What is the probability that exactly 2 fakes are selected before the second real diamond is selected ? I was working on the above problem and I have 2 issues which I would appreciate if you could give me some input. 1), From looking at the problem, I was thinking that the following must be all the cases that I must work on. If I let $f=$ the event that the diamond is fake while $r=$ the event that the diamond is real, $rffr$ The first is real, the next two is fake and the second real shows up. $frffr$ The first is fake, the next is real, two fake shows and the second real shows up. ... $f...frffr$ The first 23 fake shows, a real shows, the rest of the two fake shows and the last one is inevitably real. I though that this is what it means to have ""exactly two fakes before the second real."" However, the solution to this problem describes it as ""exactly 2 fakes must be picked in the first 3 picks and the second real diamond must occur on the 4th pick"" and calculated $ffrr,frfr$ and $rffr$ as if the problem said to pick exactly 4 gems from the box. I would like to have a confirmation.  If the problem does not explicitly say that 4 gems were picked, was the way I approaching natural ? Or, do we normally assume how the solution tells us to do ? 2), Whether what I did was right or not, I still would like to know how to calculate it. I know that the probability must be, $$\frac{10*25*24*9}{35*34*33*32}+\frac{25*10*24*23*9}{35*34*33*32*31}+\cdots +\frac{25*24\cdots*310*2*1*9}{35*34*\cdots*9}$$ Which somewhat simplifies to $$90*\frac{25*24}{35*34*33*32}*\left(1+{23\over31}+\frac{23*22}{31*30}+\cdot+\frac{23*22*\cdot*1}{31*30*\cdot*9} \right)$$ I am not familiar with these type of series and it kind of looks like geometric but it is not.  Can someone guide me to where I can study this type of calculation ? What is it called ?",,"['probability', 'actuarial-science']"
82,"Prove that $\int_{0}^1 u^{\alpha_1-1} (1-u)^{\alpha_2-1} \, {\rm d}u =\frac{\Gamma(\alpha_1)\Gamma(\alpha_2)} {\Gamma(\alpha_1+\alpha_2)}$",Prove that,"\int_{0}^1 u^{\alpha_1-1} (1-u)^{\alpha_2-1} \, {\rm d}u =\frac{\Gamma(\alpha_1)\Gamma(\alpha_2)} {\Gamma(\alpha_1+\alpha_2)}","I have the following equality in a textbook of mine $$\frac{y^{\alpha_1+\alpha_2-1} e^{-y/\beta}}{\Gamma(\alpha_1+\alpha_2) \beta^{\alpha_1+\alpha_2}} \cdot \frac{\Gamma(\alpha_1+\alpha_2)}{\Gamma(\alpha_1)\Gamma(\alpha_2)} \int_{0}^1 u^{\alpha_1-1} (1-u)^{\alpha_2-1} \, \mathrm du = \frac{y^{\alpha_1+\alpha_2-1} e^{-y/\beta}}{\Gamma(\alpha_1+\alpha_2) \beta^{\alpha_1+\alpha_2}}$$ and I see that for the equality to be true we must have $$\int_{0}^1 u^{\alpha_1-1} (1-u)^{\alpha_2-1} \, \mathrm du =\frac{\Gamma(\alpha_1)\Gamma(\alpha_2)} {\Gamma(\alpha_1+\alpha_2)}$$ However can someone give me an explanation why this is the case ?","I have the following equality in a textbook of mine $$\frac{y^{\alpha_1+\alpha_2-1} e^{-y/\beta}}{\Gamma(\alpha_1+\alpha_2) \beta^{\alpha_1+\alpha_2}} \cdot \frac{\Gamma(\alpha_1+\alpha_2)}{\Gamma(\alpha_1)\Gamma(\alpha_2)} \int_{0}^1 u^{\alpha_1-1} (1-u)^{\alpha_2-1} \, \mathrm du = \frac{y^{\alpha_1+\alpha_2-1} e^{-y/\beta}}{\Gamma(\alpha_1+\alpha_2) \beta^{\alpha_1+\alpha_2}}$$ and I see that for the equality to be true we must have $$\int_{0}^1 u^{\alpha_1-1} (1-u)^{\alpha_2-1} \, \mathrm du =\frac{\Gamma(\alpha_1)\Gamma(\alpha_2)} {\Gamma(\alpha_1+\alpha_2)}$$ However can someone give me an explanation why this is the case ?",,"['calculus', 'probability', 'integration', 'statistics']"
83,probability that two randomly chosen numbers are coprime [duplicate],probability that two randomly chosen numbers are coprime [duplicate],,This question already has answers here : Probability that two random numbers are coprime is $\frac{6}{\pi^2}$ (2 answers) Closed 2 years ago . Is this question well posed? See here for the solution Probability that two random numbers are coprime I have also seen it in some contests. The question asks to compute $p=\lim p_n$ where $p_n$ is the probability that two random chosen integers less than $n$ are coprime. There is no way to associate a uniform distribution to integers; so I would hesitate to call this limit a probability. So is there any rigorous way to understand this limit as a probability of some event? See also this post What's the mean of all real numbers? where it is mentioned (and I agree) that the mean of reals (or integers) is undefined. But one could in the same way define a uniform distribution for reals or integers with absolute value less than $x>0$ and take the limit of the mean as $x$ goes to infinity. Then the mean of reals would be $0$.,This question already has answers here : Probability that two random numbers are coprime is $\frac{6}{\pi^2}$ (2 answers) Closed 2 years ago . Is this question well posed? See here for the solution Probability that two random numbers are coprime I have also seen it in some contests. The question asks to compute $p=\lim p_n$ where $p_n$ is the probability that two random chosen integers less than $n$ are coprime. There is no way to associate a uniform distribution to integers; so I would hesitate to call this limit a probability. So is there any rigorous way to understand this limit as a probability of some event? See also this post What's the mean of all real numbers? where it is mentioned (and I agree) that the mean of reals (or integers) is undefined. But one could in the same way define a uniform distribution for reals or integers with absolute value less than $x>0$ and take the limit of the mean as $x$ goes to infinity. Then the mean of reals would be $0$.,,"['real-analysis', 'probability']"
84,Almost sure weak convergence of empirical measure,Almost sure weak convergence of empirical measure,,"Do empirical measures converge weakly to the measure almost surely? In particular suppose $\mu$ is a Borel probability measure on $\mathbb R^d$ and that $X_1,X_2,\dots$ are IID drawn from $\mu$. Let $\hat\mu_N=\frac{1}{N}\sum_{i=1}^N\delta_{X_i}$. The Strong Law of Large numbers Says that $\mathbb P(\hat\mu_N(A)\to\mu(A))=1$ for every $A$. Almost sure weak convergence, on the other hand, would be that $\mathbb P(\hat\mu_N(A)\to\mu(A)\text{ for every continuity set $A$})=1$. Is that statement true? Note that this is very different from the stronger statement $\mathbb P(\sup_{\text{continuity set $A$}}\left|{\hat\mu_N(A)-\mu(A)}\right|\to0)=1$, which would require continuity sets to constitute a Glivenco-Cantelli class, something that only happens in $d=1$ as far as I understand. Applying the Hewitt–Savage zero–one law ( https://en.wikipedia.org/wiki/Hewitt%E2%80%93Savage_zero-one_law ) it's clear that $\mathbb P(\hat\mu_N(A)\to\mu(A)\text{ for every continuity set $A$})$ is either $1$ or $0$ and nowhere in between. Can we argue it cannot be $0$? If this is not true in general are there reasonable sufficient conditions on $\mu$ that would guarantee almost sure weak convergence of $\hat\mu_N$? My feeling is that something like compactness of support should do it. Thanks.","Do empirical measures converge weakly to the measure almost surely? In particular suppose $\mu$ is a Borel probability measure on $\mathbb R^d$ and that $X_1,X_2,\dots$ are IID drawn from $\mu$. Let $\hat\mu_N=\frac{1}{N}\sum_{i=1}^N\delta_{X_i}$. The Strong Law of Large numbers Says that $\mathbb P(\hat\mu_N(A)\to\mu(A))=1$ for every $A$. Almost sure weak convergence, on the other hand, would be that $\mathbb P(\hat\mu_N(A)\to\mu(A)\text{ for every continuity set $A$})=1$. Is that statement true? Note that this is very different from the stronger statement $\mathbb P(\sup_{\text{continuity set $A$}}\left|{\hat\mu_N(A)-\mu(A)}\right|\to0)=1$, which would require continuity sets to constitute a Glivenco-Cantelli class, something that only happens in $d=1$ as far as I understand. Applying the Hewitt–Savage zero–one law ( https://en.wikipedia.org/wiki/Hewitt%E2%80%93Savage_zero-one_law ) it's clear that $\mathbb P(\hat\mu_N(A)\to\mu(A)\text{ for every continuity set $A$})$ is either $1$ or $0$ and nowhere in between. Can we argue it cannot be $0$? If this is not true in general are there reasonable sufficient conditions on $\mu$ that would guarantee almost sure weak convergence of $\hat\mu_N$? My feeling is that something like compactness of support should do it. Thanks.",,"['probability', 'measure-theory', 'probability-theory', 'convergence-divergence', 'weak-convergence']"
85,Probability of number of random answers being correct.,Probability of number of random answers being correct.,,"My teacher gave us a true-false test with 100 ""questions.""  Only there were no questions.  He had an answer key and was trying to prove that if we answered true and false questions randomly, the class average would be around 50%.  I got 7 right out of 100, so, of course, 93 wrong out of 100.  He said that he had never had a score that far from the mean. What is the probability of getting only 7 true-false questions out of 100 correct?","My teacher gave us a true-false test with 100 ""questions.""  Only there were no questions.  He had an answer key and was trying to prove that if we answered true and false questions randomly, the class average would be around 50%.  I got 7 right out of 100, so, of course, 93 wrong out of 100.  He said that he had never had a score that far from the mean. What is the probability of getting only 7 true-false questions out of 100 correct?",,['probability']
86,Chance of adjacent lockers with the same combination,Chance of adjacent lockers with the same combination,,"One weird thing that happened to me in high school was that the combination lock on my locker had the exact same combination as the locker next to it. It always struck me that the odds were crazy on this, but I never calculated it. The lock was a masterlock, the kind where you have 40 options for the first number, then 39 for the second (can't use the first number again), then 39 for the 3rd number (you can use the first number again for it). 1/59,280 chance that two locks have the same combination. (40 * 39 * 39) 1/29,640 chance that any specific combination is next to mine (w/2 lockers next to mine). That gives a 1 in (59,280 * 29,640) chance that a specific combination is next to mine? Or a 1/1,757,059,200 chance. This seems way too high, and there's probably something I'm missing here. Any thoughts on this calculation, or what it should be?","One weird thing that happened to me in high school was that the combination lock on my locker had the exact same combination as the locker next to it. It always struck me that the odds were crazy on this, but I never calculated it. The lock was a masterlock, the kind where you have 40 options for the first number, then 39 for the second (can't use the first number again), then 39 for the 3rd number (you can use the first number again for it). 1/59,280 chance that two locks have the same combination. (40 * 39 * 39) 1/29,640 chance that any specific combination is next to mine (w/2 lockers next to mine). That gives a 1 in (59,280 * 29,640) chance that a specific combination is next to mine? Or a 1/1,757,059,200 chance. This seems way too high, and there's probably something I'm missing here. Any thoughts on this calculation, or what it should be?",,['probability']
87,Probability of two rolling dice,Probability of two rolling dice,,"As I am taking some distance course on probability theory, I have no other option to ask my question than here. If they sound too basic, please bear me. Q: I roll two dice. What's the probability I get at least one six? A: P(first die is six) = $\frac{1}{6}$. P(second die is six)= $\frac{1}{6}$. P(both dice are six)= $\frac{1}{6} \cdot \frac{1}{6}=\frac{1}{36}$. P(at least one six)= $\frac{1}{6}+\frac{1}{6}-\frac{1}{36}=\frac{11}{36}=30.6\%$ This is what explained in the video. I think the question asked is ""at least one six"", so if I get six on both the dice, its ok for me, right? If that was the case, I need to add $\frac{1}{36}$ rather than subtracting, isn't it. Am I thinking right. Please clarify my doubt. So, the solution will be P(at least one six)= $\frac{1}{6}+\frac{1}{6}+\frac{1}{36}= 36.2 \%$","As I am taking some distance course on probability theory, I have no other option to ask my question than here. If they sound too basic, please bear me. Q: I roll two dice. What's the probability I get at least one six? A: P(first die is six) = $\frac{1}{6}$. P(second die is six)= $\frac{1}{6}$. P(both dice are six)= $\frac{1}{6} \cdot \frac{1}{6}=\frac{1}{36}$. P(at least one six)= $\frac{1}{6}+\frac{1}{6}-\frac{1}{36}=\frac{11}{36}=30.6\%$ This is what explained in the video. I think the question asked is ""at least one six"", so if I get six on both the dice, its ok for me, right? If that was the case, I need to add $\frac{1}{36}$ rather than subtracting, isn't it. Am I thinking right. Please clarify my doubt. So, the solution will be P(at least one six)= $\frac{1}{6}+\frac{1}{6}+\frac{1}{36}= 36.2 \%$",,"['probability', 'dice']"
88,Self study on probability and statitistics,Self study on probability and statitistics,,"i know there are similar questions already, but i specifically need a book that covers these topics: Combinatorics, conditional probability, Bayes theorem, random variables, joint probability, probability distributions, Markov Chains, Monte Carlo methods. I started with ""Understanding Probability"" by Henk Tijms but got a little frustrated along the way, because the author gives too much for granted. Should i keep using this book or can you suggest something better? Thank you.","i know there are similar questions already, but i specifically need a book that covers these topics: Combinatorics, conditional probability, Bayes theorem, random variables, joint probability, probability distributions, Markov Chains, Monte Carlo methods. I started with ""Understanding Probability"" by Henk Tijms but got a little frustrated along the way, because the author gives too much for granted. Should i keep using this book or can you suggest something better? Thank you.",,"['probability', 'statistics', 'reference-request']"
89,"What is the maximum entropy distribution for a continuous random variable on $[0,\infty)$ with given mean and variance?",What is the maximum entropy distribution for a continuous random variable on  with given mean and variance?,"[0,\infty)","I know that for a given logmean and logstdev its the lognormal, but what about where we directly specify the mean and variance? The above seems to depend on the log-transformation to the maxent for unbounded continuous RV with given mean and variance (i.e, Normal).","I know that for a given logmean and logstdev its the lognormal, but what about where we directly specify the mean and variance? The above seems to depend on the log-transformation to the maxent for unbounded continuous RV with given mean and variance (i.e, Normal).",,['probability']
90,Probability space proof,Probability space proof,,"PROBLEM Let $(\Omega, \mathcal{F}, P)$ be a probability space and let $(E_n)$ be a sequence in the $\sigma$-algebra $\mathcal{F}$. $a)$ If the sequence $(E_n)$ is increasing (in the sence that $E_n \subset E_{n+1}$) with limit $E = \cup_nE_n$, prove that $P(E_n) \rightarrow P(E)$ as $n \rightarrow \infty$ $b)$ If the sequence $(E_n)$ is decreasing with limit $E$, prove that $P(E_n) \rightarrow P(E_n)$ as $n \rightarrow \infty$ MY APPROACH $a)$ We know that $E_n \subset E_{n+1}$ so obviously $E_{n+1}$ consists of all sequences $E_i$ for $1 \leq i \leq n$. $b)$ We know that the sequence is decreasing so $E_{n+1} \subset E_n$, so $E_1$ consists of all sequences $E_i$ for $ 2 \leq i \leq n$ I don't know how to formulate these proofs properly. I hope someone could help me..","PROBLEM Let $(\Omega, \mathcal{F}, P)$ be a probability space and let $(E_n)$ be a sequence in the $\sigma$-algebra $\mathcal{F}$. $a)$ If the sequence $(E_n)$ is increasing (in the sence that $E_n \subset E_{n+1}$) with limit $E = \cup_nE_n$, prove that $P(E_n) \rightarrow P(E)$ as $n \rightarrow \infty$ $b)$ If the sequence $(E_n)$ is decreasing with limit $E$, prove that $P(E_n) \rightarrow P(E_n)$ as $n \rightarrow \infty$ MY APPROACH $a)$ We know that $E_n \subset E_{n+1}$ so obviously $E_{n+1}$ consists of all sequences $E_i$ for $1 \leq i \leq n$. $b)$ We know that the sequence is decreasing so $E_{n+1} \subset E_n$, so $E_1$ consists of all sequences $E_i$ for $ 2 \leq i \leq n$ I don't know how to formulate these proofs properly. I hope someone could help me..",,"['probability', 'measure-theory']"
91,Throwing dice: Probability Total is Divisible by 3,Throwing dice: Probability Total is Divisible by 3,,"Suppose I throw a fair die $1995$ times. What is the probability that   the total is divisible by 3? I tried to attack this problem inductively, storing the total in a variable $t \mod 6$, and then showing that for any total $t$ there is a $1/3$ probability of the next throw of the dice creating a total $t+n$ divisible by three. However, I've had difficulty formalizing the proof and I'm wondering if there isn't a better way to do this.","Suppose I throw a fair die $1995$ times. What is the probability that   the total is divisible by 3? I tried to attack this problem inductively, storing the total in a variable $t \mod 6$, and then showing that for any total $t$ there is a $1/3$ probability of the next throw of the dice creating a total $t+n$ divisible by three. However, I've had difficulty formalizing the proof and I'm wondering if there isn't a better way to do this.",,"['probability', 'combinatorics', 'discrete-mathematics', 'dice']"
92,"$P\{B^{2}-4AC\geq 0\}$ where $A,B,C \sim U(0,1)$?",where ?,"P\{B^{2}-4AC\geq 0\} A,B,C \sim U(0,1)","The actual problem is to find the probability that $Ax^{2}+Bx+C=0$ has real roots. This boils down to whether or not the discriminant $B^{2}-4AC$ is non-negative. Thus, we seek $P\{B^{2}-4AC\geq 0\}$. (Small note: $A,B,C$ are i.i.d.) Here is what I did so far: $$f_{A,B,C}(a,b,c) = f_{A}(a)f_{B}(b)f_{C}(c)=1 \,\,\, where \,\,\, 0<a,b,c< 1$$ Since $B^{2}-4AC\geq 0$ is equivalent to $B \geq 2\sqrt{AC}$, $$P\{B^{2}-4AC\geq 0\} = \int_{0}^{1} \int_{0}^{1} \int_{2\sqrt{ac}}^{1} f_{A,B,C}(a,b,c) \,db\,da\,dc$$ $$P\{B^{2}-4AC\geq 0\} = \int_{0}^{1} \int_{0}^{1} (1-2\sqrt{ac}) \,da\,dc = \frac{1}{9}$$ I evaluated the last part with wolfram alpha . However, my solution is different from another source, so I am not sure where I went wrong. Any insight would be highly appreciated!","The actual problem is to find the probability that $Ax^{2}+Bx+C=0$ has real roots. This boils down to whether or not the discriminant $B^{2}-4AC$ is non-negative. Thus, we seek $P\{B^{2}-4AC\geq 0\}$. (Small note: $A,B,C$ are i.i.d.) Here is what I did so far: $$f_{A,B,C}(a,b,c) = f_{A}(a)f_{B}(b)f_{C}(c)=1 \,\,\, where \,\,\, 0<a,b,c< 1$$ Since $B^{2}-4AC\geq 0$ is equivalent to $B \geq 2\sqrt{AC}$, $$P\{B^{2}-4AC\geq 0\} = \int_{0}^{1} \int_{0}^{1} \int_{2\sqrt{ac}}^{1} f_{A,B,C}(a,b,c) \,db\,da\,dc$$ $$P\{B^{2}-4AC\geq 0\} = \int_{0}^{1} \int_{0}^{1} (1-2\sqrt{ac}) \,da\,dc = \frac{1}{9}$$ I evaluated the last part with wolfram alpha . However, my solution is different from another source, so I am not sure where I went wrong. Any insight would be highly appreciated!",,"['probability', 'uniform-distribution']"
93,Proving that the lognormal distribution has no moment generating function,Proving that the lognormal distribution has no moment generating function,,"I need to prove that the lognormal distribution has no mgf, that is, $\int_0^\infty \frac{e^{tx}}{\sqrt{2\pi}x}e^\frac{{-(ln(x))^2}}{2} = \infty$. What is the best way to start this off?","I need to prove that the lognormal distribution has no mgf, that is, $\int_0^\infty \frac{e^{tx}}{\sqrt{2\pi}x}e^\frac{{-(ln(x))^2}}{2} = \infty$. What is the best way to start this off?",,['probability']
94,"Compute probability of winning a game with coins, without using series","Compute probability of winning a game with coins, without using series",,"I found the following question in a book with an answer. Question : You have two kinds of coins. The number of coin $A$ you have is $n$. The number of coin $B$ you have is $n+1$. When you throw all coins at the same time, calculate the probability such that the number of the-obverse-side-$B$s is larger than the number of the -obverse-side-$A$s. Answer : $1/2$. However, this book told us nothing about additional information except one sentence: ""There is a way to solve this question without using $\sum$"". I've tried to find it, but I'm facing difficulty. Then, here is my question. Question : Could you show me a way to solve above question without using $\sum$ ?","I found the following question in a book with an answer. Question : You have two kinds of coins. The number of coin $A$ you have is $n$. The number of coin $B$ you have is $n+1$. When you throw all coins at the same time, calculate the probability such that the number of the-obverse-side-$B$s is larger than the number of the -obverse-side-$A$s. Answer : $1/2$. However, this book told us nothing about additional information except one sentence: ""There is a way to solve this question without using $\sum$"". I've tried to find it, but I'm facing difficulty. Then, here is my question. Question : Could you show me a way to solve above question without using $\sum$ ?",,['probability']
95,Conditional density of Sum of two independent and continuous random variables,Conditional density of Sum of two independent and continuous random variables,,Let X and Y be independent and continuous random variables. How do you solve for the conditional density of X+Y given X,Let X and Y be independent and continuous random variables. How do you solve for the conditional density of X+Y given X,,['probability']
96,Normalizing a Gaussian Distribution,Normalizing a Gaussian Distribution,,"Assuming a Gaussian distribution with mean of zero and standard deviation of one, I would like to normalize this for an arbitrary mean and standard deviation. I know you're supposed to add the mean and multiply by the standard deviation. It's the multiplying by the standard deviation that I'm not seeing. I have a set of $2^{20}$ Gaussian-distributed random numbers generated with MatLab's randn() function. Let's call it matrix $A$. Suppose I have another matrix $B = 40 + 10A$. When I plot $A$ and $B$ in a histogram together, $B$ and $A$ have different widths as they should, but the same height, as shown in the images below. Normally (no pun intended) I would expect to see something like this, with different heights: But instead, I'm seeing something like this:","Assuming a Gaussian distribution with mean of zero and standard deviation of one, I would like to normalize this for an arbitrary mean and standard deviation. I know you're supposed to add the mean and multiply by the standard deviation. It's the multiplying by the standard deviation that I'm not seeing. I have a set of $2^{20}$ Gaussian-distributed random numbers generated with MatLab's randn() function. Let's call it matrix $A$. Suppose I have another matrix $B = 40 + 10A$. When I plot $A$ and $B$ in a histogram together, $B$ and $A$ have different widths as they should, but the same height, as shown in the images below. Normally (no pun intended) I would expect to see something like this, with different heights: But instead, I'm seeing something like this:",,"['probability', 'normal-distribution', 'matlab', 'random']"
97,probability of sample variance lying between values,probability of sample variance lying between values,,"Let $X_1,\ldots,X_n$ be a random sample of size $n = 10$ from a population which is normally distributed with mean $48$ and variance $36$ . What is the probability that the sample variance of such a sample lies between $25$ and $60$ ?",Let be a random sample of size from a population which is normally distributed with mean and variance . What is the probability that the sample variance of such a sample lies between and ?,"X_1,\ldots,X_n n = 10 48 36 25 60","['probability', 'statistics', 'normal-distribution', 'sampling']"
98,Bias/Nonbiased Probability Puzzle Question,Bias/Nonbiased Probability Puzzle Question,,I got this puzzle that I need help on.  I hope its not too easy because I really don't understand it. A bag contains 100 coins.  99 of them are fair and will give heads or tails with an equal probability.  1 biased coin will always yield heads.  I pull a coin out randomly from the bag and toss it up 3 times.  In each of the 3 times it lands on heads. What is the probability that I pulled out the biased coin?,I got this puzzle that I need help on.  I hope its not too easy because I really don't understand it. A bag contains 100 coins.  99 of them are fair and will give heads or tails with an equal probability.  1 biased coin will always yield heads.  I pull a coin out randomly from the bag and toss it up 3 times.  In each of the 3 times it lands on heads. What is the probability that I pulled out the biased coin?,,['probability']
99,Identically Distributed Uniform Variables U and 1-U,Identically Distributed Uniform Variables U and 1-U,,"In this post: Exercise regarding Poisson processes and the uniform distribution It is noted that U and 1-U are identically distributed for the r.v. U which is uniformly distributed on (0,1). If a definition for two random variables being identically distributed is: Random variables X and Y are identically distributed if  $ F_{X}(x) = F_{Y}(x)  $ for all x. Then let $ X = U$ and $ Y = 1 - U $. So $ F_{U}(u) = F_{1-U}(u) = Pr[U \le u ] = Pr[1-U \le u] $ I don't understand the last equality (if it is even correct). Isn't it that $1-U$ will be the complement of $U$ so the CDFs will be inverted?","In this post: Exercise regarding Poisson processes and the uniform distribution It is noted that U and 1-U are identically distributed for the r.v. U which is uniformly distributed on (0,1). If a definition for two random variables being identically distributed is: Random variables X and Y are identically distributed if  $ F_{X}(x) = F_{Y}(x)  $ for all x. Then let $ X = U$ and $ Y = 1 - U $. So $ F_{U}(u) = F_{1-U}(u) = Pr[U \le u ] = Pr[1-U \le u] $ I don't understand the last equality (if it is even correct). Isn't it that $1-U$ will be the complement of $U$ so the CDFs will be inverted?",,['probability']
