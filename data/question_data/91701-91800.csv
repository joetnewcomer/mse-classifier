,title_raw,title_text,title_latex,body_raw,body_text,body_latex,tags
0,"Books on complex analysis (Ahlfors, Conway and Lang)","Books on complex analysis (Ahlfors, Conway and Lang)",,"To make my question slightly different from others, I would like to know how would you rate on the complex analysis books by Ahlfors, Conway and Lang? I had a basic course on complex analysis during undergraduate (and you could imagine it's mostly about computing integrals and residues), and would like to learn more about the theory. There exist many good books, and the three books aforementioned are the ones I like the most. Of course I don't and won't have time to study all these three books in detail, so I have to pick one. The coverage of these books seem to be similar (except Conway's second volume, which should not be compared to others' single volume book). These three books contain rigorous proofs, so it's kind of hard to choose. Of course if you have read any two of them or all three of them you are very welcome to compare these books. If you ask me where am I headed to I would say I want to learn something about several complex variables. Also, if you think there is some book better than these three, you are welcome to mention it.","To make my question slightly different from others, I would like to know how would you rate on the complex analysis books by Ahlfors, Conway and Lang? I had a basic course on complex analysis during undergraduate (and you could imagine it's mostly about computing integrals and residues), and would like to learn more about the theory. There exist many good books, and the three books aforementioned are the ones I like the most. Of course I don't and won't have time to study all these three books in detail, so I have to pick one. The coverage of these books seem to be similar (except Conway's second volume, which should not be compared to others' single volume book). These three books contain rigorous proofs, so it's kind of hard to choose. Of course if you have read any two of them or all three of them you are very welcome to compare these books. If you ask me where am I headed to I would say I want to learn something about several complex variables. Also, if you think there is some book better than these three, you are welcome to mention it.",,"['complex-analysis', 'education', 'book-recommendation']"
1,When does a multivariate power series define an entire function?,When does a multivariate power series define an entire function?,,"In the single variable case, the power series $$\sum_{n=0}^\infty a_n z^n $$ defines an entire function, provided that $$R^{-1}:=\limsup_{n \to \infty} |a_n|^{1/n}=0. $$ Moreover, if $R^{-1} >0$, the series converges for $|z|<R$, and has some singularity on $|z|=R$. I'd like to know what happens in the case of $n \geq 2$ variables:  Using multi-index notation, a power series has the form $$\sum_{|\alpha| \geq 0} a_\alpha z^\alpha .$$ Is there a way to know, based on the coefficients $\{a_\alpha\}$, if the series defines an entire function? Is there any way to gain insights on the region of convergence at all? Thank you!","In the single variable case, the power series $$\sum_{n=0}^\infty a_n z^n $$ defines an entire function, provided that $$R^{-1}:=\limsup_{n \to \infty} |a_n|^{1/n}=0. $$ Moreover, if $R^{-1} >0$, the series converges for $|z|<R$, and has some singularity on $|z|=R$. I'd like to know what happens in the case of $n \geq 2$ variables:  Using multi-index notation, a power series has the form $$\sum_{|\alpha| \geq 0} a_\alpha z^\alpha .$$ Is there a way to know, based on the coefficients $\{a_\alpha\}$, if the series defines an entire function? Is there any way to gain insights on the region of convergence at all? Thank you!",,"['complex-analysis', 'entire-functions']"
2,"Entire ""periodic"" function","Entire ""periodic"" function",,I am studing for exams and am stuck on this problem. Suppose $f$ is an entire function s.t. $f(z) =f(z+1)$ and $|f(z)| < e^{|z|}$. Show $f$ is constant. I've deduced so far that: a) $f$ is bounded on every horizontal strip  b) for every bounded horizontal strip of length greater than 1 a maximum modulus must occur on a horizontal boundary.,I am studing for exams and am stuck on this problem. Suppose $f$ is an entire function s.t. $f(z) =f(z+1)$ and $|f(z)| < e^{|z|}$. Show $f$ is constant. I've deduced so far that: a) $f$ is bounded on every horizontal strip  b) for every bounded horizontal strip of length greater than 1 a maximum modulus must occur on a horizontal boundary.,,['complex-analysis']
3,Show that $\lim\limits_n \frac{a_n}{a_{n+1}} = z_0$,Show that,\lim\limits_n \frac{a_n}{a_{n+1}} = z_0,"I was wondering if anyone could give me a hint on the following problem: Let $f$ be meromorphic on the unit disc with only a simple pole at $0 \neq z_0 \in D$.  Let $a_0 +a_1z + \cdots$ be a power series representation for $f$ about $0$.  Show that $z_0 = \lim\limits_n \frac{a_n}{a_{n+1}}$ I have no idea what to do.  Since $z_0$ is the first place where $f$ isn't holomorphic, if the limit of $\frac{|a_n|}{|a_{n+1}|}$ exists, then this limit should be $|z_0|$.","I was wondering if anyone could give me a hint on the following problem: Let $f$ be meromorphic on the unit disc with only a simple pole at $0 \neq z_0 \in D$.  Let $a_0 +a_1z + \cdots$ be a power series representation for $f$ about $0$.  Show that $z_0 = \lim\limits_n \frac{a_n}{a_{n+1}}$ I have no idea what to do.  Since $z_0$ is the first place where $f$ isn't holomorphic, if the limit of $\frac{|a_n|}{|a_{n+1}|}$ exists, then this limit should be $|z_0|$.",,['complex-analysis']
4,A dodgy contour integration method giving the correct result,A dodgy contour integration method giving the correct result,,"Consider the following integral $$I = \int_{-\infty}^\infty \frac{x}{\sinh x}dx$$ Using contour integration with some rectangular contour, it is not too hard to show that the integral evaluates to $\pi^2/2$ . However, as shown in this reply , it is apparently possible to get the correct answer using a semi-circular contour in the upper half-plane, despite the fact that the integral over the contour technically doesn't vanish. As the radius of the semi-circular contour tends to infinity, there are an infinite number of poles contained within the contour with residues $i\pi n$ and $-i\pi n$ for $n$ is even and odd respectively. By assuming that the integral on the semi-circular contour vanishes, the residue theorem yields $$I = 2\pi i \sum_{n=0}^\infty (-1)^n (i\pi n) = -2\pi^2\sum_{n=0}^\infty (-1)^n n = \frac{\pi^2}{2}$$ if we consider that the last series goes to $-1/4$ . Now, despite the fact that the final answer is correct, the method used seems heretical for three reasons: Assuming the integral on the semi-circular contour vanishes when it clearly doesn't on the imaginary axis. Considering an infinite number of poles within the contour (although it's not too hard to accept that the residue theorem can also be used if the number of poles within a contour is countably infinite). Assuming that the last traditionally divergent series actually converges in this context. Why does this nevertheless yield the correct result? It's a safe assumption that it is not coincidental, since the entire family of integrals $$I(n) = \int_{-\infty}^\infty \left(\frac{x}{\sinh x}\right)^n dx$$ can seemingly be evaluated correctly by using this method, with the only difference being the type of divergent series at the end.","Consider the following integral Using contour integration with some rectangular contour, it is not too hard to show that the integral evaluates to . However, as shown in this reply , it is apparently possible to get the correct answer using a semi-circular contour in the upper half-plane, despite the fact that the integral over the contour technically doesn't vanish. As the radius of the semi-circular contour tends to infinity, there are an infinite number of poles contained within the contour with residues and for is even and odd respectively. By assuming that the integral on the semi-circular contour vanishes, the residue theorem yields if we consider that the last series goes to . Now, despite the fact that the final answer is correct, the method used seems heretical for three reasons: Assuming the integral on the semi-circular contour vanishes when it clearly doesn't on the imaginary axis. Considering an infinite number of poles within the contour (although it's not too hard to accept that the residue theorem can also be used if the number of poles within a contour is countably infinite). Assuming that the last traditionally divergent series actually converges in this context. Why does this nevertheless yield the correct result? It's a safe assumption that it is not coincidental, since the entire family of integrals can seemingly be evaluated correctly by using this method, with the only difference being the type of divergent series at the end.",I = \int_{-\infty}^\infty \frac{x}{\sinh x}dx \pi^2/2 i\pi n -i\pi n n I = 2\pi i \sum_{n=0}^\infty (-1)^n (i\pi n) = -2\pi^2\sum_{n=0}^\infty (-1)^n n = \frac{\pi^2}{2} -1/4 I(n) = \int_{-\infty}^\infty \left(\frac{x}{\sinh x}\right)^n dx,"['complex-analysis', 'contour-integration', 'divergent-series']"
5,"Maximum value $c$ s.t. $\exists$ a subset $S$ of $\{z_1,z_2,\ldots,z_n\}$ s.t. $\left|\sum_{z\in S}z\right|\geq c$ ($\sum_{i=1}^{n}|z_i|=1$).",Maximum value  s.t.  a subset  of  s.t.  ().,"c \exists S \{z_1,z_2,\ldots,z_n\} \left|\sum_{z\in S}z\right|\geq c \sum_{i=1}^{n}|z_i|=1","The original question is unclear so I completely rephrased the question and provided full context: This is the original question (from CMO 1986): Let $z_1,z_2,\cdots ,z_n$ be complex numbers satisfying $$|z_1|+|z_2|+\cdots +|z_n|=1.$$ Prove that there exists a subset $S$ of $\{z_1,z_2,\ldots,z_n\}$ such that $$\left|\sum_{z\in S}z\right|\geq\frac16.$$ But obviously the lower bound $\frac16$ is far less than optimal. For example, by simply using pigeonhole principle and the inequality $|z|\leq\Re(z)+\Im(z)$ it is easy to show that for every set of complex numbers $\{z_1,z_2,\ldots,z_n\}$ there always exists $S$ such that $\left|\sum_{z\in S}z\right|\geq\frac14$. But I wonder how to obtain the optimal value, i.e.: Let $z_1,z_2,\cdots ,z_n$ be complex numbers satisfying $$|z_1|+|z_2|+\cdots +|z_n|=1.$$ Find the maximum value $c$ such that for every set of complex numbers $\{z_1,z_2,\ldots,z_n\}$ satisfying the condition above, there always exists a subset $S$ of $\{z_1,z_2,\ldots,z_n\}$ such that $$\left|\sum_{z\in S}z\right|\geq c.$$ I read from the book 101 Algebra Problems from the Training of the USA IMO Team (p85) that ""Using advanced mathematics, the lower bound can be further improved to $\frac1{\pi}$."" But how exactly do I obtain that result?","The original question is unclear so I completely rephrased the question and provided full context: This is the original question (from CMO 1986): Let $z_1,z_2,\cdots ,z_n$ be complex numbers satisfying $$|z_1|+|z_2|+\cdots +|z_n|=1.$$ Prove that there exists a subset $S$ of $\{z_1,z_2,\ldots,z_n\}$ such that $$\left|\sum_{z\in S}z\right|\geq\frac16.$$ But obviously the lower bound $\frac16$ is far less than optimal. For example, by simply using pigeonhole principle and the inequality $|z|\leq\Re(z)+\Im(z)$ it is easy to show that for every set of complex numbers $\{z_1,z_2,\ldots,z_n\}$ there always exists $S$ such that $\left|\sum_{z\in S}z\right|\geq\frac14$. But I wonder how to obtain the optimal value, i.e.: Let $z_1,z_2,\cdots ,z_n$ be complex numbers satisfying $$|z_1|+|z_2|+\cdots +|z_n|=1.$$ Find the maximum value $c$ such that for every set of complex numbers $\{z_1,z_2,\ldots,z_n\}$ satisfying the condition above, there always exists a subset $S$ of $\{z_1,z_2,\ldots,z_n\}$ such that $$\left|\sum_{z\in S}z\right|\geq c.$$ I read from the book 101 Algebra Problems from the Training of the USA IMO Team (p85) that ""Using advanced mathematics, the lower bound can be further improved to $\frac1{\pi}$."" But how exactly do I obtain that result?",,"['complex-analysis', 'analysis', 'complex-numbers']"
6,Existence of some sort of 'infinite algebraicity' of transcendental numbers,Existence of some sort of 'infinite algebraicity' of transcendental numbers,,"Given an arbitrary number, say, $\alpha \in \mathbb{C}$, can anyone supply either (a) a reason for the existence (or general non-existence) of, or (b) the reverse engineering of a (convergent) power series $f(z) = \sum a_n z^n$ with rational coefficients for which $f(\alpha) = 0$? (This problem is trivial when the number in question is algebraic, so feel free to assume transcendentality.) I'm looking for a push in the right direction; any input is welcome. -- prototypical example: $\alpha = \pi$ with $f(z) = \sum \frac{1}{(2n+1)!} z^{2n+1}$ = sin z -- I imagine that this question could easily be recast in several different directions. Please add any appropriate tags if you suspect this too.","Given an arbitrary number, say, $\alpha \in \mathbb{C}$, can anyone supply either (a) a reason for the existence (or general non-existence) of, or (b) the reverse engineering of a (convergent) power series $f(z) = \sum a_n z^n$ with rational coefficients for which $f(\alpha) = 0$? (This problem is trivial when the number in question is algebraic, so feel free to assume transcendentality.) I'm looking for a push in the right direction; any input is welcome. -- prototypical example: $\alpha = \pi$ with $f(z) = \sum \frac{1}{(2n+1)!} z^{2n+1}$ = sin z -- I imagine that this question could easily be recast in several different directions. Please add any appropriate tags if you suspect this too.",,"['number-theory', 'analysis', 'complex-analysis']"
7,Different methods to prove $\zeta(s)=2^s\pi^{s-1}\sin\left(\frac{s\pi}{2}\right) \Gamma (1-s) \zeta (1-s)$.,Different methods to prove .,\zeta(s)=2^s\pi^{s-1}\sin\left(\frac{s\pi}{2}\right) \Gamma (1-s) \zeta (1-s),"I've recently encountered this strangely attractive equation (Riemann's functional equation), along with Riemann's original proof.   $$\displaystyle\zeta(s)=2^s\pi^{s-1}\sin\left(\frac{s\pi}{2}\right) \Gamma (1-s) \zeta (1-s)$$ There are probably countless proofs of Riemann's functional equation, but as of yet there's not a single place where they're all concentrated. Anyone care to share some particularly pretty proofs? $\zeta(s)$: Riemann zeta function. $\Gamma(s)$: Gamma function.","I've recently encountered this strangely attractive equation (Riemann's functional equation), along with Riemann's original proof.   $$\displaystyle\zeta(s)=2^s\pi^{s-1}\sin\left(\frac{s\pi}{2}\right) \Gamma (1-s) \zeta (1-s)$$ There are probably countless proofs of Riemann's functional equation, but as of yet there's not a single place where they're all concentrated. Anyone care to share some particularly pretty proofs? $\zeta(s)$: Riemann zeta function. $\Gamma(s)$: Gamma function.",,"['complex-analysis', 'special-functions', 'functional-equations', 'big-list', 'riemann-zeta']"
8,Hints for a complex limit: Prove if $\lim_{z \to \infty} f(z)/z = 0$ then $f(z)$ is constant.,Hints for a complex limit: Prove if  then  is constant.,\lim_{z \to \infty} f(z)/z = 0 f(z),"(To clarify, I would just like a hint . Please do not give me the answer to this problem. ) The solution to the following problem has really evaded me here: Problem: Assume that $f$ is entire and that $\lim_{z \to \infty} f(z)/z  = 0.$ Prove that $f(z)$ is constant. My Thoughts and Work So Far: We know that proving that $f'(z) =  0$ or that for some fixed $c \in \mathbb{C}$, $f(z) = c$ for all $z\in \mathbb{C}$. My first approach was to use Liouville's Theorem; If I could could show that $f$ is bounded then I am done. Since $\lim_{z \to \infty} f(z)/z  = 0$, for all $\varepsilon > 0$ there exists a $N \in \mathbb{C}$ so large that if $z \geq N$ then $|f(z)/z| \leq \varepsilon$. Thus, if $C_R$ is the circle of radius $R$ centered at the point $z$, then, as long as z is large enough by Cauchy's Inequality $$ |f'(z)| \leq \frac{1}{2\pi i} \oint_{C_R} \frac{f(\zeta)}{(\zeta - z)^2}\ d\zeta \leq \bigg | \frac{1}{2\pi i} \bigg | \oint_{C_R} \bigg | \frac{f(\zeta)}{(\zeta - z)^2} \bigg | d \zeta \leq \frac{1}{2\pi}  \frac{|\zeta|\varepsilon 2\pi R}{R^2} = \frac{|\zeta|\varepsilon}{R}.  $$ Now taking the limit as $R \to \infty$ (which to me says, ""let our circle around our point z dilate to an infinite radius so that it covers all of $\mathbb{C}$)  $$|f'(z)| \leq \lim_{R \to \infty} \frac{|\zeta|\varepsilon}{R} = 0.$$ Thus $f'(z) = 0$ and $z$ was arbitrary, so $f$ must be constant. Why I Think Im Wrong : I say $z$ was arbitrary, but really it is ""any $z \geq N$"" which really isn't all that arbitrary. This is where I am stuck. Am I right, wrong, close, or totally lost? Any hints would be great. Edit: I am very sorry but I accidentally posted this before I was done typing the problem.","(To clarify, I would just like a hint . Please do not give me the answer to this problem. ) The solution to the following problem has really evaded me here: Problem: Assume that $f$ is entire and that $\lim_{z \to \infty} f(z)/z  = 0.$ Prove that $f(z)$ is constant. My Thoughts and Work So Far: We know that proving that $f'(z) =  0$ or that for some fixed $c \in \mathbb{C}$, $f(z) = c$ for all $z\in \mathbb{C}$. My first approach was to use Liouville's Theorem; If I could could show that $f$ is bounded then I am done. Since $\lim_{z \to \infty} f(z)/z  = 0$, for all $\varepsilon > 0$ there exists a $N \in \mathbb{C}$ so large that if $z \geq N$ then $|f(z)/z| \leq \varepsilon$. Thus, if $C_R$ is the circle of radius $R$ centered at the point $z$, then, as long as z is large enough by Cauchy's Inequality $$ |f'(z)| \leq \frac{1}{2\pi i} \oint_{C_R} \frac{f(\zeta)}{(\zeta - z)^2}\ d\zeta \leq \bigg | \frac{1}{2\pi i} \bigg | \oint_{C_R} \bigg | \frac{f(\zeta)}{(\zeta - z)^2} \bigg | d \zeta \leq \frac{1}{2\pi}  \frac{|\zeta|\varepsilon 2\pi R}{R^2} = \frac{|\zeta|\varepsilon}{R}.  $$ Now taking the limit as $R \to \infty$ (which to me says, ""let our circle around our point z dilate to an infinite radius so that it covers all of $\mathbb{C}$)  $$|f'(z)| \leq \lim_{R \to \infty} \frac{|\zeta|\varepsilon}{R} = 0.$$ Thus $f'(z) = 0$ and $z$ was arbitrary, so $f$ must be constant. Why I Think Im Wrong : I say $z$ was arbitrary, but really it is ""any $z \geq N$"" which really isn't all that arbitrary. This is where I am stuck. Am I right, wrong, close, or totally lost? Any hints would be great. Edit: I am very sorry but I accidentally posted this before I was done typing the problem.",,['complex-analysis']
9,Property of Entire Functions,Property of Entire Functions,,Suppose $f$ and $g$ are entire functions with $|f(z)|\leq|g(z)|$ for all $z$. How can we show that $f=cg$ for some complex constant $c$? Thanks for any help :),Suppose $f$ and $g$ are entire functions with $|f(z)|\leq|g(z)|$ for all $z$. How can we show that $f=cg$ for some complex constant $c$? Thanks for any help :),,['complex-analysis']
10,What's the intuition behind the identities $\cos(z)= \cosh(iz)$ and $\sin(z)=-i\sinh(iz)$?,What's the intuition behind the identities  and ?,\cos(z)= \cosh(iz) \sin(z)=-i\sinh(iz),"I'm trying to understand in an intuitive manner the relationship between the circular and hyperbolic functions in the complex plane, i.e.: $$\cos(z)= \cosh(iz)$$ $$\sin(z)=-i\sinh(iz)$$ where $z$ is a complex number. From a geometric point of view, what I understand is that cos is the composition of a rotation through $\frac{\pi}{2}$, followed by cosh, and sin is the composition of a rotation through $\frac{\pi}{2}$, followed by sinh, followed by a rotation through $-\frac{\pi}{2}$ (where sin, cos, sinh, cosh are defined as complex functions). Where does this connection come from? Is there some way it can be visualized in terms of complex mappings? (I'm not asking for a proof of the identities, I already know one).","I'm trying to understand in an intuitive manner the relationship between the circular and hyperbolic functions in the complex plane, i.e.: $$\cos(z)= \cosh(iz)$$ $$\sin(z)=-i\sinh(iz)$$ where $z$ is a complex number. From a geometric point of view, what I understand is that cos is the composition of a rotation through $\frac{\pi}{2}$, followed by cosh, and sin is the composition of a rotation through $\frac{\pi}{2}$, followed by sinh, followed by a rotation through $-\frac{\pi}{2}$ (where sin, cos, sinh, cosh are defined as complex functions). Where does this connection come from? Is there some way it can be visualized in terms of complex mappings? (I'm not asking for a proof of the identities, I already know one).",,"['complex-analysis', 'functions', 'trigonometry', 'intuition', 'visualization']"
11,Is there an analytic function $f : \mathbb{D} → \mathbb{D}$ with $f(0) = 1/2$ and $f′(0) = 3/4?$,Is there an analytic function  with  and,f : \mathbb{D} → \mathbb{D} f(0) = 1/2 f′(0) = 3/4?,"(a) Let $\mathbb{D}$ denote the unit disk. Is there an analytic function $f \colon \mathbb{D} \to \mathbb{D}$ with $f(0) = 1/2$ and $f′(0) = 3/4?$ Either find such a function $f$ or explain why it does not exist. (b) Answer the same question for $f(0) = 1/2$ and $f′(0) = 4/5.$ It seems like I could use the Schwarz lemma, but that is not working out so well.  Any suggestions?  Thanks.","(a) Let $\mathbb{D}$ denote the unit disk. Is there an analytic function $f \colon \mathbb{D} \to \mathbb{D}$ with $f(0) = 1/2$ and $f′(0) = 3/4?$ Either find such a function $f$ or explain why it does not exist. (b) Answer the same question for $f(0) = 1/2$ and $f′(0) = 4/5.$ It seems like I could use the Schwarz lemma, but that is not working out so well.  Any suggestions?  Thanks.",,['complex-analysis']
12,$\int_{0}^{\infty}\frac{dx}{1+x^n}$,,\int_{0}^{\infty}\frac{dx}{1+x^n},"My goal is to evaluate $$\int_{0}^{\infty}\frac{dx}{1+x^n}\;\;(n\in\mathbb{N},n\geq2).$$ Here is my approach: Clearly, the integral converges. Denote the value of the integral by $I_n$. Now let $\gamma_R$ describe the section of a circle which goes from the origin to $R$ to $Re^{2\pi i/n}$ and back to the origin. If we let $C_R$ denote the relevant circular arc, then  $$\left|\int_{C_R}\frac{dz}{1+z^n}\right|\leq \left(\frac{2\pi R}{n}\right)\left(\frac{1}{R^{n}-1}\right)\rightarrow0\;\;\;as\;\;R\rightarrow\infty.$$ Furthermore, $$\int_{[R,Re^{2\pi i/n}]}\frac{dz}{1+z^n}=\int_{R}^{0}\frac{e^{2\pi i/n}dr}{1+r^n}.$$ Hence $$\lim_{R\rightarrow\infty}\int_{\gamma_R}\frac{dz}{1+z^n}=\lim_{R\rightarrow\infty}\int_{[0,R]}\frac{dx}{1+x^n}+\int_{[R,Re^{2\pi i/n}]}\frac{dx}{1+x^n}+\int_{C_R}\frac{dx}{1+x^n}=(1-e^{2\pi i/n})I_n\;\;\;(1).$$ Thus if we can obtain the value of $\int_{\gamma_R}\frac{dz}{1+z^n}$ we can evaluate $I_n$. Now the zeroes of $1+z^n$ are of the form $z=e^{i\pi/n+2\pi  i m/n}\;\;(m\in\mathbb{N})$ from which it is clear that the only zero which lies within the contour occurs at $z=e^{i\pi/n}$ with multiplicity 1. So all that remains to be done is to evaluate the residue of $\frac{1}{1+z^n}$ at $z=e^{i\pi/n}$. However, if $z=e^{i\pi/n}u$ and $u\neq1$, we have $$\frac{z^n+1}{z-e^{i\pi/n}}=\frac{1-u^n}{-e^{i\pi/n}(1-u)} =-e^{-i\pi/n}\sum_{m=0}^{n-1}u^m\;\;\;(2).$$ In particular, (2) implies $$Res_{z=e^{i\pi/n}}\frac{1}{1+z^n}=-\frac{e^{i\pi/n}}{n}\;\;\;(3).$$ Finally, (1) and (3) imply  $$I_n=\frac{2\pi i (Res_{z=e^{i\pi/n}}\frac{1}{1+z^n})}{1-e^{2\pi i/n}}=\frac{-2\pi ie^{i\pi/n}}{n(1-e^{2\pi i/n})}=\frac{\pi/n}{\sin(\pi/n)}.$$ I have three questions: One, is my method correct? Two, is there a simpler/different method to evaluate the integral? Three, is there an easier way to evaluate the residue of $\frac{1}{1+z^4}$ at $z=e^{i\pi/n}$?","My goal is to evaluate $$\int_{0}^{\infty}\frac{dx}{1+x^n}\;\;(n\in\mathbb{N},n\geq2).$$ Here is my approach: Clearly, the integral converges. Denote the value of the integral by $I_n$. Now let $\gamma_R$ describe the section of a circle which goes from the origin to $R$ to $Re^{2\pi i/n}$ and back to the origin. If we let $C_R$ denote the relevant circular arc, then  $$\left|\int_{C_R}\frac{dz}{1+z^n}\right|\leq \left(\frac{2\pi R}{n}\right)\left(\frac{1}{R^{n}-1}\right)\rightarrow0\;\;\;as\;\;R\rightarrow\infty.$$ Furthermore, $$\int_{[R,Re^{2\pi i/n}]}\frac{dz}{1+z^n}=\int_{R}^{0}\frac{e^{2\pi i/n}dr}{1+r^n}.$$ Hence $$\lim_{R\rightarrow\infty}\int_{\gamma_R}\frac{dz}{1+z^n}=\lim_{R\rightarrow\infty}\int_{[0,R]}\frac{dx}{1+x^n}+\int_{[R,Re^{2\pi i/n}]}\frac{dx}{1+x^n}+\int_{C_R}\frac{dx}{1+x^n}=(1-e^{2\pi i/n})I_n\;\;\;(1).$$ Thus if we can obtain the value of $\int_{\gamma_R}\frac{dz}{1+z^n}$ we can evaluate $I_n$. Now the zeroes of $1+z^n$ are of the form $z=e^{i\pi/n+2\pi  i m/n}\;\;(m\in\mathbb{N})$ from which it is clear that the only zero which lies within the contour occurs at $z=e^{i\pi/n}$ with multiplicity 1. So all that remains to be done is to evaluate the residue of $\frac{1}{1+z^n}$ at $z=e^{i\pi/n}$. However, if $z=e^{i\pi/n}u$ and $u\neq1$, we have $$\frac{z^n+1}{z-e^{i\pi/n}}=\frac{1-u^n}{-e^{i\pi/n}(1-u)} =-e^{-i\pi/n}\sum_{m=0}^{n-1}u^m\;\;\;(2).$$ In particular, (2) implies $$Res_{z=e^{i\pi/n}}\frac{1}{1+z^n}=-\frac{e^{i\pi/n}}{n}\;\;\;(3).$$ Finally, (1) and (3) imply  $$I_n=\frac{2\pi i (Res_{z=e^{i\pi/n}}\frac{1}{1+z^n})}{1-e^{2\pi i/n}}=\frac{-2\pi ie^{i\pi/n}}{n(1-e^{2\pi i/n})}=\frac{\pi/n}{\sin(\pi/n)}.$$ I have three questions: One, is my method correct? Two, is there a simpler/different method to evaluate the integral? Three, is there an easier way to evaluate the residue of $\frac{1}{1+z^4}$ at $z=e^{i\pi/n}$?",,['complex-analysis']
13,Solve $\sin(z) = 2$,Solve,\sin(z) = 2,"There are a number of solutions to this problem online that use identities I have not been taught. Here is where I am in relation to my own coursework: $ \sin(z) = 2 $ $ \exp(iz) - \exp(-iz) = 4i $ $ \exp(2iz) - 1 = 4i \cdot \exp (iz) $ Then, setting $w = \exp(iz),$ I get: $ w^2 - 4iw -1 = 0$ I can then use the quadratic equation to find: $ w = i(2 \pm \sqrt 3 )$ So therefore, $\exp(iz) = w = i(2 \pm \sqrt 3 ) $ implies $ e^{-y}\cos(x) = 0   $, thus $ x = \frac{\pi}{2} $ $ ie^{-y}\sin(x) = i(2 \pm \sqrt 3 ) $ so $ y = -\ln( 2 \pm \sqrt 3 ) $ So I have come up with $ z = \frac{\pi}{2}  - i  \ln( 2 \pm \sqrt 3 )$ But the back of the book has $ z = \frac{\pi}{2}  \pm i  \ln( 2 + \sqrt 3 ) +2n\pi$ Now, the $+2n\pi$ I understand because sin is periodic, but how did the plus/minus come out of the natural log? There is no identity for $\ln(a+b)$ that I am aware of. I believe I screwed up something in the calculations, but for the life of me cannot figure out what. If someone could point me in the right direction, I would appreciate it.","There are a number of solutions to this problem online that use identities I have not been taught. Here is where I am in relation to my own coursework: $ \sin(z) = 2 $ $ \exp(iz) - \exp(-iz) = 4i $ $ \exp(2iz) - 1 = 4i \cdot \exp (iz) $ Then, setting $w = \exp(iz),$ I get: $ w^2 - 4iw -1 = 0$ I can then use the quadratic equation to find: $ w = i(2 \pm \sqrt 3 )$ So therefore, $\exp(iz) = w = i(2 \pm \sqrt 3 ) $ implies $ e^{-y}\cos(x) = 0   $, thus $ x = \frac{\pi}{2} $ $ ie^{-y}\sin(x) = i(2 \pm \sqrt 3 ) $ so $ y = -\ln( 2 \pm \sqrt 3 ) $ So I have come up with $ z = \frac{\pi}{2}  - i  \ln( 2 \pm \sqrt 3 )$ But the back of the book has $ z = \frac{\pi}{2}  \pm i  \ln( 2 + \sqrt 3 ) +2n\pi$ Now, the $+2n\pi$ I understand because sin is periodic, but how did the plus/minus come out of the natural log? There is no identity for $\ln(a+b)$ that I am aware of. I believe I screwed up something in the calculations, but for the life of me cannot figure out what. If someone could point me in the right direction, I would appreciate it.",,"['complex-analysis', 'complex-numbers']"
14,Good resource/exercises for learning asymptotic analysis?,Good resource/exercises for learning asymptotic analysis?,,"I am studying asymptotic methods right now; things such as mellin transform, inverse mellin transform, saddle point method, laplaces method, etc... and I get very frustrated because I can't get very far through the proofs without getting stuck.  It usually involves some approximation of some sort that I fail to see why it works. I've had graduate level complex and real analysis, but not a whole lot of practice in applying bounding/approximation techniques.  I have tried picking up books on asymptotic analysis that begin very basic: big oh, little oh, etc... but the exercises quickly become non-trivial.  I just need practice bounding and approximating things, and applying these things to contour integrals and the other methods I've mentioned. A resource, roadmap, book, etc... any advice you can offer would be great.  I just want to be able to have the background assumed in some of these expositions on asymptotic methods...and I don't see how to get it.","I am studying asymptotic methods right now; things such as mellin transform, inverse mellin transform, saddle point method, laplaces method, etc... and I get very frustrated because I can't get very far through the proofs without getting stuck.  It usually involves some approximation of some sort that I fail to see why it works. I've had graduate level complex and real analysis, but not a whole lot of practice in applying bounding/approximation techniques.  I have tried picking up books on asymptotic analysis that begin very basic: big oh, little oh, etc... but the exercises quickly become non-trivial.  I just need practice bounding and approximating things, and applying these things to contour integrals and the other methods I've mentioned. A resource, roadmap, book, etc... any advice you can offer would be great.  I just want to be able to have the background assumed in some of these expositions on asymptotic methods...and I don't see how to get it.",,"['complex-analysis', 'reference-request', 'asymptotics', 'computer-science', 'book-recommendation']"
15,"For fixed $z_i$s inside the unit disc, can we always choose $a_i$s such that $\left|\sum_{i=1}^n a_iz_i\right|<\sqrt3$?","For fixed s inside the unit disc, can we always choose s such that ?",z_i a_i \left|\sum_{i=1}^n a_iz_i\right|<\sqrt3,"Let $z_1,z_2,\ldots,z_n$ be complex number such that $|z_i|<1$ for all $i=1,2,\ldots,n$. Show that we can choose $a_i \in\{-1,1\}$, $i=1,2,\ldots,n$ such that $$\left|\sum_{i=1}^n a_iz_i\right|<\sqrt3.$$","Let $z_1,z_2,\ldots,z_n$ be complex number such that $|z_i|<1$ for all $i=1,2,\ldots,n$. Show that we can choose $a_i \in\{-1,1\}$, $i=1,2,\ldots,n$ such that $$\left|\sum_{i=1}^n a_iz_i\right|<\sqrt3.$$",,['complex-analysis']
16,Interpolating the primorial $p_{n}\#$,Interpolating the primorial,p_{n}\#,"The primorial $p_{n}\#$ is given by the product $p_n\# = \prod_{k=1}^n p_k$ (where $p_{k}$ is the $k$th prime) -- is there a natural (a la the gamma function $\Gamma(z)$) way of interpolating it for arguments not necessarily a natural number? (or in $\mathbb{C}$?) I tried starting with the following definition of the gamma function: $$\Gamma(z) = \lim_{n \to \infty} \frac{n! \; n^z}{z \; (z+1)\cdots(z+n)} = \frac{1}{z} \prod_{n=1}^\infty \frac{\left(1+\frac{1}{n}\right)^z}{1+\frac{z}{n}}$$ My first thought was to modify the Pochhammer symbol in the denominator: $$\Gamma_{?}(z) = \lim_{n \to \infty} \frac{p_{n}\# \; p_{n}^z}{z \; (z+p_{1})\cdots(z+p_{n})}$$ But this clearly doesn't work, because the primes aren't regularly spaced.","The primorial $p_{n}\#$ is given by the product $p_n\# = \prod_{k=1}^n p_k$ (where $p_{k}$ is the $k$th prime) -- is there a natural (a la the gamma function $\Gamma(z)$) way of interpolating it for arguments not necessarily a natural number? (or in $\mathbb{C}$?) I tried starting with the following definition of the gamma function: $$\Gamma(z) = \lim_{n \to \infty} \frac{n! \; n^z}{z \; (z+1)\cdots(z+n)} = \frac{1}{z} \prod_{n=1}^\infty \frac{\left(1+\frac{1}{n}\right)^z}{1+\frac{z}{n}}$$ My first thought was to modify the Pochhammer symbol in the denominator: $$\Gamma_{?}(z) = \lim_{n \to \infty} \frac{p_{n}\# \; p_{n}^z}{z \; (z+p_{1})\cdots(z+p_{n})}$$ But this clearly doesn't work, because the primes aren't regularly spaced.",,"['complex-analysis', 'prime-numbers', 'analytic-number-theory', 'interpolation']"
17,"Definition of complex differential forms of bidegree $(p,q)$",Definition of complex differential forms of bidegree,"(p,q)","Let $M$ be a complex manifold of dimension $n$. It means that $M$ is a real smooth smooth manifold of dimension $2n$. Suppose that the real tangent bundle of $M$ has a local basis: $$\left\{\frac{\partial }{\partial x_1},\ldots,\frac{\partial}{\partial x_n},\frac{\partial }{\partial y_1},\ldots,\frac{\partial}{\partial y_n}\right\}$$ and the real cotangent bundle has local basis $$\left\{dx_1,\ldots,dx_n,dy_1,\ldots dy_n\right\}$$ Then we put $$\frac{\partial}{\partial z_j}:=\frac{1}{2}\left(\frac{\partial }{\partial x_j}-i \frac{\partial }{\partial y_j}\right)$$  $$\frac{\partial}{\partial \bar z_j}:=\frac{1}{2}\left(\frac{\partial }{\partial x_j}+i \frac{\partial }{\partial y_j}\right)$$ $$dz_j:=dx_j+idy_j$$ $$d\bar z_j:=dx_j-idy_j$$ Now consider the complexified cotangent bundle $(T^\ast M)_{\mathbb C}$, it has a decomposition: $$(T^\ast M)_\mathbb C:=T^\ast M^{1,0}\oplus T^\ast M^{0.1}$$ where $$T^\ast M^{(1,0)}=\left<dz_j:j=1,\ldots n\right>$$ $$T^\ast M^{(1,0)}=\left<d\bar z_j:j=1,\ldots n\right>\,.$$ At this point one defines the algebra of differential $(p,q)$-forms on $M$ as: $$\bigwedge^{p,q}M:=\bigwedge^{p}T^\ast M^{1,0}\otimes \bigwedge^{q}T^\ast M^{0,1}$$ So locally any $(p,q)$-differential form can be written as $$\omega=\sum_{i_1<\ldots<i_p} \alpha_{i_1,\ldots,i_p}dz_{i_1}\wedge\ldots\wedge dz_{i_p}\otimes \sum_{j_1<\ldots<j_q}\beta_{i_1,\ldots,i_q}d\bar z_{j_1}\wedge\ldots\wedge d\bar z_{j_q}\,.$$ So why in every textbook a $(p,q)$-dffierential form is written simply as:   $$\omega=\sum \alpha_{i_1,\ldots,i_p}dz_{i_1}\wedge\ldots\wedge dz_{i_p}\wedge d\bar z_{j_1}\wedge\ldots\wedge d\bar z_{j_q}\;\;?$$   Where is the tensor product?","Let $M$ be a complex manifold of dimension $n$. It means that $M$ is a real smooth smooth manifold of dimension $2n$. Suppose that the real tangent bundle of $M$ has a local basis: $$\left\{\frac{\partial }{\partial x_1},\ldots,\frac{\partial}{\partial x_n},\frac{\partial }{\partial y_1},\ldots,\frac{\partial}{\partial y_n}\right\}$$ and the real cotangent bundle has local basis $$\left\{dx_1,\ldots,dx_n,dy_1,\ldots dy_n\right\}$$ Then we put $$\frac{\partial}{\partial z_j}:=\frac{1}{2}\left(\frac{\partial }{\partial x_j}-i \frac{\partial }{\partial y_j}\right)$$  $$\frac{\partial}{\partial \bar z_j}:=\frac{1}{2}\left(\frac{\partial }{\partial x_j}+i \frac{\partial }{\partial y_j}\right)$$ $$dz_j:=dx_j+idy_j$$ $$d\bar z_j:=dx_j-idy_j$$ Now consider the complexified cotangent bundle $(T^\ast M)_{\mathbb C}$, it has a decomposition: $$(T^\ast M)_\mathbb C:=T^\ast M^{1,0}\oplus T^\ast M^{0.1}$$ where $$T^\ast M^{(1,0)}=\left<dz_j:j=1,\ldots n\right>$$ $$T^\ast M^{(1,0)}=\left<d\bar z_j:j=1,\ldots n\right>\,.$$ At this point one defines the algebra of differential $(p,q)$-forms on $M$ as: $$\bigwedge^{p,q}M:=\bigwedge^{p}T^\ast M^{1,0}\otimes \bigwedge^{q}T^\ast M^{0,1}$$ So locally any $(p,q)$-differential form can be written as $$\omega=\sum_{i_1<\ldots<i_p} \alpha_{i_1,\ldots,i_p}dz_{i_1}\wedge\ldots\wedge dz_{i_p}\otimes \sum_{j_1<\ldots<j_q}\beta_{i_1,\ldots,i_q}d\bar z_{j_1}\wedge\ldots\wedge d\bar z_{j_q}\,.$$ So why in every textbook a $(p,q)$-dffierential form is written simply as:   $$\omega=\sum \alpha_{i_1,\ldots,i_p}dz_{i_1}\wedge\ldots\wedge dz_{i_p}\wedge d\bar z_{j_1}\wedge\ldots\wedge d\bar z_{j_q}\;\;?$$   Where is the tensor product?",,"['complex-analysis', 'differential-geometry', 'manifolds', 'differential-forms']"
18,How do I study Stein & Shakarchi's Complex Analysis,How do I study Stein & Shakarchi's Complex Analysis,,"I'm currently self-studying some complex analysis. My background is limited: single- and multivariable calculus, linear algebra, introductory Fourier analysis and matrix theory. Each course, with multivariable calculus being a bit of an exception, has emphasized theory and proofs. I've also done the first four chapters of ""Baby Rudin"" in my spare time. The complex analysis book by Stein and Shakarchi is interesting. At times, the book omits quite a few steps that are not so obvious to me. I'm not unfamiliar with this from Rudin who I found was very clear despite the rather terse style. This book will sometimes omit more than I would like. For example, on pages 42-45 the book gives two examples of how real integrals can be solved using Cauchy's theorem, and I had to do a significant amount of filling in the steps and this made me kind of lose track of the bigger idea. I don't like being spoon-fed things because I like to think but at times filling in the steps too much gets in the way of the learning. At least this is how I experience it. My gripe with the book, however, is with the exercises. Many of the exercises, I find, are very difficult. With Rudin, I struggled because the material was abstract: working so rigorously in general metric spaces was extremely challenging for me but I still managed to do at least half the exercises in each chapter. I can at best a third of the exercises in S&S. Any advice on how to better absorb the material in this book? It seems to have a pretty interesting approach with many interesting topics (Zeta function, prime number theorem, Riemann mapping theorem, elliptic functions) including some that appear early (Runge's theorem, Hadamard's factorization theorem). So there's plenty of wonderful mathematics here but the exercises are making studying this terribly difficult.","I'm currently self-studying some complex analysis. My background is limited: single- and multivariable calculus, linear algebra, introductory Fourier analysis and matrix theory. Each course, with multivariable calculus being a bit of an exception, has emphasized theory and proofs. I've also done the first four chapters of ""Baby Rudin"" in my spare time. The complex analysis book by Stein and Shakarchi is interesting. At times, the book omits quite a few steps that are not so obvious to me. I'm not unfamiliar with this from Rudin who I found was very clear despite the rather terse style. This book will sometimes omit more than I would like. For example, on pages 42-45 the book gives two examples of how real integrals can be solved using Cauchy's theorem, and I had to do a significant amount of filling in the steps and this made me kind of lose track of the bigger idea. I don't like being spoon-fed things because I like to think but at times filling in the steps too much gets in the way of the learning. At least this is how I experience it. My gripe with the book, however, is with the exercises. Many of the exercises, I find, are very difficult. With Rudin, I struggled because the material was abstract: working so rigorously in general metric spaces was extremely challenging for me but I still managed to do at least half the exercises in each chapter. I can at best a third of the exercises in S&S. Any advice on how to better absorb the material in this book? It seems to have a pretty interesting approach with many interesting topics (Zeta function, prime number theorem, Riemann mapping theorem, elliptic functions) including some that appear early (Runge's theorem, Hadamard's factorization theorem). So there's plenty of wonderful mathematics here but the exercises are making studying this terribly difficult.",,"['complex-analysis', 'self-learning']"
19,Are multi-valued functions a rigorous concept or simply a conversational shorthand?,Are multi-valued functions a rigorous concept or simply a conversational shorthand?,,"In Brown and Churchill's book, the concept of multivalued functions is not discussed in a very rigorous way (if at all). But I can see that branch cuts have importance in complex analysis, so I want to clarify my understanding of multivalued functions. Is there a rigorous development of the definition of a multivalued function somewhere, along with branch cuts? Or is the whole idea of a multivalued function just a way of saying, ""Hey, there's no unique way of defining the logarithm function here, so we're going to use whatever is convenient at the time""? And if the latter, where does a rigorous understanding of branch cuts fit in? Or are they also more of an intuitive term rather than a real defined mathematical object? If they are rigorous, would a multivalued function be something like $f: \mathbb{C} \to \mathbb{C}^\infty$? I've never seen an infinite dimensional space before so I don't really know how that is developed.","In Brown and Churchill's book, the concept of multivalued functions is not discussed in a very rigorous way (if at all). But I can see that branch cuts have importance in complex analysis, so I want to clarify my understanding of multivalued functions. Is there a rigorous development of the definition of a multivalued function somewhere, along with branch cuts? Or is the whole idea of a multivalued function just a way of saying, ""Hey, there's no unique way of defining the logarithm function here, so we're going to use whatever is convenient at the time""? And if the latter, where does a rigorous understanding of branch cuts fit in? Or are they also more of an intuitive term rather than a real defined mathematical object? If they are rigorous, would a multivalued function be something like $f: \mathbb{C} \to \mathbb{C}^\infty$? I've never seen an infinite dimensional space before so I don't really know how that is developed.",,"['complex-analysis', 'reference-request', 'branch-cuts', 'multivalued-functions']"
20,Rudin Theorem 1.35 - Cauchy Schwarz Inequality,Rudin Theorem 1.35 - Cauchy Schwarz Inequality,,"Any motivation for the sum that Rudin considers in his proof of the Cauchy-Schwarz Inequality? Theorem 1.35 If $a_1,...,a_n$ and $b_1, ..., b_n$ are complex numbers, then $$\Biggl\vert\sum_{j=1}^n a_j\overline{b_j}\Biggr\vert^2 \leq \sum_{j=1}^n|a_j|^2\sum_{j=1}^n|b_j|^2.$$ For the proof, he considers this sum to kick it off: $$\sum_{j=1}^n \vert Ba_j - Cb_j\vert, \text{ where } B = \sum_{j=1}^n \vert b_j \vert^2 \text{ and } C = \sum_{j=1}^na_j\overline{b_j}.$$ I don't see where it comes from. Any help? Thank-you.","Any motivation for the sum that Rudin considers in his proof of the Cauchy-Schwarz Inequality? Theorem 1.35 If and are complex numbers, then For the proof, he considers this sum to kick it off: I don't see where it comes from. Any help? Thank-you.","a_1,...,a_n b_1, ..., b_n \Biggl\vert\sum_{j=1}^n a_j\overline{b_j}\Biggr\vert^2 \leq \sum_{j=1}^n|a_j|^2\sum_{j=1}^n|b_j|^2. \sum_{j=1}^n \vert Ba_j - Cb_j\vert, \text{ where } B = \sum_{j=1}^n \vert b_j \vert^2 \text{ and } C = \sum_{j=1}^na_j\overline{b_j}.",['complex-analysis']
21,"Zeros of Fourier transform of a function in $C[-1,1]$",Zeros of Fourier transform of a function in,"C[-1,1]","I am trying to prove the following: Let $g \in C[-1,1]$. Then the function $$G(z) = \int_{-1}^1 e^{itz}g(t)dt$$ has infinitely many zeros. I know that $G(z)$ is entire and $\lim_{x \to \pm \infty} G(x) = 0$. I have tried the following. Assume the contrary, that is, that $G(z)$ has only $n \in \mathbb{N}$ zeros. Then we can write it as $$G(z) = e^{h(z)}P(z)$$ where $P(z)$ is a polynomial of degree $n$ and $h(z)$ is entire. The limit above implies that $h(x) + \log |P(x)| \to -\infty$, i.e. that $h(x) \to -\infty$ (on the real axis) faster than some asymptotically logarithmic positive function. Unfortunately the above does not seem to solve the problem, or at least I do not know how to continue. Asking for your guidance. EDIT: Other thought were: Approximate $g(t)$ with a step function $h_n(t)$ with $2^n$ steps. Define $H_n(z)$ as the transform of $h_n(t)$, show that $|G(z) - H_n(z)| < |H_n(z)|$ and apply Rouche's theorem. One problem is that $H_n(z)$ is also small on the boundary, and even if I could prove the inequality it is still unclear how infinity of zeros follows for $G(z)$. Show that $G(z)$ is of fractional order and apply Hadamard's theorem. This is clearly false since I can show that the order of $G(z)$ is bounded by $1$ from above, and, at least for some $g(t)$, the bound is achieved.","I am trying to prove the following: Let $g \in C[-1,1]$. Then the function $$G(z) = \int_{-1}^1 e^{itz}g(t)dt$$ has infinitely many zeros. I know that $G(z)$ is entire and $\lim_{x \to \pm \infty} G(x) = 0$. I have tried the following. Assume the contrary, that is, that $G(z)$ has only $n \in \mathbb{N}$ zeros. Then we can write it as $$G(z) = e^{h(z)}P(z)$$ where $P(z)$ is a polynomial of degree $n$ and $h(z)$ is entire. The limit above implies that $h(x) + \log |P(x)| \to -\infty$, i.e. that $h(x) \to -\infty$ (on the real axis) faster than some asymptotically logarithmic positive function. Unfortunately the above does not seem to solve the problem, or at least I do not know how to continue. Asking for your guidance. EDIT: Other thought were: Approximate $g(t)$ with a step function $h_n(t)$ with $2^n$ steps. Define $H_n(z)$ as the transform of $h_n(t)$, show that $|G(z) - H_n(z)| < |H_n(z)|$ and apply Rouche's theorem. One problem is that $H_n(z)$ is also small on the boundary, and even if I could prove the inequality it is still unclear how infinity of zeros follows for $G(z)$. Show that $G(z)$ is of fractional order and apply Hadamard's theorem. This is clearly false since I can show that the order of $G(z)$ is bounded by $1$ from above, and, at least for some $g(t)$, the bound is achieved.",,"['complex-analysis', 'fourier-analysis', 'roots']"
22,Complex Analysis -- Uniform Convergence on Compact Sets,Complex Analysis -- Uniform Convergence on Compact Sets,,"I've taken a course Complex Analysis, but I don't understand why the phrase ""For Uniform Convergence on Compact Sets"" was used all the time. I always got the impression this was ""good enough"" for something. But for what? Why is it enough? Why do we care? When is it enough? When is it not enough? Did this same idea pop up somewhere in real analysis/measure theory? I really appreciate your exposition on this matter!","I've taken a course Complex Analysis, but I don't understand why the phrase ""For Uniform Convergence on Compact Sets"" was used all the time. I always got the impression this was ""good enough"" for something. But for what? Why is it enough? Why do we care? When is it enough? When is it not enough? Did this same idea pop up somewhere in real analysis/measure theory? I really appreciate your exposition on this matter!",,"['complex-analysis', 'uniform-convergence']"
23,Intuition behind euler's formula [duplicate],Intuition behind euler's formula [duplicate],,"This question already has answers here : Closed 13 years ago . Possible Duplicate: How to prove Euler's formula: $\\exp(i t)=\\cos(t)+i\\sin(t)$ ? Hi, I've been curious for quite a long time whether it is actually possible to have an intuitive understanding of euler's apparently magical formula: $$e^{ \pm i\theta } = \cos \theta \pm i\sin \theta$$ I've obviously seen the taylor series/differential equation based proofs, and perhaps I'm just going to have to accept that it's not possible to have an intuition on what it means to raise a number to an imaginary power. I obviously realise that the formula implies that an exponential with a variable imaginary part can be visualised as a complex function going around in a unit circle about the origin of the complex plane. But WHY is this? And why is e so special that it moves at just a fast enough rate so that the argument of the exponential is equal to the arc length of the path made by the locus (i.e. the angle in radians we've moved around the circle)? Is there any way anyone out there 'understand' this? Thankyou!","This question already has answers here : Closed 13 years ago . Possible Duplicate: How to prove Euler's formula: $\\exp(i t)=\\cos(t)+i\\sin(t)$ ? Hi, I've been curious for quite a long time whether it is actually possible to have an intuitive understanding of euler's apparently magical formula: $$e^{ \pm i\theta } = \cos \theta \pm i\sin \theta$$ I've obviously seen the taylor series/differential equation based proofs, and perhaps I'm just going to have to accept that it's not possible to have an intuition on what it means to raise a number to an imaginary power. I obviously realise that the formula implies that an exponential with a variable imaginary part can be visualised as a complex function going around in a unit circle about the origin of the complex plane. But WHY is this? And why is e so special that it moves at just a fast enough rate so that the argument of the exponential is equal to the arc length of the path made by the locus (i.e. the angle in radians we've moved around the circle)? Is there any way anyone out there 'understand' this? Thankyou!",,"['complex-analysis', 'intuition']"
24,Strengthened version of the Casorati-Weierstrass Theorem,Strengthened version of the Casorati-Weierstrass Theorem,,"Suppose the $f$ has an essential singularity at $z=a$.Prove that if $c\in \mathbb{C}$,and $\varepsilon >0$ are given,then for each $\delta >0$ there is a number $b$,$|c-b|<\varepsilon$,such the $f(z)=b$ has infinitely many solution in $B(a;\delta)$. This is an exercise from 'functions of one complex variable'. I solved this question by using open mapping theorem and Baire Category Theorem to argue that $\bigcap_{i=1}^{\infty}f(\{z:0<|z-a|<1/n\})$ is a dense set. Is there a solution which does not use Baire Category Theorem? Thank you.","Suppose the $f$ has an essential singularity at $z=a$.Prove that if $c\in \mathbb{C}$,and $\varepsilon >0$ are given,then for each $\delta >0$ there is a number $b$,$|c-b|<\varepsilon$,such the $f(z)=b$ has infinitely many solution in $B(a;\delta)$. This is an exercise from 'functions of one complex variable'. I solved this question by using open mapping theorem and Baire Category Theorem to argue that $\bigcap_{i=1}^{\infty}f(\{z:0<|z-a|<1/n\})$ is a dense set. Is there a solution which does not use Baire Category Theorem? Thank you.",,['complex-analysis']
25,Contour integral representation of Confluent Hypergeometric Function,Contour integral representation of Confluent Hypergeometric Function,,"My brain is spinning around in circles trying to reconcile three distinct contour-integral representation of the confluent hypergeometric function $_1F_1(a,b,z)$ for $b \in \mathbb{Z}_+$ : From K.T.Hecht QM (2000) Chapter 42, $$_1F_1(a,b,z)=\frac{\Gamma(b)}{2\pi i}\oint_{C_1} dt\,e^t\, t^{a-b}\, (t-z)^{-a}\,,\qquad\qquad (1)$$ where the contour $C_1$ ""surrounds the branch cut from $t=0$ to $t=z$ ."" From Messiah, QM (1961) Appendix B, $$_1F_1(a,b,z)=(1-e^{-2\pi i a})^{-1}\frac{\Gamma(b)}{\Gamma(a)\Gamma(b-a)}\oint_{C_2} dt\,e^{zt}\,t^{a-1}\,(1-t)^{b-a-1}\,,\qquad\qquad (2)$$ where the contour $C_2$ ""surrounds the points $t=0$ and $t=1$ ."" And finally, from NIST Handbook of mathematical functions §13.4 (ii), (either eqn 13.4.9, 13.4.10, or 13.4.11) which I don't even know how to read: $$_1F_1(a,b,z)=\frac{\Gamma(1+a-b)}{2\pi i\,\Gamma(a)\Gamma(b)}\int_0^{(1+)}dt\,e^{zt}\,t^{a-1}\,(t-1)^{b-a-1} \qquad\qquad (3)$$ I badly need help in showing the equivalence of these three functions. My highest priority is showing equivalence of (1) and (3).","My brain is spinning around in circles trying to reconcile three distinct contour-integral representation of the confluent hypergeometric function for : From K.T.Hecht QM (2000) Chapter 42, where the contour ""surrounds the branch cut from to ."" From Messiah, QM (1961) Appendix B, where the contour ""surrounds the points and ."" And finally, from NIST Handbook of mathematical functions §13.4 (ii), (either eqn 13.4.9, 13.4.10, or 13.4.11) which I don't even know how to read: I badly need help in showing the equivalence of these three functions. My highest priority is showing equivalence of (1) and (3).","_1F_1(a,b,z) b \in \mathbb{Z}_+ _1F_1(a,b,z)=\frac{\Gamma(b)}{2\pi i}\oint_{C_1} dt\,e^t\, t^{a-b}\, (t-z)^{-a}\,,\qquad\qquad (1) C_1 t=0 t=z _1F_1(a,b,z)=(1-e^{-2\pi i a})^{-1}\frac{\Gamma(b)}{\Gamma(a)\Gamma(b-a)}\oint_{C_2} dt\,e^{zt}\,t^{a-1}\,(1-t)^{b-a-1}\,,\qquad\qquad (2) C_2 t=0 t=1 _1F_1(a,b,z)=\frac{\Gamma(1+a-b)}{2\pi i\,\Gamma(a)\Gamma(b)}\int_0^{(1+)}dt\,e^{zt}\,t^{a-1}\,(t-1)^{b-a-1} \qquad\qquad (3)","['complex-analysis', 'special-functions', 'contour-integration']"
26,Conformal map from punctured disc to disc,Conformal map from punctured disc to disc,,"Let $G$ be an open subset of the complex plane.  A function $f:G \rightarrow \mathbb{C}$ is said to be conformal if it is complex differentiable with nonvanishing derivative. I am stuck trying to find a conformal map from the punctured unit disc onto the unit disc.  I know that such a map cannot be bijective, since the punctured unit disc is not simply connected, but I believe that there is a surjective conformal map. I found another post Conformal map from the punctured unit disc onto the unit disc? that asks the same question, but no solution is given.  The chosen answer provides a conformal map from the disc onto the punctured disc, not from the punctured disc onto the disc.","Let $G$ be an open subset of the complex plane.  A function $f:G \rightarrow \mathbb{C}$ is said to be conformal if it is complex differentiable with nonvanishing derivative. I am stuck trying to find a conformal map from the punctured unit disc onto the unit disc.  I know that such a map cannot be bijective, since the punctured unit disc is not simply connected, but I believe that there is a surjective conformal map. I found another post Conformal map from the punctured unit disc onto the unit disc? that asks the same question, but no solution is given.  The chosen answer provides a conformal map from the disc onto the punctured disc, not from the punctured disc onto the disc.",,['complex-analysis']
27,How does one know that a theorem is strong enough to publish?,How does one know that a theorem is strong enough to publish?,,"Question. How does one know that a theorem is strong enough to publish? Basically, I have laid out a framework in which many theorems may be proven. I'm only 18 and therefore lack knowledge of whether this framework and the theorems sprouting from it are trivial along with the theorems. What is a good indicator that work is good enough to be published? An example of a theorem I have proved is; Given a (non-constant) meromorphic function $f$ there exists at least one continuous loop over the extended complex plane, $\varphi$, such that $f\varphi :\mathbb{R}\rightarrow \mathbb{R}$ (bijective).","Question. How does one know that a theorem is strong enough to publish? Basically, I have laid out a framework in which many theorems may be proven. I'm only 18 and therefore lack knowledge of whether this framework and the theorems sprouting from it are trivial along with the theorems. What is a good indicator that work is good enough to be published? An example of a theorem I have proved is; Given a (non-constant) meromorphic function $f$ there exists at least one continuous loop over the extended complex plane, $\varphi$, such that $f\varphi :\mathbb{R}\rightarrow \mathbb{R}$ (bijective).",,"['complex-analysis', 'soft-question', 'publishing']"
28,Is $|z|^2$ complex differentiable?,Is  complex differentiable?,|z|^2,"I think I am a bit confused about the definition of (complex) differentiability. Yes, I know that's stupid, but I am hoping that someone could clear it up for me. I know that the definition of (complex) differentiability is when $\lim\limits_{h\to 0}{f(z+h)-f(z)\over h}$ exists. So, is $|z|^2$ considered differentiable? What I think is it is only differentiable at $z=0$ since at any other point if we take $f(z+h)-f(z)\over h$  as $h\to 0$ along a contour line of $|z|^2$ then the limit is $0$ whereas if we take a path say perpendicular to the contour lines, the ""gradient"" wouldn't be $0$, right? But then if this is true then all complex functions that are ""not flat"" would not be differentiable, so I must be wrong. Could someone kindly explain to me what is going on? Sorry for my stupidity!","I think I am a bit confused about the definition of (complex) differentiability. Yes, I know that's stupid, but I am hoping that someone could clear it up for me. I know that the definition of (complex) differentiability is when $\lim\limits_{h\to 0}{f(z+h)-f(z)\over h}$ exists. So, is $|z|^2$ considered differentiable? What I think is it is only differentiable at $z=0$ since at any other point if we take $f(z+h)-f(z)\over h$  as $h\to 0$ along a contour line of $|z|^2$ then the limit is $0$ whereas if we take a path say perpendicular to the contour lines, the ""gradient"" wouldn't be $0$, right? But then if this is true then all complex functions that are ""not flat"" would not be differentiable, so I must be wrong. Could someone kindly explain to me what is going on? Sorry for my stupidity!",,['complex-analysis']
29,Mean Value Theorem for complex functions?,Mean Value Theorem for complex functions?,,"Let $\varphi_t$ be an analytic function on an open domain $\Omega\subseteq\mathbb{C}$. Let $K \subset \Omega$ be a compact set. I am trying to prove that for any fixed parameter and fixed values: $$\left|\frac{\varphi_t(b) - \varphi_t(a)}{b-a}\right| \leq  \sup_{z \in K} \left|\frac{d}{dz}\varphi_t(z)\right|$$ For a second, I thought mean value theorem might work here, but then I realized that MVT does not exist for complex functions. Any ideas for proving the statement?","Let $\varphi_t$ be an analytic function on an open domain $\Omega\subseteq\mathbb{C}$. Let $K \subset \Omega$ be a compact set. I am trying to prove that for any fixed parameter and fixed values: $$\left|\frac{\varphi_t(b) - \varphi_t(a)}{b-a}\right| \leq  \sup_{z \in K} \left|\frac{d}{dz}\varphi_t(z)\right|$$ For a second, I thought mean value theorem might work here, but then I realized that MVT does not exist for complex functions. Any ideas for proving the statement?",,['complex-analysis']
30,"definition of winding number, have doubt in definition.","definition of winding number, have doubt in definition.",,could any one tell me why in the definition of index number or winding number of a curve $\gamma(t)$ around some point $a$ we take this integral : $$\frac{1}{2\pi i}\int_{\gamma}\frac{1}{z-a}  $$ why not  $$\frac{1}{2\pi i}\int_{\gamma}\frac{1}{z^2+2z+1+e^z-a}  $$?,could any one tell me why in the definition of index number or winding number of a curve $\gamma(t)$ around some point $a$ we take this integral : $$\frac{1}{2\pi i}\int_{\gamma}\frac{1}{z-a}  $$ why not  $$\frac{1}{2\pi i}\int_{\gamma}\frac{1}{z^2+2z+1+e^z-a}  $$?,,['complex-analysis']
31,Does $\sin(x+iy) = x+iy$ have infinitely many solutions?,Does  have infinitely many solutions?,\sin(x+iy) = x+iy,"How to prove that $\sin(x+iy) = x+iy$ has infinitely many solutions? I know how to prove that $\sin(x) = x$ has only one solution, but I do not know how to extend this to complex analysis.","How to prove that $\sin(x+iy) = x+iy$ has infinitely many solutions? I know how to prove that $\sin(x) = x$ has only one solution, but I do not know how to extend this to complex analysis.",,"['complex-analysis', 'trigonometry', 'complex-numbers']"
32,"Complex zeros of the polynomials $\sum_{k=0}^{n} z^k/k!$, inside balls","Complex zeros of the polynomials , inside balls",\sum_{k=0}^{n} z^k/k!,"this is a question from a Temple prelim exam, and i'm trapped in it! We have $p_n(z)=\sum_{k=0}^n\frac{z^k}{k!}$ and we have to prove that $\forall r>0 \quad \exists N\in\mathbb{N}$ s.t. $p_n(z)$ has no zeros in $B_r(0)$ for $n>N$. I tried to use Rouche's theorem observing that $1+\frac{z^n}{n!}$ has the nth roots of $n!$ as complex zeros, which are outside a fixed ball for n sufficiently large, but i'm having trouble proving that $|1+\frac{z^n}{n!}|>|z+\cdots+\frac{z^{n-1}}{(n-1)!}|$ in $|z|=r$ (if this is true). If anyone could help..","this is a question from a Temple prelim exam, and i'm trapped in it! We have $p_n(z)=\sum_{k=0}^n\frac{z^k}{k!}$ and we have to prove that $\forall r>0 \quad \exists N\in\mathbb{N}$ s.t. $p_n(z)$ has no zeros in $B_r(0)$ for $n>N$. I tried to use Rouche's theorem observing that $1+\frac{z^n}{n!}$ has the nth roots of $n!$ as complex zeros, which are outside a fixed ball for n sufficiently large, but i'm having trouble proving that $|1+\frac{z^n}{n!}|>|z+\cdots+\frac{z^{n-1}}{(n-1)!}|$ in $|z|=r$ (if this is true). If anyone could help..",,"['complex-analysis', 'polynomials', 'roots']"
33,Fourier transform of 1/cosh,Fourier transform of 1/cosh,,"How do you take the Fourier transform of  $$ f(x) = \frac{1}{\cosh x} $$ This is for a complex class so I tried expanding the denominator and calculating a residue by using the rectangular contour that goes from $-\infty$ to $\infty$ along the real axis and $i \pi +\infty$ to $i \pi - \infty$ to close the contour (with vertical sides that go to 0).  Therefore, I tried to calculate the residue at $\frac{i \pi}{2}$ of  $$ \frac{e^{-ikx}}{e^x + e^{-x}} $$ which will be give me the answer, but I don't know how to do this.  Thanks for the help!","How do you take the Fourier transform of  $$ f(x) = \frac{1}{\cosh x} $$ This is for a complex class so I tried expanding the denominator and calculating a residue by using the rectangular contour that goes from $-\infty$ to $\infty$ along the real axis and $i \pi +\infty$ to $i \pi - \infty$ to close the contour (with vertical sides that go to 0).  Therefore, I tried to calculate the residue at $\frac{i \pi}{2}$ of  $$ \frac{e^{-ikx}}{e^x + e^{-x}} $$ which will be give me the answer, but I don't know how to do this.  Thanks for the help!",,"['complex-analysis', 'fourier-analysis', 'fourier-transform', 'residue-calculus']"
34,How do you prove that $\ln|f(z)|$ is harmonic?,How do you prove that  is harmonic?,\ln|f(z)|,Suppose that $f(z)$ is analytic and nonzero in a domain $D$.  Prove that $\ln|f(z)|$ is harmonic in $D$. I know the laplacian equation but I'm not sure how to use it.,Suppose that $f(z)$ is analytic and nonzero in a domain $D$.  Prove that $\ln|f(z)|$ is harmonic in $D$. I know the laplacian equation but I'm not sure how to use it.,,"['complex-analysis', 'harmonic-functions']"
35,Complex polynomial and the unit circle,Complex polynomial and the unit circle,,"Given a polynomial $ P(z) = z^n + a_{n-1}z^{n-1} + \cdots + a_0 $, such that $\max_{|z|=1} |P(z)| = 1 $ Prove: $ P(z) = z^n $ Hint: Use cauchy derivative estimation    $$ |f^{(n)} (z_0)| \leq \frac{n!}{r^n} \max_{|z-z_0|\leq r} |f(z)| $$ and look at the function $ \frac{P(z)}{z^n} $ It seems to be maximum principle related, but I can't see how to use it and I can't understand how to use the hint.","Given a polynomial $ P(z) = z^n + a_{n-1}z^{n-1} + \cdots + a_0 $, such that $\max_{|z|=1} |P(z)| = 1 $ Prove: $ P(z) = z^n $ Hint: Use cauchy derivative estimation    $$ |f^{(n)} (z_0)| \leq \frac{n!}{r^n} \max_{|z-z_0|\leq r} |f(z)| $$ and look at the function $ \frac{P(z)}{z^n} $ It seems to be maximum principle related, but I can't see how to use it and I can't understand how to use the hint.",,['complex-analysis']
36,Does $\sqrt{i + \sqrt{i+ \sqrt{i + \sqrt{i + \cdots}}}}$ have a closed form?,Does  have a closed form?,\sqrt{i + \sqrt{i+ \sqrt{i + \sqrt{i + \cdots}}}},"I've been brushing up on my complex analysis recently, and I've come across a problem that's stumped me: What are the real and imaginary parts of $$\sqrt{i+\sqrt{i+\sqrt{i+\sqrt{i+\cdots}}}} ?$$ I really don't know how to start this problem; this is my first encounter with nested roots.","I've been brushing up on my complex analysis recently, and I've come across a problem that's stumped me: What are the real and imaginary parts of $$\sqrt{i+\sqrt{i+\sqrt{i+\sqrt{i+\cdots}}}} ?$$ I really don't know how to start this problem; this is my first encounter with nested roots.",,"['complex-analysis', 'complex-numbers']"
37,What are the branches of the square root function?,What are the branches of the square root function?,,"I am studying branches of logarithm. I came to know that there are infinitely many branches of logarithm where $\log z = \log |z| + i (\arg z +2k\pi)$ , $k \in \mathbb Z$ and $z \neq 0$ . Now for each $\alpha \in [0,2\pi)$ if we restrict $\arg z$ to lie inside $(\alpha , \alpha + 2\pi)$ this will yield a branch of logarithm having branch cut $\theta = \alpha$ which is analytic in the cut plane $D_{\alpha} = \mathbb C \setminus \{z \in \mathbb C : z \leq 0 \}$ . For each such branch there exists a principal logarithmic function where $k=0$ i.e. $\log z =\log |z| + i \arg_{\alpha} z$ where $z \neq 0$ and $\arg_{\alpha}$ is the restriction of the argument function on $(\alpha,\alpha+2\pi)$ for some $\alpha \in [0,2\pi)$ . The principal branch of logarithm corresponds to $k=0$ and $\arg=\arg_{\pi}$ as the argument function which is known as principal argument function. Now my question is : ""Is the same true for square root?"" As we know that $z^{\frac {1} {2}} = \exp (\frac {1} {2} \log z)$ . As we know that logarithm has infinitely many branches, each of which is analytic in some certain cut plane. So we can say that $z^{\frac {1} {2}}$ is analytic on a certain cut plane of the corresponding logarithmic branch. But I don't know whether it is analytic on any point on the cut plane of the corresponding logarithmic branch or not!! If it is not so then clearly there are infinitely many branches of square root function. Corresponding to each branch there are two square root functions. One is $z \mapsto |z|^{\frac {1} {2}} e^{\frac {i\arg_{\alpha} z} {2}}$ and the other is $z \mapsto -|z|^{\frac {1} {2}} e^{\frac {i\arg_{\alpha} z} {2}}$ for each $\alpha \in [0,2\pi)$ . But for that I need the answer to the question whether $z^{\frac {1} {2}}$ is analytic on the points of the cut plane of the corresponding logarithmic branch or not. If the answer to that question is ""no"" then only we can extend the concept of logarithmic function to the square root function. I only know that the principal square root function is not continuous on $\mathbb C \setminus \{0 \}$ . Is it true or not? I am in a fix. Please help me. Thank you in advance.","I am studying branches of logarithm. I came to know that there are infinitely many branches of logarithm where , and . Now for each if we restrict to lie inside this will yield a branch of logarithm having branch cut which is analytic in the cut plane . For each such branch there exists a principal logarithmic function where i.e. where and is the restriction of the argument function on for some . The principal branch of logarithm corresponds to and as the argument function which is known as principal argument function. Now my question is : ""Is the same true for square root?"" As we know that . As we know that logarithm has infinitely many branches, each of which is analytic in some certain cut plane. So we can say that is analytic on a certain cut plane of the corresponding logarithmic branch. But I don't know whether it is analytic on any point on the cut plane of the corresponding logarithmic branch or not!! If it is not so then clearly there are infinitely many branches of square root function. Corresponding to each branch there are two square root functions. One is and the other is for each . But for that I need the answer to the question whether is analytic on the points of the cut plane of the corresponding logarithmic branch or not. If the answer to that question is ""no"" then only we can extend the concept of logarithmic function to the square root function. I only know that the principal square root function is not continuous on . Is it true or not? I am in a fix. Please help me. Thank you in advance.","\log z = \log |z| + i (\arg z +2k\pi) k \in \mathbb Z z \neq 0 \alpha \in [0,2\pi) \arg z (\alpha , \alpha + 2\pi) \theta = \alpha D_{\alpha} = \mathbb C \setminus \{z \in \mathbb C : z \leq 0 \} k=0 \log z =\log |z| + i \arg_{\alpha} z z \neq 0 \arg_{\alpha} (\alpha,\alpha+2\pi) \alpha \in [0,2\pi) k=0 \arg=\arg_{\pi} z^{\frac {1} {2}} = \exp (\frac {1} {2} \log z) z^{\frac {1} {2}} z \mapsto |z|^{\frac {1} {2}} e^{\frac {i\arg_{\alpha} z} {2}} z \mapsto -|z|^{\frac {1} {2}} e^{\frac {i\arg_{\alpha} z} {2}} \alpha \in [0,2\pi) z^{\frac {1} {2}} \mathbb C \setminus \{0 \}","['complex-analysis', 'logarithms', 'branch-cuts']"
38,$f(z)$ and $\overline{f(\overline{z})}$ simultaneously holomorphic,and  simultaneously holomorphic,f(z) \overline{f(\overline{z})},"Prove that the functions $f(z)$ and $\overline{f(\overline{z})}$ are simultaneously holomorphic. I take this to mean that $f(z)$ is holomorphic if and only if $\overline{f(\overline{z})}$ is holomorphic. Let $g(z)=\overline{f(\overline{z})}$. Note that $\overline{g(\overline{z})}=f(z)$. So it suffices to prove that if $f(z)$ is holomorphic, then $g(z)$ is holomorphic. Write $f(z)=u(z)+iv(z)$. Since $f(z)$ is holomorphic, the real and imaginary parts satisfy the Cauchy-Riemann equations: $$\frac{\partial{u(z)}}{\partial{x}} = \frac{\partial{v(z)}}{\partial{y}}, \frac{\partial{u(z)}}{\partial{y}} = -\frac{\partial{v(z)}}{\partial{x}}.$$ We have $g(z) = u(\overline{z})+i(-v(\overline{z}))$. To prove that $g(z)$ is holomorphic, we must prove that its real and imaginary parts satisfy the Cauchy-Riemann equations: $$\frac{\partial{u(\overline{z})}}{\partial{x}} = \frac{\partial{(-v(\overline{z}))}}{\partial{y}}, \frac{\partial{u(\overline{z})}}{\partial{y}} = -\frac{\partial{(-v(\overline{z}))}}{\partial{x}}.$$ How can we obtain this from the above relations?","Prove that the functions $f(z)$ and $\overline{f(\overline{z})}$ are simultaneously holomorphic. I take this to mean that $f(z)$ is holomorphic if and only if $\overline{f(\overline{z})}$ is holomorphic. Let $g(z)=\overline{f(\overline{z})}$. Note that $\overline{g(\overline{z})}=f(z)$. So it suffices to prove that if $f(z)$ is holomorphic, then $g(z)$ is holomorphic. Write $f(z)=u(z)+iv(z)$. Since $f(z)$ is holomorphic, the real and imaginary parts satisfy the Cauchy-Riemann equations: $$\frac{\partial{u(z)}}{\partial{x}} = \frac{\partial{v(z)}}{\partial{y}}, \frac{\partial{u(z)}}{\partial{y}} = -\frac{\partial{v(z)}}{\partial{x}}.$$ We have $g(z) = u(\overline{z})+i(-v(\overline{z}))$. To prove that $g(z)$ is holomorphic, we must prove that its real and imaginary parts satisfy the Cauchy-Riemann equations: $$\frac{\partial{u(\overline{z})}}{\partial{x}} = \frac{\partial{(-v(\overline{z}))}}{\partial{y}}, \frac{\partial{u(\overline{z})}}{\partial{y}} = -\frac{\partial{(-v(\overline{z}))}}{\partial{x}}.$$ How can we obtain this from the above relations?",,['complex-analysis']
39,How to prove Lagrange trigonometric identity [duplicate],How to prove Lagrange trigonometric identity [duplicate],,"This question already has answers here : Finite Sum $\sum\limits_{k=0}^{n}\cos(kx)$ (7 answers) Closed 10 years ago . I would to prove that  $$1+\cos \theta+\cos 2\theta+\ldots+\cos n\theta =\displaystyle\frac{1}{2}+  \frac{\sin\left[(2n+1)\frac{\theta}{2}\right]}{2\sin\left(\frac{\theta}{2}\right)}$$ given that  $$1+z+z^2+z^3+\ldots+z^n=\displaystyle\frac {1-z^{n+1}}{1-z}$$ where $z\neq 1$. I put $z=e^{i\theta}$. I already got in left hand side cos exp in real part, but there is a problem in the right hand side, I can't split imaginary part and real part. Please help me. Thanks in advance.","This question already has answers here : Finite Sum $\sum\limits_{k=0}^{n}\cos(kx)$ (7 answers) Closed 10 years ago . I would to prove that  $$1+\cos \theta+\cos 2\theta+\ldots+\cos n\theta =\displaystyle\frac{1}{2}+  \frac{\sin\left[(2n+1)\frac{\theta}{2}\right]}{2\sin\left(\frac{\theta}{2}\right)}$$ given that  $$1+z+z^2+z^3+\ldots+z^n=\displaystyle\frac {1-z^{n+1}}{1-z}$$ where $z\neq 1$. I put $z=e^{i\theta}$. I already got in left hand side cos exp in real part, but there is a problem in the right hand side, I can't split imaginary part and real part. Please help me. Thanks in advance.",,"['complex-analysis', 'trigonometry', 'summation']"
40,"Why do we categorize all other (iso.) singularities as ""essential""?","Why do we categorize all other (iso.) singularities as ""essential""?",,"When dealing with isolated singularities, we classify each of these points as removable, pole (of order $k$ ), or essential. It easy to see that all isolated singularities must be of one of these three categories by construction: We define any isolated singularity that isn't removable or pole as an essential singularity . Why is it that we throw all other singularities into this category? Do we not care about essential singularities to classify them further? That is, are removable singularities and poles (of order $k$ ) the only isolated singularities we care about?","When dealing with isolated singularities, we classify each of these points as removable, pole (of order ), or essential. It easy to see that all isolated singularities must be of one of these three categories by construction: We define any isolated singularity that isn't removable or pole as an essential singularity . Why is it that we throw all other singularities into this category? Do we not care about essential singularities to classify them further? That is, are removable singularities and poles (of order ) the only isolated singularities we care about?",k k,['complex-analysis']
41,Prove that $f$ is a polynomial if one of the coefficients in its Taylor expansion is 0,Prove that  is a polynomial if one of the coefficients in its Taylor expansion is 0,f,"Suppose $f$ is an analytic function defined everywhere in $\mathbb{C}$ and such that for each $z_0 \in \mathbb{C}$ at least one coefficient in the expansion $$f(z) = \sum_{n = 0}^{\infty}c_n(z-z_0)^n$$ is equal to $0$. Prove that $f$ is a polynomial. The problem hints to use the fact that $c_nn! = f^{(n)}(z_0)$ and use a countability argument. My attempt at a solution The only thing I can think of in this case is that the coefficients $c_n$ of the above Taylor expansion are defined as: $$c_n = \frac{f^{(n)}(z_0)}{n!}$$ The only way one of these $c_n$ could be zero is if the derivative of order $n$ vanishes everywhere. Thus, this would mean that $f$ is a polynomial of order $k < n$. Does this suffice as a proof? How would I use a countability argument to prove this instead? Thanks.","Suppose $f$ is an analytic function defined everywhere in $\mathbb{C}$ and such that for each $z_0 \in \mathbb{C}$ at least one coefficient in the expansion $$f(z) = \sum_{n = 0}^{\infty}c_n(z-z_0)^n$$ is equal to $0$. Prove that $f$ is a polynomial. The problem hints to use the fact that $c_nn! = f^{(n)}(z_0)$ and use a countability argument. My attempt at a solution The only thing I can think of in this case is that the coefficients $c_n$ of the above Taylor expansion are defined as: $$c_n = \frac{f^{(n)}(z_0)}{n!}$$ The only way one of these $c_n$ could be zero is if the derivative of order $n$ vanishes everywhere. Thus, this would mean that $f$ is a polynomial of order $k < n$. Does this suffice as a proof? How would I use a countability argument to prove this instead? Thanks.",,['complex-analysis']
42,Show that this entire function is polynomial.,Show that this entire function is polynomial.,,Let $f$ be an entire function such that $ |f(z)| \to \infty$ as $|z| \to \infty$. Prove that $f$ is a polynomial.,Let $f$ be an entire function such that $ |f(z)| \to \infty$ as $|z| \to \infty$. Prove that $f$ is a polynomial.,,['complex-analysis']
43,Residue integral: $\int_{- \infty}^{+ \infty} \frac{e^{ax}}{1+e^x} dx$ with $0 \lt a \lt 1$.,Residue integral:  with .,\int_{- \infty}^{+ \infty} \frac{e^{ax}}{1+e^x} dx 0 \lt a \lt 1,"I'm self studying complex analysis. I've encountered the following integral: $$\int_{- \infty}^{+ \infty} \frac{e^{ax}}{1+e^x} dx \text{ with } a \in \mathbb{R},\  0 \lt a \lt 1. $$ I've done the substitution $e^x = y$. What kind of contour can I use in this case ?","I'm self studying complex analysis. I've encountered the following integral: $$\int_{- \infty}^{+ \infty} \frac{e^{ax}}{1+e^x} dx \text{ with } a \in \mathbb{R},\  0 \lt a \lt 1. $$ I've done the substitution $e^x = y$. What kind of contour can I use in this case ?",,"['complex-analysis', 'definite-integrals', 'residue-calculus']"
44,Number of roots of $x^a-1=0$ with $a \in \mathbb{C}$,Number of roots of  with,x^a-1=0 a \in \mathbb{C},"It is well known that $x^2-1=0$ has two roots in $\mathbb{C}$, namely $\pm 1$. In general $x^n-1=0$ has exactly $n$ roots in $\mathbb{C}$. But what happens when $n$ is non integer (rational or real or even complex)? For example how many roots does $x^{1.9}-1=0$ have (counting multiplicity)? Intuitively I suspect that for adequately small $\epsilon$, $x^{1+\epsilon}-1=0$ has one complex root while $x^{2-\epsilon}-1=0$ (for some other $\epsilon$) will have 2 roots and more I guess that these roots will be close to $\pm 1$ respectively. Note that multiplicity of a root $\rho$ of a (non-polynomial) function $f$ is defined as the largest integer $k$ (if exists) such that: \begin{align*} \lim_{x \rightarrow \rho} \frac{|f(x)|}{|x-\rho|^k} < \infty  \end{align*} Update 1: I understand that my claim regarding the number of roots in $\mathbb{C}$ is not correct since $x^2=1$ has two complex roots but $x^{2.1}=1$ has a lot.  My question now is whether the following holds: Claim 1: There is an $\delta \in \mathbb{R}$ such that for all $\epsilon \in \Re$ with $|\epsilon|<\delta$ it holds that $x^{2+\epsilon}=1$ has two real roots (as many as the initial one). Note that this claim is only about the number of real roots of the equation. Some simulations on MATLAB show that this might be true. Additionally, the simulations show that the real roots of the perturbed equation are close to the roots of the unperturbed. This claim, if it holds, can easily be extended to cater for equations with more exponents. Update 2: Consider for example the equation $x^{1.5}=1$. This has solutions $\rho_i=\exp\left(4k\pi i/3\right)=\cos(4k\pi/3)+i \sin(4k\pi/3)$ with $k\in\mathbb{Z}$. What is the multiplicity of each root??? According to the definition, it should be $1$, so I think it would be good when saying that the solutions of $x^a=1$ are $\exp(2k\pi i /a)$ to restrict $k$ properly so that $2k\pi / a \in [0,2\pi)$.","It is well known that $x^2-1=0$ has two roots in $\mathbb{C}$, namely $\pm 1$. In general $x^n-1=0$ has exactly $n$ roots in $\mathbb{C}$. But what happens when $n$ is non integer (rational or real or even complex)? For example how many roots does $x^{1.9}-1=0$ have (counting multiplicity)? Intuitively I suspect that for adequately small $\epsilon$, $x^{1+\epsilon}-1=0$ has one complex root while $x^{2-\epsilon}-1=0$ (for some other $\epsilon$) will have 2 roots and more I guess that these roots will be close to $\pm 1$ respectively. Note that multiplicity of a root $\rho$ of a (non-polynomial) function $f$ is defined as the largest integer $k$ (if exists) such that: \begin{align*} \lim_{x \rightarrow \rho} \frac{|f(x)|}{|x-\rho|^k} < \infty  \end{align*} Update 1: I understand that my claim regarding the number of roots in $\mathbb{C}$ is not correct since $x^2=1$ has two complex roots but $x^{2.1}=1$ has a lot.  My question now is whether the following holds: Claim 1: There is an $\delta \in \mathbb{R}$ such that for all $\epsilon \in \Re$ with $|\epsilon|<\delta$ it holds that $x^{2+\epsilon}=1$ has two real roots (as many as the initial one). Note that this claim is only about the number of real roots of the equation. Some simulations on MATLAB show that this might be true. Additionally, the simulations show that the real roots of the perturbed equation are close to the roots of the unperturbed. This claim, if it holds, can easily be extended to cater for equations with more exponents. Update 2: Consider for example the equation $x^{1.5}=1$. This has solutions $\rho_i=\exp\left(4k\pi i/3\right)=\cos(4k\pi/3)+i \sin(4k\pi/3)$ with $k\in\mathbb{Z}$. What is the multiplicity of each root??? According to the definition, it should be $1$, so I think it would be good when saying that the solutions of $x^a=1$ are $\exp(2k\pi i /a)$ to restrict $k$ properly so that $2k\pi / a \in [0,2\pi)$.",,"['complex-analysis', 'polynomials', 'roots']"
45,How to compute the values of this function ? ( Fabius function ),How to compute the values of this function ? ( Fabius function ),,How to compute the values of this function ? ( Fabius function ) It is said not to be analytic but $C^\infty$ everywhere. But I do not even know how to compute its values. Im confused. Here is the link : http://www.math.osu.edu/~edgar.2/selfdiff/,How to compute the values of this function ? ( Fabius function ) It is said not to be analytic but $C^\infty$ everywhere. But I do not even know how to compute its values. Im confused. Here is the link : http://www.math.osu.edu/~edgar.2/selfdiff/,,"['complex-analysis', 'ordinary-differential-equations', 'taylor-expansion']"
46,Radius of convergence of power series,Radius of convergence of power series,,"Given a meromorphic function on $\mathbb{C}$, is the radius of convergence in a regular point exactly the distance to the closest pole? As Robert Israel points out in his answer, that this is of course an upper bound by the Cauchy-Hadamard principle. Theo Buehler in the comments gives a refernce for the non obvious direction: Remmert, Theory of complex functions, Chapter 7, §3, p.210ff (p. 164ff of my old German edition). Look for Cauchy-Taylor.","Given a meromorphic function on $\mathbb{C}$, is the radius of convergence in a regular point exactly the distance to the closest pole? As Robert Israel points out in his answer, that this is of course an upper bound by the Cauchy-Hadamard principle. Theo Buehler in the comments gives a refernce for the non obvious direction: Remmert, Theory of complex functions, Chapter 7, §3, p.210ff (p. 164ff of my old German edition). Look for Cauchy-Taylor.",,"['reference-request', 'complex-analysis', 'power-series']"
47,Is there an entire function with $f(\mathbb{Q}) \subset \mathbb{Q}$ and a non-finite power series representation having only rational Coeffitients,Is there an entire function with  and a non-finite power series representation having only rational Coeffitients,f(\mathbb{Q}) \subset \mathbb{Q},"I'm trying to answer the following question: Is there an entire function $f(z) := \sum \limits_{n=0}^\infty c_nz^n$ such that $f(\mathbb{Q}) \subset \mathbb{Q}$ $\forall n: c_n \in \mathbb{Q}$ $f$ is not a polynomial ? I'm trying to show that no such function exists. Here's why I think so: Assuming such a function existed. We would get $f(10^k) \in \mathbb{Q}$ for all $k \in \mathbb{Z}$. So the decimal representation of $f(10^k)$ either cuts at some digit or consists of repeating digits. Now my gut is telling me that if this is true for $f(10^n)$ with $n \in \mathbb{N}$, it won't be for $f(10^{-n}).$ (e.g. for $c_n$ with a finite digit representation: that's because the number of zeroes between each non-zero digit would increase indefinitely) But, is this correct at all? And if so, how do I show it rigorously?","I'm trying to answer the following question: Is there an entire function $f(z) := \sum \limits_{n=0}^\infty c_nz^n$ such that $f(\mathbb{Q}) \subset \mathbb{Q}$ $\forall n: c_n \in \mathbb{Q}$ $f$ is not a polynomial ? I'm trying to show that no such function exists. Here's why I think so: Assuming such a function existed. We would get $f(10^k) \in \mathbb{Q}$ for all $k \in \mathbb{Z}$. So the decimal representation of $f(10^k)$ either cuts at some digit or consists of repeating digits. Now my gut is telling me that if this is true for $f(10^n)$ with $n \in \mathbb{N}$, it won't be for $f(10^{-n}).$ (e.g. for $c_n$ with a finite digit representation: that's because the number of zeroes between each non-zero digit would increase indefinitely) But, is this correct at all? And if so, how do I show it rigorously?",,"['complex-analysis', 'power-series', 'entire-functions', 'transcendental-functions']"
48,New Definition of Convex Hull,New Definition of Convex Hull,,"I have always defined the convex hull of a set $X$ to be the smallest convex set that contains $X$. I am currently reading about holomorphic convexity, and the author has introduced a new definition of the convex hull. That is, the convex hull of a set $X$ is the intersection of all half spaces containing $X$, or, in other words, it is the set of points at which any real linear function takes values not exceeding its maximum on $X$. Can someone help characterise the equivalence of these three definitions?","I have always defined the convex hull of a set $X$ to be the smallest convex set that contains $X$. I am currently reading about holomorphic convexity, and the author has introduced a new definition of the convex hull. That is, the convex hull of a set $X$ is the intersection of all half spaces containing $X$, or, in other words, it is the set of points at which any real linear function takes values not exceeding its maximum on $X$. Can someone help characterise the equivalence of these three definitions?",,"['complex-analysis', 'geometry']"
49,Which role does the $\frac{1}{24}$ in the Dedekind $\eta$-function play?,Which role does the  in the Dedekind -function play?,\frac{1}{24} \eta,"The Dedekind $\eta$-function is defined as $$\eta(z) = q^{\frac{1}{24}} \prod_{n = 1}^\infty (1 - q^n)^{-1}$$ where $q = e^{2 \pi i z}$. My question is: If I start with the Euler-product $\prod_{n = 1}^\infty (1 - q^n)^{-1}$, how do I come to the point where multiplication with $q^{\frac{1}{24}}$ makes sense? Thanks!","The Dedekind $\eta$-function is defined as $$\eta(z) = q^{\frac{1}{24}} \prod_{n = 1}^\infty (1 - q^n)^{-1}$$ where $q = e^{2 \pi i z}$. My question is: If I start with the Euler-product $\prod_{n = 1}^\infty (1 - q^n)^{-1}$, how do I come to the point where multiplication with $q^{\frac{1}{24}}$ makes sense? Thanks!",,"['complex-analysis', 'number-theory', 'dedekind-eta-function']"
50,Is there a good way to solve for z the equation $e^{i\pi} = e^{z\ln2} + e^{z\ln3}$?,Is there a good way to solve for z the equation ?,e^{i\pi} = e^{z\ln2} + e^{z\ln3},$e^{i\pi} = e^{z\ln2}  +  e^{z\ln3}$ How can I deal with this? I want to solve for z. Does this help? $e^{z\ln2}  +  e^{z\ln3} = e^{z\ln2}(1  +  e^{z(ln3-ln2)})$ If I write out z=x+iy then the expression becomes $-1 = e^{x\ln2}e^{iy\ln2}+e^{x\ln3}e^{iy\ln3}$,$e^{i\pi} = e^{z\ln2}  +  e^{z\ln3}$ How can I deal with this? I want to solve for z. Does this help? $e^{z\ln2}  +  e^{z\ln3} = e^{z\ln2}(1  +  e^{z(ln3-ln2)})$ If I write out z=x+iy then the expression becomes $-1 = e^{x\ln2}e^{iy\ln2}+e^{x\ln3}e^{iy\ln3}$,,['complex-analysis']
51,Projective closure of an algebraic curve as a compactification of Riemann surface,Projective closure of an algebraic curve as a compactification of Riemann surface,,"Assume $f \in \mathbb{C}[x,y]$ a polynomial such that the affine algebraic curve $X=V(f)$ has no singular points. Then there is a natural structure of non-compact Riemann surface on $X$, which can be made into compact Riemann surface by adding several (finitely many) points. Question: Is this compactification the same thing as taking projective closure of the curve $X$? If so, how does one generally define the holomorphic maps in the neighborhoods of the ""points at infinity""? Up until now I have thought so. However, I came across the following example (I will further assume that the projective closure is indeed the compactification): Consider a polynomial $$f(x,y)=x^ 2-g(y), $$ where $g(y)$ is a complex polynomial of an even degree $k, \; k>2$ and, for simplicity's sake, leading coefficient $1$. Assume further that $g$ has $k$ distinct roots. Say I want to compute the genus of the compactification of $V(f)$. Then the projective closure of $V(f)$ is $V_{proj}(f^{*}),$ where $$f^{*}(x,y,z)=x^2z^{k-2}-y^k-(\text{other monomials of }g\text{ multiplied by some nonzero power of }z)$$ Now I want to compute the points at infinity, this leads to the equation $y^k=0,$ hence $y=0$ and thus, there is only one such point: $(1:0:0)$. However, consider the holomorphic map $\pi: V_{proj}(f^*) \rightarrow \mathbb{S}$ defined by $\pi(x:y:1)=y, \pi(1:0:0)=\infty$. Then it is easy to compute that the degree of $\pi$ is $2$ and that $b(\pi)=k+1$ (where  $b(\pi):= \sum_{P \in V_{proj}(f^*)}(e_P-1)$ and $e_P$ denotes the ramification index at the point $P$). So by Riemann-Hurwitz formula I get $$g(V_{proj}(f^*))=1+(g(\mathbb{S})-1)\deg \pi +\frac{1}{2}b(\pi)=\frac{k+1}{2}-1,$$ which is not an integer. (Note that if tha considered curve had two points at infinity, the number $b(\pi)$ would be even and everything would work fine). So additional question is: If the compactification can really be obtained via the projective closure, where is the mistake in the previous example? Thanks in advance for any help.","Assume $f \in \mathbb{C}[x,y]$ a polynomial such that the affine algebraic curve $X=V(f)$ has no singular points. Then there is a natural structure of non-compact Riemann surface on $X$, which can be made into compact Riemann surface by adding several (finitely many) points. Question: Is this compactification the same thing as taking projective closure of the curve $X$? If so, how does one generally define the holomorphic maps in the neighborhoods of the ""points at infinity""? Up until now I have thought so. However, I came across the following example (I will further assume that the projective closure is indeed the compactification): Consider a polynomial $$f(x,y)=x^ 2-g(y), $$ where $g(y)$ is a complex polynomial of an even degree $k, \; k>2$ and, for simplicity's sake, leading coefficient $1$. Assume further that $g$ has $k$ distinct roots. Say I want to compute the genus of the compactification of $V(f)$. Then the projective closure of $V(f)$ is $V_{proj}(f^{*}),$ where $$f^{*}(x,y,z)=x^2z^{k-2}-y^k-(\text{other monomials of }g\text{ multiplied by some nonzero power of }z)$$ Now I want to compute the points at infinity, this leads to the equation $y^k=0,$ hence $y=0$ and thus, there is only one such point: $(1:0:0)$. However, consider the holomorphic map $\pi: V_{proj}(f^*) \rightarrow \mathbb{S}$ defined by $\pi(x:y:1)=y, \pi(1:0:0)=\infty$. Then it is easy to compute that the degree of $\pi$ is $2$ and that $b(\pi)=k+1$ (where  $b(\pi):= \sum_{P \in V_{proj}(f^*)}(e_P-1)$ and $e_P$ denotes the ramification index at the point $P$). So by Riemann-Hurwitz formula I get $$g(V_{proj}(f^*))=1+(g(\mathbb{S})-1)\deg \pi +\frac{1}{2}b(\pi)=\frac{k+1}{2}-1,$$ which is not an integer. (Note that if tha considered curve had two points at infinity, the number $b(\pi)$ would be even and everything would work fine). So additional question is: If the compactification can really be obtained via the projective closure, where is the mistake in the previous example? Thanks in advance for any help.",,"['complex-analysis', 'algebraic-geometry', 'riemann-surfaces']"
52,"$\int_0^\infty \frac{\cos(tx)}{(x^2 - 2x + 2)}\,\mathrm{d}x$ for $t$ real",for  real,"\int_0^\infty \frac{\cos(tx)}{(x^2 - 2x + 2)}\,\mathrm{d}x t","This was a question on an old prelim exam in complex analysis: compute $$\int_0^\infty \frac{\cos(tx)}{x^2 - 2x + 2}\,\mathrm{d}x$$ for $t$ real. I've tried… Residue calculus—it's easy to integrate the similar $\int_0^\infty \frac{\cos(tx)}{x^2+2} \mathrm{d}x$ largely because the integrand is even, but this integrand isn't. Similarly $\int_0^\infty \frac{\sin(tx)}{x^2+2} \mathrm{d}x$ seems hard. Even if the original integral was from $1$ to $\infty$, so the denominator was even about $x=1$, we seem to need this latter, hard integral involving $\sin(tx)$. Mathematica—even for $t=1$, it gives the answer in terms of $$\int_0^z \frac{\sin(t)}{t}\,\mathrm dt \quad \text{and}\quad \int_0^z \dfrac{\cos(t)}{t}\,\mathrm dt,$$ which is not helpful. Looked through Gamelin's Complex Analysis text for inspiration; everything close used even or odd integrands. Googling/searching here, though it's hard to search for such a specific type of integral. There's a chance there's just a typo on the old prelim, for what it's worth.","This was a question on an old prelim exam in complex analysis: compute $$\int_0^\infty \frac{\cos(tx)}{x^2 - 2x + 2}\,\mathrm{d}x$$ for $t$ real. I've tried… Residue calculus—it's easy to integrate the similar $\int_0^\infty \frac{\cos(tx)}{x^2+2} \mathrm{d}x$ largely because the integrand is even, but this integrand isn't. Similarly $\int_0^\infty \frac{\sin(tx)}{x^2+2} \mathrm{d}x$ seems hard. Even if the original integral was from $1$ to $\infty$, so the denominator was even about $x=1$, we seem to need this latter, hard integral involving $\sin(tx)$. Mathematica—even for $t=1$, it gives the answer in terms of $$\int_0^z \frac{\sin(t)}{t}\,\mathrm dt \quad \text{and}\quad \int_0^z \dfrac{\cos(t)}{t}\,\mathrm dt,$$ which is not helpful. Looked through Gamelin's Complex Analysis text for inspiration; everything close used even or odd integrands. Googling/searching here, though it's hard to search for such a specific type of integral. There's a chance there's just a typo on the old prelim, for what it's worth.",,"['complex-analysis', 'residue-calculus']"
53,Harmonic functions with zeros on two lines,Harmonic functions with zeros on two lines,,"For which pairs of lines $L_1$, $L_2$ do there exist real functions, harmonic in the whole plane, that are $0$ at all points of $L_1 \cup L_2$ without vanishing identically? Note: This is self-study -- not homework. My thoughts: I tried to exploit certain configurations of $L_1$ and $L_2$ and apply the reflection principle to show that the function vanishes on a closed curve and hence everywhere, but I wasn't successful. OK, I think I have a working argument. Let $u$ be a real harmonic function that vanishes on two lines $L_1$ and $L_2$. Apply the necessary translation and/or rotation to make $L_1$ match $y = 0$ and the intersection point (if any) match (0, 0). This will translate/rotate the set of zeros of $u$, but it won't remove or introduce new zeros. The resulting function will remain real and harmonic. If $L_1$ and $L_2$ are parallel, then $L_2$ is of the form $y = a$ and, by a direct computation of the Laplacian, the following function satisfies the requirements: $$u(x, y) = e^x \sin\left(\frac{2\pi y}{a}\right)$$ If $L_1$ and $L_2$ intersect at an angle $\theta = 2\pi\frac{p}{q}$ where $p, q \in \Bbb N, q \ne 0$, the following function satisfies the requirements: $$u(x, y) = \prod_{k=0}^{q-1}\left(y-x \tan\left(2\pi k \frac{p}{q}\right)\right)$$ Otherwise, $L_1$ and $L_2$ intersect at an angle $\theta = 2 \pi s$ where $s \not \in \Bbb Q$. By the Schwarz reflection principle, the function also vanishes on $\overline L_2$. With repeated rotations and applications of the reflection principle, we find that a rotation of the function has zeros in the following set: $$y = x \tan(2 \pi s n), \ n \in \Bbb N$$ But the set is dense because $s \not \in \Bbb Q$. Hence the function is identically zero. My question: Any mistakes in my argument? Is there an easier way?","For which pairs of lines $L_1$, $L_2$ do there exist real functions, harmonic in the whole plane, that are $0$ at all points of $L_1 \cup L_2$ without vanishing identically? Note: This is self-study -- not homework. My thoughts: I tried to exploit certain configurations of $L_1$ and $L_2$ and apply the reflection principle to show that the function vanishes on a closed curve and hence everywhere, but I wasn't successful. OK, I think I have a working argument. Let $u$ be a real harmonic function that vanishes on two lines $L_1$ and $L_2$. Apply the necessary translation and/or rotation to make $L_1$ match $y = 0$ and the intersection point (if any) match (0, 0). This will translate/rotate the set of zeros of $u$, but it won't remove or introduce new zeros. The resulting function will remain real and harmonic. If $L_1$ and $L_2$ are parallel, then $L_2$ is of the form $y = a$ and, by a direct computation of the Laplacian, the following function satisfies the requirements: $$u(x, y) = e^x \sin\left(\frac{2\pi y}{a}\right)$$ If $L_1$ and $L_2$ intersect at an angle $\theta = 2\pi\frac{p}{q}$ where $p, q \in \Bbb N, q \ne 0$, the following function satisfies the requirements: $$u(x, y) = \prod_{k=0}^{q-1}\left(y-x \tan\left(2\pi k \frac{p}{q}\right)\right)$$ Otherwise, $L_1$ and $L_2$ intersect at an angle $\theta = 2 \pi s$ where $s \not \in \Bbb Q$. By the Schwarz reflection principle, the function also vanishes on $\overline L_2$. With repeated rotations and applications of the reflection principle, we find that a rotation of the function has zeros in the following set: $$y = x \tan(2 \pi s n), \ n \in \Bbb N$$ But the set is dense because $s \not \in \Bbb Q$. Hence the function is identically zero. My question: Any mistakes in my argument? Is there an easier way?",,"['complex-analysis', 'harmonic-functions']"
54,How to imagine zeros of an analytic function of several variables,How to imagine zeros of an analytic function of several variables,,"Let $f(z_1,\cdots, z_n)$ be a holomorphic function of several variables in an open subset of $\mathcal C^n$. Let $Z(f)=\{ (z_1,\cdots, z_n) \: | \: f=0\}$ be the zero set of $f$. If $n=1$, the zeros set consists of isolated points. How to generalize this to  $n$ dimensions? I know that $Z(f)$ has zero Lebesgue measure in $2n$ dimensional space. But this does not help: for $n=1$ it includes both isolated points and line segments. Can the concept of limit/isolated/accumulation point be usefully generalized in this context? What's the best way to characterize $Z(f)$?","Let $f(z_1,\cdots, z_n)$ be a holomorphic function of several variables in an open subset of $\mathcal C^n$. Let $Z(f)=\{ (z_1,\cdots, z_n) \: | \: f=0\}$ be the zero set of $f$. If $n=1$, the zeros set consists of isolated points. How to generalize this to  $n$ dimensions? I know that $Z(f)$ has zero Lebesgue measure in $2n$ dimensional space. But this does not help: for $n=1$ it includes both isolated points and line segments. Can the concept of limit/isolated/accumulation point be usefully generalized in this context? What's the best way to characterize $Z(f)$?",,"['complex-analysis', 'roots', 'analyticity', 'several-complex-variables']"
55,Is there an entire function with domains for which $f(A)=B$ and $f(B)=A$?,Is there an entire function with domains for which  and ?,f(A)=B f(B)=A,"Let $f$ be an entire function. Suppose that there exist two nonempty disjoint, open, connected non-empty sets $A,B$ in the plane such that $f(A)=B$ and $f(B)=A$ . Does it follow that $f$ is linear? Equivalently, if a meromorphic function satisfies this condition is it necessarily an automorphism? Neither of the conditions of disjointness and openness can be dropped, of course. I tried to see if results in dynamics about 2-periodic domains apply, but they usually only regard Fatou components or are otherwise not suitable. But it does seem like a question simple enough that it ""ought to"" be amenable to such machinery. Any ideas?","Let be an entire function. Suppose that there exist two nonempty disjoint, open, connected non-empty sets in the plane such that and . Does it follow that is linear? Equivalently, if a meromorphic function satisfies this condition is it necessarily an automorphism? Neither of the conditions of disjointness and openness can be dropped, of course. I tried to see if results in dynamics about 2-periodic domains apply, but they usually only regard Fatou components or are otherwise not suitable. But it does seem like a question simple enough that it ""ought to"" be amenable to such machinery. Any ideas?","f A,B f(A)=B f(B)=A f","['complex-analysis', 'complex-dynamics']"
56,What Is the meaning of the residue of a complex function?,What Is the meaning of the residue of a complex function?,,"Studying complex analysis I saw that, in many cases, the residue's theorem comes really handful. I learned how to find it and how to use it, but I didn't quite understand what it really means ""geometrically"", if this makes any sense. Is there an intuitive way to explain what a residue is, or is it just a mathematical tool?","Studying complex analysis I saw that, in many cases, the residue's theorem comes really handful. I learned how to find it and how to use it, but I didn't quite understand what it really means ""geometrically"", if this makes any sense. Is there an intuitive way to explain what a residue is, or is it just a mathematical tool?",,"['complex-analysis', 'complex-integration', 'laurent-series']"
57,Complex Analysis textbook - specific criteria,Complex Analysis textbook - specific criteria,,"I'm looking for a text (textbook, lecture notes etc.) on Complex Analysis that meets some very specific desiderata. I've already searched through books recommended here and on MathOverflow, but so far I haven't found anything to suit my needs. (I should mention that I took a one-semester course in C.A. two years ago which was presented in this way, so I may be a little bit partial here.) Firstly, the terms ""holomorphic"" and ""analytic"" should not be used interchangeably. Although there is no mathematical mistake as long as the power series expansion theorem is not implicitly assumed, it's good to have some distinction of meaning. Complex integrals should be done in their general form, i.e. with Riemann sums over arbitrary (rectifiable) curves, not just over $\mathcal{C}^1$ curves (with the integral defined as $\int_a^b f(\gamma(t)) \gamma^\prime(t)\;\mathrm{d}t$). Definitions (like the integral one above) should emphasise the conceptual side of a notion, not the computational side. E.g. in the course I took the winding number was defined like this , not like this . The Cauchy integral theorems should be presented using homotopy/homology theories. (As a counterexample, the Stein/Shakarchi book proves them only on particular cases of contours.) It would be nice to have short introductions to topics which stem from complex function theory - like sheaf theory, Riemann surfaces or analytic number theory - but I think that I already narrowed the answer space too much. What can you recommend?","I'm looking for a text (textbook, lecture notes etc.) on Complex Analysis that meets some very specific desiderata. I've already searched through books recommended here and on MathOverflow, but so far I haven't found anything to suit my needs. (I should mention that I took a one-semester course in C.A. two years ago which was presented in this way, so I may be a little bit partial here.) Firstly, the terms ""holomorphic"" and ""analytic"" should not be used interchangeably. Although there is no mathematical mistake as long as the power series expansion theorem is not implicitly assumed, it's good to have some distinction of meaning. Complex integrals should be done in their general form, i.e. with Riemann sums over arbitrary (rectifiable) curves, not just over $\mathcal{C}^1$ curves (with the integral defined as $\int_a^b f(\gamma(t)) \gamma^\prime(t)\;\mathrm{d}t$). Definitions (like the integral one above) should emphasise the conceptual side of a notion, not the computational side. E.g. in the course I took the winding number was defined like this , not like this . The Cauchy integral theorems should be presented using homotopy/homology theories. (As a counterexample, the Stein/Shakarchi book proves them only on particular cases of contours.) It would be nice to have short introductions to topics which stem from complex function theory - like sheaf theory, Riemann surfaces or analytic number theory - but I think that I already narrowed the answer space too much. What can you recommend?",,"['complex-analysis', 'reference-request']"
58,Entire function with uncountably many zeros,Entire function with uncountably many zeros,,Suppose that an entire function $f$ has uncountably many zeros. Is it true that $f=0$? I have no idea how to proceed with this. Perhaps some theorem that I am not aware of. I have done an undergraduate course in complex analysis.,Suppose that an entire function $f$ has uncountably many zeros. Is it true that $f=0$? I have no idea how to proceed with this. Perhaps some theorem that I am not aware of. I have done an undergraduate course in complex analysis.,,"['complex-analysis', 'complex-numbers']"
59,Why is the bailout value of the Mandelbrot set 2?,Why is the bailout value of the Mandelbrot set 2?,,"For the past few days I've been studying the Mandelbrot set, and many say that if the iterations of a point stay within a magnitude of 2, the point converges. A very natural question of ""why is the bailout value 2?"" came to me. Is there a proof that the value will diverge if the iterations do not stay within the bailout value of 2?","For the past few days I've been studying the Mandelbrot set, and many say that if the iterations of a point stay within a magnitude of 2, the point converges. A very natural question of ""why is the bailout value 2?"" came to me. Is there a proof that the value will diverge if the iterations do not stay within the bailout value of 2?",,"['complex-analysis', 'complex-numbers', 'fractals']"
60,Regularity of root spacing of $G(z)=\sum_{n=1}^{\infty} \frac{e^{-n^{2}}}{n^{z}}$,Regularity of root spacing of,G(z)=\sum_{n=1}^{\infty} \frac{e^{-n^{2}}}{n^{z}},"Define, on $\mathbb{C}$: $$G(z)=\sum_{n=1}^{\infty} \frac{e^{-n^{2}}}{n^{z}}$$ A domain colored portrait of $G(z)$ (boxes are supposed to be negative signs): suggests that the roots of $G(z)$ are equally spaced along lines of fixed real component. So: Are the roots of $G(z)$ regularly spaced for some fixed $\Re(z)$? and if so: Are there analytic expressions for the roots? Can $G(z)$ expressed as a Weierstrass product?","Define, on $\mathbb{C}$: $$G(z)=\sum_{n=1}^{\infty} \frac{e^{-n^{2}}}{n^{z}}$$ A domain colored portrait of $G(z)$ (boxes are supposed to be negative signs): suggests that the roots of $G(z)$ are equally spaced along lines of fixed real component. So: Are the roots of $G(z)$ regularly spaced for some fixed $\Re(z)$? and if so: Are there analytic expressions for the roots? Can $G(z)$ expressed as a Weierstrass product?",,"['complex-analysis', 'roots', 'dirichlet-series']"
61,holomorphic functions and fixed points,holomorphic functions and fixed points,,"I'm studying for a complex analysis exam, and I'm stuck on this problem from an old exam: Let $g$ be a holomorphic function on $|z|<R,R>1$, with $|g(z)|\leq 1$ for all $|z|\leq 1$. (a) Show that for all $t\in C$ with $|t|<1$, the equation $$z=tg(z)$$ has a unique solution $z=s(t)$ in the disc $|z|<1$. (b) Show that $t\mapsto s(t)$ is a holomorphic function on the disc $|t|<1$. (Hint: find an integral formula for $s$.)","I'm studying for a complex analysis exam, and I'm stuck on this problem from an old exam: Let $g$ be a holomorphic function on $|z|<R,R>1$, with $|g(z)|\leq 1$ for all $|z|\leq 1$. (a) Show that for all $t\in C$ with $|t|<1$, the equation $$z=tg(z)$$ has a unique solution $z=s(t)$ in the disc $|z|<1$. (b) Show that $t\mapsto s(t)$ is a holomorphic function on the disc $|t|<1$. (Hint: find an integral formula for $s$.)",,['complex-analysis']
62,"Formula for decomposing a form into $(p,q)$ forms",Formula for decomposing a form into  forms,"(p,q)","Let $L: \mathbb{C}^n \to \mathbb{C}$ be a real linear map.  In other words, $L(a\vec{v}_1+b\vec{v_2}) = aL(\vec{v}_1)+bL(\vec{v}_2)$ for all $a,b \in \mathbb{R}$.  Then $L$ decomposes uniquely into a complex linear $T$ map and a complex antilinear map $\overline{T}$.  They have the formulas $$ \begin{align*}        T(\vec{v}) &= \frac{1}{2}\left( L(\vec{v}) - i L(i\vec{v})\right)     \\ \overline{T}(\vec{v}) &= \frac{1}{2}\left( L(\vec{v}) + iL(i\vec{v})\right)     \end{align*} $$ A real $k-$linear form $\omega:(\mathbb{C}^n)^k \to \mathbb{C}$ can similarly be decomposed into a sum of forms $\omega^{(p,q)}$ for which $\omega^{(p,q)}(z\vec{v}_1,z\vec{v}_2,...,z\vec{v}_k) = z^p\overline{z}^q\omega^{(p,q)}(\vec{v}_1,\vec{v}_2,...,\vec{v}_k)$. When this construction is applied to differential forms, we obtain the so called $(p,q)-$forms, which are very important in complex geometry. I have a question of linear algebra, or maybe combinatorics.  What is a formula for $\omega^{(p,q)}$ in terms of $\omega$?  For example, I have figured out that $$ \omega^{(1,1)}(\vec{v}_1,\vec{v}_2) = \frac{1}{2}\left(\omega(\vec{v}_1,\vec{v}_2)+\omega(i\vec{v}_1,i\vec{v}_2)\right) $$ I have a similar kind of formula for $\omega^{(2,0)}$, but it is rather ugly, and relies on getting the above formula first. Does anyone have a pretty formula for $\omega^{p,q}$ in terms of $\omega$? Also, the proof that I know that this decomposition holds is heavily basis dependent.  Does anyone have a clean basis free proof?  Maybe someone with a better handle on tensor algebra can help m out here. You may also want to look at this question When is a $k$-form a $(p, q)$-form? for further background.","Let $L: \mathbb{C}^n \to \mathbb{C}$ be a real linear map.  In other words, $L(a\vec{v}_1+b\vec{v_2}) = aL(\vec{v}_1)+bL(\vec{v}_2)$ for all $a,b \in \mathbb{R}$.  Then $L$ decomposes uniquely into a complex linear $T$ map and a complex antilinear map $\overline{T}$.  They have the formulas $$ \begin{align*}        T(\vec{v}) &= \frac{1}{2}\left( L(\vec{v}) - i L(i\vec{v})\right)     \\ \overline{T}(\vec{v}) &= \frac{1}{2}\left( L(\vec{v}) + iL(i\vec{v})\right)     \end{align*} $$ A real $k-$linear form $\omega:(\mathbb{C}^n)^k \to \mathbb{C}$ can similarly be decomposed into a sum of forms $\omega^{(p,q)}$ for which $\omega^{(p,q)}(z\vec{v}_1,z\vec{v}_2,...,z\vec{v}_k) = z^p\overline{z}^q\omega^{(p,q)}(\vec{v}_1,\vec{v}_2,...,\vec{v}_k)$. When this construction is applied to differential forms, we obtain the so called $(p,q)-$forms, which are very important in complex geometry. I have a question of linear algebra, or maybe combinatorics.  What is a formula for $\omega^{(p,q)}$ in terms of $\omega$?  For example, I have figured out that $$ \omega^{(1,1)}(\vec{v}_1,\vec{v}_2) = \frac{1}{2}\left(\omega(\vec{v}_1,\vec{v}_2)+\omega(i\vec{v}_1,i\vec{v}_2)\right) $$ I have a similar kind of formula for $\omega^{(2,0)}$, but it is rather ugly, and relies on getting the above formula first. Does anyone have a pretty formula for $\omega^{p,q}$ in terms of $\omega$? Also, the proof that I know that this decomposition holds is heavily basis dependent.  Does anyone have a clean basis free proof?  Maybe someone with a better handle on tensor algebra can help m out here. You may also want to look at this question When is a $k$-form a $(p, q)$-form? for further background.",,"['complex-analysis', 'complex-geometry', 'multilinear-algebra', 'several-complex-variables']"
63,Hadamard Product of $e^z-1$,Hadamard Product of,e^z-1,"Assuming the formulas $$\sin(z)=\frac{e^{iz}-e^{-iz}}{2i}\quad\text{and}\quad\frac{\sin(\pi z)}{\pi z}=\prod_{n=1}^\infty\left(1-\frac{z^2}{n^2}\right),$$ I want to find the Hadamard product of $e^z-1$ . I think I've done the work correctly, but I end up having an extra factor of $2\pi i$ at the end (this is from Stein and Shakarchi's book and they list the answer for this particular problem). So, my work is \begin{align} e^z-1&=e^{z/2}\left(e^{z/2}-e^{-z/2}\right)\\ \\ &=e^{z/2}\left(e^{i(-iz/2)}-e^{-i(-iz/2)}\right)\\ \\ &=2ie^{z/2}\left(\frac{e^{i(-iz/2)}-e^{-i(-iz/2)}}{2i}\right)\\ \\ &=2ie^{z/2}\sin(-iz/2)\\ \\ &=2\pi i ze^{z/2}\frac{\sin(-iz/2)}{\pi z}\\ \\ &=2\pi i ze^{z/2}\frac{\sin\left(\pi(\frac{-iz}{2\pi})\right)}{\pi z}\tag{$\ast$}\\ \\ &=2\pi i ze^{z/2}\prod_{n=1}^\infty\left(1-\frac{\left(\frac{-iz}{2\pi}\right)^2}{n^2}\right)\\ \\ &=2\pi i ze^{z/2}\prod_{n=1}^\infty\left(1+\frac{z^2}{4n^2\pi^2}\right). \end{align} The answer according to the book is $$e^z-1=e^{z/2}z\prod_{n=1}^\infty\left(1+\frac{z^2}{4n^2\pi^2}\right).$$ Have I made a simple mistake in calculation, or are the two somehow compatible with each other? Update: Thomas Andrews has pointed out a mistake in my infinite product formula. I've corrected that mistake. $(\ast)$ The error occurs from this line to the next. Instead of multiplying and dividing by $z$ , I should have multiplied and divided by $\frac{-iz}{2\pi}$ . That would cause the appropriate cancellation.","Assuming the formulas I want to find the Hadamard product of . I think I've done the work correctly, but I end up having an extra factor of at the end (this is from Stein and Shakarchi's book and they list the answer for this particular problem). So, my work is The answer according to the book is Have I made a simple mistake in calculation, or are the two somehow compatible with each other? Update: Thomas Andrews has pointed out a mistake in my infinite product formula. I've corrected that mistake. The error occurs from this line to the next. Instead of multiplying and dividing by , I should have multiplied and divided by . That would cause the appropriate cancellation.","\sin(z)=\frac{e^{iz}-e^{-iz}}{2i}\quad\text{and}\quad\frac{\sin(\pi z)}{\pi z}=\prod_{n=1}^\infty\left(1-\frac{z^2}{n^2}\right), e^z-1 2\pi i \begin{align}
e^z-1&=e^{z/2}\left(e^{z/2}-e^{-z/2}\right)\\ \\
&=e^{z/2}\left(e^{i(-iz/2)}-e^{-i(-iz/2)}\right)\\ \\
&=2ie^{z/2}\left(\frac{e^{i(-iz/2)}-e^{-i(-iz/2)}}{2i}\right)\\ \\
&=2ie^{z/2}\sin(-iz/2)\\ \\
&=2\pi i ze^{z/2}\frac{\sin(-iz/2)}{\pi z}\\ \\
&=2\pi i ze^{z/2}\frac{\sin\left(\pi(\frac{-iz}{2\pi})\right)}{\pi z}\tag{\ast}\\ \\
&=2\pi i ze^{z/2}\prod_{n=1}^\infty\left(1-\frac{\left(\frac{-iz}{2\pi}\right)^2}{n^2}\right)\\ \\
&=2\pi i ze^{z/2}\prod_{n=1}^\infty\left(1+\frac{z^2}{4n^2\pi^2}\right).
\end{align} e^z-1=e^{z/2}z\prod_{n=1}^\infty\left(1+\frac{z^2}{4n^2\pi^2}\right). (\ast) z \frac{-iz}{2\pi}","['complex-analysis', 'hadamard-product']"
64,Polynomial bounded real part of an entire function,Polynomial bounded real part of an entire function,,"Let $f(z)$ be an entire function whose real part is bounded by a polynomial in $|z|$. Does it follow that $f(z)$ is a polynomial? Or, without loss of generality and more suggestively $$(f(z)=\sum_{k=0}^\infty a_k z^k \quad \land \quad \operatorname{Re}(f(z))\leq |z|^n)\quad \forall z\in \mathbb{C} \qquad \Rightarrow \qquad f(z)=\sum_{k=0}^n a_k z^k \quad\forall z\in \mathbb{C}$$","Let $f(z)$ be an entire function whose real part is bounded by a polynomial in $|z|$. Does it follow that $f(z)$ is a polynomial? Or, without loss of generality and more suggestively $$(f(z)=\sum_{k=0}^\infty a_k z^k \quad \land \quad \operatorname{Re}(f(z))\leq |z|^n)\quad \forall z\in \mathbb{C} \qquad \Rightarrow \qquad f(z)=\sum_{k=0}^n a_k z^k \quad\forall z\in \mathbb{C}$$",,['complex-analysis']
65,Proof of Hartogs's theorem,Proof of Hartogs's theorem,,"I'd be very grateful if someone could help me understand the proof of Hartogs's theorem appearing in Huybrechts' ""Complex Geometry."" The statement is: Let $\mathbb{P}^n \subset \mathbb{C}^n$ be the unit polydisc. Let $\mathbb{P}_c:= \{ \boldsymbol{z} : 0\leq |z_i|<c\}$ for some $0<c<1$.  Then if $f: \mathbb{P}^n -\bar{\mathbb{P}}_c \rightarrow \mathbb{C}$ is holomorphic, then $f$ extends to a holomorphic function on $\mathbb{P}$. The proof goes as follows: For fixed $\boldsymbol{w} \in \mathbb{P}^{n-1}$, the function $f_w: z \mapsto f(z,\boldsymbol{w})$ defines a holomorphic function on the annular region $A= \{z: c<|z|<1 \}$ in the complex plain. Let $f_w= \Sigma_{k=-\infty}^{\infty} a_k(\boldsymbol{w})z^k$ be the Laurent expansion of $f_w$ in this region.  Then the $a_k$ define holomorphic functions on $\mathbb{P}^{n-1}$ (by a previous lemma) and if $k<0$ then $a_k$ vanishes whenever some $w_i>c$ (since $f_w$ then extends to the whole disc) and so vanishes on all of $\mathbb{P}^{n-1}$.  Therefore we can write $f|_{A \times \mathbb{P}^{n-1}}= \Sigma_{k=0}^{\infty} a_k(\boldsymbol{w})z^k$. I understand everything up to this point.  What I can't understand is why this sum of holomorphic functions defines a holomorphic function on all of the unit polydisc. Presumably it is supposed to converge uniformly on compact subsets or something? Huybrechts says something like ""the $a_k$ attain their suprema on the boundary and so uniform convergence is implied by uniform convergence on the annular region"" and I have no idea what boundary or annular region he's talking about.  A priori I don't know why I should really know anything about the uniformity of the convergence of the sum outside of a single copy of $A$ i.e. when $\boldsymbol{w}$ is fixed. Thanks for your time and sorry if this is mostly nonsense.","I'd be very grateful if someone could help me understand the proof of Hartogs's theorem appearing in Huybrechts' ""Complex Geometry."" The statement is: Let $\mathbb{P}^n \subset \mathbb{C}^n$ be the unit polydisc. Let $\mathbb{P}_c:= \{ \boldsymbol{z} : 0\leq |z_i|<c\}$ for some $0<c<1$.  Then if $f: \mathbb{P}^n -\bar{\mathbb{P}}_c \rightarrow \mathbb{C}$ is holomorphic, then $f$ extends to a holomorphic function on $\mathbb{P}$. The proof goes as follows: For fixed $\boldsymbol{w} \in \mathbb{P}^{n-1}$, the function $f_w: z \mapsto f(z,\boldsymbol{w})$ defines a holomorphic function on the annular region $A= \{z: c<|z|<1 \}$ in the complex plain. Let $f_w= \Sigma_{k=-\infty}^{\infty} a_k(\boldsymbol{w})z^k$ be the Laurent expansion of $f_w$ in this region.  Then the $a_k$ define holomorphic functions on $\mathbb{P}^{n-1}$ (by a previous lemma) and if $k<0$ then $a_k$ vanishes whenever some $w_i>c$ (since $f_w$ then extends to the whole disc) and so vanishes on all of $\mathbb{P}^{n-1}$.  Therefore we can write $f|_{A \times \mathbb{P}^{n-1}}= \Sigma_{k=0}^{\infty} a_k(\boldsymbol{w})z^k$. I understand everything up to this point.  What I can't understand is why this sum of holomorphic functions defines a holomorphic function on all of the unit polydisc. Presumably it is supposed to converge uniformly on compact subsets or something? Huybrechts says something like ""the $a_k$ attain their suprema on the boundary and so uniform convergence is implied by uniform convergence on the annular region"" and I have no idea what boundary or annular region he's talking about.  A priori I don't know why I should really know anything about the uniformity of the convergence of the sum outside of a single copy of $A$ i.e. when $\boldsymbol{w}$ is fixed. Thanks for your time and sorry if this is mostly nonsense.",,"['complex-analysis', 'complex-geometry']"
66,Test to determine if a polynomial has only real roots?,Test to determine if a polynomial has only real roots?,,"Given a polynomial $p(x)=x^n + c_{n-1} x^{n-1} + \cdots + c_0$ with real coefficients $c_{n-1}, \ldots, c_0$ , is there an efficient method to determine whether all roots to the polynomial are real and not complex? If it helps, you may assume all of its $n$ roots are distinct. I know, for the quadratic case, the discriminant $c_1^2 - 4c_0>0$ is necessary and sufficient to determine if all roots are real.","Given a polynomial with real coefficients , is there an efficient method to determine whether all roots to the polynomial are real and not complex? If it helps, you may assume all of its roots are distinct. I know, for the quadratic case, the discriminant is necessary and sufficient to determine if all roots are real.","p(x)=x^n + c_{n-1} x^{n-1} + \cdots + c_0 c_{n-1}, \ldots, c_0 n c_1^2 - 4c_0>0","['complex-analysis', 'algebra-precalculus', 'algebraic-geometry', 'polynomials', 'roots']"
67,Best estimate using Cauchy integral formula: why is a circle the optimal path?,Best estimate using Cauchy integral formula: why is a circle the optimal path?,,"I once encountered this question from Ahlfors' Complex Analysis. An analytic function $f$ has the property that for $|z|<1$, $|f(z)|\leq \frac{1}{1-|z|}$. Find the best estimate of $|f^{(n)}(0)|$ that Cauchy's formula will yield. I solved it by finding an estimate via the Cauchy integral formula over a circle of general radius and then optimized the result by differentiating it with respect to the radius. I later wondered, why should we expect the optimal path to be a circle? I applied the Euler-Lagrange Equations to this problem and got a completely untractable O.D.E. I expect that a circle is the optimal path, but is there a concrete reason why? Edit Here is my application of the Euler-Lagrange equations: Suppose we want to integrate on a path $\gamma(t)=r(t)e^{i\theta(t)}$ for $0\leq t\leq 1$ with the stipulation the $\gamma(0)=\gamma(1)$ and $0<r(t)<1$. This leaves us with $|f(z)|<\frac{1}{1-r(t)}$, and $|dz|=|r'(t)+ir(t)\theta'(t)|dt.$ Then the problem is to minimize $$|f^{(n)}(0)|\leq\frac{n!}{2\pi}\int_0^1\frac{1}{r^{n+1}(t)(1-r(t))}|r'(t)+ir\theta'(t)|dt.$$ It is sufficient that we optimize the integrand. Let the integrand be written $$F(r,\theta,r',\theta')=\frac{\sqrt{r'(t)^2+r(t)^2\theta'(t)^2}}{r^{n+1}(t)(1-r(t))}.$$ I simply apply the variational derivative: $$\frac{\partial F}{\partial r}-\frac{d}{dt}\frac{\partial F}{\partial r'}=0,$$ $$\frac{d}{dt}\frac{\partial F}{\partial \theta'}=0.$$ As much as I hate to spoil the joy of working out the differential equations on your own, I have computed them form you: $$\frac{r(t)^{-n} \theta '(t) \left(r(t) \theta '(t) \left(r(t) \left(r''(t)-n \theta '(t)^2\right)+(n+1) r(t)^2 \theta '(t)^2-r''(t)\right)+(n r(t)-n+1) r'(t)^2 \theta '(t)-(r(t)-1) r(t) r'(t) \theta ''(t)\right)}{(r(t)-1)^2 \left(r'(t)^2+r(t)^2 \theta '(t)^2\right)^{3/2}}=0,$$ $$-\frac{r(t)^{-n} r'(t) \left(r(t) \theta '(t) \left(r(t) \left(r''(t)-n \theta '(t)^2\right)+(n+1) r(t)^2 \theta '(t)^2-r''(t)\right)+(n r(t)-n+1) r'(t)^2 \theta '(t)-(r(t)-1) r(t) r'(t) \theta ''(t)\right)}{(r(t)-1)^2 \left(r'(t)^2+r(t)^2 \theta '(t)^2\right)^{3/2}}=0$$ It is not too hard to see that these reduce to equations solved by the result for the circle assuming that $r'(t)=0$ and $\theta'(t)=\text{const}$. Thus, the circle I initially found is a stationary point, but it may not be unique. I am stymied at the prospect of finding other solutions.","I once encountered this question from Ahlfors' Complex Analysis. An analytic function $f$ has the property that for $|z|<1$, $|f(z)|\leq \frac{1}{1-|z|}$. Find the best estimate of $|f^{(n)}(0)|$ that Cauchy's formula will yield. I solved it by finding an estimate via the Cauchy integral formula over a circle of general radius and then optimized the result by differentiating it with respect to the radius. I later wondered, why should we expect the optimal path to be a circle? I applied the Euler-Lagrange Equations to this problem and got a completely untractable O.D.E. I expect that a circle is the optimal path, but is there a concrete reason why? Edit Here is my application of the Euler-Lagrange equations: Suppose we want to integrate on a path $\gamma(t)=r(t)e^{i\theta(t)}$ for $0\leq t\leq 1$ with the stipulation the $\gamma(0)=\gamma(1)$ and $0<r(t)<1$. This leaves us with $|f(z)|<\frac{1}{1-r(t)}$, and $|dz|=|r'(t)+ir(t)\theta'(t)|dt.$ Then the problem is to minimize $$|f^{(n)}(0)|\leq\frac{n!}{2\pi}\int_0^1\frac{1}{r^{n+1}(t)(1-r(t))}|r'(t)+ir\theta'(t)|dt.$$ It is sufficient that we optimize the integrand. Let the integrand be written $$F(r,\theta,r',\theta')=\frac{\sqrt{r'(t)^2+r(t)^2\theta'(t)^2}}{r^{n+1}(t)(1-r(t))}.$$ I simply apply the variational derivative: $$\frac{\partial F}{\partial r}-\frac{d}{dt}\frac{\partial F}{\partial r'}=0,$$ $$\frac{d}{dt}\frac{\partial F}{\partial \theta'}=0.$$ As much as I hate to spoil the joy of working out the differential equations on your own, I have computed them form you: $$\frac{r(t)^{-n} \theta '(t) \left(r(t) \theta '(t) \left(r(t) \left(r''(t)-n \theta '(t)^2\right)+(n+1) r(t)^2 \theta '(t)^2-r''(t)\right)+(n r(t)-n+1) r'(t)^2 \theta '(t)-(r(t)-1) r(t) r'(t) \theta ''(t)\right)}{(r(t)-1)^2 \left(r'(t)^2+r(t)^2 \theta '(t)^2\right)^{3/2}}=0,$$ $$-\frac{r(t)^{-n} r'(t) \left(r(t) \theta '(t) \left(r(t) \left(r''(t)-n \theta '(t)^2\right)+(n+1) r(t)^2 \theta '(t)^2-r''(t)\right)+(n r(t)-n+1) r'(t)^2 \theta '(t)-(r(t)-1) r(t) r'(t) \theta ''(t)\right)}{(r(t)-1)^2 \left(r'(t)^2+r(t)^2 \theta '(t)^2\right)^{3/2}}=0$$ It is not too hard to see that these reduce to equations solved by the result for the circle assuming that $r'(t)=0$ and $\theta'(t)=\text{const}$. Thus, the circle I initially found is a stationary point, but it may not be unique. I am stymied at the prospect of finding other solutions.",,['complex-analysis']
68,Is this generalization of an exercise in Stein true?,Is this generalization of an exercise in Stein true?,,"The following question is exercise $14$ in chapter $2$ in Stein and Shakarchi's Complex Analysis . Suppose that $f$ is holomorphic in an open set containing the closed unit disc, except for a pole at $z_0$ on the unit circle. Show that if $$\sum_{n=0}^\infty a_nz^n$$ denotes the power series expansion $f$ in the open unit disc, then $$\lim_{n\to\infty}\frac{a_n}{a_{n+1}}=z_0.$$ A solution may be found here or here . A discussion in the comments of one of the linked questions prompted me to ask another question. Question: Does this remain true when 'pole' is replaced by 'essential singularity'? I strongly suspect the answer is no, but I can't find a convenient counterexample. It seems like something like $e^{\frac{1}{z-1}}$ should provide a counterexample, but the calculus involved in computing the power series coefficients is very messy. I recall there being some standard power series that people always use to show that anything can happen on the boundary of a disk of convergence, but I can't find them at the moment. Perhaps one of those works?","The following question is exercise $14$ in chapter $2$ in Stein and Shakarchi's Complex Analysis . Suppose that $f$ is holomorphic in an open set containing the closed unit disc, except for a pole at $z_0$ on the unit circle. Show that if $$\sum_{n=0}^\infty a_nz^n$$ denotes the power series expansion $f$ in the open unit disc, then $$\lim_{n\to\infty}\frac{a_n}{a_{n+1}}=z_0.$$ A solution may be found here or here . A discussion in the comments of one of the linked questions prompted me to ask another question. Question: Does this remain true when 'pole' is replaced by 'essential singularity'? I strongly suspect the answer is no, but I can't find a convenient counterexample. It seems like something like $e^{\frac{1}{z-1}}$ should provide a counterexample, but the calculus involved in computing the power series coefficients is very messy. I recall there being some standard power series that people always use to show that anything can happen on the boundary of a disk of convergence, but I can't find them at the moment. Perhaps one of those works?",,"['complex-analysis', 'power-series']"
69,Existence of square roots and logarithms,Existence of square roots and logarithms,,Does there exist an open connected set in the complex plane on which the identity function has an analytic square root but not an analytic logarithm?,Does there exist an open connected set in the complex plane on which the identity function has an analytic square root but not an analytic logarithm?,,['complex-analysis']
70,Convex sequences and Integral representation for the generating function,Convex sequences and Integral representation for the generating function,,"Suppose that $c_k$ is an decreasing sequence of non-negative real numbers, such that $c_0=1$ and $c_{k}\leq \frac{1}{2}(c_{k-1}+c_{k+1})$. Is it true that the generated function of $c_k$ admit an integral representation as below $$ \sum_{k=0}^{\infty}c_kz^k=\int_{\partial\mathbb D} \frac{1}{1-\zeta z}d\mu(\zeta), $$ where $\mu$ is a Borel Probability Measure in $\partial\mathbb D$ ? Motivation: This question is related a possible slight different solution for the question  asked in https://math.stackexchange.com/questions/2188/complex-analysis-question whose the answer, as pointed out by damiano, can be found at the IMC website.","Suppose that $c_k$ is an decreasing sequence of non-negative real numbers, such that $c_0=1$ and $c_{k}\leq \frac{1}{2}(c_{k-1}+c_{k+1})$. Is it true that the generated function of $c_k$ admit an integral representation as below $$ \sum_{k=0}^{\infty}c_kz^k=\int_{\partial\mathbb D} \frac{1}{1-\zeta z}d\mu(\zeta), $$ where $\mu$ is a Borel Probability Measure in $\partial\mathbb D$ ? Motivation: This question is related a possible slight different solution for the question  asked in https://math.stackexchange.com/questions/2188/complex-analysis-question whose the answer, as pointed out by damiano, can be found at the IMC website.",,['complex-analysis']
71,Integrate $\int_{-\infty}^{\infty} \frac{\cosh(\beta x)}{1+\cosh( \beta x )} e^{-x^2} x^2 \rm{d}x$,Integrate,\int_{-\infty}^{\infty} \frac{\cosh(\beta x)}{1+\cosh( \beta x )} e^{-x^2} x^2 \rm{d}x,"Integrate  $$ \int_{-\infty}^{\infty} \frac{\cosh(\beta x)}{1+\cosh( \beta x )} e^{-x^2} x^2 \rm{d}x, $$ with $\beta \in \mathbb{R}$ and $\beta > 0$. Numerical integration shows that this integral exists, but I have been unable to find a closed analytical expression (using contour integration). I have tried to use a rectangular contour $(-R,0) \to (R,0) \to (R,i\eta) \to (-R,i\eta) \to (-R,0)$. The vertical (imaginary direction) integrals at $\pm R$ vanish for $R \to \infty$. I am unable to find an $\eta$ that allows me to relate the horizontal parts of the contour, which would in turn allow me to equate the result to the sum of residues of the enclosed poles.","Integrate  $$ \int_{-\infty}^{\infty} \frac{\cosh(\beta x)}{1+\cosh( \beta x )} e^{-x^2} x^2 \rm{d}x, $$ with $\beta \in \mathbb{R}$ and $\beta > 0$. Numerical integration shows that this integral exists, but I have been unable to find a closed analytical expression (using contour integration). I have tried to use a rectangular contour $(-R,0) \to (R,0) \to (R,i\eta) \to (-R,i\eta) \to (-R,0)$. The vertical (imaginary direction) integrals at $\pm R$ vanish for $R \to \infty$. I am unable to find an $\eta$ that allows me to relate the horizontal parts of the contour, which would in turn allow me to equate the result to the sum of residues of the enclosed poles.",,"['complex-analysis', 'contour-integration']"
72,"Complex Exponential False ""Proof"" That All Integers Are $0$","Complex Exponential False ""Proof"" That All Integers Are",0,"The following false ""proof"" is attributed to Thomas Clausen in 1827, and was stated on page 79 of Nahin's An Imaginary Tale . $e^{i2\pi n}=1$ for all integers $n$. So \begin{align*} ee^{i2\pi n}=e&=e^{1+i2\pi n}\\ &=\left(e^{1+i2\pi n}\right)^{1+i2\pi n}\\ &=e^{(1+i2\pi n)^2}=e^{1+i4\pi n-4\pi^2n^2}\\ &=e^{1+i4\pi n}e^{-4\pi^2n^2} \end{align*} But $e^{1+i4\pi n}=e$, therefore $e^{-4\pi^2n^2}=1$. But that last equation is only true for $n=0$. We started with a statement true for all integers $n$, and through a series of (apparently) valid steps ended with a statement true only for $0$. Therefore all integers are $0$. Where is the mistake?","The following false ""proof"" is attributed to Thomas Clausen in 1827, and was stated on page 79 of Nahin's An Imaginary Tale . $e^{i2\pi n}=1$ for all integers $n$. So \begin{align*} ee^{i2\pi n}=e&=e^{1+i2\pi n}\\ &=\left(e^{1+i2\pi n}\right)^{1+i2\pi n}\\ &=e^{(1+i2\pi n)^2}=e^{1+i4\pi n-4\pi^2n^2}\\ &=e^{1+i4\pi n}e^{-4\pi^2n^2} \end{align*} But $e^{1+i4\pi n}=e$, therefore $e^{-4\pi^2n^2}=1$. But that last equation is only true for $n=0$. We started with a statement true for all integers $n$, and through a series of (apparently) valid steps ended with a statement true only for $0$. Therefore all integers are $0$. Where is the mistake?",,"['complex-analysis', 'complex-numbers', 'fake-proofs']"
73,Difference sets of holomorphic injections,Difference sets of holomorphic injections,,Let $D$ be a bound domain in $\mathbb C$ and let $f$ and $g$ be injective holomorphic functions on $D$. Is it possible that the set $\{{w}:f(w) - g(w) = z\}$ is infinite for all but perhaps one complex numbers $z$?,Let $D$ be a bound domain in $\mathbb C$ and let $f$ and $g$ be injective holomorphic functions on $D$. Is it possible that the set $\{{w}:f(w) - g(w) = z\}$ is infinite for all but perhaps one complex numbers $z$?,,"['complex-analysis', 'analysis']"
74,Complex integration with Cauchy's Integral Formula,Complex integration with Cauchy's Integral Formula,,"Calculate$$\int_\gamma \frac{(z+27i)(z+16)}{z(z+81)^2}dz$$ where $\gamma$ is the triangle whose vertices are the third roots of $z = -8i$, oriented counterclockwise. Answer: I calculated the third roots of $-8i$ and they all have modulus $2$. This tells me that the maximum distance of $\gamma$ from the origin will be $2$. There are singularities at $z=0, z=-81$. As $81 > 2$, this singularity falls outside $\gamma$ so the only one that matters is $z = 0.$ I then applied Cauchy's Integral Formula $$\int_\gamma \frac{(z+27i)(z+16)}{z(z+81)^2}dz = 2\pi i [\frac{(z+27i)(z+16)}{(z+81)^2}] |_{z=0}$$ And I got a final result of $\displaystyle\frac{-32\pi}{243}$. Is my analysis and final result correct?","Calculate$$\int_\gamma \frac{(z+27i)(z+16)}{z(z+81)^2}dz$$ where $\gamma$ is the triangle whose vertices are the third roots of $z = -8i$, oriented counterclockwise. Answer: I calculated the third roots of $-8i$ and they all have modulus $2$. This tells me that the maximum distance of $\gamma$ from the origin will be $2$. There are singularities at $z=0, z=-81$. As $81 > 2$, this singularity falls outside $\gamma$ so the only one that matters is $z = 0.$ I then applied Cauchy's Integral Formula $$\int_\gamma \frac{(z+27i)(z+16)}{z(z+81)^2}dz = 2\pi i [\frac{(z+27i)(z+16)}{(z+81)^2}] |_{z=0}$$ And I got a final result of $\displaystyle\frac{-32\pi}{243}$. Is my analysis and final result correct?",,['complex-analysis']
75,Solve $z^4+1=0$ algebraically,Solve  algebraically,z^4+1=0,"I know the result and how to solve it using trigonometry and De Moivre. However, given that the complex number $z$ can be rewritten as $a+bi$, how can I solve it algebraically?","I know the result and how to solve it using trigonometry and De Moivre. However, given that the complex number $z$ can be rewritten as $a+bi$, how can I solve it algebraically?",,"['complex-analysis', 'complex-numbers']"
76,Is it true that $\lvert \sin z \rvert \leq 1$ for all $z\in \mathbb{C}$?,Is it true that  for all ?,\lvert \sin z \rvert \leq 1 z\in \mathbb{C},"Is it true that $\left\lvert \sin z \right \rvert \leq 1$ for all $z \in \mathbb{C}$ ? I think that is not true, can anyone help me?","Is it true that $\left\lvert \sin z \right \rvert \leq 1$ for all $z \in \mathbb{C}$ ? I think that is not true, can anyone help me?",,"['complex-analysis', 'trigonometry']"
77,Contour Integration $\int_0^1\frac1{\sqrt[n]{1-x^n}}dx$,Contour Integration,\int_0^1\frac1{\sqrt[n]{1-x^n}}dx,"I want to compute: $$\int^{1}_{0}\frac{1}{\sqrt[n]{1-x^n}}dx$$ for natural $n>1$ using Residue Calculus. I am thinking of using some kind of a keyhole or bone contour that could go around the $n$th roots of unity (singularities in this case). The problem is I believe it is not clear how to define a suitable branch (or branches) of $\log$ in this region for it to work, also considering we only care about the segment from $0$ to $1$.","I want to compute: $$\int^{1}_{0}\frac{1}{\sqrt[n]{1-x^n}}dx$$ for natural $n>1$ using Residue Calculus. I am thinking of using some kind of a keyhole or bone contour that could go around the $n$th roots of unity (singularities in this case). The problem is I believe it is not clear how to define a suitable branch (or branches) of $\log$ in this region for it to work, also considering we only care about the segment from $0$ to $1$.",,"['complex-analysis', 'contour-integration', 'residue-calculus']"
78,"Proving that if $f: \mathbb{C} \to \mathbb{C} $ is a continuous function with $f^2, f^3$ analytic, then $f$ is also analytic","Proving that if  is a continuous function with  analytic, then  is also analytic","f: \mathbb{C} \to \mathbb{C}  f^2, f^3 f","Let $f: \mathbb{C} \to  \mathbb{C}$ be a continuous function such that $f^2$ and $f^3$ are both analytic. Prove that $f$ is also analytic. Some ideas: At $z_0$ where $f^2$ is not $0$ , then $f^3$ and $f^2$ are analytic so $f = \frac{f^3}{f^2}$ is analytic at $z_0$ but at $z_0$ where $f^2$ is $0$, I'm not able to show that $f$ is analytic.","Let $f: \mathbb{C} \to  \mathbb{C}$ be a continuous function such that $f^2$ and $f^3$ are both analytic. Prove that $f$ is also analytic. Some ideas: At $z_0$ where $f^2$ is not $0$ , then $f^3$ and $f^2$ are analytic so $f = \frac{f^3}{f^2}$ is analytic at $z_0$ but at $z_0$ where $f^2$ is $0$, I'm not able to show that $f$ is analytic.",,['complex-analysis']
79,Conformal Maps onto the Unit Disc in $\mathbb{C}$,Conformal Maps onto the Unit Disc in,\mathbb{C},"I am interested in finding explicit formulae for (better yet characterizing) conformal functions from various domains onto the open unit disc $\mathbb{D}\subset\mathbb{C}$, and in understanding the key ideas necessary to establish such functions. Specifically, what can $f$ look like when $f:G\to\mathbb{D}$ is conformal and (1) $G=\{x+iy~|~x,y>0\}$ is the open first quadrant. (2) $G=\{x+iy~|~x>0,~0<y<1\}$ is an open horizontal strip in the first quadrant. (3) $G=\{z\in\mathbb{C}~|~\frac{1}{2}<|z|<1\}$ is an annulus. (4) $G=\mathbb{D}\cap\{|z-\frac{1}{2}|>\frac{1}{2}\}$ is something else (torus?).","I am interested in finding explicit formulae for (better yet characterizing) conformal functions from various domains onto the open unit disc $\mathbb{D}\subset\mathbb{C}$, and in understanding the key ideas necessary to establish such functions. Specifically, what can $f$ look like when $f:G\to\mathbb{D}$ is conformal and (1) $G=\{x+iy~|~x,y>0\}$ is the open first quadrant. (2) $G=\{x+iy~|~x>0,~0<y<1\}$ is an open horizontal strip in the first quadrant. (3) $G=\{z\in\mathbb{C}~|~\frac{1}{2}<|z|<1\}$ is an annulus. (4) $G=\mathbb{D}\cap\{|z-\frac{1}{2}|>\frac{1}{2}\}$ is something else (torus?).",,"['complex-analysis', 'conformal-geometry']"
80,Does this proposition from complex analysis depend on AC?,Does this proposition from complex analysis depend on AC?,,"I was reading III vol. of Princeton lectures on analysis. Proposition 1.4: ""If $\Omega_{1}\supset\Omega_{2}\supset\ldots\supset\Omega_{n}\supset\ldots $ is a sequence of non-empty compact sets in $\Bbb C$ with the property that: $$\operatorname{diam}(\Omega_{n})\to 0\text{ as } n\to\infty,$$ then there exists a unique point $w\in\Bbb C$ such that $w \in \Omega_{n}$ for all $n$."" And in the proof:""Choose point $z_{n}$ in each $\Omega_{n}$"" Apparently this proof relies on Axiom of Choice. But I'm interested if it can be proved without reference to AC?","I was reading III vol. of Princeton lectures on analysis. Proposition 1.4: ""If $\Omega_{1}\supset\Omega_{2}\supset\ldots\supset\Omega_{n}\supset\ldots $ is a sequence of non-empty compact sets in $\Bbb C$ with the property that: $$\operatorname{diam}(\Omega_{n})\to 0\text{ as } n\to\infty,$$ then there exists a unique point $w\in\Bbb C$ such that $w \in \Omega_{n}$ for all $n$."" And in the proof:""Choose point $z_{n}$ in each $\Omega_{n}$"" Apparently this proof relies on Axiom of Choice. But I'm interested if it can be proved without reference to AC?",,['complex-analysis']
81,"Ahlfors ""Prove the formula of Gauss""","Ahlfors ""Prove the formula of Gauss""",,"He says: Prove the formula of Gauss:   $$ (2\pi)^\frac{n-1}{2} \Gamma(z) = n^{z - \frac{1}{2}}\Gamma(z/n)\Gamma(\frac{z+1}{n})\cdots\Gamma(\frac{z+n-1}{n}) $$ This is an exercise out of Ahlfors. By taking the logarithmic derivative, it's easy to show the left & right hand sides are the the same up to a multiplicative constant. After that I'm lost. It's easy using another identity when $n$ is even to use induction. But when $n$ is odd I am lost. It's obvious when $n$ is a power of 2.","He says: Prove the formula of Gauss:   $$ (2\pi)^\frac{n-1}{2} \Gamma(z) = n^{z - \frac{1}{2}}\Gamma(z/n)\Gamma(\frac{z+1}{n})\cdots\Gamma(\frac{z+n-1}{n}) $$ This is an exercise out of Ahlfors. By taking the logarithmic derivative, it's easy to show the left & right hand sides are the the same up to a multiplicative constant. After that I'm lost. It's easy using another identity when $n$ is even to use induction. But when $n$ is odd I am lost. It's obvious when $n$ is a power of 2.",,"['complex-analysis', 'gamma-function']"
82,Integration method for $\int_0^\infty\frac{x}{(e^x-1)(x^2+(2\pi)^2)^2}dx=\frac{1}{96} - \frac{3}{32\pi^2}.$,Integration method for,\int_0^\infty\frac{x}{(e^x-1)(x^2+(2\pi)^2)^2}dx=\frac{1}{96} - \frac{3}{32\pi^2}.,"The following definite integral is obtained directly from Hermite's integral representation of the Hurwitz zeta function. But is it possible to obtain the same result via the residue calculus or another technique? $$\int_{0}^{\infty} {x  \over  \left({\rm e}^{x} - 1\right)\left[x^{2} + \left(2\pi\right)^{2}\right]^{2}} \,{\rm d}x ={1 \over 96} - {3 \over 32\pi^{2}}. $$","The following definite integral is obtained directly from Hermite's integral representation of the Hurwitz zeta function. But is it possible to obtain the same result via the residue calculus or another technique? $$\int_{0}^{\infty} {x  \over  \left({\rm e}^{x} - 1\right)\left[x^{2} + \left(2\pi\right)^{2}\right]^{2}} \,{\rm d}x ={1 \over 96} - {3 \over 32\pi^{2}}. $$",,"['complex-analysis', 'definite-integrals', 'contour-integration', 'riemann-zeta']"
83,Intuition Behind an Identity,Intuition Behind an Identity,,"I'm currently studying for a complex analysis prelim. exam in August, so I'm working through some of the exercises in Titchmarsh. One of the exercises has us evaluate the integrals $$\int_0^\infty\frac{1}{1+x^4}\,dx\quad\text{and}\quad\int_0^\infty\frac{x^2}{1+x^4}\,dx.$$After evaluating each of them, I found $$\int_0^\infty\frac{1}{1+x^4}\,dx=\int_0^\infty\frac{x^2}{1+x^4}\,dx=\frac{\pi}{2\sqrt{2}}.$$Pretty sure I had miscalculated, I went to Wolfram Alpha to verify my answers only to find I had done it correctly. My question is why these two have the same value. Intuitively, I expected $\int\frac{x^2}{1+x^4}\,dx$ to be larger because on the interval $(1,\infty)$, $x^2>1$. The only explanation I can think of is that the $x^2$ makes the integrand much smaller in the interval $[0,1]$ than the original function, but I wouldn't have guessed it to be enough to make the values come out the same. Is there some other intuitive reason why these two integrals are the same?","I'm currently studying for a complex analysis prelim. exam in August, so I'm working through some of the exercises in Titchmarsh. One of the exercises has us evaluate the integrals $$\int_0^\infty\frac{1}{1+x^4}\,dx\quad\text{and}\quad\int_0^\infty\frac{x^2}{1+x^4}\,dx.$$After evaluating each of them, I found $$\int_0^\infty\frac{1}{1+x^4}\,dx=\int_0^\infty\frac{x^2}{1+x^4}\,dx=\frac{\pi}{2\sqrt{2}}.$$Pretty sure I had miscalculated, I went to Wolfram Alpha to verify my answers only to find I had done it correctly. My question is why these two have the same value. Intuitively, I expected $\int\frac{x^2}{1+x^4}\,dx$ to be larger because on the interval $(1,\infty)$, $x^2>1$. The only explanation I can think of is that the $x^2$ makes the integrand much smaller in the interval $[0,1]$ than the original function, but I wouldn't have guessed it to be enough to make the values come out the same. Is there some other intuitive reason why these two integrals are the same?",,"['complex-analysis', 'contour-integration']"
84,A ‘strong’ form of the Fundamental Theorem of Algebra,A ‘strong’ form of the Fundamental Theorem of Algebra,,"Let $ n \in \mathbb{N} $ and $ a_{0},\ldots,a_{n-1} \in \mathbb{C} $ be constants. By the Fundamental Theorem of Algebra, the polynomial $$ p(z) := z^{n} + \sum_{k=0}^{n-1} a_{k} z^{k} \in \mathbb{C}[z] $$ has $ n $ roots, including multiplicity. If we vary the values of $ a_{0},\ldots,a_{n-1} $, the roots will obviously change, so it seems natural to ask the following question. Do the $ n $ roots of $ p(z) $ depend on the coefficients in an analytic sort of way? More precisely, can we find holomorphic functions $ r_{1},\ldots,r_{n}: \mathbb{C}^{n} \to \mathbb{C} $ such that   $$ z^{n} + \sum_{k=0}^{n-1} a_{k} z^{k} = \prod_{j=1}^{n} [z - {r_{j}}(a_{0},\ldots,a_{n-1})]? $$ The definition of a holomorphic function of several complex variables is given as follows: Definition Let $ n \in \mathbb{N} $ and $ \Omega \subseteq \mathbb{C}^{n} $ be a domain (i.e., a connected open subset). A function $ f: \Omega \to \mathbb{C} $ is said to be holomorphic if and only if it is holomorphic in the usual sense in each of its $ n $ variables. The existence of $ r_{1},\ldots,r_{n}: \mathbb{C}^{n} \to \mathbb{C} $ that are continuous seems to be a well-known result (due to Ostrowski, perhaps?), but I am unable to find anything in the literature that is concerned with the holomorphicity of these functions. Any help would be greatly appreciated. Thank you very much!","Let $ n \in \mathbb{N} $ and $ a_{0},\ldots,a_{n-1} \in \mathbb{C} $ be constants. By the Fundamental Theorem of Algebra, the polynomial $$ p(z) := z^{n} + \sum_{k=0}^{n-1} a_{k} z^{k} \in \mathbb{C}[z] $$ has $ n $ roots, including multiplicity. If we vary the values of $ a_{0},\ldots,a_{n-1} $, the roots will obviously change, so it seems natural to ask the following question. Do the $ n $ roots of $ p(z) $ depend on the coefficients in an analytic sort of way? More precisely, can we find holomorphic functions $ r_{1},\ldots,r_{n}: \mathbb{C}^{n} \to \mathbb{C} $ such that   $$ z^{n} + \sum_{k=0}^{n-1} a_{k} z^{k} = \prod_{j=1}^{n} [z - {r_{j}}(a_{0},\ldots,a_{n-1})]? $$ The definition of a holomorphic function of several complex variables is given as follows: Definition Let $ n \in \mathbb{N} $ and $ \Omega \subseteq \mathbb{C}^{n} $ be a domain (i.e., a connected open subset). A function $ f: \Omega \to \mathbb{C} $ is said to be holomorphic if and only if it is holomorphic in the usual sense in each of its $ n $ variables. The existence of $ r_{1},\ldots,r_{n}: \mathbb{C}^{n} \to \mathbb{C} $ that are continuous seems to be a well-known result (due to Ostrowski, perhaps?), but I am unable to find anything in the literature that is concerned with the holomorphicity of these functions. Any help would be greatly appreciated. Thank you very much!",,"['complex-analysis', 'polynomials', 'complex-numbers', 'several-complex-variables']"
85,What is the difference between a holomorphic function and a meromorphic function?,What is the difference between a holomorphic function and a meromorphic function?,,"As far as I can tell, if a function is holomorphic on its domain, then it's also meromorphic and vice versa. Can someone tell me what the difference between these two properties are (if any)? A counter-example and an explanation of why it's a counter-example would be nice.","As far as I can tell, if a function is holomorphic on its domain, then it's also meromorphic and vice versa. Can someone tell me what the difference between these two properties are (if any)? A counter-example and an explanation of why it's a counter-example would be nice.",,['complex-analysis']
86,$|e^a-e^b| \leq |a-b|$ for complex numbers with non-positive real parts,for complex numbers with non-positive real parts,|e^a-e^b| \leq |a-b|,"Came across this problem on an old qualifying exam: Let $a$ and $b$ be complex numbers whose real parts are negative or 0. Prove the inequality $|e^a-e^b| \leq |a-b|$. If $f(z)=e^z$ and $z=x+iy$, then $|f'(z)|=e^x\leq 1$ given that $x \leq 0$. I played around with the limit definition of the derivative, but wasn't able to get anywhere. Not sure what else to try; a hint would be very helpful!","Came across this problem on an old qualifying exam: Let $a$ and $b$ be complex numbers whose real parts are negative or 0. Prove the inequality $|e^a-e^b| \leq |a-b|$. If $f(z)=e^z$ and $z=x+iy$, then $|f'(z)|=e^x\leq 1$ given that $x \leq 0$. I played around with the limit definition of the derivative, but wasn't able to get anywhere. Not sure what else to try; a hint would be very helpful!",,"['complex-analysis', 'inequality']"
87,Cauchy's residue theorem with an infinite number of poles,Cauchy's residue theorem with an infinite number of poles,,"Is it possible to apply Cauchy's residue theorem to a function which has a (countably) infinite number of isolated singularities within the contour of integration (say a semicircle whose radius goes to infinity), with known residues $r_n$ , assuming that $$\sum_{n=1}^\infty r_n$$ converges?","Is it possible to apply Cauchy's residue theorem to a function which has a (countably) infinite number of isolated singularities within the contour of integration (say a semicircle whose radius goes to infinity), with known residues , assuming that converges?",r_n \sum_{n=1}^\infty r_n,"['complex-analysis', 'analysis']"
88,Weierstrass $\wp$ function doubly periodic,Weierstrass  function doubly periodic,\wp,"I'm working my way through Silverman and Tate's Undergraduate Introduction to Elliptic Curves.  I haven't yet been able to study complex analysis, so it comes as no surprise that I'm having a tough time with that portion of the book right now. Let $\omega_1, \omega_2 \in \mathbb{C}$ be two complex numbers which are $\mathbb{R}$-linearly independent and let: $$L = \mathbb{Z}\omega_1 + \mathbb{Z}\omega_2 = \{n_1\omega_1 + n_2\omega_2 : n_1, n_2 \in \mathbb{Z}\}$$ Let  $\wp(u) = \frac{1}{u^2} + \sum\limits_{\omega \in L, \omega \neq 0} \left(\frac{1}{(u-\omega)^2} - \frac{1}{\omega^2}\right)$  Show that $\wp$ is a doubly periodic function, that is, show that  $$\wp(u + \omega) = \wp(u)$$ If you are able, please give me a shove in the right direction.  Thank you! Dear Answerers: Thank you, I have been able to figure it out.  Yes, convergence was quite tricky and I was trying to make this particular question much more difficult than it actually was.  Thank you!","I'm working my way through Silverman and Tate's Undergraduate Introduction to Elliptic Curves.  I haven't yet been able to study complex analysis, so it comes as no surprise that I'm having a tough time with that portion of the book right now. Let $\omega_1, \omega_2 \in \mathbb{C}$ be two complex numbers which are $\mathbb{R}$-linearly independent and let: $$L = \mathbb{Z}\omega_1 + \mathbb{Z}\omega_2 = \{n_1\omega_1 + n_2\omega_2 : n_1, n_2 \in \mathbb{Z}\}$$ Let  $\wp(u) = \frac{1}{u^2} + \sum\limits_{\omega \in L, \omega \neq 0} \left(\frac{1}{(u-\omega)^2} - \frac{1}{\omega^2}\right)$  Show that $\wp$ is a doubly periodic function, that is, show that  $$\wp(u + \omega) = \wp(u)$$ If you are able, please give me a shove in the right direction.  Thank you! Dear Answerers: Thank you, I have been able to figure it out.  Yes, convergence was quite tricky and I was trying to make this particular question much more difficult than it actually was.  Thank you!",,"['complex-analysis', 'special-functions', 'elliptic-curves', 'elliptic-functions']"
89,The family of analytic functions with positive real part is normal,The family of analytic functions with positive real part is normal,,"I'm having difficulty with the following exercise in Ahlfors' text, on page 227. Prove that in any region $\Omega$ the family of analytic functions with positive real part is normal. Under what added condition is it locally bounded? Hint : Consider the functions $e^{-f}$. Here is what I've tried: I will start with a remark: Apparently, Ahlfors wants us to show that the family is ""normal in the classical sense"". That is, every sequence of functions in the family has a subsequence which converges uniformly on compact subsets or tends uniformly to $\infty$ on compact subsets . In order to see why this is the right definition, consider the sequence $f_n(z)=n$. It is contained in the family but has no appropriate subsequence (in the sense of definition 2 in the text with $S=\mathbb C$). Now, to the attempt itself: Let $\Omega \subset \mathbb C$ be a fixed region, and consider the family $$\mathfrak F=\{f: \Omega \to \mathbb C | f \text{ is analytic and } \Re(f) >0 \}. $$ We would like to show that $\mathfrak F$ is normal in the classical sense. Following the hint, we examine the family $$ \mathfrak G=\{e^{-f}:f \in \mathfrak F \}.$$ $\mathfrak G$ is locally bounded (since $|e^{-f}|=e^{- \Re (f)}<1$ for every $f \in \mathfrak F$), thus it is normal with respect to $\mathbb C$ (theorem 15), and obviously, it is normal in the classical sense as well. Let $\{ f_n \}$ be a sequence in $\mathfrak F$, and consider the sequence $\{ g_n \}=\{e^{-f_n} \}$ in $\mathfrak G$. According to normality it has a convergent subsequence $\{ g_{n_k} \}=\{e^{-f_{n_k}} \}$ which converges uniformly on compact subsets of $\Omega$ to some function $g$ (which is analytic by Weierstrass' theorem). Since each $\{ g_{n_k} \}$ is nonvanishing, the limit function $g$ is either identically zero, or non vanishing as well (Hurwitz's theorem). In the former case it is easy to show that the subsequence $\{ f_{n_k} \}$, obtained by the same indices, tends to $\infty$ uniformly on compact sets. Hence, we will assume from now that $g(z) \neq 0$ for all $z \in \Omega$. Up until now, I was trying to show that the subsequence $\{ f_{n_k} \}$ works in all cases, but sadly, this is not the case. Consider the sequence $f_n(z) \equiv 1+2 \pi i (-1)^n \in \mathfrak F$. In that case $g_n(z)=e^{-1}$, and an admissible subsequence is $g_{n_k}=g_k=e^{-1}$. However, $f_{n_k}=1+2 \pi i (-1)^k$ diverges everywhere. Can anyone please help me finish this proof? Or maybe give me some hints? Thanks!","I'm having difficulty with the following exercise in Ahlfors' text, on page 227. Prove that in any region $\Omega$ the family of analytic functions with positive real part is normal. Under what added condition is it locally bounded? Hint : Consider the functions $e^{-f}$. Here is what I've tried: I will start with a remark: Apparently, Ahlfors wants us to show that the family is ""normal in the classical sense"". That is, every sequence of functions in the family has a subsequence which converges uniformly on compact subsets or tends uniformly to $\infty$ on compact subsets . In order to see why this is the right definition, consider the sequence $f_n(z)=n$. It is contained in the family but has no appropriate subsequence (in the sense of definition 2 in the text with $S=\mathbb C$). Now, to the attempt itself: Let $\Omega \subset \mathbb C$ be a fixed region, and consider the family $$\mathfrak F=\{f: \Omega \to \mathbb C | f \text{ is analytic and } \Re(f) >0 \}. $$ We would like to show that $\mathfrak F$ is normal in the classical sense. Following the hint, we examine the family $$ \mathfrak G=\{e^{-f}:f \in \mathfrak F \}.$$ $\mathfrak G$ is locally bounded (since $|e^{-f}|=e^{- \Re (f)}<1$ for every $f \in \mathfrak F$), thus it is normal with respect to $\mathbb C$ (theorem 15), and obviously, it is normal in the classical sense as well. Let $\{ f_n \}$ be a sequence in $\mathfrak F$, and consider the sequence $\{ g_n \}=\{e^{-f_n} \}$ in $\mathfrak G$. According to normality it has a convergent subsequence $\{ g_{n_k} \}=\{e^{-f_{n_k}} \}$ which converges uniformly on compact subsets of $\Omega$ to some function $g$ (which is analytic by Weierstrass' theorem). Since each $\{ g_{n_k} \}$ is nonvanishing, the limit function $g$ is either identically zero, or non vanishing as well (Hurwitz's theorem). In the former case it is easy to show that the subsequence $\{ f_{n_k} \}$, obtained by the same indices, tends to $\infty$ uniformly on compact sets. Hence, we will assume from now that $g(z) \neq 0$ for all $z \in \Omega$. Up until now, I was trying to show that the subsequence $\{ f_{n_k} \}$ works in all cases, but sadly, this is not the case. Consider the sequence $f_n(z) \equiv 1+2 \pi i (-1)^n \in \mathfrak F$. In that case $g_n(z)=e^{-1}$, and an admissible subsequence is $g_{n_k}=g_k=e^{-1}$. However, $f_{n_k}=1+2 \pi i (-1)^k$ diverges everywhere. Can anyone please help me finish this proof? Or maybe give me some hints? Thanks!",,"['complex-analysis', 'normal-families']"
90,Why is an integral of a complex function defined as a line integral?,Why is an integral of a complex function defined as a line integral?,,"In real analysis, we can define a line integral, but we also define (earlier) the regular definite integral. Why is it that in complex analysis we are interested only in a line integral?","In real analysis, we can define a line integral, but we also define (earlier) the regular definite integral. Why is it that in complex analysis we are interested only in a line integral?",,['complex-analysis']
91,Help with integrating $\int_0^{\infty} \frac{(\log x)^2}{x^2 + 1} \operatorname d\!x$ - contour integration?,Help with integrating  - contour integration?,\int_0^{\infty} \frac{(\log x)^2}{x^2 + 1} \operatorname d\!x,"George Arfken's book: Mathematical Methods for Physicists has the following problem in a chapter on contour integration: $\displaystyle \int_0^{\infty} \dfrac{(\log x)^2}{x^2 + 1} dx$. Their suggestion is to make the substitution $x \rightarrow z=e^t$. I'm not sure what they meant by this, but I tried making the substitution $x = e^t$, which turns the integral into: $\displaystyle\int_{-\infty}^{\infty} \dfrac{t^2 e^t}{1+e^{2t}} dt$. The hint is then to take the contour from -R to R, to R+$\pi i$, to -R + $\pi i$, to -R. Since this has a pole at $t = \pi i/2$, a Laurent series expansion about this point gives the residue as $i \pi^2/8$, so the contour integral equals $-\pi^3/4$. I've been able to show that the integral along R to R + $\pi i$ and along -R + $\pi i$ to -R goes to zero by the ML inequality - the denominator grows exponentially but the numerator quadratically. But at this point, I'm a bit lost as to what to do with the integral over Im $t = \pi$. Any help? The book gives the answer as $\pi^3 /8$.","George Arfken's book: Mathematical Methods for Physicists has the following problem in a chapter on contour integration: $\displaystyle \int_0^{\infty} \dfrac{(\log x)^2}{x^2 + 1} dx$. Their suggestion is to make the substitution $x \rightarrow z=e^t$. I'm not sure what they meant by this, but I tried making the substitution $x = e^t$, which turns the integral into: $\displaystyle\int_{-\infty}^{\infty} \dfrac{t^2 e^t}{1+e^{2t}} dt$. The hint is then to take the contour from -R to R, to R+$\pi i$, to -R + $\pi i$, to -R. Since this has a pole at $t = \pi i/2$, a Laurent series expansion about this point gives the residue as $i \pi^2/8$, so the contour integral equals $-\pi^3/4$. I've been able to show that the integral along R to R + $\pi i$ and along -R + $\pi i$ to -R goes to zero by the ML inequality - the denominator grows exponentially but the numerator quadratically. But at this point, I'm a bit lost as to what to do with the integral over Im $t = \pi$. Any help? The book gives the answer as $\pi^3 /8$.",,"['complex-analysis', 'contour-integration']"
92,What is pluripotential theory?,What is pluripotential theory?,,"My tutor for electromagnetism showed me a problem about point charges in a disk and their equilibria. He referred me to a subject called ""pluripotential theory"". I googled it and I did not find what I was looking for at all! So my question is, what is pluripotential theory, what is it used for and are there any interesting and/or intuitive aspects/results of the field. EDIT: I might have been unclear with the phrasing of the question. I am not looking for an account of potentials of more than one point charge. I am aware that pluripotential theory is a field in it's own and this is what I am interested in learning about.","My tutor for electromagnetism showed me a problem about point charges in a disk and their equilibria. He referred me to a subject called ""pluripotential theory"". I googled it and I did not find what I was looking for at all! So my question is, what is pluripotential theory, what is it used for and are there any interesting and/or intuitive aspects/results of the field. EDIT: I might have been unclear with the phrasing of the question. I am not looking for an account of potentials of more than one point charge. I am aware that pluripotential theory is a field in it's own and this is what I am interested in learning about.",,"['complex-analysis', 'several-complex-variables']"
93,"Integral by residue - ""dog bone""","Integral by residue - ""dog bone""",,"Let $I=\int_{-1}^{1}\frac{x^2 dx}{\sqrt[3]{(1-x)(1+x)^2}}$.  I used complex function $f(z)=\frac{z^2}{\sqrt[3]{(z-1)(z+1)^2}}$, which we can  define such that it is holomorphic on $\mathbb{C}\setminus[-1,1]$. I use a ""dog bone""-contur to integrate it. I have problem with integral on the big circle : $\lim_{R \to \infty}\int_{C_R}f(z)dz$. How to calculate it ? (I know that it should be nonzero.)","Let $I=\int_{-1}^{1}\frac{x^2 dx}{\sqrt[3]{(1-x)(1+x)^2}}$.  I used complex function $f(z)=\frac{z^2}{\sqrt[3]{(z-1)(z+1)^2}}$, which we can  define such that it is holomorphic on $\mathbb{C}\setminus[-1,1]$. I use a ""dog bone""-contur to integrate it. I have problem with integral on the big circle : $\lim_{R \to \infty}\int_{C_R}f(z)dz$. How to calculate it ? (I know that it should be nonzero.)",,"['complex-analysis', 'contour-integration', 'residue-calculus']"
94,"Proof that the function $\cot(\pi z)$ is uniformly bounded on the sides of the square with vertices $\pm(N+1/2)\pm i(N+1/2),n∈ℕ$.",Proof that the function  is uniformly bounded on the sides of the square with vertices .,"\cot(\pi z) \pm(N+1/2)\pm i(N+1/2),n∈ℕ","Proof that the function $\cot(\pi z)$ is uniformly bounded on the    sides of the square with vertices $\pm(N+1/2)\pm i(N+1/2),n∈ℕ$. My idea was that since those squares are compact and this function is continuous on those squares the image must be compact and therefore bounded. But I'm not sure what to do with the ""uniform"" notion.","Proof that the function $\cot(\pi z)$ is uniformly bounded on the    sides of the square with vertices $\pm(N+1/2)\pm i(N+1/2),n∈ℕ$. My idea was that since those squares are compact and this function is continuous on those squares the image must be compact and therefore bounded. But I'm not sure what to do with the ""uniform"" notion.",,['complex-analysis']
95,How to show $\int^{\infty}_{-\infty}\frac{\sin(ax)}{x(x^2+1)}dx=\pi(1-e^{-a})$? ($a\ge0$),How to show ? (),\int^{\infty}_{-\infty}\frac{\sin(ax)}{x(x^2+1)}dx=\pi(1-e^{-a}) a\ge0,"$$\int^{\infty}_{-\infty}\frac{\sin(ax)}{x(x^2+1)}dx=\pi(1-e^{-a}), \ a\ge0$$ I tried to solve but came up with $\pi(2-e^{-a}) $. Could you tell me where did I do the mistake? if $x=z$ then $dz=dx$ $$\int_\gamma \frac{e^{iaz}}{z(z^2+1)}\quad and\quad z(z+i)(z-i)=0\quad \rightarrow z=0,z=\pm i$$ for $ z=0$ $$Res(f,0)=\lim_{z\to 0}\frac{z.e^{iaz}}{z(z^2+1)}=1$$ for $z=1$ $$Res(f,i)=\lim_{z\to i}\frac{(z-i).e^{iaz}}{z(z+i)(z-i)}=\lim_{z\to i}\frac{e^{iaz}}{z(z+i)}=\frac{e^{-a}}{-2}$$ $$\int_\gamma \frac{e^{iaz}}{z(z^2+1)}=\int_{-R}^{R}\frac{e^{iax}}{x(x^2+1)}dx+\int_\gamma \frac{e^{iaz}}{z(z^2+1)}=\pi i(1-\frac{e^{-a}}{-2})$$ $$\int_{-R}^{R}\frac{e^{iax}}{x(x^2+1)}=\int^{R}_{-R}\frac{\cos(ax)}{x(x^2+1)}dx+i\int_{-R}^R \frac{\sin(ax)}{x(x^2+1)}=i(2\pi - 2\pi \frac{e^{-a}}{2})$$ $$\rightarrow \int_{-R}^R\frac{\sin(ax)}{x(x^2+1)}=2\pi -2\pi \frac{e^{-a}}{2}=\pi(2-e^{-a}) $$","$$\int^{\infty}_{-\infty}\frac{\sin(ax)}{x(x^2+1)}dx=\pi(1-e^{-a}), \ a\ge0$$ I tried to solve but came up with $\pi(2-e^{-a}) $. Could you tell me where did I do the mistake? if $x=z$ then $dz=dx$ $$\int_\gamma \frac{e^{iaz}}{z(z^2+1)}\quad and\quad z(z+i)(z-i)=0\quad \rightarrow z=0,z=\pm i$$ for $ z=0$ $$Res(f,0)=\lim_{z\to 0}\frac{z.e^{iaz}}{z(z^2+1)}=1$$ for $z=1$ $$Res(f,i)=\lim_{z\to i}\frac{(z-i).e^{iaz}}{z(z+i)(z-i)}=\lim_{z\to i}\frac{e^{iaz}}{z(z+i)}=\frac{e^{-a}}{-2}$$ $$\int_\gamma \frac{e^{iaz}}{z(z^2+1)}=\int_{-R}^{R}\frac{e^{iax}}{x(x^2+1)}dx+\int_\gamma \frac{e^{iaz}}{z(z^2+1)}=\pi i(1-\frac{e^{-a}}{-2})$$ $$\int_{-R}^{R}\frac{e^{iax}}{x(x^2+1)}=\int^{R}_{-R}\frac{\cos(ax)}{x(x^2+1)}dx+i\int_{-R}^R \frac{\sin(ax)}{x(x^2+1)}=i(2\pi - 2\pi \frac{e^{-a}}{2})$$ $$\rightarrow \int_{-R}^R\frac{\sin(ax)}{x(x^2+1)}=2\pi -2\pi \frac{e^{-a}}{2}=\pi(2-e^{-a}) $$",,"['complex-analysis', 'definite-integrals', 'contour-integration', 'residue-calculus']"
96,Branch cut for $\sqrt{1-z^2}$ - Can I use the branch cut of $\sqrt{z}$?,Branch cut for  - Can I use the branch cut of ?,\sqrt{1-z^2} \sqrt{z},"I was trying to clarify some questions I had about elliptic integrals using http://websites.math.leidenuniv.nl/algebra/ellcurves.pdf There they define the map $$\phi\colon w\mapsto \int_0^w\frac{\mathrm{d}z}{\sqrt{1-z^2}}$$ on $\mathbb{C}\setminus[-1,1]$ to get $\phi$ well-defined up to periods of the integral. The choice of the interval $[-1,1]$ is made so that $\sqrt{1-z^2}$ admits a single-valued branch. Now, I know that the principal branch of the square root $\sqrt{z}$ is discontinuous on the half-line $(-\infty,0)$, so to get a holomorphic map we restrict to $\mathbb{C}\setminus (-\infty,0]$. Substituting $1-z^2$ for $z$ we get that the appropriate branch cuts for the above mapping $\sqrt{1-z^2}$ would be $(-\infty,-1]$ and $[1,\infty)$, which is somewhat the opposite of the suggested interval $[-1,1]$. From that I conclude that they didn't choose the principal branch, otherwise for e.g. $z=2$ the map would be discontinuous. My question is: Are both choices possible? Then there must be some way to choose another branch of $\sqrt{1-z^2}$. Is there a good way to see how to choose ""elegant"" branch cuts and the corresponding holomorphic branches? A thought of my own: It should be possible to instead integrate on the Riemann sphere, using $\infty$ and not $0$ as a starting point. Then the two intervals would ""swap roles"". But I don't see how to formalize this.","I was trying to clarify some questions I had about elliptic integrals using http://websites.math.leidenuniv.nl/algebra/ellcurves.pdf There they define the map $$\phi\colon w\mapsto \int_0^w\frac{\mathrm{d}z}{\sqrt{1-z^2}}$$ on $\mathbb{C}\setminus[-1,1]$ to get $\phi$ well-defined up to periods of the integral. The choice of the interval $[-1,1]$ is made so that $\sqrt{1-z^2}$ admits a single-valued branch. Now, I know that the principal branch of the square root $\sqrt{z}$ is discontinuous on the half-line $(-\infty,0)$, so to get a holomorphic map we restrict to $\mathbb{C}\setminus (-\infty,0]$. Substituting $1-z^2$ for $z$ we get that the appropriate branch cuts for the above mapping $\sqrt{1-z^2}$ would be $(-\infty,-1]$ and $[1,\infty)$, which is somewhat the opposite of the suggested interval $[-1,1]$. From that I conclude that they didn't choose the principal branch, otherwise for e.g. $z=2$ the map would be discontinuous. My question is: Are both choices possible? Then there must be some way to choose another branch of $\sqrt{1-z^2}$. Is there a good way to see how to choose ""elegant"" branch cuts and the corresponding holomorphic branches? A thought of my own: It should be possible to instead integrate on the Riemann sphere, using $\infty$ and not $0$ as a starting point. Then the two intervals would ""swap roles"". But I don't see how to formalize this.",,"['complex-analysis', 'branch-cuts']"
97,Stone-Weierstrass Theorem in $\mathbb{C}$,Stone-Weierstrass Theorem in,\mathbb{C},"I am having difficulty understanding how to prove the Stone-Weierstrass Theorem for complex valued functions defined on the closed unit disc $\mathbb{D}\subset\mathbb{C}$. Here is a version I have from an exercise in Lang: Any continuous complex valued function defined on the closed unit disc can be uniformly approximated by polynomials. I take this to mean that for any continuous $f:\mathbb{D}\to\mathbb{C}$ there is a sequence of polynomials $\{f_n\}_{n\in\mathbb{Z}^+}$ such that for any $\epsilon>0$, there is  $N\in\mathbb{Z}^+$ so that for all $n\geq N, \sup_{z\in\mathbb{D}}|f_n(z)-f(z)|<\epsilon$. Is this a valid interpretation? What would be the best way to approach this?  Ideally I would like to use tools from elementary complex analysis but any insights could be helpful! :)","I am having difficulty understanding how to prove the Stone-Weierstrass Theorem for complex valued functions defined on the closed unit disc $\mathbb{D}\subset\mathbb{C}$. Here is a version I have from an exercise in Lang: Any continuous complex valued function defined on the closed unit disc can be uniformly approximated by polynomials. I take this to mean that for any continuous $f:\mathbb{D}\to\mathbb{C}$ there is a sequence of polynomials $\{f_n\}_{n\in\mathbb{Z}^+}$ such that for any $\epsilon>0$, there is  $N\in\mathbb{Z}^+$ so that for all $n\geq N, \sup_{z\in\mathbb{D}}|f_n(z)-f(z)|<\epsilon$. Is this a valid interpretation? What would be the best way to approach this?  Ideally I would like to use tools from elementary complex analysis but any insights could be helpful! :)",,['complex-analysis']
98,Does the set of entire functions have the same cardinality as the reals?,Does the set of entire functions have the same cardinality as the reals?,,"I've recently been thinking about entire functions and the Weierstrass factorisation theorem and it got me thinking about the cardinality of the set of entire functions. Clearly $e^{cz}$ is entire, for all $c \in \mathbb{C}$ , so the cardinality of the set of entire functions is at least that of the continuum. I thought that the set of entire functions would have a cardinality exceeding that of the continuum, but I have what I think is a proof to the contrary, though I feel like I'm missing something. Let $f$ be an entire function and let $g$ be the restriction of $f$ to the open unit disk. Then $f$ is an analytic continuation of $g$ and by uniqueness of analytic continuations, $f$ is uniquely determined by $g$ . But $f$ is holomorphic, so by the Cauchy integral formula, $g$ is completely determined by $f$ on the unit circle. Therefore $f$ is completely determined by its values on the unit circle. If we let $h: \mathbb{R} \rightarrow \mathbb{C}$ be the polar parameterisation of $f$ on the unit circle, then since $f$ is holomorphic, $\text{Re}(h(z))$ and $\text{Im}(h(z))$ are (periodic) real continuous functions and $f$ is completely specified by them. But real continuous functions are uniquely specified by their values on the rationals, so the cardinality of the set of real continuous functions is that of the continuum. Therefore the set of entire functions also has the cardinality of the continuum. Am I missing something? I feel like entire functions aren't uniquely determined by a pair of real continuous functions, but I don't see the gap in my reasoning.","I've recently been thinking about entire functions and the Weierstrass factorisation theorem and it got me thinking about the cardinality of the set of entire functions. Clearly is entire, for all , so the cardinality of the set of entire functions is at least that of the continuum. I thought that the set of entire functions would have a cardinality exceeding that of the continuum, but I have what I think is a proof to the contrary, though I feel like I'm missing something. Let be an entire function and let be the restriction of to the open unit disk. Then is an analytic continuation of and by uniqueness of analytic continuations, is uniquely determined by . But is holomorphic, so by the Cauchy integral formula, is completely determined by on the unit circle. Therefore is completely determined by its values on the unit circle. If we let be the polar parameterisation of on the unit circle, then since is holomorphic, and are (periodic) real continuous functions and is completely specified by them. But real continuous functions are uniquely specified by their values on the rationals, so the cardinality of the set of real continuous functions is that of the continuum. Therefore the set of entire functions also has the cardinality of the continuum. Am I missing something? I feel like entire functions aren't uniquely determined by a pair of real continuous functions, but I don't see the gap in my reasoning.",e^{cz} c \in \mathbb{C} f g f f g f g f g f f h: \mathbb{R} \rightarrow \mathbb{C} f f \text{Re}(h(z)) \text{Im}(h(z)) f,"['complex-analysis', 'entire-functions']"
99,Branch cuts for $\sqrt{z^2+1}$,Branch cuts for,\sqrt{z^2+1},"Consider the complex function $$f(z) = \sqrt{z^2+1}.$$ Obviously, $f(z)$ has branch points at $z = \pm i$. One way of defining a branch cut would be to exclude the points on the imaginary axis with $|z| \geq 1$. Another way of defining a branch cut appears to be to exclude the (finite) region of the imaginary axis with $|z| \leq 1$. If we define $f(z)$ as $$f(z) = e^{1/2\log(z^2+1)},$$ the first branch cut can be arrived at by taking the principal branch of $\log(z)$ with the branch cut $(-\infty,0]$. At first I thought the second branch cut could be arrived at by taking the branch cut $[0,\infty)$ for $\log(z)$. Indeed, this would exclude imaginary $z$ with $|z| \leq 1$, but it would of course also exclude all real $z$, thus constituting a different branch cut for $f(z)$. I think it would be possible to arrive at the second branch cut for $f(z)$ differently, by defining $$f(z) = \sqrt{r_1r_2}e^{i(\theta_1+\theta_2)/2}, $$ where $r_1 = |z-i|, r_2 = |z+i|$, $\theta_1  = \arg(z-i), \theta_2 = \arg(z+i).$ However, I don't really like this approach, since I think $$f(z) = e^{1/2\log(z^2+1)}$$ is the proper way to define $f$. Any comments on this? Is there a branch cut for $\log(z)$ which gives the correct branch cut for $f$?","Consider the complex function $$f(z) = \sqrt{z^2+1}.$$ Obviously, $f(z)$ has branch points at $z = \pm i$. One way of defining a branch cut would be to exclude the points on the imaginary axis with $|z| \geq 1$. Another way of defining a branch cut appears to be to exclude the (finite) region of the imaginary axis with $|z| \leq 1$. If we define $f(z)$ as $$f(z) = e^{1/2\log(z^2+1)},$$ the first branch cut can be arrived at by taking the principal branch of $\log(z)$ with the branch cut $(-\infty,0]$. At first I thought the second branch cut could be arrived at by taking the branch cut $[0,\infty)$ for $\log(z)$. Indeed, this would exclude imaginary $z$ with $|z| \leq 1$, but it would of course also exclude all real $z$, thus constituting a different branch cut for $f(z)$. I think it would be possible to arrive at the second branch cut for $f(z)$ differently, by defining $$f(z) = \sqrt{r_1r_2}e^{i(\theta_1+\theta_2)/2}, $$ where $r_1 = |z-i|, r_2 = |z+i|$, $\theta_1  = \arg(z-i), \theta_2 = \arg(z+i).$ However, I don't really like this approach, since I think $$f(z) = e^{1/2\log(z^2+1)}$$ is the proper way to define $f$. Any comments on this? Is there a branch cut for $\log(z)$ which gives the correct branch cut for $f$?",,"['complex-analysis', 'branch-cuts', 'branch-points']"
