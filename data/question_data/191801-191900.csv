,title_raw,title_text,title_latex,body_raw,body_text,body_latex,tags
0,"Value of a determinant involving logarithmic, algebraic and trigonometric functions","Value of a determinant involving logarithmic, algebraic and trigonometric functions",,"If $D(x) = \begin{vmatrix} x&(1+x^2)&x^3\\ \log(1+x^2)&e^x&\sin(x)\\ \cos(x)&\tan(x)&\sin^2(x)\\ \end{vmatrix}$, then find the correct option. (a) $D(x)$ is divisible by $x$ (b) $D(x) = 0$ (c) $\frac{d}{dx}D(x) = 0$ (d) None of these I just found that the correct option is (a). My attempt: Elaborate expansion of the given determinant is not helping towards the solution. I wonder if there is an elegant way to solve this question. Any hint would be very helpful.","If $D(x) = \begin{vmatrix} x&(1+x^2)&x^3\\ \log(1+x^2)&e^x&\sin(x)\\ \cos(x)&\tan(x)&\sin^2(x)\\ \end{vmatrix}$, then find the correct option. (a) $D(x)$ is divisible by $x$ (b) $D(x) = 0$ (c) $\frac{d}{dx}D(x) = 0$ (d) None of these I just found that the correct option is (a). My attempt: Elaborate expansion of the given determinant is not helping towards the solution. I wonder if there is an elegant way to solve this question. Any hint would be very helpful.",,"['derivatives', 'determinant']"
1,How to find critical points when you get constant value,How to find critical points when you get constant value,,"To find these critical points you must first take the derivative of the function. Second, set that derivative equal to 0 and solve for x. Each x value you find is known as a critical number. But what happens if you take derivative and you get a constant value like -1?","To find these critical points you must first take the derivative of the function. Second, set that derivative equal to 0 and solve for x. Each x value you find is known as a critical number. But what happens if you take derivative and you get a constant value like -1?",,"['calculus', 'derivatives']"
2,A true statement with a false contrapositive?,A true statement with a false contrapositive?,,"Here is the statement: Given $f$ is continuous on $[a,b]$ and differentiable on $(a,b)$ . If the graph of $f$ is not a straight line when $x\in[a,b]$ , then $\exists p,q\in(a,b)$ such that $$f'(p)\le \frac{f(b)-f(a)}{b-a}\le f'(q).$$ The statement is TRUE by Mean Value Theorem . Its contrapositive: Given $f$ is continuous on $[a,b]$ and differentiable on $(a,b)$ . If $\forall p,q\in(a,b)$ , $f'(p)>\dfrac{f(b)-f(a)}{b-a}$ or $f'(q)<\dfrac{f(b)-f(a)}{b-a}$ , then the graph of $f$ is a straight line when $x\in[a,b]$ . Which is probably FALSE since $f'(p)\equiv\dfrac{f(b)-f(a)}{b-a}\equiv f'(q)$ for any $p,q\in(a,b)$ when the graph of $f$ is a straight line. I am confused about which part on the above is incorrect..?","Here is the statement: Given is continuous on and differentiable on . If the graph of is not a straight line when , then such that The statement is TRUE by Mean Value Theorem . Its contrapositive: Given is continuous on and differentiable on . If , or , then the graph of is a straight line when . Which is probably FALSE since for any when the graph of is a straight line. I am confused about which part on the above is incorrect..?","f [a,b] (a,b) f x\in[a,b] \exists p,q\in(a,b) f'(p)\le \frac{f(b)-f(a)}{b-a}\le f'(q). f [a,b] (a,b) \forall p,q\in(a,b) f'(p)>\dfrac{f(b)-f(a)}{b-a} f'(q)<\dfrac{f(b)-f(a)}{b-a} f x\in[a,b] f'(p)\equiv\dfrac{f(b)-f(a)}{b-a}\equiv f'(q) p,q\in(a,b) f","['derivatives', 'logic']"
3,Rewrite operator $(\beta \partial/\partial\beta)^n$,Rewrite operator,(\beta \partial/\partial\beta)^n,"I am trying to calculate the following thing $$e^{A \cdot \beta \frac{\partial}{\partial \beta}}e^{i\eta \beta},$$ where $A,\eta$ are constants. Any idea how to write $e^{A \beta \partial/\partial\beta}$ in series? In general $$e^{A \cdot \beta \frac{\partial}{\partial \beta}} = \sum\limits_{n=0}^{\infty} \frac{A^n}{n!}\left(\beta \frac{\partial}{\partial \beta} \right)^n$$ and for $n=2$ I get $$\left(\beta \frac{\partial}{\partial \beta} \right)^2 = \beta \frac{\partial}{\partial \beta} \beta \frac{\partial}{\partial \beta} = \beta \frac{\partial}{\partial \beta} + \beta^2 \frac{\partial^2}{\partial\beta^2}$$","I am trying to calculate the following thing $$e^{A \cdot \beta \frac{\partial}{\partial \beta}}e^{i\eta \beta},$$ where $A,\eta$ are constants. Any idea how to write $e^{A \beta \partial/\partial\beta}$ in series? In general $$e^{A \cdot \beta \frac{\partial}{\partial \beta}} = \sum\limits_{n=0}^{\infty} \frac{A^n}{n!}\left(\beta \frac{\partial}{\partial \beta} \right)^n$$ and for $n=2$ I get $$\left(\beta \frac{\partial}{\partial \beta} \right)^2 = \beta \frac{\partial}{\partial \beta} \beta \frac{\partial}{\partial \beta} = \beta \frac{\partial}{\partial \beta} + \beta^2 \frac{\partial^2}{\partial\beta^2}$$",,"['calculus', 'derivatives']"
4,proof of Box–Muller transform (polar form),proof of Box–Muller transform (polar form),,"There're proofs of Box–Muller transform available online but my book (pattern recognition and machine learning) seems to have put it in a different form. I didn't follow the derivation of equation 11.12 , can anyone please help? Thanks! EDIT As mentioned in Nadiels's answer, there's a mistake in formula 11.10 and 11.11, as logarithm has to take in a positive number (PRML errata ).","There're proofs of Box–Muller transform available online but my book (pattern recognition and machine learning) seems to have put it in a different form. I didn't follow the derivation of equation 11.12 , can anyone please help? Thanks! EDIT As mentioned in Nadiels's answer, there's a mistake in formula 11.10 and 11.11, as logarithm has to take in a positive number (PRML errata ).",,"['calculus', 'probability', 'derivatives', 'normal-distribution', 'sampling']"
5,What's the Derivative of Order $\frac{1}{3}$ of $f(x)=x$?,What's the Derivative of Order  of ?,\frac{1}{3} f(x)=x,"So, I'm very new to Fractional Calculus, and I don't know too much about series, but I'm very interested in Fractional Calculus, so I wanted to know if you could find the solution to this problem. Using the general formula for derivatives: $$\frac{d^n}{dx^n}f(x)=\lim_{\epsilon \to 0} \frac{\Gamma(n+1)}{\epsilon^n}\sum_{j=0}^{\lfloor\frac{x}{\epsilon}\rfloor}\frac{f(x-j\epsilon)(-1)^j}{j!\Gamma(n+1-j)}$$ Where $\Gamma(x)$ is the gamma function. The upper limit to the summation (because it's a little hard to see) is $\lfloor{\frac{x}{\epsilon}\rfloor}$ , which is the floor function. I want to find: $$\frac{d^{\frac{1}{3}}}{dx^{\frac{1}{3}}}(x)$$ The place where I get stuck is interpreting the upper limit of the summation. Am I supposed to treat it as an infinite series since it has a variable? The place I got this formula is: Link w/ Formula","So, I'm very new to Fractional Calculus, and I don't know too much about series, but I'm very interested in Fractional Calculus, so I wanted to know if you could find the solution to this problem. Using the general formula for derivatives: Where is the gamma function. The upper limit to the summation (because it's a little hard to see) is , which is the floor function. I want to find: The place where I get stuck is interpreting the upper limit of the summation. Am I supposed to treat it as an infinite series since it has a variable? The place I got this formula is: Link w/ Formula",\frac{d^n}{dx^n}f(x)=\lim_{\epsilon \to 0} \frac{\Gamma(n+1)}{\epsilon^n}\sum_{j=0}^{\lfloor\frac{x}{\epsilon}\rfloor}\frac{f(x-j\epsilon)(-1)^j}{j!\Gamma(n+1-j)} \Gamma(x) \lfloor{\frac{x}{\epsilon}\rfloor} \frac{d^{\frac{1}{3}}}{dx^{\frac{1}{3}}}(x),"['calculus', 'derivatives', 'summation', 'fractional-calculus']"
6,The derivative of the logistic function,The derivative of the logistic function,,"The logistic function is $\frac{1}{1+e^{-x}}$ , and its derivative is $f(x)*(1-f(x))$ . In the following page on Wikipedia , it shows the following equation: $$f(x) = \frac{1}{1+e^{-x}} = \frac{e^x}{1+e^x}$$ which means $$f'(x) = e^x (1+e^x) - e^x \frac{e^x}{(1+e^x)^2} = \frac{e^x}{(1+e^x)^2}$$ I understand it so far, which uses the quotient rule $$\left(\frac{g(x)}{h(x)}\right)' = \frac{g'(x)h(x) - g(x)h'(x)}{h(x)^2}.$$ However, why is it then transformed into $f(x) * (1-f(x))$ ?","The logistic function is , and its derivative is . In the following page on Wikipedia , it shows the following equation: which means I understand it so far, which uses the quotient rule However, why is it then transformed into ?",\frac{1}{1+e^{-x}} f(x)*(1-f(x)) f(x) = \frac{1}{1+e^{-x}} = \frac{e^x}{1+e^x} f'(x) = e^x (1+e^x) - e^x \frac{e^x}{(1+e^x)^2} = \frac{e^x}{(1+e^x)^2} \left(\frac{g(x)}{h(x)}\right)' = \frac{g'(x)h(x) - g(x)h'(x)}{h(x)^2}. f(x) * (1-f(x)),"['derivatives', 'logistic-regression']"
7,What's the value of $\theta$ in Lagrange's form of remainder $R_{n}$ for the expansion of $\frac{1}{1-x}$,What's the value of  in Lagrange's form of remainder  for the expansion of,\theta R_{n} \frac{1}{1-x},"The n-th remainder in the expression of $\frac{1}{1-x}$ , $R_{n}$ = $\frac{x^n}{n!}f^n(\theta x)$ .  And, $f^n(\theta x)$ = $\frac{(-1)^n n!}{(1-\theta x)^{n+1}}$ I have to evaluate the value of $\theta$ here. But I'm clueless about what steps to take. I've tried expanding $\frac{1}{1-x}$ into its polynomial form but i cannot derive anything from it. any help would be appreciated.","The n-th remainder in the expression of $\frac{1}{1-x}$ , $R_{n}$ = $\frac{x^n}{n!}f^n(\theta x)$ .  And, $f^n(\theta x)$ = $\frac{(-1)^n n!}{(1-\theta x)^{n+1}}$ I have to evaluate the value of $\theta$ here. But I'm clueless about what steps to take. I've tried expanding $\frac{1}{1-x}$ into its polynomial form but i cannot derive anything from it. any help would be appreciated.",,"['calculus', 'derivatives']"
8,"Construct a continuous function $f: (0, 1) \rightarrow \mathbb R$ but is not differentiable at any point of $\{\frac{1}{1+n}: n \in\mathbb N\}$",Construct a continuous function  but is not differentiable at any point of,"f: (0, 1) \rightarrow \mathbb R \{\frac{1}{1+n}: n \in\mathbb N\}","I can construct many function continuous in this interval . I have to choose one function that is not differentiable in $(0,1)$ because all point of $\{\frac{1}{1+n}: n \in\mathbb N\}$ is in the interval $(0,1)$,but i don't know that function choose that  is not differentiable,moreover I don't know like prove that one function is not differentiable, with the definition or have to use other theorem?","I can construct many function continuous in this interval . I have to choose one function that is not differentiable in $(0,1)$ because all point of $\{\frac{1}{1+n}: n \in\mathbb N\}$ is in the interval $(0,1)$,but i don't know that function choose that  is not differentiable,moreover I don't know like prove that one function is not differentiable, with the definition or have to use other theorem?",,"['real-analysis', 'derivatives', 'continuity']"
9,Finding a line that passes through a point and lies tangent to a curve,Finding a line that passes through a point and lies tangent to a curve,,"I'm trying to find the equation of a line that passes through some point $(x_1,y_1)$ and lies tangent to an ellipse. There are two solutions in most cases. My work starts with the equation of a line $y-y_1 = m(x-x_1)$. The slope of the line, $m$, will be $$\frac{y_1 - f(x)}{x_1-x} = f'(x)$$Normally I could substitute the equation of the curve in for $f(x)$ and $f'(x)$ and after some sweat, tears and algebra, solve for $x$, which can then easily be used to solve for all other information I need. However, ellipses are a pain to work with, especially in $f(x)=$ form, which I would need to use for this method. Even using wolfram alpha, the general solution for x is so extremely long and complicated that it's basically useless to me. Ellipses can be much more simply expressed in parametric form, so I tried the same approach using parametric equations $y(t)$ and $x(t)$, but I ended up with $t \sin t = \cdots$ which isn't solvable conventionally and ended up being equally as useless. My final idea was set the equation of the line equal to the ellipse. $$m(x-x_1) + y_1 = \sqrt{b^2 \left( 1-\frac{(x-h)^2}{a^2} \right)}+k$$The tangent line touches the ellipse, not intersects it, in other words it only has one point of equality with the ellipse. So I would somehow need to find the two values of $m$ that gives one point of equality instead of two, also a potentially painful and time-consuming process which I don't know how to do. Is there a better way to solve this? If this last method is the most viable solution, how would I go about this?","I'm trying to find the equation of a line that passes through some point $(x_1,y_1)$ and lies tangent to an ellipse. There are two solutions in most cases. My work starts with the equation of a line $y-y_1 = m(x-x_1)$. The slope of the line, $m$, will be $$\frac{y_1 - f(x)}{x_1-x} = f'(x)$$Normally I could substitute the equation of the curve in for $f(x)$ and $f'(x)$ and after some sweat, tears and algebra, solve for $x$, which can then easily be used to solve for all other information I need. However, ellipses are a pain to work with, especially in $f(x)=$ form, which I would need to use for this method. Even using wolfram alpha, the general solution for x is so extremely long and complicated that it's basically useless to me. Ellipses can be much more simply expressed in parametric form, so I tried the same approach using parametric equations $y(t)$ and $x(t)$, but I ended up with $t \sin t = \cdots$ which isn't solvable conventionally and ended up being equally as useless. My final idea was set the equation of the line equal to the ellipse. $$m(x-x_1) + y_1 = \sqrt{b^2 \left( 1-\frac{(x-h)^2}{a^2} \right)}+k$$The tangent line touches the ellipse, not intersects it, in other words it only has one point of equality with the ellipse. So I would somehow need to find the two values of $m$ that gives one point of equality instead of two, also a potentially painful and time-consuming process which I don't know how to do. Is there a better way to solve this? If this last method is the most viable solution, how would I go about this?",,"['derivatives', 'systems-of-equations', 'elliptic-functions']"
10,"Show that $\nabla f \in C^{1,1}$ if the difference between $f$ and its linear interpolant has a quadratic bound",Show that  if the difference between  and its linear interpolant has a quadratic bound,"\nabla f \in C^{1,1} f","Let $f : \mathbb{R}^d \to \mathbb{R}$ be a differentiable function such that for every $x,y$ and every $\lambda \in [0,1]$: $$|\lambda f(x) + (1-\lambda) f(y) -f(\lambda x + (1-\lambda)y)| \leqslant |x-y|^2 $$ holds. How can one show that there is a constant $C>0$ such that we have : $$|\nabla f (x) - \nabla f (y)| \leqslant  C |x-y|,$$ for every $x,y$. I did not find a way to solve this.","Let $f : \mathbb{R}^d \to \mathbb{R}$ be a differentiable function such that for every $x,y$ and every $\lambda \in [0,1]$: $$|\lambda f(x) + (1-\lambda) f(y) -f(\lambda x + (1-\lambda)y)| \leqslant |x-y|^2 $$ holds. How can one show that there is a constant $C>0$ such that we have : $$|\nabla f (x) - \nabla f (y)| \leqslant  C |x-y|,$$ for every $x,y$. I did not find a way to solve this.",,"['real-analysis', 'derivatives', 'convex-analysis', 'vector-analysis']"
11,"To what extent $\frac{d}{dx} \int_{-\infty}^{\infty} f(t,x)\, dt = \int_{-\infty}^{\infty}\frac{d}{dx}f(t,x)\, dt$",To what extent,"\frac{d}{dx} \int_{-\infty}^{\infty} f(t,x)\, dt = \int_{-\infty}^{\infty}\frac{d}{dx}f(t,x)\, dt","Assume $\int_{-\infty}^{\infty} f(t,x)\, dt$ exists for every $x \in {\mathbb R}^n$. Then, to what extent, the following equation holds? $$\frac{d}{dx} \int_{-\infty}^{\infty} f(t,x)\, dt = \int_{-\infty}^{\infty}\frac{d}{dx}f(t,x) \,dt.$$","Assume $\int_{-\infty}^{\infty} f(t,x)\, dt$ exists for every $x \in {\mathbb R}^n$. Then, to what extent, the following equation holds? $$\frac{d}{dx} \int_{-\infty}^{\infty} f(t,x)\, dt = \int_{-\infty}^{\infty}\frac{d}{dx}f(t,x) \,dt.$$",,"['calculus', 'integration', 'derivatives']"
12,Matrix differentiation for the trace of matrix multiplication of Hadamard product,Matrix differentiation for the trace of matrix multiplication of Hadamard product,,"I struggle with taking the derivative of the following equation: $\frac{∂}{∂B}Tr(A(B⊙C))$ where A,B,C are matrices, $Tr(.)$ is the trace of a matrix, and ⊙ is the Hadamard product. I appreciate any help.","I struggle with taking the derivative of the following equation: $\frac{∂}{∂B}Tr(A(B⊙C))$ where A,B,C are matrices, $Tr(.)$ is the trace of a matrix, and ⊙ is the Hadamard product. I appreciate any help.",,"['derivatives', 'partial-derivative', 'matrix-calculus', 'trace', 'hadamard-product']"
13,How is position differentiable?,How is position differentiable?,,"Consider this graph of some point's position vector's endpoint - a function of time - on a Cartesian plane: This is a perfectly normal path - after all, in real world there is nothing keeping point from taking a sharp turn like this. However, in mathematical physics, we consider position vector as a differentiable function, where $\dfrac{df(t)}{dt}=(\dfrac{df_x(t)}{dt},\dfrac{df_y(t)}{dt},\dfrac{df_z(t)}{dt})$. In the graph I posted, the component $f_x$ seems to be undifferentiable at point $f_x(t)$, since there its goes through a cusp. Is there something I'm not seeing and there is a derivative of $f_x$ at $t$, or is this point really undifferentiable? And if it is, why do we define position vector as a differentiable function?","Consider this graph of some point's position vector's endpoint - a function of time - on a Cartesian plane: This is a perfectly normal path - after all, in real world there is nothing keeping point from taking a sharp turn like this. However, in mathematical physics, we consider position vector as a differentiable function, where $\dfrac{df(t)}{dt}=(\dfrac{df_x(t)}{dt},\dfrac{df_y(t)}{dt},\dfrac{df_z(t)}{dt})$. In the graph I posted, the component $f_x$ seems to be undifferentiable at point $f_x(t)$, since there its goes through a cusp. Is there something I'm not seeing and there is a derivative of $f_x$ at $t$, or is this point really undifferentiable? And if it is, why do we define position vector as a differentiable function?",,"['calculus', 'real-analysis', 'derivatives', 'physics']"
14,Minimization of a determinant,Minimization of a determinant,,"Let  $ \mathbf{x}_m = \mathbf{z}_m - \alpha\mathbf{v} \in \mathbb{C}^N$, with $m=1,\dots,M$, be $M$ complex vectors such that $\mathbf{z}_m, m=1,\dots,M$ and $\mathbf{v}$ complex $N$-dimensional vectors, $\alpha$ a complex scalar. Define the matrix $\mathbf{X}$ of dimension $N\times M$ as: $$ \mathbf{X} = [\mathbf{x}_1|\mathbf{x}_2|\cdots|\mathbf{x}_M]. $$ How can I solve the following minimization problem: $$ \min_{\alpha}\det(\mathbf{I} + \mathbf{X}^H\mathbf{S}^{-1}\mathbf{X}) $$ where $\mathbf{I}$ indicates the identity matrix of dimension $M \times M$, $\mathbf{S}$ is an invertible, Hermitian and positive definete matrix of dimension $N \times N$. Thanks!","Let  $ \mathbf{x}_m = \mathbf{z}_m - \alpha\mathbf{v} \in \mathbb{C}^N$, with $m=1,\dots,M$, be $M$ complex vectors such that $\mathbf{z}_m, m=1,\dots,M$ and $\mathbf{v}$ complex $N$-dimensional vectors, $\alpha$ a complex scalar. Define the matrix $\mathbf{X}$ of dimension $N\times M$ as: $$ \mathbf{X} = [\mathbf{x}_1|\mathbf{x}_2|\cdots|\mathbf{x}_M]. $$ How can I solve the following minimization problem: $$ \min_{\alpha}\det(\mathbf{I} + \mathbf{X}^H\mathbf{S}^{-1}\mathbf{X}) $$ where $\mathbf{I}$ indicates the identity matrix of dimension $M \times M$, $\mathbf{S}$ is an invertible, Hermitian and positive definete matrix of dimension $N \times N$. Thanks!",,"['linear-algebra', 'derivatives', 'optimization', 'convex-optimization', 'matrix-calculus']"
15,Find Minima and Maxima of $ y = \frac{x^2-3x+2}{x^2+2x+1}$,Find Minima and Maxima of, y = \frac{x^2-3x+2}{x^2+2x+1},"$$ y = \frac{x^2-3x+2}{x^2+2x+1}$$ I guess I made some mistakes cause after taking the first derivative and simlifying I have $$y = \frac{2x^3-4x^2+5}{(x+1)^2}$$ but then numerator has complex roots. which should not be, IMO","$$ y = \frac{x^2-3x+2}{x^2+2x+1}$$ I guess I made some mistakes cause after taking the first derivative and simlifying I have $$y = \frac{2x^3-4x^2+5}{(x+1)^2}$$ but then numerator has complex roots. which should not be, IMO",,"['derivatives', 'maxima-minima']"
16,differentiation under the integral sign - moments,differentiation under the integral sign - moments,,"I want to find $$\mu_r' = \int_0^\infty y^r\theta e^{-\theta y}dy $$ A day ago I read something here on MSE about differentiation under the integral sign. I am not sure of how it works, however I tried to differentiate wrt $r$ for $r$ times, but I just get a huge expression. Considering $\theta e^{-\theta y}$ a constant wrt $r$ I get first $ry^{r-1}$ then $y^{r-1}+r(r-1)y^{r-2}$ then $(r-1)y^{r-2}+(r-1)y^{r-2}+(ry^{r-2}+r(r-1)(r-2)y^{r-3})$ and so on. I can see there must be a pattern, but I can't find it. How can I find the general formula for $\mu_r'$ ? Edit I am pretty sure that the solution is something of the form $$\frac{r!}{\theta^r}$$ as this is what you get when doing each case separately Edit 2 As seen in the answers below my derivative is wrong, still I have no clue on how to solve the problem","I want to find $$\mu_r' = \int_0^\infty y^r\theta e^{-\theta y}dy $$ A day ago I read something here on MSE about differentiation under the integral sign. I am not sure of how it works, however I tried to differentiate wrt $r$ for $r$ times, but I just get a huge expression. Considering $\theta e^{-\theta y}$ a constant wrt $r$ I get first $ry^{r-1}$ then $y^{r-1}+r(r-1)y^{r-2}$ then $(r-1)y^{r-2}+(r-1)y^{r-2}+(ry^{r-2}+r(r-1)(r-2)y^{r-3})$ and so on. I can see there must be a pattern, but I can't find it. How can I find the general formula for $\mu_r'$ ? Edit I am pretty sure that the solution is something of the form $$\frac{r!}{\theta^r}$$ as this is what you get when doing each case separately Edit 2 As seen in the answers below my derivative is wrong, still I have no clue on how to solve the problem",,"['integration', 'derivatives', 'moment-generating-functions']"
17,"If $f'(a)$ exists, does $f'(a^+)$ and $f'(a^-)$ exist?","If  exists, does  and  exist?",f'(a) f'(a^+) f'(a^-),"Is it true that If $f(x)$ is  differentiable at $a$, then both $f'(a^+)$ and $f'(a^-)$ exist and $f'(a^+)=f'(a^-)=f'(a)$. My answer is NO . Consider the function $$ f(x)=\begin{cases} x^2\sin\dfrac{1}{x}&\text{for $x\ne0$}\\[1ex] 0&\text{for $x=0$} \end{cases} $$ $f'(0)$ can be found by \begin{align} \lim_{x \to 0} \dfrac{f(x) - f(0)}{x-0} & = \lim_{x \to 0} \dfrac{f(x) - 0}{x}  & \textrm{ as } f(0) = 0 \\ & = \lim_{x \to 0} \dfrac{x^2 \sin\left(\frac{1}{x}\right)}{x} & \\ & = \lim_{x \to 0} x \sin\left(\frac{1}{x}\right) & \end{align} Now we can use the Squeeze Theorem. As $-1 \leq \sin\left(\frac{1}{x}\right) \leq 1$, we have that $$0 = \lim_{x \to 0} x \cdot -1 \leq \lim_{x \to 0} x \sin\left(\frac{1}{x}\right) \leq \lim_{x \to 0} x \cdot 1 = 0$$ Therefore, $\lim_{x \to 0} x \sin\left(\frac{1}{x}\right) = 0$ and we have $f'(0)=0$. However,  $$ f'(x)=\begin{cases} -\cos\dfrac{1}{x}+2x\sin\dfrac{1}{x}&\text{for $x\ne0$}\\[1ex] 0&\text{for $x=0$} \end{cases} $$ $f'(0^+)$ nor $f'(0^-)$ exists as $x\to 0$. Is my answer correct? I have found some pages related to this question. Is $f'$ continuous at $0$ if $f(x)=x^2\sin(1/x)$ Calculating derivative by definition vs not by definition Differentiability of $f(x) = x^2 \sin{\frac{1}{x}}$ and $f'$ $f'$ exists, but $\lim \frac{f(x)-f(y)}{x-y}$ does not exist Thanks.","Is it true that If $f(x)$ is  differentiable at $a$, then both $f'(a^+)$ and $f'(a^-)$ exist and $f'(a^+)=f'(a^-)=f'(a)$. My answer is NO . Consider the function $$ f(x)=\begin{cases} x^2\sin\dfrac{1}{x}&\text{for $x\ne0$}\\[1ex] 0&\text{for $x=0$} \end{cases} $$ $f'(0)$ can be found by \begin{align} \lim_{x \to 0} \dfrac{f(x) - f(0)}{x-0} & = \lim_{x \to 0} \dfrac{f(x) - 0}{x}  & \textrm{ as } f(0) = 0 \\ & = \lim_{x \to 0} \dfrac{x^2 \sin\left(\frac{1}{x}\right)}{x} & \\ & = \lim_{x \to 0} x \sin\left(\frac{1}{x}\right) & \end{align} Now we can use the Squeeze Theorem. As $-1 \leq \sin\left(\frac{1}{x}\right) \leq 1$, we have that $$0 = \lim_{x \to 0} x \cdot -1 \leq \lim_{x \to 0} x \sin\left(\frac{1}{x}\right) \leq \lim_{x \to 0} x \cdot 1 = 0$$ Therefore, $\lim_{x \to 0} x \sin\left(\frac{1}{x}\right) = 0$ and we have $f'(0)=0$. However,  $$ f'(x)=\begin{cases} -\cos\dfrac{1}{x}+2x\sin\dfrac{1}{x}&\text{for $x\ne0$}\\[1ex] 0&\text{for $x=0$} \end{cases} $$ $f'(0^+)$ nor $f'(0^-)$ exists as $x\to 0$. Is my answer correct? I have found some pages related to this question. Is $f'$ continuous at $0$ if $f(x)=x^2\sin(1/x)$ Calculating derivative by definition vs not by definition Differentiability of $f(x) = x^2 \sin{\frac{1}{x}}$ and $f'$ $f'$ exists, but $\lim \frac{f(x)-f(y)}{x-y}$ does not exist Thanks.",,"['calculus', 'derivatives']"
18,How to differentiate the square root of the function inside another square root of the function?,How to differentiate the square root of the function inside another square root of the function?,,If a function is defined as sum of radicals in another radicals. Then how to differentiate this function $$\sqrt{\cos x+\sqrt{\cos x+\sqrt{\cos x+\dots}}}\quad?$$,If a function is defined as sum of radicals in another radicals. Then how to differentiate this function $$\sqrt{\cos x+\sqrt{\cos x+\sqrt{\cos x+\dots}}}\quad?$$,,['derivatives']
19,Point Guard Problem Derivative,Point Guard Problem Derivative,,"I am currently taking calculus course and is in derivative section. I have this problem i currently am struggling a lot on how to solve... A point guard for an NBA team averages 15 free-throw opportunities per   game. He currently hits 72% of his free throws. As he improves, the   number of free-throws opportunities decreases by 1 free throw per   game. and his percentage of free throws made increases by 0.5   percentage point per game. When his decreasing free throws and   increasing percentage are taken into account, what is the rate of   change in the number of free-throw points that this point guard makes   per game? What does it mean when it says what is the rate of change in the number of free throw point guard makes per game ?","I am currently taking calculus course and is in derivative section. I have this problem i currently am struggling a lot on how to solve... A point guard for an NBA team averages 15 free-throw opportunities per   game. He currently hits 72% of his free throws. As he improves, the   number of free-throws opportunities decreases by 1 free throw per   game. and his percentage of free throws made increases by 0.5   percentage point per game. When his decreasing free throws and   increasing percentage are taken into account, what is the rate of   change in the number of free-throw points that this point guard makes   per game? What does it mean when it says what is the rate of change in the number of free throw point guard makes per game ?",,"['calculus', 'derivatives']"
20,"When both $\frac{dy}{dt}$ and $\frac{dx}{dt} = 0$, what happens?","When both  and , what happens?",\frac{dy}{dt} \frac{dx}{dt} = 0,"As we know that if $\frac{dy}{dt}=0$ and $\frac{dx}{dt}\neq0$ at $x_0$, then the equation has a horizontal line at $x_0$. Moreover, when $\frac{dx}{dt}=0$ and $\frac{dy}{dt}\neq0$, then there is a vertical line. What happens if both $\frac{dy}{dt}=0$ and $\frac{dx}{dt}=0$?","As we know that if $\frac{dy}{dt}=0$ and $\frac{dx}{dt}\neq0$ at $x_0$, then the equation has a horizontal line at $x_0$. Moreover, when $\frac{dx}{dt}=0$ and $\frac{dy}{dt}\neq0$, then there is a vertical line. What happens if both $\frac{dy}{dt}=0$ and $\frac{dx}{dt}=0$?",,"['derivatives', 'parametric']"
21,Integration with a little bit of Leibnitz rule,Integration with a little bit of Leibnitz rule,,"The problem is simple. But the solution is where I get my doubts. Also I would appreciate if anyone could fill me in with all the necessary points where I need to check for convergation for the procedure to correct. $$\int_0^\infty \frac{\cos(bx)-\cos(cx)}x e^{-ax} \,dx $$ for all $a,b,c \in \rm I\!R  ; a>0$ From here it's obvious that we can rewrite this as $$\frac d{dt} \left.\int_0^\infty \frac{cos(tx)}x e^{-ax}\right|_c^b \,dx =\int_0^\infty \int_c^b {\sin(tx)} e^{-ax} \,dx \,dt= \int_c^b I\,dt$$ Then via double per partes  $$I=\int_0^\infty {\sin(tx)} e^{-ax}\,dx = \left.\frac{-1}{a}\sin(tx)e^{-ax}\right|_{t=0}^\infty + \frac ta \int_0^\infty {cos(tx)} e^{-ax}dx =$$ $$=\left. \frac{-\sin(tx)e^{-ax}}a \right|_{t=0}^\infty - \left. \frac t a {\cos(tx)e^{-ax}}\right|_{t=0}^\infty - \frac{t^2}{a^2}\int_0^\infty {\sin(tx)} e^{-ax}\,dx$$ $$I=\frac ta-\frac{t^2}{a^2}I$$ $$I=\frac{\frac ta}{1+\frac{t^2}{a^2}}$$ $$\int_c^b\frac{\frac ta}{1+\frac{t^2}{a^2}}=\int_{c/a}^{b/a}\frac{u}{1+u^2}=\frac12\ln(\frac dc)$$ I find it odd that my final result is not dependant at all of parameter $a$ Just does not sound right. And I would appreciate filling out where covnergation or any other possible breaking points need to be checked. EDIT: A typo...probably more to come.","The problem is simple. But the solution is where I get my doubts. Also I would appreciate if anyone could fill me in with all the necessary points where I need to check for convergation for the procedure to correct. $$\int_0^\infty \frac{\cos(bx)-\cos(cx)}x e^{-ax} \,dx $$ for all $a,b,c \in \rm I\!R  ; a>0$ From here it's obvious that we can rewrite this as $$\frac d{dt} \left.\int_0^\infty \frac{cos(tx)}x e^{-ax}\right|_c^b \,dx =\int_0^\infty \int_c^b {\sin(tx)} e^{-ax} \,dx \,dt= \int_c^b I\,dt$$ Then via double per partes  $$I=\int_0^\infty {\sin(tx)} e^{-ax}\,dx = \left.\frac{-1}{a}\sin(tx)e^{-ax}\right|_{t=0}^\infty + \frac ta \int_0^\infty {cos(tx)} e^{-ax}dx =$$ $$=\left. \frac{-\sin(tx)e^{-ax}}a \right|_{t=0}^\infty - \left. \frac t a {\cos(tx)e^{-ax}}\right|_{t=0}^\infty - \frac{t^2}{a^2}\int_0^\infty {\sin(tx)} e^{-ax}\,dx$$ $$I=\frac ta-\frac{t^2}{a^2}I$$ $$I=\frac{\frac ta}{1+\frac{t^2}{a^2}}$$ $$\int_c^b\frac{\frac ta}{1+\frac{t^2}{a^2}}=\int_{c/a}^{b/a}\frac{u}{1+u^2}=\frac12\ln(\frac dc)$$ I find it odd that my final result is not dependant at all of parameter $a$ Just does not sound right. And I would appreciate filling out where covnergation or any other possible breaking points need to be checked. EDIT: A typo...probably more to come.",,"['derivatives', 'definite-integrals']"
22,Definition of $e$,Definition of,e,"Definition of $e$ $\ln (e)= \int_\limits{1}^e\frac{1}{t}dt=1$ I found this in my calculus textbook, and I was wondering what does this exactly ""mean"".  This is more of a research question which I could do, but where would I start.   I understand that its an integral, and what it states; however, how was this derived in calculus?  Where would be a good book to research to understand an ideal proof of the equation?","Definition of I found this in my calculus textbook, and I was wondering what does this exactly ""mean"".  This is more of a research question which I could do, but where would I start.   I understand that its an integral, and what it states; however, how was this derived in calculus?  Where would be a good book to research to understand an ideal proof of the equation?",e \ln (e)= \int_\limits{1}^e\frac{1}{t}dt=1,"['calculus', 'derivatives', 'reference-request', 'soft-question']"
23,Derivatives of determinants and trace with respect a scalar parameter,Derivatives of determinants and trace with respect a scalar parameter,,"Consider the following two matrices, $A$ and $B.$ The dimension of both $A$ and $B$ are $n\times n,$ and all element of $A$ and $B$ depends on a scalar parameter $\theta .$ Then what is derivatives of $\ln \left\vert A\right\vert $ and $tr\left( AB\right) $ wrt to $\theta ?$ $\frac{\partial \ln \left\vert A\right\vert }{\partial \theta }$ and $\frac{\partial tr\left( AB\right) }{\partial \theta }$? Any reference? Thanks","Consider the following two matrices, $A$ and $B.$ The dimension of both $A$ and $B$ are $n\times n,$ and all element of $A$ and $B$ depends on a scalar parameter $\theta .$ Then what is derivatives of $\ln \left\vert A\right\vert $ and $tr\left( AB\right) $ wrt to $\theta ?$ $\frac{\partial \ln \left\vert A\right\vert }{\partial \theta }$ and $\frac{\partial tr\left( AB\right) }{\partial \theta }$? Any reference? Thanks",,"['derivatives', 'determinant', 'trace']"
24,Show that $\nabla m_t$ is a polynomial o degree $p-1$,Show that  is a polynomial o degree,\nabla m_t p-1,"If $m_t=\sum_{k=0}^p c_kt^k$ $t=0,\pm 1,\pm 2,\dots$ show that $\nabla  m_t$ is a polynomial o degree $p-1$ int $t$ and hence that   $\nabla^{p+1} m_t=0$ EDIT: I made a mistake $\nabla$ is not a gradient operator, it is a difference operator, such that $\nabla x_t=x_t-x_{ t-1}$ $$\nabla m_t=\sum_{k=0}^p c_kt^k-\sum_{k=0}^p c_k(t-1)^k$$ $$=c_pt^p+\sum_{k=0}^{p-1}c_kt^k-c_p(t-1)^p-\sum_{k=0}^{p-1}c_k(t-1)^k$$ $$=c_pt^p-c_p(t-1)^p+\sum_{k=0}^{p-1}c_kt^k-\sum_{k=0}^{p-1}c_k(t-1)^k$$ $$=c_p(t^p-(t-1)^p)+\sum_{k=0}^{p-1}c_kt^k-\sum_{k=0}^{p-1}c_k(t-1)^k$$ What I know is that when I expand $(t-1)^p$, it will give a coefficient $t^p$ that will cancel with other $t^p$ then the order of polynomial will be $p-1$, but I do not know how to develop it formally. The answer just say $$\nabla m_t=pc_pt^{p-1}+\sum_{k=0}^{p-2}b_kt^k$$","If $m_t=\sum_{k=0}^p c_kt^k$ $t=0,\pm 1,\pm 2,\dots$ show that $\nabla  m_t$ is a polynomial o degree $p-1$ int $t$ and hence that   $\nabla^{p+1} m_t=0$ EDIT: I made a mistake $\nabla$ is not a gradient operator, it is a difference operator, such that $\nabla x_t=x_t-x_{ t-1}$ $$\nabla m_t=\sum_{k=0}^p c_kt^k-\sum_{k=0}^p c_k(t-1)^k$$ $$=c_pt^p+\sum_{k=0}^{p-1}c_kt^k-c_p(t-1)^p-\sum_{k=0}^{p-1}c_k(t-1)^k$$ $$=c_pt^p-c_p(t-1)^p+\sum_{k=0}^{p-1}c_kt^k-\sum_{k=0}^{p-1}c_k(t-1)^k$$ $$=c_p(t^p-(t-1)^p)+\sum_{k=0}^{p-1}c_kt^k-\sum_{k=0}^{p-1}c_k(t-1)^k$$ What I know is that when I expand $(t-1)^p$, it will give a coefficient $t^p$ that will cancel with other $t^p$ then the order of polynomial will be $p-1$, but I do not know how to develop it formally. The answer just say $$\nabla m_t=pc_pt^{p-1}+\sum_{k=0}^{p-2}b_kt^k$$",,"['derivatives', 'polynomials', 'proof-verification', 'self-learning', 'time-series']"
25,Are there physical interpretations of fractional order integrals and derivatives?,Are there physical interpretations of fractional order integrals and derivatives?,,"Integral and differential calculus restricted to integer order operations of integration and differentiation have solid, definite connections with models of the physical world such as figuring areas, volumes, rates of change, etc. But what of the fractional calculus? Are there any physical interpretations that come from fractional order derivatives and integrals?","Integral and differential calculus restricted to integer order operations of integration and differentiation have solid, definite connections with models of the physical world such as figuring areas, volumes, rates of change, etc. But what of the fractional calculus? Are there any physical interpretations that come from fractional order derivatives and integrals?",,"['integration', 'derivatives', 'mathematical-modeling', 'fractional-calculus']"
26,"Is there an alternative definition of uniform differentiability that implies differentiability, instead of requiring it?","Is there an alternative definition of uniform differentiability that implies differentiability, instead of requiring it?",,"Let $I \subseteq \mathbb{R}$ be an interval and let $f: I \to \mathbb{R}$. Let's start looking at the definitions of continuity, uniform continuity and differentiability. Yes, I really like sentences in the language of set theory . Definition 1: $f$ is continuous at $t \in I$ if $$\forall \, \varepsilon > 0 \quad \exists \, \delta > 0 \quad \forall \, x \in (t-\delta, t+\delta) \cap I \quad |f(x)-f(t)| < \varepsilon$$ Definition 2: $f$ is continuous on $I$ if $$\forall \, t \in I \quad \forall \, \varepsilon > 0 \quad \exists \, \delta > 0 \quad \forall \, x \in (t-\delta, t+\delta) \cap I \quad |f(x)-f(t)| < \varepsilon$$ Definition 3: $f$ is uniformly continuous on $I$ if $$\forall \, \varepsilon > 0 \quad \exists \, \delta > 0 \quad \forall \, t \in I \quad \forall \, x \in (t-\delta, t+\delta) \cap I \quad |f(x)-f(t)| < \varepsilon$$ or, put more nicely, $$\forall \, \varepsilon > 0 \quad \exists \, \delta > 0 \quad \forall \, x \in I \quad \forall \, y \in I \quad \bigg[ \quad |x-y|<\delta \implies |f(x)-f(t)| < \varepsilon \quad \bigg]$$ Definition 4: $f$ is differentiable at $t \in I$ if $$\exists \, v \in \mathbb{R} \quad \forall \, \varepsilon > 0 \quad \exists \, \delta > 0 \quad \forall \, x \in (t-\delta, t+\delta) \cap I \setminus \!\{t\} \quad \left|\,\dfrac{f(x)-f(t)}{x-t} - v\,\right| < \varepsilon$$ Definition 5: $f$ is differentiable on $I$ if $$\forall \, t \in I \quad \exists \, v \in \mathbb{R} \quad \forall \, \varepsilon > 0 \quad \exists \, \delta > 0 \quad \forall \, x \in (t-\delta, t+\delta) \cap I \setminus \!\{t\} \quad \left|\,\dfrac{f(x)-f(t)}{x-t} - v\,\right| < \varepsilon$$ Now, note that in the definition of uniform continuity, we did not require the function to be continuous; instead, our definition implies it (quite trivially!). Now let's proceed to the most common definition 1 of uniform differentiability: Definition 6: $f$ is uniformly differentiable on $I$ if $$f \,\,\text{is differentiable on} \,\,I \text{,} \qquad \text{and}$$ $$\forall \, \varepsilon > 0 \quad \exists \, \delta > 0 \quad \forall \, t \in I \quad \forall \, x \in (t-\delta, t+\delta) \cap I \setminus \!\{t\} \quad \left|\,\dfrac{f(x)-f(t)}{x-t} - f'(t)\,\right| < \varepsilon$$ or, equivalently, $$f \,\,\text{is differentiable on} \,\,I \text{,} \qquad \text{and}$$ $\quad \forall \, \varepsilon > 0 \quad \exists \, \delta > 0$ $$\quad \forall \, t \in I \quad \forall \, h \quad \bigg[ \enspace \big(\,\, 0 < |h| < \delta \,\,\big) \land \big(\,\, t+h \in I \,\,\big) \implies \left|\,\dfrac{f(t+h)-f(t)}{h} - f'(t)\,\right| < \varepsilon \enspace \bigg]$$ or, as I prefer, $$f \,\,\text{is differentiable on} \,\,I \text{,} \qquad \text{and}$$ $$\forall \, \varepsilon > 0 \quad \exists \, \delta > 0 \quad \forall \, x \in I \quad \forall \, y \in I \enspace \bigg[ \enspace 0 < |x-y| < \delta \implies \left|\,\dfrac{f(x)-f(y)}{x-y} - f'(x)\,\right| < \varepsilon \enspace \bigg]$$ The given definition of uniform differentiability requires differentiability! (And as it stands it is indeed required because we refer to $f'(t)$ inside the definition) Is there a way to ""fix"" this? In other words: Is there an alternative definition of uniform differentiability, that does not require differentiability, and follows the same set-theory, $\varepsilon$-$\delta$ theme of the definitions 1 to 5? The problems I encountered in my attempt: I tried to alter the given Definition 6, to include the quantifier $\exists \, v \in \mathbb{R}$, and then use $v$ in place of $f'(t)$, but I couldn't make this work, because there seems to be a unbreakable loop : I need the ""$\forall \, t$"" somewhere $v$ clearly depends on $t$, so the ""$\exists \, v$"" would have to be placed to the right of ""$\forall \, t$"" $\delta$ cannot depend on $t$, so the ""$\exists \, \delta$"" would have to be placed to the left of ""$\forall \, t$"" But $v$ cannod depend on $\delta$ (and therefore on $\varepsilon$), since $v$ is some sort of limit, which means the same $v$ must work for all epsilons! Then ""$\exists \, v$"" would have to be placed to the left of ""$\exists \, \delta$"", which is impossible given the above considerations! After thinking about this, it seemed to me that I would need two different epsilons and two different deltas inside the definition, but that looks too ugly, and too close to simply replacing the sentence ""$f \,\,\text{is differentiable on} \,\,I$"" by its set-theory equivalent, which is clearly not what I want here. Then I got stuck. 1 the three forms presented in Definition 6 were manipulated and ""converted to the language of set theory"" by me; but are grounded on many other posts on Math.SE such as here , here , here and here .","Let $I \subseteq \mathbb{R}$ be an interval and let $f: I \to \mathbb{R}$. Let's start looking at the definitions of continuity, uniform continuity and differentiability. Yes, I really like sentences in the language of set theory . Definition 1: $f$ is continuous at $t \in I$ if $$\forall \, \varepsilon > 0 \quad \exists \, \delta > 0 \quad \forall \, x \in (t-\delta, t+\delta) \cap I \quad |f(x)-f(t)| < \varepsilon$$ Definition 2: $f$ is continuous on $I$ if $$\forall \, t \in I \quad \forall \, \varepsilon > 0 \quad \exists \, \delta > 0 \quad \forall \, x \in (t-\delta, t+\delta) \cap I \quad |f(x)-f(t)| < \varepsilon$$ Definition 3: $f$ is uniformly continuous on $I$ if $$\forall \, \varepsilon > 0 \quad \exists \, \delta > 0 \quad \forall \, t \in I \quad \forall \, x \in (t-\delta, t+\delta) \cap I \quad |f(x)-f(t)| < \varepsilon$$ or, put more nicely, $$\forall \, \varepsilon > 0 \quad \exists \, \delta > 0 \quad \forall \, x \in I \quad \forall \, y \in I \quad \bigg[ \quad |x-y|<\delta \implies |f(x)-f(t)| < \varepsilon \quad \bigg]$$ Definition 4: $f$ is differentiable at $t \in I$ if $$\exists \, v \in \mathbb{R} \quad \forall \, \varepsilon > 0 \quad \exists \, \delta > 0 \quad \forall \, x \in (t-\delta, t+\delta) \cap I \setminus \!\{t\} \quad \left|\,\dfrac{f(x)-f(t)}{x-t} - v\,\right| < \varepsilon$$ Definition 5: $f$ is differentiable on $I$ if $$\forall \, t \in I \quad \exists \, v \in \mathbb{R} \quad \forall \, \varepsilon > 0 \quad \exists \, \delta > 0 \quad \forall \, x \in (t-\delta, t+\delta) \cap I \setminus \!\{t\} \quad \left|\,\dfrac{f(x)-f(t)}{x-t} - v\,\right| < \varepsilon$$ Now, note that in the definition of uniform continuity, we did not require the function to be continuous; instead, our definition implies it (quite trivially!). Now let's proceed to the most common definition 1 of uniform differentiability: Definition 6: $f$ is uniformly differentiable on $I$ if $$f \,\,\text{is differentiable on} \,\,I \text{,} \qquad \text{and}$$ $$\forall \, \varepsilon > 0 \quad \exists \, \delta > 0 \quad \forall \, t \in I \quad \forall \, x \in (t-\delta, t+\delta) \cap I \setminus \!\{t\} \quad \left|\,\dfrac{f(x)-f(t)}{x-t} - f'(t)\,\right| < \varepsilon$$ or, equivalently, $$f \,\,\text{is differentiable on} \,\,I \text{,} \qquad \text{and}$$ $\quad \forall \, \varepsilon > 0 \quad \exists \, \delta > 0$ $$\quad \forall \, t \in I \quad \forall \, h \quad \bigg[ \enspace \big(\,\, 0 < |h| < \delta \,\,\big) \land \big(\,\, t+h \in I \,\,\big) \implies \left|\,\dfrac{f(t+h)-f(t)}{h} - f'(t)\,\right| < \varepsilon \enspace \bigg]$$ or, as I prefer, $$f \,\,\text{is differentiable on} \,\,I \text{,} \qquad \text{and}$$ $$\forall \, \varepsilon > 0 \quad \exists \, \delta > 0 \quad \forall \, x \in I \quad \forall \, y \in I \enspace \bigg[ \enspace 0 < |x-y| < \delta \implies \left|\,\dfrac{f(x)-f(y)}{x-y} - f'(x)\,\right| < \varepsilon \enspace \bigg]$$ The given definition of uniform differentiability requires differentiability! (And as it stands it is indeed required because we refer to $f'(t)$ inside the definition) Is there a way to ""fix"" this? In other words: Is there an alternative definition of uniform differentiability, that does not require differentiability, and follows the same set-theory, $\varepsilon$-$\delta$ theme of the definitions 1 to 5? The problems I encountered in my attempt: I tried to alter the given Definition 6, to include the quantifier $\exists \, v \in \mathbb{R}$, and then use $v$ in place of $f'(t)$, but I couldn't make this work, because there seems to be a unbreakable loop : I need the ""$\forall \, t$"" somewhere $v$ clearly depends on $t$, so the ""$\exists \, v$"" would have to be placed to the right of ""$\forall \, t$"" $\delta$ cannot depend on $t$, so the ""$\exists \, \delta$"" would have to be placed to the left of ""$\forall \, t$"" But $v$ cannod depend on $\delta$ (and therefore on $\varepsilon$), since $v$ is some sort of limit, which means the same $v$ must work for all epsilons! Then ""$\exists \, v$"" would have to be placed to the left of ""$\exists \, \delta$"", which is impossible given the above considerations! After thinking about this, it seemed to me that I would need two different epsilons and two different deltas inside the definition, but that looks too ugly, and too close to simply replacing the sentence ""$f \,\,\text{is differentiable on} \,\,I$"" by its set-theory equivalent, which is clearly not what I want here. Then I got stuck. 1 the three forms presented in Definition 6 were manipulated and ""converted to the language of set theory"" by me; but are grounded on many other posts on Math.SE such as here , here , here and here .",,"['real-analysis', 'derivatives', 'logic', 'epsilon-delta', 'uniform-continuity']"
27,RESEARCH Q: Finding the n-th derivative of the Quotient Rule,RESEARCH Q: Finding the n-th derivative of the Quotient Rule,,"I am a sophomore at a community college so if my writing sounds a bit gibberish please ask for clarification. My goal is to find a sequence/series that can summarize the nth derivative of a $u/v $ function. The reason why I am posting this on stack exchange is to try to see if this technique has already been used by someone else and to analyze some of the responses in order to try different techniques/approaches for achieving my goal. Therefore I am only going to show a portion of my research in order to prevent any bias. Let $u$, $v$ and $k$ be functions of $x$, and let $i$, $a$, $e$ and $n$ represent real positive integers.  $u$ and $v$ can have derivatives of order $a$ or $e$, represented by  $u^{(a)}$, $u^{(e)}$,  $v^{(a)}$ and $v^{(e)}$. I created the function $\mu(a,e,n+i) =  (-1)^i(\frac{(n+i-1)!}{(n-1)!})\frac{v^{(e)}u^{(a)} - u^{(e)}v^{(a)}}{k^{n+i}} $; by taking the derivative with respect to $x$ of $\mu(a,e,n+i)$, if I could figure out how to write complex fractions, I could prove $ \frac{d \hspace{1pt} \mu(a,e,n+i)}{dx}  = \mu(a+1,e,n+i) + \mu(a,e+1,n+i) + k^{(1)}\mu(a,e,n+i+1)$ My question is: what is the best approach for solving this sequence/series after knowing this?","I am a sophomore at a community college so if my writing sounds a bit gibberish please ask for clarification. My goal is to find a sequence/series that can summarize the nth derivative of a $u/v $ function. The reason why I am posting this on stack exchange is to try to see if this technique has already been used by someone else and to analyze some of the responses in order to try different techniques/approaches for achieving my goal. Therefore I am only going to show a portion of my research in order to prevent any bias. Let $u$, $v$ and $k$ be functions of $x$, and let $i$, $a$, $e$ and $n$ represent real positive integers.  $u$ and $v$ can have derivatives of order $a$ or $e$, represented by  $u^{(a)}$, $u^{(e)}$,  $v^{(a)}$ and $v^{(e)}$. I created the function $\mu(a,e,n+i) =  (-1)^i(\frac{(n+i-1)!}{(n-1)!})\frac{v^{(e)}u^{(a)} - u^{(e)}v^{(a)}}{k^{n+i}} $; by taking the derivative with respect to $x$ of $\mu(a,e,n+i)$, if I could figure out how to write complex fractions, I could prove $ \frac{d \hspace{1pt} \mu(a,e,n+i)}{dx}  = \mu(a+1,e,n+i) + \mu(a,e+1,n+i) + k^{(1)}\mu(a,e,n+i+1)$ My question is: what is the best approach for solving this sequence/series after knowing this?",,"['calculus', 'sequences-and-series', 'derivatives']"
28,Finding the second derivative of$f(x)=x^2\sqrt{4-x}$,Finding the second derivative of,f(x)=x^2\sqrt{4-x},Find the second derivative of the function following: $$f(x)=x^2\sqrt{4-x}$$ Here I go... $$f(x)= x^2(4-x)^{1\over 2}$$ \begin{align*} f'(x) &= 2x(4-x)^{1\over 2}+{1\over 2}x^2(4-x)^{-{1\over 2}}(-1)\\  &= 2x(4-x)^{1\over 2} - {1 \over 2}x^2(4-x)^{-{1\over 2}}\\  &={1\over 2}x(4-x)^{-{1\over 2}}[4(4-x)-x]\\  &= {1\over 2}x(4-x)^{-{1\over 2}}(16-5x)\\ f''(x)&= {-{1\over 4}}x(4-x)^{-{3\over 2}}(-1)({1\over 2})(16-5x)+(-5)[{1\over 2}x(4-x)^{-{1\over 2}}]\\ &= {-{1\over 8}}x(4-x)^{-{3\over 2}}(16-5x)-{5\over 2}(4-x)^{-{1\over 2}}\\ &= {-{1\over 8}}x(4-x)^{-{3\over 2}}[(16-5x)-20(4-x)]\\ &={-{1\over 8}}x(4-x)^{-{3\over 2}}(-64+15x^2) \end{align*} I think I messed up on the second derivative. Could anyone show me the steps for the second derivative? Thanks!,Find the second derivative of the function following: $$f(x)=x^2\sqrt{4-x}$$ Here I go... $$f(x)= x^2(4-x)^{1\over 2}$$ \begin{align*} f'(x) &= 2x(4-x)^{1\over 2}+{1\over 2}x^2(4-x)^{-{1\over 2}}(-1)\\  &= 2x(4-x)^{1\over 2} - {1 \over 2}x^2(4-x)^{-{1\over 2}}\\  &={1\over 2}x(4-x)^{-{1\over 2}}[4(4-x)-x]\\  &= {1\over 2}x(4-x)^{-{1\over 2}}(16-5x)\\ f''(x)&= {-{1\over 4}}x(4-x)^{-{3\over 2}}(-1)({1\over 2})(16-5x)+(-5)[{1\over 2}x(4-x)^{-{1\over 2}}]\\ &= {-{1\over 8}}x(4-x)^{-{3\over 2}}(16-5x)-{5\over 2}(4-x)^{-{1\over 2}}\\ &= {-{1\over 8}}x(4-x)^{-{3\over 2}}[(16-5x)-20(4-x)]\\ &={-{1\over 8}}x(4-x)^{-{3\over 2}}(-64+15x^2) \end{align*} I think I messed up on the second derivative. Could anyone show me the steps for the second derivative? Thanks!,,"['calculus', 'derivatives', 'proof-verification']"
29,Does every differentiable function have integrable derivative?,Does every differentiable function have integrable derivative?,,"While it would seem to be true, I have found that there are examples of functions that are differentiable but not Riemann Integrable. Would a step function, for example, be integrable but not Riemann Integrable? So would all differentiable functions still be integrable in the general sense?","While it would seem to be true, I have found that there are examples of functions that are differentiable but not Riemann Integrable. Would a step function, for example, be integrable but not Riemann Integrable? So would all differentiable functions still be integrable in the general sense?",,"['integration', 'derivatives']"
30,Differentiate $\sqrt{1+e^x}$ using the definition of a derivative,Differentiate  using the definition of a derivative,\sqrt{1+e^x},"This is the progress I've made so far. $$\lim_{h \to 0} \frac{\sqrt{1+e^{x+h}}-\sqrt{1+e^{x}}}{h}$$ $$= \lim_{h \to 0} \frac{\left(1+e^{x+h}\right)-\left(1+e^{x}\right)}{h\left(\sqrt{1+e^{x+h}}+\sqrt{1+e^{x}}\right)}$$ $$= \lim_{h \to 0} \frac{e^x\left(e^h-1\right)}{h\left(\sqrt{1+e^{x+h}}+\sqrt{1+e^{x}}\right)}$$ $$= \lim_{h \to 0} \frac{e^x\left(e^h-1\right)}{h\left(\sqrt{1+e^{x+h}}+\sqrt{1+e^{x}}\right)}$$ $$= \lim_{h \to 0} \frac{e^x}{\sqrt{1+e^{x+h}}+\sqrt{1+e^{x}}} \frac{e^h-1}{h}$$ I can see how as h tends to 0 the fraction on the left will tend to the desired result, but I'm not sure how to deal with the fraction on the right.","This is the progress I've made so far. $$\lim_{h \to 0} \frac{\sqrt{1+e^{x+h}}-\sqrt{1+e^{x}}}{h}$$ $$= \lim_{h \to 0} \frac{\left(1+e^{x+h}\right)-\left(1+e^{x}\right)}{h\left(\sqrt{1+e^{x+h}}+\sqrt{1+e^{x}}\right)}$$ $$= \lim_{h \to 0} \frac{e^x\left(e^h-1\right)}{h\left(\sqrt{1+e^{x+h}}+\sqrt{1+e^{x}}\right)}$$ $$= \lim_{h \to 0} \frac{e^x\left(e^h-1\right)}{h\left(\sqrt{1+e^{x+h}}+\sqrt{1+e^{x}}\right)}$$ $$= \lim_{h \to 0} \frac{e^x}{\sqrt{1+e^{x+h}}+\sqrt{1+e^{x}}} \frac{e^h-1}{h}$$ I can see how as h tends to 0 the fraction on the left will tend to the desired result, but I'm not sure how to deal with the fraction on the right.",,['derivatives']
31,Prove that if $x = \sqrt{a^{\sin^{-1} t}}$ and $y = \sqrt{a^{\cos^{-1}t}}$ then $\frac{dy}{dx}$ = $-\frac{y}x$,Prove that if  and  then  =,x = \sqrt{a^{\sin^{-1} t}} y = \sqrt{a^{\cos^{-1}t}} \frac{dy}{dx} -\frac{y}x,"Prove: If $x = \sqrt{a^{\sin^{-1} t}}$ and $y = \sqrt{a^{\cos^{-1}t}}$ where $\sin^{-1}$ and $\cos^{-1}$ are inverse trig function, show that $\frac{dy}{dx}$ = $-\frac{y}x$ Unfortunately I don't see how this can be done. By differentiating x and y separately via parametric differentiation, I get the following two results: $\frac{dx}{dt} = \frac{\log a}{2\sqrt{1 - t^2}}$ and $\frac{dy}{dt} = -\frac{\log a}{2\sqrt{1-t^2}}$ Unfortunately I don't see how I can use that to arrive at $\frac{dy}{dx} = -\frac{y}x$","Prove: If and where and are inverse trig function, show that = Unfortunately I don't see how this can be done. By differentiating x and y separately via parametric differentiation, I get the following two results: and Unfortunately I don't see how I can use that to arrive at",x = \sqrt{a^{\sin^{-1} t}} y = \sqrt{a^{\cos^{-1}t}} \sin^{-1} \cos^{-1} \frac{dy}{dx} -\frac{y}x \frac{dx}{dt} = \frac{\log a}{2\sqrt{1 - t^2}} \frac{dy}{dt} = -\frac{\log a}{2\sqrt{1-t^2}} \frac{dy}{dx} = -\frac{y}x,"['derivatives', 'parametric']"
32,What is some smooth parameterization of $y - |x| = 0$?,What is some smooth parameterization of ?,y - |x| = 0,"Apparently, $\{(x,y) \in \Bbb R^2, y - |x| = 0\}$ is an example of a level curve $C: f(x,y) = c$, such that $f$ is not smooth, but $C$ admits a smooth parameterization. What is some smooth parameterization of the the level curve $C: y - |x| = 0$? I am not sure how such a construction is possible. We would like to have $\gamma: (\alpha, \beta) \subset \Bbb R \to \Bbb R^2$, such $\gamma(t) = (\gamma_1(t), \gamma_2(t))$ satisfies $\gamma_2(t) = |\gamma_1(t)|$, for all $t \in (\alpha, \beta)$. If $\gamma_1(t) \neq 0$ for all $t$, then $\gamma$ doesn't cover $C$. On the other hand, if $\gamma_1(t) = 0$ for some $t$, then $\gamma_2$ cannot be differentiable at this point. Am I missing something here? Edit: Where a function $f$ is said to be ""smooth"" if it is $C^{\infty}$","Apparently, $\{(x,y) \in \Bbb R^2, y - |x| = 0\}$ is an example of a level curve $C: f(x,y) = c$, such that $f$ is not smooth, but $C$ admits a smooth parameterization. What is some smooth parameterization of the the level curve $C: y - |x| = 0$? I am not sure how such a construction is possible. We would like to have $\gamma: (\alpha, \beta) \subset \Bbb R \to \Bbb R^2$, such $\gamma(t) = (\gamma_1(t), \gamma_2(t))$ satisfies $\gamma_2(t) = |\gamma_1(t)|$, for all $t \in (\alpha, \beta)$. If $\gamma_1(t) \neq 0$ for all $t$, then $\gamma$ doesn't cover $C$. On the other hand, if $\gamma_1(t) = 0$ for some $t$, then $\gamma_2$ cannot be differentiable at this point. Am I missing something here? Edit: Where a function $f$ is said to be ""smooth"" if it is $C^{\infty}$",,['derivatives']
33,What is the derivative of $\mathrm{trace}((S^T S)^{-2})$ w.r.t. $S$,What is the derivative of  w.r.t.,\mathrm{trace}((S^T S)^{-2}) S,I would like to compute the derivative of $\mathrm{trace}((S^T S)^{-2})$ w.r.t. $S$. I know that $\frac{\partial \mathrm{trace}((S^T S)^{-1})}{\partial S} = -2S(S^T S)^{-2}$ but I have a higher order in my expression. Any help would be really appreciated.,I would like to compute the derivative of $\mathrm{trace}((S^T S)^{-2})$ w.r.t. $S$. I know that $\frac{\partial \mathrm{trace}((S^T S)^{-1})}{\partial S} = -2S(S^T S)^{-2}$ but I have a higher order in my expression. Any help would be really appreciated.,,"['linear-algebra', 'derivatives', 'inverse', 'trace']"
34,"Limit of derivative does not exist, while limit of difference quotient is infinite","Limit of derivative does not exist, while limit of difference quotient is infinite",,"Can anyone show an example of a function $f$ of a real variabile such that $f$ is differentiable on a neighborhood of a point $x_0 \in \mathbb{R}$, except at $x_0$ itself; $f$ is continuous at $x_0$; $\displaystyle \lim_{x \to x_0} f'(x)$ does not exist; $\displaystyle \lim_{x \to x_0} \frac{f(x) - f(x_0)}{x - x_0} = +\infty$ or $\displaystyle \lim_{x \to x_0} \frac{f(x) - f(x_0)}{x - x_0} = -\infty$ ?","Can anyone show an example of a function $f$ of a real variabile such that $f$ is differentiable on a neighborhood of a point $x_0 \in \mathbb{R}$, except at $x_0$ itself; $f$ is continuous at $x_0$; $\displaystyle \lim_{x \to x_0} f'(x)$ does not exist; $\displaystyle \lim_{x \to x_0} \frac{f(x) - f(x_0)}{x - x_0} = +\infty$ or $\displaystyle \lim_{x \to x_0} \frac{f(x) - f(x_0)}{x - x_0} = -\infty$ ?",,"['calculus', 'derivatives']"
35,How to guarantee differentiability?,How to guarantee differentiability?,,"According to my calculus book the following holds. Given that $h\to0$: $f'(x)=\frac{1}{2}(f'(x)+f'(x))=\frac{1}{2}(\frac{f(x+h)-f(x)}{h}+\frac{f(x)-f(x-h)}{h})=\frac{f(x+h)-f(x-h)}{2h}$ But then the book claims that the existence of the limit in $\frac{f(x+h)-f(x-h)}{2h}$ does not guarantee that $f$ is differentiable at $x$. This can be shown by taking $f(x)=|x|$ at $x=0$. Then the limit gives $0$, while $f'(0)$ should be not defined at $x=0$. Question: Although I can follow the example with $f(x)=|x|$ above, I do not understand why the existence of the limit in $\frac{f(x+h)-f(x-h)}{2h}$ would not guarantee that $f$ is differentiable at $x$. It seems to say that $f'(x)=\frac{f(x+h)-f(x-h)}{2h}$, then why doesn't it also guarantee that $f$ is differentiable at $x$? Any thoughts?","According to my calculus book the following holds. Given that $h\to0$: $f'(x)=\frac{1}{2}(f'(x)+f'(x))=\frac{1}{2}(\frac{f(x+h)-f(x)}{h}+\frac{f(x)-f(x-h)}{h})=\frac{f(x+h)-f(x-h)}{2h}$ But then the book claims that the existence of the limit in $\frac{f(x+h)-f(x-h)}{2h}$ does not guarantee that $f$ is differentiable at $x$. This can be shown by taking $f(x)=|x|$ at $x=0$. Then the limit gives $0$, while $f'(0)$ should be not defined at $x=0$. Question: Although I can follow the example with $f(x)=|x|$ above, I do not understand why the existence of the limit in $\frac{f(x+h)-f(x-h)}{2h}$ would not guarantee that $f$ is differentiable at $x$. It seems to say that $f'(x)=\frac{f(x+h)-f(x-h)}{2h}$, then why doesn't it also guarantee that $f$ is differentiable at $x$? Any thoughts?",,"['calculus', 'derivatives']"
36,Differentiability of $f(x+y) = f(x)f(y)$ [duplicate],Differentiability of  [duplicate],f(x+y) = f(x)f(y),"This question already has answers here : Prove that $f'$ exists for all $x$ in $R$ if $f(x+y)=f(x)f(y)$ and $f'(0)$ exists (2 answers) Closed 8 years ago . Let $f$: $\mathbb R$ $\to$ $\mathbb R$ be a function such that $f(x+y)$ = $f(x)f(y)$ for all $x,y$ $\in$ $\mathbb R$. Suppose that $f'(0)$ exists. Prove that $f$ is a differentiable function. This is what I've tried: Using the definition of differentiability and taking arbitrary $x_0$ $\in$ $\mathbb R$. $\lim_{h\to 0}$ ${f(x_0 + h)-f(x_0)\over h}$ $=$ $\cdots$ $=$  $f(x_0)$$\lim_{h\to 0}$ ${f(h) - 1\over h}$. Then since $x_0$ arbitrary, using $f(x_0+0) = f(x_0) = f(x_0)f(0)$ for $y = 0$, can I finish the proof?","This question already has answers here : Prove that $f'$ exists for all $x$ in $R$ if $f(x+y)=f(x)f(y)$ and $f'(0)$ exists (2 answers) Closed 8 years ago . Let $f$: $\mathbb R$ $\to$ $\mathbb R$ be a function such that $f(x+y)$ = $f(x)f(y)$ for all $x,y$ $\in$ $\mathbb R$. Suppose that $f'(0)$ exists. Prove that $f$ is a differentiable function. This is what I've tried: Using the definition of differentiability and taking arbitrary $x_0$ $\in$ $\mathbb R$. $\lim_{h\to 0}$ ${f(x_0 + h)-f(x_0)\over h}$ $=$ $\cdots$ $=$  $f(x_0)$$\lim_{h\to 0}$ ${f(h) - 1\over h}$. Then since $x_0$ arbitrary, using $f(x_0+0) = f(x_0) = f(x_0)f(0)$ for $y = 0$, can I finish the proof?",,"['real-analysis', 'derivatives', 'functional-equations']"
37,What technique should I apply to find the derivative of a ceiling or floor function e.g d/dx(x*⌈x⌉) and d/dx(x*⌊x⌋)?,What technique should I apply to find the derivative of a ceiling or floor function e.g d/dx(x*⌈x⌉) and d/dx(x*⌊x⌋)?,,"May I know the technique to apply to find the derivative, whenever I see a ceiling function of floor function. Thank You! e.g   $$ \frac{d}{dx}(x*\lceil x \rceil )$$  and $$ \frac{d}{dx}(x*\lfloor x \rfloor )$$ Is there a solution to $ \frac{d}{dx}(x*\lceil x \rceil )$ ?","May I know the technique to apply to find the derivative, whenever I see a ceiling function of floor function. Thank You! e.g   $$ \frac{d}{dx}(x*\lceil x \rceil )$$  and $$ \frac{d}{dx}(x*\lfloor x \rfloor )$$ Is there a solution to $ \frac{d}{dx}(x*\lceil x \rceil )$ ?",,"['derivatives', 'ceiling-and-floor-functions']"
38,Using chain rule to find the derivative of $(4x^2-2)^3$,Using chain rule to find the derivative of,(4x^2-2)^3,"I just took a quiz and one of the problems was to get the derivative of $f(x) = (4x^2-2)^3$. I used the chain rule and got $f'(x) = 24x(4x^2-2)^2$. However, plugging it into the derivative function in a TI-89 returns  $f(x) = 96x(2x^2-1)^2$. Which is right? Me or the calculator?","I just took a quiz and one of the problems was to get the derivative of $f(x) = (4x^2-2)^3$. I used the chain rule and got $f'(x) = 24x(4x^2-2)^2$. However, plugging it into the derivative function in a TI-89 returns  $f(x) = 96x(2x^2-1)^2$. Which is right? Me or the calculator?",,"['calculus', 'derivatives', 'chain-rule']"
39,Prove $x\frac{\partial f}{\partial x}+y\frac{\partial f}{\partial y}=0 $ $\rightarrow$ $f\equiv c$,Prove,x\frac{\partial f}{\partial x}+y\frac{\partial f}{\partial y}=0  \rightarrow f\equiv c,"for $f:\Bbb{R^2}\to\Bbb{R}, f\in C^1  $ and $x\frac{\partial f}{\partial x}+y\frac{\partial f}{\partial y}=0 $  for every $(x,y)\in \Bbb{R^2}$ then $f\equiv c\in\Bbb{R}$ I was thinking it's similar to $g:\Bbb{R} \to \Bbb{R}$ if $g'=0$ then $g\equiv c$ but I don't sure how to prove it.  thanks","for $f:\Bbb{R^2}\to\Bbb{R}, f\in C^1  $ and $x\frac{\partial f}{\partial x}+y\frac{\partial f}{\partial y}=0 $  for every $(x,y)\in \Bbb{R^2}$ then $f\equiv c\in\Bbb{R}$ I was thinking it's similar to $g:\Bbb{R} \to \Bbb{R}$ if $g'=0$ then $g\equiv c$ but I don't sure how to prove it.  thanks",,"['calculus', 'derivatives', 'differential-topology']"
40,Is there a concept already for $\frac{f'(x)}{f(x)/x}$?,Is there a concept already for ?,\frac{f'(x)}{f(x)/x},"Given a differentiable real function $y=f(x)$, is there a math concept/terminology already defined for $$\frac{f'(x)}{f(x)/x}?$$ This quantity is inspired from price elasticity of demand . Thanks.","Given a differentiable real function $y=f(x)$, is there a math concept/terminology already defined for $$\frac{f'(x)}{f(x)/x}?$$ This quantity is inspired from price elasticity of demand . Thanks.",,"['real-analysis', 'derivatives', 'terminology', 'economics']"
41,"Derivative to Zero, What does it intuitively mean?","Derivative to Zero, What does it intuitively mean?",,"I'm currently learning machine learning, and I came across this equation called Least Squares Regression. X and w are both matrices. The multiplication of both matrices becomes y hat, which is theoretically supposed to be equal to y. We want to minimize the squared error given by this equation by changing w. w can be solved by a derivation of the function to w, and setting the equation to zero. The question is, what does it intuitively mean? I know that in derivative, we are trying to find the rate of change. BUT what does it mean the rate of change = 0 intuitively?","I'm currently learning machine learning, and I came across this equation called Least Squares Regression. X and w are both matrices. The multiplication of both matrices becomes y hat, which is theoretically supposed to be equal to y. We want to minimize the squared error given by this equation by changing w. w can be solved by a derivation of the function to w, and setting the equation to zero. The question is, what does it intuitively mean? I know that in derivative, we are trying to find the rate of change. BUT what does it mean the rate of change = 0 intuitively?",,['derivatives']
42,What is the proper use of Leibniz notation for one-sided derivatives?,What is the proper use of Leibniz notation for one-sided derivatives?,,The only notation I've seen has been restricted to either Lagrange's prime notation or Euler's $D$. Here are some of the variants: $$f'(a^+):=\lim_{x\to a^+}\frac{f(x)-f(a)}{x-a}$$ $$D_+f(x):=\lim_{h\to0^+}\frac{f(x+h)-f(x)}{h}$$ Is there a standard notation for the right- and left-handed derivatives using $\dfrac{df}{dx}$?,The only notation I've seen has been restricted to either Lagrange's prime notation or Euler's $D$. Here are some of the variants: $$f'(a^+):=\lim_{x\to a^+}\frac{f(x)-f(a)}{x-a}$$ $$D_+f(x):=\lim_{h\to0^+}\frac{f(x+h)-f(x)}{h}$$ Is there a standard notation for the right- and left-handed derivatives using $\dfrac{df}{dx}$?,,"['derivatives', 'notation']"
43,Prove $g(x+h) = g(x) + hg'(x) + \frac{1}{2} h^2 g''(x) + o(h^2)$ from definition of limit,Prove  from definition of limit,g(x+h) = g(x) + hg'(x) + \frac{1}{2} h^2 g''(x) + o(h^2),"I want to prove $g(x+h) = g(x) + hg'(x) + \frac{1}{2} h^2 g''(x) + o(h^2)$ from the definition of limit, where x is a 1D variable, $o(h)$ is a quantity that depends on scalar h negligible compared to h as h goes to zero i.e. a small quantity Attempt: Recall definition of limit $$\lim_{h\to 0} |\frac{g(x+h) - g(x) - hg'(x)}{h}| = |\frac{o(h)}{h}|$$ which turns into a more familiar form $$\lim_{h\to 0}  |\frac{g(x+h) - g(x)}{h}| = g'(x)$$ Conclusion  $$g(x+h) - g(x) = hg'(x) + o(h)$$ Taking a derivative wrt $x$ in above expression ields $$g'(x+h) - g'(x) = hg''(x)$$ Sub in definition of derivative for both LHS terms $$g(x+h^2) - g(x+h) - g(x+h) + g(x) = h^2 g''(x) + ho(h)$$ Therefore $$g(x+h^2) = g(x) + 2hg'(x) + h^2 g''(x) + o(h^2)$$ dividing a 2 across $$\frac{g(x+h^2)}{2}  = \frac{g(x)}{2} + hg'(x) + \frac{1}{2} h^2g''(x) + o(h^2)$$ which is not quite... $$g(x+h) = g(x) + hg'(x) + \frac{1}{2} h^2 g''(x) + o(h^2)$$ the desired result Can someone show me how to prove this?","I want to prove $g(x+h) = g(x) + hg'(x) + \frac{1}{2} h^2 g''(x) + o(h^2)$ from the definition of limit, where x is a 1D variable, $o(h)$ is a quantity that depends on scalar h negligible compared to h as h goes to zero i.e. a small quantity Attempt: Recall definition of limit $$\lim_{h\to 0} |\frac{g(x+h) - g(x) - hg'(x)}{h}| = |\frac{o(h)}{h}|$$ which turns into a more familiar form $$\lim_{h\to 0}  |\frac{g(x+h) - g(x)}{h}| = g'(x)$$ Conclusion  $$g(x+h) - g(x) = hg'(x) + o(h)$$ Taking a derivative wrt $x$ in above expression ields $$g'(x+h) - g'(x) = hg''(x)$$ Sub in definition of derivative for both LHS terms $$g(x+h^2) - g(x+h) - g(x+h) + g(x) = h^2 g''(x) + ho(h)$$ Therefore $$g(x+h^2) = g(x) + 2hg'(x) + h^2 g''(x) + o(h^2)$$ dividing a 2 across $$\frac{g(x+h^2)}{2}  = \frac{g(x)}{2} + hg'(x) + \frac{1}{2} h^2g''(x) + o(h^2)$$ which is not quite... $$g(x+h) = g(x) + hg'(x) + \frac{1}{2} h^2 g''(x) + o(h^2)$$ the desired result Can someone show me how to prove this?",,"['calculus', 'derivatives']"
44,Can someone show the equivalence of the following relationship $\dfrac{\partial{v}^Tv}{\partial v}=2v^T$,Can someone show the equivalence of the following relationship,\dfrac{\partial{v}^Tv}{\partial v}=2v^T,"Let $v$ be an n dimensional vector Then how can one show that $$\dfrac{\partial{v}^Tv}{\partial v}=2v^T$$ Specifically, I am not quite understanding the difference between taking the derivative against a quantity $v$ versus its transposed quantity $v^T$?","Let $v$ be an n dimensional vector Then how can one show that $$\dfrac{\partial{v}^Tv}{\partial v}=2v^T$$ Specifically, I am not quite understanding the difference between taking the derivative against a quantity $v$ versus its transposed quantity $v^T$?",,"['calculus', 'derivatives', 'vectors', 'partial-derivative']"
45,Calculating arc length $y=x^2$,Calculating arc length,y=x^2,"I picked this example for practice and got stuck with it. Someone moderate me if I am in the right path. I need to calculate the length of arc s, on the section of the curve $y=x^2$ with $0≤x≤1$ My Workings: The formula is $s=\int^b_a\sqrt{1+[f'(x)]^2}dx$ I work out everything under the $\sqrt{}$ my $f(x) =x^2$. I need to get $f'(x)$, which =$2x$ So our function becomes $s=\int^1_0\sqrt{1+[2x]^2}dx$ Then I work out $1+[f'(x)]^2 = 1+4x^2 $ $s=\int^1_0\sqrt{1+[f'(x)]^2}dx =\int^1_0\sqrt{(1+4x^2)}dx$ $s=\int^1_0\sqrt{(1+4x^2)}dx =\int^1_0(1+4x^2)^\frac{1}{2}dx$ I reckon I have to integrate $\int^1_0(1+4x^2)^\frac{1}{2}dx$ The integration part: I used u-substitution: let $ u = 1+4x^2$ then $du = 8x dx$ hence $dx = \frac{du}{8x}$ But, now there is trouble with the $8x$ because if I plug in $dx$, I get: $\int^1_0(u)^\frac{1}{2}\frac{du}{8x} = \int^1_0\dfrac{(u)^\frac{1}{2}}{8x}du$ $= \dfrac{1}{8}\int^1_0\dfrac{1}{x}.{(u)^\frac{1}{2}}du$ Doesn't this give me $(\dfrac{1}{8}.ln|x|.\dfrac{2}{3}.u^\frac{3}{2})]^1_0$ ...but... my answer doesn't look right......shouldnt' $x >0$? How do I end this?","I picked this example for practice and got stuck with it. Someone moderate me if I am in the right path. I need to calculate the length of arc s, on the section of the curve $y=x^2$ with $0≤x≤1$ My Workings: The formula is $s=\int^b_a\sqrt{1+[f'(x)]^2}dx$ I work out everything under the $\sqrt{}$ my $f(x) =x^2$. I need to get $f'(x)$, which =$2x$ So our function becomes $s=\int^1_0\sqrt{1+[2x]^2}dx$ Then I work out $1+[f'(x)]^2 = 1+4x^2 $ $s=\int^1_0\sqrt{1+[f'(x)]^2}dx =\int^1_0\sqrt{(1+4x^2)}dx$ $s=\int^1_0\sqrt{(1+4x^2)}dx =\int^1_0(1+4x^2)^\frac{1}{2}dx$ I reckon I have to integrate $\int^1_0(1+4x^2)^\frac{1}{2}dx$ The integration part: I used u-substitution: let $ u = 1+4x^2$ then $du = 8x dx$ hence $dx = \frac{du}{8x}$ But, now there is trouble with the $8x$ because if I plug in $dx$, I get: $\int^1_0(u)^\frac{1}{2}\frac{du}{8x} = \int^1_0\dfrac{(u)^\frac{1}{2}}{8x}du$ $= \dfrac{1}{8}\int^1_0\dfrac{1}{x}.{(u)^\frac{1}{2}}du$ Doesn't this give me $(\dfrac{1}{8}.ln|x|.\dfrac{2}{3}.u^\frac{3}{2})]^1_0$ ...but... my answer doesn't look right......shouldnt' $x >0$? How do I end this?",,"['integration', 'derivatives', 'definite-integrals', 'indefinite-integrals']"
46,Find derivative of $x^{x^x}$,Find derivative of,x^{x^x},Trying to find the derivative of: $$ x^{x^x} $$ I have a solution but cannot understand the third transition:,Trying to find the derivative of: I have a solution but cannot understand the third transition:,"
x^{x^x}
","['calculus', 'derivatives', 'exponentiation']"
47,Find the derivative of $\arccos\frac{b+a\cos x}{a+b\cos x}$,Find the derivative of,\arccos\frac{b+a\cos x}{a+b\cos x},"Find the derivative of $\arccos\dfrac{b+a\cos x}{a+b\cos x}$ is there a smart way to find this derivative i tried by the conventional chain rule way, and it got very complicated","Find the derivative of $\arccos\dfrac{b+a\cos x}{a+b\cos x}$ is there a smart way to find this derivative i tried by the conventional chain rule way, and it got very complicated",,"['calculus', 'derivatives']"
48,Numerical differentiation: 2-point vs 5-point method,Numerical differentiation: 2-point vs 5-point method,,"I want to compare the following two numerical differentiation schemes: 2-point numerical differentiation: \begin{equation} \dot{\omega}_t = \frac{1}{dt} \left [ \omega_{t} - \omega_{t-dt} \right ] + \mathcal{O}(h) \end{equation} 5-point numerical differentiation: \begin{equation} \dot{\omega}_t = -\frac{1}{12dt} \left [ -25\omega_{t} + 48\omega_{t-dt} -36\omega_{t-2dt} + 16\omega_{t-3dt} - 3\omega_{t-4dt} \right ] + \mathcal{O}(h^4) \end{equation} The data that I use can be downloaded here , where $p$ is the signal without noise, $pnoisy$ is signal $p$ plus independent Gaussian noise, $pdot$ is the exact derivative of $p$ which needs to be estimated by using $pnoisy$. At last, $time$ is simply the time vector. I use the following Matlab script: clear all; clc; close all  load tempdat  %   Two-point method time_step = time(2) - time(1);  pdot_twopoint = (1/time_step)*diff(pnoisy);  %   Five-point method j = 1; for i = 5:length(time)     pdot_fivepoint(j,1) = -(1/(12*time_step))*(-25*pnoisy(i) + 48*pnoisy(i-1) - 36*pnoisy(i-2) + 16*pnoisy(i-3) - 3*pnoisy(i-4));     j = j+1; end  plot(time, pdot) hold all plot(time(2:end), pdot_twopoint) plot(time(5:end), pdot_fivepoint) hold off legend('real','2-point','5-point') xlabel('time (s)') ylabel('pdot')  sum(abs(pdot(2:end) - pdot_twopoint).^2) sum(abs(pdot(5:end) - pdot_fivepoint).^2) The RMSE (w.r.t. to $pdot$ and its estimate) of the 2-point method is 3.2038, while the RMSE of the 5-point method is 47.4570. I have also added a graph below where it can be seen that the 2-point method results in a more accurate estimate of $pdot$ compared to the 5-point method. I wonder why the 5-point method performs better than the 2-point method, because the accuracy of the 5-point method is supposed to be $\mathcal{O}(h^4)$ while that of the 2-point method is $\mathcal{O}(h)$.","I want to compare the following two numerical differentiation schemes: 2-point numerical differentiation: \begin{equation} \dot{\omega}_t = \frac{1}{dt} \left [ \omega_{t} - \omega_{t-dt} \right ] + \mathcal{O}(h) \end{equation} 5-point numerical differentiation: \begin{equation} \dot{\omega}_t = -\frac{1}{12dt} \left [ -25\omega_{t} + 48\omega_{t-dt} -36\omega_{t-2dt} + 16\omega_{t-3dt} - 3\omega_{t-4dt} \right ] + \mathcal{O}(h^4) \end{equation} The data that I use can be downloaded here , where $p$ is the signal without noise, $pnoisy$ is signal $p$ plus independent Gaussian noise, $pdot$ is the exact derivative of $p$ which needs to be estimated by using $pnoisy$. At last, $time$ is simply the time vector. I use the following Matlab script: clear all; clc; close all  load tempdat  %   Two-point method time_step = time(2) - time(1);  pdot_twopoint = (1/time_step)*diff(pnoisy);  %   Five-point method j = 1; for i = 5:length(time)     pdot_fivepoint(j,1) = -(1/(12*time_step))*(-25*pnoisy(i) + 48*pnoisy(i-1) - 36*pnoisy(i-2) + 16*pnoisy(i-3) - 3*pnoisy(i-4));     j = j+1; end  plot(time, pdot) hold all plot(time(2:end), pdot_twopoint) plot(time(5:end), pdot_fivepoint) hold off legend('real','2-point','5-point') xlabel('time (s)') ylabel('pdot')  sum(abs(pdot(2:end) - pdot_twopoint).^2) sum(abs(pdot(5:end) - pdot_fivepoint).^2) The RMSE (w.r.t. to $pdot$ and its estimate) of the 2-point method is 3.2038, while the RMSE of the 5-point method is 47.4570. I have also added a graph below where it can be seen that the 2-point method results in a more accurate estimate of $pdot$ compared to the 5-point method. I wonder why the 5-point method performs better than the 2-point method, because the accuracy of the 5-point method is supposed to be $\mathcal{O}(h^4)$ while that of the 2-point method is $\mathcal{O}(h)$.",,"['derivatives', 'numerical-methods']"
49,Laplace tranform of $t^{5/2}$,Laplace tranform of,t^{5/2},It is asked to transform $t^{5/2}$. I did $t^{5/2}=t^3\cdot t^{-1/2}$. Then followed the table result $$L\{{t^nf(t)}\}=(-1)^n\cdot\frac{d^n}{ds^n}F(s)$$ However i got $\frac{1}{2} \cdot\sqrt\pi \cdot s^{-7/2}$ instead of $\frac{15}{8} \cdot\sqrt\pi \cdot s^{-7/2}$. Can you help me with the derivations? Thanks,It is asked to transform $t^{5/2}$. I did $t^{5/2}=t^3\cdot t^{-1/2}$. Then followed the table result $$L\{{t^nf(t)}\}=(-1)^n\cdot\frac{d^n}{ds^n}F(s)$$ However i got $\frac{1}{2} \cdot\sqrt\pi \cdot s^{-7/2}$ instead of $\frac{15}{8} \cdot\sqrt\pi \cdot s^{-7/2}$. Can you help me with the derivations? Thanks,,"['derivatives', 'laplace-transform']"
50,"$f(x) = x^3-x^2+3x-1$ and $f(1) = 2$, find $g'(2)$ if $g(x) = f^{-1} (x)$","and , find  if",f(x) = x^3-x^2+3x-1 f(1) = 2 g'(2) g(x) = f^{-1} (x),"$f(x) = x^3-x^2+3x-1$ and $f(1) = 2$, find $g'(2)$ if $g(x) = f^{-1} (x)$.  I'm stuck. Can I get hints only? No anwser please.","$f(x) = x^3-x^2+3x-1$ and $f(1) = 2$, find $g'(2)$ if $g(x) = f^{-1} (x)$.  I'm stuck. Can I get hints only? No anwser please.",,"['calculus', 'derivatives']"
51,"If $f(0) = f(1)=0$ and $|f'' | \leq 1$ on $[0,1]$, then $|f'(1/2)|\le 1/4$","If  and  on , then","f(0) = f(1)=0 |f'' | \leq 1 [0,1] |f'(1/2)|\le 1/4","Let $f :  [0,1] \rightarrow \mathbb{R}$ be a function whose second order derivative $f''(x)$ is continuous on $[0,1]$. Suppose that $f(0) = f(1)=0$ and that $|f''(x)| \leq 1$ for any $x  \in [0,1]$. Prove that $|f'(\frac{1}{2})| \leq \frac{1}{4}$. I think we need to use the mean value theorem, and I have proven that $|f(x)| \leq \frac{1}{8}$ for any $x \in [0,1]$. I'm not sure how to proceed, though. Could someone please help? Thanks!","Let $f :  [0,1] \rightarrow \mathbb{R}$ be a function whose second order derivative $f''(x)$ is continuous on $[0,1]$. Suppose that $f(0) = f(1)=0$ and that $|f''(x)| \leq 1$ for any $x  \in [0,1]$. Prove that $|f'(\frac{1}{2})| \leq \frac{1}{4}$. I think we need to use the mean value theorem, and I have proven that $|f(x)| \leq \frac{1}{8}$ for any $x \in [0,1]$. I'm not sure how to proceed, though. Could someone please help? Thanks!",,"['real-analysis', 'inequality', 'derivatives']"
52,How to calculate $\frac{d}{d y} y'$,How to calculate,\frac{d}{d y} y',Is there a way to evaluate the following expression? $$ \frac{d}{dy} y' $$ where $$ y' = \frac{dy}{dx}.$$,Is there a way to evaluate the following expression? $$ \frac{d}{dy} y' $$ where $$ y' = \frac{dy}{dx}.$$,,"['derivatives', 'implicit-differentiation']"
53,Differentiation of $xx^T$ where $x$ is a vector,Differentiation of  where  is a vector,xx^T x,"How is differentiation of $xx^T$ with respect to $x$ as $2x^T$, where $x$ is a vector? $x^T $means transpose of $x$ vector.","How is differentiation of $xx^T$ with respect to $x$ as $2x^T$, where $x$ is a vector? $x^T $means transpose of $x$ vector.",,"['derivatives', 'vector-analysis', 'vectors']"
54,why is the chain rule used for the area function $A=\frac{1}{2}xy$,why is the chain rule used for the area function,A=\frac{1}{2}xy,"To differentiate the area of a triangle function, $A=\dfrac{1}{2}xy$ with respect to time $t$, my text says to use the chain rule and the product rule. So it would be: $\dfrac{dA}{dt}=\dfrac{1}{2}(x\dfrac{dy}{dt}+y\dfrac{dx}{dt})$ Isn't this only the product rule? I don't see the chain rule here. Why do we have to use the chain rule here, because area is a function of $x$ and $y$ and $x$ and $y$ are functions of $t$? Is that correct? Thanks.","To differentiate the area of a triangle function, $A=\dfrac{1}{2}xy$ with respect to time $t$, my text says to use the chain rule and the product rule. So it would be: $\dfrac{dA}{dt}=\dfrac{1}{2}(x\dfrac{dy}{dt}+y\dfrac{dx}{dt})$ Isn't this only the product rule? I don't see the chain rule here. Why do we have to use the chain rule here, because area is a function of $x$ and $y$ and $x$ and $y$ are functions of $t$? Is that correct? Thanks.",,"['calculus', 'derivatives']"
55,"Differentiability of ""positive part"" function","Differentiability of ""positive part"" function",,"Let the positive part function be defined as $\max(0,x)$; this function is obviously not differentiable in $x=0$. But what about the (more smooth) function $\big( \max(0,x) \big)^{2}$. I suspect the latter isn't differentiable in $x=0$ either, but it's not very clear.","Let the positive part function be defined as $\max(0,x)$; this function is obviously not differentiable in $x=0$. But what about the (more smooth) function $\big( \max(0,x) \big)^{2}$. I suspect the latter isn't differentiable in $x=0$ either, but it's not very clear.",,"['real-analysis', 'differential-geometry', 'derivatives']"
56,"When are $\Delta x$, $\delta x$, $dx$, and $\text{đ}x$ exactly the same? When are they approximately the same?","When are , , , and  exactly the same? When are they approximately the same?",\Delta x \delta x dx \text{đ}x,"As a follow-up to this related question, I'd like to know under what circumstances, if any, $\Delta x$, $\delta x$ and $dx$ all mean the same thing, and under what circumstances they can all be said to be approximately equivalent in a reasonably valid way. For bonus points, it would also be nice to know where the inexact differential, $\text{đ}x$, can be used in place of one of the other symbols. The reason that I ask this is that all of these symbols are commonly used as notation in thermodynamics and stat mech textbooks, and are often interchanged in ways that can be confusing to follow. In the math used in physics it's common to elide exactly correct statements with approximately correct ones, and it would be nice to have guidance when it comes to picking this sort of thing apart.","As a follow-up to this related question, I'd like to know under what circumstances, if any, $\Delta x$, $\delta x$ and $dx$ all mean the same thing, and under what circumstances they can all be said to be approximately equivalent in a reasonably valid way. For bonus points, it would also be nice to know where the inexact differential, $\text{đ}x$, can be used in place of one of the other symbols. The reason that I ask this is that all of these symbols are commonly used as notation in thermodynamics and stat mech textbooks, and are often interchanged in ways that can be confusing to follow. In the math used in physics it's common to elide exactly correct statements with approximately correct ones, and it would be nice to have guidance when it comes to picking this sort of thing apart.",,"['calculus', 'derivatives', 'physics', 'differential', 'statistical-mechanics']"
57,Limit of function tending to derivative,Limit of function tending to derivative,,"Let $f$ be a continuously differentiable function on $R$ such that $$  \lim_{x \to \infty} | f'(x) - f(x) | = 0. $$ Can we say anything about $f$? For example, does there exist a constant $c \in R$ such that $$  \lim_{x \to \infty} | f(x) - c \cdot e^{x} | = 0? $$","Let $f$ be a continuously differentiable function on $R$ such that $$  \lim_{x \to \infty} | f'(x) - f(x) | = 0. $$ Can we say anything about $f$? For example, does there exist a constant $c \in R$ such that $$  \lim_{x \to \infty} | f(x) - c \cdot e^{x} | = 0? $$",,"['real-analysis', 'derivatives']"
58,Calculation of the derivative of $e^{\cos(x)}$ from first principles,Calculation of the derivative of  from first principles,e^{\cos(x)},"The derivative of $e^{\cos(x)}$ is $-\sin(x)e^{\cos(x)}$. However I would like to prove it using first principles, i.e. by using $f'(x)=\lim_{h\to0}\frac{f(x+h)-f(x)}{h}$. I tried Taylor series but it didn't work out. Thanks for any help.","The derivative of $e^{\cos(x)}$ is $-\sin(x)e^{\cos(x)}$. However I would like to prove it using first principles, i.e. by using $f'(x)=\lim_{h\to0}\frac{f(x+h)-f(x)}{h}$. I tried Taylor series but it didn't work out. Thanks for any help.",,"['calculus', 'derivatives']"
59,How to use logarithmic differentiation in this example (I seem to be going wrong),How to use logarithmic differentiation in this example (I seem to be going wrong),,"I'm revising for my Calculus & Vectors exam in January, and one of the warm-up questions from a previous problem sheet was to differentiate $e^{-t}*\cosh{t}*\sinh{t}$ This could be done through the product rule but having just practised logarithmic differentiation I thought it might be quicker to do that. However, doing that I get $$\ln{y}=\ln{e^{-t}}+\ln{(\cosh{t})}+\ln{(\sinh{t})}$$ $$\ln{y}=-t+\ln{(\cosh{t})}+\ln{(\sinh{t})}$$ $$\frac{1}{y}*\frac{dy}{dt} = -1 + \frac{\sinh{t}}{\cosh{t}} + \frac{\cosh{t}}{\sinh{t}}$$ $$\therefore \frac{dy}{dt} = -y + y\tanh{t} + y\coth{t}$$ Which doesn't involve any form of $e$ and is quite a long way away from what wolfram alpha gives too, so my guess is I've gone wrong somewhere! I'm also pretty new to latex so although that looks ok to me, sorry if it doesn't display properly.","I'm revising for my Calculus & Vectors exam in January, and one of the warm-up questions from a previous problem sheet was to differentiate $e^{-t}*\cosh{t}*\sinh{t}$ This could be done through the product rule but having just practised logarithmic differentiation I thought it might be quicker to do that. However, doing that I get $$\ln{y}=\ln{e^{-t}}+\ln{(\cosh{t})}+\ln{(\sinh{t})}$$ $$\ln{y}=-t+\ln{(\cosh{t})}+\ln{(\sinh{t})}$$ $$\frac{1}{y}*\frac{dy}{dt} = -1 + \frac{\sinh{t}}{\cosh{t}} + \frac{\cosh{t}}{\sinh{t}}$$ $$\therefore \frac{dy}{dt} = -y + y\tanh{t} + y\coth{t}$$ Which doesn't involve any form of $e$ and is quite a long way away from what wolfram alpha gives too, so my guess is I've gone wrong somewhere! I'm also pretty new to latex so although that looks ok to me, sorry if it doesn't display properly.",,"['calculus', 'derivatives']"
60,Converse of interchanging order for derivatives,Converse of interchanging order for derivatives,,"We know that for a twice-differentiable function $f$, $$\dfrac{\partial}{\partial x}\dfrac{\partial}{\partial y}f(x,y)=\dfrac{\partial}{\partial y}\dfrac{\partial}{\partial x}f(x,y).$$ Suppose there are functions $g,h:\mathbb{R}^2\rightarrow\mathbb{R}$ such that $$\dfrac{\partial}{\partial y}g(x,y)=\dfrac{\partial}{\partial x}h(x,y)$$ for all $x,y\in\mathbb{R}$. Is it necessary that there exists a function $f:\mathbb{R}^2\rightarrow\mathbb{R}$ such that $$\dfrac{\partial}{\partial x}f(x,y)=g(x,y)$$ and $$\dfrac{\partial}{\partial y}f(x,y)=h(x,y)$$ for all $x,y\in\mathbb{R}$?","We know that for a twice-differentiable function $f$, $$\dfrac{\partial}{\partial x}\dfrac{\partial}{\partial y}f(x,y)=\dfrac{\partial}{\partial y}\dfrac{\partial}{\partial x}f(x,y).$$ Suppose there are functions $g,h:\mathbb{R}^2\rightarrow\mathbb{R}$ such that $$\dfrac{\partial}{\partial y}g(x,y)=\dfrac{\partial}{\partial x}h(x,y)$$ for all $x,y\in\mathbb{R}$. Is it necessary that there exists a function $f:\mathbb{R}^2\rightarrow\mathbb{R}$ such that $$\dfrac{\partial}{\partial x}f(x,y)=g(x,y)$$ and $$\dfrac{\partial}{\partial y}f(x,y)=h(x,y)$$ for all $x,y\in\mathbb{R}$?",,"['calculus', 'derivatives', 'partial-derivative']"
61,Find the Critical points for a trigonometric derivative,Find the Critical points for a trigonometric derivative,,"So The original function was $$f(x) = 2\sin t-\cos 2t$$ on interval $[0,2\pi]$. Now I took the derivative which was $$f'(x) = 2(\cos t+\sin2t)$$ Now I set it equal to zero but I am stuck not knowing how to proceed to find the critical points: $$\cos t + \sin 2t = 0$$ (since $2$ cannot equal $0$) So what do I do now?","So The original function was $$f(x) = 2\sin t-\cos 2t$$ on interval $[0,2\pi]$. Now I took the derivative which was $$f'(x) = 2(\cos t+\sin2t)$$ Now I set it equal to zero but I am stuck not knowing how to proceed to find the critical points: $$\cos t + \sin 2t = 0$$ (since $2$ cannot equal $0$) So what do I do now?",,"['calculus', 'derivatives']"
62,Differentiation: chain rule over multiple arguments,Differentiation: chain rule over multiple arguments,,"I am wondering whether the following generally holds: $$ \frac{ \delta f(g(x), h(x))} {\delta x} = \frac{ \delta f(g(x), h(x))} {\delta g(x)} *\frac{ \delta g(x)} {\delta x} + \frac{ \delta f(g(x), h(x))} {\delta h(x)} *\frac{ \delta h(x)} {\delta x} $$ If so, what is the name of this rule, or of which existing rule is it a special case?","I am wondering whether the following generally holds: $$ \frac{ \delta f(g(x), h(x))} {\delta x} = \frac{ \delta f(g(x), h(x))} {\delta g(x)} *\frac{ \delta g(x)} {\delta x} + \frac{ \delta f(g(x), h(x))} {\delta h(x)} *\frac{ \delta h(x)} {\delta x} $$ If so, what is the name of this rule, or of which existing rule is it a special case?",,"['derivatives', 'partial-derivative']"
63,Derivative of $\lceil 1/x \rceil$,Derivative of,\lceil 1/x \rceil,"I'm looking for the derivative $\frac{d}{dx}\lceil 1/x \rceil$. I would like to find a real number $1<x \le y$ satisfying the minimum of $\left\lceil \frac{y}{x} \right\rceil x$, when $y$ is a fixed value $>0$. Is the ceiling function a problem ?","I'm looking for the derivative $\frac{d}{dx}\lceil 1/x \rceil$. I would like to find a real number $1<x \le y$ satisfying the minimum of $\left\lceil \frac{y}{x} \right\rceil x$, when $y$ is a fixed value $>0$. Is the ceiling function a problem ?",,"['derivatives', 'ceiling-and-floor-functions']"
64,Develop second-order method for approximating f'(x),Develop second-order method for approximating f'(x),,"I am stuck on the following question: Develop a second-order method for approximating $f'(x)$ that uses the data $f(x-h), f(x)$ , and $f(x+3h)$ only. Do you have any hints or tips? Thanks in advance.","I am stuck on the following question: Develop a second-order method for approximating that uses the data , and only. Do you have any hints or tips? Thanks in advance.","f'(x) f(x-h), f(x) f(x+3h)","['derivatives', 'numerical-methods']"
65,Multiple variables in derivative and finding the slope,Multiple variables in derivative and finding the slope,,I'm working on slope and geometrical differentiation. The problem is to find the slope of any point on the curve $$\frac{x^2}{3^2} + \frac{y^2}{2^2} = 1$$ I have found its derivative: $\frac{dy}{dx} = \frac{-4}{9} \frac{x}{y}$. The next requirement is to give the numerical value of the slope at x = 1. I obviously should give x the value of 1 in the derivative to calculate but how should I approach dealing with the value of y? Thanks in advance.,I'm working on slope and geometrical differentiation. The problem is to find the slope of any point on the curve $$\frac{x^2}{3^2} + \frac{y^2}{2^2} = 1$$ I have found its derivative: $\frac{dy}{dx} = \frac{-4}{9} \frac{x}{y}$. The next requirement is to give the numerical value of the slope at x = 1. I obviously should give x the value of 1 in the derivative to calculate but how should I approach dealing with the value of y? Thanks in advance.,,"['calculus', 'differential-geometry', 'derivatives']"
66,Why is this derivative correct?,Why is this derivative correct?,,Why is the following correct? Can't understand it. $$\dfrac{d}{dz}\bigg(\dfrac{e^z-e^{-z}}{e^{z}+e^{-z}}\bigg)=1-\dfrac{(e^z-e^{-z})^2}{(e^{z}+e^{-z})^2}$$,Why is the following correct? Can't understand it. $$\dfrac{d}{dz}\bigg(\dfrac{e^z-e^{-z}}{e^{z}+e^{-z}}\bigg)=1-\dfrac{(e^z-e^{-z})^2}{(e^{z}+e^{-z})^2}$$,,"['calculus', 'derivatives']"
67,Calculus Complicated Substitution Derivative,Calculus Complicated Substitution Derivative,,"When, $$y=6u^3+2u^2+5u-2 \ , \ u= \frac{1}{w^3+2} \ , \ w=\sin x -1 $$find what the derivative of $ \ y \ $equals when $ \ x = \pi \ . $ Tried it many times, still can't seem to get the right answer (81)","When, $$y=6u^3+2u^2+5u-2 \ , \ u= \frac{1}{w^3+2} \ , \ w=\sin x -1 $$find what the derivative of $ \ y \ $equals when $ \ x = \pi \ . $ Tried it many times, still can't seem to get the right answer (81)",,"['calculus', 'derivatives']"
68,Why is this derivative incorrect?,Why is this derivative incorrect?,,We have to find the derivative of $$f(x) = \dfrac{\tan(2x)}{\sin(x)}$$ I would like to know why my approach is incorrect: $$f'(x) = \dfrac{\sin(x) \cdot \dfrac{2}{\cos^2(2x)}  - \tan(2x) \cdot \cos(x)}{\sin^2(x)}$$ $$ =  \dfrac{ 2 \sin(x) - \tan(2x) \cdot \cos(x)}{\cos^2(2x) \cdot \sin^2(x)}$$ $$ = \dfrac {2 \sin(x) - \sin(2x) \cdot \cos(x)}{\cos^3(2x) \cdot \sin^2(x)}$$ p.s. - To avoid confusion ; I wanted to get rid of the $\tan$. I'm sure there is a shorter method than this but I don't want it; I just want to know why this is wrong.,We have to find the derivative of $$f(x) = \dfrac{\tan(2x)}{\sin(x)}$$ I would like to know why my approach is incorrect: $$f'(x) = \dfrac{\sin(x) \cdot \dfrac{2}{\cos^2(2x)}  - \tan(2x) \cdot \cos(x)}{\sin^2(x)}$$ $$ =  \dfrac{ 2 \sin(x) - \tan(2x) \cdot \cos(x)}{\cos^2(2x) \cdot \sin^2(x)}$$ $$ = \dfrac {2 \sin(x) - \sin(2x) \cdot \cos(x)}{\cos^3(2x) \cdot \sin^2(x)}$$ p.s. - To avoid confusion ; I wanted to get rid of the $\tan$. I'm sure there is a shorter method than this but I don't want it; I just want to know why this is wrong.,,"['trigonometry', 'derivatives']"
69,"How to take a derivative of a function with messy parameters $\frac{d}{dt} f(x\cos(t) + y\sin(t), y\cos(t) - x\cos(t), z)$",How to take a derivative of a function with messy parameters,"\frac{d}{dt} f(x\cos(t) + y\sin(t), y\cos(t) - x\cos(t), z)","How would you compute $$\frac{d}{dt} f(x\cos(t) + y\sin(t), y\cos(t) - x\cos(t), z)$$ in terms of the original function $f$ and $x,y,t$? Assume $f$ is some well-behaved but not specified function. Note: This is a total derivative wrt $t$, not partial.","How would you compute $$\frac{d}{dt} f(x\cos(t) + y\sin(t), y\cos(t) - x\cos(t), z)$$ in terms of the original function $f$ and $x,y,t$? Assume $f$ is some well-behaved but not specified function. Note: This is a total derivative wrt $t$, not partial.",,"['calculus', 'derivatives']"
70,Find the second derivative ${{{d^2}y} \over {d{x^2}}}$ in terms of t when $x = 3 - 2{t^2}$ and $y = {1 \over t}$,Find the second derivative  in terms of t when  and,{{{d^2}y} \over {d{x^2}}} x = 3 - 2{t^2} y = {1 \over t},"This is my attempt: $\eqalign{   & x = 3 - 2{t^2}  \cr    & y = {1 \over t}  \cr    & {{dx} \over {dt}} =  - 4t  \cr    & {{dy} \over {dt}} =  - {t^{ - 2}} = {{ - 1} \over {{t^2}}}  \cr    & {{dy} \over {dx}} = {{ - 1} \over {{t^2}}} \times {1 \over { - 4t}} = {1 \over {4{t^3}}}  \cr    & {{{d^2}y} \over {d{x^2}}} = {d \over {dx}}\left( {{{dy} \over {dx}}} \right) = {{{d \over {dx}}\left( {{{dy} \over {dt}}} \right)} \over {{{dx} \over {dt}}}} = {{{d \over {dt}}\left( {{{dy} \over {dx}}} \right)} \over {{{dx} \over {dt}}}}  \cr    & {{{d^2}y} \over {d{x^2}}}:  \cr    & {d \over {dt}}\left( {{{dy} \over {dx}}} \right) =  - 1{(4{t^3})^{ - 2}} \times 12{t^2} = {{ - 12{t^2}} \over {16{t^6}}} = {{ - 3t} \over {4{t^4}}}  \cr    & {{dx} \over {dt}} =  - 4t  \cr    & {{{d^2}y} \over {d{x^2}}} = {{ - 3t} \over {4{t^4}}} \times {1 \over { - 4t}} = {{3t} \over {16{t^5}}} \cr} $ After some wrangling I've finally got that answer, which is right according to the textbook. The problem I was experiencing was knowing when to treat an expression as a function or a variable? I recognise that you have to differentiate the two differently, but I dont completely understand it. Why couldn't I differentiate the numerator of the second differential ${d \over {dt}}\left( {{{dy} \over {dx}}} \right)$ like this:  $$\eqalign{   & {{dy} \over {dx}} = 4{t^{ - 3}}  \cr    & {d \over {dt}}\left( {{{dy} \over {dx}}} \right) =  - 12{t^{ - 4}} \cr} $$ T is a variable of the function x(t), so should I not be able to differentiate it as normal variable? I hope you have kept with me and this doesnt sound convoluted, thank you.","This is my attempt: $\eqalign{   & x = 3 - 2{t^2}  \cr    & y = {1 \over t}  \cr    & {{dx} \over {dt}} =  - 4t  \cr    & {{dy} \over {dt}} =  - {t^{ - 2}} = {{ - 1} \over {{t^2}}}  \cr    & {{dy} \over {dx}} = {{ - 1} \over {{t^2}}} \times {1 \over { - 4t}} = {1 \over {4{t^3}}}  \cr    & {{{d^2}y} \over {d{x^2}}} = {d \over {dx}}\left( {{{dy} \over {dx}}} \right) = {{{d \over {dx}}\left( {{{dy} \over {dt}}} \right)} \over {{{dx} \over {dt}}}} = {{{d \over {dt}}\left( {{{dy} \over {dx}}} \right)} \over {{{dx} \over {dt}}}}  \cr    & {{{d^2}y} \over {d{x^2}}}:  \cr    & {d \over {dt}}\left( {{{dy} \over {dx}}} \right) =  - 1{(4{t^3})^{ - 2}} \times 12{t^2} = {{ - 12{t^2}} \over {16{t^6}}} = {{ - 3t} \over {4{t^4}}}  \cr    & {{dx} \over {dt}} =  - 4t  \cr    & {{{d^2}y} \over {d{x^2}}} = {{ - 3t} \over {4{t^4}}} \times {1 \over { - 4t}} = {{3t} \over {16{t^5}}} \cr} $ After some wrangling I've finally got that answer, which is right according to the textbook. The problem I was experiencing was knowing when to treat an expression as a function or a variable? I recognise that you have to differentiate the two differently, but I dont completely understand it. Why couldn't I differentiate the numerator of the second differential ${d \over {dt}}\left( {{{dy} \over {dx}}} \right)$ like this:  $$\eqalign{   & {{dy} \over {dx}} = 4{t^{ - 3}}  \cr    & {d \over {dt}}\left( {{{dy} \over {dx}}} \right) =  - 12{t^{ - 4}} \cr} $$ T is a variable of the function x(t), so should I not be able to differentiate it as normal variable? I hope you have kept with me and this doesnt sound convoluted, thank you.",,"['calculus', 'derivatives']"
71,Question about derivative,Question about derivative,,"I need to check whether I've done it correctly To find whether a point is maximum of function $f(x)$, we have to checked whether $f''(x)>0, f''(x)=0$ or $f''(x)<0?$ To find the inflection point of the function, we have to find, $f''(x)=0, f'(x)=0,$ $f(x)=0.$ When choose the value of $\sqrt{(64,3)},$ $X_o$ has the value: $64, 0,3$ or $X_o>64.$ My answers are the following. $1. f''(x)<0$ $2. f''(x)=0$ $3. X_o = 64.$","I need to check whether I've done it correctly To find whether a point is maximum of function $f(x)$, we have to checked whether $f''(x)>0, f''(x)=0$ or $f''(x)<0?$ To find the inflection point of the function, we have to find, $f''(x)=0, f'(x)=0,$ $f(x)=0.$ When choose the value of $\sqrt{(64,3)},$ $X_o$ has the value: $64, 0,3$ or $X_o>64.$ My answers are the following. $1. f''(x)<0$ $2. f''(x)=0$ $3. X_o = 64.$",,['derivatives']
72,Derivative of a delta function of a function,Derivative of a delta function of a function,,"I am just wondering why this is correct, $$ \frac{d}{dR} \delta[f(t')] = -\frac{1}{c}\frac{d}{df}\delta[f(t')], $$ where $t' = t - \frac{R}{c}$. Is this simply due to chain rule? Or something like $$ \frac{d}{dR} = \frac{d}{df}\frac{df}{dR}. $$ Thanks!","I am just wondering why this is correct, $$ \frac{d}{dR} \delta[f(t')] = -\frac{1}{c}\frac{d}{df}\delta[f(t')], $$ where $t' = t - \frac{R}{c}$. Is this simply due to chain rule? Or something like $$ \frac{d}{dR} = \frac{d}{df}\frac{df}{dR}. $$ Thanks!",,['derivatives']
73,Differentiability of a function at a point,Differentiability of a function at a point,,"My high-school calculus teacher has asserted that a function $f(x)$ can only fail to be differentiable at a point $x=a$ if one of the following is true: The function is discontinuous at $x=a$: $\lim_{x\to a}f(x) \ne f(a)$ The function has a cusp or vertical tangent at $x=a$: $\lim_{x\to a}\left|{{f(x)-f(a)}\over{x-a}}\right| = \infty$ The function has a corner at $x=a$: $\lim_{x\to a^+} {{f(x)-f(a)}\over{x-a}} \ne \lim_{x\to a^-} {{f(x)-f(a)}\over{x-a}}$ While in most cases that is probably correct, I find it somewhat hard to swallow that it is that way for all functions.  Specifically, the function $$f(x)=\begin{cases}x\sin \ln x^2, & x\ne0 \\ 0, & x=0\end{cases}$$ is most definitely not differentiable at $x=0$, but it also doesn't appear to satisfy any of the properties listed above. The derivative $\frac{\mathrm{d} }{\mathrm{d} x}f(x)$ of the function for $x\ne0$ appears to be $\sin{{\ln x^2}}+2\cos{{\ln x^2}}$, which doesn't show any signs of increasing without bounds as $x\to0$ or suddenly changing at $x=0$, and $f$ is most definitely continuous at that point. So, the Question is: What's up with$f$?  Does it actually fall into one of the cases above, or are they only good as a rough guide for some sorts of functions?","My high-school calculus teacher has asserted that a function $f(x)$ can only fail to be differentiable at a point $x=a$ if one of the following is true: The function is discontinuous at $x=a$: $\lim_{x\to a}f(x) \ne f(a)$ The function has a cusp or vertical tangent at $x=a$: $\lim_{x\to a}\left|{{f(x)-f(a)}\over{x-a}}\right| = \infty$ The function has a corner at $x=a$: $\lim_{x\to a^+} {{f(x)-f(a)}\over{x-a}} \ne \lim_{x\to a^-} {{f(x)-f(a)}\over{x-a}}$ While in most cases that is probably correct, I find it somewhat hard to swallow that it is that way for all functions.  Specifically, the function $$f(x)=\begin{cases}x\sin \ln x^2, & x\ne0 \\ 0, & x=0\end{cases}$$ is most definitely not differentiable at $x=0$, but it also doesn't appear to satisfy any of the properties listed above. The derivative $\frac{\mathrm{d} }{\mathrm{d} x}f(x)$ of the function for $x\ne0$ appears to be $\sin{{\ln x^2}}+2\cos{{\ln x^2}}$, which doesn't show any signs of increasing without bounds as $x\to0$ or suddenly changing at $x=0$, and $f$ is most definitely continuous at that point. So, the Question is: What's up with$f$?  Does it actually fall into one of the cases above, or are they only good as a rough guide for some sorts of functions?",,['derivatives']
74,Is it proper to multiply both sides of a differentiated equation by dx?,Is it proper to multiply both sides of a differentiated equation by dx?,,"I started Calculus 2 on Monday, where we're beginning with Integration. As somewhat of a refresher, the professor is having us find the derivative of several equations and writing them in the form $$\frac{d}{dx}(x^2-4x)=2x-4 \to d(x^2-4x)=(2x-4)dx$$ I understand that this has something to do with preparing us to understand why you see integrals in the form of $$\int(2x-4)dx = x^2-4x+C$$ but I was led to believe that you can't do this, because $\frac{d}{dx}$ is not a fraction.","I started Calculus 2 on Monday, where we're beginning with Integration. As somewhat of a refresher, the professor is having us find the derivative of several equations and writing them in the form $$\frac{d}{dx}(x^2-4x)=2x-4 \to d(x^2-4x)=(2x-4)dx$$ I understand that this has something to do with preparing us to understand why you see integrals in the form of $$\int(2x-4)dx = x^2-4x+C$$ but I was led to believe that you can't do this, because $\frac{d}{dx}$ is not a fraction.",,"['integration', 'derivatives']"
75,Are my derivatives worked correctly?,Are my derivatives worked correctly?,,"I have 3 derivatives that I need to find: 1: $ y = 3sin(x) - ln(x) + 2x^\frac{1}{2} $ My answer: $ 3cos(x) - \frac{1}{x} + (\frac{1}{2}) (2x^\frac{-1}{2}) $ 2: $ y = 3e^{2x} * ln(2x + 1) $ My Answer: $ 3e^{2x} (\frac{2}{2x+1}) + 6e^{2x} (ln(2x+1)) $ 3: $y = \frac{sin(2x)}{3x^2 + 5} $ My Answer: $ \frac{(3x^2 + 5)(2sin(2x)) - sin(2x)(6x)}{(3x^2 + 5)^2} $ I'm relatively positive I did these correctly, but please let me know if I haven't. EDIT 3: $ \frac{(3x^2 + 5)(2cos(2x)) - sin(2x)(6x)}{(3x^2 + 5)^2} $","I have 3 derivatives that I need to find: 1: $ y = 3sin(x) - ln(x) + 2x^\frac{1}{2} $ My answer: $ 3cos(x) - \frac{1}{x} + (\frac{1}{2}) (2x^\frac{-1}{2}) $ 2: $ y = 3e^{2x} * ln(2x + 1) $ My Answer: $ 3e^{2x} (\frac{2}{2x+1}) + 6e^{2x} (ln(2x+1)) $ 3: $y = \frac{sin(2x)}{3x^2 + 5} $ My Answer: $ \frac{(3x^2 + 5)(2sin(2x)) - sin(2x)(6x)}{(3x^2 + 5)^2} $ I'm relatively positive I did these correctly, but please let me know if I haven't. EDIT 3: $ \frac{(3x^2 + 5)(2cos(2x)) - sin(2x)(6x)}{(3x^2 + 5)^2} $",,"['calculus', 'derivatives']"
76,Derivative of $f(s) = \frac{1}{\prod_{k=1}^{n}(x_k-s)}$,Derivative of,f(s) = \frac{1}{\prod_{k=1}^{n}(x_k-s)},"Denote  $$ f'_{1}(s) =  \bigg( \frac{1}{x_1-s} \bigg)'_{s} = \frac{1}{(x_1-s)^2}\\ f'_{2}(s) =  \bigg( \frac{1}{(x_1-s)(x_2 -s)} \bigg)'_{s} = \frac{x_1 +x_2 - 2s}{((x_1-s)(x_2-s))^2} $$ and so on. Is it possible to find a general form of the derivative for $f_n(s) = \frac{1}{\prod_{k=1}^{n}(x_k-s)}$? I were thinking of something with recurrent expression, but could not come up with anything useful. This expression arises in characteristic functions of sums of random variables and queueing theory. Thanks.","Denote  $$ f'_{1}(s) =  \bigg( \frac{1}{x_1-s} \bigg)'_{s} = \frac{1}{(x_1-s)^2}\\ f'_{2}(s) =  \bigg( \frac{1}{(x_1-s)(x_2 -s)} \bigg)'_{s} = \frac{x_1 +x_2 - 2s}{((x_1-s)(x_2-s))^2} $$ and so on. Is it possible to find a general form of the derivative for $f_n(s) = \frac{1}{\prod_{k=1}^{n}(x_k-s)}$? I were thinking of something with recurrent expression, but could not come up with anything useful. This expression arises in characteristic functions of sums of random variables and queueing theory. Thanks.",,"['derivatives', 'products']"
77,Find $\frac{dy}{dx}$ when $y=\frac{x^2-1}{x^4-1}$,Find  when,\frac{dy}{dx} y=\frac{x^2-1}{x^4-1},"Find $\frac{dy}{dx}$ $$\begin{align*} y&=\frac{x^2-1}{x^4-1}\\ &=\frac{x^4-1(2x)-x^2-1(4x^3)}{(x^4-1)^2}\\ &=\frac{2x^5-2x-4x^5-4x^3}{(x^4-1)^2} \end{align*}$$ but the right answer is  $$\frac{-2x^5+4x^3-2x}{(x^4-1)^2}$$ what did I do right, I used  quotient rule. I want to use below formula, but i don't know how to $$\frac{dy}{dx}=\frac{v\frac{du}{dx}-u\frac{dv}{dx}}{v^2}$$ many thanks in advance!","Find $\frac{dy}{dx}$ $$\begin{align*} y&=\frac{x^2-1}{x^4-1}\\ &=\frac{x^4-1(2x)-x^2-1(4x^3)}{(x^4-1)^2}\\ &=\frac{2x^5-2x-4x^5-4x^3}{(x^4-1)^2} \end{align*}$$ but the right answer is  $$\frac{-2x^5+4x^3-2x}{(x^4-1)^2}$$ what did I do right, I used  quotient rule. I want to use below formula, but i don't know how to $$\frac{dy}{dx}=\frac{v\frac{du}{dx}-u\frac{dv}{dx}}{v^2}$$ many thanks in advance!",,"['calculus', 'derivatives']"
78,Proof of Taylor's series expansion with two terms,Proof of Taylor's series expansion with two terms,,"I am looking for a simple direct proof of the fact that $$ \frac{\frac{f(x + \Delta x) - f(x)}{\Delta x} -f'(x)}{\Delta x} \stackrel{\Delta x \to 0}{\to} \frac{1}{2}f''(x), $$ or, equivalently, $$ f(x+\Delta x) = f(x) + f'(x)\Delta x + \frac{1}{2}f''(x)\Delta x^2 + o(\Delta x^2). $$ holds for a twice-differentiable $f(x)$. I remember there were times when I could derive this directly from the following definition of a derivative: $$ f(x+\Delta x) = f(x) + f'(x)\Delta x + o(\Delta x) $$ in a couple of simple lines. A long time has passed since then and now I need to either recollect this magical ""obvious"" proof of mine or find out I was wrong then and the actual proof is more involved.","I am looking for a simple direct proof of the fact that $$ \frac{\frac{f(x + \Delta x) - f(x)}{\Delta x} -f'(x)}{\Delta x} \stackrel{\Delta x \to 0}{\to} \frac{1}{2}f''(x), $$ or, equivalently, $$ f(x+\Delta x) = f(x) + f'(x)\Delta x + \frac{1}{2}f''(x)\Delta x^2 + o(\Delta x^2). $$ holds for a twice-differentiable $f(x)$. I remember there were times when I could derive this directly from the following definition of a derivative: $$ f(x+\Delta x) = f(x) + f'(x)\Delta x + o(\Delta x) $$ in a couple of simple lines. A long time has passed since then and now I need to either recollect this magical ""obvious"" proof of mine or find out I was wrong then and the actual proof is more involved.",,"['calculus', 'taylor-expansion', 'derivatives']"
79,"Why is my logic incorrect? I think it is right, my answer is ALMOST the same. Derivatives and chain rule.","Why is my logic incorrect? I think it is right, my answer is ALMOST the same. Derivatives and chain rule.",,"I want to find the derivative of $y = \log_b(\log_b(x))$ I am going to let $u = \log_b(x)$ so that $y = \log_b(u)$. BY Chain Rule, I get $$\frac{dy}{dx} =\frac{dy}{du} \frac{du}{dx}$$ $$\frac{dy}{du} = \frac{1}{u\ln(b)} = \frac{1}{\log_b(x)\ln(b)}$$ $$\frac{du}{dx} = \frac{1}{x\ln(b)}$$ $$\frac{dy}{dx} =\frac{dy}{du} \frac{du}{dx} =\frac{1}{\log_b(x)\ln(b)} \frac{1}{x\ln(b)} = \frac{1}{\ln^2(b)x\log_b(x)}$$ The answer I got from Wolframalpha is http://www.wolframalpha.com/input/?i=D[log[log%28x%29]%2Cx]&a=*C.D-_*Function.dflt-&a=*FunClash.log-_*Log10.Log- Which they only have one $\ln(b)$. Note that they have $b = 10$ in this case.","I want to find the derivative of $y = \log_b(\log_b(x))$ I am going to let $u = \log_b(x)$ so that $y = \log_b(u)$. BY Chain Rule, I get $$\frac{dy}{dx} =\frac{dy}{du} \frac{du}{dx}$$ $$\frac{dy}{du} = \frac{1}{u\ln(b)} = \frac{1}{\log_b(x)\ln(b)}$$ $$\frac{du}{dx} = \frac{1}{x\ln(b)}$$ $$\frac{dy}{dx} =\frac{dy}{du} \frac{du}{dx} =\frac{1}{\log_b(x)\ln(b)} \frac{1}{x\ln(b)} = \frac{1}{\ln^2(b)x\log_b(x)}$$ The answer I got from Wolframalpha is http://www.wolframalpha.com/input/?i=D[log[log%28x%29]%2Cx]&a=*C.D-_*Function.dflt-&a=*FunClash.log-_*Log10.Log- Which they only have one $\ln(b)$. Note that they have $b = 10$ in this case.",,"['calculus', 'derivatives']"
80,On the derivative of Dirac delta,On the derivative of Dirac delta,,"Let $\delta(x)$ be the Dirac's delta, i.e. the ""strange object"" characterized by the two properties $\delta(x)=0$ for all $x\neq 0$ and $\int \delta(x) f(x)\text{ d}x= f(0)$ ( $\int f$ is the integral over $\mathbb{R}$ ). Now, let's define the derivative of $\delta(x)$ as \begin{equation*} \dot{\delta}(x)\triangleq \lim_{\tau \to 0} \frac{\delta(x+\tau)-\delta(x)}{\tau} \end{equation*} For simplicity, I'm considering the univariate case $x\in\mathbb{R}$ . Now let's try to see what happens when we compute a weighted integral of $f(x)$ involving $\dot{\delta}(x)$ . I would write naively \begin{equation*} \begin{aligned} \int \dot{\delta}(x) f(x)\text{ d}x&= \int \left(\lim_{\tau \to 0} \frac{\delta(x+\tau)-\delta(x)}{\tau}\right)f(x)\text{ d}x\\ &= \lim_{\tau \to 0} \frac{1}{\tau}\left[\int \delta(x+\tau)\,f(x)\text{ d}x - \int \delta(x)\,f(x)\text{ d}x\right]\\ &= \lim_{\tau \to 0} \frac{1}{\tau}\left[f(0-\tau) - f(0)\right]=-\dot{f}(0)\\ \end{aligned} \end{equation*} provided that we can push the limit sign outside the integral (if I'm not wrong, here Lebesgue help us to see when this can be done); the difference between the two integrals is not in the form $\pm \infty \mp\infty$ ; the derivative of $f(x)$ exists in $x=0$ . If I'm not missing anything else, we can say that, just like $\int \delta(x) f(x) \text{d}x=f(0)$ , we have $\int \dot{\delta}(x) f(x)\text{ d}x=-\dot{f}(0)$ . Question Since the $\delta(x)$ is a ""strange object"", I'm not 100% sure about my conclusions. So, my question is: am I right? If not, where I'm wrong?","Let be the Dirac's delta, i.e. the ""strange object"" characterized by the two properties for all and ( is the integral over ). Now, let's define the derivative of as For simplicity, I'm considering the univariate case . Now let's try to see what happens when we compute a weighted integral of involving . I would write naively provided that we can push the limit sign outside the integral (if I'm not wrong, here Lebesgue help us to see when this can be done); the difference between the two integrals is not in the form ; the derivative of exists in . If I'm not missing anything else, we can say that, just like , we have . Question Since the is a ""strange object"", I'm not 100% sure about my conclusions. So, my question is: am I right? If not, where I'm wrong?","\delta(x) \delta(x)=0 x\neq 0 \int \delta(x) f(x)\text{ d}x= f(0) \int f \mathbb{R} \delta(x) \begin{equation*}
\dot{\delta}(x)\triangleq \lim_{\tau \to 0} \frac{\delta(x+\tau)-\delta(x)}{\tau}
\end{equation*} x\in\mathbb{R} f(x) \dot{\delta}(x) \begin{equation*}
\begin{aligned}
\int \dot{\delta}(x) f(x)\text{ d}x&= \int \left(\lim_{\tau \to 0} \frac{\delta(x+\tau)-\delta(x)}{\tau}\right)f(x)\text{ d}x\\
&= \lim_{\tau \to 0} \frac{1}{\tau}\left[\int \delta(x+\tau)\,f(x)\text{ d}x - \int \delta(x)\,f(x)\text{ d}x\right]\\
&= \lim_{\tau \to 0} \frac{1}{\tau}\left[f(0-\tau) - f(0)\right]=-\dot{f}(0)\\
\end{aligned}
\end{equation*} \pm \infty \mp\infty f(x) x=0 \int \delta(x) f(x) \text{d}x=f(0) \int \dot{\delta}(x) f(x)\text{ d}x=-\dot{f}(0) \delta(x)","['real-analysis', 'integration', 'derivatives', 'distribution-theory', 'dirac-delta']"
81,"How do I solve: The normal to the curve $\ xe^{-y} + e^{y} = 1 + x$ at the point $(c, \ln c)$ has a $y$-intercept $\ c^2+ 1$. Determine c.",How do I solve: The normal to the curve  at the point  has a -intercept . Determine c.,"\ xe^{-y} + e^{y} = 1 + x (c, \ln c) y \ c^2+ 1","How do I solve: The normal to the curve $\ xe^{-y} + e^{y} = 1 + x$ at the point $(c, \ln c)$ has a $y$ -intercept $\ c^2+ 1$ . Determine c. I tried substituting $\ x=0$ to find out the y-intercept, but it came out to $\ e^y = 1$ , which is 0. I don't understand what I should do. How do I differentiate the left and right sides of the equation?","How do I solve: The normal to the curve at the point has a -intercept . Determine c. I tried substituting to find out the y-intercept, but it came out to , which is 0. I don't understand what I should do. How do I differentiate the left and right sides of the equation?","\ xe^{-y} + e^{y} = 1 + x (c, \ln c) y \ c^2+ 1 \ x=0 \ e^y = 1","['calculus', 'derivatives', 'implicit-differentiation']"
82,"Is there a ""hidden"" product-rule in every derivative?","Is there a ""hidden"" product-rule in every derivative?",,"This is a very quick question, and it might be pretty basic. But as a preface, I plan to dive into the actual proofs behind the derivative rules after this post, but I would like to see if my intuition is correct here first! I was toying around with derivative rules, and I had the thought that, no matter what form your function is in, you will be performing the product rule ! For example, if you want to take a derivative of the function $f(x) = x^2\times sin(x)$ , you would apply the product rule and get $\frac{d}{dx} f(x) = 2x\times sin(x) + x^2\times cos(x)$ This is an obvious use of the product rule , but I think that there is a less obvious use of the product rule when taking the derivative of $f(x) = x^2$ . The derivative is simple, if we use the power rule we get: $$\frac{d}{dx} f(x) = 2x$$ And this is done with the formula $\frac{d}{dx} [x^n] = nx^{n-1}$ . But I am positing that there is a ""hidden"" product rule being performed as well! Because if we think about it, there is still multiplication taking place with the $x$ in $f(x) = x^2$ , this is shown in the ""expanded"" version of: $$f(x) = 1\times x^2$$ And you cannot just skip over the product rule ! So in taking the derivative of this function we are actually performing a combination of the power rule and product rule : $$\frac{d}{dx} f(x) = 0\times x^2 + 1\times 2x$$ which is implicitly done with the output of the power rule alone, so this is why we do not show the product rule in action here. This is my own intuition based on what I have learned so far, and I am curious if I am correct, or making dangerous assumptions!","This is a very quick question, and it might be pretty basic. But as a preface, I plan to dive into the actual proofs behind the derivative rules after this post, but I would like to see if my intuition is correct here first! I was toying around with derivative rules, and I had the thought that, no matter what form your function is in, you will be performing the product rule ! For example, if you want to take a derivative of the function , you would apply the product rule and get This is an obvious use of the product rule , but I think that there is a less obvious use of the product rule when taking the derivative of . The derivative is simple, if we use the power rule we get: And this is done with the formula . But I am positing that there is a ""hidden"" product rule being performed as well! Because if we think about it, there is still multiplication taking place with the in , this is shown in the ""expanded"" version of: And you cannot just skip over the product rule ! So in taking the derivative of this function we are actually performing a combination of the power rule and product rule : which is implicitly done with the output of the power rule alone, so this is why we do not show the product rule in action here. This is my own intuition based on what I have learned so far, and I am curious if I am correct, or making dangerous assumptions!",f(x) = x^2\times sin(x) \frac{d}{dx} f(x) = 2x\times sin(x) + x^2\times cos(x) f(x) = x^2 \frac{d}{dx} f(x) = 2x \frac{d}{dx} [x^n] = nx^{n-1} x f(x) = x^2 f(x) = 1\times x^2 \frac{d}{dx} f(x) = 0\times x^2 + 1\times 2x,['derivatives']
83,Chain rule when sum is differentiable but individual functions are not,Chain rule when sum is differentiable but individual functions are not,,"Let $g_1 , g_2 : \mathbb{R} \to \mathbb{R}$ be differentiable. Suppose we know that the following derivative exists at some point $x_0$ : $$ \frac{d}{dx}[g_1(f_1(x))+g_2(f_2(x))] $$ but do not know if $f_1$ and $f_2$ individually are differentiable at that point. Can we write the above derivative at that point as follows? $$ \frac{d}{dx}|_{x=x_0}[g_1'(f_1(x_0))\cdot f_1(x) + g_2'(f_2(x_0))\cdot f_2(x)] $$",Let be differentiable. Suppose we know that the following derivative exists at some point : but do not know if and individually are differentiable at that point. Can we write the above derivative at that point as follows?,"g_1 , g_2 : \mathbb{R} \to \mathbb{R} x_0 
\frac{d}{dx}[g_1(f_1(x))+g_2(f_2(x))]
 f_1 f_2 
\frac{d}{dx}|_{x=x_0}[g_1'(f_1(x_0))\cdot f_1(x) + g_2'(f_2(x_0))\cdot f_2(x)]
","['calculus', 'derivatives', 'chain-rule']"
84,Confused by integration calc in my financial maths notes!,Confused by integration calc in my financial maths notes!,,"I have added a pic of the notes that I am looking at! I’m a little confused by them.In integration calcs, I’m used to seeing the integral of $f(x)$ to get the difference between two points. Here it’s integrating $f^\prime(x)$ to get the difference. I’m trying to picture the problem graphically - I’m assuming that $M$ is along the $y$ -axis and $t$ is on the $x$ -axis. In which case wouldn’t we just integrate $M(t)$ ? Unless $M(t)$ is supposed to be the CDF? Basically I don’t understand why it’s the integral of $M^\prime(t)$ and not the integral of $M(t)$ Sorry getting very confused with this! Any help is much appreciated","I have added a pic of the notes that I am looking at! I’m a little confused by them.In integration calcs, I’m used to seeing the integral of to get the difference between two points. Here it’s integrating to get the difference. I’m trying to picture the problem graphically - I’m assuming that is along the -axis and is on the -axis. In which case wouldn’t we just integrate ? Unless is supposed to be the CDF? Basically I don’t understand why it’s the integral of and not the integral of Sorry getting very confused with this! Any help is much appreciated",f(x) f^\prime(x) M y t x M(t) M(t) M^\prime(t) M(t),"['integration', 'derivatives', 'finance']"
85,Is the series $\sum_{n=1}^\infty\ln(1 + a_n)$ convergent or divergent?,Is the series  convergent or divergent?,\sum_{n=1}^\infty\ln(1 + a_n),"We are given that $\sum_{n=1}^\infty a_n$ is convergent, and we want to know if $\sum_{n=1}^\infty\ln(1 + a_n)$ is convergent or not. I used the limit comparison test. We get $$ \lim_{n\rightarrow\infty}\frac{\ln(1+a_n)}{a_n}.$$ And using L'Hospital's rule, I get $$\lim_{n\rightarrow\infty}\frac{\ln(1+a_n)}{a_n} = \lim_{n\rightarrow\infty}\frac{\ln(1+a_n)'}{{a_n}'} = \lim_{n\rightarrow\infty}\frac{\frac{1}{1+a_n}(a_n)'}{{a_n}'} = \lim_{n\rightarrow\infty}\frac{1}{1+a_n}=1.$$ However, we must know that $a_n$ can be convert to a differentiable function then we can use L'Hospital's rule, so I am just confused, is there any way to prove this, or can we prove that $a_n $ can be converted to a differentiable function? I also took a look at the same questions/answers, but none of these answered my question.","We are given that is convergent, and we want to know if is convergent or not. I used the limit comparison test. We get And using L'Hospital's rule, I get However, we must know that can be convert to a differentiable function then we can use L'Hospital's rule, so I am just confused, is there any way to prove this, or can we prove that can be converted to a differentiable function? I also took a look at the same questions/answers, but none of these answered my question.",\sum_{n=1}^\infty a_n \sum_{n=1}^\infty\ln(1 + a_n)  \lim_{n\rightarrow\infty}\frac{\ln(1+a_n)}{a_n}. \lim_{n\rightarrow\infty}\frac{\ln(1+a_n)}{a_n} = \lim_{n\rightarrow\infty}\frac{\ln(1+a_n)'}{{a_n}'} = \lim_{n\rightarrow\infty}\frac{\frac{1}{1+a_n}(a_n)'}{{a_n}'} = \lim_{n\rightarrow\infty}\frac{1}{1+a_n}=1. a_n a_n ,"['calculus', 'sequences-and-series', 'derivatives']"
86,"A vector function with ""nearly"" identity derivative","A vector function with ""nearly"" identity derivative",,"I encountered the following exercise in calculus course: Let $U$ be a convex open subset of $\mathbb{R}^n$ and $f \colon U \to \mathbb{R}^n$ differentiable. Suppose that $\lVert Df(\mathbf{x}) - I_n \rVert < 1$ for all $\mathbf{x} \in U$ where $I_n \colon \mathbb{R}^n \to \mathbb{R}^n$ is an identity map. Show that $f$ is injective. Furthermore, show that $f(U)$ is open. $\lVert \cdot\rVert$ is an operator norm, i.e. $\lVert A \rVert = \max_{x\in S}\lVert Ax\rVert$ where $S$ is an unit sphere. I think there is a convexity condition to show that $f$ is injective. If $f(a) = f(b)$ for some $a,b \in \mathbb{R}^n$ , then consider a path $c(t) = ta + (1-t)b$ and do something with this. Since only condition given is that $\lVert Df(\mathbf{x}) - I_n \rVert < 1$ , I tried to find some point such that $Df(\mathbf{x})$ is ""far away"" from $I_n$ . I have a vague idea to use MVT, or Rolle's theorem, but I am seriously stuck. How can I use those conditions to prove injectivity of $f$ and that $f(U)$ is open? Thanks in advance!","I encountered the following exercise in calculus course: Let be a convex open subset of and differentiable. Suppose that for all where is an identity map. Show that is injective. Furthermore, show that is open. is an operator norm, i.e. where is an unit sphere. I think there is a convexity condition to show that is injective. If for some , then consider a path and do something with this. Since only condition given is that , I tried to find some point such that is ""far away"" from . I have a vague idea to use MVT, or Rolle's theorem, but I am seriously stuck. How can I use those conditions to prove injectivity of and that is open? Thanks in advance!","U \mathbb{R}^n f \colon U \to \mathbb{R}^n \lVert Df(\mathbf{x}) - I_n \rVert < 1 \mathbf{x} \in U I_n \colon \mathbb{R}^n \to \mathbb{R}^n f f(U) \lVert \cdot\rVert \lVert A \rVert = \max_{x\in S}\lVert Ax\rVert S f f(a) = f(b) a,b \in \mathbb{R}^n c(t) = ta + (1-t)b \lVert Df(\mathbf{x}) - I_n \rVert < 1 Df(\mathbf{x}) I_n f f(U)","['calculus', 'derivatives', 'vector-analysis']"
87,What is the maximum number of points at which an $n$-degree polynomial intersects $e^{x^2}$?,What is the maximum number of points at which an -degree polynomial intersects ?,n e^{x^2},"What is the maximum number $\phi(n)$ of roots of the function $e^{x^2}-p(x)$ , where $p(x)$ is an $n$ -degree polynomial. It is known that an $n$ -degree polynomial intersects the function $e^x$ at at most $n+1$ points ( Number of solutions of $P(x)=e^{ax}$ if $P$ is a polynomial ). It follows from the fact that the function $e^x$ has strictly positive $(n+1)$ -order derivative whilst $p^{(n+1)}\equiv 0$ , so $e^x-p(x)$ has strictly positive $(n+1)$ -order derivative. By inspecting the derivatives of the function $f(x)=e^{x^2}$ : \begin{align*}   f'(x) &= 2x e^{x^2}, \\   f''(x) &= 2(2x^2+1)e^{x^2} \\   f'''(x) &= 4x (2x^2+3) e^{x^2},\\  & \dots \end{align*} we can observe (and prove recursively) that all of the derivatives $f^{n}$ of $f$ are strictly positive for $n$ even; strictly positive on $(0,\infty)$ and strictly negative on $(-\infty,0)$ for $n$ even. We can therefore conclude that $f$ intersects any $n$ -degree polynomial $p$ at at most $n+1$ points whenever $n$ is odd. And considering interpolation (@GregMartin's comment) we have that: $\phi(n)=n+1$ for $n$ odd. However, using the same argument for $n$ even we can only conclude that $f-p$ has at most $n+1$ roots on $(0,\infty)$ , and likewise (due to symmetry about $y$ -axis) at most $n+1$ points on $(-\infty,0)$ . Considering that another intersection might occur at $x=0$ , we can be sure that $f$ intersects $p$ at at most $2n+3$ points. Is there a better estimate than $\phi(n)\leq 2n+3$ for $n$ even?","What is the maximum number of roots of the function , where is an -degree polynomial. It is known that an -degree polynomial intersects the function at at most points ( Number of solutions of $P(x)=e^{ax}$ if $P$ is a polynomial ). It follows from the fact that the function has strictly positive -order derivative whilst , so has strictly positive -order derivative. By inspecting the derivatives of the function : we can observe (and prove recursively) that all of the derivatives of are strictly positive for even; strictly positive on and strictly negative on for even. We can therefore conclude that intersects any -degree polynomial at at most points whenever is odd. And considering interpolation (@GregMartin's comment) we have that: for odd. However, using the same argument for even we can only conclude that has at most roots on , and likewise (due to symmetry about -axis) at most points on . Considering that another intersection might occur at , we can be sure that intersects at at most points. Is there a better estimate than for even?","\phi(n) e^{x^2}-p(x) p(x) n n e^x n+1 e^x (n+1) p^{(n+1)}\equiv 0 e^x-p(x) (n+1) f(x)=e^{x^2} \begin{align*}
  f'(x) &= 2x e^{x^2}, \\
  f''(x) &= 2(2x^2+1)e^{x^2} \\
  f'''(x) &= 4x (2x^2+3) e^{x^2},\\
 & \dots
\end{align*} f^{n} f n (0,\infty) (-\infty,0) n f n p n+1 n \phi(n)=n+1 n n f-p n+1 (0,\infty) y n+1 (-\infty,0) x=0 f p 2n+3 \phi(n)\leq 2n+3 n","['real-analysis', 'derivatives', 'exponential-function', 'roots']"
88,prove two identities regarding chebyshev polynomials and derivatives,prove two identities regarding chebyshev polynomials and derivatives,,"The source of this problem is putnam and beyond #198, so I tagged it under contest-math. Let $T_n(x)$ be the sequence of polynomials defined by $T_0(x) = 1, T_1(x)=x, T_n(x)=2xT_{n-1}(x)-T_{n-2}(x)$ for $n\ge 2.$ Let $U_n(x)$ be the sequence of polynomials defined by $U_0(x)=1, U_1(x)=2x, U_n(x)=2xU_{n-1}(x)-U_{n-2}(x)$ for $n\ge 2$ . For $n\ge 1,$ prove the following identities: $\dfrac{T_n(x)}{\sqrt{1-x^2}} = \dfrac{(-1)^n}{1\cdot 3\cdots (2n-1)}\dfrac{d^n}{dx^n}(1-x^2)^{n-1/2}$ $U_n(x) \sqrt{1-x^2} = \dfrac{(-1)^n(n+1)}{1\cdot 3\cdots (2n+1)} \dfrac{d^n}{dx^n} (1-x^2)^{n+1/2}$ It is known that $\cos (n\theta) = T_n(\cos \theta)$ for all $n\ge 0$ (e.g. by induction using the fact that $\cos(n\theta)$ is a polynomial of $\cos \theta$ and that the polynomial expressing $\cos (n\theta)$ as a polynomial of $\cos \theta$ and $T_n$ agree at infinitely many points) and $\dfrac{\sin ((n+1)\theta)}{\sin (\theta)} = U_n(x)$ for $n\ge 0$ . I think one possible approach is to show that the two expressions on each side of the first identity have the same recurrence relation and base case. When $n=1,$ equality holds in both cases. For the left-hand side (LHS), we have the recurrence $T_n(x)/\sqrt{1-x^2} = 2x\dfrac{T_{n-1}(x)} {\sqrt{1-x^2}} - \dfrac{T_{n-2}}{\sqrt{1-x^2}}.$ In other words, if $a_n$ denotes the LHS, then $a_n(x) = 2xa_{n-1}(x)-a_{n-2}(x).$ Then we have $\begin{align*}\dfrac{d^{n+1}}{dx^{n+1}} (1-x^2)^{n+1-1/2} &= \dfrac{d^n}{dx^n} (n+1-1/2)(1-x^2)^{n-1/2}(-2x)\\ &= \dfrac{d^n}{dx^n} -(2n+1) x (1-x^2)^{n-1/2}\\ &= \dfrac{d^{n-1}}{dx^{n-1}} (-(2n+1) (1-x^2)^{n-1/2} -(2n+1)x\dfrac{d}{dx}(1-x^2)^{n-1/2})\\ &= -(2n+1)\dfrac{d^{n-1}}{dx^{n-1}} (1-x^2)^{n-1/2} \end{align*}$ We need to show that if $t_n(x)$ denotes the right-hand side, then $t_n(x)= 2xt_{n-1}(x) - a_{n-2}(x).$ For the second identity, it could be useful to differentiate both sides and use the fact that if two functions have the same initial conditions and the same derivative, then they must be equal. We have for $|x|<1$ that $(1-x^2)^{n+1/2} = \sum_{i=0}^\infty {n+1/2\choose i} (-x^2)^i.$ Here, for a real number x and integer $j\ge 0$ , ${x\choose j} := \dfrac{x(x-1)\cdots (x-j+1)}{j!}.$ Let $u_n(x)$ denote the RHS of the second identity. Again it may be useful to prove that $u_n(x)$ satisfies the same recurrence relation as the LHS, but I'm not sure about the details for this. Note that $-xU_{n-1}(x) + 2(1-x^2) U'_{n-1}(x)=-nT_n(x),$ which can be shown by showing that both sides equal each other whenever $x=\cos \theta$ for some $\theta\in [0,2\pi)$ .","The source of this problem is putnam and beyond #198, so I tagged it under contest-math. Let be the sequence of polynomials defined by for Let be the sequence of polynomials defined by for . For prove the following identities: It is known that for all (e.g. by induction using the fact that is a polynomial of and that the polynomial expressing as a polynomial of and agree at infinitely many points) and for . I think one possible approach is to show that the two expressions on each side of the first identity have the same recurrence relation and base case. When equality holds in both cases. For the left-hand side (LHS), we have the recurrence In other words, if denotes the LHS, then Then we have We need to show that if denotes the right-hand side, then For the second identity, it could be useful to differentiate both sides and use the fact that if two functions have the same initial conditions and the same derivative, then they must be equal. We have for that Here, for a real number x and integer , Let denote the RHS of the second identity. Again it may be useful to prove that satisfies the same recurrence relation as the LHS, but I'm not sure about the details for this. Note that which can be shown by showing that both sides equal each other whenever for some .","T_n(x) T_0(x) = 1, T_1(x)=x, T_n(x)=2xT_{n-1}(x)-T_{n-2}(x) n\ge 2. U_n(x) U_0(x)=1, U_1(x)=2x, U_n(x)=2xU_{n-1}(x)-U_{n-2}(x) n\ge 2 n\ge 1, \dfrac{T_n(x)}{\sqrt{1-x^2}} = \dfrac{(-1)^n}{1\cdot 3\cdots (2n-1)}\dfrac{d^n}{dx^n}(1-x^2)^{n-1/2} U_n(x) \sqrt{1-x^2} = \dfrac{(-1)^n(n+1)}{1\cdot 3\cdots (2n+1)} \dfrac{d^n}{dx^n} (1-x^2)^{n+1/2} \cos (n\theta) = T_n(\cos \theta) n\ge 0 \cos(n\theta) \cos \theta \cos (n\theta) \cos \theta T_n \dfrac{\sin ((n+1)\theta)}{\sin (\theta)} = U_n(x) n\ge 0 n=1, T_n(x)/\sqrt{1-x^2} = 2x\dfrac{T_{n-1}(x)} {\sqrt{1-x^2}} - \dfrac{T_{n-2}}{\sqrt{1-x^2}}. a_n a_n(x) = 2xa_{n-1}(x)-a_{n-2}(x). \begin{align*}\dfrac{d^{n+1}}{dx^{n+1}} (1-x^2)^{n+1-1/2} &= \dfrac{d^n}{dx^n} (n+1-1/2)(1-x^2)^{n-1/2}(-2x)\\
&= \dfrac{d^n}{dx^n} -(2n+1) x (1-x^2)^{n-1/2}\\
&= \dfrac{d^{n-1}}{dx^{n-1}} (-(2n+1) (1-x^2)^{n-1/2} -(2n+1)x\dfrac{d}{dx}(1-x^2)^{n-1/2})\\
&= -(2n+1)\dfrac{d^{n-1}}{dx^{n-1}} (1-x^2)^{n-1/2}
\end{align*} t_n(x) t_n(x)= 2xt_{n-1}(x) - a_{n-2}(x). |x|<1 (1-x^2)^{n+1/2} = \sum_{i=0}^\infty {n+1/2\choose i} (-x^2)^i. j\ge 0 {x\choose j} := \dfrac{x(x-1)\cdots (x-j+1)}{j!}. u_n(x) u_n(x) -xU_{n-1}(x) + 2(1-x^2) U'_{n-1}(x)=-nT_n(x), x=\cos \theta \theta\in [0,2\pi)","['calculus', 'derivatives', 'polynomials', 'contest-math', 'recurrence-relations']"
89,Second derivative and stationary points equalling zero,Second derivative and stationary points equalling zero,,"If $f'(x_{s})=0$ (and $x_{s}$ is therefore a stationary point) is it always true that : $$f''(x_{s})\ne0\,\, \forall x_{s}$$ Is this true for every $f(x)$ and if so how could I go about proving it?",If (and is therefore a stationary point) is it always true that : Is this true for every and if so how could I go about proving it?,"f'(x_{s})=0 x_{s} f''(x_{s})\ne0\,\, \forall x_{s} f(x)","['calculus', 'derivatives']"
90,"Spivak, Ch. 20, Problem 15: Prove that if $x\leq 0$, then the remainder term $R_{n,0}$ for $e^x$ satisfies $|R_{n,0}|\leq \frac{|x|^{n+1}}{(n+1)!}$.","Spivak, Ch. 20, Problem 15: Prove that if , then the remainder term  for  satisfies .","x\leq 0 R_{n,0} e^x |R_{n,0}|\leq \frac{|x|^{n+1}}{(n+1)!}","The following is a problem from Chapter 20 of Spivak's Calculus Prove that if $x\leq 0$ , then the remainder term $R_{n,0}$ for $e^x$ satisfies $$|R_{n,0}|\leq \frac{|x|^{n+1}}{(n+1)!}$$ My question is about the solution in the solution manual, which I show below. First let me show my own attempt at solving this problem. If $x=0$ , both sides are $0$ . Assume $x\lt0$ . $$e^x=\sum\limits_{i=0}^n \frac{x^i}{i!}+\frac{e^t}{(n+1)!}x^{n+1}, \quad t\in (x,0)$$ We know that for $t<0$ we have $0<e^t<1$ . Thus, $$|R_{n,0,e^x}(x)|=\frac{e^t}{(n+1)!}|x|^{n+1}<\frac{|x|^{n+1}}{(n+1)!}$$ Is my attempt correct? When I looked at the solution manual, however, I was slightly bewildered. Here is what it has $$\left | \int_0^x \frac{e^t}{n!}(x-t)^n dt \right |= \int_x^0 \frac{e^t}{n!} |x-t|^n dt $$ $$\leq \int_x^0 \frac{|x-t|^n}{n!} dt, \text{since } e^x\leq 1 \text{ for } x\leq 0$$ $$=\frac{|x|^{n+1}}{(n+1)!}$$ Why is the solution manual using an integral, and what is the expression $\frac{e^t}{n!}(x-t)^n$ ? It looks like a remainder, but I don't understand the $t$ in the $x-t$ factor that is the same as the exponent in $e^t$ .","The following is a problem from Chapter 20 of Spivak's Calculus Prove that if , then the remainder term for satisfies My question is about the solution in the solution manual, which I show below. First let me show my own attempt at solving this problem. If , both sides are . Assume . We know that for we have . Thus, Is my attempt correct? When I looked at the solution manual, however, I was slightly bewildered. Here is what it has Why is the solution manual using an integral, and what is the expression ? It looks like a remainder, but I don't understand the in the factor that is the same as the exponent in .","x\leq 0 R_{n,0} e^x |R_{n,0}|\leq \frac{|x|^{n+1}}{(n+1)!} x=0 0 x\lt0 e^x=\sum\limits_{i=0}^n \frac{x^i}{i!}+\frac{e^t}{(n+1)!}x^{n+1}, \quad t\in (x,0) t<0 0<e^t<1 |R_{n,0,e^x}(x)|=\frac{e^t}{(n+1)!}|x|^{n+1}<\frac{|x|^{n+1}}{(n+1)!} \left | \int_0^x \frac{e^t}{n!}(x-t)^n dt \right |= \int_x^0 \frac{e^t}{n!} |x-t|^n dt  \leq \int_x^0 \frac{|x-t|^n}{n!} dt, \text{since } e^x\leq 1 \text{ for } x\leq 0 =\frac{|x|^{n+1}}{(n+1)!} \frac{e^t}{n!}(x-t)^n t x-t e^t","['calculus', 'integration', 'derivatives', 'proof-explanation', 'taylor-expansion']"
91,"an example of tangent equation calculation, from a book, that I do not understand","an example of tangent equation calculation, from a book, that I do not understand",,"I am in trouble with an example of an equation of a tangent from a book. Here's what my book is writing (in french) : I translate it (summarizing a bit) : take a T(X,Y) point on the tangent, the slope between M and T is $\frac{Y - f(x_{0})}{X - x_{0}}$ This slope is also the derived number on M, $f'(x_{0})$ : $\frac{Y - f(x_{0})}{X - x_{0}} = f'(x_{0})$ The relationship between the coordinates (X,Y) of T are thus $Y - f(x_{0}) = f'(x_{0}).(X - x_{0})$ But here is the sample given, that troubles me : What the equation of the tangent in $\mathbf{x_{0} = 2}$ to the parabole of equation $\mathbf{y = x^2}$ ? ( this drawing isn't from the author, it's mine, to figure what is $y = x^2$ , and what $x_{0} = 2$ or $x_{0} = 3$ would then mean ) On $x_{0} = 3, y_{0} = 9$ ; the derivative being $y' = 2x$ , we have $y_{0}' = 6$ First question : why does the author assign $x_{0} = 3$ if he said he is looking for $x_{0} = 2$ the line just before? reassigning an $x_{0}$ looks strange to me. According to 8.7, the equation to the tangent on $x_{0} = 2$ Here, $x_{0}$ returns to its previous assignment : $x_{0} = 2$ ... to the parabole is : $Y - 9 = 6(X - 3)$ or $Y = 6X - 9$ It's very troublesome. Especially because when I check with M(2,4), $y_{m} = 6x_{m} - 9$ with $x_{m}=2$ gives $y_{m} = 6 \times 2 - 9 = 12 - 9 = 3$ which is not on the curve, and I am supposed to be on the tangent equation. If with $y = x^2 $ M has for coordinates M(2,4), why this tangent equation is returning me a point of coordinates (2, 3) for it?","I am in trouble with an example of an equation of a tangent from a book. Here's what my book is writing (in french) : I translate it (summarizing a bit) : take a T(X,Y) point on the tangent, the slope between M and T is This slope is also the derived number on M, : The relationship between the coordinates (X,Y) of T are thus But here is the sample given, that troubles me : What the equation of the tangent in to the parabole of equation ? ( this drawing isn't from the author, it's mine, to figure what is , and what or would then mean ) On ; the derivative being , we have First question : why does the author assign if he said he is looking for the line just before? reassigning an looks strange to me. According to 8.7, the equation to the tangent on Here, returns to its previous assignment : ... to the parabole is : or It's very troublesome. Especially because when I check with M(2,4), with gives which is not on the curve, and I am supposed to be on the tangent equation. If with M has for coordinates M(2,4), why this tangent equation is returning me a point of coordinates (2, 3) for it?","\frac{Y - f(x_{0})}{X - x_{0}} f'(x_{0}) \frac{Y - f(x_{0})}{X - x_{0}} = f'(x_{0}) Y - f(x_{0}) = f'(x_{0}).(X - x_{0}) \mathbf{x_{0} = 2} \mathbf{y = x^2} y = x^2 x_{0} = 2 x_{0} = 3 x_{0} = 3, y_{0} = 9 y' = 2x y_{0}' = 6 x_{0} = 3 x_{0} = 2 x_{0} x_{0} = 2 x_{0} x_{0} = 2 Y - 9 = 6(X - 3) Y = 6X - 9 y_{m} = 6x_{m} - 9 x_{m}=2 y_{m} = 6 \times 2 - 9 = 12 - 9 = 3 y = x^2 ","['derivatives', 'tangent-line']"
92,Minimum Problem,Minimum Problem,,"Let $f: \mathbb{R} \rightarrow \mathbb{R}$ be given by $$f(m) = \frac{-1}{2}m^2 + \frac{1}{\beta}\int^m_{0} g^{-1}(s)ds.$$ where $\beta$ is a positive constant, $g: \mathbb{R} \rightarrow \mathbb{R}$ is a function which positive derivative. In particular, it is strictly increasing and globally lipschitz with $g(0) = 0$ , that is, there is $k_1 > 0$ such that $|g(x) - g(y)| \leq k_1|x - y|$ , $\forall \ x, y \in \mathbb{R}$ . In particular, $|g(x)| \leq k_1|x|$ , $\forall \ x \in \mathbb{R}$ . Assume $k_1\beta > 1$ . Affirmation: $f$ has the a global minimum. Attempt: I want to show initially that $f$ does have a minimum. If $f$ has a global minimum, then we must have $f'' > 0$ . Thus, deriving $f$ , we have $$f''(m) = -1 + \frac{1}{\beta}(g^{-1})'(m) = -1 + \frac{1}{\beta g'(m)}.$$ Now, since $g$ is globally Lipschitz, we must have $|g'(m)| \leq k_1$ , from which we conclude that $\frac{1}{\beta k_1} \leq \frac{1}{\beta g'(m)}$ . Thus, $$f''(m) = -1 + \frac{1}{\beta g'(m)} \geq -1 + \frac{1}{k_1 \beta}.$$ From here, we can't conclude anything, since $k_1\beta > 1$ , by hypothesis.","Let be given by where is a positive constant, is a function which positive derivative. In particular, it is strictly increasing and globally lipschitz with , that is, there is such that , . In particular, , . Assume . Affirmation: has the a global minimum. Attempt: I want to show initially that does have a minimum. If has a global minimum, then we must have . Thus, deriving , we have Now, since is globally Lipschitz, we must have , from which we conclude that . Thus, From here, we can't conclude anything, since , by hypothesis.","f: \mathbb{R} \rightarrow \mathbb{R} f(m) = \frac{-1}{2}m^2 + \frac{1}{\beta}\int^m_{0} g^{-1}(s)ds. \beta g: \mathbb{R} \rightarrow \mathbb{R} g(0) = 0 k_1 > 0 |g(x) - g(y)| \leq k_1|x - y| \forall \ x, y \in \mathbb{R} |g(x)| \leq k_1|x| \forall \ x \in \mathbb{R} k_1\beta > 1 f f f f'' > 0 f f''(m) = -1 + \frac{1}{\beta}(g^{-1})'(m) = -1 + \frac{1}{\beta g'(m)}. g |g'(m)| \leq k_1 \frac{1}{\beta k_1} \leq \frac{1}{\beta g'(m)} f''(m) = -1 + \frac{1}{\beta g'(m)} \geq -1 + \frac{1}{k_1 \beta}. k_1\beta > 1","['real-analysis', 'derivatives']"
93,"Differentiability of the function $ f(x, y) = | e ^x-y | (e^x-1) $",Differentiability of the function," f(x, y) = | e ^x-y | (e^x-1) ","Prove that the function $ f: \mathbb R^2 \to \mathbb R $ defined by the formula $ f(x, y) = | e ^x-y | (e^x-1) $ is differentiable at $  (a, b ) \in \mathbb R ^ 2 $ if and only if $ e ^ a \neq b $ or $ a = 0, b = 1 $ . I know that function is differentiable at $(x_0,y_0)$ if exist $Df(x_0,y_0)$ such that: $$\lim_{(h,k) \to (x_0,y_0)} \frac{f(x_0+h, y_0+k)-f(x_0,y_0)-h\cdot \frac{\partial f}{\partial x}(x_0,y_0)-k \cdot \frac{\partial f}{\partial y}(x_0,y_0)}{\sqrt{h^2+k^2}}=0$$ However, in my opinion, the formula of $f$ is too complicated to use this fact and I think that exist better solution.","Prove that the function defined by the formula is differentiable at if and only if or . I know that function is differentiable at if exist such that: However, in my opinion, the formula of is too complicated to use this fact and I think that exist better solution."," f: \mathbb R^2 \to \mathbb R   f(x, y) = | e ^x-y | (e^x-1)  
 (a, b ) \in \mathbb R ^ 2   e ^ a \neq b   a = 0, b = 1  (x_0,y_0) Df(x_0,y_0) \lim_{(h,k) \to (x_0,y_0)} \frac{f(x_0+h, y_0+k)-f(x_0,y_0)-h\cdot \frac{\partial f}{\partial x}(x_0,y_0)-k \cdot \frac{\partial f}{\partial y}(x_0,y_0)}{\sqrt{h^2+k^2}}=0 f",['derivatives']
94,Nature of stationary points of reciprocal functions,Nature of stationary points of reciprocal functions,,I have solved a problem but I'm not sure if the proof is right. The problem is: Show that if the curve $y=f(x)$ has a maximum stationary point at $x=a$ then the curve $y=\frac{1}{f(x)}$ has a minimum stationary point at $x=a$ where $f(a)\neq 0$ Here is what I've done: I know that $f'(a)=0$ and $f''(a)<0$ and $$\begin{equation}[\frac{1}{f(x)}]'' = -\frac{f(x)^2f''(x)-2f(x)f'(x)^2}{f(x)^4}\end{equation}$$ assuming $y=\frac{1}{f(x)}$ has a minimum stationary point at x=a then $$\begin{equation}-\frac{f(a)^2f''(a)-2f(a)f'(a)^2}{f(a)^4}>0\end{equation}$$ which leads to $$\begin{equation}f(a)^2f''(a)-2f(a)f'(a)^2<0\end{equation}$$ because $f'(a)=0$ we get $$\begin{equation}f(a)^2f''(a)<0\end{equation}$$ because $f(a)\neq 0$ we get $$\begin{equation}f''(a)<0\end{equation}$$ which is true and therefore $y=\frac{1}{f(x)}$ does have a minimum stationary point at $x=a$ . My question is why would this show that $y=\frac{1}{f(x)}$ does has a minimum stationary point at $x=a$ just because I assumed that it's true and deduced something true from it?,I have solved a problem but I'm not sure if the proof is right. The problem is: Show that if the curve has a maximum stationary point at then the curve has a minimum stationary point at where Here is what I've done: I know that and and assuming has a minimum stationary point at x=a then which leads to because we get because we get which is true and therefore does have a minimum stationary point at . My question is why would this show that does has a minimum stationary point at just because I assumed that it's true and deduced something true from it?,y=f(x) x=a y=\frac{1}{f(x)} x=a f(a)\neq 0 f'(a)=0 f''(a)<0 \begin{equation}[\frac{1}{f(x)}]'' = -\frac{f(x)^2f''(x)-2f(x)f'(x)^2}{f(x)^4}\end{equation} y=\frac{1}{f(x)} \begin{equation}-\frac{f(a)^2f''(a)-2f(a)f'(a)^2}{f(a)^4}>0\end{equation} \begin{equation}f(a)^2f''(a)-2f(a)f'(a)^2<0\end{equation} f'(a)=0 \begin{equation}f(a)^2f''(a)<0\end{equation} f(a)\neq 0 \begin{equation}f''(a)<0\end{equation} y=\frac{1}{f(x)} x=a y=\frac{1}{f(x)} x=a,"['calculus', 'derivatives', 'solution-verification']"
95,A closed-form for higher-order derivatives of the Dawson integral?,A closed-form for higher-order derivatives of the Dawson integral?,,"We have as the Dawson integral $$ \mathcal D(x):=e^{-x^2}\int_0^xe^{t^2}\,\mathrm dt. $$ I am interested in an expression for $\mathcal D^{(n)}(x)$ that does not involve sums.  For example, we could use the relation $$ \mathcal D(x)=\tfrac{1}{2}\sqrt\pi e^{-x^2}\operatorname{erfi}(x) $$ to write $$ \mathcal D^{(n)}(x)=\tfrac{1}{2}\sqrt\pi \sum_{k=0}^n\binom{n}{k} (\partial_x^ke^{-x^2})(\partial_x^{n-k}\operatorname{erfi}(x)) $$ and each of the remaining higher-order derivatives have closed-forms in terms of hypergeometric functions and hermite polynomials.  But can we write $\mathcal D^{(n)}(x)$ without sums?  My thought was to find a hypergeometric form for $\mathcal D(x)$ and then apply the well-known formulae for derivatives of hypergeometric functions to obtain the final result; however, I have not been able to find such a form of the Dawson integral. Thoughts?","We have as the Dawson integral I am interested in an expression for that does not involve sums.  For example, we could use the relation to write and each of the remaining higher-order derivatives have closed-forms in terms of hypergeometric functions and hermite polynomials.  But can we write without sums?  My thought was to find a hypergeometric form for and then apply the well-known formulae for derivatives of hypergeometric functions to obtain the final result; however, I have not been able to find such a form of the Dawson integral. Thoughts?","
\mathcal D(x):=e^{-x^2}\int_0^xe^{t^2}\,\mathrm dt.
 \mathcal D^{(n)}(x) 
\mathcal D(x)=\tfrac{1}{2}\sqrt\pi e^{-x^2}\operatorname{erfi}(x)
 
\mathcal D^{(n)}(x)=\tfrac{1}{2}\sqrt\pi \sum_{k=0}^n\binom{n}{k} (\partial_x^ke^{-x^2})(\partial_x^{n-k}\operatorname{erfi}(x))
 \mathcal D^{(n)}(x) \mathcal D(x)","['derivatives', 'special-functions', 'hypergeometric-function', 'hermite-polynomials']"
96,"What is meant by ""the image of an interval is also an interval"" when this phrase is used to describe the intermediate value property?","What is meant by ""the image of an interval is also an interval"" when this phrase is used to describe the intermediate value property?",,"Wikipedia says of the Intermediate Value Theorem that In mathematical analysis, the intermediate value theorem states that if $f$ is a continuous function whose domain contains the interval $[a, b]$ , then it takes on any given value between $f(a)$ and $f(b)$ at some point within the interval. Of Darboux's Theorem it says It states that every function that results from the differentiation of another function has the intermediate value property: the image of an interval is also an interval. What is meant by ""the image of an interval is also an interval""?","Wikipedia says of the Intermediate Value Theorem that In mathematical analysis, the intermediate value theorem states that if is a continuous function whose domain contains the interval , then it takes on any given value between and at some point within the interval. Of Darboux's Theorem it says It states that every function that results from the differentiation of another function has the intermediate value property: the image of an interval is also an interval. What is meant by ""the image of an interval is also an interval""?","f [a, b] f(a) f(b)","['calculus', 'derivatives']"
97,"Spivak's Calculus, Ch. 11, ""Significance of the Derivative"", prob. 58: Prove $f'$ increasing then every tangent line intersects graph of $f$ only once","Spivak's Calculus, Ch. 11, ""Significance of the Derivative"", prob. 58: Prove  increasing then every tangent line intersects graph of  only once",f' f,"The following is a problem from ch. 10, ""Significance of the Derivative"", from Spivak's Calculus Prove that if $f'$ is increasing, then every tangent line of $f$ intersects the graph of $f$ only once. My proof seems at first glance to be the same as the solution manual's proof, but I don't understand the last step in the terse solution manual proof. Here is my proof $f'$ increasing means $\forall x \forall y, x<y \implies f'(x)<f'(y)$ . Consider a point $(x_0, f(x_0))$ on the graph of $f$ . The tangent line at this point has slope $f'(x_0)$ . Assume this line intersects the graph of $f$ at $(x,f(x))$ . Then $f'(x_0)=\frac{f(x)-f(x_0)}{x-x_0}$ . By the Mean Value Theorem we know that $$\exists c, c \in (x_0, x) \land f'(c)=\frac{f(x)-f(x_0)}{x-x_0}$$ $$\implies f'(x_0)=f'(c)$$ $$\bot$$ The contradiction occurs because $f'$ is increasing by assumption. Therefore, by proof by contradiction, we conclude that the tangent line line at any point does not intersect the graph of $f$ at any other point. Here is the proof from the solution manual The tangent line through $(a,f(a))$ is the graph of $$g(x)=f'(a)(x-a)+f(a)$$ $$=f'(a)x+f(a)-af'(a)$$ If $g(x_0)=f(x_0)$ for some $x_0 \neq a$ , then $$0=g'(x)-f'(x)=f'(a)-f'(x), \text{ for some } x \text{ in } (a,x_0) \text{ or } (x_0,a)\tag{1}$$ This is impossible, since $f'$ is increasing. Where does $(1)$ come from?","The following is a problem from ch. 10, ""Significance of the Derivative"", from Spivak's Calculus Prove that if is increasing, then every tangent line of intersects the graph of only once. My proof seems at first glance to be the same as the solution manual's proof, but I don't understand the last step in the terse solution manual proof. Here is my proof increasing means . Consider a point on the graph of . The tangent line at this point has slope . Assume this line intersects the graph of at . Then . By the Mean Value Theorem we know that The contradiction occurs because is increasing by assumption. Therefore, by proof by contradiction, we conclude that the tangent line line at any point does not intersect the graph of at any other point. Here is the proof from the solution manual The tangent line through is the graph of If for some , then This is impossible, since is increasing. Where does come from?","f' f f f' \forall x \forall y, x<y \implies f'(x)<f'(y) (x_0, f(x_0)) f f'(x_0) f (x,f(x)) f'(x_0)=\frac{f(x)-f(x_0)}{x-x_0} \exists c, c \in (x_0, x) \land f'(c)=\frac{f(x)-f(x_0)}{x-x_0} \implies f'(x_0)=f'(c) \bot f' f (a,f(a)) g(x)=f'(a)(x-a)+f(a) =f'(a)x+f(a)-af'(a) g(x_0)=f(x_0) x_0 \neq a 0=g'(x)-f'(x)=f'(a)-f'(x), \text{ for some } x \text{ in } (a,x_0) \text{ or } (x_0,a)\tag{1} f' (1)","['calculus', 'derivatives', 'proof-explanation']"
98,"Using the definition of a derivative, solve $\lim_{x \to 0} \frac{(2+h)^{3+h} - 8}{h}$","Using the definition of a derivative, solve",\lim_{x \to 0} \frac{(2+h)^{3+h} - 8}{h},"I can't figure out where to even start, I have looked up the answer on Desmos but it uses L'Hopitals rule which i haven't learned yet. $$\lim_{h \to 0} \frac{(2+h)^{3+h} - 8}{h}$$ I see that i can use the log rules to rewrite it as $$\lim_{h \to 0} \frac{e^{(3+h)\ln(2+h)} - 8}{h}$$ but that only confuses me more.","I can't figure out where to even start, I have looked up the answer on Desmos but it uses L'Hopitals rule which i haven't learned yet. I see that i can use the log rules to rewrite it as but that only confuses me more.",\lim_{h \to 0} \frac{(2+h)^{3+h} - 8}{h} \lim_{h \to 0} \frac{e^{(3+h)\ln(2+h)} - 8}{h},"['derivatives', 'logarithms', 'limits-without-lhopital']"
99,Negative Tetrations?,Negative Tetrations?,,"To start, I'll say that for this post I'll be using Rudy Rucker notation for tetration. That being $^2$ x= $x^x%$ , which means the number raised to the left means how many times one would exponentiate x. Ok, so now to see my logic I'll start with a pattern: $^4x$ = $x^{x^{x^x}}$ = $x^{x^{x^{x^1}}}$ $^3x$ = $x^{x^{x}}$ = $x^{x^{x^1}}$ $^2x$ = $x^x$ = $x^{x^1}$ $^1x$ = $x$ = $x^1$ Take note how for each tetration we go down by one, the previous exponent just reverts to a one, so this pattern must be true for the $0$ th tetration of x. $^0x$ = $1$ = $1^1$ Now, my question, what about for $^{-1}x$ . I didn't even know if this was possible, after jut a quick scour on the internet I ended up with zilch. So I began looking for a nice formula to help. Then I came across this nice formula: $\frac{d}{dx}$ [ $^nx$ ]=( $^nx$ ) $\left(\frac{^{n-1}x}{x}+\frac{d}{dx}[^{n-1}x]ln(x)\right)$ Assuming n=0, and plugging in, we know from before that $^0x$ =1 therefore $\frac{d}{dx}$ [ $^0x$ ]= $0$ , one returns that: $0$ = $^{(n-1)}x$ + $\frac{d}{dx}$ [ $^{(n-1)}x$ ] * ln $(x)$ * $x$ To solve this problem I just substituted $u=^{(n-1)}x$ . So my questions arise, Am I allowed to make this substitution? Even if I do, do I change the variable of differentiation? If this method I am using is not viable, is there some other method that I can use, or is it truly impossible and I just wasted a few days of my life?","To start, I'll say that for this post I'll be using Rudy Rucker notation for tetration. That being x= , which means the number raised to the left means how many times one would exponentiate x. Ok, so now to see my logic I'll start with a pattern: = = = = = = = = Take note how for each tetration we go down by one, the previous exponent just reverts to a one, so this pattern must be true for the th tetration of x. = = Now, my question, what about for . I didn't even know if this was possible, after jut a quick scour on the internet I ended up with zilch. So I began looking for a nice formula to help. Then I came across this nice formula: [ ]=( ) Assuming n=0, and plugging in, we know from before that =1 therefore [ ]= , one returns that: = + [ ] * ln * To solve this problem I just substituted . So my questions arise, Am I allowed to make this substitution? Even if I do, do I change the variable of differentiation? If this method I am using is not viable, is there some other method that I can use, or is it truly impossible and I just wasted a few days of my life?",^2 x^x% ^4x x^{x^{x^x}} x^{x^{x^{x^1}}} ^3x x^{x^{x}} x^{x^{x^1}} ^2x x^x x^{x^1} ^1x x x^1 0 ^0x 1 1^1 ^{-1}x \frac{d}{dx} ^nx ^nx \left(\frac{^{n-1}x}{x}+\frac{d}{dx}[^{n-1}x]ln(x)\right) ^0x \frac{d}{dx} ^0x 0 0 ^{(n-1)}x \frac{d}{dx} ^{(n-1)}x (x) x u=^{(n-1)}x,"['calculus', 'derivatives', 'tetration']"
