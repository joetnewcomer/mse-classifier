,title_raw,title_text,title_latex,body_raw,body_text,body_latex,tags
0,"""Functions"" that always equal zero and their gradients","""Functions"" that always equal zero and their gradients",,"I am currently enrolled in a university level Multivariable Calculus course in which my professor taught finding the tangent plane at a point on a surface the following way: To find the tangent plane at point $(x_0,y_0,z_0)$ on the surface $ z=x^2+y^2 $ set $f(x,y,z)=x^2+y^2-z=0$ and then find $\nabla f(x_0,y_0,z_0) $. According to his method, $\nabla f $ would be the following... $$ \nabla f =  \begin{bmatrix} \frac{\partial f}{\partial x} \\ \frac{\partial f}{\partial y} \\ \frac{\partial f}{\partial z} \end{bmatrix} = \begin{bmatrix}  2x \\ 2y \\ -1 \end{bmatrix}   $$ While this indeed is an expression for the tangent plane's normal vector, I don't understand how this is a mathematically valid statement. If $f(x,y,z)=0$, than how can the rates of instantaneous change (i.e.$\frac{\partial f}{\partial x}$) ever be anything other than zero? Is $f(x,y,z)=x^2+y^2-z=0$ a valid function? Why would the partials not always be equal to zero? Am I thinking about partial derivatives incorrectly? Is this just a mathematical ""hack"" to serve as a shortcut? If this is the case, is there a more mathematically rigorous method to find the tangent plane of a surface? Thank you in advance.","I am currently enrolled in a university level Multivariable Calculus course in which my professor taught finding the tangent plane at a point on a surface the following way: To find the tangent plane at point $(x_0,y_0,z_0)$ on the surface $ z=x^2+y^2 $ set $f(x,y,z)=x^2+y^2-z=0$ and then find $\nabla f(x_0,y_0,z_0) $. According to his method, $\nabla f $ would be the following... $$ \nabla f =  \begin{bmatrix} \frac{\partial f}{\partial x} \\ \frac{\partial f}{\partial y} \\ \frac{\partial f}{\partial z} \end{bmatrix} = \begin{bmatrix}  2x \\ 2y \\ -1 \end{bmatrix}   $$ While this indeed is an expression for the tangent plane's normal vector, I don't understand how this is a mathematically valid statement. If $f(x,y,z)=0$, than how can the rates of instantaneous change (i.e.$\frac{\partial f}{\partial x}$) ever be anything other than zero? Is $f(x,y,z)=x^2+y^2-z=0$ a valid function? Why would the partials not always be equal to zero? Am I thinking about partial derivatives incorrectly? Is this just a mathematical ""hack"" to serve as a shortcut? If this is the case, is there a more mathematically rigorous method to find the tangent plane of a surface? Thank you in advance.",,"['functions', 'multivariable-calculus', 'partial-derivative']"
1,Are all linear maps differentiable?,Are all linear maps differentiable?,,"Consider a linear map from $\Bbb R^n \rightarrow \Bbb R^m$. Question 1. How can we show that all such linear maps are differentiable without loss of generality? Question 2. Is it possible that even though a function is not continuous at a point, it can be differentiable at that point ? Say for example, consider a linear map $f: \Bbb R^2 \rightarrow \Bbb R$ $f(x,y) = \begin{cases} \frac{xy}{x^2 + y^2}  & (x,y) \neq (0,0) \\[2ex] 0 & (x,y) = (0,0) \end{cases}$ is not continuous at $(0,0)$ but is differentiable at $(0,0)$.","Consider a linear map from $\Bbb R^n \rightarrow \Bbb R^m$. Question 1. How can we show that all such linear maps are differentiable without loss of generality? Question 2. Is it possible that even though a function is not continuous at a point, it can be differentiable at that point ? Say for example, consider a linear map $f: \Bbb R^2 \rightarrow \Bbb R$ $f(x,y) = \begin{cases} \frac{xy}{x^2 + y^2}  & (x,y) \neq (0,0) \\[2ex] 0 & (x,y) = (0,0) \end{cases}$ is not continuous at $(0,0)$ but is differentiable at $(0,0)$.",,"['multivariable-calculus', 'derivatives', 'linear-transformations']"
2,Formula of this map (Escher's work),Formula of this map (Escher's work),,"As you know, the picture below is one of M.C. Escher's works. I have thought a lot about what would be the rule of the function drawn on the surface of sphere? Is it a $\mathbb R\rightarrow\mathbb R^3$ or $\mathbb R^3\rightarrow\mathbb R^3$ function? Honestly, the idea that he drew it is new now and I don’t think his works be outdated ever. Thanks for sharing your knowledge about this great picture with me. :)","As you know, the picture below is one of M.C. Escher's works. I have thought a lot about what would be the rule of the function drawn on the surface of sphere? Is it a $\mathbb R\rightarrow\mathbb R^3$ or $\mathbb R^3\rightarrow\mathbb R^3$ function? Honestly, the idea that he drew it is new now and I don’t think his works be outdated ever. Thanks for sharing your knowledge about this great picture with me. :)",,"['multivariable-calculus', 'math-software', 'mathematica']"
3,"Showing $\frac{x+y+z}{3} \ge \sqrt[3]{xyz}$ for $x,y,z \ge 0$",Showing  for,"\frac{x+y+z}{3} \ge \sqrt[3]{xyz} x,y,z \ge 0","How do I show that $\frac{x+y+z}{3} \ge \sqrt[3]{xyz}$ for $x,y,z \ge 0$? The answer is to: Let $A = x+y+z$, then $z=A-x-y$. Maximize $f(x,y)=xy(A-x-y)$ on region enclosed by $x=y=0$ and $x+y=A$ What I don't understand is the idea behind this method. Why $A=x+y+z$ not $A=\frac{x+y+z}{3}$? I guess $f(x,y)=xy(A-x-y)$ is to see whats the maximum of the equation But why regions $x=y=0$ and $x+y=A$? Sorry this seems like an easy question but somehow I don't get it ...","How do I show that $\frac{x+y+z}{3} \ge \sqrt[3]{xyz}$ for $x,y,z \ge 0$? The answer is to: Let $A = x+y+z$, then $z=A-x-y$. Maximize $f(x,y)=xy(A-x-y)$ on region enclosed by $x=y=0$ and $x+y=A$ What I don't understand is the idea behind this method. Why $A=x+y+z$ not $A=\frac{x+y+z}{3}$? I guess $f(x,y)=xy(A-x-y)$ is to see whats the maximum of the equation But why regions $x=y=0$ and $x+y=A$? Sorry this seems like an easy question but somehow I don't get it ...",,"['multivariable-calculus', 'optimization']"
4,An application of partitions of unity: integrating over open sets.,An application of partitions of unity: integrating over open sets.,,"In Spivak's ""Calculus on Manifolds"", Spivak first defines integration over rectangles, then bounded Jordan-measurable sets (for functions whose discontinuities form a Lebesgue null set). He then uses partitions of unity to define integration over arbitrary open sets (top of p.65). Used in the proof of this assertion is the claim that if: i) $\Phi$ is subordinate to an admissible cover $J$ of our open set $A$. ii) $f:A\rightarrow \mathbb{R}$ is locally bounded in $A$. iii) The set of discontinuities of $f$ is Lebesgue-null. then each $\int_A \phi\cdot|f|$ exists. I cannot see how this statement makes sense, seeing as the integral is thus far only defined for bounded Jordan-measurable sets. Perhaps I am missing something simple here?","In Spivak's ""Calculus on Manifolds"", Spivak first defines integration over rectangles, then bounded Jordan-measurable sets (for functions whose discontinuities form a Lebesgue null set). He then uses partitions of unity to define integration over arbitrary open sets (top of p.65). Used in the proof of this assertion is the claim that if: i) $\Phi$ is subordinate to an admissible cover $J$ of our open set $A$. ii) $f:A\rightarrow \mathbb{R}$ is locally bounded in $A$. iii) The set of discontinuities of $f$ is Lebesgue-null. then each $\int_A \phi\cdot|f|$ exists. I cannot see how this statement makes sense, seeing as the integral is thus far only defined for bounded Jordan-measurable sets. Perhaps I am missing something simple here?",,"['calculus', 'differential-geometry', 'multivariable-calculus']"
5,"Does existence of partial derivatives implies continuity at a point $(x_0,y_0)$?",Does existence of partial derivatives implies continuity at a point ?,"(x_0,y_0)","If $F: \mathbb R^2 \to \mathbb R$ and $F_x$ (partial derivative of $F$ wrt $x$) and $F_y$ exist at $(x_0,y_0)$ then the function is continuous at that point. Is this true? If not what could be a counter-example?","If $F: \mathbb R^2 \to \mathbb R$ and $F_x$ (partial derivative of $F$ wrt $x$) and $F_y$ exist at $(x_0,y_0)$ then the function is continuous at that point. Is this true? If not what could be a counter-example?",,"['calculus', 'multivariable-calculus']"
6,How to simplify expressions with del (or nabla) in them?,How to simplify expressions with del (or nabla) in them?,,"I always find it difficult to simplify expressions or open brackets in expressions that have a 'Del' (or 'Nabla') in them. For example, how would one go about simplifying this expression?: $$\nabla\boldsymbol{\cdot}(\phi\nabla\psi)$$ ( $\phi$ and $\psi$ are both scalar fields) I need it to become: $$[\phi\nabla^2\psi + (\nabla\phi)\boldsymbol{\cdot}(\nabla\psi)]$$ I would also love to know how to simplify those standard equations mentioned in Griffiths (for example - the expansion of the 'curl of the curl' of a vector field) The only method I know is to find out every single term in the expression (in terms of $a_x$ , $a_y$ etc.) and then cancel out the terms and then find patterns and regroup the terms in the remaining expression Is there a faster way to approach these 'simplify' (or 'expand') problems? Maybe there are some tricks or formulas that I am unaware of (maybe something analogous to the uv-rule for differentiating the product of two functions in simple calculus) $$\frac{d}{dx}[u(x)v(x)] = u'(x)v(x) + u(x)v'(x)$$ I understand that the uv-rule seems to work on my original expression. But I would still love some sort of formalization. The problem I have is that, in simple calculus, multiplying two functions does not have two meanings. With Nabla however, I have two choices - Dot product and Cross product . And I also have three choices for differentiation - Gradient, Divergence and Curl To explain my concern better, try answering what would have been the simplification if the original expression was - $$\nabla \times (\phi\nabla\psi)$$ or maybe $$\nabla(v\boldsymbol{\cdot}\nabla\psi)$$ where $v$ is a vector-field For the analogy, these three questions become the same question - "" Differentiation of something multiplied by the differential of something else ""","I always find it difficult to simplify expressions or open brackets in expressions that have a 'Del' (or 'Nabla') in them. For example, how would one go about simplifying this expression?: ( and are both scalar fields) I need it to become: I would also love to know how to simplify those standard equations mentioned in Griffiths (for example - the expansion of the 'curl of the curl' of a vector field) The only method I know is to find out every single term in the expression (in terms of , etc.) and then cancel out the terms and then find patterns and regroup the terms in the remaining expression Is there a faster way to approach these 'simplify' (or 'expand') problems? Maybe there are some tricks or formulas that I am unaware of (maybe something analogous to the uv-rule for differentiating the product of two functions in simple calculus) I understand that the uv-rule seems to work on my original expression. But I would still love some sort of formalization. The problem I have is that, in simple calculus, multiplying two functions does not have two meanings. With Nabla however, I have two choices - Dot product and Cross product . And I also have three choices for differentiation - Gradient, Divergence and Curl To explain my concern better, try answering what would have been the simplification if the original expression was - or maybe where is a vector-field For the analogy, these three questions become the same question - "" Differentiation of something multiplied by the differential of something else """,\nabla\boldsymbol{\cdot}(\phi\nabla\psi) \phi \psi [\phi\nabla^2\psi + (\nabla\phi)\boldsymbol{\cdot}(\nabla\psi)] a_x a_y \frac{d}{dx}[u(x)v(x)] = u'(x)v(x) + u(x)v'(x) \nabla \times (\phi\nabla\psi) \nabla(v\boldsymbol{\cdot}\nabla\psi) v,"['multivariable-calculus', 'vector-analysis', 'divergence-operator']"
7,Does there exist a gradient chain rule for this case?,Does there exist a gradient chain rule for this case?,,"My question comes from this article in Wikipedia. I noticed that there is a chain rule defined for the composition of $f:\mathbb{R}\to\mathbb{R}$ and $ g: \mathbb{R}^n \to \mathbb{R}$ given by $$ \nabla (f \circ g) = (f' \circ g) \nabla g \tag{1} $$ My question is if instead we had some functions $f: \mathbb{R}^m \to \mathbb{R}$ and $g: \mathbb{R}^n \to \mathbb{R}^m$ such that $(f \circ g): \mathbb{R}^n \to \mathbb{R}$ , does there exist an expression for $\nabla (f \circ g)$ similar to equation $(1)$ ? I tried looking for any resource who answered this but had no luck. If someone could point me in the right direction I would greatly appreciate it. Thank you!","My question comes from this article in Wikipedia. I noticed that there is a chain rule defined for the composition of and given by My question is if instead we had some functions and such that , does there exist an expression for similar to equation ? I tried looking for any resource who answered this but had no luck. If someone could point me in the right direction I would greatly appreciate it. Thank you!","f:\mathbb{R}\to\mathbb{R}  g: \mathbb{R}^n \to \mathbb{R} 
\nabla (f \circ g) = (f' \circ g) \nabla g \tag{1}
 f: \mathbb{R}^m \to \mathbb{R} g: \mathbb{R}^n \to \mathbb{R}^m (f \circ g): \mathbb{R}^n \to \mathbb{R} \nabla (f \circ g) (1)","['multivariable-calculus', 'partial-derivative', 'chain-rule']"
8,Partial derivatives zero implies function constant,Partial derivatives zero implies function constant,,"Let $f:\mathbf{R}^k\to \mathbf{R}$ be a function such that all partial derivatives exists on all of $\mathbf{R}^k$ . If $D_i f(\vec{x})=0$ for all $\vec{x}\in\mathbf{R}^k$ and all $i$ , show that there exists $c\in\mathbf{R}$ such that $f(\vec{x})=c$ for all $\vec{x}$ . Let $\vec{x}=(x_1,\ldots,x_k)\in\mathbf{R}^k$ . Define for all $1\leqslant i\leqslant k$ , $g_i:\mathbf{R}\to\mathbf{R}$ by $g_i(t)=f(x_1,\ldots,t,\ldots,x_k)$ with $t$ on the $i$ -th place. Then $g'(x_i)=D_i f(\vec{x})=0$ . Since $\vec{x}$ and thus $x_i$ is arbitrary, this implies that $g_i \equiv c_i\in\mathbf{R}$ according to some theorem in my textbook. The thing is, how do I now use this information to conclude that $f$ itself is constant?","Let be a function such that all partial derivatives exists on all of . If for all and all , show that there exists such that for all . Let . Define for all , by with on the -th place. Then . Since and thus is arbitrary, this implies that according to some theorem in my textbook. The thing is, how do I now use this information to conclude that itself is constant?","f:\mathbf{R}^k\to \mathbf{R} \mathbf{R}^k D_i f(\vec{x})=0 \vec{x}\in\mathbf{R}^k i c\in\mathbf{R} f(\vec{x})=c \vec{x} \vec{x}=(x_1,\ldots,x_k)\in\mathbf{R}^k 1\leqslant i\leqslant k g_i:\mathbf{R}\to\mathbf{R} g_i(t)=f(x_1,\ldots,t,\ldots,x_k) t i g'(x_i)=D_i f(\vec{x})=0 \vec{x} x_i g_i \equiv c_i\in\mathbf{R} f",['multivariable-calculus']
9,What does norm on gradient (bounded) of $f$ imply on the hessian of $f$ [duplicate],What does norm on gradient (bounded) of  imply on the hessian of  [duplicate],f f,"This question already has answers here : Lipschitz-constant gradient implies bounded eigenvalues on Hessian (3 answers) Closed 2 years ago . Given a $C^2$ function $f$ which has a Lipschitz gradient, that is, $ || \nabla f(x) - \nabla f(y) ||_2 \leq L || x - y ||_2$ Can we prove a bound on the largest eigenvalue of $\nabla^2 f$ ? What does it mean to say - $ \lim_{x \rightarrow y} \frac{|| \nabla f(x) - \nabla f(y) ||_2}{|| x - y ||_2} \leq L $","This question already has answers here : Lipschitz-constant gradient implies bounded eigenvalues on Hessian (3 answers) Closed 2 years ago . Given a $C^2$ function $f$ which has a Lipschitz gradient, that is, $ || \nabla f(x) - \nabla f(y) ||_2 \leq L || x - y ||_2$ Can we prove a bound on the largest eigenvalue of $\nabla^2 f$ ? What does it mean to say - $ \lim_{x \rightarrow y} \frac{|| \nabla f(x) - \nabla f(y) ||_2}{|| x - y ||_2} \leq L $",,"['multivariable-calculus', 'derivatives', 'lipschitz-functions']"
10,Deriving the Hessian from the limit definition of the derivative,Deriving the Hessian from the limit definition of the derivative,,"Could someone possibly help me understand how I can derive the Hessian matrix of a twice-differentiable function $f$ defined on $\mathbb{R}^n$ using the limit definition of the second derivative. Namely, how does: $\lim_{h -> 0}\frac{\nabla f(x+h) - \nabla f(x)}{h}$ result in the Hessian $\nabla^2 f(x)$. If I happen to be wrong about this, could you please point out what I am misunderstanding? Thank you very much!","Could someone possibly help me understand how I can derive the Hessian matrix of a twice-differentiable function $f$ defined on $\mathbb{R}^n$ using the limit definition of the second derivative. Namely, how does: $\lim_{h -> 0}\frac{\nabla f(x+h) - \nabla f(x)}{h}$ result in the Hessian $\nabla^2 f(x)$. If I happen to be wrong about this, could you please point out what I am misunderstanding? Thank you very much!",,"['calculus', 'multivariable-calculus']"
11,"Calculate $\int^{1/2}_0\int^{1-x}_x (x+y)^9(x-y)^9 \, dy \, dx$",Calculate,"\int^{1/2}_0\int^{1-x}_x (x+y)^9(x-y)^9 \, dy \, dx","How can I find the following integral: $$\int^{1/2}_0 \int^{1-x}_x (x+y)^9(x-y)^9 \, dy \, dx $$ My thoughts: Can we possibly convert this to spherical or use change of variables?","How can I find the following integral: $$\int^{1/2}_0 \int^{1-x}_x (x+y)^9(x-y)^9 \, dy \, dx $$ My thoughts: Can we possibly convert this to spherical or use change of variables?",,['multivariable-calculus']
12,About the double integral.,About the double integral.,,"In my text book it says that the volume between the some region $R$ in the $xy$ plane and the surface $z=f(x,y)$ can be found by calculating $$\iint_D f(x,y)~dxdy$$ yet in the next page it uses this formula to calculate an area in the $xy$ plane not a volume under a surface why is this the case I don't see how they go from talking about volume to talking about area. It also says the volume can be calculated as follows: $$\iiint_S dxdydz $$ but I don't understand why this is the case also I thought we needed $z=f(x,y)$ to calculate the volume not $\omega=f(x,y,z)$ as surely this would be some other quantity in $4$ dimensions, not a volume? Please help me clear this up thanks.","In my text book it says that the volume between the some region $R$ in the $xy$ plane and the surface $z=f(x,y)$ can be found by calculating $$\iint_D f(x,y)~dxdy$$ yet in the next page it uses this formula to calculate an area in the $xy$ plane not a volume under a surface why is this the case I don't see how they go from talking about volume to talking about area. It also says the volume can be calculated as follows: $$\iiint_S dxdydz $$ but I don't understand why this is the case also I thought we needed $z=f(x,y)$ to calculate the volume not $\omega=f(x,y,z)$ as surely this would be some other quantity in $4$ dimensions, not a volume? Please help me clear this up thanks.",,['multivariable-calculus']
13,Multiple variables calculus: condition for $f$ to be continuous using curves,Multiple variables calculus: condition for  to be continuous using curves,f,"Prove $f:\mathbb{R}^n\to\mathbb{R}$ is continuous iff for every curve, $\gamma:[a,b]\to\mathbb{R}^n: f\circ \gamma :\mathbb{R}\to\mathbb{R}$ is continuous. $(\Rightarrow)$ is trivial. $(\Leftarrow)$: Intuitively, $f$ is continuous at every subset on it's domain, so all in all $f$ is continuous. How to formalize this? What should I rely on? Thanks.","Prove $f:\mathbb{R}^n\to\mathbb{R}$ is continuous iff for every curve, $\gamma:[a,b]\to\mathbb{R}^n: f\circ \gamma :\mathbb{R}\to\mathbb{R}$ is continuous. $(\Rightarrow)$ is trivial. $(\Leftarrow)$: Intuitively, $f$ is continuous at every subset on it's domain, so all in all $f$ is continuous. How to formalize this? What should I rely on? Thanks.",,"['calculus', 'multivariable-calculus', 'functions', 'continuity']"
14,row vs. column vector derivative,row vs. column vector derivative,,"I am reviewing calculus from Spivak's Calculus on Manifolds , and it looks like he is being (very?) cavalier with regards to when vectors should be written as columns or rows. Let $f:\mathbf{R}^n \to \mathbf{R}^m$ be a differentiable function, and let $a \in \mathbf{R}^n$. Let $Df(a)$ denote the derivative of $f$ at $a$ (just to be clear, by the definition Spivak uses, $Df(a)$ is the linear map such that $\lim\limits_{h \to 0} \dfrac{||f(a+h)-f(a)-Df(a)(h)||}{||h||} = 0$). Now, he says that if $f(x) = (f_1(x),\ldots,f_m(x))$ then $Df(a) = (Df_1(a),\ldots,Df_m(a))$. This seems simple enough, and the proof is very easy, but he follows it up by saying that the matrix of this map, denoted $f'(a)$, is the matrix with $f_i'(a)$ as the $i$th row. But does this make sense? In order for the matrix to be like that, he would have had to have written $Df(a)$ as a column , no?","I am reviewing calculus from Spivak's Calculus on Manifolds , and it looks like he is being (very?) cavalier with regards to when vectors should be written as columns or rows. Let $f:\mathbf{R}^n \to \mathbf{R}^m$ be a differentiable function, and let $a \in \mathbf{R}^n$. Let $Df(a)$ denote the derivative of $f$ at $a$ (just to be clear, by the definition Spivak uses, $Df(a)$ is the linear map such that $\lim\limits_{h \to 0} \dfrac{||f(a+h)-f(a)-Df(a)(h)||}{||h||} = 0$). Now, he says that if $f(x) = (f_1(x),\ldots,f_m(x))$ then $Df(a) = (Df_1(a),\ldots,Df_m(a))$. This seems simple enough, and the proof is very easy, but he follows it up by saying that the matrix of this map, denoted $f'(a)$, is the matrix with $f_i'(a)$ as the $i$th row. But does this make sense? In order for the matrix to be like that, he would have had to have written $Df(a)$ as a column , no?",,"['multivariable-calculus', 'notation']"
15,"Prove that $ f(x,y) = (x^2 + y^2)\sin\frac{1}{x^2 + y^2}$ extended by $f(0,0)=0$ is differentiable.",Prove that  extended by  is differentiable.," f(x,y) = (x^2 + y^2)\sin\frac{1}{x^2 + y^2} f(0,0)=0","Prove that $  f(x,y) = \begin{cases} (x^2 + y^2)\sin\frac{1}{x^2 + y^2},  & \text{if $(x,y) \ne (0,0)$} \\ 0, & \text{if $(x,y) = (0,0)$}  \\ \end{cases}$ differentiable. I tried to use the definition: $$\lim_{{h \to 0}\\{, k\to0}} \frac{f(x+h, y+k) - f(x,y) - f'_x(x,y)h - f'_y(x,y)k}{\sqrt{h^2 + k^2}} = 0.$$ Started by find the partial derivetives: $$f'_x = 2x\sin(\frac{1}{x^2 + y^2}) - (x^2 + y^2)\frac{2x \cos(\frac{1}{x^2 + y^2})}{(x^2 + y^2)^2} = \dots = 2x\bigg(\sin(\frac{1}{x^2 + y^2}) - \frac{\cos(\frac{1}{x^2 + y^2})}{x^2 + y^2}\bigg)$$ $$f'_y = 2y\bigg(\sin(\frac{1}{x^2 + y^2}) - \frac{\cos(\frac{1}{x^2 + y^2})}{x^2 + y^2}\bigg).$$ Plug into the equation again, and it seems to be very long. Is there any theorem or something like that, that can ease this process?","Prove that $  f(x,y) = \begin{cases} (x^2 + y^2)\sin\frac{1}{x^2 + y^2},  & \text{if $(x,y) \ne (0,0)$} \\ 0, & \text{if $(x,y) = (0,0)$}  \\ \end{cases}$ differentiable. I tried to use the definition: $$\lim_{{h \to 0}\\{, k\to0}} \frac{f(x+h, y+k) - f(x,y) - f'_x(x,y)h - f'_y(x,y)k}{\sqrt{h^2 + k^2}} = 0.$$ Started by find the partial derivetives: $$f'_x = 2x\sin(\frac{1}{x^2 + y^2}) - (x^2 + y^2)\frac{2x \cos(\frac{1}{x^2 + y^2})}{(x^2 + y^2)^2} = \dots = 2x\bigg(\sin(\frac{1}{x^2 + y^2}) - \frac{\cos(\frac{1}{x^2 + y^2})}{x^2 + y^2}\bigg)$$ $$f'_y = 2y\bigg(\sin(\frac{1}{x^2 + y^2}) - \frac{\cos(\frac{1}{x^2 + y^2})}{x^2 + y^2}\bigg).$$ Plug into the equation again, and it seems to be very long. Is there any theorem or something like that, that can ease this process?",,"['multivariable-calculus', 'derivatives', 'partial-derivative']"
16,"If the Jacobian of two functions is zero, how are the two functions related?","If the Jacobian of two functions is zero, how are the two functions related?",,"Let $ x^{1} = f^{1}(u^{1},u^{2})$ and $x^{2} = f^{2}(u^{1},u^{2}) $. If the Jacobian of $f^{1}$ and $f^{2}$ is identically equal to zero (i.e. equal to 0 for all values of $u^1$ and $u^2$), why does this mean that there must be a functional relations between $x^{1}$ and $ x^{2}$ such that there exists a function $\phi$ such that $\phi(x^{1},x^{2}) = 0$ ?","Let $ x^{1} = f^{1}(u^{1},u^{2})$ and $x^{2} = f^{2}(u^{1},u^{2}) $. If the Jacobian of $f^{1}$ and $f^{2}$ is identically equal to zero (i.e. equal to 0 for all values of $u^1$ and $u^2$), why does this mean that there must be a functional relations between $x^{1}$ and $ x^{2}$ such that there exists a function $\phi$ such that $\phi(x^{1},x^{2}) = 0$ ?",,"['multivariable-calculus', 'differential-geometry']"
17,Wedge product and change of variables,Wedge product and change of variables,,"The question is: Let $\phi ： \mathbb{R}^n \to \mathbb{R}^n$ be a $C^1$ map and let $y = \phi(x)$ be the change of variables. Show that $$dy_1\wedge\dots\wedge dy_n = (\operatorname{det}D\phi(x))\cdot dx_1\wedge\dots\wedge dx_n.$$ I tried $n = 2, 3$ but still didn't get any rule. Do we need to do this under integral?",The question is: Let be a map and let be the change of variables. Show that I tried but still didn't get any rule. Do we need to do this under integral?,"\phi ： \mathbb{R}^n \to \mathbb{R}^n C^1 y = \phi(x) dy_1\wedge\dots\wedge dy_n = (\operatorname{det}D\phi(x))\cdot dx_1\wedge\dots\wedge dx_n. n = 2, 3","['differential-geometry', 'multivariable-calculus', 'exterior-algebra']"
18,Is this sloppy writing for limits?,Is this sloppy writing for limits?,,"Please note that I am not asking you to compute or show me how to do this limit. I am asking how to write out a clean and formal solution that is free of any error, ambiguity, or sloppiness. Given $$\lim\limits_{(x,y) \to (0,0)} \dfrac{3x^2 y}{x^2 + y^2}$$, find its limit So find the limit along $y = mx$ and let $f(x,y) = \dfrac{3x^2 y}{x^2 + y^2}$. So we have $f(x,mx) = \dfrac{3x^2 mx}{x^2 + m^2 x^2} = \dfrac{3mx}{1 + m^2}$ Here is the part where I am not so hot on. Can I write this? $\lim\limits_{(x,y) \to (0,0)} f(x,y) = \lim\limits_{(x,y) \to (0,0)} f(x,mx) = \lim\limits_{(x,y) \to (0,0)} \dfrac{3mx}{1 + m^2}= 0$ And conclude the limit is indeed $0$ through any line. (a formal justification involves epsilon-delta, but I omit it here because that is another question for another time). I am thinking that the first equality sign is wrong. Remark Most books I've read seem to do everything without the limit operator. Stewart for instance just argues the limit is this and this along this path and that path. I want to do my answers with the limit operators","Please note that I am not asking you to compute or show me how to do this limit. I am asking how to write out a clean and formal solution that is free of any error, ambiguity, or sloppiness. Given $$\lim\limits_{(x,y) \to (0,0)} \dfrac{3x^2 y}{x^2 + y^2}$$, find its limit So find the limit along $y = mx$ and let $f(x,y) = \dfrac{3x^2 y}{x^2 + y^2}$. So we have $f(x,mx) = \dfrac{3x^2 mx}{x^2 + m^2 x^2} = \dfrac{3mx}{1 + m^2}$ Here is the part where I am not so hot on. Can I write this? $\lim\limits_{(x,y) \to (0,0)} f(x,y) = \lim\limits_{(x,y) \to (0,0)} f(x,mx) = \lim\limits_{(x,y) \to (0,0)} \dfrac{3mx}{1 + m^2}= 0$ And conclude the limit is indeed $0$ through any line. (a formal justification involves epsilon-delta, but I omit it here because that is another question for another time). I am thinking that the first equality sign is wrong. Remark Most books I've read seem to do everything without the limit operator. Stewart for instance just argues the limit is this and this along this path and that path. I want to do my answers with the limit operators",,"['multivariable-calculus', 'proof-writing']"
19,Calculating the minimum of $\cos x \sin y$,Calculating the minimum of,\cos x \sin y,"I am about to start university in October, to study computer science, and have been asked by my university to complete a number of problem sheets.  I have become stuck on the following question, and therefore would appreciate any help possible. The numbers $x$ and $y$ are subject to the constraints $x+y=\pi$. Find the values of $x$ and $y$ for which $\cos x\sin y$ takes its minimum value. Using this question as a starting point towards a solution, I have the following steps attempted so far. \begin{align*} \Lambda(x, y, \lambda)&=\cos x\sin y+\lambda(x+y-\pi)\\ \frac{\partial\Lambda}{\partial x}&=-\sin x\sin y+\lambda=0\\ \frac{\partial\Lambda}{\partial y}&=\cos x\cos y+\lambda=0\\ \frac{\partial\Lambda}{\partial\lambda}&=x+y-\pi=0\\ x&=\cos^{-1}\left(\frac{\lambda}{\sin y}\right)\\ y&=\cos^{-1}\left(\frac{-\lambda}{\cos x}\right)\\ \cos^{-1}\left(\frac{\lambda}{\sin y}\right)+\cos^{-1}\left(\frac{-\lambda}{\cos x}\right)&=\pi\\ \end{align*} Unfortunately, such maths is way beyond my abilities, as I have only studied A-Level Maths and Further Maths, and I am working from the first answer in the referenced question and the Wikipedia articles on Lagrange Multipliers and Partial Derivatives . I'm unsure of the correct tags to apply, so any help there would also be wonderful. Edit: After receiving a number of hints, this is part of my solution, however, I'm not sure on how to properly phrase the last bit of the question with respect to properly solving the inequality or expressing values for $y$. \begin{align*} \sin x\cos y&=\sin x\cos(\pi-x)\\ &=-\sin x\cos x\\ &=\sin x\cos x\\ &=\frac12\sin2x\\ \frac{\mathrm{d}}{\mathrm{d}x}\left(\frac12\sin2x\right)&=\cos2x\\ \cos2x&=0\Rightarrow x=\frac12\left(n\pi-\frac\pi2\right),n\in\mathbb{Z}\\ \frac{\mathrm{d}}{\mathrm{d}x}\cos2x&=-2\sin2x\\ -2\sin2x>0&\Rightarrow\sin2x<0\\ \end{align*}","I am about to start university in October, to study computer science, and have been asked by my university to complete a number of problem sheets.  I have become stuck on the following question, and therefore would appreciate any help possible. The numbers $x$ and $y$ are subject to the constraints $x+y=\pi$. Find the values of $x$ and $y$ for which $\cos x\sin y$ takes its minimum value. Using this question as a starting point towards a solution, I have the following steps attempted so far. \begin{align*} \Lambda(x, y, \lambda)&=\cos x\sin y+\lambda(x+y-\pi)\\ \frac{\partial\Lambda}{\partial x}&=-\sin x\sin y+\lambda=0\\ \frac{\partial\Lambda}{\partial y}&=\cos x\cos y+\lambda=0\\ \frac{\partial\Lambda}{\partial\lambda}&=x+y-\pi=0\\ x&=\cos^{-1}\left(\frac{\lambda}{\sin y}\right)\\ y&=\cos^{-1}\left(\frac{-\lambda}{\cos x}\right)\\ \cos^{-1}\left(\frac{\lambda}{\sin y}\right)+\cos^{-1}\left(\frac{-\lambda}{\cos x}\right)&=\pi\\ \end{align*} Unfortunately, such maths is way beyond my abilities, as I have only studied A-Level Maths and Further Maths, and I am working from the first answer in the referenced question and the Wikipedia articles on Lagrange Multipliers and Partial Derivatives . I'm unsure of the correct tags to apply, so any help there would also be wonderful. Edit: After receiving a number of hints, this is part of my solution, however, I'm not sure on how to properly phrase the last bit of the question with respect to properly solving the inequality or expressing values for $y$. \begin{align*} \sin x\cos y&=\sin x\cos(\pi-x)\\ &=-\sin x\cos x\\ &=\sin x\cos x\\ &=\frac12\sin2x\\ \frac{\mathrm{d}}{\mathrm{d}x}\left(\frac12\sin2x\right)&=\cos2x\\ \cos2x&=0\Rightarrow x=\frac12\left(n\pi-\frac\pi2\right),n\in\mathbb{Z}\\ \frac{\mathrm{d}}{\mathrm{d}x}\cos2x&=-2\sin2x\\ -2\sin2x>0&\Rightarrow\sin2x<0\\ \end{align*}",,['multivariable-calculus']
20,find all points for intersection between 2 polar equations,find all points for intersection between 2 polar equations,,"I stumped at one of the exercise in my multivariable calculus textbook. I try to search online but I can't seem to search on how answer no 3 and 4 below is derived. I also plot both of polar coordinates in mathematica but I can't seem to get around on how to solve them algebraically. The question and answer as appear in textbook is as below: The polar equation for the curve $C_{1}$ is $r=\cos 2\theta$,while the curve $C_{2}$ is described by the polar equation $r=1+\cos\theta$. Find all points at which $C_{1}$ and $C_{2}$ intersect. Solution below 1) r=0 (the origin) 2) ($\frac{5-\sqrt{17}}{4}$ , $\pm\arccos \frac{1-\sqrt{17}}{4}$) (I manage this solve this, just put both equation equal to each other) I'm at loss what the formula to derive answer for 3 and 4 below. 3) ($-1$ , $\pm 90 ^\circ$) 4) ($\frac{-1}{2}$ , $\pm 60 ^\circ$) Can anyone point out to me what's the formula used ? Thanks","I stumped at one of the exercise in my multivariable calculus textbook. I try to search online but I can't seem to search on how answer no 3 and 4 below is derived. I also plot both of polar coordinates in mathematica but I can't seem to get around on how to solve them algebraically. The question and answer as appear in textbook is as below: The polar equation for the curve $C_{1}$ is $r=\cos 2\theta$,while the curve $C_{2}$ is described by the polar equation $r=1+\cos\theta$. Find all points at which $C_{1}$ and $C_{2}$ intersect. Solution below 1) r=0 (the origin) 2) ($\frac{5-\sqrt{17}}{4}$ , $\pm\arccos \frac{1-\sqrt{17}}{4}$) (I manage this solve this, just put both equation equal to each other) I'm at loss what the formula to derive answer for 3 and 4 below. 3) ($-1$ , $\pm 90 ^\circ$) 4) ($\frac{-1}{2}$ , $\pm 60 ^\circ$) Can anyone point out to me what's the formula used ? Thanks",,"['multivariable-calculus', 'polar-coordinates']"
21,Minkowski's Inequality,Minkowski's Inequality,,"I am wondering how to prove inequality $$\left\| \int f(.,y)dy \right\|_{p}\leq \int \left\| f(.,y) \right\|_{p}dy~~~~?$$ Here, $f$ is an integrable function on $\mathbb{R}^n$  and $\displaystyle \left\|f\right\|_{p}=\left( \int|f|^pdx \right)^{1/p}$ for $p\geq 1.$","I am wondering how to prove inequality $$\left\| \int f(.,y)dy \right\|_{p}\leq \int \left\| f(.,y) \right\|_{p}dy~~~~?$$ Here, $f$ is an integrable function on $\mathbb{R}^n$  and $\displaystyle \left\|f\right\|_{p}=\left( \int|f|^pdx \right)^{1/p}$ for $p\geq 1.$",,['multivariable-calculus']
22,What is a dog saddle?,What is a dog saddle?,,"I got this question as an assignment. The question is why the graph of the function $f(x,y)=x^3y-xy^3$ is called a dog saddle. I am rather confused as I don't know what our professor is really looking for. Isn't it called a dog saddle because it looks like one? Please help me with this. Thanks in advance.",I got this question as an assignment. The question is why the graph of the function is called a dog saddle. I am rather confused as I don't know what our professor is really looking for. Isn't it called a dog saddle because it looks like one? Please help me with this. Thanks in advance.,"f(x,y)=x^3y-xy^3",['multivariable-calculus']
23,Level curves and critical points,Level curves and critical points,,How can I find critical points with level curves? Well I saw this video about a method for find critical points with level curves video but he don´t proof nothing and really I don't understand what is the essential method for find maxima or minima point. I'm interested in that because I'm reading a article that use this method for find global maxima ref theorem 2 Also if you can give me an explanation of theorem 2 I appreciate,How can I find critical points with level curves? Well I saw this video about a method for find critical points with level curves video but he don´t proof nothing and really I don't understand what is the essential method for find maxima or minima point. I'm interested in that because I'm reading a article that use this method for find global maxima ref theorem 2 Also if you can give me an explanation of theorem 2 I appreciate,,"['real-analysis', 'multivariable-calculus', 'optimization', 'proof-explanation']"
24,What is the best way to make notation in multi-variable calculus rigorous?,What is the best way to make notation in multi-variable calculus rigorous?,,"(Warning: long post worrying about formalism, may seem totally pointless to many.) Related to ""Can the idea of a 'function of a variable' be made rigorous?"" . There is a disconnect in how mathematics is usually formalized and how mathematicians tend to write about derivatives. For example, it's not uncommon to see notation like: \begin{alignat}{1}    v &= a^2b + 3ab \\[5pt] \frac{\partial v}{\partial a} &= 2ab + 3b \\[5pt] \frac{\partial v}{\partial b} &= a^2 + 3a \\[5pt] a &= x^2 + y^2 \\[5pt] b &= xy \\[5pt] \frac{\partial v}{\partial x} &= \frac{\partial v}{\partial a}\frac{\partial a}{\partial x} + \frac{\partial v}{\partial b}\frac{\partial b}{\partial x}   \end{alignat} This notation assumes that A function 'knows' its variables, so that $\frac{\partial v}{\partial a}$ and $\frac{\partial v}{\partial b}$ are both well-defined and different Different Variables refer to different objects, so that $v(a,b)$ and $v(x,y)$ are both well-defined and different The same object can be both a function and a parameter, i.e., $a$ appears in the formulas $v=a^2b + 3ab$ and $a=x^2+y^2$ However, in the classical formalism, a function $f : \mathbb{R} \rightarrow \mathbb{R}$ , say the one that doubles all inputs, is just a set of tuples $\{(1,2), (2,4), (3,6), ...\}$ . (Alternatively, it may be a triple (domain, codomain, set-of-tuples), but this difference doesn't matter for this post.) This formalism doesn't support any of the above things: If $v = \{((1,1),4), ...\}$ , then $a$ and $b$ aren't objects of any kind. The function $v$ may be defineed using notation where 'a' an 'b' appear as bound variables, such as $v = \{((a,b),a^2b + 3ab) \;|\; (a,b) \in \mathbb R^2\}$ , but they're only bound within the expression that defines the set $v$ . This makes it ""impossible"" for $a = x^2 + y^2$ to refer to the $a$ that was used in $v = a^2b + 3ab$ . For the same reason, $v(a,b)$ and $v(x,y)$ cannot refer to different objects. If a function's domain consists of real numbers, it doesn't consist of functions, so if $a$ and $b$ are functions, one can never evaluate $v(a,b)$ . I see three ways to deal with this disconnect Ignore it and continue to do calculations that lead to correct results. That is, until one gets confused about things like 'wait $\frac{\partial v}{\partial a}$ is a function from what to what?'. In such a case, always stare at the expression long enough to figure it out. Proceed to pretend that there is no problem. Alternatively, never get confused, somehow (do people really do this?) Try to interpret all notation as shorthands that talk about sets Think of the notation as being grounded in a different formalism I've been doing #1.1 ever since learning about set theory, but I'm no longer satisfied with that. The problem with #2 is that it doesn't seem to correspond to how people think about what they're doing. Consider the fact that serious mathematicians write things like $v = v(a,b)$ or "" $f$ is a function of $x$ , and $g$ is a function of $y$ "". These statements have no meaning in terms of the underlying formalism (except to state that the domain of $f$ is one-dimensional): if one writes $f(x) = x^2$ , then $f$ is a well-defined set, but $x$ is nothing whatsover (except a bound variable in a formula that defines the set $f$ ). This would make the statement "" $f$ is a function of $x$ "" merely a hint for how to interpret future notation, similar to a statement like ""we write $p(x)$ for $p(\{x\})$ "". Again, this does not seem to correspond to how people are thinking about the material. #3 is something I'm very interested in but don't know much about. My question: is there a satisfying solution for #2 or #3? Could $\lambda$ -calculus be a candidate?","(Warning: long post worrying about formalism, may seem totally pointless to many.) Related to ""Can the idea of a 'function of a variable' be made rigorous?"" . There is a disconnect in how mathematics is usually formalized and how mathematicians tend to write about derivatives. For example, it's not uncommon to see notation like: This notation assumes that A function 'knows' its variables, so that and are both well-defined and different Different Variables refer to different objects, so that and are both well-defined and different The same object can be both a function and a parameter, i.e., appears in the formulas and However, in the classical formalism, a function , say the one that doubles all inputs, is just a set of tuples . (Alternatively, it may be a triple (domain, codomain, set-of-tuples), but this difference doesn't matter for this post.) This formalism doesn't support any of the above things: If , then and aren't objects of any kind. The function may be defineed using notation where 'a' an 'b' appear as bound variables, such as , but they're only bound within the expression that defines the set . This makes it ""impossible"" for to refer to the that was used in . For the same reason, and cannot refer to different objects. If a function's domain consists of real numbers, it doesn't consist of functions, so if and are functions, one can never evaluate . I see three ways to deal with this disconnect Ignore it and continue to do calculations that lead to correct results. That is, until one gets confused about things like 'wait is a function from what to what?'. In such a case, always stare at the expression long enough to figure it out. Proceed to pretend that there is no problem. Alternatively, never get confused, somehow (do people really do this?) Try to interpret all notation as shorthands that talk about sets Think of the notation as being grounded in a different formalism I've been doing #1.1 ever since learning about set theory, but I'm no longer satisfied with that. The problem with #2 is that it doesn't seem to correspond to how people think about what they're doing. Consider the fact that serious mathematicians write things like or "" is a function of , and is a function of "". These statements have no meaning in terms of the underlying formalism (except to state that the domain of is one-dimensional): if one writes , then is a well-defined set, but is nothing whatsover (except a bound variable in a formula that defines the set ). This would make the statement "" is a function of "" merely a hint for how to interpret future notation, similar to a statement like ""we write for "". Again, this does not seem to correspond to how people are thinking about the material. #3 is something I'm very interested in but don't know much about. My question: is there a satisfying solution for #2 or #3? Could -calculus be a candidate?","\begin{alignat}{1}    v &= a^2b + 3ab
\\[5pt] \frac{\partial v}{\partial a} &= 2ab + 3b
\\[5pt] \frac{\partial v}{\partial b} &= a^2 + 3a
\\[5pt] a &= x^2 + y^2
\\[5pt] b &= xy
\\[5pt] \frac{\partial v}{\partial x} &= \frac{\partial v}{\partial a}\frac{\partial a}{\partial x} + \frac{\partial v}{\partial b}\frac{\partial b}{\partial x}
  \end{alignat} \frac{\partial v}{\partial a} \frac{\partial v}{\partial b} v(a,b) v(x,y) a v=a^2b + 3ab a=x^2+y^2 f : \mathbb{R} \rightarrow \mathbb{R} \{(1,2), (2,4), (3,6), ...\} v = \{((1,1),4), ...\} a b v v = \{((a,b),a^2b + 3ab) \;|\; (a,b) \in \mathbb R^2\} v a = x^2 + y^2 a v = a^2b + 3ab v(a,b) v(x,y) a b v(a,b) \frac{\partial v}{\partial a} v = v(a,b) f x g y f f(x) = x^2 f x f f x p(x) p(\{x\}) \lambda","['multivariable-calculus', 'functions', 'notation']"
25,Why are these two double integrals different ? A question on Dirac delta distribution,Why are these two double integrals different ? A question on Dirac delta distribution,,"Consider the following two integrals: \begin{align} I_1&:=\iint_{0\leq x,\, y\leq 1,\,x=y}\,dx\, dy,\\ I_2&:=\int_0^1\int_0^1 \delta(x-y)\,dx \,dy. \end{align} I believe $I_1=0$ because it is the measure ""surface area"" of the line segment $y=x$ inside a two dimensional rectangle. From the properties of the delta function, we have $$I_2=\int_0^1\int_0^1 \delta(x-y)\, dx\, dy=\int_0^1 1 \,dy=1$$ as $$\int_0^1 \delta(x-y)\, dx =1$$ for any $y\in ]0,1[.$ Question : I understand very well that the integrands are different in $I_1$ and $I_2$ . In $I_1$ , the integrand is the characteristic function $\chi_{0\leq x,y\leq 1,x=y}(x,y)$ . But these integrands are different on a negligible set... I am confused!","Consider the following two integrals: I believe because it is the measure ""surface area"" of the line segment inside a two dimensional rectangle. From the properties of the delta function, we have as for any Question : I understand very well that the integrands are different in and . In , the integrand is the characteristic function . But these integrands are different on a negligible set... I am confused!","\begin{align}
I_1&:=\iint_{0\leq x,\, y\leq 1,\,x=y}\,dx\, dy,\\
I_2&:=\int_0^1\int_0^1 \delta(x-y)\,dx \,dy.
\end{align} I_1=0 y=x I_2=\int_0^1\int_0^1 \delta(x-y)\, dx\, dy=\int_0^1 1 \,dy=1 \int_0^1 \delta(x-y)\, dx =1 y\in ]0,1[. I_1 I_2 I_1 \chi_{0\leq x,y\leq 1,x=y}(x,y)","['multivariable-calculus', 'lebesgue-measure', 'dirac-delta']"
26,"How to prove that $|x^p-y^p| \leq p|x-y|(x^{p-1}+y^{p-1})$ when $x,y \geq 0$ and $p \geq 1$? [duplicate]",How to prove that  when  and ? [duplicate],"|x^p-y^p| \leq p|x-y|(x^{p-1}+y^{p-1}) x,y \geq 0 p \geq 1","This question already has an answer here : Prove that $|x^p - y^p| \le p|x-y|(x^{p-1} + y^{p-1})$ provided that $1 \le p \lt \infty$ and $x, y \ge 0$ (1 answer) Closed 3 years ago . For $x \geq 0$ , $y \geq 0$ , prove that $$ |x^p-y^p| \leq p|x-y|(x^{p-1}+y^{p-1}). $$ I thought it would be simple but I messed everything up. Here are my attempts. Fixing $x > y \geq 0$ , I thought about the function $$ h(p)=x^p-y^p-p(x-y)(x^{p-1}+y^{p-1}) $$ Then $h(1)=0$ and I wanted to prove that $h(p) \leq 0$ when $p \geq 1$ by discussing its derivative but the derivative is messy offering no way out. Also I thought about after fixing $y \geq 0$ , consider the function $$ \varphi(x)=x^p-y^p-p(x-y)(x^{p-1}+y^{p-1}) $$ where $\varphi(y)=0$ and I want to prove that $\varphi(x) \leq 0$ when $x \geq y$ yet again it's a messy way. I also thought about restricting $p$ to $\mathbb{N}$ or $\mathbb{Q}$ using decomposition like $$ (x^p-y^p)=(x-y)(x^{p-1}+x^{p-1}y+x^{p-2}y^2+\cdots+y^{p-1}) $$ but the number of terms does not match. But I do believe this should be a easy question with some special background. I must have missed something critical. Any hint/solution appreciated!","This question already has an answer here : Prove that $|x^p - y^p| \le p|x-y|(x^{p-1} + y^{p-1})$ provided that $1 \le p \lt \infty$ and $x, y \ge 0$ (1 answer) Closed 3 years ago . For , , prove that I thought it would be simple but I messed everything up. Here are my attempts. Fixing , I thought about the function Then and I wanted to prove that when by discussing its derivative but the derivative is messy offering no way out. Also I thought about after fixing , consider the function where and I want to prove that when yet again it's a messy way. I also thought about restricting to or using decomposition like but the number of terms does not match. But I do believe this should be a easy question with some special background. I must have missed something critical. Any hint/solution appreciated!","x \geq 0 y \geq 0 
|x^p-y^p| \leq p|x-y|(x^{p-1}+y^{p-1}).
 x > y \geq 0 
h(p)=x^p-y^p-p(x-y)(x^{p-1}+y^{p-1})
 h(1)=0 h(p) \leq 0 p \geq 1 y \geq 0 
\varphi(x)=x^p-y^p-p(x-y)(x^{p-1}+y^{p-1})
 \varphi(y)=0 \varphi(x) \leq 0 x \geq y p \mathbb{N} \mathbb{Q} 
(x^p-y^p)=(x-y)(x^{p-1}+x^{p-1}y+x^{p-2}y^2+\cdots+y^{p-1})
",['calculus']
27,A differentiable function on Euclidean Space compatible with scalar multiplication is a linear map,A differentiable function on Euclidean Space compatible with scalar multiplication is a linear map,,"Here is how the question stated: Problem $f: \mathbb{R}^n \rightarrow \mathbb{R}$ is differentiable and $$f(\lambda x)=\lambda f(x), \forall \lambda\in \mathbb{R}, x\in \mathbb{R}^n.$$ Prove that $f$ is a linear map. My thoughts The equation $f(\lambda x)=\lambda f(x)$ immediately gives the compatibility of scalar, leaving compatibility of addtion to be verified. I try to derive the addtion from $f(\lambda x)=\lambda f(x)$ . Apart from compatibility of scalar, $f$ is a homogeneous function. Suppose that $x=(x_1,x_2,\cdots,x_n)$ , then I get $f(\lambda x_1,\cdots,\lambda x_n)=\lambda f(x_1,x_2,\cdots,x_n)$ . Differentiating by $\lambda$ , I get $$ f_1x_1+f_2x_2+\cdots +f_nx_n=f\left( x_1,x_2,\cdots ,x_n \right)  $$ where $f_i$ is the partial derivative of $f$ about the $i^{\text{th}}$ variable of its domain. What I need now is $$ f\left( x+y \right) =f\left( x \right) +f\left( y \right) ,\forall x,y\in \mathbb{R}^n $$ Similarly, we suppose $y=(y_1,y_2, \cdots ,y_n)$ , then we need $$ f\left( x+y \right) =\left( x_1+y_1 \right) f_1\left( x_1+y_1 \right) +\left( x_2+y_2 \right) f_2\left( x_2+y_2 \right) +\left( x_n+y_n \right) f_n\left( x_n+y_n \right)  $$ equals to $$ f\left( x \right) +f\left( y \right) =x_1f_1\left( x_1 \right) +x_2f_2\left( x_2 \right) +\cdots +x_nf_n\left( x_n \right) +y_1f_1\left( y_1 \right) +y_2f_2\left( y_2 \right) +y_nf_n\left( y_n \right) . $$ Because $f_i$ , as derivative, is linear, we can break the brackets and cancel $x_if_i(x_i)$ and $x_if_i(y_i)$ . However, terms of form $x_if_i(y_i)$ and $y_if_i(x_i)$ cannot be cancelled, which puzzles me. It is possible that my thoughts were totally off the track! Any help or idea would be welcome!","Here is how the question stated: Problem is differentiable and Prove that is a linear map. My thoughts The equation immediately gives the compatibility of scalar, leaving compatibility of addtion to be verified. I try to derive the addtion from . Apart from compatibility of scalar, is a homogeneous function. Suppose that , then I get . Differentiating by , I get where is the partial derivative of about the variable of its domain. What I need now is Similarly, we suppose , then we need equals to Because , as derivative, is linear, we can break the brackets and cancel and . However, terms of form and cannot be cancelled, which puzzles me. It is possible that my thoughts were totally off the track! Any help or idea would be welcome!","f: \mathbb{R}^n \rightarrow \mathbb{R} f(\lambda x)=\lambda f(x), \forall \lambda\in \mathbb{R}, x\in \mathbb{R}^n. f f(\lambda x)=\lambda f(x) f(\lambda x)=\lambda f(x) f x=(x_1,x_2,\cdots,x_n) f(\lambda x_1,\cdots,\lambda x_n)=\lambda f(x_1,x_2,\cdots,x_n) \lambda 
f_1x_1+f_2x_2+\cdots +f_nx_n=f\left( x_1,x_2,\cdots ,x_n \right) 
 f_i f i^{\text{th}} 
f\left( x+y \right) =f\left( x \right) +f\left( y \right) ,\forall x,y\in \mathbb{R}^n
 y=(y_1,y_2, \cdots ,y_n) 
f\left( x+y \right) =\left( x_1+y_1 \right) f_1\left( x_1+y_1 \right) +\left( x_2+y_2 \right) f_2\left( x_2+y_2 \right) +\left( x_n+y_n \right) f_n\left( x_n+y_n \right) 
 
f\left( x \right) +f\left( y \right) =x_1f_1\left( x_1 \right) +x_2f_2\left( x_2 \right) +\cdots +x_nf_n\left( x_n \right) +y_1f_1\left( y_1 \right) +y_2f_2\left( y_2 \right) +y_nf_n\left( y_n \right) .
 f_i x_if_i(x_i) x_if_i(y_i) x_if_i(y_i) y_if_i(x_i)","['multivariable-calculus', 'linear-transformations']"
28,"Maximize $\log(2)+\log(3/2)x+\log(2)y+\log(5/2)z$ if $x+y+z\leq 1$ and $(y+z)^2+2x-x^2-2xy\leq 1-2\gamma$, $0.24 \leq \gamma \leq 0.25$","Maximize  if  and ,",\log(2)+\log(3/2)x+\log(2)y+\log(5/2)z x+y+z\leq 1 (y+z)^2+2x-x^2-2xy\leq 1-2\gamma 0.24 \leq \gamma \leq 0.25,"I am trying to maximize the function $$f(x,y,z)=\log(2)+\log(3/2)x+\log(2)y+\log(5/2)z$$ with the following constraints: $$x\geq 0, y\geq 0, z \geq 0,$$ $$x+y+z\leq 1,$$ $$x+y\geq 4/5,$$ $$(y+z)^2+2x-x^2-2xy\leq 1-2\gamma,$$ where $$0.24 \leq \gamma \leq 0.25.$$ I claim that the maximum value is $\frac{\log(12)}{2}+\frac{\sqrt{1-4\gamma}}{2}\log\left( \frac{4}{3} \right)$ , and that this maximum is obtained when $x=\frac{1+\sqrt{1-4\gamma}}{2}$ , $y=\frac{1-\sqrt{1-4\gamma}}{2}$ , and $z=0$ . I am trying to avoid using Lagrange Multipliers because it becomes complicated. I am wondering if there is another way. I would also be satisfied if I could show that $f(x,y,z)\leq \frac{\log(12)}{2}+\frac{\sqrt{1-4\gamma}}{2}\log\left( \frac{4}{3} \right)$ . Programs like Maple and Mathematica give me solutions for specific $\gamma$ , but I would like to find a step by step way to show this for ANY $\gamma$ . Thank you. Note: I want to point out that we treat $\gamma$ as a FIXED constant that lies in the real interval $[0.24, 0.25]$ . Also, all logarithms considered are real.","I am trying to maximize the function with the following constraints: where I claim that the maximum value is , and that this maximum is obtained when , , and . I am trying to avoid using Lagrange Multipliers because it becomes complicated. I am wondering if there is another way. I would also be satisfied if I could show that . Programs like Maple and Mathematica give me solutions for specific , but I would like to find a step by step way to show this for ANY . Thank you. Note: I want to point out that we treat as a FIXED constant that lies in the real interval . Also, all logarithms considered are real.","f(x,y,z)=\log(2)+\log(3/2)x+\log(2)y+\log(5/2)z x\geq 0, y\geq 0, z \geq 0, x+y+z\leq 1, x+y\geq 4/5, (y+z)^2+2x-x^2-2xy\leq 1-2\gamma, 0.24 \leq \gamma \leq 0.25. \frac{\log(12)}{2}+\frac{\sqrt{1-4\gamma}}{2}\log\left( \frac{4}{3} \right) x=\frac{1+\sqrt{1-4\gamma}}{2} y=\frac{1-\sqrt{1-4\gamma}}{2} z=0 f(x,y,z)\leq \frac{\log(12)}{2}+\frac{\sqrt{1-4\gamma}}{2}\log\left( \frac{4}{3} \right) \gamma \gamma \gamma [0.24, 0.25]","['multivariable-calculus', 'optimization', 'nonlinear-optimization', 'maxima-minima', 'lagrange-multiplier']"
29,"Why doesn't the ""actual"" path matter for line integrals?","Why doesn't the ""actual"" path matter for line integrals?",,"We have the following definition given in our textbook: Let $U \subseteq \mathbb{R}^n$ be open and $F: U \rightarrow \mathbb{R}$ be continuously partial differentiable. If $a, b \in U$ and $\gamma$ is a piecewise differentiable path from a to b, that lies completely in $U$ ( $[a,b]\in U$ ), then: $$\int_\gamma (\operatorname{grad} F) \cdot dx = F(b)-F(a)$$ This is obviously super useful for solving line integrals $$\int_\gamma f\,dx$$ where we can find $F$ such that $\operatorname{grad} F = f$ . My question is: why doesn't the path matter in these cases? If I have two paths $\gamma$ and $\gamma^*$ with the same origin/destination but with completely different paths, this tells me the line integral is the same. Why does this make sense?","We have the following definition given in our textbook: Let be open and be continuously partial differentiable. If and is a piecewise differentiable path from a to b, that lies completely in ( ), then: This is obviously super useful for solving line integrals where we can find such that . My question is: why doesn't the path matter in these cases? If I have two paths and with the same origin/destination but with completely different paths, this tells me the line integral is the same. Why does this make sense?","U \subseteq \mathbb{R}^n F: U \rightarrow \mathbb{R} a, b \in U \gamma U [a,b]\in U \int_\gamma (\operatorname{grad} F) \cdot dx = F(b)-F(a) \int_\gamma f\,dx F \operatorname{grad} F = f \gamma \gamma^*","['multivariable-calculus', 'vector-fields', 'line-integrals', 'path-connected']"
30,Find the surface of least area spanned by a given contour,Find the surface of least area spanned by a given contour,,"I'm reading a book on Calculus of Variations, and am confused by the following problem: Find the surface of least area spanned by a given contour. The answer is to write a functional of the form: $$J [z] = \iint_R \sqrt {1+z_x^2+z_y^2} dx dy $$ Now, use Euler's equation: $$F_z-\frac {\partial}{\partial x} F_{z_x}-\frac {\partial}{\partial y} F_{z_y} = 0$$ Where $F = \sqrt {1+z_x^2+z_y^2} $ The calculation does not confuse me so much as what we are actually finding. I can see that the argument of the integral is highly analogous to arclength. When finding the curve of least length that connects two points, we apply the single variable Euler equation to: $$\int_L \sqrt{1+y'^2}dx $$ However, I'm not entirely sure where the formula for $J [z] $ comes from, nor what it means for a surface to be spanned by a contour (in fact, I'm not all that certain of what a contour is) I am enjoying the book, but I have definitely been ambitious to try and tackle this subject, as it seems to take a number of subjects I am only vaguely familiar with for granted.","I'm reading a book on Calculus of Variations, and am confused by the following problem: Find the surface of least area spanned by a given contour. The answer is to write a functional of the form: $$J [z] = \iint_R \sqrt {1+z_x^2+z_y^2} dx dy $$ Now, use Euler's equation: $$F_z-\frac {\partial}{\partial x} F_{z_x}-\frac {\partial}{\partial y} F_{z_y} = 0$$ Where $F = \sqrt {1+z_x^2+z_y^2} $ The calculation does not confuse me so much as what we are actually finding. I can see that the argument of the integral is highly analogous to arclength. When finding the curve of least length that connects two points, we apply the single variable Euler equation to: $$\int_L \sqrt{1+y'^2}dx $$ However, I'm not entirely sure where the formula for $J [z] $ comes from, nor what it means for a surface to be spanned by a contour (in fact, I'm not all that certain of what a contour is) I am enjoying the book, but I have definitely been ambitious to try and tackle this subject, as it seems to take a number of subjects I am only vaguely familiar with for granted.",,"['multivariable-calculus', 'differential-geometry', 'calculus-of-variations']"
31,Proof of the generalized Clairaut's theorem.,Proof of the generalized Clairaut's theorem.,,"I'm trying to prove the following preposition: Clairaut's basic theorem says that if $f: \mathbb{R^n} \to \mathbb{R}$ is a $C^2$ function, then $\partial_{ij}f = \partial_{ji}f$ for all $1 \leq i, j, \leq n$. Use the basic theorem to prove the more generalized version of Clairaut's theorem of Clairaut's theorem. That is, if $f: \mathbb{R^n} \to \mathbb{R}$ is a $C^k$ function, then $\partial_{i_1, ..., i_k} = \partial_{j_1, ..., j_k}f$ whenever $(i_1, ..., i_k)$ and $(j_1, ..., j_k)$ are tuples of indices which are re-arrangements of each other. I'm attempting to prove this by induction. I could use some help in proving this. Is Induction the best way to go for this? Perhaps there's a better way? Proof by Induction. Base case: Let $n = 1$, so that $i = j = 1$. Since $f$ is of type $C^k$, it follows that $C^k \subseteq C^2$. Then by the basic Clairaut's theorem,  $\partial_{ij}f = \partial_{ji}f$ holds. Suppose for all $1 < i,j \leq m$ that $\partial_{i_1, ..., i_m} = \partial_{j_1, ..., j_m}$, for $\{i_1, ..., i_m\}$ and $\{j_1, ..., j_m\}$ being re-orderings of each other. Consider $m + 1$, then $\partial_{i_1, ..., i_m, i_{m+1}} = \partial_{j_1, ..., j_m, j_{m+1}}$ I am trying to consider transpositions of the indices in such a way that allows me to use my inductive hypothesis. If I swap the last $i_{m+1}$ with $i_1$, but I am not sure how to invoke the IH carefully. By swapping the first and the last i's, I can show use the IH on the first m j terms, but then I am stuck trying to show it for the m+1 j'th term. Any help appreciated.","I'm trying to prove the following preposition: Clairaut's basic theorem says that if $f: \mathbb{R^n} \to \mathbb{R}$ is a $C^2$ function, then $\partial_{ij}f = \partial_{ji}f$ for all $1 \leq i, j, \leq n$. Use the basic theorem to prove the more generalized version of Clairaut's theorem of Clairaut's theorem. That is, if $f: \mathbb{R^n} \to \mathbb{R}$ is a $C^k$ function, then $\partial_{i_1, ..., i_k} = \partial_{j_1, ..., j_k}f$ whenever $(i_1, ..., i_k)$ and $(j_1, ..., j_k)$ are tuples of indices which are re-arrangements of each other. I'm attempting to prove this by induction. I could use some help in proving this. Is Induction the best way to go for this? Perhaps there's a better way? Proof by Induction. Base case: Let $n = 1$, so that $i = j = 1$. Since $f$ is of type $C^k$, it follows that $C^k \subseteq C^2$. Then by the basic Clairaut's theorem,  $\partial_{ij}f = \partial_{ji}f$ holds. Suppose for all $1 < i,j \leq m$ that $\partial_{i_1, ..., i_m} = \partial_{j_1, ..., j_m}$, for $\{i_1, ..., i_m\}$ and $\{j_1, ..., j_m\}$ being re-orderings of each other. Consider $m + 1$, then $\partial_{i_1, ..., i_m, i_{m+1}} = \partial_{j_1, ..., j_m, j_{m+1}}$ I am trying to consider transpositions of the indices in such a way that allows me to use my inductive hypothesis. If I swap the last $i_{m+1}$ with $i_1$, but I am not sure how to invoke the IH carefully. By swapping the first and the last i's, I can show use the IH on the first m j terms, but then I am stuck trying to show it for the m+1 j'th term. Any help appreciated.",,"['real-analysis', 'multivariable-calculus', 'induction']"
32,Every convex function is locally Lipschitz ($\mathbb{R^n}$),Every convex function is locally Lipschitz (),\mathbb{R^n},I know that if $f$ is convex function so $f$ is continuous. And I know too that partial derivatives exists. What can I do?,I know that if $f$ is convex function so $f$ is continuous. And I know too that partial derivatives exists. What can I do?,,"['multivariable-calculus', 'convex-analysis', 'lipschitz-functions']"
33,Continuous differentiability of atan2,Continuous differentiability of atan2,,"Consider the function atan2 defined on the plane, minus the origin and the negative $x$-axis, as the unique $\theta$ such that $-\pi<\theta<\pi$ and $$ x = r \cos \theta, \qquad y=r \sin \theta$$ where $r = \sqrt{x^2+y^2}$. I would like to prove rigorously that atan2 is a continuously differentiable function of $x$ and $y$ in the domain above. Two approaches which I think don't work: Implicit differentiation: this doesn't help since the implicit function theorem is only local. So it cannot help us prove that atan2 is a single $C^1$ function over the entire domain described above. Replace $\mbox{atan2}(x,y)$ by $\tan^{-1}(y/x)$: this throws away information about the signs of $y$ and $x$ and hence cannot represent $\mbox{atan2}$ over the entire domain. Also, $y/x$ is undefined when $x=0$, but there are such points in the domain. I would be grateful for an outline of the proof or a reference to a book where it can be found.","Consider the function atan2 defined on the plane, minus the origin and the negative $x$-axis, as the unique $\theta$ such that $-\pi<\theta<\pi$ and $$ x = r \cos \theta, \qquad y=r \sin \theta$$ where $r = \sqrt{x^2+y^2}$. I would like to prove rigorously that atan2 is a continuously differentiable function of $x$ and $y$ in the domain above. Two approaches which I think don't work: Implicit differentiation: this doesn't help since the implicit function theorem is only local. So it cannot help us prove that atan2 is a single $C^1$ function over the entire domain described above. Replace $\mbox{atan2}(x,y)$ by $\tan^{-1}(y/x)$: this throws away information about the signs of $y$ and $x$ and hence cannot represent $\mbox{atan2}$ over the entire domain. Also, $y/x$ is undefined when $x=0$, but there are such points in the domain. I would be grateful for an outline of the proof or a reference to a book where it can be found.",,"['calculus', 'real-analysis', 'multivariable-calculus']"
34,prove $\frac{1}{2}\mathbf{\nabla (u \cdot u) = u \times (\nabla \times u ) + (u \cdot \nabla)u}$ using index notation,prove  using index notation,\frac{1}{2}\mathbf{\nabla (u \cdot u) = u \times (\nabla \times u ) + (u \cdot \nabla)u},"I'm having some trouble using index notation to prove the identity $$\frac{1}{2}\mathbf{\nabla (u \cdot u) = u \times (\nabla \times u ) + (u \cdot \nabla)u}$$ The closest I can get is by expanding the first term on the RHS, which gives $$\mathbf{u \times (\nabla \times u)} = 2u_j \partial x_i u_j - u_j\partial x_i u_i - u_i \partial x_j u_j$$ but I don't see what to do from here (if what I've done so far is correct). Any help will be appreciated! EDIT The comments so far are all a bit dubious about my expression for the first term on the RHS, here's my work: $$\mathbf{u \times (\nabla \times u)} = \epsilon_{ijk}u_j\epsilon_{klm}\partial x_lu_m$$ now I move the second Levi-Civata symbol to the left and use the identity GFR posted to get $$\epsilon_{ijk}u_j\epsilon_{klm}\partial x_lu_m = (\delta_{il}\delta_{jm}-\delta_{im}\delta_{jl})u_j\partial x_lu_m$$ expanding this gives $$\mathbf{u \times (\nabla \times u)} = u_j\partial x_i u_j - u_j\partial x_j u_i$$ This next step i'm not sure about , I move the derivatives to the left of each term $$u_j\partial x_i u_j - u_j\partial x_j u_i = \partial x_iu_ju_j - \partial x_j u_ju_i$$ then the product rule gives my original equation for $\mathbf{u \times (\nabla \times u)}$","I'm having some trouble using index notation to prove the identity $$\frac{1}{2}\mathbf{\nabla (u \cdot u) = u \times (\nabla \times u ) + (u \cdot \nabla)u}$$ The closest I can get is by expanding the first term on the RHS, which gives $$\mathbf{u \times (\nabla \times u)} = 2u_j \partial x_i u_j - u_j\partial x_i u_i - u_i \partial x_j u_j$$ but I don't see what to do from here (if what I've done so far is correct). Any help will be appreciated! EDIT The comments so far are all a bit dubious about my expression for the first term on the RHS, here's my work: $$\mathbf{u \times (\nabla \times u)} = \epsilon_{ijk}u_j\epsilon_{klm}\partial x_lu_m$$ now I move the second Levi-Civata symbol to the left and use the identity GFR posted to get $$\epsilon_{ijk}u_j\epsilon_{klm}\partial x_lu_m = (\delta_{il}\delta_{jm}-\delta_{im}\delta_{jl})u_j\partial x_lu_m$$ expanding this gives $$\mathbf{u \times (\nabla \times u)} = u_j\partial x_i u_j - u_j\partial x_j u_i$$ This next step i'm not sure about , I move the derivatives to the left of each term $$u_j\partial x_i u_j - u_j\partial x_j u_i = \partial x_iu_ju_j - \partial x_j u_ju_i$$ then the product rule gives my original equation for $\mathbf{u \times (\nabla \times u)}$",,"['multivariable-calculus', 'vector-analysis', 'index-notation']"
35,Global Max and Min Problem,Global Max and Min Problem,,"I'm working on a problem which asks me to find local and global extrema of the following function. $$f(x,y) = x^2y^2e^{(-x^2 - 2y^2)}$$ I went through and found all of the relevant partial derivatives. \begin{align*} f_x &= (2xy^2)(e^{(-x^2 - 2y^2)}) + (x^2y^2)(e^{(-x^2 - 2y^2)})(-2x)\\ f_x &= (e^{(-x^2-2y^2)})(2xy^2 -2x^3y^2)\\ \\ f_y & = (2x^2y)(e^{(-x^2-2y^2)}) + (x^2y^2)(e^{(-x^2-2y^2)})(-4y)\\ f_y &= (e^{(-x^2-2y^2)})(2x^2y-4x^2y^3)\\ \\ f_{xx} &= (e^{(-x^2-2y^2)})(-2x)(2xy^2 -2x^3y^2) + (e^{(-x^2-2y^2)})(2y^2 -6x^2y^2)\\ f_{xx} &= (e^{(-x^2-2y^2)})(-10x^2y^2 + 4x^4y^2 + 2y^2)\\ \\ f_{yy} &= (e^{(-x^2-2y^2)})(-4y)(2x^2y-4x^2y^3) + (e^{(-x^2-2y^2)})(2x^2 - 12x^2y^2)\\ f_{yy} &= (e^{(-x^2-2y^2)})(-20x^2y^2 + 16x^2y^4 + 2x^2)\\ \\ f_{xy} &= (e^{(-x^2-2y^2)})(-4y)(2xy^2 -2x^3y^2) + (e^{(-x^2-2y^2)})(4xy-4x^3y)\\ f_{xy} &= (e^{(-x^2-2y^2)})(-8xy^3 + 8x^3y^3 +4xy - 4x^3y)\\ \end{align*} However, I'm not sure what to do after this. I thought I was  supposed to set $f_x$ and $f_y$ equal to 0 but I don't know how to solve the equations that I get. Can someone please help me? Did I make a mistake while I was determining my partial derivatives? EDIT: I made a mistake calculating the partial derivatives and I edited that","I'm working on a problem which asks me to find local and global extrema of the following function. $$f(x,y) = x^2y^2e^{(-x^2 - 2y^2)}$$ I went through and found all of the relevant partial derivatives. \begin{align*} f_x &= (2xy^2)(e^{(-x^2 - 2y^2)}) + (x^2y^2)(e^{(-x^2 - 2y^2)})(-2x)\\ f_x &= (e^{(-x^2-2y^2)})(2xy^2 -2x^3y^2)\\ \\ f_y & = (2x^2y)(e^{(-x^2-2y^2)}) + (x^2y^2)(e^{(-x^2-2y^2)})(-4y)\\ f_y &= (e^{(-x^2-2y^2)})(2x^2y-4x^2y^3)\\ \\ f_{xx} &= (e^{(-x^2-2y^2)})(-2x)(2xy^2 -2x^3y^2) + (e^{(-x^2-2y^2)})(2y^2 -6x^2y^2)\\ f_{xx} &= (e^{(-x^2-2y^2)})(-10x^2y^2 + 4x^4y^2 + 2y^2)\\ \\ f_{yy} &= (e^{(-x^2-2y^2)})(-4y)(2x^2y-4x^2y^3) + (e^{(-x^2-2y^2)})(2x^2 - 12x^2y^2)\\ f_{yy} &= (e^{(-x^2-2y^2)})(-20x^2y^2 + 16x^2y^4 + 2x^2)\\ \\ f_{xy} &= (e^{(-x^2-2y^2)})(-4y)(2xy^2 -2x^3y^2) + (e^{(-x^2-2y^2)})(4xy-4x^3y)\\ f_{xy} &= (e^{(-x^2-2y^2)})(-8xy^3 + 8x^3y^3 +4xy - 4x^3y)\\ \end{align*} However, I'm not sure what to do after this. I thought I was  supposed to set $f_x$ and $f_y$ equal to 0 but I don't know how to solve the equations that I get. Can someone please help me? Did I make a mistake while I was determining my partial derivatives? EDIT: I made a mistake calculating the partial derivatives and I edited that",,['multivariable-calculus']
36,"$k(tx,ty)=tk(x,y)$ then $k(x,y)=Ax+By$",then,"k(tx,ty)=tk(x,y) k(x,y)=Ax+By","A friend asked me today the following question: Let $k(x,y)$ be differentiable in all $\mathbb{R}^{2}$ s.t for every   $(x,y)$ and for every $t$ it holds that $$k(tx,ty)=tk(x,y)$$ Prove that   there exist $A,B\in\mathbb{R}$ s.t $$k(x,y)=Ax+By$$ I want to use the chain rule somehow, but I am having difficulty using it (I am a bit rusty). I believe I can get $$\frac{\partial k}{\partial tx}\cdot\frac{\partial tx}{\partial t}+\frac{\partial k}{\partial ty}\cdot\frac{\partial ty}{\partial t}=k(x,y)$$ hence $$\frac{\partial k}{\partial tx}\cdot x+\frac{\partial k}{\partial ty}\cdot y=k(x,y)$$ but I don't see how this helps. Can someone please help me out ?","A friend asked me today the following question: Let $k(x,y)$ be differentiable in all $\mathbb{R}^{2}$ s.t for every   $(x,y)$ and for every $t$ it holds that $$k(tx,ty)=tk(x,y)$$ Prove that   there exist $A,B\in\mathbb{R}$ s.t $$k(x,y)=Ax+By$$ I want to use the chain rule somehow, but I am having difficulty using it (I am a bit rusty). I believe I can get $$\frac{\partial k}{\partial tx}\cdot\frac{\partial tx}{\partial t}+\frac{\partial k}{\partial ty}\cdot\frac{\partial ty}{\partial t}=k(x,y)$$ hence $$\frac{\partial k}{\partial tx}\cdot x+\frac{\partial k}{\partial ty}\cdot y=k(x,y)$$ but I don't see how this helps. Can someone please help me out ?",,"['multivariable-calculus', 'derivatives']"
37,Line integral without Greens formula.,Line integral without Greens formula.,,"I want to calculate: $$\oint_C\frac{1}{x^2 + y^2}dx + 2y\ dy$$ where $C = \{(x,y)\mid x^2 + y^2 = 1\}$. And I want to do it without greens formula. To calculate the line integral we must then Parameterize the circle $x^2 + y^2 = 1$ into the parameterized path function $f(t) = (\cos(t), \sin(t))$ for $0 \leq t\leq 2 \pi$. So the integral $\oint_C \frac{1}{x^2 + y^2} \, dx + 2y \, dy$ then becomes $\int_0^{2\pi} \frac{1}{\cos(\theta)^2 + \sin(\theta)^2}(-\sin(\theta)) + 2(\sin(\theta)) \cos(\theta)d\theta$ which becomes $\int_0^{2\pi} (-\sin(\theta)) + 2(\sin(\theta)) \cos(\theta)d\theta = \int_0^{2\pi} (\sin(\theta))(2\cos(\theta) -1)\,d\theta =0$? Can anybody tell me if this is right/close to right? The zero makes me doubt it.","I want to calculate: $$\oint_C\frac{1}{x^2 + y^2}dx + 2y\ dy$$ where $C = \{(x,y)\mid x^2 + y^2 = 1\}$. And I want to do it without greens formula. To calculate the line integral we must then Parameterize the circle $x^2 + y^2 = 1$ into the parameterized path function $f(t) = (\cos(t), \sin(t))$ for $0 \leq t\leq 2 \pi$. So the integral $\oint_C \frac{1}{x^2 + y^2} \, dx + 2y \, dy$ then becomes $\int_0^{2\pi} \frac{1}{\cos(\theta)^2 + \sin(\theta)^2}(-\sin(\theta)) + 2(\sin(\theta)) \cos(\theta)d\theta$ which becomes $\int_0^{2\pi} (-\sin(\theta)) + 2(\sin(\theta)) \cos(\theta)d\theta = \int_0^{2\pi} (\sin(\theta))(2\cos(\theta) -1)\,d\theta =0$? Can anybody tell me if this is right/close to right? The zero makes me doubt it.",,"['real-analysis', 'multivariable-calculus']"
38,Laplacian of a Smooth Function $f(\vec x) = 1/\|\vec x \|$ for $\| \vec x \| \geq 1$,Laplacian of a Smooth Function  for,f(\vec x) = 1/\|\vec x \| \| \vec x \| \geq 1,"This is a multivariable calculus problem from a past prelim exam. I have an answer for this written up (posted below), but it seemed rather time-intensive. If there is a slicker way to approach this problem, I'd appreciate seeing it. Thanks! Recall that for a smooth function $f: \mathbb{R}^3 \to \mathbb{R}$, the Laplacian of $f$ is defined by   $$ \Delta f = \nabla \cdot ( \nabla f). $$ Suppose that $f: \mathbb{R}^3 \to \mathbb{R}$ is a smooth function satisfying $f(\vec{x}) = 1/\|\vec{x}\|$ for $\|\vec{x}\| \geq 1$. Verify that $\Delta f(\vec{x}) = 0$ for $\|\vec{x}\| \geq 1$. Compute $\int_{\mathbb{R}^3} \Delta f \, dV$.","This is a multivariable calculus problem from a past prelim exam. I have an answer for this written up (posted below), but it seemed rather time-intensive. If there is a slicker way to approach this problem, I'd appreciate seeing it. Thanks! Recall that for a smooth function $f: \mathbb{R}^3 \to \mathbb{R}$, the Laplacian of $f$ is defined by   $$ \Delta f = \nabla \cdot ( \nabla f). $$ Suppose that $f: \mathbb{R}^3 \to \mathbb{R}$ is a smooth function satisfying $f(\vec{x}) = 1/\|\vec{x}\|$ for $\|\vec{x}\| \geq 1$. Verify that $\Delta f(\vec{x}) = 0$ for $\|\vec{x}\| \geq 1$. Compute $\int_{\mathbb{R}^3} \Delta f \, dV$.",,['multivariable-calculus']
39,"Proving $\int_{|x| \leq a} \mathrm d^n x \,\, x_\alpha^4 =\frac{3}{n(n+2)} \int_{|x| \leq a} \mathrm{d}^n x \, |x|^4$",Proving,"\int_{|x| \leq a} \mathrm d^n x \,\, x_\alpha^4 =\frac{3}{n(n+2)} \int_{|x| \leq a} \mathrm{d}^n x \, |x|^4","I am trying to compute the following $n$ dimensional integral $$I_4(n)\equiv\int_{|x| \leq a} \mathrm d^n x \,\, x_\alpha^4$$ where $x=(x_1,x_2,\ldots,x_n)\in\mathbb{R}^{n}$ and $x_\alpha$ is one of its components. I have not been able to proceed, but have managed to compute an easier integral: $$I_2 (n) \equiv \int_{|x| \leq a} \mathrm d^n x \,\, x_\alpha^2 = \frac{1}{n} \int_{|x| \leq a} \mathrm d^n {x} \,\, |x|^2 = \frac{a^{n+2} S_{n-1}}{n(n+2)}.$$ The first equality follows from the symmetry of exchanging the component $\alpha$ with any other of the components, and in the last result I have used the surface area of a unit sphere in $n$ dimensions . I tried computing $I_4$ using spherical coordinates but it seems unnecessarily involved. From the paper I am reading, I know I should be able to show that $$I_4 = \frac{3}{n(n+2)} \int \mathrm d^n x \, |x|^4,$$ from which the final result can be computed easily, but I haven't been able to do so. Are there any nice tricks to show this equality? I was also wondering if, in general, we can say $$\int \mathrm d^n x \, x_{\alpha_1} x_{\alpha_2} \cdots x_{\alpha_{2k}}  = A_{\alpha_1 \alpha_2\cdots\alpha_{2k}} \int \mathrm d^n x \, |x|^{2k}$$ and if there is a way to compute the proportionality $A$ in this case (which is likely to involve some combinations of $\delta_{\alpha_i \alpha_j}$ due to symmetry - I have a very naive guess that this has some connection to constructing traceless symmetric tensors (see this question )…) Edit Here is an attempt using spherical coordinates: From the symmetry, we know that $I_4$ will not depend on the specific index $\alpha$ , so we can choose this as the direction with the first axis. In the $n$ -dimensional spherical coordinates, we have $$\mathrm d^n x = r^{n-1} \sin^{n-2}(\phi_1) \sin^{n-3}(\phi_2)\cdots\sin(\phi_{n-2}) \,\mathrm dr \mathrm d\phi_1 \cdots \mathrm d\phi_{n-1} = r^{n-1} \,\mathrm dS_{n-1} \mathrm dr.$$ Writing $x_\alpha = r \cos(\phi_1)$ gives $$I_4(n) = \int_{|x|\leq a} r^{n+3} \,\mathrm dr \, \cos^4(\phi_1) \sin^{n-2}(\phi_1) \,\mathrm d\phi_1 \mathrm dS_{n-2}  = \frac{a^{n+4}}{n+4} \, \frac{3\sqrt{\pi}\, \Gamma(\frac{n-1}{2})}{n(n+2)\Gamma(\frac{n}{2})} S_{n-2},$$ where I used the integration result (thanks to Mathematica) $$\int_0^\pi \cos^4(\phi_1) \sin^{n-2}(\phi_1) \,\mathrm d\phi_1 = \frac{3\sqrt{\pi}\, \Gamma(\frac{n-1}{2})}{n(n+2)\Gamma(\frac{n}{2})}.$$ The final expression for $I_4$ can be simplified since $S_{n-2} \sqrt{\pi} \,\Gamma((n-1)/2)=\Gamma(n/2) S_{n-1}$ . This leaves us with $$I_4 = \frac{a^{n+4}}{n+4} \frac{3 S_{n-1}}{n(n+2)} = \frac{3}{n(n+2)}  \int \mathrm d^n x \, |x|^4.$$ However, this seems overkill to me, and I also don't know how to extend it to integrals over higher (even) powers of $x_\alpha$ .","I am trying to compute the following dimensional integral where and is one of its components. I have not been able to proceed, but have managed to compute an easier integral: The first equality follows from the symmetry of exchanging the component with any other of the components, and in the last result I have used the surface area of a unit sphere in dimensions . I tried computing using spherical coordinates but it seems unnecessarily involved. From the paper I am reading, I know I should be able to show that from which the final result can be computed easily, but I haven't been able to do so. Are there any nice tricks to show this equality? I was also wondering if, in general, we can say and if there is a way to compute the proportionality in this case (which is likely to involve some combinations of due to symmetry - I have a very naive guess that this has some connection to constructing traceless symmetric tensors (see this question )…) Edit Here is an attempt using spherical coordinates: From the symmetry, we know that will not depend on the specific index , so we can choose this as the direction with the first axis. In the -dimensional spherical coordinates, we have Writing gives where I used the integration result (thanks to Mathematica) The final expression for can be simplified since . This leaves us with However, this seems overkill to me, and I also don't know how to extend it to integrals over higher (even) powers of .","n I_4(n)\equiv\int_{|x| \leq a} \mathrm d^n x \,\, x_\alpha^4 x=(x_1,x_2,\ldots,x_n)\in\mathbb{R}^{n} x_\alpha I_2 (n) \equiv \int_{|x| \leq a} \mathrm d^n x \,\, x_\alpha^2 = \frac{1}{n} \int_{|x| \leq a} \mathrm d^n {x} \,\, |x|^2 = \frac{a^{n+2} S_{n-1}}{n(n+2)}. \alpha n I_4 I_4 = \frac{3}{n(n+2)} \int \mathrm d^n x \, |x|^4, \int \mathrm d^n x \, x_{\alpha_1} x_{\alpha_2} \cdots x_{\alpha_{2k}} 
= A_{\alpha_1 \alpha_2\cdots\alpha_{2k}} \int \mathrm d^n x \, |x|^{2k} A \delta_{\alpha_i \alpha_j} I_4 \alpha n \mathrm d^n x = r^{n-1} \sin^{n-2}(\phi_1) \sin^{n-3}(\phi_2)\cdots\sin(\phi_{n-2}) \,\mathrm dr \mathrm d\phi_1 \cdots \mathrm d\phi_{n-1} = r^{n-1} \,\mathrm dS_{n-1} \mathrm dr. x_\alpha = r \cos(\phi_1) I_4(n) = \int_{|x|\leq a} r^{n+3} \,\mathrm dr \, \cos^4(\phi_1) \sin^{n-2}(\phi_1) \,\mathrm d\phi_1 \mathrm dS_{n-2}  = \frac{a^{n+4}}{n+4} \, \frac{3\sqrt{\pi}\, \Gamma(\frac{n-1}{2})}{n(n+2)\Gamma(\frac{n}{2})} S_{n-2}, \int_0^\pi \cos^4(\phi_1) \sin^{n-2}(\phi_1) \,\mathrm d\phi_1 = \frac{3\sqrt{\pi}\, \Gamma(\frac{n-1}{2})}{n(n+2)\Gamma(\frac{n}{2})}. I_4 S_{n-2} \sqrt{\pi} \,\Gamma((n-1)/2)=\Gamma(n/2) S_{n-1} I_4 = \frac{a^{n+4}}{n+4} \frac{3 S_{n-1}}{n(n+2)} = \frac{3}{n(n+2)} 
\int \mathrm d^n x \, |x|^4. x_\alpha","['multivariable-calculus', 'multiple-integral', 'spherical-coordinates']"
40,Is this multivariable calculus?,Is this multivariable calculus?,,"The following is a question from Calculus made easy by Silvanus P. Thompson The volume of a right circular cylinder of radius $r$ and height $h$ is given by the formula $V=πr^2h$ . Find the rate of variation of volume with the radius when $r = 5.5 \text{in}$ . and $h = 20 \text{in}$ . If $r = h$ , find the dimensions of the cylinder so that a change of 1 in. in radius causes a change of 400 cubic inches in the volume. I understand the solution to the problem apart from the second part when $r=h$ . I got the first solution for that when I took h as a constant but there is second answer for the question in which $h$ varies with $r$ shown below: $$\frac {dV}{dr}=3πr^2$$ $$=400$$ and then we solve for $r$ which is equal to $h$ . My question is isn't this multivariable calculus and is this method correct?","The following is a question from Calculus made easy by Silvanus P. Thompson The volume of a right circular cylinder of radius and height is given by the formula . Find the rate of variation of volume with the radius when . and . If , find the dimensions of the cylinder so that a change of 1 in. in radius causes a change of 400 cubic inches in the volume. I understand the solution to the problem apart from the second part when . I got the first solution for that when I took h as a constant but there is second answer for the question in which varies with shown below: and then we solve for which is equal to . My question is isn't this multivariable calculus and is this method correct?",r h V=πr^2h r = 5.5 \text{in} h = 20 \text{in} r = h r=h h r \frac {dV}{dr}=3πr^2 =400 r h,"['calculus', 'multivariable-calculus']"
41,if $a^5+b^5<1$ and $c^5+d^5<1$ then prove that ${a^2}{c^3}+{b^2}{d^3}<1$,if  and  then prove that,a^5+b^5<1 c^5+d^5<1 {a^2}{c^3}+{b^2}{d^3}<1,"if $a^5+b^5<1$ and $c^5+d^5<1$ then prove that ${a^2}{c^3}+{b^2}{d^3}<1$ given $a,b,c,d$ are non  negative real numbers My try: It is easy to deduce that $a,b,c,d<1$ ,thus $$a^2c^3+b^2d^3<a^5c^5+b^5d^5<(a^5+b^5)(c^5+d^5)<1$$ I want to know if there is a more cleaner method to solve this problem and if possible with calculus. I would also  want to know if my proof has a mistake anywhere.","if and then prove that given are non  negative real numbers My try: It is easy to deduce that ,thus I want to know if there is a more cleaner method to solve this problem and if possible with calculus. I would also  want to know if my proof has a mistake anywhere.","a^5+b^5<1 c^5+d^5<1 {a^2}{c^3}+{b^2}{d^3}<1 a,b,c,d a,b,c,d<1 a^2c^3+b^2d^3<a^5c^5+b^5d^5<(a^5+b^5)(c^5+d^5)<1","['multivariable-calculus', 'inequality', 'proof-writing']"
42,Convex functions lack saddle points?,Convex functions lack saddle points?,,"I am reading ""Deep Learning"" by Ian Goodfellow. At page 86, the author explains how to use the Hessian to evaluate whether a point of a multivariate function is a maximum or a minimum At a critical point, where $ \nabla_x f(x)=0 $ , we can examine the   eigenvalues of the Hessian to determine whether the critical point is   a local maximum, local minimum or saddle point. When the Hessian is   positive definite (all its eigenvalues are positive), the point is a   local minimum. [...] Likewise when the Hessian is negative (all its   eigenvalues are negative), the point is a local maximum. In multiple   dimensions, it is actually possible to find positive evidence of   saddle points in some cases. When at least one eigenvalue is positive   and at least one eigenvalue is negative, we know that $x$ is a local   maximum on one cross section of $f$ but a local minimum on another   cross-section. [...] The test is inconclusive whenever all the nonzero   eigenvalues have the same sign but at least one eigenvalue is zero.   This is because the univariate second derivative test is inconclusive   in the cross section corresponding to the zero eigenvalue So far so good. At page 89 it talks about convex optimization, and says that: Convex functions - functions for which the Hessian is positive   semi-definite everywhere [..] are well-behaved because they lack   saddle points But if the Hessian is positive-semidefinite, it means that some eigenvalues may be zero, while the others are positive. I thought that ""whenever all the nonzero eigenvalues have the same sign but at least one eigenvalue is zero"" the test was inconclusive. So why does it says that they surely lack saddle points?","I am reading ""Deep Learning"" by Ian Goodfellow. At page 86, the author explains how to use the Hessian to evaluate whether a point of a multivariate function is a maximum or a minimum At a critical point, where , we can examine the   eigenvalues of the Hessian to determine whether the critical point is   a local maximum, local minimum or saddle point. When the Hessian is   positive definite (all its eigenvalues are positive), the point is a   local minimum. [...] Likewise when the Hessian is negative (all its   eigenvalues are negative), the point is a local maximum. In multiple   dimensions, it is actually possible to find positive evidence of   saddle points in some cases. When at least one eigenvalue is positive   and at least one eigenvalue is negative, we know that is a local   maximum on one cross section of but a local minimum on another   cross-section. [...] The test is inconclusive whenever all the nonzero   eigenvalues have the same sign but at least one eigenvalue is zero.   This is because the univariate second derivative test is inconclusive   in the cross section corresponding to the zero eigenvalue So far so good. At page 89 it talks about convex optimization, and says that: Convex functions - functions for which the Hessian is positive   semi-definite everywhere [..] are well-behaved because they lack   saddle points But if the Hessian is positive-semidefinite, it means that some eigenvalues may be zero, while the others are positive. I thought that ""whenever all the nonzero eigenvalues have the same sign but at least one eigenvalue is zero"" the test was inconclusive. So why does it says that they surely lack saddle points?", \nabla_x f(x)=0  x f,"['multivariable-calculus', 'optimization', 'convex-optimization', 'hessian-matrix']"
43,Change of variables and the partial derivative,Change of variables and the partial derivative,,"From time to time, I suddenly get confused with a change of variables in a partial derivative. Here, I am trying to perform a change of variables $(x,t) \mapsto (\xi, \eta)$ where $$\xi = t \qquad \qquad \text{and} \qquad \qquad \eta = x+t$$ The question is, how to compute $$\frac{\partial u}{\partial t}$$ in the new coordinate system? Intuitively, since $\xi = t$ (or rather $t=\xi$ ), we should have $$\frac{\partial u}{\partial t} = \frac{\partial u}{\partial \xi}$$ However, applying the chain rule for partial derivatives, we instead get $$\frac{\partial u}{\partial t} = \frac{\partial u}{\partial \xi}\frac{\partial \xi}{\partial t} + \frac{\partial u}{\partial \eta}\frac{\partial \eta}{\partial t} = \frac{\partial u}{\partial \xi}(1) + \frac{\partial u}{\partial \eta}(1) = \frac{\partial u}{\partial \xi} + \frac{\partial u}{\partial \eta}$$ So which one is correct?","From time to time, I suddenly get confused with a change of variables in a partial derivative. Here, I am trying to perform a change of variables where The question is, how to compute in the new coordinate system? Intuitively, since (or rather ), we should have However, applying the chain rule for partial derivatives, we instead get So which one is correct?","(x,t) \mapsto (\xi, \eta) \xi = t \qquad \qquad \text{and} \qquad \qquad \eta = x+t \frac{\partial u}{\partial t} \xi = t t=\xi \frac{\partial u}{\partial t} = \frac{\partial u}{\partial \xi} \frac{\partial u}{\partial t} = \frac{\partial u}{\partial \xi}\frac{\partial \xi}{\partial t} + \frac{\partial u}{\partial \eta}\frac{\partial \eta}{\partial t} = \frac{\partial u}{\partial \xi}(1) + \frac{\partial u}{\partial \eta}(1) = \frac{\partial u}{\partial \xi} + \frac{\partial u}{\partial \eta}","['multivariable-calculus', 'partial-derivative', 'change-of-variable']"
44,Conversion of Surface integral to a suitable Volume integral.,Conversion of Surface integral to a suitable Volume integral.,,"While deriving the Euler's equations of motion in case of Fluid dynamics, I came across this part - Here $p$ denotes the hydrostatic pressure(scalar function) I am unable to understand how it transformed this - $\iint _{\Delta S} - p\hat{n}~ds = -\iiint_{\Delta V} \nabla p~dv$ It says its a consequence of Gauss Divergence theorem but I could try only the below - $\iint _{\Delta S} - p\hat{n}~ds = \iiint _{\Delta V} \nabla\cdot(-p)~dv$ , but this seems wrong as how can i take the divergence of the scalar function? Reference - Textbook of Fluid Dynamics by F. Chorlton, page 96, Euler's equation of motion","While deriving the Euler's equations of motion in case of Fluid dynamics, I came across this part - Here $p$ denotes the hydrostatic pressure(scalar function) I am unable to understand how it transformed this - $\iint _{\Delta S} - p\hat{n}~ds = -\iiint_{\Delta V} \nabla p~dv$ It says its a consequence of Gauss Divergence theorem but I could try only the below - $\iint _{\Delta S} - p\hat{n}~ds = \iiint _{\Delta V} \nabla\cdot(-p)~dv$ , but this seems wrong as how can i take the divergence of the scalar function? Reference - Textbook of Fluid Dynamics by F. Chorlton, page 96, Euler's equation of motion",,"['multivariable-calculus', 'vector-analysis', 'surface-integrals', 'fluid-dynamics', 'divergence-operator']"
45,Understanding Lagrange multiplier,Understanding Lagrange multiplier,,"I am trying to understand Lagrange Multiplier. I think I have grasped the theory and can follow less difficult examples but I feel I am still missing full understanding. I think to optimize a function $f(\mathbf{x})$ subject to constraint $g(\mathbf{x})=C$, I can build a new function, Lagrangian, as follows: $$L(\mathbf{x},\lambda)=f(\mathbf{x})-\lambda \left(g(\mathbf{x})-C\right)$$ If I take gradient of the Lagrangian, I'll get a vector function of derivatives ($D+1$) while $D$ is dimension of $\mathbf{x}$: $$ \nabla L(\mathbf{x},\lambda)= \begin{bmatrix} \frac{\partial L(\mathbf{x},\lambda)}{\partial x_1} \\ \vdots\\ \frac{\partial L(\mathbf{x},\lambda)}{\partial x_d} \\ \frac{\partial L(\mathbf{x},\lambda)}{\partial \lambda} \\ \end{bmatrix} = \begin{bmatrix} \frac{\partial f(\mathbf{x})}{\partial x_1}-\lambda \left[ \frac{\partial }{\partial x_1} \left(g(\mathbf{x})-C \right) \right]\\ \vdots\\ \frac{\partial f(\mathbf{x})}{\partial x_d}-\lambda \left[ \frac{\partial }{\partial x_d} \left(g(\mathbf{x})-C \right) \right]\\ -\left(g(\mathbf{x})-C\right) \\ \end{bmatrix} =0 $$ Problem . I believe gradients of Lagrangian are equal to 0 because we want to find a point(s) $\mathbf{x}_{\,0}$ for which gradients are proportional, having the same direction. I followed a couple of theory explanations and cannot figure out why we want gradients to have the same direction, why do we look for a ""point"" where gradients are proportional. If I add multiple constraints, each constraint add its own gradient, how do we find ""the point of the same gradient"" when we have multiple gradients from different constraints? Example . Also, I tried a very trivial example and I failed to understand its output. I guess it is because I don't fully understand how to apply constraints using Lagrange Multipliers. For instance: $f(x)=(x-1)(x-5)=x^2-6x+5 \\ g(x)=x-3 \\ L(x)=x^2-6x+5 - \lambda (x-3) \\ $ This is a parabola and I was looking for max value. If I don't apply constraint, max of this function is in infinity! I tried to apply a line $g(x)=x-3$ as a constraint. I build the Lagrangian, calculated its derivatives and have everything equal to 0, I have following result: $\begin{cases} \frac{\mathrm dL(x)}{\mathrm d x} = 2x-6-\lambda = 0\\ \frac{\mathrm dL(x)}{\mathrm d \lambda} = x-3 = 0\\ \end{cases}$ This is awkward because the first and the second equations are the same if I don't have lambda. Also, substituting second to the first cause $\lambda=0$. When I draw parabola and the line, I have two points of intersection. I don't know why the Lagrange Multiplier doesn't work here, giving me a point $x_0 \approx 5.5$. Thanks","I am trying to understand Lagrange Multiplier. I think I have grasped the theory and can follow less difficult examples but I feel I am still missing full understanding. I think to optimize a function $f(\mathbf{x})$ subject to constraint $g(\mathbf{x})=C$, I can build a new function, Lagrangian, as follows: $$L(\mathbf{x},\lambda)=f(\mathbf{x})-\lambda \left(g(\mathbf{x})-C\right)$$ If I take gradient of the Lagrangian, I'll get a vector function of derivatives ($D+1$) while $D$ is dimension of $\mathbf{x}$: $$ \nabla L(\mathbf{x},\lambda)= \begin{bmatrix} \frac{\partial L(\mathbf{x},\lambda)}{\partial x_1} \\ \vdots\\ \frac{\partial L(\mathbf{x},\lambda)}{\partial x_d} \\ \frac{\partial L(\mathbf{x},\lambda)}{\partial \lambda} \\ \end{bmatrix} = \begin{bmatrix} \frac{\partial f(\mathbf{x})}{\partial x_1}-\lambda \left[ \frac{\partial }{\partial x_1} \left(g(\mathbf{x})-C \right) \right]\\ \vdots\\ \frac{\partial f(\mathbf{x})}{\partial x_d}-\lambda \left[ \frac{\partial }{\partial x_d} \left(g(\mathbf{x})-C \right) \right]\\ -\left(g(\mathbf{x})-C\right) \\ \end{bmatrix} =0 $$ Problem . I believe gradients of Lagrangian are equal to 0 because we want to find a point(s) $\mathbf{x}_{\,0}$ for which gradients are proportional, having the same direction. I followed a couple of theory explanations and cannot figure out why we want gradients to have the same direction, why do we look for a ""point"" where gradients are proportional. If I add multiple constraints, each constraint add its own gradient, how do we find ""the point of the same gradient"" when we have multiple gradients from different constraints? Example . Also, I tried a very trivial example and I failed to understand its output. I guess it is because I don't fully understand how to apply constraints using Lagrange Multipliers. For instance: $f(x)=(x-1)(x-5)=x^2-6x+5 \\ g(x)=x-3 \\ L(x)=x^2-6x+5 - \lambda (x-3) \\ $ This is a parabola and I was looking for max value. If I don't apply constraint, max of this function is in infinity! I tried to apply a line $g(x)=x-3$ as a constraint. I build the Lagrangian, calculated its derivatives and have everything equal to 0, I have following result: $\begin{cases} \frac{\mathrm dL(x)}{\mathrm d x} = 2x-6-\lambda = 0\\ \frac{\mathrm dL(x)}{\mathrm d \lambda} = x-3 = 0\\ \end{cases}$ This is awkward because the first and the second equations are the same if I don't have lambda. Also, substituting second to the first cause $\lambda=0$. When I draw parabola and the line, I have two points of intersection. I don't know why the Lagrange Multiplier doesn't work here, giving me a point $x_0 \approx 5.5$. Thanks",,"['multivariable-calculus', 'optimization', 'lagrange-multiplier']"
46,Prove that a function is contractive,Prove that a function is contractive,,"I'm stuck with the following. I need to prove that in $D:=[0,1]\times[0,1]$ the function $F$ is contractive, where $F:\mathbb{R}^2\rightarrow\mathbb{R}^2$ is defined as: \begin{align} F(x,y):=(\frac{1}{2} e^{-x}+\frac{y}{2},\frac{1}{3} e^{-y}+\frac{x}{3})  \end{align} Actually the problem is to prove: $\exists !_{(x^*,y^*)\in D}: F(x^*,y^*)=(x^*,y^*)$ using Banach fixed point theorem. First of all $D$ must be closed and $F(D)\subset D$, and both are true in this case. But I have to prove $F$ is contractive to use Banach fixed point theorem. I tried Mean Value theorem in both components $F_1(x,y)$ and $F_2(x,y)$ but that didn't help. I would love to see methods for proving contraction in general. Thanks!","I'm stuck with the following. I need to prove that in $D:=[0,1]\times[0,1]$ the function $F$ is contractive, where $F:\mathbb{R}^2\rightarrow\mathbb{R}^2$ is defined as: \begin{align} F(x,y):=(\frac{1}{2} e^{-x}+\frac{y}{2},\frac{1}{3} e^{-y}+\frac{x}{3})  \end{align} Actually the problem is to prove: $\exists !_{(x^*,y^*)\in D}: F(x^*,y^*)=(x^*,y^*)$ using Banach fixed point theorem. First of all $D$ must be closed and $F(D)\subset D$, and both are true in this case. But I have to prove $F$ is contractive to use Banach fixed point theorem. I tried Mean Value theorem in both components $F_1(x,y)$ and $F_2(x,y)$ but that didn't help. I would love to see methods for proving contraction in general. Thanks!",,"['real-analysis', 'multivariable-calculus', 'fixed-point-theorems']"
47,"Second order partial of $f(x,y)=\frac{xy(x^2-y^2)}{x^2+y^2}$ [duplicate]",Second order partial of  [duplicate],"f(x,y)=\frac{xy(x^2-y^2)}{x^2+y^2}","This question already has an answer here : partial derivation 2nd order for a function defined by parts (1 answer) Closed 2 years ago . Consider the function $f(x,y)=\dfrac{xy(x^2-y^2)}{x^2+y^2}$ for $(x,y) \neq (0,0)$, $f=0$ otherwise. I have to compute $\dfrac{d^2f}{dydx}(0,0)$. I know that I have to calculate $\frac{df}{dx}$ first. But that is , $\frac{df}{dx} = \frac{\partial f}{\partial y}\frac{dy}{dx}+\frac{\partial f}{\partial x}$. When I put it in wolframalpha, it gives me this calculation, and also just $\frac{\partial f}{\partial x}$ as an alternative form. wolframalpha calculation link Why do I just ignore the y'? I don't know what y(x) is, as a function of x.  y and x are independent functions, no?","This question already has an answer here : partial derivation 2nd order for a function defined by parts (1 answer) Closed 2 years ago . Consider the function $f(x,y)=\dfrac{xy(x^2-y^2)}{x^2+y^2}$ for $(x,y) \neq (0,0)$, $f=0$ otherwise. I have to compute $\dfrac{d^2f}{dydx}(0,0)$. I know that I have to calculate $\frac{df}{dx}$ first. But that is , $\frac{df}{dx} = \frac{\partial f}{\partial y}\frac{dy}{dx}+\frac{\partial f}{\partial x}$. When I put it in wolframalpha, it gives me this calculation, and also just $\frac{\partial f}{\partial x}$ as an alternative form. wolframalpha calculation link Why do I just ignore the y'? I don't know what y(x) is, as a function of x.  y and x are independent functions, no?",,"['multivariable-calculus', 'partial-derivative']"
48,Prove equality of two vectors if they have equal divergence and equal curls,Prove equality of two vectors if they have equal divergence and equal curls,,"I have following question: Fields with equal divergence and equal curls $F_1$ and $F_2$ are two vectors fields, you may write them as $F_1 = M_1i+N_1j+P_1k$, $F_2 = M_2i+N_2j+P_2k$. Suppose that $\nabla \cdot F_1 = \nabla \cdot F_2$ and $\nabla \times F_1 = \nabla \times F_2$ over a region D enclosed by the oriented surface S with outward unit normal n and that $F_1 \cdot n = F_2 \cdot n$ on S. Prove that $F_1=F_2$ throughout D. I came across this question when studying ""Stokes's Theorem and Divergence Theorem"" in Thomas Calculus, so I suppose we should use either of them to solve this, except I don't know how. All I could figure out is  $$F_1 \cdot n = F_2 \cdot n\Rightarrow\iint\limits_s F_1 \cdot n \, d\sigma = \iint\limits_s F_2 \cdot n \, d\sigma\Rightarrow\iiint\limits_D \nabla \times F_1\, dv = \iiint\limits_D \nabla \times F_2 \, dv,$$ the last step using divergence theorem. Or $$\nabla \times F_1 = \nabla \times F_2 \Rightarrow\iint\limits_s \nabla \times F_1\cdot n \, d\sigma= \iint\limits_s \nabla \times F_2 \cdot n \, d\sigma \Rightarrow \oint\limits_c F_1 \cdot dr =\oint\limits_c F_2 \cdot dr,$$ the last step using Stokes's theorem. This is all I get, then what? Am I on a wrong path? Can anyone solve this problem? Thanks.","I have following question: Fields with equal divergence and equal curls $F_1$ and $F_2$ are two vectors fields, you may write them as $F_1 = M_1i+N_1j+P_1k$, $F_2 = M_2i+N_2j+P_2k$. Suppose that $\nabla \cdot F_1 = \nabla \cdot F_2$ and $\nabla \times F_1 = \nabla \times F_2$ over a region D enclosed by the oriented surface S with outward unit normal n and that $F_1 \cdot n = F_2 \cdot n$ on S. Prove that $F_1=F_2$ throughout D. I came across this question when studying ""Stokes's Theorem and Divergence Theorem"" in Thomas Calculus, so I suppose we should use either of them to solve this, except I don't know how. All I could figure out is  $$F_1 \cdot n = F_2 \cdot n\Rightarrow\iint\limits_s F_1 \cdot n \, d\sigma = \iint\limits_s F_2 \cdot n \, d\sigma\Rightarrow\iiint\limits_D \nabla \times F_1\, dv = \iiint\limits_D \nabla \times F_2 \, dv,$$ the last step using divergence theorem. Or $$\nabla \times F_1 = \nabla \times F_2 \Rightarrow\iint\limits_s \nabla \times F_1\cdot n \, d\sigma= \iint\limits_s \nabla \times F_2 \cdot n \, d\sigma \Rightarrow \oint\limits_c F_1 \cdot dr =\oint\limits_c F_2 \cdot dr,$$ the last step using Stokes's theorem. This is all I get, then what? Am I on a wrong path? Can anyone solve this problem? Thanks.",,"['multivariable-calculus', 'vector-analysis']"
49,Twice differentiable implies equality of mixed partials?,Twice differentiable implies equality of mixed partials?,,"I have seen the following theorem claimed in several places: Claim: If a function $f$ of two variables has partial derivatives $f_x$ and $f_y$ which are defined in a neighborhood of $(a,b)$ and are also differentiable at $(a,b)$, then the mixed partials are equal: $f_{xy}= f_{yx}$. This is a stronger claim than what I find in most textbooks and also on Wikipedia , which also assumes that the second partial derivatives are continuous. If the above claim is true, where can I find a proof of it?  And if not, what is a counterexample?  (The latter is basically this question .) It is stated on page 201 or 220 (maybe depending on edition) of Advanced Calculus by Angus Taylor, but no proof is given. I have also found it in a 1908 paper called On Differentials by W. H. Young in Proc. LMS, but I don't believe the proof given.  He uses differentiability of $f_x$ to show that $$ \frac{f_x(x+h,y+k) - f_x(x+h,y)}{k} = \frac{h}{k}e + f_{yx} + e' $$ where $e,e'\to 0$ as $(h,k)\to 0$, and therefore that the LHS goes to $f_{yx}$ as $(h,k)\to 0$ ""provided $h/k$ does not become infinite"".  Similarly, $\frac{f_y(x+h,y+k) - f_y(x,y+k)}{h} \to f_{xy}$ as $(h,k)\to 0$ ""provided $k/h$ does not become infinite"".  Then he roughly follows the usual proof of equality of mixed partials, using the mean value theorem to show that $$ \frac{f(x+h,y+k)-f(x+h,y) - f(x,y+k) + f(x,y)}{hk} = \frac{f_y(x+h,y+\theta k) - f_y(x,y+\theta k)}{h} $$ for some $0<\theta<1$, and dually.  Then he says ""if $(h,k)$ moves towards $(0,0)$ in such a way that $h/k$ has not zero as limit, $h/\theta k$ will not have zero for a limit"", so that the RHS above goes to $f_{xy}$, and dually also to $f_{yx}$.  But it seems to me that $\theta$ depends on $h$ and $k$, so there is no reason it might not blow up as $(h,k)\to 0$ even if $h/k$ has a finite nonzero limit.  Is there some reason why this works?","I have seen the following theorem claimed in several places: Claim: If a function $f$ of two variables has partial derivatives $f_x$ and $f_y$ which are defined in a neighborhood of $(a,b)$ and are also differentiable at $(a,b)$, then the mixed partials are equal: $f_{xy}= f_{yx}$. This is a stronger claim than what I find in most textbooks and also on Wikipedia , which also assumes that the second partial derivatives are continuous. If the above claim is true, where can I find a proof of it?  And if not, what is a counterexample?  (The latter is basically this question .) It is stated on page 201 or 220 (maybe depending on edition) of Advanced Calculus by Angus Taylor, but no proof is given. I have also found it in a 1908 paper called On Differentials by W. H. Young in Proc. LMS, but I don't believe the proof given.  He uses differentiability of $f_x$ to show that $$ \frac{f_x(x+h,y+k) - f_x(x+h,y)}{k} = \frac{h}{k}e + f_{yx} + e' $$ where $e,e'\to 0$ as $(h,k)\to 0$, and therefore that the LHS goes to $f_{yx}$ as $(h,k)\to 0$ ""provided $h/k$ does not become infinite"".  Similarly, $\frac{f_y(x+h,y+k) - f_y(x,y+k)}{h} \to f_{xy}$ as $(h,k)\to 0$ ""provided $k/h$ does not become infinite"".  Then he roughly follows the usual proof of equality of mixed partials, using the mean value theorem to show that $$ \frac{f(x+h,y+k)-f(x+h,y) - f(x,y+k) + f(x,y)}{hk} = \frac{f_y(x+h,y+\theta k) - f_y(x,y+\theta k)}{h} $$ for some $0<\theta<1$, and dually.  Then he says ""if $(h,k)$ moves towards $(0,0)$ in such a way that $h/k$ has not zero as limit, $h/\theta k$ will not have zero for a limit"", so that the RHS above goes to $f_{xy}$, and dually also to $f_{yx}$.  But it seems to me that $\theta$ depends on $h$ and $k$, so there is no reason it might not blow up as $(h,k)\to 0$ even if $h/k$ has a finite nonzero limit.  Is there some reason why this works?",,"['real-analysis', 'multivariable-calculus']"
50,"Find all critical points of $f(x,y) = x^3 - 12xy + 8y^3$ and state maximum, minimum, or saddle points.","Find all critical points of  and state maximum, minimum, or saddle points.","f(x,y) = x^3 - 12xy + 8y^3","Find all critical points of $f(x,y) = x^3 - 12xy  + 8y^3$ and state whether the function has a relative minimum, relative maximum, or a saddle at the critical points. So I have: $f_x = 3x^2 -12 y$ $f_y = -12x + 24y^2$ $f_{xx} = 6x$ $f_{yy} = 48y$ $f_{xy} = -12$ I found that my critical points were: $(0,0)$ and $(2,1)$, but I still need to classify them. My question is, how do I check if $f_{xx}$ is positive or negative? This seems like a silly question (I'm sure it is), but do I plug in $(0,0)$ for $f_{xx}$? Wouldn't that then be $6\cdot 0 > 0$? That seems wrong.","Find all critical points of $f(x,y) = x^3 - 12xy  + 8y^3$ and state whether the function has a relative minimum, relative maximum, or a saddle at the critical points. So I have: $f_x = 3x^2 -12 y$ $f_y = -12x + 24y^2$ $f_{xx} = 6x$ $f_{yy} = 48y$ $f_{xy} = -12$ I found that my critical points were: $(0,0)$ and $(2,1)$, but I still need to classify them. My question is, how do I check if $f_{xx}$ is positive or negative? This seems like a silly question (I'm sure it is), but do I plug in $(0,0)$ for $f_{xx}$? Wouldn't that then be $6\cdot 0 > 0$? That seems wrong.",,"['multivariable-calculus', 'optimization']"
51,How to determine sign of second derivative from contour plot?,How to determine sign of second derivative from contour plot?,,"I am working on a practice problem for my Calculus 3 course. I am given a contour plot and one of the questions is to determine the sign of the second derivative with respect to $x$ at $(3,2)$: $f_{xx}(3,2)$. I have no clue how to determine this?  Could someone please explain the process of how to figure this out merely from a graph? Thanks. Contour Plot:","I am working on a practice problem for my Calculus 3 course. I am given a contour plot and one of the questions is to determine the sign of the second derivative with respect to $x$ at $(3,2)$: $f_{xx}(3,2)$. I have no clue how to determine this?  Could someone please explain the process of how to figure this out merely from a graph? Thanks. Contour Plot:",,"['calculus', 'multivariable-calculus']"
52,How to graph gradient vector?,How to graph gradient vector?,,"I'm working on a practice problem for my Calculus 3 course.  It gives the function: $z=x^2+y^2$, and asks to graph the contours for $c=1,2,3$.  Than asks to calculate the gradient at point $(2,1)$ and graph the result. I'm fine with the first part.  I'm letting z=c and solving for y, then graphing the result.It's the gradient portion I'm having issues with. The gradient I came up with is: $\nabla(x^2+y^2)=\langle 2x,2y\rangle$, at $(2,1)$, =  (4,2) I'm not exactly sure how to graph this.  Am I supposed to graph a line from $(2,1)$ to $(4,2)$ or what?  Any help/explanation would be very much appreciated!","I'm working on a practice problem for my Calculus 3 course.  It gives the function: $z=x^2+y^2$, and asks to graph the contours for $c=1,2,3$.  Than asks to calculate the gradient at point $(2,1)$ and graph the result. I'm fine with the first part.  I'm letting z=c and solving for y, then graphing the result.It's the gradient portion I'm having issues with. The gradient I came up with is: $\nabla(x^2+y^2)=\langle 2x,2y\rangle$, at $(2,1)$, =  (4,2) I'm not exactly sure how to graph this.  Am I supposed to graph a line from $(2,1)$ to $(4,2)$ or what?  Any help/explanation would be very much appreciated!",,"['calculus', 'multivariable-calculus']"
53,Equation of the Plane,Equation of the Plane,,"I have been working through all the problems in my textbook and I have finally got to a difficult one. The problem ask Find the equation of the plane.The plane that passes through the points $(-1,2,1)$ and contains the line of intersection of the planes $x+y-z =2$ and $2x-y+3z=1$ So far I found the direction vector of the line of intersection to be $<1,-2,1>$ and I have identified a point on this line when $x=0$ to be $(0,5,-1)$. I do not know how to find the desired plane from here. Any assistance would be appreciated.","I have been working through all the problems in my textbook and I have finally got to a difficult one. The problem ask Find the equation of the plane.The plane that passes through the points $(-1,2,1)$ and contains the line of intersection of the planes $x+y-z =2$ and $2x-y+3z=1$ So far I found the direction vector of the line of intersection to be $<1,-2,1>$ and I have identified a point on this line when $x=0$ to be $(0,5,-1)$. I do not know how to find the desired plane from here. Any assistance would be appreciated.",,['multivariable-calculus']
54,Showing something isn't a manifold,Showing something isn't a manifold,,"So I'm following some notes that are introducing manifolds with pretty minimal prerequisites.  What I want to do is show where the image of $\phi: \mathbb{R}\rightarrow \mathbb{R^2}$ $t\mapsto (t-\sin(t),1-\cos(t))$ isn't a manifold.  Since this is the standard cycloid, it's pretty clear that things go bad at the cusps, but how do I rigorously show that $\phi(\mathbb{R})$ isn't a manifold there?  The derivative of $\phi$ isn't 1:1, but that's not enough is it?  How do I know there's not some better parametrization of $\phi(\mathbb{R})$ around the cusps?","So I'm following some notes that are introducing manifolds with pretty minimal prerequisites.  What I want to do is show where the image of $\phi: \mathbb{R}\rightarrow \mathbb{R^2}$ $t\mapsto (t-\sin(t),1-\cos(t))$ isn't a manifold.  Since this is the standard cycloid, it's pretty clear that things go bad at the cusps, but how do I rigorously show that $\phi(\mathbb{R})$ isn't a manifold there?  The derivative of $\phi$ isn't 1:1, but that's not enough is it?  How do I know there's not some better parametrization of $\phi(\mathbb{R})$ around the cusps?",,"['multivariable-calculus', 'manifolds']"
55,Converting an integral from Cartesian to Polar coordinates.,Converting an integral from Cartesian to Polar coordinates.,,"I'm trying to evaluate the double integral $$\int_0^{10}\int_0^{(5/2)\sqrt{4-x^2}}4-x^2-\frac{4}{25}y^2 \; dy \; dx$$ but using Cartesian coordinates requires the idea of trigonometric substitution and the limits aren't very nice either. So, my question is, how can I change this integral into another one in polar coordinates that would make the evaluation easier? I don't know if this helps, but the original problem was to find the solid bound in the first octant by $25z=100-25x^2-4y^2$. Thank you.","I'm trying to evaluate the double integral $$\int_0^{10}\int_0^{(5/2)\sqrt{4-x^2}}4-x^2-\frac{4}{25}y^2 \; dy \; dx$$ but using Cartesian coordinates requires the idea of trigonometric substitution and the limits aren't very nice either. So, my question is, how can I change this integral into another one in polar coordinates that would make the evaluation easier? I don't know if this helps, but the original problem was to find the solid bound in the first octant by $25z=100-25x^2-4y^2$. Thank you.",,['multivariable-calculus']
56,Changing variables in a partial derivative,Changing variables in a partial derivative,,"If I have the equation and function $$f_1(x_1,x_2,x_3,...,x_n) = 0,\qquad x_1 = g_1(y_1, y_2, y_3,...,y_m)$$ then what is $\frac {\partial f_1}{\partial x_1}$ in terms of $g_1$ and $y_i$?","If I have the equation and function $$f_1(x_1,x_2,x_3,...,x_n) = 0,\qquad x_1 = g_1(y_1, y_2, y_3,...,y_m)$$ then what is $\frac {\partial f_1}{\partial x_1}$ in terms of $g_1$ and $y_i$?",,['multivariable-calculus']
57,How do I differentiate this integral?,How do I differentiate this integral?,,"That is: $$\left(\int_{a(x)}^{b(x)}\!f(x,t)\,dt\right)'$$ I don't know how to differentiate a integral if functions of $x$ are at its limits. Can you guys show me how to do this?","That is: $$\left(\int_{a(x)}^{b(x)}\!f(x,t)\,dt\right)'$$ I don't know how to differentiate a integral if functions of $x$ are at its limits. Can you guys show me how to do this?",,"['calculus', 'multivariable-calculus']"
58,Understanding this partial derivative problem,Understanding this partial derivative problem,,"Problem: Considering $x$ and $y$ as independent    variables, find $\dfrac{\partial  r}{\partial x}, \dfrac{\partial  r}{\partial y}, \dfrac{\partial  \theta}{\partial x}, \dfrac{\partial  \theta}{\partial y}$ when $x = e^{2r}  \cos \theta, y = e^{3r} \sin \theta$ . Solution: First differentiate the given    relations with respect to $x$ : $1 = 2e^{2r} \cos \theta  \dfrac{\partial r}{\partial x} - e^{2r}  \sin \theta \dfrac{\partial  \theta}{\partial x}$ and $0 =  3e^{3r}\sin \theta \dfrac{\partial  r}{\partial x} + e^{3r} \cos \theta  \dfrac{\partial \theta}{\partial x}$ . Then solve simultaneously to obtain $\dfrac{\partial r}{\partial x} =  \dfrac{\cos \theta}{e^{2r}(2+\sin^{2}  \theta)}$ and $\dfrac{\partial  \theta}{\partial x} = - \dfrac{3 \sin  \theta}{e^{2r}(2+sin^2 \theta)}$ Question: (1) So first of all, why does differentiating with respect to $x$ result in $\dfrac{\partial r}{\partial x}$ and $\dfrac{\partial \theta}{\partial x}$ (by the way what are these called?)? Is this because the problem says "" $x$ and $y$ as independent variables"" ? My initial reaction was to do $r$ and $\theta$ separately while regarding all other variables as constants... This is implicit (partial?) differentiation, right? How should I understand what is being done here? A lot of times it seems that things like this turn out to really just be a mapping. Can I think of this that way as well? I couldn't even say what the domain and codomain would be... Whenever I perform this operation, do I just take partial derivatives of both sides treating all independent variables (other than the variable with respect to which I am differentiating) as constants and all non-independent (dependent?) variables as variables that need to be differentiated and will have that partial derivative symbol? Once I accept that I can see how they got the first implicit partial differentiation (if that's what it is called). (2) What do they mean by ""solve simultaneously"" ? I tried to solve for each (again I don't know what they're called yet) ""partial differential"" resulting in: $$\dfrac{\partial \theta}{\partial x} = -\dfrac{\sin \theta \frac{\partial r}{\partial x}}{\cos \theta}$$ and $$\dfrac{\partial r}{\partial x} = \dfrac{1+e^{2r}\sin \theta \frac{\partial \theta}{\partial x}}{2e^{2r}\cos \theta}$$ But I couldn't get the same answer... I tried to substitute this further, but looking at that answer, I was sure I was missing something... Could somebody please show me what to do? Thank you in advance for any help!","Problem: Considering and as independent    variables, find when . Solution: First differentiate the given    relations with respect to : and . Then solve simultaneously to obtain and Question: (1) So first of all, why does differentiating with respect to result in and (by the way what are these called?)? Is this because the problem says "" and as independent variables"" ? My initial reaction was to do and separately while regarding all other variables as constants... This is implicit (partial?) differentiation, right? How should I understand what is being done here? A lot of times it seems that things like this turn out to really just be a mapping. Can I think of this that way as well? I couldn't even say what the domain and codomain would be... Whenever I perform this operation, do I just take partial derivatives of both sides treating all independent variables (other than the variable with respect to which I am differentiating) as constants and all non-independent (dependent?) variables as variables that need to be differentiated and will have that partial derivative symbol? Once I accept that I can see how they got the first implicit partial differentiation (if that's what it is called). (2) What do they mean by ""solve simultaneously"" ? I tried to solve for each (again I don't know what they're called yet) ""partial differential"" resulting in: and But I couldn't get the same answer... I tried to substitute this further, but looking at that answer, I was sure I was missing something... Could somebody please show me what to do? Thank you in advance for any help!","x y \dfrac{\partial
 r}{\partial x}, \dfrac{\partial
 r}{\partial y}, \dfrac{\partial
 \theta}{\partial x}, \dfrac{\partial
 \theta}{\partial y} x = e^{2r}
 \cos \theta, y = e^{3r} \sin \theta x 1 = 2e^{2r} \cos \theta
 \dfrac{\partial r}{\partial x} - e^{2r}
 \sin \theta \dfrac{\partial
 \theta}{\partial x} 0 =
 3e^{3r}\sin \theta \dfrac{\partial
 r}{\partial x} + e^{3r} \cos \theta
 \dfrac{\partial \theta}{\partial x} \dfrac{\partial r}{\partial x} =
 \dfrac{\cos \theta}{e^{2r}(2+\sin^{2}
 \theta)} \dfrac{\partial
 \theta}{\partial x} = - \dfrac{3 \sin
 \theta}{e^{2r}(2+sin^2 \theta)} x \dfrac{\partial r}{\partial x} \dfrac{\partial \theta}{\partial x} x y r \theta \dfrac{\partial \theta}{\partial x} = -\dfrac{\sin \theta \frac{\partial r}{\partial x}}{\cos \theta} \dfrac{\partial r}{\partial x} = \dfrac{1+e^{2r}\sin \theta \frac{\partial \theta}{\partial x}}{2e^{2r}\cos \theta}",['multivariable-calculus']
59,Using infinitesimals in multivariable calculus,Using infinitesimals in multivariable calculus,,"In ordinary calculus,if we don't make any absurd notation,the use of infinitesimals is extremely useful and intuitive which ended up giving correct results. For example $\frac{dy}{dx}=\frac{\frac{dy}{dt}}{\frac{dx}{dt}}$ are obvious if we treat $dy,dx,dt$ as infinitesimals. Can we do the same in case of multivariable calculus? For example if $V$ is a function of $x$ and $y$ and $x,y$ are both functions of $r,\theta$ ,can we still do stuffs like $\frac{\delta V}{\delta x}=\frac{\frac{\delta V}{\delta \theta}}{\frac{\delta x}{\delta \theta}}$ ? Or are there scenarios where such use of infinitesimals lead to incorrect results?","In ordinary calculus,if we don't make any absurd notation,the use of infinitesimals is extremely useful and intuitive which ended up giving correct results. For example are obvious if we treat as infinitesimals. Can we do the same in case of multivariable calculus? For example if is a function of and and are both functions of ,can we still do stuffs like ? Or are there scenarios where such use of infinitesimals lead to incorrect results?","\frac{dy}{dx}=\frac{\frac{dy}{dt}}{\frac{dx}{dt}} dy,dx,dt V x y x,y r,\theta \frac{\delta V}{\delta x}=\frac{\frac{\delta V}{\delta \theta}}{\frac{\delta x}{\delta \theta}}","['multivariable-calculus', 'functions', 'partial-derivative', 'infinitesimals']"
60,Approximation used in Hutton's computation in the Schiehallion experiment,Approximation used in Hutton's computation in the Schiehallion experiment,,"I have been trying to understand the numerical integration approach employed by Charles Hutton in calculating the gravitational attraction of the Schiehallion mountain on a plumb line. The paper is available here: https://royalsocietypublishing.org/doi/10.1098/rstl.1778.0034 . In the paper, he provides a geometric argument and I'm trying to formulate it in terms of calculus. The relevant pages (if you are interested) are 750-755. The basic approach can be seen in the following diagram: The plumb line is at A in the figure. He is using cylindrical coordinates to do the integration. He takes the meridian going toward north to be the positive X-axis and computes the X-component of the gravitational attraction of each mass element on the plumb line. If you have a mass element enclosed in the ring between $r = r_1$ and $r = r_2$ , and the sector between $\theta = \theta_1$ and $\theta = \theta_2$ and planes $z = 0$ and $z = h$ , the X-component of its gravitational attraction would be: $$\int_{\theta_1}^{\theta_2}\int_{0}^{h}\int_{r_1}^{r_2} Gm\rho\;\frac{r^2}{(r^2 + z^2)^\frac{3}{2}}\;\cos{\theta}\;dr\;d\theta\;dz$$ Here, $G$ is the gravitational constant, $m$ is the mass of the plumb bob and $\rho$ is the density of the mountain. The integral (if you factor out $Gm\rho$ ) evaluates to: $$h\;(\sin{\theta_2} - \sin{\theta_1})\;\Biggl\{\log\Biggl(\sqrt{\frac{r_2^2}{h^2} + 1} + \frac{r_2}{h}\Biggr) - \log\Biggl(\sqrt{\frac{r_1^2}{h^2} + 1} + \frac{r_1}{h}\Biggr)\Biggr\}$$ (I hope I have calculated the integral correctly.) He seems to be assuming that $r_2$ is close to $r_1$ and $h \ll \frac{r_1 + r_2}{2}$ . With these assumptions, it seems to me that the following approximation should hold: $$\Biggl\{\log\Biggl(\sqrt{\frac{r_2^2}{h^2} + 1} + \frac{r_2}{h}\Biggr) - \log\Biggl(\sqrt{\frac{r_1^2}{h^2} + 1} + \frac{r_1}{h}\Biggr)\Biggr\} \approx \frac{r_2 - r_1}{\sqrt{(\frac{r_1 + r_2}{2})^2 + h^2}}$$ But I'm unable to see how to derive this approximation. I would greatly appreciate any help you can provide in helping me understand this. Thanks!!","I have been trying to understand the numerical integration approach employed by Charles Hutton in calculating the gravitational attraction of the Schiehallion mountain on a plumb line. The paper is available here: https://royalsocietypublishing.org/doi/10.1098/rstl.1778.0034 . In the paper, he provides a geometric argument and I'm trying to formulate it in terms of calculus. The relevant pages (if you are interested) are 750-755. The basic approach can be seen in the following diagram: The plumb line is at A in the figure. He is using cylindrical coordinates to do the integration. He takes the meridian going toward north to be the positive X-axis and computes the X-component of the gravitational attraction of each mass element on the plumb line. If you have a mass element enclosed in the ring between and , and the sector between and and planes and , the X-component of its gravitational attraction would be: Here, is the gravitational constant, is the mass of the plumb bob and is the density of the mountain. The integral (if you factor out ) evaluates to: (I hope I have calculated the integral correctly.) He seems to be assuming that is close to and . With these assumptions, it seems to me that the following approximation should hold: But I'm unable to see how to derive this approximation. I would greatly appreciate any help you can provide in helping me understand this. Thanks!!",r = r_1 r = r_2 \theta = \theta_1 \theta = \theta_2 z = 0 z = h \int_{\theta_1}^{\theta_2}\int_{0}^{h}\int_{r_1}^{r_2} Gm\rho\;\frac{r^2}{(r^2 + z^2)^\frac{3}{2}}\;\cos{\theta}\;dr\;d\theta\;dz G m \rho Gm\rho h\;(\sin{\theta_2} - \sin{\theta_1})\;\Biggl\{\log\Biggl(\sqrt{\frac{r_2^2}{h^2} + 1} + \frac{r_2}{h}\Biggr) - \log\Biggl(\sqrt{\frac{r_1^2}{h^2} + 1} + \frac{r_1}{h}\Biggr)\Biggr\} r_2 r_1 h \ll \frac{r_1 + r_2}{2} \Biggl\{\log\Biggl(\sqrt{\frac{r_2^2}{h^2} + 1} + \frac{r_2}{h}\Biggr) - \log\Biggl(\sqrt{\frac{r_1^2}{h^2} + 1} + \frac{r_1}{h}\Biggr)\Biggr\} \approx \frac{r_2 - r_1}{\sqrt{(\frac{r_1 + r_2}{2})^2 + h^2}},"['multivariable-calculus', 'numerical-methods', 'approximation', 'approximation-theory']"
61,Lagrange Multipliers. What do I do if the Lagrange multiplier is 0 or the gradient of the constraint is 0?,Lagrange Multipliers. What do I do if the Lagrange multiplier is 0 or the gradient of the constraint is 0?,,"Consider the example below. Here, We found that $\nabla g$ is only zero at $(0, 0)$ but we see that the point $(0,0)$ does not satisfy $g(x,y) = 0$ . But I don't understand why we need to ensure that that the $\nabla g \ne 0$ . What would it mean if $\nabla g = 0$ satisfies the constraint? What would I need to do in this case? How would my steps change? Also, later they mention that $\lambda$ is not equal to $0$ since it would lead to a contradiction if it did. Once again, I don't understand why we need to make sure that $\lambda \ne 0$ . Why do we need to make that step? What happens if $\lambda$ can be zero? What would that mean? What measures should I take if that happens? In brief, I'm asking what do I do if $\nabla g = 0$ satisfies the constraint or $\lambda = 0$ .","Consider the example below. Here, We found that is only zero at but we see that the point does not satisfy . But I don't understand why we need to ensure that that the . What would it mean if satisfies the constraint? What would I need to do in this case? How would my steps change? Also, later they mention that is not equal to since it would lead to a contradiction if it did. Once again, I don't understand why we need to make sure that . Why do we need to make that step? What happens if can be zero? What would that mean? What measures should I take if that happens? In brief, I'm asking what do I do if satisfies the constraint or .","\nabla g (0, 0) (0,0) g(x,y) = 0 \nabla g \ne 0 \nabla g = 0 \lambda 0 \lambda \ne 0 \lambda \nabla g = 0 \lambda = 0","['calculus', 'multivariable-calculus', 'optimization', 'lagrange-multiplier', 'constraints']"
62,Find the max and min values of a multivariable function on the boundary of a domain,Find the max and min values of a multivariable function on the boundary of a domain,,"I have the multivariable function: $f(x,y) = x^3 + y^3 - 3xy + 2$ I want to find the maximum and minimum values of this function on the domain: $$ D=[(x,y) : x,y\ge 0, x^2 +y^2\le4] $$ I found the partial derivatives to be: $f_x=3x^2 -3y$ $f_y=3y^2-3x$ From here I set both my partial derivatives to equal $0$ and solved for $x$ and $y$ and got $(1,1)$ and $(0,0)$ , I believe these two could potentially be the max/min I'm looking for, however I know there are more possibilities. I then created a function $g$ where $g=x^2 + y^2$ ( $g = 4$ ) I know that $\nabla f= \lambda \nabla g$ so $(3x^2-3y,3y^2-3x) = \lambda(2x,2y) $ I then set up $3$ equations: $\lambda x = \frac32  (x^2) - \frac32  (y)$ $\lambda y = \frac32  (y^2) - \frac32  (x)$ $x^2 + y^2 = 4$ But I'm not sure where to go from here and how to solve these equations for $x$ and $y$ and $\lambda$ . To be honest I'm not even sure if this is the best way to go about answering this question. If anyone could show me how I would find the maximum and minimum of the function on this domain with a better method, or help me continue with mine, it would really help.","I have the multivariable function: I want to find the maximum and minimum values of this function on the domain: I found the partial derivatives to be: From here I set both my partial derivatives to equal and solved for and and got and , I believe these two could potentially be the max/min I'm looking for, however I know there are more possibilities. I then created a function where ( ) I know that so I then set up equations: But I'm not sure where to go from here and how to solve these equations for and and . To be honest I'm not even sure if this is the best way to go about answering this question. If anyone could show me how I would find the maximum and minimum of the function on this domain with a better method, or help me continue with mine, it would really help.","f(x,y) = x^3 + y^3 - 3xy + 2  D=[(x,y) : x,y\ge 0, x^2 +y^2\le4]  f_x=3x^2 -3y f_y=3y^2-3x 0 x y (1,1) (0,0) g g=x^2 + y^2 g = 4 \nabla f= \lambda \nabla g (3x^2-3y,3y^2-3x) = \lambda(2x,2y)  3 \lambda x = \frac32  (x^2) - \frac32  (y) \lambda y = \frac32  (y^2) - \frac32  (x) x^2 + y^2 = 4 x y \lambda","['multivariable-calculus', 'partial-derivative', 'maxima-minima', 'lagrange-multiplier']"
63,Is true that a closed $k$-form defined on an open connected set is exact iff its integral on a compact $k$-manifold without boundary is zero?,Is true that a closed -form defined on an open connected set is exact iff its integral on a compact -manifold without boundary is zero?,k k,"So let be $U$ an open connected set of $\Bbb R^n$ and thus let be $\omega$ a $k$ -form there defined. So if $\omega$ is exact than by the Stoke's theorem $$ \int_M\omega=0 $$ where $M$ is a compact $k$ -manifold contained in $U$ . However if $\omega$ is such that the last identity holds for any compact $k$ -manifold contained in $U$ then is it exact? How prove it? Is it much complicate? What I have to study to prove it? Where can I find the proof? So could someone help me, please?","So let be an open connected set of and thus let be a -form there defined. So if is exact than by the Stoke's theorem where is a compact -manifold contained in . However if is such that the last identity holds for any compact -manifold contained in then is it exact? How prove it? Is it much complicate? What I have to study to prove it? Where can I find the proof? So could someone help me, please?","U \Bbb R^n \omega k \omega 
\int_M\omega=0
 M k U \omega k U","['multivariable-calculus', 'differential-geometry', 'manifolds', 'differential-forms', 'de-rham-cohomology']"
64,Why is this function not differentiable?,Why is this function not differentiable?,,"Why is the function $$\begin{align} f(x,y)=\begin{cases}\frac{y^3}{x^2+y^2},&(x,y)\ne(0,0)\\\\ 0,&(x,y)=(0,0) \end{cases} \end{align}$$ not differentiable at $(0,0)$ ? By setting $x=0$ or $y=0$ I would get $f_x(0,0)=0$ , $f_y(0,0)=-2$ and when I look at the 3-D Surface plot, $f(x,y)$ looks quite ""normal"". I gaze already hours on it and cannot identify the problem. Obviously the partial derivates exist, but are not continuous, but I cannot see where. Is there an analytic way, I can obtain the result from? How can I come to the conclusion, without gazing at the curve?","Why is the function not differentiable at ? By setting or I would get , and when I look at the 3-D Surface plot, looks quite ""normal"". I gaze already hours on it and cannot identify the problem. Obviously the partial derivates exist, but are not continuous, but I cannot see where. Is there an analytic way, I can obtain the result from? How can I come to the conclusion, without gazing at the curve?","\begin{align}
f(x,y)=\begin{cases}\frac{y^3}{x^2+y^2},&(x,y)\ne(0,0)\\\\
0,&(x,y)=(0,0)
\end{cases}
\end{align} (0,0) x=0 y=0 f_x(0,0)=0 f_y(0,0)=-2 f(x,y)","['calculus', 'multivariable-calculus', 'derivatives', 'partial-derivative']"
65,"Find the volume formula of a simplex proving that $\int_0^1\int_0^{1-x_n}...\int_0^{1-(x_n+...+x_2)}1\,\,\,dx_1...dx_n=\frac 1{n!}$",Find the volume formula of a simplex proving that,"\int_0^1\int_0^{1-x_n}...\int_0^{1-(x_n+...+x_2)}1\,\,\,dx_1...dx_n=\frac 1{n!}","Definition If $x_0,...,x_n$ are $(n+1)$ affinely independent point of $\Bbb R^n$ (which means that the vectors $(x_1-x_0),...,(x_n-x_0)$ are linearly independent) then simplex determined by them is the set $$ S:=\Biggl\{x\in\Bbb R^n: x=\alpha_1v_1+...+\alpha_nv_n, \sum_{i=1}^n\alpha_i\le1\,\,\,\text{and}\,\,\,\alpha_i\ge0\,\,\,\text{for all}\,i\Biggl\} $$ where $v_i:=(x_i-x_0)$ for each $i>0$ . So with the previous definition I try to show that the volume of a symplex $S$ is given by the formula $$ v(S)=\Big|\frac{1}{n!}\det\big[(x_1-x_0),...,(x_n-x_0)\big]\Big| $$ for each $n\in\Bbb N$ . First of all we observe that any simplex is the intersection of two parallelopides (see here for details) so that any simplex is rectifiable (indeed any parallelopiped is rectifiable and the intersection of rectifiable sets is rectifiable too) and thus any continuous function is there integrable. Now if $x_0,...,x_n$ are $(n+1)$ point affinely independent we define the transformation $h:\Bbb R^n\rightarrow\Bbb R^n$ through the condition $$ h(x):=A\cdot x+x_0 $$ where $A$ is the matrix whose $j$ -th column is the vectors $(x_j-x_0)$ for each $j=1,...,n$ . So now we observe that the transformation $h$ carries the simplex $$ E:=\{x\in\Bbb R^n:x_1+...+x_n\le 1\,\,\,\text{and}\,\,\, x_i\ge 0\,\,\,\text{for all}\,i\} $$ onto the simplex $S$ generated by the points $x_0,...,x_n$ . So if we prove that $$ v(E):=\frac 1{n!} $$ for all $n\in\Bbb N$ then by the change variable theorem (it is easy to verify that $h$ is a diffeomorphism) $$ v(S)=\int_S 1=\int_E|\det A|=\frac 1{n!}|\det A| $$ for each $n\in\Bbb N$ . So let's start to prove by induction that $$ v(E)=\frac 1{n!} $$ for each $n\in\Bbb N$ . So if $n=1$ then $E=[0,1]$ and so clearly the formula trivially holds. So we suppose that the formula holds for $(n-1)$ and we prove that it holds for $n$ . So we have to prove that $$ \int_E1=\frac 1{n!} $$ and to do this we will use Fubini's formula. So I have to prove that $$ \int_0^1\int_0^{1-x_n}...\int_0^{1-(x_n+...+x_2)}1\,\,\,dx_1...dx_{n-1}dx_n=\frac 1{n!} $$ but unfortunately I don't be able to prove it. For sake of completeness I point out that it seems that here there is a similar solution to that I gave (see the answer of the professor Blatter) but I don't fully understand it. In particular the solution I linked says that if we define $$ E_\xi:=\{x\in\Bbb R^n:x_1+...+x_{n-1}\le1,\,\,\,\text{and}\,\,\,x_1,...,x_{n-1}\ge 0\,\,\,\text{and}\,\,\,x_n=\xi\} $$ for any $\xi\in[0,1]$ then $E=\bigcup_{\xi\in[0,1]}E_\xi$ and so if we observe that the projection of $E_\xi$ is a $(n-1)$ dimensional simplex then $\int_E 1=\int_0^1(1-x_n)^{n-1}v(E_\xi)\,dx_n=\frac 1 n(1-x_n)^nv(E_\xi)=\frac 1{n!}$ that complete the proof but I don't understand how to prove effectively last equality. So I ask to prove the last equality and then I ask to prove that $h[E]=S$ too. So could someone help me, please?","Definition If are affinely independent point of (which means that the vectors are linearly independent) then simplex determined by them is the set where for each . So with the previous definition I try to show that the volume of a symplex is given by the formula for each . First of all we observe that any simplex is the intersection of two parallelopides (see here for details) so that any simplex is rectifiable (indeed any parallelopiped is rectifiable and the intersection of rectifiable sets is rectifiable too) and thus any continuous function is there integrable. Now if are point affinely independent we define the transformation through the condition where is the matrix whose -th column is the vectors for each . So now we observe that the transformation carries the simplex onto the simplex generated by the points . So if we prove that for all then by the change variable theorem (it is easy to verify that is a diffeomorphism) for each . So let's start to prove by induction that for each . So if then and so clearly the formula trivially holds. So we suppose that the formula holds for and we prove that it holds for . So we have to prove that and to do this we will use Fubini's formula. So I have to prove that but unfortunately I don't be able to prove it. For sake of completeness I point out that it seems that here there is a similar solution to that I gave (see the answer of the professor Blatter) but I don't fully understand it. In particular the solution I linked says that if we define for any then and so if we observe that the projection of is a dimensional simplex then that complete the proof but I don't understand how to prove effectively last equality. So I ask to prove the last equality and then I ask to prove that too. So could someone help me, please?","x_0,...,x_n (n+1) \Bbb R^n (x_1-x_0),...,(x_n-x_0) 
S:=\Biggl\{x\in\Bbb R^n: x=\alpha_1v_1+...+\alpha_nv_n, \sum_{i=1}^n\alpha_i\le1\,\,\,\text{and}\,\,\,\alpha_i\ge0\,\,\,\text{for all}\,i\Biggl\}
 v_i:=(x_i-x_0) i>0 S 
v(S)=\Big|\frac{1}{n!}\det\big[(x_1-x_0),...,(x_n-x_0)\big]\Big|
 n\in\Bbb N x_0,...,x_n (n+1) h:\Bbb R^n\rightarrow\Bbb R^n 
h(x):=A\cdot x+x_0
 A j (x_j-x_0) j=1,...,n h 
E:=\{x\in\Bbb R^n:x_1+...+x_n\le 1\,\,\,\text{and}\,\,\, x_i\ge 0\,\,\,\text{for all}\,i\}
 S x_0,...,x_n 
v(E):=\frac 1{n!}
 n\in\Bbb N h 
v(S)=\int_S 1=\int_E|\det A|=\frac 1{n!}|\det A|
 n\in\Bbb N 
v(E)=\frac 1{n!}
 n\in\Bbb N n=1 E=[0,1] (n-1) n 
\int_E1=\frac 1{n!}
 
\int_0^1\int_0^{1-x_n}...\int_0^{1-(x_n+...+x_2)}1\,\,\,dx_1...dx_{n-1}dx_n=\frac 1{n!}
 
E_\xi:=\{x\in\Bbb R^n:x_1+...+x_{n-1}\le1,\,\,\,\text{and}\,\,\,x_1,...,x_{n-1}\ge 0\,\,\,\text{and}\,\,\,x_n=\xi\}
 \xi\in[0,1] E=\bigcup_{\xi\in[0,1]}E_\xi E_\xi (n-1) \int_E 1=\int_0^1(1-x_n)^{n-1}v(E_\xi)\,dx_n=\frac 1 n(1-x_n)^nv(E_\xi)=\frac 1{n!} h[E]=S","['multivariable-calculus', 'differential-geometry', 'volume', 'multiple-integral', 'simplex']"
66,Show that $g(x)=x + f(x)$ is surjective,Show that  is surjective,g(x)=x + f(x),"Let $f: {\mathbb{R}}^n \rightarrow {\mathbb{R}}^n$ be continuously differentiable and $C \in (0,1)$ a constant, so that ${||Df(x)||}_{op} \leq C$ $\forall x \in {\mathbb{R}}^n$ with $op$ being a operator norm. Show that $g: {\mathbb{R}}^n \rightarrow {\mathbb{R}}^n$ , $g(x)=x+f(x)$ is surjective. I tried following: $g(x)=x+f(x)$ $\Leftrightarrow Dg(x)=Dx+Df(x)$ $\Leftrightarrow {||Df(x)||}_{op}={||Dg(x)-Dx||}_{op} \leq C$ I'm not realy sure how to get on from here. Is it possible to use the sub-additive of matrix norms even if there is a minus in the equation? I'm thankfull for every hint.","Let be continuously differentiable and a constant, so that with being a operator norm. Show that , is surjective. I tried following: I'm not realy sure how to get on from here. Is it possible to use the sub-additive of matrix norms even if there is a minus in the equation? I'm thankfull for every hint.","f: {\mathbb{R}}^n \rightarrow {\mathbb{R}}^n C \in (0,1) {||Df(x)||}_{op} \leq C \forall x \in {\mathbb{R}}^n op g: {\mathbb{R}}^n \rightarrow {\mathbb{R}}^n g(x)=x+f(x) g(x)=x+f(x) \Leftrightarrow Dg(x)=Dx+Df(x) \Leftrightarrow {||Df(x)||}_{op}={||Dg(x)-Dx||}_{op} \leq C","['real-analysis', 'multivariable-calculus', 'normed-spaces']"
67,Divergence of a vector field in cylindrical coordinates,Divergence of a vector field in cylindrical coordinates,,"Let $\bar{F}:\mathbb{R}^3\rightarrow\mathbb{R}^3$ be a vector field such that $\bar{F}(x,y,z)=(x,y,z)$ . Then we know that: $$\nabla\cdot\bar{F}=\frac{\partial\bar{F}_x}{\partial x}+\frac{\partial\bar{F}_y}{\partial y}+\frac{\partial\bar{F}_z}{\partial z} = 1+1+1=3$$ However, we also know that $\bar{F}$ in cylindrical coordinates equals to: $\bar{F}=(r\cos\theta,r\sin\theta,z)$ , and the divergence in cylindrical coordinates is the following: $$\nabla\cdot\bar{F}=\frac{1}{r}\frac{\partial(r\bar{F}_r)}{\partial r}+\frac{1}{r}\frac{\partial(\bar{F}_\theta)}{\partial \theta}+\frac{\partial(\bar{F}_z)}{\partial z}$$ The big question is: what are $\bar{F}_{r,\theta,z}$ ? No matter what I decide them to be, I get weird answers (using the formula above); the ones I got most frequently are $3+\frac{1}{r}$ and $3\cos\theta+1$ , but I believe them both to be incorrect, since I belive the answer must be $3$ : the vector fields are equivalent (I was just changing the coordinates), so their divergences also must be equivalent, am I right? Thanks!","Let be a vector field such that . Then we know that: However, we also know that in cylindrical coordinates equals to: , and the divergence in cylindrical coordinates is the following: The big question is: what are ? No matter what I decide them to be, I get weird answers (using the formula above); the ones I got most frequently are and , but I believe them both to be incorrect, since I belive the answer must be : the vector fields are equivalent (I was just changing the coordinates), so their divergences also must be equivalent, am I right? Thanks!","\bar{F}:\mathbb{R}^3\rightarrow\mathbb{R}^3 \bar{F}(x,y,z)=(x,y,z) \nabla\cdot\bar{F}=\frac{\partial\bar{F}_x}{\partial x}+\frac{\partial\bar{F}_y}{\partial y}+\frac{\partial\bar{F}_z}{\partial z} = 1+1+1=3 \bar{F} \bar{F}=(r\cos\theta,r\sin\theta,z) \nabla\cdot\bar{F}=\frac{1}{r}\frac{\partial(r\bar{F}_r)}{\partial r}+\frac{1}{r}\frac{\partial(\bar{F}_\theta)}{\partial \theta}+\frac{\partial(\bar{F}_z)}{\partial z} \bar{F}_{r,\theta,z} 3+\frac{1}{r} 3\cos\theta+1 3","['calculus', 'multivariable-calculus', 'vector-analysis', 'divergence-operator', 'cylindrical-coordinates']"
68,Lie Bracket of Pushforwards,Lie Bracket of Pushforwards,,"Suppose $f(x,y)=(f_1,f_2,...,f_n)(x,y):\mathbb R^2\to\mathbb R^n$ is a smooth parameterization of a smooth surface $S$ in $\mathbb R^n$ . The pushforwards $\mathrm{d}f\frac{\partial}{\partial x}$ and $\mathrm{d}f\frac{\partial}{\partial y}$ are vector fields in $\mathbb R^n$ tangent to $S$ , so their Lie bracket $X=\left[\mathrm{d}f\frac{\partial}{\partial x},\mathrm{d}f\frac{\partial}{\partial y}\right]$ is too. Therefore $X$ should admit a representation in local coordinates, as the pushforward of some $\left(\alpha\frac{\partial}{\partial x}+\beta\frac{\partial}{\partial y}\right)\in\mathfrak{X}(\mathbb{R}^2)$ for some $\alpha,\beta:\mathbb{R}^2\to\mathbb{R}$ . Is my reasoning above correct? If so, how may we find $\alpha$ and $\beta$ ? If not, what can we say about $X$ ? An example would be extremely helpful. I have seen the formula $[Y,Z]=\left(Y^i\frac{\partial Z^j}{\partial x^i}-Z^i\frac{\partial Y^j}{\partial x^i}\right)\frac{\partial}{\partial x^j}$ , and I think this is what I need, but I do not see how to apply it in this case. I have also seen $\mathrm{d}f\left[\frac{\partial}{\partial x},\frac{\partial}{\partial y}\right]=\left[\mathrm{d}f\frac{\partial}{\partial x},\mathrm{d}f\frac{\partial}{\partial y}\right]$ when $f$ is a local diffeomorphism, but that does not appear to be the case here. Thank you! P.S. this is not homework. I am working through Lee's Introduction to Smooth Manifolds and I cannot find any examples where $S$ is not simply $\mathbb{R}^n$ to answer my question.","Suppose is a smooth parameterization of a smooth surface in . The pushforwards and are vector fields in tangent to , so their Lie bracket is too. Therefore should admit a representation in local coordinates, as the pushforward of some for some . Is my reasoning above correct? If so, how may we find and ? If not, what can we say about ? An example would be extremely helpful. I have seen the formula , and I think this is what I need, but I do not see how to apply it in this case. I have also seen when is a local diffeomorphism, but that does not appear to be the case here. Thank you! P.S. this is not homework. I am working through Lee's Introduction to Smooth Manifolds and I cannot find any examples where is not simply to answer my question.","f(x,y)=(f_1,f_2,...,f_n)(x,y):\mathbb R^2\to\mathbb R^n S \mathbb R^n \mathrm{d}f\frac{\partial}{\partial x} \mathrm{d}f\frac{\partial}{\partial y} \mathbb R^n S X=\left[\mathrm{d}f\frac{\partial}{\partial x},\mathrm{d}f\frac{\partial}{\partial y}\right] X \left(\alpha\frac{\partial}{\partial x}+\beta\frac{\partial}{\partial y}\right)\in\mathfrak{X}(\mathbb{R}^2) \alpha,\beta:\mathbb{R}^2\to\mathbb{R} \alpha \beta X [Y,Z]=\left(Y^i\frac{\partial Z^j}{\partial x^i}-Z^i\frac{\partial Y^j}{\partial x^i}\right)\frac{\partial}{\partial x^j} \mathrm{d}f\left[\frac{\partial}{\partial x},\frac{\partial}{\partial y}\right]=\left[\mathrm{d}f\frac{\partial}{\partial x},\mathrm{d}f\frac{\partial}{\partial y}\right] f S \mathbb{R}^n","['multivariable-calculus', 'differential-geometry', 'vector-fields']"
69,Edit: Show that $f$ is differentiable on $\Bbb{R}^n$ and compute $f'$,Edit: Show that  is differentiable on  and compute,f \Bbb{R}^n f',"Good day all! Edit: I'm currently doing a personal study on differentiation on $\Bbb{R}^n$ but I have this challenging problem. Although, some answers have been provided on how to show that show that $f$ is differentiable on $\Bbb{R}^n$ but I would further like to compute $f'$ on $\Bbb{R}^n$. There is this function $$f:\Bbb{R}^n\to \Bbb{R}$$  $$x\mapsto f(x)=\frac{1}{2}\langle x,u(x)\rangle+\langle x,b\rangle$$ where $u:\Bbb{R}^n\to\Bbb{R}^n$ is linear and symmetric $:$ $(\forall\;x,y\in \Bbb{R}^n,\langle x,u(y)\rangle=\langle u(x), y \rangle)$ and $b\in \Bbb{R}^n.$ Honestly, I am just coming across this kind of function. I want to know what name it's called. How do I show that $f$ is differentiable on $\Bbb{R}^n$ and how do I compute $f'$? Thanks for your help!","Good day all! Edit: I'm currently doing a personal study on differentiation on $\Bbb{R}^n$ but I have this challenging problem. Although, some answers have been provided on how to show that show that $f$ is differentiable on $\Bbb{R}^n$ but I would further like to compute $f'$ on $\Bbb{R}^n$. There is this function $$f:\Bbb{R}^n\to \Bbb{R}$$  $$x\mapsto f(x)=\frac{1}{2}\langle x,u(x)\rangle+\langle x,b\rangle$$ where $u:\Bbb{R}^n\to\Bbb{R}^n$ is linear and symmetric $:$ $(\forall\;x,y\in \Bbb{R}^n,\langle x,u(y)\rangle=\langle u(x), y \rangle)$ and $b\in \Bbb{R}^n.$ Honestly, I am just coming across this kind of function. I want to know what name it's called. How do I show that $f$ is differentiable on $\Bbb{R}^n$ and how do I compute $f'$? Thanks for your help!",,"['calculus', 'multivariable-calculus', 'derivatives', 'normed-spaces']"
70,"Show that the function $f(x, y) = |xy|$ is differentiable at $(0, 0)$",Show that the function  is differentiable at,"f(x, y) = |xy| (0, 0)","Firstly I understand that this question has already been asked : Show that the function $f(x,y) = |xy|$ is differentiable at 0, but is not of class $C^1$ in any neighborhood of 0. However that question did not address the question I'm about to ask. In trying to show that $f : \mathbb{R}^2 \to \mathbb{R}$ defined by $f(x, y) = |xy|$ is differentaible at $(0, 0)$ I first calculated the directional derivatives at $(0, 0)$ and I got $D_1f(0, 0) =D_2f(0, 0) = 0$, thus if $f$ is differentiable at $(0, 0)$ then we'd have $Df(0, 0) = [0 \ \ \ 0 ]$ Now I tried to prove that $f$ is differentiable at $(0, 0)$ by using the definition and $Df(0, 0)$ above as a canditate matrix for the derivative (because if the limit doesn't tend to zero using this matrix then it won't exist) and I arrived at the following (letting $h = (c, d)$) $$\lim_{h \to (0, 0)} \frac{f((0, 0) + h) - f(0, 0) - Df(0, 0) \cdot h}{|h|} = \lim_{(c, d) \to (0, 0)} \frac{|cd|}{\sqrt{c^2 + d^2}}$$ but from the above it's not clear (to me) that $$\lim_{(c, d) \to (0, 0)} \frac{|cd|}{\sqrt{c^2 + d^2}} = 0$$ so I can't conclude that $f(x, y) = |xy|$ is differentiable at $(0, 0)$ using the definition. Now there's one other way I think I can prove differentiability which is by using the following theorem Theorem: Let $A$ be open in $\mathbb{R}^m$. Suppose that the partial derivatives of the component functions of $f$ exist at each point $x$ of $A$ and are continuous on $A$. Then $f$ is differentiable at each point of $A$. So I tried to show then that $D_1f(x, y)$ existed and was continuous on $\mathbb{R}^2$, in attempting to show this I ended up with the following $$D_1f(x, y) = \lim_{t \to 0} \frac{f((x, y) + t(1, 0))-f(x, y)}{t} = \lim_{t \to 0}\frac{|(x+t)y|-|xy|}{t}$$ and again I'm not show how to evaluate this limit  so I can't make use of the above theorem. Is there any other way to show that $f$ is differentiable at $(0, 0)$? How could I show that $f$ is differentiable at $(0, 0)$?","Firstly I understand that this question has already been asked : Show that the function $f(x,y) = |xy|$ is differentiable at 0, but is not of class $C^1$ in any neighborhood of 0. However that question did not address the question I'm about to ask. In trying to show that $f : \mathbb{R}^2 \to \mathbb{R}$ defined by $f(x, y) = |xy|$ is differentaible at $(0, 0)$ I first calculated the directional derivatives at $(0, 0)$ and I got $D_1f(0, 0) =D_2f(0, 0) = 0$, thus if $f$ is differentiable at $(0, 0)$ then we'd have $Df(0, 0) = [0 \ \ \ 0 ]$ Now I tried to prove that $f$ is differentiable at $(0, 0)$ by using the definition and $Df(0, 0)$ above as a canditate matrix for the derivative (because if the limit doesn't tend to zero using this matrix then it won't exist) and I arrived at the following (letting $h = (c, d)$) $$\lim_{h \to (0, 0)} \frac{f((0, 0) + h) - f(0, 0) - Df(0, 0) \cdot h}{|h|} = \lim_{(c, d) \to (0, 0)} \frac{|cd|}{\sqrt{c^2 + d^2}}$$ but from the above it's not clear (to me) that $$\lim_{(c, d) \to (0, 0)} \frac{|cd|}{\sqrt{c^2 + d^2}} = 0$$ so I can't conclude that $f(x, y) = |xy|$ is differentiable at $(0, 0)$ using the definition. Now there's one other way I think I can prove differentiability which is by using the following theorem Theorem: Let $A$ be open in $\mathbb{R}^m$. Suppose that the partial derivatives of the component functions of $f$ exist at each point $x$ of $A$ and are continuous on $A$. Then $f$ is differentiable at each point of $A$. So I tried to show then that $D_1f(x, y)$ existed and was continuous on $\mathbb{R}^2$, in attempting to show this I ended up with the following $$D_1f(x, y) = \lim_{t \to 0} \frac{f((x, y) + t(1, 0))-f(x, y)}{t} = \lim_{t \to 0}\frac{|(x+t)y|-|xy|}{t}$$ and again I'm not show how to evaluate this limit  so I can't make use of the above theorem. Is there any other way to show that $f$ is differentiable at $(0, 0)$? How could I show that $f$ is differentiable at $(0, 0)$?",,"['calculus', 'real-analysis', 'multivariable-calculus']"
71,How many local extrema can a multidimensional polynomial have?,How many local extrema can a multidimensional polynomial have?,,"So say I have a $n$ dimensional polynomial of degree $m$. Assume that $n\geq m$. Now for a degree $m$ polynomial in one dimension, we know that there can be at most $m-1$ local extrema. Is there a similar rule for multidimensional polynomials? One important thing to note is that the polynomial I am working with is only linear terms of each dimension, so there will never be a $x_k^y$ for any $y>1$, for example Good: $f(\vec x) = x_1x_2x_3 + x_1x_2 - x_3$ Bad: $f(\vec x) = x_1^2x_2 + x_3^3 - x_2$","So say I have a $n$ dimensional polynomial of degree $m$. Assume that $n\geq m$. Now for a degree $m$ polynomial in one dimension, we know that there can be at most $m-1$ local extrema. Is there a similar rule for multidimensional polynomials? One important thing to note is that the polynomial I am working with is only linear terms of each dimension, so there will never be a $x_k^y$ for any $y>1$, for example Good: $f(\vec x) = x_1x_2x_3 + x_1x_2 - x_3$ Bad: $f(\vec x) = x_1^2x_2 + x_3^3 - x_2$",,"['calculus', 'multivariable-calculus', 'polynomials', 'optimization']"
72,Chain rule for multivariable gradients - a matrix of gradients,Chain rule for multivariable gradients - a matrix of gradients,,"In my coursebook, there was a function to be differentiated. Its definition was: $$\varphi(x,y) = f(u(x,y), v(x,y)) $$ where $$f(u(x,y), v(x,y)) \in \mathbb R$$ This function is clearly a composition: $$(x,y) \mapsto_{g} (u(x,y), v(x,y)) \mapsto_f \varphi $$ Therefore, to calculate its derivative we need to apply the chain rule. On the one hand, the derivative is: $$\nabla \varphi = \left(\frac {\partial \varphi}{\partial x}, \frac{\partial \varphi}{\partial y} \right)$$ On the other hand, the derivative of $\varphi$ is: $$f' \cdot g' = \nabla f \cdot \nabla g$$ Now, here comes the part which I do not understand. I know that the derivative of a multivaiable function is a vector, therefore, I expect $\nabla g$ to be a vector looking something like this: $$\nabla g = \left(\frac{\partial g}{\partial u}, \frac{\partial g}{\partial v}  \right)$$ However, the textbook presentes $\nabla g$ as a matrix looking exactly like this: $$ \nabla g = \begin{bmatrix}\nabla u \\ \nabla v \end{bmatrix} $$ I do not understand why it is possible to present one gradient as a matrix of other gradients. Could you, please, clarify a little bit what is going on here?","In my coursebook, there was a function to be differentiated. Its definition was: $$\varphi(x,y) = f(u(x,y), v(x,y)) $$ where $$f(u(x,y), v(x,y)) \in \mathbb R$$ This function is clearly a composition: $$(x,y) \mapsto_{g} (u(x,y), v(x,y)) \mapsto_f \varphi $$ Therefore, to calculate its derivative we need to apply the chain rule. On the one hand, the derivative is: $$\nabla \varphi = \left(\frac {\partial \varphi}{\partial x}, \frac{\partial \varphi}{\partial y} \right)$$ On the other hand, the derivative of $\varphi$ is: $$f' \cdot g' = \nabla f \cdot \nabla g$$ Now, here comes the part which I do not understand. I know that the derivative of a multivaiable function is a vector, therefore, I expect $\nabla g$ to be a vector looking something like this: $$\nabla g = \left(\frac{\partial g}{\partial u}, \frac{\partial g}{\partial v}  \right)$$ However, the textbook presentes $\nabla g$ as a matrix looking exactly like this: $$ \nabla g = \begin{bmatrix}\nabla u \\ \nabla v \end{bmatrix} $$ I do not understand why it is possible to present one gradient as a matrix of other gradients. Could you, please, clarify a little bit what is going on here?",,"['calculus', 'multivariable-calculus', 'vector-analysis']"
73,Inequality $||\nabla f(x)|| \geqslant c|f(x)|^{\frac{1}{2}}$ (S. Łojasiewicz),Inequality  (S. Łojasiewicz),||\nabla f(x)|| \geqslant c|f(x)|^{\frac{1}{2}},"I need to prove the special case of Łojasiewicz inequality: Fact. Let $\Omega \subset \mathbb{R}^m$ be open and $0 \in \Omega$. Let $f \in \mathcal{C}^2(\Omega,\mathbb{R})$ and $f(0)=0$. Suppose also that the matrix of second partial derivatives is invertible. Prove that there exists an open neighbourhood $U \subset \Omega$ of $0$ such that the following inequality holds: $$||\nabla f(x)|| \geqslant c|f(x)|^{\frac{1}{2}}$$ for some constant $c>0$. My attempt: We can use Taylor expansion formula and hence for $||u||$, small enough, we have: $$|f(u)| \leqslant c||u||^2$$ (derivatives are continuous). But what to do next? Writing down Taylor formula for $f'$ seems to me fine but I have no idea how to figure that out.  Thanks in advance.","I need to prove the special case of Łojasiewicz inequality: Fact. Let $\Omega \subset \mathbb{R}^m$ be open and $0 \in \Omega$. Let $f \in \mathcal{C}^2(\Omega,\mathbb{R})$ and $f(0)=0$. Suppose also that the matrix of second partial derivatives is invertible. Prove that there exists an open neighbourhood $U \subset \Omega$ of $0$ such that the following inequality holds: $$||\nabla f(x)|| \geqslant c|f(x)|^{\frac{1}{2}}$$ for some constant $c>0$. My attempt: We can use Taylor expansion formula and hence for $||u||$, small enough, we have: $$|f(u)| \leqslant c||u||^2$$ (derivatives are continuous). But what to do next? Writing down Taylor formula for $f'$ seems to me fine but I have no idea how to figure that out.  Thanks in advance.",,"['real-analysis', 'multivariable-calculus', 'inequality']"
74,Extending Function to Make it Continuous,Extending Function to Make it Continuous,,"I've been struggling with extending this function to make it continuous: $$f(x,y)=\frac{1}{x}\sin\left(\frac{x^3}{x^2+y^2}\right)$$ So, the domain is $\mathbb{R}^2\backslash\{(0,y):y\in\mathbb{R}\}$. Set of limit points of the domain is obviously $\mathbb{R}^2$. I'm not sure how to approach this problem because none of the methods we've done in class work. I'm not even sure how to a priori assume whether I can or can't extend this function to be continuous (or even in which points is it possible to do so). This is all new to me so I would appreciate any help or advice on how to predict the behaviour of this function. Thanks in advance.","I've been struggling with extending this function to make it continuous: $$f(x,y)=\frac{1}{x}\sin\left(\frac{x^3}{x^2+y^2}\right)$$ So, the domain is $\mathbb{R}^2\backslash\{(0,y):y\in\mathbb{R}\}$. Set of limit points of the domain is obviously $\mathbb{R}^2$. I'm not sure how to approach this problem because none of the methods we've done in class work. I'm not even sure how to a priori assume whether I can or can't extend this function to be continuous (or even in which points is it possible to do so). This is all new to me so I would appreciate any help or advice on how to predict the behaviour of this function. Thanks in advance.",,"['multivariable-calculus', 'continuity']"
75,Gradient of cross product,Gradient of cross product,,"Consider $\mathbb{R}^3 \times \mathbb{R}^3$ with standard coordinates $(q_1, q_2, q_3, p_1, p_2, p_3)$. For a fixed $v \in \mathbb{R}^3$, consider the function $f : \mathbb{R}^3 \times \mathbb{R}^3 \to \mathbb{R}$ given by $$f(q, p) = \langle v, q \times p \rangle$$ Writing everything out, it's easy to show that $\nabla f = (- v \times p, v \times q)$. Is there an easier way to see this, that doesn't involve writing out the coordinate-wise formulas for cross product and inner product?","Consider $\mathbb{R}^3 \times \mathbb{R}^3$ with standard coordinates $(q_1, q_2, q_3, p_1, p_2, p_3)$. For a fixed $v \in \mathbb{R}^3$, consider the function $f : \mathbb{R}^3 \times \mathbb{R}^3 \to \mathbb{R}$ given by $$f(q, p) = \langle v, q \times p \rangle$$ Writing everything out, it's easy to show that $\nabla f = (- v \times p, v \times q)$. Is there an easier way to see this, that doesn't involve writing out the coordinate-wise formulas for cross product and inner product?",,"['multivariable-calculus', 'differential-geometry', 'cross-product']"
76,Prove this basic inequality $f(y)\geq f(x)+\nabla f(x)^T(y-x)$for differentiable convex functions,Prove this basic inequality for differentiable convex functions,f(y)\geq f(x)+\nabla f(x)^T(y-x),Prove this basic inequality $f(y)\geq f(x)+\nabla f(x)^T(y-x)$for differentiable convex functions. I tried using Taylor Expansion to get $$f(y)=f(x)+\nabla f(x)^T(y-x)+\frac{1}{2}(y-x)^TH_f(x)(y-x)+\dots$$ Can I simply claim that the inequality follows? How?,Prove this basic inequality $f(y)\geq f(x)+\nabla f(x)^T(y-x)$for differentiable convex functions. I tried using Taylor Expansion to get $$f(y)=f(x)+\nabla f(x)^T(y-x)+\frac{1}{2}(y-x)^TH_f(x)(y-x)+\dots$$ Can I simply claim that the inequality follows? How?,,"['multivariable-calculus', 'convex-analysis']"
77,Does there exist an injective function $f:\mathbb R^2 \to \mathbb R$ such that $f$ is continuous in one of the variables $x$ or $y$?,Does there exist an injective function  such that  is continuous in one of the variables  or ?,f:\mathbb R^2 \to \mathbb R f x y,Does there exist an injective function $f:\mathbb R^2 \to \mathbb R$ such that $f$ is continuous in one of the variables $x$ or $y$ ? I only know that such an injection cannot be continuous in each real variable $x$ and $y$ . Please help . Thanks in advance,Does there exist an injective function $f:\mathbb R^2 \to \mathbb R$ such that $f$ is continuous in one of the variables $x$ or $y$ ? I only know that such an injection cannot be continuous in each real variable $x$ and $y$ . Please help . Thanks in advance,,"['real-analysis', 'multivariable-calculus']"
78,Multivariate normal value standardization,Multivariate normal value standardization,,"I am wonder how to standardize multivariate normal value . Normal standard multivariate distribution of $q$ variables is $z\sim N_q(0,I_q)$. I have found that $a+Bz\sim N_q(Ba,BB^T)$ and based on this fact normalization could be performed throught two ways: 1) Substracting mean vector and then taking $B=\Sigma^{-0.5}$ (as it gives $\Sigma^{-0.5} \Sigma (\Sigma^{-0.5})^T=I_q$) we get normalized value: $z=\Sigma^{-0.5}(x-a)$. 2) Using Cholesky decomposition $\Sigma=U^TU$ we get normalization rule $z=(U^T)^{-1}(x-a)$ But both strategies does not work! So you can check it using matlab code below: MU=[0.1333,-0.1]; SIGMA=[0.95,0.19;0.19,0.97]; x=[0.5, 0.7]; mvncdf(x,MU,SIGMA) z=(SIGMA^(-0.5))*transpose(x-MU); mvncdf(z) z=transpose(chol(SIGMA))^(-1)*transpose(x-MU); mvncdf(z) I also add code for R that gives the same results proving that the problem is not due to programm but due to incorrect appoach: SIGMA=matrix(c(0.95,0.19,0.19,0.97), ncol=2) MU=c(0.1333, -0.1) x=c(0.5,0.7) pmvnorm(mean = MU, sigma=SIGMA, lower=-Inf, upper=x) z=as.numeric((solve(sqrtm(SIGMA)))%*%(x-MU)) pmvnorm(mean = MU, sigma=SIGMA, lower=-Inf, upper=z) As the difference is rather big I am sure that such standardization approach is incorrect. But it is the only technicks I have found despite it is a hot and applied subject. Will be very greatful for help! PS: I also performed Jordan form approach but the result is still wrong : [v,j]=jordan(SIGMA) B=v*j^(-0.5)*v^(-1) z=B*transpose(x-MU); mvncdf(z)","I am wonder how to standardize multivariate normal value . Normal standard multivariate distribution of $q$ variables is $z\sim N_q(0,I_q)$. I have found that $a+Bz\sim N_q(Ba,BB^T)$ and based on this fact normalization could be performed throught two ways: 1) Substracting mean vector and then taking $B=\Sigma^{-0.5}$ (as it gives $\Sigma^{-0.5} \Sigma (\Sigma^{-0.5})^T=I_q$) we get normalized value: $z=\Sigma^{-0.5}(x-a)$. 2) Using Cholesky decomposition $\Sigma=U^TU$ we get normalization rule $z=(U^T)^{-1}(x-a)$ But both strategies does not work! So you can check it using matlab code below: MU=[0.1333,-0.1]; SIGMA=[0.95,0.19;0.19,0.97]; x=[0.5, 0.7]; mvncdf(x,MU,SIGMA) z=(SIGMA^(-0.5))*transpose(x-MU); mvncdf(z) z=transpose(chol(SIGMA))^(-1)*transpose(x-MU); mvncdf(z) I also add code for R that gives the same results proving that the problem is not due to programm but due to incorrect appoach: SIGMA=matrix(c(0.95,0.19,0.19,0.97), ncol=2) MU=c(0.1333, -0.1) x=c(0.5,0.7) pmvnorm(mean = MU, sigma=SIGMA, lower=-Inf, upper=x) z=as.numeric((solve(sqrtm(SIGMA)))%*%(x-MU)) pmvnorm(mean = MU, sigma=SIGMA, lower=-Inf, upper=z) As the difference is rather big I am sure that such standardization approach is incorrect. But it is the only technicks I have found despite it is a hot and applied subject. Will be very greatful for help! PS: I also performed Jordan form approach but the result is still wrong : [v,j]=jordan(SIGMA) B=v*j^(-0.5)*v^(-1) z=B*transpose(x-MU); mvncdf(z)",,"['multivariable-calculus', 'probability-distributions', 'normal-distribution', 'matlab']"
79,Help for convolution of two Multivariate Gaussian PDFs,Help for convolution of two Multivariate Gaussian PDFs,,"I am looking for a proof for convolution of two multivariate Gaussians (where each Gaussian has multi-dimensional mean and co-variance). I found a proof in here: http://www.tina-vision.net/docs/memos/2003-003.pdf where it provides a proof for convolution of two uni-variate Gaussians and also it provides the mean and variance of the convolved PDF. Now I am looking forward to find a proof for (or extend it to) multivariate Gaussian PDFs. Does anybody know any solution already available, or can help me with it? Thank you.","I am looking for a proof for convolution of two multivariate Gaussians (where each Gaussian has multi-dimensional mean and co-variance). I found a proof in here: http://www.tina-vision.net/docs/memos/2003-003.pdf where it provides a proof for convolution of two uni-variate Gaussians and also it provides the mean and variance of the convolved PDF. Now I am looking forward to find a proof for (or extend it to) multivariate Gaussian PDFs. Does anybody know any solution already available, or can help me with it? Thank you.",,"['multivariable-calculus', 'normal-distribution', 'convolution']"
80,Total derivative of inner product.,Total derivative of inner product.,,"Let $$F:\Bbb R^n\times\Bbb R^n\to\Bbb R$$ be the function $F(x,y)=\langle Ax,y\rangle$ where $\langle , \rangle$ denotes the standard inner product on $\Bbb R^n$ and $A$ be an $n\times n$ real matrix. If $D$ denotes the total derivative. which of the fallowing is correct? 1). $(DF(x,y))(u,v)=\langle Au,y\rangle+\langle Ax,v\rangle$ 2). $(DF(x,y))(0,0)=(0,0)$ 3). $DF(x,y)$ may not exist for some $(x,y)\in\Bbb R^n\times\Bbb R^n$ 4). $DF(x,y)$ doesnot exist at $(x,y)=(0,0)$ I don't know, how the total derivative is defined for Inner product. So, tell me the definition for total derivative on inner product space, also tell, How to proceed further to solve this problem? Thank you","Let $$F:\Bbb R^n\times\Bbb R^n\to\Bbb R$$ be the function $F(x,y)=\langle Ax,y\rangle$ where $\langle , \rangle$ denotes the standard inner product on $\Bbb R^n$ and $A$ be an $n\times n$ real matrix. If $D$ denotes the total derivative. which of the fallowing is correct? 1). $(DF(x,y))(u,v)=\langle Au,y\rangle+\langle Ax,v\rangle$ 2). $(DF(x,y))(0,0)=(0,0)$ 3). $DF(x,y)$ may not exist for some $(x,y)\in\Bbb R^n\times\Bbb R^n$ 4). $DF(x,y)$ doesnot exist at $(x,y)=(0,0)$ I don't know, how the total derivative is defined for Inner product. So, tell me the definition for total derivative on inner product space, also tell, How to proceed further to solve this problem? Thank you",,"['multivariable-calculus', 'inner-products', 'matrix-calculus']"
81,Why spherical coordinates is not a covering?,Why spherical coordinates is not a covering?,,"Maybe this is an idiot question and I'm committing a trivial mistake. Let $\phi (\theta, \varphi) = (\cos \theta \sin \varphi, \sin \theta\sin \varphi, \cos \varphi)$ be the usual covering of the circle $\mathbb{S}^2 (1)$ by spherical coordinates I always thought that polar coordinates could be seen as the picture, for instance, in here http://en.wikipedia.org/wiki/Spherical_coordinate_system (second picture). In this case it would be a covering space map. However when $\theta = 0$, the coordinates satisfies $(\sin \varphi, 0, \cos \varphi)$, which is a circle in the plane $y = 0$, but the angle $\theta$ is $0$ and $(-1, 0, 0)$ belongs to circle! However according to the image in the link $\theta$ should be $\pi$. So $\phi (0 \times (0, 2\pi)) \cap \phi (\pi \times (0, 2\pi)) \neq \emptyset$. Therefore I'm now not sure if $\phi$ is a covering space. So, is $\phi$ a covering space map? And what's wrong with that $\theta$? Rephrasing my question into one: in squares of what size $\phi$ is injective? Thanks in advance. EDIT I've just realized that the map $\phi$ cannot be a covering. Since $\mathbb{S}^2(1)$ is already simply connected, $\mathbb{R}^2$ cannot be a covering space, otherwise it would be the universal covering. Let $U_{a, b} = (a, 2\pi + a)\times(b, \pi + b)$. The function $\phi|_{U_{a, b}}$ is not an injection unless $a, b \in \mathbb{Z}$ (as noted by user86418). As discussed in the comments, I (and I think the other commenters) thought that $\mathbb{R}^2$ could be tessellated by the rectangles $U_{i, j}$ where $i, j \in \mathbb{Z}$. However this is not true, since this tessellation would not be induced by a group $G$ (otherwise $G \cong \pi_1 (\mathbb{S}^2) = 0$) By making a more detailed analysis, it's possible to see that in the rectangle $R_{0,0} = \overline{U_{0, 0}}$, a vertical arrow pointing down is equivalent to an arrow pointing up (with the source) translated by $(\pi, 2\pi)$. More precisely, a point $(\theta, \varphi)$ is identified with a point $(\theta + \pi, 2\pi - \varphi)$. Together with this ""action"", there is the usual identification beetween  $(\theta, \varphi)$ and $(\theta + k2\pi, \varphi + l2\pi)$. In this case, the second action is given by $\mathbb{Z}^2$ (by translating by $2\pi$) and, in the quotient $X \cong \mathbb{R}^2/ \mathbb{Z}^2$, the first ""action"" turns in to a real action given by $\mathbb{Z}_2$. Therefore $\mathbb{S}^2 \cong X /\mathbb{Z}^2$. But, since $X \cong \mathbb{T}^2$ is a torus, this would imply that the sphere is a quotient of a torus by $\mathbb{Z}^2$. Is this correct? So summarizing, the action on the torus by  the additive group $\mathbb{Z}^2$ is given by $1. (\theta, \varphi) = (\theta + \pi, 2\pi - \varphi)$. So the questions are: Is $\mathbb{S}^2 \cong \mathbb{T}^2/ \mathbb{Z}^2$ as descripted above? Furthermore, for what open set $\phi$ fails to be a covering (i.e, when $\phi^{-1} (U)$ fails to be a disjoint union of isomorphic open sets)?","Maybe this is an idiot question and I'm committing a trivial mistake. Let $\phi (\theta, \varphi) = (\cos \theta \sin \varphi, \sin \theta\sin \varphi, \cos \varphi)$ be the usual covering of the circle $\mathbb{S}^2 (1)$ by spherical coordinates I always thought that polar coordinates could be seen as the picture, for instance, in here http://en.wikipedia.org/wiki/Spherical_coordinate_system (second picture). In this case it would be a covering space map. However when $\theta = 0$, the coordinates satisfies $(\sin \varphi, 0, \cos \varphi)$, which is a circle in the plane $y = 0$, but the angle $\theta$ is $0$ and $(-1, 0, 0)$ belongs to circle! However according to the image in the link $\theta$ should be $\pi$. So $\phi (0 \times (0, 2\pi)) \cap \phi (\pi \times (0, 2\pi)) \neq \emptyset$. Therefore I'm now not sure if $\phi$ is a covering space. So, is $\phi$ a covering space map? And what's wrong with that $\theta$? Rephrasing my question into one: in squares of what size $\phi$ is injective? Thanks in advance. EDIT I've just realized that the map $\phi$ cannot be a covering. Since $\mathbb{S}^2(1)$ is already simply connected, $\mathbb{R}^2$ cannot be a covering space, otherwise it would be the universal covering. Let $U_{a, b} = (a, 2\pi + a)\times(b, \pi + b)$. The function $\phi|_{U_{a, b}}$ is not an injection unless $a, b \in \mathbb{Z}$ (as noted by user86418). As discussed in the comments, I (and I think the other commenters) thought that $\mathbb{R}^2$ could be tessellated by the rectangles $U_{i, j}$ where $i, j \in \mathbb{Z}$. However this is not true, since this tessellation would not be induced by a group $G$ (otherwise $G \cong \pi_1 (\mathbb{S}^2) = 0$) By making a more detailed analysis, it's possible to see that in the rectangle $R_{0,0} = \overline{U_{0, 0}}$, a vertical arrow pointing down is equivalent to an arrow pointing up (with the source) translated by $(\pi, 2\pi)$. More precisely, a point $(\theta, \varphi)$ is identified with a point $(\theta + \pi, 2\pi - \varphi)$. Together with this ""action"", there is the usual identification beetween  $(\theta, \varphi)$ and $(\theta + k2\pi, \varphi + l2\pi)$. In this case, the second action is given by $\mathbb{Z}^2$ (by translating by $2\pi$) and, in the quotient $X \cong \mathbb{R}^2/ \mathbb{Z}^2$, the first ""action"" turns in to a real action given by $\mathbb{Z}_2$. Therefore $\mathbb{S}^2 \cong X /\mathbb{Z}^2$. But, since $X \cong \mathbb{T}^2$ is a torus, this would imply that the sphere is a quotient of a torus by $\mathbb{Z}^2$. Is this correct? So summarizing, the action on the torus by  the additive group $\mathbb{Z}^2$ is given by $1. (\theta, \varphi) = (\theta + \pi, 2\pi - \varphi)$. So the questions are: Is $\mathbb{S}^2 \cong \mathbb{T}^2/ \mathbb{Z}^2$ as descripted above? Furthermore, for what open set $\phi$ fails to be a covering (i.e, when $\phi^{-1} (U)$ fails to be a disjoint union of isomorphic open sets)?",,"['multivariable-calculus', 'differential-geometry', 'covering-spaces']"
82,Derivative with respect to the normal?,Derivative with respect to the normal?,,"I am trying to use greens theorem to show the following: $$\int \int (f_{xx}+f_{yy}) \, dx \, dy=\int\frac{\partial f}{\partial n} \, ds$$ I am not completely sure how to treat the $d/dn$.  I have simplified to a point where I have $$\int f_x \, dy-f_y \, dx=\int \int (f_{xx}+f_{yy}) \, dx \, dy$$ So I need to go from the left side of the last equation to the right side of the first equation.  Any intuition on what the derivative implies?","I am trying to use greens theorem to show the following: $$\int \int (f_{xx}+f_{yy}) \, dx \, dy=\int\frac{\partial f}{\partial n} \, ds$$ I am not completely sure how to treat the $d/dn$.  I have simplified to a point where I have $$\int f_x \, dy-f_y \, dx=\int \int (f_{xx}+f_{yy}) \, dx \, dy$$ So I need to go from the left side of the last equation to the right side of the first equation.  Any intuition on what the derivative implies?",,"['multivariable-calculus', 'greens-theorem']"
83,Why is a vector function not smooth if $r'=0$,Why is a vector function not smooth if,r'=0,"It is stated that a vector function is smooth if its derivative is continuous and nowhere zero, but I can’t find a proof or a geometric interpretation. Is this a definition or a theorem?","It is stated that a vector function is smooth if its derivative is continuous and nowhere zero, but I can’t find a proof or a geometric interpretation. Is this a definition or a theorem?",,"['calculus', 'multivariable-calculus', 'curves']"
84,Can't understand this simple passage involving multivariable calculus,Can't understand this simple passage involving multivariable calculus,,"A certain passage in my Fundamentals of Thermodynamics book is driving me crazy. I considered posting this is Physics.SE, but I think the question is eminently mathematical. Here is the passage: As we know, $c_p$ is defined as: $$c_p = \left(\frac{\partial h}{\partial T} \right)_p \tag1$$   We also have seen that: $$T\ ds = dh - v\ dp \tag2$$   Therefore: $$c_p = T \left({\partial s \over \partial T} \right)_p \tag3$$ I've spent hours trying to figure out how to get to $(3)$. I really want to do this as rigorously as a graduated mechanical engineer can. So far I have two ""approaches,"" one that is fine for most of my friends but is very informal and another that is more correct, but leads me nowhere. In case it's needed, let me give some background on the thermodynamic variables: for curiosity's sake, $c_p$ means specific heat in a constant pressure process; $p, v, T, u, h, s$ are properties, of which any one can be seen as a function of any other two; the notation $\left({\partial a \over \partial b}\right)_c$ means that, in that derivative, $a = a(b,c)$, so its purpose is to make it clear that the variable held constant is $c$. Approach 1: simple but ""wrong"" From $(2)$, we have: $$dh = T\ ds + v\ dp \tag4$$ We plug that directly into $\partial h$ of $(1)$ to obtain: $$c_p = \left({T\ \partial s + v\ \partial p \over \partial T} \right)_p \tag5$$ Since $p$ is held constant, $\partial p = 0$, so $(3)$ emerges. My problems with this: Making $\partial h = dh$ and plugging it in $(1)$ looks awful to me. I've never seen a ""partial differential"" outside of a partial derivative (e.g. $\partial h$ by itself). We usually take that liberty with ""total differentials,"" but I've never seen that done with partials. $\partial p$ can't even exist inside the derivative of $(5)$, since it would mean $p = p(T, p)$, which... does it make any sense?! Anyway, it can be argued that the partial derivative would indeed be zero, but it sucks. Approach 2: correct but dead end If we make $h = h(T, p)$, then comes: $$dh = \left({\partial h \over \partial T} \right)_p  dT + \left({\partial h \over \partial p} \right)_T dp\tag6$$ Equating with $(4)$ gives: $$\left({\partial h \over \partial T} \right)_p = T\frac{ds}{dT} + \frac{dp}{dT}\left[v - \left({\partial h \over \partial p} \right)_T \right] \tag7$$ So, from $(1)$: $$c_p = T\frac{ds}{dT} + \frac{dp}{dT}\left[v - \left({\partial h \over \partial p} \right)_T \right] \tag8$$ Which doesn't look like $(3)$ at all. In order to make it better, we could ""open up"" $\frac{ds}{dT}$ with $s(T,p)$ and eventually find: $$c_p = T \left({\partial s \over \partial T} \right)_p + \frac{dp}{dT}\left[v - \left({\partial h \over \partial p} \right)_T + T \left({\partial s \over \partial p} \right)_T \right] \tag9$$ We'd need the second term to be zero, but $\frac{dp}{dT}$ certainly isn't and I don't see why the $[\cdots]$ would be either. Did I go wrong somewhere? Is Approach 2 really optimal? I appreciate any help.","A certain passage in my Fundamentals of Thermodynamics book is driving me crazy. I considered posting this is Physics.SE, but I think the question is eminently mathematical. Here is the passage: As we know, $c_p$ is defined as: $$c_p = \left(\frac{\partial h}{\partial T} \right)_p \tag1$$   We also have seen that: $$T\ ds = dh - v\ dp \tag2$$   Therefore: $$c_p = T \left({\partial s \over \partial T} \right)_p \tag3$$ I've spent hours trying to figure out how to get to $(3)$. I really want to do this as rigorously as a graduated mechanical engineer can. So far I have two ""approaches,"" one that is fine for most of my friends but is very informal and another that is more correct, but leads me nowhere. In case it's needed, let me give some background on the thermodynamic variables: for curiosity's sake, $c_p$ means specific heat in a constant pressure process; $p, v, T, u, h, s$ are properties, of which any one can be seen as a function of any other two; the notation $\left({\partial a \over \partial b}\right)_c$ means that, in that derivative, $a = a(b,c)$, so its purpose is to make it clear that the variable held constant is $c$. Approach 1: simple but ""wrong"" From $(2)$, we have: $$dh = T\ ds + v\ dp \tag4$$ We plug that directly into $\partial h$ of $(1)$ to obtain: $$c_p = \left({T\ \partial s + v\ \partial p \over \partial T} \right)_p \tag5$$ Since $p$ is held constant, $\partial p = 0$, so $(3)$ emerges. My problems with this: Making $\partial h = dh$ and plugging it in $(1)$ looks awful to me. I've never seen a ""partial differential"" outside of a partial derivative (e.g. $\partial h$ by itself). We usually take that liberty with ""total differentials,"" but I've never seen that done with partials. $\partial p$ can't even exist inside the derivative of $(5)$, since it would mean $p = p(T, p)$, which... does it make any sense?! Anyway, it can be argued that the partial derivative would indeed be zero, but it sucks. Approach 2: correct but dead end If we make $h = h(T, p)$, then comes: $$dh = \left({\partial h \over \partial T} \right)_p  dT + \left({\partial h \over \partial p} \right)_T dp\tag6$$ Equating with $(4)$ gives: $$\left({\partial h \over \partial T} \right)_p = T\frac{ds}{dT} + \frac{dp}{dT}\left[v - \left({\partial h \over \partial p} \right)_T \right] \tag7$$ So, from $(1)$: $$c_p = T\frac{ds}{dT} + \frac{dp}{dT}\left[v - \left({\partial h \over \partial p} \right)_T \right] \tag8$$ Which doesn't look like $(3)$ at all. In order to make it better, we could ""open up"" $\frac{ds}{dT}$ with $s(T,p)$ and eventually find: $$c_p = T \left({\partial s \over \partial T} \right)_p + \frac{dp}{dT}\left[v - \left({\partial h \over \partial p} \right)_T + T \left({\partial s \over \partial p} \right)_T \right] \tag9$$ We'd need the second term to be zero, but $\frac{dp}{dT}$ certainly isn't and I don't see why the $[\cdots]$ would be either. Did I go wrong somewhere? Is Approach 2 really optimal? I appreciate any help.",,['multivariable-calculus']
85,Newton's Method for Roots of Polynomials,Newton's Method for Roots of Polynomials,,"The standard way to use Newton's Method for finding a root of a polynomial $p(x)$ is to use the iteration formula $$x_{n+1}=x_n-{p(x)\over p'(x)}$$ I recently thought of a new way of finding the roots. Letting $x_1,...,x_n$ denote the $n$ roots and setting $p(x)=x^n+\cdots +a_1x+a_0$, we have the formulas $$x_1+\cdots +x_n=-a_{n-1}$$ $$x_1x_2+\cdots +x_{n-1}x_n=a_{n-2}$$ and so on to $$x_1x_2\cdots x_n=(-1)^na_0$$ This gives a non-linear system of $n$ variables and $n$ equations, which can be solved using the multivariable version of Newton's method. We would use the iteration $$\bf x_{n+1}=x_n-[Df(x_n)]^{-1}f(x_n)$$ where $$f(x_1,...,x_n)=\begin{bmatrix}x_1+\cdots +x_n+a_{n-1} \\ x_1x_2+\cdots +x_{n-1}x_n-a_{n-2} \\ \vdots \\ x_1\cdots x_n-(-1)^na_0 \end{bmatrix}$$ and $Df$ is the derivative matrix of $f$. My question is: is there any advantage to using this method for finding roots? The obvious plus is that using the multivariable Newton's method provides all $n$ roots simultaneously while the regular method only gives them one at a time. However, I'm also interested in how the rate of convergence of this method would compare with the other, and if one would tend to be more numerically stable than the other.","The standard way to use Newton's Method for finding a root of a polynomial $p(x)$ is to use the iteration formula $$x_{n+1}=x_n-{p(x)\over p'(x)}$$ I recently thought of a new way of finding the roots. Letting $x_1,...,x_n$ denote the $n$ roots and setting $p(x)=x^n+\cdots +a_1x+a_0$, we have the formulas $$x_1+\cdots +x_n=-a_{n-1}$$ $$x_1x_2+\cdots +x_{n-1}x_n=a_{n-2}$$ and so on to $$x_1x_2\cdots x_n=(-1)^na_0$$ This gives a non-linear system of $n$ variables and $n$ equations, which can be solved using the multivariable version of Newton's method. We would use the iteration $$\bf x_{n+1}=x_n-[Df(x_n)]^{-1}f(x_n)$$ where $$f(x_1,...,x_n)=\begin{bmatrix}x_1+\cdots +x_n+a_{n-1} \\ x_1x_2+\cdots +x_{n-1}x_n-a_{n-2} \\ \vdots \\ x_1\cdots x_n-(-1)^na_0 \end{bmatrix}$$ and $Df$ is the derivative matrix of $f$. My question is: is there any advantage to using this method for finding roots? The obvious plus is that using the multivariable Newton's method provides all $n$ roots simultaneously while the regular method only gives them one at a time. However, I'm also interested in how the rate of convergence of this method would compare with the other, and if one would tend to be more numerically stable than the other.",,"['multivariable-calculus', 'numerical-methods', 'newton-raphson']"
86,Is Gradient really the direction of steepest ascent?,Is Gradient really the direction of steepest ascent?,,"I want to intuitively understand why the gradient gives you the direction of the steepest ascent of a function. Apart from the already posted questions, my confusion arises from the fact that we form the gradient vector from the derivative of each dimension separately . Then take the vector consisting of both (for 2D) derivatives take it as the steepest ascent. What if in both directions the derivative is say $5$, so our vector will be $45$ degrees from both axis, But in that direction specifically the function goes down ? If it's not clear what I'm confused with, consider this function represented as an image : $$ \begin{pmatrix}100&5&-100\\0&\textit{0}&5\\0& 0&   0\end{pmatrix}$$ at 0 , it makes sense that the derivative is $5$ in $x$ and in $y$, but a vector of $(5,5)$ goes to a direction that's not a steepest ascent. Does this have to do with the differentiability of the function ? what am I missing ?","I want to intuitively understand why the gradient gives you the direction of the steepest ascent of a function. Apart from the already posted questions, my confusion arises from the fact that we form the gradient vector from the derivative of each dimension separately . Then take the vector consisting of both (for 2D) derivatives take it as the steepest ascent. What if in both directions the derivative is say $5$, so our vector will be $45$ degrees from both axis, But in that direction specifically the function goes down ? If it's not clear what I'm confused with, consider this function represented as an image : $$ \begin{pmatrix}100&5&-100\\0&\textit{0}&5\\0& 0&   0\end{pmatrix}$$ at 0 , it makes sense that the derivative is $5$ in $x$ and in $y$, but a vector of $(5,5)$ goes to a direction that's not a steepest ascent. Does this have to do with the differentiability of the function ? what am I missing ?",,['multivariable-calculus']
87,What is two-dimensional curl in terms of Stokes' theorem?,What is two-dimensional curl in terms of Stokes' theorem?,,"I'm a little confused about the divergence and the curl for two-dimensional vector fields. In 3d, I understand the curl as $d:\Omega^1(M^3)\to\Omega^2(M^3)$ and the divergence as $d:\Omega^2(M^3) \to \Omega^3(M^3)$. But what is the analog in 2d? It seems the curl is the operator $d:\Omega^1(M^2)\to \Omega^2(M^2)$, and then what could the divergence be? I recall using before the divergence theorem for two-dimensional vector fields...that $$\int_D\text{div}\,\sigma=\int_{\partial D}\sigma\cdot \nu, \tag{1}$$ where $\nu$ is the unit normal to the two dimensional region $D$. Where does this fit in with generalized Stokes' theorem, if $d:\Omega^1(M^2)\to \Omega^2(M^2)$ is the curl, not the divergence? I'm thinking now as I write this that (1) just treats the 2d vector field as a 3d field with third component zero.","I'm a little confused about the divergence and the curl for two-dimensional vector fields. In 3d, I understand the curl as $d:\Omega^1(M^3)\to\Omega^2(M^3)$ and the divergence as $d:\Omega^2(M^3) \to \Omega^3(M^3)$. But what is the analog in 2d? It seems the curl is the operator $d:\Omega^1(M^2)\to \Omega^2(M^2)$, and then what could the divergence be? I recall using before the divergence theorem for two-dimensional vector fields...that $$\int_D\text{div}\,\sigma=\int_{\partial D}\sigma\cdot \nu, \tag{1}$$ where $\nu$ is the unit normal to the two dimensional region $D$. Where does this fit in with generalized Stokes' theorem, if $d:\Omega^1(M^2)\to \Omega^2(M^2)$ is the curl, not the divergence? I'm thinking now as I write this that (1) just treats the 2d vector field as a 3d field with third component zero.",,"['multivariable-calculus', 'differential-geometry']"
88,Does $S:\Bbb{R}^m\to \Bbb{R}^n$ have to be a matrix?,Does  have to be a matrix?,S:\Bbb{R}^m\to \Bbb{R}^n,"Let $S:\Bbb{R}^m\to \Bbb{R}^n$ be a linear transformation. Every textbook that I have come across states that $S$ is an $n\times m$ matrix. It is easy to see that such a matrix satisfies the properties of such a linear transformation. However, can $S$ be something other than a matrix? I can't think of a way of proving that $S$ can only be a matrix and nothing else. Thanks in advance!","Let $S:\Bbb{R}^m\to \Bbb{R}^n$ be a linear transformation. Every textbook that I have come across states that $S$ is an $n\times m$ matrix. It is easy to see that such a matrix satisfies the properties of such a linear transformation. However, can $S$ be something other than a matrix? I can't think of a way of proving that $S$ can only be a matrix and nothing else. Thanks in advance!",,[]
89,"Prove that $\frac{d}{dx}\int_0^xf(x,y)dy = f(x,x)+\int_0^x\frac{\partial}{\partial x}f(x,y)dy$",Prove that,"\frac{d}{dx}\int_0^xf(x,y)dy = f(x,x)+\int_0^x\frac{\partial}{\partial x}f(x,y)dy","Let $f:\mathbb{R}^2\rightarrow\mathbb{R}$.  Assume that $$\frac{d}{dx}\int_a^bf(x,y)dy=\int_a^b\frac{\partial}{\partial x}f(x,y)dy.$$ Use the above property and the chain rule to prove that $$\frac{d}{dx}\int_0^xf(x,y)dy = f(x,x)+\int_0^x\frac{\partial}{\partial x}f(x,y)dy.$$ This doesn't seem like it should be that hard but I haven't been able to get it, I especially can't figure out how to take advantage of the chain rule.  Here's my best shot: Let $a<x$.  Then we have \begin{align} \frac{d}{dx}\int_0^xf(x,y)dy &= \frac{d}{dx}\int_a^xf(x,y)dy + \int_0^a\frac{\partial}{\partial x}f(x,y)dy.\\ &=\frac{d}{dx}(F(x,x)-F(x,a)) + \int_0^a\frac{\partial}{\partial x}f(x,y)dy.\\ &=f(x,x) - \frac{d}{dx}F(x,a)+\int_0^a\frac{\partial}{\partial x}f(x,y)dy. \end{align} Anyways I'm not sure where to go from here or if this is even correct, but it's the best I've been able to come up with.","Let $f:\mathbb{R}^2\rightarrow\mathbb{R}$.  Assume that $$\frac{d}{dx}\int_a^bf(x,y)dy=\int_a^b\frac{\partial}{\partial x}f(x,y)dy.$$ Use the above property and the chain rule to prove that $$\frac{d}{dx}\int_0^xf(x,y)dy = f(x,x)+\int_0^x\frac{\partial}{\partial x}f(x,y)dy.$$ This doesn't seem like it should be that hard but I haven't been able to get it, I especially can't figure out how to take advantage of the chain rule.  Here's my best shot: Let $a<x$.  Then we have \begin{align} \frac{d}{dx}\int_0^xf(x,y)dy &= \frac{d}{dx}\int_a^xf(x,y)dy + \int_0^a\frac{\partial}{\partial x}f(x,y)dy.\\ &=\frac{d}{dx}(F(x,x)-F(x,a)) + \int_0^a\frac{\partial}{\partial x}f(x,y)dy.\\ &=f(x,x) - \frac{d}{dx}F(x,a)+\int_0^a\frac{\partial}{\partial x}f(x,y)dy. \end{align} Anyways I'm not sure where to go from here or if this is even correct, but it's the best I've been able to come up with.",,"['multivariable-calculus', 'partial-derivative']"
90,Leibniz integral rule confusion,Leibniz integral rule confusion,,"Consider a vector field $\mathbf B:\mathbb R\times \mathbb R^3\to\mathbb R^3$. Let there exists  some $\mathbf A:\mathbb R\times \mathbb R^3\to\mathbb R^3$ for which $\mathbf B = \nabla \times\mathbf A$ where the curl is taken over the ""$\mathbb R^3$"" argument. Now for each $t\in\mathbb R$, let a surface element $\Sigma_t$ with the topology of a disk be given.  Let $\gamma:\mathbb R\times[0,1]\to\mathbb R^3$ parameterize $\partial \Sigma_t$.  In particular, for each $t\in\mathbb R$, let $\gamma(t, \cdot)$ traverse $\partial\Sigma_t$ exactly once.  Then, using the convention that repeated indices are summed from $1$ to $3$ we have \begin{align}   \frac{d}{dt}\int_{\Sigma_t}\mathbf B\cdot d\mathbf a &= \frac{d}{dt}\int_{\partial\Sigma_t} \mathbf A\cdot d\boldsymbol \ell \\ &= \frac{d}{dt}\int_0^1 A_i \frac{\partial\gamma^i}{\partial \lambda}d\lambda \\ &= \int_0^1 \frac{\partial A_i}{\partial t}\frac{\partial\gamma^i}{\partial \lambda} d\lambda +\int_0^1\left(\frac{\partial\gamma^j}{\partial t}\frac{\partial A_i}{\partial x^j}\frac{\partial\gamma^i}{\partial\lambda} +A_i\frac{\partial^2\gamma^i}{\partial t\partial\lambda}\right)d\lambda\\ &= \int_{\Sigma_t} \frac{\partial\mathbf B}{\partial t}\cdot d\mathbf a+\int_0^1\left(\frac{\partial\gamma^j}{\partial t}\frac{\partial A_i}{\partial x^j}\frac{\partial\gamma^i}{\partial\lambda} +A_i\frac{\partial^2\gamma^i}{\partial t\partial\lambda}\right)d\lambda\\ \end{align} On the other hand, for a divergence-free vector field $\mathbf B$ (which we have here since $\mathbf B$ is the curl of $\mathbf A$), the Leibniz integral rule gives \begin{align}   \frac{d}{dt}\int_{\Sigma_t}\mathbf B\cdot d\mathbf a &= \int_{\Sigma_t} \frac{\partial\mathbf B}{\partial t}\cdot d\mathbf a - \int_{\partial\Sigma_t} \mathbf v\times \mathbf B\cdot d\boldsymbol \ell \end{align} Question. Presumably, the formula I derived above should agree with this expression, but I can't see how it does. As far as I can tell, the second term on the right of the Leibniz integral formula can be written as follows.  Firstly, as far as I can tell the notation can be translated as $$   \mathbf v = \frac{\partial\gamma}{\partial t}, \qquad d\boldsymbol \ell = \frac{\partial\gamma}{\partial\lambda}d\lambda $$ Secondly, we note the following identity $$   \mathbf v\times\mathbf B = (v^j\partial_i A_j - v^j\partial_jA_i)\mathbf e^i $$ Putting these facts together gives $$   -\int_{\partial\Sigma_t} \mathbf v\times\mathbf B\cdot d\boldsymbol\ell = \int_0^1\left(\frac{\partial\gamma^j}{\partial t}\frac{\partial A_i}{\partial x^j}\frac{\partial\gamma^i}{\partial\lambda} -\frac{\partial\gamma^j}{\partial t}\frac{\partial A_j}{\partial x^i}\frac{\partial\gamma^i}{\partial\lambda}\right)d\lambda $$ The second term in the integrand does not seem to match the second term in the integrand at the end of the first computation above. Have I interpreted the integral formula incorrectly?  Perhaps my first computation has an error in it?  Thanks for the help.","Consider a vector field $\mathbf B:\mathbb R\times \mathbb R^3\to\mathbb R^3$. Let there exists  some $\mathbf A:\mathbb R\times \mathbb R^3\to\mathbb R^3$ for which $\mathbf B = \nabla \times\mathbf A$ where the curl is taken over the ""$\mathbb R^3$"" argument. Now for each $t\in\mathbb R$, let a surface element $\Sigma_t$ with the topology of a disk be given.  Let $\gamma:\mathbb R\times[0,1]\to\mathbb R^3$ parameterize $\partial \Sigma_t$.  In particular, for each $t\in\mathbb R$, let $\gamma(t, \cdot)$ traverse $\partial\Sigma_t$ exactly once.  Then, using the convention that repeated indices are summed from $1$ to $3$ we have \begin{align}   \frac{d}{dt}\int_{\Sigma_t}\mathbf B\cdot d\mathbf a &= \frac{d}{dt}\int_{\partial\Sigma_t} \mathbf A\cdot d\boldsymbol \ell \\ &= \frac{d}{dt}\int_0^1 A_i \frac{\partial\gamma^i}{\partial \lambda}d\lambda \\ &= \int_0^1 \frac{\partial A_i}{\partial t}\frac{\partial\gamma^i}{\partial \lambda} d\lambda +\int_0^1\left(\frac{\partial\gamma^j}{\partial t}\frac{\partial A_i}{\partial x^j}\frac{\partial\gamma^i}{\partial\lambda} +A_i\frac{\partial^2\gamma^i}{\partial t\partial\lambda}\right)d\lambda\\ &= \int_{\Sigma_t} \frac{\partial\mathbf B}{\partial t}\cdot d\mathbf a+\int_0^1\left(\frac{\partial\gamma^j}{\partial t}\frac{\partial A_i}{\partial x^j}\frac{\partial\gamma^i}{\partial\lambda} +A_i\frac{\partial^2\gamma^i}{\partial t\partial\lambda}\right)d\lambda\\ \end{align} On the other hand, for a divergence-free vector field $\mathbf B$ (which we have here since $\mathbf B$ is the curl of $\mathbf A$), the Leibniz integral rule gives \begin{align}   \frac{d}{dt}\int_{\Sigma_t}\mathbf B\cdot d\mathbf a &= \int_{\Sigma_t} \frac{\partial\mathbf B}{\partial t}\cdot d\mathbf a - \int_{\partial\Sigma_t} \mathbf v\times \mathbf B\cdot d\boldsymbol \ell \end{align} Question. Presumably, the formula I derived above should agree with this expression, but I can't see how it does. As far as I can tell, the second term on the right of the Leibniz integral formula can be written as follows.  Firstly, as far as I can tell the notation can be translated as $$   \mathbf v = \frac{\partial\gamma}{\partial t}, \qquad d\boldsymbol \ell = \frac{\partial\gamma}{\partial\lambda}d\lambda $$ Secondly, we note the following identity $$   \mathbf v\times\mathbf B = (v^j\partial_i A_j - v^j\partial_jA_i)\mathbf e^i $$ Putting these facts together gives $$   -\int_{\partial\Sigma_t} \mathbf v\times\mathbf B\cdot d\boldsymbol\ell = \int_0^1\left(\frac{\partial\gamma^j}{\partial t}\frac{\partial A_i}{\partial x^j}\frac{\partial\gamma^i}{\partial\lambda} -\frac{\partial\gamma^j}{\partial t}\frac{\partial A_j}{\partial x^i}\frac{\partial\gamma^i}{\partial\lambda}\right)d\lambda $$ The second term in the integrand does not seem to match the second term in the integrand at the end of the first computation above. Have I interpreted the integral formula incorrectly?  Perhaps my first computation has an error in it?  Thanks for the help.",,"['differential-geometry', 'multivariable-calculus', 'vector-analysis']"
91,"$T: \mathbb{R}^n\to \mathbb{R}^m$, $n>m$, $\operatorname{rank}(dT)=m$, show $T$ maps open sets to open sets.",", , , show  maps open sets to open sets.",T: \mathbb{R}^n\to \mathbb{R}^m n>m \operatorname{rank}(dT)=m T,"Suppose $T: \mathbb{R}^n\to \mathbb{R}^m$, $n>m$, with $dT$ having rank $m$ at all points in an open set $D \subset \mathbb{R}^n$. What is a proof that $T$ maps $D$ into an open set in $\mathbb{R}^m$? One thing I'm thinking is that $dT$ cannot have a zero row, so at each point $p\in T(D)$, no component is independent of the domain...for each component $y_j$ in the target, at a point $p\in T(D)$, there is some variable $x_i$ in the domain such that if we wiggle $x_i$, $y_j$ should wiggle too. This should allow us to prove that $T(D)$ is open, right (since by ""wiggling"" in each component of the target, we can get an open neighborhood)? In fact $T(S)$ is open for $S\subset D$ open. Is this the standard proof, or is there a ""cleaner"" one?","Suppose $T: \mathbb{R}^n\to \mathbb{R}^m$, $n>m$, with $dT$ having rank $m$ at all points in an open set $D \subset \mathbb{R}^n$. What is a proof that $T$ maps $D$ into an open set in $\mathbb{R}^m$? One thing I'm thinking is that $dT$ cannot have a zero row, so at each point $p\in T(D)$, no component is independent of the domain...for each component $y_j$ in the target, at a point $p\in T(D)$, there is some variable $x_i$ in the domain such that if we wiggle $x_i$, $y_j$ should wiggle too. This should allow us to prove that $T(D)$ is open, right (since by ""wiggling"" in each component of the target, we can get an open neighborhood)? In fact $T(S)$ is open for $S\subset D$ open. Is this the standard proof, or is there a ""cleaner"" one?",,"['calculus', 'multivariable-calculus', 'differential-topology']"
92,Irrotational Vortices,Irrotational Vortices,,"When trying to find some further information regarding irrotational flows, I encountered the notion of an ""irrotational vortex"": http://en.wikipedia.org/wiki/Vortex#Irrotational_vortices In the animation wiki has (see above), one can see that both a rotational vortex, and an irrotational one cause a particle to rotate around itself... I can't understand from their explanation why is the irrotational vortex is ""irrotational"" ... Can someone please explain it to me? Thanks in advance","When trying to find some further information regarding irrotational flows, I encountered the notion of an ""irrotational vortex"": http://en.wikipedia.org/wiki/Vortex#Irrotational_vortices In the animation wiki has (see above), one can see that both a rotational vortex, and an irrotational one cause a particle to rotate around itself... I can't understand from their explanation why is the irrotational vortex is ""irrotational"" ... Can someone please explain it to me? Thanks in advance",,"['multivariable-calculus', 'fluid-dynamics']"
93,Gauss's divergence theorem for a scalar field,Gauss's divergence theorem for a scalar field,,"I know Gauss's divergence theorem for a vector field: $$\iint{\vec{F}\cdot\hat{n}}\space{dS}=\iiint\nabla\cdot\vec{F}\space{dV}$$ But how do you apply this to a scalar field? For example, if you wanted to find the surface integral of $z^2$ over a unit cube: $$\iint_{S}z^2{dS}$$ where $S$ is the surface of unit cube, how would you approach this using Gauss's divergence theorem?","I know Gauss's divergence theorem for a vector field: $$\iint{\vec{F}\cdot\hat{n}}\space{dS}=\iiint\nabla\cdot\vec{F}\space{dV}$$ But how do you apply this to a scalar field? For example, if you wanted to find the surface integral of $z^2$ over a unit cube: $$\iint_{S}z^2{dS}$$ where $S$ is the surface of unit cube, how would you approach this using Gauss's divergence theorem?",,"['multivariable-calculus', 'vector-analysis', 'divergence-operator', 'scalar-fields']"
94,"If $\frac{\partial \varphi}{\partial x}=f(x,y),\frac{\partial\varphi}{\partial y}=g(x,y)$, what is $\varphi$?","If , what is ?","\frac{\partial \varphi}{\partial x}=f(x,y),\frac{\partial\varphi}{\partial y}=g(x,y) \varphi","Suppose we have a real-valued function $\varphi(x,y)$ such that $$ \frac{\partial \varphi}{\partial x}=f(x,y)\quad\text{and}\quad\frac{\partial\varphi}{\partial y}=g(x,y) $$ for some functions $f$ and $g$. How can we recover the function $\varphi$? Using the chain rule, we have $$ d\varphi=\frac{\partial\varphi}{\partial x}dx+\frac{\partial\varphi}{\partial y}dy=f(x,y)dx+g(x,y)dy $$ So maybe $\varphi$ can be found by $$\varphi(x,y)=\int f(x,y)dx+g(x,y)dy$$ but I am not sure what this integral means. It seems like a line integral, but over what curve? What is an indefinite line integral?","Suppose we have a real-valued function $\varphi(x,y)$ such that $$ \frac{\partial \varphi}{\partial x}=f(x,y)\quad\text{and}\quad\frac{\partial\varphi}{\partial y}=g(x,y) $$ for some functions $f$ and $g$. How can we recover the function $\varphi$? Using the chain rule, we have $$ d\varphi=\frac{\partial\varphi}{\partial x}dx+\frac{\partial\varphi}{\partial y}dy=f(x,y)dx+g(x,y)dy $$ So maybe $\varphi$ can be found by $$\varphi(x,y)=\int f(x,y)dx+g(x,y)dy$$ but I am not sure what this integral means. It seems like a line integral, but over what curve? What is an indefinite line integral?",,"['multivariable-calculus', 'partial-differential-equations', 'indefinite-integrals', 'partial-derivative']"
95,Finding the absolute max and min of a function bounded by a domain D.,Finding the absolute max and min of a function bounded by a domain D.,,"The task is to find the absolute max and absolute min of this function: $f(x,y) = 4xy^2-x^2y^2-xy^3$ on the domain D: $D=\{(x,y) | x\ge0, y\ge0, x + y \le 6\}$ So I get the partial derivatives of $f(x,y)$ to find the critical points $f_x(x,y) = 4y^2-2xy^2-y^3$ $f_y(x,y) = 8xy-2x^2y-3xy^2$ Solving these for zero I get the critical point of $(0,0)$ . and $f(0,0) = 0$ . Then I find the extreme values at points on domain D which are $(0,0),(6,0),(0,6).$ In all cases the extreme values $f(0,6)$ and $f(6,0)$ are zero. I get the feeling I'm doing something wrong here. What are the absolute min/max values?",The task is to find the absolute max and absolute min of this function: on the domain D: So I get the partial derivatives of to find the critical points Solving these for zero I get the critical point of . and . Then I find the extreme values at points on domain D which are In all cases the extreme values and are zero. I get the feeling I'm doing something wrong here. What are the absolute min/max values?,"f(x,y) = 4xy^2-x^2y^2-xy^3 D=\{(x,y) | x\ge0, y\ge0, x + y \le 6\} f(x,y) f_x(x,y) = 4y^2-2xy^2-y^3 f_y(x,y) = 8xy-2x^2y-3xy^2 (0,0) f(0,0) = 0 (0,0),(6,0),(0,6). f(0,6) f(6,0)","['calculus', 'multivariable-calculus']"
96,Is there a reason why curvature is defined as the change in $\mathbf{T}$ with respect to arc length $s$,Is there a reason why curvature is defined as the change in  with respect to arc length,\mathbf{T} s,"And not with respect to time $t$? (or whatever parameter one is using) $\displaystyle |\frac{d\mathbf{T}(t)}{\mathit{dt}}|$ seems more intuitive to me. I can also see that $\displaystyle |\frac{d\mathbf{T}(t)}{\mathit{ds}}| = |\frac{d\mathbf{r}'(t)}{dt}|$ (because $\displaystyle |\mathbf{r}'(t)| = \frac{ds}{dt}$, which does make sense, but I don't quite understand the implications of $\displaystyle |\frac{d\mathbf{T}(t)}{\mathit{dt}}|$ vs. $\displaystyle |\frac{d\mathbf{T}(t)}{\mathit{ds}}|$ and why the one was chosen over the other.","And not with respect to time $t$? (or whatever parameter one is using) $\displaystyle |\frac{d\mathbf{T}(t)}{\mathit{dt}}|$ seems more intuitive to me. I can also see that $\displaystyle |\frac{d\mathbf{T}(t)}{\mathit{ds}}| = |\frac{d\mathbf{r}'(t)}{dt}|$ (because $\displaystyle |\mathbf{r}'(t)| = \frac{ds}{dt}$, which does make sense, but I don't quite understand the implications of $\displaystyle |\frac{d\mathbf{T}(t)}{\mathit{dt}}|$ vs. $\displaystyle |\frac{d\mathbf{T}(t)}{\mathit{ds}}|$ and why the one was chosen over the other.",,"['differential-geometry', 'multivariable-calculus']"
97,invariance of dimension under diffeomorphism of real subspaces,invariance of dimension under diffeomorphism of real subspaces,,"This is a problem from Arnold's book on ODEs I cannot solve. Prove that if $f:U\to V$ is a diffeomorphism, then the Euclidean spaces with the domains $U$ and $V$ as subsets have the same dimension. Hint. Use the implicit function theorem. Thanks.","This is a problem from Arnold's book on ODEs I cannot solve. Prove that if $f:U\to V$ is a diffeomorphism, then the Euclidean spaces with the domains $U$ and $V$ as subsets have the same dimension. Hint. Use the implicit function theorem. Thanks.",,"['real-analysis', 'multivariable-calculus']"
98,Using calculus notation arithmetically,Using calculus notation arithmetically,,"Today in microeconomics class the following question was covered by the tutor: If $z = x^2 + y^2$ changes from $(1,2)$ to $(1.05,2.1)$ , compute the value of $\Delta z$ and $dz$ . If we write $f(x,y)=z$ , then simply $\Delta z = f(1.05,2.1) - f(1,2)$ . However in the case of $dz$ things get interesting. The tutor derived the formula: $$ df(x,y) = \frac{d}{dx}f(x,y)dx + \frac{d}{dy}f(x,y)dy \\ = 2xdx + 2ydy $$ Then we filled in $x=1,y=2,dx=0.05,dy=0.1$ to get $dz=df(1,2)=0.5$ (actually we arrived at $0.9$ but I can't see how that happened). The idea is that it is an approximation of $\Delta z$ . I was under the impression that you weren't allowed to use notation like this, to assign values to e.g. $dx$ . When I pressed the tutor about it he said he doesn't think  it's mathematically sound but that this is what the lecturer wants to see.","Today in microeconomics class the following question was covered by the tutor: If changes from to , compute the value of and . If we write , then simply . However in the case of things get interesting. The tutor derived the formula: Then we filled in to get (actually we arrived at but I can't see how that happened). The idea is that it is an approximation of . I was under the impression that you weren't allowed to use notation like this, to assign values to e.g. . When I pressed the tutor about it he said he doesn't think  it's mathematically sound but that this is what the lecturer wants to see.","z = x^2 + y^2 (1,2) (1.05,2.1) \Delta z dz f(x,y)=z \Delta z = f(1.05,2.1) - f(1,2) dz  df(x,y) = \frac{d}{dx}f(x,y)dx + \frac{d}{dy}f(x,y)dy \\
= 2xdx + 2ydy  x=1,y=2,dx=0.05,dy=0.1 dz=df(1,2)=0.5 0.9 \Delta z dx",['multivariable-calculus']
99,Clarification about the triple product identity for partial derivatives,Clarification about the triple product identity for partial derivatives,,"I'm having a hard time understanding why \begin{equation}\frac{\partial x}{\partial y}\frac{\partial y}{\partial z}\frac{\partial z}{\partial x}=-1 \tag{1}\label{1} \end{equation} Wikipedia provides this derivation. I have two problems with it. The proof starts by stating that there is a function f such that $f(x,y,z)=0$ and that $z$ can be made a function of $x,y$ . Furthermore it states that there can be found a curve, along which $dz=0$ and $y$ is a function of $x$ , such that we can then write the differential of $z$ in terms of the differential of $x$ as $$dz=\frac{\partial z}{\partial x}dx+\frac{\partial z}{\partial y}\frac{\partial y}{\partial x}dx$$ The rest follows naturally from setting $dz=0$ and multiplying some partial derivatives by their inverses. I have two problems with this proof 1.Chain rule The first one is that since \begin{equation} \tag{2} \label{2} \frac{\partial z}{\partial x}=-\frac{\partial z}{\partial y}\frac{\partial y}{\partial x} \end{equation} That would mean that, by chain rule, $$\frac{\partial z}{\partial x}=-\frac{\partial z}{\partial x}$$ which would imply this partial derivative to be zero. However if this is true, \ref{1} yields $0=-1$ . Is the chain rule not valid in this case? If so, why? 2.Inverse of the partials The second is that, while applying the last step, it is implied that we obtain \ref{1} by multiplying by the inverse of the righthand-side in \ref{2}. I thought the relationship $$\frac{\partial y}{\partial x}=\frac{1}{\frac{\partial x}{\partial y}}$$ was in general not true, as pointed out in this post . Is it true in this case? And if so, why is that? Also, if that really is the case, then using the chain rule again yields, from \ref{1}, $$\frac{\partial x}{\partial z}\frac{\partial z}{\partial x}=-1 \iff 1=-1$$ What am I doing wrong?","I'm having a hard time understanding why Wikipedia provides this derivation. I have two problems with it. The proof starts by stating that there is a function f such that and that can be made a function of . Furthermore it states that there can be found a curve, along which and is a function of , such that we can then write the differential of in terms of the differential of as The rest follows naturally from setting and multiplying some partial derivatives by their inverses. I have two problems with this proof 1.Chain rule The first one is that since That would mean that, by chain rule, which would imply this partial derivative to be zero. However if this is true, \ref{1} yields . Is the chain rule not valid in this case? If so, why? 2.Inverse of the partials The second is that, while applying the last step, it is implied that we obtain \ref{1} by multiplying by the inverse of the righthand-side in \ref{2}. I thought the relationship was in general not true, as pointed out in this post . Is it true in this case? And if so, why is that? Also, if that really is the case, then using the chain rule again yields, from \ref{1}, What am I doing wrong?","\begin{equation}\frac{\partial x}{\partial y}\frac{\partial y}{\partial z}\frac{\partial z}{\partial x}=-1 \tag{1}\label{1} \end{equation} f(x,y,z)=0 z x,y dz=0 y x z x dz=\frac{\partial z}{\partial x}dx+\frac{\partial z}{\partial y}\frac{\partial y}{\partial x}dx dz=0 \begin{equation} \tag{2} \label{2} \frac{\partial z}{\partial x}=-\frac{\partial z}{\partial y}\frac{\partial y}{\partial x} \end{equation} \frac{\partial z}{\partial x}=-\frac{\partial z}{\partial x} 0=-1 \frac{\partial y}{\partial x}=\frac{1}{\frac{\partial x}{\partial y}} \frac{\partial x}{\partial z}\frac{\partial z}{\partial x}=-1 \iff 1=-1","['calculus', 'multivariable-calculus', 'proof-explanation', 'partial-derivative']"
