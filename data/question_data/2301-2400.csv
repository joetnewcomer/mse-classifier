,title_raw,title_text,title_latex,body_raw,body_text,body_latex,tags
0,"If $\lambda_n \sim \mu_n$, is it true that $\sum \exp(-\lambda_n x) \sim \sum \exp(-\mu_n x)$ as $x \to 0$?","If , is it true that  as ?",\lambda_n \sim \mu_n \sum \exp(-\lambda_n x) \sim \sum \exp(-\mu_n x) x \to 0,"If $\lambda_n,\mu_n \in \mathbb{R}$, $\lambda_n \sim \mu_n$ as $n \to +\infty$, and $\mu_n \to +\infty$ as $n \to +\infty$, is it true that   $$ \sum_{n=1}^{\infty} \exp(-\lambda_n x) \sim \sum_{n=1}^{\infty} \exp(-\mu_n x) $$   as $x \to 0^{+}$? In other words, is it true that   $$ \lim_{x \to 0^+} \frac{\sum_{n=1}^{\infty} \exp(-\lambda_n x)}{\sum_{n=1}^{\infty} \exp(-\mu_n x)} = 1? $$ Note that since $\mu_n \to +\infty$ we must also have $\lambda_n \to +\infty$ to ensure that $\lambda_n \sim \mu_n$ as $n \to +\infty$, i.e. that $$ \lim_{n \to +\infty} \frac{\lambda_n}{\mu_n} = 1. $$ We also assume that each series converges for all $x>0$. I believe this is true (and some numerical examples agree), but I can't see how to prove it.  Using the idea presented in this answer we have an upper bound like $$ \sum_{n=1}^{\infty} e^{-\lambda n x} \leq \sum_{n=1}^{\infty} e^{-(1-\epsilon)\mu_n x} + O(1) $$ with an analogous lower bound, where the term in $O(1)$ in bounded independently of $x$ (but does depend on $\epsilon$).  So, dividing through by $\sum_n e^{-\mu_n x}$, we're really interested in whether $$ \lim_{\epsilon \to 0} \lim_{x \to 0^+} \frac{\sum_{n=1}^{\infty} e^{-(1-\epsilon)\mu_n x}}{\sum_{n=1}^{\infty} e^{-\mu_n x}} = 1. $$ If this were true the result would follow. Sometimes it's possible to show this a posteriori if we know an elementary closed form or asymptotic for $\sum_n \exp(-C\lambda_n x)$ valid for all $C$ in some neighborhood of $1$ and small $x > 0$, as was the case in the second half of this answer .  In this question I am interested in the case when we do not. It was noted by PavelM in the comments that it may very well be false when $\lambda_n$ is almost $\log n$. I am definitely interested in the general question.  However, I am specifically interested in the special case where $$ \lambda_n \sim a n $$ as $n \to \infty$ for some constant $a > 0$.  Any help with this specific problem would likewise be much appreciated.","If $\lambda_n,\mu_n \in \mathbb{R}$, $\lambda_n \sim \mu_n$ as $n \to +\infty$, and $\mu_n \to +\infty$ as $n \to +\infty$, is it true that   $$ \sum_{n=1}^{\infty} \exp(-\lambda_n x) \sim \sum_{n=1}^{\infty} \exp(-\mu_n x) $$   as $x \to 0^{+}$? In other words, is it true that   $$ \lim_{x \to 0^+} \frac{\sum_{n=1}^{\infty} \exp(-\lambda_n x)}{\sum_{n=1}^{\infty} \exp(-\mu_n x)} = 1? $$ Note that since $\mu_n \to +\infty$ we must also have $\lambda_n \to +\infty$ to ensure that $\lambda_n \sim \mu_n$ as $n \to +\infty$, i.e. that $$ \lim_{n \to +\infty} \frac{\lambda_n}{\mu_n} = 1. $$ We also assume that each series converges for all $x>0$. I believe this is true (and some numerical examples agree), but I can't see how to prove it.  Using the idea presented in this answer we have an upper bound like $$ \sum_{n=1}^{\infty} e^{-\lambda n x} \leq \sum_{n=1}^{\infty} e^{-(1-\epsilon)\mu_n x} + O(1) $$ with an analogous lower bound, where the term in $O(1)$ in bounded independently of $x$ (but does depend on $\epsilon$).  So, dividing through by $\sum_n e^{-\mu_n x}$, we're really interested in whether $$ \lim_{\epsilon \to 0} \lim_{x \to 0^+} \frac{\sum_{n=1}^{\infty} e^{-(1-\epsilon)\mu_n x}}{\sum_{n=1}^{\infty} e^{-\mu_n x}} = 1. $$ If this were true the result would follow. Sometimes it's possible to show this a posteriori if we know an elementary closed form or asymptotic for $\sum_n \exp(-C\lambda_n x)$ valid for all $C$ in some neighborhood of $1$ and small $x > 0$, as was the case in the second half of this answer .  In this question I am interested in the case when we do not. It was noted by PavelM in the comments that it may very well be false when $\lambda_n$ is almost $\log n$. I am definitely interested in the general question.  However, I am specifically interested in the special case where $$ \lambda_n \sim a n $$ as $n \to \infty$ for some constant $a > 0$.  Any help with this specific problem would likewise be much appreciated.",,"['real-analysis', 'sequences-and-series', 'analysis', 'asymptotics']"
1,Can you find a function $\beta(x)$ where if $a+b=n^m$ then $\beta(\frac{a}{b})$ is irrational?,Can you find a function  where if  then  is irrational?,\beta(x) a+b=n^m \beta(\frac{a}{b}),"Can you find a function $\beta(x)$ where if $a+b=n^m$ then $\beta(\frac{a}{b})$ is irrational but if $a+b$ isn't equal to $n^m$ then it is rational ( $a$ and $b$ are co-prime)? $n>0$ and $m>1$ $m$ and $n$ are integers When $x$ is an irrational number, $\beta(x)$ can be either rational or irrational. $\beta(0)=\pi$ $\beta(x)$ is differentiable everywhere.","Can you find a function where if then is irrational but if isn't equal to then it is rational ( and are co-prime)? and and are integers When is an irrational number, can be either rational or irrational. is differentiable everywhere.",\beta(x) a+b=n^m \beta(\frac{a}{b}) a+b n^m a b n>0 m>1 m n x \beta(x) \beta(0)=\pi \beta(x),"['real-analysis', 'recreational-mathematics', 'perfect-powers']"
2,Prove this Kenneth S. Williams inequality,Prove this Kenneth S. Williams inequality,,"If $0<a_1\le a_2\le \cdots\le a_n$ , then the following inequality holds: $$\frac{1}{2n^2a_n}{\sum_{1\le i < j\le n}^{} {(a_i-a_j)^2}}\le \frac{a_1+a_2+\cdots + a_n}{n}-\sqrt [n]{a_1 a_2 \cdots a_n }{\le \frac{1}{2n^2a_1}\sum_{1\le i < j\le n}^{} {(a_i-a_j)^2}}.$$ This problem was proposed by Kenneth S. Williams, Carleton University, Ottawa in CRUX 247[1977;131] and in CRUX[1978;23,37] it is said that there is a nice simple proof but I can't find this G. Szekeres (October 1977 was published by Rennie in JCMN, NO.12) shorter proof. Can help me? Thanks. Maybe now this inequality have some methods to solve it, such as AM-GM inequality?","If , then the following inequality holds: This problem was proposed by Kenneth S. Williams, Carleton University, Ottawa in CRUX 247[1977;131] and in CRUX[1978;23,37] it is said that there is a nice simple proof but I can't find this G. Szekeres (October 1977 was published by Rennie in JCMN, NO.12) shorter proof. Can help me? Thanks. Maybe now this inequality have some methods to solve it, such as AM-GM inequality?",0<a_1\le a_2\le \cdots\le a_n \frac{1}{2n^2a_n}{\sum_{1\le i < j\le n}^{} {(a_i-a_j)^2}}\le \frac{a_1+a_2+\cdots + a_n}{n}-\sqrt [n]{a_1 a_2 \cdots a_n }{\le \frac{1}{2n^2a_1}\sum_{1\le i < j\le n}^{} {(a_i-a_j)^2}}.,"['real-analysis', 'multivariable-calculus', 'inequality', 'a.m.-g.m.-inequality']"
3,Limits and substitution,Limits and substitution,,"As in @RobertZ's answer to this question , we often perform substitutions when evaluating limits. For instance, if you're asked to show that  $$ L = \lim_{t \to 0} \frac{\sin t^3}{t^3} = 1, $$ it's pretty common to say ""Let $x = t^3$; then as $t \to 0$, we have $x \to 0$, so  $$ L = \lim_{x \to 0} \frac{\sin x}{x} $$ which we know is $1$, and we're done."" What's going on here in general is an application of the following ""Theorem"" Theorem 1 : If the function $g$ satisfies [ fill in missing properties ] and $$\lim_{t \to a} g(t) = b,$$ then  $$ \lim_{t \to a} f(g(t)) = \lim_{x \to b} f(x), $$ i.e., one limit exists if and only if the other does , and if they both exist, they're equal. In the example above, $f(x) = \frac{\sin x}{x}$ and $g(t) = t^3$ and $a = b = 0$. There's an alternative form, in which we're asked to show that $$ L = \lim_{t \to 0} \frac{\sin \sqrt[3]{t}}{\sqrt[3]{t}} = 1, $$ it's pretty common to say ""Let $t = x^3$; then as $t \to 0$, we have $x \to 0$, so  $$ L = \lim_{x \to 0} \frac{\sin x}{x} $$ which we know is $1$, and we're done."" In this case, the implicit theorem is very similar to the other, but with the role of $g$ reversed (i.e., we're substituting $ t = x^3$ instead of $x = t^3$, so the natural form of the theorem puts $g$ on the other side): Theorem 2 : If the function $g$ satisfies [ fill in missing properties ] and $$\lim_{x \to b} g(x) = a,$$ then  $$ \lim_{t \to a} f(t) = \lim_{x \to b} f(g(x)). $$ In the second example above, we have $a = b = 0$, $f(t) = \frac{\sin \sqrt[3]{t}}{\sqrt[3]{t}}$, and $g(x) = x^3$. The two theorems are obviously the same: if you swap $a$ and $b$, $x$ and $t$, and reverse the equality in the last line, they're identical. But each represents a different approach to working with limits, so I've stated both. In the second form, it's clearly important that $g$ be surjective near $a$ (i.e., for every small enough interval $I = (b-\epsilon, b + \epsilon)$, there's an interval $I' = (a-\delta, a + \delta)$ such that $I- \{b\} \subset g(I' - \{a\})$. (Hat-tip to MathematicsStudent1122 for the observation that I need to delete $a$ and $b$ from those intervals).  Otherwise you could use things substitutions like $s = t^2$, which would turn a two-sided limit into a one-sided one (or vice versa), in which case one limit might exists and the other might not. Addendum to clarify why this might matter, for @MathematicsStudent1122: Consider $$f(x) = \begin{cases} 1 & x \ge 0 \\ 0 & x < 0 \end{cases}.$$ and look at $L = \lim_{x \to 0} f(x^2)$. It's clear that $L$ exists and is $1$. But if we substitute $t = x^2$, then we get $L = \lim_{t \to 0} f(t)$, which does not exist; hence this ""substitution"" is not valid: I've turned what amounts to a 1-sided limit (which exists) into a two-sided limit (which does not exist). The domains of $f$ and $g$ are both all of $\Bbb R$. (End of addendum) My question is this: What is a reasonable set of missing properties for each of these theorems? (I can work out the exact properties easily enough by running through the definitions, but they don't seem to be very helpful/checkable.) One answer might be ""$g$ is locally a bijection"", but that rules out things like $y = x + x\sin \frac{1}{x}$ near $x = 0$, so it seems too limited. (It also rules out things like $x \mapsto x + \sin x$ for limits as $x \to \infty$, which is a pity.) I recognize that this is not a strictly mathematical question. But my goal is to come up with a ""calculus student's theorem"", one that says ""if you're trying to work out a limit, which may or may not exist, then it's OK to do substitutions of this sort along the way,"" and which will cover the vast majority of the problems that they might encounter in a standard calculus book, or even in Spivak's book. This question gives two theorems, but both have assumptions about the existence of limits. This one comes a little closer, but still isn't entirely satisfactory. I'd love any nice-enough condition to be broadly useful. In particular, I think it's completely reasonable to require, for instance, that the ""substitution function"" $g$ be continuous, and perhaps even differentiable (although I doubt that's of much use).","As in @RobertZ's answer to this question , we often perform substitutions when evaluating limits. For instance, if you're asked to show that  $$ L = \lim_{t \to 0} \frac{\sin t^3}{t^3} = 1, $$ it's pretty common to say ""Let $x = t^3$; then as $t \to 0$, we have $x \to 0$, so  $$ L = \lim_{x \to 0} \frac{\sin x}{x} $$ which we know is $1$, and we're done."" What's going on here in general is an application of the following ""Theorem"" Theorem 1 : If the function $g$ satisfies [ fill in missing properties ] and $$\lim_{t \to a} g(t) = b,$$ then  $$ \lim_{t \to a} f(g(t)) = \lim_{x \to b} f(x), $$ i.e., one limit exists if and only if the other does , and if they both exist, they're equal. In the example above, $f(x) = \frac{\sin x}{x}$ and $g(t) = t^3$ and $a = b = 0$. There's an alternative form, in which we're asked to show that $$ L = \lim_{t \to 0} \frac{\sin \sqrt[3]{t}}{\sqrt[3]{t}} = 1, $$ it's pretty common to say ""Let $t = x^3$; then as $t \to 0$, we have $x \to 0$, so  $$ L = \lim_{x \to 0} \frac{\sin x}{x} $$ which we know is $1$, and we're done."" In this case, the implicit theorem is very similar to the other, but with the role of $g$ reversed (i.e., we're substituting $ t = x^3$ instead of $x = t^3$, so the natural form of the theorem puts $g$ on the other side): Theorem 2 : If the function $g$ satisfies [ fill in missing properties ] and $$\lim_{x \to b} g(x) = a,$$ then  $$ \lim_{t \to a} f(t) = \lim_{x \to b} f(g(x)). $$ In the second example above, we have $a = b = 0$, $f(t) = \frac{\sin \sqrt[3]{t}}{\sqrt[3]{t}}$, and $g(x) = x^3$. The two theorems are obviously the same: if you swap $a$ and $b$, $x$ and $t$, and reverse the equality in the last line, they're identical. But each represents a different approach to working with limits, so I've stated both. In the second form, it's clearly important that $g$ be surjective near $a$ (i.e., for every small enough interval $I = (b-\epsilon, b + \epsilon)$, there's an interval $I' = (a-\delta, a + \delta)$ such that $I- \{b\} \subset g(I' - \{a\})$. (Hat-tip to MathematicsStudent1122 for the observation that I need to delete $a$ and $b$ from those intervals).  Otherwise you could use things substitutions like $s = t^2$, which would turn a two-sided limit into a one-sided one (or vice versa), in which case one limit might exists and the other might not. Addendum to clarify why this might matter, for @MathematicsStudent1122: Consider $$f(x) = \begin{cases} 1 & x \ge 0 \\ 0 & x < 0 \end{cases}.$$ and look at $L = \lim_{x \to 0} f(x^2)$. It's clear that $L$ exists and is $1$. But if we substitute $t = x^2$, then we get $L = \lim_{t \to 0} f(t)$, which does not exist; hence this ""substitution"" is not valid: I've turned what amounts to a 1-sided limit (which exists) into a two-sided limit (which does not exist). The domains of $f$ and $g$ are both all of $\Bbb R$. (End of addendum) My question is this: What is a reasonable set of missing properties for each of these theorems? (I can work out the exact properties easily enough by running through the definitions, but they don't seem to be very helpful/checkable.) One answer might be ""$g$ is locally a bijection"", but that rules out things like $y = x + x\sin \frac{1}{x}$ near $x = 0$, so it seems too limited. (It also rules out things like $x \mapsto x + \sin x$ for limits as $x \to \infty$, which is a pity.) I recognize that this is not a strictly mathematical question. But my goal is to come up with a ""calculus student's theorem"", one that says ""if you're trying to work out a limit, which may or may not exist, then it's OK to do substitutions of this sort along the way,"" and which will cover the vast majority of the problems that they might encounter in a standard calculus book, or even in Spivak's book. This question gives two theorems, but both have assumptions about the existence of limits. This one comes a little closer, but still isn't entirely satisfactory. I'd love any nice-enough condition to be broadly useful. In particular, I think it's completely reasonable to require, for instance, that the ""substitution function"" $g$ be continuous, and perhaps even differentiable (although I doubt that's of much use).",,"['calculus', 'real-analysis']"
4,Checking a Limit Proof,Checking a Limit Proof,,"I have to prove that $\displaystyle \lim_{x \to c} \sqrt{x}=\sqrt{c},\;c>0, x>0$ So I have to show that given any $\epsilon>0$, there exists a $\delta>0$ that for all x in the domain $0<|x-c|<\delta$ implies $|\sqrt{x}-\sqrt{c}|<\epsilon$ So I have $|\sqrt{x}-\sqrt{c}|=|x-c|/(\sqrt{x}+\sqrt{c})<|x-c|/\sqrt{c}<\epsilon$, So for any $\epsilon$>0, I let $\delta=\sqrt{c}\epsilon$, So now $|\sqrt{x}-\sqrt{c}|=|x-c|/(\sqrt{x}+\sqrt{c})<|x-c|/\sqrt{c}|<\delta/\sqrt{c}=\epsilon\sqrt{c}/\sqrt{c}=\epsilon$ Does this proof work?","I have to prove that $\displaystyle \lim_{x \to c} \sqrt{x}=\sqrt{c},\;c>0, x>0$ So I have to show that given any $\epsilon>0$, there exists a $\delta>0$ that for all x in the domain $0<|x-c|<\delta$ implies $|\sqrt{x}-\sqrt{c}|<\epsilon$ So I have $|\sqrt{x}-\sqrt{c}|=|x-c|/(\sqrt{x}+\sqrt{c})<|x-c|/\sqrt{c}<\epsilon$, So for any $\epsilon$>0, I let $\delta=\sqrt{c}\epsilon$, So now $|\sqrt{x}-\sqrt{c}|=|x-c|/(\sqrt{x}+\sqrt{c})<|x-c|/\sqrt{c}|<\delta/\sqrt{c}=\epsilon\sqrt{c}/\sqrt{c}=\epsilon$ Does this proof work?",,"['real-analysis', 'solution-verification', 'epsilon-delta']"
5,Pointwise estimate for a sequence of mollified functions,Pointwise estimate for a sequence of mollified functions,,"In the answer to Characterisation of one-dimensional Sobolev space Tomás wrote ... let $\eta_\delta$ be the standard mollifier sequence . Let $u_\delta=\eta_\delta\star u$ and note that for any $c\in (a,b)$ $$|u_\delta(x)-u_\epsilon(x)|\le \int_c^x |u'_\delta (t)-u'_\epsilon(t)|dt+|u_\delta (c)-u_\epsilon(c)|\tag{1}.$$ Since I am new to this subject, I'd like to know which theorem/lemma Tomás used to get inequality (1).","In the answer to Characterisation of one-dimensional Sobolev space Tomás wrote ... let $\eta_\delta$ be the standard mollifier sequence . Let $u_\delta=\eta_\delta\star u$ and note that for any $c\in (a,b)$ $$|u_\delta(x)-u_\epsilon(x)|\le \int_c^x |u'_\delta (t)-u'_\epsilon(t)|dt+|u_\delta (c)-u_\epsilon(c)|\tag{1}.$$ Since I am new to this subject, I'd like to know which theorem/lemma Tomás used to get inequality (1).",,['real-analysis']
6,Changing one point does not change the Riemann integral,Changing one point does not change the Riemann integral,,"I tried to prove the following. Please could somebody tell me if my proof is correct? Let $f: [a,b]\to \mathbb R$ be Riemann integrable. Then changing one   value of $f$ then $f$ is still integrable and it integrates to the   same value. My proof. Let $x \in [a,b]$ denote the point where $f$ is changed. Let $\widetilde{f}$ denote the new function with $\widetilde{f}(x) = z$ and the old function is $f(x) = y$. Let $M = |y-z|$. Let $\varepsilon > 0$. Let $U(f,P)$ denote the upper sum and $L(f,P)$ the lower sum for partition $P$. Since $f$ is integrable there exists a partition $P$ such that the upper sums minus the lower sums are less than epsilon: $$ U(f,P) - L(f,P) < \varepsilon $$ Let $Q$ be the refinement of $P$ consisting of $P$ and $\{x-{\varepsilon \over 2M}, x + {\varepsilon \over 2M}\}$. Then $U(f,P) \ge U(f,Q)$ and $L(f,P) \le L(f,Q)$. Furthermore, $|U(f,Q)-U(\widetilde{f},Q)| \le {\varepsilon \over M}\cdot M = \varepsilon$. This is true because $f$ and $\widetilde{f}$ only differ at $x$ and at $x$ they can maximally differ by $M$. Since the partition $Q$ contains the interval $(x- {\varepsilon \over 2M}, x + {\varepsilon \over 2M})$ and this interval has lenght ${\varepsilon \over M}$ the maximal difference of these sums can be only $\varepsilon$. Similarly, $|L(f,Q)-L(\widetilde{f},Q)| \le \varepsilon$. Hence $$ |U(\widetilde{f},Q) - L(\widetilde{f},Q)| \le |U(\widetilde{f},Q) - U(f,Q)| + |U(f,Q) - L(f,Q)| +  |L(f,Q) - L(\widetilde{f},Q)| \le \varepsilon $$","I tried to prove the following. Please could somebody tell me if my proof is correct? Let $f: [a,b]\to \mathbb R$ be Riemann integrable. Then changing one   value of $f$ then $f$ is still integrable and it integrates to the   same value. My proof. Let $x \in [a,b]$ denote the point where $f$ is changed. Let $\widetilde{f}$ denote the new function with $\widetilde{f}(x) = z$ and the old function is $f(x) = y$. Let $M = |y-z|$. Let $\varepsilon > 0$. Let $U(f,P)$ denote the upper sum and $L(f,P)$ the lower sum for partition $P$. Since $f$ is integrable there exists a partition $P$ such that the upper sums minus the lower sums are less than epsilon: $$ U(f,P) - L(f,P) < \varepsilon $$ Let $Q$ be the refinement of $P$ consisting of $P$ and $\{x-{\varepsilon \over 2M}, x + {\varepsilon \over 2M}\}$. Then $U(f,P) \ge U(f,Q)$ and $L(f,P) \le L(f,Q)$. Furthermore, $|U(f,Q)-U(\widetilde{f},Q)| \le {\varepsilon \over M}\cdot M = \varepsilon$. This is true because $f$ and $\widetilde{f}$ only differ at $x$ and at $x$ they can maximally differ by $M$. Since the partition $Q$ contains the interval $(x- {\varepsilon \over 2M}, x + {\varepsilon \over 2M})$ and this interval has lenght ${\varepsilon \over M}$ the maximal difference of these sums can be only $\varepsilon$. Similarly, $|L(f,Q)-L(\widetilde{f},Q)| \le \varepsilon$. Hence $$ |U(\widetilde{f},Q) - L(\widetilde{f},Q)| \le |U(\widetilde{f},Q) - U(f,Q)| + |U(f,Q) - L(f,Q)| +  |L(f,Q) - L(\widetilde{f},Q)| \le \varepsilon $$",,"['real-analysis', 'proof-verification']"
7,"What are the uses of ""squeezing""?","What are the uses of ""squeezing""?",,"Off hand, the uses of ""squeezing"" that I can think of are: showing that $\lim_{x\to0}\dfrac{\sin x}x = 1$, which is then used in finding derivatives (PS: I've just remembered this item showing $\tan'=\sec^2$ by squeezing, without first differentiating any other trigonometric functions nor finding any limits besides the one in the definition of differentiation.  I'm inclined to consider that another part of the same item on the list rather than a separate item, but I'm glad I also know this argument.); exercises like finding the limit of the above when $x\to0$ is changed to $x\to\infty$, or ascertaining the convergence or divergence of a series by doing something involving finding limits of that sort, etc.; where all of this is not subsequently used to derive other results; showing that there are functions that are differentiable everywhere but whose derivatives have discontinuities; Various things in probability, possibly the most prominent of which is the proof of the weak law of large numbers.  I'm adding this bullet point after writing the "" PS: "" below. an argument that I wrote.  Here are some specifics: Let $$ N = \text{number of persons whose income strictly exceeds }x; $$ $$ M = \text{total income of all whose income strictly exceeds }x. $$ Doing a continuous approximation to discrete variables, we pretend these vary continuously as functions of $x$.  Although they may be non-one-to-one functions of $x$, they are easily seen to be one-to-one functions of each other.  I demonstrated this proposition: Lemma: Except when $x$ is within a closed interval on which $M$ and $N$ are constant as functions of $x$, we have $\dfrac{dM}{dN}=x$. This is readily shown by squeezing: If $\Delta x>0$ then $x<\dfrac{\Delta M}{\Delta N}\le x+\Delta x$ and if $\Delta x<0$ then $x+\Delta x<\dfrac{\Delta M}{\Delta N} \le x$. Quite possibly there are other uses that I know of very well but that don't come to mind.  If you were to say to me ""How do you prove $P$?"" I might instantaneously know that it's by squeezing, but if I ask myself ""What things are done by squeezing?"", perhaps most of them don't come to mind. So my question is: How shall we extend this bulleted list of applications of squeezing, listing items in order of their importance in the work of the generic working mathematician, including, but not limited to, uses in research, scholarship, pedagogy, and applications of mathematics to other fields ? PS: I'd forgotten this item, which might be what made me think of asking this question in the first place: Independent, random variables with equal distribution satisfy: $\lim_{n \to \infty}\mathbb{P}\left(X_{n+1} > \sum_{i = 1}^{n}X_i\right) = 0$","Off hand, the uses of ""squeezing"" that I can think of are: showing that $\lim_{x\to0}\dfrac{\sin x}x = 1$, which is then used in finding derivatives (PS: I've just remembered this item showing $\tan'=\sec^2$ by squeezing, without first differentiating any other trigonometric functions nor finding any limits besides the one in the definition of differentiation.  I'm inclined to consider that another part of the same item on the list rather than a separate item, but I'm glad I also know this argument.); exercises like finding the limit of the above when $x\to0$ is changed to $x\to\infty$, or ascertaining the convergence or divergence of a series by doing something involving finding limits of that sort, etc.; where all of this is not subsequently used to derive other results; showing that there are functions that are differentiable everywhere but whose derivatives have discontinuities; Various things in probability, possibly the most prominent of which is the proof of the weak law of large numbers.  I'm adding this bullet point after writing the "" PS: "" below. an argument that I wrote.  Here are some specifics: Let $$ N = \text{number of persons whose income strictly exceeds }x; $$ $$ M = \text{total income of all whose income strictly exceeds }x. $$ Doing a continuous approximation to discrete variables, we pretend these vary continuously as functions of $x$.  Although they may be non-one-to-one functions of $x$, they are easily seen to be one-to-one functions of each other.  I demonstrated this proposition: Lemma: Except when $x$ is within a closed interval on which $M$ and $N$ are constant as functions of $x$, we have $\dfrac{dM}{dN}=x$. This is readily shown by squeezing: If $\Delta x>0$ then $x<\dfrac{\Delta M}{\Delta N}\le x+\Delta x$ and if $\Delta x<0$ then $x+\Delta x<\dfrac{\Delta M}{\Delta N} \le x$. Quite possibly there are other uses that I know of very well but that don't come to mind.  If you were to say to me ""How do you prove $P$?"" I might instantaneously know that it's by squeezing, but if I ask myself ""What things are done by squeezing?"", perhaps most of them don't come to mind. So my question is: How shall we extend this bulleted list of applications of squeezing, listing items in order of their importance in the work of the generic working mathematician, including, but not limited to, uses in research, scholarship, pedagogy, and applications of mathematics to other fields ? PS: I'd forgotten this item, which might be what made me think of asking this question in the first place: Independent, random variables with equal distribution satisfy: $\lim_{n \to \infty}\mathbb{P}\left(X_{n+1} > \sum_{i = 1}^{n}X_i\right) = 0$",,"['real-analysis', 'big-list']"
8,"Prove $\left|\sum\limits_{k=2001}^{m}a_{k}\sin{(kx)}\right|\le 1+\pi $ ,$m\ge 2001,x\in R$","Prove  ,","\left|\sum\limits_{k=2001}^{m}a_{k}\sin{(kx)}\right|\le 1+\pi  m\ge 2001,x\in R","let $\{a_{n}\}$ is non-increasing postive sequence;show that if for $n\ge 2001,na_{n}\le 1$, then for any positive integer numbers $m\ge 2001,x\in R$, we have $$\left|\sum\limits_{k=2001}^{m}a_{k}\sin{(kx)}\right|\le 1+\pi $$ This problem is 2001 china team problem. http://www.doc88.com/p-30356357991.html It is well known that Fejér-Jackson-Gronwall inequality: $$\sum\limits_{k=1}^{n}\dfrac{\sin{kx}}{k}>0$$ Inequality $\sum\limits_{1\le k\le n}\frac{\sin kx}{k}\ge 0$ (Fejer-Jackson) and we know that have $$\left|\sum\limits_{k=1}^{n}\dfrac{\sin{(kx)}}{k}\right| \le 2\sqrt{\pi}$$ Please check my answer to $\sum\limits_{i=1}^n \frac{\sin{(ix)}}{i} < 2\sqrt{\pi}$ and the simple link: How prove this inequlity $\sum_{k=n}^{2n-3}\frac{|\sin{k}|}{k}<\frac{1}{\sqrt{2}},n\ge 3$ Proving that $\sum_{k=n}^{2n-2} \frac{|\sin k\ |}{k} < 0.7\ln 2$ for $n\ge2$ But for my question How prove it?  Thank you","let $\{a_{n}\}$ is non-increasing postive sequence;show that if for $n\ge 2001,na_{n}\le 1$, then for any positive integer numbers $m\ge 2001,x\in R$, we have $$\left|\sum\limits_{k=2001}^{m}a_{k}\sin{(kx)}\right|\le 1+\pi $$ This problem is 2001 china team problem. http://www.doc88.com/p-30356357991.html It is well known that Fejér-Jackson-Gronwall inequality: $$\sum\limits_{k=1}^{n}\dfrac{\sin{kx}}{k}>0$$ Inequality $\sum\limits_{1\le k\le n}\frac{\sin kx}{k}\ge 0$ (Fejer-Jackson) and we know that have $$\left|\sum\limits_{k=1}^{n}\dfrac{\sin{(kx)}}{k}\right| \le 2\sqrt{\pi}$$ Please check my answer to $\sum\limits_{i=1}^n \frac{\sin{(ix)}}{i} < 2\sqrt{\pi}$ and the simple link: How prove this inequlity $\sum_{k=n}^{2n-3}\frac{|\sin{k}|}{k}<\frac{1}{\sqrt{2}},n\ge 3$ Proving that $\sum_{k=n}^{2n-2} \frac{|\sin k\ |}{k} < 0.7\ln 2$ for $n\ge2$ But for my question How prove it?  Thank you",,"['real-analysis', 'trigonometry', 'inequality', 'fourier-series', 'contest-math']"
9,Infinite dimensional integral inequality,Infinite dimensional integral inequality,,"Let $f \colon \mathbb{R} \times \mathbb{R} \to \mathbb{R}$ be a measurable function. I would like to prove the following inequality: $$\left(\int_{\mathbb{R}}\left\lvert \int_0^t f(s, x)\,ds\right\rvert^q\, dx \right)^{\frac{1}{q}} \le \int_0^t \left(\int_{\mathbb{R}}\lvert f(s, x)\rvert^q\,dx\right)^{\frac{1}{q}}\, ds, $$ under the minimal assumption that all integrals make sense and the rightmost term above is finite. The idea was to rewrite the inequality this way $$\left\lVert \int_0^t f(s,\cdot)\, ds\right\rVert_q \le \int_0^t \lVert f(s, \cdot) \rVert_q\, ds,$$ which looks so obviously true... But I'm afraid of some pitfall here. In fact, we cannot guarantee continuous dependence of $f$ on the first variable, so $\int_0^t f(s, \cdot)\, ds$ is not the usual Riemann integral in a Banach space. What do you think: is this approach leading somewhere or I'd better try another one? (Which one, just in case? :-) ) EDIT: Answer I've found a very satisfactory answer in Hardy-Littlewood-Polya's Inequalities (@Willie Wong: thank you!). I'm glad to expose it here (with slightly different language, in case you ask). Theorem Let $\Omega_t, \Omega_x$ be $\sigma$ -finite measure spaces and $f \colon \Omega_t \times \Omega_x \to [0, \infty]$ be a measurable function. If $1 < p < \infty$ then $$\left\lVert \int_{\Omega_t} f(s, \cdot)\, ds\right\rVert_p \le \int_{\Omega_t}\lVert f(s, \cdot) \rVert_p \,ds,$$ where $\lVert \cdot \rVert_p$ refers to $L^p(\Omega_x)$ . Lemma Let $\Omega$ be a $\sigma$ -finite measure space and $J \colon \Omega \to [0, \infty]$ a measurable function. If $1 < p < \infty$ and $F \ge 0$ then the following statements are equivalent: $\lVert J \rVert_p \le F$ ; $\forall g \in L^{p'}(\Omega), g \ge 0, \int_{\Omega} g^{p'}dx \le 1$ we have $\int_{\Omega}Jg\, dx \le F$ . Proof of Theorem Let $J(y)=\int_{\Omega_t}f(s, y)\, ds$ . $J$ is a measurable positive function on $\Omega_x$ . Take $g \in L^{p'}(\Omega_x), g \ge 0, \int_{\Omega_x}g(y)dy \le 1$ . Then by Fubini's theorem and Hölder's inequality we have $$\int_{\Omega_x}J(y)g(y)\, dy = \int_{\Omega_t}ds \int_{\Omega_x}f(s, y)g(y)dy\le \int_{\Omega_t}\left(\int_{\Omega_x}f(s, y)^p dy\right)^{\frac{1}{p}}\, ds, $$ that is, $\int_{\Omega_x}J(y)g(y)dy\le \int_{\Omega_t} \lVert f(s, \cdot) \rVert_p\, ds$ and so $\lVert J \rVert_p \le \int_{\Omega_t} \lVert f(s, \cdot)\rVert_p\, ds$ by the lemma.    //// The general principle here is very interesting: if you want to prove an inequality like $\int J^p\, dx \le \text{something}$ , you can get past that annoying exponent $p$ by proving $\int Jg\, dx \le \text{something}$ for all suitable $g$ . References Hardy-Littlewood-Polya, Inequalities : my Theorem is their Theorem 202 , my Lemma is their Theorem 191 . EDIT 2020 Let us see how to prove the Lemma. It looks MUCH tougher than it actually is. We don't actually need any functional analysis to prove it, no dual spaces or anything like that. The proof that 1. $\Rightarrow$ 2. is literally just an immediate application of the inequality of Hölder. The proof that 2. $\Rightarrow$ 1. is based on the obvious computation $$ \lVert J^{p-1}\rVert_{p'}=\lVert J\rVert_p^{p-1}, $$ which motivates us to write $$ \lVert J\rVert_p^p=\lVert J\rVert_p^{p-1}\int J(x)\frac{J^{p-1}(x)}{\lVert J^{p-1}\rVert_{p'}}\, dx, $$ and now we can apply the assumption 2 to bound the integral, because $\frac{J^{p-1}(x)}{\lVert J^{p-1}\rVert_{p'}}$ has $p'$ norm equal to 1. We conclude $$ \lVert J\rVert_p^p\le \lVert J\rVert_p^{p-1} F, $$ from which 1. immediately follows. $\Box$ Remark . As stressed by the book Analysis of Lieb and Loss, in the case of $L^p$ spaces the abstract functional analysis is not necessary, and it actually sometimes obscures what is going on. This is a good example of that phenomenon. This edit comes nine years after the original question, one of my first ones. I am still here. Looks like I got hooked for real. :-)","Let be a measurable function. I would like to prove the following inequality: under the minimal assumption that all integrals make sense and the rightmost term above is finite. The idea was to rewrite the inequality this way which looks so obviously true... But I'm afraid of some pitfall here. In fact, we cannot guarantee continuous dependence of on the first variable, so is not the usual Riemann integral in a Banach space. What do you think: is this approach leading somewhere or I'd better try another one? (Which one, just in case? :-) ) EDIT: Answer I've found a very satisfactory answer in Hardy-Littlewood-Polya's Inequalities (@Willie Wong: thank you!). I'm glad to expose it here (with slightly different language, in case you ask). Theorem Let be -finite measure spaces and be a measurable function. If then where refers to . Lemma Let be a -finite measure space and a measurable function. If and then the following statements are equivalent: ; we have . Proof of Theorem Let . is a measurable positive function on . Take . Then by Fubini's theorem and Hölder's inequality we have that is, and so by the lemma.    //// The general principle here is very interesting: if you want to prove an inequality like , you can get past that annoying exponent by proving for all suitable . References Hardy-Littlewood-Polya, Inequalities : my Theorem is their Theorem 202 , my Lemma is their Theorem 191 . EDIT 2020 Let us see how to prove the Lemma. It looks MUCH tougher than it actually is. We don't actually need any functional analysis to prove it, no dual spaces or anything like that. The proof that 1. 2. is literally just an immediate application of the inequality of Hölder. The proof that 2. 1. is based on the obvious computation which motivates us to write and now we can apply the assumption 2 to bound the integral, because has norm equal to 1. We conclude from which 1. immediately follows. Remark . As stressed by the book Analysis of Lieb and Loss, in the case of spaces the abstract functional analysis is not necessary, and it actually sometimes obscures what is going on. This is a good example of that phenomenon. This edit comes nine years after the original question, one of my first ones. I am still here. Looks like I got hooked for real. :-)","f \colon \mathbb{R} \times \mathbb{R} \to \mathbb{R} \left(\int_{\mathbb{R}}\left\lvert \int_0^t f(s, x)\,ds\right\rvert^q\, dx \right)^{\frac{1}{q}} \le \int_0^t \left(\int_{\mathbb{R}}\lvert f(s, x)\rvert^q\,dx\right)^{\frac{1}{q}}\, ds,  \left\lVert \int_0^t f(s,\cdot)\, ds\right\rVert_q \le \int_0^t \lVert f(s, \cdot) \rVert_q\, ds, f \int_0^t f(s, \cdot)\, ds \Omega_t, \Omega_x \sigma f \colon \Omega_t \times \Omega_x \to [0, \infty] 1 < p < \infty \left\lVert \int_{\Omega_t} f(s, \cdot)\, ds\right\rVert_p \le \int_{\Omega_t}\lVert f(s, \cdot) \rVert_p \,ds, \lVert \cdot \rVert_p L^p(\Omega_x) \Omega \sigma J \colon \Omega \to [0, \infty] 1 < p < \infty F \ge 0 \lVert J \rVert_p \le F \forall g \in L^{p'}(\Omega), g \ge 0, \int_{\Omega} g^{p'}dx \le 1 \int_{\Omega}Jg\, dx \le F J(y)=\int_{\Omega_t}f(s, y)\, ds J \Omega_x g \in L^{p'}(\Omega_x), g \ge 0, \int_{\Omega_x}g(y)dy \le 1 \int_{\Omega_x}J(y)g(y)\, dy = \int_{\Omega_t}ds \int_{\Omega_x}f(s, y)g(y)dy\le \int_{\Omega_t}\left(\int_{\Omega_x}f(s, y)^p dy\right)^{\frac{1}{p}}\, ds,  \int_{\Omega_x}J(y)g(y)dy\le \int_{\Omega_t} \lVert f(s, \cdot) \rVert_p\, ds \lVert J \rVert_p \le \int_{\Omega_t} \lVert f(s, \cdot)\rVert_p\, ds \int J^p\, dx \le \text{something} p \int Jg\, dx \le \text{something} g \Rightarrow \Rightarrow 
\lVert J^{p-1}\rVert_{p'}=\lVert J\rVert_p^{p-1},  
\lVert J\rVert_p^p=\lVert J\rVert_p^{p-1}\int J(x)\frac{J^{p-1}(x)}{\lVert J^{p-1}\rVert_{p'}}\, dx,  \frac{J^{p-1}(x)}{\lVert J^{p-1}\rVert_{p'}} p' 
\lVert J\rVert_p^p\le \lVert J\rVert_p^{p-1} F,  \Box L^p","['real-analysis', 'functional-analysis']"
10,Inaccuracy in Schilling Proposition 6.5,Inaccuracy in Schilling Proposition 6.5,,"I found an inaccuracy in the Proposition 6.5 of the book ""Measures, Integrals and Martingales"" From René Schilling [EDIT: I'm talking about the first edition of the book] and I would like to correct this error. To be sure that I'm clear in asking the question I will list the definitions I use in this post so that there are no notational misunderstandings. I will try to make sure that even those unfamiliar with the book can answer the question. But obviously those who have studied from that book are more likely to be able to answer the question. I organized this post in the following way: In the first part there's the question. In the second part there is the list of definitions and notations that I use, so if you come across a symbol or a definition that you do not know go to read the second part. In the third part there is the reason that led me to formulate the question, how the question is related to proposition 6.5 and what was my attempt to answer the question. FIRST PART: THE QUESTION [Edit: as suggested I changed the name of the ""Theorem 0.2"" calling it ""Statement 0.2""] Statement 0.2 : Let $X$ be a set, $\mathcal F$ a semi-ring on $X$ and $\mu : \mathcal F \to [0,+\infty]$ a function then $\mu$ is a pre-measure on $X$ if and only if [ \begin{split}  	&(i) \hspace{0.5cm} \mu(\emptyset)=0 \\ 	&(ii) \hspace{0.5cm} A \in \mathcal F, B \in \mathcal F , A \cap B = \emptyset , A \cup B \in \mathcal F \implies \mu(A\cup B)=\mu (A) + \mu (B)  \\ 	&(iii) \hspace{0.5cm} A_n \in \mathcal F \forall n \in \mathbb N , A_n \subseteq A_{n+1} \forall n \in \mathbb N , \bigcup_{n \in \mathbb N} A_n \in \mathcal F \implies \mu(\bigcup_{n \in \mathbb N} A_n)=\lim _{n \to +\infty} \mu (A_n) \end{split} ] Moreover, with the additional hypothesis that $\mu (A) < + \infty \forall A \in \mathcal F$ , (iii) can be replaced by either of the following equivalent conditions: [ \begin{split}  	&(iii') \hspace{0.5cm} A_n \in \mathcal F \forall n \in \mathbb N , A_{n+1} \subseteq A_n \forall n \in \mathbb N  , \bigcap_{n \in \mathbb N} A_n \in \mathcal F \implies \mu(\bigcap_{n \in \mathbb N} A_n)=\lim _{n \to +\infty} \mu (A_n) \\ 	&(iii'') \hspace{0.5cm} A_n \in \mathcal F \forall n \in \mathbb N , A_{n+1} \subseteq A_n \forall n \in \mathbb N , \bigcap_{n \in \mathbb N} A_n = \emptyset \implies \lim _{n \to +\infty} \mu (A_n) = 0 	\end{split} ] End of the Statement 0.2 MY QUESTION IS : is Statement 0.2 True? And what is the proof of the implication ""from the right to the left""? If this implication of Statement 0.2 is not true what is an example of a set $X$ with a semi-ring $\mathcal F$ and a function $\mu$ on $\mathcal F$ which satisfies properties (i), (ii), (iii'') of Statement 0.2 but such that $\mu$ is not a pre-measure on $X$ ? I could only prove the implication ""from the left to the right"" of theorem 0.2 but i couldn't prove the viceversa. If you want to see a sketch of the proof of the implication i proved, you can find it in the third part of the post. SECOND PART: Notations and  definitions I use if $X$ is a set and $\mathcal F \subseteq \mathcal P (X)$ then we define: $\mathcal F$ is a ring on $X$ if and only if (by definition) [ \begin{split}  	&1) \hspace{0.5cm} \emptyset \in \mathcal{F} \\ 	&2) \hspace{0.5cm} A,B \in  \mathcal{F} \implies B \setminus A \in \mathcal{F} \\ 	&3) \hspace{0.5cm} A,B \in \mathcal{F} \implies A \cup B  \in \mathcal{F}  \end{split} ] $\mathcal F$ is a semiring on $X$ if and only if (by definition) [ \begin{split}  	&1) \hspace{0.5cm} \emptyset \in \mathcal{F} \\ 	&2) \hspace{0.5cm} A,B \in  \mathcal{F} \implies \exists A_1,A_2,\dots,A_n \in \mathcal{F} \text{ pairwise disjoint } : \,\; B \setminus A = \bigcup_{k=1}^{n}{A_k}\\ 	&3) \hspace{0.5cm} A,B \in \mathcal{F} \implies A \cap B  \in \mathcal{F}  \end{split} ] it is easy to prove that a ring on $X$ is also a semiring on $X$ If $X$ is a set, $ \mathcal F \subseteq \mathcal P (X) : \emptyset \in  \mathcal F$ and $\mu :  \mathcal F \to [0,+\infty] $ then $ \mu $ is a pre-measure on $(X,  \mathcal F )$ [or simply on $X$ ] if and only if (by definition) [ \begin{split}  	&1)  \hspace{0.5cm}\mu(\emptyset) = 0 \\ 	&2) \hspace{0.5cm} A_1,A_2,\dots \in \mathcal{F} \text{ pairwise disjoint , } \bigcup_{n\in\mathbb{N}}{A_n} \in \mathcal{F} \implies \mu\bigg( \bigcup_{n \in \mathbb{N}}{A_n}\bigg) = \sum_{n=1}^{\infty}{\mu(A_n)}\\ \end{split} ] so basically a pre-measure is the same as a measure with the only differences that a pre-measure isn't necessarily defined on a $\sigma$ -agebra and the $\sigma$ -additivity holds only when the union of the sets is in the domain of the function. Moreover $\mathcal J ^n$ is the set whose elements are the subset of $\mathbb R ^n$ in the form $[a_1,b_1) \times ... \times [a_n,b_n)$ where $a_i,b_i \in \mathbb R \forall i = 1,...,n$ THIRD PART: Correlation of the question with proposition 6.5 and my attempt to answer the question Now i will list some theorems of the Schilling required to understand the problem with proposition 6.5 and how it is related to Statement 0.2 Theorem 4.4 : Let $X$ be a set, $\mathcal F$ a $\sigma$ -algebra on $X$ and $\mu : \mathcal F \to [0,+\infty]$ a function then $\mu$ is a measure on $X$ if and only if [ \begin{split}  	&(i) \hspace{0.5cm} \mu(\emptyset)=0 \\ 	&(ii) \hspace{0.5cm} A \in \mathcal F, B \in \mathcal F , A \cap B = \emptyset \implies \mu(A\cup B)=\mu (A) + \mu (B)  \\ 	&(iii) \hspace{0.5cm} A_n \in \mathcal F \forall n \in \mathbb N : A_n \subseteq A_{n+1} \forall n \in \mathbb N  \implies \mu(\bigcup_{n \in \mathbb N} A_n)=\lim _{n \to +\infty} \mu (A_n) \end{split} ] Moreover, with the additional hypothesis that $\mu (A) < + \infty \forall A \in \mathcal F$ , (iii) can be replaced by either of the following equivalent conditions: [ \begin{split}  	&(iii') \hspace{0.5cm} A_n \in \mathcal F \forall n \in \mathbb N , A_{n+1} \subseteq A_n \forall n \in \mathbb N   \implies \mu(\bigcap_{n \in \mathbb N} A_n)=\lim _{n \to +\infty} \mu (A_n) \\ 	&(iii'') \hspace{0.5cm} A_n \in \mathcal F \forall n \in \mathbb N , A_{n+1} \subseteq A_n \forall n \in \mathbb N , \bigcap_{n \in \mathbb N} A_n = \emptyset \implies \lim _{n \to +\infty} \mu (A_n) = 0 	\end{split} ] end of the Theorem 4.4 Then the book states that Theorem4.4 is still valid in a more general case, regarding pre-measures instead of measures. But, due to the fact that the proof of the Theorem 4.4 requires that the family $\mathcal F$ is closed under finite intersection, union and difference of sets, then this more general theorem is true in the hypotesis that $\mathcal F$ is a ring on $X$ . I could easily prove this more general theorem, I'll copy the statement below under the name of Theorem 0.1 Theorem 0.1 :  Let $X$ be a set, $\mathcal F$ a Ring on $X$ and $\mu : \mathcal F \to [0,+\infty]$ a function then $\mu$ is a pre-measure on $X$ if and only if [ \begin{split}  	&(i) \hspace{0.5cm} \mu(\emptyset)=0 \\ 	&(ii) \hspace{0.5cm} A \in \mathcal F, B \in \mathcal F , A \cap B = \emptyset \implies \mu(A\cup B)=\mu (A) + \mu (B)  \\ 	&(iii) \hspace{0.5cm} A_n \in \mathcal F \forall n \in \mathbb N , A_n \subseteq A_{n+1} \forall n \in \mathbb N , \bigcup_{n \in \mathbb N} A_n \in \mathcal F \implies \mu(\bigcup_{n \in \mathbb N} A_n)=\lim _{n \to +\infty} \mu (A_n) \end{split} ] Moreover, with the additional hypothesis that $\mu (A) < + \infty \forall A \in \mathcal F$ , (iii) can be replaced by either of the following equivalent conditions: [ \begin{split}  	&(iii') \hspace{0.5cm} A_n \in \mathcal F \forall n \in \mathbb N , A_{n+1} \subseteq A_n \forall n \in \mathbb N  , \bigcap_{n \in \mathbb N} A_n \in \mathcal F \implies \mu(\bigcap_{n \in \mathbb N} A_n)=\lim _{n \to +\infty} \mu (A_n) \\ 	&(iii'') \hspace{0.5cm} A_n \in \mathcal F \forall n \in \mathbb N , A_{n+1} \subseteq A_n \forall n \in \mathbb N , \bigcap_{n \in \mathbb N} A_n = \emptyset \implies \lim _{n \to +\infty} \mu (A_n) = 0 	\end{split} ] End of theorem 0.1 Now the problems begin. The book defined the function $\lambda ^n : \mathcal J ^n \to [0,+\infty)$ that associates each element of $\mathcal J ^n $ to its volume. The book also proved that $\mathcal J ^n$ is a semi-ring on $\mathbb R^n$ (Proposition 6.4) My problem with the Proposition 6.5 is that the book wants to prove that $\lambda ^n$ is a pre-measure on $\mathcal J ^n$ using some sort of Theorem 0.1 but the problem is that $\mathcal J ^n$ is not a Ring on $\mathbb R^n$ but only a semi-ring. Let me be clearer, this is the proposition6.5 of the book: Proposition 6.5 : $\lambda ^n$ is a pre-measure on $\mathcal J ^n$ Proof : the proof of the book consists in showing that the properties (i) (ii) (iii'') of the theorem 0.1 holds for $\lambda ^n$ and i'm ok with this proof (obviously you need to modify the property (ii) requiring that $A \cup B \in \mathcal J ^n$ ) but the problem is that then he uses a not well specified version of the theorem 0.1 to conclude that $\lambda ^n $ is a pre-measure on $\mathbb R^n$ . But, again, you can't use theorem0.1 because $\mathcal J ^n$ is not a Ring on $\mathbb R^n$ but only a semi-ring. What was my attempt to solve this issue? Firstly I have formulated a version of the theorem suitable for semi-rings which goes under the name of Statement 0.2 (You've read it in the first part of the post) I will shortly show how i proved the first implication of the Statement 0.2 (The implication ""from the left to the right""): First of all I proved the following theorems: Theorem 0.3 : Let $X$ be a set and $\mathcal F \subseteq \mathcal P (X)$ then it does exist the smallest Ring on $X$ containing $\mathcal F$ and it's denoted with the simbol $R(\mathcal F)$ Theorem 0.4 : Let $X$ be a set and $\mathcal F$ a semi-ring on $X$ then $R(\mathcal F)$ is the set of all the finite disjoint unions of elements of $\mathcal F$ Theorem 0.5 : Let $X$ be a set and $\mathcal F$ a semi-ring on $X$ and $\mu : \mathcal F \to [0,+\infty] $ a pre-measure on $X$ then there exists and is unique an extension of $\mu$ to $R(\mathcal F)$ such that this extension is a pre-measure. Proof of the implication from the left to the right of Statement 0.2 : if $\mu$ is a pre-measure on the semi-ring $\mathcal F$ then denoted the unique extension of $\mu$ to $R(\mathcal F)$ still with $\mu$ we get the assertion thanks to Theorem0.1","I found an inaccuracy in the Proposition 6.5 of the book ""Measures, Integrals and Martingales"" From René Schilling [EDIT: I'm talking about the first edition of the book] and I would like to correct this error. To be sure that I'm clear in asking the question I will list the definitions I use in this post so that there are no notational misunderstandings. I will try to make sure that even those unfamiliar with the book can answer the question. But obviously those who have studied from that book are more likely to be able to answer the question. I organized this post in the following way: In the first part there's the question. In the second part there is the list of definitions and notations that I use, so if you come across a symbol or a definition that you do not know go to read the second part. In the third part there is the reason that led me to formulate the question, how the question is related to proposition 6.5 and what was my attempt to answer the question. FIRST PART: THE QUESTION [Edit: as suggested I changed the name of the ""Theorem 0.2"" calling it ""Statement 0.2""] Statement 0.2 : Let be a set, a semi-ring on and a function then is a pre-measure on if and only if [ ] Moreover, with the additional hypothesis that , (iii) can be replaced by either of the following equivalent conditions: [ ] End of the Statement 0.2 MY QUESTION IS : is Statement 0.2 True? And what is the proof of the implication ""from the right to the left""? If this implication of Statement 0.2 is not true what is an example of a set with a semi-ring and a function on which satisfies properties (i), (ii), (iii'') of Statement 0.2 but such that is not a pre-measure on ? I could only prove the implication ""from the left to the right"" of theorem 0.2 but i couldn't prove the viceversa. If you want to see a sketch of the proof of the implication i proved, you can find it in the third part of the post. SECOND PART: Notations and  definitions I use if is a set and then we define: is a ring on if and only if (by definition) [ ] is a semiring on if and only if (by definition) [ ] it is easy to prove that a ring on is also a semiring on If is a set, and then is a pre-measure on [or simply on ] if and only if (by definition) [ ] so basically a pre-measure is the same as a measure with the only differences that a pre-measure isn't necessarily defined on a -agebra and the -additivity holds only when the union of the sets is in the domain of the function. Moreover is the set whose elements are the subset of in the form where THIRD PART: Correlation of the question with proposition 6.5 and my attempt to answer the question Now i will list some theorems of the Schilling required to understand the problem with proposition 6.5 and how it is related to Statement 0.2 Theorem 4.4 : Let be a set, a -algebra on and a function then is a measure on if and only if [ ] Moreover, with the additional hypothesis that , (iii) can be replaced by either of the following equivalent conditions: [ ] end of the Theorem 4.4 Then the book states that Theorem4.4 is still valid in a more general case, regarding pre-measures instead of measures. But, due to the fact that the proof of the Theorem 4.4 requires that the family is closed under finite intersection, union and difference of sets, then this more general theorem is true in the hypotesis that is a ring on . I could easily prove this more general theorem, I'll copy the statement below under the name of Theorem 0.1 Theorem 0.1 :  Let be a set, a Ring on and a function then is a pre-measure on if and only if [ ] Moreover, with the additional hypothesis that , (iii) can be replaced by either of the following equivalent conditions: [ ] End of theorem 0.1 Now the problems begin. The book defined the function that associates each element of to its volume. The book also proved that is a semi-ring on (Proposition 6.4) My problem with the Proposition 6.5 is that the book wants to prove that is a pre-measure on using some sort of Theorem 0.1 but the problem is that is not a Ring on but only a semi-ring. Let me be clearer, this is the proposition6.5 of the book: Proposition 6.5 : is a pre-measure on Proof : the proof of the book consists in showing that the properties (i) (ii) (iii'') of the theorem 0.1 holds for and i'm ok with this proof (obviously you need to modify the property (ii) requiring that ) but the problem is that then he uses a not well specified version of the theorem 0.1 to conclude that is a pre-measure on . But, again, you can't use theorem0.1 because is not a Ring on but only a semi-ring. What was my attempt to solve this issue? Firstly I have formulated a version of the theorem suitable for semi-rings which goes under the name of Statement 0.2 (You've read it in the first part of the post) I will shortly show how i proved the first implication of the Statement 0.2 (The implication ""from the left to the right""): First of all I proved the following theorems: Theorem 0.3 : Let be a set and then it does exist the smallest Ring on containing and it's denoted with the simbol Theorem 0.4 : Let be a set and a semi-ring on then is the set of all the finite disjoint unions of elements of Theorem 0.5 : Let be a set and a semi-ring on and a pre-measure on then there exists and is unique an extension of to such that this extension is a pre-measure. Proof of the implication from the left to the right of Statement 0.2 : if is a pre-measure on the semi-ring then denoted the unique extension of to still with we get the assertion thanks to Theorem0.1","X \mathcal F X \mu : \mathcal F \to [0,+\infty] \mu X \begin{split} 
	&(i) \hspace{0.5cm} \mu(\emptyset)=0 \\
	&(ii) \hspace{0.5cm} A \in \mathcal F, B \in \mathcal F , A \cap B = \emptyset , A \cup B \in \mathcal F \implies \mu(A\cup B)=\mu (A) + \mu (B)  \\
	&(iii) \hspace{0.5cm} A_n \in \mathcal F \forall n \in \mathbb N , A_n \subseteq A_{n+1} \forall n \in \mathbb N , \bigcup_{n \in \mathbb N} A_n \in \mathcal F \implies \mu(\bigcup_{n \in \mathbb N} A_n)=\lim _{n \to +\infty} \mu (A_n)
\end{split} \mu (A) < + \infty \forall A \in \mathcal F \begin{split} 
	&(iii') \hspace{0.5cm} A_n \in \mathcal F \forall n \in \mathbb N , A_{n+1} \subseteq A_n \forall n \in \mathbb N  , \bigcap_{n \in \mathbb N} A_n \in \mathcal F \implies \mu(\bigcap_{n \in \mathbb N} A_n)=\lim _{n \to +\infty} \mu (A_n) \\
	&(iii'') \hspace{0.5cm} A_n \in \mathcal F \forall n \in \mathbb N , A_{n+1} \subseteq A_n \forall n \in \mathbb N , \bigcap_{n \in \mathbb N} A_n = \emptyset \implies \lim _{n \to +\infty} \mu (A_n) = 0
	\end{split} X \mathcal F \mu \mathcal F \mu X X \mathcal F \subseteq \mathcal P (X) \mathcal F X \begin{split} 
	&1) \hspace{0.5cm} \emptyset \in \mathcal{F} \\
	&2) \hspace{0.5cm} A,B \in  \mathcal{F} \implies B \setminus A \in \mathcal{F} \\
	&3) \hspace{0.5cm} A,B \in \mathcal{F} \implies A \cup B  \in \mathcal{F} 
\end{split} \mathcal F X \begin{split} 
	&1) \hspace{0.5cm} \emptyset \in \mathcal{F} \\
	&2) \hspace{0.5cm} A,B \in  \mathcal{F} \implies \exists A_1,A_2,\dots,A_n \in \mathcal{F} \text{ pairwise disjoint } : \,\; B \setminus A = \bigcup_{k=1}^{n}{A_k}\\
	&3) \hspace{0.5cm} A,B \in \mathcal{F} \implies A \cap B  \in \mathcal{F} 
\end{split} X X X  \mathcal F \subseteq \mathcal P (X) : \emptyset \in  \mathcal F \mu :  \mathcal F \to [0,+\infty]   \mu  (X,  \mathcal F ) X \begin{split} 
	&1)  \hspace{0.5cm}\mu(\emptyset) = 0 \\
	&2) \hspace{0.5cm} A_1,A_2,\dots \in \mathcal{F} \text{ pairwise disjoint , } \bigcup_{n\in\mathbb{N}}{A_n} \in \mathcal{F} \implies \mu\bigg( \bigcup_{n \in \mathbb{N}}{A_n}\bigg) = \sum_{n=1}^{\infty}{\mu(A_n)}\\
\end{split} \sigma \sigma \mathcal J ^n \mathbb R ^n [a_1,b_1) \times ... \times [a_n,b_n) a_i,b_i \in \mathbb R \forall i = 1,...,n X \mathcal F \sigma X \mu : \mathcal F \to [0,+\infty] \mu X \begin{split} 
	&(i) \hspace{0.5cm} \mu(\emptyset)=0 \\
	&(ii) \hspace{0.5cm} A \in \mathcal F, B \in \mathcal F , A \cap B = \emptyset \implies \mu(A\cup B)=\mu (A) + \mu (B)  \\
	&(iii) \hspace{0.5cm} A_n \in \mathcal F \forall n \in \mathbb N : A_n \subseteq A_{n+1} \forall n \in \mathbb N  \implies \mu(\bigcup_{n \in \mathbb N} A_n)=\lim _{n \to +\infty} \mu (A_n)
\end{split} \mu (A) < + \infty \forall A \in \mathcal F \begin{split} 
	&(iii') \hspace{0.5cm} A_n \in \mathcal F \forall n \in \mathbb N , A_{n+1} \subseteq A_n \forall n \in \mathbb N   \implies \mu(\bigcap_{n \in \mathbb N} A_n)=\lim _{n \to +\infty} \mu (A_n) \\
	&(iii'') \hspace{0.5cm} A_n \in \mathcal F \forall n \in \mathbb N , A_{n+1} \subseteq A_n \forall n \in \mathbb N , \bigcap_{n \in \mathbb N} A_n = \emptyset \implies \lim _{n \to +\infty} \mu (A_n) = 0
	\end{split} \mathcal F \mathcal F X X \mathcal F X \mu : \mathcal F \to [0,+\infty] \mu X \begin{split} 
	&(i) \hspace{0.5cm} \mu(\emptyset)=0 \\
	&(ii) \hspace{0.5cm} A \in \mathcal F, B \in \mathcal F , A \cap B = \emptyset \implies \mu(A\cup B)=\mu (A) + \mu (B)  \\
	&(iii) \hspace{0.5cm} A_n \in \mathcal F \forall n \in \mathbb N , A_n \subseteq A_{n+1} \forall n \in \mathbb N , \bigcup_{n \in \mathbb N} A_n \in \mathcal F \implies \mu(\bigcup_{n \in \mathbb N} A_n)=\lim _{n \to +\infty} \mu (A_n)
\end{split} \mu (A) < + \infty \forall A \in \mathcal F \begin{split} 
	&(iii') \hspace{0.5cm} A_n \in \mathcal F \forall n \in \mathbb N , A_{n+1} \subseteq A_n \forall n \in \mathbb N  , \bigcap_{n \in \mathbb N} A_n \in \mathcal F \implies \mu(\bigcap_{n \in \mathbb N} A_n)=\lim _{n \to +\infty} \mu (A_n) \\
	&(iii'') \hspace{0.5cm} A_n \in \mathcal F \forall n \in \mathbb N , A_{n+1} \subseteq A_n \forall n \in \mathbb N , \bigcap_{n \in \mathbb N} A_n = \emptyset \implies \lim _{n \to +\infty} \mu (A_n) = 0
	\end{split} \lambda ^n : \mathcal J ^n \to [0,+\infty) \mathcal J ^n  \mathcal J ^n \mathbb R^n \lambda ^n \mathcal J ^n \mathcal J ^n \mathbb R^n \lambda ^n \mathcal J ^n \lambda ^n A \cup B \in \mathcal J ^n \lambda ^n  \mathbb R^n \mathcal J ^n \mathbb R^n X \mathcal F \subseteq \mathcal P (X) X \mathcal F R(\mathcal F) X \mathcal F X R(\mathcal F) \mathcal F X \mathcal F X \mu : \mathcal F \to [0,+\infty]  X \mu R(\mathcal F) \mu \mathcal F \mu R(\mathcal F) \mu","['real-analysis', 'analysis', 'measure-theory', 'solution-verification']"
11,prove a challenging inequality or find a counterexample to it,prove a challenging inequality or find a counterexample to it,,"Suppose $\mathcal{M}_1$ represents the space of smooth probability density functions with unit mean, whose support is contained in $[0,\infty)$ (or $\mathbb{R}_+$ ). Define the following functional $$\mathrm{J}(f):= \int_0^\infty x\frac{(f')^2}{f} \mathrm{d}x$$ for $f \in \mathcal{M}_1.$ I am conjecturing that $$\mathrm{J}(\rho) \leq \mathrm{J}(f) \quad \text{with} \quad \rho(x): = \int_{z\geq x} \frac{(f*f)(z)}{z} \mathrm{d}z,$$ in which $f*f$ denotes the self-convolution of $f$ . I have tried some specific examples (even though not too many) and I did not find any counter-examples (analytically or numerically), so I am wondering whether there exists a proof of this conjecture/inequality, if not, a counter-example is welcome! Edit: The motivation behind this question is as follows. Take a random variable $X$ with law $f \in \mathcal{M}_1$ unit mean and supported on $[0,\infty)$ , we can think of $\mathrm{J}(f)$ as the information contained in $X$ (note that this is not the usual Fisher information, which is often denoted by $\mathrm{I}(f)$ or $\mathrm{I}(X)$ ). Here $\rho$ is the law of $U(X+Y)$ with $U \sim \mathrm{Uniform}([0,1])$ and $Y$ being an i.i.d. copy of $X$ , in which $U$ is also independent of $X$ and $Y$ . I am conjecting that the $\mathrm{J}$ information of $U(X+Y)$ is no larger than the $\mathrm{J}$ information of $X$ .","Suppose represents the space of smooth probability density functions with unit mean, whose support is contained in (or ). Define the following functional for I am conjecturing that in which denotes the self-convolution of . I have tried some specific examples (even though not too many) and I did not find any counter-examples (analytically or numerically), so I am wondering whether there exists a proof of this conjecture/inequality, if not, a counter-example is welcome! Edit: The motivation behind this question is as follows. Take a random variable with law unit mean and supported on , we can think of as the information contained in (note that this is not the usual Fisher information, which is often denoted by or ). Here is the law of with and being an i.i.d. copy of , in which is also independent of and . I am conjecting that the information of is no larger than the information of .","\mathcal{M}_1 [0,\infty) \mathbb{R}_+ \mathrm{J}(f):= \int_0^\infty x\frac{(f')^2}{f} \mathrm{d}x f \in \mathcal{M}_1. \mathrm{J}(\rho) \leq \mathrm{J}(f) \quad \text{with} \quad \rho(x): = \int_{z\geq x} \frac{(f*f)(z)}{z} \mathrm{d}z, f*f f X f \in \mathcal{M}_1 [0,\infty) \mathrm{J}(f) X \mathrm{I}(f) \mathrm{I}(X) \rho U(X+Y) U \sim \mathrm{Uniform}([0,1]) Y X U X Y \mathrm{J} U(X+Y) \mathrm{J} X","['real-analysis', 'functional-inequalities']"
12,"On the integral $\int_{e}^{\infty}\frac{t^{1/2}}{\log^{1/2}\left(t\right)}\alpha^{-t/\log\left(t\right)}dt,\,\alpha>1.$",On the integral,"\int_{e}^{\infty}\frac{t^{1/2}}{\log^{1/2}\left(t\right)}\alpha^{-t/\log\left(t\right)}dt,\,\alpha>1.","Let $\alpha>1$. I would like to find a closed form or an upper bound of $$f\left(\alpha\right)=\int_{e}^{\infty}\frac{t^{1/2}}{\log^{1/2}\left(t\right)}\alpha^{-t/\log\left(t\right)}dt.$$ For the closed form I'm very skeptical but I have trouble also for an upper bound. I tried, manipulating a bit, to integrate w.r.t. $\alpha$ since $$\frac{\partial}{\partial\alpha}\alpha^{-t/\log\left(t\right)}=-\frac{t}{\alpha\log\left(t\right)}\alpha^{-t/\log\left(t\right)}$$ but it seems quite useless and at this moment I didn't see a good way to proceed.  Maybe it is interesting to see, using some trivial substitutions, that $$f\left(\alpha\right)=\int_{e}^{\infty}\frac{\left(e^{3/2}\right)^{-W_{-1}\left(-1/v\right)}}{v\left(-W_{-1}\left(-\frac{1}{v}\right)\right){}^{1/2}}\frac{W_{-1}\left(-\frac{1}{v}\right)}{W_{-1}\left(-\frac{1}{v}\right)+1}\alpha^{-v}dv$$ $$=\int_{e}^{\infty}g\left(w\right)\alpha^{-v}dv$$ where $W_{-1}\left(x\right)$ is the Lambert $W$ function. So it seems that $f(\alpha)$ is somehow connected to the Mellin transform of $g(w).$ Thank you.","Let $\alpha>1$. I would like to find a closed form or an upper bound of $$f\left(\alpha\right)=\int_{e}^{\infty}\frac{t^{1/2}}{\log^{1/2}\left(t\right)}\alpha^{-t/\log\left(t\right)}dt.$$ For the closed form I'm very skeptical but I have trouble also for an upper bound. I tried, manipulating a bit, to integrate w.r.t. $\alpha$ since $$\frac{\partial}{\partial\alpha}\alpha^{-t/\log\left(t\right)}=-\frac{t}{\alpha\log\left(t\right)}\alpha^{-t/\log\left(t\right)}$$ but it seems quite useless and at this moment I didn't see a good way to proceed.  Maybe it is interesting to see, using some trivial substitutions, that $$f\left(\alpha\right)=\int_{e}^{\infty}\frac{\left(e^{3/2}\right)^{-W_{-1}\left(-1/v\right)}}{v\left(-W_{-1}\left(-\frac{1}{v}\right)\right){}^{1/2}}\frac{W_{-1}\left(-\frac{1}{v}\right)}{W_{-1}\left(-\frac{1}{v}\right)+1}\alpha^{-v}dv$$ $$=\int_{e}^{\infty}g\left(w\right)\alpha^{-v}dv$$ where $W_{-1}\left(x\right)$ is the Lambert $W$ function. So it seems that $f(\alpha)$ is somehow connected to the Mellin transform of $g(w).$ Thank you.",,"['real-analysis', 'integration', 'closed-form']"
13,"integral $\int_0^{\pi} \left( \frac{\pi}{2} - x \right) \frac{\tan x}{x} \, {\rm d}x$",integral,"\int_0^{\pi} \left( \frac{\pi}{2} - x \right) \frac{\tan x}{x} \, {\rm d}x","Evaluate , if possible in a closed form, the integral: $$\int_0^{\pi} \left( \frac{\pi}{2} - x \right) \frac{\tan x}{x} \, {\rm d}x$$ Basically, I have not done that much. I broke the integral \begin{align*} \int_{0}^{\pi} \left ( \frac{\pi}{2}-x \right ) \frac{\tan x}{x} \, {\rm d}x  &= \int_{0}^{\pi/2} \left ( \frac{\pi}{2} - x \right ) \frac{\tan x}{x} \, {\rm d}x  + \int_{\pi/2}^{\pi} \left ( \frac{\pi}{2} - x \right ) \frac{\tan x}{x} \, {\rm d}x\\   &\!\!\!\!\!\!\overset{u=\pi/2-x}{=\! =\! =\! =\! =\! =\!} \int_{0}^{\pi/2} \frac{u \cot u}{\frac{\pi}{2}-u} \, {\rm d}u + \int_{-\pi/2}^{0} \frac{u \cot u}{\frac{\pi}{2}-u} \, {\rm d}u\\   &= \int_{-\pi/2}^{\pi/2} \frac{u \cot u}{\frac{\pi}{2}-u} \, {\rm d}u\\   &\approx 2.13897 \end{align*} I have no idea how to evaluate this. I was thinking of IBP and then some kind of Fourier , but I cannot get it to work. Any ideas?","Evaluate , if possible in a closed form, the integral: $$\int_0^{\pi} \left( \frac{\pi}{2} - x \right) \frac{\tan x}{x} \, {\rm d}x$$ Basically, I have not done that much. I broke the integral \begin{align*} \int_{0}^{\pi} \left ( \frac{\pi}{2}-x \right ) \frac{\tan x}{x} \, {\rm d}x  &= \int_{0}^{\pi/2} \left ( \frac{\pi}{2} - x \right ) \frac{\tan x}{x} \, {\rm d}x  + \int_{\pi/2}^{\pi} \left ( \frac{\pi}{2} - x \right ) \frac{\tan x}{x} \, {\rm d}x\\   &\!\!\!\!\!\!\overset{u=\pi/2-x}{=\! =\! =\! =\! =\! =\!} \int_{0}^{\pi/2} \frac{u \cot u}{\frac{\pi}{2}-u} \, {\rm d}u + \int_{-\pi/2}^{0} \frac{u \cot u}{\frac{\pi}{2}-u} \, {\rm d}u\\   &= \int_{-\pi/2}^{\pi/2} \frac{u \cot u}{\frac{\pi}{2}-u} \, {\rm d}u\\   &\approx 2.13897 \end{align*} I have no idea how to evaluate this. I was thinking of IBP and then some kind of Fourier , but I cannot get it to work. Any ideas?",,"['real-analysis', 'improper-integrals']"
14,Reducing multi-variable functions to a composition of 1- or 2-variable functions,Reducing multi-variable functions to a composition of 1- or 2-variable functions,,"There are some special functions of 3 or more complex variables that are analytic in some domain (a region in $\mathbb C^n$) with respect to each variable. To give some examples: the incomplete beta function $B(z; a, b)$, the Lerch transcendent $\Phi(z, s, a)$, the Weierstrass elliptic function $\wp(z;g_2,g_3)$, hypergeometric-family functions, etc. Is it possible to express each (or at least some) of these functions as a composition of several analytic functions of 1 or 2 complex variables? Or, if we restrict their domain to reals, it is possible to express them as a composition of several inifinitely differentiable (with respect to each variable) functions of 1 or 2 real variables? The same question applies to functions of 2 variables (e.g. polylogarithms, incomplete elliptic integrals, Hurwitz zeta function, Bessel-family functions, etc.): Is it possible to represent them as a composition of several infinitely differentiable functions of 1 variable and the single fixed function of 2 variables $(x,y)\mapsto x+y$? To give an example when the answer to the last question is positive, consider the complete beta function $B(a,b)$. It can be represented as $$B(a,b)=\exp\big((\ln\Gamma(a)+\ln\Gamma(b))+(-\ln\Gamma(a+b))\big)$$ that is a composition of the 2-variable sum function and several infinitely differentiable 1-variable functions $x\mapsto\exp(x)$, $x\mapsto\ln\Gamma(x)$ and $x\mapsto -x$.","There are some special functions of 3 or more complex variables that are analytic in some domain (a region in $\mathbb C^n$) with respect to each variable. To give some examples: the incomplete beta function $B(z; a, b)$, the Lerch transcendent $\Phi(z, s, a)$, the Weierstrass elliptic function $\wp(z;g_2,g_3)$, hypergeometric-family functions, etc. Is it possible to express each (or at least some) of these functions as a composition of several analytic functions of 1 or 2 complex variables? Or, if we restrict their domain to reals, it is possible to express them as a composition of several inifinitely differentiable (with respect to each variable) functions of 1 or 2 real variables? The same question applies to functions of 2 variables (e.g. polylogarithms, incomplete elliptic integrals, Hurwitz zeta function, Bessel-family functions, etc.): Is it possible to represent them as a composition of several infinitely differentiable functions of 1 variable and the single fixed function of 2 variables $(x,y)\mapsto x+y$? To give an example when the answer to the last question is positive, consider the complete beta function $B(a,b)$. It can be represented as $$B(a,b)=\exp\big((\ln\Gamma(a)+\ln\Gamma(b))+(-\ln\Gamma(a+b))\big)$$ that is a composition of the 2-variable sum function and several infinitely differentiable 1-variable functions $x\mapsto\exp(x)$, $x\mapsto\ln\Gamma(x)$ and $x\mapsto -x$.",,"['calculus', 'real-analysis', 'complex-analysis', 'multivariable-calculus', 'special-functions']"
15,"What is the strongest possible statement of the idea that ""the tangent line is the best linear approximation""?","What is the strongest possible statement of the idea that ""the tangent line is the best linear approximation""?",,"For instance, I've just checked that that if you take the best linear approximation (in the $L^2$ sense) to a sufficiently nice function $f$ on the interval $[-\varepsilon, \varepsilon]$, and then let $\varepsilon \to 0$, you get $f(0) + x f'(0)$. Surely we could make this stronger -- I imagine the analogous statements should hold for, say, the $L^1$ norm as well, or for most reasonable norms.  Can we go farther, though? Question: What is the strongest precise definition we can give the word ""best"" so that we have a statement of the form ""the tangent line is the best linear approximation to a differentiable function""?  (Feel free to replace ""differentiable"" with, say, $C^2$ or something if it makes for a more interesting answer.) (Note: I'm aware of similar-sounding questions here, such as In what sense is the derivative the ""best"" linear approximation? , but the answers there don't answer my question.)","For instance, I've just checked that that if you take the best linear approximation (in the $L^2$ sense) to a sufficiently nice function $f$ on the interval $[-\varepsilon, \varepsilon]$, and then let $\varepsilon \to 0$, you get $f(0) + x f'(0)$. Surely we could make this stronger -- I imagine the analogous statements should hold for, say, the $L^1$ norm as well, or for most reasonable norms.  Can we go farther, though? Question: What is the strongest precise definition we can give the word ""best"" so that we have a statement of the form ""the tangent line is the best linear approximation to a differentiable function""?  (Feel free to replace ""differentiable"" with, say, $C^2$ or something if it makes for a more interesting answer.) (Note: I'm aware of similar-sounding questions here, such as In what sense is the derivative the ""best"" linear approximation? , but the answers there don't answer my question.)",,"['calculus', 'real-analysis', 'functional-analysis']"
16,Does there exist a function such that $\int_{\mathbb{R}_+^{\star} } t^nf(t)dt=0$?,Does there exist a function such that ?,\int_{\mathbb{R}_+^{\star} } t^nf(t)dt=0,"Let $f\in C([a,b],\mathbb{R})$ such that $\displaystyle\int_{a}^{b} t^nf(t)dt=0$ for all integer n. We know that $f\equiv 0$. It's ""easy"" to prove with Weierstrass theorem or with How to prove that $\,\,f\equiv 0,$ without using Weierstrass theorem? This theorem is wrong on $\mathbb{R^+}$, we can choose : $$f(x)=\exp(-x^{\frac{1}{4}})\sin(x^\frac{1}{4})$$ Let  $$ I_n=\displaystyle\int_{0}^{+\infty}t^n\exp(-\omega t)dt=\frac{n!}{\omega^{n+1}},\quad n\in \mathbb{N}, \quad \omega=exp(\frac{i\pi}{4}) $$ Proof. $$ |t^n\exp(-\omega t)|=t^n\exp \bigr(\frac{-t \sqrt{2}}{2}\bigl)\in L^1(\mathbb{R}) $$ by integration by parts we get  $$ I_n=\frac{n}{\omega}I_{n-1} $$ Thus, $$ I_n=\frac{n!}{\omega^{n+1}} $$ Plus for $n\geq 1$, $\quad \omega^{4(n+1)}=-(1)^{n+1}$ Then,  $$ I_{4n+3}\in \mathbb{R} $$ Therefore,  $$ 0=\Im(I_{4n+3})=\displaystyle\int_{0}^{+\infty}x^nf(x)dx $$ Let $f:\mathbb{R}_+ \longrightarrow \mathbb{C}$, I would like to prove the existence of a function $f$ such that $\int_{0}^{+\infty}t^n f(t)dt=0$, In fact this example it's not mine (I have already read it in a book) and the question is to find $f:\mathbb{R}_+ \longrightarrow \mathbb{C}$. So I would like to know if we can proove the existence more generally or just how can I construct a such function. Thank you in advance,","Let $f\in C([a,b],\mathbb{R})$ such that $\displaystyle\int_{a}^{b} t^nf(t)dt=0$ for all integer n. We know that $f\equiv 0$. It's ""easy"" to prove with Weierstrass theorem or with How to prove that $\,\,f\equiv 0,$ without using Weierstrass theorem? This theorem is wrong on $\mathbb{R^+}$, we can choose : $$f(x)=\exp(-x^{\frac{1}{4}})\sin(x^\frac{1}{4})$$ Let  $$ I_n=\displaystyle\int_{0}^{+\infty}t^n\exp(-\omega t)dt=\frac{n!}{\omega^{n+1}},\quad n\in \mathbb{N}, \quad \omega=exp(\frac{i\pi}{4}) $$ Proof. $$ |t^n\exp(-\omega t)|=t^n\exp \bigr(\frac{-t \sqrt{2}}{2}\bigl)\in L^1(\mathbb{R}) $$ by integration by parts we get  $$ I_n=\frac{n}{\omega}I_{n-1} $$ Thus, $$ I_n=\frac{n!}{\omega^{n+1}} $$ Plus for $n\geq 1$, $\quad \omega^{4(n+1)}=-(1)^{n+1}$ Then,  $$ I_{4n+3}\in \mathbb{R} $$ Therefore,  $$ 0=\Im(I_{4n+3})=\displaystyle\int_{0}^{+\infty}x^nf(x)dx $$ Let $f:\mathbb{R}_+ \longrightarrow \mathbb{C}$, I would like to prove the existence of a function $f$ such that $\int_{0}^{+\infty}t^n f(t)dt=0$, In fact this example it's not mine (I have already read it in a book) and the question is to find $f:\mathbb{R}_+ \longrightarrow \mathbb{C}$. So I would like to know if we can proove the existence more generally or just how can I construct a such function. Thank you in advance,",,['real-analysis']
17,Convexity of a complicated function,Convexity of a complicated function,,"Let $\mathbb{S}$ be a $2$ -D convex set in the positive quadrant. Let us define \begin{align} y_L=\min_{(x,y)\in\mathbb{S} }y \\ y_R=\max_{(x,y)\in\mathbb{S} }y  \end{align} For any positive number $p\in[p_L,p_R]$ where $p_L=\frac{1}{y_R}$ and $p_R=\frac{1}{y_L}$ , define the function \begin{align} f(p)=\min_{(x,y)\in\mathbb{S} }px~,~\text{s.t.}~~py\geq 1 \end{align} where s.t. means ""subject to"" . What is the nature of $f(p)$ ? Is it convex or concave?","Let be a -D convex set in the positive quadrant. Let us define For any positive number where and , define the function where s.t. means ""subject to"" . What is the nature of ? Is it convex or concave?","\mathbb{S} 2 \begin{align}
y_L=\min_{(x,y)\in\mathbb{S} }y \\
y_R=\max_{(x,y)\in\mathbb{S} }y 
\end{align} p\in[p_L,p_R] p_L=\frac{1}{y_R} p_R=\frac{1}{y_L} \begin{align}
f(p)=\min_{(x,y)\in\mathbb{S} }px~,~\text{s.t.}~~py\geq 1
\end{align} f(p)","['real-analysis', 'convex-analysis']"
18,Inequality involving partial sums of $\frac{|\sin{kx}|}{k}$,Inequality involving partial sums of,\frac{|\sin{kx}|}{k},"How to prove that $\forall x \in \mathbb{R}$, $n \in \mathbb{N}$, we have   \begin{align} \sum_{k=1}^{n}\frac{|\sin{kx}|}{k}\ge |\sin{nx}| ? \end{align} I know that this partial sum will diverge for $x\not = m\pi$, but I don't know how to prove this inequality. I have tried Abel summation, but it doesn't work because I can't give a lower bound for $\sum |\sin{kx}|$. Thanks for your attention.","How to prove that $\forall x \in \mathbb{R}$, $n \in \mathbb{N}$, we have   \begin{align} \sum_{k=1}^{n}\frac{|\sin{kx}|}{k}\ge |\sin{nx}| ? \end{align} I know that this partial sum will diverge for $x\not = m\pi$, but I don't know how to prove this inequality. I have tried Abel summation, but it doesn't work because I can't give a lower bound for $\sum |\sin{kx}|$. Thanks for your attention.",,"['real-analysis', 'sequences-and-series', 'analysis', 'inequality']"
19,A version of Riemann's theorem,A version of Riemann's theorem,,"Suppose that series $\sum^\infty_{n=1} u_n=s$ converges conditionally. Then for each $s'\gt s$ there exists a permutation of positive integers $\sigma:\mathbb{N}\to\mathbb{N}$ such that if $u_n\geq 0,$ then $\sigma(n)=n;$ $\sum^\infty_{n=1} u_{\sigma(n)}=s'.$ The standard proof of Riemann's theorem fails because of the condition (1). I know one construction which may give needed permutation, but I don't know how to prove the convergence. The construction is as follows. Denote $I_+=\{n\in \mathbb{N}:u_n\geq 0\}, \ I_-=\{n\in \mathbb{N}:u_n\lt 0\}$ - positions of positive and negative terms respectively. Choose infinite set $F\subset I_-,$ such that $\sum\limits_{n\in F} u_n\gt-\infty.$ The permutation $\sigma$ is built inductively. For  all $n\in I_+$ define $\sigma(n)=n.$ Next, on every position $n\in I_-$ permutation $\sigma$ puts elements of $F$ in their order. There exists first number $n_1$ such that $\sum\limits^{n_1}_{n=1} u_{\sigma(n)}\gt s'.$ The existence follows from the divergence $\sum\limits_{n\in I_+} u_n=\infty$ and convergence $\sum\limits_{n\in F} u_n\gt-\infty.$ After $n_1$ on each $n\in I_-$ permutation $\sigma$ puts elements of $I_-$ which were not used before in their order. There exists first number $n_2$ such that $\sum\limits^{n_2}_{n=1} u_{\sigma(n)}\lt s'.$ Existence follows from the fact that from some moment we'll obtain the permutation of series, which changes only finite number of points and converges to $s\lt s'.$ After $n_2$ at each position $n\in I_-$ put elements of $F,$ which were not used before, until first moment $n_3$ for which $\sum\limits^{n_3}_{n=1} u_{\sigma(n)}\gt s'.$ And so on. I know how to prove convergence of partial sums of new series to $s'$ for indices $n\in[n_{2p},n_{2p+1}],$ (where sums less then $s'$). Is it possible that infinitely often partial sums will exceed some $s'+\varepsilon?$","Suppose that series $\sum^\infty_{n=1} u_n=s$ converges conditionally. Then for each $s'\gt s$ there exists a permutation of positive integers $\sigma:\mathbb{N}\to\mathbb{N}$ such that if $u_n\geq 0,$ then $\sigma(n)=n;$ $\sum^\infty_{n=1} u_{\sigma(n)}=s'.$ The standard proof of Riemann's theorem fails because of the condition (1). I know one construction which may give needed permutation, but I don't know how to prove the convergence. The construction is as follows. Denote $I_+=\{n\in \mathbb{N}:u_n\geq 0\}, \ I_-=\{n\in \mathbb{N}:u_n\lt 0\}$ - positions of positive and negative terms respectively. Choose infinite set $F\subset I_-,$ such that $\sum\limits_{n\in F} u_n\gt-\infty.$ The permutation $\sigma$ is built inductively. For  all $n\in I_+$ define $\sigma(n)=n.$ Next, on every position $n\in I_-$ permutation $\sigma$ puts elements of $F$ in their order. There exists first number $n_1$ such that $\sum\limits^{n_1}_{n=1} u_{\sigma(n)}\gt s'.$ The existence follows from the divergence $\sum\limits_{n\in I_+} u_n=\infty$ and convergence $\sum\limits_{n\in F} u_n\gt-\infty.$ After $n_1$ on each $n\in I_-$ permutation $\sigma$ puts elements of $I_-$ which were not used before in their order. There exists first number $n_2$ such that $\sum\limits^{n_2}_{n=1} u_{\sigma(n)}\lt s'.$ Existence follows from the fact that from some moment we'll obtain the permutation of series, which changes only finite number of points and converges to $s\lt s'.$ After $n_2$ at each position $n\in I_-$ put elements of $F,$ which were not used before, until first moment $n_3$ for which $\sum\limits^{n_3}_{n=1} u_{\sigma(n)}\gt s'.$ And so on. I know how to prove convergence of partial sums of new series to $s'$ for indices $n\in[n_{2p},n_{2p+1}],$ (where sums less then $s'$). Is it possible that infinitely often partial sums will exceed some $s'+\varepsilon?$",,"['real-analysis', 'sequences-and-series', 'divergent-series']"
20,Is $f^{-1}\big( \sqrt{xf(x)} \big)$ convex (for large $x$) when $f(x) = o(x)$ is concave and strictly increasing?,Is  convex (for large ) when  is concave and strictly increasing?,f^{-1}\big( \sqrt{xf(x)} \big) x f(x) = o(x),"Question Let $f : (N, \infty) \to \mathbb (0,\infty)$ be a strictly increasing, concave function such that $f(x) \to \infty$ and $f(x) = o(x)$ for $x \to \infty$ , where $N > 0$ . Also, let $f$ be twice differentiable (or more if needed). Define $g(x):= f^{-1}\big(\sqrt{xf(x)} \big)$ where the definition makes sense (i.e. for all $x$ such that $\sqrt{x f(x)} > f(N)$ ). Does it follow that $g$ is convex for large $x$ ? That is, if the restriction $g|_{[K, \infty)}$ is convex for some $K>0$ . Also: I'd like to know if $h(x):= g(x)/x$ is increasing for large $x$ . Thoughts The idea is that the geometric mean $\sqrt{x f(x)}$ sort of pulls $f$ up toward the 45 degree line, straighening it out a bit. This is still concave (which isn't too hard to show, knowing the geometric mean is a concave function), but ""less"" so. Applying the convex $f^{-1}$ to $f(x)$ yields $x$ , sort of cancelling out the concavity. But applying $f^{-1}$ to $[xf(x)]^{1/2}$ should yield a convex function, since we already brought $f(x)$ a bit in this direction with the geometric mean.","Question Let be a strictly increasing, concave function such that and for , where . Also, let be twice differentiable (or more if needed). Define where the definition makes sense (i.e. for all such that ). Does it follow that is convex for large ? That is, if the restriction is convex for some . Also: I'd like to know if is increasing for large . Thoughts The idea is that the geometric mean sort of pulls up toward the 45 degree line, straighening it out a bit. This is still concave (which isn't too hard to show, knowing the geometric mean is a concave function), but ""less"" so. Applying the convex to yields , sort of cancelling out the concavity. But applying to should yield a convex function, since we already brought a bit in this direction with the geometric mean.","f : (N, \infty) \to \mathbb (0,\infty) f(x) \to \infty f(x) = o(x) x \to \infty N > 0 f g(x):= f^{-1}\big(\sqrt{xf(x)} \big) x \sqrt{x f(x)} > f(N) g x g|_{[K, \infty)} K>0 h(x):= g(x)/x x \sqrt{x f(x)} f f^{-1} f(x) x f^{-1} [xf(x)]^{1/2} f(x)","['real-analysis', 'convex-analysis', 'asymptotics']"
21,A version of Ampère's law,A version of Ampère's law,,"The most common proof that I have found of the fact that Ampère's law is entailed by the Biot-Savart law uses the fact that , if $\boldsymbol{J}:\mathbb{R}^3\to\mathbb{R}^3$, $\boldsymbol{J}\in C_c^2(\mathbb{R}^3)$, is a compactly supported twice continuously differentiable field such that $\nabla\cdot\boldsymbol{J}\equiv 0 $ and $\Sigma$ is a smooth surface satisfying the assumptions of Stokes' theorem, then $$\oint_{\partial^+ \Sigma}\left(\frac{\mu_0}{4\pi}\int_{\mathbb{R}^3}\frac{\boldsymbol{J}(\boldsymbol{x})\times(\boldsymbol{r}-\boldsymbol{x})}{\|\boldsymbol{r}-\boldsymbol{x}\|^3}d\mu_{\boldsymbol{x}}\right)\cdot d\boldsymbol{r}=\mu_0\int_\Sigma \boldsymbol{J}\cdot\boldsymbol{N}_e \,d\sigma\quad(1)$$where $\mu_0$ is any constant (the magnetic permeability in the physical interpretation), $\boldsymbol{N}_e$ is the surface's external normal unit vector and $\mu_{\boldsymbol{x}}$ is Lebesgue $3$-dimensional measure. Nevertheless, common exercises and applications of Ampère's law found in books of physics use current densities $\boldsymbol{J}\notin C_c^2(\mathbb{R}^3)$, one example being $\boldsymbol{J}$ constant on an infinite cylinder and constantly $\mathbf{0}$ outside the infinite cylinder. Do mathematically rigourous formulations of Ampère's law $(1)$ exist under more relaxed assumptions on $\boldsymbol{J}$, like the quoted case of $\boldsymbol{J}$ constant on a (bounded or unbounded) region and null outside of it, and, if they do, how can they be proved? I have thought about approximating such a $\boldsymbol{J}$ with $\boldsymbol{J}_n\in C_c^2(\mathbb{R}^3)$, but it is not easy to see that the required sequence really exists. I heartily thank any answerer!","The most common proof that I have found of the fact that Ampère's law is entailed by the Biot-Savart law uses the fact that , if $\boldsymbol{J}:\mathbb{R}^3\to\mathbb{R}^3$, $\boldsymbol{J}\in C_c^2(\mathbb{R}^3)$, is a compactly supported twice continuously differentiable field such that $\nabla\cdot\boldsymbol{J}\equiv 0 $ and $\Sigma$ is a smooth surface satisfying the assumptions of Stokes' theorem, then $$\oint_{\partial^+ \Sigma}\left(\frac{\mu_0}{4\pi}\int_{\mathbb{R}^3}\frac{\boldsymbol{J}(\boldsymbol{x})\times(\boldsymbol{r}-\boldsymbol{x})}{\|\boldsymbol{r}-\boldsymbol{x}\|^3}d\mu_{\boldsymbol{x}}\right)\cdot d\boldsymbol{r}=\mu_0\int_\Sigma \boldsymbol{J}\cdot\boldsymbol{N}_e \,d\sigma\quad(1)$$where $\mu_0$ is any constant (the magnetic permeability in the physical interpretation), $\boldsymbol{N}_e$ is the surface's external normal unit vector and $\mu_{\boldsymbol{x}}$ is Lebesgue $3$-dimensional measure. Nevertheless, common exercises and applications of Ampère's law found in books of physics use current densities $\boldsymbol{J}\notin C_c^2(\mathbb{R}^3)$, one example being $\boldsymbol{J}$ constant on an infinite cylinder and constantly $\mathbf{0}$ outside the infinite cylinder. Do mathematically rigourous formulations of Ampère's law $(1)$ exist under more relaxed assumptions on $\boldsymbol{J}$, like the quoted case of $\boldsymbol{J}$ constant on a (bounded or unbounded) region and null outside of it, and, if they do, how can they be proved? I have thought about approximating such a $\boldsymbol{J}$ with $\boldsymbol{J}_n\in C_c^2(\mathbb{R}^3)$, but it is not easy to see that the required sequence really exists. I heartily thank any answerer!",,"['real-analysis', 'integration', 'physics', 'vector-analysis', 'mathematical-physics']"
22,Could it possibly have a nice closed form? $\int _0^1\int _0^1\frac{x y}{(x+1) (y+1) \log (x y)}\ dx \ dy$,Could it possibly have a nice closed form?,\int _0^1\int _0^1\frac{x y}{(x+1) (y+1) \log (x y)}\ dx \ dy,"Using multiple integrals it's not hard to show that the present integral reduces to some integral over squared digamma functions, but then things become harder. How would you tackle the problem? $$\int _0^1\int _0^1\frac{x y}{(x+1) (y+1) \log (x y)}\ dx \ dy$$","Using multiple integrals it's not hard to show that the present integral reduces to some integral over squared digamma functions, but then things become harder. How would you tackle the problem? $$\int _0^1\int _0^1\frac{x y}{(x+1) (y+1) \log (x y)}\ dx \ dy$$",,"['calculus', 'real-analysis', 'integration', 'definite-integrals']"
23,Number of flips to get to a Set of Positive Lebesgue Measure,Number of flips to get to a Set of Positive Lebesgue Measure,,"A consequence of Exercise 1.1.19 on page 13 of Stroock's ""Probability Theory: An Analytic View"" is that if a set $E\subset[0,1)$ has positive (Lebesgue) measure, then for almost every $x\in[0,1)$, a finite number of flips of its binary expansion will move $x$ to $E$. The exercise in Stroock suggests using the Kolmogorov 0-1 Law but it can also also be proved measure theoretically using an open cover approximation of $E$. I read and did this a long time ago but when thinking of something else recently, I wondered Given a set of positive measure, what is the expected number of flips required? (No clue.) Is the expected value finite? (I think so but I don't have a proof.) Even for a set that is a finite union of dyadic intervals, it is not clear to me what the expected value is. For example, what is the maximum of the expected value over the collection of $E\subset[0,1)$ that is a finite union of dyadic intervals with measure 0.5? I would appreciate it if someone can give me a reference if this is well known or some hint on how to do this if it is reasonably doable for someone who knows some measure theory (say at the level of Folland, Royden, or Rudin.)","A consequence of Exercise 1.1.19 on page 13 of Stroock's ""Probability Theory: An Analytic View"" is that if a set $E\subset[0,1)$ has positive (Lebesgue) measure, then for almost every $x\in[0,1)$, a finite number of flips of its binary expansion will move $x$ to $E$. The exercise in Stroock suggests using the Kolmogorov 0-1 Law but it can also also be proved measure theoretically using an open cover approximation of $E$. I read and did this a long time ago but when thinking of something else recently, I wondered Given a set of positive measure, what is the expected number of flips required? (No clue.) Is the expected value finite? (I think so but I don't have a proof.) Even for a set that is a finite union of dyadic intervals, it is not clear to me what the expected value is. For example, what is the maximum of the expected value over the collection of $E\subset[0,1)$ that is a finite union of dyadic intervals with measure 0.5? I would appreciate it if someone can give me a reference if this is well known or some hint on how to do this if it is reasonably doable for someone who knows some measure theory (say at the level of Folland, Royden, or Rudin.)",,"['real-analysis', 'probability', 'measure-theory', 'lebesgue-measure']"
24,How does one take limit along a path?,How does one take limit along a path?,,"So in multivariable calculus for a limit of a function to exist, the limits of the function along all possible paths must exist and equal the same value. But how does one calculate the limit along a given path? Say I have $f(x,y) = (x^3(y+1), x-y^2)$ (this is off the top of my head, so sorry if it doesn't work out well) and I want to know what the value of $\lim_{(x,y) \to (2,3)}$ is along the path $y= x^2 -1$ .  How would I calculate the limit? My guess is we'd need to parametrize the curve?  In that case, I guess we'd need some $\phi(t): [a,b] \subset \Bbb R \to \Bbb R^2$ .  So we'll let $t=x$ , then $\phi(t) = (t, t^2-1)$ .  Then would $\lim_{(x,y) \to (2,3)} f(x,y) = \lim_{t \to 2} f(\phi(t))$ .  I don't know for sure, bu it seems like we'd need something like the chain rule here -- that is can we just plug stuff in like this? Moreover, is $\frac {\partial f}{\partial x}$ the limit of the gradient in the x-direction?  If so, is there some notation that tells us what the limit of the gradient at a point is along some other given path (like $y=x^2 -1$ for instance)? I could be way off here.  I just realized that even though I know how to calculate partials and gradients and multiple integrals, I don't really understand the basics.","So in multivariable calculus for a limit of a function to exist, the limits of the function along all possible paths must exist and equal the same value. But how does one calculate the limit along a given path? Say I have (this is off the top of my head, so sorry if it doesn't work out well) and I want to know what the value of is along the path .  How would I calculate the limit? My guess is we'd need to parametrize the curve?  In that case, I guess we'd need some .  So we'll let , then .  Then would .  I don't know for sure, bu it seems like we'd need something like the chain rule here -- that is can we just plug stuff in like this? Moreover, is the limit of the gradient in the x-direction?  If so, is there some notation that tells us what the limit of the gradient at a point is along some other given path (like for instance)? I could be way off here.  I just realized that even though I know how to calculate partials and gradients and multiple integrals, I don't really understand the basics.","f(x,y) = (x^3(y+1), x-y^2) \lim_{(x,y) \to (2,3)} y= x^2 -1 \phi(t): [a,b] \subset \Bbb R \to \Bbb R^2 t=x \phi(t) = (t, t^2-1) \lim_{(x,y) \to (2,3)} f(x,y) = \lim_{t \to 2} f(\phi(t)) \frac {\partial f}{\partial x} y=x^2 -1","['real-analysis', 'limits', 'multivariable-calculus']"
25,"Find all pairs of functions $(f,g)$, $\forall x, y \in \mathbb{R}, f(x+g(y))=x f(y) - y f(x) + g(x)$","Find all pairs of functions ,","(f,g) \forall x, y \in \mathbb{R}, f(x+g(y))=x f(y) - y f(x) + g(x)","Find all pairs of functions $(f,g)$ : $\mathbb{R} \to \mathbb{R}, g : \mathbb{R} \to \mathbb{R}$ satisfying :    $$\forall x, y \in \mathbb{R}, f(x+g(y))=x f(y) - y f(x) + g(x)$$ I am really stuck with this problem, Any ideas will be grateful, Thanks.","Find all pairs of functions $(f,g)$ : $\mathbb{R} \to \mathbb{R}, g : \mathbb{R} \to \mathbb{R}$ satisfying :    $$\forall x, y \in \mathbb{R}, f(x+g(y))=x f(y) - y f(x) + g(x)$$ I am really stuck with this problem, Any ideas will be grateful, Thanks.",,"['real-analysis', 'functional-equations']"
26,How to prove $\sin x+\frac{\sin 2x}{2}+\frac{\sin 3x}{3}+\cdots$ is positive? [duplicate],How to prove  is positive? [duplicate],\sin x+\frac{\sin 2x}{2}+\frac{\sin 3x}{3}+\cdots,This question already has answers here : Inequality $\sum\limits_{1\le k\le n}\frac{\sin kx}{k}\ge 0$ (Fejer-Jackson) (3 answers) Closed 4 years ago . Let $0<x<\pi$ . $n$ be a natural number. How to prove $$\sin x+\frac{\sin 2x}{2}+\frac{\sin 3x}{3}+\cdots+ \frac{\sin nx}{n}>0$$,This question already has answers here : Inequality $\sum\limits_{1\le k\le n}\frac{\sin kx}{k}\ge 0$ (Fejer-Jackson) (3 answers) Closed 4 years ago . Let . be a natural number. How to prove,0<x<\pi n \sin x+\frac{\sin 2x}{2}+\frac{\sin 3x}{3}+\cdots+ \frac{\sin nx}{n}>0,"['real-analysis', 'inequality', 'induction']"
27,"Real analysis textbok that develops the subject in a self-motivated, coherent fashion?","Real analysis textbok that develops the subject in a self-motivated, coherent fashion?",,"Well, it seems as though I just failed my analysis prelim for the second time... I have one more try in about $5$ months. I'm failing to build up a framework for how to think about analysis problems. When I am confronted with an exam-type problem in, for example, algebraic topology, I can usually immediately see the ideas and theorems that may be relevant, why they are connected, and how to go between them. (In part this is due to the phenomenal professor who taught the subject!) Too bad my department doesn't offer a topology exam as an alternative... Real analysis, on the other hand, appears to me as a jumbled toolbox with no clear common purpose or function, and my studying is reduced to pure memorization, no matter how many exercises and practice exams I solve. The textbook my department uses is Hunter and Nachtergaele's Applied Analysis , and the exam covers Banach spaces, Hilbert spaces, linear operators on these spaces, spectral theory (of operators), Fourier analysis, distributions, Sobolev embedding theorems... and of course the various connections between these things, which is what I'm really struggling to figure out how to learn. So, what, if any, textbooks, would you recommend to a struggling graduate student? I am looking for a book that is more than just a list of theorems and definitions, which is essentially what our book is, if such a book exists.","Well, it seems as though I just failed my analysis prelim for the second time... I have one more try in about $5$ months. I'm failing to build up a framework for how to think about analysis problems. When I am confronted with an exam-type problem in, for example, algebraic topology, I can usually immediately see the ideas and theorems that may be relevant, why they are connected, and how to go between them. (In part this is due to the phenomenal professor who taught the subject!) Too bad my department doesn't offer a topology exam as an alternative... Real analysis, on the other hand, appears to me as a jumbled toolbox with no clear common purpose or function, and my studying is reduced to pure memorization, no matter how many exercises and practice exams I solve. The textbook my department uses is Hunter and Nachtergaele's Applied Analysis , and the exam covers Banach spaces, Hilbert spaces, linear operators on these spaces, spectral theory (of operators), Fourier analysis, distributions, Sobolev embedding theorems... and of course the various connections between these things, which is what I'm really struggling to figure out how to learn. So, what, if any, textbooks, would you recommend to a struggling graduate student? I am looking for a book that is more than just a list of theorems and definitions, which is essentially what our book is, if such a book exists.",,"['real-analysis', 'analysis', 'functional-analysis', 'reference-request', 'soft-question']"
28,Prove that a compact metric space can be covered by open balls that don't overlap too much.,Prove that a compact metric space can be covered by open balls that don't overlap too much.,,"The problem is: For compact metric space $(X,d)$ prove that for every $r>0$ there exists a subset $S$ of $X$ such that $\{\mbox{Open balls of radius }r\mbox{ centered at }p \mid\mbox{ for all }p \in S\}$ forms a cover for $X$ and for every $p,q \in S$, $d(p,q) > r/2$. I have an algorithm that would go like this: Form an open cover of $X$ by the set of open balls of radius $r/2$ around all points in $X$.  By compactness there exists a finite number of these balls which cover $X$. Then for each point with a ball around it, ""delete"" the points which are inside of the ball and not the center of the ball.  Then you will have a collection of points that are at least $r/2$ distance apart and the set of balls of radius $r$ around these points will cover $X$. Does this ""algorithm"" work, and if so how do you denote such a set?  I'm having problems figuring out exactly how to ""delete"" as I've used the word above. Thanks!","The problem is: For compact metric space $(X,d)$ prove that for every $r>0$ there exists a subset $S$ of $X$ such that $\{\mbox{Open balls of radius }r\mbox{ centered at }p \mid\mbox{ for all }p \in S\}$ forms a cover for $X$ and for every $p,q \in S$, $d(p,q) > r/2$. I have an algorithm that would go like this: Form an open cover of $X$ by the set of open balls of radius $r/2$ around all points in $X$.  By compactness there exists a finite number of these balls which cover $X$. Then for each point with a ball around it, ""delete"" the points which are inside of the ball and not the center of the ball.  Then you will have a collection of points that are at least $r/2$ distance apart and the set of balls of radius $r$ around these points will cover $X$. Does this ""algorithm"" work, and if so how do you denote such a set?  I'm having problems figuring out exactly how to ""delete"" as I've used the word above. Thanks!",,"['real-analysis', 'metric-spaces', 'compactness']"
29,Solving a formal power series equation,Solving a formal power series equation,,"I want to find a function $f(x,y)$ which can satisfy the following equation, $$\prod _{n=1} ^{\infty} \frac{1+x^n}{(1-x^{n/2}y^{n/2})(1-x^{n/2}y^{-n/2})} = \exp \left[ \sum _{n=1} ^\infty \frac{f(x^n,y^n)}{n(1-x^{2n})}\right]$$ I would like to know how this is solved. In a certain paper where I ran into this, it is claimed that the function is, $$f(x,y) = \sqrt{x}(y + 1/y) + x(1+y^2 + 1/y) + x^{3/2}(y^3+1/y^3) + x^2(y^4+1/y^4) + \sum _{n=5}^\infty x^{n/2}(y^n + 1/y^n - y^{n-4} - 1/y^{n-4})$$ The paper doesn't state any proof or explanation for how this was obtained but perturbatively the above can be checked to be correct! Now I tried to do something obvious but it didn't work! \begin{eqnarray}  \prod _ {n=1} ^{\infty} \frac{ (1+x^n) }{1+x^n -x^{\frac{n}{2}} \left(y^{\frac{n}{2}}  + y^{-\frac{n}{2}}\right) } = \exp \left[ \sum _ {n=1} ^{\infty}   \frac{ I_{ST}(x^n,y^n) } {n (1-x^{2n}) } \right] \\  \Rightarrow \sum_{n=1}^{\infty} \left\{ \ln (1+x^n) - \ln(1-(\sqrt{xy})^n) - \ln\left(1- \left(\sqrt{\frac{x}{y}}\right)^n\right) \right\}  = \sum_{n=1}^\infty  \frac{I_{ST}(x^n,y^n)} {n(1-x^{2n})}  \end{eqnarray} Now we expand the logarithms and we have, \begin{eqnarray}  \sum _ {n=1} ^ {\infty} \left \{  \sum _{a=1}^{\infty} (-1)^{a+1} \frac{x^{na}}{a} + \sum_{b=1} ^{\infty} \frac{ (\sqrt{xy})^{nb} } {b} + \sum _{c=1}^{\infty} \frac{ \left(\sqrt{\frac{x}{y}}\right)^{nc} }{c}   \right \} =  \sum _{n=1} ^\infty  \frac{I_{ST}(x^n,y^n)} {n(1-x^{2n})} \\ \Rightarrow \sum _{a=1} ^{\infty} \frac{1}{a} \left\{  \sum _{n=1} ^{\infty} \left( (-1)^{a+1}x^{na} + (xy)^{\frac{na}{2}} + \left(\frac{x}{y}\right)^{\frac{na}{2}} \right)  \right\} =  \sum _{n=1} ^\infty  \frac{I_{ST}(x^n,y^n)} {n(1-x^{2n})} \end{eqnarray} By matching the patterns on both sides one sees that one way this equality can hold is if,  $$ \begin{eqnarray}  I_{ST}(x,y) = (1-x^2) \sum _{n=1} ^{\infty} \left\{ x^n + (xy)^{\frac{n}{2}} + (\frac{x}{y})^{\frac{n}{2}}  \right\} \\ \Rightarrow I_{ST} (x,y) = (1-x^2) \left(-1 + \frac{1}{1-x} -1 + \frac{1}{1-\sqrt{xy}} - 1 + \frac{1}{1-\sqrt{\frac{x}{y}} } \right) \end {eqnarray}  $$ But this solution doesn't satisfy the original equation!","I want to find a function $f(x,y)$ which can satisfy the following equation, $$\prod _{n=1} ^{\infty} \frac{1+x^n}{(1-x^{n/2}y^{n/2})(1-x^{n/2}y^{-n/2})} = \exp \left[ \sum _{n=1} ^\infty \frac{f(x^n,y^n)}{n(1-x^{2n})}\right]$$ I would like to know how this is solved. In a certain paper where I ran into this, it is claimed that the function is, $$f(x,y) = \sqrt{x}(y + 1/y) + x(1+y^2 + 1/y) + x^{3/2}(y^3+1/y^3) + x^2(y^4+1/y^4) + \sum _{n=5}^\infty x^{n/2}(y^n + 1/y^n - y^{n-4} - 1/y^{n-4})$$ The paper doesn't state any proof or explanation for how this was obtained but perturbatively the above can be checked to be correct! Now I tried to do something obvious but it didn't work! \begin{eqnarray}  \prod _ {n=1} ^{\infty} \frac{ (1+x^n) }{1+x^n -x^{\frac{n}{2}} \left(y^{\frac{n}{2}}  + y^{-\frac{n}{2}}\right) } = \exp \left[ \sum _ {n=1} ^{\infty}   \frac{ I_{ST}(x^n,y^n) } {n (1-x^{2n}) } \right] \\  \Rightarrow \sum_{n=1}^{\infty} \left\{ \ln (1+x^n) - \ln(1-(\sqrt{xy})^n) - \ln\left(1- \left(\sqrt{\frac{x}{y}}\right)^n\right) \right\}  = \sum_{n=1}^\infty  \frac{I_{ST}(x^n,y^n)} {n(1-x^{2n})}  \end{eqnarray} Now we expand the logarithms and we have, \begin{eqnarray}  \sum _ {n=1} ^ {\infty} \left \{  \sum _{a=1}^{\infty} (-1)^{a+1} \frac{x^{na}}{a} + \sum_{b=1} ^{\infty} \frac{ (\sqrt{xy})^{nb} } {b} + \sum _{c=1}^{\infty} \frac{ \left(\sqrt{\frac{x}{y}}\right)^{nc} }{c}   \right \} =  \sum _{n=1} ^\infty  \frac{I_{ST}(x^n,y^n)} {n(1-x^{2n})} \\ \Rightarrow \sum _{a=1} ^{\infty} \frac{1}{a} \left\{  \sum _{n=1} ^{\infty} \left( (-1)^{a+1}x^{na} + (xy)^{\frac{na}{2}} + \left(\frac{x}{y}\right)^{\frac{na}{2}} \right)  \right\} =  \sum _{n=1} ^\infty  \frac{I_{ST}(x^n,y^n)} {n(1-x^{2n})} \end{eqnarray} By matching the patterns on both sides one sees that one way this equality can hold is if,  $$ \begin{eqnarray}  I_{ST}(x,y) = (1-x^2) \sum _{n=1} ^{\infty} \left\{ x^n + (xy)^{\frac{n}{2}} + (\frac{x}{y})^{\frac{n}{2}}  \right\} \\ \Rightarrow I_{ST} (x,y) = (1-x^2) \left(-1 + \frac{1}{1-x} -1 + \frac{1}{1-\sqrt{xy}} - 1 + \frac{1}{1-\sqrt{\frac{x}{y}} } \right) \end {eqnarray}  $$ But this solution doesn't satisfy the original equation!",,"['real-analysis', 'algebra-precalculus', 'sequences-and-series', 'power-series']"
30,Hardy's inequality proof using Doob's inquality,Hardy's inequality proof using Doob's inquality,,"Consider a probability space $([0,1],\mathcal{B}([0,1],\lambda),p>1$ and $f \in L^p(]0,\infty[).$ We want to prove Hardy's inequality using martingale theory and Doob's maximal inequalities. Let $\mathcal{F}_n$ be the $\sigma$ -algebra generated by $]k2^{-n},(k+1)2^{-n}],k=0,...,2^n-1.$ I managed to find, for an integrable function $h,$ $E[h|\mathcal{F}_n](x): E[h|\mathcal{F}_n]=2^n\sum_{k=0}^{2^n-1}\int_{k2^{-n}}^{(k+1)2^{-n}}h(y)dy1_{]k2^{-n},(k+1)2^{-n}]}(x),x \in [0,1].$ It is sufficient to prove $$\left(\int_0^1\left|\frac{1}{x}\int_0^xf(y)dy\right|^pdx\right)^{1/p} \leq \frac{p}{p-1} \left(\int_0^1\left|f(x)\right|^p dx\right)^{1/p},$$ since the general result follows from this and the monotone convergence theorem (applied to $f_n(x)=f(nx),x \geq 0$ ). How to relate $E[h|\mathcal{F}_n](x)$ (for a convenient $h$ ) to $\frac{1}{x}\int_0^xf(y)dy$ ?","Consider a probability space and We want to prove Hardy's inequality using martingale theory and Doob's maximal inequalities. Let be the -algebra generated by I managed to find, for an integrable function It is sufficient to prove since the general result follows from this and the monotone convergence theorem (applied to ). How to relate (for a convenient ) to ?","([0,1],\mathcal{B}([0,1],\lambda),p>1 f \in L^p(]0,\infty[). \mathcal{F}_n \sigma ]k2^{-n},(k+1)2^{-n}],k=0,...,2^n-1. h, E[h|\mathcal{F}_n](x): E[h|\mathcal{F}_n]=2^n\sum_{k=0}^{2^n-1}\int_{k2^{-n}}^{(k+1)2^{-n}}h(y)dy1_{]k2^{-n},(k+1)2^{-n}]}(x),x \in [0,1]. \left(\int_0^1\left|\frac{1}{x}\int_0^xf(y)dy\right|^pdx\right)^{1/p} \leq \frac{p}{p-1} \left(\int_0^1\left|f(x)\right|^p dx\right)^{1/p}, f_n(x)=f(nx),x \geq 0 E[h|\mathcal{F}_n](x) h \frac{1}{x}\int_0^xf(y)dy","['real-analysis', 'probability-theory', 'measure-theory', 'stochastic-processes', 'martingales']"
31,Understanding Legendre transform (and convex conjugate),Understanding Legendre transform (and convex conjugate),,"I am learning convex analysis on my own and I would appreciate some help. I know that convex conjugate is the generalization of the Legendre transform. I also know the formula known for Legendre transform (as stated on Wikipedia ). However, I am reading a paper on which I can not understand how Legendre transform is calculated. There is a section called Fact II.2 that claims the dual of the quantum relative entropy (it's Legendre transform) holds. Could you explain how these two expressions in Fact II.2 are equivalent? How can I calculate similar Legendre transforms? Edit: More specifically, I don't understand why the following statements are dual in the mentioned paper: $$  D(\rho\,\|\,\sigma) = \sup_{w \in P_{\geq}(A)}\Big\{\operatorname{tr} \rho \log w - \log\operatorname{tr}\exp(\log w + \log \sigma)\Big\} $$ and $$ \log\operatorname{tr}\exp(H + \log \sigma) = \sup_{w \in S(A)}\Big\{\operatorname{tr} H w - D(w\,\|\,\sigma)\Big\} $$ [55]: D. Sutter, M. Berta, and M. Tomamichel. Multivariate trace inequalities. Communications in Mathematical Physics, 352(1):37–58, 2017. DOI: 10.1007/s00220-016-2778-5.","I am learning convex analysis on my own and I would appreciate some help. I know that convex conjugate is the generalization of the Legendre transform. I also know the formula known for Legendre transform (as stated on Wikipedia ). However, I am reading a paper on which I can not understand how Legendre transform is calculated. There is a section called Fact II.2 that claims the dual of the quantum relative entropy (it's Legendre transform) holds. Could you explain how these two expressions in Fact II.2 are equivalent? How can I calculate similar Legendre transforms? Edit: More specifically, I don't understand why the following statements are dual in the mentioned paper: and [55]: D. Sutter, M. Berta, and M. Tomamichel. Multivariate trace inequalities. Communications in Mathematical Physics, 352(1):37–58, 2017. DOI: 10.1007/s00220-016-2778-5."," 
D(\rho\,\|\,\sigma) = \sup_{w \in P_{\geq}(A)}\Big\{\operatorname{tr} \rho \log w - \log\operatorname{tr}\exp(\log w + \log \sigma)\Big\}
 
\log\operatorname{tr}\exp(H + \log \sigma) = \sup_{w \in S(A)}\Big\{\operatorname{tr} H w - D(w\,\|\,\sigma)\Big\}
","['real-analysis', 'convex-analysis', 'legendre-transformation']"
32,"Rearranging series and ""placid"" permutations","Rearranging series and ""placid"" permutations",,"This question came out of a conversation with my students about Riemann's rearrangement theorem , and the general problem of which permutations are ""safe"" w/r/t summing infinite series. Let $S_\infty$ be the group of permutations of $\mathbb{N}$ . For a sequence $\mathscr{A}=(a_i)_{i\in\mathbb{N}}$ , say that a permutation $p\in S_\infty$ is $\mathscr{A}$ -placid iff for every $q\in S_\infty$ and every pair of integers $z_0, z_1$ we have $$\sum_{i\in\mathbb{N}}a_{q(i)}\simeq \sum_{i\in\mathbb{N}}a_{p^{z_0}\circ q\circ p^{z_1}(i)}$$ where "" $s\simeq t$ "" means ""either $s$ and $t$ are each undefined, or they are defined and equal."" Basically, $p$ is $\mathscr{A}$ -placid if $p$ is never interesting from the point of view of rearranging the terms in $\mathscr{A}$ . For example, the permutation swapping $2i$ and $2i+1$ for each $i$ is $\mathscr{A}$ -placid for every $\mathscr{A}$ . I'm curious whether placidity actually depends on the sequence in question (restricting attention to sequences whose corresponding series converge conditionally, to avoid triviality) . For example, are the following equivalent? $p$ is placid with respect to the alternating harmonic sequence $((-1)^{i+1}{1\over i})_{i\in\mathbb{N}}$ . $p$ is $\mathscr{A}$ -placid for every conditionally convergent sequence $\mathscr{A}$ . I suspect the answer is negative, but I don't immediately see how to prove it. EDIT: as far as I can tell, the only ""obviously placid"" permutations are those in which there is a finite bound on the distance an element of $\mathbb{N}$ is moved. Merely having finite orbits isn't enough: for example, for any conditionally convergent sequence $\mathscr{S}$ there is a permutation of $\mathbb{N}$ of order $2$ which applied to $\mathscr{S}$ results in a series with limit $+\infty$ . This was pointed out to me by a colleague after I brashly claimed otherwise! EDIT THE SECOND: Now asked at MO .","This question came out of a conversation with my students about Riemann's rearrangement theorem , and the general problem of which permutations are ""safe"" w/r/t summing infinite series. Let be the group of permutations of . For a sequence , say that a permutation is -placid iff for every and every pair of integers we have where "" "" means ""either and are each undefined, or they are defined and equal."" Basically, is -placid if is never interesting from the point of view of rearranging the terms in . For example, the permutation swapping and for each is -placid for every . I'm curious whether placidity actually depends on the sequence in question (restricting attention to sequences whose corresponding series converge conditionally, to avoid triviality) . For example, are the following equivalent? is placid with respect to the alternating harmonic sequence . is -placid for every conditionally convergent sequence . I suspect the answer is negative, but I don't immediately see how to prove it. EDIT: as far as I can tell, the only ""obviously placid"" permutations are those in which there is a finite bound on the distance an element of is moved. Merely having finite orbits isn't enough: for example, for any conditionally convergent sequence there is a permutation of of order which applied to results in a series with limit . This was pointed out to me by a colleague after I brashly claimed otherwise! EDIT THE SECOND: Now asked at MO .","S_\infty \mathbb{N} \mathscr{A}=(a_i)_{i\in\mathbb{N}} p\in S_\infty \mathscr{A} q\in S_\infty z_0, z_1 \sum_{i\in\mathbb{N}}a_{q(i)}\simeq \sum_{i\in\mathbb{N}}a_{p^{z_0}\circ q\circ p^{z_1}(i)} s\simeq t s t p \mathscr{A} p \mathscr{A} 2i 2i+1 i \mathscr{A} \mathscr{A} p ((-1)^{i+1}{1\over i})_{i\in\mathbb{N}} p \mathscr{A} \mathscr{A} \mathbb{N} \mathscr{S} \mathbb{N} 2 \mathscr{S} +\infty","['real-analysis', 'sequences-and-series', 'analysis', 'convergence-divergence', 'conditional-convergence']"
33,How close can polynomials be in magnitude?,How close can polynomials be in magnitude?,,"If $p$ and $q$ are polynomials and $|p(t)|=|q(t)|$ on the interval $[0,1]$ , then in fact $p(t)=\omega q(t)$ for some constant phase $\omega$ . I am curious about quantitative strengthenings of this fact.  For example, suppose that $p$ and $q$ are polynomials (with possibly complex coefficients) of degree $d$ and $$ \max_{t\in[0,1]} ||p(t)|-|q(t)|| \leq \delta. $$ When it is possible to conclude that there exist a phase $\omega\in\mathbb{C}$ , $|\omega|=1$ , such that $$ \max_{t\in[0,1]} |p(t) - \omega q(t)|  $$ is small? For example, if $q=1$ and $p_d$ is the Taylor approximation to $e^{it/2}$ given by $$ p_d(t) = \sum_{j=0}^d \frac{1}{j!}(it/2)^j, $$ then $$ \max_{t\in[0,1]} |p_d(t) - e^{it/2}| \leq C \frac{2^{-d}}{d!}, $$ which means that $||p_d(t)|-1|$ is very small on the interval $[0,1]$ , but $p_d$ is also not close to any constant phase. I suspect for example that this is close to the best possible, meaning that for example if $p$ is a polynomial of degree $d$ and $$ \max_{t\in[0,1]} ||p(t)|-1| \leq \delta (Cd)^{-d} $$ for some large $C$ , then it follows that $|p(t)-\omega|$ can be bounded by $\delta$ somehow.  I am curious what bounds are available to this effect.","If and are polynomials and on the interval , then in fact for some constant phase . I am curious about quantitative strengthenings of this fact.  For example, suppose that and are polynomials (with possibly complex coefficients) of degree and When it is possible to conclude that there exist a phase , , such that is small? For example, if and is the Taylor approximation to given by then which means that is very small on the interval , but is also not close to any constant phase. I suspect for example that this is close to the best possible, meaning that for example if is a polynomial of degree and for some large , then it follows that can be bounded by somehow.  I am curious what bounds are available to this effect.","p q |p(t)|=|q(t)| [0,1] p(t)=\omega q(t) \omega p q d 
\max_{t\in[0,1]}
||p(t)|-|q(t)|| \leq \delta.
 \omega\in\mathbb{C} |\omega|=1 
\max_{t\in[0,1]}
|p(t) - \omega q(t)| 
 q=1 p_d e^{it/2} 
p_d(t) = \sum_{j=0}^d \frac{1}{j!}(it/2)^j,
 
\max_{t\in[0,1]} |p_d(t) - e^{it/2}|
\leq C \frac{2^{-d}}{d!},
 ||p_d(t)|-1| [0,1] p_d p d 
\max_{t\in[0,1]} ||p(t)|-1| \leq \delta (Cd)^{-d}
 C |p(t)-\omega| \delta","['real-analysis', 'polynomials']"
34,What does $\frac{1}{1+\frac{2}{2+\frac{3}{{\vdots}}}}$ evaluate to? [duplicate],What does  evaluate to? [duplicate],\frac{1}{1+\frac{2}{2+\frac{3}{{\vdots}}}},"This question already has answers here : Continued Fraction: Please prove $\frac{1}{e \gamma (x+1,1)}=x+\frac{1}{x+1+\frac{2}{x+2+\frac{3}{x+3+\frac{4}{\dots}}}}$ (1 answer) Continued fraction for $\frac{1}{e-2}$ (2 answers) Closed 3 years ago . I was curious what does $$\cfrac{1}{1+\cfrac{2}{2+\cfrac{3}{3+\cfrac{4}{\vdots}}}}$$ evaluate to. Empirically, I observed that it equals approximately $0.5819767$ , and a calculator found that this value agrees with $\frac{1}{e-1}$ to at least 8 places. Is $\frac{1}{e-1}$ the exact value of this continued fraction? If this is true, is this result new? And how could the equivalence be proved?","This question already has answers here : Continued Fraction: Please prove $\frac{1}{e \gamma (x+1,1)}=x+\frac{1}{x+1+\frac{2}{x+2+\frac{3}{x+3+\frac{4}{\dots}}}}$ (1 answer) Continued fraction for $\frac{1}{e-2}$ (2 answers) Closed 3 years ago . I was curious what does evaluate to. Empirically, I observed that it equals approximately , and a calculator found that this value agrees with to at least 8 places. Is the exact value of this continued fraction? If this is true, is this result new? And how could the equivalence be proved?",\cfrac{1}{1+\cfrac{2}{2+\cfrac{3}{3+\cfrac{4}{\vdots}}}} 0.5819767 \frac{1}{e-1} \frac{1}{e-1},"['real-analysis', 'continued-fractions', 'eulers-number-e']"
35,Existence of function satisfying $f(f'(x))=x$ almost everywhere,Existence of function satisfying  almost everywhere,f(f'(x))=x,"My project is to Study the existence of a continuous function $f : \mathbb{R} \rightarrow \mathbb{R}$ differentiable almost everywhere satisfying $  f\circ f'(x)=x$ almost everywhere $x \in \mathbb{R}$ I began the study by supposing $f\in   C ^ 1(\mathbb{R}) $ , I have shown that f does not exist. After, I found some difficulties when we assume only f differentiable on $\mathbb{R}$ , I had an answer using Darboux's theorem Questions about the existence of a function . Now, I want to attack the initial problem. Previous arguments do not work! Do you have any suggestions for me?","My project is to Study the existence of a continuous function differentiable almost everywhere satisfying almost everywhere I began the study by supposing , I have shown that f does not exist. After, I found some difficulties when we assume only f differentiable on , I had an answer using Darboux's theorem Questions about the existence of a function . Now, I want to attack the initial problem. Previous arguments do not work! Do you have any suggestions for me?",f : \mathbb{R} \rightarrow \mathbb{R}   f\circ f'(x)=x x \in \mathbb{R} f\in   C ^ 1(\mathbb{R})  \mathbb{R},"['real-analysis', 'functional-analysis', 'derivatives', 'functional-equations']"
36,"A Question on certain Hilbert space of continuous functions, and a characteristic of convergence in it","A Question on certain Hilbert space of continuous functions, and a characteristic of convergence in it",,"Define $T^k(\Omega)$, $\Omega$ an open subset of $\mathbb{R}^m$ (with a smooth boundary), as a space of function equivalance classes, with the norm defined as $$ \|f\|_{T^k(\Omega)}^2 =     \|f\|_{L^2(\Omega)}^2 + \|(\sum\limits_{i=1}^m(\frac{\partial^{k}f}{\partial x_i^{k}})^2)^{\frac{1}{2}}\|_{L^2(\Omega)}^2 $$ It can be easily noted that $T^k(\Omega)$ is a Hilbert space. Also note that this norm is not a Sobolev norm, as we don't consider cross derivatives. Consider the set $$M = C^0(\bar{\Omega}) \cap T^k(\Omega)  $$ Prove that : If $k > \frac{m}{2}$, every sequence $\{f_n\},f_n \in M$, that converges in the norm $\|.\|_{T^k(\Omega)}$, also converges in the norm $\|.\|_{C^0(\bar{\Omega})}$ (and to a limit $f \in M$) Proof : Consider a sequence $f_n \in M$ and let $f_n\to f \in M$ in the norm $\|.\|_{T^k(\Omega)}$ Idea is to add a small perturbation in the form of a shrinking bump, to produce a simple discontinuity in the limit function $f$. Lets add a small bump function $\psi(n\boldsymbol{x})$ to $f_n(\boldsymbol{x})$ to form a new sequence $$\phi_n(\boldsymbol{x}) = \psi(n\boldsymbol{x}) + f_n(\boldsymbol{x}) $$ Now we show that, in doing so, we blow up the norm and spoil the convergence of the sequence. For simplicity, assume $\psi_n(\boldsymbol{x}) = \psi(n\boldsymbol{x})$ is radially symmetric. With a change of variable $\boldsymbol{t} = n\boldsymbol{x}$  we can easily see that  $$\|f_n(\boldsymbol{x}) + \psi(n\boldsymbol{x})\|_{L^2(\Omega)} \to \|f\|_{L^2(\Omega)}$$ But when we consider the other term of the norm, again with a change of variable $\boldsymbol{t} = n\boldsymbol{x}$, we can see that $$\begin{align} \int_{\Omega} |\frac{\partial^k{\phi_n}}{\partial{x_i^k}}|^2 \mathop{}\!\mathrm{d}^m\boldsymbol{x} & = \int_{\Omega}|\frac{\partial^k{f_n}}{\partial{x_i^k}}|^2 \mathop{}\!\mathrm{d}^m\boldsymbol{x} + 2\int_{\Omega}\frac{\partial^k{f_n}}{\partial{x_i^k}} \frac{\partial^k{\psi_n}}{\partial{x_i^k}}\mathop{}\!\mathrm{d}^m\boldsymbol{x} +  \int_{\Omega}|\frac{\partial^k{\psi_n}}{\partial{x_i^k}}|^2 \mathop{}\!\mathrm{d}^m\boldsymbol{x} \\\\ & = \|\frac{\partial^k{f_n}}{\partial{t_i^k}}\|_{L^2}^2 + O(n^{(k-m)}\|\frac{\partial^k{f_n}}{\partial{t_i^k}}\|_{L^2} \|\frac{\partial^k{\psi}}{\partial{t_i^k}}\|_{L^2}) +  n^{(2k-m)}\|\frac{\partial^k{\psi}}{\partial{t_i^k}}\|_{L^2}^2\end{align} $$ The last term blows up, when $k > \frac{m}{2}$. So one cannot produce a discontinuity by way of adding a shrinking bump. Hence all sequences in $M$ that converge in the norm $\|.\|_{T^k(\Omega)}$ also converge in the norm $\|.\|_{C^0(\bar{\Omega})}$ Other cases : For a jump discontinuity, we let the bump have a flatter region and we shrink only the transition region. Same logic applies here. For a blow up situation, consider $\phi_n(\boldsymbol{x}) = g(n)\psi(n\boldsymbol{x})$, where $g(n) = \omega(1)$ ( Bachmann–Landau notations ), which means $g(n)$ grows faster than 1, or $\lim\limits_{n\to\infty}g(n) = \infty$. In this case, one can see that the last term in the RHS of the last equation in my proof, blows up when $k\ge\frac{m}{2}$. Hence blow up discontinuity is also ruled out in case of $k>\frac{m}{2}$. Case of Oscillatory discontinuity remains: For oscillatory case consider $\phi_n(\boldsymbol{x}) = \sin(n\boldsymbol{x})\psi_(n\boldsymbol{x})$ Question : Is the above proof complete?(if at all it makes sense) Is there any simpler proof with a direct application of a well known theorem? Similar references appreciated. For $\Omega = [0,1)^m$, with periodic boundary conditions, this norm is equivalent to Sobolev norm, hence by general Sobolev inequality, the result follows. But I am seeking a direct proof as in this approach, without invoking Sobolev inequality.","Define $T^k(\Omega)$, $\Omega$ an open subset of $\mathbb{R}^m$ (with a smooth boundary), as a space of function equivalance classes, with the norm defined as $$ \|f\|_{T^k(\Omega)}^2 =     \|f\|_{L^2(\Omega)}^2 + \|(\sum\limits_{i=1}^m(\frac{\partial^{k}f}{\partial x_i^{k}})^2)^{\frac{1}{2}}\|_{L^2(\Omega)}^2 $$ It can be easily noted that $T^k(\Omega)$ is a Hilbert space. Also note that this norm is not a Sobolev norm, as we don't consider cross derivatives. Consider the set $$M = C^0(\bar{\Omega}) \cap T^k(\Omega)  $$ Prove that : If $k > \frac{m}{2}$, every sequence $\{f_n\},f_n \in M$, that converges in the norm $\|.\|_{T^k(\Omega)}$, also converges in the norm $\|.\|_{C^0(\bar{\Omega})}$ (and to a limit $f \in M$) Proof : Consider a sequence $f_n \in M$ and let $f_n\to f \in M$ in the norm $\|.\|_{T^k(\Omega)}$ Idea is to add a small perturbation in the form of a shrinking bump, to produce a simple discontinuity in the limit function $f$. Lets add a small bump function $\psi(n\boldsymbol{x})$ to $f_n(\boldsymbol{x})$ to form a new sequence $$\phi_n(\boldsymbol{x}) = \psi(n\boldsymbol{x}) + f_n(\boldsymbol{x}) $$ Now we show that, in doing so, we blow up the norm and spoil the convergence of the sequence. For simplicity, assume $\psi_n(\boldsymbol{x}) = \psi(n\boldsymbol{x})$ is radially symmetric. With a change of variable $\boldsymbol{t} = n\boldsymbol{x}$  we can easily see that  $$\|f_n(\boldsymbol{x}) + \psi(n\boldsymbol{x})\|_{L^2(\Omega)} \to \|f\|_{L^2(\Omega)}$$ But when we consider the other term of the norm, again with a change of variable $\boldsymbol{t} = n\boldsymbol{x}$, we can see that $$\begin{align} \int_{\Omega} |\frac{\partial^k{\phi_n}}{\partial{x_i^k}}|^2 \mathop{}\!\mathrm{d}^m\boldsymbol{x} & = \int_{\Omega}|\frac{\partial^k{f_n}}{\partial{x_i^k}}|^2 \mathop{}\!\mathrm{d}^m\boldsymbol{x} + 2\int_{\Omega}\frac{\partial^k{f_n}}{\partial{x_i^k}} \frac{\partial^k{\psi_n}}{\partial{x_i^k}}\mathop{}\!\mathrm{d}^m\boldsymbol{x} +  \int_{\Omega}|\frac{\partial^k{\psi_n}}{\partial{x_i^k}}|^2 \mathop{}\!\mathrm{d}^m\boldsymbol{x} \\\\ & = \|\frac{\partial^k{f_n}}{\partial{t_i^k}}\|_{L^2}^2 + O(n^{(k-m)}\|\frac{\partial^k{f_n}}{\partial{t_i^k}}\|_{L^2} \|\frac{\partial^k{\psi}}{\partial{t_i^k}}\|_{L^2}) +  n^{(2k-m)}\|\frac{\partial^k{\psi}}{\partial{t_i^k}}\|_{L^2}^2\end{align} $$ The last term blows up, when $k > \frac{m}{2}$. So one cannot produce a discontinuity by way of adding a shrinking bump. Hence all sequences in $M$ that converge in the norm $\|.\|_{T^k(\Omega)}$ also converge in the norm $\|.\|_{C^0(\bar{\Omega})}$ Other cases : For a jump discontinuity, we let the bump have a flatter region and we shrink only the transition region. Same logic applies here. For a blow up situation, consider $\phi_n(\boldsymbol{x}) = g(n)\psi(n\boldsymbol{x})$, where $g(n) = \omega(1)$ ( Bachmann–Landau notations ), which means $g(n)$ grows faster than 1, or $\lim\limits_{n\to\infty}g(n) = \infty$. In this case, one can see that the last term in the RHS of the last equation in my proof, blows up when $k\ge\frac{m}{2}$. Hence blow up discontinuity is also ruled out in case of $k>\frac{m}{2}$. Case of Oscillatory discontinuity remains: For oscillatory case consider $\phi_n(\boldsymbol{x}) = \sin(n\boldsymbol{x})\psi_(n\boldsymbol{x})$ Question : Is the above proof complete?(if at all it makes sense) Is there any simpler proof with a direct application of a well known theorem? Similar references appreciated. For $\Omega = [0,1)^m$, with periodic boundary conditions, this norm is equivalent to Sobolev norm, hence by general Sobolev inequality, the result follows. But I am seeking a direct proof as in this approach, without invoking Sobolev inequality.",,"['real-analysis', 'functional-analysis', 'partial-differential-equations', 'sobolev-spaces']"
37,Schilling's proof of the Feynman-Kac Formula for Brownian motion,Schilling's proof of the Feynman-Kac Formula for Brownian motion,,"This is part of a proof to the Feynman-Kac formula from Schilling's Brownian motion. I need some help understanding the proof to this theorem. Theorem (Kac 1949). Let $(B_t)_{t\ge 0}$ be a $d$ -dimensional Brownian motion, $A$ be the generator of the Feller semigroup $P_t u(x)=E^x u(B_t)$ . If $f \in D(A)$ and $c \in C_b (\mathbb{R}^d)$ , then the unique solution to the initial value problem (8.8) is given by $$w(t,x)=E^x \bigg[ f(B_t) \exp \bigg( \int_0^t c(B_r)dr\bigg)\bigg].$$ This solution is bounded by $e^{\alpha t}$ where $\alpha=\sup_{x\in \mathbb{R}^d} c(x)$ . Proof. Denote by $P_t$ the Brownian semigroup. Set $C_t := \int_0^t c(B_r)dr$ and $T_t u(x):= E^x (u(B_t)e^{C_t})$ . If we can show that $(T_t)_{t\ge 0}$ is a Feller semigroup with generator $Lu=Au+cu$ , we are done. Existence follows from Lemma 7.10, uniqueness from the fact that $A$ has a resolvent which is uniquely determined by $T_t$ , cf. Proposition 7.13. Questions. 7.10 is the lemma below and 7.13 states that there is a one-one relationship between the semigroup and the resolvent operator. It seems to me that, to solve this problem, we need to use the uniqueness of the Laplace transform. For instance, below, the author uses it to prove the heat equation Lemma 8.1. However, I cannot directly apply this same method here because $c$ is not a positive constant, so I do not get something of the form $\alpha id - A$ , which would be the inverse of the resolvent operator. So, I don't understand how we get uniqueness from Proposition 7.13 as we did in Lemma 8.1 below. Moreover, I have a question about existence as well. It seems like the author states that existence follows naturally as by Lemma 7.10 we have $\frac{\partial}{\partial t} w(t,x) = Lw$ and $Aw=\frac{1}{2} \Delta w$ . However, it is shown in the text that $A= \frac{1}{2} \Delta$ when restricted to $C_\infty^2 (\mathbb{R}^d):=\{u\in C_\infty (\mathbb{R}^d): \partial_j u, \partial_j \partial_k u \in C_\infty (\mathbb{R}^d), j,k=1,\dots ,d\}.$ Also, $C_\infty^2 (\mathbb{R}^d) \subset D(A)$ , but is strictly smaller than $D(A)$ for $d>1$ . Hence, even if we have $Lu - Au - cu=0$ , we cannot guarantee that we have the form $(8.8a)$ where $A= \frac{1}{2} \Delta_x w(t,x)$ . So what allows us to conclude the proof here? I would greatly appreciate some help.","This is part of a proof to the Feynman-Kac formula from Schilling's Brownian motion. I need some help understanding the proof to this theorem. Theorem (Kac 1949). Let be a -dimensional Brownian motion, be the generator of the Feller semigroup . If and , then the unique solution to the initial value problem (8.8) is given by This solution is bounded by where . Proof. Denote by the Brownian semigroup. Set and . If we can show that is a Feller semigroup with generator , we are done. Existence follows from Lemma 7.10, uniqueness from the fact that has a resolvent which is uniquely determined by , cf. Proposition 7.13. Questions. 7.10 is the lemma below and 7.13 states that there is a one-one relationship between the semigroup and the resolvent operator. It seems to me that, to solve this problem, we need to use the uniqueness of the Laplace transform. For instance, below, the author uses it to prove the heat equation Lemma 8.1. However, I cannot directly apply this same method here because is not a positive constant, so I do not get something of the form , which would be the inverse of the resolvent operator. So, I don't understand how we get uniqueness from Proposition 7.13 as we did in Lemma 8.1 below. Moreover, I have a question about existence as well. It seems like the author states that existence follows naturally as by Lemma 7.10 we have and . However, it is shown in the text that when restricted to Also, , but is strictly smaller than for . Hence, even if we have , we cannot guarantee that we have the form where . So what allows us to conclude the proof here? I would greatly appreciate some help.","(B_t)_{t\ge 0} d A P_t u(x)=E^x u(B_t) f \in D(A) c \in C_b (\mathbb{R}^d) w(t,x)=E^x \bigg[ f(B_t) \exp \bigg( \int_0^t c(B_r)dr\bigg)\bigg]. e^{\alpha t} \alpha=\sup_{x\in \mathbb{R}^d} c(x) P_t C_t := \int_0^t c(B_r)dr T_t u(x):= E^x (u(B_t)e^{C_t}) (T_t)_{t\ge 0} Lu=Au+cu A T_t c \alpha id - A \frac{\partial}{\partial t} w(t,x) = Lw Aw=\frac{1}{2} \Delta w A= \frac{1}{2} \Delta C_\infty^2 (\mathbb{R}^d):=\{u\in C_\infty (\mathbb{R}^d): \partial_j u, \partial_j \partial_k u \in C_\infty (\mathbb{R}^d), j,k=1,\dots ,d\}. C_\infty^2 (\mathbb{R}^d) \subset D(A) D(A) d>1 Lu - Au - cu=0 (8.8a) A= \frac{1}{2} \Delta_x w(t,x)","['real-analysis', 'analysis', 'partial-differential-equations', 'stochastic-processes', 'brownian-motion']"
38,Covering a set $A \subset \Bbb R$ by two families of disjoint intervals taken from given intervals,Covering a set  by two families of disjoint intervals taken from given intervals,A \subset \Bbb R,"Let $A \subset \Bbb R$ be a bounded set. Every element $a \in A$ is the center of some given open interval, let's denote it by $I_a=(a-r_a, a+r_a)$. I'm interested in knowing the following: Can we always color some of the given intervals $I_a$ in red and some others in blue such that intervals of the same color are disjoint, and the colored intervals together contain all of $A$? I know that there is some constant number of colors that suffices for all possible $A$ and $I_a$, and this holds even in higher dimensions for different constants; this is Besicovitch's covering theorem . Thus we can rephrase the question as: what is the optimal constant for Besicovitch's covering theorem in $\Bbb R$? Note that $A$ is not necessarily closed, so it seems we cannot choose some ""extremal"" values such as largest intervals to construct a greedy approach to coloring. A now-deleted answer linked to this post containing a proof. However, I don't understand the proof at all. So it suffices if someone would write it in a way that is easier to comprehend.","Let $A \subset \Bbb R$ be a bounded set. Every element $a \in A$ is the center of some given open interval, let's denote it by $I_a=(a-r_a, a+r_a)$. I'm interested in knowing the following: Can we always color some of the given intervals $I_a$ in red and some others in blue such that intervals of the same color are disjoint, and the colored intervals together contain all of $A$? I know that there is some constant number of colors that suffices for all possible $A$ and $I_a$, and this holds even in higher dimensions for different constants; this is Besicovitch's covering theorem . Thus we can rephrase the question as: what is the optimal constant for Besicovitch's covering theorem in $\Bbb R$? Note that $A$ is not necessarily closed, so it seems we cannot choose some ""extremal"" values such as largest intervals to construct a greedy approach to coloring. A now-deleted answer linked to this post containing a proof. However, I don't understand the proof at all. So it suffices if someone would write it in a way that is easier to comprehend.",,"['real-analysis', 'general-topology']"
39,Fréchet L-Spaces,Fréchet L-Spaces,,"NOTE: The question has now been posted on MathOverflow: Fréchet L-Spaces According to the paper The emergence of open sets, closed sets, and limit points in analysis and topology famous mathematician Maurice Fréchet who introduced the concept of metric spaces has also introduced another similar class of abstract spaces  called Limit spaces based on the primitive idea of the limit of an infinite sequence in 1904, which was defined as follows: An L-space is a set $X$ together with a function $F : S\to X,$ where $S$ is a set of infinite sequences of members of $X$. If $(x_n)\in S$, then $F(x_n)$ was said to be the “limit of the sequence $(x_n)$"" satisfying following two axioms: $A_1$: If $(x_n)$ is a constant sequence whose value is $a$, then $F(x_n)=a$. $A_2$: If $F(x_n)=a$, then for any sub-sequence of $(x_n)$ given by $(x_{n_k})$ we have $F(x_{n_k})=a$. I would like to know more about mathematics on L-spaces. But I could not find any thing by Googling. Where could I find about these spaces? Also I wonder; why this concept is not popular in mathematics?","NOTE: The question has now been posted on MathOverflow: Fréchet L-Spaces According to the paper The emergence of open sets, closed sets, and limit points in analysis and topology famous mathematician Maurice Fréchet who introduced the concept of metric spaces has also introduced another similar class of abstract spaces  called Limit spaces based on the primitive idea of the limit of an infinite sequence in 1904, which was defined as follows: An L-space is a set $X$ together with a function $F : S\to X,$ where $S$ is a set of infinite sequences of members of $X$. If $(x_n)\in S$, then $F(x_n)$ was said to be the “limit of the sequence $(x_n)$"" satisfying following two axioms: $A_1$: If $(x_n)$ is a constant sequence whose value is $a$, then $F(x_n)=a$. $A_2$: If $F(x_n)=a$, then for any sub-sequence of $(x_n)$ given by $(x_{n_k})$ we have $F(x_{n_k})=a$. I would like to know more about mathematics on L-spaces. But I could not find any thing by Googling. Where could I find about these spaces? Also I wonder; why this concept is not popular in mathematics?",,"['real-analysis', 'general-topology', 'reference-request', 'convergence-divergence', 'math-history']"
40,Fourier transform of $ |x|^{s} $ and $\log|x| $,Fourier transform of  and, |x|^{s}  \log|x| ,"Can anyone provide or give an expression in the sense of distribution theory for the functions $|x|^{s} , \log|x| $? I mean I would like to evaluate the Fourier transform $ \int_{-\infty}^{\infty}f(x)\exp(-iux) $ of these transforms in case it is possible.","Can anyone provide or give an expression in the sense of distribution theory for the functions $|x|^{s} , \log|x| $? I mean I would like to evaluate the Fourier transform $ \int_{-\infty}^{\infty}f(x)\exp(-iux) $ of these transforms in case it is possible.",,"['fourier-analysis', 'integral-transforms']"
41,Function whose gradient is of constant norm,Function whose gradient is of constant norm,,"Let $f:\mathbb R^n\rightarrow \mathbb R$ be a smooth function such that $\|\nabla f(x)\|=1$ for all $x\in \mathbb R^n$ and $f(0)=0$. I would like to prove that $f$ is linear. I first looked at the solution of the O.D.E. $$\dfrac{d\gamma}{dt}(t)=\nabla f(\gamma(t))$$ I noticed that there exists only one solution $\gamma_x$ passing through $x\in \mathbb R^n$ at $t=0$, and such a solution is defined over $\mathbb R$. I also proved that $$\gamma_x(t)=x+t\nabla f(x)$$ and $$f(\gamma_x(t))=f(x)+t.$$ Furthermore, the following hint is given : Show that if $f(x)=f(y)$ then $\langle \nabla f(x),x-y\rangle=\langle \nabla f(y),x-y\rangle=0$. I did it, but now, I don't understand why it follows that $f$ is linear. Edit : I write here the proof of the hint; Let $c:[0,1]\rightarrow \mathbb R^n$ be the (usual) parametrization of the segment $[x+t\nabla f(x),y]$. Since $f(x)=f(y)$, we get $$\begin{align*}|t|=|f(\gamma_x(t))-f(x)| & =|f(x+t\nabla f(x))-f(y)|\\ & =|\int_0^1 \langle \nabla f(c(s)),c'(s)\rangle ds| \\ & \leq \int_0^1 \|c'(s)\|ds \\ & =\|y-x-t\nabla f(x)\|\end{align*}$$ From this inequality, which is true for any $t\in \mathbb R$ and using the fact that $\nabla f(x)$ is a unitary vector, it follows that : $$t^2\leq \|x-y\|^2 + t^2 +2t\langle x-y,\nabla f(x) \rangle$$ Dividing by $t$ and taking limits to $\pm \infty$ gives the hint.","Let $f:\mathbb R^n\rightarrow \mathbb R$ be a smooth function such that $\|\nabla f(x)\|=1$ for all $x\in \mathbb R^n$ and $f(0)=0$. I would like to prove that $f$ is linear. I first looked at the solution of the O.D.E. $$\dfrac{d\gamma}{dt}(t)=\nabla f(\gamma(t))$$ I noticed that there exists only one solution $\gamma_x$ passing through $x\in \mathbb R^n$ at $t=0$, and such a solution is defined over $\mathbb R$. I also proved that $$\gamma_x(t)=x+t\nabla f(x)$$ and $$f(\gamma_x(t))=f(x)+t.$$ Furthermore, the following hint is given : Show that if $f(x)=f(y)$ then $\langle \nabla f(x),x-y\rangle=\langle \nabla f(y),x-y\rangle=0$. I did it, but now, I don't understand why it follows that $f$ is linear. Edit : I write here the proof of the hint; Let $c:[0,1]\rightarrow \mathbb R^n$ be the (usual) parametrization of the segment $[x+t\nabla f(x),y]$. Since $f(x)=f(y)$, we get $$\begin{align*}|t|=|f(\gamma_x(t))-f(x)| & =|f(x+t\nabla f(x))-f(y)|\\ & =|\int_0^1 \langle \nabla f(c(s)),c'(s)\rangle ds| \\ & \leq \int_0^1 \|c'(s)\|ds \\ & =\|y-x-t\nabla f(x)\|\end{align*}$$ From this inequality, which is true for any $t\in \mathbb R$ and using the fact that $\nabla f(x)$ is a unitary vector, it follows that : $$t^2\leq \|x-y\|^2 + t^2 +2t\langle x-y,\nabla f(x) \rangle$$ Dividing by $t$ and taking limits to $\pm \infty$ gives the hint.",,"['real-analysis', 'ordinary-differential-equations', 'differential-geometry', 'riemannian-geometry', 'gradient-flows']"
42,Why are proofs not written as collections of logic symbols but are instead written in sentences? [duplicate],Why are proofs not written as collections of logic symbols but are instead written in sentences? [duplicate],,"This question already has answers here : Why do proof authors use natural language sentences to write proofs? (5 answers) Closed 3 years ago . Mathematical proofs are written as sentences and not as collections of logic symbols. Through logical operations, it is much easier for me to visualize what the symbols are trying to tell us rather than English text filled with grammar. This is my personal opinion, others may have different opinions. I just asked this question on another website to find out logical mistakes in my work which is entirely done in the language of propositional logic. Some people suggested to write it down in sentences in English. Is there any kind of tragedy in writing proofs as collections of logic symbols?","This question already has answers here : Why do proof authors use natural language sentences to write proofs? (5 answers) Closed 3 years ago . Mathematical proofs are written as sentences and not as collections of logic symbols. Through logical operations, it is much easier for me to visualize what the symbols are trying to tell us rather than English text filled with grammar. This is my personal opinion, others may have different opinions. I just asked this question on another website to find out logical mistakes in my work which is entirely done in the language of propositional logic. Some people suggested to write it down in sentences in English. Is there any kind of tragedy in writing proofs as collections of logic symbols?",,"['real-analysis', 'multivariable-calculus', 'proof-writing', 'propositional-calculus']"
43,Is irrational times rational always irrational? [duplicate],Is irrational times rational always irrational? [duplicate],,"This question already has answers here : Prove that the product of a non-zero rational and irrational number is irrational. (9 answers) Closed 9 years ago . Is an irrational number times a rational number always irrational? If the rational number is zero, then the result will be rational. So can we conclude that in general, we can't decide, and it depends on the rational number?","This question already has answers here : Prove that the product of a non-zero rational and irrational number is irrational. (9 answers) Closed 9 years ago . Is an irrational number times a rational number always irrational? If the rational number is zero, then the result will be rational. So can we conclude that in general, we can't decide, and it depends on the rational number?",,['real-analysis']
44,Proving the inequality $e^{-2x}\leq 1-x$,Proving the inequality,e^{-2x}\leq 1-x,How do I prove the inequality $e^{-2x}\leq1-x$ for $0\leq x\leq1/2$?,How do I prove the inequality $e^{-2x}\leq1-x$ for $0\leq x\leq1/2$?,,"['calculus', 'real-analysis', 'inequality', 'exponential-function', 'exponentiation']"
45,"Prove that: $\sqrt{2\sqrt{3\sqrt{4\cdots\sqrt{n}}}}<3,\,\forall n\in\mathbb N.$",Prove that:,"\sqrt{2\sqrt{3\sqrt{4\cdots\sqrt{n}}}}<3,\,\forall n\in\mathbb N.","I know that: $\sqrt{1+2\sqrt{1+3\sqrt{1+4\sqrt{1+\cdots}}}}=3,$ which is one of Ramanujan's infinite radicals. So surely the expression in question is less than $3.$ But how can I prove this without mentioning this or in general how to prove: $\sqrt{2\sqrt{3\sqrt{4\sqrt{\cdots\infty}}}}<3$ ? I'm not quite sure, how to approach this? Expressing the expression as an infinite product: $$\prod_{i=1}^{n} i^{\frac1{2^{i-1}}},\text{ as }n\to\infty$$ and then using some sort underlying inequalities might help! Please suggest. Thanks in advance.","I know that: which is one of Ramanujan's infinite radicals. So surely the expression in question is less than But how can I prove this without mentioning this or in general how to prove: ? I'm not quite sure, how to approach this? Expressing the expression as an infinite product: and then using some sort underlying inequalities might help! Please suggest. Thanks in advance.","\sqrt{1+2\sqrt{1+3\sqrt{1+4\sqrt{1+\cdots}}}}=3, 3. \sqrt{2\sqrt{3\sqrt{4\sqrt{\cdots\infty}}}}<3 \prod_{i=1}^{n} i^{\frac1{2^{i-1}}},\text{ as }n\to\infty","['real-analysis', 'nested-radicals']"
46,"If an inequality is true for all natural numbers, is it necessarily true for all real numbers inbetween?","If an inequality is true for all natural numbers, is it necessarily true for all real numbers inbetween?",,"A lot of the time in lectures, my professors prove (by induction) an inequality (e.g. $(1+x)^n \geq 1+nx$) in the natural numbers (or any subsets thereof), and I've noticed (not rigourously; only by graphing the functions) that such statements are also true for all real numbers inbetween. Another example is that exponential growth beats polynomial growth. My question is: If an inequality is true for all $n \in \mathbb{N},$ does it necessarily follow that the same inequality is true for all $n \in \mathbb{R^+}$? I'm not in the market for a rigourous proof; (if the answer's no) just  a counter-example, or (if the answer's yes) an intuitive reason why this is the case.","A lot of the time in lectures, my professors prove (by induction) an inequality (e.g. $(1+x)^n \geq 1+nx$) in the natural numbers (or any subsets thereof), and I've noticed (not rigourously; only by graphing the functions) that such statements are also true for all real numbers inbetween. Another example is that exponential growth beats polynomial growth. My question is: If an inequality is true for all $n \in \mathbb{N},$ does it necessarily follow that the same inequality is true for all $n \in \mathbb{R^+}$? I'm not in the market for a rigourous proof; (if the answer's no) just  a counter-example, or (if the answer's yes) an intuitive reason why this is the case.",,"['real-analysis', 'elementary-number-theory']"
47,Determine whether $f(x)={\sin x \over x}$ is uniformly continuous in $\mathbb R$,Determine whether  is uniformly continuous in,f(x)={\sin x \over x} \mathbb R,Determine whether the function$$f(x)={\sin x \over x}$$is uniformly continuous in $\mathbb{R}$. I am using the definition that for $\epsilon>0$ there exists $\delta>0$ such that $|f(x)-f(y)|<\epsilon$ whenever $|x-y|<\delta$.,Determine whether the function$$f(x)={\sin x \over x}$$is uniformly continuous in $\mathbb{R}$. I am using the definition that for $\epsilon>0$ there exists $\delta>0$ such that $|f(x)-f(y)|<\epsilon$ whenever $|x-y|<\delta$.,,"['real-analysis', 'uniform-continuity']"
48,Alternating series; first term is 0. Do I have a problem?,Alternating series; first term is 0. Do I have a problem?,,"I have an alternate series which I want to test for convergence or divergence. The series is as follows: $$\sum_{n=1}^\infty (-1)^n \frac{n^2-1}{n^3+1}$$ I know how to test this for convergence, but the first term is $0$ and so ""$n+1$"" terms are not allways smaller than $n$ terms. I have seen the answer and the series is convergent (although not absolutely, but I knew that from testing $\sum_{n=1}^\infty \frac{n^2-1}{n^3+1}$ in a previous exercise), can I just ""throw out"" the $0$ and say it doesn't matter in the grand scheme of things? The terms of the series tend to $0$, so the conditions for convergence in alternate series are satisfied except for that nasty $0$.","I have an alternate series which I want to test for convergence or divergence. The series is as follows: $$\sum_{n=1}^\infty (-1)^n \frac{n^2-1}{n^3+1}$$ I know how to test this for convergence, but the first term is $0$ and so ""$n+1$"" terms are not allways smaller than $n$ terms. I have seen the answer and the series is convergent (although not absolutely, but I knew that from testing $\sum_{n=1}^\infty \frac{n^2-1}{n^3+1}$ in a previous exercise), can I just ""throw out"" the $0$ and say it doesn't matter in the grand scheme of things? The terms of the series tend to $0$, so the conditions for convergence in alternate series are satisfied except for that nasty $0$.",,"['real-analysis', 'sequences-and-series']"
49,"Does $\,x>0\,$ hint that $\,x\in\mathbb R\,$?",Does  hint that ?,"\,x>0\, \,x\in\mathbb R\,","Does $x>0$ suggest that $x\in\mathbb R$? For numbers not in $\,\mathbb R\,$ (e.g. in $\mathbb C\setminus \mathbb R$), their sizes can't be compared. So can I omit ""$\,x\in\mathbb R\,$"" and just write $\,x>0\,$? Thank you.","Does $x>0$ suggest that $x\in\mathbb R$? For numbers not in $\,\mathbb R\,$ (e.g. in $\mathbb C\setminus \mathbb R$), their sizes can't be compared. So can I omit ""$\,x\in\mathbb R\,$"" and just write $\,x>0\,$? Thank you.",,"['real-analysis', 'notation']"
50,"If $(f_n')$ converges uniformly on an interval, does $(f_n)$ converge?","If  converges uniformly on an interval, does  converge?",(f_n') (f_n),"Let $(f_n)$ be a sequence of functions that are all differentiable on an interval A, and suppose the sequence of derivatives $(f_n')$ converges uniformly on A to a limit function $g$ . Does it follow that $(f_n)$ converges to a limit function f on A? What I tried: As $(f_n')$ converges uniformly to $g$ , we may write that the limit of integral of $(f_n')$ is the integral of the limit of $(f_n')$ . Hence, $(f_n)$ converges point wise to the integral of $g$ . How does this sound?","Let be a sequence of functions that are all differentiable on an interval A, and suppose the sequence of derivatives converges uniformly on A to a limit function . Does it follow that converges to a limit function f on A? What I tried: As converges uniformly to , we may write that the limit of integral of is the integral of the limit of . Hence, converges point wise to the integral of . How does this sound?",(f_n) (f_n') g (f_n) (f_n') g (f_n') (f_n') (f_n) g,"['real-analysis', 'uniform-convergence', 'sequence-of-function', 'pointwise-convergence']"
51,Why is the empty set bounded?,Why is the empty set bounded?,,"Why is the empty set bounded below and bounded above? If it has no elements, how can you say that an upper or lower bound exists?","Why is the empty set bounded below and bounded above? If it has no elements, how can you say that an upper or lower bound exists?",,['real-analysis']
52,Calculate $\underset{x\rightarrow7}{\lim}\frac{\sqrt{x+2}-\sqrt[3]{x+20}}{\sqrt[4]{x+9}-2}$,Calculate,\underset{x\rightarrow7}{\lim}\frac{\sqrt{x+2}-\sqrt[3]{x+20}}{\sqrt[4]{x+9}-2},"Please help me calculate this: $$\underset{x\rightarrow7}{\lim}\frac{\sqrt{x+2}-\sqrt[3]{x+20}}{\sqrt[4]{x+9}-2}$$ Here I've tried multiplying by $\sqrt[4]{x+9}+2$ and few other method. Thanks in advance for solution / hints using simple methods. Edit Please don't use l'Hosplital rule. We are before derivatives, don't know how to use it correctly yet. Thanks!","Please help me calculate this: $$\underset{x\rightarrow7}{\lim}\frac{\sqrt{x+2}-\sqrt[3]{x+20}}{\sqrt[4]{x+9}-2}$$ Here I've tried multiplying by $\sqrt[4]{x+9}+2$ and few other method. Thanks in advance for solution / hints using simple methods. Edit Please don't use l'Hosplital rule. We are before derivatives, don't know how to use it correctly yet. Thanks!",,['real-analysis']
53,"If $f,g$ are continuous at $a$, show that $h(x)=\max\{f(x),g(x)\}$ and $k(x)=\min\{f(x),g(x)\}$ are also continuous at $a$","If  are continuous at , show that  and  are also continuous at","f,g a h(x)=\max\{f(x),g(x)\} k(x)=\min\{f(x),g(x)\} a","If $f,g$ are continuous at $a$, show that $h(x)=\max\{f(x),g(x)\}$ and $k(x)=\min\{f(x),g(x)\}$ are also continuous at $a$. Here is my attempt at a proof. It feels very elaborate and I am not sure if it is correct. Can someone please point out any mistakes or places where I may improve. Thanks! By the definition of continuity at $a$ of $f,g$ we have that $\lim\limits_{x\to a}f(x)=f(a)$ and $\lim\limits_{x\to a}g(x)=g(a)$. Suppose that $f(a)>g(a)$. Then there exists a $\delta>0$ such that $f>g$ for all $x$ satisfying $|x-a|<\delta$. Then for $x$ satisfying $|x-a|<\delta$ we have $h(x)=f(x)$ and $k(x)=g(x)$ and thus $h$ and $k$ are continuous at $a$ because $f$ and $g$ are continuous at $a$. Now if $f(a)<g(a)$ we simply relabel $f=\tilde{g}$ and $g=\tilde{f}$, so that $\tilde{f}(a)>\tilde{g}(a)$ and we apply the previous result to show that again $h$ and $k$ are continuous at $a$.\ Now if $f(a)=g(a)$ then we can distinguish three cases. $f\geq g$ in a small neighborhood about $a$, $f\leq g$  in a small neighborhood about $a$ or $f\geq g$ on one side of $a$ and $f\leq g$ on the other side. In the first case we assume that $f\geq g$ in some small neighborhood about $a$. Then, since $f(a)=g(a)$ we have $h(x) = f(x)$ in this neighborhood and $k(x) = g(x)$ and again we see that $h$ and $k$ are continuous at $a$ due to the continuity of $f$ and $g$ at $a$. Similarly if $f\leq g$ in a neighborhood around $a$ then $h(x)=g(x)$ and $k(x)=f(x)$ in this neighborhood and $h$ and $k$ are continuous at $a$. Suppose that just to the left of $a$ we have $f\geq g$ and just to the right of $a$ we have $f\leq g$. Then $\lim\limits_{x\to a^-}h(x)=\lim\limits_{x\to a^-}f(x)=f(a)=h(a)=g(a)=\lim\limits_{x\to a^+}g(x)=\lim\limits_{x\to a^+}h(x)$. Similarly $\lim\limits_{x\to a^-}k(x)=\lim\limits_{x\to a^-}g(x)=g(a)=k(a)=f(a)=\lim\limits_{x\to a^+}f(x)=\lim\limits_{x\to a^+}k(x)$. So $\lim\limits_{x\to a}h(x)=h(a)$ and $\lim\limits_{x\to a}k(x)=k(a)$ which means that $h$ and $k$ are continuous at $a$. Lastly if $f\leq g$ just to the left of $a$ and $f\geq g$ just to the right of $a$ we can relabel $f=\tilde{g}$ and $g=\tilde{f}$ and apply the last result to show $h$ and $k$ are continuous at $a$. Again, please point out any mistakes I may have made. Thanks!!","If $f,g$ are continuous at $a$, show that $h(x)=\max\{f(x),g(x)\}$ and $k(x)=\min\{f(x),g(x)\}$ are also continuous at $a$. Here is my attempt at a proof. It feels very elaborate and I am not sure if it is correct. Can someone please point out any mistakes or places where I may improve. Thanks! By the definition of continuity at $a$ of $f,g$ we have that $\lim\limits_{x\to a}f(x)=f(a)$ and $\lim\limits_{x\to a}g(x)=g(a)$. Suppose that $f(a)>g(a)$. Then there exists a $\delta>0$ such that $f>g$ for all $x$ satisfying $|x-a|<\delta$. Then for $x$ satisfying $|x-a|<\delta$ we have $h(x)=f(x)$ and $k(x)=g(x)$ and thus $h$ and $k$ are continuous at $a$ because $f$ and $g$ are continuous at $a$. Now if $f(a)<g(a)$ we simply relabel $f=\tilde{g}$ and $g=\tilde{f}$, so that $\tilde{f}(a)>\tilde{g}(a)$ and we apply the previous result to show that again $h$ and $k$ are continuous at $a$.\ Now if $f(a)=g(a)$ then we can distinguish three cases. $f\geq g$ in a small neighborhood about $a$, $f\leq g$  in a small neighborhood about $a$ or $f\geq g$ on one side of $a$ and $f\leq g$ on the other side. In the first case we assume that $f\geq g$ in some small neighborhood about $a$. Then, since $f(a)=g(a)$ we have $h(x) = f(x)$ in this neighborhood and $k(x) = g(x)$ and again we see that $h$ and $k$ are continuous at $a$ due to the continuity of $f$ and $g$ at $a$. Similarly if $f\leq g$ in a neighborhood around $a$ then $h(x)=g(x)$ and $k(x)=f(x)$ in this neighborhood and $h$ and $k$ are continuous at $a$. Suppose that just to the left of $a$ we have $f\geq g$ and just to the right of $a$ we have $f\leq g$. Then $\lim\limits_{x\to a^-}h(x)=\lim\limits_{x\to a^-}f(x)=f(a)=h(a)=g(a)=\lim\limits_{x\to a^+}g(x)=\lim\limits_{x\to a^+}h(x)$. Similarly $\lim\limits_{x\to a^-}k(x)=\lim\limits_{x\to a^-}g(x)=g(a)=k(a)=f(a)=\lim\limits_{x\to a^+}f(x)=\lim\limits_{x\to a^+}k(x)$. So $\lim\limits_{x\to a}h(x)=h(a)$ and $\lim\limits_{x\to a}k(x)=k(a)$ which means that $h$ and $k$ are continuous at $a$. Lastly if $f\leq g$ just to the left of $a$ and $f\geq g$ just to the right of $a$ we can relabel $f=\tilde{g}$ and $g=\tilde{f}$ and apply the last result to show $h$ and $k$ are continuous at $a$. Again, please point out any mistakes I may have made. Thanks!!",,['real-analysis']
54,"Examples of non-continuous, non-piecewise-constant, idempotent map?","Examples of non-continuous, non-piecewise-constant, idempotent map?",,"So a projection $P$ is a linear map such that $P^2 = P$ . If we don't require linearity, then there are other examples of functions $f$ such that $f^2 = f$ . For example, the floor and ceiling functions. If $f: \mathbb{R} \to \mathbb{R}$ and we require continuity, it seems that the image is a closed interval and $f(x)=x$ for all $x$ in the image. I'm wondering if there are any examples of idempotent maps that are not continuous but also not piecewise constant?","So a projection is a linear map such that . If we don't require linearity, then there are other examples of functions such that . For example, the floor and ceiling functions. If and we require continuity, it seems that the image is a closed interval and for all in the image. I'm wondering if there are any examples of idempotent maps that are not continuous but also not piecewise constant?",P P^2 = P f f^2 = f f: \mathbb{R} \to \mathbb{R} f(x)=x x,"['real-analysis', 'functions']"
55,Difficulties in stating mean value theorem for functions which are not continuous on a closed interval.,Difficulties in stating mean value theorem for functions which are not continuous on a closed interval.,,"The mean value theorem is stated as follows Let there be a function $f$ which is continuous on $[a,b]$ and differentiable on $(a,b)$. Then there exists $c$ belonging to $(a,b)$ such that $f′(c)=\frac{f(b)−f(a)}{b−a}$. Now, here it is assumed that the function is continuous on a closed interval. What I don't understand is the use of making the function continuous on a closed interval. I understand that then $f(a)$ and $f(b)$ will not be defined if we use rather an open interval (a,b), but then we can take into account the limits as x approaches a and b of the function (sometimes one-sided limits) instead of the values of the function at $a$ and $b$. I know this would be tedious and would make the theorem complicated, but what I want to ask is that whether there is any other reason for proposing a closed continuous function rather than an open or half-open one.","The mean value theorem is stated as follows Let there be a function $f$ which is continuous on $[a,b]$ and differentiable on $(a,b)$. Then there exists $c$ belonging to $(a,b)$ such that $f′(c)=\frac{f(b)−f(a)}{b−a}$. Now, here it is assumed that the function is continuous on a closed interval. What I don't understand is the use of making the function continuous on a closed interval. I understand that then $f(a)$ and $f(b)$ will not be defined if we use rather an open interval (a,b), but then we can take into account the limits as x approaches a and b of the function (sometimes one-sided limits) instead of the values of the function at $a$ and $b$. I know this would be tedious and would make the theorem complicated, but what I want to ask is that whether there is any other reason for proposing a closed continuous function rather than an open or half-open one.",,"['calculus', 'real-analysis']"
56,How to construct a dense subset of $\mathbb R$ other than rationals.,How to construct a dense subset of  other than rationals.,\mathbb R,"How to construct a dense subset say $A$, of the real numbers other than rationals? By dense I mean that there should be an element of $A$ between any two real numbers.","How to construct a dense subset say $A$, of the real numbers other than rationals? By dense I mean that there should be an element of $A$ between any two real numbers.",,"['real-analysis', 'general-topology']"
57,Example of a function continuous at only one point. [duplicate],Example of a function continuous at only one point. [duplicate],,This question already has answers here : Closed 11 years ago . Possible Duplicate: Find a function $f: \mathbb{R} \to \mathbb{R}$ that is continuous at precisely one point? I want to know some example of a continuous function which is continuous at exactly one point. We know that $f(x)=\frac{1}{x}$ is continuous everywhere except at $x=0$. But i think this in reverse manner but i dont get any example. So please help me out!,This question already has answers here : Closed 11 years ago . Possible Duplicate: Find a function $f: \mathbb{R} \to \mathbb{R}$ that is continuous at precisely one point? I want to know some example of a continuous function which is continuous at exactly one point. We know that $f(x)=\frac{1}{x}$ is continuous everywhere except at $x=0$. But i think this in reverse manner but i dont get any example. So please help me out!,,"['real-analysis', 'functions', 'continuity']"
58,Proof verification: the product of two continuous functions is continuous,Proof verification: the product of two continuous functions is continuous,,"Prove that if $f, g$ : $X$ → $\mathbb R$ are continuous at $a$ ∈ $\mathbb R$, then $f · g$ is continuous in $a$. If $f$ is continuous at $a$, then $∀ε_f > 0, ∃δ_f > 0$ such that $|x-a| < δ_f$ iff $|f(x) - f(a)| < ε_f$ and if $g$ is continuous at $a$, then $∀ε_g > 0, ∃δ_g > 0$ such that $|x-a| < δ_g$ iff $|g(x) - g(a)| < ε_g$. Now make $|f(x)g(x) - f(a)g(a)|$ =  $|f(x)g(x) - f(x)g(a) + f(x)g(a) - f(a)g(a)|$ $≤$  $|f(x)||g(x)-g(a)| + |g(a)||f(x)-f(a)|$ < $ε_f|g(a)| + ε_g|f(x)|$. (1) Notice that if $|f(x) - f(a)| < ε_f$ then $|f(x)| - |f(a)| < ε_f$, so $|f(x)| < ε_f + |f(a)|$. That implies, from (1) : $|f(x)g(x) - f(a)g(a)| < ε_f|g(a)| + ε_g(ε_f + |f(a)|) = ε_f(|g(a)| + ε_g|f(a)|)$. Now I'm a little lost. From the proofs I've seen, it seems to me that I could simply take $δ = min(δ_g, δ_f)$. Also, since $ε_f$ and $ε_g$ can be made as small as one wants, there will be some δ that satisfies the conditions for continuity. But from what I read, I should explicitly show a δ in terms of ε. Anyway, I reasoned that since $f$ and $g$ are continuous, I can make bounds on $x$ symmetrically* as done to $f(x)g(x)$ and got the following: δ = $δ_f(|a| + δ_g|a|)$ = $δ_f|a|(1 + δ_g)$ so: $|x-a| < δ_f|a|(1 + δ_g)$ iff $|f(x)g(x) - f(a)g(a)| <  ε_f(|g(a)| + ε_g|f(a)|)$ I need to clarify/better define this, but it is basically the notion that making the same operations I made on the bounds of $f(x)g(x)$ in relation to the bounds of $f(x)$ and $g(x)$ individually over the bounds of $x$ for each function, should leave me with the adequate bounds for $x$ on the composite function.","Prove that if $f, g$ : $X$ → $\mathbb R$ are continuous at $a$ ∈ $\mathbb R$, then $f · g$ is continuous in $a$. If $f$ is continuous at $a$, then $∀ε_f > 0, ∃δ_f > 0$ such that $|x-a| < δ_f$ iff $|f(x) - f(a)| < ε_f$ and if $g$ is continuous at $a$, then $∀ε_g > 0, ∃δ_g > 0$ such that $|x-a| < δ_g$ iff $|g(x) - g(a)| < ε_g$. Now make $|f(x)g(x) - f(a)g(a)|$ =  $|f(x)g(x) - f(x)g(a) + f(x)g(a) - f(a)g(a)|$ $≤$  $|f(x)||g(x)-g(a)| + |g(a)||f(x)-f(a)|$ < $ε_f|g(a)| + ε_g|f(x)|$. (1) Notice that if $|f(x) - f(a)| < ε_f$ then $|f(x)| - |f(a)| < ε_f$, so $|f(x)| < ε_f + |f(a)|$. That implies, from (1) : $|f(x)g(x) - f(a)g(a)| < ε_f|g(a)| + ε_g(ε_f + |f(a)|) = ε_f(|g(a)| + ε_g|f(a)|)$. Now I'm a little lost. From the proofs I've seen, it seems to me that I could simply take $δ = min(δ_g, δ_f)$. Also, since $ε_f$ and $ε_g$ can be made as small as one wants, there will be some δ that satisfies the conditions for continuity. But from what I read, I should explicitly show a δ in terms of ε. Anyway, I reasoned that since $f$ and $g$ are continuous, I can make bounds on $x$ symmetrically* as done to $f(x)g(x)$ and got the following: δ = $δ_f(|a| + δ_g|a|)$ = $δ_f|a|(1 + δ_g)$ so: $|x-a| < δ_f|a|(1 + δ_g)$ iff $|f(x)g(x) - f(a)g(a)| <  ε_f(|g(a)| + ε_g|f(a)|)$ I need to clarify/better define this, but it is basically the notion that making the same operations I made on the bounds of $f(x)g(x)$ in relation to the bounds of $f(x)$ and $g(x)$ individually over the bounds of $x$ for each function, should leave me with the adequate bounds for $x$ on the composite function.",,"['real-analysis', 'continuity', 'solution-verification', 'epsilon-delta']"
59,Is the infinite-dimensional unit sphere compact?,Is the infinite-dimensional unit sphere compact?,,"Riesz' lemma gives us that in infinite-dimensional spaces no ball is compact, but what about the sphere $\{x \in X : \|x\| = 1\}$? Can we say something about the compactness of the sphere in infinite-dimensional spaces? (I guess the sphere is also not compact and I think one can also show this by constructing a sequence with Riesz lemma that has no convergent subsequence). Is this idea correct?","Riesz' lemma gives us that in infinite-dimensional spaces no ball is compact, but what about the sphere $\{x \in X : \|x\| = 1\}$? Can we say something about the compactness of the sphere in infinite-dimensional spaces? (I guess the sphere is also not compact and I think one can also show this by constructing a sequence with Riesz lemma that has no convergent subsequence). Is this idea correct?",,['real-analysis']
60,The interior of a connected set in $\mathbb R^k$,The interior of a connected set in,\mathbb R^k,Is the interior of a connected set in $\mathbb R^k$ connected?,Is the interior of a connected set in $\mathbb R^k$ connected?,,"['real-analysis', 'general-topology', 'examples-counterexamples', 'connectedness']"
61,Is there a set that is both a sigma algebra and a topology but not a powerset?,Is there a set that is both a sigma algebra and a topology but not a powerset?,,"Is there a set that is both a sigma algebra, $\Sigma$, and a topology, $\tau$, but not a powerset, $\mathcal{P}$?","Is there a set that is both a sigma algebra, $\Sigma$, and a topology, $\tau$, but not a powerset, $\mathcal{P}$?",,"['real-analysis', 'general-topology', 'set-theory']"
62,Which method to use to integrate this function?,Which method to use to integrate this function?,,Recently I've been working on problems including integration and I'm having some difficulties solving this integral : $$\int_0^1 (1-x)\sqrt{4x-x^2} {dx}$$ I tried to rewrite it in this form: $$\int_0^1 (1-x)x\sqrt{\frac{4-x}{x}} dx$$ and then subsitute $\frac{4-x}{x}=t^2$ and by this substitution I get a function way easier integrated of this form: $$-32\int\frac{(t^2-3)t^2}{(1+t^2)^4}dt$$ I can integrate this function easily by simplifying it but the problem is that I cannot define its borders because $t$ is not defined for $x=0$ so this means that my substitution doesn't hold. But I don't have any other idea how to solve it so any help will be appreciated.Thank you!,Recently I've been working on problems including integration and I'm having some difficulties solving this integral : $$\int_0^1 (1-x)\sqrt{4x-x^2} {dx}$$ I tried to rewrite it in this form: $$\int_0^1 (1-x)x\sqrt{\frac{4-x}{x}} dx$$ and then subsitute $\frac{4-x}{x}=t^2$ and by this substitution I get a function way easier integrated of this form: $$-32\int\frac{(t^2-3)t^2}{(1+t^2)^4}dt$$ I can integrate this function easily by simplifying it but the problem is that I cannot define its borders because $t$ is not defined for $x=0$ so this means that my substitution doesn't hold. But I don't have any other idea how to solve it so any help will be appreciated.Thank you!,,"['real-analysis', 'integration', 'definite-integrals']"
63,Product of measurable functions is measurable,Product of measurable functions is measurable,,"Here's the question I'm having a go at: ""Prove that if $f$ and $g$ are measurable then $fg$ is also measurable (express the product using sums and powers of functions)"" I've had a look for the proof that the pointwise sum of measurable functions is measurable and this is relatively simple, however I'm not sure how to do this question using the tip in brackets. Any help or tips are much appreciated, thanks.","Here's the question I'm having a go at: ""Prove that if $f$ and $g$ are measurable then $fg$ is also measurable (express the product using sums and powers of functions)"" I've had a look for the proof that the pointwise sum of measurable functions is measurable and this is relatively simple, however I'm not sure how to do this question using the tip in brackets. Any help or tips are much appreciated, thanks.",,['real-analysis']
64,Does this derivation on differentiating the Euclidean norm make sense?,Does this derivation on differentiating the Euclidean norm make sense?,,Grateful if somebody could help me have a look at the following — does it make sense? The derivative of the $f:=\Vert\cdot\Vert_\mathrm{eucl}$ for $v\in \mathbb R^n-\{0\}$ can be obtained by noting that the $$Df=Dg[h(v)]\circ Dh(v)$$ where $$g(x):= \sqrt x;\qquad h(v):=\Vert v\Vert_\mathrm{eucl}^2$$ Then $Dh(v)=2v$ and $Dg(x)={1\over 2}x^{-{1\over2}}\implies Dg[h(v)]={1\over 2}\Vert v\Vert_\mathrm{eucl}^{-{1\over2}}$ So $Df(v)={1\over 2}\Vert2v\Vert^{-{1\over 2}}$?,Grateful if somebody could help me have a look at the following — does it make sense? The derivative of the $f:=\Vert\cdot\Vert_\mathrm{eucl}$ for $v\in \mathbb R^n-\{0\}$ can be obtained by noting that the $$Df=Dg[h(v)]\circ Dh(v)$$ where $$g(x):= \sqrt x;\qquad h(v):=\Vert v\Vert_\mathrm{eucl}^2$$ Then $Dh(v)=2v$ and $Dg(x)={1\over 2}x^{-{1\over2}}\implies Dg[h(v)]={1\over 2}\Vert v\Vert_\mathrm{eucl}^{-{1\over2}}$ So $Df(v)={1\over 2}\Vert2v\Vert^{-{1\over 2}}$?,,['real-analysis']
65,Is this a valid way to prove this modified harmonic series diverges?,Is this a valid way to prove this modified harmonic series diverges?,,"I am trying to find a way to prove that $$\dfrac 11 + \dfrac 12 + \dfrac 13 + \dfrac 14  + \cdots \color{red}{-} \dfrac 18 + \cdots$$ where the pattern repeats every $8$ terms. Knowing about the Riemann Series Theorem , I am a little hesitant about manipulating conditionally convergent series at all. Granted that the harmonic series diverges, is the following a valid way to prove my series diverges? $$\dfrac 11 + \dfrac 12 + \dfrac 13 + \cdots + \dfrac 17 - \dfrac 18 + \cdots = \sum_{n=1}^{\infty} \dfrac 1n - 2 \cdot \dfrac 18\sum_{n=1}^{\infty} \dfrac 1n $$ $$=\dfrac 34 \sum_{n=1}^{\infty} \dfrac 1n$$ Since the harmonic series diverges, so does $\dfrac 34$ times it.","I am trying to find a way to prove that $$\dfrac 11 + \dfrac 12 + \dfrac 13 + \dfrac 14  + \cdots \color{red}{-} \dfrac 18 + \cdots$$ where the pattern repeats every $8$ terms. Knowing about the Riemann Series Theorem , I am a little hesitant about manipulating conditionally convergent series at all. Granted that the harmonic series diverges, is the following a valid way to prove my series diverges? $$\dfrac 11 + \dfrac 12 + \dfrac 13 + \cdots + \dfrac 17 - \dfrac 18 + \cdots = \sum_{n=1}^{\infty} \dfrac 1n - 2 \cdot \dfrac 18\sum_{n=1}^{\infty} \dfrac 1n $$ $$=\dfrac 34 \sum_{n=1}^{\infty} \dfrac 1n$$ Since the harmonic series diverges, so does $\dfrac 34$ times it.",,"['calculus', 'real-analysis', 'sequences-and-series', 'divergent-series']"
66,Are the smooth functions dense in either $\mathcal L_2$ or $\mathcal L_1$?,Are the smooth functions dense in either  or ?,\mathcal L_2 \mathcal L_1,"Is the subset consisting of all integrable (or square integrable) smooth functions of the set of all integrable (or square integrable) functions, dense under the usual Euclidean or integral of absolute difference metric? By smooth I mean derivatives of all orders exist.","Is the subset consisting of all integrable (or square integrable) smooth functions of the set of all integrable (or square integrable) functions, dense under the usual Euclidean or integral of absolute difference metric? By smooth I mean derivatives of all orders exist.",,"['real-analysis', 'functions']"
67,How to derive this infinite product formula?,How to derive this infinite product formula?,,"Show: $$\prod_{n=0}^{\infty}\left(1 + x^{2^n}\right) = \frac{1}{1-x}$$ I tried numerous things, multiplying by $x$, dividing, but none of that worked. Also, I realized that: $$\prod_{n=0}^{\infty} \left(1 + x^{2^n}\right) = \sum_{n=0}^{\infty} x^n$$ But I cannot prove the relation. I get: $$(1 + x)(1+x^2)(1+x^4)(1+x^8)...$$ But for  a general $n$ it is more difficult.","Show: $$\prod_{n=0}^{\infty}\left(1 + x^{2^n}\right) = \frac{1}{1-x}$$ I tried numerous things, multiplying by $x$, dividing, but none of that worked. Also, I realized that: $$\prod_{n=0}^{\infty} \left(1 + x^{2^n}\right) = \sum_{n=0}^{\infty} x^n$$ But I cannot prove the relation. I get: $$(1 + x)(1+x^2)(1+x^4)(1+x^8)...$$ But for  a general $n$ it is more difficult.",,"['calculus', 'real-analysis', 'analysis', 'elementary-number-theory', 'contest-math']"
68,"Show that the function $g(x) = x^2 \sin(\frac{1}{x}) ,(g(0) = 0)$ is everywhere differentiable and that $g′(0) = 0$",Show that the function  is everywhere differentiable and that,"g(x) = x^2 \sin(\frac{1}{x}) ,(g(0) = 0) g′(0) = 0","Show that the function $g(x) = x^2 \sin\left(\frac{1}{x}\right) ,(g(0) = 0)$ is everywhere differentiable and that $g′(0) = 0$.","Show that the function $g(x) = x^2 \sin\left(\frac{1}{x}\right) ,(g(0) = 0)$ is everywhere differentiable and that $g′(0) = 0$.",,"['calculus', 'real-analysis', 'derivatives']"
69,Why solving a differentiated integral equation might eventually lead to erroneous solutions of the original problem?,Why solving a differentiated integral equation might eventually lead to erroneous solutions of the original problem?,,"Consider the following integral equation araising in a mathematical-physical problem: $$ \int_0^r f(t) \arcsin \left( \frac{t}{r} \right) \, \mathrm{d}t  + \frac{\pi}{2} \int_r^R f(t) \, \mathrm{d} t = r \, \qquad (0<r<R) \, ,  $$ where $f(t)$ is the unknown function, and $R$ is a positive real number. By differentiating both sides of this equation with respect to the variable $r$ , one obtains $$ -\frac{1}{r} \int_0^r \frac{f(t)t \, \mathrm{d}t}{\sqrt{r^2-t^2}} = 1 \, ,  $$ the solution of which can readily be obtained as $$ f(t) = -1 \, . $$ Nevertheless, upon substitution of the latter solution into the original integral equation given above, one rather gets an additional $-\pi R/2$ term on the left hand side. i was wondering whether some math details or assumptions are overlooked here during this resolution. Any help would be highly appreciated. An alternative resolution approach that leads to the desired solution is also most welcome. Thank you","Consider the following integral equation araising in a mathematical-physical problem: where is the unknown function, and is a positive real number. By differentiating both sides of this equation with respect to the variable , one obtains the solution of which can readily be obtained as Nevertheless, upon substitution of the latter solution into the original integral equation given above, one rather gets an additional term on the left hand side. i was wondering whether some math details or assumptions are overlooked here during this resolution. Any help would be highly appreciated. An alternative resolution approach that leads to the desired solution is also most welcome. Thank you","
\int_0^r f(t) \arcsin \left( \frac{t}{r} \right) \, \mathrm{d}t 
+ \frac{\pi}{2} \int_r^R f(t) \, \mathrm{d} t = r \, \qquad
(0<r<R) \, , 
 f(t) R r 
-\frac{1}{r} \int_0^r \frac{f(t)t \, \mathrm{d}t}{\sqrt{r^2-t^2}} = 1 \, , 
 
f(t) = -1 \, .
 -\pi R/2","['real-analysis', 'integration', 'analysis', 'definite-integrals', 'integral-equations']"
70,Negation of $0 = 1$,Negation of,0 = 1,"I'm taking my first proof-heavy class (real analysis), and one practice problem on the first homework is to write the negation of $$0 = 1$$ My immediate thought was that it would simply be $$0 \neq 1$$ but I'm not 100% certain of that answer. I was wondering if there's more to it than just inverting the $=$ sign, and perhaps you'd distribute the negation like $$\neg 0 \neq \neg1$$ but logically that doesn't make sense to me. I've tried looking this up, but a statement as simple as $0 = 1$ has given me a hard time finding any good search results. Basically to break down my questions: Is $0 \neq 1$ right? if so, do I prove it somehow? if not, how do you negate expressions like $\langle expr \rangle = \langle expr \rangle$?","I'm taking my first proof-heavy class (real analysis), and one practice problem on the first homework is to write the negation of $$0 = 1$$ My immediate thought was that it would simply be $$0 \neq 1$$ but I'm not 100% certain of that answer. I was wondering if there's more to it than just inverting the $=$ sign, and perhaps you'd distribute the negation like $$\neg 0 \neq \neg1$$ but logically that doesn't make sense to me. I've tried looking this up, but a statement as simple as $0 = 1$ has given me a hard time finding any good search results. Basically to break down my questions: Is $0 \neq 1$ right? if so, do I prove it somehow? if not, how do you negate expressions like $\langle expr \rangle = \langle expr \rangle$?",,"['real-analysis', 'logic']"
71,"How to integrate $\int_0^{\infty} \frac{e^{ax} - e^{bx}}{(1 + e^{ax})(1+ e^{bx})}dx$ where $a,b > 0$.",How to integrate  where .,"\int_0^{\infty} \frac{e^{ax} - e^{bx}}{(1 + e^{ax})(1+ e^{bx})}dx a,b > 0","This  $$\ \int_0^{\infty} \frac{e^{ax} - e^{bx}}{(1 + e^{ax})(1+ e^{bx})}dx \text{ where } a,b > 0. $$ is a problem that showed up on a GRE practice test. I believe you're supposed to use complex contour integration, but I'm not sure which contour to use. I think it's the keyhole contour, but I wasn't able to get anything useful.","This  $$\ \int_0^{\infty} \frac{e^{ax} - e^{bx}}{(1 + e^{ax})(1+ e^{bx})}dx \text{ where } a,b > 0. $$ is a problem that showed up on a GRE practice test. I believe you're supposed to use complex contour integration, but I'm not sure which contour to use. I think it's the keyhole contour, but I wasn't able to get anything useful.",,"['calculus', 'real-analysis', 'integration', 'improper-integrals']"
72,"Show $\max{\{a,b\}}=\frac1{2}(a+b+|a-b|)$",Show,"\max{\{a,b\}}=\frac1{2}(a+b+|a-b|)","I am tasked with showing that If $a,b\in \mathbb{R}$, show that $\max{\{a,b\}}=\frac1{2}(a+b+|a-b|)$ I think I can say ""without loss of generality, let $a<b$.""  Then $b-a>0$  But also, $$\max{\{a,b\}}=b=\frac1{2}a+\frac1{2}b-\frac1{2}a+\frac1{2}b$$ $$=\frac1{2}(a+b-a+b)$$ $$=\frac1{2}(a+b+(-a+b))$$ $$=\frac1{2}(a+b+|b-a|)$$ $$=\frac1{2}(a+b+|a-b|)$$ Is this valid?  Does the proof (if valid) work the same way for the $\min$ function?","I am tasked with showing that If $a,b\in \mathbb{R}$, show that $\max{\{a,b\}}=\frac1{2}(a+b+|a-b|)$ I think I can say ""without loss of generality, let $a<b$.""  Then $b-a>0$  But also, $$\max{\{a,b\}}=b=\frac1{2}a+\frac1{2}b-\frac1{2}a+\frac1{2}b$$ $$=\frac1{2}(a+b-a+b)$$ $$=\frac1{2}(a+b+(-a+b))$$ $$=\frac1{2}(a+b+|b-a|)$$ $$=\frac1{2}(a+b+|a-b|)$$ Is this valid?  Does the proof (if valid) work the same way for the $\min$ function?",,"['real-analysis', 'algebra-precalculus', 'proof-verification', 'absolute-value']"
73,Convergence of the series $\sum \limits_{n=2}^{\infty} \frac{1}{n\log^s n}$ [duplicate],Convergence of the series  [duplicate],\sum \limits_{n=2}^{\infty} \frac{1}{n\log^s n},"This question already has an answer here : Find the values of $p$ for which $\sum_{n=2}^\infty \frac{1}{n(\ln n)^p}$ is convergent (1 answer) Closed 3 years ago . We all know that $\displaystyle \sum_{n=1}^{\infty} \frac{1}{n^s}$ converges for $s>1$ and diverges for $s \leq 1$ (Assume $s \in \mathbb{R}$). I was curious to see till what extent I can push the denominator so that it will still diverges. So I took $\displaystyle \sum_{n=2}^{\infty} \frac{1}{n\log n}$ and found that it still diverges. (This can be checked by using the well known test that if we have a monotone decreasing sequence, then $\displaystyle \sum_{n=2}^{\infty} a_n$ converges iff $\displaystyle \sum_{n=2}^{\infty} 2^na_{2^n}$ converges). No surprises here. I expected it to diverge since $\log n$ grows slowly than any power of $n$. However, when I take $\displaystyle \sum_{n=2}^{\infty} \frac{1}{n(\log n)^s}$, I find that it converges $\forall s>1$. (By the same argument as previous). This doesn't make sense to me though. If this were to converge, then I should be able to find a $s_1 > 1$ such that $\displaystyle \sum_{n=2}^{\infty} \frac{1}{n^{s_1}}$ is greater than $\displaystyle \sum_{n=2}^{\infty} \frac{1}{n (\log n)^s}$ Doesn't this mean that in some sense $\log n$ grows faster than a power of $n$? (or) How should I make sense of (or) interpret this result? (I am assuming that my convergence and divergence conclusions are right).","This question already has an answer here : Find the values of $p$ for which $\sum_{n=2}^\infty \frac{1}{n(\ln n)^p}$ is convergent (1 answer) Closed 3 years ago . We all know that $\displaystyle \sum_{n=1}^{\infty} \frac{1}{n^s}$ converges for $s>1$ and diverges for $s \leq 1$ (Assume $s \in \mathbb{R}$). I was curious to see till what extent I can push the denominator so that it will still diverges. So I took $\displaystyle \sum_{n=2}^{\infty} \frac{1}{n\log n}$ and found that it still diverges. (This can be checked by using the well known test that if we have a monotone decreasing sequence, then $\displaystyle \sum_{n=2}^{\infty} a_n$ converges iff $\displaystyle \sum_{n=2}^{\infty} 2^na_{2^n}$ converges). No surprises here. I expected it to diverge since $\log n$ grows slowly than any power of $n$. However, when I take $\displaystyle \sum_{n=2}^{\infty} \frac{1}{n(\log n)^s}$, I find that it converges $\forall s>1$. (By the same argument as previous). This doesn't make sense to me though. If this were to converge, then I should be able to find a $s_1 > 1$ such that $\displaystyle \sum_{n=2}^{\infty} \frac{1}{n^{s_1}}$ is greater than $\displaystyle \sum_{n=2}^{\infty} \frac{1}{n (\log n)^s}$ Doesn't this mean that in some sense $\log n$ grows faster than a power of $n$? (or) How should I make sense of (or) interpret this result? (I am assuming that my convergence and divergence conclusions are right).",,"['real-analysis', 'sequences-and-series']"
74,Finding $\int_{0}^{\pi/2} \frac{\tan x}{1+m^2\tan^2{x}} \mathrm{d}x$,Finding,\int_{0}^{\pi/2} \frac{\tan x}{1+m^2\tan^2{x}} \mathrm{d}x,"How do we prove that $$I(m)=\int_{0}^{\pi/2} \frac{\tan x}{1+m^2\tan^2{x}} \mathrm{d}x=\frac{\log{m}}{m^2-1}$$ I see that $$I(m)=\frac{\partial}{\partial m} \int_{0}^{\pi/2} \arctan({m\tan x}) \ \mathrm{d}x$$ But I don't see how to use this fact. Can we? Please help me out, and if possible please post a solution using differentiation under the integral sign.","How do we prove that $$I(m)=\int_{0}^{\pi/2} \frac{\tan x}{1+m^2\tan^2{x}} \mathrm{d}x=\frac{\log{m}}{m^2-1}$$ I see that $$I(m)=\frac{\partial}{\partial m} \int_{0}^{\pi/2} \arctan({m\tan x}) \ \mathrm{d}x$$ But I don't see how to use this fact. Can we? Please help me out, and if possible please post a solution using differentiation under the integral sign.",,"['calculus', 'real-analysis', 'integration', 'definite-integrals', 'improper-integrals']"
75,Does this series involving sine converge or diverge: $\sum\limits_{k=1}^\infty \frac{1}{k}\cdot \sin\frac{(-1)^k}{1+k^2}$?,Does this series involving sine converge or diverge: ?,\sum\limits_{k=1}^\infty \frac{1}{k}\cdot \sin\frac{(-1)^k}{1+k^2},I have been trying to show the convergence of this series but I can't seem to find a way to do it. $$\sum_{k=1}^\infty \frac{1}{k}\cdot \sin\frac{(-1)^k}{1+k^2}$$,I have been trying to show the convergence of this series but I can't seem to find a way to do it. $$\sum_{k=1}^\infty \frac{1}{k}\cdot \sin\frac{(-1)^k}{1+k^2}$$,,"['real-analysis', 'sequences-and-series', 'limits']"
76,Why the $\nabla f(x)$ in the direction orthogonal to $f(x)$?,Why the  in the direction orthogonal to ?,\nabla f(x) f(x),Why the $\nabla f$ in the direction orthogonal to $f$? I can't understand it intuitively. Is it because of the definition of gradient?,Why the $\nabla f$ in the direction orthogonal to $f$? I can't understand it intuitively. Is it because of the definition of gradient?,,"['calculus', 'real-analysis', 'derivatives', 'intuition']"
77,An exact definition of multiplication,An exact definition of multiplication,,"I am looking into repeated operations, and it seems really hard to precisely define multiplication. Of course, for integer $b$ and real number $a$ , we use the grade school definition we all know: $$ab = \underbrace{a + a + a + \cdots + a}_{b\text{ times}}$$ but what about for real numbers $a$ and $b$ ? For exponentiating (for integers: repeated multiplication), we have a precise formula to define it, which is easy to derive: $$a^x = \sum_{n=0}^{\infty} \frac{x^n \left(\ln(a)\right)^n}{n!}$$ which is nice because we only have integer powers in the sum, which we already know how to define: $$x^n = \underbrace{x \times x\times x \times \cdots \times x}_{n\text{ times}}$$ But this just raises the question of how we define $x \times x$ precisely. Is there an analogous formula to this for multiplication? How does the calculator compute multiplication of reals? Note: According to sources, just approximating multiplication for real numbers uses calculus or numerical methods. I cannot grasp why we need these advanced concepts to precisely define this fundamental operation, especially when comparing it to the simple formula for exponentiation. But I still don’t have a formula yet.","I am looking into repeated operations, and it seems really hard to precisely define multiplication. Of course, for integer and real number , we use the grade school definition we all know: but what about for real numbers and ? For exponentiating (for integers: repeated multiplication), we have a precise formula to define it, which is easy to derive: which is nice because we only have integer powers in the sum, which we already know how to define: But this just raises the question of how we define precisely. Is there an analogous formula to this for multiplication? How does the calculator compute multiplication of reals? Note: According to sources, just approximating multiplication for real numbers uses calculus or numerical methods. I cannot grasp why we need these advanced concepts to precisely define this fundamental operation, especially when comparing it to the simple formula for exponentiation. But I still don’t have a formula yet.",b a ab = \underbrace{a + a + a + \cdots + a}_{b\text{ times}} a b a^x = \sum_{n=0}^{\infty} \frac{x^n \left(\ln(a)\right)^n}{n!} x^n = \underbrace{x \times x\times x \times \cdots \times x}_{n\text{ times}} x \times x,"['real-analysis', 'definition', 'arithmetic', 'real-numbers']"
78,A mathematical fallacy concerning the integrability of a function and cancellation,A mathematical fallacy concerning the integrability of a function and cancellation,,"I am reading the Florida Mu Alpha Theta Sponsors Guide . Page 43 is a list of clarifications and disputes commonly made, and their resolutions. One of their clarifications is this: A function which is not integrable on an interval A is not integrable on any interval B, where B contains A. I.e. no “the negative signs cancel” arguments. What is this argument they're referring to?","I am reading the Florida Mu Alpha Theta Sponsors Guide . Page 43 is a list of clarifications and disputes commonly made, and their resolutions. One of their clarifications is this: A function which is not integrable on an interval A is not integrable on any interval B, where B contains A. I.e. no “the negative signs cancel” arguments. What is this argument they're referring to?",,"['real-analysis', 'integration', 'improper-integrals']"
79,Calculate the limit $\lim \limits_{n \to \infty} |\sin(\pi \sqrt{n^2+n+1})|$,Calculate the limit,\lim \limits_{n \to \infty} |\sin(\pi \sqrt{n^2+n+1})|,Calculate $$\lim \limits_{n \to \infty} |\sin(\pi \sqrt{n^2+n+1})|$$,Calculate $$\lim \limits_{n \to \infty} |\sin(\pi \sqrt{n^2+n+1})|$$,,"['real-analysis', 'limits', 'trigonometry', 'radicals']"
80,Uniform Continuity of $x \sin x$,Uniform Continuity of,x \sin x,How does one go about proving that the function $x \sin x$ is not uniformly continuous on the set of real numbers ? Any method that uses the sequential criterion for discontinuity would be preferred. Thanks in advance !,How does one go about proving that the function $x \sin x$ is not uniformly continuous on the set of real numbers ? Any method that uses the sequential criterion for discontinuity would be preferred. Thanks in advance !,,"['real-analysis', 'uniform-continuity']"
81,"$\int_{0}^{x} f(t) dt \ge {f(x)}$ holds or not on $[0,1]$",holds or not on,"\int_{0}^{x} f(t) dt \ge {f(x)} [0,1]","Let $$f\colon [0,1]\rightarrow [0, \infty )$$  be  continuous. Suppose that $$\int_{0}^{x} f(t)\,\mathrm dt \ge {f(x)} \quad\text{for  all }x\in[0,1].$$ Then $A.$ No  such  function  exists. $B.$ There  are  infinitely  many  such  functions. $C.$ There is  only  one  such  function $D.$ There  are  exactly  two  functions. Now  I  was  thinking,  since $\int_{0}^{x} f(t) dt$  is  the  area  under  the  graph  of  $f(t)$  from $0$  to  $x$ so it  can  be  easily  made  greater  than  the  value  of  $f(t)$  at  one  point, namely  $x$.  So  there   will  be  infinitely  many  such  functions  satisfying  this  condition. Is  my  reasoning  correct or  not? Thanks.","Let $$f\colon [0,1]\rightarrow [0, \infty )$$  be  continuous. Suppose that $$\int_{0}^{x} f(t)\,\mathrm dt \ge {f(x)} \quad\text{for  all }x\in[0,1].$$ Then $A.$ No  such  function  exists. $B.$ There  are  infinitely  many  such  functions. $C.$ There is  only  one  such  function $D.$ There  are  exactly  two  functions. Now  I  was  thinking,  since $\int_{0}^{x} f(t) dt$  is  the  area  under  the  graph  of  $f(t)$  from $0$  to  $x$ so it  can  be  easily  made  greater  than  the  value  of  $f(t)$  at  one  point, namely  $x$.  So  there   will  be  infinitely  many  such  functions  satisfying  this  condition. Is  my  reasoning  correct or  not? Thanks.",,"['real-analysis', 'integration']"
82,A step function is right continuous with left limits,A step function is right continuous with left limits,,"What does it mean that the characteristic function $f(x)=1_{[b \le x \lt \infty]}$ is right continuous with left limits? Here $x ,b \in \mathbb{R}$ .",What does it mean that the characteristic function is right continuous with left limits? Here .,"f(x)=1_{[b \le x \lt \infty]} x ,b \in \mathbb{R}",['real-analysis']
83,Evaluating the series $\sum\limits_{n=1}^\infty \frac1{4n^2+2n}$,Evaluating the series,\sum\limits_{n=1}^\infty \frac1{4n^2+2n},"How do we evaluate the following series: $$\sum_{n=1}^\infty \frac1{4n^2+2n}$$ I know that it converges by the comparison test. Wolfram Alpha gives the answer $1 - \ln(2)$, but I cannot see how to get it. The Taylor series of logarithm is nowhere near this series.","How do we evaluate the following series: $$\sum_{n=1}^\infty \frac1{4n^2+2n}$$ I know that it converges by the comparison test. Wolfram Alpha gives the answer $1 - \ln(2)$, but I cannot see how to get it. The Taylor series of logarithm is nowhere near this series.",,"['calculus', 'real-analysis', 'sequences-and-series']"
84,"Is there a shorthand or symbolic notation for ""differentiable"" or ""continuous""?","Is there a shorthand or symbolic notation for ""differentiable"" or ""continuous""?",,"In basic calculus an analysis we end up writing the words ""continuous"" and ""differentiable"" nearly as often as we use the term ""function"", yet, while there are plenty of convenient (and even fairly precise) shorthands for representing the latter, I'm not aware of a way to concisely represent the former. Are there commonly used symbols or shorthands to represent ""continuous"" or ""differentiable""?","In basic calculus an analysis we end up writing the words ""continuous"" and ""differentiable"" nearly as often as we use the term ""function"", yet, while there are plenty of convenient (and even fairly precise) shorthands for representing the latter, I'm not aware of a way to concisely represent the former. Are there commonly used symbols or shorthands to represent ""continuous"" or ""differentiable""?",,"['calculus', 'real-analysis', 'functions', 'notation']"
85,Show that $\lim\limits_{n \to +\infty}(\sin(\frac{1}{n^2})+\sin(\frac{2}{n^2})+\cdots+\sin(\frac{n}{n^2})) = \frac{1}{2}$,Show that,\lim\limits_{n \to +\infty}(\sin(\frac{1}{n^2})+\sin(\frac{2}{n^2})+\cdots+\sin(\frac{n}{n^2})) = \frac{1}{2},"Show that the sequence defined as $$x_n = \sin\left(\frac{1}{n^2}\right)+\sin\left(\frac{2}{n^2}\right)+\cdots+\sin\left(\frac{n}{n^2}\right)$$ converges to $\frac{1}{2}$ . My attempt was to evaluate this limit by using squeeze theorem. I managed to show that $x_n < \frac{n+1}{2n}$ by using $\sin(x) < x$ , but I haven't been able to find a sequence smaller than $x_n$ that also converges to $\frac{1}{2}$ . I tried showing by induction that $x_n > \frac{1}{2}-\frac{1}{n}$ , but I got nowhere with that. Any help would be appreciated.","Show that the sequence defined as converges to . My attempt was to evaluate this limit by using squeeze theorem. I managed to show that by using , but I haven't been able to find a sequence smaller than that also converges to . I tried showing by induction that , but I got nowhere with that. Any help would be appreciated.",x_n = \sin\left(\frac{1}{n^2}\right)+\sin\left(\frac{2}{n^2}\right)+\cdots+\sin\left(\frac{n}{n^2}\right) \frac{1}{2} x_n < \frac{n+1}{2n} \sin(x) < x x_n \frac{1}{2} x_n > \frac{1}{2}-\frac{1}{n},"['real-analysis', 'sequences-and-series', 'limits']"
86,Proving that $\sqrt{1 + x^2}$ is not a polynomial function,Proving that  is not a polynomial function,\sqrt{1 + x^2},"The given task goes as follows: Show that $ f: \mathbb{R} \longrightarrow \mathbb{R}$ defined by $f(x) = \sqrt{1 + x^2} $ is not a polynomial function. I tried this approach - if $f(x)$ is a $n$ -degree polynomial function, then the $(n+1)$ -st derivative equals to 0 and I was trying to determine the $k$ -th derivative of $f(x)$ (and show it differs from 0 for any $k$ ) but without success. Since $f(x)$ is continuous and defined over whole R domain, I have no idea how to carry on. Any ideas?","The given task goes as follows: Show that defined by is not a polynomial function. I tried this approach - if is a -degree polynomial function, then the -st derivative equals to 0 and I was trying to determine the -th derivative of (and show it differs from 0 for any ) but without success. Since is continuous and defined over whole R domain, I have no idea how to carry on. Any ideas?", f: \mathbb{R} \longrightarrow \mathbb{R} f(x) = \sqrt{1 + x^2}  f(x) n (n+1) k f(x) k f(x),"['real-analysis', 'algebra-precalculus', 'polynomials']"
87,Is $ \sin: \mathbb{N} \to \mathbb{R}$ injective?,Is  injective?, \sin: \mathbb{N} \to \mathbb{R},"I was trying to show that $\sin(x)$ is non-zero for integers $x$ other than zero and I thought that this result might emerge as a corollary if I managed to show that the result in question is true. I think it's possible to demonstrate this by looking at the power series expansion of $\sin(x)$ and assuming that we don't know anything about the existence of $\pi$. All of the answers below insist that the proposition '$\exists p,q \in \mathbb{Z}, \sin(p)=\sin(q)$' -where $\sin(x)$ is the power series representation-is undecidable without using the properties of $\pi$. If so this is a truly wonderful conjecture and I would like to be provided with a proof. Until then, I insist that methods for analyzing infinite series from analysis should suffice to show that the proposition is false. Note: In a previous version of this post the question said ""bijective"" instead of ""injective"". Some of the answers below have answered the first version of this post.","I was trying to show that $\sin(x)$ is non-zero for integers $x$ other than zero and I thought that this result might emerge as a corollary if I managed to show that the result in question is true. I think it's possible to demonstrate this by looking at the power series expansion of $\sin(x)$ and assuming that we don't know anything about the existence of $\pi$. All of the answers below insist that the proposition '$\exists p,q \in \mathbb{Z}, \sin(p)=\sin(q)$' -where $\sin(x)$ is the power series representation-is undecidable without using the properties of $\pi$. If so this is a truly wonderful conjecture and I would like to be provided with a proof. Until then, I insist that methods for analyzing infinite series from analysis should suffice to show that the proposition is false. Note: In a previous version of this post the question said ""bijective"" instead of ""injective"". Some of the answers below have answered the first version of this post.",,"['real-analysis', 'number-theory']"
88,Finding the limit of $a_{n} = \frac{n!}{\left(\frac{2}{7} + 1\right)\left(\frac{2}{7} + 2\right)\ldots\left(\frac{2}{7} + n\right)}$,Finding the limit of,a_{n} = \frac{n!}{\left(\frac{2}{7} + 1\right)\left(\frac{2}{7} + 2\right)\ldots\left(\frac{2}{7} + n\right)},"My task was to prove that the limit in the title exists and to calculate it. First I showed that the sequence is monotonically decreasing $\left(\frac{a_{n+1}}{a_{n}} = \frac{n+1}{\frac{2}{7} + n + 1} < 1\right)$ and it's obvious that $a_{n} > 0$ for all $n$ . Therefore, by the Monotone Convergence theorem $(a_n)_{n \geq 1}$ must converge. My first idea was to use the relation I obtained earlier between $a_{n+1}$ and $a_{n}$ : $a_{n+1} = \frac{n+1}{\frac{2}{7} + n + 1} \cdot a_{n}$ and passing the limit on both sides, but I arrive at the trivial equality that $\lim_{n \to \infty} a_{n+1} = \lim_{n \to \infty} a_{n}$ . My second idea was to rationalize the denominator to see if a more familiar expression will arise, here is what I arrived at: $$a_{n} = \frac{7^n \cdot n!}{(2+7 \cdot 1)(2 + 7 \cdot 2)...(2 + 7 \cdot n)}$$ which didn't help. If it is of any help, the first exercise was to study the convergence of: $$\sum_{n=1}^{\infty} \frac{n!}{(2x+1)...(2x+n)} \text{ for } x \in (0, +\infty)$$ I determined that it converges when $x \in \left(\frac{1}{2}, \infty\right)$ by Raabe-Duhammel and diverges otherwise.","My task was to prove that the limit in the title exists and to calculate it. First I showed that the sequence is monotonically decreasing and it's obvious that for all . Therefore, by the Monotone Convergence theorem must converge. My first idea was to use the relation I obtained earlier between and : and passing the limit on both sides, but I arrive at the trivial equality that . My second idea was to rationalize the denominator to see if a more familiar expression will arise, here is what I arrived at: which didn't help. If it is of any help, the first exercise was to study the convergence of: I determined that it converges when by Raabe-Duhammel and diverges otherwise.","\left(\frac{a_{n+1}}{a_{n}} = \frac{n+1}{\frac{2}{7} + n + 1} < 1\right) a_{n} > 0 n (a_n)_{n \geq 1} a_{n+1} a_{n} a_{n+1} = \frac{n+1}{\frac{2}{7} + n + 1} \cdot a_{n} \lim_{n \to \infty} a_{n+1} = \lim_{n \to \infty} a_{n} a_{n} = \frac{7^n \cdot n!}{(2+7 \cdot 1)(2 + 7 \cdot 2)...(2 + 7 \cdot n)} \sum_{n=1}^{\infty} \frac{n!}{(2x+1)...(2x+n)} \text{ for } x \in (0, +\infty) x \in \left(\frac{1}{2}, \infty\right)","['real-analysis', 'sequences-and-series', 'limits', 'convergence-divergence']"
89,Proving that the second derivative of a convex function is nonnegative,Proving that the second derivative of a convex function is nonnegative,,"My task is as follows: Let $f:\mathbb{R}\to\mathbb{R}$ be a twice-differentiable function,   and let $f$'s second derivative be continuous. Let $f$ be convex with   the following definition of convexity: for any $a<b \in \mathbb{R}$:   $$f\left(\frac{a+b}{2}\right) \leq \frac{f(a)+f(b)}{2}$$ Prove that   $f'' \geq 0$ everywhere. I've thought of trying to show that there exists a $c$ in every $[a,b] \subset \mathbb{R}$ such that $f''(c) \geq 0$, and then just generalizing that, but I haven't been able to actually do it -- I don't know how to approach this. I'm thinking that I should use the mean-value theorem. I've also thought about picking $a < v < w < b$ and then using the MVT on $[a,v]$ and $[w,b]$ to identify points in these intervals and then to take the second derivative between them, and showing that it's nonnegative. However I'm really having trouble even formalizing any of these thoughts: I can't even get to a any statements about $f'$. I've looked at a few proofs of similar statements, but they used different definitions of convexity, and I haven't really been able to bend them to my situation. I'd appreciate any help/hints/sketches of proofs or directions.","My task is as follows: Let $f:\mathbb{R}\to\mathbb{R}$ be a twice-differentiable function,   and let $f$'s second derivative be continuous. Let $f$ be convex with   the following definition of convexity: for any $a<b \in \mathbb{R}$:   $$f\left(\frac{a+b}{2}\right) \leq \frac{f(a)+f(b)}{2}$$ Prove that   $f'' \geq 0$ everywhere. I've thought of trying to show that there exists a $c$ in every $[a,b] \subset \mathbb{R}$ such that $f''(c) \geq 0$, and then just generalizing that, but I haven't been able to actually do it -- I don't know how to approach this. I'm thinking that I should use the mean-value theorem. I've also thought about picking $a < v < w < b$ and then using the MVT on $[a,v]$ and $[w,b]$ to identify points in these intervals and then to take the second derivative between them, and showing that it's nonnegative. However I'm really having trouble even formalizing any of these thoughts: I can't even get to a any statements about $f'$. I've looked at a few proofs of similar statements, but they used different definitions of convexity, and I haven't really been able to bend them to my situation. I'd appreciate any help/hints/sketches of proofs or directions.",,"['calculus', 'real-analysis', 'derivatives', 'convex-analysis']"
90,Showing that the closure of a totally bounded set is totally bounded,Showing that the closure of a totally bounded set is totally bounded,,"I would like to show that if I have a subset $M$ of a metric space $(X,d)$ such that $M$ is totally bounded, then its closure $cl(M)$ is also totally bounded. My general strategy would be to show that for every $\epsilon > 0$ , there exist $x_1,...,x_n \in M$ such that $M = \bigcup B_{\epsilon/2}(x_i)$ . And then I would  use the fact that this works also for $\epsilon$ since it works for $\epsilon /2$ . However, should I be having two cases? I am listing the cases as follows: $cl(M) = M;$ $cl(M) \neq M$ .","I would like to show that if I have a subset of a metric space such that is totally bounded, then its closure is also totally bounded. My general strategy would be to show that for every , there exist such that . And then I would  use the fact that this works also for since it works for . However, should I be having two cases? I am listing the cases as follows: .","M (X,d) M cl(M) \epsilon > 0 x_1,...,x_n \in M M = \bigcup B_{\epsilon/2}(x_i) \epsilon \epsilon /2 cl(M) = M; cl(M) \neq M","['real-analysis', 'general-topology', 'complex-analysis']"
91,The dimension of the real continuous functions as a vector space over $\mathbb{R}$ is not countable?,The dimension of the real continuous functions as a vector space over  is not countable?,\mathbb{R},"This question is out of curiosity. I first attempted a web crawl for this answer but was befuddled when Google didn't turn up the result after a couple of tries. If anyone has a reference, I'd be appreciative. The ultimate thing I want to know is whether or not $C(\mathbb{R},\mathbb{R})$ has countably-infinite dimension over $\mathbb{R}$. However, I have an inkling that $C([a,b],\mathbb{R})$ itself does not have countably-infinite dimension. I attempted to show that if $\{f_n\}_{n=1}^\infty$ is a purported basis for $C([a,b])$ then $\sum_{n=1}^\infty 2^{-n}f_n$ is not in their span, but I quickly found out that I don't know how quickly any of these $f_n$ increase. I would like a constructive proof if one can be given; however, I would begrudgingly accept an existence proof. I don't mind if you feel the need to add a metric or topologize the space in any way.","This question is out of curiosity. I first attempted a web crawl for this answer but was befuddled when Google didn't turn up the result after a couple of tries. If anyone has a reference, I'd be appreciative. The ultimate thing I want to know is whether or not $C(\mathbb{R},\mathbb{R})$ has countably-infinite dimension over $\mathbb{R}$. However, I have an inkling that $C([a,b],\mathbb{R})$ itself does not have countably-infinite dimension. I attempted to show that if $\{f_n\}_{n=1}^\infty$ is a purported basis for $C([a,b])$ then $\sum_{n=1}^\infty 2^{-n}f_n$ is not in their span, but I quickly found out that I don't know how quickly any of these $f_n$ increase. I would like a constructive proof if one can be given; however, I would begrudgingly accept an existence proof. I don't mind if you feel the need to add a metric or topologize the space in any way.",,"['real-analysis', 'general-topology']"
92,Is $\sum (n^{\frac{1}{n}}-1)^n$ divergent or convergent?,Is  divergent or convergent?,\sum (n^{\frac{1}{n}}-1)^n,"Is $\sum (n^{\frac{1}{n}}-1)^n$ divergent or convergent? I tried to follow the idea of $k^{th}$ partial sum from another question that I asked: Verify if $\sum(\sqrt{n+1}-\sqrt{n})$ is convergent or divergent $$S_k=\sum_{n=1}^k (n^\frac{1}{n}-1)^n=(1-1)+(2^\frac{1}{2}-1)^2+\dots+(k^\frac{1}{k}-1)^k$$ I know that $k^\frac{1}{k}\rightarrow 1$ when $k\rightarrow \infty$ and then $(k^\frac{1}{k}-1)^k\rightarrow 0$. So I have an infinite sum of small terms, what makes me think that this series is divergent. Since a sum of infinite terms bigger than $0$ is infinite $$\lim S_k= \infty$$","Is $\sum (n^{\frac{1}{n}}-1)^n$ divergent or convergent? I tried to follow the idea of $k^{th}$ partial sum from another question that I asked: Verify if $\sum(\sqrt{n+1}-\sqrt{n})$ is convergent or divergent $$S_k=\sum_{n=1}^k (n^\frac{1}{n}-1)^n=(1-1)+(2^\frac{1}{2}-1)^2+\dots+(k^\frac{1}{k}-1)^k$$ I know that $k^\frac{1}{k}\rightarrow 1$ when $k\rightarrow \infty$ and then $(k^\frac{1}{k}-1)^k\rightarrow 0$. So I have an infinite sum of small terms, what makes me think that this series is divergent. Since a sum of infinite terms bigger than $0$ is infinite $$\lim S_k= \infty$$",,"['real-analysis', 'sequences-and-series', 'convergence-divergence']"
93,Why is/isn't the derivative of a differentiable function continuous?,Why is/isn't the derivative of a differentiable function continuous?,,"I am confused about the following Theorem: Let $f: I \to \mathbb{R}^n$ , $a \in I$ . Then the function $f$ is differentiable at $a$ if and only if there exists a function $\varphi: I \to \mathbb{R}^n$ that is continuous in $a$ , and such that $f(x) - f(a) = (x - a)\varphi(x)$ , for all $x \in I$ ; furthermore, $\varphi(a) = f'(a)$ . I understand the proof of this theorem, but something confuses me. Doesn't this theorem state that the derivative of a function in a point is always continuous in that point, since $f'(a) = \varphi(a)$ is continuous in $a$ ? This would mean that the derivative of a function is always continuous on the domain of the function, but I have encountered counterexamples. I have probably misinterpreted something; any help would be welcome.","I am confused about the following Theorem: Let , . Then the function is differentiable at if and only if there exists a function that is continuous in , and such that , for all ; furthermore, . I understand the proof of this theorem, but something confuses me. Doesn't this theorem state that the derivative of a function in a point is always continuous in that point, since is continuous in ? This would mean that the derivative of a function is always continuous on the domain of the function, but I have encountered counterexamples. I have probably misinterpreted something; any help would be welcome.",f: I \to \mathbb{R}^n a \in I f a \varphi: I \to \mathbb{R}^n a f(x) - f(a) = (x - a)\varphi(x) x \in I \varphi(a) = f'(a) f'(a) = \varphi(a) a,"['real-analysis', 'analysis', 'derivatives', 'continuity']"
94,"Why the radius of convergence and not ""areas of convergence"" for power series?","Why the radius of convergence and not ""areas of convergence"" for power series?",,"My calculus is quite rusty and I'm trying to rebuild it on an intuitive basis. Currently, I am looking at power series and have trouble understanding the radius of convergence. I am comfortable with the fact that if the limit of the function at a certain point $a$ doesn't exist, the power series won't converge for this value of $x$. But why can't the series converge for $x$ whose absolute value is greater than this point? Why aren't there ""areas of convergence"" instead of a single radius of convergence?","My calculus is quite rusty and I'm trying to rebuild it on an intuitive basis. Currently, I am looking at power series and have trouble understanding the radius of convergence. I am comfortable with the fact that if the limit of the function at a certain point $a$ doesn't exist, the power series won't converge for this value of $x$. But why can't the series converge for $x$ whose absolute value is greater than this point? Why aren't there ""areas of convergence"" instead of a single radius of convergence?",,"['calculus', 'real-analysis', 'power-series']"
95,Must the (continuous) image of a null set be null?,Must the (continuous) image of a null set be null?,,"Say $E \subset [0,1]$ is a null set. Let $f: [0,1] \rightarrow [0,1] $. Do you think $f(E)$ is a null set or not? Just being curious. (DEF): A set $A$ is null if given any $\epsilon > 0$, there exists a sequence of intervals $\{I_n\}_{n\geq1}$ such that $$ A \subseteq \bigcup _{n=1}^{\infty}I_n$$ and $$ \sum |I_n| < \epsilon $$ if $f$ is continuous, is $f(E)$ nullset or not?","Say $E \subset [0,1]$ is a null set. Let $f: [0,1] \rightarrow [0,1] $. Do you think $f(E)$ is a null set or not? Just being curious. (DEF): A set $A$ is null if given any $\epsilon > 0$, there exists a sequence of intervals $\{I_n\}_{n\geq1}$ such that $$ A \subseteq \bigcup _{n=1}^{\infty}I_n$$ and $$ \sum |I_n| < \epsilon $$ if $f$ is continuous, is $f(E)$ nullset or not?",,"['real-analysis', 'analysis', 'measure-theory']"
96,Why does zero derivative imply a function is locally constant?,Why does zero derivative imply a function is locally constant?,,"I've been trying to prove to myself that if $\Omega$ is an open connected set in $\mathbb{R}^n$, then if $f\colon\Omega\to\mathbb{R}^m$ is a differentiable function such that $f'(x)=0$ for all $x\in\Omega$, then $f$ is constant. I've reduced the problem to just showing $f$ is locally constant on $\Omega$. Given $x_0\in\Omega$, I know that  $$ \lim_{x\to x_0}\frac{\|f(x)-f(x_0)-f'(x_0)(x-x_0)\|}{\|x-x_0\|}=\lim_{x\to x_0}\frac{\|f(x)-f(x_0)\|}{\|x-x_0\|}=0. $$ This implies $\lim_{x\to x_0}\|f(x)-f(x_0)\|=0$. So for any $\epsilon>0$, there exists some open ball $B(x_0,\delta)$ around $x_0$ in $\Omega$ such that $\|f(x)-f(x_0)\|<\epsilon$ for $x\in B(x_0,\delta)$. But since the choice of the open ball changes with $\epsilon$, I don't think I can conclude $\|f(x)-f(x_0)\|=0$ for $x\in B$. So this doesn't convince me that there actually exists a neighborhood of $x_0$ on which $f$ is constant. How can you make the leap from zero derivative to locally constant?","I've been trying to prove to myself that if $\Omega$ is an open connected set in $\mathbb{R}^n$, then if $f\colon\Omega\to\mathbb{R}^m$ is a differentiable function such that $f'(x)=0$ for all $x\in\Omega$, then $f$ is constant. I've reduced the problem to just showing $f$ is locally constant on $\Omega$. Given $x_0\in\Omega$, I know that  $$ \lim_{x\to x_0}\frac{\|f(x)-f(x_0)-f'(x_0)(x-x_0)\|}{\|x-x_0\|}=\lim_{x\to x_0}\frac{\|f(x)-f(x_0)\|}{\|x-x_0\|}=0. $$ This implies $\lim_{x\to x_0}\|f(x)-f(x_0)\|=0$. So for any $\epsilon>0$, there exists some open ball $B(x_0,\delta)$ around $x_0$ in $\Omega$ such that $\|f(x)-f(x_0)\|<\epsilon$ for $x\in B(x_0,\delta)$. But since the choice of the open ball changes with $\epsilon$, I don't think I can conclude $\|f(x)-f(x_0)\|=0$ for $x\in B$. So this doesn't convince me that there actually exists a neighborhood of $x_0$ on which $f$ is constant. How can you make the leap from zero derivative to locally constant?",,"['real-analysis', 'general-topology', 'derivatives', 'connectedness']"
97,The series $ \sum\limits_{k=1}^{\infty} \frac1{\sqrt{{k}{(k^2+1)}}}$,The series, \sum\limits_{k=1}^{\infty} \frac1{\sqrt{{k}{(k^2+1)}}},Given the series $$\sum_{k=1}^{\infty} \frac1{\sqrt{{k}{(k^2+1)}}}$$ How can I calculate its exact limit (if that is possible)?,Given the series $$\sum_{k=1}^{\infty} \frac1{\sqrt{{k}{(k^2+1)}}}$$ How can I calculate its exact limit (if that is possible)?,,"['real-analysis', 'sequences-and-series']"
98,Having trouble understanding condition for Rolle's Theorem (russian translation),Having trouble understanding condition for Rolle's Theorem (russian translation),,"I understand the conditions for Rolle's theorem in english : a function $f$ has to be continous on $[a,b]$ , differentiable on $(a,b)$ , and $f(a)=f(b)$ . But I'm studying in Russian, and they write the conditions like this: a) function $f$ is continous on $[a,b]$ , b) function $f$ has at all points of the interval $(a,b)$ a finite or definite sign infinite derivative, c) $f(a)=f(b)$ . I'm having trouble understanding point b), what does 'definite sign infinite derivative' mean? Is there a difference between that, and simply being differentiable on $(a,b)$ ? Maybe the translation isn't quite right, my russian isn't perfect, but I think that's what it means in English. All I could think of was, maybe they are refering to when the limit of $f(x)$ when $x \to a$ or $x \to b$ is equal to $\pm\infty$ , but then $f$ wouldn't be continous on $[a,b]$ . So, I'm lost. Do you guys have any ideas? Thanks a lot in advance.","I understand the conditions for Rolle's theorem in english : a function has to be continous on , differentiable on , and . But I'm studying in Russian, and they write the conditions like this: a) function is continous on , b) function has at all points of the interval a finite or definite sign infinite derivative, c) . I'm having trouble understanding point b), what does 'definite sign infinite derivative' mean? Is there a difference between that, and simply being differentiable on ? Maybe the translation isn't quite right, my russian isn't perfect, but I think that's what it means in English. All I could think of was, maybe they are refering to when the limit of when or is equal to , but then wouldn't be continous on . So, I'm lost. Do you guys have any ideas? Thanks a lot in advance.","f [a,b] (a,b) f(a)=f(b) f [a,b] f (a,b) f(a)=f(b) (a,b) f(x) x \to a x \to b \pm\infty f [a,b]","['real-analysis', 'calculus']"
99,Triangle inequality for infinite number of terms,Triangle inequality for infinite number of terms,,We can prove that for any $n\in \mathbb{N}$ we have triangle inequality: $$|x_1+x_2+\cdots+x_n|\leqslant |x_1|+|x_2|+\cdots+|x_n|.$$ How to prove it for series i.e. $$\left|\sum \limits_{n=1}^{\infty}a_n\right|\leqslant \sum \limits_{n=1}^{\infty}|a_n|.$$ Can anyone help to me with this?,We can prove that for any $n\in \mathbb{N}$ we have triangle inequality: $$|x_1+x_2+\cdots+x_n|\leqslant |x_1|+|x_2|+\cdots+|x_n|.$$ How to prove it for series i.e. $$\left|\sum \limits_{n=1}^{\infty}a_n\right|\leqslant \sum \limits_{n=1}^{\infty}|a_n|.$$ Can anyone help to me with this?,,['real-analysis']
