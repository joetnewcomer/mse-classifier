,title_raw,title_text,title_latex,body_raw,body_text,body_latex,tags
0,Probability of a probability to happen,Probability of a probability to happen,,"I'm trying to solve this problem but I'm struggling to. A university has 300 students enrolled. You sample 30 students and see that 20 are boys and 10 are girls. If the university had 150 male students and 150 females, what would be the probability of your observation? More generally,  if there are N students enrolled in the university, B of which are boys and N − B girls, what is the probability that if you sample n (≤ N), then exactly b are boys and n − b are girls? So from the sample we know that there are (approximating) 66% boys and 33% girls. But I don't know how to estimate the probability of this observation, what am I missing?","I'm trying to solve this problem but I'm struggling to. A university has 300 students enrolled. You sample 30 students and see that 20 are boys and 10 are girls. If the university had 150 male students and 150 females, what would be the probability of your observation? More generally,  if there are N students enrolled in the university, B of which are boys and N − B girls, what is the probability that if you sample n (≤ N), then exactly b are boys and n − b are girls? So from the sample we know that there are (approximating) 66% boys and 33% girls. But I don't know how to estimate the probability of this observation, what am I missing?",,"['probability', 'statistics']"
1,Random walk with 3 cases.,Random walk with 3 cases.,,"Let the walking start be at $x=0$. With probability $p_1$ new $x=x+1$, with probability $p_2$: $x=x-1$ and with probability $1-p_1-p_2 \geq 0$ walking ends. The question is what is the probability of ending on point $n$. I've computed it numerically and getting something close to normal distribution with peak at zero, but with different dispersion on sides of $x=0$. How to get it analytically? The provided answer seems ok, but I hope I can find some smooth function for it. Maybe with another type of solution. Upd: my idea of solution. $$P_{n+1} (x) = p_1 P_{n} (x-\delta x) + p_2 P_{n} (x + \delta x)$$ $$P_{n+1} (x) - P_{n} (x) = p_1 [P_n(x-\delta x) - P_n(x)] + p_2 [P_n(x+\delta x) - P_n(x)] - q\cdot P_{n}(x)$$ Dividing by $\delta n$ and $\delta x$ getting something like: $$\frac{\partial P(n,x)}{\partial n}= \frac{\delta x}{\delta n}\left[ (p_2 + p_1) \frac{\partial P(n,x)}{\partial x} - (1 - p_2 - p_1)\cdot P(n,x)\right]$$ Solving it leads to something like this: $$P(n,x) = Ae^{\frac{-q x}{p_1 + p_2}}(n + \frac{x}{p_1 + p_2})$$ I'm not sure yet  if it means something.","Let the walking start be at $x=0$. With probability $p_1$ new $x=x+1$, with probability $p_2$: $x=x-1$ and with probability $1-p_1-p_2 \geq 0$ walking ends. The question is what is the probability of ending on point $n$. I've computed it numerically and getting something close to normal distribution with peak at zero, but with different dispersion on sides of $x=0$. How to get it analytically? The provided answer seems ok, but I hope I can find some smooth function for it. Maybe with another type of solution. Upd: my idea of solution. $$P_{n+1} (x) = p_1 P_{n} (x-\delta x) + p_2 P_{n} (x + \delta x)$$ $$P_{n+1} (x) - P_{n} (x) = p_1 [P_n(x-\delta x) - P_n(x)] + p_2 [P_n(x+\delta x) - P_n(x)] - q\cdot P_{n}(x)$$ Dividing by $\delta n$ and $\delta x$ getting something like: $$\frac{\partial P(n,x)}{\partial n}= \frac{\delta x}{\delta n}\left[ (p_2 + p_1) \frac{\partial P(n,x)}{\partial x} - (1 - p_2 - p_1)\cdot P(n,x)\right]$$ Solving it leads to something like this: $$P(n,x) = Ae^{\frac{-q x}{p_1 + p_2}}(n + \frac{x}{p_1 + p_2})$$ I'm not sure yet  if it means something.",,"['probability', 'random-walk']"
2,Series of independent Bernoulli variables,Series of independent Bernoulli variables,,"Let $X_1, X_2, \ldots$ be independent, identically distributed random variables with distribution $\text{Ber}(\frac{1}{2})$ . Define the random varible: $$Y:=\sum_{n=1}^\infty \frac{X_n}{2^n}$$ Prove that $Y$ is uniformly distributed over the unit interval $[0,1]$ . The only way I currently know how to attack this problem is by using Levy's theorem: For $Y_n:=\sum_{k=1}^n \frac{X_k}{2^k}$ , it's enough to prove that the characteristic function of $Y_n$ converges to the characteristic function of a uniform random variable, i.e.: $$\lim_{n\to\infty}\mathbb{E}[e^{itY_n}]=\frac{e^{it}-1}{it}$$ Since $X_1, X_2, \ldots$ are independent, we have: $$\mathbb{E}[e^{itY_n}]=\prod_{k=1}^n \mathbb{E} \left[e^{it\frac{X_k}{2^k}} \right] = \prod_{k=1}^n \frac{1+e^{it/2^k}}{2}.$$ I'm basically stuck because I don't know what to do with the product as $n\to\infty$ . Is there a trick to it? Is there maybe a simpler solution? Thanks in advance!","Let be independent, identically distributed random variables with distribution . Define the random varible: Prove that is uniformly distributed over the unit interval . The only way I currently know how to attack this problem is by using Levy's theorem: For , it's enough to prove that the characteristic function of converges to the characteristic function of a uniform random variable, i.e.: Since are independent, we have: I'm basically stuck because I don't know what to do with the product as . Is there a trick to it? Is there maybe a simpler solution? Thanks in advance!","X_1, X_2, \ldots \text{Ber}(\frac{1}{2}) Y:=\sum_{n=1}^\infty \frac{X_n}{2^n} Y [0,1] Y_n:=\sum_{k=1}^n \frac{X_k}{2^k} Y_n \lim_{n\to\infty}\mathbb{E}[e^{itY_n}]=\frac{e^{it}-1}{it} X_1, X_2, \ldots \mathbb{E}[e^{itY_n}]=\prod_{k=1}^n \mathbb{E} \left[e^{it\frac{X_k}{2^k}} \right] = \prod_{k=1}^n \frac{1+e^{it/2^k}}{2}. n\to\infty","['probability', 'random-variables']"
3,Is there a deeper meaning to the identity ${{\sin x}\over x} = \prod_{k=1}^{\infty} \cos\left({x\over{2^k}}\right)$?,Is there a deeper meaning to the identity ?,{{\sin x}\over x} = \prod_{k=1}^{\infty} \cos\left({x\over{2^k}}\right),Is there any deeper meaning to trigonometric identity  $${{\sin x}\over x} = \prod_{k=1}^{\infty} \cos\left({x\over{2^k}}\right)$$ beyond it corresponding to characteristic functions of i.i.d. random variables?,Is there any deeper meaning to trigonometric identity  $${{\sin x}\over x} = \prod_{k=1}^{\infty} \cos\left({x\over{2^k}}\right)$$ beyond it corresponding to characteristic functions of i.i.d. random variables?,,"['probability', 'algebra-precalculus', 'probability-theory', 'trigonometry']"
4,A family has two children. One child is a girl. What is the probability that the other child is a boy? [duplicate],A family has two children. One child is a girl. What is the probability that the other child is a boy? [duplicate],,"This question already has answers here : In a family with two children, what are the chances, if one of the children is a girl, that both children are girls? (21 answers) Closed 10 years ago . My initial thought process: Sample space: GG, GB, BG, BB. I then crossed out BG because it's the same as GB because order doesn't matter here. And because we know one is a girl, there leaves two possibilities left. If the other is a boy, the probability should be 1/2, much like how they deduced it Finding probability of other child also being a boy . However, I was told by my teacher that the answer is not 1/2 . I'm wondering if any of you guys can see a way in how the question is worded so that it's not 1/2... I don't think there's any other factors?","This question already has answers here : In a family with two children, what are the chances, if one of the children is a girl, that both children are girls? (21 answers) Closed 10 years ago . My initial thought process: Sample space: GG, GB, BG, BB. I then crossed out BG because it's the same as GB because order doesn't matter here. And because we know one is a girl, there leaves two possibilities left. If the other is a boy, the probability should be 1/2, much like how they deduced it Finding probability of other child also being a boy . However, I was told by my teacher that the answer is not 1/2 . I'm wondering if any of you guys can see a way in how the question is worded so that it's not 1/2... I don't think there's any other factors?",,['probability']
5,Calculating probabilities over longer period of time,Calculating probabilities over longer period of time,,"There's a great question/answer at: Calculating probabilities over different time intervals This is an awesome answer, but I'd like to ask a related question: What if the period goes the other direction, for example, the probability is determined for a year, but you want to see the probability of it happening over 50 years? For example, let's say there's a 5% chance of a fire during the course of a month.  How likely would this be over the course of a year?  What about over 30 years? And, what if there's a 5% chance during 5 months of the year, and 10% chance during 7 months of the year.  What would be the chance of a fire during the year?  What about over 30 years?","There's a great question/answer at: Calculating probabilities over different time intervals This is an awesome answer, but I'd like to ask a related question: What if the period goes the other direction, for example, the probability is determined for a year, but you want to see the probability of it happening over 50 years? For example, let's say there's a 5% chance of a fire during the course of a month.  How likely would this be over the course of a year?  What about over 30 years? And, what if there's a 5% chance during 5 months of the year, and 10% chance during 7 months of the year.  What would be the chance of a fire during the year?  What about over 30 years?",,['probability']
6,Asymmetric Normal Probability Distribution,Asymmetric Normal Probability Distribution,,"I'm looking for a continuous probability distribution a little bit like the normal distribution but asymmetric. In my opinion this distribution applies to phenomenons related to response time in environments marked by resource contention. Examples I have in mind are: In real life : the time it takes for my bus to go from my home to my office in the morning. In average it's around 15 minutes. However the way this duration varies each side of the mean value is asymmetric: it can hardly be 10 minutes less than the average but can easily take 10 minutes more. In computer capacity planning (which is the real domain where I want to use it ;-).  One transaction needs to take place in 5 seconds (average) but my QoS constraint is that 90% of the time it takes less than 15 seconds. Here is a diagram to illustrate my ideal distribution. In this last example I could approximate the distribution to a Gaussian distribution and decide that 90% is roughly equivalent to a 1.5 standard deviation. However I'm curious to know whether there is probability distribution more adapted to my problem. The end goal is to deduce what percentage of my resources should be free (e.g. each CPU core should in average be at least 50% free, disk controllers bandwidth should be 50% free, etc) in order to satisfy the 90% threshold constraints. Edit I'm adding more information here because I'm not convinced that the Log-Normal distribution fits the bill. Going back to the example of my Bus journey, there is a minimum travel time which depends on propagation law limits (dictated by highway code or physics). Similarly in a computer system, when my request runs unhampered by concurrent usage of the available physical resource, one can probably observe consistently close response times.  I term this minimum latency and I ascribe the variations above this minimum latency time to other concurrent requests in real life. The important thing here is that when contention increases, mean, median and mode values all increase when $\sigma$ increases. Here is another diagram to illustrate what I mean. So it looks like the Rayleigh distribution seems closer to what I need.  However it also looks like it lacks some kind of ""$\mu$"" parameter since I have three sizing conditions to satisfy: average response time: 5s . In the CDF when the cumulated probability = 0.9 then the response time is 15s .","I'm looking for a continuous probability distribution a little bit like the normal distribution but asymmetric. In my opinion this distribution applies to phenomenons related to response time in environments marked by resource contention. Examples I have in mind are: In real life : the time it takes for my bus to go from my home to my office in the morning. In average it's around 15 minutes. However the way this duration varies each side of the mean value is asymmetric: it can hardly be 10 minutes less than the average but can easily take 10 minutes more. In computer capacity planning (which is the real domain where I want to use it ;-).  One transaction needs to take place in 5 seconds (average) but my QoS constraint is that 90% of the time it takes less than 15 seconds. Here is a diagram to illustrate my ideal distribution. In this last example I could approximate the distribution to a Gaussian distribution and decide that 90% is roughly equivalent to a 1.5 standard deviation. However I'm curious to know whether there is probability distribution more adapted to my problem. The end goal is to deduce what percentage of my resources should be free (e.g. each CPU core should in average be at least 50% free, disk controllers bandwidth should be 50% free, etc) in order to satisfy the 90% threshold constraints. Edit I'm adding more information here because I'm not convinced that the Log-Normal distribution fits the bill. Going back to the example of my Bus journey, there is a minimum travel time which depends on propagation law limits (dictated by highway code or physics). Similarly in a computer system, when my request runs unhampered by concurrent usage of the available physical resource, one can probably observe consistently close response times.  I term this minimum latency and I ascribe the variations above this minimum latency time to other concurrent requests in real life. The important thing here is that when contention increases, mean, median and mode values all increase when $\sigma$ increases. Here is another diagram to illustrate what I mean. So it looks like the Rayleigh distribution seems closer to what I need.  However it also looks like it lacks some kind of ""$\mu$"" parameter since I have three sizing conditions to satisfy: average response time: 5s . In the CDF when the cumulated probability = 0.9 then the response time is 15s .",,"['probability', 'normal-distribution']"
7,Two different books are giving two different solutions.,Two different books are giving two different solutions.,,"So I am solving some probability/finance books and I've gone through two similar problems that conflict in their answers. Paul Wilmott The first book is Paul Wilmott's Frequently Asked Questions in Quantitative Finance . This book poses the following question: Every day a trader either makes 50% with probability 0.6 or loses 50% with probability 0.4. What is the probability the trader will be ahead at the end of a year, 260 trading days? Over what number of days does the trader have the maximum probability of making money? Solution: This is a nice one because it is extremely counterintuitive. At first glance it looks like you are going to make money in the long run, but this is not the case. Let n be the number of days on which you make 50%. After $n$ days your returns, $R_n$ will be: $$R_n = 1.5^n 0.5^{260−n}$$ So the question can be recast in terms of finding $n$ for which this expression is equal to 1. He does some math, which you can do as well, that leads to $n=164.04$ . So a trader needs to win at least 165 days to make a profit. He then says that the average profit per day is: $1−e^{0.6 \ln1.5 + 0.4\ln0.5}$ = −3.34% Which is mathematically wrong, but assuming he just switched the numbers and it should be: $e^{0.6 \ln1.5 + 0.4\ln0.5} - 1$ = −3.34% That still doesn't make sense to me. Why are the probabilities in the exponents? I don't get Wilmott's approach here. *PS: I ignore the second question, just focused on daily average return here. Mark Joshi The second book is Mark Joshi's Quant Job Interview Question and Answers which poses this question: Suppose you have a fair coin. You start off with a dollar, and if you toss an H your position doubles, if you toss a T it halves. What is the expected value of your portfolio if you toss infinitely? Solution Let $X$ denote a toss, then: $$E(X) = \frac{1}{2}*2 + \frac{1}{2}\frac{1}{2} = \frac{5}{4}$$ So for $n$ tosses: $$R_n = (\frac{5}{4})^n$$ Which tends to infinity as $n$ tends to infinity Uhm, excuse me what? Who is right here and who is wrong? Why do they use different formula's? Using Wilmott's (second, corrected) formula for Joshi's situation I get the average return per day is: $$ e^{0.5\ln(2) + 0.5\ln(0.5)} - 1 = 0% $$ I ran a Python simulation of this, simulating $n$ days/tosses/whatever and it seems that the above is not correct. Joshi was right, the portfolio tends to infinity. Wilmott was also right, the portfolio goes to zero when I use his parameters. Wilmott also explicitly dismisses Joshi's approach saying: As well as being counterintuitive, this question does give a nice insight into money management and is clearly related to the Kelly criterion. If you see a question like this it is meant to trick you if the expected profit, here 0.6 × 0.5 + 0.4 × (−0.5) = 0.1, is positive with the expected return, here −3.34%, negative. So what is going on? Here is the code: import random def traderToss(n_tries, p_win, win_ratio, loss_ratio):     SIM = 10**5 # Number of times to run the simulation     ret = 0.0     for _ in range(SIM):         curr = 1 # Starting portfolio         for _ in range(n_tries): # number of flips/days/whatever             if random.random() > p_win:                 curr *= win_ratio # LINE 9             else:                 curr *= loss_ratio # LINE 11          ret += curr # LINE 13: add portfolio value after this simulation      print(ret/SIM) # Print average return value (E[X]) Use: traderToss(260, 0.6, 1.5, 0.5) to test Wilmott's trader scenario. Use: traderToss(260, 0.5, 2, 0.5) to test Joshi's coin flip scenario. Thanks to the followup comments from Robert Shore and Steve Kass below, I have figured one part of the issue. Joshi's answer assumes you play once, therefore the returns would be additive and not multiplicative. His question is vague enough, using the word ""your portfolio"", suggesting we place our returns back in for each consecutive toss. If this were the case, we need the geometric mean not the arithmetic mean, which is the expected value calculation he does. This is verifiable by changing the python simulation to: import random def traderToss():     SIM = 10**5 # Number of times to run the simulation     ret = 0.0     for _ in range(SIM):        if random.random() > 0.5:                 curr = 2 # Our portfolio becomes 2             else:                 curr = 0.5 # Our portfolio becomes 0.5          ret += curr       print(ret/SIM) # Print single day return This yields $\approx 1.25$ as in the book. However, if returns are multiplicative, therefore we need a different approach, which I assume is Wilmott's formula. This is where I'm stuck. Because I still don't understand the Wilmott formula. Why is the end of day portfolio on average: $$ R_{day} = r_1^{p_1} * r_2^{p_2} * .... * r_n^{p_n} $$ Where $r_i$ , $p_i$ are the portfolio multiplier, probability for each scenario $i$ , and there are $n$ possible scenarios. Where does this (generalized) formula come from in probability theory? This isn't a geometric mean. Then what is it?","So I am solving some probability/finance books and I've gone through two similar problems that conflict in their answers. Paul Wilmott The first book is Paul Wilmott's Frequently Asked Questions in Quantitative Finance . This book poses the following question: Every day a trader either makes 50% with probability 0.6 or loses 50% with probability 0.4. What is the probability the trader will be ahead at the end of a year, 260 trading days? Over what number of days does the trader have the maximum probability of making money? Solution: This is a nice one because it is extremely counterintuitive. At first glance it looks like you are going to make money in the long run, but this is not the case. Let n be the number of days on which you make 50%. After days your returns, will be: So the question can be recast in terms of finding for which this expression is equal to 1. He does some math, which you can do as well, that leads to . So a trader needs to win at least 165 days to make a profit. He then says that the average profit per day is: = −3.34% Which is mathematically wrong, but assuming he just switched the numbers and it should be: = −3.34% That still doesn't make sense to me. Why are the probabilities in the exponents? I don't get Wilmott's approach here. *PS: I ignore the second question, just focused on daily average return here. Mark Joshi The second book is Mark Joshi's Quant Job Interview Question and Answers which poses this question: Suppose you have a fair coin. You start off with a dollar, and if you toss an H your position doubles, if you toss a T it halves. What is the expected value of your portfolio if you toss infinitely? Solution Let denote a toss, then: So for tosses: Which tends to infinity as tends to infinity Uhm, excuse me what? Who is right here and who is wrong? Why do they use different formula's? Using Wilmott's (second, corrected) formula for Joshi's situation I get the average return per day is: I ran a Python simulation of this, simulating days/tosses/whatever and it seems that the above is not correct. Joshi was right, the portfolio tends to infinity. Wilmott was also right, the portfolio goes to zero when I use his parameters. Wilmott also explicitly dismisses Joshi's approach saying: As well as being counterintuitive, this question does give a nice insight into money management and is clearly related to the Kelly criterion. If you see a question like this it is meant to trick you if the expected profit, here 0.6 × 0.5 + 0.4 × (−0.5) = 0.1, is positive with the expected return, here −3.34%, negative. So what is going on? Here is the code: import random def traderToss(n_tries, p_win, win_ratio, loss_ratio):     SIM = 10**5 # Number of times to run the simulation     ret = 0.0     for _ in range(SIM):         curr = 1 # Starting portfolio         for _ in range(n_tries): # number of flips/days/whatever             if random.random() > p_win:                 curr *= win_ratio # LINE 9             else:                 curr *= loss_ratio # LINE 11          ret += curr # LINE 13: add portfolio value after this simulation      print(ret/SIM) # Print average return value (E[X]) Use: traderToss(260, 0.6, 1.5, 0.5) to test Wilmott's trader scenario. Use: traderToss(260, 0.5, 2, 0.5) to test Joshi's coin flip scenario. Thanks to the followup comments from Robert Shore and Steve Kass below, I have figured one part of the issue. Joshi's answer assumes you play once, therefore the returns would be additive and not multiplicative. His question is vague enough, using the word ""your portfolio"", suggesting we place our returns back in for each consecutive toss. If this were the case, we need the geometric mean not the arithmetic mean, which is the expected value calculation he does. This is verifiable by changing the python simulation to: import random def traderToss():     SIM = 10**5 # Number of times to run the simulation     ret = 0.0     for _ in range(SIM):        if random.random() > 0.5:                 curr = 2 # Our portfolio becomes 2             else:                 curr = 0.5 # Our portfolio becomes 0.5          ret += curr       print(ret/SIM) # Print single day return This yields as in the book. However, if returns are multiplicative, therefore we need a different approach, which I assume is Wilmott's formula. This is where I'm stuck. Because I still don't understand the Wilmott formula. Why is the end of day portfolio on average: Where , are the portfolio multiplier, probability for each scenario , and there are possible scenarios. Where does this (generalized) formula come from in probability theory? This isn't a geometric mean. Then what is it?",n R_n R_n = 1.5^n 0.5^{260−n} n n=164.04 1−e^{0.6 \ln1.5 + 0.4\ln0.5} e^{0.6 \ln1.5 + 0.4\ln0.5} - 1 X E(X) = \frac{1}{2}*2 + \frac{1}{2}\frac{1}{2} = \frac{5}{4} n R_n = (\frac{5}{4})^n n  e^{0.5\ln(2) + 0.5\ln(0.5)} - 1 = 0%  n \approx 1.25  R_{day} = r_1^{p_1} * r_2^{p_2} * .... * r_n^{p_n}  r_i p_i i n,"['probability', 'probability-theory', 'markov-chains', 'expected-value', 'markov-process']"
8,Pick the closest number game,Pick the closest number game,,"Suppose three players play the following game: Player 1 picks a number in $[0,1]$. Then player $2$ picks a number in the same range but different from the number player $1$ picked. Player $3$ also picks a number in the same range but different from the previous two. We then pick a random number in $[0,1]$ uniformly randomly. Whoever has a number closer to the random number we picked wins the game. Assume all players play optimally with the goal of maximizing their probability of winning. If one of them has several optimal choices, they pick one of them at random. 1)If Player 1 chooses zero, what is the best choice for player 2? 2)What is the best choice for player 1? I have some trouble seeing how this problem is well-defined. For instance, if Player $1$ picks $0$ and Player $2$ picks 1, then I cannot see what the optimal choice would be for the last player since he has to pick different numbers. Can someone help? EDIT: I now understand better how the problem works, but I still have no idea how to approach this. Can someone give me some hints?","Suppose three players play the following game: Player 1 picks a number in $[0,1]$. Then player $2$ picks a number in the same range but different from the number player $1$ picked. Player $3$ also picks a number in the same range but different from the previous two. We then pick a random number in $[0,1]$ uniformly randomly. Whoever has a number closer to the random number we picked wins the game. Assume all players play optimally with the goal of maximizing their probability of winning. If one of them has several optimal choices, they pick one of them at random. 1)If Player 1 chooses zero, what is the best choice for player 2? 2)What is the best choice for player 1? I have some trouble seeing how this problem is well-defined. For instance, if Player $1$ picks $0$ and Player $2$ picks 1, then I cannot see what the optimal choice would be for the last player since he has to pick different numbers. Can someone help? EDIT: I now understand better how the problem works, but I still have no idea how to approach this. Can someone give me some hints?",,"['probability', 'game-theory']"
9,A function of a Markov chain is necessarily again a Markov chain?,A function of a Markov chain is necessarily again a Markov chain?,,"I was curious about the above questions. If the answer is not, is there a mathematical proof of that? e.g. given a function $f:\mathcal X \rightarrow \mathcal Y$, with $\mathcal X$ a finite state space, find a Markov Chain $(X_n)_{n\ge 0}$ such that $(Y_n)_{n\ge 0}$ with $Y_n = f(X_n)$ is not a Markov Chain.","I was curious about the above questions. If the answer is not, is there a mathematical proof of that? e.g. given a function $f:\mathcal X \rightarrow \mathcal Y$, with $\mathcal X$ a finite state space, find a Markov Chain $(X_n)_{n\ge 0}$ such that $(Y_n)_{n\ge 0}$ with $Y_n = f(X_n)$ is not a Markov Chain.",,"['probability', 'probability-theory', 'markov-chains', 'markov-process', 'information-theory']"
10,What is the probability that this harmonic series with randomly chosen signs will converge?,What is the probability that this harmonic series with randomly chosen signs will converge?,,"Suppose we fix $p$ between $0$ and $1$ (without loss of generalization, we can assume $p \leq 1/2$). Then suppose we form the series $\sum_n a_n / n$ where the $a_n$ are independent random variables and each $a_n$ equals $1$ with probability $p$ and equals $-1$ with probability $1-p$. (Hence why we can assume $p \leq 1/2$.) What is the probability that this series converges, as a function of $p$? Clearly if the series diverges with probability 1 for $p = 1/2$ then it always diverges with probability $1$ for any $p$, so perhaps the case $p=1/2$ is the most interesting. (However, if other values of $p$ give non-zero probability of convergence that would also obviously be interesting.)","Suppose we fix $p$ between $0$ and $1$ (without loss of generalization, we can assume $p \leq 1/2$). Then suppose we form the series $\sum_n a_n / n$ where the $a_n$ are independent random variables and each $a_n$ equals $1$ with probability $p$ and equals $-1$ with probability $1-p$. (Hence why we can assume $p \leq 1/2$.) What is the probability that this series converges, as a function of $p$? Clearly if the series diverges with probability 1 for $p = 1/2$ then it always diverges with probability $1$ for any $p$, so perhaps the case $p=1/2$ is the most interesting. (However, if other values of $p$ give non-zero probability of convergence that would also obviously be interesting.)",,"['probability', 'sequences-and-series']"
11,When the sum of independent Markov chains is a Markov chain?,When the sum of independent Markov chains is a Markov chain?,,"I try to find as much as possible cases, when the chain $Z(t) = |X_1(t)-X_2(t)|$ is Markov, where $X_1(t)$ and $X_2(t)$ are independent, discrete-time and space, preferably non-homogeneous Markov chains. I started to search for the sum of independent Markov chains and I found this statement in Stoyanov J. - Counterexamples in Probability (2ed., Wiley, 1997)(p.229, one can google it and find in google books): ... the sum of two Markov processes need not be a Markov process. Note, however, that that the sum of two independent Markov processes preserves this property. That seems very strange to me and I wish to find the proof or at least the statement elsewhere. EDIT: The way of thinking how it may look like for a sum of two independent Markov chains (If I want to prove that the sum of two independent Markov chains is again a Markov chain): Let $Y(n) = X_1(n)+X_2(n)$. Then $P(Y(n+1) = i_{n+1} \ | \ Y(n) = i_n, ..., Y(0)=i_0) =$ $= P(X_1(n+1)+X_2(n+1)=i_{n+1}\ | \ X_1(n)+X_2(n)=i_n,...,X_1(0)+X_2(0)=i_0)=$ $=/ (?) / = \sum_{j+k=i_{n+1}}P(X_1(n+1)=j,X_2(n+1)=k \ | \ \cdot)=$ $=/\text{X's are independent}/ = \sum_{j+k=i_{n+1}}P(X_1(n+1)=j \ | \ \cdot)\cdot P(X_2(n+1)=k \ | \ \cdot) = $ $= /\text{Markov property + (??) } / =  \sum_{j+k=i_{n+1}}P(X_1(n+1)=j \ | \ X_1(n)+X_2(n)=i_n)\cdot P(X_2(n+1)=k \ | \ X_1(n)+X_2(n)=i_n)=$ $=P(X_1(n+1)+X_2(n+1)=i_{n+1} \ | \ X_1(n)+X_2(n)=i_n) = P(Y(n+1)=i_{n+1}\ | \ Y(n)=i_n)$. Here I am uncertain about (?) and (??) steps.","I try to find as much as possible cases, when the chain $Z(t) = |X_1(t)-X_2(t)|$ is Markov, where $X_1(t)$ and $X_2(t)$ are independent, discrete-time and space, preferably non-homogeneous Markov chains. I started to search for the sum of independent Markov chains and I found this statement in Stoyanov J. - Counterexamples in Probability (2ed., Wiley, 1997)(p.229, one can google it and find in google books): ... the sum of two Markov processes need not be a Markov process. Note, however, that that the sum of two independent Markov processes preserves this property. That seems very strange to me and I wish to find the proof or at least the statement elsewhere. EDIT: The way of thinking how it may look like for a sum of two independent Markov chains (If I want to prove that the sum of two independent Markov chains is again a Markov chain): Let $Y(n) = X_1(n)+X_2(n)$. Then $P(Y(n+1) = i_{n+1} \ | \ Y(n) = i_n, ..., Y(0)=i_0) =$ $= P(X_1(n+1)+X_2(n+1)=i_{n+1}\ | \ X_1(n)+X_2(n)=i_n,...,X_1(0)+X_2(0)=i_0)=$ $=/ (?) / = \sum_{j+k=i_{n+1}}P(X_1(n+1)=j,X_2(n+1)=k \ | \ \cdot)=$ $=/\text{X's are independent}/ = \sum_{j+k=i_{n+1}}P(X_1(n+1)=j \ | \ \cdot)\cdot P(X_2(n+1)=k \ | \ \cdot) = $ $= /\text{Markov property + (??) } / =  \sum_{j+k=i_{n+1}}P(X_1(n+1)=j \ | \ X_1(n)+X_2(n)=i_n)\cdot P(X_2(n+1)=k \ | \ X_1(n)+X_2(n)=i_n)=$ $=P(X_1(n+1)+X_2(n+1)=i_{n+1} \ | \ X_1(n)+X_2(n)=i_n) = P(Y(n+1)=i_{n+1}\ | \ Y(n)=i_n)$. Here I am uncertain about (?) and (??) steps.",,"['probability', 'probability-theory', 'examples-counterexamples', 'markov-chains']"
12,If $ P(A) = 0 $ is $ A $ a null event?,If  is  a null event?, P(A) = 0   A ,"I know that $ P(\text{null event}) = 0 $, but is the reverse true? i.e. if $ P(A) = 0 $ is $ A $ a null event? I'm not too sure I even understand what a null event is, to be honest. Could anyone give me an example of one?","I know that $ P(\text{null event}) = 0 $, but is the reverse true? i.e. if $ P(A) = 0 $ is $ A $ a null event? I'm not too sure I even understand what a null event is, to be honest. Could anyone give me an example of one?",,"['probability', 'probability-theory']"
13,An unexpected application of non-trivial combinatorics,An unexpected application of non-trivial combinatorics,,"PROBLEM STATEMENT Given two finite sets $A$ and $B$, each containing $s \in \mathbb N$ elements, how many pairs of functions $f \colon A \rightarrow B$ and $g \colon B \rightarrow A$ are there, such that their composition $h = g \circ f$ has no fixed points? THE ORIGIN OF THE PROBLEM It is highly improbable that two people could happen to like each other by chance. Which, under the following assumptions: We are considering a group of $2n$ people with $n$ men and $n$ women; Each person likes exactly one person of the opposite sex with equal probability; reduces to Given two finite sets $A$ and $B$, each containing $n \in \mathbb N$ elements, and two arbitrary functions $f \colon A \rightarrow B$ and $g \colon B \rightarrow A$, what is the expected number of pairs of elements $(a \in A, b \in B)$ such that $f(a) = b$ and $g(b) = a$? WHAT I HAVE DONE Here I am using $E$ for expectation, $P$ for probability and $N$ for number. $$E(\text{number of pairs}) = \sum\limits_{k=0}^n k \cdot P(\text{there are exactly }k\text{ pairs})$$ $$P(\text{there are exactly }k\text{ pairs}) = \frac {N(\text{situations with }k\text{ pairs})} {N(\text{total situations})}$$ $$N(\text{total situations}) = (n^n)^2,$$ because there are $n^n$ functions from $A$ to $B$ and the same number in the reverse direction. $$ \begin{aligned}  N & (\text{situations with }k\text{ pairs}) = \\  & = N(\text{pairs of functions }(f \colon A \rightarrow B, g \colon B \rightarrow A) \\  & \quad \text{ whose composition has exactly }k\text{ fixed points}) = \\  & = \sum [\text{over selections of }k\text{ elements from both }A\text{ and }B] \\  & \quad N(\text{pairs of functions whose composition is identity only on the selection}) = \\  & = {\binom {n} {k}}^2 \cdot k! \cdot N(\text{pairs of functions on two sets containing } \\  & \quad s = n - k\text{ elements whose composition has no fixed points}) \end{aligned} $$ This is how I reduced the original problem to the problem in question. If the solution to that problem was $N(s)$, the answer to the original problem would be $$E(\text{number of pairs}) = \frac {1} {(n^n)^2} \sum\limits_{k=0}^n k \cdot {\binom {n} {k}}^2 \cdot k! \cdot N(n - k)$$ WHAT I TRIED TO DO Approach: fix $g$, count $f$s. $$ \begin{multline} N(s) = \sum [\text{over all functions }g \colon B \rightarrow A] \\ N(\text{functions }f \colon A \rightarrow B\text{ such that the composition }h = g \circ f \\ \text{ has no fixed points}) \end{multline} $$ Problem: all such $f$s have to send every $a$ to anywhere but $g^{-1}(a)$. Therefore, there are $\prod\limits_{a \in A} (n - \left|g^{-1}(a)\right|)$ such functions. The sets $\left\{g^{-1}(a)\right\}_{a \in A}$ form a partition of $B$. The obvious next idea is to classify all $g$s based on that partition, but the formulas start to get really scary. UPD: I wrote a simulation that actually counted directly the expectation for $n=1..5$ (the $O(n\cdot(n^n)^2)$ approach). The expected value is always exactly 1. I have no idea how to interpret that, given the crazy formula for expectation. I would mostly appreciate an idea for an approach, not a complete solution.","PROBLEM STATEMENT Given two finite sets $A$ and $B$, each containing $s \in \mathbb N$ elements, how many pairs of functions $f \colon A \rightarrow B$ and $g \colon B \rightarrow A$ are there, such that their composition $h = g \circ f$ has no fixed points? THE ORIGIN OF THE PROBLEM It is highly improbable that two people could happen to like each other by chance. Which, under the following assumptions: We are considering a group of $2n$ people with $n$ men and $n$ women; Each person likes exactly one person of the opposite sex with equal probability; reduces to Given two finite sets $A$ and $B$, each containing $n \in \mathbb N$ elements, and two arbitrary functions $f \colon A \rightarrow B$ and $g \colon B \rightarrow A$, what is the expected number of pairs of elements $(a \in A, b \in B)$ such that $f(a) = b$ and $g(b) = a$? WHAT I HAVE DONE Here I am using $E$ for expectation, $P$ for probability and $N$ for number. $$E(\text{number of pairs}) = \sum\limits_{k=0}^n k \cdot P(\text{there are exactly }k\text{ pairs})$$ $$P(\text{there are exactly }k\text{ pairs}) = \frac {N(\text{situations with }k\text{ pairs})} {N(\text{total situations})}$$ $$N(\text{total situations}) = (n^n)^2,$$ because there are $n^n$ functions from $A$ to $B$ and the same number in the reverse direction. $$ \begin{aligned}  N & (\text{situations with }k\text{ pairs}) = \\  & = N(\text{pairs of functions }(f \colon A \rightarrow B, g \colon B \rightarrow A) \\  & \quad \text{ whose composition has exactly }k\text{ fixed points}) = \\  & = \sum [\text{over selections of }k\text{ elements from both }A\text{ and }B] \\  & \quad N(\text{pairs of functions whose composition is identity only on the selection}) = \\  & = {\binom {n} {k}}^2 \cdot k! \cdot N(\text{pairs of functions on two sets containing } \\  & \quad s = n - k\text{ elements whose composition has no fixed points}) \end{aligned} $$ This is how I reduced the original problem to the problem in question. If the solution to that problem was $N(s)$, the answer to the original problem would be $$E(\text{number of pairs}) = \frac {1} {(n^n)^2} \sum\limits_{k=0}^n k \cdot {\binom {n} {k}}^2 \cdot k! \cdot N(n - k)$$ WHAT I TRIED TO DO Approach: fix $g$, count $f$s. $$ \begin{multline} N(s) = \sum [\text{over all functions }g \colon B \rightarrow A] \\ N(\text{functions }f \colon A \rightarrow B\text{ such that the composition }h = g \circ f \\ \text{ has no fixed points}) \end{multline} $$ Problem: all such $f$s have to send every $a$ to anywhere but $g^{-1}(a)$. Therefore, there are $\prod\limits_{a \in A} (n - \left|g^{-1}(a)\right|)$ such functions. The sets $\left\{g^{-1}(a)\right\}_{a \in A}$ form a partition of $B$. The obvious next idea is to classify all $g$s based on that partition, but the formulas start to get really scary. UPD: I wrote a simulation that actually counted directly the expectation for $n=1..5$ (the $O(n\cdot(n^n)^2)$ approach). The expected value is always exactly 1. I have no idea how to interpret that, given the crazy formula for expectation. I would mostly appreciate an idea for an approach, not a complete solution.",,"['probability', 'combinatorics', 'applications']"
14,Game theory games with very counter-intuitive results?,Game theory games with very counter-intuitive results?,,"I have heard of an interesting game that produces a very counter-intuitive result. It is an auction of a 100 dollar bill, but one in which both the first person in the auction and the second need to pay for the bill (while only the first one gets the bill). What is the most interesting, most often the bill is sold for over 100 USD, sometimes going up to 500 USD. The explanation is that the second-highest bidder doesn't want to lose a lot of money, so they opt to outbid the highest bidder by a bit to minimize their losses. So my question is, what other game theory games are out there that produce very counter-intuitive results?","I have heard of an interesting game that produces a very counter-intuitive result. It is an auction of a 100 dollar bill, but one in which both the first person in the auction and the second need to pay for the bill (while only the first one gets the bill). What is the most interesting, most often the bill is sold for over 100 USD, sometimes going up to 500 USD. The explanation is that the second-highest bidder doesn't want to lose a lot of money, so they opt to outbid the highest bidder by a bit to minimize their losses. So my question is, what other game theory games are out there that produce very counter-intuitive results?",,"['probability', 'game-theory']"
15,Rolling three dice...am I doing this correctly?,Rolling three dice...am I doing this correctly?,,"Tree dice are thrown. What is the probability the same number appears on exactly two of the three dice? Since you need exactly two to be the same, there are three possibilities: 1. First and second, not third 2. First and third, not second 3. Second and third, not first For 1) The first die, you have $\frac{6}{6}$ . The second die needs to be equal to the first, so you have probability of $\frac{1}{6}$ . Then the third die can't be equal to the first and second dice, so it's $\frac{5}{6}$ .  All together you get $1 \cdot \frac{1}{6} \cdot \frac{5}{6}$ . And since the next two cases yield the same results, then the probability that the same number apears on exactly two of the three dice is $$ 3 \cdot \left(1 \cdot\frac{1}{6} \cdot \frac{5}{6}\right)=\frac{5}{12}$$ Did I do this correctly? Thank you.","Tree dice are thrown. What is the probability the same number appears on exactly two of the three dice? Since you need exactly two to be the same, there are three possibilities: 1. First and second, not third 2. First and third, not second 3. Second and third, not first For 1) The first die, you have . The second die needs to be equal to the first, so you have probability of . Then the third die can't be equal to the first and second dice, so it's .  All together you get . And since the next two cases yield the same results, then the probability that the same number apears on exactly two of the three dice is Did I do this correctly? Thank you.",\frac{6}{6} \frac{1}{6} \frac{5}{6} 1 \cdot \frac{1}{6} \cdot \frac{5}{6}  3 \cdot \left(1 \cdot\frac{1}{6} \cdot \frac{5}{6}\right)=\frac{5}{12},"['probability', 'solution-verification']"
16,Throwing All Numbers From 2 to 12 With Two Dice [duplicate],Throwing All Numbers From 2 to 12 With Two Dice [duplicate],,This question already has answers here : Expected number of rolling a pair of dice to generate all possible sums (2 answers) Closed 7 years ago . What is the expected number of times one must throw two fair dice before all numbers from 2 to 12 have appeared at least once?,This question already has answers here : Expected number of rolling a pair of dice to generate all possible sums (2 answers) Closed 7 years ago . What is the expected number of times one must throw two fair dice before all numbers from 2 to 12 have appeared at least once?,,['probability']
17,Do the moments characterize a distribution with compact support?,Do the moments characterize a distribution with compact support?,,"Given a random variable $X=(X_1,...,X_p)$ with $P(X \in M) = 1$ for compact $M$, do the values of $E[X_1]$, $E[X_2], ..., E[X_1^2], E[X_1 X_2],..., E[X_1^3],...,E[X_1 X_2 X_3]... $ determine the distribution $F(x)=P(X_1 \leq x_1 \wedge \dots\wedge X_p \leq x_p)$ uniquely?","Given a random variable $X=(X_1,...,X_p)$ with $P(X \in M) = 1$ for compact $M$, do the values of $E[X_1]$, $E[X_2], ..., E[X_1^2], E[X_1 X_2],..., E[X_1^3],...,E[X_1 X_2 X_3]... $ determine the distribution $F(x)=P(X_1 \leq x_1 \wedge \dots\wedge X_p \leq x_p)$ uniquely?",,"['probability', 'moment-problem']"
18,"Proof of expected length in random division of $[0,1]$ interval",Proof of expected length in random division of  interval,"[0,1]","I have trouble fully understanding the proof in a formal manner for the expected length of the $k^{th}$ smallest interval when we randomly divide the $[0,1]$ interval using $n$ points. The $k^{th}$ smallest interval's expected length is equal to $$\frac{\frac{1}{k} + \frac{1}{k+1} + \dots + \frac{1}{n+1}}{n+1}$$ Proof : Without loss of generality, assume the $[0,1]$ segment is broken into segments of length $s_1 \geq s_2 \geq \dots \geq s_n \geq s_{n+1}$ , in that order. We are given that $ s_1 + \dots + s_{n+1} = 1$ , and want to find the expected value of each $s_k$ . Set $ s_i = x_i + \dots + x_{n+1} $ for each $ i = 1, \dots, n+1 $ . Then, we have $ x_1 + 2x_2 + \dots + (n+1)x_{n+1} = 1 $ , and want to find the expected value of $ s_k = x_k + \dots + x_{n+1} $ . If we set $y_i = ix_i $ , then we have $ y_1 + \dots + y_{n+1} = 1 $ , so by symmetry $ E[y_i] = \frac{1}{n+1} $ for all $ i $ . Thus, $ E[x_i] = \frac{1}{i(n+1)} $ for each $ i $ , and now by linearity of expectation $ E[s_k] = E[x_k] + \dots + E[x_{n+1}] = \frac{1}{n+1} \left( \frac{1}{k} + \dots + \frac{1}{n+1} \right) $ QED I don't quite get how we use symmetry to claim that $E[y_i] = \frac{1}{n+1}$ ? I would really appreciate help in filling out the gaps in my understanding.","I have trouble fully understanding the proof in a formal manner for the expected length of the smallest interval when we randomly divide the interval using points. The smallest interval's expected length is equal to Proof : Without loss of generality, assume the segment is broken into segments of length , in that order. We are given that , and want to find the expected value of each . Set for each . Then, we have , and want to find the expected value of . If we set , then we have , so by symmetry for all . Thus, for each , and now by linearity of expectation QED I don't quite get how we use symmetry to claim that ? I would really appreciate help in filling out the gaps in my understanding.","k^{th} [0,1] n k^{th} \frac{\frac{1}{k} + \frac{1}{k+1} + \dots + \frac{1}{n+1}}{n+1} [0,1] s_1 \geq s_2 \geq \dots \geq s_n \geq s_{n+1}  s_1 + \dots + s_{n+1} = 1 s_k  s_i = x_i + \dots + x_{n+1}   i = 1, \dots, n+1   x_1 + 2x_2 + \dots + (n+1)x_{n+1} = 1   s_k = x_k + \dots + x_{n+1}  y_i = ix_i   y_1 + \dots + y_{n+1} = 1   E[y_i] = \frac{1}{n+1}   i   E[x_i] = \frac{1}{i(n+1)}   i  
E[s_k] = E[x_k] + \dots + E[x_{n+1}] = \frac{1}{n+1} \left( \frac{1}{k} + \dots + \frac{1}{n+1} \right)
 E[y_i] = \frac{1}{n+1}","['probability', 'combinatorics', 'proof-explanation', 'expected-value']"
19,Probability of an obtuse triangle in a circle.,Probability of an obtuse triangle in a circle.,,"Suppose we randomly pick 2 points A, B within a circle centered at point O. What is the probability that the triangle formed by ABO is an obtuse one? (Note that A and B are not exclusively on the circumference). And what is the conclusion extended to A, B within a ball instead? Thanks! PS this is from a Quant interview. The following is what I have derived during the exam: (Edited, thank you for your corrections!) consider the joint probability of x, y coordinate for any point in a unit circle, then $f_{XY}(x, y) = \frac{1}{\pi}$ , uniformly distributed inside the circular region. The distance between the point and the center, has thus a distribution $f_Z(z) = 2z$ for $z$ in [0, 1] ( $z^2 = x^2 + y^2$ ). Randomly pick an A, rotate the circle so that A is right on top of the center O. Suppose now A has a distance $z = a$ $(a > 0)$ away from the origin, then B could only be chosen in the region of $y > a$ $y < 0$ within an inner circle whose diameter is $OA$ Therefore, given that the distance is $z = a$ , the probability of ABO being an obtuse triangle is given corresponds to area $\arccos(a) - a\sqrt{1-a^2} +  \frac{a^2\pi}{4} + \frac{\pi}{2}$ (Upper, inner, and lower). By this conditional probability, we could derive the total probability, which is $$ \int_0^1 \frac{1}{\pi}\left(\arccos(a) - a\sqrt{1-a^2} +  \frac{a^2\pi}{4} + \frac{\pi}{2}\right) \cdot 2a \; da = \frac{3}{4} $$ But is there an easier way to solve this? This looks like a math competition style of question and I expect some tricks to be at play. Thanks!","Suppose we randomly pick 2 points A, B within a circle centered at point O. What is the probability that the triangle formed by ABO is an obtuse one? (Note that A and B are not exclusively on the circumference). And what is the conclusion extended to A, B within a ball instead? Thanks! PS this is from a Quant interview. The following is what I have derived during the exam: (Edited, thank you for your corrections!) consider the joint probability of x, y coordinate for any point in a unit circle, then , uniformly distributed inside the circular region. The distance between the point and the center, has thus a distribution for in [0, 1] ( ). Randomly pick an A, rotate the circle so that A is right on top of the center O. Suppose now A has a distance away from the origin, then B could only be chosen in the region of within an inner circle whose diameter is Therefore, given that the distance is , the probability of ABO being an obtuse triangle is given corresponds to area (Upper, inner, and lower). By this conditional probability, we could derive the total probability, which is But is there an easier way to solve this? This looks like a math competition style of question and I expect some tricks to be at play. Thanks!","f_{XY}(x, y) = \frac{1}{\pi} f_Z(z) = 2z z z^2 = x^2 + y^2 z = a (a > 0) y > a y < 0 OA z = a \arccos(a) - a\sqrt{1-a^2} +  \frac{a^2\pi}{4} + \frac{\pi}{2} 
\int_0^1 \frac{1}{\pi}\left(\arccos(a) - a\sqrt{1-a^2} +  \frac{a^2\pi}{4} + \frac{\pi}{2}\right) \cdot 2a \; da = \frac{3}{4}
","['probability', 'geometry']"
20,Distribution of joint Gaussian conditional on their sum,Distribution of joint Gaussian conditional on their sum,,"Let $X = (X_1, X_2, \dots, X_n)$ be jointly Gaussian with mean vector $\mu$ and covariance matrix $\Sigma$ . Let $S$ be their sum. I know that the distribution of each $X_i \mid S = s$ is also Gaussian. When $n=2$ , I know that $$ E\left( X_1\mid S = s \right) = s \frac{\sigma_1^2}{\sigma_1^2 + \sigma_2^2} $$ and $$ V\left(X_1\mid S = s \right) = \frac{\sigma_1^2\sigma_2^2}{\sigma_1^2 + \sigma_2^2} $$ (see here and here ). I could probably work out analogous expressions for an arbitrary $n$ if I sat down with a pencil and paper and worked at it for a bit. What I want to know is, what is the distribution of $X$ given $S = s$ ? I know that this can't be Gaussian, since the sum is bounded. It's clearly not Dirichlet or anything Dirichlet-esque, since the marginal distributions are Gaussian. But beyond that I don't have a clue.","Let be jointly Gaussian with mean vector and covariance matrix . Let be their sum. I know that the distribution of each is also Gaussian. When , I know that and (see here and here ). I could probably work out analogous expressions for an arbitrary if I sat down with a pencil and paper and worked at it for a bit. What I want to know is, what is the distribution of given ? I know that this can't be Gaussian, since the sum is bounded. It's clearly not Dirichlet or anything Dirichlet-esque, since the marginal distributions are Gaussian. But beyond that I don't have a clue.","X = (X_1, X_2, \dots, X_n) \mu \Sigma S X_i \mid S = s n=2 
E\left( X_1\mid S = s \right) = s \frac{\sigma_1^2}{\sigma_1^2 + \sigma_2^2}
 
V\left(X_1\mid S = s \right) = \frac{\sigma_1^2\sigma_2^2}{\sigma_1^2 + \sigma_2^2}
 n X S = s","['probability', 'statistics', 'probability-distributions', 'normal-distribution']"
21,"Show/Prove that $F_{\alpha,n,m} =1/F_{1-\alpha,m,n}$",Show/Prove that,"F_{\alpha,n,m} =1/F_{1-\alpha,m,n}","Show/Prove that $F_{\alpha,n,m} = \frac{1}{F_{1-\alpha,m,n}}$ The distributions I'm working with are the Fisher distribution and the Chi-square distribution. I can prove that n and m switch for the F distribution as follows: $$X = \frac{ \frac{A}{n} } { \frac{B}{m} }$$ where A and B follow $\chi^2$ distributions, are independent, and have n, m degrees of freedom respectively.  Then the following holds: $$\frac{1}{X} = \frac{ \frac{B}{m} } { \frac{A}{n} }$$ My question is how do I show that the $F_{\alpha}$ part goes to $F_{1-\alpha}$ Is there a way to do this all at once or do I need to show the degrees of freedom switch separately from the $1-\alpha$ part of the proof?","Show/Prove that $F_{\alpha,n,m} = \frac{1}{F_{1-\alpha,m,n}}$ The distributions I'm working with are the Fisher distribution and the Chi-square distribution. I can prove that n and m switch for the F distribution as follows: $$X = \frac{ \frac{A}{n} } { \frac{B}{m} }$$ where A and B follow $\chi^2$ distributions, are independent, and have n, m degrees of freedom respectively.  Then the following holds: $$\frac{1}{X} = \frac{ \frac{B}{m} } { \frac{A}{n} }$$ My question is how do I show that the $F_{\alpha}$ part goes to $F_{1-\alpha}$ Is there a way to do this all at once or do I need to show the degrees of freedom switch separately from the $1-\alpha$ part of the proof?",,"['probability', 'statistics', 'probability-distributions']"
22,Linearity of expectation for infinite sums?,Linearity of expectation for infinite sums?,,"I have a question related to this post: Expected value of infinite sum Is the condition listed necessary/sufficient (or both?) For instance, I'm thinking of $X_n=\frac{1}{n}Z_n$, where $Z_n \sim$N(0,1) are iid.  It ""feels like"" $E\sum_{n=1}^\infty X_n$=0, but $\sum_{n=1}^\infty E|X_n|$ =$\sum_{n=1}^\infty \frac{1}{n}E|Z_n|$ =$E|Z_1|\sum_{n=1}^\infty \frac{1}{n}=\infty$.  Thus, we can't use the result listed above to justify interchanging expectation and summation.  But I was wondering if my feeling is right.","I have a question related to this post: Expected value of infinite sum Is the condition listed necessary/sufficient (or both?) For instance, I'm thinking of $X_n=\frac{1}{n}Z_n$, where $Z_n \sim$N(0,1) are iid.  It ""feels like"" $E\sum_{n=1}^\infty X_n$=0, but $\sum_{n=1}^\infty E|X_n|$ =$\sum_{n=1}^\infty \frac{1}{n}E|Z_n|$ =$E|Z_1|\sum_{n=1}^\infty \frac{1}{n}=\infty$.  Thus, we can't use the result listed above to justify interchanging expectation and summation.  But I was wondering if my feeling is right.",,['probability']
23,"Are ""most"" continuous functions also differentiable?","Are ""most"" continuous functions also differentiable?",,"Let $A$ be a nonempty open subset of $\mathbb{R}$. Consider a function $f : A \rightarrow \mathbb{R}$. Given that $f$ is continuous, what is the probability that it is differentiable? I suspect it is $0$.","Let $A$ be a nonempty open subset of $\mathbb{R}$. Consider a function $f : A \rightarrow \mathbb{R}$. Given that $f$ is continuous, what is the probability that it is differentiable? I suspect it is $0$.",,"['probability', 'measure-theory', 'derivatives', 'continuity']"
24,Exponential Distribution with changing (time-varying) rate parameter,Exponential Distribution with changing (time-varying) rate parameter,,"In short, how would one sample an Exponential Distribution with an exponentially-increasing rate parameter, e.g., $\lambda=e^t$ ? What distribution does such a random variable follow? Background: The Exponential Distribution models ""time until failure"" of, say, lightbulbs. It is parametrized by a constant parameter $\lambda$ called the failure rate that is the average rate of lightbulb burnouts. If $\lambda$ is time-varying, say, proportionally to a power of time, then it turns out that the time until failure is distributed by the more general Weibull distribution. This question involves this relationship, although the time-variance of $\lambda$ is not made explicit. My question is: What if $\lambda$ varies exponentially in time? Is there a distribution for this case? More simply, how would I sample a random variable that is exponentially distributed with time-varying rate parameter $\lambda = e^t$ ? I've searched Google and Wikipedia without success. I'd really appreciate any advice or pointers.","In short, how would one sample an Exponential Distribution with an exponentially-increasing rate parameter, e.g., ? What distribution does such a random variable follow? Background: The Exponential Distribution models ""time until failure"" of, say, lightbulbs. It is parametrized by a constant parameter called the failure rate that is the average rate of lightbulb burnouts. If is time-varying, say, proportionally to a power of time, then it turns out that the time until failure is distributed by the more general Weibull distribution. This question involves this relationship, although the time-variance of is not made explicit. My question is: What if varies exponentially in time? Is there a distribution for this case? More simply, how would I sample a random variable that is exponentially distributed with time-varying rate parameter ? I've searched Google and Wikipedia without success. I'd really appreciate any advice or pointers.",\lambda=e^t \lambda \lambda \lambda \lambda \lambda = e^t,"['probability', 'probability-distributions', 'transformation']"
25,Continuous uniform distribution over a circle with radius R,Continuous uniform distribution over a circle with radius R,,"I started to do this problem with the standard integration techniques, but I cant help but think that there has got to be something I am not seeing. Since it is a uniform distribution, even though x and y are not independent, it seems like there should be some shortcut. Here is the problem: Take a random point (x, y) which is uniformly distributed over the circle with radius R $f(x,y) = \begin{cases} \frac{1}{\pi R^2} & x^2 + y^2 \le R^2, \\ 0 & \text{otherwise} \end{cases}$ calculate $g(x)$, $h(y)$, the mean of $x$, $y$, and $xy$.","I started to do this problem with the standard integration techniques, but I cant help but think that there has got to be something I am not seeing. Since it is a uniform distribution, even though x and y are not independent, it seems like there should be some shortcut. Here is the problem: Take a random point (x, y) which is uniformly distributed over the circle with radius R $f(x,y) = \begin{cases} \frac{1}{\pi R^2} & x^2 + y^2 \le R^2, \\ 0 & \text{otherwise} \end{cases}$ calculate $g(x)$, $h(y)$, the mean of $x$, $y$, and $xy$.",,"['probability', 'probability-distributions', 'uniform-continuity']"
26,Calculating probabilities over different time intervals,Calculating probabilities over different time intervals,,"If there was a 10% chance of an event occurring over a year, how would I work out how likely this event is to happen per month, per day etc? Now because if, using per month as an example, if it occurs in January it cannot happen again for that particular instance, which takes away simple division.","If there was a 10% chance of an event occurring over a year, how would I work out how likely this event is to happen per month, per day etc? Now because if, using per month as an example, if it occurs in January it cannot happen again for that particular instance, which takes away simple division.",,['probability']
27,Expected number of frog jumps,Expected number of frog jumps,,"There is frog jumping forward on a line. Each jump distance is random with a known cumulative distribution function $F$. What is the expected number of jumps to reach (or go beyond) distance $d$ from the origin? If there is a Wikipedia or other web page describing the formula in terms of $F$, just give a (deep) link. I'd prefer to have the proof as well.","There is frog jumping forward on a line. Each jump distance is random with a known cumulative distribution function $F$. What is the expected number of jumps to reach (or go beyond) distance $d$ from the origin? If there is a Wikipedia or other web page describing the formula in terms of $F$, just give a (deep) link. I'd prefer to have the proof as well.",,"['probability', 'probability-theory', 'probability-distributions']"
28,Gaussian Mean Width,Gaussian Mean Width,,"Consider $K\subset S^{n-1}$. Its Gaussian Mean Width is defined to be $$ \mathbb{E}\,\sup_{x\in K}\vert \big<g, x\big>\vert  $$ where $g$ is a standard Gaussian Random Vector in $\mathbb{R}^{n}$.  I think that this is somehow supposed to be an averaging of the distance between planes needed to bound $K$, but I'm not sure how this definition actually does this. Is this definition dependent on $g$? Can anyone explain the motivation for this definition for me?","Consider $K\subset S^{n-1}$. Its Gaussian Mean Width is defined to be $$ \mathbb{E}\,\sup_{x\in K}\vert \big<g, x\big>\vert  $$ where $g$ is a standard Gaussian Random Vector in $\mathbb{R}^{n}$.  I think that this is somehow supposed to be an averaging of the distance between planes needed to bound $K$, but I'm not sure how this definition actually does this. Is this definition dependent on $g$? Can anyone explain the motivation for this definition for me?",,"['probability', 'probability-theory', 'probability-distributions']"
29,Does any moment generating function implies an existence of moments?,Does any moment generating function implies an existence of moments?,,"For a real-valued r.v. $\xi$ we suppose and existence of its moment generating function $m(t) = \mathsf E\mathrm e^{t\xi}$ for all $t\in (-h,h)$ where $h>0$. I wonder how many moments $M_n = \mathsf E\xi^n$ are finite. I know that using the differentiation under the Lebesgue integral, local existence of $m(t)$ implies an existence of $M_1 = \mathsf E\xi$. On the other hand for the 2nd moment it seems that the finiteness of $M_2$ is equivalent to the statement that $m''(0)$ exists, hence there should be examples when $m(t)$ in the neighborhood of $0$ while $M_2 = \infty$. For an example when $M_2$ does exist without an existence of $m(t)$ for $t>0$ we can consider a r.v. with a density $$ f(x) = \frac{2}{\pi(1+x^2)^2}. $$ Could you provide such an example when $m(t)$ exists in the neighborhood of $t=0$ but $M_2 = \infty$? Maybe you can also refer me to the literature since this question is not covered in my book on the probability.","For a real-valued r.v. $\xi$ we suppose and existence of its moment generating function $m(t) = \mathsf E\mathrm e^{t\xi}$ for all $t\in (-h,h)$ where $h>0$. I wonder how many moments $M_n = \mathsf E\xi^n$ are finite. I know that using the differentiation under the Lebesgue integral, local existence of $m(t)$ implies an existence of $M_1 = \mathsf E\xi$. On the other hand for the 2nd moment it seems that the finiteness of $M_2$ is equivalent to the statement that $m''(0)$ exists, hence there should be examples when $m(t)$ in the neighborhood of $0$ while $M_2 = \infty$. For an example when $M_2$ does exist without an existence of $m(t)$ for $t>0$ we can consider a r.v. with a density $$ f(x) = \frac{2}{\pi(1+x^2)^2}. $$ Could you provide such an example when $m(t)$ exists in the neighborhood of $t=0$ but $M_2 = \infty$? Maybe you can also refer me to the literature since this question is not covered in my book on the probability.",,['probability']
30,Alice and Bob sometimes lie; a die is thrown and they claim different results; what is the probability that Bob is being honest?,Alice and Bob sometimes lie; a die is thrown and they claim different results; what is the probability that Bob is being honest?,,"Alice speaks the truth with probability $3/4$ and Bob speaks the truth with probability $2/3$ . A die is thrown and both Alice and Bob observe the number. Afterwards, Alice asserts to Carl (who does not know the number) that the number is $3$ while Bob says (to Carl) the number is $1$ . Find the probability that the number is actually $1$ . UPDATE: To clear ambiguity, note that if person decides to lie, he/she will choose a false answer randomly from all the possible false answer ( $\{1, 2, \cdots, 6\}$ - $\{\text{The number that actually showed up}\}$ ). Also, a die is thrown, and then both Alice and Bob will see the number. Then, they will lie/say truth accordingly. My attempt: Case $1$ : Number $1$ showed up. Chance of all this happening = $\frac{1}{6} \cdot \frac{2}{3} \cdot \big(\frac{1}{4} \cdot \frac{1}{5}\big) = \frac{1}{180}$ Case $2$ : Number $3$ showed up Chance of all this happening = $\frac{1}{6} \cdot \frac{3}{4} \cdot \big(\frac{1}{3} \cdot \frac{1}{5}\big) = \frac{1}{120}$ Case $3$ : Other number showed up Chance of all this happening = $\frac{4}{6} \cdot (\frac{1}{4} \cdot \frac{1}{5}) \cdot (\frac{1}{3} \cdot \frac{1}{5}) = \frac{1}{450}$ So, total = $\boxed{\frac{\frac{1}{180}}{\frac{29}{1800}} = \frac{10}{29}}$ Is my attempt correct? If not, how to do this problem?","Alice speaks the truth with probability and Bob speaks the truth with probability . A die is thrown and both Alice and Bob observe the number. Afterwards, Alice asserts to Carl (who does not know the number) that the number is while Bob says (to Carl) the number is . Find the probability that the number is actually . UPDATE: To clear ambiguity, note that if person decides to lie, he/she will choose a false answer randomly from all the possible false answer ( - ). Also, a die is thrown, and then both Alice and Bob will see the number. Then, they will lie/say truth accordingly. My attempt: Case : Number showed up. Chance of all this happening = Case : Number showed up Chance of all this happening = Case : Other number showed up Chance of all this happening = So, total = Is my attempt correct? If not, how to do this problem?","3/4 2/3 3 1 1 \{1, 2, \cdots, 6\} \{\text{The number that actually showed up}\} 1 1 \frac{1}{6} \cdot \frac{2}{3} \cdot \big(\frac{1}{4} \cdot \frac{1}{5}\big) = \frac{1}{180} 2 3 \frac{1}{6} \cdot \frac{3}{4} \cdot \big(\frac{1}{3} \cdot \frac{1}{5}\big) = \frac{1}{120} 3 \frac{4}{6} \cdot (\frac{1}{4} \cdot \frac{1}{5}) \cdot (\frac{1}{3} \cdot \frac{1}{5}) = \frac{1}{450} \boxed{\frac{\frac{1}{180}}{\frac{29}{1800}} = \frac{10}{29}}",['probability']
31,The probability of forming Mississippi by choosing random letters from Mississippi,The probability of forming Mississippi by choosing random letters from Mississippi,,"I'm having difficulty with the following problem: You choose a letter at random from the word Mississippi eleven times   without replacement. What is the probability that you can form the   word Mississippi with the eleven chosen letters? Hint : it may be   helpful to number the eleven letters as $1, 2, . . . , 11$. This is how I approached it: the number of possible outcomes is $11!$, since we're taking one word at a time without replacement, so there are $11$ choices for the first letter, $10$ choices for the second letter and so on. Now, as to the number of 'success' outcomes, I noticed that there are $4$ s's to choose from, $4$ i's, $1$ M and $2$ p's. So, in order to form the word Mississippi, we have for the first letter $1$ option, $4$ for the second and third letters, $3$ for the fourth (since we've already used one ""s"") and so on, which amounts to a total of $4^2*3^2*2^3=1152$ different ways of doing so. However, my answer does not match the one provided in my book (Henk Tijn's Understanding Probability 3rd edition). What am I doing wrong? Thanks very much in advance.","I'm having difficulty with the following problem: You choose a letter at random from the word Mississippi eleven times   without replacement. What is the probability that you can form the   word Mississippi with the eleven chosen letters? Hint : it may be   helpful to number the eleven letters as $1, 2, . . . , 11$. This is how I approached it: the number of possible outcomes is $11!$, since we're taking one word at a time without replacement, so there are $11$ choices for the first letter, $10$ choices for the second letter and so on. Now, as to the number of 'success' outcomes, I noticed that there are $4$ s's to choose from, $4$ i's, $1$ M and $2$ p's. So, in order to form the word Mississippi, we have for the first letter $1$ option, $4$ for the second and third letters, $3$ for the fourth (since we've already used one ""s"") and so on, which amounts to a total of $4^2*3^2*2^3=1152$ different ways of doing so. However, my answer does not match the one provided in my book (Henk Tijn's Understanding Probability 3rd edition). What am I doing wrong? Thanks very much in advance.",,['probability']
32,Convergence of maximum of iid random variables in distribution,Convergence of maximum of iid random variables in distribution,,"Given $X_i, i \geq 1$ iid random variables, with mean zero and variance one, I would like  to show that $$ M_n = \max_{1 \leq k \leq n}\left\{\frac{|X_k|}{\sqrt n}\right\} \xrightarrow{d} 0$$ as $n \to \infty$. I know that $P(M_n \leq x) = \big(P(|X_1| \leq x \sqrt n)\big)^{n}$. However, as $n \to  \infty$, it may not be necessary that this goes to one for positive $x$. I tried using Chebyshev's inequality after finding the distribution functions, but the factors coming from the variance and the scaling factor cancel out, so the right hand side is not meaningful there. I would like to see if I can use characteristic functions to do this (find the characteristic function of $M_n$, and show that it goes to $1$ everywhere). But I was unable to proceed this way as well.","Given $X_i, i \geq 1$ iid random variables, with mean zero and variance one, I would like  to show that $$ M_n = \max_{1 \leq k \leq n}\left\{\frac{|X_k|}{\sqrt n}\right\} \xrightarrow{d} 0$$ as $n \to \infty$. I know that $P(M_n \leq x) = \big(P(|X_1| \leq x \sqrt n)\big)^{n}$. However, as $n \to  \infty$, it may not be necessary that this goes to one for positive $x$. I tried using Chebyshev's inequality after finding the distribution functions, but the factors coming from the variance and the scaling factor cancel out, so the right hand side is not meaningful there. I would like to see if I can use characteristic functions to do this (find the characteristic function of $M_n$, and show that it goes to $1$ everywhere). But I was unable to proceed this way as well.",,"['probability', 'probability-theory', 'central-limit-theorem']"
33,Independence vs Marginal independence,Independence vs Marginal independence,,"I known that for give $x,y$ , if we have $$p(x,y)=p(x)p(y)$$ Then we call $x,y$ are independent. For marginal independence, I found the definition here . Random variable $x$ is marginal independent of random variable $y$ if: $$p(x|y)=p(x)$$ However, I cannot see the difference between them. The question come to my mind when I want to figure out a question about Bayesian network. In 3-way Bayesian network, there are three nodes A,B,C in a common parent struct. A     | |    B   C A is the parent of B and C. It says B and C are conditional independent given A, and I can see it because: $$P(B|A,C)=\frac{P(A,B,C)}{P(A,C)}\\=\frac{P(A)P(B|A)P(C|A)}{P(A,C)}\\=P(B|A)$$ But , when the value of A is unknown, why $B,C$ are not independent?","I known that for give , if we have Then we call are independent. For marginal independence, I found the definition here . Random variable is marginal independent of random variable if: However, I cannot see the difference between them. The question come to my mind when I want to figure out a question about Bayesian network. In 3-way Bayesian network, there are three nodes A,B,C in a common parent struct. A     | |    B   C A is the parent of B and C. It says B and C are conditional independent given A, and I can see it because: But , when the value of A is unknown, why are not independent?","x,y p(x,y)=p(x)p(y) x,y x y p(x|y)=p(x) P(B|A,C)=\frac{P(A,B,C)}{P(A,C)}\\=\frac{P(A)P(B|A)P(C|A)}{P(A,C)}\\=P(B|A) B,C","['probability', 'independence', 'bayesian-network']"
34,Rock Paper Scissors Team Building with Rule Change,Rock Paper Scissors Team Building with Rule Change,,"There is a simple icebreaker game where a group of people compete in a rock-paper-scissors (RPS) tournament. When person A beats person B, they become a team where person A is the leader. And in general, when team A beats team B, they all become one team, and A's leader becomes the leader of the new team. This game always ends quickly with everyone on the same team. As Ross Millikan pointed out in the comments, it always takes exactly $n-1$ games. But I'm wondering what would happen if we changed the rules slightly. Suppose team A beats team B. Then team A acquires B's leader, and the rest of team B dissolves into a bunch of teams of 1. For example, if a team of 4 beats a team of 6, then it will result in a team of 5 and 5 teams of 1. It makes sense that this game would last longer because teams can be dissolved. I am specifically interested in finding out the average number of RPS games played in a tournament of $n$ people (they all start out as teams of 1) before there is only 1 team. To make this number well defined, I assume the following: Only one RPS game is played at a time. Each RPS game is played between a random pair of teams, with equal probabilities. The result of each RPS game is completely random (50/50). (Perhaps rock paper scissors was a bad analogy because it has ties. Just imagine that ties are played out until there is a winner, and that we're not counting ties in our running total of RPS games.) I have constructed Markov Chains for a few values of $n$ where the nodes are the different partitions of teams. Using them, I have found that the average number of RPS games played seems to be $$2^{n-1}-1$$ I have no idea why this would be, or how to prove it, but maybe it is just a coincidence that it works for $1\le n\le6$ (or maybe I calculated them wrong). Can anyone prove a formula for the average number of RPS games? I would also appreciate it if anyone can programmatically verify my formula for larger values of $n$.","There is a simple icebreaker game where a group of people compete in a rock-paper-scissors (RPS) tournament. When person A beats person B, they become a team where person A is the leader. And in general, when team A beats team B, they all become one team, and A's leader becomes the leader of the new team. This game always ends quickly with everyone on the same team. As Ross Millikan pointed out in the comments, it always takes exactly $n-1$ games. But I'm wondering what would happen if we changed the rules slightly. Suppose team A beats team B. Then team A acquires B's leader, and the rest of team B dissolves into a bunch of teams of 1. For example, if a team of 4 beats a team of 6, then it will result in a team of 5 and 5 teams of 1. It makes sense that this game would last longer because teams can be dissolved. I am specifically interested in finding out the average number of RPS games played in a tournament of $n$ people (they all start out as teams of 1) before there is only 1 team. To make this number well defined, I assume the following: Only one RPS game is played at a time. Each RPS game is played between a random pair of teams, with equal probabilities. The result of each RPS game is completely random (50/50). (Perhaps rock paper scissors was a bad analogy because it has ties. Just imagine that ties are played out until there is a winner, and that we're not counting ties in our running total of RPS games.) I have constructed Markov Chains for a few values of $n$ where the nodes are the different partitions of teams. Using them, I have found that the average number of RPS games played seems to be $$2^{n-1}-1$$ I have no idea why this would be, or how to prove it, but maybe it is just a coincidence that it works for $1\le n\le6$ (or maybe I calculated them wrong). Can anyone prove a formula for the average number of RPS games? I would also appreciate it if anyone can programmatically verify my formula for larger values of $n$.",,"['probability', 'markov-chains']"
35,Defining a probability on the natural numbers set,Defining a probability on the natural numbers set,,"The standard definition of a probability on a set A is as a sigma-additive function from a sigma-algebra constructed on the power set of A to the extended real positive line (also called a measure), such that the measure of the whole set A is 1. If A is the set of real numbers, such a definition cannot be made in such a way that any set consisting of only one natural number has equal probability, for, if mu({n}) = c > 0 for any n natural (where mu is the measure), then any infinite set would have infinite measure, and, in particular, mu(A) != 1. But it is trully natural to think, for example, about the probability of certain events relating to the choice of a random natural number, such as the probability of getting an even number out of a random choice of a natural number, which would be 1/2. This kind of probability should satisfy the above condition of having odds 0 of getting any particular natural number. I searched the forum and saw that many people work with probability on the natural numbers by relaxing the assumption of the probability being sigma-additive to just being additive. This seems to solve the problem, although I still didn't find an algebra and a probability function which would seem satisfying (but I guess it is not so hard). The question, then, is what exactly one misses when relaxing the assumption of sigma-additivity. Does a definition of probability in the subsets of natural numbers via a finitely additive set function lead to inconsistencies? What theorems about probability stop applying under such a definition? Also, am I being to restrictive in my general definition of probability, and is it acceptable in modern mathematical culture to define probabilities via only finitely aditive functions?","The standard definition of a probability on a set A is as a sigma-additive function from a sigma-algebra constructed on the power set of A to the extended real positive line (also called a measure), such that the measure of the whole set A is 1. If A is the set of real numbers, such a definition cannot be made in such a way that any set consisting of only one natural number has equal probability, for, if mu({n}) = c > 0 for any n natural (where mu is the measure), then any infinite set would have infinite measure, and, in particular, mu(A) != 1. But it is trully natural to think, for example, about the probability of certain events relating to the choice of a random natural number, such as the probability of getting an even number out of a random choice of a natural number, which would be 1/2. This kind of probability should satisfy the above condition of having odds 0 of getting any particular natural number. I searched the forum and saw that many people work with probability on the natural numbers by relaxing the assumption of the probability being sigma-additive to just being additive. This seems to solve the problem, although I still didn't find an algebra and a probability function which would seem satisfying (but I guess it is not so hard). The question, then, is what exactly one misses when relaxing the assumption of sigma-additivity. Does a definition of probability in the subsets of natural numbers via a finitely additive set function lead to inconsistencies? What theorems about probability stop applying under such a definition? Also, am I being to restrictive in my general definition of probability, and is it acceptable in modern mathematical culture to define probabilities via only finitely aditive functions?",,"['probability', 'elementary-number-theory', 'measure-theory']"
36,Why does probability need the Axiom of pairwise disjoint events? [duplicate],Why does probability need the Axiom of pairwise disjoint events? [duplicate],,"This question already has answers here : Why do we want probabilities to be *countably* additive? (2 answers) Closed 8 years ago . I'm a beginning student of Probability and Statistics and I've been reading the book Elementary Probability for Applications by Rick Durret. In this book, he outlines the 4 Axioms of Probability. For any event $A$, $0 \leq P (A) \leq 1$. If $\Omega $ is the sample space then $P (\Omega) =1$. If $A$ and $B$ are disjoint, that is, the intersection $A \cap B = \emptyset$, then $$P(A\cup B) = P(A) + P(B)$$ If $A_1, A_2,\ldots$, is an infinite sequence of pairwise disjoint events (that is, $A_i\cap A_j = \emptyset$ when $i \neq j $) then  $$P\left(\bigcup_{i=1}^\infty A_i\right)=\sum_{i=1}^\infty P(A_i).$$ The book fails to explain why we need Axiom 4. I have tried searching on Wikipedia but I haven't had any luck. I don't understand how we can have a probability of disjoint infinite events. The book states that when the you have infinitely many events, the last argument breaks down and this is now a new assumption. But then the book states that we need this or else the theory of Probability becomes useless. I was wondering if there were any intuitive examples of situations where this fourth axiom applies. Why is it so important for probabilty theory? And why does the author state that not everyone believes we should use this axiom.","This question already has answers here : Why do we want probabilities to be *countably* additive? (2 answers) Closed 8 years ago . I'm a beginning student of Probability and Statistics and I've been reading the book Elementary Probability for Applications by Rick Durret. In this book, he outlines the 4 Axioms of Probability. For any event $A$, $0 \leq P (A) \leq 1$. If $\Omega $ is the sample space then $P (\Omega) =1$. If $A$ and $B$ are disjoint, that is, the intersection $A \cap B = \emptyset$, then $$P(A\cup B) = P(A) + P(B)$$ If $A_1, A_2,\ldots$, is an infinite sequence of pairwise disjoint events (that is, $A_i\cap A_j = \emptyset$ when $i \neq j $) then  $$P\left(\bigcup_{i=1}^\infty A_i\right)=\sum_{i=1}^\infty P(A_i).$$ The book fails to explain why we need Axiom 4. I have tried searching on Wikipedia but I haven't had any luck. I don't understand how we can have a probability of disjoint infinite events. The book states that when the you have infinitely many events, the last argument breaks down and this is now a new assumption. But then the book states that we need this or else the theory of Probability becomes useless. I was wondering if there were any intuitive examples of situations where this fourth axiom applies. Why is it so important for probabilty theory? And why does the author state that not everyone believes we should use this axiom.",,"['probability', 'probability-theory', 'statistics', 'axioms']"
37,How to apply triangle inequality in a probability statement?,How to apply triangle inequality in a probability statement?,,"$$\begin{align} P\left(\left| \bar{Y}+X_n-\mu\right| \ge \delta \right) &\le P\left( \left|\bar{Y}-\mu\right| \ge \frac{\delta}{2} \cup \left|X_n\right| \ge \frac{\delta}{2} \right)  \tag{1} \\ & \le P\left( \left|\bar{Y}-\mu\right| \ge \frac{\delta}{2} \right) + \left( \left|X_n\right| \ge \frac{\delta}{2} \right) \tag{2} \end{align}$$ I don't know how to get the right hand side of (1). I do know that by the triangle inequality: $$\left| \bar{Y}+X_n-\mu\right|\le \left| \bar{Y}-\mu\right| + \left|X_n\right|$$ But am unsure how that translate in terms of probability. This is the first time I am seeing such an application of the triangle inequality and my text simply states it without further explanation. Please advice. Also, would such an inequality hold if the relation in $P(\dots)$ was $\le$ instead?","$$\begin{align} P\left(\left| \bar{Y}+X_n-\mu\right| \ge \delta \right) &\le P\left( \left|\bar{Y}-\mu\right| \ge \frac{\delta}{2} \cup \left|X_n\right| \ge \frac{\delta}{2} \right)  \tag{1} \\ & \le P\left( \left|\bar{Y}-\mu\right| \ge \frac{\delta}{2} \right) + \left( \left|X_n\right| \ge \frac{\delta}{2} \right) \tag{2} \end{align}$$ I don't know how to get the right hand side of (1). I do know that by the triangle inequality: $$\left| \bar{Y}+X_n-\mu\right|\le \left| \bar{Y}-\mu\right| + \left|X_n\right|$$ But am unsure how that translate in terms of probability. This is the first time I am seeing such an application of the triangle inequality and my text simply states it without further explanation. Please advice. Also, would such an inequality hold if the relation in $P(\dots)$ was $\le$ instead?",,"['probability', 'inequality']"
38,$L^2$ limit of Gaussian random variables,limit of Gaussian random variables,L^2,"Let $X_n$ be a sequence of Gaussian random variables defined on the same probability space. The statement is that if $X_n$ converges to some random variable $X$ in $L^2$-sense, then $X$ is also Gaussian. I think it is possible to do with Characteristic functions; but I am curious what is the simplest way to see it.","Let $X_n$ be a sequence of Gaussian random variables defined on the same probability space. The statement is that if $X_n$ converges to some random variable $X$ in $L^2$-sense, then $X$ is also Gaussian. I think it is possible to do with Characteristic functions; but I am curious what is the simplest way to see it.",,"['probability', 'convergence-divergence', 'normal-distribution']"
39,The probability of an ace from a 5-card hand?,The probability of an ace from a 5-card hand?,,"Is it $\cfrac{\binom{4}{1} \cdot \binom{48}{4}}{\binom{52}{5}}$? The way to choose 1 of 4 aces * the way to choose 4 cards from the remaining non-aces, divided by 52 choose 5 (total)? I know it's the total of the probability of the event over the probability of the total, but I'm not sure about the top part.","Is it $\cfrac{\binom{4}{1} \cdot \binom{48}{4}}{\binom{52}{5}}$? The way to choose 1 of 4 aces * the way to choose 4 cards from the remaining non-aces, divided by 52 choose 5 (total)? I know it's the total of the probability of the event over the probability of the total, but I'm not sure about the top part.",,"['probability', 'card-games']"
40,"show that if $X\ge 0$ , $E(X)\le \sum_{n=0}^{\infty}P(X>n)$.","show that if  , .",X\ge 0 E(X)\le \sum_{n=0}^{\infty}P(X>n),"if $X$ is a random variable and also let $X\ge 0$ , I want to show $E(X)\le \sum_{n=0}^{\infty}P(X>n)$.","if $X$ is a random variable and also let $X\ge 0$ , I want to show $E(X)\le \sum_{n=0}^{\infty}P(X>n)$.",,"['probability', 'probability-theory', 'expectation']"
41,To what extent are divisibility by different primes independent?,To what extent are divisibility by different primes independent?,,"Let's prove: the probability that two randomly chosen integers are relatively prime is $ \frac{6}{\pi^2} $. and a ""proof"" by separating relative prime-ness into a product of indendent events Let $p$ be a prime. The probability of not dividing two random numbers $p \nmid m $ and $p \nmid n $ is $1 - \frac{1}{p^2}$ The probability of $m$ and $n$ having no common prime factor is the product over all prime divisors $\displaystyle \prod_p \left( 1 - \frac{1}{p^2} \right) = \left(\sum_{n \geq 1} \frac{1}{n^2}\right)^{-1} = \frac{6}{\pi^2}$. I am committing all kinds of probability and number theory errors but the ""idea"" is correct. How do I improve this statement and proof to make them more rigorous? I could say: $\displaystyle \lim_{N \to \infty} \frac{\{ 0\leq m, n \leq N :(m,n) = 1  \}}{N^2} = \frac{6}{\pi^2}$ How do I justify that divisibility by $p$ and divisibility by $q$ are independent?  For any number of primes: $$ \mathbb{P}\big( p_1 \dots p_n \big| \; m  \big) = \prod_{i=1}^n \mathbb{P}\big(p_i\big| \; m \big)$$ Or can we build a counter example? UPDATE :  It seems to have to do with checking the $\displaystyle \lim_{N \to \infty}$ holds for arbitrary increasing sequences of integers $0< N_1 < N_2 < N_3 < \dots $ This question appears in Math.SE a bunch of times from different angles: Probability that two random numbers are coprime On the probability that two positive integers are relatively prime Probability that a vector in $\mathbb{Z}^n$ is primitive Probability that $x \equiv 3 \pmod{4}$","Let's prove: the probability that two randomly chosen integers are relatively prime is $ \frac{6}{\pi^2} $. and a ""proof"" by separating relative prime-ness into a product of indendent events Let $p$ be a prime. The probability of not dividing two random numbers $p \nmid m $ and $p \nmid n $ is $1 - \frac{1}{p^2}$ The probability of $m$ and $n$ having no common prime factor is the product over all prime divisors $\displaystyle \prod_p \left( 1 - \frac{1}{p^2} \right) = \left(\sum_{n \geq 1} \frac{1}{n^2}\right)^{-1} = \frac{6}{\pi^2}$. I am committing all kinds of probability and number theory errors but the ""idea"" is correct. How do I improve this statement and proof to make them more rigorous? I could say: $\displaystyle \lim_{N \to \infty} \frac{\{ 0\leq m, n \leq N :(m,n) = 1  \}}{N^2} = \frac{6}{\pi^2}$ How do I justify that divisibility by $p$ and divisibility by $q$ are independent?  For any number of primes: $$ \mathbb{P}\big( p_1 \dots p_n \big| \; m  \big) = \prod_{i=1}^n \mathbb{P}\big(p_i\big| \; m \big)$$ Or can we build a counter example? UPDATE :  It seems to have to do with checking the $\displaystyle \lim_{N \to \infty}$ holds for arbitrary increasing sequences of integers $0< N_1 < N_2 < N_3 < \dots $ This question appears in Math.SE a bunch of times from different angles: Probability that two random numbers are coprime On the probability that two positive integers are relatively prime Probability that a vector in $\mathbb{Z}^n$ is primitive Probability that $x \equiv 3 \pmod{4}$",,"['probability', 'number-theory']"
42,Expected Value Function,Expected Value Function,,"My text-book defines expected value as $$E(X) = \mu_x = \sum_{x \in D} ~x  \cdot p(x)$$  And so, if I was to find the expected value of a random variable $X$, where $X = 1,2,3$, then it would resemble this:  $$E(X)= \sum_{x=1}^3~ x \cdot p(x)= 1\cdot p(1) + 2\cdot p(2) + 3 \cdot p(3)$$  Furthermore, if I wanted to calculate $E(X^2)$, it would be $E(X^2) = 1^2 \cdot P(1) + 2^2 \cdot p(2) + 3^2 \cdot p(3)$. My question is, why don't we square the x-values in the probability function $p(x)$? Also, is computing the expected value a way of calculating the average of the random variable? It seems a little odd to calculate it that way. PS: If any use of notation, or vocabulary, is incorrect, please inform me.","My text-book defines expected value as $$E(X) = \mu_x = \sum_{x \in D} ~x  \cdot p(x)$$  And so, if I was to find the expected value of a random variable $X$, where $X = 1,2,3$, then it would resemble this:  $$E(X)= \sum_{x=1}^3~ x \cdot p(x)= 1\cdot p(1) + 2\cdot p(2) + 3 \cdot p(3)$$  Furthermore, if I wanted to calculate $E(X^2)$, it would be $E(X^2) = 1^2 \cdot P(1) + 2^2 \cdot p(2) + 3^2 \cdot p(3)$. My question is, why don't we square the x-values in the probability function $p(x)$? Also, is computing the expected value a way of calculating the average of the random variable? It seems a little odd to calculate it that way. PS: If any use of notation, or vocabulary, is incorrect, please inform me.",,"['probability', 'definition']"
43,What is the Nash Equilibrium of the Monty Hall Problem?,What is the Nash Equilibrium of the Monty Hall Problem?,,"The Monty Hall problem or paradox is famous and well-studied. But what confused me about the description was an unstated assumption. Suppose you're on a game show, and you're given the choice of three   doors: behind one door   is a car; behind the others, goats. You pick   a door, say No. 1, and the host, who knows what's behind the doors,   opens another door, say No. 3, which has a goat. He then says to you,   ""Do you want to pick door No. 2?"" Is it to your advantage to switch   your choice? The assumption is that the host of the show does not have a choice whether to offer the switch. In fact, Monty Hall himself, in response to Steve Selvin's original formulation of the problem, pointed out that as the host he did not always offer the switch. Because the host knows what's behind the doors, it would be possible and to his advantage to offer a switch more often to contestants who guess correctly. If he only offered the switch to contestants who guess correctly, all contestants who accept the offer would lose. However, if he did this consistently, the public would learn not to accept the offer and soon all contestants who first guess correctly would win. If, for instance, he gave the offer to one third of incorrect guessers and two thirds of correct guessers, 2/9 contestants would be given the offer and should not switch and 2/9 contestants would be given the offer and should, which would bring the chances of winning back to 1/2 whether one accepts the offer or not, instead of 1/3 or 2/3. Is this a Nash equilibrium for the Monty Hall problem (or the iterated Monty Hall problem) as a two-player zero-sum game?  And if not, what is it, or aren't there any?","The Monty Hall problem or paradox is famous and well-studied. But what confused me about the description was an unstated assumption. Suppose you're on a game show, and you're given the choice of three   doors: behind one door   is a car; behind the others, goats. You pick   a door, say No. 1, and the host, who knows what's behind the doors,   opens another door, say No. 3, which has a goat. He then says to you,   ""Do you want to pick door No. 2?"" Is it to your advantage to switch   your choice? The assumption is that the host of the show does not have a choice whether to offer the switch. In fact, Monty Hall himself, in response to Steve Selvin's original formulation of the problem, pointed out that as the host he did not always offer the switch. Because the host knows what's behind the doors, it would be possible and to his advantage to offer a switch more often to contestants who guess correctly. If he only offered the switch to contestants who guess correctly, all contestants who accept the offer would lose. However, if he did this consistently, the public would learn not to accept the offer and soon all contestants who first guess correctly would win. If, for instance, he gave the offer to one third of incorrect guessers and two thirds of correct guessers, 2/9 contestants would be given the offer and should not switch and 2/9 contestants would be given the offer and should, which would bring the chances of winning back to 1/2 whether one accepts the offer or not, instead of 1/3 or 2/3. Is this a Nash equilibrium for the Monty Hall problem (or the iterated Monty Hall problem) as a two-player zero-sum game?  And if not, what is it, or aren't there any?",,"['probability', 'game-theory', 'nash-equilibrium', 'monty-hall']"
44,Weak convergence of probability measure,Weak convergence of probability measure,,"I am working on a problem. Show that for each probability measure $\mu$, there exists probability measure $\mu_n$ with finite support such that $\mu_n$ converges weakly to $\mu$. I am thinking about the empirical measure, which has the distribution function $F_n(t)=1/n\sum 1(X_i\le t)$. So from LLN, we have $F_n(t)\rightarrow F(t)$ for each fixed $t$ a.s. But the convergence here is almost surely. So does this still means $F_n$ converges to $F$ weakly?","I am working on a problem. Show that for each probability measure $\mu$, there exists probability measure $\mu_n$ with finite support such that $\mu_n$ converges weakly to $\mu$. I am thinking about the empirical measure, which has the distribution function $F_n(t)=1/n\sum 1(X_i\le t)$. So from LLN, we have $F_n(t)\rightarrow F(t)$ for each fixed $t$ a.s. But the convergence here is almost surely. So does this still means $F_n$ converges to $F$ weakly?",,"['probability', 'measure-theory', 'probability-theory']"
45,The Tuesday Birthday Problem - why does the probability change when the father specifies the birthday of a son? [duplicate],The Tuesday Birthday Problem - why does the probability change when the father specifies the birthday of a son? [duplicate],,"This question already has answers here : Boy Born on a Tuesday - is it just a language trick? (12 answers) Closed 9 years ago . I've most recently read about the Tuesday Boy Problem via twitter and I, as probably most other people, was sure that the probability has to be 1/2. After having read through a lot of solutions which were not identical at all, I've come to the conclusion that $P = \frac{13}{27}$ sounds the most reasonable. The corresponding argument was as follows: Say the older child is the boy born on Tuesday. Then, if the younger child is female, there are seven possibilities and analogous if the younger child is male. In case the younger child is the boy born on Tuesday, for an older daughter, again there are seven possibilities, but for an older son, there are only six, because the case that the older son was born on Tuesday has already been counted. Thus, $$P = \frac{6+7}{6+7+7+7} = \frac{13}{27}. $$ My first question is: Is this the correct solution? I've found other websites giving different solutions, however I could never agree with any of those. In case this is the correct solution: Why does the probability change when the father specifies the birthday of a son? (does it actually change? A lot of answers/posts stated that the statement does matter) What I mean is: It is clear that (in case he has a son) his son is born on some day of the week. I could replace Tuesday with any day of the week and the probability would be the same. Say the father would have stated: I have two children. (At least) One of them is a son. Then, without loss of generality, we could say that this son is born on a Tuesday and again we would have $P = \frac{13}{27}$ . But looking at an equivalent (?) problem, we get a completely different probability: Let's toss two coins consecutively and say heads is equivalent to ""son"" and tails to ""daughter"". Then, if we know that we tossed at least one heads, if it was the first tossed coin, the probability for another heads is 1/2. If the second coin was heads, then we can only take the case in account where the first coin was tails. So the probability for two heads is $P = \frac{1}{3}$ . So my second (actually third) question is: Where did I go wrong? Lastly I want to ask (as my knowledge in probability/statistics is limited to what I have been taught in high school) whether the argument which gives $P=\frac{13}{27}$ for the original Tuesday Birthday Problem already takes the possibility of twins in consideration. Can it be regarded as a special case of two boys both born on Tuesday (I ask because in every argument leading to this probability there was a distinction between older child = boy vs. younger child = boy)? Thank you very much in advance for any answer.","This question already has answers here : Boy Born on a Tuesday - is it just a language trick? (12 answers) Closed 9 years ago . I've most recently read about the Tuesday Boy Problem via twitter and I, as probably most other people, was sure that the probability has to be 1/2. After having read through a lot of solutions which were not identical at all, I've come to the conclusion that sounds the most reasonable. The corresponding argument was as follows: Say the older child is the boy born on Tuesday. Then, if the younger child is female, there are seven possibilities and analogous if the younger child is male. In case the younger child is the boy born on Tuesday, for an older daughter, again there are seven possibilities, but for an older son, there are only six, because the case that the older son was born on Tuesday has already been counted. Thus, My first question is: Is this the correct solution? I've found other websites giving different solutions, however I could never agree with any of those. In case this is the correct solution: Why does the probability change when the father specifies the birthday of a son? (does it actually change? A lot of answers/posts stated that the statement does matter) What I mean is: It is clear that (in case he has a son) his son is born on some day of the week. I could replace Tuesday with any day of the week and the probability would be the same. Say the father would have stated: I have two children. (At least) One of them is a son. Then, without loss of generality, we could say that this son is born on a Tuesday and again we would have . But looking at an equivalent (?) problem, we get a completely different probability: Let's toss two coins consecutively and say heads is equivalent to ""son"" and tails to ""daughter"". Then, if we know that we tossed at least one heads, if it was the first tossed coin, the probability for another heads is 1/2. If the second coin was heads, then we can only take the case in account where the first coin was tails. So the probability for two heads is . So my second (actually third) question is: Where did I go wrong? Lastly I want to ask (as my knowledge in probability/statistics is limited to what I have been taught in high school) whether the argument which gives for the original Tuesday Birthday Problem already takes the possibility of twins in consideration. Can it be regarded as a special case of two boys both born on Tuesday (I ask because in every argument leading to this probability there was a distinction between older child = boy vs. younger child = boy)? Thank you very much in advance for any answer.",P = \frac{13}{27} P = \frac{6+7}{6+7+7+7} = \frac{13}{27}.  P = \frac{13}{27} P = \frac{1}{3} P=\frac{13}{27},['probability']
46,Expected difference between the largest and second largest observations in a sample of i.i.d. normal variables,Expected difference between the largest and second largest observations in a sample of i.i.d. normal variables,,"Let $X_1,\dots,X_n$ be an i.i.d. sample from the standard normal distribution. Let \begin{align} \mu_n = \mathbb{E}[X_{(n)} - X_{(n-1)}],  \end{align} be the expected difference between the largest and second largest observations. Question : Can we show that $\mu_n$ is decreasing in $n$ ? In this answer it is shown that $\mu_n$ is asymptotically decreasing, i.e., it is decreasing for large enough $n$ , and in this answer it is argued numerically that $\mu_n$ is also decreasing for finite $n$ . A good first step is to use a known order statistics formula (see e.g. this answer ) to write \begin{align} \mu_n = n\int_{-\infty}^{\infty}\Phi^{n-1}(x)(1-\Phi(x))dx, \end{align} where $\Phi$ is the standard normal CDF. However, I am unable to show from this that $\mu_n$ is decreasing. Addition: The intuition provided in a previously linked answer is that $\mu_n$ is decreasing if the $X_i$ 's come from a thin-tailed distributions like the normal distribution, and that $\mu_n$ is constant or even increasing if the $X_i$ 's come from a more heavy-tailed distribution (in particular, it is easy to show that $\mu_n$ is constant if the $X_i$ 's come from the exponential distribution). Perhaps this intuition can help in showing that $\mu_n$ is decreasing for the normal distribution.","Let be an i.i.d. sample from the standard normal distribution. Let be the expected difference between the largest and second largest observations. Question : Can we show that is decreasing in ? In this answer it is shown that is asymptotically decreasing, i.e., it is decreasing for large enough , and in this answer it is argued numerically that is also decreasing for finite . A good first step is to use a known order statistics formula (see e.g. this answer ) to write where is the standard normal CDF. However, I am unable to show from this that is decreasing. Addition: The intuition provided in a previously linked answer is that is decreasing if the 's come from a thin-tailed distributions like the normal distribution, and that is constant or even increasing if the 's come from a more heavy-tailed distribution (in particular, it is easy to show that is constant if the 's come from the exponential distribution). Perhaps this intuition can help in showing that is decreasing for the normal distribution.","X_1,\dots,X_n \begin{align}
\mu_n = \mathbb{E}[X_{(n)} - X_{(n-1)}], 
\end{align} \mu_n n \mu_n n \mu_n n \begin{align}
\mu_n = n\int_{-\infty}^{\infty}\Phi^{n-1}(x)(1-\Phi(x))dx,
\end{align} \Phi \mu_n \mu_n X_i \mu_n X_i \mu_n X_i \mu_n","['probability', 'probability-theory', 'statistics', 'stochastic-processes', 'order-statistics']"
47,The probability that the minimum number is unique,The probability that the minimum number is unique,,"If I draw $n$ integers $X$ from $[1, n]$ uniformly at random, what is the probability that $\min\left(X\right)$ is unique, i.e. that there is only one number in $X$ that is $\min\left(X\right)$ ? I tried to find a closed form solution for this, but I have not been successful. I've tried to estimate the number both with sampling and also exactly, and it seems like it converges towards something like $< 0.59$ , but I have no idea what this number is. More generally, I would like to find the probability that the minimum is unique if I draw $n$ integers from $[1, r]$ for some integers $n \in \mathbb{N}$ and $r \in \mathbb{N}$ with $n \leq r$ . (Source: I want to estimate the success rate of an algorithm suggested as a solution to a question on cs.sx ).","If I draw integers from uniformly at random, what is the probability that is unique, i.e. that there is only one number in that is ? I tried to find a closed form solution for this, but I have not been successful. I've tried to estimate the number both with sampling and also exactly, and it seems like it converges towards something like , but I have no idea what this number is. More generally, I would like to find the probability that the minimum is unique if I draw integers from for some integers and with . (Source: I want to estimate the success rate of an algorithm suggested as a solution to a question on cs.sx ).","n X [1, n] \min\left(X\right) X \min\left(X\right) < 0.59 n [1, r] n \in \mathbb{N} r \in \mathbb{N} n \leq r","['probability', 'combinatorics']"
48,Coin Flipping Riddle,Coin Flipping Riddle,,"2 criminals A and B, were recently captured and brought to prison. They were then locked in two separate rooms. Known for being exceedingly smart, the prison warden set a test for them. They flip a fair coin an infinite number of times and told outcomes of odd trials to A and even trials to B. Now A and B are separately told to pick a trial whose outcome they don't know, i.e., A is supposed to pick an even number, and B is supposed to pick an odd number. If the outcomes of the trials A and B picked are the same, the prison warden will release them. If they are different, they will spend life in jail. The Warden told them what they were going to do and let them agree upon a common strategy in advance but after that they can't communicate. Find a strategy such that the chance of winning is higher than $0.5$ . I recently tried this problem in university and I am interested to see other people's solutions, and perhaps the optimal solution. Edit: My strategy is that both players agree that as soon as they see 2 tails in a row for themselves, they pick the number in the middle, i.e. A sees a tail on coin flip 2 and 4 so he picks 3, B does the same. After running this on a computer simulation I get a 60% winrate. Although I don't fully understand why. Edit: a 70% chance of winning has been found on my Puzzling StackExchange duplicate post! Strategies so Far 70% chance of winning by @Jaap Scherphuis 2/3 probability of winning by @Mike Earnest 62.5% chance of winning by @Teo Miklethun","2 criminals A and B, were recently captured and brought to prison. They were then locked in two separate rooms. Known for being exceedingly smart, the prison warden set a test for them. They flip a fair coin an infinite number of times and told outcomes of odd trials to A and even trials to B. Now A and B are separately told to pick a trial whose outcome they don't know, i.e., A is supposed to pick an even number, and B is supposed to pick an odd number. If the outcomes of the trials A and B picked are the same, the prison warden will release them. If they are different, they will spend life in jail. The Warden told them what they were going to do and let them agree upon a common strategy in advance but after that they can't communicate. Find a strategy such that the chance of winning is higher than . I recently tried this problem in university and I am interested to see other people's solutions, and perhaps the optimal solution. Edit: My strategy is that both players agree that as soon as they see 2 tails in a row for themselves, they pick the number in the middle, i.e. A sees a tail on coin flip 2 and 4 so he picks 3, B does the same. After running this on a computer simulation I get a 60% winrate. Although I don't fully understand why. Edit: a 70% chance of winning has been found on my Puzzling StackExchange duplicate post! Strategies so Far 70% chance of winning by @Jaap Scherphuis 2/3 probability of winning by @Mike Earnest 62.5% chance of winning by @Teo Miklethun",0.5,"['probability', 'puzzle']"
49,Ratio of two binomial distributions,Ratio of two binomial distributions,,"How to estimate $$ E\left[\frac{X}{X+Y}\right] $$ for two independent random variables $X\sim Bin(n,p)$ and $Y\sim Bin(m,p)$ ? Are there any connection with $\frac{n}{n+m}$ e.g., $1-\varepsilon\leq E\left[\frac{X}{X+Y}\right]/\frac{n}{n+m}\leq 1+\varepsilon$?","How to estimate $$ E\left[\frac{X}{X+Y}\right] $$ for two independent random variables $X\sim Bin(n,p)$ and $Y\sim Bin(m,p)$ ? Are there any connection with $\frac{n}{n+m}$ e.g., $1-\varepsilon\leq E\left[\frac{X}{X+Y}\right]/\frac{n}{n+m}\leq 1+\varepsilon$?",,"['probability', 'binomial-distribution', 'ratio']"
50,Constrained optimization using calculus of variations (entropy maximization),Constrained optimization using calculus of variations (entropy maximization),,"Please note that I don't know (almost) anything about the calculus of variations, and I'm familiar with analysis only at an undergraduate level (i.e., Baby-Rudin level). Let $[a, b] \subset \mathbb{R}$, and $p: [a,b] \to \mathbb{R}_{\geq 0}$ be such that $\int_{\mathbb{R}}p = 1$. Let $$H(p) = -\int_{\mathbb{R}}p\log p\text{.}$$ We define $p(x)\log[p(x)] = 0$ whenever $p(x) = 0$. Consider the problem $$\max H(p) \text{ subject to }\int_{\mathbb{R}}p = 1\text{.}$$ (This is the uncountable-set analogue of a previous question I asked .) According to this textbook I have, this is solved using the calculus of variations. Every time I read something about the calculus of variations, the Euler-Lagrange equations always pop up. However, this is slightly different from most of the examples I've seen online, in that there is a constraint applied. I know that the solution is apparently (and this is provided in the textbook) $$p(x) = \dfrac{1}{b-a}\mathbf{1}_{[a, b]}(x)$$ where $\mathbf{1}_{A}(x) = 1$ for $x \in A$, and $0$ otherwise. But I'm not sure at all how to show this.","Please note that I don't know (almost) anything about the calculus of variations, and I'm familiar with analysis only at an undergraduate level (i.e., Baby-Rudin level). Let $[a, b] \subset \mathbb{R}$, and $p: [a,b] \to \mathbb{R}_{\geq 0}$ be such that $\int_{\mathbb{R}}p = 1$. Let $$H(p) = -\int_{\mathbb{R}}p\log p\text{.}$$ We define $p(x)\log[p(x)] = 0$ whenever $p(x) = 0$. Consider the problem $$\max H(p) \text{ subject to }\int_{\mathbb{R}}p = 1\text{.}$$ (This is the uncountable-set analogue of a previous question I asked .) According to this textbook I have, this is solved using the calculus of variations. Every time I read something about the calculus of variations, the Euler-Lagrange equations always pop up. However, this is slightly different from most of the examples I've seen online, in that there is a constraint applied. I know that the solution is apparently (and this is provided in the textbook) $$p(x) = \dfrac{1}{b-a}\mathbf{1}_{[a, b]}(x)$$ where $\mathbf{1}_{A}(x) = 1$ for $x \in A$, and $0$ otherwise. But I'm not sure at all how to show this.",,"['probability', 'calculus-of-variations', 'machine-learning', 'entropy']"
51,Solution conflict: Expected number of distinct birthdays for $100$ people,Solution conflict: Expected number of distinct birthdays for  people,100,"I was given a homework question that is stated in the title. Although I have a conflict with the solution provided, and was wondering if you could help me understand why the solution is correct or if it is indeed incorrect. Define $X$ to be number of distinct birthdays. The answer given is to set up a RV $X_i$ which is $1$ if the ith day is a birthday or $0$ otherwise, where: $P(X_i = 1) = P(\text{at least one person has birthday on day i}) = 1- P(\text{no one has birthday on this day}) = 1 - \frac{364}{365}^{100}$. And so $\mathrm{E}X_i = 1 - \frac{364}{365}^{100}$ Thus $\mathrm{E}X =\mathrm{E}[X_1 + X_2 \dots X_{365}] = 365\left (1 - \frac{364}{365}^{100} \right)$ I think this is incorrect, however. The reason being is that it seems like they are calculating the expected number of birthdays not the expected number of distinct birthdays. The answer that I think is correct is to define $X_i$ as $1$ if the ith day is a distinct birthday and $0$ otherwise. Then: $P(X_i = 1) = 100 \times \left(\frac{1}{365}\right)\left(\frac{364}{365}\right)^{99}$. Thus $\mathrm{E}X =\mathrm{E}[X_1 + X_2 \dots X_{365}] = 365 \times 100 \times \left(\frac{1}{365}\right)\left(\frac{364}{365}\right)^{99} = 100 \times \left(\frac{364}{365}\right)^{99}$. This has been bothering me for quite some time. Any help would be great.","I was given a homework question that is stated in the title. Although I have a conflict with the solution provided, and was wondering if you could help me understand why the solution is correct or if it is indeed incorrect. Define $X$ to be number of distinct birthdays. The answer given is to set up a RV $X_i$ which is $1$ if the ith day is a birthday or $0$ otherwise, where: $P(X_i = 1) = P(\text{at least one person has birthday on day i}) = 1- P(\text{no one has birthday on this day}) = 1 - \frac{364}{365}^{100}$. And so $\mathrm{E}X_i = 1 - \frac{364}{365}^{100}$ Thus $\mathrm{E}X =\mathrm{E}[X_1 + X_2 \dots X_{365}] = 365\left (1 - \frac{364}{365}^{100} \right)$ I think this is incorrect, however. The reason being is that it seems like they are calculating the expected number of birthdays not the expected number of distinct birthdays. The answer that I think is correct is to define $X_i$ as $1$ if the ith day is a distinct birthday and $0$ otherwise. Then: $P(X_i = 1) = 100 \times \left(\frac{1}{365}\right)\left(\frac{364}{365}\right)^{99}$. Thus $\mathrm{E}X =\mathrm{E}[X_1 + X_2 \dots X_{365}] = 365 \times 100 \times \left(\frac{1}{365}\right)\left(\frac{364}{365}\right)^{99} = 100 \times \left(\frac{364}{365}\right)^{99}$. This has been bothering me for quite some time. Any help would be great.",,"['probability', 'random-variables', 'expected-value', 'birthday']"
52,Pairwise independence implies independence for normal random variables,Pairwise independence implies independence for normal random variables,,"I'm reading a book on Brownian Motion. In the proof of the existence of such random function (Wiener, 1923), the following is stated: Indeed, all increments $B(d)-B(d-2^{-n})$, for $d\in \mathcal{D}_n\setminus \{0\}$, are independent. To see this it suffices to show that they are pairwise independent. as the vector of these increments is Gaussian. The last part of this quote is the claim that pairwise independent normal variables from a Gaussian family are independent. Could anyone provide/direct me to a proof of this claim? Thanks!","I'm reading a book on Brownian Motion. In the proof of the existence of such random function (Wiener, 1923), the following is stated: Indeed, all increments $B(d)-B(d-2^{-n})$, for $d\in \mathcal{D}_n\setminus \{0\}$, are independent. To see this it suffices to show that they are pairwise independent. as the vector of these increments is Gaussian. The last part of this quote is the claim that pairwise independent normal variables from a Gaussian family are independent. Could anyone provide/direct me to a proof of this claim? Thanks!",,"['probability', 'normal-distribution', 'independence']"
53,Basic probability: Romeo and Juliette meet for a date.,Basic probability: Romeo and Juliette meet for a date.,,"Romeo and Juliet have a date at a given time, and each will arrive at the meeting place with a delay between 0 and 1 hour, with all pairs of delays being equally likely. The first to arrive will wait for 15 minutes and will leave if the other has not yet arrived. What is the probability that they will meet? My text has the answer as 7/16, and I just don't get it.  I'm just reading the book for self study - no one to ask! My logic: One of them has to arrive first, or they both arrive at the same time.  The probability they arrive at the exact same time is zero.  Suppose Romeo arrives first. If Romeo is the first to arrive, and he arrives after 45min, they are guaranteed to meet. A = P(Romeo arrives within the first 45min) = 45/60 = 3/4. B = P(Juliette arrives within some 15min interval) = 15/60 = 1/4. P(A and B) = 3/4 * 1/4 = 3/16. Help?","Romeo and Juliet have a date at a given time, and each will arrive at the meeting place with a delay between 0 and 1 hour, with all pairs of delays being equally likely. The first to arrive will wait for 15 minutes and will leave if the other has not yet arrived. What is the probability that they will meet? My text has the answer as 7/16, and I just don't get it.  I'm just reading the book for self study - no one to ask! My logic: One of them has to arrive first, or they both arrive at the same time.  The probability they arrive at the exact same time is zero.  Suppose Romeo arrives first. If Romeo is the first to arrive, and he arrives after 45min, they are guaranteed to meet. A = P(Romeo arrives within the first 45min) = 45/60 = 3/4. B = P(Juliette arrives within some 15min interval) = 15/60 = 1/4. P(A and B) = 3/4 * 1/4 = 3/16. Help?",,['probability']
54,Can the probability of an event be an irrational number?,Can the probability of an event be an irrational number?,,"I am wondering whether it is possible to construct an experiment, where the probability of occurrence of an event comes out to be an irrational number.","I am wondering whether it is possible to construct an experiment, where the probability of occurrence of an event comes out to be an irrational number.",,['probability']
55,Find probability that random triangle covers centre of circumscribed circle,Find probability that random triangle covers centre of circumscribed circle,,"We are given the equilateral triangle A. On each edge of the triangle we pick a point: randomly (probability distribution is uniform) independently of others We construct new triangle B from randomized points. Task is to find the chance of B containing the centre of circle circumscribed around triangle A. I would appreciate hints and pointers. Thanks! Edit Suppose $P_0$ is chosen on the bottom line of triangle A. $P_O = (x_0,0), x_0 \in [0,1]$ Let's find geometrical place of points $P_1, P_2$. If $P_1$ is on the upper left edge, that $P_1 = (x_1,\sqrt 3 x_1), x_1 \in [0, \frac 1 2]$. Then $P_2$ is upper right edge, and $P_2 = (x_2, \sqrt 3 (1 - x_2)), x_2 \in [\frac 1 2, 1]$. Centre point is $C = (\frac 1 2, \frac 1 {2 \sqrt 3 })$. Now, we shall find the constraints for points. For fixed $x_0$, $P_1$ and $P_2$ must lay below the line between $P_0$ and $C$. But I wonder, if there is a way to describe constraint better? Since this most trivial way suffers from situation when line $CP_0$ and one of the edges intersects outside of range $x_m$. Edit 2 I have listened to the useful hints about advantages of angle-based view on the problem, and believe that now I have the geometry part figured: As earlier, We fixed $P_0$ on the bottom edge and on the step 1 we pick $P_1$ on the left edge. Consider $\angle \alpha$ line between bottom edge and line $CP_0$. Then $\alpha \in [\frac \pi 6, \frac \pi 2]$. Then prohibited sector for left edge has angle $\frac \pi 2 - \alpha$. Using same logic on the step 2 we obtain prohibited sector for point $P_2$. And now it looks like I have to get down to all that integration, so your suggestions are welcome!","We are given the equilateral triangle A. On each edge of the triangle we pick a point: randomly (probability distribution is uniform) independently of others We construct new triangle B from randomized points. Task is to find the chance of B containing the centre of circle circumscribed around triangle A. I would appreciate hints and pointers. Thanks! Edit Suppose $P_0$ is chosen on the bottom line of triangle A. $P_O = (x_0,0), x_0 \in [0,1]$ Let's find geometrical place of points $P_1, P_2$. If $P_1$ is on the upper left edge, that $P_1 = (x_1,\sqrt 3 x_1), x_1 \in [0, \frac 1 2]$. Then $P_2$ is upper right edge, and $P_2 = (x_2, \sqrt 3 (1 - x_2)), x_2 \in [\frac 1 2, 1]$. Centre point is $C = (\frac 1 2, \frac 1 {2 \sqrt 3 })$. Now, we shall find the constraints for points. For fixed $x_0$, $P_1$ and $P_2$ must lay below the line between $P_0$ and $C$. But I wonder, if there is a way to describe constraint better? Since this most trivial way suffers from situation when line $CP_0$ and one of the edges intersects outside of range $x_m$. Edit 2 I have listened to the useful hints about advantages of angle-based view on the problem, and believe that now I have the geometry part figured: As earlier, We fixed $P_0$ on the bottom edge and on the step 1 we pick $P_1$ on the left edge. Consider $\angle \alpha$ line between bottom edge and line $CP_0$. Then $\alpha \in [\frac \pi 6, \frac \pi 2]$. Then prohibited sector for left edge has angle $\frac \pi 2 - \alpha$. Using same logic on the step 2 we obtain prohibited sector for point $P_2$. And now it looks like I have to get down to all that integration, so your suggestions are welcome!",,"['probability', 'geometry', 'geometric-probability']"
56,"Pólya's urn scheme, proof using conditional probability and induction","Pólya's urn scheme, proof using conditional probability and induction",,"Problem An urn contains $B$ blue balls and $R$ red balls. Suppose that one extracts successively $n$ balls at random such that when a ball is chosen, it is returned to the urn again along with $c$ extra balls of the same color. For each $n \in \mathbb N$, we define $R_n=\{\text{the n-th ball extracted is red}\}$, and $B_n=\{\text{the n-th ball extracted is blue}\}.$ Prove that $P(R_n)=\dfrac{R}{R+B}$. I thought of trying to condition the event $R_n$ to another event in order to use induction. For example, if $n=2$, I can express $$P(R_2)=P(R_2|R_1)P(R_1)+P(R_2|B_1)P(B_1)$$$$=\dfrac{R+c}{R+B+c}\dfrac{R}{R+B}+\dfrac{R}{R+B+c}\dfrac{B}{R+B}$$$$=\dfrac{R}{R+B}.$$ Now, suppose the formula is true for $n$, I want to show it is true for $n+1$. So, $P(R_{n+1}=P(R_{n+1}|R_n)P(R_n)+P(R_{n+1}|B_n)P(B_n)$$$=P(R_{n+1}|R_n)P(R_n)+P(R_{n+1}|B_n)(1-P(R_n)$$$$=P(R_{n+1}|R_n)\dfrac{R}{R+B}+P(R_{n+1}|B_n)(1-\dfrac{R}{R+B}).$$ I am having some difficulty trying to calculate $P(R_{n+1}|R_n)$ and $P(R_{n+1}|B_n)$. I would appreciate if someone could complete my answer or suggest me how can I finish the proof if what I've done up to now is correct.","Problem An urn contains $B$ blue balls and $R$ red balls. Suppose that one extracts successively $n$ balls at random such that when a ball is chosen, it is returned to the urn again along with $c$ extra balls of the same color. For each $n \in \mathbb N$, we define $R_n=\{\text{the n-th ball extracted is red}\}$, and $B_n=\{\text{the n-th ball extracted is blue}\}.$ Prove that $P(R_n)=\dfrac{R}{R+B}$. I thought of trying to condition the event $R_n$ to another event in order to use induction. For example, if $n=2$, I can express $$P(R_2)=P(R_2|R_1)P(R_1)+P(R_2|B_1)P(B_1)$$$$=\dfrac{R+c}{R+B+c}\dfrac{R}{R+B}+\dfrac{R}{R+B+c}\dfrac{B}{R+B}$$$$=\dfrac{R}{R+B}.$$ Now, suppose the formula is true for $n$, I want to show it is true for $n+1$. So, $P(R_{n+1}=P(R_{n+1}|R_n)P(R_n)+P(R_{n+1}|B_n)P(B_n)$$$=P(R_{n+1}|R_n)P(R_n)+P(R_{n+1}|B_n)(1-P(R_n)$$$$=P(R_{n+1}|R_n)\dfrac{R}{R+B}+P(R_{n+1}|B_n)(1-\dfrac{R}{R+B}).$$ I am having some difficulty trying to calculate $P(R_{n+1}|R_n)$ and $P(R_{n+1}|B_n)$. I would appreciate if someone could complete my answer or suggest me how can I finish the proof if what I've done up to now is correct.",,"['probability', 'combinatorics']"
57,convergence in mean square implies convergence of variance,convergence in mean square implies convergence of variance,,"I need some hints for the following question: Suppose $X,X_1,X_2, \cdots \in L^2(\Omega)$ are random variables that converge in mean square. Show that $Var[X_n] \rightarrow Var[X]$. Convergence in mean square implies that as $n \rightarrow \infty$ we have that $\mathbb{E}[(X_n-X)^2] \rightarrow 0$. I tried to use the definition of variance $Var[X]=\mathbb{E}[X^2]-\mathbb{E}[X]^2$ and trying to prove that $|Var[X_n]-Var[X]|\leq \mathbb{E}[(X_n-X)^2]$ but I don't get any result.","I need some hints for the following question: Suppose $X,X_1,X_2, \cdots \in L^2(\Omega)$ are random variables that converge in mean square. Show that $Var[X_n] \rightarrow Var[X]$. Convergence in mean square implies that as $n \rightarrow \infty$ we have that $\mathbb{E}[(X_n-X)^2] \rightarrow 0$. I tried to use the definition of variance $Var[X]=\mathbb{E}[X^2]-\mathbb{E}[X]^2$ and trying to prove that $|Var[X_n]-Var[X]|\leq \mathbb{E}[(X_n-X)^2]$ but I don't get any result.",,"['probability', 'expectation']"
58,Expected value of normal CDF,Expected value of normal CDF,,"I am trying to calculate the expected value of a Normal CDF, but I have gotten stuck. I want to find the expected value of $\Phi\left( \frac{a-bX}{c} \right)$ where $X$ is distributed as $\mathcal{N}(0,1)$ and $\Phi$ is the standard normal CDF. I know I can transform $\frac{a-bX}{c}$ to be a normal random variable $\mathcal{N}\left(\frac{a}{c},\frac{b^2}{c^2}\right)$ where $\frac{b^2}{c^2}$ is the variance of the normal random variable. I'm not sure if this helps though. I think that the expected value of a CDF is $0.5$ but since $\Phi$ is the CDF of a standard normal CDF and $\frac{a-bX}{c}$ is not standard normal I do not think the expected value should be $0.5$ . I tried integrating the CDF, but I do not believe I did it correctly. When $a = -2.3338$ , $b = 0.32164$ , $c = 0.94686$ , I believe the correct answer is approximately $0.009803$ . I found this through simulation. I would appreciate any help or suggestions.","I am trying to calculate the expected value of a Normal CDF, but I have gotten stuck. I want to find the expected value of where is distributed as and is the standard normal CDF. I know I can transform to be a normal random variable where is the variance of the normal random variable. I'm not sure if this helps though. I think that the expected value of a CDF is but since is the CDF of a standard normal CDF and is not standard normal I do not think the expected value should be . I tried integrating the CDF, but I do not believe I did it correctly. When , , , I believe the correct answer is approximately . I found this through simulation. I would appreciate any help or suggestions.","\Phi\left( \frac{a-bX}{c} \right) X \mathcal{N}(0,1) \Phi \frac{a-bX}{c} \mathcal{N}\left(\frac{a}{c},\frac{b^2}{c^2}\right) \frac{b^2}{c^2} 0.5 \Phi \frac{a-bX}{c} 0.5 a = -2.3338 b = 0.32164 c = 0.94686 0.009803","['probability', 'probability-distributions', 'normal-distribution', 'expected-value', 'cumulative-distribution-functions']"
59,Product of probability density functions,Product of probability density functions,,"I was going through a problem in Geoffrey Grimmett and David Stirzaker's book ( Probability and Random Processes ). The problem is as follows: If $f$ and $g$ are probability density functions, then prove that for $ 0 \leq \lambda \leq 1$ the function $\lambda f + (1-\lambda)g$ is a density function. Is the product $fg$ a density function as well? It is straightforward to prove $\lambda f + (1-\lambda)g$ is a density function. For the second question as well, one can construct trivial functions for $f$ and $g$ as $f(x)=g(x)=1$ for $ 0 \leq x \leq 1$. Are there any other non-trivial examples of a family or class of distributions for which one can find $\int_{-\infty}^{\infty} f(x)g(x) dx=1$?","I was going through a problem in Geoffrey Grimmett and David Stirzaker's book ( Probability and Random Processes ). The problem is as follows: If $f$ and $g$ are probability density functions, then prove that for $ 0 \leq \lambda \leq 1$ the function $\lambda f + (1-\lambda)g$ is a density function. Is the product $fg$ a density function as well? It is straightforward to prove $\lambda f + (1-\lambda)g$ is a density function. For the second question as well, one can construct trivial functions for $f$ and $g$ as $f(x)=g(x)=1$ for $ 0 \leq x \leq 1$. Are there any other non-trivial examples of a family or class of distributions for which one can find $\int_{-\infty}^{\infty} f(x)g(x) dx=1$?",,['probability']
60,Probability in flipping a coin,Probability in flipping a coin,,"Independent flips of a coin that lands on heads with probability p are made. What is the probability that the pattern T, H, H, H occurs before the pattern H, H, H, H? Hint: How can the pattern H, H, H, H occur first? my approach is $\sum_{n=0}^\infty (1-p^4)^n(1-p)^3$ then i get $$\frac {(1-p)p^3}{1-(1-p^4)}$$ However, the suggested answer is Am I wrong? What is the rationale behind the suggested answers? Another answer I found online is : Our first observation is that for any 0 < p < 1 we will eventually see the pattern (H, H, H, H). Suppose that the first such pattern starts at the nth flip. If n > 1, then the n − 1th flip cannot be H since then the first (H, H, H, H) pattern would have started before the nth flip. Hence in this case the n − 1th flip is necessarily T, and starting with the n − 1th flip we see the pattern (T, H, H, H, H). In that sequence, (T, H, H, H) appears before (H, H, H, H). Summarizing, (H, H, H, H) can only appear before (T, H, H, H) if it starts immediately at the n = 1st flip, that is if all four first flips are heads. The probability of that is $p^4$.","Independent flips of a coin that lands on heads with probability p are made. What is the probability that the pattern T, H, H, H occurs before the pattern H, H, H, H? Hint: How can the pattern H, H, H, H occur first? my approach is $\sum_{n=0}^\infty (1-p^4)^n(1-p)^3$ then i get $$\frac {(1-p)p^3}{1-(1-p^4)}$$ However, the suggested answer is Am I wrong? What is the rationale behind the suggested answers? Another answer I found online is : Our first observation is that for any 0 < p < 1 we will eventually see the pattern (H, H, H, H). Suppose that the first such pattern starts at the nth flip. If n > 1, then the n − 1th flip cannot be H since then the first (H, H, H, H) pattern would have started before the nth flip. Hence in this case the n − 1th flip is necessarily T, and starting with the n − 1th flip we see the pattern (T, H, H, H, H). In that sequence, (T, H, H, H) appears before (H, H, H, H). Summarizing, (H, H, H, H) can only appear before (T, H, H, H) if it starts immediately at the n = 1st flip, that is if all four first flips are heads. The probability of that is $p^4$.",,['probability']
61,Seemingly similar but different probability games,Seemingly similar but different probability games,,"Burger King is currently running its ""family food"" game in which each piece can be modeled as a scratch off game where exactly one of three slots is a winner and you may only scratch one slot as your guess. As I was standing in line the other day I realized that their advertisement of ""make it a large to double your chances of winning"" (where large drinks/fries have two pieces on them) was actually not exactly true.  The real probability of having at least one winning with two tickets is $\frac{5}{9}$ rather than $\frac{2}{3}$ which would really be double the chance. This is because the probability of both losing is $\frac{2}{3}\times\frac{2}{3}$ so the probability of at least one winning is $1-\frac{2}{3}\times\frac{2}{3}=\frac{5}{9}$. Now this was all very clear to me but while I sipped my strawberry banana smoothie, I wondered why this game of two three-slot tickets where each one has one winner and you can scratch one from each is different from the game of a single six-slot ticket where there are two winning slots and you get two scratches.  The games must be different because the probabilities of winning are different.  The six-slot game has $1-\frac{4}{6}\times\frac{3}{5}=\frac{3}{5}$ chance of getting at least one win.  The two games seem the same to me intuitively. Can anyone explain how they are different? EDIT: I had noticed that in the two tickets, by making one guess, you actually eliminate 2 other possibilities with it so maybe this is the core of it, but I am still trying to see it more intuitively.","Burger King is currently running its ""family food"" game in which each piece can be modeled as a scratch off game where exactly one of three slots is a winner and you may only scratch one slot as your guess. As I was standing in line the other day I realized that their advertisement of ""make it a large to double your chances of winning"" (where large drinks/fries have two pieces on them) was actually not exactly true.  The real probability of having at least one winning with two tickets is $\frac{5}{9}$ rather than $\frac{2}{3}$ which would really be double the chance. This is because the probability of both losing is $\frac{2}{3}\times\frac{2}{3}$ so the probability of at least one winning is $1-\frac{2}{3}\times\frac{2}{3}=\frac{5}{9}$. Now this was all very clear to me but while I sipped my strawberry banana smoothie, I wondered why this game of two three-slot tickets where each one has one winner and you can scratch one from each is different from the game of a single six-slot ticket where there are two winning slots and you get two scratches.  The games must be different because the probabilities of winning are different.  The six-slot game has $1-\frac{4}{6}\times\frac{3}{5}=\frac{3}{5}$ chance of getting at least one win.  The two games seem the same to me intuitively. Can anyone explain how they are different? EDIT: I had noticed that in the two tickets, by making one guess, you actually eliminate 2 other possibilities with it so maybe this is the core of it, but I am still trying to see it more intuitively.",,"['probability', 'combinatorics', 'intuition', 'game-theory']"
62,"Given $2$ randomly chosen integers $x,y$ what is $P(k=gcd(x,y))$?",Given  randomly chosen integers  what is ?,"2 x,y P(k=gcd(x,y))","Given $2$ randomly chosen integers $x,y$ what is the probability that a integer $k$ is the greatest common divisor of $x$ and $y$? I know that the probability of $x,y$ being coprime is $\frac{1}{\zeta(2)} = \frac{6}{\pi^2}$ this could be a starting point. I think one could try to analyse $P(x/k $  and  $ y/k$ coprime $| k$ divides $x$ and $y)$ But then the distribution of $x$ and $y$ is no longer uniform. Maybe there is some helping hand here?","Given $2$ randomly chosen integers $x,y$ what is the probability that a integer $k$ is the greatest common divisor of $x$ and $y$? I know that the probability of $x,y$ being coprime is $\frac{1}{\zeta(2)} = \frac{6}{\pi^2}$ this could be a starting point. I think one could try to analyse $P(x/k $  and  $ y/k$ coprime $| k$ divides $x$ and $y)$ But then the distribution of $x$ and $y$ is no longer uniform. Maybe there is some helping hand here?",,"['number-theory', 'probability', 'prime-numbers']"
63,"Threshold for the ""number of UUIDs generated per millisecond"" at which the collision probability of UUID v4 and UUID v7 is equal","Threshold for the ""number of UUIDs generated per millisecond"" at which the collision probability of UUID v4 and UUID v7 is equal",,"I post this question here instead of StackOverflow because the mathematical element is stronger than engineering one. First, let me clarify the definition of terms. UUID v4 : Random value of $122$ bits ( $2^{122}$ possible values) UUID v7 : Random value of $74$ bits ( $2^{74}$ possible values), independent every millisecond UUID v4 starts with an almost zero chance of collision, but as a certain number of UUIDs accumulate, the collision probability increases gradually due to the birthday paradox problem. On the other hand, if UUID v7 is generated less than once per millisecond, the collision probability is absolutely zero. If it is generated more than twice per millisecond, collisions occur at a certain probability. For simplicity, let's only consider what happens just after 50 years ( $31557600000 \cdot 50$ milliseconds), and avoid treating time $t$ as a variable. If UUIDs are generated at a rate of $x$ per millisecond, after 50 years, $(31557600000 \cdot 50)x$ UUIDs will have accumulated. UUID v4 is affected by the number of accumulated UUIDs, so it is necessary to consider both the collision probability between UUIDs that are about to be created and the collision probability with UUIDs created in the past . For UUID v7, it is enough to consider only the collision probability between UUIDs that are about to be created . Based on this, I want to know the minimum value of $x$ at which the collision probability of UUID v7 becomes higher than that of UUID v4 after 50 years . Similarly, I want to calculate the patterns for 0 seconds, 5 seconds, 1 year, 5 years, 10 years, 50 years, 100 years, 500 years, and 1000 years, and observe how they change. I tried to get ChatGPT to solve this problem, but it gave me answers that seemed wrong, or said there were no solutions. Could it be that the number of UUIDs generated per millisecond $x$ is not a parameter, and everything is determined by the time $t$ ? I'm not very good at math and I'm having trouble. I would appreciate your help.","I post this question here instead of StackOverflow because the mathematical element is stronger than engineering one. First, let me clarify the definition of terms. UUID v4 : Random value of bits ( possible values) UUID v7 : Random value of bits ( possible values), independent every millisecond UUID v4 starts with an almost zero chance of collision, but as a certain number of UUIDs accumulate, the collision probability increases gradually due to the birthday paradox problem. On the other hand, if UUID v7 is generated less than once per millisecond, the collision probability is absolutely zero. If it is generated more than twice per millisecond, collisions occur at a certain probability. For simplicity, let's only consider what happens just after 50 years ( milliseconds), and avoid treating time as a variable. If UUIDs are generated at a rate of per millisecond, after 50 years, UUIDs will have accumulated. UUID v4 is affected by the number of accumulated UUIDs, so it is necessary to consider both the collision probability between UUIDs that are about to be created and the collision probability with UUIDs created in the past . For UUID v7, it is enough to consider only the collision probability between UUIDs that are about to be created . Based on this, I want to know the minimum value of at which the collision probability of UUID v7 becomes higher than that of UUID v4 after 50 years . Similarly, I want to calculate the patterns for 0 seconds, 5 seconds, 1 year, 5 years, 10 years, 50 years, 100 years, 500 years, and 1000 years, and observe how they change. I tried to get ChatGPT to solve this problem, but it gave me answers that seemed wrong, or said there were no solutions. Could it be that the number of UUIDs generated per millisecond is not a parameter, and everything is determined by the time ? I'm not very good at math and I'm having trouble. I would appreciate your help.",122 2^{122} 74 2^{74} 31557600000 \cdot 50 t x (31557600000 \cdot 50)x x x t,"['probability', 'computer-science', 'random']"
64,Finding the limit of $\mathbb{E}[\theta^n]/\mathbb{E}[\theta^{n-1}]$,Finding the limit of,\mathbb{E}[\theta^n]/\mathbb{E}[\theta^{n-1}],"Let $\theta$ denote a smoothly distributed random variable with support $[0, 1]$ . I am trying to evaluate $$ \lim_{n \rightarrow \infty} \frac{\mathbb{E}[\theta^n]}{\mathbb{E}[\theta^{n-1}]}$$ I suspect, but cannot show, that the limit equals $1$ . Does anyone know how to do this? My attempts so far: Since $\theta \in [0, 1]$ , it seems reasonably clear that both $\mathbb{E}[\theta^n] \rightarrow 0$ and $\mathbb{E}[\theta^{n-1}]  \rightarrow 0$ as $n \rightarrow \infty$ (we are raising numbers that are less than $1$ to ever higher powers). Thus, we can apply L'Hopital's rule to find that $$ \lim_{n \rightarrow \infty} \frac{\mathbb{E}[\theta^n]}{\mathbb{E}[\theta^{n-1}]} \equiv \lim_{n \rightarrow \infty} \frac{\int_0^1 \theta^nf(\theta)d\theta}{\int_0^1 \theta^{n-1}f(\theta)d\theta} =  \lim_{n \rightarrow \infty} \frac{\int_0^1 \ln(\theta)\theta^nf(\theta)d\theta}{\int_0^1 \ln(\theta)\theta^{n-1}f(\theta)d\theta}$$ I am a bit unclear, however, how to proceed from this point (or whether better approaches are available).","Let denote a smoothly distributed random variable with support . I am trying to evaluate I suspect, but cannot show, that the limit equals . Does anyone know how to do this? My attempts so far: Since , it seems reasonably clear that both and as (we are raising numbers that are less than to ever higher powers). Thus, we can apply L'Hopital's rule to find that I am a bit unclear, however, how to proceed from this point (or whether better approaches are available).","\theta [0, 1]  \lim_{n \rightarrow \infty} \frac{\mathbb{E}[\theta^n]}{\mathbb{E}[\theta^{n-1}]} 1 \theta \in [0, 1] \mathbb{E}[\theta^n] \rightarrow 0 \mathbb{E}[\theta^{n-1}]  \rightarrow 0 n \rightarrow \infty 1  \lim_{n \rightarrow \infty} \frac{\mathbb{E}[\theta^n]}{\mathbb{E}[\theta^{n-1}]} \equiv \lim_{n \rightarrow \infty} \frac{\int_0^1 \theta^nf(\theta)d\theta}{\int_0^1 \theta^{n-1}f(\theta)d\theta} =  \lim_{n \rightarrow \infty} \frac{\int_0^1 \ln(\theta)\theta^nf(\theta)d\theta}{\int_0^1 \ln(\theta)\theta^{n-1}f(\theta)d\theta}","['probability', 'limits', 'expected-value']"
65,How many zeros in the decimal representation of $5^n$?,How many zeros in the decimal representation of ?,5^n,"I'm curious about some properties of the powers of 5 $$5^2=25,\quad5^3=125,\quad 5^4=625,\quad 5^5=3125,\quad ...$$ Is it true that at least $50$ % of the digits in the decimal representation of $5^n$ are non-zero? This seems pretty modest since assuming each digit will be equally likely, only about $10$ % of the digits will be zero on average. The first zero occurs at $5^8=390625$ and the power with the largest percentage of zeros seems to be $$5^{45}=28421709430404007434844970703125$$ in which $\approx 22$ % of the digits are zero. I checked up to $5^{1000}$ . The difficulty is the statement seems so obvious from a probabilistic perspective, yet I can't pin down any definite theorems! Obviously the first and the last 3 digits will always be non-zero so we have at least 4 non-zero digits. But I'm hoping to prove some properties about $5^n$ in general which require the non-zero digits to be at least linear in $n$ . So $50$ % would be more than enough. Really any probability $\epsilon>0$ will do -- bigger the better though. Maybe analyze $\langle 5 \rangle^\times$ in $\mathbb{Z}/10^k\mathbb{Z}$ ? Could probability theory produce the bound in question?","I'm curious about some properties of the powers of 5 Is it true that at least % of the digits in the decimal representation of are non-zero? This seems pretty modest since assuming each digit will be equally likely, only about % of the digits will be zero on average. The first zero occurs at and the power with the largest percentage of zeros seems to be in which % of the digits are zero. I checked up to . The difficulty is the statement seems so obvious from a probabilistic perspective, yet I can't pin down any definite theorems! Obviously the first and the last 3 digits will always be non-zero so we have at least 4 non-zero digits. But I'm hoping to prove some properties about in general which require the non-zero digits to be at least linear in . So % would be more than enough. Really any probability will do -- bigger the better though. Maybe analyze in ? Could probability theory produce the bound in question?","5^2=25,\quad5^3=125,\quad 5^4=625,\quad 5^5=3125,\quad ... 50 5^n 10 5^8=390625 5^{45}=28421709430404007434844970703125 \approx 22 5^{1000} 5^n n 50 \epsilon>0 \langle 5 \rangle^\times \mathbb{Z}/10^k\mathbb{Z}",['probability']
66,Expected global clustering coefficient for Erdős–Rényi graph,Expected global clustering coefficient for Erdős–Rényi graph,,"What is the expected global clustering coefficient $\mathbb{E}[C_{GC}]$ for the Erdős–Rényi random graph (ER-graph) $\mathcal{G}(n,p)$ (expectation is over the ensemble of all ER-graphs) as $n \rightarrow \infty$ and $p$ fixed? The global clustering coefficient $C_{GC}$ is defined as $C_{GC}={\frac  {3\times {\mbox{number of triangles}}}{{\mbox{number of connected triplets of vertices}}}}={\frac  {{\mbox{number of closed triplets}}}{{\mbox{number of connected triplets of vertices}}}}$. A connected triplet is defined to be a connected subgraph consisting of three vertices and two edges. A closed triplet is a connected triplet that induces a triangle. While it is easy to see that the expected mean local clustering coefficient is $p$ (see next section), the expected global clustering coefficient is not identically $p$ for any $n$. For example, for $n=3$, $C_{GC} = 1$ only when all edges are present (with probability $p^3$) and is otherwise zero (with probability $1-p^3$). Hence the $\mathbb{E}[E_{GC}] = p^3$ when $n=3$. Computationally, I have found that $\mathbb{E}[C_{GC}]\approx p$ for large $n$. Is there a way to prove that $\mathbb{E}[C_{GC}]= p$ as $n\rightarrow\infty$? My current theory is to use Chebyshev's inequality on this, but I haven't tried it out yet. Expected local clustering coefficient = p In contrast, it is easy to see that the expected local clustering coefficient $\mathbb{E}[C_i]$ for any node $i$ is $p$. The local clustering coefficient $C_i$ of node $i$ (for an undirected network) is defined as $C_i = \frac{\text{number of triangles that contain $i$}}{k_i (k_i-1)/2}$, where $k_i$ is the degree of $i$. In other words, it is the proportion of links between the vertices within its neighbourhood divided by the number of links that could possibly exist between them. We have $\mathbb{E}[C_i]=p$ in an ER-graph because the probability for an edge between any neighbours of the node is $p$, independent for any other edge. (For an alternative answer involving more algebra, see here ). Hence the expected mean local clustering coefficient $\mathbb{E}[\sum_i C_i]$ is $p$ for any $n$.","What is the expected global clustering coefficient $\mathbb{E}[C_{GC}]$ for the Erdős–Rényi random graph (ER-graph) $\mathcal{G}(n,p)$ (expectation is over the ensemble of all ER-graphs) as $n \rightarrow \infty$ and $p$ fixed? The global clustering coefficient $C_{GC}$ is defined as $C_{GC}={\frac  {3\times {\mbox{number of triangles}}}{{\mbox{number of connected triplets of vertices}}}}={\frac  {{\mbox{number of closed triplets}}}{{\mbox{number of connected triplets of vertices}}}}$. A connected triplet is defined to be a connected subgraph consisting of three vertices and two edges. A closed triplet is a connected triplet that induces a triangle. While it is easy to see that the expected mean local clustering coefficient is $p$ (see next section), the expected global clustering coefficient is not identically $p$ for any $n$. For example, for $n=3$, $C_{GC} = 1$ only when all edges are present (with probability $p^3$) and is otherwise zero (with probability $1-p^3$). Hence the $\mathbb{E}[E_{GC}] = p^3$ when $n=3$. Computationally, I have found that $\mathbb{E}[C_{GC}]\approx p$ for large $n$. Is there a way to prove that $\mathbb{E}[C_{GC}]= p$ as $n\rightarrow\infty$? My current theory is to use Chebyshev's inequality on this, but I haven't tried it out yet. Expected local clustering coefficient = p In contrast, it is easy to see that the expected local clustering coefficient $\mathbb{E}[C_i]$ for any node $i$ is $p$. The local clustering coefficient $C_i$ of node $i$ (for an undirected network) is defined as $C_i = \frac{\text{number of triangles that contain $i$}}{k_i (k_i-1)/2}$, where $k_i$ is the degree of $i$. In other words, it is the proportion of links between the vertices within its neighbourhood divided by the number of links that could possibly exist between them. We have $\mathbb{E}[C_i]=p$ in an ER-graph because the probability for an edge between any neighbours of the node is $p$, independent for any other edge. (For an alternative answer involving more algebra, see here ). Hence the expected mean local clustering coefficient $\mathbb{E}[\sum_i C_i]$ is $p$ for any $n$.",,"['probability', 'probability-distributions', 'graph-theory', 'random-graphs']"
67,"Calculate probability $P(\min\left\{X,Y\right\} \leq x)$ and $P(\max\left\{X,Y\right\} \leq x)$",Calculate probability  and,"P(\min\left\{X,Y\right\} \leq x) P(\max\left\{X,Y\right\} \leq x)","$X,Y$ are independent, identical distributed with $$P(X=k) = P(Y=k)=\frac{1}{2^k} \,\,\,\,\,\,\,\,\,\,\,\, (k=1,2,...,n,...)$$ Calculate the probabilities $P(\min\left\{X,Y\right\} \leq x)$ and   $P(\max\left\{X,Y\right\} \leq x)$ For the minimum I do like this: $$\begin{split}F_M(x) &= P(\min\left\{X,Y\right\} \leq x) \\ &= 1-P(x<\min\left\{X,Y\right\} ) \\ &= 1-P(x<X, x<Y) \\ & = 1-P(X>x)\,P(Y>x)\\ & = 1-(1-P(X \leq x))\,(1-P(Y \leq x))\\ & = 1-(1-F_X(x))\,(1-F_Y(x))\end{split}$$ Is this correct for minimum? I'm not sure how do it for $\max$? Maybe I do it too complicated because they are equal these $P(X=k)=P(Y=k)$ maybe you can do it more elegant? But I don't know how?","$X,Y$ are independent, identical distributed with $$P(X=k) = P(Y=k)=\frac{1}{2^k} \,\,\,\,\,\,\,\,\,\,\,\, (k=1,2,...,n,...)$$ Calculate the probabilities $P(\min\left\{X,Y\right\} \leq x)$ and   $P(\max\left\{X,Y\right\} \leq x)$ For the minimum I do like this: $$\begin{split}F_M(x) &= P(\min\left\{X,Y\right\} \leq x) \\ &= 1-P(x<\min\left\{X,Y\right\} ) \\ &= 1-P(x<X, x<Y) \\ & = 1-P(X>x)\,P(Y>x)\\ & = 1-(1-P(X \leq x))\,(1-P(Y \leq x))\\ & = 1-(1-F_X(x))\,(1-F_Y(x))\end{split}$$ Is this correct for minimum? I'm not sure how do it for $\max$? Maybe I do it too complicated because they are equal these $P(X=k)=P(Y=k)$ maybe you can do it more elegant? But I don't know how?",,"['probability', 'probability-distributions']"
68,"Multivariate Gaussian Definition when Covariance matrix is singular, What's wrong?","Multivariate Gaussian Definition when Covariance matrix is singular, What's wrong?",,"Given $$\mathbf{\Sigma} \in \mathbb R^{k \times k}$$ $$\mathbf{u} \in \mathbb R^k$$ The multivariate Gaussian pdf can be determined By definition: $$f(\mathbf{x})=\frac{1}{2\pi^{\frac{-k}{2}}|\Sigma|^{\frac{1}{2}}}e^{\frac{1}{2}(\mathbf{x-u})^T\mathbf{\Sigma}^{-1}(\mathbf{x-u})}$$ The Covariance matrix is only limited to be positive semidefinite. So it could be singular (Non-invertible) This will also lead to a zero in the denumerator, and also the $\Sigma^{-1}$ doesn't exist. What we do in that case to write the joint pdf?","Given The multivariate Gaussian pdf can be determined By definition: The Covariance matrix is only limited to be positive semidefinite. So it could be singular (Non-invertible) This will also lead to a zero in the denumerator, and also the doesn't exist. What we do in that case to write the joint pdf?",\mathbf{\Sigma} \in \mathbb R^{k \times k} \mathbf{u} \in \mathbb R^k f(\mathbf{x})=\frac{1}{2\pi^{\frac{-k}{2}}|\Sigma|^{\frac{1}{2}}}e^{\frac{1}{2}(\mathbf{x-u})^T\mathbf{\Sigma}^{-1}(\mathbf{x-u})} \Sigma^{-1},['probability']
69,Expected maximum absolute value of $n$ iid standard Gaussians?,Expected maximum absolute value of  iid standard Gaussians?,n,"I have a problem where my errors are normally distributed and I want to know what the expected maximum error is if I repeat the process $n$ times. What is the smallest constant $C$ such that the following statement is true for all $n\geq 2$ ? Let $X_1, X_2, \cdots, X_n$ be independent standard Gaussian random variables. Then $$\mathbb{E}\left[\max_{i=1}^n \left|X_i\right| \right] \leq C \sqrt{\log_e n}.$$ I can show that the answer is between $1.35$ and $2$ .",I have a problem where my errors are normally distributed and I want to know what the expected maximum error is if I repeat the process times. What is the smallest constant such that the following statement is true for all ? Let be independent standard Gaussian random variables. Then I can show that the answer is between and .,"n C n\geq 2 X_1, X_2, \cdots, X_n \mathbb{E}\left[\max_{i=1}^n \left|X_i\right| \right] \leq C \sqrt{\log_e n}. 1.35 2","['probability', 'normal-distribution', 'expectation', 'order-statistics']"
70,Help provide a proof of the Helly–Bray theorem,Help provide a proof of the Helly–Bray theorem,,"Given a probability space $(\Omega,\cal F, \Bbb P)$, the distribution function of a random variable $X$ is defined as $F(x)=\Bbb P\{X \le x\}$. Now if $F_1,F_2,...,F_{\infty}$ are distribution functions, then the question is Is $F_n \xrightarrow{w} F_{\infty}$ equivalent to $\lim_{n\uparrow\infty}\int\phi dF_n=\int\phi dF_{\infty}$ for every $\phi \in C(\Bbb R)$? Here ${F_n}\xrightarrow{w}{F_\infty }$ means weak convergence, and the integral involved are Riemann-Stieltjes integrals. Someone has pointed out that this is the Helly-Bray theorem , which says the above claim is true when $\phi$ is bounded. I have searched the Internet but was not able to find a proof of the theorem. Can anyone help provide a proof? Thanks!","Given a probability space $(\Omega,\cal F, \Bbb P)$, the distribution function of a random variable $X$ is defined as $F(x)=\Bbb P\{X \le x\}$. Now if $F_1,F_2,...,F_{\infty}$ are distribution functions, then the question is Is $F_n \xrightarrow{w} F_{\infty}$ equivalent to $\lim_{n\uparrow\infty}\int\phi dF_n=\int\phi dF_{\infty}$ for every $\phi \in C(\Bbb R)$? Here ${F_n}\xrightarrow{w}{F_\infty }$ means weak convergence, and the integral involved are Riemann-Stieltjes integrals. Someone has pointed out that this is the Helly-Bray theorem , which says the above claim is true when $\phi$ is bounded. I have searched the Internet but was not able to find a proof of the theorem. Can anyone help provide a proof? Thanks!",,['probability']
71,Probability that no two consecutive heads occur?,Probability that no two consecutive heads occur?,,"A fair coin is tossed $10$ times. What is the probability that no two   consecutive tosses are heads? Possibilities are (dont mind the number of terms): $H TTTTTTH$, $HTHTHTHTHTHTHT$. But except for those, let $y(n)$ be the number of sequences that start with $T$ $T _$, there are two options, $T$ and $H$ so, $y(n) = y(n - 1) + x(n-1) = y(n - 1) + y(n - 2)$ Let $x(n)$ be the number of sequences that start with $H$, $H _$ the next option is only $T$ hence, $x(n) = y(n - 1)$ The total number of sequences $F(n)$ is: $$F(n) = y(n) + x(n) = 2y(n - 1) + y(n - 2)$$ We are after $F(10)$, $$F(10) = 2y(9) + y(8)$$ $$F(3) = 2y(2) + y(1) = 1 + 4 = 5$$ $$F(4) = 2y(3) + y(2) = 6 + 2 = 8$$ But I'm not quite sure where this will take me though.","A fair coin is tossed $10$ times. What is the probability that no two   consecutive tosses are heads? Possibilities are (dont mind the number of terms): $H TTTTTTH$, $HTHTHTHTHTHTHT$. But except for those, let $y(n)$ be the number of sequences that start with $T$ $T _$, there are two options, $T$ and $H$ so, $y(n) = y(n - 1) + x(n-1) = y(n - 1) + y(n - 2)$ Let $x(n)$ be the number of sequences that start with $H$, $H _$ the next option is only $T$ hence, $x(n) = y(n - 1)$ The total number of sequences $F(n)$ is: $$F(n) = y(n) + x(n) = 2y(n - 1) + y(n - 2)$$ We are after $F(10)$, $$F(10) = 2y(9) + y(8)$$ $$F(3) = 2y(2) + y(1) = 1 + 4 = 5$$ $$F(4) = 2y(3) + y(2) = 6 + 2 = 8$$ But I'm not quite sure where this will take me though.",,"['probability', 'sequences-and-series', 'combinatorics', 'algebra-precalculus', 'contest-math']"
72,What is the probability of my sum reaching exactly 10? [closed],What is the probability of my sum reaching exactly 10? [closed],,"Closed . This question needs details or clarity . It is not currently accepting answers. Want to improve this question? Add details and clarify the problem by editing this post . Closed 9 years ago . Improve this question I throw a 6-sided dice (with values: 0,1,2,3,4,5) multiple times and add each value to a sum, which is 0 in the beginning. What is the probability of my sum reaching exactly 10, 11, 12, 13, 14? After reaching a requested sum, the sum will return to it's original 0 value. E.g: 5 + 5 = 10, and afterwards the sum returns to 0.  Also, the probability for each number on the dice is different (it's not a fair dice).","Closed . This question needs details or clarity . It is not currently accepting answers. Want to improve this question? Add details and clarify the problem by editing this post . Closed 9 years ago . Improve this question I throw a 6-sided dice (with values: 0,1,2,3,4,5) multiple times and add each value to a sum, which is 0 in the beginning. What is the probability of my sum reaching exactly 10, 11, 12, 13, 14? After reaching a requested sum, the sum will return to it's original 0 value. E.g: 5 + 5 = 10, and afterwards the sum returns to 0.  Also, the probability for each number on the dice is different (it's not a fair dice).",,"['probability', 'dice']"
73,What is $\omega$ in probability theory?,What is  in probability theory?,\omega,"I have a hard time understanding what is $\omega$ in probability theory. I understand that for a probability space $(\Omega, A, \mathbb{P})$, A is the sigma-algebra containing all the events which ""may happen"" ie are mesurable for $\mathbb{P}$. But when we consider $\omega \in \Omega$, what is it exactly ?","I have a hard time understanding what is $\omega$ in probability theory. I understand that for a probability space $(\Omega, A, \mathbb{P})$, A is the sigma-algebra containing all the events which ""may happen"" ie are mesurable for $\mathbb{P}$. But when we consider $\omega \in \Omega$, what is it exactly ?",,"['probability', 'probability-theory']"
74,conditional probability question from sheldon ross,conditional probability question from sheldon ross,,"In any given year a male automobile policyholder will make a claim with probability $p_{m}$, and a female policyholder will make a claim with probability $p_{f}$, where $p_{f} \neq p_{m}$. The fraction of the policyholders that are male is $\alpha, 0 < \alpha < 1$. A policyholder is randomly chosen. If $A_{i}$ denotes the event that this policyholder will make a claim in year $i$, show that $P(A_{2}\mid A_{1}) > P(A_{1})$. It appears that $A_{2}$ and $A_{1}$ are independent events, hence I don't understand why the inequality should hold. Can anybody please help? Thanks a lot.","In any given year a male automobile policyholder will make a claim with probability $p_{m}$, and a female policyholder will make a claim with probability $p_{f}$, where $p_{f} \neq p_{m}$. The fraction of the policyholders that are male is $\alpha, 0 < \alpha < 1$. A policyholder is randomly chosen. If $A_{i}$ denotes the event that this policyholder will make a claim in year $i$, show that $P(A_{2}\mid A_{1}) > P(A_{1})$. It appears that $A_{2}$ and $A_{1}$ are independent events, hence I don't understand why the inequality should hold. Can anybody please help? Thanks a lot.",,"['probability', 'probability-theory']"
75,Probability of at least N events occuring,Probability of at least N events occuring,,"I have a series of N events, each with its own probability of occurring . How would I calculate the probability that at least M of the N events actually do occur? I think this is conditional, in that getting at least M occurrences depends on getting at least M-1 occurrences. Past that I'm getting stuck.","I have a series of N events, each with its own probability of occurring . How would I calculate the probability that at least M of the N events actually do occur? I think this is conditional, in that getting at least M occurrences depends on getting at least M-1 occurrences. Past that I'm getting stuck.",,['probability']
76,Intuition behind conditional expectation when sigma algebra isn't generated by a partition,Intuition behind conditional expectation when sigma algebra isn't generated by a partition,,"I'm struggling with the concept of conditional expectation, when the sigma algebra on which it is conditioned isn't generated by a partition. If $(\Omega,\mathcal{F},P)$ is a probability field such that $\mathcal{F}$ is generated by a partition $\Lambda_n$. Then we know that: $E[X|\mathcal{F}]$ = $E_1$$[X|\mathcal{F}]$ $I(\omega \in \Lambda_1)$ +$E_2$$[X|\mathcal{F}]$ $I(\omega \in \Lambda_2)$ + $E_3$$[X|\mathcal{F}]$ $I(\omega \in \Lambda_3)$ + ..... Where $E_i[.]$ is the expectation calculated as per the conditional probability $P(.|\Lambda_i)$ Hence when $\omega$ is in $\Lambda_i$ the conditional expectation gives the expectation of random variable X given that the observed event is $\Lambda_i$ and hence use the modified conditional probability rather than the original one. However this interpretation is only valid as long as the conditioning sigma algebra is generated by a partition. Is there a similar interpretation for a general case? i.e what will it physically represent? Any help will be greatly appreciated! Thanks! Best, Adwait","I'm struggling with the concept of conditional expectation, when the sigma algebra on which it is conditioned isn't generated by a partition. If $(\Omega,\mathcal{F},P)$ is a probability field such that $\mathcal{F}$ is generated by a partition $\Lambda_n$. Then we know that: $E[X|\mathcal{F}]$ = $E_1$$[X|\mathcal{F}]$ $I(\omega \in \Lambda_1)$ +$E_2$$[X|\mathcal{F}]$ $I(\omega \in \Lambda_2)$ + $E_3$$[X|\mathcal{F}]$ $I(\omega \in \Lambda_3)$ + ..... Where $E_i[.]$ is the expectation calculated as per the conditional probability $P(.|\Lambda_i)$ Hence when $\omega$ is in $\Lambda_i$ the conditional expectation gives the expectation of random variable X given that the observed event is $\Lambda_i$ and hence use the modified conditional probability rather than the original one. However this interpretation is only valid as long as the conditioning sigma algebra is generated by a partition. Is there a similar interpretation for a general case? i.e what will it physically represent? Any help will be greatly appreciated! Thanks! Best, Adwait",,"['probability', 'probability-theory', 'conditional-probability']"
77,Probability that sum of rolling a 6-sided die 10 times is divisible by 10?,Probability that sum of rolling a 6-sided die 10 times is divisible by 10?,,"Here's a question I've been considering: Suppose you roll a usual 6-sided die 10 times and sum up the results of your rolls. What's the probability that it's divisible by 10? I've managed to solve it in a somewhat ugly fashion using the following generating series: $(x+x^2+x^3+x^4+x^5+x^6)^{10} = x^{10}(x^6 - 1)(1+x+x^2+\cdots)^{10}$ which makes finding the probability somewhat doable if I have a calculator or lots of free time to evaluate binomials. What's interesting though is that the probability ends up being just short of $\frac{1}{10}$ (in fact, it's about 0.099748). If instead, I roll the die $n$ times and find whether the sum is divisible by $n$, the probability is well approximated by $\frac{1}{n} - \epsilon$. Does anyone know how I can find the ""error"" term $\epsilon$ in terms of $n$?","Here's a question I've been considering: Suppose you roll a usual 6-sided die 10 times and sum up the results of your rolls. What's the probability that it's divisible by 10? I've managed to solve it in a somewhat ugly fashion using the following generating series: $(x+x^2+x^3+x^4+x^5+x^6)^{10} = x^{10}(x^6 - 1)(1+x+x^2+\cdots)^{10}$ which makes finding the probability somewhat doable if I have a calculator or lots of free time to evaluate binomials. What's interesting though is that the probability ends up being just short of $\frac{1}{10}$ (in fact, it's about 0.099748). If instead, I roll the die $n$ times and find whether the sum is divisible by $n$, the probability is well approximated by $\frac{1}{n} - \epsilon$. Does anyone know how I can find the ""error"" term $\epsilon$ in terms of $n$?",,"['probability', 'generating-functions']"
78,How many coin flips would it take to have a 90% chance of flipping 3 heads in a row?,How many coin flips would it take to have a 90% chance of flipping 3 heads in a row?,,"If you were to flip a fair coin independently over and over, hoping to get 3 heads in a row, how many coin flips would it take for you to have a 90+% chance of having succeeded? The way I've been thinking about this problem is as a Markov Chain with states: 0, 1, 2, and 3 heads in a row, where 3 is the absorptive state. The transition matrix T is: $$T=\begin{bmatrix}0.5&0.5&0&0\\0.5&0&0.5&0\\0.5&0&0&0.5\\0&0&0&1\end{bmatrix}$$ So roughly, what I'd like to be able to do is solve for n in the following equation  (let $k_1, k_2$ , and $k_3$ be arbitrary constants): $$\begin{bmatrix}1&0&0&0\end{bmatrix}*T^n =\begin{bmatrix}k_1&k_2&k_3&0.9\end{bmatrix}$$ Through trial and error on the calculator, I can figure out that [1 0 0 0] $*T^{30}\approx$ [0.050 0.027 0.014 0.908]. However, I'd like to be able to do this in a systematic way that could be applied to other Markov Chains, but I'm not sure how. Thanks for any help!","If you were to flip a fair coin independently over and over, hoping to get 3 heads in a row, how many coin flips would it take for you to have a 90+% chance of having succeeded? The way I've been thinking about this problem is as a Markov Chain with states: 0, 1, 2, and 3 heads in a row, where 3 is the absorptive state. The transition matrix T is: So roughly, what I'd like to be able to do is solve for n in the following equation  (let , and be arbitrary constants): Through trial and error on the calculator, I can figure out that [1 0 0 0] [0.050 0.027 0.014 0.908]. However, I'd like to be able to do this in a systematic way that could be applied to other Markov Chains, but I'm not sure how. Thanks for any help!","T=\begin{bmatrix}0.5&0.5&0&0\\0.5&0&0.5&0\\0.5&0&0&0.5\\0&0&0&1\end{bmatrix} k_1, k_2 k_3 \begin{bmatrix}1&0&0&0\end{bmatrix}*T^n =\begin{bmatrix}k_1&k_2&k_3&0.9\end{bmatrix} *T^{30}\approx","['probability', 'markov-chains']"
79,$\frac{1}{n}\sum_{k=1}^n(X_k-\mathbb E[X_k])$ converges a.s. to $0$,converges a.s. to,\frac{1}{n}\sum_{k=1}^n(X_k-\mathbb E[X_k]) 0,"Let $X_1,X_2,\dots$ be a sequence of independent random variables, such that the series $$\sum_{n=1}^\infty\frac{\operatorname{Var}(X_n)}{n^2}$$ converges. Show that as $n\to\infty$ , $$\frac{1}{n}\sum_{k=1}^n(X_k-\mathbb E[X_k])$$ converges almost surely to $0$ . There is a quick solution via the martingale convergence theorem: we have that $$Y_n=\sum_{k=1}^n\frac{X_k-\mathbb E[X_k]}{k}$$ is martingale, and $\sup_n\mathbb E[|Y_n|]$ is finite, so $Y_n$ converges almost surely to some random variable $Y$ , and we can finish with Kronecker's lemma. I'm interested though in any approaches avoiding the heavy machinery of the martingale convergence theorem. I feel like defining the $Y_n$ 's as I did above could be fruitful. For example, the Kolmogorov inequality gives the bound $$\mathbb P\left(\max_{1\leq i\leq n}|Y_i|>\varepsilon\right)\leq\frac{1}{\varepsilon^2}\mathbb E\left[Y_n^2\right]=\frac{1}{\varepsilon^2}\cdot\sum_{k=1}^n\frac{\operatorname{Var}(X_k)}{k^2}.$$ But I'm unsure what to make from all this.","Let be a sequence of independent random variables, such that the series converges. Show that as , converges almost surely to . There is a quick solution via the martingale convergence theorem: we have that is martingale, and is finite, so converges almost surely to some random variable , and we can finish with Kronecker's lemma. I'm interested though in any approaches avoiding the heavy machinery of the martingale convergence theorem. I feel like defining the 's as I did above could be fruitful. For example, the Kolmogorov inequality gives the bound But I'm unsure what to make from all this.","X_1,X_2,\dots \sum_{n=1}^\infty\frac{\operatorname{Var}(X_n)}{n^2} n\to\infty \frac{1}{n}\sum_{k=1}^n(X_k-\mathbb E[X_k]) 0 Y_n=\sum_{k=1}^n\frac{X_k-\mathbb E[X_k]}{k} \sup_n\mathbb E[|Y_n|] Y_n Y Y_n \mathbb P\left(\max_{1\leq i\leq n}|Y_i|>\varepsilon\right)\leq\frac{1}{\varepsilon^2}\mathbb E\left[Y_n^2\right]=\frac{1}{\varepsilon^2}\cdot\sum_{k=1}^n\frac{\operatorname{Var}(X_k)}{k^2}.","['probability', 'probability-theory']"
80,Median is twice the mean,Median is twice the mean,,"I am stuck at solving the following problem (at what I believe is the last step): Determine which distributions on the non-negative reals, if any, with mean $\mu$ are such that $2\mu$ is a median. My thoughts so far: Let's call the distribution in question $X$. Given that $X$ takes values in the non-negative reals, we can apply the Markov inequality $\left(P(X\geq t)\leq\frac{E[X]}{t}, \mbox{ for } t\geq 0\right)$. Taking $t = \mu$ in this inequality we get that  $$ P(X\geq 2\mu)\leq \frac{\mu}{2\mu} = \frac{1}{2} $$ However, given that $2\mu$ is the median, we have that $P(X\geq 2\mu)\geq \frac{1}{2}$. Hence we get that $P(X\geq 2\mu) = \frac{1}{2}$. Therefore we have equality in the Markov inequality . Hence $X\in\left\{ 0,2\mu\right\}$. How do I determine where $X$ takes the value $0$ and where $X$ takes the value $2\mu$ (if there is any such way)? From 1, how do I get the pdf of $X$?","I am stuck at solving the following problem (at what I believe is the last step): Determine which distributions on the non-negative reals, if any, with mean $\mu$ are such that $2\mu$ is a median. My thoughts so far: Let's call the distribution in question $X$. Given that $X$ takes values in the non-negative reals, we can apply the Markov inequality $\left(P(X\geq t)\leq\frac{E[X]}{t}, \mbox{ for } t\geq 0\right)$. Taking $t = \mu$ in this inequality we get that  $$ P(X\geq 2\mu)\leq \frac{\mu}{2\mu} = \frac{1}{2} $$ However, given that $2\mu$ is the median, we have that $P(X\geq 2\mu)\geq \frac{1}{2}$. Hence we get that $P(X\geq 2\mu) = \frac{1}{2}$. Therefore we have equality in the Markov inequality . Hence $X\in\left\{ 0,2\mu\right\}$. How do I determine where $X$ takes the value $0$ and where $X$ takes the value $2\mu$ (if there is any such way)? From 1, how do I get the pdf of $X$?",,"['probability', 'probability-distributions', 'means', 'median']"
81,"Is it ""bad form"" to count the number of outcomes in an event to compute a probability?","Is it ""bad form"" to count the number of outcomes in an event to compute a probability?",,"I've been reading this very interesting blog post entitled "" A review of probability theory "" from Terence Tao. Here are a few quotes from the blog post: Elements of the sample space $\Omega$ will be denoted $\omega$.   However, for reasons that will be explained shortly, we will try to   avoid actually referring to such elements unless absolutely required   to. ... In order to have the freedom to perform extensions every time we need   to introduce a new source of randomness, we will try to adhere to the   following important dogma: probability theory is only “allowed” to   study concepts and perform operations which are preserved with respect   to extension of the underlying sample space. (This is analogous to how   differential geometry is only “allowed” to study concepts and perform   operations that are preserved with respect to coordinate change, or   how graph theory is only “allowed” to study concepts and perform   operations that are preserved with respect to relabeling of the   vertices, etc..) As long as one is adhering strictly to this dogma,   one can insert as many new sources of randomness (or reorganise   existing sources of randomness) as one pleases; but if one deviates   from this dogma and uses specific properties of a single sample space,   then one has left the category of probability theory and must now take   care when doing any subsequent operation that could alter that sample   space. This dogma is an important aspect of the probabilistic way of   thinking, much as the insistence on studying concepts and performing   operations that are invariant with respect to coordinate changes or   other symmetries is an important aspect of the modern geometric way of   thinking. With this probabilistic viewpoint, we shall soon see the   sample space essentially disappear from view altogether, after a few   foundational issues are dispensed with. Let’s give some simple examples of what is and what is not a   probabilistic concept or operation. The probability $P(E)$ of an   event is a probabilistic concept; it is preserved under extensions.   Similarly, boolean operations on events such as union, intersection,   and complement are also preserved under extensions and are thus also   probabilistic operations. The emptiness or non-emptiness of an event   $E$ is also probabilistic, as is the equality or non-equality of two   events $E,F$ (note how it was important here that we demanded the map    $\pi$ to be surjective in the definition of an extension). On the   other hand, the cardinality of an event is not a probabilistic   concept; for instance, the event that the roll of a given die gives   $4$ has cardinality one in the sample space $\{1,\ldots,6\}$, but has   cardinality six in the sample space $\{1,\ldots,6\} \times \{1,\ldots,6\}$ when the values of an additional die are used to   extend the sample space. Thus, in the probabilistic way of thinking,   one should avoid thinking about events as having cardinality , except   to the extent that they are either empty or non-empty. [The bold is mine.] This seems to be a very insightful viewpoint. But, in introductory probability classes, it is very common to compute the probability of an event by counting the number of outcomes in the event, and dividing by the total number of outcomes in the sample space (assuming that all outcomes in the sample space are equally likely). Is this bad form?  In such cases, would it be preferable to compute the probabilities using an approach that does not involve counting the number of elements of an event? Perhaps an anology is that, in linear algebra, we often prefer proofs that don't use coordinates.","I've been reading this very interesting blog post entitled "" A review of probability theory "" from Terence Tao. Here are a few quotes from the blog post: Elements of the sample space $\Omega$ will be denoted $\omega$.   However, for reasons that will be explained shortly, we will try to   avoid actually referring to such elements unless absolutely required   to. ... In order to have the freedom to perform extensions every time we need   to introduce a new source of randomness, we will try to adhere to the   following important dogma: probability theory is only “allowed” to   study concepts and perform operations which are preserved with respect   to extension of the underlying sample space. (This is analogous to how   differential geometry is only “allowed” to study concepts and perform   operations that are preserved with respect to coordinate change, or   how graph theory is only “allowed” to study concepts and perform   operations that are preserved with respect to relabeling of the   vertices, etc..) As long as one is adhering strictly to this dogma,   one can insert as many new sources of randomness (or reorganise   existing sources of randomness) as one pleases; but if one deviates   from this dogma and uses specific properties of a single sample space,   then one has left the category of probability theory and must now take   care when doing any subsequent operation that could alter that sample   space. This dogma is an important aspect of the probabilistic way of   thinking, much as the insistence on studying concepts and performing   operations that are invariant with respect to coordinate changes or   other symmetries is an important aspect of the modern geometric way of   thinking. With this probabilistic viewpoint, we shall soon see the   sample space essentially disappear from view altogether, after a few   foundational issues are dispensed with. Let’s give some simple examples of what is and what is not a   probabilistic concept or operation. The probability $P(E)$ of an   event is a probabilistic concept; it is preserved under extensions.   Similarly, boolean operations on events such as union, intersection,   and complement are also preserved under extensions and are thus also   probabilistic operations. The emptiness or non-emptiness of an event   $E$ is also probabilistic, as is the equality or non-equality of two   events $E,F$ (note how it was important here that we demanded the map    $\pi$ to be surjective in the definition of an extension). On the   other hand, the cardinality of an event is not a probabilistic   concept; for instance, the event that the roll of a given die gives   $4$ has cardinality one in the sample space $\{1,\ldots,6\}$, but has   cardinality six in the sample space $\{1,\ldots,6\} \times \{1,\ldots,6\}$ when the values of an additional die are used to   extend the sample space. Thus, in the probabilistic way of thinking,   one should avoid thinking about events as having cardinality , except   to the extent that they are either empty or non-empty. [The bold is mine.] This seems to be a very insightful viewpoint. But, in introductory probability classes, it is very common to compute the probability of an event by counting the number of outcomes in the event, and dividing by the total number of outcomes in the sample space (assuming that all outcomes in the sample space are equally likely). Is this bad form?  In such cases, would it be preferable to compute the probabilities using an approach that does not involve counting the number of elements of an event? Perhaps an anology is that, in linear algebra, we often prefer proofs that don't use coordinates.",,['probability']
82,Two definitions of p-value,Two definitions of p-value,,"According to Casella-Berger (2002) the definition of p-value (def. 8.3.26, §8.3.4, p. 397) is: A p-value $p(X)$ is a test statistic satisfying $0 \le p(x) \le 1$ for every sample point $x$ . Small values of $p(X)$ give evidence that $H_1$ is true. A p-value is valid if, for every $\theta \in \Theta_0$ and every $0 \le \alpha \le 1$ , $P_{\theta}(p(X) \le \alpha) \le  \alpha$ . However, other books such as Rohatgi (2001) define it as: The probability of observing under $H_0$ a sample outcome at least as extreme as the one observed is called the P-value. The smaller the P-value, the more extreme the outcome and the stronger the evidence against $H_0$ . I feel this definition is similar in spirit to the one by Schervish (2012): p-value. In general, the p-value is the smallest level $\alpha_0$ such that we would reject the null-hypothesis at level $\alpha_0$ with the observed data. How are these definitions equivalent?","According to Casella-Berger (2002) the definition of p-value (def. 8.3.26, §8.3.4, p. 397) is: A p-value is a test statistic satisfying for every sample point . Small values of give evidence that is true. A p-value is valid if, for every and every , . However, other books such as Rohatgi (2001) define it as: The probability of observing under a sample outcome at least as extreme as the one observed is called the P-value. The smaller the P-value, the more extreme the outcome and the stronger the evidence against . I feel this definition is similar in spirit to the one by Schervish (2012): p-value. In general, the p-value is the smallest level such that we would reject the null-hypothesis at level with the observed data. How are these definitions equivalent?","p(X) 0 \le p(x) \le 1 x p(X) H_1 \theta \in \Theta_0 0 \le \alpha \le 1 P_{\theta}(p(X) \le \alpha) \le
 \alpha H_0 H_0 \alpha_0 \alpha_0","['probability', 'statistics', 'statistical-inference', 'hypothesis-testing', 'p-value']"
83,Finding the method of moments estimator for the Uniform Distribution,Finding the method of moments estimator for the Uniform Distribution,,"Let $X_1, \ldots, X_n \sim \text{Uniform}(a,b)$ where $a$ and $b$ are unknown paramaters and $a < b$. (a) Find the method of moments estimators for $a$ and $b$. (b) Find the MLE $\hat{a}$ and $\hat{b}$. For part (b), consider that $$ f(x) = \begin{cases} 0 & \text{ if } x \notin [a,b] \\                      1/(b-a) & \text{ if } x \in [a,b] \\ \end{cases}  $$ Thus, the MLE estimate will be $(\min \{X_1, \ldots, X_n \}$, $\max \{X_1, \ldots, X_n \})$. But what about part (a)?","Let $X_1, \ldots, X_n \sim \text{Uniform}(a,b)$ where $a$ and $b$ are unknown paramaters and $a < b$. (a) Find the method of moments estimators for $a$ and $b$. (b) Find the MLE $\hat{a}$ and $\hat{b}$. For part (b), consider that $$ f(x) = \begin{cases} 0 & \text{ if } x \notin [a,b] \\                      1/(b-a) & \text{ if } x \in [a,b] \\ \end{cases}  $$ Thus, the MLE estimate will be $(\min \{X_1, \ldots, X_n \}$, $\max \{X_1, \ldots, X_n \})$. But what about part (a)?",,"['probability', 'statistics']"
84,How to derive the posterior predictive distribution?,How to derive the posterior predictive distribution?,,"I often seen the posterior predictive distribution mentioned in the context of machine learning and bayesian inference. The definition is as follows: $ p(D'|D) = \int_\theta p(D'|\theta)p(\theta|D)$ How/why does the integral on the right equal the probability distribution on the left? In other words, which laws of probability can I use to derive $p(D'|D)$ given the integral? Edit - After further consideration, I think I am able to see much of the derivation. That is, $p(D'|D) = \int_\theta p(D', \theta | D)$ via the law of total probability $p(D'|D) = \int_\theta p(D' | D, \theta) * p(\theta | D)$  via the chain rule But I don't understand why $D$ may be dropped from the list of conditioned variables belonging to the integral's first term.","I often seen the posterior predictive distribution mentioned in the context of machine learning and bayesian inference. The definition is as follows: $ p(D'|D) = \int_\theta p(D'|\theta)p(\theta|D)$ How/why does the integral on the right equal the probability distribution on the left? In other words, which laws of probability can I use to derive $p(D'|D)$ given the integral? Edit - After further consideration, I think I am able to see much of the derivation. That is, $p(D'|D) = \int_\theta p(D', \theta | D)$ via the law of total probability $p(D'|D) = \int_\theta p(D' | D, \theta) * p(\theta | D)$  via the chain rule But I don't understand why $D$ may be dropped from the list of conditioned variables belonging to the integral's first term.",,"['probability', 'statistics', 'probability-distributions', 'bayesian']"
85,Probability: the average times to make all the balls the same color,Probability: the average times to make all the balls the same color,,"Suppose there are n balls with different colors with each other in a bag. In one loop, One take two balls in sequence out of the bag and replace them with two balls with the same color of the first ball.  Q: how many loops does it take to make all the balls the same color on average?","Suppose there are n balls with different colors with each other in a bag. In one loop, One take two balls in sequence out of the bag and replace them with two balls with the same color of the first ball.  Q: how many loops does it take to make all the balls the same color on average?",,['probability']
86,"What does ""twice as likely"" mean?","What does ""twice as likely"" mean?",,"Once in a while I hear people say something like X is twice as likely as Y. What they usually mean is: $$p(X) = 2 \cdot p(Y)$$ and - in the context they refer to - they usually have $p(Y) < \frac{1}{2}$. But what do you do if $p(Y) > \frac{1}{2}$? Can there be an event $X$ that is twice as likely as $Y$? It also feels wrong to me to say that $p(X) = 100 \%$ is twice as likely as $p(Y) = 50\%$. Is there a good definition what twice as likely means? Some thoughts about this Let's call this ""twice as likely"" a function $$d: D \rightarrow [0, 1]$$ I would expect $d$ to have the following properties: $D = [0, m]\subseteq [0,1]$ $d(0) = 0 $ $d$ is monotonous","Once in a while I hear people say something like X is twice as likely as Y. What they usually mean is: $$p(X) = 2 \cdot p(Y)$$ and - in the context they refer to - they usually have $p(Y) < \frac{1}{2}$. But what do you do if $p(Y) > \frac{1}{2}$? Can there be an event $X$ that is twice as likely as $Y$? It also feels wrong to me to say that $p(X) = 100 \%$ is twice as likely as $p(Y) = 50\%$. Is there a good definition what twice as likely means? Some thoughts about this Let's call this ""twice as likely"" a function $$d: D \rightarrow [0, 1]$$ I would expect $d$ to have the following properties: $D = [0, m]\subseteq [0,1]$ $d(0) = 0 $ $d$ is monotonous",,"['probability', 'recreational-mathematics']"
87,"If, tossing a coin 400 times, we count the heads, what is the probability that the number of heads is [160,190]?","If, tossing a coin 400 times, we count the heads, what is the probability that the number of heads is [160,190]?",,"I wanted to solve the problem with the Central Limit Theorem. Analyzing the question, I modeled the situation with a random variable : $$\begin{cases} 1 & \text{with probability } 1/2; \\ 0 & \text{with probability } 1/2; \end{cases} $$ Calculating the mean $\mu = \frac{1}{2}$ and the variance $\sigma^2 = \frac{1}{4}$. Then, I thougt that since the number of repetitions is >> 30 I tried to fit the probability function to a Gaussian normal $N(400\mu,400\sigma) = N(200,100)$. Calculating and ""normalizing""... $$\begin{align} & P(160 < x < 190)\\ &= P\left( \frac{160-200}{20 \times 100} < z < \frac{190-200}{20 \times 100}\right) \\ &= P(-0.02 < z < -0.005). \end{align}$$ And here I'm stuck because I wanted to use the relation $$        P(160 < x < 190) = \Phi(-0.005) - \Phi(-0.02) $$ That result negative.. Do you see any error in my strategy?","I wanted to solve the problem with the Central Limit Theorem. Analyzing the question, I modeled the situation with a random variable : $$\begin{cases} 1 & \text{with probability } 1/2; \\ 0 & \text{with probability } 1/2; \end{cases} $$ Calculating the mean $\mu = \frac{1}{2}$ and the variance $\sigma^2 = \frac{1}{4}$. Then, I thougt that since the number of repetitions is >> 30 I tried to fit the probability function to a Gaussian normal $N(400\mu,400\sigma) = N(200,100)$. Calculating and ""normalizing""... $$\begin{align} & P(160 < x < 190)\\ &= P\left( \frac{160-200}{20 \times 100} < z < \frac{190-200}{20 \times 100}\right) \\ &= P(-0.02 < z < -0.005). \end{align}$$ And here I'm stuck because I wanted to use the relation $$        P(160 < x < 190) = \Phi(-0.005) - \Phi(-0.02) $$ That result negative.. Do you see any error in my strategy?",,['probability']
88,Central Limit Theorem Definition,Central Limit Theorem Definition,,"My friend and I have a bet going about the definition of the Central Limit Theorem. If we define an example as a number drawn at random from some probability density function where the function has a defined finite mean and variance. And we define a sample as a set of size N examples (with N>1). Then, we take S samples and create a sampling distribution D over the means of each individual sample. I am arguing that the Central Limit Theorem states that as the number of samples S approaches infinity, then the sampling distribution D will approximate a normal distribution. My friend is arguing that the Central Limit Theorem states that given any number of samples S, sampling distribution D will not necessarily approximate a normal distribution, but as the number of examples per sample N approaches infinity, then D will approximate a normal distribution. Who is right? Update: I lost this bet.","My friend and I have a bet going about the definition of the Central Limit Theorem. If we define an example as a number drawn at random from some probability density function where the function has a defined finite mean and variance. And we define a sample as a set of size N examples (with N>1). Then, we take S samples and create a sampling distribution D over the means of each individual sample. I am arguing that the Central Limit Theorem states that as the number of samples S approaches infinity, then the sampling distribution D will approximate a normal distribution. My friend is arguing that the Central Limit Theorem states that given any number of samples S, sampling distribution D will not necessarily approximate a normal distribution, but as the number of examples per sample N approaches infinity, then D will approximate a normal distribution. Who is right? Update: I lost this bet.",,"['probability', 'statistics', 'probability-theory', 'probability-distributions', 'normal-distribution']"
89,Expected number of Pareto-optimal points,Expected number of Pareto-optimal points,,"Suppose $S$ is a set of $n$ points in a plane. A point is called maximal (or Pareto-optimal) if no other point in $S$ is both above and to the right of that point. If each point in $S$ is chosen independently and uniformly at random from the unit square $[0,1]\times [1,0]$.  What is the exact expected number of Pareto-optimal points in $S$?","Suppose $S$ is a set of $n$ points in a plane. A point is called maximal (or Pareto-optimal) if no other point in $S$ is both above and to the right of that point. If each point in $S$ is chosen independently and uniformly at random from the unit square $[0,1]\times [1,0]$.  What is the exact expected number of Pareto-optimal points in $S$?",,['probability']
90,Most Probable Sum [duplicate],Most Probable Sum [duplicate],,This question already has answers here : Closed 12 years ago . Possible Duplicate: Probability of dice sum just greater than 100 A fair dice is rolled and the outcome of the face is summed up each time. We stop rolling when the sum becomes greater than 100.  Which of the following is most probable sum? 103 102 100 101 All have equal probability How best do I approach these types of problems?,This question already has answers here : Closed 12 years ago . Possible Duplicate: Probability of dice sum just greater than 100 A fair dice is rolled and the outcome of the face is summed up each time. We stop rolling when the sum becomes greater than 100.  Which of the following is most probable sum? 103 102 100 101 All have equal probability How best do I approach these types of problems?,,"['probability', 'dice']"
91,Expected value of the stochastic integral $\int_0^t e^{as} dW_s$,Expected value of the stochastic integral,\int_0^t e^{as} dW_s,"I am trying to calculate a stochastic integral  $\mathbb{E}[\int_0^t e^{as} dW_s]$. I tried breaking it up into a Riemann sum  $\mathbb{E}[\sum e^{as_{t_i}}(W_{t_i}-W_{t_{i-1}})]$, but I get expected value of $0$, since $\mathbb{E}(W_{t_i}-W_{t_{i-1}}) =0$. But I think it's wrong.  Thanks! And I want to calculate $\mathbb{E}[W_t \int_0^t e^{as} dW_s]$ as well, I write $W_t=\int_0^t dW_s$ and get  $\mathbb{E}[W_t \int_0^t e^{as} dW_s]=\mathbb{E}[\int_0^t e^{as} dW_s]$. Is that ok? ($W_t$ is brownian motion.)","I am trying to calculate a stochastic integral  $\mathbb{E}[\int_0^t e^{as} dW_s]$. I tried breaking it up into a Riemann sum  $\mathbb{E}[\sum e^{as_{t_i}}(W_{t_i}-W_{t_{i-1}})]$, but I get expected value of $0$, since $\mathbb{E}(W_{t_i}-W_{t_{i-1}}) =0$. But I think it's wrong.  Thanks! And I want to calculate $\mathbb{E}[W_t \int_0^t e^{as} dW_s]$ as well, I write $W_t=\int_0^t dW_s$ and get  $\mathbb{E}[W_t \int_0^t e^{as} dW_s]=\mathbb{E}[\int_0^t e^{as} dW_s]$. Is that ok? ($W_t$ is brownian motion.)",,"['probability', 'probability-theory', 'stochastic-processes', 'stochastic-integrals']"
92,Manifold with minimum surface distance between two points,Manifold with minimum surface distance between two points,,"The book ""The World is Flat"" uses flatness as a metaphor for a global economy. In fact, a spherical world would seem to be better than a flat world in terms of reducing the distances between two random points on the surface of the world. The shorter the distance between any two points, the easier it is for information and objects to travel between different places.    While it may be obvious that a spherical world is better than a flat world, it's far from obvious that a spherical world is optimal in this regard, which brings me to the question: what should the book have been titled?  More precisely: Question: Define a world to be a 2-manifold with some fixed surface area S and a metric d that calculates distance on the surface of the manifold. What shaped world minimizes average distance between any two randomly selected points in the world? Does the answer depend on whether the world can be embedded in $\mathbb{R}^3$? Does it depend on the specific metric used? Does it depend on how we define ""average distance"" or ""randomly selected?""","The book ""The World is Flat"" uses flatness as a metaphor for a global economy. In fact, a spherical world would seem to be better than a flat world in terms of reducing the distances between two random points on the surface of the world. The shorter the distance between any two points, the easier it is for information and objects to travel between different places.    While it may be obvious that a spherical world is better than a flat world, it's far from obvious that a spherical world is optimal in this regard, which brings me to the question: what should the book have been titled?  More precisely: Question: Define a world to be a 2-manifold with some fixed surface area S and a metric d that calculates distance on the surface of the manifold. What shaped world minimizes average distance between any two randomly selected points in the world? Does the answer depend on whether the world can be embedded in $\mathbb{R}^3$? Does it depend on the specific metric used? Does it depend on how we define ""average distance"" or ""randomly selected?""",,"['probability', 'geometry', 'optimization']"
93,Does repeated activity increase the probability of a specific event happening?,Does repeated activity increase the probability of a specific event happening?,,"I am hoping that this is not too basic a question for this site, but I am seeking to better understand a conversation I have had with my more mathematically inclined friend. The discussion was around the probability of an event occuring with repeated exposure to a specific activity. The example in the conversation was the risk of (for example) injury occuring compared between someone who was a frequent skydiver (perhaps weekly) and someone who skydives once or twice a year. For the point of this exercise we are ignoring all external factors (wind, experience, etc) and are assuming that the risk of any injury (i.e. not one that would preclude someone from continuing to skydive) is fixed for each event. That is, as with a coin toss, each individual event is unaffected by the other events. To my mind it makes more sense that the person who chooses to skydive regularly is accepting a higher risk over the course of the year then the person who skydives once a year. My friend contests that the risk of an event happening is equal for both people over the course of the year. He attempted to explain this to me, but could not in a way that I understood. To clarify, I don't believe the risk increases cumulatively (i.e. if there was a 1/10 chance of something happening in one dive then I understand that this does not mean that something will definitely happen if a person dives 10 times). I am quite curious about which answer is correct, probability was always one of those areas that I sometimes found to be non-intuitive. Any insight here would be appreciated Thanks!","I am hoping that this is not too basic a question for this site, but I am seeking to better understand a conversation I have had with my more mathematically inclined friend. The discussion was around the probability of an event occuring with repeated exposure to a specific activity. The example in the conversation was the risk of (for example) injury occuring compared between someone who was a frequent skydiver (perhaps weekly) and someone who skydives once or twice a year. For the point of this exercise we are ignoring all external factors (wind, experience, etc) and are assuming that the risk of any injury (i.e. not one that would preclude someone from continuing to skydive) is fixed for each event. That is, as with a coin toss, each individual event is unaffected by the other events. To my mind it makes more sense that the person who chooses to skydive regularly is accepting a higher risk over the course of the year then the person who skydives once a year. My friend contests that the risk of an event happening is equal for both people over the course of the year. He attempted to explain this to me, but could not in a way that I understood. To clarify, I don't believe the risk increases cumulatively (i.e. if there was a 1/10 chance of something happening in one dive then I understand that this does not mean that something will definitely happen if a person dives 10 times). I am quite curious about which answer is correct, probability was always one of those areas that I sometimes found to be non-intuitive. Any insight here would be appreciated Thanks!",,['probability']
94,Probability that Mercury is the nearest planet to Earth.,Probability that Mercury is the nearest planet to Earth.,,"Motivation : We tend to think of Venus as the nearest planet to Earth because at its nearest approach to Earth, Venus is the closest at 39 million Km away. This is followed by Mars at 56 million Km and Mercury at 83 million Km. Surprisingly, if we look at the entire year, Mercury ends up being the closest to Earth 46% of the time, followed by Venus 36% of the time, and Mars 18%. This is because orbits of Venus and Mars are huge compared to that of Mercury so while Mercury does not have the nearest approach to Earth, its is never too far away; but Venus and Mars spend most of their time far away from Earth. In fact on an average, Mercury is the nearest planet to every other planet in the solar system . Our simplistic system : Consider a circular co-planar planetary system with a sun in the center and $n$ planets $p_1, p_2,\ldots, p_n$ orbiting counter clockwise at radius $r_1 < r_2 <\ldots < r_n$ respectively. Assuming the mass of the planets are negligible compared to that of the sun, orbital velocity is $v_i = \displaystyle \frac{\beta}{\sqrt{r_i}}$ , where $\beta$ is some constant. For any given planet, the the remaining planets are equally likely to be anywhere in their respective orbits i.e. uniformly distributed relative to one another over a long period of time. Also, if $r_{i+1} - r_i > r_{i-1} + r_{i}$ then $p_{i+1}$ will never be the nearest planet to $p_{i}$ . So to make the system interesting, we must have $r_{i+1} < r_{i-1} + 2 r_{i} < 3r_i$ . Hence we assume that $r_{i+1} = \alpha r_{i} = r_1 \alpha^i$ where $1 < \alpha < 3$ and $i \ge 1$ . This makes sense because most planetary system are know to follow some kind of Titius Bode type rules with their own set of constants. Question : What is the probability that the inner most planet is the nearest planet to the $k$ -th planet? Is it possible to have a closed form in term of $r_i, \alpha$ and $\beta$ ? Update : I think the direction and orbital velocity matters because once a planet $p_i$ becomes the nearest planet of $p_k$ , it remains the nearest planet to $p_k$ for some time until another planet $p_j$ becomes the nearest i.e. even though the relative positions of the planets are uniformly distributed over a long period of time, there is a non-random order which determines which is the nearest at time $T+1$ given the state of the system at time $T$ . I have updated the questions accordingly.","Motivation : We tend to think of Venus as the nearest planet to Earth because at its nearest approach to Earth, Venus is the closest at 39 million Km away. This is followed by Mars at 56 million Km and Mercury at 83 million Km. Surprisingly, if we look at the entire year, Mercury ends up being the closest to Earth 46% of the time, followed by Venus 36% of the time, and Mars 18%. This is because orbits of Venus and Mars are huge compared to that of Mercury so while Mercury does not have the nearest approach to Earth, its is never too far away; but Venus and Mars spend most of their time far away from Earth. In fact on an average, Mercury is the nearest planet to every other planet in the solar system . Our simplistic system : Consider a circular co-planar planetary system with a sun in the center and planets orbiting counter clockwise at radius respectively. Assuming the mass of the planets are negligible compared to that of the sun, orbital velocity is , where is some constant. For any given planet, the the remaining planets are equally likely to be anywhere in their respective orbits i.e. uniformly distributed relative to one another over a long period of time. Also, if then will never be the nearest planet to . So to make the system interesting, we must have . Hence we assume that where and . This makes sense because most planetary system are know to follow some kind of Titius Bode type rules with their own set of constants. Question : What is the probability that the inner most planet is the nearest planet to the -th planet? Is it possible to have a closed form in term of and ? Update : I think the direction and orbital velocity matters because once a planet becomes the nearest planet of , it remains the nearest planet to for some time until another planet becomes the nearest i.e. even though the relative positions of the planets are uniformly distributed over a long period of time, there is a non-random order which determines which is the nearest at time given the state of the system at time . I have updated the questions accordingly.","n p_1, p_2,\ldots, p_n r_1 < r_2 <\ldots < r_n v_i = \displaystyle \frac{\beta}{\sqrt{r_i}} \beta r_{i+1} - r_i > r_{i-1} + r_{i} p_{i+1} p_{i} r_{i+1} < r_{i-1} + 2 r_{i} < 3r_i r_{i+1} = \alpha r_{i} = r_1 \alpha^i 1 < \alpha < 3 i \ge 1 k r_i, \alpha \beta p_i p_k p_k p_j T+1 T","['probability', 'geometry', 'circles', 'uniform-distribution', 'geometric-probability']"
95,"Uniformly choose two derangements $\sigma_i,\sigma_j$. What is the distribution of $\sigma_i\circ \sigma_j$?",Uniformly choose two derangements . What is the distribution of ?,"\sigma_i,\sigma_j \sigma_i\circ \sigma_j","I was working on this question and come up with this problem. Let $\sigma_i,\sigma_j$ be two uniformly chosen derangements, i.e. $\sigma_i,\sigma_j \in D_n = \{\sigma \in S_n : \sigma(i)\neq i~\forall i\}$ . What is the distribution of $\sigma_i\circ \sigma_j$ ? Here we can see that every $\sigma \in S_n$ is the composition of two $\sigma_i,\sigma_j\in D_n$ . But some permutations are easier to produce than others. For instance, let $\sigma^*$ be the identity, we have $\sigma_i\circ\sigma_j = \sigma^*$ iif $\sigma_j =\sigma_i^{-1}$ . So, we are free to choose $\sigma_i$ , but then $\sigma_j$ is defined, therefore: $$P(\sigma_i\circ\sigma_j = \sigma^*) = \frac{1}{|D_n|} > \frac{1}{|S_n|} $$ But $\lim_{n\to\infty}\frac{|D_n|}{|S_n|} = \frac{1}{e}$ (see here ), it leads me to think that although it is not uniform, it could at least have the same order. So, my guess is that we can find $c,C > 0$ such that $$c\frac{1}{|S_n|} \leq P(\sigma_i\circ\sigma_j = \sigma) \leq C \frac{1}{|S_n|}~\forall \sigma\in S_n$$ for sufficiently large values of $n$ .","I was working on this question and come up with this problem. Let be two uniformly chosen derangements, i.e. . What is the distribution of ? Here we can see that every is the composition of two . But some permutations are easier to produce than others. For instance, let be the identity, we have iif . So, we are free to choose , but then is defined, therefore: But (see here ), it leads me to think that although it is not uniform, it could at least have the same order. So, my guess is that we can find such that for sufficiently large values of .","\sigma_i,\sigma_j \sigma_i,\sigma_j \in D_n = \{\sigma \in S_n : \sigma(i)\neq i~\forall i\} \sigma_i\circ \sigma_j \sigma \in S_n \sigma_i,\sigma_j\in D_n \sigma^* \sigma_i\circ\sigma_j = \sigma^* \sigma_j =\sigma_i^{-1} \sigma_i \sigma_j P(\sigma_i\circ\sigma_j = \sigma^*) = \frac{1}{|D_n|} > \frac{1}{|S_n|}  \lim_{n\to\infty}\frac{|D_n|}{|S_n|} = \frac{1}{e} c,C > 0 c\frac{1}{|S_n|} \leq P(\sigma_i\circ\sigma_j = \sigma) \leq C \frac{1}{|S_n|}~\forall \sigma\in S_n n","['probability', 'group-theory', 'probability-distributions', 'permutations', 'symmetric-groups']"
96,Dice: Rolling at least N successes where number of succeses vary by dice value,Dice: Rolling at least N successes where number of succeses vary by dice value,,"Rules I have four different types of dice: six-, eight-, ten- and twelve-sided (d6, d8, d10 & d12, respectively). The number of successes vary by the value rolled (and thus indirectly by dice type). One success is gained by rolling 6 or 7. Two successes are gained by rolling 8 or 9. Three successes are gained by rolling 10 or 11. Four successes are gained by rolling 12. This means that a 1d6 can result in at most 1 success, 1d8 1-2 successes, 1d10 1-3, and 1d12 1-4. Successes are added together after the roll, so rolling 6 dice and getting [12, 3, 8, 7, 10, 1] will result in 4 + 2 + 1 + 3 = 10 successes. Input is the number of dice and how many sides they have, and the minimum amount of successes I want to achieve. Question My main question is this: Given that I roll a known combination of d6s, d8s, d10s and d12s, how do I calculate the probability of rolling N or more successes? Q1 (though feel free to answer any other questions in this post as well, indexed Q $n$ for your convenience) Context I know how to calculate the probability of rolling at least $N$ successes for an arbitrary number of d6's, since they can only yield one success at most. I am stuck, however, when it comes to calculating at least $N$ successes when rolling a mix of differently sided dice, where some of them can yield more than one success. For example, with $5$ d6, $1$ d8, $1$ d12, how likely am I to roll $\geq$ 4 successes? Q2 EDIT: It's been brought to my attention that there is no closed form solution to this question. That is fine; any solution or clever approximation that's more efficient than running 100k simulated rolls is a sufficient answer. Can the problem be split into separate probabilities that can later be combined? E.g., given 5d6 & 1d12 and that I'm looking for the probability of at least $k$ successes, can I calculate the probabilities for each die type separately and later combine them somehow? Q3 Also, how would I go about calculating $\geq k$ successes for 1d12? For 2d12? For $n$ d12? Q4 Currently, I can 'solve' the problem by running a simulation, but it irks me that I am not able come up with anything better.","Rules I have four different types of dice: six-, eight-, ten- and twelve-sided (d6, d8, d10 & d12, respectively). The number of successes vary by the value rolled (and thus indirectly by dice type). One success is gained by rolling 6 or 7. Two successes are gained by rolling 8 or 9. Three successes are gained by rolling 10 or 11. Four successes are gained by rolling 12. This means that a 1d6 can result in at most 1 success, 1d8 1-2 successes, 1d10 1-3, and 1d12 1-4. Successes are added together after the roll, so rolling 6 dice and getting [12, 3, 8, 7, 10, 1] will result in 4 + 2 + 1 + 3 = 10 successes. Input is the number of dice and how many sides they have, and the minimum amount of successes I want to achieve. Question My main question is this: Given that I roll a known combination of d6s, d8s, d10s and d12s, how do I calculate the probability of rolling N or more successes? Q1 (though feel free to answer any other questions in this post as well, indexed Q for your convenience) Context I know how to calculate the probability of rolling at least successes for an arbitrary number of d6's, since they can only yield one success at most. I am stuck, however, when it comes to calculating at least successes when rolling a mix of differently sided dice, where some of them can yield more than one success. For example, with d6, d8, d12, how likely am I to roll 4 successes? Q2 EDIT: It's been brought to my attention that there is no closed form solution to this question. That is fine; any solution or clever approximation that's more efficient than running 100k simulated rolls is a sufficient answer. Can the problem be split into separate probabilities that can later be combined? E.g., given 5d6 & 1d12 and that I'm looking for the probability of at least successes, can I calculate the probabilities for each die type separately and later combine them somehow? Q3 Also, how would I go about calculating successes for 1d12? For 2d12? For d12? Q4 Currently, I can 'solve' the problem by running a simulation, but it irks me that I am not able come up with anything better.",n N N 5 1 1 \geq k \geq k n,"['probability', 'combinatorics', 'dice']"
97,Proof of sub-additivity for Shannon Entropy,Proof of sub-additivity for Shannon Entropy,,"in A Mathematical Theory of Communication (CE Shannon, 1948) , the entropy of a categorical random variable is defined as: $$H(X)=-\sum_{i}P(X=i)\log P(X=i)$$ while the joint entropy of two such variables is defined as: $$H(X,Y)=-\sum_{i,j}P(X=i,Y=j)\log P(X=i,Y=j)$$ Then it is stated (p.12) that ""It is easily shown that"": $$H(X,Y)\leq H(X)+H(Y)$$ with equality for independence. I believe this property is referred to as sub-additivity, and I'm wondering what this ""easy"" way to prove it might be. What I have tried so far: I believe, using the Law of Total Probability, we can get $H(X)+H(Y)=-\sum_{i,j}P(X=i,Y=j)\log (P(X=)P(Y=j))$ which would establish the equality for independence, but I don't know how to get the inequality. That would seem to require $P(X=i,Y=j)\leq P(X=i)P(Y=i)$ which is not always true. I've found a source that proves the inequality using two other properties ( Chain Rule and Dropping the Conditional ). But these are introduced later in Shannon's paper so can't be the ""easy"" proof he had in mind?","in A Mathematical Theory of Communication (CE Shannon, 1948) , the entropy of a categorical random variable is defined as: while the joint entropy of two such variables is defined as: Then it is stated (p.12) that ""It is easily shown that"": with equality for independence. I believe this property is referred to as sub-additivity, and I'm wondering what this ""easy"" way to prove it might be. What I have tried so far: I believe, using the Law of Total Probability, we can get which would establish the equality for independence, but I don't know how to get the inequality. That would seem to require which is not always true. I've found a source that proves the inequality using two other properties ( Chain Rule and Dropping the Conditional ). But these are introduced later in Shannon's paper so can't be the ""easy"" proof he had in mind?","H(X)=-\sum_{i}P(X=i)\log P(X=i) H(X,Y)=-\sum_{i,j}P(X=i,Y=j)\log P(X=i,Y=j) H(X,Y)\leq H(X)+H(Y) H(X)+H(Y)=-\sum_{i,j}P(X=i,Y=j)\log (P(X=)P(Y=j)) P(X=i,Y=j)\leq P(X=i)P(Y=i)","['probability', 'entropy']"
98,Centre in N-sided polygon on circle [closed],Centre in N-sided polygon on circle [closed],,"Closed. This question is off-topic . It is not currently accepting answers. This question is missing context or other details : Please improve the question by providing additional context, which ideally includes your thoughts on the problem and any attempts you have made to solve it. This information helps others identify where you have difficulties and helps them write answers appropriate to your experience level. Closed 6 years ago . Improve this question What's the probability that a n-sided polygon made from n distinct random points on circle contain the centre?","Closed. This question is off-topic . It is not currently accepting answers. This question is missing context or other details : Please improve the question by providing additional context, which ideally includes your thoughts on the problem and any attempts you have made to solve it. This information helps others identify where you have difficulties and helps them write answers appropriate to your experience level. Closed 6 years ago . Improve this question What's the probability that a n-sided polygon made from n distinct random points on circle contain the centre?",,"['probability', 'geometry', 'geometric-probability']"
99,"Expected value of $\max\{X_1,\ldots,X_n\}$ where $X_i$ are iid uniform. [duplicate]",Expected value of  where  are iid uniform. [duplicate],"\max\{X_1,\ldots,X_n\} X_i","This question already has answers here : Unbiased Estimator for a Uniform Variable Support (4 answers) Closed 4 years ago . Let $X_i\sim\mathrm{Uniform}(0,\theta)$ be iid, what is $E[\max\{X_1,\ldots,X_n\}]$? Apparently the answer is $$\frac{n}{n+1}\theta,$$ but I do not see why? It seems intuitive in that you would ""expect"" them to be spaced out evenly, hence the maximum would be $\frac{n}{n+1}$-of-the-way along the interval, but how can we prove this mathematically? I feel like it should be simple, but evidently $$E[\max\{X_1,\ldots,X_n\}]\neq\max\{E[X_1],\ldots,E[X_n]\}.$$ Thanks.","This question already has answers here : Unbiased Estimator for a Uniform Variable Support (4 answers) Closed 4 years ago . Let $X_i\sim\mathrm{Uniform}(0,\theta)$ be iid, what is $E[\max\{X_1,\ldots,X_n\}]$? Apparently the answer is $$\frac{n}{n+1}\theta,$$ but I do not see why? It seems intuitive in that you would ""expect"" them to be spaced out evenly, hence the maximum would be $\frac{n}{n+1}$-of-the-way along the interval, but how can we prove this mathematically? I feel like it should be simple, but evidently $$E[\max\{X_1,\ldots,X_n\}]\neq\max\{E[X_1],\ldots,E[X_n]\}.$$ Thanks.",,"['probability', 'expectation']"
