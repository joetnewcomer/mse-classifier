,title_raw,title_text,title_latex,body_raw,body_text,body_latex,tags
0,non-homogenous differential equation eigenvalues?,non-homogenous differential equation eigenvalues?,,"I am confused by the fact that the following system is not homogenous: How can we speak of the eigenvectors of a system like this? without the $2$'s, we simply have the equation $Av=\lambda v\to (A-\lambda I)v=0$. So we know that the determinant of $A-\lambda I$ is zero. But I don't know what I can conclude from the equation $(A-\lambda I)v=\overrightarrow 2$ So how do I answer this question?","I am confused by the fact that the following system is not homogenous: How can we speak of the eigenvectors of a system like this? without the $2$'s, we simply have the equation $Av=\lambda v\to (A-\lambda I)v=0$. So we know that the determinant of $A-\lambda I$ is zero. But I don't know what I can conclude from the equation $(A-\lambda I)v=\overrightarrow 2$ So how do I answer this question?",,['ordinary-differential-equations']
1,Showing the existence of an unbounded solution to a periodic system of ODE,Showing the existence of an unbounded solution to a periodic system of ODE,,"We consider the system $$ \begin{pmatrix} x \\ y\\ z\end{pmatrix}' = \begin{pmatrix} \cos^4(t) && 0 && -\sin(2t) \\ \sin(4t) && \sin(t) && -4 \\ -\sin(5t) && 0 && -\cos(t) \end{pmatrix} \begin{pmatrix} x \\ y\\ z\end{pmatrix}. $$ I'd like to show it has at least one unbounded solution. Since it's $2\pi$-periodic, I can use Floquet theory. I know what I have to show is that it has a characteristic exponent $\lambda$ with positive real part, since then $e^{\lambda t}p(t)$ will be a solution to the ODE where $p(t)$ is a non-vanishing $2\pi$-periodic function. This solution will go to infinity as $t\to \infty$, hence will be unbounded. The main issue is that I'm not sure how to compute the monodromy matrix. The matrix in the ODE doesn't look too nice, so I don't think I could compute a fundamental matrix directly. Any suggestions? Also, are there any standard tricks on how to find a monodromy matrix if a fundamental matrix is too hard to compute?","We consider the system $$ \begin{pmatrix} x \\ y\\ z\end{pmatrix}' = \begin{pmatrix} \cos^4(t) && 0 && -\sin(2t) \\ \sin(4t) && \sin(t) && -4 \\ -\sin(5t) && 0 && -\cos(t) \end{pmatrix} \begin{pmatrix} x \\ y\\ z\end{pmatrix}. $$ I'd like to show it has at least one unbounded solution. Since it's $2\pi$-periodic, I can use Floquet theory. I know what I have to show is that it has a characteristic exponent $\lambda$ with positive real part, since then $e^{\lambda t}p(t)$ will be a solution to the ODE where $p(t)$ is a non-vanishing $2\pi$-periodic function. This solution will go to infinity as $t\to \infty$, hence will be unbounded. The main issue is that I'm not sure how to compute the monodromy matrix. The matrix in the ODE doesn't look too nice, so I don't think I could compute a fundamental matrix directly. Any suggestions? Also, are there any standard tricks on how to find a monodromy matrix if a fundamental matrix is too hard to compute?",,['ordinary-differential-equations']
2,Inverse Laplace Transform Formula,Inverse Laplace Transform Formula,,"I'm trying to learn how to evaluate inverse Laplace transforms without the aid of a table of transforms, and I've found the inversion formula: $$\mathcal{L}^{-1}\{F\}(t)=\frac{1}{2\pi i}\int_{\gamma-i\infty}^{\gamma+i\infty}F(s)e^{st}ds$$ I'm currently in high school, and I don't a lot of knowledge in terms of complex analysis, but I do have a pretty good base in calculus, so if anyone could help me out, it'd be great if you could also explain some of the complex analysis methods that may be involved in evaluating this integral. A step by step solution for the ILT of $F(s)=\frac{1}{s}$ would be awesome. Thanks in advance.","I'm trying to learn how to evaluate inverse Laplace transforms without the aid of a table of transforms, and I've found the inversion formula: $$\mathcal{L}^{-1}\{F\}(t)=\frac{1}{2\pi i}\int_{\gamma-i\infty}^{\gamma+i\infty}F(s)e^{st}ds$$ I'm currently in high school, and I don't a lot of knowledge in terms of complex analysis, but I do have a pretty good base in calculus, so if anyone could help me out, it'd be great if you could also explain some of the complex analysis methods that may be involved in evaluating this integral. A step by step solution for the ILT of $F(s)=\frac{1}{s}$ would be awesome. Thanks in advance.",,"['ordinary-differential-equations', 'multivariable-calculus']"
3,"Don't know how to solve systems, any help or resources would be appreciated","Don't know how to solve systems, any help or resources would be appreciated",,"I missed a week of class because I was sick and I'm trying to do my homework but I simply don't know how. I tried looking for videos on youtube but couldn't find anything similar (not sure what these are called? Differential system of equations maybe?). Any explanation on the steps needed to solve these problems (or the one below) would be greatly appreciated. Please note I'm looking for how to solve them, not the answer (answers are already in the back of the book). Thanks! Solve the systems   \begin{align*} \frac{\mathrm{d}x}{\mathrm{d}t} &= 2x -y\\ \frac{\mathrm{d}y}{\mathrm{d}t} &= x \end{align*}","I missed a week of class because I was sick and I'm trying to do my homework but I simply don't know how. I tried looking for videos on youtube but couldn't find anything similar (not sure what these are called? Differential system of equations maybe?). Any explanation on the steps needed to solve these problems (or the one below) would be greatly appreciated. Please note I'm looking for how to solve them, not the answer (answers are already in the back of the book). Thanks! Solve the systems   \begin{align*} \frac{\mathrm{d}x}{\mathrm{d}t} &= 2x -y\\ \frac{\mathrm{d}y}{\mathrm{d}t} &= x \end{align*}",,"['ordinary-differential-equations', 'systems-of-equations']"
4,derivative $\frac{\ln{x}}{e^x}$,derivative,\frac{\ln{x}}{e^x},Im asked to solve find the derivative of: $$ \frac{\ln x}{e^x}$$ my attempt $$D\frac{\ln x}{e^x} = \frac{\frac{1}{x}e^x + \ln (x) e^x}{e^x} = e^x \frac{\frac{1}{x}+\ln x}{e^{2x}} = \frac{\frac{1}{x}+\ln x}{e^x}$$ But this is apparently wrong and the correct answer is:  $$\frac{\frac{1}{x} - \ln x}{e^x}$$ Where do I go wrong?,Im asked to solve find the derivative of: $$ \frac{\ln x}{e^x}$$ my attempt $$D\frac{\ln x}{e^x} = \frac{\frac{1}{x}e^x + \ln (x) e^x}{e^x} = e^x \frac{\frac{1}{x}+\ln x}{e^{2x}} = \frac{\frac{1}{x}+\ln x}{e^x}$$ But this is apparently wrong and the correct answer is:  $$\frac{\frac{1}{x} - \ln x}{e^x}$$ Where do I go wrong?,,['ordinary-differential-equations']
5,Show $\nabla^2g=-f$,Show,\nabla^2g=-f,"Let a continuous function $f(x,y,z)$ have first order partial derivatives $f_x,f_y,f_z$ exist at every point and the function and all its first order partial derivatives are absolutely integrable. If for $x',y',z' \in R$ there is a non negative integrable function $h(x',y',z')$ such that for some measurable set $A \subset R^3$ with finite Lebesegue outer measure: $|f_x(x+x',y',z')|< h(x',y',z')$ almost everywhere in $A$ for some $m-\epsilon<x<m+\epsilon$ $|f_y(x',y+y',z')|< h(x',y',z')$ almost everywhere in $A$ for some $m-\epsilon<y<m+\epsilon$ $|f_z(x',y',z+z') |< h(x',y',z')$ almost everywhere in $A$ for some $m-\epsilon<z<m+\epsilon$ $ r'=\sqrt{|x-x'|^2+|y-y'|^2+|z-z'|^2}$ let: $$g(x,y,z)=\int_{-\infty}^{\infty} \int_{-\infty}^{\infty} \int_{-\infty}^{\infty} \frac {f(x',y',z')}{4\pi r'}dx'dy'dz',\ \ \text{s.t.}\ \ \nabla^2g= \frac {\partial^2g}{\partial x^2}+\frac {\partial^2g}{\partial y^2}+\frac {\partial^2g}{\partial z^2}$$ How to show that $\nabla^2g=-f$ ?",Let a continuous function have first order partial derivatives exist at every point and the function and all its first order partial derivatives are absolutely integrable. If for there is a non negative integrable function such that for some measurable set with finite Lebesegue outer measure: almost everywhere in for some almost everywhere in for some almost everywhere in for some let: How to show that ?,"f(x,y,z) f_x,f_y,f_z x',y',z' \in R h(x',y',z') A \subset R^3 |f_x(x+x',y',z')|< h(x',y',z') A m-\epsilon<x<m+\epsilon |f_y(x',y+y',z')|< h(x',y',z') A m-\epsilon<y<m+\epsilon |f_z(x',y',z+z') |< h(x',y',z') A m-\epsilon<z<m+\epsilon  r'=\sqrt{|x-x'|^2+|y-y'|^2+|z-z'|^2} g(x,y,z)=\int_{-\infty}^{\infty} \int_{-\infty}^{\infty} \int_{-\infty}^{\infty} \frac {f(x',y',z')}{4\pi r'}dx'dy'dz',\ \ \text{s.t.}\ \ \nabla^2g= \frac {\partial^2g}{\partial x^2}+\frac {\partial^2g}{\partial y^2}+\frac {\partial^2g}{\partial z^2} \nabla^2g=-f","['real-analysis', 'ordinary-differential-equations', 'partial-differential-equations', 'proof-writing', 'poissons-equation']"
6,Can every separable differential equation be rewritten to potentially be exact (or NOT exact)?,Can every separable differential equation be rewritten to potentially be exact (or NOT exact)?,,"Let's say an ordinary linear DE is separable. Then $$\frac{dy}{dx} = P(y)Q(x) \Leftrightarrow \frac{1}{P(y)}dy = Q(x)dx \Leftrightarrow Q(x)dx + R(y)dy = 0$$ is in exact form where $R(y)=-\frac{1}{P(y)}.$ This means that $M(x,y) = Q(x)$ and $N(x,y) = R(y)$ if we're using standard text book notation. To test for exactness, note that $$\frac{\partial M}{\partial y} = 0 = \frac{\partial N}{\partial x},$$ meaning that any separable equation could be made to be exact. As a concrete example, consider $$\frac{dy}{dx} = x\ln (y) + x$$ which is separable as $$\frac{1}{\ln(y)+1}dy = xdx$$ and everything I wrote above follows (demonstrating that our equation is indeed exact). HOWEVER, if we instead write it as $$dy= (x\ln (y) + x)dx \Leftrightarrow -(x \ln (y) + x)dx + dy = 0$$ then we have that $M(x,y) = -(x\ln(y) + x)$ and $N(x,y) = 1$ and our test for exactness yields $$\frac{\partial M}{\partial y} = -\frac{x}{y} \neq 0 = \frac{\partial N}{\partial x}$$ showing that the SAME differential equation is NOT exact! What am I missing here?","Let's say an ordinary linear DE is separable. Then $$\frac{dy}{dx} = P(y)Q(x) \Leftrightarrow \frac{1}{P(y)}dy = Q(x)dx \Leftrightarrow Q(x)dx + R(y)dy = 0$$ is in exact form where $R(y)=-\frac{1}{P(y)}.$ This means that $M(x,y) = Q(x)$ and $N(x,y) = R(y)$ if we're using standard text book notation. To test for exactness, note that $$\frac{\partial M}{\partial y} = 0 = \frac{\partial N}{\partial x},$$ meaning that any separable equation could be made to be exact. As a concrete example, consider $$\frac{dy}{dx} = x\ln (y) + x$$ which is separable as $$\frac{1}{\ln(y)+1}dy = xdx$$ and everything I wrote above follows (demonstrating that our equation is indeed exact). HOWEVER, if we instead write it as $$dy= (x\ln (y) + x)dx \Leftrightarrow -(x \ln (y) + x)dx + dy = 0$$ then we have that $M(x,y) = -(x\ln(y) + x)$ and $N(x,y) = 1$ and our test for exactness yields $$\frac{\partial M}{\partial y} = -\frac{x}{y} \neq 0 = \frac{\partial N}{\partial x}$$ showing that the SAME differential equation is NOT exact! What am I missing here?",,"['calculus', 'ordinary-differential-equations']"
7,Raising e to the power of both sides of an equation,Raising e to the power of both sides of an equation,,"I have a simple question: in differential equations, it has been common in several of my homework problems to raise a base $e$ to the power of both sides of an equation to get variables out of natural log functions. My question is what happens if you have a zero term? For example: $$ ln(x) + ln(y) = z $$ is equivalent to $$ e^{ln(x)} + e^{ln(y)} = e^z $$ which simplifies to $$ x + y = e^z $$ But what happens if $y=1$? $$ ln(x) + ln(1) = z $$ $$ ln(x) + 0 = z $$ At this point, if you removed zero from the equation and then raised both sides by $e$, you would get $x = e^z$ when it should be $x + 1 = e^z$. Thinking about it in this manner, one could claim the following: $$ 1 + 0 = 1 $$ $$ e^1 + e^0 = e^1 $$ $$ e + 1 = e $$ $$ 1 = 0 $$ Is there some rule that explains why this algebra breaks down, and when it is wrong to raise both sides of an equation by $e$? This thought experiment is obviously incorrect, but in my first example, it is incorrect if you don't include zero in your raising-by-e operation.","I have a simple question: in differential equations, it has been common in several of my homework problems to raise a base $e$ to the power of both sides of an equation to get variables out of natural log functions. My question is what happens if you have a zero term? For example: $$ ln(x) + ln(y) = z $$ is equivalent to $$ e^{ln(x)} + e^{ln(y)} = e^z $$ which simplifies to $$ x + y = e^z $$ But what happens if $y=1$? $$ ln(x) + ln(1) = z $$ $$ ln(x) + 0 = z $$ At this point, if you removed zero from the equation and then raised both sides by $e$, you would get $x = e^z$ when it should be $x + 1 = e^z$. Thinking about it in this manner, one could claim the following: $$ 1 + 0 = 1 $$ $$ e^1 + e^0 = e^1 $$ $$ e + 1 = e $$ $$ 1 = 0 $$ Is there some rule that explains why this algebra breaks down, and when it is wrong to raise both sides of an equation by $e$? This thought experiment is obviously incorrect, but in my first example, it is incorrect if you don't include zero in your raising-by-e operation.",,['ordinary-differential-equations']
8,A good pictorial explanation of separation of variables?,A good pictorial explanation of separation of variables?,,"I'm teaching ordinary differential equations for the first time, and I would like to give a compelling visual explanation of why it makes sense to ""multiply by $dx$"" and integrate when you want to solve a separable equation like $\frac{dy}{dx} g(y) = f(x)$. Roughly what came to mind: one can use the tangent slope of a solution to the equation to give two congruent right triangles with ""adjacent"" and ""opposite"" side lengths $(\Delta x, \Delta y)$ (approximately) and $(g(y), f(x))$.  Using the fact that these are similar triangles, we see that cross multiplying yields $g(y)\,\Delta y = f(x) \,\Delta x$ (really approximately), and these are approximations of the areas represented by $\int g(y) \,dy$ and $\int g(x) \,dx$. Question: Are there other visual explanations for the method of separation of variables?  Perhaps one that can make a more direct connection with areas and/or are more precise?","I'm teaching ordinary differential equations for the first time, and I would like to give a compelling visual explanation of why it makes sense to ""multiply by $dx$"" and integrate when you want to solve a separable equation like $\frac{dy}{dx} g(y) = f(x)$. Roughly what came to mind: one can use the tangent slope of a solution to the equation to give two congruent right triangles with ""adjacent"" and ""opposite"" side lengths $(\Delta x, \Delta y)$ (approximately) and $(g(y), f(x))$.  Using the fact that these are similar triangles, we see that cross multiplying yields $g(y)\,\Delta y = f(x) \,\Delta x$ (really approximately), and these are approximations of the areas represented by $\int g(y) \,dy$ and $\int g(x) \,dx$. Question: Are there other visual explanations for the method of separation of variables?  Perhaps one that can make a more direct connection with areas and/or are more precise?",,"['ordinary-differential-equations', 'visualization']"
9,differential equation $f(x)= x^3 +x^2 \cdot f (1) + x \cdot f'' (2)+f''' (3)$,differential equation,f(x)= x^3 +x^2 \cdot f (1) + x \cdot f'' (2)+f''' (3),"If $f:\mathbb R\to \mathbb R$ such that $f(x)=x^3+x^2f'(1)+xf''(2)+f'''(3)$  ∀$x∈\mathbb R$ then find the value of $f(1)-f(0)$. I am solving in this manner and I was stuck in $4$th  step. Please help me to continue to solve it. $f(x)=x^3+x^2f'(1)+xf''(2)+f'''(3)$ Let ,  $f'(1)=\rho$ , $f''(2)=\theta$ and $f'''(3)=μ$ then $$\begin{align} f(x)&=x^3+\rho x^2+ \theta x+ \mu \\  f'(x)&=3x^2+2\rho x+\theta \end{align}$$","If $f:\mathbb R\to \mathbb R$ such that $f(x)=x^3+x^2f'(1)+xf''(2)+f'''(3)$  ∀$x∈\mathbb R$ then find the value of $f(1)-f(0)$. I am solving in this manner and I was stuck in $4$th  step. Please help me to continue to solve it. $f(x)=x^3+x^2f'(1)+xf''(2)+f'''(3)$ Let ,  $f'(1)=\rho$ , $f''(2)=\theta$ and $f'''(3)=μ$ then $$\begin{align} f(x)&=x^3+\rho x^2+ \theta x+ \mu \\  f'(x)&=3x^2+2\rho x+\theta \end{align}$$",,['ordinary-differential-equations']
10,How do you solve a $n$-th order system of differential equations in the form of $\mathbf x^{(n)}=A\mathbf x$?,How do you solve a -th order system of differential equations in the form of ?,n \mathbf x^{(n)}=A\mathbf x,"How do you solve a $n$-th order system of differential equations. I know how to solve first order equations like $\mathbf{x}'=A\mathbf{x}$ but how would I solve $$\mathbf x^{(n)}=A\mathbf x$$ Could you maybe show me how it is done or link me some sources where I can look this up. I have looked at Paul's notes , Mathematical Methods for Physicsists and Engineers and Wikipedia but they seem to all focus on first order equations. Edit: Winther demonstrated a standard procedure of reducing nth order systems to first order systems in the comments. I am looking for sources that explain this in detail.","How do you solve a $n$-th order system of differential equations. I know how to solve first order equations like $\mathbf{x}'=A\mathbf{x}$ but how would I solve $$\mathbf x^{(n)}=A\mathbf x$$ Could you maybe show me how it is done or link me some sources where I can look this up. I have looked at Paul's notes , Mathematical Methods for Physicsists and Engineers and Wikipedia but they seem to all focus on first order equations. Edit: Winther demonstrated a standard procedure of reducing nth order systems to first order systems in the comments. I am looking for sources that explain this in detail.",,['ordinary-differential-equations']
11,"Positively invariant $(S,I)$-triangle for SIS dynamical system",Positively invariant -triangle for SIS dynamical system,"(S,I)","Consider the following differential equations $${dS \over dt} = \lambda-\beta SI-\mu S+\theta I$$ $${dI \over dt} =\beta SI-(\mu +d)I-\theta I$$ In all papers that I have read it is only mentioned that $$\Omega = \left\{ (S,I) : I\geq 0, S \geq 0, S+I \leq {\lambda \over \mu} \right\}$$ is positively invariant. How can I explicitly show that the set $\Omega $ is positively invariant?","Consider the following differential equations $${dS \over dt} = \lambda-\beta SI-\mu S+\theta I$$ $${dI \over dt} =\beta SI-(\mu +d)I-\theta I$$ In all papers that I have read it is only mentioned that $$\Omega = \left\{ (S,I) : I\geq 0, S \geq 0, S+I \leq {\lambda \over \mu} \right\}$$ is positively invariant. How can I explicitly show that the set $\Omega $ is positively invariant?",,"['ordinary-differential-equations', 'dynamical-systems', 'set-invariance']"
12,Prove that $y_1(x)=\sin(x^2)$ can't be a solution for a linear homogeneous second order differential equation.,Prove that  can't be a solution for a linear homogeneous second order differential equation.,y_1(x)=\sin(x^2),"let $y_1(x)=\sin(x^2)$, $y_2(x)=\cos(x^2)$. Prove that the function $y_2$ can't be a solution for a linear homogeneous second order equation which fulfills the conditions of Existence and Uniqueness Theorem. Prove that the function $y_1$ can't be a solution for a linear homogeneous second order equation which fulfills the conditions of Existence and Uniqueness Theorem. My solution attempt: I thought about using the Wronskian property for the proof. but I didn't have and direction for how to do it. any kind of help would be appreciated.","let $y_1(x)=\sin(x^2)$, $y_2(x)=\cos(x^2)$. Prove that the function $y_2$ can't be a solution for a linear homogeneous second order equation which fulfills the conditions of Existence and Uniqueness Theorem. Prove that the function $y_1$ can't be a solution for a linear homogeneous second order equation which fulfills the conditions of Existence and Uniqueness Theorem. My solution attempt: I thought about using the Wronskian property for the proof. but I didn't have and direction for how to do it. any kind of help would be appreciated.",,['ordinary-differential-equations']
13,How to solve a functional differential equation?,How to solve a functional differential equation?,,"$$(1) \quad \cfrac{d}{dx} (f(x^n))=\cfrac{-f(x^n)^2}{f(n \cdot x^{n-1})}$$ How do I solve this functional differential equation? I need a closed form solution, so approximations won't cut it, I'll need the whole explicit solution. I don't have any work, there isn't really an obvious method that pops out at me. However, I do have some context posted here . That's why I need a whole solution. In general, is there a method to solve certain functional differential equations like $(1)$? I'm interested in finding the function $f$, so setting $n$ to one or something like that probably wouldn't help. Also I'm particularly interested in the initial condition where $f(0)$ is infinite.","$$(1) \quad \cfrac{d}{dx} (f(x^n))=\cfrac{-f(x^n)^2}{f(n \cdot x^{n-1})}$$ How do I solve this functional differential equation? I need a closed form solution, so approximations won't cut it, I'll need the whole explicit solution. I don't have any work, there isn't really an obvious method that pops out at me. However, I do have some context posted here . That's why I need a whole solution. In general, is there a method to solve certain functional differential equations like $(1)$? I'm interested in finding the function $f$, so setting $n$ to one or something like that probably wouldn't help. Also I'm particularly interested in the initial condition where $f(0)$ is infinite.",,"['ordinary-differential-equations', 'functional-equations', 'closed-form']"
14,How to find the degree of following differential equation? [closed],How to find the degree of following differential equation? [closed],,Closed . This question needs details or clarity . It is not currently accepting answers. Want to improve this question? Add details and clarify the problem by editing this post . Closed 8 years ago . Improve this question $$\frac{d^2y}{dx^2}=\sin\left(\frac{dy}{dx}\right) $$ As differential coefficients are not in polynomial function the degree is not defined. But we can write the this by expanding sine function by Maclaurin series. $$\frac{d^2y}{dx^2}=\left(\frac{dy}{dx}\right)-\frac{\left(\frac{dy}{dx}\right)^3}{3!} \cdots $$ Now its a  polynomial. Now the degree must be $1$. But the degree is not defined.,Closed . This question needs details or clarity . It is not currently accepting answers. Want to improve this question? Add details and clarify the problem by editing this post . Closed 8 years ago . Improve this question $$\frac{d^2y}{dx^2}=\sin\left(\frac{dy}{dx}\right) $$ As differential coefficients are not in polynomial function the degree is not defined. But we can write the this by expanding sine function by Maclaurin series. $$\frac{d^2y}{dx^2}=\left(\frac{dy}{dx}\right)-\frac{\left(\frac{dy}{dx}\right)^3}{3!} \cdots $$ Now its a  polynomial. Now the degree must be $1$. But the degree is not defined.,,"['calculus', 'ordinary-differential-equations']"
15,Why does Abel's identity imply either $W = 0$ or $W \neq 0$ everywhere?,Why does Abel's identity imply either  or  everywhere?,W = 0 W \neq 0,"Let $y_1$ and $y_2$ be solutions to the linear differential equation $A(x)y'' + B(x)y' + C(x)y = 0 $ and let $W = W(y_1, y_2)$ be the Wronskian of the solutions. Why does Abel's identity $\displaystyle W(y_1, y_2)(x) = W(y_1, y_2)(x_0)\cdot exp\left(-\int{\frac{B(x)}{A(x)}dx}\right) $ imply the Wronskian is either zero everywhere or nonzero everywhere? I know that $W \neq 0$ if the solutions are linearly independent and $W=0$ if the solutions are linearly dependent. But how does Abel's identity show this for all $x$ in the domain? Thanks for the help.","Let $y_1$ and $y_2$ be solutions to the linear differential equation $A(x)y'' + B(x)y' + C(x)y = 0 $ and let $W = W(y_1, y_2)$ be the Wronskian of the solutions. Why does Abel's identity $\displaystyle W(y_1, y_2)(x) = W(y_1, y_2)(x_0)\cdot exp\left(-\int{\frac{B(x)}{A(x)}dx}\right) $ imply the Wronskian is either zero everywhere or nonzero everywhere? I know that $W \neq 0$ if the solutions are linearly independent and $W=0$ if the solutions are linearly dependent. But how does Abel's identity show this for all $x$ in the domain? Thanks for the help.",,"['calculus', 'ordinary-differential-equations']"
16,Problem with Justifying the Formula for First Order Seperable Differential Equations,Problem with Justifying the Formula for First Order Seperable Differential Equations,,"I am reading this text http://www.math-cs.gordon.edu/courses/ma225/handouts/sepvar.pdf to justify the method to solve first order seperable differentiable equations, where we are told first told that: and then: Now, while I can understand 1), I am struggling to understand how exactly the integral on the left hand side of 3) surmounts to $$\int n(y) dy$$ since $\frac{dy}{dx}$ can't be treated as a fraction.","I am reading this text http://www.math-cs.gordon.edu/courses/ma225/handouts/sepvar.pdf to justify the method to solve first order seperable differentiable equations, where we are told first told that: and then: Now, while I can understand 1), I am struggling to understand how exactly the integral on the left hand side of 3) surmounts to $$\int n(y) dy$$ since $\frac{dy}{dx}$ can't be treated as a fraction.",,"['calculus', 'ordinary-differential-equations', 'intuition']"
17,Steady states of a system,Steady states of a system,,How can I find the steady states? I am aware that the condition is to equal 0 but I am not able to say how many steady states there are... $$\begin{cases} \dot x=x-y^2 \\ \dot y= -x+2y-z^2 \\ \dot z= -y+z^2 \end{cases} $$,How can I find the steady states? I am aware that the condition is to equal 0 but I am not able to say how many steady states there are... $$\begin{cases} \dot x=x-y^2 \\ \dot y= -x+2y-z^2 \\ \dot z= -y+z^2 \end{cases} $$,,"['ordinary-differential-equations', 'systems-of-equations']"
18,Laplace equation in a circle - where is my mistake,Laplace equation in a circle - where is my mistake,,"We want to solve $r^2u''_{rr}+ru'_r+u''_{\theta \theta}=0$ where $0\leq \theta <2\pi$ and $0\leq r \leq 2$, given that $u(2,\theta)=\cos(2\theta)$. I managed to work out a simple solution, but when I checked it, I saw that it does not agree with $r^2u''_{rr}+ru'_r+u''_{\theta \theta}=0$ and I'd like to know where is the mistake. My solution Let $u(r,\theta)=R(r)T(\theta)$. Then from $r^2u''_{rr}+ru'_r+u''_{\theta \theta}=0$ we have $\frac{r^2R''(r)+rR'(r)}{R(r)}=-\frac{T''(\theta)}{T(\theta)}$ On the left hand side we have a function of just $r$, on the right hand just $\theta$, so they must both be scalar $\frac{r^2R''(r)+rR'(r)}{R(r)}=-\frac{T''(\theta)}{T(\theta)}=\lambda$ From this we have $T''(\theta)+\lambda T(\theta)=0$, but since $u$ is periodic with respect to $\theta$ with period of $2\pi$, we must have $T(0)=T(2\pi)$ and $T'(0)=T'(2\pi)$. This is a sturm-liouville problem whos solution is given by: $\lambda_k=k^2$ and $T_k(\theta)=A_k\cos(k\theta)+B_k\sin(k\theta)$. where $k=1,2,...$ and $T_0=1$. But we also had $\frac{r^2R''(r)+rR'(r)}{R(r)}=\lambda=k^2$, so $r^2R''_k(r)+rR'_k(r)-k^2R_k(r)=0$ If we guess a solution $R(r)=r^\alpha$ then we have $\alpha(\alpha-1)\alpha-k^2=\alpha^2-k^2=0$ so $\alpha=\pm k$ and so $R_k(r)=C_kr^k+D_kr^{-k}$ when $k\neq 0$, and $R_0=C_0+D_0\ln(r)$. So overall our solution will be of the form $u(r,\theta)=T_0(\theta)R_0(r)+\sum_{k=1}^{\infty}T_k(\theta)R_k(r)$ which equals to $u(r,\theta)=C_0+D_0\ln(r)+\sum_{k=1}^{\infty}[A_k\cos(k\theta)+B_k\sin(k\theta)](C_kr^k+D_kr^{-k})$ We impose another restriction that $\lim_{r\to 0}u(r,\theta)$ will be bounded. it will diverge to infinity or negative infinity. This can only happen when $D_0=0$ and $D_k=0$, so we have: $u(r,\theta)=C_0+\sum_{k=1}^{\infty}[A_k\cos(k\theta)+B_k\sin(k\theta)]C_kr^k$ To make things simple, since we don't know either $A_k,B_k,C_k$ let's just say $C_k=1$ and find $A_k,B_k$ that all the conditions are met. So overall: $u(r,\theta)=C_0+\sum_{k=1}^{\infty}[A_k\cos(k\theta)+B_k\sin(k\theta)]r^k$. but we also had a condition that $u(2,\theta)=\cos(2\theta)$. so $u(2,\theta)=C_0+\sum_{k=1}^{\infty}[A_k\cos(k\theta)+B_k\sin(k\theta)]2^k=\cos(2\theta)$. This can only hold if $C_0=0$, $B_k=0$ for all $k$. In addition we must have $A_k=0$ when $k\neq 2$, and we must also have $2^2A_2=1$, so $A_2=\frac{1}{4}$ so my answer is: $u(r,\theta)=\frac{1}{4}r^2\cos(2\theta)$. That's the solution I got. Indeed it is true that $\lim_{r \to 0}u(r,\theta)=0$ is bounded. And it is also true that $u(2,\theta)=\frac{1}{4}4\cos(2\theta)=\cos(2\theta)$. So these conditions are met. However, we have $u'_r=\frac{1}{2}r\cos(2\theta)$, $u''_{rr}=\cos(2\theta)$ and $u''_{\theta \theta}=-r^2\cos(2\theta)$. so: $r^2u''_{rr}+ru'_{r}+u''_{\theta \theta}=\frac{1}{2}r^2\cos(2\theta)$. And it should be zero. So I'm really not sure what's going on.","We want to solve $r^2u''_{rr}+ru'_r+u''_{\theta \theta}=0$ where $0\leq \theta <2\pi$ and $0\leq r \leq 2$, given that $u(2,\theta)=\cos(2\theta)$. I managed to work out a simple solution, but when I checked it, I saw that it does not agree with $r^2u''_{rr}+ru'_r+u''_{\theta \theta}=0$ and I'd like to know where is the mistake. My solution Let $u(r,\theta)=R(r)T(\theta)$. Then from $r^2u''_{rr}+ru'_r+u''_{\theta \theta}=0$ we have $\frac{r^2R''(r)+rR'(r)}{R(r)}=-\frac{T''(\theta)}{T(\theta)}$ On the left hand side we have a function of just $r$, on the right hand just $\theta$, so they must both be scalar $\frac{r^2R''(r)+rR'(r)}{R(r)}=-\frac{T''(\theta)}{T(\theta)}=\lambda$ From this we have $T''(\theta)+\lambda T(\theta)=0$, but since $u$ is periodic with respect to $\theta$ with period of $2\pi$, we must have $T(0)=T(2\pi)$ and $T'(0)=T'(2\pi)$. This is a sturm-liouville problem whos solution is given by: $\lambda_k=k^2$ and $T_k(\theta)=A_k\cos(k\theta)+B_k\sin(k\theta)$. where $k=1,2,...$ and $T_0=1$. But we also had $\frac{r^2R''(r)+rR'(r)}{R(r)}=\lambda=k^2$, so $r^2R''_k(r)+rR'_k(r)-k^2R_k(r)=0$ If we guess a solution $R(r)=r^\alpha$ then we have $\alpha(\alpha-1)\alpha-k^2=\alpha^2-k^2=0$ so $\alpha=\pm k$ and so $R_k(r)=C_kr^k+D_kr^{-k}$ when $k\neq 0$, and $R_0=C_0+D_0\ln(r)$. So overall our solution will be of the form $u(r,\theta)=T_0(\theta)R_0(r)+\sum_{k=1}^{\infty}T_k(\theta)R_k(r)$ which equals to $u(r,\theta)=C_0+D_0\ln(r)+\sum_{k=1}^{\infty}[A_k\cos(k\theta)+B_k\sin(k\theta)](C_kr^k+D_kr^{-k})$ We impose another restriction that $\lim_{r\to 0}u(r,\theta)$ will be bounded. it will diverge to infinity or negative infinity. This can only happen when $D_0=0$ and $D_k=0$, so we have: $u(r,\theta)=C_0+\sum_{k=1}^{\infty}[A_k\cos(k\theta)+B_k\sin(k\theta)]C_kr^k$ To make things simple, since we don't know either $A_k,B_k,C_k$ let's just say $C_k=1$ and find $A_k,B_k$ that all the conditions are met. So overall: $u(r,\theta)=C_0+\sum_{k=1}^{\infty}[A_k\cos(k\theta)+B_k\sin(k\theta)]r^k$. but we also had a condition that $u(2,\theta)=\cos(2\theta)$. so $u(2,\theta)=C_0+\sum_{k=1}^{\infty}[A_k\cos(k\theta)+B_k\sin(k\theta)]2^k=\cos(2\theta)$. This can only hold if $C_0=0$, $B_k=0$ for all $k$. In addition we must have $A_k=0$ when $k\neq 2$, and we must also have $2^2A_2=1$, so $A_2=\frac{1}{4}$ so my answer is: $u(r,\theta)=\frac{1}{4}r^2\cos(2\theta)$. That's the solution I got. Indeed it is true that $\lim_{r \to 0}u(r,\theta)=0$ is bounded. And it is also true that $u(2,\theta)=\frac{1}{4}4\cos(2\theta)=\cos(2\theta)$. So these conditions are met. However, we have $u'_r=\frac{1}{2}r\cos(2\theta)$, $u''_{rr}=\cos(2\theta)$ and $u''_{\theta \theta}=-r^2\cos(2\theta)$. so: $r^2u''_{rr}+ru'_{r}+u''_{\theta \theta}=\frac{1}{2}r^2\cos(2\theta)$. And it should be zero. So I'm really not sure what's going on.",,"['calculus', 'ordinary-differential-equations', 'partial-differential-equations', 'partial-derivative']"
19,What does a 3D periodic solution of a differential equation look like?,What does a 3D periodic solution of a differential equation look like?,,"The Pointcare-Bendixson Theorem implies that if a solution stays in a bounded region with no equilibrium points then it is either a periodic solution or it approaches a periodic orbit as t goes to infinity. But this is only in 2D. In higher order dimensions, are we able to find that solutions are periodic solution or approaching a periodic solution in that higher dimension space?","The Pointcare-Bendixson Theorem implies that if a solution stays in a bounded region with no equilibrium points then it is either a periodic solution or it approaches a periodic orbit as t goes to infinity. But this is only in 2D. In higher order dimensions, are we able to find that solutions are periodic solution or approaching a periodic solution in that higher dimension space?",,"['ordinary-differential-equations', 'dynamical-systems', 'stability-in-odes']"
20,Differentiate $f(x) = xe^x$ from first principles,Differentiate  from first principles,f(x) = xe^x,I am trying to prove the derivative of $f(x) = xe^x$ is $f'(x) = e^x + xe^x$ from first principles. The derivative is $ \frac{f(x+h) - f(x)}{h} $ as h tends to zero So: $ \frac{f(x+h) - f(x)}{h}  =  \frac{(x+h)e^{x+h} - xe^x}{h} $ $ = \frac{x(e^{x+h}-e^x)+he^{x+h}}{h} $ $ = \frac{x(e^{x+h}-e^x)}{h} + e^{x+h} $ This tends to infinity as h tends to zero. Where have I gone wrong?,I am trying to prove the derivative of $f(x) = xe^x$ is $f'(x) = e^x + xe^x$ from first principles. The derivative is $ \frac{f(x+h) - f(x)}{h} $ as h tends to zero So: $ \frac{f(x+h) - f(x)}{h}  =  \frac{(x+h)e^{x+h} - xe^x}{h} $ $ = \frac{x(e^{x+h}-e^x)+he^{x+h}}{h} $ $ = \frac{x(e^{x+h}-e^x)}{h} + e^{x+h} $ This tends to infinity as h tends to zero. Where have I gone wrong?,,['ordinary-differential-equations']
21,Numerical Approximation of Differential Equations with Midpoint Method,Numerical Approximation of Differential Equations with Midpoint Method,,"I want to proof that the local truncation error of the Midpoint Method is $d_{k+1}=O\left(h^{3}\right)$ Approach The local truncation error is defined as: $d_{k+1}=u(t_{k+1})-\underset{u_{k+1}}{\underbrace{\left[u(t_{k})+\triangle t\cdot\phi\right]}}$ $u(t_{k+1})\rightarrow$ exact solution of the differential equation $u'(t)=f(u,t)$, which is being approximated $\Phi\rightarrow f\left(u(t_{k+1})+\frac{1}{2}\left(\triangle t\right)\cdot f\left(u(t_{k}),t_{k}\right),t_{k}+\frac{1}{2}\left(\triangle t\right)\right)$ incremental Function called the Midpoint Method The first step is to write the term $u(t_{k+1})$ as its Taylor expansion: $u(t_{k+1})=u(t_{k})+\frac{1}{1!}\cdot h^{1}\cdot u^{(1)}(t_{k})+\frac{1}{2!}\cdot h^{2}\cdot u^{(2)}(t_{k})+\frac{1}{3!}\cdot h^{3}\cdot u^{(3)}(t_{k})+O\left(h^{4}\right)$ Then the second step is to write the incremental function in a fashion that the elements can cancel out with the previously introduced Taylor expansion of $u(t_{k+1})$. I found a solution to the second step, but i do not understand how it is formed. And i would be happy if someone could explain in detail and write out the rules which  lead to it. The solution goes as follows: $\Phi\rightarrow f\left(u(t_{k+1})+\frac{1}{2}\left(\triangle t\right)\cdot f\left(u(t_{k}),t_{k}\right),t_{k}+\frac{1}{2}\left(\triangle t\right)\right)$ What happened here? $=f\left(u\left(t_{k}\right),t_{k}\right)+\frac{1}{2}\cdot h\cdot f_{t}\left(u\left(t_{k}\right),t_{k}\right)+\frac{1}{2}\cdot h\cdot f\left(u\left(t_{k}\right),t_{k}\right)\cdot f_{y}\left(u\left(t_{k}\right),t_{k}\right)$ $+\frac{1}{2}\cdot\left(\frac{1}{2}\cdot h\right)^{2}\cdot f_{tt}+\left(\frac{1}{2}\cdot h\right)^{2}f\cdot f_{tu}+\frac{1}{2}\left(\frac{1}{2}\cdot h\right)^{2}\cdot f^{2}f_{uu}+O\left(h^{3}\right)$ I understand that there is some kind of differentiating going on, but i cant figure it out. Can someone provide the steps involved? Edits I found out that total derivatives are used somehow, but i do not have written out the exact transformation steps.","I want to proof that the local truncation error of the Midpoint Method is $d_{k+1}=O\left(h^{3}\right)$ Approach The local truncation error is defined as: $d_{k+1}=u(t_{k+1})-\underset{u_{k+1}}{\underbrace{\left[u(t_{k})+\triangle t\cdot\phi\right]}}$ $u(t_{k+1})\rightarrow$ exact solution of the differential equation $u'(t)=f(u,t)$, which is being approximated $\Phi\rightarrow f\left(u(t_{k+1})+\frac{1}{2}\left(\triangle t\right)\cdot f\left(u(t_{k}),t_{k}\right),t_{k}+\frac{1}{2}\left(\triangle t\right)\right)$ incremental Function called the Midpoint Method The first step is to write the term $u(t_{k+1})$ as its Taylor expansion: $u(t_{k+1})=u(t_{k})+\frac{1}{1!}\cdot h^{1}\cdot u^{(1)}(t_{k})+\frac{1}{2!}\cdot h^{2}\cdot u^{(2)}(t_{k})+\frac{1}{3!}\cdot h^{3}\cdot u^{(3)}(t_{k})+O\left(h^{4}\right)$ Then the second step is to write the incremental function in a fashion that the elements can cancel out with the previously introduced Taylor expansion of $u(t_{k+1})$. I found a solution to the second step, but i do not understand how it is formed. And i would be happy if someone could explain in detail and write out the rules which  lead to it. The solution goes as follows: $\Phi\rightarrow f\left(u(t_{k+1})+\frac{1}{2}\left(\triangle t\right)\cdot f\left(u(t_{k}),t_{k}\right),t_{k}+\frac{1}{2}\left(\triangle t\right)\right)$ What happened here? $=f\left(u\left(t_{k}\right),t_{k}\right)+\frac{1}{2}\cdot h\cdot f_{t}\left(u\left(t_{k}\right),t_{k}\right)+\frac{1}{2}\cdot h\cdot f\left(u\left(t_{k}\right),t_{k}\right)\cdot f_{y}\left(u\left(t_{k}\right),t_{k}\right)$ $+\frac{1}{2}\cdot\left(\frac{1}{2}\cdot h\right)^{2}\cdot f_{tt}+\left(\frac{1}{2}\cdot h\right)^{2}f\cdot f_{tu}+\frac{1}{2}\left(\frac{1}{2}\cdot h\right)^{2}\cdot f^{2}f_{uu}+O\left(h^{3}\right)$ I understand that there is some kind of differentiating going on, but i cant figure it out. Can someone provide the steps involved? Edits I found out that total derivatives are used somehow, but i do not have written out the exact transformation steps.",,"['ordinary-differential-equations', 'numerical-methods', 'approximation']"
22,Differentiate a Differential equation,Differentiate a Differential equation,,"Given the Differential equation $y'=-2xy^{2}$. Find the derivative $\frac{d(y')}{dx}$! My approach , which is not correct according to Wolfram Alpha : Plugging in: $\frac{d(y')}{dx}=\frac{d(-2xy^{2})}{dx}$ Pulling out constants #1:$-2\cdot\frac{d(x^{1}\cdot y^{2})}{dx}$ Pulling out constants #2:$-2\cdot y^{2}\cdot\frac{d(x^{1})}{dx}$ Do the differential: $-2\cdot y^{2}\cdot1=\frac{d(y')}{dx}$ Wolfram Alpha computes this: $\frac{d(y')}{dx}=-4x\cdot y\cdot y'(x)-2y^{2}$ Since i do not have the Pro-Version of Wolfram Alpha and i am curious of knowing the maths behind it: What steps happen here and why can't i do it in my way ? Background info: I want to find those Derivatives in order to compute a numerical approximation for the Differential equation using a Taylor series expansion of 4th order. Edits (after comments and answers were given): So by the product rule, stated as follows: $\left(f(x)\cdot g(x)\right)^{'}=f'(x)\cdot g(x)+f(x)\cdot g'(x)$ I identified the following terms as the elements of that product rule: $f(x)=x$ $g(x)=\left(y(x)\right)^{2}=y(x)\cdot y(x)$ (the product rule has to be applied here a second time) $f'(x)=1$ $g'(x)=y'(x)\cdot y(x)+y(x)\cdot y'(x)$ $\left(f(x)\cdot g(x)\right)^{'}=f'(x)\cdot g(x)+f(x)\cdot g'(x)$ $\left(f(x)\cdot g(x)\right)^{'}=1\cdot\left(y(x)\right)^{2}+x\cdot\left(y'(x)\cdot y(x)+y(x)\cdot y'(x)\right)$ $\left(f(x)\cdot g(x)\right)^{'}=\left(y(x)\right)^{2}+2\cdot x\cdot y'(x)\cdot y(x)$","Given the Differential equation $y'=-2xy^{2}$. Find the derivative $\frac{d(y')}{dx}$! My approach , which is not correct according to Wolfram Alpha : Plugging in: $\frac{d(y')}{dx}=\frac{d(-2xy^{2})}{dx}$ Pulling out constants #1:$-2\cdot\frac{d(x^{1}\cdot y^{2})}{dx}$ Pulling out constants #2:$-2\cdot y^{2}\cdot\frac{d(x^{1})}{dx}$ Do the differential: $-2\cdot y^{2}\cdot1=\frac{d(y')}{dx}$ Wolfram Alpha computes this: $\frac{d(y')}{dx}=-4x\cdot y\cdot y'(x)-2y^{2}$ Since i do not have the Pro-Version of Wolfram Alpha and i am curious of knowing the maths behind it: What steps happen here and why can't i do it in my way ? Background info: I want to find those Derivatives in order to compute a numerical approximation for the Differential equation using a Taylor series expansion of 4th order. Edits (after comments and answers were given): So by the product rule, stated as follows: $\left(f(x)\cdot g(x)\right)^{'}=f'(x)\cdot g(x)+f(x)\cdot g'(x)$ I identified the following terms as the elements of that product rule: $f(x)=x$ $g(x)=\left(y(x)\right)^{2}=y(x)\cdot y(x)$ (the product rule has to be applied here a second time) $f'(x)=1$ $g'(x)=y'(x)\cdot y(x)+y(x)\cdot y'(x)$ $\left(f(x)\cdot g(x)\right)^{'}=f'(x)\cdot g(x)+f(x)\cdot g'(x)$ $\left(f(x)\cdot g(x)\right)^{'}=1\cdot\left(y(x)\right)^{2}+x\cdot\left(y'(x)\cdot y(x)+y(x)\cdot y'(x)\right)$ $\left(f(x)\cdot g(x)\right)^{'}=\left(y(x)\right)^{2}+2\cdot x\cdot y'(x)\cdot y(x)$",,"['ordinary-differential-equations', 'numerical-methods', 'taylor-expansion']"
23,First Order Linear PDE with Boundary Value Conditions,First Order Linear PDE with Boundary Value Conditions,,"I have encountered a problem here. Solve the partial differential equation, $$\frac{\partial u(t,x)}{\partial t}+\frac{\partial u(t,x)}{\partial x}+u(t,x)=1$$ with the inflow boundary value $$u(0,t)=t^2$$ I know how to solve this with an initial condition, just don't know how to implement the boundary condition.","I have encountered a problem here. Solve the partial differential equation, $$\frac{\partial u(t,x)}{\partial t}+\frac{\partial u(t,x)}{\partial x}+u(t,x)=1$$ with the inflow boundary value $$u(0,t)=t^2$$ I know how to solve this with an initial condition, just don't know how to implement the boundary condition.",,"['ordinary-differential-equations', 'partial-differential-equations']"
24,"Why $F(\mathbf q,\dot{\mathbf q},t)$ and not $F(\mathbf q,t)$?",Why  and not ?,"F(\mathbf q,\dot{\mathbf q},t) F(\mathbf q,t)","In beginner classical mechanics, which I've just started learning, a particle with coordinates $\mathbf q\in\mathbb R^n$ has its equation of motion specified by $F(\mathbf q,\dot{\mathbf q},t)=m\ddot{\mathbf q}$. Force is a function of all the coordinates necessary to describe the (rigid) body, and should cover all the degrees of freedom in the system. However, it seems to me that since $\dot{\mathbf q}=\frac{d\mathbf q}{dt}$, it's only necessary to specify $F(\mathbf q,t)$ for a complete description. I'm not sure I understand why this isn't the case, but my best guess is as follows. If $\mathbf q,\dot{\mathbf q}$ are given in a differential equation, such as $\dot{\mathbf q}=t^\mathbf q\mathbf q^\dot{\mathbf q}$, then it's necessary to specify all of $\mathbf q,\dot{\mathbf q},t$ in order to locate its position and velocity at any given time, unless the differential equation has a solution and we use it. But this explanation is strange to me. Since we can have a $k$th-order differential equation which specifies $\mathbf q,\frac{d}{dt}\mathbf q,\ldots,\frac{d^k}{dt^k}\mathbf q$ with no obvious solution, wouldn't that mean our equation of motion is actually $F(\mathbf q,\frac{d}{dt}\mathbf q,\ldots,\frac{d^k}{dt^k}\mathbf q,t)=m\frac{d^2}{dt^2}\mathbf q$? Edit The differential equation above which has a vector exponentiated by a vector is just a poorly thought-out attempt at an example of a diff eq with no obvious solution to me, it doesn't really matter what it is. Or if you want to consider that case, treat it as a 1D system, I guess.","In beginner classical mechanics, which I've just started learning, a particle with coordinates $\mathbf q\in\mathbb R^n$ has its equation of motion specified by $F(\mathbf q,\dot{\mathbf q},t)=m\ddot{\mathbf q}$. Force is a function of all the coordinates necessary to describe the (rigid) body, and should cover all the degrees of freedom in the system. However, it seems to me that since $\dot{\mathbf q}=\frac{d\mathbf q}{dt}$, it's only necessary to specify $F(\mathbf q,t)$ for a complete description. I'm not sure I understand why this isn't the case, but my best guess is as follows. If $\mathbf q,\dot{\mathbf q}$ are given in a differential equation, such as $\dot{\mathbf q}=t^\mathbf q\mathbf q^\dot{\mathbf q}$, then it's necessary to specify all of $\mathbf q,\dot{\mathbf q},t$ in order to locate its position and velocity at any given time, unless the differential equation has a solution and we use it. But this explanation is strange to me. Since we can have a $k$th-order differential equation which specifies $\mathbf q,\frac{d}{dt}\mathbf q,\ldots,\frac{d^k}{dt^k}\mathbf q$ with no obvious solution, wouldn't that mean our equation of motion is actually $F(\mathbf q,\frac{d}{dt}\mathbf q,\ldots,\frac{d^k}{dt^k}\mathbf q,t)=m\frac{d^2}{dt^2}\mathbf q$? Edit The differential equation above which has a vector exponentiated by a vector is just a poorly thought-out attempt at an example of a diff eq with no obvious solution to me, it doesn't really matter what it is. Or if you want to consider that case, treat it as a 1D system, I guess.",,"['ordinary-differential-equations', 'physics', 'classical-mechanics']"
25,Differential equation: $A(x)y''(x)+A'(x)y'(x)+y(x)/A(x)=0$,Differential equation:,A(x)y''(x)+A'(x)y'(x)+y(x)/A(x)=0,"So give the differential equation $$A(x)y''(x)+A'(x)y'(x)+\frac{y(x)}{A(x)}=0,$$ with $A(x)$ a known function and $y(x)$ te be determined. What is the solution for this differential equation ? I've tried substituting $y(x)=A(x)u(x)$, but unfortunately this didn't eliminate my unknown variable $A(x)$. I don't know if there are any other tricks or substitutions that I can try to solve this situation ? I als considered switching to $u=A(x)$ as my independent variable, but that also didn't help me that much ...","So give the differential equation $$A(x)y''(x)+A'(x)y'(x)+\frac{y(x)}{A(x)}=0,$$ with $A(x)$ a known function and $y(x)$ te be determined. What is the solution for this differential equation ? I've tried substituting $y(x)=A(x)u(x)$, but unfortunately this didn't eliminate my unknown variable $A(x)$. I don't know if there are any other tricks or substitutions that I can try to solve this situation ? I als considered switching to $u=A(x)$ as my independent variable, but that also didn't help me that much ...",,"['real-analysis', 'analysis', 'ordinary-differential-equations']"
26,Describing non-vanishing $1$-forms on two dimensional manifolds.,Describing non-vanishing -forms on two dimensional manifolds.,1,"Let $h_1 \mathrm{d}x_1 + h_2 \mathrm{d}x_2$ be a non-vanishing $1$-form on a $2$-dimensional manifold. Why do locally exist smooth functions $f,g$ with $f\mathrm{d}g= h_1 \mathrm{d}x_1 + h_2 \mathrm{d}x_2$? I think this should follow from some statement about differential equations (and existence of some integrating factor?) in usual analysis, but sadly in all of mathematics differential equations are probably the topic I know the least about. Edit: It would suffice to prove the following: let $h_1, h_2: \mathbb{R}^2 \rightarrow \mathbb{R}$ be smooth functions. Then exists for all $x \in \mathbb{R}^2$ a neighborhood $U$ and $f,g_1, g_2$ smooth functions on $U$, such that $h_i=f g_i$ and $\partial_{x_1}g_2= \partial_{x_2}g_1$.","Let $h_1 \mathrm{d}x_1 + h_2 \mathrm{d}x_2$ be a non-vanishing $1$-form on a $2$-dimensional manifold. Why do locally exist smooth functions $f,g$ with $f\mathrm{d}g= h_1 \mathrm{d}x_1 + h_2 \mathrm{d}x_2$? I think this should follow from some statement about differential equations (and existence of some integrating factor?) in usual analysis, but sadly in all of mathematics differential equations are probably the topic I know the least about. Edit: It would suffice to prove the following: let $h_1, h_2: \mathbb{R}^2 \rightarrow \mathbb{R}$ be smooth functions. Then exists for all $x \in \mathbb{R}^2$ a neighborhood $U$ and $f,g_1, g_2$ smooth functions on $U$, such that $h_i=f g_i$ and $\partial_{x_1}g_2= \partial_{x_2}g_1$.",,"['integration', 'ordinary-differential-equations', 'differential-geometry']"
27,ODE Guidance Needed,ODE Guidance Needed,,"This isn't from any homework assignment. I'm just trying to find the solution (either explicit or implicit) for this ODE. $$(x+2y)\,dx + (y)\,dy = 0$$ The first obvious step to take is to check if this is exact, and if not, find an integrating factor $\mu(x,y)$ such that it will become one when multiplied by the function $\mu$. It isn't exact (can be quickly checked), so here's what I did. Multiply by $\mu$ and assume that the equation is then exact: $$ \frac{\partial}{\partial y}(\mu(x,y)(x+2y))= \frac{\partial}{\partial x}(\mu(x,y)(y)) $$ $$ \mu_y(x,y)(x+2y) + 2\mu(x,y) = \mu_x(x,y)y $$ Now we assume that $\mu(x,y)$ is either an equation of only $x$ or only $y$. If we assume the former, we will inevitably end up with $\mu'$ multiplied by $y$, and so the solution ($\mu$(x)) would have to have a $y$ in it, which would contradict our original assumption. If we go with the latter ($\mu = \mu(y)$), then it's clear that we would end up with the same problem/contradiction. Thus, our integrating function/factor $\mu = \mu(x,y)$, but now that would be out of the scope of ordinary differential equations. In addition to that, whenever I try to make some nifty substitution to simplify this and end up with an explicit solution, it never works. Could you give me some guidance please? P.S: Don't just give me the solution (either implicit or explicit). I can easily get that on Wolframalpha","This isn't from any homework assignment. I'm just trying to find the solution (either explicit or implicit) for this ODE. $$(x+2y)\,dx + (y)\,dy = 0$$ The first obvious step to take is to check if this is exact, and if not, find an integrating factor $\mu(x,y)$ such that it will become one when multiplied by the function $\mu$. It isn't exact (can be quickly checked), so here's what I did. Multiply by $\mu$ and assume that the equation is then exact: $$ \frac{\partial}{\partial y}(\mu(x,y)(x+2y))= \frac{\partial}{\partial x}(\mu(x,y)(y)) $$ $$ \mu_y(x,y)(x+2y) + 2\mu(x,y) = \mu_x(x,y)y $$ Now we assume that $\mu(x,y)$ is either an equation of only $x$ or only $y$. If we assume the former, we will inevitably end up with $\mu'$ multiplied by $y$, and so the solution ($\mu$(x)) would have to have a $y$ in it, which would contradict our original assumption. If we go with the latter ($\mu = \mu(y)$), then it's clear that we would end up with the same problem/contradiction. Thus, our integrating function/factor $\mu = \mu(x,y)$, but now that would be out of the scope of ordinary differential equations. In addition to that, whenever I try to make some nifty substitution to simplify this and end up with an explicit solution, it never works. Could you give me some guidance please? P.S: Don't just give me the solution (either implicit or explicit). I can easily get that on Wolframalpha",,"['calculus', 'ordinary-differential-equations']"
28,how to find matrix from its exponential form,how to find matrix from its exponential form,,I know about the relation $$\frac{d}{dt}e^{At}=Ae^{At}$$ Is the only way to use it is to find the inverse of $e^{At}$ and then post-multiply on both sides? Is there a better approach?,I know about the relation $$\frac{d}{dt}e^{At}=Ae^{At}$$ Is the only way to use it is to find the inverse of $e^{At}$ and then post-multiply on both sides? Is there a better approach?,,['ordinary-differential-equations']
29,The volume is preserved by the flow: where is the absolute value?,The volume is preserved by the flow: where is the absolute value?,,"Consider the following excerpt of the Liouville's theorem proof taken from ""Arnold - mathematical methods of classical mechanics"": In changing the variables in the integral, I don't understand why there is not the absolute value of the Jacobian determinant. Why the determinant of the Jacobian of the flow is positive?","Consider the following excerpt of the Liouville's theorem proof taken from ""Arnold - mathematical methods of classical mechanics"": In changing the variables in the integral, I don't understand why there is not the absolute value of the Jacobian determinant. Why the determinant of the Jacobian of the flow is positive?",,"['ordinary-differential-equations', 'definite-integrals', 'classical-mechanics']"
30,Differential equation $\frac{dy}{dt}=y(1-y)$,Differential equation,\frac{dy}{dt}=y(1-y),I'm starting with $$\frac{dy}{dt}=y(1-y)$$ Then I take the obvious steps. $$\int\left(\frac1y+\frac1{1-y}\right)dy=t+C\\\ln|y|+\ln|1-y|=t+C\\\ln(|y||1-y|)=t+C\\\sqrt{y^2}\sqrt{(1-y)^2}=Ce^t\\\sqrt{y^2(1-y)^2}=Ce^t\\y^2(1-y)^2=Ce^{2t}\\y^2-2y^3+y^4=Ce^{2t}$$ I am stuck here for finding an explicit solution.,I'm starting with $$\frac{dy}{dt}=y(1-y)$$ Then I take the obvious steps. $$\int\left(\frac1y+\frac1{1-y}\right)dy=t+C\\\ln|y|+\ln|1-y|=t+C\\\ln(|y||1-y|)=t+C\\\sqrt{y^2}\sqrt{(1-y)^2}=Ce^t\\\sqrt{y^2(1-y)^2}=Ce^t\\y^2(1-y)^2=Ce^{2t}\\y^2-2y^3+y^4=Ce^{2t}$$ I am stuck here for finding an explicit solution.,,['ordinary-differential-equations']
31,"Is there any general function $x(t)$ that gives the solution to $x''(t) = k/x(t)^2$, where k is a constant?","Is there any general function  that gives the solution to , where k is a constant?",x(t) x''(t) = k/x(t)^2,"In physics class, I  often come across various inverse square law equations like the following: $F_G= G\frac{m_1m_2}{r^2}$ $F_E = k_e\frac{q_1q_2}{r^2}$ Specifically, we are typically given problems involving one fixed body and one body that interacts with that body over a one-dimensional axis (in a manner given by one of the above laws). Thus, the acceleration at any given time of the free body can be modeled by the following differential equation, where the masses/charges/physical constants involved collapse into a single value, k. (This physics stuff is just included for some context.) $x''(t)=\frac{k}{x(t)^2}$ However, my exact mathematical question is, given any initial conditions for x(0) and x'(0), is there a general solution to the above equation? In short, tell me about solving the subsequent equations for x(t) please. $x(0) = \mathrm{some\, initial\, value} $ $x'(0) = \mathrm{some\, initial\, value} $ $x''(t) = \frac{k}{x(t)^2} $, where k is a constant Again -- please help, A Curious Calculus Student Who Doesn't Know Much P.S. Additionally, how does the sign of k -- whether the force is attractive or repulsive -- affect the result?","In physics class, I  often come across various inverse square law equations like the following: $F_G= G\frac{m_1m_2}{r^2}$ $F_E = k_e\frac{q_1q_2}{r^2}$ Specifically, we are typically given problems involving one fixed body and one body that interacts with that body over a one-dimensional axis (in a manner given by one of the above laws). Thus, the acceleration at any given time of the free body can be modeled by the following differential equation, where the masses/charges/physical constants involved collapse into a single value, k. (This physics stuff is just included for some context.) $x''(t)=\frac{k}{x(t)^2}$ However, my exact mathematical question is, given any initial conditions for x(0) and x'(0), is there a general solution to the above equation? In short, tell me about solving the subsequent equations for x(t) please. $x(0) = \mathrm{some\, initial\, value} $ $x'(0) = \mathrm{some\, initial\, value} $ $x''(t) = \frac{k}{x(t)^2} $, where k is a constant Again -- please help, A Curious Calculus Student Who Doesn't Know Much P.S. Additionally, how does the sign of k -- whether the force is attractive or repulsive -- affect the result?",,"['calculus', 'ordinary-differential-equations']"
32,Stability of a feedback system,Stability of a feedback system,,"Take the following feedback system: $\dot{x} = (\theta - k_1) x - k_2  x^3$ Now my book says: For $\theta > k_1$, the equilibrium $x = 0$ is unstable. I wonder why... Furthermore my book indicates that it is easy to see that $x(t)$ will converge to one of the two new equilibria $\pm \sqrt{\frac{\theta-k_1}{k_2}}$. Again, how did they obtain this result?","Take the following feedback system: $\dot{x} = (\theta - k_1) x - k_2  x^3$ Now my book says: For $\theta > k_1$, the equilibrium $x = 0$ is unstable. I wonder why... Furthermore my book indicates that it is easy to see that $x(t)$ will converge to one of the two new equilibria $\pm \sqrt{\frac{\theta-k_1}{k_2}}$. Again, how did they obtain this result?",,"['ordinary-differential-equations', 'dynamical-systems', 'nonlinear-system']"
33,How to match eigenvalues with directional field graphs?,How to match eigenvalues with directional field graphs?,,How they were able to match eigenvalues with the graphs?,How they were able to match eigenvalues with the graphs?,,"['ordinary-differential-equations', 'eigenvalues-eigenvectors']"
34,"How to solve the system $t\frac{dx}{dt}=-x+yt$, $t\frac{dy}{dt}=-2x+yt$?","How to solve the system , ?",t\frac{dx}{dt}=-x+yt t\frac{dy}{dt}=-2x+yt,"Could you show me how to solve the following simultaneous differential equations? I tried substitution such that $u=xt$, yet I couldn't find the solution. $$\frac{dx}{dt}t=-x+yt$$ $$\frac{dy}{dt}t=-2x+yt$$","Could you show me how to solve the following simultaneous differential equations? I tried substitution such that $u=xt$, yet I couldn't find the solution. $$\frac{dx}{dt}t=-x+yt$$ $$\frac{dy}{dt}t=-2x+yt$$",,"['ordinary-differential-equations', 'systems-of-equations']"
35,Find the orthogonal trajectories of the following family of curves: $x^3y - xy^3 = \alpha$,Find the orthogonal trajectories of the following family of curves:,x^3y - xy^3 = \alpha,"The answer is $-6x^2y^2 + x^4 + y^4 = \beta$, but I get $-3x^2y^2 + \frac{x^4}{4} + \frac{y^4}{4} = \beta$ and I can't seem to find my mistake. My work: Differentiaing the given equation, we get  $$3x^2y + x^3\frac{dy}{dx} - y^3 - 3y^2x\frac{dy}{dx} = 0. $$ So $$3x^2y - y^3 = (3y^2x -x^3)\frac{dy}{dx}, $$ and hence  $$ \frac{dy}{dx} = \frac{3x^2y - y^3}{3y^2x - x^3}. $$ Now, the slope of the other family of curves will be the negative inverse, so for the orthogonal trajectory, we have $$\frac{dy}{dx} = -\frac{3y^2x-x^3}{3x^2y-y^3}, $$ which we can write as  $$-(3y^2x-x^3)dx = (3x^2y-y^3)dy. $$ Integration of the last differential equation yields  $$-\frac{3}{2}y^2x^2 + \frac{x^4}{4} = \frac{3}{2}x^2y^2 - \frac{y^4}{4} + \beta, $$ $\beta$ being an arbitrary constant.  This simplifies to $$-3y^2x^2 + \frac{x^4}{4} + \frac{y^4}{4} = \beta. $$ which is not correct. Where am I going wrong?","The answer is $-6x^2y^2 + x^4 + y^4 = \beta$, but I get $-3x^2y^2 + \frac{x^4}{4} + \frac{y^4}{4} = \beta$ and I can't seem to find my mistake. My work: Differentiaing the given equation, we get  $$3x^2y + x^3\frac{dy}{dx} - y^3 - 3y^2x\frac{dy}{dx} = 0. $$ So $$3x^2y - y^3 = (3y^2x -x^3)\frac{dy}{dx}, $$ and hence  $$ \frac{dy}{dx} = \frac{3x^2y - y^3}{3y^2x - x^3}. $$ Now, the slope of the other family of curves will be the negative inverse, so for the orthogonal trajectory, we have $$\frac{dy}{dx} = -\frac{3y^2x-x^3}{3x^2y-y^3}, $$ which we can write as  $$-(3y^2x-x^3)dx = (3x^2y-y^3)dy. $$ Integration of the last differential equation yields  $$-\frac{3}{2}y^2x^2 + \frac{x^4}{4} = \frac{3}{2}x^2y^2 - \frac{y^4}{4} + \beta, $$ $\beta$ being an arbitrary constant.  This simplifies to $$-3y^2x^2 + \frac{x^4}{4} + \frac{y^4}{4} = \beta. $$ which is not correct. Where am I going wrong?",,"['calculus', 'ordinary-differential-equations']"
36,Exercises about Distributions,Exercises about Distributions,,"I'm looking for references (books or pdf) about the following themes (especially the first two) : Fourier Series of Distributions. Distributional solutions of ordinary differential equations. Fourier Transform of Distributions. Convolution of Distributions. Tempered Distributions. with exercises (with answers) : direct applications of the theory, simple problems with detailed questions. Thank you a lot.","I'm looking for references (books or pdf) about the following themes (especially the first two) : Fourier Series of Distributions. Distributional solutions of ordinary differential equations. Fourier Transform of Distributions. Convolution of Distributions. Tempered Distributions. with exercises (with answers) : direct applications of the theory, simple problems with detailed questions. Thank you a lot.",,"['ordinary-differential-equations', 'reference-request', 'fourier-analysis', 'distribution-theory']"
37,Why is subspace of solutions of linear ODE n dim?,Why is subspace of solutions of linear ODE n dim?,,"If we are considering homogeneous linear ordinary differential equations among differentiable real-valued functions on $\mathbb{R}$, i.e. equations of the form $\mathrm{D} f =0$, then why is the subspace of solutions $n$-dimensional, where $n$ is the largest integer such that the differential operator $\mathrm{D}$ contains a derivative of degree $n$? Thanks!","If we are considering homogeneous linear ordinary differential equations among differentiable real-valued functions on $\mathbb{R}$, i.e. equations of the form $\mathrm{D} f =0$, then why is the subspace of solutions $n$-dimensional, where $n$ is the largest integer such that the differential operator $\mathrm{D}$ contains a derivative of degree $n$? Thanks!",,['ordinary-differential-equations']
38,How to solve differential equations using fft?,How to solve differential equations using fft?,,Can anyone point me to the principles and books/websites about it? Which properties must the differential equation have that a solution with fft is possible? Why can it be solved that way?,Can anyone point me to the principles and books/websites about it? Which properties must the differential equation have that a solution with fft is possible? Why can it be solved that way?,,"['ordinary-differential-equations', 'reference-request', 'fourier-analysis']"
39,Verify the real solution of a linear system of differential equation,Verify the real solution of a linear system of differential equation,,"I'm trying to solve $Y' = AY$ where $A= \left[   \begin{array}{ c c }      -2 & 6 \\      -3 & 4   \end{array} \right]$ I have found the eigenvalue $1 \pm 3i$ with eigenvector for $1+3i: $ $v =  \left[ \begin{array}{ c c }      1-i \\      1    \end{array} \right]$ Which seems to be correct by testing $Av = (1+3i)v$ but when I try to write it as a real solution I don't seem to get the right answer. $$\left[ \begin{array}{ c c }      1-i \\      1    \end{array} \right](\cos 3t + i\sin 3t) = \left[ \begin{array}{ c c }      \cos 3t + \sin 3t - i\cos 3t + i\sin 3t \\      \cos 3t + i\sin 3t    \end{array} \right]$$ $$v_1 = \left[ \begin{array}{ c c }      \cos 3t + \sin 3t \\      \cos 3t    \end{array} \right],\ v_2 = \left[ \begin{array}{ c c }      \cos 3t + \sin 3t \\      \sin 3t    \end{array} \right]$$ If I now verify by $v_1' = Av_1$ $$\left[ \begin{array}{ c c }      3(\cos 3t - \sin 3t) \\      -3\sin 3t    \end{array} \right] \neq  \left[ \begin{array}{ c c }      4\cos 3t - 2\sin 3t \\      \cos 3t - 3\sin 3t    \end{array} \right]$$","I'm trying to solve $Y' = AY$ where $A= \left[   \begin{array}{ c c }      -2 & 6 \\      -3 & 4   \end{array} \right]$ I have found the eigenvalue $1 \pm 3i$ with eigenvector for $1+3i: $ $v =  \left[ \begin{array}{ c c }      1-i \\      1    \end{array} \right]$ Which seems to be correct by testing $Av = (1+3i)v$ but when I try to write it as a real solution I don't seem to get the right answer. $$\left[ \begin{array}{ c c }      1-i \\      1    \end{array} \right](\cos 3t + i\sin 3t) = \left[ \begin{array}{ c c }      \cos 3t + \sin 3t - i\cos 3t + i\sin 3t \\      \cos 3t + i\sin 3t    \end{array} \right]$$ $$v_1 = \left[ \begin{array}{ c c }      \cos 3t + \sin 3t \\      \cos 3t    \end{array} \right],\ v_2 = \left[ \begin{array}{ c c }      \cos 3t + \sin 3t \\      \sin 3t    \end{array} \right]$$ If I now verify by $v_1' = Av_1$ $$\left[ \begin{array}{ c c }      3(\cos 3t - \sin 3t) \\      -3\sin 3t    \end{array} \right] \neq  \left[ \begin{array}{ c c }      4\cos 3t - 2\sin 3t \\      \cos 3t - 3\sin 3t    \end{array} \right]$$",,['ordinary-differential-equations']
40,Solve $y'+a(x)y=b(x)$ where $b(x)$ is not continuous,Solve  where  is not continuous,y'+a(x)y=b(x) b(x),"Find all the solutionsof the equation: $$y'+ay=b(x),\ 0<x<\infty,\ $$ where $a$ is a constant and $b(x)=1$ for $0\le x\le \alpha$, and $b(x)=0$ for $x\gt \alpha$ and $\alpha$ here is a positive constant. Well, I really got stuck on this problems where the right hand side of  the differential equation $b(x)$ is not continuous. Could anyone give me some hints on this, please?","Find all the solutionsof the equation: $$y'+ay=b(x),\ 0<x<\infty,\ $$ where $a$ is a constant and $b(x)=1$ for $0\le x\le \alpha$, and $b(x)=0$ for $x\gt \alpha$ and $\alpha$ here is a positive constant. Well, I really got stuck on this problems where the right hand side of  the differential equation $b(x)$ is not continuous. Could anyone give me some hints on this, please?",,['ordinary-differential-equations']
41,Help with special function differential equation,Help with special function differential equation,,"this is my first time to use this site.  Please let me know if the equations are unreadable, latex isn't my first language. We've been covering Legendre, Bessel, and Confluent Hypergeometric Functions as special functions frequently observed in solving differential and integral equations.  This chapter is difficult to understand. I seem to be hitting a wall and/or overthinking the following: Suppose a linear second order differential equation has the following solution: $$ x^{\alpha }J_{\pm m}(\beta x^{\gamma }) $$ What differential equation might this be?  Using this, give the general solution of: $$ y''+x^{2}y=0 $$ Any help is appreciated.","this is my first time to use this site.  Please let me know if the equations are unreadable, latex isn't my first language. We've been covering Legendre, Bessel, and Confluent Hypergeometric Functions as special functions frequently observed in solving differential and integral equations.  This chapter is difficult to understand. I seem to be hitting a wall and/or overthinking the following: Suppose a linear second order differential equation has the following solution: $$ x^{\alpha }J_{\pm m}(\beta x^{\gamma }) $$ What differential equation might this be?  Using this, give the general solution of: $$ y''+x^{2}y=0 $$ Any help is appreciated.",,"['ordinary-differential-equations', 'special-functions']"
42,$\omega(x)=\mathbb{R}^2$ is false.,is false.,\omega(x)=\mathbb{R}^2,"Let $ \phi $ be a flow on $ \mathbb{R}^2 $ coming from $ C^1$ class function. I want to prove that there is no point $x \in \mathbb{R}^2 $ such that $\omega(x)=\mathbb{R}^2$ (where $\omega (x) := \{y \in \mathbb{R}^2 \ | \ \exists t_{n} \to \infty : \text{ }\phi(t_{n},x) \to y \}$). It is an implication from Poincare-Bendixson theorem? (I've studied only this version: $ \emptyset \neq \omega (x) $ - bounded $ \Rightarrow \omega (x)$ has an equilibrium point or is a closed orbit.) Thank you for help.","Let $ \phi $ be a flow on $ \mathbb{R}^2 $ coming from $ C^1$ class function. I want to prove that there is no point $x \in \mathbb{R}^2 $ such that $\omega(x)=\mathbb{R}^2$ (where $\omega (x) := \{y \in \mathbb{R}^2 \ | \ \exists t_{n} \to \infty : \text{ }\phi(t_{n},x) \to y \}$). It is an implication from Poincare-Bendixson theorem? (I've studied only this version: $ \emptyset \neq \omega (x) $ - bounded $ \Rightarrow \omega (x)$ has an equilibrium point or is a closed orbit.) Thank you for help.",,"['ordinary-differential-equations', 'dynamical-systems']"
43,Stuck on this linear equation $y'=(3x^2-e^x)/2y-5$,Stuck on this linear equation,y'=(3x^2-e^x)/2y-5,"Given this separable equation: $y'=(3x^2-e^x)/2y-5$. I did integration, I found my constant $c=-3$. And now I am stuck on this part: $y^2-5y=x^3-e^x-3$. No matter what I do, I cannot come to this correct answer, according to the textbook, $y=-5/2-sqrt(x^3-e^x+13/4)$.","Given this separable equation: $y'=(3x^2-e^x)/2y-5$. I did integration, I found my constant $c=-3$. And now I am stuck on this part: $y^2-5y=x^3-e^x-3$. No matter what I do, I cannot come to this correct answer, according to the textbook, $y=-5/2-sqrt(x^3-e^x+13/4)$.",,"['integration', 'ordinary-differential-equations']"
44,General Solution to Quasilinear PDE using Method of Characteristics,General Solution to Quasilinear PDE using Method of Characteristics,,"This is a homework that I'm having a bit of trouble with.  I posted it previously but there was a typo in my original post.  Since I received an answer for the incorrect problem it was suggested that I post the correct problem as a new question: Find a general solution of: $(x^2+3y^2+3u^2)u_x-2xyu_y+2xu=0~.$ Of course this is to be done using the method of characteristics. The characteristic equations are: $\dfrac{dx}{x^2+3y^2+3z^2}=-\dfrac{dy}{2xy}=-\dfrac{dz}{2xz}$ The idea is to find two functions, say $\phi$ and $\psi$, that are independent (the gradients are not colinear) and that are constant along the characteristics. We can then express the general solution as F($\phi$,$\psi$)=0 where F is an arbitrary $C^1$ function.  It is easy to find that $\psi=\dfrac{y}{z}=const$.  Finding a suitable $\phi$ is what I'm having trouble with.  I tried substituting $\dfrac{y}{const}$ for $z$ in the $dx$ equation and solving for $\dfrac{dx}{dy}$ but the resulting ODE has no obvious (to me) method of solution. Any suggestions?","This is a homework that I'm having a bit of trouble with.  I posted it previously but there was a typo in my original post.  Since I received an answer for the incorrect problem it was suggested that I post the correct problem as a new question: Find a general solution of: $(x^2+3y^2+3u^2)u_x-2xyu_y+2xu=0~.$ Of course this is to be done using the method of characteristics. The characteristic equations are: $\dfrac{dx}{x^2+3y^2+3z^2}=-\dfrac{dy}{2xy}=-\dfrac{dz}{2xz}$ The idea is to find two functions, say $\phi$ and $\psi$, that are independent (the gradients are not colinear) and that are constant along the characteristics. We can then express the general solution as F($\phi$,$\psi$)=0 where F is an arbitrary $C^1$ function.  It is easy to find that $\psi=\dfrac{y}{z}=const$.  Finding a suitable $\phi$ is what I'm having trouble with.  I tried substituting $\dfrac{y}{const}$ for $z$ in the $dx$ equation and solving for $\dfrac{dx}{dy}$ but the resulting ODE has no obvious (to me) method of solution. Any suggestions?",,"['calculus', 'real-analysis', 'analysis', 'ordinary-differential-equations', 'partial-differential-equations']"
45,Any other proof for the Gronwall's inequality?,Any other proof for the Gronwall's inequality?,,"The Gronwall's inequality can be stated as: Suppose $u,v$ be continuous function on $[a,b]$ with $u\geq 0$, if $$v(t)\leq C+\int_a^t u(s)v(s)ds,\quad \forall t\in[a,b]$$ where $C$ is a constant, then  $$ v(t)\leq C\exp\left(\int_a^t u(s)ds\right). $$ I only find a proof in WiKi, and in fact I don't know the idea of that proof there, so any guide of ideas of proof or another proof would be helpful to me.","The Gronwall's inequality can be stated as: Suppose $u,v$ be continuous function on $[a,b]$ with $u\geq 0$, if $$v(t)\leq C+\int_a^t u(s)v(s)ds,\quad \forall t\in[a,b]$$ where $C$ is a constant, then  $$ v(t)\leq C\exp\left(\int_a^t u(s)ds\right). $$ I only find a proof in WiKi, and in fact I don't know the idea of that proof there, so any guide of ideas of proof or another proof would be helpful to me.",,"['ordinary-differential-equations', 'inequality', 'gronwall-type-inequality']"
46,General solution of a differential equation,General solution of a differential equation,,what is the general solution of this diff. equation $$x^2y''-4xy'+6y=x$$ Tried calling $y=xv$ but didnt work. ($x^2v''-2xv'+v=1$) what can I try else?,what is the general solution of this diff. equation $$x^2y''-4xy'+6y=x$$ Tried calling $y=xv$ but didnt work. ($x^2v''-2xv'+v=1$) what can I try else?,,['ordinary-differential-equations']
47,Special Differential Equation (continued-2),Special Differential Equation (continued-2),,May you help me out in solving inhomogeneous differential equation looking like[this is radial part of Schrodinger equation]: $$R^{\prime\prime}+\frac{1}{r}R^{\prime}-\frac{a^{2}}{r^{2}}R+b^{2}R-\frac{c^{2}}{r}\delta(r-\rho)R-d^{2}R=F\exp(ikr)$$,May you help me out in solving inhomogeneous differential equation looking like[this is radial part of Schrodinger equation]: $$R^{\prime\prime}+\frac{1}{r}R^{\prime}-\frac{a^{2}}{r^{2}}R+b^{2}R-\frac{c^{2}}{r}\delta(r-\rho)R-d^{2}R=F\exp(ikr)$$,,['ordinary-differential-equations']
48,Final step in solving differential equation,Final step in solving differential equation,,"I can't figure out the last step of the following question: Show that the general solution of the differential equation: $dy/dx + y\tan x=1$ is given by: $y\sec x=\ln(\sec x + \tan x) + C$ This is what I've worked out so far: To make things easier let $u(x)=\exp(\int\! \tan(x)\,\mathrm dx)=\sec(x)$ Multyply both sides by $u(x): \sec(x)\frac {dy(x)} {dx} + {\sec(x)}{\tan(x)}y(x)=\sec(x)$ Then substitute $\sec(x)\tan(x)= \frac {d\sec(x)} {dx}$ $\sec(x) \dfrac {dy(x)}{dx}+ \dfrac{d \sec(x)} {dx}y(x)= \sec(x)$ Using the reverse product rule of the LHS: $\displaystyle\frac d {dx} (\sec(x)y(x))dx= \int \sec(x)\,dx$ then integrating both sides with respect to x $\int \frac d {dx} (\sec(x)y(x))dx= \int \sec(x)dx$ Therefore: $\sec(x)y= -\log(\cos(\tfrac x 2)- \sin(\tfrac x 2))+\log(\cos(\tfrac x 2) + \sin(\tfrac x 2)) + C$ Where do I go from here to find the solution (given in the question)? Thanks in advance!","I can't figure out the last step of the following question: Show that the general solution of the differential equation: $dy/dx + y\tan x=1$ is given by: $y\sec x=\ln(\sec x + \tan x) + C$ This is what I've worked out so far: To make things easier let $u(x)=\exp(\int\! \tan(x)\,\mathrm dx)=\sec(x)$ Multyply both sides by $u(x): \sec(x)\frac {dy(x)} {dx} + {\sec(x)}{\tan(x)}y(x)=\sec(x)$ Then substitute $\sec(x)\tan(x)= \frac {d\sec(x)} {dx}$ $\sec(x) \dfrac {dy(x)}{dx}+ \dfrac{d \sec(x)} {dx}y(x)= \sec(x)$ Using the reverse product rule of the LHS: $\displaystyle\frac d {dx} (\sec(x)y(x))dx= \int \sec(x)\,dx$ then integrating both sides with respect to x $\int \frac d {dx} (\sec(x)y(x))dx= \int \sec(x)dx$ Therefore: $\sec(x)y= -\log(\cos(\tfrac x 2)- \sin(\tfrac x 2))+\log(\cos(\tfrac x 2) + \sin(\tfrac x 2)) + C$ Where do I go from here to find the solution (given in the question)? Thanks in advance!",,['ordinary-differential-equations']
49,To find closed form of $\int_0^{\frac{\pi}{2}} e^{-x\tan t+\alpha t} \;dt $,To find closed form of,\int_0^{\frac{\pi}{2}} e^{-x\tan t+\alpha t} \;dt ,"Let $x\geq 0$, then $$\int_0^{\frac{\pi}{2}} e^{-x\tan t+\alpha t} \;dt = U_{\alpha} (x) $$ $$-\int_0^{\frac{\pi}{2}} \tan t \ e^{-x\tan t+\alpha t} \;dt = \frac{d (U_{\alpha} (x) )}{dx} $$ $$\int_0^{\frac{\pi}{2}} \tan^2 t \ e^{-x\tan t+\alpha t} \;dt = \frac{d^2(U_{\alpha} (x) )}{dx^2} $$ $$\int_0^{\frac{\pi}{2}} [-x(1+\tan^2 t)+\alpha] \ e^{-x\tan t+\alpha t} \;dt = -x\frac{d^2(U_{\alpha} (x) )}{dx^2}+ (\alpha-x) U_{\alpha} (x)$$ $$ \ e^{-x\tan t+\alpha t} |_0^{\frac{\pi}{2}} = -x\frac{d^2(U_{\alpha} (x) )}{dx^2}+ (\alpha-x) U_{\alpha} (x)$$ $$  -x\frac{d^2(U_{\alpha} (x) )}{dx^2}+ (\alpha-x) U_{\alpha} (x)=-1$$ $$  x\frac{d^2(U_{\alpha} (x) )}{dx^2}+ (x-\alpha) U_{\alpha} (x)=1$$ Boundry conditions:  for x=0 $$\int_0^{\frac{\pi}{2}} e^{\alpha t} \;dt = U_{\alpha} (0) $$ $$\int_0^{\frac{\pi}{2}} e^{\alpha t} \;dt = U_{\alpha} (0)= \frac{e^{\frac{\pi}{2}\alpha}-1}{\alpha}$$ for $x=\infty $  ,$U_{\alpha} (\infty)=0$ Could you please help me to find the closed form of $U_{\alpha} (x)$? Series and special function expressions all are welcome. Thanks a lot for answers","Let $x\geq 0$, then $$\int_0^{\frac{\pi}{2}} e^{-x\tan t+\alpha t} \;dt = U_{\alpha} (x) $$ $$-\int_0^{\frac{\pi}{2}} \tan t \ e^{-x\tan t+\alpha t} \;dt = \frac{d (U_{\alpha} (x) )}{dx} $$ $$\int_0^{\frac{\pi}{2}} \tan^2 t \ e^{-x\tan t+\alpha t} \;dt = \frac{d^2(U_{\alpha} (x) )}{dx^2} $$ $$\int_0^{\frac{\pi}{2}} [-x(1+\tan^2 t)+\alpha] \ e^{-x\tan t+\alpha t} \;dt = -x\frac{d^2(U_{\alpha} (x) )}{dx^2}+ (\alpha-x) U_{\alpha} (x)$$ $$ \ e^{-x\tan t+\alpha t} |_0^{\frac{\pi}{2}} = -x\frac{d^2(U_{\alpha} (x) )}{dx^2}+ (\alpha-x) U_{\alpha} (x)$$ $$  -x\frac{d^2(U_{\alpha} (x) )}{dx^2}+ (\alpha-x) U_{\alpha} (x)=-1$$ $$  x\frac{d^2(U_{\alpha} (x) )}{dx^2}+ (x-\alpha) U_{\alpha} (x)=1$$ Boundry conditions:  for x=0 $$\int_0^{\frac{\pi}{2}} e^{\alpha t} \;dt = U_{\alpha} (0) $$ $$\int_0^{\frac{\pi}{2}} e^{\alpha t} \;dt = U_{\alpha} (0)= \frac{e^{\frac{\pi}{2}\alpha}-1}{\alpha}$$ for $x=\infty $  ,$U_{\alpha} (\infty)=0$ Could you please help me to find the closed form of $U_{\alpha} (x)$? Series and special function expressions all are welcome. Thanks a lot for answers",,"['integration', 'ordinary-differential-equations', 'special-functions']"
50,"Find $y$ which satisfies: $y'=y^a$, $y(a)=a-2$, for $a \in \mathbb{N}$","Find  which satisfies: , , for",y y'=y^a y(a)=a-2 a \in \mathbb{N},"I'd love your help with finding the function $y$ which satisfies: $y'=y^a$, $y(a)=a-2$, for $a \in \mathbb{N}$ This is what I did: $$\begin{align*} \int \frac{y'}{y^a}dx&=\int 1dx\\ \frac{y^{-a+1}}{-a+1}+c_1&=x+c_2\\ y^{-a+1}&=(x+C)(-a+1), \end{align*}$$ where $C=c_2-c_1$, and for $a=1$ there's no solution. So I get  $$y=\left(\frac{1}{(x+c)(1-a)}\right)^{a-1},$$  finding $c$ is not pleasant. I assume that something is wrong, Am I suppose leave $y$ in the way that I find it after finding $c$? Thanks a lot!","I'd love your help with finding the function $y$ which satisfies: $y'=y^a$, $y(a)=a-2$, for $a \in \mathbb{N}$ This is what I did: $$\begin{align*} \int \frac{y'}{y^a}dx&=\int 1dx\\ \frac{y^{-a+1}}{-a+1}+c_1&=x+c_2\\ y^{-a+1}&=(x+C)(-a+1), \end{align*}$$ where $C=c_2-c_1$, and for $a=1$ there's no solution. So I get  $$y=\left(\frac{1}{(x+c)(1-a)}\right)^{a-1},$$  finding $c$ is not pleasant. I assume that something is wrong, Am I suppose leave $y$ in the way that I find it after finding $c$? Thanks a lot!",,['ordinary-differential-equations']
51,How long does it take for the new bills to reach 90% of the total number of $20 bills in circulation?,How long does it take for the new bills to reach 90% of the total number of $20 bills in circulation?,,"Suppose that Canada has a total of \$10 billion in \$20 bills in circulation, and each day \$40 million of these \$20 bills passes through one bank or another. A new harder-to-forge version of the \$20 bill is developed, and the banks replace the old bills with new ones whenever they can. How long does it take for the new bills to reach 90% of the total number of $20 bills in circulation?","Suppose that Canada has a total of \$10 billion in \$20 bills in circulation, and each day \$40 million of these \$20 bills passes through one bank or another. A new harder-to-forge version of the \$20 bill is developed, and the banks replace the old bills with new ones whenever they can. How long does it take for the new bills to reach 90% of the total number of $20 bills in circulation?",,['ordinary-differential-equations']
52,Asymptotic Expansion for heat operator $e^{-t\triangle}$,Asymptotic Expansion for heat operator,e^{-t\triangle},"I'm afraid the question below might turn out to be very stupid - I just don't know how to make sense of two asymptotic expansions, given the heat operator $e^{-t\triangle}$ with $\triangle$ a Laplace type operator on smooth functions over an n-dimensional manifold we have an asymptotic expansion of the kernel \begin{equation} \text{k}(e^{-t\triangle}) \backsim \sum_{j \geq 0} c_{\frac{-n - j}{2}} \,t^{\frac{-n - j}{2}} \end{equation} and we also have an asymptotic expansion for the trace, \begin{equation} \text{Tr}(e^{-t\triangle}) \backsim \sum_{j \geq 0} c_{\frac{-n - j}{2}} \,t^{\frac{-n - j}{2}} \end{equation} Now, where is the difference in these two expansions ? I realize this question suggests I don't know enough about both the kernel and the definition of the trace - which is absolutely true. The problem though is I don't know how to find an answer since all sources I have go on a large detour on how to construct the heat kernel, and on general definitions and properties of trace structures on operator algebras. I am totally aware I need to eventually understand these concepts, what I hope for with this question is to get a first idea in order to proceed. If that's not possible I'd also appreciate a reference to literature that starts at a low level so that I can get an overview. Hope this makes sense, thanks a lot for your help!","I'm afraid the question below might turn out to be very stupid - I just don't know how to make sense of two asymptotic expansions, given the heat operator $e^{-t\triangle}$ with $\triangle$ a Laplace type operator on smooth functions over an n-dimensional manifold we have an asymptotic expansion of the kernel \begin{equation} \text{k}(e^{-t\triangle}) \backsim \sum_{j \geq 0} c_{\frac{-n - j}{2}} \,t^{\frac{-n - j}{2}} \end{equation} and we also have an asymptotic expansion for the trace, \begin{equation} \text{Tr}(e^{-t\triangle}) \backsim \sum_{j \geq 0} c_{\frac{-n - j}{2}} \,t^{\frac{-n - j}{2}} \end{equation} Now, where is the difference in these two expansions ? I realize this question suggests I don't know enough about both the kernel and the definition of the trace - which is absolutely true. The problem though is I don't know how to find an answer since all sources I have go on a large detour on how to construct the heat kernel, and on general definitions and properties of trace structures on operator algebras. I am totally aware I need to eventually understand these concepts, what I hope for with this question is to get a first idea in order to proceed. If that's not possible I'd also appreciate a reference to literature that starts at a low level so that I can get an overview. Hope this makes sense, thanks a lot for your help!",,"['ordinary-differential-equations', 'differential-geometry', 'operator-theory']"
53,Applied ODEs in trajectory problem,Applied ODEs in trajectory problem,,"I'm having a hard time solving this problem: Let there be a town $A$ in a shore of a river. Let $x=0$ be the shore. Let $(0,0)$ be the location of the town. Let $B$ be another town, in the oppossite shore, $x=b$, and let the town be in $(b,0)$. Suppose a person from town $A$ goes in a boat with velocity $v$ to $B$, always poiting at $B$ (see the picture), and let the river flow in the positive direction of $y$ with velocity $u$. Find the curve that gives the person's trayectory over time. In an arbitrary point of the curve there will always be a system of three vectors, $v$, $u$ and $w=v+u$ in the following manner, where $w$, the tangent vector, is $u+v$. Their modulus is constant, thus since $v+u$ varies, the variables here are the angles and the modulus. Remember that $v$ always points at $B$. IN the image, the lengths of $v$ and $v_1$ and of $u$ and $u_1$ should be the same (since they're the same vector). You can see that $v$ is ""pushing"" to get to $B$ but the vector $u$ will always modify $v$'s direction (and thus the man's), making the tangent actually be $w = u+v$. The dotted parallel lines are a reference to the tangent angle which is that between the dotted line and $w$. My approach is: $$ \tan \theta = \frac{dy}{dx}$$ and the modulus of $v+u$ will satisfy, being a function of time. $$|| v+u||(t) = \frac{ds}{dt}$$ I just need then to find a way of relating the modulus to the angle to find the solution. Another thing that comes to mind is thinking as the solution in a parametric way, thus first finding $$\frac{dy}{dt} = f(t) $$ and $$\frac{dx}{dt} = g(t) $$ then taking the quotient and integrating.","I'm having a hard time solving this problem: Let there be a town $A$ in a shore of a river. Let $x=0$ be the shore. Let $(0,0)$ be the location of the town. Let $B$ be another town, in the oppossite shore, $x=b$, and let the town be in $(b,0)$. Suppose a person from town $A$ goes in a boat with velocity $v$ to $B$, always poiting at $B$ (see the picture), and let the river flow in the positive direction of $y$ with velocity $u$. Find the curve that gives the person's trayectory over time. In an arbitrary point of the curve there will always be a system of three vectors, $v$, $u$ and $w=v+u$ in the following manner, where $w$, the tangent vector, is $u+v$. Their modulus is constant, thus since $v+u$ varies, the variables here are the angles and the modulus. Remember that $v$ always points at $B$. IN the image, the lengths of $v$ and $v_1$ and of $u$ and $u_1$ should be the same (since they're the same vector). You can see that $v$ is ""pushing"" to get to $B$ but the vector $u$ will always modify $v$'s direction (and thus the man's), making the tangent actually be $w = u+v$. The dotted parallel lines are a reference to the tangent angle which is that between the dotted line and $w$. My approach is: $$ \tan \theta = \frac{dy}{dx}$$ and the modulus of $v+u$ will satisfy, being a function of time. $$|| v+u||(t) = \frac{ds}{dt}$$ I just need then to find a way of relating the modulus to the angle to find the solution. Another thing that comes to mind is thinking as the solution in a parametric way, thus first finding $$\frac{dy}{dt} = f(t) $$ and $$\frac{dx}{dt} = g(t) $$ then taking the quotient and integrating.",,"['ordinary-differential-equations', 'analytic-geometry']"
54,Calculating the Logarithm of a Non-Diagonalizable Matrix,Calculating the Logarithm of a Non-Diagonalizable Matrix,,"So I've worked through a couple examples which were straight forward. Matrices like $A = \left[\begin{array}{cc} 4 & 1\\ 0 & e\end{array}\right]$, were easy because $A$ is diagonalizable. The only non-diagonalizable example we covered in class were of the form $\lambda I + N$, where $N^r = 0$ for some positive integer $r$, then we used the formula $\log(\lambda I + N) = \log(\lambda I) + \sum\limits_{n=1}^{r-1}\frac{(-N)^{n}}{j\lambda^{j}}$. How can you calculate the log if it doesn't fall under one of these two forms?","So I've worked through a couple examples which were straight forward. Matrices like $A = \left[\begin{array}{cc} 4 & 1\\ 0 & e\end{array}\right]$, were easy because $A$ is diagonalizable. The only non-diagonalizable example we covered in class were of the form $\lambda I + N$, where $N^r = 0$ for some positive integer $r$, then we used the formula $\log(\lambda I + N) = \log(\lambda I) + \sum\limits_{n=1}^{r-1}\frac{(-N)^{n}}{j\lambda^{j}}$. How can you calculate the log if it doesn't fall under one of these two forms?",,"['linear-algebra', 'ordinary-differential-equations']"
55,Finite differences of function composition,Finite differences of function composition,,"I'm trying to express the following in finite differences: $$\frac{d}{dx}\left[ A(x)\frac{d\, u(x)}{dx} \right].$$ Let $h$ be the step size and $x_{i-1} = x_i - h$ and $x_{i+ 1} = x_i + h$ If I take centered differences evaluated in $x_i$, I get: $\begin{align*}\left\{\frac{d}{dx}\left[ A(x)\frac{d\, u(x)}{dx}\right]\right\}_i   &= \frac{\left[A(x)\frac{d\, u(x)}{dx}\right]_{i+1/2} - \left[A(x)\frac{d\, u(x)}{dx}\right]_{i-1/2}}{h} \\ &= \frac{A_{i+1/2}\left[\frac{u_{i+1}-u_{i}}{h}\right] - A_{i-1/2}\left[\frac{u_{i}-u_{i-1}}{h}\right]}{h} \end{align*}$ So, if I use centered differences I would have to have values for $A$ at $i + \frac 12$ and $A$ at $i - \frac 12$; however those nodes don't exist (in my stencil I only have $i \pm$ integer nodes); is that correct? If I use forward or backward differences I need A values at $i$, $i + 1$, $i + 2$ and at $i$, $i -1$, $i -2$ respectively. Am I on the correct path? I would really appreciate any hint. Thanks in advance, Federico","I'm trying to express the following in finite differences: $$\frac{d}{dx}\left[ A(x)\frac{d\, u(x)}{dx} \right].$$ Let $h$ be the step size and $x_{i-1} = x_i - h$ and $x_{i+ 1} = x_i + h$ If I take centered differences evaluated in $x_i$, I get: $\begin{align*}\left\{\frac{d}{dx}\left[ A(x)\frac{d\, u(x)}{dx}\right]\right\}_i   &= \frac{\left[A(x)\frac{d\, u(x)}{dx}\right]_{i+1/2} - \left[A(x)\frac{d\, u(x)}{dx}\right]_{i-1/2}}{h} \\ &= \frac{A_{i+1/2}\left[\frac{u_{i+1}-u_{i}}{h}\right] - A_{i-1/2}\left[\frac{u_{i}-u_{i-1}}{h}\right]}{h} \end{align*}$ So, if I use centered differences I would have to have values for $A$ at $i + \frac 12$ and $A$ at $i - \frac 12$; however those nodes don't exist (in my stencil I only have $i \pm$ integer nodes); is that correct? If I use forward or backward differences I need A values at $i$, $i + 1$, $i + 2$ and at $i$, $i -1$, $i -2$ respectively. Am I on the correct path? I would really appreciate any hint. Thanks in advance, Federico",,"['ordinary-differential-equations', 'numerical-methods', 'simulation', 'finite-differences']"
56,Homogeneous(?) Differential equation,Homogeneous(?) Differential equation,,I was looking over some old notes and I found that the differential equation $y' = \frac{x-y}{x+y}$ is supposedly a homogeneous equation...for some reason I'm blanking on how to batter it into a form where it can be solved with the substitution $u = \frac{y}{x}$.  I think I solved it previously by making the substitution $u = x + y$.  Any advice would be appreciated.,I was looking over some old notes and I found that the differential equation $y' = \frac{x-y}{x+y}$ is supposedly a homogeneous equation...for some reason I'm blanking on how to batter it into a form where it can be solved with the substitution $u = \frac{y}{x}$.  I think I solved it previously by making the substitution $u = x + y$.  Any advice would be appreciated.,,['ordinary-differential-equations']
57,"What is the formal definition of $d$, or $\partial$, in differation and integration","What is the formal definition of , or , in differation and integration",d \partial,"This might sound a bit like a silly question, but i'm a second year math student, and so far i've encountered $d$ or $\partial$ in many cases ofcourse (mostly in calculus :)). Those letters or symbols came in many forms, when derivating and integrating, e.g: $\frac{df}{dx},\,\int g\left(x\right)dx,\,\frac{\partial^{2}h}{\partial y^{2}}$ I can work with them, and i'm capable of derivating and integrating with ease. However, whenever i ask a professor about a formal definition, i get an answer of the sort: ""Consider this to be some sort of symbol, we won't define this yet"". Those things must have formal definitions, as some actions can be performed on them, they sometime cancel eachother, etc... So, can anyone give me something with meat? Thanks!","This might sound a bit like a silly question, but i'm a second year math student, and so far i've encountered $d$ or $\partial$ in many cases ofcourse (mostly in calculus :)). Those letters or symbols came in many forms, when derivating and integrating, e.g: $\frac{df}{dx},\,\int g\left(x\right)dx,\,\frac{\partial^{2}h}{\partial y^{2}}$ I can work with them, and i'm capable of derivating and integrating with ease. However, whenever i ask a professor about a formal definition, i get an answer of the sort: ""Consider this to be some sort of symbol, we won't define this yet"". Those things must have formal definitions, as some actions can be performed on them, they sometime cancel eachother, etc... So, can anyone give me something with meat? Thanks!",,"['calculus', 'ordinary-differential-equations']"
58,ODE theory - conditions for differentiating an ODE,ODE theory - conditions for differentiating an ODE,,"I’m trying to find out more about solving ODEs, spurred on by my previous question about solving $(x’(t))^2+a(x(t))^2+b=0$ with say $x(0)=1,x(2)=3$ . In this case differentiating reveals the equation can be reduced to $x’’(t)+2ax(t)=0$ . So my question is can you always apply the derivative to any $x’’(t)=f(t)$ ? Can it introduce solutions that don’t solve the original equation? Is there a way to generalise this for other differential equations? I tried to think about it with an example like $u(x)=\sin(u(x))\implies u’(x)=u’(x)\cos(u(x))\implies \cos(u(x))=0$ so in this case it seems to work. Is there a chapter in a textbook where I can learn more about this?","I’m trying to find out more about solving ODEs, spurred on by my previous question about solving with say . In this case differentiating reveals the equation can be reduced to . So my question is can you always apply the derivative to any ? Can it introduce solutions that don’t solve the original equation? Is there a way to generalise this for other differential equations? I tried to think about it with an example like so in this case it seems to work. Is there a chapter in a textbook where I can learn more about this?","(x’(t))^2+a(x(t))^2+b=0 x(0)=1,x(2)=3 x’’(t)+2ax(t)=0 x’’(t)=f(t) u(x)=\sin(u(x))\implies u’(x)=u’(x)\cos(u(x))\implies \cos(u(x))=0",['ordinary-differential-equations']
59,Can an unstable focus be an $\omega$-limit point?,Can an unstable focus be an -limit point?,\omega,"Consider the ODE $\dot x=F(x)$ with $F:U\to\mathbb R^2$ ( $U\subset \mathbb R^2$ open) a $C^1$ function. Assume we have an equilibrium point $(x_0,y_0)$ ( $F(x_0,y_0)=0$ ) that is an unstable focus (i.e., $(x_0,y_0)$ is an unstable focus in the linearized system). I am wondering if it's possible that $(x_0,y_0)\in\omega(x,y)$ for some $(x,y)\neq (x_0,y_0)$ , where by $\omega(x,y)$ I mean the $\omega$ -limit set. Phrased differently: Is the stable manifold of $(x_0,y_0)$ equal to $\{(x_0,y_0)\}$ ? My intuition (to the rephrased question) says yes: close to $(x_0,y_0)$ the solution behaves as a solution for the linearized system, and in the linear case we know the solution has to spiral outwards. Is my intuition correct? If so, how to prove this?","Consider the ODE with ( open) a function. Assume we have an equilibrium point ( ) that is an unstable focus (i.e., is an unstable focus in the linearized system). I am wondering if it's possible that for some , where by I mean the -limit set. Phrased differently: Is the stable manifold of equal to ? My intuition (to the rephrased question) says yes: close to the solution behaves as a solution for the linearized system, and in the linear case we know the solution has to spiral outwards. Is my intuition correct? If so, how to prove this?","\dot x=F(x) F:U\to\mathbb R^2 U\subset \mathbb R^2 C^1 (x_0,y_0) F(x_0,y_0)=0 (x_0,y_0) (x_0,y_0)\in\omega(x,y) (x,y)\neq (x_0,y_0) \omega(x,y) \omega (x_0,y_0) \{(x_0,y_0)\} (x_0,y_0)",['ordinary-differential-equations']
60,"I have been integrating differential equations for many, many years. How should I justify a certain step in the process though?","I have been integrating differential equations for many, many years. How should I justify a certain step in the process though?",,"Someone with plenty of experience of integrating differential equations, if faced with the simple first order equation \begin{equation*} \frac{dy}{dx}= x \tag{1} \end{equation*} Might, almost without thinking, write down something like \begin{equation*} \int \left\{\frac{dy}{dx}\right\} dx  =  \int x dx \tag{2} \end{equation*} \begin{equation*} y(x)+d=\frac{x^2}{2}+k  \tag{3} \end{equation*} \begin{equation*} y(x)  = \frac{x^2}{2}+ (~k-d~) \end{equation*} ( some thought may have gone into choosing to use ‘ $d$ ’ on the RHS of $(3)$ , rather than ‘ $c$ ’ ) They would find \begin{equation*} y(x)  = \frac{x^2}{2}+ c~~~~~~~~~\text{where} ~~c=~k-d \end{equation*} The solution can be completed by having some extra information about $y(x)$ , say $y(0)=a$ . From information like this, the value of ‘ $c$ ’ may be determined. My question is: How can getting from (2) to (3) , be justified?","Someone with plenty of experience of integrating differential equations, if faced with the simple first order equation Might, almost without thinking, write down something like ( some thought may have gone into choosing to use ‘ ’ on the RHS of , rather than ‘ ’ ) They would find The solution can be completed by having some extra information about , say . From information like this, the value of ‘ ’ may be determined. My question is: How can getting from (2) to (3) , be justified?","\begin{equation*}
\frac{dy}{dx}= x \tag{1}
\end{equation*} \begin{equation*}
\int \left\{\frac{dy}{dx}\right\} dx  =  \int x dx \tag{2}
\end{equation*} \begin{equation*}
y(x)+d=\frac{x^2}{2}+k  \tag{3}
\end{equation*} \begin{equation*}
y(x)  = \frac{x^2}{2}+ (~k-d~)
\end{equation*} d (3) c \begin{equation*}
y(x)  = \frac{x^2}{2}+ c~~~~~~~~~\text{where} ~~c=~k-d
\end{equation*} y(x) y(0)=a c","['integration', 'ordinary-differential-equations']"
61,Area under two curves and between two curves are equal.,Area under two curves and between two curves are equal.,,"In the accompanying figure, y=f(x) is the graph of a one to one continuous function f. At each point P on the graph of y=2x^2, assume that the areas OAP and OBP are equal. Here PA, PB are the horizontal and vertical segments. Determine the function f. Now my approach is this: First area under $y=2x^2$ and $y=x^2$ at a point x is $x^3/3$ . Now we need area $OAP$ . If we rotate the axes by π/2 radians and reflect the rotated graph about the y axis, we should still preserve the area of the region OAP. So, first we rotate the axes by π/2 radians and get the equation of $y=2x^2$ and $y=f(x)$ in the rotated system as $y=\sqrt{\frac{-x}{2}}$ and $y=f^{-1}(-x)$ Now to reflect the graph about the y-axis, we replace x with -x to get the transformed equations as $y=\sqrt{\frac{x}{2}}$ ---(1) and $y=f^{-1}(x)$ ---(2) Now we need to find the area under the two curves (1) and (2) from 0 to f(x). So we have Area $OAP = \int_{t=0}^{f(x)}{\sqrt{\frac{t}{2}}-f^{-1}(t) dt}$ Since Area $OAP = x^3/3$ , upon differentiating we have $\sqrt{\frac{f(x)}{2}} - (x-f^{-1}(0)) = x^2$ . Thus we have $f(x)= 2(x^2+x-f^{-1}(0))^2$ And from the figure, and the fact that area OPB tends to 0 as x tends to 0, $f^{-1}(0)=0$ Im unable to find the mistake here, and would be grateful to anyone that could point it out.","In the accompanying figure, y=f(x) is the graph of a one to one continuous function f. At each point P on the graph of y=2x^2, assume that the areas OAP and OBP are equal. Here PA, PB are the horizontal and vertical segments. Determine the function f. Now my approach is this: First area under and at a point x is . Now we need area . If we rotate the axes by π/2 radians and reflect the rotated graph about the y axis, we should still preserve the area of the region OAP. So, first we rotate the axes by π/2 radians and get the equation of and in the rotated system as and Now to reflect the graph about the y-axis, we replace x with -x to get the transformed equations as ---(1) and ---(2) Now we need to find the area under the two curves (1) and (2) from 0 to f(x). So we have Area Since Area , upon differentiating we have . Thus we have And from the figure, and the fact that area OPB tends to 0 as x tends to 0, Im unable to find the mistake here, and would be grateful to anyone that could point it out.",y=2x^2 y=x^2 x^3/3 OAP y=2x^2 y=f(x) y=\sqrt{\frac{-x}{2}} y=f^{-1}(-x) y=\sqrt{\frac{x}{2}} y=f^{-1}(x) OAP = \int_{t=0}^{f(x)}{\sqrt{\frac{t}{2}}-f^{-1}(t) dt} OAP = x^3/3 \sqrt{\frac{f(x)}{2}} - (x-f^{-1}(0)) = x^2 f(x)= 2(x^2+x-f^{-1}(0))^2 f^{-1}(0)=0,"['calculus', 'integration', 'ordinary-differential-equations', 'area']"
62,Exactly one homogeneous differential equation of second order to given fundamental solution,Exactly one homogeneous differential equation of second order to given fundamental solution,,"I am working on: Let $\phi_1,\phi_2$ , so that $\phi_1(x)\phi_2'(x)-\phi_1'(x)\phi_2(x)\neq 0.$ for all $x\in\mathbb{R}$ . Then there exists exactly on homogeneous differential equation of second order $$y''(x)=f(x)y'(x)+g(x)y(x)$$ so that the functions $\phi_1,\phi_2$ are fundamental solutions. My idea is to suppose that there are two homogeneous ODE of second order. Is there a possibility to combine the terms, so I can use $W(x)\neq 0$ ? Thanks for your answers!","I am working on: Let , so that for all . Then there exists exactly on homogeneous differential equation of second order so that the functions are fundamental solutions. My idea is to suppose that there are two homogeneous ODE of second order. Is there a possibility to combine the terms, so I can use ? Thanks for your answers!","\phi_1,\phi_2 \phi_1(x)\phi_2'(x)-\phi_1'(x)\phi_2(x)\neq 0. x\in\mathbb{R} y''(x)=f(x)y'(x)+g(x)y(x) \phi_1,\phi_2 W(x)\neq 0","['ordinary-differential-equations', 'wronskian', 'fundamental-solution']"
63,How to solve the first order ODE,How to solve the first order ODE,,"I have a differential equation: $$y(x)\frac{dy(x)}{dx}+nxy(x)=C,$$ where $n$ and $C$ are constants. Is there a way to get $y(x)$ ?",I have a differential equation: where and are constants. Is there a way to get ?,"y(x)\frac{dy(x)}{dx}+nxy(x)=C, n C y(x)","['ordinary-differential-equations', 'nonlinear-system']"
64,Simplifying arctan expression using Puiseux series,Simplifying arctan expression using Puiseux series,,"Mathematica gives me the following expression which works well for $s$ near $0$ , any idea how to derive this manually? $$\left(\frac{s}{2}+\frac{\sqrt{2} \sqrt{s}}{\pi -2 \tan ^{-1}\left(\frac{\sqrt{s}}{\sqrt{2}}\right)}\right)^{-1} \approx \frac{\pi }{\sqrt{2} \sqrt{s}}$$ expr = 1/(s/2 + (Sqrt[2] Sqrt[s])/(\[Pi] - 2 ArcTan[Sqrt[s]/Sqrt[2]])); asymp = Asymptotic[expr, s -> 0] This appears to be the first term in Series expansion below. Using $x=2s^2$ replacement, the series looks like below $$ \begin{array}{ccc}   & \text{order} & \text{expr} \\   & 0 & \frac{\pi }{2 x}-\frac{\pi ^2}{4}-1 \\   & 1 & \frac{1}{8} \left(8 \pi +\pi ^3\right) x+\frac{\pi }{2 x}-\frac{\pi ^2}{4}-1 \\   & 2 & \frac{1}{48} \left(-32-36 \pi ^2-3 \pi ^4\right) x^2+\frac{1}{8} \left(8 \pi +\pi ^3\right) x+\frac{\pi }{2 x}-\frac{\pi ^2}{4}-1 \\ \end{array}$$ Notebook","Mathematica gives me the following expression which works well for near , any idea how to derive this manually? expr = 1/(s/2 + (Sqrt[2] Sqrt[s])/(\[Pi] - 2 ArcTan[Sqrt[s]/Sqrt[2]])); asymp = Asymptotic[expr, s -> 0] This appears to be the first term in Series expansion below. Using replacement, the series looks like below Notebook","s 0 \left(\frac{s}{2}+\frac{\sqrt{2} \sqrt{s}}{\pi -2 \tan ^{-1}\left(\frac{\sqrt{s}}{\sqrt{2}}\right)}\right)^{-1} \approx \frac{\pi }{\sqrt{2} \sqrt{s}} x=2s^2 
\begin{array}{ccc}
  & \text{order} & \text{expr} \\
  & 0 & \frac{\pi }{2 x}-\frac{\pi ^2}{4}-1 \\
  & 1 & \frac{1}{8} \left(8 \pi +\pi ^3\right) x+\frac{\pi }{2 x}-\frac{\pi ^2}{4}-1 \\
  & 2 & \frac{1}{48} \left(-32-36 \pi ^2-3 \pi ^4\right) x^2+\frac{1}{8} \left(8 \pi +\pi ^3\right) x+\frac{\pi }{2 x}-\frac{\pi ^2}{4}-1 \\
\end{array}","['calculus', 'ordinary-differential-equations', 'laplace-transform']"
65,Calculus involved in math-modelling,Calculus involved in math-modelling,,"Background: I am trying to model my arm as a spring with constant b. As the arm is on the door it compresses and it accelerates the door and decelerates me. I am trying to solve for door angle theta as a function of time, from the moment when the arm touches the door to the moment the arm is fully compressed. The compression is expressed as the difference in tangential velocity of the door at contact point and the velocity of the man. I have worked through the derivation of the formulas but came up with an error function at the end which I think is not reasonable. I may have done this wrong, so please please help if you have any thoughts. I really need your help. the derivation is as follows: At $t=0$ , Arm just touches the door and being compressed at a rate of $X(t)=\Delta x=(u_{man}-u_{door})t$ Where $u_{door}$ is the tangential velocity of the door at the current position $r$ , which can be expressed as $r=\frac{r_0}{\cos\theta}$ . From rotational kinematics, $u_{door}\cos\theta=\dot{\theta}r \Rightarrow u_{door}(t)=r_0\frac{\dot{\theta}(t)}{\cos^2{\theta(t)}}$ . For the man, he experiences a deceleration due to compression of the spring, $F_{spring}=bX=ma \Rightarrow a(t)=\frac{b}{m}X(t)$ . Hence, $u_{man}=u_0-\frac{b}{m}\int X(t)dt$ . This gives: $X(t)=(u_0-r_0\frac{\dot{\theta}(t)}{\cos^2\theta(t)} -\frac{b}{m}\int X(t)dt)t$ Rearrange and substitute $g(\dot{\theta},\theta)=u_0-r_0\frac{\dot{\theta}(t)}{\cos^2\theta(t)}$ : $t^{-1}\cdot X(t)=g(\dot{\theta},\theta)  -\frac{b}{m} \int X(t)dt$ $\frac{d}{dt}(t^{-1}\cdot X(t))=\frac{d}{dt} (g(\dot{\theta},\theta)  -\frac{b}{m} \int X(t)dt)$ $-t^{-2} X(t)+t^{-1} X'(t)=\frac{d}{dt} (g(\dot{\theta},\theta))-\frac{b}{m} X(t)$ Rearrange: $\frac{1}{t} X'(t)+(\frac{b}{m}-\frac{1}{t^2})X(t)=t\frac{d}{dt} (g(\dot{\theta},\theta))$ $X'(t)+(\frac{bt}{m}-\frac{1}{t})X(t)=t\frac{d}{dt}(g(\dot{\theta},\theta))$ Denote function $f$ , such that $f'(t)=(\frac{bt}{m}-\frac{1}{t})$ $X'(t)+f'(t)X(t)=t\frac{d}{dt}(g(\dot{\theta},\theta))$ Multiply both sides by $e^{f(t)}$ : $e^{f(t)}\cdot X'(t)+e^{f(t)}\cdot f'(t)\cdot X(t)=e^{f(t)}\cdot t\cdot \frac{d}{dt} (g(\dot{\theta},\theta))$ LHS can be rewritten as $e^{f(t)} X'(t)+e^{f(t)} f'(t)X(t)=\frac{d}{dt} (e^{f(t)} X(t))$ Hence, $\frac{d}{dt} (e^{f(t)} X(t))=t e^{f(t)}\frac{d}{dt} (g(\dot{\theta},\theta))$ $e^{f(t)} X(t)=\int t e^{f(t)} dt\cdot (g(\dot{\theta},\theta))$ from here if you substitute them in, you will find an integral of (exp(2t^2/bm))dt which is non solvable.","Background: I am trying to model my arm as a spring with constant b. As the arm is on the door it compresses and it accelerates the door and decelerates me. I am trying to solve for door angle theta as a function of time, from the moment when the arm touches the door to the moment the arm is fully compressed. The compression is expressed as the difference in tangential velocity of the door at contact point and the velocity of the man. I have worked through the derivation of the formulas but came up with an error function at the end which I think is not reasonable. I may have done this wrong, so please please help if you have any thoughts. I really need your help. the derivation is as follows: At , Arm just touches the door and being compressed at a rate of Where is the tangential velocity of the door at the current position , which can be expressed as . From rotational kinematics, . For the man, he experiences a deceleration due to compression of the spring, . Hence, . This gives: Rearrange and substitute : Rearrange: Denote function , such that Multiply both sides by : LHS can be rewritten as Hence, from here if you substitute them in, you will find an integral of (exp(2t^2/bm))dt which is non solvable.","t=0 X(t)=\Delta x=(u_{man}-u_{door})t u_{door} r r=\frac{r_0}{\cos\theta} u_{door}\cos\theta=\dot{\theta}r \Rightarrow u_{door}(t)=r_0\frac{\dot{\theta}(t)}{\cos^2{\theta(t)}} F_{spring}=bX=ma \Rightarrow a(t)=\frac{b}{m}X(t) u_{man}=u_0-\frac{b}{m}\int X(t)dt X(t)=(u_0-r_0\frac{\dot{\theta}(t)}{\cos^2\theta(t)} -\frac{b}{m}\int X(t)dt)t g(\dot{\theta},\theta)=u_0-r_0\frac{\dot{\theta}(t)}{\cos^2\theta(t)} t^{-1}\cdot X(t)=g(\dot{\theta},\theta)  -\frac{b}{m} \int X(t)dt \frac{d}{dt}(t^{-1}\cdot X(t))=\frac{d}{dt} (g(\dot{\theta},\theta)  -\frac{b}{m} \int X(t)dt) -t^{-2} X(t)+t^{-1} X'(t)=\frac{d}{dt} (g(\dot{\theta},\theta))-\frac{b}{m} X(t) \frac{1}{t} X'(t)+(\frac{b}{m}-\frac{1}{t^2})X(t)=t\frac{d}{dt} (g(\dot{\theta},\theta)) X'(t)+(\frac{bt}{m}-\frac{1}{t})X(t)=t\frac{d}{dt}(g(\dot{\theta},\theta)) f f'(t)=(\frac{bt}{m}-\frac{1}{t}) X'(t)+f'(t)X(t)=t\frac{d}{dt}(g(\dot{\theta},\theta)) e^{f(t)} e^{f(t)}\cdot X'(t)+e^{f(t)}\cdot f'(t)\cdot X(t)=e^{f(t)}\cdot t\cdot \frac{d}{dt} (g(\dot{\theta},\theta)) e^{f(t)} X'(t)+e^{f(t)} f'(t)X(t)=\frac{d}{dt} (e^{f(t)} X(t)) \frac{d}{dt} (e^{f(t)} X(t))=t e^{f(t)}\frac{d}{dt} (g(\dot{\theta},\theta)) e^{f(t)} X(t)=\int t e^{f(t)} dt\cdot (g(\dot{\theta},\theta))","['calculus', 'integration', 'ordinary-differential-equations', 'multivariable-calculus', 'mathematical-modeling']"
66,An inequality considering linear ODE,An inequality considering linear ODE,,"Let $p(t),q(t)$ , and $r(t)$ be continuous functions on an open interval $I$ and let $t_0\in I$ . Assume that there exists a positive constant $M$ such that $$|p(t)|+|q(t)|+|r(t)|<M$$ for all $t\in I$ . Let $f$ be a three times differentiable function which is a solution of a differential equation $$f'''(t) + p(t)f''(t) + q(t)f'(t) + r(t)f(t) = 0$$ on $I$ . Finally, define $$\psi(t) := |f(t)|^2 + |f'(t)|^2 + |f''(t)|^2$$ I want to show that (1) $|\psi'(t)|\leq 2(1+M)\psi(t)$ (2) $\psi(t_0)e^{-2(1+M)|t-t_0|} \leq \psi(t) \leq \psi(t_0)e^{2(1+M)|t-t_0|}$ If the inequality in (1) is shown, (2) can be proven using an integrating factor. But I cannot figure out how to prove (1). $$\psi'(t) = 2f(t)f'(t) + 2f'(t)f''(t) + 2f''(t)f'''(t)$$ The form of $\psi'(t)$ seems to require the Cauchy-Schwarz inequality to prove the inquality, but where to use the bounds of $p,q,r$ ? Another try was to substitute $-pf'' - qf' - rf$ in the place of $f'''$ , but I have no idea how to relate its upper bound with the function $\psi$ itself. Thanks in advance for any form of help, hint, or solution!","Let , and be continuous functions on an open interval and let . Assume that there exists a positive constant such that for all . Let be a three times differentiable function which is a solution of a differential equation on . Finally, define I want to show that (1) (2) If the inequality in (1) is shown, (2) can be proven using an integrating factor. But I cannot figure out how to prove (1). The form of seems to require the Cauchy-Schwarz inequality to prove the inquality, but where to use the bounds of ? Another try was to substitute in the place of , but I have no idea how to relate its upper bound with the function itself. Thanks in advance for any form of help, hint, or solution!","p(t),q(t) r(t) I t_0\in I M |p(t)|+|q(t)|+|r(t)|<M t\in I f f'''(t) + p(t)f''(t) + q(t)f'(t) + r(t)f(t) = 0 I \psi(t) := |f(t)|^2 + |f'(t)|^2 + |f''(t)|^2 |\psi'(t)|\leq 2(1+M)\psi(t) \psi(t_0)e^{-2(1+M)|t-t_0|} \leq \psi(t) \leq \psi(t_0)e^{2(1+M)|t-t_0|} \psi'(t) = 2f(t)f'(t) + 2f'(t)f''(t) + 2f''(t)f'''(t) \psi'(t) p,q,r -pf'' - qf' - rf f''' \psi","['real-analysis', 'ordinary-differential-equations', 'inequality', 'functional-inequalities']"
67,"How do I prove that the BVP $y'' = y\sin\left(x\right), y\left(0\right) = a, y\left(1\right) = b$ has a unique solution?",How do I prove that the BVP  has a unique solution?,"y'' = y\sin\left(x\right), y\left(0\right) = a, y\left(1\right) = b","I want to prove that the BVP $$ y'' = y\sin\left(x\right), y\left(0\right) = a, y\left(1\right) = b $$ has a unique solution. I know how to prove uniqueness: Suppose there are two different solutions $y_1,y_2$ . We look at $z= y_1 - y_2$ . $z\left(0\right) = z\left(1\right) = 0$ . So by Rolle's theorem $z$ has a positive maximum or a negative minimum. W.L.O.G $z$ has a positive maximum at point $x_0$ . Then on one side $z''\left(x_0\right) < 0$ . On the other side $z''\left(x_0\right) = z\sin\left(x_0\right) > 0$ . Now I need to prove existence of a solution. I think that I need to start by looking at the IVP $$ {y_q}'' = {y_q}\sin\left(x\right), {y_q}\left(0\right) = a, {y_q}'\left(0\right) = q $$ I know that it has a solution, so I need to show that for a big q, ${y_q} > b$ and for a small q, $y_q < b$ but I don't know how to prove that.","I want to prove that the BVP has a unique solution. I know how to prove uniqueness: Suppose there are two different solutions . We look at . . So by Rolle's theorem has a positive maximum or a negative minimum. W.L.O.G has a positive maximum at point . Then on one side . On the other side . Now I need to prove existence of a solution. I think that I need to start by looking at the IVP I know that it has a solution, so I need to show that for a big q, and for a small q, but I don't know how to prove that."," y'' = y\sin\left(x\right), y\left(0\right) = a, y\left(1\right) = b  y_1,y_2 z= y_1 - y_2 z\left(0\right) = z\left(1\right) = 0 z z x_0 z''\left(x_0\right) < 0 z''\left(x_0\right) = z\sin\left(x_0\right) > 0  {y_q}'' = {y_q}\sin\left(x\right), {y_q}\left(0\right) = a, {y_q}'\left(0\right) = q  {y_q} > b y_q < b","['ordinary-differential-equations', 'boundary-value-problem']"
68,ODE: a specific question,ODE: a specific question,,"Determine the  equation of the curve for which the $y$ intercept of the normal drawn to a point on the curve is equal to the distance of that point from the origin . My attempt : Consider an arbitrary point, $(x,y)$ , whose slope is $\frac{dy}{dx} $ . Thus the slope of the normal is $\frac{-dx}{dy}$ . Using the $y$ intercept form of a line $$y= \frac{-dx}{dy}x+ \sqrt{x^2 +y^2}$$ Now I have tried solving this using substitutions,( $y=ux$ , and $x=uy$ ), but that didn't work. To use the method of the integrating factor, Unless I'm mistaken, I'll need a subsitution to do that, but I can't seem to find any appropriate substitutions. Thanks for the help. This problem is problem 54, in chapter 1, in the second volume of N.piskunov's differential and integral calculus. There is a solution, but that does absolutely nothing to explain how to solve this The solution they have given $y+\frac{x}{y'}= \sqrt{x^2 +y^2}$ Whence $x^2=C(2y+C)$ Which is what I'm unable to understand TL;DR How did $y+\frac{x}{y'}= \sqrt{x^2 +y^2}$ Result in $x^2=C(2y+C)$","Determine the  equation of the curve for which the intercept of the normal drawn to a point on the curve is equal to the distance of that point from the origin . My attempt : Consider an arbitrary point, , whose slope is . Thus the slope of the normal is . Using the intercept form of a line Now I have tried solving this using substitutions,( , and ), but that didn't work. To use the method of the integrating factor, Unless I'm mistaken, I'll need a subsitution to do that, but I can't seem to find any appropriate substitutions. Thanks for the help. This problem is problem 54, in chapter 1, in the second volume of N.piskunov's differential and integral calculus. There is a solution, but that does absolutely nothing to explain how to solve this The solution they have given Whence Which is what I'm unable to understand TL;DR How did Result in","y (x,y) \frac{dy}{dx}  \frac{-dx}{dy} y y= \frac{-dx}{dy}x+ \sqrt{x^2 +y^2} y=ux x=uy y+\frac{x}{y'}= \sqrt{x^2 +y^2} x^2=C(2y+C) y+\frac{x}{y'}= \sqrt{x^2 +y^2} x^2=C(2y+C)","['calculus', 'integration', 'ordinary-differential-equations']"
69,ODE theory question,ODE theory question,,"I would like to get a hint for this question: ""let $p(x)$ , $q(x)$ , be continuous functions at $\mathbb{R}$ , and the ODE $y''+p(x)y'+q(x)y=0 $ . Prove that if $y_1(x) \neq 0$ , $y_2(x)$ solve the ODE, so $h(x) =\frac{y_2(x)}{y_1(x)}$ is a strictly monotone function or a constant function."" I didn't write my attempts because I barely know how to begin with this question. Thanks.","I would like to get a hint for this question: ""let , , be continuous functions at , and the ODE . Prove that if , solve the ODE, so is a strictly monotone function or a constant function."" I didn't write my attempts because I barely know how to begin with this question. Thanks.",p(x) q(x) \mathbb{R} y''+p(x)y'+q(x)y=0  y_1(x) \neq 0 y_2(x) h(x) =\frac{y_2(x)}{y_1(x)},['ordinary-differential-equations']
70,Obtaining the formula of Bessel functions of the second kind $Y_{n}(x)$ as a series,Obtaining the formula of Bessel functions of the second kind  as a series,Y_{n}(x),"I am reading a textbook about engineering mathematics. To solve the Bessel differential equation $$ x^2 y'' + xy' + (x^2 - \nu^2) y = 0 $$ the book first obtains the function $J_{n}(x)$ using the series method as follows: $$ J_n(x) = x^n \sum_{m=0}^{\infty}  \frac{(-1)^m x^{2m}}{2^{2m+n} m! (n+m)!} $$ then obtains the Bessel function of the second kind of zero order $Y_{0}(x)$ using the series method as follows: $$ Y_{0}(x) = \frac{2}{\pi}  \left[ J_0(x) \left( \ln\frac{x}{2} + \gamma \right)  + \sum_{m=1}^{\infty}  \frac{(-1)^{m-1} h_m}{2^{2m}(m!)^2} x^{2m} \right] $$ in which $$ h_m = 1 + \frac12 + \cdots + \frac1m $$ and $\gamma \simeq 0.5772$ is the so-called ''Euler constant'', which is defined as the limit of $$ 1 + \frac12 + \cdots + \frac1s - \ln s $$ as $s$ approaches infinity. Then the author states that the Bessel function of the second kind of order $n$ is obtained in a similar way as: \begin{align} Y_{n}(x) &= \frac{2}{\pi} J_{n}(x)  \left(\ln\frac{x}{2} + \gamma \right) \\[2pt] &\quad{} + \frac{x^n}{\pi} \sum_{m=0}^{\infty}  \frac{(-1)^{m-1}(h_{m} + h_{m+n})}{2^{2m+n} m! (m+n)!} x^{2m} \\ &\quad{} - \frac{x^{-n}}{\pi} \sum_{m=0}^{n-1}  \frac{(n-m-1)!}{2^{2m-n} m!} x^{2m} \end{align} The book does not provide any proof for the latter formula. Can you explain me how to get it? The mentioned book is ""ADVANCED ENGINEERING MATHEMATICS"" by ""ERWIN KREYSZIG"". If you know a textbook that explains how to get this formula well, please introduce it to me Edit: I found a solution myself, but I'm not sure if the details are correct. here is the solution: According to the Frobenius method, a second solution is obtained from this formula: $${\it y}_{{\rm 2}}{\rm (}{\it x}{\rm )}={\it kJ}_{{\it n}}{\rm (}{\it x}{\rm )}{\it Lnx}+{\it x}^{{\rm -}{\it n}}\sum\limits_{{\it m}={\rm 0}}^{\infty}{{\it a}_{{\it m}}{\it x}^{{\it m}}}$$ By differentiating this function twice, we get $y{_{2}}^{\prime}$ and $y{_{2}}^{\prime\prime}$ $$y{_{2}}^{\prime}(x)=kJ{_{n}}^{\prime}(x)Lnx+{{KJ_{n}(x)}\over{x}}+\sum\limits_{m=0}^{\infty}{(m-n)a_{m}x^{m-n-1}}$$ $$y{_{2}}^{\prime\prime}(x)=kJ{_{n}}^{\prime\prime}(x)Lnx+{{2KJ{_{n}}^{\prime}(x)}\over{x}}-{{KJ_{n}(x)}\over{x^{2}}}+\sum\limits_{m=0}^{\infty}{(m-n)(m-n-1)a_{m}x^{m-n-2}}$$ Substituting ${\it y}_{{\rm 2}}{\rm (}{\it x}{\rm )}$ , $y{_{2}}^{\prime}(x)$ , and $y{_{2}}^{\prime\prime}(x)$ into Bessel equation $x^{2}y''+xy'+(x^{2}-\upsilon^{2})y=0$ , we get $$kx^{2}J{_{n}}^{\prime}(x)Ln(x)+2kxJ{_{n}}^{\prime}(x)+kxJ{_{n}}^{\prime}(x)Ln(x)+k(x^{2}-n^{2})J_{n}(x)Ln(x)$$ $$+\sum\limits_{{\it m}={\rm 0}}^{\infty}{{\rm (}{\it m}-{\it n}{\rm )(}{\it m}-{\it n}-{\rm 1}{\rm )}{\it a}_{{\it m}}{\it x}^{{\it m}{\rm -}{\it n}}}+\sum\limits_{{\it m}{\rm =0}}^{{\rm \infty}}{{\rm (}{\it m}-{\it n}{\rm )}}{\it a}_{{\it m}}{\it x}^{{\it m}{\rm -}{\it n}}+\sum\limits_{{\it m}={\rm 0}}^{\infty}{{\it a}_{{\it m}}}{\it x}^{{\it m}{\rm -}{\it n}{\rm +2}}-{\it n}^{{\rm 2}}\sum\limits_{{\it m}={\rm 0}}^{\infty}{{\it a}_{{\it m}}}{\it x}^{{\it m}{\rm -}{\it n}}={\rm 0}$$ $$\Rightarrow 2kxJ{_{n}}^{\prime}(x)+\sum\limits_{m=0}^{\infty}{\left[{(m-n)^{2}-n^{2}}\right]}a_{m}x^{m-n}+\sum\limits_{m=0}^{\infty}{a_{m}}x^{m-n+2}=0$$ or $$\Rightarrow 2kxJ{_{n}}^{\prime}(x)+\sum\limits_{m=0}^{\infty}{m(m-2n)}a_{m}x^{m-n}+\sum\limits_{m=0}^{\infty}{a_{m}}x^{m-n+2}=0\ \ \ \ \ \ (1)$$ find the derivative of ${\it J}_{{\it n}}{\rm (}{\it x}{\rm )}$ $$J_{n}(x)=\sum\limits_{m=0}^{\infty}{{{(-1)^{m}}\over{2^{2m+n}m!(m+n)!}}}x^{2m+n}\Rightarrow J{_{n}}^{\prime}(x)=\sum\limits_{m=0}^{\infty}{{{(-1)^{m}(2m+n)}\over{2^{2m+n}m!(m+n)!}}}x^{2m+n-1}$$ Substituting in $(1)$ we have: $${\rm 2}{\it k}\sum\limits_{{\it m}{\rm =0}}^{{\rm \infty}}{{{{\rm (}-{\rm 1}{\rm )}^{{\it m}}{\rm (}{\rm 2}{\it m}+{\it n}{\rm )}}\over{{\rm 2}^{{\rm 2}{\it m}{\rm +}{\it n}}{\it m}{\rm !(}{\it m}+{\it n}{\rm )!}}}}{\it x}^{{\rm 2}{\it m}{\rm +}{\it n}}+\sum\limits_{{\it m}={\rm 0}}^{\infty}{{\it m}{\rm (}{\it m}-{\rm 2}{\it n}{\rm )}{\rm a}_{{\it m}}{\it x}^{{\it m}{\rm -}{\it n}}}{\rm  }+\sum\limits_{{\it m}={\rm 0}}^{\infty}{{\it a}_{{\it m}}}{\it x}^{{\it m}{\rm -}{\it n}{\rm +2}}={\rm 0}$$ For the coefficient of $x^{-n}$ : $0a_{0}=0$ so $a_{0}$ is an arbitrary constant For the coefficient of $x^{1-n}$ : $1(1-2n)a_{1}=0\Rightarrow a_{1}=0$ And in general for the coefficient of $x^{p-n+2}$ $(p\leq 2n-3)$ : $${\rm (}{\it p}+{\rm 2}{\rm )(}{\it p}+{\rm 2}-{\rm 2}{\it n}{\rm )}{\it a}_{{\it p}{\rm +2}}+{\it a}_{{\it p}}={\rm 0}\Rightarrow{\it a}_{{\it p}{\rm +2}}=-{{{\it a}_{{\it p}}}\over{{\rm (}{\it p}+{\rm 2}{\rm )(}{\it p}+{\rm 2}-{\rm 2}{\it n}{\rm )}}}$$ And according to this recursive relation, we have: $${\it a}_{{\rm 3}}={\it a}_{{\rm 5}}=\cdots={\it a}_{{\rm 2}{\it n}{\rm -1}}={\rm 0}$$ That is, all the odd coefficients up to ${\it a}_{{\rm 2}{\it n}{\rm -1}}$ are equal to zero, but this result is also true for the odd coefficients greater than ${\it a}_{{\rm 2}{\it n}{\rm -1}}$ , because if the power of $x$ is $2k+n$ , ${\it m}$ is $2n+2k$ in the second sigma and $2n+2k-2$ in the third sigma, that is, the even coefficients of ${\it a}$ are produced So the equation $${\it a}_{{\it p}{\rm +2}}=-{{{\it a}_{{\it p}}}\over{{\rm (}{\it p}+{\rm 2}{\rm )(}{\it p}+{\rm 2}-{\rm 2}{\it n}{\rm )}}}$$ holds for all odd coefficients and we can conclude that all odd coefficients are equal to zero and we only calculate even coefficients. $${\it a}_{{\rm 2}}=-{{{\it a}_{{\rm 0}}}\over{{\rm 2}{\rm (}{\rm 2}-{\rm 2}{\it n}{\rm )}}}={{{\it a}_{{\rm 0}}}\over{{\rm 2}{\rm (}{\rm 2}{\it n}-{\rm 2}{\rm )}}}$$ $${\it a}_{{\rm 4}}=-{{{\it a}_{{\rm 2}}}\over{{\rm 4}{\rm (}{\rm 4}-{\rm 2}{\it n}{\rm )}}}={{{\it a}_{{\rm 2}}}\over{{\rm 4}{\rm (}{\rm 2}{\it n}-{\rm 4}{\rm )}}}={{{\it a}_{{\rm 0}}}\over{{\rm 4}\times{\rm 2}{\rm (}{\rm 2}{\it n}-{\rm 2}{\rm )(}{\rm 2}{\it n}-{\rm 4}{\rm )}}}$$ $${\it a}_{{\rm 6}}=-{{{\it a}_{{\rm 4}}}\over{{\rm 6}{\rm (}{\rm 6}-{\rm 2}{\it n}{\rm )}}}={{{\it a}_{{\rm 4}}}\over{{\rm 6}{\rm (}{\rm 2}{\it n}-{\rm 6}{\rm )}}}={{{\it a}_{{\rm 0}}}\over{{\rm 6}\times{\rm 4}\times{\rm 2}{\rm (}{\rm 2}{\it n}-{\rm 2}{\rm )(}{\rm 2}{\it n}-{\rm 4}{\rm )(}{\rm 2}{\it n}-{\rm 6}{\rm )}}}$$ and in general for $k\leq n-1$ : $${\it a}_{{\rm 2}{\it k}}={{{\it a}_{{\rm 0}}}\over{{\rm 2}{\it k}{\rm (}{\rm 2}{\it k}-{\rm 2}{\rm )}\cdots\times{\rm 4}\times{\rm 2}{\rm (}{\rm 2}{\it n}-{\rm 2}{\rm )(}{\rm 2}{\it n}-{\rm 4}{\rm )}\cdots{\rm (}{\rm 2}{\it n}-{\rm 2}{\it k}{\rm )}}}={{{\it a}_{{\rm 0}}}\over{{\rm 2}^{{\it k}}\times{\it k}{\rm !}\times{\rm 2}^{{\it k}}{\rm (}{\it n}-{\rm 1}{\rm )}\cdots{\rm (}{\it n}-{\it k}{\rm )}}}$$ $$={{{\it a}_{{\rm 0}}}\over{{\rm 2}^{{\rm 2}{\it k}}\times{\it k}{\rm !(}{\it n}-{\rm 1}{\rm )}\cdots{\rm (}{\it n}-{\it k}{\rm )}}}={{{\it a}_{{\rm 0}}}\over{{\rm 2}^{{\rm 2}{\it k}}\times{\it k}{\rm !}{{{\rm (}{\it n}-{\rm 1}{\rm )!}}\over{{\rm (}{\it n}-{\it k}-{\rm 1}{\rm )!}}}}}={{{\rm (}{\it n}-{\it k}-{\rm 1}{\rm )!}{\it a}_{{\rm 0}}}\over{{\rm 2}^{{\rm 2}{\it k}}\times{\it k}{\rm !(}{\it n}-{\rm 1}{\rm )!}}}$$ We assume that ${\it a}_{{\rm 0}}=-{\rm 2}^{{\it n}{\rm -1}}{\rm (}{\it n}-{\rm 1}{\rm )!}$ . with this assumption, the coefficients will be in this form: $${\it a}_{{\rm 2}{\it k}}={{-{\rm 2}^{{\it n}{\rm -1}}{\rm (}{\it n}-{\rm 1}{\rm )!(}{\it n}-{\it k}-{\rm 1}{\rm )!}}\over{{\rm 2}^{{\rm 2}{\it k}}\times{\it k}{\rm !(}{\it n}-{\rm 1}{\rm )!}}}=-{{{\rm (}{\it n}-{\it k}-{\rm 1}{\rm )!}}\over{{\rm 2}^{{\rm 2}{\it k}{\rm -}{\it n}{\rm +1}}{\it k}{\rm !}}}{\rm                       (}I{\rm )}$$ In particular ${\it a}_{{\rm 2}{\it n}{\rm -2}}=-{{{\rm 1}}\over{{\rm 2}^{{\it n}{\rm -1}}{\rm (}{\it n}-{\rm 1}{\rm )!}}}$ . For the coefficient of $x^{n}$ : $${\rm 2}{\it k}{{{\it n}}\over{{\rm 2}^{{\it n}}{\it n}{\rm !}}}+{\rm 0}{\it a}_{{\rm 2}{\it n}}+{\it a}_{{\rm 2}{\it n}{\rm -2}}={\rm 0}\Rightarrow{{{\it k}}\over{{\rm 2}^{{\it n}{\rm -1}}{\rm (}{\it n}-{\rm 1}{\rm )!}}}-{{{\rm 1}}\over{{\rm 2}^{{\it n}{\rm -1}}{\rm (}{\it n}-{\rm 1}{\rm )!}}}={\rm 0}$$ $$\Rightarrow k=1$$ The above relationship holds for all values of ${\it a}_{{\rm 2}{\it n}}$ , so ${\it a}_{{\rm 2}{\it n}}$ is an arbitrary parameter and we can assume that $${\it a}_{{\rm 2}{\it n}}=-{{{\rm 1}+{{{\rm 1}}\over{{\rm 2}}}+\cdots+{{{\rm 1}}\over{{\it n}}}}\over{{\rm 2}^{{\it n}{\rm +1}}{\it n}{\rm !}}}$$ For the coefficient of $x^{2+n}$ : $${\rm 2}{{{\rm (}-{\rm 1}{\rm )(}{\rm 2}+{\it n}{\rm )}}\over{{\rm 2}^{{\rm 2+}{\it n}}{\rm (}{\rm 1}+{\it n}{\rm )!}}}+{\rm (}{\rm 2}{\it n}+{\rm 2}{\rm )(}{\rm 2}{\rm )}{\it a}_{{\rm 2}{\it n}{\rm +2}}+{\it a}_{{\rm 2}{\it n}}={\rm 0}$$ $$\Rightarrow-{{{\it n}+{\rm 2}}\over{{\rm 2}^{{\it n}{\rm +1}}{\rm (}{\it n}+{\rm 1}{\rm )!}}}+{\rm 2}{\rm (}{\rm 2}{\it n}+{\rm 2}{\rm )}{\it a}_{{\rm 2}{\it n}{\rm +2}}-{{{\rm 1}+{{{\rm 1}}\over{{\rm 2}}}+\cdots+{{{\rm 1}}\over{{\it n}}}}\over{{\rm 2}^{{\it n}{\rm +1}}{\it n}{\rm !}}}={\rm 0}$$ $$\Rightarrow{\rm 2}^{{\rm 2}}{\rm (}{\it n}+{\rm 1}{\rm )}{\it a}_{{\rm 2}{\it n}{\rm +2}}={{{\it n}+{\rm 2}}\over{{\rm 2}^{{\it n}{\rm +1}}{\rm (}{\it n}+{\rm 1}{\rm )!}}}+{{{\rm 1}+{{{\rm 1}}\over{{\rm 2}}}+\cdots+{{{\rm 1}}\over{{\it n}}}}\over{{\rm 2}^{{\it n}{\rm +1}}{\it n}{\rm !}}}={{{\it n}+{\rm 2}+{\rm (}{\it n}+{\rm 1}{\rm )(}{\rm 1}+{{{\rm 1}}\over{{\rm 2}}}+\cdots+{{{\rm 1}}\over{{\it n}}}{\rm )}}\over{{\rm 2}^{{\it n}{\rm +1}}{\rm (}{\it n}+{\rm 1}{\rm )!}}}$$ $$\Rightarrow{\it a}_{{\rm 2}{\it n}{\rm +2}}={{{\rm 1}+{{{\rm 1}}\over{{\it n}+{\rm 1}}}+{\rm 1}+{{{\rm 1}}\over{{\rm 2}}}+\cdots+{{{\rm 1}}\over{{\it n}}}}\over{{\rm 2}^{{\it n}{\rm +3}}{\rm (}{\it n}+{\rm 1}{\rm )!}}}$$ For the coefficient of $x^{4+n}$ : $${\rm 2}{{{\rm 4}+{\it n}}\over{{\rm 2}^{{\rm 4+}{\it n}}{\rm 2}{\rm !(}{\rm 2}+{\it n}{\rm )!}}}+{\rm (}{\rm 2}{\it n}+{\rm 4}{\rm )(}{\rm 4}{\rm )}{\it a}_{{\rm 2}{\it n}{\rm +4}}+{\it a}_{{\rm 2}{\it n}{\rm +2}}={\rm 0}$$ $${{{\it n}+{\rm 4}}\over{{\rm 2}^{{\it n}{\rm +3}}{\rm 2}{\rm !(}{\it n}+{\rm 2}{\rm )!}}}+{\rm 2}^{{\rm 3}}{\rm (}{\it n}+{\rm 2}{\rm )}{\it a}_{{\rm 2}{\it n}{\rm +4}}+{{{\rm 1}+{\rm (}{\rm 1}+{{{\rm 1}}\over{{\rm 2}}}+\cdots+{{{\rm 1}}\over{{\it n}+{\rm 1}}}{\rm )}}\over{{\rm 2}^{{\it n}{\rm +3}}{\rm (}{\it n}+{\rm 1}{\rm )!}}}={\rm 0}$$ $${\rm 2}^{{\rm 3}}{\rm (}{\it n}+{\rm 2}{\rm )}{\it a}_{{\rm 2}{\it n}{\rm +4}}=-{{{\it n}+{\rm 4}}\over{{\rm 2}^{{\it n}{\rm +3}}{\rm 2}{\rm !(}{\it n}+{\rm 2}{\rm )!}}}-{{{\rm 1}+{\rm (}{\rm 1}+{{{\rm 1}}\over{{\rm 2}}}+\cdots+{{{\rm 1}}\over{{\it n}+{\rm 1}}}{\rm )}}\over{{\rm 2}^{{\it n}{\rm +3}}{\rm (}{\it n}+{\rm 1}{\rm )!}}}=-{{{\it n}+{\rm 4}+{\rm 2}{\rm (}{\it n}+{\rm 2}{\rm )}\left[{{\rm 1}+{\rm (}{\rm 1}+{{{\rm 1}}\over{{\rm 2}}}+\cdots+{{{\rm 1}}\over{{\it n}+{\rm 1}}}{\rm )}}\right]}\over{{\rm 2}^{{\it n}{\rm +3}}{\rm 2}{\rm !(}{\it n}+{\rm 2}{\rm )!}}}$$ $${\it \Rightarrow}{\it a}_{{\rm 2}{\it n}{\rm +4}}=-{{{{{\it n}+{\rm 4}}\over{{\rm 2}{\rm (}{\it n}+{\rm 2}{\rm )}}}+{\rm 1}+{\rm (}{\rm 1}+{{{\rm 1}}\over{{\rm 2}}}+\cdots+{{{\rm 1}}\over{{\it n}+{\rm 1}}}{\rm )}}\over{{\rm 2}^{{\it n}{\rm +5}}{\rm 2}{\rm !(}{\it n}+{\rm 2}{\rm )!}}}=-{{{{{\rm 1}}\over{{\rm 2}}}+{{{\rm 1}}\over{{\it n}+{\rm 2}}}+{\rm 1}+{\rm (}{\rm 1}+{{{\rm 1}}\over{{\rm 2}}}+\cdots+{{{\rm 1}}\over{{\it n}+{\rm 1}}}{\rm )}}\over{{\rm 2}^{{\it n}{\rm +5}}{\rm 2}{\rm !(}{\it n}+{\rm 2}{\rm )!}}}$$ $$=-{{{\rm (}{\rm 1}+{{{\rm 1}}\over{{\rm 2}}}{\rm )}+{\rm (}{\rm 1}+{{{\rm 1}}\over{{\rm 2}}}+\cdots+{{{\rm 1}}\over{{\it n}+{\rm 1}}}+{{{\rm 1}}\over{{\it n}+{\rm 2}}}{\rm )}}\over{{\rm 2}^{{\it n}{\rm +5}}{\rm 2}{\rm !(}{\it n}+{\rm 2}{\rm )!}}}$$ And in general, it can be shown by induction that: $${\it a}_{{\rm 2}{\it m}}={\rm (}-{\rm 1}{\rm )}^{{\it m}{\rm -}{\it n}{\rm +1}}{{{\rm (}{\rm 1}+{{{\rm 1}}\over{{\rm 2}}}+\cdots+{{{\rm 1}}\over{{\it m}-{\it n}}}{\rm )}+{\rm (}{\rm 1}+{{{\rm 1}}\over{{\rm 2}}}+\cdots+{{{\rm 1}}\over{{\it m}}}{\rm )}}\over{{\rm 2}^{{\rm 2}{\it m}{\rm -}{\it n}{\rm +1}}{\rm (}{\it m}-{\it n}{\rm )!}{\it m}{\rm !}}}{\rm   }{\rm (}{\it m}\geq{\it n}{\rm )}{\rm               (}II{\rm )}$$ Therefore, according to relations ${\rm (}I )$ and ${\rm (}II )$ $${\it y}_{{\rm 2}}{\rm (}{\it x}{\rm )}={\it J}_{{\it n}}{\rm (}{\it x}{\rm )}{\it Lnx}+\sum\limits_{{\it m}{\rm =0}}^{{\it n}{\rm -1}}{-{{{\rm (}{\it n}-{\it m}-{\rm 1}{\rm )!}}\over{{\rm 2}^{{\rm 2}{\it m}{\rm -}{\it n}{\rm +1}}{\it m}{\rm !}}}}{\it x}^{{\rm 2}{\it m}{\rm -}{\it n}}+\sum\limits_{{\it m}={\it n}}^{\infty}{{\rm (}-{\rm 1}{\rm )}^{{\it m}{\rm -}{\it n}{\rm +1}}}{{{\rm (}{\rm 1}+\cdots+{{{\rm 1}}\over{{\it m}-{\it n}}}{\rm )}+{\rm (}{\rm 1}+\cdots+{{{\rm 1}}\over{{\it m}}}{\rm )}}\over{{\rm 2}^{{\rm 2}{\it m}{\rm -}{\it n}{\rm +1}}{\rm (}{\it m}-{\it n}{\rm )!}{\it m}{\rm !}}}{\it x}^{{\rm 2}{\it m}{\rm -}{\it n}}$$ $$={\it J}_{{\it n}}{\rm (}{\it x}{\rm )}{\it Lnx}-{{{\rm 1}}\over{{\rm 2}}}\sum\limits_{{\it m}{\rm =0}}^{{\it n}{\rm -1}}{{{{\rm (}{\it n}-{\it m}-{\rm 1}{\rm )!}}\over{{\it m}{\rm !}}}}{{{\it x}^{{\rm 2}{\it m}{\rm -}{\it n}}}\over{{\rm 2}^{{\rm 2}{\it m}{\rm -}{\it n}}}}-{{{\rm 1}}\over{{\rm 2}}}\sum\limits_{{\it m}={\it n}}^{\infty}{{\rm (}-{\rm 1}{\rm )}^{{\it m}{\rm -}{\it n}}}{{{\rm (}{\rm 1}+\cdots+{{{\rm 1}}\over{{\it m}-{\it n}}}{\rm )}+{\rm (}{\rm 1}+\cdots+{{{\rm 1}}\over{{\it m}}}{\rm )}}\over{{\rm (}{\it m}-{\it n}{\rm )!}{\it m}{\rm !}}}{{{\it x}^{{\rm 2}{\it m}{\rm -}{\it n}}}\over{{\rm 2}^{{\rm 2}{\it m}{\rm -}{\it n}}}}$$ $$={\it J}_{{\it n}}{\rm (}{\it x}{\rm )}{\it Lnx}-{{{\rm 1}}\over{{\rm 2}}}\sum\limits_{{\it m}{\rm =0}}^{{\it n}{\rm -1}}{{{{\rm (}{\it n}-{\it m}-{\rm 1}{\rm )!}}\over{{\it m}{\rm !}}}}{\rm (}{{{\it x}}\over{{\rm 2}}}{\rm )}^{{\rm 2}{\it m}{\rm -}{\it n}}-{{{\rm 1}}\over{{\rm 2}}}\sum\limits_{{\it m}={\it n}}^{\infty}{{\rm (}-{\rm 1}{\rm )}^{{\it m}{\rm -}{\it n}}}{{{\rm (}{\rm 1}+\cdots+{{{\rm 1}}\over{{\it m}-{\it n}}}{\rm )}+{\rm (}{\rm 1}+\cdots+{{{\rm 1}}\over{{\it m}}}{\rm )}}\over{{\rm (}{\it m}-{\it n}{\rm )!}{\it m}{\rm !}}}{\rm (}{{{\it x}}\over{{\rm 2}}}{\rm )}^{{\rm 2}{\it m}{\rm -}{\it n}}$$ If we reduce the lower limit of the last sigma by $n$ units, we have: $${\it y}_{{\rm 2}}{\rm (}{\it x}{\rm )}={\it J}_{{\it n}}{\rm (}{\it x}{\rm )}{\it Lnx}-{{{\rm 1}}\over{{\rm 2}}}\sum\limits_{{\it m}{\rm =0}}^{{\it n}{\rm -1}}{{{{\rm (}{\it n}-{\it m}-{\rm 1}{\rm )!}}\over{{\it m}{\rm !}}}}{\rm (}{{{\it x}}\over{{\rm 2}}}{\rm )}^{{\rm 2}{\it m}{\rm -}{\it n}}-{{{\rm 1}}\over{{\rm 2}}}\sum\limits_{{\it m}={\rm 0}}^{\infty}{{\rm (}-{\rm 1}{\rm )}^{{\it m}}}{{{\rm (}{\rm 1}+\cdots+{{{\rm 1}}\over{{\it m}}}{\rm )}+{\rm (}{\rm 1}+\cdots+{{{\rm 1}}\over{{\it m}+{\it n}}}{\rm )}}\over{{\it m}{\rm !(}{\it m}+{\it n}{\rm )!}}}{\rm (}{{{\it x}}\over{{\rm 2}}}{\rm )}^{{\rm 2}{\it m}{\rm +}{\it n}}$$ Then, instead of the solution ${\it y}_{{\rm 2}}$ , we consider the following solution which is linearly independent with ${\it J}_{{\it n}}{\rm (}{\it x}{\rm )}$ : $${\it Y}_{{\it n}}{\rm (}{\it x}{\rm )}={\it a}{\rm (}{\it y}_{{\rm 2}}{\rm (}{\it x}{\rm )}+{\it bJ}_{{\it n}}{\rm (}{\it x}{\rm ))}$$ where ${\it a}={{{\rm 2}}\over{{\rm \pi}}}$ and ${\it b}={\rm \gamma}-{\it Ln}{\rm 2}$ . So the solution is $${\it Y}_{{\it n}}{\rm (}{\it x}{\rm )}={{{\rm 2}}\over{{\rm \pi}}}\left({{\it y}_{{\rm 2}}{\rm (}{\it x}{\rm )}+{\rm (}{\rm \gamma}-{\it Ln}{\rm 2}{\rm )}{\it J}_{{\it n}}{\rm (}{\it x}{\rm )}}\right)$$ $${\it Y}_{{\it n}}{\rm (}{\it x}{\rm )}={{{\rm 2}}\over{{\rm \pi}}}\left({{\rm (}{\it Lnx}+{\rm \gamma}-{\it Ln}{\rm 2}{\rm )}{\it J}_{{\it n}}{\rm (}{\it x}{\rm )}-{{{\rm 1}}\over{{\rm 2}}}\sum\limits_{{\it m}{\rm =0}}^{{\it n}{\rm -1}}{{{{\rm (}{\it n}-{\it m}-{\rm 1}{\rm )!}}\over{{\it m}{\rm !}}}}{\rm (}{{{\it x}}\over{{\rm 2}}}{\rm )}^{{\rm 2}{\it m}{\rm -}{\it n}}-{{{\rm 1}}\over{{\rm 2}}}\sum\limits_{{\it m}={\rm 0}}^{\infty}{{\rm (}-{\rm 1}{\rm )}^{{\it m}}}{{{\rm (}{\rm 1}+\cdots+{{{\rm 1}}\over{{\it m}}}{\rm )}+{\rm (}{\rm 1}+\cdots+{{{\rm 1}}\over{{\it m}+{\it n}}}{\rm )}}\over{{\it m}{\rm !(}{\it m}+{\it n}{\rm )!}}}{\rm (}{{{\it x}}\over{{\rm 2}}}{\rm )}^{{\rm 2}{\it m}{\rm +}{\it n}}}\right)$$ $$={{{\rm 2}}\over{{\rm \pi}}}{\rm (}{\it Lnx}+{\rm \gamma}-{\it Ln}{\rm 2}{\rm )}{\it J}_{{\it n}}{\rm (}{\it x}{\rm )}-{{{\rm 1}}\over{{\rm \pi}}}\sum\limits_{{\it m}{\rm =0}}^{{\it n}{\rm -1}}{{{{\rm (}{\it n}-{\it m}-{\rm 1}{\rm )!}}\over{{\it m}{\rm !}}}}{\rm (}{{{\it x}}\over{{\rm 2}}}{\rm )}^{{\rm 2}{\it m}{\rm -}{\it n}}-{{{\rm 1}}\over{{\rm \pi}}}\sum\limits_{{\it m}={\rm 0}}^{\infty}{{\rm (}-{\rm 1}{\rm )}^{{\it m}}}{{{\rm (}{\rm 1}+\cdots+{{{\rm 1}}\over{{\it m}}}{\rm )}+{\rm (}{\rm 1}+\cdots+{{{\rm 1}}\over{{\it m}+{\it n}}}{\rm )}}\over{{\it m}{\rm !(}{\it m}+{\it n}{\rm )!}}}{\rm (}{{{\it x}}\over{{\rm 2}}}{\rm )}^{{\rm 2}{\it m}{\rm +}{\it n}}$$ .","I am reading a textbook about engineering mathematics. To solve the Bessel differential equation the book first obtains the function using the series method as follows: then obtains the Bessel function of the second kind of zero order using the series method as follows: in which and is the so-called ''Euler constant'', which is defined as the limit of as approaches infinity. Then the author states that the Bessel function of the second kind of order is obtained in a similar way as: The book does not provide any proof for the latter formula. Can you explain me how to get it? The mentioned book is ""ADVANCED ENGINEERING MATHEMATICS"" by ""ERWIN KREYSZIG"". If you know a textbook that explains how to get this formula well, please introduce it to me Edit: I found a solution myself, but I'm not sure if the details are correct. here is the solution: According to the Frobenius method, a second solution is obtained from this formula: By differentiating this function twice, we get and Substituting , , and into Bessel equation , we get or find the derivative of Substituting in we have: For the coefficient of : so is an arbitrary constant For the coefficient of : And in general for the coefficient of : And according to this recursive relation, we have: That is, all the odd coefficients up to are equal to zero, but this result is also true for the odd coefficients greater than , because if the power of is , is in the second sigma and in the third sigma, that is, the even coefficients of are produced So the equation holds for all odd coefficients and we can conclude that all odd coefficients are equal to zero and we only calculate even coefficients. and in general for : We assume that . with this assumption, the coefficients will be in this form: In particular . For the coefficient of : The above relationship holds for all values of , so is an arbitrary parameter and we can assume that For the coefficient of : For the coefficient of : And in general, it can be shown by induction that: Therefore, according to relations and If we reduce the lower limit of the last sigma by units, we have: Then, instead of the solution , we consider the following solution which is linearly independent with : where and . So the solution is .","
x^2 y'' + xy' + (x^2 - \nu^2) y = 0
 J_{n}(x) 
J_n(x) = x^n \sum_{m=0}^{\infty} 
\frac{(-1)^m x^{2m}}{2^{2m+n} m! (n+m)!}
 Y_{0}(x) 
Y_{0}(x) = \frac{2}{\pi} 
\left[ J_0(x) \left( \ln\frac{x}{2} + \gamma \right) 
+ \sum_{m=1}^{\infty} 
\frac{(-1)^{m-1} h_m}{2^{2m}(m!)^2} x^{2m} \right]
 
h_m = 1 + \frac12 + \cdots + \frac1m
 \gamma \simeq 0.5772 
1 + \frac12 + \cdots + \frac1s - \ln s
 s n \begin{align}
Y_{n}(x) &= \frac{2}{\pi} J_{n}(x) 
\left(\ln\frac{x}{2} + \gamma \right) \\[2pt]
&\quad{} + \frac{x^n}{\pi} \sum_{m=0}^{\infty} 
\frac{(-1)^{m-1}(h_{m} + h_{m+n})}{2^{2m+n} m! (m+n)!} x^{2m} \\
&\quad{} - \frac{x^{-n}}{\pi} \sum_{m=0}^{n-1} 
\frac{(n-m-1)!}{2^{2m-n} m!} x^{2m}
\end{align} {\it y}_{{\rm 2}}{\rm (}{\it x}{\rm )}={\it kJ}_{{\it n}}{\rm (}{\it x}{\rm )}{\it Lnx}+{\it x}^{{\rm -}{\it n}}\sum\limits_{{\it m}={\rm 0}}^{\infty}{{\it a}_{{\it m}}{\it x}^{{\it m}}} y{_{2}}^{\prime} y{_{2}}^{\prime\prime} y{_{2}}^{\prime}(x)=kJ{_{n}}^{\prime}(x)Lnx+{{KJ_{n}(x)}\over{x}}+\sum\limits_{m=0}^{\infty}{(m-n)a_{m}x^{m-n-1}} y{_{2}}^{\prime\prime}(x)=kJ{_{n}}^{\prime\prime}(x)Lnx+{{2KJ{_{n}}^{\prime}(x)}\over{x}}-{{KJ_{n}(x)}\over{x^{2}}}+\sum\limits_{m=0}^{\infty}{(m-n)(m-n-1)a_{m}x^{m-n-2}} {\it y}_{{\rm 2}}{\rm (}{\it x}{\rm )} y{_{2}}^{\prime}(x) y{_{2}}^{\prime\prime}(x) x^{2}y''+xy'+(x^{2}-\upsilon^{2})y=0 kx^{2}J{_{n}}^{\prime}(x)Ln(x)+2kxJ{_{n}}^{\prime}(x)+kxJ{_{n}}^{\prime}(x)Ln(x)+k(x^{2}-n^{2})J_{n}(x)Ln(x) +\sum\limits_{{\it m}={\rm 0}}^{\infty}{{\rm (}{\it m}-{\it n}{\rm )(}{\it m}-{\it n}-{\rm 1}{\rm )}{\it a}_{{\it m}}{\it x}^{{\it m}{\rm -}{\it n}}}+\sum\limits_{{\it m}{\rm =0}}^{{\rm \infty}}{{\rm (}{\it m}-{\it n}{\rm )}}{\it a}_{{\it m}}{\it x}^{{\it m}{\rm -}{\it n}}+\sum\limits_{{\it m}={\rm 0}}^{\infty}{{\it a}_{{\it m}}}{\it x}^{{\it m}{\rm -}{\it n}{\rm +2}}-{\it n}^{{\rm 2}}\sum\limits_{{\it m}={\rm 0}}^{\infty}{{\it a}_{{\it m}}}{\it x}^{{\it m}{\rm -}{\it n}}={\rm 0} \Rightarrow 2kxJ{_{n}}^{\prime}(x)+\sum\limits_{m=0}^{\infty}{\left[{(m-n)^{2}-n^{2}}\right]}a_{m}x^{m-n}+\sum\limits_{m=0}^{\infty}{a_{m}}x^{m-n+2}=0 \Rightarrow 2kxJ{_{n}}^{\prime}(x)+\sum\limits_{m=0}^{\infty}{m(m-2n)}a_{m}x^{m-n}+\sum\limits_{m=0}^{\infty}{a_{m}}x^{m-n+2}=0\ \ \ \ \ \ (1) {\it J}_{{\it n}}{\rm (}{\it x}{\rm )} J_{n}(x)=\sum\limits_{m=0}^{\infty}{{{(-1)^{m}}\over{2^{2m+n}m!(m+n)!}}}x^{2m+n}\Rightarrow J{_{n}}^{\prime}(x)=\sum\limits_{m=0}^{\infty}{{{(-1)^{m}(2m+n)}\over{2^{2m+n}m!(m+n)!}}}x^{2m+n-1} (1) {\rm 2}{\it k}\sum\limits_{{\it m}{\rm =0}}^{{\rm \infty}}{{{{\rm (}-{\rm 1}{\rm )}^{{\it m}}{\rm (}{\rm 2}{\it m}+{\it n}{\rm )}}\over{{\rm 2}^{{\rm 2}{\it m}{\rm +}{\it n}}{\it m}{\rm !(}{\it m}+{\it n}{\rm )!}}}}{\it x}^{{\rm 2}{\it m}{\rm +}{\it n}}+\sum\limits_{{\it m}={\rm 0}}^{\infty}{{\it m}{\rm (}{\it m}-{\rm 2}{\it n}{\rm )}{\rm a}_{{\it m}}{\it x}^{{\it m}{\rm -}{\it n}}}{\rm  }+\sum\limits_{{\it m}={\rm 0}}^{\infty}{{\it a}_{{\it m}}}{\it x}^{{\it m}{\rm -}{\it n}{\rm +2}}={\rm 0} x^{-n} 0a_{0}=0 a_{0} x^{1-n} 1(1-2n)a_{1}=0\Rightarrow a_{1}=0 x^{p-n+2} (p\leq 2n-3) {\rm (}{\it p}+{\rm 2}{\rm )(}{\it p}+{\rm 2}-{\rm 2}{\it n}{\rm )}{\it a}_{{\it p}{\rm +2}}+{\it a}_{{\it p}}={\rm 0}\Rightarrow{\it a}_{{\it p}{\rm +2}}=-{{{\it a}_{{\it p}}}\over{{\rm (}{\it p}+{\rm 2}{\rm )(}{\it p}+{\rm 2}-{\rm 2}{\it n}{\rm )}}} {\it a}_{{\rm 3}}={\it a}_{{\rm 5}}=\cdots={\it a}_{{\rm 2}{\it n}{\rm -1}}={\rm 0} {\it a}_{{\rm 2}{\it n}{\rm -1}} {\it a}_{{\rm 2}{\it n}{\rm -1}} x 2k+n {\it m} 2n+2k 2n+2k-2 {\it a} {\it a}_{{\it p}{\rm +2}}=-{{{\it a}_{{\it p}}}\over{{\rm (}{\it p}+{\rm 2}{\rm )(}{\it p}+{\rm 2}-{\rm 2}{\it n}{\rm )}}} {\it a}_{{\rm 2}}=-{{{\it a}_{{\rm 0}}}\over{{\rm 2}{\rm (}{\rm 2}-{\rm 2}{\it n}{\rm )}}}={{{\it a}_{{\rm 0}}}\over{{\rm 2}{\rm (}{\rm 2}{\it n}-{\rm 2}{\rm )}}} {\it a}_{{\rm 4}}=-{{{\it a}_{{\rm 2}}}\over{{\rm 4}{\rm (}{\rm 4}-{\rm 2}{\it n}{\rm )}}}={{{\it a}_{{\rm 2}}}\over{{\rm 4}{\rm (}{\rm 2}{\it n}-{\rm 4}{\rm )}}}={{{\it a}_{{\rm 0}}}\over{{\rm 4}\times{\rm 2}{\rm (}{\rm 2}{\it n}-{\rm 2}{\rm )(}{\rm 2}{\it n}-{\rm 4}{\rm )}}} {\it a}_{{\rm 6}}=-{{{\it a}_{{\rm 4}}}\over{{\rm 6}{\rm (}{\rm 6}-{\rm 2}{\it n}{\rm )}}}={{{\it a}_{{\rm 4}}}\over{{\rm 6}{\rm (}{\rm 2}{\it n}-{\rm 6}{\rm )}}}={{{\it a}_{{\rm 0}}}\over{{\rm 6}\times{\rm 4}\times{\rm 2}{\rm (}{\rm 2}{\it n}-{\rm 2}{\rm )(}{\rm 2}{\it n}-{\rm 4}{\rm )(}{\rm 2}{\it n}-{\rm 6}{\rm )}}} k\leq n-1 {\it a}_{{\rm 2}{\it k}}={{{\it a}_{{\rm 0}}}\over{{\rm 2}{\it k}{\rm (}{\rm 2}{\it k}-{\rm 2}{\rm )}\cdots\times{\rm 4}\times{\rm 2}{\rm (}{\rm 2}{\it n}-{\rm 2}{\rm )(}{\rm 2}{\it n}-{\rm 4}{\rm )}\cdots{\rm (}{\rm 2}{\it n}-{\rm 2}{\it k}{\rm )}}}={{{\it a}_{{\rm 0}}}\over{{\rm 2}^{{\it k}}\times{\it k}{\rm !}\times{\rm 2}^{{\it k}}{\rm (}{\it n}-{\rm 1}{\rm )}\cdots{\rm (}{\it n}-{\it k}{\rm )}}} ={{{\it a}_{{\rm 0}}}\over{{\rm 2}^{{\rm 2}{\it k}}\times{\it k}{\rm !(}{\it n}-{\rm 1}{\rm )}\cdots{\rm (}{\it n}-{\it k}{\rm )}}}={{{\it a}_{{\rm 0}}}\over{{\rm 2}^{{\rm 2}{\it k}}\times{\it k}{\rm !}{{{\rm (}{\it n}-{\rm 1}{\rm )!}}\over{{\rm (}{\it n}-{\it k}-{\rm 1}{\rm )!}}}}}={{{\rm (}{\it n}-{\it k}-{\rm 1}{\rm )!}{\it a}_{{\rm 0}}}\over{{\rm 2}^{{\rm 2}{\it k}}\times{\it k}{\rm !(}{\it n}-{\rm 1}{\rm )!}}} {\it a}_{{\rm 0}}=-{\rm 2}^{{\it n}{\rm -1}}{\rm (}{\it n}-{\rm 1}{\rm )!} {\it a}_{{\rm 2}{\it k}}={{-{\rm 2}^{{\it n}{\rm -1}}{\rm (}{\it n}-{\rm 1}{\rm )!(}{\it n}-{\it k}-{\rm 1}{\rm )!}}\over{{\rm 2}^{{\rm 2}{\it k}}\times{\it k}{\rm !(}{\it n}-{\rm 1}{\rm )!}}}=-{{{\rm (}{\it n}-{\it k}-{\rm 1}{\rm )!}}\over{{\rm 2}^{{\rm 2}{\it k}{\rm -}{\it n}{\rm +1}}{\it k}{\rm !}}}{\rm                       (}I{\rm )} {\it a}_{{\rm 2}{\it n}{\rm -2}}=-{{{\rm 1}}\over{{\rm 2}^{{\it n}{\rm -1}}{\rm (}{\it n}-{\rm 1}{\rm )!}}} x^{n} {\rm 2}{\it k}{{{\it n}}\over{{\rm 2}^{{\it n}}{\it n}{\rm !}}}+{\rm 0}{\it a}_{{\rm 2}{\it n}}+{\it a}_{{\rm 2}{\it n}{\rm -2}}={\rm 0}\Rightarrow{{{\it k}}\over{{\rm 2}^{{\it n}{\rm -1}}{\rm (}{\it n}-{\rm 1}{\rm )!}}}-{{{\rm 1}}\over{{\rm 2}^{{\it n}{\rm -1}}{\rm (}{\it n}-{\rm 1}{\rm )!}}}={\rm 0} \Rightarrow k=1 {\it a}_{{\rm 2}{\it n}} {\it a}_{{\rm 2}{\it n}} {\it a}_{{\rm 2}{\it n}}=-{{{\rm 1}+{{{\rm 1}}\over{{\rm 2}}}+\cdots+{{{\rm 1}}\over{{\it n}}}}\over{{\rm 2}^{{\it n}{\rm +1}}{\it n}{\rm !}}} x^{2+n} {\rm 2}{{{\rm (}-{\rm 1}{\rm )(}{\rm 2}+{\it n}{\rm )}}\over{{\rm 2}^{{\rm 2+}{\it n}}{\rm (}{\rm 1}+{\it n}{\rm )!}}}+{\rm (}{\rm 2}{\it n}+{\rm 2}{\rm )(}{\rm 2}{\rm )}{\it a}_{{\rm 2}{\it n}{\rm +2}}+{\it a}_{{\rm 2}{\it n}}={\rm 0} \Rightarrow-{{{\it n}+{\rm 2}}\over{{\rm 2}^{{\it n}{\rm +1}}{\rm (}{\it n}+{\rm 1}{\rm )!}}}+{\rm 2}{\rm (}{\rm 2}{\it n}+{\rm 2}{\rm )}{\it a}_{{\rm 2}{\it n}{\rm +2}}-{{{\rm 1}+{{{\rm 1}}\over{{\rm 2}}}+\cdots+{{{\rm 1}}\over{{\it n}}}}\over{{\rm 2}^{{\it n}{\rm +1}}{\it n}{\rm !}}}={\rm 0} \Rightarrow{\rm 2}^{{\rm 2}}{\rm (}{\it n}+{\rm 1}{\rm )}{\it a}_{{\rm 2}{\it n}{\rm +2}}={{{\it n}+{\rm 2}}\over{{\rm 2}^{{\it n}{\rm +1}}{\rm (}{\it n}+{\rm 1}{\rm )!}}}+{{{\rm 1}+{{{\rm 1}}\over{{\rm 2}}}+\cdots+{{{\rm 1}}\over{{\it n}}}}\over{{\rm 2}^{{\it n}{\rm +1}}{\it n}{\rm !}}}={{{\it n}+{\rm 2}+{\rm (}{\it n}+{\rm 1}{\rm )(}{\rm 1}+{{{\rm 1}}\over{{\rm 2}}}+\cdots+{{{\rm 1}}\over{{\it n}}}{\rm )}}\over{{\rm 2}^{{\it n}{\rm +1}}{\rm (}{\it n}+{\rm 1}{\rm )!}}} \Rightarrow{\it a}_{{\rm 2}{\it n}{\rm +2}}={{{\rm 1}+{{{\rm 1}}\over{{\it n}+{\rm 1}}}+{\rm 1}+{{{\rm 1}}\over{{\rm 2}}}+\cdots+{{{\rm 1}}\over{{\it n}}}}\over{{\rm 2}^{{\it n}{\rm +3}}{\rm (}{\it n}+{\rm 1}{\rm )!}}} x^{4+n} {\rm 2}{{{\rm 4}+{\it n}}\over{{\rm 2}^{{\rm 4+}{\it n}}{\rm 2}{\rm !(}{\rm 2}+{\it n}{\rm )!}}}+{\rm (}{\rm 2}{\it n}+{\rm 4}{\rm )(}{\rm 4}{\rm )}{\it a}_{{\rm 2}{\it n}{\rm +4}}+{\it a}_{{\rm 2}{\it n}{\rm +2}}={\rm 0} {{{\it n}+{\rm 4}}\over{{\rm 2}^{{\it n}{\rm +3}}{\rm 2}{\rm !(}{\it n}+{\rm 2}{\rm )!}}}+{\rm 2}^{{\rm 3}}{\rm (}{\it n}+{\rm 2}{\rm )}{\it a}_{{\rm 2}{\it n}{\rm +4}}+{{{\rm 1}+{\rm (}{\rm 1}+{{{\rm 1}}\over{{\rm 2}}}+\cdots+{{{\rm 1}}\over{{\it n}+{\rm 1}}}{\rm )}}\over{{\rm 2}^{{\it n}{\rm +3}}{\rm (}{\it n}+{\rm 1}{\rm )!}}}={\rm 0} {\rm 2}^{{\rm 3}}{\rm (}{\it n}+{\rm 2}{\rm )}{\it a}_{{\rm 2}{\it n}{\rm +4}}=-{{{\it n}+{\rm 4}}\over{{\rm 2}^{{\it n}{\rm +3}}{\rm 2}{\rm !(}{\it n}+{\rm 2}{\rm )!}}}-{{{\rm 1}+{\rm (}{\rm 1}+{{{\rm 1}}\over{{\rm 2}}}+\cdots+{{{\rm 1}}\over{{\it n}+{\rm 1}}}{\rm )}}\over{{\rm 2}^{{\it n}{\rm +3}}{\rm (}{\it n}+{\rm 1}{\rm )!}}}=-{{{\it n}+{\rm 4}+{\rm 2}{\rm (}{\it n}+{\rm 2}{\rm )}\left[{{\rm 1}+{\rm (}{\rm 1}+{{{\rm 1}}\over{{\rm 2}}}+\cdots+{{{\rm 1}}\over{{\it n}+{\rm 1}}}{\rm )}}\right]}\over{{\rm 2}^{{\it n}{\rm +3}}{\rm 2}{\rm !(}{\it n}+{\rm 2}{\rm )!}}} {\it \Rightarrow}{\it a}_{{\rm 2}{\it n}{\rm +4}}=-{{{{{\it n}+{\rm 4}}\over{{\rm 2}{\rm (}{\it n}+{\rm 2}{\rm )}}}+{\rm 1}+{\rm (}{\rm 1}+{{{\rm 1}}\over{{\rm 2}}}+\cdots+{{{\rm 1}}\over{{\it n}+{\rm 1}}}{\rm )}}\over{{\rm 2}^{{\it n}{\rm +5}}{\rm 2}{\rm !(}{\it n}+{\rm 2}{\rm )!}}}=-{{{{{\rm 1}}\over{{\rm 2}}}+{{{\rm 1}}\over{{\it n}+{\rm 2}}}+{\rm 1}+{\rm (}{\rm 1}+{{{\rm 1}}\over{{\rm 2}}}+\cdots+{{{\rm 1}}\over{{\it n}+{\rm 1}}}{\rm )}}\over{{\rm 2}^{{\it n}{\rm +5}}{\rm 2}{\rm !(}{\it n}+{\rm 2}{\rm )!}}} =-{{{\rm (}{\rm 1}+{{{\rm 1}}\over{{\rm 2}}}{\rm )}+{\rm (}{\rm 1}+{{{\rm 1}}\over{{\rm 2}}}+\cdots+{{{\rm 1}}\over{{\it n}+{\rm 1}}}+{{{\rm 1}}\over{{\it n}+{\rm 2}}}{\rm )}}\over{{\rm 2}^{{\it n}{\rm +5}}{\rm 2}{\rm !(}{\it n}+{\rm 2}{\rm )!}}} {\it a}_{{\rm 2}{\it m}}={\rm (}-{\rm 1}{\rm )}^{{\it m}{\rm -}{\it n}{\rm +1}}{{{\rm (}{\rm 1}+{{{\rm 1}}\over{{\rm 2}}}+\cdots+{{{\rm 1}}\over{{\it m}-{\it n}}}{\rm )}+{\rm (}{\rm 1}+{{{\rm 1}}\over{{\rm 2}}}+\cdots+{{{\rm 1}}\over{{\it m}}}{\rm )}}\over{{\rm 2}^{{\rm 2}{\it m}{\rm -}{\it n}{\rm +1}}{\rm (}{\it m}-{\it n}{\rm )!}{\it m}{\rm !}}}{\rm   }{\rm (}{\it m}\geq{\it n}{\rm )}{\rm               (}II{\rm )} {\rm (}I ) {\rm (}II ) {\it y}_{{\rm 2}}{\rm (}{\it x}{\rm )}={\it J}_{{\it n}}{\rm (}{\it x}{\rm )}{\it Lnx}+\sum\limits_{{\it m}{\rm =0}}^{{\it n}{\rm -1}}{-{{{\rm (}{\it n}-{\it m}-{\rm 1}{\rm )!}}\over{{\rm 2}^{{\rm 2}{\it m}{\rm -}{\it n}{\rm +1}}{\it m}{\rm !}}}}{\it x}^{{\rm 2}{\it m}{\rm -}{\it n}}+\sum\limits_{{\it m}={\it n}}^{\infty}{{\rm (}-{\rm 1}{\rm )}^{{\it m}{\rm -}{\it n}{\rm +1}}}{{{\rm (}{\rm 1}+\cdots+{{{\rm 1}}\over{{\it m}-{\it n}}}{\rm )}+{\rm (}{\rm 1}+\cdots+{{{\rm 1}}\over{{\it m}}}{\rm )}}\over{{\rm 2}^{{\rm 2}{\it m}{\rm -}{\it n}{\rm +1}}{\rm (}{\it m}-{\it n}{\rm )!}{\it m}{\rm !}}}{\it x}^{{\rm 2}{\it m}{\rm -}{\it n}} ={\it J}_{{\it n}}{\rm (}{\it x}{\rm )}{\it Lnx}-{{{\rm 1}}\over{{\rm 2}}}\sum\limits_{{\it m}{\rm =0}}^{{\it n}{\rm -1}}{{{{\rm (}{\it n}-{\it m}-{\rm 1}{\rm )!}}\over{{\it m}{\rm !}}}}{{{\it x}^{{\rm 2}{\it m}{\rm -}{\it n}}}\over{{\rm 2}^{{\rm 2}{\it m}{\rm -}{\it n}}}}-{{{\rm 1}}\over{{\rm 2}}}\sum\limits_{{\it m}={\it n}}^{\infty}{{\rm (}-{\rm 1}{\rm )}^{{\it m}{\rm -}{\it n}}}{{{\rm (}{\rm 1}+\cdots+{{{\rm 1}}\over{{\it m}-{\it n}}}{\rm )}+{\rm (}{\rm 1}+\cdots+{{{\rm 1}}\over{{\it m}}}{\rm )}}\over{{\rm (}{\it m}-{\it n}{\rm )!}{\it m}{\rm !}}}{{{\it x}^{{\rm 2}{\it m}{\rm -}{\it n}}}\over{{\rm 2}^{{\rm 2}{\it m}{\rm -}{\it n}}}} ={\it J}_{{\it n}}{\rm (}{\it x}{\rm )}{\it Lnx}-{{{\rm 1}}\over{{\rm 2}}}\sum\limits_{{\it m}{\rm =0}}^{{\it n}{\rm -1}}{{{{\rm (}{\it n}-{\it m}-{\rm 1}{\rm )!}}\over{{\it m}{\rm !}}}}{\rm (}{{{\it x}}\over{{\rm 2}}}{\rm )}^{{\rm 2}{\it m}{\rm -}{\it n}}-{{{\rm 1}}\over{{\rm 2}}}\sum\limits_{{\it m}={\it n}}^{\infty}{{\rm (}-{\rm 1}{\rm )}^{{\it m}{\rm -}{\it n}}}{{{\rm (}{\rm 1}+\cdots+{{{\rm 1}}\over{{\it m}-{\it n}}}{\rm )}+{\rm (}{\rm 1}+\cdots+{{{\rm 1}}\over{{\it m}}}{\rm )}}\over{{\rm (}{\it m}-{\it n}{\rm )!}{\it m}{\rm !}}}{\rm (}{{{\it x}}\over{{\rm 2}}}{\rm )}^{{\rm 2}{\it m}{\rm -}{\it n}} n {\it y}_{{\rm 2}}{\rm (}{\it x}{\rm )}={\it J}_{{\it n}}{\rm (}{\it x}{\rm )}{\it Lnx}-{{{\rm 1}}\over{{\rm 2}}}\sum\limits_{{\it m}{\rm =0}}^{{\it n}{\rm -1}}{{{{\rm (}{\it n}-{\it m}-{\rm 1}{\rm )!}}\over{{\it m}{\rm !}}}}{\rm (}{{{\it x}}\over{{\rm 2}}}{\rm )}^{{\rm 2}{\it m}{\rm -}{\it n}}-{{{\rm 1}}\over{{\rm 2}}}\sum\limits_{{\it m}={\rm 0}}^{\infty}{{\rm (}-{\rm 1}{\rm )}^{{\it m}}}{{{\rm (}{\rm 1}+\cdots+{{{\rm 1}}\over{{\it m}}}{\rm )}+{\rm (}{\rm 1}+\cdots+{{{\rm 1}}\over{{\it m}+{\it n}}}{\rm )}}\over{{\it m}{\rm !(}{\it m}+{\it n}{\rm )!}}}{\rm (}{{{\it x}}\over{{\rm 2}}}{\rm )}^{{\rm 2}{\it m}{\rm +}{\it n}} {\it y}_{{\rm 2}} {\it J}_{{\it n}}{\rm (}{\it x}{\rm )} {\it Y}_{{\it n}}{\rm (}{\it x}{\rm )}={\it a}{\rm (}{\it y}_{{\rm 2}}{\rm (}{\it x}{\rm )}+{\it bJ}_{{\it n}}{\rm (}{\it x}{\rm ))} {\it a}={{{\rm 2}}\over{{\rm \pi}}} {\it b}={\rm \gamma}-{\it Ln}{\rm 2} {\it Y}_{{\it n}}{\rm (}{\it x}{\rm )}={{{\rm 2}}\over{{\rm \pi}}}\left({{\it y}_{{\rm 2}}{\rm (}{\it x}{\rm )}+{\rm (}{\rm \gamma}-{\it Ln}{\rm 2}{\rm )}{\it J}_{{\it n}}{\rm (}{\it x}{\rm )}}\right) {\it Y}_{{\it n}}{\rm (}{\it x}{\rm )}={{{\rm 2}}\over{{\rm \pi}}}\left({{\rm (}{\it Lnx}+{\rm \gamma}-{\it Ln}{\rm 2}{\rm )}{\it J}_{{\it n}}{\rm (}{\it x}{\rm )}-{{{\rm 1}}\over{{\rm 2}}}\sum\limits_{{\it m}{\rm =0}}^{{\it n}{\rm -1}}{{{{\rm (}{\it n}-{\it m}-{\rm 1}{\rm )!}}\over{{\it m}{\rm !}}}}{\rm (}{{{\it x}}\over{{\rm 2}}}{\rm )}^{{\rm 2}{\it m}{\rm -}{\it n}}-{{{\rm 1}}\over{{\rm 2}}}\sum\limits_{{\it m}={\rm 0}}^{\infty}{{\rm (}-{\rm 1}{\rm )}^{{\it m}}}{{{\rm (}{\rm 1}+\cdots+{{{\rm 1}}\over{{\it m}}}{\rm )}+{\rm (}{\rm 1}+\cdots+{{{\rm 1}}\over{{\it m}+{\it n}}}{\rm )}}\over{{\it m}{\rm !(}{\it m}+{\it n}{\rm )!}}}{\rm (}{{{\it x}}\over{{\rm 2}}}{\rm )}^{{\rm 2}{\it m}{\rm +}{\it n}}}\right) ={{{\rm 2}}\over{{\rm \pi}}}{\rm (}{\it Lnx}+{\rm \gamma}-{\it Ln}{\rm 2}{\rm )}{\it J}_{{\it n}}{\rm (}{\it x}{\rm )}-{{{\rm 1}}\over{{\rm \pi}}}\sum\limits_{{\it m}{\rm =0}}^{{\it n}{\rm -1}}{{{{\rm (}{\it n}-{\it m}-{\rm 1}{\rm )!}}\over{{\it m}{\rm !}}}}{\rm (}{{{\it x}}\over{{\rm 2}}}{\rm )}^{{\rm 2}{\it m}{\rm -}{\it n}}-{{{\rm 1}}\over{{\rm \pi}}}\sum\limits_{{\it m}={\rm 0}}^{\infty}{{\rm (}-{\rm 1}{\rm )}^{{\it m}}}{{{\rm (}{\rm 1}+\cdots+{{{\rm 1}}\over{{\it m}}}{\rm )}+{\rm (}{\rm 1}+\cdots+{{{\rm 1}}\over{{\it m}+{\it n}}}{\rm )}}\over{{\it m}{\rm !(}{\it m}+{\it n}{\rm )!}}}{\rm (}{{{\it x}}\over{{\rm 2}}}{\rm )}^{{\rm 2}{\it m}{\rm +}{\it n}}","['ordinary-differential-equations', 'power-series', 'special-functions', 'bessel-functions', 'derivation-of-formulae']"
71,Solve $xdx-ydy=y^2(x^2-y^2)dy$,Solve,xdx-ydy=y^2(x^2-y^2)dy,"Question: $$xdx-ydy=y^2(x^2-y^2)dy$$ I'm having trouble matching the solution in the book, which is $\frac{1}{2}\ln(x^2-y^2)=\frac{1}{3}y^3+C$ . I'm getting an integral that requires the incomplete gamma function. My attempt: Rewrite the equation: $x + (-(x^2 - y^2) y^2 - y)\frac{dy}{dx} = 0$ This is not an exact equation, but I found the integrating factor: $μ(y) = e^{-(2 y^3)/3}$ Multiply both sides of $x + \frac{dy}{dx} (-(x^2 - y^2) y^2 - y) = 0$ by $μ(y):$ $xe^{-\frac{2}{3}y^3} + (e^{-\frac{2}{3} y^3} (y^3 - x^2 y - 1) y) \frac{dy}{dx} = 0$ Let $R(x,y) =xe^{-\frac{2}{3}y^3} $ and $S(x,y) = (e^{-\frac{2}{3} y^3} (y^3 - x^2 y - 1) y)$ . So I want to seek $f(x,y)$ such that $\frac{\partial f(x,y)}{x} = R(x,y)$ and $\frac{\partial f(x,y)}{y} = S(x,y)$ Integrating w.r.t $x$ : $f(x,y) = \int xe^{-\frac{2}{3}y^3}\,dx = \frac{1}{2}x^2 e^{-\frac{2}{3}y^3} + g(y)$ $\frac{dg(y)}{dy} = e^{-\frac{2}{3}y} y (y^3 - 1)$ Integrating w.r.t $y$ : $g(y) = \int e^{-\frac{2}{3}y} y (y^3 - 1) dy = ?$ I'm stuck here.","Question: I'm having trouble matching the solution in the book, which is . I'm getting an integral that requires the incomplete gamma function. My attempt: Rewrite the equation: This is not an exact equation, but I found the integrating factor: Multiply both sides of by Let and . So I want to seek such that and Integrating w.r.t : Integrating w.r.t : I'm stuck here.","xdx-ydy=y^2(x^2-y^2)dy \frac{1}{2}\ln(x^2-y^2)=\frac{1}{3}y^3+C x + (-(x^2 - y^2) y^2 - y)\frac{dy}{dx} = 0 μ(y) = e^{-(2 y^3)/3} x + \frac{dy}{dx} (-(x^2 - y^2) y^2 - y) = 0 μ(y): xe^{-\frac{2}{3}y^3} + (e^{-\frac{2}{3} y^3} (y^3 - x^2 y - 1) y) \frac{dy}{dx} = 0 R(x,y) =xe^{-\frac{2}{3}y^3}  S(x,y) = (e^{-\frac{2}{3} y^3} (y^3 - x^2 y - 1) y) f(x,y) \frac{\partial f(x,y)}{x} = R(x,y) \frac{\partial f(x,y)}{y} = S(x,y) x f(x,y) = \int xe^{-\frac{2}{3}y^3}\,dx = \frac{1}{2}x^2 e^{-\frac{2}{3}y^3} + g(y) \frac{dg(y)}{dy} = e^{-\frac{2}{3}y} y (y^3 - 1) y g(y) = \int e^{-\frac{2}{3}y} y (y^3 - 1) dy = ?","['ordinary-differential-equations', 'indefinite-integrals']"
72,The lyapunov function of gradient system,The lyapunov function of gradient system,,"Given a dynamical system $$\frac{dx}{dt}=-\nabla f(x)$$ which $x=0$ is the only equilibrium point, i.e. $-\nabla f(x)|_{x=0}=0$ . I am reading this tutorial , and it states: $f(x)$ is a lyapunov function such that $x=0$ is a locally asymptotic stable equilibrium point. I am confusing about this statement because from my understanding, to be a lyapunov function, the function should satisfy the following three conditions. However I can only see the condition (3) is satisfied and have no idea how condition (1) (2) are satisfied. Could someone help me to understand? Thanks a lot! (1) $f(x)|_{x=0}=0$ (2) $f(x)|_{x\neq 0}>0$ (3) $\frac{df(x)}{dt}|_{x\neq 0}<0$ Note: Condition (3) holds, since $\frac{df(x)}{dt}|_{x\neq 0}=\nabla f(x)\cdot (-\nabla f(x))|_{x\neq 0}=-\|\nabla f(x)\|^2_{x\neq 0}<0$ . But I have no idea about condition (1) and (2). I think maybe I have a misunderstanding of this point, maybe this would be correct: $f(x)$ is not neccessary to be a lyapunov function. If in addition, $f(x)$ meets the conditions (1) (2), then it is a lyapunov funciton. Could someone help me to clarify this point? Thanks a lot for any suggestion.","Given a dynamical system which is the only equilibrium point, i.e. . I am reading this tutorial , and it states: is a lyapunov function such that is a locally asymptotic stable equilibrium point. I am confusing about this statement because from my understanding, to be a lyapunov function, the function should satisfy the following three conditions. However I can only see the condition (3) is satisfied and have no idea how condition (1) (2) are satisfied. Could someone help me to understand? Thanks a lot! (1) (2) (3) Note: Condition (3) holds, since . But I have no idea about condition (1) and (2). I think maybe I have a misunderstanding of this point, maybe this would be correct: is not neccessary to be a lyapunov function. If in addition, meets the conditions (1) (2), then it is a lyapunov funciton. Could someone help me to clarify this point? Thanks a lot for any suggestion.",\frac{dx}{dt}=-\nabla f(x) x=0 -\nabla f(x)|_{x=0}=0 f(x) x=0 f(x)|_{x=0}=0 f(x)|_{x\neq 0}>0 \frac{df(x)}{dt}|_{x\neq 0}<0 \frac{df(x)}{dt}|_{x\neq 0}=\nabla f(x)\cdot (-\nabla f(x))|_{x\neq 0}=-\|\nabla f(x)\|^2_{x\neq 0}<0 f(x) f(x),"['ordinary-differential-equations', 'dynamical-systems', 'lyapunov-functions', 'gradient-flows']"
73,Integrating factor $e^{\int \frac{1}{x} dx}$ in differential equation,Integrating factor  in differential equation,e^{\int \frac{1}{x} dx},"When integrating $\int \frac{1}{x} dx$ , we typically write the integrated expression as $ln|x| + C$ . The absolute value of the $x$ -variable is introduced to account for the scenario where we have $ln(-x)$ and $x$ is a negative number. In a calculus book I am currently working with, the following differential equation is used in an example: $$xy'+y=3x^{2} +4x, x\neq 0$$ In this case the integrating factor becomes $e^{\int \frac{1}{x} dx}$ . The textbook example then states that $e^{\int \frac{1}{x} dx}=e^{ln x} = x$ . Question : Why is it that we do not have to take the absolute value of the $x$ -variable here? I could understand this if it was explicitly stated that $x>0$ in the given problem, but this is not stated. All we know is that $x\neq0$ . So how does this account for the second scenario outlined above? If anyone can explain this to me, then I would greatly appreciate it!","When integrating , we typically write the integrated expression as . The absolute value of the -variable is introduced to account for the scenario where we have and is a negative number. In a calculus book I am currently working with, the following differential equation is used in an example: In this case the integrating factor becomes . The textbook example then states that . Question : Why is it that we do not have to take the absolute value of the -variable here? I could understand this if it was explicitly stated that in the given problem, but this is not stated. All we know is that . So how does this account for the second scenario outlined above? If anyone can explain this to me, then I would greatly appreciate it!","\int \frac{1}{x} dx ln|x| + C x ln(-x) x xy'+y=3x^{2} +4x, x\neq 0 e^{\int \frac{1}{x} dx} e^{\int \frac{1}{x} dx}=e^{ln x} = x x x>0 x\neq0","['integration', 'ordinary-differential-equations', 'logarithms']"
74,Need help solving this set of differential equations of motion,Need help solving this set of differential equations of motion,,"I'm a masters student and I have a set of Differential equations I need to solve for my research. For context they are equations of motion for scalar fields coupled to gravity, though I suppose that's not really relevant to the maths involved. Specifically, they take the form $$\chi''(\rho)+2A'(\rho)\chi'(\rho)-(\chi'(\rho))^2-12=0$$ $$A''(\rho)-A'(\rho)\chi'(\rho)+2(A'(\rho))^2-24=0$$ $$2(A'(\rho))^2-2(\chi'(\rho))^2-24=0$$ Now, I already know that there are solutions where $\chi'$ and A' are constant ( $\chi'=2$ , $A'=4$ ). However, I know from existing literature using a different number of dimensions that there should also be another solution taking the form of a sum of (natural) logarithms of hyperbolic functions, such that when $\rho\rightarrow\infty$ , $\chi$ and A tend to the constant solutions. The problem is that for the life of me I cannot figure out how to find this additional solution. I have tried writing out trial solutions of the form $$\chi=\chi_0+\chi_1\log(\cosh(\chi_2\rho))+\chi_3\log(\sinh(\chi_4\rho))$$ $$A=A_0+A_1\log(\cosh(A_2\rho))+A_3\log(\sinh(A_4\rho))$$ But despite my best efforts I haven't been able to decipher any kind of solution or determine the constants so far. I feel a little embarrassed because I should probably be able to solve this kind of thing by now but I desperately need some help here. Im not even really sure where to start, apart from redefining $\chi_4$ and $A_4$ in terms of the other constants. I'm using mathematica if that helps at all. Apologies if this is a stupid question.","I'm a masters student and I have a set of Differential equations I need to solve for my research. For context they are equations of motion for scalar fields coupled to gravity, though I suppose that's not really relevant to the maths involved. Specifically, they take the form Now, I already know that there are solutions where and A' are constant ( , ). However, I know from existing literature using a different number of dimensions that there should also be another solution taking the form of a sum of (natural) logarithms of hyperbolic functions, such that when , and A tend to the constant solutions. The problem is that for the life of me I cannot figure out how to find this additional solution. I have tried writing out trial solutions of the form But despite my best efforts I haven't been able to decipher any kind of solution or determine the constants so far. I feel a little embarrassed because I should probably be able to solve this kind of thing by now but I desperately need some help here. Im not even really sure where to start, apart from redefining and in terms of the other constants. I'm using mathematica if that helps at all. Apologies if this is a stupid question.",\chi''(\rho)+2A'(\rho)\chi'(\rho)-(\chi'(\rho))^2-12=0 A''(\rho)-A'(\rho)\chi'(\rho)+2(A'(\rho))^2-24=0 2(A'(\rho))^2-2(\chi'(\rho))^2-24=0 \chi' \chi'=2 A'=4 \rho\rightarrow\infty \chi \chi=\chi_0+\chi_1\log(\cosh(\chi_2\rho))+\chi_3\log(\sinh(\chi_4\rho)) A=A_0+A_1\log(\cosh(A_2\rho))+A_3\log(\sinh(A_4\rho)) \chi_4 A_4,"['ordinary-differential-equations', 'systems-of-equations', 'nonlinear-system']"
75,Observing the limiting behavior of the ODE $z' = z(z-a)(1-z)$,Observing the limiting behavior of the ODE,z' = z(z-a)(1-z),"Consider the ODE given by $$\left\{ \begin{aligned} z'(t) &= z(t) \cdot (z(t) - a) \cdot (1-z(t)) && t>0\\ z(0) &= z_0 \end{aligned} \right.$$ where $z_0 \in (0,a)$ and $a \in (0,1)$ . I would like to show that $$\lim_{t\to \infty} z(t) = 0$$ in this scenario - preferably whilst avoid solving the ODE explicitly (since I have reason to believe it should be an ""obvious"" result). As context, this comes up in solving the nonlinear PDE $$\left\{ \begin{aligned} &u_t - \Delta u = u(u-a)(1-u) &&(x,t) \in \Omega \times (0,\infty) \\ &\partial_n u \equiv 0 &&(x,t) \in \partial \Omega \times (0,\infty) \\ &u(x,0) = \varphi(x) &&x \in \Omega \end{aligned} \right.$$ wherein: $\Omega \subseteq \mathbb{R}^n$ $\varphi(x) \ge 0$ $\varphi\not \equiv 0$ $\| \varphi\|_\infty < a$ which is Example $3.4$ in Mingxin Wang's Nonlinear Second Order Parabolic Equations (ISBN-10: 0367711982). It is claimed without proof that $$ z_0 < a  \implies z(t;z_0) \xrightarrow[\text{uniformly}]{t \to +\infty} 0$$ and $$ z_0 > a  \implies z(t;z_0) \xrightarrow[\text{uniformly}]{t \to +\infty} 1 $$ but I cannot for the life of me see why. Clearly, $z_0 < a$ gives $z'(t) < 0$ at $t=0$ , and similarly $z_0 > a$ gives $z'(t) > 0$ there. However, I'm not sure what to do with this information; what does it imply, if anything, about the future times? Ideally we'd be able to bound $z$ above/below as needed and use some sort of monotone convergence theorem to establish the desired result. If we do calculate an explicit solution to the ODE ( Wolfram link ), then we get a result that is implicit and uses logarithms: without accounting for $z_0$ , we'd have $$C + t = \frac{1}{a(a-1)} \Big( a \ln(1 - z(t)) - (a-1) \ln(z(t)) - \ln(z(t) - a) \Big)$$ and if we desire this to be well-defined over the reals, then we see that $$0 < a < z(t) < 1$$ in the scenario I wish to focus on (where $z_0 < a$ ). This at least gives upper and lower bounds on $z$ , but it's not clear if those are the tightest (or if, indeed, some amount of information is lost in solving the ODE). Does anyone have any ideas? Hopefully I'm just overlooking something obvious.","Consider the ODE given by where and . I would like to show that in this scenario - preferably whilst avoid solving the ODE explicitly (since I have reason to believe it should be an ""obvious"" result). As context, this comes up in solving the nonlinear PDE wherein: which is Example in Mingxin Wang's Nonlinear Second Order Parabolic Equations (ISBN-10: 0367711982). It is claimed without proof that and but I cannot for the life of me see why. Clearly, gives at , and similarly gives there. However, I'm not sure what to do with this information; what does it imply, if anything, about the future times? Ideally we'd be able to bound above/below as needed and use some sort of monotone convergence theorem to establish the desired result. If we do calculate an explicit solution to the ODE ( Wolfram link ), then we get a result that is implicit and uses logarithms: without accounting for , we'd have and if we desire this to be well-defined over the reals, then we see that in the scenario I wish to focus on (where ). This at least gives upper and lower bounds on , but it's not clear if those are the tightest (or if, indeed, some amount of information is lost in solving the ODE). Does anyone have any ideas? Hopefully I'm just overlooking something obvious.","\left\{
\begin{aligned}
z'(t) &= z(t) \cdot (z(t) - a) \cdot (1-z(t)) && t>0\\
z(0) &= z_0
\end{aligned}
\right. z_0 \in (0,a) a \in (0,1) \lim_{t\to \infty} z(t) = 0 \left\{
\begin{aligned}
&u_t - \Delta u = u(u-a)(1-u) &&(x,t) \in \Omega \times (0,\infty) \\
&\partial_n u \equiv 0 &&(x,t) \in \partial \Omega \times (0,\infty) \\
&u(x,0) = \varphi(x) &&x \in \Omega
\end{aligned}
\right. \Omega \subseteq \mathbb{R}^n \varphi(x) \ge 0 \varphi\not \equiv 0 \| \varphi\|_\infty < a 3.4  z_0 < a  \implies z(t;z_0) \xrightarrow[\text{uniformly}]{t \to +\infty} 0 
z_0 > a  \implies z(t;z_0) \xrightarrow[\text{uniformly}]{t \to +\infty} 1
 z_0 < a z'(t) < 0 t=0 z_0 > a z'(t) > 0 z z_0 C + t = \frac{1}{a(a-1)} \Big( a \ln(1 - z(t)) - (a-1) \ln(z(t)) - \ln(z(t) - a) \Big) 0 < a < z(t) < 1 z_0 < a z","['ordinary-differential-equations', 'limits', 'partial-differential-equations', 'uniform-convergence']"
76,How to solve this nonlinear diff eq of celestial mechanics?,How to solve this nonlinear diff eq of celestial mechanics?,,"$$(\dot{r})^2 = \frac{2 \mu}{r} + 2h$$ Where mu and h are constants. I have no idea how to solve it, maybe there is a trick I didn't know. The only thing that came in mind is to integrate $$\int \frac{dr}{\sqrt{\frac{2 \mu}{r} + 2h}} = \int dt$$ but I don't think this is really a solution, since I don't know too how to evaluate the integral in terms of elementary functions. So, any tips?","Where mu and h are constants. I have no idea how to solve it, maybe there is a trick I didn't know. The only thing that came in mind is to integrate but I don't think this is really a solution, since I don't know too how to evaluate the integral in terms of elementary functions. So, any tips?",(\dot{r})^2 = \frac{2 \mu}{r} + 2h \int \frac{dr}{\sqrt{\frac{2 \mu}{r} + 2h}} = \int dt,"['calculus', 'ordinary-differential-equations', 'indefinite-integrals']"
77,Application of Rolle's,Application of Rolle's,,"Suppose $q$ is a nonzero function of a real-variable such that $$u^2q''(u)+uq'(u)=u^2q(u)+q(u)$$ for all $u$ . Assume there exist $x,y$ such that $q(x)=q(y)=0$ . By Rolle's there exists $x<z<y$ such that $q'(z)=0$ . Plugging $z$ into the above equation is $$z^2q''(z)+zq'(z)=z^2q(z)+q(z)\iff z^2q''(z)=z^2q(z)+q(z).$$ And plugging $x,y$ in the equation is $$x^2q''(x)+xq'(x)=0\land y^2q''(y)+yq'(y)=0\implies xq''(x)+q(x)=0\land yq''(y)+q(y)=0.$$ I try to deduce contradiction from above but I don't know the next step. One strategy can be to show one of three equalities is actually is strict inequality. Another one can be to pursue second derivatives in the intervals $(x,z)$ and $(z,y)$ by mean value theorem. But none of these seem to show anything important. I would appreciate a hint.",Suppose is a nonzero function of a real-variable such that for all . Assume there exist such that . By Rolle's there exists such that . Plugging into the above equation is And plugging in the equation is I try to deduce contradiction from above but I don't know the next step. One strategy can be to show one of three equalities is actually is strict inequality. Another one can be to pursue second derivatives in the intervals and by mean value theorem. But none of these seem to show anything important. I would appreciate a hint.,"q u^2q''(u)+uq'(u)=u^2q(u)+q(u) u x,y q(x)=q(y)=0 x<z<y q'(z)=0 z z^2q''(z)+zq'(z)=z^2q(z)+q(z)\iff z^2q''(z)=z^2q(z)+q(z). x,y x^2q''(x)+xq'(x)=0\land y^2q''(y)+yq'(y)=0\implies xq''(x)+q(x)=0\land yq''(y)+q(y)=0. (x,z) (z,y)","['ordinary-differential-equations', 'derivatives', 'continuity']"
78,"Solving the system of ODE's $f'(t)=kg(t)e^{i\delta t}, g(t)=-kf(t)e^{-i\delta t}$",Solving the system of ODE's,"f'(t)=kg(t)e^{i\delta t}, g(t)=-kf(t)e^{-i\delta t}","So I want to solve a system of differential equations with the two equations $$f'(t)=kg(t)e^{i\delta t}$$ $$g'(t)=-kf(t)e^{-i\delta t}$$ With $k,\delta$ being real valued constants. I'm unfortunately not very experienced with coupled differential equations like this. My initial thought was to perhaps use the laplace transform, but since I have products of two functions of $t$ on the RHS, I'm not quite sure how I'd go about transforming these. Any help here would be deeply appreciated.","So I want to solve a system of differential equations with the two equations With being real valued constants. I'm unfortunately not very experienced with coupled differential equations like this. My initial thought was to perhaps use the laplace transform, but since I have products of two functions of on the RHS, I'm not quite sure how I'd go about transforming these. Any help here would be deeply appreciated.","f'(t)=kg(t)e^{i\delta t} g'(t)=-kf(t)e^{-i\delta t} k,\delta t",['ordinary-differential-equations']
79,General solution to a certain simple recurrence relation,General solution to a certain simple recurrence relation,,"This is follow up to a question answered here https://math.stackexchange.com/a/4224037/168758 Fix $\lambda \in [0, 1)$ . Define $m_{-1}(\lambda) = m_0(\lambda) = 1$ , and for $k \ge 1$ , define $m_k(\lambda) \in \mathbb R$ by $$ m_{k}(\lambda) = \frac{1}{k(1-\lambda)^2} \Big((2k-3)(1+\lambda) m_{k-1}(\lambda) - (k-3)m_{k-2}(\lambda)\Big). \quad (\star) $$ It is easy to deduce that $m_1(\lambda)=\dfrac{1}{(1-\lambda)^2}(-(1+\lambda)+2) = \dfrac{1}{1-\lambda}$ and $m_2(\lambda) = \dfrac{1}{2(1-\lambda)^2} = \dfrac{1}{2(1-\lambda)^2}((1+\lambda)(1-\lambda)^{-1}+1)=\dfrac{1}{(1-\lambda)^3}$ . Question. What is a general formula for $m_k(\lambda)$ , perhaps in terms of well-known (special ?) functions, valid for all $k \ge 1$ ?","This is follow up to a question answered here https://math.stackexchange.com/a/4224037/168758 Fix . Define , and for , define by It is easy to deduce that and . Question. What is a general formula for , perhaps in terms of well-known (special ?) functions, valid for all ?","\lambda \in [0, 1) m_{-1}(\lambda) = m_0(\lambda) = 1 k \ge 1 m_k(\lambda) \in \mathbb R 
m_{k}(\lambda) = \frac{1}{k(1-\lambda)^2} \Big((2k-3)(1+\lambda) m_{k-1}(\lambda) - (k-3)m_{k-2}(\lambda)\Big). \quad (\star)
 m_1(\lambda)=\dfrac{1}{(1-\lambda)^2}(-(1+\lambda)+2) = \dfrac{1}{1-\lambda} m_2(\lambda) = \dfrac{1}{2(1-\lambda)^2} = \dfrac{1}{2(1-\lambda)^2}((1+\lambda)(1-\lambda)^{-1}+1)=\dfrac{1}{(1-\lambda)^3} m_k(\lambda) k \ge 1","['real-analysis', 'ordinary-differential-equations', 'recurrence-relations', 'special-functions']"
80,What is the philosophy behind solving ODE's? (a question from someone who hasn't taken a formal ODE's class),What is the philosophy behind solving ODE's? (a question from someone who hasn't taken a formal ODE's class),,"I've started learning ODE's on my own and here is something that I don't understand. I've noticed that the book I am following (and all the other books that I have) is hand wavy when it comes to specifying the interval of the solution and doesn't realy worry too much about dividing by $0$ . I will provide an example: let's solve the ODE $$t^2x'=x^2+tx+t^2,$$ where $x=x(t)$ . We divide by $t$ and the equation becomes $x'=\left(\frac{x}{t}\right)^2+\frac{x}{t}+1$ . We make the variable change $y=\frac{x}{t}$ and after some computations we get that $\arctan y=\ln t+C$ for some constant $C\in \mathbb{R}$ . Now, the book says that this implies that $y=\tan(\ln t+C)$ , so $x=t\tan(\ln t+C), C\in \mathbb{R}$ . I have two questions here: Why can we divide by $t$ at the beginning? I mean, yes, I agree that this solves our equation, but aren't we kind of missing some solutions? Here is the first philosophical problem that I have with ODE's: is the focus on somehow obtaining a solution, even though we make some assumptions along the way, that is defined on some interval $I\subset \mathbb{R}$ that we don't even care if it is really really small rather than on trying to find all the differentiable functions that satisfy our identity (as the focus was in, say, functional equations that appear at high school math contests)? Why after $\arctan y=\ln t+C$ we may write that $y=\tan(\ln t+C)$ for any real constant $C$ ? I mean, the $\tan$ function is not defined everywhere and we most certainly can choose some $C$ such that for some $t$ we have $\ln t+C=\frac{\pi}{2}$ for instance. Is the philosophy here the same that I presented in 1 i.e. assuming that the interval on which our solution is defined is chosen appropriately so that everything makes sense?","I've started learning ODE's on my own and here is something that I don't understand. I've noticed that the book I am following (and all the other books that I have) is hand wavy when it comes to specifying the interval of the solution and doesn't realy worry too much about dividing by . I will provide an example: let's solve the ODE where . We divide by and the equation becomes . We make the variable change and after some computations we get that for some constant . Now, the book says that this implies that , so . I have two questions here: Why can we divide by at the beginning? I mean, yes, I agree that this solves our equation, but aren't we kind of missing some solutions? Here is the first philosophical problem that I have with ODE's: is the focus on somehow obtaining a solution, even though we make some assumptions along the way, that is defined on some interval that we don't even care if it is really really small rather than on trying to find all the differentiable functions that satisfy our identity (as the focus was in, say, functional equations that appear at high school math contests)? Why after we may write that for any real constant ? I mean, the function is not defined everywhere and we most certainly can choose some such that for some we have for instance. Is the philosophy here the same that I presented in 1 i.e. assuming that the interval on which our solution is defined is chosen appropriately so that everything makes sense?","0 t^2x'=x^2+tx+t^2, x=x(t) t x'=\left(\frac{x}{t}\right)^2+\frac{x}{t}+1 y=\frac{x}{t} \arctan y=\ln t+C C\in \mathbb{R} y=\tan(\ln t+C) x=t\tan(\ln t+C), C\in \mathbb{R} t I\subset \mathbb{R} \arctan y=\ln t+C y=\tan(\ln t+C) C \tan C t \ln t+C=\frac{\pi}{2}","['calculus', 'ordinary-differential-equations']"
81,Finding the region of attraction using a Lyapunov function,Finding the region of attraction using a Lyapunov function,,"I'm trying to find an estimate for the region of attraction of an equilibrium point. The notes from Nonlinear Control by Khalil suggest that defining $$ V(x) = x^TPx, $$ where $P$ is the solution of $$ PA+A^TP=-I, $$ will yield the best results for an estimate. It also assures that the estimate can be found from these types of Lyapunov functions for exponentially stable equilibrium points. The system I am studying is defined by $$ \begin{gathered} \dot{x}_1 = x_1 - x_1^3 + x_2 \\ \dot{x}_2 = x_1 - 3x_2.  \end{gathered} $$ The linealization matrix around the equilibrium point $x^* = \{\frac{2}{\sqrt{3}},\frac{2}{3\sqrt{3}}\}$ , is $A =  \begin{bmatrix}  -3 & 1 \\  1 & -3\\  \end{bmatrix} $ , and so, $P =  \begin{bmatrix}  3/16 & 1/16 \\  1/16 & 3/16\\  \end{bmatrix} $ . Given the previous values, I took the derivative of V(x) w.r.t. time and substituted the system in the equation (did it in Mathematica to try and simplify it as much as possible), giving $$ \dot{V}(x) = \frac{1}{8}\left(4x_1^2-3x_1^4+4x_1x_2-x_1^3x_2-8x_2^2\right). $$ Now I need to find a region where $\dot{V}(x)$ is negative definite, but when I try to use inequalities with the norm of $x$ I get only positive norms raised to a power, and so $\dot{V}(x)$ can never be negative definite. Using $$ |x_1|\leq||x||, \quad |x_1x_2|\leq\frac{1}{2}||x||^2, $$ I arrived at $$ \begin{gathered} \dot{V}(x) \leq \frac{1}{8}\left(4||x||^2+3||x||^4+2||x||^2+\frac{1}{2}||x||^4+8||x||^2\right) \\ \leq \frac{1}{8}\left(14||x||^2+\frac{7}{2}||x||^4\right). \end{gathered} $$ As you can see, it never is negative. Am I being overly aggresive with converting everything to norms? How can I find the region where $\dot{V}(x)$ is negative? Thanks in advance.","I'm trying to find an estimate for the region of attraction of an equilibrium point. The notes from Nonlinear Control by Khalil suggest that defining where is the solution of will yield the best results for an estimate. It also assures that the estimate can be found from these types of Lyapunov functions for exponentially stable equilibrium points. The system I am studying is defined by The linealization matrix around the equilibrium point , is , and so, . Given the previous values, I took the derivative of V(x) w.r.t. time and substituted the system in the equation (did it in Mathematica to try and simplify it as much as possible), giving Now I need to find a region where is negative definite, but when I try to use inequalities with the norm of I get only positive norms raised to a power, and so can never be negative definite. Using I arrived at As you can see, it never is negative. Am I being overly aggresive with converting everything to norms? How can I find the region where is negative? Thanks in advance.","
V(x) = x^TPx,
 P 
PA+A^TP=-I,
 
\begin{gathered}
\dot{x}_1 = x_1 - x_1^3 + x_2 \\
\dot{x}_2 = x_1 - 3x_2. 
\end{gathered}
 x^* = \{\frac{2}{\sqrt{3}},\frac{2}{3\sqrt{3}}\} A = 
\begin{bmatrix} 
-3 & 1 \\
 1 & -3\\ 
\end{bmatrix}
 P = 
\begin{bmatrix} 
3/16 & 1/16 \\
 1/16 & 3/16\\ 
\end{bmatrix}
 
\dot{V}(x) = \frac{1}{8}\left(4x_1^2-3x_1^4+4x_1x_2-x_1^3x_2-8x_2^2\right).
 \dot{V}(x) x \dot{V}(x) 
|x_1|\leq||x||, \quad |x_1x_2|\leq\frac{1}{2}||x||^2,
 
\begin{gathered}
\dot{V}(x) \leq \frac{1}{8}\left(4||x||^2+3||x||^4+2||x||^2+\frac{1}{2}||x||^4+8||x||^2\right) \\
\leq \frac{1}{8}\left(14||x||^2+\frac{7}{2}||x||^4\right).
\end{gathered}
 \dot{V}(x)","['ordinary-differential-equations', 'dynamical-systems', 'stability-theory', 'lyapunov-functions', 'basins-of-attraction']"
82,Can there exist a unique solution to an initial value problem if the hypotheses of the existence and uniqueness theorem are not satisfied?,Can there exist a unique solution to an initial value problem if the hypotheses of the existence and uniqueness theorem are not satisfied?,,"I have been thinking about this question for a while. I haven't found a definite answer, but I am led to believe that there can be a unique solution to an IVP outside of interval of validity. I just fail to prove it.","I have been thinking about this question for a while. I haven't found a definite answer, but I am led to believe that there can be a unique solution to an IVP outside of interval of validity. I just fail to prove it.",,['ordinary-differential-equations']
83,Laplace transform of a solution of differential equation,Laplace transform of a solution of differential equation,,"Consider the differenial equation $$t\frac{d^2y}{dt^2}+2\frac{dy}{dt}+ty=0\tag{1}$$ $$t>0,y(0+)=1,y'(0+)=0$$ If $Y(s)$ is Laplace transform of $y(t)$ , then find the value of $Y(1)$ . My attempt : we know that $\mathcal{L}({tf(t)})=-\frac{d}{ds}F(s)$ ,where $F(s)$ is the Laplace transform of $f(t)$ . Now taking Laplace transform both side in equation $(1)$ , we have $$\begin{align*}&\mathcal{L}[t\frac{d^2y}{dt^2}+2\frac{dy}{dt}+ty]=0 \\\Rightarrow&\mathcal{L}[t\frac{d^2y}{dt^2}]+2\mathcal{L}[\frac{dy}{dt}]+\mathcal{L}[ty]=0 \\\Rightarrow&-\frac{d}{ds}[s^2Y(s)-sy'(0)-y(0)]+2[sY(s)-y(0)]-\frac{d}{ds}Y(s)=0 \\\Rightarrow&-2sY(s)-s^2Y'(s)+2sY(s)-2-Y'(s)=0 \\\Rightarrow&Y'(s)=\frac{-2}{1+s^2} \\\Rightarrow&Y(s)=-2\tan^{-1}s+C\end{align*}$$ Where $C$ is some constant, therefore $Y(1)=-2\tan^{-1}(1)+C=-\frac{\pi}{2}+C$ . What I need to know is only constant $C$ . I tried my best but didn't found the value of this constant. Any help will be appreciable Thanks in advance.","Consider the differenial equation If is Laplace transform of , then find the value of . My attempt : we know that ,where is the Laplace transform of . Now taking Laplace transform both side in equation , we have Where is some constant, therefore . What I need to know is only constant . I tried my best but didn't found the value of this constant. Any help will be appreciable Thanks in advance.","t\frac{d^2y}{dt^2}+2\frac{dy}{dt}+ty=0\tag{1} t>0,y(0+)=1,y'(0+)=0 Y(s) y(t) Y(1) \mathcal{L}({tf(t)})=-\frac{d}{ds}F(s) F(s) f(t) (1) \begin{align*}&\mathcal{L}[t\frac{d^2y}{dt^2}+2\frac{dy}{dt}+ty]=0
\\\Rightarrow&\mathcal{L}[t\frac{d^2y}{dt^2}]+2\mathcal{L}[\frac{dy}{dt}]+\mathcal{L}[ty]=0
\\\Rightarrow&-\frac{d}{ds}[s^2Y(s)-sy'(0)-y(0)]+2[sY(s)-y(0)]-\frac{d}{ds}Y(s)=0
\\\Rightarrow&-2sY(s)-s^2Y'(s)+2sY(s)-2-Y'(s)=0
\\\Rightarrow&Y'(s)=\frac{-2}{1+s^2}
\\\Rightarrow&Y(s)=-2\tan^{-1}s+C\end{align*} C Y(1)=-2\tan^{-1}(1)+C=-\frac{\pi}{2}+C C","['calculus', 'ordinary-differential-equations', 'laplace-transform', 'inverse-laplace']"
84,"Without solving explicitly show that for the IVP $x'=x^{3}-x,x(0)=0.5$ the solution converges to $0$ for $t\rightarrow\infty$",Without solving explicitly show that for the IVP  the solution converges to  for,"x'=x^{3}-x,x(0)=0.5 0 t\rightarrow\infty","I am trying to determine whether the solution to the following IVP converges to $0$ for $t\rightarrow\infty$ . The IVP is $x'(t)=x^{3}-x=f(x)$ , $x(0)=\frac{1}{2}$ . By looking at the ""extended phase portrait"" I would infer  that this is indeed the case. However I would like to have solid argument. I would argue as following: Let $x(t)$ be the solution to the above IVP. For $x\in(0,1)$ $x^{3}-x<0$ . Therefore $x'(t)<0$ and $x(t)$ is decreasing. Since $\tilde{x}(t)=0$ is a (equilibrium) solution $x(t)>0$ , which follows from the uniqueness theorem. Hence, $0$ is a lower bound. Because $x(t)$ is bounded below and monotonically decreasing $lim_{t\rightarrow\infty}x(t)$ exists and $lim_{t\rightarrow\infty}x(t)\geq 0$ . Here is the part were I am stuck. I have two approaches. One works and the other doesn't I think. What I want to show is that $\lim_{t\rightarrow\infty}x'(t)=0$ . Because then $0=\lim_{t\rightarrow\infty}x'(t)=lim_{t\rightarrow\infty}f(x(t))=f(lim_{t\rightarrow\infty}x(t))=f(c)=c^{3}-c\iff c=0,c=1,c=-1$ . $c=-1$ is not possible, because the $x(t)$ would intersect $\tilde{x}(t)=0$ , hence by the uniqueness it follows that $x(t)=0$ and thus $0>x'{t}=0$ . If $c=1$ , then we can find $t_{1}\in\mathbb{R}_{>0}$ such that $|x(t_{1})-1|<\frac{1}{2}\iff x(0)=\frac{1}{2}<x(t_{1})$ . Hence, by the mean value theorem it follow that there is $\eta\in(0,t_{1})$ such that $f'(\eta)=\frac{x(t_{1})-x(0)}{t_{0}-0}>0$ , but $f'(\eta)<0$ . Therefore $c=0$ . Approach 1. Here I am stuck: From the inequality $x'(t)<0$ it follows that $0\geq lim_{t\rightarrow\infty}x'(t)$ . Since $x(t)$ is a solution and because of the continuity of the r.h.s. of the IVP it follows that $0\geq lim_{t\rightarrow\infty}x'(t)=lim_{t\rightarrow\infty}f(x(t))=f(lim_{t\rightarrow\infty}x(t))=f(c)=c^{3}-c$ . $(*)$ Therefore $c\in[0,1]$ or $c\in(-\infty,-1]$ . If the latter is the case then $x(t)$ would intersect with at least one equilibrium solution. Hence, this is not possible. Therefore $c\in[0,1]$ . I think I can rule out any $c\in[\frac{1}{2},1]$ , since then by the mean value theorem. $f'(\eta)>0$ , which is not possible. Can I somehow show that $f(c)=c^{3}-c\geq 0$ . Then it would follow that $\lim_{\rightarrow\infty}x'(t)=0$ , wouldn't it? But isn't it the case that $f(c)\leq 0$ , since $c\in[0,1]$ in equation $(*)$ ? Therefore we would have $0\leq f(c)\leq 0\iff f(c)=0$ . Hence, $\lim_{t\rightarrow\infty}x'(t)=f(c)=0$ Is there a shorter way? Approach 2: We know that $\lim_{t\rightarrow\infty}x(t)=c$ . Then $\lim_{t\rightarrow\infty}x'(t)=\lim_{t\rightarrow\infty}\lim_{h\rightarrow 0}\frac{x(t+h)-x(t)}{h}=\lim_{h\rightarrow 0}\lim_{t\rightarrow\infty}\frac{x(t+h)-x(t)}{h}=\lim_{h\rightarrow 0}\frac{c-c}{h}=0$ . I think all requirements for changing the order of the limits to be legal are met...or at least I hope so. I would really appreciate any help. Thank you very much in advance!","I am trying to determine whether the solution to the following IVP converges to for . The IVP is , . By looking at the ""extended phase portrait"" I would infer  that this is indeed the case. However I would like to have solid argument. I would argue as following: Let be the solution to the above IVP. For . Therefore and is decreasing. Since is a (equilibrium) solution , which follows from the uniqueness theorem. Hence, is a lower bound. Because is bounded below and monotonically decreasing exists and . Here is the part were I am stuck. I have two approaches. One works and the other doesn't I think. What I want to show is that . Because then . is not possible, because the would intersect , hence by the uniqueness it follows that and thus . If , then we can find such that . Hence, by the mean value theorem it follow that there is such that , but . Therefore . Approach 1. Here I am stuck: From the inequality it follows that . Since is a solution and because of the continuity of the r.h.s. of the IVP it follows that . Therefore or . If the latter is the case then would intersect with at least one equilibrium solution. Hence, this is not possible. Therefore . I think I can rule out any , since then by the mean value theorem. , which is not possible. Can I somehow show that . Then it would follow that , wouldn't it? But isn't it the case that , since in equation ? Therefore we would have . Hence, Is there a shorter way? Approach 2: We know that . Then . I think all requirements for changing the order of the limits to be legal are met...or at least I hope so. I would really appreciate any help. Thank you very much in advance!","0 t\rightarrow\infty x'(t)=x^{3}-x=f(x) x(0)=\frac{1}{2} x(t) x\in(0,1) x^{3}-x<0 x'(t)<0 x(t) \tilde{x}(t)=0 x(t)>0 0 x(t) lim_{t\rightarrow\infty}x(t) lim_{t\rightarrow\infty}x(t)\geq 0 \lim_{t\rightarrow\infty}x'(t)=0 0=\lim_{t\rightarrow\infty}x'(t)=lim_{t\rightarrow\infty}f(x(t))=f(lim_{t\rightarrow\infty}x(t))=f(c)=c^{3}-c\iff c=0,c=1,c=-1 c=-1 x(t) \tilde{x}(t)=0 x(t)=0 0>x'{t}=0 c=1 t_{1}\in\mathbb{R}_{>0} |x(t_{1})-1|<\frac{1}{2}\iff x(0)=\frac{1}{2}<x(t_{1}) \eta\in(0,t_{1}) f'(\eta)=\frac{x(t_{1})-x(0)}{t_{0}-0}>0 f'(\eta)<0 c=0 x'(t)<0 0\geq lim_{t\rightarrow\infty}x'(t) x(t) 0\geq lim_{t\rightarrow\infty}x'(t)=lim_{t\rightarrow\infty}f(x(t))=f(lim_{t\rightarrow\infty}x(t))=f(c)=c^{3}-c (*) c\in[0,1] c\in(-\infty,-1] x(t) c\in[0,1] c\in[\frac{1}{2},1] f'(\eta)>0 f(c)=c^{3}-c\geq 0 \lim_{\rightarrow\infty}x'(t)=0 f(c)\leq 0 c\in[0,1] (*) 0\leq f(c)\leq 0\iff f(c)=0 \lim_{t\rightarrow\infty}x'(t)=f(c)=0 \lim_{t\rightarrow\infty}x(t)=c \lim_{t\rightarrow\infty}x'(t)=\lim_{t\rightarrow\infty}\lim_{h\rightarrow 0}\frac{x(t+h)-x(t)}{h}=\lim_{h\rightarrow 0}\lim_{t\rightarrow\infty}\frac{x(t+h)-x(t)}{h}=\lim_{h\rightarrow 0}\frac{c-c}{h}=0","['ordinary-differential-equations', 'initial-value-problems']"
85,If $\int {yy''dx} = 3xy$ is it possible to find y'?,If  is it possible to find y'?,\int {yy''dx} = 3xy,"So one of the people I know on discord gave me this problem to solve for fun. I've tried using integration by parts on the left side and it didn't do anything. The integral transforms into the ODE $yy''- (3xy)'=0$ but it's non-linear and I have no idea how to solve that. Would a series solution work? Notice that I'm looking for y' not y so maybe there's some trick there? Any help is appreciated. Thank you for reading. Here, $y'$ and $y''$ denote the first and second order derivatives of $y$ w.r.t. $x$ .","So one of the people I know on discord gave me this problem to solve for fun. I've tried using integration by parts on the left side and it didn't do anything. The integral transforms into the ODE but it's non-linear and I have no idea how to solve that. Would a series solution work? Notice that I'm looking for y' not y so maybe there's some trick there? Any help is appreciated. Thank you for reading. Here, and denote the first and second order derivatives of w.r.t. .",yy''- (3xy)'=0 y' y'' y x,"['calculus', 'integration', 'ordinary-differential-equations']"
86,Second Order Finite Difference Scheme to Solve Initial Value Problem,Second Order Finite Difference Scheme to Solve Initial Value Problem,,"I would appreciate help in a problem that I cannot figure out where am I doing the mistake. The mistake could be something I am unaware of so please excuse my lack of knowledge. I am trying to solve the equation of motion, $$M \ddot{x} + K x = F,$$ where $M$ , $K$ , and $F$ are variables independent of $x$ , and $\ddot{x}=\frac{d^2x}{dt^2}$ . Using a central difference finite scheme, I rewrote the equation as, $$M \frac{ x_{i+1} - 2x_i + x_{i-1} }{ {\Delta t} ^2 } + Kx = F.$$ Then, I am solving this equation for $x_{i+1}$ at every time step $i$ as, $$ x_{i+1} = \frac{{\Delta t}^2}{M} \cdot \left( F - K x + \frac{M}{{\Delta t}^2} \cdot 2 x_i - \frac{M}{{\Delta t}^2} \cdot x_{i-1} \right).$$ Now to solve this problem, one needs two initial conditions, to my knowledge, namely, $x(0)$ and $\dot{x}(0)$ . At the first time step, I am setting $x_i$ to the $x(0)$ initial condition. Now I am stuck with two problems: What value should I set $x_{i-1}$ at first time step? How can I apply the initial condition $\dot{x}(0)$ ?","I would appreciate help in a problem that I cannot figure out where am I doing the mistake. The mistake could be something I am unaware of so please excuse my lack of knowledge. I am trying to solve the equation of motion, where , , and are variables independent of , and . Using a central difference finite scheme, I rewrote the equation as, Then, I am solving this equation for at every time step as, Now to solve this problem, one needs two initial conditions, to my knowledge, namely, and . At the first time step, I am setting to the initial condition. Now I am stuck with two problems: What value should I set at first time step? How can I apply the initial condition ?","M \ddot{x} + K x = F, M K F x \ddot{x}=\frac{d^2x}{dt^2} M \frac{ x_{i+1} - 2x_i + x_{i-1} }{ {\Delta t} ^2 } + Kx = F. x_{i+1} i  x_{i+1} = \frac{{\Delta t}^2}{M} \cdot \left( F - K x + \frac{M}{{\Delta t}^2} \cdot 2 x_i - \frac{M}{{\Delta t}^2} \cdot x_{i-1} \right). x(0) \dot{x}(0) x_i x(0) x_{i-1} \dot{x}(0)","['ordinary-differential-equations', 'numerical-methods', 'initial-value-problems']"
87,How does $\int \frac{1}{e^x +1} dx$ become $\ln{\big( \frac{1+e^x}{2} \big)} +1 -x $?,How does  become ?,\int \frac{1}{e^x +1} dx \ln{\big( \frac{1+e^x}{2} \big)} +1 -x ,I'm solving the equation $(1+e^x)yy'=e^{y}$ with the constraint $y(0)=0$ . I've got pretty far but I'm struggling in the last part. I got stuck solving this integral: $$\int \frac{1}{e^x +1} dx $$ which is $x-\ln{|1+e^x|}$ but in order to satisfy the constraint I need to make it $\ln{\big( \frac{1+e^x}{2} \big)} +1 -x $ . How does $\int \frac{1}{e^x +1} dx$ become $\ln{\big( \frac{1+e^x}{2} \big)} +1 -x $ ?,I'm solving the equation with the constraint . I've got pretty far but I'm struggling in the last part. I got stuck solving this integral: which is but in order to satisfy the constraint I need to make it . How does become ?,(1+e^x)yy'=e^{y} y(0)=0 \int \frac{1}{e^x +1} dx  x-\ln{|1+e^x|} \ln{\big( \frac{1+e^x}{2} \big)} +1 -x  \int \frac{1}{e^x +1} dx \ln{\big( \frac{1+e^x}{2} \big)} +1 -x ,"['calculus', 'ordinary-differential-equations']"
88,ODE $f''''(x)+f(x) = \cos{x} - \left( \sin{x} \right)^2$ with boundary conditions $f(0)=f(\pi); \quad \quad f'(0)=f'(\pi)$,ODE  with boundary conditions,f''''(x)+f(x) = \cos{x} - \left( \sin{x} \right)^2 f(0)=f(\pi); \quad \quad f'(0)=f'(\pi),"Today I've attended a Mathematical Methods for Physicist exam and I've found an ordinary differential equation that I could not solve. The exercise asked to find a solution to the equation $$ f''''(x)+f(x) = \cos{x} - \left( \sin{x} \right)^2$$ with boundary conditions $$ f(0)=f(\pi); \quad \quad f'(0)=f'(\pi)$$ First of all, I've written $\sin^2{x}$ = $\frac{1}{2}-\frac{1}{2} \cos{2x}$ ; then I've tried to expand $f(x)$ in Fourier series between $0$ and $\pi$ : $$f(x)=\sum_{n=0}^{\infty} \left( a_n \cos{nx} + b_n \sin{nx}\right)$$ Then I've imposed the first boundary condition. Since $f(0)$ is just the sum of $a_n$ , and $f(\pi)$ is the sum of $a_n (-1)^n$ , the condition is fulfilled if only the even terms are non-zero; hence: $$f(x) = \sum_{n=0}^{\infty} \left( a_{2n} \cos{2nx} + b_n \sin{nx} \right)$$ $$f'(x) = \sum_{n=0}^{\infty} \left( n b_n \cos{nx} - 2n \; a_{2n} \sin{2nx} \right)$$ At this point imposing the second boundary condition gave me a result specular to the first one (only even terms for $b_n$ ): $$f(x) = \sum_{n=0}^{\infty} \left( a_{2n} \cos{2nx} + b_{2n} \sin{2nx} \right)$$ Then I've finally substituted the expanded form of $f(x)$ into the differential equation in order to find the coefficients: $$f''''(x)+f(x) = \sum_{n=0}^{\infty} \left( a_{2n} (16n^4+1) \cos{2nx}  +  b_{2n} (16n^4+1) \sin{2nx} \right) = \cos{x}-\frac{1}{2}+\frac{1}{2}\cos{2x}$$ And now I'm stuck... I can find the coefficients for $n=0$ and $n=1$ , but I don't know what to do with the $\cos{x}$ term. Thanks in advance for any help or suggestion you may give, Lorenzo","Today I've attended a Mathematical Methods for Physicist exam and I've found an ordinary differential equation that I could not solve. The exercise asked to find a solution to the equation with boundary conditions First of all, I've written = ; then I've tried to expand in Fourier series between and : Then I've imposed the first boundary condition. Since is just the sum of , and is the sum of , the condition is fulfilled if only the even terms are non-zero; hence: At this point imposing the second boundary condition gave me a result specular to the first one (only even terms for ): Then I've finally substituted the expanded form of into the differential equation in order to find the coefficients: And now I'm stuck... I can find the coefficients for and , but I don't know what to do with the term. Thanks in advance for any help or suggestion you may give, Lorenzo", f''''(x)+f(x) = \cos{x} - \left( \sin{x} \right)^2  f(0)=f(\pi); \quad \quad f'(0)=f'(\pi) \sin^2{x} \frac{1}{2}-\frac{1}{2} \cos{2x} f(x) 0 \pi f(x)=\sum_{n=0}^{\infty} \left( a_n \cos{nx} + b_n \sin{nx}\right) f(0) a_n f(\pi) a_n (-1)^n f(x) = \sum_{n=0}^{\infty} \left( a_{2n} \cos{2nx} + b_n \sin{nx} \right) f'(x) = \sum_{n=0}^{\infty} \left( n b_n \cos{nx} - 2n \; a_{2n} \sin{2nx} \right) b_n f(x) = \sum_{n=0}^{\infty} \left( a_{2n} \cos{2nx} + b_{2n} \sin{2nx} \right) f(x) f''''(x)+f(x) = \sum_{n=0}^{\infty} \left( a_{2n} (16n^4+1) \cos{2nx}  +  b_{2n} (16n^4+1) \sin{2nx} \right) = \cos{x}-\frac{1}{2}+\frac{1}{2}\cos{2x} n=0 n=1 \cos{x},"['ordinary-differential-equations', 'fourier-series', 'mathematical-physics']"
89,Existence of curve of constant curvature connecting two points,Existence of curve of constant curvature connecting two points,,"Let $S$ be a two-dimensional Riemannian manifold, i.e., a surface. If $S$ is complete as a metric space, then it follows (by the Hopf–Rinow theorem) that any two points of $S$ can be joined by a (minimizing) geodesic. My question is: Assume that $S$ is complete. Can any two points of $S$ be joined by a curve of constant, nonzero (geodesic) curvature? The reason I am aking is that I read from Eisenhart's classic book ( A Treatise on the Differential Geometry of Curves and Surfaces ) the following statement, which seems to assume existence: ""Of all the curves of equal length joining two points, the one which, together with a fixed curve through the points, incloses the area of greatest extent, has constant geodesic curvature.""","Let be a two-dimensional Riemannian manifold, i.e., a surface. If is complete as a metric space, then it follows (by the Hopf–Rinow theorem) that any two points of can be joined by a (minimizing) geodesic. My question is: Assume that is complete. Can any two points of be joined by a curve of constant, nonzero (geodesic) curvature? The reason I am aking is that I read from Eisenhart's classic book ( A Treatise on the Differential Geometry of Curves and Surfaces ) the following statement, which seems to assume existence: ""Of all the curves of equal length joining two points, the one which, together with a fixed curve through the points, incloses the area of greatest extent, has constant geodesic curvature.""",S S S S S,"['ordinary-differential-equations', 'differential-geometry']"
90,Consider $\dot{x}=4x^{2}-16$.,Consider .,\dot{x}=4x^{2}-16,"I am solving the ODE above, it is a question from Strogatz Nonlinear dynamics and chaos, chapter 2 question 2.2.1. Question \begin{equation} \dot{x}=4x^{2}-16 \end{equation} Answer \begin{equation} \frac{\dot{x}}{4x^{2}-16} = 1\\ \frac{\dot{x}}{x^{2}-4} = 4\\  {{dx\over dt}\over x^2-4}=4 \\  {{dx\over dt}\over x^2-4}. dt=4. dt  \\  {dx\over x^2-4}=4dt \\ \int \frac{1}{x^{2}-4} dx = \int 4 dt \\ \frac{1}{4} \ln(\frac{x-2}{x+2}) = 4t + C_{1} \\ x = 2 \frac{1 + C_{2}e^{16t}}{1 - C_{2}e^{16t}} \end{equation} \begin{equation} C_{2}(t=0) = \frac{x-2}{x+2} \end{equation} Summary I am looking to understand the intermediary step in the proof above. How do we get to this step $\frac{1}{4} \ln(\frac{x-2}{x+2}) = 4t + C_{1} $ from the previous step. Can we remove the constant $\frac{1}{4} $ then integrate the remaining portion?","I am solving the ODE above, it is a question from Strogatz Nonlinear dynamics and chaos, chapter 2 question 2.2.1. Question Answer Summary I am looking to understand the intermediary step in the proof above. How do we get to this step from the previous step. Can we remove the constant then integrate the remaining portion?","\begin{equation}
\dot{x}=4x^{2}-16
\end{equation} \begin{equation}
\frac{\dot{x}}{4x^{2}-16} = 1\\
\frac{\dot{x}}{x^{2}-4} = 4\\
 {{dx\over dt}\over x^2-4}=4 \\
 {{dx\over dt}\over x^2-4}. dt=4. dt  \\
 {dx\over x^2-4}=4dt \\
\int \frac{1}{x^{2}-4} dx = \int 4 dt \\
\frac{1}{4} \ln(\frac{x-2}{x+2}) = 4t + C_{1} \\
x = 2 \frac{1 + C_{2}e^{16t}}{1 - C_{2}e^{16t}}
\end{equation} \begin{equation}
C_{2}(t=0) = \frac{x-2}{x+2}
\end{equation} \frac{1}{4} \ln(\frac{x-2}{x+2}) = 4t + C_{1}  \frac{1}{4} ","['calculus', 'ordinary-differential-equations', 'proof-explanation', 'solution-verification', 'physics']"
91,Proof to n-th order inhomogenous differential equation,Proof to n-th order inhomogenous differential equation,,"Let be $ I \subset \mathbb{R} $ an intervall and $ s \in C^{ ( \infty ) }( I) $ How can I show that every solution $ y \in C^{ (n)} (I) $ of $$  y^{ (n)} + \sum_{j=0}^{n-1} a_jy^{(j)} = s(x) $$ ( $ a_0,...,a_{n-1} \in \mathbb{R} $ constant ) is in $ C^{( \infty )} (I) $ ?",Let be an intervall and How can I show that every solution of ( constant ) is in ?," I \subset \mathbb{R}   s \in C^{ ( \infty ) }( I)   y \in C^{ (n)} (I)    y^{ (n)} + \sum_{j=0}^{n-1} a_jy^{(j)} = s(x)   a_0,...,a_{n-1} \in \mathbb{R}   C^{( \infty )} (I) ",['ordinary-differential-equations']
92,Differential of Derivative,Differential of Derivative,,"I have encountered with a problem, and I can't follow the intermediate steps. Consider the following differential equation: $$ y^{\prime \prime} = -2y + f(y)-0.5y^3 $$ Here, the first derivative is expressed as the following integral: $$ (y^{\prime})^2 = 2\int_0^{y^\prime} y^\prime dy^\prime  $$ Without giving the details of the intermediate steps, this equation is converted into the following equation using the differential equation given above: $$ (y^{\prime})^2 = -2 \int_{Y}^{y} \left( 2y - f(y)+0.5y^3 \right) dy$$ where $Y$ is the maximum value of $y$ when $y^\prime$ is zero. Here, I would like to learn how to express differential of a derivative $ dy^\prime $ in terms of $dy$ , and end up with the above equation. If I am not wrong, the following equality must hold: $$ y^\prime dy^\prime = y^{\prime \prime} dy $$ How can I prove this equality? I am confused to think a differential like $ dy^\prime = d(\frac{dy}{dx}) $ . Any help would be appreciated. Thanks.","I have encountered with a problem, and I can't follow the intermediate steps. Consider the following differential equation: Here, the first derivative is expressed as the following integral: Without giving the details of the intermediate steps, this equation is converted into the following equation using the differential equation given above: where is the maximum value of when is zero. Here, I would like to learn how to express differential of a derivative in terms of , and end up with the above equation. If I am not wrong, the following equality must hold: How can I prove this equality? I am confused to think a differential like . Any help would be appreciated. Thanks.", y^{\prime \prime} = -2y + f(y)-0.5y^3   (y^{\prime})^2 = 2\int_0^{y^\prime} y^\prime dy^\prime    (y^{\prime})^2 = -2 \int_{Y}^{y} \left( 2y - f(y)+0.5y^3 \right) dy Y y y^\prime  dy^\prime  dy  y^\prime dy^\prime = y^{\prime \prime} dy   dy^\prime = d(\frac{dy}{dx}) ,"['ordinary-differential-equations', 'differential']"
93,Poincare Bendixson theorem,Poincare Bendixson theorem,,"Poincare-Bendixson states that if the $\omega$ -limit set of the trajectory doesn't contain critical point, then the $\omega$ -limit set is a periodic orbit, I am just wondering if there is an example showing that $\omega$ -limit set contains both critical point and periodic orbit","Poincare-Bendixson states that if the -limit set of the trajectory doesn't contain critical point, then the -limit set is a periodic orbit, I am just wondering if there is an example showing that -limit set contains both critical point and periodic orbit",\omega \omega \omega,['ordinary-differential-equations']
94,Similar Matrices and Conjugate Flows,Similar Matrices and Conjugate Flows,,"This question is an attempt to resuscitate and generalize this question , which lived one hour and was deleted by its author for reasons unknown. The notion of topological conjugacy of flows is central to the theories of dynamical systems and ordinary differential equations, for it formalizes the concept that the orbit structure of two flows may be equivalent; that is, that the flows exhibit essentially the same behavior.  Specifically, we say that two flows $\phi_A$ and $\phi_B$ on a topological space $X$ are conjugate when there exists a homeomorphism $h:X \to X \tag 1$ such that for all $x \in X$ and $t \in \Bbb R$ $h(\phi_A(x, t)) = \phi_B(h(x), t). \tag 2$ A simple and particularly useful class of flows arises from linear time-invariant ordinary differential equations $\dot x = Ax, \tag 3$ where we assume $x \in \Bbb C^n \tag 4$ and $A \in M(n, \Bbb C), \tag 5$ the set of square complex matrices of size $n$ ; the solution to (3) which takes the value $x(t_0)$ at $t_0$ is well-known to be $x(t) = e^{A(t - t_0)}x(t_0); \tag 6$ thus the flow of (3) is given by $\phi_A(x, t) = e^{At} x. \tag 7$ We recall that two matrices $A, B \in M(n, \Bbb C) \tag 8$ are said to be similar if there exists a non-singular matrix $T \in M(n, \Bbb C) \tag 9$ such that $B = TAT^{-1}. \tag{10}$ The Question is then: Show that there is a linear homeomorphism $T: \Bbb C^n \to \Bbb C^n \tag{11}$ conjugating $\phi_A(x, t)$ and $\phi_B(x, t)$ if and only if $A$ and $B$ are $T$ -similar, that is, $B = TAT^{-1}. \tag{12}$ If $A$ and $B$ are real matrices, must $T$ also be real? Note Added in Edit, Saturday 8 February 2020, 11:25 AM PST: In light of Conifold's comment, we see that the existence of a suitable real $T$ gives rise to a purely imaginary matrix $iT$ which also satisfies the requisite conditions.  Therefore I modify my closing question above and ask If $A$ and $B$ are real matrices, must $T$ be either purely real or purely imaginary? End of Note.","This question is an attempt to resuscitate and generalize this question , which lived one hour and was deleted by its author for reasons unknown. The notion of topological conjugacy of flows is central to the theories of dynamical systems and ordinary differential equations, for it formalizes the concept that the orbit structure of two flows may be equivalent; that is, that the flows exhibit essentially the same behavior.  Specifically, we say that two flows and on a topological space are conjugate when there exists a homeomorphism such that for all and A simple and particularly useful class of flows arises from linear time-invariant ordinary differential equations where we assume and the set of square complex matrices of size ; the solution to (3) which takes the value at is well-known to be thus the flow of (3) is given by We recall that two matrices are said to be similar if there exists a non-singular matrix such that The Question is then: Show that there is a linear homeomorphism conjugating and if and only if and are -similar, that is, If and are real matrices, must also be real? Note Added in Edit, Saturday 8 February 2020, 11:25 AM PST: In light of Conifold's comment, we see that the existence of a suitable real gives rise to a purely imaginary matrix which also satisfies the requisite conditions.  Therefore I modify my closing question above and ask If and are real matrices, must be either purely real or purely imaginary? End of Note.","\phi_A \phi_B X h:X \to X \tag 1 x \in X t \in \Bbb R h(\phi_A(x, t)) = \phi_B(h(x), t). \tag 2 \dot x = Ax, \tag 3 x \in \Bbb C^n \tag 4 A \in M(n, \Bbb C), \tag 5 n x(t_0) t_0 x(t) = e^{A(t - t_0)}x(t_0); \tag 6 \phi_A(x, t) = e^{At} x. \tag 7 A, B \in M(n, \Bbb C) \tag 8 T \in M(n, \Bbb C) \tag 9 B = TAT^{-1}. \tag{10} T: \Bbb C^n \to \Bbb C^n \tag{11} \phi_A(x, t) \phi_B(x, t) A B T B = TAT^{-1}. \tag{12} A B T T iT A B T","['linear-algebra', 'ordinary-differential-equations', 'dynamical-systems']"
95,How to fit ordinary differential equations to empirical data?,How to fit ordinary differential equations to empirical data?,,"For some biological systems, there exists ordinary or partial differential equations that allow one to simulate their activity/behavior over time. Some of these models even produce data that is very difficult to tell apart from real data. What I haven't been able to figure out is how were those equations found? Suppose I have some empirical time-series data, that has very little noise in it. How could I ""fit"" or find ODEs or PDEs that mimic them? Are there any paper and pen based methods for this? Or is this something that you would do numerically; say measure the difference between the output of a given ODE and empirical data and optimize the parameters? Thanks for any help!","For some biological systems, there exists ordinary or partial differential equations that allow one to simulate their activity/behavior over time. Some of these models even produce data that is very difficult to tell apart from real data. What I haven't been able to figure out is how were those equations found? Suppose I have some empirical time-series data, that has very little noise in it. How could I ""fit"" or find ODEs or PDEs that mimic them? Are there any paper and pen based methods for this? Or is this something that you would do numerically; say measure the difference between the output of a given ODE and empirical data and optimize the parameters? Thanks for any help!",,"['ordinary-differential-equations', 'partial-differential-equations', 'optimization', 'time-series', 'empirical-processes']"
96,$y''+y'+y=\sin^2x$: particular solution?,: particular solution?,y''+y'+y=\sin^2x,"The problem I am trying to solve is finding the particular solution of the equation: $$y''+y'+y=\sin^2x$$ I don't know what format the particular solution has. Once I know that, I can probably solve the problem with little difficulty.  I haven't seen any examples in my textbook or else where with a exponential trig function on the right side.  Using the format $y=Asin(x)+Bsin(x)$ (and therefore $y'=Acos(x)-Bsin(x)$ , $y''=-Asin(x)-Bcos(x)$ ) and substituting these values for y and its derivatives doesn't give me any value with a $sin^2x$ in it. What format does the specific solution have? How is this sort of equation supposed to be solved? The solution of the characteristic equation $r^2+r+1=0$ is $(r+0.5)^2+0.75=0$ is $r=0.866i-0.5$ ; then $y_c=e^{0.5x}(Acos0.866x+Bsin0.866x)$ , but I don't know if this is is useful or at all how this is to be applied.","The problem I am trying to solve is finding the particular solution of the equation: I don't know what format the particular solution has. Once I know that, I can probably solve the problem with little difficulty.  I haven't seen any examples in my textbook or else where with a exponential trig function on the right side.  Using the format (and therefore , ) and substituting these values for y and its derivatives doesn't give me any value with a in it. What format does the specific solution have? How is this sort of equation supposed to be solved? The solution of the characteristic equation is is ; then , but I don't know if this is is useful or at all how this is to be applied.",y''+y'+y=\sin^2x y=Asin(x)+Bsin(x) y'=Acos(x)-Bsin(x) y''=-Asin(x)-Bcos(x) sin^2x r^2+r+1=0 (r+0.5)^2+0.75=0 r=0.866i-0.5 y_c=e^{0.5x}(Acos0.866x+Bsin0.866x),"['calculus', 'ordinary-differential-equations']"
97,$(xy+2xy\ln^2y+y\ln y)\text{d}x+(2x^2\ln y+x)\text{d}y=0$,,(xy+2xy\ln^2y+y\ln y)\text{d}x+(2x^2\ln y+x)\text{d}y=0,"Solve: (Hint: use $x\ln y=t$ ) $$(xy+2xy\ln^2y+y\ln y)\text{d}x+(2x^2\ln y+x)\text{d}y=0$$ My Work: $$x\ln y=t, \text{ d}t=\ln y \text{ d}x+\frac{x}{y} \text{ d}y$$ $$(x+2x\ln^2y+\ln y)\text{ d}x+\left(\frac{2x^2}{y}\ln y+\frac{x}{y}\right)\text{d}y=0$$ $$(x+2x\ln^2y)\text{ d}x+\left(\frac{2x^2}{y}\ln y\right)\text{d}y+\text{d}t=0$$ $$(x+2t\ln y)\text{ d}x+\left(\frac{2x}{y}t\right)\text{d}y+\text{d}t=0$$ $$\left(\frac{x}{t}+2\ln y\right)\text{d}x+\left(2\frac{x}{y}\right)\text{d}y+\frac{\text{d}t}{t}=0$$ $$\frac{x}{t}\text{d}x+2\text{ d}t+\frac{\text{d}t}{t}=0$$ $$\frac{x}{t}\text{d}x=\left(-2-\frac{1}{t}\right)\text{d}t$$ $$\frac{x^2}{2}=-t^2-t+c$$ $$\frac{x^2}{2}=-x^2\ln^2y-x\ln y+c$$ 1. Is my answer correct? 2. How we could recognize that we should use $x\ln y=t$ . If question didn't Hint? 3. All the way that I did to solve this equation was weird for me (because for example we had $dx,dy,dt$ in line 3) and had not saw this way to solve differential equation before is There any other way to simplification?",Solve: (Hint: use ) My Work: 1. Is my answer correct? 2. How we could recognize that we should use . If question didn't Hint? 3. All the way that I did to solve this equation was weird for me (because for example we had in line 3) and had not saw this way to solve differential equation before is There any other way to simplification?,"x\ln y=t (xy+2xy\ln^2y+y\ln y)\text{d}x+(2x^2\ln y+x)\text{d}y=0 x\ln y=t, \text{ d}t=\ln y \text{ d}x+\frac{x}{y} \text{ d}y (x+2x\ln^2y+\ln y)\text{ d}x+\left(\frac{2x^2}{y}\ln y+\frac{x}{y}\right)\text{d}y=0 (x+2x\ln^2y)\text{ d}x+\left(\frac{2x^2}{y}\ln y\right)\text{d}y+\text{d}t=0 (x+2t\ln y)\text{ d}x+\left(\frac{2x}{y}t\right)\text{d}y+\text{d}t=0 \left(\frac{x}{t}+2\ln y\right)\text{d}x+\left(2\frac{x}{y}\right)\text{d}y+\frac{\text{d}t}{t}=0 \frac{x}{t}\text{d}x+2\text{ d}t+\frac{\text{d}t}{t}=0 \frac{x}{t}\text{d}x=\left(-2-\frac{1}{t}\right)\text{d}t \frac{x^2}{2}=-t^2-t+c \frac{x^2}{2}=-x^2\ln^2y-x\ln y+c x\ln y=t dx,dy,dt",['ordinary-differential-equations']
98,Phase plane portrait center ellipse equations,Phase plane portrait center ellipse equations,,"If I have a system of differential equations, with coefficient matrix, A= $\begin{bmatrix} 1&13\\-2&-1\end{bmatrix}$ . The eigenvalues are $\lambda= \pm 5i$ . The phase portrait is a center with a clockwise direction field. How would I go about determining the equation of one of the rotated ellipse orbits?","If I have a system of differential equations, with coefficient matrix, A= . The eigenvalues are . The phase portrait is a center with a clockwise direction field. How would I go about determining the equation of one of the rotated ellipse orbits?",\begin{bmatrix} 1&13\\-2&-1\end{bmatrix} \lambda= \pm 5i,"['ordinary-differential-equations', 'dynamical-systems']"
99,Solving $y'(x)+y^2=1+x^2$,Solving,y'(x)+y^2=1+x^2,"I'm trying to solve $y'(x)+y^2(x)=1+x^2$ using the property of the Ricatti equation that lets me find the general solution from a particular solution. A particular solution is $y_1(x)=x$ , so using the substitution $y=y_1+1/v$ the Ricatti equation $y'(x)=A(x)y^2(x)+B(x)y(x)+C(x)$ becomes $v'(x)+(B(x)+2A(x)y_1(x))v(x)=-A(x)$ . In this case that gives $v'(x)-2xv(x)=-x^2-1$ . The problem I'm having is that this 1st order linear ODE is too hard to solve on its own for this course so something must be wrong. This is the method I'm supposed to use specifically. Solution apparently involves erf function which can't be right.","I'm trying to solve using the property of the Ricatti equation that lets me find the general solution from a particular solution. A particular solution is , so using the substitution the Ricatti equation becomes . In this case that gives . The problem I'm having is that this 1st order linear ODE is too hard to solve on its own for this course so something must be wrong. This is the method I'm supposed to use specifically. Solution apparently involves erf function which can't be right.",y'(x)+y^2(x)=1+x^2 y_1(x)=x y=y_1+1/v y'(x)=A(x)y^2(x)+B(x)y(x)+C(x) v'(x)+(B(x)+2A(x)y_1(x))v(x)=-A(x) v'(x)-2xv(x)=-x^2-1,['ordinary-differential-equations']
