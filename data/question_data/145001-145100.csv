,title_raw,title_text,title_latex,body_raw,body_text,body_latex,tags
0,Prove $\lim_{n \to \infty} ( \frac{\psi(-1/2+n i )- \psi(-1/2-n i)}{2i}- \frac{n}{n^2+1/4} ) = \frac{\pi}{2}$,Prove,\lim_{n \to \infty} ( \frac{\psi(-1/2+n i )- \psi(-1/2-n i)}{2i}- \frac{n}{n^2+1/4} ) = \frac{\pi}{2},"Consider the sequence: $$A_n=  \frac{\psi(-1/2+n i )- \psi(-1/2-n i)}{2i}- \frac{n}{n^2+1/4} $$ How would you prove that: $$\lim_{n \to \infty} A_n= \frac{\pi}{2}$$ This sequence converges extremely fast, in fact for $n=10$ we have: $$\left( \frac{\psi(-1/2+10 i )- \psi(-1/2-10 i)}{2i}- \frac{10}{100+1/4} \right)=  \\ = 1.57079632679489661923132169 \ldots$$ Where all the digits shown are the same as for $\pi/2$ . I have found this limit by considering the usual integral $\int_0^1 \frac{dx}{1+x^2}$ , using the midpoint rule to derive a Riemann sum, then finding its explicit hypergeometric form, which simplified to the difference of digammas. I was quite surprised by the accuracy of this method, because it only takes $10$ points to get $25$ correct digits, so my questions are: What other proofs can you offer for the limit? Why does it converge so fast? Is it because the midpoint rule is very accurate and the integrated function is very nice? The motivation for this question is of course, generalization of this result, because any integral of the form: $$\int_a^b \frac{P(x)}{Q(x)} c^x dx$$ where $P,Q$ are polynomials, can be approximated by a hypergeometric Riemann sum.","Consider the sequence: How would you prove that: This sequence converges extremely fast, in fact for we have: Where all the digits shown are the same as for . I have found this limit by considering the usual integral , using the midpoint rule to derive a Riemann sum, then finding its explicit hypergeometric form, which simplified to the difference of digammas. I was quite surprised by the accuracy of this method, because it only takes points to get correct digits, so my questions are: What other proofs can you offer for the limit? Why does it converge so fast? Is it because the midpoint rule is very accurate and the integrated function is very nice? The motivation for this question is of course, generalization of this result, because any integral of the form: where are polynomials, can be approximated by a hypergeometric Riemann sum.","A_n=  \frac{\psi(-1/2+n i )- \psi(-1/2-n i)}{2i}- \frac{n}{n^2+1/4}  \lim_{n \to \infty} A_n= \frac{\pi}{2} n=10 \left( \frac{\psi(-1/2+10 i )- \psi(-1/2-10 i)}{2i}- \frac{10}{100+1/4} \right)=  \\ = 1.57079632679489661923132169 \ldots \pi/2 \int_0^1 \frac{dx}{1+x^2} 10 25 \int_a^b \frac{P(x)}{Q(x)} c^x dx P,Q","['limits', 'riemann-sum', 'digamma-function']"
1,How to prove that $\lim_{x\to 0+}\left[\frac{1}{x^{3/2}}-\frac{1}{x^{1/2}\sin(x)}\right]=0$?,How to prove that ?,\lim_{x\to 0+}\left[\frac{1}{x^{3/2}}-\frac{1}{x^{1/2}\sin(x)}\right]=0,"This exercise $$\lim_{x\to 0+}\left[\frac{1}{x^\frac{3}{2}}-\frac{1}{x^\frac{1}{2}\sin(x)}\right]$$ was in my calculus III test at college, after trying hard to solve it I was not really able to do so, I know intuitively that this limit equals 0 but did not find an appropriate way to prove it, as the limit gives an indeterminate form $\infty - \infty$ . I worked out the fractions and got $$\lim_{x\to 0+}\frac{x^\frac{1}{2}\sin(x) - x^\frac{3}{2} }{x^2\sin(x)}$$ which gives me an indeterminate form $\frac{0}{0}$ , then using L'hopitals rule, the limit starts getting uglier as more indeterminate forms keep showing up. My college test is over, but, still I want to find out how to solve this limit, any help would be highly appreciated.","This exercise was in my calculus III test at college, after trying hard to solve it I was not really able to do so, I know intuitively that this limit equals 0 but did not find an appropriate way to prove it, as the limit gives an indeterminate form . I worked out the fractions and got which gives me an indeterminate form , then using L'hopitals rule, the limit starts getting uglier as more indeterminate forms keep showing up. My college test is over, but, still I want to find out how to solve this limit, any help would be highly appreciated.",\lim_{x\to 0+}\left[\frac{1}{x^\frac{3}{2}}-\frac{1}{x^\frac{1}{2}\sin(x)}\right] \infty - \infty \lim_{x\to 0+}\frac{x^\frac{1}{2}\sin(x) - x^\frac{3}{2} }{x^2\sin(x)} \frac{0}{0},"['calculus', 'limits', 'trigonometry']"
2,"During U-substitution, is the $du$ that we use as the infinitesimal in $\int f(u) \ du$ changing size?","During U-substitution, is the  that we use as the infinitesimal in  changing size?",du \int f(u) \ du,"I am familiar and comfortable with the following definition of an integral (defined as the the Riemann Sum where n approaches infinity). I have circled in red two terms to emphasize a concept that I was taught: the $\frac{b-a}{n}$ term effectively becomes the $dx$ term. This intuitively makes sense to me. Additionally, I am familiar with the following statement (as expressed here: How does $u$-substitution work? ) However, I have boxed in red something that I find rather confusing when trying to interpret it through the understanding that $\frac{b-a}{n}$ sorta kinda equals $dx$ ... Specifically, the $du$ value appears to be changing throughout the interval $[ g(a), g(b)]$ . To reframe this in the ""Riemann Sum"" version,we would have something like: $\text{ ""value that varies depending on where you are""} * \frac{g(b)-g(a)}{n}$ This is very confusing to me...because, if this is the correct interpretation, I'm not quite certain I understand why this is allowed.","I am familiar and comfortable with the following definition of an integral (defined as the the Riemann Sum where n approaches infinity). I have circled in red two terms to emphasize a concept that I was taught: the term effectively becomes the term. This intuitively makes sense to me. Additionally, I am familiar with the following statement (as expressed here: How does $u$-substitution work? ) However, I have boxed in red something that I find rather confusing when trying to interpret it through the understanding that sorta kinda equals ... Specifically, the value appears to be changing throughout the interval . To reframe this in the ""Riemann Sum"" version,we would have something like: This is very confusing to me...because, if this is the correct interpretation, I'm not quite certain I understand why this is allowed.","\frac{b-a}{n} dx \frac{b-a}{n} dx du [ g(a), g(b)] \text{ ""value that varies depending on where you are""} * \frac{g(b)-g(a)}{n}","['calculus', 'integration', 'limits', 'chain-rule']"
3,To show that $o(o(x^3)-\frac{1}{2}x^2)-o(x^3)=o(x^2) \: as \: x \rightarrow 0.$ Apostol Calculus Example 1 pg.288,To show that  Apostol Calculus Example 1 pg.288,o(o(x^3)-\frac{1}{2}x^2)-o(x^3)=o(x^2) \: as \: x \rightarrow 0.,"I encounter this problem when I was trying to show that $\sec x=1+\frac{1}{2}x^2+o(x^2) \: as \: x \rightarrow 0.$ We know that $\cos x=1-\frac{1}{2}x^2+o(x^3).$ So $\sec x=\frac{1}{1-\frac{1}{2}x^2+o(x^3)} = 1+\frac{1}{2}x^2-o(x^3)+o(o(x^3)-\frac{1}{2}x^2)$ In the the book Calculus Apostol, it states that the the two little o's behind is equal to $o(x^2)$ without an explanation. I could prove that they equal to $o(x)$ since $-o(x^3)+o(o(x^3)-\frac{1}{2}x^2)$ $= -o(x^3)+o(o(x^3)-o(x))$ $= -o(x^3)+o(o(x^3)+o(x)) = -o(x^3)+o(o(x))=o(x)+o(x^3)=o(x).$ But is this correct, if then how can it equal $o(x^2)$ ?","I encounter this problem when I was trying to show that We know that So In the the book Calculus Apostol, it states that the the two little o's behind is equal to without an explanation. I could prove that they equal to since But is this correct, if then how can it equal ?",\sec x=1+\frac{1}{2}x^2+o(x^2) \: as \: x \rightarrow 0. \cos x=1-\frac{1}{2}x^2+o(x^3). \sec x=\frac{1}{1-\frac{1}{2}x^2+o(x^3)} = 1+\frac{1}{2}x^2-o(x^3)+o(o(x^3)-\frac{1}{2}x^2) o(x^2) o(x) -o(x^3)+o(o(x^3)-\frac{1}{2}x^2) = -o(x^3)+o(o(x^3)-o(x)) = -o(x^3)+o(o(x^3)+o(x)) = -o(x^3)+o(o(x))=o(x)+o(x^3)=o(x). o(x^2),"['real-analysis', 'calculus', 'limits', 'asymptotics']"
4,"Given a Cauchy sequence $a_n$, show that $\sqrt{a_n}$ is Cauchy when $a_n>0$ for all $n$.","Given a Cauchy sequence , show that  is Cauchy when  for all .",a_n \sqrt{a_n} a_n>0 n,"We have a sequence $a_n$ , that is Cauchy and every term is positive. How do I find that $\sqrt{a_n}$ is also Cauchy? I have seen a similar question posted but in that question $a_n>1$ so it is not the same. I understand well how to do it if $a_n>1$ but I can't understand how to alter the solution to account for the cases where $a_n$ and $a_m$ are less than 1. Thank you.","We have a sequence , that is Cauchy and every term is positive. How do I find that is also Cauchy? I have seen a similar question posted but in that question so it is not the same. I understand well how to do it if but I can't understand how to alter the solution to account for the cases where and are less than 1. Thank you.",a_n \sqrt{a_n} a_n>1 a_n>1 a_n a_m,"['calculus', 'limits', 'cauchy-sequences']"
5,Calculate $\lim_{x\rightarrow1}\frac{(1-x^2)}x$ by definition,Calculate  by definition,\lim_{x\rightarrow1}\frac{(1-x^2)}x,How do I calculate the $\lim_{x\rightarrow1}\frac{(1-x^2)}x$ using the δ/ε definition? I'm losing my mind on this :c I'm like lost after something like this: | $\frac{(1+x)  (1-x)}x$ - L| < ε,How do I calculate the using the δ/ε definition? I'm losing my mind on this :c I'm like lost after something like this: | - L| < ε,\lim_{x\rightarrow1}\frac{(1-x^2)}x \frac{(1+x)  (1-x)}x,"['calculus', 'limits', 'definition']"
6,Manhattan distance problem with infinite zig zags,Manhattan distance problem with infinite zig zags,,"If you turn left/right any finite number of times going from point to point, it will be the same as if you traveled $x$ then turned once and traveled $y$ to get there. I hear that even an infinite number of turns will not suddenly shorten the distance to $\sqrt{x^2+y^2}$ . We have a similar situation in the staircase problem in which $\pi \ne 4$ because something happens [forgive me] at infinity. We no longer have size to the xy vectors. We have a set of points that coincide with the curve that is a circle. Here, we have a set of points that are $not$ $close$ $to$ , but $on$ the line that is the hypotenuse of a right triangle. In the staircase problem, we still have an infinite number of vectors not pointing in the direction of the curve except at four points. How does the Manhattan distance differ from $\pi \ne4$ in the staircase problem above where we know the answer $\pi$ as a given? I never learn why the staircase problem ends up as $3.14...$ . Could we need new theorems to explain both of these? Or will they both forever be nothing more than paradoxes? Perhaps the staircase problem has no answer. Can someone verify that that it always results in $\pi=4$ ? If so, I can accept that the Manhattan distance never changes.","If you turn left/right any finite number of times going from point to point, it will be the same as if you traveled then turned once and traveled to get there. I hear that even an infinite number of turns will not suddenly shorten the distance to . We have a similar situation in the staircase problem in which because something happens [forgive me] at infinity. We no longer have size to the xy vectors. We have a set of points that coincide with the curve that is a circle. Here, we have a set of points that are , but the line that is the hypotenuse of a right triangle. In the staircase problem, we still have an infinite number of vectors not pointing in the direction of the curve except at four points. How does the Manhattan distance differ from in the staircase problem above where we know the answer as a given? I never learn why the staircase problem ends up as . Could we need new theorems to explain both of these? Or will they both forever be nothing more than paradoxes? Perhaps the staircase problem has no answer. Can someone verify that that it always results in ? If so, I can accept that the Manhattan distance never changes.",x y \sqrt{x^2+y^2} \pi \ne 4 not close to on \pi \ne4 \pi 3.14... \pi=4,"['limits', 'intuition', 'infinity', 'plane-geometry', 'arc-length']"
7,The limit of $((1+x)^{1/x}-e)/x\;$ as $\;x$ tends to $ 0$ [duplicate],The limit of  as  tends to  [duplicate],((1+x)^{1/x}-e)/x\; \;x  0,"This question already has answers here : Limit as $x\to 0$ of $\frac{(1+x)^{1/x}-e}{x}$ (6 answers) Closed 4 years ago . So, I have to solve the limit problem using a derivative. The problem is as follows: $$\lim_{x\to0}\frac{(1+x)^{1/x}-e}{x}$$ I really don't know what to do. Can someone help?","This question already has answers here : Limit as $x\to 0$ of $\frac{(1+x)^{1/x}-e}{x}$ (6 answers) Closed 4 years ago . So, I have to solve the limit problem using a derivative. The problem is as follows: I really don't know what to do. Can someone help?",\lim_{x\to0}\frac{(1+x)^{1/x}-e}{x},"['limits', 'derivatives', 'exponential-function']"
8,How to find $\lim_{x \to 0^+} x^{\sqrt{x}}$ using L'Hospital's Rule.,How to find  using L'Hospital's Rule.,\lim_{x \to 0^+} x^{\sqrt{x}},"I am trying to find $\lim_{x \to 0^+} x^{\sqrt{x}}$ using L'Hospital's Rule. I've differentiate the function, but it doesn't seem like that has helped at all. $$\lim_{x \to 0^+} x^{\sqrt{x}} = \lim_{x \to 0^+} x^{\sqrt{x}} (\frac{\ln(x)}{2\sqrt{x}} + \frac{1}{\sqrt{x}})$$ $$\frac{d}{dx} y = \frac{d}{dx} \ln(x^{\sqrt{x}}) \to \frac{1}{y} \cdot y'= \sqrt{x}\ln(x) \to y' = y(\frac{\ln(x)}{2\sqrt{x}} + \frac{1}{\sqrt{x}}) \to y' = x^{\sqrt{x}} (\frac{\ln(x)}{2\sqrt{x}} + \frac{1}{\sqrt{x}})$$ How can I figure out what this limit represents?","I am trying to find using L'Hospital's Rule. I've differentiate the function, but it doesn't seem like that has helped at all. How can I figure out what this limit represents?",\lim_{x \to 0^+} x^{\sqrt{x}} \lim_{x \to 0^+} x^{\sqrt{x}} = \lim_{x \to 0^+} x^{\sqrt{x}} (\frac{\ln(x)}{2\sqrt{x}} + \frac{1}{\sqrt{x}}) \frac{d}{dx} y = \frac{d}{dx} \ln(x^{\sqrt{x}}) \to \frac{1}{y} \cdot y'= \sqrt{x}\ln(x) \to y' = y(\frac{\ln(x)}{2\sqrt{x}} + \frac{1}{\sqrt{x}}) \to y' = x^{\sqrt{x}} (\frac{\ln(x)}{2\sqrt{x}} + \frac{1}{\sqrt{x}}),"['calculus', 'limits']"
9,Evaluate $\lim\limits_{x \to 0} \dfrac{(ax+1)^b - (bx+1)^a}{x^2}$,Evaluate,\lim\limits_{x \to 0} \dfrac{(ax+1)^b - (bx+1)^a}{x^2},"Evaluate $$\lim\limits_{x \to 0} \dfrac{(ax+1)^b - (bx+1)^a}{x^2}$$ where $a, b$ are non-negative integers. I solved this with the help of L'Hopital's rule, but still I'm looking for a different solution (i.e no L'Hopital's rule). Thanks in advance.","Evaluate where are non-negative integers. I solved this with the help of L'Hopital's rule, but still I'm looking for a different solution (i.e no L'Hopital's rule). Thanks in advance.","\lim\limits_{x \to 0} \dfrac{(ax+1)^b - (bx+1)^a}{x^2} a, b","['calculus', 'limits', 'limits-without-lhopital']"
10,"$ \lim_{(x,y)\rightarrow (0,0)} \frac{x^{5}y^{3}}{x^{6}+y^{4}}. $ Does it exist or not?",Does it exist or not?," \lim_{(x,y)\rightarrow (0,0)} \frac{x^{5}y^{3}}{x^{6}+y^{4}}. ","I have this limit: $$ \lim_{(x,y)\rightarrow (0,0)} \frac{x^{5}y^{3}}{x^{6}+y^{4}}. $$ I think that this limit does not exist (and wolfram|alpha agrees with me). But I can't find a way to prove it. I chose some paths and the limit was always equal to zero. I tried polar coordinates but I couldn't get an answer. Any tips? (I also have this one: $$ \lim_{(x,y)\rightarrow (0,0)} x^{y^{2}} $$ Can I get 2 different paths to show that the limit does not exist? $$x=0: \lim_{(x,y)\rightarrow (0,0)} 0^{y^{2}} = \lim_{(x,y)\rightarrow (0,0)} 0 = 0,  $$ $$y=0: \lim_{(x,y)\rightarrow (0,0)} x^{0^{2}} = \lim_{(x,y)\rightarrow (0,0)} 1= 1. $$ Am I right? )",I have this limit: I think that this limit does not exist (and wolfram|alpha agrees with me). But I can't find a way to prove it. I chose some paths and the limit was always equal to zero. I tried polar coordinates but I couldn't get an answer. Any tips? (I also have this one: Can I get 2 different paths to show that the limit does not exist? Am I right? )," \lim_{(x,y)\rightarrow (0,0)} \frac{x^{5}y^{3}}{x^{6}+y^{4}}.   \lim_{(x,y)\rightarrow (0,0)} x^{y^{2}}  x=0: \lim_{(x,y)\rightarrow (0,0)} 0^{y^{2}} = \lim_{(x,y)\rightarrow (0,0)} 0 = 0,   y=0: \lim_{(x,y)\rightarrow (0,0)} x^{0^{2}} = \lim_{(x,y)\rightarrow (0,0)} 1= 1. ",['limits']
11,If $\lim\limits_{x \to \infty} f(x) = \infty$ then a special interval exists?,If  then a special interval exists?,\lim\limits_{x \to \infty} f(x) = \infty,"Sorry for the bad title, I didn't know any word to describe what I need. Suppose $f:\mathbb R \to \mathbb R$ is $C^1$ and that $\lim\limits_{x\to \infty}f(x) = \infty$ Can we say for sure that there is some interval $[0, b]$ such that $f(x) >\max\limits_{y \in [0,b]}f(y)$ for all $x >b$ ? More generally, suppose $f:\mathbb R^n \to \mathbb R^n$ a $C^1$ function such that $\lim\limits_{|x| \to \infty}|f(x)| = \infty$ Can we say for certain that there is closed ball $B(0, r)$ such that $|f(x)| > \max\limits_{y\in B(0, r)} |f(y)|$ ? this seems natural and trivial but I have yet to find a formal proof.","Sorry for the bad title, I didn't know any word to describe what I need. Suppose is and that Can we say for sure that there is some interval such that for all ? More generally, suppose a function such that Can we say for certain that there is closed ball such that ? this seems natural and trivial but I have yet to find a formal proof.","f:\mathbb R \to \mathbb R C^1 \lim\limits_{x\to \infty}f(x) = \infty [0, b] f(x) >\max\limits_{y \in [0,b]}f(y) x >b f:\mathbb R^n \to \mathbb R^n C^1 \lim\limits_{|x| \to \infty}|f(x)| = \infty B(0, r) |f(x)| > \max\limits_{y\in B(0, r)} |f(y)|","['calculus', 'limits', 'normed-spaces']"
12,show that $\lim_{ h\to 0} (a^h-1)/h=\ln(a)$,show that,\lim_{ h\to 0} (a^h-1)/h=\ln(a),"I would like to show: $$\lim_{ h\to 0} \frac{a^h-1}{h}=\ln(a)$$ So I've got two questions: 1) Find the value of $\lim_{h\to 0}(a^h-1)/h$ . You don't know that the answer is $\ln(a)$ , you can't guess, so you have to derive it without using it. You can only use the $\lim_{n\to\infty}(1+1/n)^n$ definition for $e$ . The definition for $\ln(a)=x$ is $e^x=a$ . 2) Show that $\lim_{h\to 0}(a^h-1)/h$ and $\ln(a)$ is the same by whatever means.","I would like to show: So I've got two questions: 1) Find the value of . You don't know that the answer is , you can't guess, so you have to derive it without using it. You can only use the definition for . The definition for is . 2) Show that and is the same by whatever means.",\lim_{ h\to 0} \frac{a^h-1}{h}=\ln(a) \lim_{h\to 0}(a^h-1)/h \ln(a) \lim_{n\to\infty}(1+1/n)^n e \ln(a)=x e^x=a \lim_{h\to 0}(a^h-1)/h \ln(a),"['calculus', 'limits', 'exponential-function']"
13,"Prove that $\lim_{n\to\infty}\int_{0}^{1}\cos^n\left(\frac{1}{x}\right)\,dx=0$",Prove that,"\lim_{n\to\infty}\int_{0}^{1}\cos^n\left(\frac{1}{x}\right)\,dx=0","Prove that: $$\lim_{n\to\infty}\int_{0}^{1}\cos^n\left(\frac{1}{x}\right)\,dx=0$$ I have a idea about this,but I can't complete proof. Hint: We can split the integral into two parts,then estimate the two parts separately. I tried to let $t=1/x$ ,but there are many difficulties in the proof. Any help would be greatly appreciated :-)","Prove that: I have a idea about this,but I can't complete proof. Hint: We can split the integral into two parts,then estimate the two parts separately. I tried to let ,but there are many difficulties in the proof. Any help would be greatly appreciated :-)","\lim_{n\to\infty}\int_{0}^{1}\cos^n\left(\frac{1}{x}\right)\,dx=0 t=1/x","['real-analysis', 'integration', 'limits']"
14,How do I calculate $\lim_{x\to \infty} \sqrt{(2\pi x)}^\frac{1}{x}$,How do I calculate,\lim_{x\to \infty} \sqrt{(2\pi x)}^\frac{1}{x},"I just started studying limits a week ago and today I got this question on my YouTube feed. I am having a hard time with it. The question is: Find the value of: $\lim_{x\to \infty}(\frac{x!}{x^x})^\frac{1}{x}$ The answer to this question is $\frac{1}{e}$ There in the comment section, someone suggested to use the Stirling's approximation.  It states $n! \approx   e^{-n}*n^n*\sqrt{2\pi n}$ After putting this in the question it reduces down to:  proving that $\lim_{x\to \infty} \sqrt{(2\pi x)}^\frac{1}{x}=1$ I just don’t know how to eliminate that $\pi$ in the expression. Wolfram Alpha suggested to use Puiseux series. Apparently, the wiki page is just too advanced for me to comprehend. I would be really thankful to anyone who could shed some light on this problem! ( The video link is: https://www.youtube.com/watch?v=89d5f8WUf1Y&t=0s )","I just started studying limits a week ago and today I got this question on my YouTube feed. I am having a hard time with it. The question is: Find the value of: The answer to this question is There in the comment section, someone suggested to use the Stirling's approximation.  It states After putting this in the question it reduces down to:  proving that I just don’t know how to eliminate that in the expression. Wolfram Alpha suggested to use Puiseux series. Apparently, the wiki page is just too advanced for me to comprehend. I would be really thankful to anyone who could shed some light on this problem! ( The video link is: https://www.youtube.com/watch?v=89d5f8WUf1Y&t=0s )",\lim_{x\to \infty}(\frac{x!}{x^x})^\frac{1}{x} \frac{1}{e} n! \approx   e^{-n}*n^n*\sqrt{2\pi n} \lim_{x\to \infty} \sqrt{(2\pi x)}^\frac{1}{x}=1 \pi,"['calculus', 'limits']"
15,"Proof verification that $\{x_n\} = 0,\underbrace{77\dots 7}_{\text{n times}}$ is a Cauchy sequence.",Proof verification that  is a Cauchy sequence.,"\{x_n\} = 0,\underbrace{77\dots 7}_{\text{n times}}","Given a sequence $\{x_n\}$ : $$ x_n =  0,\underbrace{77\dots 7}_{\text n\ times} $$ Prove that $\{x_n\}$ is a Cauchy sequence. Recall the definition of a fundamental sequence: $$ x_n\ \text{is fundamental} \ \iff \forall \epsilon>0 \exists N\in \Bbb N: \forall n, m >N\implies |x_n - x_m| < \epsilon $$ Rewrite $x_n$ : $$ x_n = {7\over 10^1} + {7\over 10^2} + \cdots + {7\over 10^n} = \sum_{k=1}^n \frac{7}{10^k} $$ By geometric series sum: $$ x_n = \sum_{k=1}^n \frac{7}{10^k} = \frac{7}{9}\left(1 - {1\over 10^n}\right) \\ x_m = \sum_{k=1}^m \frac{7}{10^k} = \frac{7}{9}\left(1 - {1\over 10^m}\right) \\ $$ Suppose $m > n$ : $$ \begin{align} |x_n - x_m| &= |x_m - x_n| = \\ &= \left|\frac{7}{9}\left(1 - {1\over 10^m}\right) - \frac{7}{9}\left(1 - {1\over 10^n}\right)\right| = \\ &= \left|\frac{7}{9}\left(1 - {1\over 10^m} - 1 + {1\over 10^n}\right)\right| = \\ &= \left|\frac{7}{9}\left({1\over 10^n} - {1\over 10^m}\right)\right| \le \left|\frac{7}{9}{1\over 10^n}\right| \le \frac{7}{9\cdot 10^N} < \epsilon \end{align} $$ This shows we've found $N$ which depends on $\epsilon$ and satisfies the definition of a Cauchy sequence. This is the first time I'm dealing with proving a sequence is fundamental, could someone please verify whether my proof is valid?","Given a sequence : Prove that is a Cauchy sequence. Recall the definition of a fundamental sequence: Rewrite : By geometric series sum: Suppose : This shows we've found which depends on and satisfies the definition of a Cauchy sequence. This is the first time I'm dealing with proving a sequence is fundamental, could someone please verify whether my proof is valid?","\{x_n\} 
x_n =  0,\underbrace{77\dots 7}_{\text n\ times}
 \{x_n\} 
x_n\ \text{is fundamental} \ \iff \forall \epsilon>0 \exists N\in \Bbb N: \forall n, m >N\implies |x_n - x_m| < \epsilon
 x_n 
x_n = {7\over 10^1} + {7\over 10^2} + \cdots + {7\over 10^n} = \sum_{k=1}^n \frac{7}{10^k}
 
x_n = \sum_{k=1}^n \frac{7}{10^k} = \frac{7}{9}\left(1 - {1\over 10^n}\right) \\
x_m = \sum_{k=1}^m \frac{7}{10^k} = \frac{7}{9}\left(1 - {1\over 10^m}\right) \\
 m > n 
\begin{align}
|x_n - x_m| &= |x_m - x_n| = \\
&= \left|\frac{7}{9}\left(1 - {1\over 10^m}\right) - \frac{7}{9}\left(1 - {1\over 10^n}\right)\right| = \\
&= \left|\frac{7}{9}\left(1 - {1\over 10^m} - 1 + {1\over 10^n}\right)\right| =
\\
&= \left|\frac{7}{9}\left({1\over 10^n} - {1\over 10^m}\right)\right| \le \left|\frac{7}{9}{1\over 10^n}\right| \le \frac{7}{9\cdot 10^N} < \epsilon
\end{align}
 N \epsilon","['calculus', 'sequences-and-series', 'limits', 'proof-verification', 'cauchy-sequences']"
16,Find the limit of the expression $\lim_{x\to 0}\left(\frac{\sin x}{\arcsin x}\right)^{1/\ln(1+x^2)}$,Find the limit of the expression,\lim_{x\to 0}\left(\frac{\sin x}{\arcsin x}\right)^{1/\ln(1+x^2)},"Limit: $\lim_{x\to 0}\left(\dfrac{\sin x}{\arcsin x}\right)^{1/\ln(1+x^2)}$ I have tried to do this: it is equal to $e^{\lim\frac{\log{\frac{\sin x}{\arcsin x}}}{\log(1+x^2)}}$ , but I can't calculate this with the help of l'Hopital rule or using Taylor series, because there is very complex and big derivatives, so I wish to find more easier way. $$\lim_{x\rightarrow 0}{\frac{\log{\frac{\sin x}{\arcsin x}}}{\log(1+x^2)}} = \lim_{x\rightarrow 0}\frac{\log1 + \frac{-\frac{1}{3}x^2}{1+\frac{1}{6}x^2+o(x^2)}}{\log(1+x^2)} = \lim_{x\rightarrow0}\frac{\frac{-\frac{1}{3}x^2}{1+\frac{1}{6}x^2+o(x^2)} + o(\frac{-\frac{1}{3}x^2}{1+\frac{1}{6}x^2+o(x^2)})}{x^2+o(x^2)}$$ using Taylor series. Now I think that it's not clear for me how to simplify $o\left(\frac{-\frac{1}{3}x^2}{1+\frac{1}{6}x^2+o(x^2)}\right)$ .","Limit: I have tried to do this: it is equal to , but I can't calculate this with the help of l'Hopital rule or using Taylor series, because there is very complex and big derivatives, so I wish to find more easier way. using Taylor series. Now I think that it's not clear for me how to simplify .",\lim_{x\to 0}\left(\dfrac{\sin x}{\arcsin x}\right)^{1/\ln(1+x^2)} e^{\lim\frac{\log{\frac{\sin x}{\arcsin x}}}{\log(1+x^2)}} \lim_{x\rightarrow 0}{\frac{\log{\frac{\sin x}{\arcsin x}}}{\log(1+x^2)}} = \lim_{x\rightarrow 0}\frac{\log1 + \frac{-\frac{1}{3}x^2}{1+\frac{1}{6}x^2+o(x^2)}}{\log(1+x^2)} = \lim_{x\rightarrow0}\frac{\frac{-\frac{1}{3}x^2}{1+\frac{1}{6}x^2+o(x^2)} + o(\frac{-\frac{1}{3}x^2}{1+\frac{1}{6}x^2+o(x^2)})}{x^2+o(x^2)} o\left(\frac{-\frac{1}{3}x^2}{1+\frac{1}{6}x^2+o(x^2)}\right),['real-analysis']
17,Show that $\lim_{x\to0}f(x) = \lim_{x\to0}f(x^3)$ (Real Analysis),Show that  (Real Analysis),\lim_{x\to0}f(x) = \lim_{x\to0}f(x^3),"I'm trying to prove  that $$\lim_{x\to0}f(x) = \lim_{x\to0}f(x^3)$$ (The domain is not specified and neither the continuity, so it really is only about the limit of an arbitrary function).. I'm guessing I need to simply use the definition. However I'm not sure where to start. Here is what I had in mind, however I feel like it's not at all how it should be proved. Proof Let $\lim_{x\to0}f(x) = L$ . Then, we have that $$ \forall \epsilon > 0, \exists \delta>0  \text{ such that whenever } 0<|x|< \delta \Rightarrow |f(x) - L| < \epsilon$$ Let $y = x^3$ , then we want to show that $$|f(y) - L| < \epsilon \text{ whenever } 0<|x| = |y^{\frac{1}{3}}| < \delta$$ And here is where I get stuck. Is there any more efficient way to prove this? Thank you!","I'm trying to prove  that (The domain is not specified and neither the continuity, so it really is only about the limit of an arbitrary function).. I'm guessing I need to simply use the definition. However I'm not sure where to start. Here is what I had in mind, however I feel like it's not at all how it should be proved. Proof Let . Then, we have that Let , then we want to show that And here is where I get stuck. Is there any more efficient way to prove this? Thank you!","\lim_{x\to0}f(x) = \lim_{x\to0}f(x^3) \lim_{x\to0}f(x) = L  \forall \epsilon > 0, \exists \delta>0
 \text{ such that whenever } 0<|x|< \delta \Rightarrow |f(x) - L| < \epsilon y = x^3 |f(y) - L| < \epsilon \text{ whenever } 0<|x| = |y^{\frac{1}{3}}| < \delta","['real-analysis', 'limits', 'proof-verification']"
18,$\lim_{x\to -\infty} x+\sqrt{x^2-3x}$,,\lim_{x\to -\infty} x+\sqrt{x^2-3x},"Hey so I'm having a bit of a hard time understanding this one. $\lim_{x\to -\infty} x+\sqrt{x^2-3x}$ 1) $x+\sqrt{x^2-3x}$ * $(\frac{x-\sqrt{x^2-3x}}{x-\sqrt{x^2-3x}})$ 2) $\frac{x^2-(x^2-3x)}{x-\sqrt{x^2-3x}}$ 3) $\frac{3x}{x-\sqrt{x^2(1-\frac{3}{x}})}$ 4) $\frac{3x}{x-\sqrt{x^2}*\sqrt{1-\frac{3}{x}}}$ 5) $\frac{3x}{x-x(\sqrt{1-\frac{3}{x}})}$ 6) $\frac{3}{1-(\sqrt{1-\frac{3}{x}})}$ Now I would just take the limit, it would result in $\frac{3}{1-1}$ which would be undefined. For some reason, the $x$ in the denominator of step 5 should turn into $-(-x)$ which in turn would be positive and therefore be $\frac{3}{1+\sqrt{1=\frac{3}{x}}}$ which would equal $\frac{3}{2}$ . I really don't get it. Apparently the $-\infty$ would mean that $\sqrt{x^2}$ = $-x$ . We didn't even evaluate the limit yet.. how does that turn into $-x$ , just because we know the limit is negative does not mean we evaluated it yet..., why not simplify until there is no more simplification to be done, which is what I did in my steps, which would evaluate to undefined? Would love some help, thanks!","Hey so I'm having a bit of a hard time understanding this one. 1) * 2) 3) 4) 5) 6) Now I would just take the limit, it would result in which would be undefined. For some reason, the in the denominator of step 5 should turn into which in turn would be positive and therefore be which would equal . I really don't get it. Apparently the would mean that = . We didn't even evaluate the limit yet.. how does that turn into , just because we know the limit is negative does not mean we evaluated it yet..., why not simplify until there is no more simplification to be done, which is what I did in my steps, which would evaluate to undefined? Would love some help, thanks!",\lim_{x\to -\infty} x+\sqrt{x^2-3x} x+\sqrt{x^2-3x} (\frac{x-\sqrt{x^2-3x}}{x-\sqrt{x^2-3x}}) \frac{x^2-(x^2-3x)}{x-\sqrt{x^2-3x}} \frac{3x}{x-\sqrt{x^2(1-\frac{3}{x}})} \frac{3x}{x-\sqrt{x^2}*\sqrt{1-\frac{3}{x}}} \frac{3x}{x-x(\sqrt{1-\frac{3}{x}})} \frac{3}{1-(\sqrt{1-\frac{3}{x}})} \frac{3}{1-1} x -(-x) \frac{3}{1+\sqrt{1=\frac{3}{x}}} \frac{3}{2} -\infty \sqrt{x^2} -x -x,"['calculus', 'limits']"
19,"Does $\lim_{(a, b) \to (0, 0)} \sqrt{\frac{a^2b^2}{a^2 + b^2}} = 0$?",Does ?,"\lim_{(a, b) \to (0, 0)} \sqrt{\frac{a^2b^2}{a^2 + b^2}} = 0","Does $$\lim_{(a, b) \to (0, 0)} \ \sqrt{\frac{a^2b^2}{a^2 + b^2}} = 0 \ ?$$ I'm trying to show that $$\lim_{(a, b) \to (0, 0)} \ \sqrt{\frac{a^2b^2}{a^2 + b^2}} = 0 $$ but I am getting stuck. I was thinking that as a starting point I could show that $$\lim_{(a, b) \to (0, 0)} \ \frac{a^2b^2}{a^2 + b^2} = 0 $$ and then conclude that since $$\lim_{x \to 0} \sqrt{x} = 0$$ and $\frac{a^2b^2}{a^2 + b^2} \to 0$ as $(a, b) \to (0, 0)$ we arrive at $$\lim_{(a, b) \to (0, 0)} \ \sqrt{\frac{a^2b^2}{a^2 + b^2}} = 0.$$ Firstly is my approach above a correct one. Secondly how can show that $$\lim_{(a, b) \to (0, 0)} \ \frac{a^2b^2}{a^2 + b^2} = 0. $$ Because I don't see any way to show the above (apart from perhaps proving it from the definition directly, which I would like to avoid if there is an easier way to do it). Also it could be the case that the initial limit doesn't even exist.","Does I'm trying to show that but I am getting stuck. I was thinking that as a starting point I could show that and then conclude that since and as we arrive at Firstly is my approach above a correct one. Secondly how can show that Because I don't see any way to show the above (apart from perhaps proving it from the definition directly, which I would like to avoid if there is an easier way to do it). Also it could be the case that the initial limit doesn't even exist.","\lim_{(a, b) \to (0, 0)} \ \sqrt{\frac{a^2b^2}{a^2 + b^2}} = 0 \ ? \lim_{(a, b) \to (0, 0)} \ \sqrt{\frac{a^2b^2}{a^2 + b^2}} = 0  \lim_{(a, b) \to (0, 0)} \ \frac{a^2b^2}{a^2 + b^2} = 0  \lim_{x \to 0} \sqrt{x} = 0 \frac{a^2b^2}{a^2 + b^2} \to 0 (a, b) \to (0, 0) \lim_{(a, b) \to (0, 0)} \ \sqrt{\frac{a^2b^2}{a^2 + b^2}} = 0. \lim_{(a, b) \to (0, 0)} \ \frac{a^2b^2}{a^2 + b^2} = 0. ","['limits', 'multivariable-calculus']"
20,Double Infinite sum of $1/n^2$,Double Infinite sum of,1/n^2,"I am trying to use an identity we showed on our homework: $$ \sum_{-\infty}^{\infty} \frac{1}{(n+a)^2}  = \frac{\pi^2}{\sin^2(\pi a)} $$ to show that $$ \sum_{1}^{\infty} \frac{1}{n^2}  = \frac{\pi^2}{6}.$$ I have broken the first double infinite sum into the sum from $-\infty$ to $1$ plus the the $0^{th}$ term, plus the sum from $1$ to $+\infty$ and then I want to take the limit of $a$ going to $0$ . This results in taking the limit of the following: $$\lim_{a\to 0} \frac{\pi^2}{\sin^2(\pi a)} - \frac{1}{a^2}   .$$ Which I know should result in $\frac{\pi^3}{3}$ from Wolfram Alpha, as desired, but I am struggling with showing it analytically.  My idea was to try and find the Maclaurien Expansion of $\sin^2(\pi a)$ , but then taking that series to the exponent of negative 1 since it is in the denominator is causing issues.  Is there a trick I am not seeing or a possible better way to use the above property to show the other infinite sum? This question is also for a complex analysis class, so perhaps there is a way to use complex Laurent or power series?","I am trying to use an identity we showed on our homework: to show that I have broken the first double infinite sum into the sum from to plus the the term, plus the sum from to and then I want to take the limit of going to . This results in taking the limit of the following: Which I know should result in from Wolfram Alpha, as desired, but I am struggling with showing it analytically.  My idea was to try and find the Maclaurien Expansion of , but then taking that series to the exponent of negative 1 since it is in the denominator is causing issues.  Is there a trick I am not seeing or a possible better way to use the above property to show the other infinite sum? This question is also for a complex analysis class, so perhaps there is a way to use complex Laurent or power series?", \sum_{-\infty}^{\infty} \frac{1}{(n+a)^2}  = \frac{\pi^2}{\sin^2(\pi a)}   \sum_{1}^{\infty} \frac{1}{n^2}  = \frac{\pi^2}{6}. -\infty 1 0^{th} 1 +\infty a 0 \lim_{a\to 0} \frac{\pi^2}{\sin^2(\pi a)} - \frac{1}{a^2}   . \frac{\pi^3}{3} \sin^2(\pi a),"['sequences-and-series', 'limits', 'power-series']"
21,Proving a series is convergent - $\sum _{n=1} ^\infty \frac{(-1)^n}{n}$ without using alternating series test,Proving a series is convergent -  without using alternating series test,\sum _{n=1} ^\infty \frac{(-1)^n}{n},"$$\sum _{k=1} ^\infty \frac{(-1)^k}{k}$$ I know this question has been answered a few times but my professor has not taught alternating series test yet or anything other than ratio test, root test and comparison test where $a_i \geq 0$ for every $i \in \mathbb N$ and $\sum_{i=1} ^\infty a_i$ converges and if $|b_i| \leq a_i$ for every i then $\sum_{i=1} ^\infty b_i$ converges absolutely. So here's my attempt using Cauchy criterion. What we know: We say that the series $\sum _{i=1}^\infty a_i$ converges if the sequence of partial sums $(S_i)_i$$_\in$$_\mathbb N$ converges. From Cauchy criterion, $(S_i)_i$$_\in$$_\mathbb N$ converges if and only if it is a Cauchy sequence. It is quite obvious that $$\lim_{k\to\infty} S_k = \sum _{k=1}^\infty \frac{(-1)^k}{k}$$ . I denote $\lim_{k\to\infty} S_k = S$ Suppose ( $S_k$ ) is convergent. Then $\forall \epsilon \gt 0, \exists N \in \mathbb N$ such that $\forall n \geq N,$ $|S_n - S|$ = $\vert \sum_{k=n+1} ^\infty \vert$ $\lt \epsilon$ I am stuck here. Is it possible to find such N for all $\epsilon \gt 0$ to hold $\vert \sum_{k=n+1} ^\infty \vert$ $\lt \epsilon$ to be true? (If yes, then the sequence ( $S_k$ ) is convergent so $\sum_{k=1} ^\infty \frac{(-1)^k}{k}$ is convergent by the definition but I don't quite understand if we can always find such N) edit: I don't think ratio test or root test are applicable to solve this and is the alternating series test the only way to solve this problem?","I know this question has been answered a few times but my professor has not taught alternating series test yet or anything other than ratio test, root test and comparison test where for every and converges and if for every i then converges absolutely. So here's my attempt using Cauchy criterion. What we know: We say that the series converges if the sequence of partial sums converges. From Cauchy criterion, converges if and only if it is a Cauchy sequence. It is quite obvious that . I denote Suppose ( ) is convergent. Then such that = I am stuck here. Is it possible to find such N for all to hold to be true? (If yes, then the sequence ( ) is convergent so is convergent by the definition but I don't quite understand if we can always find such N) edit: I don't think ratio test or root test are applicable to solve this and is the alternating series test the only way to solve this problem?","\sum _{k=1} ^\infty \frac{(-1)^k}{k} a_i \geq 0 i \in \mathbb N \sum_{i=1} ^\infty a_i |b_i| \leq a_i \sum_{i=1} ^\infty b_i \sum _{i=1}^\infty a_i (S_i)_i_\in_\mathbb N (S_i)_i_\in_\mathbb N \lim_{k\to\infty} S_k = \sum _{k=1}^\infty \frac{(-1)^k}{k} \lim_{k\to\infty} S_k = S S_k \forall \epsilon \gt 0, \exists N \in \mathbb N \forall n \geq N, |S_n - S| \vert \sum_{k=n+1} ^\infty \vert \lt \epsilon \epsilon \gt 0 \vert \sum_{k=n+1} ^\infty \vert \lt \epsilon S_k \sum_{k=1} ^\infty \frac{(-1)^k}{k}","['real-analysis', 'sequences-and-series', 'limits', 'convergence-divergence', 'alternative-proof']"
22,Limit of an operator and eigenvalues,Limit of an operator and eigenvalues,,"can you help me how to solve this exercice? Define in $L^2(-1,1)$ $$ T_k[f](x):= \int_{-1}^1 \frac{(-1)^k}{(2k+1)!}(xy)^{2k+1} f(y)\,\,dy $$ i) show that $||T_k||\to 0$ ; ii) find eigenvalues of $T_0$ . I don't know how to solve the first point, while I tried to solve the second point solving a Fredholm equation: $\int_{-1}^1xyf(y)\,dy=\lambda f(x)$ . So I have $xC=\lambda f(x)$ , where $C=\int_{-1}^1yf(y)\,dy$ , multiply $x$ in $L^2$ and I find $(x,x) C= \lambda C$ . So $\lambda=2/3$ is correct?","can you help me how to solve this exercice? Define in i) show that ; ii) find eigenvalues of . I don't know how to solve the first point, while I tried to solve the second point solving a Fredholm equation: . So I have , where , multiply in and I find . So is correct?","L^2(-1,1)  T_k[f](x):= \int_{-1}^1 \frac{(-1)^k}{(2k+1)!}(xy)^{2k+1} f(y)\,\,dy  ||T_k||\to 0 T_0 \int_{-1}^1xyf(y)\,dy=\lambda f(x) xC=\lambda f(x) C=\int_{-1}^1yf(y)\,dy x L^2 (x,x) C= \lambda C \lambda=2/3","['limits', 'eigenvalues-eigenvectors', 'operator-theory', 'hilbert-spaces', 'compact-operators']"
23,Extend $f(x )= \frac{|x^2-4|}{x^2-4}$ in a way that $f(x)$ is continuous,Extend  in a way that  is continuous,f(x )= \frac{|x^2-4|}{x^2-4} f(x),"Problem What is domain of $f$ when: $$ f(x)=\frac{|x^2-4|}{x^2-4} $$ Map $f: X \rightarrow Y$ where $X$ is domain (all values $x \in \mathbb{R}$ that are defined by $f$ ) of $f$ and $Y$ is codomain (all values of $f(x)$ ). Is it possible to extend $\forall x \not\in X$ in a way that $f$ is continuous ? Attempt to solve I made plot of $f(x)$ It is quite easily visible that $$ \lim_{x \rightarrow -2} f(x)= \text{""non existent two sided limit""} $$ $$ \lim_{x \rightarrow 2} f(x)=\text{""non existent two sided limit""} $$ I can define $X$ $$ x^2-4 \neq 0 $$ $$ x^2 \neq 4 \iff x \neq \pm 2 $$ which makes $X = \mathbb{R}\setminus \{-2,2\}$ $$ f: \mathbb{R}\setminus \{-2,2\} \rightarrow \{-1,1\} $$ Is it possible to extend $\forall x \not\in X = \{-2,2\}$ in way that $f$ would become continuous ? I would say this is not possible if I can only redefine values for $f(-2)$ and $f(2)$ . In order $f$ to be continuous is to redefine $f(x)$ when $x\in[-2,2]$ or $x\in(-\infty,2]\wedge x\in[2,\infty)$ . Since I could only redefine $f(x)$ when $x\in \{-2,2\}$ this is not possible.",Problem What is domain of when: Map where is domain (all values that are defined by ) of and is codomain (all values of ). Is it possible to extend in a way that is continuous ? Attempt to solve I made plot of It is quite easily visible that I can define which makes Is it possible to extend in way that would become continuous ? I would say this is not possible if I can only redefine values for and . In order to be continuous is to redefine when or . Since I could only redefine when this is not possible.,"f  f(x)=\frac{|x^2-4|}{x^2-4}  f: X \rightarrow Y X x \in \mathbb{R} f f Y f(x) \forall x \not\in X f f(x)  \lim_{x \rightarrow -2} f(x)= \text{""non existent two sided limit""}   \lim_{x \rightarrow 2} f(x)=\text{""non existent two sided limit""}  X  x^2-4 \neq 0   x^2 \neq 4 \iff x \neq \pm 2  X = \mathbb{R}\setminus \{-2,2\}  f: \mathbb{R}\setminus \{-2,2\} \rightarrow \{-1,1\}  \forall x \not\in X = \{-2,2\} f f(-2) f(2) f f(x) x\in[-2,2] x\in(-\infty,2]\wedge x\in[2,\infty) f(x) x\in \{-2,2\}","['real-analysis', 'limits', 'continuity']"
24,Is there another way of evaluating $\lim_{x \to 0} \Gamma(x)(\gamma+\psi(1+x))=\frac{\pi^2}{6}$,Is there another way of evaluating,\lim_{x \to 0} \Gamma(x)(\gamma+\psi(1+x))=\frac{\pi^2}{6},"I was messing around with the Zeta-Function and I got what I thought was an interesting limit: $$\lim_{x \to 0} \Gamma(x)(\gamma+\psi(1+x)) = \frac{\pi^2}{6} $$ Where $\Gamma$ is the gamma function, $\gamma $ is the Euler-Mascheroni constant, and $\psi$ is the digamma function. I got it by writing the Zeta Function as: $$\sum_{n=1}^{\infty} \frac{1}{n^2} = -\int_{0}^1 \frac{\ln(x)}{1-x}dx$$ Then using the beta function and differentiating with respect to x you get the limit. What other ways can be used to evaluate the limit? Here's how I got to my answer: We know $$\sum_{n=1}^{\infty}\frac{1}{n^2} = \frac{\pi^2}{6} $$ To convert the sum into an integral consider: $$\int_{0}^1 x^{n-1}dx=\frac{1}{n}$$ Differentiating once: $$\int_{0}^1 \ln(x)x^{n-1}dx=\frac{-1}{n^2}$$ Plugging in the integral into the sum you get: $$-\sum_{n=1}^\infty \int_0^1 \ln(x)x^{n-1}dx=-\int_0^1\ln(x)\sum_{n=1}^\infty  x^{n-1}dx=-\int_0^1 \frac{\ln(x)}{1-x}dx$$ Euler's Beta Function is defined as: $$\operatorname{B(x,y)}=\int_0^1 t^{x-1}(1-t)^{y-1} = \frac{\Gamma(x)\Gamma(y)}{\Gamma(x+y)}$$ Differentiation with respect to x: $$\operatorname{B_x(x,y)}=\int_0^1 \ln(t)t^{x-1}(1-t)^{y-1} = \frac{\Gamma(x)\Gamma(y)}{\Gamma(x+y)}(\psi(x)-\psi(x+y))$$ $$-\int_0^1 \frac{\ln(x)}{1-x}dx = -\operatorname{B_x(1,0)}=\frac{\pi^2}{6} $$ Taking the limit as (x,y)->(1,0) $$-\lim_{(x,y)\to(1,0)} \frac{\Gamma(x)\Gamma(y)}{\Gamma(x+y)}(\psi(x)-\psi(x+y))=-\lim_{y \to 0} \Gamma(y)(\psi(1)-\psi(1+y)) = \lim_{y \to 0} \Gamma(y)(\gamma+\psi(1+y))$$","I was messing around with the Zeta-Function and I got what I thought was an interesting limit: $$\lim_{x \to 0} \Gamma(x)(\gamma+\psi(1+x)) = \frac{\pi^2}{6} $$ Where $\Gamma$ is the gamma function, $\gamma $ is the Euler-Mascheroni constant, and $\psi$ is the digamma function. I got it by writing the Zeta Function as: $$\sum_{n=1}^{\infty} \frac{1}{n^2} = -\int_{0}^1 \frac{\ln(x)}{1-x}dx$$ Then using the beta function and differentiating with respect to x you get the limit. What other ways can be used to evaluate the limit? Here's how I got to my answer: We know $$\sum_{n=1}^{\infty}\frac{1}{n^2} = \frac{\pi^2}{6} $$ To convert the sum into an integral consider: $$\int_{0}^1 x^{n-1}dx=\frac{1}{n}$$ Differentiating once: $$\int_{0}^1 \ln(x)x^{n-1}dx=\frac{-1}{n^2}$$ Plugging in the integral into the sum you get: $$-\sum_{n=1}^\infty \int_0^1 \ln(x)x^{n-1}dx=-\int_0^1\ln(x)\sum_{n=1}^\infty  x^{n-1}dx=-\int_0^1 \frac{\ln(x)}{1-x}dx$$ Euler's Beta Function is defined as: $$\operatorname{B(x,y)}=\int_0^1 t^{x-1}(1-t)^{y-1} = \frac{\Gamma(x)\Gamma(y)}{\Gamma(x+y)}$$ Differentiation with respect to x: $$\operatorname{B_x(x,y)}=\int_0^1 \ln(t)t^{x-1}(1-t)^{y-1} = \frac{\Gamma(x)\Gamma(y)}{\Gamma(x+y)}(\psi(x)-\psi(x+y))$$ $$-\int_0^1 \frac{\ln(x)}{1-x}dx = -\operatorname{B_x(1,0)}=\frac{\pi^2}{6} $$ Taking the limit as (x,y)->(1,0) $$-\lim_{(x,y)\to(1,0)} \frac{\Gamma(x)\Gamma(y)}{\Gamma(x+y)}(\psi(x)-\psi(x+y))=-\lim_{y \to 0} \Gamma(y)(\psi(1)-\psi(1+y)) = \lim_{y \to 0} \Gamma(y)(\gamma+\psi(1+y))$$",,"['integration', 'limits', 'summation', 'gamma-function', 'riemann-zeta']"
25,Find $ \lim _{n\to \infty} \frac{n^1+\dots+n^n}{1^n+\dots+n^n}$,Find, \lim _{n\to \infty} \frac{n^1+\dots+n^n}{1^n+\dots+n^n},"What is the limit of: $$\lim_{n\to \infty} \frac{n^1+\dots+n^n}{1^n+\dots+n^n} = ?$$ I did some computation with big numbers, I guess it is in the interval $\left(\frac{1}{2},1\right)$.","What is the limit of: $$\lim_{n\to \infty} \frac{n^1+\dots+n^n}{1^n+\dots+n^n} = ?$$ I did some computation with big numbers, I guess it is in the interval $\left(\frac{1}{2},1\right)$.",,['real-analysis']
26,Finding $\lim\limits_{n→∞}\left(\frac1π\arctan\left(\frac{nx}π\right)+\frac12\right)^n$,Finding,\lim\limits_{n→∞}\left(\frac1π\arctan\left(\frac{nx}π\right)+\frac12\right)^n,"Well, as the title says, I'm trying to solve the following limit: $$\lim_{n\to\infty}\left[\frac{1}{\pi}\arctan\left(\frac{nx}{\pi}\right)+\frac{1}{2}\right]^n.$$ This arised in the following context: let $(X_i)$ be a sequence of independent Cauchy random variables. Let $M_n=\max_{1\leq i\leq n}X_i$. I want to calculate the distribution to which $\pi M_n/n$ tends to. My approach went as follows: $$F_{\pi M_n/n}(x) = \mathbb{P}(\pi M_n/n\leq x)=\mathbb{P}(M_n\leq nx/\pi)=(\mathbb{P}(X_1\leq nx/\pi))^n.$$ Which is equals to $$\left[\frac{1}{\pi}\arctan\left(\frac{nx}{\pi}\right)+\frac{1}{2}\right]^n.$$","Well, as the title says, I'm trying to solve the following limit: $$\lim_{n\to\infty}\left[\frac{1}{\pi}\arctan\left(\frac{nx}{\pi}\right)+\frac{1}{2}\right]^n.$$ This arised in the following context: let $(X_i)$ be a sequence of independent Cauchy random variables. Let $M_n=\max_{1\leq i\leq n}X_i$. I want to calculate the distribution to which $\pi M_n/n$ tends to. My approach went as follows: $$F_{\pi M_n/n}(x) = \mathbb{P}(\pi M_n/n\leq x)=\mathbb{P}(M_n\leq nx/\pi)=(\mathbb{P}(X_1\leq nx/\pi))^n.$$ Which is equals to $$\left[\frac{1}{\pi}\arctan\left(\frac{nx}{\pi}\right)+\frac{1}{2}\right]^n.$$",,"['calculus', 'limits']"
27,Limit involving Series and Greatest Integer Function,Limit involving Series and Greatest Integer Function,,"If $[$.$]$ denotes the greatest integer function, then find the value of $\lim_{n \to \infty} \frac{[x] + [2x] + [3x] + … + [nx]}{n^2}$ What I did was, I wrote each greatest integer function $[x]$ as $x - \{x\}$, where $\{.\}$ is the fractional part. Hence, you get $\lim_{n \to \infty} \frac{\frac{n(n+1)}{2}(x-\{x\})}{n^2}$ The limit should then evaluate to $\frac{x-\{x\}}{2}$ But the answer given is $\frac{x}{2}$. What am I missing here?","If $[$.$]$ denotes the greatest integer function, then find the value of $\lim_{n \to \infty} \frac{[x] + [2x] + [3x] + … + [nx]}{n^2}$ What I did was, I wrote each greatest integer function $[x]$ as $x - \{x\}$, where $\{.\}$ is the fractional part. Hence, you get $\lim_{n \to \infty} \frac{\frac{n(n+1)}{2}(x-\{x\})}{n^2}$ The limit should then evaluate to $\frac{x-\{x\}}{2}$ But the answer given is $\frac{x}{2}$. What am I missing here?",,"['sequences-and-series', 'limits', 'limits-without-lhopital', 'ceiling-and-floor-functions', 'fractional-part']"
28,Evaluate $\lim\limits_{n\to+\infty}\int_{0}^{1}\ln(1-x^2+2x^{n}+x^{2n}){\rm d}x$.,Evaluate .,\lim\limits_{n\to+\infty}\int_{0}^{1}\ln(1-x^2+2x^{n}+x^{2n}){\rm d}x,Problem Evaluate $$\lim_{n\to+\infty}\int_{0}^{1}\ln(1-x^2+2x^{n}+x^{2n}){\rm d}x.$$ I guess it needs to apply Integral Mean Value Theorem or Integral Inequality . But I fail to find the upper-bound function and the lower-bound function. Who can offer a hint?Thanks!,Problem Evaluate I guess it needs to apply Integral Mean Value Theorem or Integral Inequality . But I fail to find the upper-bound function and the lower-bound function. Who can offer a hint?Thanks!,\lim_{n\to+\infty}\int_{0}^{1}\ln(1-x^2+2x^{n}+x^{2n}){\rm d}x.,"['limits', 'definite-integrals']"
29,How to find $\lim_{n\rightarrow \infty} \frac {n^2}{n^3+n +1} + \frac {n^2}{n^3 +n +2} +.....+\frac {n^2}{n^3 + 2n}$?,How to find ?,\lim_{n\rightarrow \infty} \frac {n^2}{n^3+n +1} + \frac {n^2}{n^3 +n +2} +.....+\frac {n^2}{n^3 + 2n},"How to find $$ \lim_{n\rightarrow \infty} \frac {n^2}{n^3+n +1} + \frac {n^2}{n^3 +n +2} +.....+\frac {n^2}{n^3 + 2n}? $$ I was thinking about the Riemann sum, but I am not able to do it.","How to find I was thinking about the Riemann sum, but I am not able to do it.","
\lim_{n\rightarrow \infty} \frac {n^2}{n^3+n +1} + \frac {n^2}{n^3 +n +2} +.....+\frac {n^2}{n^3 + 2n}?
","['calculus', 'real-analysis']"
30,"Calculate $ \lim_{n\to \infty} \frac{n}{\ln(n)} \int_{1}^{n^2} \frac{\ln(x)}{x^2+nx+n^2}\,dx $",Calculate," \lim_{n\to \infty} \frac{n}{\ln(n)} \int_{1}^{n^2} \frac{\ln(x)}{x^2+nx+n^2}\,dx ","For every $ n\in \mathbb{N} $ and positive $ x $ , $ x \neq 0 $ we consider the function $ f_{n}(x) = \frac{\ln(x)} {x^2+nx+n^2} $ Calculate $ \lim_{n\to \infty} \frac{n}{\ln(n)} \int_{1}^{n^2} f_{n}(x) \,\mathrm dx$ The correct answer should be $ \frac{2\pi \sqrt{3}}{9} $ How to approach this using high-school techniques? The result suggests that  we have to work with an arctangent function probably.","For every $ n\in \mathbb{N} $ and positive $ x $ , $ x \neq 0 $ we consider the function $ f_{n}(x) = \frac{\ln(x)} {x^2+nx+n^2} $ Calculate $ \lim_{n\to \infty} \frac{n}{\ln(n)} \int_{1}^{n^2} f_{n}(x) \,\mathrm dx$ The correct answer should be $ \frac{2\pi \sqrt{3}}{9} $ How to approach this using high-school techniques? The result suggests that  we have to work with an arctangent function probably.",,"['calculus', 'integration', 'limits']"
31,"Given a limit value, prove that a given series is absolutely convergent","Given a limit value, prove that a given series is absolutely convergent",,"The question is: If $\lim\limits_{n \to \infty} n^4|a_n|=1$, then show that $\sum_{i=1}^\infty (-1)^{n+1}a_n$ absolutely converges. What I've got so far: Since the given limit is equal to 1, the series $\sum_{i=1}^\infty n^4|a_n|$ diverges. To show that the series in the problem absolutely converges, I need to show that $\sum_{i=1}^\infty |a_n|$ converges. I thought of using the comparison test, but it would be inconclusive if I compared $|a_n|$ with $n^4|a_n|$. Any help would be appreciated!","The question is: If $\lim\limits_{n \to \infty} n^4|a_n|=1$, then show that $\sum_{i=1}^\infty (-1)^{n+1}a_n$ absolutely converges. What I've got so far: Since the given limit is equal to 1, the series $\sum_{i=1}^\infty n^4|a_n|$ diverges. To show that the series in the problem absolutely converges, I need to show that $\sum_{i=1}^\infty |a_n|$ converges. I thought of using the comparison test, but it would be inconclusive if I compared $|a_n|$ with $n^4|a_n|$. Any help would be appreciated!",,['sequences-and-series']
32,How to show that the $\lim_{n \to \infty} x_n$ exists?,How to show that the  exists?,\lim_{n \to \infty} x_n,"Consider the sequence $x_{n+1}=\frac{x_n+3}{3x_n+1}$ where $n \geq 1$ with $0<x_1<1$. I tried it this way, splitting the terms gives $x_{n+1}-x_n=3(1-x_{n+1}x_n)$. But that doesn’t seems to be useful. Actually I am trying to find a recurrence relation between $x_{n+1}-x_n$ and $x_n-x_{n-1}$. But don’t know how?? Any other approach?? Thanks for any help!!","Consider the sequence $x_{n+1}=\frac{x_n+3}{3x_n+1}$ where $n \geq 1$ with $0<x_1<1$. I tried it this way, splitting the terms gives $x_{n+1}-x_n=3(1-x_{n+1}x_n)$. But that doesn’t seems to be useful. Actually I am trying to find a recurrence relation between $x_{n+1}-x_n$ and $x_n-x_{n-1}$. But don’t know how?? Any other approach?? Thanks for any help!!",,"['calculus', 'real-analysis', 'limits']"
33,Limit of $S_n = \sum_{i = 1}^n \frac{i^k}{n^{k+1}} $ [duplicate],Limit of  [duplicate],S_n = \sum_{i = 1}^n \frac{i^k}{n^{k+1}} ,"This question already has answers here : Evaluate $\lim\limits_{n\to\infty}\frac{\sum_{k=1}^n k^m}{n^{m+1}}$ (5 answers) Closed 5 years ago . Let $S_n = \sum_{i = 1}^n \frac{i^k}{n^{k+1}} $ . For what values of $k$ the series $S_n$ is convergent and what is the value of convergence ? I'm really unable to understand $S_n$ because I haven't seen any series which is similar to that , so applying tests isn't possible for me . Also I've searched over the internet but didn't find any useful result .","This question already has answers here : Evaluate $\lim\limits_{n\to\infty}\frac{\sum_{k=1}^n k^m}{n^{m+1}}$ (5 answers) Closed 5 years ago . Let $S_n = \sum_{i = 1}^n \frac{i^k}{n^{k+1}} $ . For what values of $k$ the series $S_n$ is convergent and what is the value of convergence ? I'm really unable to understand $S_n$ because I haven't seen any series which is similar to that , so applying tests isn't possible for me . Also I've searched over the internet but didn't find any useful result .",,"['sequences-and-series', 'limits']"
34,Find $\lim_{x\to\infty}\left(\sqrt{\ln(e^x+1)}-\sqrt{x}\right)^{1/x}$,Find,\lim_{x\to\infty}\left(\sqrt{\ln(e^x+1)}-\sqrt{x}\right)^{1/x},"The question is to evaluate $$\lim_{x\to\infty}\left(\sqrt{\ln(e^x+1)}-\sqrt{x}\right)^{1/x}$$ This is an indeterminate form of type $0^0$, so I've tried using the identity $a^b=e^{b\ln a}$ and somehow apply l'Hospital's, which leads to pretty complex derivatives and I'm getting nowhere. I've also tried multiplying by the conjugate and perhaps factorize, without success.","The question is to evaluate $$\lim_{x\to\infty}\left(\sqrt{\ln(e^x+1)}-\sqrt{x}\right)^{1/x}$$ This is an indeterminate form of type $0^0$, so I've tried using the identity $a^b=e^{b\ln a}$ and somehow apply l'Hospital's, which leads to pretty complex derivatives and I'm getting nowhere. I've also tried multiplying by the conjugate and perhaps factorize, without success.",,['limits']
35,$\epsilon−\delta$ limit proof,limit proof,\epsilon−\delta,"Using the $\epsilon−\delta$ definition of limits, prove: $$\lim\limits_{x \to 1} \frac{x^5+1}{x}=2$$ The factor $(x-1)$ I can control. And I can also limit the other factor in the numerator. But the $x$ in the denominator is my problem because if I limit $(x-1)$ it seems to grow. I'm not sure what to do with it.","Using the $\epsilon−\delta$ definition of limits, prove: $$\lim\limits_{x \to 1} \frac{x^5+1}{x}=2$$ The factor $(x-1)$ I can control. And I can also limit the other factor in the numerator. But the $x$ in the denominator is my problem because if I limit $(x-1)$ it seems to grow. I'm not sure what to do with it.",,"['calculus', 'limits', 'epsilon-delta']"
36,Calculating a multivariable limit.,Calculating a multivariable limit.,,"Find if it exists the limit : $$\lim_{(x,y)\to(0,0)}\frac{x^4y^4}{(x^2+y^4)^3}$$ I've tried the following : 1st attempt : Using polar coordinates: Set $x= r\cos\theta$ and $y=r\sin\theta$ $$\frac{x^4y^4}{(x^2+y^4)^3}=\frac{r^8\cos^4\theta \sin^4\theta}{(r^2\cos^2\theta+r^4\sin^4\theta)^3}=\frac{r^2\cos^4\theta\sin^4\theta}{(\cos^2\theta+r^2\sin^4\theta)^3}$$ $$\lim_{r\to0}\frac{r^2\cos^4\theta\sin^4\theta}{(\cos^2\theta+r^2\sin^4\theta)^3}=\frac{0}{\cos^6\theta}$$ Now the limit mentioned above would be equal to zero if and only if the denominator is different than zero. Hence we need to calculate the limit in the case where $\theta = \frac{\pi}{2}+k\pi$ and compare it with the precalculated limit. However I was not able to get rid of the indeterminate form. 2nd attempt : Choosing a specific path $y = ax$. $$\frac{x^4y^4}{(x^2+y^4)^3}=\frac{a^4x^2}{(1+a^4x^2)^3}$$ $$\lim_{x\to0}\frac{a^4x^2}{(1+a^4x^2)^3}=0.$$ No conclusion about the limit. If anyone could give me hints or point me in the right direction I would be grateful. Thanks in advance.","Find if it exists the limit : $$\lim_{(x,y)\to(0,0)}\frac{x^4y^4}{(x^2+y^4)^3}$$ I've tried the following : 1st attempt : Using polar coordinates: Set $x= r\cos\theta$ and $y=r\sin\theta$ $$\frac{x^4y^4}{(x^2+y^4)^3}=\frac{r^8\cos^4\theta \sin^4\theta}{(r^2\cos^2\theta+r^4\sin^4\theta)^3}=\frac{r^2\cos^4\theta\sin^4\theta}{(\cos^2\theta+r^2\sin^4\theta)^3}$$ $$\lim_{r\to0}\frac{r^2\cos^4\theta\sin^4\theta}{(\cos^2\theta+r^2\sin^4\theta)^3}=\frac{0}{\cos^6\theta}$$ Now the limit mentioned above would be equal to zero if and only if the denominator is different than zero. Hence we need to calculate the limit in the case where $\theta = \frac{\pi}{2}+k\pi$ and compare it with the precalculated limit. However I was not able to get rid of the indeterminate form. 2nd attempt : Choosing a specific path $y = ax$. $$\frac{x^4y^4}{(x^2+y^4)^3}=\frac{a^4x^2}{(1+a^4x^2)^3}$$ $$\lim_{x\to0}\frac{a^4x^2}{(1+a^4x^2)^3}=0.$$ No conclusion about the limit. If anyone could give me hints or point me in the right direction I would be grateful. Thanks in advance.",,"['calculus', 'limits', 'multivariable-calculus', 'polar-coordinates']"
37,Derivative of $\sin(x)$ issues,Derivative of  issues,\sin(x),"$$\frac{d}{dx} \sin(x) = \lim_{h\to 0} \frac{\sin(x+h) - \sin(x)}{h}$$ $$\frac{d}{dx} \sin(x) = \lim_{h\to 0} \frac{\sin(x)\cos(h) + \cos(x)\sin(h) - \sin(x)}{h}$$ $$\frac{d}{dx} \sin(x) = \sin(x)\lim_{h\to 0} \frac{\cos(h) -1}{h} + \cos(x)\lim_{h\to 0} \frac{\sin(h)}{h}$$ Normally I'd use L'Hopital's Rule here but considering that I'm trying to find the derivative in the first place, that kind of defeats the purpose. Is there an easier way to approach these limits? I'm not seeing anything obvious.","$$\frac{d}{dx} \sin(x) = \lim_{h\to 0} \frac{\sin(x+h) - \sin(x)}{h}$$ $$\frac{d}{dx} \sin(x) = \lim_{h\to 0} \frac{\sin(x)\cos(h) + \cos(x)\sin(h) - \sin(x)}{h}$$ $$\frac{d}{dx} \sin(x) = \sin(x)\lim_{h\to 0} \frac{\cos(h) -1}{h} + \cos(x)\lim_{h\to 0} \frac{\sin(h)}{h}$$ Normally I'd use L'Hopital's Rule here but considering that I'm trying to find the derivative in the first place, that kind of defeats the purpose. Is there an easier way to approach these limits? I'm not seeing anything obvious.",,"['calculus', 'limits', 'trigonometry', 'derivatives', 'proof-explanation']"
38,Show that $f(n)=n^3+20n+1=O(n^3)$,Show that,f(n)=n^3+20n+1=O(n^3),"In my theoretical CS class we covered Big $O$-notation and I had some problems that needed to be solved. Show that $$f(n)=n^3+20n+1=O(n^3)$$  $$l(n)=n^3+20n+1≠O(n^2)$$  $$h(n)=n\sqrt{n}=O(n^2)$$ The rule states that $f(n)\leq C*g(n)$, so for the first question it's $$n^3+20n+1 \leq C*n^3$$ $$1+\frac {20}{n^2}+\frac {1}{n^3} \leq C$$ As $n$ increases to infinity, the left side approaches $1$. Here's where it's confusing. Since the left side would approach infinity, wouldn't the equation become $1 \leq C$? But the answer says that the answer is $22 \leq C$ For $l(n)$ I did the same thing as above but then it's obvious that no matter what the input of $n$ it will approach infinity and not a constant. For $h(x)$ I showed that $$n\sqrt{n} \leq C*n^2$$ $$\frac{n\sqrt{n}}{n^2} = \frac {\sqrt{n}}{n} \leq C$$ As $n$ increases $h(n)$ decreases and eventually approaches $0 $ as $n$ approaches inifity. So how can I show that $n\sqrt{n}=O(n^2)$?","In my theoretical CS class we covered Big $O$-notation and I had some problems that needed to be solved. Show that $$f(n)=n^3+20n+1=O(n^3)$$  $$l(n)=n^3+20n+1≠O(n^2)$$  $$h(n)=n\sqrt{n}=O(n^2)$$ The rule states that $f(n)\leq C*g(n)$, so for the first question it's $$n^3+20n+1 \leq C*n^3$$ $$1+\frac {20}{n^2}+\frac {1}{n^3} \leq C$$ As $n$ increases to infinity, the left side approaches $1$. Here's where it's confusing. Since the left side would approach infinity, wouldn't the equation become $1 \leq C$? But the answer says that the answer is $22 \leq C$ For $l(n)$ I did the same thing as above but then it's obvious that no matter what the input of $n$ it will approach infinity and not a constant. For $h(x)$ I showed that $$n\sqrt{n} \leq C*n^2$$ $$\frac{n\sqrt{n}}{n^2} = \frac {\sqrt{n}}{n} \leq C$$ As $n$ increases $h(n)$ decreases and eventually approaches $0 $ as $n$ approaches inifity. So how can I show that $n\sqrt{n}=O(n^2)$?",,"['real-analysis', 'number-theory', 'limits', 'algorithms', 'notation']"
39,Why is it $|f(x) - L| < \epsilon$ in the definition for a limit and not $0 < |f(x) - L| < \epsilon$?,Why is it  in the definition for a limit and not ?,|f(x) - L| < \epsilon 0 < |f(x) - L| < \epsilon,"In the epsilon-delta definition of a limit it says $0 < |x - a| < \delta$ must exist, i.e. the distance between $x$ and $a$ must be positive. And then this leads to the implication that $|f(x) - L| < \epsilon$ holds where $\epsilon > 0$. But why isn't it $0 < |f(x) - L| < \epsilon$? I thought limits were all about getting closer to a $y$ as you narrow in on an $x$. This definition implies that it is possible to bring $x$ near $a$ and yet somehow the limit $L$ is can possibly equal $f(x)$ as opposed to just getting closer and closer to it.","In the epsilon-delta definition of a limit it says $0 < |x - a| < \delta$ must exist, i.e. the distance between $x$ and $a$ must be positive. And then this leads to the implication that $|f(x) - L| < \epsilon$ holds where $\epsilon > 0$. But why isn't it $0 < |f(x) - L| < \epsilon$? I thought limits were all about getting closer to a $y$ as you narrow in on an $x$. This definition implies that it is possible to bring $x$ near $a$ and yet somehow the limit $L$ is can possibly equal $f(x)$ as opposed to just getting closer and closer to it.",,"['calculus', 'limits', 'definition', 'epsilon-delta']"
40,Is function is uniformly continuous on $\mathbb{R}$ then it is uniformly continuous on subset of $\mathbb{R}$?,Is function is uniformly continuous on  then it is uniformly continuous on subset of ?,\mathbb{R} \mathbb{R},"We know that, the function $f: \mathbb{R}→ \mathbb{R}$ such that  $f(x) = x$ is uniformly continuous on $\mathbb{R}$. Now, my Question is,  if we define the function $g:\mathbb{Q} →\mathbb{R}$ such that $f(x) = x$ for all $x ∈ Q$ then is function $g$ is uniformly continuous on $\mathbb{Q}$? My attempt :  as we know, continuity is domain based property. So to check continuity of $g$, we must consider the point of domain first!!. Now let $q ∈\mathbb{Q}$ then as we know, rationals are dense in $\mathbb{R}$ so, $q$ must be limit point of domain and not an isolated point of domain. So $g$ is continuous at $q$ if and only if, $$\lim_{x\to q} g(x) = g(q)$$ But is $$\lim_{x\to q+} g(x)=\lim_{x\to q-} g(x)$$? (I stuck here) As between two rationals there is an irrational number and function is not defined their. So is left hand limit and right hand limit of $g$ as $x$ tends to $q$ are equal? Is $g$ is continuous at $q$ ? and what about uniform continuity? Please help me.","We know that, the function $f: \mathbb{R}→ \mathbb{R}$ such that  $f(x) = x$ is uniformly continuous on $\mathbb{R}$. Now, my Question is,  if we define the function $g:\mathbb{Q} →\mathbb{R}$ such that $f(x) = x$ for all $x ∈ Q$ then is function $g$ is uniformly continuous on $\mathbb{Q}$? My attempt :  as we know, continuity is domain based property. So to check continuity of $g$, we must consider the point of domain first!!. Now let $q ∈\mathbb{Q}$ then as we know, rationals are dense in $\mathbb{R}$ so, $q$ must be limit point of domain and not an isolated point of domain. So $g$ is continuous at $q$ if and only if, $$\lim_{x\to q} g(x) = g(q)$$ But is $$\lim_{x\to q+} g(x)=\lim_{x\to q-} g(x)$$? (I stuck here) As between two rationals there is an irrational number and function is not defined their. So is left hand limit and right hand limit of $g$ as $x$ tends to $q$ are equal? Is $g$ is continuous at $q$ ? and what about uniform continuity? Please help me.",,"['real-analysis', 'analysis', 'limits', 'continuity', 'uniform-continuity']"
41,Convergence of the sequence $ \sqrt {n-2\sqrt n} - \sqrt n $,Convergence of the sequence, \sqrt {n-2\sqrt n} - \sqrt n ,Here's my attempt at proving it: Given the sequence $$ a_n =\left( \sqrt {n-2\sqrt n} - \sqrt n\right)_{n\geq1} $$ To get rid of the square root in the numerator: \begin{align} \frac {\sqrt {n-2\sqrt n} - \sqrt n} 1 \cdot \frac {\sqrt {n-2\sqrt n} + \sqrt n}{\sqrt {n-2\sqrt n} + \sqrt n} &= \frac { {n-2\sqrt n} - \ n}{\sqrt {n-2\sqrt n} + \sqrt n} = \frac { {-2\sqrt n}}{\sqrt {n-2\sqrt n} + \sqrt n} \\&= \frac { {-2}}{\frac {\sqrt {n-2\sqrt n}} {\sqrt n} + 1} \end{align} By using the limit laws it should converge against: $$ \frac { \lim_{x \to \infty} -2 } { \lim_{x \to \infty} \frac {\sqrt {n-2\sqrt n}}{\sqrt n}  ~~+~~\lim_{x \to \infty} 1} $$ So now we have to figure out what $\frac {\sqrt {n-2\sqrt n}}{\sqrt n}$ converges against: $$ \frac {\sqrt {n-2\sqrt n}}{\sqrt n} \leftrightarrow \frac { {n-2\sqrt n}}{ n} = \frac {1-\frac{2\sqrt n}{n}}{1} $$ ${\frac{2\sqrt n}{n}}$ converges to $0$ since: $$ 2\sqrt n = \sqrt n + \sqrt n \leq \sqrt n ~\cdot ~  \sqrt n = n $$ Therefore $~\lim_{n\to \infty} a_n = -1$ Is this correct and sufficient enough?,Here's my attempt at proving it: Given the sequence $$ a_n =\left( \sqrt {n-2\sqrt n} - \sqrt n\right)_{n\geq1} $$ To get rid of the square root in the numerator: \begin{align} \frac {\sqrt {n-2\sqrt n} - \sqrt n} 1 \cdot \frac {\sqrt {n-2\sqrt n} + \sqrt n}{\sqrt {n-2\sqrt n} + \sqrt n} &= \frac { {n-2\sqrt n} - \ n}{\sqrt {n-2\sqrt n} + \sqrt n} = \frac { {-2\sqrt n}}{\sqrt {n-2\sqrt n} + \sqrt n} \\&= \frac { {-2}}{\frac {\sqrt {n-2\sqrt n}} {\sqrt n} + 1} \end{align} By using the limit laws it should converge against: $$ \frac { \lim_{x \to \infty} -2 } { \lim_{x \to \infty} \frac {\sqrt {n-2\sqrt n}}{\sqrt n}  ~~+~~\lim_{x \to \infty} 1} $$ So now we have to figure out what $\frac {\sqrt {n-2\sqrt n}}{\sqrt n}$ converges against: $$ \frac {\sqrt {n-2\sqrt n}}{\sqrt n} \leftrightarrow \frac { {n-2\sqrt n}}{ n} = \frac {1-\frac{2\sqrt n}{n}}{1} $$ ${\frac{2\sqrt n}{n}}$ converges to $0$ since: $$ 2\sqrt n = \sqrt n + \sqrt n \leq \sqrt n ~\cdot ~  \sqrt n = n $$ Therefore $~\lim_{n\to \infty} a_n = -1$ Is this correct and sufficient enough?,,"['real-analysis', 'analysis', 'limits', 'convergence-divergence']"
42,How to prove this Dirac delta limit representation is correct?,How to prove this Dirac delta limit representation is correct?,,"According to the 7th representation in this site : $$\lim_{M\to \infty} \frac{1}{2\pi\sin(\omega/2)}\sin\left(\omega\left(M+\frac12\right)\right) = \delta(\omega)$$ I'm trying to understand why this is true. I haven't been able to find anything on the Internet. I must state that I have a very shallow knowledge about distribution theory, so the simpler the explanation, the better. If there is no way to show why that limit is correct without using complicated arguments, go ahead anyway, I'll try to follow up.","According to the 7th representation in this site : $$\lim_{M\to \infty} \frac{1}{2\pi\sin(\omega/2)}\sin\left(\omega\left(M+\frac12\right)\right) = \delta(\omega)$$ I'm trying to understand why this is true. I haven't been able to find anything on the Internet. I must state that I have a very shallow knowledge about distribution theory, so the simpler the explanation, the better. If there is no way to show why that limit is correct without using complicated arguments, go ahead anyway, I'll try to follow up.",,"['limits', 'proof-writing', 'distribution-theory', 'dirac-delta']"
43,Continuity by the right of a function defined by an improper integral,Continuity by the right of a function defined by an improper integral,,"Suppose we have a continuous function defined by $$f:\Bbb R\times(0,\infty)\to\Bbb R,\quad (t,x)\mapsto f(t,x)$$ such that $F(x):=\int_0^\infty f(t,x)\,\mathrm dt$ converges uniformly in $[c,\infty)$ for any chosen $c>a$, where the integral is an improper integral of Riemann. When we can say that $$\lim_{x\to a^+}F(x)=\int_0^\infty \lim_{x\to a^+}f(t,x)\,\mathrm dt=\int_0^\infty f(t,a)\,\mathrm dt$$ for some $a>0$? That is, when we can say that $F$ is continuous at $a$ by the right? Background: this question comes just from curiosity, I dont have a clear answer by now.","Suppose we have a continuous function defined by $$f:\Bbb R\times(0,\infty)\to\Bbb R,\quad (t,x)\mapsto f(t,x)$$ such that $F(x):=\int_0^\infty f(t,x)\,\mathrm dt$ converges uniformly in $[c,\infty)$ for any chosen $c>a$, where the integral is an improper integral of Riemann. When we can say that $$\lim_{x\to a^+}F(x)=\int_0^\infty \lim_{x\to a^+}f(t,x)\,\mathrm dt=\int_0^\infty f(t,a)\,\mathrm dt$$ for some $a>0$? That is, when we can say that $F$ is continuous at $a$ by the right? Background: this question comes just from curiosity, I dont have a clear answer by now.",,"['real-analysis', 'limits', 'improper-integrals']"
44,Limit of recurrence sequence [duplicate],Limit of recurrence sequence [duplicate],,"This question already has answers here : Proof of Convergence: Babylonian Method $x_{n+1}=\frac{1}{2}(x_n + \frac{a}{x_n})$ (9 answers) Closed 3 years ago . I have to find a limit (or prove it doesn't exist) for the following recurrence sequence. $a_1 = 2; a_{n+1} = \frac{1}{2}(a_n + \frac{2}{a_n})$ Now I know, in order to find the limit, I first need to prove that the sequence is monotonic and bounded. I've made a partial table of values and concluded that the sequence is decreasing, thus to prove monotonicity, I've written down: $ a_{n+1} < a_n  \rightarrow a_n > \sqrt{2} $ And that's all I could think of. I don't think the inequality above proves anything so I don't know how to continue. I tried to calculate limit of the sequence by using limits of elements as follows: $ \lim a_{n+1} = \frac{1}{2}(\lim a_n + \lim \frac{2}{a_n}) = a\Rightarrow a = \sqrt{2}$ But without proving monotonicity and bounding, there's no proof the limit exists at all. Thank you for any help in advance.","This question already has answers here : Proof of Convergence: Babylonian Method $x_{n+1}=\frac{1}{2}(x_n + \frac{a}{x_n})$ (9 answers) Closed 3 years ago . I have to find a limit (or prove it doesn't exist) for the following recurrence sequence. $a_1 = 2; a_{n+1} = \frac{1}{2}(a_n + \frac{2}{a_n})$ Now I know, in order to find the limit, I first need to prove that the sequence is monotonic and bounded. I've made a partial table of values and concluded that the sequence is decreasing, thus to prove monotonicity, I've written down: $ a_{n+1} < a_n  \rightarrow a_n > \sqrt{2} $ And that's all I could think of. I don't think the inequality above proves anything so I don't know how to continue. I tried to calculate limit of the sequence by using limits of elements as follows: $ \lim a_{n+1} = \frac{1}{2}(\lim a_n + \lim \frac{2}{a_n}) = a\Rightarrow a = \sqrt{2}$ But without proving monotonicity and bounding, there's no proof the limit exists at all. Thank you for any help in advance.",,"['sequences-and-series', 'limits', 'recurrence-relations']"
45,Prove that $\lim_{n \to \infty}\bigg(1+\frac{\alpha}{f(n)}\bigg)^{f(n)}=e^\alpha$ [closed],Prove that  [closed],\lim_{n \to \infty}\bigg(1+\frac{\alpha}{f(n)}\bigg)^{f(n)}=e^\alpha,"Closed. This question is off-topic . It is not currently accepting answers. This question is missing context or other details : Please improve the question by providing additional context, which ideally includes your thoughts on the problem and any attempts you have made to solve it. This information helps others identify where you have difficulties and helps them write answers appropriate to your experience level. Closed 6 years ago . Improve this question how to prove that $$\lim_{n \to \infty}\bigg(1+\frac{\alpha}{f(n)}\bigg)^{f(n)}=e^\alpha$$ where $f(n)$ is any function, such  $$\lim_{n \to \infty}f(n)=\infty$$","Closed. This question is off-topic . It is not currently accepting answers. This question is missing context or other details : Please improve the question by providing additional context, which ideally includes your thoughts on the problem and any attempts you have made to solve it. This information helps others identify where you have difficulties and helps them write answers appropriate to your experience level. Closed 6 years ago . Improve this question how to prove that $$\lim_{n \to \infty}\bigg(1+\frac{\alpha}{f(n)}\bigg)^{f(n)}=e^\alpha$$ where $f(n)$ is any function, such  $$\lim_{n \to \infty}f(n)=\infty$$",,"['limits', 'constants']"
46,Solve the following limit.,Solve the following limit.,,"$\lim_{x\to 1} [\sin^{-1} x]$ ; where [.] is the 'Greatest Integer Function'. The left hand limit will be $ [π/2]$ = $1$. But how can there be a right hand limit (as $ 'x'$ can't take values greater than $1$)? The answer in my textbook is given as $1$. But how can the limit exist when there is no right hand limit because for a limit to exist, LHL should be equal to the RHL.","$\lim_{x\to 1} [\sin^{-1} x]$ ; where [.] is the 'Greatest Integer Function'. The left hand limit will be $ [π/2]$ = $1$. But how can there be a right hand limit (as $ 'x'$ can't take values greater than $1$)? The answer in my textbook is given as $1$. But how can the limit exist when there is no right hand limit because for a limit to exist, LHL should be equal to the RHL.",,"['calculus', 'limits', 'limits-without-lhopital', 'inverse-function']"
47,Evaluate the limit $\lim_{x\to \infty} x(16x^4 + x^2+1)^{1/4}-2x^2$,Evaluate the limit,\lim_{x\to \infty} x(16x^4 + x^2+1)^{1/4}-2x^2,Can someone please check my conclusion to the evaluation of the following limit? $$\lim_{x\to \infty} x(16x^4 + x^2+1)^{1/4}-2x^2$$ I got that the limit is equal to infinity. If limit is equal to infinity does this mean that limit does not exist?,Can someone please check my conclusion to the evaluation of the following limit? $$\lim_{x\to \infty} x(16x^4 + x^2+1)^{1/4}-2x^2$$ I got that the limit is equal to infinity. If limit is equal to infinity does this mean that limit does not exist?,,['calculus']
48,Have I found a counterexample in this question?,Have I found a counterexample in this question?,,"If $f:X\rightarrow \mathbb{R} \,$ is a function with $x_0 \in \overline{X}   \,\setminus \partial(\overline{X}) $ such that : $$\exists \,\,\,\,f'_-(x_0)=\lim_{x\rightarrow x_0^-}\dfrac{f(x)-f(x_0)}{x-x_0},$$ $$\exists \,\,\,\,f'_+(x_0)=\lim_{x\rightarrow x_0^+}\dfrac{f(x)-f(x_0)}{x-x_0}$$  but with possibly $f'_-(x_0) \not= f'_+(x_0)$,  does this still imply continuity of $f$ ? if so then why can I have a function such as $$f(x) =       \begin{cases}       3x \, , \text{  if}  \,\,\, x<x_0 \\       10x+1 \, ,\text{  if}  \,\,\, x=x_0 \\       -2x \, , \text{  if}  \,\,\, x>x_0       \end{cases} $$ that does have $f'_-(x_0)=3$ and $f'_+(x_0)=-2$ but is not continuous, does that mean that only the existence of the left and right side derivatives on a point do not guarantee that $f $ is continuous at $x_0$ ?","If $f:X\rightarrow \mathbb{R} \,$ is a function with $x_0 \in \overline{X}   \,\setminus \partial(\overline{X}) $ such that : $$\exists \,\,\,\,f'_-(x_0)=\lim_{x\rightarrow x_0^-}\dfrac{f(x)-f(x_0)}{x-x_0},$$ $$\exists \,\,\,\,f'_+(x_0)=\lim_{x\rightarrow x_0^+}\dfrac{f(x)-f(x_0)}{x-x_0}$$  but with possibly $f'_-(x_0) \not= f'_+(x_0)$,  does this still imply continuity of $f$ ? if so then why can I have a function such as $$f(x) =       \begin{cases}       3x \, , \text{  if}  \,\,\, x<x_0 \\       10x+1 \, ,\text{  if}  \,\,\, x=x_0 \\       -2x \, , \text{  if}  \,\,\, x>x_0       \end{cases} $$ that does have $f'_-(x_0)=3$ and $f'_+(x_0)=-2$ but is not continuous, does that mean that only the existence of the left and right side derivatives on a point do not guarantee that $f $ is continuous at $x_0$ ?",,"['calculus', 'real-analysis', 'limits', 'derivatives', 'examples-counterexamples']"
49,Can only the existence of the right and left derivatives imply continuity?,Can only the existence of the right and left derivatives imply continuity?,,"If $f:X\rightarrow \mathbb{R} \,$ is a function with $x_0 \in \overline{X}   \,\setminus \partial(\overline{X}) $ such that : $$\exists \,\,\,\,f'_-(x_0)=\lim_{x\rightarrow x_0^-}\dfrac{f(x)-f(x_0)}{x-x_0},$$ $$\exists \,\,\,\,f'_+(x_0)=\lim_{x\rightarrow x_0^+}\dfrac{f(x)-f(x_0)}{x-x_0}$$  but with possibly $f'_-(x_0) \not= f'_+(x_0)$,  does this still imply continuity of $f$ ?","If $f:X\rightarrow \mathbb{R} \,$ is a function with $x_0 \in \overline{X}   \,\setminus \partial(\overline{X}) $ such that : $$\exists \,\,\,\,f'_-(x_0)=\lim_{x\rightarrow x_0^-}\dfrac{f(x)-f(x_0)}{x-x_0},$$ $$\exists \,\,\,\,f'_+(x_0)=\lim_{x\rightarrow x_0^+}\dfrac{f(x)-f(x_0)}{x-x_0}$$  but with possibly $f'_-(x_0) \not= f'_+(x_0)$,  does this still imply continuity of $f$ ?",,"['real-analysis', 'calculus', 'limits', 'derivatives', 'continuity']"
50,Is $\lim_{n\to\infty}2^{-n/2}\sum_{k=1}^{n-1}\frac{k^2\cdot2^{k/2}}{k+\frac{1}{2}}\left(\frac{\Gamma(k)}{\Gamma(k+\frac{1}{2})}\right)^2=1+\sqrt{2}$?,Is ?,\lim_{n\to\infty}2^{-n/2}\sum_{k=1}^{n-1}\frac{k^2\cdot2^{k/2}}{k+\frac{1}{2}}\left(\frac{\Gamma(k)}{\Gamma(k+\frac{1}{2})}\right)^2=1+\sqrt{2},"After I've read a question posted on MathOverflow I tried to combine it with the statement of a problem published in the American Mathematical Monthly (that I've omitted, in fact in my sum are dropped two terms since the Gamma function is undefined at such points). That is, the MONTLHY published a theorem that now I've specialized for some sequeces and numbers, and after I've calculated with Wolfram Alpha online calculator my conjecture, the following in next question. Question. Prove or refute $$\lim_{n\to\infty}2^{\frac{-n}{2}}\sum_{k=1}^{n-1}\frac{k^2\cdot2^{\frac{k}{2}}}{k+\frac{1}{2}}\left(\frac{\Gamma(k)}{\Gamma(k+\frac{1}{2})}\right)^2=1+\sqrt{2},\tag{1}$$   where $\Gamma(s)$ denotes the gamma function. Many thanks. I don't know if from the detailed and interesting solution of MONTHLY's problem can be deduced my closed-form $(1)$ easily (I do not think so). Thus my conjecture is based on experiments with the mentioned online tool and my code sum 2^(-100000000/2) k^2 2^(k/2)/(k+1/2) (Gamma(k)/Gamma(k+1/2))^2, from k=1 to 100000000-1 or clik over More digits after you run this code sum 2^(-1000000000/2) k^2 2^(k/2)/(k+1/2) (Gamma(k)/Gamma(k+1/2))^2, from k=1 to 1000000000-1 I know that should be useful Stirling's approximation or some inequality related to the gamma function, combined with some numeric method of summation. References: [1] The sum of an hydrogen atom related infinite series , posted on MathOverflow with quote to the article by Tamar Friedmann and C. R. Hagen, Quantum Mechanical Derivation of the Wallis Formula for $\pi$, Journal of Mathematical Physics 56 , 112101 (2015). [2] Problem E 1760 [1965,183] A Convergent Sequence Arising from a Difference Equation , proposed by I. I. Kolodner, American Mathematical Monthly Vol. 73, No. 4, (1966), pages 414-415.","After I've read a question posted on MathOverflow I tried to combine it with the statement of a problem published in the American Mathematical Monthly (that I've omitted, in fact in my sum are dropped two terms since the Gamma function is undefined at such points). That is, the MONTLHY published a theorem that now I've specialized for some sequeces and numbers, and after I've calculated with Wolfram Alpha online calculator my conjecture, the following in next question. Question. Prove or refute $$\lim_{n\to\infty}2^{\frac{-n}{2}}\sum_{k=1}^{n-1}\frac{k^2\cdot2^{\frac{k}{2}}}{k+\frac{1}{2}}\left(\frac{\Gamma(k)}{\Gamma(k+\frac{1}{2})}\right)^2=1+\sqrt{2},\tag{1}$$   where $\Gamma(s)$ denotes the gamma function. Many thanks. I don't know if from the detailed and interesting solution of MONTHLY's problem can be deduced my closed-form $(1)$ easily (I do not think so). Thus my conjecture is based on experiments with the mentioned online tool and my code sum 2^(-100000000/2) k^2 2^(k/2)/(k+1/2) (Gamma(k)/Gamma(k+1/2))^2, from k=1 to 100000000-1 or clik over More digits after you run this code sum 2^(-1000000000/2) k^2 2^(k/2)/(k+1/2) (Gamma(k)/Gamma(k+1/2))^2, from k=1 to 1000000000-1 I know that should be useful Stirling's approximation or some inequality related to the gamma function, combined with some numeric method of summation. References: [1] The sum of an hydrogen atom related infinite series , posted on MathOverflow with quote to the article by Tamar Friedmann and C. R. Hagen, Quantum Mechanical Derivation of the Wallis Formula for $\pi$, Journal of Mathematical Physics 56 , 112101 (2015). [2] Problem E 1760 [1965,183] A Convergent Sequence Arising from a Difference Equation , proposed by I. I. Kolodner, American Mathematical Monthly Vol. 73, No. 4, (1966), pages 414-415.",,"['real-analysis', 'sequences-and-series']"
51,$\lim_{x \to x_0}{\frac{\arcsin x-\arcsin x_0}{x-x_0}}=?$ [duplicate],[duplicate],\lim_{x \to x_0}{\frac{\arcsin x-\arcsin x_0}{x-x_0}}=?,"This question already has answers here : question about the limit $\lim_{h\to0}\frac{\arcsin(x+h)-\arcsin(x)}{h}$ (4 answers) Closed 6 years ago . I have the following problem: $$ \lim_{x \to x_0}{\frac{\arcsin x-\arcsin x_0}{x-x_0}}=\text{?} $$ What I have: Let $x=\sin t$. Then the problem becomes: $$ \lim_{t \to t_0}{\frac{t-t_0}{\sin t-\sin t_0}} = \lim_{t \to t_0}{\frac{\frac{t-t_0}{2}}{\sin (\frac{t-t_0}{2}) \cos{\frac{t+t_0}{2}}}}=\lim_{t \to t_0}{\frac{1}{\cos(\frac{t+t_0}{2})}}=\frac{1}{\sqrt{1-\sin^2(t_0)}} $$ So I'm kind of stuck here. I'm not very good with trigonometric formulas. Assuming so far the solution is correct, how do I continue from here? Thanks in advance!","This question already has answers here : question about the limit $\lim_{h\to0}\frac{\arcsin(x+h)-\arcsin(x)}{h}$ (4 answers) Closed 6 years ago . I have the following problem: $$ \lim_{x \to x_0}{\frac{\arcsin x-\arcsin x_0}{x-x_0}}=\text{?} $$ What I have: Let $x=\sin t$. Then the problem becomes: $$ \lim_{t \to t_0}{\frac{t-t_0}{\sin t-\sin t_0}} = \lim_{t \to t_0}{\frac{\frac{t-t_0}{2}}{\sin (\frac{t-t_0}{2}) \cos{\frac{t+t_0}{2}}}}=\lim_{t \to t_0}{\frac{1}{\cos(\frac{t+t_0}{2})}}=\frac{1}{\sqrt{1-\sin^2(t_0)}} $$ So I'm kind of stuck here. I'm not very good with trigonometric formulas. Assuming so far the solution is correct, how do I continue from here? Thanks in advance!",,"['real-analysis', 'limits', 'trigonometry']"
52,"Does ""limit not equal X"" mean that the limit exist?","Does ""limit not equal X"" mean that the limit exist?",,"If we say that $\lim_{n\to \infty}a_n\not=X$, does it implicitly mean that the limit exists? In other words, does this negation means: ""it's not true that 'X is the limit' "" (and maybe there is not a limit at all) or "" the limit doesn't equal X"" (and therefore, there is a limit, because otherwise, we couldn't speak about it)","If we say that $\lim_{n\to \infty}a_n\not=X$, does it implicitly mean that the limit exists? In other words, does this negation means: ""it's not true that 'X is the limit' "" (and maybe there is not a limit at all) or "" the limit doesn't equal X"" (and therefore, there is a limit, because otherwise, we couldn't speak about it)",,"['limits', 'terminology']"
53,Prove $e^\alpha = \lim_{n\to\infty}(1+\frac{\alpha}{n})^n$ from first principles.,Prove  from first principles.,e^\alpha = \lim_{n\to\infty}(1+\frac{\alpha}{n})^n,"Prove $$e^\alpha = \lim_{n\to\infty}(1+\frac{\alpha}{n})^n$$ from first principles. I know the proof for $e^1$, so in particular how could a generic formula for that limit (either with squeeze theorem, or just basic manipulation of your choice) lead to the general formula above? Feel free to use whatever proof you want, as long as it is generally understandable for people with only a beginning/intermediate understanding of calculus.","Prove $$e^\alpha = \lim_{n\to\infty}(1+\frac{\alpha}{n})^n$$ from first principles. I know the proof for $e^1$, so in particular how could a generic formula for that limit (either with squeeze theorem, or just basic manipulation of your choice) lead to the general formula above? Feel free to use whatever proof you want, as long as it is generally understandable for people with only a beginning/intermediate understanding of calculus.",,"['limits', 'exponential-function']"
54,Evaluate $\lim_{n\rightarrow \infty} \Gamma(n+\frac{1}{2})/ \left( \sqrt{2n\pi} \Gamma(n) \right)$ using Stirling's formula.,Evaluate  using Stirling's formula.,\lim_{n\rightarrow \infty} \Gamma(n+\frac{1}{2})/ \left( \sqrt{2n\pi} \Gamma(n) \right),"I am working on the limit $$ \displaystyle\lim_{n\rightarrow \infty} \frac{\Gamma(n+\frac{1}{2})}{ \sqrt{2n\pi}\, \Gamma(n)}\,. $$ I am thinking I may be able to use Stirling's formula, but they are slightly different, and I am having trouble relating the two. Any help is appreciated. Stirling's formula says that the limit is 1 as $n$ approaches infinity of the following: $$\Gamma(n) / ( \sqrt{2\pi} n^{n - \frac{1}{2}}e^{-n})$$ In particular, how do I relate $\Gamma(n)$ to $n^{n}$ and $e^{-n}$? Not sure how do deal with those two terms.","I am working on the limit $$ \displaystyle\lim_{n\rightarrow \infty} \frac{\Gamma(n+\frac{1}{2})}{ \sqrt{2n\pi}\, \Gamma(n)}\,. $$ I am thinking I may be able to use Stirling's formula, but they are slightly different, and I am having trouble relating the two. Any help is appreciated. Stirling's formula says that the limit is 1 as $n$ approaches infinity of the following: $$\Gamma(n) / ( \sqrt{2\pi} n^{n - \frac{1}{2}}e^{-n})$$ In particular, how do I relate $\Gamma(n)$ to $n^{n}$ and $e^{-n}$? Not sure how do deal with those two terms.",,"['calculus', 'limits', 'special-functions', 'gamma-function']"
55,Calculating $\lim_{x \rightarrow 1}(\frac{x^2-\sqrt x}{\sqrt x-1})$ algebraically,Calculating  algebraically,\lim_{x \rightarrow 1}(\frac{x^2-\sqrt x}{\sqrt x-1}),"I'm trying to calculate $\lim_\limits{x \rightarrow 1}(\frac{x^2-\sqrt x}{\sqrt x-1})$ algebraically and I've pretty much tried everything I can think of and I keep getting the limit to be $\frac{0}{0}$ which is obviously not right. I've tried multiplying the numerator and denominator by the conjugate of the denominator, I've tried using the difference of two squares on the numerator and I've tried dividing the rational function by $x^2$ but I keep on getting the wrong answer.","I'm trying to calculate $\lim_\limits{x \rightarrow 1}(\frac{x^2-\sqrt x}{\sqrt x-1})$ algebraically and I've pretty much tried everything I can think of and I keep getting the limit to be $\frac{0}{0}$ which is obviously not right. I've tried multiplying the numerator and denominator by the conjugate of the denominator, I've tried using the difference of two squares on the numerator and I've tried dividing the rational function by $x^2$ but I keep on getting the wrong answer.",,"['calculus', 'limits', 'limits-without-lhopital']"
56,"If lim x→0 f(x) exists, then lim x→0 f(x) > 0.”","If lim x→0 f(x) exists, then lim x→0 f(x) > 0.”",,"Show that the following claim is false with a counterexample: “Let f be a function with domain R. Assume that ∀x ∈ R, f(x) > 0. If lim x→0 f(x) exists, then lim x→0 f(x) > 0.” Do i negate this and then prove it? and how can i fix it to be true","Show that the following claim is false with a counterexample: “Let f be a function with domain R. Assume that ∀x ∈ R, f(x) > 0. If lim x→0 f(x) exists, then lim x→0 f(x) > 0.” Do i negate this and then prove it? and how can i fix it to be true",,"['calculus', 'limits', 'functions']"
57,Indeterminate forms limits,Indeterminate forms limits,,Why did $∞^∞$ is not an indeterminate form ?  We have seven indeterminate form $$0/0 $$ $$∞/∞$$ $$0\cdot∞$$ $$∞-∞$$ $$0^0$$ $$1^{\infty}$$ $$∞^0$$ but it does not have $$∞^∞ $$why,Why did $∞^∞$ is not an indeterminate form ?  We have seven indeterminate form $$0/0 $$ $$∞/∞$$ $$0\cdot∞$$ $$∞-∞$$ $$0^0$$ $$1^{\infty}$$ $$∞^0$$ but it does not have $$∞^∞ $$why,,['limits']
58,Establishing the convergence or divergence of the sequences $(x_n)$,Establishing the convergence or divergence of the sequences,(x_n),Establish the convergence or the divergence of the sequence $$(x_n) = \frac{(-1)^n n}{n+1}$$ At the moment all I can conclude is that $$(x_n) = \frac{(-1)^n n}{n+1} < (-1)^n\left(\frac{n}{n}\right) = (-1)^n$$ So $(x_n)$ is bounded above by $1$ and below by $-1$. How can I show that $(x_n)$ is convergent or isn't convergent?,Establish the convergence or the divergence of the sequence $$(x_n) = \frac{(-1)^n n}{n+1}$$ At the moment all I can conclude is that $$(x_n) = \frac{(-1)^n n}{n+1} < (-1)^n\left(\frac{n}{n}\right) = (-1)^n$$ So $(x_n)$ is bounded above by $1$ and below by $-1$. How can I show that $(x_n)$ is convergent or isn't convergent?,,"['real-analysis', 'sequences-and-series', 'limits']"
59,L'Hopital Application Problem [closed],L'Hopital Application Problem [closed],,"Closed. This question is off-topic . It is not currently accepting answers. This question is missing context or other details : Please improve the question by providing additional context, which ideally includes your thoughts on the problem and any attempts you have made to solve it. This information helps others identify where you have difficulties and helps them write answers appropriate to your experience level. Closed 6 years ago . Improve this question Suppose that $f(x)$ is a twice-differentiable function such that $\lim_{x\to 3}\dfrac{12x-36}{3f(x)-6}=-1$. Find the equation of the tangent line to $f(x)$ at $x=3$. Write your answer in the form $\verb#y=mx+b#$. My issue with this is that I've tried to simplify it multiple times but I keep irrationally leading to 0=-1. Any suggestions?","Closed. This question is off-topic . It is not currently accepting answers. This question is missing context or other details : Please improve the question by providing additional context, which ideally includes your thoughts on the problem and any attempts you have made to solve it. This information helps others identify where you have difficulties and helps them write answers appropriate to your experience level. Closed 6 years ago . Improve this question Suppose that $f(x)$ is a twice-differentiable function such that $\lim_{x\to 3}\dfrac{12x-36}{3f(x)-6}=-1$. Find the equation of the tangent line to $f(x)$ at $x=3$. Write your answer in the form $\verb#y=mx+b#$. My issue with this is that I've tried to simplify it multiple times but I keep irrationally leading to 0=-1. Any suggestions?",,"['calculus', 'limits']"
60,Asymptotics of sum without Euler-Maclaurin [duplicate],Asymptotics of sum without Euler-Maclaurin [duplicate],,This question already has answers here : Methods to find $\lim\limits_{n\to\infty}\frac1n\sum\limits_{k=1}^nn^{1/k} $ (5 answers) Closed 6 years ago . I want to show that $$\lim_{n \to \infty} \frac{1}{n} \sum_{k=1}^n n^{1/k} = 2$$ I found the limit with Euler-Maclaurin but this was messy involving integral $\int_1^nn^{1/x}$dx. What is an easier way using squeeze or other technique?,This question already has answers here : Methods to find $\lim\limits_{n\to\infty}\frac1n\sum\limits_{k=1}^nn^{1/k} $ (5 answers) Closed 6 years ago . I want to show that $$\lim_{n \to \infty} \frac{1}{n} \sum_{k=1}^n n^{1/k} = 2$$ I found the limit with Euler-Maclaurin but this was messy involving integral $\int_1^nn^{1/x}$dx. What is an easier way using squeeze or other technique?,,"['limits', 'asymptotics']"
61,Show that $\left( \frac {11} {10}\right) ^{n}$ is divergent.,Show that  is divergent.,\left( \frac {11} {10}\right) ^{n},"Show that $\left( \dfrac {11} {10}\right) ^{n}$ is divergent. My proof. Let $B\in\mathbb{R}$. By the Archimedean property there is a $N$ in $\mathbb{N}$ such that $N>B$. Let $\varepsilon >0$ By the Bernoulli inequality, we have $\left( 1+\varepsilon \right) ^{n}\geq 1+n\varepsilon$ for all $n\in\mathbb{N}$. Now, take $\varepsilon=( \dfrac {11} {10}-1)$. Then, we obtain, $\left( \dfrac {11} {10}\right) ^{n}\geq \dfrac {n} {10}+1$. So, for all $n\geq N$ we have $\dfrac {n} {10}+1>\dfrac {n} {10}>n>N>B.$ Thus, since $\left( \dfrac {11} {10}\right) ^{n}\geq \dfrac {n} {10}+1$,  $\left( \dfrac {11} {10}\right) ^{n}>B$ for all $n\geq N$. We are done. Can you check my proof?","Show that $\left( \dfrac {11} {10}\right) ^{n}$ is divergent. My proof. Let $B\in\mathbb{R}$. By the Archimedean property there is a $N$ in $\mathbb{N}$ such that $N>B$. Let $\varepsilon >0$ By the Bernoulli inequality, we have $\left( 1+\varepsilon \right) ^{n}\geq 1+n\varepsilon$ for all $n\in\mathbb{N}$. Now, take $\varepsilon=( \dfrac {11} {10}-1)$. Then, we obtain, $\left( \dfrac {11} {10}\right) ^{n}\geq \dfrac {n} {10}+1$. So, for all $n\geq N$ we have $\dfrac {n} {10}+1>\dfrac {n} {10}>n>N>B.$ Thus, since $\left( \dfrac {11} {10}\right) ^{n}\geq \dfrac {n} {10}+1$,  $\left( \dfrac {11} {10}\right) ^{n}>B$ for all $n\geq N$. We are done. Can you check my proof?",,"['analysis', 'limits']"
62,$(a_n)$ is a bounded sequence and every convergent subsequence of $(a_n)$ converges to $a\in\mathbb{R}$. Show $(a_{n})$ converges to $a$.,is a bounded sequence and every convergent subsequence of  converges to . Show  converges to .,(a_n) (a_n) a\in\mathbb{R} (a_{n}) a,"To me, the problem is asking: Assume $(a_n)$ is a bounded sequence.  Prove:$[\text{Every convergent subsequence of }(a_n)\text{ converges to }a]\implies[\lim a_n=a]$ Step 0: We decide we'll prove the contrapositive, so the problem is now: Assume $(a_n)$ is a bounded sequence. Prove: $[\lim a_n\ne a]\implies[\text{There exists a convergent subsequence of }(a_n)\text{ that does not converge to }a ]$ Step 1: $\lim a_n\ne a\implies\exists\ \epsilon_{0}>0:|a_n-a|\ge \epsilon_{0}\ \forall\ n\in\mathbb{N}$ Step 2: $(a_n)$ is bounded $\implies$ $(a_n)$ has a convergent subsequence. Call this convergent subsequence $(a_{n_{k}})$. Step 3: The only things we know about $(a_{n_{k}})$ are that A) it converges to something and B) it's a subsequence of $(a_n)$. I think the only task left is to show $(a_{n_{k}})$ cannot converge specifically to $a$. Step 4: To show $(a_{n_{k}})$ does not converge to $a$,  we must show $\exists\ \epsilon_{1}>0:|a_{n_{k}}-a|\ge\epsilon_{1}\ \forall\ k\in\mathbb{N}$. Step 5: $(a_{n_{k}})$ is a subsequence of $(a_n)$ $\implies$ All the terms of $(a_{n_{k}})$ come from $(a_{n})$. Since all terms of $(a_{n})$ are at least $\epsilon_{0}$ away from  $a$, then so are all terms of $(a_{n_{k}})$. Step 6: Let $\epsilon_{1}=\epsilon_{0}$. Then we have found an $\epsilon_{1}$ such that $|a_{n_{k}}-a|\ge \epsilon_{1}\ \forall\ k\in\mathbb{N}$. Conclusion: $(a_{n_{k}})$ is a convergent subsequence of $(a_{n})$ (step 2) that does not converge to $a$ (steps 4-6). So we've found a convergent subsequence of $(a_{n})$ that doesn't converge to $a$. I wrote this in these steps so it's easier to point out where I'm wrong. I don't understand why every solution I read (that doesn't involve limit superior) requires a sub-subsequence. It seems to me the reason the convergent sub-subsequence doesn't converge to $a$ is the same reason the convergent subsequence $(a_{n_{k}})$ doesn't converge to $a$, so why do we have to make a sub-subsequence? I know this exact question has been asked on here ad nauseam. I read the posts I could find, but I still don't understand why a sub-subsequence is necessary. I'm self-learning, and I've been stuck on this for the past three days. I decided to finally just ask. I'm sorry.","To me, the problem is asking: Assume $(a_n)$ is a bounded sequence.  Prove:$[\text{Every convergent subsequence of }(a_n)\text{ converges to }a]\implies[\lim a_n=a]$ Step 0: We decide we'll prove the contrapositive, so the problem is now: Assume $(a_n)$ is a bounded sequence. Prove: $[\lim a_n\ne a]\implies[\text{There exists a convergent subsequence of }(a_n)\text{ that does not converge to }a ]$ Step 1: $\lim a_n\ne a\implies\exists\ \epsilon_{0}>0:|a_n-a|\ge \epsilon_{0}\ \forall\ n\in\mathbb{N}$ Step 2: $(a_n)$ is bounded $\implies$ $(a_n)$ has a convergent subsequence. Call this convergent subsequence $(a_{n_{k}})$. Step 3: The only things we know about $(a_{n_{k}})$ are that A) it converges to something and B) it's a subsequence of $(a_n)$. I think the only task left is to show $(a_{n_{k}})$ cannot converge specifically to $a$. Step 4: To show $(a_{n_{k}})$ does not converge to $a$,  we must show $\exists\ \epsilon_{1}>0:|a_{n_{k}}-a|\ge\epsilon_{1}\ \forall\ k\in\mathbb{N}$. Step 5: $(a_{n_{k}})$ is a subsequence of $(a_n)$ $\implies$ All the terms of $(a_{n_{k}})$ come from $(a_{n})$. Since all terms of $(a_{n})$ are at least $\epsilon_{0}$ away from  $a$, then so are all terms of $(a_{n_{k}})$. Step 6: Let $\epsilon_{1}=\epsilon_{0}$. Then we have found an $\epsilon_{1}$ such that $|a_{n_{k}}-a|\ge \epsilon_{1}\ \forall\ k\in\mathbb{N}$. Conclusion: $(a_{n_{k}})$ is a convergent subsequence of $(a_{n})$ (step 2) that does not converge to $a$ (steps 4-6). So we've found a convergent subsequence of $(a_{n})$ that doesn't converge to $a$. I wrote this in these steps so it's easier to point out where I'm wrong. I don't understand why every solution I read (that doesn't involve limit superior) requires a sub-subsequence. It seems to me the reason the convergent sub-subsequence doesn't converge to $a$ is the same reason the convergent subsequence $(a_{n_{k}})$ doesn't converge to $a$, so why do we have to make a sub-subsequence? I know this exact question has been asked on here ad nauseam. I read the posts I could find, but I still don't understand why a sub-subsequence is necessary. I'm self-learning, and I've been stuck on this for the past three days. I decided to finally just ask. I'm sorry.",,"['real-analysis', 'sequences-and-series', 'limits']"
63,When is the limit of a sequence over $n$ the limit of the differences? [closed],When is the limit of a sequence over  the limit of the differences? [closed],n,"Closed. This question does not meet Mathematics Stack Exchange guidelines . It is not currently accepting answers. Please provide additional context , which ideally explains why the question is relevant to you and our  community. Some forms of context include: background and motivation, relevant definitions, source, possible strategies, your current progress, why the question is interesting or important, etc. Closed 4 years ago . Improve this question Under what conditions on the sequence $a_n$ does $$ \lim_{n→∞} \frac{a_n}n = \lim_{n→∞} (a_{n+1} − a_n ) $$ hold?","Closed. This question does not meet Mathematics Stack Exchange guidelines . It is not currently accepting answers. Please provide additional context , which ideally explains why the question is relevant to you and our  community. Some forms of context include: background and motivation, relevant definitions, source, possible strategies, your current progress, why the question is interesting or important, etc. Closed 4 years ago . Improve this question Under what conditions on the sequence $a_n$ does $$ \lim_{n→∞} \frac{a_n}n = \lim_{n→∞} (a_{n+1} − a_n ) $$ hold?",,"['sequences-and-series', 'limits']"
64,"$f:[0,\infty)\rightarrow\mathbb{R}$ is differentiable and $\lim_{x\to\infty}f'(x)=0$, prove $\lim_{x \rightarrow \infty}[f(x+1)-f(x)]=0$. [duplicate]","is differentiable and , prove . [duplicate]","f:[0,\infty)\rightarrow\mathbb{R} \lim_{x\to\infty}f'(x)=0 \lim_{x \rightarrow \infty}[f(x+1)-f(x)]=0","This question already has an answer here : If $f:\mathbb{R}\rightarrow \mathbb{R}$ is differentiable and $\lim_{x\to \infty } f^\prime(x)=0$ show that $\lim _{x\to \infty } (f(x+1)-f(x))=0$. (1 answer) Closed 5 years ago . Assume that $f: [0, \infty) \rightarrow \mathbb{R}$ is differentiable    for all $x>0$ and $\lim_{x \rightarrow \infty} f'(x) = 0$ . Prove that $\lim_{x \rightarrow \infty}[f(x+1)-f(x)] = 0$ I was hinted that I should use the mean value theorem here. My attempt is as follows. Consider the closed interval $[x, x+1]$ where $x>0$ . Clearly, $f$ is continuous on $[x, x+1]$ and also differentiable on $(x, x+1)$ by the assumptions of the question. So we can apply the MVT and conclude that there exists a $c \in (x, x+1)$ such that $f(x+1) - f(x) = f'(c)$ . Now if I take the limit to infinity on the left hand side, I can see the $\lim_{x \rightarrow \infty} f(x+1) - f(x)$ come into play, but what is $\lim_{x \rightarrow \infty} f'(c)$ ? I thought about something like this, but not sure if it is right. Clearly, $c = x+t$ for some $0<t<1$ , so $f'(c) = f'(x+t)$ , so $\lim_{x \rightarrow \infty} f'(c) = \lim_{x \rightarrow \infty} f'(x+t)$ . Now I am not sure how to bring $\lim_{x \rightarrow \infty}f'(x) = 0$ into the picture.","This question already has an answer here : If $f:\mathbb{R}\rightarrow \mathbb{R}$ is differentiable and $\lim_{x\to \infty } f^\prime(x)=0$ show that $\lim _{x\to \infty } (f(x+1)-f(x))=0$. (1 answer) Closed 5 years ago . Assume that is differentiable    for all and . Prove that I was hinted that I should use the mean value theorem here. My attempt is as follows. Consider the closed interval where . Clearly, is continuous on and also differentiable on by the assumptions of the question. So we can apply the MVT and conclude that there exists a such that . Now if I take the limit to infinity on the left hand side, I can see the come into play, but what is ? I thought about something like this, but not sure if it is right. Clearly, for some , so , so . Now I am not sure how to bring into the picture.","f: [0, \infty) \rightarrow \mathbb{R} x>0 \lim_{x \rightarrow \infty} f'(x) = 0 \lim_{x \rightarrow \infty}[f(x+1)-f(x)] = 0 [x, x+1] x>0 f [x, x+1] (x, x+1) c \in (x, x+1) f(x+1) - f(x) = f'(c) \lim_{x \rightarrow \infty} f(x+1) - f(x) \lim_{x \rightarrow \infty} f'(c) c = x+t 0<t<1 f'(c) = f'(x+t) \lim_{x \rightarrow \infty} f'(c) = \lim_{x \rightarrow \infty} f'(x+t) \lim_{x \rightarrow \infty}f'(x) = 0","['calculus', 'limits']"
65,solve trigonometric lim with tan,solve trigonometric lim with tan,,"Hi guys I tried to solve the following problem lately and got stucked. I'd love to get some help and guidance.  consider this: $$\lim_{x \rightarrow\infty}\tan\left(\frac{\pi x}{2x+1}\right)^{\frac{1}{x}} $$ I did the following: 1. break to sin and cos : $$ \lim_{x \rightarrow\infty}\tan\left(\frac{\pi x}{2x+1}\right)^{\frac{1}{x}} = \lim_{x\rightarrow\infty}\frac{\left(\sin\left(\frac{\pi x}{2x+1}\right)\right)^\frac{1}{x}}{\left(\cos\left(\frac{\pi x}{2x+1}\right)\right)^\frac{1}{x}}$$ use log rules: $$\lim_{x\rightarrow\infty}\frac{\left(\sin\left(\frac{\pi x}{2x+1}\right)\right)^\frac{1}{x}}{\left(\cos\left(\frac{\pi x}{2x+1}\right)\right)^\frac{1}{x}} =  \lim_{x\rightarrow\infty}\frac{e^{\frac{1}{x}\ln\left(\sin\left(\frac{\pi x}{2x+1}\right)\right)}}{e^{\frac{1}{x}\ln\left(\cos\left(\frac{\pi x}{2x+1}\right)\right)}}$$ Numerator converges to one so I am going to work with the denominator: $$  \lim_{x \rightarrow \infty}e^{\frac{1}{x}\ln\left(\cos\left(\frac{\pi x}{2x+1}\right)\right)} = e^\left(\lim_{x \rightarrow \infty}{\frac{1}{x}\ln\left(\cos\left(\frac{\pi x}{2x+1}\right)\right)}\right)  $$ $$ \lim_{x \rightarrow \infty}{\frac{1}{x}\ln\left(\cos\left(\frac{\pi x}{2x+1}\right)\right)} = \frac{-\infty}{\infty}$$ at this point I tried to apply l'hopital but encountered a huge mess, and now I feel that maybe I chose the wrong way. Please help me guys.","Hi guys I tried to solve the following problem lately and got stucked. I'd love to get some help and guidance.  consider this: $$\lim_{x \rightarrow\infty}\tan\left(\frac{\pi x}{2x+1}\right)^{\frac{1}{x}} $$ I did the following: 1. break to sin and cos : $$ \lim_{x \rightarrow\infty}\tan\left(\frac{\pi x}{2x+1}\right)^{\frac{1}{x}} = \lim_{x\rightarrow\infty}\frac{\left(\sin\left(\frac{\pi x}{2x+1}\right)\right)^\frac{1}{x}}{\left(\cos\left(\frac{\pi x}{2x+1}\right)\right)^\frac{1}{x}}$$ use log rules: $$\lim_{x\rightarrow\infty}\frac{\left(\sin\left(\frac{\pi x}{2x+1}\right)\right)^\frac{1}{x}}{\left(\cos\left(\frac{\pi x}{2x+1}\right)\right)^\frac{1}{x}} =  \lim_{x\rightarrow\infty}\frac{e^{\frac{1}{x}\ln\left(\sin\left(\frac{\pi x}{2x+1}\right)\right)}}{e^{\frac{1}{x}\ln\left(\cos\left(\frac{\pi x}{2x+1}\right)\right)}}$$ Numerator converges to one so I am going to work with the denominator: $$  \lim_{x \rightarrow \infty}e^{\frac{1}{x}\ln\left(\cos\left(\frac{\pi x}{2x+1}\right)\right)} = e^\left(\lim_{x \rightarrow \infty}{\frac{1}{x}\ln\left(\cos\left(\frac{\pi x}{2x+1}\right)\right)}\right)  $$ $$ \lim_{x \rightarrow \infty}{\frac{1}{x}\ln\left(\cos\left(\frac{\pi x}{2x+1}\right)\right)} = \frac{-\infty}{\infty}$$ at this point I tried to apply l'hopital but encountered a huge mess, and now I feel that maybe I chose the wrong way. Please help me guys.",,"['calculus', 'limits', 'trigonometry']"
66,How do we prove $\lim\inf s_n\le\lim\inf\sigma_n$ in the following case?,How do we prove  in the following case?,\lim\inf s_n\le\lim\inf\sigma_n,"Let $s_n$ be a sequence of nonnegative numbers, and for each $n$ define $\sigma_n=\frac1n (s_1+s_2+...+s_n)$, how do we prove  $\lim\inf s_n\le\lim\inf\sigma_n$? I was told two ways could be achieved: $1.$ Suppose we have proved $\lim\sup \sigma_n\le\lim\sup s_n$, then we can do a little trick to prove the $\lim\inf$ version. My thought is that if we can somehow make $\lim\sup -\sigma_n\le\lim\sup -s_n$, then we can do $\lim\inf\sigma_n=-\lim\sup -\sigma_n\ge-\lim\sup -s_n=\lim\inf s_n$, but the problem is how do we make $\lim\sup -\sigma_n\le\lim\sup -s_n$. $2.$ Directly prove it. My thought is that we can pick a $N$, $n>N\implies$ $\sigma_n=\frac1n (s_1+s_2+...+s_n)\ge \min\{s_1, ...,s_n\}\ge\inf s_n$. Since $\inf s_n$ is lower bound for $\sigma_n$, $\inf s_n\le\inf\sigma_n$, then taking the limit, we get $\lim\inf s_n\le\lim\inf\sigma_n$. But I felt taking limit part is not quite logical to me. Could someone help?","Let $s_n$ be a sequence of nonnegative numbers, and for each $n$ define $\sigma_n=\frac1n (s_1+s_2+...+s_n)$, how do we prove  $\lim\inf s_n\le\lim\inf\sigma_n$? I was told two ways could be achieved: $1.$ Suppose we have proved $\lim\sup \sigma_n\le\lim\sup s_n$, then we can do a little trick to prove the $\lim\inf$ version. My thought is that if we can somehow make $\lim\sup -\sigma_n\le\lim\sup -s_n$, then we can do $\lim\inf\sigma_n=-\lim\sup -\sigma_n\ge-\lim\sup -s_n=\lim\inf s_n$, but the problem is how do we make $\lim\sup -\sigma_n\le\lim\sup -s_n$. $2.$ Directly prove it. My thought is that we can pick a $N$, $n>N\implies$ $\sigma_n=\frac1n (s_1+s_2+...+s_n)\ge \min\{s_1, ...,s_n\}\ge\inf s_n$. Since $\inf s_n$ is lower bound for $\sigma_n$, $\inf s_n\le\inf\sigma_n$, then taking the limit, we get $\lim\inf s_n\le\lim\inf\sigma_n$. But I felt taking limit part is not quite logical to me. Could someone help?",,"['calculus', 'limits', 'proof-verification', 'proof-explanation', 'limsup-and-liminf']"
67,"Limits of sequences from given conditions and relations between the general terms $a_n, b_n, c_n$.",Limits of sequences from given conditions and relations between the general terms .,"a_n, b_n, c_n","Let $a_n, b_n, c_n$ be sequences such that    $$a_n+b_n+c_n=2n+1,$$    $$a_nb_n +b_nc_n +a_nc_n=2n-1,$$    $${a_n}{b_n}{c_n} =-1.$$    It is also given that $a_n \gt b_n \gt c_n $. Then what is the value of the following? a) $\lim_{{n}\rightarrow {\infty}}\frac{a_n}{n} $. b) $\lim_{n\rightarrow{-\infty}}\frac{a_n}{n}$. I took $b_n+c_n=2n+1-a_n$ and $b_nc_n=\frac{-1}{a_n}$. Now the second condition which can be written as $(c_n+b_n)a_n + b_nc_n=2n-1$ now changes to $(2n+1-a_n)a_n - \frac{1}{a_n}=2n-1$. I have absolutely no idea on what to do next. I tried reducing $a_n$ into something in terms of $n$, but I have gotten nowhere. How can this question be solved?","Let $a_n, b_n, c_n$ be sequences such that    $$a_n+b_n+c_n=2n+1,$$    $$a_nb_n +b_nc_n +a_nc_n=2n-1,$$    $${a_n}{b_n}{c_n} =-1.$$    It is also given that $a_n \gt b_n \gt c_n $. Then what is the value of the following? a) $\lim_{{n}\rightarrow {\infty}}\frac{a_n}{n} $. b) $\lim_{n\rightarrow{-\infty}}\frac{a_n}{n}$. I took $b_n+c_n=2n+1-a_n$ and $b_nc_n=\frac{-1}{a_n}$. Now the second condition which can be written as $(c_n+b_n)a_n + b_nc_n=2n-1$ now changes to $(2n+1-a_n)a_n - \frac{1}{a_n}=2n-1$. I have absolutely no idea on what to do next. I tried reducing $a_n$ into something in terms of $n$, but I have gotten nowhere. How can this question be solved?",,"['sequences-and-series', 'limits']"
68,If $\int_{\frac{1}{n+1}}^{\frac{1}{n}}\frac{\arctan(nx)}{\arcsin(nx)}dx=c_n$ then $\lim_{n\to\infty}{n^2c_n}=?$,If  then,\int_{\frac{1}{n+1}}^{\frac{1}{n}}\frac{\arctan(nx)}{\arcsin(nx)}dx=c_n \lim_{n\to\infty}{n^2c_n}=?,$$\int_{\frac{1}{n+1}}^{\frac{1}{n}}\frac{\arctan(nx)}{\arcsin(nx)}dx=c_n$$ Find the value of  $$\lim_{n\to\infty}{n^2c_n}$$ I tried integrating the expression but couldn't. Need help.,$$\int_{\frac{1}{n+1}}^{\frac{1}{n}}\frac{\arctan(nx)}{\arcsin(nx)}dx=c_n$$ Find the value of  $$\lim_{n\to\infty}{n^2c_n}$$ I tried integrating the expression but couldn't. Need help.,,"['calculus', 'limits', 'limits-without-lhopital']"
69,Help with inverse trig derivatives with L'Hopital rule,Help with inverse trig derivatives with L'Hopital rule,,"so I have a problem with my teacher's notes and they have confused me. take this equation   \begin{align}       \lim _{x\to0}  \frac{\arcsin x}{ \sin x}      \end{align} if we do the derivative of the top and bottom using l'hopitals rule we get \begin{align}       (\arcsin x)'= \frac{1}{ \sqrt{1-x^2}}      \end{align} \begin{align}       (\sin x)'= \cos x      \end{align} but when my teacher put them together she did't explain how she somehow got this. \begin{align}        \lim _{x\to0}  \frac{\sqrt{1-x^2}}{ \cos x}      \end{align} Please help this makes no sense to me, why is the numerator no longer a fraction?","so I have a problem with my teacher's notes and they have confused me. take this equation   \begin{align}       \lim _{x\to0}  \frac{\arcsin x}{ \sin x}      \end{align} if we do the derivative of the top and bottom using l'hopitals rule we get \begin{align}       (\arcsin x)'= \frac{1}{ \sqrt{1-x^2}}      \end{align} \begin{align}       (\sin x)'= \cos x      \end{align} but when my teacher put them together she did't explain how she somehow got this. \begin{align}        \lim _{x\to0}  \frac{\sqrt{1-x^2}}{ \cos x}      \end{align} Please help this makes no sense to me, why is the numerator no longer a fraction?",,['calculus']
70,"Prove that $\lim_{(x,y)\to (0.0)} \frac{4xy^2 - 3x^3}{x^2 - y^2}$ does not exist",Prove that  does not exist,"\lim_{(x,y)\to (0.0)} \frac{4xy^2 - 3x^3}{x^2 - y^2}","I'm struggling with this limit. I have to approach using different curves and show that there is one curve wich prove that this limit does not exist , despite the fact that when trying with a lot of curves shows that the limit is 0. If someone helps me to find that curve I will be very pleased. Thanks! $$\lim_{(x,y)\to (0.0)} \frac{4xy^2 - 3x^3}{x^2 - y^2}$$","I'm struggling with this limit. I have to approach using different curves and show that there is one curve wich prove that this limit does not exist , despite the fact that when trying with a lot of curves shows that the limit is 0. If someone helps me to find that curve I will be very pleased. Thanks! $$\lim_{(x,y)\to (0.0)} \frac{4xy^2 - 3x^3}{x^2 - y^2}$$",,"['analysis', 'limits']"
71,"How do I prove using the definition of limit that $\lim_{(x,y)\rightarrow (0,0)}\frac {x^2+y^3}{y^2 - x + xy}$?",How do I prove using the definition of limit that ?,"\lim_{(x,y)\rightarrow (0,0)}\frac {x^2+y^3}{y^2 - x + xy}","I want to use the definition of limit in order to prove $\lim_{(x,y)\rightarrow (0,0)}\frac {x^2+y^3}{y^2 - x + xy}$, but I almost crack my head open trying to do this, it seems to be easy way but y cant find a $\delta$ such that $|\frac {x^2+y^3}{y^2 - x + xy}| \leq \epsilon$, any idea of how I might be able to find an inequality to transform the denominator in a nicer thing like $\frac{1}{\left\lVert (x,y)\right\rVert}$?","I want to use the definition of limit in order to prove $\lim_{(x,y)\rightarrow (0,0)}\frac {x^2+y^3}{y^2 - x + xy}$, but I almost crack my head open trying to do this, it seems to be easy way but y cant find a $\delta$ such that $|\frac {x^2+y^3}{y^2 - x + xy}| \leq \epsilon$, any idea of how I might be able to find an inequality to transform the denominator in a nicer thing like $\frac{1}{\left\lVert (x,y)\right\rVert}$?",,"['calculus', 'limits', 'multivariable-calculus']"
72,Proving that limit of dot product equals dot product of limit,Proving that limit of dot product equals dot product of limit,,"Suppose $\lim_{\mathbf{x} \to \mathbf{c}} \mathbf{f}(\mathbf{x})=\mathbf{L}$ and $\lim_{\mathbf{x} \to \mathbf{c}} \mathbf{g}(\mathbf{x})=\mathbf{K}$. I want to prove that $\lim_{\mathbf{x} \to \mathbf{c}} \mathbf{f}(\mathbf{x})\bullet \mathbf{g}(\mathbf{x})=\mathbf{L}\bullet \mathbf{K}$, where $\bullet$ denotes the Euclidean dot product. Let $\epsilon >0$. We know that $\exists \delta_1, \delta_2>0$ such that $\forall \mathbf{x} \in A : 0<\|\mathbf{x}-\mathbf{c}\|<\delta_1$ implies $\|\mathbf{f}(\mathbf{x})-\mathbf{L}\|<\epsilon$ and $0<\|\mathbf{x}-\mathbf{c}\|<\delta_2$ implies $\|\mathbf{g}(\mathbf{x})-\mathbf{K}\|<\epsilon$. Let $\delta=\min\{\delta_1, \delta_2\}$. We have $|\mathbf{f}(\mathbf{x})\bullet \mathbf{g}(\mathbf{x}) -\mathbf{L}\bullet \mathbf{K}|= |\mathbf{f}(\mathbf{x})\bullet \mathbf{g}(\mathbf{x}) -\mathbf{K} \bullet \mathbf{f}(\mathbf{x})+\mathbf{K} \bullet \mathbf{f}(\mathbf{x})-\mathbf{L}\bullet \mathbf{K}|=|\mathbf{f}(\mathbf{x})\bullet (\mathbf{g}(\mathbf{x})-\mathbf{K})+\mathbf{K}\bullet (\mathbf{f}(\mathbf{x})-\mathbf{L})|\leq |\mathbf{f}(\mathbf{x})\bullet (\mathbf{g}(\mathbf{x})-\mathbf{K})|+|\mathbf{K} \bullet (\mathbf{f}(\mathbf{x})-\mathbf{L})|$. The Cauchy-Schwartz inequality yields $|\mathbf{f}(\mathbf{x})\bullet \mathbf{g}(\mathbf{x}) -\mathbf{L}\bullet \mathbf{K}|\leq \|\mathbf{f}(\mathbf{x})\|\|\mathbf{g}(\mathbf{x})-\mathbf{K}\|+\|\mathbf{K}\|\|\mathbf{f}(\mathbf{x})-\mathbf{L}\|.$ If we let  $0<\|\mathbf{x} -\mathbf{c}\|<\delta$, I'll just need an upper bound for $\|\mathbf{f}(\mathbf{x})\|$, but I can't find one. I'd appreciate help finishing the argument.","Suppose $\lim_{\mathbf{x} \to \mathbf{c}} \mathbf{f}(\mathbf{x})=\mathbf{L}$ and $\lim_{\mathbf{x} \to \mathbf{c}} \mathbf{g}(\mathbf{x})=\mathbf{K}$. I want to prove that $\lim_{\mathbf{x} \to \mathbf{c}} \mathbf{f}(\mathbf{x})\bullet \mathbf{g}(\mathbf{x})=\mathbf{L}\bullet \mathbf{K}$, where $\bullet$ denotes the Euclidean dot product. Let $\epsilon >0$. We know that $\exists \delta_1, \delta_2>0$ such that $\forall \mathbf{x} \in A : 0<\|\mathbf{x}-\mathbf{c}\|<\delta_1$ implies $\|\mathbf{f}(\mathbf{x})-\mathbf{L}\|<\epsilon$ and $0<\|\mathbf{x}-\mathbf{c}\|<\delta_2$ implies $\|\mathbf{g}(\mathbf{x})-\mathbf{K}\|<\epsilon$. Let $\delta=\min\{\delta_1, \delta_2\}$. We have $|\mathbf{f}(\mathbf{x})\bullet \mathbf{g}(\mathbf{x}) -\mathbf{L}\bullet \mathbf{K}|= |\mathbf{f}(\mathbf{x})\bullet \mathbf{g}(\mathbf{x}) -\mathbf{K} \bullet \mathbf{f}(\mathbf{x})+\mathbf{K} \bullet \mathbf{f}(\mathbf{x})-\mathbf{L}\bullet \mathbf{K}|=|\mathbf{f}(\mathbf{x})\bullet (\mathbf{g}(\mathbf{x})-\mathbf{K})+\mathbf{K}\bullet (\mathbf{f}(\mathbf{x})-\mathbf{L})|\leq |\mathbf{f}(\mathbf{x})\bullet (\mathbf{g}(\mathbf{x})-\mathbf{K})|+|\mathbf{K} \bullet (\mathbf{f}(\mathbf{x})-\mathbf{L})|$. The Cauchy-Schwartz inequality yields $|\mathbf{f}(\mathbf{x})\bullet \mathbf{g}(\mathbf{x}) -\mathbf{L}\bullet \mathbf{K}|\leq \|\mathbf{f}(\mathbf{x})\|\|\mathbf{g}(\mathbf{x})-\mathbf{K}\|+\|\mathbf{K}\|\|\mathbf{f}(\mathbf{x})-\mathbf{L}\|.$ If we let  $0<\|\mathbf{x} -\mathbf{c}\|<\delta$, I'll just need an upper bound for $\|\mathbf{f}(\mathbf{x})\|$, but I can't find one. I'd appreciate help finishing the argument.",,"['real-analysis', 'limits']"
73,Does the limit exist when there is a removable discontinuity but the function takes a different value there?,Does the limit exist when there is a removable discontinuity but the function takes a different value there?,,"I know that when there is a hole the limit still exists because the limit is asking what value the function approaches, but does the limit still exist at the hole when the function takes a different value? In the graph on the right is the limit at $x=c$ equal to $L$?","I know that when there is a hole the limit still exists because the limit is asking what value the function approaches, but does the limit still exist at the hole when the function takes a different value? In the graph on the right is the limit at $x=c$ equal to $L$?",,"['calculus', 'limits']"
74,"Show that $f(x,y)=\frac{xy^2}{x^2+y^4}$ is bounded",Show that  is bounded,"f(x,y)=\frac{xy^2}{x^2+y^4}","Let $f\colon\mathbb R^2\to\mathbb R$ be a function given by:   $$ f(x,y)=\begin{cases}\frac{xy^2}{x^2+y^4}&\text{if }(x,y)\neq(0,0),\\ 0&\text{if }(x,y)=(0,0). \end{cases} $$ I need to show that $f$ is bounded on $\mathbb R^2$, so I need to show that there exists $M>0:\vert f(x,y)\vert\leq M$ for all $(x,y)\in\mathbb R^2$. We can rewrite $\begin{align}f(x,y)=\frac{x/y^2}{1+(x/y^2)^2}=\frac{z}{1+z^2}\end{align}$, where $z=x/y^2$ (and $y\neq0$). So for $z$ large enough, our expression will go to 0. Now we only need to worry for the case that $z$ approaches 0. We rewrite again: $\begin{align}f(x,y)=\frac{1}{1/z+z}\end{align}$, so $\lim_{z\to0}\frac{1}{1/z+z}=0$. Whatever $(x,y)$ do, if their values get small enough, we see that $f(x,y)$ will get arbitrarily close to zero, en if their values get big enough, $f(x,y)$ also comes arbitrarily close to zero. However, this can't be true, because $f(cy^2,y)=\frac{c}{1+c^2}$, so the function isn't even continuous on 0 to begin with. I'm stuck; can someone help me with this?","Let $f\colon\mathbb R^2\to\mathbb R$ be a function given by:   $$ f(x,y)=\begin{cases}\frac{xy^2}{x^2+y^4}&\text{if }(x,y)\neq(0,0),\\ 0&\text{if }(x,y)=(0,0). \end{cases} $$ I need to show that $f$ is bounded on $\mathbb R^2$, so I need to show that there exists $M>0:\vert f(x,y)\vert\leq M$ for all $(x,y)\in\mathbb R^2$. We can rewrite $\begin{align}f(x,y)=\frac{x/y^2}{1+(x/y^2)^2}=\frac{z}{1+z^2}\end{align}$, where $z=x/y^2$ (and $y\neq0$). So for $z$ large enough, our expression will go to 0. Now we only need to worry for the case that $z$ approaches 0. We rewrite again: $\begin{align}f(x,y)=\frac{1}{1/z+z}\end{align}$, so $\lim_{z\to0}\frac{1}{1/z+z}=0$. Whatever $(x,y)$ do, if their values get small enough, we see that $f(x,y)$ will get arbitrarily close to zero, en if their values get big enough, $f(x,y)$ also comes arbitrarily close to zero. However, this can't be true, because $f(cy^2,y)=\frac{c}{1+c^2}$, so the function isn't even continuous on 0 to begin with. I'm stuck; can someone help me with this?",,"['limits', 'functions']"
75,Can calculus be used for real number?,Can calculus be used for real number?,,"I recently learned that there is no positive infinitesimal real number. The only infinitesimal real number is 0. For calculus integral, dx is always interpreted as infinitesimal small but non-zero which contradict with the property of real number. And I don't think zero would be a valid infinitesimal number for calculus as real numbers divided by zero is undefined. So my questions are: 1) Can we use calculus for real number in strictly speaking? 2) Is it correct to say the infinitesimal concept for calculus doesn't exist under real number system?","I recently learned that there is no positive infinitesimal real number. The only infinitesimal real number is 0. For calculus integral, dx is always interpreted as infinitesimal small but non-zero which contradict with the property of real number. And I don't think zero would be a valid infinitesimal number for calculus as real numbers divided by zero is undefined. So my questions are: 1) Can we use calculus for real number in strictly speaking? 2) Is it correct to say the infinitesimal concept for calculus doesn't exist under real number system?",,"['calculus', 'real-analysis', 'limits', 'infinitesimals']"
76,How smooth is the generalized power mean function?,How smooth is the generalized power mean function?,,"Consider the function of power mean of $1$ and $e$ (so that the computation is simpler). That is, define $\displaystyle f(x)=(\frac{1+e^x}{2})^{1/x}$ when $x\neq 0$ and $f(0)=\sqrt{e}$. It's not difficult to use L'Hopital rule with std. exponential trick to verify the function is continuous. The question is, do we know how smooth (in which $C^k$) is this function (and a closed form formula of $f^{(n)}(0)$ if possible)? The only way I can think of is trying to compute $\displaystyle \frac{f^{(n)}(h)-f^{(n)}(0)}{h}$ to verify if $f^{(n+1)}(0)$ exists, but the formula becomes extremely complicated and it's practically impossible to compute that limit even for $n=1$.","Consider the function of power mean of $1$ and $e$ (so that the computation is simpler). That is, define $\displaystyle f(x)=(\frac{1+e^x}{2})^{1/x}$ when $x\neq 0$ and $f(0)=\sqrt{e}$. It's not difficult to use L'Hopital rule with std. exponential trick to verify the function is continuous. The question is, do we know how smooth (in which $C^k$) is this function (and a closed form formula of $f^{(n)}(0)$ if possible)? The only way I can think of is trying to compute $\displaystyle \frac{f^{(n)}(h)-f^{(n)}(0)}{h}$ to verify if $f^{(n+1)}(0)$ exists, but the formula becomes extremely complicated and it's practically impossible to compute that limit even for $n=1$.",,"['calculus', 'real-analysis']"
77,Compute $\lim\limits_{n\to \infty} \int_{0}^{\infty} \exp(-x)(\cos(x))^n dx$,Compute,\lim\limits_{n\to \infty} \int_{0}^{\infty} \exp(-x)(\cos(x))^n dx,"In the context of a measure theory and Lebesgue integration course I have to compute the limit $$\lim_{n\to \infty} \int_{1}^{\infty} \exp(-x)(\text{cos}(x))^n dx $$ To me this looks like the application of some convergence theorem, either the dominated or monotone one. I first checked some general properties relating to the convergence theorems: Pointwise convergence: Since $\vert \text{cos}(x) \vert \leq 1$ , it holds that $\vert (\text{cos}(x))^n \vert \leq 1 \forall n \in \mathbb{N}$ . Hence I know that for the pointwise limit holds that $$-\exp(-x) \leq \lim_{n \to \infty} \exp(-x) (\text{cos}(x))^n \leq \exp(-x)$$ But what can I infer about pointwise convergence from this? Since cosine oscillates between 1 and -1 there seems to be no pointwise limit. But there has to be in order to apply either one of the convergence theorems. So what do I miss here? Dominating function: Let's apply the dominated convergence theorem. For this we need a non-negative function $g \in L(\lambda)$ for which it holds that $$\vert f_n(x) \vert \leq g(x) \quad \forall x \in [1,\infty),\quad \forall n \in \mathbb{N}$$ . Obviously, a good candidate is $g(x) = \exp(-x)$ since it is non-negative and $$\int_{[1,\infty)} \vert\exp(-x)\vert d\lambda(x) < \infty$$ Now we can apply the dominated convergence theorem and write $$ \lim_{n \to \infty} \int_{0}^{\infty} \exp(-x) (\text{cos}(x))^n = \int_{0}^{\infty} \lim_{n \to \infty} \exp(-x) (\text{cos}(x))^n d\lambda(x)$$ Now all I need to do is evaluate the actual limit $$ \lim_{n \to \infty} \exp(-x) (\text{cos}(x))^n$$ but I don't see it since $(\text{cos(x)})^n$ jumps back and forth for $\lim_{n \to \infty}$ .","In the context of a measure theory and Lebesgue integration course I have to compute the limit To me this looks like the application of some convergence theorem, either the dominated or monotone one. I first checked some general properties relating to the convergence theorems: Pointwise convergence: Since , it holds that . Hence I know that for the pointwise limit holds that But what can I infer about pointwise convergence from this? Since cosine oscillates between 1 and -1 there seems to be no pointwise limit. But there has to be in order to apply either one of the convergence theorems. So what do I miss here? Dominating function: Let's apply the dominated convergence theorem. For this we need a non-negative function for which it holds that . Obviously, a good candidate is since it is non-negative and Now we can apply the dominated convergence theorem and write Now all I need to do is evaluate the actual limit but I don't see it since jumps back and forth for .","\lim_{n\to \infty} \int_{1}^{\infty} \exp(-x)(\text{cos}(x))^n dx  \vert \text{cos}(x) \vert \leq 1 \vert (\text{cos}(x))^n \vert \leq 1 \forall n \in \mathbb{N} -\exp(-x) \leq \lim_{n \to \infty} \exp(-x) (\text{cos}(x))^n \leq \exp(-x) g \in L(\lambda) \vert f_n(x) \vert \leq g(x) \quad \forall x \in [1,\infty),\quad \forall n \in \mathbb{N} g(x) = \exp(-x) \int_{[1,\infty)} \vert\exp(-x)\vert d\lambda(x) < \infty  \lim_{n \to \infty} \int_{0}^{\infty} \exp(-x) (\text{cos}(x))^n = \int_{0}^{\infty} \lim_{n \to \infty} \exp(-x) (\text{cos}(x))^n d\lambda(x)  \lim_{n \to \infty} \exp(-x) (\text{cos}(x))^n (\text{cos(x)})^n \lim_{n \to \infty}","['limits', 'lebesgue-integral']"
78,Computing: $\lim_{x\to\infty}\frac{\sqrt{1-\cos^2\frac{1}{x}}\left(3^\frac{1}{x}-5^\frac{-1}{x}\right)}{\log_2(1+x^{-2}+x^{-3})}$,Computing:,\lim_{x\to\infty}\frac{\sqrt{1-\cos^2\frac{1}{x}}\left(3^\frac{1}{x}-5^\frac{-1}{x}\right)}{\log_2(1+x^{-2}+x^{-3})},Find the following limit: $$\lim_{x\to\infty}\frac{\sqrt{1-\cos^2\frac{1}{x}}\left(3^\frac{1}{x}-5^\frac{-1}{x}\right)}{\log_2(1+x^{-2}+x^{-3})}$$ I'm not sure whether my solution is correct. $t:=\frac{1}{x}$ $$\lim_{x\to\infty}\frac{\sqrt{1-\cos^2\frac{1}{x}}\left(3^\frac{1}{x}-5^\frac{-1}{x}\right)}{\log_2(1+x^{-2}+x^{-3})}=\lim_{t\to 0}\frac{\sqrt{1-\cos^2 t}\left(3^t-5^{-t}\right)}{\log_2(1+t^2+t^3)}$$ $$=\lim_{t\to 0}\frac{\frac{\sqrt{1-\cos^2t}}{\sqrt t^2}\cdot t\cdot\left(\frac{3^t-1}{t}\cdot t+(-t)\frac{(-5)^{-t}+1}{-t}\right)}{\log_2(1+t^2+t^3)}$$ $$=\frac{1}{2}(\ln 3+\ln 5)\left[\lim_{t\to 0}\log_2(1+t^2+t^3)^\frac{1}{t^2}\right]^{-1}=\frac{1}{2}(\ln3+\ln 5)\left(e^{{\lim_{t\to 0}\frac{t^2+t^3}{t^2}}^{-1}}\right)^{-1}=\frac{\ln3+\ln5}{2e}$$,Find the following limit: I'm not sure whether my solution is correct.,\lim_{x\to\infty}\frac{\sqrt{1-\cos^2\frac{1}{x}}\left(3^\frac{1}{x}-5^\frac{-1}{x}\right)}{\log_2(1+x^{-2}+x^{-3})} t:=\frac{1}{x} \lim_{x\to\infty}\frac{\sqrt{1-\cos^2\frac{1}{x}}\left(3^\frac{1}{x}-5^\frac{-1}{x}\right)}{\log_2(1+x^{-2}+x^{-3})}=\lim_{t\to 0}\frac{\sqrt{1-\cos^2 t}\left(3^t-5^{-t}\right)}{\log_2(1+t^2+t^3)} =\lim_{t\to 0}\frac{\frac{\sqrt{1-\cos^2t}}{\sqrt t^2}\cdot t\cdot\left(\frac{3^t-1}{t}\cdot t+(-t)\frac{(-5)^{-t}+1}{-t}\right)}{\log_2(1+t^2+t^3)} =\frac{1}{2}(\ln 3+\ln 5)\left[\lim_{t\to 0}\log_2(1+t^2+t^3)^\frac{1}{t^2}\right]^{-1}=\frac{1}{2}(\ln3+\ln 5)\left(e^{{\lim_{t\to 0}\frac{t^2+t^3}{t^2}}^{-1}}\right)^{-1}=\frac{\ln3+\ln5}{2e},"['real-analysis', 'calculus', 'limits', 'limits-without-lhopital']"
79,Combinatorics and limit problem,Combinatorics and limit problem,,"For every natural number $n$ let us consider $a_n$ the greatest natural non-zero number such that: $$\binom {a_n}{n-1} \gt \binom {a_n-1}{n}.$$ Compute $$\lim_{n \to \infty} \frac {a_n}{n}.$$ I started by using the formula for the binomial coefficient, I obtained a second degree inequation in $a_n$, but I can't find the greatest $a_n$. That's where I got stuck. The equation I got is $a_n^2+a_n(1-3n)+n^2-n<0$.","For every natural number $n$ let us consider $a_n$ the greatest natural non-zero number such that: $$\binom {a_n}{n-1} \gt \binom {a_n-1}{n}.$$ Compute $$\lim_{n \to \infty} \frac {a_n}{n}.$$ I started by using the formula for the binomial coefficient, I obtained a second degree inequation in $a_n$, but I can't find the greatest $a_n$. That's where I got stuck. The equation I got is $a_n^2+a_n(1-3n)+n^2-n<0$.",,"['sequences-and-series', 'limits', 'binomial-coefficients']"
80,"How to show that $\lim_{(x,y)\to(0,0)}\frac{xy^2}{x^2+y^3}$ does not exist?",How to show that  does not exist?,"\lim_{(x,y)\to(0,0)}\frac{xy^2}{x^2+y^3}","I have a question. I have to check whether the limit exist. But I want to check it with a) two paths or b) polar coordinates, but it doesn't work. It is about  $\lim_{(x,y)\to(0,0)}\frac{xy^2}{x^2+y^3}$. Can anyone help with to show that the limit does not exist? Thank you","I have a question. I have to check whether the limit exist. But I want to check it with a) two paths or b) polar coordinates, but it doesn't work. It is about  $\lim_{(x,y)\to(0,0)}\frac{xy^2}{x^2+y^3}$. Can anyone help with to show that the limit does not exist? Thank you",,"['calculus', 'limits', 'multivariable-calculus']"
81,Evaluating $\lim_{ x \to+ \infty }\left[\frac{[x]}{x}\right]$ and $\lim_{ x \to- \infty }\left[\frac{[x]}{x}\right]$,Evaluating  and,\lim_{ x \to+ \infty }\left[\frac{[x]}{x}\right] \lim_{ x \to- \infty }\left[\frac{[x]}{x}\right],Find the limit : $$\lim_{ x \to+ \infty }\left[\frac{[x]}{x}\right]=? \\\lim_{ x \to- \infty }\left[\frac{[x]}{x}\right]=?$$ $[x]:$ floor function I tried: $[u]∼u :\text{where}  x→∞$ $$\lim_{ x \to+ \infty }\left[\frac{[x]}{x}\right]=\lim_{ x \to+ \infty }\frac{[x]}{x}=1!!!$$,Find the limit : floor function I tried:,"\lim_{ x \to+ \infty }\left[\frac{[x]}{x}\right]=?
\\\lim_{ x \to- \infty }\left[\frac{[x]}{x}\right]=? [x]: [u]∼u :\text{where}  x→∞ \lim_{ x \to+ \infty }\left[\frac{[x]}{x}\right]=\lim_{ x \to+ \infty }\frac{[x]}{x}=1!!!","['limits', 'fractional-part']"
82,"Prove $\lim_{x\to2} \sqrt{4x-x^2} = 2$ by definition with $\epsilon, \delta$",Prove  by definition with,"\lim_{x\to2} \sqrt{4x-x^2} = 2 \epsilon, \delta","I need to prove using $\epsilon, \delta$: $$\lim_{x\to2} \sqrt{4x-x^2} = 2$$ Meaning I need to prove that for every $\epsilon > 0$, there is a $\delta >0$ such as that $0<|x-2|<δ→|f(x)-2|<ϵ$ I tried going like this: $|\sqrt{4x-x^2} -2|<|\sqrt{4x}-2|<|2\sqrt x-2|=|2(\sqrt x-1)|=2|\sqrt x-1|<2|\sqrt x-1|$... But I don't know how to choose my $\delta$ from here.","I need to prove using $\epsilon, \delta$: $$\lim_{x\to2} \sqrt{4x-x^2} = 2$$ Meaning I need to prove that for every $\epsilon > 0$, there is a $\delta >0$ such as that $0<|x-2|<δ→|f(x)-2|<ϵ$ I tried going like this: $|\sqrt{4x-x^2} -2|<|\sqrt{4x}-2|<|2\sqrt x-2|=|2(\sqrt x-1)|=2|\sqrt x-1|<2|\sqrt x-1|$... But I don't know how to choose my $\delta$ from here.",,['limits']
83,sequence multiplication limit,sequence multiplication limit,,Need help with the following question :,Need help with the following question :,,['sequences-and-series']
84,Is it enough to show that $\lim_{x\rightarrow 0}\cos(1/x)$ doesn't exist to show that $\lim_{x \rightarrow0}(2x\sin(1/x)-\cos(1/x))$ doesn't exist?,Is it enough to show that  doesn't exist to show that  doesn't exist?,\lim_{x\rightarrow 0}\cos(1/x) \lim_{x \rightarrow0}(2x\sin(1/x)-\cos(1/x)),"My initial thought would have been yes, but my professor's solution to proving that $\lim_{x \rightarrow0}(2x\sin(1/x)-\cos(1/x))$ doesn't exist has me thinking otherwise. Professor's solution: Assume for a contradiction that there exists  $\lim_{x \rightarrow0}(2x\sin(1/x)-\cos(1/x))=l$ for some $l\in\mathbb{R}.$ Notice that $\cos(1/x)=(\cos(1/x)-2x\sin(1/x))+2x\sin(1/x).$ Then by our assumption, the fact that $\lim_{x\rightarrow0}2x\sin(1/x)=0$ (proof by Sandwich Theorem omitted) and using the Algebra of Limits we have that $$\lim_{x\rightarrow0}(\cos(1/x)-2x\sin(1/x))+2x\sin(1/x) \\=\lim_{x\rightarrow 0}(\cos(1/x)-2x\sin(1/x))+\lim_{x\rightarrow0}2x\sin(1/x)$$ But observe that $\lim_{x\rightarrow 0}\cos(1/x)$ does not exist (proof using sequences omitted). Hence we have a contradiction. Ok so I have to admit that I don't even see where the contradiction is nor why we had to go through such a long process, just to end up with needing to show that the limit doesn't exist because $\lim_{x\rightarrow0}\cos(1/x)$ doesn't exist. Couldn't we just have done that from the start?","My initial thought would have been yes, but my professor's solution to proving that $\lim_{x \rightarrow0}(2x\sin(1/x)-\cos(1/x))$ doesn't exist has me thinking otherwise. Professor's solution: Assume for a contradiction that there exists  $\lim_{x \rightarrow0}(2x\sin(1/x)-\cos(1/x))=l$ for some $l\in\mathbb{R}.$ Notice that $\cos(1/x)=(\cos(1/x)-2x\sin(1/x))+2x\sin(1/x).$ Then by our assumption, the fact that $\lim_{x\rightarrow0}2x\sin(1/x)=0$ (proof by Sandwich Theorem omitted) and using the Algebra of Limits we have that $$\lim_{x\rightarrow0}(\cos(1/x)-2x\sin(1/x))+2x\sin(1/x) \\=\lim_{x\rightarrow 0}(\cos(1/x)-2x\sin(1/x))+\lim_{x\rightarrow0}2x\sin(1/x)$$ But observe that $\lim_{x\rightarrow 0}\cos(1/x)$ does not exist (proof using sequences omitted). Hence we have a contradiction. Ok so I have to admit that I don't even see where the contradiction is nor why we had to go through such a long process, just to end up with needing to show that the limit doesn't exist because $\lim_{x\rightarrow0}\cos(1/x)$ doesn't exist. Couldn't we just have done that from the start?",,"['calculus', 'limits']"
85,"How to prove that $\lim\limits_{n\to\infty} \int_0^1 \cos^n(x)\, dx = 0$",How to prove that,"\lim\limits_{n\to\infty} \int_0^1 \cos^n(x)\, dx = 0","I've got this tasks to prove that: $\lim\limits_{n\to\infty} \int_0^1 \cos^n(x) \,dx = 0$ I tried to think about a partition ${0,t,1}$, and say that if $t$ is small enough, I can get: $\lim\limits_{n\to\infty} \int_0^t \cos(x)\, dx = 0$ But then I'm stuck with the rest section $[t,1]$ which approaches $1$. Any clue?","I've got this tasks to prove that: $\lim\limits_{n\to\infty} \int_0^1 \cos^n(x) \,dx = 0$ I tried to think about a partition ${0,t,1}$, and say that if $t$ is small enough, I can get: $\lim\limits_{n\to\infty} \int_0^t \cos(x)\, dx = 0$ But then I'm stuck with the rest section $[t,1]$ which approaches $1$. Any clue?",,"['calculus', 'limits', 'definite-integrals', 'trigonometric-integrals']"
86,Show that in $\lim_{x\to 0}\frac{x^3\sin(1/x)}{\sin^2 x}$ can't be applied L'Hopital's Rule,Show that in  can't be applied L'Hopital's Rule,\lim_{x\to 0}\frac{x^3\sin(1/x)}{\sin^2 x},"I was trying to demonstrate this by showing that one or some of the conditions for the application of L'Hopital's Rule are not met, but it seems to me that the conditions work just fine. According to my textbook. One of the condition of applicability of L'Hopital's Rule is that: If: $1)$ $f$ anf $g$ are differentiable on $(a,b)$; $b-a <\infty$; $2)$ $\lim_{x\to a+0}f(x) = \lim_{x\to a+0}g(x) = 0$; $3)$ $g' \neq 0$ on $(a,b)$; $4)$ $\exists \lim_{x\to a+0}\frac{f'(x)}{g'(x)} \in \mathbb{\overline{R}}$ Then $$\lim_{x\to a+0}\frac{f(x)}{g(x)} = \lim_{x\to a+0}\frac{f'(x)}{g'(x)}$$ My thoughts: Here $a = 0$, so $(0,b)$ is our open interval. $1)$ Both the numerator and the denominator are differentiable on this open interval. $2)$ as $x \to +0 f(x) = g(x)$ (considering the boundeness of $\sin (\frac{1}{x}$) $3)$ we can choose a $b$ small enough so $g'(x) = \sin (2x) \neq 0$ on this interval (any $b<\pi$ works). $4)$ on $(0, b)$, $\lim_{x\to a+0}\frac{f'(x)}{g'(x)}$ is indeterminate but I can apply again L'Hopital's Rule. What am I missing?","I was trying to demonstrate this by showing that one or some of the conditions for the application of L'Hopital's Rule are not met, but it seems to me that the conditions work just fine. According to my textbook. One of the condition of applicability of L'Hopital's Rule is that: If: $1)$ $f$ anf $g$ are differentiable on $(a,b)$; $b-a <\infty$; $2)$ $\lim_{x\to a+0}f(x) = \lim_{x\to a+0}g(x) = 0$; $3)$ $g' \neq 0$ on $(a,b)$; $4)$ $\exists \lim_{x\to a+0}\frac{f'(x)}{g'(x)} \in \mathbb{\overline{R}}$ Then $$\lim_{x\to a+0}\frac{f(x)}{g(x)} = \lim_{x\to a+0}\frac{f'(x)}{g'(x)}$$ My thoughts: Here $a = 0$, so $(0,b)$ is our open interval. $1)$ Both the numerator and the denominator are differentiable on this open interval. $2)$ as $x \to +0 f(x) = g(x)$ (considering the boundeness of $\sin (\frac{1}{x}$) $3)$ we can choose a $b$ small enough so $g'(x) = \sin (2x) \neq 0$ on this interval (any $b<\pi$ works). $4)$ on $(0, b)$, $\lim_{x\to a+0}\frac{f'(x)}{g'(x)}$ is indeterminate but I can apply again L'Hopital's Rule. What am I missing?",,"['calculus', 'real-analysis', 'limits', 'limits-without-lhopital']"
87,Calculating limit of $\lim_{x\to\infty}\dfrac{\sqrt{x+1}-2\sqrt{x+2}+\sqrt{x}}{\sqrt{x+2}-2\sqrt{x}+\sqrt{x-4}}$,Calculating limit of,\lim_{x\to\infty}\dfrac{\sqrt{x+1}-2\sqrt{x+2}+\sqrt{x}}{\sqrt{x+2}-2\sqrt{x}+\sqrt{x-4}},"As the title says we want to calculate: $$\lim_{x\to\infty}\dfrac{\sqrt{x+1}-2\sqrt{x+2}+\sqrt{x}}{\sqrt{x+2}-2\sqrt{x}+\sqrt{x-4}}$$ By multiplying nominator and denominator in their conjugates $=\lim_{x\to\infty}\dfrac{(\sqrt{x+2}+2\sqrt{x}+\sqrt{x-4})(x+1+x+2\sqrt{x(x+1)}-4(x+2))}{(\sqrt{x+1}+2\sqrt{x+2}+\sqrt{x})(x+2+x-4+2\sqrt{(x+2)(x-4)})-4x)}$ $=\lim_{x\to\infty}\dfrac{(\sqrt{x+2}+2\sqrt{x}+\sqrt{x-4})(-2x-7+2\sqrt{x^2+x})}{(\sqrt{x+1}+2\sqrt{x+2}+\sqrt{x})(-2x-2+2\sqrt{x^2-2x-8})}$ I think now we can take $$2x\approx2\sqrt{x^2+x}\approx2\sqrt{x^2-2x-8}\\[2ex] \sqrt{x}\approx\sqrt{x+1}\approx\sqrt{x+2}\approx\sqrt{x-4}$$ as $x$ goes to infinity. Hence the limit of above fraction would be $\dfrac{7}{2}$, but wolframalpha gives me $\dfrac{3}{2}$ as the limit of the above fraction. What am I doing wrong?","As the title says we want to calculate: $$\lim_{x\to\infty}\dfrac{\sqrt{x+1}-2\sqrt{x+2}+\sqrt{x}}{\sqrt{x+2}-2\sqrt{x}+\sqrt{x-4}}$$ By multiplying nominator and denominator in their conjugates $=\lim_{x\to\infty}\dfrac{(\sqrt{x+2}+2\sqrt{x}+\sqrt{x-4})(x+1+x+2\sqrt{x(x+1)}-4(x+2))}{(\sqrt{x+1}+2\sqrt{x+2}+\sqrt{x})(x+2+x-4+2\sqrt{(x+2)(x-4)})-4x)}$ $=\lim_{x\to\infty}\dfrac{(\sqrt{x+2}+2\sqrt{x}+\sqrt{x-4})(-2x-7+2\sqrt{x^2+x})}{(\sqrt{x+1}+2\sqrt{x+2}+\sqrt{x})(-2x-2+2\sqrt{x^2-2x-8})}$ I think now we can take $$2x\approx2\sqrt{x^2+x}\approx2\sqrt{x^2-2x-8}\\[2ex] \sqrt{x}\approx\sqrt{x+1}\approx\sqrt{x+2}\approx\sqrt{x-4}$$ as $x$ goes to infinity. Hence the limit of above fraction would be $\dfrac{7}{2}$, but wolframalpha gives me $\dfrac{3}{2}$ as the limit of the above fraction. What am I doing wrong?",,"['limits', 'proof-verification']"
88,How can we solve $\lim\limits_{n \to \infty} \left (1 + \frac{1}{n} \right)^n$? [duplicate],How can we solve ? [duplicate],\lim\limits_{n \to \infty} \left (1 + \frac{1}{n} \right)^n,This question already has answers here : How to prove $\lim\limits_{n \to \infty} (1+\frac1n)^n = e$? (4 answers) Closed 6 years ago . Taken from Wikipedia : The number $e$ is the limit $$e = \lim_{n \to \infty} \left (1 + \frac{1}{n} \right)^n$$ Graph of $f(x) = \left (1 + \dfrac{1}{x} \right)^x$ taken from here. Its evident from the graph that the limit actually approaches $e$ as $x$ approaches $\infty$. So I tried approaching the value algebraically. My attempt: $$\lim_{n \to \infty} \left (1 + \frac{1}{n} \right)^n$$ $$= \lim_{n \to \infty} \left(\frac{n + 1}{n}\right)^n$$ $$= \left(\lim_{n \to \infty} \left(\frac{n + 1}{n}\right) \right)^n$$ $$= 1^\infty$$ which is an indeterminate form. I cannot think of any other algebraic manipulation. My question is that how can I solve this limit algebraically?,This question already has answers here : How to prove $\lim\limits_{n \to \infty} (1+\frac1n)^n = e$? (4 answers) Closed 6 years ago . Taken from Wikipedia : The number $e$ is the limit $$e = \lim_{n \to \infty} \left (1 + \frac{1}{n} \right)^n$$ Graph of $f(x) = \left (1 + \dfrac{1}{x} \right)^x$ taken from here. Its evident from the graph that the limit actually approaches $e$ as $x$ approaches $\infty$. So I tried approaching the value algebraically. My attempt: $$\lim_{n \to \infty} \left (1 + \frac{1}{n} \right)^n$$ $$= \lim_{n \to \infty} \left(\frac{n + 1}{n}\right)^n$$ $$= \left(\lim_{n \to \infty} \left(\frac{n + 1}{n}\right) \right)^n$$ $$= 1^\infty$$ which is an indeterminate form. I cannot think of any other algebraic manipulation. My question is that how can I solve this limit algebraically?,,"['limits', 'exponential-function', 'indeterminate-forms']"
89,Difficulty understanding manipulation of $\epsilon$ and $N$ in the definition of a limit in proofs,Difficulty understanding manipulation of  and  in the definition of a limit in proofs,\epsilon N,"The definition I am working with is from Foundations of Mathematical Analysis by Johnsonbaugh and Pfaffenberger. Def : Let $\{  a_n \}_{n=1}^\infty$ be a sequence of real numbers. We say that $\{  a_n \}_{n=1}^\infty$ has limit $L\in \Bbb R$ if for every $\epsilon > 0$, there exists a positive integer $N$, such that if $n \geq N$, then $$|a_n - L|<\epsilon$$ In the proof of the theorem stating that the sum of the the two convergent sequences is convergent to the sum of their limits a simple strategy is to take the epsilon for each sequence individually and use the definition and divide the epsilon by two so that using triangle inequality you can say: $$|(a_n + b_n)-(L+M)| \leq |a_n - L| + |b_n - M| < \epsilon/2 + \epsilon/2 = \epsilon$$ I don't understand why this is okay to do, I kind of understand the manipulation of $N$ so that $n$  holds the same inequality of the definition but something seems logically flawed in the assertion above and it is used often for a variety of limit proofs, particularly in proving basic algebra of limit proofs.","The definition I am working with is from Foundations of Mathematical Analysis by Johnsonbaugh and Pfaffenberger. Def : Let $\{  a_n \}_{n=1}^\infty$ be a sequence of real numbers. We say that $\{  a_n \}_{n=1}^\infty$ has limit $L\in \Bbb R$ if for every $\epsilon > 0$, there exists a positive integer $N$, such that if $n \geq N$, then $$|a_n - L|<\epsilon$$ In the proof of the theorem stating that the sum of the the two convergent sequences is convergent to the sum of their limits a simple strategy is to take the epsilon for each sequence individually and use the definition and divide the epsilon by two so that using triangle inequality you can say: $$|(a_n + b_n)-(L+M)| \leq |a_n - L| + |b_n - M| < \epsilon/2 + \epsilon/2 = \epsilon$$ I don't understand why this is okay to do, I kind of understand the manipulation of $N$ so that $n$  holds the same inequality of the definition but something seems logically flawed in the assertion above and it is used often for a variety of limit proofs, particularly in proving basic algebra of limit proofs.",,"['real-analysis', 'limits', 'proof-writing']"
90,A limit involving the Regularized Incomplete Beta Function,A limit involving the Regularized Incomplete Beta Function,,"I'm trying to evaluate $$\lim_{n\to\infty} nx^{n-1}I\left(1-\frac{x^2}{4}; \frac{n+1}{2}, \frac{1}{2}\right)$$ where $I(x; a, b)$ is the Regularized Incomplete Beta Function and $-2\leq x \leq 2$, and I'm stuck on where to begin. I've tried expanding the beta function as the quotient of two integrals and then differentiating using L'Hopital's rule, and I've tried using a series representation for the Beta function, but nothing seems to work. I've tried plotting this function of $x$ with $n=199999$ using WolframAlpha , and because of the result I strongly suspect that it tends to $0$ everywhere but $x=\sqrt{2}$ where it tends to $\infty$, but I can't seem to prove this. How would I start?","I'm trying to evaluate $$\lim_{n\to\infty} nx^{n-1}I\left(1-\frac{x^2}{4}; \frac{n+1}{2}, \frac{1}{2}\right)$$ where $I(x; a, b)$ is the Regularized Incomplete Beta Function and $-2\leq x \leq 2$, and I'm stuck on where to begin. I've tried expanding the beta function as the quotient of two integrals and then differentiating using L'Hopital's rule, and I've tried using a series representation for the Beta function, but nothing seems to work. I've tried plotting this function of $x$ with $n=199999$ using WolframAlpha , and because of the result I strongly suspect that it tends to $0$ everywhere but $x=\sqrt{2}$ where it tends to $\infty$, but I can't seem to prove this. How would I start?",,"['limits', 'special-functions', 'beta-function']"
91,Contradictions between the Alternating Series Test & Divergence Test?,Contradictions between the Alternating Series Test & Divergence Test?,,"By the Alternating Test : The series $\sum_{n=1}^{\infty}(-1)^{n+1}\cdot b_n$ converges if all three of the following conditions are satisfied: The $b_n$'s are all positive. The positive $b_n$'s are (eventually) decreasing: $b_n\ge b_{n+1}$ $\forall n\ge N$. $b_n \to 0$ But in the Divergence Test : Given $\sum_{n=1}^{\infty} a_n$, iff $\lim_{n \to \infty} a_n \neq 0 \implies$  $\sum_{n=1}^{\infty} a_n$ diverges Now given an arbitrary alternating series: $$S = \sum_{n=1}^{\infty}(-1)^{n+1}\cdot b_n$$ If we take the limit of $a_n$ in the series above $$\lim_{n \to \infty}(-1)^{n+1}\cdot b_n = \underbrace{\left(\lim_{n \to \infty }(-1)^{n+1}\right)}_\text{This limit doesn't exist}\left(\lim_{n \to \infty} b_n\right)$$ Therefore by the Divergence test, $S$ should be a divergent series, regardless of the conditions needed for the alternating test. But by the Alternating Series Test, $S$ is a convergent series provided the three conditions stipulated initially are met. So how is this seeming contradiction resolved, by the alternating series test?","By the Alternating Test : The series $\sum_{n=1}^{\infty}(-1)^{n+1}\cdot b_n$ converges if all three of the following conditions are satisfied: The $b_n$'s are all positive. The positive $b_n$'s are (eventually) decreasing: $b_n\ge b_{n+1}$ $\forall n\ge N$. $b_n \to 0$ But in the Divergence Test : Given $\sum_{n=1}^{\infty} a_n$, iff $\lim_{n \to \infty} a_n \neq 0 \implies$  $\sum_{n=1}^{\infty} a_n$ diverges Now given an arbitrary alternating series: $$S = \sum_{n=1}^{\infty}(-1)^{n+1}\cdot b_n$$ If we take the limit of $a_n$ in the series above $$\lim_{n \to \infty}(-1)^{n+1}\cdot b_n = \underbrace{\left(\lim_{n \to \infty }(-1)^{n+1}\right)}_\text{This limit doesn't exist}\left(\lim_{n \to \infty} b_n\right)$$ Therefore by the Divergence test, $S$ should be a divergent series, regardless of the conditions needed for the alternating test. But by the Alternating Series Test, $S$ is a convergent series provided the three conditions stipulated initially are met. So how is this seeming contradiction resolved, by the alternating series test?",,"['calculus', 'real-analysis', 'sequences-and-series', 'limits', 'convergence-divergence']"
92,Values of b for which this series will converge,Values of b for which this series will converge,,"I'm trying to solve the following problem: Find the positive values of $b$ for which the series $\sum_{n=1}^\infty b^{\ln(n)} $ converges. I started by doing the integral test with the function $b^{\ln(x)}$. To integrate this: $\int b^{\ln(x)} \, dx  $ I used integration by parts: I picked $u=b^{\ln(x)}$ $du= b^{\ln(x)} \ln(b)(1/x)dx$ $v=\int dx = x$ Doing the substitution I get: $$\int b^{\ln(x)} \, d x= b^{\ln(x)}x-\int x b^{\ln(x)}\ln(b)(1/x)\,dx$$ $$\int b^{\ln(x)} dx= b^{\ln(x)}x-\int b^{\ln(x)}\ln(b)(x/x)dx$$ $$\int b^{\ln(x)} dx= b^{\ln(x)}x-\int b^{\ln(x)}\ln(b)dx$$ $$\int b^{ln(x)} dx= b^{ln(x)}x-ln(b)\int b^{ln(x)}dx$$ $$\int b^{\ln(x)} dx + \ln(b)\int b^{\ln(x)}dx= b^{\ln(x)}x$$ $$[1 + \ln(b)]\int b^{\ln(x)} dx = b^{\ln(x)}x$$ $$\bbox[5px,border:2px solid red]{\int b^{\ln(x)} dx = \frac{b^{\ln(x)}x}{[1 + \ln(b)]}}\qquad$$ Then I solve evaluate the improper integral between $1$ and $\infty$ $$\int_1^\infty b^{\ln(x)} dx =\lim_{t\rightarrow \infty} \int_1^t b^{\ln(x)} dx =\lim_{t\rightarrow \infty} \left| \begin{array}{c} \frac{ b^{\ln(x)}x }{[1 + \ln(b)]} \end{array}  \right|_1^t$$ $$\lim_{t\rightarrow \infty} \left| \begin{array}{c} \frac{b^{ln(x)}x}{[1 + \ln(b)]} \end{array}  \right|_1^t =\lim_{t\rightarrow \infty} \frac{b^{\ln(t)}t}{[1 + \ln(b)]} -\frac{b^{\ln(1)}1}{[1 + \ln(b)]}$$ $$=\lim\limits_{t\rightarrow \infty} \frac{b^{\ln(t)}t}{[1 + \ln(b)]} -\frac{b^{0}1}{[1 + \ln(b)]}$$ $$=\lim\limits_{t\rightarrow \infty} \frac{b^{\ln(t)}t}{[1 + \ln(b)]} -\frac{1(1)}{[1 + \ln(b)]}$$ $$=\lim\limits_{t\rightarrow \infty} \frac{b^{\ln(t)}t}{[1 + \ln(b)]} -\frac{1}{[1 + \ln(b)]}$$ $$=[\frac{1}{[1 + \ln(b)]}]\lim\limits_{t\rightarrow \infty} b^{\ln(t)}t - 1$$ Here is where I get a little confused: I assume that $0<b < 1$ because that way I have an indeterminate form $0\cdots\infty$ which allows me to apply l'Hôpital's  rule as following: $$=\left[\frac{1}{[1 + \ln(b)]}\right] \lim_{t\rightarrow \infty} \frac {b^{\ln(t)}}{t^{-1}} - 1$$ I differentiate numerator and denominator $$=\left[\frac{1}{[1 + \ln(b)]}\right] \lim_{t\rightarrow \infty} \frac {b^{\ln(t)}\ln(b)t^{-1}}{-t^{-2}} - 1$$ And I end up getting the same indeterminate form again ($0\cdot\infty$) $$=\left[\frac{1}{[1 + \ln(b)]} \right] \lim_{t\rightarrow \infty} -b^{\ln(t)} \ln(b)t - 1$$ What should I do? The book gives $b<(1/e)$ as an answer, which makes sense because if $b=1/e$ then $\ln(b) = -1$ and the denominator of $[\frac{1}{[1 + \ln(b)]}]$ would be zero. But I'm stuck here. How do I solve evaluate the limit? I'm assuming that my mistake is not in the indefinite integral because Mathematica and other CAS give the same answer. So the problem is likely to be in the improper integral and the limit.","I'm trying to solve the following problem: Find the positive values of $b$ for which the series $\sum_{n=1}^\infty b^{\ln(n)} $ converges. I started by doing the integral test with the function $b^{\ln(x)}$. To integrate this: $\int b^{\ln(x)} \, dx  $ I used integration by parts: I picked $u=b^{\ln(x)}$ $du= b^{\ln(x)} \ln(b)(1/x)dx$ $v=\int dx = x$ Doing the substitution I get: $$\int b^{\ln(x)} \, d x= b^{\ln(x)}x-\int x b^{\ln(x)}\ln(b)(1/x)\,dx$$ $$\int b^{\ln(x)} dx= b^{\ln(x)}x-\int b^{\ln(x)}\ln(b)(x/x)dx$$ $$\int b^{\ln(x)} dx= b^{\ln(x)}x-\int b^{\ln(x)}\ln(b)dx$$ $$\int b^{ln(x)} dx= b^{ln(x)}x-ln(b)\int b^{ln(x)}dx$$ $$\int b^{\ln(x)} dx + \ln(b)\int b^{\ln(x)}dx= b^{\ln(x)}x$$ $$[1 + \ln(b)]\int b^{\ln(x)} dx = b^{\ln(x)}x$$ $$\bbox[5px,border:2px solid red]{\int b^{\ln(x)} dx = \frac{b^{\ln(x)}x}{[1 + \ln(b)]}}\qquad$$ Then I solve evaluate the improper integral between $1$ and $\infty$ $$\int_1^\infty b^{\ln(x)} dx =\lim_{t\rightarrow \infty} \int_1^t b^{\ln(x)} dx =\lim_{t\rightarrow \infty} \left| \begin{array}{c} \frac{ b^{\ln(x)}x }{[1 + \ln(b)]} \end{array}  \right|_1^t$$ $$\lim_{t\rightarrow \infty} \left| \begin{array}{c} \frac{b^{ln(x)}x}{[1 + \ln(b)]} \end{array}  \right|_1^t =\lim_{t\rightarrow \infty} \frac{b^{\ln(t)}t}{[1 + \ln(b)]} -\frac{b^{\ln(1)}1}{[1 + \ln(b)]}$$ $$=\lim\limits_{t\rightarrow \infty} \frac{b^{\ln(t)}t}{[1 + \ln(b)]} -\frac{b^{0}1}{[1 + \ln(b)]}$$ $$=\lim\limits_{t\rightarrow \infty} \frac{b^{\ln(t)}t}{[1 + \ln(b)]} -\frac{1(1)}{[1 + \ln(b)]}$$ $$=\lim\limits_{t\rightarrow \infty} \frac{b^{\ln(t)}t}{[1 + \ln(b)]} -\frac{1}{[1 + \ln(b)]}$$ $$=[\frac{1}{[1 + \ln(b)]}]\lim\limits_{t\rightarrow \infty} b^{\ln(t)}t - 1$$ Here is where I get a little confused: I assume that $0<b < 1$ because that way I have an indeterminate form $0\cdots\infty$ which allows me to apply l'Hôpital's  rule as following: $$=\left[\frac{1}{[1 + \ln(b)]}\right] \lim_{t\rightarrow \infty} \frac {b^{\ln(t)}}{t^{-1}} - 1$$ I differentiate numerator and denominator $$=\left[\frac{1}{[1 + \ln(b)]}\right] \lim_{t\rightarrow \infty} \frac {b^{\ln(t)}\ln(b)t^{-1}}{-t^{-2}} - 1$$ And I end up getting the same indeterminate form again ($0\cdot\infty$) $$=\left[\frac{1}{[1 + \ln(b)]} \right] \lim_{t\rightarrow \infty} -b^{\ln(t)} \ln(b)t - 1$$ What should I do? The book gives $b<(1/e)$ as an answer, which makes sense because if $b=1/e$ then $\ln(b) = -1$ and the denominator of $[\frac{1}{[1 + \ln(b)]}]$ would be zero. But I'm stuck here. How do I solve evaluate the limit? I'm assuming that my mistake is not in the indefinite integral because Mathematica and other CAS give the same answer. So the problem is likely to be in the improper integral and the limit.",,"['calculus', 'sequences-and-series', 'limits', 'convergence-divergence', 'improper-integrals']"
93,To prove limit of function,To prove limit of function,,To prove $\lim_{x\to 0} x\sin(\frac{1}{x})=0$ I tried sandwich theorem but I have no clear idea to prove the problem,To prove $\lim_{x\to 0} x\sin(\frac{1}{x})=0$ I tried sandwich theorem but I have no clear idea to prove the problem,,['limits']
94,Radius and Interval of Convergence for $\sum_{n=1}^{\infty}\frac{5^n}{n^2}x^n$,Radius and Interval of Convergence for,\sum_{n=1}^{\infty}\frac{5^n}{n^2}x^n,"$$\sum_{n=1}^\infty \frac{5^n}{n^2}x^n$$ After doing the ratio test I end up with: $$5|x| < 1$$ I'm confused, though, as to what is considered my interval of convergence and what is my radius.  I recognize that $1$ is my limit, so does this mean my radius of convergence is $\frac{1}{5}$ and my interval of convergence is: $$\left(-\frac{1}{5},\frac{1}{5}\right)$$","$$\sum_{n=1}^\infty \frac{5^n}{n^2}x^n$$ After doing the ratio test I end up with: $$5|x| < 1$$ I'm confused, though, as to what is considered my interval of convergence and what is my radius.  I recognize that $1$ is my limit, so does this mean my radius of convergence is $\frac{1}{5}$ and my interval of convergence is: $$\left(-\frac{1}{5},\frac{1}{5}\right)$$",,"['real-analysis', 'sequences-and-series', 'limits']"
95,Evaluate $\lim_{x \to 0^-} \left( \frac{1}{x} - \frac{1}{|x|} \right)$ (possible textbook mistake - James Stewart 7th),Evaluate  (possible textbook mistake - James Stewart 7th),\lim_{x \to 0^-} \left( \frac{1}{x} - \frac{1}{|x|} \right),"I was working on a few problems from James Stewart's Calculus book (seventh edition) and I found the following: Find $$\lim_{x \to 0^-} \left( \frac{1}{x} - \frac{1}{|x|} \right)$$ Since there's a $|x|$ on the limit and knowing that $|x| = -x$ for any value less than zero, we have $$\lim_{x \to 0^-} \left( \frac{1}{x} - \frac{1}{|x|} \right) = \lim_{x \to 0^-} \frac{2}{x}$$ So far so good. Continuing, $$\lim_{x \to 0^-} \left( \frac{1}{x} - \frac{1}{|x|} \right) = \lim_{x \to 0^-} \frac{2}{x} = - \infty$$ since the denominator becomes smaller and smaller. When checking the textbook's answer I've found the following: Am I missing something or should the limit really be $- \infty$ ?","I was working on a few problems from James Stewart's Calculus book (seventh edition) and I found the following: Find $$\lim_{x \to 0^-} \left( \frac{1}{x} - \frac{1}{|x|} \right)$$ Since there's a $|x|$ on the limit and knowing that $|x| = -x$ for any value less than zero, we have $$\lim_{x \to 0^-} \left( \frac{1}{x} - \frac{1}{|x|} \right) = \lim_{x \to 0^-} \frac{2}{x}$$ So far so good. Continuing, $$\lim_{x \to 0^-} \left( \frac{1}{x} - \frac{1}{|x|} \right) = \lim_{x \to 0^-} \frac{2}{x} = - \infty$$ since the denominator becomes smaller and smaller. When checking the textbook's answer I've found the following: Am I missing something or should the limit really be $- \infty$ ?",,"['calculus', 'limits', 'absolute-value']"
96,Dilemma about value of limit,Dilemma about value of limit,,$$ \lim_{x\to 0^{+}}\left[\left(1+\frac{1}{x}\right)^x+\left(\frac{1}{x}\right)^x+\left(\tan(x)\right)^{\frac{1}{x}}\right]$$ Attempt: I used $\tan(x)\approx x$ also $(1+n)^{1/n}=e$ so I let $x=0+h$ and lim changes to lim h tending to $0$. But that gives me $e+\infty+h^{1/h}$. While the answer is an integer between $0-9$ . Where is my mistake?.Thanks,$$ \lim_{x\to 0^{+}}\left[\left(1+\frac{1}{x}\right)^x+\left(\frac{1}{x}\right)^x+\left(\tan(x)\right)^{\frac{1}{x}}\right]$$ Attempt: I used $\tan(x)\approx x$ also $(1+n)^{1/n}=e$ so I let $x=0+h$ and lim changes to lim h tending to $0$. But that gives me $e+\infty+h^{1/h}$. While the answer is an integer between $0-9$ . Where is my mistake?.Thanks,,"['limits', 'limits-without-lhopital']"
97,if $f_n(x)$ converges uniformly to a function $f(x)$ does $f_n'(x)$ converge uniformly to $f'(x)$?,if  converges uniformly to a function  does  converge uniformly to ?,f_n(x) f(x) f_n'(x) f'(x),"Let [a,b] denote a finite interval and consider a sequence $\{f_n(x)\}_{n=0}^\infty$ in $C^1([a,b])$. if $f_n(x)$ converges uniformly to a function $f(x)$ on $[a,b]$, does $\{f_n'(x)\}$ converge uniformly to $f'(x)$? My intuition for this problem is that the converse is true, but I'm not sure how to justify this. Any help is appreciated!","Let [a,b] denote a finite interval and consider a sequence $\{f_n(x)\}_{n=0}^\infty$ in $C^1([a,b])$. if $f_n(x)$ converges uniformly to a function $f(x)$ on $[a,b]$, does $\{f_n'(x)\}$ converge uniformly to $f'(x)$? My intuition for this problem is that the converse is true, but I'm not sure how to justify this. Any help is appreciated!",,"['real-analysis', 'sequences-and-series', 'limits', 'convergence-divergence', 'uniform-convergence']"
98,Showing that $\log(n)^{\log(\log(n))} \in \mathcal{O}(n)$,Showing that,\log(n)^{\log(\log(n))} \in \mathcal{O}(n),"I want to show that $$\log(n)^{\log(\log(n))} \in \mathcal{O}(n)$$ where $n \in \mathbb{N}_{≥2}$, and $\mathcal{O}$ is the big-O-notation. It seems like a relatively simply statement, but so far, I've had no luck. I first started with the limit $\lim_{n \to \infty} \frac{\log(n)^{\log(\log(n))}}{n}$. In order to show the statement, it would be sufficient to show that this limit is $< \infty$. I searched for any way to simplify the expression, but so far couldn't find any. Sure we can write the limit as $\lim_{n \to \infty} \frac{\log(n)}{n^{\frac{1}{\log(\log(n))}}}$, but that doesn't seem to help either. Next I thought about considering the derivative of $\log(n)^{\log(\log(n))}$. If it's derivative is bounded, then it grows at most linearly, which would also show the statement. Unfortunately though, it's derivative is (according to Wolframalpha) $\frac{2 \log^{\log(\log(n))-1} (n)\log(\log(n))}{n}$ which looks even more complicated than the limit I wanted to examine before, and it is (to me at least) not visible right away why this derivative must be bounded. I also thought about induction arguments. When looking at a graph of the function, I noticed that $\log(n)^{\log(\log(n))}$ gets more and more flat for  higher $n$, so maybe I can use an induction argument by fixing a fitting $c \in \mathbb{R}_{> 0}$ and showing that $\log(n)^{\log(\log(n))} ≤ c n$ for almost all $n \in \mathbb{N}$? The way this exercise was posed to me makes me think that I'm probably thinking way too complicated and that there is a much easier way to show the desired statement, but so far, I haven't found any way that works.","I want to show that $$\log(n)^{\log(\log(n))} \in \mathcal{O}(n)$$ where $n \in \mathbb{N}_{≥2}$, and $\mathcal{O}$ is the big-O-notation. It seems like a relatively simply statement, but so far, I've had no luck. I first started with the limit $\lim_{n \to \infty} \frac{\log(n)^{\log(\log(n))}}{n}$. In order to show the statement, it would be sufficient to show that this limit is $< \infty$. I searched for any way to simplify the expression, but so far couldn't find any. Sure we can write the limit as $\lim_{n \to \infty} \frac{\log(n)}{n^{\frac{1}{\log(\log(n))}}}$, but that doesn't seem to help either. Next I thought about considering the derivative of $\log(n)^{\log(\log(n))}$. If it's derivative is bounded, then it grows at most linearly, which would also show the statement. Unfortunately though, it's derivative is (according to Wolframalpha) $\frac{2 \log^{\log(\log(n))-1} (n)\log(\log(n))}{n}$ which looks even more complicated than the limit I wanted to examine before, and it is (to me at least) not visible right away why this derivative must be bounded. I also thought about induction arguments. When looking at a graph of the function, I noticed that $\log(n)^{\log(\log(n))}$ gets more and more flat for  higher $n$, so maybe I can use an induction argument by fixing a fitting $c \in \mathbb{R}_{> 0}$ and showing that $\log(n)^{\log(\log(n))} ≤ c n$ for almost all $n \in \mathbb{N}$? The way this exercise was posed to me makes me think that I'm probably thinking way too complicated and that there is a much easier way to show the desired statement, but so far, I haven't found any way that works.",,"['calculus', 'limits', 'asymptotics']"
99,How to calculate $\lim\limits_{x\to 0} \frac{[\sin{x}-x][\cos({3x})-1]}{x[e^x -1]^4}$ without using L'Hôpital's Rule?,How to calculate  without using L'Hôpital's Rule?,\lim\limits_{x\to 0} \frac{[\sin{x}-x][\cos({3x})-1]}{x[e^x -1]^4},How to calculate $\lim\limits_{x\to 0} \frac{[\sin{x}-x][\cos({3x})-1]}{x[e^x -1]^4}$ without using L'Hôpital's Rule? I tried Taylor expansion but I couldn't solve the resulting summations. I also tried expanding them out but there were way too many terms. What is a valid way to solve the question?,How to calculate $\lim\limits_{x\to 0} \frac{[\sin{x}-x][\cos({3x})-1]}{x[e^x -1]^4}$ without using L'Hôpital's Rule? I tried Taylor expansion but I couldn't solve the resulting summations. I also tried expanding them out but there were way too many terms. What is a valid way to solve the question?,,"['calculus', 'limits', 'limits-without-lhopital']"
