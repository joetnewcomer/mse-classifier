,title_raw,title_text,title_latex,body_raw,body_text,body_latex,tags
0,Population Variation with two variables,Population Variation with two variables,,"I have a dataset with two variables. I want to treat my dataset as a population not a sample. I am wondering if I can just use the formula for population variance as below: $$\frac{\sum(X-\mu)^2}{N}$$ Here, do I need to think about the degree of freedom?  Does the degree of freedom have anything with the number of variables? If you have expertise in this, please let me have an idea. Thank you.","I have a dataset with two variables. I want to treat my dataset as a population not a sample. I am wondering if I can just use the formula for population variance as below: $$\frac{\sum(X-\mu)^2}{N}$$ Here, do I need to think about the degree of freedom?  Does the degree of freedom have anything with the number of variables? If you have expertise in this, please let me have an idea. Thank you.",,"['statistics', 'statistical-inference', 'descriptive-statistics']"
1,The variance of Lincoln-Peterson estimator,The variance of Lincoln-Peterson estimator,,"Capture-recapture is a method commonly used in ecology to estimate an animal population's size. Let $N$ = Number of animals in the population; $K$ = Number of animals marked on the first visit; $n$ = Number of animals captured on the second visit; $k$ = Number of recaptured animals that were marked. It is assumed that all individuals have the same probability of being captured in the second sample, regardless of whether they were previously captured in the first sample.  This implies that, in the second sample, the proportion of marked individuals that are caught ($k/K$) should equal the proportion of the total population that is caught ($n/N$).  In symbols, $\frac{k}{K} = \frac{n}{N}$. This derives the Lincoln–Petersen estimator$$\hat{N}_{LP}=\frac{Kn}{k}$$ The problem is how to calculate the variance of $\hat{N}_{LP}$. I have tried the hypergeometric distribution likelihood, and treated the sample size $K,n$ as fixed. This model contains one parameter $N$ and one random variable $k$. I can get that $\hat{N}_{LP}$ is the MLE of $N$, but get stucked with its variance. Thanks for your help.","Capture-recapture is a method commonly used in ecology to estimate an animal population's size. Let $N$ = Number of animals in the population; $K$ = Number of animals marked on the first visit; $n$ = Number of animals captured on the second visit; $k$ = Number of recaptured animals that were marked. It is assumed that all individuals have the same probability of being captured in the second sample, regardless of whether they were previously captured in the first sample.  This implies that, in the second sample, the proportion of marked individuals that are caught ($k/K$) should equal the proportion of the total population that is caught ($n/N$).  In symbols, $\frac{k}{K} = \frac{n}{N}$. This derives the Lincoln–Petersen estimator$$\hat{N}_{LP}=\frac{Kn}{k}$$ The problem is how to calculate the variance of $\hat{N}_{LP}$. I have tried the hypergeometric distribution likelihood, and treated the sample size $K,n$ as fixed. This model contains one parameter $N$ and one random variable $k$. I can get that $\hat{N}_{LP}$ is the MLE of $N$, but get stucked with its variance. Thanks for your help.",,"['probability', 'statistics', 'biology']"
2,Statistics: How to use Poisson to determine if event would not occur?,Statistics: How to use Poisson to determine if event would not occur?,,This is a follow up to my previous question disk manufacturer averages 0.2 missing pulses per disk. let X denote # of missing pulses I am using poisson distribution I'm having trouble determining the probability that a disk would have no missing pulses So I did P(X=0)$e^{-0.2}(0.2)^0 /0!$ = $0.81873$ So I think that is the probability of getting missing pulses so I did complement $1-0.81873 = 0.18127$ another question asks me to determine the probability that next 2 disks will contain no missing pulses. So I did 0.18127 * 0.18127 = 0.03285 Can anyone confirm if this is right?,This is a follow up to my previous question disk manufacturer averages 0.2 missing pulses per disk. let X denote # of missing pulses I am using poisson distribution I'm having trouble determining the probability that a disk would have no missing pulses So I did P(X=0)$e^{-0.2}(0.2)^0 /0!$ = $0.81873$ So I think that is the probability of getting missing pulses so I did complement $1-0.81873 = 0.18127$ another question asks me to determine the probability that next 2 disks will contain no missing pulses. So I did 0.18127 * 0.18127 = 0.03285 Can anyone confirm if this is right?,,"['statistics', 'poisson-distribution']"
3,Paternity probability calculator based on blood group and eye color,Paternity probability calculator based on blood group and eye color,,"I am currently writing a paternity probability calculator. I am struggling with finding the correct statistical approach to determining probability based on blood type and on eye colour. For example, assume the following family: Blood Type     Eye Colour Alleged Father    A              Blue Mother            AB             Blue Child             A              Green A father of type A and a mother of type B can produce the following blood types for their child: Child's possible blood types    Occurrence A                               50% B                               25% AB                              25% O                               0% A father of eye colour Blue and a mother of eye colour Blue can produce a child of the following eye colours: Child's possible eye colours    Occurrence Brown                           0% Blue                            90% Green                           10% I would like to provide the probability of paternity for the alleged father given this data. I am struggling with the following: If the father is of blood type A and the mother is B , then knowing that the child is A should increase our confidence in the paternity of the father - Because we eliminated the less likely results (e.g. O ) from the equation. Should I start the calculation at a ""pseudo-random"" value (say 50%), then multiply this value using a confidence factor derived from the occurrence percentage? How to derive the confidence factor from the occurrence percentage? In this case, even though the occurrence of blood type A is 50%, it seams that I should multiply the pseudo-random value by a factor above 1. If the blood type of the child would be B instead, it also seems that the factor should be above 1, but less than blood type A would be. If the child's blood type would be O , the multiplication factor should be zero.","I am currently writing a paternity probability calculator. I am struggling with finding the correct statistical approach to determining probability based on blood type and on eye colour. For example, assume the following family: Blood Type     Eye Colour Alleged Father    A              Blue Mother            AB             Blue Child             A              Green A father of type A and a mother of type B can produce the following blood types for their child: Child's possible blood types    Occurrence A                               50% B                               25% AB                              25% O                               0% A father of eye colour Blue and a mother of eye colour Blue can produce a child of the following eye colours: Child's possible eye colours    Occurrence Brown                           0% Blue                            90% Green                           10% I would like to provide the probability of paternity for the alleged father given this data. I am struggling with the following: If the father is of blood type A and the mother is B , then knowing that the child is A should increase our confidence in the paternity of the father - Because we eliminated the less likely results (e.g. O ) from the equation. Should I start the calculation at a ""pseudo-random"" value (say 50%), then multiply this value using a confidence factor derived from the occurrence percentage? How to derive the confidence factor from the occurrence percentage? In this case, even though the occurrence of blood type A is 50%, it seams that I should multiply the pseudo-random value by a factor above 1. If the blood type of the child would be B instead, it also seems that the factor should be above 1, but less than blood type A would be. If the child's blood type would be O , the multiplication factor should be zero.",,['statistics']
4,"Expected Value, elevators","Expected Value, elevators",,"A building contains two elevators, one fast and one slow. The average waiting time for the slow elevator is 3 min. and the average waiting time of the fast elevator is 1 min. If a passenger chooses the fast elevator with probability 2/3 and the slow elevator with probability 1/3, what is the expected waiting time? So I know you can just do it this way:  [3*2/3] + [1*1/3] mins = 2 1/3 min How do I do it using the theorem E(Y) = E[E(Y|X)]","A building contains two elevators, one fast and one slow. The average waiting time for the slow elevator is 3 min. and the average waiting time of the fast elevator is 1 min. If a passenger chooses the fast elevator with probability 2/3 and the slow elevator with probability 1/3, what is the expected waiting time? So I know you can just do it this way:  [3*2/3] + [1*1/3] mins = 2 1/3 min How do I do it using the theorem E(Y) = E[E(Y|X)]",,"['probability', 'statistics']"
5,"Pooled sample variance, how to prove","Pooled sample variance, how to prove",,"I did read the related question, and if it did contain the answer to my question, it must have been above my level. This is my very first post, so I'll stick to letters for now. The pooled sample variance for two stochastic variables with the same variance, is defined as: $$\frac{((n-1)(∑X-(\bar{X}))^2 +(m-1)∑(Y-(\bar{Y})^2)}{n + m - 2}$$ Why on earth would you use this cumbersome expression? Why not simply add the two sample variances and divide by two? Like this:  $$\frac{((m-1)(∑X-(\bar{X}))^2 +(n-1)∑(Y-(\bar{Y})^2)}{2(n-1)(m-1)}$$ I did the math and...the expected value of this is ""also"" equal to the variance. It looks more complicated....but it certainly feels more intuitive. Is there a reason for using the first expression, and not the second? Thanks a lot! /Magnus","I did read the related question, and if it did contain the answer to my question, it must have been above my level. This is my very first post, so I'll stick to letters for now. The pooled sample variance for two stochastic variables with the same variance, is defined as: $$\frac{((n-1)(∑X-(\bar{X}))^2 +(m-1)∑(Y-(\bar{Y})^2)}{n + m - 2}$$ Why on earth would you use this cumbersome expression? Why not simply add the two sample variances and divide by two? Like this:  $$\frac{((m-1)(∑X-(\bar{X}))^2 +(n-1)∑(Y-(\bar{Y})^2)}{2(n-1)(m-1)}$$ I did the math and...the expected value of this is ""also"" equal to the variance. It looks more complicated....but it certainly feels more intuitive. Is there a reason for using the first expression, and not the second? Thanks a lot! /Magnus",,['statistics']
6,Expected Total Number,Expected Total Number,,"To determine whether or not they have a certain disease, 160 people are to have their blood tested. However, rather than testing each individual separately, it has been decided first to group the people in groups of 10. The blood samples of the 10 people in each group will be pooled and analyzed together. If the test is negative. one test will suffice for the 10 people (we are assuming that the pooled test will be positive if and only if at least one person in the pool has the disease); whereas, if the test is positive each of the 10 people will also be individually tested and, in all, 11 tests will be made on this group. Assume the probability that a person has the disease is 0.04 for all people, independently of each other. I know the expected number of tests necessary for each group is $$11-10*(.96)^{10}$$ I just need help computing the expected total number of tests necessary for the entire population of 160 people.","To determine whether or not they have a certain disease, 160 people are to have their blood tested. However, rather than testing each individual separately, it has been decided first to group the people in groups of 10. The blood samples of the 10 people in each group will be pooled and analyzed together. If the test is negative. one test will suffice for the 10 people (we are assuming that the pooled test will be positive if and only if at least one person in the pool has the disease); whereas, if the test is positive each of the 10 people will also be individually tested and, in all, 11 tests will be made on this group. Assume the probability that a person has the disease is 0.04 for all people, independently of each other. I know the expected number of tests necessary for each group is $$11-10*(.96)^{10}$$ I just need help computing the expected total number of tests necessary for the entire population of 160 people.",,"['probability', 'statistics']"
7,Statistics book recommendation [duplicate],Statistics book recommendation [duplicate],,"This question already has answers here : Recommend a statistics fundamentals book (6 answers) Closed 3 years ago . this is a subject that time and again shows up, and I've read old postings. Still: I am taking an intro class in statistics from a Math department (Junior/Senior level). It is pretty intense (it's statistics and probability) and has a strong math component (calculus as well as linear algebra are required and used throughout the class). Exercises are done using R most of the time. The book we follow is an unedited book from the lecturer, and it's OK, but even though it's strong in the math part, it doesn't give a lot of examples nor tackles the very important aspects of why we do some things or what's the importance of this or that. I read here that a lot of people love Sheldon Ross' books, but when I looked at them, the equation parts are way below what we are doing. Is there a book like that, but more in depth in the math part?  Thank you so much for any suggestion and why.","This question already has answers here : Recommend a statistics fundamentals book (6 answers) Closed 3 years ago . this is a subject that time and again shows up, and I've read old postings. Still: I am taking an intro class in statistics from a Math department (Junior/Senior level). It is pretty intense (it's statistics and probability) and has a strong math component (calculus as well as linear algebra are required and used throughout the class). Exercises are done using R most of the time. The book we follow is an unedited book from the lecturer, and it's OK, but even though it's strong in the math part, it doesn't give a lot of examples nor tackles the very important aspects of why we do some things or what's the importance of this or that. I read here that a lot of people love Sheldon Ross' books, but when I looked at them, the equation parts are way below what we are doing. Is there a book like that, but more in depth in the math part?  Thank you so much for any suggestion and why.",,"['probability', 'statistics', 'book-recommendation']"
8,Injective functions and sufficient statistics,Injective functions and sufficient statistics,,"I'm trying to prove that for a random sample $X_1,\ldots,X_n$ that depends on $\theta$, if $T$ is a sufficient statistic for $\theta$, then so is $T'=f\circ T$, for any injective function $f$. My attempt: Since $f$ is injective, if we restrict the target space of $f$ to $\operatorname{range}(f)$, then we can consider $f$ as bijective. Now $T'\in \operatorname{range}(f)$ since $T'=f\circ T$, so $T=f^{-1}\circ T'$. Define $\phi'(u(\vec{x}),\theta)=\phi(f^{-1}\circ u(\vec{x}),\theta)$. Since $T$ is sufficient, we have that $f(\vec{x};\theta)=\phi(T(\vec{x};\theta))h(\vec{x})=\phi(f^{-1}\circ T'(\vec{x};\theta))h(\vec{x})=\phi'(T'(\vec{x};\theta))h(\vec{x})$. Hence, $T'$ is also sufficient. Does this look alright?","I'm trying to prove that for a random sample $X_1,\ldots,X_n$ that depends on $\theta$, if $T$ is a sufficient statistic for $\theta$, then so is $T'=f\circ T$, for any injective function $f$. My attempt: Since $f$ is injective, if we restrict the target space of $f$ to $\operatorname{range}(f)$, then we can consider $f$ as bijective. Now $T'\in \operatorname{range}(f)$ since $T'=f\circ T$, so $T=f^{-1}\circ T'$. Define $\phi'(u(\vec{x}),\theta)=\phi(f^{-1}\circ u(\vec{x}),\theta)$. Since $T$ is sufficient, we have that $f(\vec{x};\theta)=\phi(T(\vec{x};\theta))h(\vec{x})=\phi(f^{-1}\circ T'(\vec{x};\theta))h(\vec{x})=\phi'(T'(\vec{x};\theta))h(\vec{x})$. Hence, $T'$ is also sufficient. Does this look alright?",,['statistics']
9,Estimate large covariance matrix using few samples.,Estimate large covariance matrix using few samples.,,"Let $\mathbf{x}$ be a random vector in $\Bbb{R}^n$, such that $\mathbf{x}\sim N(\bar{\mathbf{x}}, \Sigma)$. $N$ observations of $\mathbf{x}$ are available, say $\{\mathbf{x}_i, i=1,\ldots,N\}$. The mean vector $\bar{\mathbf{x}}$ is given (no need to compute it by the available samples). We want to compute the covariance matrix $\Sigma$. By definition, $$ \Sigma = \frac{1}{N-1}\sum_{i=1}^N (\mathbf{x}_i-\bar{\mathbf{x}})(\mathbf{x}_i-\bar{\mathbf{x}})^\top, $$ but what is the case when $N\ll n$? For instance, let the dimensionality of $\mathbf{x}$ be $n=1000$ and suppose that we have solely $N=10$ observations of it. I think that because of the so-called ""curse of dimensionality"" we cannot find a sufficient estimation of $\Sigma$. Is that true? Finally, could we have some better estimation given that $\Sigma=\sigma I_n$? That is, does it help if we require to find just one parameter ($\sigma$), not the full covariance matrix? EDIT I: What I have tried so far is to find that $\sigma^\star$ which minimizes the Frobenius norm of $\Sigma-\sigma I_n$: $$ \sigma^\star = \underset{\sigma} {\mathrm{argmin}} \left\| \Sigma - \sigma I_n \right\|_F^2, $$ and thus $$ \sigma^\star = \frac{1}{n}\sum_{i=1}^{n}\sigma_{ii}. $$ Does it make sense? Thanks a lot! EDIT II: The above approach seems to produce meaningful covariance matrices (are they trully meaningful?), but it does not work in practice. I would like to discuss a meaningful procedure using which would give acceptable estimations of the covariance matrix. Has anyone experience on this kind of estimation/modeling problems? Thanks a lot!","Let $\mathbf{x}$ be a random vector in $\Bbb{R}^n$, such that $\mathbf{x}\sim N(\bar{\mathbf{x}}, \Sigma)$. $N$ observations of $\mathbf{x}$ are available, say $\{\mathbf{x}_i, i=1,\ldots,N\}$. The mean vector $\bar{\mathbf{x}}$ is given (no need to compute it by the available samples). We want to compute the covariance matrix $\Sigma$. By definition, $$ \Sigma = \frac{1}{N-1}\sum_{i=1}^N (\mathbf{x}_i-\bar{\mathbf{x}})(\mathbf{x}_i-\bar{\mathbf{x}})^\top, $$ but what is the case when $N\ll n$? For instance, let the dimensionality of $\mathbf{x}$ be $n=1000$ and suppose that we have solely $N=10$ observations of it. I think that because of the so-called ""curse of dimensionality"" we cannot find a sufficient estimation of $\Sigma$. Is that true? Finally, could we have some better estimation given that $\Sigma=\sigma I_n$? That is, does it help if we require to find just one parameter ($\sigma$), not the full covariance matrix? EDIT I: What I have tried so far is to find that $\sigma^\star$ which minimizes the Frobenius norm of $\Sigma-\sigma I_n$: $$ \sigma^\star = \underset{\sigma} {\mathrm{argmin}} \left\| \Sigma - \sigma I_n \right\|_F^2, $$ and thus $$ \sigma^\star = \frac{1}{n}\sum_{i=1}^{n}\sigma_{ii}. $$ Does it make sense? Thanks a lot! EDIT II: The above approach seems to produce meaningful covariance matrices (are they trully meaningful?), but it does not work in practice. I would like to discuss a meaningful procedure using which would give acceptable estimations of the covariance matrix. Has anyone experience on this kind of estimation/modeling problems? Thanks a lot!",,"['statistics', 'normal-distribution', 'estimation', 'covariance']"
10,Are the Fibonacci numbers' prevalence in nature due to confirmation bias?,Are the Fibonacci numbers' prevalence in nature due to confirmation bias?,,"The Fibonacci numbers are frequently found in nature, such as in the petals of flowers or the shape of pinecones. But are the numbers actually any more prevalent than other numbers? Could it all be because of confirmation bias? Is there any research that shows that the Fibonacci numbers occur in nature to a statistically significant extent?","The Fibonacci numbers are frequently found in nature, such as in the petals of flowers or the shape of pinecones. But are the numbers actually any more prevalent than other numbers? Could it all be because of confirmation bias? Is there any research that shows that the Fibonacci numbers occur in nature to a statistically significant extent?",,"['statistics', 'fibonacci-numbers']"
11,Generating random variables with complicated probability distribution functions,Generating random variables with complicated probability distribution functions,,"I have an interesting question I need to solve, and as much as I try, I cannot wrap my head around it. Given a postive random variable X with p.d.f. f , it can be proven if you generate a standard uniform value U , the value a will be a random variable with p.d.f. f , as given by the formula below: $$ \int_0^a f(x) dx = U$$ I am told to use this relationship to write a function which generates values for the random variable X with p.d.f.: $ f(x) = 4x^3 $ if $ 0 \le x < 1 $ Can someone be so kind as to point me in the direction I need to go to begin solving this problem?","I have an interesting question I need to solve, and as much as I try, I cannot wrap my head around it. Given a postive random variable X with p.d.f. f , it can be proven if you generate a standard uniform value U , the value a will be a random variable with p.d.f. f , as given by the formula below: $$ \int_0^a f(x) dx = U$$ I am told to use this relationship to write a function which generates values for the random variable X with p.d.f.: $ f(x) = 4x^3 $ if $ 0 \le x < 1 $ Can someone be so kind as to point me in the direction I need to go to begin solving this problem?",,"['statistics', 'probability-distributions']"
12,CLT - infinite variance,CLT - infinite variance,,"Student t distribution with df=2 has infinite variance => CLT should not hold(?) Yet, when I draw a large sample thousands of times and plot the histogram of the standardized sample mean, it has a normal distribution. Why is this?","Student t distribution with df=2 has infinite variance => CLT should not hold(?) Yet, when I draw a large sample thousands of times and plot the histogram of the standardized sample mean, it has a normal distribution. Why is this?",,"['probability', 'statistics']"
13,"In composition of two mappings, can the outer mapping access the arguments of the inner mapping?","In composition of two mappings, can the outer mapping access the arguments of the inner mapping?",,"In composition of two mappings, can the outer mapping access the arguments of the inner mapping? Here is an example to illustrate my question and my thought. E.g. $f: \cup_{n \in \mathbb N} \mathbb R^n \to \mathbb R$ is taking sum, i.e.  $f(x_1, \dots, x_n) =  \sum_i x_i$. Note here $n$ can vary in $\mathbb N$. $g: \mathbb R \to \mathbb R$ is to further divide the sum by the sample size, to get the sample mean , i.e. $g(f(x_1, \dots, x_n)) = \sum_i x_i/n$. Is $g(f(x_1, \dots, x_n))$ a composition of the two mappings $f$ and $g$? I think that in a  composition of two mappings, the outer mapping  only acts on the codomain of the inner mapping, and thus shouldn't know the input to the inner mapping. In the example, $g$ should not know  $n$. So it doesn't make sense to  compose $f$ and $g$. A revision would be $f: \cup_{n \in \mathbb N} \mathbb R^n \to \mathbb  R \times \mathbb N$ with $f(x_1, \dots, x_n) =  [\sum_i x_i, n]^T$, and $g: \mathbb  R \times \mathbb N \to \mathbb R$ with $g(x,n) = x/n$. Only then the composition of $f$ and $g$ would make sense to me. What would you think? The question comes from statistics, where $f$ is a ""statistic"" which takes in a sample of size $n$, and $n$ can vary in $\mathbb N$, and $g$ is a transform on the statistic. To see more of my specific question, please refer to https://stats.stackexchange.com/questions/114240/can-the-measurable-mapping-in-the-definition-of-complete-statistics-depend-on-sa . Here at math.se, I ask a more general question for general math. In the link to stats.se, I asked a specific question in statistics. I understand that the answers to the two may be related and different. Thanks.","In composition of two mappings, can the outer mapping access the arguments of the inner mapping? Here is an example to illustrate my question and my thought. E.g. $f: \cup_{n \in \mathbb N} \mathbb R^n \to \mathbb R$ is taking sum, i.e.  $f(x_1, \dots, x_n) =  \sum_i x_i$. Note here $n$ can vary in $\mathbb N$. $g: \mathbb R \to \mathbb R$ is to further divide the sum by the sample size, to get the sample mean , i.e. $g(f(x_1, \dots, x_n)) = \sum_i x_i/n$. Is $g(f(x_1, \dots, x_n))$ a composition of the two mappings $f$ and $g$? I think that in a  composition of two mappings, the outer mapping  only acts on the codomain of the inner mapping, and thus shouldn't know the input to the inner mapping. In the example, $g$ should not know  $n$. So it doesn't make sense to  compose $f$ and $g$. A revision would be $f: \cup_{n \in \mathbb N} \mathbb R^n \to \mathbb  R \times \mathbb N$ with $f(x_1, \dots, x_n) =  [\sum_i x_i, n]^T$, and $g: \mathbb  R \times \mathbb N \to \mathbb R$ with $g(x,n) = x/n$. Only then the composition of $f$ and $g$ would make sense to me. What would you think? The question comes from statistics, where $f$ is a ""statistic"" which takes in a sample of size $n$, and $n$ can vary in $\mathbb N$, and $g$ is a transform on the statistic. To see more of my specific question, please refer to https://stats.stackexchange.com/questions/114240/can-the-measurable-mapping-in-the-definition-of-complete-statistics-depend-on-sa . Here at math.se, I ask a more general question for general math. In the link to stats.se, I asked a specific question in statistics. I understand that the answers to the two may be related and different. Thanks.",,"['probability', 'statistics', 'functions', 'elementary-set-theory', 'notation']"
14,Statistical Estimation Book Request,Statistical Estimation Book Request,,"I am seeking a clear book for parameter estimation, estimation methods, properties of estimators, minimum variance estimators, asymptotic properties of estimators and interval estimation reducution.","I am seeking a clear book for parameter estimation, estimation methods, properties of estimators, minimum variance estimators, asymptotic properties of estimators and interval estimation reducution.",,"['statistics', 'reference-request', 'parameter-estimation']"
15,Does affine equivariance implies shape unbiasedness?,Does affine equivariance implies shape unbiasedness?,,"Basically, I'm dealing with an algorithm that, given an $n\times p$ data matrix $\pmb X$ with iid rows, returns $\hat{\pmb\sigma}(\pmb X)\in\mathbb{R}^{p\times p}$. To simplify things, I will also assume that $\pmb \varSigma$ (the true covariance matrix of $\pmb X$) has full rank. The algorithm is demonstrably affine equivariant, e.g.: $$\hat{\pmb \sigma}(\pmb A\pmb X+\pmb 1_n\pmb b')=\pmb A\hat{\pmb \sigma}(\pmb X)\pmb A'$$ --$\forall$ non singular $\pmb A\in \mathbb{R}^{p\times p}$ and $p$-vector $\pmb b$. However, it is not known whether the algorithm satisfies a particular form of unbiasedness. More precisely, the interest is in establishing whether (or possibly under which conditions) the estimate returned by the algorithm is shape unbiased. By this I mean that: $$E[\hat{\pmb\sigma}(\pmb X)/|\hat{\pmb\sigma}(\pmb X)|^{1/p}]=\pmb \varSigma/|\pmb \varSigma|^{1/p}$$ where for any $p$ by $p$ symmetric matrix $\pmb B$,  $|\pmb B|$ denotes the determinant of $\pmb B$. The problem is that, in this case, establishing shape unbiasedness appears quiet complicated. The question, I guess, is whether (or possibly under what set of additional conditions does) affine equivariance implies shape unbiasedness.","Basically, I'm dealing with an algorithm that, given an $n\times p$ data matrix $\pmb X$ with iid rows, returns $\hat{\pmb\sigma}(\pmb X)\in\mathbb{R}^{p\times p}$. To simplify things, I will also assume that $\pmb \varSigma$ (the true covariance matrix of $\pmb X$) has full rank. The algorithm is demonstrably affine equivariant, e.g.: $$\hat{\pmb \sigma}(\pmb A\pmb X+\pmb 1_n\pmb b')=\pmb A\hat{\pmb \sigma}(\pmb X)\pmb A'$$ --$\forall$ non singular $\pmb A\in \mathbb{R}^{p\times p}$ and $p$-vector $\pmb b$. However, it is not known whether the algorithm satisfies a particular form of unbiasedness. More precisely, the interest is in establishing whether (or possibly under which conditions) the estimate returned by the algorithm is shape unbiased. By this I mean that: $$E[\hat{\pmb\sigma}(\pmb X)/|\hat{\pmb\sigma}(\pmb X)|^{1/p}]=\pmb \varSigma/|\pmb \varSigma|^{1/p}$$ where for any $p$ by $p$ symmetric matrix $\pmb B$,  $|\pmb B|$ denotes the determinant of $\pmb B$. The problem is that, in this case, establishing shape unbiasedness appears quiet complicated. The question, I guess, is whether (or possibly under what set of additional conditions does) affine equivariance implies shape unbiasedness.",,"['linear-algebra', 'probability', 'statistics']"
16,Math formulas on Clustering,Math formulas on Clustering,,"I am currently studying Clustering in Machine Learning. I have found a document regarding guessing the right number of clusters. I am reading the first part of it, having difficulties in understanding the first four formulas in the red circles below in the image. I believe I have understood a little bit about it but not 100% sure. Can somebody give me expertise in what these four formulas are saying? Thank you and I am sorry if I have placed my questions in the wrong places.","I am currently studying Clustering in Machine Learning. I have found a document regarding guessing the right number of clusters. I am reading the first part of it, having difficulties in understanding the first four formulas in the red circles below in the image. I believe I have understood a little bit about it but not 100% sure. Can somebody give me expertise in what these four formulas are saying? Thank you and I am sorry if I have placed my questions in the wrong places.",,"['statistics', 'mathematical-physics', 'machine-learning']"
17,"Statistical test for ""too perfect"" random number generator?","Statistical test for ""too perfect"" random number generator?",,"I am attempting to characterize some random number generator programs in a very simple way.  Specifically, I'm rolling a simulated 6-sided die $3 \times 10^8$ times and keeping a count of how many times each of the six possible outcomes happens.  I have used a $\chi^2$ test for each of the different random number generators.  That is, for all of the observation counts $O_i$ and the expected value of $E=3\times10^8/6$ I calculate: $$ \chi^2 = \sum_{i=1}^{6}\frac{(O_i-E)^2}{E}$$ This value is then compared to 20.52, which, if I've understood correctly is the correct value for $p=0.001$ and 5 degrees of freedom as in this chart . All of the random number generators pass this test, but I notice that some of them look a little too good.  That is, the value of $\displaystyle \frac{(O_i-E)^2}{E}$ is less than 0.1 for each observation count. I have looked at the fifteen NIST random number generator tests but I'd rather not run all of those tests and don't know which (if any) of those tests captures the notion I'm trying to convey. So my questions is: Is there a statistically valid test that expresses this notion of ""too good"" in this context?","I am attempting to characterize some random number generator programs in a very simple way.  Specifically, I'm rolling a simulated 6-sided die $3 \times 10^8$ times and keeping a count of how many times each of the six possible outcomes happens.  I have used a $\chi^2$ test for each of the different random number generators.  That is, for all of the observation counts $O_i$ and the expected value of $E=3\times10^8/6$ I calculate: $$ \chi^2 = \sum_{i=1}^{6}\frac{(O_i-E)^2}{E}$$ This value is then compared to 20.52, which, if I've understood correctly is the correct value for $p=0.001$ and 5 degrees of freedom as in this chart . All of the random number generators pass this test, but I notice that some of them look a little too good.  That is, the value of $\displaystyle \frac{(O_i-E)^2}{E}$ is less than 0.1 for each observation count. I have looked at the fifteen NIST random number generator tests but I'd rather not run all of those tests and don't know which (if any) of those tests captures the notion I'm trying to convey. So my questions is: Is there a statistically valid test that expresses this notion of ""too good"" in this context?",,"['probability', 'statistics', 'dice']"
18,Show the inequality $P[|S_n-mn| \ge n \epsilon] \le \frac{ σ ^2}{n \epsilon^2}$,Show the inequality,P[|S_n-mn| \ge n \epsilon] \le \frac{ σ ^2}{n \epsilon^2},"Show the inequality $P[|S_n-mn| \ge n \epsilon ] \le \frac{ σ ^2}{n \epsilon ^2}$ for every $ \epsilon>0$ where $S_n = X_1+...+X_n$ and $X_i$ are independant random variables under the same law. Also $m=E[X]$, $σ=Var[X]$ I think that i need to apply Chebyshev's inequality but i am not sure how...","Show the inequality $P[|S_n-mn| \ge n \epsilon ] \le \frac{ σ ^2}{n \epsilon ^2}$ for every $ \epsilon>0$ where $S_n = X_1+...+X_n$ and $X_i$ are independant random variables under the same law. Also $m=E[X]$, $σ=Var[X]$ I think that i need to apply Chebyshev's inequality but i am not sure how...",,"['probability', 'statistics']"
19,Linear combination of normally distributed variables,Linear combination of normally distributed variables,,"We know that if $X \sim N_p(\mu, \Sigma)$ then $a'X \sim N(a'\mu,a'\Sigma a)$ for and $a \in \mathbb{R}_p$. What I need to know is if the converse of this is also true. Can this be proved? Would appreciate any assistance. Thanks Edit: Now I know this is true from what I have copied from Johnson's and Wichern's Applied multivariate statistical analysis. Only thing required now is the proof. Result 4.2 If $X$ is distributed normally as $N_p(\mu, \Sigma)$ then any linear combination of variables $a'X = a_1X_1 + a_2X_2 + \cdots + a_pX_p$ is distributed as $N(a'\mu, a'\Sigma a)$. Also, if $a'X$ is distributed as $N(a'\mu, a'\Sigma a)$ for every $a$, then $X$ must be $N_p(\mu, \Sigma)$.","We know that if $X \sim N_p(\mu, \Sigma)$ then $a'X \sim N(a'\mu,a'\Sigma a)$ for and $a \in \mathbb{R}_p$. What I need to know is if the converse of this is also true. Can this be proved? Would appreciate any assistance. Thanks Edit: Now I know this is true from what I have copied from Johnson's and Wichern's Applied multivariate statistical analysis. Only thing required now is the proof. Result 4.2 If $X$ is distributed normally as $N_p(\mu, \Sigma)$ then any linear combination of variables $a'X = a_1X_1 + a_2X_2 + \cdots + a_pX_p$ is distributed as $N(a'\mu, a'\Sigma a)$. Also, if $a'X$ is distributed as $N(a'\mu, a'\Sigma a)$ for every $a$, then $X$ must be $N_p(\mu, \Sigma)$.",,['statistics']
20,How to show that two multivariate normal distributed random variables are independent?,How to show that two multivariate normal distributed random variables are independent?,,"Let $X\sim N(\mu_1,V_1),~~Y\sim N(\mu_2,V_2)$. How can I show that $X$ and $Y$ are independent? I am wondering how I can show this. I only know the following case: $Z=(Z_1,\ldots,Z_n)\sim N(\mu_3,V_3)$: Then $Z_i$ are independent if $\text{cov}(Z_i,Z_j)$ for all $i\neq j$. But here the situation is different, because $X$ and $Y$ are both multivariate normal distributed. Indeed I do not know how to show the independence in this case. Can you help me?","Let $X\sim N(\mu_1,V_1),~~Y\sim N(\mu_2,V_2)$. How can I show that $X$ and $Y$ are independent? I am wondering how I can show this. I only know the following case: $Z=(Z_1,\ldots,Z_n)\sim N(\mu_3,V_3)$: Then $Z_i$ are independent if $\text{cov}(Z_i,Z_j)$ for all $i\neq j$. But here the situation is different, because $X$ and $Y$ are both multivariate normal distributed. Indeed I do not know how to show the independence in this case. Can you help me?",,[]
21,Show that a $-\log$ transformation of a Pareto distribution is exponentially distributed,Show that a  transformation of a Pareto distribution is exponentially distributed,-\log,"Question: Given that $y$ is distributed as: $$ f(y; \theta) = \theta y^{(\theta-1)} $$ $$0<y<1 , \theta>0$$ If $Z = -\log(Y)$, show that $Z$ has an exponential distribution (ie $E(Z) = 1/\theta$). My Working: $Y = e^{-z}$ $f(z; \theta)  =  \theta e^{-z(\theta - 1)}$ However I can't seem to get that into the standard exponential form of: $$\lambda e^{-z\lambda}$$ The question states that the fact for the gamma random variable $X$, the following may be useful: $$E\left(\frac{1}{X}\right) = \frac{1}{\beta ( \alpha - 1 )}$$ My other avenue of thought was that to find the expected value of a continuous variable, the following is used: $$E(Z) = \int z  f(z) dz$$ When I use that on the function I derived ($\theta e^{-z(\theta - 1)}$) using the support $-\log(0)$ to $-\log(1)$, ie $0$ to infinity, i don't get the correct answer. Do I need to make some sort of transformation of my function to get it into the standard exponential form?","Question: Given that $y$ is distributed as: $$ f(y; \theta) = \theta y^{(\theta-1)} $$ $$0<y<1 , \theta>0$$ If $Z = -\log(Y)$, show that $Z$ has an exponential distribution (ie $E(Z) = 1/\theta$). My Working: $Y = e^{-z}$ $f(z; \theta)  =  \theta e^{-z(\theta - 1)}$ However I can't seem to get that into the standard exponential form of: $$\lambda e^{-z\lambda}$$ The question states that the fact for the gamma random variable $X$, the following may be useful: $$E\left(\frac{1}{X}\right) = \frac{1}{\beta ( \alpha - 1 )}$$ My other avenue of thought was that to find the expected value of a continuous variable, the following is used: $$E(Z) = \int z  f(z) dz$$ When I use that on the function I derived ($\theta e^{-z(\theta - 1)}$) using the support $-\log(0)$ to $-\log(1)$, ie $0$ to infinity, i don't get the correct answer. Do I need to make some sort of transformation of my function to get it into the standard exponential form?",,"['statistics', 'probability-distributions', 'substitution']"
22,Deriving MGF for binomial distribution,Deriving MGF for binomial distribution,,"Using the definition of the binomial distribution, I obtain that: $$\Psi (t) = (pe^t+q)^n $$ I then compute $\Psi ' = npe^t(pe^t+q)^{n-1}$ I then evaluated this at $\Psi'(0)$ and got $\Psi'(0)=np(p+q)^{n-1}$ and so $E[X]$ which I know is $np$. How do I then differentiate $\Psi'(t)$ again, the $n-1$ is throwing me off?","Using the definition of the binomial distribution, I obtain that: $$\Psi (t) = (pe^t+q)^n $$ I then compute $\Psi ' = npe^t(pe^t+q)^{n-1}$ I then evaluated this at $\Psi'(0)$ and got $\Psi'(0)=np(p+q)^{n-1}$ and so $E[X]$ which I know is $np$. How do I then differentiate $\Psi'(t)$ again, the $n-1$ is throwing me off?",,"['statistics', 'probability-distributions', 'moment-generating-functions']"
23,An Application of the Neyman-Pearson Lemma.,An Application of the Neyman-Pearson Lemma.,,"Problem: Using the Neyman-Pearson Lemma, determine the most powerful test of size $ 5 \% $. I know the Neyman-Pearson Lemma says that the test with the critical region $$ \left\{ x \in \{ 1,2,3,4 \} ~ \Bigg| ~ \frac{L(x \mid \theta = 0)}{L(x \mid \theta = 1)} \leq A \right\}, $$ where $ A $ satisfies $$ \mathbf{Pr} \left(             \frac{L(X \mid \theta = 0)}{L(X \mid \theta = 1)} \leq A ~ \Bigg| ~ H_{0}             \right) = 0.05, $$ is the most powerful test of size $ 5 \% $. However, I’m not sure what the likelihood functions are, as I’m more used to seeing them given by a formula. Thanks.","Problem: Using the Neyman-Pearson Lemma, determine the most powerful test of size $ 5 \% $. I know the Neyman-Pearson Lemma says that the test with the critical region $$ \left\{ x \in \{ 1,2,3,4 \} ~ \Bigg| ~ \frac{L(x \mid \theta = 0)}{L(x \mid \theta = 1)} \leq A \right\}, $$ where $ A $ satisfies $$ \mathbf{Pr} \left(             \frac{L(X \mid \theta = 0)}{L(X \mid \theta = 1)} \leq A ~ \Bigg| ~ H_{0}             \right) = 0.05, $$ is the most powerful test of size $ 5 \% $. However, I’m not sure what the likelihood functions are, as I’m more used to seeing them given by a formula. Thanks.",,"['statistics', 'probability-distributions', 'hypothesis-testing']"
24,Mean & SD of Sampling Distribution,Mean & SD of Sampling Distribution,,"A population consists of $4$ numbers $\{0, 2, 4, 6\}$ . Consider drawing a random sample of size $n = 2$ with replacement. (a) What is the sampling distribution of $\bar x$ ? Is this a normal distribution ? Since $\bar x $ ~ $N\left(\mu, \dfrac{\sigma^2}{n}\right)$ ? (b) Calculate the mean & standard deviation of the sampling distribution of $\bar x$ . I got the answer of mean $\mu$ by $\frac{0+2+4+6}{4} = 3$ Thereafter, I proceed to calculate $\sigma$ $$\sigma = \frac{(0 - 3)^2 + (2 - 3)^2 + (4 - 3)^2 + (6 - 3)^2}{4} = 5$$ Substituting it back into the sample distribution gives: $$\bar x \sim N\left(3, \dfrac{5^2}{4}\right)$$ Thus, I derive the standard deviation to be: $$\sqrt{\frac{\sigma^2}{n}} = \dfrac{5}{2}.$$ However, the answer given was $\dfrac{\sqrt{5}}{\sqrt{2}}$ . Can someone explain why is this so? I'm really quite confused with the whole concept of sampling distribution.. Thanks a lot!","A population consists of numbers . Consider drawing a random sample of size with replacement. (a) What is the sampling distribution of ? Is this a normal distribution ? Since ~ ? (b) Calculate the mean & standard deviation of the sampling distribution of . I got the answer of mean by Thereafter, I proceed to calculate Substituting it back into the sample distribution gives: Thus, I derive the standard deviation to be: However, the answer given was . Can someone explain why is this so? I'm really quite confused with the whole concept of sampling distribution.. Thanks a lot!","4 \{0, 2, 4, 6\} n = 2 \bar x \bar x  N\left(\mu, \dfrac{\sigma^2}{n}\right) \bar x \mu \frac{0+2+4+6}{4} = 3 \sigma \sigma = \frac{(0 - 3)^2 + (2 - 3)^2 + (4 - 3)^2 + (6 - 3)^2}{4} = 5 \bar x \sim N\left(3, \dfrac{5^2}{4}\right) \sqrt{\frac{\sigma^2}{n}} = \dfrac{5}{2}. \dfrac{\sqrt{5}}{\sqrt{2}}","['statistics', 'normal-distribution', 'sampling']"
25,Minimum of variance when sample is unbiased?,Minimum of variance when sample is unbiased?,,"Show that if an estimator $\hat\mu=a_1X_1 +a_2X_2 +\cdots+a_nX_n$, where $a_1, a_2,\ldots,a_n$ are constants, is unbiased, then its variance is minimum when  $a_1=a_2=\cdots=a_n=\frac{1}{n} \hat\mu=\bar X$. Ive tried subjecting it to $\sum a_i=1$, and I know that $\sum a_i^2$ is minimized by choosing $a_1=a_2=\cdots=a_n=1/n$), not sure what to do next. We are assuming all observations are iid.","Show that if an estimator $\hat\mu=a_1X_1 +a_2X_2 +\cdots+a_nX_n$, where $a_1, a_2,\ldots,a_n$ are constants, is unbiased, then its variance is minimum when  $a_1=a_2=\cdots=a_n=\frac{1}{n} \hat\mu=\bar X$. Ive tried subjecting it to $\sum a_i=1$, and I know that $\sum a_i^2$ is minimized by choosing $a_1=a_2=\cdots=a_n=1/n$), not sure what to do next. We are assuming all observations are iid.",,"['statistics', 'statistical-inference']"
26,Incorrect calculation? (Statistics homework),Incorrect calculation? (Statistics homework),,"I'm currently working on some statistics homework, and as you might guess from the title, I got the wrong result, and the reason why I'm writing it here is because I don't have a clue why it's wrong. The question is as follows: A student is driving from city A to B. On route to B there's 2 intersections where she will randomly choose which one to pass through. 1. At the first intersection she can choose between Bridge A or B in the proportion 3:1 (Meaning bridge A is chosen with probability 3/4). At bridge A there's a 0.5 probability of being delayed by 0 mins. and a 0.5 probability of being delayed by 10mins. At bridge B there's a 0.4 probability of being delayed by 5mins. and a 0.6 probability of being delayed by 7mins. 2. At the second intersection she can choose between road A or B in the proportion 2:1. At road A there's a 0.5 probability of being delayed by 1min. and a 0.5 probability of being delayed by 2 mins. At road B there's a 0.1 probability of being delayed by 3mins. and a 0.9 probability of being delayed by 9mins. Out from this answer the following: Let X be a stochastic variable which shows the student's total delay in minutes between city A to B. Calculate the following: $S_x$, $E(X)$, $Var(X)$ and $\sigma_X$ Let's just take the calculation for $S_X$ since I find that one to be the hardest. Since X is stochastic I can find that it has the following values: 1, 2, 3, 6, 7, 8, 8, 9, 10, 11, 12, 13, 14, 16, 19 In order to calculate $S_x$ I have to calculate the expected value which I did as: E(X)=$\frac{(1+2+3+6+7+8+8+9+10+11+12+13+14+16+19)}{15} = 9.2667$ Then I believe I calculate $S_X$ the following way: $S_X$ = $\sqrt(\frac{E(X)}{N})$ = 4.8634 Unless I'm using the wrong method (which I most likely am), then there's something missing in my book because what my teacher calls $S_X$ ""Support"". Which I can find nothing about, I also tried to look up standard deviation, which also turned out to be wrong.","I'm currently working on some statistics homework, and as you might guess from the title, I got the wrong result, and the reason why I'm writing it here is because I don't have a clue why it's wrong. The question is as follows: A student is driving from city A to B. On route to B there's 2 intersections where she will randomly choose which one to pass through. 1. At the first intersection she can choose between Bridge A or B in the proportion 3:1 (Meaning bridge A is chosen with probability 3/4). At bridge A there's a 0.5 probability of being delayed by 0 mins. and a 0.5 probability of being delayed by 10mins. At bridge B there's a 0.4 probability of being delayed by 5mins. and a 0.6 probability of being delayed by 7mins. 2. At the second intersection she can choose between road A or B in the proportion 2:1. At road A there's a 0.5 probability of being delayed by 1min. and a 0.5 probability of being delayed by 2 mins. At road B there's a 0.1 probability of being delayed by 3mins. and a 0.9 probability of being delayed by 9mins. Out from this answer the following: Let X be a stochastic variable which shows the student's total delay in minutes between city A to B. Calculate the following: $S_x$, $E(X)$, $Var(X)$ and $\sigma_X$ Let's just take the calculation for $S_X$ since I find that one to be the hardest. Since X is stochastic I can find that it has the following values: 1, 2, 3, 6, 7, 8, 8, 9, 10, 11, 12, 13, 14, 16, 19 In order to calculate $S_x$ I have to calculate the expected value which I did as: E(X)=$\frac{(1+2+3+6+7+8+8+9+10+11+12+13+14+16+19)}{15} = 9.2667$ Then I believe I calculate $S_X$ the following way: $S_X$ = $\sqrt(\frac{E(X)}{N})$ = 4.8634 Unless I'm using the wrong method (which I most likely am), then there's something missing in my book because what my teacher calls $S_X$ ""Support"". Which I can find nothing about, I also tried to look up standard deviation, which also turned out to be wrong.",,['statistics']
27,weighted sum of two i.i.d. random variables,weighted sum of two i.i.d. random variables,,"Suppose we know that $X_1$ and $X_2$ are two independently and identically distributed random variables. The distribution of $X_i$ ($i=1,2$) is $P$, and we have some constraints on $P$ that $$\mathbb{E} X_1 = 0$$ (zero-mean) and $$\mathbb{E} X_1^2 = 1$$ (variance is normalized). We denote the set of all feasible distributions as $\mathcal{D}$. Then, my question is about the function  $$f(w_1,w_2,a): = \sup_{P\in \mathcal{D}} \Pr(w_1X_1+w_2X_2\ge a)$$ where $w_1\ge 0$, $w_2\ge 0$ and $a> 0$. Q1 : Can we get an analytic solution of $f?$ Q2 : Can we know some properties of $f$? For example, it is easy to show that $f$ is monotonically decreasing as $a$ increases when $w_1$ and $w_2$ are fixed. I want to ask if $a$ is fixed, and $w_1+w_2$ is fixed, is $f$ (quasi-)convex (or concave, monotonic...) on $w_1$? For the two questions above, I also appreciate answers for another definition of the set $\mathcal{D}:=\{ P \mid \mathbb{E} X_1 = 0, P(|X_1|>1)=0 \text{ (bounded instead of restriction on variance)}\}.$ Thank you in advance.","Suppose we know that $X_1$ and $X_2$ are two independently and identically distributed random variables. The distribution of $X_i$ ($i=1,2$) is $P$, and we have some constraints on $P$ that $$\mathbb{E} X_1 = 0$$ (zero-mean) and $$\mathbb{E} X_1^2 = 1$$ (variance is normalized). We denote the set of all feasible distributions as $\mathcal{D}$. Then, my question is about the function  $$f(w_1,w_2,a): = \sup_{P\in \mathcal{D}} \Pr(w_1X_1+w_2X_2\ge a)$$ where $w_1\ge 0$, $w_2\ge 0$ and $a> 0$. Q1 : Can we get an analytic solution of $f?$ Q2 : Can we know some properties of $f$? For example, it is easy to show that $f$ is monotonically decreasing as $a$ increases when $w_1$ and $w_2$ are fixed. I want to ask if $a$ is fixed, and $w_1+w_2$ is fixed, is $f$ (quasi-)convex (or concave, monotonic...) on $w_1$? For the two questions above, I also appreciate answers for another definition of the set $\mathcal{D}:=\{ P \mid \mathbb{E} X_1 = 0, P(|X_1|>1)=0 \text{ (bounded instead of restriction on variance)}\}.$ Thank you in advance.",,"['probability', 'statistics']"
28,"How can I find a function that accurately matches a x,y scatter plot","How can I find a function that accurately matches a x,y scatter plot",,"I have a complicated piecewise function(see below), that I use in a spreadsheet to determine what a subcontractor get paid on a particular job. For example if a job is worth $$270 dollars the subcontractor gets paid 60% of that. Or if a job is worth $18 the subcontractor keeps 95% of the value. I have a few problems: The spread sheet gets really slow when there are a ton of these equations in there the equation isn't smooth, so a subcontractor who does a job worth $$269.00 makes $174.85, while a subcontractor who does a job worth $291.41 makes $174.85. This doesn't make any sense. Attempts: I tried to come up with a few equations, by graphing them in excel as a scatterplot and using the ""add trendline"" function. This failed and seemed like the function it came up with never very accurate. I also tried to just tinker with a formula for a long time and the best I could come up with was: -7.1257 * ln(x + 0.9999) + 100.6672 . It was pretty accurate at the low and some what accurate at the higher end, but is in accurate in the middle. Questions Do I just need to give up and find some constant based off of statistically the profit I want to make? ( about 20% on average) Is there a formula that really accurately fits this piecewise? -","I have a complicated piecewise function(see below), that I use in a spreadsheet to determine what a subcontractor get paid on a particular job. For example if a job is worth $$270 dollars the subcontractor gets paid 60% of that. Or if a job is worth $18 the subcontractor keeps 95% of the value. I have a few problems: The spread sheet gets really slow when there are a ton of these equations in there the equation isn't smooth, so a subcontractor who does a job worth $$269.00 makes $174.85, while a subcontractor who does a job worth $291.41 makes $174.85. This doesn't make any sense. Attempts: I tried to come up with a few equations, by graphing them in excel as a scatterplot and using the ""add trendline"" function. This failed and seemed like the function it came up with never very accurate. I also tried to just tinker with a formula for a long time and the best I could come up with was: -7.1257 * ln(x + 0.9999) + 100.6672 . It was pretty accurate at the low and some what accurate at the higher end, but is in accurate in the middle. Questions Do I just need to give up and find some constant based off of statistically the profit I want to make? ( about 20% on average) Is there a formula that really accurately fits this piecewise? -",,"['statistics', 'mathematical-modeling']"
29,"Calculating variance of estimated intercept parameter, $\hat\beta_0$","Calculating variance of estimated intercept parameter,",\hat\beta_0,"I have the following sample : $$ \begin{array}{c|lr} X&80&100&120&140&160&180&200&220&240&260\\ \hline Y & 70 &65&90&95&110&115&120&140&155&150 \\ \end{array} $$ Now i have to calculate $\mathbb V(\hat\beta_0)=\sigma^2[\frac{1}{n}+\frac{\bar X^2}{\sum(X_i-\bar X)^2}]$ here, X is independent  and Y response variable. $\beta_0$ intercept parameter $\sigma^2$ population variance But i have my sample data. How can i calculate $\sigma^2$ ?","I have the following sample : $$ \begin{array}{c|lr} X&80&100&120&140&160&180&200&220&240&260\\ \hline Y & 70 &65&90&95&110&115&120&140&155&150 \\ \end{array} $$ Now i have to calculate $\mathbb V(\hat\beta_0)=\sigma^2[\frac{1}{n}+\frac{\bar X^2}{\sum(X_i-\bar X)^2}]$ here, X is independent  and Y response variable. $\beta_0$ intercept parameter $\sigma^2$ population variance But i have my sample data. How can i calculate $\sigma^2$ ?",,"['statistics', 'regression']"
30,Calculating likelihood of event based on retrospective analysis,Calculating likelihood of event based on retrospective analysis,,"I have a simple dataset consisting of the dates/times at which certain medications were taken by a patient. By looking retrospectively I'd like to make a best guess estimate as to which medication that same patient is most likely to be taking at any given date/time in the future. Essentially I'm looking to base my estimate on the following factors: Time : medication is usually taken at fairly regulated intervals. If a certain medication is always taken at a specific time of day, and we're close to that time, increase the likelihood that that medication will be taken. Day : similar to time, certain medications may only be taken on certain days of the week. This should help determine the probability of a given medication being taken. However, it's worth nothing that this should not strictly exclude certain medications (i.e: don't exclude a medication simply because it's never been taken on a given day before). I'm fairly green when it comes to statistical analysis and any kind of mathematics in general, but I'm posting here in the hopes that someone can point me in the right direction. EDIT : I've made changes to my question to hopefully offer a bit more clarity at Xoque55's request","I have a simple dataset consisting of the dates/times at which certain medications were taken by a patient. By looking retrospectively I'd like to make a best guess estimate as to which medication that same patient is most likely to be taking at any given date/time in the future. Essentially I'm looking to base my estimate on the following factors: Time : medication is usually taken at fairly regulated intervals. If a certain medication is always taken at a specific time of day, and we're close to that time, increase the likelihood that that medication will be taken. Day : similar to time, certain medications may only be taken on certain days of the week. This should help determine the probability of a given medication being taken. However, it's worth nothing that this should not strictly exclude certain medications (i.e: don't exclude a medication simply because it's never been taken on a given day before). I'm fairly green when it comes to statistical analysis and any kind of mathematics in general, but I'm posting here in the hopes that someone can point me in the right direction. EDIT : I've made changes to my question to hopefully offer a bit more clarity at Xoque55's request",,"['probability', 'statistics', 'parameter-estimation']"
31,Statistics question: appropriate test of significance?,Statistics question: appropriate test of significance?,,"The following question came to mind while I was playing tennis. Even when I'm taking a break, I can't turn the math brain off. I'm sure this is a simple question for the statisticians here. Say a tennis player wins 100 out of 200 points in a match. The player faces 30 break points in the match (not serving so well, apparently), and saves 20 of them. Is there evidence that the player plays ""better"" when facing break point? My thought (from my limited knowledge of statistics) is to compare the ratio $\frac{20}{30}$ of break-points-against won to the ratio $\frac{80}{170}$ of all other points won, with the expectation for each ratio being 50% $(=\frac{100}{200})$. We could use Pearson's chi-squared test with one degree of freedom. Is this an appropriate approach or am I missing something? Thanks for your input!","The following question came to mind while I was playing tennis. Even when I'm taking a break, I can't turn the math brain off. I'm sure this is a simple question for the statisticians here. Say a tennis player wins 100 out of 200 points in a match. The player faces 30 break points in the match (not serving so well, apparently), and saves 20 of them. Is there evidence that the player plays ""better"" when facing break point? My thought (from my limited knowledge of statistics) is to compare the ratio $\frac{20}{30}$ of break-points-against won to the ratio $\frac{80}{170}$ of all other points won, with the expectation for each ratio being 50% $(=\frac{100}{200})$. We could use Pearson's chi-squared test with one degree of freedom. Is this an appropriate approach or am I missing something? Thanks for your input!",,['statistics']
32,Probability and statistics involving infinite series.,Probability and statistics involving infinite series.,,"For a certain discrete random variable on the non-negative integers, the probability function satisfies the relationship $P(0)=P(1)$ and $P(k+1)={1\over k}P(k)$ for $k=1,2,3,\dots$ find $P(0)$. Immediately looking at this I can see P(2)=P(1)=P(0) and P(3)=(1/2)P(2) and P(4)=(1/6)P(2) and P(k+1)= (1/(k!))P(2). But I'm still not sure how to approach this.","For a certain discrete random variable on the non-negative integers, the probability function satisfies the relationship $P(0)=P(1)$ and $P(k+1)={1\over k}P(k)$ for $k=1,2,3,\dots$ find $P(0)$. Immediately looking at this I can see P(2)=P(1)=P(0) and P(3)=(1/2)P(2) and P(4)=(1/6)P(2) and P(k+1)= (1/(k!))P(2). But I'm still not sure how to approach this.",,"['statistics', 'probability-distributions']"
33,"The number of nuts in a package of ""Premium Cashews"" is normally distributed with a mean of 433 and a standard deviation of 6 nuts.","The number of nuts in a package of ""Premium Cashews"" is normally distributed with a mean of 433 and a standard deviation of 6 nuts.",,"The number of nuts in a package of ""Premium Cashews"" is normally distributed with a mean of 433 and a standard deviation of 6 nuts. Packages with fewer than 420 nuts or more than 445 nuts will be rejected by quality control. a) what is the probability that a package selected at random has at most 430 nuts? Attempt : (430.5 - 433) / 6 = -0.42 using area under normal distribution curve. -0.42 = 0.3372 therefore there is a 33.72% chance? Am i right? Thanks in advance!","The number of nuts in a package of ""Premium Cashews"" is normally distributed with a mean of 433 and a standard deviation of 6 nuts. Packages with fewer than 420 nuts or more than 445 nuts will be rejected by quality control. a) what is the probability that a package selected at random has at most 430 nuts? Attempt : (430.5 - 433) / 6 = -0.42 using area under normal distribution curve. -0.42 = 0.3372 therefore there is a 33.72% chance? Am i right? Thanks in advance!",,"['statistics', 'random-variables', 'data-analysis']"
34,Where do the definitions in statistics come from? [duplicate],Where do the definitions in statistics come from? [duplicate],,"This question already has answers here : Motivation behind standard deviation? (6 answers) Closed 10 years ago . The most statistics I ever took was a few lessons on it back in high school. What always bothered me is how arbitrary the definitions seemed. For instance, I remember having trouble with the definition of standard deviation. The standard deviation of a set of values $X$ is the square root of the average of the squared of the differences between each value in $X$ and the average of $X$. At school, standard deviation was never given any more precise definition than ""a number that gives you a rough idea how 'diffuse' the dataset is"". While I can see in a very approximate way that this is basically correct, it's a long way from how definitions of concepts in math are usually explained. Usually there's a precise notion that we're trying to capture, and a clear explanation as to how our definition captures that notion. But here, when I asked for further information, I was told things like ""you square the differences to make them positive"", when what I was hoping for was something like: A specific real-world concept that the definition captures. A class of problems in which the definition arises naturally. A specific mathematical property that we would like to have, which leads necessarily to this particular definition. Is there any rigorous basis for the definitions in statistics, or do we genuinely just make up formulae that kinda sorta get us something like what we're trying to calculate? Have I just never seen statistics as it really is, or is it actually very different to every other field of mathematics? If the former, can you recommend a book that explains statistics in the way I'd like?","This question already has answers here : Motivation behind standard deviation? (6 answers) Closed 10 years ago . The most statistics I ever took was a few lessons on it back in high school. What always bothered me is how arbitrary the definitions seemed. For instance, I remember having trouble with the definition of standard deviation. The standard deviation of a set of values $X$ is the square root of the average of the squared of the differences between each value in $X$ and the average of $X$. At school, standard deviation was never given any more precise definition than ""a number that gives you a rough idea how 'diffuse' the dataset is"". While I can see in a very approximate way that this is basically correct, it's a long way from how definitions of concepts in math are usually explained. Usually there's a precise notion that we're trying to capture, and a clear explanation as to how our definition captures that notion. But here, when I asked for further information, I was told things like ""you square the differences to make them positive"", when what I was hoping for was something like: A specific real-world concept that the definition captures. A class of problems in which the definition arises naturally. A specific mathematical property that we would like to have, which leads necessarily to this particular definition. Is there any rigorous basis for the definitions in statistics, or do we genuinely just make up formulae that kinda sorta get us something like what we're trying to calculate? Have I just never seen statistics as it really is, or is it actually very different to every other field of mathematics? If the former, can you recommend a book that explains statistics in the way I'd like?",,['statistics']
35,Finding aggregate score from incomplete data,Finding aggregate score from incomplete data,,"$$ \begin{align} n & = \text{number of reviews}\\ x & = \text{review score}\\ \bar{x} & = \text{aggregate score} \end{align} $$ I have a specified number of reviews for a product, I have the review scores themselves, and I want to calculate the average. $$\frac{x_1 + x_2 + x_3 + \dots + x_n}{n} = \bar{x}$$ This is how I would find the average/aggregate score given all of the individual review scores. What if I have incomplete data. Say I only know the number of reviews ($n$), the latest review ($x$), could I calculate the aggregate accurately provided only those two bits of information? $$\frac{\bar{x}_\text{old} \cdot n_\text{old} + x}{n_\text{new}} = \bar{x}_\text{new}$$ Is this right? Sorry I'm not sure how best to articulate my question. I'm trying to implement a review scoring system for my business website and instead of keeping SQL records of every single review ever made, I'd like to save space/query time and just keep track of $n$ and $\bar{x}$. For example: Review Scores = 4, 2, 5, 3, 6, 7 Number of Reviews = 6 Average = 4.5 If a new review with a score of 5 happens, the average using the first equation yields $4.571$, but I don't want to keep record of every review. Using the second formula gives $4.571$ as well, but I'm working with small $n$; is this method accurate beyond as well? tl;dr: $$\frac{\frac{x_1+x_2+x_3+\dots+x_n}{n} \cdot n+x_{n+1}}{n+1} = \frac{x_1+x_2+x_3+\dots+x_n+x_{n+1}}{n+1} = \bar{x} ?$$","$$ \begin{align} n & = \text{number of reviews}\\ x & = \text{review score}\\ \bar{x} & = \text{aggregate score} \end{align} $$ I have a specified number of reviews for a product, I have the review scores themselves, and I want to calculate the average. $$\frac{x_1 + x_2 + x_3 + \dots + x_n}{n} = \bar{x}$$ This is how I would find the average/aggregate score given all of the individual review scores. What if I have incomplete data. Say I only know the number of reviews ($n$), the latest review ($x$), could I calculate the aggregate accurately provided only those two bits of information? $$\frac{\bar{x}_\text{old} \cdot n_\text{old} + x}{n_\text{new}} = \bar{x}_\text{new}$$ Is this right? Sorry I'm not sure how best to articulate my question. I'm trying to implement a review scoring system for my business website and instead of keeping SQL records of every single review ever made, I'd like to save space/query time and just keep track of $n$ and $\bar{x}$. For example: Review Scores = 4, 2, 5, 3, 6, 7 Number of Reviews = 6 Average = 4.5 If a new review with a score of 5 happens, the average using the first equation yields $4.571$, but I don't want to keep record of every review. Using the second formula gives $4.571$ as well, but I'm working with small $n$; is this method accurate beyond as well? tl;dr: $$\frac{\frac{x_1+x_2+x_3+\dots+x_n}{n} \cdot n+x_{n+1}}{n+1} = \frac{x_1+x_2+x_3+\dots+x_n+x_{n+1}}{n+1} = \bar{x} ?$$",,"['algebra-precalculus', 'statistics', 'computer-science']"
36,How do I 'reverse engineer' the standard deviation?,How do I 'reverse engineer' the standard deviation?,,"My problem is fairly concrete and direct. My company loves to do major business decisions based on many reports available on the media. These reports relates how our products are fairing in comparison to the competitor's offerings. The latest report had these scores (as percentages): Input values (%) 73.5, 16.34, 1.2, 1.15, 0.97, 0.94, 0.9, 0.89, 0.81, 0.31 Our product in the 'long' tail of this list. I argued with them that besides spots #2 and #1 all the other following the tail are on a 'stand', since, probably, the standard deviation will be much bigger that the points that separates everyone in the tail. So the question is: How may I calculate the standard deviation having only these percentual values available?","My problem is fairly concrete and direct. My company loves to do major business decisions based on many reports available on the media. These reports relates how our products are fairing in comparison to the competitor's offerings. The latest report had these scores (as percentages): Input values (%) 73.5, 16.34, 1.2, 1.15, 0.97, 0.94, 0.9, 0.89, 0.81, 0.31 Our product in the 'long' tail of this list. I argued with them that besides spots #2 and #1 all the other following the tail are on a 'stand', since, probably, the standard deviation will be much bigger that the points that separates everyone in the tail. So the question is: How may I calculate the standard deviation having only these percentual values available?",,"['probability', 'statistics', 'probability-distributions', 'statistical-inference', 'data-analysis']"
37,Generating points from a standard Gaussian,Generating points from a standard Gaussian,,"I'm new to Gaussian distributions and I'm trying to generate say, $ N$ points from a $ M$ dimensional standard gaussian. What does this mean? How would I do this in matlab?","I'm new to Gaussian distributions and I'm trying to generate say, $ N$ points from a $ M$ dimensional standard gaussian. What does this mean? How would I do this in matlab?",,"['statistics', 'matlab', 'signal-processing', 'research']"
38,What approximately is the probability that she will lose 2 or more pieces of luggage,What approximately is the probability that she will lose 2 or more pieces of luggage,,"According to an airline report, roughly 1 piece of luggage out of every 2000 that are checked is lost. Suppose that a frequent-flying businesswoman is checking 1200 bags over the course of the next year. What approximately is the probability that she will lose 2 or more pieces of luggage","According to an airline report, roughly 1 piece of luggage out of every 2000 that are checked is lost. Suppose that a frequent-flying businesswoman is checking 1200 bags over the course of the next year. What approximately is the probability that she will lose 2 or more pieces of luggage",,['probability']
39,Comparing annualised volatility from monthly and annual data,Comparing annualised volatility from monthly and annual data,,"I fear there is a very simple answer to this question and its killing me that I can't see it. I am interested in calculating historical volatility: I have monthly index values starting in Jan 2005 and ending in August 2013. I have calculated Ln(m+1/m) for each month. I have also calculated the year-on-year returns (8 data points in total for my data set). If I calculate the standard deviation of the monthly return series I get 1.74%. On an annualised basis, I make it 1.74%*sqrt(12)=6.01%.  The standard deviation of the annual series is 16.25% (i.e the standard deviation of my 8 data points). can someone please give me an explanation as to why the annualised volatility values are so different? I know the sqrt(t) rule is an approximation and subject to constraints such as i.i.d and no autocorreclation of the series but should it not be the case that: standard deviation of monthly returns*sqrt(12) approx = standard deviation of annual returns Thanks in advance for any help!!","I fear there is a very simple answer to this question and its killing me that I can't see it. I am interested in calculating historical volatility: I have monthly index values starting in Jan 2005 and ending in August 2013. I have calculated Ln(m+1/m) for each month. I have also calculated the year-on-year returns (8 data points in total for my data set). If I calculate the standard deviation of the monthly return series I get 1.74%. On an annualised basis, I make it 1.74%*sqrt(12)=6.01%.  The standard deviation of the annual series is 16.25% (i.e the standard deviation of my 8 data points). can someone please give me an explanation as to why the annualised volatility values are so different? I know the sqrt(t) rule is an approximation and subject to constraints such as i.i.d and no autocorreclation of the series but should it not be the case that: standard deviation of monthly returns*sqrt(12) approx = standard deviation of annual returns Thanks in advance for any help!!",,"['statistics', 'finance', 'standard-deviation', 'data-analysis']"
40,Maximum likelihood estimator of minimum function with exponential RV and a random number,Maximum likelihood estimator of minimum function with exponential RV and a random number,,"I'm having some problems with the following assignment: Let $X_1, X_2, ...,, X_n$ be samples from a exponential distribution with parameter $\lambda$, and let $c_1, c_2, ..., c_n$ be a sequence of positive numbers. Define \begin{align*} Y_i = \min(X_i, c_i) \quad \text{and} \quad \Delta_i = \textbf{1} \{ Y_i = X_i \}. \end{align*} Determine the likelihood of the observed data $(Y_1, \Delta_1), (Y_2, \Delta_2), ....,(Y_n, \Delta_n)$. My main problems are that I do not know how to calculate the pdf of the $Y_i$, and even if knowing those pdf's, I'm not sure how to use the $\Delta_i$ in the correct way. What I've done so far: I've first followed this post for calculating the pdf of $Y_i$: How to find the pdf of [min(RV.1,RV.2)]/RV.2 So, first notice that $Y_i$ is a random variable supported on $[ 0, c_i]$. For $0 \leq y < c_i$ we see: \begin{align*} P(Y_i \leq y) = P(\min(X_i, c_i) \leq y) = P(X_i \leq y) = 1 - e^{-\lambda \; y}  \end{align*} For $y = c_i$ we see: \begin{align*} P(Y_i = y) = P(\min(X_i, c_i) = y) = P(X_i > y) = e^{- \lambda \; c_i} \end{align*} Let $F_{Y_i}$ denote the distribution function of $Y_i$, then we see: \begin{align*} F_{Y_i}(y) &= 1 - e^{-\lambda \; y}  &0 \leq y < c_i \\ F_{Y_i}(y) &= 1 &y = c_i \end{align*} and \begin{align*} f_{Y_i}(y) = \lambda e^{- \lambda y}, \quad 0 \leq y < c_i \end{align*} which obviously does not integrate to 1. Disregarding that for the moment, it seems logical to me that the likelihoodfunction $\mathcal{L}(\lambda)$ then becomes \begin{align*} \mathcal{L}(\lambda) = \prod_{i = 1}^n \left( \Delta_i \lambda e^{- \lambda \; Y_i}  + (1 - \Delta_i) e^{- \lambda \; c_i} \right). \end{align*} Logging this statement we get \begin{align*} \log \mathcal{L}(\lambda) = \sum_{i = 1}^n \log\left( \Delta_i \lambda e^{- \lambda \; Y_i}  + (1 - \Delta_i) e^{- \lambda \; c_i}  \right) \end{align*} and calculating the derivative gives us \begin{align*} \frac{ \mathrm{d} \log \mathcal{L}(\lambda)}{ \mathrm{d} \lambda} = \sum_{i = 1}^n \frac{\Delta_i e^{\lambda \; c_i}( \lambda Y_i - 1) - c_i (\Delta_i - 1)e^{Y_i \; \lambda} }{(\Delta_i - 1)\lambda e^{\lambda \; Y_i}  - \Delta_i \lambda e^{\lambda \; c_i}}. \end{align*} But as far as I'm concerned, this brings us nowhere. If someone could point out where I went wrong, I'd really appreciate it. Thanks in advance for any replies!","I'm having some problems with the following assignment: Let $X_1, X_2, ...,, X_n$ be samples from a exponential distribution with parameter $\lambda$, and let $c_1, c_2, ..., c_n$ be a sequence of positive numbers. Define \begin{align*} Y_i = \min(X_i, c_i) \quad \text{and} \quad \Delta_i = \textbf{1} \{ Y_i = X_i \}. \end{align*} Determine the likelihood of the observed data $(Y_1, \Delta_1), (Y_2, \Delta_2), ....,(Y_n, \Delta_n)$. My main problems are that I do not know how to calculate the pdf of the $Y_i$, and even if knowing those pdf's, I'm not sure how to use the $\Delta_i$ in the correct way. What I've done so far: I've first followed this post for calculating the pdf of $Y_i$: How to find the pdf of [min(RV.1,RV.2)]/RV.2 So, first notice that $Y_i$ is a random variable supported on $[ 0, c_i]$. For $0 \leq y < c_i$ we see: \begin{align*} P(Y_i \leq y) = P(\min(X_i, c_i) \leq y) = P(X_i \leq y) = 1 - e^{-\lambda \; y}  \end{align*} For $y = c_i$ we see: \begin{align*} P(Y_i = y) = P(\min(X_i, c_i) = y) = P(X_i > y) = e^{- \lambda \; c_i} \end{align*} Let $F_{Y_i}$ denote the distribution function of $Y_i$, then we see: \begin{align*} F_{Y_i}(y) &= 1 - e^{-\lambda \; y}  &0 \leq y < c_i \\ F_{Y_i}(y) &= 1 &y = c_i \end{align*} and \begin{align*} f_{Y_i}(y) = \lambda e^{- \lambda y}, \quad 0 \leq y < c_i \end{align*} which obviously does not integrate to 1. Disregarding that for the moment, it seems logical to me that the likelihoodfunction $\mathcal{L}(\lambda)$ then becomes \begin{align*} \mathcal{L}(\lambda) = \prod_{i = 1}^n \left( \Delta_i \lambda e^{- \lambda \; Y_i}  + (1 - \Delta_i) e^{- \lambda \; c_i} \right). \end{align*} Logging this statement we get \begin{align*} \log \mathcal{L}(\lambda) = \sum_{i = 1}^n \log\left( \Delta_i \lambda e^{- \lambda \; Y_i}  + (1 - \Delta_i) e^{- \lambda \; c_i}  \right) \end{align*} and calculating the derivative gives us \begin{align*} \frac{ \mathrm{d} \log \mathcal{L}(\lambda)}{ \mathrm{d} \lambda} = \sum_{i = 1}^n \frac{\Delta_i e^{\lambda \; c_i}( \lambda Y_i - 1) - c_i (\Delta_i - 1)e^{Y_i \; \lambda} }{(\Delta_i - 1)\lambda e^{\lambda \; Y_i}  - \Delta_i \lambda e^{\lambda \; c_i}}. \end{align*} But as far as I'm concerned, this brings us nowhere. If someone could point out where I went wrong, I'd really appreciate it. Thanks in advance for any replies!",,"['probability', 'statistics', 'probability-distributions']"
41,Probability density function of a summation of continuous random variables,Probability density function of a summation of continuous random variables,,"Let $Z_{i} = \tau + X_{i}$, where $X_{i}$ is a exponential random variable ($X_i \sim \varepsilon(\lambda)$),  $0<\tau, \lambda < \infty$ Assume $X_i$ are independent random variables. Suppose $T_{n} = Z_{1} + Z_{2} + \cdots + Z_{n}$, find the PDF of $T_{n}$, $h(t_n)$. My approach: $T_{n} = \sum_{i=1}^{n}\left ( \tau + X_{i}   \right )$ $T_{n} = n\tau+\sum_{i=1}^{n}\left (X_{i}  \right )$ Let $Y$ be $\sum_{i=1}^{n}\left (X_{i}  \right )$, I find the PDF of $Y$, $g(y)$ $T_{n} = n\tau+ Y$ $Y = T_{n} - n\tau$ Using the Jacobian of transformation, I find PDF of $T_{n}$: $h(t_n) = \left | \frac{\mathrm{d} }{\mathrm{d} t_{n}}(t_{n} - n\tau) \right |\times g(t_{n} - n\tau)$ I am stuck on step 3, as I have no idea how to find the PDF of $\sum_{i=1}^{n}\left (X_{i}  \right )$. Any ideas? By the way, if there's any wrong with my approach, please say so. Thanks a lot.","Let $Z_{i} = \tau + X_{i}$, where $X_{i}$ is a exponential random variable ($X_i \sim \varepsilon(\lambda)$),  $0<\tau, \lambda < \infty$ Assume $X_i$ are independent random variables. Suppose $T_{n} = Z_{1} + Z_{2} + \cdots + Z_{n}$, find the PDF of $T_{n}$, $h(t_n)$. My approach: $T_{n} = \sum_{i=1}^{n}\left ( \tau + X_{i}   \right )$ $T_{n} = n\tau+\sum_{i=1}^{n}\left (X_{i}  \right )$ Let $Y$ be $\sum_{i=1}^{n}\left (X_{i}  \right )$, I find the PDF of $Y$, $g(y)$ $T_{n} = n\tau+ Y$ $Y = T_{n} - n\tau$ Using the Jacobian of transformation, I find PDF of $T_{n}$: $h(t_n) = \left | \frac{\mathrm{d} }{\mathrm{d} t_{n}}(t_{n} - n\tau) \right |\times g(t_{n} - n\tau)$ I am stuck on step 3, as I have no idea how to find the PDF of $\sum_{i=1}^{n}\left (X_{i}  \right )$. Any ideas? By the way, if there's any wrong with my approach, please say so. Thanks a lot.",,"['probability', 'statistics']"
42,Find the value of theta.,Find the value of theta.,,"Let $X$ have the density function $f(x) = \dfrac{3x^2}{\theta^3}$ for $0 < x < \theta$, and $f(x) = 0$ otherwise. If $P\{X > 1\} = 7/8$, find the value of $\theta$. I don't know","Let $X$ have the density function $f(x) = \dfrac{3x^2}{\theta^3}$ for $0 < x < \theta$, and $f(x) = 0$ otherwise. If $P\{X > 1\} = 7/8$, find the value of $\theta$. I don't know",,['probability']
43,Do the algebraic properties of the exponential and log functions specify them uniquely in probability theory?,Do the algebraic properties of the exponential and log functions specify them uniquely in probability theory?,,"I come from a physics background and in classical mechanics, we construct a Hamiltonian function whose partial derivatives generates a vector field, two independent systems are assigned a total Hamiltonian which is the algebraic sum of the Hamiltonians for the isolated systems. In statistical mechanics, Liouville's theorem requires that in equilibrium, the probability that a system's dynamical variables have specific values is a function of the value Hamiltonian for those specific values. So the standard probability theory argument that the probability to measure state a in system A and state b in system B for independent systems A and B is the product of the individual probabilities: $Pr(a \text{ & } b) = Pr(a)*Pr(b)$. This is essentially a quick derivation of the maxwell-boltzmann distribution. I've seen this argument about probability theory in several places, and can easily see that the exponential satisfies the requirement. So I think my question is essentially this, does $$ f(a+b) = f(a)*f(b) \\ f(x) > 0 \text{ for all real x} \\ $$ uniquely specify that $f(x)=Ae^{cx}$? Or is there some other consideration that I must be overlooking?","I come from a physics background and in classical mechanics, we construct a Hamiltonian function whose partial derivatives generates a vector field, two independent systems are assigned a total Hamiltonian which is the algebraic sum of the Hamiltonians for the isolated systems. In statistical mechanics, Liouville's theorem requires that in equilibrium, the probability that a system's dynamical variables have specific values is a function of the value Hamiltonian for those specific values. So the standard probability theory argument that the probability to measure state a in system A and state b in system B for independent systems A and B is the product of the individual probabilities: $Pr(a \text{ & } b) = Pr(a)*Pr(b)$. This is essentially a quick derivation of the maxwell-boltzmann distribution. I've seen this argument about probability theory in several places, and can easily see that the exponential satisfies the requirement. So I think my question is essentially this, does $$ f(a+b) = f(a)*f(b) \\ f(x) > 0 \text{ for all real x} \\ $$ uniquely specify that $f(x)=Ae^{cx}$? Or is there some other consideration that I must be overlooking?",,"['probability', 'statistics', 'probability-distributions', 'statistical-mechanics']"
44,statistics - expected and probable numbers,statistics - expected and probable numbers,,"The task: The number of oil tankers, arriving at a certain refinery on one day, follow a Poisson distribution with the parameter $u = 2$. The present harbour facilities can serve 3 oil tankers a day. If more than 3 oil tankers arrive on a given day, the additional tankers (in excess of the 3) are sent to another refinery Question 1: What is the probability that oil tankers are sent to another refinery? Question 2: By how much should the capacity of the refinery harbour be extended, if it is desired that on a given day the probability of sending away arriving oil tankers does not exceed 0.05? 0.01? Question 3: What is the expected number of oil tankers arriving on a given day? Question 4 : What is the most probable number of oil tankers arriving on a given day? Question 5: What is the expected number of oil tankers served on a given day? Question 6: What is the expected number of oil tankers sent on to other refineries on a given day? I've tried to answer Question 1 and 3 and got stuck on question 4. Q 1: We let X be the value of oil tankers arriving each day. Then we find the expected value of $P(X>3) = 0.35277$ (sorry don't know how to do the fancy equation stuff, but if the answer is correct I assume I've done that one right) Q 3: The expected number of a Poisson distribution with paramter $u = u$ therefor the expected number of oil tankers arriving on a given day is 2 right? Q 4: I belive it's something to do with the poisson formula right? I've gotten so far: $$\frac{2^k e^{-k}} k = P(k)$$ where i find P(0),P(1),P(2),P(3) etc? this is how far I've gotten. and I don't know what to do with these numbers as they don't make sense by themselves. Obviously there is no a 100% probability of 0 oil tankers arriving daily when the mean is 2 As for Question 2. I've tried using the same method I did for question 1. But replacing the maximum number of tankers it can serve to $y%, and replacing 3 with $y$ in the entire equation then solving for it gives 0.05. but that didn't work out at all. :/ and I am completly blank on question 5 and 6 :( hope you can help me out! Thanks in advance geniuses!","The task: The number of oil tankers, arriving at a certain refinery on one day, follow a Poisson distribution with the parameter $u = 2$. The present harbour facilities can serve 3 oil tankers a day. If more than 3 oil tankers arrive on a given day, the additional tankers (in excess of the 3) are sent to another refinery Question 1: What is the probability that oil tankers are sent to another refinery? Question 2: By how much should the capacity of the refinery harbour be extended, if it is desired that on a given day the probability of sending away arriving oil tankers does not exceed 0.05? 0.01? Question 3: What is the expected number of oil tankers arriving on a given day? Question 4 : What is the most probable number of oil tankers arriving on a given day? Question 5: What is the expected number of oil tankers served on a given day? Question 6: What is the expected number of oil tankers sent on to other refineries on a given day? I've tried to answer Question 1 and 3 and got stuck on question 4. Q 1: We let X be the value of oil tankers arriving each day. Then we find the expected value of $P(X>3) = 0.35277$ (sorry don't know how to do the fancy equation stuff, but if the answer is correct I assume I've done that one right) Q 3: The expected number of a Poisson distribution with paramter $u = u$ therefor the expected number of oil tankers arriving on a given day is 2 right? Q 4: I belive it's something to do with the poisson formula right? I've gotten so far: $$\frac{2^k e^{-k}} k = P(k)$$ where i find P(0),P(1),P(2),P(3) etc? this is how far I've gotten. and I don't know what to do with these numbers as they don't make sense by themselves. Obviously there is no a 100% probability of 0 oil tankers arriving daily when the mean is 2 As for Question 2. I've tried using the same method I did for question 1. But replacing the maximum number of tankers it can serve to $y%, and replacing 3 with $y$ in the entire equation then solving for it gives 0.05. but that didn't work out at all. :/ and I am completly blank on question 5 and 6 :( hope you can help me out! Thanks in advance geniuses!",,['statistics']
45,A limit technique in shifting the mean of normal distribution,A limit technique in shifting the mean of normal distribution,,"This question arises from showing the existence of some strange admissible estimators in the theory of point estimate in statistics. Let $\varphi(x)=\exp(-{1\over 2}x^2)$, then show for any non-negative measurable $r(\cdot)$ with $\{r=0\}$ of zero Lebesgue measure  that $$\frac{\int_0^\infty r(x)\varphi(x-\theta)\ dx}{\int_{-\infty}^0 \varphi(x-\theta)\ dx}\to \infty$$ as $\theta \to +\infty$. The conclusion shall be obvious for constant $r(\cdot)$, while if we let $r(x)=\exp(-{1\over 2}x^2)$, we get the limit $0/0$-type, and is hard to expect showing one is bounded from below and one tends to 0.","This question arises from showing the existence of some strange admissible estimators in the theory of point estimate in statistics. Let $\varphi(x)=\exp(-{1\over 2}x^2)$, then show for any non-negative measurable $r(\cdot)$ with $\{r=0\}$ of zero Lebesgue measure  that $$\frac{\int_0^\infty r(x)\varphi(x-\theta)\ dx}{\int_{-\infty}^0 \varphi(x-\theta)\ dx}\to \infty$$ as $\theta \to +\infty$. The conclusion shall be obvious for constant $r(\cdot)$, while if we let $r(x)=\exp(-{1\over 2}x^2)$, we get the limit $0/0$-type, and is hard to expect showing one is bounded from below and one tends to 0.",,"['real-analysis', 'probability', 'statistics']"
46,Calculate percentage with Days,Calculate percentage with Days,,"I'm programming a projectscheduler and want to calculate the percentage of finishing by looping through all the subprojects and calculate their weight and how many percent is done by now and then summate them to a final result for the mainproject. The structur would look similar to this: Project 1 Project 1.1 (5 days,30% done) Project 1.2 Project 1.2.1 (59 days, 52% done) Proejct 1.2.2 (10 days, 100% done) So now I would start on the lowest layer (1.2.x) to calculate the weight of 1.2. Then I would do the same for the layer 1.x to get the final result. But I think calculating with days is not very accurate. Somebody have a pointer or a hint how I could calculate such an operation the best way to get as much accurancy as possible? I thought about calculating with 24 hours and not with days. Cheers","I'm programming a projectscheduler and want to calculate the percentage of finishing by looping through all the subprojects and calculate their weight and how many percent is done by now and then summate them to a final result for the mainproject. The structur would look similar to this: Project 1 Project 1.1 (5 days,30% done) Project 1.2 Project 1.2.1 (59 days, 52% done) Proejct 1.2.2 (10 days, 100% done) So now I would start on the lowest layer (1.2.x) to calculate the weight of 1.2. Then I would do the same for the layer 1.x to get the final result. But I think calculating with days is not very accurate. Somebody have a pointer or a hint how I could calculate such an operation the best way to get as much accurancy as possible? I thought about calculating with 24 hours and not with days. Cheers",,"['statistics', 'recursion']"
47,expectation calculation in probability and statistics,expectation calculation in probability and statistics,,"2 four-sided dice are rolled. X = number of odd dice Y = number of even dice Z = number of dice showing 1 or 2 So each of X, Y, Z only takes on the values 0, 1, 2. (a)  joint p.m.f. of (X,Y),  joint p.m.f. of (X,Z). You can give your answers in the form of 3 by 3 tables. (b) Are X and Y independent? Are X and Z independent? (c) Compute E(XY ) and E(XZ). WHAT I TRIED IS - The faces on a four-sided die are labeled with the numbers 1, 2, 3, and 4. Upon throwing the dice, each one will land with exactly one of its faces upwards. You will therefore see two numbers, one for each die. Suppose, for the sake of example, that you see a 2 and a 4. Both of these are even numbers. None of them are odd numbers. Let's count: no dice show an odd number, two dice show even numbers, and one die shows a number in the set {1,2}. Therefore, for this throw,, X=0, Y=2, and Z=1. There are 16 combinations (1,1) (1,2)(1,3) (1,4)(2,1)(2,2)(2,3)(2,4) ... (4,1 )(4,2 )(4,3 )(4,4 ) There are only 2 case when no dice show an odd number, two dice show even numbers, and one die shows a number in the set {1,2}. ---> when (2,2) (2,4 ) occurs. Here X= 0, Y =2, Z =1 Both odd and one is in {1,2} is --> (1,1)(1,3)(3,1) Here, X= 2, Y =0, Z =1 Just as we have to in the case with one discrete random variable, in order to find the ""joint probability distribution"" of X and Y, we first need to define the support of X and Y. X, Y and Z combinations are -  $$\left\{\raise{5ex}{(1,1), (1,2), (1,3), (1,4), \\(2,1), (2,2), (2,3), (2,4),\\ (3,1), (3,2), (3,3), (3,4), \\(4,1), (4,2), (4,3), (4,4)}\right\}$$ Out of this, the X, Y, Z values are  respectively, $${(2,0,1) , (1,1,2) , (2,0,1),(1,1,1),\\(1,1,2),(0,2,1),(1,1,1),(0,2,1),\\(2,0,1),(1,1,1),(2,0,0),(1,1,0),\\(1,1,1),(0,2,1),(1,1,0),(0,2,0)}$$ b) The random variables X and Y are independent if and only if: P(X= x, Y = y) = P(X = x) × P(Y = y) for allx∈S1,y∈S2. if we again take a look back at the representation of our joint p.m.f. in tabular form, you might notice that the following holds true: P(X=x,Y=y) for allx∈S1,y∈S2. When this happens, we say that X and Y are independent. Similarly for X, Z, they are also independent. c) I am not sure how to do help please.","2 four-sided dice are rolled. X = number of odd dice Y = number of even dice Z = number of dice showing 1 or 2 So each of X, Y, Z only takes on the values 0, 1, 2. (a)  joint p.m.f. of (X,Y),  joint p.m.f. of (X,Z). You can give your answers in the form of 3 by 3 tables. (b) Are X and Y independent? Are X and Z independent? (c) Compute E(XY ) and E(XZ). WHAT I TRIED IS - The faces on a four-sided die are labeled with the numbers 1, 2, 3, and 4. Upon throwing the dice, each one will land with exactly one of its faces upwards. You will therefore see two numbers, one for each die. Suppose, for the sake of example, that you see a 2 and a 4. Both of these are even numbers. None of them are odd numbers. Let's count: no dice show an odd number, two dice show even numbers, and one die shows a number in the set {1,2}. Therefore, for this throw,, X=0, Y=2, and Z=1. There are 16 combinations (1,1) (1,2)(1,3) (1,4)(2,1)(2,2)(2,3)(2,4) ... (4,1 )(4,2 )(4,3 )(4,4 ) There are only 2 case when no dice show an odd number, two dice show even numbers, and one die shows a number in the set {1,2}. ---> when (2,2) (2,4 ) occurs. Here X= 0, Y =2, Z =1 Both odd and one is in {1,2} is --> (1,1)(1,3)(3,1) Here, X= 2, Y =0, Z =1 Just as we have to in the case with one discrete random variable, in order to find the ""joint probability distribution"" of X and Y, we first need to define the support of X and Y. X, Y and Z combinations are -  $$\left\{\raise{5ex}{(1,1), (1,2), (1,3), (1,4), \\(2,1), (2,2), (2,3), (2,4),\\ (3,1), (3,2), (3,3), (3,4), \\(4,1), (4,2), (4,3), (4,4)}\right\}$$ Out of this, the X, Y, Z values are  respectively, $${(2,0,1) , (1,1,2) , (2,0,1),(1,1,1),\\(1,1,2),(0,2,1),(1,1,1),(0,2,1),\\(2,0,1),(1,1,1),(2,0,0),(1,1,0),\\(1,1,1),(0,2,1),(1,1,0),(0,2,0)}$$ b) The random variables X and Y are independent if and only if: P(X= x, Y = y) = P(X = x) × P(Y = y) for allx∈S1,y∈S2. if we again take a look back at the representation of our joint p.m.f. in tabular form, you might notice that the following holds true: P(X=x,Y=y) for allx∈S1,y∈S2. When this happens, we say that X and Y are independent. Similarly for X, Z, they are also independent. c) I am not sure how to do help please.",,"['probability', 'statistics', 'probability-distributions', 'expected-value']"
48,Constraining estimated linear regression coefficients over several regressions,Constraining estimated linear regression coefficients over several regressions,,"I'm trying to run a series of simultaneous linear regressions, and I want to constrain the regression coefficients. For the standard ordinary least squares regression, the specification of the estimated coefficients in matrix form is B = (X'.X)-1X'.Y Where X are the independent variables, Y are the dependent variables, and B are the estimated coefficients for X on Y. I also have seen in Amemiya's Advanced Econometrics (1985) that Constrained Linear Least Squares can be used to impose a constraint on regression coefficients for an independent regression, such that the constrained estimated coefficients are: Bc = B - (X'.X)-1.Q.(Q'.(X'.X)-1.Q)-1.(Q'.B - C) Where Bc is the constrained estimated coefficients, B are the standard OLS coefficients, Q is a matrix of known constants governing the linear constraints on the coefficients, and C are the right-hand-side terms for the constraints (such that Q'.Bc = C). However, I would like to constrain coefficients over several regressions. For example, I have the equations: Y1 = A0 + A1.X1 + A2.X2 Y2 = B0 + B1.X1 + B2.X2 The Amemiya approach would allow me to impose a constraint such as A0 + A1 + A2 = 1 However, I would like to impose a constraint like: A1 + B1 = 1 I would really appreciate it if someone could give me some insight in to how to do this? Thanks in advance.","I'm trying to run a series of simultaneous linear regressions, and I want to constrain the regression coefficients. For the standard ordinary least squares regression, the specification of the estimated coefficients in matrix form is B = (X'.X)-1X'.Y Where X are the independent variables, Y are the dependent variables, and B are the estimated coefficients for X on Y. I also have seen in Amemiya's Advanced Econometrics (1985) that Constrained Linear Least Squares can be used to impose a constraint on regression coefficients for an independent regression, such that the constrained estimated coefficients are: Bc = B - (X'.X)-1.Q.(Q'.(X'.X)-1.Q)-1.(Q'.B - C) Where Bc is the constrained estimated coefficients, B are the standard OLS coefficients, Q is a matrix of known constants governing the linear constraints on the coefficients, and C are the right-hand-side terms for the constraints (such that Q'.Bc = C). However, I would like to constrain coefficients over several regressions. For example, I have the equations: Y1 = A0 + A1.X1 + A2.X2 Y2 = B0 + B1.X1 + B2.X2 The Amemiya approach would allow me to impose a constraint such as A0 + A1 + A2 = 1 However, I would like to impose a constraint like: A1 + B1 = 1 I would really appreciate it if someone could give me some insight in to how to do this? Thanks in advance.",,"['statistics', 'regression', 'economics']"
49,"How to generate a good guess from other guesses, if the average has already been taken?","How to generate a good guess from other guesses, if the average has already been taken?",,"There are votes two days from now* and someone made an excel sheet where people at work would make their guesses on the outcome. Since I'm not from this country, I had no idea what to do and decided to be super clever and make my collumn just the average of the others. Well, it turned out that I'm not the only foreigner and a college did just that before me. I tested to see what excel would do if I take the average anyway and not surprisingly the program killed both entries, which tried to compute an average $m_1$ from other entries of which one is average $m_1$ involving $m_2$ itself. So what I ended up with manipulating his entry to exclude mine and added a positive epsilon so there is a 50%-50% change I'm closer. So much for the motivation. Now this got me thinking: Lets say $n+2$ people, the last two being called Alice and Bob, respectively make a guess $G_i$ on the outcome of some experiment. They make their guesses publicly, one after the other, and say for all $i\in \{1,2,3,\dots,n-1,n,n+1,n+2\}$, the guess $G_i$ is some rational number.  Now after the first $n$ people have publicly made their guess, alice thinks she's clever and just chooses to make a guess which is just the average of all other guesses, including bob who still has to guess: $G_{n+1}:=\frac{(\sum_{i=1}^n G_i)+G_{n+2}}{n+1}$. Bob thinks taking an average will be a good move, but now that this is done he tends to make a slightly bigger guess than what he sees the average to be. He will give a procedure to compute his $G_{n+2}$ which shall be slightly above $G_{n+1}$. The task is to compute a number which is as close to $G_{n+1}$ as possible but not equal to it. He can only use one additional number $\varepsilon$ and there is a smallest value for this $\varepsilon$ he can manually enter into the code. So again: You can use the program to compute a number involving one instance of $+\varepsilon$ with fixed positive value at some point, and for $\varepsilon=0$ you naturally want the computed function to coincide with the average (Alice guess). Now I've had two ideas which both seem natural: Either set $G_{n+2}:=G_{n+1}+\varepsilon$, or set $G_{n+2}:=\frac{(\sum_{i=1}^n G_i)+G_{n+1}}{n+1}+\varepsilon$. I tried some random numbers for $n$, $\sum_{i=1}^n G_i$ and $\varepsilon$ and the second approach for $G_{n+2}$ seems to be quite closer to $G_{n+1}$, but moreally, I don't really know why. Does anyone see why the one approach turns out to be superiour to the other, and are there better places to put the $\varepsilon$. *Merkel will win again.","There are votes two days from now* and someone made an excel sheet where people at work would make their guesses on the outcome. Since I'm not from this country, I had no idea what to do and decided to be super clever and make my collumn just the average of the others. Well, it turned out that I'm not the only foreigner and a college did just that before me. I tested to see what excel would do if I take the average anyway and not surprisingly the program killed both entries, which tried to compute an average $m_1$ from other entries of which one is average $m_1$ involving $m_2$ itself. So what I ended up with manipulating his entry to exclude mine and added a positive epsilon so there is a 50%-50% change I'm closer. So much for the motivation. Now this got me thinking: Lets say $n+2$ people, the last two being called Alice and Bob, respectively make a guess $G_i$ on the outcome of some experiment. They make their guesses publicly, one after the other, and say for all $i\in \{1,2,3,\dots,n-1,n,n+1,n+2\}$, the guess $G_i$ is some rational number.  Now after the first $n$ people have publicly made their guess, alice thinks she's clever and just chooses to make a guess which is just the average of all other guesses, including bob who still has to guess: $G_{n+1}:=\frac{(\sum_{i=1}^n G_i)+G_{n+2}}{n+1}$. Bob thinks taking an average will be a good move, but now that this is done he tends to make a slightly bigger guess than what he sees the average to be. He will give a procedure to compute his $G_{n+2}$ which shall be slightly above $G_{n+1}$. The task is to compute a number which is as close to $G_{n+1}$ as possible but not equal to it. He can only use one additional number $\varepsilon$ and there is a smallest value for this $\varepsilon$ he can manually enter into the code. So again: You can use the program to compute a number involving one instance of $+\varepsilon$ with fixed positive value at some point, and for $\varepsilon=0$ you naturally want the computed function to coincide with the average (Alice guess). Now I've had two ideas which both seem natural: Either set $G_{n+2}:=G_{n+1}+\varepsilon$, or set $G_{n+2}:=\frac{(\sum_{i=1}^n G_i)+G_{n+1}}{n+1}+\varepsilon$. I tried some random numbers for $n$, $\sum_{i=1}^n G_i$ and $\varepsilon$ and the second approach for $G_{n+2}$ seems to be quite closer to $G_{n+1}$, but moreally, I don't really know why. Does anyone see why the one approach turns out to be superiour to the other, and are there better places to put the $\varepsilon$. *Merkel will win again.",,['statistics']
50,"Given mean and standard deviation, find the probability","Given mean and standard deviation, find the probability",,"Lets say that you know the mean and the standard deviation of a regularly distributed dataset. How do you find the probability that a random sample of n datapoints results in a sample mean less than some x? Example- Lets say the population mean is 12, and the standard deviation is 4, what is the probability that a random sample of 40 datapoints results in a sample mean less than ten? Yes, this is a homework problem, but I changed the numbers. Go ahead and change them again if you like- I just want to know how to do these kinds of problems. The professor is... less than helpful.","Lets say that you know the mean and the standard deviation of a regularly distributed dataset. How do you find the probability that a random sample of n datapoints results in a sample mean less than some x? Example- Lets say the population mean is 12, and the standard deviation is 4, what is the probability that a random sample of 40 datapoints results in a sample mean less than ten? Yes, this is a homework problem, but I changed the numbers. Go ahead and change them again if you like- I just want to know how to do these kinds of problems. The professor is... less than helpful.",,['statistics']
51,Probability of being up in roulette,Probability of being up in roulette,,"A player bets $\$1$ on a single number in a standard US roulette, that is, 38 possible numbers ($\frac{1}{38}$ chance of a win each game). A win pays 35 times the stake plus the stake returned, otherwise the stake is lost. So, the expected loss per game is $\left(\frac{1}{38}\right)(35) + \left(37/38\right)(-1) = -\frac{2}{38}$ dollars, and in 36 games $36\left(-\frac{2}{38}\right) = -1.89$ dollars. But, the player is up within 35 games if he wins a single game, thus the probability of being up in 35 games is $1 - \left(\frac{37}{38}\right)^{35} = 0.607$. And even in 26 games, the probability of being up is still slightly greater than half. This is perhaps surprising as it seems to suggest you can win at roulette if you play often enough. I'm assuming that that this result is offset by a very high variance, but wouldn't that also imply you could win big by winning multiple times? Can someone with a better statistics brain shed some light onto this problem, and extend my analyse? Thanks.","A player bets $\$1$ on a single number in a standard US roulette, that is, 38 possible numbers ($\frac{1}{38}$ chance of a win each game). A win pays 35 times the stake plus the stake returned, otherwise the stake is lost. So, the expected loss per game is $\left(\frac{1}{38}\right)(35) + \left(37/38\right)(-1) = -\frac{2}{38}$ dollars, and in 36 games $36\left(-\frac{2}{38}\right) = -1.89$ dollars. But, the player is up within 35 games if he wins a single game, thus the probability of being up in 35 games is $1 - \left(\frac{37}{38}\right)^{35} = 0.607$. And even in 26 games, the probability of being up is still slightly greater than half. This is perhaps surprising as it seems to suggest you can win at roulette if you play often enough. I'm assuming that that this result is offset by a very high variance, but wouldn't that also imply you could win big by winning multiple times? Can someone with a better statistics brain shed some light onto this problem, and extend my analyse? Thanks.",,"['probability', 'statistics']"
52,finding the maximum likelihood estimator: conditional generalised linear model,finding the maximum likelihood estimator: conditional generalised linear model,,"Find the maximum likelihood estimator of $\alpha$,$\beta$ and $\lambda$  given the model \begin{equation*}P(Z=z|X=x)=\exp\bigg\{\sum_{j=1}^m(\alpha_j+\beta_j{x})z_j+\sum_{j<k}\lambda_{jk}z_jz_k-\Psi(\alpha+\beta{x},\lambda)\bigg\}\end{equation*} where \begin{equation*}\Psi (\alpha+\beta{x},\lambda)=\log\bigg\{\sum_{z\in\Omega}\exp\big\{\sum_{j=1}^m\big(\alpha_j+\beta_jx\big)z_j+\sum_{j<k}\lambda_{jk}z_jz_k\big\}\bigg\}\end{equation*}  \begin{equation*}z_j=\begin{cases}  1 & \text{if success}\\  0 & \text{if failure}  \end{cases}\end{equation*} $x$ is the vector of covariates, $\alpha$,$\beta$ and $\lambda$ are the unknown parameters $\sum_{z\in\Omega}$ is the summation over all possible values $z$ can take. i was able to find the likelihood function and the log likelihood functions as follows:  Denote $L$ the likelihood function of the model for a random  sample of size $N$ so that we have the following\  \begin{equation*}L(\theta;z)=\prod_{i=1}^N\exp\bigg\{\sum_{j=1}^m(\alpha_j+\beta_j{x_i})z_{ij}+\sum_{j<k}\lambda_{jk}z_{ij}z_{ik}-\Psi(\alpha+\beta{x_i},\lambda)\bigg\}\end{equation*} where $\theta=(\alpha,\beta,\lambda)$ so that the log likelihood function $l$ becomes  \begin{equation}l=\sum_{j=1}^m\alpha_j\sum_{i=1}^Nz_{ij}+\sum_{j=1}^m\beta_j\sum_{i=1}^Nx_iz_{ij}+  \sum_{j<k}\lambda_{jk}\sum_{i=1}^Nz_{ij}z_{ik}-\sum_{i=1}^N\Psi(\alpha+\beta{x_i},\lambda)\end{equation} my main problem is to find the maximum likelihood estimator of  $\alpha$,$\beta$ and $\lambda$. I had a problem in finding the first and second partial of the log likelihood function more especially finding the partial of the $\Psi$ function. Help me find the score and fisher information.","Find the maximum likelihood estimator of $\alpha$,$\beta$ and $\lambda$  given the model \begin{equation*}P(Z=z|X=x)=\exp\bigg\{\sum_{j=1}^m(\alpha_j+\beta_j{x})z_j+\sum_{j<k}\lambda_{jk}z_jz_k-\Psi(\alpha+\beta{x},\lambda)\bigg\}\end{equation*} where \begin{equation*}\Psi (\alpha+\beta{x},\lambda)=\log\bigg\{\sum_{z\in\Omega}\exp\big\{\sum_{j=1}^m\big(\alpha_j+\beta_jx\big)z_j+\sum_{j<k}\lambda_{jk}z_jz_k\big\}\bigg\}\end{equation*}  \begin{equation*}z_j=\begin{cases}  1 & \text{if success}\\  0 & \text{if failure}  \end{cases}\end{equation*} $x$ is the vector of covariates, $\alpha$,$\beta$ and $\lambda$ are the unknown parameters $\sum_{z\in\Omega}$ is the summation over all possible values $z$ can take. i was able to find the likelihood function and the log likelihood functions as follows:  Denote $L$ the likelihood function of the model for a random  sample of size $N$ so that we have the following\  \begin{equation*}L(\theta;z)=\prod_{i=1}^N\exp\bigg\{\sum_{j=1}^m(\alpha_j+\beta_j{x_i})z_{ij}+\sum_{j<k}\lambda_{jk}z_{ij}z_{ik}-\Psi(\alpha+\beta{x_i},\lambda)\bigg\}\end{equation*} where $\theta=(\alpha,\beta,\lambda)$ so that the log likelihood function $l$ becomes  \begin{equation}l=\sum_{j=1}^m\alpha_j\sum_{i=1}^Nz_{ij}+\sum_{j=1}^m\beta_j\sum_{i=1}^Nx_iz_{ij}+  \sum_{j<k}\lambda_{jk}\sum_{i=1}^Nz_{ij}z_{ik}-\sum_{i=1}^N\Psi(\alpha+\beta{x_i},\lambda)\end{equation} my main problem is to find the maximum likelihood estimator of  $\alpha$,$\beta$ and $\lambda$. I had a problem in finding the first and second partial of the log likelihood function more especially finding the partial of the $\Psi$ function. Help me find the score and fisher information.",,"['probability', 'statistics', 'conditional-probability', 'statistical-inference']"
53,Series involving Marcum Q function,Series involving Marcum Q function,,"I would like to have a better form of this series: $$\sum_{k=0}^{\infty}\,\frac{1}{k!}\,\left(\frac{ab\sin(c)}{\sqrt{2}}\right)^{2k}\,Q_{k+\frac{3}{2}}\left(ab\cos(c),bx\right)$$ where $Q_m(\alpha,\beta)$ is the generalized Marcum Q-function . Does anyone an idea on how to proceed, if possible to make it better (e.g. getting rid of the series)? Thanks! EDIT: could the result be just proportional to $Q_{\frac{3}{2}}\left(ab,bx\right)$???","I would like to have a better form of this series: $$\sum_{k=0}^{\infty}\,\frac{1}{k!}\,\left(\frac{ab\sin(c)}{\sqrt{2}}\right)^{2k}\,Q_{k+\frac{3}{2}}\left(ab\cos(c),bx\right)$$ where $Q_m(\alpha,\beta)$ is the generalized Marcum Q-function . Does anyone an idea on how to proceed, if possible to make it better (e.g. getting rid of the series)? Thanks! EDIT: could the result be just proportional to $Q_{\frac{3}{2}}\left(ab,bx\right)$???",,"['sequences-and-series', 'statistics', 'special-functions']"
54,Decreasing Sequence of Events in Statistics,Decreasing Sequence of Events in Statistics,,There is a question from lecture notes whose answer I do not quite understand. The question is Let $X$ have a cdf $F$. Let $x$ be any point and suppose $x_n$ is a decreasing sequence such that $x_n \rightarrow x$ as $n \rightarrow \infty$. Show $F(x_n) \rightarrow F(x)$ as $n \rightarrow \infty$. The solution according to lecture notes is: If $x_n \downarrow -\infty$ Then $\lim_{m\to\infty}F(x_m) = \lim_{m\to\infty}P(X \leq x_m) = P(\lim_{m\to\infty}(X \leq x_m)) = P(\bigcap_{m}(X \leq x_m)) = P(\varnothing) = 0$ Hence  $\lim_{x\to -\infty}F(x) = 0$ which can be written as $F(-\infty) = 0$. Then this implies the conclusion. Can someone help me understand how this argument answers the question or suggest an alternative answer?,There is a question from lecture notes whose answer I do not quite understand. The question is Let $X$ have a cdf $F$. Let $x$ be any point and suppose $x_n$ is a decreasing sequence such that $x_n \rightarrow x$ as $n \rightarrow \infty$. Show $F(x_n) \rightarrow F(x)$ as $n \rightarrow \infty$. The solution according to lecture notes is: If $x_n \downarrow -\infty$ Then $\lim_{m\to\infty}F(x_m) = \lim_{m\to\infty}P(X \leq x_m) = P(\lim_{m\to\infty}(X \leq x_m)) = P(\bigcap_{m}(X \leq x_m)) = P(\varnothing) = 0$ Hence  $\lim_{x\to -\infty}F(x) = 0$ which can be written as $F(-\infty) = 0$. Then this implies the conclusion. Can someone help me understand how this argument answers the question or suggest an alternative answer?,,['statistics']
55,Confidence intervals on maximum likelihoods of observed data,Confidence intervals on maximum likelihoods of observed data,,"I observed 400 episodes of nursing care in a hospital. I tracked the movement of the nurses between 5 rooms $A-E$.  The maximum likelihood of them moving from room $i\rightarrow j$ is given by: \begin{equation} P_{ij}=\displaystyle \dfrac{\text{# of times from room $i\rightarrow j$}}{\displaystyle \text{Total # of transitions to any room}}\end{equation} Is there a way of defining a confidence interval on this maximum likelihood estimate $P_{ij}$? And for all maximum likelihood estimates of all possible room combinations? Reference : I have come across a reference: http://arxiv.org/pdf/0905.4131v1.pdf This suggests that for n observations $X_i$, the empirical maximum likelihood estimate $\hat P_{ij}$  minus the actual transition probability $P_{ij}$ would tend to a multivariate normal distribution with mean 0 and matrix of variance-covariances $\Sigma$. $$\sqrt{n}|\hat{P_{ij}}-P_{ij}|\sim N(0,\Sigma)\quad \text{as}\quad n\rightarrow \infty$$ How to I calculate $\Sigma$ from my observed data? And how does this relate to confidence intervals?","I observed 400 episodes of nursing care in a hospital. I tracked the movement of the nurses between 5 rooms $A-E$.  The maximum likelihood of them moving from room $i\rightarrow j$ is given by: \begin{equation} P_{ij}=\displaystyle \dfrac{\text{# of times from room $i\rightarrow j$}}{\displaystyle \text{Total # of transitions to any room}}\end{equation} Is there a way of defining a confidence interval on this maximum likelihood estimate $P_{ij}$? And for all maximum likelihood estimates of all possible room combinations? Reference : I have come across a reference: http://arxiv.org/pdf/0905.4131v1.pdf This suggests that for n observations $X_i$, the empirical maximum likelihood estimate $\hat P_{ij}$  minus the actual transition probability $P_{ij}$ would tend to a multivariate normal distribution with mean 0 and matrix of variance-covariances $\Sigma$. $$\sqrt{n}|\hat{P_{ij}}-P_{ij}|\sim N(0,\Sigma)\quad \text{as}\quad n\rightarrow \infty$$ How to I calculate $\Sigma$ from my observed data? And how does this relate to confidence intervals?",,"['probability', 'statistics', 'markov-chains']"
56,Where can I find a longitudinal study with a binary response?,Where can I find a longitudinal study with a binary response?,,"I've spent the last couple of months trying to find real data (not simulated) of a longitudinal study, but I can't seem to find one. Any topic and time frame is fine, but the dependent variable must be binary, and a few covariables are also desirable. I appreciate any help immensely!","I've spent the last couple of months trying to find real data (not simulated) of a longitudinal study, but I can't seem to find one. Any topic and time frame is fine, but the dependent variable must be binary, and a few covariables are also desirable. I appreciate any help immensely!",,"['statistics', 'reference-request', 'data-analysis']"
57,Wealth indicator function for bidder agent logic,Wealth indicator function for bidder agent logic,,"I want to create a wealth indicator function used by the logic of a bidder agent, that tells the agent if he's rich (in comparison to others). Given: Total number of competitors $n$ Amount of all the money $m$ in the system mean (displaying money per agent) $a$. (E.g. arithmetic mean etc.) Searching for:  $w(x) \in \mathbb{R}, x \in \mathbb{N}$ so that: $w(m) = 1$ $w(a) = \frac{1}{2}$ $w(0) = 0$ (doesn't need to be an odd function, although it would be nice) I had a linear function, but that didn't fit my needs, because most of the values were around ~ 0.5, also had a quadratic and a kubic function that didn't fit (created an equoation and solved it with wolfram alpha). Exponential growth seems to be the solution (assuming money is normally distributed, the desired function would be a logistic curve), but I'm having problems defining a monotonically nondecreasing function. Would be glad if you could help me out.","I want to create a wealth indicator function used by the logic of a bidder agent, that tells the agent if he's rich (in comparison to others). Given: Total number of competitors $n$ Amount of all the money $m$ in the system mean (displaying money per agent) $a$. (E.g. arithmetic mean etc.) Searching for:  $w(x) \in \mathbb{R}, x \in \mathbb{N}$ so that: $w(m) = 1$ $w(a) = \frac{1}{2}$ $w(0) = 0$ (doesn't need to be an odd function, although it would be nice) I had a linear function, but that didn't fit my needs, because most of the values were around ~ 0.5, also had a quadratic and a kubic function that didn't fit (created an equoation and solved it with wolfram alpha). Exponential growth seems to be the solution (assuming money is normally distributed, the desired function would be a logistic curve), but I'm having problems defining a monotonically nondecreasing function. Would be glad if you could help me out.",,"['real-analysis', 'statistics', 'functional-equations', 'economics']"
58,Probability distribution of the product of two independent complex gaussian random variables,Probability distribution of the product of two independent complex gaussian random variables,,"I have to calculate the pdf of $Z = X*Y$, where $X \in \mathcal{C}(\mu_x,\Sigma_x)$ and $Y \in \mathcal{C}(\mu_y,\Sigma_y)$, where $\mathcal{C}$ is a complex distribution. It can be assumed that $\mu_x$ and $\mu_y$ are real and $\Sigma_x$ and $\Sigma_y$ are equal to $\sigma^2 \left[\begin{matrix}1 & 0\\0 & 1\end{matrix}\right]$, i.e. the real and imaginary part have equal variance and are independently distributed. Also, X and Y are independent. From the literature (Papers referencing Rohatgi 76) and from the book Rohatgi-2001 I have found that the real-case can be calculated using $$ p_{XY}(v) = \int_{-\infty}^\infty \frac{1}{|x|}p_X(x)p_Y\left(\frac{v}{x}\right) dx ~~~~~(1) $$ My guess would be a solution similar to this along the form $$ p_{XY}(z) = \int_{-\infty}^\infty \int_{-\infty}^\infty\frac{1}{(a+ib)(a-ib)}p_X(a+ib)p_Y\left(\frac{z}{a+ib}\right) da db $$ but I have not been able to find any sources for this. Also, when I looked in Rohatgi 2001 on p. 141 for the 1D case derivation for hints, it only said that the proof was left as an exercise. This is all well and fine if you know someone to help you. So... Can anyone give me a link/solution to my problem or perhaps a link to a rigorous derivation of (1) I would be very thankful. I am ""simply"" looking for a solution in integral form that I can solve so I don't have to use excessive Monte Carlo simulation. Thank you in advance to all who post. /Henrik Andresen","I have to calculate the pdf of $Z = X*Y$, where $X \in \mathcal{C}(\mu_x,\Sigma_x)$ and $Y \in \mathcal{C}(\mu_y,\Sigma_y)$, where $\mathcal{C}$ is a complex distribution. It can be assumed that $\mu_x$ and $\mu_y$ are real and $\Sigma_x$ and $\Sigma_y$ are equal to $\sigma^2 \left[\begin{matrix}1 & 0\\0 & 1\end{matrix}\right]$, i.e. the real and imaginary part have equal variance and are independently distributed. Also, X and Y are independent. From the literature (Papers referencing Rohatgi 76) and from the book Rohatgi-2001 I have found that the real-case can be calculated using $$ p_{XY}(v) = \int_{-\infty}^\infty \frac{1}{|x|}p_X(x)p_Y\left(\frac{v}{x}\right) dx ~~~~~(1) $$ My guess would be a solution similar to this along the form $$ p_{XY}(z) = \int_{-\infty}^\infty \int_{-\infty}^\infty\frac{1}{(a+ib)(a-ib)}p_X(a+ib)p_Y\left(\frac{z}{a+ib}\right) da db $$ but I have not been able to find any sources for this. Also, when I looked in Rohatgi 2001 on p. 141 for the 1D case derivation for hints, it only said that the proof was left as an exercise. This is all well and fine if you know someone to help you. So... Can anyone give me a link/solution to my problem or perhaps a link to a rigorous derivation of (1) I would be very thankful. I am ""simply"" looking for a solution in integral form that I can solve so I don't have to use excessive Monte Carlo simulation. Thank you in advance to all who post. /Henrik Andresen",,"['statistics', 'probability-distributions', 'complex-numbers', 'random']"
59,Ordered statistics: find $E(Y(i) Y(j))$,Ordered statistics: find,E(Y(i) Y(j)),"Let $Y1 ,…., Yn$ be independent, uniformly distributed random variables on the interval [0,1]. Find: $Var[Y_{(j)}- Y_{(i)}]$, the variance of the difference between two ordered statistics  Where i and j are integers with 1≤i < j ≤n. My Steps are: 1) find the joint density function. My answer: $$\frac{n!}{(i-1)!(j-i-1)!(n-j)!}(y_{(i)})^{i-1}(y_{(j)}-y_{(i)})^{j-1-i}(1-y_{(j)})^{n-j}$$ where $0< y_{(i)}< y_{(j)}<1$ 2) Find $E[Y_{(i)}Y_{(j)}]$ from first principle $$a\int_{0}^{1}\int_{y(_{j})}^{1}y_{(j)}(y_{(j)}-y_{(i)})^{(j-i-1)}(1-y_{(j)})^{n-j}y_{i}^{i} dy_{(i)}dy_{(j)}$$ where $$a = \frac{n!}{(i-1)!(j-i-1)!(n-j)!}$$ This is where I am stuck. I am not sure how to integrate this integrand. Any tips/hints would be great. My aim is to convert this to a Beta form. 3)  Find $\operatorname{Cov}[Y_{(i)}, Y_{(j)}]$ using $\operatorname{Cov}[Y_{(i)},Y_{(j)}] = E[Y_{(i)}Y_{(j)}] - E[Y_{(i)}]E[Y_{(j)}]$. But this depends on step 2. 4)  Find the variance using the generic $\operatorname{Var}[A-B]=\operatorname{Var}[A]+\operatorname{Var}[B]-2\operatorname{Cov}[A,B]$ . However, this all depends on part 3 and the steps before it. Please provide some guidance as to whether my overall approach is correct and what can I do to overcome step 2. Thanks. Edit: sorry for step 2 -> it should be $E[Y_{(i)}Y_{(j)}]$ not $E[Y_{(i)}-Y_{(j)}]$. Typing error.","Let $Y1 ,…., Yn$ be independent, uniformly distributed random variables on the interval [0,1]. Find: $Var[Y_{(j)}- Y_{(i)}]$, the variance of the difference between two ordered statistics  Where i and j are integers with 1≤i < j ≤n. My Steps are: 1) find the joint density function. My answer: $$\frac{n!}{(i-1)!(j-i-1)!(n-j)!}(y_{(i)})^{i-1}(y_{(j)}-y_{(i)})^{j-1-i}(1-y_{(j)})^{n-j}$$ where $0< y_{(i)}< y_{(j)}<1$ 2) Find $E[Y_{(i)}Y_{(j)}]$ from first principle $$a\int_{0}^{1}\int_{y(_{j})}^{1}y_{(j)}(y_{(j)}-y_{(i)})^{(j-i-1)}(1-y_{(j)})^{n-j}y_{i}^{i} dy_{(i)}dy_{(j)}$$ where $$a = \frac{n!}{(i-1)!(j-i-1)!(n-j)!}$$ This is where I am stuck. I am not sure how to integrate this integrand. Any tips/hints would be great. My aim is to convert this to a Beta form. 3)  Find $\operatorname{Cov}[Y_{(i)}, Y_{(j)}]$ using $\operatorname{Cov}[Y_{(i)},Y_{(j)}] = E[Y_{(i)}Y_{(j)}] - E[Y_{(i)}]E[Y_{(j)}]$. But this depends on step 2. 4)  Find the variance using the generic $\operatorname{Var}[A-B]=\operatorname{Var}[A]+\operatorname{Var}[B]-2\operatorname{Cov}[A,B]$ . However, this all depends on part 3 and the steps before it. Please provide some guidance as to whether my overall approach is correct and what can I do to overcome step 2. Thanks. Edit: sorry for step 2 -> it should be $E[Y_{(i)}Y_{(j)}]$ not $E[Y_{(i)}-Y_{(j)}]$. Typing error.",,"['statistics', 'order-statistics']"
60,Asymptotics for infinite sum with erf,Asymptotics for infinite sum with erf,,"I'm interested in approximating the infinite sum $$ \sum_{i=1}^\infty Z\left(\frac{\alpha i\pm1}{\beta}\right) $$ where $\alpha,\beta$ are constant and $$ Z(a\pm b)=\frac{1}{2\pi}\int_{a-b}^{a+b}e^{-x^2/2}dx=\frac{\operatorname{erf}(a+b)-\operatorname{erf}(a-b)}{2} $$ is the standard normal distribution. Any useful asymptotics?","I'm interested in approximating the infinite sum $$ \sum_{i=1}^\infty Z\left(\frac{\alpha i\pm1}{\beta}\right) $$ where $\alpha,\beta$ are constant and $$ Z(a\pm b)=\frac{1}{2\pi}\int_{a-b}^{a+b}e^{-x^2/2}dx=\frac{\operatorname{erf}(a+b)-\operatorname{erf}(a-b)}{2} $$ is the standard normal distribution. Any useful asymptotics?",,"['statistics', 'asymptotics', 'special-functions']"
61,Prediction Model for forecasting using Linear regression,Prediction Model for forecasting using Linear regression,,"I am very new to inferral statistics. I am trying to build a prediction model for forecasting the revenue for physicians based on some historical data. I was planning to use Multiple Linear Regression model where the Payment is dependent to predictors such as Number Of Patient Visits,Number Of Charges for that month. The structure of the data looks like :- Payment(Monthly) | Patient Visits | Charges Count | Month Date | Now after I have built an regression model,I have to forecast the payments of the physician based on the model. But now due to extrapolation while doing the forecast I assume I cannot give values of predictors outside the range of data with which it was built. For example the range for patient Visits is from 100 to 1000,now I want to predict what will be the payment if I had 2000 patient visits. I am not getting correct results with the model that I have built. One of the other thoughts that I have is building Time Series models. But in that case time will be in X axis and the Payments will be in the Y - axis and we will be looking at the trend of the payments over time to make future predictions. But I want to use the effect of other predictors also when making future predictions. Please let me know how best I can achieve this. As I am new to this any guidance will be highly appreciated. Thanks in Advance!!","I am very new to inferral statistics. I am trying to build a prediction model for forecasting the revenue for physicians based on some historical data. I was planning to use Multiple Linear Regression model where the Payment is dependent to predictors such as Number Of Patient Visits,Number Of Charges for that month. The structure of the data looks like :- Payment(Monthly) | Patient Visits | Charges Count | Month Date | Now after I have built an regression model,I have to forecast the payments of the physician based on the model. But now due to extrapolation while doing the forecast I assume I cannot give values of predictors outside the range of data with which it was built. For example the range for patient Visits is from 100 to 1000,now I want to predict what will be the payment if I had 2000 patient visits. I am not getting correct results with the model that I have built. One of the other thoughts that I have is building Time Series models. But in that case time will be in X axis and the Payments will be in the Y - axis and we will be looking at the trend of the payments over time to make future predictions. But I want to use the effect of other predictors also when making future predictions. Please let me know how best I can achieve this. As I am new to this any guidance will be highly appreciated. Thanks in Advance!!",,"['statistics', 'regression', 'statistical-inference']"
62,Expected value with a kronecker product and Gaussian distributional assumption,Expected value with a kronecker product and Gaussian distributional assumption,,"What is the expected value, $ \mathbb{E}\left[ I \otimes \left( \operatorname{diag}(ZZ^T\mathbf{1}) - ZZ^T\right)\right]$ where $Z \sim N(0, \sigma^2I) $ is a random variable? The kronecker product and the $diag$ is where the confusion is setting in. Hints: i) $ \mathbb{E}\left[ I \otimes \left( \operatorname{diag}(ZZ^T\mathbf{1}) - ZZ^T\right)\right]= \left[ I \otimes \mathbb{E} \left( \operatorname{diag}(ZZ^T\mathbf{1}) - ZZ^T\right)\right]$ ii)  I guess $ZZ^T \sim nWishart(.)$, where $n$ is the number of rows in $Z$, but am not very sure.","What is the expected value, $ \mathbb{E}\left[ I \otimes \left( \operatorname{diag}(ZZ^T\mathbf{1}) - ZZ^T\right)\right]$ where $Z \sim N(0, \sigma^2I) $ is a random variable? The kronecker product and the $diag$ is where the confusion is setting in. Hints: i) $ \mathbb{E}\left[ I \otimes \left( \operatorname{diag}(ZZ^T\mathbf{1}) - ZZ^T\right)\right]= \left[ I \otimes \mathbb{E} \left( \operatorname{diag}(ZZ^T\mathbf{1}) - ZZ^T\right)\right]$ ii)  I guess $ZZ^T \sim nWishart(.)$, where $n$ is the number of rows in $Z$, but am not very sure.",,"['probability', 'statistics', 'stochastic-processes', 'random-variables', 'random-matrices']"
63,Proof of the consistency of unbiased estimators,Proof of the consistency of unbiased estimators,,Prove that $\hat{\theta}$ is an unbiased estimator of $\theta$ and $\lim_{n \rightarrow \infty}$ Var($\hat{\theta})=0$ then $\hat{\theta}$ is consistent.,Prove that $\hat{\theta}$ is an unbiased estimator of $\theta$ and $\lim_{n \rightarrow \infty}$ Var($\hat{\theta})=0$ then $\hat{\theta}$ is consistent.,,"['probability', 'statistics']"
64,Probability Question relating prison break,Probability Question relating prison break,,"I am stuck in a question regarding a prisoner trapped in a cell with 3 doors that actually has a probability associated with each door chosen(say $.5$ for door $A$, $.3$ for door $B$ and $.2$ for door $C$). The first door leads to his own cell after traveling $2$ days, whereas the second door leads to his own cell after $3$ days and the third to freedom after $1$ day. ""A prisoner is trapped in a cell containig three doors. The first door leads to a tunnel that returns him to his cell after two days of travel. The second leads to a tunnel that returns him to his cell after three days of travel. The third door leads immediately to freedom. a)  Assuming that the prisoner will always select doors 1,2,and 3 with probability 0.5, 0.3, 0.2 what is teh expected number of days until he reaches freedom? b)  Assuming that the prisoner is always equally likely to choose among those doors that he not used, what is the expected number of days until he reaches freedom? (In this version, for instance, if the prisoner initially tries door1, then when he returns to the cell, he will now select only from doors 2 and 3) c)    For parts (a) and (b) find the variance of the number of days until the prisoner reaches freedom. "" In the problem I was able to find the $E[X]$ (Expected number of days until prisoner is free where X is # of days to be free). Where I get stuck is how to find the variances for this problem. I do not know how to find $E[X^2$ given door $1$ (or $2$ or $3$) chosen$]$. My understanding is that he does not learn from choosing the wrong door. Could anyone help me out? Thank you very much","I am stuck in a question regarding a prisoner trapped in a cell with 3 doors that actually has a probability associated with each door chosen(say $.5$ for door $A$, $.3$ for door $B$ and $.2$ for door $C$). The first door leads to his own cell after traveling $2$ days, whereas the second door leads to his own cell after $3$ days and the third to freedom after $1$ day. ""A prisoner is trapped in a cell containig three doors. The first door leads to a tunnel that returns him to his cell after two days of travel. The second leads to a tunnel that returns him to his cell after three days of travel. The third door leads immediately to freedom. a)  Assuming that the prisoner will always select doors 1,2,and 3 with probability 0.5, 0.3, 0.2 what is teh expected number of days until he reaches freedom? b)  Assuming that the prisoner is always equally likely to choose among those doors that he not used, what is the expected number of days until he reaches freedom? (In this version, for instance, if the prisoner initially tries door1, then when he returns to the cell, he will now select only from doors 2 and 3) c)    For parts (a) and (b) find the variance of the number of days until the prisoner reaches freedom. "" In the problem I was able to find the $E[X]$ (Expected number of days until prisoner is free where X is # of days to be free). Where I get stuck is how to find the variances for this problem. I do not know how to find $E[X^2$ given door $1$ (or $2$ or $3$) chosen$]$. My understanding is that he does not learn from choosing the wrong door. Could anyone help me out? Thank you very much",,"['probability', 'statistics', 'puzzle']"
65,Why is $\sum x^2 _t \times \text{Var}(\beta)=\frac{\sum x^2 _t \times \sigma^2}{ \sum x^2 _t} = \sigma^2$?,Why is ?,\sum x^2 _t \times \text{Var}(\beta)=\frac{\sum x^2 _t \times \sigma^2}{ \sum x^2 _t} = \sigma^2,I do not get this connection. Is is reliable to divide this equation by $\sum x^2 _t$ to get just $\sigma^2$ ? $$\sum x^2 _t \times E(\hat \beta - \beta)^2=\sum x^2 _t \times \text{Var}(\beta)=\frac{\sum x^2 _t \times \sigma^2}{ \sum x^2 _t} = \sigma^2$$,I do not get this connection. Is is reliable to divide this equation by $\sum x^2 _t$ to get just $\sigma^2$ ? $$\sum x^2 _t \times E(\hat \beta - \beta)^2=\sum x^2 _t \times \text{Var}(\beta)=\frac{\sum x^2 _t \times \sigma^2}{ \sum x^2 _t} = \sigma^2$$,,"['probability', 'statistics', 'regression']"
66,Mathematical Statistics,Mathematical Statistics,,"How do I find the answers to this question? State Tech’s basketball team, the Fighting Loga- rithms, have a 70% foul-shooting percentage. (a) Write a formula for the exact probability that out of their next one hundred free throws, they will make between seventy-ﬁve and eighty, inclusive. (b) Approximate the probability asked for in part (a).","How do I find the answers to this question? State Tech’s basketball team, the Fighting Loga- rithms, have a 70% foul-shooting percentage. (a) Write a formula for the exact probability that out of their next one hundred free throws, they will make between seventy-ﬁve and eighty, inclusive. (b) Approximate the probability asked for in part (a).",,"['probability', 'statistics', 'logarithms']"
67,Numerical calculation of fisher information,Numerical calculation of fisher information,,"I am trying to obtain numerically the fisher information. Given a likelihood function $$ f(X,\theta),$$ with $X \in [0,1]$. The fisher information is given by $$ \mathbb{I}(\theta)=\mathbb{E}\left[\left. \frac{\partial^2 \text{log }f( X|\theta)}{\partial \theta^2}\right|_{\theta=\theta^*} \right].$$ To calculate this numerically in Matlab is use this formula: $$\mathbb{I}(\theta) = \int_0^1 \frac{\partial^2 \text{log }f( X|\theta)}{\partial \theta^2} \cdot f(X,\theta) \quad dX$$ Am I doing this correct?","I am trying to obtain numerically the fisher information. Given a likelihood function $$ f(X,\theta),$$ with $X \in [0,1]$. The fisher information is given by $$ \mathbb{I}(\theta)=\mathbb{E}\left[\left. \frac{\partial^2 \text{log }f( X|\theta)}{\partial \theta^2}\right|_{\theta=\theta^*} \right].$$ To calculate this numerically in Matlab is use this formula: $$\mathbb{I}(\theta) = \int_0^1 \frac{\partial^2 \text{log }f( X|\theta)}{\partial \theta^2} \cdot f(X,\theta) \quad dX$$ Am I doing this correct?",,"['statistics', 'estimation', 'parameter-estimation', 'statistical-inference']"
68,Bernoulli distribution: expectation problem with independent random variables,Bernoulli distribution: expectation problem with independent random variables,,"We have $X_1,...,X_n$ as $n$ independent random variables under the Bernoulli distribution i.e.: $$P(X_i=1)=p$$ $$P(X_i=0)=1-p$$ where, $p$ is an unknown parameter. The distribution $Y=\sum_1^n X_i$ is the binomial distribution such that $Y\sim \mathrm{Binom}(n,p)$. I have found that $\bar X$ is an estimate of $Y/n$ but now it asks to find $E(\bar X(1-\bar X))$ as $(n-1)p(1-p)/n$ Any ideas how to calculate this the best way, would I expand or separate to $\mathrm{E}(\bar X)\mathrm{E}(1-\bar X)$? Would this involve manipulating the binomial formula?","We have $X_1,...,X_n$ as $n$ independent random variables under the Bernoulli distribution i.e.: $$P(X_i=1)=p$$ $$P(X_i=0)=1-p$$ where, $p$ is an unknown parameter. The distribution $Y=\sum_1^n X_i$ is the binomial distribution such that $Y\sim \mathrm{Binom}(n,p)$. I have found that $\bar X$ is an estimate of $Y/n$ but now it asks to find $E(\bar X(1-\bar X))$ as $(n-1)p(1-p)/n$ Any ideas how to calculate this the best way, would I expand or separate to $\mathrm{E}(\bar X)\mathrm{E}(1-\bar X)$? Would this involve manipulating the binomial formula?",,"['probability', 'statistics']"
69,Estimating the number of tickets bought in a lottery,Estimating the number of tickets bought in a lottery,,"A national lottery has the format where $7$ numbers are chosen from $45$ without replacement. The first $6$ numbers chosen constitute the ""winning numbers"", while the last number chosen is the ""additional number"". Participants purchase tickets containing six numbers each. Prizes and the published odds of winning for the lottery are as follows, where $(n,m)$ denotes how many of the numbers on your ticket match the winning and additional numbers respectively. Take note that a ticket is only considered to win from the highest group it can win - for example, a ticket winning in Group 1 is not considered to have won any of the lower groups. Group 1 : (6,0) - $1/8145060$ Group 2 : (5,1) - $1/1357510$ Group 3 : (5,0) - $19/678755$ Group 4 : (4,1) - $19/271502$ Group 5 : (4,0) - $703/543004$ Group 6 : (3,1) - $703/407253$ Now, the lottery board publishes statistics on how many winners there are in each group. For instance, during the last draw, the number of winners were as follows: $(1,7,280,875,14347,21993)$. Using these numbers, what is a good way for me to estimate the number of tickets bought, with a confidence interval? If I just look at the information from one set of probabilities and number of winners, I will be able to get an estimate (for example, I would estimate that there were 8145060 tickets bought if I had only considered Group 1 prizes). How can I combine all the information I have using Bayesian statistics to generate a better estimate? Currently, I'm looking at treating the number of winning tickets in a given group as a Binomial Distribution, and then apply the Agresti-Coull interval, but I'm not sure how to combine all these intervals.","A national lottery has the format where $7$ numbers are chosen from $45$ without replacement. The first $6$ numbers chosen constitute the ""winning numbers"", while the last number chosen is the ""additional number"". Participants purchase tickets containing six numbers each. Prizes and the published odds of winning for the lottery are as follows, where $(n,m)$ denotes how many of the numbers on your ticket match the winning and additional numbers respectively. Take note that a ticket is only considered to win from the highest group it can win - for example, a ticket winning in Group 1 is not considered to have won any of the lower groups. Group 1 : (6,0) - $1/8145060$ Group 2 : (5,1) - $1/1357510$ Group 3 : (5,0) - $19/678755$ Group 4 : (4,1) - $19/271502$ Group 5 : (4,0) - $703/543004$ Group 6 : (3,1) - $703/407253$ Now, the lottery board publishes statistics on how many winners there are in each group. For instance, during the last draw, the number of winners were as follows: $(1,7,280,875,14347,21993)$. Using these numbers, what is a good way for me to estimate the number of tickets bought, with a confidence interval? If I just look at the information from one set of probabilities and number of winners, I will be able to get an estimate (for example, I would estimate that there were 8145060 tickets bought if I had only considered Group 1 prizes). How can I combine all the information I have using Bayesian statistics to generate a better estimate? Currently, I'm looking at treating the number of winning tickets in a given group as a Binomial Distribution, and then apply the Agresti-Coull interval, but I'm not sure how to combine all these intervals.",,"['probability', 'statistics', 'probability-distributions', 'lotteries']"
70,limit of an integral of a copula density function,limit of an integral of a copula density function,,"let's say I have a copula density function which I denote as $c(x,y)$. $X$ and $Y$ are uniformly distributed RVs. I am curious if the following limit exists: $\lim_{u\rightarrow 1^{-}} \int_0^u c(u,y) dy$. I have been told that it does not as $c(1,\cdot)$ is not defined while on the other hand $\int_0^u c(u,y) dy = P(Y \leq u|X=u)$ and that $\lim_{u \rightarrow 1} P(Y \leq u|X=u) = 1$. Which one holds merit?","let's say I have a copula density function which I denote as $c(x,y)$. $X$ and $Y$ are uniformly distributed RVs. I am curious if the following limit exists: $\lim_{u\rightarrow 1^{-}} \int_0^u c(u,y) dy$. I have been told that it does not as $c(1,\cdot)$ is not defined while on the other hand $\int_0^u c(u,y) dy = P(Y \leq u|X=u)$ and that $\lim_{u \rightarrow 1} P(Y \leq u|X=u) = 1$. Which one holds merit?",,"['probability', 'statistics']"
71,"Finding an efficient estimator for $ \beta $ in a sample of $ n $ random variables having the $ \text{Gamma}(\alpha,\beta) $-distribution.",Finding an efficient estimator for  in a sample of  random variables having the -distribution.," \beta   n   \text{Gamma}(\alpha,\beta) ","Problem: Suppose that we have i.i.d. random variables $ X_{1},\dots,X_{n} \sim \text{Gamma}(\alpha,\beta) $, where $ \alpha > 0 $ is known. Find an efficient estimator for $ \beta $. Recall that the probability density function of the $ \text{Gamma}(\alpha,\beta) $-distribution is given by $$ \forall x > 0: \quad f(x;\alpha,\beta) = \frac{1}{\Gamma(\alpha) \beta^{\alpha}} \cdot x^{\alpha - 1} e^{- x/\beta}. $$ I am a little lost, but I am guessing that I need to find the Cramér-Rao Lower Bound (CRLB), look for an unbiased estimator and then compare it to the CRLB. Any help would be greatly appreciated.","Problem: Suppose that we have i.i.d. random variables $ X_{1},\dots,X_{n} \sim \text{Gamma}(\alpha,\beta) $, where $ \alpha > 0 $ is known. Find an efficient estimator for $ \beta $. Recall that the probability density function of the $ \text{Gamma}(\alpha,\beta) $-distribution is given by $$ \forall x > 0: \quad f(x;\alpha,\beta) = \frac{1}{\Gamma(\alpha) \beta^{\alpha}} \cdot x^{\alpha - 1} e^{- x/\beta}. $$ I am a little lost, but I am guessing that I need to find the Cramér-Rao Lower Bound (CRLB), look for an unbiased estimator and then compare it to the CRLB. Any help would be greatly appreciated.",,['statistics']
72,average waiting time,average waiting time,,"who can help me to resolution of this statistic exercise? below the track: Caio go in a bank,the number of customers ahead him are described by a Poisson random variable of parameter a>0. Calculate the average waiting time knowing that: -the waiting time is  given by the sum of service time of single person. -the timing of customer service that precede it are modeled as random variables, independent, marginally exponential of parameter lambda >0. ///// I thought that average waiting time is given by theorem of conditional mean: E[X]=E[E[X|Y]]; then call: Ta average waiting time -> (Ta=Summation of Ts) ,Ts time service customer ,X number of customer. E[Ta]=E[E[Ta|X]] is right? What will i do now?. Thank all!","who can help me to resolution of this statistic exercise? below the track: Caio go in a bank,the number of customers ahead him are described by a Poisson random variable of parameter a>0. Calculate the average waiting time knowing that: -the waiting time is  given by the sum of service time of single person. -the timing of customer service that precede it are modeled as random variables, independent, marginally exponential of parameter lambda >0. ///// I thought that average waiting time is given by theorem of conditional mean: E[X]=E[E[X|Y]]; then call: Ta average waiting time -> (Ta=Summation of Ts) ,Ts time service customer ,X number of customer. E[Ta]=E[E[Ta|X]] is right? What will i do now?. Thank all!",,"['probability', 'statistics']"
73,Question of maximum likelihood estimation.,Question of maximum likelihood estimation.,,"A population density function is definded as $$ f(s)=\begin{cases}  (W-1)s^{-W} & s\geq 1, \\ 0& \text{elsewhere}, \end{cases} $$ where $W>1$ is unknown. I just want to ask, how do i find the maximum likelihood estimator of $W$? Do I do the normal differentiation of this function first, and then do a 2nd order differentiation? Thanks.","A population density function is definded as $$ f(s)=\begin{cases}  (W-1)s^{-W} & s\geq 1, \\ 0& \text{elsewhere}, \end{cases} $$ where $W>1$ is unknown. I just want to ask, how do i find the maximum likelihood estimator of $W$? Do I do the normal differentiation of this function first, and then do a 2nd order differentiation? Thanks.",,['statistics']
74,Why was I wrong doing this problem?,Why was I wrong doing this problem?,,"""A waste disposal company averages $6.5$ spills of toxic waste per month. Assume spills occur randomly at a uniform rate, and independently of each other, with a negligible chance of $2$ or more occurring at the same time. Find the probability there are $4$ or more spills in a $2$ month period."" The way I did it was to first say ""the probability of a spill on a random selected day is $\frac{65}{300}$ (assuming $30$-day months)"". Then I calculated the probability that there will be $3$ or $2$ or $1$ or no spills over $600$ days, then I subtracted it from 1. I got a tiny answer (a few percent), but the correct solution was $0.9989$. What did I do wrong, and why is the correct answer $0.9989$?","""A waste disposal company averages $6.5$ spills of toxic waste per month. Assume spills occur randomly at a uniform rate, and independently of each other, with a negligible chance of $2$ or more occurring at the same time. Find the probability there are $4$ or more spills in a $2$ month period."" The way I did it was to first say ""the probability of a spill on a random selected day is $\frac{65}{300}$ (assuming $30$-day months)"". Then I calculated the probability that there will be $3$ or $2$ or $1$ or no spills over $600$ days, then I subtracted it from 1. I got a tiny answer (a few percent), but the correct solution was $0.9989$. What did I do wrong, and why is the correct answer $0.9989$?",,"['probability', 'statistics']"
75,Group mean vector and common group covariance matrix in discriminant analysis?,Group mean vector and common group covariance matrix in discriminant analysis?,,"I'm looking over the answers for old statistics exams in preparation for my own exam, while reviewing a question concerning discriminant analysis I found something really odd. I have input variables and a class label for each observation x1 = [2, 10, 6, 14, 6, 10] x2 = [8, 6, 4, 10, 12, 8] t  = [1, 1, 1, 2, 2, 2] Normally for DA I would calculate the mean for each group (both groups have mean 8) however the exam asks for the 'group mean vectors' which according to the answers should be: u1 = [6, 6]  u2 = [10, 10] However I can't find anywhere how these were calculated. A Google search for 'group mean vector' turns up nothing and if I search for Discriminant Analysis methods all I find is using the means for each group (scalars not vectors). The second strange things happens in calculating the covariance matrix. Instead of calculating one covariance matrix for both groups, they ask for a ""common group covariance matrix"" (again Google is useless here, so strange!) Apparently you have to calculate a matrix per group first and then add these for both groups together (cell per cell) and divide each cell by 2. For x1 this 'group matrix' is: top_left = 0.5[(2-6)^2 + (6-6)^2 + (10-6)^2 = 16 bottom_right = 0.5[(8-6)^2 + (6-6)^2 + (4-6)^2 = 4 others = 0.5[(2-6)(8-6) + (10-6)(6-6) + (6-6)(4-6) = -4  =>  16 -4     -4  4 A couple of strange things here: The number 14, from data set x1 is never used! The number 8 does not appear in group x1 (but does in group 2, but no other numbers from group 2 are used). I'm at a loss: how is a 'group mean vector' calculated and how is the matrix for each individual group calculated?","I'm looking over the answers for old statistics exams in preparation for my own exam, while reviewing a question concerning discriminant analysis I found something really odd. I have input variables and a class label for each observation x1 = [2, 10, 6, 14, 6, 10] x2 = [8, 6, 4, 10, 12, 8] t  = [1, 1, 1, 2, 2, 2] Normally for DA I would calculate the mean for each group (both groups have mean 8) however the exam asks for the 'group mean vectors' which according to the answers should be: u1 = [6, 6]  u2 = [10, 10] However I can't find anywhere how these were calculated. A Google search for 'group mean vector' turns up nothing and if I search for Discriminant Analysis methods all I find is using the means for each group (scalars not vectors). The second strange things happens in calculating the covariance matrix. Instead of calculating one covariance matrix for both groups, they ask for a ""common group covariance matrix"" (again Google is useless here, so strange!) Apparently you have to calculate a matrix per group first and then add these for both groups together (cell per cell) and divide each cell by 2. For x1 this 'group matrix' is: top_left = 0.5[(2-6)^2 + (6-6)^2 + (10-6)^2 = 16 bottom_right = 0.5[(8-6)^2 + (6-6)^2 + (4-6)^2 = 4 others = 0.5[(2-6)(8-6) + (10-6)(6-6) + (6-6)(4-6) = -4  =>  16 -4     -4  4 A couple of strange things here: The number 14, from data set x1 is never used! The number 8 does not appear in group x1 (but does in group 2, but no other numbers from group 2 are used). I'm at a loss: how is a 'group mean vector' calculated and how is the matrix for each individual group calculated?",,"['statistics', 'data-analysis']"
76,Can a random function of a normal variable be independent of it,Can a random function of a normal variable be independent of it,,"I have two normal variables $X,Y$. Is it possible to have $X = f(Y,Z)$ for some non-trivial function $f$, an independent normal variable $Z$ and that $X$ will be independent of $Y$? Thanks.","I have two normal variables $X,Y$. Is it possible to have $X = f(Y,Z)$ for some non-trivial function $f$, an independent normal variable $Z$ and that $X$ will be independent of $Y$? Thanks.",,"['probability', 'statistics']"
77,Conjugate prior for noisy Bernoulli,Conjugate prior for noisy Bernoulli,,"It is well known that the Beta distribution serves as a conjugate prior for the Bernoulli distribution, and that when you observe a Bernouilli random variable, you need only increment the appropriate hyperparameter of the Beta distribution. However when the Bernoulli distribution is ""noisy"" in the sense that you do not observe the the Bernoulli random variable directly, but instead observe a random variable that is equal to the Bernoulli random variable with probability 1-p and flipped with probability p, where p is known, and represents an error rate in observing the Bernoulli random variable, the posterior distribution obtained is a linear combination of two Beta distributions. In the case when p=0, p=.5, and p=1, the posterior distribution is again Beta, but for other values of p, this is not the case. In my particular application analytic tractability is important. Is there a conjugate prior that would be appropriate for this type of problem? Failing that, it seems there might be a sensible way to update the hyperparameters of the Beta distribution in an approximate sense. Intuitively, when you observe a Bernoulli random variable with error rate p, the information you have about the parameter theta of the Bernoulli distribution is nothing when p=.5 (observations are completely uninformative) and maximum when p=0 or p=1, and somewhere in between for other values of p. More specifically, in the case when p=1 or p=0, the sum of the hyperparameters of the beta in the posterior distribution is 1 greater than the sum in the prior, and for p=.5, the sum of the hyperparameters remains the same. For other values of p, the change in the sum of the hyperparameters should be intermediate, but I'm not sure how to best choose them.","It is well known that the Beta distribution serves as a conjugate prior for the Bernoulli distribution, and that when you observe a Bernouilli random variable, you need only increment the appropriate hyperparameter of the Beta distribution. However when the Bernoulli distribution is ""noisy"" in the sense that you do not observe the the Bernoulli random variable directly, but instead observe a random variable that is equal to the Bernoulli random variable with probability 1-p and flipped with probability p, where p is known, and represents an error rate in observing the Bernoulli random variable, the posterior distribution obtained is a linear combination of two Beta distributions. In the case when p=0, p=.5, and p=1, the posterior distribution is again Beta, but for other values of p, this is not the case. In my particular application analytic tractability is important. Is there a conjugate prior that would be appropriate for this type of problem? Failing that, it seems there might be a sensible way to update the hyperparameters of the Beta distribution in an approximate sense. Intuitively, when you observe a Bernoulli random variable with error rate p, the information you have about the parameter theta of the Bernoulli distribution is nothing when p=.5 (observations are completely uninformative) and maximum when p=0 or p=1, and somewhere in between for other values of p. More specifically, in the case when p=1 or p=0, the sum of the hyperparameters of the beta in the posterior distribution is 1 greater than the sum in the prior, and for p=.5, the sum of the hyperparameters remains the same. For other values of p, the change in the sum of the hyperparameters should be intermediate, but I'm not sure how to best choose them.",,"['statistics', 'bayesian']"
78,How many citations to read before convergence?,How many citations to read before convergence?,,"So I have the following question assuming I start with N academic papers, though I was thinking to make this simple I start with one academic paper. And say it has C citations, and each one of these C citations, has their own D citations. Is there a way I could estimate when there would be a convergence of the number of citations. Basically when I would end up reading citations that I had already read? (So basically D would be one of the N papers) (The inspiration for this question is because I have to do a literature review in Bioengienering, and I was wondering, what depth I should go to to read relevant papers, that are cited by a relevant paper, that is sufficient to have done an in depth analysis of my sub field)","So I have the following question assuming I start with N academic papers, though I was thinking to make this simple I start with one academic paper. And say it has C citations, and each one of these C citations, has their own D citations. Is there a way I could estimate when there would be a convergence of the number of citations. Basically when I would end up reading citations that I had already read? (So basically D would be one of the N papers) (The inspiration for this question is because I have to do a literature review in Bioengienering, and I was wondering, what depth I should go to to read relevant papers, that are cited by a relevant paper, that is sufficient to have done an in depth analysis of my sub field)",,"['combinatorics', 'statistics', 'graph-theory', 'estimation', 'network']"
79,Probabilities for unknown finite population from sample?,Probabilities for unknown finite population from sample?,,If i have a known population ($N$ marbles of which $M$ are black) and draw $n$ samples without replacement the probability to draw $x$ black marbles is given by the hypergeometric distribution. Is there a way to get probabilities for the total number of blacks $M$ from the number of black marbles $x$ in my sample?,If i have a known population ($N$ marbles of which $M$ are black) and draw $n$ samples without replacement the probability to draw $x$ black marbles is given by the hypergeometric distribution. Is there a way to get probabilities for the total number of blacks $M$ from the number of black marbles $x$ in my sample?,,"['probability', 'statistics']"
80,Hypothesis test: Type I error,Hypothesis test: Type I error,,"I've been trying to understand the solution to this exercise: It is estimated that the proportion of adults living in a small town who are college graduates is $p = 0.6$. To test this hypothesis, we selected a random sample of $200$ adults. If the number of graduates in our sample is any number between $110\leq x\leq 130$, accept the null hypothesis that $p = 0.6$, otherwise, we conclude that $ p\neq 0.6$. Evaluate $\alpha$ ( Type I error ) with the assumption that $p = 0.6$. Use the normal distribution. My anwers is: \begin{align*} \alpha&= P(\text{Type I error})\\ &=P\left(z\leq \frac{109,5-200(0,6)}{\sqrt{200(0,6)(0,4)}}\right)+P\left(z\geq \frac{130,0-200(0,6)}{\sqrt{200(0,6)(0,4)}}\right)\\ &\approx2\cdot(0,0655)\\ &=0,131 \end{align*} Is this correct? I do not know why (and when) I have to use the values $​​109.5$ and $130.5$, because the theorem of normal approximation to the binomial does not say anything about it. Can anyone help? Thank you very much.","I've been trying to understand the solution to this exercise: It is estimated that the proportion of adults living in a small town who are college graduates is $p = 0.6$. To test this hypothesis, we selected a random sample of $200$ adults. If the number of graduates in our sample is any number between $110\leq x\leq 130$, accept the null hypothesis that $p = 0.6$, otherwise, we conclude that $ p\neq 0.6$. Evaluate $\alpha$ ( Type I error ) with the assumption that $p = 0.6$. Use the normal distribution. My anwers is: \begin{align*} \alpha&= P(\text{Type I error})\\ &=P\left(z\leq \frac{109,5-200(0,6)}{\sqrt{200(0,6)(0,4)}}\right)+P\left(z\geq \frac{130,0-200(0,6)}{\sqrt{200(0,6)(0,4)}}\right)\\ &\approx2\cdot(0,0655)\\ &=0,131 \end{align*} Is this correct? I do not know why (and when) I have to use the values $​​109.5$ and $130.5$, because the theorem of normal approximation to the binomial does not say anything about it. Can anyone help? Thank you very much.",,"['probability', 'statistics']"
81,Find expectation of conditional random variables,Find expectation of conditional random variables,,Let $X$ and $Y$ be independent exponential random variables with means 1 and 2 respectively. Let $Z = 2X + Y$. How can I find $E(X|Z)$?,Let $X$ and $Y$ be independent exponential random variables with means 1 and 2 respectively. Let $Z = 2X + Y$. How can I find $E(X|Z)$?,,"['probability', 'statistics', 'problem-solving']"
82,How do I show expectations according to this distribution?,How do I show expectations according to this distribution?,,"Let $A$ (or $X$) be $\log A \sim N(\mu,\sigma^2)$,   (lognormal distribution) I have to show $$E[A] = \exp[\mu + (\sigma^2/2)]\mbox{ and }E[A^2] = \exp[2\mu + 2\sigma^2].$$ Do I have to use mgf of the normal dist. ? It is easy to show E[$A^2$] since it is the second order derivative of the mgf.","Let $A$ (or $X$) be $\log A \sim N(\mu,\sigma^2)$,   (lognormal distribution) I have to show $$E[A] = \exp[\mu + (\sigma^2/2)]\mbox{ and }E[A^2] = \exp[2\mu + 2\sigma^2].$$ Do I have to use mgf of the normal dist. ? It is easy to show E[$A^2$] since it is the second order derivative of the mgf.",,"['probability', 'statistics', 'probability-distributions']"
83,Least squares estimator of mu,Least squares estimator of mu,,"The question is: Assuming that $y_i = \mu + \epsilon_i $,$i = 1,\ldots,n$ with independent and identically distributed errors $\epsilon_i$ such that $E[\epsilon_i] = 0$ and $Var[\epsilon_i] = \sigma^2$, find the least squares estimator of $\mu$. Find its variance. I'm not sure how to go about doing this. I know that the least squares bit means that I minimize the sum of the errors, and so I would have to use the formula: $$\sum_i (y_i - \mu)^2$$ and then differentiate (wrt to $\mu$?) and then let it equal 0. Is that correct? Once I've done this, I would I calculate its $E[\mu]$, because I don't have any definition for $\mu$. Or is $\mu = \beta_0 + \beta_1 \cdot x_i$? If it is, then isn't the estimator the same?","The question is: Assuming that $y_i = \mu + \epsilon_i $,$i = 1,\ldots,n$ with independent and identically distributed errors $\epsilon_i$ such that $E[\epsilon_i] = 0$ and $Var[\epsilon_i] = \sigma^2$, find the least squares estimator of $\mu$. Find its variance. I'm not sure how to go about doing this. I know that the least squares bit means that I minimize the sum of the errors, and so I would have to use the formula: $$\sum_i (y_i - \mu)^2$$ and then differentiate (wrt to $\mu$?) and then let it equal 0. Is that correct? Once I've done this, I would I calculate its $E[\mu]$, because I don't have any definition for $\mu$. Or is $\mu = \beta_0 + \beta_1 \cdot x_i$? If it is, then isn't the estimator the same?",,"['statistics', 'regression']"
84,"sale price, interest per annum and percentage increase or decrease","sale price, interest per annum and percentage increase or decrease",,Mary bought a computer on hire purchase. If the cash price on the computer was $3 million and 25% was down payment if bought on hire purchase. If the balance was paid in 12 monthly installment at 12% interest per annum. Find the percentage increase or decrease if computer was bought on cash price.,Mary bought a computer on hire purchase. If the cash price on the computer was $3 million and 25% was down payment if bought on hire purchase. If the balance was paid in 12 monthly installment at 12% interest per annum. Find the percentage increase or decrease if computer was bought on cash price.,,['statistics']
85,Binary Logistic Regression Model Processing,Binary Logistic Regression Model Processing,,"Thanks for showing interest and wanting to help out. My aim is to develop a model that - as accurately as possible - predicts how entities in a population will either cooperate or defect, as a % of total population. For this purpose, I have 70 predictor variables, however, not all of them may be significant (some are though). There could be a degree of multicollinearity for these variables. There are other variables that could potentially affect the outcome, but they are currently unknown. I have approximately 300 datapoints. So far, I have used the glmfit function in Matlab to create a binary logistic regression model for all predictor variables. Now, my statistics expertise is limited at best (I'm sorry about that), and I struggle to choose how to proceed at this point. I would very much appreciate if you could help me out with solving following questions in matlab: How do I best assess the accuracy of the model? Would it be better to reduce the number of predictor variables to improve the accuracy of the model? If so, how should I best do this? How do I check whether multicollinearity is significant? If so, what actions should I take to improve the model? What outputs/plots should I produce to demonstrate the above? Finally, is there a better way of doing things? I would very much appreciate your help. Sorry if some of this seems basic - I assure you I have read up on this, but I find myself unable to make an informed decision as to how I should proceed to obtain optimum results. Thank you very much for your time. EDIT: For example, would it be a good idea to look at the individual p-values for all the predictors and eliminate all those that fail a chosen significance level (say 0.05), then reconstructing the model with predictors that pass the test, and then see whether a better deviance (D) is obtained? How would I be able to judge whether the model is suitable, even if the deviance is better? Is there a better way of doing this? I just don't understand the maths behind these statistics well enough in order to choose an effective strategy. EDIT 2: Thanks to Zhiyong Wang, I have managed to do a LASSO on my data to discriminate predictor variables... I'm now down to 14. However, some of the p-values are still very high, and I'm not quite sure how I should continue to process my model. Please find below my diagnosis: Estimated Coefficients:                    Estimate      SE            tStat        (Intercept)       -9.3957       0.45246     -20.766     x2              0.0032055     0.0043646     0.73443     x3             -0.0095759     0.0022003      -4.352     x4              0.0023242    0.00090184      2.5772     x5              0.0033171      0.001955      1.6968     x7              0.0017115    0.00090373      1.8938     x9              0.0031377     0.0013612      2.3051     x11            0.00024809     0.0013823     0.17947     x16             0.0014808     0.0021081     0.70244     x22            -0.0017803     0.0014742     -1.2077     x26            -0.0025935     0.0045821    -0.56601     x35            -0.0077807      0.014286    -0.54464     x37            -0.0086488     0.0079046     -1.0942     x45            -0.0038264     0.0019328     -1.9797     x52            0.00032738     0.0043498    0.075264                      pValue         (Intercept)    8.7732e-96     x2                0.46269     x3              1.349e-05     x4              0.0099602     x5               0.089743     x7               0.058253     x9               0.021161     x11               0.85757     x16               0.48241     x22               0.22718     x26               0.57139     x35                 0.586     x37               0.27389     x45              0.047742     x52                  0.94   126 observations, 111 error degrees of freedom Dispersion: 1 Chi^2-statistic vs. constant model: 319, p-value = 1.51e-59 How do I best proceed from there? Thank you very much.","Thanks for showing interest and wanting to help out. My aim is to develop a model that - as accurately as possible - predicts how entities in a population will either cooperate or defect, as a % of total population. For this purpose, I have 70 predictor variables, however, not all of them may be significant (some are though). There could be a degree of multicollinearity for these variables. There are other variables that could potentially affect the outcome, but they are currently unknown. I have approximately 300 datapoints. So far, I have used the glmfit function in Matlab to create a binary logistic regression model for all predictor variables. Now, my statistics expertise is limited at best (I'm sorry about that), and I struggle to choose how to proceed at this point. I would very much appreciate if you could help me out with solving following questions in matlab: How do I best assess the accuracy of the model? Would it be better to reduce the number of predictor variables to improve the accuracy of the model? If so, how should I best do this? How do I check whether multicollinearity is significant? If so, what actions should I take to improve the model? What outputs/plots should I produce to demonstrate the above? Finally, is there a better way of doing things? I would very much appreciate your help. Sorry if some of this seems basic - I assure you I have read up on this, but I find myself unable to make an informed decision as to how I should proceed to obtain optimum results. Thank you very much for your time. EDIT: For example, would it be a good idea to look at the individual p-values for all the predictors and eliminate all those that fail a chosen significance level (say 0.05), then reconstructing the model with predictors that pass the test, and then see whether a better deviance (D) is obtained? How would I be able to judge whether the model is suitable, even if the deviance is better? Is there a better way of doing this? I just don't understand the maths behind these statistics well enough in order to choose an effective strategy. EDIT 2: Thanks to Zhiyong Wang, I have managed to do a LASSO on my data to discriminate predictor variables... I'm now down to 14. However, some of the p-values are still very high, and I'm not quite sure how I should continue to process my model. Please find below my diagnosis: Estimated Coefficients:                    Estimate      SE            tStat        (Intercept)       -9.3957       0.45246     -20.766     x2              0.0032055     0.0043646     0.73443     x3             -0.0095759     0.0022003      -4.352     x4              0.0023242    0.00090184      2.5772     x5              0.0033171      0.001955      1.6968     x7              0.0017115    0.00090373      1.8938     x9              0.0031377     0.0013612      2.3051     x11            0.00024809     0.0013823     0.17947     x16             0.0014808     0.0021081     0.70244     x22            -0.0017803     0.0014742     -1.2077     x26            -0.0025935     0.0045821    -0.56601     x35            -0.0077807      0.014286    -0.54464     x37            -0.0086488     0.0079046     -1.0942     x45            -0.0038264     0.0019328     -1.9797     x52            0.00032738     0.0043498    0.075264                      pValue         (Intercept)    8.7732e-96     x2                0.46269     x3              1.349e-05     x4              0.0099602     x5               0.089743     x7               0.058253     x9               0.021161     x11               0.85757     x16               0.48241     x22               0.22718     x26               0.57139     x35                 0.586     x37               0.27389     x45              0.047742     x52                  0.94   126 observations, 111 error degrees of freedom Dispersion: 1 Chi^2-statistic vs. constant model: 319, p-value = 1.51e-59 How do I best proceed from there? Thank you very much.",,"['statistics', 'matlab', 'regression', 'mathematical-modeling', 'binary']"
86,Statistics Question,Statistics Question,,"I'm revising for my exams and I want to check if I did this exercise correctly: 10 measurements were done using a certain tool. The average and standard deviations of measurements using a this tool are 0.4495 and 0.014 respectively. Test, using a 5% significance level, wether or not the average measurement value deviates from the true value 0.45 and interpret the result. What I have is: My null-hypothesis is ""the measurement doesn't deviate"" Alternative hypotheses: ""The measurement deviates"" My test statistic, assuming null hypothesis, is $$ T = \frac{0.4495-0.45}{0.014/\sqrt{10}}  \approx -0.113 $$ Now, $$ |T| = 0.113 < t_{9, 0.025} = 2.262 $$ This is not enough evidence to refute the null hypothesis, so it ends here? Also, I apologise should my English be sub-par, I'm not natively English so feel free to correct me.","I'm revising for my exams and I want to check if I did this exercise correctly: 10 measurements were done using a certain tool. The average and standard deviations of measurements using a this tool are 0.4495 and 0.014 respectively. Test, using a 5% significance level, wether or not the average measurement value deviates from the true value 0.45 and interpret the result. What I have is: My null-hypothesis is ""the measurement doesn't deviate"" Alternative hypotheses: ""The measurement deviates"" My test statistic, assuming null hypothesis, is $$ T = \frac{0.4495-0.45}{0.014/\sqrt{10}}  \approx -0.113 $$ Now, $$ |T| = 0.113 < t_{9, 0.025} = 2.262 $$ This is not enough evidence to refute the null hypothesis, so it ends here? Also, I apologise should my English be sub-par, I'm not natively English so feel free to correct me.",,['statistics']
87,is the approximation of the sum true?,is the approximation of the sum true?,,"Someone commented under my question Calculation of the moments using Hypergeometric distribution that  $$ \sum_{k=0}^l\frac{{l \choose k}{2n-l \choose n-k}(2k-l)^q}{{2n\choose n}}\sim \sum_{k=0}^l (2k-l)^q {l \choose k}. $$ I've tried to use the Stirling's approximation formula, but I did not get the result. Maybe I have mistake... So is this approximation is true? Thank you for your help.","Someone commented under my question Calculation of the moments using Hypergeometric distribution that  $$ \sum_{k=0}^l\frac{{l \choose k}{2n-l \choose n-k}(2k-l)^q}{{2n\choose n}}\sim \sum_{k=0}^l (2k-l)^q {l \choose k}. $$ I've tried to use the Stirling's approximation formula, but I did not get the result. Maybe I have mistake... So is this approximation is true? Thank you for your help.",,"['statistics', 'binomial-coefficients', 'approximation']"
88,Mathematical idea behind tax bracket,Mathematical idea behind tax bracket,,"This may be a lame question. The tax bracket is a way to calculate the tax based on the taxable income. For example Imagine that there are three tax brackets: 10%, 20%, and 30%. The 10%   rate applies to income from $\$$1 to $\$$10,000; the 20% rate applies   to income from $\$$10,001 to $\$$20,000; and the 30% rate applies to   all income above $\$$20,000. Under this system, someone earning $\$$10,000 would be taxed at a rate of   10%, paying a total of $\$$1,000. Someone earning $\$$5,000 would pay   $\$$500, and so on. Meanwhile, someone earning $\$$35,000 would face a more complicated   calculation. The rate on the first $\$$10,000 would be 10%; the rate   from $\$$10,001 to $\$$20,000 would be 20%; and the rate above that   would be 30%. Thus, he would pay $\$$1,000 for the first $\$$10,000 of   income; $\$$2,000 for the second $\$$10,000 of income; and $\$$4,500   for the last $\$$15,000 of income; in total, he would pay $\$$7,500,   or about 21.4%. In my understanding, the idea of tax bracket for computing tax is as follows: first fill the taxable income into the tax brackets in the order of  tax rates from low to high, then multiply the amount in each tax bracket, by the corresponding tax rate, finally sum up the the products over the tax brackets to obtain the tax. I was wondering if such an idea is also used (elsewhere) in mathematics, including statistics, mathematical modelling? If yes, in what situations is this idea used usually? Thanks! Added: After some further thought, I realized the following thing. What we called tax rate at a certain amount of income is actually the tax per unit increase from the income, also called marginal tax rate , so the tax rate is a function of the income, i.e. a function from $[0, \infty) \to [0,1]$. The tax for income $X$ is calculated as the integral of the tax rate over $[0, X]$.","This may be a lame question. The tax bracket is a way to calculate the tax based on the taxable income. For example Imagine that there are three tax brackets: 10%, 20%, and 30%. The 10%   rate applies to income from $\$$1 to $\$$10,000; the 20% rate applies   to income from $\$$10,001 to $\$$20,000; and the 30% rate applies to   all income above $\$$20,000. Under this system, someone earning $\$$10,000 would be taxed at a rate of   10%, paying a total of $\$$1,000. Someone earning $\$$5,000 would pay   $\$$500, and so on. Meanwhile, someone earning $\$$35,000 would face a more complicated   calculation. The rate on the first $\$$10,000 would be 10%; the rate   from $\$$10,001 to $\$$20,000 would be 20%; and the rate above that   would be 30%. Thus, he would pay $\$$1,000 for the first $\$$10,000 of   income; $\$$2,000 for the second $\$$10,000 of income; and $\$$4,500   for the last $\$$15,000 of income; in total, he would pay $\$$7,500,   or about 21.4%. In my understanding, the idea of tax bracket for computing tax is as follows: first fill the taxable income into the tax brackets in the order of  tax rates from low to high, then multiply the amount in each tax bracket, by the corresponding tax rate, finally sum up the the products over the tax brackets to obtain the tax. I was wondering if such an idea is also used (elsewhere) in mathematics, including statistics, mathematical modelling? If yes, in what situations is this idea used usually? Thanks! Added: After some further thought, I realized the following thing. What we called tax rate at a certain amount of income is actually the tax per unit increase from the income, also called marginal tax rate , so the tax rate is a function of the income, i.e. a function from $[0, \infty) \to [0,1]$. The tax for income $X$ is calculated as the integral of the tax rate over $[0, X]$.",,"['statistics', 'mathematical-modeling']"
89,Probability that X<Y for X and Y with known mean and standard deviation,Probability that X<Y for X and Y with known mean and standard deviation,,"I have physically measured two random variables X and Y and determined their respective mean  and standard deviation. Both variables have a gaussian distribution. Now I wish to calculate the probability that a given value of X will be less than a given value of Y. For example, if variable X has a mean of 100 and a standard deviation of 10 and variable Y has a mean of 120 and a standard deviation of 15, what is the probability of X being less than Y given Y=120?","I have physically measured two random variables X and Y and determined their respective mean  and standard deviation. Both variables have a gaussian distribution. Now I wish to calculate the probability that a given value of X will be less than a given value of Y. For example, if variable X has a mean of 100 and a standard deviation of 10 and variable Y has a mean of 120 and a standard deviation of 15, what is the probability of X being less than Y given Y=120?",,"['statistics', 'probability-distributions']"
90,An equation that occurs in ecology,An equation that occurs in ecology,,"In the field of ecology, a well known relation between the number of species and the size of an island can be approximated by a power function of the form: $S = c  A^z$ $S$ = number of species $c$ = a fitted constant $A$ = area of the island $z$ = a constant equal to $\log_{10}(S) / \log_{10}(A)$ Variable $z$ is, in fact, the slope of the linear relation between $\log_{10}(S)$ and $\log_{10}(A)$.  I’ve never encountered a multivariate equation where one of the independent variables consists of the slope of a linear relation where a non-linear transform has been applied to the major independent and dependent variable. I’m curious if the previously mentioned equation belongs to a more general class of equations?  Is there a special name for these types of equations?","In the field of ecology, a well known relation between the number of species and the size of an island can be approximated by a power function of the form: $S = c  A^z$ $S$ = number of species $c$ = a fitted constant $A$ = area of the island $z$ = a constant equal to $\log_{10}(S) / \log_{10}(A)$ Variable $z$ is, in fact, the slope of the linear relation between $\log_{10}(S)$ and $\log_{10}(A)$.  I’ve never encountered a multivariate equation where one of the independent variables consists of the slope of a linear relation where a non-linear transform has been applied to the major independent and dependent variable. I’m curious if the previously mentioned equation belongs to a more general class of equations?  Is there a special name for these types of equations?",,['statistics']
91,Formal definition of maximum likelihood estimation,Formal definition of maximum likelihood estimation,,"We typically introduce maximum likelihood by considering a family of distributions $\{P_\theta: \theta \in \Theta\}$ which admit densities $f_\theta$ with respect to some underlying measure $\lambda$. We observe a random vector $X \sim P_{\theta_0}$ where $\theta_0$ intuitively represents the ""true, unknown"" value of $\theta$. The maximum likelihood estimator of $\theta_0$ is then defined to be $\hat \theta = \arg \max_\theta f(X|\theta)$. How do we resolve the fact that $f_\theta$ is only unique almost surely? Normally we write these things off, but it actually matters here. A typical example is $X_1, X_2 \sim \mbox{Uniform}(0, \theta), \theta > 0$. Depending on whether we take $f(x|\theta) \propto I[0 \le x \le \theta]$ or $I[0 < x < \theta]$ affects what the estimator is. If it is the former we have $\hat \theta = \max\{X_1, X_2\}$ whereas with the latter the MLE doesn't exist. They are both valid densities for a Uniform$(0, \theta)$ but provide different answers. It seems natural in this case to prefer $I[0 \le x \le \theta]$ since, in this case, the density is positive and continuous when restricted to the support but I find it distaseful to define things in terms of densities instead of distributions. My two thoughts for solutions are (1) maybe once we have specified $\{f_\theta: \theta \in \Theta\}$, provided that the MLE exists, it may be the case that it is unique a.s. and (2) maybe we just bite the bullet and take the most natural family of densities for the problem at hand; when we want to make use of theoretical results, we make sure to choose $f_\theta$ so that it satisfies the required regularity conditions. I'm unsure whether this is better for stats.stackexchange or here since it's statistics but focuses on the mathematical formalism more than statisticians prefer to.","We typically introduce maximum likelihood by considering a family of distributions $\{P_\theta: \theta \in \Theta\}$ which admit densities $f_\theta$ with respect to some underlying measure $\lambda$. We observe a random vector $X \sim P_{\theta_0}$ where $\theta_0$ intuitively represents the ""true, unknown"" value of $\theta$. The maximum likelihood estimator of $\theta_0$ is then defined to be $\hat \theta = \arg \max_\theta f(X|\theta)$. How do we resolve the fact that $f_\theta$ is only unique almost surely? Normally we write these things off, but it actually matters here. A typical example is $X_1, X_2 \sim \mbox{Uniform}(0, \theta), \theta > 0$. Depending on whether we take $f(x|\theta) \propto I[0 \le x \le \theta]$ or $I[0 < x < \theta]$ affects what the estimator is. If it is the former we have $\hat \theta = \max\{X_1, X_2\}$ whereas with the latter the MLE doesn't exist. They are both valid densities for a Uniform$(0, \theta)$ but provide different answers. It seems natural in this case to prefer $I[0 \le x \le \theta]$ since, in this case, the density is positive and continuous when restricted to the support but I find it distaseful to define things in terms of densities instead of distributions. My two thoughts for solutions are (1) maybe once we have specified $\{f_\theta: \theta \in \Theta\}$, provided that the MLE exists, it may be the case that it is unique a.s. and (2) maybe we just bite the bullet and take the most natural family of densities for the problem at hand; when we want to make use of theoretical results, we make sure to choose $f_\theta$ so that it satisfies the required regularity conditions. I'm unsure whether this is better for stats.stackexchange or here since it's statistics but focuses on the mathematical formalism more than statisticians prefer to.",,['statistics']
92,What's the expected value in this jackpot winning experiment.,What's the expected value in this jackpot winning experiment.,,"If there exists a fair National Lottery, that someone bets £1, Jackpot increases by £1, and there is p chance that he wins a Jackpot. If a Jackpot is won, it is reset to 0. repeats. We can easily model this as a Bernoulli trials and the average Jackpot size is 1/p. Now, if we observe that there are £M bets staked on the National Lottery, and it is won n times. What's the average Jackpot size in this case?","If there exists a fair National Lottery, that someone bets £1, Jackpot increases by £1, and there is p chance that he wins a Jackpot. If a Jackpot is won, it is reset to 0. repeats. We can easily model this as a Bernoulli trials and the average Jackpot size is 1/p. Now, if we observe that there are £M bets staked on the National Lottery, and it is won n times. What's the average Jackpot size in this case?",,"['statistics', 'probability-distributions']"
93,Use Pearson's correlation coefficient on a matrix,Use Pearson's correlation coefficient on a matrix,,"I have a problem to interpret the following formula which is said to be the Pearson's correlation coefficient: $$r = \frac{N \left(\sum XY\right) - \left(\sum X\right) \left(\sum Y\right)}{\sqrt{\left[N \left(\sum X^2\right) - \left(\sum X\right)^2\right] \left[N \left(\sum Y^2\right) - \left(\sum Y\right)^2\right]}}$$ It is from Mining a Web Citation Database for author co-citation analysis (p.7). I have problems with its interpretation, since the authors says $X$ and $Y$ are vectors with length $N + 1$ and the product of two column vectors is not defined, at least not normally, isn't it? I have found a similiar notation of this formular on this Wikipedia article . Here, the formula does not take vectors as arguments, but a series of $n$ measurements with $x_i$ and $y_i$, where $i = 1,2,\dots,n$. I have problems to combine both formulas and understand what my calculations should look like when applying it. Maybe an example would help: Let's take this two vectors: $$X = (0,0.5,0,0)$$ $$Y = (0.5,0,0,0)$$ with $N = 3$ which would be from this matrix: $$\begin{pmatrix} 0 & 0.5 & 0 & 0\\ 0.5 & 0 & 0 & 0\\ 0 & 0 & 0 & 0\\ 0 & 0 & 0 & 0\\ \end{pmatrix}$$","I have a problem to interpret the following formula which is said to be the Pearson's correlation coefficient: $$r = \frac{N \left(\sum XY\right) - \left(\sum X\right) \left(\sum Y\right)}{\sqrt{\left[N \left(\sum X^2\right) - \left(\sum X\right)^2\right] \left[N \left(\sum Y^2\right) - \left(\sum Y\right)^2\right]}}$$ It is from Mining a Web Citation Database for author co-citation analysis (p.7). I have problems with its interpretation, since the authors says $X$ and $Y$ are vectors with length $N + 1$ and the product of two column vectors is not defined, at least not normally, isn't it? I have found a similiar notation of this formular on this Wikipedia article . Here, the formula does not take vectors as arguments, but a series of $n$ measurements with $x_i$ and $y_i$, where $i = 1,2,\dots,n$. I have problems to combine both formulas and understand what my calculations should look like when applying it. Maybe an example would help: Let's take this two vectors: $$X = (0,0.5,0,0)$$ $$Y = (0.5,0,0,0)$$ with $N = 3$ which would be from this matrix: $$\begin{pmatrix} 0 & 0.5 & 0 & 0\\ 0.5 & 0 & 0 & 0\\ 0 & 0 & 0 & 0\\ 0 & 0 & 0 & 0\\ \end{pmatrix}$$",,"['linear-algebra', 'statistics', 'correlation']"
94,Odds of getting specific color of Jelly Beans in a handful?,Odds of getting specific color of Jelly Beans in a handful?,,"I have a bag of jelly beans with approx 1190 Jelly Belly's in it. There are 50 different flavors. Assuming the amount of Jelly Belly's per flavor are equal (so, 23.8 of each bean): If I pull 6 Jelly Beans from my unopened bag, what are the odds that 3 of them will be the same color? I'm asking because I got a bag of jelly beans for Christmas and got 3 of the same color in the handful I just pulled out... and it's been many many years since my statistics class in college. Thank you for the help. It's driving me batty and nobody at work cares about my Jelly Belly question except me =( I would have tried to figure this out on my own as it seems very easy, but I don't even know where to begin looking.","I have a bag of jelly beans with approx 1190 Jelly Belly's in it. There are 50 different flavors. Assuming the amount of Jelly Belly's per flavor are equal (so, 23.8 of each bean): If I pull 6 Jelly Beans from my unopened bag, what are the odds that 3 of them will be the same color? I'm asking because I got a bag of jelly beans for Christmas and got 3 of the same color in the handful I just pulled out... and it's been many many years since my statistics class in college. Thank you for the help. It's driving me batty and nobody at work cares about my Jelly Belly question except me =( I would have tried to figure this out on my own as it seems very easy, but I don't even know where to begin looking.",,"['probability', 'statistics']"
95,MLE for independent Poisson distributions with different mean variable,MLE for independent Poisson distributions with different mean variable,,"Assume that $X_i \sim \text{Poisson}(\lambda^i)$, then we want to find the maximum likelihood estimate (MLE) of $\lambda$ and its asymptotics. I did in the following way, but got stuck here. Since $\mathbb{P}(X_i=x_i)=e^{\lambda^i}\frac{\lambda^{i x_i}}{x_i!}$. Then the likelihood is $\mathcal L(\lambda;X)=\prod_i^n e^{\lambda^i}\frac{\lambda^{i x_i}}{x_i!}$. And the loglikelihood is $\ell(\lambda;X)=C-\sum_i^n \lambda^i+(\log\lambda)\sum_{i=1}^n ix_i$. By taking derivative w.r.t. $ \lambda$, I got $-\sum_i^n i\lambda^{i-1}+\frac{\sum_{i=1}^n ix_i}{\lambda}=0$, i.e. $\sum_i^n i \lambda^i=\sum_{i=1}^n ix_i$. But I don't know how to proceed to find the MLE of $\lambda$, then.","Assume that $X_i \sim \text{Poisson}(\lambda^i)$, then we want to find the maximum likelihood estimate (MLE) of $\lambda$ and its asymptotics. I did in the following way, but got stuck here. Since $\mathbb{P}(X_i=x_i)=e^{\lambda^i}\frac{\lambda^{i x_i}}{x_i!}$. Then the likelihood is $\mathcal L(\lambda;X)=\prod_i^n e^{\lambda^i}\frac{\lambda^{i x_i}}{x_i!}$. And the loglikelihood is $\ell(\lambda;X)=C-\sum_i^n \lambda^i+(\log\lambda)\sum_{i=1}^n ix_i$. By taking derivative w.r.t. $ \lambda$, I got $-\sum_i^n i\lambda^{i-1}+\frac{\sum_{i=1}^n ix_i}{\lambda}=0$, i.e. $\sum_i^n i \lambda^i=\sum_{i=1}^n ix_i$. But I don't know how to proceed to find the MLE of $\lambda$, then.",,['statistics']
96,A question about finding the convergence of a distribution,A question about finding the convergence of a distribution,,"Let $X$ have the Gamma$(s,1)$ and given $X=x$, let $Y$ have the Possion distribution with parameter $x$. Show that $$\frac{Y-E(Y)}{\sqrt{\operatorname{var}(Y)}}\longrightarrow W$$ where $\longrightarrow$ means converges in distribution as $s$ goes to infinity. And $W$ needs to be identified. I have worked out the moment generating function of $Y$, $$ M_Y(t)=\left(\frac{1}{2-e^{t}}\right)^s$$ Then I work out the mgf of $\frac{Y-E(Y)}{\sqrt{\operatorname{var}(Y)}}$, $$ M(t)=e^{-\frac{s}{\sqrt{2s}}t}\left(\frac{1}{2-e^{\frac{t}{\sqrt{2s}}}}\right)^s$$ But I don't know what does it converges to. Anything wrong with my above calculation? Thanks.","Let $X$ have the Gamma$(s,1)$ and given $X=x$, let $Y$ have the Possion distribution with parameter $x$. Show that $$\frac{Y-E(Y)}{\sqrt{\operatorname{var}(Y)}}\longrightarrow W$$ where $\longrightarrow$ means converges in distribution as $s$ goes to infinity. And $W$ needs to be identified. I have worked out the moment generating function of $Y$, $$ M_Y(t)=\left(\frac{1}{2-e^{t}}\right)^s$$ Then I work out the mgf of $\frac{Y-E(Y)}{\sqrt{\operatorname{var}(Y)}}$, $$ M(t)=e^{-\frac{s}{\sqrt{2s}}t}\left(\frac{1}{2-e^{\frac{t}{\sqrt{2s}}}}\right)^s$$ But I don't know what does it converges to. Anything wrong with my above calculation? Thanks.",,"['statistics', 'probability-distributions']"
97,How to find the mode of a continuous distribution from a sample?,How to find the mode of a continuous distribution from a sample?,,"First, my background is not math. My objective is to find the value that occurs most frequently in a data sample OR the value that is most likely. Let's say my sample is [1,5,6,6,7,10]. Finding the mode for this sample is simple (the mode is 6). But if let's say I change the sample to [1,5,6,7,10], I don't know how to find the mode. The results that I want is 6 since 6 is the most probable data. Problem is, I don't even know what to google (tried for hours), and even when I do find something that MAY be the answer (kernel density estimation, continuous probability distribution), I don't understand what the hell they're talking about. The actual situation consist of hundreds of data (in floats) that are saved in Excel. I would appreciate if someone could demo it in Excel.","First, my background is not math. My objective is to find the value that occurs most frequently in a data sample OR the value that is most likely. Let's say my sample is [1,5,6,6,7,10]. Finding the mode for this sample is simple (the mode is 6). But if let's say I change the sample to [1,5,6,7,10], I don't know how to find the mode. The results that I want is 6 since 6 is the most probable data. Problem is, I don't even know what to google (tried for hours), and even when I do find something that MAY be the answer (kernel density estimation, continuous probability distribution), I don't understand what the hell they're talking about. The actual situation consist of hundreds of data (in floats) that are saved in Excel. I would appreciate if someone could demo it in Excel.",,"['probability', 'statistics']"
98,Dependence of certain random variables,Dependence of certain random variables,,"Consider $X_1,X_2$ i.i.d. standard normal random variables(mean 0, variance 1). Are the random variables $Y=X_1+X_2$ and $Z=X_1-X_2$ dependent? I am not sure how to prove this one way or the other.","Consider $X_1,X_2$ i.i.d. standard normal random variables(mean 0, variance 1). Are the random variables $Y=X_1+X_2$ and $Z=X_1-X_2$ dependent? I am not sure how to prove this one way or the other.",,"['probability', 'statistics', 'probability-distributions']"
99,Using Correlation for mouse gesture recognition,Using Correlation for mouse gesture recognition,,"I am in need to build a mouse gesture recognition system which will compare given recognition to the the gestures in training data and will say where a given gesture best fits. I am planning to use correlation to accomplish this. I would run Correlation on given input against all the gestures in training data and will select the action associated to the gesture with best correlation co-efficient (and cross a threshold). I am not sure how robust correlation is for this purpose, so need your insight into this.  Also please suggest if you think I should better be using something other than Correlation... Regards, Microkernel PS: I am more of a programmer than a mathematician :(","I am in need to build a mouse gesture recognition system which will compare given recognition to the the gestures in training data and will say where a given gesture best fits. I am planning to use correlation to accomplish this. I would run Correlation on given input against all the gestures in training data and will select the action associated to the gesture with best correlation co-efficient (and cross a threshold). I am not sure how robust correlation is for this purpose, so need your insight into this.  Also please suggest if you think I should better be using something other than Correlation... Regards, Microkernel PS: I am more of a programmer than a mathematician :(",,"['statistics', 'linear-programming', 'correlation', 'pattern-recognition']"
