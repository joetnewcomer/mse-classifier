,title_raw,title_text,title_latex,body_raw,body_text,body_latex,tags
0,Is there a non-decreasing sequence $(a_n)$ such that $\sum 1/a_n=\infty$ and $\sum1/(n+a_n)<\infty$?,Is there a non-decreasing sequence  such that  and ?,(a_n) \sum 1/a_n=\infty \sum1/(n+a_n)<\infty,"This question is motivated by this one . In the accepted answer, two positive non-decreasing sequences $(a_n)$ and $(b_n)$ are given such that $$ \sum_{n=1}^\infty\frac{1}{a_n}=\sum_{n=1}^\infty\frac{1}{b_n}=\infty,\quad\text{but}\quad \sum_{n=1}^\infty\frac{1}{a_n+b_n}<\infty. $$ Now take $b_n=n$. Is there a positive non-decreasing sequence $(a_n)$ such that   $$ \sum_{n=1}^\infty\frac{1}{a_n}=\infty,\quad\text{but}\quad \sum_{n=1}^\infty\frac{1}{n+a_n}<\infty\,? $$ Some remarks: If $a,b>0$, then $\max(a,b)\le a+b\le2\max(a,b)$. Thus $$ \sum_{n=1}^\infty\frac{1}{n+a_n}<\infty\quad\text{is equivalent to}\quad\sum_{n=1}^\infty\frac{1}{\max(n,a_n)}<\infty $$ If we eliminate the condition that $(a_n)$ be non-decreasing, it is easy to find an example, like $a_n=n^2$ if $n$ is not a squere, $a_n=\sqrt{n}$ is $n$ is a square. If such a sequence existes, it must satisfy $$ \sup\frac{a_n}{n}=\sup\frac{n}{a_n}=\infty. $$","This question is motivated by this one . In the accepted answer, two positive non-decreasing sequences $(a_n)$ and $(b_n)$ are given such that $$ \sum_{n=1}^\infty\frac{1}{a_n}=\sum_{n=1}^\infty\frac{1}{b_n}=\infty,\quad\text{but}\quad \sum_{n=1}^\infty\frac{1}{a_n+b_n}<\infty. $$ Now take $b_n=n$. Is there a positive non-decreasing sequence $(a_n)$ such that   $$ \sum_{n=1}^\infty\frac{1}{a_n}=\infty,\quad\text{but}\quad \sum_{n=1}^\infty\frac{1}{n+a_n}<\infty\,? $$ Some remarks: If $a,b>0$, then $\max(a,b)\le a+b\le2\max(a,b)$. Thus $$ \sum_{n=1}^\infty\frac{1}{n+a_n}<\infty\quad\text{is equivalent to}\quad\sum_{n=1}^\infty\frac{1}{\max(n,a_n)}<\infty $$ If we eliminate the condition that $(a_n)$ be non-decreasing, it is easy to find an example, like $a_n=n^2$ if $n$ is not a squere, $a_n=\sqrt{n}$ is $n$ is a square. If such a sequence existes, it must satisfy $$ \sup\frac{a_n}{n}=\sup\frac{n}{a_n}=\infty. $$",,"['real-analysis', 'sequences-and-series']"
1,Limit of measurable functions is measurable?,Limit of measurable functions is measurable?,,"Suppose $(\Omega, \cal F)$ is a measurable space and $(X, \mathcal B_X)$ is a topological space with its Borel sigma algebra. If $f_n: \Omega \to X$ is a sequence of $(\cal F , B$$_X)$-measurable functions and if $f_n \to f$ pointwise, then is it true that $f$ is $(\cal F , B$$_X)$-measurable? Of course, we know it is true if $X = \Bbb R$ with the usual topology. This is just a standard result in real analysis which can be proved easily using the order structure of $\Bbb R$. I am more interested in what happens when $X$ is not some Euclidean Space. I claim it is still true for metrizable $X$. Indeed, supposes $d$ induces the topology of $X$, and $C \subset X$ is closed. For $\varepsilon >0$, let $C_{\varepsilon} = \{x \in X: d(x,C) < \varepsilon \}$, which is open. Then $$f^{-1}(C) = \bigcap_{n \in \Bbb N} \bigcup_{N \in \Bbb N} \bigcap_{ k \geq N} f_k^{-1}\big(C_{2^{-n}}\big)$$ which is in $\cal F$. Since preimages of closed sets are in $\cal F$, it easily follows $f$ is $(\cal F, B$$_X)$-measurable. I guess the crucial thing here was that any closed set in a metrizable space is $G_{\delta}$. Does the result still hold for any first countable Hausdorff space? What about uniformizable spaces? I guess the answer would probably be no if $X$ is not Hausdorff since the limit function wouldn't necessarily be unique. I doubt this would be a useful thing to know, but I'm curious nonetheless.","Suppose $(\Omega, \cal F)$ is a measurable space and $(X, \mathcal B_X)$ is a topological space with its Borel sigma algebra. If $f_n: \Omega \to X$ is a sequence of $(\cal F , B$$_X)$-measurable functions and if $f_n \to f$ pointwise, then is it true that $f$ is $(\cal F , B$$_X)$-measurable? Of course, we know it is true if $X = \Bbb R$ with the usual topology. This is just a standard result in real analysis which can be proved easily using the order structure of $\Bbb R$. I am more interested in what happens when $X$ is not some Euclidean Space. I claim it is still true for metrizable $X$. Indeed, supposes $d$ induces the topology of $X$, and $C \subset X$ is closed. For $\varepsilon >0$, let $C_{\varepsilon} = \{x \in X: d(x,C) < \varepsilon \}$, which is open. Then $$f^{-1}(C) = \bigcap_{n \in \Bbb N} \bigcup_{N \in \Bbb N} \bigcap_{ k \geq N} f_k^{-1}\big(C_{2^{-n}}\big)$$ which is in $\cal F$. Since preimages of closed sets are in $\cal F$, it easily follows $f$ is $(\cal F, B$$_X)$-measurable. I guess the crucial thing here was that any closed set in a metrizable space is $G_{\delta}$. Does the result still hold for any first countable Hausdorff space? What about uniformizable spaces? I guess the answer would probably be no if $X$ is not Hausdorff since the limit function wouldn't necessarily be unique. I doubt this would be a useful thing to know, but I'm curious nonetheless.",,"['real-analysis', 'general-topology', 'analysis', 'measure-theory', 'metric-spaces']"
2,Uniform $L^p$ bound on finite measure implies uniform integrability,Uniform  bound on finite measure implies uniform integrability,L^p,"Suppose that $X$ have finite measure, let $1<p<\infty$ , and suppose that $f_n \colon X\rightarrow \mathbb{R}$ is a sequence of measurable functions such that $\sup_n \int_X |f_n|^p d\mu < \infty$ . Show that the sequence $f_n$ is uniformly integrable. In another word, show that $$\sup_n \int_X |f_n|^p d\mu < \infty \;\;\;\Longrightarrow\;\;\; \sup_n \int_{|f_n|>M} |f_n| d\mu \rightarrow 0 \text{ when } M\rightarrow \infty.$$ I have tried using contradiction, but I am not sure how to use the power $p$ in the problem. I can see why it would work, in some sense, the power $p$ gets rid of the case often called ""escape to vertical infinity"". For example, define $f_n :[0,1] \rightarrow \mathbb{R}$ with $f_n = n\chi_{[0,\frac{1}{n}]} $ , without the power $p$ , we have $$\sup_n \int_X |f_n| d\mu =1 \;\;\text{ and }\;\; \sup_n \int_{|f_n|>M} |f_n| d\mu =1 \text{ for each } M,$$ but $$\sup_n \int_X |f_n|^p d\mu = \infty$$","Suppose that have finite measure, let , and suppose that is a sequence of measurable functions such that . Show that the sequence is uniformly integrable. In another word, show that I have tried using contradiction, but I am not sure how to use the power in the problem. I can see why it would work, in some sense, the power gets rid of the case often called ""escape to vertical infinity"". For example, define with , without the power , we have but","X 1<p<\infty f_n \colon X\rightarrow \mathbb{R} \sup_n \int_X |f_n|^p d\mu < \infty f_n \sup_n \int_X |f_n|^p d\mu < \infty \;\;\;\Longrightarrow\;\;\; \sup_n \int_{|f_n|>M} |f_n| d\mu \rightarrow 0 \text{ when } M\rightarrow \infty. p p f_n :[0,1] \rightarrow \mathbb{R} f_n = n\chi_{[0,\frac{1}{n}]}  p \sup_n \int_X |f_n| d\mu =1 \;\;\text{ and }\;\; \sup_n \int_{|f_n|>M} |f_n| d\mu =1 \text{ for each } M, \sup_n \int_X |f_n|^p d\mu = \infty","['real-analysis', 'integration', 'measure-theory', 'lp-spaces', 'uniform-integrability']"
3,"Can I benefit from directly using analysis textbooks to self-learn calculus, instead of calculus textbooks?","Can I benefit from directly using analysis textbooks to self-learn calculus, instead of calculus textbooks?",,"My purpose is self-learning, neither for exam nor degree courses. My goal is to research dynamic System, theoretically oriented. Question Description: I've been reading  calculus books by Weinstein & Marsden, UTM, Springer for weeks. I solved 90% of text, 30%-40% of exercises.  UTM seems engineering-oriented (not theoretical/rigorous-oriented), compared with others within series. Their advantages are: they suitably explain concepts, in clear Structure. Their disadvantages are: not  enough theorems, too many exercises in formula-calculation/real application, too little deep/proof exercises. They total approximately 8000 exercises, 300-400 exercises/chapter, but 80% is simple-formula-calculation/realistic application. My question :   Will I benefit from starting with the analysis textbooks below now, instead of continuing with the aforementioned calculus books? I think so, for 3 reasons: (1) Most good EU bachelor in maths, they use analysis directly in first semester instead of calculus. (e.g. Bonn University/ETH Zurich) (2) Since the aforementioned books contains too many exercises of formula-using/real application ones but not deep/proof, if I continue to work with it (solve all exercises/ second time reading), books will still cost several months. (3) Will the analysis textbooks below  also contain needed  intuition, calculation skills for calculus? If it's the case that these analysis books train both theory and calculation ( compute derivatives/integrals which are useful later such as ODE, PDE), then there'd be no need to read calculus books. Rose, Elementary Analysis, UTM, Springer. Serge Lang, A First Course in Calculus/Calculus of Several Variables, UTM, Springer(Even though it's still calculus, but Lang's book is more abstract-oriented) Zorich, Analysis, Universitext, Springer. As @nbubis said, analysis needs intuition. Zorich's analysis seems to contain many physical problems, will it works for teaching intuition? Courant, Introduction to Calculus and Analysis I&II, Springer","My purpose is self-learning, neither for exam nor degree courses. My goal is to research dynamic System, theoretically oriented. Question Description: I've been reading  calculus books by Weinstein & Marsden, UTM, Springer for weeks. I solved 90% of text, 30%-40% of exercises.  UTM seems engineering-oriented (not theoretical/rigorous-oriented), compared with others within series. Their advantages are: they suitably explain concepts, in clear Structure. Their disadvantages are: not  enough theorems, too many exercises in formula-calculation/real application, too little deep/proof exercises. They total approximately 8000 exercises, 300-400 exercises/chapter, but 80% is simple-formula-calculation/realistic application. My question :   Will I benefit from starting with the analysis textbooks below now, instead of continuing with the aforementioned calculus books? I think so, for 3 reasons: (1) Most good EU bachelor in maths, they use analysis directly in first semester instead of calculus. (e.g. Bonn University/ETH Zurich) (2) Since the aforementioned books contains too many exercises of formula-using/real application ones but not deep/proof, if I continue to work with it (solve all exercises/ second time reading), books will still cost several months. (3) Will the analysis textbooks below  also contain needed  intuition, calculation skills for calculus? If it's the case that these analysis books train both theory and calculation ( compute derivatives/integrals which are useful later such as ODE, PDE), then there'd be no need to read calculus books. Rose, Elementary Analysis, UTM, Springer. Serge Lang, A First Course in Calculus/Calculus of Several Variables, UTM, Springer(Even though it's still calculus, but Lang's book is more abstract-oriented) Zorich, Analysis, Universitext, Springer. As @nbubis said, analysis needs intuition. Zorich's analysis seems to contain many physical problems, will it works for teaching intuition? Courant, Introduction to Calculus and Analysis I&II, Springer",,"['real-analysis', 'calculus', 'soft-question', 'learning']"
4,Is the sum of a Darboux function and a continuous function Darboux?,Is the sum of a Darboux function and a continuous function Darboux?,,"A Darboux function is a function that has the intermediate value property.  That is a function $f$ such that $$ \forall a,b \in \mathbb{R} : f[a,b] \supseteq [f(a),f(b)] \cup[f(b),f(a)] $$ We define the sum of two functions as such $$ (f+g)(x) = f(x)+g(x)$$ Now the question is: If $f$ is a Darboux function and $g$ is a continuous function, must $f+g$ be a Darboux function as well?","A Darboux function is a function that has the intermediate value property.  That is a function $f$ such that $$ \forall a,b \in \mathbb{R} : f[a,b] \supseteq [f(a),f(b)] \cup[f(b),f(a)] $$ We define the sum of two functions as such $$ (f+g)(x) = f(x)+g(x)$$ Now the question is: If $f$ is a Darboux function and $g$ is a continuous function, must $f+g$ be a Darboux function as well?",,"['real-analysis', 'riemann-sum']"
5,"Prove that $a^2b+b^2c+c^2a \leqslant 3$ for $a,b,c >0$ with $a^ab^bc^c=1$",Prove that  for  with,"a^2b+b^2c+c^2a \leqslant 3 a,b,c >0 a^ab^bc^c=1","Let $a,b,c >0$ and $a^ab^bc^c=1$ . Prove that $$a^2b+b^2c+c^2a \leqslant 3.$$ I don't even know what to do with the condition $a^ab^bc^c=1$ . At first I think $x^x>1$ , but I was wrong. This inequality is true, following by the verification from Mathematica","Let and . Prove that I don't even know what to do with the condition . At first I think , but I was wrong. This inequality is true, following by the verification from Mathematica","a,b,c >0 a^ab^bc^c=1 a^2b+b^2c+c^2a \leqslant 3. a^ab^bc^c=1 x^x>1","['real-analysis', 'inequality']"
6,Need some help on baby Rudin theorem 6.15,Need some help on baby Rudin theorem 6.15,,"Following is theorem 6.15 of baby Rudin: If $a<s<b$, $f$ is bounded on $[a,b]$. $f$ is continuous at $s$, then $\alpha(x) = I(x-s)$, then $\int_a^b f d \alpha = f(s)$. $\alpha(x)= I(x-s)$ is the unit step function, $\alpha= 0$ if $x \le s ,\alpha= 1$ if $x >s.$ Proof: Consider partitions $P = \{x_0,x_1,x_2,x_3 \}$, where $x_0 = a, x_1=s, x_2< x_3=b$. Then $U(P,f, \alpha) = M_2, L(P,f, \alpha)=m_2$. Since $f$ is continuous at $s$, we see that $M_2$ and $m_2$ converge to $f(s)$ as $x_2 \to s$. $\square$ Need some help on shedding some light on the bold sentence, especially on how it is used to show that $\int_a^b f d \alpha = f(s)$.  I calculated that the difference between upper sum and lower sum is $M_2 - m_2$, but then I am stuck and I do not get the last sentence of the proof.","Following is theorem 6.15 of baby Rudin: If $a<s<b$, $f$ is bounded on $[a,b]$. $f$ is continuous at $s$, then $\alpha(x) = I(x-s)$, then $\int_a^b f d \alpha = f(s)$. $\alpha(x)= I(x-s)$ is the unit step function, $\alpha= 0$ if $x \le s ,\alpha= 1$ if $x >s.$ Proof: Consider partitions $P = \{x_0,x_1,x_2,x_3 \}$, where $x_0 = a, x_1=s, x_2< x_3=b$. Then $U(P,f, \alpha) = M_2, L(P,f, \alpha)=m_2$. Since $f$ is continuous at $s$, we see that $M_2$ and $m_2$ converge to $f(s)$ as $x_2 \to s$. $\square$ Need some help on shedding some light on the bold sentence, especially on how it is used to show that $\int_a^b f d \alpha = f(s)$.  I calculated that the difference between upper sum and lower sum is $M_2 - m_2$, but then I am stuck and I do not get the last sentence of the proof.",,['real-analysis']
7,are non-degenerate critical points always isolated?,are non-degenerate critical points always isolated?,,"I have a question regarding the isolation of critical points of a function: Suppose $f : \mathbb{R}^n \to \mathbb{R}$ is a $C^\infty$ function such that $f$ has a non - degenerate critical point at $0 \in \mathbb{R}^n$. That is, we have $\triangledown f (0) = 0$, and the Hessian $\left((\partial^2 f/ \partial x_i \partial x_j ) (0) \right)$ is invertible. Can I deduce from this that the critical point at $0$ is an isolated critical point ? My guess is to say yes, because the fact that the Hessian is non-degenerate forces $f$ to change its value in the vicinity, and in all directions. But I am unsure, in particular with regards to the last statements (""in all directions"") - though that should be encoded by the fact all eigenvalues of the Hessian are non - zero. Is this the right way to think about the question of whether the critical point is isolated ? Thanks for your thoughts !","I have a question regarding the isolation of critical points of a function: Suppose $f : \mathbb{R}^n \to \mathbb{R}$ is a $C^\infty$ function such that $f$ has a non - degenerate critical point at $0 \in \mathbb{R}^n$. That is, we have $\triangledown f (0) = 0$, and the Hessian $\left((\partial^2 f/ \partial x_i \partial x_j ) (0) \right)$ is invertible. Can I deduce from this that the critical point at $0$ is an isolated critical point ? My guess is to say yes, because the fact that the Hessian is non-degenerate forces $f$ to change its value in the vicinity, and in all directions. But I am unsure, in particular with regards to the last statements (""in all directions"") - though that should be encoded by the fact all eigenvalues of the Hessian are non - zero. Is this the right way to think about the question of whether the critical point is isolated ? Thanks for your thoughts !",,"['real-analysis', 'differential-geometry']"
8,Is a continuous function that satisfies a certain condition uniformly continuous?,Is a continuous function that satisfies a certain condition uniformly continuous?,,"Let $f:[0,+\infty)$ be a continuous function that satisfies: $f(x+q)$ ~ $f(x)$ for $x\to\infty$ (for any $q$ ) Does it follow that $f$ is uniformly continuous? I have managed to show that if there exists $\space$ $\displaystyle\lim_{x\to\infty}\space f(x)=G\in\Bbb{R}$ $\space$ then the function must be uniformly continuos by for given $\epsilon$ picking an $N$ big enough that $\forall_{x>N} |f(x)-G|<\frac{\epsilon}{2}$ and then showing that the function is uniformly continuous on $[0,N]$ and satisfies the definition of unifom continuity for that $\epsilon$ on $[N,+\infty)$ , thus proving it must be uniformly continuous,since we could have chosen any $\epsilon$ . However, that approach fails when we consider the cases where $\displaystyle\lim_{x\to\infty}\space f(x)$ is infinite or non-existent. I've also tried to find a counterexample by experimenting with functions like $\frac{1}{x}\sin(x^{3})$ (which appeared promising since its derivative is unbounded) but so far I haven't found one and my intuition does not steer me to either of the answers. I would appreciate any hints :)","Let be a continuous function that satisfies: ~ for (for any ) Does it follow that is uniformly continuous? I have managed to show that if there exists then the function must be uniformly continuos by for given picking an big enough that and then showing that the function is uniformly continuous on and satisfies the definition of unifom continuity for that on , thus proving it must be uniformly continuous,since we could have chosen any . However, that approach fails when we consider the cases where is infinite or non-existent. I've also tried to find a counterexample by experimenting with functions like (which appeared promising since its derivative is unbounded) but so far I haven't found one and my intuition does not steer me to either of the answers. I would appreciate any hints :)","f:[0,+\infty) f(x+q) f(x) x\to\infty q f \space \displaystyle\lim_{x\to\infty}\space f(x)=G\in\Bbb{R} \space \epsilon N \forall_{x>N} |f(x)-G|<\frac{\epsilon}{2} [0,N] \epsilon [N,+\infty) \epsilon \displaystyle\lim_{x\to\infty}\space f(x) \frac{1}{x}\sin(x^{3})","['real-analysis', 'uniform-continuity']"
9,"Is every scalar differential operator on $(M,g)$ that commutes with isometries a polynomial of the Laplacian?",Is every scalar differential operator on  that commutes with isometries a polynomial of the Laplacian?,"(M,g)","On $(\mathbb{R}^n, g_{\text{std}})$ with $\Delta$ the Laplacian, the following holds: Fact: Every scalar differential operator $D$ that satisfies $D \circ F^* = F^* \circ D$ for all isometries $F \in \text{Isom}(\mathbb{R}^n)$ can be written as $D = P(\Delta)$ for some polynomial $P$. My question is whether a similar statement is true on any Riemannian manifold $(M,g)$, where $\Delta$ becomes the Laplacian with respect to the metric. As usual, this question is a refinement of a previous question of mine .","On $(\mathbb{R}^n, g_{\text{std}})$ with $\Delta$ the Laplacian, the following holds: Fact: Every scalar differential operator $D$ that satisfies $D \circ F^* = F^* \circ D$ for all isometries $F \in \text{Isom}(\mathbb{R}^n)$ can be written as $D = P(\Delta)$ for some polynomial $P$. My question is whether a similar statement is true on any Riemannian manifold $(M,g)$, where $\Delta$ becomes the Laplacian with respect to the metric. As usual, this question is a refinement of a previous question of mine .",,"['real-analysis', 'differential-geometry', 'partial-differential-equations', 'riemannian-geometry']"
10,"What exactly is the ""Carathéodory Extension theorem""?","What exactly is the ""Carathéodory Extension theorem""?",,"I have read four texts introducing a theorem so-called ""Carathéodory's Extension Theorem"", and they all differ. Here is the statement of the Carathéodory Extension Theorem in Wikipedia : Let $\mathfrak{R}$ be a ring of subsets of $X$   Let $\mu:\mathfrak{R} \rightarrow [0,\infty]$ be a premeasure.   Then, there exists a measure on the σ-algebra generated by $\mathfrak{R}$ which is a extension of $\mu$. I like this statement since it is very simple and clear. However mainstream textbooks don't introduce the Caratheodory Extension Theorem as this. For example, Royden's Real Analysis (4th edition) defines premeasure as a set function which is finitely additive , countably monotone (is this a widely used term?), which is different from the definition in wikipedia. Then, he proves a theorem so-called Carathéodory-Hahn Extension Theorem. This theorem does not imply the 'Carathéodory Extension Theorem in Wikipedia' but is deeper than that in Wikipedia, in my opinion. Since he defined premeasure differently, I am quite hesitant to memorize this. I don't want to be out of the mainstream. He even doesn't require the domain of a given set function to have an empty set. Another example, Folland's Real Analysis states Carathéodory's Theorem as follows: If $\mu^*$ is an outer measure on $X$, the collection $\mathcal{M}$ of $\mu^*$-measureable sets is a σ-algebra, and the restriction of $\mu^*$ to $\mathcal{M}$ is a complete measure. Even though this is critical in any proof for any kind of Extension Theorem, this is obviously not 'the Carathéodory Extension Theorem'. What is the Carathéodory Extension Theorem? It is off the topic, but I have one more question. Why Royden tries to include set functions whose domain does not contain an empty set? If the emptyset is not in a domain, can't we just extend a given set function by defining $\mu(\emptyset)=0$? Does this sometimes break structure of a given set? E.g. ring of sets, algebra of sets or whatever.","I have read four texts introducing a theorem so-called ""Carathéodory's Extension Theorem"", and they all differ. Here is the statement of the Carathéodory Extension Theorem in Wikipedia : Let $\mathfrak{R}$ be a ring of subsets of $X$   Let $\mu:\mathfrak{R} \rightarrow [0,\infty]$ be a premeasure.   Then, there exists a measure on the σ-algebra generated by $\mathfrak{R}$ which is a extension of $\mu$. I like this statement since it is very simple and clear. However mainstream textbooks don't introduce the Caratheodory Extension Theorem as this. For example, Royden's Real Analysis (4th edition) defines premeasure as a set function which is finitely additive , countably monotone (is this a widely used term?), which is different from the definition in wikipedia. Then, he proves a theorem so-called Carathéodory-Hahn Extension Theorem. This theorem does not imply the 'Carathéodory Extension Theorem in Wikipedia' but is deeper than that in Wikipedia, in my opinion. Since he defined premeasure differently, I am quite hesitant to memorize this. I don't want to be out of the mainstream. He even doesn't require the domain of a given set function to have an empty set. Another example, Folland's Real Analysis states Carathéodory's Theorem as follows: If $\mu^*$ is an outer measure on $X$, the collection $\mathcal{M}$ of $\mu^*$-measureable sets is a σ-algebra, and the restriction of $\mu^*$ to $\mathcal{M}$ is a complete measure. Even though this is critical in any proof for any kind of Extension Theorem, this is obviously not 'the Carathéodory Extension Theorem'. What is the Carathéodory Extension Theorem? It is off the topic, but I have one more question. Why Royden tries to include set functions whose domain does not contain an empty set? If the emptyset is not in a domain, can't we just extend a given set function by defining $\mu(\emptyset)=0$? Does this sometimes break structure of a given set? E.g. ring of sets, algebra of sets or whatever.",,"['real-analysis', 'measure-theory']"
11,Topology of uniform convergence?,Topology of uniform convergence?,,"Stone starts with an arbitrary compact Hausdorff space X and considers   the algebra C(X,R) of real-valued continuous functions on X, with the   topology of uniform convergence I am having a hard time understanding what the topology of uniform convergence means. I of course know what uniform convergence means - a sequence of functions satisfying a particular set of convergence properties - but the ""topology of uniform convergence""? Not sure.","Stone starts with an arbitrary compact Hausdorff space X and considers   the algebra C(X,R) of real-valued continuous functions on X, with the   topology of uniform convergence I am having a hard time understanding what the topology of uniform convergence means. I of course know what uniform convergence means - a sequence of functions satisfying a particular set of convergence properties - but the ""topology of uniform convergence""? Not sure.",,"['real-analysis', 'functional-analysis', 'definition']"
12,The space of Riemann integrable functions with $L^2$ inner product is not complete,The space of Riemann integrable functions with  inner product is not complete,L^2,"I am trying to find a sequence of Riemann integrable functions on $[0,1]$ converging in $L^2$ to a Lebesgue but not Riemann integrable function. I tried Dirichlet function but could not find a sequence converging to it.","I am trying to find a sequence of Riemann integrable functions on $[0,1]$ converging in $L^2$ to a Lebesgue but not Riemann integrable function. I tried Dirichlet function but could not find a sequence converging to it.",,"['real-analysis', 'functional-analysis', 'measure-theory']"
13,sum and product of Lipschitz functions,sum and product of Lipschitz functions,,"I have the following question from my notes, where $f$ and $g$ are Lipschitz functions on $A ⊂ \Bbb R$. I'm able to show that the sum $f + g$ is also a Lipschitz function, however I'm stuck on trying to show that if $f,g$ are bounded on $A$, then the product $fg$ is Lipschitz on $A$. Also, is there a valid example of a Lipschitz function $f$ on $[0,+∞)$ such that $f^2$ is not Lipschitz on $[0,+∞)$?","I have the following question from my notes, where $f$ and $g$ are Lipschitz functions on $A ⊂ \Bbb R$. I'm able to show that the sum $f + g$ is also a Lipschitz function, however I'm stuck on trying to show that if $f,g$ are bounded on $A$, then the product $fg$ is Lipschitz on $A$. Also, is there a valid example of a Lipschitz function $f$ on $[0,+∞)$ such that $f^2$ is not Lipschitz on $[0,+∞)$?",,"['real-analysis', 'lipschitz-functions']"
14,Partition of Unity in Spivak's Calculus on Manifolds,Partition of Unity in Spivak's Calculus on Manifolds,,"I have a question about partitions of unity specifically in the book Calculus on Manifolds by Spivak. In case 1 for the proof of existence of partition of unity, why is there a need for the function $f$? The set $\Phi = \{\varphi_1, \dotsc, \varphi_n\}$ looks like is already the desired partition of unity. Following is the theorem and proof. Only Case 1 in the proof is relevant.","I have a question about partitions of unity specifically in the book Calculus on Manifolds by Spivak. In case 1 for the proof of existence of partition of unity, why is there a need for the function $f$? The set $\Phi = \{\varphi_1, \dotsc, \varphi_n\}$ looks like is already the desired partition of unity. Following is the theorem and proof. Only Case 1 in the proof is relevant.",,"['real-analysis', 'multivariable-calculus']"
15,What is the difference between open ball and neighborhood in real analysis?,What is the difference between open ball and neighborhood in real analysis?,,"I'm learning real analysis. Open ball: The collection of points $x \in X$ satisfying $|x - x_{0}| < r$ is called the open ball of radius $r$ centered at $x_{0}$ Neighborhood: A neighborhood of  $x_{0} \in X$ is an open ball of   radius r > 0 in $X$ that is centered at $x_{0}$ I'm using Real and Complex Analysis written by Christopher Apelian and Steve Surace. In my mind, open ball = a collection of points satisfy certain requirement = neighborhood. I do not find out any differences between open ball and neighborhood. Could any one explain it? Thanks!","I'm learning real analysis. Open ball: The collection of points $x \in X$ satisfying $|x - x_{0}| < r$ is called the open ball of radius $r$ centered at $x_{0}$ Neighborhood: A neighborhood of  $x_{0} \in X$ is an open ball of   radius r > 0 in $X$ that is centered at $x_{0}$ I'm using Real and Complex Analysis written by Christopher Apelian and Steve Surace. In my mind, open ball = a collection of points satisfy certain requirement = neighborhood. I do not find out any differences between open ball and neighborhood. Could any one explain it? Thanks!",,['real-analysis']
16,Confusion with the narrow and weak* convergence of measures,Confusion with the narrow and weak* convergence of measures,,"Think of a LCH space $X.$ Consider the spaces $C_{0}(X)$ of continuous functions ""vanishing at infinity"" and the space $BC(X)$ of bounded continuous functions. Consider as well the space of Radon (Borel regular) measures $M(X).$ What follows is an incorrect reasoning which gets me to an absurd situation. However, I fail to see the mistake(s) in it, and that's where I would like some help! By the Riesz's representation theorem, we have that the topological dual of $C_{0}(X)$ is isometrically isomorphic to the space of finite Radon measures. Hence, we have in $M(X)$ the natural weak* star topology: We say that $\mu_{n}$ converges weak* to $\mu$ if $$\int _{X} \psi \ d\mu_{n} \rightarrow \int _{X} \psi \ d\mu, \ \forall \psi \in C_{0}(X).$$ On the other hand, we say that $\mu_{n}$ converges in the narrow topology to $\mu$ if $$\int _{X} \phi \ d\mu_{n} \rightarrow \int _{X} \phi \ d\mu, \ \forall \phi \in BC(X). $$ Prokhorov's theorem gives a characterization of sequential compactness in $M(X)$ with the narrow topology (actually, of compactness, since it is metrizable). This theorems is quite technical and involves the notion of tightness, which is a necessary condition for compactness. However, if we think of $M(X)$ with the weak* topology it inherits from being a dual space, compactness is very easy to characterize thanks to the Banach-Alaoglu theorem. Furthermore, it is a not-too-hard exercise to show the equivalence of the following two propositions: i) $\mu_{n}$ converges narrowly to $\mu$ ii) $\mu_{n}$ converges weakly* to $\mu$ and $\mu_{n}(X) \rightarrow \mu(X).$ And here is where I got confused: Consider a collection of probability measures, $\{ \mu_{n} \}_{n \in \mathbb{N}}.$ Clearly, this family is bounded in the dual norm. Therefore, by the Banach-Alouglu theorem, it must contain a weakly* convergente subsequence (which I do not relabel) to a measure $\mu$. This subsequence trivilly satisfies $\mu_{n}(X)=1 \rightarrow 1=\mu(X).$ By the remark above, $\mu_{n}$ must converge narrowly to $\mu...$ But the tightness condition has not appeared anywhere! What is my mistake? What am I doing wrong? Also, if this were so easy, Prokhorov's theorem would be meaningless (of course that's not the case!) Thank you for your help","Think of a LCH space $X.$ Consider the spaces $C_{0}(X)$ of continuous functions ""vanishing at infinity"" and the space $BC(X)$ of bounded continuous functions. Consider as well the space of Radon (Borel regular) measures $M(X).$ What follows is an incorrect reasoning which gets me to an absurd situation. However, I fail to see the mistake(s) in it, and that's where I would like some help! By the Riesz's representation theorem, we have that the topological dual of $C_{0}(X)$ is isometrically isomorphic to the space of finite Radon measures. Hence, we have in $M(X)$ the natural weak* star topology: We say that $\mu_{n}$ converges weak* to $\mu$ if $$\int _{X} \psi \ d\mu_{n} \rightarrow \int _{X} \psi \ d\mu, \ \forall \psi \in C_{0}(X).$$ On the other hand, we say that $\mu_{n}$ converges in the narrow topology to $\mu$ if $$\int _{X} \phi \ d\mu_{n} \rightarrow \int _{X} \phi \ d\mu, \ \forall \phi \in BC(X). $$ Prokhorov's theorem gives a characterization of sequential compactness in $M(X)$ with the narrow topology (actually, of compactness, since it is metrizable). This theorems is quite technical and involves the notion of tightness, which is a necessary condition for compactness. However, if we think of $M(X)$ with the weak* topology it inherits from being a dual space, compactness is very easy to characterize thanks to the Banach-Alaoglu theorem. Furthermore, it is a not-too-hard exercise to show the equivalence of the following two propositions: i) $\mu_{n}$ converges narrowly to $\mu$ ii) $\mu_{n}$ converges weakly* to $\mu$ and $\mu_{n}(X) \rightarrow \mu(X).$ And here is where I got confused: Consider a collection of probability measures, $\{ \mu_{n} \}_{n \in \mathbb{N}}.$ Clearly, this family is bounded in the dual norm. Therefore, by the Banach-Alouglu theorem, it must contain a weakly* convergente subsequence (which I do not relabel) to a measure $\mu$. This subsequence trivilly satisfies $\mu_{n}(X)=1 \rightarrow 1=\mu(X).$ By the remark above, $\mu_{n}$ must converge narrowly to $\mu...$ But the tightness condition has not appeared anywhere! What is my mistake? What am I doing wrong? Also, if this were so easy, Prokhorov's theorem would be meaningless (of course that's not the case!) Thank you for your help",,"['real-analysis', 'probability-theory', 'convergence-divergence', 'weak-convergence']"
17,Riesz Representation Theorem from Rudin Real and Complex Analysis,Riesz Representation Theorem from Rudin Real and Complex Analysis,,"I've been annotating the steps in Riesz Rep. Theorem. So far, I have almost all of them. I just have six questions, some are short questions. So these questions are for anyone who has Rudin Real and Complex analysis on hand. (1) My first question starts on page $44$ for step IV. We start with two disjoint compact sets $K_1, K_2$. I'm not really clear on what $f(x) = 0$ on $K_2$. My reasoning is that we can an open set $V$ such that it contains $K_1$ and $V \cap K_2 = \emptyset$. So using Urysohn's lemma, we get $K_1 \prec f \prec V$ for some $f \in C_c(X)$. The support of $f$ lies in $V$, and since $K_2 \cap V = \emptyset$, $f(x) = 0$ for all $x \in K_2$. Are we guaranteed that we can find such open set $V$? (2) My next question is on the same step but at equation $(13)$. So far, it says $(9)$ shows that $(13)$ holds. I don't see how this is obvious. For me, i started with step I, used that $\mu(E) \leq \sum_{1}^{\infty}\mu(E_i)$. I just use the fact that there must be infinite number of zero sets or else $\mu(E) < \infty$ is violated. Also there must be finite number of non-zero sets, so is that why we get $(13)$? (3) On page 46 Step $X$. Rudin says, ""Clearly, it is enough to prove this for real $f$"". I don't have a complex analysis background but is he insinuating that the case for $f$ being complex is almost the same? (4) On the same page, between equation $(18)$ and $(19)$, Rudin mentions that the sets $E_i$ are therefore disjoint Borel sets whose union is $K$. I understand this paragraph except the part that $E_i$ are Borel sets. I'm thinking that I have to verify that $E_i$ are either closed or open. We have that $f$ is continuous so the pre-image of an open set is open. But I am having trouble getting to verify that fact. (5) In the same paragraph, Rudin says ""There are open sets $V_i \supset E_i$ such that $\mu(V_i) < \mu(E_i) + \frac{\epsilon}{n}$"" equation $(19)$ and such that $f(x) < y_i + \epsilon$ for all $x \in V_i$. Using equation $(2)$, I understand that $\mu(E_i) + \epsilon > \mu(V)$ for some open set $V$ such that $V \supset E$. Why does Rudin define equation $(19)$ as that way (the epsilon term divided by n) and was wondering why $f(x) < y_i + \epsilon$ for $x \in V_i$ <--- for this part, I can get a $V_i$ satisfying this inequality but dont see how it also satisfy $(19)$ (6) Same page, at the bottom, Rudin says that Step II shows that $\mu(K) \leq \Lambda(\sum h_i) = \sum \Lambda h_i$. This isn't obvious at all. I looked at $\mu(K) = \inf\{\Lambda f \mid K \prec f\}$ but we have that $h_i \prec V_i$. I tried showing that $K \prec \sum h_i$. But what seems to be the trouble is verifying that $0 \leq \sum h_i \leq 1$ for all $x \in X$ Thanks a bunch!","I've been annotating the steps in Riesz Rep. Theorem. So far, I have almost all of them. I just have six questions, some are short questions. So these questions are for anyone who has Rudin Real and Complex analysis on hand. (1) My first question starts on page $44$ for step IV. We start with two disjoint compact sets $K_1, K_2$. I'm not really clear on what $f(x) = 0$ on $K_2$. My reasoning is that we can an open set $V$ such that it contains $K_1$ and $V \cap K_2 = \emptyset$. So using Urysohn's lemma, we get $K_1 \prec f \prec V$ for some $f \in C_c(X)$. The support of $f$ lies in $V$, and since $K_2 \cap V = \emptyset$, $f(x) = 0$ for all $x \in K_2$. Are we guaranteed that we can find such open set $V$? (2) My next question is on the same step but at equation $(13)$. So far, it says $(9)$ shows that $(13)$ holds. I don't see how this is obvious. For me, i started with step I, used that $\mu(E) \leq \sum_{1}^{\infty}\mu(E_i)$. I just use the fact that there must be infinite number of zero sets or else $\mu(E) < \infty$ is violated. Also there must be finite number of non-zero sets, so is that why we get $(13)$? (3) On page 46 Step $X$. Rudin says, ""Clearly, it is enough to prove this for real $f$"". I don't have a complex analysis background but is he insinuating that the case for $f$ being complex is almost the same? (4) On the same page, between equation $(18)$ and $(19)$, Rudin mentions that the sets $E_i$ are therefore disjoint Borel sets whose union is $K$. I understand this paragraph except the part that $E_i$ are Borel sets. I'm thinking that I have to verify that $E_i$ are either closed or open. We have that $f$ is continuous so the pre-image of an open set is open. But I am having trouble getting to verify that fact. (5) In the same paragraph, Rudin says ""There are open sets $V_i \supset E_i$ such that $\mu(V_i) < \mu(E_i) + \frac{\epsilon}{n}$"" equation $(19)$ and such that $f(x) < y_i + \epsilon$ for all $x \in V_i$. Using equation $(2)$, I understand that $\mu(E_i) + \epsilon > \mu(V)$ for some open set $V$ such that $V \supset E$. Why does Rudin define equation $(19)$ as that way (the epsilon term divided by n) and was wondering why $f(x) < y_i + \epsilon$ for $x \in V_i$ <--- for this part, I can get a $V_i$ satisfying this inequality but dont see how it also satisfy $(19)$ (6) Same page, at the bottom, Rudin says that Step II shows that $\mu(K) \leq \Lambda(\sum h_i) = \sum \Lambda h_i$. This isn't obvious at all. I looked at $\mu(K) = \inf\{\Lambda f \mid K \prec f\}$ but we have that $h_i \prec V_i$. I tried showing that $K \prec \sum h_i$. But what seems to be the trouble is verifying that $0 \leq \sum h_i \leq 1$ for all $x \in X$ Thanks a bunch!",,"['real-analysis', 'functional-analysis', 'measure-theory', 'riesz-representation-theorem']"
18,Riesz representation and vector-valued functions,Riesz representation and vector-valued functions,,"A version of the Riesz Representation Theorem says that a continuous linear functional on the space of continuous real-valued mappings on a compact metric space, $C(X)$, can be identified with a signed Borel measure on the set $X$. Are there any similar results when we replace $C(X)$ by the space of continuous functions of $X$ (compact metric) into $Y$ when (1) $Y=R^N$ or in general (2) $Y$ is a Banach space? I suspect the answer is yes, but I would like to find the right reference to start looking at. Thanks.","A version of the Riesz Representation Theorem says that a continuous linear functional on the space of continuous real-valued mappings on a compact metric space, $C(X)$, can be identified with a signed Borel measure on the set $X$. Are there any similar results when we replace $C(X)$ by the space of continuous functions of $X$ (compact metric) into $Y$ when (1) $Y=R^N$ or in general (2) $Y$ is a Banach space? I suspect the answer is yes, but I would like to find the right reference to start looking at. Thanks.",,['real-analysis']
19,How to approach $\sum _{n=1}^{\infty } \frac{16^n}{n^4 \binom{2 n}{n}^2}$?,How to approach ?,\sum _{n=1}^{\infty } \frac{16^n}{n^4 \binom{2 n}{n}^2},@User mentioned in the comments that $$\sum _{n=1}^{\infty } \frac{16^n}{n^3 \binom{2 n}{n}^2}=8\pi\text{G}-14 \zeta (3)\tag1$$ $$\small{\sum _{n=1}^{\infty } \frac{16^n}{n^4 \binom{2 n}{n}^2}=64 \pi  \Im(\text{Li}_3(1+i))+64 \text{Li}_4\left(\frac{1}{2}\right)-233 \zeta(4)-40  \ln ^2(2)\zeta(2)+\frac{8}{3}\ln ^4(2)}\tag2$$ I was able to prove $(1)$ but had some difficulty proving $(2)$ . Any idea? I am going to show my proof of $(1)$ hoping it helps you prove $(2)$ : We showed in this question that $$\sum_{n=1}^\infty\frac{4^ny^n}{n^2{2n\choose n}}=2\int_0^y \frac{\arcsin \sqrt{x}}{\sqrt{x}\sqrt{1-x}}dx$$ multiply both sides by $\frac{1}{y\sqrt{1-y}}$ then $\int_0^1$ with respect to $y$ and use $\int_0^1\frac{y^{n-1}}{\sqrt{1-y}}dy=\frac{4^n}{n{2n\choose n}}$ we obtain $$\sum _{n=1}^{\infty } \frac{16^n}{n^3 \binom{2 n}{n}^2}=2\int_0^1\int_0^y \frac{\arcsin \sqrt{x}}{y\sqrt{x}\sqrt{1-x}\sqrt{1-y}}dxdy$$ $$=2\int_0^1\frac{\arcsin\sqrt{x}}{\sqrt{x}\sqrt{1-x}}\left(\int_x^1\frac{dy}{y\sqrt{1-y}}\right)dx$$ $$=2\int_0^1\frac{\arcsin\sqrt{x}}{\sqrt{x}\sqrt{1-x}}\left(2\ln(1+\sqrt{1-x})-\ln x\right)dx$$ $$\overset{\sqrt{x}=\sin \theta}{=}8\int_0^{\pi/2}x\ln(1+\cos x)dx-8\int_0^{\pi/2}x\ln(\sin x)dx$$ $$=8\int_0^{\pi/2}x\ln(2\cos^2\frac x2)dx-8\int_0^{\pi/2}x\ln(\sin x)dx$$ $$=32\int_0^{\pi/4}x\ln(2\cos^2x)dx-8\int_0^{\pi/2}x\ln(\sin x)dx$$ $$=32\underbrace{\int_0^{\pi/4}x\ln(2)dx}_{\frac3{16}\ln(2)\zeta(2)}+64\underbrace{\int_0^{\pi/4}x\ln(\cos x)dx}_{\frac{\pi}{8}\text{G}-\frac3{16}\ln(2)\zeta(2)-\frac{21}{128}\zeta(3)}-8\underbrace{\int_0^{\pi/2}x\ln(\sin x)dx}_{\frac7{16}\zeta(3)-\frac34\ln(2)\zeta(2)}$$ $$=8\pi\text{G}-14 \zeta (3)$$ The last two integrals follow from using the Fourier series of $\ln(\cos x)$ and $\ln(\sin x)$ . All approaches are appreciated. Thank you. Addendum: Here is an easier way to prove $(1)$ : We have $$\arcsin^2(x)=\frac12\sum_{n=1}^\infty\frac{(2x)^{2n}}{n^2{2n\choose n}}$$ or $$\sum_{n=1}^\infty\frac{4^nx^n}{n^2{2n\choose n}}=2\arcsin^2(\sqrt{x})$$ Divide both sides by $x\sqrt{1-x}$ then $\int_0^1$ and use $\int_0^1\frac{x^{n-1}}{\sqrt{1-x}}dx=\frac{4^n}{n{2n\choose n}}$ we have $$\sum_{n=1}^\infty\frac{16^n}{n^3{2n\choose n}^2}=2\int_0^1\frac{\arcsin^2(\sqrt{x})}{x\sqrt{1-x}}dx$$ $$\overset{\sqrt{x}=\sin x}{=}4\int_0^{\pi/2}x^2 \csc(x)dx$$ $$\overset{IBP}{=}-8\int_0^{\pi/4} x\ln(\tan\frac x2)dx=8\pi\text{G}-14\zeta(3)$$ where the last result follows from the Fourier series of $\ln(\tan\frac x2)$ .,@User mentioned in the comments that I was able to prove but had some difficulty proving . Any idea? I am going to show my proof of hoping it helps you prove : We showed in this question that multiply both sides by then with respect to and use we obtain The last two integrals follow from using the Fourier series of and . All approaches are appreciated. Thank you. Addendum: Here is an easier way to prove : We have or Divide both sides by then and use we have where the last result follows from the Fourier series of .,\sum _{n=1}^{\infty } \frac{16^n}{n^3 \binom{2 n}{n}^2}=8\pi\text{G}-14 \zeta (3)\tag1 \small{\sum _{n=1}^{\infty } \frac{16^n}{n^4 \binom{2 n}{n}^2}=64 \pi  \Im(\text{Li}_3(1+i))+64 \text{Li}_4\left(\frac{1}{2}\right)-233 \zeta(4)-40  \ln ^2(2)\zeta(2)+\frac{8}{3}\ln ^4(2)}\tag2 (1) (2) (1) (2) \sum_{n=1}^\infty\frac{4^ny^n}{n^2{2n\choose n}}=2\int_0^y \frac{\arcsin \sqrt{x}}{\sqrt{x}\sqrt{1-x}}dx \frac{1}{y\sqrt{1-y}} \int_0^1 y \int_0^1\frac{y^{n-1}}{\sqrt{1-y}}dy=\frac{4^n}{n{2n\choose n}} \sum _{n=1}^{\infty } \frac{16^n}{n^3 \binom{2 n}{n}^2}=2\int_0^1\int_0^y \frac{\arcsin \sqrt{x}}{y\sqrt{x}\sqrt{1-x}\sqrt{1-y}}dxdy =2\int_0^1\frac{\arcsin\sqrt{x}}{\sqrt{x}\sqrt{1-x}}\left(\int_x^1\frac{dy}{y\sqrt{1-y}}\right)dx =2\int_0^1\frac{\arcsin\sqrt{x}}{\sqrt{x}\sqrt{1-x}}\left(2\ln(1+\sqrt{1-x})-\ln x\right)dx \overset{\sqrt{x}=\sin \theta}{=}8\int_0^{\pi/2}x\ln(1+\cos x)dx-8\int_0^{\pi/2}x\ln(\sin x)dx =8\int_0^{\pi/2}x\ln(2\cos^2\frac x2)dx-8\int_0^{\pi/2}x\ln(\sin x)dx =32\int_0^{\pi/4}x\ln(2\cos^2x)dx-8\int_0^{\pi/2}x\ln(\sin x)dx =32\underbrace{\int_0^{\pi/4}x\ln(2)dx}_{\frac3{16}\ln(2)\zeta(2)}+64\underbrace{\int_0^{\pi/4}x\ln(\cos x)dx}_{\frac{\pi}{8}\text{G}-\frac3{16}\ln(2)\zeta(2)-\frac{21}{128}\zeta(3)}-8\underbrace{\int_0^{\pi/2}x\ln(\sin x)dx}_{\frac7{16}\zeta(3)-\frac34\ln(2)\zeta(2)} =8\pi\text{G}-14 \zeta (3) \ln(\cos x) \ln(\sin x) (1) \arcsin^2(x)=\frac12\sum_{n=1}^\infty\frac{(2x)^{2n}}{n^2{2n\choose n}} \sum_{n=1}^\infty\frac{4^nx^n}{n^2{2n\choose n}}=2\arcsin^2(\sqrt{x}) x\sqrt{1-x} \int_0^1 \int_0^1\frac{x^{n-1}}{\sqrt{1-x}}dx=\frac{4^n}{n{2n\choose n}} \sum_{n=1}^\infty\frac{16^n}{n^3{2n\choose n}^2}=2\int_0^1\frac{\arcsin^2(\sqrt{x})}{x\sqrt{1-x}}dx \overset{\sqrt{x}=\sin x}{=}4\int_0^{\pi/2}x^2 \csc(x)dx \overset{IBP}{=}-8\int_0^{\pi/4} x\ln(\tan\frac x2)dx=8\pi\text{G}-14\zeta(3) \ln(\tan\frac x2),"['real-analysis', 'integration', 'sequences-and-series', 'harmonic-numbers', 'polylogarithm']"
20,Continuous right derivative implies differentiability,Continuous right derivative implies differentiability,,"A book of mine says the following is true, and I am having some trouble proving it. (I've considered using the Lebesgue differentiation theorem and absolute continuity, as well as elementary analysis methods.) Let $f: [0, \infty) \rightarrow \mathbb{R}$ be continuous and have right derivatives at each point in the domain, with the right derivative function being continuous.  Then $f$ is differentiable.","A book of mine says the following is true, and I am having some trouble proving it. (I've considered using the Lebesgue differentiation theorem and absolute continuity, as well as elementary analysis methods.) Let $f: [0, \infty) \rightarrow \mathbb{R}$ be continuous and have right derivatives at each point in the domain, with the right derivative function being continuous.  Then $f$ is differentiable.",,"['real-analysis', 'derivatives']"
21,Continuous extension of a uniformly continuous function from a dense subset.,Continuous extension of a uniformly continuous function from a dense subset.,,"I'm trying to understand an alternative proof of the idea that if $E$ is a dense subset of a metric space $X$, and $f\colon E\to\mathbb{R}$ is uniformly continuous, then $f$ has a uniform continuous extension to $X$. I think I know how to do this using Cauchy sequences, but there is this suggested alternative. For each $p\in X$, let $V_n(p)$ be the set of $q\in E$ such that $d(p,q)<\frac{1}{n}$. Then prove that the intersections of the closures $$ A=\bigcap_{n=1}^\infty\overline{f(V_n(p))} $$ consists of a single point, $g(p)$, and so $g$ is the desired continuous extension of $f$. Why is this intersection a single point, and why is $g$ continuous? This is what I did so far. Since $f$ is uniformly continuous, for given $\epsilon>0$, there is $\delta>0$ such that $\text{diam }f(V)<\epsilon$ whenever $\text{diam }V<\delta$. Since $V_n(p)$ has diameter at most $\frac{2}{n}$, taking $n>2/\delta$ would imply  $$ \text{diam }f(V_n(p))=\text{diam }\overline{f(V_n(p))}<\epsilon $$ So I think $\lim_{n\to\infty}\text{diam }\overline{f(V_n(p))}=0$, which would imply $A$ consists of at most one point. I noticed that the closures form a descending sequence of closed sets, but I couldn't tell if they are bounded since $X$ is an arbitrary metric space, in order to conclude that the intersection is nonempty, and hence a single point. Lastly, why is $g$ continuous at points $p\in X\setminus E$? I was trying to think of an argument with sequences converging to $p$ since $p$ is a limit point of $E$, but got stumping on how to show $g$ is actually continuous. Thanks.","I'm trying to understand an alternative proof of the idea that if $E$ is a dense subset of a metric space $X$, and $f\colon E\to\mathbb{R}$ is uniformly continuous, then $f$ has a uniform continuous extension to $X$. I think I know how to do this using Cauchy sequences, but there is this suggested alternative. For each $p\in X$, let $V_n(p)$ be the set of $q\in E$ such that $d(p,q)<\frac{1}{n}$. Then prove that the intersections of the closures $$ A=\bigcap_{n=1}^\infty\overline{f(V_n(p))} $$ consists of a single point, $g(p)$, and so $g$ is the desired continuous extension of $f$. Why is this intersection a single point, and why is $g$ continuous? This is what I did so far. Since $f$ is uniformly continuous, for given $\epsilon>0$, there is $\delta>0$ such that $\text{diam }f(V)<\epsilon$ whenever $\text{diam }V<\delta$. Since $V_n(p)$ has diameter at most $\frac{2}{n}$, taking $n>2/\delta$ would imply  $$ \text{diam }f(V_n(p))=\text{diam }\overline{f(V_n(p))}<\epsilon $$ So I think $\lim_{n\to\infty}\text{diam }\overline{f(V_n(p))}=0$, which would imply $A$ consists of at most one point. I noticed that the closures form a descending sequence of closed sets, but I couldn't tell if they are bounded since $X$ is an arbitrary metric space, in order to conclude that the intersection is nonempty, and hence a single point. Lastly, why is $g$ continuous at points $p\in X\setminus E$? I was trying to think of an argument with sequences converging to $p$ since $p$ is a limit point of $E$, but got stumping on how to show $g$ is actually continuous. Thanks.",,"['real-analysis', 'metric-spaces']"
22,Proving the limit of a function of a sequence is equal to the function of the limit of that sequence,Proving the limit of a function of a sequence is equal to the function of the limit of that sequence,,"Suppose $f$ is a continuous function at $x = c$ in $[a,b]$ .  Prove that for any sequence ${x_n}$ in $[a,b]$ converging to $c$ , the sequence $\{f(x_n)\}$ converges to $f(c)$ .  That is, $$ \lim_{n\to\infty}f(x_n)= f\left(\lim_{n\to\infty}x_n\right)$$ This proof seems simple but there are a few things that I need to know first. If $\{x_n\}$ converges to $c$ , is it sufficient to substitute $c$ in for $\lim_{n\to\infty}x_n$ ?  Also needing some guidance on the structure of this proof.  Thanks!","Suppose is a continuous function at in .  Prove that for any sequence in converging to , the sequence converges to .  That is, This proof seems simple but there are a few things that I need to know first. If converges to , is it sufficient to substitute in for ?  Also needing some guidance on the structure of this proof.  Thanks!","f x = c [a,b] {x_n} [a,b] c \{f(x_n)\} f(c)  \lim_{n\to\infty}f(x_n)= f\left(\lim_{n\to\infty}x_n\right) \{x_n\} c c \lim_{n\to\infty}x_n","['real-analysis', 'proof-writing']"
23,What subsets of $\Bbb R$ are closed under countable sums?,What subsets of  are closed under countable sums?,\Bbb R,"More precisely: Definition. A subset $S \subset \Bbb R$ is called good if the following hold: if $x, y \in S$ , then $x + y \in S,$ and if $(x_n)_{n = 1}^\infty \subset S$ is a sequence in $S$ and $\sum_{n = 1}^\infty x_n$ converges, then $\sum_{n = 1}^\infty x_n \in S$ . In other words, a good subset is closed under finite sums and countable sums whenever the sum does exist. Question: What are all the good subsets of $\Bbb R$ ? Origin This question was asked recently and Conifold had commented how the only subsets of $\Bbb R$ closed under countable summations are $\varnothing$ and $\{0\}$ . It was then natural to ask ""closed under countable summation, assuming it exists"". My thoughts Here are some examples of familiar sets which are good: $\varnothing$ , $\{0\}$ , $\Bbb Z_{\geq 0}$ , $\Bbb Z_{> 0}$ , $\Bbb Z$ , $n\Bbb Z$ , $\Bbb R$ . We even have the following: $$r \Bbb Z := \{rn : n \in \Bbb Z\},$$ where $r$ is any real number. But the examples apart from $\Bbb R$ are good for a trivial reason: Those are sets that are closed under finite summation and have the property that they are discrete enough so that the only convergent sums are those where the terms are eventually $0$ . (In the case of $\Bbb Z_{> 0}$ , there is no such sum.) On the same note, intervals of the form $[a, \infty)$ and $(a, \infty)$ are good for $a > 0$ . In general, suppose that $S$ satisfies the following: $S$ is closed under finite sums and there exists $\epsilon > 0$ such that $|s| > \epsilon$ for all $s \in S$ . Then, $S$ and $S \cup \{0\}$ are good. Another example: $[0, \infty)$ and $(0, \infty)$ are good and do not follow the criteria above. The following are some examples of not good sets: $\Bbb Q$ , $\Bbb R \setminus \Bbb Q$ , $\Bbb R \setminus \Bbb Z$ , a proper cofinite subset of $\Bbb R$ , any bounded set apart from $\{0\}$ and $\varnothing$ . In fact, excluding $\Bbb Q$ , the other ones are not even closed under finite sums. In the same vein as $\Bbb Q$ , we also have the set of real algebraic numbers which is not good (but is indeed closed under finite sums). Here's a nontrivial one: Consider the set $$B = \left\{\frac{1}{2^k} : k \in \Bbb Z_{> 0}\right\}.$$ Then, any countable subset of $\Bbb R$ that contains $B$ is not good. Proof. $(0, 1]$ is uncountable and every element in it can be written as a sum of elements of $B$ . (Binary expansions.) $\Box$ A bit more thought actually shows that more is true: Since $(0, 1]$ is contained in the set of all possible sums, any good set containing $B$ must contain all of $(0, \infty)$ . Another one: let $(a_n)_{n \ge 1}$ be any real sequence such that $\sum a_n$ converges conditionally. Then, the only good subset containing $\{a_n\}_{n \ge 1}$ is $\Bbb R$ , by the Riemann rearrangement theorem. Additional comments There are some variants that come to mind. Not sure if any of them are any more interesting. But I'd be happy with an answer that answers only one of the following variants as well. What if I exclude point 1. from my definition? Let's call such a set nice. In that case, the set $\{1\}$ is nice but not good. (Of course, if $0 \in S$ , then nice is equivalent to good.) What are the nice subsets of $\Bbb R$ ? What are the nice subsets which are not good? What if I consider only those sums which have converge absolutely? More observations (These are edits, which I'm adding later) Here are some additional observations: Arbitrary intersection of good sets is good and $\Bbb R$ is good. Thus, it makes sense to talk about the smallest good set containing a given subset of $\Bbb R$ . So, given a subset $A \subset \Bbb R$ , let us call this smallest good set to be the good set generated by $A$ and notationally denote it as $\langle A \rangle$ . (In particular, $A$ is good iff $A = \langle A \rangle$ .) $A \subset B \implies \langle A \rangle \subset \langle B \rangle$ . $\langle (0, \epsilon) \rangle = (0, \infty)$ and similar symmetric results. Suppose $S \subset (0, \infty)$ is dense in $(0, \infty)$ , then $\langle S \rangle = (0, \infty)$ . Indeed, pick $a_0 \in (0, \infty)$ . Then, there exists $s_1 \in (a_0/2, a_0)$ . Put $a_1 := a - s_1$ . Then, $a_1 \in (0, a_0/2)$ . Now pick $s_2 \in (a_1/2, a_1)$ and so on. Then, $\sum s_n = a_0$ . Symmetric results apply. In particular, the only dense good subset of $\Bbb R$ (or $\Bbb R^+$ or $\Bbb R^-$ ) is the whole set. Suppose $S \subset (0, \infty)$ contains arbitrarily small elements (i.e., $S \cap (0, \epsilon) \neq \varnothing$ for all $\epsilon > 0)$ ), then $\langle S \rangle = (0, \infty)$ . To see this, let $a > 0$ be arbitrary. Pick $s_1 \in (0, a)$ . Let $n_1$ be the largest positive integer such that $n_1 s_1 < a$ . Then pick $s_2 \in (0, a - n_1s_1)$ . Let $n_2$ be the largest such $n_2s_2 < a - n_1s_1$ and so on. Then, $$\underbrace{s_1 + \cdots + s_1}_{n_1} + \underbrace{s_2 + \cdots + s_2}_{n_2} + \cdots = a.$$","More precisely: Definition. A subset is called good if the following hold: if , then and if is a sequence in and converges, then . In other words, a good subset is closed under finite sums and countable sums whenever the sum does exist. Question: What are all the good subsets of ? Origin This question was asked recently and Conifold had commented how the only subsets of closed under countable summations are and . It was then natural to ask ""closed under countable summation, assuming it exists"". My thoughts Here are some examples of familiar sets which are good: , , , , , , . We even have the following: where is any real number. But the examples apart from are good for a trivial reason: Those are sets that are closed under finite summation and have the property that they are discrete enough so that the only convergent sums are those where the terms are eventually . (In the case of , there is no such sum.) On the same note, intervals of the form and are good for . In general, suppose that satisfies the following: is closed under finite sums and there exists such that for all . Then, and are good. Another example: and are good and do not follow the criteria above. The following are some examples of not good sets: , , , a proper cofinite subset of , any bounded set apart from and . In fact, excluding , the other ones are not even closed under finite sums. In the same vein as , we also have the set of real algebraic numbers which is not good (but is indeed closed under finite sums). Here's a nontrivial one: Consider the set Then, any countable subset of that contains is not good. Proof. is uncountable and every element in it can be written as a sum of elements of . (Binary expansions.) A bit more thought actually shows that more is true: Since is contained in the set of all possible sums, any good set containing must contain all of . Another one: let be any real sequence such that converges conditionally. Then, the only good subset containing is , by the Riemann rearrangement theorem. Additional comments There are some variants that come to mind. Not sure if any of them are any more interesting. But I'd be happy with an answer that answers only one of the following variants as well. What if I exclude point 1. from my definition? Let's call such a set nice. In that case, the set is nice but not good. (Of course, if , then nice is equivalent to good.) What are the nice subsets of ? What are the nice subsets which are not good? What if I consider only those sums which have converge absolutely? More observations (These are edits, which I'm adding later) Here are some additional observations: Arbitrary intersection of good sets is good and is good. Thus, it makes sense to talk about the smallest good set containing a given subset of . So, given a subset , let us call this smallest good set to be the good set generated by and notationally denote it as . (In particular, is good iff .) . and similar symmetric results. Suppose is dense in , then . Indeed, pick . Then, there exists . Put . Then, . Now pick and so on. Then, . Symmetric results apply. In particular, the only dense good subset of (or or ) is the whole set. Suppose contains arbitrarily small elements (i.e., for all ), then . To see this, let be arbitrary. Pick . Let be the largest positive integer such that . Then pick . Let be the largest such and so on. Then,","S \subset \Bbb R x, y \in S x + y \in S, (x_n)_{n = 1}^\infty \subset S S \sum_{n = 1}^\infty x_n \sum_{n = 1}^\infty x_n \in S \Bbb R \Bbb R \varnothing \{0\} \varnothing \{0\} \Bbb Z_{\geq 0} \Bbb Z_{> 0} \Bbb Z n\Bbb Z \Bbb R r \Bbb Z := \{rn : n \in \Bbb Z\}, r \Bbb R 0 \Bbb Z_{> 0} [a, \infty) (a, \infty) a > 0 S S \epsilon > 0 |s| > \epsilon s \in S S S \cup \{0\} [0, \infty) (0, \infty) \Bbb Q \Bbb R \setminus \Bbb Q \Bbb R \setminus \Bbb Z \Bbb R \{0\} \varnothing \Bbb Q \Bbb Q B = \left\{\frac{1}{2^k} : k \in \Bbb Z_{> 0}\right\}. \Bbb R B (0, 1] B \Box (0, 1] B (0, \infty) (a_n)_{n \ge 1} \sum a_n \{a_n\}_{n \ge 1} \Bbb R \{1\} 0 \in S \Bbb R \Bbb R \Bbb R A \subset \Bbb R A \langle A \rangle A A = \langle A \rangle A \subset B \implies \langle A \rangle \subset \langle B \rangle \langle (0, \epsilon) \rangle = (0, \infty) S \subset (0, \infty) (0, \infty) \langle S \rangle = (0, \infty) a_0 \in (0, \infty) s_1 \in (a_0/2, a_0) a_1 := a - s_1 a_1 \in (0, a_0/2) s_2 \in (a_1/2, a_1) \sum s_n = a_0 \Bbb R \Bbb R^+ \Bbb R^- S \subset (0, \infty) S \cap (0, \epsilon) \neq \varnothing \epsilon > 0) \langle S \rangle = (0, \infty) a > 0 s_1 \in (0, a) n_1 n_1 s_1 < a s_2 \in (0, a - n_1s_1) n_2 n_2s_2 < a - n_1s_1 \underbrace{s_1 + \cdots + s_1}_{n_1} + \underbrace{s_2 + \cdots + s_2}_{n_2} + \cdots = a.","['real-analysis', 'sequences-and-series', 'analysis']"
24,How to analyze $\sup_{x>0}|e^xf(x)| < \infty$ and $\sup_{n\in\mathbb{N}} |f^{(n)}(0)|< \infty$?,How to analyze  and ?,\sup_{x>0}|e^xf(x)| < \infty \sup_{n\in\mathbb{N}} |f^{(n)}(0)|< \infty,"Suppose that $$f(x)=1+\sum_{n=1}^\infty a_n \frac{x^n}{n!}\ \ \forall \ x\in \mathbb{R}$$ where $\sup_{x>0}\left|e^xf(x)\right| < \infty$ and $\sup_{n\in\mathbb{N}} |a_n|< \infty$ . Prove that $a_n = (-1)^n$ , $\forall n\in \mathbb{N}$ It seems amazing to me. What we need to prove is $f(x)=e^{-x}$ . It seems insufficient to prove this strong conclusion, but actually it is true and all the ""counterexamples"" I found were wrong. My attempt Put $g(x)=e^x f(x)$ . $$\left|g^{(n)}(0)\right|=\left|\sum_{k=0}^n \binom{n}{k} f^{(k)}(0)\right|\le 2^n\sup_{n\in\mathbb{N}} |a_n|  $$ Put $h(x)=g(\frac{x}{2})$ . Thus $$h^{(n)}(0)=\frac{1}{2^n}g(0) \le \sup_{n\in\mathbb{N}} |a_n|$$ which implies that $$h(x)=1+\sum_{n=1}^\infty b_n \frac{x^n}{n!}\ \ \forall \ x\in \mathbb{R}$$ where $$|b_n|\le \sup_{n\in\mathbb{N}} |a_n| \ \ \forall \ n\in\mathbb{N}  \,\,\,\,\,\& \,\,\,\,\,  \sup_{x>0}\left|h(x)\right| < \infty $$ And hence if $b_k<0$ , then there exists $l>k$ such that $b_l>0$ . I want to yield a contradiction by supposing this, but I failed. Any hints or other new ideas? Thanks in advance! (I heard that this problem can be solved by complex analysis. This is the reason why I attach the complex-analysis tag.)","Suppose that where and . Prove that , It seems amazing to me. What we need to prove is . It seems insufficient to prove this strong conclusion, but actually it is true and all the ""counterexamples"" I found were wrong. My attempt Put . Put . Thus which implies that where And hence if , then there exists such that . I want to yield a contradiction by supposing this, but I failed. Any hints or other new ideas? Thanks in advance! (I heard that this problem can be solved by complex analysis. This is the reason why I attach the complex-analysis tag.)","f(x)=1+\sum_{n=1}^\infty a_n \frac{x^n}{n!}\ \ \forall \ x\in \mathbb{R} \sup_{x>0}\left|e^xf(x)\right| < \infty \sup_{n\in\mathbb{N}} |a_n|< \infty a_n = (-1)^n \forall n\in \mathbb{N} f(x)=e^{-x} g(x)=e^x f(x) \left|g^{(n)}(0)\right|=\left|\sum_{k=0}^n \binom{n}{k} f^{(k)}(0)\right|\le 2^n\sup_{n\in\mathbb{N}} |a_n|   h(x)=g(\frac{x}{2}) h^{(n)}(0)=\frac{1}{2^n}g(0) \le \sup_{n\in\mathbb{N}} |a_n| h(x)=1+\sum_{n=1}^\infty b_n \frac{x^n}{n!}\ \ \forall \ x\in \mathbb{R} |b_n|\le \sup_{n\in\mathbb{N}} |a_n| \ \ \forall \ n\in\mathbb{N} 
\,\,\,\,\,\& \,\,\,\,\,  \sup_{x>0}\left|h(x)\right| < \infty  b_k<0 l>k b_l>0","['real-analysis', 'complex-analysis', 'power-series']"
25,Are eigenvalues of the limit of a sequence of matrices limits of eigenvalue sequences?,Are eigenvalues of the limit of a sequence of matrices limits of eigenvalue sequences?,,"Let $\{A_n\}\in \mathbb{R}^{m\times m}$ be a sequence of symmetric matrices such that $A_n\to A$ as $n\to \infty$ , i.e. $\lim_{n\to \infty}a_{ij}(n)=a_{ij}\ \forall 1\le i,j\le m$ where $A_n=[a_{ij}(n)],A=[a_{ij}]$ . Let $\rho(A_n)=\{\lambda_1(n),\cdots,\ \lambda_m(n)\}$ be the eigenvalues of $A_n$ and similarly, $\rho(A)=\{\lambda_1,\cdots,\ \lambda_m\}$ be the eigenvalues of $A$ , arranged in, say, increasing order. Here are my questions 1)Can I write $\lambda_k(n)\to \lambda_k,\ 1\le k\le m$ ? 2)If I define (with a slight abuse of standard notation) $\delta_s(n),\ 1\le s\le m$ as the maximum eigenvalue of any $s\times s$ submatrix of $A_n$ and if $\delta_s$ be the corresponding quantity for $A$ , then can I say that $$\lim_{n\to \infty}\delta_s(n)=\delta_s$$ ? Intuitively it seems to me that the answers are positive since the eigenvalues of a matrix are continuous functions of the elements of the matrix and $\delta_s$ is just the maximum of some eigenvalues of submatrices. However, I am not sure if this argument is sound enough. Maybe this is a very trivial issue for people here, but I would really appreciate if someone can kindly provide some explanation regarding the correct answer. Thanks in advance.","Let be a sequence of symmetric matrices such that as , i.e. where . Let be the eigenvalues of and similarly, be the eigenvalues of , arranged in, say, increasing order. Here are my questions 1)Can I write ? 2)If I define (with a slight abuse of standard notation) as the maximum eigenvalue of any submatrix of and if be the corresponding quantity for , then can I say that ? Intuitively it seems to me that the answers are positive since the eigenvalues of a matrix are continuous functions of the elements of the matrix and is just the maximum of some eigenvalues of submatrices. However, I am not sure if this argument is sound enough. Maybe this is a very trivial issue for people here, but I would really appreciate if someone can kindly provide some explanation regarding the correct answer. Thanks in advance.","\{A_n\}\in \mathbb{R}^{m\times m} A_n\to A n\to \infty \lim_{n\to \infty}a_{ij}(n)=a_{ij}\ \forall 1\le i,j\le m A_n=[a_{ij}(n)],A=[a_{ij}] \rho(A_n)=\{\lambda_1(n),\cdots,\ \lambda_m(n)\} A_n \rho(A)=\{\lambda_1,\cdots,\ \lambda_m\} A \lambda_k(n)\to \lambda_k,\ 1\le k\le m \delta_s(n),\ 1\le s\le m s\times s A_n \delta_s A \lim_{n\to \infty}\delta_s(n)=\delta_s \delta_s","['real-analysis', 'sequences-and-series', 'matrices', 'eigenvalues-eigenvectors']"
26,"a ""natural"" real number that is not computable","a ""natural"" real number that is not computable",,"Most of the examples of non-computable real numbers use some kind of a diagonalization construction over some turing computable model of computation. See Are there any examples of non-computable real numbers? . I want to know if there are ""natural"" real numbers that are not computable. I'm having difficulty in formalizing what I mean by ""natural"". Here is a necessary condition for naturality: The description of that number should not mention any turing computable model of computation. Ideally, this number should have existed in the literature even before Turing invented Turing machines. Somehow this is analogous to the way Solomon Feferman says: Finally, we must take note of the fact that up to now, no previously (w.r.t. the day Gödel announced his incompleteness theorems) formulated open problem from number theory or finite combinatorics, such as the Goldbach conjecture or the Riemann Hypothesis or the twin prime conjecture or the P=NP problem, is known to be independent of the kinds of formal systems we have been talking about,not even of PA. in http://math.stanford.edu/~feferman/papers/newaxioms.pdf . The parts in parenthesis have been added by me to put his quote in proper context. My question was partly motivated by this quote.","Most of the examples of non-computable real numbers use some kind of a diagonalization construction over some turing computable model of computation. See Are there any examples of non-computable real numbers? . I want to know if there are ""natural"" real numbers that are not computable. I'm having difficulty in formalizing what I mean by ""natural"". Here is a necessary condition for naturality: The description of that number should not mention any turing computable model of computation. Ideally, this number should have existed in the literature even before Turing invented Turing machines. Somehow this is analogous to the way Solomon Feferman says: Finally, we must take note of the fact that up to now, no previously (w.r.t. the day Gödel announced his incompleteness theorems) formulated open problem from number theory or finite combinatorics, such as the Goldbach conjecture or the Riemann Hypothesis or the twin prime conjecture or the P=NP problem, is known to be independent of the kinds of formal systems we have been talking about,not even of PA. in http://math.stanford.edu/~feferman/papers/newaxioms.pdf . The parts in parenthesis have been added by me to put his quote in proper context. My question was partly motivated by this quote.",,"['real-analysis', 'logic', 'constructive-mathematics']"
27,Is this Riemann sum formula for definite integral using of prime numbers true?,Is this Riemann sum formula for definite integral using of prime numbers true?,,"While answering another question in MSE , I had used the following result which I thought was a trivial consequence of the prime number theorem and equidistribution.  However, I realized from the comments that many people  thought that this was not either true or counter intuitive. Hence I am posting this as a question looking for a proof or disproof. Let $p_k$ be the $k$-th prime and $f$ be a continuous function   Riemann integrable in $(0,1)$ such that $$\lim_{n \to \infty}\frac{1}{n}\sum_{r = 1}^{n}f\Big(\frac{r}{n}\Big) = \int_{0}^{1}f(x)dx.  $$ Then,    $$ \lim_{n \to \infty}\frac{1}{n}\sum_{r = 1}^{n}f\Big(\frac{p_r}{p_n}\Big) = \int_{0}^{1}f(x)dx. $$ My proof was based on showing that as $n \to \infty$, the ratios $p_r/p_n$ approached equidistribution in $(0,1)$ hence the integral follows as a trivial property of equidistributed sequence. Motivation : There are several identities, limits etc on prime numbers which can be easily proven using this simple formula, including all answers to all three questions on the arithmetic, geometric and harmonic means of primes mentioned in the above link.","While answering another question in MSE , I had used the following result which I thought was a trivial consequence of the prime number theorem and equidistribution.  However, I realized from the comments that many people  thought that this was not either true or counter intuitive. Hence I am posting this as a question looking for a proof or disproof. Let $p_k$ be the $k$-th prime and $f$ be a continuous function   Riemann integrable in $(0,1)$ such that $$\lim_{n \to \infty}\frac{1}{n}\sum_{r = 1}^{n}f\Big(\frac{r}{n}\Big) = \int_{0}^{1}f(x)dx.  $$ Then,    $$ \lim_{n \to \infty}\frac{1}{n}\sum_{r = 1}^{n}f\Big(\frac{p_r}{p_n}\Big) = \int_{0}^{1}f(x)dx. $$ My proof was based on showing that as $n \to \infty$, the ratios $p_r/p_n$ approached equidistribution in $(0,1)$ hence the integral follows as a trivial property of equidistributed sequence. Motivation : There are several identities, limits etc on prime numbers which can be easily proven using this simple formula, including all answers to all three questions on the arithmetic, geometric and harmonic means of primes mentioned in the above link.",,"['real-analysis', 'integration', 'number-theory', 'definite-integrals', 'prime-numbers']"
28,"Show that a set is dense in $[-1,1]$",Show that a set is dense in,"[-1,1]","Show that $\{\cos\ n:n \in \mathbb{N}\}$ is dense in $[-1,1]$ by using the fact below: Suppose $x$ is irrational. Then there exists $p_n,q_n \in \mathbb{Z}$   such that $\bigg|x -\frac{p_n}{q_n}\bigg| < \frac{1}{q_n^2}$ I have no idea on how to apply the fact above to show the set is dense in $[-1,1]$. Can anyone help me?","Show that $\{\cos\ n:n \in \mathbb{N}\}$ is dense in $[-1,1]$ by using the fact below: Suppose $x$ is irrational. Then there exists $p_n,q_n \in \mathbb{Z}$   such that $\bigg|x -\frac{p_n}{q_n}\bigg| < \frac{1}{q_n^2}$ I have no idea on how to apply the fact above to show the set is dense in $[-1,1]$. Can anyone help me?",,['real-analysis']
29,How to deal with Homeomorphisms?,How to deal with Homeomorphisms?,,"I have one doubt that may be too general, I don't know, so sorry if this is not a good place to ask it. I've also seem many other people with the same problem that I have, so I think that if this question fits this site, it'll help other people to. I've been studying multivariable analysis, metric spaces and manifolds, and in all of them I find the same problem: although I already understood the main definitions and results I find myself a little lost when it comes to construct and to prove homeomorphisms. For instance, in linear algebra when it comes to prove isomorphism of vector spaces I know a ""procedure"", I have a line of thought that even though can be dificult in some cases, will end up giving what I was seeking for. One point of this ""procedure"" is that we now that once we have the map it suffices to show that it's linear (a simple check of a property), show that it's kernel is just the null vector and to show surjectivity we look at the dimensions. However, when it comes to construct and prove homeomorphism it seems like ""the only way is to make a good guess"", without some procedure and something like that. For example, it's not intuitive, at least to me that to show that the open ball is homeomorphic to $\mathbb{R}^n$ we would need to take the map $f(x) = x/(1+|x|)$. It's just that I look to this map and I think: ""I would never have thought of it"". Anyway, if finding the map seems a problem, proving that the map indeed is homeomorphism seems even worse, because the most common way: find an $\epsilon$ also seems like depending on ""good guesses"". Even for simple function on the real line I look at the $\epsilon$'s that usually are used to prove continuity and I think: ""I would never have thought of such a thing"". My question is: there is a systematic way of attacking those problems? Is there a procedure to find and prove hoemorphisms like there's in linear algebra to find and prove isomorphisms? Is there a way to make this less dependent on guesses? In the real line people often draw the small intervals, however, this kind of thing seems not too good, since we won't have this ""graphical resource"" to find a way to prove homeomorphisms between higher dimensional manifolds. Where can I really learn those things? My interest is really the study of manifolds, and I'm working with Spivak's ""A Comprehensive Introduction to Differential Geometry Vol. 1"", however I'm feeling the need to better understand these questions about how to construct and how to prove homeomorphisms, since all the charts for the manifolds must be construct as homeomorphisms. Thanks in advance for your help.","I have one doubt that may be too general, I don't know, so sorry if this is not a good place to ask it. I've also seem many other people with the same problem that I have, so I think that if this question fits this site, it'll help other people to. I've been studying multivariable analysis, metric spaces and manifolds, and in all of them I find the same problem: although I already understood the main definitions and results I find myself a little lost when it comes to construct and to prove homeomorphisms. For instance, in linear algebra when it comes to prove isomorphism of vector spaces I know a ""procedure"", I have a line of thought that even though can be dificult in some cases, will end up giving what I was seeking for. One point of this ""procedure"" is that we now that once we have the map it suffices to show that it's linear (a simple check of a property), show that it's kernel is just the null vector and to show surjectivity we look at the dimensions. However, when it comes to construct and prove homeomorphism it seems like ""the only way is to make a good guess"", without some procedure and something like that. For example, it's not intuitive, at least to me that to show that the open ball is homeomorphic to $\mathbb{R}^n$ we would need to take the map $f(x) = x/(1+|x|)$. It's just that I look to this map and I think: ""I would never have thought of it"". Anyway, if finding the map seems a problem, proving that the map indeed is homeomorphism seems even worse, because the most common way: find an $\epsilon$ also seems like depending on ""good guesses"". Even for simple function on the real line I look at the $\epsilon$'s that usually are used to prove continuity and I think: ""I would never have thought of such a thing"". My question is: there is a systematic way of attacking those problems? Is there a procedure to find and prove hoemorphisms like there's in linear algebra to find and prove isomorphisms? Is there a way to make this less dependent on guesses? In the real line people often draw the small intervals, however, this kind of thing seems not too good, since we won't have this ""graphical resource"" to find a way to prove homeomorphisms between higher dimensional manifolds. Where can I really learn those things? My interest is really the study of manifolds, and I'm working with Spivak's ""A Comprehensive Introduction to Differential Geometry Vol. 1"", however I'm feeling the need to better understand these questions about how to construct and how to prove homeomorphisms, since all the charts for the manifolds must be construct as homeomorphisms. Thanks in advance for your help.",,"['real-analysis', 'general-topology', 'soft-question', 'manifolds']"
30,Converse of the Weierstrass $M$-Test?,Converse of the Weierstrass -Test?,M,"I was assigned a few problems in my Honors Calculus II class, and one of them was kind of interesting to do: Suppose that $f_{n}$ are nonnegative bounded functions on $A$ and let $M_{n} =  \sup f_{n}$. If $\displaystyle\sum\limits_{n=1}^\infty f_{n}$ converges uniformly on $A$, does it follow that $\displaystyle\sum\limits_{n=1}^\infty M_{n}$ converges (a converse to the Weirstrass $M$-test)? I know that this question has been asked before , but I'm trying not to just copy an answer off the internet and instead to come up with an example of my own to see if I can actually understand the theorems that I'm learning. To provide a counterexample, I tried to create a function which has a diverging $\sup$, but I'm not too confident that my proof is valid. Here it goes: $$ \text Let \ f_{n}(x) = \begin{cases}   \begin{cases}     \frac{1 + x}{2}, & \text{if }x \in (-1,0]\\     \frac{1 - x}{2}, & \text{if }x \in (0,1)\\     0, & \text{elsewhere}   \end{cases}, & \text{if }n \text{ even}\\   \begin{cases}     x, & \text{if }x \in (0,1]\\     2 - x, & \text{if }x \in (1,2)\\     0, & \text{elsewhere}   \end{cases}, & \text{if }n \text{ odd} \end{cases} $$ Now, $\text Let f(x) = 0$. From this definition, I can conclude that $$ \sup\{f_{n}\} = \begin{cases}   \frac{1}{2}, & \text{if }n \text{ even}\\   1, & \text{if }n \text{ odd} \end{cases} $$ Now, to show that ${f_{n}(x)}$ is uniformly convergent, the definition of uniform convergence is used: $$ \forall_{\epsilon > 0}\ \exists_{N}\ \text s.t.\ \forall_{x}\ \text if\ n > N, |f(x) - f_{n}(x)| < \epsilon $$ Since $f_{n}(x)$ is strictly nonnegative and $f(x) = 0$, $|f(x) - f_{n}(x)| = f_{n}(x)$. By definition, $\epsilon > 0$, and since $f_{n}(x) = 0$ for $x \geq 2, f(x) - f_{n}(x) = 0 < \epsilon$ for $x \geq 2$. Therefore there exists a $N$ (namely, $N = 1$) which proves that the sequence is uniformly convergent. Since $\lim_{n\to\infty} f_{n} \neq 0$, by the Limit Test, the infinite sum  $\displaystyle\sum\limits_{n=1}^\infty \sup{f_{n}}$ diverges, which disproves the converse of the Weierstrass $M$-Test. $\blacksquare$ This is the first time I've actually used LaTeX, so I'm sorry for the way it looks. Is there anything that I can do to make this proof better (or even valid, if it's wrong), or is it fine the way it is? This might be a bit of a long question...","I was assigned a few problems in my Honors Calculus II class, and one of them was kind of interesting to do: Suppose that $f_{n}$ are nonnegative bounded functions on $A$ and let $M_{n} =  \sup f_{n}$. If $\displaystyle\sum\limits_{n=1}^\infty f_{n}$ converges uniformly on $A$, does it follow that $\displaystyle\sum\limits_{n=1}^\infty M_{n}$ converges (a converse to the Weirstrass $M$-test)? I know that this question has been asked before , but I'm trying not to just copy an answer off the internet and instead to come up with an example of my own to see if I can actually understand the theorems that I'm learning. To provide a counterexample, I tried to create a function which has a diverging $\sup$, but I'm not too confident that my proof is valid. Here it goes: $$ \text Let \ f_{n}(x) = \begin{cases}   \begin{cases}     \frac{1 + x}{2}, & \text{if }x \in (-1,0]\\     \frac{1 - x}{2}, & \text{if }x \in (0,1)\\     0, & \text{elsewhere}   \end{cases}, & \text{if }n \text{ even}\\   \begin{cases}     x, & \text{if }x \in (0,1]\\     2 - x, & \text{if }x \in (1,2)\\     0, & \text{elsewhere}   \end{cases}, & \text{if }n \text{ odd} \end{cases} $$ Now, $\text Let f(x) = 0$. From this definition, I can conclude that $$ \sup\{f_{n}\} = \begin{cases}   \frac{1}{2}, & \text{if }n \text{ even}\\   1, & \text{if }n \text{ odd} \end{cases} $$ Now, to show that ${f_{n}(x)}$ is uniformly convergent, the definition of uniform convergence is used: $$ \forall_{\epsilon > 0}\ \exists_{N}\ \text s.t.\ \forall_{x}\ \text if\ n > N, |f(x) - f_{n}(x)| < \epsilon $$ Since $f_{n}(x)$ is strictly nonnegative and $f(x) = 0$, $|f(x) - f_{n}(x)| = f_{n}(x)$. By definition, $\epsilon > 0$, and since $f_{n}(x) = 0$ for $x \geq 2, f(x) - f_{n}(x) = 0 < \epsilon$ for $x \geq 2$. Therefore there exists a $N$ (namely, $N = 1$) which proves that the sequence is uniformly convergent. Since $\lim_{n\to\infty} f_{n} \neq 0$, by the Limit Test, the infinite sum  $\displaystyle\sum\limits_{n=1}^\infty \sup{f_{n}}$ diverges, which disproves the converse of the Weierstrass $M$-Test. $\blacksquare$ This is the first time I've actually used LaTeX, so I'm sorry for the way it looks. Is there anything that I can do to make this proof better (or even valid, if it's wrong), or is it fine the way it is? This might be a bit of a long question...",,"['calculus', 'real-analysis']"
31,Polynomials $f$ and $f'$ with all roots distinct integers,Polynomials  and  with all roots distinct integers,f f',"Edit 2. Since the question below appears to be open for degree seven and above, I have re-tagged appropriately, and also suggested this on MathOverflow ( link ) as a potential polymath project . Edit 1. A re-phrasing thanks to a comment below: Is it true that, for all $n \in \mathbb{N}$, there exists a degree $n$ polynomial $f \in \mathbb{Z}[x]$ such that both $f$ and $f'$ have all of their roots being distinct integers? (If not, what is the minimal $n$ to serve as a counterexample?) The worked example below for $n = 3$ uses $f$ with roots $\{-9, 0, 24\}$ and $f'$ with roots $\{-18, -4\}$. (See also the note at the end, and the linked arXiv paper.) Question. For all $n \in \mathbb{N}$: Is it possible to find a polynomial in $\mathbb{Z}[x]$ with $n$ distinct $x$-intercepts, and all of its turning points, at lattice points? This is clearly true when $n = 1$ and $n = 2$. A bit of investigation around $n = 3$ leads to, e.g., the polynomial defined by: $$f(x) = x^3 + 33x^2 + 216x = x(x+9)(x+24)$$ which has $x$-intercepts at $(0,0)$, $(-9, 0)$, and $(-24, 0)$. Taking the derivative, we find that: $$f'(x) = 3x^2 + 66x + 216 = 3(x+4)(x+18)$$ so that the turning points of $f$ occur at $(-4, -400)$ and $(-18, 972)$. I am not even sure if this is true in the quartic ${^1}$ case; nevertheless, this question concerns the more general setting. In particular, is the statement true for all $n \in \mathbb{N}$ and if not , then what is the minimal $n$ for which this is not possible? $1$. Will Jagy kindly resolves $n=4$ since the monic quartic $f$ with integer roots $\{-7, -1, 1, 7\}$ leads to an $f'$ with roots $\{-5, 0, 5\}$. This example is also found as B5 in the paper here (PDF 22/24). The same paper has the cubic example above as B1 , and includes a quintic example as B7 : $$f(x) = x(x-180)(x-285)(x-460)(x-780)$$ $$\text{ and }$$ $$f'(x) = 5(x-60)(x-230)(x-390)(x-684)$$ The linked arXiv (unpublished) manuscript seems to suggest that this problem is open.","Edit 2. Since the question below appears to be open for degree seven and above, I have re-tagged appropriately, and also suggested this on MathOverflow ( link ) as a potential polymath project . Edit 1. A re-phrasing thanks to a comment below: Is it true that, for all $n \in \mathbb{N}$, there exists a degree $n$ polynomial $f \in \mathbb{Z}[x]$ such that both $f$ and $f'$ have all of their roots being distinct integers? (If not, what is the minimal $n$ to serve as a counterexample?) The worked example below for $n = 3$ uses $f$ with roots $\{-9, 0, 24\}$ and $f'$ with roots $\{-18, -4\}$. (See also the note at the end, and the linked arXiv paper.) Question. For all $n \in \mathbb{N}$: Is it possible to find a polynomial in $\mathbb{Z}[x]$ with $n$ distinct $x$-intercepts, and all of its turning points, at lattice points? This is clearly true when $n = 1$ and $n = 2$. A bit of investigation around $n = 3$ leads to, e.g., the polynomial defined by: $$f(x) = x^3 + 33x^2 + 216x = x(x+9)(x+24)$$ which has $x$-intercepts at $(0,0)$, $(-9, 0)$, and $(-24, 0)$. Taking the derivative, we find that: $$f'(x) = 3x^2 + 66x + 216 = 3(x+4)(x+18)$$ so that the turning points of $f$ occur at $(-4, -400)$ and $(-18, 972)$. I am not even sure if this is true in the quartic ${^1}$ case; nevertheless, this question concerns the more general setting. In particular, is the statement true for all $n \in \mathbb{N}$ and if not , then what is the minimal $n$ for which this is not possible? $1$. Will Jagy kindly resolves $n=4$ since the monic quartic $f$ with integer roots $\{-7, -1, 1, 7\}$ leads to an $f'$ with roots $\{-5, 0, 5\}$. This example is also found as B5 in the paper here (PDF 22/24). The same paper has the cubic example above as B1 , and includes a quintic example as B7 : $$f(x) = x(x-180)(x-285)(x-460)(x-780)$$ $$\text{ and }$$ $$f'(x) = 5(x-60)(x-230)(x-390)(x-684)$$ The linked arXiv (unpublished) manuscript seems to suggest that this problem is open.",,"['calculus', 'real-analysis', 'derivatives', 'polynomials', 'open-problem']"
32,What makes the Cauchy-Schwarz inequality so important?,What makes the Cauchy-Schwarz inequality so important?,,"The Cauchy-Schwarz inequality is $(a\cdot b)^2 \leq |a|^2|b|^2$.  Why is this considered such an important inequality: to quote my textbook it's ""one of the most important inequalities in all of mathematics"".  But why?  Doesn't it just immediately follow from the definition of the dot product: $a\cdot b = |a||b|\cos(\theta)$?  And even if you define the dot product differently, like maybe $a\cdot b = a_1b_1 + a_2b_2 +...$, it still doesn't seem all THAT important to me.  So what makes this particular inequality so important/ interesting?","The Cauchy-Schwarz inequality is $(a\cdot b)^2 \leq |a|^2|b|^2$.  Why is this considered such an important inequality: to quote my textbook it's ""one of the most important inequalities in all of mathematics"".  But why?  Doesn't it just immediately follow from the definition of the dot product: $a\cdot b = |a||b|\cos(\theta)$?  And even if you define the dot product differently, like maybe $a\cdot b = a_1b_1 + a_2b_2 +...$, it still doesn't seem all THAT important to me.  So what makes this particular inequality so important/ interesting?",,"['real-analysis', 'linear-algebra', 'analysis']"
33,Is Zorn's lemma necessary to show discontinuous $f\colon {\mathbb R} \to {\mathbb R}$ satisfying $f(x+y) = f(x) + f(y)$?,Is Zorn's lemma necessary to show discontinuous  satisfying ?,f\colon {\mathbb R} \to {\mathbb R} f(x+y) = f(x) + f(y),"A UC Berkeley prelim exam problem asked whether an additive function $f\colon {\mathbb R} \to {\mathbb R}$, i.e. satisfying $f(x + y) = f(x) + f(y)$ must be continuous. The counterexample involved taking a positive-valued Hamel basis $X$ of $\mathbb{R}$ as a vector space over ${\mathbb Q}$, and then letting $f(x_1) =1$ and $f(x_2)=-1$ for two different $x_1,x_2 \in X$, and letting $f(x)$ be arbitrary for other $x \in X$, and then extending the function to all of ${\mathbb R}$ using the property of additivity. Then a sequence $a_n = {p_nx_1 + q_nx_2}$ could be found with rational $p_n,q_n$ such that $a_n \to 0$ but $\lim \limits_{n\to \infty} f(a_n) \neq 0$, showing discontinuity. But is Zorn's Lemma necessary to produce such an example? In other words, is Zorn's Lemma saying we can find a Hamel basis of ${\mathbb R}$ over ${\mathbb Q}$ equivalent to being able to construct a discontinuous additive function $f\colon {\mathbb R} \to {\mathbb R}$?","A UC Berkeley prelim exam problem asked whether an additive function $f\colon {\mathbb R} \to {\mathbb R}$, i.e. satisfying $f(x + y) = f(x) + f(y)$ must be continuous. The counterexample involved taking a positive-valued Hamel basis $X$ of $\mathbb{R}$ as a vector space over ${\mathbb Q}$, and then letting $f(x_1) =1$ and $f(x_2)=-1$ for two different $x_1,x_2 \in X$, and letting $f(x)$ be arbitrary for other $x \in X$, and then extending the function to all of ${\mathbb R}$ using the property of additivity. Then a sequence $a_n = {p_nx_1 + q_nx_2}$ could be found with rational $p_n,q_n$ such that $a_n \to 0$ but $\lim \limits_{n\to \infty} f(a_n) \neq 0$, showing discontinuity. But is Zorn's Lemma necessary to produce such an example? In other words, is Zorn's Lemma saying we can find a Hamel basis of ${\mathbb R}$ over ${\mathbb Q}$ equivalent to being able to construct a discontinuous additive function $f\colon {\mathbb R} \to {\mathbb R}$?",,"['real-analysis', 'set-theory', 'functional-equations', 'axiom-of-choice']"
34,Evaluate $\lim_{x\to1^-}\left(\sum_{n=0}^{\infty}\left(x^{(2^n)}\right)-\log_2\frac{1}{1-x}\right)$,Evaluate,\lim_{x\to1^-}\left(\sum_{n=0}^{\infty}\left(x^{(2^n)}\right)-\log_2\frac{1}{1-x}\right),"Evaluate$$\lim_{x\to1^-}\left(\sum_{n=0}^{\infty}\left(x^{(2^n)}\right)-\log_2\frac{1}{1-x}\right)$$ Difficult problem. Been thinking about it for a few hours now. Pretty sure it's beyond my ability. Very frustrating to show that the limit even exists. Help, please. Either I'm not smart enough to solve this, or I haven't learned enough to solve this. And I want to know which!","Evaluate$$\lim_{x\to1^-}\left(\sum_{n=0}^{\infty}\left(x^{(2^n)}\right)-\log_2\frac{1}{1-x}\right)$$ Difficult problem. Been thinking about it for a few hours now. Pretty sure it's beyond my ability. Very frustrating to show that the limit even exists. Help, please. Either I'm not smart enough to solve this, or I haven't learned enough to solve this. And I want to know which!",,"['calculus', 'real-analysis', 'sequences-and-series', 'limits']"
35,Monotonic version of Weierstrass approximation theorem,Monotonic version of Weierstrass approximation theorem,,"Let $f\in\mathcal{C}^1([0,1])$ be an increasing function over $[0,1]$. Prove or disprove the existence of a sequence of real polynomials $\{p_n(x)\}_{n\in\mathbb{N}}$ with the properties: $p_n(x)$ is a non-decreasing function over $[0,1]$; the degree of $p_n$ is $n$; $\|f-p_n\|_{\infty}=\max_{x\in[0,1]}|f(x)-p_n(x)|=O\left(\frac{1}{n}\right)$.","Let $f\in\mathcal{C}^1([0,1])$ be an increasing function over $[0,1]$. Prove or disprove the existence of a sequence of real polynomials $\{p_n(x)\}_{n\in\mathbb{N}}$ with the properties: $p_n(x)$ is a non-decreasing function over $[0,1]$; the degree of $p_n$ is $n$; $\|f-p_n\|_{\infty}=\max_{x\in[0,1]}|f(x)-p_n(x)|=O\left(\frac{1}{n}\right)$.",,"['calculus', 'real-analysis', 'polynomials', 'approximation-theory']"
36,Differentiability of composition with every path implies differentiability,Differentiability of composition with every path implies differentiability,,"I'm struggling with this problem from my book Curso de Analise Vol. 2 by Elon Lages Lima. Let $U\subseteq \mathbb{R}^n$ be an open set, $a\in U$ and $f:U\to \mathbb{R}$ with the following property: for every $v\in \mathbb{R}^n$ and every path and $g:(-\epsilon,\epsilon)\to U$ with $g(0)=a$ and $g'(0)=v$ the composite map $f\circ g:(-\epsilon,\epsilon)\to R$ satisfies $(f\circ g)'(0)=T(v)$ where $T:\mathbb{R}^n\to\mathbb{R}$ is a fixed linear transformation. Prove that $f$ is differentiable at $a$. It's clear that one must have $f'(a)=T$ so one has to prove \begin{equation} \tag 0\lim_{v\to 0}\frac{f(a+v)-f(a)-T(v)}{|v|}=0 \end{equation} I can't think of any path other than $t\mapsto a+tv$ but then I'm not using the hypothesis to its full strength. I also tried a proof by negating that limit and trying to construct a path from there, but didn't succeed. How do I approach the problem? Any hints?","I'm struggling with this problem from my book Curso de Analise Vol. 2 by Elon Lages Lima. Let $U\subseteq \mathbb{R}^n$ be an open set, $a\in U$ and $f:U\to \mathbb{R}$ with the following property: for every $v\in \mathbb{R}^n$ and every path and $g:(-\epsilon,\epsilon)\to U$ with $g(0)=a$ and $g'(0)=v$ the composite map $f\circ g:(-\epsilon,\epsilon)\to R$ satisfies $(f\circ g)'(0)=T(v)$ where $T:\mathbb{R}^n\to\mathbb{R}$ is a fixed linear transformation. Prove that $f$ is differentiable at $a$. It's clear that one must have $f'(a)=T$ so one has to prove \begin{equation} \tag 0\lim_{v\to 0}\frac{f(a+v)-f(a)-T(v)}{|v|}=0 \end{equation} I can't think of any path other than $t\mapsto a+tv$ but then I'm not using the hypothesis to its full strength. I also tried a proof by negating that limit and trying to construct a path from there, but didn't succeed. How do I approach the problem? Any hints?",,"['real-analysis', 'derivatives']"
37,Least Upper Bound Property Implies Greatest Lower Bound Property,Least Upper Bound Property Implies Greatest Lower Bound Property,,"In Rudin $1.11$ Theorem Proof he claims the following Theorem . Suppose $S$ is an ordered set with the least upper bound property $B \subset S$, $B$ is not empty, and $B$ is bounded below. Let $L$ be the set of all lower bounds of $B$. Then    $$\alpha = \sup L$$   exists in $S$, and $\alpha = \inf B$. Proof . Since $B$ is bounded below, $L$ is not empty. Since $L$ consists of exactly those $y \in S$ which satisfy the inequality $y \leq x$ for every $x \in B$, we see that every $x \in B$ is an upper bound of $L$. Thus $L$ is bounded above. Our hypothesis about $S$ implies therefore that $L$ has a supremum in $S$ call it $\alpha$ If $\gamma < \alpha$ then $\gamma$ is not an upper bound of $L$, hence $\gamma \notin B$. It follows that $\alpha \leq x $ for every $x \in B$. Thus $\alpha \in L$ If $\alpha < \beta$ then $\beta \notin L$ since $\alpha$ is an upper bound of $L$ We have shown that $\alpha \in L$ but $\beta \notin L$ if $\beta > \alpha$. In other words, $\alpha$ is a lower bound of $B$, but $\beta $ is not if $\beta > \alpha$. This means that $\alpha = \inf B$ I am confused in the following: I don't follow why $L \subset S$ given $S$ is an ordered set with the least upper bound property and $B \subset S$, $B$ is not empty and $B$ is bounded below. If $L$ is not a subset of $S$, then the assumption of the proof will not follow, I think I have missed something. Can someone help me out? This proof is in Rudin's analysis page 5 Thanks Edit for clarification Suppose the following let $S = (0, x]$, for which $x $ is some real positive number, we know $S$ is an ordered set with the least upper bound property, let $B = (0, y]$ for which $y < x$ and $y$ is positive real number, then $L = (-\infty, 0]$, we note that $\inf B = \sup L = 0$ however $0 \notin S$, thus we proved that an order set $S$ with the least upper bound property with $B = (0, y] \subset S \Rightarrow \inf B \notin S$","In Rudin $1.11$ Theorem Proof he claims the following Theorem . Suppose $S$ is an ordered set with the least upper bound property $B \subset S$, $B$ is not empty, and $B$ is bounded below. Let $L$ be the set of all lower bounds of $B$. Then    $$\alpha = \sup L$$   exists in $S$, and $\alpha = \inf B$. Proof . Since $B$ is bounded below, $L$ is not empty. Since $L$ consists of exactly those $y \in S$ which satisfy the inequality $y \leq x$ for every $x \in B$, we see that every $x \in B$ is an upper bound of $L$. Thus $L$ is bounded above. Our hypothesis about $S$ implies therefore that $L$ has a supremum in $S$ call it $\alpha$ If $\gamma < \alpha$ then $\gamma$ is not an upper bound of $L$, hence $\gamma \notin B$. It follows that $\alpha \leq x $ for every $x \in B$. Thus $\alpha \in L$ If $\alpha < \beta$ then $\beta \notin L$ since $\alpha$ is an upper bound of $L$ We have shown that $\alpha \in L$ but $\beta \notin L$ if $\beta > \alpha$. In other words, $\alpha$ is a lower bound of $B$, but $\beta $ is not if $\beta > \alpha$. This means that $\alpha = \inf B$ I am confused in the following: I don't follow why $L \subset S$ given $S$ is an ordered set with the least upper bound property and $B \subset S$, $B$ is not empty and $B$ is bounded below. If $L$ is not a subset of $S$, then the assumption of the proof will not follow, I think I have missed something. Can someone help me out? This proof is in Rudin's analysis page 5 Thanks Edit for clarification Suppose the following let $S = (0, x]$, for which $x $ is some real positive number, we know $S$ is an ordered set with the least upper bound property, let $B = (0, y]$ for which $y < x$ and $y$ is positive real number, then $L = (-\infty, 0]$, we note that $\inf B = \sup L = 0$ however $0 \notin S$, thus we proved that an order set $S$ with the least upper bound property with $B = (0, y] \subset S \Rightarrow \inf B \notin S$",,"['real-analysis', 'analysis', 'supremum-and-infimum', 'upper-lower-bounds']"
38,A reason for $ 64\int_0^1 \left(\frac \pi 4+\arctan t\right)^2\cdot \log t\cdot\frac 1{1-t^2}\; dt =-\pi^4$ ...,A reason for  ..., 64\int_0^1 \left(\frac \pi 4+\arctan t\right)^2\cdot \log t\cdot\frac 1{1-t^2}\; dt =-\pi^4,"Question: How to show the relation $$ J:=\int_0^1 \left(\frac \pi 4+\arctan t\right)^2\cdot \log t\cdot\frac 1{1-t^2}\; dt  =-\frac 1{64}\pi^4 $$ (using a ""minimal industry"" of relations, possibly remaining inside the real analysis)? So i have found a solution to the problem, it is part of my solution for math.stackexchange.com - questions - 3854736 , but not a satisfactory solution. ""There should be more"", explaining why there is a ""clean result"" for the integral. Here, i am not strictly interested in a computational approach. I just want to share this with the community in these days of isolation. Any idea to attack this, or a related integral involving ""three log factors"" is welcome. (Well, the $\arctan$ is a sort of $\log$ in a sense that i don't want to define closer, see below.) Computations may be safely done ""modulo integrals involving two or one log factor"". But an illuminating, short way to show the above formula for $J$ would be wonderful. Motivation: The above relation appeared as i tried to solve the integral posted at the above link: Calculate $\displaystyle\int_0^{2\pi} x^2\; \cos x \cdot\operatorname{Li}_2(\cos x)\; dx$ . After several simplifications and substitutions, it turns out that the above integral is related to integrals of the shape $\int_0^1\log t\; R(t)\; dt$ ,  and $\int_0^1\arctan t\cdot \log t\; R(t)\; dt$ ,  and $\int_0^1\arctan^2 t\cdot \log t\; R(t)\; dt$ , and ""similar"" expressions. Here $R$ is in each case a (rather simple) rational function. (The more log and/or arctangent factors, the higher the computational complexity.) I could compute more or less algorithmically most of the the needed integrals to solve the linked problem, all of them but the integral $$ K=\int_0^1\arctan^2 t\cdot\log t\cdot\frac2{1-t^2}\; dt\ , $$ which turned out to be very hard to attack with the methods of real analysis. Computing this integral is more or less equivalent to computing $J$ , and the question wants $J$ instead, since we have a ""clean formula"", so that some speculation about a ""clever substitution"" may be accepted. My solution (for $K$ ) works in complex analysis, the first step is to write $$ \int_0^1 =\int_0^i+\int_i^1\ , $$ then parametrize the first integral using a linear path, the second one using a path on the unit circle. Some comments: I will say some more words, because the situation is rich in coincidences. Since a numerical evidence is the simplest and shortes way to present (instead of showing how to show), i will use this method to at least list the coincidences. Many equalities below are ""equivalent"" (modulo computation of integrals of lower complexity) to the formula for $J$ . First of all, a numerical experiment using pari/gp delivers some connection between $K$ and a ""cousin"" of $J$ : ? 2 * intnum( t=0, 1, atan(t)^2 * log(t) / (1-t^2) )   %88 = -0.357038604620289042902893412499686912781214141574556097366337   ? real(intnum( t=0, I, (pi/4 - atan(t))^2 * log(t) / (1-t^2) ))   %89 = -0.357038604620289042902893412499686912781214141574556097366337   ? intnum( t=0, 1, (pi/4 - atan(t))^2 * log(t) / (1-t^2) )   %90 = -0.357038604620289042902893412499686912781214141574556097366337 In words: $$ \begin{aligned} K &= \int_0^1\arctan^2 t\cdot\log t\;\frac{2}{1-t^2}\; dt \\ &= \Re \int_0^i\left(\frac \pi 4-\arctan t\right)^2 \cdot\log t\;\frac 1{1-t^2}\; dt \\ &= \int_0^1\left(\frac \pi 4-\arctan t\right)^2 \cdot\log t\;\frac 1{1-t^2}\; dt \ . \end{aligned} $$ Note the integration margins. What happens if we take the integral on $[0,i]$ instead of $[0,1]$ in the $K$ -integral? Numerically: ? 2 * real(intnum( t=0, i, atan(t)^2 * log(t) / (1-t^2) ))     %98 = 1.52201704740628808181938019826101736327699352613570971392919     ? pi^4/64     %99 = 1.52201704740628808181938019826101736327699352613570971392919 In words: $$ \begin{aligned} K^* &:= \Re\int_0^i\arctan^2 t\cdot\log t\;\frac{2}{1-t^2}\; dt \\ &=\frac 1{64}\pi^4 \\ &= -\int_0^1\left(\frac \pi 4+\arctan t\right)^2 \cdot\log t\;\frac 1{1-t^2}\; dt \\ &=-J\ . \end{aligned} $$ (These observations were leading to the formula for $K$ in loc. cit. .) One idea is to use partial integration in $J$ or $K$ . Well, we have for $K$ : $$ \begin{aligned} K  &= \int_0^1\arctan^2 t\cdot\log t\;\left(-\log\frac {1-t}{1+t}\right)'\; dt \\ &= \underbrace{\int_0^1\arctan^2 t\cdot\frac 1t\cdot \log\frac {1-t}{1+t}\; dt}_{=2K\text{ (why?)}} \\ &\qquad\qquad+ \underbrace{ \int_0^1 2\arctan t\cdot\frac 1{1+t^2}\cdot \log t\cdot \log\frac {1-t}{1+t}\; dt }_{=-K\text{ (why?)}} \ . \end{aligned} $$ Note that $\arctan$ is related to the logarithm (over $\Bbb C$ ), we have the relation (around $0$ ) $$ \arctan t=\frac 1{2i}\log\frac {1+it}{1-it}\ . $$ The substitution $t=\frac{1-u}{1+u}$ and the formula for $\tan(\arctan 1-\arctan u)$ are giving: $$ \begin{aligned} K  &= \int_0^1\arctan^2 t\cdot\log t\;\frac{2}{1-t^2}\; dt \\ &=\int_0^1 \left(\frac\pi2-\arctan u\right)^2\cdot\log\frac {1-u}{1+u}\cdot \frac {du}u\ . \\ &=\int_1^\infty \left(\frac\pi2-\arctan u\right)^2\cdot\log\frac {u-1}{1+u}\cdot \frac {du}u\ . \end{aligned} $$ (Write $\log t=\frac 12\log t^2$ to have the same expression under the integral on $(0,1)$ and on $(1,\infty)$ .) Note the fact that the factor $\frac 2{1-t^2}$ is not ""random"". It is the right one to make things feasible. It is the derivative of $\displaystyle -\log\frac{1-t}{1+t}$ , and plugging in $t=iu$ into $\displaystyle \log\frac{1-t}{1+t}$ leads to an expression related to $\arctan u$ . And conversely, $\arctan(iu)$ is related to such a logarithmic expression in $u$ .","Question: How to show the relation (using a ""minimal industry"" of relations, possibly remaining inside the real analysis)? So i have found a solution to the problem, it is part of my solution for math.stackexchange.com - questions - 3854736 , but not a satisfactory solution. ""There should be more"", explaining why there is a ""clean result"" for the integral. Here, i am not strictly interested in a computational approach. I just want to share this with the community in these days of isolation. Any idea to attack this, or a related integral involving ""three log factors"" is welcome. (Well, the is a sort of in a sense that i don't want to define closer, see below.) Computations may be safely done ""modulo integrals involving two or one log factor"". But an illuminating, short way to show the above formula for would be wonderful. Motivation: The above relation appeared as i tried to solve the integral posted at the above link: Calculate . After several simplifications and substitutions, it turns out that the above integral is related to integrals of the shape ,  and ,  and , and ""similar"" expressions. Here is in each case a (rather simple) rational function. (The more log and/or arctangent factors, the higher the computational complexity.) I could compute more or less algorithmically most of the the needed integrals to solve the linked problem, all of them but the integral which turned out to be very hard to attack with the methods of real analysis. Computing this integral is more or less equivalent to computing , and the question wants instead, since we have a ""clean formula"", so that some speculation about a ""clever substitution"" may be accepted. My solution (for ) works in complex analysis, the first step is to write then parametrize the first integral using a linear path, the second one using a path on the unit circle. Some comments: I will say some more words, because the situation is rich in coincidences. Since a numerical evidence is the simplest and shortes way to present (instead of showing how to show), i will use this method to at least list the coincidences. Many equalities below are ""equivalent"" (modulo computation of integrals of lower complexity) to the formula for . First of all, a numerical experiment using pari/gp delivers some connection between and a ""cousin"" of : ? 2 * intnum( t=0, 1, atan(t)^2 * log(t) / (1-t^2) )   %88 = -0.357038604620289042902893412499686912781214141574556097366337   ? real(intnum( t=0, I, (pi/4 - atan(t))^2 * log(t) / (1-t^2) ))   %89 = -0.357038604620289042902893412499686912781214141574556097366337   ? intnum( t=0, 1, (pi/4 - atan(t))^2 * log(t) / (1-t^2) )   %90 = -0.357038604620289042902893412499686912781214141574556097366337 In words: Note the integration margins. What happens if we take the integral on instead of in the -integral? Numerically: ? 2 * real(intnum( t=0, i, atan(t)^2 * log(t) / (1-t^2) ))     %98 = 1.52201704740628808181938019826101736327699352613570971392919     ? pi^4/64     %99 = 1.52201704740628808181938019826101736327699352613570971392919 In words: (These observations were leading to the formula for in loc. cit. .) One idea is to use partial integration in or . Well, we have for : Note that is related to the logarithm (over ), we have the relation (around ) The substitution and the formula for are giving: (Write to have the same expression under the integral on and on .) Note the fact that the factor is not ""random"". It is the right one to make things feasible. It is the derivative of , and plugging in into leads to an expression related to . And conversely, is related to such a logarithmic expression in .","
J:=\int_0^1 \left(\frac \pi 4+\arctan t\right)^2\cdot \log t\cdot\frac 1{1-t^2}\; dt 
=-\frac 1{64}\pi^4
 \arctan \log J \displaystyle\int_0^{2\pi} x^2\; \cos x \cdot\operatorname{Li}_2(\cos x)\; dx \int_0^1\log t\; R(t)\; dt \int_0^1\arctan t\cdot \log t\; R(t)\; dt \int_0^1\arctan^2 t\cdot \log t\; R(t)\; dt R 
K=\int_0^1\arctan^2 t\cdot\log t\cdot\frac2{1-t^2}\; dt\ ,
 J J K 
\int_0^1 =\int_0^i+\int_i^1\ ,
 J K J 
\begin{aligned}
K
&=
\int_0^1\arctan^2 t\cdot\log t\;\frac{2}{1-t^2}\; dt
\\
&=
\Re
\int_0^i\left(\frac \pi 4-\arctan t\right)^2 \cdot\log t\;\frac 1{1-t^2}\; dt
\\
&=
\int_0^1\left(\frac \pi 4-\arctan t\right)^2 \cdot\log t\;\frac 1{1-t^2}\; dt
\ .
\end{aligned}
 [0,i] [0,1] K 
\begin{aligned}
K^*
&:=
\Re\int_0^i\arctan^2 t\cdot\log t\;\frac{2}{1-t^2}\; dt
\\
&=\frac 1{64}\pi^4
\\
&=
-\int_0^1\left(\frac \pi 4+\arctan t\right)^2 \cdot\log t\;\frac 1{1-t^2}\; dt
\\
&=-J\ .
\end{aligned}
 K J K K 
\begin{aligned}
K 
&=
\int_0^1\arctan^2 t\cdot\log t\;\left(-\log\frac {1-t}{1+t}\right)'\; dt
\\
&=
\underbrace{\int_0^1\arctan^2 t\cdot\frac 1t\cdot \log\frac {1-t}{1+t}\; dt}_{=2K\text{ (why?)}}
\\
&\qquad\qquad+
\underbrace{
\int_0^1
2\arctan t\cdot\frac 1{1+t^2}\cdot \log t\cdot \log\frac {1-t}{1+t}\; dt
}_{=-K\text{ (why?)}}
\ .
\end{aligned}
 \arctan \Bbb C 0 
\arctan t=\frac 1{2i}\log\frac {1+it}{1-it}\ .
 t=\frac{1-u}{1+u} \tan(\arctan 1-\arctan u) 
\begin{aligned}
K 
&=
\int_0^1\arctan^2 t\cdot\log t\;\frac{2}{1-t^2}\; dt
\\
&=\int_0^1
\left(\frac\pi2-\arctan u\right)^2\cdot\log\frac {1-u}{1+u}\cdot \frac {du}u\ .
\\
&=\int_1^\infty
\left(\frac\pi2-\arctan u\right)^2\cdot\log\frac {u-1}{1+u}\cdot \frac {du}u\ .
\end{aligned}
 \log t=\frac 12\log t^2 (0,1) (1,\infty) \frac 2{1-t^2} \displaystyle -\log\frac{1-t}{1+t} t=iu \displaystyle \log\frac{1-t}{1+t} \arctan u \arctan(iu) u","['real-analysis', 'integration', 'complex-analysis', 'definite-integrals', 'polylogarithm']"
39,Is there a way to evaluate analytically the following infinite double sum?,Is there a way to evaluate analytically the following infinite double sum?,,"Consider the following double sum $$ S = \sum_{n=1}^\infty \sum_{m=1}^\infty \frac{1}{a (2n-1)^2 - b (2m-1)^2} \, , $$ where $a$ and $b$ are both positive real numbers given by \begin{align} 	a &= \frac{1}{2} - \frac{\sqrt{2}}{32}  \, , \\ 	b     &= \frac{1}{4} - \frac{3\sqrt{2}}{32}  \, . \end{align} It turns out that one of the two sums can readily be calculated and expressed in terms of the tangente function. Specifically, $$ S = \frac{\pi}{4\sqrt{ab}} \sum_{m=1}^\infty  \frac{\tan \left( \frac{\pi}{2} \sqrt{\frac{b}{a}} (2m-1) \right)}{2m-1} \, . $$ The latter result does not seem to be further simplified. i was wondering whether someone here could be of help and let me know in case there exists a method to evaluate the sum above. Hints and suggestions welcome. Thank you PS From numerical evaluation using computer algebra systems, it seems that the series is convergent. This apparently would not be the case if $b<0$ .","Consider the following double sum where and are both positive real numbers given by It turns out that one of the two sums can readily be calculated and expressed in terms of the tangente function. Specifically, The latter result does not seem to be further simplified. i was wondering whether someone here could be of help and let me know in case there exists a method to evaluate the sum above. Hints and suggestions welcome. Thank you PS From numerical evaluation using computer algebra systems, it seems that the series is convergent. This apparently would not be the case if .","
S = \sum_{n=1}^\infty \sum_{m=1}^\infty
\frac{1}{a (2n-1)^2 - b (2m-1)^2} \, ,
 a b \begin{align}
	a &= \frac{1}{2} - \frac{\sqrt{2}}{32}  \, , \\
	b     &= \frac{1}{4} - \frac{3\sqrt{2}}{32}  \, .
\end{align} 
S = \frac{\pi}{4\sqrt{ab}} \sum_{m=1}^\infty 
\frac{\tan \left( \frac{\pi}{2} \sqrt{\frac{b}{a}} (2m-1) \right)}{2m-1} \, .
 b<0","['real-analysis', 'calculus', 'sequences-and-series', 'trigonometry', 'summation']"
40,history of the contraction mapping technique,history of the contraction mapping technique,,"If $|f(x)-f(y)| \leq k|x-y|$ for all $x,y$ then $f$ is Lipschitz with constant $k$, if $k<1$ then $f$ is called a contraction mapping . The beautiful result that a fixed point is associated to a contraction mapping on a metric space is attributed to Banach see here . The proofs of many interesting theorems of basic analysis or advanced calculus are in part driven by the contraction mapping technique.The usual story: identify an equation you want to solve, replace the real solution with an affine approximation based on derivative data, propose a mapping whose fixed point provides a solution to the equation, this map is inspired by the approximation show the mapping is a contraction mapping and use the theorem associated to the context which provides the existence of the fixed point as a limit of a sequence of approximate functions. I hope this gives you a ball-park idea of what I intend by saying contraction mapping technique . In Edwards Advanced Calculus text he uses this scheme to prove the multivariate mean value theorem and the inverse mapping theorem. I know a similar arguments can be used to prove existence theorems for ODEs. Question: what is the history of the scheme above. Is it due to Banach? Is it due to Lipschitz? Is there a clear history of who first proposed the 4-step process I sketch above? I hope the spirit of the question is clear if the words are not. In any event, I thank the MSE community in advance for its useful insights on this matter!","If $|f(x)-f(y)| \leq k|x-y|$ for all $x,y$ then $f$ is Lipschitz with constant $k$, if $k<1$ then $f$ is called a contraction mapping . The beautiful result that a fixed point is associated to a contraction mapping on a metric space is attributed to Banach see here . The proofs of many interesting theorems of basic analysis or advanced calculus are in part driven by the contraction mapping technique.The usual story: identify an equation you want to solve, replace the real solution with an affine approximation based on derivative data, propose a mapping whose fixed point provides a solution to the equation, this map is inspired by the approximation show the mapping is a contraction mapping and use the theorem associated to the context which provides the existence of the fixed point as a limit of a sequence of approximate functions. I hope this gives you a ball-park idea of what I intend by saying contraction mapping technique . In Edwards Advanced Calculus text he uses this scheme to prove the multivariate mean value theorem and the inverse mapping theorem. I know a similar arguments can be used to prove existence theorems for ODEs. Question: what is the history of the scheme above. Is it due to Banach? Is it due to Lipschitz? Is there a clear history of who first proposed the 4-step process I sketch above? I hope the spirit of the question is clear if the words are not. In any event, I thank the MSE community in advance for its useful insights on this matter!",,"['real-analysis', 'functional-analysis', 'metric-spaces', 'math-history']"
41,If $\left| f'(x) \right| \leq A |f(x)|^\beta $ then f is a constant function,If  then f is a constant function,\left| f'(x) \right| \leq A |f(x)|^\beta ,"Problem Let $f(x)$ be a differentiable function on $[a,b]$ satisfying $f(a)=0$. If there exist $A \ge 0$ and $\beta \ge 1$ such that the inequality $$\left| f'(x) \right| \leq A \left| f(x) \right|^\beta $$ holds for all $x$ in $[a,b]$, then $f(x) = 0$ for any x in that interval. My attempted solution We shall prove by contradiction. Suppose that for some $x$ in $[a,b]$ we have $f(x) \neq 0$. Put $S:= \left\lbrace x \right. $ such that $a \le x \le b $ and $f(x) \neq 0 \left. \right\rbrace$ and $c:=\inf S$ (the infimum value of set $S$). If $c=b$ then $ f(x) = 0$   $\forall  x < b$ and the continuity of $f$ implies that $f(b) = 0$, too (contradiction). Therefore $c \neq b$. If $c=a$, then $f(c) = 0$ by the hypothesis. Otherwise, when $a < c < b$ we have $f(x) = 0$ $\forall x < c$ and again, by continuity, $f(c) = 0$. Thus in all cases considered, we have $f(c) = 0$ and $a \leq c < b$ As $f$ is continuous at $c$ and $f(c) = 0$, one can choose $d>c$ and close enough to $c$ such that $|f(x)| \le 1$ and $$\tag{$\star$} A(x-c) \le \frac12\quad\text{for all }x\text{ with }c \le x \le d.$$ Since $f$ is continuous on $[c,d]$, it has maximum and minimum values on this closed interval, leading to the existence of $c \le t \le d$ such that $|f(t)| = \max_{c \le x \le d} |f(x)|$. As already noted, $f(c) = 0$ and $c$ is the infimum of the set of numbers whose image under $f$ is nonzero. Therefore, for any $\epsilon>0$, there is some $c<m<c+\epsilon$ satisfying $f(m) \neq 0$. This observation, together with the definition of $t$, gives us $$\tag{$\star\!\star$}f(t) \neq 0,$$ $t \neq c$ and so $c<t$. Applying the Lagrange theorem gives: $$ |f(t)| = |f(t) - f(c)| = |t-c||f'(u)| \leq|t-c|A|f(u)|^\beta $$ where $c<u<t\le d$. By $(\star)$: $$ |t-c|A|f(u)|^\beta \le \frac12 |f(u)|^\beta \leq \frac12 |f(u)| $$ Combining the two inequalities and noting that $|f(t)| \ge |f(u)|$ (because of the definition of $t$), we have $f(u) = f(t) = 0$ and arrived at a contradiction with $(\star\star)$. Question I hope someone can verify if my solution is correct. Of course, other ideas, comments or solutions are welcome. I like to post problems and my solutions to the forum because I think it's beneficial to the community, and for learners like me. First, I can hardly know if there's flaw in my own argument. Second, I may get new insights/solutions for my problem. Thank you.","Problem Let $f(x)$ be a differentiable function on $[a,b]$ satisfying $f(a)=0$. If there exist $A \ge 0$ and $\beta \ge 1$ such that the inequality $$\left| f'(x) \right| \leq A \left| f(x) \right|^\beta $$ holds for all $x$ in $[a,b]$, then $f(x) = 0$ for any x in that interval. My attempted solution We shall prove by contradiction. Suppose that for some $x$ in $[a,b]$ we have $f(x) \neq 0$. Put $S:= \left\lbrace x \right. $ such that $a \le x \le b $ and $f(x) \neq 0 \left. \right\rbrace$ and $c:=\inf S$ (the infimum value of set $S$). If $c=b$ then $ f(x) = 0$   $\forall  x < b$ and the continuity of $f$ implies that $f(b) = 0$, too (contradiction). Therefore $c \neq b$. If $c=a$, then $f(c) = 0$ by the hypothesis. Otherwise, when $a < c < b$ we have $f(x) = 0$ $\forall x < c$ and again, by continuity, $f(c) = 0$. Thus in all cases considered, we have $f(c) = 0$ and $a \leq c < b$ As $f$ is continuous at $c$ and $f(c) = 0$, one can choose $d>c$ and close enough to $c$ such that $|f(x)| \le 1$ and $$\tag{$\star$} A(x-c) \le \frac12\quad\text{for all }x\text{ with }c \le x \le d.$$ Since $f$ is continuous on $[c,d]$, it has maximum and minimum values on this closed interval, leading to the existence of $c \le t \le d$ such that $|f(t)| = \max_{c \le x \le d} |f(x)|$. As already noted, $f(c) = 0$ and $c$ is the infimum of the set of numbers whose image under $f$ is nonzero. Therefore, for any $\epsilon>0$, there is some $c<m<c+\epsilon$ satisfying $f(m) \neq 0$. This observation, together with the definition of $t$, gives us $$\tag{$\star\!\star$}f(t) \neq 0,$$ $t \neq c$ and so $c<t$. Applying the Lagrange theorem gives: $$ |f(t)| = |f(t) - f(c)| = |t-c||f'(u)| \leq|t-c|A|f(u)|^\beta $$ where $c<u<t\le d$. By $(\star)$: $$ |t-c|A|f(u)|^\beta \le \frac12 |f(u)|^\beta \leq \frac12 |f(u)| $$ Combining the two inequalities and noting that $|f(t)| \ge |f(u)|$ (because of the definition of $t$), we have $f(u) = f(t) = 0$ and arrived at a contradiction with $(\star\star)$. Question I hope someone can verify if my solution is correct. Of course, other ideas, comments or solutions are welcome. I like to post problems and my solutions to the forum because I think it's beneficial to the community, and for learners like me. First, I can hardly know if there's flaw in my own argument. Second, I may get new insights/solutions for my problem. Thank you.",,"['calculus', 'real-analysis', 'analysis', 'solution-verification']"
42,"Radon–Nikodym derivative and ""normal"" derivative","Radon–Nikodym derivative and ""normal"" derivative",,"The Radon–Nikodym theorem states that, given a measurable space $(X,\Sigma)$, if a $\sigma$-finite measure $\nu$ on $(X,\Sigma)$ is absolutely continuous with respect to a $\sigma$-finite measure $\mu$ on $(X,\Sigma)$, then there is a measurable function $f$ on $X$ and taking values in $[0,\infty)$, such that $$\nu(A) = \int_A f \, d\mu$$ for any measurable set $A$. $f$ is called the Radon–Nikodym derivative of $\nu$ wrt $\mu$. I was wondering in what cases the concept of Radon–Nikodym derivative and the concept of derivative in real analysis can coincide and how? Thanks and regards!","The Radon–Nikodym theorem states that, given a measurable space $(X,\Sigma)$, if a $\sigma$-finite measure $\nu$ on $(X,\Sigma)$ is absolutely continuous with respect to a $\sigma$-finite measure $\mu$ on $(X,\Sigma)$, then there is a measurable function $f$ on $X$ and taking values in $[0,\infty)$, such that $$\nu(A) = \int_A f \, d\mu$$ for any measurable set $A$. $f$ is called the Radon–Nikodym derivative of $\nu$ wrt $\mu$. I was wondering in what cases the concept of Radon–Nikodym derivative and the concept of derivative in real analysis can coincide and how? Thanks and regards!",,"['real-analysis', 'measure-theory', 'radon-nikodym']"
43,Astonishing: the sum of two infinite products of nested radicals equal to $ \pi $.,Astonishing: the sum of two infinite products of nested radicals equal to ., \pi ,"Recently, I found the following handwritten expression in an old math book of my family. Probably it belonged to my great grandfather Boris, who had a P.D. in mathematics. $ \pi = \frac{4\sqrt{5}}{5}.\frac{2}{\sqrt{2 + \frac{4}{\sqrt{5}}}}.\frac{2}{\sqrt{2 + \sqrt{2 +\frac{4}{\sqrt{5}}}}}.\frac{2}{\sqrt{2 + \sqrt{2 + \sqrt{2 +\frac{4}{\sqrt{5}}}}}}... +\frac{2\sqrt{10}}{5}.\frac{2}{\sqrt{2 + \frac{6}{\sqrt{10}}}}.\frac{2}{\sqrt{2 + \sqrt{2 +\frac{6}{\sqrt{10}}}}}.\frac{2}{\sqrt{2 + \sqrt{2 + \sqrt{2 +\frac{6}{\sqrt{10}}}}}}... $ I found this identity extremely interesting, and, in fact, I had never seen it. It's similar, but different, from Vieta's formula for $ \pi $ How to prove this identity?","Recently, I found the following handwritten expression in an old math book of my family. Probably it belonged to my great grandfather Boris, who had a P.D. in mathematics. $ \pi = \frac{4\sqrt{5}}{5}.\frac{2}{\sqrt{2 + \frac{4}{\sqrt{5}}}}.\frac{2}{\sqrt{2 + \sqrt{2 +\frac{4}{\sqrt{5}}}}}.\frac{2}{\sqrt{2 + \sqrt{2 + \sqrt{2 +\frac{4}{\sqrt{5}}}}}}... +\frac{2\sqrt{10}}{5}.\frac{2}{\sqrt{2 + \frac{6}{\sqrt{10}}}}.\frac{2}{\sqrt{2 + \sqrt{2 +\frac{6}{\sqrt{10}}}}}.\frac{2}{\sqrt{2 + \sqrt{2 + \sqrt{2 +\frac{6}{\sqrt{10}}}}}}... $ I found this identity extremely interesting, and, in fact, I had never seen it. It's similar, but different, from Vieta's formula for $ \pi $ How to prove this identity?",,"['calculus', 'real-analysis']"
44,$f : \mathbb{R} \to \mathbb{R}$ (Lipschitz) continuous implies $f(A)$ is Borel for all Borel $A$.,(Lipschitz) continuous implies  is Borel for all Borel .,f : \mathbb{R} \to \mathbb{R} f(A) A,"Full question: Let $(\mathbb{R}, \mathfrak{M}, m)$ denote the measure space $\mathbb{R}$ equipped with the Borel $\sigma$-algebra and the Lebesgue measure. Suppose $f : \mathbb{R} \to \mathbb{R}$ is Lipschitz continuous with Lipschitz constant $L$. Show $f(A) \in \mathfrak{M}$ and $m(f(A)) \le L m(A)$ for all $A \in \mathfrak{M}$. This is a question from an old preliminary exam. I feel quite comfortable, after assuming measurability of $f(A)$, in showing that $m(f(A)) \le L m(A)$ for all $A \in \mathfrak{M}$. I have found similar questions on this site, but none that do not assume $f$ is either one-to-one or onto, as well as the closest one to my question having an accepted answer that says the question is wrong. My progress (aka simple cases): If $A \subset \mathbb{R}$ is connected, we know that $A$ is either a point, or an interval (possibly infinite, and can be open, closed, or half-open). However, since $f$ is continuous, the set $f(A)$ must also be connected, so $f(A)$ is also a point, or an interval. In particular, if $A$ is connected, then $f(A)$ is either an $F_{\sigma}$ or a $G_{\delta}$ set (countable union of closed sets and countable intersection of open sets respectively). If $A \subset \mathbb{R}$ is open, due to the separability of $\mathbb{R}$, it must be a countable union of open intervals, so $A = \cup_{i=1}^{\infty} I_{i}$ for intervals $I_{i}$ and consequently $f(A) = \cup_{i=1}^{\infty} f(I_{i})$ is an $F_{\sigma}$ or $G_{\sigma \delta}$ set (the latter being countable unions of $G_{\delta}$ sets). If $A \subset \mathbb{R}$ is compact, then $f(A)$ is compact, and hence closed. If $A \subset \mathbb{R}$ is closed but not compact, we can write $A = \cup_{n = 1}^{\infty} A \cap [-n,n]$, and consequently, $f(A) = \cup_{n=1}^{\infty} f(A \cap [-n,n])$ is the countable union of compact sets, and hence a $G_{\sigma}$ set. My issue: I know that the Borel hierarchy continues on for an uncountable number of steps. So, I cannot use induction to try to finish the proof based off of these simple cases. So far unsuccessful, but yet a promising idea: I'm tempted to appeal to the monotone class lemma, (see Folland's Real Analysis page 66, Lemma 2.35) by showing that the sets from my simple case combined with some sets that involve $\mathbb{R} \setminus f(A)$ form an algebra and a monotone class, and consequently their $\sigma$-algebra is the same $\sigma$-algebra that is generated by the sets from the simple cases (which I believe is the Borel $\sigma$-algebra). The big guns: There's a remark (2.2.13) on the bottom of page 69 and top of page 70 in Federer's ""Geometric Measure Theory"" that says: If $f : X \to Y$ is continuous, $X$ is a complete, separable metric space, $Y$ is a Hausdorff space, $\mu$ measures $Y$, and every closed subset of $Y$ is $\mu$-measurable, then the $f$ image of every Borel subset of $X$ is $\mu$-measurable. This remark seems to imply the problem is true. However, this remark follows from two sections of Federer's GMT that are far beyond the expected knowledge for the prelim I'm studying for, and the sections are too terse for me to yet understand. Moreover, Federer goes through the process of defining Suslin sets, and considering the image of Borel sets as projections of Suslin sets. Again, none of this seems like ""prelim-level"" material. However, since Lebesgue originally thought he'd proved something along these lines (the error in his proof was found by Suslin) I'd be surprised if there's a more elementary proof. Maybe some of the restrictions in this specific problem make it possible. Thanks for reading. Cheers.","Full question: Let $(\mathbb{R}, \mathfrak{M}, m)$ denote the measure space $\mathbb{R}$ equipped with the Borel $\sigma$-algebra and the Lebesgue measure. Suppose $f : \mathbb{R} \to \mathbb{R}$ is Lipschitz continuous with Lipschitz constant $L$. Show $f(A) \in \mathfrak{M}$ and $m(f(A)) \le L m(A)$ for all $A \in \mathfrak{M}$. This is a question from an old preliminary exam. I feel quite comfortable, after assuming measurability of $f(A)$, in showing that $m(f(A)) \le L m(A)$ for all $A \in \mathfrak{M}$. I have found similar questions on this site, but none that do not assume $f$ is either one-to-one or onto, as well as the closest one to my question having an accepted answer that says the question is wrong. My progress (aka simple cases): If $A \subset \mathbb{R}$ is connected, we know that $A$ is either a point, or an interval (possibly infinite, and can be open, closed, or half-open). However, since $f$ is continuous, the set $f(A)$ must also be connected, so $f(A)$ is also a point, or an interval. In particular, if $A$ is connected, then $f(A)$ is either an $F_{\sigma}$ or a $G_{\delta}$ set (countable union of closed sets and countable intersection of open sets respectively). If $A \subset \mathbb{R}$ is open, due to the separability of $\mathbb{R}$, it must be a countable union of open intervals, so $A = \cup_{i=1}^{\infty} I_{i}$ for intervals $I_{i}$ and consequently $f(A) = \cup_{i=1}^{\infty} f(I_{i})$ is an $F_{\sigma}$ or $G_{\sigma \delta}$ set (the latter being countable unions of $G_{\delta}$ sets). If $A \subset \mathbb{R}$ is compact, then $f(A)$ is compact, and hence closed. If $A \subset \mathbb{R}$ is closed but not compact, we can write $A = \cup_{n = 1}^{\infty} A \cap [-n,n]$, and consequently, $f(A) = \cup_{n=1}^{\infty} f(A \cap [-n,n])$ is the countable union of compact sets, and hence a $G_{\sigma}$ set. My issue: I know that the Borel hierarchy continues on for an uncountable number of steps. So, I cannot use induction to try to finish the proof based off of these simple cases. So far unsuccessful, but yet a promising idea: I'm tempted to appeal to the monotone class lemma, (see Folland's Real Analysis page 66, Lemma 2.35) by showing that the sets from my simple case combined with some sets that involve $\mathbb{R} \setminus f(A)$ form an algebra and a monotone class, and consequently their $\sigma$-algebra is the same $\sigma$-algebra that is generated by the sets from the simple cases (which I believe is the Borel $\sigma$-algebra). The big guns: There's a remark (2.2.13) on the bottom of page 69 and top of page 70 in Federer's ""Geometric Measure Theory"" that says: If $f : X \to Y$ is continuous, $X$ is a complete, separable metric space, $Y$ is a Hausdorff space, $\mu$ measures $Y$, and every closed subset of $Y$ is $\mu$-measurable, then the $f$ image of every Borel subset of $X$ is $\mu$-measurable. This remark seems to imply the problem is true. However, this remark follows from two sections of Federer's GMT that are far beyond the expected knowledge for the prelim I'm studying for, and the sections are too terse for me to yet understand. Moreover, Federer goes through the process of defining Suslin sets, and considering the image of Borel sets as projections of Suslin sets. Again, none of this seems like ""prelim-level"" material. However, since Lebesgue originally thought he'd proved something along these lines (the error in his proof was found by Suslin) I'd be surprised if there's a more elementary proof. Maybe some of the restrictions in this specific problem make it possible. Thanks for reading. Cheers.",,"['real-analysis', 'measure-theory', 'geometric-measure-theory']"
45,"Prob. 17, Chap. 2, in Baby Rudin: The set of all numbers in $[0,1]$ with only $4$ and $7$ as decimal digits is countable, dense, compact, perfect?","Prob. 17, Chap. 2, in Baby Rudin: The set of all numbers in  with only  and  as decimal digits is countable, dense, compact, perfect?","[0,1] 4 7","Here is Prob. 17 in the Exercises after Chapter 2 in the book Principles of Mathematical Analysis by Walter Rudin, 3rd edition. Let $E$ be the set of all $x \in [0,1]$ whose decimal expansion contains only the digits $4$ and $7$. Is $E$ countable? Is $E$ dense in $[0,1]$? Is $E$ compact? Is $E$ perfect? My effort: The set $E$ is not countable. The proof is essentially the same as that for showing that the set of all the binary sequences is uncountable. Am I right? The set $E$ is not dense in $[0, 1]$. The smallest element in $E$ is  $$x_\min \colon= \frac{4}{10} + \frac{4}{10^2} + \frac{4}{10^3} + \ldots = \frac{4}{9},$$ and the largest element in $E$ is  $$x_\max \colon= \frac{7}{10} + \frac{7}{10^2} + \frac{7}{10^3} + \ldots = \frac{7}{9}.$$ Thus, the set $E$ is (strictly) contained in the closed interval $[\frac{4}{9}, \frac{7}{9}]$. So, the element $\frac{1}{5}$ of $[0,1]$, for example, does not lie in the closure of $E$. Am I right? The set $E$ is clearly bounded. So, for compactness, it suffices to show that $E$ is closed. [It does not matter if $E$ is closed in $[0,1]$ or $\mathbb{R}$, as the former is a closed set in the latter.] So we show that the complement of $E$ in $[0,1]$ is open in $[0,1]$. Let $x \colon= \sum_{n=1}^\infty \frac{d_n}{10^n}$ be an arbitrary element of $[0,1]-E$, where each $d_n \in \{ 0, 1, 2, \ldots, 9 \}$. Then there is a positive integer $n$ such that $d_n \not\in \{4, 7\}$. Let $N$ be the least such positive integer, and let $\delta$ be a real number such that  $$0< \delta < \frac{\min\left( \vert d_N - 4 \vert, \vert d_N - 7 \vert \right)}{10^{N+2}}.  $$ Let $y \colon= \sum_{n=1}^\infty \frac{e_n}{10^n}$ be an element of $E$, where each $e_n$ is either $4$ or $7$. Let's also assume that $e_n = d_n$ for all $n \in \{1, \ldots, N-1\}$. What next? How to show that $y$ fails to be within $\delta$ of $x$? For showing that $E$ is perfect, we need to show that $E$ is closed and that each element of $E$ is a limit point of $E$. Let $x \colon= \sum_{n=1}^\infty \frac{d_n}{10^n}$ be an arbitrary element of $E$, where each $d_n$ is either $4$ or $7$. Let $\delta > 0$. Then there exists a smallest positive integer $N$ such that $$\frac{3}{10^N} < \delta.$$ Let $y \colon= \sum_{n=1}^\infty \frac{d_n^\prime}{10^n}$, where each $d_n^\prime$ is either $4$ or $7$, be the element of $E$ such that $$d_n^\prime =  \begin{cases}  d_n \ \mbox{ if } \ n \in \mathbb{N} \ \mbox{ and } n \neq  N; \\ 4 \ \mbox{ if } \ n = N \ \mbox{ and } d_N = 7; \\ 7 \ \mbox{ if } \ n = N \ \mbox{ and } d_N = 4.  \end{cases} $$ Then  $$0< \vert x -y \vert < \delta.$$ This shows that each element $x$ of $E$ is also a limit point of $E$. Am I right?","Here is Prob. 17 in the Exercises after Chapter 2 in the book Principles of Mathematical Analysis by Walter Rudin, 3rd edition. Let $E$ be the set of all $x \in [0,1]$ whose decimal expansion contains only the digits $4$ and $7$. Is $E$ countable? Is $E$ dense in $[0,1]$? Is $E$ compact? Is $E$ perfect? My effort: The set $E$ is not countable. The proof is essentially the same as that for showing that the set of all the binary sequences is uncountable. Am I right? The set $E$ is not dense in $[0, 1]$. The smallest element in $E$ is  $$x_\min \colon= \frac{4}{10} + \frac{4}{10^2} + \frac{4}{10^3} + \ldots = \frac{4}{9},$$ and the largest element in $E$ is  $$x_\max \colon= \frac{7}{10} + \frac{7}{10^2} + \frac{7}{10^3} + \ldots = \frac{7}{9}.$$ Thus, the set $E$ is (strictly) contained in the closed interval $[\frac{4}{9}, \frac{7}{9}]$. So, the element $\frac{1}{5}$ of $[0,1]$, for example, does not lie in the closure of $E$. Am I right? The set $E$ is clearly bounded. So, for compactness, it suffices to show that $E$ is closed. [It does not matter if $E$ is closed in $[0,1]$ or $\mathbb{R}$, as the former is a closed set in the latter.] So we show that the complement of $E$ in $[0,1]$ is open in $[0,1]$. Let $x \colon= \sum_{n=1}^\infty \frac{d_n}{10^n}$ be an arbitrary element of $[0,1]-E$, where each $d_n \in \{ 0, 1, 2, \ldots, 9 \}$. Then there is a positive integer $n$ such that $d_n \not\in \{4, 7\}$. Let $N$ be the least such positive integer, and let $\delta$ be a real number such that  $$0< \delta < \frac{\min\left( \vert d_N - 4 \vert, \vert d_N - 7 \vert \right)}{10^{N+2}}.  $$ Let $y \colon= \sum_{n=1}^\infty \frac{e_n}{10^n}$ be an element of $E$, where each $e_n$ is either $4$ or $7$. Let's also assume that $e_n = d_n$ for all $n \in \{1, \ldots, N-1\}$. What next? How to show that $y$ fails to be within $\delta$ of $x$? For showing that $E$ is perfect, we need to show that $E$ is closed and that each element of $E$ is a limit point of $E$. Let $x \colon= \sum_{n=1}^\infty \frac{d_n}{10^n}$ be an arbitrary element of $E$, where each $d_n$ is either $4$ or $7$. Let $\delta > 0$. Then there exists a smallest positive integer $N$ such that $$\frac{3}{10^N} < \delta.$$ Let $y \colon= \sum_{n=1}^\infty \frac{d_n^\prime}{10^n}$, where each $d_n^\prime$ is either $4$ or $7$, be the element of $E$ such that $$d_n^\prime =  \begin{cases}  d_n \ \mbox{ if } \ n \in \mathbb{N} \ \mbox{ and } n \neq  N; \\ 4 \ \mbox{ if } \ n = N \ \mbox{ and } d_N = 7; \\ 7 \ \mbox{ if } \ n = N \ \mbox{ and } d_N = 4.  \end{cases} $$ Then  $$0< \vert x -y \vert < \delta.$$ This shows that each element $x$ of $E$ is also a limit point of $E$. Am I right?",,"['real-analysis', 'general-topology', 'analysis', 'compactness']"
46,Cauchy-Schwarz inequality for bilinear forms valued in an abstract vector space,Cauchy-Schwarz inequality for bilinear forms valued in an abstract vector space,,"This question is perhaps a little vague; part of what I want to know is what question I should ask. First, recall the following form of the Cauchy-Schwarz inequality: let $V$ be a real vector space, and suppose $(\cdot, \cdot) : V \times V \to \mathbb{R}$ is a symmetric bilinear form which is positive semidefinite, that is, $(x,x) \ge 0$ for all $x$.  Then for any $x,y \in V$ we have $|(x,y)|^2 \le (x,x) (y,y)$. I'd like to know what happens if we replace $\mathbb{R}$ by some other space $W$.  Suppose at first that $W$ is a real vector space, equipped with a partial order $\le$ that makes it an an ordered vector space , as well as a multiplication operation $\cdot$ that makes it an algebra.  Then it makes sense to speak of a positive semidefinite symmetric bilinear form $(\cdot, \cdot) : V \times V \to W$, and ask whether it satisfies the Cauchy-Schwarz inequality $(v,w)\cdot(v,w) \le (v,v) \cdot (v,w)$. Under what conditions on $W$ does this ""generalized Cauchy-Schwarz inequality"" hold? At a minimum I expect we will need some more structure on $W$; in particular I assume we would like the multiplication and the partial ordering in $W$ to interact in some reasonable way, so that for instance $w\cdot w \ge 0$ for all $w \in W$.  Are there other properties that $W$ should have? There are lots of proofs of the classical Cauchy-Schwarz inequality ; presumably one should try to find one of them which generalizes.  But I couldn't immediately see how to do this. Here are some motivating examples. As a fairly simple one, let $X$ be any set, and $W = \mathbb{R}^X$ the vector space of all real-valued functions on $X$.  We can equip $W$ with the pointwise multiplication and ordering.  Then let $V$ be any linear subspace of $W$, and let the bilinear form $V \times V \to W$ also be pointwise multiplication.  Then of course Cauchy-Schwarz holds since we can just prove it pointwise. For a slightly less trivial example, let $(X,\mu)$ be a measure space, and $W = L^0(X,\mu)$ be the vector space of all measurable functions on $X$, mod $\mu$-almost-everywhere equality (so an element of $W$ is in fact an equivalence class of functions).  Again let $\cdot$ be pointwise multiplication (which is well defined), and the ordering $f \le g$ when $f(x) \le g(x)$ almost everywhere.  Take again a linear subspace $V \subset W$, and pointwise multiplication as the bilinear form.  Now Cauchy-Schwarz holds because we can prove it pointwise on a set of full measure. A related but more complicated example from probability (and my original motivation) is the quadratic variation form from probability.  For instance, we could take $V$ to be the vector space of continuous $L^2$ martingales on some filtered probability space over some time interval $[0,T]$, and $W$ the vector space of continuous adapted processes of bounded variation, mod indistinguishability , with pointwise multiplication and the partial order $X \le Y$ iff $X_t \le Y_t$ for all $t$ almost surely.  Then the quadratic variation $\langle M,N \rangle$ is a symmetric positive semidefinite bilinear form from $V \times V$ to $W$. In this case I can prove the Cauchy-Schwarz inequality pointwise: fix $M,N \in V$.  For almost every $\omega$, for all $t \in [0,T]$ and all $q \in \mathbb{Q}$ I can say $$q^2 \langle M,M \rangle_t(\omega) \pm 2 \langle M,N \rangle_t(\omega) + \frac{1}{q^2} \langle N,N \rangle_t(\omega) = \langle q M \pm \frac{1}{q} N \rangle_t(\omega) \ge 0$$ and then letting $q$ be a rational very close to $\sqrt{\langle N,N \rangle_t(\omega) / \langle M,M \rangle_t(\omega)}$ shows that $$|\langle M,N \rangle_t(\omega)| \le \sqrt{\langle M,M \rangle_t(\omega) \langle N,N \rangle_t(\omega)}$$ which is what we want. In each of these examples, we are working on function spaces (or quotients thereof), and the proof essentially operates pointwise.  I'm hoping for some kind of more abstract global argument.","This question is perhaps a little vague; part of what I want to know is what question I should ask. First, recall the following form of the Cauchy-Schwarz inequality: let $V$ be a real vector space, and suppose $(\cdot, \cdot) : V \times V \to \mathbb{R}$ is a symmetric bilinear form which is positive semidefinite, that is, $(x,x) \ge 0$ for all $x$.  Then for any $x,y \in V$ we have $|(x,y)|^2 \le (x,x) (y,y)$. I'd like to know what happens if we replace $\mathbb{R}$ by some other space $W$.  Suppose at first that $W$ is a real vector space, equipped with a partial order $\le$ that makes it an an ordered vector space , as well as a multiplication operation $\cdot$ that makes it an algebra.  Then it makes sense to speak of a positive semidefinite symmetric bilinear form $(\cdot, \cdot) : V \times V \to W$, and ask whether it satisfies the Cauchy-Schwarz inequality $(v,w)\cdot(v,w) \le (v,v) \cdot (v,w)$. Under what conditions on $W$ does this ""generalized Cauchy-Schwarz inequality"" hold? At a minimum I expect we will need some more structure on $W$; in particular I assume we would like the multiplication and the partial ordering in $W$ to interact in some reasonable way, so that for instance $w\cdot w \ge 0$ for all $w \in W$.  Are there other properties that $W$ should have? There are lots of proofs of the classical Cauchy-Schwarz inequality ; presumably one should try to find one of them which generalizes.  But I couldn't immediately see how to do this. Here are some motivating examples. As a fairly simple one, let $X$ be any set, and $W = \mathbb{R}^X$ the vector space of all real-valued functions on $X$.  We can equip $W$ with the pointwise multiplication and ordering.  Then let $V$ be any linear subspace of $W$, and let the bilinear form $V \times V \to W$ also be pointwise multiplication.  Then of course Cauchy-Schwarz holds since we can just prove it pointwise. For a slightly less trivial example, let $(X,\mu)$ be a measure space, and $W = L^0(X,\mu)$ be the vector space of all measurable functions on $X$, mod $\mu$-almost-everywhere equality (so an element of $W$ is in fact an equivalence class of functions).  Again let $\cdot$ be pointwise multiplication (which is well defined), and the ordering $f \le g$ when $f(x) \le g(x)$ almost everywhere.  Take again a linear subspace $V \subset W$, and pointwise multiplication as the bilinear form.  Now Cauchy-Schwarz holds because we can prove it pointwise on a set of full measure. A related but more complicated example from probability (and my original motivation) is the quadratic variation form from probability.  For instance, we could take $V$ to be the vector space of continuous $L^2$ martingales on some filtered probability space over some time interval $[0,T]$, and $W$ the vector space of continuous adapted processes of bounded variation, mod indistinguishability , with pointwise multiplication and the partial order $X \le Y$ iff $X_t \le Y_t$ for all $t$ almost surely.  Then the quadratic variation $\langle M,N \rangle$ is a symmetric positive semidefinite bilinear form from $V \times V$ to $W$. In this case I can prove the Cauchy-Schwarz inequality pointwise: fix $M,N \in V$.  For almost every $\omega$, for all $t \in [0,T]$ and all $q \in \mathbb{Q}$ I can say $$q^2 \langle M,M \rangle_t(\omega) \pm 2 \langle M,N \rangle_t(\omega) + \frac{1}{q^2} \langle N,N \rangle_t(\omega) = \langle q M \pm \frac{1}{q} N \rangle_t(\omega) \ge 0$$ and then letting $q$ be a rational very close to $\sqrt{\langle N,N \rangle_t(\omega) / \langle M,M \rangle_t(\omega)}$ shows that $$|\langle M,N \rangle_t(\omega)| \le \sqrt{\langle M,M \rangle_t(\omega) \langle N,N \rangle_t(\omega)}$$ which is what we want. In each of these examples, we are working on function spaces (or quotients thereof), and the proof essentially operates pointwise.  I'm hoping for some kind of more abstract global argument.",,"['real-analysis', 'linear-algebra', 'abstract-algebra', 'inequality']"
47,Function $f$ such that $|f(x)-f(y)| \ge \sqrt{|x-y|}$,Function  such that,f |f(x)-f(y)| \ge \sqrt{|x-y|},"After having spent some time on this problem and having found little on this topic in existing articles, I decided to post it here. My question is : Does there exist a bounded (injective) function $f: [0,1] \rightarrow \mathbb{C}$ such that $$\forall (x,y) \in [0,1]^2,\ |f(x)-f(y)| \ge \sqrt{|x-y|}$$ (It is not necessary to assume $f$ to be injective, as it follows from the last hypothesis.) Note that I am not asking the function to be continuous. Indeed, using a result of Besicovitch and Schoenberg, I can prove that such a function can not be continuous (however this is not really easy to prove ; see On Jordan Arcs and... ). By the way, in the same paper, they proved that given any $\varepsilon>0$ there exists a continuous injection $f : [0,1] \rightarrow \mathbb{C}$ such that $\forall (x,y) \in [0,1],\ |f(x)-f(y)| \ge |x-y|^{\frac{1}{2}+\varepsilon}$. I really find this result noteworthy. At first I thought I could directly use it, by taking the limit (in a certain sense) to answer my question, but the supremum of the functions they give goes to $+\infty$ when $\varepsilon$ goes to $0$, and I am requiring boundedness : it did not work. Regarding my problem, I believe that there are no such functions, but did not manage to prove it. I tried to consider the reciprocal function : from this point of view, we must study Holder continuous functions of order $2$ defined on subsets of $\mathbb{C}$. However, not much is known about Holder continuous functions of order $>1$ (when they are not constant), or rather, I did not manage to find articles investigating this matter. I also tried to study the discrete case : assuming such a function exists, and taking the image of $\left \{ \frac{k}{n}\ |\ 0 \le k \le n \right \}$, we have, for all $n \in \mathbb{N}^*$, $n$ complex points $x_1,...,x_n$ such that for all $i,j$, $|x_i-x_j| \ge \sqrt{\frac{|i-j|}{n}}$. All those points are in the same bounded set (because $f$ is bounded). However I think that the diameter of such sets grows unbounded when $n$ approaches $+\infty$. Same problem here, I could not find a proper reference dealing with this topic. I would be glad if someone had an idea on how to tackle this problem, or references to related articles. Edit : I posted an answer, which I hope is correct, but I would still be glad to see suggestions or different solutions.","After having spent some time on this problem and having found little on this topic in existing articles, I decided to post it here. My question is : Does there exist a bounded (injective) function $f: [0,1] \rightarrow \mathbb{C}$ such that $$\forall (x,y) \in [0,1]^2,\ |f(x)-f(y)| \ge \sqrt{|x-y|}$$ (It is not necessary to assume $f$ to be injective, as it follows from the last hypothesis.) Note that I am not asking the function to be continuous. Indeed, using a result of Besicovitch and Schoenberg, I can prove that such a function can not be continuous (however this is not really easy to prove ; see On Jordan Arcs and... ). By the way, in the same paper, they proved that given any $\varepsilon>0$ there exists a continuous injection $f : [0,1] \rightarrow \mathbb{C}$ such that $\forall (x,y) \in [0,1],\ |f(x)-f(y)| \ge |x-y|^{\frac{1}{2}+\varepsilon}$. I really find this result noteworthy. At first I thought I could directly use it, by taking the limit (in a certain sense) to answer my question, but the supremum of the functions they give goes to $+\infty$ when $\varepsilon$ goes to $0$, and I am requiring boundedness : it did not work. Regarding my problem, I believe that there are no such functions, but did not manage to prove it. I tried to consider the reciprocal function : from this point of view, we must study Holder continuous functions of order $2$ defined on subsets of $\mathbb{C}$. However, not much is known about Holder continuous functions of order $>1$ (when they are not constant), or rather, I did not manage to find articles investigating this matter. I also tried to study the discrete case : assuming such a function exists, and taking the image of $\left \{ \frac{k}{n}\ |\ 0 \le k \le n \right \}$, we have, for all $n \in \mathbb{N}^*$, $n$ complex points $x_1,...,x_n$ such that for all $i,j$, $|x_i-x_j| \ge \sqrt{\frac{|i-j|}{n}}$. All those points are in the same bounded set (because $f$ is bounded). However I think that the diameter of such sets grows unbounded when $n$ approaches $+\infty$. Same problem here, I could not find a proper reference dealing with this topic. I would be glad if someone had an idea on how to tackle this problem, or references to related articles. Edit : I posted an answer, which I hope is correct, but I would still be glad to see suggestions or different solutions.",,"['real-analysis', 'measure-theory', 'reference-request', 'holder-spaces']"
48,Is $1$ a limit point of the fractional part of $1.5^n$?,Is  a limit point of the fractional part of ?,1 1.5^n,"It is an open problem whether the fractional part of $\left(\dfrac32\right)^n$ is dense in $[0...1]$. The problem is: is $1$ a limit point of the above sequence? An equivalent formulation is: $\forall \epsilon > 0: \exists n \in \Bbb N: 1 - \{1.5^n\} < \epsilon$ where $\{x\}$ denotes the fractional part of $x$. Here is a table of $n$ against $\epsilon$ that I computed: $\begin{array}{|c|c|}\hline \epsilon & n \\\hline 1 & 1 \\\hline 0.5 & 5 \\\hline 0.4 & 8 \\\hline 0.35 & 10 \\\hline 0.3 & 12 \\\hline 0.1 & 14 \\\hline 0.05 & 46 \\\hline 0.01 & 157 \\\hline 0.005 & 163 \\\hline 0.001 & 1256 \\\hline 0.0005 & 2677 \\\hline 0.0001 & 8093 \\\hline 0.00001 & 49304 \\\hline 0.000005 & 158643 \\\hline 0.0000005 & 835999 \\\hline \end{array}$ References Unsolved Problems, edited by O. Strauch , in section 2.4 Exponential sequences it is explicitly mentioned that both questions whether $(3/2)^n\bmod 1$ is dense in $[0,1]$ and whether it is uniformly distributed in $[0,1]$ are open conjectures. Power Fractional Parts, on Wolfram Mathworld , ""just because the Internet says so""","It is an open problem whether the fractional part of $\left(\dfrac32\right)^n$ is dense in $[0...1]$. The problem is: is $1$ a limit point of the above sequence? An equivalent formulation is: $\forall \epsilon > 0: \exists n \in \Bbb N: 1 - \{1.5^n\} < \epsilon$ where $\{x\}$ denotes the fractional part of $x$. Here is a table of $n$ against $\epsilon$ that I computed: $\begin{array}{|c|c|}\hline \epsilon & n \\\hline 1 & 1 \\\hline 0.5 & 5 \\\hline 0.4 & 8 \\\hline 0.35 & 10 \\\hline 0.3 & 12 \\\hline 0.1 & 14 \\\hline 0.05 & 46 \\\hline 0.01 & 157 \\\hline 0.005 & 163 \\\hline 0.001 & 1256 \\\hline 0.0005 & 2677 \\\hline 0.0001 & 8093 \\\hline 0.00001 & 49304 \\\hline 0.000005 & 158643 \\\hline 0.0000005 & 835999 \\\hline \end{array}$ References Unsolved Problems, edited by O. Strauch , in section 2.4 Exponential sequences it is explicitly mentioned that both questions whether $(3/2)^n\bmod 1$ is dense in $[0,1]$ and whether it is uniformly distributed in $[0,1]$ are open conjectures. Power Fractional Parts, on Wolfram Mathworld , ""just because the Internet says so""",,"['real-analysis', 'sequences-and-series', 'number-theory', 'exponentiation', 'fractional-part']"
49,"Is $f(x)=\sum_{n=1}^\infty\frac{nx^2}{n^3+x^3}$ uniformly continuous on $[0,\infty)$?",Is  uniformly continuous on ?,"f(x)=\sum_{n=1}^\infty\frac{nx^2}{n^3+x^3} [0,\infty)","Last week I had an assignment to show $f(x)=\sum\limits_{n=1}^\infty\frac{nx^2}{n^3+x^3}$ for $x\ge0$ does not converge uniformly, but I misread the question as ""show $f(x)$ is not uniformly continuous."" The actual problem went on to show that $f(x)$ is continuous, but I have been stumped by the question I misread: Is $f(x)=\sum\limits_{n=1}^\infty\frac{nx^2}{n^3+x^3}$ uniformly continuous on $[0,\infty)$? I asked my professor about the problem today, but unfortunately we still didn't come up with an answer (and while my professor believes that $f(x)$ is not uniformly continuous, I suspect that it is). __ Things I have proven about $f(x)$ that I can explain or one can assume in an answer: The series does not converge uniformly to $f(x)$, $f(x)$ is continuous, and $f(x)>\frac{x-1}{2}$.","Last week I had an assignment to show $f(x)=\sum\limits_{n=1}^\infty\frac{nx^2}{n^3+x^3}$ for $x\ge0$ does not converge uniformly, but I misread the question as ""show $f(x)$ is not uniformly continuous."" The actual problem went on to show that $f(x)$ is continuous, but I have been stumped by the question I misread: Is $f(x)=\sum\limits_{n=1}^\infty\frac{nx^2}{n^3+x^3}$ uniformly continuous on $[0,\infty)$? I asked my professor about the problem today, but unfortunately we still didn't come up with an answer (and while my professor believes that $f(x)$ is not uniformly continuous, I suspect that it is). __ Things I have proven about $f(x)$ that I can explain or one can assume in an answer: The series does not converge uniformly to $f(x)$, $f(x)$ is continuous, and $f(x)>\frac{x-1}{2}$.",,"['real-analysis', 'continuity', 'uniform-continuity']"
50,Prove that:$\int_{a}^{b} f(x) dx = b \cdot f(b) - a \cdot f(a) - \int_{f(a)}^{f(b)} f^{-1}(x) dx$ [duplicate],Prove that: [duplicate],\int_{a}^{b} f(x) dx = b \cdot f(b) - a \cdot f(a) - \int_{f(a)}^{f(b)} f^{-1}(x) dx,"This question already has answers here : Show rigorously that the sum of integrals of $f$ and of its inverse is $bf(b)-af(a)$ (6 answers) Closed 6 years ago . I just wanted to ask, if my proof is correct. I haven't seen the equation before, but I think it's quite useful. Let $f$ be an bijective differentiable function. Then the inverse function $f^{-1}$ exists and the following equation holds: $$\int\limits_{a}^{b} f(x) dx = b \cdot f(b) - a \cdot f(a) - \int\limits_{f(a)}^{f(b)} f^{-1}(x) dx$$ Proof. $f$ is an bijective differentiable function and $F$ is an antiderivative of $f$. First we need to find an antiderivative of $f^{-1}$. $\int f^{-1}(x) dx$ with substitution $x = f(y)$ yields: $$\int y \cdot f'(y) dy = y \cdot f(y) - \int f(y) dy = y \cdot f(y) - F(y)$$ resubstitution yields: $$\int f^{-1}(x) dx = x \cdot f^{-1}(x) - F(f^{-1}(x))$$ hence $\int\limits_{f(a)}^{f(b)} f^{-1}(x) dx = \left[x \cdot f^{-1}(x) - F(f^{-1}(x)) \right ]_{f(a)}^{f(b)}$ $$=b \cdot f(b) - F(b) - (a \cdot f(a) - F(a)) = F(a) - F(b) + b \cdot f(b) - a \cdot f(a)$$ $$= \int\limits_{b}^{a} f(x)  dx + b \cdot f(b) - a \cdot f(a) = -\int\limits_{a}^{b} f(x)  dx + b \cdot f(b) - a \cdot f(a)$$ All in all: $$\int\limits_{f(a)}^{f(b)} f^{-1}(x) dx = -\int\limits_{a}^{b} f(x)  dx + b \cdot f(b) - a \cdot f(a)$$ which is equal to $$\int\limits_{a}^{b} f(x) dx = b \cdot f(b) - a \cdot f(a) - \int\limits_{f(a)}^{f(b)} f^{-1}(x) dx$$ q.e.d.","This question already has answers here : Show rigorously that the sum of integrals of $f$ and of its inverse is $bf(b)-af(a)$ (6 answers) Closed 6 years ago . I just wanted to ask, if my proof is correct. I haven't seen the equation before, but I think it's quite useful. Let $f$ be an bijective differentiable function. Then the inverse function $f^{-1}$ exists and the following equation holds: $$\int\limits_{a}^{b} f(x) dx = b \cdot f(b) - a \cdot f(a) - \int\limits_{f(a)}^{f(b)} f^{-1}(x) dx$$ Proof. $f$ is an bijective differentiable function and $F$ is an antiderivative of $f$. First we need to find an antiderivative of $f^{-1}$. $\int f^{-1}(x) dx$ with substitution $x = f(y)$ yields: $$\int y \cdot f'(y) dy = y \cdot f(y) - \int f(y) dy = y \cdot f(y) - F(y)$$ resubstitution yields: $$\int f^{-1}(x) dx = x \cdot f^{-1}(x) - F(f^{-1}(x))$$ hence $\int\limits_{f(a)}^{f(b)} f^{-1}(x) dx = \left[x \cdot f^{-1}(x) - F(f^{-1}(x)) \right ]_{f(a)}^{f(b)}$ $$=b \cdot f(b) - F(b) - (a \cdot f(a) - F(a)) = F(a) - F(b) + b \cdot f(b) - a \cdot f(a)$$ $$= \int\limits_{b}^{a} f(x)  dx + b \cdot f(b) - a \cdot f(a) = -\int\limits_{a}^{b} f(x)  dx + b \cdot f(b) - a \cdot f(a)$$ All in all: $$\int\limits_{f(a)}^{f(b)} f^{-1}(x) dx = -\int\limits_{a}^{b} f(x)  dx + b \cdot f(b) - a \cdot f(a)$$ which is equal to $$\int\limits_{a}^{b} f(x) dx = b \cdot f(b) - a \cdot f(a) - \int\limits_{f(a)}^{f(b)} f^{-1}(x) dx$$ q.e.d.",,"['calculus', 'real-analysis', 'integration', 'analysis', 'surfaces']"
51,"Show that there is a $c\in(0,1)$ such that $f(c)=\int_0^cf(x)dx$.",Show that there is a  such that .,"c\in(0,1) f(c)=\int_0^cf(x)dx","Question: Let $f:[0,1]\to\mathbb{R}$ be a continuous function such that $$\int_0^1f(x)dx=\int_0^1xf(x)dx.$$ Show that there is a $c\in(0,1)$ such that $$f(c)=\int_0^cf(x)dx.$$ My solution: Define the function $g:[0,1]\to\mathbb{R}$ , such that $$g(x)=x\int_0^x f(t)dt-\int_0^x tf(t)dt, \forall x\in[0,1].$$ Now since $f$ is continuous on $[0,1]$ , thus we can conclude by Fundamental Theorem of Calculus that $g$ is differentiable $\forall x\in[0,1]$ and $$g'(x)=\int_0^x f(t)dt+xf(x)-xf(x)=\int_0^xf(t)dt, \forall x\in[0,1].$$ Observe that $g(0)=g(1)=0$ . Hence by Rolle's Theorem we can conclude that $\exists b\in(0,1)$ , such that $g'(b)=0$ , i.e $$\int_0^b f(t)dt=0.$$ Now define $h:[0,1]\to\mathbb{R}$ , such that $$h(x)=e^{-x}g'(x),  \forall x\in[0,1].$$ Now $h'(x)=-e^{-x}g'(x)+g''(x)e^{-x}=e^{-x}(g''(x)-g'(x)), \forall x\in[0,1].$ Observe that $h(0)=h(b)=0$ . Hence by Rolle's Theorem we can conclude that $\exists c\in(0,b)\subseteq (0,1)$ , such that $h'(c)=0$ . This implies that $$e^{-c}(g''(c)-g'(c))=0\\\implies g''(c)-g'(c)=0\hspace{0.3 cm}(\because e^{-c}\neq 0)\\\implies f(c)=\int_0^cf(x)dx.$$ Is this solution correct? And is there a better solution that this?","Question: Let be a continuous function such that Show that there is a such that My solution: Define the function , such that Now since is continuous on , thus we can conclude by Fundamental Theorem of Calculus that is differentiable and Observe that . Hence by Rolle's Theorem we can conclude that , such that , i.e Now define , such that Now Observe that . Hence by Rolle's Theorem we can conclude that , such that . This implies that Is this solution correct? And is there a better solution that this?","f:[0,1]\to\mathbb{R} \int_0^1f(x)dx=\int_0^1xf(x)dx. c\in(0,1) f(c)=\int_0^cf(x)dx. g:[0,1]\to\mathbb{R} g(x)=x\int_0^x f(t)dt-\int_0^x tf(t)dt, \forall x\in[0,1]. f [0,1] g \forall x\in[0,1] g'(x)=\int_0^x f(t)dt+xf(x)-xf(x)=\int_0^xf(t)dt, \forall x\in[0,1]. g(0)=g(1)=0 \exists b\in(0,1) g'(b)=0 \int_0^b f(t)dt=0. h:[0,1]\to\mathbb{R} h(x)=e^{-x}g'(x), 
\forall x\in[0,1]. h'(x)=-e^{-x}g'(x)+g''(x)e^{-x}=e^{-x}(g''(x)-g'(x)), \forall x\in[0,1]. h(0)=h(b)=0 \exists c\in(0,b)\subseteq (0,1) h'(c)=0 e^{-c}(g''(c)-g'(c))=0\\\implies g''(c)-g'(c)=0\hspace{0.3 cm}(\because e^{-c}\neq 0)\\\implies f(c)=\int_0^cf(x)dx.","['real-analysis', 'solution-verification']"
52,"Prove that $\sum_{k=0}^n{e^{ik^2}} = o(n^\alpha)$, $ \forall \alpha >0$","Prove that ,",\sum_{k=0}^n{e^{ik^2}} = o(n^\alpha)  \forall \alpha >0,"I want to prove that :  $\sum_{k=0}^n{e^{ik^2}} = o(n^\alpha)$, $ \forall \alpha >0$ when $n$ tends to $+\infty$ Perhaps $\sum_{k=0}^n{e^{ik^2}}$ is bounded, I don't know. Do you have ideas ?","I want to prove that :  $\sum_{k=0}^n{e^{ik^2}} = o(n^\alpha)$, $ \forall \alpha >0$ when $n$ tends to $+\infty$ Perhaps $\sum_{k=0}^n{e^{ik^2}}$ is bounded, I don't know. Do you have ideas ?",,"['real-analysis', 'summation']"
53,Simpler proof of the Hardy-Littlewood-Sobolev inequality in the inhomogeneous case,Simpler proof of the Hardy-Littlewood-Sobolev inequality in the inhomogeneous case,,"The Hardy-Littlewood-Sobolev inequality is the statement that there is a $C>0$ such that $$ \tag{HLS}  \lVert f\ast \lvert\cdot\rvert^{-\alpha}\rVert_p\le C\lVert f\rVert_q, $$ for all $f\in L^q(\mathbb R^d)$ , where the convolution is defined as $$(f\ast \lvert\cdot\rvert^{-\alpha} )(x)= \int_{\mathbb R^d} \frac{f(y)}{\lvert x-y\rvert^{\alpha}}\, dy, $$ and the parameters satisfy the conditions $$\tag{1} 0<\alpha<d, \quad \frac1p-\frac1 q+1=\frac\alpha d.$$ Now let us define the Japanese bracket $$\langle x \rangle:= \sqrt{1+ \lvert x\rvert^2},\qquad x\in\mathbb R^d,$$ which is a non-homogeneous version of $\lvert\cdot\rvert$ . The function $\langle x\rangle^{-\alpha}$ has the exact same decay at $\lvert x \rvert \to \infty$ of its homogeneous counterpart $\lvert x \rvert^{-\alpha}$ , but $\langle x \rangle^{-\alpha}$ is not singular at $x=0$ . By the obvious estimate $\langle x\rangle^{-\alpha}\le \lvert x\rvert^{-\alpha}$ , (HLS) immediately implies its non-homogeneous version $$\tag{n-HLS}  \lVert f\ast \langle\cdot\rangle^{-\alpha}\rVert_p\le C\lVert f\rVert_q, $$ under the assumptions (1). Question . Is there a direct proof of (n-HLS) that is simpler than a proof of (HLS)? Remark 1 . The inequality (n-HLS) actually holds for $\frac1 p - \frac1q +1 \le \frac{\alpha}{d}$ . However, the non-endpoint case $\frac1 p - \frac1q +1 < \frac{\alpha}{d}$ can be immediately proved by an application of the Young inequality for convolutions. Thus, the present question is concerned solely with the endpoint case (1). In the following, the letter $C$ will always denote an irrelevant positive constant, whose value may change from line to line. Remark 2 . There are many proofs of (HLS). One that I know uses the Hardy-Littlewood maximal function. By decomposing the ball $B(x, r)$ into dyadic annuli, we obtain the local estimate $$\tag{2}\left\lvert \int_{B(x, r)}\frac{f(y)}{\lvert x-y \rvert^\alpha}dy\right\rvert \le r^{d-\alpha}\sum_{j=0}^\infty 2^{\alpha j - dj} Mf(x)= C_{d, \alpha}r^{d-\alpha}Mf(x).$$ This constant $C_{d,\alpha}$ equals $\sum 2^{(\alpha  - d)j}$ , which is finite because $\alpha<d$ . Here $Mf$ denotes the Hardy-Littlewood maximal function $$ Mf(x):=\sup \frac1{\lvert B\rvert} \int_B \lvert f(y)\rvert\, dy,$$ where the sup is taken over all balls $B$ containing $x$ . We then estimate the tail of the convolution via the Hölder inequality; $$\tag{3} \left\lvert \int_{\mathbb R^d\setminus B(x, r)}\frac{f(y)}{\lvert x-y \rvert^\alpha}dy\right\rvert\le C r^{\frac{d}{q'}-\alpha} \lVert f\rVert_q.$$ Combining (2) and (3) gives the pointwise bound $$ \tag{4} \lvert f\ast \lvert\cdot\rvert^{-\alpha}(x)\rvert \le C\left( r^{d-\alpha} Mf(x) + r^{\frac{d}{q'}-\alpha} \lVert f\rVert_q\right).$$ Choosing the $r$ that minimises the right-hand side, then integrating and applying the Hardy-Littewood maximal estimate $\lVert Mf\rVert_q\le C\lVert f \rVert_q$ , we obtain (HLS). This procedure will of course work if $\lvert\cdot\rvert^{-\alpha}$ is replaced by $\langle\cdot\rangle^{-\alpha}$ . However, I think that there shouldn't be the need for the careful dyadic analysis of equation (2). Indeed, $\langle x-y\rangle^{-\alpha}$ is not singular at $y=x$ . This is why I am asking for a simpler proof.","The Hardy-Littlewood-Sobolev inequality is the statement that there is a such that for all , where the convolution is defined as and the parameters satisfy the conditions Now let us define the Japanese bracket which is a non-homogeneous version of . The function has the exact same decay at of its homogeneous counterpart , but is not singular at . By the obvious estimate , (HLS) immediately implies its non-homogeneous version under the assumptions (1). Question . Is there a direct proof of (n-HLS) that is simpler than a proof of (HLS)? Remark 1 . The inequality (n-HLS) actually holds for . However, the non-endpoint case can be immediately proved by an application of the Young inequality for convolutions. Thus, the present question is concerned solely with the endpoint case (1). In the following, the letter will always denote an irrelevant positive constant, whose value may change from line to line. Remark 2 . There are many proofs of (HLS). One that I know uses the Hardy-Littlewood maximal function. By decomposing the ball into dyadic annuli, we obtain the local estimate This constant equals , which is finite because . Here denotes the Hardy-Littlewood maximal function where the sup is taken over all balls containing . We then estimate the tail of the convolution via the Hölder inequality; Combining (2) and (3) gives the pointwise bound Choosing the that minimises the right-hand side, then integrating and applying the Hardy-Littewood maximal estimate , we obtain (HLS). This procedure will of course work if is replaced by . However, I think that there shouldn't be the need for the careful dyadic analysis of equation (2). Indeed, is not singular at . This is why I am asking for a simpler proof.","C>0 
\tag{HLS} 
\lVert f\ast \lvert\cdot\rvert^{-\alpha}\rVert_p\le C\lVert f\rVert_q,  f\in L^q(\mathbb R^d) (f\ast \lvert\cdot\rvert^{-\alpha} )(x)= \int_{\mathbb R^d} \frac{f(y)}{\lvert x-y\rvert^{\alpha}}\, dy,  \tag{1} 0<\alpha<d, \quad \frac1p-\frac1 q+1=\frac\alpha d. \langle x \rangle:= \sqrt{1+ \lvert x\rvert^2},\qquad x\in\mathbb R^d, \lvert\cdot\rvert \langle x\rangle^{-\alpha} \lvert x \rvert \to \infty \lvert x \rvert^{-\alpha} \langle x \rangle^{-\alpha} x=0 \langle x\rangle^{-\alpha}\le \lvert x\rvert^{-\alpha} \tag{n-HLS} 
\lVert f\ast \langle\cdot\rangle^{-\alpha}\rVert_p\le C\lVert f\rVert_q,  \frac1 p - \frac1q +1 \le \frac{\alpha}{d} \frac1 p - \frac1q +1 < \frac{\alpha}{d} C B(x, r) \tag{2}\left\lvert \int_{B(x, r)}\frac{f(y)}{\lvert x-y \rvert^\alpha}dy\right\rvert \le r^{d-\alpha}\sum_{j=0}^\infty 2^{\alpha j - dj} Mf(x)= C_{d, \alpha}r^{d-\alpha}Mf(x). C_{d,\alpha} \sum 2^{(\alpha  - d)j} \alpha<d Mf 
Mf(x):=\sup \frac1{\lvert B\rvert} \int_B \lvert f(y)\rvert\, dy, B x \tag{3}
\left\lvert \int_{\mathbb R^d\setminus B(x, r)}\frac{f(y)}{\lvert x-y \rvert^\alpha}dy\right\rvert\le C r^{\frac{d}{q'}-\alpha} \lVert f\rVert_q. 
\tag{4} \lvert f\ast \lvert\cdot\rvert^{-\alpha}(x)\rvert \le C\left( r^{d-\alpha} Mf(x) + r^{\frac{d}{q'}-\alpha} \lVert f\rVert_q\right). r \lVert Mf\rVert_q\le C\lVert f \rVert_q \lvert\cdot\rvert^{-\alpha} \langle\cdot\rangle^{-\alpha} \langle x-y\rangle^{-\alpha} y=x",['real-analysis']
54,"Is series $\sum_{n=1}^{\infty}\frac{\cos(nx)}{n^\alpha}$, for $\alpha>0$, convergent?","Is series , for , convergent?",\sum_{n=1}^{\infty}\frac{\cos(nx)}{n^\alpha} \alpha>0,"I've done the following exercise: Is series $\displaystyle\sum^{\infty}_{n=1}\frac{\cos(nx)}{n^\alpha}$ , for $\alpha>0$ , convergent? My approach: We're going to use the Dirichlet's criterion for convergence of series. Let $\displaystyle\ \{a_n\}=\frac{1}{n^{\alpha}}$ and $\{b_n\}=\cos(nx)$ . We see that $\{a_n\}$ is decreasing and has limit $0$ . We have to see now that $$\sup_{N}{\left| \sum_{n=1}^{N}{\cos(nx)} \right|}<\infty.$$ $$\displaystyle \sum_{n=0}^{N}e^{inx}=\sum_{n=0}^{N}(e^{ix})^{n}= \frac{1-e^{ix(n+1)}}{1-e^{ix}}, \text{if x} \neq 2k\pi. $$ So $${\left| \sum_{n=0}^{N}{\cos(nx)} \right|}={\left| \Re\left(\sum_{n=0}^{N}{e^{inx}}\right) \right|}={\left| \Re\left(\frac{1-e^{ix(N+1)}}{1-e^{ix}}\right) \right|} \leq {\left| \frac{1-e^{ix(N+1)}}{1-e^{ix}} \right|}\leq {\frac{\left|1\right|+\left| e^{i(N+1)x} \right|}{\left|1-e^{{ix}}  \right|}}=\frac{2}{\left|1-e^{{ix}}  \right|}.$$ So, by Dirichlet, if $x\neq 2k\pi$ , the series is convergent. What happens if $x= 2k\pi$ ? $$\displaystyle\sum^{\infty}_{n=1}\frac{\cos(n(2k\pi))}{n^\alpha}=\displaystyle\sum^{\infty}_{n=1}\frac{1}{n^\alpha}$$ and we know that this series converges when $\alpha>1$ and diverges if $\alpha\leq 1$ . Have I done any mistake/s? Is my approach correct? Thank you.","I've done the following exercise: Is series , for , convergent? My approach: We're going to use the Dirichlet's criterion for convergence of series. Let and . We see that is decreasing and has limit . We have to see now that So So, by Dirichlet, if , the series is convergent. What happens if ? and we know that this series converges when and diverges if . Have I done any mistake/s? Is my approach correct? Thank you.","\displaystyle\sum^{\infty}_{n=1}\frac{\cos(nx)}{n^\alpha} \alpha>0 \displaystyle\ \{a_n\}=\frac{1}{n^{\alpha}} \{b_n\}=\cos(nx) \{a_n\} 0 \sup_{N}{\left| \sum_{n=1}^{N}{\cos(nx)} \right|}<\infty. \displaystyle \sum_{n=0}^{N}e^{inx}=\sum_{n=0}^{N}(e^{ix})^{n}= \frac{1-e^{ix(n+1)}}{1-e^{ix}}, \text{if x} \neq 2k\pi.  {\left| \sum_{n=0}^{N}{\cos(nx)} \right|}={\left| \Re\left(\sum_{n=0}^{N}{e^{inx}}\right) \right|}={\left| \Re\left(\frac{1-e^{ix(N+1)}}{1-e^{ix}}\right) \right|} \leq {\left| \frac{1-e^{ix(N+1)}}{1-e^{ix}} \right|}\leq {\frac{\left|1\right|+\left| e^{i(N+1)x} \right|}{\left|1-e^{{ix}}  \right|}}=\frac{2}{\left|1-e^{{ix}}  \right|}. x\neq 2k\pi x= 2k\pi \displaystyle\sum^{\infty}_{n=1}\frac{\cos(n(2k\pi))}{n^\alpha}=\displaystyle\sum^{\infty}_{n=1}\frac{1}{n^\alpha} \alpha>1 \alpha\leq 1","['calculus', 'real-analysis', 'sequences-and-series', 'convergence-divergence', 'dirichlet-series']"
55,"Completeness of $\{ f_n : n \in \mathbb N \} \subset C[0,1]$ in $L^1[0,1]$",Completeness of  in,"\{ f_n : n \in \mathbb N \} \subset C[0,1] L^1[0,1]","Suppose that $\{ f_n : n \in \mathbb N \} \subset C[0,1]$ is a system of continuous functions which is complete in $L^1[0,1]$ , i.e. each $g \in L^1[0,1]$ can be approximated arbitrarily well by a finite linear combination of the $f_n$ 's. Is it true that if $g \in L^1[0,1]$ is an arbitrary $L^1$ -functions such that $$ \int_0^1g(t)f_n(t) \, dt = 0 \ \ \ \ \forall n \in \mathbb N $$ then $g=0$ almost everywhere? I know that this is true if $L^1[0,1]$ gets replaced by $L^2[0,1]$ since then we're in a Hilbert space setting. Thanks for any help!","Suppose that is a system of continuous functions which is complete in , i.e. each can be approximated arbitrarily well by a finite linear combination of the 's. Is it true that if is an arbitrary -functions such that then almost everywhere? I know that this is true if gets replaced by since then we're in a Hilbert space setting. Thanks for any help!","\{ f_n : n \in \mathbb N \} \subset C[0,1] L^1[0,1] g \in L^1[0,1] f_n g \in L^1[0,1] L^1 
\int_0^1g(t)f_n(t) \, dt = 0 \ \ \ \ \forall n \in \mathbb N
 g=0 L^1[0,1] L^2[0,1]","['real-analysis', 'functional-analysis', 'measure-theory', 'lp-spaces']"
56,How find this nice minmum of this value $\frac{\prod_{i=1}^{n-1}(a_{i}+a_{i+1})\sum a_{i}}{\prod_{i=1}^{n}a_{i}}$,How find this nice minmum of this value,\frac{\prod_{i=1}^{n-1}(a_{i}+a_{i+1})\sum a_{i}}{\prod_{i=1}^{n}a_{i}},"Let $n$ be give postive integer number. For any $a_{i}>0 (i=1,2,\cdots,n)$ , find the minimum of the value $$F_{n}(a_{1},a_{2},\cdots,a_{n})=\dfrac{(a_{1}+a_{2})(a_{2}+a_{3})\cdots (a_{n-1}+a_{n})(a_{1}+a_{2}+\cdots+a_{n})}{a_{1}a_{2}a_{3}\cdots a_{n}}.$$ (by wang yong xi) I try when $n=2$ ,then $$F_{2}(a_{1},a_{2})=\dfrac{(a_{1}+a_{2})(a_{1}+a_{2})}{a_{1}a_{2}}=\dfrac{(a_{1}+a_{2})^2}{a_{1}a_{2}}\ge 4$$ when $a_{1}=a_{2}$ is minimum (2):when $n=3$ , $$F_{3}=\dfrac{(a_{1}+a_{2})(a_{2}+a_{3})(a_{1}+a_{2}+a_{3})}{a_{1}a_{2}a_{3}}$$ WLOG $a_{3}=1$ ,so $$F=\dfrac{(a_{1}+a_{2})(a_{2}+1)(a_{1}+a_{2}+1)}{a_{1}a_{2}}$$ I use this find this minimum is $$(F_{3})_{min}=\dfrac{1}{2}(11+5\sqrt{5})$$ when $a_{1}=1,a_{2}=\dfrac{\sqrt{5}-1}{2}$ see links","Let be give postive integer number. For any , find the minimum of the value (by wang yong xi) I try when ,then when is minimum (2):when , WLOG ,so I use this find this minimum is when see links","n a_{i}>0 (i=1,2,\cdots,n) F_{n}(a_{1},a_{2},\cdots,a_{n})=\dfrac{(a_{1}+a_{2})(a_{2}+a_{3})\cdots (a_{n-1}+a_{n})(a_{1}+a_{2}+\cdots+a_{n})}{a_{1}a_{2}a_{3}\cdots a_{n}}. n=2 F_{2}(a_{1},a_{2})=\dfrac{(a_{1}+a_{2})(a_{1}+a_{2})}{a_{1}a_{2}}=\dfrac{(a_{1}+a_{2})^2}{a_{1}a_{2}}\ge 4 a_{1}=a_{2} n=3 F_{3}=\dfrac{(a_{1}+a_{2})(a_{2}+a_{3})(a_{1}+a_{2}+a_{3})}{a_{1}a_{2}a_{3}} a_{3}=1 F=\dfrac{(a_{1}+a_{2})(a_{2}+1)(a_{1}+a_{2}+1)}{a_{1}a_{2}} (F_{3})_{min}=\dfrac{1}{2}(11+5\sqrt{5}) a_{1}=1,a_{2}=\dfrac{\sqrt{5}-1}{2}","['real-analysis', 'inequality']"
57,Is It Always Possible to Draw A Connected Compact Set in $\mathbb R^2$?,Is It Always Possible to Draw A Connected Compact Set in ?,\mathbb R^2,"Inspired by this answer , I wondered whether a printer could render all continuous functions ""well enough"". In particular, I am curious about the following statement: Let $S$ be a compact, connected subset of $\mathbb R^2$. For any $\varepsilon>0$ define $$S_{\varepsilon}=\bigcup_{s\in S}B(s,\varepsilon)$$ where $B(s,\varepsilon)$ ithe ball of radius $\varepsilon$ around $s$ (under the usual metric) - that is $S_{\varepsilon}$ is $S$ ""expanded"" everywhere by $\varepsilon$. For all $S$ and $\varepsilon$, must there exist a curve $\gamma:\mathbb [0,1]\rightarrow\mathbb R^2$ of finite length such that $$S_{\varepsilon}=\bigcup_{x\in[0,1]}B(\gamma(x),\varepsilon)$$ Or, putting it informally: Given a compact, connected set, is it possible for a printer, which always draws a swath of radius $\varepsilon$, to render the set as closely as possible? My thinking is ""yes"", because, we can clearly do this if we allow any sloppiness -that is, if we don't care whether the the points within some $0<\varepsilon'$ of the boundary of $S_{\varepsilon}$ are covered. Hopefully, complex figures on the boundary will be swallowed up when we expand out the set $S$ - but I'm suspicious of pathological examples and can't seem to draw up a proof or counterexample. A statement I believe is equivalent (and of which equivalence I believe a simple proof likely exists) is the following: Define $F_{\varepsilon}(S)$ to be the ""inflation"" of $S$ by $\varepsilon$ - the set of points within $\varepsilon$ of some $s\in S$. Define $f_{\varepsilon}(S)$ to be the ""deflation"" of $S$ by $\varepsilon$ - the set of points $s\in S$ such that the ball of radius $\varepsilon$ around is contained in $S$. Then, we wish to show that the boundary of $f_{\varepsilon}(F_{\varepsilon}(S))$ has finite length for any compact $S$.","Inspired by this answer , I wondered whether a printer could render all continuous functions ""well enough"". In particular, I am curious about the following statement: Let $S$ be a compact, connected subset of $\mathbb R^2$. For any $\varepsilon>0$ define $$S_{\varepsilon}=\bigcup_{s\in S}B(s,\varepsilon)$$ where $B(s,\varepsilon)$ ithe ball of radius $\varepsilon$ around $s$ (under the usual metric) - that is $S_{\varepsilon}$ is $S$ ""expanded"" everywhere by $\varepsilon$. For all $S$ and $\varepsilon$, must there exist a curve $\gamma:\mathbb [0,1]\rightarrow\mathbb R^2$ of finite length such that $$S_{\varepsilon}=\bigcup_{x\in[0,1]}B(\gamma(x),\varepsilon)$$ Or, putting it informally: Given a compact, connected set, is it possible for a printer, which always draws a swath of radius $\varepsilon$, to render the set as closely as possible? My thinking is ""yes"", because, we can clearly do this if we allow any sloppiness -that is, if we don't care whether the the points within some $0<\varepsilon'$ of the boundary of $S_{\varepsilon}$ are covered. Hopefully, complex figures on the boundary will be swallowed up when we expand out the set $S$ - but I'm suspicious of pathological examples and can't seem to draw up a proof or counterexample. A statement I believe is equivalent (and of which equivalence I believe a simple proof likely exists) is the following: Define $F_{\varepsilon}(S)$ to be the ""inflation"" of $S$ by $\varepsilon$ - the set of points within $\varepsilon$ of some $s\in S$. Define $f_{\varepsilon}(S)$ to be the ""deflation"" of $S$ by $\varepsilon$ - the set of points $s\in S$ such that the ball of radius $\varepsilon$ around is contained in $S$. Then, we wish to show that the boundary of $f_{\varepsilon}(F_{\varepsilon}(S))$ has finite length for any compact $S$.",,"['real-analysis', 'metric-spaces', 'continuity']"
58,"If a Radon measure is a tempered distribution, does it integrate all Schwartz functions?","If a Radon measure is a tempered distribution, does it integrate all Schwartz functions?",,"The question might at first sight sound like the answer is trivially ""yes"", so let me clarify the question a bit. Consider given a nonnegative Radon measure $\mu$ on $\mathbb{R}^n$. Let $\mathcal{D}(\mathbb{R}^n)$ denote the space of real test functions, that is, the space of infinitely differentiable functions with compact support from $\mathbb{R}^n$ to $\mathbb{R}$. We may then define the operator $T:\mathcal{D}(\mathbb{R}^n)\to\mathbb{R}$ by $$ T(\varphi) = \int_{\mathbb{R}^n} \varphi(x) d\mu(x). $$ This is then a distribution, in the sense of being a continuous linear functional $\mathcal{D}(\mathbb{R}^n)\to\mathbb{R}$, when $\mathcal{D}(\mathbb{R}^n)$ is given its usual topology based on a family of norms as in Rudin's ""Functional analysis"". Now let $\mathcal{S}(\mathbb{R}^n)$ denote the space of real Schwartz functions, meaning the space of rapidly decreasing functions, endowed with its usual topology, again see Rudin's book for details. $\textbf{Assume the following:}$ That $T$ can be extended to a linear functional on $\mathcal{S}(\mathbb{R}^n)$ which is continuous in the topology of $\mathcal{S}(\mathbb{R}^n)$. $\textbf{My question is this:}$ Does it holds that $$ T(\varphi) = \int_{\mathbb{R}^n} \varphi(x) d\mu(x). $$ for all $\varphi\in\mathcal{S}(\mathbb{R}^n)$? $\textbf{Some remarks:}$ The problem is that while we know that $T$ extends from $\mathcal{D}(\mathbb{R}^n)$ to $\mathcal{S}(\mathbb{R}^n)$, we do not know that the integral form of the operator carries over from $\mathcal{D}(\mathbb{R}^n)$ to $\mathcal{S}(\mathbb{R}^n)$. In fact, we do not even know that all functions in $\mathcal{S}(\mathbb{R}^n)$ are integrable with respect to $\mu$. A natural first approach to the problem would be to consider some kind of approximation argument. For example, take $\varphi\in\mathcal{S}(\mathbb{R}^n)$. Assume that $\varphi\ge0$. Using convolutions, we may then construct a sequence $(\varphi_n)$ in $\mathcal{D}(\mathbb{R}^n)$ which converges monotonely and in $\mathcal{S}(\mathbb{R}^n)$ to $\varphi$. This then yields $$   \int_{\mathbb{R}^n} \varphi(x) d\mu(x)            = \lim_n \int_{\mathbb{R}^n} \varphi_n(x) d\mu(x)            = \lim_n T(\varphi_n), $$ where the limit a priori may be infinite. However, as we have assumed that $T$ extends to $\mathcal{S}(\mathbb{R}^n)$, the limit of $T(\varphi_n)$ is finite, and so $\varphi$ is integrable with respect to $\mu$. This should show that the integral form of the operator is preserved for all nonnegative Schwartz functions. For a general Schwartz function $\varphi$, the natural thing would be to write $\varphi = \varphi^+-\varphi^-$, where $\varphi^+$ and $\varphi^-$ are the positive and negative parts of $\varphi$, respectively. However, while these two functions are nonnegative, they are not Schwartz functions, nor test functions for that matter, and so we cannot apply $T$ to them.","The question might at first sight sound like the answer is trivially ""yes"", so let me clarify the question a bit. Consider given a nonnegative Radon measure $\mu$ on $\mathbb{R}^n$. Let $\mathcal{D}(\mathbb{R}^n)$ denote the space of real test functions, that is, the space of infinitely differentiable functions with compact support from $\mathbb{R}^n$ to $\mathbb{R}$. We may then define the operator $T:\mathcal{D}(\mathbb{R}^n)\to\mathbb{R}$ by $$ T(\varphi) = \int_{\mathbb{R}^n} \varphi(x) d\mu(x). $$ This is then a distribution, in the sense of being a continuous linear functional $\mathcal{D}(\mathbb{R}^n)\to\mathbb{R}$, when $\mathcal{D}(\mathbb{R}^n)$ is given its usual topology based on a family of norms as in Rudin's ""Functional analysis"". Now let $\mathcal{S}(\mathbb{R}^n)$ denote the space of real Schwartz functions, meaning the space of rapidly decreasing functions, endowed with its usual topology, again see Rudin's book for details. $\textbf{Assume the following:}$ That $T$ can be extended to a linear functional on $\mathcal{S}(\mathbb{R}^n)$ which is continuous in the topology of $\mathcal{S}(\mathbb{R}^n)$. $\textbf{My question is this:}$ Does it holds that $$ T(\varphi) = \int_{\mathbb{R}^n} \varphi(x) d\mu(x). $$ for all $\varphi\in\mathcal{S}(\mathbb{R}^n)$? $\textbf{Some remarks:}$ The problem is that while we know that $T$ extends from $\mathcal{D}(\mathbb{R}^n)$ to $\mathcal{S}(\mathbb{R}^n)$, we do not know that the integral form of the operator carries over from $\mathcal{D}(\mathbb{R}^n)$ to $\mathcal{S}(\mathbb{R}^n)$. In fact, we do not even know that all functions in $\mathcal{S}(\mathbb{R}^n)$ are integrable with respect to $\mu$. A natural first approach to the problem would be to consider some kind of approximation argument. For example, take $\varphi\in\mathcal{S}(\mathbb{R}^n)$. Assume that $\varphi\ge0$. Using convolutions, we may then construct a sequence $(\varphi_n)$ in $\mathcal{D}(\mathbb{R}^n)$ which converges monotonely and in $\mathcal{S}(\mathbb{R}^n)$ to $\varphi$. This then yields $$   \int_{\mathbb{R}^n} \varphi(x) d\mu(x)            = \lim_n \int_{\mathbb{R}^n} \varphi_n(x) d\mu(x)            = \lim_n T(\varphi_n), $$ where the limit a priori may be infinite. However, as we have assumed that $T$ extends to $\mathcal{S}(\mathbb{R}^n)$, the limit of $T(\varphi_n)$ is finite, and so $\varphi$ is integrable with respect to $\mu$. This should show that the integral form of the operator is preserved for all nonnegative Schwartz functions. For a general Schwartz function $\varphi$, the natural thing would be to write $\varphi = \varphi^+-\varphi^-$, where $\varphi^+$ and $\varphi^-$ are the positive and negative parts of $\varphi$, respectively. However, while these two functions are nonnegative, they are not Schwartz functions, nor test functions for that matter, and so we cannot apply $T$ to them.",,"['real-analysis', 'functional-analysis', 'measure-theory', 'distribution-theory']"
59,Is the minimum of this functional $C^{\infty}$?,Is the minimum of this functional ?,C^{\infty},"The problem: Let us define $$ \mathscr{F}(u)=\int_0^1(u'(x))^4-e^x\sin (u(x))\, \mathrm{d}x $$ for $u \in W^{1,1}([0,1])$ such that $u(0)=A$ and $u(1)=B$ . It don't matters what $A$ or $B$ are, because I am interested in the reasoning. I am asked to study if the minimum $u$ is such that $u \in C^{\infty}([0,1])$ or eventually $u \in C^{\infty}([0,1]-E)$ where $E$ is a closed and negligible subset of $[0,1]$ , i.e. it is closed and $\mu(E)=0$ . An attempt: First of all, thanks to Ioffe's theorem, we know that $\mathscr{F}$ is sequentially weakly lower semi continuous in $W^{1,1}([0,1])$ . Further more, because $(u'(x))^4-e^x\sin (u(x)) \geq (u'(x))^4-e$ , a minimum $u \in W^{1,1}$ exists. But what can be said about the regularity of $u$ ? I know the Tonelli’s partial regularity theorem but I cannot directly apply it here because $F_{pp}$ is not defined positive but we only have $F_{pp} \geq 0$ . I know that if $u'(x) \neq 0$ then $u$ is $C^{\infty}$ in a neighborhood of $x$ . My question is: $u \in C^{\infty}([0,1])$ ? Further more I would like to understand when can we apply Tonelli’s theorem if $F_{pp} \geq 0$ but not $F_{pp} > 0$ i.e. if $F_{pp}$ is positive semidefinite but not positive definite. Remark: I tried first solving the related Question .","The problem: Let us define for such that and . It don't matters what or are, because I am interested in the reasoning. I am asked to study if the minimum is such that or eventually where is a closed and negligible subset of , i.e. it is closed and . An attempt: First of all, thanks to Ioffe's theorem, we know that is sequentially weakly lower semi continuous in . Further more, because , a minimum exists. But what can be said about the regularity of ? I know the Tonelli’s partial regularity theorem but I cannot directly apply it here because is not defined positive but we only have . I know that if then is in a neighborhood of . My question is: ? Further more I would like to understand when can we apply Tonelli’s theorem if but not i.e. if is positive semidefinite but not positive definite. Remark: I tried first solving the related Question .","
\mathscr{F}(u)=\int_0^1(u'(x))^4-e^x\sin (u(x))\, \mathrm{d}x
 u \in W^{1,1}([0,1]) u(0)=A u(1)=B A B u u \in C^{\infty}([0,1]) u \in C^{\infty}([0,1]-E) E [0,1] \mu(E)=0 \mathscr{F} W^{1,1}([0,1]) (u'(x))^4-e^x\sin (u(x)) \geq (u'(x))^4-e u \in W^{1,1} u F_{pp} F_{pp} \geq 0 u'(x) \neq 0 u C^{\infty} x u \in C^{\infty}([0,1]) F_{pp} \geq 0 F_{pp} > 0 F_{pp}","['real-analysis', 'functional-analysis', 'optimization', 'maxima-minima', 'calculus-of-variations']"
60,"For what kind of infinite subset A of $\mathbb Z$ and irrational number $\alpha$, is $\{e^{k\alpha \pi i}: k\in A \}$ dense in $S^1 $?","For what kind of infinite subset A of  and irrational number , is  dense in ?",\mathbb Z \alpha \{e^{k\alpha \pi i}: k\in A \} S^1 ,"There is a well-known result saying that $\{e^{k\alpha \pi i}: k\in \mathbb Z \}$ is dense in $S^1$. By density, we can select an infinite subset $A$ of $\mathbb Z$ such that $\{e^{k\alpha \pi i}: k\in A \}$ is not dense in $S^1$ any more. So there is a natural question: For what kind of infinite subset A of $\mathbb Z$ and irrational number $\alpha$, is $\{e^{k\alpha \pi i}: k\in A \}$ dense in $S^1 $ ? I feel like if A contains a collection of an infinite Arithmetic sequence(like 1,3,5,...), then $\{e^{k\alpha \pi i}: k\in A \}$ should be dense in $S^1$(I don't know how to prove this). But this doesn't look like a necessary condition. I think such a ""basic"" problem must be studied before, so every solution or partial solution(for specific types of A) or reference will be appreciated!","There is a well-known result saying that $\{e^{k\alpha \pi i}: k\in \mathbb Z \}$ is dense in $S^1$. By density, we can select an infinite subset $A$ of $\mathbb Z$ such that $\{e^{k\alpha \pi i}: k\in A \}$ is not dense in $S^1$ any more. So there is a natural question: For what kind of infinite subset A of $\mathbb Z$ and irrational number $\alpha$, is $\{e^{k\alpha \pi i}: k\in A \}$ dense in $S^1 $ ? I feel like if A contains a collection of an infinite Arithmetic sequence(like 1,3,5,...), then $\{e^{k\alpha \pi i}: k\in A \}$ should be dense in $S^1$(I don't know how to prove this). But this doesn't look like a necessary condition. I think such a ""basic"" problem must be studied before, so every solution or partial solution(for specific types of A) or reference will be appreciated!",,"['real-analysis', 'general-topology', 'number-theory', 'dynamical-systems']"
61,A random variable is symmetric if and only if its characteristic function is real-valued,A random variable is symmetric if and only if its characteristic function is real-valued,,"Quick summary: I am stuck on the implication: $\phi_X$ real-valued $\rightarrow$ $X$ symmetric. Assume you have a probability space $(\Omega, \mathcal{F},P)$, and a random varaiable $X: \Omega \rightarrow \mathbb{R^d}$, it is measurable with respect to $\mathcal{B}(\mathbb{R^d})$. We define the characteristic function $\phi_X: \mathbb{R^d}\rightarrow\mathbb{C}$. With $\phi_X(u)=\int_\Omega e^{i<u,X(\omega)>}dP=\int_{\mathbb{R^d}}e^{i<u,x>}d\,u_x$, the measure $u_x$ is the distribution function:$u_x(B)=P(X^{-1}(B)), B \in \mathcal{B}(\mathbb{R^d}))$. We say that X is symmetric iff $P(X\in B)=P(X\in-B)), B \in \mathcal{B}(\mathbb{R^d})$. Or equivalently $\mu_X(B)=\mu_X(-B)$. I am supposed to show that X is symmetric if and only if the characteristic function of X: $\phi_X$ is real valued. One part I seem to have managed: If we start with that X is symmetric, by then splitting $\mathbb{R^d}$ in all its hyperdimensional ""octants"" or ""quadrants"" and $\{0\}$, the result follows(but there is some work involved, but I think it is ok this way). The problem is the other way: If I start with the knowledge that the characteristic function is real valued, how should I then work to show that X is symmetric? I tried assumeing that there is one set B such that $P(X \in B)\ne P(X \in -B)$,  and then try to arrive at a contradiction: If I split $\mathbb{R}^d$ in: $\{0\}, L_1=(+,+,+..,+), L_2=(-,-,-,,-), L_3=(+,+,+,..,-)L_4=(-,-,-,..,+)$ etc. So that I get $\{0\}$ and $2^d$ $L_k$s. Where $L_{2i-1}=-L_{2i}$. Then I must have that $P(X \in B)\ne P(X \in -B)$, implies that there is atleast one i, such that $P(X \in B\cap L_{2i-1})\ne P(X \in -B\cap L_{2i})$ or $P(X \in B\cap L_{2i})\ne P(X \in -B\cap L_{2i-1})$, because if all these were equal, and since they are a partition of $\mathbb{R}^d$(with $\{0\}$) we would end up with: $P(X \in B)= P(X \in -B)$. But this is as far as I got, beacuse even if I have that: $P(X \in B\cap L_{2i-1})\ne P(X \in -B\cap L_{2i})$ I can't see how this would contradict the characteristic function beeing real valued. My only idea is to chose one u, such that we get a complex number when we integrate, my only idea is choosing u for instance 1 in those parts that is a + in $L_{2_i-1}$, but I can't really see this leading anywere, since 0 in a exponential funciton, doesnt lead to a 0 in the function itself. Any tips or hints or help on how to solve this?","Quick summary: I am stuck on the implication: $\phi_X$ real-valued $\rightarrow$ $X$ symmetric. Assume you have a probability space $(\Omega, \mathcal{F},P)$, and a random varaiable $X: \Omega \rightarrow \mathbb{R^d}$, it is measurable with respect to $\mathcal{B}(\mathbb{R^d})$. We define the characteristic function $\phi_X: \mathbb{R^d}\rightarrow\mathbb{C}$. With $\phi_X(u)=\int_\Omega e^{i<u,X(\omega)>}dP=\int_{\mathbb{R^d}}e^{i<u,x>}d\,u_x$, the measure $u_x$ is the distribution function:$u_x(B)=P(X^{-1}(B)), B \in \mathcal{B}(\mathbb{R^d}))$. We say that X is symmetric iff $P(X\in B)=P(X\in-B)), B \in \mathcal{B}(\mathbb{R^d})$. Or equivalently $\mu_X(B)=\mu_X(-B)$. I am supposed to show that X is symmetric if and only if the characteristic function of X: $\phi_X$ is real valued. One part I seem to have managed: If we start with that X is symmetric, by then splitting $\mathbb{R^d}$ in all its hyperdimensional ""octants"" or ""quadrants"" and $\{0\}$, the result follows(but there is some work involved, but I think it is ok this way). The problem is the other way: If I start with the knowledge that the characteristic function is real valued, how should I then work to show that X is symmetric? I tried assumeing that there is one set B such that $P(X \in B)\ne P(X \in -B)$,  and then try to arrive at a contradiction: If I split $\mathbb{R}^d$ in: $\{0\}, L_1=(+,+,+..,+), L_2=(-,-,-,,-), L_3=(+,+,+,..,-)L_4=(-,-,-,..,+)$ etc. So that I get $\{0\}$ and $2^d$ $L_k$s. Where $L_{2i-1}=-L_{2i}$. Then I must have that $P(X \in B)\ne P(X \in -B)$, implies that there is atleast one i, such that $P(X \in B\cap L_{2i-1})\ne P(X \in -B\cap L_{2i})$ or $P(X \in B\cap L_{2i})\ne P(X \in -B\cap L_{2i-1})$, because if all these were equal, and since they are a partition of $\mathbb{R}^d$(with $\{0\}$) we would end up with: $P(X \in B)= P(X \in -B)$. But this is as far as I got, beacuse even if I have that: $P(X \in B\cap L_{2i-1})\ne P(X \in -B\cap L_{2i})$ I can't see how this would contradict the characteristic function beeing real valued. My only idea is to chose one u, such that we get a complex number when we integrate, my only idea is choosing u for instance 1 in those parts that is a + in $L_{2_i-1}$, but I can't really see this leading anywere, since 0 in a exponential funciton, doesnt lead to a 0 in the function itself. Any tips or hints or help on how to solve this?",,"['real-analysis', 'probability-theory', 'characteristic-functions']"
62,"References about Iterating integration, $\int_{a_0}^{\int_{a_1}^\vdots I_1dx}I_0\,dx$","References about Iterating integration,","\int_{a_0}^{\int_{a_1}^\vdots I_1dx}I_0\,dx","Are there any references that discuss Iterating integration in general, $\int_{a_0}^{\int_{a_1}^\vdots I_1dx}I_0\,dx$, conditions in which they converge, some special values, some special tricks to compute them, for example $$\Large\int_0^{\displaystyle\int_0^{\displaystyle\int_0^{\vdots}(1-x)^3dx}(1-x)^2dx}(1-x)^1dx$$","Are there any references that discuss Iterating integration in general, $\int_{a_0}^{\int_{a_1}^\vdots I_1dx}I_0\,dx$, conditions in which they converge, some special values, some special tricks to compute them, for example $$\Large\int_0^{\displaystyle\int_0^{\displaystyle\int_0^{\vdots}(1-x)^3dx}(1-x)^2dx}(1-x)^1dx$$",,"['real-analysis', 'integration', 'reference-request']"
63,Convexity of matrix exponential,Convexity of matrix exponential,,"Consider the matrix-valued function $A: \mathbb{R}^n \rightarrow \mathbb{R}^{n \times n}$ defined as $$ A( x ) := \left[ \begin{matrix} x_1 & a_{1,2} & \cdots & a_{1,n} \\ a_{2,1} & x_2 & \cdots & a_{2,n} \\ \vdots & \vdots & \ddots & \vdots \\ a_{n,1} & a_{n,2} & \cdots & x_n \end{matrix} \right] $$ where $a_{i,j} \geq 0$ for all $i,j$ . Define the matrix-valued function $F: \mathbb{R}^{ n } \rightarrow \mathbb{R}^{ n \times n }$ as $$ F( x ) := \text{exp}( A(x) )$$ where $\text{exp}(\cdot)$ denotes the matrix exponential , i.e., $$\exp(A) := \sum_{k=0}^{\infty} \frac{A^k}{k!} = I + A + \frac{1}{2!} A^2 + \frac{1}{3! }A^3 + \cdots$$ Notice that $F$ is element-wise nonnegative, because it is the exponential of a Metzler matrix . Let $f_{i,j} : \mathbb{R}^{ n } \rightarrow \mathbb{R}$ be the $(i,j)$ -component of $F$ , $f_{ij}(x) = F(x)_{ij}$ . Prove that, for all $i,j$ , the function $f_{i,j}$ is convex . Comment: I am trying to show that the second derivative of $f_{i,j}$ is nonnegative. I am also trying to show that the second derivative of $\mathbb{R} \ni t \mapsto \exp( A( x + t y ) )$ is nonnegative for all $x,y \in \mathbb{R}^n$ .","Consider the matrix-valued function defined as where for all . Define the matrix-valued function as where denotes the matrix exponential , i.e., Notice that is element-wise nonnegative, because it is the exponential of a Metzler matrix . Let be the -component of , . Prove that, for all , the function is convex . Comment: I am trying to show that the second derivative of is nonnegative. I am also trying to show that the second derivative of is nonnegative for all .","A: \mathbb{R}^n \rightarrow \mathbb{R}^{n \times n}  A( x ) := \left[ \begin{matrix} x_1 & a_{1,2} & \cdots & a_{1,n} \\ a_{2,1} & x_2 & \cdots & a_{2,n} \\ \vdots & \vdots & \ddots & \vdots \\ a_{n,1} & a_{n,2} & \cdots & x_n \end{matrix} \right]  a_{i,j} \geq 0 i,j F: \mathbb{R}^{ n } \rightarrow \mathbb{R}^{ n \times n }  F( x ) := \text{exp}( A(x) ) \text{exp}(\cdot) \exp(A) := \sum_{k=0}^{\infty} \frac{A^k}{k!} = I + A + \frac{1}{2!} A^2 + \frac{1}{3! }A^3 + \cdots F f_{i,j} : \mathbb{R}^{ n } \rightarrow \mathbb{R} (i,j) F f_{ij}(x) = F(x)_{ij} i,j f_{i,j} f_{i,j} \mathbb{R} \ni t \mapsto \exp( A( x + t y ) ) x,y \in \mathbb{R}^n","['real-analysis', 'linear-algebra', 'matrices', 'convex-analysis', 'matrix-calculus']"
64,Understanding the Legendre transform,Understanding the Legendre transform,,"In physics, I've seen the Legendre transform motivated by ""changing the variable $x$ of a function $x \mapsto f(x)$ to the variable $u = \frac{df}{dx}$ ."" I don't quite see what that means and why the Legendre transform is the answer to this heuristic. I'd understand it as follows: Let $f: \mathbb{R} \to\mathbb{R}$ be strictly convex and differentiable. Then for every $x_0 \in \mathbb{R}$ the slope $\frac{df(x_0)}{dx}$ is unique. We want to find a function $f^*: f'(\mathbb{R}) \to \mathbb{R}$ such that $f(x_0)=f^*(\frac{df(x_0)}{dx})$ for every $x_0 \in \mathbb{R}$ . In fact we can view the function $f^*$ as a function on a subset of the dual space $\mathbb{R^*}$ such that $f^*(df(x_0))=f(x_0)$ . However, this doesn't seem to capture the Legendre transform, for example by the above the Legendre transform of the exponential function should be the identity function. How can the physicists heuristic be made precise?","In physics, I've seen the Legendre transform motivated by ""changing the variable of a function to the variable ."" I don't quite see what that means and why the Legendre transform is the answer to this heuristic. I'd understand it as follows: Let be strictly convex and differentiable. Then for every the slope is unique. We want to find a function such that for every . In fact we can view the function as a function on a subset of the dual space such that . However, this doesn't seem to capture the Legendre transform, for example by the above the Legendre transform of the exponential function should be the identity function. How can the physicists heuristic be made precise?",x x \mapsto f(x) u = \frac{df}{dx} f: \mathbb{R} \to\mathbb{R} x_0 \in \mathbb{R} \frac{df(x_0)}{dx} f^*: f'(\mathbb{R}) \to \mathbb{R} f(x_0)=f^*(\frac{df(x_0)}{dx}) x_0 \in \mathbb{R} f^* \mathbb{R^*} f^*(df(x_0))=f(x_0),"['real-analysis', 'functions']"
65,A generalization of the product of harmonic numbers to non-integer arguments,A generalization of the product of harmonic numbers to non-integer arguments,,"This question is somewhat related to one of my previous questions: Fibonorial of a fractional or complex argument . Recall the definition of harmonic numbers: $$H_n=\sum_{k=1}^n\frac1k=1+\frac12+\,...\,+\frac1n\tag1$$ Obviously, harmonic numbers satisfy the following functional equation: $$H_n-H_{n-1}=\frac1n\tag2$$ The definition $(1)$ is valid only for $n\in\mathbb N$, but it can be generalized to all positive indices. There are several equivalent ways to do this: $$H_a=\sum_{k=1}^\infty\left(\frac1k-\frac1{k+a}\right)=\int_0^1\frac{1-x^a}{1-x}\,dx=\frac{\Gamma'(a+1)}{\Gamma(a+1)}+\gamma\tag3$$ This generalized definition gives a real-analytic function (that can be extended to a complex-analytic if needed) and still satisfies the functional equation $(2)$ even for non-integer values of $a$. Now, consider the product of harmonic numbers: $$P_n=\prod_{k=1}^nH_k=H_1\,H_2\,...H_n=1\times\left(1+\frac12\right)\times\,...\times\left(1+\frac12+\,...+\frac1n\right)\tag4$$ The numerators and denominators of the terms of this sequence appear as A097423 and A097424 in the OEIS. Obviously, the following function equations hold: $$\frac{P_n}{P_{n-1}}=H_n,\quad\quad\frac{P_n}{P_{n-1}}-\frac{P_{n-1}}{P_{n-2}}=\frac1n\tag5$$ I'm looking for a continuous generalization $P_a$ of the discrete sequence $P_n$, which is real-analytic for all $a>0$ and satisfies the functional equations $(5)$. Could you suggest a way to construct such a function? Is there a series or integral representation for it? Can we generalize it to complex arguments? Update: It seems we can use the same trick that is used to define $\Gamma$-function using a limit involving factorials of integers: $$P_a=\lim_{n\to\infty}\left[\left(H_n\right)^a\cdot\prod_{k=1}^n\frac{H_k}{H_{a+k}}\right]=\frac1{H_{a+1}}\cdot\prod_{n=1}^\infty\frac{\left(H_{n+1}\right)^{a+1}}{\left(H_n\right)^a\,H_{a+n+1}}\tag6$$","This question is somewhat related to one of my previous questions: Fibonorial of a fractional or complex argument . Recall the definition of harmonic numbers: $$H_n=\sum_{k=1}^n\frac1k=1+\frac12+\,...\,+\frac1n\tag1$$ Obviously, harmonic numbers satisfy the following functional equation: $$H_n-H_{n-1}=\frac1n\tag2$$ The definition $(1)$ is valid only for $n\in\mathbb N$, but it can be generalized to all positive indices. There are several equivalent ways to do this: $$H_a=\sum_{k=1}^\infty\left(\frac1k-\frac1{k+a}\right)=\int_0^1\frac{1-x^a}{1-x}\,dx=\frac{\Gamma'(a+1)}{\Gamma(a+1)}+\gamma\tag3$$ This generalized definition gives a real-analytic function (that can be extended to a complex-analytic if needed) and still satisfies the functional equation $(2)$ even for non-integer values of $a$. Now, consider the product of harmonic numbers: $$P_n=\prod_{k=1}^nH_k=H_1\,H_2\,...H_n=1\times\left(1+\frac12\right)\times\,...\times\left(1+\frac12+\,...+\frac1n\right)\tag4$$ The numerators and denominators of the terms of this sequence appear as A097423 and A097424 in the OEIS. Obviously, the following function equations hold: $$\frac{P_n}{P_{n-1}}=H_n,\quad\quad\frac{P_n}{P_{n-1}}-\frac{P_{n-1}}{P_{n-2}}=\frac1n\tag5$$ I'm looking for a continuous generalization $P_a$ of the discrete sequence $P_n$, which is real-analytic for all $a>0$ and satisfies the functional equations $(5)$. Could you suggest a way to construct such a function? Is there a series or integral representation for it? Can we generalize it to complex arguments? Update: It seems we can use the same trick that is used to define $\Gamma$-function using a limit involving factorials of integers: $$P_a=\lim_{n\to\infty}\left[\left(H_n\right)^a\cdot\prod_{k=1}^n\frac{H_k}{H_{a+k}}\right]=\frac1{H_{a+1}}\cdot\prod_{n=1}^\infty\frac{\left(H_{n+1}\right)^{a+1}}{\left(H_n\right)^a\,H_{a+n+1}}\tag6$$",,"['real-analysis', 'number-theory', 'gamma-function', 'harmonic-numbers', 'oeis']"
66,"Prove that $f(x, y) \le 3 $ for $x \ge 0, y > 0$",Prove that  for,"f(x, y) \le 3  x \ge 0, y > 0","Let $x \ge 0, y>0$ and \begin{align*} f(x,y)&=\sqrt{\dfrac{y}{y+x^2}}+4\sqrt{\dfrac{y}{(y+(x+1)^2)(y+(x+3)^2)}}\\[6pt] &\qquad +4\sqrt{\dfrac{y}{(y+(x-1)^2)(y+(x-3)^2)}}. \end{align*} Prove that $f(x,y) \le 3$ . I can prove when $x\ge 2, f(x,y) < 3$ , but $f(x,y)=3$ when $x=0,y=3$ , so the key part is $x\le 2 $ I found when $x<2, f'_x(y \ge 1.5) <0$ but I can't prove it as it has  high degree equations. And for $x<2,y<1.5$ , I have no idea how to prove $f(x,y)<3$","Let and Prove that . I can prove when , but when , so the key part is I found when but I can't prove it as it has  high degree equations. And for , I have no idea how to prove","x \ge 0, y>0 \begin{align*}
f(x,y)&=\sqrt{\dfrac{y}{y+x^2}}+4\sqrt{\dfrac{y}{(y+(x+1)^2)(y+(x+3)^2)}}\\[6pt]
&\qquad +4\sqrt{\dfrac{y}{(y+(x-1)^2)(y+(x-3)^2)}}.
\end{align*} f(x,y) \le 3 x\ge 2, f(x,y) < 3 f(x,y)=3 x=0,y=3 x\le 2  x<2, f'_x(y \ge 1.5) <0 x<2,y<1.5 f(x,y)<3","['real-analysis', 'inequality', 'radicals', 'cauchy-schwarz-inequality']"
67,Does $\sum_n |\sin n|^{cn^2}$ converge?,Does  converge?,\sum_n |\sin n|^{cn^2},"So I recently asked a question about convergence of $\sum_n |\sin n|^{cn}$ for arbitrary $c > 0$ and it turns out that the terms of the series don't even converge, for any $c > 0$, so the series is always divergent. But what about  $\sum_n |\sin n|^{cn^2}$ for $c > 0$? Are there $c$ so that the series converges, and if there are $c > 0$ such that the series diverges, do the terms of the series still converge? If that's too hard, what if we replace $n^2$ in the exponent by $n^\alpha$ for some different $\alpha > 1$? Which $\alpha$ do we know the answer for?","So I recently asked a question about convergence of $\sum_n |\sin n|^{cn}$ for arbitrary $c > 0$ and it turns out that the terms of the series don't even converge, for any $c > 0$, so the series is always divergent. But what about  $\sum_n |\sin n|^{cn^2}$ for $c > 0$? Are there $c$ so that the series converges, and if there are $c > 0$ such that the series diverges, do the terms of the series still converge? If that's too hard, what if we replace $n^2$ in the exponent by $n^\alpha$ for some different $\alpha > 1$? Which $\alpha$ do we know the answer for?",,"['real-analysis', 'sequences-and-series', 'convergence-divergence']"
68,Other approaches to $\int_{0}^{1} \frac{K\left ( x \right ) }{\sqrt{3-x} } \text{d}x$,Other approaches to,\int_{0}^{1} \frac{K\left ( x \right ) }{\sqrt{3-x} } \text{d}x,"Let $K(x) = \int_0^{\pi/2}\frac{1}{\sqrt{1-x^2 \sin^2 \theta}}d\theta$ be the complete elliptic integral of first kind. It could be shown that $$ \int_{0}^{1} \frac{K\left ( x \right ) }{\sqrt{3-x} } \text{d}x=\frac{\Gamma\left ( \frac{1}{24}  \right )  \Gamma\left ( \frac{5}{24}  \right)\Gamma\left ( \frac{7}{24}  \right ) \Gamma\left (  \frac{11}{24}\right )  }{96\pi\sqrt{3}} $$ thanks to the known results here . With its simple appearance, I wonder whether   a brief approach exists so that we can understand them better. Appreciate your creative efforts. Applying the well-known quadratic transformation formula $$ K\left ( \frac{1-x}{1+x}  \right )  =\frac{1+x}{2} K^\prime(x),K^\prime(x):=K\left ( \sqrt{1-x^2}  \right ) $$ we get $$ \int_{0}^{1} \frac{K(x)}{\sqrt{3-x} }\text{d}x =\frac{1}{\sqrt{2} } \int_{0}^{1} \frac{K^\prime(x)}{\sqrt{\left ( 1+x\right )\left ( 1+2x \right )  } } \text{d} x. $$ If using the expansion $$ \frac{1}{\sqrt{\left ( 1+x\right )\left ( 1+2x \right )  } } =\sum_{n\ge0} a_n x^n $$ and $$ \int_{0}^{1}x^n K^\prime(x)\text{d}x =\frac\pi4\frac{\Gamma\left ( \frac{n+1}{2}  \right )^2 }{ \Gamma\left ( \frac{n+2}{2}  \right )^2},$$ we could obtain a series representation for the integral. This works more on the case mentioned by @MiracleInvoker, because the series could split into two hypergeometric series, both of which are normally evaluated.","Let be the complete elliptic integral of first kind. It could be shown that thanks to the known results here . With its simple appearance, I wonder whether   a brief approach exists so that we can understand them better. Appreciate your creative efforts. Applying the well-known quadratic transformation formula we get If using the expansion and we could obtain a series representation for the integral. This works more on the case mentioned by @MiracleInvoker, because the series could split into two hypergeometric series, both of which are normally evaluated.","K(x) = \int_0^{\pi/2}\frac{1}{\sqrt{1-x^2 \sin^2 \theta}}d\theta 
\int_{0}^{1} \frac{K\left ( x \right ) }{\sqrt{3-x} } \text{d}x=\frac{\Gamma\left ( \frac{1}{24}  \right ) 
\Gamma\left ( \frac{5}{24}  \right)\Gamma\left ( \frac{7}{24}  \right ) \Gamma\left (  \frac{11}{24}\right )  }{96\pi\sqrt{3}}
 
K\left ( \frac{1-x}{1+x}  \right ) 
=\frac{1+x}{2} K^\prime(x),K^\prime(x):=K\left ( \sqrt{1-x^2}  \right )
 
\int_{0}^{1} \frac{K(x)}{\sqrt{3-x} }\text{d}x
=\frac{1}{\sqrt{2} } \int_{0}^{1} \frac{K^\prime(x)}{\sqrt{\left ( 1+x\right )\left ( 1+2x \right )  } }
\text{d} x.
 
\frac{1}{\sqrt{\left ( 1+x\right )\left ( 1+2x \right )  } }
=\sum_{n\ge0} a_n x^n
  \int_{0}^{1}x^n K^\prime(x)\text{d}x
=\frac\pi4\frac{\Gamma\left ( \frac{n+1}{2}  \right )^2 }{
\Gamma\left ( \frac{n+2}{2}  \right )^2},","['real-analysis', 'integration', 'alternative-proof', 'elliptic-integrals']"
69,Does there exist $f:\Bbb{R}\to \Bbb{R}$ additive onto function such that $f(F) \subset \Bbb{R}$ has the property of Baire for every $F$?,Does there exist  additive onto function such that  has the property of Baire for every ?,f:\Bbb{R}\to \Bbb{R} f(F) \subset \Bbb{R} F,"Let $F\subset\Bbb{R} $ intersect every uncountable $\mathcal{F}_{\sigma}$ set. $B\subset \Bbb{R}$ is said to have the property of Baire if $B=U\triangle M$ where $U$ is open and $M$ is meager. Does there exist $f:\Bbb{R}\to \Bbb{R}$ additive onto function such that $f(F) \subset \Bbb{R}$ has the property of Baire for every $F$ defined above ? Edit $1$ : $F$ intersects every uncountable $F_{\sigma}$ set iff F intersects every uncountable closed set. Edit $2$ : If $f:\Bbb{R}\to \Bbb{R}$ is additive i.e $f(x+y) =f(x) +f(y) $ then $f$ is $\Bbb{Q}$ -linear.In other words $f$ is a linear function if we consider $\Bbb{R}$ as a vector space of $\Bbb{Q}$ . $f(x+y) =f(x) +f(y) $ $f(qx) =qf(x) $ $\forall x, y\in\Bbb{R} $ and $\forall q\in\Bbb{Q}$ $\underline{ \text{Case }  1}$ : $( f \text{ is } \Bbb{R} \text{ linear}) $ Then $f(x) =ax$ for some $a\in \Bbb{R}$ . Hence clearly $f$ is additive onto map $($ and moreover $f$ is a linear homeomorphism / topological isomorphism $)$ . But $f$ fails to hold the third property mentioned above. For an example ,if we  take $F=\mathcal{B}( \text{Bernstein set})$ then $f(F) =a\mathcal{B}$ doesn't have the property of Baire. $\boxed{\text{ Required function can't be $\Bbb{R}$ -linear}}$ $\underline{\text{Case }2} $ : $f$ is $\mathbb{Q}$ -linear but not $\Bbb{R}$ -linear. Points to be considered: If $g:\Bbb{R}\to \Bbb{R}$ defined by $g(x) =qx , q\in\Bbb{Q}$ is continuous then $g=f$ on $\Bbb{Q}$ implies $g=f $ on $\Bbb{R} $ i.e any continuous $\Bbb{Q}$ -linear map extends linearly on $\Bbb{R}$ . $\boxed{ \text{ So $g$ can't be a continuous on $\Bbb{Q}$}}$ A linear map is completely determined by it's action on the basis. So our task is reduced to construct a discontinuous linear map from the vector space $\Bbb{R}_{\Bbb{Q}}$ to $\Bbb{R}$ . A non-continuous solution of an additive function ( called ugly function) is non-measurable. Conjecture $1$ : $f:\Bbb{R}\to \Bbb{R} $ is $\Bbb{Q}$ -linear and non-measurable function. Then $\exists F\subset \Bbb{R}$ closed uncountable set such that $f(F) $ doesn't have the property of Baire. There exists a discontinuous additive function $f:\Bbb{R}\to\Bbb{R}$ satisfying Darboux property ( It can be shown by defining $f$ linealy on a hamel basis of $\Bbb{R}_{\Bbb{Q}}$ and then extending additively on $\Bbb{R}$ ) Conjecture $2$ : The Darboux property of $f$ ( discontinuous $\Bbb{Q}$ -linear function) is sufficient to conclude that $f(F) $ have property of Baire for every closed uncountable set $F$ . Any second category (non meager) subset of $\Bbb{R}$ contains a set that fails to have the property of Baire. $[$ Suppose $ A\subset \Bbb{R}$ non meager. Then $A\cap \mathcal{B} $ or $A\cap \mathcal{B}^c$ atleast one of the set doesn't have the B.P otherwise both would be meager and eventually $A$ would be meager $]$ $f:\Bbb{R}\to \Bbb{R}$ is an additive onto function such that $f$ is not injective then $f^{-1}(y)$ is dense in $\Bbb{R}$ for all $y\in\Bbb{R}$ . Question: Does there exist $f:\Bbb{R}\to \Bbb{R}$ additive onto function such that $f(F) \subset \Bbb{R}$ has the property of Baire for every $F$ defined above ? Here is the MO post of this question.","Let intersect every uncountable set. is said to have the property of Baire if where is open and is meager. Does there exist additive onto function such that has the property of Baire for every defined above ? Edit : intersects every uncountable set iff F intersects every uncountable closed set. Edit : If is additive i.e then is -linear.In other words is a linear function if we consider as a vector space of . and : Then for some . Hence clearly is additive onto map and moreover is a linear homeomorphism / topological isomorphism . But fails to hold the third property mentioned above. For an example ,if we  take then doesn't have the property of Baire. : is -linear but not -linear. Points to be considered: If defined by is continuous then on implies on i.e any continuous -linear map extends linearly on . A linear map is completely determined by it's action on the basis. So our task is reduced to construct a discontinuous linear map from the vector space to . A non-continuous solution of an additive function ( called ugly function) is non-measurable. Conjecture : is -linear and non-measurable function. Then closed uncountable set such that doesn't have the property of Baire. There exists a discontinuous additive function satisfying Darboux property ( It can be shown by defining linealy on a hamel basis of and then extending additively on ) Conjecture : The Darboux property of ( discontinuous -linear function) is sufficient to conclude that have property of Baire for every closed uncountable set . Any second category (non meager) subset of contains a set that fails to have the property of Baire. Suppose non meager. Then or atleast one of the set doesn't have the B.P otherwise both would be meager and eventually would be meager is an additive onto function such that is not injective then is dense in for all . Question: Does there exist additive onto function such that has the property of Baire for every defined above ? Here is the MO post of this question.","F\subset\Bbb{R}  \mathcal{F}_{\sigma} B\subset \Bbb{R} B=U\triangle M U M f:\Bbb{R}\to \Bbb{R} f(F) \subset \Bbb{R} F 1 F F_{\sigma} 2 f:\Bbb{R}\to \Bbb{R} f(x+y) =f(x) +f(y)  f \Bbb{Q} f \Bbb{R} \Bbb{Q} f(x+y) =f(x) +f(y)  f(qx) =qf(x)  \forall x, y\in\Bbb{R}  \forall q\in\Bbb{Q} \underline{ \text{Case }  1} ( f \text{ is } \Bbb{R} \text{ linear})  f(x) =ax a\in \Bbb{R} f ( f ) f F=\mathcal{B}( \text{Bernstein set}) f(F) =a\mathcal{B} \boxed{\text{ Required function can't be \Bbb{R} -linear}} \underline{\text{Case }2}  f \mathbb{Q} \Bbb{R} g:\Bbb{R}\to \Bbb{R} g(x) =qx , q\in\Bbb{Q} g=f \Bbb{Q} g=f  \Bbb{R}  \Bbb{Q} \Bbb{R} \boxed{ \text{ So g can't be a continuous on \Bbb{Q}}} \Bbb{R}_{\Bbb{Q}} \Bbb{R} 1 f:\Bbb{R}\to \Bbb{R}  \Bbb{Q} \exists F\subset \Bbb{R} f(F)  f:\Bbb{R}\to\Bbb{R} f \Bbb{R}_{\Bbb{Q}} \Bbb{R} 2 f \Bbb{Q} f(F)  F \Bbb{R} [  A\subset \Bbb{R} A\cap \mathcal{B}  A\cap \mathcal{B}^c A ] f:\Bbb{R}\to \Bbb{R} f f^{-1}(y) \Bbb{R} y\in\Bbb{R} f:\Bbb{R}\to \Bbb{R} f(F) \subset \Bbb{R} F","['real-analysis', 'general-topology', 'functional-analysis', 'axiom-of-choice', 'descriptive-set-theory']"
70,Symbolic approximation through integration by parts,Symbolic approximation through integration by parts,,"This is a slightly soft question. Suppose I have an integral $f(x) =\int_a^x g(t) dt $ which cannot be expressed in terms of elementary functions. One might still be able to integrate by parts to get something like: $$ f(x) = h_1(t) |_{a}^{x} - \int_a^x g_1(x)$$ by repeating this process, you could end up with: $$f(x) =  \left( \sum_{k=0}^n h_k(t)|_a^x \right) + \int_a^x g_n(x) $$ and if you can bound $\int_a^x g_n(x)$ by something small then you have an approximation for the integral, $f$ , in terms of elementary functions. My questions are: Are there any instances in which something like this turns out to be a successful strategy? Could someone direct me toward a book or resource about symbolically approximating integrals? By symbolic approximation I mean finding a good approximation to an integral $f(x) = \int_a^x g(t) dt $ of an elementary function in terms of elementary functions. When I google this, I only get results about numerically approximating integrals.","This is a slightly soft question. Suppose I have an integral which cannot be expressed in terms of elementary functions. One might still be able to integrate by parts to get something like: by repeating this process, you could end up with: and if you can bound by something small then you have an approximation for the integral, , in terms of elementary functions. My questions are: Are there any instances in which something like this turns out to be a successful strategy? Could someone direct me toward a book or resource about symbolically approximating integrals? By symbolic approximation I mean finding a good approximation to an integral of an elementary function in terms of elementary functions. When I google this, I only get results about numerically approximating integrals.",f(x) =\int_a^x g(t) dt   f(x) = h_1(t) |_{a}^{x} - \int_a^x g_1(x) f(x) =  \left( \sum_{k=0}^n h_k(t)|_a^x \right) + \int_a^x g_n(x)  \int_a^x g_n(x) f f(x) = \int_a^x g(t) dt ,"['real-analysis', 'calculus', 'indefinite-integrals', 'approximation', 'symbolic-computation']"
71,properties that real numbers hold but complex numbers does not,properties that real numbers hold but complex numbers does not,,"I need to find a few examples about the differences between real numbers and complex numbers like: 1) if $x \in \mathbb R $ then $x^2 \geq0$ is true if $z \in \mathbb C $ then $z^2 \geq0$ is false 2) let $a \in \mathbb R/\{0, 1\} $ if $a^x =a^y$ then $x=y$ is true let $a\in \mathbb z/\{0, 1\} \in \mathbb C $ if $a^x =a^y$ then $x=y$ is false But these examples are not cool enough and feel very trivial.  Can you suggest some other properties like these? Thanks.",I need to find a few examples about the differences between real numbers and complex numbers like: 1) if then is true if then is false 2) let if then is true let if then is false But these examples are not cool enough and feel very trivial.  Can you suggest some other properties like these? Thanks.,"x \in \mathbb R  x^2 \geq0 z \in \mathbb C  z^2 \geq0 a \in \mathbb R/\{0, 1\}  a^x =a^y x=y a\in \mathbb z/\{0, 1\} \in \mathbb C  a^x =a^y x=y","['real-analysis', 'complex-numbers', 'real-numbers']"
72,Is the determinant differentiable?,Is the determinant differentiable?,,"I was wondering, given an $n \times n$ square matrix, let function $\det : \left(a_1,a_2,\ldots,a_{n^2}\right) \to \textbf{R}$ give the determinant, where $a_{k}$ 's are the entries of the $n \times n$ matrix. Is this function (determinant) a differentiable kind? If so, is the derivative continuous? That is, is $d\left(\det\right)$ a continuous function? Furthermore, if so, to what differentiability class does this $\det$ function belong? Thanks in advance.","I was wondering, given an square matrix, let function give the determinant, where 's are the entries of the matrix. Is this function (determinant) a differentiable kind? If so, is the derivative continuous? That is, is a continuous function? Furthermore, if so, to what differentiability class does this function belong? Thanks in advance.","n \times n \det : \left(a_1,a_2,\ldots,a_{n^2}\right) \to \textbf{R} a_{k} n \times n d\left(\det\right) \det","['real-analysis', 'matrices', 'derivatives', 'determinant', 'matrix-calculus']"
73,Examples of closed sets with empty interior,Examples of closed sets with empty interior,,"Could I please have an example of closed sets with empty interior? Any topological space. Everything goes. Remark:  This is not homework.  I'm in the middle of proving the space of $n$-degree polynomials in $(C[0,1], \lVert \cdot \rVert_\infty)$  is meager. Empty interior of this set (if I'm at all correct...) is the punchline and I just want to understand a bit more the nature of closed sets with empty interior.","Could I please have an example of closed sets with empty interior? Any topological space. Everything goes. Remark:  This is not homework.  I'm in the middle of proving the space of $n$-degree polynomials in $(C[0,1], \lVert \cdot \rVert_\infty)$  is meager. Empty interior of this set (if I'm at all correct...) is the punchline and I just want to understand a bit more the nature of closed sets with empty interior.",,"['real-analysis', 'general-topology', 'examples-counterexamples', 'baire-category']"
74,$1/4$ is in the Cantor set?,is in the Cantor set?,1/4,"I would like to know if $1/4$ is in the Cantor set, I tried a lot without success, I need some hint or help how to proceed in this case. Maybe there is some tool or trick I can use. Thanks a lot","I would like to know if $1/4$ is in the Cantor set, I tried a lot without success, I need some hint or help how to proceed in this case. Maybe there is some tool or trick I can use. Thanks a lot",,"['real-analysis', 'cantor-set']"
75,What is the purpose of showing some numbers exist?,What is the purpose of showing some numbers exist?,,For example in my Analysis class the professor showed $\sqrt{2}$ exists using Archimedean properties of $\mathbb{R}$ and we showed $e$ exists. I want to know why it's important to show their existence?,For example in my Analysis class the professor showed $\sqrt{2}$ exists using Archimedean properties of $\mathbb{R}$ and we showed $e$ exists. I want to know why it's important to show their existence?,,"['real-analysis', 'soft-question', 'real-numbers', 'motivation']"
76,"Idea behind ""reparameterization hiding a corner"" in single variable calculus","Idea behind ""reparameterization hiding a corner"" in single variable calculus",,"I just solved question #2 on p. 248 from Spivak's Calculus Fourth Edition (2008). Solving it wasn't the issue. I'm trying to understand the idea behind it. This is a screenshot of the question: I'm trying to understand what is meant by ""reparameterizing hides a corner"". What does the author mean by ""hide"" in what sense is it being hidden? For reference, the function that this is being applied to is the following: $$ f(x) =  \left\{        \begin{array}\         x^{2},\ x \geq 0 \\        -x^{2},\ x \leq 0 \\       \end{array}  \right.$$ EDIT: Image of fig 21 as requested","I just solved question #2 on p. 248 from Spivak's Calculus Fourth Edition (2008). Solving it wasn't the issue. I'm trying to understand the idea behind it. This is a screenshot of the question: I'm trying to understand what is meant by ""reparameterizing hides a corner"". What does the author mean by ""hide"" in what sense is it being hidden? For reference, the function that this is being applied to is the following: EDIT: Image of fig 21 as requested"," f(x) =  \left\{
       \begin{array}\
        x^{2},\ x \geq 0 \\
       -x^{2},\ x \leq 0 \\
      \end{array}  \right.","['real-analysis', 'calculus', 'parametrization']"
77,Problem with definition of limit (why not big delta?),Problem with definition of limit (why not big delta?),,"I've been thinking about this for a little bit and I just can't shake my issue. So I'm sure we all know the definition but I'll just write it here: $$ \forall \epsilon > 0, \exists \delta > 0: \forall x \in D \; \text{that satisfy} \; 0 < \vert{x-c}\vert < \delta \; \text{the inequality} \; \vert f(x) - L\vert < \epsilon \; \text{holds}. $$ Now whenever people solve limits using the definition, they always follow a ""you give me an $\epsilon$ neighbourhood around L and I'll give you a $\delta$ neighbourhood around c that snugly fits around the pre-image of the $\epsilon$ neigbourhood."" My question is why does the $\delta$ neighbourhood necessarily fit snugly around the pre-image of the $\epsilon$ neighbourhood? When I think intuitively about limits, what I'd like the definition to be is something like this: ""As I take an increasingly smaller $\epsilon$ neighbourhood around L, if I can find an increasingly smaller $\delta$ neighbourhood around c that contains the preimage of the $\epsilon$ neighbourhood, then $\lim_{x \to c} \ f(x) = L$."" But I don't see that in the standard definition because why can't I take a $\delta$ neighbourhood that is arbitrarily large? For example if I'm considering $$\lim_{x \to 2} 2x$$ Why don't I just set $\delta$ = 1,000,000 or something big if $\epsilon$ = 1 ? and If episolon is two million then I set delta to a billion or whatever? If I set delta arbitrarily large wouldn't I still be satisfying $0 < \vert{x-c}\vert < \delta \; \text{such that the inequality} \; \vert f(x) - L\vert < \epsilon \; \text{holds} $? I just can't figure it out! Thank you!","I've been thinking about this for a little bit and I just can't shake my issue. So I'm sure we all know the definition but I'll just write it here: $$ \forall \epsilon > 0, \exists \delta > 0: \forall x \in D \; \text{that satisfy} \; 0 < \vert{x-c}\vert < \delta \; \text{the inequality} \; \vert f(x) - L\vert < \epsilon \; \text{holds}. $$ Now whenever people solve limits using the definition, they always follow a ""you give me an $\epsilon$ neighbourhood around L and I'll give you a $\delta$ neighbourhood around c that snugly fits around the pre-image of the $\epsilon$ neigbourhood."" My question is why does the $\delta$ neighbourhood necessarily fit snugly around the pre-image of the $\epsilon$ neighbourhood? When I think intuitively about limits, what I'd like the definition to be is something like this: ""As I take an increasingly smaller $\epsilon$ neighbourhood around L, if I can find an increasingly smaller $\delta$ neighbourhood around c that contains the preimage of the $\epsilon$ neighbourhood, then $\lim_{x \to c} \ f(x) = L$."" But I don't see that in the standard definition because why can't I take a $\delta$ neighbourhood that is arbitrarily large? For example if I'm considering $$\lim_{x \to 2} 2x$$ Why don't I just set $\delta$ = 1,000,000 or something big if $\epsilon$ = 1 ? and If episolon is two million then I set delta to a billion or whatever? If I set delta arbitrarily large wouldn't I still be satisfying $0 < \vert{x-c}\vert < \delta \; \text{such that the inequality} \; \vert f(x) - L\vert < \epsilon \; \text{holds} $? I just can't figure it out! Thank you!",,"['real-analysis', 'limits']"
78,Evaluating $\int{ \frac{x^n}{1 + x + \frac{x^2}{2} + \cdots + \frac{x^n}{n!}}}dx$ using Pascal inversion,Evaluating  using Pascal inversion,\int{ \frac{x^n}{1 + x + \frac{x^2}{2} + \cdots + \frac{x^n}{n!}}}dx,"(Note: I apreciate very much who marked this as a duplicate but I would like an answer for why my proof is wrong) This is my solution, I have no clue why it failed. Let's start: define $$I_n(m) = \int_{0}^{x} \frac{t^m}{1 + t + t^2/2 + ... + t^n/n!}\ dt$$ so it should be true that $$\sum_{m=0}^{n}\frac{I_n(m)}{m!} = x$$ Then I use Pascal inversion: $$\sum_{m=0}^n \frac{n! I_n(m)}{m!} = n!x$$ $$\sum_{m=0}^n {n\choose m} B_n(m) = n!x$$ where $B_n(m) = (n-m)!I_n(m)$ by Pascal's formula: $$I_n(n) = (-1)^nxn! \sum_{m=0}^{n} \frac{(-1)^m}{(n-m)!}$$ what did I do wrong ????","(Note: I apreciate very much who marked this as a duplicate but I would like an answer for why my proof is wrong) This is my solution, I have no clue why it failed. Let's start: define $$I_n(m) = \int_{0}^{x} \frac{t^m}{1 + t + t^2/2 + ... + t^n/n!}\ dt$$ so it should be true that $$\sum_{m=0}^{n}\frac{I_n(m)}{m!} = x$$ Then I use Pascal inversion: $$\sum_{m=0}^n \frac{n! I_n(m)}{m!} = n!x$$ $$\sum_{m=0}^n {n\choose m} B_n(m) = n!x$$ where $B_n(m) = (n-m)!I_n(m)$ by Pascal's formula: $$I_n(n) = (-1)^nxn! \sum_{m=0}^{n} \frac{(-1)^m}{(n-m)!}$$ what did I do wrong ????",,"['calculus', 'real-analysis', 'integration', 'indefinite-integrals', 'closed-form']"
79,Limit of the nested radical $\sqrt{7+\sqrt{7+\sqrt{7+\cdots}}}$ [duplicate],Limit of the nested radical  [duplicate],\sqrt{7+\sqrt{7+\sqrt{7+\cdots}}},"This question already has answers here : Closed 11 years ago . Possible Duplicate: On the sequence $x_{n+1} = \sqrt{c+x_n}$ Where does this sequence converge? $\sqrt{7},\sqrt{7+\sqrt{7}},\sqrt{7+\sqrt{7+\sqrt{7}}}$,...","This question already has answers here : Closed 11 years ago . Possible Duplicate: On the sequence $x_{n+1} = \sqrt{c+x_n}$ Where does this sequence converge? $\sqrt{7},\sqrt{7+\sqrt{7}},\sqrt{7+\sqrt{7+\sqrt{7}}}$,...",,"['real-analysis', 'sequences-and-series', 'nested-radicals']"
80,A closed form for the infinite series $\sum_{n=1}^\infty (-1)^{n+1}\arctan \left( \frac 1 n \right)$,A closed form for the infinite series,\sum_{n=1}^\infty (-1)^{n+1}\arctan \left( \frac 1 n \right),"It is known that $$\sum_{n=1}^{\infty} \arctan \left(\frac{1}{n^{2}} \right) = \frac{\pi}{4}-\tan^{-1}\left(\frac{\tanh(\frac{\pi}{\sqrt{2}})}{\tan(\frac{\pi}{\sqrt{2}})}\right). $$ Can we also find a closed form for the value of  $$\sum_{n=1}^{\infty} (-1)^{n+1} \arctan \left(\frac{1}{n} \right)? $$ Unlike the other infinite series, this infinite series only converges conditionally.","It is known that $$\sum_{n=1}^{\infty} \arctan \left(\frac{1}{n^{2}} \right) = \frac{\pi}{4}-\tan^{-1}\left(\frac{\tanh(\frac{\pi}{\sqrt{2}})}{\tan(\frac{\pi}{\sqrt{2}})}\right). $$ Can we also find a closed form for the value of  $$\sum_{n=1}^{\infty} (-1)^{n+1} \arctan \left(\frac{1}{n} \right)? $$ Unlike the other infinite series, this infinite series only converges conditionally.",,"['real-analysis', 'sequences-and-series', 'complex-analysis']"
81,Does the epsilon-delta definition of limits truly capture our intuitive understanding of limits?,Does the epsilon-delta definition of limits truly capture our intuitive understanding of limits?,,"I've been delving into the concept of limits and the Epsilon-Delta definition. The most basic definition, as I understand it, states that for every real number $\epsilon \gt 0$ , there exists a real number $\delta \gt 0$ such that if $0 \lt |x - a| \lt \delta$ then $|f(x) - L| \lt \epsilon$ , where $a$ is the limit point and $L$ is the limit of the function $f$ at $a$ . While I grasp the formal definition, I'm grappling with the philosophical aspect of it. Specifically, I'm questioning whether this definition truly encapsulates our intuitive understanding of what a limit is. The idea of a limit, as I see it, is about a function's behavior as it approaches a certain point. However, the Epsilon-Delta definition seems to be more about the precision of the approximation rather than the behavior of the function. In the book ""The Philosophy of Mathematics Today"" by Matthias Schirn, on page 159, it is stated that: ""At one point, Etchemendy asks: 'How do we know that our semantic definition of consequence is extensionally correct?' He goes on to say: 'That [this question] now strikes us odd just indicates how deeply ingrained is our assumption that the standard semantic definition captures, or comes close to capturing, the genuine notion of consequence' (Etchemendy 1990, 4-5). I do not think that this diagnosis is correct for some people: for some logicians, the question is similar to: How do we know that our epsilon-delta definition of continuity is correct?"". This quote resonates with my current dilemma. Does the Epsilon-Delta definition truly capture the essence of what we mean by a 'limit'? though the epsilon-delta definition is a mathematical construct, what evidence do we have that it accurately reflects our intuitive concept of a limit? How can we be sure it is not merely a useful formalism, but a true representation of the limit as a variable approaching some value? Are there alternative definitions or perspectives that might align more closely with our intuitive understanding of limits? I would appreciate any insights or resources that could help me reconcile these aspects of the concept of limits. Thank you in advance for your help. edit:i think i should add my motivation of asking the question, what i really want is an argument which can demonstrate that this definition of limit is the definition of limit which no better definition can come up, i can accept the definition as it is in its own axiomatic system and in itself, but whats the certainty that a hundred years from now we come up with a better definition still? its not about the thing that we cant understand i am more worried it there is something out our sphere of recognition if we are not taking note of, because everybody just seem to accept the definition without any further doubt an examination.","I've been delving into the concept of limits and the Epsilon-Delta definition. The most basic definition, as I understand it, states that for every real number , there exists a real number such that if then , where is the limit point and is the limit of the function at . While I grasp the formal definition, I'm grappling with the philosophical aspect of it. Specifically, I'm questioning whether this definition truly encapsulates our intuitive understanding of what a limit is. The idea of a limit, as I see it, is about a function's behavior as it approaches a certain point. However, the Epsilon-Delta definition seems to be more about the precision of the approximation rather than the behavior of the function. In the book ""The Philosophy of Mathematics Today"" by Matthias Schirn, on page 159, it is stated that: ""At one point, Etchemendy asks: 'How do we know that our semantic definition of consequence is extensionally correct?' He goes on to say: 'That [this question] now strikes us odd just indicates how deeply ingrained is our assumption that the standard semantic definition captures, or comes close to capturing, the genuine notion of consequence' (Etchemendy 1990, 4-5). I do not think that this diagnosis is correct for some people: for some logicians, the question is similar to: How do we know that our epsilon-delta definition of continuity is correct?"". This quote resonates with my current dilemma. Does the Epsilon-Delta definition truly capture the essence of what we mean by a 'limit'? though the epsilon-delta definition is a mathematical construct, what evidence do we have that it accurately reflects our intuitive concept of a limit? How can we be sure it is not merely a useful formalism, but a true representation of the limit as a variable approaching some value? Are there alternative definitions or perspectives that might align more closely with our intuitive understanding of limits? I would appreciate any insights or resources that could help me reconcile these aspects of the concept of limits. Thank you in advance for your help. edit:i think i should add my motivation of asking the question, what i really want is an argument which can demonstrate that this definition of limit is the definition of limit which no better definition can come up, i can accept the definition as it is in its own axiomatic system and in itself, but whats the certainty that a hundred years from now we come up with a better definition still? its not about the thing that we cant understand i am more worried it there is something out our sphere of recognition if we are not taking note of, because everybody just seem to accept the definition without any further doubt an examination.",\epsilon \gt 0 \delta \gt 0 0 \lt |x - a| \lt \delta |f(x) - L| \lt \epsilon a L f a,"['real-analysis', 'calculus', 'soft-question', 'epsilon-delta', 'philosophy']"
82,Rudin against Pugh for Textbook for First Course in Real Analysis,Rudin against Pugh for Textbook for First Course in Real Analysis,,"So as I have said before in a previous question, I am taking a first course in Mathematical Analysis, and I'm quite excited. I just found out though that unlike the other professors at my university, my professor is using Real Mathematical Analysis by Pugh. I thought it was rather strange because I have read from so many places that Rudin's text on the topic is ""the bible"" of mathematical analysis, and also he is the only professor who doesn't use it. So I was wondering what some of you experienced mathemeticians thought of choosing this book over Rudin? Is this book a little easier to use than Rudin's? I have heard the Rudin is quite rigorous.","So as I have said before in a previous question, I am taking a first course in Mathematical Analysis, and I'm quite excited. I just found out though that unlike the other professors at my university, my professor is using Real Mathematical Analysis by Pugh. I thought it was rather strange because I have read from so many places that Rudin's text on the topic is ""the bible"" of mathematical analysis, and also he is the only professor who doesn't use it. So I was wondering what some of you experienced mathemeticians thought of choosing this book over Rudin? Is this book a little easier to use than Rudin's? I have heard the Rudin is quite rigorous.",,"['real-analysis', 'soft-question', 'advice']"
83,Asymptotic behavior of the partial sums $\sum\limits_{k=1}^{n}k^{1/4} $,Asymptotic behavior of the partial sums,\sum\limits_{k=1}^{n}k^{1/4} ,What is the asymptotic behavior of the sequence: \begin{equation} s_n=\sum_{k=1}^{n}k^{1/4} \end{equation} when $n\to \infty$?,What is the asymptotic behavior of the sequence: \begin{equation} s_n=\sum_{k=1}^{n}k^{1/4} \end{equation} when $n\to \infty$?,,"['calculus', 'real-analysis', 'sequences-and-series']"
84,How to express $(1+x+x^2+\cdots+x^m)^n$ as a power series?,How to express  as a power series?,(1+x+x^2+\cdots+x^m)^n,Is it possible to express $(1+x+x^2+\cdots+x^m)^n$ as a power series?,Is it possible to express $(1+x+x^2+\cdots+x^m)^n$ as a power series?,,"['real-analysis', 'power-series']"
85,Does the series $ \sum_{n=1}^{\infty}\left( 1-\cos\big(\frac{1}{n} \big) \right)$ converge?,Does the series  converge?, \sum_{n=1}^{\infty}\left( 1-\cos\big(\frac{1}{n} \big) \right),"I'm having trouble determining whether the series: $$ \sum_{n=1}^{\infty}\left[1-\cos\left(1 \over n\right)\right] $$ converges. I have tried the root test : $$\lim_{n\rightarrow\infty}\sqrt[n]{1-\cos\frac{1}{n}}=\lim_{n\rightarrow\infty}\left(1-\cos\frac{1}{n}\right)^{1/n}=\lim_{n\rightarrow\infty}\mathrm{e}^{\frac{\log(1-\cos\frac{1}{n})}{n}}=\mathrm{e}^{\lim_{n\rightarrow\infty}\frac{\log(1-\cos\frac{1}{n})}{n}}$$ Now by applying the Stolz–Cesàro theorem , that upper limit is equal to: \begin{align} \lim_{n\rightarrow\infty}\frac{\log(1-\cos\frac{1}{n+1})-\log(1-\cos\frac{1}{n})}{(n+1)-n}&=\lim_{n\rightarrow\infty}\left(\log(1-\cos\frac{1}{n+1})-\log(1-\cos\frac{1}{n})\right) \\&=\lim_{n\rightarrow\infty}\log{\frac{1-\cos{\frac{1}{n+1}}}{1-\cos{\frac{1}{n}}}} \end{align} Now I'm totally stuck, unless that quotient is actually 1 , in which case the limit would be 0 , the Root test result would be $\mathrm{e}^0=1$ and all this would have been to no avail. I'm not sure this method was the best idea, the series sure seems way simpler than that, so probably another method is more appropriate?","I'm having trouble determining whether the series: converges. I have tried the root test : Now by applying the Stolz–Cesàro theorem , that upper limit is equal to: Now I'm totally stuck, unless that quotient is actually 1 , in which case the limit would be 0 , the Root test result would be and all this would have been to no avail. I'm not sure this method was the best idea, the series sure seems way simpler than that, so probably another method is more appropriate?","
\sum_{n=1}^{\infty}\left[1-\cos\left(1 \over n\right)\right]
 \lim_{n\rightarrow\infty}\sqrt[n]{1-\cos\frac{1}{n}}=\lim_{n\rightarrow\infty}\left(1-\cos\frac{1}{n}\right)^{1/n}=\lim_{n\rightarrow\infty}\mathrm{e}^{\frac{\log(1-\cos\frac{1}{n})}{n}}=\mathrm{e}^{\lim_{n\rightarrow\infty}\frac{\log(1-\cos\frac{1}{n})}{n}} \begin{align}
\lim_{n\rightarrow\infty}\frac{\log(1-\cos\frac{1}{n+1})-\log(1-\cos\frac{1}{n})}{(n+1)-n}&=\lim_{n\rightarrow\infty}\left(\log(1-\cos\frac{1}{n+1})-\log(1-\cos\frac{1}{n})\right)
\\&=\lim_{n\rightarrow\infty}\log{\frac{1-\cos{\frac{1}{n+1}}}{1-\cos{\frac{1}{n}}}}
\end{align} \mathrm{e}^0=1","['real-analysis', 'calculus', 'sequences-and-series', 'limits', 'convergence-divergence']"
86,Why can't Cantor sets cover $\mathbb{ R}$?,Why can't Cantor sets cover ?,\mathbb{ R},"The Cantor set is uncountable so I expect countably many of them to be able to cover $\mathbb R$, but the set has measure $0$ so countably many of them also has set of measure $0$ and thus can't cover the real line. Why/where is my intuition broken?","The Cantor set is uncountable so I expect countably many of them to be able to cover $\mathbb R$, but the set has measure $0$ so countably many of them also has set of measure $0$ and thus can't cover the real line. Why/where is my intuition broken?",,['soft-question']
87,Another beautiful integral (Part 2),Another beautiful integral (Part 2),,"One of the ways of calculating the integral in closed form is to think of crafitly using the geometric series, but even so it seems evil enough. $$\int_0^1\int_0^1\int_0^1\int_0^1\frac{1}{(1+x) (1+y) (1+z)(1+w) (1+ x y z w)} \ dx \ dy \ dz \ dw$$ Maybe you can guide me, bless me with another precious hints, clues. Thanks MSE users! Supplementary question : How about the generalization? $$\int_0^1\int_0^1\cdots\int_0^1\frac{1}{(1+x_1) (1+x_2)\cdots  (1+x_n)(1+ x_1 x_2 \cdots x_n)} \ dx_1 \ dx_2 \cdots \ dx_n$$","One of the ways of calculating the integral in closed form is to think of crafitly using the geometric series, but even so it seems evil enough. $$\int_0^1\int_0^1\int_0^1\int_0^1\frac{1}{(1+x) (1+y) (1+z)(1+w) (1+ x y z w)} \ dx \ dy \ dz \ dw$$ Maybe you can guide me, bless me with another precious hints, clues. Thanks MSE users! Supplementary question : How about the generalization? $$\int_0^1\int_0^1\cdots\int_0^1\frac{1}{(1+x_1) (1+x_2)\cdots  (1+x_n)(1+ x_1 x_2 \cdots x_n)} \ dx_1 \ dx_2 \cdots \ dx_n$$",,"['real-analysis', 'calculus', 'integration', 'definite-integrals', 'harmonic-numbers']"
88,Show $1/(1+ x^2)$ is uniformly continuous on $\Bbb R$.,Show  is uniformly continuous on .,1/(1+ x^2) \Bbb R,"Prove that the function $x \mapsto \dfrac 1{1+ x^2}$ is uniformly continuous on $\mathbb{R}$ . Attempt: By definition a function $f: E →\Bbb R$ is uniformly continuous iff for every $ε > 0$ , there is a $δ > 0$ such that $|x-a| < δ$ and $x,a$ are elements of $E$ implies $|f(x) - f(a)| < ε.$ Then suppose $x, a$ are elements of $\Bbb R. $ Now \begin{align} |f(x) - f(a)|  &= \left|\frac1{1 + x^2} - \frac1{1 + a^2}\right| \\&= \left|  \frac{a^2 - x^2}{(1 + x^2)(1 + a^2)}\right| \\&= |x - a| \frac{|x + a|}{(1 + x^2)(1 + a^2)} \\&≤ |x - a| \frac{|x| + |a|}{(1 + x^2)(1 + a^2)} \\&= |x - a| \left[\frac{|x|}{(1 + x^2)(1 + a^2)} + \frac{|a|}{(1 + x^2)(1 + a^2)}\right] \end{align} I don't know how to simplify more. Can someone please help me finish? Thank very much.","Prove that the function is uniformly continuous on . Attempt: By definition a function is uniformly continuous iff for every , there is a such that and are elements of implies Then suppose are elements of Now I don't know how to simplify more. Can someone please help me finish? Thank very much.","x \mapsto \dfrac 1{1+ x^2} \mathbb{R} f: E →\Bbb R ε > 0 δ > 0 |x-a| < δ x,a E |f(x) - f(a)| < ε. x, a \Bbb R.  \begin{align}
|f(x) - f(a)| 
&= \left|\frac1{1 + x^2} - \frac1{1 + a^2}\right|
\\&= \left|  \frac{a^2 - x^2}{(1 + x^2)(1 + a^2)}\right|
\\&= |x - a| \frac{|x + a|}{(1 + x^2)(1 + a^2)}
\\&≤ |x - a| \frac{|x| + |a|}{(1 + x^2)(1 + a^2)}
\\&= |x - a| \left[\frac{|x|}{(1 + x^2)(1 + a^2)} + \frac{|a|}{(1 + x^2)(1 + a^2)}\right]
\end{align}","['real-analysis', 'uniform-continuity']"
89,Why aren't all dense subsets of $\mathbb{R}$ uncountable?,Why aren't all dense subsets of  uncountable?,\mathbb{R},"1) we say that $\mathbb{R}$ is uncountable and $\mathbb{Q}$ is countable. That implies $\mathbb{R}-\mathbb{Q}$, that is irrational numbers are uncountable. 2) Archimedian property of $\mathbb{R}$ suggests that there exists a rational between any two numbers. i.e.  $\mathbb{Q}$ is dense in $\mathbb{R}$. Then how come $\mathbb{Q}$ is countable while irrational is uncountable?","1) we say that $\mathbb{R}$ is uncountable and $\mathbb{Q}$ is countable. That implies $\mathbb{R}-\mathbb{Q}$, that is irrational numbers are uncountable. 2) Archimedian property of $\mathbb{R}$ suggests that there exists a rational between any two numbers. i.e.  $\mathbb{Q}$ is dense in $\mathbb{R}$. Then how come $\mathbb{Q}$ is countable while irrational is uncountable?",,['real-analysis']
90,Series $\sum_{n=1}^{\infty}\frac{\cos(nx)}{n^2}$,Series,\sum_{n=1}^{\infty}\frac{\cos(nx)}{n^2},"Is it true that for $x\in[0,2\pi]$ we have $$\sum_{n=1}^{\infty}\frac{\cos(nx)}{n^2}=\frac{x^2}{4}-\frac{\pi x}{2}+\frac{\pi^2}{6}$$ How can I prove it? For other intervals what is the value of above series if is convergent?","Is it true that for $x\in[0,2\pi]$ we have $$\sum_{n=1}^{\infty}\frac{\cos(nx)}{n^2}=\frac{x^2}{4}-\frac{\pi x}{2}+\frac{\pi^2}{6}$$ How can I prove it? For other intervals what is the value of above series if is convergent?",,"['calculus', 'real-analysis']"
91,Is the empty function differentiable?,Is the empty function differentiable?,,"Given the empty function $\varnothing: \emptyset \to X$ where $X$ is a set, is $\varnothing$ a differentiable function? If so, what is its derivative? Also, is the empty set a differentiable manifold? If so, what is its dimension?","Given the empty function where is a set, is a differentiable function? If so, what is its derivative? Also, is the empty set a differentiable manifold? If so, what is its dimension?",\varnothing: \emptyset \to X X \varnothing,"['calculus', 'real-analysis', 'functions', 'differential-topology']"
92,Prove $\sum^{\infty}_{n=1} \frac{a_{n+1}-a_{n}}{a_{n}}=\infty$ for an increasing sequence $a_n$ of positive integers [duplicate],Prove  for an increasing sequence  of positive integers [duplicate],\sum^{\infty}_{n=1} \frac{a_{n+1}-a_{n}}{a_{n}}=\infty a_n,"This question already has answers here : If the positive series $\sum a_n$ diverges and $s_n=\sum\limits_{k\leqslant n}a_k$ then $\sum \frac{a_n}{s_n}$ diverges as well (2 answers) Closed 8 years ago . The $a_n$'s are integers, positive, and increasing: $0< a_1 < a_2 < \cdots$, the problem asks us to prove that: $$ \sum^{\infty}_{n=1} \frac{a_{n+1}-a_{n}}{a_{n}}=\infty $$  While I have checked this results for several series like $a_n = n$, $a_n = n^2$, $a_n = n^p$, or $a_n = p^n$ type stuff, I don't know how to prove this general result. A hint is appreciated. Thanks dudes!","This question already has answers here : If the positive series $\sum a_n$ diverges and $s_n=\sum\limits_{k\leqslant n}a_k$ then $\sum \frac{a_n}{s_n}$ diverges as well (2 answers) Closed 8 years ago . The $a_n$'s are integers, positive, and increasing: $0< a_1 < a_2 < \cdots$, the problem asks us to prove that: $$ \sum^{\infty}_{n=1} \frac{a_{n+1}-a_{n}}{a_{n}}=\infty $$  While I have checked this results for several series like $a_n = n$, $a_n = n^2$, $a_n = n^p$, or $a_n = p^n$ type stuff, I don't know how to prove this general result. A hint is appreciated. Thanks dudes!",,"['real-analysis', 'sequences-and-series', 'convergence-divergence']"
93,Where does one even need Bernoulli's Inequality?,Where does one even need Bernoulli's Inequality?,,"Which Theorems/Lemmas/Results actually use Bernoulli's inequality? I don't seem to remember using it very often - which probably makes sense, as it's not a very strong inequality and can be proven easily. However, where do you actually use Bernoulli?","Which Theorems/Lemmas/Results actually use Bernoulli's inequality? I don't seem to remember using it very often - which probably makes sense, as it's not a very strong inequality and can be proven easily. However, where do you actually use Bernoulli?",,"['real-analysis', 'inequality']"
94,Show that the Cantor set is nowhere dense,Show that the Cantor set is nowhere dense,,"I want to prove that the cantor set is nowhere dense. First, a subset $A$ of $\mathbb{R}$ is said to be nowhere dense provided that for every open set $\mathcal{O}$ has an open subset that is disjoint from $A$. Proof: Since the Cantor set is closed, then the closure of the Cantor set is the Cantor set. Then, to prove that the Cantor set is nowhere dense, it is enough to show that the interior is empty.  Observe that for a subset $A$ of a topological space $X$, the interior of $A$ is defined as the union of all open sets contained in $A$. Let $A = C$, the Cantor set and $X = \mathbb{R}$, notice that $C = \bigcap_{k\in\mathbb{N}}C_k$, where each $C_k$ is closed. Moreover, we have $$\forall k\in \mathbb{N} \quad C_k = \bigsqcup_{k\in\mathbb{N}}F_k$$ where each $F_k$ is the disjoint union of $2^k$ closed intervals, each of length $1/3^k$. We thus conclude that $C$ dont contain any open sets and thus $$\text{int}(C) = \emptyset.$$ Is this proof correct?","I want to prove that the cantor set is nowhere dense. First, a subset $A$ of $\mathbb{R}$ is said to be nowhere dense provided that for every open set $\mathcal{O}$ has an open subset that is disjoint from $A$. Proof: Since the Cantor set is closed, then the closure of the Cantor set is the Cantor set. Then, to prove that the Cantor set is nowhere dense, it is enough to show that the interior is empty.  Observe that for a subset $A$ of a topological space $X$, the interior of $A$ is defined as the union of all open sets contained in $A$. Let $A = C$, the Cantor set and $X = \mathbb{R}$, notice that $C = \bigcap_{k\in\mathbb{N}}C_k$, where each $C_k$ is closed. Moreover, we have $$\forall k\in \mathbb{N} \quad C_k = \bigsqcup_{k\in\mathbb{N}}F_k$$ where each $F_k$ is the disjoint union of $2^k$ closed intervals, each of length $1/3^k$. We thus conclude that $C$ dont contain any open sets and thus $$\text{int}(C) = \emptyset.$$ Is this proof correct?",,"['real-analysis', 'measure-theory', 'lebesgue-measure', 'cantor-set']"
95,Why sets that aren't closed can't be compact?,Why sets that aren't closed can't be compact?,,"In $\mathbb{R}^n$ we prove that a set is compact (using the definition about open covers) if and only if it's closed and bounded. It is pretty clear that if $\mathcal{O}$ is an open cover of one unbounded set $X$, then $\mathcal{O}$ cannot have a finite subcover, it'll clearly need in general infinitely many sets to cover the set $X$. Now, if a set isn't closed, I cannot see in which way it fails to be compact. For instance, if $X$ is the closed unit ball centered at the origin, then it is compact. If on the other hand we consider $Y=X\setminus\{0\}$, then it's not compact anymore, because $Y$ isn't closed (the point $0$ is a limit point of $X$ and so, $0 \in \operatorname{Cl}(X)$ and on the same time $0 \notin Y$. So, what should be the intuition about this? How can we intuitively see that $Y$ isn't compact? Thanks very much in advance!","In $\mathbb{R}^n$ we prove that a set is compact (using the definition about open covers) if and only if it's closed and bounded. It is pretty clear that if $\mathcal{O}$ is an open cover of one unbounded set $X$, then $\mathcal{O}$ cannot have a finite subcover, it'll clearly need in general infinitely many sets to cover the set $X$. Now, if a set isn't closed, I cannot see in which way it fails to be compact. For instance, if $X$ is the closed unit ball centered at the origin, then it is compact. If on the other hand we consider $Y=X\setminus\{0\}$, then it's not compact anymore, because $Y$ isn't closed (the point $0$ is a limit point of $X$ and so, $0 \in \operatorname{Cl}(X)$ and on the same time $0 \notin Y$. So, what should be the intuition about this? How can we intuitively see that $Y$ isn't compact? Thanks very much in advance!",,"['real-analysis', 'general-topology', 'compactness']"
96,How to prove :$\sqrt{1!\sqrt{2!\sqrt{3!\sqrt{\cdots\sqrt{n!}}}}} <3$,How to prove :,\sqrt{1!\sqrt{2!\sqrt{3!\sqrt{\cdots\sqrt{n!}}}}} <3,In  fact initially I wanted to prove that $$\sqrt{1!\sqrt{2!\sqrt{3!\sqrt{\cdots\sqrt{n!}}}}} <2$$ Which by the accepted answer here fails to be true. @Barry Cipra advised me to ask this question in a different post: How can I now prove that: $$\sqrt{1!\sqrt{2!\sqrt{3!\sqrt{\cdots\sqrt{n!}}}}} <3$$ Note that the answers here do not provide an estimate of this sequence Can anyone have any idea?,In  fact initially I wanted to prove that Which by the accepted answer here fails to be true. @Barry Cipra advised me to ask this question in a different post: How can I now prove that: Note that the answers here do not provide an estimate of this sequence Can anyone have any idea?,\sqrt{1!\sqrt{2!\sqrt{3!\sqrt{\cdots\sqrt{n!}}}}} <2 \sqrt{1!\sqrt{2!\sqrt{3!\sqrt{\cdots\sqrt{n!}}}}} <3,"['calculus', 'real-analysis']"
97,Why does $ a_n = \frac {a_{n-1} + \frac {2}{a_{n-1}}}{2}$ converge to an irrational number?,Why does  converge to an irrational number?, a_n = \frac {a_{n-1} + \frac {2}{a_{n-1}}}{2},"There is a problem in my textbook that goes like this $$ a_n = \frac {a_{n-1} + \frac {2}{a_{n-1}}}{2}$$ and $$a_0 =1$$ for all $n\ge1$. It is monotonically decreasing sequence of rational numbers and bounded below. However, it cannot converge to a rational number. Then the task is to find the limit. The problem itself is easy but I don't understand how the author judged the limit to be irrational even before solving the question? Is there any property or did they just know the answer beforehand?","There is a problem in my textbook that goes like this $$ a_n = \frac {a_{n-1} + \frac {2}{a_{n-1}}}{2}$$ and $$a_0 =1$$ for all $n\ge1$. It is monotonically decreasing sequence of rational numbers and bounded below. However, it cannot converge to a rational number. Then the task is to find the limit. The problem itself is easy but I don't understand how the author judged the limit to be irrational even before solving the question? Is there any property or did they just know the answer beforehand?",,"['real-analysis', 'sequences-and-series', 'limits', 'real-numbers', 'cauchy-sequences']"
98,The product of two Riemann integrable functions is integrable,The product of two Riemann integrable functions is integrable,,"The goal is to show that the product of two Riemann integrable functions is integrable. First step is to use the identity $f\cdot g = \frac{1}{4} \left[(f+g)^2 - (f-g)^2\right]$ so that we  only need to consider squares of functions. The second step is to reduce to   positive valued functions because $f(x)^2=\left|f(x)\right|^2$. The third step is to use that if $0 \leq f(x) \leq M$ on $\left[a,b\right]$, $$f^2(x) - f^2(y) \leq 2M \left(\,f(x)-f(y)\right)$$ How should  I go about implementing the above steps?","The goal is to show that the product of two Riemann integrable functions is integrable. First step is to use the identity $f\cdot g = \frac{1}{4} \left[(f+g)^2 - (f-g)^2\right]$ so that we  only need to consider squares of functions. The second step is to reduce to   positive valued functions because $f(x)^2=\left|f(x)\right|^2$. The third step is to use that if $0 \leq f(x) \leq M$ on $\left[a,b\right]$, $$f^2(x) - f^2(y) \leq 2M \left(\,f(x)-f(y)\right)$$ How should  I go about implementing the above steps?",,['real-analysis']
99,Prove $ \frac{x_1-x_2}{x_n+x_1} + \frac{x_2-x_3}{x_1+x_2}+\cdots+ \frac{x_n-x_1}{x_{n-1} +x_n}\le 0$ s.t. $x_1+\cdots+x_n=1$,Prove  s.t., \frac{x_1-x_2}{x_n+x_1} + \frac{x_2-x_3}{x_1+x_2}+\cdots+ \frac{x_n-x_1}{x_{n-1} +x_n}\le 0 x_1+\cdots+x_n=1,"Is it true that: $$ f_n(x_1,\ldots,x_n)=\frac{x_1-x_2}{x_n+x_1} + \frac{x_2-x_3}{x_1+x_2}+\cdots+ \frac{x_n-x_1}{x_{n-1} +x_n}\le 0 $$ for $x_1,\ldots,x_n>0$ , such that $x_1+\cdots+x_n=1$ ? It is a cyclic inequality that I came up with while proving similar ones, although it may well be already solved/proposed somewhere else. Equality is attained at $x_1=\cdots=x_n=1/n$ . The first approach I’ve tried is to represent the inequality equivalently as $$ n\le \frac{x_1 +x_3}{x_1+x_2} + \frac{x_2+x_4}{x_2+x_3}+\cdots+\frac{x_n+x_2}{x_n+x_1}, $$ which is trivially true for $n=3$ by AM-GM, but not so for $n>3$ , since the terms don’t cancel. I’ve also tried the following approach: let $x_1$ be the largest wlog, then we can examine the function $$ g(t)=f_n(x_1-t, x_2 +t,\ldots,x_n) = \frac{x_1-x_2-2t}{x_n+x_1-t} + \frac{x_2+t-x_3}{x_1+x_2}+ \frac{x_3-x_4}{x_2+t+x_3} +\cdots+ \frac{x_n-x_1+t}{x_{n-1} +x_n} $$ and see if it is increasing for small enough $t$ . Then we can proceed by generating sequences that approach the average while increasing the value of $f$ . For example $$ g(t)-g(0)=\frac{t}{x_1+x_2}+\frac{t}{x_n+x_{n-1}}-t\frac{x_1+x_2+2x_n}{(x_1+x_n-t)(x_1+x_n)}-t\frac{x_3-x_4}{(x_2+x_3+t)(x_2+x_3)}, $$ but not sure if this is nonnegative. EDIT : I've made some progress towards the $n=5$ case. Let us assume wlog that $x_1$ is the largest throughout the proof. We also know that the $n=4$ case is true (see comments). Thus $$ f_5(x_1,\ldots,x_5)=f_4(x_2,\ldots,x_5)+\frac{x_2-x_1}{x_4+x_5}+\frac{x_1-x_2}{x_5+x_1}+\frac{x_2-x_3}{x_1+x_2}-\frac{x_2-x_3}{x_5+x_1}\leq\\\leq \frac{x_2-x_1}{x_4+x_5}+\frac{x_1-x_2}{x_5+x_1}+\frac{x_2-x_3}{x_1+x_2}-\frac{x_2-x_3}{x_5+x_1}=\\=\frac{(x_1-x_2)(x_4-x_1)}{(x_1+x_5)(x_4+x_5)}+\frac{(x_2-x_3)(x_5-x_1)}{(x_1+x_2)(x_2+x_5)}.\tag{1}\label{ineq:1} $$ By successively cyclically permuting the sequence we also derive the inequalities: \begin{align} f_5(x_1,\ldots,x_5) &\leq \frac{(x_2-x_3)(x_5-x_2)}{(x_2+x_1)(x_5+x_1)}+\frac{(x_3-x_4)(x_1-x_2)}{(x_2+x_3)(x_3+x_1)}\label{ineq:2}\tag{2}, \\ f_5(x_1,\ldots,x_5) &\leq \frac{(x_3-x_4)(x_1-x_3)}{(x_3+x_2)(x_1+x_2)}+\frac{(x_4-x_5)(x_2-x_3)}{(x_3+x_4)(x_4+x_2)}\label{ineq:3}\tag{3}, \\ f_5(x_1,\ldots,x_5) &\leq \frac{(x_4-x_5)(x_2-x_4)}{(x_4+x_3)(x_2+x_3)}+\frac{(x_5-x_1)(x_3-x_4)}{(x_4+x_5)(x_5+x_3)}\label{ineq:4}\tag{4}, \\ f_5(x_1,\ldots,x_5) &\leq \frac{(x_5-x_1)(x_3-x_5)}{(x_5+x_4)(x_3+x_4)}+\frac{(x_1-x_2)(x_4-x_5)}{(x_5+x_1)(x_1+x_4)}\label{ineq:5}\tag{5}. \end{align} Now we consider several cases. Case 1 : If $x_2\geq x_3$ then $\eqref{ineq:1}$ is $\leq 0$ . Case 3 : If $x_2 < x_3 \leq x_4 \leq x_5$ then $\eqref{ineq:2}$ is $\leq 0$ . Case 2 : If $x_2 < x_3 \leq x_4$ and $x_4 > x_5$ then $\eqref{ineq:3}$ is $\leq 0$ . Case 4 : If $x_2 < x_3$ , $x_3 > x_4, x_4 \leq x_5$ and $x_3 \geq x_5$ then $\eqref{ineq:5}$ is $\leq 0$ . Case 5 : If $x_2 < x_3$ , $x_3 > x_4, x_4 \leq x_5$ and $x_2 \geq x_4$ then $\eqref{ineq:4}$ is $\leq 0$ . Case 6 : If $x_2 < x_3$ , $x_3 > x_4, x_4 > x_5$ and $x_2 \leq x_4$ then $\eqref{ineq:4}$ is $\leq 0$ . There are two more cases to consider. Case 7 : If $x_3>x_2>x_4 > x_5$ then it seems to be the case that $$ f_5(x_1,\ldots,x_5) \leq f_5(x_1,x_2,x_4,x_3,x_5). $$ via simulation. If proven true, then we just need to apply case 1 to it and we're done. Case 8 : Since case 4 and 5 are true independently, we need consider only $x_5>x_3>x_4>x_2$ as a subcase. However I haven't the slightest clue how to approach this. EDIT 2 According to WolframAlpha , it seems that the individual fractions are concave functions of $\mathbf{x}=(x_1,\ldots,x_n)$ . Does that mean that $f$ is also concave, therefore we can bound by $$ f_n(\mathbf{x}) \leq f_n(\mathbf{1}/n) + \nabla f_n(\mathbf{1}/n)^T(\mathbf{x} - \mathbf{1}/n) = 0? $$","Is it true that: for , such that ? It is a cyclic inequality that I came up with while proving similar ones, although it may well be already solved/proposed somewhere else. Equality is attained at . The first approach I’ve tried is to represent the inequality equivalently as which is trivially true for by AM-GM, but not so for , since the terms don’t cancel. I’ve also tried the following approach: let be the largest wlog, then we can examine the function and see if it is increasing for small enough . Then we can proceed by generating sequences that approach the average while increasing the value of . For example but not sure if this is nonnegative. EDIT : I've made some progress towards the case. Let us assume wlog that is the largest throughout the proof. We also know that the case is true (see comments). Thus By successively cyclically permuting the sequence we also derive the inequalities: Now we consider several cases. Case 1 : If then is . Case 3 : If then is . Case 2 : If and then is . Case 4 : If , and then is . Case 5 : If , and then is . Case 6 : If , and then is . There are two more cases to consider. Case 7 : If then it seems to be the case that via simulation. If proven true, then we just need to apply case 1 to it and we're done. Case 8 : Since case 4 and 5 are true independently, we need consider only as a subcase. However I haven't the slightest clue how to approach this. EDIT 2 According to WolframAlpha , it seems that the individual fractions are concave functions of . Does that mean that is also concave, therefore we can bound by","
f_n(x_1,\ldots,x_n)=\frac{x_1-x_2}{x_n+x_1} + \frac{x_2-x_3}{x_1+x_2}+\cdots+ \frac{x_n-x_1}{x_{n-1} +x_n}\le 0
 x_1,\ldots,x_n>0 x_1+\cdots+x_n=1 x_1=\cdots=x_n=1/n 
n\le \frac{x_1 +x_3}{x_1+x_2} + \frac{x_2+x_4}{x_2+x_3}+\cdots+\frac{x_n+x_2}{x_n+x_1},
 n=3 n>3 x_1 
g(t)=f_n(x_1-t, x_2 +t,\ldots,x_n) = \frac{x_1-x_2-2t}{x_n+x_1-t} + \frac{x_2+t-x_3}{x_1+x_2}+ \frac{x_3-x_4}{x_2+t+x_3} +\cdots+ \frac{x_n-x_1+t}{x_{n-1} +x_n}
 t f 
g(t)-g(0)=\frac{t}{x_1+x_2}+\frac{t}{x_n+x_{n-1}}-t\frac{x_1+x_2+2x_n}{(x_1+x_n-t)(x_1+x_n)}-t\frac{x_3-x_4}{(x_2+x_3+t)(x_2+x_3)},
 n=5 x_1 n=4 
f_5(x_1,\ldots,x_5)=f_4(x_2,\ldots,x_5)+\frac{x_2-x_1}{x_4+x_5}+\frac{x_1-x_2}{x_5+x_1}+\frac{x_2-x_3}{x_1+x_2}-\frac{x_2-x_3}{x_5+x_1}\leq\\\leq
\frac{x_2-x_1}{x_4+x_5}+\frac{x_1-x_2}{x_5+x_1}+\frac{x_2-x_3}{x_1+x_2}-\frac{x_2-x_3}{x_5+x_1}=\\=\frac{(x_1-x_2)(x_4-x_1)}{(x_1+x_5)(x_4+x_5)}+\frac{(x_2-x_3)(x_5-x_1)}{(x_1+x_2)(x_2+x_5)}.\tag{1}\label{ineq:1}
 \begin{align}
f_5(x_1,\ldots,x_5) &\leq \frac{(x_2-x_3)(x_5-x_2)}{(x_2+x_1)(x_5+x_1)}+\frac{(x_3-x_4)(x_1-x_2)}{(x_2+x_3)(x_3+x_1)}\label{ineq:2}\tag{2}, \\
f_5(x_1,\ldots,x_5) &\leq \frac{(x_3-x_4)(x_1-x_3)}{(x_3+x_2)(x_1+x_2)}+\frac{(x_4-x_5)(x_2-x_3)}{(x_3+x_4)(x_4+x_2)}\label{ineq:3}\tag{3}, \\
f_5(x_1,\ldots,x_5) &\leq \frac{(x_4-x_5)(x_2-x_4)}{(x_4+x_3)(x_2+x_3)}+\frac{(x_5-x_1)(x_3-x_4)}{(x_4+x_5)(x_5+x_3)}\label{ineq:4}\tag{4}, \\
f_5(x_1,\ldots,x_5) &\leq \frac{(x_5-x_1)(x_3-x_5)}{(x_5+x_4)(x_3+x_4)}+\frac{(x_1-x_2)(x_4-x_5)}{(x_5+x_1)(x_1+x_4)}\label{ineq:5}\tag{5}.
\end{align} x_2\geq x_3 \eqref{ineq:1} \leq 0 x_2 < x_3 \leq x_4 \leq x_5 \eqref{ineq:2} \leq 0 x_2 < x_3 \leq x_4 x_4 > x_5 \eqref{ineq:3} \leq 0 x_2 < x_3 x_3 > x_4, x_4 \leq x_5 x_3 \geq x_5 \eqref{ineq:5} \leq 0 x_2 < x_3 x_3 > x_4, x_4 \leq x_5 x_2 \geq x_4 \eqref{ineq:4} \leq 0 x_2 < x_3 x_3 > x_4, x_4 > x_5 x_2 \leq x_4 \eqref{ineq:4} \leq 0 x_3>x_2>x_4 > x_5 
f_5(x_1,\ldots,x_5) \leq f_5(x_1,x_2,x_4,x_3,x_5).
 x_5>x_3>x_4>x_2 \mathbf{x}=(x_1,\ldots,x_n) f 
f_n(\mathbf{x}) \leq f_n(\mathbf{1}/n) + \nabla f_n(\mathbf{1}/n)^T(\mathbf{x} - \mathbf{1}/n) = 0?
","['real-analysis', 'inequality', 'optimization', 'lagrange-multiplier']"
