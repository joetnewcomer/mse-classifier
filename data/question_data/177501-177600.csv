,title_raw,title_text,title_latex,body_raw,body_text,body_latex,tags
0,What's the direction of parametrization for the following question?,What's the direction of parametrization for the following question?,,"Suppose we want to parametrize the boundary curve of hemisphere $x^2+y^2+z^2=16, y\ge0$, oriented in the direrction of the positive $y-$axis. The answer the book gives is $r(t)=<4\cos(-t),0, 4\sin(-t)>$, and this confuses me because ""direrction of the positive $y-$axis"" means positive(counter-clockwise) orientation of the circle on $xz$ plane. Why is there negative $t$?","Suppose we want to parametrize the boundary curve of hemisphere $x^2+y^2+z^2=16, y\ge0$, oriented in the direrction of the positive $y-$axis. The answer the book gives is $r(t)=<4\cos(-t),0, 4\sin(-t)>$, and this confuses me because ""direrction of the positive $y-$axis"" means positive(counter-clockwise) orientation of the circle on $xz$ plane. Why is there negative $t$?",,"['multivariable-calculus', '3d', 'parametrization', 'stokes-theorem']"
1,Determining if a limit does not exist.,Determining if a limit does not exist.,,"I am having trouble determining if the limit defined below exists. I do not believe it does, but I am not sure how to find 2 paths where the limits differ. I've tried $f(x, mx)$, $f(my, y)$, $f(x, mx^2)$, but nothing really seems to work. Show that the limit does not exist: $$ \lim_{(x,y) \to (0,0)} \frac{y^2 + (1-\cos(x))^2}{x^4 + y^2} $$ I thought about parameterizing using polar coordinates, but I wasn't sure how to show that the $\displaystyle \lim_{r^+ \to 0}$ will depend on $\theta$ (and hence does not exist). Any help appreciated. Thanks.","I am having trouble determining if the limit defined below exists. I do not believe it does, but I am not sure how to find 2 paths where the limits differ. I've tried $f(x, mx)$, $f(my, y)$, $f(x, mx^2)$, but nothing really seems to work. Show that the limit does not exist: $$ \lim_{(x,y) \to (0,0)} \frac{y^2 + (1-\cos(x))^2}{x^4 + y^2} $$ I thought about parameterizing using polar coordinates, but I wasn't sure how to show that the $\displaystyle \lim_{r^+ \to 0}$ will depend on $\theta$ (and hence does not exist). Any help appreciated. Thanks.",,['multivariable-calculus']
2,Is the integral squared equal to two times the integral from $a$ to $b$ and from $x$ to $b$?,Is the integral squared equal to two times the integral from  to  and from  to ?,a b x b,"I have to prove that  $$2\int_a^b\int_x^bf(x)f(y)\,dx\,dy = \left(\int_a^bf(x)\,dx\right)^2  $$ where $f$ is continuous in $[a,b].$ I tried to separate the integrals in a way that I get that \begin{align*} \left(\int_a^bf(x)\,dx\right)^2 &= \left(\int_a^bf(x)\,dx\right) \left(\int_a^bf(y)\,dy\right)\\ &= \left(\int_a^bf(x)\,dx\right)\left(\int_a^xf(y)\,dy+\int_x^bf(y)\,dy\right). \end{align*} But I don't understand how am I supposed to prove that $$\int_a^xf(y)\,dy=\int_x^bf(y)\,dy$$ becuase that expression depends on the $x$ that I take.  It seems that I need a certain symmetry in the function to accomplish something similar, where I can take another $x_{o}$ in $[a,b]$ and show that $$\int_a^{x_{o}}f(y)\,dy=\int_{x_{o}}^bf(y)\,dy.$$ There something that I am not seeing, any hint or idea would be helpful.","I have to prove that  $$2\int_a^b\int_x^bf(x)f(y)\,dx\,dy = \left(\int_a^bf(x)\,dx\right)^2  $$ where $f$ is continuous in $[a,b].$ I tried to separate the integrals in a way that I get that \begin{align*} \left(\int_a^bf(x)\,dx\right)^2 &= \left(\int_a^bf(x)\,dx\right) \left(\int_a^bf(y)\,dy\right)\\ &= \left(\int_a^bf(x)\,dx\right)\left(\int_a^xf(y)\,dy+\int_x^bf(y)\,dy\right). \end{align*} But I don't understand how am I supposed to prove that $$\int_a^xf(y)\,dy=\int_x^bf(y)\,dy$$ becuase that expression depends on the $x$ that I take.  It seems that I need a certain symmetry in the function to accomplish something similar, where I can take another $x_{o}$ in $[a,b]$ and show that $$\int_a^{x_{o}}f(y)\,dy=\int_{x_{o}}^bf(y)\,dy.$$ There something that I am not seeing, any hint or idea would be helpful.",,"['calculus', 'multivariable-calculus']"
3,How to set up polar parametrization for the following double integral?,How to set up polar parametrization for the following double integral?,,"If the mass per unit area of a surface is given by $\rho=xy $ find the mass $\int\int_S xy\ dS$ if $S$ is the part of the cylinder $y^2+z^2=16$ which is in the first octant and contained within the cylinder $x^2+y^2=9$? First I set up parametrization as $x=x, y=4\cos\theta, z=4\sin\theta$. So $|r_x\times r_\theta|=\sqrt{(4\cos\theta)^2+(4\sin\theta)^2}=4$ Then $dS=4dxd\theta$. So $\int\int_S xy\ dS=\int^\frac{\pi}{2}_0\int^?_0 16x\cos\theta\ dxd\theta$. $($it's in first quadrant$)$ I am uncertain about the '?' part. I guessed $?=\sqrt{9-(4\cos\theta)^2}$ but it gives answer different from correct answer, which is $11.36$.","If the mass per unit area of a surface is given by $\rho=xy $ find the mass $\int\int_S xy\ dS$ if $S$ is the part of the cylinder $y^2+z^2=16$ which is in the first octant and contained within the cylinder $x^2+y^2=9$? First I set up parametrization as $x=x, y=4\cos\theta, z=4\sin\theta$. So $|r_x\times r_\theta|=\sqrt{(4\cos\theta)^2+(4\sin\theta)^2}=4$ Then $dS=4dxd\theta$. So $\int\int_S xy\ dS=\int^\frac{\pi}{2}_0\int^?_0 16x\cos\theta\ dxd\theta$. $($it's in first quadrant$)$ I am uncertain about the '?' part. I guessed $?=\sqrt{9-(4\cos\theta)^2}$ but it gives answer different from correct answer, which is $11.36$.",,"['multivariable-calculus', 'area', 'density-function']"
4,"If there's a line $L\subset \mathbb{R}^n$ and a sequence $t_k\to a$ where $f(t_k)\in L$, then $L$ is the tangent line at $f(a)$","If there's a line  and a sequence  where , then  is the tangent line at",L\subset \mathbb{R}^n t_k\to a f(t_k)\in L L f(a),"Let $f:I\to\mathbb{R}^n$ be a differentiable path, with $f'(a)\neq 0$   for some $a\in I$. If there is a line $L\subset \mathbb{R}^n$ and a   sequence of distinct numbers such that $t_k\to a$ such that $f(t_k)\in  L$, then the line $L$ is the tangent line to $f$ at the point $f(a)$ First, the tangent line $L$ at the point $a$ is just the line that has 'slope' $f'(a)$ and passes at the point $f(a)$, right? So I need to prove that $$t_k\to a \implies f(t_k)\in L \implies \lim_{t\to 0}\frac{f(a+t)-f(a)}{t} = L$$ ? Intuitively, I see that $t_k\to a$ implying $f(t_k)\in L$ means that, no matter which side we approximate, if we're getting closer and closer to $a$, then $f(t_k)$ is in $L$. I guess the other definition of limit would help me better: $$L = \lim_{t\to a}\frac{f(t)-f(a)}{t-a}$$ But how to show that $$t_k\to a \implies f(t_k)\in L \implies \lim_{t\to 0}\frac{f(a+t)-f(a)}{t} = L$$ implies in the limit definition above?","Let $f:I\to\mathbb{R}^n$ be a differentiable path, with $f'(a)\neq 0$   for some $a\in I$. If there is a line $L\subset \mathbb{R}^n$ and a   sequence of distinct numbers such that $t_k\to a$ such that $f(t_k)\in  L$, then the line $L$ is the tangent line to $f$ at the point $f(a)$ First, the tangent line $L$ at the point $a$ is just the line that has 'slope' $f'(a)$ and passes at the point $f(a)$, right? So I need to prove that $$t_k\to a \implies f(t_k)\in L \implies \lim_{t\to 0}\frac{f(a+t)-f(a)}{t} = L$$ ? Intuitively, I see that $t_k\to a$ implying $f(t_k)\in L$ means that, no matter which side we approximate, if we're getting closer and closer to $a$, then $f(t_k)$ is in $L$. I guess the other definition of limit would help me better: $$L = \lim_{t\to a}\frac{f(t)-f(a)}{t-a}$$ But how to show that $$t_k\to a \implies f(t_k)\in L \implies \lim_{t\to 0}\frac{f(a+t)-f(a)}{t} = L$$ implies in the limit definition above?",,"['calculus', 'real-analysis', 'multivariable-calculus', 'derivatives']"
5,magnetic field with a gradient?,magnetic field with a gradient?,,"Usually it is said that the Maxwell equation $\vec \nabla \cdot \vec{B}=0$ is solved by introducing the vector potential according to $\vec B=\vec \nabla \times \vec A$. However, I supose that one could write the more general decomposition $\vec B=\vec \nabla \times \vec A+\vec \nabla f$ and require $\nabla^2 f=0$ to enforce $\vec \nabla \cdot \vec{B}=0$. Why is this never done? Why must magnetic fields be purely rotational?","Usually it is said that the Maxwell equation $\vec \nabla \cdot \vec{B}=0$ is solved by introducing the vector potential according to $\vec B=\vec \nabla \times \vec A$. However, I supose that one could write the more general decomposition $\vec B=\vec \nabla \times \vec A+\vec \nabla f$ and require $\nabla^2 f=0$ to enforce $\vec \nabla \cdot \vec{B}=0$. Why is this never done? Why must magnetic fields be purely rotational?",,"['linear-algebra', 'multivariable-calculus']"
6,Volume of the intersection of two cylinders $x^2+y^2=1$ and $y^2+z^2=1$.,Volume of the intersection of two cylinders  and .,x^2+y^2=1 y^2+z^2=1,"Consider two intersecting cylinders. I know the regular way to do this is: $$ \int_{-1}^{1} \left(2\sqrt{1-2x^2}\right)^2\,dx = \frac{16}{3} $$ This methods integrates the square sides of the solid that you get However, is it just as viable to flip the picture 90° (have the yellow tube going up) and trying to integrate then? You will get discs where the front and back are circular but the interior is square (integrate along the yellow face on the right side image) e.g. $$ \int_{-1}^{1} \int_{-\sqrt{1-x^2}}^{\sqrt{1-x^2}} 2\sqrt{1-y^2}\,dy\,dx $$ Should these two integrals be equal or am I doing something wrong?","Consider two intersecting cylinders. I know the regular way to do this is: This methods integrates the square sides of the solid that you get However, is it just as viable to flip the picture 90° (have the yellow tube going up) and trying to integrate then? You will get discs where the front and back are circular but the interior is square (integrate along the yellow face on the right side image) e.g. Should these two integrals be equal or am I doing something wrong?"," \int_{-1}^{1} \left(2\sqrt{1-2x^2}\right)^2\,dx = \frac{16}{3}   \int_{-1}^{1} \int_{-\sqrt{1-x^2}}^{\sqrt{1-x^2}} 2\sqrt{1-y^2}\,dy\,dx ","['integration', 'geometry', 'multivariable-calculus']"
7,Extrema under constraints,Extrema under constraints,,"Find the critical points of the function $f(x_1, x_2)=x_1x_2$ under the constraint $2x_1+x_2=b$. Using the method of Lagrange multipliers I got the following: \begin{equation*}L(x_1,x_2,\lambda )=x_1x_2-\lambda \cdot \left (2x_1+x_2-b\right )\end{equation*} \begin{align*}&L_{x_1}(x_1,x_2,\lambda)=0 \Rightarrow x_2-2\lambda=0  \\ & L_{x_2}(x_1,x_2,\lambda)=0 \Rightarrow x_1-\lambda=0  \\ &  L_{\lambda}(x_1,x_2,\lambda)=0 \Rightarrow -\left (2x_1+x_2-b\right )=0\end{align*} Solving this system we get the critical point $\left (\frac{b}{4}, \frac{b}{2}\right )$. To check what extrema (if there exists) it is, we do the following:  $$f_{x_1} =x_2 , \ f_{x_2}=x_1 , \ f_{x_1x_1}=0 . \ f_{x_1x_2}=1 , \ f_{x_2x_2}=0$$ Then:  \begin{equation*}f_{x_1x_2}\left (\frac{b}{4}, \frac{b}{2}\right )=1>0 \ \text{ and } \ f_{x_1x_1}\left (\frac{b}{4}, \frac{b}{2}\right )f_{x_2x_2}\left (\frac{b}{4}, \frac{b}{2}\right )-\left (f_{x_1x_2}\left (\frac{b}{4}, \frac{b}{2}\right )\right )^2=0\cdot 0-1=-1<0\end{equation*} Therefore, $\left (\frac{b}{4}, \frac{b}{2}\right )$ is a saddle point. Is this correct? Because at Wolfram there are some maxima. $$$$ Then I want to check if there are extrema if we have an other constraint, $\{(x_1, x_2)\in \mathbb{R}^2 \mid x_1\geq 0, x_2\geq 0\}$. A critical point is \begin{equation*}\nabla f=\begin{pmatrix}0 \\ 0\end{pmatrix}\Rightarrow \begin{pmatrix}x_2 \\ x_1\end{pmatrix}=\begin{pmatrix}0 \\ 0\end{pmatrix} \Rightarrow x_1=0 \ \text{ und } \ x_2=0\end{equation*} But this point is again a saddle point, right? Then I wan to check if there are extrema if we have an other constraint, $\{(x_1, x_2)\in \mathbb{R}^2 \mid x_1\geq 0, x_2\geq 0\}$. A critival point is \begin{equation*}\nabla f=\begin{pmatrix}0 \\ 0\end{pmatrix}\Rightarrow \begin{pmatrix}x_2 \\ x_1\end{pmatrix}=\begin{pmatrix}0 \\ 0\end{pmatrix} \Rightarrow x_1=0 \ \text{ and } \ x_2=0\end{equation*} But this point is again a saddle point, right?","Find the critical points of the function $f(x_1, x_2)=x_1x_2$ under the constraint $2x_1+x_2=b$. Using the method of Lagrange multipliers I got the following: \begin{equation*}L(x_1,x_2,\lambda )=x_1x_2-\lambda \cdot \left (2x_1+x_2-b\right )\end{equation*} \begin{align*}&L_{x_1}(x_1,x_2,\lambda)=0 \Rightarrow x_2-2\lambda=0  \\ & L_{x_2}(x_1,x_2,\lambda)=0 \Rightarrow x_1-\lambda=0  \\ &  L_{\lambda}(x_1,x_2,\lambda)=0 \Rightarrow -\left (2x_1+x_2-b\right )=0\end{align*} Solving this system we get the critical point $\left (\frac{b}{4}, \frac{b}{2}\right )$. To check what extrema (if there exists) it is, we do the following:  $$f_{x_1} =x_2 , \ f_{x_2}=x_1 , \ f_{x_1x_1}=0 . \ f_{x_1x_2}=1 , \ f_{x_2x_2}=0$$ Then:  \begin{equation*}f_{x_1x_2}\left (\frac{b}{4}, \frac{b}{2}\right )=1>0 \ \text{ and } \ f_{x_1x_1}\left (\frac{b}{4}, \frac{b}{2}\right )f_{x_2x_2}\left (\frac{b}{4}, \frac{b}{2}\right )-\left (f_{x_1x_2}\left (\frac{b}{4}, \frac{b}{2}\right )\right )^2=0\cdot 0-1=-1<0\end{equation*} Therefore, $\left (\frac{b}{4}, \frac{b}{2}\right )$ is a saddle point. Is this correct? Because at Wolfram there are some maxima. $$$$ Then I want to check if there are extrema if we have an other constraint, $\{(x_1, x_2)\in \mathbb{R}^2 \mid x_1\geq 0, x_2\geq 0\}$. A critical point is \begin{equation*}\nabla f=\begin{pmatrix}0 \\ 0\end{pmatrix}\Rightarrow \begin{pmatrix}x_2 \\ x_1\end{pmatrix}=\begin{pmatrix}0 \\ 0\end{pmatrix} \Rightarrow x_1=0 \ \text{ und } \ x_2=0\end{equation*} But this point is again a saddle point, right? Then I wan to check if there are extrema if we have an other constraint, $\{(x_1, x_2)\in \mathbb{R}^2 \mid x_1\geq 0, x_2\geq 0\}$. A critival point is \begin{equation*}\nabla f=\begin{pmatrix}0 \\ 0\end{pmatrix}\Rightarrow \begin{pmatrix}x_2 \\ x_1\end{pmatrix}=\begin{pmatrix}0 \\ 0\end{pmatrix} \Rightarrow x_1=0 \ \text{ and } \ x_2=0\end{equation*} But this point is again a saddle point, right?",,"['calculus', 'multivariable-calculus', 'optimization', 'lagrange-multiplier', 'constraints']"
8,Symmetry of higher order mixed partial derivatives under weaker assumptions,Symmetry of higher order mixed partial derivatives under weaker assumptions,,"I have asked this question on Mathoverflow, but it did not receive much attention there. Suppose $U$ is an open subset of $\mathbb{R}^n$, and $f:U\to \mathbb{R}$. When $f$ is $C^2$ we know that the mixed partial derivatives are symmetric, i.e.  $\partial_i\partial_jf= \partial_j\partial_if.$  But as it is famous  the continuity of the 2nd order partial derivatives is not necessary for this to happen. For example if $\partial_if$, $\partial_jf$ exist on $U$ and they are both differentiable (in the sense of Fréchet ) at some point $a\in U$ then  $$\partial_i\partial_jf(a)= \partial_j\partial_if(a).$$ Now for the 3rd order partial derivatives we can obtain the symmetry if we assume that the 1st order partial derivatives of $f$ are differentiable on $U$ and its 2nd order partial derivatives are differentiable at $a$. Let me explain the proof for the particular case  $$\partial_3\partial_2\partial_1f(a)= \partial_2\partial_1 \partial_3f(a).\tag{$\star$}$$ First as $\partial_1 f$ has 1st order partial derivatives in $U$ and they are differentiable at $a$ we have  $$\partial_3\partial_2\partial_1f(a)= \partial_2\partial_3 \partial_1f(a).\tag{1}$$ Then since the 1st order partial derivatives of $f$ are differentiable in $U$ we have $\partial_3\partial_1f(x)= \partial_1\partial_3 f(x)$ for all $x\in U$. Hence we can differentiate to obtain  $$\partial_2\partial_3\partial_1f(a)= \partial_2\partial_1 \partial_3f(a).\tag{2}$$ By combining (1) and (2) we get ($\star$). As you can see the full force of differentiability of the 1st order partial derivatives of $f$ on all of $U$ is only used for the equality of the 3rd order partial derivatives appeared in (2). So my question is Question: Can we prove the symmetry of 3rd order mixed partial derivatives of $f$ at $a$ by merely assuming that the 1st and 2nd order partial derivatives of $f$ exist on $U$ and they are all differentiable at $a$ ? If not, can you provide a counterexample? Finally, if the answer is positive, can we generalize it to higher order mixed partial derivatives?","I have asked this question on Mathoverflow, but it did not receive much attention there. Suppose $U$ is an open subset of $\mathbb{R}^n$, and $f:U\to \mathbb{R}$. When $f$ is $C^2$ we know that the mixed partial derivatives are symmetric, i.e.  $\partial_i\partial_jf= \partial_j\partial_if.$  But as it is famous  the continuity of the 2nd order partial derivatives is not necessary for this to happen. For example if $\partial_if$, $\partial_jf$ exist on $U$ and they are both differentiable (in the sense of Fréchet ) at some point $a\in U$ then  $$\partial_i\partial_jf(a)= \partial_j\partial_if(a).$$ Now for the 3rd order partial derivatives we can obtain the symmetry if we assume that the 1st order partial derivatives of $f$ are differentiable on $U$ and its 2nd order partial derivatives are differentiable at $a$. Let me explain the proof for the particular case  $$\partial_3\partial_2\partial_1f(a)= \partial_2\partial_1 \partial_3f(a).\tag{$\star$}$$ First as $\partial_1 f$ has 1st order partial derivatives in $U$ and they are differentiable at $a$ we have  $$\partial_3\partial_2\partial_1f(a)= \partial_2\partial_3 \partial_1f(a).\tag{1}$$ Then since the 1st order partial derivatives of $f$ are differentiable in $U$ we have $\partial_3\partial_1f(x)= \partial_1\partial_3 f(x)$ for all $x\in U$. Hence we can differentiate to obtain  $$\partial_2\partial_3\partial_1f(a)= \partial_2\partial_1 \partial_3f(a).\tag{2}$$ By combining (1) and (2) we get ($\star$). As you can see the full force of differentiability of the 1st order partial derivatives of $f$ on all of $U$ is only used for the equality of the 3rd order partial derivatives appeared in (2). So my question is Question: Can we prove the symmetry of 3rd order mixed partial derivatives of $f$ at $a$ by merely assuming that the 1st and 2nd order partial derivatives of $f$ exist on $U$ and they are all differentiable at $a$ ? If not, can you provide a counterexample? Finally, if the answer is positive, can we generalize it to higher order mixed partial derivatives?",,"['real-analysis', 'multivariable-calculus', 'partial-derivative']"
9,"$G$ be a subgroup of $GL(n,\mathbb R)$ such that $G$ is also a surface , does there exist a no where vanishing tangent vector field on $G$?","be a subgroup of  such that  is also a surface , does there exist a no where vanishing tangent vector field on ?","G GL(n,\mathbb R) G G","Let $G$ be a subgroup of $GL(n,\mathbb R)$ which is also a $m$-surface for some $m$, how to find a no where vanishing tangent vector field for $G$ ?","Let $G$ be a subgroup of $GL(n,\mathbb R)$ which is also a $m$-surface for some $m$, how to find a no where vanishing tangent vector field for $G$ ?",,['multivariable-calculus']
10,Chain rule using total derivative,Chain rule using total derivative,,"Let $\Omega,\hat{\Omega}\subset\mathbb{R}^n$ be two bounded open connected sets with Lipschitz boundary related by the following relation. There is an affine transformation  $$ T:\hat{\Omega}\to\Omega\\ \qquad\;\;\:\: \hat{x}\mapsto B\hat{x}+b, $$ where $B$ is an invertible matrix. Let now consider a function $u:\Omega\to\mathbb{R}$ and its pull-back $\hat{u}:=u\circ T:\hat{\Omega}\to \mathbb{R}$. Suppose that both $u,\hat{u}$ are smooth enough functions to consider derivatives up to order $k$. I read that: For every $p\in \Omega$, $D^ku(p)\in {(\mathbb{R}^n)^\ast}^{\otimes  k}$ (space of $k$-linear maps from   $\mathbb{R}^n\times\dots\times\mathbb{R}^n$ to $\mathbb{R}$) and   moreover it's symmetric. Now, what I don't understand is the following relation. For every $p\in\hat{\Omega}$, $$ D^k\hat{u}(\hat{p})(\xi_1,\dots,\xi_k)=D^k  u(T\hat{p})(B\xi_1,\dots,B\xi_k), $$ for any $\xi_i\in\mathbb{R^n}$.","Let $\Omega,\hat{\Omega}\subset\mathbb{R}^n$ be two bounded open connected sets with Lipschitz boundary related by the following relation. There is an affine transformation  $$ T:\hat{\Omega}\to\Omega\\ \qquad\;\;\:\: \hat{x}\mapsto B\hat{x}+b, $$ where $B$ is an invertible matrix. Let now consider a function $u:\Omega\to\mathbb{R}$ and its pull-back $\hat{u}:=u\circ T:\hat{\Omega}\to \mathbb{R}$. Suppose that both $u,\hat{u}$ are smooth enough functions to consider derivatives up to order $k$. I read that: For every $p\in \Omega$, $D^ku(p)\in {(\mathbb{R}^n)^\ast}^{\otimes  k}$ (space of $k$-linear maps from   $\mathbb{R}^n\times\dots\times\mathbb{R}^n$ to $\mathbb{R}$) and   moreover it's symmetric. Now, what I don't understand is the following relation. For every $p\in\hat{\Omega}$, $$ D^k\hat{u}(\hat{p})(\xi_1,\dots,\xi_k)=D^k  u(T\hat{p})(B\xi_1,\dots,B\xi_k), $$ for any $\xi_i\in\mathbb{R^n}$.",,"['real-analysis', 'multivariable-calculus']"
11,"Changing order of integration for the triple integral $\int_{0}^{1}dx\int_{0}^{1-x}dy\int_{0}^{x+y}f(x,y,z)dz$",Changing order of integration for the triple integral,"\int_{0}^{1}dx\int_{0}^{1-x}dy\int_{0}^{x+y}f(x,y,z)dz","In questions about changing the order of integration for triple integrals on this website, I see most answers (like this one ) give as a rule of thumb, that the outermost variable in the new order should be in the maximal domain. That is, its minimum value should be the smallest, and its maximum value should be the largest, given the limits of the other two variables in the old order. But I have an integral where this rule isn't so clear: I should change this integral: $$\int_{0}^{1}dx\int_{0}^{1-x}dy\int_{0}^{x+y}f(x,y,z)dz$$ to this integral: $$\int dz\int dx\int f(x,y,z)dy$$ with matching limits of integration. Since $z$ is the outermost variable in the new order, let's start with finding its maximal domain. There are two options. Since $0\leq z\leq x+y$ and $0\leq y\leq 1-x$ , we have $x\leq y+x \leq 1$ so $0\leq z \leq 1$ . Since $0\leq x\leq 1$ and $0\leq y\leq 1-x$ , we have $0\leq y\leq 1$ and so $0\leq z\leq 2$ . I think option $1$ is correct, since we are interested in the maximal value of $x+y$ , which is given by $1$ in the corresponding inequality. Now, next: I need to find the limits of integration of $x$ . Given a constant $z$ , we get $z-y\leq x\leq 1-y$ (by the two relevant inequalities). But we want only $z$ to be in this inequality. So we have $z-x \leq y\leq 1-x$ , and plugging that in gives $z+x-1 \leq x \leq 1+x-z$ , which gives $z-1\leq 0\leq 1-z$ , and $x$ is eliminated. How do I get around that? (without having to draw the domain) Any hint, tip or general approach would be greatly apreciated.","In questions about changing the order of integration for triple integrals on this website, I see most answers (like this one ) give as a rule of thumb, that the outermost variable in the new order should be in the maximal domain. That is, its minimum value should be the smallest, and its maximum value should be the largest, given the limits of the other two variables in the old order. But I have an integral where this rule isn't so clear: I should change this integral: to this integral: with matching limits of integration. Since is the outermost variable in the new order, let's start with finding its maximal domain. There are two options. Since and , we have so . Since and , we have and so . I think option is correct, since we are interested in the maximal value of , which is given by in the corresponding inequality. Now, next: I need to find the limits of integration of . Given a constant , we get (by the two relevant inequalities). But we want only to be in this inequality. So we have , and plugging that in gives , which gives , and is eliminated. How do I get around that? (without having to draw the domain) Any hint, tip or general approach would be greatly apreciated.","\int_{0}^{1}dx\int_{0}^{1-x}dy\int_{0}^{x+y}f(x,y,z)dz \int dz\int dx\int f(x,y,z)dy z 0\leq z\leq x+y 0\leq y\leq 1-x x\leq y+x \leq 1 0\leq z \leq 1 0\leq x\leq 1 0\leq y\leq 1-x 0\leq y\leq 1 0\leq z\leq 2 1 x+y 1 x z z-y\leq x\leq 1-y z z-x \leq y\leq 1-x z+x-1 \leq x \leq 1+x-z z-1\leq 0\leq 1-z x","['integration', 'multivariable-calculus', 'definite-integrals']"
12,Why $\frac{\partial f}{\partial v}=\sum_{i=1}^n\alpha_i\frac{\partial f}{\partial x_i}$,Why,\frac{\partial f}{\partial v}=\sum_{i=1}^n\alpha_i\frac{\partial f}{\partial x_i},"Let $U\subset \mathbb R^n$ be an open set and $f:U\to \mathbb R^m$ and suppose $\alpha=(\alpha_1,\ldots,\alpha_n)\in \mathbb R^n$ I'm trying to prove that $\frac{\partial f}{\partial v}(\alpha)=\sum_{i=1}^n\alpha_i\frac{\partial f}{\partial x_i}(\alpha)$. I know that the directional derivative is $$\frac{\partial f}{\partial v}(\alpha)=\lim_{t\to 0}\frac{f(\alpha+tv)-f(\alpha)}{t}$$ and the partial derivatives are $$\frac{\partial f}{\partial x_j}(\alpha)=\lim_{t\to 0}\frac{f(\alpha+te_j)-f(\alpha)}{t}$$ I've just written down these formulas without any success.","Let $U\subset \mathbb R^n$ be an open set and $f:U\to \mathbb R^m$ and suppose $\alpha=(\alpha_1,\ldots,\alpha_n)\in \mathbb R^n$ I'm trying to prove that $\frac{\partial f}{\partial v}(\alpha)=\sum_{i=1}^n\alpha_i\frac{\partial f}{\partial x_i}(\alpha)$. I know that the directional derivative is $$\frac{\partial f}{\partial v}(\alpha)=\lim_{t\to 0}\frac{f(\alpha+tv)-f(\alpha)}{t}$$ and the partial derivatives are $$\frac{\partial f}{\partial x_j}(\alpha)=\lim_{t\to 0}\frac{f(\alpha+te_j)-f(\alpha)}{t}$$ I've just written down these formulas without any success.",,"['real-analysis', 'multivariable-calculus']"
13,Inequality for Hardy-Littlewood type function,Inequality for Hardy-Littlewood type function,,"Consider $r\gt0,x\in R^n$ and the average  of a 1-Lipschitz function $u:R^n\longrightarrow R$ over the $B(x,r) $ $$u_{r}(x)=\frac{1}{Vol(B(x,r))}\int_{B(x,r)}u(z)dz$$ Then there is an absolute constant such that $$\Vert\nabla{u_{r}}(x)-\nabla{u_{r}(y)}\Vert_{2}\leq{\frac{c\sqrt{n}}{r}}\Vert{x-y}\Vert_{2}$$ we can transform and use the unit ball instead along with $u(x+rz)$ and Leibniz rule to differentiate under the integral.Then the $\sqrt{n} $ comes from bounding the norm of the gradient of $u$...but i can't get to the ratio $1/r$ which seems to come from the ratio of surface/volume...","Consider $r\gt0,x\in R^n$ and the average  of a 1-Lipschitz function $u:R^n\longrightarrow R$ over the $B(x,r) $ $$u_{r}(x)=\frac{1}{Vol(B(x,r))}\int_{B(x,r)}u(z)dz$$ Then there is an absolute constant such that $$\Vert\nabla{u_{r}}(x)-\nabla{u_{r}(y)}\Vert_{2}\leq{\frac{c\sqrt{n}}{r}}\Vert{x-y}\Vert_{2}$$ we can transform and use the unit ball instead along with $u(x+rz)$ and Leibniz rule to differentiate under the integral.Then the $\sqrt{n} $ comes from bounding the norm of the gradient of $u$...but i can't get to the ratio $1/r$ which seems to come from the ratio of surface/volume...",,"['multivariable-calculus', 'harmonic-analysis', 'lipschitz-functions', 'geometric-inequalities']"
14,Evaluating a double integral of a complicated rational function,Evaluating a double integral of a complicated rational function,,"Define the function $Q:\mathbb{C}^{2}\rightarrow\mathbb{C}$ to be the binary quadratic form, $$Q{\left(z,w\right)}:=z^{2}+w^{2}.\tag{1a}$$ Also, define $P:\mathbb{C}^{4}\rightarrow\mathbb{C}$ to be the polynomial of degree $5$, in four variables, $$\begin{align} P{\left(a,b,x,y\right)} &:=a\left[\left(a^{2}+1\right)\left(a^{2}+b^{2}+1\right)-4b^{2}\right]\\ &~~~~~+2\left[\left(a^{2}+1\right)\left(a^{2}+b^{2}+1\right)-2b^{2}\right]x\\ &~~~~~+a\left(a^{2}+b^{2}+1\right)x^{2}\\ &~~~~~+a\left(a^{2}+b^{2}+1\right)y^{2}.\tag{1b}\\ \end{align}$$ Note that $P{\left(a,b,x,y\right)}$ is obviously even in both $b$ and $y$. Then, define the function $\mathcal{I}:\mathbb{R}^{2}\rightarrow\mathbb{R}$ via the double integral $$\mathcal{I}{\left(a,b\right)}:=\int_{-\infty}^{\infty}\mathrm{d}x\int_{0}^{\infty}\mathrm{d}y\,\frac{2^{4}xy\,P{\left(a,b,x,y\right)}}{Q{\left(a+x,1-y\right)}\,Q{\left(a+x,1+y\right)}\,Q{\left(x,b-y\right)}\,Q{\left(x,b+y\right)}}.\tag{1c}$$ It's not hard to show then that $\mathcal{I}{\left(a,b\right)}$ is even in the second parameter $b$: $$\mathcal{I}{\left(a,-b\right)}=\mathcal{I}{\left(a,b\right)};~~~\small{\left(a,b\right)\in\mathbb{R}^{2}}.$$ Problem: Given the pair of real parameters $\left(a,b\right)\in\mathbb{R}\times\mathbb{R}_{\ge0}$, find a closed form expression for the double integral $\mathcal{I}{\left(a,b\right)}$ in terms of elementary functions. The obstacle in the way of solving this problem appears to be tedium more than anything else. Integrating the rational integrand in $(1c)$ over $x$ should in principle yield a piecewise rational function. Thus, subsequent integration over $y$ should lead to a function that is at the very least piecewise elementary, if not simpler. However, attempting to solve the problem by brute force with partial fraction decompositions quickly leads to large numbers of cumbersome expressions, rending the integral quite unmanageable without a program such as Mathematica. It is my hope that there is some cleverly efficient approach to this integral that I'm just not seeing at the moment. Any advice here would be welcome. Cheers!","Define the function $Q:\mathbb{C}^{2}\rightarrow\mathbb{C}$ to be the binary quadratic form, $$Q{\left(z,w\right)}:=z^{2}+w^{2}.\tag{1a}$$ Also, define $P:\mathbb{C}^{4}\rightarrow\mathbb{C}$ to be the polynomial of degree $5$, in four variables, $$\begin{align} P{\left(a,b,x,y\right)} &:=a\left[\left(a^{2}+1\right)\left(a^{2}+b^{2}+1\right)-4b^{2}\right]\\ &~~~~~+2\left[\left(a^{2}+1\right)\left(a^{2}+b^{2}+1\right)-2b^{2}\right]x\\ &~~~~~+a\left(a^{2}+b^{2}+1\right)x^{2}\\ &~~~~~+a\left(a^{2}+b^{2}+1\right)y^{2}.\tag{1b}\\ \end{align}$$ Note that $P{\left(a,b,x,y\right)}$ is obviously even in both $b$ and $y$. Then, define the function $\mathcal{I}:\mathbb{R}^{2}\rightarrow\mathbb{R}$ via the double integral $$\mathcal{I}{\left(a,b\right)}:=\int_{-\infty}^{\infty}\mathrm{d}x\int_{0}^{\infty}\mathrm{d}y\,\frac{2^{4}xy\,P{\left(a,b,x,y\right)}}{Q{\left(a+x,1-y\right)}\,Q{\left(a+x,1+y\right)}\,Q{\left(x,b-y\right)}\,Q{\left(x,b+y\right)}}.\tag{1c}$$ It's not hard to show then that $\mathcal{I}{\left(a,b\right)}$ is even in the second parameter $b$: $$\mathcal{I}{\left(a,-b\right)}=\mathcal{I}{\left(a,b\right)};~~~\small{\left(a,b\right)\in\mathbb{R}^{2}}.$$ Problem: Given the pair of real parameters $\left(a,b\right)\in\mathbb{R}\times\mathbb{R}_{\ge0}$, find a closed form expression for the double integral $\mathcal{I}{\left(a,b\right)}$ in terms of elementary functions. The obstacle in the way of solving this problem appears to be tedium more than anything else. Integrating the rational integrand in $(1c)$ over $x$ should in principle yield a piecewise rational function. Thus, subsequent integration over $y$ should lead to a function that is at the very least piecewise elementary, if not simpler. However, attempting to solve the problem by brute force with partial fraction decompositions quickly leads to large numbers of cumbersome expressions, rending the integral quite unmanageable without a program such as Mathematica. It is my hope that there is some cleverly efficient approach to this integral that I'm just not seeing at the moment. Any advice here would be welcome. Cheers!",,"['calculus', 'integration', 'multivariable-calculus', 'definite-integrals', 'closed-form']"
15,Difficulty proving chain rule for composition of linear map with function,Difficulty proving chain rule for composition of linear map with function,,"I am trying to prove that for $f:V\rightarrow W$ differentiable at $\alpha\in V$ and $T:W\rightarrow Z$ linear, we have  $$ d(T\circ f)_\alpha=T\circ Df_\alpha $$ by showing  $$ \frac{||T[f(x)-f(\alpha)-Df_\alpha(x-\alpha)]||}{||x-\alpha||}\rightarrow0,x\rightarrow\alpha $$ but am struggling. Any tips on how to estimate the above difference? I am doing this just for fun so I would prefer just a hint.","I am trying to prove that for $f:V\rightarrow W$ differentiable at $\alpha\in V$ and $T:W\rightarrow Z$ linear, we have  $$ d(T\circ f)_\alpha=T\circ Df_\alpha $$ by showing  $$ \frac{||T[f(x)-f(\alpha)-Df_\alpha(x-\alpha)]||}{||x-\alpha||}\rightarrow0,x\rightarrow\alpha $$ but am struggling. Any tips on how to estimate the above difference? I am doing this just for fun so I would prefer just a hint.",,"['calculus', 'real-analysis', 'multivariable-calculus']"
16,"Find the values of $c\neq 0$, such that $P(x,y)>0$","Find the values of , such that","c\neq 0 P(x,y)>0","Consider the polynomial $P:\mathbb{R}^{2}\to\mathbb{R}$ given by, $P(x,y)=1+c(x^3+y^3)+(x^2+y^2)^2$. Find all values of $c\neq 0$, such that $P(x,y)>0$, for all $(x,y)\in\mathbb{R}^{2}$ My approach: Clearly $(x^2+y^2)^2$ is positive. However, how should I determine the values of c such that $P(x,y)$ is positive, because the cubic terms could be negative or positive?","Consider the polynomial $P:\mathbb{R}^{2}\to\mathbb{R}$ given by, $P(x,y)=1+c(x^3+y^3)+(x^2+y^2)^2$. Find all values of $c\neq 0$, such that $P(x,y)>0$, for all $(x,y)\in\mathbb{R}^{2}$ My approach: Clearly $(x^2+y^2)^2$ is positive. However, how should I determine the values of c such that $P(x,y)$ is positive, because the cubic terms could be negative or positive?",,['multivariable-calculus']
17,Derivative of Integral with Function in Bound,Derivative of Integral with Function in Bound,,"I need to evaluate the following derivative in the scope of a PDE class $\frac{d}{dt}\int_a^{x_s(t)} \rho(x,t) \text dx$ For some reason I am not able to obtain the right answer using a simple chain rule with the Fundamental theorem of Calculus. Here is what I tried (where $\frac {\partial\mu}{\partial x} =\rho$). Please note the function has a jump discontinuity at $x_s(t)$. $\begin {align} \frac{d}{dt}\int_a^{x_s(t)} \rho(x,t) \text dx &= \frac{d}{dt} [\mu(x_s(t),t)-\mu(a,t)]\\ &= \frac{dx_s}{dt}\frac {\partial\mu}{\partial x}+\frac{\partial\mu}{\partial t}-(\frac{da}{dt}\frac {\partial\mu}{\partial x}+\frac{\partial\mu}{\partial t})\\ &= \frac{dx_s}{dt}\frac {\partial\mu}{\partial x}\\ &= \frac{dx_s}{dt}\rho \end{align}$ Which is only one term of what I should get. Any hint of where I did something wrong is appreciated, this is making me feel like I forgot calculus... EDIT: Here is what I should get $\frac{d}{dt}\int_a^{x_s(t)} \rho(x,t) \text dx = \int_a^{x_s(t)} \frac{\partial\rho}{\partial t} \text dx + \frac{dx_s}{dt}\rho$","I need to evaluate the following derivative in the scope of a PDE class $\frac{d}{dt}\int_a^{x_s(t)} \rho(x,t) \text dx$ For some reason I am not able to obtain the right answer using a simple chain rule with the Fundamental theorem of Calculus. Here is what I tried (where $\frac {\partial\mu}{\partial x} =\rho$). Please note the function has a jump discontinuity at $x_s(t)$. $\begin {align} \frac{d}{dt}\int_a^{x_s(t)} \rho(x,t) \text dx &= \frac{d}{dt} [\mu(x_s(t),t)-\mu(a,t)]\\ &= \frac{dx_s}{dt}\frac {\partial\mu}{\partial x}+\frac{\partial\mu}{\partial t}-(\frac{da}{dt}\frac {\partial\mu}{\partial x}+\frac{\partial\mu}{\partial t})\\ &= \frac{dx_s}{dt}\frac {\partial\mu}{\partial x}\\ &= \frac{dx_s}{dt}\rho \end{align}$ Which is only one term of what I should get. Any hint of where I did something wrong is appreciated, this is making me feel like I forgot calculus... EDIT: Here is what I should get $\frac{d}{dt}\int_a^{x_s(t)} \rho(x,t) \text dx = \int_a^{x_s(t)} \frac{\partial\rho}{\partial t} \text dx + \frac{dx_s}{dt}\rho$",,"['multivariable-calculus', 'partial-differential-equations', 'chain-rule']"
18,How does this prove the function has derivatives in all directions?,How does this prove the function has derivatives in all directions?,,"This is the function I'm analyzing: $$ g(x,y)= \begin{cases} \dfrac{xy(x^2-y^2)}{x^2+y^2} & \text{if } (x,y)\neq(0,0)\\ 0 & \text{if } (x,y) = (0,0)\\ \end{cases} $$ I need to prove it has derivatives in all directions at $(0,0)$, so I applied the definition using a generic vector $v=(A,B)$: $$ \lim_{h\to 0} \frac {g(0+hA;0+hB)-g(0,0)}{h} $$ and I finally got to $A^3Bh-AB^3h$ Now I'm not sure what the conclusion is. Have I proven the function has derivatives in all directions at $(0,0)$? Also, how can I tell a function doesn't have derivatives in all directions? EDIT: corrected my result to add the missing $h$. However, my question remains the same: how does this prove the directional derivatives exist? What would a result be if they didn't exist? Thanks.","This is the function I'm analyzing: $$ g(x,y)= \begin{cases} \dfrac{xy(x^2-y^2)}{x^2+y^2} & \text{if } (x,y)\neq(0,0)\\ 0 & \text{if } (x,y) = (0,0)\\ \end{cases} $$ I need to prove it has derivatives in all directions at $(0,0)$, so I applied the definition using a generic vector $v=(A,B)$: $$ \lim_{h\to 0} \frac {g(0+hA;0+hB)-g(0,0)}{h} $$ and I finally got to $A^3Bh-AB^3h$ Now I'm not sure what the conclusion is. Have I proven the function has derivatives in all directions at $(0,0)$? Also, how can I tell a function doesn't have derivatives in all directions? EDIT: corrected my result to add the missing $h$. However, my question remains the same: how does this prove the directional derivatives exist? What would a result be if they didn't exist? Thanks.",,"['multivariable-calculus', 'derivatives']"
19,What does the co-factor of a matrix represent?,What does the co-factor of a matrix represent?,,What does the co-factor of a matrix represent. I am not asking for the formula to calculate co-factors or for their specific usage in matrix operations but rather for the meaning behind co-factors. What are the co-factors of a matrix in physical terms?,What does the co-factor of a matrix represent. I am not asking for the formula to calculate co-factors or for their specific usage in matrix operations but rather for the meaning behind co-factors. What are the co-factors of a matrix in physical terms?,,"['linear-algebra', 'multivariable-calculus']"
20,If the integral of a $k$ form over any $k$ chain is $0$ then the differential form itself is $0$?,If the integral of a  form over any  chain is  then the differential form itself is ?,k k 0 0,"Let $\omega$ be a $k$ form on an open set $A\subseteq \mathbb R^n$ such that for any $k$-chain $c$ in $A$ , $\int_c \omega =0$ , then is it true that $\omega =0$ ?","Let $\omega$ be a $k$ form on an open set $A\subseteq \mathbb R^n$ such that for any $k$-chain $c$ in $A$ , $\int_c \omega =0$ , then is it true that $\omega =0$ ?",,['integration']
21,Continuity - Why does it require open sets in this definition?,Continuity - Why does it require open sets in this definition?,,"My notes claim the following theorem: A function $f: \mathbb{R}^n \to \mathbb{R}^m$ is continuous if and only if whenever $U \subseteq R^m$ is an open set, then $f^{-1}(U) \subseteq \mathbb{R}^n$ is an open set. Why do we require both sets to be open? What if they were closed?","My notes claim the following theorem: A function $f: \mathbb{R}^n \to \mathbb{R}^m$ is continuous if and only if whenever $U \subseteq R^m$ is an open set, then $f^{-1}(U) \subseteq \mathbb{R}^n$ is an open set. Why do we require both sets to be open? What if they were closed?",,['multivariable-calculus']
22,Is this a valid proof of Young's theorem?,Is this a valid proof of Young's theorem?,,"I tried to prove Young's theorem (symmetry of mixed partial derivatives) myself, but my proof seems considerably easier than the one I could find in my textbook. Can anybody point out where I went wrong? Let $f$ be a $C^2$ function, so that the first two derivatives of $f$ are continues  and that $f$ is two times differentiable in a point $(a,b)$. We wish to show that $\frac{\partial^2 f}{\partial x \partial y}(a,b)=\frac{\partial^2 f}{\partial y \partial x}(a,b)$. Define $$\Delta(h,k)=f(a+h,b+k)-f(a,b+k)-f(a+h,b)+f(a,b)$$ Notice that  $$\lim_{h\rightarrow 0} \frac{\Delta(h,k)}{h}=\lim_{h \rightarrow 0}(\frac{f(a+h,b+k)-f(a,b+k)}{h}-\frac{f(a+h,b)-f(a,b)}{h})=$$ $$\frac{\partial f(a,b+k)}{\partial x}-\frac{\partial f(a,b)}{\partial x}$$ and  $$\lim_{k \rightarrow 0} \frac{1}{k}\cdot \left (\lim_{h\rightarrow 0} \frac{\Delta(h,k)}{h} \right )=\lim_{k \rightarrow 0} \frac{\frac{\partial f(a,b+k)}{\partial x}-\frac{\partial f(a,b)}{\partial x}}{k}=\frac{\partial}{\partial y}(\frac{\partial f(a,b)}{\partial x})=\frac{\partial^2 f(a,b)}{\partial y \partial x}$$ Likewise, taking the limits in opposite order gives $$\lim_{h \rightarrow 0}(\lim_{k \rightarrow 0}\Delta(h,k))=\frac{\partial^2 f(a,b)}{\partial x \partial y}$$ I then conclude that: $$\lim_{(h,k) \rightarrow (0,0)} \frac{\Delta(h,k)}{hk}=\frac{\partial^2 f(a,b)}{\partial y \partial x}=\frac{\partial^2 f(a,b)}{\partial x \partial y}$$ as $f$ is twice differentiable at $(a,b)$, so it shouldn't matter how we approach $(a,b)$. Is this argument correct?","I tried to prove Young's theorem (symmetry of mixed partial derivatives) myself, but my proof seems considerably easier than the one I could find in my textbook. Can anybody point out where I went wrong? Let $f$ be a $C^2$ function, so that the first two derivatives of $f$ are continues  and that $f$ is two times differentiable in a point $(a,b)$. We wish to show that $\frac{\partial^2 f}{\partial x \partial y}(a,b)=\frac{\partial^2 f}{\partial y \partial x}(a,b)$. Define $$\Delta(h,k)=f(a+h,b+k)-f(a,b+k)-f(a+h,b)+f(a,b)$$ Notice that  $$\lim_{h\rightarrow 0} \frac{\Delta(h,k)}{h}=\lim_{h \rightarrow 0}(\frac{f(a+h,b+k)-f(a,b+k)}{h}-\frac{f(a+h,b)-f(a,b)}{h})=$$ $$\frac{\partial f(a,b+k)}{\partial x}-\frac{\partial f(a,b)}{\partial x}$$ and  $$\lim_{k \rightarrow 0} \frac{1}{k}\cdot \left (\lim_{h\rightarrow 0} \frac{\Delta(h,k)}{h} \right )=\lim_{k \rightarrow 0} \frac{\frac{\partial f(a,b+k)}{\partial x}-\frac{\partial f(a,b)}{\partial x}}{k}=\frac{\partial}{\partial y}(\frac{\partial f(a,b)}{\partial x})=\frac{\partial^2 f(a,b)}{\partial y \partial x}$$ Likewise, taking the limits in opposite order gives $$\lim_{h \rightarrow 0}(\lim_{k \rightarrow 0}\Delta(h,k))=\frac{\partial^2 f(a,b)}{\partial x \partial y}$$ I then conclude that: $$\lim_{(h,k) \rightarrow (0,0)} \frac{\Delta(h,k)}{hk}=\frac{\partial^2 f(a,b)}{\partial y \partial x}=\frac{\partial^2 f(a,b)}{\partial x \partial y}$$ as $f$ is twice differentiable at $(a,b)$, so it shouldn't matter how we approach $(a,b)$. Is this argument correct?",,"['calculus', 'multivariable-calculus', 'proof-verification']"
23,How to integrate this function over this surface?,How to integrate this function over this surface?,,"I'm solving a physics problem, and at some point I need to solve this integral $$\iint\limits_S \frac{1}{x+2a} dxdy$$ where $$S=\{(x,y,z):x^2+y^2\leq a^2, \ x\geq0, \ z=0\}$$ So $S$ is the right semicircle corresponding to the circle centered at the origin with radius $a$. This seemed a bit difficult so I decided to calculate the integral over the quarter circle in the first quadrant (let's call this surface $S'$) and then multiply that by two to get the value of the whole surface integral. Doing this, I got $$2\iint\limits_{S'} \frac{1}{x+2a} dxdy=2\int\limits_0^a dy\int\limits_0^{\sqrt{a^2-y^2}}\frac{1}{2a+x}dx=2\int\limits_0^a\ln\left(\sqrt{a^2-y^2}+2a\right)-\ln(2a)\ dy=\int\limits_0^a 2\ln\left(\sqrt{a^2-y^2}+2a\right)\ dy \ - a\ln(2a)$$ And this function doesn't seem to have a primitive, or at least it is very hard to find it (I looked up in some tables and found nothing). The problem shouldn't be so hard so I expect that this integral can be calculated in a much easier way. Any ideas?","I'm solving a physics problem, and at some point I need to solve this integral $$\iint\limits_S \frac{1}{x+2a} dxdy$$ where $$S=\{(x,y,z):x^2+y^2\leq a^2, \ x\geq0, \ z=0\}$$ So $S$ is the right semicircle corresponding to the circle centered at the origin with radius $a$. This seemed a bit difficult so I decided to calculate the integral over the quarter circle in the first quadrant (let's call this surface $S'$) and then multiply that by two to get the value of the whole surface integral. Doing this, I got $$2\iint\limits_{S'} \frac{1}{x+2a} dxdy=2\int\limits_0^a dy\int\limits_0^{\sqrt{a^2-y^2}}\frac{1}{2a+x}dx=2\int\limits_0^a\ln\left(\sqrt{a^2-y^2}+2a\right)-\ln(2a)\ dy=\int\limits_0^a 2\ln\left(\sqrt{a^2-y^2}+2a\right)\ dy \ - a\ln(2a)$$ And this function doesn't seem to have a primitive, or at least it is very hard to find it (I looked up in some tables and found nothing). The problem shouldn't be so hard so I expect that this integral can be calculated in a much easier way. Any ideas?",,"['calculus', 'integration', 'multivariable-calculus', 'definite-integrals', 'surface-integrals']"
24,"Integrate $\frac{x^3-3xy^2}{(x^2+y^2)^2}$ over $1\leq x^3-3xy^2\leq2,2\leq3x^2y-y^3\leq4$ and $x,y\geq0.$",Integrate  over  and,"\frac{x^3-3xy^2}{(x^2+y^2)^2} 1\leq x^3-3xy^2\leq2,2\leq3x^2y-y^3\leq4 x,y\geq0.","Integrate $\frac{x^3-3xy^2}{(x^2+y^2)^2}$ over $1\leq x^3-3xy^2\leq2,2\leq3x^2y-y^3\leq4$ and $x,y\geq0.$ Let $R$ be the region. I substitute $u=x^3-3xy^2,v=3x^2y-y^3$. $\det J(x,y)=9(x^2+y^2)^2.$ Also notice that $(x+yi)^3=u+vi$, so $(x^2+y^2)^3=u^2+v^2$. Then the rquired integral $$=\int_R\frac{x^3-3xy^2}{9(x^2+y^2)^4}9(x^2+y^2)^2dxdy$$ $$=\int^4_2\int^2_1\frac u{9(u^2+v^2)^{4/3}}dudv$$ $$=\int^4_2\frac 1 6[\frac 1 {(1+v^2)^{1/3}}-\frac 1{(4+v^2)^{1/3}}]dv$$ This has no closed form solution. I wonder where I did wrong... Thank you.","Integrate $\frac{x^3-3xy^2}{(x^2+y^2)^2}$ over $1\leq x^3-3xy^2\leq2,2\leq3x^2y-y^3\leq4$ and $x,y\geq0.$ Let $R$ be the region. I substitute $u=x^3-3xy^2,v=3x^2y-y^3$. $\det J(x,y)=9(x^2+y^2)^2.$ Also notice that $(x+yi)^3=u+vi$, so $(x^2+y^2)^3=u^2+v^2$. Then the rquired integral $$=\int_R\frac{x^3-3xy^2}{9(x^2+y^2)^4}9(x^2+y^2)^2dxdy$$ $$=\int^4_2\int^2_1\frac u{9(u^2+v^2)^{4/3}}dudv$$ $$=\int^4_2\frac 1 6[\frac 1 {(1+v^2)^{1/3}}-\frac 1{(4+v^2)^{1/3}}]dv$$ This has no closed form solution. I wonder where I did wrong... Thank you.",,"['calculus', 'multivariable-calculus']"
25,Volume of a vector field embedded in 4D,Volume of a vector field embedded in 4D,,"Given a surface in 4D, $\bigl(s, t, u(s, t), v(s, t)\bigr)$, if we define vectors between each point $(s, t, 0, 0)$ in some region of the $(s, t)$-plane, and a corresponding point $\bigl(s, t, u(s, t), v(s, t)\bigr)$, it seems that these vectors would define a 3D volume in 4D space, assuming $u(s, t)$ and $v(s, t)$ are continuous and differentiable. Assuming that is in fact true, how would one calculate the volume? I don't have a specific application in mind, just a question that occurred to me musing about higher dimensional functions and space. I appreciate any insight. I'm an engineer, not a mathematician, so the terminology of modern mathematics will lose me quickly, although I have some very minimal exposure to the terminology of fiber bundles. Thanks in advance","Given a surface in 4D, $\bigl(s, t, u(s, t), v(s, t)\bigr)$, if we define vectors between each point $(s, t, 0, 0)$ in some region of the $(s, t)$-plane, and a corresponding point $\bigl(s, t, u(s, t), v(s, t)\bigr)$, it seems that these vectors would define a 3D volume in 4D space, assuming $u(s, t)$ and $v(s, t)$ are continuous and differentiable. Assuming that is in fact true, how would one calculate the volume? I don't have a specific application in mind, just a question that occurred to me musing about higher dimensional functions and space. I appreciate any insight. I'm an engineer, not a mathematician, so the terminology of modern mathematics will lose me quickly, although I have some very minimal exposure to the terminology of fiber bundles. Thanks in advance",,"['calculus', 'integration', 'multivariable-calculus']"
26,"Meaning of bounds in $ \iint_{0\leq x^2+y^2\leq 4} \, dx\, dy $",Meaning of bounds in," \iint_{0\leq x^2+y^2\leq 4} \, dx\, dy ","If the bounds are written as $$ \iint_{0\leq x^2+y^2\leq 4} \, dx\, dy $$ What does it mean? I want the bounds written as $$ \int_{x_\text{lower}}^{x_\text{upper}}\int_{y_\text{lower}}^{y_\text{upper}} \, dx\,dy $$ Same for a triple integral, if $$ \iiint_{2\leq x^2+y^2+z^2 \leq 6} \, dx\,dy\,dz $$ What is $ \int_{x_\text{lower}}^{x_\text{upper}}\int_{y_\text{lower}}^{y_\text{upper}} \int_{z_\text{lower}}^{z_\text{upper}} \, dx\,dy \,dz $? Thanks!","If the bounds are written as $$ \iint_{0\leq x^2+y^2\leq 4} \, dx\, dy $$ What does it mean? I want the bounds written as $$ \int_{x_\text{lower}}^{x_\text{upper}}\int_{y_\text{lower}}^{y_\text{upper}} \, dx\,dy $$ Same for a triple integral, if $$ \iiint_{2\leq x^2+y^2+z^2 \leq 6} \, dx\,dy\,dz $$ What is $ \int_{x_\text{lower}}^{x_\text{upper}}\int_{y_\text{lower}}^{y_\text{upper}} \int_{z_\text{lower}}^{z_\text{upper}} \, dx\,dy \,dz $? Thanks!",,"['calculus', 'multivariable-calculus']"
27,Adapt variation of variables respectively to their sum,Adapt variation of variables respectively to their sum,,"Excuse me for the weird title.  I'm trying to control a character with a joystick. The joystick gives me two normalized values for the two respective axis. I'm using those values to determine the velocity needed to move my character (in each axis). When I move sideways or forwards, I get $(1, 0)$ or $(0, 1)$. My problem is when I move digonally, the values I get are i.e. $(1, 1)$ which makes the character move faster. I looked for a way to normalize those values accordingly to their sum, as it should never be greater than $1$, but I couldn't find anything. It should leave the first 2 examples unchanged and change the last one to $(0.5, 0.5)$. Thank you","Excuse me for the weird title.  I'm trying to control a character with a joystick. The joystick gives me two normalized values for the two respective axis. I'm using those values to determine the velocity needed to move my character (in each axis). When I move sideways or forwards, I get $(1, 0)$ or $(0, 1)$. My problem is when I move digonally, the values I get are i.e. $(1, 1)$ which makes the character move faster. I looked for a way to normalize those values accordingly to their sum, as it should never be greater than $1$, but I couldn't find anything. It should leave the first 2 examples unchanged and change the last one to $(0.5, 0.5)$. Thank you",,"['functions', 'multivariable-calculus']"
28,Multivariable Taylor Series,Multivariable Taylor Series,,"I would like to show the validity of the multivariable version of Taylor series expansion up to second-order terms (if possible without using one of the explicit forms for the remainder term): Let $f : \mathbb{R}^n \to \mathbb{R}$ be twice differentiable at   $\vec{a} = (a_1, \ldots, a_n)$ and let $\vec{x} = (x_1, \ldots, x_n)$.   Then $f(x) = f(a) + \sum_{i=1}^{n} \frac{\partial f(a)}{\partial x_i}  (x_i-a_i) + \frac{1}{2}\sum_{i,j=1}^n \frac{\partial^2 f(a)}{\partial  x_i \partial x_j} (x_i-a_i)(x_j-a_j) + R(x)$ where $\lim_{x \to a} \frac{R(x)}{||x-a||^2} = 0$. I let $g(t) = f(a + t(x-a))$. Then I write $g$ in terms of its Taylor Series about $0$, evaluated at $t=1$ to get         $g(1) = g(0) + g'(0) + \frac{1}{2} g''(0)  + R(1)$.         Then, using the chain rule         \begin{align*} 			g(1) &= f(x) \\ 			g(0) &= f(a) \\ 			g'(0) &= \sum_{i=1}^n \frac{\partial f(a)}{\partial x_i} (x_i-a_i) \\ 			g''(0) &= \sum_{i,j=1}^n \frac{\partial^2 f(a)}{\partial x_i \partial x_j} (x_i-a_i)(x_j-a_j) 		\end{align*} Then letting $R(x) = R(1)$, how do I show that $\lim_{x \to a} \frac{R(x)}{||x-a||^2} = 0$ using the fact that $g(t) = g(0) + g'(0)t + \frac{1}{2} g''(0)t^2  + R(t)$ where $\lim_{x \to 0} \frac{R(t)}{t^2} = 0$? Or perhaps another approach is necessary?","I would like to show the validity of the multivariable version of Taylor series expansion up to second-order terms (if possible without using one of the explicit forms for the remainder term): Let $f : \mathbb{R}^n \to \mathbb{R}$ be twice differentiable at   $\vec{a} = (a_1, \ldots, a_n)$ and let $\vec{x} = (x_1, \ldots, x_n)$.   Then $f(x) = f(a) + \sum_{i=1}^{n} \frac{\partial f(a)}{\partial x_i}  (x_i-a_i) + \frac{1}{2}\sum_{i,j=1}^n \frac{\partial^2 f(a)}{\partial  x_i \partial x_j} (x_i-a_i)(x_j-a_j) + R(x)$ where $\lim_{x \to a} \frac{R(x)}{||x-a||^2} = 0$. I let $g(t) = f(a + t(x-a))$. Then I write $g$ in terms of its Taylor Series about $0$, evaluated at $t=1$ to get         $g(1) = g(0) + g'(0) + \frac{1}{2} g''(0)  + R(1)$.         Then, using the chain rule         \begin{align*} 			g(1) &= f(x) \\ 			g(0) &= f(a) \\ 			g'(0) &= \sum_{i=1}^n \frac{\partial f(a)}{\partial x_i} (x_i-a_i) \\ 			g''(0) &= \sum_{i,j=1}^n \frac{\partial^2 f(a)}{\partial x_i \partial x_j} (x_i-a_i)(x_j-a_j) 		\end{align*} Then letting $R(x) = R(1)$, how do I show that $\lim_{x \to a} \frac{R(x)}{||x-a||^2} = 0$ using the fact that $g(t) = g(0) + g'(0)t + \frac{1}{2} g''(0)t^2  + R(t)$ where $\lim_{x \to 0} \frac{R(t)}{t^2} = 0$? Or perhaps another approach is necessary?",,"['calculus', 'real-analysis', 'multivariable-calculus', 'taylor-expansion']"
29,Show that $σ_n$ converges uniformly to $σ$.,Show that  converges uniformly to .,σ_n σ,"Let $a$ and $b$ be two points of $\mathbb{R}^2$.Let $σ_n : [0, 1] → \mathbb{R}^2$ be a sequence of continuously differentiable constant speed curves with $||σ_n'(t)|| = L_n$ for all $t ∈ [0, 1]$ and $σ_n(0) = a$ and $σ_n(1) = b$ for all $n$. Suppose that $\lim_{n→∞} L_n = ||b − a||$. Show that $σ_n$ converges uniformly to $σ$, where $σ(t) = a + t(b − a)$ for $t ∈ [0, 1]$. My Try: Intuitively this is clear since it talks about a sequence of paths from $a$ to $b$ that converges to the straight line through $a$ and $b$. For a rigorous proof, I wanted to show first that $σ_n'(t)$ converges uniformly. But all I have is given $\epsilon>0$ there is $N$ such that $|   \;||σ_n'(t)||-||b − a||\;|<\epsilon$ for all $n>N$, which does not support what I need to prove. Any suggestion please..","Let $a$ and $b$ be two points of $\mathbb{R}^2$.Let $σ_n : [0, 1] → \mathbb{R}^2$ be a sequence of continuously differentiable constant speed curves with $||σ_n'(t)|| = L_n$ for all $t ∈ [0, 1]$ and $σ_n(0) = a$ and $σ_n(1) = b$ for all $n$. Suppose that $\lim_{n→∞} L_n = ||b − a||$. Show that $σ_n$ converges uniformly to $σ$, where $σ(t) = a + t(b − a)$ for $t ∈ [0, 1]$. My Try: Intuitively this is clear since it talks about a sequence of paths from $a$ to $b$ that converges to the straight line through $a$ and $b$. For a rigorous proof, I wanted to show first that $σ_n'(t)$ converges uniformly. But all I have is given $\epsilon>0$ there is $N$ such that $|   \;||σ_n'(t)||-||b − a||\;|<\epsilon$ for all $n>N$, which does not support what I need to prove. Any suggestion please..",,"['real-analysis', 'multivariable-calculus', 'uniform-convergence']"
30,inverse function theorem and matrix square root,inverse function theorem and matrix square root,,"Define $\mathbf{f}(A) = A^2$, for $A \in \mathbb{R}^{n \times n}$. (a) Applying the Inverse Function Theorem, show that  every matrix $B$ in a neighbourhood of $I$ has (atleast) 2 square roots $A$, that is $A^2 = B$, each varying as a $C^1$ function of $B$. (b) Can you decide if there are precisely 2 or more ? (Hint: in  2x2 case, what is $D\mathbf{f}\left ( \begin{bmatrix} 1 & 0 \\                                            0  & -1 \end{bmatrix} \right )$ ? Now I can apply the inverse function theorem to $\mathbf{f}(A)$ at $A =I$  to conclude the inverse exists. But I cannot see how to apply it and ``show that  every matrix $B$ in a neighbourhood of $I$ has (atleast) 2 square roots $A$"" is to be solved and the bit about number of square roots. I would appreciate some hints. This is a problem from Ted Shifrin's book on Mutivariable Mathematics.","Define $\mathbf{f}(A) = A^2$, for $A \in \mathbb{R}^{n \times n}$. (a) Applying the Inverse Function Theorem, show that  every matrix $B$ in a neighbourhood of $I$ has (atleast) 2 square roots $A$, that is $A^2 = B$, each varying as a $C^1$ function of $B$. (b) Can you decide if there are precisely 2 or more ? (Hint: in  2x2 case, what is $D\mathbf{f}\left ( \begin{bmatrix} 1 & 0 \\                                            0  & -1 \end{bmatrix} \right )$ ? Now I can apply the inverse function theorem to $\mathbf{f}(A)$ at $A =I$  to conclude the inverse exists. But I cannot see how to apply it and ``show that  every matrix $B$ in a neighbourhood of $I$ has (atleast) 2 square roots $A$"" is to be solved and the bit about number of square roots. I would appreciate some hints. This is a problem from Ted Shifrin's book on Mutivariable Mathematics.",,"['multivariable-calculus', 'matrix-calculus', 'inverse-function-theorem']"
31,Theorem regarding Change of Variables in finite dimesnion,Theorem regarding Change of Variables in finite dimesnion,,"My question is based on Change of Variables in Multiple Integrals II Peter D. Lax > It is not necessary to read the paper before answering this question.The author tried to prove change of variables formula using elementary method but turns out to be buried in complexity. My question is essentially theorem 2(on this post) To ask my question , I will first state and prove the obvious result in one-dimension: $$\quad \epsilon>0, n>0,x \in R,[a b] \subset R,b>a,y:R->R$$ Theorem 1 : Given any continuously differentiable function $y(x)$  and if $\frac{dy}{dx}>0 $ for $x \in [a, b]$: Then there exists a variable $\varphi_n$, and $\varphi_n(x)$ is continuously differentiable over $\mathbb{R}$ with the following properties: i.  $|\varphi_n(x)-y(x)| \le \epsilon$ for $x \in (a, b)$ ii. $\varphi_n(x)=x$ for sufficiently large $|x|$ iii. $\lim\limits_{n\mapsto 0}\varphi_n(x)=y$ and $\lim\limits_{n\mapsto 0}\frac{d\varphi_n}{dx}=\frac{dy}{dx}>0$ for $x \in (a,  b)$ iv. The mapping $\varphi:x\to R$ is one-to-one, so it is invertible Proof : $r(x)=-{\frac {n}{x^2-a^2}}$ $h(x)=-{\frac {n}{b^2-x^2}}$ $\psi_n(x) = \begin{cases} e^{r(x)+h(x)},  & \text{if $a <x< b$ } \\ 0, & \text{otherwise} \end{cases}$ define $\varphi_n=x(1-e^{-nx^2})+\psi_n(x) y(x)+\int_{-\infty}^x2nt(\frac{-1}{(t^2-a^2)^2}+\frac{1}{(b^2-t^2)^2})\psi_n(t)y(t)dt$ it is easily seen $\frac{d\varphi_n}{dx}=(1-e^{-nx^2})+2nx^2e^{-nx^2}+\psi_n(x)\frac{dy}{dx}$ all the terms on the right hand side are greater than zero Taking limits and using inverse function theorem in one dimension, the theorem follows Theorem 2 : $f:\mathbb{R}^N\to\mathbb{R}^N$, $x \in R^N$, $y=f(x)$, $y=[y_1(x), y_2(x),y_3(x),..y_N(x)]$,  $x=[x_1,x_2,\dots,x_N]$, and the the determinant of the Jacobian matrix of $y$ denoted as $\det J$ . Under what conditions can we always find a conitnuously differentiable $\varphi_n: \mathbb{R}^N\to\mathbb{R}^N$ such that: i. $|\varphi_n(x)-y(x)| \le \epsilon$ inside some bounded region A ii. $\varphi_n(x)=x$ for sufficiently large norm $|x|$ iii. $\lim\limits_{n\mapsto 0}\varphi_n(x)=y$ and $\lim\limits_{n\mapsto 0}\det J_n =\det J$ inside some bounded region  and $|\det J_n|>0$ for all $x$,where $\det J_n$ is determinant of Jacobian matrix of $\varphi_n(x)$ ? This is of practical importance because if the above properties are satsified by $\varphi_n$, then according to Hadamard's global inverse function theorem then the mapping $\varphi_n :R^N->R^N$ is  globally bijective.","My question is based on Change of Variables in Multiple Integrals II Peter D. Lax > It is not necessary to read the paper before answering this question.The author tried to prove change of variables formula using elementary method but turns out to be buried in complexity. My question is essentially theorem 2(on this post) To ask my question , I will first state and prove the obvious result in one-dimension: $$\quad \epsilon>0, n>0,x \in R,[a b] \subset R,b>a,y:R->R$$ Theorem 1 : Given any continuously differentiable function $y(x)$  and if $\frac{dy}{dx}>0 $ for $x \in [a, b]$: Then there exists a variable $\varphi_n$, and $\varphi_n(x)$ is continuously differentiable over $\mathbb{R}$ with the following properties: i.  $|\varphi_n(x)-y(x)| \le \epsilon$ for $x \in (a, b)$ ii. $\varphi_n(x)=x$ for sufficiently large $|x|$ iii. $\lim\limits_{n\mapsto 0}\varphi_n(x)=y$ and $\lim\limits_{n\mapsto 0}\frac{d\varphi_n}{dx}=\frac{dy}{dx}>0$ for $x \in (a,  b)$ iv. The mapping $\varphi:x\to R$ is one-to-one, so it is invertible Proof : $r(x)=-{\frac {n}{x^2-a^2}}$ $h(x)=-{\frac {n}{b^2-x^2}}$ $\psi_n(x) = \begin{cases} e^{r(x)+h(x)},  & \text{if $a <x< b$ } \\ 0, & \text{otherwise} \end{cases}$ define $\varphi_n=x(1-e^{-nx^2})+\psi_n(x) y(x)+\int_{-\infty}^x2nt(\frac{-1}{(t^2-a^2)^2}+\frac{1}{(b^2-t^2)^2})\psi_n(t)y(t)dt$ it is easily seen $\frac{d\varphi_n}{dx}=(1-e^{-nx^2})+2nx^2e^{-nx^2}+\psi_n(x)\frac{dy}{dx}$ all the terms on the right hand side are greater than zero Taking limits and using inverse function theorem in one dimension, the theorem follows Theorem 2 : $f:\mathbb{R}^N\to\mathbb{R}^N$, $x \in R^N$, $y=f(x)$, $y=[y_1(x), y_2(x),y_3(x),..y_N(x)]$,  $x=[x_1,x_2,\dots,x_N]$, and the the determinant of the Jacobian matrix of $y$ denoted as $\det J$ . Under what conditions can we always find a conitnuously differentiable $\varphi_n: \mathbb{R}^N\to\mathbb{R}^N$ such that: i. $|\varphi_n(x)-y(x)| \le \epsilon$ inside some bounded region A ii. $\varphi_n(x)=x$ for sufficiently large norm $|x|$ iii. $\lim\limits_{n\mapsto 0}\varphi_n(x)=y$ and $\lim\limits_{n\mapsto 0}\det J_n =\det J$ inside some bounded region  and $|\det J_n|>0$ for all $x$,where $\det J_n$ is determinant of Jacobian matrix of $\varphi_n(x)$ ? This is of practical importance because if the above properties are satsified by $\varphi_n$, then according to Hadamard's global inverse function theorem then the mapping $\varphi_n :R^N->R^N$ is  globally bijective.",,"['real-analysis', 'multivariable-calculus', 'constructive-mathematics']"
32,"Examine whether the function $f(x, y)$ is continuous on $\Bbb R^2$ or not",Examine whether the function  is continuous on  or not,"f(x, y) \Bbb R^2","Given, $f: \Bbb R^2 \rightarrow \Bbb R,$ $$f(x, y) := |\frac y {x^2}| e^{-|\frac y {x^2}|}, x \neq 0, y \in \Bbb R,$$ $$f(x, y) := 0, x = 0,$$ I have to decide whether the function is continuous on $\Bbb R^2$ or not. I'm still new to multivariable calculus, so I'm not sure how to work with the critical values yet, but I gave it a try. Assume $f(x, y)$ would be continuous on $\Bbb R^2.$ Then, $f(x, y)$ must be continuous in $f(x, y) = 0,$ which means that, given any sequence $(x, y)_{_n \in \Bbb N},$ $$\lim_{n\to \infty} (x, y)_n = 0 \Rightarrow \lim_{n\to \infty} f(x, y)_n = f(\lim_{x \rightarrow 0}, 0)$$ Now, in my opinion, this can't be true. When $\lim_{n\to \infty} (x, y)_n = 0$ , then $|\frac y {x^2}|$ converges to $1,$ since $x^2$ becomes (at a certain point) smaller than $1,$ such that $x^2$ converges much faster to $0$ than $y$ does. Therefore, the fraction becomes bigger. Transfering this result to $e^{-|\frac y {x^2}|},$ this term should simply converge to $1 \over e$ , and so does the whole expression. On the other hand, putting in $y = 0$ doesn't give the same result, therefore, the function can't be continious on $\Bbb R^2$ .","Given, I have to decide whether the function is continuous on or not. I'm still new to multivariable calculus, so I'm not sure how to work with the critical values yet, but I gave it a try. Assume would be continuous on Then, must be continuous in which means that, given any sequence Now, in my opinion, this can't be true. When , then converges to since becomes (at a certain point) smaller than such that converges much faster to than does. Therefore, the fraction becomes bigger. Transfering this result to this term should simply converge to , and so does the whole expression. On the other hand, putting in doesn't give the same result, therefore, the function can't be continious on .","f: \Bbb R^2 \rightarrow \Bbb R, f(x, y) := |\frac y {x^2}| e^{-|\frac y {x^2}|}, x \neq 0, y \in \Bbb R, f(x, y) := 0, x = 0, \Bbb R^2 f(x, y) \Bbb R^2. f(x, y) f(x, y) = 0, (x, y)_{_n \in \Bbb N}, \lim_{n\to \infty} (x, y)_n = 0 \Rightarrow \lim_{n\to \infty} f(x, y)_n = f(\lim_{x \rightarrow 0}, 0) \lim_{n\to \infty} (x, y)_n = 0 |\frac y {x^2}| 1, x^2 1, x^2 0 y e^{-|\frac y {x^2}|}, 1 \over e y = 0 \Bbb R^2","['multivariable-calculus', 'continuity']"
33,Why the norm in the definition of differentiability?,Why the norm in the definition of differentiability?,,A function $f: \Bbb R^m \to \Bbb R^n$ is differentiable  at $x_0$ iff $$\lim_{h \to 0} \frac{\|f(x_0+h)-f(x_0)-J(h)\|_{\Bbb R^n}}{\|h\|_{\Bbb R^m}}=0$$ Is there any particular reason we use the norm in the numerator here?  Isn't this equivalent to $$\lim_{h \to 0} \frac{f(x_0+h)-f(x_0)-J(h)}{\|h\|_{\Bbb R^m}}=0?$$,A function $f: \Bbb R^m \to \Bbb R^n$ is differentiable  at $x_0$ iff $$\lim_{h \to 0} \frac{\|f(x_0+h)-f(x_0)-J(h)\|_{\Bbb R^n}}{\|h\|_{\Bbb R^m}}=0$$ Is there any particular reason we use the norm in the numerator here?  Isn't this equivalent to $$\lim_{h \to 0} \frac{f(x_0+h)-f(x_0)-J(h)}{\|h\|_{\Bbb R^m}}=0?$$,,"['multivariable-calculus', 'derivatives']"
34,Spivak's Calculus on Manifolds - Statement of Lemma 2-10 is incorrect?,Spivak's Calculus on Manifolds - Statement of Lemma 2-10 is incorrect?,,"In Spivak's Calculus on Manifolds , there is a Lemma 2-10 that is later used to prove the Inverse Function Theorem. Lemma 2-10 : Let $A \subset \mathbb{R}^n$ be a rectangle and let $f : A \to \mathbb{R}^n$ be continuously differentiable. If there is a number $M$ such that   $| D_j f^i (x) | \leq M$ for all $x$ in the interior of $A$, then   $$ |f(x)-f(y)| \leq n^2 M |x-y| $$   for all $x,y \in A$. To prove this lemma, we first write $$ f^i(y)-f^i(x) = \sum_{j=1}^n (f^i(y^1,\dots,y^j,x^{j+1},\dots,x^n) - f^i(y^1,\dots,y^{j-1},x^j,\dots,x^n)). $$ Applying the mean-value theorem, we get $$ f^i(y^1,\dots,y^j,x^{j+1},\dots,x^n) - f^i(y^1,\dots,y^{j-1},x^j,\dots,x^n) = (y^j - x^j) \cdot D_j f^i(z_{ij}) $$ for some $z_{ij}$. Then, Spivak uses the hypothesis in the lemma to say that the right hand side has absolute value less than or equal to $M \cdot |y^j - x^j|$. But strictly speaking we cannot use this hypothesis yet, because it is possible that both $x$ and $y$ are boundary points such that $z_{ij}$ is also a boundary point. For example, $A \subset \mathbb{R}^2$ could be $[0,1]\times[0,1]$ and $x$ and $y$ could be $(0,0)$ and $(1,0)$, respectively. Then $$ f^i(1,0) - f^i(0,0) = (f^i(1,0) - f^i(0,0)) + (f^i(1,0)-f^i(1,0))\\ \Rightarrow f^i(1,0) - f^i(0,0) = (1-0) \cdot D_1 f^i (z_{i1}) $$ where $z_{i1}$ lies on the $x$-axis and so is a boundary point of $A$. So, the lemma should be stated more accurately as follows: Lemma 2-10 : Let $A \subset \mathbb{R}^n$ be a rectangle and let $f : A \to \mathbb{R}^n$ be continuously differentiable. If there is a number $M$ such that   $| D_j f^i (x) | \leq M$ for all $x$ in $A$, then   $$ |f(x)-f(y)| \leq n^2 M |x-y| $$   for all $x,y \in A$. The same proof as earlier should work, provided $z_{ij}$ is appropriately chosen from $A \cup \text{boundary $A$}$. Note: Spivak has defined a function $f : A \subset \mathbb{R}^n \to \mathbb{R}^m$ to be differentiable, where $A$ is any subset, if $f$ can be extended to a differentiable function on some open set containing $A$. I would like to know whether my reasoning is accurate. Thank you in advance. EDIT: @rogerl has clarified my original question in the comments below. I would like to add another question though: is it necessary that $f$ be continuously differentiable? Can't it just be differentiable? To apply the Mean Value Theorem on an interval $[a,b]$ we only require that the function be continuous on $[a,b]$ and differentiable on $(a,b)$. The proof does not seem to require continuity of the derivative. The proof continues as follows: $$ |f^i (y) - f^i (x)| \leq \sum_{j=1}^n |y^j - x^j | \cdot M \leq nM |y - x| $$ since each $|y^j - x^j| < |y-x|$. Finally $$ |f(y) - f(x)| \leq \sum_{i=1}^n |f^i (y) - f^i (x)| \leq n^2 M |y - x|. $$","In Spivak's Calculus on Manifolds , there is a Lemma 2-10 that is later used to prove the Inverse Function Theorem. Lemma 2-10 : Let $A \subset \mathbb{R}^n$ be a rectangle and let $f : A \to \mathbb{R}^n$ be continuously differentiable. If there is a number $M$ such that   $| D_j f^i (x) | \leq M$ for all $x$ in the interior of $A$, then   $$ |f(x)-f(y)| \leq n^2 M |x-y| $$   for all $x,y \in A$. To prove this lemma, we first write $$ f^i(y)-f^i(x) = \sum_{j=1}^n (f^i(y^1,\dots,y^j,x^{j+1},\dots,x^n) - f^i(y^1,\dots,y^{j-1},x^j,\dots,x^n)). $$ Applying the mean-value theorem, we get $$ f^i(y^1,\dots,y^j,x^{j+1},\dots,x^n) - f^i(y^1,\dots,y^{j-1},x^j,\dots,x^n) = (y^j - x^j) \cdot D_j f^i(z_{ij}) $$ for some $z_{ij}$. Then, Spivak uses the hypothesis in the lemma to say that the right hand side has absolute value less than or equal to $M \cdot |y^j - x^j|$. But strictly speaking we cannot use this hypothesis yet, because it is possible that both $x$ and $y$ are boundary points such that $z_{ij}$ is also a boundary point. For example, $A \subset \mathbb{R}^2$ could be $[0,1]\times[0,1]$ and $x$ and $y$ could be $(0,0)$ and $(1,0)$, respectively. Then $$ f^i(1,0) - f^i(0,0) = (f^i(1,0) - f^i(0,0)) + (f^i(1,0)-f^i(1,0))\\ \Rightarrow f^i(1,0) - f^i(0,0) = (1-0) \cdot D_1 f^i (z_{i1}) $$ where $z_{i1}$ lies on the $x$-axis and so is a boundary point of $A$. So, the lemma should be stated more accurately as follows: Lemma 2-10 : Let $A \subset \mathbb{R}^n$ be a rectangle and let $f : A \to \mathbb{R}^n$ be continuously differentiable. If there is a number $M$ such that   $| D_j f^i (x) | \leq M$ for all $x$ in $A$, then   $$ |f(x)-f(y)| \leq n^2 M |x-y| $$   for all $x,y \in A$. The same proof as earlier should work, provided $z_{ij}$ is appropriately chosen from $A \cup \text{boundary $A$}$. Note: Spivak has defined a function $f : A \subset \mathbb{R}^n \to \mathbb{R}^m$ to be differentiable, where $A$ is any subset, if $f$ can be extended to a differentiable function on some open set containing $A$. I would like to know whether my reasoning is accurate. Thank you in advance. EDIT: @rogerl has clarified my original question in the comments below. I would like to add another question though: is it necessary that $f$ be continuously differentiable? Can't it just be differentiable? To apply the Mean Value Theorem on an interval $[a,b]$ we only require that the function be continuous on $[a,b]$ and differentiable on $(a,b)$. The proof does not seem to require continuity of the derivative. The proof continues as follows: $$ |f^i (y) - f^i (x)| \leq \sum_{j=1}^n |y^j - x^j | \cdot M \leq nM |y - x| $$ since each $|y^j - x^j| < |y-x|$. Finally $$ |f(y) - f(x)| \leq \sum_{i=1}^n |f^i (y) - f^i (x)| \leq n^2 M |y - x|. $$",,['multivariable-calculus']
35,Integrating Line Segment and Path,Integrating Line Segment and Path,,"Just wondering if anyone can give some assistance. I'm stuck on an exam question: $\ \vec F (x,y,z) = (6xy + 4xz)\vec i + (3x^2 + 2yz)\vec j + (2x^2 + y^2)\vec k , x,y,z ∈\Bbb R.$ Evaluate $$\ \int_Γ \vec F.\vec {dr} $$ where Γ is the path $\ y=x^2, z=0$ from (0,0,0) to (2,4,0) followed by the line seqment from (2,4,0) to (1,1,2). My immediate thought was green's theorem but I don't think that would be possible. Previous parts to the question included checking if the function was conservative if that helps in any way. Any help would be really appreciated!","Just wondering if anyone can give some assistance. I'm stuck on an exam question: $\ \vec F (x,y,z) = (6xy + 4xz)\vec i + (3x^2 + 2yz)\vec j + (2x^2 + y^2)\vec k , x,y,z ∈\Bbb R.$ Evaluate $$\ \int_Γ \vec F.\vec {dr} $$ where Γ is the path $\ y=x^2, z=0$ from (0,0,0) to (2,4,0) followed by the line seqment from (2,4,0) to (1,1,2). My immediate thought was green's theorem but I don't think that would be possible. Previous parts to the question included checking if the function was conservative if that helps in any way. Any help would be really appreciated!",,"['integration', 'multivariable-calculus']"
36,Surface integral of function over intersection between plane and unit sphere,Surface integral of function over intersection between plane and unit sphere,,"I've been asked to compute the integral of $f(x, y, z)= 1 - x^2 - y^2 - z^2$ over the surface of the plane $x + y + z = t$ cut off by the sphere $x^2 + y^2 + z^2 = 1$ for $t \leq \sqrt3$ and prove it is equal to $\frac{\pi}{18}(3-t^2)^2$. A hint is provided to introduce a new coordinate system $(x_1, y_1, z_1)$ where $z_1$ is normal to the plane and use polar coordinates in the $x_1 y_1$ plane to parametrize the surface as a circle. I started to do this, letting $z_1 = \frac{1}{\sqrt3}(1,1,1)$, choosing $x_1 = \frac{1}{\sqrt2}(1, 0, 1)$ since it is orthogonal to $z_1$, and letting $y_1 = (\frac{-1}{\sqrt6},\sqrt\frac{2}{3}, \frac{-1}{\sqrt6})$ (their cross product). Then I tried to take the surface integral: $$ \int\int_S{f(x_1,y_1,z_1) dS}=\int\int_S(1-x_1^2-y_1^2-(t-x_1-y_1)^2)dS $$ Then by the polar parametrization $g(r, \theta)=(r\cos\theta, r\sin\theta, t-r\cos\theta-r\sin\theta)$, this becomes $$\int_0^{2\pi}{\int_0^1{(1-2r^2-t^2+2rt(\sin\theta+\cos\theta)-2r^2\sin\theta\cos\theta)\sqrt3r}dr}d\theta$$ This integral does not evaluate to anything resembling what I'm supposed to prove it does. Where am I going wrong? I feel it has something to do with switching coordinate systems, but I'm not exactly sure where I've made my mistake.","I've been asked to compute the integral of $f(x, y, z)= 1 - x^2 - y^2 - z^2$ over the surface of the plane $x + y + z = t$ cut off by the sphere $x^2 + y^2 + z^2 = 1$ for $t \leq \sqrt3$ and prove it is equal to $\frac{\pi}{18}(3-t^2)^2$. A hint is provided to introduce a new coordinate system $(x_1, y_1, z_1)$ where $z_1$ is normal to the plane and use polar coordinates in the $x_1 y_1$ plane to parametrize the surface as a circle. I started to do this, letting $z_1 = \frac{1}{\sqrt3}(1,1,1)$, choosing $x_1 = \frac{1}{\sqrt2}(1, 0, 1)$ since it is orthogonal to $z_1$, and letting $y_1 = (\frac{-1}{\sqrt6},\sqrt\frac{2}{3}, \frac{-1}{\sqrt6})$ (their cross product). Then I tried to take the surface integral: $$ \int\int_S{f(x_1,y_1,z_1) dS}=\int\int_S(1-x_1^2-y_1^2-(t-x_1-y_1)^2)dS $$ Then by the polar parametrization $g(r, \theta)=(r\cos\theta, r\sin\theta, t-r\cos\theta-r\sin\theta)$, this becomes $$\int_0^{2\pi}{\int_0^1{(1-2r^2-t^2+2rt(\sin\theta+\cos\theta)-2r^2\sin\theta\cos\theta)\sqrt3r}dr}d\theta$$ This integral does not evaluate to anything resembling what I'm supposed to prove it does. Where am I going wrong? I feel it has something to do with switching coordinate systems, but I'm not exactly sure where I've made my mistake.",,"['integration', 'multivariable-calculus', 'surface-integrals', 'spheres', 'parametrization']"
37,solving linear gradient PDE,solving linear gradient PDE,,Let $f:\mathbb R^n \to \mathbb R$ be a function that satisfies the equation: $$\nabla^T f = - (\nabla^T \phi ) f $$ where $\phi:\mathbb R^n \to \mathbb R$ is some given function and $\nabla^T$ is the column partial derivatives operator. Then obviousely $f(x)=k e^{-\phi(x)}$ solves the equation. Assume now that we have the equation: $$Q \nabla^T f = - (\nabla^T \phi ) f $$ for some symmetric positive definite matrix $Q$. Can it be solved similarly?,Let $f:\mathbb R^n \to \mathbb R$ be a function that satisfies the equation: $$\nabla^T f = - (\nabla^T \phi ) f $$ where $\phi:\mathbb R^n \to \mathbb R$ is some given function and $\nabla^T$ is the column partial derivatives operator. Then obviousely $f(x)=k e^{-\phi(x)}$ solves the equation. Assume now that we have the equation: $$Q \nabla^T f = - (\nabla^T \phi ) f $$ for some symmetric positive definite matrix $Q$. Can it be solved similarly?,,"['multivariable-calculus', 'partial-differential-equations', 'vector-analysis']"
38,Why being the discriminant $D<0$ guarantees that there is a saddle point in some direction?,Why being the discriminant  guarantees that there is a saddle point in some direction?,D<0,"I'm studying multi-variable calculus, and stopped at this theorem: I'm wondering why being the discriminant $D<0$ guarantees that there is a saddle point in some direction ?","I'm studying multi-variable calculus, and stopped at this theorem: I'm wondering why being the discriminant guarantees that there is a saddle point in some direction ?",D<0,['multivariable-calculus']
39,The directional derivative in cylindrical coordinates.,The directional derivative in cylindrical coordinates.,,"I found the gradient operator in cylindrical coordinates to be $$\nabla f = \frac{\partial f}{\partial r} \vec{e_r} + \frac{1}{r}\frac{\partial f}{\partial \theta} \vec{e_{\theta}} + \frac{\partial f}{\partial z} \vec{e_z} $$ Is it as easy as defining $\vec u = u_r\vec{e_r} + u_{\theta}\vec{e_{\theta}}  + u_z \vec{e_{z}}$ then taking the dot product and noting that our basis is an orthogonal set to obtain $$(\vec{u} \cdot \nabla) f = u_r \frac{\partial f}{\partial r} + u_{\theta} \frac{1}{r}\frac{\partial f}{\partial \theta}  + u_z \frac{\partial f}{\partial z}  $$? I feel like this is too good to be true. So my question is,  Is this the correct expression for the directional derivative of a scalar field $f$?","I found the gradient operator in cylindrical coordinates to be $$\nabla f = \frac{\partial f}{\partial r} \vec{e_r} + \frac{1}{r}\frac{\partial f}{\partial \theta} \vec{e_{\theta}} + \frac{\partial f}{\partial z} \vec{e_z} $$ Is it as easy as defining $\vec u = u_r\vec{e_r} + u_{\theta}\vec{e_{\theta}}  + u_z \vec{e_{z}}$ then taking the dot product and noting that our basis is an orthogonal set to obtain $$(\vec{u} \cdot \nabla) f = u_r \frac{\partial f}{\partial r} + u_{\theta} \frac{1}{r}\frac{\partial f}{\partial \theta}  + u_z \frac{\partial f}{\partial z}  $$? I feel like this is too good to be true. So my question is,  Is this the correct expression for the directional derivative of a scalar field $f$?",,['multivariable-calculus']
40,Calculate the vector surface integral,Calculate the vector surface integral,,"Let $V=\{(x,y,z)\in \mathbb{R}:\frac{1}{4}\le x^2+y^2+z^2\le1\}$ and $\vec{F}=\frac{x\hat{i}+y\hat{j}+z\hat{k}}{(x^2+y^2+z^2)^2}$ for $(x,y,z)\in V$. Let $\hat{n}$ denote the outward unit normal vector to the boundary of $V$ and $S$ denote the part $\{(x,y,z)\in \mathbb{R}^3:x^2+y^2+z^2=\frac{1}{4}\}$ of the boundary $V$. Then find $\int\int_S\vec{F}.\hat{n}dS$. Here is what I did. $$\int\int_S\vec{F}.\hat{n}dS$$ $$=\int\int_S\frac{x\hat{i}+y\hat{j}+z\hat{k}}{(x^2+y^2+z^2)^2}.(2x\hat{i}+2y\hat{j}+2z\hat{k})dxdy$$ $$\int\int_S2(x^2+y^2+z^2)^{-1}dxdy$$ $$\int\int_S\frac{2}{1/4}dxdy=8\int\int_Sdxdy=8\pi$$ where $\int\int_Sdxdy$ is the surface area of a sphere of radius $1/2$. Please advise on my solution.","Let $V=\{(x,y,z)\in \mathbb{R}:\frac{1}{4}\le x^2+y^2+z^2\le1\}$ and $\vec{F}=\frac{x\hat{i}+y\hat{j}+z\hat{k}}{(x^2+y^2+z^2)^2}$ for $(x,y,z)\in V$. Let $\hat{n}$ denote the outward unit normal vector to the boundary of $V$ and $S$ denote the part $\{(x,y,z)\in \mathbb{R}^3:x^2+y^2+z^2=\frac{1}{4}\}$ of the boundary $V$. Then find $\int\int_S\vec{F}.\hat{n}dS$. Here is what I did. $$\int\int_S\vec{F}.\hat{n}dS$$ $$=\int\int_S\frac{x\hat{i}+y\hat{j}+z\hat{k}}{(x^2+y^2+z^2)^2}.(2x\hat{i}+2y\hat{j}+2z\hat{k})dxdy$$ $$\int\int_S2(x^2+y^2+z^2)^{-1}dxdy$$ $$\int\int_S\frac{2}{1/4}dxdy=8\int\int_Sdxdy=8\pi$$ where $\int\int_Sdxdy$ is the surface area of a sphere of radius $1/2$. Please advise on my solution.",,"['multivariable-calculus', 'proof-verification', 'vector-analysis']"
41,Proving that $x^{\alpha}(1+\Vert x\Vert^{2})^{-k}$ belongs to $L^{2}(\mathbb{R}^{n})$,Proving that  belongs to,x^{\alpha}(1+\Vert x\Vert^{2})^{-k} L^{2}(\mathbb{R}^{n}),"Let $\alpha\in\mathbb{N}^{n}$ be a multi-index, i.e. $\alpha=(\alpha_{1},\dots,\alpha_{n})$ such that $x^{\alpha}:=\prod_{i=1}^{n}x^{\alpha_{i}}_{i}$. The modulus of a multi-index is defined as the quantity $\vert\alpha\vert:=\sum_{i=1}^{n}\alpha_{i}$. Let $\Vert x\Vert$ denote the euclidean norm of $x$, i.e. $\Vert x\Vert:=\sqrt{\sum_{i=1}^{n}x^{2}_{i}}$. Define the following function, for a fixed $\alpha\in\mathbb{N}^{n}$ and a fixed $k\in\mathbb{N}_{0}$ $$f_{\alpha,k}:\mathbb{R}^{n}\to\mathbb{C}:x\mapsto\frac{x^{\alpha}}{(1+\Vert x\Vert^{2})^{k}}$$ I want to prove that $f_{\alpha,k}\in L^{2}(\mathbb{R}^{n})$ (the space of square integrable functions over $\mathbb{R}^{n}$) if and only if $2k>\vert\alpha\vert+\tfrac{n}{2}$. It is possible that this bound is false. In that case, do not hesitate to correct it. Anyway, here is my attempt: \begin{align*} \int_{\mathbb{R}^{n}}\vert f_{\alpha,k}(x)\vert^{2}\text{d}x &= \int_{\mathbb{R}^{n}}\left\vert \frac{x^{\alpha}}{(1+\Vert x\Vert^{2})^{k}}\right\vert^{2}\text{d}x \\ &= \int_{\mathbb{R}^{n}}\left\vert \frac{x^{2\alpha}}{(1+\Vert x\Vert^{2})^{2k}}\right\vert\text{d}x\\ \end{align*} Since $\vert x^{2\alpha}\vert\le C(1+\Vert x\Vert^{2})^{\vert\alpha\vert}$, for a constant $C>0$, we have \begin{align*} \int_{\mathbb{R}^{n}}\vert f_{\alpha,k}(x)\vert^{2}\text{d}x &= \int_{\mathbb{R}^{n}}\left\vert \frac{x^{2\alpha}}{(1+\Vert x\Vert^{2})^{2k}}\right\vert\text{d}x\\ &\le C\int_{\mathbb{R}^{n}}\left\vert \frac{1}{(1+\Vert x\Vert^{2})^{2k-\vert\alpha\vert}}\right\vert\text{d}x\\ &=C\int_{\mathbb{R}^{n}}\frac{1}{(1+\Vert x\Vert^{2})^{2k-\vert\alpha\vert}}\text{d}x \end{align*} But I'm stuck here. I don't know how to get rid of this integral. Any hint is appreciated. Thank you.","Let $\alpha\in\mathbb{N}^{n}$ be a multi-index, i.e. $\alpha=(\alpha_{1},\dots,\alpha_{n})$ such that $x^{\alpha}:=\prod_{i=1}^{n}x^{\alpha_{i}}_{i}$. The modulus of a multi-index is defined as the quantity $\vert\alpha\vert:=\sum_{i=1}^{n}\alpha_{i}$. Let $\Vert x\Vert$ denote the euclidean norm of $x$, i.e. $\Vert x\Vert:=\sqrt{\sum_{i=1}^{n}x^{2}_{i}}$. Define the following function, for a fixed $\alpha\in\mathbb{N}^{n}$ and a fixed $k\in\mathbb{N}_{0}$ $$f_{\alpha,k}:\mathbb{R}^{n}\to\mathbb{C}:x\mapsto\frac{x^{\alpha}}{(1+\Vert x\Vert^{2})^{k}}$$ I want to prove that $f_{\alpha,k}\in L^{2}(\mathbb{R}^{n})$ (the space of square integrable functions over $\mathbb{R}^{n}$) if and only if $2k>\vert\alpha\vert+\tfrac{n}{2}$. It is possible that this bound is false. In that case, do not hesitate to correct it. Anyway, here is my attempt: \begin{align*} \int_{\mathbb{R}^{n}}\vert f_{\alpha,k}(x)\vert^{2}\text{d}x &= \int_{\mathbb{R}^{n}}\left\vert \frac{x^{\alpha}}{(1+\Vert x\Vert^{2})^{k}}\right\vert^{2}\text{d}x \\ &= \int_{\mathbb{R}^{n}}\left\vert \frac{x^{2\alpha}}{(1+\Vert x\Vert^{2})^{2k}}\right\vert\text{d}x\\ \end{align*} Since $\vert x^{2\alpha}\vert\le C(1+\Vert x\Vert^{2})^{\vert\alpha\vert}$, for a constant $C>0$, we have \begin{align*} \int_{\mathbb{R}^{n}}\vert f_{\alpha,k}(x)\vert^{2}\text{d}x &= \int_{\mathbb{R}^{n}}\left\vert \frac{x^{2\alpha}}{(1+\Vert x\Vert^{2})^{2k}}\right\vert\text{d}x\\ &\le C\int_{\mathbb{R}^{n}}\left\vert \frac{1}{(1+\Vert x\Vert^{2})^{2k-\vert\alpha\vert}}\right\vert\text{d}x\\ &=C\int_{\mathbb{R}^{n}}\frac{1}{(1+\Vert x\Vert^{2})^{2k-\vert\alpha\vert}}\text{d}x \end{align*} But I'm stuck here. I don't know how to get rid of this integral. Any hint is appreciated. Thank you.",,"['integration', 'multivariable-calculus', 'lp-spaces']"
42,Evaluate Integral Using Change of Variables,Evaluate Integral Using Change of Variables,,"Let $A = \{(x, y)\in \mathbb{R}^2 | x^2-xy+2y^2<1 \}$; define $f: \mathbb{R}^2 \to \mathbb{R}$ by $f(x, y) = xy$. Express the integral $$\int_A f$$ as an integral over the unit ball $B = \{(x, y)\in \mathbb{R}^2 | x^2+y^2<1 \}$. Someone gave a solution here but it seems to be wrong. My attempt: Observe that $x^2-xy+2y^2=(x-\frac{1}{2}y)^2+(\frac{\sqrt{7}}{2}y)^2$. Let $B^\ast = \{(r, \theta) | 0< r < 1, 0 < \theta < 2\pi\}$. Let $A^\ast = \{(x, y)\in \mathbb{R}^2 | x^2-xy+2y^2<1$, and $x<0$ if $y=0\}$. Then the function $$g(r, \theta) = (r\cos\theta+\frac{1}{\sqrt{7}}r\sin\theta, \frac{2}{\sqrt{7}}r\sin\theta)$$ carries $B^\ast$ in a one-to-one fasion onto $A-\{(0, 0)\}$. Now we obtain that \begin{align*} \text{det} Dg & =  \begin{vmatrix}\cos\theta+\frac{1}{\sqrt{7}}\sin\theta & -r\sin\theta+\frac{1}{\sqrt{7}}r\cos\theta \\ \frac{2}{\sqrt{7}}\sin\theta & \frac{2}{\sqrt{7}}r\cos\theta \\ \end{vmatrix} \\ & = \frac{2}{\sqrt{7}}r \\ & > 0 \end{align*} if $(r, \theta)\in B^\ast$. Since the non-negative $x$-axis has measure zero, using the formula of change of variables, we have that \begin{align*} \int_A f  & = \int_{A^\ast} f \\ & = \int_{B^\ast} f \circ g \cdot |\text{det} Dg| \\ & = \int_0^{2\pi} \int_0^1 \Big((r\cos\theta+\frac{1}{\sqrt{7}}r\sin\theta)\frac{2}{\sqrt{7}}r\sin\theta\frac{2}{\sqrt{7}}r \Big) drd\theta \\ & = \frac{2}{7}\int_0^{2\pi} \int_0^1 (2\cos\theta\sin\theta+\frac{2}{\sqrt{7}}\sin^2 \theta)r^3 drd\theta \\ & = \frac{2}{7}\int_0^{2\pi} \int_0^1 \Big( \sin2\theta+\frac{1}{\sqrt{7}}(1-\cos2\theta) \Big)r^3 drd\theta \\ & = \frac{2}{7}\int_0^{2\pi} \int_0^1 \Big( \sin2\theta-\frac{1}{\sqrt{7}}\cos2\theta+\frac{1}{\sqrt{7}} \Big)r^3 drd\theta \\ & = \frac{2}{7}\int_0^{2\pi} \Big[ \frac{1}{4}\Big( \sin2\theta-\frac{1}{\sqrt{7}}\cos2\theta+\frac{1}{\sqrt{7}} \Big)r^4 \Big]_{r=0}^{r=1} d\theta \\ & = \frac{2}{7}\int_0^{2\pi} \frac{1}{4}\Big( \sin2\theta-\frac{1}{\sqrt{7}}\cos2\theta+\frac{1}{\sqrt{7}} \Big) d\theta \\ & = \frac{1}{14} \Big[ -\frac{1}{2}\cos2\theta-\frac{1}{2\sqrt{7}}\sin2\theta+\frac{1}{\sqrt{7}}\theta \Big]_{\theta=0}^{\theta=2\pi} \\ & = \frac{\pi}{7\sqrt{7}} \end{align*} Am I making any mistake? Any help or suggestion is appreciated.","Let $A = \{(x, y)\in \mathbb{R}^2 | x^2-xy+2y^2<1 \}$; define $f: \mathbb{R}^2 \to \mathbb{R}$ by $f(x, y) = xy$. Express the integral $$\int_A f$$ as an integral over the unit ball $B = \{(x, y)\in \mathbb{R}^2 | x^2+y^2<1 \}$. Someone gave a solution here but it seems to be wrong. My attempt: Observe that $x^2-xy+2y^2=(x-\frac{1}{2}y)^2+(\frac{\sqrt{7}}{2}y)^2$. Let $B^\ast = \{(r, \theta) | 0< r < 1, 0 < \theta < 2\pi\}$. Let $A^\ast = \{(x, y)\in \mathbb{R}^2 | x^2-xy+2y^2<1$, and $x<0$ if $y=0\}$. Then the function $$g(r, \theta) = (r\cos\theta+\frac{1}{\sqrt{7}}r\sin\theta, \frac{2}{\sqrt{7}}r\sin\theta)$$ carries $B^\ast$ in a one-to-one fasion onto $A-\{(0, 0)\}$. Now we obtain that \begin{align*} \text{det} Dg & =  \begin{vmatrix}\cos\theta+\frac{1}{\sqrt{7}}\sin\theta & -r\sin\theta+\frac{1}{\sqrt{7}}r\cos\theta \\ \frac{2}{\sqrt{7}}\sin\theta & \frac{2}{\sqrt{7}}r\cos\theta \\ \end{vmatrix} \\ & = \frac{2}{\sqrt{7}}r \\ & > 0 \end{align*} if $(r, \theta)\in B^\ast$. Since the non-negative $x$-axis has measure zero, using the formula of change of variables, we have that \begin{align*} \int_A f  & = \int_{A^\ast} f \\ & = \int_{B^\ast} f \circ g \cdot |\text{det} Dg| \\ & = \int_0^{2\pi} \int_0^1 \Big((r\cos\theta+\frac{1}{\sqrt{7}}r\sin\theta)\frac{2}{\sqrt{7}}r\sin\theta\frac{2}{\sqrt{7}}r \Big) drd\theta \\ & = \frac{2}{7}\int_0^{2\pi} \int_0^1 (2\cos\theta\sin\theta+\frac{2}{\sqrt{7}}\sin^2 \theta)r^3 drd\theta \\ & = \frac{2}{7}\int_0^{2\pi} \int_0^1 \Big( \sin2\theta+\frac{1}{\sqrt{7}}(1-\cos2\theta) \Big)r^3 drd\theta \\ & = \frac{2}{7}\int_0^{2\pi} \int_0^1 \Big( \sin2\theta-\frac{1}{\sqrt{7}}\cos2\theta+\frac{1}{\sqrt{7}} \Big)r^3 drd\theta \\ & = \frac{2}{7}\int_0^{2\pi} \Big[ \frac{1}{4}\Big( \sin2\theta-\frac{1}{\sqrt{7}}\cos2\theta+\frac{1}{\sqrt{7}} \Big)r^4 \Big]_{r=0}^{r=1} d\theta \\ & = \frac{2}{7}\int_0^{2\pi} \frac{1}{4}\Big( \sin2\theta-\frac{1}{\sqrt{7}}\cos2\theta+\frac{1}{\sqrt{7}} \Big) d\theta \\ & = \frac{1}{14} \Big[ -\frac{1}{2}\cos2\theta-\frac{1}{2\sqrt{7}}\sin2\theta+\frac{1}{\sqrt{7}}\theta \Big]_{\theta=0}^{\theta=2\pi} \\ & = \frac{\pi}{7\sqrt{7}} \end{align*} Am I making any mistake? Any help or suggestion is appreciated.",,"['calculus', 'integration', 'multivariable-calculus', 'proof-verification']"
43,Where can I learn about other types of coordinate systems?,Where can I learn about other types of coordinate systems?,,"As a junior math major the only coordinate systems I've thus far been exposed to are Cartesian and polar (including cylindrical and spherical) coordinates.  But of course these aren't the only ones.  What book (online lectures, etc) can you recommend where I can study different types of coordinate systems in depth?  I'm looking to learn more about coordinate systems such as bipolar, skew, triangular, and even more interesting ones like the following Thanks in advance for any suggestions.","As a junior math major the only coordinate systems I've thus far been exposed to are Cartesian and polar (including cylindrical and spherical) coordinates.  But of course these aren't the only ones.  What book (online lectures, etc) can you recommend where I can study different types of coordinate systems in depth?  I'm looking to learn more about coordinate systems such as bipolar, skew, triangular, and even more interesting ones like the following Thanks in advance for any suggestions.",,"['multivariable-calculus', 'reference-request', 'coordinate-systems']"
44,Does there exist a vector field $\vec F$ such that $curl \vec F=x \vec i+y\vec j+z \vec k$? [closed],Does there exist a vector field  such that ? [closed],\vec F curl \vec F=x \vec i+y\vec j+z \vec k,"Closed. This question is off-topic . It is not currently accepting answers. This question is missing context or other details : Please improve the question by providing additional context, which ideally includes your thoughts on the problem and any attempts you have made to solve it. This information helps others identify where you have difficulties and helps them write answers appropriate to your experience level. Closed 8 years ago . Improve this question Does there exist a vector field $\vec F$ such that curl of $\vec F$ is $x \vec i+y\vec j+z \vec k$ ? UPDATE : I did $div(curl \vec F)=0$ as the answers did ; but that assumes a lot i.e. it assumes that components of $F$ have second partial derivatives and continuous mixed partial derivatives ; whereas for curl to be defined , we only need components of $F$ to have first order partial derivatives . Is the answer still no with this less assumption ? Please help . Thanks in advance","Closed. This question is off-topic . It is not currently accepting answers. This question is missing context or other details : Please improve the question by providing additional context, which ideally includes your thoughts on the problem and any attempts you have made to solve it. This information helps others identify where you have difficulties and helps them write answers appropriate to your experience level. Closed 8 years ago . Improve this question Does there exist a vector field $\vec F$ such that curl of $\vec F$ is $x \vec i+y\vec j+z \vec k$ ? UPDATE : I did $div(curl \vec F)=0$ as the answers did ; but that assumes a lot i.e. it assumes that components of $F$ have second partial derivatives and continuous mixed partial derivatives ; whereas for curl to be defined , we only need components of $F$ to have first order partial derivatives . Is the answer still no with this less assumption ? Please help . Thanks in advance",,['multivariable-calculus']
45,find the tangent to the sphere,find the tangent to the sphere,,"obtain the equations of tangent to sphere $$x^2+ y^2+z^2+6x-2z+1 = 0$$ which pass through the line $$3 (16-x) = 3z=2y+30$$ Now I know if the plane is $$lx +my+n z=p$$ then $$-I/3 +m/2+n/3=0$$ also $(16,-15,0)$ is a point on the plane I know that there is $2$ answers , but how to proceed","obtain the equations of tangent to sphere $$x^2+ y^2+z^2+6x-2z+1 = 0$$ which pass through the line $$3 (16-x) = 3z=2y+30$$ Now I know if the plane is $$lx +my+n z=p$$ then $$-I/3 +m/2+n/3=0$$ also $(16,-15,0)$ is a point on the plane I know that there is $2$ answers , but how to proceed",,"['calculus', 'multivariable-calculus']"
46,Integration involving Inner Product,Integration involving Inner Product,,"Suppose $f: {\bf R}^n \to {\bf R}^n $be a continuous function such that $\int_{{\bf R}^n} \vert f(x) \vert \, dx < \infty$. Let $ A \in GL_n({\bf R})$. Show that $$ \int_{{\bf R}^n} f(Ax) e^{i\langle y,x\rangle} \, dx = \int_{{\bf R}^n} f(x) e^{i\langle (A^{-1})^ty,x\rangle} \frac {dx}{\vert \det(A)\vert }.$$ Here,$<x,y>$ denote standard inner product on $\bf R^n$.Clearly using $Ax=t$ in the L.H.S. gives the R.H.S expression except $\vert \det(A)\vert ^{-1}$. I guess this $\vert \det(A)\vert^ {-1} $ is Jacobian of Matrix of change of variable but unable to verify this. Please help.","Suppose $f: {\bf R}^n \to {\bf R}^n $be a continuous function such that $\int_{{\bf R}^n} \vert f(x) \vert \, dx < \infty$. Let $ A \in GL_n({\bf R})$. Show that $$ \int_{{\bf R}^n} f(Ax) e^{i\langle y,x\rangle} \, dx = \int_{{\bf R}^n} f(x) e^{i\langle (A^{-1})^ty,x\rangle} \frac {dx}{\vert \det(A)\vert }.$$ Here,$<x,y>$ denote standard inner product on $\bf R^n$.Clearly using $Ax=t$ in the L.H.S. gives the R.H.S expression except $\vert \det(A)\vert ^{-1}$. I guess this $\vert \det(A)\vert^ {-1} $ is Jacobian of Matrix of change of variable but unable to verify this. Please help.",,"['integration', 'matrices', 'multivariable-calculus', 'inner-products']"
47,Applying Stoke's Theorem onto Geometric Solids with edges and faces.,Applying Stoke's Theorem onto Geometric Solids with edges and faces.,,"In my textbook, a question asks to find the curl $\int\int \nabla \times F \cdot \vec{dS}$ through all faces of a cube sitting on the $xy$ plane but through the base (the base has no face). The clever answer is to realize that the integration of curl on these 5 regions of interest is the same as the line integral ($\int F \cdot \vec{dr}$) around the perimeter of the base. This line integral of the base's perimeter is also equal to the curl through this one face (as if it were there). 1) I cannot convince myself that this boundary ""covers"" all 5 sides of the square (ie, that the line integral of the boundary equals the integration of curl on the five higher faces). I would think that each of these faces is only describable, with a line integral, using its own perimeter. 2) If this is true, I am curious to know whether the curl through any solid with edges (and with one empty face) can be found simply by finding the line integral of the vector field on the empty face's perimeter. For example, can taking the line integral around the pentagon shown in the buckeyball give me the flux through all other faces?","In my textbook, a question asks to find the curl $\int\int \nabla \times F \cdot \vec{dS}$ through all faces of a cube sitting on the $xy$ plane but through the base (the base has no face). The clever answer is to realize that the integration of curl on these 5 regions of interest is the same as the line integral ($\int F \cdot \vec{dr}$) around the perimeter of the base. This line integral of the base's perimeter is also equal to the curl through this one face (as if it were there). 1) I cannot convince myself that this boundary ""covers"" all 5 sides of the square (ie, that the line integral of the boundary equals the integration of curl on the five higher faces). I would think that each of these faces is only describable, with a line integral, using its own perimeter. 2) If this is true, I am curious to know whether the curl through any solid with edges (and with one empty face) can be found simply by finding the line integral of the vector field on the empty face's perimeter. For example, can taking the line integral around the pentagon shown in the buckeyball give me the flux through all other faces?",,"['multivariable-calculus', 'stokes-theorem']"
48,"Evaluating $\iiint_\Omega z\,\mathrm dx\mathrm dy\mathrm dz$, wrong book solution?","Evaluating , wrong book solution?","\iiint_\Omega z\,\mathrm dx\mathrm dy\mathrm dz","Evaluate $$\iiint_\Omega z\,\mathrm dx\mathrm dy\mathrm dz,$$ where   $$\Omega = \{(x, y, z) \in \mathbb R^3 \mid y \geq 0,\ z \geq 0,\ x^2 + y^2 \leq 4,\ (y - 1)^2 + z^2 \leq 1\}.$$ From the second and last conditions we get a semi-cylinder parallel to the $x$ axis, tangent to it and with radius $1$. The other two conditions give another cylinder, this time parallel to the $z$ axis. Therefore I choose to integrate as follows: $$\iint_K\int_0^{\sqrt{1 - (y - 1)^2}} z\,\mathrm dz\,\mathrm dx\mathrm dy,$$ where $K$ is the projection on the $xy$ plane of the intersection of the two cylinders. That is, the red region in this graph: The integral becomes $$\begin{align} \iint_K\left[\frac12 z^2\right]_0^{\sqrt{2y - y^2}}\,\mathrm dx\mathrm dy &= \frac12\int_0^1\int_{-\sqrt{4 - y^2}}^{\sqrt{4 - y^2}}(2y - y^2)\mathrm dx\mathrm dy =\\ &= \int_0^1(2y - y^2)\sqrt{4 - y^2}\mathrm dy =\\ &= \frac{16}3 - 2\sqrt3 + \frac{\sqrt3}4 - \frac\pi3 =\\ &= \frac{16}3 - \frac{7\sqrt3}4 - \frac\pi3 \end{align}$$ However, the book's solution is just $$\frac{16}3 - \pi$$ Is there an error in my solution or in the book?","Evaluate $$\iiint_\Omega z\,\mathrm dx\mathrm dy\mathrm dz,$$ where   $$\Omega = \{(x, y, z) \in \mathbb R^3 \mid y \geq 0,\ z \geq 0,\ x^2 + y^2 \leq 4,\ (y - 1)^2 + z^2 \leq 1\}.$$ From the second and last conditions we get a semi-cylinder parallel to the $x$ axis, tangent to it and with radius $1$. The other two conditions give another cylinder, this time parallel to the $z$ axis. Therefore I choose to integrate as follows: $$\iint_K\int_0^{\sqrt{1 - (y - 1)^2}} z\,\mathrm dz\,\mathrm dx\mathrm dy,$$ where $K$ is the projection on the $xy$ plane of the intersection of the two cylinders. That is, the red region in this graph: The integral becomes $$\begin{align} \iint_K\left[\frac12 z^2\right]_0^{\sqrt{2y - y^2}}\,\mathrm dx\mathrm dy &= \frac12\int_0^1\int_{-\sqrt{4 - y^2}}^{\sqrt{4 - y^2}}(2y - y^2)\mathrm dx\mathrm dy =\\ &= \int_0^1(2y - y^2)\sqrt{4 - y^2}\mathrm dy =\\ &= \frac{16}3 - 2\sqrt3 + \frac{\sqrt3}4 - \frac\pi3 =\\ &= \frac{16}3 - \frac{7\sqrt3}4 - \frac\pi3 \end{align}$$ However, the book's solution is just $$\frac{16}3 - \pi$$ Is there an error in my solution or in the book?",,"['integration', 'multivariable-calculus']"
49,How to parametrize the surface $x^3 + 3xy + z^2 = 2$ and compute a tangent plane,How to parametrize the surface  and compute a tangent plane,x^3 + 3xy + z^2 = 2,"How do I parametrize the surface $x^3 + 3xy + z^2 = 2$ and compute the tangent plane at $(1, \frac{1}{3}, 0)$ using the resulting parametrization?  I know that the tangent plane should be  $$\nabla(x^3 + 3xy + z^2)\cdot(x - x_0, y - y_0, z - z_0) = 0 \implies \\ (3x_0^2 + 3y_0)(x - x_0) + 3x_0(y - y_0) + 2z_0(z - z_0) = 0,$$ which ends up being $4x + 3y = 5$ at $(1, \frac{1}{3}, 0)$.  However, the parametrization I thought was right did not give me this answer (I'm not ruling out the possibility that I did the gradient method wrong).","How do I parametrize the surface $x^3 + 3xy + z^2 = 2$ and compute the tangent plane at $(1, \frac{1}{3}, 0)$ using the resulting parametrization?  I know that the tangent plane should be  $$\nabla(x^3 + 3xy + z^2)\cdot(x - x_0, y - y_0, z - z_0) = 0 \implies \\ (3x_0^2 + 3y_0)(x - x_0) + 3x_0(y - y_0) + 2z_0(z - z_0) = 0,$$ which ends up being $4x + 3y = 5$ at $(1, \frac{1}{3}, 0)$.  However, the parametrization I thought was right did not give me this answer (I'm not ruling out the possibility that I did the gradient method wrong).",,"['multivariable-calculus', 'surfaces', 'parametrization']"
50,"Finding the flux of the vector field $F=y^2\hat{i}+xy\hat{j}-z^2\hat{k}$ outward through the surface $z=2\sqrt{x^2+y^2}$ , $0\leq z\leq2$","Finding the flux of the vector field  outward through the surface  ,",F=y^2\hat{i}+xy\hat{j}-z^2\hat{k} z=2\sqrt{x^2+y^2} 0\leq z\leq2,"I need to find $\iint_\limits{S}F\cdot\hat{n} d\sigma$ where $\hat{n}$ is the unit outward normal to the surface $S$. Here $S$ is just the conical surface without the base. If I parameterize the surface as follows : $$\gamma=r \cos\theta\hat{i}+ r\sin\theta\hat{j}+2r\hat{k}$$  then, $$\iint_\limits{S}F\cdot\hat{n}d\sigma=\int_0^{2\pi}\int_0^1F\cdot(\gamma_r\times\gamma_\theta) dr d\theta$$ This gives the answer $-2\pi$. However the correct answer is $2\pi$. So I think I have not taken the correct outward direction for $\hat{n}$. My question is why is the unit outward normal not given by $\dfrac{\gamma_r\times\gamma_\theta}{|\gamma_r\times\gamma_\theta|}$ but by the negative of it for this problem? P.S. This is a homework question. Thank you!","I need to find $\iint_\limits{S}F\cdot\hat{n} d\sigma$ where $\hat{n}$ is the unit outward normal to the surface $S$. Here $S$ is just the conical surface without the base. If I parameterize the surface as follows : $$\gamma=r \cos\theta\hat{i}+ r\sin\theta\hat{j}+2r\hat{k}$$  then, $$\iint_\limits{S}F\cdot\hat{n}d\sigma=\int_0^{2\pi}\int_0^1F\cdot(\gamma_r\times\gamma_\theta) dr d\theta$$ This gives the answer $-2\pi$. However the correct answer is $2\pi$. So I think I have not taken the correct outward direction for $\hat{n}$. My question is why is the unit outward normal not given by $\dfrac{\gamma_r\times\gamma_\theta}{|\gamma_r\times\gamma_\theta|}$ but by the negative of it for this problem? P.S. This is a homework question. Thank you!",,['multivariable-calculus']
51,Computing a double integral,Computing a double integral,,"Consider the double integral $$\int dp \int dp' \exp[-\frac{1}{2}p^2 + Kpq] \exp[-\frac{1}{2}p'^2 + Kpp' + Kp'q'],$$ where $q$ and $q'$ are constants. How should one solve such an integral? I tried completing the square on various terms, e.g $$\exp[-\frac{1}{2}p^2 + Kpq] = \exp[-\frac{1}{2}(p-Kq)^2 + \frac{K^2}{2}q^2].$$ Then can take the $q^2$ term outside and work with $$\exp[-\frac{1}{2}(p-Kq)^2 + Kpp'] \exp[-\frac{1}{2}p'^2 + Kp'q']$$ $$ = \exp[-\frac{1}{2}((p-Kq) - Kp')^2 + \frac{K^2}{2}p'^2] \exp[-\frac{1}{2}p'^2 + Kp'q']$$ but I still haven't managed to separate the $p'$ and $p$ terms. Many thanks for any tips!","Consider the double integral $$\int dp \int dp' \exp[-\frac{1}{2}p^2 + Kpq] \exp[-\frac{1}{2}p'^2 + Kpp' + Kp'q'],$$ where $q$ and $q'$ are constants. How should one solve such an integral? I tried completing the square on various terms, e.g $$\exp[-\frac{1}{2}p^2 + Kpq] = \exp[-\frac{1}{2}(p-Kq)^2 + \frac{K^2}{2}q^2].$$ Then can take the $q^2$ term outside and work with $$\exp[-\frac{1}{2}(p-Kq)^2 + Kpp'] \exp[-\frac{1}{2}p'^2 + Kp'q']$$ $$ = \exp[-\frac{1}{2}((p-Kq) - Kp')^2 + \frac{K^2}{2}p'^2] \exp[-\frac{1}{2}p'^2 + Kp'q']$$ but I still haven't managed to separate the $p'$ and $p$ terms. Many thanks for any tips!",,"['integration', 'multivariable-calculus', 'completing-the-square']"
52,Why are these two definitions of differentiability identical?,Why are these two definitions of differentiability identical?,,"Recently, I have learned the following as the  definition of multivariable differentiability. Assume that one can express $f(x, y)$ in the following form: $$f\left(x, y\right) = f\left(x_0, y_0\right) + \frac{\partial f}{\partial x} \Delta x + \frac{\partial f}{\partial y} \Delta y + \xi \left(x, y\right) $$ Where $\xi$ can be viewed as the ""error"" function between the surface and the tangent plane. Then for $f$ to be differentiable at $\left(x_0, y_0\right)$, the following must hold true for all $\epsilon > 0$: For any $\epsilon > 0$, there exists a $\delta > 0$ such that whenever $r < \delta$, $\xi < \epsilon r$ where $r = \sqrt{\left(x - x_0 \right)^2 + \left(y - y_0\right)^2}$. In more intuitive terms, as we approach the point, $\xi$ needs to ""disappear"" faster than any linear multiple of $r$. If $\xi$ is linear in $r$, then we can say that $f$ is not differentiable (sort of analogous to a cusp in single variable calculus). This can also be stated as (I think): $$\lim_{r \rightarrow 0} \frac{\xi(x, y)}{r} = 0$$ Intuitively, this makes sense to be, because when we get ""infinitesimally"" close to the point in question, we can utilize the tangent plane to obtain the differential expression: $$\mathrm{d}f = \frac{\partial f}{\partial x} \mathrm{d}x + \frac{\partial f}{\partial y} \mathrm{d}y$$ But in the textbook I am using, differentiability is defined as follows: $f(x, y)$ is differentiable at $(x_0, y_0)$ if we can express $\Delta f$ in the following manner: $$\Delta f = \frac{\partial f}{\partial x} \Delta x + \frac{\partial f}{\partial y} \Delta y + \epsilon_1 \Delta x + \epsilon_2 \Delta y$$ where as $\Delta x \rightarrow 0$ and $\Delta y \rightarrow 0$, $\epsilon_1 \rightarrow 0$ and $\epsilon_2 \rightarrow 0$. You can view $\epsilon_1 \Delta x + \epsilon_2 \Delta y$ as the $\xi$ function mentioned in the first definition of differentiability. But I don't see how the two definitions are identical. It seems that in both of them, the ""error"" from the tangent plane is required to approach $0$ ""quickly enough,"" but I don't quite see how the two are identical.","Recently, I have learned the following as the  definition of multivariable differentiability. Assume that one can express $f(x, y)$ in the following form: $$f\left(x, y\right) = f\left(x_0, y_0\right) + \frac{\partial f}{\partial x} \Delta x + \frac{\partial f}{\partial y} \Delta y + \xi \left(x, y\right) $$ Where $\xi$ can be viewed as the ""error"" function between the surface and the tangent plane. Then for $f$ to be differentiable at $\left(x_0, y_0\right)$, the following must hold true for all $\epsilon > 0$: For any $\epsilon > 0$, there exists a $\delta > 0$ such that whenever $r < \delta$, $\xi < \epsilon r$ where $r = \sqrt{\left(x - x_0 \right)^2 + \left(y - y_0\right)^2}$. In more intuitive terms, as we approach the point, $\xi$ needs to ""disappear"" faster than any linear multiple of $r$. If $\xi$ is linear in $r$, then we can say that $f$ is not differentiable (sort of analogous to a cusp in single variable calculus). This can also be stated as (I think): $$\lim_{r \rightarrow 0} \frac{\xi(x, y)}{r} = 0$$ Intuitively, this makes sense to be, because when we get ""infinitesimally"" close to the point in question, we can utilize the tangent plane to obtain the differential expression: $$\mathrm{d}f = \frac{\partial f}{\partial x} \mathrm{d}x + \frac{\partial f}{\partial y} \mathrm{d}y$$ But in the textbook I am using, differentiability is defined as follows: $f(x, y)$ is differentiable at $(x_0, y_0)$ if we can express $\Delta f$ in the following manner: $$\Delta f = \frac{\partial f}{\partial x} \Delta x + \frac{\partial f}{\partial y} \Delta y + \epsilon_1 \Delta x + \epsilon_2 \Delta y$$ where as $\Delta x \rightarrow 0$ and $\Delta y \rightarrow 0$, $\epsilon_1 \rightarrow 0$ and $\epsilon_2 \rightarrow 0$. You can view $\epsilon_1 \Delta x + \epsilon_2 \Delta y$ as the $\xi$ function mentioned in the first definition of differentiability. But I don't see how the two definitions are identical. It seems that in both of them, the ""error"" from the tangent plane is required to approach $0$ ""quickly enough,"" but I don't quite see how the two are identical.",,"['calculus', 'multivariable-calculus', 'derivatives', 'definition']"
53,"$\iiint_Ez(x^2+y^2+z^2)^{-3/2}dV$ where $E=\{(x,y,z):x^2+y^2+z^2\leq16, z\geq2\}$",where,"\iiint_Ez(x^2+y^2+z^2)^{-3/2}dV E=\{(x,y,z):x^2+y^2+z^2\leq16, z\geq2\}","Convert to spherical coordinates and evaluate: $$\iiint_Ez(x^2+y^2+z^2)^{-3/2}dV$$ where $E$ is the region satisfying the following inequalities: $x^2+y^2+z^2\leq16$, $z\geq2$. When I drew out the graph. I got the boundaries to be $2\leq\rho\leq4$, $0\leq\theta\leq2\pi$, $0\leq\phi\leq \frac{\pi}{3}$. But apparently, my $\rho$ is wrong. it's supposed to be $\frac{2}{\cos\phi}\leq\rho\leq4$. I can't seem to see why. How exactly was it chosen? From what i gather it's suppose to be the range of values that the parameters can take.","Convert to spherical coordinates and evaluate: $$\iiint_Ez(x^2+y^2+z^2)^{-3/2}dV$$ where $E$ is the region satisfying the following inequalities: $x^2+y^2+z^2\leq16$, $z\geq2$. When I drew out the graph. I got the boundaries to be $2\leq\rho\leq4$, $0\leq\theta\leq2\pi$, $0\leq\phi\leq \frac{\pi}{3}$. But apparently, my $\rho$ is wrong. it's supposed to be $\frac{2}{\cos\phi}\leq\rho\leq4$. I can't seem to see why. How exactly was it chosen? From what i gather it's suppose to be the range of values that the parameters can take.",,"['integration', 'multivariable-calculus', 'spherical-coordinates']"
54,Differentiability of a scalar field,Differentiability of a scalar field,,"Is $f(x,y,z)=x\sqrt{y^2+z^2}$ differentiable at the point $(0,0,0)$? Prove your assertion. I'm pretty sure this function is actually differentiable at $0$. The problem is I'm unsure how exactly I should go about proving this. My understanding is that if the partial derivatives all exist at zero and are also continuous about zero, this would be enough to prove differentiability. How would I show these partial derivatives are continuous? Note that the partial derivatives are all equal to zero at the origin. With partial derivatives defined as: (1) $\frac{\partial f}{\partial x}=\sqrt{y^2+z^2}$ (2) $\frac{\partial f}{\partial y}=\frac{xy}{\sqrt{y^2+z^2}}$ (3) $\frac{\partial f}{\partial z}=\frac{xz}{\sqrt{y^2+z^2}}$","Is $f(x,y,z)=x\sqrt{y^2+z^2}$ differentiable at the point $(0,0,0)$? Prove your assertion. I'm pretty sure this function is actually differentiable at $0$. The problem is I'm unsure how exactly I should go about proving this. My understanding is that if the partial derivatives all exist at zero and are also continuous about zero, this would be enough to prove differentiability. How would I show these partial derivatives are continuous? Note that the partial derivatives are all equal to zero at the origin. With partial derivatives defined as: (1) $\frac{\partial f}{\partial x}=\sqrt{y^2+z^2}$ (2) $\frac{\partial f}{\partial y}=\frac{xy}{\sqrt{y^2+z^2}}$ (3) $\frac{\partial f}{\partial z}=\frac{xz}{\sqrt{y^2+z^2}}$",,"['multivariable-calculus', 'derivatives']"
55,How to properly handle multivariate limits.,How to properly handle multivariate limits.,,"Consider the function $f : \mathbb{R}^2 \to \mathbb{R}$ defined by $$ f(x,y) = \begin{cases}     \frac{xy(x^2-y^2)}{x^2+y^2} & \text{for  $(x,y) \neq (0,0),$}\\     0 & \text{for  $(x,y) = (0,0).$}   \end{cases} $$ I am hoping to show this is continuous, but I do not know how to handle multivariate limits, can I simply take one and then the other or what do you do? I have no $\varepsilon-\delta$ version of continuity at hand, only the usual limit formulation.","Consider the function $f : \mathbb{R}^2 \to \mathbb{R}$ defined by $$ f(x,y) = \begin{cases}     \frac{xy(x^2-y^2)}{x^2+y^2} & \text{for  $(x,y) \neq (0,0),$}\\     0 & \text{for  $(x,y) = (0,0).$}   \end{cases} $$ I am hoping to show this is continuous, but I do not know how to handle multivariate limits, can I simply take one and then the other or what do you do? I have no $\varepsilon-\delta$ version of continuity at hand, only the usual limit formulation.",,"['calculus', 'multivariable-calculus']"
56,How to use the Lagrange Multipliers to find the min and max of this function?,How to use the Lagrange Multipliers to find the min and max of this function?,,"So I have the function $$f(x,y)=x^2+y^2$$ with constraint $$(x-1)^2+4y^2=4$$ How can I find the minimum and maximum values for this using Lagrange multipliers? My attempt: I got the equations: 1) $$2x = λ(2x-2)$$ 2) $$2y = λ8y$$ 3) $$(x-1)^2+4y^2=4$$ I solved for x and y with no luck. Cant seem to find the min and max. Any help is appreciated","So I have the function $$f(x,y)=x^2+y^2$$ with constraint $$(x-1)^2+4y^2=4$$ How can I find the minimum and maximum values for this using Lagrange multipliers? My attempt: I got the equations: 1) $$2x = λ(2x-2)$$ 2) $$2y = λ8y$$ 3) $$(x-1)^2+4y^2=4$$ I solved for x and y with no luck. Cant seem to find the min and max. Any help is appreciated",,"['multivariable-calculus', 'optimization', 'lagrange-multiplier']"
57,Evaluate Integrals by Changing to Polar Coordinates,Evaluate Integrals by Changing to Polar Coordinates,,"I'm working on this question for my Calculus III Homework: Evaluate the given integral by changing to polar coordinates. $$\iint_{R} (5x-y)\,dA$$ where R is the region in the first quadrant enclosed by the circle $x^2 + y^2 = 16$ and the lines $x = 0$ and $y = x$. I mapped out the coordinates and got $\displaystyle\iint_R (5r\cos\theta-r\sin\theta)\,r \,dr\, d\theta$, where $0 \le r \le 4$ and $0 \le \theta \le \pi/4 $. Working it out it came out to $64 \sqrt{2} \, 64/3$, which was incorrect. If anyone could point out where I went wrong (most likely with defining coordinates), I would appreciate it very much.","I'm working on this question for my Calculus III Homework: Evaluate the given integral by changing to polar coordinates. $$\iint_{R} (5x-y)\,dA$$ where R is the region in the first quadrant enclosed by the circle $x^2 + y^2 = 16$ and the lines $x = 0$ and $y = x$. I mapped out the coordinates and got $\displaystyle\iint_R (5r\cos\theta-r\sin\theta)\,r \,dr\, d\theta$, where $0 \le r \le 4$ and $0 \le \theta \le \pi/4 $. Working it out it came out to $64 \sqrt{2} \, 64/3$, which was incorrect. If anyone could point out where I went wrong (most likely with defining coordinates), I would appreciate it very much.",,"['integration', 'multivariable-calculus', 'polar-coordinates']"
58,A question about gradient.,A question about gradient.,,"The gradient of a function of two variable $f(x,y)$ is given by  $$\left( \frac{\partial f}{\partial x} ,\frac{\partial f}{\partial y}\right). $$ It is also evident that gradient points in the direction of the greatest increase or decrease of a function at a point. My question is that whether the gradient vector is tangent to the function $f(x,y)$ at a point or not. The tangent here means that if you cut the graph of the function f(x,y) along the direction of the gradient vector then you will have a 2D curve formed where the function is cut. So is the gradient vector tangent to that curve at the particular point.","The gradient of a function of two variable $f(x,y)$ is given by  $$\left( \frac{\partial f}{\partial x} ,\frac{\partial f}{\partial y}\right). $$ It is also evident that gradient points in the direction of the greatest increase or decrease of a function at a point. My question is that whether the gradient vector is tangent to the function $f(x,y)$ at a point or not. The tangent here means that if you cut the graph of the function f(x,y) along the direction of the gradient vector then you will have a 2D curve formed where the function is cut. So is the gradient vector tangent to that curve at the particular point.",,['multivariable-calculus']
59,Multivariable continuity in ball,Multivariable continuity in ball,,"How do you show continuity for ball-based functions such as $$f:B[(0,0),1)]\rightarrow\mathbb{R}, \space f(x,y) = \sqrt{(1-(x^2+y^2)}$$","How do you show continuity for ball-based functions such as $$f:B[(0,0),1)]\rightarrow\mathbb{R}, \space f(x,y) = \sqrt{(1-(x^2+y^2)}$$",,"['multivariable-calculus', 'continuity']"
60,Two dimensional function that belongs to $C^{1}$ but does not belong to $C^{2}$.,Two dimensional function that belongs to  but does not belong to .,C^{1} C^{2},"Give an example of a two dimensional function that belongs to $C^{1}$ but does not belong to $C^{2}$ globally. The function must have a minimum point and at that minimum point the function must be twice continuously differentiable. In one dimension I have got one, $$f(x)= \begin{cases}-x^3,\quad & x<1 \\    x^2-5x+3, \quad & x>=1 \end{cases} $$ How to make a two-dimensional example?","Give an example of a two dimensional function that belongs to $C^{1}$ but does not belong to $C^{2}$ globally. The function must have a minimum point and at that minimum point the function must be twice continuously differentiable. In one dimension I have got one, $$f(x)= \begin{cases}-x^3,\quad & x<1 \\    x^2-5x+3, \quad & x>=1 \end{cases} $$ How to make a two-dimensional example?",,"['real-analysis', 'multivariable-calculus', 'derivatives', 'continuity']"
61,"Why does max. increase have to be along the x,y,z axis in gradient?","Why does max. increase have to be along the x,y,z axis in gradient?",,"$$\nabla f = \left(\frac{\partial f}{\partial x}, \frac{\partial f}{\partial y}, \frac{\partial f}{\partial z}\right)$$ These components are the rate of increase along the $x$, $y$ and $z$ directions respectively and according to gradient operation maximum increase is along the axis. Why does it have to be along the axis? Why does maximum increase have to be along the $x$, $y$ and $z$ direction?","$$\nabla f = \left(\frac{\partial f}{\partial x}, \frac{\partial f}{\partial y}, \frac{\partial f}{\partial z}\right)$$ These components are the rate of increase along the $x$, $y$ and $z$ directions respectively and according to gradient operation maximum increase is along the axis. Why does it have to be along the axis? Why does maximum increase have to be along the $x$, $y$ and $z$ direction?",,"['calculus', 'multivariable-calculus', 'vector-analysis']"
62,Find the critical point and show it is not a global minimizer (using Hessian),Find the critical point and show it is not a global minimizer (using Hessian),,"Consider the function $f(x,y) = x^3 + e^{3y}-3xe^y$. Show that $f$ has exactly one critical point and that this point is a local minimizer, but not a global minimizer. I have attempted this, but it seems that I have made a mistake somewhere along the way. Can someone please help me find where I went wrong? My attempt: \begin{align}\nabla f = \begin{bmatrix}3x^2 -3e^y \\3e^{3y} -3xe^y\end{bmatrix} = \bar{0}\end{align} Thus giving the following two homogeneous equations \begin{align}3x^2 - 3e^y &= 0\\ 3e^{3y} - 3xe^y &=0\end{align} From the second equation we find that $$x = e^{2y}$$ and upon substitution into the first equation we find that $$e^y(e^{3y} -1) =0$$ Notice however that $e^y \neq 0$ and we must that have that $e^{3y}=1 \implies y=0$ and thus $x =1$ Our critical point is thus $(x,y) = (1,0)$. How consider the Hessian \begin{align}H_f &= \begin{bmatrix}6x & -3e^y \\ -3e^y & 9e^{3y}-3xe^y\end{bmatrix}\end{align} Evaluated at our critical point we obtain $$H = \begin{bmatrix}6 & -3 \\ -3& 6\end{bmatrix}$$ We can then deduce that $H_f$ is positive definite and thus our critical point must be a strict global minimizer. This, however is the exact opposite of what the question stated? Edit: This is the theorem in my notes that I tried to use:","Consider the function $f(x,y) = x^3 + e^{3y}-3xe^y$. Show that $f$ has exactly one critical point and that this point is a local minimizer, but not a global minimizer. I have attempted this, but it seems that I have made a mistake somewhere along the way. Can someone please help me find where I went wrong? My attempt: \begin{align}\nabla f = \begin{bmatrix}3x^2 -3e^y \\3e^{3y} -3xe^y\end{bmatrix} = \bar{0}\end{align} Thus giving the following two homogeneous equations \begin{align}3x^2 - 3e^y &= 0\\ 3e^{3y} - 3xe^y &=0\end{align} From the second equation we find that $$x = e^{2y}$$ and upon substitution into the first equation we find that $$e^y(e^{3y} -1) =0$$ Notice however that $e^y \neq 0$ and we must that have that $e^{3y}=1 \implies y=0$ and thus $x =1$ Our critical point is thus $(x,y) = (1,0)$. How consider the Hessian \begin{align}H_f &= \begin{bmatrix}6x & -3e^y \\ -3e^y & 9e^{3y}-3xe^y\end{bmatrix}\end{align} Evaluated at our critical point we obtain $$H = \begin{bmatrix}6 & -3 \\ -3& 6\end{bmatrix}$$ We can then deduce that $H_f$ is positive definite and thus our critical point must be a strict global minimizer. This, however is the exact opposite of what the question stated? Edit: This is the theorem in my notes that I tried to use:",,"['multivariable-calculus', 'nonlinear-optimization']"
63,How to simplify $ \int_{\Bbb{R}^2}\Delta\varphi(x)\log|x|^2\ dx $ using Green's indentity?,How to simplify  using Green's indentity?, \int_{\Bbb{R}^2}\Delta\varphi(x)\log|x|^2\ dx ,"Let $\varphi\in C_c^\infty(\Bbb{R^2})$ (infinitely differentiable functions with compact support) and consider   $$ I=\int_{\Bbb{R}^2}\Delta\varphi(x)\log|x|^2\ dx, $$   the existence of which is given by the fact that $\log |x|^2$ is locally integrable. How far can one simplify this integral using the Green's identity? Let $R>0$ be large enough so that $\{|x|<R\}$ contains the support of $\varphi$. Denote $\Omega_{\varepsilon}=\{\varepsilon<|x|<R\}$ and $g(x)=\log|x|^2$ Then  $$ I=\lim_{\varepsilon\to 0+}\int_{\Omega_\varepsilon}\Delta\varphi(x)\log|x|^2\ dx =\lim_{\varepsilon\to 0+}\left(\int_{\Omega_\varepsilon}\Delta(\log|x|^2)\cdot  \varphi(x) dx +\int_{\partial\Omega_{\varepsilon}}g\frac{\partial\varphi}{\partial\nu}-\varphi\frac{\partial g}{\partial\nu}ds\right) $$ where $\frac{\partial\varphi}{\partial\nu}$ is the normal derivative.  Note that the first term is zero since $\Delta g=0$ on $\Omega_\varepsilon$. Let $\Gamma_\varepsilon=\{|x|=\varepsilon\}$ and $\Gamma_2=\{|x|=R\}$. Then by straightforward calculations, one has $$ \lim_{\varepsilon\to 0}\int_{\Gamma_\varepsilon}g\frac{\partial\varphi}{\partial\nu}-\varphi\frac{\partial g}{\partial\nu}ds=0.  $$ Everything now boils down to the calculation of  $$ \lim_{\varepsilon\to 0}\int_{\Gamma_2}g\frac{\partial\varphi}{\partial\nu}-\varphi\frac{\partial g}{\partial\nu}ds =\int_{\Gamma_2}\log R^2\frac{\partial\varphi}{\partial\nu}-\varphi\frac{2}{R}ds. $$ Could anyone simplify it further?","Let $\varphi\in C_c^\infty(\Bbb{R^2})$ (infinitely differentiable functions with compact support) and consider   $$ I=\int_{\Bbb{R}^2}\Delta\varphi(x)\log|x|^2\ dx, $$   the existence of which is given by the fact that $\log |x|^2$ is locally integrable. How far can one simplify this integral using the Green's identity? Let $R>0$ be large enough so that $\{|x|<R\}$ contains the support of $\varphi$. Denote $\Omega_{\varepsilon}=\{\varepsilon<|x|<R\}$ and $g(x)=\log|x|^2$ Then  $$ I=\lim_{\varepsilon\to 0+}\int_{\Omega_\varepsilon}\Delta\varphi(x)\log|x|^2\ dx =\lim_{\varepsilon\to 0+}\left(\int_{\Omega_\varepsilon}\Delta(\log|x|^2)\cdot  \varphi(x) dx +\int_{\partial\Omega_{\varepsilon}}g\frac{\partial\varphi}{\partial\nu}-\varphi\frac{\partial g}{\partial\nu}ds\right) $$ where $\frac{\partial\varphi}{\partial\nu}$ is the normal derivative.  Note that the first term is zero since $\Delta g=0$ on $\Omega_\varepsilon$. Let $\Gamma_\varepsilon=\{|x|=\varepsilon\}$ and $\Gamma_2=\{|x|=R\}$. Then by straightforward calculations, one has $$ \lim_{\varepsilon\to 0}\int_{\Gamma_\varepsilon}g\frac{\partial\varphi}{\partial\nu}-\varphi\frac{\partial g}{\partial\nu}ds=0.  $$ Everything now boils down to the calculation of  $$ \lim_{\varepsilon\to 0}\int_{\Gamma_2}g\frac{\partial\varphi}{\partial\nu}-\varphi\frac{\partial g}{\partial\nu}ds =\int_{\Gamma_2}\log R^2\frac{\partial\varphi}{\partial\nu}-\varphi\frac{2}{R}ds. $$ Could anyone simplify it further?",,"['real-analysis', 'functional-analysis']"
64,"Intersection of $36x^2 -9y^2+4z^2+36 = 0$ with plane $x=1$, derivative at a point","Intersection of  with plane , derivative at a point",36x^2 -9y^2+4z^2+36 = 0 x=1,"The exercise asks me to find the inclination of the line tangent to the intersection of $36x^2 -9y^2+4z^2+36 = 0$ with the plane $x=1$ in the point $(1,\sqrt{12},3)$, and then say to me that I have to interpret this as a partial derivative. So, that's what I did: if we solve for $z$ in the equation, we have: $$z = \pm \frac{\sqrt{9y^2-36x^2-36}}{2} \implies $$ if I take the partial with respect to $x$ for the $+$ function (because the point in $z$ is positive), all I'm doing is taking the partial derivative with respect to $x$, of the function defined by the intersection of that graph and the plaxe $x=1$, so I have: $$\frac{\partial z}{\partial x} = \frac{-6x}{\sqrt{-4-4 x^2+y^2}}$$ that, in the point $(1,\sqrt(12),3)$ gives: $$-3$$ Is everything ok? UPDATE: I think now that this partial derivative should be with   respect to $y$, rigth?","The exercise asks me to find the inclination of the line tangent to the intersection of $36x^2 -9y^2+4z^2+36 = 0$ with the plane $x=1$ in the point $(1,\sqrt{12},3)$, and then say to me that I have to interpret this as a partial derivative. So, that's what I did: if we solve for $z$ in the equation, we have: $$z = \pm \frac{\sqrt{9y^2-36x^2-36}}{2} \implies $$ if I take the partial with respect to $x$ for the $+$ function (because the point in $z$ is positive), all I'm doing is taking the partial derivative with respect to $x$, of the function defined by the intersection of that graph and the plaxe $x=1$, so I have: $$\frac{\partial z}{\partial x} = \frac{-6x}{\sqrt{-4-4 x^2+y^2}}$$ that, in the point $(1,\sqrt(12),3)$ gives: $$-3$$ Is everything ok? UPDATE: I think now that this partial derivative should be with   respect to $y$, rigth?",,"['calculus', 'multivariable-calculus', 'derivatives', 'partial-derivative']"
65,Can't Finish Double Integral in Polar or Cartesian,Can't Finish Double Integral in Polar or Cartesian,,"Alright, so I'm stuck on what I think should be a simple double integral. It is $\int_0^1\int_{\sqrt x}^1e^{y^3} \, dy \, dx$. This is just the volume between the surface $z=e^{y^3}$ and the area bounded by $y=\sqrt x$ and $y=1$, $0\le x \le 1$. I could see pretty quickly that I can't integrate this, so I tried to switch the $dy$ with $dx$ and change limits. When I do that, though, I end up with $\int_0^1[y^2e^{y^3}] \, dy$, which I still can't integrate. So I try by converting to polar coordinates, and I get that $0\le\theta\le\frac{\pi}{2}$ and $\frac{2\cos\theta}{1-\cos 2\theta}\le r \le 1-\sin\theta$. That radius will be a major problem when I go to integrate, so I know I went wrong somewhere. Does someone see what I did wrong? Thanks in advance!","Alright, so I'm stuck on what I think should be a simple double integral. It is $\int_0^1\int_{\sqrt x}^1e^{y^3} \, dy \, dx$. This is just the volume between the surface $z=e^{y^3}$ and the area bounded by $y=\sqrt x$ and $y=1$, $0\le x \le 1$. I could see pretty quickly that I can't integrate this, so I tried to switch the $dy$ with $dx$ and change limits. When I do that, though, I end up with $\int_0^1[y^2e^{y^3}] \, dy$, which I still can't integrate. So I try by converting to polar coordinates, and I get that $0\le\theta\le\frac{\pi}{2}$ and $\frac{2\cos\theta}{1-\cos 2\theta}\le r \le 1-\sin\theta$. That radius will be a major problem when I go to integrate, so I know I went wrong somewhere. Does someone see what I did wrong? Thanks in advance!",,['multivariable-calculus']
66,"Points on Surface, Distance Optimized","Points on Surface, Distance Optimized",,How do I find the points on the surface: $$x^3+y^3+z^3=1$$ such that the distance to the origin is minimized? My Thoughts: Perhaps we can minimize the distance squared? Not sure.,How do I find the points on the surface: $$x^3+y^3+z^3=1$$ such that the distance to the origin is minimized? My Thoughts: Perhaps we can minimize the distance squared? Not sure.,,['multivariable-calculus']
67,Stokes' theorem generalized the FTC part 2. Is there a known generalization for part 1?,Stokes' theorem generalized the FTC part 2. Is there a known generalization for part 1?,,"Stokes' theorem generalizes the fundamental theorem of calculus (part 2) using differential forms. Is there a known generalization of part 1? edit In case anyone is unaware, The fundamental theorem of calculus part 1 states that the derivative of the map $t \mapsto \int_{a}^{t} f(s) ds$ is equal to  $f(t)$. From this, it easily follows that if $F' = f$, then $\int_{a}^{b} f(x) dx = F(b) - F(a)$ (part 2). Stokes' theorem ($\int_{\Sigma} d \omega = \oint_{\partial \Sigma} \omega$) generalizes part 2 is analogous to part 2 in that in both cases one does a calculation on the boundary. But is there an analogous version of part 1? This question comes from my previous question in which I did such a calculation. In the GTC part 1, we consider a perametrized set of intervals $[0,t]_{t \in R}$. So the generalization ought to consist of a set of (hyper-)surfaces $\{\Sigma_{t}\}_{t \in R}$ in $R^N$. And thus, we wish to calculate the derivative of the mapping $t \mapsto \int_{\Sigma_t} \omega$. Suppose there exists a smooth $\phi (r,s): U\times R \to R^N$ ($U$ open subset of $R^{N-1}$)such that the restriction of $\phi$ on $[0,1]\times[0,t]$ parametrizes $\Sigma_{t}$. Then for fixed $s$, the map $r \mapsto \phi(r,s)$ perametrizes a subsurface $\sigma_{s}$ of $\Sigma_{s}$ whose dimension is one less ($\sigma_{s} \subset \partial \Sigma_{s}$). I believe that the derivative of the map $t \mapsto \int_{\Sigma_{t}} \omega$ is equal to $\oint_{\sigma_t} \omega_{t}$, where $\omega_{t}$ is some differential form that represents $\omega$ ""evaluated at"" $\sigma_{t}$. Am I in the right direction?","Stokes' theorem generalizes the fundamental theorem of calculus (part 2) using differential forms. Is there a known generalization of part 1? edit In case anyone is unaware, The fundamental theorem of calculus part 1 states that the derivative of the map $t \mapsto \int_{a}^{t} f(s) ds$ is equal to  $f(t)$. From this, it easily follows that if $F' = f$, then $\int_{a}^{b} f(x) dx = F(b) - F(a)$ (part 2). Stokes' theorem ($\int_{\Sigma} d \omega = \oint_{\partial \Sigma} \omega$) generalizes part 2 is analogous to part 2 in that in both cases one does a calculation on the boundary. But is there an analogous version of part 1? This question comes from my previous question in which I did such a calculation. In the GTC part 1, we consider a perametrized set of intervals $[0,t]_{t \in R}$. So the generalization ought to consist of a set of (hyper-)surfaces $\{\Sigma_{t}\}_{t \in R}$ in $R^N$. And thus, we wish to calculate the derivative of the mapping $t \mapsto \int_{\Sigma_t} \omega$. Suppose there exists a smooth $\phi (r,s): U\times R \to R^N$ ($U$ open subset of $R^{N-1}$)such that the restriction of $\phi$ on $[0,1]\times[0,t]$ parametrizes $\Sigma_{t}$. Then for fixed $s$, the map $r \mapsto \phi(r,s)$ perametrizes a subsurface $\sigma_{s}$ of $\Sigma_{s}$ whose dimension is one less ($\sigma_{s} \subset \partial \Sigma_{s}$). I believe that the derivative of the map $t \mapsto \int_{\Sigma_{t}} \omega$ is equal to $\oint_{\sigma_t} \omega_{t}$, where $\omega_{t}$ is some differential form that represents $\omega$ ""evaluated at"" $\sigma_{t}$. Am I in the right direction?",,"['multivariable-calculus', 'differential-forms']"
68,a vector calculus problem when coping with problem 9 chapter 5 evans,a vector calculus problem when coping with problem 9 chapter 5 evans,,"Hi I was trying to understand the solution given by Ray Yang in the post Question $5.9$ - Evans PDE $2$nd edition It gets to the sort of things I am quite bad at... When I get to the point $$\int_{\Omega}|Du|^p\,dx=-\sum_{i=1}^n\int_{\Omega}u\left[(\frac{\partial^2 u}{\partial x_i^2}|Du|^{p-2})+\frac{\partial}{\partial x_i}(u)\frac{\partial}{\partial x_i}|Du|^{p-2}\right]dx,$$ I deal with the claim, $$ \frac{\partial}{\partial x_i}|Du|^{p-2}=(p-2)|Du|^{p-4}\sum_j\frac{\partial^2 u}{\partial x_i\partial x_j}\frac{\partial u}{\partial x_j},$$ I feel I start making mistakes, what I did was…I have $$ \frac{\partial}{\partial x_i}|Du|^{p-2}=(p-2)|Du|^{p-3}\frac{\partial}{\partial x_i}(|Du|)$$, Treat $Du$ as y, I have $$(p-2)|Du|^{p-3}\frac{Du}{|Du|}\frac{\partial}{\partial x_i}(Du)$$, What I could have done wrong? Next, putting things together to get the second term, could anyone show me how to get from $$ -\sum_{i=1}^n\int_{\Omega} u\frac{\partial u}{\partial x_i}\frac{\partial}{\partial x_i}|Du|^{p-2} dx$$ to  $$-\int_{\Omega}(p-2)(\nabla u^T\cdot D^2 u\nabla u)|Du|^{p-4}?$$, for this term, what I have was $$=-\sum_{i=1}^n\int_{\Omega} u\frac{\partial u}{\partial x_i}(p-2)|Du|^{p-4} Du\frac{\partial}{\partial x_i}(Du).$$ I think my $\frac{\partial u}{\partial x_i}$ corresponds to $\nabla u^T$, $\frac{\partial}{\partial x_i}(Du)$ corresponds to $D^2 u$, $Du$ corresponds $\nabla u$ in the solution posted. Could anyone help?","Hi I was trying to understand the solution given by Ray Yang in the post Question $5.9$ - Evans PDE $2$nd edition It gets to the sort of things I am quite bad at... When I get to the point $$\int_{\Omega}|Du|^p\,dx=-\sum_{i=1}^n\int_{\Omega}u\left[(\frac{\partial^2 u}{\partial x_i^2}|Du|^{p-2})+\frac{\partial}{\partial x_i}(u)\frac{\partial}{\partial x_i}|Du|^{p-2}\right]dx,$$ I deal with the claim, $$ \frac{\partial}{\partial x_i}|Du|^{p-2}=(p-2)|Du|^{p-4}\sum_j\frac{\partial^2 u}{\partial x_i\partial x_j}\frac{\partial u}{\partial x_j},$$ I feel I start making mistakes, what I did was…I have $$ \frac{\partial}{\partial x_i}|Du|^{p-2}=(p-2)|Du|^{p-3}\frac{\partial}{\partial x_i}(|Du|)$$, Treat $Du$ as y, I have $$(p-2)|Du|^{p-3}\frac{Du}{|Du|}\frac{\partial}{\partial x_i}(Du)$$, What I could have done wrong? Next, putting things together to get the second term, could anyone show me how to get from $$ -\sum_{i=1}^n\int_{\Omega} u\frac{\partial u}{\partial x_i}\frac{\partial}{\partial x_i}|Du|^{p-2} dx$$ to  $$-\int_{\Omega}(p-2)(\nabla u^T\cdot D^2 u\nabla u)|Du|^{p-4}?$$, for this term, what I have was $$=-\sum_{i=1}^n\int_{\Omega} u\frac{\partial u}{\partial x_i}(p-2)|Du|^{p-4} Du\frac{\partial}{\partial x_i}(Du).$$ I think my $\frac{\partial u}{\partial x_i}$ corresponds to $\nabla u^T$, $\frac{\partial}{\partial x_i}(Du)$ corresponds to $D^2 u$, $Du$ corresponds $\nabla u$ in the solution posted. Could anyone help?",,"['integration', 'multivariable-calculus', 'partial-differential-equations', 'sobolev-spaces']"
69,Double integral over trapezoid,Double integral over trapezoid,,"Compute $$\iint_D \frac{1}{(1+(x+2y)^2)^2} \,dx\,dy$$ where $D$ is given by $x \geq 0 , \, y \geq 0, \, 1 \leq x+2y \leq 2 \\$. I am supposed to solve it with the help of contour lines . By drawing $D$ we get a trapezoid with the corners $(1,0), \, (2,0), \, (0, 1/2), \, (0,1)$. A contour line $\gamma$ to $f(x,y) = x+2y$ would be a line segment with end points $(\lambda, \frac{\lambda}{2})$. Consider the area of a trapezoid bounded by the line segment and the line segment with corners $(0, \frac{1}{2}), \, (1,0)$:  $$ A(\lambda) = \frac{1}{2}(\sqrt{(1/2)^2+1}+\sqrt{\lambda + (\lambda /2)^2)})\sqrt{(\lambda-1)^2+(\lambda/2-1/2)^2} = \frac{1}{2} \sqrt{5/4} (\lambda +1)(\lambda -1)\sqrt{5/4}=\frac{5}{8} (\lambda^2-1) $$  where, on the right side of the first equality, we are taking the mean of the sides of the trapezoid times the height.  The derivative is $A'(\lambda) = \frac{5}{4}\lambda$. The double integral can be computed as a single integral: $$\int_{\lambda=1}^{\lambda=2} \frac{1}{(1+\lambda^2)^2} \frac{5}{4} \lambda \, d \lambda =\cdots=3/16$$ This answer is wrong. The answer sheet says it is $3/40$. What have I done wrong?","Compute $$\iint_D \frac{1}{(1+(x+2y)^2)^2} \,dx\,dy$$ where $D$ is given by $x \geq 0 , \, y \geq 0, \, 1 \leq x+2y \leq 2 \\$. I am supposed to solve it with the help of contour lines . By drawing $D$ we get a trapezoid with the corners $(1,0), \, (2,0), \, (0, 1/2), \, (0,1)$. A contour line $\gamma$ to $f(x,y) = x+2y$ would be a line segment with end points $(\lambda, \frac{\lambda}{2})$. Consider the area of a trapezoid bounded by the line segment and the line segment with corners $(0, \frac{1}{2}), \, (1,0)$:  $$ A(\lambda) = \frac{1}{2}(\sqrt{(1/2)^2+1}+\sqrt{\lambda + (\lambda /2)^2)})\sqrt{(\lambda-1)^2+(\lambda/2-1/2)^2} = \frac{1}{2} \sqrt{5/4} (\lambda +1)(\lambda -1)\sqrt{5/4}=\frac{5}{8} (\lambda^2-1) $$  where, on the right side of the first equality, we are taking the mean of the sides of the trapezoid times the height.  The derivative is $A'(\lambda) = \frac{5}{4}\lambda$. The double integral can be computed as a single integral: $$\int_{\lambda=1}^{\lambda=2} \frac{1}{(1+\lambda^2)^2} \frac{5}{4} \lambda \, d \lambda =\cdots=3/16$$ This answer is wrong. The answer sheet says it is $3/40$. What have I done wrong?",,['multivariable-calculus']
70,I need some help understanding proofs for an upside-down cycloid being the tautochrone curve. Could someone show me or point me to a simple proof?,I need some help understanding proofs for an upside-down cycloid being the tautochrone curve. Could someone show me or point me to a simple proof?,,"The tautochrone curve has fascinated me since I first heard about it and I want to share it with my Calculus class as an end of the year project. I think something similar to this ( Demonstrating that a cycloid solves the Tautochrone Problem ) would suffice. I don't completely understand it myself, so that is a little bit of a problem. It is an AP calculus BC course in high school, so the kids are for the most part unmotivated and I want to help them realize how cool it can be. I don't need anything super complicated, just something simple enough for high schoolers with Calulus 2 under their belt to understand. Thank you so much!","The tautochrone curve has fascinated me since I first heard about it and I want to share it with my Calculus class as an end of the year project. I think something similar to this ( Demonstrating that a cycloid solves the Tautochrone Problem ) would suffice. I don't completely understand it myself, so that is a little bit of a problem. It is an AP calculus BC course in high school, so the kids are for the most part unmotivated and I want to help them realize how cool it can be. I don't need anything super complicated, just something simple enough for high schoolers with Calulus 2 under their belt to understand. Thank you so much!",,"['multivariable-calculus', 'education', 'mathematical-physics']"
71,Recommendation of multivariable calculus books,Recommendation of multivariable calculus books,,"I am looking for some suggestions on a good calculus book I shall keep on hand all the time. I am a graduate student who will be commencing research in the area of theoretical PDE (nonlinear). However I often get stuck on some basic calculus facts where most undergrad knows. My maths background is very applied(financial maths) hence I am lacking the actual preparation to work in theoretical PDE. However, it is too late for me to turn back. Very often as I feel, research in theoretical discipline (especially the analysis of PDE) requires nothing advanced but rather some delicate calculus and real analysis (perhaps at high school level) I found Stewart Calculus: concepts and context helpful helpful since we did not learn how to calculate stuff like surface integral (or any those engineering kind). But the book is too big and quite difficult to find a copy from the library (since first year students have the priority) Spivak is also good. But too little multivariable stuff. If I can find a book contains all that calculus facts allows one to study functional analytics aspects of nonlinear PDE, would be great! Any suggestions appreciated.","I am looking for some suggestions on a good calculus book I shall keep on hand all the time. I am a graduate student who will be commencing research in the area of theoretical PDE (nonlinear). However I often get stuck on some basic calculus facts where most undergrad knows. My maths background is very applied(financial maths) hence I am lacking the actual preparation to work in theoretical PDE. However, it is too late for me to turn back. Very often as I feel, research in theoretical discipline (especially the analysis of PDE) requires nothing advanced but rather some delicate calculus and real analysis (perhaps at high school level) I found Stewart Calculus: concepts and context helpful helpful since we did not learn how to calculate stuff like surface integral (or any those engineering kind). But the book is too big and quite difficult to find a copy from the library (since first year students have the priority) Spivak is also good. But too little multivariable stuff. If I can find a book contains all that calculus facts allows one to study functional analytics aspects of nonlinear PDE, would be great! Any suggestions appreciated.",,"['multivariable-calculus', 'reference-request', 'soft-question', 'book-recommendation']"
72,"What is the volume inside $S$, which is the surface given by the level set $\{ (x,y,z): x^2 + xy + y^2 + z^2 =1 \}$?","What is the volume inside , which is the surface given by the level set ?","S \{ (x,y,z): x^2 + xy + y^2 + z^2 =1 \}","The solution given uses a linear algebraic argument that doesn't seem very instructive -- and may not even be correct, I think. We notice from the equation, that the surface is a quadratic form, level set = 1. $\bullet$ The solution rewrites it as $x^TAx$, $\bullet$ Finds a symmetric matrix $A$ that gives the resulting quadratic form, $\bullet$ Computes the eigenvalues of $A$, And then (here's where the explanation doesn't really follow, I think) $\bullet$ A change of variables is made so that the surface becomes an ellipsoid, $\bullet$ Finally using the volume of an ellipsoid formula, the final answer is given. Is there another / better way of finding the volume of $S$? Thanks,","The solution given uses a linear algebraic argument that doesn't seem very instructive -- and may not even be correct, I think. We notice from the equation, that the surface is a quadratic form, level set = 1. $\bullet$ The solution rewrites it as $x^TAx$, $\bullet$ Finds a symmetric matrix $A$ that gives the resulting quadratic form, $\bullet$ Computes the eigenvalues of $A$, And then (here's where the explanation doesn't really follow, I think) $\bullet$ A change of variables is made so that the surface becomes an ellipsoid, $\bullet$ Finally using the volume of an ellipsoid formula, the final answer is given. Is there another / better way of finding the volume of $S$? Thanks,",,"['linear-algebra', 'multivariable-calculus', 'volume']"
73,Jacobi's Derivative of the Determinant,Jacobi's Derivative of the Determinant,,"I've been given the following theorem for the derivative of the determinant of a matrix: ""Let $A\in \mathbb{R}^{n\times n}$ be a square matrix. Then the Fréchet derivative of det$: \mathbb{R}^{n\times n}\to \mathbb{R}$ at $A$ is given by: d det(A)B = tr(adj(A)B)."" What I don't understand is what $B$ actually is. Where does it come from? Why did the theorem not follow up with ""where $B$ is..."" Could anyone please explain this to me? Thanks in advance!","I've been given the following theorem for the derivative of the determinant of a matrix: ""Let $A\in \mathbb{R}^{n\times n}$ be a square matrix. Then the Fréchet derivative of det$: \mathbb{R}^{n\times n}\to \mathbb{R}$ at $A$ is given by: d det(A)B = tr(adj(A)B)."" What I don't understand is what $B$ actually is. Where does it come from? Why did the theorem not follow up with ""where $B$ is..."" Could anyone please explain this to me? Thanks in advance!",,"['multivariable-calculus', 'derivatives', 'matrix-calculus']"
74,Divergence and Curl of the vectors,Divergence and Curl of the vectors,,"How to find the divergence and the curl of the given vectors? a. $( \vec{u} \cdot \vec{r}) \vec{v}$ b. $( \vec{u} \cdot \vec{r}) \vec{r}$ c. $( \vec{u} \times \vec{r})$ d. $ \vec{r} \times(\vec{u} \times \vec{r})$ e. $ \psi (r) (\vec{u} \times \vec{r})$ where $\vec{u}$ and $\vec{v}$ are the constant vectors, $\vec{r}$ is the radius vector and $\psi(r)$ is a scalar function of the magnitude r of the $\vec{r}$ Thanks.","How to find the divergence and the curl of the given vectors? a. $( \vec{u} \cdot \vec{r}) \vec{v}$ b. $( \vec{u} \cdot \vec{r}) \vec{r}$ c. $( \vec{u} \times \vec{r})$ d. $ \vec{r} \times(\vec{u} \times \vec{r})$ e. $ \psi (r) (\vec{u} \times \vec{r})$ where $\vec{u}$ and $\vec{v}$ are the constant vectors, $\vec{r}$ is the radius vector and $\psi(r)$ is a scalar function of the magnitude r of the $\vec{r}$ Thanks.",,"['multivariable-calculus', 'vector-analysis']"
75,parametric equation of level curve in three dimensional plane,parametric equation of level curve in three dimensional plane,,"What is the parametric equation for the tangent plane to the level curve of the function $$w(x,y,z) = xy+yz+xz$$ at the point $(1,-1,2)$? My answer was: $$(x,y,z) = (1,-1,2)+r<1,0,δz/δx>+<0,1,δz/δy>$$ but since $δz/δx$ and $δz/δy$ are not defined at $(1,-1,2)$, is there another way to find the parametric equation to the tangent plane?","What is the parametric equation for the tangent plane to the level curve of the function $$w(x,y,z) = xy+yz+xz$$ at the point $(1,-1,2)$? My answer was: $$(x,y,z) = (1,-1,2)+r<1,0,δz/δx>+<0,1,δz/δy>$$ but since $δz/δx$ and $δz/δy$ are not defined at $(1,-1,2)$, is there another way to find the parametric equation to the tangent plane?",,['multivariable-calculus']
76,"prove that there is a function $g:\mathbb R\rightarrow \mathbb R$ such that $\forall {x,y}\in \mathbb R^2\space ;\space f(x,y)=g(x+cy)$.",prove that there is a function  such that .,"g:\mathbb R\rightarrow \mathbb R \forall {x,y}\in \mathbb R^2\space ;\space f(x,y)=g(x+cy)","Let $f:\mathbb R^2 \rightarrow \mathbb R$ be a differentiable function. assume that there is $c\in \mathbb R$ such that for every $(x_0,y_0)\in \mathbb{ R}^2$ ; $$ \frac{\partial f}{\partial y}(x_0,y_0)=c\cdot\frac{\partial f}{\partial x}(x_0,y_0)$$ Prove that there is a function $g:\mathbb R\rightarrow \mathbb R$ such that $$\forall {x,y}\in \mathbb R^2\space ;\space f(x,y)=g(x+cy)$$ I am pretty sure that you have to show that if $x_1+c\cdot y_1=x_2+c\cdot y_2$ then $f(x_1,y_1)=f(x_2,y_2)$ and define for example  $g(x)=f(x,0)$ yet I don't know how to do that or how the partial derivatives connect to that.","Let $f:\mathbb R^2 \rightarrow \mathbb R$ be a differentiable function. assume that there is $c\in \mathbb R$ such that for every $(x_0,y_0)\in \mathbb{ R}^2$ ; $$ \frac{\partial f}{\partial y}(x_0,y_0)=c\cdot\frac{\partial f}{\partial x}(x_0,y_0)$$ Prove that there is a function $g:\mathbb R\rightarrow \mathbb R$ such that $$\forall {x,y}\in \mathbb R^2\space ;\space f(x,y)=g(x+cy)$$ I am pretty sure that you have to show that if $x_1+c\cdot y_1=x_2+c\cdot y_2$ then $f(x_1,y_1)=f(x_2,y_2)$ and define for example  $g(x)=f(x,0)$ yet I don't know how to do that or how the partial derivatives connect to that.",,"['real-analysis', 'multivariable-calculus']"
77,Why do lagrange multipliers have the form $\nabla G$,Why do lagrange multipliers have the form,\nabla G,"I was studying some multivariable Calculus and we were covering the topic of Lagrange multipliers. I didn't understand exactly why the equations take the form: $$ \nabla f = \lambda \nabla  G $$ Where $G$ is the constraint curve and $f$ is the function being maximized. I understand that $\nabla f$ being the gradient, points in the direction that maximizes f the most and therefore the point on the constraint curve edge G where $\nabla f$ is perpendicular to G (since there is nowhere along the curve that we can travel to increase $f$ without actually moving off of the curve G itself. My question is why must the gradient of G(x,y) the curve such that $G(x,y)= 0$ is our constraint curve G ALWAYS be perpendicular to the constraint curve G. I understood visually why this works with a few examples such as a little hill or mountain etc... But can somebody give a clear and explicit proof why: $$ \begin{pmatrix} \frac{\partial G}{\partial x} \\ \frac{\partial G}{\partial y} \end{pmatrix} \perp  \begin{pmatrix} 1 \\  \frac{dy}{dx} \end{pmatrix} $$ Given that $G(x,y) = 0$","I was studying some multivariable Calculus and we were covering the topic of Lagrange multipliers. I didn't understand exactly why the equations take the form: $$ \nabla f = \lambda \nabla  G $$ Where $G$ is the constraint curve and $f$ is the function being maximized. I understand that $\nabla f$ being the gradient, points in the direction that maximizes f the most and therefore the point on the constraint curve edge G where $\nabla f$ is perpendicular to G (since there is nowhere along the curve that we can travel to increase $f$ without actually moving off of the curve G itself. My question is why must the gradient of G(x,y) the curve such that $G(x,y)= 0$ is our constraint curve G ALWAYS be perpendicular to the constraint curve G. I understood visually why this works with a few examples such as a little hill or mountain etc... But can somebody give a clear and explicit proof why: $$ \begin{pmatrix} \frac{\partial G}{\partial x} \\ \frac{\partial G}{\partial y} \end{pmatrix} \perp  \begin{pmatrix} 1 \\  \frac{dy}{dx} \end{pmatrix} $$ Given that $G(x,y) = 0$",,"['calculus', 'linear-algebra', 'multivariable-calculus', 'optimization', 'lagrange-multiplier']"
78,Is the isosurface of a smooth function a smooth surface?,Is the isosurface of a smooth function a smooth surface?,,"suppose $ f(x)\in C_c^{\infty}(R^n, R)$, an infinitely differentiable function with compact support, from $R^n$ to $R$. If $f\not\equiv 0$, is the boundary of its support, i.e. $\partial\{x\in R^n: f(x)\neq 0\}$, always a smooth surface of $n-1$-dimension in $R^n$? Or at least piece-wise smooth? (of course it may comprise of disjoint components) Thanks a lot :-D","suppose $ f(x)\in C_c^{\infty}(R^n, R)$, an infinitely differentiable function with compact support, from $R^n$ to $R$. If $f\not\equiv 0$, is the boundary of its support, i.e. $\partial\{x\in R^n: f(x)\neq 0\}$, always a smooth surface of $n-1$-dimension in $R^n$? Or at least piece-wise smooth? (of course it may comprise of disjoint components) Thanks a lot :-D",,['multivariable-calculus']
79,Propagation of a Shock,Propagation of a Shock,,"I understand that we do not know the value of $u(x,t)$ when $0<x<1$ and $t≥1$ (because the charachtersitics do not pass through these points). However I am confused by 'ahead of the shock'. If I sketch $u(x,t)$ at $t=1$ (with reference to the graph $\color{green}{(*)}$ as below by using the values of u after the shock at $x=0$ and before the shock have switched around. However my notes state that  that up to $t=1$ the value of $u$ before the shock is $1$ and after it is $−1$. Where am I going wrong here?","I understand that we do not know the value of $u(x,t)$ when $0<x<1$ and $t≥1$ (because the charachtersitics do not pass through these points). However I am confused by 'ahead of the shock'. If I sketch $u(x,t)$ at $t=1$ (with reference to the graph $\color{green}{(*)}$ as below by using the values of u after the shock at $x=0$ and before the shock have switched around. However my notes state that  that up to $t=1$ the value of $u$ before the shock is $1$ and after it is $−1$. Where am I going wrong here?",,['multivariable-calculus']
80,Taylor Expansion of Inverse of Difference of Vectors,Taylor Expansion of Inverse of Difference of Vectors,,"I am trying to derive the multipole moment of a gravitational potential, but I'm getting stuck on some math I believe.  So basically the problem is finding the Taylor Expansion for $$\frac{1}{|\mathbf{x}-\mathbf{x'}|}=\frac{1}{\sqrt{(x-x')^2+(y-y')^2+(z-z')^2}}.$$  The expansion is supposed to be around $\mathbf{x'}=0$. I have two questions: 1) Do I take the partial derivatives in terms of x or x' (I'm thinking it should be just x, but I'm not sure)?  2) When I'm supposed to multiply by a factor that is the equivalent of $(x-a)$, what should that be?  I was thinking I should just dot with x' , but that doesn't give me the correct answer.","I am trying to derive the multipole moment of a gravitational potential, but I'm getting stuck on some math I believe.  So basically the problem is finding the Taylor Expansion for $$\frac{1}{|\mathbf{x}-\mathbf{x'}|}=\frac{1}{\sqrt{(x-x')^2+(y-y')^2+(z-z')^2}}.$$  The expansion is supposed to be around $\mathbf{x'}=0$. I have two questions: 1) Do I take the partial derivatives in terms of x or x' (I'm thinking it should be just x, but I'm not sure)?  2) When I'm supposed to multiply by a factor that is the equivalent of $(x-a)$, what should that be?  I was thinking I should just dot with x' , but that doesn't give me the correct answer.",,"['multivariable-calculus', 'taylor-expansion']"
81,Working with norms,Working with norms,,"I was hoping to get some help with being able to properly work with norms and derivatives so I can actually understand my PDE course. We are currently working on Sobolev spaces. Example, I want to show that: $$-\int_{U} u \Delta u dx \leq C \int_{U}|u||D^2u|dx$$ $u \in C_{c}^{\infty}(U)$ with $U$ bounded. I get to: $$-\int_{U} u \Delta u dx \leq \int_{U} |u||\Delta u|dx$$ I know it is not very far at all, but I am so confused. I am missing some key skills in multivariable calculus. My main question: What is $|D^2u|$? I thought that $D^2u$ was the hessian, and I'm confused about taking the norm. If you have any links that would help me better understand operations with norms and $D^ku$ I would greatly appreciate it. I'm really trying, but its just not clicking. Its really frustrating to be undone by the simpler concepts in an extremely theoretical PDE course.","I was hoping to get some help with being able to properly work with norms and derivatives so I can actually understand my PDE course. We are currently working on Sobolev spaces. Example, I want to show that: $$-\int_{U} u \Delta u dx \leq C \int_{U}|u||D^2u|dx$$ $u \in C_{c}^{\infty}(U)$ with $U$ bounded. I get to: $$-\int_{U} u \Delta u dx \leq \int_{U} |u||\Delta u|dx$$ I know it is not very far at all, but I am so confused. I am missing some key skills in multivariable calculus. My main question: What is $|D^2u|$? I thought that $D^2u$ was the hessian, and I'm confused about taking the norm. If you have any links that would help me better understand operations with norms and $D^ku$ I would greatly appreciate it. I'm really trying, but its just not clicking. Its really frustrating to be undone by the simpler concepts in an extremely theoretical PDE course.",,"['multivariable-calculus', 'partial-differential-equations']"
82,Need help converting this to Polar integral and evaluating it,Need help converting this to Polar integral and evaluating it,,"I have to convert this to polar integral and evaluate it. $$\int _{-1}^0\int _{-\sqrt{1-x^2}}^0\:\frac{2}{1\:+\:\sqrt{x^2\:+\:y^2}}\:dy\:dx$$ I attempted the conversion and ended up with this $$\int _{\pi }^{\frac{3\pi }{2}}\int _0^1\:\:2r\:\frac{1}{1\:+\:r}\:dr\:d\theta $$ Now, I'm stuck. Integration by parts does seem to be cooperating on this.","I have to convert this to polar integral and evaluate it. $$\int _{-1}^0\int _{-\sqrt{1-x^2}}^0\:\frac{2}{1\:+\:\sqrt{x^2\:+\:y^2}}\:dy\:dx$$ I attempted the conversion and ended up with this $$\int _{\pi }^{\frac{3\pi }{2}}\int _0^1\:\:2r\:\frac{1}{1\:+\:r}\:dr\:d\theta $$ Now, I'm stuck. Integration by parts does seem to be cooperating on this.",,"['multivariable-calculus', 'definite-integrals', 'polar-coordinates']"
83,Why can we turn this into a function like that?,Why can we turn this into a function like that?,,"To find volume bounded by $z=x^2+y^2$ and $z=y$. We can rewrite this as  $$y=x^2+y^2$$ $$0=y-x^2-y^2$$ And then we can say this is a function $f(x,y)=y-x^2-y^2$ and take a double integral of it. But why can we rewrite it like this and make it a function? I know this may seem like I should know it already, but I don't think this was ever explained to me. I'm used to having some variable equal to some function of variable(s). And I solved other volume problems by integrating $z=f(x,y)$, having found the boundaries with the help of other given equations. Although in this case it didn't work, for some reason, I found the boundaries correct but integrating $z=f(x,y)=x^2+y^2$ gave a wrong result.","To find volume bounded by $z=x^2+y^2$ and $z=y$. We can rewrite this as  $$y=x^2+y^2$$ $$0=y-x^2-y^2$$ And then we can say this is a function $f(x,y)=y-x^2-y^2$ and take a double integral of it. But why can we rewrite it like this and make it a function? I know this may seem like I should know it already, but I don't think this was ever explained to me. I'm used to having some variable equal to some function of variable(s). And I solved other volume problems by integrating $z=f(x,y)$, having found the boundaries with the help of other given equations. Although in this case it didn't work, for some reason, I found the boundaries correct but integrating $z=f(x,y)=x^2+y^2$ gave a wrong result.",,"['algebra-precalculus', 'multivariable-calculus']"
84,Integrating a twice differentiable function,Integrating a twice differentiable function,,"Let $f = f(x,y) \in C^2(\mathbb{R} \times \mathbb{R})$ and $\frac{\partial^2 f}{\partial x\,\partial y} > 0$. Then for $(a,b)\times(c,d) \in \mathbb{R}\times\mathbb{R}$, we have \begin{equation} 0 < \int_c^d \int_a^b \frac{\partial^2 f}{\partial x \, \partial y} \, dx \, dy = f(b,d) - f(b,c) - f(a,d) + f(a,c). \end{equation} Now suppose we are in higher dimensions and $f \in C^2(\mathbb{R}^n \times \mathbb{R}^n)$ with $\det \frac{\partial^2 f}{\partial x_i\partial y_j} > 0$. What I would like to be able to conclude is similar to the $1$D problem: that for some $a,b,c,d \in \mathbb{R}^n$ (unrelated to previous $a,b,c,d$) \begin{equation} 0 < f(b,d) - f(b,c) - f(a,d) + f(a,c). \end{equation} I would like to be able to conclude this from the fact that $\det \frac{\partial^2 f}{\partial x_i \, \partial y_j} > 0$, so there are linearly independent vectors $u,v \in \mathbb{R}^n$ with $u^t\frac{\partial^2 f}{\partial x_i \, \partial y_j} v > 0$. Then perhaps by integrating over a region in the $uv$-plane we can have something like \begin{equation} 0 < \int_{?}^{?}\int_{?}^{?}?? = f(b,d) - f(b,c) - f(a,d) + f(a,c). \end{equation} But I seem to be stuck in the details (assuming the desired conclusion is true). Apologies in advance - I wish I was able to formulate the question in a more precise way!","Let $f = f(x,y) \in C^2(\mathbb{R} \times \mathbb{R})$ and $\frac{\partial^2 f}{\partial x\,\partial y} > 0$. Then for $(a,b)\times(c,d) \in \mathbb{R}\times\mathbb{R}$, we have \begin{equation} 0 < \int_c^d \int_a^b \frac{\partial^2 f}{\partial x \, \partial y} \, dx \, dy = f(b,d) - f(b,c) - f(a,d) + f(a,c). \end{equation} Now suppose we are in higher dimensions and $f \in C^2(\mathbb{R}^n \times \mathbb{R}^n)$ with $\det \frac{\partial^2 f}{\partial x_i\partial y_j} > 0$. What I would like to be able to conclude is similar to the $1$D problem: that for some $a,b,c,d \in \mathbb{R}^n$ (unrelated to previous $a,b,c,d$) \begin{equation} 0 < f(b,d) - f(b,c) - f(a,d) + f(a,c). \end{equation} I would like to be able to conclude this from the fact that $\det \frac{\partial^2 f}{\partial x_i \, \partial y_j} > 0$, so there are linearly independent vectors $u,v \in \mathbb{R}^n$ with $u^t\frac{\partial^2 f}{\partial x_i \, \partial y_j} v > 0$. Then perhaps by integrating over a region in the $uv$-plane we can have something like \begin{equation} 0 < \int_{?}^{?}\int_{?}^{?}?? = f(b,d) - f(b,c) - f(a,d) + f(a,c). \end{equation} But I seem to be stuck in the details (assuming the desired conclusion is true). Apologies in advance - I wish I was able to formulate the question in a more precise way!",,"['integration', 'multivariable-calculus']"
85,Showing the Existence of Total Derivatives,Showing the Existence of Total Derivatives,,"I was presented with the following problem regarding a function that has discontinuous partial derivatives: $$ f(x,y) =\begin{array}{lr} x y \sin(\frac{1}{x^2 + y^2}) : (x,y) \neq 0\\ 0 : (x,y) = 0\end{array}$$ I showed the existence of the $x$ partial derivative using two cases when $y$ constant is $0$ and when $y \neq 0$ in conjunction with product, chain, and quotient rule. This results in: $$\begin{align} \frac{\partial f}{\partial x} (x,y) &= -\frac {2x^2 y \cos(\frac{1}{x^2 + y^2})} {(x^2 + y^2)^2} - y\sin(\frac{1}{x^2 + y^2}) : (x,y) \neq 0 \\ \frac{\partial f}{\partial x} (0,0) &= 0 \end{align}$$ I can see that the first part of this piece-wise function does not converge to 0 as $(x,y) \rightarrow 0 $ (in fact it doesn't seem to converge to anything, including $-\infty$). This would imply that the function $f(x,y)$ is not continuously differentiable . However, I am asked to show that this function is differentiable at every point and I have no idea how to do this. At the continuous points I believe I can use chain rule to maybe show differentiability. Would this be a good strategy? I am thinking about setting: $$ f(x,y) = g(h(x,y)) $$ where : $$ h(x,y) = (x,y,xy\sin(\frac{1}{x^2+y2})) $$ $$ g(x,y,z) = xyz $$ However, its not clear to me exactly how easy it is to show differentiability of $h$ and $g$ and address the discontinuous point. Sorry if I am missing something trivial. I don't often work with not-so-nice functions. But I would be very grateful if someone could push me in the right direction.","I was presented with the following problem regarding a function that has discontinuous partial derivatives: $$ f(x,y) =\begin{array}{lr} x y \sin(\frac{1}{x^2 + y^2}) : (x,y) \neq 0\\ 0 : (x,y) = 0\end{array}$$ I showed the existence of the $x$ partial derivative using two cases when $y$ constant is $0$ and when $y \neq 0$ in conjunction with product, chain, and quotient rule. This results in: $$\begin{align} \frac{\partial f}{\partial x} (x,y) &= -\frac {2x^2 y \cos(\frac{1}{x^2 + y^2})} {(x^2 + y^2)^2} - y\sin(\frac{1}{x^2 + y^2}) : (x,y) \neq 0 \\ \frac{\partial f}{\partial x} (0,0) &= 0 \end{align}$$ I can see that the first part of this piece-wise function does not converge to 0 as $(x,y) \rightarrow 0 $ (in fact it doesn't seem to converge to anything, including $-\infty$). This would imply that the function $f(x,y)$ is not continuously differentiable . However, I am asked to show that this function is differentiable at every point and I have no idea how to do this. At the continuous points I believe I can use chain rule to maybe show differentiability. Would this be a good strategy? I am thinking about setting: $$ f(x,y) = g(h(x,y)) $$ where : $$ h(x,y) = (x,y,xy\sin(\frac{1}{x^2+y2})) $$ $$ g(x,y,z) = xyz $$ However, its not clear to me exactly how easy it is to show differentiability of $h$ and $g$ and address the discontinuous point. Sorry if I am missing something trivial. I don't often work with not-so-nice functions. But I would be very grateful if someone could push me in the right direction.",,"['real-analysis', 'multivariable-calculus', 'derivatives']"
86,Change of variables in multi-variable calculus?,Change of variables in multi-variable calculus?,,"About the last equality, I know it is change of variables. Let $\xi=x+t,\eta=-x+t$, but I don't know how to get the integration domain? I have been thinking for an hour and I can't get the result? Can anyone help me about this? Thanks so much!","About the last equality, I know it is change of variables. Let $\xi=x+t,\eta=-x+t$, but I don't know how to get the integration domain? I have been thinking for an hour and I can't get the result? Can anyone help me about this? Thanks so much!",,"['calculus', 'multivariable-calculus']"
87,Derivative with Respect to a Ratio of Variables,Derivative with Respect to a Ratio of Variables,,We have two strictly positive real-valued variables $x$ and $y$ and a third one defined as $z=\frac{x}{y}$. Question 1: How do I compute the derivative $\frac{\partial x}{\partial z}$? Is it $\frac{\partial x}{\partial z} = \frac{1}{\frac{\partial z}{\partial x}}=\frac{1}{\frac{1}{y}}=y?$ Question 2: What is the derivative $\frac{\partial (xz)}{\partial z}$? Do I use the answer to Question 1 and the product rule to get $\frac{\partial x}{\partial z}z+x\frac{\partial z}{\partial z}=yz+x=y\frac{x}{y}+x=2x$? Or is it something else? Thanks!,We have two strictly positive real-valued variables $x$ and $y$ and a third one defined as $z=\frac{x}{y}$. Question 1: How do I compute the derivative $\frac{\partial x}{\partial z}$? Is it $\frac{\partial x}{\partial z} = \frac{1}{\frac{\partial z}{\partial x}}=\frac{1}{\frac{1}{y}}=y?$ Question 2: What is the derivative $\frac{\partial (xz)}{\partial z}$? Do I use the answer to Question 1 and the product rule to get $\frac{\partial x}{\partial z}z+x\frac{\partial z}{\partial z}=yz+x=y\frac{x}{y}+x=2x$? Or is it something else? Thanks!,,"['calculus', 'multivariable-calculus']"
88,Line Integral $\int_{C} \frac{x dy - y dx}{x^{2}+y^{2}}$,Line Integral,\int_{C} \frac{x dy - y dx}{x^{2}+y^{2}},"Find $$\int_{C} \frac{x dy - y dx}{x^{2}+y^{2}}$$ along the oriented broken line $C$ with vertices $(2,-2)$, $(4,4)$, $(-5,5)$ oriented counterclockwise. I noted that $C$ is a closed curve which passes through the origin, so Green's theorem cannot be applied here. Also, the vector field is not conservative, so the integral is nonzero.","Find $$\int_{C} \frac{x dy - y dx}{x^{2}+y^{2}}$$ along the oriented broken line $C$ with vertices $(2,-2)$, $(4,4)$, $(-5,5)$ oriented counterclockwise. I noted that $C$ is a closed curve which passes through the origin, so Green's theorem cannot be applied here. Also, the vector field is not conservative, so the integral is nonzero.",,['multivariable-calculus']
89,Optimization - find the dimensions of a box as functions of volume - minimal surface area,Optimization - find the dimensions of a box as functions of volume - minimal surface area,,"Had a basic calculus course exam today. This was one of the problems: We have a rectangular box of a given volume V . Present the width, height, and length of the box as functions of V so that the box can be made with the least amount of materials.  The box has a lid and we can assume the walls have no thickness. I know this problem is somehow related to multivariable calculus. During the course we have learned the basics of multivariable functions: partial derivatives, directional derivatives, limits, local min/max values and such. How would you use these concepts to solve the task? I'm pretty sure the box has to be cubical but can't prove it.","Had a basic calculus course exam today. This was one of the problems: We have a rectangular box of a given volume V . Present the width, height, and length of the box as functions of V so that the box can be made with the least amount of materials.  The box has a lid and we can assume the walls have no thickness. I know this problem is somehow related to multivariable calculus. During the course we have learned the basics of multivariable functions: partial derivatives, directional derivatives, limits, local min/max values and such. How would you use these concepts to solve the task? I'm pretty sure the box has to be cubical but can't prove it.",,"['multivariable-calculus', 'functions', 'optimization']"
90,Using Stokes' Theorem to evaluate $\int_{C}{(xyz)dx+(xy)dy+(x)dz}$,Using Stokes' Theorem to evaluate,\int_{C}{(xyz)dx+(xy)dy+(x)dz},"Let $C$ be the closed, piecewise smooth curve formed by traveling in straight lines between the points $(0,0,0),(2,1,5),(1,1,3)$ and back to the origin, in that order. Use Stokes' theorem to evaluate the integral: $$\displaystyle\int_{C}{(xyz)dx+(xy)dy+(x)dz}$$ First, Stokes' theorem is: $$\displaystyle\int_{∂S}{F \cdot dS} = \displaystyle\iint_{S}{(\nabla \times F)\cdot dS} = \displaystyle\iint_{D}{((\nabla \times F)\cdot n)\,dx\,dy}$$ I started by creating two vectors by subtracting the vertex points of the triangle. The I took the cross product of those two vectors to get $n = (-2,-1,1)$. Which is the normal vector to the surface. Taking $n \cdot (x,y,z) = 0$ will give me the tangent plane. Which will be: $$-2x - y + z = 0$$ so, $$z = 2x + y$$ I also can deduce from the question that (correct me if I'm wrong) $$F = (xyz,xy,x)$$ So, $$\nabla \times F = (0,xy-1,y-xz)$$ so now the integral is: $$\displaystyle\iint_{D}{((1-xy+y-xz)\,dx\,dy}$$ However, I'm not sure what to do for the boundaries of the integral. In the xy-plane, I believe D would be the triangle with the vertices (0,0), (2,1), (1,1). But that doesn't really do me much good. Would I need to do some sort of transformation to make D into something easier to work with and then just include the Jacobian determinant of that transformation? Or is there a better way to go about this? Thanks!","Let $C$ be the closed, piecewise smooth curve formed by traveling in straight lines between the points $(0,0,0),(2,1,5),(1,1,3)$ and back to the origin, in that order. Use Stokes' theorem to evaluate the integral: $$\displaystyle\int_{C}{(xyz)dx+(xy)dy+(x)dz}$$ First, Stokes' theorem is: $$\displaystyle\int_{∂S}{F \cdot dS} = \displaystyle\iint_{S}{(\nabla \times F)\cdot dS} = \displaystyle\iint_{D}{((\nabla \times F)\cdot n)\,dx\,dy}$$ I started by creating two vectors by subtracting the vertex points of the triangle. The I took the cross product of those two vectors to get $n = (-2,-1,1)$. Which is the normal vector to the surface. Taking $n \cdot (x,y,z) = 0$ will give me the tangent plane. Which will be: $$-2x - y + z = 0$$ so, $$z = 2x + y$$ I also can deduce from the question that (correct me if I'm wrong) $$F = (xyz,xy,x)$$ So, $$\nabla \times F = (0,xy-1,y-xz)$$ so now the integral is: $$\displaystyle\iint_{D}{((1-xy+y-xz)\,dx\,dy}$$ However, I'm not sure what to do for the boundaries of the integral. In the xy-plane, I believe D would be the triangle with the vertices (0,0), (2,1), (1,1). But that doesn't really do me much good. Would I need to do some sort of transformation to make D into something easier to work with and then just include the Jacobian determinant of that transformation? Or is there a better way to go about this? Thanks!",,"['multivariable-calculus', 'multiple-integral', 'surface-integrals']"
91,Divergence Theorem to calculate flux,Divergence Theorem to calculate flux,,"Take the vector field given by: $F= (y^2+yz)i+(\sin(xz)+z^2)j+z^2k$ a) Calculate the divergence, $\operatorname{div}F$. b) Use the divergence theorem to calculate the flux $$\int_S F\cdot dA $$ through a sphere or radius 2 centered at the origin oriented with an outward pointing unit normal. For the divergence of $F$, I found it to be $2z$. I'm pretty sure I need to change the integral into spherical coordinates, but I'm not sure if that's right. I'm also not understanding how I would find the limits for the integral as well.","Take the vector field given by: $F= (y^2+yz)i+(\sin(xz)+z^2)j+z^2k$ a) Calculate the divergence, $\operatorname{div}F$. b) Use the divergence theorem to calculate the flux $$\int_S F\cdot dA $$ through a sphere or radius 2 centered at the origin oriented with an outward pointing unit normal. For the divergence of $F$, I found it to be $2z$. I'm pretty sure I need to change the integral into spherical coordinates, but I'm not sure if that's right. I'm also not understanding how I would find the limits for the integral as well.",,"['multivariable-calculus', 'divergent-series', 'vector-fields']"
92,"definition of differential does not depend on the choice of the curve, differential is a linear map","definition of differential does not depend on the choice of the curve, differential is a linear map",,"Let $\varphi: M \to N$ be a differentiable map. How do I show that the definition of the differential $d\varphi_p: T_pM \to T_{\varphi(p)}N$ of $\varphi$ at $p$ does not depend on the choice of the curve and that $d\varphi_p$ is a linear map? Any help would be appreciated, thanks.","Let $\varphi: M \to N$ be a differentiable map. How do I show that the definition of the differential $d\varphi_p: T_pM \to T_{\varphi(p)}N$ of $\varphi$ at $p$ does not depend on the choice of the curve and that $d\varphi_p$ is a linear map? Any help would be appreciated, thanks.",,"['multivariable-calculus', 'differential-geometry']"
93,Find $\iint\limits_\Sigma(x\sqrt{x^2+y^2+z^2}\hat{\imath}+y\sqrt{x^2+y^2+z^2}\hat{\jmath}+z\sqrt{x^2+y^2+z^2}\hat{k})\cdot\hat{n}dS$,Find,\iint\limits_\Sigma(x\sqrt{x^2+y^2+z^2}\hat{\imath}+y\sqrt{x^2+y^2+z^2}\hat{\jmath}+z\sqrt{x^2+y^2+z^2}\hat{k})\cdot\hat{n}dS,"Find $\iint\limits_\Sigma(x\sqrt{x^2+y^2+z^2}\hat{\imath}+y\sqrt{x^2+y^2+z^2}\hat{\jmath}+z\sqrt{x^2+y^2+z^2}\hat{k})\cdot\hat{n}dS$ Where $\Sigma$ is the torus $\rho=\sin\phi$ Oriented by the normal directed outward. I got this as a study question for an upcoming exam of mine, and I've been having some trouble with it. What I've been able to do so far: Using the Divergence Theorem, which states that $\iint\limits_\Sigma\vec{F}\cdot\hat{n}dS = \iiint\limits_E div(\vec{F})dV$ $$ div(\vec{F}(x,y,z)=P\hat{\imath}+Q\hat{\jmath}+R\hat{k})=\frac{\partial P}{\partial x}+\frac{\partial Q}{\partial y} + \frac{\partial R}{\partial z} $$ $$ \implies div\left(x\sqrt{x^2+y^2+z^2}\hat{\imath}+y\sqrt{x^2+y^2+z^2}\hat{\jmath}+z\sqrt{x^2+y^2+z^2}\hat{k}\right) $$ $$ = \frac{2x^2+y^2+z^2}{\sqrt{x^2+y^2+z^2}}+ \frac{x^2+2y^2+z^2}{\sqrt{x^2+y^2+z^2}}+\frac{x^2+y^2+2z^2}{\sqrt{x^2+y^2+z^2}} = \frac{4x^2+4y^2+4z^2}{\sqrt{x^2+y^2+z^2}} = 4(x^2+y^2+z^2)^{3/2} $$ $$ \implies \iint\limits_\Sigma\vec{F}\cdot\hat{n}dS = 4\iiint\limits_E(x^2+y^2+z^2)^{3/2}dV $$ What I still can't figure out: How to get the bounds. I now have this triple integral, which needs bounds. Most of the problems I've done in the past have come out to be the triple integral of some constant, which is just the constant multiplied by the volume of the object, which is usually something simple like a cylindar or sphere. This one seems to actually require evaluating. How do I find the bounds to my new triple integral?","Find $\iint\limits_\Sigma(x\sqrt{x^2+y^2+z^2}\hat{\imath}+y\sqrt{x^2+y^2+z^2}\hat{\jmath}+z\sqrt{x^2+y^2+z^2}\hat{k})\cdot\hat{n}dS$ Where $\Sigma$ is the torus $\rho=\sin\phi$ Oriented by the normal directed outward. I got this as a study question for an upcoming exam of mine, and I've been having some trouble with it. What I've been able to do so far: Using the Divergence Theorem, which states that $\iint\limits_\Sigma\vec{F}\cdot\hat{n}dS = \iiint\limits_E div(\vec{F})dV$ $$ div(\vec{F}(x,y,z)=P\hat{\imath}+Q\hat{\jmath}+R\hat{k})=\frac{\partial P}{\partial x}+\frac{\partial Q}{\partial y} + \frac{\partial R}{\partial z} $$ $$ \implies div\left(x\sqrt{x^2+y^2+z^2}\hat{\imath}+y\sqrt{x^2+y^2+z^2}\hat{\jmath}+z\sqrt{x^2+y^2+z^2}\hat{k}\right) $$ $$ = \frac{2x^2+y^2+z^2}{\sqrt{x^2+y^2+z^2}}+ \frac{x^2+2y^2+z^2}{\sqrt{x^2+y^2+z^2}}+\frac{x^2+y^2+2z^2}{\sqrt{x^2+y^2+z^2}} = \frac{4x^2+4y^2+4z^2}{\sqrt{x^2+y^2+z^2}} = 4(x^2+y^2+z^2)^{3/2} $$ $$ \implies \iint\limits_\Sigma\vec{F}\cdot\hat{n}dS = 4\iiint\limits_E(x^2+y^2+z^2)^{3/2}dV $$ What I still can't figure out: How to get the bounds. I now have this triple integral, which needs bounds. Most of the problems I've done in the past have come out to be the triple integral of some constant, which is just the constant multiplied by the volume of the object, which is usually something simple like a cylindar or sphere. This one seems to actually require evaluating. How do I find the bounds to my new triple integral?",,"['multivariable-calculus', 'surface-integrals']"
94,"The partial derivatives of $F(x,y)=x^3y^4-\ln (x^2y)+e^{3x-6y}$",The partial derivatives of,"F(x,y)=x^3y^4-\ln (x^2y)+e^{3x-6y}","This is what I have so far, I am not really sure what I am doing is correct. Can anyone help with this? Given $F(x,y)=x^3y^4-\ln (x^2y)+e^{3x-6y}$ Find $f_x,f_y,f_{xx},f_{xy},f_{yy}$ $$P_x=3x^2y^4-\frac{2xy}{xy}+e^{x-6y}$$ $$P_{xx}=6xy^4-2+e^{-6y}$$ $$P_y=x^34y^3-\frac{x^2}{x^2}+e^{3x-6}$$ $$P_x=x^312y^2-1+e^{3x}$$ $$P_{xy}=\frac{6xy^4-2+e^{-6y}}{x^3-12y^2-1+e^{3x}}$$","This is what I have so far, I am not really sure what I am doing is correct. Can anyone help with this? Given Find","F(x,y)=x^3y^4-\ln (x^2y)+e^{3x-6y} f_x,f_y,f_{xx},f_{xy},f_{yy} P_x=3x^2y^4-\frac{2xy}{xy}+e^{x-6y} P_{xx}=6xy^4-2+e^{-6y} P_y=x^34y^3-\frac{x^2}{x^2}+e^{3x-6} P_x=x^312y^2-1+e^{3x} P_{xy}=\frac{6xy^4-2+e^{-6y}}{x^3-12y^2-1+e^{3x}}","['calculus', 'multivariable-calculus', 'partial-derivative']"
95,Stokes’ Theorem to find integration,Stokes’ Theorem to find integration,,"Use Stokes’ Theorem to evaluate integration $c (xy \,dx+ yz\, dy + zx\, dz)$ where and $C$ is the triangle with vertices $(1,0,0),(0,1,0),(0,0,1)$, oriented counter-clockwise rotation as viewed from above. can anyone please help me with this?","Use Stokes’ Theorem to evaluate integration $c (xy \,dx+ yz\, dy + zx\, dz)$ where and $C$ is the triangle with vertices $(1,0,0),(0,1,0),(0,0,1)$, oriented counter-clockwise rotation as viewed from above. can anyone please help me with this?",,"['calculus', 'integration', 'multivariable-calculus']"
96,Converse of Euler Homogeneous Thm. How to show that $\lambda \mathbf{x}\cdot \frac{d}{d\lambda}(\nabla{f(\mathbf{\lambda x})})=\mathbf{0}$?,Converse of Euler Homogeneous Thm. How to show that ?,\lambda \mathbf{x}\cdot \frac{d}{d\lambda}(\nabla{f(\mathbf{\lambda x})})=\mathbf{0},"So basically I read someone else's answer to a question regarding Euler Homogeneous function theorem source: https://quant.stackexchange.com/questions/8911/what-is-exactly-eulers-decomposition Also here https://planetmath.org/converseofeulershomogeneousfunctiontheorem I tried to do the proof by hand and this is what I get using product rules etc. Given $$\mathbf{x}\cdot \nabla{f(\mathbf{x})}\equiv kf(\mathbf{x})$$ Replace x by $\lambda x$ (The same as the letting $\phi(\lambda)$ step) $$\lambda \mathbf{x}\cdot \nabla{f(\lambda \mathbf{x})}\equiv kf(\lambda \mathbf{x})$$ Differentiating both sides wrt $\lambda$ $$\frac{d}{d\lambda}(\lambda \mathbf{x}\cdot \nabla{f(\lambda \mathbf{x})})\equiv \frac{d}{d\lambda}(kf(\lambda \mathbf{x}))$$ $$\mathbf{x}\cdot \nabla{f(\lambda \mathbf{x})}+\lambda \mathbf{x}\cdot \frac{d}{d\lambda}(\nabla{f(\mathbf{\lambda x})})\equiv k\frac{d}{d\lambda}(f(\lambda \mathbf{x}))$$ So basically the proof is identical except for this extra term $$\lambda \mathbf{x}\cdot \frac{d}{d\lambda}(\nabla{f(\mathbf{\lambda x})})$$ As we know the proof is true, what is the rationale/reasoning or constraint that forces this term to go to zero? ============================================================= Edit: Further attempt to simplify the problematic term using index notation: $$P=\lambda \mathbf{x}\cdot \frac{d}{d\lambda}(\nabla{f(\mathbf{\lambda x})})$$ This can be rewritten using Einstein Notation (and for convenience and clarity, using $d_{\nu}$ to denote $\frac{d}{d\nu}$ and $\lambda_i$ to denote $\lambda x^i$ when it appeared as an index. Also $\lambda$ is NOT an index in this evaluation) as $$=\lambda x^id_{\lambda}(\partial_i f(\lambda x^j))$$ Using chain rule $$=\lambda x^id_{\lambda}(\lambda\partial_{\lambda_i} f(\lambda x^j))$$ $$=\lambda x^id_{\lambda}(\lambda\partial_{\lambda_i} f(\lambda x^j))$$ Now letting $y^{i'}=\lambda x^i$ and $z^{j'}=\lambda x^j$ then $$=y^{i'}d_{\lambda}(\lambda\partial_{i'} f(z^{j'}))$$ $$=y^{i'}(\partial_{i'} f(z^{j'})+\lambda d_{\lambda}(\partial_{i'} f(z^{j'}))$$ Using chain rule again on $d_\lambda$ $$=y^{i'}(\partial_{i'} f(z^{j'})+\lambda (x^{k}\partial_{k'}\partial_{i'} f(z^{j'})+(\partial_{\lambda}x^k)\partial_{i'} f(z^{j'}))$$ Unpacking the expression by reverting to normal indices $$=\lambda x^i(\frac{1}{\lambda}\partial_{i} f(\lambda x^j)+\lambda (x^{k}\frac{1}{\lambda^{(2)}}\partial_{k}\partial_{i} f(\lambda x^j)+(\partial_{\lambda}x^k)\frac{1}{\lambda}\partial_{i} f(\lambda x^j))$$ $$= x^i\partial_{i} f(\lambda x^j)+ x^ix^{k}\partial_{k}\partial_{i} f(\lambda x^j)+\lambda x^i(\partial_{\lambda}x^k)\partial_{i} f(\lambda x^j)$$ Unpacking the expression by reverting to del operators $$= \mathbf{x}\cdot\nabla f(\lambda \mathbf{x})+ \mathbf{x}\cdot ( (\mathbf{x}\cdot\nabla)\otimes(\nabla f(\lambda \mathbf{x}))+\lambda \frac{\partial \mathbf{x}}{\partial\lambda}\otimes(\mathbf{x}\cdot\nabla) f(\lambda \mathbf{x})$$ The partial derivative is zero since $\mathbf{x}$ is independent of $\lambda$ Thus finally $$= \mathbf{x}\cdot\nabla f(\lambda \mathbf{x})+ \mathbf{x}\cdot ( (\mathbf{x}\cdot\nabla)\otimes(\nabla f(\lambda \mathbf{x}))$$ But this term is not necessary zero for all $\mathbf{x}$ , so how does the proof of the converse of the Euler Homogeneous function theorem in the pics above got rid of it?","So basically I read someone else's answer to a question regarding Euler Homogeneous function theorem source: https://quant.stackexchange.com/questions/8911/what-is-exactly-eulers-decomposition Also here https://planetmath.org/converseofeulershomogeneousfunctiontheorem I tried to do the proof by hand and this is what I get using product rules etc. Given Replace x by (The same as the letting step) Differentiating both sides wrt So basically the proof is identical except for this extra term As we know the proof is true, what is the rationale/reasoning or constraint that forces this term to go to zero? ============================================================= Edit: Further attempt to simplify the problematic term using index notation: This can be rewritten using Einstein Notation (and for convenience and clarity, using to denote and to denote when it appeared as an index. Also is NOT an index in this evaluation) as Using chain rule Now letting and then Using chain rule again on Unpacking the expression by reverting to normal indices Unpacking the expression by reverting to del operators The partial derivative is zero since is independent of Thus finally But this term is not necessary zero for all , so how does the proof of the converse of the Euler Homogeneous function theorem in the pics above got rid of it?",\mathbf{x}\cdot \nabla{f(\mathbf{x})}\equiv kf(\mathbf{x}) \lambda x \phi(\lambda) \lambda \mathbf{x}\cdot \nabla{f(\lambda \mathbf{x})}\equiv kf(\lambda \mathbf{x}) \lambda \frac{d}{d\lambda}(\lambda \mathbf{x}\cdot \nabla{f(\lambda \mathbf{x})})\equiv \frac{d}{d\lambda}(kf(\lambda \mathbf{x})) \mathbf{x}\cdot \nabla{f(\lambda \mathbf{x})}+\lambda \mathbf{x}\cdot \frac{d}{d\lambda}(\nabla{f(\mathbf{\lambda x})})\equiv k\frac{d}{d\lambda}(f(\lambda \mathbf{x})) \lambda \mathbf{x}\cdot \frac{d}{d\lambda}(\nabla{f(\mathbf{\lambda x})}) P=\lambda \mathbf{x}\cdot \frac{d}{d\lambda}(\nabla{f(\mathbf{\lambda x})}) d_{\nu} \frac{d}{d\nu} \lambda_i \lambda x^i \lambda =\lambda x^id_{\lambda}(\partial_i f(\lambda x^j)) =\lambda x^id_{\lambda}(\lambda\partial_{\lambda_i} f(\lambda x^j)) =\lambda x^id_{\lambda}(\lambda\partial_{\lambda_i} f(\lambda x^j)) y^{i'}=\lambda x^i z^{j'}=\lambda x^j =y^{i'}d_{\lambda}(\lambda\partial_{i'} f(z^{j'})) =y^{i'}(\partial_{i'} f(z^{j'})+\lambda d_{\lambda}(\partial_{i'} f(z^{j'})) d_\lambda =y^{i'}(\partial_{i'} f(z^{j'})+\lambda (x^{k}\partial_{k'}\partial_{i'} f(z^{j'})+(\partial_{\lambda}x^k)\partial_{i'} f(z^{j'})) =\lambda x^i(\frac{1}{\lambda}\partial_{i} f(\lambda x^j)+\lambda (x^{k}\frac{1}{\lambda^{(2)}}\partial_{k}\partial_{i} f(\lambda x^j)+(\partial_{\lambda}x^k)\frac{1}{\lambda}\partial_{i} f(\lambda x^j)) = x^i\partial_{i} f(\lambda x^j)+ x^ix^{k}\partial_{k}\partial_{i} f(\lambda x^j)+\lambda x^i(\partial_{\lambda}x^k)\partial_{i} f(\lambda x^j) = \mathbf{x}\cdot\nabla f(\lambda \mathbf{x})+ \mathbf{x}\cdot ( (\mathbf{x}\cdot\nabla)\otimes(\nabla f(\lambda \mathbf{x}))+\lambda \frac{\partial \mathbf{x}}{\partial\lambda}\otimes(\mathbf{x}\cdot\nabla) f(\lambda \mathbf{x}) \mathbf{x} \lambda = \mathbf{x}\cdot\nabla f(\lambda \mathbf{x})+ \mathbf{x}\cdot ( (\mathbf{x}\cdot\nabla)\otimes(\nabla f(\lambda \mathbf{x})) \mathbf{x},"['multivariable-calculus', 'functions', 'tensors']"
97,Function from $\mathbb R^{2}$ to $\mathbb R^{2}$,Function from  to,\mathbb R^{2} \mathbb R^{2},"Let, $z=x+iy$ and $f:\mathbb R^{2}\to \mathbb R^{2}$ be the function $f(x,y)=f(z)=z^{2}=(x^{2}-y^{2},2xy). Let $$(Df)(a)$ denotes the derivative of $f$ at $a$. Then which are correct? $(Df)(a)h=2ah$, where $a=a_{1}+ia_{2}$ & $h=h_{1}+ih_{2}.$ $(Df)(a)=2\begin{bmatrix}a_{1} & -a_{2} \\ a_{2} & a_{1}\end{bmatrix}$. $f$  is one-one. For any $a\in \mathbb R^{n}\setminus \{(0,0)\}$, $f$ is one-one on some nbd. of $a$. We know that, $(Df)(a)=\bigtriangledown f(a).$ So, option (2) is correct. But, how option (a) is correct ? Also the others are true or not?","Let, $z=x+iy$ and $f:\mathbb R^{2}\to \mathbb R^{2}$ be the function $f(x,y)=f(z)=z^{2}=(x^{2}-y^{2},2xy). Let $$(Df)(a)$ denotes the derivative of $f$ at $a$. Then which are correct? $(Df)(a)h=2ah$, where $a=a_{1}+ia_{2}$ & $h=h_{1}+ih_{2}.$ $(Df)(a)=2\begin{bmatrix}a_{1} & -a_{2} \\ a_{2} & a_{1}\end{bmatrix}$. $f$  is one-one. For any $a\in \mathbb R^{n}\setminus \{(0,0)\}$, $f$ is one-one on some nbd. of $a$. We know that, $(Df)(a)=\bigtriangledown f(a).$ So, option (2) is correct. But, how option (a) is correct ? Also the others are true or not?",,"['calculus', 'multivariable-calculus']"
98,Find minima of the multivariable function $\frac{4}{x^2+y^2+1}+2 xy$,Find minima of the multivariable function,\frac{4}{x^2+y^2+1}+2 xy,"$$f(x,y)=\frac{4}{x^2+y^2+1}+2 xy \\ \text{within the  domain: }1/5\leq x^2+y^2\leq 4$$ I am able to find the maximum of the function at $x^2+y^2=4$ by substituting x,y for $\cos(t)$ and $\sin(t)$ and I am able to start working for a solution in the lower boundary: $$ x=\frac{1}{\sqrt5}\cos(t), \  y=\frac{1}{\sqrt5}\sin(t) \\  \frac{4}{\frac{6}{5}}+\frac{2}{\sqrt5}(\cos(t)\sin(t))\\  \frac{df}{dt}=\frac{2}{\sqrt5}\cos(2t)=0\\ \tan(t)=1\\ x=\frac{1}{\sqrt5}\lvert\cos(\frac{\pi}{4})\rvert \ ,  \ \ y=\frac{1}{\sqrt5}\lvert\sin(\frac{\pi}{4})\rvert \\ x=\pm\frac{1}{\sqrt{10}},y=\pm\frac{1}{\sqrt{10}}$$ When i resubstitute this into the original function I get the wrong answer. $$f\left(\frac{1}{\sqrt{10}},\frac{1}{\sqrt{10}}\right)=\frac{4}{\frac{12}{10}}+\frac{2}{10}=53/15 $$","$$f(x,y)=\frac{4}{x^2+y^2+1}+2 xy \\ \text{within the  domain: }1/5\leq x^2+y^2\leq 4$$ I am able to find the maximum of the function at $x^2+y^2=4$ by substituting x,y for $\cos(t)$ and $\sin(t)$ and I am able to start working for a solution in the lower boundary: $$ x=\frac{1}{\sqrt5}\cos(t), \  y=\frac{1}{\sqrt5}\sin(t) \\  \frac{4}{\frac{6}{5}}+\frac{2}{\sqrt5}(\cos(t)\sin(t))\\  \frac{df}{dt}=\frac{2}{\sqrt5}\cos(2t)=0\\ \tan(t)=1\\ x=\frac{1}{\sqrt5}\lvert\cos(\frac{\pi}{4})\rvert \ ,  \ \ y=\frac{1}{\sqrt5}\lvert\sin(\frac{\pi}{4})\rvert \\ x=\pm\frac{1}{\sqrt{10}},y=\pm\frac{1}{\sqrt{10}}$$ When i resubstitute this into the original function I get the wrong answer. $$f\left(\frac{1}{\sqrt{10}},\frac{1}{\sqrt{10}}\right)=\frac{4}{\frac{12}{10}}+\frac{2}{10}=53/15 $$",,['multivariable-calculus']
99,Setting up a Green's Theorem Problem where C is not oriented counterclockwise,Setting up a Green's Theorem Problem where C is not oriented counterclockwise,,"Use Green's Theorem to evaluate $\mathbf{F}(x,y)=\langle y^{2}\cos x, x^{2}+2y\sin x\rangle$, where $C$ is the triangle from $(0,0)$ to $(2,6)$ to $(2,0)$ to $(0,0)$. Taking the appropriate partial derivatives, I have my integral set up as $\displaystyle \int_{C}y^{2}\cos x\, dx +(x^{2}+2y\sin x)\,dy = \displaystyle \int_{2}^{0}\int_{0}^{3x}2x\, dy\, dx$.  My reasoning for the order of the limits of integration was that since we are going from 2 to 0 along the bottom edge of the triangle, the $dx$ integral should have limits of integration from 2 to 0. For the $dy$, integral, however, I'm not quite as sure.  In the y-direction, it seems as though we're going from 0 to $3x$, because of how the curve is oriented along the hypotenuse. But, I'm second-guessing myself - I still don't feel like I have the hang of setting these problems up, so if you could 1) tell me whether I'm right, and 2) if I'm not right, explain to me why, so I don't make the same mistake again.","Use Green's Theorem to evaluate $\mathbf{F}(x,y)=\langle y^{2}\cos x, x^{2}+2y\sin x\rangle$, where $C$ is the triangle from $(0,0)$ to $(2,6)$ to $(2,0)$ to $(0,0)$. Taking the appropriate partial derivatives, I have my integral set up as $\displaystyle \int_{C}y^{2}\cos x\, dx +(x^{2}+2y\sin x)\,dy = \displaystyle \int_{2}^{0}\int_{0}^{3x}2x\, dy\, dx$.  My reasoning for the order of the limits of integration was that since we are going from 2 to 0 along the bottom edge of the triangle, the $dx$ integral should have limits of integration from 2 to 0. For the $dy$, integral, however, I'm not quite as sure.  In the y-direction, it seems as though we're going from 0 to $3x$, because of how the curve is oriented along the hypotenuse. But, I'm second-guessing myself - I still don't feel like I have the hang of setting these problems up, so if you could 1) tell me whether I'm right, and 2) if I'm not right, explain to me why, so I don't make the same mistake again.",,['calculus']
