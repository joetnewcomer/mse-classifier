,title_raw,title_text,title_latex,body_raw,body_text,body_latex,tags
0,Self Study of Fractals,Self Study of Fractals,,"I am looking for a book to self-study fractals with a certain criteria. I have checked out Getting Aquainted with Fractals . Note that Getting Aquainted with Fractals does not include exercises/problems. I would like to find a book that has exercises/problems, geometrical interpretations and more explanations (I do not like textbooks that are full of fluff, but I also do not think I am ready to fill in every blank either). To clarify about the last criteria, I do like books that are in the definition, theorem, proof format, however I would like a book that presents easier to read proofs that contain more detail. Also, I would like some discussions that prelude definitions which give their motivation and possibly give rise to geometric intuition. I would also like a supplementary text covering the material in Mathematical Analysis as a lot of vocabulary, concepts and ideas come from analysis. I know the basic concepts and properties: Self-Similarity, Recursively defined, and the Box Counting Dimension. I know about the classic fractals: Koch Curve, Sierpinski triangle and their variations. However, I do not know anything about fractals defined in the complex plane such as Mandelbrot set and Julia set. I also do not know much about the link between fractal geometry and chaotic theory. Thanks for all the help!","I am looking for a book to self-study fractals with a certain criteria. I have checked out Getting Aquainted with Fractals . Note that Getting Aquainted with Fractals does not include exercises/problems. I would like to find a book that has exercises/problems, geometrical interpretations and more explanations (I do not like textbooks that are full of fluff, but I also do not think I am ready to fill in every blank either). To clarify about the last criteria, I do like books that are in the definition, theorem, proof format, however I would like a book that presents easier to read proofs that contain more detail. Also, I would like some discussions that prelude definitions which give their motivation and possibly give rise to geometric intuition. I would also like a supplementary text covering the material in Mathematical Analysis as a lot of vocabulary, concepts and ideas come from analysis. I know the basic concepts and properties: Self-Similarity, Recursively defined, and the Box Counting Dimension. I know about the classic fractals: Koch Curve, Sierpinski triangle and their variations. However, I do not know anything about fractals defined in the complex plane such as Mandelbrot set and Julia set. I also do not know much about the link between fractal geometry and chaotic theory. Thanks for all the help!",,"['analysis', 'reference-request', 'self-learning', 'fractals']"
1,Weakly differentiable but classically nowhere differentiable,Weakly differentiable but classically nowhere differentiable,,Is there any example of a function which is weakly differentiable but none of its versions are classically differentiable anywhere (or differentiable only on a set of measure 0) ? Thanks,Is there any example of a function which is weakly differentiable but none of its versions are classically differentiable anywhere (or differentiable only on a set of measure 0) ? Thanks,,"['analysis', 'partial-differential-equations', 'weak-derivatives']"
2,$|f(x)-f(y)|\geq k|x-y|$.Then $f$ is bijective and its inverse is continuous.,.Then  is bijective and its inverse is continuous.,|f(x)-f(y)|\geq k|x-y| f,"My exercise says: Let $f:\mathbb{R} \rightarrow \mathbb{R}$ a continuous function e suppose that exists $k$ such that: $$|f(x)-f(y)|\geq k|x-y|$$ Then $f$ is bijective and  its inverse is continuous. Well, there's a Theorem , Invariance of domain, that says ""If $U$ is an open subset of $\mathbb {R^n}$ and $f : U \rightarrow\mathbb{R}$ is an injective continuous map, then $V = f(U)$ is open and $f$ is a homeomorphism between $U$ and $V$"". But I'm not knowing how to proceed...need a clue...Thanks for attention!!!","My exercise says: Let $f:\mathbb{R} \rightarrow \mathbb{R}$ a continuous function e suppose that exists $k$ such that: $$|f(x)-f(y)|\geq k|x-y|$$ Then $f$ is bijective and  its inverse is continuous. Well, there's a Theorem , Invariance of domain, that says ""If $U$ is an open subset of $\mathbb {R^n}$ and $f : U \rightarrow\mathbb{R}$ is an injective continuous map, then $V = f(U)$ is open and $f$ is a homeomorphism between $U$ and $V$"". But I'm not knowing how to proceed...need a clue...Thanks for attention!!!",,"['analysis', 'continuity']"
3,How prove this function inequality $xf(x)>\frac{1}{x}f\left(\frac{1}{x}\right)$,How prove this function inequality,xf(x)>\frac{1}{x}f\left(\frac{1}{x}\right),"Let $f(x)$ be monotone decreasing on $(0,+\infty)$, such that   $$0<f(x)<\lvert f'(x) \rvert,\qquad\forall x\in (0,+\infty).$$ Show that   $$xf(x)>\dfrac{1}{x}f\left(\dfrac{1}{x}\right),\qquad\forall x\in(0,1).$$ My ideas: Since $f(x)$ is monotone decreasing, $f'(x)<0$, hence $$f(x)+f'(x)<0.$$ Let $$F(x)=e^xf(x)\Longrightarrow F'(x)=e^x(f(x)+f'(x))<0$$ so $F(x)$ is also monotone decreasing. Since $0<x<1$, $$F(x)>F\left(\dfrac{1}{x}\right)$$ so $$e^xf(x)>e^{\frac{1}{x}}f\left(\frac1x\right).$$ So we must prove $$e^{\frac{1}{x}-x}>\dfrac{1}{x^2},\qquad0<x<1$$ $$\Longleftrightarrow \ln{x}-x>\ln{\dfrac{1}{x}}-\dfrac{1}{x},0<x<1$$ because $0<x<1,\dfrac{1}{x}>1$ so I can't.  But I don't know whether this inequality is true. I tried Wolfram Alpha but it didn't tell me anything definitive. PS: This problem is from a Chinese analysis problem book by Huimin Xie.","Let $f(x)$ be monotone decreasing on $(0,+\infty)$, such that   $$0<f(x)<\lvert f'(x) \rvert,\qquad\forall x\in (0,+\infty).$$ Show that   $$xf(x)>\dfrac{1}{x}f\left(\dfrac{1}{x}\right),\qquad\forall x\in(0,1).$$ My ideas: Since $f(x)$ is monotone decreasing, $f'(x)<0$, hence $$f(x)+f'(x)<0.$$ Let $$F(x)=e^xf(x)\Longrightarrow F'(x)=e^x(f(x)+f'(x))<0$$ so $F(x)$ is also monotone decreasing. Since $0<x<1$, $$F(x)>F\left(\dfrac{1}{x}\right)$$ so $$e^xf(x)>e^{\frac{1}{x}}f\left(\frac1x\right).$$ So we must prove $$e^{\frac{1}{x}-x}>\dfrac{1}{x^2},\qquad0<x<1$$ $$\Longleftrightarrow \ln{x}-x>\ln{\dfrac{1}{x}}-\dfrac{1}{x},0<x<1$$ because $0<x<1,\dfrac{1}{x}>1$ so I can't.  But I don't know whether this inequality is true. I tried Wolfram Alpha but it didn't tell me anything definitive. PS: This problem is from a Chinese analysis problem book by Huimin Xie.",,"['analysis', 'inequality', 'functional-inequalities']"
4,Interesting phenomenon with the $\zeta(3)$ series,Interesting phenomenon with the  series,\zeta(3),"I noticed that if one takes certain partial sums of the series for $\zeta(3)$: $$\zeta(3) = \sum_{n=1}^{\infty} \frac{1}{n^3} \approx \sum_{n=1}^{N} \frac{1}{n^3}$$ an interesting phenomenon occurs for some values of $N$. For example, with $N = 100000$, the sum is $$1.2020569031095947853972381615115330296...$$ while the exact value is $$1.2020569031595942853997381615114499908...$$ . Note that there are stretches of agreement of the digits beyond the initial segment: $$1.2020569031(0)9594(7)8539(72)38161511\ (\mathrm{pattern}\ \mathrm{ends})$$ where the parentheses represent disagreeing digits. Why does this happen, what values of ""N"" give the best ""pseudo-approximations"", and what is a proof of those answers?","I noticed that if one takes certain partial sums of the series for $\zeta(3)$: $$\zeta(3) = \sum_{n=1}^{\infty} \frac{1}{n^3} \approx \sum_{n=1}^{N} \frac{1}{n^3}$$ an interesting phenomenon occurs for some values of $N$. For example, with $N = 100000$, the sum is $$1.2020569031095947853972381615115330296...$$ while the exact value is $$1.2020569031595942853997381615114499908...$$ . Note that there are stretches of agreement of the digits beyond the initial segment: $$1.2020569031(0)9594(7)8539(72)38161511\ (\mathrm{pattern}\ \mathrm{ends})$$ where the parentheses represent disagreeing digits. Why does this happen, what values of ""N"" give the best ""pseudo-approximations"", and what is a proof of those answers?",,"['analysis', 'number-theory', 'riemann-zeta', 'dirichlet-series']"
5,Is the function differentiable,Is the function differentiable,,Let $f: \mathbb{R} \rightarrow \mathbb{R}$ be a continuous function such that for $x_0 \in \mathbb{R}$ $$ \lim_{\mathbb{Q} \ni h \to 0} \frac{f(x_0 + h) - f(x_0)}{h}$$ exists. Is this function differentiable at $x_0$?,Let $f: \mathbb{R} \rightarrow \mathbb{R}$ be a continuous function such that for $x_0 \in \mathbb{R}$ $$ \lim_{\mathbb{Q} \ni h \to 0} \frac{f(x_0 + h) - f(x_0)}{h}$$ exists. Is this function differentiable at $x_0$?,,['analysis']
6,Divided differences and differentiability,Divided differences and differentiability,,"Let  $f: R \rightarrow R$, $y_0,y_1,y_2 \in R$.   We define divided differences: $$[y_0;f]=f(y_0),$$ $$[y_0,y_1;f]=\frac{f(y_1)-f(y_0)}{y_1-y_0},$$ $$[y_0,y_1,y_2;f]=\frac{[y_1,y_2;f]-[y_0,y_1;f]}{y_2-y_0}.$$ Assume that for each $x \in R$ and  $\varepsilon>0$ there exists a $\delta>0$ such that $$|[y_0,y_1,y_2;f]-[y_0',y_1',y_2';f]|< \varepsilon$$ for arbitrary sets $\{y_0,y_1,y_2\}$, $\{y_0',y_1',y_2'\}$ of distinct points such that $|y_i-x|<\delta$, $|y_i'-x|<\delta$ ($i=0,1,2$). Is it $f$ of class $C^2$ ?","Let  $f: R \rightarrow R$, $y_0,y_1,y_2 \in R$.   We define divided differences: $$[y_0;f]=f(y_0),$$ $$[y_0,y_1;f]=\frac{f(y_1)-f(y_0)}{y_1-y_0},$$ $$[y_0,y_1,y_2;f]=\frac{[y_1,y_2;f]-[y_0,y_1;f]}{y_2-y_0}.$$ Assume that for each $x \in R$ and  $\varepsilon>0$ there exists a $\delta>0$ such that $$|[y_0,y_1,y_2;f]-[y_0',y_1',y_2';f]|< \varepsilon$$ for arbitrary sets $\{y_0,y_1,y_2\}$, $\{y_0',y_1',y_2'\}$ of distinct points such that $|y_i-x|<\delta$, $|y_i'-x|<\delta$ ($i=0,1,2$). Is it $f$ of class $C^2$ ?",,['analysis']
7,Proof Verification : Prove -(-a)=a using only ordered field axioms [duplicate],Proof Verification : Prove -(-a)=a using only ordered field axioms [duplicate],,"This question already has answers here : Prove $-(-a)=a$ using only ordered field axioms (6 answers) Closed 7 years ago . I need to prove for all real numbers $a$, $-(-a) = a$ using only the following axioms: Thanks to many members of the Mathematics Stackexchange Community, I have the following proof worked out: Theorem: The Additive Inverse Identity is Unique $ ( \forall a,b,c \in \mathbb{R} )(a+b=0) \land (a+c=0) \\ ( \forall a,b,c \in \mathbb{R} )(a+b=a+c) \\ ( \forall a,b,c \in \mathbb{R} )(b=c) \\ \text{QED} \\ $ Theorem $\textbf{a} \cdot \textbf{0 = 0}$ $ \begin{align} a \cdot 0 &= a \cdot 0 \\ a \cdot (0 + 0) &= a \cdot 0 \\ a \cdot 0 + a \cdot 0 &= a \cdot 0 \\ a \cdot 0 + a \cdot 0 + (-a) \cdot 0 &= a \cdot 0 + (-a) \cdot 0 \\ a \cdot 0 &= 0 \\ &\text{QED} \\ \end{align} $ Theorem: $-\textbf{1} \cdot \textbf{(a) = (}-\textbf{a)}$ $ \begin{align} a \cdot 0 &= 0 \\ a \cdot \left[ 1 + (-1) \right] &= a + (-a) \\ 1 \cdot a + (-1) \cdot a &= a + (-a) \\ -1 \cdot (a) &= (-a) \\ &\text{QED} \\ \end{align} $ Theorem: $-\textbf{(}-\textbf{a) = a}$ $ \begin{align} 0 &= 0 \\ -a \cdot 0 &= 0 \\ -a \cdot \left[ 1 + (-1) \right] &= 0 \\ -a \cdot 1 + -a \cdot (-1) &= 0 \\ -a + \left[ -(-a) \right] &= 0 \\ -(-a) &= a \\ \text{QED} \\ \end{align} $ Does everything look alright? Have I missed anything? Also--why is it necessary to show that the additive inverse is unique? Many thanks in advance.","This question already has answers here : Prove $-(-a)=a$ using only ordered field axioms (6 answers) Closed 7 years ago . I need to prove for all real numbers $a$, $-(-a) = a$ using only the following axioms: Thanks to many members of the Mathematics Stackexchange Community, I have the following proof worked out: Theorem: The Additive Inverse Identity is Unique $ ( \forall a,b,c \in \mathbb{R} )(a+b=0) \land (a+c=0) \\ ( \forall a,b,c \in \mathbb{R} )(a+b=a+c) \\ ( \forall a,b,c \in \mathbb{R} )(b=c) \\ \text{QED} \\ $ Theorem $\textbf{a} \cdot \textbf{0 = 0}$ $ \begin{align} a \cdot 0 &= a \cdot 0 \\ a \cdot (0 + 0) &= a \cdot 0 \\ a \cdot 0 + a \cdot 0 &= a \cdot 0 \\ a \cdot 0 + a \cdot 0 + (-a) \cdot 0 &= a \cdot 0 + (-a) \cdot 0 \\ a \cdot 0 &= 0 \\ &\text{QED} \\ \end{align} $ Theorem: $-\textbf{1} \cdot \textbf{(a) = (}-\textbf{a)}$ $ \begin{align} a \cdot 0 &= 0 \\ a \cdot \left[ 1 + (-1) \right] &= a + (-a) \\ 1 \cdot a + (-1) \cdot a &= a + (-a) \\ -1 \cdot (a) &= (-a) \\ &\text{QED} \\ \end{align} $ Theorem: $-\textbf{(}-\textbf{a) = a}$ $ \begin{align} 0 &= 0 \\ -a \cdot 0 &= 0 \\ -a \cdot \left[ 1 + (-1) \right] &= 0 \\ -a \cdot 1 + -a \cdot (-1) &= 0 \\ -a + \left[ -(-a) \right] &= 0 \\ -(-a) &= a \\ \text{QED} \\ \end{align} $ Does everything look alright? Have I missed anything? Also--why is it necessary to show that the additive inverse is unique? Many thanks in advance.",,"['analysis', 'proof-verification', 'field-theory']"
8,Curious facts about ordinal numbers,Curious facts about ordinal numbers,,"I have some notes about curious facts about ordinal numbers , for example that their addition is not commutative, multiplication is not distributive from the right hand side and that the exponent rule doesn't always hold. Also that some things that are undefined in analysis like $0^0, \infty^0, 1^\infty$ are actually defined for ordinal numbers. I know that there's been some investigations devoted to pursuing ordinal arithmetic along the lines of classical results in number theory for example. Do you happen to know other curious facts about ordinal numbers (compared to facts in analysis or other)?","I have some notes about curious facts about ordinal numbers , for example that their addition is not commutative, multiplication is not distributive from the right hand side and that the exponent rule doesn't always hold. Also that some things that are undefined in analysis like $0^0, \infty^0, 1^\infty$ are actually defined for ordinal numbers. I know that there's been some investigations devoted to pursuing ordinal arithmetic along the lines of classical results in number theory for example. Do you happen to know other curious facts about ordinal numbers (compared to facts in analysis or other)?",,"['number-theory', 'analysis', 'set-theory', 'ordinals']"
9,"Determining the action of the operator $D\left(z, \frac d{dz}\right)$",Determining the action of the operator,"D\left(z, \frac d{dz}\right)","This question was motivated by a question by Tobias Kienzler and its wonderful answers. I begin as in the linked question... Using the Taylor expansion $$f(z+a) = \sum_{k=0}^\infty \frac{a^k}{k!}\frac{d^k }{dz^k}f(z)$$ one can formally express the sum as the linear operator $e^{a\frac{d}{dz}}$ to obtain $$f(z+a) = e^{a\frac{d}{dz}}f(z).$$ Other relationships were given in an answer by Tom Copeland : $$ f(e^b z) = \exp\left(bz\frac d{dz}\right)f(z), $$ $$ f\left(\frac z{1-cz}\right) = \exp\left(c z^{2}\frac d{dz}\right)f(z). $$ My question is about the reverse. What if we start on the right-hand side with a function different from $\exp$ in the operator? I asked about the specific case for $\sin$ in the comments of joriki's answer and Tobias found that $$ \sin\!\left(a\frac{d}{dz}\right)f(z) = \frac{1}{2i}(f(z+ia) - f(z-ia)), $$ and similarly that $$ \cosh\!\left(a\frac{d}{dz}\right)f(z) = \frac{1}{2}(f(z+a) - f(z-a)). $$ He also conjectured that the symmetrization of a function might be obtained from an operator like $$\exp\left(i\frac\pi2\frac d{d\ln z}\right)\cosh\left(i\frac\pi2\frac d{d\ln z}\right)$$ I don't know much about Lie algebras so I apologize if this is too broad: For which operators $\text{D}$ like these is $\text{D}f$ something 'nice' as in these examples?","This question was motivated by a question by Tobias Kienzler and its wonderful answers. I begin as in the linked question... Using the Taylor expansion $$f(z+a) = \sum_{k=0}^\infty \frac{a^k}{k!}\frac{d^k }{dz^k}f(z)$$ one can formally express the sum as the linear operator $e^{a\frac{d}{dz}}$ to obtain $$f(z+a) = e^{a\frac{d}{dz}}f(z).$$ Other relationships were given in an answer by Tom Copeland : $$ f(e^b z) = \exp\left(bz\frac d{dz}\right)f(z), $$ $$ f\left(\frac z{1-cz}\right) = \exp\left(c z^{2}\frac d{dz}\right)f(z). $$ My question is about the reverse. What if we start on the right-hand side with a function different from $\exp$ in the operator? I asked about the specific case for $\sin$ in the comments of joriki's answer and Tobias found that $$ \sin\!\left(a\frac{d}{dz}\right)f(z) = \frac{1}{2i}(f(z+ia) - f(z-ia)), $$ and similarly that $$ \cosh\!\left(a\frac{d}{dz}\right)f(z) = \frac{1}{2}(f(z+a) - f(z-a)). $$ He also conjectured that the symmetrization of a function might be obtained from an operator like $$\exp\left(i\frac\pi2\frac d{d\ln z}\right)\cosh\left(i\frac\pi2\frac d{d\ln z}\right)$$ I don't know much about Lie algebras so I apologize if this is too broad: For which operators $\text{D}$ like these is $\text{D}f$ something 'nice' as in these examples?",,"['analysis', 'lie-algebras', 'operator-theory', 'differential-operators']"
10,The inner product of the Cartesian Product space,The inner product of the Cartesian Product space,,"I want to know how can one define the inner product in the Cartesian product of spaces, i.e. let $A,B$ two hilbert spaces. Let $a_1, a_2 \in A$  and $b_1, b_2 \in B$, how can one express the inner product $\langle (a_1, b_1), (a_2, b_2) \rangle _{A \times B}$? is it just the sum of the $A$ and $B$ products: $\langle (a_1, b_1), (a_2, b_2) \rangle _{A \times B}= \langle a_1 , a_2 \rangle_A + \langle b_1 ,b_2 \rangle_B$? thank you, I will appreciate your help","I want to know how can one define the inner product in the Cartesian product of spaces, i.e. let $A,B$ two hilbert spaces. Let $a_1, a_2 \in A$  and $b_1, b_2 \in B$, how can one express the inner product $\langle (a_1, b_1), (a_2, b_2) \rangle _{A \times B}$? is it just the sum of the $A$ and $B$ products: $\langle (a_1, b_1), (a_2, b_2) \rangle _{A \times B}= \langle a_1 , a_2 \rangle_A + \langle b_1 ,b_2 \rangle_B$? thank you, I will appreciate your help",,"['analysis', 'hilbert-spaces']"
11,$C^\infty$ version of Urysohn Lemma in $\Bbb R^n$,version of Urysohn Lemma in,C^\infty \Bbb R^n,"I'm trying to solve an exercise which its conclusion seems to be the title of this post. The exercise is: Show that the function $h:\Bbb R\to [0,1[$ given by   $$h(t)=\begin{cases} e^{-1/t^2} &\text{if } t\neq 0\\ 0          &\text{otherwise} \end{cases}$$   is $C^\infty$. Show that the functions   $$h_+(t)=\begin{cases} e^{-1/t^2} &\text{if } t\gt 0\\ 0          &\text{otherwise} \end{cases}\quad\text{and}\quad h_{-}(t)=\begin{cases} e^{-1/t^2} &\text{if } t\lt 0\\ 0          &\text{otherwise} \end{cases}$$   are $C^\infty$. Show that the function $k:\Bbb R\to [0,1[$ given by $k(t)=h_-(t-b)h_+(t-a)$ is $C^\infty$ and positive for $t\in ]a,b[$. Let $R$ the rentangle $]a_1,b_1[\times\cdots\times]a_n,b_n[$. Show that there is a $C^\infty$ function $g:\Bbb R^n\to [0,1[$ strictly positive on $R$. Conclude that if $K$ is a compact subset of $\Bbb R^n$ and $U$ is an open neighborhood of $K$, there is a $C^\infty$ function $f:\Bbb R^n\to [0,1]$ such that $f_{|K}\equiv 1$ and its support is contained in $U$. From 1.-4. I can prove that for any open and bounded set $O\subset \Bbb R^n$, there is a $C^\infty$ function with its support contained in $O$. So my first attempt was apply this to the open $U\setminus K$. Then I get a $C^\infty$ function $f$ that is $0$ (in particular) over $K$. If I just consider $\chi_K+f$ that function can fail to be $C^\infty$. In a discussion on the chat, robjohn suggest this . It works fine, but then my question is: Can 5. be proved by using 1.-4.? If yes, how?","I'm trying to solve an exercise which its conclusion seems to be the title of this post. The exercise is: Show that the function $h:\Bbb R\to [0,1[$ given by   $$h(t)=\begin{cases} e^{-1/t^2} &\text{if } t\neq 0\\ 0          &\text{otherwise} \end{cases}$$   is $C^\infty$. Show that the functions   $$h_+(t)=\begin{cases} e^{-1/t^2} &\text{if } t\gt 0\\ 0          &\text{otherwise} \end{cases}\quad\text{and}\quad h_{-}(t)=\begin{cases} e^{-1/t^2} &\text{if } t\lt 0\\ 0          &\text{otherwise} \end{cases}$$   are $C^\infty$. Show that the function $k:\Bbb R\to [0,1[$ given by $k(t)=h_-(t-b)h_+(t-a)$ is $C^\infty$ and positive for $t\in ]a,b[$. Let $R$ the rentangle $]a_1,b_1[\times\cdots\times]a_n,b_n[$. Show that there is a $C^\infty$ function $g:\Bbb R^n\to [0,1[$ strictly positive on $R$. Conclude that if $K$ is a compact subset of $\Bbb R^n$ and $U$ is an open neighborhood of $K$, there is a $C^\infty$ function $f:\Bbb R^n\to [0,1]$ such that $f_{|K}\equiv 1$ and its support is contained in $U$. From 1.-4. I can prove that for any open and bounded set $O\subset \Bbb R^n$, there is a $C^\infty$ function with its support contained in $O$. So my first attempt was apply this to the open $U\setminus K$. Then I get a $C^\infty$ function $f$ that is $0$ (in particular) over $K$. If I just consider $\chi_K+f$ that function can fail to be $C^\infty$. In a discussion on the chat, robjohn suggest this . It works fine, but then my question is: Can 5. be proved by using 1.-4.? If yes, how?",,['analysis']
12,About a measurable function in $\mathbb{R}$,About a measurable function in,\mathbb{R},"Let $h:\mathbb{R}\rightarrow\mathbb{R}$ be a measurable function such that $$\left|\int_I h\right|\leq c \sqrt{|I|}$$ for each interval $I$.  Then $h_\epsilon(x)=h(x/\epsilon)$ satisfies $$\int_Ah_\epsilon(x)dx\rightarrow0 $$ as $\epsilon$ goes to zero, for each Borel set $A$ such that $|A|<\infty$.","Let $h:\mathbb{R}\rightarrow\mathbb{R}$ be a measurable function such that $$\left|\int_I h\right|\leq c \sqrt{|I|}$$ for each interval $I$.  Then $h_\epsilon(x)=h(x/\epsilon)$ satisfies $$\int_Ah_\epsilon(x)dx\rightarrow0 $$ as $\epsilon$ goes to zero, for each Borel set $A$ such that $|A|<\infty$.",,"['analysis', 'measure-theory']"
13,Is category theory useful in higher level Analysis?,Is category theory useful in higher level Analysis?,,"What I mean by higher level before this gets closed is functional analysis, complex analysis and harmonic analysis? I've read looked at the examples in most category theory books and it normally has little Analysis. Which, is strange as I've even seen lattice theory be used to motivate a whole book on category theory. I was wondering is there a nice application of category theory to functional analysis? It's weird as read that higher category theory is used in Quantum mechanics as it foundation, yet QM has heavy use of Hilbert spaces.","What I mean by higher level before this gets closed is functional analysis, complex analysis and harmonic analysis? I've read looked at the examples in most category theory books and it normally has little Analysis. Which, is strange as I've even seen lattice theory be used to motivate a whole book on category theory. I was wondering is there a nice application of category theory to functional analysis? It's weird as read that higher category theory is used in Quantum mechanics as it foundation, yet QM has heavy use of Hilbert spaces.",,"['analysis', 'soft-question', 'category-theory']"
14,Analytic number theory primer -- sequences and series,Analytic number theory primer -- sequences and series,,"For a book like Titchmarsh, or Iwaniec and Kowalski, it seems a thorough knowledge of asymptotics is a prerequisite. What are  good books for training oneself in such manipulation of asymptotics, bounding of integrals and all that paraphernalia which analytic number theorists seem to be very good at; but is not explicitly developed in these books? In other words, what is a good book for hard analysis?","For a book like Titchmarsh, or Iwaniec and Kowalski, it seems a thorough knowledge of asymptotics is a prerequisite. What are  good books for training oneself in such manipulation of asymptotics, bounding of integrals and all that paraphernalia which analytic number theorists seem to be very good at; but is not explicitly developed in these books? In other words, what is a good book for hard analysis?",,"['analysis', 'reference-request', 'asymptotics', 'analytic-number-theory', 'book-recommendation']"
15,Absolute convergence when all the rotated series converge,Absolute convergence when all the rotated series converge,,"The question here might be standard in some textbook. Let $a_n, n\ge1$ be a series of real numbers. It is evident that if  $\displaystyle \sum_{n\ge 1} |a_n|<+\infty$, then $\displaystyle \sum_{n\ge1} e^{2n\pi i t}a_n$ converges for all $0\le t< 1$. What about the converse implication? That is, Assume $\displaystyle \sum_{n\ge1} e^{2n\pi i t}a_n$ converges for all $0\le t< 1$. Does this imply $\displaystyle \sum_{n\ge 1} |a_n|<+\infty$?","The question here might be standard in some textbook. Let $a_n, n\ge1$ be a series of real numbers. It is evident that if  $\displaystyle \sum_{n\ge 1} |a_n|<+\infty$, then $\displaystyle \sum_{n\ge1} e^{2n\pi i t}a_n$ converges for all $0\le t< 1$. What about the converse implication? That is, Assume $\displaystyle \sum_{n\ge1} e^{2n\pi i t}a_n$ converges for all $0\le t< 1$. Does this imply $\displaystyle \sum_{n\ge 1} |a_n|<+\infty$?",,"['analysis', 'convergence-divergence', 'absolute-convergence']"
16,Function that is discontinuous only for integer fractions,Function that is discontinuous only for integer fractions,,I have this question: Find a function $f :\mathbb R \to\mathbb R$ which is discontinuous at the points of the   set $\{\frac1n : n \text{ a positive integer}\} \cup \{0\}$ but is continuous everywhere   else. I really don't know what to do. I was thinking maybe: $$ f(x) = \begin{cases} 1 \quad&\text{if }x=0  \\ 0 &\text{if } x \text{ is in } \{\tfrac1n : n \text{ a positive integer}\}\\      x &\text{otherwise} \end{cases} $$ But that kind of seems like 'cheating'. Is there a better example? EDIT: Would it be better to have: $$ f(x) = \begin{cases} 1 &\text{if } x \text{ is in } \{\tfrac1n : n \text{ a positive integer}\}\cup \{0\}\\      0 &\text{otherwise} \end{cases} $$,I have this question: Find a function $f :\mathbb R \to\mathbb R$ which is discontinuous at the points of the   set $\{\frac1n : n \text{ a positive integer}\} \cup \{0\}$ but is continuous everywhere   else. I really don't know what to do. I was thinking maybe: $$ f(x) = \begin{cases} 1 \quad&\text{if }x=0  \\ 0 &\text{if } x \text{ is in } \{\tfrac1n : n \text{ a positive integer}\}\\      x &\text{otherwise} \end{cases} $$ But that kind of seems like 'cheating'. Is there a better example? EDIT: Would it be better to have: $$ f(x) = \begin{cases} 1 &\text{if } x \text{ is in } \{\tfrac1n : n \text{ a positive integer}\}\cup \{0\}\\      0 &\text{otherwise} \end{cases} $$,,"['analysis', 'functions', 'continuity']"
17,Why are diffeomorphism-invariant PDE not elliptic?,Why are diffeomorphism-invariant PDE not elliptic?,,"In reading geometric analysis papers, I frequently encounter a statement of the form, ""The PDE in question is diffeomorphism-invariant, and therefore cannot be elliptic."" My vague understanding is that this might have to do with the space of solutions being infinite-dimensional, or maybe it has to do with elliptic regularity failing.  Either way, I would like more precise clarification.","In reading geometric analysis papers, I frequently encounter a statement of the form, ""The PDE in question is diffeomorphism-invariant, and therefore cannot be elliptic."" My vague understanding is that this might have to do with the space of solutions being infinite-dimensional, or maybe it has to do with elliptic regularity failing.  Either way, I would like more precise clarification.",,"['analysis', 'partial-differential-equations', 'riemannian-geometry']"
18,Estimates on Derivatives,Estimates on Derivatives,,"I have trouble in filling in the details of the proof on Estimates on derivates, from page 29 of PDE Evans, 2nd edition. Namely, I am lost at some steps. The book gives: Theorem 7 (Estimates on derivatives).  Assume u is harmonic in U. Then   \begin{align} |D^\alpha u(x_0)| \le \frac{C_k}{r^{n+k}} \|u\|_{L^1(B(x_0,r))} \tag{18} \end{align}   for each ball $B(x_0,r) \subseteq U$ and each multiindex $\alpha$ of order $|\alpha| = k$. Here    \begin{align} C_0 = \frac{1}{\alpha(n)}, C_k = \frac{(2^{n+1}nk)^k}{\alpha (n)} \text{ for } k=1,2,\ldots \tag{19} \end{align} Proof. 1. We establish $(\text{18}), (\text{19})$ by induction on $k$, with the case $k=0$ being immediate from the mean value formula $u(x) = \frac{1}{\alpha(n)r^n} \int_{B(x_0,r)} u \, dx = \frac{1}{\alpha(n)r^{n-1}} \int_{\partial B(x,r)} u \, dS$ (which denote average values of $u$ over the ball and sphere, respectively). For $k = 1$, we note upon differentiating Laplace's equation that $u_{x_i}$ (for $i=1,...n$) is harmonic. Consequently, \begin{align} \left|u_{x_i}(x_0)\right| &= \left|\frac{1}{\alpha(n) (\frac{r}{2})^n} \int_{B(x_0,\frac{r}{2})} u_{x_i} dx\right| \tag{20} \\ &= \left|\frac{2^n}{\alpha(n) r^n} \int_{B(x_0,\frac{r}{2})} u_{x_i} dx\right| \\ &= \left|\frac{2^n}{\alpha(n) r^n} \int_{\partial B(x_0,\frac{r}{2})} u \nu_i dS\right| \\ &\le \frac{2n}{r} \|u\|_{L^\infty(\partial B(x_0,\frac{r}{2})} \end{align} Now if $x \in \partial B(x_0,\frac{r}{2})$, then $B(x,\frac{r}{2}) \subseteq B(x_0,r) \subseteq U$, and so   \begin{align} |u(x)| \le \frac{1}{\alpha(n)} \left(\frac{2}{r}\right)^n \|u\|_{L^1(B(x_0,r))} \end{align}   by (18), (19) for $k=0$. Combining the inequalities above, we get   \begin{align}|D^\alpha u(x_0)| &\le \frac{2^{n+1}n}{\alpha(n)} \frac{1}{r^{n+1}} \|u\|_{L^1(B(x_0,r))} \\ &= \frac{2^{n+1}n}{r^{n+1}} \|u\|_{L^1(B(x_0,r))} \\ \end{align}   if $|\alpha| = 1$. This verifies $(\text{18})$ and $(\text{19})$ for $k = 1$. (There is a second part of this proof for $k \ge 2$, but I think I can understand that on my own, once I fully understand $k=1$.) I had major trouble understanding all the steps that involve inequalities (with the $\le$), though I completely understand the equality steps (=). May I seek help in understanding how these inequalities are obtained and how they are true? (I think Evans likes to skip a lot of steps...) $\displaystyle \left|\frac{2^n}{\alpha(n) r^n} \int_{\partial B(x_0,\frac{r}{2})} u \nu_i dS \right| \le \frac{2n}{r} \|u\|_{L^\infty(\partial B(x_0,\frac{r}{2})}$ $\displaystyle |u(x)| \le \frac{1}{\alpha(n)} \left(\frac{2}{r}\right)^n \|u\|_{L^1(B(x_0,r))}$ $\displaystyle |D^\alpha u(x_0)| \le \frac{2^{n+1}n}{\alpha(n)} \frac{1}{r^{n+1}} \|u\|_{L^1(B(x_0,r))}$ I would love to show my work attempt in filling in the details for these relations, but they are a mess and probably won't be helpful here. But hopefully what's given above will suffice. (As far as only equalities are concerned, I tried to add a little more detail to the steps in the above proof than what is originally presented by the textbook.)","I have trouble in filling in the details of the proof on Estimates on derivates, from page 29 of PDE Evans, 2nd edition. Namely, I am lost at some steps. The book gives: Theorem 7 (Estimates on derivatives).  Assume u is harmonic in U. Then   \begin{align} |D^\alpha u(x_0)| \le \frac{C_k}{r^{n+k}} \|u\|_{L^1(B(x_0,r))} \tag{18} \end{align}   for each ball $B(x_0,r) \subseteq U$ and each multiindex $\alpha$ of order $|\alpha| = k$. Here    \begin{align} C_0 = \frac{1}{\alpha(n)}, C_k = \frac{(2^{n+1}nk)^k}{\alpha (n)} \text{ for } k=1,2,\ldots \tag{19} \end{align} Proof. 1. We establish $(\text{18}), (\text{19})$ by induction on $k$, with the case $k=0$ being immediate from the mean value formula $u(x) = \frac{1}{\alpha(n)r^n} \int_{B(x_0,r)} u \, dx = \frac{1}{\alpha(n)r^{n-1}} \int_{\partial B(x,r)} u \, dS$ (which denote average values of $u$ over the ball and sphere, respectively). For $k = 1$, we note upon differentiating Laplace's equation that $u_{x_i}$ (for $i=1,...n$) is harmonic. Consequently, \begin{align} \left|u_{x_i}(x_0)\right| &= \left|\frac{1}{\alpha(n) (\frac{r}{2})^n} \int_{B(x_0,\frac{r}{2})} u_{x_i} dx\right| \tag{20} \\ &= \left|\frac{2^n}{\alpha(n) r^n} \int_{B(x_0,\frac{r}{2})} u_{x_i} dx\right| \\ &= \left|\frac{2^n}{\alpha(n) r^n} \int_{\partial B(x_0,\frac{r}{2})} u \nu_i dS\right| \\ &\le \frac{2n}{r} \|u\|_{L^\infty(\partial B(x_0,\frac{r}{2})} \end{align} Now if $x \in \partial B(x_0,\frac{r}{2})$, then $B(x,\frac{r}{2}) \subseteq B(x_0,r) \subseteq U$, and so   \begin{align} |u(x)| \le \frac{1}{\alpha(n)} \left(\frac{2}{r}\right)^n \|u\|_{L^1(B(x_0,r))} \end{align}   by (18), (19) for $k=0$. Combining the inequalities above, we get   \begin{align}|D^\alpha u(x_0)| &\le \frac{2^{n+1}n}{\alpha(n)} \frac{1}{r^{n+1}} \|u\|_{L^1(B(x_0,r))} \\ &= \frac{2^{n+1}n}{r^{n+1}} \|u\|_{L^1(B(x_0,r))} \\ \end{align}   if $|\alpha| = 1$. This verifies $(\text{18})$ and $(\text{19})$ for $k = 1$. (There is a second part of this proof for $k \ge 2$, but I think I can understand that on my own, once I fully understand $k=1$.) I had major trouble understanding all the steps that involve inequalities (with the $\le$), though I completely understand the equality steps (=). May I seek help in understanding how these inequalities are obtained and how they are true? (I think Evans likes to skip a lot of steps...) $\displaystyle \left|\frac{2^n}{\alpha(n) r^n} \int_{\partial B(x_0,\frac{r}{2})} u \nu_i dS \right| \le \frac{2n}{r} \|u\|_{L^\infty(\partial B(x_0,\frac{r}{2})}$ $\displaystyle |u(x)| \le \frac{1}{\alpha(n)} \left(\frac{2}{r}\right)^n \|u\|_{L^1(B(x_0,r))}$ $\displaystyle |D^\alpha u(x_0)| \le \frac{2^{n+1}n}{\alpha(n)} \frac{1}{r^{n+1}} \|u\|_{L^1(B(x_0,r))}$ I would love to show my work attempt in filling in the details for these relations, but they are a mess and probably won't be helpful here. But hopefully what's given above will suffice. (As far as only equalities are concerned, I tried to add a little more detail to the steps in the above proof than what is originally presented by the textbook.)",,"['analysis', 'partial-differential-equations', 'harmonic-functions']"
19,The Helmholtz equation: How prove this $T\psi{(x)}\in\Omega$.,The Helmholtz equation: How prove this .,T\psi{(x)}\in\Omega,"Let $\Omega\subset  R^2$  be a simply connected bounded domain with inﬁnitely diﬀerentiable boundary  $\partial\Omega$and unit normal vector $v$ directed into the exterior of $\Omega$ $$\Phi{(x,y)}=\dfrac{i}{4}H^{(1)}_{0}(k|x-y|),x\neq y$$ we denote the fundamental solution to the two-dimensional Helmholtz equation in terms of the ﬁrst kind Hankel function of order zero where the  Helmholtz equation $$\Delta u+k^2u=0, \mbox{in}   R^2\overline{\Omega}$$ and  $$(T\psi)(x):=\dfrac{\partial}{\partial v(x)}\int_{\partial\Omega}\dfrac{\partial\Phi{(x,y)}}{\partial v(y)}\psi{(y)}ds(y),x\in\partial\Omega.$$ show that:   $$(T\psi)(x)=\dfrac{\partial}{\partial s(x)}\int_{\partial\Omega}\Phi{(x,y)}\dfrac{\partial \psi}{\partial s}(y)ds(y)+k^2v(x)\cdot\int_{\partial \Omega}\Phi{(x,y)}v(y)\psi{(y)}ds(y),x\in\partial\Omega $$ This relusut is from this paper: http://num.math.uni-goettingen.de/kress/kress2013.pdf The author say can in [12], http://link.springer.com/article/10.1007%2FBF02941090#page-1 Now I find this paper,because I don't know french,so  I'm not sure  this is the proof, if someone understand this proof, can you  explain it  to me.  thank you very much. Thank you This follow  picture is from [12]","Let $\Omega\subset  R^2$  be a simply connected bounded domain with inﬁnitely diﬀerentiable boundary  $\partial\Omega$and unit normal vector $v$ directed into the exterior of $\Omega$ $$\Phi{(x,y)}=\dfrac{i}{4}H^{(1)}_{0}(k|x-y|),x\neq y$$ we denote the fundamental solution to the two-dimensional Helmholtz equation in terms of the ﬁrst kind Hankel function of order zero where the  Helmholtz equation $$\Delta u+k^2u=0, \mbox{in}   R^2\overline{\Omega}$$ and  $$(T\psi)(x):=\dfrac{\partial}{\partial v(x)}\int_{\partial\Omega}\dfrac{\partial\Phi{(x,y)}}{\partial v(y)}\psi{(y)}ds(y),x\in\partial\Omega.$$ show that:   $$(T\psi)(x)=\dfrac{\partial}{\partial s(x)}\int_{\partial\Omega}\Phi{(x,y)}\dfrac{\partial \psi}{\partial s}(y)ds(y)+k^2v(x)\cdot\int_{\partial \Omega}\Phi{(x,y)}v(y)\psi{(y)}ds(y),x\in\partial\Omega $$ This relusut is from this paper: http://num.math.uni-goettingen.de/kress/kress2013.pdf The author say can in [12], http://link.springer.com/article/10.1007%2FBF02941090#page-1 Now I find this paper,because I don't know french,so  I'm not sure  this is the proof, if someone understand this proof, can you  explain it  to me.  thank you very much. Thank you This follow  picture is from [12]",,"['analysis', 'partial-differential-equations']"
20,Outer measure of outer measure,Outer measure of outer measure,,"I have a homework question, and I'm totally confused... I would appreciate every help or a clue. Let $S$ be an algebra of subsets of a set $X$, and let $\mu$ be a pre-measure on $S$. We can construct an outer measure $\mu^*$, as shown here: $$\mu^* \left({E}\right) = \inf \ \left\{{\sum_{n=0}^\infty \mu\left({A_n}\right) : A_n \in S, \ E \subseteq \bigcup_{n=0}^\infty A_n}\right\}.$$ Now, we can look at the $\sigma$-algebra of all $\mu^*$-measurable sets, let's denote it $M$. The outer measure $\mu^*$ is then a measure on $M$. Now, we can take this measure on $\mu^*|_M$, and again construct an outer measure: $$\mu^{**} \left({E}\right) = \inf \ \left\{{\sum_{n=0}^\infty \mu^*\left({A_n}\right) : A_n \in M, \ E \subseteq \bigcup_{n=0}^\infty A_n}\right\}.$$ I have to prove that $\mu^*(E)=\mu^{**}(E)$ for every $E\subseteq X$.","I have a homework question, and I'm totally confused... I would appreciate every help or a clue. Let $S$ be an algebra of subsets of a set $X$, and let $\mu$ be a pre-measure on $S$. We can construct an outer measure $\mu^*$, as shown here: $$\mu^* \left({E}\right) = \inf \ \left\{{\sum_{n=0}^\infty \mu\left({A_n}\right) : A_n \in S, \ E \subseteq \bigcup_{n=0}^\infty A_n}\right\}.$$ Now, we can look at the $\sigma$-algebra of all $\mu^*$-measurable sets, let's denote it $M$. The outer measure $\mu^*$ is then a measure on $M$. Now, we can take this measure on $\mu^*|_M$, and again construct an outer measure: $$\mu^{**} \left({E}\right) = \inf \ \left\{{\sum_{n=0}^\infty \mu^*\left({A_n}\right) : A_n \in M, \ E \subseteq \bigcup_{n=0}^\infty A_n}\right\}.$$ I have to prove that $\mu^*(E)=\mu^{**}(E)$ for every $E\subseteq X$.",,"['analysis', 'measure-theory']"
21,If Integral is zero then function is zero almost everywhere,If Integral is zero then function is zero almost everywhere,,"I've been trying to prove the following statement: Suppose a measure space $(X, M, \mu)$. If $\int_E f \: d\mu = 0$ for every measurable set $E \in M$, then $f = 0$ almost everywhere in $X$. I've definitively proved it for the case when $\mu$ is a positive measure, but the complex case completely eludes me. I've already proven  that if $f = u + iv$, where $u$ and $v$ are real functions, then $u = 0$ almost everywhere and $v = 0$ almost everywhere. To me it seems really clear then that $f = 0$ almost everywhere, but the formal definitions we are working with is the following: A property P holds almost everywhere in a set E if there is a measurable set N such that P holds in E \ N, and the measure of N is zero. I therefore know that there are sets $A_u$ and $A_v$ with measure zero such that $u = 0$ in $X \setminus A_u$,  and analogous for $A_v$. What I don't know yet is if $A_u \cup A_v $ has measure zero. Or maybe I'm going through this the wrong way.","I've been trying to prove the following statement: Suppose a measure space $(X, M, \mu)$. If $\int_E f \: d\mu = 0$ for every measurable set $E \in M$, then $f = 0$ almost everywhere in $X$. I've definitively proved it for the case when $\mu$ is a positive measure, but the complex case completely eludes me. I've already proven  that if $f = u + iv$, where $u$ and $v$ are real functions, then $u = 0$ almost everywhere and $v = 0$ almost everywhere. To me it seems really clear then that $f = 0$ almost everywhere, but the formal definitions we are working with is the following: A property P holds almost everywhere in a set E if there is a measurable set N such that P holds in E \ N, and the measure of N is zero. I therefore know that there are sets $A_u$ and $A_v$ with measure zero such that $u = 0$ in $X \setminus A_u$,  and analogous for $A_v$. What I don't know yet is if $A_u \cup A_v $ has measure zero. Or maybe I'm going through this the wrong way.",,"['analysis', 'measure-theory', 'functions', 'lebesgue-integral']"
22,Higher Order Derivative Proof .,Higher Order Derivative Proof .,,"I would appreciate if someone could check over my proof for this question and advise me if it is correct. My attempt so far; Now as $f$ is k times differentiable , it taylor series about $x_{0}$ can be written as follows, $$f(x)=f(x_{0})+f'(x_{0})(x-x_{0})+f''(x_{0})\frac{(x-x_{0})^2}{2!}+...+f^{(k)}\frac{(x-x_{0})^k}{n!}$$ We are given that; $$f'(x_0)=...=f^{(k-1)}(x_0)=0 $$ and $$f^{(k)} \neq 0$$ Sow we can conclude that we end up with something looking like this... $$f(x)=f(x_{0})+f^{(k)}\frac{(x-x_{0})^k}{n!}$$. And I also know the fact in the neighbourhood of $x_0$ 1) if $x_0$ is a local minimum, then in a neighbourhood of $x_0$, $f(x)-f(x_0)>0,$ 2) if $x_0$ is a local maximum, then in a neighbourhood of $x_0$, $f(x)-f(x_0)<0.$ And from what we have : $$f(x)-f(x_{0})=f^{(k)}\frac{(x-x_{0})^k}{n!}$$. Now I just have to put this together; CASE 1: $k$ is Even Subcase 1 :   If  $f^{(k)}<0$. Then looking at the interval $x \in (x_0 - \delta,x_0 + \delta)$ We have $x<x_0$  implies that $(x-x_0)^{k}>0$ as $k$ is even and so $$f(x)-f(x_{0})=f^{(k)}\frac{(x-x_{0})^k}{n!}<0$$ Implying $f(x)$ is increasing on $(x,x_0)$. $x>x_0$  implies that $(x-x_0)^{k}>0$ and so $$f(x)-f(x_{0})=f^{(k)}\frac{(x-x_{0})^k}{n!}<0$$ Implying $f(x)$ is decreasing on $(x_0,x)$. So this implies a maximum, as in the neighbourhood of $x_0$, $f(x)-f(x_0)<0.$. Subcase 2: If  $f^{(k)}>0$. Then looking at the interval $x \in (x_0 - \delta,x_0 + \delta)$ We have   ; $x<x_0$  implies that $(x-x_0)^{k}>0$ as $k$ is even and so $$f(x)-f(x_{0})=f^{(k)}\frac{(x-x_{0})^k}{n!}>0$$ Implying $f(x)$ is decreasing on $(x,x_0)$. $x>x_0$  implies that $(x-x_0)^{k}>0$ and so $$f(x)-f(x_{0})=f^{(k)}\frac{(x-x_{0})^k}{n!}>0$$ Implying $f(x)$ is Increasing on $(x_0,x)$. So this implies a minimum, as in the neighbourhood of $x_0$, $f(x)-f(x_0)>0.$. And so this proves $(i)$; CASE 2: $k$ is odd; Subcase 1 :   If  $f^{(k)}<0$. Then looking at the interval $x \in (x_0 - \delta,x_0 + \delta)$ We have $x<x_0$  implies that $(x-x_0)^{k}<0$ as $k$ is odd and so $$f(x)-f(x_{0})=f^{(k)}\frac{(x-x_{0})^k}{n!}>0$$ Implying $f(x)$ is decreasing on $(x,x_0)$. $x>x_0$  implies that $(x-x_0)^{k}>0$ and so $$f(x)-f(x_{0})=f^{(k)}\frac{(x-x_{0})^k}{n!}<0$$ Implying $f(x)$ is decreasing on $(x_0,x)$. So $f$ is strictly decreasing on neighbourhood $x_0$. Subcase 2: If  $f^{(k)}>0$. Then looking at the interval $x \in (x_0 - \delta,x_0 + \delta)$ We have   ; $x<x_0$  implies that $(x-x_0)^{k}<0$ as $k$ is even and so $$f(x)-f(x_{0})=f^{(k)}\frac{(x-x_{0})^k}{n!}<0$$ Implying $f(x)$ increasing on $(x,x_0)$. $x>x_0$  implies that $(x-x_0)^{k}>0$ and so $$f(x)-f(x_{0})=f^{(k)}\frac{(x-x_{0})^k}{n!}>0$$ Implying $f(x)$ is Increasing on $(x_0,x)$. So $f$ is strictly increasing on neighbourhood $x_0$. So this Proves part $(ii)$.      $\blacksquare$ EDIT: I know this proof is tedious and long but is it correct... I also realise induction would have been much quicker but felt that this strengthens my understanding better.","I would appreciate if someone could check over my proof for this question and advise me if it is correct. My attempt so far; Now as $f$ is k times differentiable , it taylor series about $x_{0}$ can be written as follows, $$f(x)=f(x_{0})+f'(x_{0})(x-x_{0})+f''(x_{0})\frac{(x-x_{0})^2}{2!}+...+f^{(k)}\frac{(x-x_{0})^k}{n!}$$ We are given that; $$f'(x_0)=...=f^{(k-1)}(x_0)=0 $$ and $$f^{(k)} \neq 0$$ Sow we can conclude that we end up with something looking like this... $$f(x)=f(x_{0})+f^{(k)}\frac{(x-x_{0})^k}{n!}$$. And I also know the fact in the neighbourhood of $x_0$ 1) if $x_0$ is a local minimum, then in a neighbourhood of $x_0$, $f(x)-f(x_0)>0,$ 2) if $x_0$ is a local maximum, then in a neighbourhood of $x_0$, $f(x)-f(x_0)<0.$ And from what we have : $$f(x)-f(x_{0})=f^{(k)}\frac{(x-x_{0})^k}{n!}$$. Now I just have to put this together; CASE 1: $k$ is Even Subcase 1 :   If  $f^{(k)}<0$. Then looking at the interval $x \in (x_0 - \delta,x_0 + \delta)$ We have $x<x_0$  implies that $(x-x_0)^{k}>0$ as $k$ is even and so $$f(x)-f(x_{0})=f^{(k)}\frac{(x-x_{0})^k}{n!}<0$$ Implying $f(x)$ is increasing on $(x,x_0)$. $x>x_0$  implies that $(x-x_0)^{k}>0$ and so $$f(x)-f(x_{0})=f^{(k)}\frac{(x-x_{0})^k}{n!}<0$$ Implying $f(x)$ is decreasing on $(x_0,x)$. So this implies a maximum, as in the neighbourhood of $x_0$, $f(x)-f(x_0)<0.$. Subcase 2: If  $f^{(k)}>0$. Then looking at the interval $x \in (x_0 - \delta,x_0 + \delta)$ We have   ; $x<x_0$  implies that $(x-x_0)^{k}>0$ as $k$ is even and so $$f(x)-f(x_{0})=f^{(k)}\frac{(x-x_{0})^k}{n!}>0$$ Implying $f(x)$ is decreasing on $(x,x_0)$. $x>x_0$  implies that $(x-x_0)^{k}>0$ and so $$f(x)-f(x_{0})=f^{(k)}\frac{(x-x_{0})^k}{n!}>0$$ Implying $f(x)$ is Increasing on $(x_0,x)$. So this implies a minimum, as in the neighbourhood of $x_0$, $f(x)-f(x_0)>0.$. And so this proves $(i)$; CASE 2: $k$ is odd; Subcase 1 :   If  $f^{(k)}<0$. Then looking at the interval $x \in (x_0 - \delta,x_0 + \delta)$ We have $x<x_0$  implies that $(x-x_0)^{k}<0$ as $k$ is odd and so $$f(x)-f(x_{0})=f^{(k)}\frac{(x-x_{0})^k}{n!}>0$$ Implying $f(x)$ is decreasing on $(x,x_0)$. $x>x_0$  implies that $(x-x_0)^{k}>0$ and so $$f(x)-f(x_{0})=f^{(k)}\frac{(x-x_{0})^k}{n!}<0$$ Implying $f(x)$ is decreasing on $(x_0,x)$. So $f$ is strictly decreasing on neighbourhood $x_0$. Subcase 2: If  $f^{(k)}>0$. Then looking at the interval $x \in (x_0 - \delta,x_0 + \delta)$ We have   ; $x<x_0$  implies that $(x-x_0)^{k}<0$ as $k$ is even and so $$f(x)-f(x_{0})=f^{(k)}\frac{(x-x_{0})^k}{n!}<0$$ Implying $f(x)$ increasing on $(x,x_0)$. $x>x_0$  implies that $(x-x_0)^{k}>0$ and so $$f(x)-f(x_{0})=f^{(k)}\frac{(x-x_{0})^k}{n!}>0$$ Implying $f(x)$ is Increasing on $(x_0,x)$. So $f$ is strictly increasing on neighbourhood $x_0$. So this Proves part $(ii)$.      $\blacksquare$ EDIT: I know this proof is tedious and long but is it correct... I also realise induction would have been much quicker but felt that this strengthens my understanding better.",,"['analysis', 'derivatives', 'proof-verification', 'taylor-expansion']"
23,Asymptotic formula for twice iterated factorial $(n!)!$,Asymptotic formula for twice iterated factorial,(n!)!,"Being familiar with Stirling's formula for factorial: $$n!\sim\sqrt{2\pi n}\left(\frac n e\right)^n,\quad\color{gray}{n\to\infty}$$ I naïvely assumed that for twice iterated factorial we can simply substitute the right-hand side into itself and write $$(n!)!\,\stackrel{\color{red}{\small\text{wrong}}}\sim\,\sqrt{2\pi \sqrt{2\pi n}\left(\frac n e\right)^n}\left(\frac{\sqrt{2\pi n}\left(\frac n e\right)^n}e\right)^{\sqrt{2\pi n}\left(\frac n e\right)^n},$$ but, as it turns out, I was wrong. Actually, it grows faster than that: $$(n!)!\,\succ\,\sqrt{2\pi \sqrt{2\pi n}\left(\frac n e\right)^n}\left(\frac{\sqrt{2\pi n}\left(\frac n e\right)^n}e\right)^{\sqrt{2\pi n}\left(\frac n e\right)^n}.$$ Can we express the correct asymptotic growth rate of twice iterated factorial $(n!)!$ using only elementary functions and, possibly, also their inverses, such as the Lambert W-function ? Update: Apparently, the same issue arises for simpler functions like $2^{n!}$ . If we simply substitute Stirling's formula for the exponent $n!$ , we will get an incorrect asymptotic.","Being familiar with Stirling's formula for factorial: I naïvely assumed that for twice iterated factorial we can simply substitute the right-hand side into itself and write but, as it turns out, I was wrong. Actually, it grows faster than that: Can we express the correct asymptotic growth rate of twice iterated factorial using only elementary functions and, possibly, also their inverses, such as the Lambert W-function ? Update: Apparently, the same issue arises for simpler functions like . If we simply substitute Stirling's formula for the exponent , we will get an incorrect asymptotic.","n!\sim\sqrt{2\pi n}\left(\frac n e\right)^n,\quad\color{gray}{n\to\infty} (n!)!\,\stackrel{\color{red}{\small\text{wrong}}}\sim\,\sqrt{2\pi \sqrt{2\pi n}\left(\frac n e\right)^n}\left(\frac{\sqrt{2\pi n}\left(\frac n e\right)^n}e\right)^{\sqrt{2\pi n}\left(\frac n e\right)^n}, (n!)!\,\succ\,\sqrt{2\pi \sqrt{2\pi n}\left(\frac n e\right)^n}\left(\frac{\sqrt{2\pi n}\left(\frac n e\right)^n}e\right)^{\sqrt{2\pi n}\left(\frac n e\right)^n}. (n!)! 2^{n!} n!","['number-theory', 'analysis', 'asymptotics', 'factorial', 'lambert-w']"
24,An inequality about quasi-linear function,An inequality about quasi-linear function,,"Let $\gamma$ be a positive, nondecreasing, continuous, function defined on $[0,\infty]$ . Suppose that $\gamma(x+y)\le C(\gamma(x)+\gamma(y))$ . In addition, suppose $$ \int_{2}^{\infty}\frac{dr}{\gamma(r)}=\infty.$$ suppose further that $$ f(t)\le f(0)+\int_0^t\gamma(f(r))dr+\gamma \Big(\int_0^tf(r)dr\Big). $$ How to show that $$ \gamma(\int_0^t f(r)dr)\le C(t+1)\int_0^t \gamma(f(r))dr $$ Actually, this question come from lemma 1.2 in this paper . My effort: Following the hint in this paper, I want to show that $\gamma$ cannot grow faster than $t^2$ . However, I can't rule out the possibility that $$\limsup_{t\to +\infty}\frac{\gamma(t)}{t^2}=\infty.$$ I have no idea how to proceed it. Thanks for any help. Update: If $C\le 1$ , I can give a proof. \begin{align*} \gamma(\int_0^t f(r)dr)&=\gamma(\sum_{k=1}^n\int_{\frac{(k-1)t}{n}}^{\frac{kt}{n}}f(r)dr)\\ &\le \sum_{k=1}^{n}C^k\gamma(f(\xi_k))\frac{t}{n},\xi_k\in (\frac{k-1}{n}t,\frac{k}{n}t) \end{align*} If $C\le 1$ , we can send $n$ to $\infty$ and get the proof. (Sorry. I find that the last inequality above this line is false. Because $\gamma(ax+by)\not \le a\gamma(x)+b\gamma(y)$ ) In addition, we can get a upper bound of $\gamma$ . In fact, $$\gamma(2^n)\le 2C\gamma(2^{n-1})\le (2C)^n\gamma(1).$$ Using $\gamma$ is increasing, we have that $$ \gamma(t)\le Ct^{1+\frac{\ln C}{\ln 2}} $$","Let be a positive, nondecreasing, continuous, function defined on . Suppose that . In addition, suppose suppose further that How to show that Actually, this question come from lemma 1.2 in this paper . My effort: Following the hint in this paper, I want to show that cannot grow faster than . However, I can't rule out the possibility that I have no idea how to proceed it. Thanks for any help. Update: If , I can give a proof. If , we can send to and get the proof. (Sorry. I find that the last inequality above this line is false. Because ) In addition, we can get a upper bound of . In fact, Using is increasing, we have that","\gamma [0,\infty] \gamma(x+y)\le C(\gamma(x)+\gamma(y))  \int_{2}^{\infty}\frac{dr}{\gamma(r)}=\infty. 
f(t)\le f(0)+\int_0^t\gamma(f(r))dr+\gamma \Big(\int_0^tf(r)dr\Big).
 
\gamma(\int_0^t f(r)dr)\le C(t+1)\int_0^t \gamma(f(r))dr
 \gamma t^2 \limsup_{t\to +\infty}\frac{\gamma(t)}{t^2}=\infty. C\le 1 \begin{align*}
\gamma(\int_0^t f(r)dr)&=\gamma(\sum_{k=1}^n\int_{\frac{(k-1)t}{n}}^{\frac{kt}{n}}f(r)dr)\\
&\le \sum_{k=1}^{n}C^k\gamma(f(\xi_k))\frac{t}{n},\xi_k\in (\frac{k-1}{n}t,\frac{k}{n}t)
\end{align*} C\le 1 n \infty \gamma(ax+by)\not \le a\gamma(x)+b\gamma(y) \gamma \gamma(2^n)\le 2C\gamma(2^{n-1})\le (2C)^n\gamma(1). \gamma 
\gamma(t)\le Ct^{1+\frac{\ln C}{\ln 2}}
","['analysis', 'inequality', 'partial-differential-equations']"
25,A problem about periodic functions,A problem about periodic functions,,"Suppose that $f(x):\mathbb{R}\rightarrow \mathbb{R}$ is a periodic function with a minimal positive period $T$. Can $g(x)=f(x^2)$ be periodic? I know it is impossible if the condition $\forall x \in (0,T),f(x)\not=f(0)$ is added. But Is there a counterexample for the general case? Thanks for any help in advance.","Suppose that $f(x):\mathbb{R}\rightarrow \mathbb{R}$ is a periodic function with a minimal positive period $T$. Can $g(x)=f(x^2)$ be periodic? I know it is impossible if the condition $\forall x \in (0,T),f(x)\not=f(0)$ is added. But Is there a counterexample for the general case? Thanks for any help in advance.",,"['analysis', 'functions', 'periodic-functions']"
26,Integral over set of measure zero.,Integral over set of measure zero.,,"Let $f:(X,\eta,\mu)\to [0,\infty]$ measurable and $E\in\eta$ with $\mu(E)=0$ ( $\eta$ is a $\sigma$ -algebra, $\mu$ a positive measure). Then $\displaystyle\int_E f\,d\mu=0$ . We have $$\int_E f\,d\mu=\sup\left\{\int_E s\,d\mu\ : \ s \text{ is a measurable simple function }, s\leq f \right\} \tag{*}.$$ But if $s$ is simple (and measurable) then $s=\displaystyle\sum_{i=1}^n \alpha_i\chi_{E_i}$ , with $E_i\in\eta$ . Now $$\int_E s\,d\mu=\sum_{i=1}^n \alpha_i\mu(E\cap E_i)=0,$$ because $E\cap E_i \subseteq E \Rightarrow \mu(E\cap E_i)\leq \mu(E)=0$ . So that, for all simple function $s\leq f$ , we have $\displaystyle\int_E s\,d\mu=0$ , therefore $\displaystyle\int_E f\,d\mu=0$ Is this correct? Thank you all.","Let measurable and with ( is a -algebra, a positive measure). Then . We have But if is simple (and measurable) then , with . Now because . So that, for all simple function , we have , therefore Is this correct? Thank you all.","f:(X,\eta,\mu)\to [0,\infty] E\in\eta \mu(E)=0 \eta \sigma \mu \displaystyle\int_E f\,d\mu=0 \int_E f\,d\mu=\sup\left\{\int_E s\,d\mu\ : \ s \text{ is a measurable simple function }, s\leq f \right\} \tag{*}. s s=\displaystyle\sum_{i=1}^n \alpha_i\chi_{E_i} E_i\in\eta \int_E s\,d\mu=\sum_{i=1}^n \alpha_i\mu(E\cap E_i)=0, E\cap E_i \subseteq E \Rightarrow \mu(E\cap E_i)\leq \mu(E)=0 s\leq f \displaystyle\int_E s\,d\mu=0 \displaystyle\int_E f\,d\mu=0","['analysis', 'measure-theory']"
27,How can we prove a simple case of the High Indices Theorem?,How can we prove a simple case of the High Indices Theorem?,,"Let $\{a_n\}$ be a sequence of real numbers such that $$f(x) = \sum_{n=1}^{\infty} a_n x^{2^n}$$ converges for $|x| < 1$ and $f(x)$ converges to $a$ as $x \to 1^{-}$ . Then I have to prove that $\sum a_n$ exists and is equal to $a$ . (This is a particular instance of so called High Indices Theorem, which was originally proved by Hardy and Littlewood.) Here, we are given a hint that, assuming $\sum a_n = a \ (A)$ , we have $\sum a_n = a$ if and only if $$\lim_{n \to \infty} \frac{1}{n} \sum_{k=1}^{n} k a_k = 0.$$ This is not hard to prove, by comparing the difference between the partial sum $s_N$ and $p_N = \sum a_n e^{-n/N}$ . The result follows once we show that $a_n \to 0$ as $n \to \infty$ . But I failed to make any useful estimation except for a trivial bound $a_n \ll_{\varepsilon}(1+\varepsilon)^{2^n}$ obtained by considering the radius of convergence of $f$ , and this is obviously of no use. I'm a bit depressed, because my professor told me it would follow from the hint above. Can somebody give any idea or additional hint to me?","Let be a sequence of real numbers such that converges for and converges to as . Then I have to prove that exists and is equal to . (This is a particular instance of so called High Indices Theorem, which was originally proved by Hardy and Littlewood.) Here, we are given a hint that, assuming , we have if and only if This is not hard to prove, by comparing the difference between the partial sum and . The result follows once we show that as . But I failed to make any useful estimation except for a trivial bound obtained by considering the radius of convergence of , and this is obviously of no use. I'm a bit depressed, because my professor told me it would follow from the hint above. Can somebody give any idea or additional hint to me?",\{a_n\} f(x) = \sum_{n=1}^{\infty} a_n x^{2^n} |x| < 1 f(x) a x \to 1^{-} \sum a_n a \sum a_n = a \ (A) \sum a_n = a \lim_{n \to \infty} \frac{1}{n} \sum_{k=1}^{n} k a_k = 0. s_N p_N = \sum a_n e^{-n/N} a_n \to 0 n \to \infty a_n \ll_{\varepsilon}(1+\varepsilon)^{2^n} f,"['analysis', 'analytic-number-theory']"
28,set in $\mathbb{R}$ which is not a Borel-set [duplicate],set in  which is not a Borel-set [duplicate],\mathbb{R},"This question already has answers here : Closed 11 years ago . The community reviewed whether to reopen this question last year and left it closed: Original close reason(s) were not resolved Possible Duplicate: Lebesgue measurable but not Borel measurable Constructing a subset not in $\mathcal{B}(\mathbb{R})$ explicitly if i start from the topology of $\mathbb{R}$, i.e. all open sets, and then build the closure under countable union and complement i get the so called Borel-$\sigma$-Algebra, the smallest set which contains all open sets (i.e the topology) of $\mathbb{R}$ and is a $\sigma$-algebra? do you know any sets on $\mathbb{R}$ which are not cointained in this Borel-$\sigma$-algebra?","This question already has answers here : Closed 11 years ago . The community reviewed whether to reopen this question last year and left it closed: Original close reason(s) were not resolved Possible Duplicate: Lebesgue measurable but not Borel measurable Constructing a subset not in $\mathcal{B}(\mathbb{R})$ explicitly if i start from the topology of $\mathbb{R}$, i.e. all open sets, and then build the closure under countable union and complement i get the so called Borel-$\sigma$-Algebra, the smallest set which contains all open sets (i.e the topology) of $\mathbb{R}$ and is a $\sigma$-algebra? do you know any sets on $\mathbb{R}$ which are not cointained in this Borel-$\sigma$-algebra?",,"['analysis', 'measure-theory', 'descriptive-set-theory']"
29,What is the relationship between Fourier transformation and Fourier series?,What is the relationship between Fourier transformation and Fourier series?,,Is there any connection between Fourier transformation of a function and its Fourier series of the function? I only know the formula to find Fourier transformation and to find Fourier coefficients to find the corresponding Fourier series.,Is there any connection between Fourier transformation of a function and its Fourier series of the function? I only know the formula to find Fourier transformation and to find Fourier coefficients to find the corresponding Fourier series.,,"['analysis', 'fourier-analysis']"
30,Why use neighborhood to define boundary? Not open ball?,Why use neighborhood to define boundary? Not open ball?,,"One way to define a boundary point of set S is that ""every neighborhood of it contains at least one point of S and at least one point outside S"". I wonder if it's OK to replace ""neighborhood of it"" by ""open ball centered at it""? What's the difference? A further question is why introduce the concept ""neighborhood"" in the first place? Its role seems very similar to open balls.","One way to define a boundary point of set S is that ""every neighborhood of it contains at least one point of S and at least one point outside S"". I wonder if it's OK to replace ""neighborhood of it"" by ""open ball centered at it""? What's the difference? A further question is why introduce the concept ""neighborhood"" in the first place? Its role seems very similar to open balls.",,['analysis']
31,Is there a way to solve composite exponential equations precisely?,Is there a way to solve composite exponential equations precisely?,,"I have a, seemingly, trivial question. Find $x$ such that $$ l = x^x, $$ for some constant value $l \in \mathbb{R}^{>0}$ and $x \in \mathbb{R}^{>0}$. Obviously, this equation has a unique solution that can aprroximated. Neverthelss, I do not see an obvious approach to solve this equation precisly, nor can I find one on this website or using google. Maybe I am only missing the appropriate terminology to express the question. EDIT: I would also be fine with a good explanation why it is difficult or not possible. EDIT 2: As discussed in the comments, the equation, of course, has no unique solution for $l, x \in \mathbb{R}^{>0}$ as stated by me above.","I have a, seemingly, trivial question. Find $x$ such that $$ l = x^x, $$ for some constant value $l \in \mathbb{R}^{>0}$ and $x \in \mathbb{R}^{>0}$. Obviously, this equation has a unique solution that can aprroximated. Neverthelss, I do not see an obvious approach to solve this equation precisly, nor can I find one on this website or using google. Maybe I am only missing the appropriate terminology to express the question. EDIT: I would also be fine with a good explanation why it is difficult or not possible. EDIT 2: As discussed in the comments, the equation, of course, has no unique solution for $l, x \in \mathbb{R}^{>0}$ as stated by me above.",,"['analysis', 'exponentiation']"
32,Extreme Value Theorem proof help,Extreme Value Theorem proof help,,"Extreme Value Theorem : If $f$ is a continuous function on an interval [a,b], then $f$ attains its maximum and minimum values on [a,b]. Proof from my book :  Since $f$ is continuous, then $f$ has the least upper   bound, call it $M$. Assume there is no value $c \in [a,b]$ for which $f(c)=M$. Therefore, $f(x)<M$ for all $x \in [a,b]$. Define a new function $g$ by $g(x)=\frac{1}{M-f(x)}$ Observe $g(x)>0$ for every $x\in[a,b]$ and that $g$ is continuous and bounded on [a,b]. Therefore there exists $K>0$ such that $g(x)\le K$ for every $x\in [a,b]$. Since for each $x \in [a,b]$, $g(x)= \frac{1}{M-f(x)} \le K$ is equivalent to $f(x)\le M-\frac{1}{K}$, we have contradicted the fact that $M$ was assumed to be the least upper bound of $f$ on [a,b]. Hence, there must be a balue $c\in[a,b]$ such that $f(c)=M$. Q : Where does the function $g$ come from? Is there a popular alternative proof?","Extreme Value Theorem : If $f$ is a continuous function on an interval [a,b], then $f$ attains its maximum and minimum values on [a,b]. Proof from my book :  Since $f$ is continuous, then $f$ has the least upper   bound, call it $M$. Assume there is no value $c \in [a,b]$ for which $f(c)=M$. Therefore, $f(x)<M$ for all $x \in [a,b]$. Define a new function $g$ by $g(x)=\frac{1}{M-f(x)}$ Observe $g(x)>0$ for every $x\in[a,b]$ and that $g$ is continuous and bounded on [a,b]. Therefore there exists $K>0$ such that $g(x)\le K$ for every $x\in [a,b]$. Since for each $x \in [a,b]$, $g(x)= \frac{1}{M-f(x)} \le K$ is equivalent to $f(x)\le M-\frac{1}{K}$, we have contradicted the fact that $M$ was assumed to be the least upper bound of $f$ on [a,b]. Hence, there must be a balue $c\in[a,b]$ such that $f(c)=M$. Q : Where does the function $g$ come from? Is there a popular alternative proof?",,"['analysis', 'alternative-proof']"
33,"Books, Video lectures, other resources to Teach Yourself Analysis","Books, Video lectures, other resources to Teach Yourself Analysis",,"So my limited mathematics education has been especially ignorant of analysis.  In this vein, I'd like to teach myself some of the introductory basics. I'm intrigued by sources that might contain video lectures to complement readings and problems.  Self-contained packages of all of the above are especially welcome! Free is always better, but if quality comes at a price, feel free to suggest pocket lightening options... And I'm certainly not ignorant of MIT's (awesome) opencourseware!  Options are always nice though...","So my limited mathematics education has been especially ignorant of analysis.  In this vein, I'd like to teach myself some of the introductory basics. I'm intrigued by sources that might contain video lectures to complement readings and problems.  Self-contained packages of all of the above are especially welcome! Free is always better, but if quality comes at a price, feel free to suggest pocket lightening options... And I'm certainly not ignorant of MIT's (awesome) opencourseware!  Options are always nice though...",,"['analysis', 'reference-request', 'self-learning']"
34,Archimedean property of the rational numbers,Archimedean property of the rational numbers,,"Can you prove the Archimedean property of the rational numbers without constructing the reals and using the least upper bound property? It seems odd to have to take this roundabout approach, but I don't know any proof that avoids it.","Can you prove the Archimedean property of the rational numbers without constructing the reals and using the least upper bound property? It seems odd to have to take this roundabout approach, but I don't know any proof that avoids it.",,['analysis']
35,Are Legendre transforms of non-convex functions useful?,Are Legendre transforms of non-convex functions useful?,,Do Legendre transforms have any applications that do not appeal to convexity? What is the intuitive interpretation of the Legendre transform of a non-convex function?,Do Legendre transforms have any applications that do not appeal to convexity? What is the intuitive interpretation of the Legendre transform of a non-convex function?,,"['analysis', 'optimization', 'intuition', 'convex-analysis', 'transformation']"
36,PDE/Analysis graduate courses,PDE/Analysis graduate courses,,"I'm just starting my graduate studies in Analysis and PDE's and am a bit lost about what topics should I cover in order to do a good Phd program. I`ve already done the usual undergrad courses, plus Real and complex analysis (graduate level), functional analysis and measure theory. So, if you guys can recommend me which courses I should do, (I can get my university to open new courses as needed), and which books I should study, it'd make me really happy","I'm just starting my graduate studies in Analysis and PDE's and am a bit lost about what topics should I cover in order to do a good Phd program. I`ve already done the usual undergrad courses, plus Real and complex analysis (graduate level), functional analysis and measure theory. So, if you guys can recommend me which courses I should do, (I can get my university to open new courses as needed), and which books I should study, it'd make me really happy",,"['analysis', 'partial-differential-equations', 'book-recommendation']"
37,Prove that if a sequence $\{a_{n}\}$ converges then $\{\sqrt a_{n}\}$ converges to the square root of the limit.,Prove that if a sequence  converges then  converges to the square root of the limit.,\{a_{n}\} \{\sqrt a_{n}\},My attempt and the question:Can you tell if I am right :)? thank you,My attempt and the question:Can you tell if I am right :)? thank you,,"['analysis', 'proof-verification']"
38,Showing a contraction without a fixed point,Showing a contraction without a fixed point,,"Suppose $f: [1, \infty) \to [1, \infty]$ defined by $f(x) = x + \frac{1}{x}$ for all $x \geq 1$. I want to prove that: \begin{equation} |f(x)-f(y)| < |x-y| \end{equation} except when $x=y$, but $f$ does not have a fixed point. By the Banach fixed point theorem we know that if a function $f: X \to X$ is a contraction of a complete metric space, then $f$ has a unique fixed point $p$ and the sequence of $(f, f \circ f, f\circ f\circ f, ...)$ that is the sequence of $f$ composed with itself $n$ times at index $n$ converges to p for all $x$. But $[1, \infty]$ is not a complete metric space. So it seems like a good idea to proceed via contradiction? Where can I go from here All help is greatly appreciated","Suppose $f: [1, \infty) \to [1, \infty]$ defined by $f(x) = x + \frac{1}{x}$ for all $x \geq 1$. I want to prove that: \begin{equation} |f(x)-f(y)| < |x-y| \end{equation} except when $x=y$, but $f$ does not have a fixed point. By the Banach fixed point theorem we know that if a function $f: X \to X$ is a contraction of a complete metric space, then $f$ has a unique fixed point $p$ and the sequence of $(f, f \circ f, f\circ f\circ f, ...)$ that is the sequence of $f$ composed with itself $n$ times at index $n$ converges to p for all $x$. But $[1, \infty]$ is not a complete metric space. So it seems like a good idea to proceed via contradiction? Where can I go from here All help is greatly appreciated",,"['analysis', 'fixed-point-theorems']"
39,For how many functions $f$ is $f(x)^{2}=x^{2}$?,For how many functions  is ?,f f(x)^{2}=x^{2},"How many functions $f$ are there that satisfy $f(x)^{2}=x^{2}$ for all $x$? My text (Spivak's Calculus ; chapter 7 problem 7) asks this question for continuous $f$, for which the answer is, of course 4:$$f(x)=x$$ $$f(x)=-x$$ $$f(x)=\lvert x \rvert$$ $$f(x)=-\lvert x \rvert,$$ and I want to make sure I'm correct that if $f$ does not have to be continuous, there are infinitely many: any piecwise combination of them (infinitely many of which are one of the above, and infinitely many of which are not).","How many functions $f$ are there that satisfy $f(x)^{2}=x^{2}$ for all $x$? My text (Spivak's Calculus ; chapter 7 problem 7) asks this question for continuous $f$, for which the answer is, of course 4:$$f(x)=x$$ $$f(x)=-x$$ $$f(x)=\lvert x \rvert$$ $$f(x)=-\lvert x \rvert,$$ and I want to make sure I'm correct that if $f$ does not have to be continuous, there are infinitely many: any piecwise combination of them (infinitely many of which are one of the above, and infinitely many of which are not).",,"['analysis', 'functions', 'graphing-functions']"
40,Continuity of sin(1/x),Continuity of sin(1/x),,"I'm dealing with the continuity of $\sin(\frac{1}{x})$. I think that I have a proof but I'm not sure if it's right! Here is my proof: We take the functions $g(x)=\frac{1}{x}$ and $h(x)=\sin(x)$, now we see that: $g(x)$ is continuous in the open interval $(-\infty,0) \cup (0,\infty)$ because it's defined for every $x \in R$ except in $x=0$. On the other hand $h(x)$ is continuous all over reals. So it's also continuous in $(-\infty,0) \cup (0,\infty)$, then by the fact that the composition of two continuous functions is continuous  we conclude  $(hog)(x)=h(g(x))=\sin(\frac{1}{x})$ is also continuous in $(-\infty,0) \cup (0,\infty)$, so it's continuous all over reals except in $x=0$. Am I right?! I'm not interested in other proofs because I have seen them, but I want to know if this is right! Thanks!","I'm dealing with the continuity of $\sin(\frac{1}{x})$. I think that I have a proof but I'm not sure if it's right! Here is my proof: We take the functions $g(x)=\frac{1}{x}$ and $h(x)=\sin(x)$, now we see that: $g(x)$ is continuous in the open interval $(-\infty,0) \cup (0,\infty)$ because it's defined for every $x \in R$ except in $x=0$. On the other hand $h(x)$ is continuous all over reals. So it's also continuous in $(-\infty,0) \cup (0,\infty)$, then by the fact that the composition of two continuous functions is continuous  we conclude  $(hog)(x)=h(g(x))=\sin(\frac{1}{x})$ is also continuous in $(-\infty,0) \cup (0,\infty)$, so it's continuous all over reals except in $x=0$. Am I right?! I'm not interested in other proofs because I have seen them, but I want to know if this is right! Thanks!",,['analysis']
41,Find a Lipschitz constant,Find a Lipschitz constant,,how do I find a Lipschitz constant for $e^{-x^2}$? This is homework so I'd just like some tips on how to proceed. I've got $|e^{-x^2}-e^{-y^2}| = |(e^{-x^2+y^2} -1)||e^{-y^2}| < a$ where $a = -x^2 +y^2$. I'm not sure how to proceed so any help will be appreciated! Thanks! Thanks that makes sense!,how do I find a Lipschitz constant for $e^{-x^2}$? This is homework so I'd just like some tips on how to proceed. I've got $|e^{-x^2}-e^{-y^2}| = |(e^{-x^2+y^2} -1)||e^{-y^2}| < a$ where $a = -x^2 +y^2$. I'm not sure how to proceed so any help will be appreciated! Thanks! Thanks that makes sense!,,"['analysis', 'differential']"
42,Lipschitz continuity and sup of derivative norm,Lipschitz continuity and sup of derivative norm,,"On this wikipedia page , it is stated: For a differentiable Lipschitz map $f : U \rightarrow R^m$ the inequality   $\|Df\|_{\infty,U}\le K$ holds for the best Lipschitz constant of $f$, and   it turns out to be an equality if the domain $U$ is convex. I have two questions about this statement. Question 1: does a best Lipschitz constant necessarily exists? Shouldn't be $K$ the $\inf$ of all Lipschitz constants? Question 2: do you know a proof of the wikipedia statement, or a reference where this is proved?  I tried to prove it by using Taylor expansion with integral remainder (which seems to make sense with the convex assumption for equality), but it did not work... Thank you.","On this wikipedia page , it is stated: For a differentiable Lipschitz map $f : U \rightarrow R^m$ the inequality   $\|Df\|_{\infty,U}\le K$ holds for the best Lipschitz constant of $f$, and   it turns out to be an equality if the domain $U$ is convex. I have two questions about this statement. Question 1: does a best Lipschitz constant necessarily exists? Shouldn't be $K$ the $\inf$ of all Lipschitz constants? Question 2: do you know a proof of the wikipedia statement, or a reference where this is proved?  I tried to prove it by using Taylor expansion with integral remainder (which seems to make sense with the convex assumption for equality), but it did not work... Thank you.",,"['analysis', 'multivariable-calculus', 'derivatives']"
43,"Proving $x^x+y^y\ge\sqrt2$ when $x,y\in \mathbb R^+$ and $x+y=1$",Proving  when  and,"x^x+y^y\ge\sqrt2 x,y\in \mathbb R^+ x+y=1","Assuming  $x,y\in \mathbb R^+$,  $x+y=1$ how to prove $$x^x+y^y\ge\sqrt2$$thanks in advance","Assuming  $x,y\in \mathbb R^+$,  $x+y=1$ how to prove $$x^x+y^y\ge\sqrt2$$thanks in advance",,"['analysis', 'inequality']"
44,Compact support functions dense in $L_1$,Compact support functions dense in,L_1,"How do you show that the space $C_c (\mathbb{R})$ of continuous functions on $\mathbb{R}$ is dense in $L_1(\mu)$, where $\mu$ is a regular measure on the reals, without using Lusin's theorem directly? More concretly, given an integrable function $f$, I'd like to know how to explicitely construct a sequence of functions in $C_c(\mathbb{R})$ that converges to it in $L_1$, without just using simple functions at all, or their density in $L_1(\mu)$. I thought about defining $f_k:=f\chi_{E_k}$, where $E_k:=\{x\in\mathbb{R}: |f(x)|\geq \frac 1k\}$, but there's no guarantee that these would be continuous (there's no guarantee that $f$ is continuous).  Perhaps there is a way to ""smooth"" them out enough to make them continuous?","How do you show that the space $C_c (\mathbb{R})$ of continuous functions on $\mathbb{R}$ is dense in $L_1(\mu)$, where $\mu$ is a regular measure on the reals, without using Lusin's theorem directly? More concretly, given an integrable function $f$, I'd like to know how to explicitely construct a sequence of functions in $C_c(\mathbb{R})$ that converges to it in $L_1$, without just using simple functions at all, or their density in $L_1(\mu)$. I thought about defining $f_k:=f\chi_{E_k}$, where $E_k:=\{x\in\mathbb{R}: |f(x)|\geq \frac 1k\}$, but there's no guarantee that these would be continuous (there's no guarantee that $f$ is continuous).  Perhaps there is a way to ""smooth"" them out enough to make them continuous?",,"['analysis', 'measure-theory', 'convergence-divergence']"
45,Confused about proof that diameter of a closure of a set is the same as the diameter of the set.,Confused about proof that diameter of a closure of a set is the same as the diameter of the set.,,"Definition Let $E$ be a nonempty subset of a metric space $X$ , and let $S$ be the set of all real numbers of the form $d(p,q)$ , with $p,q \in E$ . The supremum of $S$ is called the diameter of $E$ . Theorem If $\bar{E}$ is the closure of the set $E$ in a metric space $X$ , then $ \text{diam}  \ \bar{E} = \text{diam} \  E.$ Proof Since $E \subset \bar{E},$ it is clear that $$ \text{diam} \  E \leq \text{diam} \ \bar{E}.$$ Fix $\epsilon > 0,$ and choose $p,q \in \bar{E}.$ By the definition of $\bar{E}$ , there are points $p', q',$ in $E$ such that $d(p,q') < \epsilon$ and $d(q,q') < \epsilon.$ Hence \begin{align*} d(p,q) &\leq d(p,p') + d(p',q') + d(q',q)\\ &< 2 \epsilon + d(p',q')\\  &\leq 2 \epsilon + \text{diam} \ E. \end{align*} It follows that $$ \text{diam} \ \bar{E} \leq 2 \epsilon +  \text{diam} \ E,$$ and since $\epsilon$ was arbitrary, (a) was proved. The step prior to the last, namely that $ \text{diam} \ \bar{E} \leq 2 \epsilon +  \text{diam} \ E$ , was lost to me. We have $d(p,q) \leq \text{diam} \ \bar{E} $ , but how do we know $ \text{diam} \ \bar{E}$ is less than or equal to the term on the right in the previous inequality?","Definition Let be a nonempty subset of a metric space , and let be the set of all real numbers of the form , with . The supremum of is called the diameter of . Theorem If is the closure of the set in a metric space , then Proof Since it is clear that Fix and choose By the definition of , there are points in such that and Hence It follows that and since was arbitrary, (a) was proved. The step prior to the last, namely that , was lost to me. We have , but how do we know is less than or equal to the term on the right in the previous inequality?","E X S d(p,q) p,q \in E S E \bar{E} E X  \text{diam}  \ \bar{E} = \text{diam} \  E. E \subset \bar{E},  \text{diam} \  E \leq \text{diam} \ \bar{E}. \epsilon > 0, p,q \in \bar{E}. \bar{E} p', q', E d(p,q') < \epsilon d(q,q') < \epsilon. \begin{align*}
d(p,q) &\leq d(p,p') + d(p',q') + d(q',q)\\
&< 2 \epsilon + d(p',q')\\ 
&\leq 2 \epsilon + \text{diam} \ E.
\end{align*}  \text{diam} \ \bar{E} \leq 2 \epsilon +  \text{diam} \ E, \epsilon  \text{diam} \ \bar{E} \leq 2 \epsilon +  \text{diam} \ E d(p,q) \leq \text{diam} \ \bar{E}   \text{diam} \ \bar{E}","['analysis', 'metric-spaces']"
46,"A Learning Roadmap to the ""foundations"" of Nonlinear Analysis (and certain specific topics)","A Learning Roadmap to the ""foundations"" of Nonlinear Analysis (and certain specific topics)",,"I'm searching for throughout references that -- in the long term -- can help me gradually gain a solid background and firm foundations to understand the main methods and theorems to deal with nonlinear problems (in particular, wave equations, solitary wave solutions (solitons) , nonlinear elliptic and hyperbolic PDEs,  periodic solutions of Lagrangian and Hamiltonian Systems , etc .) that arise in science (specifically, mathematical and theoretical physics ). The following textbooks caught my attention: Zdzislaw Denkowski, Stanislaw Migórski, and Nikolaos S. Papageorgiou, An Introduction to Nonlinear Analysis: Theory ; Antonio Ambrosetti and Giovanni Prodi, A Primer of Nonlinear Analysis ; Antonio Ambrosetti and David Arcoya, An Introduction to Nonlinear Functional Analysis  and Elliptic Problems ; Abdul-Majid Wazwaz, Partial differential equations and solitary waves theory ; Herbert Koch, Daniel Tataru, and Monica Vişan, Dispersive Equations and Nonlinear Waves ; Kung Ching Chang, Methods in Nonlinear Analysis . I would like to receive some advice from the experienced researchers in nonlinear analysis and mathematical physics of Mathematics Stack Exchange : Question: How should I go about learning nonlinear analysis? That is, assuming knowledge of real analysis, what resources and what kind of approach (and order) to read through them would you     recommend to build a solid knowledge of nonlinear analysis?","I'm searching for throughout references that -- in the long term -- can help me gradually gain a solid background and firm foundations to understand the main methods and theorems to deal with nonlinear problems (in particular, wave equations, solitary wave solutions (solitons) , nonlinear elliptic and hyperbolic PDEs,  periodic solutions of Lagrangian and Hamiltonian Systems , etc .) that arise in science (specifically, mathematical and theoretical physics ). The following textbooks caught my attention: Zdzislaw Denkowski, Stanislaw Migórski, and Nikolaos S. Papageorgiou, An Introduction to Nonlinear Analysis: Theory ; Antonio Ambrosetti and Giovanni Prodi, A Primer of Nonlinear Analysis ; Antonio Ambrosetti and David Arcoya, An Introduction to Nonlinear Functional Analysis  and Elliptic Problems ; Abdul-Majid Wazwaz, Partial differential equations and solitary waves theory ; Herbert Koch, Daniel Tataru, and Monica Vişan, Dispersive Equations and Nonlinear Waves ; Kung Ching Chang, Methods in Nonlinear Analysis . I would like to receive some advice from the experienced researchers in nonlinear analysis and mathematical physics of Mathematics Stack Exchange : Question: How should I go about learning nonlinear analysis? That is, assuming knowledge of real analysis, what resources and what kind of approach (and order) to read through them would you     recommend to build a solid knowledge of nonlinear analysis?",,"['analysis', 'reference-request', 'partial-differential-equations', 'mathematical-physics', 'nonlinear-analysis']"
47,How to prove the equations have only one real solution?,How to prove the equations have only one real solution?,,"There are $n$ equations. I need answer for the case $n=3$ . $$ \frac{1}{x_1}(1-x_1)^2+\frac{1}{x_2}(1-x_2)^2+\cdots+\frac{1}{x_n}(1-x_n)^2=0,  $$ and $$ \frac{1}{x_1}(1-x_1^2)^j+\frac{1}{x_2}(1-x_2^2)^j+\cdots+\frac{1}{x_n}(1-x_n^2)^j=0,\; 2\leq j\leq n,$$ where $x_j\geq 1$ or $x_j \leq -1$ . If $n=2,3,4$ , computer told me that the only real solution is $(1,1,\cdots,1)$ . I did not tell computer the condition $|x_j|\geq 1$ . I do not know if some inequality can be applied like chebyshev's inequality and rearrangement inequality. It seems also a little like the Vandermonde determinant. Comments: : The following three answers perfectly solve the case $n=3$ , two of them drop the condition $|x_j|\geq 1$ . The case $n=3$ is enough for me, however, for more interested you can try with larger $n\geq 4$ . I am also very curious about that.","There are equations. I need answer for the case . and where or . If , computer told me that the only real solution is . I did not tell computer the condition . I do not know if some inequality can be applied like chebyshev's inequality and rearrangement inequality. It seems also a little like the Vandermonde determinant. Comments: : The following three answers perfectly solve the case , two of them drop the condition . The case is enough for me, however, for more interested you can try with larger . I am also very curious about that.","n n=3 
\frac{1}{x_1}(1-x_1)^2+\frac{1}{x_2}(1-x_2)^2+\cdots+\frac{1}{x_n}(1-x_n)^2=0, 
 
\frac{1}{x_1}(1-x_1^2)^j+\frac{1}{x_2}(1-x_2^2)^j+\cdots+\frac{1}{x_n}(1-x_n^2)^j=0,\; 2\leq j\leq n, x_j\geq 1 x_j \leq -1 n=2,3,4 (1,1,\cdots,1) |x_j|\geq 1 n=3 |x_j|\geq 1 n=3 n\geq 4","['analysis', 'inequality', 'roots', 'rearrangement-inequality']"
48,proving that $f(x) = x^s$ is holder continuous with holder exponent s,proving that  is holder continuous with holder exponent s,f(x) = x^s,"I want to show that $$f: \mathbb{R}_{\geq0} \to \mathbb{R}_{\geq0}$$ $$x \mapsto x^s$$ is Holder continuous with Holder exponent $s \in \mathbb{R}$, where $0<s \leq 1$. So what I want to show is that $\exists \hspace{2 mm} C \in \mathbb{R}_{\geq0}$ sucht that for all $ x,y \in \mathbb{R}_{\geq0},  $ $$|x^s -y^s| \leq C|x-y|^s$$ and therefore, assuming, wlog $\hspace{1mm} x>y$ $$(x^s -y^s) \leq C(x-y)^s.$$ I thought about Bernoulli's inequality but couldn't make that work.I thought about the binomial theorem, but didn't know how to handle the fact that $s \in \mathbb{R}$.","I want to show that $$f: \mathbb{R}_{\geq0} \to \mathbb{R}_{\geq0}$$ $$x \mapsto x^s$$ is Holder continuous with Holder exponent $s \in \mathbb{R}$, where $0<s \leq 1$. So what I want to show is that $\exists \hspace{2 mm} C \in \mathbb{R}_{\geq0}$ sucht that for all $ x,y \in \mathbb{R}_{\geq0},  $ $$|x^s -y^s| \leq C|x-y|^s$$ and therefore, assuming, wlog $\hspace{1mm} x>y$ $$(x^s -y^s) \leq C(x-y)^s.$$ I thought about Bernoulli's inequality but couldn't make that work.I thought about the binomial theorem, but didn't know how to handle the fact that $s \in \mathbb{R}$.",,"['analysis', 'continuity', 'holder-spaces']"
49,"Show that the norm of the multiplication operator $M_f$ on $L^2[0,1]$ is $\|f\|_\infty$",Show that the norm of the multiplication operator  on  is,"M_f L^2[0,1] \|f\|_\infty","I'm having some (hopefully small) issues computing the norm of an operator. Firstly, the problem, For $f\in L^\infty[0,1]$, define $M_f: L^2[0,1]\to L^2[0,1]$ by $M_f(g)(x) = f(x)g(x)$. Show that $M_f$ is a bounded linear operator and $\|M_f\| = \|f\|_\infty$. What I've done so far: To see that $M_f$ is linear, let $g_1,g_2\in L^2[0,1]$ and $\lambda\in\mathbb{R}$. Then $$ M_f(g_1 + \lambda g_2)(x) = f(x)(g_1(x) + \lambda g_2(x)) = f(x)g_1(x) + \lambda f(x)g_2(x) = M_f(g_1)(x) + \lambda M_f(g_2)(x).$$ As $f\in L^\infty$ we know there is some minimal $N\in \mathbb{R}$ such that $|f(x)| \leq N$ almost everywhere (i.e., $\|f\|_\infty = N \lt \infty$). The fact that $M_f$ is bounded comes straight this assupmtion, as $$ \|M_f\| = \sup \frac{\|fg\|_2}{\|g\|_2} \leq \sup \frac{\|Ng\|_2}{\|g\|_2} = N\sup \frac{\|g\|_2}{\|g\|_2} = N < \infty.$$ almost everywhere in $[0,1]$, for all non-zero $g\in L^2[0,1]$. So $M_f$ is a bounded linear operator on $L^2[0,1]$. To prove equality, we must show there is some $g\in L^2[0,1]$ so that $\frac{\|fg\|_2}{\|g\|_2} = N$. Now I want to say something along the lines of pick $g = 1$, but this won't have $\|g\|_2 = 1$ for all measures, so this won't work. Is there any simple way of picking a $g$ that does what we want? Or am I farther off than I'm expecting? Thanks!","I'm having some (hopefully small) issues computing the norm of an operator. Firstly, the problem, For $f\in L^\infty[0,1]$, define $M_f: L^2[0,1]\to L^2[0,1]$ by $M_f(g)(x) = f(x)g(x)$. Show that $M_f$ is a bounded linear operator and $\|M_f\| = \|f\|_\infty$. What I've done so far: To see that $M_f$ is linear, let $g_1,g_2\in L^2[0,1]$ and $\lambda\in\mathbb{R}$. Then $$ M_f(g_1 + \lambda g_2)(x) = f(x)(g_1(x) + \lambda g_2(x)) = f(x)g_1(x) + \lambda f(x)g_2(x) = M_f(g_1)(x) + \lambda M_f(g_2)(x).$$ As $f\in L^\infty$ we know there is some minimal $N\in \mathbb{R}$ such that $|f(x)| \leq N$ almost everywhere (i.e., $\|f\|_\infty = N \lt \infty$). The fact that $M_f$ is bounded comes straight this assupmtion, as $$ \|M_f\| = \sup \frac{\|fg\|_2}{\|g\|_2} \leq \sup \frac{\|Ng\|_2}{\|g\|_2} = N\sup \frac{\|g\|_2}{\|g\|_2} = N < \infty.$$ almost everywhere in $[0,1]$, for all non-zero $g\in L^2[0,1]$. So $M_f$ is a bounded linear operator on $L^2[0,1]$. To prove equality, we must show there is some $g\in L^2[0,1]$ so that $\frac{\|fg\|_2}{\|g\|_2} = N$. Now I want to say something along the lines of pick $g = 1$, but this won't have $\|g\|_2 = 1$ for all measures, so this won't work. Is there any simple way of picking a $g$ that does what we want? Or am I farther off than I'm expecting? Thanks!",,"['analysis', 'normed-spaces']"
50,Difference between “for some $k$” and “for some arbitrary $k$”,Difference between “for some ” and “for some arbitrary ”,k k,"I am told that the “for some” and “for some arbitrary”  are different. For example, when proving the statement “if n is odd, then $n^2$ is odd”, one of the steps includes writing $$\text{$n = 2k+1,\:\:$ where $k$ is some integer}.$$ I am told that writing “ $k$ is some arbitrary integer” here is wrong? On the other hand, in mathematical induction, when performing the inductive step, before writing the inductive hypothesis for $k,$ we write “for some arbitrary $k\text”$ . I am then told that the “arbitrary” here is compulsory to write. To me, they sound the same. Am I wrong? Are they different in terms of notation? PS I’m relatively new to this stuff, maybe explain less using notation and appeal more to logical understanding.","I am told that the “for some” and “for some arbitrary”  are different. For example, when proving the statement “if n is odd, then is odd”, one of the steps includes writing I am told that writing “ is some arbitrary integer” here is wrong? On the other hand, in mathematical induction, when performing the inductive step, before writing the inductive hypothesis for we write “for some arbitrary . I am then told that the “arbitrary” here is compulsory to write. To me, they sound the same. Am I wrong? Are they different in terms of notation? PS I’m relatively new to this stuff, maybe explain less using notation and appeal more to logical understanding.","n^2 \text{n = 2k+1,\:\: where k is some integer}. k k, k\text”","['analysis', 'logic', 'proof-writing', 'quantifiers', 'logic-translation']"
51,isomorphism of Dedekind complete ordered fields,isomorphism of Dedekind complete ordered fields,,"In the last chapter of Spivak's Calculus, there is a proof that complete ordered fields are unique up to isomorphism. I find the first steps in it somewhat suspicious. Specifically, I believe he is treating numbers as rational numbers which are at best isomorphic to them. (Here $\mathbb{R}$ is the field of Dedekind cuts. Also: Spivak is going to denote addition in $F$ with $\oplus$, in contrast to addition in $\mathbb{R}$, denoted by $+$.) Verbatim: Theorem : If $F$ is a complete ordered field, then $F$ is isomorphic to $\mathbb{R}$. Proof : Since two fields are defined to be isomorphic if there is an isomorphism between them, we must actually construct a function $f$ from $\mathbb{R}$ to $F$ which is an isomorphism. We begin by defining $f$ on the integers as follows: $$f(0) = \mathbf{0}$$   $$f(n) = \underbrace{ \mathbf{1} \oplus \text{ } ... \text{ } \oplus \mathbf{1}}_{n \text{ times}} \text{, }\text{   for $n > 0$}$$   $$f(n) = -( \underbrace{ \mathbf{1} \oplus \text{ } ... \text{ } \oplus \mathbf{1}}_{|n| \text{ times}}) \text{, }\text{   for $n < 0$.}$$ It is easy to check that $$f(m + n) = f(m) \oplus f(n) $$   $$f(m \cdot n) = f(m) \odot f(n) $$ for all integers $m$ and $n$, and it is convenient to denote $f(n)$ by $\bf{n}$. We then >define $f$ on the rational numbers by $$f(\frac{m}{n}) = \bf{\frac{m}{n}} = \bf{m \odot n^{-1}}$$ (notice that the $n$-fold sum $\mathbf{1} \oplus \text{ } ... \text{ } \oplus \mathbf{1} \neq \bf{0}$ if $n>0$, since $F$ is an ordered field). This definition makes sense because if $\frac{m}{n} = \frac{k}{l}$, then $ml = nk$, so $\bf{m} \odot \bf{l} = \bf{k} \odot \bf{n}$, so $\bf{m} \odot \bf{n^{-1}} = \bf{k} \odot \bf{l ^ {-1}}.$ It is easy to check that $$f(r_1 + r_2) = f(r_1) \oplus f(r_2)$$   $$f(r_1 \cdot r_2) = f(r_1) \odot f(r_2)$$ for all rational numbers $r_1$ and $r_2$ and that $f(r_1) \prec f(r_2)$ if $r_1 < r_2$. My problem with this is - well, actually, I have two problems with this. The first one is, \begin{align} \text{what does  } \underbrace{ \mathbf{1} \oplus \text{ } ... \text{ } \oplus \mathbf{1}}_{n \text{ times}} \text{  mean?} \end{align} I get that you can say, ""look, $1x = x$, $2x = x+x$, $3x = (x+x)+x$, and so on."" But hold it, I say: ""and so on"" sounds like induction. And the $n$ we referred to isn't really an integer (much less a positive integer in $\mathbb{N}$), but rather a member of $\mathbb{R}$. So, how can we induct on its value? My other question needs still more introduction! If $z$ is an ""integer"" $\{t \in \mathbb{Q}: t<n\} \in \mathbb{R}$, we say $z \in \mathbb{Z_R}$. By $\mathrm{quot}_{\mathbb{R}}$, we will denote the set of products $\{m \cdot n^{-1} \in \mathbb{R}: m, n \in \mathbb{Z_R} \land n \neq 0 \}$. Finally, put $\Theta_x = \{f(y) \in F: y < x, y \in \mathrm{quot_\mathbb{R}} \}$.  In these terms, Spivak subsequently claims things like ""Given $x, y \in \mathbb{R}$, it is clear that $x<y \implies \Theta_x \subset \Theta_y$."" (He really does say ""clear"". This would be clear if $\mathrm{quot_\mathbb{R}}$ were equal to $\mathbb{Q}$; then it would just require us to say, ""if $x$ and $y$ are Dedekind cuts, then the set $X$ of rationals contained in $x$ is a subset of those rationals $Y$ contained in $y$; thus, $f(X) \subset f(Y)$"". But you can't freakin' do that!) His basic goal is to say that $\phi: x \rightarrow \sup \Theta_x$ is an isomorphism (he claims that $\phi$ agrees with $f$ wherever both are defined, i.e. on $\mathrm{quot}_{\mathbb{R}}$). So, this has all left me wondering, \begin{align} \text{how do I navigate his multiple meanings for rationals?} \end{align} I mean, I really don't know how to prove any of his claims without playing fast and loose with how $\mathbb{N}, \mathbb{Q}$ and $\mathrm{quot}_{\mathbb{R}}$ relate. Can somebody give me some ideas/hints? (A little note here: when I think of $\mathbb{N}$, formally I think of the minimal successor set, satisfying the Peano axioms. I'm willing to accept $\mathbb{N}$, $\mathbb{Z}$ and $\mathbb{Q}$ as god-given, but $\{t \in \mathbb{Q}: t<n\} \in \mathbb{R}$ is very different from $n \in \mathbb{N}$. I do know the obvious isomorphism between them.)","In the last chapter of Spivak's Calculus, there is a proof that complete ordered fields are unique up to isomorphism. I find the first steps in it somewhat suspicious. Specifically, I believe he is treating numbers as rational numbers which are at best isomorphic to them. (Here $\mathbb{R}$ is the field of Dedekind cuts. Also: Spivak is going to denote addition in $F$ with $\oplus$, in contrast to addition in $\mathbb{R}$, denoted by $+$.) Verbatim: Theorem : If $F$ is a complete ordered field, then $F$ is isomorphic to $\mathbb{R}$. Proof : Since two fields are defined to be isomorphic if there is an isomorphism between them, we must actually construct a function $f$ from $\mathbb{R}$ to $F$ which is an isomorphism. We begin by defining $f$ on the integers as follows: $$f(0) = \mathbf{0}$$   $$f(n) = \underbrace{ \mathbf{1} \oplus \text{ } ... \text{ } \oplus \mathbf{1}}_{n \text{ times}} \text{, }\text{   for $n > 0$}$$   $$f(n) = -( \underbrace{ \mathbf{1} \oplus \text{ } ... \text{ } \oplus \mathbf{1}}_{|n| \text{ times}}) \text{, }\text{   for $n < 0$.}$$ It is easy to check that $$f(m + n) = f(m) \oplus f(n) $$   $$f(m \cdot n) = f(m) \odot f(n) $$ for all integers $m$ and $n$, and it is convenient to denote $f(n)$ by $\bf{n}$. We then >define $f$ on the rational numbers by $$f(\frac{m}{n}) = \bf{\frac{m}{n}} = \bf{m \odot n^{-1}}$$ (notice that the $n$-fold sum $\mathbf{1} \oplus \text{ } ... \text{ } \oplus \mathbf{1} \neq \bf{0}$ if $n>0$, since $F$ is an ordered field). This definition makes sense because if $\frac{m}{n} = \frac{k}{l}$, then $ml = nk$, so $\bf{m} \odot \bf{l} = \bf{k} \odot \bf{n}$, so $\bf{m} \odot \bf{n^{-1}} = \bf{k} \odot \bf{l ^ {-1}}.$ It is easy to check that $$f(r_1 + r_2) = f(r_1) \oplus f(r_2)$$   $$f(r_1 \cdot r_2) = f(r_1) \odot f(r_2)$$ for all rational numbers $r_1$ and $r_2$ and that $f(r_1) \prec f(r_2)$ if $r_1 < r_2$. My problem with this is - well, actually, I have two problems with this. The first one is, \begin{align} \text{what does  } \underbrace{ \mathbf{1} \oplus \text{ } ... \text{ } \oplus \mathbf{1}}_{n \text{ times}} \text{  mean?} \end{align} I get that you can say, ""look, $1x = x$, $2x = x+x$, $3x = (x+x)+x$, and so on."" But hold it, I say: ""and so on"" sounds like induction. And the $n$ we referred to isn't really an integer (much less a positive integer in $\mathbb{N}$), but rather a member of $\mathbb{R}$. So, how can we induct on its value? My other question needs still more introduction! If $z$ is an ""integer"" $\{t \in \mathbb{Q}: t<n\} \in \mathbb{R}$, we say $z \in \mathbb{Z_R}$. By $\mathrm{quot}_{\mathbb{R}}$, we will denote the set of products $\{m \cdot n^{-1} \in \mathbb{R}: m, n \in \mathbb{Z_R} \land n \neq 0 \}$. Finally, put $\Theta_x = \{f(y) \in F: y < x, y \in \mathrm{quot_\mathbb{R}} \}$.  In these terms, Spivak subsequently claims things like ""Given $x, y \in \mathbb{R}$, it is clear that $x<y \implies \Theta_x \subset \Theta_y$."" (He really does say ""clear"". This would be clear if $\mathrm{quot_\mathbb{R}}$ were equal to $\mathbb{Q}$; then it would just require us to say, ""if $x$ and $y$ are Dedekind cuts, then the set $X$ of rationals contained in $x$ is a subset of those rationals $Y$ contained in $y$; thus, $f(X) \subset f(Y)$"". But you can't freakin' do that!) His basic goal is to say that $\phi: x \rightarrow \sup \Theta_x$ is an isomorphism (he claims that $\phi$ agrees with $f$ wherever both are defined, i.e. on $\mathrm{quot}_{\mathbb{R}}$). So, this has all left me wondering, \begin{align} \text{how do I navigate his multiple meanings for rationals?} \end{align} I mean, I really don't know how to prove any of his claims without playing fast and loose with how $\mathbb{N}, \mathbb{Q}$ and $\mathrm{quot}_{\mathbb{R}}$ relate. Can somebody give me some ideas/hints? (A little note here: when I think of $\mathbb{N}$, formally I think of the minimal successor set, satisfying the Peano axioms. I'm willing to accept $\mathbb{N}$, $\mathbb{Z}$ and $\mathbb{Q}$ as god-given, but $\{t \in \mathbb{Q}: t<n\} \in \mathbb{R}$ is very different from $n \in \mathbb{N}$. I do know the obvious isomorphism between them.)",,"['analysis', 'field-theory', 'ordered-fields']"
52,"Uniform convergence, but no absolute uniform convergence","Uniform convergence, but no absolute uniform convergence",,"Can someone give an example of a series of functions $f_k(x)$ for which $\sum_{k=0}^{\infty} f_k(x)$ converges uniformly, and $\sum_{k=0}^{\infty} |f_k(x)|$ converges pointwise, but $\sum_{k=0}^{\infty} |f_k(x)|$ does not converge uniformly. I'm having trouble finding an example (or proving this is impossible), as there is no certainty the limit of the absolute and relative series are equal.","Can someone give an example of a series of functions $f_k(x)$ for which $\sum_{k=0}^{\infty} f_k(x)$ converges uniformly, and $\sum_{k=0}^{\infty} |f_k(x)|$ converges pointwise, but $\sum_{k=0}^{\infty} |f_k(x)|$ does not converge uniformly. I'm having trouble finding an example (or proving this is impossible), as there is no certainty the limit of the absolute and relative series are equal.",,"['analysis', 'convergence-divergence']"
53,Is it possible to extend an arbitrary smooth function on a closed subset of $R^n$ to a smooth function on $R^n$?,Is it possible to extend an arbitrary smooth function on a closed subset of  to a smooth function on ?,R^n R^n,Assume that $K$ is a closed (or compact if necessary) subset in $\mathbb{R^n}$ and $f:K \rightarrow \mathbb{R}$ is a smoth function in the following sense: for each $x \in K$ there exists a neighbourhood $V_x$  in $\mathbb{R}^n$ of $x$ and a function $F_x: V_x \rightarrow \mathbb{R}$ of class $C^\infty(V_x)$ such that $F_x| (V_x \cap K)=f|(V_x \cap K)$. Is it posible to extend $f$ to a function of class $C^\infty$ on $\mathbb{R}^n$ ?,Assume that $K$ is a closed (or compact if necessary) subset in $\mathbb{R^n}$ and $f:K \rightarrow \mathbb{R}$ is a smoth function in the following sense: for each $x \in K$ there exists a neighbourhood $V_x$  in $\mathbb{R}^n$ of $x$ and a function $F_x: V_x \rightarrow \mathbb{R}$ of class $C^\infty(V_x)$ such that $F_x| (V_x \cap K)=f|(V_x \cap K)$. Is it posible to extend $f$ to a function of class $C^\infty$ on $\mathbb{R}^n$ ?,,['analysis']
54,An overview of analysis,An overview of analysis,,"I'm looking for a book that gives an overview of analysis, a bit like Shafarevich's Basic Notions of Algebra but for analysis. The book I have in mind would give definitions, theorems, examples, and sometimes sketches of proofs. It would cover a broad swathe of analysis (real, complex, functional, differential equations) and discuss a range of applications (i.e. in physics and in prime numbers). I've looked at the Analysis I volume of the Encyclopaedia of Mathematical Sciences which Shafarevich's book is also a part of, but it focuses more on methods and isn't quite what I have in mind. Thank you!","I'm looking for a book that gives an overview of analysis, a bit like Shafarevich's Basic Notions of Algebra but for analysis. The book I have in mind would give definitions, theorems, examples, and sometimes sketches of proofs. It would cover a broad swathe of analysis (real, complex, functional, differential equations) and discuss a range of applications (i.e. in physics and in prime numbers). I've looked at the Analysis I volume of the Encyclopaedia of Mathematical Sciences which Shafarevich's book is also a part of, but it focuses more on methods and isn't quite what I have in mind. Thank you!",,"['analysis', 'reference-request']"
55,Does Riemann integrable imply Lebesgue integrable?,Does Riemann integrable imply Lebesgue integrable?,,"Suppose a definite integral exists in the Riemann sense. Does that mean the integral exists as a Lebesgue integral, and do we get the same result either way? ------- BTW: I have a MS in Electrical Engineering and a strong interest in math. I had one semester of real analysis 25 years ago, I tried to learn Lebesgue integration on my own by reading a book on real analysis, and that was a few years ago. Hence, I don't have a solid grasp of the subject.","Suppose a definite integral exists in the Riemann sense. Does that mean the integral exists as a Lebesgue integral, and do we get the same result either way? ------- BTW: I have a MS in Electrical Engineering and a strong interest in math. I had one semester of real analysis 25 years ago, I tried to learn Lebesgue integration on my own by reading a book on real analysis, and that was a few years ago. Hence, I don't have a solid grasp of the subject.",,"['analysis', 'lebesgue-integral']"
56,How to prove that the sum of reciprocals of one plus perfect powers is $\frac{\pi^{2}}{3}-\frac{5}{2}$,How to prove that the sum of reciprocals of one plus perfect powers is,\frac{\pi^{2}}{3}-\frac{5}{2},"Let $S$ be Set of perfect powers without duplicates $1,4,8,9,\dots$ ( http://oeis.org/A001597 ) How to prove the following? $$\sum_{s\in S}\frac{1}{s+1}=\frac{\pi^{2}}{3}-\frac{5}{2}$$ (starting with $s=4$ ) I found this formula in the book ""Mathematical Constants"" by Steven R. Finch on page 113.","Let be Set of perfect powers without duplicates ( http://oeis.org/A001597 ) How to prove the following? (starting with ) I found this formula in the book ""Mathematical Constants"" by Steven R. Finch on page 113.","S 1,4,8,9,\dots \sum_{s\in S}\frac{1}{s+1}=\frac{\pi^{2}}{3}-\frac{5}{2} s=4","['analysis', 'number-theory']"
57,Are there any simple examples of Kolmogorov-Arnold representation?,Are there any simple examples of Kolmogorov-Arnold representation?,,"I had never heard of the Kolmogorov-Arnold Representation Theorem before. It states roughly that any multivariable function can be represented by repeatedly adding a single variable function whose input is a sum of single variable functions for each of the variables (of course, that isn't a precise statement). Here is the formula for a function of $x$ and $y$ (i.e. that $n=2$) using Theorem $2$ from this paper : $$f(x,y)=\sum_{k=1}^5 g(\phi_k(x)+\lambda \phi_k(y))$$ Note that this representation is a bit differently written. We have $k=q+1$ and $\phi(x+k\epsilon)=\phi_k(x).$ Are there any simple (and non-trivial) examples? I have searched around online and can only find what seems to be very complicated proofs of the theorem but no actual concrete examples. How about $f(x,y)=x^2+y$ or $f(x,y)=xy^3$, etc.? I really don't see a simple way to construct such $g$ and $\phi_k$. Also, I couldn't figure out which tags would be appropriate here.","I had never heard of the Kolmogorov-Arnold Representation Theorem before. It states roughly that any multivariable function can be represented by repeatedly adding a single variable function whose input is a sum of single variable functions for each of the variables (of course, that isn't a precise statement). Here is the formula for a function of $x$ and $y$ (i.e. that $n=2$) using Theorem $2$ from this paper : $$f(x,y)=\sum_{k=1}^5 g(\phi_k(x)+\lambda \phi_k(y))$$ Note that this representation is a bit differently written. We have $k=q+1$ and $\phi(x+k\epsilon)=\phi_k(x).$ Are there any simple (and non-trivial) examples? I have searched around online and can only find what seems to be very complicated proofs of the theorem but no actual concrete examples. How about $f(x,y)=x^2+y$ or $f(x,y)=xy^3$, etc.? I really don't see a simple way to construct such $g$ and $\phi_k$. Also, I couldn't figure out which tags would be appropriate here.",,"['analysis', 'multivariable-calculus', 'examples-counterexamples']"
58,Show the image of a continuous function on a closed interval is closed.,Show the image of a continuous function on a closed interval is closed.,,"I tried this problem on my own, but got 1 out of 5. Now we are supposed to find someone to help us. Here is what I did: Let $f:[a,b] \rightarrow \mathbb{R}$ be continuous on a closed interval $I$ with $a,b \in I$, $a \leq b$ If $f(a), f(b) \in f(I)$ let $f(a)\leq y \leq f(b)$. Then by IVT there exists $x$, $a\leq x \leq b$ where $f(x)=y$ $Rightarrow$ The image is also an interval. Show closed: Let m be the lowest upper bound and M the greatest lower bound of the image interval. $I=[a,b]$ must be a subset of $[M,m]$ and the function attains its bounds,  $m,M\in f(I)$. so $f(I)$ is a subset of $[M,m]$, thus is closed. Can anyone provide a proof of this statement? Thanks!","I tried this problem on my own, but got 1 out of 5. Now we are supposed to find someone to help us. Here is what I did: Let $f:[a,b] \rightarrow \mathbb{R}$ be continuous on a closed interval $I$ with $a,b \in I$, $a \leq b$ If $f(a), f(b) \in f(I)$ let $f(a)\leq y \leq f(b)$. Then by IVT there exists $x$, $a\leq x \leq b$ where $f(x)=y$ $Rightarrow$ The image is also an interval. Show closed: Let m be the lowest upper bound and M the greatest lower bound of the image interval. $I=[a,b]$ must be a subset of $[M,m]$ and the function attains its bounds,  $m,M\in f(I)$. so $f(I)$ is a subset of $[M,m]$, thus is closed. Can anyone provide a proof of this statement? Thanks!",,['analysis']
59,Bolzano-Weierstrass for sequences of sets,Bolzano-Weierstrass for sequences of sets,,"Let $\mathcal{A}_n,\,n\in\mathbb{N}$ be a sequence of subsets of, say, $\mathbb{R}$. Let $\limsup_{n\rightarrow\infty} \mathcal{A}_n = \{x:x\in\mathcal{A}_n\mbox{ for infinitely many } n\}$, and $\liminf_{n\rightarrow\infty} \mathcal{A}_n = \{x:x\in\mathcal{A}_n\mbox{ for all but finitely many } n\}$ as usual. We say the sequence $\mathcal{A}_n,\,n\in\mathbb{N}$ has a limit if $\limsup_{n\rightarrow\infty} \mathcal{A}_n = \liminf_{n\rightarrow\infty} \mathcal{A}_n$ and write $\lim_{n\rightarrow\infty} \mathcal{A}_n$ for the limit. My question is, given an arbitrary sequence $\mathcal{A}_n,\,n\in\mathbb{N}$, is there a subsequence $\mathcal{A}_{n_k},\,k\in\mathbb{N}$ such that $\lim_{k\rightarrow\infty} \mathcal{A}_{n_k}$ exists? This should be well-known, but I could not find a reference.","Let $\mathcal{A}_n,\,n\in\mathbb{N}$ be a sequence of subsets of, say, $\mathbb{R}$. Let $\limsup_{n\rightarrow\infty} \mathcal{A}_n = \{x:x\in\mathcal{A}_n\mbox{ for infinitely many } n\}$, and $\liminf_{n\rightarrow\infty} \mathcal{A}_n = \{x:x\in\mathcal{A}_n\mbox{ for all but finitely many } n\}$ as usual. We say the sequence $\mathcal{A}_n,\,n\in\mathbb{N}$ has a limit if $\limsup_{n\rightarrow\infty} \mathcal{A}_n = \liminf_{n\rightarrow\infty} \mathcal{A}_n$ and write $\lim_{n\rightarrow\infty} \mathcal{A}_n$ for the limit. My question is, given an arbitrary sequence $\mathcal{A}_n,\,n\in\mathbb{N}$, is there a subsequence $\mathcal{A}_{n_k},\,k\in\mathbb{N}$ such that $\lim_{k\rightarrow\infty} \mathcal{A}_{n_k}$ exists? This should be well-known, but I could not find a reference.",,"['analysis', 'elementary-set-theory']"
60,Continuous Brouwer's fixed point theorem via Stokes's theorem?,Continuous Brouwer's fixed point theorem via Stokes's theorem?,,"Let $B$ denote the closed unit ball in $\mathbf{R}^n$. Brouwer's fixed point theorem states that every continuous map $f:B\to B$ has a fixed point. There is a simple proof using Stokes's theorem, at least for the special case in which $f$ is smooth, as presented on Wikipedia here . The page also states that this case contains the full generality of the theorem, because if $f:B\to B$ is continuous without fixed points then $\epsilon = \inf_{x\in B} |x-f(x)| > 0$, so we can just convolve (each component of) $f$ with a smooth bump $\psi:\mathbf{R}^n\to\mathbf{R}$ supported on $\epsilon B$ to get a smooth counterexample to the theorem. Unfortunately, as it stands the proof doesn't work, because the distance of $f(B)$ to $\partial B$ could well be zero, in which case $\tilde{f} = \psi\ast f$ might not satisfy $\tilde{f}(B)\subset B$. Does anybody see a resolution to this difficulty? EDIT, following Willy's answer. I've just realised that I was confused when I asked this question. $\tilde{f}(B)\subset B$ was never really an issue; the issue was rather that convolution isn't fully defined near the boundary. The most immediate interpretation is to extend $f:B\to B$ by $0$ to $\mathbf{R}^n\to B$, but then mollifying $f$ doesn't give you a uniformly nearby $\tilde{f}$. The interpretation that works is to extend $f:B\to B$ to any uniformly continuous $F:\mathbf{R}^n\to B$, such as $$F(x) = \begin{cases} f(x) & \text{if $|x|\leq 1$,}\\  f(x/|x|) & \text{if $|x|\geq 1$,}\end{cases}$$ and then mollify.","Let $B$ denote the closed unit ball in $\mathbf{R}^n$. Brouwer's fixed point theorem states that every continuous map $f:B\to B$ has a fixed point. There is a simple proof using Stokes's theorem, at least for the special case in which $f$ is smooth, as presented on Wikipedia here . The page also states that this case contains the full generality of the theorem, because if $f:B\to B$ is continuous without fixed points then $\epsilon = \inf_{x\in B} |x-f(x)| > 0$, so we can just convolve (each component of) $f$ with a smooth bump $\psi:\mathbf{R}^n\to\mathbf{R}$ supported on $\epsilon B$ to get a smooth counterexample to the theorem. Unfortunately, as it stands the proof doesn't work, because the distance of $f(B)$ to $\partial B$ could well be zero, in which case $\tilde{f} = \psi\ast f$ might not satisfy $\tilde{f}(B)\subset B$. Does anybody see a resolution to this difficulty? EDIT, following Willy's answer. I've just realised that I was confused when I asked this question. $\tilde{f}(B)\subset B$ was never really an issue; the issue was rather that convolution isn't fully defined near the boundary. The most immediate interpretation is to extend $f:B\to B$ by $0$ to $\mathbf{R}^n\to B$, but then mollifying $f$ doesn't give you a uniformly nearby $\tilde{f}$. The interpretation that works is to extend $f:B\to B$ to any uniformly continuous $F:\mathbf{R}^n\to B$, such as $$F(x) = \begin{cases} f(x) & \text{if $|x|\leq 1$,}\\  f(x/|x|) & \text{if $|x|\geq 1$,}\end{cases}$$ and then mollify.",,"['analysis', 'fixed-point-theorems']"
61,A question about Cauchy product,A question about Cauchy product,,"Consider the following two properties of a real divergent sequence $\{c_n\}$ . $(1) \;$ $\displaystyle\lim_{n\to\infty}\frac{c_n}{n}=M$ for some $M\in\mathbb{R}$ . $(2) \;$ There exist two real convergent  sequences $\{a_n\}$ , $\{b_n\}$ such that $c_n=\sum_{k=0}^{n}a_kb_{n-k}$ . Are $(1)$ and $(2)$ equivalent? I know how to prove $(2) \implies (1)$ , but I don't know whether $(1)$ implies $(2)$ .","Consider the following two properties of a real divergent sequence . for some . There exist two real convergent  sequences , such that . Are and equivalent? I know how to prove , but I don't know whether implies .",\{c_n\} (1) \; \displaystyle\lim_{n\to\infty}\frac{c_n}{n}=M M\in\mathbb{R} (2) \; \{a_n\} \{b_n\} c_n=\sum_{k=0}^{n}a_kb_{n-k} (1) (2) (2) \implies (1) (1) (2),['analysis']
62,Frechet derivative of squared norm $\|x\|^2$,Frechet derivative of squared norm,\|x\|^2,"I got this from Analysis II, H. Amann, J. Escher, p. 152. Their definition of the derivative of a map $f$ between Banach spaces $E,F$ over the field $\mathbb{K}$ is a bounded linear operator $A\in\mathcal{L}(E,F)$ such that $$\lim_{x\to x_0} \frac{f(x) - f(x_0) - A(x - x_0)}{\|x - x_0\|} = 0.$$ Squared-Norm Example: Suppose $H$ is a Hilbert space and define $f\colon H\to\mathbb{K}$, $x\mapsto \|x\|^2$. They claim that $f$ is continuously differentiable and compute the derivative as  $$Df(x) = 2\operatorname{Re}\langle x,\cdot\rangle,\ \text{for $x\in H$}.$$ For a real Hilbert space $H$, $\mathbb{K} = \mathbb{R}$, this seems ok: $Df(x)h = 2\langle x,h\rangle$. Questions: For $\mathbb{K} = \mathbb{C}$, I think this is wrong. The calculation $$f(x+h) - f(x) - 2\operatorname{Re}\langle x,h\rangle = \|h\|^2 = o(\|h\|)$$ still works.  For example, if we define $f\colon\mathbb{C}\to\mathbb{C}$ by $f(x) = |x|^2$, then their claim is that $Df(x)h = 2\operatorname{Re}(x\overline{h})$. This doesn't seem to be linear.   So, their map $Df(x) = 2\operatorname{Re}\langle x,\cdot\rangle$ is not linear in the complex case, right? Does $f(x) = \|x\|^2$ have a Frechet derivative in the complex case?","I got this from Analysis II, H. Amann, J. Escher, p. 152. Their definition of the derivative of a map $f$ between Banach spaces $E,F$ over the field $\mathbb{K}$ is a bounded linear operator $A\in\mathcal{L}(E,F)$ such that $$\lim_{x\to x_0} \frac{f(x) - f(x_0) - A(x - x_0)}{\|x - x_0\|} = 0.$$ Squared-Norm Example: Suppose $H$ is a Hilbert space and define $f\colon H\to\mathbb{K}$, $x\mapsto \|x\|^2$. They claim that $f$ is continuously differentiable and compute the derivative as  $$Df(x) = 2\operatorname{Re}\langle x,\cdot\rangle,\ \text{for $x\in H$}.$$ For a real Hilbert space $H$, $\mathbb{K} = \mathbb{R}$, this seems ok: $Df(x)h = 2\langle x,h\rangle$. Questions: For $\mathbb{K} = \mathbb{C}$, I think this is wrong. The calculation $$f(x+h) - f(x) - 2\operatorname{Re}\langle x,h\rangle = \|h\|^2 = o(\|h\|)$$ still works.  For example, if we define $f\colon\mathbb{C}\to\mathbb{C}$ by $f(x) = |x|^2$, then their claim is that $Df(x)h = 2\operatorname{Re}(x\overline{h})$. This doesn't seem to be linear.   So, their map $Df(x) = 2\operatorname{Re}\langle x,\cdot\rangle$ is not linear in the complex case, right? Does $f(x) = \|x\|^2$ have a Frechet derivative in the complex case?",,"['analysis', 'derivatives']"
63,How to prove there is no positive and continuous function satisfying some conditions,How to prove there is no positive and continuous function satisfying some conditions,,Let $\alpha \in \mathbb R $. How could I prove there isn't any positive and continuous function $f$ such that the following conditions hold? $\int_{0}^1 f(x)dx=1$ $\int_{0}^1 xf(x)dx=\alpha $ $\int_{0}^1 x^2f(x)dx=\alpha ^2$,Let $\alpha \in \mathbb R $. How could I prove there isn't any positive and continuous function $f$ such that the following conditions hold? $\int_{0}^1 f(x)dx=1$ $\int_{0}^1 xf(x)dx=\alpha $ $\int_{0}^1 x^2f(x)dx=\alpha ^2$,,"['analysis', 'contest-math']"
64,Characterization of measurable sets $E$ with $|E|_e<\infty$,Characterization of measurable sets  with,E |E|_e<\infty,"I will use the notation of Wheeden & Zygmund's Measure and Integral because the problem comes from that book: $|\cdot|_e$ for outer measure and $|\cdot|$ for Lebesgue measure. An exercise says: prove (3.29) and (3.29) is: Theorem Suppose that $|E|_e<\infty$ . Then $E$ is measurable if and only if given $\epsilon>0$ , $E=(S\cup N_1)\setminus N_2$ , where $S$ is a finite union of non-overlapping intervals and $|N_1|_e,|N_2|_e<\epsilon$ . I did "" $\implies$ "", and I was thinking that "" $\impliedby$ "" would be easier. But is not after all. I appreciate any advice.","I will use the notation of Wheeden & Zygmund's Measure and Integral because the problem comes from that book: for outer measure and for Lebesgue measure. An exercise says: prove (3.29) and (3.29) is: Theorem Suppose that . Then is measurable if and only if given , , where is a finite union of non-overlapping intervals and . I did "" "", and I was thinking that "" "" would be easier. But is not after all. I appreciate any advice.","|\cdot|_e |\cdot| |E|_e<\infty E \epsilon>0 E=(S\cup N_1)\setminus N_2 S |N_1|_e,|N_2|_e<\epsilon \implies \impliedby","['analysis', 'measure-theory']"
65,How prove this they have a common fixed point,How prove this they have a common fixed point,,"If two continuous mappings $f$ and $g$ of an interval into ifself conumute,that is $$f(g(x))=g(f(x))$$,then they have a common fixed point? This problem is from Mathemmatical Analysis (Zorich) PP169,Thank you everyone.","If two continuous mappings $f$ and $g$ of an interval into ifself conumute,that is $$f(g(x))=g(f(x))$$,then they have a common fixed point? This problem is from Mathemmatical Analysis (Zorich) PP169,Thank you everyone.",,['analysis']
66,"Solving the system $\tan x+\sin y+\sin z=3x$, $\sin x+\tan y+\sin z=3y$, $\sin x+\sin y+\tan z=3z$","Solving the system , ,",\tan x+\sin y+\sin z=3x \sin x+\tan y+\sin z=3y \sin x+\sin y+\tan z=3z,"If $x,y,z\in (0,\frac{\pi}2 )$ , find all solutions to: $$\begin{cases} \tan x+\sin y+\sin z=3x \\ \sin x+\tan y+\sin z=3y \\ \sin x+\sin y+\tan z=3z\end{cases}$$ This question was deleted here: https://math.stackexchange.com/questions/3747782/ My solution. Let $f(x)=\tan{x}+2\sin{x}-3x$ . Thus, by AM-GM $$f'(x)=\frac{1}{\cos^2x}+2\cos{x}-3\geq3\sqrt[3]{\frac{1}{\cos^2x}\cdot(\cos{x})^2}-3=0,$$ which says that $f$ increases. Thus, $$f(x)>f(0)=0$$ and $$3\sum_{cyc}x=\sum_{cyc}(\tan{x}+\sin{y}+\sin{z})=\sum_{cyc}(\tan{x}+2\sin{y})>3\sum_{cyc}x,$$ which is impossible. Id est, our system has no solutions $(x,y,z)$ , where $\{x,y,z\}\subset\left(0,\frac{\pi}{2}\right).$ Is there some alternative solution for this system?","If , find all solutions to: This question was deleted here: https://math.stackexchange.com/questions/3747782/ My solution. Let . Thus, by AM-GM which says that increases. Thus, and which is impossible. Id est, our system has no solutions , where Is there some alternative solution for this system?","x,y,z\in (0,\frac{\pi}2 ) \begin{cases} \tan x+\sin y+\sin z=3x \\
\sin x+\tan y+\sin z=3y \\
\sin x+\sin y+\tan z=3z\end{cases} f(x)=\tan{x}+2\sin{x}-3x f'(x)=\frac{1}{\cos^2x}+2\cos{x}-3\geq3\sqrt[3]{\frac{1}{\cos^2x}\cdot(\cos{x})^2}-3=0, f f(x)>f(0)=0 3\sum_{cyc}x=\sum_{cyc}(\tan{x}+\sin{y}+\sin{z})=\sum_{cyc}(\tan{x}+2\sin{y})>3\sum_{cyc}x, (x,y,z) \{x,y,z\}\subset\left(0,\frac{\pi}{2}\right).","['analysis', 'trigonometry', 'alternative-proof', 'a.m.-g.m.-inequality']"
67,A functional equation problem on $\mathbb{Q}^{+}$: $f(x)+f\left(\frac{1}{x}\right)=1$ and $f(2x)=2f\bigl(f(x)\bigr)$,A functional equation problem on :  and,\mathbb{Q}^{+} f(x)+f\left(\frac{1}{x}\right)=1 f(2x)=2f\bigl(f(x)\bigr),"Let $f$ be a function which maps $\mathbb{Q}^{+}$ to $\mathbb{Q}^{+}$ and satisfies $$ \begin{cases} f(x)+f\left(\frac{1}{x}\right)=1\\ f(2x)=2f\bigl(f(x)\bigr) \end{cases} $$ Show that $f\left(\frac{2012}{2013}\right)=\frac{2012}{4025}$ . I tried to prove that $f(x)=\frac{x}{x+1}$ (which satisfies all these comditions), but failed. So I tried induction: Surely, $f(1)=\frac{1}{2}$ by $f(1)+f\left(\frac{1}{1}\right)=1$ . By $f(2)=2f\bigl(f(1)\bigr)=2f\left(\frac{1}{2}\right)$ , we know $f\left(\frac{1}{2}\right)=\frac{1}{3}$ and $f(2)=\frac{2}{3}$ . Note that $f(1)=2f\Bigl(f\left(\frac{1}{2}\right)\Bigr)$ , we have $f\left(\frac{1}{3}\right)=\frac{1}{4}$ . By $f(4)=2f\bigl(f(2)\bigr)=2f\left(\frac{2}{3}\right)$ as well as $f\left(\frac{2}{3}\right)=2f\left(\frac{1}{4}\right)$ , we know $f\left(\frac{1}{4}\right)=\frac{1}{5}$ , $f\left(\frac{2}{3}\right)=\frac{2}{5}$ and $f(4)=\frac{4}{5}.$ Unfortunately, these procedures seems to have no similarity, and the larger the denominator, the more complex the procedure is. Is there any hints or solutions? Thanks for attention!","Let be a function which maps to and satisfies Show that . I tried to prove that (which satisfies all these comditions), but failed. So I tried induction: Surely, by . By , we know and . Note that , we have . By as well as , we know , and Unfortunately, these procedures seems to have no similarity, and the larger the denominator, the more complex the procedure is. Is there any hints or solutions? Thanks for attention!","f \mathbb{Q}^{+} \mathbb{Q}^{+} 
\begin{cases}
f(x)+f\left(\frac{1}{x}\right)=1\\
f(2x)=2f\bigl(f(x)\bigr)
\end{cases}
 f\left(\frac{2012}{2013}\right)=\frac{2012}{4025} f(x)=\frac{x}{x+1} f(1)=\frac{1}{2} f(1)+f\left(\frac{1}{1}\right)=1 f(2)=2f\bigl(f(1)\bigr)=2f\left(\frac{1}{2}\right) f\left(\frac{1}{2}\right)=\frac{1}{3} f(2)=\frac{2}{3} f(1)=2f\Bigl(f\left(\frac{1}{2}\right)\Bigr) f\left(\frac{1}{3}\right)=\frac{1}{4} f(4)=2f\bigl(f(2)\bigr)=2f\left(\frac{2}{3}\right) f\left(\frac{2}{3}\right)=2f\left(\frac{1}{4}\right) f\left(\frac{1}{4}\right)=\frac{1}{5} f\left(\frac{2}{3}\right)=\frac{2}{5} f(4)=\frac{4}{5}.","['analysis', 'functional-equations']"
68,Asymptotic behavior of $\Gamma^{-1}(x)$,Asymptotic behavior of,\Gamma^{-1}(x),"For real $x,$ it's well-known that $$\Gamma^{-1}(x)\sim\frac{\log x}{\log\log x}$$ So a natural question is to bound $$G(x)=\Gamma^{-1}(x)\frac{\log\log x}{\log x}$$ which of course is 1 + o(1).  Interestingly, its value is near 2 (that is, far from its asymptotic value) for many useful values of x: for example, $1.8<G(x)<2.1$ for $14<x<10^{77}.$ It seems that $G$ has a maximum near 3637.133905003816072848664328 of 2.01119450670696919822787997170113557148977275... and to decrease very slowly thereafter. Question 1 : Is the above the unique maximum? Question 2 : Is there an $x>5$ such that $G(x)<1$? Question 3 : Is there a useful factor or secondary term that makes this approximation more precise for useful values of x?  I'm being intentionally vague on this point—if I knew exactly what I was looking for I probably wouldn't need to ask. :)  For example, had I asked an analogous question about the prime-counting function, telling me about Li would be better than just giving the next asymptotic term $cx/\log^k x.$","For real $x,$ it's well-known that $$\Gamma^{-1}(x)\sim\frac{\log x}{\log\log x}$$ So a natural question is to bound $$G(x)=\Gamma^{-1}(x)\frac{\log\log x}{\log x}$$ which of course is 1 + o(1).  Interestingly, its value is near 2 (that is, far from its asymptotic value) for many useful values of x: for example, $1.8<G(x)<2.1$ for $14<x<10^{77}.$ It seems that $G$ has a maximum near 3637.133905003816072848664328 of 2.01119450670696919822787997170113557148977275... and to decrease very slowly thereafter. Question 1 : Is the above the unique maximum? Question 2 : Is there an $x>5$ such that $G(x)<1$? Question 3 : Is there a useful factor or secondary term that makes this approximation more precise for useful values of x?  I'm being intentionally vague on this point—if I knew exactly what I was looking for I probably wouldn't need to ask. :)  For example, had I asked an analogous question about the prime-counting function, telling me about Li would be better than just giving the next asymptotic term $cx/\log^k x.$",,"['analysis', 'special-functions']"
69,What is a fewnomial?,What is a fewnomial?,,"I came across the theory of ""fewnomials"" (by Khovanskii), which (I guess) are related to polynomials. However, I was surprised that there is no single question on stackexchange concerning fewnomials, and few in mathematical research in general (there are some papers on arXiv). Does anyone know something more about the concept and can please explain me the idea behind fewnomials? Thanks.","I came across the theory of ""fewnomials"" (by Khovanskii), which (I guess) are related to polynomials. However, I was surprised that there is no single question on stackexchange concerning fewnomials, and few in mathematical research in general (there are some papers on arXiv). Does anyone know something more about the concept and can please explain me the idea behind fewnomials? Thanks.",,"['analysis', 'reference-request', 'polynomials', 'terminology']"
70,How to perform a Fourier transform in spherical coordinates?,How to perform a Fourier transform in spherical coordinates?,,"For a function $f(r, \vartheta, \varphi)$ given in spherical coordinates, how can the Fourier transform be calculated best? Possible ideas: express $(r,\vartheta,\varphi)$ in cartesian coordinates, yielding a nonlinear argument of $f$ express $\vec k,\vec r$ in the $e^{i\vec k\vec r}$ term in spherical coordinates, yielding a nonlinear exponent in $\vartheta$ and $\varphi$ decompose $f$ into Spherical Harmonics and then change base to Fourier space, requiring the Fourier transform of the Spherical Harmonics ( it is obviously not possible to calculate them using this very method... , can that be be found somewhere?)","For a function $f(r, \vartheta, \varphi)$ given in spherical coordinates, how can the Fourier transform be calculated best? Possible ideas: express $(r,\vartheta,\varphi)$ in cartesian coordinates, yielding a nonlinear argument of $f$ express $\vec k,\vec r$ in the $e^{i\vec k\vec r}$ term in spherical coordinates, yielding a nonlinear exponent in $\vartheta$ and $\varphi$ decompose $f$ into Spherical Harmonics and then change base to Fourier space, requiring the Fourier transform of the Spherical Harmonics ( it is obviously not possible to calculate them using this very method... , can that be be found somewhere?)",,"['analysis', 'fourier-analysis', 'spherical-coordinates']"
71,How close to zero can a Dirichlet series get?,How close to zero can a Dirichlet series get?,,"Suppose I have an integral Dirichlet series $f(s) = \sum c_n n^{-s}$, $c_n \in \mathbb{Z}$, with at least one non-zero term $c_N$.  Suppose furthermore that this series converges absolutely and uniformly for $\mathrm{Re}(s) > 1 + \delta$ for any $\delta > 0$ (so that $c_n$ grows slower than $n^\epsilon$ for any $\epsilon$). I want $f(s)$ to be ""small"" (in terms of $N$) for fixed $\mathrm{Re}(s)$ and a range of $\mathrm{Im}(s)$.  How small can I get it over any given range? I think one can take sums of $(N + \Delta n)^{-s}$ for $\Delta n \approx O(\ln N)$ and cancel out terms in the Taylor series up to order $N^{-(s + O(\ln N))}$, all while keeping the coefficients polylog in N for $Im(s) \approx O(\ln N)$.  Can we do any better?  What about larger values of $s$?","Suppose I have an integral Dirichlet series $f(s) = \sum c_n n^{-s}$, $c_n \in \mathbb{Z}$, with at least one non-zero term $c_N$.  Suppose furthermore that this series converges absolutely and uniformly for $\mathrm{Re}(s) > 1 + \delta$ for any $\delta > 0$ (so that $c_n$ grows slower than $n^\epsilon$ for any $\epsilon$). I want $f(s)$ to be ""small"" (in terms of $N$) for fixed $\mathrm{Re}(s)$ and a range of $\mathrm{Im}(s)$.  How small can I get it over any given range? I think one can take sums of $(N + \Delta n)^{-s}$ for $\Delta n \approx O(\ln N)$ and cancel out terms in the Taylor series up to order $N^{-(s + O(\ln N))}$, all while keeping the coefficients polylog in N for $Im(s) \approx O(\ln N)$.  Can we do any better?  What about larger values of $s$?",,['analysis']
72,Is there a closed form solution of $f(x)^2+(g*f)(x)+h(x)=0$ for $f(x)$?,Is there a closed form solution of  for ?,f(x)^2+(g*f)(x)+h(x)=0 f(x),"When $g(x)$ and $h(x)$ are given functions, can $f(x)^2+(g*f)(x)+h(x)=0$ be solved for $f(x)$ in closed form (at least with some restrictions to $g,h$)? (The $*$ is not a typo, it really means convolution as in $(f*g)(x)=\int\limits_{\mathbb R}f(y)g(x-y)\,dy$)","When $g(x)$ and $h(x)$ are given functions, can $f(x)^2+(g*f)(x)+h(x)=0$ be solved for $f(x)$ in closed form (at least with some restrictions to $g,h$)? (The $*$ is not a typo, it really means convolution as in $(f*g)(x)=\int\limits_{\mathbb R}f(y)g(x-y)\,dy$)",,"['analysis', 'convolution', 'integral-equations']"
73,How to know if a term is divisible by 10 [closed],How to know if a term is divisible by 10 [closed],,"Closed. This question is off-topic . It is not currently accepting answers. This question is missing context or other details : Please improve the question by providing additional context, which ideally includes your thoughts on the problem and any attempts you have made to solve it. This information helps others identify where you have difficulties and helps them write answers appropriate to your experience level. Closed 9 years ago . Improve this question I have some difficulties to solve this easy problem, could someone help me? Is $4^{1000}-6^{500}$ divisible by $10$?","Closed. This question is off-topic . It is not currently accepting answers. This question is missing context or other details : Please improve the question by providing additional context, which ideally includes your thoughts on the problem and any attempts you have made to solve it. This information helps others identify where you have difficulties and helps them write answers appropriate to your experience level. Closed 9 years ago . Improve this question I have some difficulties to solve this easy problem, could someone help me? Is $4^{1000}-6^{500}$ divisible by $10$?",,['analysis']
74,Ways to arrive at the existence of irrational numbers,Ways to arrive at the existence of irrational numbers,,"I know about a way Greeks arrived at the existence of irrational numbers by showing that sometimes two line segments can be incommensurable. And about a simple way by which it can be shown that some numbers are irrational, for example, as is usually shown that $\sqrt2$ is irrational. Also, it can be shown that some numbers are transcendental and because all rationals are algebraic that shows that there are some non-rational, that is, irrational numbers. And also there is a way that shows that all rational numbers have periodic expansion in every base and since there are non-periodic expansions that also shows the existence of irrationals. And there is countability/uncountability way. Are there some other ways?","I know about a way Greeks arrived at the existence of irrational numbers by showing that sometimes two line segments can be incommensurable. And about a simple way by which it can be shown that some numbers are irrational, for example, as is usually shown that $\sqrt2$ is irrational. Also, it can be shown that some numbers are transcendental and because all rationals are algebraic that shows that there are some non-rational, that is, irrational numbers. And also there is a way that shows that all rational numbers have periodic expansion in every base and since there are non-periodic expansions that also shows the existence of irrationals. And there is countability/uncountability way. Are there some other ways?",,['analysis']
75,Extension of bounded convex function to boundary,Extension of bounded convex function to boundary,,"Suppose $U$ is a convex open set in $\mathbb R^n$, and $f$ is a bounded convex function on $U$. $f$ is then automatically continuous on $U$. The question is: Can we always extend $f$ continuously to $\bar{U}$? I know boundedness of $f$ is necessary, otherwise we can construct $g=1/[x-x^2]$ on unit open interval. Boundedness of $f$ is also sufficient in one dimensional case. If the extension in general exists, the only reasonable way to construct it is via: find a fixed point $x_0$ in $U$; for each point x on $\partial U$, consider the line $L$ joining $x_0$ and $x$; $f$ restricting on $L \cap U$ is convex; we do the extension in this one dimensional case. But I can not prove the continuity of the extended function, nor can I prove the extension is irrelevant to choice of $x_0$. I know $f$ is locally Lipschitz; perhaps it can help in some way.","Suppose $U$ is a convex open set in $\mathbb R^n$, and $f$ is a bounded convex function on $U$. $f$ is then automatically continuous on $U$. The question is: Can we always extend $f$ continuously to $\bar{U}$? I know boundedness of $f$ is necessary, otherwise we can construct $g=1/[x-x^2]$ on unit open interval. Boundedness of $f$ is also sufficient in one dimensional case. If the extension in general exists, the only reasonable way to construct it is via: find a fixed point $x_0$ in $U$; for each point x on $\partial U$, consider the line $L$ joining $x_0$ and $x$; $f$ restricting on $L \cap U$ is convex; we do the extension in this one dimensional case. But I can not prove the continuity of the extended function, nor can I prove the extension is irrelevant to choice of $x_0$. I know $f$ is locally Lipschitz; perhaps it can help in some way.",,"['analysis', 'convex-analysis']"
76,The 2-norm of the integral vs the integral of the 2-norm,The 2-norm of the integral vs the integral of the 2-norm,,"I`m currently having some issues with a seemingly innocent problem. I would like to show that $$\Bigg\|\int_\mathbb{R}\begin{pmatrix}A(x)\\B(x)\end{pmatrix}dx\Bigg\|_2 \leq \int_{\mathbb{R}}\Bigg\|\begin{pmatrix}A(x)\\B(x)\end{pmatrix}\Bigg\|_2dx$$ Where $A(x),B(x) \in L^2(\mathbb{R})$ and the two norm is defined as $$\Bigg\|\begin{pmatrix}A(x)\\B(x)\end{pmatrix}\Bigg\|_2=\sqrt{|A(x)|^2+|B(x)|^2}$$ I've asked around and people have tended to say ""that's very simple"" and then spent half an hour staring at it. I've tried plugging stuff in and it seems to hold but I do need a proof. Any help would be much appreciated! Thanks in advance","I`m currently having some issues with a seemingly innocent problem. I would like to show that Where and the two norm is defined as I've asked around and people have tended to say ""that's very simple"" and then spent half an hour staring at it. I've tried plugging stuff in and it seems to hold but I do need a proof. Any help would be much appreciated! Thanks in advance","\Bigg\|\int_\mathbb{R}\begin{pmatrix}A(x)\\B(x)\end{pmatrix}dx\Bigg\|_2 \leq \int_{\mathbb{R}}\Bigg\|\begin{pmatrix}A(x)\\B(x)\end{pmatrix}\Bigg\|_2dx A(x),B(x) \in L^2(\mathbb{R}) \Bigg\|\begin{pmatrix}A(x)\\B(x)\end{pmatrix}\Bigg\|_2=\sqrt{|A(x)|^2+|B(x)|^2}","['analysis', 'normed-spaces']"
77,What are the disadvantages of non-standard analysis?,What are the disadvantages of non-standard analysis?,,"Most students are first taught standard analysis, and the might learn about NSA later on. However, what has kept NSA from becoming the standard? At least from what I've been told, it is much more intuitive than the standard. So why has it not been adopted as the mainstream analysis, especially for lower-level students?","Most students are first taught standard analysis, and the might learn about NSA later on. However, what has kept NSA from becoming the standard? At least from what I've been told, it is much more intuitive than the standard. So why has it not been adopted as the mainstream analysis, especially for lower-level students?",,"['analysis', 'soft-question', 'intuition', 'math-history', 'nonstandard-analysis']"
78,Notation: What's meant by $C^{\infty}_{0}(\mathbb{R}^{+})$?,Notation: What's meant by ?,C^{\infty}_{0}(\mathbb{R}^{+}),"In Chapter 0 of Iwaniec's Spectral Methods of Automorphic Forms Iwaneic uses the notation $C^{\infty}_{0}(\mathbb{R}^{+})$ without definition. I assume that it's the set of infinitely differentiable functions from $\mathbb{R} \to \mathbb{R}$ with range a subset of the positive reals and which tend toward zero ""sufficiently quickly,"" but I don't know whether my guess is right or what the precise definition of ""sufficiently quickly"" is in this context. What does the subscript of $0$ refer to?","In Chapter 0 of Iwaniec's Spectral Methods of Automorphic Forms Iwaneic uses the notation $C^{\infty}_{0}(\mathbb{R}^{+})$ without definition. I assume that it's the set of infinitely differentiable functions from $\mathbb{R} \to \mathbb{R}$ with range a subset of the positive reals and which tend toward zero ""sufficiently quickly,"" but I don't know whether my guess is right or what the precise definition of ""sufficiently quickly"" is in this context. What does the subscript of $0$ refer to?",,['analysis']
79,"Show that $\sup(A \cup B)=\max\{\sup A,\sup B\}$",Show that,"\sup(A \cup B)=\max\{\sup A,\sup B\}","Let $A,B$ not empty,bounded subsets of $\mathbb{R}$.Show that $$\sup(A \cup B)= \max \{\sup A, \sup B \}.$$ That's what I have done so far: Let $x\in A \cup B \Rightarrow x \in A \text{ or } x\in B \Rightarrow x\leq \sup(A) \text{ or } x\leq \sup(B) \Rightarrow x\leq \max \{\sup(A),\sup(B)\}$. But,how can I continue?","Let $A,B$ not empty,bounded subsets of $\mathbb{R}$.Show that $$\sup(A \cup B)= \max \{\sup A, \sup B \}.$$ That's what I have done so far: Let $x\in A \cup B \Rightarrow x \in A \text{ or } x\in B \Rightarrow x\leq \sup(A) \text{ or } x\leq \sup(B) \Rightarrow x\leq \max \{\sup(A),\sup(B)\}$. But,how can I continue?",,['analysis']
80,Is a set without limit points necessarily closed?,Is a set without limit points necessarily closed?,,"According to the definition on Rudin' Principles of Mathematicial Analysis, closed set is defined as: $E$ is closed if every limit point of $E$ is a point of $E$. Then I have a question: if a set has no limit point, is it necessarily closed? I think this idea doesn't contradict the definition of closed sets. Am I right?  And another question is #10 in Chap.2 of Rudin's: Let $X$ be an infinite set. For $p\in X$ and $q\in X$, define    $d(p,q)=0$ when $p=q$. Otherwises,$d(p,q)=1$. I have known that every set in the metric space is closed. To prove it, we can show that the complement of any set is open, which is quite trivial. My question is: how can we prove that every set is closed just by checking according to the definition of closed set, i.e. how can we show that every limit point of any arbitrary set $E$ in the metric space is a point of $E$? Thanks in advanced!","According to the definition on Rudin' Principles of Mathematicial Analysis, closed set is defined as: $E$ is closed if every limit point of $E$ is a point of $E$. Then I have a question: if a set has no limit point, is it necessarily closed? I think this idea doesn't contradict the definition of closed sets. Am I right?  And another question is #10 in Chap.2 of Rudin's: Let $X$ be an infinite set. For $p\in X$ and $q\in X$, define    $d(p,q)=0$ when $p=q$. Otherwises,$d(p,q)=1$. I have known that every set in the metric space is closed. To prove it, we can show that the complement of any set is open, which is quite trivial. My question is: how can we prove that every set is closed just by checking according to the definition of closed set, i.e. how can we show that every limit point of any arbitrary set $E$ in the metric space is a point of $E$? Thanks in advanced!",,['analysis']
81,"Examples of function sequences in C[0,1] that are Cauchy but not convergent","Examples of function sequences in C[0,1] that are Cauchy but not convergent",,"To better train my intuition, what are some illustrative examples of function sequences in C[0,1] that are Cauchy but do not converge under the integral norm?","To better train my intuition, what are some illustrative examples of function sequences in C[0,1] that are Cauchy but do not converge under the integral norm?",,"['analysis', 'convergence-divergence', 'metric-spaces']"
82,Terence Tao Exercise 5.4.3: Integer part of $x$ proof.,Terence Tao Exercise 5.4.3: Integer part of  proof.,x,"I am reading Terence Tao: Analysis 1.  As you may be aware, certain objects are introduced bit by bit, so if i am not 'allowed' to use something yet, please understand. Show that for every real number $x$ there is exactly one integer $N$ such that $ N \leq x < N + 1$ If $x = 0$ then we can take $N = 0$.  If $x > 0$ then $x$ is the formal limit of some Cauchy sequence $(a_n)$, of which is positively bounded away from zero. Since $(a_n)$ is Cauchy it is bounded by some rational $M$, this implies that $x \leq M$, if we take $M < N + 1$ for some integer $N$ then $x < N + 1$. I seem to think this part is fine, but thinking is dangerous. I now need to show that $ N \leq x $. I know that the sequence $(a_n)$ is bounded away from zero, so every term of the sequence $a_n > c$ for some rational $c$ But i don't think this gets me any further. Surely i could use this as i know x is the formal limit of the sequence of rationals $(a_n)$ but i cant quite make the connection. EDIT: I know i have to prove that this is true when x < 0, but i have not completed from x > 0. I know i can use Cauchy sequences, division algorithm... its pretty hard for me to say what i can't use because I am an undergraduate. I'm sorry if that makes it impossible to answer for some, it's very awkward i must admit. What i can tell you is the next chapter is on the least upper bound property.","I am reading Terence Tao: Analysis 1.  As you may be aware, certain objects are introduced bit by bit, so if i am not 'allowed' to use something yet, please understand. Show that for every real number $x$ there is exactly one integer $N$ such that $ N \leq x < N + 1$ If $x = 0$ then we can take $N = 0$.  If $x > 0$ then $x$ is the formal limit of some Cauchy sequence $(a_n)$, of which is positively bounded away from zero. Since $(a_n)$ is Cauchy it is bounded by some rational $M$, this implies that $x \leq M$, if we take $M < N + 1$ for some integer $N$ then $x < N + 1$. I seem to think this part is fine, but thinking is dangerous. I now need to show that $ N \leq x $. I know that the sequence $(a_n)$ is bounded away from zero, so every term of the sequence $a_n > c$ for some rational $c$ But i don't think this gets me any further. Surely i could use this as i know x is the formal limit of the sequence of rationals $(a_n)$ but i cant quite make the connection. EDIT: I know i have to prove that this is true when x < 0, but i have not completed from x > 0. I know i can use Cauchy sequences, division algorithm... its pretty hard for me to say what i can't use because I am an undergraduate. I'm sorry if that makes it impossible to answer for some, it's very awkward i must admit. What i can tell you is the next chapter is on the least upper bound property.",,[]
83,Sum with sine in denominator,Sum with sine in denominator,,Here is some sum that I can guess an answer but somehow lack of nice solution. $$\displaystyle\sum _{k=0}^{n-2} (-1)^k\frac{1}{\sin\left(\frac{(2k+1)\pi}{4n-2}\right)}=\;?$$,Here is some sum that I can guess an answer but somehow lack of nice solution. $$\displaystyle\sum _{k=0}^{n-2} (-1)^k\frac{1}{\sin\left(\frac{(2k+1)\pi}{4n-2}\right)}=\;?$$,,['analysis']
84,Continuity of one partial derivative implies differentiability,Continuity of one partial derivative implies differentiability,,Let $f: \mathbb{R}^2 \rightarrow \mathbb{R}$ be a function such that the partial derivatives with respect to $x$ and $y$ exist and one of them is continuous. Prove that $f$ is differentiable.,Let $f: \mathbb{R}^2 \rightarrow \mathbb{R}$ be a function such that the partial derivatives with respect to $x$ and $y$ exist and one of them is continuous. Prove that $f$ is differentiable.,,"['analysis', 'multivariable-calculus', 'derivatives']"
85,What is $\int_0^\infty \ln \left(1-\frac{\sin x}{x} \right) dx $?,What is ?,\int_0^\infty \ln \left(1-\frac{\sin x}{x} \right) dx ,"I'm looking for closed-form expressions for the integral $$I:=\int_0^\infty \ln \left(1-\frac{\sin x}{x} \right) dx .$$ Some related integrals that I've found include : $$J:=-\int_0^\infty \log(1-\cosh(x))\frac{x^2}{e^x}\,dx = 6 + 2\ln(2) - {2\pi^{2} \over 3} - 4\zeta(3) - 2\pi i,$$ and this one: $$K:= \int_0^{\frac{\pi}{2}}\log\Big{(}\cos(x)\Big{)}dx = - \frac{\pi}{2} \log(2) ,$$ and (p. 530) $$L:=\int_{0}^{\infty} \big{(} \ln(1-e^{-x}) \big{)}  dx = - \frac{\pi^{2}}{6} .$$ Moreover, I've encountered various integrals of the sinc function, including : $$M:= \int_{0}^{\pi/2} \ln \bigg{(} \frac{\sin(x)}{x} \bigg{)} dx = \frac{\pi}{2} \Big{(} 1 - \ln(\pi) \Big{)} .$$ However, I haven't found any closed-form expressions for $I$ yet, only an approximation: $$I \approx -5.75555891011162780816$$ and I'm not sure how to proceed with the integral.","I'm looking for closed-form expressions for the integral Some related integrals that I've found include : and this one: and (p. 530) Moreover, I've encountered various integrals of the sinc function, including : However, I haven't found any closed-form expressions for yet, only an approximation: and I'm not sure how to proceed with the integral.","I:=\int_0^\infty \ln \left(1-\frac{\sin x}{x} \right) dx . J:=-\int_0^\infty \log(1-\cosh(x))\frac{x^2}{e^x}\,dx = 6 + 2\ln(2) - {2\pi^{2} \over 3} - 4\zeta(3) - 2\pi i, K:= \int_0^{\frac{\pi}{2}}\log\Big{(}\cos(x)\Big{)}dx = - \frac{\pi}{2} \log(2) , L:=\int_{0}^{\infty} \big{(} \ln(1-e^{-x}) \big{)}  dx = - \frac{\pi^{2}}{6} . M:= \int_{0}^{\pi/2} \ln \bigg{(} \frac{\sin(x)}{x} \bigg{)} dx = \frac{\pi}{2} \Big{(} 1 - \ln(\pi) \Big{)} . I I \approx -5.75555891011162780816","['analysis', 'definite-integrals', 'trigonometric-integrals']"
86,Why is volume of a high-D ball concentrated near its surface?,Why is volume of a high-D ball concentrated near its surface?,,"I came across the following sentence while reading a book on applied math: Volume of a high dimensional unit ball is concentrated near its   surface and is also concentrated at its equator. This is from book's introduction, and I believe the sentence will be explained at some later point in the book. However, it is hard to me to accept it on intuition level. Can some of you explain this sentence to me, in a sort of laymen-style, if possible?","I came across the following sentence while reading a book on applied math: Volume of a high dimensional unit ball is concentrated near its   surface and is also concentrated at its equator. This is from book's introduction, and I believe the sentence will be explained at some later point in the book. However, it is hard to me to accept it on intuition level. Can some of you explain this sentence to me, in a sort of laymen-style, if possible?",,['analysis']
87,Definition of weak solutions from geometrical point of view,Definition of weak solutions from geometrical point of view,,"Why are weak solutions defined like: A function $u \in H^1(\Omega)$ is a weak solution of  $$ Lu:=\operatorname{div}(A\nabla u)+b\cdot\nabla u+cu=f+\operatorname{div}F,   \;\text{in } \Omega $$ if $$ \int_\Omega \nabla \phi\cdot (A\nabla u-F)dx=\int_\Omega\phi(b\cdot \nabla u+cu-f) dx  $$ holds for every $\phi \in C_0^\infty(\Omega).$ I mean, we define this new notion ""weak solutions"" in order to generalize classical solutions. Then we just need one purpose which is ""classical solution $\Rightarrow$ weak solution"" and if everything is sufficiently smooth, then it follows that a weak solution is automatically a classical one. If this is our motivation to define weak solutions, then there would have tons of methods to do. My point is why this type of definition of weak solutions is so important. Is there any other motivations like from geometry or other subjects to make this definition so outstanding and is there any other type of definitions of generalized solutions?","Why are weak solutions defined like: A function $u \in H^1(\Omega)$ is a weak solution of  $$ Lu:=\operatorname{div}(A\nabla u)+b\cdot\nabla u+cu=f+\operatorname{div}F,   \;\text{in } \Omega $$ if $$ \int_\Omega \nabla \phi\cdot (A\nabla u-F)dx=\int_\Omega\phi(b\cdot \nabla u+cu-f) dx  $$ holds for every $\phi \in C_0^\infty(\Omega).$ I mean, we define this new notion ""weak solutions"" in order to generalize classical solutions. Then we just need one purpose which is ""classical solution $\Rightarrow$ weak solution"" and if everything is sufficiently smooth, then it follows that a weak solution is automatically a classical one. If this is our motivation to define weak solutions, then there would have tons of methods to do. My point is why this type of definition of weak solutions is so important. Is there any other motivations like from geometry or other subjects to make this definition so outstanding and is there any other type of definitions of generalized solutions?",,"['analysis', 'differential-geometry', 'partial-differential-equations', 'intuition', 'sobolev-spaces']"
88,Does the boundary of an open set have measure zero (in $\mathbb{R}^n$)?,Does the boundary of an open set have measure zero (in )?,\mathbb{R}^n,"When studying weak border conditions (in Sobolev Spaces), the usual motivation for the weak meaning of inequalities is that the boundary of most open sets in $\mathbb{R}^n$ has zero Lebesgue measure. But is there any open set $U\subseteq\mathbb{R}^n$ such that it's boundary has positive Lebesgue measure? I really can't think of anything like that.","When studying weak border conditions (in Sobolev Spaces), the usual motivation for the weak meaning of inequalities is that the boundary of most open sets in $\mathbb{R}^n$ has zero Lebesgue measure. But is there any open set $U\subseteq\mathbb{R}^n$ such that it's boundary has positive Lebesgue measure? I really can't think of anything like that.",,"['analysis', 'measure-theory']"
89,How is analysis beautiful? -- confusion from an algebraist [closed],How is analysis beautiful? -- confusion from an algebraist [closed],,"Closed . This question is opinion-based . It is not currently accepting answers. Want to improve this question? Update the question so it can be answered with facts and citations by editing this post . Closed 3 years ago . Improve this question As a math major about to go into grad school, I find the algebra-side of mathematics beautiful and inspiring -- I like to explore the hidden structure of things. I also find the geometry/topology interesting as they bring intuition to something we can visualize. Yet I can't feel the beauty of analysis but only its difficulty to visualize and its complicated ad-hoc techniques for manipulating the epsilons. I couldn't find satisfactory answers online. Most math educators seem to like algebra (or more easy-to-explain thing in general). It just seems hard to tell the big picture of the analysis. On others sites reddit and quora , I have mostly seen evidence for why people love algebra, but little evidence for why people love analysis. (possible exceptions may be Riemannian geometry/complex manifolds but I know very little about the details) For what it's worth, I am also physics/string theory inclined and know that much of theoretical physics that's hard to experimentally verify is driven by ""mathematical appeal"". I have not studied much analysis beyond measure theory. I would like to invite experts in analysis or people who have had any inspiring analysis courses to share their excitement. (You are also welcome to share why you hated analysis if you really want to. I just watched 3B1B's monster group video and felt more excited about algebra) More specifically, can you share results from analysis, which provide a deeper understanding of the underlying structures analysts work with? I am trying to get a sense of the beauty of analysis, and struggling to do so because it all seems very ad-hoc. Sorry if this question seems too opinion-based. But I believe the answer is illuminating to many rising math students. Technically, this post belongs to ""Constructive subjective questions"" so it should be reopened. If you also think so, you can vote for ""reopen"" below.","Closed . This question is opinion-based . It is not currently accepting answers. Want to improve this question? Update the question so it can be answered with facts and citations by editing this post . Closed 3 years ago . Improve this question As a math major about to go into grad school, I find the algebra-side of mathematics beautiful and inspiring -- I like to explore the hidden structure of things. I also find the geometry/topology interesting as they bring intuition to something we can visualize. Yet I can't feel the beauty of analysis but only its difficulty to visualize and its complicated ad-hoc techniques for manipulating the epsilons. I couldn't find satisfactory answers online. Most math educators seem to like algebra (or more easy-to-explain thing in general). It just seems hard to tell the big picture of the analysis. On others sites reddit and quora , I have mostly seen evidence for why people love algebra, but little evidence for why people love analysis. (possible exceptions may be Riemannian geometry/complex manifolds but I know very little about the details) For what it's worth, I am also physics/string theory inclined and know that much of theoretical physics that's hard to experimentally verify is driven by ""mathematical appeal"". I have not studied much analysis beyond measure theory. I would like to invite experts in analysis or people who have had any inspiring analysis courses to share their excitement. (You are also welcome to share why you hated analysis if you really want to. I just watched 3B1B's monster group video and felt more excited about algebra) More specifically, can you share results from analysis, which provide a deeper understanding of the underlying structures analysts work with? I am trying to get a sense of the beauty of analysis, and struggling to do so because it all seems very ad-hoc. Sorry if this question seems too opinion-based. But I believe the answer is illuminating to many rising math students. Technically, this post belongs to ""Constructive subjective questions"" so it should be reopened. If you also think so, you can vote for ""reopen"" below.",,"['analysis', 'soft-question', 'riemannian-geometry', 'mathematical-physics', 'learning']"
90,Convergence of finite differences to zero and polynomials,Convergence of finite differences to zero and polynomials,,"Assume that $f:\mathbb R \rightarrow \mathbb R$ is continuous and $h\in \mathbb R$.  Let $\Delta_h^n f(x)$ be a finite difference of $f$ of order $n$, i.e $$ \Delta_h^1 f(x)=f(x+h)-f(x), $$ $$ \Delta_h^2f(x)=\Delta_h^1f(x+h)-\Delta_h^1 f(x)=f(x+2h)-2f(x+h)+f(x), $$ $$ \Delta_h^3 f(x)=\Delta_h^2f(x+h)-\Delta_h^2f(x)=f(x+3h)-3f(x+2h)+3f(x+h)-f(x), $$ etc. There is an explicite formula for $n$-th difference:  $$ \Delta_h^n f(x)=\sum_{k=0}^n (-1)^{n-k}\frac{n!}{k!(n-k)!} f(x+kh). $$ Assume now that $n\in \mathbb N$ and  $f:\mathbb R \rightarrow \mathbb R$ are such that for each $x \in \mathbb R$: $$ \frac{\Delta_h^n f(x)}{h^n} \rightarrow 0 \textrm{ as } h \rightarrow 0. $$ Is it then $f$ a polynomial of degree $\leq n-1$? It is clear if $n=1$, because then $f'(x)=0$ for $x\in \mathbb R$. Edit. Without continuity assumption about $f$ it is not true, because for $n-1$-additive function $F$ which is not $n-1$-linear we have $\Delta_h^nf(x)=0$, where $f(x)=F(x,...,x)$.","Assume that $f:\mathbb R \rightarrow \mathbb R$ is continuous and $h\in \mathbb R$.  Let $\Delta_h^n f(x)$ be a finite difference of $f$ of order $n$, i.e $$ \Delta_h^1 f(x)=f(x+h)-f(x), $$ $$ \Delta_h^2f(x)=\Delta_h^1f(x+h)-\Delta_h^1 f(x)=f(x+2h)-2f(x+h)+f(x), $$ $$ \Delta_h^3 f(x)=\Delta_h^2f(x+h)-\Delta_h^2f(x)=f(x+3h)-3f(x+2h)+3f(x+h)-f(x), $$ etc. There is an explicite formula for $n$-th difference:  $$ \Delta_h^n f(x)=\sum_{k=0}^n (-1)^{n-k}\frac{n!}{k!(n-k)!} f(x+kh). $$ Assume now that $n\in \mathbb N$ and  $f:\mathbb R \rightarrow \mathbb R$ are such that for each $x \in \mathbb R$: $$ \frac{\Delta_h^n f(x)}{h^n} \rightarrow 0 \textrm{ as } h \rightarrow 0. $$ Is it then $f$ a polynomial of degree $\leq n-1$? It is clear if $n=1$, because then $f'(x)=0$ for $x\in \mathbb R$. Edit. Without continuity assumption about $f$ it is not true, because for $n-1$-additive function $F$ which is not $n-1$-linear we have $\Delta_h^nf(x)=0$, where $f(x)=F(x,...,x)$.",,['analysis']
91,Essential Supremum vs. Uniform norm,Essential Supremum vs. Uniform norm,,"I just went to check something about the $||\cdot||_\infty$ norm and realized that it can perhaps refer to two quite different things. I'm coming at this from an Analysis class so I am use to having $||f||_\infty = ess sup |f(t)|$. But, according to wikipedia ( http://en.wikipedia.org/wiki/Supremum_norm ) the uniform norm is the same notation but is defined as $||f||_\infty = sup |f(t)|$. Unless I am really missing something, these are not equivalent. I'm a little uncomfortable with the possible ambiguity. How does one know which definition is actually being used in any one case? It seems like it is safe to say it is ess sup if anything explicitly references the $L^\infty$ space. I guesss I'm not sure what to ask. Can you verify that these are not necessarily equivalent? How do you differentiate which is being referred to?","I just went to check something about the $||\cdot||_\infty$ norm and realized that it can perhaps refer to two quite different things. I'm coming at this from an Analysis class so I am use to having $||f||_\infty = ess sup |f(t)|$. But, according to wikipedia ( http://en.wikipedia.org/wiki/Supremum_norm ) the uniform norm is the same notation but is defined as $||f||_\infty = sup |f(t)|$. Unless I am really missing something, these are not equivalent. I'm a little uncomfortable with the possible ambiguity. How does one know which definition is actually being used in any one case? It seems like it is safe to say it is ess sup if anything explicitly references the $L^\infty$ space. I guesss I'm not sure what to ask. Can you verify that these are not necessarily equivalent? How do you differentiate which is being referred to?",,['analysis']
92,Extensions of measurable functions,Extensions of measurable functions,,"So sorry to bother y'all, I'm on a bit of a deadline and am not an expert on this and can't find the reference. So it turns out that a measurable function defined on a subset $U$ of a measurable space $X$ can be extended to a measurable function on the whole space by defining it to be $0$ on $X\setminus U$. I needed the reference for this, anyone? Thanks","So sorry to bother y'all, I'm on a bit of a deadline and am not an expert on this and can't find the reference. So it turns out that a measurable function defined on a subset $U$ of a measurable space $X$ can be extended to a measurable function on the whole space by defining it to be $0$ on $X\setminus U$. I needed the reference for this, anyone? Thanks",,"['analysis', 'measure-theory']"
93,Pointwise but not uniform convergence of a Fourier series,Pointwise but not uniform convergence of a Fourier series,,"What is an example of a continuous , or even better, differentiable, $2\pi$ (or 1) periodic function whose Fourier series converges pointwise but not uniformly? (Such function cannot be of Hölder class, or absolutely continuous.)","What is an example of a continuous , or even better, differentiable, $2\pi$ (or 1) periodic function whose Fourier series converges pointwise but not uniformly? (Such function cannot be of Hölder class, or absolutely continuous.)",,"['analysis', 'fourier-analysis', 'fourier-series']"
94,Is power set of a power set of a set equal to the power set of the same set?,Is power set of a power set of a set equal to the power set of the same set?,,"I have to decide whether this statement is true, I think it is not. Since the power set of a set with cardinality $n$, will have $2^n$ subsets, however the power set of this set will include the subsets themselves and subsets of the subsets.","I have to decide whether this statement is true, I think it is not. Since the power set of a set with cardinality $n$, will have $2^n$ subsets, however the power set of this set will include the subsets themselves and subsets of the subsets.",,[]
95,Fourier transform of the characteristic function,Fourier transform of the characteristic function,,"My qustion is about the Fourier transform of the characteristic function $\chi_{[0,1]}$. How can I find what it is? The problem is I got something really messy, so I think I didn't get it right.","My qustion is about the Fourier transform of the characteristic function $\chi_{[0,1]}$. How can I find what it is? The problem is I got something really messy, so I think I didn't get it right.",,"['analysis', 'fourier-analysis']"
96,Euler-Poisson-Darboux equation - PDE Evans,Euler-Poisson-Darboux equation - PDE Evans,,"suppose $n \geq 2,\;m \geq 2$ and $u \in C^{m}(\mathbb{R}^n \times[0,+\infty))$ solves the following IVP : \begin{cases} u_{tt} - \Delta u = 0 \; & \text{in} \; \mathbb{R}^n \times(0,+\infty) \\ u = g, \; u_t = h \; & \text{on} \;  \mathbb{R}^n \times\{t = 0\} \end{cases} let $x \in \mathbb{R}^n, \; t > 0, \; r > 0 $. Define \begin{align} U(x;r,t)  & = \frac{1}{n\alpha(n)r^{n-1}}\int_{\partial B(x,r)}u(y,t)dS(y) \\ G(x;r)  & = \frac{1}{n\alpha(n)r^{n-1}}\int_{\partial B(x,r)}g(y)dS(y) \\ H(x;r)  & = \frac{1}{n\alpha(n)r^{n-1}}\int_{\partial B(x,r)}h(y)dS(y) \\ \end{align} $$\alpha(n) = \frac{\pi^{\frac{n}{2}}}{\Gamma(\frac{n}{2}+ 1)} = \text{volume of the unit ball in } \mathbb{R}^n $$ (Euler-Poisson-Darboux equation) fix $x$ in $\mathbb{R}^n$ and let $u$ satisfies the IVP above. then : \begin{cases} U_{tt} - U_{rr} - \frac{n-1}{r}U_r= 0 \; & \text{in} \; \mathbb{R}_{+}\times(0,+\infty) \\ U = G, \; U_t = H \; & \text{on} \;  \mathbb{R}_{+}\times\{t = 0\} \end{cases} the author states that : \begin{align} U_r(x;,r,t)  & = \frac{1}{n\alpha(n)r^{n-1}}\int_{ B(x,r)}\Delta u(y,t)dy \\ U_{rr}(x;,r,t)  & = \frac{1}{n\alpha(n)r^{n-1}}\int_{ \partial B(x,r)}\Delta u(y,t)dS(y)  +(\frac1n - 1)(\frac{1}{\alpha(n)r^n})\int_{ B(x,r)}\Delta u(y,t)dy \\ \end{align} I could derive $U_r$ on my own but I'm having trouble trying to compute $U_{rr}$ if I differentiate $U_r$ with respect to $r$ I get : \begin{align} U_{rr}(x;,r,t)  & = \frac{1}{n\alpha(n)r^n}\int_{ B(x,r)}\Delta u(y,t)dy + \frac{r}{n} [\frac{1}{\alpha(n)r^n}\int_{ B(x,r)}\Delta u(y,t)dy]'_{r} \\ \end{align} this means we must have : \begin{align} \frac{r}{n}[\frac{1}{\alpha(n)r^n}\int_{ B(x,r)}\Delta u(y,t)dy]'_{r} & =  \frac{1}{n\alpha(n)r^{n-1}}\int_{ \partial B(x,r)}\Delta u(y,t)dS(y)  -\frac{1}{\alpha(n)r^n}\int_{ B(x,r)}\Delta u(y,t)dy    \\ \end{align} how do you prove this ?","suppose $n \geq 2,\;m \geq 2$ and $u \in C^{m}(\mathbb{R}^n \times[0,+\infty))$ solves the following IVP : \begin{cases} u_{tt} - \Delta u = 0 \; & \text{in} \; \mathbb{R}^n \times(0,+\infty) \\ u = g, \; u_t = h \; & \text{on} \;  \mathbb{R}^n \times\{t = 0\} \end{cases} let $x \in \mathbb{R}^n, \; t > 0, \; r > 0 $. Define \begin{align} U(x;r,t)  & = \frac{1}{n\alpha(n)r^{n-1}}\int_{\partial B(x,r)}u(y,t)dS(y) \\ G(x;r)  & = \frac{1}{n\alpha(n)r^{n-1}}\int_{\partial B(x,r)}g(y)dS(y) \\ H(x;r)  & = \frac{1}{n\alpha(n)r^{n-1}}\int_{\partial B(x,r)}h(y)dS(y) \\ \end{align} $$\alpha(n) = \frac{\pi^{\frac{n}{2}}}{\Gamma(\frac{n}{2}+ 1)} = \text{volume of the unit ball in } \mathbb{R}^n $$ (Euler-Poisson-Darboux equation) fix $x$ in $\mathbb{R}^n$ and let $u$ satisfies the IVP above. then : \begin{cases} U_{tt} - U_{rr} - \frac{n-1}{r}U_r= 0 \; & \text{in} \; \mathbb{R}_{+}\times(0,+\infty) \\ U = G, \; U_t = H \; & \text{on} \;  \mathbb{R}_{+}\times\{t = 0\} \end{cases} the author states that : \begin{align} U_r(x;,r,t)  & = \frac{1}{n\alpha(n)r^{n-1}}\int_{ B(x,r)}\Delta u(y,t)dy \\ U_{rr}(x;,r,t)  & = \frac{1}{n\alpha(n)r^{n-1}}\int_{ \partial B(x,r)}\Delta u(y,t)dS(y)  +(\frac1n - 1)(\frac{1}{\alpha(n)r^n})\int_{ B(x,r)}\Delta u(y,t)dy \\ \end{align} I could derive $U_r$ on my own but I'm having trouble trying to compute $U_{rr}$ if I differentiate $U_r$ with respect to $r$ I get : \begin{align} U_{rr}(x;,r,t)  & = \frac{1}{n\alpha(n)r^n}\int_{ B(x,r)}\Delta u(y,t)dy + \frac{r}{n} [\frac{1}{\alpha(n)r^n}\int_{ B(x,r)}\Delta u(y,t)dy]'_{r} \\ \end{align} this means we must have : \begin{align} \frac{r}{n}[\frac{1}{\alpha(n)r^n}\int_{ B(x,r)}\Delta u(y,t)dy]'_{r} & =  \frac{1}{n\alpha(n)r^{n-1}}\int_{ \partial B(x,r)}\Delta u(y,t)dS(y)  -\frac{1}{\alpha(n)r^n}\int_{ B(x,r)}\Delta u(y,t)dy    \\ \end{align} how do you prove this ?",,"['analysis', 'partial-differential-equations']"
97,"In general, when does it hold that $f(\sup(X)) = \sup f(X)$?","In general, when does it hold that ?",f(\sup(X)) = \sup f(X),"Let $f: [-\infty, \infty] \to [-\infty, \infty]$ . What conditions should we impose on $f$ so that the following statement becomes true? $$\forall \ X \subset [-\infty, \infty], \sup f(X) = f(\sup X)$$ If that doesn't make much sense, then for some function with certain conditions, what kind of sets $X$ satisfy $f(\sup X) = \sup f(X)$ ? Some background to the question: While doing a certain proof, I was about to swap $\sqrt{\cdot}$ and $\sup$ , but I soon realized that such a step probably needs some scrutiny. I still do not know whether such a step is valid, and I would like to know what sort of functions satisfy the requirement. I supposed that $f$ is an extended real valued function for the possibility of $\sup = \infty$ .","Let . What conditions should we impose on so that the following statement becomes true? If that doesn't make much sense, then for some function with certain conditions, what kind of sets satisfy ? Some background to the question: While doing a certain proof, I was about to swap and , but I soon realized that such a step probably needs some scrutiny. I still do not know whether such a step is valid, and I would like to know what sort of functions satisfy the requirement. I supposed that is an extended real valued function for the possibility of .","f: [-\infty, \infty] \to [-\infty, \infty] f \forall \ X \subset [-\infty, \infty], \sup f(X) = f(\sup X) X f(\sup X) = \sup f(X) \sqrt{\cdot} \sup f \sup = \infty",['analysis']
98,"If a Laplacian eigenfunction is zero in an open set, is it identically zero?","If a Laplacian eigenfunction is zero in an open set, is it identically zero?",,"This seems like it should be easy but I can't seem to figure it out. Suppose $f$ satisfies $\Delta f + \lambda f = 0$ in $\mathbb{R}^n$ (where $\lambda > 0$ is a constant), and in some open set $U$ we have $f = 0$. Does this mean $f = 0$ in $\mathbb{R}^n$? My idea for solving it: Suppose not; WLOG suppose $U$ is a maximal open set ordered by inclusion in which $f = 0$. Let $x \in \partial U$. If $x$ has a neighborhood where $f \le 0$, then $\Delta f \ge 0$ there, so $f$ is subharmonic and by the maximum principle, $f = 0$ in the neighborhood, contradicting the maximality of $U$. If $x$ has a neighborhood where $f \ge 0$ then the same argument applies to $-f$ (or the same argument with ""superharmonic"" and ""minimum principle""). Here is where I'm stuck: How do I rule out a third possibility, where every neighborhood of $x$ contains positive and negative points?","This seems like it should be easy but I can't seem to figure it out. Suppose $f$ satisfies $\Delta f + \lambda f = 0$ in $\mathbb{R}^n$ (where $\lambda > 0$ is a constant), and in some open set $U$ we have $f = 0$. Does this mean $f = 0$ in $\mathbb{R}^n$? My idea for solving it: Suppose not; WLOG suppose $U$ is a maximal open set ordered by inclusion in which $f = 0$. Let $x \in \partial U$. If $x$ has a neighborhood where $f \le 0$, then $\Delta f \ge 0$ there, so $f$ is subharmonic and by the maximum principle, $f = 0$ in the neighborhood, contradicting the maximality of $U$. If $x$ has a neighborhood where $f \ge 0$ then the same argument applies to $-f$ (or the same argument with ""superharmonic"" and ""minimum principle""). Here is where I'm stuck: How do I rule out a third possibility, where every neighborhood of $x$ contains positive and negative points?",,"['analysis', 'partial-differential-equations', 'harmonic-functions']"
99,"How to prove that exists distinct $x_1,x_2 \in(a,b)$ such that $f '(x_1)f '(x_2)=1$?",How to prove that exists distinct  such that ?,"x_1,x_2 \in(a,b) f '(x_1)f '(x_2)=1","Assume $f:[a,b]\to[a,b]$ be continuous and differentiable on $(a,b)$ and $f(a)=a$, $f(b)=b$. How to prove that exists distinct $x_1,x_2 \in(a,b)$  such that $f '(x_1)f '(x_2)=1$? Thanks in advance.","Assume $f:[a,b]\to[a,b]$ be continuous and differentiable on $(a,b)$ and $f(a)=a$, $f(b)=b$. How to prove that exists distinct $x_1,x_2 \in(a,b)$  such that $f '(x_1)f '(x_2)=1$? Thanks in advance.",,"['analysis', 'contest-math']"
