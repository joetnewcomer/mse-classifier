,title_raw,title_text,title_latex,body_raw,body_text,body_latex,tags
0,Matrix Algebra Question (Linear Algebra),Matrix Algebra Question (Linear Algebra),,"Find all values of $a$ such that $A^3 = 2A$, where $$A = \begin{bmatrix} -2 & 2 \\ -1 & a \end{bmatrix}.$$ The matrix I got for $A^3$ at the end didn't match up, but I probably made a multiplication mistake somewhere.","Find all values of $a$ such that $A^3 = 2A$, where $$A = \begin{bmatrix} -2 & 2 \\ -1 & a \end{bmatrix}.$$ The matrix I got for $A^3$ at the end didn't match up, but I probably made a multiplication mistake somewhere.",,['linear-algebra']
1,Intuition behind determinant of a matrix with $2$ equal rows [closed],Intuition behind determinant of a matrix with  equal rows [closed],2,"Closed. This question does not meet Mathematics Stack Exchange guidelines . It is not currently accepting answers. Please provide additional context , which ideally explains why the question is relevant to you and our  community. Some forms of context include: background and motivation, relevant definitions, source, possible strategies, your current progress, why the question is interesting or important, etc. Closed 5 years ago . Improve this question In my linear algebra course, we have just proved that if a matrix $A$ contains $2$ equal rows, then $\det(A)=0$ . I understand how the proof works, but could somebody offer a more intuitive explanation of why this is the case?","Closed. This question does not meet Mathematics Stack Exchange guidelines . It is not currently accepting answers. Please provide additional context , which ideally explains why the question is relevant to you and our  community. Some forms of context include: background and motivation, relevant definitions, source, possible strategies, your current progress, why the question is interesting or important, etc. Closed 5 years ago . Improve this question In my linear algebra course, we have just proved that if a matrix contains equal rows, then . I understand how the proof works, but could somebody offer a more intuitive explanation of why this is the case?",A 2 \det(A)=0,"['linear-algebra', 'determinant', 'intuition']"
2,"Given that it appears impossible to make the set out of linear combinations of its elements, why is it still dependent?","Given that it appears impossible to make the set out of linear combinations of its elements, why is it still dependent?",,"The answer key says the following set of functions is linearly dependent: $\{5, \cos^2x, \sin^2x\}$. Without calculating the Wronskian, I would've guessed it was independent because there's apparently no way you can form a linear combination out of any of these functions to get others: you can't multiply $5$ to get $\cos x$; you can't multiply $\cos x$ to get $\sin x$, etc. What's wrong with my reasoning?","The answer key says the following set of functions is linearly dependent: $\{5, \cos^2x, \sin^2x\}$. Without calculating the Wronskian, I would've guessed it was independent because there's apparently no way you can form a linear combination out of any of these functions to get others: you can't multiply $5$ to get $\cos x$; you can't multiply $\cos x$ to get $\sin x$, etc. What's wrong with my reasoning?",,"['calculus', 'linear-algebra']"
3,Is there any matrix $2\times 2$ such that $A\neq I$ but $ A^3=I$?,Is there any matrix  such that  but ?,2\times 2 A\neq I  A^3=I,Is there any $2 \times 2$ matrix $A \neq I$ such that $A^3=I$ ? In my opinion: no. Thank you very much.,Is there any matrix such that ? In my opinion: no. Thank you very much.,2 \times 2 A \neq I A^3=I,"['linear-algebra', 'matrices', 'matrix-equations']"
4,Eigenvalues and eigenvectors in physics?,Eigenvalues and eigenvectors in physics?,,How can we physically interpret an eigenvalue or an eigenvector in linear algebra?,How can we physically interpret an eigenvalue or an eigenvector in linear algebra?,,"['linear-algebra', 'eigenvalues-eigenvectors']"
5,A Five Equations problem?,A Five Equations problem?,,"If, $$\begin{align*}     y+u+x+v&=0\\     z+y+v+u&=1\\     x+y+z+u&=5\\     z+u+v+x&=2\\     v+x+y+z&=4\,, \end{align*}$$ What is the value of $xyzuv$?","If, $$\begin{align*}     y+u+x+v&=0\\     z+y+v+u&=1\\     x+y+z+u&=5\\     z+u+v+x&=2\\     v+x+y+z&=4\,, \end{align*}$$ What is the value of $xyzuv$?",,"['linear-algebra', 'algebra-precalculus', 'systems-of-equations']"
6,Prove Or Disprove: tr(AB)=tr(A)*tr(B),Prove Or Disprove: tr(AB)=tr(A)*tr(B),,$\mathrm{tr}(AB)=\sum\limits_{i=1}^n \sum\limits_{j=1}^n a_{ij}*b_{ji}$ $\mathrm{tr}(A)*\mathrm{tr}(B)=\sum\limits_{i=1}^n a_{ii}*\sum\limits_{i=1}^n b_{ii}$ Therefore $\mathrm{tr}(AB) \neq \mathrm{tr}(A)*\mathrm{tr}(B)$ Is the proof valid?,$\mathrm{tr}(AB)=\sum\limits_{i=1}^n \sum\limits_{j=1}^n a_{ij}*b_{ji}$ $\mathrm{tr}(A)*\mathrm{tr}(B)=\sum\limits_{i=1}^n a_{ii}*\sum\limits_{i=1}^n b_{ii}$ Therefore $\mathrm{tr}(AB) \neq \mathrm{tr}(A)*\mathrm{tr}(B)$ Is the proof valid?,,"['linear-algebra', 'proof-verification']"
7,Eigenvalues of reflection,Eigenvalues of reflection,,"Why are the eigenvalues of a reflection $Rx=\rho x$ in a $n$ -dimensional vector space just $\lambda=-1,1$ ? I can't seem to convince myself of this.",Why are the eigenvalues of a reflection in a -dimensional vector space just ? I can't seem to convince myself of this.,"Rx=\rho x n \lambda=-1,1","['linear-algebra', 'linear-transformations']"
8,How can three vectors be orthogonal to each other?,How can three vectors be orthogonal to each other?,,"In a scenario, say that: Vectors $\mathbf{U}$ , $\mathbf{V}$ and $\mathbf{W}$ are all orthogonal such that the dot product between each of these $(\mathbf{UV}\;\mathbf{VW}\;\mathbf{WU})$ is equal to zero. I imagine that for any potential vector space $\mathbf{R}$ this would only be possible in two situations. 1) $\mathbf{U}$ , $\mathbf{W}$ and/or $\mathbf{V}$ is the zero vector. 2) $\mathbf{U}=(1, 0, 0)$ , $\mathbf{V} = (0, 1, 0)$ and $\mathbf{W} = (0, 0, 1)$ . Is there any other situation where three vectors are all orthogonal to each other?","In a scenario, say that: Vectors , and are all orthogonal such that the dot product between each of these is equal to zero. I imagine that for any potential vector space this would only be possible in two situations. 1) , and/or is the zero vector. 2) , and . Is there any other situation where three vectors are all orthogonal to each other?","\mathbf{U} \mathbf{V} \mathbf{W} (\mathbf{UV}\;\mathbf{VW}\;\mathbf{WU}) \mathbf{R} \mathbf{U} \mathbf{W} \mathbf{V} \mathbf{U}=(1, 0, 0) \mathbf{V} = (0, 1, 0) \mathbf{W} = (0, 0, 1)","['linear-algebra', 'vectors', 'orthogonality']"
9,Linear Algebra - Rank of a matrix,Linear Algebra - Rank of a matrix,,A is a $100 \times  100$ matrix. The element in the $i^{th}$ row and $j^{th}$ column is given by $i^2 + j^2$ Find the rank,A is a $100 \times  100$ matrix. The element in the $i^{th}$ row and $j^{th}$ column is given by $i^2 + j^2$ Find the rank,,"['linear-algebra', 'matrices']"
10,Determine matrix of linear map,Determine matrix of linear map,,Linear map is given through: $\phi\begin{pmatrix} 3 \\ -2 \end{pmatrix} =\begin{pmatrix} -3 \\ -14 \end{pmatrix} $ $\phi\begin{pmatrix} 3 \\ 0 \end{pmatrix} =\begin{pmatrix} -9 \\ -6 \end{pmatrix}$ Determine matrix $A$ linear map. Here I have solution but I dont understand how to get it. $A=\begin{pmatrix} -3 & -3 \\ -2 & 4 \end{pmatrix}  $,Linear map is given through: $\phi\begin{pmatrix} 3 \\ -2 \end{pmatrix} =\begin{pmatrix} -3 \\ -14 \end{pmatrix} $ $\phi\begin{pmatrix} 3 \\ 0 \end{pmatrix} =\begin{pmatrix} -9 \\ -6 \end{pmatrix}$ Determine matrix $A$ linear map. Here I have solution but I dont understand how to get it. $A=\begin{pmatrix} -3 & -3 \\ -2 & 4 \end{pmatrix}  $,,"['calculus', 'linear-algebra', 'matrices']"
11,Find values of $x$ so that the matrix is invertible,Find values of  so that the matrix is invertible,x,"Find values of $x$ so that the matrix is invertible $$A=\begin{pmatrix} x &  0 & x \\ x &  2 &  1 \\ 2x &  0 &   2x \\ \end{pmatrix}$$ I know that a matrix is invertible if determinant is not $0$ , but I don't know how to find the $x$ values. I feel is a tricky question and this matrix will not be invertible no matter which value $x$ takes, but I don't know how to prove that either.","Find values of so that the matrix is invertible I know that a matrix is invertible if determinant is not , but I don't know how to find the values. I feel is a tricky question and this matrix will not be invertible no matter which value takes, but I don't know how to prove that either.","x A=\begin{pmatrix}
x &  0 & x \\
x &  2 &  1 \\
2x &  0 &   2x \\
\end{pmatrix} 0 x x","['linear-algebra', 'matrices', 'inverse']"
12,"Prove that $1, x, x^2, \dots , x^n$ are linearly independent in $C[-1,1]$",Prove that  are linearly independent in,"1, x, x^2, \dots , x^n C[-1,1]","As it states in the title, I'd like to prove that $1, x, x^2, \ldots , x^n$ are linearly independent in $C[-1,1]$. Should I use an induction argument or integrate for $x^m$ and $x^n$ with cases $m=n$ and $m \neq n$? The inner product is $$ \langle f,g \rangle = \int_{-1}^1 f(x)g(x)dx.$$ Do both methods work?","As it states in the title, I'd like to prove that $1, x, x^2, \ldots , x^n$ are linearly independent in $C[-1,1]$. Should I use an induction argument or integrate for $x^m$ and $x^n$ with cases $m=n$ and $m \neq n$? The inner product is $$ \langle f,g \rangle = \int_{-1}^1 f(x)g(x)dx.$$ Do both methods work?",,"['linear-algebra', 'inner-products']"
13,Why are all homogenous systems consistent?,Why are all homogenous systems consistent?,,A linear system of form $A\vec{x}=\vec{0}$ is called homogeneous. Why are all homogenous systems consistent?,A linear system of form $A\vec{x}=\vec{0}$ is called homogeneous. Why are all homogenous systems consistent?,,['linear-algebra']
14,Proof for corresponding eigenvalue,Proof for corresponding eigenvalue,,"If x is an eigenvector of a matrix A, then show that its corresponding eigenvalue is given by $\lambda=\dfrac{Ax\cdot x}{x\cdot x}$ I tried starting from $(A-\lambda{I})x=0$. $Ax-\lambda{Ix}=0$ $\lambda{Ix}=Ax$ $\lambda=\dfrac{Ax}{Ix}$. This now is a bit confusing. Any help?","If x is an eigenvector of a matrix A, then show that its corresponding eigenvalue is given by $\lambda=\dfrac{Ax\cdot x}{x\cdot x}$ I tried starting from $(A-\lambda{I})x=0$. $Ax-\lambda{Ix}=0$ $\lambda{Ix}=Ax$ $\lambda=\dfrac{Ax}{Ix}$. This now is a bit confusing. Any help?",,['linear-algebra']
15,Uniqueness of additive identity element of vector space (or group or monoid),Uniqueness of additive identity element of vector space (or group or monoid),,"Please rate and comment. I want to improve; constructive criticism is highly appreciated. Please take style into account as well. Proof of uniqueness of identity element of addition of vector space This proof is solely based on vector space axioms. Axiom names are italicised . They are defined in Wikipedia (vector space). Let $V$ be a vector space. We prove the uniqueness of an identity element of addition (IEOA). By Identity element of addition , there exists an IEOA. Let this element be denoted by $0$. For the sake of contradiction, we assume that the IEOA is not unique. That is, there exists an IEOA $0'$ such that $0' \ne 0$. Obviously, $V \ne \emptyset$. Let $v \in V$. By Identity element of addition , $v + 0 = v$ and $v + 0' = v$. Hence, $$v + 0 = v + 0'.$$ By Commutativity of addition , $$0 + v = 0' + v.$$ By Inverse elements of addition , there exists an additive inverse of $v$. Let $-v$ denote the additive inverse of $v$. Due to the foregoing equality, $$(0 + v) + (-v) = (0' + v) + (-v).$$ By Associativity of addition , $$0 + (v + (-v)) = 0' + (v + (-v)).$$ By Inverse elements of addition , $$0 + 0 = 0' + 0.$$ By Identity element of addition , $$0 = 0'.$$ The foregoing equality contradicts our assumption. Thus, our assumption is false and its negation is true. That is, the IEOA is unique. QED P.S.: I wrote another version of the proof , which is less roundabout.","Please rate and comment. I want to improve; constructive criticism is highly appreciated. Please take style into account as well. Proof of uniqueness of identity element of addition of vector space This proof is solely based on vector space axioms. Axiom names are italicised . They are defined in Wikipedia (vector space). Let $V$ be a vector space. We prove the uniqueness of an identity element of addition (IEOA). By Identity element of addition , there exists an IEOA. Let this element be denoted by $0$. For the sake of contradiction, we assume that the IEOA is not unique. That is, there exists an IEOA $0'$ such that $0' \ne 0$. Obviously, $V \ne \emptyset$. Let $v \in V$. By Identity element of addition , $v + 0 = v$ and $v + 0' = v$. Hence, $$v + 0 = v + 0'.$$ By Commutativity of addition , $$0 + v = 0' + v.$$ By Inverse elements of addition , there exists an additive inverse of $v$. Let $-v$ denote the additive inverse of $v$. Due to the foregoing equality, $$(0 + v) + (-v) = (0' + v) + (-v).$$ By Associativity of addition , $$0 + (v + (-v)) = 0' + (v + (-v)).$$ By Inverse elements of addition , $$0 + 0 = 0' + 0.$$ By Identity element of addition , $$0 = 0'.$$ The foregoing equality contradicts our assumption. Thus, our assumption is false and its negation is true. That is, the IEOA is unique. QED P.S.: I wrote another version of the proof , which is less roundabout.",,"['linear-algebra', 'group-theory', 'vector-spaces', 'solution-verification']"
16,Show that every subspace of $\mathbb{R}^n$ is a kernel of a linear map.,Show that every subspace of  is a kernel of a linear map.,\mathbb{R}^n,Let $S$ be a subspace of $\mathbb R^n$. Show that there is an $n \times n$ matrix $A$ such that $$S= \{x \in \mathbb R^n : Ax=0\}.$$ How to proceed?,Let $S$ be a subspace of $\mathbb R^n$. Show that there is an $n \times n$ matrix $A$ such that $$S= \{x \in \mathbb R^n : Ax=0\}.$$ How to proceed?,,"['linear-algebra', 'matrices', 'vector-spaces']"
17,Show any matrix $A_{n\times n}$ can be written as sum of two nonsingular matrices,Show any matrix  can be written as sum of two nonsingular matrices,A_{n\times n},"A nonsingular matrix is one which is invertible, and hence the determinant is not equal to $0$. So at first I thought about having $\det(A_{1})+\det(A_{2})=\det(A_{1}+A_{2})$ and then the resulting sum being a invertible matrix, but this is not generally the case.  Then I thought about eigenvalues, as the determinant is the product of the eigenvalues and that using that I could showing that the spectrum of the sum is equal to the sum of the spectrums. Is that a better way?","A nonsingular matrix is one which is invertible, and hence the determinant is not equal to $0$. So at first I thought about having $\det(A_{1})+\det(A_{2})=\det(A_{1}+A_{2})$ and then the resulting sum being a invertible matrix, but this is not generally the case.  Then I thought about eigenvalues, as the determinant is the product of the eigenvalues and that using that I could showing that the spectrum of the sum is equal to the sum of the spectrums. Is that a better way?",,"['linear-algebra', 'eigenvalues-eigenvectors']"
18,How can I show that this set is linearly independent?,How can I show that this set is linearly independent?,,"Given the set: $\{(1,β,1),(β,1,0),(0,1,β)\}$ For which values of $β \in \mathbb R$ is the set a basis for $\mathbb R^3$ ? I decided to write the set as a matrix: \begin{bmatrix}1&β&1\\β&1&0\\0&1&β\end{bmatrix} Then through pivoting, I thought I could find out if it is linearly independent with $\dim=3$ , which would make it a basis for $R^3$ . However, I'm stuck. How can I proceed?","Given the set: For which values of is the set a basis for ? I decided to write the set as a matrix: Then through pivoting, I thought I could find out if it is linearly independent with , which would make it a basis for . However, I'm stuck. How can I proceed?","\{(1,β,1),(β,1,0),(0,1,β)\} β \in \mathbb R \mathbb R^3 \begin{bmatrix}1&β&1\\β&1&0\\0&1&β\end{bmatrix} \dim=3 R^3","['linear-algebra', 'solution-verification']"
19,Book recommendation for Linear algebra. [duplicate],Book recommendation for Linear algebra. [duplicate],,"This question already has answers here : Prerequisites/Books for A First Course in Linear Algebra (6 answers) Closed last year . I am looking for suggestions, it has to be a self study book and should be able to relate to applications to real world problems. If it is more computer science oriented , that would be great.","This question already has answers here : Prerequisites/Books for A First Course in Linear Algebra (6 answers) Closed last year . I am looking for suggestions, it has to be a self study book and should be able to relate to applications to real world problems. If it is more computer science oriented , that would be great.",,"['linear-algebra', 'reference-request', 'book-recommendation']"
20,Proof of orthogonal matrix property: $A^{-1} = A^t$,Proof of orthogonal matrix property:,A^{-1} = A^t,I have prooved this orthogonal property. Please correct it or show your version of the proof if I am wrong: $A^{-1} = A^t$ $A^{-1} \times A = A^t \times A$ $I = I$ I appreciate your answer.,I have prooved this orthogonal property. Please correct it or show your version of the proof if I am wrong: I appreciate your answer.,A^{-1} = A^t A^{-1} \times A = A^t \times A I = I,"['linear-algebra', 'matrices', 'orthogonality', 'orthogonal-matrices']"
21,Why does the reduced matrix $\left(\begin{array}{ccc|c}0&1&0&-7\\ 0&0&1&10\end{array}\right)$ have infinitely many solutions?,Why does the reduced matrix  have infinitely many solutions?,\left(\begin{array}{ccc|c}0&1&0&-7\\ 0&0&1&10\end{array}\right),$$\left(\begin{array}{ccc|c}0&1&0&-7\\ 0&0&1&10\end{array}\right)$$ I thought the requirement for a matrix to have a unique solution was that when every variable is leading. It seems like both 1's in the above matrix are leading and the other variables are 0. So why doesn't this matrix have only a unique solution?,$$\left(\begin{array}{ccc|c}0&1&0&-7\\ 0&0&1&10\end{array}\right)$$ I thought the requirement for a matrix to have a unique solution was that when every variable is leading. It seems like both 1's in the above matrix are leading and the other variables are 0. So why doesn't this matrix have only a unique solution?,,"['linear-algebra', 'matrices', 'systems-of-equations']"
22,Unique representation of a vector,Unique representation of a vector,,"In a book I am reading the author states without proof that in an $n$-dimensional vector space $X$, the representation of any $x$ as a linear combination of a given basis $e_{1},e_{2},...,e_{n}$ is unique. How to proof that?","In a book I am reading the author states without proof that in an $n$-dimensional vector space $X$, the representation of any $x$ as a linear combination of a given basis $e_{1},e_{2},...,e_{n}$ is unique. How to proof that?",,"['linear-algebra', 'vector-spaces', 'vectors']"
23,Basis for set of nxn matrices with trace = 0,Basis for set of nxn matrices with trace = 0,,"I am trying to find a basis for the set of all $n \times n$ matrices with trace $0$.  I know that part of that basis will be matrices with $1$ in only one entry and $0$ for all others for entries outside the diagonal, as they are not relevant. I don't understand though how to generalize for the entries on the diagonal.  Maybe just one matrix with $1$ in the $(1, 1)$ position and a $-1$ in all other $n - 1$ positions?","I am trying to find a basis for the set of all $n \times n$ matrices with trace $0$.  I know that part of that basis will be matrices with $1$ in only one entry and $0$ for all others for entries outside the diagonal, as they are not relevant. I don't understand though how to generalize for the entries on the diagonal.  Maybe just one matrix with $1$ in the $(1, 1)$ position and a $-1$ in all other $n - 1$ positions?",,['linear-algebra']
24,"Why is it that if A is an n by m matrix, and both BA and AB are indentity matrices, then A is square and can't be rectangular?","Why is it that if A is an n by m matrix, and both BA and AB are indentity matrices, then A is square and can't be rectangular?",,"Some background: I was in class today, and the professor was proving that given a matrix $A$ that is $n$ by $m$ and a $B$ such that $AB=I$ where $B$ is obviously $m$ by $n$ and $I$ is $n$ by $n$, if it is also the case that $BA=I$, then $A$ must be a square matrix (that is, $m=n$). We didn't manage to get through however, because we couldn't find a reason why $A$ actually had to be square in that case -- it seemed to me that it would just be $BA=I$ where $I$ is $m$ by $m$. So in both cases, we would have an Identity matrix, but of different dimensions (sounds strange, but as far as I recall, as long as the product yields an Identity, $B$ counts as an inverse, even if dimensions vary depending on whether you multiply it from left or right). I was thinking something along the lines of overdetermined systems -- if $A$ is invertible (and not square) at least from the right, it has to have more equations than variables, but that did not bring me anywhere. Thanks!","Some background: I was in class today, and the professor was proving that given a matrix $A$ that is $n$ by $m$ and a $B$ such that $AB=I$ where $B$ is obviously $m$ by $n$ and $I$ is $n$ by $n$, if it is also the case that $BA=I$, then $A$ must be a square matrix (that is, $m=n$). We didn't manage to get through however, because we couldn't find a reason why $A$ actually had to be square in that case -- it seemed to me that it would just be $BA=I$ where $I$ is $m$ by $m$. So in both cases, we would have an Identity matrix, but of different dimensions (sounds strange, but as far as I recall, as long as the product yields an Identity, $B$ counts as an inverse, even if dimensions vary depending on whether you multiply it from left or right). I was thinking something along the lines of overdetermined systems -- if $A$ is invertible (and not square) at least from the right, it has to have more equations than variables, but that did not bring me anywhere. Thanks!",,['linear-algebra']
25,How do you prove that the determinant of a 3 by 3 matrix with entries of either 1 and -1 (assume linearly independent rows) will always be 4 or -4?,How do you prove that the determinant of a 3 by 3 matrix with entries of either 1 and -1 (assume linearly independent rows) will always be 4 or -4?,,"Assume that A is a $3\times3$ matrix, where all entries are either $1$ or $-1$ and the rows are linearly independent. Why is it the case that the determinant will always be $4$ or $-4$ ?","Assume that A is a matrix, where all entries are either or and the rows are linearly independent. Why is it the case that the determinant will always be or ?",3\times3 1 -1 4 -4,['linear-algebra']
26,Span of vectors with more entries in each vector than the amount of vectors,Span of vectors with more entries in each vector than the amount of vectors,,"I can easily visualize why $\begin{bmatrix}     1\\ 0 \end{bmatrix}$ and $\begin{bmatrix}     0\\ 1 \end{bmatrix}$ spans $R^2$ . I have learned that we need at least two linearly independent vectors to span $R^2$ , $3$ to span $R^3$ and so on... This is easy to visualize when the entries in the vectors are the same as the amount of total vectors. But what happens if we for instance have the two vectors $\begin{bmatrix}     1\\ 0 \\ 1 \\ 0 \end{bmatrix}$ and $\begin{bmatrix}     0\\1\\0\\1  \end{bmatrix}$ . Clearly,  the two vectors are linearly independent, but do they still only span $R^2$ ? If we add the two vectors we produce $\begin{bmatrix}     1\\ 1\\1\\1 \end{bmatrix}$ which is a vector in $R^4$ , right? So my question boils down to what is the span of $\begin{bmatrix}     1\\ 0 \\ 1 \\ 0 \end{bmatrix}$ and $\begin{bmatrix}     0\\1\\0\\1  \end{bmatrix}$ , and in general what is the span of a set of vectors with more entries in each vector than vectors in total?","I can easily visualize why and spans . I have learned that we need at least two linearly independent vectors to span , to span and so on... This is easy to visualize when the entries in the vectors are the same as the amount of total vectors. But what happens if we for instance have the two vectors and . Clearly,  the two vectors are linearly independent, but do they still only span ? If we add the two vectors we produce which is a vector in , right? So my question boils down to what is the span of and , and in general what is the span of a set of vectors with more entries in each vector than vectors in total?","\begin{bmatrix}
    1\\ 0
\end{bmatrix} \begin{bmatrix}
    0\\ 1
\end{bmatrix} R^2 R^2 3 R^3 \begin{bmatrix}
    1\\ 0 \\ 1 \\ 0
\end{bmatrix} \begin{bmatrix}
    0\\1\\0\\1 
\end{bmatrix} R^2 \begin{bmatrix}
    1\\ 1\\1\\1
\end{bmatrix} R^4 \begin{bmatrix}
    1\\ 0 \\ 1 \\ 0
\end{bmatrix} \begin{bmatrix}
    0\\1\\0\\1 
\end{bmatrix}","['linear-algebra', 'vector-spaces']"
27,Is this matrix a root of the given polynomial?,Is this matrix a root of the given polynomial?,,"The question is whether a given matrix  $$ \begin{pmatrix} 1 & 0 & c & d\\ 0 & 2 & e & f \\ 0 & 0 & 3 & g \\ 0 & 0 & 0 & 4\\ \end{pmatrix} $$ satisfies $f(A) = A^2 - 5A +4I=0$? My attempt was to use the Cayley–Hamilton theorem  $$ \Delta(\lambda)=\Pi_{k=1}^4(\lambda - k)^4 = (\lambda-2)(\lambda-3)(\lambda^2-5\lambda+4)=0. $$ Then $$ \Delta(A) = (A-2)(A-3)(A^2-5A+4)=(A-2)(A-3)f(A)=0. $$ But in general it does not mean that $f(A)$ is $0$. Moreover, by some numeric examples I see that in general $f(A)\neq0$. Is there any theorem or consequnce that I am missing?","The question is whether a given matrix  $$ \begin{pmatrix} 1 & 0 & c & d\\ 0 & 2 & e & f \\ 0 & 0 & 3 & g \\ 0 & 0 & 0 & 4\\ \end{pmatrix} $$ satisfies $f(A) = A^2 - 5A +4I=0$? My attempt was to use the Cayley–Hamilton theorem  $$ \Delta(\lambda)=\Pi_{k=1}^4(\lambda - k)^4 = (\lambda-2)(\lambda-3)(\lambda^2-5\lambda+4)=0. $$ Then $$ \Delta(A) = (A-2)(A-3)(A^2-5A+4)=(A-2)(A-3)f(A)=0. $$ But in general it does not mean that $f(A)$ is $0$. Moreover, by some numeric examples I see that in general $f(A)\neq0$. Is there any theorem or consequnce that I am missing?",,"['linear-algebra', 'matrices']"
28,Show that AB is singular if A is singular,Show that AB is singular if A is singular,,"Actually I need to show that $\det(AB) = \det(A)\det(B)$ if $A$ is a singular matrix. The determinant of $A$ is $0$ if $A$ is singular, so $\det(AB)$ has to be $0$ as well, but I have problems showing that $AB$ is singular if $A$ is singular. How can I show that?","Actually I need to show that $\det(AB) = \det(A)\det(B)$ if $A$ is a singular matrix. The determinant of $A$ is $0$ if $A$ is singular, so $\det(AB)$ has to be $0$ as well, but I have problems showing that $AB$ is singular if $A$ is singular. How can I show that?",,"['linear-algebra', 'matrices']"
29,"Find the eigenvalues of a matrix with ones in the diagonal, and all the other elements equal [duplicate]","Find the eigenvalues of a matrix with ones in the diagonal, and all the other elements equal [duplicate]",,"This question already has answers here : Determinant of a rank $1$ update of a scalar matrix, or characteristic polynomial of a rank $1$ matrix (2 answers) Closed 9 years ago . Let $A$ be a real $n\times n$ matrix, with ones in the diagonal, and all of the other elements equal to $r$ with $0<r<1$. How can I prove that the eigenvalues of $A$ are $1+(n-1)r$ and $1-r$,  with multiplicity $n-1$?","This question already has answers here : Determinant of a rank $1$ update of a scalar matrix, or characteristic polynomial of a rank $1$ matrix (2 answers) Closed 9 years ago . Let $A$ be a real $n\times n$ matrix, with ones in the diagonal, and all of the other elements equal to $r$ with $0<r<1$. How can I prove that the eigenvalues of $A$ are $1+(n-1)r$ and $1-r$,  with multiplicity $n-1$?",,"['linear-algebra', 'matrices', 'eigenvalues-eigenvectors', 'matrix-decomposition']"
30,Good typesetting software for linear algebra?,Good typesetting software for linear algebra?,,"I have broken my dominant hand, leaving me unable to write. Thus far, I have been using LaTeX, but have been disappointed by how long it takes to display matrices, format, etc. Does anyone know of a linear algebra specific typesetting software?","I have broken my dominant hand, leaving me unable to write. Thus far, I have been using LaTeX, but have been disappointed by how long it takes to display matrices, format, etc. Does anyone know of a linear algebra specific typesetting software?",,"['linear-algebra', 'math-software']"
31,Why is the dot product of two vectors a scalar value?,Why is the dot product of two vectors a scalar value?,,"I'm having some trouble seeing why dot products are said to give scalar values. As a far as I can see, it just gives another vector that is projected onto one of the 2 original vectors. How, then, is the result a scalar quantity. Can someone please explain this to me? Thank you.","I'm having some trouble seeing why dot products are said to give scalar values. As a far as I can see, it just gives another vector that is projected onto one of the 2 original vectors. How, then, is the result a scalar quantity. Can someone please explain this to me? Thank you.",,"['linear-algebra', 'vectors', 'intuition']"
32,"Linear independence of $\sin(x)$, $\sin(2x)$, $\sin(3x)$ in Map($\mathbb{R},\mathbb{R}$)","Linear independence of , ,  in Map()","\sin(x) \sin(2x) \sin(3x) \mathbb{R},\mathbb{R}","It seems rather obvious to me that $\sin(x)$, $\sin(2x)$, $\sin(3x)$ are linearly independent in $\operatorname{Map}(\mathbb{R},\mathbb{R})$, but I'm not sure how to prove it (or disprove it if I'm wrong?). I know that for linear independence it must $$ a\sin(x)+b\sin(2x)+c\sin(3x)=0 $$ $$ \implies a=b=c=0$$ but how can I show this is true for any $x \in \mathbb{R}$?","It seems rather obvious to me that $\sin(x)$, $\sin(2x)$, $\sin(3x)$ are linearly independent in $\operatorname{Map}(\mathbb{R},\mathbb{R})$, but I'm not sure how to prove it (or disprove it if I'm wrong?). I know that for linear independence it must $$ a\sin(x)+b\sin(2x)+c\sin(3x)=0 $$ $$ \implies a=b=c=0$$ but how can I show this is true for any $x \in \mathbb{R}$?",,"['linear-algebra', 'abstract-algebra']"
33,Is Matrix $A^2$ invertible if $A$ is invertible?,Is Matrix  invertible if  is invertible?,A^2 A,"I want to say that squaring is a form of scaling so that it should be true; however I can't make sense out of it and clearly see why. The problem is that while it's a form of scaling, if we think about a matrix as a list of coefficients of variables of a system of equations, then the coefficients are being multiplied among all the variables. For example, with the simple scaling by a single value of a matrix, a system of equations would retain the same exact solutions it had before scaling. However, when squaring the matrix (I tried with a specific example), the solutions differ in the squared version of the system.","I want to say that squaring is a form of scaling so that it should be true; however I can't make sense out of it and clearly see why. The problem is that while it's a form of scaling, if we think about a matrix as a list of coefficients of variables of a system of equations, then the coefficients are being multiplied among all the variables. For example, with the simple scaling by a single value of a matrix, a system of equations would retain the same exact solutions it had before scaling. However, when squaring the matrix (I tried with a specific example), the solutions differ in the squared version of the system.",,"['linear-algebra', 'matrices']"
34,How to find a matrix square root with all real entries (if it exists),How to find a matrix square root with all real entries (if it exists),,Let $X$ be a $n\times n$ matrix. Assume there exists a square root $Y$ of $X$ ($YY=X$) such that all entries of $Y$ are real. How to find all such $Y$'s?,Let $X$ be a $n\times n$ matrix. Assume there exists a square root $Y$ of $X$ ($YY=X$) such that all entries of $Y$ are real. How to find all such $Y$'s?,,"['linear-algebra', 'matrices']"
35,Discriminant formula - do the coefficients include their signs?,Discriminant formula - do the coefficients include their signs?,,"Beginner here. Can I ask someone to explain to me the following? Given the following general form of quadratic equation: $$ ax^2+bx+c=0 $$ and the following formula for discriminant: $$ D=b^2−4ac, $$ what is the $b$ coefficient in the following equation? $$ 2x^2 - 2x = 0 $$ Is it $-2$ or $2$ ? Does the coefficient count with the positive/negative sign? I am confused because for example $-2^2= -4$ , and so here the sign is not taken into consideration, unless it's $(-2)^2$ .","Beginner here. Can I ask someone to explain to me the following? Given the following general form of quadratic equation: and the following formula for discriminant: what is the coefficient in the following equation? Is it or ? Does the coefficient count with the positive/negative sign? I am confused because for example , and so here the sign is not taken into consideration, unless it's .","
ax^2+bx+c=0
 
D=b^2−4ac,
 b 
2x^2 - 2x = 0
 -2 2 -2^2= -4 (-2)^2","['linear-algebra', 'algebra-precalculus', 'quadratics']"
36,"Do we need to check all 10 axioms to verify that some set, call it $V$, is a vector space?","Do we need to check all 10 axioms to verify that some set, call it , is a vector space?",V,"The definition of the vector space from the book: Let $\bf u, v$ and $\bf w$ be vectors in the set $V$ . The set $V$ is   called a vector space if it satisfies the following 10 axioms. The vector addition $\bf u + v$ is also in the vector space $V$ . Commutative law: $\bf u + v = v + u$ Associative law: $\bf (u+v) + w = u + (v + w)$ Neutral element. There is a vector called the zero vector in $V$ denoted by $\bf O$ which satisfies $$\bf u + O = u$$ for every vector $\bf u $ in $V$ Additive inverse: $\bf u + (-u) = O$ Let $k$ be a real scalar. Then $k\bf u$ is also in $V$ . Let $k$ and $c$ be scalars, then $k(c\mathbf{u}) = (kc)\mathbf{v}$ Let $k$ be a real scalar. Then $k(\mathbf{u + v}) = k\mathbf u + k\mathbf v $ Let $k$ and $c$ be real scalars, then $(k+c)\mathbf u = k\mathbf{u} + c\mathbf{u}$ For every vector $\bf u$ : $1\mathbf{u} = \mathbf{u}$ We say that if the elements of the set $V$ satisfy the above 10 axioms   then $V$ is called a vector space and the elements are known as vectors. Suppose we have some set, call it $S$ , and we verified that axiom $1$ and $6$ hold. Does it make sense to check other axioms? Isn't it self-evident that they will be satisfied? If it is not, could you provide me an example when axiom $1$ and $6$ hold but at least one other fail?","The definition of the vector space from the book: Let and be vectors in the set . The set is   called a vector space if it satisfies the following 10 axioms. The vector addition is also in the vector space . Commutative law: Associative law: Neutral element. There is a vector called the zero vector in denoted by which satisfies for every vector in Additive inverse: Let be a real scalar. Then is also in . Let and be scalars, then Let be a real scalar. Then Let and be real scalars, then For every vector : We say that if the elements of the set satisfy the above 10 axioms   then is called a vector space and the elements are known as vectors. Suppose we have some set, call it , and we verified that axiom and hold. Does it make sense to check other axioms? Isn't it self-evident that they will be satisfied? If it is not, could you provide me an example when axiom and hold but at least one other fail?","\bf u, v \bf w V V \bf u + v V \bf u + v = v + u \bf (u+v) + w = u + (v + w) V \bf O \bf u + O = u \bf u  V \bf u + (-u) = O k k\bf u V k c k(c\mathbf{u}) = (kc)\mathbf{v} k k(\mathbf{u + v}) = k\mathbf u + k\mathbf v  k c (k+c)\mathbf u = k\mathbf{u} + c\mathbf{u} \bf u 1\mathbf{u} = \mathbf{u} V V S 1 6 1 6",['linear-algebra']
37,"If for two suitable matrices $B$ and $C$, $AA^TB=AA^TC$ can we conclude that $A^TB=A^TC$?","If for two suitable matrices  and ,  can we conclude that ?",B C AA^TB=AA^TC A^TB=A^TC,"Let $A$ be a $m\times n$ real matrix. If for two suitable matrices $B$ and $C$ , $$AA^TB=AA^TC$$ can we conclude that $A^TB=A^TC$ ? I know $AA^T$ is a symmetric matrix. Every real symmetric matrix is diagonalizable. But I am not sure how to use this information. Thanks for reading and helping me out with hints.","Let be a real matrix. If for two suitable matrices and , can we conclude that ? I know is a symmetric matrix. Every real symmetric matrix is diagonalizable. But I am not sure how to use this information. Thanks for reading and helping me out with hints.",A m\times n B C AA^TB=AA^TC A^TB=A^TC AA^T,"['linear-algebra', 'matrices']"
38,Find two square roots a 2 by 2 matrix filled with twos,Find two square roots a 2 by 2 matrix filled with twos,,"Given matrix $A = \begin{pmatrix} 2 && 2 \\  2 && 2\end{pmatrix}$, I want to find two square roots of A. I have to go about this with only very introductory-type tools, those covered in an introductory matrix operations chapter. My Approach Since I know that the square root matrix is a 2x2 matrix, let the square root matrix be $B = \begin{pmatrix} a && b \\  c && d\end{pmatrix}$. Now, for B to be a square root matrix of A, the following must hold true: BB = A. Evaluating BB I get $BB = \begin{pmatrix} a^2 + bc && b(a + d) \\  c(a+d) && d^2 + bc\end{pmatrix}$ This leaves me with the following equations: $a^2 + bc=2$ $d^2 + bc=2$ $b(a+d)=2$ $c(a+d)=2$ From here on I've tried solving the equations but none of my attempts yielded the correct solution. I get the feeling I'm overlooking something very basic.","Given matrix $A = \begin{pmatrix} 2 && 2 \\  2 && 2\end{pmatrix}$, I want to find two square roots of A. I have to go about this with only very introductory-type tools, those covered in an introductory matrix operations chapter. My Approach Since I know that the square root matrix is a 2x2 matrix, let the square root matrix be $B = \begin{pmatrix} a && b \\  c && d\end{pmatrix}$. Now, for B to be a square root matrix of A, the following must hold true: BB = A. Evaluating BB I get $BB = \begin{pmatrix} a^2 + bc && b(a + d) \\  c(a+d) && d^2 + bc\end{pmatrix}$ This leaves me with the following equations: $a^2 + bc=2$ $d^2 + bc=2$ $b(a+d)=2$ $c(a+d)=2$ From here on I've tried solving the equations but none of my attempts yielded the correct solution. I get the feeling I'm overlooking something very basic.",,['linear-algebra']
39,"If a vector $v$ is an eigenvector of both matrices $A$ and $B$, is $v$ necessarily an eigenvector of $AB$?","If a vector  is an eigenvector of both matrices  and , is  necessarily an eigenvector of ?",v A B v AB,"I'm preparing for my final and this question came up in one of the practices. I am tempted to say no, but I've been having trouble proving this. If a vector $v$ is an eigenvector of both matrices $A$ and $B$, is $v$ necessarily an eigenvector of $AB$?","I'm preparing for my final and this question came up in one of the practices. I am tempted to say no, but I've been having trouble proving this. If a vector $v$ is an eigenvector of both matrices $A$ and $B$, is $v$ necessarily an eigenvector of $AB$?",,['linear-algebra']
40,Any trick on finding the inverse of this matrix?,Any trick on finding the inverse of this matrix?,,"Supposing I have a matrix, $\pmatrix{0&0&\lambda\\0&\lambda&-1\\ \lambda&-1&0}$. Without question you can work out the inverse if this matrix. But since it is highly structured, I suppose there should be some quick way to find out the inverse of it? Can anyone show me a quick way of thinking about this problem?","Supposing I have a matrix, $\pmatrix{0&0&\lambda\\0&\lambda&-1\\ \lambda&-1&0}$. Without question you can work out the inverse if this matrix. But since it is highly structured, I suppose there should be some quick way to find out the inverse of it? Can anyone show me a quick way of thinking about this problem?",,['linear-algebra']
41,Does this matrix identity hold?,Does this matrix identity hold?,,"For invertible matrices A and B does the identity: $$ (A^{-1} + B^{-1})^{-1} = A - A(A+B)^{-1}A  $$ hold? My supervisor suggested that they are equal but I haven't been able to prove this and in the matrix cookbook ( http://www.math.uwaterloo.ca/~hwolkowi/matrixcookbook.pdf ) there are separate identities for both sides of this equation, but they are not given as equal to each other.","For invertible matrices A and B does the identity: $$ (A^{-1} + B^{-1})^{-1} = A - A(A+B)^{-1}A  $$ hold? My supervisor suggested that they are equal but I haven't been able to prove this and in the matrix cookbook ( http://www.math.uwaterloo.ca/~hwolkowi/matrixcookbook.pdf ) there are separate identities for both sides of this equation, but they are not given as equal to each other.",,"['linear-algebra', 'matrices', 'inverse']"
42,Solve a system of differential equations,Solve a system of differential equations,,"I want to know how solve the following system of differential equations. $$\frac {dy}{dx} = z-x$$ $$\frac{dz}{dx}=y+x$$ Where $y(0)=1, z(0)=1$ As I can understand I cannot represent this system as $x'=Ax$. What is the correct method to solve this system.","I want to know how solve the following system of differential equations. $$\frac {dy}{dx} = z-x$$ $$\frac{dz}{dx}=y+x$$ Where $y(0)=1, z(0)=1$ As I can understand I cannot represent this system as $x'=Ax$. What is the correct method to solve this system.",,"['linear-algebra', 'ordinary-differential-equations']"
43,Is it true that $\det(A + I) = \operatorname{trace} (A) + 1$?,Is it true that ?,\det(A + I) = \operatorname{trace} (A) + 1,Let $A$ be an $n\times n$ matrix such that $\operatorname{rank} A = 1$ and that $n- 2$ rows of $A$   are the zero rows. Is it true that $\det(A + I) = \operatorname{trace} (A) + 1$? I already have the answer which can be viewed here . I don't understand how they expanded the determinant and got: $\begin{vmatrix} a_i+1 & a_k  \\ \beta a_i & \beta a_k+1 \end{vmatrix}$ . I saw another answer where he divided the determinant to two upper diagonals matrices but I didn't get how he did that. An explaination of these two answers and/or another way to solve it would be appreciated. NOTE: we haven't covered eigenvalues.,Let $A$ be an $n\times n$ matrix such that $\operatorname{rank} A = 1$ and that $n- 2$ rows of $A$   are the zero rows. Is it true that $\det(A + I) = \operatorname{trace} (A) + 1$? I already have the answer which can be viewed here . I don't understand how they expanded the determinant and got: $\begin{vmatrix} a_i+1 & a_k  \\ \beta a_i & \beta a_k+1 \end{vmatrix}$ . I saw another answer where he divided the determinant to two upper diagonals matrices but I didn't get how he did that. An explaination of these two answers and/or another way to solve it would be appreciated. NOTE: we haven't covered eigenvalues.,,['linear-algebra']
44,Value of Vandermonde type determinant,Value of Vandermonde type determinant,,"Let $x_1,...,x_n $ are distinct real numbers. Is it a formula for the Vandermonde type determinant $V(x_1, \cdots,x_n)$  whose last column is $x_1^k,\ \cdots,\ x_n^k$, where $k \geq n$, instead of $x_1^{n-1},\ \cdots,\ x_n^{n-1}$? Thanks","Let $x_1,...,x_n $ are distinct real numbers. Is it a formula for the Vandermonde type determinant $V(x_1, \cdots,x_n)$  whose last column is $x_1^k,\ \cdots,\ x_n^k$, where $k \geq n$, instead of $x_1^{n-1},\ \cdots,\ x_n^{n-1}$? Thanks",,"['linear-algebra', 'determinant']"
45,Does full rank matrix have a null space? [closed],Does full rank matrix have a null space? [closed],,"Closed. This question does not meet Mathematics Stack Exchange guidelines . It is not currently accepting answers. Please provide additional context , which ideally explains why the question is relevant to you and our  community. Some forms of context include: background and motivation, relevant definitions, source, possible strategies, your current progress, why the question is interesting or important, etc. Closed 3 years ago . Improve this question The null space is defined as all vector that is set to null by matrix $A$ , where $Ax = 0$ . If the matrix $A$ is full rank, does it mean that it has no null space?","Closed. This question does not meet Mathematics Stack Exchange guidelines . It is not currently accepting answers. Please provide additional context , which ideally explains why the question is relevant to you and our  community. Some forms of context include: background and motivation, relevant definitions, source, possible strategies, your current progress, why the question is interesting or important, etc. Closed 3 years ago . Improve this question The null space is defined as all vector that is set to null by matrix , where . If the matrix is full rank, does it mean that it has no null space?",A Ax = 0 A,['linear-algebra']
46,Proving a symmetric Cauchy matrix is positive semidefinite,Proving a symmetric Cauchy matrix is positive semidefinite,,"Let $x_1, x_2,\dots, x_n$ be positive real numbers. Let $A$ be the $n\times n$ matrix whose $i,j^\text{th}$ entry is $$a_{ij}=\frac{1}{x_i+x_j}.$$ This is a Cauchy matrix . I am trying to show that this matrix is positive semi-definite. I have been given the following hint: Consider the matrix $T=(t^{x_i+x_j})$ where $t>0$ . Use the fact that $T$ is positive semi-definite and that $$\frac{1}{x_i+x_j}=\int_0^1t^{x_i+x_j-1}dt.$$ I have managed to show that $T$ is positive semi-definite but I don't understand where to go from there or how to use the rest of the hint. I would like another way to do this, preferably without involving integrals Thank you.","Let be positive real numbers. Let be the matrix whose entry is This is a Cauchy matrix . I am trying to show that this matrix is positive semi-definite. I have been given the following hint: Consider the matrix where . Use the fact that is positive semi-definite and that I have managed to show that is positive semi-definite but I don't understand where to go from there or how to use the rest of the hint. I would like another way to do this, preferably without involving integrals Thank you.","x_1, x_2,\dots, x_n A n\times n i,j^\text{th} a_{ij}=\frac{1}{x_i+x_j}. T=(t^{x_i+x_j}) t>0 T \frac{1}{x_i+x_j}=\int_0^1t^{x_i+x_j-1}dt. T","['linear-algebra', 'matrices', 'positive-semidefinite', 'cauchy-matrices']"
47,"Prove that if ${x_1, x_2, x_3}$ are roots of ${x^3 + px + q = 0}$ then ${x_1^3+x_2^3 + x_3^3 = 3x_1x_2x_3}$",Prove that if  are roots of  then,"{x_1, x_2, x_3} {x^3 + px + q = 0} {x_1^3+x_2^3 + x_3^3 = 3x_1x_2x_3}","How to prove that ${x_1^3+x_2^3 + x_3^3 = 3x_1x_2x_3}$ holds in case ${x_1, x_2, x_3}$ are roots of the polynomial? I've tried the following approach: If $x_1$, $x_2$ and $x_3$ are roots then $$(x-x_1)(x-x_2)(x-x_3) = x^3+px+q = 0$$ Now find the coefficient near the powers of $x$: $$ x^3 - (x_1 + x_2 + x_3)x^2 + (x_1x_2 + x_1x_3 + x_2x_3)x - x_1x_2x_3 = x^3+px+q $$ That means that I can write a system of equations: $$ \begin{cases} -(x_1 + x_2 + x_3) = 0 \\ x_1x_2 + x_1x_3 + x_2x_3 =  p \\ - x_1x_2x_3 = q \end{cases} $$ At this point I got stuck. I've tried to raise $x_1 + x_2 + x_3$ to 3 power and expand the terms, but that didn't give me any insights. It feels like I have to play with the system of equations in some way but not sure what exact.","How to prove that ${x_1^3+x_2^3 + x_3^3 = 3x_1x_2x_3}$ holds in case ${x_1, x_2, x_3}$ are roots of the polynomial? I've tried the following approach: If $x_1$, $x_2$ and $x_3$ are roots then $$(x-x_1)(x-x_2)(x-x_3) = x^3+px+q = 0$$ Now find the coefficient near the powers of $x$: $$ x^3 - (x_1 + x_2 + x_3)x^2 + (x_1x_2 + x_1x_3 + x_2x_3)x - x_1x_2x_3 = x^3+px+q $$ That means that I can write a system of equations: $$ \begin{cases} -(x_1 + x_2 + x_3) = 0 \\ x_1x_2 + x_1x_3 + x_2x_3 =  p \\ - x_1x_2x_3 = q \end{cases} $$ At this point I got stuck. I've tried to raise $x_1 + x_2 + x_3$ to 3 power and expand the terms, but that didn't give me any insights. It feels like I have to play with the system of equations in some way but not sure what exact.",,"['linear-algebra', 'polynomials']"
48,Prove three eigenvectors from three distinct eigenvalues are linearly independent [duplicate],Prove three eigenvectors from three distinct eigenvalues are linearly independent [duplicate],,"This question already has answers here : If $v_1,...,v_r$ are eigenvectors that correspond to distinct eigenvalues, then they are linearly independent. (3 answers) Closed 6 years ago . Here is the formal statement: Let $\lambda_1, \lambda_2, \lambda_3$ be distinct eigenvalues of $n\times n$ matrix $A$. Let $S=\{v_1, v_2, v_3\}$, where $Av_i = \lambda_i v_i$ for $1\leq i\leq 3$. Prove $S$ is linearly independent. Many resources online state the general proof or the proof for two eigenvectors. What is the proof for specifically 3? I tried to derive the 3 eigenvector proof from the 2 eigenvector proofs, but failed.","This question already has answers here : If $v_1,...,v_r$ are eigenvectors that correspond to distinct eigenvalues, then they are linearly independent. (3 answers) Closed 6 years ago . Here is the formal statement: Let $\lambda_1, \lambda_2, \lambda_3$ be distinct eigenvalues of $n\times n$ matrix $A$. Let $S=\{v_1, v_2, v_3\}$, where $Av_i = \lambda_i v_i$ for $1\leq i\leq 3$. Prove $S$ is linearly independent. Many resources online state the general proof or the proof for two eigenvectors. What is the proof for specifically 3? I tried to derive the 3 eigenvector proof from the 2 eigenvector proofs, but failed.",,"['linear-algebra', 'matrices', 'eigenvalues-eigenvectors']"
49,"How to find the center of a circle from a point on it, and a tangent line.","How to find the center of a circle from a point on it, and a tangent line.",,"I am clearly not typing this in to search engines correctly, if it's possible. The teacher likes to ask trick questions, so it might actually not be possible. I can't see anything like this in my notes, nor does anything come up when I Google. When I turned in the work, I assumed the point on the circle was opposite to the tangent line. I was told this wasn't a valid assumption to make. I'm also not getting any answers from him. I can't think of any way to solve this, so I turn to you guys. Consider a circle $C$ that is tangent to $3x+4y-12=0$ at $(0,3)$ and contains $(2,-1)$.        Set up equations that would determine the center $(h,k)$ and radius $r$ from the circle.        DO NOT SOLVE THESE EQUATIONS.","I am clearly not typing this in to search engines correctly, if it's possible. The teacher likes to ask trick questions, so it might actually not be possible. I can't see anything like this in my notes, nor does anything come up when I Google. When I turned in the work, I assumed the point on the circle was opposite to the tangent line. I was told this wasn't a valid assumption to make. I'm also not getting any answers from him. I can't think of any way to solve this, so I turn to you guys. Consider a circle $C$ that is tangent to $3x+4y-12=0$ at $(0,3)$ and contains $(2,-1)$.        Set up equations that would determine the center $(h,k)$ and radius $r$ from the circle.        DO NOT SOLVE THESE EQUATIONS.",,"['linear-algebra', 'euclidean-geometry', 'analytic-geometry']"
50,$A_{n\times n}$ that implies : $A^2-2A+I=0$ Proof $1$ is an eigevalue of $A$,that implies :  Proof  is an eigevalue of,A_{n\times n} A^2-2A+I=0 1 A,"I have the following question : Let $A_{n \times n}$ that implies : $A^2-2A+I=0$ Proof $1$ is an eigevalue of $A$ I don't really know how to approach this this what I manage to do (its not much though): $A(A-2I)=-I$ We know that if $\lambda$ is an eigenvalue then $Ax=\lambda x$ $(x \neq 0)$ $$A(A-2I)=I$$ Can I say now that since $Ax=\lambda x$ and Let $x=(A-2I)$ but $x$ is vector, not a matrix. I don't really understand what to do next. Any help will be be dearly appreciated, Thanks.","I have the following question : Let $A_{n \times n}$ that implies : $A^2-2A+I=0$ Proof $1$ is an eigevalue of $A$ I don't really know how to approach this this what I manage to do (its not much though): $A(A-2I)=-I$ We know that if $\lambda$ is an eigenvalue then $Ax=\lambda x$ $(x \neq 0)$ $$A(A-2I)=I$$ Can I say now that since $Ax=\lambda x$ and Let $x=(A-2I)$ but $x$ is vector, not a matrix. I don't really understand what to do next. Any help will be be dearly appreciated, Thanks.",,"['linear-algebra', 'matrices', 'eigenvalues-eigenvectors']"
51,Skew Symmetric Matrix Properties,Skew Symmetric Matrix Properties,,"We have a theorem says that ""ODD-SIZED SKEW-SYMMETRIC MATRICES ARE SINGULAR"" . Proof link is given here if needed. Now let us assume we have a $3\times 3$ skew symmetric matrices of the form $ \begin{bmatrix}\,0&\!-a_3&\,\,a_2\\ \,\,a_3&0&\!-a_1\\-a_2&\,\,a_1&\,0\end{bmatrix}$ and an Identity matrix $I_{3\times3}$ Question Can we say determinant of $I_{3\times3}+\begin{bmatrix}\,0&\!-a_3&\,\,a_2\\ \,\,a_3&0&\!-a_1\\-a_2&\,\,a_1&\,0\end{bmatrix} \tag 1$ is not zero always? if so how can we prove it mathematically ? NB:: $a_1,a_2,a_3$ cant be zero together at a time","We have a theorem says that ""ODD-SIZED SKEW-SYMMETRIC MATRICES ARE SINGULAR"" . Proof link is given here if needed. Now let us assume we have a skew symmetric matrices of the form and an Identity matrix Question Can we say determinant of is not zero always? if so how can we prove it mathematically ? NB:: cant be zero together at a time","3\times 3  \begin{bmatrix}\,0&\!-a_3&\,\,a_2\\ \,\,a_3&0&\!-a_1\\-a_2&\,\,a_1&\,0\end{bmatrix} I_{3\times3} I_{3\times3}+\begin{bmatrix}\,0&\!-a_3&\,\,a_2\\ \,\,a_3&0&\!-a_1\\-a_2&\,\,a_1&\,0\end{bmatrix} \tag 1 a_1,a_2,a_3","['linear-algebra', 'matrices', 'matrix-equations', 'block-matrices', 'matrix-decomposition']"
52,"Prove that if $\langle x,z\rangle = 0$ for all $z,$ then $x=0.$",Prove that if  for all  then,"\langle x,z\rangle = 0 z, x=0.","I just wanted to check if my reasoning in this proof was correct. The question is as follows. Let $\beta$ be a basis for a finite dimensional inner product space $V.$ a) Prove that if $\langle x,z \rangle = 0$ for all $z \in \beta,$ then $x=0.$ OK, I reasoned it this way: from the definition of inner products, we have that $\langle x,0 \rangle = \langle 0,x \rangle = 0.$ Further, since $z$ is a basis element and linearly independent, then $x_1z_1+x_2z_2+\dots+x_nz_n = 0$ only if either all the $z_i$ s are zero or all the $x_i$ s are zero. Therefore, $x=0$ since in the initial construction, we defined $z$ as a basis for $V,$ and it doesn't have to be zero. b) Prove that if $\langle x, z \rangle = \langle y,z \rangle$ for all $z \in \beta,$ then $x=y.$ Again, we can go to the fact that $z$ is a basis and linearly independent. We have that $x_1z_1+x_2z_2+\dots+x_nz_n = y_1z_1+y_2z_2+\dots+y_nz_n,$ so if $z$ is linearly independent, then it can only be zero if all the $z$ s are zero or all the $x$ s and $y$ s are zero. In which case, $x$ would be equal to $y$ if $z$ isn't zero. For other cases, we just divide the left side of the equation by the $z$ s on the right, and we get $x_1+x_2+\dots+x_n = y_1+y_2+\dots+y_n$ Is this correct? Did I miss something important? I know this is a simple exercise, but I want to be sure.","I just wanted to check if my reasoning in this proof was correct. The question is as follows. Let be a basis for a finite dimensional inner product space a) Prove that if for all then OK, I reasoned it this way: from the definition of inner products, we have that Further, since is a basis element and linearly independent, then only if either all the s are zero or all the s are zero. Therefore, since in the initial construction, we defined as a basis for and it doesn't have to be zero. b) Prove that if for all then Again, we can go to the fact that is a basis and linearly independent. We have that so if is linearly independent, then it can only be zero if all the s are zero or all the s and s are zero. In which case, would be equal to if isn't zero. For other cases, we just divide the left side of the equation by the s on the right, and we get Is this correct? Did I miss something important? I know this is a simple exercise, but I want to be sure.","\beta V. \langle x,z \rangle = 0 z \in \beta, x=0. \langle x,0 \rangle = \langle 0,x \rangle = 0. z x_1z_1+x_2z_2+\dots+x_nz_n = 0 z_i x_i x=0 z V, \langle x, z \rangle = \langle y,z \rangle z \in \beta, x=y. z x_1z_1+x_2z_2+\dots+x_nz_n = y_1z_1+y_2z_2+\dots+y_nz_n, z z x y x y z z x_1+x_2+\dots+x_n = y_1+y_2+\dots+y_n","['linear-algebra', 'vector-spaces']"
53,"Which group is isomorphic to $\left\langle\begin{bmatrix}0&1\\1&0\end{bmatrix},\begin{bmatrix}1&-1\\0 & -1 \end{bmatrix} \right\rangle$?",Which group is isomorphic to ?,"\left\langle\begin{bmatrix}0&1\\1&0\end{bmatrix},\begin{bmatrix}1&-1\\0 & -1 \end{bmatrix} \right\rangle","Both matrices have determinant equal to -1, so their products are matrices with determinant $\in \{1,-1\}$. Can I conclude that this is isomorphic to $ O_2(\mathbb{R}) $ ?","Both matrices have determinant equal to -1, so their products are matrices with determinant $\in \{1,-1\}$. Can I conclude that this is isomorphic to $ O_2(\mathbb{R}) $ ?",,['linear-algebra']
54,"$T:\Bbb R^2 \to \Bbb R^2$, linear, diagonal with respect to any basis.",", linear, diagonal with respect to any basis.",T:\Bbb R^2 \to \Bbb R^2,"Is there a linear transformation from $\Bbb R^2$ to $\Bbb R^2$ which is represented by a diagonal matrix when written with respect to any fixed basis? If such linear transformation $T$ exists, then its eigenvector should be the identity matrix for any fixed basis $\beta$ of $\Bbb R^2$. Then, I don't see, if this is possible or not.","Is there a linear transformation from $\Bbb R^2$ to $\Bbb R^2$ which is represented by a diagonal matrix when written with respect to any fixed basis? If such linear transformation $T$ exists, then its eigenvector should be the identity matrix for any fixed basis $\beta$ of $\Bbb R^2$. Then, I don't see, if this is possible or not.",,[]
55,Vector perpendicular to timelike vector must be spacelike?,Vector perpendicular to timelike vector must be spacelike?,,"Given $\mathbb{R}^4$, we define the Minkowski inner product on it by $$ \langle v,w \rangle = -v_1w_1 + v_2w_2 + v_3w_3 + v_4w_4$$ We say a vector is spacelike if $ \langle v,v\rangle >0 $, and it is timelike if $ \langle v,v \rangle < 0 $. How can I show that if $v$ is timelike and $ \langle v,w \rangle = 0$ , then $w$ is either the zero vector or spacelike? I've tried to use the polarization identity, but don't have any information regarding the $\langle v+w,v+w \rangle$ term in the identity. Context: I'm reading a book on Riemannian geometry, and the book gives a proof of a more general result: if $z$ is timelike, then its perpendicular subspace $z^\perp$ is spacelike. It does so using arguments regarding the degeneracy index of the subspace, which I don't fully understand. Since the statement above seems fairly elementary, I was wondering whether it would be possible to give an elementary proof of it as well. Any help is appreciated!","Given $\mathbb{R}^4$, we define the Minkowski inner product on it by $$ \langle v,w \rangle = -v_1w_1 + v_2w_2 + v_3w_3 + v_4w_4$$ We say a vector is spacelike if $ \langle v,v\rangle >0 $, and it is timelike if $ \langle v,v \rangle < 0 $. How can I show that if $v$ is timelike and $ \langle v,w \rangle = 0$ , then $w$ is either the zero vector or spacelike? I've tried to use the polarization identity, but don't have any information regarding the $\langle v+w,v+w \rangle$ term in the identity. Context: I'm reading a book on Riemannian geometry, and the book gives a proof of a more general result: if $z$ is timelike, then its perpendicular subspace $z^\perp$ is spacelike. It does so using arguments regarding the degeneracy index of the subspace, which I don't fully understand. Since the statement above seems fairly elementary, I was wondering whether it would be possible to give an elementary proof of it as well. Any help is appreciated!",,"['linear-algebra', 'riemannian-geometry']"
56,Matrix equation question,Matrix equation question,,"Could anybody help me with solving such matrix problem: Find all matrices $A$ (non zero and identity matrices), which corresponds to this equation: $$ A^2=A^3 $$","Could anybody help me with solving such matrix problem: Find all matrices $A$ (non zero and identity matrices), which corresponds to this equation: $$ A^2=A^3 $$",,"['linear-algebra', 'matrices']"
57,"Matrix with entries $A_{ij} = u^{|i-j|} - u^{i+j}$ is positive semidefinite when $u \in (0, 1)$?",Matrix with entries  is positive semidefinite when ?,"A_{ij} = u^{|i-j|} - u^{i+j} u \in (0, 1)","Consider the square $n \times n$ matrix $A_n$ with entries $u^{|i-j|} - u^{i+j}$ where $u \in (0, 1)$ . Is it true that $A$ is positive semidefinite? In the case $n = 1$ , we have $A_1 = 1-u^2 \geq 0$ . For the case $n = 2$ , we have $A_2 = (1 - u^2)\begin{pmatrix} 1 & u \\ u & 1+u^2 \end{pmatrix}$ . We can look at a quadratic form to see it is again positive semidefinite: $$\frac{(x, y)^T A_2 (x, y)}{1-u^2} = x^2 + (1 +u^2)y^2 + 2xyu \geq x^2 + u^2 y^2 + 2xyu = (x + uy)^2 \geq 0. $$ I can't tell how to generalize this to $n \geq 3$ . Perhaps there is a better way establish the claim? For instance, it suffices (by induction) to prove that $A_n$ has positive determinant, but this also seems a little tricky.","Consider the square matrix with entries where . Is it true that is positive semidefinite? In the case , we have . For the case , we have . We can look at a quadratic form to see it is again positive semidefinite: I can't tell how to generalize this to . Perhaps there is a better way establish the claim? For instance, it suffices (by induction) to prove that has positive determinant, but this also seems a little tricky.","n \times n A_n u^{|i-j|} - u^{i+j} u \in (0, 1) A n = 1 A_1 = 1-u^2 \geq 0 n = 2 A_2 = (1 - u^2)\begin{pmatrix} 1 & u \\ u & 1+u^2 \end{pmatrix} \frac{(x, y)^T A_2 (x, y)}{1-u^2} = x^2 + (1 +u^2)y^2 + 2xyu \geq x^2 + u^2 y^2 + 2xyu = (x + uy)^2 \geq 0.
 n \geq 3 A_n","['linear-algebra', 'inequality', 'positive-definite', 'positive-semidefinite']"
58,Find the eigenvalues of a 5x5 (symmetric) matrix containing a null 4x4 matrix,Find the eigenvalues of a 5x5 (symmetric) matrix containing a null 4x4 matrix,,Find the eigenvalues of $$A=\begin{bmatrix}     0 & 1 & 1 & 1 &1 \\     1 & 0& 0 & 0& 0\\     1 & 0& 0 & 0& 0\\     1 & 0& 0 & 0& 0\\     1 & 0& 0 & 0& 0 \end{bmatrix}$$ It doesn't appear to be a block matrix. I can't find a way other than explicitly calculating the determinant of $\det (A-\lambda I)$ . Is there something else i'm missing?,Find the eigenvalues of It doesn't appear to be a block matrix. I can't find a way other than explicitly calculating the determinant of . Is there something else i'm missing?,"A=\begin{bmatrix}
    0 & 1 & 1 & 1 &1 \\
    1 & 0& 0 & 0& 0\\
    1 & 0& 0 & 0& 0\\
    1 & 0& 0 & 0& 0\\
    1 & 0& 0 & 0& 0
\end{bmatrix} \det (A-\lambda I)","['linear-algebra', 'matrices', 'eigenvalues-eigenvectors']"
59,Non-diagonal n-th roots of the identity matrix,Non-diagonal n-th roots of the identity matrix,,"Q . Are there $n$ -th root analogs of this non-diagonal cube-root of the $3 \times 3$ identity matrix? \begin{align*} \left( \begin{array}{ccc}  0 & 0 & -i \\  i & 0 & 0 \\  0 & 1 & 0 \\ \end{array} \right)^3 = \left( \begin{array}{ccc}  1 & 0 & 0 \\  0 & 1 & 0 \\  0 & 0 & 1 \\ \end{array} \right) \end{align*} I am looking for $A^n=I$ , where $I$ is any dimension $\le n$ . (A naive question: I am not an expert in this area.)","Q . Are there -th root analogs of this non-diagonal cube-root of the identity matrix? I am looking for , where is any dimension . (A naive question: I am not an expert in this area.)","n 3 \times 3 \begin{align*}
\left(
\begin{array}{ccc}
 0 & 0 & -i \\
 i & 0 & 0 \\
 0 & 1 & 0 \\
\end{array}
\right)^3
= \left(
\begin{array}{ccc}
 1 & 0 & 0 \\
 0 & 1 & 0 \\
 0 & 0 & 1 \\
\end{array}
\right)
\end{align*} A^n=I I \le n","['linear-algebra', 'matrices', 'roots-of-unity']"
60,When does one show a function is well defined?,When does one show a function is well defined?,,"Sometimes in Linear Algebra, we just use linear transformations, and other times we have to check to make sure they are well defined. Since I'm not an experienced mathematician or anything, I can't seem to see the fine line between needing to show if a linear transformation is well defined or not. Could anyone who understands the nuance please explain it to me? Edit: For example, when we began working with quotient spaces we had to show addition was well defined. In problems, if we wanted to construct a linear transformation from a quotient space to a vector space, we had to show that the linear transformation was well defined and linear. In the past, however, when we constructed linear transformations between vector spaces, we just had to show that they were linear, and we didn't talk about it being well defined.","Sometimes in Linear Algebra, we just use linear transformations, and other times we have to check to make sure they are well defined. Since I'm not an experienced mathematician or anything, I can't seem to see the fine line between needing to show if a linear transformation is well defined or not. Could anyone who understands the nuance please explain it to me? Edit: For example, when we began working with quotient spaces we had to show addition was well defined. In problems, if we wanted to construct a linear transformation from a quotient space to a vector space, we had to show that the linear transformation was well defined and linear. In the past, however, when we constructed linear transformations between vector spaces, we just had to show that they were linear, and we didn't talk about it being well defined.",,"['linear-algebra', 'functions', 'linear-transformations']"
61,Prove $\mathrm{tr}(A^2) \leq\mathrm{tr}(A^TA)$,Prove,\mathrm{tr}(A^2) \leq\mathrm{tr}(A^TA),"Prove that $\mathrm{tr}(A^2) \leq \mathrm{tr}(A^TA)$ . I saw the below link before and think it is related to this question $A,B$ be Hermitian.Is this true that $tr[(AB)^2]\le tr(A^2B^2)$? but still can't solve it. I also tried to use $\mathrm{tr}(A^TA)\geq0$ .","Prove that . I saw the below link before and think it is related to this question $A,B$ be Hermitian.Is this true that $tr[(AB)^2]\le tr(A^2B^2)$? but still can't solve it. I also tried to use .",\mathrm{tr}(A^2) \leq \mathrm{tr}(A^TA) \mathrm{tr}(A^TA)\geq0,"['linear-algebra', 'matrices', 'trace']"
62,How to confirm the $k$-th power of this matrix?,How to confirm the -th power of this matrix?,k,"For any $k \in \mathbb N$, $$\begin{bmatrix}2&-2\\0.5&0\end{bmatrix}^k = \begin{bmatrix}k+1&-2k\\k/2&-(k-1)\end{bmatrix}$$ I found this empirically using numerical computation, but I cannot prove it. I have tried computing the eigendecomposition, but this matrix is not diagonalizable.","For any $k \in \mathbb N$, $$\begin{bmatrix}2&-2\\0.5&0\end{bmatrix}^k = \begin{bmatrix}k+1&-2k\\k/2&-(k-1)\end{bmatrix}$$ I found this empirically using numerical computation, but I cannot prove it. I have tried computing the eigendecomposition, but this matrix is not diagonalizable.",,"['linear-algebra', 'matrices']"
63,Gradient of a function as the direction of steepest ascent/descent,Gradient of a function as the direction of steepest ascent/descent,,"I am trying to really understand why the gradient of a function gives the direction of steepest ascent intuitively. Assuming that the function is differentiable at the point in question, a) I had a look at a few resources online and also looked at this Why is gradient the direction of steepest ascent? , a popular question on this stackexchange site. The accepted answer basically says that we multiply the gradient with an arbitrary vector and then say that the product is maximum when the vector points in the same direction as the gradient? This to me really does not answer the question, but it has 31 upvotes so can someone please point out what I am obviously missing? b) Does the gradient of a function tell us a way to reach the maxima or minima? if yes, then how and which one - maxima or minima or both? Edit: I read the gradient descent algorithm and that answers this part of my question. c) Since gradient is a feature of the function at some particular point - am I right in assuming that it can only point to the local maxima or minima?","I am trying to really understand why the gradient of a function gives the direction of steepest ascent intuitively. Assuming that the function is differentiable at the point in question, a) I had a look at a few resources online and also looked at this Why is gradient the direction of steepest ascent? , a popular question on this stackexchange site. The accepted answer basically says that we multiply the gradient with an arbitrary vector and then say that the product is maximum when the vector points in the same direction as the gradient? This to me really does not answer the question, but it has 31 upvotes so can someone please point out what I am obviously missing? b) Does the gradient of a function tell us a way to reach the maxima or minima? if yes, then how and which one - maxima or minima or both? Edit: I read the gradient descent algorithm and that answers this part of my question. c) Since gradient is a feature of the function at some particular point - am I right in assuming that it can only point to the local maxima or minima?",,"['linear-algebra', 'gradient-descent']"
64,Is this true that $\mathrm{Tr}(ABC)=\mathrm{Tr}(ACB)$? [closed],Is this true that ? [closed],\mathrm{Tr}(ABC)=\mathrm{Tr}(ACB),"Closed. This question is off-topic . It is not currently accepting answers. This question is missing context or other details : Please improve the question by providing additional context, which ideally includes your thoughts on the problem and any attempts you have made to solve it. This information helps others identify where you have difficulties and helps them write answers appropriate to your experience level. Closed 7 years ago . Improve this question Let $A,B,C\in M_n$. Is this true that $\mathrm{Tr}(ABC)=\mathrm{Tr}(ACB)$?","Closed. This question is off-topic . It is not currently accepting answers. This question is missing context or other details : Please improve the question by providing additional context, which ideally includes your thoughts on the problem and any attempts you have made to solve it. This information helps others identify where you have difficulties and helps them write answers appropriate to your experience level. Closed 7 years ago . Improve this question Let $A,B,C\in M_n$. Is this true that $\mathrm{Tr}(ABC)=\mathrm{Tr}(ACB)$?",,"['linear-algebra', 'matrices']"
65,Find an equation of the plane perpendicular to vector v and passing through the tip of u,Find an equation of the plane perpendicular to vector v and passing through the tip of u,,"The given vectors are $v = (3,0,1)$ and $u = (3,1,0)$. I have used the following formula and plugged in what I have been given using vector $v$ as the variables for $i,j,k$ and vector $u$ for $x_0,y_0,z_0$ and have worked out the solution as follows: $$3(x-3) - 1(y-1) + 1(z-0) = 0$$ ultimately having $3x - y - 8 = 0$ as the equation of the plane. Have I understood this correctly? Any advice appreciated!","The given vectors are $v = (3,0,1)$ and $u = (3,1,0)$. I have used the following formula and plugged in what I have been given using vector $v$ as the variables for $i,j,k$ and vector $u$ for $x_0,y_0,z_0$ and have worked out the solution as follows: $$3(x-3) - 1(y-1) + 1(z-0) = 0$$ ultimately having $3x - y - 8 = 0$ as the equation of the plane. Have I understood this correctly? Any advice appreciated!",,"['linear-algebra', 'vectors']"
66,Matrix with integer entries,Matrix with integer entries,,"The determinant of a given square matrix $A$, with rational entries, equals 1. It is known that all entries of $A^{2015}$ are integers. Is it true that all entries of $A$ are integers? My attemt : I've tried to construct a counter example but failed. I believe it's true but don't know how to prove it.","The determinant of a given square matrix $A$, with rational entries, equals 1. It is known that all entries of $A^{2015}$ are integers. Is it true that all entries of $A$ are integers? My attemt : I've tried to construct a counter example but failed. I believe it's true but don't know how to prove it.",,"['linear-algebra', 'matrices', 'determinant']"
67,Transpose Operator is diagonalizable?,Transpose Operator is diagonalizable?,,"Let $T \colon \mathbb{M}_{n\times n}(\mathbb{R}) \to \mathbb{M}_{n\times n}(\mathbb{R})$ the linear operator such that $T(M)=M^t$, where $M^t$ is the transpose of the matrix $M$. Prove that $T$ is diagonalizable. I thought I'd follow the path that $T^3=T$ but I have not resolved the problem.","Let $T \colon \mathbb{M}_{n\times n}(\mathbb{R}) \to \mathbb{M}_{n\times n}(\mathbb{R})$ the linear operator such that $T(M)=M^t$, where $M^t$ is the transpose of the matrix $M$. Prove that $T$ is diagonalizable. I thought I'd follow the path that $T^3=T$ but I have not resolved the problem.",,['linear-algebra']
68,Find bases for eigenspaces of A,Find bases for eigenspaces of A,,$$A = \begin{pmatrix} 6 & 4 \\ -3 & -1\end{pmatrix}$$ Find the bases for eigenspaces $E_{\lambda_1}$ and $E_{\lambda_2}$ of $A$. I don't really know where to start on this problem.,$$A = \begin{pmatrix} 6 & 4 \\ -3 & -1\end{pmatrix}$$ Find the bases for eigenspaces $E_{\lambda_1}$ and $E_{\lambda_2}$ of $A$. I don't really know where to start on this problem.,,"['linear-algebra', 'matrices', 'diagonalization', 'matrix-decomposition']"
69,"If $A$ and $B$ are symmetric matrices, so is $A+B$","If  and  are symmetric matrices, so is",A B A+B,"This question looks pretty easy and hard at the same time. Here's how it goes: Let A and B be symmmetric nxn matrices. Show that A + B is also symmetric. To me this sounds like a pretty obvious fact, well, if its not obvious I still have a good idea of why this is true, but I'm new to university level maths and I don't really know how I would go about writing a formal proof for this. Any help would be much appreciated. =D","This question looks pretty easy and hard at the same time. Here's how it goes: Let A and B be symmmetric nxn matrices. Show that A + B is also symmetric. To me this sounds like a pretty obvious fact, well, if its not obvious I still have a good idea of why this is true, but I'm new to university level maths and I don't really know how I would go about writing a formal proof for this. Any help would be much appreciated. =D",,"['linear-algebra', 'proof-writing']"
70,"Show that if $A^3=0$ but $A^2\ne0$, then $A^2v=0$ has a nontrivial solution","Show that if  but , then  has a nontrivial solution",A^3=0 A^2\ne0 A^2v=0,"Question. Let $A$ be a $3\times 3$ matrix. If $A^3=0$ and $A^2 \neq 0$, prove that $A^2v=0$ for some $v\in\mathbb{R}^3\setminus0$. To generalize the solution I'm defining $A$ as: \begin{pmatrix} a_1 & a_2 & a_3\\  a_4 & a_5 & a_6\\  a_7 & a_8 & a_9 \end{pmatrix} and have calculated $A^2$ to be: \begin{pmatrix} a_1^2+a_2a_4+a_3a_7 & a_1 a_2+a_2 a_5+a_3 a_8 & a_1 a_3+a_2 a_6+a_3 a_9\\  a_4 a_1+a_5 a_4+a_6 a_7 & a_4 a_2+a_5^2+a_6 a_8 & a_4 a_3+a_5 a_6+a_6 a_9\\  a_7 a_1+a_8 a_4+a_9 a_7 & a_7 a_2+a_8 a_5+a_9 a_8 & a_7 a_3+a_8 a_6+a_9^2 \end{pmatrix} Just the thought of having to calculate $A^3$ gives me a headache... I've read something about diagnolization , but I couldn't apply it here. I assume there must be some characteristic of a matrix $A$ for which $A^3=0$ I couldn't think about.","Question. Let $A$ be a $3\times 3$ matrix. If $A^3=0$ and $A^2 \neq 0$, prove that $A^2v=0$ for some $v\in\mathbb{R}^3\setminus0$. To generalize the solution I'm defining $A$ as: \begin{pmatrix} a_1 & a_2 & a_3\\  a_4 & a_5 & a_6\\  a_7 & a_8 & a_9 \end{pmatrix} and have calculated $A^2$ to be: \begin{pmatrix} a_1^2+a_2a_4+a_3a_7 & a_1 a_2+a_2 a_5+a_3 a_8 & a_1 a_3+a_2 a_6+a_3 a_9\\  a_4 a_1+a_5 a_4+a_6 a_7 & a_4 a_2+a_5^2+a_6 a_8 & a_4 a_3+a_5 a_6+a_6 a_9\\  a_7 a_1+a_8 a_4+a_9 a_7 & a_7 a_2+a_8 a_5+a_9 a_8 & a_7 a_3+a_8 a_6+a_9^2 \end{pmatrix} Just the thought of having to calculate $A^3$ gives me a headache... I've read something about diagnolization , but I couldn't apply it here. I assume there must be some characteristic of a matrix $A$ for which $A^3=0$ I couldn't think about.",,"['linear-algebra', 'matrices']"
71,Proving AM ≥ GM in 3 variables using the Cauchy-Schwarz inequality,Proving AM ≥ GM in 3 variables using the Cauchy-Schwarz inequality,,"This question is from the textbook Introduction to Linear Algebra by Gilbert Strang, where the author has asked to prove that $$\sqrt[3]{xyz} \le \frac{x+y+z}{3}. $$ The only equations at hand are the Schwarz inequality : $$\newcommand{\norm}[1]{\left\lVert#1\right\rVert} \left | \vec u \ {\large\cdot} \ \vec v \right| \le \norm{\vec u} \ \norm{\vec v},$$ and the Triangle inequality : $$ \norm{\vec u +\vec v} \le \norm{\vec u} + \norm{\vec v}.$$ For only two variables $x$ and $y$, I can prove it in the following way: Let's say $\vec u = (x,y)$ and $\vec v = (y,x)$. Then by the Schwarz inequality, $$\left | 2xy \right | \le x^2 + y^2 \implies \sqrt{x^2y^2} \le \frac{x^2+y^2}{2}$$ But I am unable to extend it to 3 variables. Could you please help.","This question is from the textbook Introduction to Linear Algebra by Gilbert Strang, where the author has asked to prove that $$\sqrt[3]{xyz} \le \frac{x+y+z}{3}. $$ The only equations at hand are the Schwarz inequality : $$\newcommand{\norm}[1]{\left\lVert#1\right\rVert} \left | \vec u \ {\large\cdot} \ \vec v \right| \le \norm{\vec u} \ \norm{\vec v},$$ and the Triangle inequality : $$ \norm{\vec u +\vec v} \le \norm{\vec u} + \norm{\vec v}.$$ For only two variables $x$ and $y$, I can prove it in the following way: Let's say $\vec u = (x,y)$ and $\vec v = (y,x)$. Then by the Schwarz inequality, $$\left | 2xy \right | \le x^2 + y^2 \implies \sqrt{x^2y^2} \le \frac{x^2+y^2}{2}$$ But I am unable to extend it to 3 variables. Could you please help.",,"['linear-algebra', 'inequality']"
72,"If $A\vec{v}=\lambda\vec{v}$, then does $A=\lambda$?","If , then does ?",A\vec{v}=\lambda\vec{v} A=\lambda,"When my professor started teaching eigenvectors and eigenvalues the other day, the very first thing I noticed was the fact that $A\vec{v}=\lambda\vec{v}$ (assuming that the equation is satisfied under the given variables) implies that $A=\lambda$... But given that A is an $n\times n$ matrix, and $\lambda\in\mathbb{R}$, how can a matrix equal a real number? What is the consequence of this?","When my professor started teaching eigenvectors and eigenvalues the other day, the very first thing I noticed was the fact that $A\vec{v}=\lambda\vec{v}$ (assuming that the equation is satisfied under the given variables) implies that $A=\lambda$... But given that A is an $n\times n$ matrix, and $\lambda\in\mathbb{R}$, how can a matrix equal a real number? What is the consequence of this?",,"['linear-algebra', 'eigenvalues-eigenvectors']"
73,Determine all $2\times2$ matrices $A$ such that $A^2=0$.,Determine all  matrices  such that .,2\times2 A A^2=0,"This is from Lang's introduction to Linear Algebra page no 61. Determine all $2\times 2$ matrices $A$ such that $A^2 = 0$. Let $A=\begin{pmatrix}    a & b \\    c & d    \end{pmatrix}$ $A^2=\begin{pmatrix}    a^2+bc & ab+bd \\    ac+cd & d^2+cb    \end{pmatrix}$ Equating all the four terms of the matrix to zero and solving, $a^2+bc=0$ and $d^2+bc=0$ gives $a=\pm d$ Solving $ab+bd=0$ and $ac+cd=0$ gives $a=-d$ OR $b=c$, which gives $a=d=0$ My question: I know that if $A$ is an $n\times n$ matrix with all the diagonal elements and all the elements below it equal to zero then $A^n=0$, so in this question  \begin{equation}    \begin{pmatrix}    0 & x \\    0 & 0    \end{pmatrix}    \end{equation} qualifies as $A$, but it does not agree with $b=c$ as in above solution, so where did I go wrong? And if I go with $b=c$, then $A^2$ does not equal $0$.","This is from Lang's introduction to Linear Algebra page no 61. Determine all $2\times 2$ matrices $A$ such that $A^2 = 0$. Let $A=\begin{pmatrix}    a & b \\    c & d    \end{pmatrix}$ $A^2=\begin{pmatrix}    a^2+bc & ab+bd \\    ac+cd & d^2+cb    \end{pmatrix}$ Equating all the four terms of the matrix to zero and solving, $a^2+bc=0$ and $d^2+bc=0$ gives $a=\pm d$ Solving $ab+bd=0$ and $ac+cd=0$ gives $a=-d$ OR $b=c$, which gives $a=d=0$ My question: I know that if $A$ is an $n\times n$ matrix with all the diagonal elements and all the elements below it equal to zero then $A^n=0$, so in this question  \begin{equation}    \begin{pmatrix}    0 & x \\    0 & 0    \end{pmatrix}    \end{equation} qualifies as $A$, but it does not agree with $b=c$ as in above solution, so where did I go wrong? And if I go with $b=c$, then $A^2$ does not equal $0$.",,"['linear-algebra', 'matrices']"
74,"If $Ax=B$ has two solution, then there must be a third one?","If  has two solution, then there must be a third one?",Ax=B,"How do I prove this conjecture? Let $A$ be a matrix, and $B$ be a column vectore. If $Ax=B$ has two solutions, then there must be a third one. Thanks in a advance!","How do I prove this conjecture? Let $A$ be a matrix, and $B$ be a column vectore. If $Ax=B$ has two solutions, then there must be a third one. Thanks in a advance!",,"['linear-algebra', 'matrices', 'vector-spaces']"
75,How to prove the uniqueness of the solution of $ax+b=0$?,How to prove the uniqueness of the solution of ?,ax+b=0,"I have no background in mathematical analysis or the like, but I am interested to know how to prove the uniqueness of the solution of $ax+b=0$? Perhaps your answers will help me to prove other uniqueness problems.","I have no background in mathematical analysis or the like, but I am interested to know how to prove the uniqueness of the solution of $ax+b=0$? Perhaps your answers will help me to prove other uniqueness problems.",,['linear-algebra']
76,Where do I start with self learning linear algebra [duplicate],Where do I start with self learning linear algebra [duplicate],,"This question already has answers here : Prerequisites/Books for A First Course in Linear Algebra (6 answers) Closed last year . I'm a physics major and I'd probably go with mathematical/theoretical physics path. Where do I start with self learning linear algebra? I'm good with proofs but I'm not comfortable with learning math without intuition or motivation behind the axioms. Still, I hate math without rigor (cookbook engineer math). I'm looking for an intro book for linear algebra. Thanks. May be my first exposure to pure math excluding intro to proofs.","This question already has answers here : Prerequisites/Books for A First Course in Linear Algebra (6 answers) Closed last year . I'm a physics major and I'd probably go with mathematical/theoretical physics path. Where do I start with self learning linear algebra? I'm good with proofs but I'm not comfortable with learning math without intuition or motivation behind the axioms. Still, I hate math without rigor (cookbook engineer math). I'm looking for an intro book for linear algebra. Thanks. May be my first exposure to pure math excluding intro to proofs.",,"['reference-request', 'linear-algebra']"
77,"Prove\Refute: for every norm in $\mathbb{R}^n: \left \| x \right \|\leq \max (\left \| x+y \right \|,\left \| x-y \right \|)$",Prove\Refute: for every norm in,"\mathbb{R}^n: \left \| x \right \|\leq \max (\left \| x+y \right \|,\left \| x-y \right \|)","I need to prove or refute that for every norm in $\mathbb{R}^n$:$ \left \| x \right \|\leq \max (\left \| x+y \right \|,\left \| x-y \right \|)$. It's been quite a while since I studied Linear algebra 1. I tried to look for vectors $x$ and $y$ such that they will refute the claim, but I didn't find any, so I tried to prove the question by showing the explicit sum of each norm, and go on form that, but it didn't do either. (sorry if the question is too easy or silly) Any help? Thank you very much!","I need to prove or refute that for every norm in $\mathbb{R}^n$:$ \left \| x \right \|\leq \max (\left \| x+y \right \|,\left \| x-y \right \|)$. It's been quite a while since I studied Linear algebra 1. I tried to look for vectors $x$ and $y$ such that they will refute the claim, but I didn't find any, so I tried to prove the question by showing the explicit sum of each norm, and go on form that, but it didn't do either. (sorry if the question is too easy or silly) Any help? Thank you very much!",,['linear-algebra']
78,"$A \in M_n(\mathbb R)$ is symmetric: If one of the entries in its diagonal is positive, prove that it has at least one positive eigenvalue","is symmetric: If one of the entries in its diagonal is positive, prove that it has at least one positive eigenvalue",A \in M_n(\mathbb R),"Let $A \in M_n(\mathbb R)$ be symmetric. Given that one of the entries in its diagonal is positive, prove that it has at least one positive eigenvalue. I didn't come to any conclusion. Thanks guys.","Let $A \in M_n(\mathbb R)$ be symmetric. Given that one of the entries in its diagonal is positive, prove that it has at least one positive eigenvalue. I didn't come to any conclusion. Thanks guys.",,['linear-algebra']
79,Pseudo Inverse Solution for Linear Equation System Using the SVD,Pseudo Inverse Solution for Linear Equation System Using the SVD,,"I read about the SVD theory and its usage for solving Linear Equation System. I saw many papers mentioning property of the solution yet no proof of it. The Property: The solution given the Pseudo Inverse ( $ V {\Sigma}^{-1} {U}^{H} $ ) minimizes both the error norm and the solution norm. The minimization of the error norm is easily proved using the Normal Equations ( $ \hat{x} $ is the Least Squares solution iff $ {A}^{H} A \hat{x} = {A}^{H} b $ ). Yet beyond the intuition of $ \hat{x} $ must lie in the Row Space of A hence its norm is minimized I couldn't find a formal proof for that. Moreover, Let's define $ A = U \Sigma {V}^{H} $ then when we calculate its pseudo inverse we we handle $ \Sigma $ with extra care, only reversing its non zero entries. What the formal reasoning for that? Thanks!","I read about the SVD theory and its usage for solving Linear Equation System. I saw many papers mentioning property of the solution yet no proof of it. The Property: The solution given the Pseudo Inverse ( $ V {\Sigma}^{-1} {U}^{H} $ ) minimizes both the error norm and the solution norm. The minimization of the error norm is easily proved using the Normal Equations ( $ \hat{x} $ is the Least Squares solution iff $ {A}^{H} A \hat{x} = {A}^{H} b $ ). Yet beyond the intuition of $ \hat{x} $ must lie in the Row Space of A hence its norm is minimized I couldn't find a formal proof for that. Moreover, Let's define $ A = U \Sigma {V}^{H} $ then when we calculate its pseudo inverse we we handle $ \Sigma $ with extra care, only reversing its non zero entries. What the formal reasoning for that? Thanks!",,['linear-algebra']
80,"Is my proof wrong? ""Prove that the additive inverse is unique""","Is my proof wrong? ""Prove that the additive inverse is unique""",,"""Prove that for each x $\in V$ , where $V$ is a vector space, the additive inverse $-x$ is unique."" My proof is as follows; Let $V$ be a vector space, let $x \in V$ . For contradiction, assume the additive inverse $-x$ is not unique. Then, there exists $-x'$ such that \begin{align*} x + (-x') = 0 \\ x + (-x) = 0 \\ \end{align*} We can rearrange this to see \begin{align*} x = -(-x') \\ x = -(-x) \\ \Rightarrow -(-x) = -(-x') \\ -x = -x' \\ \end{align*} Therefore, the additive inverse $-x$ is unique. In the book and other places I've seen other approaches to this problem, and I'm wondering if this proof is wrong? Other proofs referred to the axioms of vector spaces but I didn't find it necessary. I guess rate my proof would be the point of this question as I'm still uncertain if my proofs are solid. edit: The axioms referenced are the 8 axioms on Vector Space operations found in ""A course in linear algebra"" by David Damiano.","""Prove that for each x , where is a vector space, the additive inverse is unique."" My proof is as follows; Let be a vector space, let . For contradiction, assume the additive inverse is not unique. Then, there exists such that We can rearrange this to see Therefore, the additive inverse is unique. In the book and other places I've seen other approaches to this problem, and I'm wondering if this proof is wrong? Other proofs referred to the axioms of vector spaces but I didn't find it necessary. I guess rate my proof would be the point of this question as I'm still uncertain if my proofs are solid. edit: The axioms referenced are the 8 axioms on Vector Space operations found in ""A course in linear algebra"" by David Damiano.","\in V V -x V x \in V -x -x' \begin{align*}
x + (-x') = 0 \\
x + (-x) = 0 \\
\end{align*} \begin{align*}
x = -(-x') \\
x = -(-x) \\
\Rightarrow -(-x) = -(-x') \\
-x = -x' \\
\end{align*} -x","['linear-algebra', 'solution-verification', 'axioms']"
81,"$\langle f(t),g(t)\rangle' = \langle f'(t),g(t)\rangle + \langle f(t),g'(t)\rangle$ for differentiable $f,g : \mathcal{R} \to \mathcal{R}^n$?",for differentiable ?,"\langle f(t),g(t)\rangle' = \langle f'(t),g(t)\rangle + \langle f(t),g'(t)\rangle f,g : \mathcal{R} \to \mathcal{R}^n","Suppose that $f,g$ are differentiable functions from $\mathcal{R}$ to $\mathcal{R}^n$ . Show that $\langle f(t),g(t)\rangle' = \langle f'(t),g(t)\rangle + \langle f(t),g'(t)\rangle$ I've been banging my head against a brick wall on this for a while, I can see that the RHS is something like the product rule for differentiation. We're not given an inner product but even just trying to by parts integrate the standard inner wasn't working. Thanks in advance for any help.","Suppose that are differentiable functions from to . Show that I've been banging my head against a brick wall on this for a while, I can see that the RHS is something like the product rule for differentiation. We're not given an inner product but even just trying to by parts integrate the standard inner wasn't working. Thanks in advance for any help.","f,g \mathcal{R} \mathcal{R}^n \langle f(t),g(t)\rangle' = \langle f'(t),g(t)\rangle + \langle f(t),g'(t)\rangle","['linear-algebra', 'derivatives', 'inner-products']"
82,What is the derivative of Ax where A is a matrix and x is a vector? [closed],What is the derivative of Ax where A is a matrix and x is a vector? [closed],,"Closed . This question needs details or clarity . It is not currently accepting answers. Want to improve this question? Add details and clarify the problem by editing this post . Closed 5 years ago . Improve this question I see two different formulae. https://www.comp.nus.edu.sg/~cs5240/lecture/matrix-differentiation-c.pdf In this slides page 9, the derivative of $Ax$ is $A$. But in some other documents such as http://faculty.arts.ubc.ca/vmarmer/econ627/vector%20diff.pdf . The derivative is $A^T$. How to properly evaluate the derivative of Ax?","Closed . This question needs details or clarity . It is not currently accepting answers. Want to improve this question? Add details and clarify the problem by editing this post . Closed 5 years ago . Improve this question I see two different formulae. https://www.comp.nus.edu.sg/~cs5240/lecture/matrix-differentiation-c.pdf In this slides page 9, the derivative of $Ax$ is $A$. But in some other documents such as http://faculty.arts.ubc.ca/vmarmer/econ627/vector%20diff.pdf . The derivative is $A^T$. How to properly evaluate the derivative of Ax?",,"['calculus', 'linear-algebra']"
83,Are these vectors in the image of the matrix?,Are these vectors in the image of the matrix?,,"I took a test today and ran across this question:$$Are\space the\space vectors\space \pmatrix{ 1\\ 1\\} and\space \pmatrix{ 1\\ 20\\} in\space the\space image\space of\space the\space matrix\space \pmatrix{ -1&1\\ 1&2\\} $$ First, the answer is yes . I gave my professor two reasons: Reason #$1$: $$$$The matrix $A=\pmatrix{ -1&1\\ 1&2\\}$ is invertible, since it has a nonzero determinant, thus all of $\mathbb{R}^2$ is in the image of $A$. In other words, the row vectors form a basis for $\mathbb{R}^2$ since they are linearly independent and span $\mathbb{R}^2.$ She marked, ""explain."" Reason #$2$:$$$$ The systems $A\vec{x_1}=\vec{b_1}$ and $A\vec{x_2}=\vec{b_2}$ are consistent where $A=\pmatrix{ -1&1\\ 1&2\\}$ and $\vec{b_1}$ and $\vec{b_2}$ are $\pmatrix{ 1\\ 1\\} and\space \pmatrix{ 1\\ 20\\}$ respectively. $$$$I feel as if this is ample reason and explanation for the question in absence of a proof which was not required. Is there anything wrong with my answer? Even after asking my prof, I still could not see my error or where there was an ambiguity in my first reason.","I took a test today and ran across this question:$$Are\space the\space vectors\space \pmatrix{ 1\\ 1\\} and\space \pmatrix{ 1\\ 20\\} in\space the\space image\space of\space the\space matrix\space \pmatrix{ -1&1\\ 1&2\\} $$ First, the answer is yes . I gave my professor two reasons: Reason #$1$: $$$$The matrix $A=\pmatrix{ -1&1\\ 1&2\\}$ is invertible, since it has a nonzero determinant, thus all of $\mathbb{R}^2$ is in the image of $A$. In other words, the row vectors form a basis for $\mathbb{R}^2$ since they are linearly independent and span $\mathbb{R}^2.$ She marked, ""explain."" Reason #$2$:$$$$ The systems $A\vec{x_1}=\vec{b_1}$ and $A\vec{x_2}=\vec{b_2}$ are consistent where $A=\pmatrix{ -1&1\\ 1&2\\}$ and $\vec{b_1}$ and $\vec{b_2}$ are $\pmatrix{ 1\\ 1\\} and\space \pmatrix{ 1\\ 20\\}$ respectively. $$$$I feel as if this is ample reason and explanation for the question in absence of a proof which was not required. Is there anything wrong with my answer? Even after asking my prof, I still could not see my error or where there was an ambiguity in my first reason.",,['linear-algebra']
84,Cayley-Hamilton theorem to compute this.,Cayley-Hamilton theorem to compute this.,,"$ A = \pmatrix{0&-3&0\\3&0&0\\0&0&-1}$ Compute the $e^{At}$. Well, the first problem of this is to calculate the inverse of $A$ using Cayley-Hamilton theorem. But for this second problem, I don't know how to solve it, should I use the Cayley-Hamilton theorem?","$ A = \pmatrix{0&-3&0\\3&0&0\\0&0&-1}$ Compute the $e^{At}$. Well, the first problem of this is to calculate the inverse of $A$ using Cayley-Hamilton theorem. But for this second problem, I don't know how to solve it, should I use the Cayley-Hamilton theorem?",,"['linear-algebra', 'cayley-hamilton']"
85,Calculate the determinant $\left|\begin{smallmatrix} a&b&c&d\\ b&a&d&c\\ c&d&a&b\\d&c&b&a\end{smallmatrix}\right|$,Calculate the determinant,\left|\begin{smallmatrix} a&b&c&d\\ b&a&d&c\\ c&d&a&b\\d&c&b&a\end{smallmatrix}\right|,"Question: Calculate the following determinant $$A=\begin{vmatrix} a&b&c&d\\ b&a&d&c\\ c&d&a&b\\d&c&b&a\end{vmatrix}$$ Progress: So I apply $R1'=R1+R2+R3+R4$ and get $$A=(a+b+c+d)\begin{vmatrix} 1&1&1&1\\ b&a&d&c\\ c&d&a&b\\d&c&b&a\end{vmatrix}$$ Then, I apply $C2'=C2-C1,\, C3'=C3-C1$, etc to get $$A=(a+b+c+d)\begin{vmatrix}1&0&0&0\\b&a-b&d-b&c-b\\c&d-c&a-c&b-c\\d&c-d&b-d&a-d \end{vmatrix}$$ Thus, $$A=(a+b+c+d)\begin{vmatrix}a-b&d-b&c-b\\d-c&a-c&b-c\\c-d&b-d&a-d \end{vmatrix}$$ But now I'm stuck here. I don't really want to expand the whole thing, because it is really messy. Is there any approach that don't require much calculation?","Question: Calculate the following determinant $$A=\begin{vmatrix} a&b&c&d\\ b&a&d&c\\ c&d&a&b\\d&c&b&a\end{vmatrix}$$ Progress: So I apply $R1'=R1+R2+R3+R4$ and get $$A=(a+b+c+d)\begin{vmatrix} 1&1&1&1\\ b&a&d&c\\ c&d&a&b\\d&c&b&a\end{vmatrix}$$ Then, I apply $C2'=C2-C1,\, C3'=C3-C1$, etc to get $$A=(a+b+c+d)\begin{vmatrix}1&0&0&0\\b&a-b&d-b&c-b\\c&d-c&a-c&b-c\\d&c-d&b-d&a-d \end{vmatrix}$$ Thus, $$A=(a+b+c+d)\begin{vmatrix}a-b&d-b&c-b\\d-c&a-c&b-c\\c-d&b-d&a-d \end{vmatrix}$$ But now I'm stuck here. I don't really want to expand the whole thing, because it is really messy. Is there any approach that don't require much calculation?",,"['linear-algebra', 'determinant']"
86,Second order derivative of the inverse matrix operator,Second order derivative of the inverse matrix operator,,"Let $f : Gl_{n}(\mathbb{R}) \to Gl_{n}(\mathbb{R})$ defined by $f(X)=X^{-1}$ . Compute $f''(X)(H,K)$ . I calculated $f'(X).H=-X^{-1}HX^{-1}$ so I tried to use some composition of linear functions but did not find the appropriate functions. Can anyone help me in this matter?",Let defined by . Compute . I calculated so I tried to use some composition of linear functions but did not find the appropriate functions. Can anyone help me in this matter?,"f : Gl_{n}(\mathbb{R}) \to Gl_{n}(\mathbb{R}) f(X)=X^{-1} f''(X)(H,K) f'(X).H=-X^{-1}HX^{-1}","['linear-algebra', 'matrices', 'derivatives', 'matrix-calculus', 'hessian-matrix']"
87,Prove this polynomial falls within $\mathbb R[x]$,Prove this polynomial falls within,\mathbb R[x],"[ The problem below is from Yao Musheng (姚慕生), Wu Quanshui (吴泉水), Advanced Algebra (高等代数学) Ed $2$, Fudan University Press, page $207$. ] Suppose $f(x)\in \mathbb C[x]$. If $\forall c\in \mathbb R$, $f(c)\in\mathbb R$, prove:   $$f(x)\in \mathbb R[x].$$ My attempt is as follows: For convenience I first specify that and $\deg f(x)\ge 2$ (otherwise it is easy to prove). Suppose $f(x)=a_0+a_1x+\cdots+a_{n-1}x^{n-1}+a_nx^n\notin \mathbb R[x]$ , then at least one of its coefficients, say $a_i$ is not real number. Therefore let $a_i=A+Bi$ where $B\ne 0$, and let $\tilde{f}(x):=f(x)-a_ix^i$. If $\tilde{x}\ne 0$ and it satisfies $\tilde{f}(\tilde{x})=0$, then  $$f(\tilde{x})=a_i\tilde{x}^i$$ If $\tilde{x}$ is real, then so is $\tilde{x}^i$. But $a_i$ is not real, thus $f(\tilde{x})=a_i\tilde{x}^i$ is not real. However, since $\tilde x\in\mathbb R$, $f(\tilde{x})$ must be real. Contradiction. Therefore $\tilde{x}$ cannot be real. Then I don't know how to proceed. I have tried disproving the possibility of $\tilde{x}=0$ case because that doesn't seem to lead to anything. But I also failed. Seems that I have got on the wrong track into a dead end. I am really struggling with this problem now. Could anybody drop a hint or help me out? Best regards.","[ The problem below is from Yao Musheng (姚慕生), Wu Quanshui (吴泉水), Advanced Algebra (高等代数学) Ed $2$, Fudan University Press, page $207$. ] Suppose $f(x)\in \mathbb C[x]$. If $\forall c\in \mathbb R$, $f(c)\in\mathbb R$, prove:   $$f(x)\in \mathbb R[x].$$ My attempt is as follows: For convenience I first specify that and $\deg f(x)\ge 2$ (otherwise it is easy to prove). Suppose $f(x)=a_0+a_1x+\cdots+a_{n-1}x^{n-1}+a_nx^n\notin \mathbb R[x]$ , then at least one of its coefficients, say $a_i$ is not real number. Therefore let $a_i=A+Bi$ where $B\ne 0$, and let $\tilde{f}(x):=f(x)-a_ix^i$. If $\tilde{x}\ne 0$ and it satisfies $\tilde{f}(\tilde{x})=0$, then  $$f(\tilde{x})=a_i\tilde{x}^i$$ If $\tilde{x}$ is real, then so is $\tilde{x}^i$. But $a_i$ is not real, thus $f(\tilde{x})=a_i\tilde{x}^i$ is not real. However, since $\tilde x\in\mathbb R$, $f(\tilde{x})$ must be real. Contradiction. Therefore $\tilde{x}$ cannot be real. Then I don't know how to proceed. I have tried disproving the possibility of $\tilde{x}=0$ case because that doesn't seem to lead to anything. But I also failed. Seems that I have got on the wrong track into a dead end. I am really struggling with this problem now. Could anybody drop a hint or help me out? Best regards.",,"['linear-algebra', 'abstract-algebra', 'polynomials', 'field-theory']"
88,Can a matrix have the same range and nullspace?,Can a matrix have the same range and nullspace?,,"If you can pick any $3\times 3$ matrix, is there a matrix that its $R(A) = N(A)$? If you can pick any $4\times 4$ matrix, is $R(A) = N(A)$ possible? Here, $R(A)$ is the range of matrix A, and $N(A)$ is the nullspace of matrix A I thought the answers to both questions would be ""no"" because $R(A)$ is obtained from $Ax=b$, where $b \not= 0$ $N(A)$ is obtained from $Ax=0$. The vectors in both spaces will never be the same. I wonder why the question would be structured in such a way.","If you can pick any $3\times 3$ matrix, is there a matrix that its $R(A) = N(A)$? If you can pick any $4\times 4$ matrix, is $R(A) = N(A)$ possible? Here, $R(A)$ is the range of matrix A, and $N(A)$ is the nullspace of matrix A I thought the answers to both questions would be ""no"" because $R(A)$ is obtained from $Ax=b$, where $b \not= 0$ $N(A)$ is obtained from $Ax=0$. The vectors in both spaces will never be the same. I wonder why the question would be structured in such a way.",,['linear-algebra']
89,Questioning the need of Dual Space,Questioning the need of Dual Space,,"I am dealing with the dual spaces for the first time. I just wanted to ask is their any practical application of Dual space or is it just some random mathematical thing? If there is, please give a few.","I am dealing with the dual spaces for the first time. I just wanted to ask is their any practical application of Dual space or is it just some random mathematical thing? If there is, please give a few.",,['linear-algebra']
90,Difference between Euclidean space and inner product space?,Difference between Euclidean space and inner product space?,,Is it that Inner product space can have infinite dimensions?,Is it that Inner product space can have infinite dimensions?,,"['linear-algebra', 'inner-products']"
91,Eigenvalues of product/sum of two matrices,Eigenvalues of product/sum of two matrices,,"Find an example of matrices, $A$ and $B$ , with $AB=BA$ and for which $\lambda$ is an eigenvalue of $A$ , $\mu$ an eigenvalue of $B$ , but $\lambda+\mu$ is not an eigenvalue of $A+B$ , and $\lambda \mu$ not an eigenvalue of $AB$ . Can anyone please provide an example of two such matrices?","Find an example of matrices, and , with and for which is an eigenvalue of , an eigenvalue of , but is not an eigenvalue of , and not an eigenvalue of . Can anyone please provide an example of two such matrices?",A B AB=BA \lambda A \mu B \lambda+\mu A+B \lambda \mu AB,"['linear-algebra', 'matrices', 'eigenvalues-eigenvectors']"
92,Inverse eigenvalue of a linear transformation [duplicate],Inverse eigenvalue of a linear transformation [duplicate],,"This question already has answers here : If A is invertible, prove that $\lambda \neq 0$, and $\vec{v}$ is also an eigenvector for $A^{-1}$, what is the corresponding eigenvalue? (3 answers) Closed 10 years ago . T is a linear transformation, and $\lambda$ is N eigenvalue of T. How do I prove that $\lambda^{-1}$ is an eigenvalue for $T^{-1}$? I know for a matrix, I can use the fact that $Av=\lambda v$, but how does a linear transformation work?","This question already has answers here : If A is invertible, prove that $\lambda \neq 0$, and $\vec{v}$ is also an eigenvector for $A^{-1}$, what is the corresponding eigenvalue? (3 answers) Closed 10 years ago . T is a linear transformation, and $\lambda$ is N eigenvalue of T. How do I prove that $\lambda^{-1}$ is an eigenvalue for $T^{-1}$? I know for a matrix, I can use the fact that $Av=\lambda v$, but how does a linear transformation work?",,"['linear-algebra', 'matrices', 'eigenvalues-eigenvectors']"
93,why is the square of this matrix with sin and cos equal to the identity matrix?,why is the square of this matrix with sin and cos equal to the identity matrix?,,"I have a question about why the square of the  matrix  Q, below, is equal to the identity matrix. Q = cos X  -sin X    sin X   cos X My knowledge of trigonometry seems to have rotted away, and I can't  figure out what rules are used to justify the statement  made by professor Strang at 11:55 of this video > Lec 20 | MIT 18.06 Linear Algebra, Spring 2005 (he says the square of  Q is equal to I) The thing that has me stuck is the following:   In order to calculate the  first cell of  Q * Q   we take the dot product of the first row and the  first column.  That would be cos^2 Q  -  sin^2 Q But how is that equal to the ""1"" that we want in the upper left most cell  of the identity matrix ? If this were        cos^2 Q  +  sin^2 Q then it would be equal to 1.. but we have a difference here, not a sum. Thanks in advance !    chris epilogue:  thanks for the answers ! I don't know how I missed the fact that he was not making this claim for Q^2,  but  Q-transpose * Q, instead. Appreciate your pointing it out.","I have a question about why the square of the  matrix  Q, below, is equal to the identity matrix. Q = cos X  -sin X    sin X   cos X My knowledge of trigonometry seems to have rotted away, and I can't  figure out what rules are used to justify the statement  made by professor Strang at 11:55 of this video > Lec 20 | MIT 18.06 Linear Algebra, Spring 2005 (he says the square of  Q is equal to I) The thing that has me stuck is the following:   In order to calculate the  first cell of  Q * Q   we take the dot product of the first row and the  first column.  That would be cos^2 Q  -  sin^2 Q But how is that equal to the ""1"" that we want in the upper left most cell  of the identity matrix ? If this were        cos^2 Q  +  sin^2 Q then it would be equal to 1.. but we have a difference here, not a sum. Thanks in advance !    chris epilogue:  thanks for the answers ! I don't know how I missed the fact that he was not making this claim for Q^2,  but  Q-transpose * Q, instead. Appreciate your pointing it out.",,"['linear-algebra', 'matrices', 'trigonometry']"
94,Show that if $A$ is positive definite then $A + A^{-1} - 2I$ is positive semidefinite,Show that if  is positive definite then  is positive semidefinite,A A + A^{-1} - 2I,"Let $A$ be a real symmetric positive definite matrix. Show that $$A + A^{-1} -2I$$ is positive semidefinite. I found that $A^{-1}$ is a positive definite matrix, thus $A + A^{-1}$ is also a positive definite matrix, moreover I know the form of the $z^TIz$ is as follows $(a^2 + b^2 + ... )$, where $a,b, ...$ are the components of vector $z$.  I don't know what to do next...","Let $A$ be a real symmetric positive definite matrix. Show that $$A + A^{-1} -2I$$ is positive semidefinite. I found that $A^{-1}$ is a positive definite matrix, thus $A + A^{-1}$ is also a positive definite matrix, moreover I know the form of the $z^TIz$ is as follows $(a^2 + b^2 + ... )$, where $a,b, ...$ are the components of vector $z$.  I don't know what to do next...",,"['linear-algebra', 'matrices', 'positive-semidefinite']"
95,project a point in 3D on a given plane,project a point in 3D on a given plane,,"A point in a 3D space is given as $ P(x,y,z) $. I want to find the position of this point projected parallel to the normal on a plane Q defined by $3$ non-collinear points $ Q1(x1,y1,z1), Q2(x2,y2,z2)\; and \;Q3(x3,y3,z3) $. How to derive this point of projection? I saw this entry on Wikipedia but I cannot understand it. I'd prefer an answer that gives the derivation of a $4$-ary transformation function $f$ such that $ f \; (P, Q1, Q2, Q3) = P' $ where P' is a point in 2D with co-ordinates as $ P'(x',y') $ such that P' is the required projection of P on the plane Q.","A point in a 3D space is given as $ P(x,y,z) $. I want to find the position of this point projected parallel to the normal on a plane Q defined by $3$ non-collinear points $ Q1(x1,y1,z1), Q2(x2,y2,z2)\; and \;Q3(x3,y3,z3) $. How to derive this point of projection? I saw this entry on Wikipedia but I cannot understand it. I'd prefer an answer that gives the derivation of a $4$-ary transformation function $f$ such that $ f \; (P, Q1, Q2, Q3) = P' $ where P' is a point in 2D with co-ordinates as $ P'(x',y') $ such that P' is the required projection of P on the plane Q.",,"['linear-algebra', 'geometry', 'coordinate-systems']"
96,"How to solve these equations with $x^2$, $xy$, and $y^2$?","How to solve these equations with , , and ?",x^2 xy y^2,"Is there any good method to solve equations like this? $$ \begin{cases} 2y-2xy-y^2=0\\2x-x^2-2xy=0\end{cases} $$ This is what I did: $$ \begin{cases} y(2-2x-y)=0\\x(2-x-2y)=0\end{cases} $$ now I see, that: $$ x=0 $$ $$ y=0 $$ and it's the first solution, now: $$ \begin{cases} 2-2x-y=0\\2-x-2y=0\end{cases} $$ and: $$ x=\frac{2}{3}, y=\frac{2}{3} $$ I have 2 solutions, but wolfram found more... Is there any good method for equations like this?","Is there any good method to solve equations like this? $$ \begin{cases} 2y-2xy-y^2=0\\2x-x^2-2xy=0\end{cases} $$ This is what I did: $$ \begin{cases} y(2-2x-y)=0\\x(2-x-2y)=0\end{cases} $$ now I see, that: $$ x=0 $$ $$ y=0 $$ and it's the first solution, now: $$ \begin{cases} 2-2x-y=0\\2-x-2y=0\end{cases} $$ and: $$ x=\frac{2}{3}, y=\frac{2}{3} $$ I have 2 solutions, but wolfram found more... Is there any good method for equations like this?",,"['linear-algebra', 'algebra-precalculus']"
97,Matrix Determinant,Matrix Determinant,,"So I'm reading through my linear algebra textbook to review for my final, and happened upon this statement: The determinant of a matrix with positive entries must be positive. Off the top of my head, I can think of an exception to this: $$A=\begin{bmatrix}1 & 2\\8 & 3\end{bmatrix}$$ where $\det A= (1\cdot 3) - (2\cdot 8) = 3 - 16 = -13$ Am I misinterpreting what I am reading, or is this a misprint? The book is Elementary Linear Algebra, 2nd ed. by Spence, Insel, Friedberg","So I'm reading through my linear algebra textbook to review for my final, and happened upon this statement: The determinant of a matrix with positive entries must be positive. Off the top of my head, I can think of an exception to this: $$A=\begin{bmatrix}1 & 2\\8 & 3\end{bmatrix}$$ where $\det A= (1\cdot 3) - (2\cdot 8) = 3 - 16 = -13$ Am I misinterpreting what I am reading, or is this a misprint? The book is Elementary Linear Algebra, 2nd ed. by Spence, Insel, Friedberg",,"['linear-algebra', 'matrices', 'determinant']"
98,Showing that this is a group under matrix multiplication,Showing that this is a group under matrix multiplication,,I need to prove that $P=\left\{A\in M_2(\mathbb{R})\mid A^TXA = X\right\}$ is a group under matrix multiplication.,I need to prove that $P=\left\{A\in M_2(\mathbb{R})\mid A^TXA = X\right\}$ is a group under matrix multiplication.,,"['linear-algebra', 'group-theory']"
99,How can I find the nth exponent of the matrix using the diagonalization algorithm?,How can I find the nth exponent of the matrix using the diagonalization algorithm?,,"Hey guys, simple question in linear algebra. I want to find the nth exponent of this matrix: $$ \left[ \begin{array}{cc}  1 & 1 \\ 0 & 2 \end{array} \right] $$ I'm trying to use the diagonal algorithm, first by finding the eigenvalues, so I get $t_1=1$, and $t_2=2$, and than I realize that the matrix is not diagonalizable... So how can I still calculate the nth exponent? Thanks","Hey guys, simple question in linear algebra. I want to find the nth exponent of this matrix: $$ \left[ \begin{array}{cc}  1 & 1 \\ 0 & 2 \end{array} \right] $$ I'm trying to use the diagonal algorithm, first by finding the eigenvalues, so I get $t_1=1$, and $t_2=2$, and than I realize that the matrix is not diagonalizable... So how can I still calculate the nth exponent? Thanks",,['linear-algebra']
