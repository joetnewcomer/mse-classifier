,title_raw,title_text,title_latex,body_raw,body_text,body_latex,tags
0,"Expected value, variance and probability from a joint distribution function","Expected value, variance and probability from a joint distribution function",,"Lets say I am given the following table that shows the joint probability function of X and Y: $$\begin{array} \\{}&y=1&y=2&y=3 \\x_=1&0.1&0.2&0.1 \\x=2&0.1&0.25&0.15 \\x=3&0&0&0.1 \end{array}$$ Then I am fairly sure the marginal probability functions of $X$ and $Y$ are given by their totals across each row and column, eg: $$\begin{array} \\{}&y=1&y=2&y=3&P_X(X) \\x=1&0.1&0.2&0.1&0.4 \\x=2&0.1&0.25&0.15&0.5 \\x=3&0&0&0.1&0.1 \\P_Y(Y)&0.2&0.45&0.35&1 \end{array}$$ I want to workout $$E(X),E(Y),Var(X),Var(Y),Cov(X,Y)$$ Now I assume that E(X) comes from P(X) here, so I get $ 0.4*1 + 0.5 * 2 + 0.1 * 3=1.7$ E(Y) same procedure $= 2.15$ Var(X) = $ 0.4*1^2 + 0.5 * 2^2 + 0.1 * 3^2 - 1.7^2 = .41$ Var(Y) same procedure = .5275 Cov(X,Y) I am not sure how to calculate this really. Is any of the above wrong? Any hints on solving Cov(X,Y)? Thank you for your time, and I am sorry about any bad latex formatting, this much latex actually slows my little laptop to a crawl.","Lets say I am given the following table that shows the joint probability function of X and Y: $$\begin{array} \\{}&y=1&y=2&y=3 \\x_=1&0.1&0.2&0.1 \\x=2&0.1&0.25&0.15 \\x=3&0&0&0.1 \end{array}$$ Then I am fairly sure the marginal probability functions of $X$ and $Y$ are given by their totals across each row and column, eg: $$\begin{array} \\{}&y=1&y=2&y=3&P_X(X) \\x=1&0.1&0.2&0.1&0.4 \\x=2&0.1&0.25&0.15&0.5 \\x=3&0&0&0.1&0.1 \\P_Y(Y)&0.2&0.45&0.35&1 \end{array}$$ I want to workout $$E(X),E(Y),Var(X),Var(Y),Cov(X,Y)$$ Now I assume that E(X) comes from P(X) here, so I get $ 0.4*1 + 0.5 * 2 + 0.1 * 3=1.7$ E(Y) same procedure $= 2.15$ Var(X) = $ 0.4*1^2 + 0.5 * 2^2 + 0.1 * 3^2 - 1.7^2 = .41$ Var(Y) same procedure = .5275 Cov(X,Y) I am not sure how to calculate this really. Is any of the above wrong? Any hints on solving Cov(X,Y)? Thank you for your time, and I am sorry about any bad latex formatting, this much latex actually slows my little laptop to a crawl.",,"['probability', 'statistics', 'probability-distributions']"
1,Mean of two normal variates,Mean of two normal variates,,If $X$ and $Y$ are independent standard normal variates find the mean value of the greater of $|X|$ and $|Y|$,If $X$ and $Y$ are independent standard normal variates find the mean value of the greater of $|X|$ and $|Y|$,,"['statistics', 'probability-theory']"
2,Fishers Exact test alternative to chi-squared?,Fishers Exact test alternative to chi-squared?,,"Background* strong text * Am analysing results from a questionnaire (n=130). The respondents fall into four categories (A / B / C / D), and each question/statement has 5 possible answers (Likert Scale so Disagree/Agree etc). I want to see if there are any significant differences or similarities between the response and the group that they belong too. If I use the chi-squared test, approx 75% cells have a expected count LESS than 5 - even when I pool the responses from 5 to 3 by merging Strongly Agree and Agree, then it is still 40% of cells have an expected count less than 5 (the threshold is 20%). Conclusion = can't use chi-squared for stat analysis Question Is an acceptable alternative the Fishers Exact Test? http://en.wikipedia.org/wiki/Fisher 's_exact_test And because the the contingency tables is 4x5 (my understanding is that Fishers Exact test should be used for 2x2), is it okay to use the Monte Carlo method: http://en.wikipedia.org/wiki/Monte_Carlo_method Any other suggestions welcome. Thanks! (PS I'm a biologist so my stats know-how is a little limited)","Background* strong text * Am analysing results from a questionnaire (n=130). The respondents fall into four categories (A / B / C / D), and each question/statement has 5 possible answers (Likert Scale so Disagree/Agree etc). I want to see if there are any significant differences or similarities between the response and the group that they belong too. If I use the chi-squared test, approx 75% cells have a expected count LESS than 5 - even when I pool the responses from 5 to 3 by merging Strongly Agree and Agree, then it is still 40% of cells have an expected count less than 5 (the threshold is 20%). Conclusion = can't use chi-squared for stat analysis Question Is an acceptable alternative the Fishers Exact Test? http://en.wikipedia.org/wiki/Fisher 's_exact_test And because the the contingency tables is 4x5 (my understanding is that Fishers Exact test should be used for 2x2), is it okay to use the Monte Carlo method: http://en.wikipedia.org/wiki/Monte_Carlo_method Any other suggestions welcome. Thanks! (PS I'm a biologist so my stats know-how is a little limited)",,['statistics']
3,"Confidence interval for n=1, unknown standard deviation","Confidence interval for n=1, unknown standard deviation",,"Is there any way to calculate a confidence interval (or otherwise gauge the reliability of a sample) when you have a sample size of one and you don't know the population standard deviation? I work at a farm. We sample our soil yearly for analysis and fertilize recommendations. For each relatively homogeneous field, the standard procedure is to take a number of samples, mix them together, and send a total of 1 lb of soil to the lab. If the field is 5 acres , that would be 1 lb of soil out of $\approx$ 10 millions . The results would say that we have say 3 lbs of phosphorus per acre . Fertilizer recommendations would then be something like: For < 2 lbs phosphorus per acre , add 75 lbs of rock phosphate; for 2-4 lbs per acre , add 65 lbs ; etc... I've never been totally comfortable with this procedure, as I can't figure out how to put a confidence interval around the results. Is there a way to do so?","Is there any way to calculate a confidence interval (or otherwise gauge the reliability of a sample) when you have a sample size of one and you don't know the population standard deviation? I work at a farm. We sample our soil yearly for analysis and fertilize recommendations. For each relatively homogeneous field, the standard procedure is to take a number of samples, mix them together, and send a total of 1 lb of soil to the lab. If the field is 5 acres , that would be 1 lb of soil out of $\approx$ 10 millions . The results would say that we have say 3 lbs of phosphorus per acre . Fertilizer recommendations would then be something like: For < 2 lbs phosphorus per acre , add 75 lbs of rock phosphate; for 2-4 lbs per acre , add 65 lbs ; etc... I've never been totally comfortable with this procedure, as I can't figure out how to put a confidence interval around the results. Is there a way to do so?",,"['statistics', 'standard-deviation']"
4,What is the pivotal quantity,What is the pivotal quantity,,"I had a question of two parts. I solved the first part but I am stuck on the second. Any hints or partial solutions would be greatly appreciated. a)$ X_1,....X_n$ are uniform iid on the interval $(0,\theta)$. If $Y=max(X_1,....X_n)$ find the pdf of Y?  I worked the pdf to be $n \dfrac{\bigl(\dfrac{x}{\theta}\bigr )^{n-1}}{\theta}$ b) Construct a random variable $T = g(Y;\theta)$ such that T has the same distribution as Y for all values of $\theta$ . Such a random variable T will then be a pivot? I am honestly not sure where to start with this one?","I had a question of two parts. I solved the first part but I am stuck on the second. Any hints or partial solutions would be greatly appreciated. a)$ X_1,....X_n$ are uniform iid on the interval $(0,\theta)$. If $Y=max(X_1,....X_n)$ find the pdf of Y?  I worked the pdf to be $n \dfrac{\bigl(\dfrac{x}{\theta}\bigr )^{n-1}}{\theta}$ b) Construct a random variable $T = g(Y;\theta)$ such that T has the same distribution as Y for all values of $\theta$ . Such a random variable T will then be a pivot? I am honestly not sure where to start with this one?",,"['probability', 'statistics', 'statistical-inference']"
5,Computing $\langle\sin(\gamma_i)\rangle= \int_{(S^2)^N} \sin(\gamma_i)p(\Theta)dS$,Computing,\langle\sin(\gamma_i)\rangle= \int_{(S^2)^N} \sin(\gamma_i)p(\Theta)dS,"I'm trying to evaluate the following integral, which I know must be zero, $$\langle\sin(\gamma_i)\rangle= \int_{(S^2)^N} \sin(\gamma_i)p(\Gamma)dS$$ Where, $$\langle \vec{a}(\vec{r_1},...,\vec{r_N})\rangle= \int_{(\mathbb{R})^N} \vec{a}(\vec{r_1},...,\vec{r_N})p(\vec{r_1},...,\vec{r_N})dV$$ and $p(\vec{r_1},...,\vec{r_N})$ is the pdf of the configuration $(\vec{r_1},...,\vec{r_N})$.  There are a lot of definitions here which can be found in the reference material, but they should not be needed. We also have $$p(\Gamma)=\frac1Z \exp(\delta\sum_{j=1}^N \cos(\gamma_j))$$ Where $Z=\int_{(S^2)^N} \exp(\delta\sum_{j=1}^N \cos(\gamma_j))dS$ and $\Gamma=\{\gamma_i\}$. Therefore $$\langle\sin(\gamma_i)\rangle=$$ $$ \frac1Z \int_0^{2\pi} \cdots\int_0^{2\pi} \int_0^{\pi} \cdots\int_0^{\pi} \sin(\gamma_i)\exp(\delta\sum_{j=1}^N \cos(\gamma_j))\sin(\gamma_1)d\gamma_1 \cdots \sin(\gamma_N)d\gamma_N d\phi_1\cdots d\phi_N$$ $$=\frac{(4\pi)^N}{Z} \prod_{j=1}^N \int_0^\pi \sin(\gamma_i) \sin(\gamma_j) \exp(\delta\cos( \gamma_j)) d\gamma_j$$ Now $$Z=(4\pi)^N \prod_{j=1}^N \int_0^\pi \sin(\gamma_j) \exp(\delta \cos(\gamma_j)) d\gamma_j$$ Therefore $$\langle\sin(\gamma_i)\rangle=\frac{\int_0^\pi \sin^2(\gamma_i) \exp(\delta\cos( \gamma_i)) d\gamma_i}{\int_0^\pi \sin(\gamma_i) \exp(\delta \cos(\gamma_i)) d\gamma_i}\neq0 $$ ( http://www.wolframalpha.com/input/?i=int_0%5Epi+sin%28x%29%5E2+e%5E%28cos%28x%29%29+dx ) I can't for the life of me see what's going wrong here, any help would be greatly appreciated. EDIT: With reference to the pdf file, here is the proof as to why I know this integral must be zero, On page 10 there is the proof of: $w_n=ww_{n-1}$ $w_n=\langle \vec{t}_i \cdot \vec{t}_{i+n}\rangle=\langle \cos(\gamma_i+...+Î³_{i+n-1})\rangle$ We then expand the cos term into: $w_n=\langle \cos(Î³_i)\rangle \langle\ \cos(\gamma_{i+1}+...+Î³_{i+n-1})\rangle-\langle \sin(Î³_i)\rangle \langle \sin(\gamma_{i+1}+...+Î³_{i+n-1})\rangle$ $\Rightarrow w_n=ww_{n-1}+\langle \sin(Î³_i)\rangle \langle\cdots\rangle$ $\Rightarrow \langle \sin(Î³_i)\rangle=0$","I'm trying to evaluate the following integral, which I know must be zero, $$\langle\sin(\gamma_i)\rangle= \int_{(S^2)^N} \sin(\gamma_i)p(\Gamma)dS$$ Where, $$\langle \vec{a}(\vec{r_1},...,\vec{r_N})\rangle= \int_{(\mathbb{R})^N} \vec{a}(\vec{r_1},...,\vec{r_N})p(\vec{r_1},...,\vec{r_N})dV$$ and $p(\vec{r_1},...,\vec{r_N})$ is the pdf of the configuration $(\vec{r_1},...,\vec{r_N})$.  There are a lot of definitions here which can be found in the reference material, but they should not be needed. We also have $$p(\Gamma)=\frac1Z \exp(\delta\sum_{j=1}^N \cos(\gamma_j))$$ Where $Z=\int_{(S^2)^N} \exp(\delta\sum_{j=1}^N \cos(\gamma_j))dS$ and $\Gamma=\{\gamma_i\}$. Therefore $$\langle\sin(\gamma_i)\rangle=$$ $$ \frac1Z \int_0^{2\pi} \cdots\int_0^{2\pi} \int_0^{\pi} \cdots\int_0^{\pi} \sin(\gamma_i)\exp(\delta\sum_{j=1}^N \cos(\gamma_j))\sin(\gamma_1)d\gamma_1 \cdots \sin(\gamma_N)d\gamma_N d\phi_1\cdots d\phi_N$$ $$=\frac{(4\pi)^N}{Z} \prod_{j=1}^N \int_0^\pi \sin(\gamma_i) \sin(\gamma_j) \exp(\delta\cos( \gamma_j)) d\gamma_j$$ Now $$Z=(4\pi)^N \prod_{j=1}^N \int_0^\pi \sin(\gamma_j) \exp(\delta \cos(\gamma_j)) d\gamma_j$$ Therefore $$\langle\sin(\gamma_i)\rangle=\frac{\int_0^\pi \sin^2(\gamma_i) \exp(\delta\cos( \gamma_i)) d\gamma_i}{\int_0^\pi \sin(\gamma_i) \exp(\delta \cos(\gamma_i)) d\gamma_i}\neq0 $$ ( http://www.wolframalpha.com/input/?i=int_0%5Epi+sin%28x%29%5E2+e%5E%28cos%28x%29%29+dx ) I can't for the life of me see what's going wrong here, any help would be greatly appreciated. EDIT: With reference to the pdf file, here is the proof as to why I know this integral must be zero, On page 10 there is the proof of: $w_n=ww_{n-1}$ $w_n=\langle \vec{t}_i \cdot \vec{t}_{i+n}\rangle=\langle \cos(\gamma_i+...+Î³_{i+n-1})\rangle$ We then expand the cos term into: $w_n=\langle \cos(Î³_i)\rangle \langle\ \cos(\gamma_{i+1}+...+Î³_{i+n-1})\rangle-\langle \sin(Î³_i)\rangle \langle \sin(\gamma_{i+1}+...+Î³_{i+n-1})\rangle$ $\Rightarrow w_n=ww_{n-1}+\langle \sin(Î³_i)\rangle \langle\cdots\rangle$ $\Rightarrow \langle \sin(Î³_i)\rangle=0$",,"['integration', 'statistics', 'mathematical-modeling', 'biology']"
6,Combinations/probability calculations using ball/bag analogy,Combinations/probability calculations using ball/bag analogy,,"I'm wondering how to approach this question? I'm analysing data for a research project, but I feel like it falls into the category of choosing combinations of balls in a bag. Any help would be much appreciated. So - say I have 5 types (A-E) of balls, with a total of 457 in a bag A - 127; B - 100; C - 61; D - 52; E - 117; In a simple approach, each handful of balls I choose contains 4 balls, of some combination (a type of ball can only be repeated up to twice; ie. AABC can happen, but AAAC cannot). 1) How would I find the probability of grabbing, say, ABCE? ABCD? ABEE? 2) Now, to complicate things a bit, each handful can contain 4 OR 5 balls. What do I do now? 3) If I know test 101 handfuls, and find that A is grabbed twice (eg AAXX or AAXXX) 17 times, how can I see if that is statistically significant? Thank you so much for your time!","I'm wondering how to approach this question? I'm analysing data for a research project, but I feel like it falls into the category of choosing combinations of balls in a bag. Any help would be much appreciated. So - say I have 5 types (A-E) of balls, with a total of 457 in a bag A - 127; B - 100; C - 61; D - 52; E - 117; In a simple approach, each handful of balls I choose contains 4 balls, of some combination (a type of ball can only be repeated up to twice; ie. AABC can happen, but AAAC cannot). 1) How would I find the probability of grabbing, say, ABCE? ABCD? ABEE? 2) Now, to complicate things a bit, each handful can contain 4 OR 5 balls. What do I do now? 3) If I know test 101 handfuls, and find that A is grabbed twice (eg AAXX or AAXXX) 17 times, how can I see if that is statistically significant? Thank you so much for your time!",,"['probability', 'statistics', 'combinations']"
7,Is a 99% upper confidence bound the upper limit of a 99% confidence interval?,Is a 99% upper confidence bound the upper limit of a 99% confidence interval?,,"I have to find a ""99% confidence bound"" for a standard deviation.  This is not hard.  The only question I have is whether this is finding the $\chi^2_{.99}$ value or just the upper bound for the 99% confidence interval (between $\chi^2_{.005}$ and $\chi^2_{.995}$).  Any help would be appreciated.  Thanks in advanced!","I have to find a ""99% confidence bound"" for a standard deviation.  This is not hard.  The only question I have is whether this is finding the $\chi^2_{.99}$ value or just the upper bound for the 99% confidence interval (between $\chi^2_{.005}$ and $\chi^2_{.995}$).  Any help would be appreciated.  Thanks in advanced!",,"['probability', 'statistics', 'confidence-interval']"
8,Median of a sequence of random variables.,Median of a sequence of random variables.,,Let $\{X_n\}_{n=1}^{\infty}$ be a sequence of i.i.d. random variables such that almost surely $X_n \rightarrow X$. Given just the information above (i.e. no information about distribution) can one determine the median of $X_n$ in the limit as $n \rightarrow \infty$?  Or could anyone give me an appropriate reference?,Let $\{X_n\}_{n=1}^{\infty}$ be a sequence of i.i.d. random variables such that almost surely $X_n \rightarrow X$. Given just the information above (i.e. no information about distribution) can one determine the median of $X_n$ in the limit as $n \rightarrow \infty$?  Or could anyone give me an appropriate reference?,,['statistics']
9,Calculate Linear regression segment [duplicate],Calculate Linear regression segment [duplicate],,"This question already has answers here : Derivation of the formula for Ordinary Least Squares Linear Regression (3 answers) Closed 9 years ago . I have array of random numbers. How can I calculate linear regression segment? I am interested in finding the exact formula so I be able to use it in my work, please help me finding this formula with the next declarations: $N$ - the number of random numbers in the array $S$ - the sum of the numbers in the array. $array_{min}$ - the minimum number in the array $array_{max}$ - the maximum number in the array $E$ - the average number in the array. We are looking for $y = mx + c$. So you need to find a formula to represent $m$ and $c$ with the above declarations. The array of random number are the $Y$. $X$ is just neutral numbers from $0$ to $N$","This question already has answers here : Derivation of the formula for Ordinary Least Squares Linear Regression (3 answers) Closed 9 years ago . I have array of random numbers. How can I calculate linear regression segment? I am interested in finding the exact formula so I be able to use it in my work, please help me finding this formula with the next declarations: $N$ - the number of random numbers in the array $S$ - the sum of the numbers in the array. $array_{min}$ - the minimum number in the array $array_{max}$ - the maximum number in the array $E$ - the average number in the array. We are looking for $y = mx + c$. So you need to find a formula to represent $m$ and $c$ with the above declarations. The array of random number are the $Y$. $X$ is just neutral numbers from $0$ to $N$",,"['linear-algebra', 'statistics']"
10,Hypothesis testing: how do you call the variable that is being hypothesized about?,Hypothesis testing: how do you call the variable that is being hypothesized about?,,"The question is easy but it is really hard to find via Google. Say you have the following hypothesis: $H0: \mu = 0$ $Ha: \mu \neq 0 $ Now I know that $ \mu $ is called the population mean. But how do you call a variable (or value) that is hypothesized with in general? Because instead of $\mu$ I could also test for a different variable (e.g. $\sigma = 0$ vs. $\sigma \neq 0$). It's also not the ""statistic"" (e.g. t-statistic). So how do you call it?","The question is easy but it is really hard to find via Google. Say you have the following hypothesis: $H0: \mu = 0$ $Ha: \mu \neq 0 $ Now I know that $ \mu $ is called the population mean. But how do you call a variable (or value) that is hypothesized with in general? Because instead of $\mu$ I could also test for a different variable (e.g. $\sigma = 0$ vs. $\sigma \neq 0$). It's also not the ""statistic"" (e.g. t-statistic). So how do you call it?",,['statistics']
11,"Limiting distribution of $X_n1(|X_n|\le 1-\frac{1}{n})+n1(|X_n|>1-\frac{1}{n})$ if $X_n\sim Unif(-1,1)$ and are iid.",Limiting distribution of  if  and are iid.,"X_n1(|X_n|\le 1-\frac{1}{n})+n1(|X_n|>1-\frac{1}{n}) X_n\sim Unif(-1,1)","Limiting distribution of $X_n1(|X_n|\le 1-\frac{1}{n})+n1(|X_n|>1-\frac{1}{n})$ if $X_n\sim Unif(-1,1)$ and are iid. From looking at the term, if $n$ goes to infinity, then $Y_n$ would be $X_n$ so it should have the same distribution as $X_n$. $\displaystyle\psi_{Y_n}(t)=E[e^{itY_n}]=E[e^{itX_n1(|X_n\le 1-\frac{1}{n})}]E[e^{itn1(|X_n|>1-\frac{1}{n})}]=\left(\int_{-1+1/n}^{1-1/n}e^{itx}\frac{1}{2}dx\right)\left(\int_{-1}^{-1+1/n}e^{itn}\frac{1}{2}dx+\int_{1-1/n}^1\frac{1}{2}e^{itn}dx\right)=\frac{\exp{(it(1-\frac{1}{n}+n))}+\exp(-it(1-\frac{1}{n}-n)}{2itn}$ So I get $Y_n\sim Unif(1-\frac{1}{n}-n, 1-\frac{1}{n}+n)$ My problem is as $n\rightarrow\infty$, this characteristic function doesn't converge, instead it diverges to infinity.","Limiting distribution of $X_n1(|X_n|\le 1-\frac{1}{n})+n1(|X_n|>1-\frac{1}{n})$ if $X_n\sim Unif(-1,1)$ and are iid. From looking at the term, if $n$ goes to infinity, then $Y_n$ would be $X_n$ so it should have the same distribution as $X_n$. $\displaystyle\psi_{Y_n}(t)=E[e^{itY_n}]=E[e^{itX_n1(|X_n\le 1-\frac{1}{n})}]E[e^{itn1(|X_n|>1-\frac{1}{n})}]=\left(\int_{-1+1/n}^{1-1/n}e^{itx}\frac{1}{2}dx\right)\left(\int_{-1}^{-1+1/n}e^{itn}\frac{1}{2}dx+\int_{1-1/n}^1\frac{1}{2}e^{itn}dx\right)=\frac{\exp{(it(1-\frac{1}{n}+n))}+\exp(-it(1-\frac{1}{n}-n)}{2itn}$ So I get $Y_n\sim Unif(1-\frac{1}{n}-n, 1-\frac{1}{n}+n)$ My problem is as $n\rightarrow\infty$, this characteristic function doesn't converge, instead it diverges to infinity.",,"['statistics', 'characteristic-functions']"
12,Differentiate $P_{x_n}(z) = \prod_{i=1}^n\frac{1+z+z^2+...+z^{i-1}}{i}$ twice to calculate the variance of involutions.,Differentiate  twice to calculate the variance of involutions.,P_{x_n}(z) = \prod_{i=1}^n\frac{1+z+z^2+...+z^{i-1}}{i},"Use the Probability Generating Function for Involutions: $P_{x_n}(z) = \prod_{i=1}^n\frac{1+z+z^2+...+z^{i-1}}{i}$ To Calculate the Variance of Involutions where: $Variance \space X_n = P_{x_n}''(1) + P_{x_n}'(1) - (P_{x_n}'(1))^2$ Firstly, I have that $P_{x_n}'(z)=\sum_{i=1}^n(\prod_{j\neq i}\frac{1+z+...+z^{j-1}}{j})(\frac{1+2z+3z^2+...+(i-1)z^{i-2}}{i})$ as established in Derivative of a product $P_{x_n}(z) = \prod_{i=1}^n\frac{1+z+z^2 +...+z^{i-1}}{i} $ But now for the second derivative (and please correct me if I made a mistake) $P_{x_n}''(z)=\sum_{i=1}^n(\prod_{j\neq i}\frac{1+z+...+z^{j-1}}{j})(\frac{2+6z+...+(i-1)(i-2)z^{i-3}}{i}) + \sum_{i=1}^n \sum_{j\neq i}(\prod_{k\neq j}\frac{1+z+...+z^{k-1}}{k})(\frac{1+2z+3z^2+...+(j-1)z^{j-2}}{j})(\frac{1+2z+3z^2+...+(i-1)z^{i-2}}{i}) $ setting $z=1$ we obtain $P_{x_n}''(1)=\sum_{i=1}^n(\frac{2+6+...+(i-1)(i-2)}{i}) + \sum_{i=1}^n \sum_{j\neq i}(\frac{1+2+3+...+(j-1)}{j})(\frac{1+2+3+...+(i-1)}{i}) $ Now for triangular numbers, $2,6,12,20,...$ we have that $\space 2+6+12+20+...+(n-1)n = \frac{n(n+1)(n+2)}{3}$ and since $\sum_{i=1}^n i = \frac{n(n+1)}{2}$ $P_{x_n}''(1)=\sum_{i=1}^n(\frac{(i-1)i(i+1)}{3i}) + \sum_{i=1}^n \sum_{j\neq i}(\frac{j-1}{2})(\frac{i-1}{2}) $ which simplifies to $P_{x_n}''(1)=\frac{1}{3}\sum_{i=1}^n((i-1)(i+1)) + \sum_{i=1}^n \sum_{j\neq i}(\frac{j-1}{2})(\frac{i-1}{2}) $ $P_{x_n}''(1)=\frac{1}{3}\sum_{i=1}^{n}(i^2-1) + \sum_{i=1}^n \sum_{j\neq i}(\frac{j-1}{2})(\frac{i-1}{2}) $ and since $\sum_{i=1}^ni^2 =\frac{n(n+1)(2n+1)}{6}$ $P_{x_n}''(1)=\frac{1}{3}(\frac{n(n+1)(2n+1)}{6} - n) + \sum_{i=1}^n \sum_{j\neq i}(\frac{j-1}{2})(\frac{i-1}{2}) $ my question is how do I simplify further? and was my differentiation correct? Any help would be greatly appreciated. update: from Is there some way to simplify $\sum_{i=1}^n \sum_{j\neq i}(\frac{j-1}{2})(\frac{i-1}{2}) $ To obtain a closed form. We get that $P_{x_n}''(1)=\frac{1}{3}(\frac{n(n+1)(2n+1)}{6} - n) + \sum_{i=1}^n \sum_{j=1}^n(\frac{j-1}{2})(\frac{i-1}{2}) - \sum_{i=1}^n\frac{(i-1)^2}{4} $ and this simplifies to $P_{x_n}''(1)=\frac{1}{3}(\frac{n(n+1)(2n+1)}{6} - n) + \frac{n(n-1)}{4}\frac{n(n-1)}{4} - \frac{1}{4}(\frac{(n-1)n(2(n-1)+1)}{6}) $ Now using the formula for variance, $Var \space X_n = P_{X_n}''(1)+P_{X_n}'(1)-(P_{X_n}'(1))$ where $P_{X_n}'(1)$ is obtained from Derivative of a product $P_{x_n}(z) = \prod_{i=1}^n\frac{1+z+z^2 +...+z^{i-1}}{i} $ We get that $Var \space X_n = \frac{1}{3}(\frac{n(n+1)(2n+1)}{6} - n) + \frac{n(n-1)}{4}\frac{n(n-1)}{4} - \frac{1}{4}(\frac{(n-1)n(2(n-1)+1)}{6}) + \frac{n(n-1)}{4} - (\frac{n(n-1)}{4})^2$ and hence $Var \space X_n = \frac{2n^3+39n^2-42n}{72}$ which is not the correct answer however if you instead take $Var \space X_n = \frac{1}{3}(\frac{n(n+1)(2n+1)}{6} - n) + \frac{n(n-1)}{4}\frac{n(n-1)}{4} - \frac{1}{4}(\frac{(n-1)n(2(n-1)+1)}{6}) - \frac{n(n-1)}{4} - (\frac{n(n-1)}{4})^2$ with a $- \frac{n(n-1)}{4}$ you obtain $Var \space X_n = \frac{2n^3+3n^2-5n}{72} = \frac{(n-1)n(2n+5)}{72}$ which is the correct answer. Where am I making a mistake? Did I somehow leave out a term of $-2\frac{n(n-1)}{4}$","Use the Probability Generating Function for Involutions: $P_{x_n}(z) = \prod_{i=1}^n\frac{1+z+z^2+...+z^{i-1}}{i}$ To Calculate the Variance of Involutions where: $Variance \space X_n = P_{x_n}''(1) + P_{x_n}'(1) - (P_{x_n}'(1))^2$ Firstly, I have that $P_{x_n}'(z)=\sum_{i=1}^n(\prod_{j\neq i}\frac{1+z+...+z^{j-1}}{j})(\frac{1+2z+3z^2+...+(i-1)z^{i-2}}{i})$ as established in Derivative of a product $P_{x_n}(z) = \prod_{i=1}^n\frac{1+z+z^2 +...+z^{i-1}}{i} $ But now for the second derivative (and please correct me if I made a mistake) $P_{x_n}''(z)=\sum_{i=1}^n(\prod_{j\neq i}\frac{1+z+...+z^{j-1}}{j})(\frac{2+6z+...+(i-1)(i-2)z^{i-3}}{i}) + \sum_{i=1}^n \sum_{j\neq i}(\prod_{k\neq j}\frac{1+z+...+z^{k-1}}{k})(\frac{1+2z+3z^2+...+(j-1)z^{j-2}}{j})(\frac{1+2z+3z^2+...+(i-1)z^{i-2}}{i}) $ setting $z=1$ we obtain $P_{x_n}''(1)=\sum_{i=1}^n(\frac{2+6+...+(i-1)(i-2)}{i}) + \sum_{i=1}^n \sum_{j\neq i}(\frac{1+2+3+...+(j-1)}{j})(\frac{1+2+3+...+(i-1)}{i}) $ Now for triangular numbers, $2,6,12,20,...$ we have that $\space 2+6+12+20+...+(n-1)n = \frac{n(n+1)(n+2)}{3}$ and since $\sum_{i=1}^n i = \frac{n(n+1)}{2}$ $P_{x_n}''(1)=\sum_{i=1}^n(\frac{(i-1)i(i+1)}{3i}) + \sum_{i=1}^n \sum_{j\neq i}(\frac{j-1}{2})(\frac{i-1}{2}) $ which simplifies to $P_{x_n}''(1)=\frac{1}{3}\sum_{i=1}^n((i-1)(i+1)) + \sum_{i=1}^n \sum_{j\neq i}(\frac{j-1}{2})(\frac{i-1}{2}) $ $P_{x_n}''(1)=\frac{1}{3}\sum_{i=1}^{n}(i^2-1) + \sum_{i=1}^n \sum_{j\neq i}(\frac{j-1}{2})(\frac{i-1}{2}) $ and since $\sum_{i=1}^ni^2 =\frac{n(n+1)(2n+1)}{6}$ $P_{x_n}''(1)=\frac{1}{3}(\frac{n(n+1)(2n+1)}{6} - n) + \sum_{i=1}^n \sum_{j\neq i}(\frac{j-1}{2})(\frac{i-1}{2}) $ my question is how do I simplify further? and was my differentiation correct? Any help would be greatly appreciated. update: from Is there some way to simplify $\sum_{i=1}^n \sum_{j\neq i}(\frac{j-1}{2})(\frac{i-1}{2}) $ To obtain a closed form. We get that $P_{x_n}''(1)=\frac{1}{3}(\frac{n(n+1)(2n+1)}{6} - n) + \sum_{i=1}^n \sum_{j=1}^n(\frac{j-1}{2})(\frac{i-1}{2}) - \sum_{i=1}^n\frac{(i-1)^2}{4} $ and this simplifies to $P_{x_n}''(1)=\frac{1}{3}(\frac{n(n+1)(2n+1)}{6} - n) + \frac{n(n-1)}{4}\frac{n(n-1)}{4} - \frac{1}{4}(\frac{(n-1)n(2(n-1)+1)}{6}) $ Now using the formula for variance, $Var \space X_n = P_{X_n}''(1)+P_{X_n}'(1)-(P_{X_n}'(1))$ where $P_{X_n}'(1)$ is obtained from Derivative of a product $P_{x_n}(z) = \prod_{i=1}^n\frac{1+z+z^2 +...+z^{i-1}}{i} $ We get that $Var \space X_n = \frac{1}{3}(\frac{n(n+1)(2n+1)}{6} - n) + \frac{n(n-1)}{4}\frac{n(n-1)}{4} - \frac{1}{4}(\frac{(n-1)n(2(n-1)+1)}{6}) + \frac{n(n-1)}{4} - (\frac{n(n-1)}{4})^2$ and hence $Var \space X_n = \frac{2n^3+39n^2-42n}{72}$ which is not the correct answer however if you instead take $Var \space X_n = \frac{1}{3}(\frac{n(n+1)(2n+1)}{6} - n) + \frac{n(n-1)}{4}\frac{n(n-1)}{4} - \frac{1}{4}(\frac{(n-1)n(2(n-1)+1)}{6}) - \frac{n(n-1)}{4} - (\frac{n(n-1)}{4})^2$ with a $- \frac{n(n-1)}{4}$ you obtain $Var \space X_n = \frac{2n^3+3n^2-5n}{72} = \frac{(n-1)n(2n+5)}{72}$ which is the correct answer. Where am I making a mistake? Did I somehow leave out a term of $-2\frac{n(n-1)}{4}$",,"['calculus', 'probability', 'combinatorics', 'statistics', 'generating-functions']"
13,How do I find if the probability of the sample proportion is greater than something?,How do I find if the probability of the sample proportion is greater than something?,,"I have this problem and I have no clue how to solve it. In 2012, 31% of the adult population in the US had earned a bachelorâs degree or higher. One hundred people are randomly sampled from the population. What is the probability that the  sample proportion p-hat is greater than 0.40?","I have this problem and I have no clue how to solve it. In 2012, 31% of the adult population in the US had earned a bachelorâs degree or higher. One hundred people are randomly sampled from the population. What is the probability that the  sample proportion p-hat is greater than 0.40?",,"['statistics', 'probability-distributions']"
14,Finding the cdf of a distribution,Finding the cdf of a distribution,,"Two people wish to explore Asia, where both tourists have a lifetime that is exponentially distributed with mean $\theta$. $T$ is the length of time that Asia is being explored by at least one of the tourists. How do I find the cdf of the distribution given the following: 1) Both tourists leave the hotel at time $t = 0$, where their lifetimes are independently distributed. 2) The second tourist leaves the hotel on the event of the first tourist becoming sick.","Two people wish to explore Asia, where both tourists have a lifetime that is exponentially distributed with mean $\theta$. $T$ is the length of time that Asia is being explored by at least one of the tourists. How do I find the cdf of the distribution given the following: 1) Both tourists leave the hotel at time $t = 0$, where their lifetimes are independently distributed. 2) The second tourist leaves the hotel on the event of the first tourist becoming sick.",,"['statistics', 'probability-distributions']"
15,What is the rationale behind ROC curves?,What is the rationale behind ROC curves?,,"I am not sure how ROC curves work. I see that the X-Axis is the false positive rate while the Y axis is the true positive rate. 1) I don't understand how for a given statistical learning model, you could have the true positive and false positive rate to vary from 0 to 1. Are you changing parameters in the model to make it so? 2) What about true negatives and false negatives? How are these represented in the curve? Best, Redshadow","I am not sure how ROC curves work. I see that the X-Axis is the false positive rate while the Y axis is the true positive rate. 1) I don't understand how for a given statistical learning model, you could have the true positive and false positive rate to vary from 0 to 1. Are you changing parameters in the model to make it so? 2) What about true negatives and false negatives? How are these represented in the curve? Best, Redshadow",,"['statistics', 'machine-learning']"
16,Showing independence of random variables,Showing independence of random variables,,"When proving $\bar x$ and $S^2$ are independent in my noted it says that ""functions of independent quantities are independent "". Can someone tell me how functions of independent quantities are independent happen? Also let $X_1,X_2,X_3,\dots,X_n$ be a random sample.And suppose we want to estimate parameter $\theta$ using $T(x)$ as an estimator. In this I have a expression as $E\{ [  T(X)-E(T(X))][E(T(x)-\theta)]\}$. I want to know if I can say that in $[T(X)-E(T(X))]$ since $E(T(X)$ (expected value is a one particular fixed value) and $T(X)$ are independent of one another . Also the expression $[E(T(x)-\theta)]$ is independent of $X$. Hence the two expressions $[T(X)-E(T(X))]$ and $[E(T(x)-\theta)]$  are independent of one another. So that I can use if $a,b$ are independent $E(ab)=E(a)E(b)$ form","When proving $\bar x$ and $S^2$ are independent in my noted it says that ""functions of independent quantities are independent "". Can someone tell me how functions of independent quantities are independent happen? Also let $X_1,X_2,X_3,\dots,X_n$ be a random sample.And suppose we want to estimate parameter $\theta$ using $T(x)$ as an estimator. In this I have a expression as $E\{ [  T(X)-E(T(X))][E(T(x)-\theta)]\}$. I want to know if I can say that in $[T(X)-E(T(X))]$ since $E(T(X)$ (expected value is a one particular fixed value) and $T(X)$ are independent of one another . Also the expression $[E(T(x)-\theta)]$ is independent of $X$. Hence the two expressions $[T(X)-E(T(X))]$ and $[E(T(x)-\theta)]$  are independent of one another. So that I can use if $a,b$ are independent $E(ab)=E(a)E(b)$ form",,"['statistics', 'statistical-inference']"
17,Error In Proving Variance Of Sample Mean,Error In Proving Variance Of Sample Mean,,"So the variance of the sample mean $\bar X$ is $\sigma^2/n$, and I understand how to prove it through the formula $Var(nX) = n^2Var(X)$. However, I was approaching it through another method, and seem to have proven that the variance of the sample mean is $\sigma^2$, and I can't seem to find the error in my proof: $ Var(\bar X) = E[\bar X^2] - E[\bar X]^2                = E[\bar X^2] - \mu^2                = E[(1/n\sum X_i)^2] - \mu^2                = 1/n^2 E[(\sum X_i)^2] - \mu^2                = 1/n^2 (n^2E[X^2]) - \mu^2               = Var(X) $. I've been looking for a while, but I can't find what's wrong with this. If someone could help point out the error in this, that would be very helpful, thanks!","So the variance of the sample mean $\bar X$ is $\sigma^2/n$, and I understand how to prove it through the formula $Var(nX) = n^2Var(X)$. However, I was approaching it through another method, and seem to have proven that the variance of the sample mean is $\sigma^2$, and I can't seem to find the error in my proof: $ Var(\bar X) = E[\bar X^2] - E[\bar X]^2                = E[\bar X^2] - \mu^2                = E[(1/n\sum X_i)^2] - \mu^2                = 1/n^2 E[(\sum X_i)^2] - \mu^2                = 1/n^2 (n^2E[X^2]) - \mu^2               = Var(X) $. I've been looking for a while, but I can't find what's wrong with this. If someone could help point out the error in this, that would be very helpful, thanks!",,"['probability', 'statistics', 'expectation']"
18,Probability Question I can't get around.,Probability Question I can't get around.,,"This is the question from my assignment, which I can't get around. Suppose that a water distribution system is composed of a number of independent  pipes. At temperatures below 0 deg C, the pipes may burst; on such occasions the  probability that one pipe will catastrophically burst is 0.05. Assume that the failure  (catastrophic bursting) of two or more pipes at the same time is unlikely. If the  distribution system must be shut down when 3 of the pipes have failed, determine the  probability that the distribution system can withstand at least 5 below 0 deg C events  before a shut down occurs. How do you do this question without knowing number of pipes ? I understand how it is possible if one knows the number of pipe. But how do you do it if number of pipes are not given ?","This is the question from my assignment, which I can't get around. Suppose that a water distribution system is composed of a number of independent  pipes. At temperatures below 0 deg C, the pipes may burst; on such occasions the  probability that one pipe will catastrophically burst is 0.05. Assume that the failure  (catastrophic bursting) of two or more pipes at the same time is unlikely. If the  distribution system must be shut down when 3 of the pipes have failed, determine the  probability that the distribution system can withstand at least 5 below 0 deg C events  before a shut down occurs. How do you do this question without knowing number of pipes ? I understand how it is possible if one knows the number of pipe. But how do you do it if number of pipes are not given ?",,"['probability', 'combinatorics', 'statistics', 'probability-theory', 'probability-distributions']"
19,Max Word Size as a Function of Number of Words,Max Word Size as a Function of Number of Words,,"I want to describe the relationship between largest word length, l , and the number of words in a set, n . Example: For the set of words {""the"", ""large"", ""rock""} , the largest word length is l = 5 For {""ten"", ""two"", ""cat""} , the largest word length is l = 3 For {""a"", ""Mathematics""} , the largest word length is l = 11 Can l be approximated as a function of n ? How is l related to n ? If n were allowed to be very large, l would obviously be the length of the largest word in the English language. If n were limited to 1, l would obviously be the average size of all words. Intuition tells me that choosing more words gives me a statistically better chance of requiring a more buckets, but I can't explain why. I am not sure whether this question belongs on StackOverflow or Mathematics. This is not a homework problem. It came up in an answer I posted over on StackOverflow .","I want to describe the relationship between largest word length, l , and the number of words in a set, n . Example: For the set of words {""the"", ""large"", ""rock""} , the largest word length is l = 5 For {""ten"", ""two"", ""cat""} , the largest word length is l = 3 For {""a"", ""Mathematics""} , the largest word length is l = 11 Can l be approximated as a function of n ? How is l related to n ? If n were allowed to be very large, l would obviously be the length of the largest word in the English language. If n were limited to 1, l would obviously be the average size of all words. Intuition tells me that choosing more words gives me a statistically better chance of requiring a more buckets, but I can't explain why. I am not sure whether this question belongs on StackOverflow or Mathematics. This is not a homework problem. It came up in an answer I posted over on StackOverflow .",,"['statistics', 'computational-complexity']"
20,How to appropriately normalize financial data?,How to appropriately normalize financial data?,,"I am evaluating a normalization of financial data. Two colleagues feel certain of a certain approach, and I am befuddled. The approach shapes the results how they want (i.e., brings possibly extreme results toward a notional center line) but I'm not convinced it is mathematically cogent. For background, we are analyzing the performance of loans we provide. We define $purchase\ price$ as the money we lend, $purchase\ amount$ is the total we expect to be paid back, $revenue$ to be the difference of $purchase\ price$ and $purchase\ amount$, and $margin$ to be $revenue - commissions$. For the sake of simplification, let us assume that there are no commissions, such that $revenue \equiv margin$. We say that the $default\ rate$ as a percentage of margin is as follows, with $defaults$ being $anticipated\ margin - actual\ margin$: $$default\ rate = 1 - \frac{actual\ margin}{anticipated\ margin} = \frac{defaults}{anticipated\ margin}$$ During the underwriting and quoting phase, we collect all the expected information and generate a couple of risk scores, and then a human makes qualitative judgements of the risk of the customer and of the loan. This risk is captured by the margin we charge and the time length of the repayment schedule. Riskier loans will skew higher in margin or (not XOR) shorter term length, less risky loans lower in margin or longer in term length. We want to evaluate the performance of various classes of historical loans to decide if we properly priced against the resultant behavior of that business. That is, we want to decide if we adequately priced for risk. We do this by calculating the default rate for one class of business (e.g., from a particular industry or from certain partner sales offices) and comparing it to some benchmark default rate. We want to normalize the default rates to equivalent units. It may be that one segment of the business hit a 30% default rate and another 20%, but unless we can reflect our anticipated risk on these segments and adjust the rates for comparability, we're concerned our analysis will be flawed. This is where my colleagues' notion arrives. They feel it appropriate to ""normalize"" by calculating margin per month as $$margin\ per\ month = \frac{margin}{term\ length}$$ and taking the mean across all the business as average margin per month $$average\ margin\ per\ month = \frac{\sum{margins}}{\sum{term\ lengths}}$$ to be a benchmark. With this, they will calculate a (purportedly) normalized default rate as $$normalized\ default\ rate = default\ rate \times \frac{average\ margin\ per\ month}{margin\ per\ month}$$ They seemed so sure that this gave them the ""normalization"" they wanted that I assumed my befuddlement was due to my being slow. I algebraically expanded their target formula and arrived at something that didn't make much sense: $$\frac{defaults}{margin^2} \times \frac{(\sum{margins}) \times term}{\sum{term\ lengths}}$$ There a couple of ways to rewrite that but they seem equivalently unuseful. My fundamental point of contention is not with the effort of normalization, but the method. Here, we pit the default rate for a class of business against the mean margin/month across all business divided by the margin/month for that class of business. They are right that this corrals the values some, e.g., a lower-than-average risk loan will have lower-than-average anticipated margin, and so its default rate in this analysis will actually be increased . Similarly, the actual default rate for high-risk loans will be decreased . If we expect a class of customer to be inherently riskier, we will price with a proportionately higher margin/month than a class of customer with ""average"" risk, so achieving a higher-than-average actual default rate for that class of customer doesn't itself worry us. Only if the actual default rate is higher than the target for that class of business/customer will we be worried. Does this make sense as a means of comparability, and if so, what am I missing? EDIT I could see if the various categories are distributed approximately normally and calculate a z score, but even then I'd like to determine if I'm missing something with this particular approach.","I am evaluating a normalization of financial data. Two colleagues feel certain of a certain approach, and I am befuddled. The approach shapes the results how they want (i.e., brings possibly extreme results toward a notional center line) but I'm not convinced it is mathematically cogent. For background, we are analyzing the performance of loans we provide. We define $purchase\ price$ as the money we lend, $purchase\ amount$ is the total we expect to be paid back, $revenue$ to be the difference of $purchase\ price$ and $purchase\ amount$, and $margin$ to be $revenue - commissions$. For the sake of simplification, let us assume that there are no commissions, such that $revenue \equiv margin$. We say that the $default\ rate$ as a percentage of margin is as follows, with $defaults$ being $anticipated\ margin - actual\ margin$: $$default\ rate = 1 - \frac{actual\ margin}{anticipated\ margin} = \frac{defaults}{anticipated\ margin}$$ During the underwriting and quoting phase, we collect all the expected information and generate a couple of risk scores, and then a human makes qualitative judgements of the risk of the customer and of the loan. This risk is captured by the margin we charge and the time length of the repayment schedule. Riskier loans will skew higher in margin or (not XOR) shorter term length, less risky loans lower in margin or longer in term length. We want to evaluate the performance of various classes of historical loans to decide if we properly priced against the resultant behavior of that business. That is, we want to decide if we adequately priced for risk. We do this by calculating the default rate for one class of business (e.g., from a particular industry or from certain partner sales offices) and comparing it to some benchmark default rate. We want to normalize the default rates to equivalent units. It may be that one segment of the business hit a 30% default rate and another 20%, but unless we can reflect our anticipated risk on these segments and adjust the rates for comparability, we're concerned our analysis will be flawed. This is where my colleagues' notion arrives. They feel it appropriate to ""normalize"" by calculating margin per month as $$margin\ per\ month = \frac{margin}{term\ length}$$ and taking the mean across all the business as average margin per month $$average\ margin\ per\ month = \frac{\sum{margins}}{\sum{term\ lengths}}$$ to be a benchmark. With this, they will calculate a (purportedly) normalized default rate as $$normalized\ default\ rate = default\ rate \times \frac{average\ margin\ per\ month}{margin\ per\ month}$$ They seemed so sure that this gave them the ""normalization"" they wanted that I assumed my befuddlement was due to my being slow. I algebraically expanded their target formula and arrived at something that didn't make much sense: $$\frac{defaults}{margin^2} \times \frac{(\sum{margins}) \times term}{\sum{term\ lengths}}$$ There a couple of ways to rewrite that but they seem equivalently unuseful. My fundamental point of contention is not with the effort of normalization, but the method. Here, we pit the default rate for a class of business against the mean margin/month across all business divided by the margin/month for that class of business. They are right that this corrals the values some, e.g., a lower-than-average risk loan will have lower-than-average anticipated margin, and so its default rate in this analysis will actually be increased . Similarly, the actual default rate for high-risk loans will be decreased . If we expect a class of customer to be inherently riskier, we will price with a proportionately higher margin/month than a class of customer with ""average"" risk, so achieving a higher-than-average actual default rate for that class of customer doesn't itself worry us. Only if the actual default rate is higher than the target for that class of business/customer will we be worried. Does this make sense as a means of comparability, and if so, what am I missing? EDIT I could see if the various categories are distributed approximately normally and calculate a z score, but even then I'd like to determine if I'm missing something with this particular approach.",,"['statistics', 'data-analysis']"
21,Let $X$ be symmetrically distributed about 0. Show that $X$ and -$X$ are identically distributed,Let  be symmetrically distributed about 0. Show that  and - are identically distributed,X X X,"Hello Mathematicians! The definition of being symmetrically distributed about zero means that the PDF (Continuous) or PMF (Discrete) $f(x) = f(-x)$. And the definition of being identically distributed is $$P(X \in A) = P(Y \in A)$$ for every $A \in \beta$, where $\beta$ is a sigma field. Actually, I don't see what to prove, because  Could you give any idea?","Hello Mathematicians! The definition of being symmetrically distributed about zero means that the PDF (Continuous) or PMF (Discrete) $f(x) = f(-x)$. And the definition of being identically distributed is $$P(X \in A) = P(Y \in A)$$ for every $A \in \beta$, where $\beta$ is a sigma field. Actually, I don't see what to prove, because  Could you give any idea?",,"['probability', 'statistics']"
22,Determine the distribution underlying several given scores and their percentiles,Determine the distribution underlying several given scores and their percentiles,,"I am trying to build a FICO score calculator that estimates one's FICO score given one's percentile from another credit report.  FICO score data is kept fairly secret, but the following information is publicly available for 2012: A score of 750 had a percentile of 62.8% A score of 800 had a percentile of 81.7% A score of 850 had a percentile of 95.5% The average score was 689. The median score was 723. I tried to guess the standard distribution using Z-scores for the known data in combination with Excel's NORMSDIST function.  The ""Percentile"" row below is calculated with NORMSDIST while the ""Actual"" row reflects the percentiles listed above: The problem I ran into is this: when I approximate the standard deviation needed to place one of the given data points in the correct percentile, this results in incorrect percentiles for the other given data.  This indicates that the known score/percentile data do not fit the standard normal distribution.  How do I find a distribution that fits the given data?","I am trying to build a FICO score calculator that estimates one's FICO score given one's percentile from another credit report.  FICO score data is kept fairly secret, but the following information is publicly available for 2012: A score of 750 had a percentile of 62.8% A score of 800 had a percentile of 81.7% A score of 850 had a percentile of 95.5% The average score was 689. The median score was 723. I tried to guess the standard distribution using Z-scores for the known data in combination with Excel's NORMSDIST function.  The ""Percentile"" row below is calculated with NORMSDIST while the ""Actual"" row reflects the percentiles listed above: The problem I ran into is this: when I approximate the standard deviation needed to place one of the given data points in the correct percentile, this results in incorrect percentiles for the other given data.  This indicates that the known score/percentile data do not fit the standard normal distribution.  How do I find a distribution that fits the given data?",,"['statistics', 'statistical-inference', 'standard-deviation', 'percentile']"
23,"Asymptotic normal-behaviour of the MLE, question about proof.","Asymptotic normal-behaviour of the MLE, question about proof.",,"In this proof they prove that the MLE is asymptotically normal. But as you see, they divide by $S'(\theta)$. But what if this value is 0?, you can't divide by zero? Is there a special case when this happen, or can it never happen?","In this proof they prove that the MLE is asymptotically normal. But as you see, they divide by $S'(\theta)$. But what if this value is 0?, you can't divide by zero? Is there a special case when this happen, or can it never happen?",,"['statistics', 'proof-verification']"
24,Does the order of samples taken affect the probability of getting a certain value?,Does the order of samples taken affect the probability of getting a certain value?,,"The question is asking what is the probability of taking two samples and having the first one being shorter than 20 and the second one being larger than 25 based on the given density curve The density curve shows that the P(length< 20) = 0.35 and that P(length>25) = 0.24. So would you just multiply the two probabilities together, giving 0.084? Or do you have to factor in order somehow? What if the question says that order doesn't matter, but one has to have length < 20 and one has to have length > 25? All I can think of doing is multiplying them together again and getting the same P = 0.084. Am I missing something? Thank you!","The question is asking what is the probability of taking two samples and having the first one being shorter than 20 and the second one being larger than 25 based on the given density curve The density curve shows that the P(length< 20) = 0.35 and that P(length>25) = 0.24. So would you just multiply the two probabilities together, giving 0.084? Or do you have to factor in order somehow? What if the question says that order doesn't matter, but one has to have length < 20 and one has to have length > 25? All I can think of doing is multiplying them together again and getting the same P = 0.084. Am I missing something? Thank you!",,"['probability', 'statistics']"
25,Fitting a simple linear regression,Fitting a simple linear regression,,"A professor in the School of Business in a university polled a dozen colleagues about the number of professional meetings they attended in the past five years $x$ and the number of papers they submitted to refereed journals $y$ during the same period. The summary data are given as follows: $n = 12$, $\bar{x}= 4$, $\bar{y}=12$, $\sum x^2 = 232$, $\sum xy = 318$. Fit a simple linear regression model between x and y by finding out the estimates of intercept and slope. Comment on whether attending more professional meetings would result in publishing more papers. I know that this is a linear regression problem. But I only know how to solve it using given values of $x$ and $y$. The only values given with the problem are the sample $n$, mean of $x$,mean of $y$, summation of $x^2$ and summation of $x*y$. I really don't know how to start with this problem.","A professor in the School of Business in a university polled a dozen colleagues about the number of professional meetings they attended in the past five years $x$ and the number of papers they submitted to refereed journals $y$ during the same period. The summary data are given as follows: $n = 12$, $\bar{x}= 4$, $\bar{y}=12$, $\sum x^2 = 232$, $\sum xy = 318$. Fit a simple linear regression model between x and y by finding out the estimates of intercept and slope. Comment on whether attending more professional meetings would result in publishing more papers. I know that this is a linear regression problem. But I only know how to solve it using given values of $x$ and $y$. The only values given with the problem are the sample $n$, mean of $x$,mean of $y$, summation of $x^2$ and summation of $x*y$. I really don't know how to start with this problem.",,"['statistics', 'regression']"
26,Calculating joint MGF,Calculating joint MGF,,"This is an end-of-chapter question from a Korean textbook, and unfortunately it only has solutions to the even-numbered q's, so I'm seeking for some hints or tips to work out this particular joint moment generating function question. or I better state that I cannot find a way to solve this particular double integral) The question (roughly translated) states that Given the joint pdf of random variables X and Y, $$     f(x,y) = \frac{1}{\sqrt(2\pi)}e^{-x}e^{\frac{-(y-x)^2}{2}},   x \ge  0,  -\infty \le y \le \infty $$ Find the joint mgf M(s,t), and for what values of s and t does the mgf exist? The following is how I approached this problem. $ M(s,t)=\int_{-\infty}^\infty\int_{0}^\infty e^{sx+ty}\frac{1}{\sqrt(2\pi)}e^{-x}e^{\frac{{-(y-x)}^2}{2}}dxdy $ $ =\int_{0}^\infty \int_{-\infty}^{\infty}e^{sx+ty}\frac{1}{\sqrt(2\pi)}e^{-x}e^{\frac{{-(y-x)}^2}{2}}dydx $ $           =\int_{0}^\infty \int_{-\infty}^{\infty} \frac{1}{\sqrt(2\pi)} e^{\frac{2(s-1)x-x^2}{2}}e^{\frac{ty-y^2}{2}}e^{xy} dydx $ $           =\int_{0}^\infty \int_{-\infty}^{\infty} \frac{1}{\sqrt(2\pi)} e^{\frac{-(x-(s-1))^2}{2}}e^{\frac{-(y-t)^2}{2}}e^{xy}e^{\frac{(s-1)^2+t^2}{2}} dydx $ I see $\frac{1}{\sqrt{2\pi}}$ in the intergral and y is to be integrated from $\infty $ to $-\infty$, so I believe that I need to make the expression in the form $\frac{1}{\sqrt(2\pi)}*e^-{\frac{(y-something)^2}{2}}*f(x)$  (pdf of a Normal distribution, and integral will be evaluated to 1 whatever the value something is) and then integrating the expression w.r.t x from 0 to $\infty$ from there. But I cannot find any useful integration technique to evaluate this double integral. I tried polar coordinate; making substitution $ x = r cos{\theta} $ and $ y = r sin{\theta} $ which will turn the integral to $ =\int_{\frac{-\pi}{2}}^{\frac{\pi}{2}} \int_0^{\infty} \frac{1}{\sqrt(2\pi)} r*exp(r*((s-1)cos{\theta}+tsin{\theta})-\frac{{r^2}(1-\frac{sin{2\theta}}{2})}{4})  drd{\theta} $ Evaluating this I got $\sqrt{\frac{2\pi}{3}}$ which I feel is incorrect as it's neither of a function of x nor y. So can anyone suggest me with any hints (possible substitution?) as to approach this question?","This is an end-of-chapter question from a Korean textbook, and unfortunately it only has solutions to the even-numbered q's, so I'm seeking for some hints or tips to work out this particular joint moment generating function question. or I better state that I cannot find a way to solve this particular double integral) The question (roughly translated) states that Given the joint pdf of random variables X and Y, $$     f(x,y) = \frac{1}{\sqrt(2\pi)}e^{-x}e^{\frac{-(y-x)^2}{2}},   x \ge  0,  -\infty \le y \le \infty $$ Find the joint mgf M(s,t), and for what values of s and t does the mgf exist? The following is how I approached this problem. $ M(s,t)=\int_{-\infty}^\infty\int_{0}^\infty e^{sx+ty}\frac{1}{\sqrt(2\pi)}e^{-x}e^{\frac{{-(y-x)}^2}{2}}dxdy $ $ =\int_{0}^\infty \int_{-\infty}^{\infty}e^{sx+ty}\frac{1}{\sqrt(2\pi)}e^{-x}e^{\frac{{-(y-x)}^2}{2}}dydx $ $           =\int_{0}^\infty \int_{-\infty}^{\infty} \frac{1}{\sqrt(2\pi)} e^{\frac{2(s-1)x-x^2}{2}}e^{\frac{ty-y^2}{2}}e^{xy} dydx $ $           =\int_{0}^\infty \int_{-\infty}^{\infty} \frac{1}{\sqrt(2\pi)} e^{\frac{-(x-(s-1))^2}{2}}e^{\frac{-(y-t)^2}{2}}e^{xy}e^{\frac{(s-1)^2+t^2}{2}} dydx $ I see $\frac{1}{\sqrt{2\pi}}$ in the intergral and y is to be integrated from $\infty $ to $-\infty$, so I believe that I need to make the expression in the form $\frac{1}{\sqrt(2\pi)}*e^-{\frac{(y-something)^2}{2}}*f(x)$  (pdf of a Normal distribution, and integral will be evaluated to 1 whatever the value something is) and then integrating the expression w.r.t x from 0 to $\infty$ from there. But I cannot find any useful integration technique to evaluate this double integral. I tried polar coordinate; making substitution $ x = r cos{\theta} $ and $ y = r sin{\theta} $ which will turn the integral to $ =\int_{\frac{-\pi}{2}}^{\frac{\pi}{2}} \int_0^{\infty} \frac{1}{\sqrt(2\pi)} r*exp(r*((s-1)cos{\theta}+tsin{\theta})-\frac{{r^2}(1-\frac{sin{2\theta}}{2})}{4})  drd{\theta} $ Evaluating this I got $\sqrt{\frac{2\pi}{3}}$ which I feel is incorrect as it's neither of a function of x nor y. So can anyone suggest me with any hints (possible substitution?) as to approach this question?",,"['integration', 'statistics', 'random-variables', 'self-learning', 'moment-generating-functions']"
27,Why isn't a t test used when comparing two proportions?,Why isn't a t test used when comparing two proportions?,,"All the examples I've seen say to use a z test to compare two proportions. For example, n=13, x=0.22 versus n=10, x=0.44. Then all the examples warn that the z test doesn't work with low sample sizes. So why can't we just use a t test, which provides p values appropriate to the sample size?","All the examples I've seen say to use a z test to compare two proportions. For example, n=13, x=0.22 versus n=10, x=0.44. Then all the examples warn that the z test doesn't work with low sample sizes. So why can't we just use a t test, which provides p values appropriate to the sample size?",,"['statistics', 'hypothesis-testing']"
28,sampling distribution with no continuity correction,sampling distribution with no continuity correction,,"Assume that the distribution of the height of a plant is normally distributed with a mean height of $\mu = 83.4$ inches and standard deviation of $\sigma = 9.1$ inches. Find: if three such plants are chosen at random find $\Pr[ \bar y > 90 {\rm \; inches}]$. if two such plants are chosen at random, then what is the probability that one plant will be $90$ or more inches while the other plant will be less than $90$ inches tall? (no continuity correction) if three such plants are chosen at random, then what is the probability that two out of three plants will be $90$ or more inches tall? (no continuity correction) For #1 I think it should be like this : $\frac{90-83.4}{9.1/\sqrt{4}}$ which would be $0.3409$ but I'm not sure about #2 and #3.","Assume that the distribution of the height of a plant is normally distributed with a mean height of $\mu = 83.4$ inches and standard deviation of $\sigma = 9.1$ inches. Find: if three such plants are chosen at random find $\Pr[ \bar y > 90 {\rm \; inches}]$. if two such plants are chosen at random, then what is the probability that one plant will be $90$ or more inches while the other plant will be less than $90$ inches tall? (no continuity correction) if three such plants are chosen at random, then what is the probability that two out of three plants will be $90$ or more inches tall? (no continuity correction) For #1 I think it should be like this : $\frac{90-83.4}{9.1/\sqrt{4}}$ which would be $0.3409$ but I'm not sure about #2 and #3.",,"['probability', 'statistics']"
29,Determining parameters of Poisson distributions from their difference,Determining parameters of Poisson distributions from their difference,,"Let $X, Y$ be two independent random variables with Poisson distributions $Po(\lambda), Po(\mu)$ respectively. We don't know the values of $\mu$ and $\lambda$, but we know that $P(X>Y) = a, P(X=Y) = b$, where $a, b$ are known constants. Is there some good way how to determine $\mu$ and $\lambda$ from this?","Let $X, Y$ be two independent random variables with Poisson distributions $Po(\lambda), Po(\mu)$ respectively. We don't know the values of $\mu$ and $\lambda$, but we know that $P(X>Y) = a, P(X=Y) = b$, where $a, b$ are known constants. Is there some good way how to determine $\mu$ and $\lambda$ from this?",,"['statistics', 'probability-distributions']"
30,Question about expectation notation,Question about expectation notation,,"$E[X]=\sum_x{x \space f_x(x)}$ for the discrete case. So let's say I want to find $E[XY]$, By the definition that would be $E[XY]=\sum_{(x,y)}xy \space f_{xy}(x,y)$ where $f_{xy}(x,y)$ is the joint pdf of X and Y (correct me if I'm wrong). Now here comes the part that I don't understand, suppose I have $E[X^2Y]=\sum_{(x,y)}x^2y \space f_{x^2y}(x,y)$ (by def.) or some other ""fancy"" expectation how do I get the probability distribution? In the $E[XY]$ case,     $\space f_{x^2y}(x,y)=f_{xy}(x,y)$ right? (if $x$ is always positive) When can these simplifications occur? Thank you very much!","$E[X]=\sum_x{x \space f_x(x)}$ for the discrete case. So let's say I want to find $E[XY]$, By the definition that would be $E[XY]=\sum_{(x,y)}xy \space f_{xy}(x,y)$ where $f_{xy}(x,y)$ is the joint pdf of X and Y (correct me if I'm wrong). Now here comes the part that I don't understand, suppose I have $E[X^2Y]=\sum_{(x,y)}x^2y \space f_{x^2y}(x,y)$ (by def.) or some other ""fancy"" expectation how do I get the probability distribution? In the $E[XY]$ case,     $\space f_{x^2y}(x,y)=f_{xy}(x,y)$ right? (if $x$ is always positive) When can these simplifications occur? Thank you very much!",,"['probability', 'statistics', 'conditional-probability']"
31,Standard deviation and intervals,Standard deviation and intervals,,"I'm working on some homework and I'm stuck on a question that is not making sense to me in terms of my answer. The question states: The following data is the last digit of the social security number of a group of students 1, 6, 9, 1, 5, 9, 0, 2, 8, 4, 0, 7, 3, 4, 2, 5, 8, 4, 2, 3, 2, 0, 0, 2, 1, 2, 7, 7, 4, 0,  0, 9, 9, 5, 3, 8, 4, 7, 4, 6, 6, 9, 0, 2, 6, 2, 9, 5, 8, 5, 1, 7, 7, 7, 8, 7, 5, 1, 8, 3,        4, 1, 9, 3, 8, 6, 6, 6, 6  a. Determine the intervals x +- s; x +- 2s; and x +- 3s. b. Find the proportion of the measurements that lie in each of these intervals. The standard deviation that I got was aprox 2.90647, which means my three intervals are 1.70223 to 7.51517, -1.20424 to 10.42164, and -4.11071 to 13.32811. Which leads to my first question, is it possible to have ranges that go from negative? Because for question b, for the 2 last intervals I get 100% for both intervals, again is that possible? Something doesn't seem right to me. Thank you.","I'm working on some homework and I'm stuck on a question that is not making sense to me in terms of my answer. The question states: The following data is the last digit of the social security number of a group of students 1, 6, 9, 1, 5, 9, 0, 2, 8, 4, 0, 7, 3, 4, 2, 5, 8, 4, 2, 3, 2, 0, 0, 2, 1, 2, 7, 7, 4, 0,  0, 9, 9, 5, 3, 8, 4, 7, 4, 6, 6, 9, 0, 2, 6, 2, 9, 5, 8, 5, 1, 7, 7, 7, 8, 7, 5, 1, 8, 3,        4, 1, 9, 3, 8, 6, 6, 6, 6  a. Determine the intervals x +- s; x +- 2s; and x +- 3s. b. Find the proportion of the measurements that lie in each of these intervals. The standard deviation that I got was aprox 2.90647, which means my three intervals are 1.70223 to 7.51517, -1.20424 to 10.42164, and -4.11071 to 13.32811. Which leads to my first question, is it possible to have ranges that go from negative? Because for question b, for the 2 last intervals I get 100% for both intervals, again is that possible? Something doesn't seem right to me. Thank you.",,"['statistics', 'standard-deviation']"
32,statistics and probability. Homework,statistics and probability. Homework,,I've been given the following problem: if I have 8 red and 10 blue marbles what is the probability that from 4 marbles that I took fom tha bag 2 are red? I missed some lessons. Any help would be appreciated.,I've been given the following problem: if I have 8 red and 10 blue marbles what is the probability that from 4 marbles that I took fom tha bag 2 are red? I missed some lessons. Any help would be appreciated.,,"['probability', 'statistics']"
33,How would I calculate the better performance based on percentages?,How would I calculate the better performance based on percentages?,,"I've been looking at how the characteristics of web content relate to the number of views it receives. I looked at the percentage of content with that characteristic, and then the percentage of views on that content. For example - 16% of pages are in format X - but receive 12% of pageviews 1% of pages are in format Y - but receive 2% of pageviews ...and so on. So in simple terms we can see that Y is better than X, despite having a lower number of pageviews overall. But I'm struggling with best way to present that - can I say that Y is _% better than X and how would I make that calculation? Thanks,","I've been looking at how the characteristics of web content relate to the number of views it receives. I looked at the percentage of content with that characteristic, and then the percentage of views on that content. For example - 16% of pages are in format X - but receive 12% of pageviews 1% of pages are in format Y - but receive 2% of pageviews ...and so on. So in simple terms we can see that Y is better than X, despite having a lower number of pageviews overall. But I'm struggling with best way to present that - can I say that Y is _% better than X and how would I make that calculation? Thanks,",,['statistics']
34,Algebraically expanding this function,Algebraically expanding this function,,"I have an algebra question. I am currently working on this function: $$\left(\frac{{e^{t\sqrt{3/n}}} -{e^{-t\sqrt{3/n}}}}{2t\sqrt{3/n}}\right)^n$$ Now this is the MGF of a function called $Z$ and I have to find the MGF as $n$ goes to $\infty$ which of course, since I am using the CLT, will have to be $e^{t^2/2}$ (the MGF of the Standard Normal). All of this is correct, it's just that I am having difficulty simplifying this. I also know that I need to use the following: $(1+a/n)^n$ as $n$ goes to $\infty$ is equal to $e^a$. Help would be greatly appreciated as I am quite unsure of how to proceed. I do not even know where to begin as the function is raised to the power of $n$. Thanks so much!","I have an algebra question. I am currently working on this function: $$\left(\frac{{e^{t\sqrt{3/n}}} -{e^{-t\sqrt{3/n}}}}{2t\sqrt{3/n}}\right)^n$$ Now this is the MGF of a function called $Z$ and I have to find the MGF as $n$ goes to $\infty$ which of course, since I am using the CLT, will have to be $e^{t^2/2}$ (the MGF of the Standard Normal). All of this is correct, it's just that I am having difficulty simplifying this. I also know that I need to use the following: $(1+a/n)^n$ as $n$ goes to $\infty$ is equal to $e^a$. Help would be greatly appreciated as I am quite unsure of how to proceed. I do not even know where to begin as the function is raised to the power of $n$. Thanks so much!",,"['linear-algebra', 'statistics', 'moment-generating-functions']"
35,Central limit theorem inequality with binomial distribution,Central limit theorem inequality with binomial distribution,,"In a theater there are 600 seating's and only two cloakroom's. Every person independently from others randomly leaves a cloak in one of them. At least how many ticket's should there be in each of the cloakroom's so the probability of directing guest's to the other one is less or equal to 0.01 ? So we have 600 Bernoulli trials. $n = 600, p =  \frac{1}{2}$  So to obtain a solution I have to estimate this:  $P(X  > k ) \le 0.01$?","In a theater there are 600 seating's and only two cloakroom's. Every person independently from others randomly leaves a cloak in one of them. At least how many ticket's should there be in each of the cloakroom's so the probability of directing guest's to the other one is less or equal to 0.01 ? So we have 600 Bernoulli trials. $n = 600, p =  \frac{1}{2}$  So to obtain a solution I have to estimate this:  $P(X  > k ) \le 0.01$?",,"['probability', 'statistics', 'binomial-theorem']"
36,Variance of sum of square differences corrupted by noise,Variance of sum of square differences corrupted by noise,,"I've been trying to extend a proof I have for a univariate function $f(x)$ to a multivariate function $f(x,y)$ but the result I get looks weird. I'm wondering whether there is a mistake in any step I'm making. The background is that $f(x,y)$ is an image that is superimposed with some gaussian noise $n\sim\mathcal{N}(0,\sigma^2)$. This image is shifted by a true but unknown offset $d_x$ and $d_y$ and the goal is to find an estimate $(\Delta x,\Delta y)^\mathsf{T}$ for this offset and to estimate its variance. This leads to the following expression that is to be minimized: $$ e(\Delta x,\Delta y) = \iint \left(f(x-d_x+\Delta x+\lambda, y-d_y+\Delta y+\eta) - f(x+\lambda,y+\eta) + n_1(x+\lambda,y+\eta) - n_0(x+\lambda,y+\eta)\right)^2\text{d}\lambda\text{d}\eta $$ I am computing the second order taylor expansion around $d_x=\Delta x$ and $d_y=\Delta y$. This yields $$ a(\Delta x - d_x)^2 + b(\Delta y - d_y)^2 +\\ c(\Delta x - d_x)(\Delta y - d_y) + d(\Delta x - d_x) + e(\Delta y - d_y) + f $$ where \begin{eqnarray*} a \approx \iint f_{xx}(x+\lambda,y+\eta)^2\text{d}\lambda\text{d}\eta\\ b \approx \iint f_{yy}(x+\lambda,y+\eta)^2\text{d}\lambda\text{d}\eta\\ c \approx \iint 2 f_{x}(x+\lambda,y+\eta)f_y(x+\lambda,y+\eta)\text{d}\lambda\text{d}\eta\\ d = \iint 2(n_1(x+\lambda,y+\eta)-n_0(x+\lambda,y+\eta)f_x(x+\lambda,y+\eta)\text{d}\lambda\text{d}\eta\\ e = \iint 2(n_1(x+\lambda,y+\eta)-n_0(x+\lambda,y+\eta)f_y(x+\lambda,y+\eta)\text{d}\lambda\text{d}\eta\\ f = \iint (n_1(x+\lambda,y+\eta)-n_0(x+\lambda,y+\eta)^2 \text{d}\lambda\text{d}\eta \end{eqnarray*} Subscripts denote partial derivatives. The second order partial derivative terms  have been omitted in the formula above. This is the only step I don't feel very comfortable with, but it is also done in the original, univariate proof. Note that $a$, $b$ and $c$ are constants and only $d$, $e$ and $f$ are random variables. The vertex of this paraboloid -- if it exists -- is found by the equating the partial derivatives of the paraboloid to 0, which results in $$ \Delta x - d_x = -\frac{2bd-ce}{4ab-c^2},\; \Delta y - d_y = -\frac{2ae-cd}{4ab-c^2}\;. $$ I am interested in the variance of the estimate, $$ \text{Var}\left(\begin{pmatrix}\Delta x\\\Delta y\end{pmatrix}\right) = \text{Var}\left(\begin{pmatrix}d_x\\d_y\end{pmatrix} - \begin{pmatrix}\frac{2bd-ce}{4ab-c^2}\\\frac{2ae-cd}{4ab-c^2}\end{pmatrix}\right) =  \text{Var}\left(-\begin{pmatrix}\frac{2bd-ce}{4ab-c^2}\\\frac{2ae-cd}{4ab-c^2}\end{pmatrix}\right) $$ In the next few steps, I will use $\text{Var}(AX)=A\text{Var}(X)A^\mathsf{T}$ repeatedly. First I pull out the denominator: $$ \text{Var}\left(\begin{pmatrix}\Delta x\\\Delta y\end{pmatrix}\right) =  \frac{1}{\left(4ab-c^2\right)^2}\text{Var}\left(\begin{pmatrix}2bd-ce\\2ae-cd\end{pmatrix}\right) $$ Then I put back the definitions of the polynomial coefficients. I'm using a short-hand notation here where I omit the function arguments and abbreviate $n_1(x+\lambda,y+\eta)-n_0(x+\lambda,y+\eta)$ with $\delta n$. $$ \text{Var}\left(\begin{pmatrix}\Delta x\\\Delta y\end{pmatrix}\right) =  \frac{1}{\left(4\iint f_{xx}^2\iint f_{yy}^2 - 4(\iint f_{xy})^2\right)^2}\text{Var}\left(\begin{pmatrix}4\iint f_{yy}^2\iint f_{x}\delta n-4\iint f_{xy}\iint f_{y}\delta_n\\4\iint f_{xx}^2\iint f_{y}\delta n-4\iint f_{xy}\iint f_{x}\delta n\end{pmatrix}\right) $$ Now I decompose the matrix and pull it out of the variance. $$ \text{Var}\left(\begin{pmatrix}\Delta x\\\Delta y\end{pmatrix}\right) =  \frac{1}{\left(4\iint f_{xx}^2\iint f_{yy}^2 - 4(\iint f_{xy})^2\right)^2}\text{Var}\left(4\begin{pmatrix}\iint f_{yy}^2 & -\iint f_xf_y \\ -\iint f_xf_y & \iint f_{xx}^2\end{pmatrix}\begin{pmatrix}\iint f_x\delta_n\\\iint f_y\delta_n\end{pmatrix}\right)= \frac{16}{\left(4\iint f_{xx}^2\iint f_{yy}^2 - 4(\iint f_{xy})^2\right)^2}\begin{pmatrix}\iint f_{yy}^2 & -\iint f_xf_y \\ -\iint f_xf_y & \iint f_{xx}^2\end{pmatrix}\text{Var}\left(\begin{pmatrix}\iint f_x\delta_n\\\iint f_y\delta_n\end{pmatrix}\right)\begin{pmatrix}\iint f_{yy}^2 & -\iint f_xf_y \\ -\iint f_xf_y & \iint f_{xx}^2\end{pmatrix} $$ Then I proceed similarly for the remaining integral: $$ \text{Var}\left(\begin{pmatrix}\Delta x\\\Delta y\end{pmatrix}\right) =  \frac{32\sigma^2}{\left(4\iint f_{xx}^2\iint f_{yy}^2 - 4(\iint f_{xy})^2\right)^2}\begin{pmatrix}\iint f_{yy}^2 & -\iint f_xf_y \\ -\iint f_xf_y & \iint f_{xx}^2\end{pmatrix}\begin{pmatrix}\iint f_x\\\iint f_y\end{pmatrix}\begin{pmatrix}\iint f_x\\\iint f_y\end{pmatrix}^\mathsf{T}\begin{pmatrix}\iint f_{yy}^2 & -\iint f_xf_y \\ -\iint f_xf_y & \iint f_{xx}^2\end{pmatrix} $$ Now here is the problem: I would expect the solution $$ \text{Var}\left(\begin{pmatrix}\Delta x\\\Delta y\end{pmatrix}\right) =  \frac{k\sigma^2}{\iint f_{xx}^2\iint f_{yy}^2 - (\iint f_{xy})^2}\begin{pmatrix}\iint f_{yy}^2 & -\iint f_xf_y \\ -\iint f_xf_y & \iint f_{xx}^2\end{pmatrix} $$ where $k$ is some constant. This could be achieved if the square in the denominator would cancel out with the first three matrices. However it does not, I always get these cross-terms. Hence I am wondering whether there is any mistake in my derivation, or alternatively if I could use some approximation that escapes me to get the desired result. Thank you very much!","I've been trying to extend a proof I have for a univariate function $f(x)$ to a multivariate function $f(x,y)$ but the result I get looks weird. I'm wondering whether there is a mistake in any step I'm making. The background is that $f(x,y)$ is an image that is superimposed with some gaussian noise $n\sim\mathcal{N}(0,\sigma^2)$. This image is shifted by a true but unknown offset $d_x$ and $d_y$ and the goal is to find an estimate $(\Delta x,\Delta y)^\mathsf{T}$ for this offset and to estimate its variance. This leads to the following expression that is to be minimized: $$ e(\Delta x,\Delta y) = \iint \left(f(x-d_x+\Delta x+\lambda, y-d_y+\Delta y+\eta) - f(x+\lambda,y+\eta) + n_1(x+\lambda,y+\eta) - n_0(x+\lambda,y+\eta)\right)^2\text{d}\lambda\text{d}\eta $$ I am computing the second order taylor expansion around $d_x=\Delta x$ and $d_y=\Delta y$. This yields $$ a(\Delta x - d_x)^2 + b(\Delta y - d_y)^2 +\\ c(\Delta x - d_x)(\Delta y - d_y) + d(\Delta x - d_x) + e(\Delta y - d_y) + f $$ where \begin{eqnarray*} a \approx \iint f_{xx}(x+\lambda,y+\eta)^2\text{d}\lambda\text{d}\eta\\ b \approx \iint f_{yy}(x+\lambda,y+\eta)^2\text{d}\lambda\text{d}\eta\\ c \approx \iint 2 f_{x}(x+\lambda,y+\eta)f_y(x+\lambda,y+\eta)\text{d}\lambda\text{d}\eta\\ d = \iint 2(n_1(x+\lambda,y+\eta)-n_0(x+\lambda,y+\eta)f_x(x+\lambda,y+\eta)\text{d}\lambda\text{d}\eta\\ e = \iint 2(n_1(x+\lambda,y+\eta)-n_0(x+\lambda,y+\eta)f_y(x+\lambda,y+\eta)\text{d}\lambda\text{d}\eta\\ f = \iint (n_1(x+\lambda,y+\eta)-n_0(x+\lambda,y+\eta)^2 \text{d}\lambda\text{d}\eta \end{eqnarray*} Subscripts denote partial derivatives. The second order partial derivative terms  have been omitted in the formula above. This is the only step I don't feel very comfortable with, but it is also done in the original, univariate proof. Note that $a$, $b$ and $c$ are constants and only $d$, $e$ and $f$ are random variables. The vertex of this paraboloid -- if it exists -- is found by the equating the partial derivatives of the paraboloid to 0, which results in $$ \Delta x - d_x = -\frac{2bd-ce}{4ab-c^2},\; \Delta y - d_y = -\frac{2ae-cd}{4ab-c^2}\;. $$ I am interested in the variance of the estimate, $$ \text{Var}\left(\begin{pmatrix}\Delta x\\\Delta y\end{pmatrix}\right) = \text{Var}\left(\begin{pmatrix}d_x\\d_y\end{pmatrix} - \begin{pmatrix}\frac{2bd-ce}{4ab-c^2}\\\frac{2ae-cd}{4ab-c^2}\end{pmatrix}\right) =  \text{Var}\left(-\begin{pmatrix}\frac{2bd-ce}{4ab-c^2}\\\frac{2ae-cd}{4ab-c^2}\end{pmatrix}\right) $$ In the next few steps, I will use $\text{Var}(AX)=A\text{Var}(X)A^\mathsf{T}$ repeatedly. First I pull out the denominator: $$ \text{Var}\left(\begin{pmatrix}\Delta x\\\Delta y\end{pmatrix}\right) =  \frac{1}{\left(4ab-c^2\right)^2}\text{Var}\left(\begin{pmatrix}2bd-ce\\2ae-cd\end{pmatrix}\right) $$ Then I put back the definitions of the polynomial coefficients. I'm using a short-hand notation here where I omit the function arguments and abbreviate $n_1(x+\lambda,y+\eta)-n_0(x+\lambda,y+\eta)$ with $\delta n$. $$ \text{Var}\left(\begin{pmatrix}\Delta x\\\Delta y\end{pmatrix}\right) =  \frac{1}{\left(4\iint f_{xx}^2\iint f_{yy}^2 - 4(\iint f_{xy})^2\right)^2}\text{Var}\left(\begin{pmatrix}4\iint f_{yy}^2\iint f_{x}\delta n-4\iint f_{xy}\iint f_{y}\delta_n\\4\iint f_{xx}^2\iint f_{y}\delta n-4\iint f_{xy}\iint f_{x}\delta n\end{pmatrix}\right) $$ Now I decompose the matrix and pull it out of the variance. $$ \text{Var}\left(\begin{pmatrix}\Delta x\\\Delta y\end{pmatrix}\right) =  \frac{1}{\left(4\iint f_{xx}^2\iint f_{yy}^2 - 4(\iint f_{xy})^2\right)^2}\text{Var}\left(4\begin{pmatrix}\iint f_{yy}^2 & -\iint f_xf_y \\ -\iint f_xf_y & \iint f_{xx}^2\end{pmatrix}\begin{pmatrix}\iint f_x\delta_n\\\iint f_y\delta_n\end{pmatrix}\right)= \frac{16}{\left(4\iint f_{xx}^2\iint f_{yy}^2 - 4(\iint f_{xy})^2\right)^2}\begin{pmatrix}\iint f_{yy}^2 & -\iint f_xf_y \\ -\iint f_xf_y & \iint f_{xx}^2\end{pmatrix}\text{Var}\left(\begin{pmatrix}\iint f_x\delta_n\\\iint f_y\delta_n\end{pmatrix}\right)\begin{pmatrix}\iint f_{yy}^2 & -\iint f_xf_y \\ -\iint f_xf_y & \iint f_{xx}^2\end{pmatrix} $$ Then I proceed similarly for the remaining integral: $$ \text{Var}\left(\begin{pmatrix}\Delta x\\\Delta y\end{pmatrix}\right) =  \frac{32\sigma^2}{\left(4\iint f_{xx}^2\iint f_{yy}^2 - 4(\iint f_{xy})^2\right)^2}\begin{pmatrix}\iint f_{yy}^2 & -\iint f_xf_y \\ -\iint f_xf_y & \iint f_{xx}^2\end{pmatrix}\begin{pmatrix}\iint f_x\\\iint f_y\end{pmatrix}\begin{pmatrix}\iint f_x\\\iint f_y\end{pmatrix}^\mathsf{T}\begin{pmatrix}\iint f_{yy}^2 & -\iint f_xf_y \\ -\iint f_xf_y & \iint f_{xx}^2\end{pmatrix} $$ Now here is the problem: I would expect the solution $$ \text{Var}\left(\begin{pmatrix}\Delta x\\\Delta y\end{pmatrix}\right) =  \frac{k\sigma^2}{\iint f_{xx}^2\iint f_{yy}^2 - (\iint f_{xy})^2}\begin{pmatrix}\iint f_{yy}^2 & -\iint f_xf_y \\ -\iint f_xf_y & \iint f_{xx}^2\end{pmatrix} $$ where $k$ is some constant. This could be achieved if the square in the denominator would cancel out with the first three matrices. However it does not, I always get these cross-terms. Hence I am wondering whether there is any mistake in my derivation, or alternatively if I could use some approximation that escapes me to get the desired result. Thank you very much!",,"['probability', 'statistics', 'probability-theory']"
37,Variance of Function involving random variable and its conditional expectation,Variance of Function involving random variable and its conditional expectation,,"$\newcommand{\Var}{\operatorname{Var}}$ $\newcommand{\Cov}{\operatorname{Cov}}$ I am supposed to show the following: $$ \Var(Y-E(Y\mid X)) = E(\Var(Y\mid X)) $$ My attempt involved using simple property of conditional expectations and variances, I get a close result, but not quite: $$ \Var(Y-E(Y\mid X)) = \Var(Y) + \Var(E(Y\mid X)) - 2\Cov(Y,E(Y\mid X)) $$ Since $\Cov(Y,E(Y\mid X)) = \Cov(Y,Y) = \Var(Y)$, then: $$ \Var(Y) + \Var(E(Y\mid X)) - 2\Cov(Y,E(Y\mid X)) = \Var(E(Y\mid X)) - \Var(Y) $$ But $\Var(Y) = \Var(E(Y\mid X)) - E(\Var(Y\mid X))$ and then: $$ \Var(E(Y\mid X)) - \Var(Y) = \Var(E(Y\mid X)) - \Var(E(Y\mid X)) - E(\Var(Y\mid X)) = -E(\Var(Y\mid X)) $$ Thanks!","$\newcommand{\Var}{\operatorname{Var}}$ $\newcommand{\Cov}{\operatorname{Cov}}$ I am supposed to show the following: $$ \Var(Y-E(Y\mid X)) = E(\Var(Y\mid X)) $$ My attempt involved using simple property of conditional expectations and variances, I get a close result, but not quite: $$ \Var(Y-E(Y\mid X)) = \Var(Y) + \Var(E(Y\mid X)) - 2\Cov(Y,E(Y\mid X)) $$ Since $\Cov(Y,E(Y\mid X)) = \Cov(Y,Y) = \Var(Y)$, then: $$ \Var(Y) + \Var(E(Y\mid X)) - 2\Cov(Y,E(Y\mid X)) = \Var(E(Y\mid X)) - \Var(Y) $$ But $\Var(Y) = \Var(E(Y\mid X)) - E(\Var(Y\mid X))$ and then: $$ \Var(E(Y\mid X)) - \Var(Y) = \Var(E(Y\mid X)) - \Var(E(Y\mid X)) - E(\Var(Y\mid X)) = -E(\Var(Y\mid X)) $$ Thanks!",,"['probability', 'statistics']"
38,Why is regression analysis also statistical test?,Why is regression analysis also statistical test?,,"According to wikipedia , regression analysis is a statistical process for estimating the relationships among variables. Regression analysis is widely used for prediction and forecasting. So why is regression analysis also used as statistical test? For example, in this page , logistic regression and linear regression are listed among t-test, ANOVA, chi-square test etc.","According to wikipedia , regression analysis is a statistical process for estimating the relationships among variables. Regression analysis is widely used for prediction and forecasting. So why is regression analysis also used as statistical test? For example, in this page , logistic regression and linear regression are listed among t-test, ANOVA, chi-square test etc.",,"['statistics', 'regression']"
39,"Statisticsï¼How to prove that one variable's change is influenced by another variable,that is to say ,they are related?","Statisticsï¼How to prove that one variable's change is influenced by another variable,that is to say ,they are related?",,"I major in Bioinformatics. Now,I am in a problem: we all know that temperature changes during a year , I find that a disease incidence is really high when temperature is relatively high , while it becomes really low when the temperature is relatively low , that is to say ,they are related. So ,I want to find out a way to prove that they are related ,not just intuitively feel that they are related. So, any suggestions?","I major in Bioinformatics. Now,I am in a problem: we all know that temperature changes during a year , I find that a disease incidence is really high when temperature is relatively high , while it becomes really low when the temperature is relatively low , that is to say ,they are related. So ,I want to find out a way to prove that they are related ,not just intuitively feel that they are related. So, any suggestions?",,['statistics']
40,Lottery game question?,Lottery game question?,,"In a lottery, you must match all 6 numbers drawn at random from 1 to 40 without replacement to win the grand prize, 5 out of 6 numbers to win second prize, and 4 out of 6 numbers to win third. The ordering of the drawn numbers is irrelevant. Find the probability of winning each prize. To win the grand prize, I think the probability is just 1/(40 Combination 6). How do you account for the fewer numbers drawn in the other two parts of the question though?","In a lottery, you must match all 6 numbers drawn at random from 1 to 40 without replacement to win the grand prize, 5 out of 6 numbers to win second prize, and 4 out of 6 numbers to win third. The ordering of the drawn numbers is irrelevant. Find the probability of winning each prize. To win the grand prize, I think the probability is just 1/(40 Combination 6). How do you account for the fewer numbers drawn in the other two parts of the question though?",,"['probability', 'statistics', 'discrete-mathematics']"
41,"Let $\{x_1, \ldots, x_n \}$ be a sample. Why can one subtract $L$ from every sample value $x_i$ to get equivalent results?",Let  be a sample. Why can one subtract  from every sample value  to get equivalent results?,"\{x_1, \ldots, x_n \} L x_i","Suppose we have a sample $\{x_1, \ldots ,x_n\}$ obtained from i.i.d RV's  $X_i\sim N$ (normal). Then in books on statistics, depending on the values obtained, they subtract a fixed quantity $L$ from every $x_i$, to get smaller values for computations, resulting in the sample $\{x_1 - L, \ldots, x_n -L\}$. The books states that this has no influence on statistical descisions based on the sample (why?). Below I've attached a picture from a practical case (using SAS), where the sample consist of values $x_i \approx \pm 40$. Since the sample values $x_i$ are close to $40$ the book has decided to substract $40$ from all $x_i$. The variable diameter is the original values, while the variable diam_40 are the values with $40$ subtracted from them. Indeed the computed values seems pretty similar, however $t$ values are different also $Pr >$. Why can we subtract from every sample value without getting different results ""later on"" ? How can I show my descisions will always be the same regardless of having subtracted or not ?","Suppose we have a sample $\{x_1, \ldots ,x_n\}$ obtained from i.i.d RV's  $X_i\sim N$ (normal). Then in books on statistics, depending on the values obtained, they subtract a fixed quantity $L$ from every $x_i$, to get smaller values for computations, resulting in the sample $\{x_1 - L, \ldots, x_n -L\}$. The books states that this has no influence on statistical descisions based on the sample (why?). Below I've attached a picture from a practical case (using SAS), where the sample consist of values $x_i \approx \pm 40$. Since the sample values $x_i$ are close to $40$ the book has decided to substract $40$ from all $x_i$. The variable diameter is the original values, while the variable diam_40 are the values with $40$ subtracted from them. Indeed the computed values seems pretty similar, however $t$ values are different also $Pr >$. Why can we subtract from every sample value without getting different results ""later on"" ? How can I show my descisions will always be the same regardless of having subtracted or not ?",,"['statistics', 'statistical-inference']"
42,Statistic Textbooks,Statistic Textbooks,,What is a good textbook for introductions to continuous and discrete distributions? The one that my university offers is a thin scrap put together by the department. Could I get some recommendations on what the standard introduction textbook to probabilities and statistics.,What is a good textbook for introductions to continuous and discrete distributions? The one that my university offers is a thin scrap put together by the department. Could I get some recommendations on what the standard introduction textbook to probabilities and statistics.,,"['probability', 'statistics', 'reference-request']"
43,Mixed Distribution Problem,Mixed Distribution Problem,,"You design an insurance policy that pays a random amount Payment $= 1000 \cdot A$, where $A$ denotes the age at death. $A$ is assumed to have a continuous uniform distribution on $[50,110]$ (that is, a constant density function). Modify this policy to pay an amount Modified payment = $\begin{cases} 1000 \cdot A\; \text{if} \;A<90\\ 100,000 \;\text{if}\; A \geq 90 \end{cases}$ a) Find $E[X]$ and $\sigma$ of the original payment. b) Find $E[X]$ and $\sigma$ of the modified payment. I know the for the original payment the function jumps from 0-50 (like the step function) but I don't know how to compute $E[X]$ because my thinking for the 1st one was to just do $\dfrac{50}{110}(1000)\cdot 25.5 + \dfrac{60}{110}\cdot 1000 \displaystyle\int_{50}^{110} x$ which will give me my $E[X]$ for part a but that didn't work. So need some help understanding these sorts of problems.","You design an insurance policy that pays a random amount Payment $= 1000 \cdot A$, where $A$ denotes the age at death. $A$ is assumed to have a continuous uniform distribution on $[50,110]$ (that is, a constant density function). Modify this policy to pay an amount Modified payment = $\begin{cases} 1000 \cdot A\; \text{if} \;A<90\\ 100,000 \;\text{if}\; A \geq 90 \end{cases}$ a) Find $E[X]$ and $\sigma$ of the original payment. b) Find $E[X]$ and $\sigma$ of the modified payment. I know the for the original payment the function jumps from 0-50 (like the step function) but I don't know how to compute $E[X]$ because my thinking for the 1st one was to just do $\dfrac{50}{110}(1000)\cdot 25.5 + \dfrac{60}{110}\cdot 1000 \displaystyle\int_{50}^{110} x$ which will give me my $E[X]$ for part a but that didn't work. So need some help understanding these sorts of problems.",,"['probability', 'statistics']"
44,Two different M/G/Infinity queues,Two different M/G/Infinity queues,,"I need some help with this problem. We have a service system where customers arrive randomly, following a Poisson process (intensity Î»). The times that the customers spend in service are independent, following a Weibull distribution with parameters Î± > 0 and Î² > 0. The times in service are also independent of the arrival process. The system has an infinite number of service stations and infinite capacity. In the problem, the system is simulated and we get a set of data. The data is a random sample of number of customers at random times. We do two simulations with the parameters: Î±1 = 0.0241 Î²1 =  2 Î±2 = 1.75 Î²2 = 1 Since the system has got an infinite number of service stations and an infinite capacity, we get two different M/G/â queues, and in this case it is known that L/W = Î», which gives me the expected number of customers in the system L = Î»*W. W is the theoretic average (the expected value) of the Weibull distribution. We get L1 = 19.980 L2 = 2.000 My question is: What's the reason to why the two simulations behave so differently?","I need some help with this problem. We have a service system where customers arrive randomly, following a Poisson process (intensity Î»). The times that the customers spend in service are independent, following a Weibull distribution with parameters Î± > 0 and Î² > 0. The times in service are also independent of the arrival process. The system has an infinite number of service stations and infinite capacity. In the problem, the system is simulated and we get a set of data. The data is a random sample of number of customers at random times. We do two simulations with the parameters: Î±1 = 0.0241 Î²1 =  2 Î±2 = 1.75 Î²2 = 1 Since the system has got an infinite number of service stations and an infinite capacity, we get two different M/G/â queues, and in this case it is known that L/W = Î», which gives me the expected number of customers in the system L = Î»*W. W is the theoretic average (the expected value) of the Weibull distribution. We get L1 = 19.980 L2 = 2.000 My question is: What's the reason to why the two simulations behave so differently?",,"['statistics', 'queueing-theory']"
45,Very tricky probability,Very tricky probability,,"I just came by this ""easy"" question but i am abit worried, ill tell you why. We have a box with 2 yellow balls and 1 red ball. We have also a second box, with 2 red balls and 1 yellow. We randomly choose one box , and one ball from it. Then we pick another ball from it (we don't put the 1st ball back to it). What is the chance that we choose two balls of different color? 1st and foremost i think that it doesn't matter which box we choose because they look symmetric to me. Now theoretically, if we (let's say) choose the 1st box, the possible  picks are 2 {YY and YR} . Also we can see that we will always pick at least one Yellow ball. Therefore the chance is 50% to choose a pair of ball of different color. Now i was quite bored at home, so decided to experiment a bit. I painted 3 paper balls (lol) just to see if this is true. i shacked them and in fact i only piked 1 of them, just to see what color is the pair left. Most of the times it was YR! I don't get this, seriously!!!!","I just came by this ""easy"" question but i am abit worried, ill tell you why. We have a box with 2 yellow balls and 1 red ball. We have also a second box, with 2 red balls and 1 yellow. We randomly choose one box , and one ball from it. Then we pick another ball from it (we don't put the 1st ball back to it). What is the chance that we choose two balls of different color? 1st and foremost i think that it doesn't matter which box we choose because they look symmetric to me. Now theoretically, if we (let's say) choose the 1st box, the possible  picks are 2 {YY and YR} . Also we can see that we will always pick at least one Yellow ball. Therefore the chance is 50% to choose a pair of ball of different color. Now i was quite bored at home, so decided to experiment a bit. I painted 3 paper balls (lol) just to see if this is true. i shacked them and in fact i only piked 1 of them, just to see what color is the pair left. Most of the times it was YR! I don't get this, seriously!!!!",,"['probability', 'combinatorics', 'statistics', 'recreational-mathematics']"
46,Bug in deriving PCA,Bug in deriving PCA,,"Ok, I feel dumb. From what follows it looks like there is a bug in my reasoning. There are $N$ datapoints $\boldsymbol{x}_n$. $\hat{\boldsymbol{u}}$ will be the direction of my principal component, so that $\boldsymbol{y} = \hat{\boldsymbol{u}} \boldsymbol{x}^\top \hat{\boldsymbol{u}}$ and $\left \| \hat{\boldsymbol{u}} \right \|^2=1$. $$\bar{\boldsymbol{x}} = \frac{1}{N} \sum_{n=1}^{N}\boldsymbol{x}_n,$$ $$\bar{\boldsymbol{y}} = \hat{\boldsymbol{u}} \bar{\boldsymbol{x}}^\top \hat{\boldsymbol{u}}.$$ Then there is the tricky part, where I compute the variance of $\boldsymbol{y}$ (which I want to maximise, i.e. finding the direction that spreads the most the data apart). If I write as follows: $$\text{var}[\boldsymbol{y}] = \frac{1}{N} \sum_{n=1}^{N}(\boldsymbol{x}_n^\top \hat{\boldsymbol{u}}-\bar{\boldsymbol{x}}^\top \hat{\boldsymbol{u}})^2= \frac{1}{N} \sum_{n=1}^{N}(\boldsymbol{x}_n^\top \hat{\boldsymbol{u}}-\bar{\boldsymbol{x}}^\top \hat{\boldsymbol{u}})^\top(\boldsymbol{x}_n^\top \hat{\boldsymbol{u}}-\bar{\boldsymbol{x}}^\top \hat{\boldsymbol{u}})= \hat{\boldsymbol{u}}^\top\mathbf{S}\hat{\boldsymbol{u}},$$ $$\mathbf{S}=\frac{1}{N} \sum_{n=1}^{N}(\boldsymbol{x}_n -\bar{\boldsymbol{x}} )(\boldsymbol{x}_n -\bar{\boldsymbol{x}})^\top.$$ And then, with Lagrange multiplier over the constrain $\left \| \hat{\boldsymbol{u}} \right \|^2=1$, I get $$\mathbf{S}\hat{\boldsymbol{u}}=\lambda\hat{\boldsymbol{u}}.$$ If I write instead the variance in the following way, everything collapse... $$\text{var}[\boldsymbol{y}] = \frac{1}{N} \sum_{n=1}^{N}(\hat{\boldsymbol{u}}^\top \boldsymbol{x}_n-\hat{\boldsymbol{u}}^\top \bar{\boldsymbol{x}})^2= \frac{1}{N} \sum_{n=1}^{N}(\hat{\boldsymbol{u}}^\top \boldsymbol{x}_n-\hat{\boldsymbol{u}}^\top \bar{\boldsymbol{x}})^\top (\hat{\boldsymbol{u}}^\top \boldsymbol{x}_n-\hat{\boldsymbol{u}}^\top \bar{\boldsymbol{x}})=\\ =\frac{1}{N} \sum_{n=1}^{N}(\boldsymbol{x}_n -\bar{\boldsymbol{x}})^\top\hat{\boldsymbol{u}}\hat{\boldsymbol{u}}^\top(\boldsymbol{x}_n -\bar{\boldsymbol{x}}) \color{red}{\cancel{\color{black}{=}}} \require{cancel} \color{red}{\cancel{\color{black}{ \frac{1}{N} \sum_{n=1}^{N}(\boldsymbol{x}_n -\bar{\boldsymbol{x}})^\top(\boldsymbol{x}_n -\bar{\boldsymbol{x}})= \text{var}[\boldsymbol{x}] }}}$$ Does anyone understand what is going on here? o.O","Ok, I feel dumb. From what follows it looks like there is a bug in my reasoning. There are $N$ datapoints $\boldsymbol{x}_n$. $\hat{\boldsymbol{u}}$ will be the direction of my principal component, so that $\boldsymbol{y} = \hat{\boldsymbol{u}} \boldsymbol{x}^\top \hat{\boldsymbol{u}}$ and $\left \| \hat{\boldsymbol{u}} \right \|^2=1$. $$\bar{\boldsymbol{x}} = \frac{1}{N} \sum_{n=1}^{N}\boldsymbol{x}_n,$$ $$\bar{\boldsymbol{y}} = \hat{\boldsymbol{u}} \bar{\boldsymbol{x}}^\top \hat{\boldsymbol{u}}.$$ Then there is the tricky part, where I compute the variance of $\boldsymbol{y}$ (which I want to maximise, i.e. finding the direction that spreads the most the data apart). If I write as follows: $$\text{var}[\boldsymbol{y}] = \frac{1}{N} \sum_{n=1}^{N}(\boldsymbol{x}_n^\top \hat{\boldsymbol{u}}-\bar{\boldsymbol{x}}^\top \hat{\boldsymbol{u}})^2= \frac{1}{N} \sum_{n=1}^{N}(\boldsymbol{x}_n^\top \hat{\boldsymbol{u}}-\bar{\boldsymbol{x}}^\top \hat{\boldsymbol{u}})^\top(\boldsymbol{x}_n^\top \hat{\boldsymbol{u}}-\bar{\boldsymbol{x}}^\top \hat{\boldsymbol{u}})= \hat{\boldsymbol{u}}^\top\mathbf{S}\hat{\boldsymbol{u}},$$ $$\mathbf{S}=\frac{1}{N} \sum_{n=1}^{N}(\boldsymbol{x}_n -\bar{\boldsymbol{x}} )(\boldsymbol{x}_n -\bar{\boldsymbol{x}})^\top.$$ And then, with Lagrange multiplier over the constrain $\left \| \hat{\boldsymbol{u}} \right \|^2=1$, I get $$\mathbf{S}\hat{\boldsymbol{u}}=\lambda\hat{\boldsymbol{u}}.$$ If I write instead the variance in the following way, everything collapse... $$\text{var}[\boldsymbol{y}] = \frac{1}{N} \sum_{n=1}^{N}(\hat{\boldsymbol{u}}^\top \boldsymbol{x}_n-\hat{\boldsymbol{u}}^\top \bar{\boldsymbol{x}})^2= \frac{1}{N} \sum_{n=1}^{N}(\hat{\boldsymbol{u}}^\top \boldsymbol{x}_n-\hat{\boldsymbol{u}}^\top \bar{\boldsymbol{x}})^\top (\hat{\boldsymbol{u}}^\top \boldsymbol{x}_n-\hat{\boldsymbol{u}}^\top \bar{\boldsymbol{x}})=\\ =\frac{1}{N} \sum_{n=1}^{N}(\boldsymbol{x}_n -\bar{\boldsymbol{x}})^\top\hat{\boldsymbol{u}}\hat{\boldsymbol{u}}^\top(\boldsymbol{x}_n -\bar{\boldsymbol{x}}) \color{red}{\cancel{\color{black}{=}}} \require{cancel} \color{red}{\cancel{\color{black}{ \frac{1}{N} \sum_{n=1}^{N}(\boldsymbol{x}_n -\bar{\boldsymbol{x}})^\top(\boldsymbol{x}_n -\bar{\boldsymbol{x}})= \text{var}[\boldsymbol{x}] }}}$$ Does anyone understand what is going on here? o.O",,"['statistics', 'eigenvalues-eigenvectors']"
47,Continuity correction: Change P(2 â¤ x < 9) to continuous?,Continuity correction: Change P(2 â¤ x < 9) to continuous?,,Convert discrete probability into continuous probability using continuous correction: attempt: Discrete: P(2 â¤ x < 9) therefore continuous should be Continuous: P(1.5 < X < 8.5) Is this right? or should it be P (1.5 < x < 9)?,Convert discrete probability into continuous probability using continuous correction: attempt: Discrete: P(2 â¤ x < 9) therefore continuous should be Continuous: P(1.5 < X < 8.5) Is this right? or should it be P (1.5 < x < 9)?,,"['statistics', 'data-analysis']"
48,Finding new probability density function with change of variable Y=sqrt(X),Finding new probability density function with change of variable Y=sqrt(X),,"Say we have a given distribution, such as X~No(a, b). I am trying to find the pdf and mean for $Y=\sqrt{X}$. I know the steps for finding the PDF, but since Y can only take on positive values, then the new PDF is only valid for Y>0. Then how does this affect the mean, and how would I go about finding the mean?","Say we have a given distribution, such as X~No(a, b). I am trying to find the pdf and mean for $Y=\sqrt{X}$. I know the steps for finding the PDF, but since Y can only take on positive values, then the new PDF is only valid for Y>0. Then how does this affect the mean, and how would I go about finding the mean?",,"['probability', 'statistics', 'normal-distribution']"
49,How to explain last part of Two-sample KolmogorovâSmirnov test,How to explain last part of Two-sample KolmogorovâSmirnov test,,"Sorry for the dumb question. I am trying to understand the last part of Two-sample KolmogorovâSmirnov test.. The part is: $$ \sqrt{\frac{n + n'}{nn'}} $$ I am thinking that $n$ and $n'$ are the two observations arrays and, since I think that the result has to be a scalar value, $nn'$ could be the dot product while I am unable to understand what is $n + n'$.. Thanks","Sorry for the dumb question. I am trying to understand the last part of Two-sample KolmogorovâSmirnov test.. The part is: $$ \sqrt{\frac{n + n'}{nn'}} $$ I am thinking that $n$ and $n'$ are the two observations arrays and, since I think that the result has to be a scalar value, $nn'$ could be the dot product while I am unable to understand what is $n + n'$.. Thanks",,"['statistics', 'notation']"
50,Statistics Confidence Interval Estimators,Statistics Confidence Interval Estimators,,I'm working on this problem and don't know how to approach it: problem http://puu.sh/5B36g.png Could anyone help steer me in the right direction? Thanks,I'm working on this problem and don't know how to approach it: problem http://puu.sh/5B36g.png Could anyone help steer me in the right direction? Thanks,,['statistics']
51,"Interesting question regarding Weibull distribution, sample mean, and sample median","Interesting question regarding Weibull distribution, sample mean, and sample median",,"Consider a random sample from a Weibull distribution, $X_i \sim WEI(1,2)$. Find approximate values $a$ and $b$ such that for $n=35$: (a) $P[a < \bar{X} < b]=0.95$ (b) $P[a < X_{18:35} < b] = 0.95$ Here $\bar{X}=\frac{X_1+\cdots+X_n}n$ (the sample mean) and $X_{18:35}$ is the sample median. Also the pdf of $X_i$ is $2xe^{-x^2}$. We need $P[\bar{X}<b]-P[\bar{X}<a]=0.95$. But is $(a,b)$ unique? If so, why? I think it's not unique.","Consider a random sample from a Weibull distribution, $X_i \sim WEI(1,2)$. Find approximate values $a$ and $b$ such that for $n=35$: (a) $P[a < \bar{X} < b]=0.95$ (b) $P[a < X_{18:35} < b] = 0.95$ Here $\bar{X}=\frac{X_1+\cdots+X_n}n$ (the sample mean) and $X_{18:35}$ is the sample median. Also the pdf of $X_i$ is $2xe^{-x^2}$. We need $P[\bar{X}<b]-P[\bar{X}<a]=0.95$. But is $(a,b)$ unique? If so, why? I think it's not unique.",,"['probability', 'statistics', 'probability-distributions', 'order-statistics']"
52,Statistical Test for Two Group Means,Statistical Test for Two Group Means,,"I've got two group means and I want to see if the difference is statistically significant. I'm using US Census data and race.  For instance, I'll have one group of 100 people that is 50% white and another group of 50 people that is 70% white. What test should I use? Also if I want to compare a median income for two different groups, is there a different test I should use (as it is impossible to calculate the variance)?","I've got two group means and I want to see if the difference is statistically significant. I'm using US Census data and race.  For instance, I'll have one group of 100 people that is 50% white and another group of 50 people that is 70% white. What test should I use? Also if I want to compare a median income for two different groups, is there a different test I should use (as it is impossible to calculate the variance)?",,['statistics']
53,Unbiased estimator with conditional expectation.,Unbiased estimator with conditional expectation.,,"Suppose that $X$ has a binomial distribution with parameter $N=1$ and $p=1/2$. Y, which is independent of $X$, has a normal distribution with mean $\mu$ and variance 1. Consider the estimator $\mu$ of the form $W_1 = Y + 2X -1$. (Please see my work after parenthesis.) (a.) Is $W_1$ unbiased? (Yes, because $E(W_1) = E(Y) + 2E(X) - 1 = \mu$) (b.) What is the variance of $W_1$? ($Var(W_1)=Var(Y)+4Var(X)=1 + 4p(1-p)$) (c.) Consider the estimator $W_2 = E[W_1|Y]$. Is $W_2$ unbiased? How does its variance compare to that of $W_1$? (I am not sure how to deal with conditional expectation here.) Thank you very much.","Suppose that $X$ has a binomial distribution with parameter $N=1$ and $p=1/2$. Y, which is independent of $X$, has a normal distribution with mean $\mu$ and variance 1. Consider the estimator $\mu$ of the form $W_1 = Y + 2X -1$. (Please see my work after parenthesis.) (a.) Is $W_1$ unbiased? (Yes, because $E(W_1) = E(Y) + 2E(X) - 1 = \mu$) (b.) What is the variance of $W_1$? ($Var(W_1)=Var(Y)+4Var(X)=1 + 4p(1-p)$) (c.) Consider the estimator $W_2 = E[W_1|Y]$. Is $W_2$ unbiased? How does its variance compare to that of $W_1$? (I am not sure how to deal with conditional expectation here.) Thank you very much.",,"['statistics', 'parameter-estimation']"
54,Probability: Why assume an equal relevant magnitude of two mutually exclusive events for P(A|B)?,Probability: Why assume an equal relevant magnitude of two mutually exclusive events for P(A|B)?,,"While reading Grinstead and Snellâs Introduction to Probability p.134, I came across the following: ""Let Î© = {Ï1,Ï2,...,Ïr} be the original sample space with distribution function m(Ïj) assigned. Suppose we learn that the event E has occurred. We want to assign a new distribution function m(Ïj|E) to Î© to reflect this fact. Clearly, if a sample point Ïj is not in E, we want m(Ïj|E) = 0. Moreover, in the absence of information to the contrary, it is reasonable to assume that the probabilities for Ïk in E should have the same relative magnitudes that they had before we learned that E had occurred. "" I do not study mathematics. I'm reading this on my own so I do not have a professor to ask. Why must the assumption hold?","While reading Grinstead and Snellâs Introduction to Probability p.134, I came across the following: ""Let Î© = {Ï1,Ï2,...,Ïr} be the original sample space with distribution function m(Ïj) assigned. Suppose we learn that the event E has occurred. We want to assign a new distribution function m(Ïj|E) to Î© to reflect this fact. Clearly, if a sample point Ïj is not in E, we want m(Ïj|E) = 0. Moreover, in the absence of information to the contrary, it is reasonable to assume that the probabilities for Ïk in E should have the same relative magnitudes that they had before we learned that E had occurred. "" I do not study mathematics. I'm reading this on my own so I do not have a professor to ask. Why must the assumption hold?",,"['probability', 'statistics']"
55,Independent sequences,Independent sequences,,"Let $\{x_i\}_{i = 1, ...,n}$ , $\{y_i\}_{i = 1, ...,n}$ be sequences generated by a pseudo-random number generator using different seed keys, for example $ x_0$ and $y_0$. Are $\{x_i\}$ and $\{y_i\}$ independent? If not, under what conditions can I say $\{x_i\}$ and $\{y_i\}$ are independent?","Let $\{x_i\}_{i = 1, ...,n}$ , $\{y_i\}_{i = 1, ...,n}$ be sequences generated by a pseudo-random number generator using different seed keys, for example $ x_0$ and $y_0$. Are $\{x_i\}$ and $\{y_i\}$ independent? If not, under what conditions can I say $\{x_i\}$ and $\{y_i\}$ are independent?",,"['probability', 'sequences-and-series', 'statistics', 'random']"
56,Calculating the probability of an event occurring in a specific time period,Calculating the probability of an event occurring in a specific time period,,"I am confused at how to approach the following question, i.e. what probability formula I am supposed to use. If the probability of a flood is 0.12 during a year, what is the probability of two floods over the next 10 years...? I have thought perhaps trying Geometric distribution at first, but it didn't seem to work out properly. I also tried Poisson, but it turned out to be quite a small number... which doesn't seem viable. So my question is, how can I go about solving this and which probability distribution am I supposed to use? Thanks in advance","I am confused at how to approach the following question, i.e. what probability formula I am supposed to use. If the probability of a flood is 0.12 during a year, what is the probability of two floods over the next 10 years...? I have thought perhaps trying Geometric distribution at first, but it didn't seem to work out properly. I also tried Poisson, but it turned out to be quite a small number... which doesn't seem viable. So my question is, how can I go about solving this and which probability distribution am I supposed to use? Thanks in advance",,"['probability', 'statistics']"
57,Unbiased estimators of theta,Unbiased estimators of theta,,"Suppose $\hat\theta_1$ and theta $\hat\theta_2$ are both uncorrelated and unbiased estimators of $\theta$, and that $\text{var}\hat\theta_1=2\cdot \text{var}(\hat\theta_2)$. a) Show that for any constant $c$, the weighted average theta $\hat\theta_3= c\cdot\hat\theta_1 +(1-c)\cdot\hat\theta_2$ is an unbiased estimator of $\theta$. b) Find the $c$ for which $\hat\theta_3$ has the smallest MSE. c) Are there any values for $c$ $(0\le c\le 1)$ for which $\hat\theta_3$ is better (in the sense of MSE) than both $\hat\theta_1$ and $\hat\theta_2$?","Suppose $\hat\theta_1$ and theta $\hat\theta_2$ are both uncorrelated and unbiased estimators of $\theta$, and that $\text{var}\hat\theta_1=2\cdot \text{var}(\hat\theta_2)$. a) Show that for any constant $c$, the weighted average theta $\hat\theta_3= c\cdot\hat\theta_1 +(1-c)\cdot\hat\theta_2$ is an unbiased estimator of $\theta$. b) Find the $c$ for which $\hat\theta_3$ has the smallest MSE. c) Are there any values for $c$ $(0\le c\le 1)$ for which $\hat\theta_3$ is better (in the sense of MSE) than both $\hat\theta_1$ and $\hat\theta_2$?",,"['probability', 'statistics', 'probability-distributions', 'mean-square-error']"
58,An inequality involving expectation,An inequality involving expectation,,"Let $f,g$ be two pdfs, and suppose $X$ is a random variable that has pdf $f$. Is it necessarily true that $E[f(X)] \ge E[g(X)]$? Although I doubt this will help, but I got this problem from studying the Kullback-Leibler divergence , which is defined as $D(f,g) = E[ln (f(X)/g(X))]$ (with $X$ having pdf $f$ like above). It can be shown using Jensen's inequality that $D(f,g) \ge 0$, which is equivalent to saying that $E[ln(f(X))] \ge E[ln(g(X))]$. But my question removes the $ln$ and I am wondering whether it will still be true. In fact, I hypothesize that not only is it true for the $ln$ function, but it will be true for any function that is increasing (which includes the identity function, which is my question here). So as a bonus, maybe we could ask if $E[h(f(X))] \ge E[h(g(X))]$ where $h$ is any strictly increasing function on positive reals. It seems like a continuous form of the rearrangement inequality to me.","Let $f,g$ be two pdfs, and suppose $X$ is a random variable that has pdf $f$. Is it necessarily true that $E[f(X)] \ge E[g(X)]$? Although I doubt this will help, but I got this problem from studying the Kullback-Leibler divergence , which is defined as $D(f,g) = E[ln (f(X)/g(X))]$ (with $X$ having pdf $f$ like above). It can be shown using Jensen's inequality that $D(f,g) \ge 0$, which is equivalent to saying that $E[ln(f(X))] \ge E[ln(g(X))]$. But my question removes the $ln$ and I am wondering whether it will still be true. In fact, I hypothesize that not only is it true for the $ln$ function, but it will be true for any function that is increasing (which includes the identity function, which is my question here). So as a bonus, maybe we could ask if $E[h(f(X))] \ge E[h(g(X))]$ where $h$ is any strictly increasing function on positive reals. It seems like a continuous form of the rearrangement inequality to me.",,"['statistics', 'inequality', 'expectation']"
59,Expected value of a certain game,Expected value of a certain game,,Consider a game with n players playing m rounds. Players play in the same time. Player wins k dollars with probability p and loses everything otherwise. If player loses the rest divides his money between themselves. I'd like to calculate expected value. I don't have much experience in probability theory. If this is known problem i would really appreciate any references.,Consider a game with n players playing m rounds. Players play in the same time. Player wins k dollars with probability p and loses everything otherwise. If player loses the rest divides his money between themselves. I'd like to calculate expected value. I don't have much experience in probability theory. If this is known problem i would really appreciate any references.,,"['probability', 'statistics', 'probability-theory']"
60,Probability of Combinations which have different probabilties.,Probability of Combinations which have different probabilties.,,"I cannot figure this out. The problem goes A boy has a bag filled with balls: 1 red 2 green 4 Blue 7 White A child plays a game in which he withdraws one ball, writes down its color, and then replaces the ball. In order to win the game he must write down at least one of each color of ball. What is the probability that this will happen on, or before the N-th trial??? I initially assumed this problem involved a geometric probability distribution function, based on the 4 probabilities multiplied together, for N trials. But I dont know. Please help!","I cannot figure this out. The problem goes A boy has a bag filled with balls: 1 red 2 green 4 Blue 7 White A child plays a game in which he withdraws one ball, writes down its color, and then replaces the ball. In order to win the game he must write down at least one of each color of ball. What is the probability that this will happen on, or before the N-th trial??? I initially assumed this problem involved a geometric probability distribution function, based on the 4 probabilities multiplied together, for N trials. But I dont know. Please help!",,"['probability', 'statistics', 'discrete-mathematics', 'probability-distributions', 'combinations']"
61,Finding meaningful differences in a set of time measures,Finding meaningful differences in a set of time measures,,"An individual is presented with a list of words, one at a time, and asked to answer the first thing to comes to her mind. The delay of all answers is recorded. So I have a list of words, each paired with the time it took for the individual to find an answer (response delay). Some psychological studies indicate that words with a longer or shorter than average response delay have some special meaning in the subject's psychological profile. Despite the fact that this statement could be right or wrong, I'm interested in the best mathematical approach to find such words that exhibit a ""longer than average"" or ""shorter than average"" response time. So far we've been just taking the average response time, and setting some fixed percentage of delay we consider long or short enough. This is of course a very simplistic approach. I think I could apply some standard hypothesis tests to answer the question: what are the odds that this response delay is equal to the average? and keep those results with a low p-value, but I'm not very versed on statistics, so I don't know the exact test I would need here. Also, I'm interested in a slightly more complicated version, where the subject is presented with two list of words intermingled. The words from the first list are considered neutral, and are expected to have more or less the same response delay. However, the words from the second list are loaded, which means we expect they to hit something in the subject's subconscious part, and so he might have longer or shorter response delays. I would like to take the times from the first list to make a baseline for what are considered average response delays, and then check the times in the second list, looking for longer or shorter than expected response delays. What kind of test would I need here?","An individual is presented with a list of words, one at a time, and asked to answer the first thing to comes to her mind. The delay of all answers is recorded. So I have a list of words, each paired with the time it took for the individual to find an answer (response delay). Some psychological studies indicate that words with a longer or shorter than average response delay have some special meaning in the subject's psychological profile. Despite the fact that this statement could be right or wrong, I'm interested in the best mathematical approach to find such words that exhibit a ""longer than average"" or ""shorter than average"" response time. So far we've been just taking the average response time, and setting some fixed percentage of delay we consider long or short enough. This is of course a very simplistic approach. I think I could apply some standard hypothesis tests to answer the question: what are the odds that this response delay is equal to the average? and keep those results with a low p-value, but I'm not very versed on statistics, so I don't know the exact test I would need here. Also, I'm interested in a slightly more complicated version, where the subject is presented with two list of words intermingled. The words from the first list are considered neutral, and are expected to have more or less the same response delay. However, the words from the second list are loaded, which means we expect they to hit something in the subject's subconscious part, and so he might have longer or shorter response delays. I would like to take the times from the first list to make a baseline for what are considered average response delays, and then check the times in the second list, looking for longer or shorter than expected response delays. What kind of test would I need here?",,"['probability', 'statistics']"
62,"The MLE of a $N(\theta, 1)$ distribution",The MLE of a  distribution,"N(\theta, 1)","I am trying to find the Maximum Likelihood Estimator of an i.i.d. sample $X_1, \ldots, X_n$ arising from the model $N(\theta, 1)$, where $\theta \in [0,\infty)$. I have done this problem previously where the mean was not restricted to be non-negative, and found the MLE to be equal to the sample mean (as you would expect). Please could you explain why this situation is different and how it should be approached, as this is confusing me quite a bit! Many thanks.","I am trying to find the Maximum Likelihood Estimator of an i.i.d. sample $X_1, \ldots, X_n$ arising from the model $N(\theta, 1)$, where $\theta \in [0,\infty)$. I have done this problem previously where the mean was not restricted to be non-negative, and found the MLE to be equal to the sample mean (as you would expect). Please could you explain why this situation is different and how it should be approached, as this is confusing me quite a bit! Many thanks.",,"['statistics', 'normal-distribution']"
63,Finding a marginal PDF of a joint probability distribution,Finding a marginal PDF of a joint probability distribution,,"I understand the idea of how to do it, but I'm currently getting a constant as my marginal PDF, which doesn't make sense. The overall distribution is as follows:  $f(x,y) = 5ye^{-xy}$ for $0 < x, 0.2 < y < 0.4$ I'm trying to find the probability that $0 < x < 2$ given that $y = 0.25$, which should be $\frac{f(0<x<2,y=0.25)}{f_y(0.25)}$. As a minor double-check, should the top be a single integral evaluated with $y=0.25$? The bulk of my question is that I'm finding $f_y$ to be a constant, which doesn't make sense. What am I doing wrong? $f_y = \int_{0}^{\infty} 5ye^{-xy} dx = 5[-e^{-xy}]_{0}^{\infty} = 5[0-(-1)]=5$","I understand the idea of how to do it, but I'm currently getting a constant as my marginal PDF, which doesn't make sense. The overall distribution is as follows:  $f(x,y) = 5ye^{-xy}$ for $0 < x, 0.2 < y < 0.4$ I'm trying to find the probability that $0 < x < 2$ given that $y = 0.25$, which should be $\frac{f(0<x<2,y=0.25)}{f_y(0.25)}$. As a minor double-check, should the top be a single integral evaluated with $y=0.25$? The bulk of my question is that I'm finding $f_y$ to be a constant, which doesn't make sense. What am I doing wrong? $f_y = \int_{0}^{\infty} 5ye^{-xy} dx = 5[-e^{-xy}]_{0}^{\infty} = 5[0-(-1)]=5$",,"['probability', 'statistics']"
64,Elevated percentage of standard BMI associated with Carcinoma,Elevated percentage of standard BMI associated with Carcinoma,,"Percentage of Standard BMI$\hspace{30mm}$Male$\hspace{40mm}$Female $\hspace{70mm}$Case$\hspace{10mm}$Control$\hspace{20mm}$Case$\hspace{10mm}$Control <130$\hspace{61mm}$123$\hspace{12mm}$150$\hspace{27mm}$55$\hspace{15mm}$59 $\ge$130$\hspace{61mm}$85$\hspace{13mm}$45$\hspace{29mm}$51$\hspace{15mm}$46 *Standard BMI: male, 22.1; female, 20.6. Percentage of standard BMI=(observed BMI/standard BMI)x100 Is elevated percentage of standard BMI associated with renal cell carcinoma after controlling the effects of sex? Anyone know what kind of test to run on the data?","Percentage of Standard BMI$\hspace{30mm}$Male$\hspace{40mm}$Female $\hspace{70mm}$Case$\hspace{10mm}$Control$\hspace{20mm}$Case$\hspace{10mm}$Control <130$\hspace{61mm}$123$\hspace{12mm}$150$\hspace{27mm}$55$\hspace{15mm}$59 $\ge$130$\hspace{61mm}$85$\hspace{13mm}$45$\hspace{29mm}$51$\hspace{15mm}$46 *Standard BMI: male, 22.1; female, 20.6. Percentage of standard BMI=(observed BMI/standard BMI)x100 Is elevated percentage of standard BMI associated with renal cell carcinoma after controlling the effects of sex? Anyone know what kind of test to run on the data?",,['statistics']
65,Help needed......Statistics probability and z table...stuck,Help needed......Statistics probability and z table...stuck,,"The question is: **An estimated 1.8 million students take on student loans to pay ever-rising tuition and room and board (New York Times, April 17, 2009). It is also known that the average cumulative debt of recent college graduates is about $22,500. The cumulative debt for college graduates is normally distributed with a standard deviation of $7,000. Approximately how many recent college graduates have accumulated a student loan of more than $30,000?** What I've done: 30,000-22500/7000=1.071428571 I then looked this up on the z table but that wouldn't give me the correct answer. What exactly am I doing wrong?","The question is: **An estimated 1.8 million students take on student loans to pay ever-rising tuition and room and board (New York Times, April 17, 2009). It is also known that the average cumulative debt of recent college graduates is about $22,500. The cumulative debt for college graduates is normally distributed with a standard deviation of $7,000. Approximately how many recent college graduates have accumulated a student loan of more than $30,000?** What I've done: 30,000-22500/7000=1.071428571 I then looked this up on the z table but that wouldn't give me the correct answer. What exactly am I doing wrong?",,"['statistics', 'statistical-inference', 'order-statistics', 'descriptive-statistics']"
66,The probability that a randomly chosen grain weighs less than the mean grain weight,The probability that a randomly chosen grain weighs less than the mean grain weight,,"If Y has a log-normal distribution with parameters $\mu$ and $\sigma^2$, it can be shown that $E(Y)=e^\frac{\mu + \sigma^2}{2}$ and $V(Y)=e^{2\mu +\sigma^2}(e^{\sigma^2}-1)$. The grains composing polyerstalline metals tend to have weights that follow a log-normal distribution. For a type of aluminum, grain weights have a log-normal distribution with $\mu=3$ and $\sigma^2=4$ (in units of $10^{-4}$g). Part A: Find the mean and variance of the grain weights. Answer: $E(Y)=e^\frac{19}{2}$ and $V(Y)=e^{22}(e^{16}-1)$ Part B: Find an interval in which at least 75% of the grain weights should lie. Answer: (0, $e^\frac{19}{2}+2e^{11}(e^{16}-1)^{1/2}$) Part C: Find the probability that a randomly chosen grain weighs less than the mean grain weight. I am unsure what to do for this part. Can someone sketch the procedures for me? As of 12/28/13 there still instead an answer to this question.","If Y has a log-normal distribution with parameters $\mu$ and $\sigma^2$, it can be shown that $E(Y)=e^\frac{\mu + \sigma^2}{2}$ and $V(Y)=e^{2\mu +\sigma^2}(e^{\sigma^2}-1)$. The grains composing polyerstalline metals tend to have weights that follow a log-normal distribution. For a type of aluminum, grain weights have a log-normal distribution with $\mu=3$ and $\sigma^2=4$ (in units of $10^{-4}$g). Part A: Find the mean and variance of the grain weights. Answer: $E(Y)=e^\frac{19}{2}$ and $V(Y)=e^{22}(e^{16}-1)$ Part B: Find an interval in which at least 75% of the grain weights should lie. Answer: (0, $e^\frac{19}{2}+2e^{11}(e^{16}-1)^{1/2}$) Part C: Find the probability that a randomly chosen grain weighs less than the mean grain weight. I am unsure what to do for this part. Can someone sketch the procedures for me? As of 12/28/13 there still instead an answer to this question.",,"['statistics', 'normal-distribution']"
67,Probabilities with three events,Probabilities with three events,,"I have a probability problem where I have to calculate the total probability and a Bayes probability from two events. The chances as given are: $P(A) = 0.1, P(A^c) = 0.9$ $P(B|A) = 0.9, P(B|A^c) = 0.05$ Using these probabilities, I was able to work out the total probability: $P(B) = P(A)P(B|A) + P(A^c)P(B|A^c) = (0.1*0.9) + (0.9*0.05) = 0.135$ And the chance of A given B using Bayes: $P(A|B) = \frac{P(B|A)P(A)}{P(B)} = \frac{0.9*0.1}{0.135} = \frac{2}{3}$ Pretty standard stuff so far. However, the last part of the question adds an event C. The chance of C given A and B is 0.1. Knowing this, I have to calculate the chance of an outcome having A, B, and C. Is this really as easy as $P(A \cap B \cap C) = P(A)*P(B)*P(C) = 0.1*0.9*0.1 = 0.009?$","I have a probability problem where I have to calculate the total probability and a Bayes probability from two events. The chances as given are: $P(A) = 0.1, P(A^c) = 0.9$ $P(B|A) = 0.9, P(B|A^c) = 0.05$ Using these probabilities, I was able to work out the total probability: $P(B) = P(A)P(B|A) + P(A^c)P(B|A^c) = (0.1*0.9) + (0.9*0.05) = 0.135$ And the chance of A given B using Bayes: $P(A|B) = \frac{P(B|A)P(A)}{P(B)} = \frac{0.9*0.1}{0.135} = \frac{2}{3}$ Pretty standard stuff so far. However, the last part of the question adds an event C. The chance of C given A and B is 0.1. Knowing this, I have to calculate the chance of an outcome having A, B, and C. Is this really as easy as $P(A \cap B \cap C) = P(A)*P(B)*P(C) = 0.1*0.9*0.1 = 0.009?$",,"['probability', 'statistics', 'bayes-theorem']"
68,Sample size from population?,Sample size from population?,,"This is probably very rudimentary maths, but given a strict population size ($N = 20$ for example), is the sample size any number $<N$? For use in calculation of confidence intervals using a population size, all of the formulas use $n$ and not $N$, meaning I need a sample size rather than population size. $$N = 20;\qquad n = 1, 2, 3, \dots , 19$$  $$N = 10;\qquad n = 1, 2, 3, \dots , 9$$ Is this right? This isn't really a maths question, but moreso a semantics question; is a sample size just taken from a population? A sample size of $n = 5$ can come from a population size of $N = 10$? Basically: for determining confidence intervals of a population, is the sample size used in calculations an arbitrary number as long as it fits within the amount of the population? edit; according to the textbook (and online sources), a sample should be an approximation of a population, so $n = N - 1$? Math is my weakest area, so I apologize if this is silly.","This is probably very rudimentary maths, but given a strict population size ($N = 20$ for example), is the sample size any number $<N$? For use in calculation of confidence intervals using a population size, all of the formulas use $n$ and not $N$, meaning I need a sample size rather than population size. $$N = 20;\qquad n = 1, 2, 3, \dots , 19$$  $$N = 10;\qquad n = 1, 2, 3, \dots , 9$$ Is this right? This isn't really a maths question, but moreso a semantics question; is a sample size just taken from a population? A sample size of $n = 5$ can come from a population size of $N = 10$? Basically: for determining confidence intervals of a population, is the sample size used in calculations an arbitrary number as long as it fits within the amount of the population? edit; according to the textbook (and online sources), a sample should be an approximation of a population, so $n = N - 1$? Math is my weakest area, so I apologize if this is silly.",,"['statistics', 'definition', 'sampling']"
69,Finding Conditional Probabilities/Requirements on lambda,Finding Conditional Probabilities/Requirements on lambda,,"I'm looking to get some insight into a moderately challenging conditional probability problem: Consider a sequence of random variables $X_1$,$X_2$...,$X_n$ which each take the values 0 and 1. Assume that Pr($X_j$ = 1) = 1 - Pr($X_{j-1}$= 0) = $\phi$, $\hspace{5mm}$j =1,...,n where 0 < $\phi$ < 1 and that $\hspace{10mm}$Pr($X_j$ = 1|$X_{j-1}$=1) = $\lambda$, $\hspace{5mm}$j = 2,...,n. (a) Find Pr($X_j$ = 0|$X_{j-1}$=1), Pr($X_j$ = 1|$X_{j-1}$=0),Pr($X_j$ = 0|$X_{j-1}$=0). (b) Find the requirements on $\lambda$ so that this describes a valid probability distribution for $X_1$,$X_2$...,$X_n$. So for part (a) I have: 1-$\lambda$, $\phi$, and 1-$\phi$, respectively. I do not know how to begin part (b).","I'm looking to get some insight into a moderately challenging conditional probability problem: Consider a sequence of random variables $X_1$,$X_2$...,$X_n$ which each take the values 0 and 1. Assume that Pr($X_j$ = 1) = 1 - Pr($X_{j-1}$= 0) = $\phi$, $\hspace{5mm}$j =1,...,n where 0 < $\phi$ < 1 and that $\hspace{10mm}$Pr($X_j$ = 1|$X_{j-1}$=1) = $\lambda$, $\hspace{5mm}$j = 2,...,n. (a) Find Pr($X_j$ = 0|$X_{j-1}$=1), Pr($X_j$ = 1|$X_{j-1}$=0),Pr($X_j$ = 0|$X_{j-1}$=0). (b) Find the requirements on $\lambda$ so that this describes a valid probability distribution for $X_1$,$X_2$...,$X_n$. So for part (a) I have: 1-$\lambda$, $\phi$, and 1-$\phi$, respectively. I do not know how to begin part (b).",,"['probability', 'statistics', 'conditional-probability', 'expectation']"
70,"Central limit theorem, taking repeated measurements","Central limit theorem, taking repeated measurements",,"I'm using a book from R.J.Barlow, a stat book for science students. I was confused by some of the wordings in the book.The first two paragraph are some true statements.The bottom paragraph confused me most. Suppose you took n measurements right? e.g. 1.0,2.0,3.0,4.0? If n =4 , 4 measurements and you took average of them to get a mean, How could this mean fluctuate you only have a fixed mean = 2.5 ? Is it saying that they repeat taking these 4 measurements? if YES by how many times? ""That's where the confusion comes in. Is the xi mentioned here the variable or a set of variables i.e.sample from a population? Is there anything to do with sample mean? sample variable? Population mean? I got completely confused by some of the wordings. Thanks for any help!","I'm using a book from R.J.Barlow, a stat book for science students. I was confused by some of the wordings in the book.The first two paragraph are some true statements.The bottom paragraph confused me most. Suppose you took n measurements right? e.g. 1.0,2.0,3.0,4.0? If n =4 , 4 measurements and you took average of them to get a mean, How could this mean fluctuate you only have a fixed mean = 2.5 ? Is it saying that they repeat taking these 4 measurements? if YES by how many times? ""That's where the confusion comes in. Is the xi mentioned here the variable or a set of variables i.e.sample from a population? Is there anything to do with sample mean? sample variable? Population mean? I got completely confused by some of the wordings. Thanks for any help!",,['statistics']
71,A problem about Bivariate Transformation,A problem about Bivariate Transformation,,"I need some help on the following problem: Let $X_1$ and $X_2$ be two random variables with the following joint distribution pdf $$f_{X_1,X_2}(x_1,x_2)= \begin{cases} 4x_1x_2,  & \text{if   } 0<x_1<1,0<x_2<1 \\ 0, & \text{otherwise}  \\ \end{cases}.$$   Find the joint pdf of random variables $U=X_1/X_2$ and $V=X_1X_2$. By using the transformation of the formula, I got the result $f_{U,V}(u,v)=2u^{-1}v$. I hope this is right.  However, I may got stuck on the domain of $U$ and $V$. I may feel that the domain for $U$ is $(0,+\infty)$ and for $V$ is $(0,1)$. However, if I integrate the pdf I got, the result is NOT 1. I really do not know where went wrong. Any help, please.","I need some help on the following problem: Let $X_1$ and $X_2$ be two random variables with the following joint distribution pdf $$f_{X_1,X_2}(x_1,x_2)= \begin{cases} 4x_1x_2,  & \text{if   } 0<x_1<1,0<x_2<1 \\ 0, & \text{otherwise}  \\ \end{cases}.$$   Find the joint pdf of random variables $U=X_1/X_2$ and $V=X_1X_2$. By using the transformation of the formula, I got the result $f_{U,V}(u,v)=2u^{-1}v$. I hope this is right.  However, I may got stuck on the domain of $U$ and $V$. I may feel that the domain for $U$ is $(0,+\infty)$ and for $V$ is $(0,1)$. However, if I integrate the pdf I got, the result is NOT 1. I really do not know where went wrong. Any help, please.",,"['probability', 'statistics']"
72,How to sample N points between 0 and R if they are exponentially distributed?,How to sample N points between 0 and R if they are exponentially distributed?,,"The density of my points x $\in$ [0,R] is exponential: $\rho(x) \approx e^x$ How can I sample N points from there? Thanks,","The density of my points x $\in$ [0,R] is exponential: $\rho(x) \approx e^x$ How can I sample N points from there? Thanks,",,"['statistics', 'probability-distributions']"
73,Probability of at least one event occuring through n trials.,Probability of at least one event occuring through n trials.,,"How do I calculate the probability of an event happening at least once given a specific amount of trials? For example: There are 5 large black dogs, 2 large white dogs, 3 small black dogs, and 4 small white dogs. You decide to walk four of them at random. What is the probability that at least one of the first two walked dogs is white? I know that to solve for the probability for ""at least one"" problems is just the opposite of ""none"" (ie. P(event happening >= 1 times) = 1 - P(event happening 0 times)), but I'm not sure how to do it for two trials. I know the answer to four decimal places is P = 0.6923, but I don't particularly grasp the process involved. Math is not my strongest suit, so I apologize in advance if this is a trivial question. As an aside, how would I answer the following? Find the probability that 3 of the 4 dogs were small white dogs? I don't want to make a separate question since these are under the same problem. I'm not too familiar with combinatorics/permutations, but I know they are involved.","How do I calculate the probability of an event happening at least once given a specific amount of trials? For example: There are 5 large black dogs, 2 large white dogs, 3 small black dogs, and 4 small white dogs. You decide to walk four of them at random. What is the probability that at least one of the first two walked dogs is white? I know that to solve for the probability for ""at least one"" problems is just the opposite of ""none"" (ie. P(event happening >= 1 times) = 1 - P(event happening 0 times)), but I'm not sure how to do it for two trials. I know the answer to four decimal places is P = 0.6923, but I don't particularly grasp the process involved. Math is not my strongest suit, so I apologize in advance if this is a trivial question. As an aside, how would I answer the following? Find the probability that 3 of the 4 dogs were small white dogs? I don't want to make a separate question since these are under the same problem. I'm not too familiar with combinatorics/permutations, but I know they are involved.",,"['probability', 'statistics', 'probability-theory']"
74,Multivariate normal distribution conditional on two random variables,Multivariate normal distribution conditional on two random variables,,"I am given a dataset with in each column a set of data pertaining to a different random variable. I know that the data are normally distributed. Now how can I find the estimated mean and variance of the last observation from the first random variable conditional on the last observation of the other two random variables? I thought of finding the conditional pdf and the calculating the maximum likelihood estimator. However, I am not sure how I can condition on two random variables. Could anyone help me in the right direction? Thank you in advance for your help.","I am given a dataset with in each column a set of data pertaining to a different random variable. I know that the data are normally distributed. Now how can I find the estimated mean and variance of the last observation from the first random variable conditional on the last observation of the other two random variables? I thought of finding the conditional pdf and the calculating the maximum likelihood estimator. However, I am not sure how I can condition on two random variables. Could anyone help me in the right direction? Thank you in advance for your help.",,"['statistics', 'probability-distributions', 'statistical-inference']"
75,How to decide for the number of decimals for rounding,How to decide for the number of decimals for rounding,,"I am computing some logarithms and have a problem for the result's presentation. These are my numbers. 1.0986122886681098, 1.0986122886681098, 0.0, 0.8109302162163288, 1.0986122886681098, 1.0986122886681098, 1.0986122886681098, 1.0986122886681098, 1.0986122886681098, 0.4054651081081644, 1.0986122886681098, 1.0986122886681098, 0.0, 1.0986122886681098, 1.0986122886681098, 1.0986122886681098, 1.0986122886681098 With the standard deviation 0.37440091166480144 Showing the complete precision is too much, it would not help to understand the actual context. So where do I round? To two decimal places, four? Is there a rule of thumb? Can the standard deviation help here? The input was: $\log(3/1)$, $\log(3/2)$, $\log(3/3)$ and $2 \cdot \log(3/1)$, $2 \cdot \log(3/2)$, $2 \cdot \log(3/3)$","I am computing some logarithms and have a problem for the result's presentation. These are my numbers. 1.0986122886681098, 1.0986122886681098, 0.0, 0.8109302162163288, 1.0986122886681098, 1.0986122886681098, 1.0986122886681098, 1.0986122886681098, 1.0986122886681098, 0.4054651081081644, 1.0986122886681098, 1.0986122886681098, 0.0, 1.0986122886681098, 1.0986122886681098, 1.0986122886681098, 1.0986122886681098 With the standard deviation 0.37440091166480144 Showing the complete precision is too much, it would not help to understand the actual context. So where do I round? To two decimal places, four? Is there a rule of thumb? Can the standard deviation help here? The input was: $\log(3/1)$, $\log(3/2)$, $\log(3/3)$ and $2 \cdot \log(3/1)$, $2 \cdot \log(3/2)$, $2 \cdot \log(3/3)$",,"['probability', 'statistics', 'logarithms']"
76,Pdf of the n-fold transformation of uniform random variables?,Pdf of the n-fold transformation of uniform random variables?,,"For independent variables $X_1$,$X_2$, $X_3$, $...$, $X_n$ with $n>2$, all uniformly distributed in the interval $[0,1]$ and if $X_1 < X_2 < ... < X_n$ I am trying to find the pdf of $Y_n$ = $\frac{X_n}{(X_1 X_2 \dots X_n)^\frac{1}{n-1}}$ How should I go about this? I used $logY$ to break down the fraction into a sum of $log X_k$s but kind of stuck from here. Playing around with the following facts but feel like there could be a better way.. The log of a uniform random variable on [0,1] follows an exponential distribution with mean 1, and the sum of $n$ exponential random variables with mean 1 follows $Gamma(1, n)$ Any suggestions on how to tackle this?","For independent variables $X_1$,$X_2$, $X_3$, $...$, $X_n$ with $n>2$, all uniformly distributed in the interval $[0,1]$ and if $X_1 < X_2 < ... < X_n$ I am trying to find the pdf of $Y_n$ = $\frac{X_n}{(X_1 X_2 \dots X_n)^\frac{1}{n-1}}$ How should I go about this? I used $logY$ to break down the fraction into a sum of $log X_k$s but kind of stuck from here. Playing around with the following facts but feel like there could be a better way.. The log of a uniform random variable on [0,1] follows an exponential distribution with mean 1, and the sum of $n$ exponential random variables with mean 1 follows $Gamma(1, n)$ Any suggestions on how to tackle this?",,['statistics']
77,Standard deviation of less than one,Standard deviation of less than one,,How would I find the approximate percentage of values within a standard deviation of less than one on the normal model? Chebyshev's rule is only used when the standard deviation is greater than or equal to 1.,How would I find the approximate percentage of values within a standard deviation of less than one on the normal model? Chebyshev's rule is only used when the standard deviation is greater than or equal to 1.,,"['statistics', 'normal-distribution', 'standard-deviation']"
78,Suppose that $n$ indistinguishable balls are arranged in N distinguishable boxes,Suppose that  indistinguishable balls are arranged in N distinguishable boxes,n,"Suppose that $n$ indistinguishable balls are to be arranged in N distinguishable boxes so that each distinguishable arrangement is equally likely. If n is greater than or equal to N, what is that probability that no box will be empty?","Suppose that $n$ indistinguishable balls are to be arranged in N distinguishable boxes so that each distinguishable arrangement is equally likely. If n is greater than or equal to N, what is that probability that no box will be empty?",,"['probability', 'statistics']"
79,Error minimum when it is minimum,Error minimum when it is minimum,,Let $X$ be a continuous random variable with pdf $f > 0$. I need to show that $E|X-c|$ is minimum when c=median. Now after some algebraic manipulation I have got the following expression for $$E|X-c|=2cP(X \leq c) -c +2\int_c^\infty xf(x)dx -E[X]$$ How to proceed from there ?,Let $X$ be a continuous random variable with pdf $f > 0$. I need to show that $E|X-c|$ is minimum when c=median. Now after some algebraic manipulation I have got the following expression for $$E|X-c|=2cP(X \leq c) -c +2\int_c^\infty xf(x)dx -E[X]$$ How to proceed from there ?,,['probability']
80,Inverse quantile function for $\sin^2(x)$,Inverse quantile function for,\sin^2(x),"What is the transformation which takes the standard uniform distribution $U[0,1]$ to the following probability density function $f$: $$f(x)=\sin^{2}x$$ Where $x\in\left[0,\pi\right]$ and $f\in[0,1]$. (This is useful for distributing points on $S^3$, the hypersphere in 4D, because the Jacobian of the transformation from uniform rectilinear coordinates to spherical coordinates creates a term containing $\sin^{2}x$.) Following the method of Inverse transform sampling , we need to integrate and invert the PDF; applying the standard uniform distribution to the inverted CDF gives the desired distribution. The CDF is easy enough to calculate: $$F(x) = \frac{x}{2}-\frac{1}{4}\sin2x+C$$ Where $C$ is determined by the bounds. However, I don't see how to invert this function (regardless of $C$). Here's a numerical simulation of the inverse after $F(x)$ is normalized to [0,1] by multiplying by $\frac{2}{\pi}$: Although the CDF appears to be 1-to-1 and onto in the relevant domain, I don't see how to get a true analytical inverse because the combination of $arcsin$ and the linear term just doesn't work out. I tried a Taylor's series approximation and got $\sqrt[3]{3U}$, but that only works for small $U$, say $U\in\left[0,0.5\right]$, and it definitely doesn't cover the full range of $\sin^{2}x$. The question is how do you transform the standard uniform distribution to $\sin^{2}x$, as stated above. Uniform random number generators are readily available, but I want to draw samples from the $\sin^{2}x$ distribution.","What is the transformation which takes the standard uniform distribution $U[0,1]$ to the following probability density function $f$: $$f(x)=\sin^{2}x$$ Where $x\in\left[0,\pi\right]$ and $f\in[0,1]$. (This is useful for distributing points on $S^3$, the hypersphere in 4D, because the Jacobian of the transformation from uniform rectilinear coordinates to spherical coordinates creates a term containing $\sin^{2}x$.) Following the method of Inverse transform sampling , we need to integrate and invert the PDF; applying the standard uniform distribution to the inverted CDF gives the desired distribution. The CDF is easy enough to calculate: $$F(x) = \frac{x}{2}-\frac{1}{4}\sin2x+C$$ Where $C$ is determined by the bounds. However, I don't see how to invert this function (regardless of $C$). Here's a numerical simulation of the inverse after $F(x)$ is normalized to [0,1] by multiplying by $\frac{2}{\pi}$: Although the CDF appears to be 1-to-1 and onto in the relevant domain, I don't see how to get a true analytical inverse because the combination of $arcsin$ and the linear term just doesn't work out. I tried a Taylor's series approximation and got $\sqrt[3]{3U}$, but that only works for small $U$, say $U\in\left[0,0.5\right]$, and it definitely doesn't cover the full range of $\sin^{2}x$. The question is how do you transform the standard uniform distribution to $\sin^{2}x$, as stated above. Uniform random number generators are readily available, but I want to draw samples from the $\sin^{2}x$ distribution.",,"['statistics', 'probability-distributions']"
81,Recommendation request: Reasoning behind statistics,Recommendation request: Reasoning behind statistics,,"It seems, to me at least, that most Statistics textbooks focus on the Statistical methods and techniques, or on the mathematics behind them. Would you recommend me some textbooks (or any online source) that discuss the ""logical"" reasoning behind the techniques? Thanks in advance!","It seems, to me at least, that most Statistics textbooks focus on the Statistical methods and techniques, or on the mathematics behind them. Would you recommend me some textbooks (or any online source) that discuss the ""logical"" reasoning behind the techniques? Thanks in advance!",,"['statistics', 'philosophy']"
82,Finding the pdf of a function,Finding the pdf of a function,,"Let X have pdf $f_X(x)=\frac{2}{9}(x+1)$, $-1$ less than or equal to $x$ less than or equal to $2$. I have to find the pdf of $Y=X^2$. I notice that I am not able to use that familiar theorem of differentiation and inverses. What should I do?","Let X have pdf $f_X(x)=\frac{2}{9}(x+1)$, $-1$ less than or equal to $x$ less than or equal to $2$. I have to find the pdf of $Y=X^2$. I notice that I am not able to use that familiar theorem of differentiation and inverses. What should I do?",,['probability']
83,Estimating word frequency for a set of texts,Estimating word frequency for a set of texts,,"I have 37 files of different sizes, containing a total of 7 million words. I want to estimate the frequency of each word in the corpus. Let's say I find the word ""Cranberry"" only twice in the corpus, then it would be naive to state the frequency of this word as 2/7000000. This is because the odds are very high that I could, for another corpus establish a frequency of 0/7000000 or 4/7000000 for the very same word. So in other words, the statistical error for estimating the frequency increases as the frequency of a word decreases. Is there a way of estimating how great the statistical error is? For example, lets say I was to say that ""the"" occurs 405139 times in the corpus, how accurate is it to say that the frequency of the word ""the"" in the English language is 405139/7000000? Does it help, if I present the frequency of ""the"" in each of my 37 files? 219/3293 992/15072 321/4792 358/6736 147/1819 388/4047 312/6567 349/7182 735/11233 5789/118655 18359/300927 236/2677 91/1135 1664/26761 585/9709 477/10414 248/3253 238/3857 41634/670576 1703/34261 246/3080 69971/1005119 195/5372 1362/25915 896/22999 1186/25515 109/3360 72/1832 203/4261 78673/1363843 67210/1229371 69277/1330254 93/2542 1888/66501 1451/27679 32382/514278 5080/87831 If I only have 2 cranberries in my corpus, how large a corpus would I need to get a reasonably accurate indication as to its frequency?","I have 37 files of different sizes, containing a total of 7 million words. I want to estimate the frequency of each word in the corpus. Let's say I find the word ""Cranberry"" only twice in the corpus, then it would be naive to state the frequency of this word as 2/7000000. This is because the odds are very high that I could, for another corpus establish a frequency of 0/7000000 or 4/7000000 for the very same word. So in other words, the statistical error for estimating the frequency increases as the frequency of a word decreases. Is there a way of estimating how great the statistical error is? For example, lets say I was to say that ""the"" occurs 405139 times in the corpus, how accurate is it to say that the frequency of the word ""the"" in the English language is 405139/7000000? Does it help, if I present the frequency of ""the"" in each of my 37 files? 219/3293 992/15072 321/4792 358/6736 147/1819 388/4047 312/6567 349/7182 735/11233 5789/118655 18359/300927 236/2677 91/1135 1664/26761 585/9709 477/10414 248/3253 238/3857 41634/670576 1703/34261 246/3080 69971/1005119 195/5372 1362/25915 896/22999 1186/25515 109/3360 72/1832 203/4261 78673/1363843 67210/1229371 69277/1330254 93/2542 1888/66501 1451/27679 32382/514278 5080/87831 If I only have 2 cranberries in my corpus, how large a corpus would I need to get a reasonably accurate indication as to its frequency?",,['statistics']
84,Standard deviation of sample: why $n-1$ instead of $n$?,Standard deviation of sample: why  instead of ?,n-1 n,"I'm trying to undertand why we use $n-1$ instead of $n$ while calculating the standard deviation of a sample. This site says that it is because $\sum{x_i-\overline{x}}=0$, and $\overline{x}$ is aready determined from before. Hence, the sample has $n-1$ degrees of freedom, while the population has $n$. My question is what does degrees of freedom have to do with calculating the standard deviation? Is the definition of standard deviation $\sqrt{\frac{\sum{(x_i-\overline{x}})^2}{\text{ degrees of freedom}}}$ instead of $\sqrt{\frac{\sum{(x_i-\overline{x}})^2}{n}}$?","I'm trying to undertand why we use $n-1$ instead of $n$ while calculating the standard deviation of a sample. This site says that it is because $\sum{x_i-\overline{x}}=0$, and $\overline{x}$ is aready determined from before. Hence, the sample has $n-1$ degrees of freedom, while the population has $n$. My question is what does degrees of freedom have to do with calculating the standard deviation? Is the definition of standard deviation $\sqrt{\frac{\sum{(x_i-\overline{x}})^2}{\text{ degrees of freedom}}}$ instead of $\sqrt{\frac{\sum{(x_i-\overline{x}})^2}{n}}$?",,[]
85,"Prove or disprove: If $E[Y]>E[X]$, then $F_X(z)>F_Y(z)$ for some $z$.","Prove or disprove: If , then  for some .",E[Y]>E[X] F_X(z)>F_Y(z) z,"Prove or disprove: (a) If $F_X(z)>F_Y(z)$ for all $z$, then $E[Y]>E[X]$. By definition $E[X] = \displaystyle\int_{0}^{\infty} [1-F_X(z)]dz + \displaystyle\int_{-\infty}^{0} [F_X(z)]dz$ Given that $F_X(z)>F_Y(z)$ then $$\displaystyle\int_{0}^{\infty} [-F_X(z)]dz < \displaystyle\int_{0}^{\infty} [-F_Y(z)]dz$$ $$\displaystyle\int_{0}^{\infty} [1-F_X(z)]dz < \displaystyle\int_{0}^{\infty} [1-F_Y(z)]dz$$ $$\displaystyle\int_{0}^{\infty} [1-F_X(z)]dz - \displaystyle\int_{-\infty}^{0} [F_X(z)]dz < \displaystyle\int_{0}^{\infty} [1-F_Y(z)]dz - \displaystyle\int_{-\infty}^{0} [F_Y(z)]dz$$ $$E[X]<E[Y]$$ (b) If $E[Y]>E[X]$, then $F_X(z)>F_Y(z)$ for all $z$. A simple counterexample $$P[Y=1]=0.5 , P[Y=4]=0.5$ then $E[Y] = 3$$ $$P[X=2]=0.98 , P[X=6]=0.02 $ then $E[X] = 2.08$$ $$E[Y] > E[X]$$ but $$F_X(4)<F_Y(4)$$ (c) If $E[Y]>E[X]$,then $F_X(z)>F_Y(z)$ for some $z$. Can someone help me with this? Intuitively, I think that it's true (d) If $F_X(z)=F_Y(z)$ for all $z$, then $P[X=Y]=1$. If $F_X(z)=F_Y(z)$ then $X$ and $Y$ have the same distribution by counterexample $$P[Y=1]=0.5 , P[Y=2]=0.5$$ $$P[X=1]=0.5 , P[X=2]=0.5$$ but $$P[X=Y]= (1/2)$$ (e) If $F_X(z)>F_Y(z)$ for all $z$, then $P[X<Y]>0$. $$P[X\leq{}z] > P[Y\leq{}z]$$ $$P[X<z] > P[Y<z]$$ $$P[X-Y<z] > P[Y-Y<z]$$ $$P[X-Y<{}0] > P[0<0]$$ $$P[X-Y<0] > 0$$ $$P[X<Y] > 0$$ (f) If $Y=X+1$, then $F_X(z)=F_Y(z+1)$ for all $z$ $$F_X(z) = P[X\leq{}z] = P[X+1\leq{}z+1] = P[Y \leq{}z+1] = F_Y(z+1)$$ Any suggestion? Thank so much, have an excellent day","Prove or disprove: (a) If $F_X(z)>F_Y(z)$ for all $z$, then $E[Y]>E[X]$. By definition $E[X] = \displaystyle\int_{0}^{\infty} [1-F_X(z)]dz + \displaystyle\int_{-\infty}^{0} [F_X(z)]dz$ Given that $F_X(z)>F_Y(z)$ then $$\displaystyle\int_{0}^{\infty} [-F_X(z)]dz < \displaystyle\int_{0}^{\infty} [-F_Y(z)]dz$$ $$\displaystyle\int_{0}^{\infty} [1-F_X(z)]dz < \displaystyle\int_{0}^{\infty} [1-F_Y(z)]dz$$ $$\displaystyle\int_{0}^{\infty} [1-F_X(z)]dz - \displaystyle\int_{-\infty}^{0} [F_X(z)]dz < \displaystyle\int_{0}^{\infty} [1-F_Y(z)]dz - \displaystyle\int_{-\infty}^{0} [F_Y(z)]dz$$ $$E[X]<E[Y]$$ (b) If $E[Y]>E[X]$, then $F_X(z)>F_Y(z)$ for all $z$. A simple counterexample $$P[Y=1]=0.5 , P[Y=4]=0.5$ then $E[Y] = 3$$ $$P[X=2]=0.98 , P[X=6]=0.02 $ then $E[X] = 2.08$$ $$E[Y] > E[X]$$ but $$F_X(4)<F_Y(4)$$ (c) If $E[Y]>E[X]$,then $F_X(z)>F_Y(z)$ for some $z$. Can someone help me with this? Intuitively, I think that it's true (d) If $F_X(z)=F_Y(z)$ for all $z$, then $P[X=Y]=1$. If $F_X(z)=F_Y(z)$ then $X$ and $Y$ have the same distribution by counterexample $$P[Y=1]=0.5 , P[Y=2]=0.5$$ $$P[X=1]=0.5 , P[X=2]=0.5$$ but $$P[X=Y]= (1/2)$$ (e) If $F_X(z)>F_Y(z)$ for all $z$, then $P[X<Y]>0$. $$P[X\leq{}z] > P[Y\leq{}z]$$ $$P[X<z] > P[Y<z]$$ $$P[X-Y<z] > P[Y-Y<z]$$ $$P[X-Y<{}0] > P[0<0]$$ $$P[X-Y<0] > 0$$ $$P[X<Y] > 0$$ (f) If $Y=X+1$, then $F_X(z)=F_Y(z+1)$ for all $z$ $$F_X(z) = P[X\leq{}z] = P[X+1\leq{}z+1] = P[Y \leq{}z+1] = F_Y(z+1)$$ Any suggestion? Thank so much, have an excellent day",,"['probability', 'statistics']"
86,Awkward question - pairing average for two different types of elements where ratio is 1:1,Awkward question - pairing average for two different types of elements where ratio is 1:1,,"Not sure if this is adequate for math.stackexchange but please let me know and I will delete it. I've tried to make it more abstract, but then it would raise more questions than give answers. I've come across a statistic where it says that men have slept on average with 13 women in their lifetime, and women with 7 men. Now, I know that this is not reliable information and so on but let's make the following presumptions: Man/Woman ratio is 1:1 Only type of coitus is in a heterosexual pair People don't lie in this sort of studies When two people pair up, it counts for both Is there any possibility for this? I just can't wrap my head around it, and would appreciate any sort of suggestions.","Not sure if this is adequate for math.stackexchange but please let me know and I will delete it. I've tried to make it more abstract, but then it would raise more questions than give answers. I've come across a statistic where it says that men have slept on average with 13 women in their lifetime, and women with 7 men. Now, I know that this is not reliable information and so on but let's make the following presumptions: Man/Woman ratio is 1:1 Only type of coitus is in a heterosexual pair People don't lie in this sort of studies When two people pair up, it counts for both Is there any possibility for this? I just can't wrap my head around it, and would appreciate any sort of suggestions.",,['statistics']
87,Question on sufficient statistics,Question on sufficient statistics,,"Let ${\bf X}=(X_1,\dots,X_n)$ be a random sample from the pdf $$f(x\mid\mu,\sigma)=\frac{1}{\sigma}e^{\frac{-(x-\mu)}{\sigma}}\;\; ,\;\; \mu<x<\infty\;,\;0<\sigma<\infty.$$ Find a two-dimensional sufficient statistic for $(\mu,\sigma)$. The Factorization Theorem says that I should be able to find $T_1({\bf x})$ and $T_2({\bf x})$ such that we have the decomposition $$\hat{f}({\bf x}\mid\mu,\sigma)=g(T_1({\bf x}),T_2({\bf x})\mid\mu,\sigma)\cdot h({\bf x}).$$ Where $\hat{f}$ is the joint distribution for $\bf X$. I just want to make sure I'm not missing something obvious, since by independence I can write $\hat{f}$ as $$\hat{f}({\bf x}\mid\mu,\sigma)=\hat{f}(x_1,\dots,x_n\mid\mu,\sigma)=\left(\frac{e^{\mu/\sigma}}{\sigma}\right)^ne^{(-1/\sigma)\sum_{k=1}^nx_k}\cdot 1.$$ It appears that just setting $T_1({\bf x})=\sum_{k=1}^nx_k$ is a statistic sufficient to determine both $\mu$ and $\sigma$.  Which worries me since why would the book ask specifically for a two dimensional statistic if a one dimensional one was all that was necessary to obtain sufficiency with regard to both parameters.","Let ${\bf X}=(X_1,\dots,X_n)$ be a random sample from the pdf $$f(x\mid\mu,\sigma)=\frac{1}{\sigma}e^{\frac{-(x-\mu)}{\sigma}}\;\; ,\;\; \mu<x<\infty\;,\;0<\sigma<\infty.$$ Find a two-dimensional sufficient statistic for $(\mu,\sigma)$. The Factorization Theorem says that I should be able to find $T_1({\bf x})$ and $T_2({\bf x})$ such that we have the decomposition $$\hat{f}({\bf x}\mid\mu,\sigma)=g(T_1({\bf x}),T_2({\bf x})\mid\mu,\sigma)\cdot h({\bf x}).$$ Where $\hat{f}$ is the joint distribution for $\bf X$. I just want to make sure I'm not missing something obvious, since by independence I can write $\hat{f}$ as $$\hat{f}({\bf x}\mid\mu,\sigma)=\hat{f}(x_1,\dots,x_n\mid\mu,\sigma)=\left(\frac{e^{\mu/\sigma}}{\sigma}\right)^ne^{(-1/\sigma)\sum_{k=1}^nx_k}\cdot 1.$$ It appears that just setting $T_1({\bf x})=\sum_{k=1}^nx_k$ is a statistic sufficient to determine both $\mu$ and $\sigma$.  Which worries me since why would the book ask specifically for a two dimensional statistic if a one dimensional one was all that was necessary to obtain sufficiency with regard to both parameters.",,"['probability', 'statistics']"
88,Methods for determining thresholds for future use based on historical data (in R or Excel),Methods for determining thresholds for future use based on historical data (in R or Excel),,"Let's say I have some data about the amounts of reports to Help Desk (about technical problems) which were monitored and registered every five minutes for the whole day (even during the night). Is there any way to efficiently calculate the ""alarming"" thresholds: top - above this means there are too many reports in the system! Time to get to work! (most important). bottom - lower than this means there are too few of them (report system got broken?) The problems I face: System is fairly new, so at the start of the registering process there were extreme outliers. I want to have thresholds for a more stable system in the future. People obviously don't work during the night, so there's a lot of observations (let's call them ""moments"" - those ""five minutes"") where the number of reports stays the same.","Let's say I have some data about the amounts of reports to Help Desk (about technical problems) which were monitored and registered every five minutes for the whole day (even during the night). Is there any way to efficiently calculate the ""alarming"" thresholds: top - above this means there are too many reports in the system! Time to get to work! (most important). bottom - lower than this means there are too few of them (report system got broken?) The problems I face: System is fairly new, so at the start of the registering process there were extreme outliers. I want to have thresholds for a more stable system in the future. People obviously don't work during the night, so there's a lot of observations (let's call them ""moments"" - those ""five minutes"") where the number of reports stays the same.",,['statistics']
89,Variable substitution in probability,Variable substitution in probability,,"In modeling the number of claims filed by an individual under an automobile policy during a three-year period, an actuary makes the simplifying assumption that for all integers $n \ge 0$, $p_{n+1}=\frac{1}{5} p_n$, where $p_n$ represents the probability that the policyholder files $n$ claims during the period.  Under this assumption, what is the probability that a policyholder files more than one claim during the period? My first step was the convert the probability function from future values by variable substitution.  So I assign $k=n+1 \implies n=k-1$: $\therefore p_k=\frac{1}{5} p_{k-1} \text{ for } k \ge 1\tag{1}$ Now we can calculate the probabilites for some k values: $k=0:  p_0$ $k=1:  p_1=\frac{1}{5}p_0$ $k=2:  p_2=\frac{1}{5}p_1=(\frac{1}{5})^2 p_0$ $\vdots$ $k=k:  p_k=(\frac{1}{5})^k p_0$ Since the probabilities sum to 1: We can solve for $p_0:  1 = p_0 ( 1 + (\frac{1}{5}) + (\frac{1}{5})^2 + \cdots )=\cfrac{p_0}{1-1/5}$ $\implies p_0=0.8$ Now this is where I get confused.  The question asks us to calculate $\Pr[n>1]$, so: $\Pr[n>1]=1-\Pr[n \le 1]\tag{2}$ Now when calculating equation(2), I thought  I would have to transform $n$ to $k$, so Equation (2) becomes: $\Pr[n>1]=1-\Pr[k \le 2]=1-\Pr[k=0]-\Pr[k=1]-\Pr[k=2]\tag{3}$ But this is not correct.  The solution is just replacing exactly n with k with no substitution.  In other words, the solution is: $\Pr[k>1]=1-\Pr[k \le 1]\tag{4}=1-0.8-0.16$ Can someone please explain why this the case?  Why don't I have to transform n to k when solving this question?  If $n$ is the number of claims during the period, then k is the number of claims per period PLUS ONE.  Thanks in advance!  Have a great weekend!","In modeling the number of claims filed by an individual under an automobile policy during a three-year period, an actuary makes the simplifying assumption that for all integers $n \ge 0$, $p_{n+1}=\frac{1}{5} p_n$, where $p_n$ represents the probability that the policyholder files $n$ claims during the period.  Under this assumption, what is the probability that a policyholder files more than one claim during the period? My first step was the convert the probability function from future values by variable substitution.  So I assign $k=n+1 \implies n=k-1$: $\therefore p_k=\frac{1}{5} p_{k-1} \text{ for } k \ge 1\tag{1}$ Now we can calculate the probabilites for some k values: $k=0:  p_0$ $k=1:  p_1=\frac{1}{5}p_0$ $k=2:  p_2=\frac{1}{5}p_1=(\frac{1}{5})^2 p_0$ $\vdots$ $k=k:  p_k=(\frac{1}{5})^k p_0$ Since the probabilities sum to 1: We can solve for $p_0:  1 = p_0 ( 1 + (\frac{1}{5}) + (\frac{1}{5})^2 + \cdots )=\cfrac{p_0}{1-1/5}$ $\implies p_0=0.8$ Now this is where I get confused.  The question asks us to calculate $\Pr[n>1]$, so: $\Pr[n>1]=1-\Pr[n \le 1]\tag{2}$ Now when calculating equation(2), I thought  I would have to transform $n$ to $k$, so Equation (2) becomes: $\Pr[n>1]=1-\Pr[k \le 2]=1-\Pr[k=0]-\Pr[k=1]-\Pr[k=2]\tag{3}$ But this is not correct.  The solution is just replacing exactly n with k with no substitution.  In other words, the solution is: $\Pr[k>1]=1-\Pr[k \le 1]\tag{4}=1-0.8-0.16$ Can someone please explain why this the case?  Why don't I have to transform n to k when solving this question?  If $n$ is the number of claims during the period, then k is the number of claims per period PLUS ONE.  Thanks in advance!  Have a great weekend!",,"['probability', 'statistics', 'actuarial-science']"
90,First step in statistics: something-like-a-mode for a sequence where each value is different from another,First step in statistics: something-like-a-mode for a sequence where each value is different from another,,"Sorry but I'm not a statics expert at all, but I'm following some on line course and it is fascinating me. I just found the existence of the mode: The mode is the value that appears most often in a set of data. (Wikipedia) What about a sequence where the values are different one from each other? Like l = [1, 1.2, 1.3, 0.9, 12, 5] I'd like to take out from this sequence a value that is not the average (that is 3.5) because it is not explicative enough so I thought to discretize (like [1, 1, 1, 1, 12, 5] ) them and then take the mode (that would be 1) but I'm sure that there is a better way.","Sorry but I'm not a statics expert at all, but I'm following some on line course and it is fascinating me. I just found the existence of the mode: The mode is the value that appears most often in a set of data. (Wikipedia) What about a sequence where the values are different one from each other? Like l = [1, 1.2, 1.3, 0.9, 12, 5] I'd like to take out from this sequence a value that is not the average (that is 3.5) because it is not explicative enough so I thought to discretize (like [1, 1, 1, 1, 12, 5] ) them and then take the mode (that would be 1) but I'm sure that there is a better way.",,['statistics']
91,Simple Linear Regression Model,Simple Linear Regression Model,,"I'm having trouble on this question and was wondering if you guys could hep me out. I have the linear regression model $Y_i=\alpha+\beta x_i +R_i$, with data $y=(y_1, \ldots,y_n) $ and $x = (x_1, \ldots, x_n)$ from the model. I know that the correlation coefficient between $y$ and $x$ is defined as $r(y,x)=s_{xy}/(\sqrt{s_{yy}}\sqrt{s_{xx}})$. Let $\hat{r}=(\hat{r_1}, \ldots , \hat{r_n})$ where $r_i=y_i-\hat{\alpha} - \hat{\beta} x_i$, $i=1,\ldots,n$ be the residuals from fitting the simple regression model. I'm can't seem to show that $r(\hat{r}, x) = 0$.","I'm having trouble on this question and was wondering if you guys could hep me out. I have the linear regression model $Y_i=\alpha+\beta x_i +R_i$, with data $y=(y_1, \ldots,y_n) $ and $x = (x_1, \ldots, x_n)$ from the model. I know that the correlation coefficient between $y$ and $x$ is defined as $r(y,x)=s_{xy}/(\sqrt{s_{yy}}\sqrt{s_{xx}})$. Let $\hat{r}=(\hat{r_1}, \ldots , \hat{r_n})$ where $r_i=y_i-\hat{\alpha} - \hat{\beta} x_i$, $i=1,\ldots,n$ be the residuals from fitting the simple regression model. I'm can't seem to show that $r(\hat{r}, x) = 0$.",,"['probability', 'statistics']"
92,Confused by Kolmogorov's Strong law and Borel-Cantelli lemma,Confused by Kolmogorov's Strong law and Borel-Cantelli lemma,,"Notation: Let $X_n$ be i.i.d. random variables with mean $\mathrm{E}[X_n]=\mu$ and variance $\mathrm{E}[(X_n-\mu)^2]=\sigma^2$. Denote the sample average as $\bar{x}_m = \frac{1}{m}\sum_{n=1}^m X_n$. Accoring to (my interpretation of) Kolmogorov's SLLN since the series $\sum_{n=1}^\infty\sigma^2/n^2$ converges then $\mathrm{P}\{\bar{x}_n-\mu>\epsilon, \mathrm{i.o}\} = 0, \forall \epsilon>0$. Question: Does this mean that (due to Borel-Cantelli lemma) the series $\sum_{n=1}^\infty \mathrm{P}\{\bar{x}_n-\mu>\epsilon\}, \forall \epsilon>0$ converges?","Notation: Let $X_n$ be i.i.d. random variables with mean $\mathrm{E}[X_n]=\mu$ and variance $\mathrm{E}[(X_n-\mu)^2]=\sigma^2$. Denote the sample average as $\bar{x}_m = \frac{1}{m}\sum_{n=1}^m X_n$. Accoring to (my interpretation of) Kolmogorov's SLLN since the series $\sum_{n=1}^\infty\sigma^2/n^2$ converges then $\mathrm{P}\{\bar{x}_n-\mu>\epsilon, \mathrm{i.o}\} = 0, \forall \epsilon>0$. Question: Does this mean that (due to Borel-Cantelli lemma) the series $\sum_{n=1}^\infty \mathrm{P}\{\bar{x}_n-\mu>\epsilon\}, \forall \epsilon>0$ converges?",,"['probability', 'statistics']"
93,Conditions for positive dependence,Conditions for positive dependence,,"Consider two random variables $X$ and $Y$ with joint distribution $F_{X,Y}$ and strictly positive density function $f_{X,Y}$. Additionally, let  $x^*$ be the value of $x$ that solves: $$ \Pr[Y\leq y\mid X=x^*]=1-k $$ for some constant $k \in [0,1]$. I would like to know the minimum conditions I should impose on $F$ in order to ensure that $$\frac{\partial x^*}{\partial k}\geq 0$$ It seems quite intuitive that $COV[X,Y]>0$ would do the job, but this is not the case. I found in the literature that Affiliation* or Decreasing Inverse Hazard Rate** are sufficient, but I would like to know if there are weaker conditions. *The pdf $f$ is afï¬liated if $x \leq x'$ and $y \leq y'$ imply $f(x, y')f(x',y)\leq f(x', y')f(x,y)$ ** $Y$ is Inverse Hazard Rate Decreasing in $X$ if $\frac{F(y|x)}{f(y|x)}$ is non-increasing in $x$ for all $y$ (where $f(y|x)$ denotes the pdf of $Y$ conditional on $X=x$). NOTE: This question had also been posted on the Stats Stackexchange site (link here ), but was subsequently deleted as it received no comments or answers","Consider two random variables $X$ and $Y$ with joint distribution $F_{X,Y}$ and strictly positive density function $f_{X,Y}$. Additionally, let  $x^*$ be the value of $x$ that solves: $$ \Pr[Y\leq y\mid X=x^*]=1-k $$ for some constant $k \in [0,1]$. I would like to know the minimum conditions I should impose on $F$ in order to ensure that $$\frac{\partial x^*}{\partial k}\geq 0$$ It seems quite intuitive that $COV[X,Y]>0$ would do the job, but this is not the case. I found in the literature that Affiliation* or Decreasing Inverse Hazard Rate** are sufficient, but I would like to know if there are weaker conditions. *The pdf $f$ is afï¬liated if $x \leq x'$ and $y \leq y'$ imply $f(x, y')f(x',y)\leq f(x', y')f(x,y)$ ** $Y$ is Inverse Hazard Rate Decreasing in $X$ if $\frac{F(y|x)}{f(y|x)}$ is non-increasing in $x$ for all $y$ (where $f(y|x)$ denotes the pdf of $Y$ conditional on $X=x$). NOTE: This question had also been posted on the Stats Stackexchange site (link here ), but was subsequently deleted as it received no comments or answers",,"['statistics', 'probability-theory', 'probability-distributions']"
94,Cholesky Decomposition - Correlation,Cholesky Decomposition - Correlation,,"I understand, How Cholesky decomposition can be used to generate correlated random numbers. But unable to understand, why and how does it allow for correlation?","I understand, How Cholesky decomposition can be used to generate correlated random numbers. But unable to understand, why and how does it allow for correlation?",,"['linear-algebra', 'statistics', 'correlation', 'cholesky-decomposition']"
95,Statistics percentage problem!,Statistics percentage problem!,,"the english major at a university revealed the following:  10% failed math, 20% failed biology, 5% failed both math and biology. find prob: a) failed math, given he passed bio b) passed bio, given passed math c)passed math, known that he failed bio c)pass both courses\ p(a|b)= p(aU(upsidedown)b) / p(b) i would use that but i dont know the prob of a(failed math) and b (passing bio) it only gives the failures but even then i could use the a^c but still i wouldnt know the prob of thoe event together","the english major at a university revealed the following:  10% failed math, 20% failed biology, 5% failed both math and biology. find prob: a) failed math, given he passed bio b) passed bio, given passed math c)passed math, known that he failed bio c)pass both courses\ p(a|b)= p(aU(upsidedown)b) / p(b) i would use that but i dont know the prob of a(failed math) and b (passing bio) it only gives the failures but even then i could use the a^c but still i wouldnt know the prob of thoe event together",,['statistics']
96,"Derive which values $P(A\cap B \cap C)$ can take, using only the axioms of probability","Derive which values  can take, using only the axioms of probability",P(A\cap B \cap C),"For three events $A, B, C$ we know that $P(A\cap B) = 0.4, P(A \cap C)=0.5 \text{ and } P(B\cap C)=0.6$ (a) Derive which values $P(A\cap B \cap C)$ can take, with a clear explanation. Show in every step which axiom you use. You can only make use of the following axioms: $P(A)\geq 0$ $P(S)=1$ $P(A_1\cup A_2 \cup \dots)=\sum\limits_{i=1}^\infty A_i, \text{ } \forall i\neq j$ (b) Let $D=(A\cap B^c \cap C^c) \cup (A^c\cap B \cap C^c) \cup (A^c\cap B^c \cap C)$. Which are the values that $P(D)$ can take? Give a clear explanation of your derivations. What I attempted so far (a) I came this far: $A\cap B=(A\cap B \cap C) \cup (A\cap B \cap C^c)$ (these sets are disjoint). Hence $P(A\cap B \cap C) \leq P(A\cap B) = 0.4$, by axioms 3 and 2, since $P(A\cap B \cap C^c)\geq 0$. By making a Venn-diagram I figured out that the lower boundary should probably be $0.25$, as this is $0.4+0.5+0.6-1=0.5$ (which is everything that should be in some intersection because the total probability cannot be greater than one) devided by $2$ (because the $0.5$ can be split over two intersections). However, I am not sure how to show this using the axioms provided. Could anyone please help? (b) I think I will need the result from a. I am however not at all sure yet what I should do to solve this question. Any help would be appreciated.","For three events $A, B, C$ we know that $P(A\cap B) = 0.4, P(A \cap C)=0.5 \text{ and } P(B\cap C)=0.6$ (a) Derive which values $P(A\cap B \cap C)$ can take, with a clear explanation. Show in every step which axiom you use. You can only make use of the following axioms: $P(A)\geq 0$ $P(S)=1$ $P(A_1\cup A_2 \cup \dots)=\sum\limits_{i=1}^\infty A_i, \text{ } \forall i\neq j$ (b) Let $D=(A\cap B^c \cap C^c) \cup (A^c\cap B \cap C^c) \cup (A^c\cap B^c \cap C)$. Which are the values that $P(D)$ can take? Give a clear explanation of your derivations. What I attempted so far (a) I came this far: $A\cap B=(A\cap B \cap C) \cup (A\cap B \cap C^c)$ (these sets are disjoint). Hence $P(A\cap B \cap C) \leq P(A\cap B) = 0.4$, by axioms 3 and 2, since $P(A\cap B \cap C^c)\geq 0$. By making a Venn-diagram I figured out that the lower boundary should probably be $0.25$, as this is $0.4+0.5+0.6-1=0.5$ (which is everything that should be in some intersection because the total probability cannot be greater than one) devided by $2$ (because the $0.5$ can be split over two intersections). However, I am not sure how to show this using the axioms provided. Could anyone please help? (b) I think I will need the result from a. I am however not at all sure yet what I should do to solve this question. Any help would be appreciated.",,"['probability', 'statistics']"
97,Method of Moments and Maximum Likelihood estimators?,Method of Moments and Maximum Likelihood estimators?,,"The random variables $X_1,...X_n$ are independent draws from continuous unifirm distribution with support $[0,\theta]$. Derive a method of moments and maximum likelihood estimators of $\theta$. Your research assistant has constructed new random variables $Y_i$ such that: $$Y_i = \begin{cases} 0,  & \text{if} \, X_i \le k \\ 1, & \text{if} \, X_i \gt k\\ \end{cases}$$ where $k$ is a constant chosen by RA and known to you. What are the Maximum Likelihood and Method of Moments Estimators in this case? Assume $(k \lt \theta)$. After trying, these are the answers which I am getting: first part: By method of moments:  $\hat{\theta} = 2\bar{X}$ By MLE:   $\hat{\theta} = Max\{X_i\}$ Second Part: By Method of Moments: $\hat{\theta} = \frac{k}{1-\overline{Y}}$ By MLE: $\hat{\theta} =\frac{k}{1-\overline{Y}}$ (same as above) Please tell me which answer is wrong so that I can redo that very part. PS: This question was asked in an exam a couple of years back, so I dont know the correct answers.","The random variables $X_1,...X_n$ are independent draws from continuous unifirm distribution with support $[0,\theta]$. Derive a method of moments and maximum likelihood estimators of $\theta$. Your research assistant has constructed new random variables $Y_i$ such that: $$Y_i = \begin{cases} 0,  & \text{if} \, X_i \le k \\ 1, & \text{if} \, X_i \gt k\\ \end{cases}$$ where $k$ is a constant chosen by RA and known to you. What are the Maximum Likelihood and Method of Moments Estimators in this case? Assume $(k \lt \theta)$. After trying, these are the answers which I am getting: first part: By method of moments:  $\hat{\theta} = 2\bar{X}$ By MLE:   $\hat{\theta} = Max\{X_i\}$ Second Part: By Method of Moments: $\hat{\theta} = \frac{k}{1-\overline{Y}}$ By MLE: $\hat{\theta} =\frac{k}{1-\overline{Y}}$ (same as above) Please tell me which answer is wrong so that I can redo that very part. PS: This question was asked in an exam a couple of years back, so I dont know the correct answers.",,"['probability', 'statistics', 'estimation']"
98,"$X \sim U(0, 2)$, $Y \sim U(0, 3)$ independently: $P(Y \le X^2)$",",  independently:","X \sim U(0, 2) Y \sim U(0, 3) P(Y \le X^2)","Suppose $X \sim U(0,2)$ and $Y \sim U(0,3)$ independently. Then $0 < X <2$ and $0 < Y < 3$. Evaluate $P(Y \leq X^2)$. By integrating over the region, I get $P = \dfrac{4}{9}$, but the solutions indicate that I should be getting $P = 0.4226$.","Suppose $X \sim U(0,2)$ and $Y \sim U(0,3)$ independently. Then $0 < X <2$ and $0 < Y < 3$. Evaluate $P(Y \leq X^2)$. By integrating over the region, I get $P = \dfrac{4}{9}$, but the solutions indicate that I should be getting $P = 0.4226$.",,"['statistics', 'integration']"
99,"Neyman-Pearson, solving for $k$","Neyman-Pearson, solving for",k,"I've got a one major problem using the Neyman-Pearson lemma. We're testing hypotheses $H_0: \theta \le \theta_0$ vs. $H_1: \theta > \theta_0$. Our $$f(x,\theta) = \frac{1}{\theta}e^{-\frac{x}{\theta}}$$ We use the Neyman-Pearson lemma and get the following expression for the critical value of the test: $$\phi(X) = \mathbf{1}_{\{f(x,\theta_1) > k f(x,\theta_0)\}}(x) + \gamma \mathbf{1}_{\{f(x,\theta_1) = k f(x,\theta_0)\}}(x)$$ Now I can get some expression in terms of $\frac{1}{n} \sum_{i=1}^n X_i > g(k)$ for the test, but I need a value for $k$ from the expression for $\phi(X)$. How would I compute $$\mathbb{E}[\phi(X) |H_0] =\alpha$$ so that I can solve for $k$?","I've got a one major problem using the Neyman-Pearson lemma. We're testing hypotheses $H_0: \theta \le \theta_0$ vs. $H_1: \theta > \theta_0$. Our $$f(x,\theta) = \frac{1}{\theta}e^{-\frac{x}{\theta}}$$ We use the Neyman-Pearson lemma and get the following expression for the critical value of the test: $$\phi(X) = \mathbf{1}_{\{f(x,\theta_1) > k f(x,\theta_0)\}}(x) + \gamma \mathbf{1}_{\{f(x,\theta_1) = k f(x,\theta_0)\}}(x)$$ Now I can get some expression in terms of $\frac{1}{n} \sum_{i=1}^n X_i > g(k)$ for the test, but I need a value for $k$ from the expression for $\phi(X)$. How would I compute $$\mathbb{E}[\phi(X) |H_0] =\alpha$$ so that I can solve for $k$?",,"['probability', 'statistics']"
