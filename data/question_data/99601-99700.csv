,title_raw,title_text,title_latex,body_raw,body_text,body_latex,tags
0,Show that $Im(f(re^{i\theta})) = r \sin(\theta)(1+O(r))$ for small $r$,Show that  for small,Im(f(re^{i\theta})) = r \sin(\theta)(1+O(r)) r,"Suppose f is a analytic function such that $f(0) = 1 = f'(0)$ and $/f^{(k)}(0) \in \mathbb{R} \; \forall k$ Show that $v(r,\theta) = Im(f(re^{i\theta})) = r \sin(\theta)(1+O(r))$ for small $r$ . I need to show this, but I don't believe it's true, here's why: I know that $f(re^{i\theta}) = \sum a_kr^k(\cos(k\theta)+i\sin(k\theta)) \implies v(r,\theta) = \sum a_kr^k \sin(k\theta)$ where $a_k = \frac{f^{(k)}(0)}{k!}$ . Now, $v(r,\theta) = 1+r\sin(\theta)+\sum_{k\geq 2}a_kr^ksin(k\theta) = r\sin(\theta)\left(1+\frac{1}{r\sin(\theta)}+\sum_{k\geq 2}a_kr^{k-2}\frac{sin(k\theta)}{sin(\theta)}\right)$ Now, $\frac{1}{r\sin(\theta)}$ is not limited by $Cr$ for any constant $C$ for small r, so, how can $\frac{1}{r\sin(\theta)}+\sum_{k\geq 2}a_kr^{k-2}\frac{sin(k\theta)}{sin(\theta)} = O(r)$ ? I'm sorry, I'm not used to the big Oh notation, regardless I don't know how that statement could be true.","Suppose f is a analytic function such that and Show that for small . I need to show this, but I don't believe it's true, here's why: I know that where . Now, Now, is not limited by for any constant for small r, so, how can ? I'm sorry, I'm not used to the big Oh notation, regardless I don't know how that statement could be true.","f(0) = 1 = f'(0) /f^{(k)}(0) \in \mathbb{R} \; \forall k v(r,\theta) = Im(f(re^{i\theta})) = r \sin(\theta)(1+O(r)) r f(re^{i\theta}) = \sum a_kr^k(\cos(k\theta)+i\sin(k\theta)) \implies v(r,\theta) = \sum a_kr^k \sin(k\theta) a_k = \frac{f^{(k)}(0)}{k!} v(r,\theta) = 1+r\sin(\theta)+\sum_{k\geq 2}a_kr^ksin(k\theta) = r\sin(\theta)\left(1+\frac{1}{r\sin(\theta)}+\sum_{k\geq 2}a_kr^{k-2}\frac{sin(k\theta)}{sin(\theta)}\right) \frac{1}{r\sin(\theta)} Cr C \frac{1}{r\sin(\theta)}+\sum_{k\geq 2}a_kr^{k-2}\frac{sin(k\theta)}{sin(\theta)} = O(r)","['real-analysis', 'complex-analysis', 'asymptotics']"
1,Application of Rouché's theorem outside the unit circle; Confusion about argument principle,Application of Rouché's theorem outside the unit circle; Confusion about argument principle,,"I am preparing for my function theory exam and came across this problem. Let $f(z) := z^8 - 8z^5+2$. I want to count the number of roots in B:= $\big\{z \ \big| \left| z-2 \right| < \frac{1}{2} \big\}$. From plotting $f$ I know it has one root in $B$. Using that for $z \in B$ we have $\frac{3}{2} < |z| < \frac{5}{2}$, and trying out several candidates for $g$, I sought upper and lower bounds such that one of the following equations is satisfied on $\partial B$: If $|f|<|g|$ on $\partial B$, then  $\#_{roots} \ (f, B) = \#_{roots} (f+ g, B)$, and if $|f-g| < |f| + |g|$ on $\partial B$, then  $\#_{roots} \ (f, B) = \#_{roots} (g, B)$, while in class we also used if $|f+g| < |g|$ on $\partial B$, then  $\#_{roots} \ (f, B) = \#_{roots} (g, B)$. Taking a closer look, $g$ has to have one complex root within $B$. I can't even think of a candidate right now. I don't see a clear path to the solution of my problem. I believe I have a hard time understanding the argument principle, due to it's various equivalent forms. Practically, do I chose one of its inequalities and stick to it or is it better to chose it depending on the problem I am trying to solve? I know there are $5$ roots within the unit circle. And I know I can find 3 more roots in $B_2(0)$.","I am preparing for my function theory exam and came across this problem. Let $f(z) := z^8 - 8z^5+2$. I want to count the number of roots in B:= $\big\{z \ \big| \left| z-2 \right| < \frac{1}{2} \big\}$. From plotting $f$ I know it has one root in $B$. Using that for $z \in B$ we have $\frac{3}{2} < |z| < \frac{5}{2}$, and trying out several candidates for $g$, I sought upper and lower bounds such that one of the following equations is satisfied on $\partial B$: If $|f|<|g|$ on $\partial B$, then  $\#_{roots} \ (f, B) = \#_{roots} (f+ g, B)$, and if $|f-g| < |f| + |g|$ on $\partial B$, then  $\#_{roots} \ (f, B) = \#_{roots} (g, B)$, while in class we also used if $|f+g| < |g|$ on $\partial B$, then  $\#_{roots} \ (f, B) = \#_{roots} (g, B)$. Taking a closer look, $g$ has to have one complex root within $B$. I can't even think of a candidate right now. I don't see a clear path to the solution of my problem. I believe I have a hard time understanding the argument principle, due to it's various equivalent forms. Practically, do I chose one of its inequalities and stick to it or is it better to chose it depending on the problem I am trying to solve? I know there are $5$ roots within the unit circle. And I know I can find 3 more roots in $B_2(0)$.",,"['complex-analysis', 'rouches-theorem']"
2,Two definitions of the space of pointed Riemann surfaces,Two definitions of the space of pointed Riemann surfaces,,"I was reading Wendl's notes on closed holomorphic curves, and could not seem to figure out the following assertion. I think I am missing some fact or the other about Riemann surfaces. Let $\mathcal{M}_{g,m}$ be the space of pointed Riemann surfaces. To construct this, take the space of tuples $(\Sigma, j, \Theta)$ where $\Sigma$ is a topological closed surface of genus $g$, $j$ is a complex structure, and $\Theta$ is an ordered tuple of $m$ points in $\Sigma$. Then, quotient out by the equivalence relation $(\Sigma, j, \Theta) \sim (\Sigma', j', \Theta')$ if and only if there exists some biholomorphic diffeomorphism $\phi: \Sigma \to \Sigma'$ such that $\phi$ maps $\Theta$ to $\Theta'$ in an order-preserving manner. We can define a related space as follows. Fix a surface $\Sigma$ and a tuple of $m$ points $\Phi$. Let $\text{Diff}_+(\Sigma, \Phi)$ be the set of all orientation-preserving diffeomorphisms on $\Sigma$ that restrict to the identity map on $\Phi$. Furthermore let $J(\Sigma)$ be the space of complex structures on $\Sigma$. Then one can take the quotient $J(\Sigma)/\text{Diff}_+(\Sigma, \Phi)$. The claim is that $\mathcal{M}_{g,m}$ is homeomorphic to $J(\Sigma)/\text{Diff}_+(\Sigma, \Phi)$, likely by the map $[(\Sigma, j, \Theta)] \to [j]$ in one direction and $[j] \to [(\Sigma, j, \Phi)]$ in the opposite direction. Right off the bat, to show that the forward map is injective, we must show that, for any complex structure $j'$ and points $\Theta$, the tuple $(\Sigma, j', \Theta)$ is equivalent to $(\Sigma, j', \Phi)$. This reduces to the following statement: There exists a biholomorphic automorphism on $\Sigma$ that takes the set of points $\Theta$ to $\Phi$? I am confused because this statement is certainly not true in general. For example, if we take $g = 0$ and $m = 4$, and $j$ the standard complex structure on $\Sigma = S^2$, the biholomorphic automorphisms are Mobius transformations. These are determined by the image of $\{0, 1, \infty\}$, so the action of the Mobius transformations is not transitive on quadruples of points.","I was reading Wendl's notes on closed holomorphic curves, and could not seem to figure out the following assertion. I think I am missing some fact or the other about Riemann surfaces. Let $\mathcal{M}_{g,m}$ be the space of pointed Riemann surfaces. To construct this, take the space of tuples $(\Sigma, j, \Theta)$ where $\Sigma$ is a topological closed surface of genus $g$, $j$ is a complex structure, and $\Theta$ is an ordered tuple of $m$ points in $\Sigma$. Then, quotient out by the equivalence relation $(\Sigma, j, \Theta) \sim (\Sigma', j', \Theta')$ if and only if there exists some biholomorphic diffeomorphism $\phi: \Sigma \to \Sigma'$ such that $\phi$ maps $\Theta$ to $\Theta'$ in an order-preserving manner. We can define a related space as follows. Fix a surface $\Sigma$ and a tuple of $m$ points $\Phi$. Let $\text{Diff}_+(\Sigma, \Phi)$ be the set of all orientation-preserving diffeomorphisms on $\Sigma$ that restrict to the identity map on $\Phi$. Furthermore let $J(\Sigma)$ be the space of complex structures on $\Sigma$. Then one can take the quotient $J(\Sigma)/\text{Diff}_+(\Sigma, \Phi)$. The claim is that $\mathcal{M}_{g,m}$ is homeomorphic to $J(\Sigma)/\text{Diff}_+(\Sigma, \Phi)$, likely by the map $[(\Sigma, j, \Theta)] \to [j]$ in one direction and $[j] \to [(\Sigma, j, \Phi)]$ in the opposite direction. Right off the bat, to show that the forward map is injective, we must show that, for any complex structure $j'$ and points $\Theta$, the tuple $(\Sigma, j', \Theta)$ is equivalent to $(\Sigma, j', \Phi)$. This reduces to the following statement: There exists a biholomorphic automorphism on $\Sigma$ that takes the set of points $\Theta$ to $\Phi$? I am confused because this statement is certainly not true in general. For example, if we take $g = 0$ and $m = 4$, and $j$ the standard complex structure on $\Sigma = S^2$, the biholomorphic automorphisms are Mobius transformations. These are determined by the image of $\{0, 1, \infty\}$, so the action of the Mobius transformations is not transitive on quadruples of points.",,"['complex-analysis', 'riemann-surfaces', 'symplectic-geometry']"
3,How to prove $\int_0^\infty e^{-x}\ln^n \frac{1}{x}dx<n!$?,How to prove ?,\int_0^\infty e^{-x}\ln^n \frac{1}{x}dx<n!,"Once I met a problem about limits: $$\lim_{n\rightarrow\infty}\int_0^\infty e^{-x}\frac{\ln^n \frac{1}{x}}{n!} dx=1$$ After I proved it, I used Mathematica and discovered that the sequence $\int_0^\infty e^{-x}\frac{\ln^n\frac{1}{x}}{n!} dx$ seems to be monotone increasing to its supremum $1$. I guess so, but I’m confused about how to prove. My question is: How to prove that $$\int_0^\infty e^{-x}\ln^n \frac{1}{x}dx<n!,\quad \forall n\ge 1$$ and the monotonity of $$\int_0^\infty e^{-x}\frac{\ln^n\frac{1}{x}}{n!} dx$$ Additional: I have already proved that $$\lim_{n\rightarrow\infty} \frac{2^{n+1}}{n!}\left(n!-\int_0^\infty e^{-x}\ln^n \frac{1}{x}dx\right)=\lim_{n\rightarrow\infty} \frac{2^{n+1}}{n!}\left(\int_0^1 \left(1-e^{-x}\right)\ln^n\frac{1}{x} dx-\int_1^\infty e^{-x}\ln^n\frac{1}{x} dx\right)=1$$ by using the inequality $x-\frac{x^2}{2}\le 1-e^{-x}\le x$ and $\ln^n x<n! x\left(x\ge 1\right)$. It can indicate that $1-\int_0^\infty e^{-x}\frac{\ln^n \frac{1}{x}}{n!} dx$ is EVENTUALLY  (which means for sufficient large $n$) positive and monotone decreasing at $O\left(2^{-n}\right)$.","Once I met a problem about limits: $$\lim_{n\rightarrow\infty}\int_0^\infty e^{-x}\frac{\ln^n \frac{1}{x}}{n!} dx=1$$ After I proved it, I used Mathematica and discovered that the sequence $\int_0^\infty e^{-x}\frac{\ln^n\frac{1}{x}}{n!} dx$ seems to be monotone increasing to its supremum $1$. I guess so, but I’m confused about how to prove. My question is: How to prove that $$\int_0^\infty e^{-x}\ln^n \frac{1}{x}dx<n!,\quad \forall n\ge 1$$ and the monotonity of $$\int_0^\infty e^{-x}\frac{\ln^n\frac{1}{x}}{n!} dx$$ Additional: I have already proved that $$\lim_{n\rightarrow\infty} \frac{2^{n+1}}{n!}\left(n!-\int_0^\infty e^{-x}\ln^n \frac{1}{x}dx\right)=\lim_{n\rightarrow\infty} \frac{2^{n+1}}{n!}\left(\int_0^1 \left(1-e^{-x}\right)\ln^n\frac{1}{x} dx-\int_1^\infty e^{-x}\ln^n\frac{1}{x} dx\right)=1$$ by using the inequality $x-\frac{x^2}{2}\le 1-e^{-x}\le x$ and $\ln^n x<n! x\left(x\ge 1\right)$. It can indicate that $1-\int_0^\infty e^{-x}\frac{\ln^n \frac{1}{x}}{n!} dx$ is EVENTUALLY  (which means for sufficient large $n$) positive and monotone decreasing at $O\left(2^{-n}\right)$.",,"['real-analysis', 'complex-analysis', 'limits', 'analysis', 'improper-integrals']"
4,Is $\frac{1-z}{|1-z|}$ is holomorphic in $|z|<1$?,Is  is holomorphic in ?,\frac{1-z}{|1-z|} |z|<1,"Let $f(z) = \frac{1-z}{|1-z|}$ with $z\in\mathbb{C}$ such that $|z|< 1$. I want to know whether $f(z)$ is holomorphic in $|z|<1.$ Write $\frac{1-z}{|1-z|}=e^{i\arg(1-z)}$, where $\arg$ denotes the principal argument (from $-\pi$ to $\pi$). Then $$f(z)=e^{i\arg(1-z)}.$$ I know $e^w$ is an entire function of $w$. So it suffices to see if $\arg(1-z)$ is holomorphic in $|z|<1.$ Is this true?","Let $f(z) = \frac{1-z}{|1-z|}$ with $z\in\mathbb{C}$ such that $|z|< 1$. I want to know whether $f(z)$ is holomorphic in $|z|<1.$ Write $\frac{1-z}{|1-z|}=e^{i\arg(1-z)}$, where $\arg$ denotes the principal argument (from $-\pi$ to $\pi$). Then $$f(z)=e^{i\arg(1-z)}.$$ I know $e^w$ is an entire function of $w$. So it suffices to see if $\arg(1-z)$ is holomorphic in $|z|<1.$ Is this true?",,"['complex-analysis', 'complex-numbers']"
5,Algebraic trick to map $|z|<2$,Algebraic trick to map,|z|<2,Suppose that we want to find the image of the region $|z|<1$ under the mapping $w=\frac z{z+1}$ . Since $z=\frac{-w}{w-1}$ (and assuming $w= u+iv$ ) we should have $\left|\frac w{w-1}\right|=\left|\frac{u(u-1)+v^2-iv}{(u-1)^2+v^2}\right|<1$ or equivalently $$u^2(u-1)^2+v^4+2v^2u(u-1)+v^2<(u-1)^4+v^4+2v^2(u-1)^2.$$ Now we can use the following algebraic trick to write $$(u-1+1)^2(u-1)^2+2v^2(u-1+1)(u-1)+v^2<(u-1)^4+2v^2(u-1)^2$$ which implies that $$(u-1)^2+2(u-1)(u-1)^2+2v^2(u-1)+v^2<0$$ or $$((u-1)^2+v^2))(2u-1)<0.$$ Thus the image would be $2u-1<0$ . Now I wonder if there is some similar algebraic trick for $|z|<2$ . In this case we have $$u^2(u-1)^2+v^4+2v^2u(u-1)+v^2<4(u-1)^4+4v^4+8v^2(u-1)^2$$ I've tried similar method but couldn't arrive to something useful. Could anyone help me in this case? Thanks!,Suppose that we want to find the image of the region under the mapping . Since (and assuming ) we should have or equivalently Now we can use the following algebraic trick to write which implies that or Thus the image would be . Now I wonder if there is some similar algebraic trick for . In this case we have I've tried similar method but couldn't arrive to something useful. Could anyone help me in this case? Thanks!,|z|<1 w=\frac z{z+1} z=\frac{-w}{w-1} w= u+iv \left|\frac w{w-1}\right|=\left|\frac{u(u-1)+v^2-iv}{(u-1)^2+v^2}\right|<1 u^2(u-1)^2+v^4+2v^2u(u-1)+v^2<(u-1)^4+v^4+2v^2(u-1)^2. (u-1+1)^2(u-1)^2+2v^2(u-1+1)(u-1)+v^2<(u-1)^4+2v^2(u-1)^2 (u-1)^2+2(u-1)(u-1)^2+2v^2(u-1)+v^2<0 ((u-1)^2+v^2))(2u-1)<0. 2u-1<0 |z|<2 u^2(u-1)^2+v^4+2v^2u(u-1)+v^2<4(u-1)^4+4v^4+8v^2(u-1)^2,"['complex-analysis', 'complex-numbers']"
6,Number of poles of an algebraic function,Number of poles of an algebraic function,,"I don't know much about complex analysis, so I'm not sure if this has an obvious answer or not. I am reading a paper and got stuck on a parenthetical comment. $w(z)$ is a meromorphic function on some simply connected open subset $U\subset \mathbb{C}$, satisfying a polynomial equation in two variables $P(z,w)=0$ over the field of rational functions $\mathbb{C}(t)$. Once this conclusion is reached, the author states in parentheses, ""so it has only finitely many poles"". I'm not sure why this is true, and I'm having trouble proving it.","I don't know much about complex analysis, so I'm not sure if this has an obvious answer or not. I am reading a paper and got stuck on a parenthetical comment. $w(z)$ is a meromorphic function on some simply connected open subset $U\subset \mathbb{C}$, satisfying a polynomial equation in two variables $P(z,w)=0$ over the field of rational functions $\mathbb{C}(t)$. Once this conclusion is reached, the author states in parentheses, ""so it has only finitely many poles"". I'm not sure why this is true, and I'm having trouble proving it.",,['complex-analysis']
7,Construct an analytic function which has simple zeros at all $m+in$,Construct an analytic function which has simple zeros at all,m+in,"I found a ""duplicate"" question: Entire function having zeros at $m+in$ , and I went through some wiki pages of Weierstrass sigma function, but they don't seem to have constructions. So basically I'm quite confused with the exponent. We can begin with $$f(z) = z\prod_{\omega\in\Lambda^*}\left(1-\frac{z}{\omega}\right)\exp(\dots), \quad \Lambda=\Lambda^*\cup\{0\}=\{m+in\} ,$$ also notice that $$\log\left(1-\frac{z}{\omega}\right) = -\frac{z}{\omega} - \frac{z^2}{2\omega^2} - \dots$$ So why does Weierstrass sigma function put two terms in exponential to get rid of the first two terms? It seems one term is ok to kill all exponent that $\leq 1$. Is Weierstrass sigma function the ""best"" construction though?","I found a ""duplicate"" question: Entire function having zeros at $m+in$ , and I went through some wiki pages of Weierstrass sigma function, but they don't seem to have constructions. So basically I'm quite confused with the exponent. We can begin with $$f(z) = z\prod_{\omega\in\Lambda^*}\left(1-\frac{z}{\omega}\right)\exp(\dots), \quad \Lambda=\Lambda^*\cup\{0\}=\{m+in\} ,$$ also notice that $$\log\left(1-\frac{z}{\omega}\right) = -\frac{z}{\omega} - \frac{z^2}{2\omega^2} - \dots$$ So why does Weierstrass sigma function put two terms in exponential to get rid of the first two terms? It seems one term is ok to kill all exponent that $\leq 1$. Is Weierstrass sigma function the ""best"" construction though?",,"['complex-analysis', 'roots', 'weierstrass-factorization']"
8,Showing that $\int_{0}^{\infty} \frac{\cos(x)}{x^{2}+3}dx = \frac{e^{-\sqrt{3}} \pi}{2 \sqrt{3}}$ via Contour Integration?,Showing that  via Contour Integration?,\int_{0}^{\infty} \frac{\cos(x)}{x^{2}+3}dx = \frac{e^{-\sqrt{3}} \pi}{2 \sqrt{3}},"In the text ""Function Theory of One Complex Variable"" Third Edition, I'm inquiring if my proof of $\text{Proposition (1)}$ is sound ? $\text{Proposition (1)}$ $$\int_{0}^{\infty} \frac{\cos(x)}{x^{2}+3}dx = \frac{e^{-\sqrt{3}} \pi}{2 \sqrt{3}}$$ $\text{Proof}$ Assume that $R>1$ define $\gamma_{R}$ such that, $$\gamma_{R}^{1}(t) =  t + i0 \, \,   \text{if} \, \, -R \leq t \leq R$$ $$\gamma_{R}^{2}(t) = Re^{it}  \, \text{if} \, \, \, \, \, \, 0\leq t \leq \pi.$$ $\, \, \, \, \, \, \, \, \, \, \, \, \, \, \, \, \, \,  \, \, \, \, \, \, \, \, \, \, \, \, \, \, \, \, \, \, \, \, \, \, \, \, \, \, \, \, \, \, \, \, \, \, \, \, \, \, \, \, $ Consider our choice $f$ and that, $$\oint_{\gamma_{R}} \frac{e^{iz}}{z^{2}+3} \, dz.$$ Clearly it's obvious that $$\oint_{\gamma_{R}} \frac{e^{iz}}{z^{2}+3}dz = \sum_{\psi = 1,2} \oint_{\gamma_{R}^{\psi}}\frac{e^{iz}}{z^{2}+3}dz.$$ It's trivial that, $$\oint_{\gamma_{R}^{1}}e^{iz}/({z^{2}+3})dx \rightarrow \int_{0}^{\infty} \frac{e^{ix}}{x^{2}+3}\operatorname{dx}$$ It's natural to claim that, $$\Bigg|  \lim_{R \rightarrow \infty}\oint_{ \gamma_{R}^{2}} \frac{e^{iz}}{z^{2}+3} dz  \Bigg|  \rightarrow 0. $$ Using the Estimation Lemma one can be relived that, $$\bigg |\oint_{\gamma_{R}^{2}} \frac{e^{iz}}{z^{2}+3} dz  \bigg | \leq \big\{\text{length}(\gamma_{R}^{2}) \big\} \cdot \sup_{\gamma_{R}^{2}}|\frac{e^{iz}}{z^{2}+3}|\leq \pi R \cdot \frac{1}{R^{2} - 3} \rightarrow 0 \, \text{as} \, R \rightarrow \infty$$ Thus, $$ \operatorname{Re}\int_{0}^{\infty} \frac{\cos(x)}{x^{2}+3}dx =  \operatorname{Re} \frac{e^{-\sqrt{3}} \pi}{2 \sqrt{3}} = \frac{e^{-\sqrt{3}} \pi}{2 \sqrt{3}}.$$ However we need to consider that $$\oint_{\gamma_{R}} \frac{e^{iz}}{z^{2}+3} \, dz = 2 \pi i \sum_{j=1,2} \operatorname{Ind_{\gamma}} \cdot \operatorname{Res_{f}(P_{j})}$$ It's easy to observe that, $$\oint_{\gamma_{R}} \frac{e^{iz}}{z^{2}+3} \, dz = \big(2\pi i \cdot \frac{ie^{\sqrt{3}}}{2 \sqrt{3}}) = \frac{e^{-\sqrt{3}} \pi}{2 \sqrt{3}}$$","In the text ""Function Theory of One Complex Variable"" Third Edition, I'm inquiring if my proof of is sound ? Assume that define such that, Consider our choice and that, Clearly it's obvious that It's trivial that, It's natural to claim that, Using the Estimation Lemma one can be relived that, Thus, However we need to consider that It's easy to observe that,","\text{Proposition (1)} \text{Proposition (1)} \int_{0}^{\infty} \frac{\cos(x)}{x^{2}+3}dx = \frac{e^{-\sqrt{3}} \pi}{2 \sqrt{3}} \text{Proof} R>1 \gamma_{R} \gamma_{R}^{1}(t) =  t + i0 \, \,   \text{if} \, \, -R \leq t \leq R \gamma_{R}^{2}(t) = Re^{it}  \, \text{if} \, \, \, \, \, \, 0\leq t \leq \pi. \, \, \, \, \, \, \, \, \, \, \, \, \, \, \, \, \, \,  \, \, \, \, \, \, \, \, \, \, \, \, \, \, \, \, \, \, \, \, \, \, \, \, \, \, \, \, \, \, \, \, \, \, \, \, \, \, \, \,  f \oint_{\gamma_{R}} \frac{e^{iz}}{z^{2}+3} \, dz. \oint_{\gamma_{R}} \frac{e^{iz}}{z^{2}+3}dz = \sum_{\psi = 1,2} \oint_{\gamma_{R}^{\psi}}\frac{e^{iz}}{z^{2}+3}dz. \oint_{\gamma_{R}^{1}}e^{iz}/({z^{2}+3})dx \rightarrow \int_{0}^{\infty} \frac{e^{ix}}{x^{2}+3}\operatorname{dx} \Bigg|  \lim_{R \rightarrow \infty}\oint_{ \gamma_{R}^{2}} \frac{e^{iz}}{z^{2}+3} dz  \Bigg|  \rightarrow 0.  \bigg |\oint_{\gamma_{R}^{2}} \frac{e^{iz}}{z^{2}+3} dz  \bigg | \leq \big\{\text{length}(\gamma_{R}^{2}) \big\} \cdot \sup_{\gamma_{R}^{2}}|\frac{e^{iz}}{z^{2}+3}|\leq \pi R \cdot \frac{1}{R^{2} - 3} \rightarrow 0 \, \text{as} \, R \rightarrow \infty  \operatorname{Re}\int_{0}^{\infty} \frac{\cos(x)}{x^{2}+3}dx =  \operatorname{Re} \frac{e^{-\sqrt{3}} \pi}{2 \sqrt{3}} = \frac{e^{-\sqrt{3}} \pi}{2 \sqrt{3}}. \oint_{\gamma_{R}} \frac{e^{iz}}{z^{2}+3} \, dz = 2 \pi i \sum_{j=1,2} \operatorname{Ind_{\gamma}} \cdot \operatorname{Res_{f}(P_{j})} \oint_{\gamma_{R}} \frac{e^{iz}}{z^{2}+3} \, dz = \big(2\pi i \cdot \frac{ie^{\sqrt{3}}}{2 \sqrt{3}}) = \frac{e^{-\sqrt{3}} \pi}{2 \sqrt{3}}","['complex-analysis', 'proof-verification', 'contour-integration']"
9,Continuity of an exponential series [duplicate],Continuity of an exponential series [duplicate],,"This question already has an answer here : continuity of power series (1 answer) Closed 5 years ago . Let $(a_n)_{n \in \mathbb{N}}$ be an infinite positive sequence of integers. Assume that $f(\lambda) = \sum\limits_{n=0}^{\infty} a_n \lambda^n$ is finite for any $\lambda \in [0, \lambda_0)$. Does this necessarily imply that $f(\lambda)$ is also continuous in $(0, \lambda_0)$, or might something strange happen?","This question already has an answer here : continuity of power series (1 answer) Closed 5 years ago . Let $(a_n)_{n \in \mathbb{N}}$ be an infinite positive sequence of integers. Assume that $f(\lambda) = \sum\limits_{n=0}^{\infty} a_n \lambda^n$ is finite for any $\lambda \in [0, \lambda_0)$. Does this necessarily imply that $f(\lambda)$ is also continuous in $(0, \lambda_0)$, or might something strange happen?",,"['real-analysis', 'complex-analysis', 'functional-analysis', 'analysis', 'continuity']"
10,Check my Proof Please (Complex Analysis),Check my Proof Please (Complex Analysis),,"Question: Let $f$ be an analytic function in an open set $U$. Let $V = \{z \in \mathbb{C} : \overline{z} \in U\}$. Define $g$ on $V$ by $g(z) = \overline{f(\overline{z})}$. Show that $g$ is analytic on $V$ without using the fact that $g$ is holomorphic. Proof Attempt: Choose $z_0 \in V$. Then $\overline{z_0} \in U$. Since $f$ is analytic on $U$, there is an $r > 0$ and a sequence of complex numbers $\{a_n\}_{n = 0}^{\infty}$ such that for $\overline{z} \in D(\overline{z_0}, r)$, we have $$f(\overline{z}) = \sum_{n = 0}^{\infty} a_n(\overline{z} - \overline{z_0})^n$$ It follows that $$g(z) = \overline{f(\overline{z})} = \sum_{n = 0}^{\infty} \overline{a_n}(z - z_0)^n$$ for all $z \in D(z_0, r)$. Thus, $g$ is analytic on $V$.","Question: Let $f$ be an analytic function in an open set $U$. Let $V = \{z \in \mathbb{C} : \overline{z} \in U\}$. Define $g$ on $V$ by $g(z) = \overline{f(\overline{z})}$. Show that $g$ is analytic on $V$ without using the fact that $g$ is holomorphic. Proof Attempt: Choose $z_0 \in V$. Then $\overline{z_0} \in U$. Since $f$ is analytic on $U$, there is an $r > 0$ and a sequence of complex numbers $\{a_n\}_{n = 0}^{\infty}$ such that for $\overline{z} \in D(\overline{z_0}, r)$, we have $$f(\overline{z}) = \sum_{n = 0}^{\infty} a_n(\overline{z} - \overline{z_0})^n$$ It follows that $$g(z) = \overline{f(\overline{z})} = \sum_{n = 0}^{\infty} \overline{a_n}(z - z_0)^n$$ for all $z \in D(z_0, r)$. Thus, $g$ is analytic on $V$.",,"['complex-analysis', 'proof-verification', 'power-series']"
11,Finding a holomorphic function with real part $u=\frac{x(1+x^2+y^2)}{1+2x^2-2y^2+(x^2+y^2)^2}$.,Finding a holomorphic function with real part .,u=\frac{x(1+x^2+y^2)}{1+2x^2-2y^2+(x^2+y^2)^2},"I am asked to find the holomorphic function $f(z)=f(x\pm iy)$ with real part   $$u=\frac{x(1+x^2+y^2)}{1+2x^2-2y^2+(x^2+y^2)^2}$$   and such that $f(0)=0$. I have tried to apply Cauchy-Riemann equations directly but I don't seem to be getting anywhere. I've also tried writing $u$ in terms of $z$ and $\bar{z}$ and then applying $f'=2\partial_z{u}$, but the expression of the derivative seems unmanagable. Since $z$ and $\bar{z}$ are symmetric in the expression of $u$, I also have the relation $\partial_z{u}=\overline{\partial_{\overline{z}}u}$, which I've tried to plug into $\partial_{\overline{z}}f=0$. But I dont know how to proceed from here.","I am asked to find the holomorphic function $f(z)=f(x\pm iy)$ with real part   $$u=\frac{x(1+x^2+y^2)}{1+2x^2-2y^2+(x^2+y^2)^2}$$   and such that $f(0)=0$. I have tried to apply Cauchy-Riemann equations directly but I don't seem to be getting anywhere. I've also tried writing $u$ in terms of $z$ and $\bar{z}$ and then applying $f'=2\partial_z{u}$, but the expression of the derivative seems unmanagable. Since $z$ and $\bar{z}$ are symmetric in the expression of $u$, I also have the relation $\partial_z{u}=\overline{\partial_{\overline{z}}u}$, which I've tried to plug into $\partial_{\overline{z}}f=0$. But I dont know how to proceed from here.",,"['complex-analysis', 'analysis']"
12,Singular points of analytic continuation,Singular points of analytic continuation,,"In Knopp's theory of functions part 1, the following fact about analytic continuation is mentioned without proof. And I am looking for a simple proof. Let $$f(z) = a_0 + a_1(z-z_0) + a_2(z-z_0)^2 + \dots$$ be a power series around $z_0$ with radius of convergence $R$ where $0 < R < \infty.$ Let $z_1$ be a point in the region of convergence with $ 0 < |z_1 - z_0| <  R.$ Assume that $f$ can be expanded in a power series around $z_1$ with a radius of convergence exactly equal to $R_1 = R - |z_0 - z_1|$. Clearly, $C$, the circle with center $z_0$ with radius $R$ and, $C_1$ the circle with its center at $z_1$ with radius $R_1$ intersect at exactly one point, say $z_2$ and $C_1$ lies within $C$.  Knopp mentions that $z_2$ is a singular point of $f$ in the following sense : given any neighborhood of $z_2$,say $U = B(z_2,r)$, there is no analytic function, $g$ defined on $U$  whose values coincide with the values of $f$ on $U \cap B(z_0,R)$. My observations : To prove this, it is sufficient to show that given any $r > 0$ there exists a $\delta > 0$ such that $B(z_1,R_1+\delta) \subseteq B(z_0,R) \bigcup B(z_2,r) \tag{*} \label{e:1}. $ To see this notice that if an analytic function $g$ exists on $B(z_2,r)$ for some $r > 0$, such that the values of g coincide with the values of $f$ on $B(z_0,R) \bigcap B(z_2,r)$  then $g$ is an analytical extension of $f$ on $B(z_0,R) \bigcup B(z_2,r)$ and moreover assuming $\eqref{e:1}$ it means that $f$ can be extended to an analytical function on $B(z_1,R_1+\delta)$. However this means the radius of convergence of the power series determined by $f$ at $z_1$ must be strictly larger than $R_1$ which is a contradiction. So it remains to prove $\eqref{e:1}$ which is a purely geometric problem. A visual representation suggests that if $\delta$ is chosen to be less than the length of a segment joining $z_1$ and $A$ where $A$ is a point of intersection of the circle at $z_2$ with radius $r$ and $C$ the circle with center at $z_0$ with radius $R$ we should be good. But I don't see an easy proof.","In Knopp's theory of functions part 1, the following fact about analytic continuation is mentioned without proof. And I am looking for a simple proof. Let $$f(z) = a_0 + a_1(z-z_0) + a_2(z-z_0)^2 + \dots$$ be a power series around $z_0$ with radius of convergence $R$ where $0 < R < \infty.$ Let $z_1$ be a point in the region of convergence with $ 0 < |z_1 - z_0| <  R.$ Assume that $f$ can be expanded in a power series around $z_1$ with a radius of convergence exactly equal to $R_1 = R - |z_0 - z_1|$. Clearly, $C$, the circle with center $z_0$ with radius $R$ and, $C_1$ the circle with its center at $z_1$ with radius $R_1$ intersect at exactly one point, say $z_2$ and $C_1$ lies within $C$.  Knopp mentions that $z_2$ is a singular point of $f$ in the following sense : given any neighborhood of $z_2$,say $U = B(z_2,r)$, there is no analytic function, $g$ defined on $U$  whose values coincide with the values of $f$ on $U \cap B(z_0,R)$. My observations : To prove this, it is sufficient to show that given any $r > 0$ there exists a $\delta > 0$ such that $B(z_1,R_1+\delta) \subseteq B(z_0,R) \bigcup B(z_2,r) \tag{*} \label{e:1}. $ To see this notice that if an analytic function $g$ exists on $B(z_2,r)$ for some $r > 0$, such that the values of g coincide with the values of $f$ on $B(z_0,R) \bigcap B(z_2,r)$  then $g$ is an analytical extension of $f$ on $B(z_0,R) \bigcup B(z_2,r)$ and moreover assuming $\eqref{e:1}$ it means that $f$ can be extended to an analytical function on $B(z_1,R_1+\delta)$. However this means the radius of convergence of the power series determined by $f$ at $z_1$ must be strictly larger than $R_1$ which is a contradiction. So it remains to prove $\eqref{e:1}$ which is a purely geometric problem. A visual representation suggests that if $\delta$ is chosen to be less than the length of a segment joining $z_1$ and $A$ where $A$ is a point of intersection of the circle at $z_2$ with radius $r$ and $C$ the circle with center at $z_0$ with radius $R$ we should be good. But I don't see an easy proof.",,['complex-analysis']
13,New/useful method for summation of divergent series?,New/useful method for summation of divergent series?,,"Questions $$  S(n,x) = x+e^x + e^{e^x} + e^{e^{e^x}} + \dots \text{$n$ times}$$ Also obeys (see background for argument): $$ \frac{1}{2 \pi i} \oint e^{S(k,x)}  \frac{\partial \ln(\frac{\int_0^\infty e^{-t} t^k dt }{ \int_0^\infty  e^{-t} t^{(k-n)} dt})}{\partial k} dk  =  \frac{\partial S(n,x)}{\partial x}$$ Can this be used in the Borel summation sense for divergent series? If so, when can it be used for analytical continuity (convergence issues)? Is it useful(intuitively I feel it should be more powerful than Borel summation)? In the following heuristic sense: $$ \kappa  = \sum_{n=1}^\infty a_n = \sum_n a_n \frac{\frac{\partial S(n,x)}{\partial x}}{1 + e^x + e^x e^{e^x} + e^x e^{e^x}  e^{e^{e^x}} + \dots \text{$n$ times} } $$ Using the L.H.S of the first equation: $$ \kappa  = \frac{1}{2 \pi i} \sum_n a_n \frac{\oint e^{S(k,x)}  \frac{\partial \ln(\frac{\int_0^\infty e^{-t} t^k dt }{ \int_0^\infty  e^{-t} t^{(k-n)} dt})}{\partial k} dk }{1 + e^x + e^x e^{e^x} + e^x e^{e^x}  e^{e^{e^x}} + \dots \text{$n$ times} } $$ Swapping order of summation and contour integral: $$ \kappa  =^! \frac{1}{2 \pi i} \oint e^{S(k,x)} \sum_n \frac{ a_n  \frac{\partial \ln(\frac{\int_0^\infty e^{-t} t^k dt }{ \int_0^\infty  e^{-t} t^{(k-n)} dt})}{\partial k}  }{1 + e^x + e^x e^{e^x} + e^x e^{e^x}  e^{e^{e^x}} + \dots \text{$n$ times} }dk $$ How can I make this rigorous? Background I've been recently studying the following series: $$ S(n,x) = x+e^x + e^{e^x} + e^{e^{e^x}} + \dots \text{$n$ times}$$ Where the $n$'th term is raising the $x$ exponentially $n$ number of times. $$ b_n(x) = \underbrace{e^{e^{e^{\dots}x}}}_{\text{$n$ times exponentially raised}} $$ $n$ number of times. Hence, we notice: $$ e^{S(r,x)} = \frac{\partial b_{r+1}(x)}{\partial x}$$ Summing both sides and defining $S(0,x) \equiv 0$: $$ \sum_{r=0}^n e^{S(r,x)}  = \sum_{r=1}^{n+1} \frac{\partial b_{r}(x)}{\partial x}  $$ Hence, we get: $$ \sum_{r=0}^n e^{S(r,x)}  =  \frac{\partial S(n+1,x)}{\partial x}$$ Rewriting the R.H.S using complex analysis as a contour integral over the whole complex plane: $$\frac{1}{2 \pi i} \oint \sum_{r=0}^n \frac{e^{S(k,x)}}{k-r}dk  =  \frac{\partial S(n+1,x)}{\partial x}$$ Taking $e^{S(k,x)}$ common: $$ \frac{1}{2 \pi i} \oint e^{S(k,x)} \sum_{r=0}^n \frac{1}{k-r}dk  =  \frac{\partial S(n+1,x)}{\partial x}$$ Further using $d \ln x = dx/x$ $$  \frac{1}{2 \pi i} \oint e^{S(k,x)} \sum_{r=0}^n d \ln({k-r})  =  \frac{\partial S(n+1,x)}{\partial x}$$ Rewriting as factorial: $$ \frac{1}{2 \pi i} \oint e^{S(k,x)} \frac{\partial \ln(\frac{(k)!}{(k-n-1)!})}{\partial k} dk  =  \frac{\partial S(n+1,x)}{\partial x}$$ Analytically continuing $k!$ using the gamma function: $$ \frac{1}{2 \pi i} \oint e^{S(k,x)}  \frac{\partial \ln(\frac{\int_0^\infty e^{-t} t^k dt }{ \int_0^\infty  e^{-t} t^{(k-n-1)} dt})}{\partial k} dk  =  \frac{\partial S(n+1,x)}{\partial x}$$","Questions $$  S(n,x) = x+e^x + e^{e^x} + e^{e^{e^x}} + \dots \text{$n$ times}$$ Also obeys (see background for argument): $$ \frac{1}{2 \pi i} \oint e^{S(k,x)}  \frac{\partial \ln(\frac{\int_0^\infty e^{-t} t^k dt }{ \int_0^\infty  e^{-t} t^{(k-n)} dt})}{\partial k} dk  =  \frac{\partial S(n,x)}{\partial x}$$ Can this be used in the Borel summation sense for divergent series? If so, when can it be used for analytical continuity (convergence issues)? Is it useful(intuitively I feel it should be more powerful than Borel summation)? In the following heuristic sense: $$ \kappa  = \sum_{n=1}^\infty a_n = \sum_n a_n \frac{\frac{\partial S(n,x)}{\partial x}}{1 + e^x + e^x e^{e^x} + e^x e^{e^x}  e^{e^{e^x}} + \dots \text{$n$ times} } $$ Using the L.H.S of the first equation: $$ \kappa  = \frac{1}{2 \pi i} \sum_n a_n \frac{\oint e^{S(k,x)}  \frac{\partial \ln(\frac{\int_0^\infty e^{-t} t^k dt }{ \int_0^\infty  e^{-t} t^{(k-n)} dt})}{\partial k} dk }{1 + e^x + e^x e^{e^x} + e^x e^{e^x}  e^{e^{e^x}} + \dots \text{$n$ times} } $$ Swapping order of summation and contour integral: $$ \kappa  =^! \frac{1}{2 \pi i} \oint e^{S(k,x)} \sum_n \frac{ a_n  \frac{\partial \ln(\frac{\int_0^\infty e^{-t} t^k dt }{ \int_0^\infty  e^{-t} t^{(k-n)} dt})}{\partial k}  }{1 + e^x + e^x e^{e^x} + e^x e^{e^x}  e^{e^{e^x}} + \dots \text{$n$ times} }dk $$ How can I make this rigorous? Background I've been recently studying the following series: $$ S(n,x) = x+e^x + e^{e^x} + e^{e^{e^x}} + \dots \text{$n$ times}$$ Where the $n$'th term is raising the $x$ exponentially $n$ number of times. $$ b_n(x) = \underbrace{e^{e^{e^{\dots}x}}}_{\text{$n$ times exponentially raised}} $$ $n$ number of times. Hence, we notice: $$ e^{S(r,x)} = \frac{\partial b_{r+1}(x)}{\partial x}$$ Summing both sides and defining $S(0,x) \equiv 0$: $$ \sum_{r=0}^n e^{S(r,x)}  = \sum_{r=1}^{n+1} \frac{\partial b_{r}(x)}{\partial x}  $$ Hence, we get: $$ \sum_{r=0}^n e^{S(r,x)}  =  \frac{\partial S(n+1,x)}{\partial x}$$ Rewriting the R.H.S using complex analysis as a contour integral over the whole complex plane: $$\frac{1}{2 \pi i} \oint \sum_{r=0}^n \frac{e^{S(k,x)}}{k-r}dk  =  \frac{\partial S(n+1,x)}{\partial x}$$ Taking $e^{S(k,x)}$ common: $$ \frac{1}{2 \pi i} \oint e^{S(k,x)} \sum_{r=0}^n \frac{1}{k-r}dk  =  \frac{\partial S(n+1,x)}{\partial x}$$ Further using $d \ln x = dx/x$ $$  \frac{1}{2 \pi i} \oint e^{S(k,x)} \sum_{r=0}^n d \ln({k-r})  =  \frac{\partial S(n+1,x)}{\partial x}$$ Rewriting as factorial: $$ \frac{1}{2 \pi i} \oint e^{S(k,x)} \frac{\partial \ln(\frac{(k)!}{(k-n-1)!})}{\partial k} dk  =  \frac{\partial S(n+1,x)}{\partial x}$$ Analytically continuing $k!$ using the gamma function: $$ \frac{1}{2 \pi i} \oint e^{S(k,x)}  \frac{\partial \ln(\frac{\int_0^\infty e^{-t} t^k dt }{ \int_0^\infty  e^{-t} t^{(k-n-1)} dt})}{\partial k} dk  =  \frac{\partial S(n+1,x)}{\partial x}$$",,"['sequences-and-series', 'complex-analysis', 'divergent-series', 'tetration', 'regularization']"
14,Inequality with complex number,Inequality with complex number,,"I have the following homework question for a course in complex analysis: Determine all the $z\in\mathbb{C}$ with $|\sin z| ≤ 1$, and find an $n\in \mathbb{N}$ such that $|\sin(in)| > 10 000$. I have however no clue how to do this, also because during the lecture the teacher said that is no ordering in the complex numbers. I tried substituting $\sin(z)=\frac{1}{2i}(e^{iz}$ - $e^{-iz})$ but this didn't bring my any further.","I have the following homework question for a course in complex analysis: Determine all the $z\in\mathbb{C}$ with $|\sin z| ≤ 1$, and find an $n\in \mathbb{N}$ such that $|\sin(in)| > 10 000$. I have however no clue how to do this, also because during the lecture the teacher said that is no ordering in the complex numbers. I tried substituting $\sin(z)=\frac{1}{2i}(e^{iz}$ - $e^{-iz})$ but this didn't bring my any further.",,['complex-analysis']
15,"Prove that $f$ is constant, when $ \operatorname{Re}(f)^m = \operatorname{Im}(f)^n$","Prove that  is constant, when",f  \operatorname{Re}(f)^m = \operatorname{Im}(f)^n,"Take $f:U\rightarrow \mathbb{C}$ is holomorphic function in $U$ and $U$ connected open subset. If exist $m,n\in \mathbb{N}$ such that $$ [  \operatorname{Re}(f(z))]^m = [ \operatorname{Im}(f(z))]^n ,$$ $f$ is constant in $U$ . Note: The exercise requires using the theorem and equations of Cauchy-Riemann. NB: An attempt is in the comments. Question : In my first attemp, I want to see that is not possible that $det(A_{(x,y)})=0 \ \ \forall (x,y) \in U$ One possibilities is $v^{2n-2} (x,y) =0$ . This implies that $f(x,y)=0$ . But, is in this point. And others? $f=0$ ever?","Take is holomorphic function in and connected open subset. If exist such that is constant in . Note: The exercise requires using the theorem and equations of Cauchy-Riemann. NB: An attempt is in the comments. Question : In my first attemp, I want to see that is not possible that One possibilities is . This implies that . But, is in this point. And others? ever?","f:U\rightarrow \mathbb{C} U U m,n\in \mathbb{N}  [
 \operatorname{Re}(f(z))]^m = [ \operatorname{Im}(f(z))]^n , f U det(A_{(x,y)})=0 \ \ \forall (x,y) \in U v^{2n-2} (x,y) =0 f(x,y)=0 f=0",['complex-analysis']
16,"Laplace transform of a ""heat kernel""","Laplace transform of a ""heat kernel""",,"This question is closely releted to this question: How do we solve the laplace transform of the Heat Kernel? Let $A>0$ and $$f(t) = \frac{A^2}{2\sqrt{\pi}t^\frac{3}{2}}e^{-\frac{A^2}{4t}}$$ Following the same computations as in the question I linked, one can prove that for $s \in \mathbb{R}$, $s\geq 0$ one has  $$\mathcal{L[f](s) :=\int_0^{+\infty}e^{-st}f(t)\,dt}= e^{-A\sqrt{s}}$$ (see also https://projecteuclid.org/download/pdf_1/euclid.aoms/1177731708 at page 252 ) Is this formula true also for $s \in \mathbb{C}, \mathrm{Re(s)} \geq 0$, thinking the square root of $s$ as the principal branch of the root (i.e. the square root of $s$ with positive real part)?","This question is closely releted to this question: How do we solve the laplace transform of the Heat Kernel? Let $A>0$ and $$f(t) = \frac{A^2}{2\sqrt{\pi}t^\frac{3}{2}}e^{-\frac{A^2}{4t}}$$ Following the same computations as in the question I linked, one can prove that for $s \in \mathbb{R}$, $s\geq 0$ one has  $$\mathcal{L[f](s) :=\int_0^{+\infty}e^{-st}f(t)\,dt}= e^{-A\sqrt{s}}$$ (see also https://projecteuclid.org/download/pdf_1/euclid.aoms/1177731708 at page 252 ) Is this formula true also for $s \in \mathbb{C}, \mathrm{Re(s)} \geq 0$, thinking the square root of $s$ as the principal branch of the root (i.e. the square root of $s$ with positive real part)?",,"['calculus', 'real-analysis', 'complex-analysis', 'laplace-transform', 'integral-transforms']"
17,Finding the cauchy integral for $I(n)=\int_\gamma \frac{\cos{z}}{(z-z_0)^3}{dz}$ when $\gamma: |z|=3.$,Finding the cauchy integral for  when,I(n)=\int_\gamma \frac{\cos{z}}{(z-z_0)^3}{dz} \gamma: |z|=3.,"$$\int_\gamma \frac{\cos z}{(z-z_0)^3}dz,\quad  \gamma \colon |z|=3$$ I was able to find the solution for this problem which included  $z=2 \in \gamma, -3<z<3$, therefore $-3<2<3$. And when comparing to Cauchy formula we find that $n+1=3 \Rightarrow n=2$. Now the rest of the solution is obvious. What I wasn't able to understand was how did we determine that $<=2 \in \gamma$  and that it is the point that we are looking for. Please note that English isn't my first language so I'm having problems translating the exact mathematical words to English so pardon the quality of this post. Any help would be greatly appreciated.","$$\int_\gamma \frac{\cos z}{(z-z_0)^3}dz,\quad  \gamma \colon |z|=3$$ I was able to find the solution for this problem which included  $z=2 \in \gamma, -3<z<3$, therefore $-3<2<3$. And when comparing to Cauchy formula we find that $n+1=3 \Rightarrow n=2$. Now the rest of the solution is obvious. What I wasn't able to understand was how did we determine that $<=2 \in \gamma$  and that it is the point that we are looking for. Please note that English isn't my first language so I'm having problems translating the exact mathematical words to English so pardon the quality of this post. Any help would be greatly appreciated.",,"['complex-analysis', 'cauchy-integral-formula']"
18,"$w \mapsto \frac{1}{2i\pi} \int_{\gamma}\frac{1}{z-w}\mathrm{d}z$ from $\mathbb{C} \setminus \gamma([a,b]) \to \mathbb{C}$ is continuous",from  is continuous,"w \mapsto \frac{1}{2i\pi} \int_{\gamma}\frac{1}{z-w}\mathrm{d}z \mathbb{C} \setminus \gamma([a,b]) \to \mathbb{C}","Let $\gamma: [a,b] \rightarrow \mathbb{C}$ be continuous and piecewise continuously differentiable but not necessarily closed. I want to show that $$w \mapsto \frac{1}{2i\pi} \int_{\gamma}\frac{1}{z-w}\mathrm{d}z$$ from $\mathbb{C} \setminus \gamma([a,b]) \to \mathbb{C}$ is continuous. $f(w)=\frac{1}{2i\pi} \int_{\gamma}\frac{1}{z-w}\mathrm{d}z=\frac{1}{2i\pi} \int_{a}^b\frac{1}{\gamma(t)-w}\gamma'(t)\mathrm{d}t=\frac{1}{2\pi i}\sum_{i=1}^n\int_{\xi_{i-1}}^{\xi_i}\frac{\gamma'(t)}{\gamma(t)-w}\mathrm{d}t$ $a=\xi_1<...<\xi_n=b.$ Now, because $\gamma$ is piecewise continuously differentiable for every $i=1...n$ $\gamma'(t)$ is continuous on $[\xi_{i-1},\xi_i]$ and because $\mathbb{C} \setminus \gamma([a,b])$ the denominator is also continuous therefore $f(w)$ is continuous. Is that correct? Maybe there is a way arguing using parameter integrals","Let $\gamma: [a,b] \rightarrow \mathbb{C}$ be continuous and piecewise continuously differentiable but not necessarily closed. I want to show that $$w \mapsto \frac{1}{2i\pi} \int_{\gamma}\frac{1}{z-w}\mathrm{d}z$$ from $\mathbb{C} \setminus \gamma([a,b]) \to \mathbb{C}$ is continuous. $f(w)=\frac{1}{2i\pi} \int_{\gamma}\frac{1}{z-w}\mathrm{d}z=\frac{1}{2i\pi} \int_{a}^b\frac{1}{\gamma(t)-w}\gamma'(t)\mathrm{d}t=\frac{1}{2\pi i}\sum_{i=1}^n\int_{\xi_{i-1}}^{\xi_i}\frac{\gamma'(t)}{\gamma(t)-w}\mathrm{d}t$ $a=\xi_1<...<\xi_n=b.$ Now, because $\gamma$ is piecewise continuously differentiable for every $i=1...n$ $\gamma'(t)$ is continuous on $[\xi_{i-1},\xi_i]$ and because $\mathbb{C} \setminus \gamma([a,b])$ the denominator is also continuous therefore $f(w)$ is continuous. Is that correct? Maybe there is a way arguing using parameter integrals",,"['complex-analysis', 'continuity', 'line-integrals']"
19,Number of oscillations of a Gaussian convolution,Number of oscillations of a Gaussian convolution,,"Let $f(x)=e^{-x^2/2}$ and $g(t)$ be some symmetric, positive and  bounded function and let the convolution of $f(x)$ and $g(x)$ be defined as follows: \begin{align} h(x)= \int f(x-t)g(t)\, dt. \end{align} My questions is: Suppose we choose some value of $y=c$.  How many times does the function $h(x)$ equals $c$ on some interval $[-a,a]$? In other words, can we say something about the number of zeros of the function  on $[-a,a]$ \begin{align} F(x)= h(x)-c.  \end{align} Let $S_a(F)$ denote the set of zeros of $F$ on $[-a,a]$. Partial Answer: Because convolution ""increases"" analyticity we have that $h(x)$ and  $F(x)$ are analytic. Therefore, by the standard identity theorem argument, we have that   $F(x)$ can have finitely many zeros on any interval $[-a,a]$. So, $S_a(F)$ is a finite set. My question: Can we say more about the cardinality of the set of zeros $S_a(F)$? In particular, can we give a bound on it? Most likely a uniform bound is impossible. However, I think we can give a bound that depends on $h(x)$. Comment I would also appreciate a reference if this question been addressed before.  Also, if you can think of some keywords that would be great too.","Let $f(x)=e^{-x^2/2}$ and $g(t)$ be some symmetric, positive and  bounded function and let the convolution of $f(x)$ and $g(x)$ be defined as follows: \begin{align} h(x)= \int f(x-t)g(t)\, dt. \end{align} My questions is: Suppose we choose some value of $y=c$.  How many times does the function $h(x)$ equals $c$ on some interval $[-a,a]$? In other words, can we say something about the number of zeros of the function  on $[-a,a]$ \begin{align} F(x)= h(x)-c.  \end{align} Let $S_a(F)$ denote the set of zeros of $F$ on $[-a,a]$. Partial Answer: Because convolution ""increases"" analyticity we have that $h(x)$ and  $F(x)$ are analytic. Therefore, by the standard identity theorem argument, we have that   $F(x)$ can have finitely many zeros on any interval $[-a,a]$. So, $S_a(F)$ is a finite set. My question: Can we say more about the cardinality of the set of zeros $S_a(F)$? In particular, can we give a bound on it? Most likely a uniform bound is impossible. However, I think we can give a bound that depends on $h(x)$. Comment I would also appreciate a reference if this question been addressed before.  Also, if you can think of some keywords that would be great too.",,"['real-analysis', 'complex-analysis', 'convolution', 'gaussian-integral']"
20,Checking directly the integral of $1/(z-a)$ around a disc centered at the origin is $2\pi i$,Checking directly the integral of  around a disc centered at the origin is,1/(z-a) 2\pi i,We know by Cauchy's integral formula that $$\int\limits_{|z|=1} \frac{1}{z-a} \ dz = 2 \pi i$$ for $|a| < 1$. We can try to calculate this directly. If $a = 0$ then simply substituting in $z = e^{i\theta}$ works. However if $a \neq 0$ say $a = \frac{1}{2}$ then upon substituting we get $$\int\limits_{|z|=1} \frac{1}{z-\frac{1}{2}} \ dz = i\int\limits_{0}^{2\pi} \frac{e^{i\theta}}{e^{i\theta}-\frac{1}{2}} \ d\theta = i\int\limits_{0}^{2\pi} 1+\frac{1}{2e^{i\theta}-1} \ d\theta = 2 \pi i + i\int\limits_{0}^{2\pi} \frac{1}{2e^{i\theta}-1} \ d\theta$$ From the integral formula we know that the last integral must be zero. Is there an elementary way of seeing this?,We know by Cauchy's integral formula that $$\int\limits_{|z|=1} \frac{1}{z-a} \ dz = 2 \pi i$$ for $|a| < 1$. We can try to calculate this directly. If $a = 0$ then simply substituting in $z = e^{i\theta}$ works. However if $a \neq 0$ say $a = \frac{1}{2}$ then upon substituting we get $$\int\limits_{|z|=1} \frac{1}{z-\frac{1}{2}} \ dz = i\int\limits_{0}^{2\pi} \frac{e^{i\theta}}{e^{i\theta}-\frac{1}{2}} \ d\theta = i\int\limits_{0}^{2\pi} 1+\frac{1}{2e^{i\theta}-1} \ d\theta = 2 \pi i + i\int\limits_{0}^{2\pi} \frac{1}{2e^{i\theta}-1} \ d\theta$$ From the integral formula we know that the last integral must be zero. Is there an elementary way of seeing this?,,"['integration', 'complex-analysis']"
21,Factorization of $\Delta$-function,Factorization of -function,\Delta,"I am reading Serre's 'A course in Arithmetic' and got stuck in his Theorem 6 (Jacobi) on page 95. Specifically, he defines $$ H_1(z) = \sum_n \sum_m' \frac{1}{(m-1+nz)(m+nz)}, \hspace{5mm} H(z) = \sum_m \sum_n' \frac{1}{(m-1+nz)(m+nz)} $$ where the prime means the sum runs through the pairs $(m,n)\neq (0,0),(1,0)$. He writes: $\textit{The series $H_1$ and $H$ are easy to calculate explicitly because of the formula}$ $$ \frac{1}{(m-1+nz)(m+nz)} = \frac{1}{m-1+nz} - \frac{1}{m+nz} $$ $\textit{One finds that they converge, and that}$ $$ H_1 = 2, \hspace{5mm} H = 2-2\pi i/z $$ I understand the fraction identity, and I see how you obtain $H_1 = 2$, but I can't seem to get the right result for $H$. Any help is appreciated.","I am reading Serre's 'A course in Arithmetic' and got stuck in his Theorem 6 (Jacobi) on page 95. Specifically, he defines $$ H_1(z) = \sum_n \sum_m' \frac{1}{(m-1+nz)(m+nz)}, \hspace{5mm} H(z) = \sum_m \sum_n' \frac{1}{(m-1+nz)(m+nz)} $$ where the prime means the sum runs through the pairs $(m,n)\neq (0,0),(1,0)$. He writes: $\textit{The series $H_1$ and $H$ are easy to calculate explicitly because of the formula}$ $$ \frac{1}{(m-1+nz)(m+nz)} = \frac{1}{m-1+nz} - \frac{1}{m+nz} $$ $\textit{One finds that they converge, and that}$ $$ H_1 = 2, \hspace{5mm} H = 2-2\pi i/z $$ I understand the fraction identity, and I see how you obtain $H_1 = 2$, but I can't seem to get the right result for $H$. Any help is appreciated.",,"['complex-analysis', 'modular-forms']"
22,Complex Analysis: Sketch the image of a domain under the mapping $f(z)=\sqrt{z^2+1}$,Complex Analysis: Sketch the image of a domain under the mapping,f(z)=\sqrt{z^2+1},"I'm having some trouble solving the following: Suppose $D=\{z: Re(z)>0\}$ and suppose $f(z)=\sqrt{z^2+1}$ where we mean the principal branch of the squared root. Find and sketch the image of $D$ under $f$. I've tried to rewrite $f(z)=w=u+iv$ in terms of $x$ and $y$ to determine a $u$- and $v$-part which I could try to sketch. So I got this: $w=\sqrt{z^2+1}=\sqrt{(x+iy)^2+1}=\sqrt{x^2+i2xy-y^2+1}$. But this is where I'm stuck. I can't seem to get rid of the squared root. Also, I have yet to see how to proceed after rewriting. Can anyone help me out?","I'm having some trouble solving the following: Suppose $D=\{z: Re(z)>0\}$ and suppose $f(z)=\sqrt{z^2+1}$ where we mean the principal branch of the squared root. Find and sketch the image of $D$ under $f$. I've tried to rewrite $f(z)=w=u+iv$ in terms of $x$ and $y$ to determine a $u$- and $v$-part which I could try to sketch. So I got this: $w=\sqrt{z^2+1}=\sqrt{(x+iy)^2+1}=\sqrt{x^2+i2xy-y^2+1}$. But this is where I'm stuck. I can't seem to get rid of the squared root. Also, I have yet to see how to proceed after rewriting. Can anyone help me out?",,"['complex-analysis', 'conformal-geometry']"
23,Counting Zeros with Rouche's Theorem,Counting Zeros with Rouche's Theorem,,"I'm attempting to answer the question, ""Prove that for any positive number $\epsilon$, the function $f(z)=\frac{1}{z+i} + \sin(z)$ has infinitely many zeros on the strip $|Im(z)|<\epsilon$"". My Work So Far: I know that I want to apply Rouche's Theorem. Assuming I understand Roche's Theorem correctly, I want to think of a function $g(z)$ that has infinitely many zeros in the strip $|Im(z)| < \epsilon$ and then show that $|f(z)-g(z)|$ is less than either $|g(z)|$ or $|f(z)|$. To this end, I thought of $g(z) = sin(z)$ since it has infinitely many zeros in the region of interest, which in of itself seems fairly indicative. However, when I attempt to use this $g(z)$, I end up with $|\frac{1}{z+i}|$, which seems like it can become arbitrarily small for $\epsilon \geq i$. As such, I can't bound this term by either $|g(z)|$ or $|f(z)|$. Am I missing something in my application of Roche's Theorem, or am I perhaps looking at the wrong $g(z)$ to begin with? To address a more general question for the community, am I perhaps misapplying Rouche's Theorem, or is there perhaps a different way/trick that I should be thinking when addressing Roche's Theorem and problems of a similar nature to this one?","I'm attempting to answer the question, ""Prove that for any positive number $\epsilon$, the function $f(z)=\frac{1}{z+i} + \sin(z)$ has infinitely many zeros on the strip $|Im(z)|<\epsilon$"". My Work So Far: I know that I want to apply Rouche's Theorem. Assuming I understand Roche's Theorem correctly, I want to think of a function $g(z)$ that has infinitely many zeros in the strip $|Im(z)| < \epsilon$ and then show that $|f(z)-g(z)|$ is less than either $|g(z)|$ or $|f(z)|$. To this end, I thought of $g(z) = sin(z)$ since it has infinitely many zeros in the region of interest, which in of itself seems fairly indicative. However, when I attempt to use this $g(z)$, I end up with $|\frac{1}{z+i}|$, which seems like it can become arbitrarily small for $\epsilon \geq i$. As such, I can't bound this term by either $|g(z)|$ or $|f(z)|$. Am I missing something in my application of Roche's Theorem, or am I perhaps looking at the wrong $g(z)$ to begin with? To address a more general question for the community, am I perhaps misapplying Rouche's Theorem, or is there perhaps a different way/trick that I should be thinking when addressing Roche's Theorem and problems of a similar nature to this one?",,"['complex-analysis', 'roots', 'rouches-theorem']"
24,If $f$ is entire and for some $r>0$ we have $f(z)=Cz^n$ for a constant $C \in\mathbb C$ on $|z|=r$ then does it follow that $f(z)=Cz^n$ for all $z$?,If  is entire and for some  we have  for a constant  on  then does it follow that  for all ?,f r>0 f(z)=Cz^n C \in\mathbb C |z|=r f(z)=Cz^n z,"If $f$ is entire and for some $r>0$ we have $f(z)=Cz^n$ for a constant $C \in\mathbb C$ on $|z|=r$ then does it follow that $f(z)=Cz^n$ for all $z$? Since a circle is not open the identity principle doesn't apply so I'm not sure how to prove it, assuming it is even true.","If $f$ is entire and for some $r>0$ we have $f(z)=Cz^n$ for a constant $C \in\mathbb C$ on $|z|=r$ then does it follow that $f(z)=Cz^n$ for all $z$? Since a circle is not open the identity principle doesn't apply so I'm not sure how to prove it, assuming it is even true.",,"['complex-analysis', 'entire-functions']"
25,Domain of complex function / singularity,Domain of complex function / singularity,,"The definition of singularity in Wolfram MathWorld says ""Complex singularities are points $z_0$ in the domain of a function $f$ where $f$ fails to be analytic."" http://mathworld.wolfram.com/Singularity.html But in the function $f(z)=\frac{1}{z}$, zero is a pole, thus a singularity. So according to the above definition, $z=0$ is in the domain of $f$?","The definition of singularity in Wolfram MathWorld says ""Complex singularities are points $z_0$ in the domain of a function $f$ where $f$ fails to be analytic."" http://mathworld.wolfram.com/Singularity.html But in the function $f(z)=\frac{1}{z}$, zero is a pole, thus a singularity. So according to the above definition, $z=0$ is in the domain of $f$?",,"['complex-analysis', 'complex-numbers']"
26,"For $z_1, z_2 \in \mathbb C$ prove that $|z_1+z_2|^2+|z_1-z_2|^2=2(|z_1|^2+|z_2|^2)$, and interpret this result geometrically.","For  prove that , and interpret this result geometrically.","z_1, z_2 \in \mathbb C |z_1+z_2|^2+|z_1-z_2|^2=2(|z_1|^2+|z_2|^2)","For $z_1, z_2 \in \mathbb C$ prove that $|z_1+z_2|^2+|z_1-z_2|^2=2(|z_1|^2+|z_2|^2)$, and interpret this result geometrically. We have \begin{align*} 	|z_1+z_2|^2+|z_1-z_2|^2&=(z_1+z_2)(\overline{z_1+z_2})+(z_1-z_2)(\overline{z_1-z_2})\\ 	&=(z_1+z_2)(\overline{z_1}+\overline{z_2})+(z_1-z_2)(\overline{z_1}-\overline{z_2})\\ 	&=z_1\overline{z_1}+z_2\overline{z_2}+z_1\overline{z_2}+\overline{z_1}z_2+z_1\overline{z_1}+z_2\overline{z_2}-z_1\overline{z_2}-\overline{z_1}z_2\\ 	&=2z_1\overline{z_1}+2z_2\overline{z_2}\\ &=2|z_1|^2+2|z_2|^2\\ 	&=2(|z_1|^2+|z_2|^2). \end{align*} However, I am having trouble ""interpreting this result geometrically""? What does this mean?","For $z_1, z_2 \in \mathbb C$ prove that $|z_1+z_2|^2+|z_1-z_2|^2=2(|z_1|^2+|z_2|^2)$, and interpret this result geometrically. We have \begin{align*} 	|z_1+z_2|^2+|z_1-z_2|^2&=(z_1+z_2)(\overline{z_1+z_2})+(z_1-z_2)(\overline{z_1-z_2})\\ 	&=(z_1+z_2)(\overline{z_1}+\overline{z_2})+(z_1-z_2)(\overline{z_1}-\overline{z_2})\\ 	&=z_1\overline{z_1}+z_2\overline{z_2}+z_1\overline{z_2}+\overline{z_1}z_2+z_1\overline{z_1}+z_2\overline{z_2}-z_1\overline{z_2}-\overline{z_1}z_2\\ 	&=2z_1\overline{z_1}+2z_2\overline{z_2}\\ &=2|z_1|^2+2|z_2|^2\\ 	&=2(|z_1|^2+|z_2|^2). \end{align*} However, I am having trouble ""interpreting this result geometrically""? What does this mean?",,"['complex-analysis', 'complex-numbers', 'complex-geometry']"
27,Calculating the Laurent Series for complicated functions,Calculating the Laurent Series for complicated functions,,"I would appreciate a nudge in the right direction for the following questions, as I haven't really had any exposure to finding Laurent Series expansions for more complicated functions Find the Laurent Series expansion for: $$1)\frac{z-7}{z^2+z-2}$$    $ 1<|z|<2$ , $|z|>2$ ,  $0<|z-1|<1$ For this question I tried to follow the standard route for this question by using partial fraction decomposition and then attempting to put each fraction in the form of $\frac{1}{1-z}$ but I feel like I'm having an issue when I come to this stage: By partial fraction decomposition we have: $$\frac{z-7}{(z-1)(z+2)}=\frac{3}{z+2} - \frac{2}{z-1}$$ And attempting to put $\frac{3}{z+2}$ and $-\frac{2}{z-1}$ in the form $\frac{1}{1-z}$ has so far yielded an embrassing $$\frac{3}{z+2}=\frac{3}{2+z}=\frac{3}{2}\frac{1}{1+\frac{z}{2}}$$ At this point I think it's as simple as doing some algebraic manipulation but I can't even seem to do this? And once I've done that is this still just a simple ""plug and play"" type scenario with Taylor Series? $$2)\frac{1-cosz}{(z-2\pi)^3}$$ I don't even know where to start for this. I thought perhaps partial fraction decomposition again but I don't see how this would help (if even possible) since we would still struggle to get the form of $\frac{1}{1-z}$ $3)$Let $f(z)= \sqrt {z^2-3z+2}$, $1<|z|<2$. Does the Laurent series expansion for $f(z)$ exist? Justify your answer. This is completely beyond me as I can't imagine where I would begin to check if the expansion exists. I'm only used to (as you can probably tell) calculating Laurent Series of the form $\frac{1}{1-z}$ Additionally I'm not entirely sure how the conditions i.e $1<|z|<2$ etc affect the question or how the Laurent Series expansion is calculated, so if someone could briefly explain that I'd appreciate it. Thanks","I would appreciate a nudge in the right direction for the following questions, as I haven't really had any exposure to finding Laurent Series expansions for more complicated functions Find the Laurent Series expansion for: $$1)\frac{z-7}{z^2+z-2}$$    $ 1<|z|<2$ , $|z|>2$ ,  $0<|z-1|<1$ For this question I tried to follow the standard route for this question by using partial fraction decomposition and then attempting to put each fraction in the form of $\frac{1}{1-z}$ but I feel like I'm having an issue when I come to this stage: By partial fraction decomposition we have: $$\frac{z-7}{(z-1)(z+2)}=\frac{3}{z+2} - \frac{2}{z-1}$$ And attempting to put $\frac{3}{z+2}$ and $-\frac{2}{z-1}$ in the form $\frac{1}{1-z}$ has so far yielded an embrassing $$\frac{3}{z+2}=\frac{3}{2+z}=\frac{3}{2}\frac{1}{1+\frac{z}{2}}$$ At this point I think it's as simple as doing some algebraic manipulation but I can't even seem to do this? And once I've done that is this still just a simple ""plug and play"" type scenario with Taylor Series? $$2)\frac{1-cosz}{(z-2\pi)^3}$$ I don't even know where to start for this. I thought perhaps partial fraction decomposition again but I don't see how this would help (if even possible) since we would still struggle to get the form of $\frac{1}{1-z}$ $3)$Let $f(z)= \sqrt {z^2-3z+2}$, $1<|z|<2$. Does the Laurent series expansion for $f(z)$ exist? Justify your answer. This is completely beyond me as I can't imagine where I would begin to check if the expansion exists. I'm only used to (as you can probably tell) calculating Laurent Series of the form $\frac{1}{1-z}$ Additionally I'm not entirely sure how the conditions i.e $1<|z|<2$ etc affect the question or how the Laurent Series expansion is calculated, so if someone could briefly explain that I'd appreciate it. Thanks",,"['complex-analysis', 'laurent-series']"
28,"Showing that the Gamma Function has Poles by ""staying close"" to its integral identity $\Gamma(z) = \int_0^\infty x^{z-1}e^{-x}dx$","Showing that the Gamma Function has Poles by ""staying close"" to its integral identity",\Gamma(z) = \int_0^\infty x^{z-1}e^{-x}dx,"Consider the canonical definition of the gamma function on $\mathbb{C}$: $$\Gamma(z) = \int_0^\infty x^{z-1}e^{-x}dx$$ Problem: How can one show that $\Gamma$ has poles for $z=0, -1, \ldots$? One proof I've seen uses various facts about the gamma function to show that: $$\Gamma(z) = {\Gamma(z + n + 1) \over z(z+1) \ldots (z +n)}$$ This clearly shows the result in question; however, this latter representation of the gamma function is ""far away"" from the former, canonical representation. Question: Is there a way to show that $\Gamma$ has poles by sticking close to its canonical representation (i.e., by using facts about integrals and so forth)?","Consider the canonical definition of the gamma function on $\mathbb{C}$: $$\Gamma(z) = \int_0^\infty x^{z-1}e^{-x}dx$$ Problem: How can one show that $\Gamma$ has poles for $z=0, -1, \ldots$? One proof I've seen uses various facts about the gamma function to show that: $$\Gamma(z) = {\Gamma(z + n + 1) \over z(z+1) \ldots (z +n)}$$ This clearly shows the result in question; however, this latter representation of the gamma function is ""far away"" from the former, canonical representation. Question: Is there a way to show that $\Gamma$ has poles by sticking close to its canonical representation (i.e., by using facts about integrals and so forth)?",,"['complex-analysis', 'gamma-function']"
29,Number of roots of $z^4+z^3+4z^2+2z+3=0$ in each quadrants,Number of roots of  in each quadrants,z^4+z^3+4z^2+2z+3=0,"This question has been asked but I am stuck with my method. I have shown that the roots can not be on the real and imaginary axis. Since the coefficients are real and thus the roots must be in conjugate pairs. So If there is no root in first quadrant (or right half plane), then we have 2 roots in second quadrant and third quadrant respectively. (this is the answer to this question) So I am going to show that there is no root in right half plane. Consider the integral along the semi-circle in right half plane with radius $R$  $$\int_{C}\dfrac{f'}{f}dz=\int_{-Ri}^{Ri}+\int_{-\pi/2}^{\pi/2}\dfrac{4z^3+3z^2+8z+2}{z^4+z^3+4z^2+2z+3}dz$$ For the second part, $$\int_{-\pi/2}^{\pi/2}\dfrac{4z^3+3z^2+8z+2}{z^4+z^3+4z^2+2z+3}dz=i\int_{-\pi/2}^{\pi/2}\dfrac{4z^4+3z^3+8z^2+2z}{z^4+z^3+4z^2+2z+3}d\theta=4\pi i$$ as $R\rightarrow \infty$. For the first part, \begin{align*} &\int_{-Ri}^{Ri}\dfrac{4z^3+3z^2+8z+2}{z^4+z^3+4z^2+2z+3}dz\\ &=\int^R_{-R}\dfrac{3y^2-2+i(4y^3-8y)}{y^4-4y^2+3-i(y^3+2y)}idy\\ \\ &=\int^R_{-R}\dfrac{(3y^2-3)(y^4-4y^2+3)-(4y^3-8y)(y^3-2y)+i[(4y^3-8y)(y^4-4y^2+3)+(y^3-2y)(3y^2-2)]}{(y^4-4y^2+3)^2+(y^3+2y)^2}idy\\ &=\int^R_{-R} i(even \,\,part)-(odd \,\,part) dy\\ &=\int^R_{-R} i(even \,\,part) \,\,dy \end{align*} It is expected that $\int^R_{-R} i(even \,\,part) \,\,dy=-4\pi i$, then we are done, but how? Also is there any method using Rouche theorem?","This question has been asked but I am stuck with my method. I have shown that the roots can not be on the real and imaginary axis. Since the coefficients are real and thus the roots must be in conjugate pairs. So If there is no root in first quadrant (or right half plane), then we have 2 roots in second quadrant and third quadrant respectively. (this is the answer to this question) So I am going to show that there is no root in right half plane. Consider the integral along the semi-circle in right half plane with radius $R$  $$\int_{C}\dfrac{f'}{f}dz=\int_{-Ri}^{Ri}+\int_{-\pi/2}^{\pi/2}\dfrac{4z^3+3z^2+8z+2}{z^4+z^3+4z^2+2z+3}dz$$ For the second part, $$\int_{-\pi/2}^{\pi/2}\dfrac{4z^3+3z^2+8z+2}{z^4+z^3+4z^2+2z+3}dz=i\int_{-\pi/2}^{\pi/2}\dfrac{4z^4+3z^3+8z^2+2z}{z^4+z^3+4z^2+2z+3}d\theta=4\pi i$$ as $R\rightarrow \infty$. For the first part, \begin{align*} &\int_{-Ri}^{Ri}\dfrac{4z^3+3z^2+8z+2}{z^4+z^3+4z^2+2z+3}dz\\ &=\int^R_{-R}\dfrac{3y^2-2+i(4y^3-8y)}{y^4-4y^2+3-i(y^3+2y)}idy\\ \\ &=\int^R_{-R}\dfrac{(3y^2-3)(y^4-4y^2+3)-(4y^3-8y)(y^3-2y)+i[(4y^3-8y)(y^4-4y^2+3)+(y^3-2y)(3y^2-2)]}{(y^4-4y^2+3)^2+(y^3+2y)^2}idy\\ &=\int^R_{-R} i(even \,\,part)-(odd \,\,part) dy\\ &=\int^R_{-R} i(even \,\,part) \,\,dy \end{align*} It is expected that $\int^R_{-R} i(even \,\,part) \,\,dy=-4\pi i$, then we are done, but how? Also is there any method using Rouche theorem?",,"['complex-analysis', 'roots']"
30,Complex power series property with real numbers,Complex power series property with real numbers,,"I am trying to prove the following statement, seems clear to me but I am not able to give a formal proof. I have tried using the binomial expansion and Taylor's theorem for complex numbers. Let $f(z)= \sum\limits_{n=0}^\infty c_nz^n$, $z \in \mathbb{C}$  be a power series with radius of convergence $R_1=\infty$. Prove that $f(z-c), c \in \mathbb{R}$ can also be written as a power series $f(z-c)=\sum\limits_{n=0}^\infty b_nz^n$ with radius of convergence $R_2=\infty$. Thanks in advance!","I am trying to prove the following statement, seems clear to me but I am not able to give a formal proof. I have tried using the binomial expansion and Taylor's theorem for complex numbers. Let $f(z)= \sum\limits_{n=0}^\infty c_nz^n$, $z \in \mathbb{C}$  be a power series with radius of convergence $R_1=\infty$. Prove that $f(z-c), c \in \mathbb{R}$ can also be written as a power series $f(z-c)=\sum\limits_{n=0}^\infty b_nz^n$ with radius of convergence $R_2=\infty$. Thanks in advance!",,"['complex-analysis', 'convergence-divergence', 'complex-numbers', 'power-series']"
31,How does this follows from the maximum principle?,How does this follows from the maximum principle?,,Suppose $u$ is a holomorphic function in the unit disk and $\lvert u(z)\rvert\leq 1$. It follows from Cauchy's inequality that $$ \left\lvert u(z) - \sum_0^{2N-1}\frac{u^{(k)}(0)z^k}{k!}\right\rvert \leq (2N+1) $$ The author claims that from the maximal principle we obtain   $$ \left\lvert u(z) - \sum_0^{2N-1}\frac{u^{(k)}(0)z^k}{k!}\right\rvert \leq (2N+1)\lvert z\rvert^{2N} $$   How? I don't see how it follows. If anyone can show me how to obtain this from the maximum modulus principle I would appreciate it.,Suppose $u$ is a holomorphic function in the unit disk and $\lvert u(z)\rvert\leq 1$. It follows from Cauchy's inequality that $$ \left\lvert u(z) - \sum_0^{2N-1}\frac{u^{(k)}(0)z^k}{k!}\right\rvert \leq (2N+1) $$ The author claims that from the maximal principle we obtain   $$ \left\lvert u(z) - \sum_0^{2N-1}\frac{u^{(k)}(0)z^k}{k!}\right\rvert \leq (2N+1)\lvert z\rvert^{2N} $$   How? I don't see how it follows. If anyone can show me how to obtain this from the maximum modulus principle I would appreciate it.,,"['complex-analysis', 'maximum-principle']"
32,"Does it make sense to talk about ""frequency"" when expanding a function using spherical harmonics?","Does it make sense to talk about ""frequency"" when expanding a function using spherical harmonics?",,"If I had a function defined in the sphere and I expand it using spherical harmonics, such as $$ f(\theta,\phi) = \sum_{l=0}^{+\infty} \sum_{m=-l}^{l} c_{lm}Y_l^m(\theta,\phi) $$ does it make any sense to talk about ""low frequency"" and ""high frequency""? If yes, what is usually meant by that? My analogy is with the Fourier analysis where low frequency is usually associated wit the first terms of a series. I think something similar can be said here, is it correct?","If I had a function defined in the sphere and I expand it using spherical harmonics, such as $$ f(\theta,\phi) = \sum_{l=0}^{+\infty} \sum_{m=-l}^{l} c_{lm}Y_l^m(\theta,\phi) $$ does it make any sense to talk about ""low frequency"" and ""high frequency""? If yes, what is usually meant by that? My analogy is with the Fourier analysis where low frequency is usually associated wit the first terms of a series. I think something similar can be said here, is it correct?",,"['real-analysis', 'complex-analysis', 'fourier-analysis', 'spherical-harmonics']"
33,Determine the value of $p(1) + q(1)$,Determine the value of,p(1) + q(1),"Let $p(z)$ and $q(z)$ ($z$ here is a complex number)  both as polynomial so that $$p(z) \sin^2 z + q(z) \cos^2 z = 2. \quad \forall z \in \mathbb{C}$$ Determine the value of $p(1) + q(1)$. My attempt: First I tried to manipulate the equation by using trigonometric identity to make $\tan z$ appear in it, but seems lead no result. Factorizing also gives no result, I think. So, do you have any idea? Please, help.","Let $p(z)$ and $q(z)$ ($z$ here is a complex number)  both as polynomial so that $$p(z) \sin^2 z + q(z) \cos^2 z = 2. \quad \forall z \in \mathbb{C}$$ Determine the value of $p(1) + q(1)$. My attempt: First I tried to manipulate the equation by using trigonometric identity to make $\tan z$ appear in it, but seems lead no result. Factorizing also gives no result, I think. So, do you have any idea? Please, help.",,"['complex-analysis', 'trigonometry']"
34,Is there an entire function satisfying $|f(z)|=|z|+1$ for every $|z|\geq2017$? [duplicate],Is there an entire function satisfying  for every ? [duplicate],|f(z)|=|z|+1 |z|\geq2017,"This question already has an answer here : Does there exists an entire function such that for each $|z|>C$ for some constant $C$, $|f(z)|=|z|+1$? (1 answer) Closed 6 years ago . Is there a holomorphic function $f \in Hol(\Bbb C)$ such that $|f(z)|=|z|+1$ for every $|z|\geq2017$ ? I tried defining $g(z)=\frac{1}{f(z)}$. Such $g$ is clearly holomorphic and bounded for $|z| \geq 2017$. I want to show that $g$ is holomorphic in $|z| \leq 2017$ and then conclude that $g$ is constant. For that I want to show that $f$ has no zeros in $|z| \leq 2017$. I tried to prove it by Rouche theorem, but didn't succeed. Any ideas? Edit: the post similar to mine doesn't solve my problem, because the ending of the proof is not clear. Where is the contradiction?","This question already has an answer here : Does there exists an entire function such that for each $|z|>C$ for some constant $C$, $|f(z)|=|z|+1$? (1 answer) Closed 6 years ago . Is there a holomorphic function $f \in Hol(\Bbb C)$ such that $|f(z)|=|z|+1$ for every $|z|\geq2017$ ? I tried defining $g(z)=\frac{1}{f(z)}$. Such $g$ is clearly holomorphic and bounded for $|z| \geq 2017$. I want to show that $g$ is holomorphic in $|z| \leq 2017$ and then conclude that $g$ is constant. For that I want to show that $f$ has no zeros in $|z| \leq 2017$. I tried to prove it by Rouche theorem, but didn't succeed. Any ideas? Edit: the post similar to mine doesn't solve my problem, because the ending of the proof is not clear. Where is the contradiction?",,"['complex-analysis', 'holomorphic-functions', 'entire-functions']"
35,Existence of partial derivatives & Cauchy-Riemann does not imply differentiability example,Existence of partial derivatives & Cauchy-Riemann does not imply differentiability example,,"I learned about the Cauchy-Riemann equations today, and my instructor used the following example to show that differentiability is not guaranteed if the partial derivatives are not continuous. Let $$ f(z)=f(x+iy)= \begin{cases} \frac{xy(x+iy)}{x^2+y^2},&\quad\mbox{$z\neq 0$}\\ 0+0i,&\quad\mbox{$z=0$}. \end{cases} $$ Similar to the example here , writing $f(z)=f(x+iy)=u(x,y)+iv(x,y)$, we can show that $u_x(0,0)=0$ and $v_y(0,0)=0$ since the partial derivatives are $0$ on the axes, but $f$ is not continuous and thus not differentiable at $(0,0)$ since the limits are different if we approach $(0,0)$ along the imaginary axis or along $x=y$. My instructor mentioned that reason why in the above example, differentiability fails even though the partial derivatives exist and satisfy the Cauchy-Riemann equations, is that the partial derivatives are not continuous at $(0,0)$ (there is a similar comment in this document ). I wrote out the partial derivatives \begin{align*} u_x &= \left(\frac{x^2y}{x^2+y^2}\right)'=\frac{2xy(x^2+y^2)-x^2y(2x)}{(x^2+y^2)^2}=\frac{2xy^3}{(x^2+y^2)^2}\\ v_y &= \left(\frac{xy^2}{x^2+y^2}\right)'=\frac{2yx^3}{(x^2+y^2)^2}. \end{align*} E.g., consider $u_x$ - if I approach $(0,0)$ from the real axis, the limit of $u_x$ is $0$; if I approach $(0,0)$ from the line $x=y$, the limit of $u_x$ would be $1/2$. I thought this is why $u_x$ is not continuous at $(0,0)$. But looking at the formulas alone, it seems that the partial derivatives are simply not defined at $(0,0)$ because the denominators would be $0$ (if so, then this would already imply the partial derivatives are not continuous at $(0,0)$ and we wouldn't even need the argument using unequal limits). But we have just shown before that the partial derivatives exist at $(0,0)$. I must be missing something. According to a post here , it may have something to do with the definition of $f$ at $(0,0)$. Any help is much appreciated!","I learned about the Cauchy-Riemann equations today, and my instructor used the following example to show that differentiability is not guaranteed if the partial derivatives are not continuous. Let $$ f(z)=f(x+iy)= \begin{cases} \frac{xy(x+iy)}{x^2+y^2},&\quad\mbox{$z\neq 0$}\\ 0+0i,&\quad\mbox{$z=0$}. \end{cases} $$ Similar to the example here , writing $f(z)=f(x+iy)=u(x,y)+iv(x,y)$, we can show that $u_x(0,0)=0$ and $v_y(0,0)=0$ since the partial derivatives are $0$ on the axes, but $f$ is not continuous and thus not differentiable at $(0,0)$ since the limits are different if we approach $(0,0)$ along the imaginary axis or along $x=y$. My instructor mentioned that reason why in the above example, differentiability fails even though the partial derivatives exist and satisfy the Cauchy-Riemann equations, is that the partial derivatives are not continuous at $(0,0)$ (there is a similar comment in this document ). I wrote out the partial derivatives \begin{align*} u_x &= \left(\frac{x^2y}{x^2+y^2}\right)'=\frac{2xy(x^2+y^2)-x^2y(2x)}{(x^2+y^2)^2}=\frac{2xy^3}{(x^2+y^2)^2}\\ v_y &= \left(\frac{xy^2}{x^2+y^2}\right)'=\frac{2yx^3}{(x^2+y^2)^2}. \end{align*} E.g., consider $u_x$ - if I approach $(0,0)$ from the real axis, the limit of $u_x$ is $0$; if I approach $(0,0)$ from the line $x=y$, the limit of $u_x$ would be $1/2$. I thought this is why $u_x$ is not continuous at $(0,0)$. But looking at the formulas alone, it seems that the partial derivatives are simply not defined at $(0,0)$ because the denominators would be $0$ (if so, then this would already imply the partial derivatives are not continuous at $(0,0)$ and we wouldn't even need the argument using unequal limits). But we have just shown before that the partial derivatives exist at $(0,0)$. I must be missing something. According to a post here , it may have something to do with the definition of $f$ at $(0,0)$. Any help is much appreciated!",,"['complex-analysis', 'derivatives', 'continuity', 'partial-derivative']"
36,"Evaluate the integral $ I=\frac{1}{2\pi i}\int_{\vert z \vert =R}(z-3)\sin \left(\frac{1}{z+2}\right)dz$,","Evaluate the integral ,", I=\frac{1}{2\pi i}\int_{\vert z \vert =R}(z-3)\sin \left(\frac{1}{z+2}\right)dz,"Evaluate the integral $$ I=\frac{1}{2\pi i}\int_{\vert z \vert =R}(z-3)\sin\left(\frac{1}{z+2}\right)dz$$ where $R \geq 4$ My work: Here the zeros of $\sin\left(\frac{1}{z+2}\right)$ is $\frac{1}{n \pi}-2$, so $-2$ is non-isolated singularity. But after I am stuck, Please help. Thanks.","Evaluate the integral $$ I=\frac{1}{2\pi i}\int_{\vert z \vert =R}(z-3)\sin\left(\frac{1}{z+2}\right)dz$$ where $R \geq 4$ My work: Here the zeros of $\sin\left(\frac{1}{z+2}\right)$ is $\frac{1}{n \pi}-2$, so $-2$ is non-isolated singularity. But after I am stuck, Please help. Thanks.",,"['complex-analysis', 'complex-integration']"
37,Contour integration with absolute value,Contour integration with absolute value,,"I am trying to use contour integration to evaluate the integral $$ \int_\mathbb{R}\frac{|x|e^{i\omega x}}{x^2+ a}dx \,, $$ where $a>0$ and $\omega$ can take any real value. I am worried about the existence of the absolute value and I am not sure how to proceed. Any help appreciated. EDIT: Contour integration might not be the way to go here (see comments). In any case, it is easy to see that the integral in question is also equal to $$ 2\int_0^\infty \frac{x \cos(\omega x)}{x^2+ a}dx \,, $$ and I am wondering whether anyone can produce a solution regardless of the method used.","I am trying to use contour integration to evaluate the integral $$ \int_\mathbb{R}\frac{|x|e^{i\omega x}}{x^2+ a}dx \,, $$ where $a>0$ and $\omega$ can take any real value. I am worried about the existence of the absolute value and I am not sure how to proceed. Any help appreciated. EDIT: Contour integration might not be the way to go here (see comments). In any case, it is easy to see that the integral in question is also equal to $$ 2\int_0^\infty \frac{x \cos(\omega x)}{x^2+ a}dx \,, $$ and I am wondering whether anyone can produce a solution regardless of the method used.",,"['integration', 'complex-analysis', 'improper-integrals', 'contour-integration', 'complex-integration']"
38,Slick proof that $x^a(1-x)^{r-a}$ are linearly independent?,Slick proof that  are linearly independent?,x^a(1-x)^{r-a},"I'm interested in showing that for every real number $r>0$, the set of functions $$\{f_a : (0,1) \to \mathbb R \,|\, a \in (0,r)\}$$ is linearly independent where $$f_a(x) = x^a(1-x)^{r-a}.$$ The proposition can be shown for $a,r \in \mathbb N$ by the binomial theorem. For real parameters, my approach was to evaluate the Wronskian at $x=\frac 1 2$ (by symmetry, this is sort of the canonical value to try, e.g. all $f_a$ agree there). Some calculations suggest that $$ W(f_{a_1},\ldots,f_{a_n})\left(\frac 1 2\right)  = \left(\frac 1 2\right)^{nr - n(n-1)} \cdot V(a_1,\ldots,a_n)$$ where $V(a_1,\ldots,a_n)$ is the Vandermonde determinant. This would indeed show the proposition and I suppose I could just brute-force my way through the proof with determinant expansions. I am wondering though if there is a more elegant proof. I am a bit reminded of the orthogonality relations behind Fourier or Laplace transforms. A generalization of the linear independence question would be to consider the transform $$\{Bw\}(s) = \int_0^1 w(s)x^s(1-x)^{r-s}\mathrm dx$$ Is anything known about that? Anyway, I suspect that a suitable integration in the complex plane might show the proposition. Or I am missing something entirely elementary about the $f_a$ and some factorization/limit argument is enough as in the case of the power functions $x^a$.","I'm interested in showing that for every real number $r>0$, the set of functions $$\{f_a : (0,1) \to \mathbb R \,|\, a \in (0,r)\}$$ is linearly independent where $$f_a(x) = x^a(1-x)^{r-a}.$$ The proposition can be shown for $a,r \in \mathbb N$ by the binomial theorem. For real parameters, my approach was to evaluate the Wronskian at $x=\frac 1 2$ (by symmetry, this is sort of the canonical value to try, e.g. all $f_a$ agree there). Some calculations suggest that $$ W(f_{a_1},\ldots,f_{a_n})\left(\frac 1 2\right)  = \left(\frac 1 2\right)^{nr - n(n-1)} \cdot V(a_1,\ldots,a_n)$$ where $V(a_1,\ldots,a_n)$ is the Vandermonde determinant. This would indeed show the proposition and I suppose I could just brute-force my way through the proof with determinant expansions. I am wondering though if there is a more elegant proof. I am a bit reminded of the orthogonality relations behind Fourier or Laplace transforms. A generalization of the linear independence question would be to consider the transform $$\{Bw\}(s) = \int_0^1 w(s)x^s(1-x)^{r-s}\mathrm dx$$ Is anything known about that? Anyway, I suspect that a suitable integration in the complex plane might show the proposition. Or I am missing something entirely elementary about the $f_a$ and some factorization/limit argument is enough as in the case of the power functions $x^a$.",,"['real-analysis', 'complex-analysis']"
39,$f(z)=0$ for all $z \in \mathbb{R}$,for all,f(z)=0 z \in \mathbb{R},Prove that there is no non-constant analytic function $f$ on $\mathbb{C}$ such that $f(z)=0$ for all $z \in \mathbb{R}$ . Doesn't this follow immediately by the identity theorem? There is a sequence along the real axis that converges to 0... Attempt Suppose $f$ is entire and $f(z)=0$ for all real numbers. Now let $E=\mathbb{D}$ . Since the set of points of $f=0$ has a limit point in $E$ we conclude $f$ is identically 0.,Prove that there is no non-constant analytic function on such that for all . Doesn't this follow immediately by the identity theorem? There is a sequence along the real axis that converges to 0... Attempt Suppose is entire and for all real numbers. Now let . Since the set of points of has a limit point in we conclude is identically 0.,f \mathbb{C} f(z)=0 z \in \mathbb{R} f f(z)=0 E=\mathbb{D} f=0 E f,"['complex-analysis', 'analytic-functions']"
40,Optimizing a function,Optimizing a function,,"I'm optimizing the sum rate of two users in a communication system. The problem can be formulated as follows: $\begin{array}{l} \mathop {\max }\limits_{\alpha ,\rho } {\rm{    }}\left( {1 + \frac{{\left( {1 - \rho } \right)\alpha P{g_1}}}{{\left( {1 - \rho  + \mu } \right){N_0}}}} \right)\left( {1 + \frac{{\left( {1 - \alpha } \right)P{g_2}}}{{\alpha P{g_2} + \left( {1 + \mu } \right){N_0}}} + \frac{{\rho \eta P{g_1}{g_3}}}{{\left( {1 + \mu } \right){N_0}}}} \right)\\ \text{subject to}\\ \;\;\;\;\;\;\;\;\;\;\;\;\;\;\; \frac{{\left( {1 - \rho } \right)\left( {1 - \alpha } \right)P{g_1}}}{{\left( {1 - \rho } \right)\alpha P{g_1} + \left( {1 - \rho  + \mu } \right){N_0}}} \ge {T_2} \;\;\;\;\;\;\;\;\;\;\;\; (C1) \\ \;\;\;\;\;\;\;\;\;\;\;\;\;\;\; \frac{{\left( {1 - \rho } \right)\alpha P{g_1}}}{{\left( {1 - \rho  + \mu } \right){N_0}}} \ge {T_1} \;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\; (C2) \\ \;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\ \frac{{\left( {1 - \alpha } \right)P{g_2}}}{{\alpha P{g_2} + \left( {1 + \mu } \right){N_0}}} + \frac{{\rho \eta P{g_1}{g_3}}}{{\left( {1 + \mu } \right){N_0}}} \ge {T_2} \;\;\;\;\;\;\; (C3)\\ \;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\ 0 \le \alpha  \le 0.5 \;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\; (C4)\\ \;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\ 0 \le \rho  \le 1 \;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\; (C5) \end{array}$ where $P$ is transmitted power, $N_0$ is noise power, ${g_i} = {\left| {{h_i}} \right|^2}$ with $h_i$ is the channel gain, $\mu$, $T_1$, $T_2$ are constants. Without the constraints (C1)-(C3), we can prove that the objective function is maximized at $\alpha = 0$ or 0.5. Then, at $\alpha = 0$, $\rho$ will be 1. In addition, when $\alpha = 0.5$, the objective is a quadratic function, and thus, we can also find the solution analytically. However, with the constraints (C1)-(C3), $\alpha$ and $\rho$ are coupled with each other in new constraints. This makes the problem much more difficult, and we are struggling to solve it. Do you have any suggestions? Thank you very much.","I'm optimizing the sum rate of two users in a communication system. The problem can be formulated as follows: $\begin{array}{l} \mathop {\max }\limits_{\alpha ,\rho } {\rm{    }}\left( {1 + \frac{{\left( {1 - \rho } \right)\alpha P{g_1}}}{{\left( {1 - \rho  + \mu } \right){N_0}}}} \right)\left( {1 + \frac{{\left( {1 - \alpha } \right)P{g_2}}}{{\alpha P{g_2} + \left( {1 + \mu } \right){N_0}}} + \frac{{\rho \eta P{g_1}{g_3}}}{{\left( {1 + \mu } \right){N_0}}}} \right)\\ \text{subject to}\\ \;\;\;\;\;\;\;\;\;\;\;\;\;\;\; \frac{{\left( {1 - \rho } \right)\left( {1 - \alpha } \right)P{g_1}}}{{\left( {1 - \rho } \right)\alpha P{g_1} + \left( {1 - \rho  + \mu } \right){N_0}}} \ge {T_2} \;\;\;\;\;\;\;\;\;\;\;\; (C1) \\ \;\;\;\;\;\;\;\;\;\;\;\;\;\;\; \frac{{\left( {1 - \rho } \right)\alpha P{g_1}}}{{\left( {1 - \rho  + \mu } \right){N_0}}} \ge {T_1} \;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\; (C2) \\ \;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\ \frac{{\left( {1 - \alpha } \right)P{g_2}}}{{\alpha P{g_2} + \left( {1 + \mu } \right){N_0}}} + \frac{{\rho \eta P{g_1}{g_3}}}{{\left( {1 + \mu } \right){N_0}}} \ge {T_2} \;\;\;\;\;\;\; (C3)\\ \;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\ 0 \le \alpha  \le 0.5 \;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\; (C4)\\ \;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\ 0 \le \rho  \le 1 \;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\; (C5) \end{array}$ where $P$ is transmitted power, $N_0$ is noise power, ${g_i} = {\left| {{h_i}} \right|^2}$ with $h_i$ is the channel gain, $\mu$, $T_1$, $T_2$ are constants. Without the constraints (C1)-(C3), we can prove that the objective function is maximized at $\alpha = 0$ or 0.5. Then, at $\alpha = 0$, $\rho$ will be 1. In addition, when $\alpha = 0.5$, the objective is a quadratic function, and thus, we can also find the solution analytically. However, with the constraints (C1)-(C3), $\alpha$ and $\rho$ are coupled with each other in new constraints. This makes the problem much more difficult, and we are struggling to solve it. Do you have any suggestions? Thank you very much.",,"['complex-analysis', 'optimization', 'nonlinear-optimization']"
41,Question about complex analysis; writing $f(iy) = u(y) + iv(y)$,Question about complex analysis; writing,f(iy) = u(y) + iv(y),"Let $f(z) = z^n + a_{n-1}z^{n-1} + ... $ be a monic polynomial of   degree $n$, and assume that $f(iy) \ne 0$ for all $y \in \mathbb R$.   Write $f(iy) = u(y) + iv(y)$ and express the number of roots of $f$ in   the right half plane {$\Re(z)> 0$} in terms of the number and mutual   position of the real roots of $u$ and $v$. Now we have the first constant is equal to 1 and I searched and got the idea that it is always possible to write $f(z)=u(x,y)+iv(x,y)$ in here . But I do not know where to start in this question. Any help is appreciated!","Let $f(z) = z^n + a_{n-1}z^{n-1} + ... $ be a monic polynomial of   degree $n$, and assume that $f(iy) \ne 0$ for all $y \in \mathbb R$.   Write $f(iy) = u(y) + iv(y)$ and express the number of roots of $f$ in   the right half plane {$\Re(z)> 0$} in terms of the number and mutual   position of the real roots of $u$ and $v$. Now we have the first constant is equal to 1 and I searched and got the idea that it is always possible to write $f(z)=u(x,y)+iv(x,y)$ in here . But I do not know where to start in this question. Any help is appreciated!",,"['complex-analysis', 'polynomials']"
42,Compute the Integral via Residue Theorem and Laurent Expansion,Compute the Integral via Residue Theorem and Laurent Expansion,,"Let $f(z)=\dfrac{z^{15}}{(z^2+1)^2(z^4+2)^3}$ . My goal is to compute $\displaystyle I=\int_{|z|=4}f(z) \,dz$. Since all the singular points (except $∞$) lie in the circle $|z|<4$, we obtain $I=-2\pi i\operatorname{Res}(f,∞)$. Thus, we just need to find the Laurent coefficient $a_{-1}$ at $∞$. $f\left(\dfrac 1 w\right) = \dfrac w {(1+w^2)^2(1+2w^4)^3}$. Any convenient ways to find $a_{-1}$?","Let $f(z)=\dfrac{z^{15}}{(z^2+1)^2(z^4+2)^3}$ . My goal is to compute $\displaystyle I=\int_{|z|=4}f(z) \,dz$. Since all the singular points (except $∞$) lie in the circle $|z|<4$, we obtain $I=-2\pi i\operatorname{Res}(f,∞)$. Thus, we just need to find the Laurent coefficient $a_{-1}$ at $∞$. $f\left(\dfrac 1 w\right) = \dfrac w {(1+w^2)^2(1+2w^4)^3}$. Any convenient ways to find $a_{-1}$?",,"['complex-analysis', 'laurent-series']"
43,Please help me calculate a integral in complex analysis.,Please help me calculate a integral in complex analysis.,,"EXERCISE: Let $C_p$ is a semi-circle $\{z\in \mathbb{C}:\left| {z - 1} \right| = p,\operatorname{Im}(z)>0\}$, described in the counterclockwise direction. Prove that: $$I=\mathop {\lim }\limits_{p \to {0^ + }} \int\limits_{{C_p}} {\left( {\frac{1}{{z - 1}} + \frac{{{e^z}}}{{z + 1}}} \right)d} z = \pi i$$ if $C_p$ is a circle $z\in \mathbb{C}:\left| {z - 1} \right| = p$, I can prove that $I=\operatorname{Res}(f,1)=2\pi i$. But $C_p$ is a semi-circle, can some one help me?","EXERCISE: Let $C_p$ is a semi-circle $\{z\in \mathbb{C}:\left| {z - 1} \right| = p,\operatorname{Im}(z)>0\}$, described in the counterclockwise direction. Prove that: $$I=\mathop {\lim }\limits_{p \to {0^ + }} \int\limits_{{C_p}} {\left( {\frac{1}{{z - 1}} + \frac{{{e^z}}}{{z + 1}}} \right)d} z = \pi i$$ if $C_p$ is a circle $z\in \mathbb{C}:\left| {z - 1} \right| = p$, I can prove that $I=\operatorname{Res}(f,1)=2\pi i$. But $C_p$ is a semi-circle, can some one help me?",,"['complex-analysis', 'complex-integration']"
44,Is there a natural way to prove trig identities also hold for complex numbers?,Is there a natural way to prove trig identities also hold for complex numbers?,,"In complex analysis trig functions are defined via $\exp$ which in turn is defined via power series. It's of course easy to see that on $\Bbb R$, all these functions agree with their usual real-variable versions that we are well familiar with. In the real case there are some basic identities like $\cos$ is even, $\sin$ is odd and $\cos(x)=\sin(\frac\pi 2 -x)$ etc. There are also more complicated ones like  $$\cos(x-y)=\cos x\cos y+\sin x\sin y.\tag{1}$$ Q1: do real variable identities like $(1)$ also hold when $x,y$ are complex numbers? (Of course, identities involving square roots are not included here. We basically only care about $\cos(x\pm y), \sin(x\pm y)$ etc. Q2: if so, is there any natural (or slick) way to see this, other than going down to the basic $\exp$ definition and calculating term by term? Thanks!","In complex analysis trig functions are defined via $\exp$ which in turn is defined via power series. It's of course easy to see that on $\Bbb R$, all these functions agree with their usual real-variable versions that we are well familiar with. In the real case there are some basic identities like $\cos$ is even, $\sin$ is odd and $\cos(x)=\sin(\frac\pi 2 -x)$ etc. There are also more complicated ones like  $$\cos(x-y)=\cos x\cos y+\sin x\sin y.\tag{1}$$ Q1: do real variable identities like $(1)$ also hold when $x,y$ are complex numbers? (Of course, identities involving square roots are not included here. We basically only care about $\cos(x\pm y), \sin(x\pm y)$ etc. Q2: if so, is there any natural (or slick) way to see this, other than going down to the basic $\exp$ definition and calculating term by term? Thanks!",,"['complex-analysis', 'algebra-precalculus', 'trigonometry', 'complex-numbers']"
45,"$\int _{C} (z^3 + 2z +{\bf Re} z)\,dz$ where C is a triangle of vertices $z=0$, $z=1+2i $ and $z=1$.","where C is a triangle of vertices ,  and .","\int _{C} (z^3 + 2z +{\bf Re} z)\,dz z=0 z=1+2i  z=1","How do I compute    $ \int _{C} (z^3 + 2z +{\bf Re} z)\,dz$ where C is a triangle of vertices $z=0$, $z=1+2i $ and $z=1$. The solution given is $i$ Anyone showing me how to deal with these problems will be extremely helpful, as this entire subject quite unclear to me.","How do I compute    $ \int _{C} (z^3 + 2z +{\bf Re} z)\,dz$ where C is a triangle of vertices $z=0$, $z=1+2i $ and $z=1$. The solution given is $i$ Anyone showing me how to deal with these problems will be extremely helpful, as this entire subject quite unclear to me.",,"['complex-analysis', 'complex-numbers', 'cauchy-integral-formula']"
46,Suppose that each function in a sequence $\{f_n\}$ is continuous in an open set $U$ and that the sequence converges normally in $U$ to the limit...,Suppose that each function in a sequence  is continuous in an open set  and that the sequence converges normally in  to the limit...,\{f_n\} U U,"I am reading the book of Palka, An Introduction to Complex Function Theory , and the author on page 248  states two theorems, which does not prove and I would like to know what the demonstration is. The topics are the following: Theorem $1.4.$ Suppose that each function in a sequence $\{f_n\}$ is continuous in an open set $U$ and that the sequence converges normally in $U$ to the limit function $f$. Then $f$ is continuous in $U$. Furthermore $\int_{\gamma}f(z)dz=\lim_{n\rightarrow \infty }\int_{\gamma}f_n(z)dz$ for every piecewise  smooth path $\gamma$ in $U$. Theorem $1.5.$ A sequence $\{f_n\}$ of functions defined in on open set $U$ converges normally in $U$ if and only if it is a uniform Cauchy sequence on each closed  disk that is contained in $U$. I do not understand why Palka does not show these results, could someone explain to me please, I think that to solve this one can use one of the following results:","I am reading the book of Palka, An Introduction to Complex Function Theory , and the author on page 248  states two theorems, which does not prove and I would like to know what the demonstration is. The topics are the following: Theorem $1.4.$ Suppose that each function in a sequence $\{f_n\}$ is continuous in an open set $U$ and that the sequence converges normally in $U$ to the limit function $f$. Then $f$ is continuous in $U$. Furthermore $\int_{\gamma}f(z)dz=\lim_{n\rightarrow \infty }\int_{\gamma}f_n(z)dz$ for every piecewise  smooth path $\gamma$ in $U$. Theorem $1.5.$ A sequence $\{f_n\}$ of functions defined in on open set $U$ converges normally in $U$ if and only if it is a uniform Cauchy sequence on each closed  disk that is contained in $U$. I do not understand why Palka does not show these results, could someone explain to me please, I think that to solve this one can use one of the following results:",,"['integration', 'sequences-and-series', 'complex-analysis', 'analysis', 'convergence-divergence']"
47,What will happen if a sequence of rectangles with decresing diameter tends to a point?,What will happen if a sequence of rectangles with decresing diameter tends to a point?,,"I am studying complex analysis, particularly different versions of Cauchy's theorem. Now while going through the proof of Cauchy's theorem for rectangles I found a concept that a sequence of rectangles with decreasing diameter tends to a point of the initial rectangle I have started with. I know the the concept of sequence in metric spaces where sequence of points of the space tends to some point in that space or some extended space. But how can I relate this concept to above when some nested sets with decreasing diameter tends to a point. What I have thought about it is as follows $:$ Suppose $\{R_n \}$ be nested sequence of rectangles in the complex plane with $\mathrm{diam} (R_n) \rightarrow 0$ as $n \rightarrow \infty$. Then since $(\mathbb C, ||\ .||_2)$ is complete so by Cantor's intersection theorem we have $\cap_{n=1}^{\infty} R_n = \{z^* \}$.  I think it means to say that for a given $\epsilon > 0$, $\exists$ $k \in \mathbb N$ such that $\forall n \geq k$ we have $||z-z^*|| < \epsilon$ for all $z \in R_n$. Is it the correct interpretation of such a convergence? I am not sure of that. Please check it. Thank you in advance.","I am studying complex analysis, particularly different versions of Cauchy's theorem. Now while going through the proof of Cauchy's theorem for rectangles I found a concept that a sequence of rectangles with decreasing diameter tends to a point of the initial rectangle I have started with. I know the the concept of sequence in metric spaces where sequence of points of the space tends to some point in that space or some extended space. But how can I relate this concept to above when some nested sets with decreasing diameter tends to a point. What I have thought about it is as follows $:$ Suppose $\{R_n \}$ be nested sequence of rectangles in the complex plane with $\mathrm{diam} (R_n) \rightarrow 0$ as $n \rightarrow \infty$. Then since $(\mathbb C, ||\ .||_2)$ is complete so by Cantor's intersection theorem we have $\cap_{n=1}^{\infty} R_n = \{z^* \}$.  I think it means to say that for a given $\epsilon > 0$, $\exists$ $k \in \mathbb N$ such that $\forall n \geq k$ we have $||z-z^*|| < \epsilon$ for all $z \in R_n$. Is it the correct interpretation of such a convergence? I am not sure of that. Please check it. Thank you in advance.",,"['complex-analysis', 'complex-integration']"
48,Laurent series that does not define holomorphic function: $\sum_{n=-\infty }^{\infty} (z+3i)^n/2^n$,Laurent series that does not define holomorphic function:,\sum_{n=-\infty }^{\infty} (z+3i)^n/2^n,"I have a doubt about this, I think I am right about it, but I want to check since I couldn't find any similar question online. The problem is the following, very early introduction to Laurent Series. It goes like this. $$\sum_{n=-\infty }^{\infty} \frac{(z+3i)^n}{2^n}$$ I want to make sure that, this series indeed does not represent any holomorphic function. Each subseries i.e. $$\sum_{n=0 }^{\infty} \frac{(z+3i)^n}{2^n}$$ $$\sum_{n=1 }^{\infty} \frac{2^n}{(z+3i)^n}$$ has a radius of convergence: $|z+3i|<2$ for the first one and $|z+3i|>2$ . So the set is empty in which this series converges, therefore, there is no holomorphic function defined by it. Thanks in advance.","I have a doubt about this, I think I am right about it, but I want to check since I couldn't find any similar question online. The problem is the following, very early introduction to Laurent Series. It goes like this. I want to make sure that, this series indeed does not represent any holomorphic function. Each subseries i.e. has a radius of convergence: for the first one and . So the set is empty in which this series converges, therefore, there is no holomorphic function defined by it. Thanks in advance.",\sum_{n=-\infty }^{\infty} \frac{(z+3i)^n}{2^n} \sum_{n=0 }^{\infty} \frac{(z+3i)^n}{2^n} \sum_{n=1 }^{\infty} \frac{2^n}{(z+3i)^n} |z+3i|<2 |z+3i|>2,"['complex-analysis', 'proof-verification', 'laurent-series', 'holomorphic-functions']"
49,Partition of $\mathbb{C}$ into Borel sets for which a polynomial is a bijection with $\mathbb{C}$,Partition of  into Borel sets for which a polynomial is a bijection with,\mathbb{C} \mathbb{C},"Background Let $P$ be a polynomial of degree $n>0$ with coefficients in $\mathbb{C}$ . By the Fundamental Theorem of Algebra, for any complex $y$ , there exist $n$ complex numbers $z$ such that $P(z)=y$ (possibly with multiplicity). So, we can consider $P$ to be a sort of "" $n$ -injective"" function, in that, for each output, there exist $n$ inputs that give that output. Question We can assign each complex $c$ with a ""degree"" $\mu(z)\leq n$ in accordance with the multiplicity of the root $z=c$ in the polynomial $P(z)-P(c)$ . Does there exist a (almost) partition of $\mathbb{C}$ into $n$ Borel sets such that:the polynomial $P$ is bijective from each set to $\mathbb{C}$ and each complex number $z$ appears in exactly $\mu(z)$ of the sets? This would almost be a partition since, for almost all $z$ , $\mu(z)=1$ (in fact, $\mu(z)\neq 1$ only for a finite number of $z$ as that would require $P'(z)=0$ ). I've chosen Borel as the qualifier as, as far as I can tell, it's the best way to define a ""nice set"" - obviously if the quantifier is taken away the problem is true - but I'm not sure if I'm using it properly. Progress The statement is trivially true if $n=1$ (all non-constant linear polynomials are bijective, so one can simply take the set to be $\mathbb{C}$ ). If $n=2$ , the polynomial can be completed into $P(z)=a\left((z-b)^2+c\right)$ for some complex $a,b,c$ , so we can choose our sets to be, if $b=x+iy$ where $x,y\in\mathbb{R}$ , The half-plane above $\Im(z)=y$ and the ray $\Re(z)\geq x, \Im(z)=y$ The half-plane below $\Im(z)=-y$ and the ray $\Re(z)\leq x, \Im(z)=y$ . However, this is all based on the fact that, as functions, all quadratic polynomials act like $z^2$ . So this sort of thing is, as far as I can see, not readily generalizable to $n>2$ . Is this a known problem (and if so, where can I read more about it)? Am I missing something simple?","Background Let be a polynomial of degree with coefficients in . By the Fundamental Theorem of Algebra, for any complex , there exist complex numbers such that (possibly with multiplicity). So, we can consider to be a sort of "" -injective"" function, in that, for each output, there exist inputs that give that output. Question We can assign each complex with a ""degree"" in accordance with the multiplicity of the root in the polynomial . Does there exist a (almost) partition of into Borel sets such that:the polynomial is bijective from each set to and each complex number appears in exactly of the sets? This would almost be a partition since, for almost all , (in fact, only for a finite number of as that would require ). I've chosen Borel as the qualifier as, as far as I can tell, it's the best way to define a ""nice set"" - obviously if the quantifier is taken away the problem is true - but I'm not sure if I'm using it properly. Progress The statement is trivially true if (all non-constant linear polynomials are bijective, so one can simply take the set to be ). If , the polynomial can be completed into for some complex , so we can choose our sets to be, if where , The half-plane above and the ray The half-plane below and the ray . However, this is all based on the fact that, as functions, all quadratic polynomials act like . So this sort of thing is, as far as I can see, not readily generalizable to . Is this a known problem (and if so, where can I read more about it)? Am I missing something simple?","P n>0 \mathbb{C} y n z P(z)=y P n n c \mu(z)\leq n z=c P(z)-P(c) \mathbb{C} n P \mathbb{C} z \mu(z) z \mu(z)=1 \mu(z)\neq 1 z P'(z)=0 n=1 \mathbb{C} n=2 P(z)=a\left((z-b)^2+c\right) a,b,c b=x+iy x,y\in\mathbb{R} \Im(z)=y \Re(z)\geq x, \Im(z)=y \Im(z)=-y \Re(z)\leq x, \Im(z)=y z^2 n>2","['complex-analysis', 'polynomials', 'complex-numbers']"
50,A proof: Closure of a set of complex numbers is closed,A proof: Closure of a set of complex numbers is closed,,"I'm assuming the proof should apply for complex, and real numbers. I've offered a proof for complex numbers, and I am not so sure about some of the steps (namely $a_n \in \bar S_c)$ I take so a second look/hints would be appreciated! The only other theorem I have is that a closed set implies every sequence in the set converges to a point in the set. Let $\bar S = S \cup \partial S$ denote the closure of S, with $\partial S$ being the boundary. Also let $\bar S_c$ be the complement of the closure (sorry if that is unconventional). Suppose $\bar S_c$ is closed. Consider $z \in \partial S$, then $D(z, r) \cap S_c \neq \emptyset$ for any $r > 0$. Take $a_n \in D\left(z, \frac{r}{n}\right) \cap S_c$ for $n = 1, 2, ...$ Then $a_n \in \bar S_c$ because $a_n \notin S, a_n \notin \partial S$. Clearly, $a_n \rightarrow z$ as $ n \rightarrow \infty$. However, $z \in \bar S$ and $a_n \in \bar S_c$ which means there is a convergent sequence in $\bar S_c$ converging to a point outside $\bar S_c \implies \bar S_c$ is open $\implies \bar S$ is closed.","I'm assuming the proof should apply for complex, and real numbers. I've offered a proof for complex numbers, and I am not so sure about some of the steps (namely $a_n \in \bar S_c)$ I take so a second look/hints would be appreciated! The only other theorem I have is that a closed set implies every sequence in the set converges to a point in the set. Let $\bar S = S \cup \partial S$ denote the closure of S, with $\partial S$ being the boundary. Also let $\bar S_c$ be the complement of the closure (sorry if that is unconventional). Suppose $\bar S_c$ is closed. Consider $z \in \partial S$, then $D(z, r) \cap S_c \neq \emptyset$ for any $r > 0$. Take $a_n \in D\left(z, \frac{r}{n}\right) \cap S_c$ for $n = 1, 2, ...$ Then $a_n \in \bar S_c$ because $a_n \notin S, a_n \notin \partial S$. Clearly, $a_n \rightarrow z$ as $ n \rightarrow \infty$. However, $z \in \bar S$ and $a_n \in \bar S_c$ which means there is a convergent sequence in $\bar S_c$ converging to a point outside $\bar S_c \implies \bar S_c$ is open $\implies \bar S$ is closed.",,"['complex-analysis', 'proof-verification']"
51,"Lemma 10.8 Rudin functional analysis, proof.","Lemma 10.8 Rudin functional analysis, proof.",,"There's the following theorem I'm trying to understand (Lemma 10.8 Rudin, Functional Analysis). Let $f$ be an entire function on $\mathbb{C}$ such that $f(0) = 1, f'(0) = 0, 0 < \lvert f(\lambda) \rvert \leq e^{\lvert \lambda \rvert}$ for all $\lambda \in \mathbb{C}$. Then $f(\lambda) = 1$ for   all $\lambda \in \mathbb{C}$. The maximum modulus theorem is used (Theorem 10.24 Rudin, Real and Complex analysis). Suppose $\Omega$ is a region, $f \in H(\Omega)$, and $\overline{D}(a;r) \subset \Omega$. Then $$ \lvert f(a) \rvert \leq \max_{\theta} \lvert f(a + re^{i\theta}) \rvert $$ For the lemma 10.8 the proof goes as follows. Because $f$ doesn't have zeroes then there's a function $g$ such that $f = \exp\left\{ g \right\}$, $g$ entire in $\mathbb{C}$ and $Re[g(\lambda)] \leq |\lambda|$ with $\lambda \in \mathbb{C}$, $g(0) = g'(0) = 0$. Let $r \geq |\lambda|$, we can verify that $$ \lvert g(\lambda) \rvert \leq \lvert 2r - g(\lambda) \rvert $$ $$ h_{r}(\lambda) = \frac{r^2g(\lambda)}{\lambda^2(2r - g(\lambda))} $$ such a function is holomorphic in $\left\{\lambda : |\lambda| < 2r \right\}$. Clearly $\lvert h_r(\lambda) \rvert \leq 1$ if $\vert \lambda \rvert = r$. By the maximum modulus theorem we have for $\lvert \lambda \rvert \leq r$ $$ \lvert h_r(\lambda) \rvert \leq 1. $$ If we fix $\lambda$ and let $r \to \infty$, we have $g(\lambda) = 0$. My main question is how the maximum modulus theorem is actually used in this context? My function $h_r$ is holomorphic in $\Omega = D(0;2r)$ and also I have $\overline{D}(0,r) \subset D(0;2r)$, this should imply $$ \lvert h_r(0) \rvert \leq \max_{\lambda \in \partial \overline{D}(0,r)}\lvert h_r(\lambda) \rvert \leq 1 $$ where with $\partial A$ I denote the boundary set of a set $A$. Why does this imply $\lvert h_r(\lambda) \rvert \leq 1$ if $| \lambda | \leq r$?. Also why if fix $\lambda$ and let $r$ diverging implies $g(\lambda) = 0$?","There's the following theorem I'm trying to understand (Lemma 10.8 Rudin, Functional Analysis). Let $f$ be an entire function on $\mathbb{C}$ such that $f(0) = 1, f'(0) = 0, 0 < \lvert f(\lambda) \rvert \leq e^{\lvert \lambda \rvert}$ for all $\lambda \in \mathbb{C}$. Then $f(\lambda) = 1$ for   all $\lambda \in \mathbb{C}$. The maximum modulus theorem is used (Theorem 10.24 Rudin, Real and Complex analysis). Suppose $\Omega$ is a region, $f \in H(\Omega)$, and $\overline{D}(a;r) \subset \Omega$. Then $$ \lvert f(a) \rvert \leq \max_{\theta} \lvert f(a + re^{i\theta}) \rvert $$ For the lemma 10.8 the proof goes as follows. Because $f$ doesn't have zeroes then there's a function $g$ such that $f = \exp\left\{ g \right\}$, $g$ entire in $\mathbb{C}$ and $Re[g(\lambda)] \leq |\lambda|$ with $\lambda \in \mathbb{C}$, $g(0) = g'(0) = 0$. Let $r \geq |\lambda|$, we can verify that $$ \lvert g(\lambda) \rvert \leq \lvert 2r - g(\lambda) \rvert $$ $$ h_{r}(\lambda) = \frac{r^2g(\lambda)}{\lambda^2(2r - g(\lambda))} $$ such a function is holomorphic in $\left\{\lambda : |\lambda| < 2r \right\}$. Clearly $\lvert h_r(\lambda) \rvert \leq 1$ if $\vert \lambda \rvert = r$. By the maximum modulus theorem we have for $\lvert \lambda \rvert \leq r$ $$ \lvert h_r(\lambda) \rvert \leq 1. $$ If we fix $\lambda$ and let $r \to \infty$, we have $g(\lambda) = 0$. My main question is how the maximum modulus theorem is actually used in this context? My function $h_r$ is holomorphic in $\Omega = D(0;2r)$ and also I have $\overline{D}(0,r) \subset D(0;2r)$, this should imply $$ \lvert h_r(0) \rvert \leq \max_{\lambda \in \partial \overline{D}(0,r)}\lvert h_r(\lambda) \rvert \leq 1 $$ where with $\partial A$ I denote the boundary set of a set $A$. Why does this imply $\lvert h_r(\lambda) \rvert \leq 1$ if $| \lambda | \leq r$?. Also why if fix $\lambda$ and let $r$ diverging implies $g(\lambda) = 0$?",,"['complex-analysis', 'functional-analysis', 'banach-algebras']"
52,Stability result for analytic continuations,Stability result for analytic continuations,,"Let $f(x):\mathbf{R} \rightarrow \mathbf{R}$ be a real function which extends meromorphically to the complex $\mathbf{C}$ plane to a function $\tilde f(z) : \mathbf{C} \rightarrow \mathbf{C}$. Let then $f_n(x): \mathbf{R} \rightarrow \mathbf{R}$, with again meromorphic extensions $\tilde f_n(x): \mathbf{C} \rightarrow \mathbf{C}$, such that: $f_n(x) \rightarrow f(x), x \in \mathbf{R}$ with some convergence criterion (for example uniformly or in some normed sense). Is it true that also: $\tilde f_n(z) \rightarrow \tilde f(z), z \in \mathbf{C}$ for some convergence criterion??? For example can we say that the set of poles of $\tilde f_n$ will tend to the poles of $\tilde f$ I came across this question when trying to understand how analytical continuations can be practically performed. I did not find an answer yet.","Let $f(x):\mathbf{R} \rightarrow \mathbf{R}$ be a real function which extends meromorphically to the complex $\mathbf{C}$ plane to a function $\tilde f(z) : \mathbf{C} \rightarrow \mathbf{C}$. Let then $f_n(x): \mathbf{R} \rightarrow \mathbf{R}$, with again meromorphic extensions $\tilde f_n(x): \mathbf{C} \rightarrow \mathbf{C}$, such that: $f_n(x) \rightarrow f(x), x \in \mathbf{R}$ with some convergence criterion (for example uniformly or in some normed sense). Is it true that also: $\tilde f_n(z) \rightarrow \tilde f(z), z \in \mathbf{C}$ for some convergence criterion??? For example can we say that the set of poles of $\tilde f_n$ will tend to the poles of $\tilde f$ I came across this question when trying to understand how analytical continuations can be practically performed. I did not find an answer yet.",,"['complex-analysis', 'analytic-continuation']"
53,The exponential of a straight line is a straight line if and only if the line is horizontal,The exponential of a straight line is a straight line if and only if the line is horizontal,,"Suppose that $f(z) = e^{z}$. Show that the image of a straight line $\ell$ in $\mathbb{C}$ under $f$ is a straight line if and only if $\ell$ is horizontal. My attempt: Suppose $\ell$ is horizontal. Then for the complex number $z=x+iy$, $y=d$ for fixed $d \in \mathbb{R}$. Hence, $$e^z = e^x e^{id} = e^{x}\cos d + i \sin d$$ which, geometrically, is a complex number with fixed argument $d$ radians, with a variable length $e^{x}$ for $x \in \mathbb{R}$. Hence, as $x$ varies, then $e^{z}$ traces out a straight line. Now suppose that the image of $z$ under $f$ is a straight line. Then $$e^{z} = e^x \cos y + e^x i \sin y.$$ If the image of $z$ under $f$ is a straight line, then the argument of $e^z$ for any $z$ is fixed. This implies $y$ is fixed. However, I'm quite confused now. Even though I fix $y=d$ in the first implication, $z=x+id$ still doesn't look horizontal to me in the Argand diagram... ( scratch this , I realise it actually does haha) I'm not sure if I proved the last implication correctly, it seems kind of dodgy. I say that $y$ is fixed, but do I have to say anything about $x$?","Suppose that $f(z) = e^{z}$. Show that the image of a straight line $\ell$ in $\mathbb{C}$ under $f$ is a straight line if and only if $\ell$ is horizontal. My attempt: Suppose $\ell$ is horizontal. Then for the complex number $z=x+iy$, $y=d$ for fixed $d \in \mathbb{R}$. Hence, $$e^z = e^x e^{id} = e^{x}\cos d + i \sin d$$ which, geometrically, is a complex number with fixed argument $d$ radians, with a variable length $e^{x}$ for $x \in \mathbb{R}$. Hence, as $x$ varies, then $e^{z}$ traces out a straight line. Now suppose that the image of $z$ under $f$ is a straight line. Then $$e^{z} = e^x \cos y + e^x i \sin y.$$ If the image of $z$ under $f$ is a straight line, then the argument of $e^z$ for any $z$ is fixed. This implies $y$ is fixed. However, I'm quite confused now. Even though I fix $y=d$ in the first implication, $z=x+id$ still doesn't look horizontal to me in the Argand diagram... ( scratch this , I realise it actually does haha) I'm not sure if I proved the last implication correctly, it seems kind of dodgy. I say that $y$ is fixed, but do I have to say anything about $x$?",,"['complex-analysis', 'functions', 'proof-verification']"
54,Analytic continuation commuting with series,Analytic continuation commuting with series,,"Suppose $f_1,f_2,...$ are entire functions, and there is an open subset $U \subseteq \mathbb{C}$ such that the series $F(z) = \sum_{n=1}^{\infty} f_n(z)$ converges normally on $U$. Also suppose that $F$ can be analytically continued to an entire function. I have a situation where all $f_n$ vanish at some point $z_0$, but unfortunately $z_0 \notin U.$ Can we still say that $F(z_0) = 0$? I would guess not, since it feels like bending the rules of analytic continuation in a way that shouldn't be allowed. But I didn't think of a counterexample.","Suppose $f_1,f_2,...$ are entire functions, and there is an open subset $U \subseteq \mathbb{C}$ such that the series $F(z) = \sum_{n=1}^{\infty} f_n(z)$ converges normally on $U$. Also suppose that $F$ can be analytically continued to an entire function. I have a situation where all $f_n$ vanish at some point $z_0$, but unfortunately $z_0 \notin U.$ Can we still say that $F(z_0) = 0$? I would guess not, since it feels like bending the rules of analytic continuation in a way that shouldn't be allowed. But I didn't think of a counterexample.",,"['sequences-and-series', 'complex-analysis', 'convergence-divergence', 'analyticity', 'analytic-continuation']"
55,An exercise from Conway's Complex Analysis on an application of Schwarz's Lemma,An exercise from Conway's Complex Analysis on an application of Schwarz's Lemma,,"Let $f$ be analytic in $B(0;1)$, and suppose that $|f(z)|\leq M$ for all $z\in B(0,1)$. If $f(z_{k})=0$ for $1\leq k \leq n$. Show that  $$|f(z)| \leq M\prod_{k=1}^{n}\frac{|z-z_{k}|}{|1-\bar{z_{k}}z|}$$ for $|z|<1$. What I have done is proved the result when  $k=1$. That is if $f(z_{1})=0$, with $|f(z)|\leq M$ on $B(0;1)$, I have proved that  $$|f(z)|\leq M \frac{|z-z_{1}|}{|1-\bar{z_{1}}z|}$$ for $|z|<1$. Now I tried to consider this function:$g:B(0;1)\to \mathbb{C}$ $$g(z)=\frac{f(z)}{\prod_{k=2}^{n}\frac{|z-z_{k}|}{|1-\bar{z_{k}}z|}}$$ Now $g$ has removable singularity at the points $z_{2}, z_{3},.... z_{n}$. So after removing these singularities we can consider $g$ to be analytic. Now wanted to invoke the result I have proved above since $g(z_{1})=0$. But couldn't do so because $g(z)$ might not be bounded by $M$ on $B(0;1)$. How can I work this problem out. Thanks in advance!!","Let $f$ be analytic in $B(0;1)$, and suppose that $|f(z)|\leq M$ for all $z\in B(0,1)$. If $f(z_{k})=0$ for $1\leq k \leq n$. Show that  $$|f(z)| \leq M\prod_{k=1}^{n}\frac{|z-z_{k}|}{|1-\bar{z_{k}}z|}$$ for $|z|<1$. What I have done is proved the result when  $k=1$. That is if $f(z_{1})=0$, with $|f(z)|\leq M$ on $B(0;1)$, I have proved that  $$|f(z)|\leq M \frac{|z-z_{1}|}{|1-\bar{z_{1}}z|}$$ for $|z|<1$. Now I tried to consider this function:$g:B(0;1)\to \mathbb{C}$ $$g(z)=\frac{f(z)}{\prod_{k=2}^{n}\frac{|z-z_{k}|}{|1-\bar{z_{k}}z|}}$$ Now $g$ has removable singularity at the points $z_{2}, z_{3},.... z_{n}$. So after removing these singularities we can consider $g$ to be analytic. Now wanted to invoke the result I have proved above since $g(z_{1})=0$. But couldn't do so because $g(z)$ might not be bounded by $M$ on $B(0;1)$. How can I work this problem out. Thanks in advance!!",,"['complex-analysis', 'analyticity']"
56,Show that coefficients of power series expansion are the fibonacci sequence,Show that coefficients of power series expansion are the fibonacci sequence,,"Show that the power series expansion of $f(z)=\frac{1}{1-z-z^2}$ in $z_0=0$ is $\sum_n F_n z^n$, where $F_n$ is the Fibonacci sequence. Proof . Since $f$ is holomorphic it has a taylor expansion where $F_n=\frac{f^{(n)}(0)}{n!}$. By the Cauchy Integral Formula this equals $\displaystyle F_n=\frac{1}{2\pi i}\int_{|z|=r}\frac{f(z)}{z^{n+1}}dz$ Let us worry about the starting value later. If $F_n$ is the Fibonacci sequence we have $\displaystyle F_n=F_{n-1}+F_{n-2}$ so by inserting $F_n$ and the linearity of the integral we have to show $\displaystyle\frac{f(z)}{z^{n+1}}-\frac{f(z)}{z^{n}}-\frac{f(z)}{z^{n-1}}=0$ which is equal to $\displaystyle\frac{f(z)-z\cdot f(z)-z^2\cdot f(z)}{z^{n+1}}=0$ so $\displaystyle f(z)-z\cdot f(z)-z^2\cdot f(z)=0$ for $z\neq 0$. By inserting $f$ I get the contradiction $1=0$. What am I doing wrong? The mistake is in step 3 where I conclude $\int f=0\implies f=0$. Continuing with the integral proofs the statement.","Show that the power series expansion of $f(z)=\frac{1}{1-z-z^2}$ in $z_0=0$ is $\sum_n F_n z^n$, where $F_n$ is the Fibonacci sequence. Proof . Since $f$ is holomorphic it has a taylor expansion where $F_n=\frac{f^{(n)}(0)}{n!}$. By the Cauchy Integral Formula this equals $\displaystyle F_n=\frac{1}{2\pi i}\int_{|z|=r}\frac{f(z)}{z^{n+1}}dz$ Let us worry about the starting value later. If $F_n$ is the Fibonacci sequence we have $\displaystyle F_n=F_{n-1}+F_{n-2}$ so by inserting $F_n$ and the linearity of the integral we have to show $\displaystyle\frac{f(z)}{z^{n+1}}-\frac{f(z)}{z^{n}}-\frac{f(z)}{z^{n-1}}=0$ which is equal to $\displaystyle\frac{f(z)-z\cdot f(z)-z^2\cdot f(z)}{z^{n+1}}=0$ so $\displaystyle f(z)-z\cdot f(z)-z^2\cdot f(z)=0$ for $z\neq 0$. By inserting $f$ I get the contradiction $1=0$. What am I doing wrong? The mistake is in step 3 where I conclude $\int f=0\implies f=0$. Continuing with the integral proofs the statement.",,"['complex-analysis', 'functions']"
57,Laurent Expansion - Complex Analysis question,Laurent Expansion - Complex Analysis question,,"Exercise : Find the Laurent Expansion of the function : $$f(z) = \frac{1}{(z-2i)(z^2 +4)}$$ around $z_0 = -2i$, in the biggest possible ring that includes the point $z=-2 + 2i$. Attempt : We have : $$f(z) = \frac{1}{(z-2i)(z^2 +4)} = \frac{1}{(z-2i)(z + 2i)(z-2i)} = \frac{1}{(z-2i)^2(z + 2i)} $$ We have the rings : $$D_1 : 0<|z+2i|<4$$ $$D_2 : |z+2i|>4$$ Because $|(-2+2i) + 2i| = |-2 + 4i| = \sqrt{20} > 4$, the biggest ring with center $z_0=-2i$ that contains the point $z=-2+2i$, is : $$D_2 : |z+2i|>4$$ Now, such exercises can be handled by creating such a fraction, such as to use the usual geometric series : $$\frac{1}{1-w} = \sum_{n=0}^{\infty}w^n ,|w|<1$$ $$\frac{1}{1+w} = \sum_{n=0}^{\infty}(-1)^nw^n,|w|<1$$ It is : $$f(z) = \frac{1}{(z-2i)^2(z + 2i)} = \frac{1}{(z-2i)^2[(z - 2i)+4i]} = \frac{1}{(z-2i)^2(z - 2i)\big(1-\frac{4i}{z-2i}\big)} = \frac{1}{(z-2i)^3\big(1-\frac{4i}{z-2i}\big)}  $$ But in order to have $|w|<1 $ it must hold that : $|z-2i| > 4$, but our ring is $|z+2i|>4$. My question is : Is the exercise wrong ? Maybe it means around the center $z_0 = 2i$ ? Or is it something that I am doing wrong ? If nothing is wrong, how do I proceed here ? Except if I use : $$\frac{1}{(z-2i)^2(z + 2i)} = \frac{1}{(z+2i)^3\big(1-\frac{4i}{z+2i}\big)^2} = \frac{1}{(z+2i)^3}\frac{1}{ \big(1-\frac{4i}{z+2i}\big)^2} = \frac{1}{(z+2i)^3} \Bigg( \sum_{n=0}^{\infty} \bigg(\frac{4i}{z+2i}\bigg)^n\Bigg)^2 $$ because if so, it is $|z+2i|>4$. I would really appreciate some thorough help.","Exercise : Find the Laurent Expansion of the function : $$f(z) = \frac{1}{(z-2i)(z^2 +4)}$$ around $z_0 = -2i$, in the biggest possible ring that includes the point $z=-2 + 2i$. Attempt : We have : $$f(z) = \frac{1}{(z-2i)(z^2 +4)} = \frac{1}{(z-2i)(z + 2i)(z-2i)} = \frac{1}{(z-2i)^2(z + 2i)} $$ We have the rings : $$D_1 : 0<|z+2i|<4$$ $$D_2 : |z+2i|>4$$ Because $|(-2+2i) + 2i| = |-2 + 4i| = \sqrt{20} > 4$, the biggest ring with center $z_0=-2i$ that contains the point $z=-2+2i$, is : $$D_2 : |z+2i|>4$$ Now, such exercises can be handled by creating such a fraction, such as to use the usual geometric series : $$\frac{1}{1-w} = \sum_{n=0}^{\infty}w^n ,|w|<1$$ $$\frac{1}{1+w} = \sum_{n=0}^{\infty}(-1)^nw^n,|w|<1$$ It is : $$f(z) = \frac{1}{(z-2i)^2(z + 2i)} = \frac{1}{(z-2i)^2[(z - 2i)+4i]} = \frac{1}{(z-2i)^2(z - 2i)\big(1-\frac{4i}{z-2i}\big)} = \frac{1}{(z-2i)^3\big(1-\frac{4i}{z-2i}\big)}  $$ But in order to have $|w|<1 $ it must hold that : $|z-2i| > 4$, but our ring is $|z+2i|>4$. My question is : Is the exercise wrong ? Maybe it means around the center $z_0 = 2i$ ? Or is it something that I am doing wrong ? If nothing is wrong, how do I proceed here ? Except if I use : $$\frac{1}{(z-2i)^2(z + 2i)} = \frac{1}{(z+2i)^3\big(1-\frac{4i}{z+2i}\big)^2} = \frac{1}{(z+2i)^3}\frac{1}{ \big(1-\frac{4i}{z+2i}\big)^2} = \frac{1}{(z+2i)^3} \Bigg( \sum_{n=0}^{\infty} \bigg(\frac{4i}{z+2i}\bigg)^n\Bigg)^2 $$ because if so, it is $|z+2i|>4$. I would really appreciate some thorough help.",,"['calculus', 'sequences-and-series', 'complex-analysis', 'taylor-expansion', 'laurent-series']"
58,How does stokes theorem generalize to sets rather than manifold.,How does stokes theorem generalize to sets rather than manifold.,,"I learned the Stokes theorem in the context of differential topology, it says that if I have a $n-1$ form $w$ with compact support on an oriented smooth n-manifold, then $ \int_{\partial M} w = \int_M dw$. However I was asked to prove the following fact using the stokes theorem: given a compact set $K \subset \mathbb C$, and a holomorphic function on $\mathbb C$, $\int_{ \partial K} f =0 $, how should I use the stokes theorem since $K$ here is necessarily a manifold? May be it involves arguing that $\partial K$ can be realized as the boundary of some manifold?","I learned the Stokes theorem in the context of differential topology, it says that if I have a $n-1$ form $w$ with compact support on an oriented smooth n-manifold, then $ \int_{\partial M} w = \int_M dw$. However I was asked to prove the following fact using the stokes theorem: given a compact set $K \subset \mathbb C$, and a holomorphic function on $\mathbb C$, $\int_{ \partial K} f =0 $, how should I use the stokes theorem since $K$ here is necessarily a manifold? May be it involves arguing that $\partial K$ can be realized as the boundary of some manifold?",,"['complex-analysis', 'differential-topology']"
59,$f'(a)=0 \Rightarrow f$ constant,constant,f'(a)=0 \Rightarrow f,"I am currently reviewing theorems I already learned in complex analysis and looking at Holomorphic function with zero derivative is constant on an open connected set I asked myself wether it is necessary that $f'(x)=0 \forall x \in G$ for $f:G\to \mathbb{C}$ and $G$ being a domain to get that $f$ is constant or wether a single point in the domain could be sufficient. If so, how to show that? How is this related to the maximum principle? When I think about real analysis, $f'(a)=0$ implies that there is an extremum in $a$ but it seems this is not the case in complex analysis, at least I have not found anything but the maximum principle regarding this issue.","I am currently reviewing theorems I already learned in complex analysis and looking at Holomorphic function with zero derivative is constant on an open connected set I asked myself wether it is necessary that $f'(x)=0 \forall x \in G$ for $f:G\to \mathbb{C}$ and $G$ being a domain to get that $f$ is constant or wether a single point in the domain could be sufficient. If so, how to show that? How is this related to the maximum principle? When I think about real analysis, $f'(a)=0$ implies that there is an extremum in $a$ but it seems this is not the case in complex analysis, at least I have not found anything but the maximum principle regarding this issue.",,['complex-analysis']
60,Trouble understanding this Equality of Complex number question.,Trouble understanding this Equality of Complex number question.,,I have this question here $$(3-2j)(x+yj)=4+j^9$$ I know that $a + bj$ and $x + yj$ are equal when $a = x$ and $b = y$. This question is confusing me because I have had three different answers but they are all wrong. What feels like my closest attempt was: $$(3-2j)(x+yj)=4+j^9.$$ $x + yj = (-3+2j)(4-j)$ $= -12 + 3j + 8j -2j^2$ $= -12 + 11j -2(-1)$ $= -12 + 11j + 2$ $= -10 + 11j \:\: \Rightarrow$ $x = -10$ $y = 11$ But the answer is supposed to be $x = 10/13$ $y = 11/13$ I am just looking for some guidance on how to properly solve this and figure out where I went wrong. Thank you!,I have this question here $$(3-2j)(x+yj)=4+j^9$$ I know that $a + bj$ and $x + yj$ are equal when $a = x$ and $b = y$. This question is confusing me because I have had three different answers but they are all wrong. What feels like my closest attempt was: $$(3-2j)(x+yj)=4+j^9.$$ $x + yj = (-3+2j)(4-j)$ $= -12 + 3j + 8j -2j^2$ $= -12 + 11j -2(-1)$ $= -12 + 11j + 2$ $= -10 + 11j \:\: \Rightarrow$ $x = -10$ $y = 11$ But the answer is supposed to be $x = 10/13$ $y = 11/13$ I am just looking for some guidance on how to properly solve this and figure out where I went wrong. Thank you!,,"['complex-analysis', 'complex-numbers', 'complex-geometry']"
61,Rouche's Theorem proof modified,Rouche's Theorem proof modified,,"Here is a modified proof of Rouche's Theorem, i only changed the last part where one needs to show the integral will be $0$. However, the book Bak and Newman uses winding numbers, in which i have yet to read. Will anyone verify if my logic is correct if i just use merely cauchy goursat to do? Note first that if $f(z) = h(z) \cdot k(z)$, then $$\dfrac{f^{'}(z)}{f(z)} = \dfrac{h^{'}(z)}{h(z)}+ \dfrac{k^{'}(z)}{k(z)}$$ So that we have $$\int_{C}\dfrac{f^{'}(z)}{f(z)}dz = \int_{C}\dfrac{h^{'}(z)}{h(z)}dz  + \int_{C}\dfrac{k^{'}(z)}{k(z)}dz$$ Thus if we write $$f(z) + g(z) = f(z) \cdot \left(1+ \frac{g(z)}{f(z)}\right)$$ It follows from the Argument Principle that the number of zeros of $f+g$ in $C$ is evaluated as $\begin{aligned} Z(f+g) & = \dfrac{1}{2\pi i}\int_{C}\dfrac{(f+g)^{'}}{f+g}dz\\ 		& = \dfrac{1}{2\pi i}\int_{C} \dfrac{f^{'}(1+\frac{g}{f})+f(1+\frac{g}{f})^{'}}{f(1+\frac{g}{f})}dz\\ 		&= \dfrac{1}{2\pi i}\int_{C} \dfrac{f^{'}}{f}dz + \dfrac{1}{2\pi i} \int_{C}\dfrac{(1+\frac{g}{f})^{'}}{1+\frac{g}{f}}dz\\ 		&= Z(f)  + \dfrac{1}{2\pi i} \int_{C}\dfrac{(1+\frac{g}{f})^{'}}{1+\frac{g}{f}}dz\\ \end{aligned}$ Since we are dealing with a compact set in $\mathbb{C}$, we have $|g|$ and $|f|$ to be both bounded in the circle $C$. Consequently, $\left|\dfrac{g}{f}\right|$ is bounded in $C$. Assume that $f$ has zeros $z_1,z_2,...,z_n$, and since $\left|\dfrac{g}{f}\right|$ is bounded in each deleted neighborhood of an isolated singularity, the singularity is removable. Consequently, $\dfrac{g}{f}$ is analytic in and on $C$. It follows that $1+\dfrac{g}{f}$ is analytic in and on $C$. Next we look at if there are any singularities in $C$ for the function $\dfrac{(1+\frac{g}{f})^{'}}{1+\frac{g}{f}}$. The singularities of this will occur iff $$1+\dfrac{g}{f} = 0 \Leftrightarrow \dfrac{g}{f} = -1 \Leftrightarrow f = -g \Leftrightarrow |f| = |-g| = |g|$$ Thus this leads to a contradiction as our hypothesis and extreme value theorem says $|g| < |f|\leq M$ on the boundary, by the maximum modulus, the maximum of $|g|$ occurs on the boundary and hence we are sure that inside the circle $C$, $|g| < |f| \leq M$ as well. Hence there are no zeros or singularities inside the $C$ as $1+\dfrac{g}{f}$ is non vanishing. Hence we have $\dfrac{1}{2\pi i} \int_{C}\dfrac{(1+\frac{g}{f})^{'}}{1+\frac{g}{f}}dz = 0$ by Cauchy Goursat. Consequently, $Z(f+g) = Z(f)$","Here is a modified proof of Rouche's Theorem, i only changed the last part where one needs to show the integral will be $0$. However, the book Bak and Newman uses winding numbers, in which i have yet to read. Will anyone verify if my logic is correct if i just use merely cauchy goursat to do? Note first that if $f(z) = h(z) \cdot k(z)$, then $$\dfrac{f^{'}(z)}{f(z)} = \dfrac{h^{'}(z)}{h(z)}+ \dfrac{k^{'}(z)}{k(z)}$$ So that we have $$\int_{C}\dfrac{f^{'}(z)}{f(z)}dz = \int_{C}\dfrac{h^{'}(z)}{h(z)}dz  + \int_{C}\dfrac{k^{'}(z)}{k(z)}dz$$ Thus if we write $$f(z) + g(z) = f(z) \cdot \left(1+ \frac{g(z)}{f(z)}\right)$$ It follows from the Argument Principle that the number of zeros of $f+g$ in $C$ is evaluated as $\begin{aligned} Z(f+g) & = \dfrac{1}{2\pi i}\int_{C}\dfrac{(f+g)^{'}}{f+g}dz\\ 		& = \dfrac{1}{2\pi i}\int_{C} \dfrac{f^{'}(1+\frac{g}{f})+f(1+\frac{g}{f})^{'}}{f(1+\frac{g}{f})}dz\\ 		&= \dfrac{1}{2\pi i}\int_{C} \dfrac{f^{'}}{f}dz + \dfrac{1}{2\pi i} \int_{C}\dfrac{(1+\frac{g}{f})^{'}}{1+\frac{g}{f}}dz\\ 		&= Z(f)  + \dfrac{1}{2\pi i} \int_{C}\dfrac{(1+\frac{g}{f})^{'}}{1+\frac{g}{f}}dz\\ \end{aligned}$ Since we are dealing with a compact set in $\mathbb{C}$, we have $|g|$ and $|f|$ to be both bounded in the circle $C$. Consequently, $\left|\dfrac{g}{f}\right|$ is bounded in $C$. Assume that $f$ has zeros $z_1,z_2,...,z_n$, and since $\left|\dfrac{g}{f}\right|$ is bounded in each deleted neighborhood of an isolated singularity, the singularity is removable. Consequently, $\dfrac{g}{f}$ is analytic in and on $C$. It follows that $1+\dfrac{g}{f}$ is analytic in and on $C$. Next we look at if there are any singularities in $C$ for the function $\dfrac{(1+\frac{g}{f})^{'}}{1+\frac{g}{f}}$. The singularities of this will occur iff $$1+\dfrac{g}{f} = 0 \Leftrightarrow \dfrac{g}{f} = -1 \Leftrightarrow f = -g \Leftrightarrow |f| = |-g| = |g|$$ Thus this leads to a contradiction as our hypothesis and extreme value theorem says $|g| < |f|\leq M$ on the boundary, by the maximum modulus, the maximum of $|g|$ occurs on the boundary and hence we are sure that inside the circle $C$, $|g| < |f| \leq M$ as well. Hence there are no zeros or singularities inside the $C$ as $1+\dfrac{g}{f}$ is non vanishing. Hence we have $\dfrac{1}{2\pi i} \int_{C}\dfrac{(1+\frac{g}{f})^{'}}{1+\frac{g}{f}}dz = 0$ by Cauchy Goursat. Consequently, $Z(f+g) = Z(f)$",,['complex-analysis']
62,How does Wolfram Alpha perform Taylor Expansions of functions about singular points?,How does Wolfram Alpha perform Taylor Expansions of functions about singular points?,,"I am trying to use a Taylor Expansion to expand a function about a singular point z_0 to obtain a Laurent Series about the point z_0 . As an example, consider the function f(z) = exp(2z) / (1-z)^3 about z_0=1 First, I converted the denominator from (1-z)^3 to -(z-1)^3 . I've done Taylor Expansions about non-singular points many times using this process . According to this post , it is impossible to do a Taylor Expansion about a singular point. Yet, typing the operation into Wolfram Alpha produces the correct Laurent Series. How can I manually reproduce the output returned by Wolfram Alpha? I'd appreciate even the smallest hint, as my problem is that evaluating the function and a few consequent derivatives at z=z_0 returns .../0 , as is easier to see here .","I am trying to use a Taylor Expansion to expand a function about a singular point z_0 to obtain a Laurent Series about the point z_0 . As an example, consider the function f(z) = exp(2z) / (1-z)^3 about z_0=1 First, I converted the denominator from (1-z)^3 to -(z-1)^3 . I've done Taylor Expansions about non-singular points many times using this process . According to this post , it is impossible to do a Taylor Expansion about a singular point. Yet, typing the operation into Wolfram Alpha produces the correct Laurent Series. How can I manually reproduce the output returned by Wolfram Alpha? I'd appreciate even the smallest hint, as my problem is that evaluating the function and a few consequent derivatives at z=z_0 returns .../0 , as is easier to see here .",,"['complex-analysis', 'taylor-expansion']"
63,"Local constancy of winding number, proof by differentiation","Local constancy of winding number, proof by differentiation",,"May someone explain the proof meant in Terren's Notes for Ex 45? Lemma 44: Let $\gamma$ be a closed curve. Then $W_\gamma: z_0 \mapsto \int_{\gamma}\frac{1}{z-z_0} \, dz $ is locally constant. If $z_0 \notin Im(\gamma)$, then exists $D(z_0,r) \cap Im (\gamma) = \emptyset $ such that $W_\gamma(z) = W_\gamma(z_0)$ for all $z \in D(z_0,r)$. Exercise 45: Give a proof based on differentiation under the integral sign and using the fact that $\frac{1}{(z-z_0)^2}$ has an antiderivative away from $z_0$. What I had in mind was  $$ \Big| \frac{W_\gamma (a+h) - W_\gamma(a) }{h} \Big | = \Big | \int _\gamma \Big( \frac{1}{(z-z_0)^2}+\frac{1}{(z-a)(z-a-h)} \Big)\, dz \Big| \le  |h||\gamma|M \rightarrow 0$$ as $h \rightarrow 0 $, for some constant $M$. The second equality follows from adding $\int_\gamma \frac{1}{(z-z_0)^2} = 0$. Is this right? And is this what was intended?","May someone explain the proof meant in Terren's Notes for Ex 45? Lemma 44: Let $\gamma$ be a closed curve. Then $W_\gamma: z_0 \mapsto \int_{\gamma}\frac{1}{z-z_0} \, dz $ is locally constant. If $z_0 \notin Im(\gamma)$, then exists $D(z_0,r) \cap Im (\gamma) = \emptyset $ such that $W_\gamma(z) = W_\gamma(z_0)$ for all $z \in D(z_0,r)$. Exercise 45: Give a proof based on differentiation under the integral sign and using the fact that $\frac{1}{(z-z_0)^2}$ has an antiderivative away from $z_0$. What I had in mind was  $$ \Big| \frac{W_\gamma (a+h) - W_\gamma(a) }{h} \Big | = \Big | \int _\gamma \Big( \frac{1}{(z-z_0)^2}+\frac{1}{(z-a)(z-a-h)} \Big)\, dz \Big| \le  |h||\gamma|M \rightarrow 0$$ as $h \rightarrow 0 $, for some constant $M$. The second equality follows from adding $\int_\gamma \frac{1}{(z-z_0)^2} = 0$. Is this right? And is this what was intended?",,"['complex-analysis', 'derivatives', 'contour-integration', 'winding-number']"
64,Complex integration of $\log(z)$ over a closed counter-clockwise curve containing the origin only once,Complex integration of  over a closed counter-clockwise curve containing the origin only once,\log(z),"What's the integral of $\log(z)$ in $C$, where $C$ is a closed curve enclosing the origin only once (counter-clockwise)? I tried to use the circle with radius $r$, $\{z=re^{\theta i} : \theta \in (0,2\pi) \}$, but then I obtain the result $2 \pi r i$, and I think that the result should not depend on the radius.","What's the integral of $\log(z)$ in $C$, where $C$ is a closed curve enclosing the origin only once (counter-clockwise)? I tried to use the circle with radius $r$, $\{z=re^{\theta i} : \theta \in (0,2\pi) \}$, but then I obtain the result $2 \pi r i$, and I think that the result should not depend on the radius.",,"['complex-analysis', 'contour-integration', 'complex-integration']"
65,"If $f(z)$ is entire, and $f(z)$ is real iff $z$ is real, prove $f'(z)$ is not equal to $0$ for all real $z$","If  is entire, and  is real iff  is real, prove  is not equal to  for all real",f(z) f(z) z f'(z) 0 z,"If $f(z)$ is entire, and $f(z)$ is real iff $z$ is real, prove $f'(z) \ne 0$ for all real $z$. Edit: Sorry, somehow my preliminary efforts didn't make it in here: I know that $f$ cannot be constant, that's a contradiction, and that if $f$ is entire, then it is analytic and holomorphic everywhere. I have seen (but not quite understood) proofs that $f$ can have at most one zero. I have not done ""winding numbers"" and it seems most proofs of this employ the use of these. Because $f$ is entire, it is continuous and so all $z$ in the upper half plane have the same sign of their imaginary part. My first thought was by contradiction. Assume $f'(z)=0$ for some $z_0 \in C$. Then $a_1 = 0$ in the taylor series of $f$, so then $f(z)= f(z_0) + a_2*(z-z_0)^2 + O((z-z_0)^2)$.  Then I'm not sure where to go from here. I was also thinking of setting $f = u(x,y) + iv(x,y)$, and noting that $v(x,0)=0$, but I can't see how this would get me anywhere with derivatives.","If $f(z)$ is entire, and $f(z)$ is real iff $z$ is real, prove $f'(z) \ne 0$ for all real $z$. Edit: Sorry, somehow my preliminary efforts didn't make it in here: I know that $f$ cannot be constant, that's a contradiction, and that if $f$ is entire, then it is analytic and holomorphic everywhere. I have seen (but not quite understood) proofs that $f$ can have at most one zero. I have not done ""winding numbers"" and it seems most proofs of this employ the use of these. Because $f$ is entire, it is continuous and so all $z$ in the upper half plane have the same sign of their imaginary part. My first thought was by contradiction. Assume $f'(z)=0$ for some $z_0 \in C$. Then $a_1 = 0$ in the taylor series of $f$, so then $f(z)= f(z_0) + a_2*(z-z_0)^2 + O((z-z_0)^2)$.  Then I'm not sure where to go from here. I was also thinking of setting $f = u(x,y) + iv(x,y)$, and noting that $v(x,0)=0$, but I can't see how this would get me anywhere with derivatives.",,['complex-analysis']
66,$\dfrac{1}{2\pi i}\int_{C}\dfrac{e^{z}}{z^{3}-1}dz = \sum_{n=0}^{\infty}\dfrac{1}{(3n+2)!}$,,\dfrac{1}{2\pi i}\int_{C}\dfrac{e^{z}}{z^{3}-1}dz = \sum_{n=0}^{\infty}\dfrac{1}{(3n+2)!},"a) Use residue at infinity to solve the problem: If $C$ is a circle $C(0,2)$ traversed in the counter clockwise direction, then $$\dfrac{1}{2\pi i}\int_{C}\dfrac{e^{z}}{z^{3}-1}dz = \sum_{n=0}^{\infty}\dfrac{1}{(3n+2)!}$$ b) By evaluating $$\dfrac{1}{2\pi i}\int_{C}\dfrac{e^{z}}{z^{3}-1}dz$$ using Cauchy's integral formula, show that $$\sum_{n=0}^{\infty}\dfrac{1}{(3n+2)!} = \dfrac{1}{3}(e-\dfrac{2}{\sqrt{e}}\cos(\dfrac{\pi}{3}-\dfrac{\sqrt{3}}{2}))$$ Hint: One may use the identity $\prod_{j=1,j\neq k}^{m}(e^{2\pi ij/m}-e^{2\pi ik/m}) = \dfrac{m}{e^{2\pi ik/m}}$ a) Note that the function $f(z) = \dfrac{e^{z}}{z^{3}-1}$ and we see that there will be 3 singularity, namely $z =1, z = e^{2\pi i/3}\text{ and } z = e^{4\pi i/3}$. All of which are contained in the $C(0,2)$. Well since we are given the hint to solve the first part with residue at infinity, we will follow it. $$\dfrac{1}{2\pi i}\int_{C}f(z)dz = \text{Res}\left[\dfrac{1}{z^{2}}f\left(\dfrac{1}{z}\right),0\right] = \text{Res}\left[\dfrac{1}{z^{2}}\dfrac{e^{1/z}}{\dfrac{1}{z^{3}}-1},0\right] = \text{Res}\left[\dfrac{ze^{1/z}}{1-z^{3}},0\right]$$ We want to find the reside of $\dfrac{ze^{1/z}}{1-z^{3}}$ at $0$. Hence we expand this equation about $0$ in Laurent Series form, $$ze^{1/z}\dfrac{1}{1-z^{3}} = z\sum_{k=0}^{\infty}\dfrac{1}{k!z^{k}}\sum_{m=0}^{\infty}z^{3m} = \sum_{m=0}^{\infty}\left(\sum_{k=0}^{\infty}\dfrac{1}{k!}z^{3n-k+1}\right)$$ We want to find the coefficient of $z^{-1}$ and it happens when $3n-k+1 = -1 \Rightarrow k = 3m+2$. Hence the residue is $$\sum_{m=0}^{\infty}\dfrac{1}{(3m+2)!}$$ First question is part a), why can't i get the answer if i interchange the summation????? By right it should be the same answer. b) We easily find out that the three singularities for the function is $z=1,z=e^{2\pi i/3},z=e^{4\pi i/3}$ And hence we are solving for $$\dfrac{1}{2\pi i}\int_{C}\dfrac{e^{z}}{(z-1)(z-e^{2\pi i/3})(z-e^{4\pi i/3})}dz$$ Since we are required to use CIF to solve, we construct 3 circles $C_1,C_2,C_3$ of radius $\epsilon_1,\epsilon_2,\epsilon_3$ and centered at $z=1,z=e^{2\pi i/3},z=e^{4\pi i/3}$. Then from here, i can only use brute force (REALLY TOOK ME VERY LONG) to solve it, anyone knows how to solve it with the hint?","a) Use residue at infinity to solve the problem: If $C$ is a circle $C(0,2)$ traversed in the counter clockwise direction, then $$\dfrac{1}{2\pi i}\int_{C}\dfrac{e^{z}}{z^{3}-1}dz = \sum_{n=0}^{\infty}\dfrac{1}{(3n+2)!}$$ b) By evaluating $$\dfrac{1}{2\pi i}\int_{C}\dfrac{e^{z}}{z^{3}-1}dz$$ using Cauchy's integral formula, show that $$\sum_{n=0}^{\infty}\dfrac{1}{(3n+2)!} = \dfrac{1}{3}(e-\dfrac{2}{\sqrt{e}}\cos(\dfrac{\pi}{3}-\dfrac{\sqrt{3}}{2}))$$ Hint: One may use the identity $\prod_{j=1,j\neq k}^{m}(e^{2\pi ij/m}-e^{2\pi ik/m}) = \dfrac{m}{e^{2\pi ik/m}}$ a) Note that the function $f(z) = \dfrac{e^{z}}{z^{3}-1}$ and we see that there will be 3 singularity, namely $z =1, z = e^{2\pi i/3}\text{ and } z = e^{4\pi i/3}$. All of which are contained in the $C(0,2)$. Well since we are given the hint to solve the first part with residue at infinity, we will follow it. $$\dfrac{1}{2\pi i}\int_{C}f(z)dz = \text{Res}\left[\dfrac{1}{z^{2}}f\left(\dfrac{1}{z}\right),0\right] = \text{Res}\left[\dfrac{1}{z^{2}}\dfrac{e^{1/z}}{\dfrac{1}{z^{3}}-1},0\right] = \text{Res}\left[\dfrac{ze^{1/z}}{1-z^{3}},0\right]$$ We want to find the reside of $\dfrac{ze^{1/z}}{1-z^{3}}$ at $0$. Hence we expand this equation about $0$ in Laurent Series form, $$ze^{1/z}\dfrac{1}{1-z^{3}} = z\sum_{k=0}^{\infty}\dfrac{1}{k!z^{k}}\sum_{m=0}^{\infty}z^{3m} = \sum_{m=0}^{\infty}\left(\sum_{k=0}^{\infty}\dfrac{1}{k!}z^{3n-k+1}\right)$$ We want to find the coefficient of $z^{-1}$ and it happens when $3n-k+1 = -1 \Rightarrow k = 3m+2$. Hence the residue is $$\sum_{m=0}^{\infty}\dfrac{1}{(3m+2)!}$$ First question is part a), why can't i get the answer if i interchange the summation????? By right it should be the same answer. b) We easily find out that the three singularities for the function is $z=1,z=e^{2\pi i/3},z=e^{4\pi i/3}$ And hence we are solving for $$\dfrac{1}{2\pi i}\int_{C}\dfrac{e^{z}}{(z-1)(z-e^{2\pi i/3})(z-e^{4\pi i/3})}dz$$ Since we are required to use CIF to solve, we construct 3 circles $C_1,C_2,C_3$ of radius $\epsilon_1,\epsilon_2,\epsilon_3$ and centered at $z=1,z=e^{2\pi i/3},z=e^{4\pi i/3}$. Then from here, i can only use brute force (REALLY TOOK ME VERY LONG) to solve it, anyone knows how to solve it with the hint?",,"['complex-analysis', 'contour-integration', 'residue-calculus']"
67,Possible values of $\int_\gamma{\frac{1}{z}}dz$,Possible values of,\int_\gamma{\frac{1}{z}}dz,"What are the possible values $\int_\gamma{\frac{1}{z}}dz$ where $\gamma$ is a path that starts at $z=-i$  and ends $z=2i$ and avoids the origin? I pick a specific branch cut, for example negative x axis, so $-\pi \leq \theta \leq \pi$. $\mathbb{C} -$branch cut is a simply connected region so integral will be the same over all the paths. Then I have $\int_\gamma {\frac{1}{z}dz} = \int_\gamma{\frac{d}{dz}(\log(z))dz} = Log(2i) - Log(i) = \ln2 -i\frac{\pi}{2} - \ln(1) - i\frac{\pi}{2} = \ln(2) - i\pi$ Is this correct? I am confused about the 'possible values' in the problem statement","What are the possible values $\int_\gamma{\frac{1}{z}}dz$ where $\gamma$ is a path that starts at $z=-i$  and ends $z=2i$ and avoids the origin? I pick a specific branch cut, for example negative x axis, so $-\pi \leq \theta \leq \pi$. $\mathbb{C} -$branch cut is a simply connected region so integral will be the same over all the paths. Then I have $\int_\gamma {\frac{1}{z}dz} = \int_\gamma{\frac{d}{dz}(\log(z))dz} = Log(2i) - Log(i) = \ln2 -i\frac{\pi}{2} - \ln(1) - i\frac{\pi}{2} = \ln(2) - i\pi$ Is this correct? I am confused about the 'possible values' in the problem statement",,['complex-analysis']
68,Choosing a branch of the square root so that this rewriting works,Choosing a branch of the square root so that this rewriting works,,"Gamelin writes the following in his book: Example. Consider $\sqrt{z-1/z}$. We rewrite this as $\sqrt{z-1}\sqrt{z+ 1}/\sqrt{z}$. How can this rewriting actually be done? For no branch we have universally that the square root is multiplicative, so maybe he means different branches in the four square roots in this equation? But then this equation is not really an equation since the domains of the two functions are different...","Gamelin writes the following in his book: Example. Consider $\sqrt{z-1/z}$. We rewrite this as $\sqrt{z-1}\sqrt{z+ 1}/\sqrt{z}$. How can this rewriting actually be done? For no branch we have universally that the square root is multiplicative, so maybe he means different branches in the four square roots in this equation? But then this equation is not really an equation since the domains of the two functions are different...",,"['complex-analysis', 'branch-cuts']"
69,"$\{a_n\} \subseteq \mathbb C$ discrete set with no limit point . For any sequence $\{z_n\} $ , there is entire $f $ on $\mathbb C$ with $f(a_n)=z_n$?","discrete set with no limit point . For any sequence  , there is entire  on  with ?",\{a_n\} \subseteq \mathbb C \{z_n\}  f  \mathbb C f(a_n)=z_n,"Let $\{a_n\} \subseteq \mathbb C$ be a discrete set with no limit point . Then for every sequence $\{z_n\}$ of complex numbers , can we find an entire function $f:\mathbb C \to \mathbb C$ such that $f(a_n)=z_n , \forall n \in \mathbb N$ ? I feel I have to use Weierstrass factorization or Mittag-Leffler , but I can't quite crack it . Please help . Thanks in advance","Let $\{a_n\} \subseteq \mathbb C$ be a discrete set with no limit point . Then for every sequence $\{z_n\}$ of complex numbers , can we find an entire function $f:\mathbb C \to \mathbb C$ such that $f(a_n)=z_n , \forall n \in \mathbb N$ ? I feel I have to use Weierstrass factorization or Mittag-Leffler , but I can't quite crack it . Please help . Thanks in advance",,"['complex-analysis', 'holomorphic-functions']"
70,polynomial in entire function with polynomial coefficients,polynomial in entire function with polynomial coefficients,,"Let $f$ be an entire function. Assume that there are polynomials $p_0, \ldots , p_n(z)$, not all zero, such that $p_n(z)(f(z))^n + p_{n−1}(z)(f(z))^{n−1} + \cdots + p_0(z) = 0$. Prove that f is a polynomial.","Let $f$ be an entire function. Assume that there are polynomials $p_0, \ldots , p_n(z)$, not all zero, such that $p_n(z)(f(z))^n + p_{n−1}(z)(f(z))^{n−1} + \cdots + p_0(z) = 0$. Prove that f is a polynomial.",,"['complex-analysis', 'polynomials', 'entire-functions']"
71,On a pair of equalities between contour integrals.,On a pair of equalities between contour integrals.,,"I was looking at this lovely answer to finding the characteristic function of a standard normal random variable. Unfortunately I am stuck on a pair of equalities, the first one is the following: $$ \int_{-L}^L \frac{1}{\sqrt{2\pi}} \mathrm{e}^{-\frac{1}{2} \left( x - j \omega \right)^2 } \mathrm{d} x = \int_{-L-j \omega}^{L-j \omega} \frac{1}{\sqrt{2\pi}} \mathrm{e}^{-\frac{z^2}{2}  } \mathrm{d} z $$ I understand that a change of variable has took place. But I have only ever proven the change of variable formula with real numbers so it's not completely obvious to me why this should work. Is it because the reals are just a subset of the complex field? Moreover the fact that we can write the limits of the complex integral as two points in the complex plane is because the integrand function is holomorphic so we don't care for the path that connects the point, correct? The second one is: $$  \left(\int_{-L-j \omega}^{L-j \omega} \frac{1}{\sqrt{2\pi}} \mathrm{e}^{-\frac{z^2}{2}  } \mathrm{d} z - \int_{-L}^{L} \frac{1}{\sqrt{2\pi}} \mathrm{e}^{-\frac{z^2}{2}  } \mathrm{d} z\right) \\ = -\int_\mathcal{C} \frac{1}{\sqrt{2\pi}} \mathrm{e}^{-\frac{z^2}{2}  } \mathrm{d} z - \int_{L}^{L-j \omega} \frac{1}{\sqrt{2\pi}} \mathrm{e}^{-\frac{z^2}{2}  } \mathrm{d} z - \int_{-L-j \omega}^{-L} \frac{1}{\sqrt{2\pi}} \mathrm{e}^{-\frac{z^2}{2}  } \mathrm{d} z $$ Where the contour $\mathcal{C}$ is given by $-L \to L \to L - j \omega \to -L - j \omega \to -L$ Here I am quite lost, I don't understand how the contour is defined an neither how the equality is achieved. Maybe an extra step in-between the two equalities could help me.","I was looking at this lovely answer to finding the characteristic function of a standard normal random variable. Unfortunately I am stuck on a pair of equalities, the first one is the following: $$ \int_{-L}^L \frac{1}{\sqrt{2\pi}} \mathrm{e}^{-\frac{1}{2} \left( x - j \omega \right)^2 } \mathrm{d} x = \int_{-L-j \omega}^{L-j \omega} \frac{1}{\sqrt{2\pi}} \mathrm{e}^{-\frac{z^2}{2}  } \mathrm{d} z $$ I understand that a change of variable has took place. But I have only ever proven the change of variable formula with real numbers so it's not completely obvious to me why this should work. Is it because the reals are just a subset of the complex field? Moreover the fact that we can write the limits of the complex integral as two points in the complex plane is because the integrand function is holomorphic so we don't care for the path that connects the point, correct? The second one is: $$  \left(\int_{-L-j \omega}^{L-j \omega} \frac{1}{\sqrt{2\pi}} \mathrm{e}^{-\frac{z^2}{2}  } \mathrm{d} z - \int_{-L}^{L} \frac{1}{\sqrt{2\pi}} \mathrm{e}^{-\frac{z^2}{2}  } \mathrm{d} z\right) \\ = -\int_\mathcal{C} \frac{1}{\sqrt{2\pi}} \mathrm{e}^{-\frac{z^2}{2}  } \mathrm{d} z - \int_{L}^{L-j \omega} \frac{1}{\sqrt{2\pi}} \mathrm{e}^{-\frac{z^2}{2}  } \mathrm{d} z - \int_{-L-j \omega}^{-L} \frac{1}{\sqrt{2\pi}} \mathrm{e}^{-\frac{z^2}{2}  } \mathrm{d} z $$ Where the contour $\mathcal{C}$ is given by $-L \to L \to L - j \omega \to -L - j \omega \to -L$ Here I am quite lost, I don't understand how the contour is defined an neither how the equality is achieved. Maybe an extra step in-between the two equalities could help me.",,"['integration', 'complex-analysis', 'complex-integration']"
72,Describe and draw the half-strip $R$ under the mapping $f(z) = z^2$,Describe and draw the half-strip  under the mapping,R f(z) = z^2,"Describe mathematically and draw what happens to the half-strip $R = \{z=x+iy: 0 \leq x \leq 1, y \geq 0 \}$ under the mapping $f(z) = z^2$ I need help with describing and drawing the mapping. solution: For, $z = x+iy$ and $w=f(z)=z^2$ $\Rightarrow w=(x+iy)^2 = (x^2-y^2) + 2xyi$. So $w=u+iv \Rightarrow u=x^2-y^2$ and $v = 2xy$ case i: $u=x^2-y^2 = c_1, c_1 >0$ The graph in the $xy$-plane is the hyperbola cutting the $x$-axis. The $uv$-plane $u=c_1$ represents a vertical line. That is, hyperbolas in the  $xy$-plane are mapped to vertical lines in the $uv$-plane. caseii: $v = 2xy = c_2, c_2>0$ $v=c_2$ represents a horizontal line. The hyperbolas in the  $xy$-plane are mapped to horizontal lines in the $uv$-plane. drawing:","Describe mathematically and draw what happens to the half-strip $R = \{z=x+iy: 0 \leq x \leq 1, y \geq 0 \}$ under the mapping $f(z) = z^2$ I need help with describing and drawing the mapping. solution: For, $z = x+iy$ and $w=f(z)=z^2$ $\Rightarrow w=(x+iy)^2 = (x^2-y^2) + 2xyi$. So $w=u+iv \Rightarrow u=x^2-y^2$ and $v = 2xy$ case i: $u=x^2-y^2 = c_1, c_1 >0$ The graph in the $xy$-plane is the hyperbola cutting the $x$-axis. The $uv$-plane $u=c_1$ represents a vertical line. That is, hyperbolas in the  $xy$-plane are mapped to vertical lines in the $uv$-plane. caseii: $v = 2xy = c_2, c_2>0$ $v=c_2$ represents a horizontal line. The hyperbolas in the  $xy$-plane are mapped to horizontal lines in the $uv$-plane. drawing:",,['complex-analysis']
73,Singular points,Singular points,,"Find all the (isolated) singular points of $f$, classify them, and find the residue of $f$ at each singular point. $$f(z) = \frac{z^{1/2}}{z^2 + 1}$$ I think I have $3$ singularities at $z=0,-i$ and $i$ but am unsure about what to do next and what type of singularities they are. I don't fully understand singularities or residues, can someone explain them to me in this example please.","Find all the (isolated) singular points of $f$, classify them, and find the residue of $f$ at each singular point. $$f(z) = \frac{z^{1/2}}{z^2 + 1}$$ I think I have $3$ singularities at $z=0,-i$ and $i$ but am unsure about what to do next and what type of singularities they are. I don't fully understand singularities or residues, can someone explain them to me in this example please.",,"['complex-analysis', 'residue-calculus', 'singularity']"
74,Calculate the integral $\int_{C_1} e^{X(z+iz-z^2/2)} dz$ as $X \rightarrow \infty$,Calculate the integral  as,\int_{C_1} e^{X(z+iz-z^2/2)} dz X \rightarrow \infty,"I need help calculating the following integral as $X \rightarrow \infty$ $$\int_{C_1} e^{X(z+iz-z^2/2)} dz$$ along the path $$z=x+i\frac{x+1}{x-1},$$ for $x\in (-\infty,-1]$. It seems very difficult. $C_2: z=x+i,x\in\mathbb{R}$, $C_3: z=\infty+iy, y\in [0,1]$, and $C: z=x, \in [-1,\infty)$. So the closed contour is $C=-C_1+C_2-C_3$. The integral along the path $C_2$ I calculated as $$e^{iX}\sqrt{\frac{2\pi }{X}}$$ as $X\rightarrow \infty$.","I need help calculating the following integral as $X \rightarrow \infty$ $$\int_{C_1} e^{X(z+iz-z^2/2)} dz$$ along the path $$z=x+i\frac{x+1}{x-1},$$ for $x\in (-\infty,-1]$. It seems very difficult. $C_2: z=x+i,x\in\mathbb{R}$, $C_3: z=\infty+iy, y\in [0,1]$, and $C: z=x, \in [-1,\infty)$. So the closed contour is $C=-C_1+C_2-C_3$. The integral along the path $C_2$ I calculated as $$e^{iX}\sqrt{\frac{2\pi }{X}}$$ as $X\rightarrow \infty$.",,"['complex-analysis', 'asymptotics', 'contour-integration', 'complex-integration']"
75,Explanation Of Cauchy's Integral Theorem,Explanation Of Cauchy's Integral Theorem,,When we integrate in terms of  real variables over a closed loop  then we get a positive area enclosed by that loop( in 2D). I don't understand why integration of a complex analytic function comes out to be zero.  Please explain this to me. Pardon me if you find this question silly.,When we integrate in terms of  real variables over a closed loop  then we get a positive area enclosed by that loop( in 2D). I don't understand why integration of a complex analytic function comes out to be zero.  Please explain this to me. Pardon me if you find this question silly.,,['complex-analysis']
76,Conformal map from unit disc into itself,Conformal map from unit disc into itself,,"Is it true that any conformal (derivative is not zero) map $f:D(0,1) \rightarrow D(0,1) $ (not necessarily surjective) is in fact a homography (Möbius transformation)?","Is it true that any conformal (derivative is not zero) map $f:D(0,1) \rightarrow D(0,1) $ (not necessarily surjective) is in fact a homography (Möbius transformation)?",,"['complex-analysis', 'complex-numbers', 'analytic-functions']"
77,For what values of $z$ is $\wp(z)$ real-valued?,For what values of  is  real-valued?,z \wp(z),"I am trying to show that the output of the inverse Weierstrass p-function applied to some real argument is real valued. I have $$ \alpha = \wp^{-1}(E_1 + z) $$ where $E_1,z \in \mathbb{R}$. I want to show that $\alpha$ is real-valued. Of course this isn't always true. In my case $g_2, g_3\in\mathbb{R}, \omega_1\in \mathbb{R}$ and $\omega_3\in i\mathbb{R}$. So one counter-example is $E_1 + z = e_3 \in \mathbb{R}$ and $\alpha = \omega_3$. However, I am working in a special case and all of the numerical evidence I have produced implies that in fact $\alpha$ is real valued in this case. I have $E_1 = e_1 + \sqrt{(e_1 - e_2)(e_1 - e_3)}>0$ and am considering $z\geq 0$ (so $E_1 + z > 0$ which is important as mentioned below in the comments). Of course for $z=0, \alpha = \omega_1/2$ and I think this plays a role. In my attempts to show this I have been writing $$ \frac{\omega_1}{2} + \alpha = \wp^{-1}(E_1 + z) $$ and trying to take advantage of the information I have since it clearly is important here. It is also interesting to note that my numerical evidence also points to the exact opposite of this when $z<0$, e.g. $\alpha \in i\mathbb{R}$. I am aware of the Whittaker and Watson exercise which shows that in this case $\wp$ takes on real values on the fundamental lattice, but this does not imply that it doesn't take on real values away from the lattice. Further I am aware that if $\omega_1/2+\alpha \in \mathbb{R}$ is a solution then so is $\omega_1/2+\alpha + 2\omega_3 \notin \mathbb{R}$ so I want an answer mod the lattice.","I am trying to show that the output of the inverse Weierstrass p-function applied to some real argument is real valued. I have $$ \alpha = \wp^{-1}(E_1 + z) $$ where $E_1,z \in \mathbb{R}$. I want to show that $\alpha$ is real-valued. Of course this isn't always true. In my case $g_2, g_3\in\mathbb{R}, \omega_1\in \mathbb{R}$ and $\omega_3\in i\mathbb{R}$. So one counter-example is $E_1 + z = e_3 \in \mathbb{R}$ and $\alpha = \omega_3$. However, I am working in a special case and all of the numerical evidence I have produced implies that in fact $\alpha$ is real valued in this case. I have $E_1 = e_1 + \sqrt{(e_1 - e_2)(e_1 - e_3)}>0$ and am considering $z\geq 0$ (so $E_1 + z > 0$ which is important as mentioned below in the comments). Of course for $z=0, \alpha = \omega_1/2$ and I think this plays a role. In my attempts to show this I have been writing $$ \frac{\omega_1}{2} + \alpha = \wp^{-1}(E_1 + z) $$ and trying to take advantage of the information I have since it clearly is important here. It is also interesting to note that my numerical evidence also points to the exact opposite of this when $z<0$, e.g. $\alpha \in i\mathbb{R}$. I am aware of the Whittaker and Watson exercise which shows that in this case $\wp$ takes on real values on the fundamental lattice, but this does not imply that it doesn't take on real values away from the lattice. Further I am aware that if $\omega_1/2+\alpha \in \mathbb{R}$ is a solution then so is $\omega_1/2+\alpha + 2\omega_3 \notin \mathbb{R}$ so I want an answer mod the lattice.",,"['complex-analysis', 'elliptic-functions']"
78,Taylor series in complex analysis -- change base point,Taylor series in complex analysis -- change base point,,"Develop $$ \sum_{n=1}^{\infty} z^n $$ in a Taylor series, around $z=a$ with $|a|<1.$ What is the new radius of convergence? Solution. Now i think this is a ""clitche"", but i know that $$\frac{1}{1-z}=\sum_{n=1}^{\infty} z^n $$ for $|z|<1.$ Now if i take $$ \frac{1}{1-z} =  \frac{1}{1-a} \, \frac{1}{1-\frac{z-a}{1-a}} = \frac{1}{1-a} \, \sum_{n=1}^{\infty} \left(\frac{z-a}{1-a}\right)^n $$ for $|\frac{z-a}{1-a}|<1$, so $|z-a|<|1-a|$ is the new radius of convergence. Is it right? Cause i'm a little confused. Is this the representation of an analytic function in that disk? Is the analytic continuation of the original function? i had never learn this in a simple words. Thanks for your time.","Develop $$ \sum_{n=1}^{\infty} z^n $$ in a Taylor series, around $z=a$ with $|a|<1.$ What is the new radius of convergence? Solution. Now i think this is a ""clitche"", but i know that $$\frac{1}{1-z}=\sum_{n=1}^{\infty} z^n $$ for $|z|<1.$ Now if i take $$ \frac{1}{1-z} =  \frac{1}{1-a} \, \frac{1}{1-\frac{z-a}{1-a}} = \frac{1}{1-a} \, \sum_{n=1}^{\infty} \left(\frac{z-a}{1-a}\right)^n $$ for $|\frac{z-a}{1-a}|<1$, so $|z-a|<|1-a|$ is the new radius of convergence. Is it right? Cause i'm a little confused. Is this the representation of an analytic function in that disk? Is the analytic continuation of the original function? i had never learn this in a simple words. Thanks for your time.",,"['complex-analysis', 'analysis', 'complex-numbers', 'power-series', 'taylor-expansion']"
79,Calculate Inverse Laplace Transform,Calculate Inverse Laplace Transform,,"I need to get the Inverse Laplace Transform badly for the following function $$\frac{1}{\beta + \sqrt{p}} e^{-\alpha \sqrt{p + \gamma}},$$ $\alpha,\, \beta,\, \gamma$ being some parameters. I have looked through Erdelyi's book of tables and found only the expression for $$\mathcal{L}^{-1}_{p} \left( \frac{1}{\beta + \sqrt{p}} e^{-\alpha \sqrt{p }} \right)(t) = \frac{1}{\sqrt{\pi t}}e^{-\frac{\alpha^{2}}{4t}} - \beta e^{\alpha \beta + \beta^{2} t} \mathbb{Erfc} \left( \frac{\alpha}{2\sqrt{t}} + \beta \sqrt{t} \right).$$ My idea was to understand how to compute the known formula from Erdelyi and then to deduce the one I need. However, I didn't even succeed with understanding the derivation of the known formula. Here is what I have already tried: Bromwich integral; Convolution: $\mathcal{L}^{-1}_{p} \left( \frac{1}{\beta + \sqrt{p}} e^{-\alpha \sqrt{p }} \right) (t) =\mathcal{L}^{-1}_{p} \left( \frac{1}{\beta + \sqrt{p}} \right) * \mathcal{L}^{-1}_{p} \left( e^{-\alpha \sqrt{p}} \right) (t)$; Applying the formula for a square-root: $\mathcal{L}^{-1}_{p} \left( F(\sqrt{p}) \right) (t) = \frac{1}{2\sqrt{\pi} t^{3/2}} \int_{0}^{\infty}x e^{-x^{2}/4t} f(x) dx$, where $F(p) = \frac{1}{\beta + p} e^{-\alpha p}$ and $\mathcal{L}^{-1}_{p} \left( F(p) \right) (t) = f(t) = e^{-\beta (t-\alpha)} I_{ \{ t > \alpha \} }$. But this method is not very useful for performing the initial task because of the absence of the possibility to make a shift in one of two square-roots (as I am aware, only both can be performed simultaneously). In the two first methods, I cannot guarantee that I didn't miss something important and therefore cannot obtain the explicit formulae. To sum the things up, I would appreciate the step by step derivation for both the initial task and for the easier one being the second task.","I need to get the Inverse Laplace Transform badly for the following function $$\frac{1}{\beta + \sqrt{p}} e^{-\alpha \sqrt{p + \gamma}},$$ $\alpha,\, \beta,\, \gamma$ being some parameters. I have looked through Erdelyi's book of tables and found only the expression for $$\mathcal{L}^{-1}_{p} \left( \frac{1}{\beta + \sqrt{p}} e^{-\alpha \sqrt{p }} \right)(t) = \frac{1}{\sqrt{\pi t}}e^{-\frac{\alpha^{2}}{4t}} - \beta e^{\alpha \beta + \beta^{2} t} \mathbb{Erfc} \left( \frac{\alpha}{2\sqrt{t}} + \beta \sqrt{t} \right).$$ My idea was to understand how to compute the known formula from Erdelyi and then to deduce the one I need. However, I didn't even succeed with understanding the derivation of the known formula. Here is what I have already tried: Bromwich integral; Convolution: $\mathcal{L}^{-1}_{p} \left( \frac{1}{\beta + \sqrt{p}} e^{-\alpha \sqrt{p }} \right) (t) =\mathcal{L}^{-1}_{p} \left( \frac{1}{\beta + \sqrt{p}} \right) * \mathcal{L}^{-1}_{p} \left( e^{-\alpha \sqrt{p}} \right) (t)$; Applying the formula for a square-root: $\mathcal{L}^{-1}_{p} \left( F(\sqrt{p}) \right) (t) = \frac{1}{2\sqrt{\pi} t^{3/2}} \int_{0}^{\infty}x e^{-x^{2}/4t} f(x) dx$, where $F(p) = \frac{1}{\beta + p} e^{-\alpha p}$ and $\mathcal{L}^{-1}_{p} \left( F(p) \right) (t) = f(t) = e^{-\beta (t-\alpha)} I_{ \{ t > \alpha \} }$. But this method is not very useful for performing the initial task because of the absence of the possibility to make a shift in one of two square-roots (as I am aware, only both can be performed simultaneously). In the two first methods, I cannot guarantee that I didn't miss something important and therefore cannot obtain the explicit formulae. To sum the things up, I would appreciate the step by step derivation for both the initial task and for the easier one being the second task.",,"['complex-analysis', 'laplace-transform', 'contour-integration', 'convolution']"
80,inverse Laplace transform of a piecewise defined function,inverse Laplace transform of a piecewise defined function,,"I understand the conditions for the existence of the inverse Laplace transforms are $\lim_{s\to\infty}F(s) = 0$ and $\lim_{s\to\infty}(sF(s))<\infty$. I am interested in finding the inverse Laplace transform of a piecewise defined function defined, such as $$F(s) =\begin{cases} 1-s &\text{ if }0\le s\le1 \text{ and}\\  0&\text{ if } s>1 \end{cases}$$  Clearly the limits above do satisfy the existence of the inverse condition, but I'm not sure how to determine the inverse. I'm not sure whether the Bromwich integral method can be applied, since it would appear that if I choose gamma between 0 and 1 the function to integrate is (1-s), whereas if I choose gamma > 1 then the Bromwich integral is obviously 0. I'm also not sure whether Post's inversion formula can be used since I'm not sure I understand how to evaluate high-order derivatives of a function which is not differentiable at $s = 1$. Clearly for a finite $k$, the $k$th order derivative of $F$ exists for all $s$ except $1$, but how about as $k\to\infty$? Finally, just wondering if the two conditions I listed initially (the two limits) are sufficient for the inverse Laplace transform of $F(s)$ to exist.","I understand the conditions for the existence of the inverse Laplace transforms are $\lim_{s\to\infty}F(s) = 0$ and $\lim_{s\to\infty}(sF(s))<\infty$. I am interested in finding the inverse Laplace transform of a piecewise defined function defined, such as $$F(s) =\begin{cases} 1-s &\text{ if }0\le s\le1 \text{ and}\\  0&\text{ if } s>1 \end{cases}$$  Clearly the limits above do satisfy the existence of the inverse condition, but I'm not sure how to determine the inverse. I'm not sure whether the Bromwich integral method can be applied, since it would appear that if I choose gamma between 0 and 1 the function to integrate is (1-s), whereas if I choose gamma > 1 then the Bromwich integral is obviously 0. I'm also not sure whether Post's inversion formula can be used since I'm not sure I understand how to evaluate high-order derivatives of a function which is not differentiable at $s = 1$. Clearly for a finite $k$, the $k$th order derivative of $F$ exists for all $s$ except $1$, but how about as $k\to\infty$? Finally, just wondering if the two conditions I listed initially (the two limits) are sufficient for the inverse Laplace transform of $F(s)$ to exist.",,"['calculus', 'integration', 'complex-analysis', 'laplace-transform']"
81,Better approach for evaluating $\int_{0}^{\infty} \frac{x\sin(\pi x)}{(x^2+4)^3} dx$,Better approach for evaluating,\int_{0}^{\infty} \frac{x\sin(\pi x)}{(x^2+4)^3} dx,"I'm looking for a better approach to compute $\int_{0}^{\infty} \frac{x\sin(\pi x)}{(x^2+4)^3} dx$. Our contour $\mathcal C$ consists of a upper half-plane semicircle and the real axis joining the ends, where we take the limit of the radius to infinity. By Jordan's lemma, the semicircular part of $\mathcal C$ vanishes (first equality). Which leaves us to use the residue theorem (second equality) to compute, $$ \require{cancel} I := \int_{0}^{\infty} \frac{x\sin(\pi x)}{(x^2+4)^3}dx = \frac{1}{2}\oint_{\mathcal C}f(z)dz = \pi \big(R\{f,2i\}\cancel{+R\{f,-2i\} }\big)\ . $$ As both poles are the pole is of third order, we need to take two a second derivative $$ \lim_{z\to 2i} \frac{1}{2!} \frac{d^2}{dz^2}\bigg( (z+2i)^{-3}z\sin(\pi z)  \bigg) $$ in order to find the residue at $z=2i$. Although this seems tedious, we'll give it a try. To simplify the notation, put $a:=(z+2i)^{-1}$, $\ b:= \pi \cos(\pi z)\ $ and $\ c:= \sin(\pi z).$ For the first derivative on the left, this yields $$ (-3a^4 z + a^3)c + a^3bz \ , $$ which we can reorganize as $$ -3a^4 zc + a^3(c + bz) \ .  $$ The second derivative yields $$ 12a^5 cz-3a^4(c+bz) -3a^4(c+bz) + a^3(b-\pi^2 cz + b) , $$ which we can reorganize as $$ 12a^5 cz -6a^4(c+bz) + a^3(2b-\pi^2 cz) \\ = 12(z+2i)^{-5}z\sin(\pi z) -6(z+2i)^{-4}\big(\sin(\pi z) + \pi z \cos(\pi z)\big) +(z+2i)^{-3}\big( \sin(\pi z) + \pi z \cos(\pi z) \big) . $$ Plugging in $z=2i$ yields (if I didn't make any calculation mistakes) $$ R\{f,2i\} = -2^{-7}\big(\sin(2i \pi)(2\pi^2 -3) + i \cos(2i\pi) \big) \ . $$ Wolfram Alpha says the result should be $I = 2^{-7}e^{-2\pi} \pi^2 (1+2\pi)$. Even ìf this approach turns out to work, short of a few calculations mistakes, I don't find it particularly efficient. I'm wondering whether there is a more practical approach for computing the residue $R\{f,2i\}$, or even $I$.","I'm looking for a better approach to compute $\int_{0}^{\infty} \frac{x\sin(\pi x)}{(x^2+4)^3} dx$. Our contour $\mathcal C$ consists of a upper half-plane semicircle and the real axis joining the ends, where we take the limit of the radius to infinity. By Jordan's lemma, the semicircular part of $\mathcal C$ vanishes (first equality). Which leaves us to use the residue theorem (second equality) to compute, $$ \require{cancel} I := \int_{0}^{\infty} \frac{x\sin(\pi x)}{(x^2+4)^3}dx = \frac{1}{2}\oint_{\mathcal C}f(z)dz = \pi \big(R\{f,2i\}\cancel{+R\{f,-2i\} }\big)\ . $$ As both poles are the pole is of third order, we need to take two a second derivative $$ \lim_{z\to 2i} \frac{1}{2!} \frac{d^2}{dz^2}\bigg( (z+2i)^{-3}z\sin(\pi z)  \bigg) $$ in order to find the residue at $z=2i$. Although this seems tedious, we'll give it a try. To simplify the notation, put $a:=(z+2i)^{-1}$, $\ b:= \pi \cos(\pi z)\ $ and $\ c:= \sin(\pi z).$ For the first derivative on the left, this yields $$ (-3a^4 z + a^3)c + a^3bz \ , $$ which we can reorganize as $$ -3a^4 zc + a^3(c + bz) \ .  $$ The second derivative yields $$ 12a^5 cz-3a^4(c+bz) -3a^4(c+bz) + a^3(b-\pi^2 cz + b) , $$ which we can reorganize as $$ 12a^5 cz -6a^4(c+bz) + a^3(2b-\pi^2 cz) \\ = 12(z+2i)^{-5}z\sin(\pi z) -6(z+2i)^{-4}\big(\sin(\pi z) + \pi z \cos(\pi z)\big) +(z+2i)^{-3}\big( \sin(\pi z) + \pi z \cos(\pi z) \big) . $$ Plugging in $z=2i$ yields (if I didn't make any calculation mistakes) $$ R\{f,2i\} = -2^{-7}\big(\sin(2i \pi)(2\pi^2 -3) + i \cos(2i\pi) \big) \ . $$ Wolfram Alpha says the result should be $I = 2^{-7}e^{-2\pi} \pi^2 (1+2\pi)$. Even ìf this approach turns out to work, short of a few calculations mistakes, I don't find it particularly efficient. I'm wondering whether there is a more practical approach for computing the residue $R\{f,2i\}$, or even $I$.",,"['integration', 'complex-analysis', 'improper-integrals', 'contour-integration', 'residue-calculus']"
82,Infinite product of cos and convergence,Infinite product of cos and convergence,,"I am recently self-studying Complex Analysis and came up with a question with regard to infinite products. I am trying to show that $\prod_{k=1} {\cos(z/2^k)}$ converges. My first thought is to use the complex representation of cosine, $\sum (-1)^n z^2n/(2n)!$ But I just don't know how to get to the form $\prod (1+b_n)$ for which has a theorem for convergence. Thanks for all the advice.","I am recently self-studying Complex Analysis and came up with a question with regard to infinite products. I am trying to show that $\prod_{k=1} {\cos(z/2^k)}$ converges. My first thought is to use the complex representation of cosine, $\sum (-1)^n z^2n/(2n)!$ But I just don't know how to get to the form $\prod (1+b_n)$ for which has a theorem for convergence. Thanks for all the advice.",,"['complex-analysis', 'convergence-divergence']"
83,Does every set of orthogonal coordinates in 2D Euclidean space yield a conformally-separable metric?,Does every set of orthogonal coordinates in 2D Euclidean space yield a conformally-separable metric?,,"Consider the 2D Euclidean plane in Cartesian coordinates which has the metric $$ds^2 = dx^2 + dy^2$$ If we transform into polar coordinates , we obtain the metric in the form $$ds^2 = r^2 (\frac{1}{r^2} dr^2 + d\vartheta^2)$$ If we transform into parabolic coordinates , we obtain the metric in the form $$ds^2 = (\sigma^2 + \tau^2)(d\sigma^2 + d\tau^2)$$ Etc. etc. In any case, it seems that every orthogonal coordinate transformation into coordinates $w,z$ transforms the metric into a ""conformally-separable"" form $$ds^2 = \Omega^2(w,z) (W(w) dw^2 + Z(z) dz^2)$$ That is, each of the diagonal metric components are, up to a common conformal factor $\Omega^2$, functions only of the respective coordinates. (This means that e.g. the Laplace equation will be separable in these coordinates.) Is this the case for any analytic 2D orthogonal coordinates? (Can you give a nice canonical reference for further reading?) My attempts: I tried to approach this question by first investigating the question: does every orthogonal coordinate transformation generated by a holomorphic function yield such a metric? Consider a holomorphic function $f(z)$ with $x=\Re (f(z)),\, y=\Im (f(z))$ and the transformed coordinates are $w=\Re (z),\,v=\Im (z)$. This gives us the coordinate transform as $x=x(w,v),\,y=y(w,v)$ The holomorphic nature of $f(z)$ means that it fulfills the Cauchy-Riemann equations which, in this case, can be written as $$\partial_w x= \partial_v y$$ $$\partial_v x= -\partial_w y$$ By transforming the Euclidean metric using this relation we obtain $$d s^2 = [(\partial_w x)^2 + (\partial_w y)^2] d w^2 + [(\partial_v x)^2 + (\partial_v y)^2] d v^2$$ which can be also rewritten as $$d s^2 = |\partial_w f|^2 d w^2 + |\partial_v f|^2 d v^2$$ However, it is not clear how does this amount to the conformal-separable form...","Consider the 2D Euclidean plane in Cartesian coordinates which has the metric $$ds^2 = dx^2 + dy^2$$ If we transform into polar coordinates , we obtain the metric in the form $$ds^2 = r^2 (\frac{1}{r^2} dr^2 + d\vartheta^2)$$ If we transform into parabolic coordinates , we obtain the metric in the form $$ds^2 = (\sigma^2 + \tau^2)(d\sigma^2 + d\tau^2)$$ Etc. etc. In any case, it seems that every orthogonal coordinate transformation into coordinates $w,z$ transforms the metric into a ""conformally-separable"" form $$ds^2 = \Omega^2(w,z) (W(w) dw^2 + Z(z) dz^2)$$ That is, each of the diagonal metric components are, up to a common conformal factor $\Omega^2$, functions only of the respective coordinates. (This means that e.g. the Laplace equation will be separable in these coordinates.) Is this the case for any analytic 2D orthogonal coordinates? (Can you give a nice canonical reference for further reading?) My attempts: I tried to approach this question by first investigating the question: does every orthogonal coordinate transformation generated by a holomorphic function yield such a metric? Consider a holomorphic function $f(z)$ with $x=\Re (f(z)),\, y=\Im (f(z))$ and the transformed coordinates are $w=\Re (z),\,v=\Im (z)$. This gives us the coordinate transform as $x=x(w,v),\,y=y(w,v)$ The holomorphic nature of $f(z)$ means that it fulfills the Cauchy-Riemann equations which, in this case, can be written as $$\partial_w x= \partial_v y$$ $$\partial_v x= -\partial_w y$$ By transforming the Euclidean metric using this relation we obtain $$d s^2 = [(\partial_w x)^2 + (\partial_w y)^2] d w^2 + [(\partial_v x)^2 + (\partial_v y)^2] d v^2$$ which can be also rewritten as $$d s^2 = |\partial_w f|^2 d w^2 + |\partial_v f|^2 d v^2$$ However, it is not clear how does this amount to the conformal-separable form...",,"['euclidean-geometry', 'riemannian-geometry', 'coordinate-systems', 'orthogonality', 'holomorphic-functions']"
84,"How to integrate $\frac{1}{z}$ around square with vertices $(1,1),(-1,1),(1,-1),(-1,-1)$?",How to integrate  around square with vertices ?,"\frac{1}{z} (1,1),(-1,1),(1,-1),(-1,-1)","My attempt $$\int_1^{-1} \frac{1}{x+i} \, dx+\int_1^{-1}    \frac{i}{-1+i y} \, dy+\int_{-1}^1 \frac{1}{x-i} \, dx+\int_{-1}^1 \frac{i}{1+i y} \, dy=2\pi i$$ Is this correct?","My attempt $$\int_1^{-1} \frac{1}{x+i} \, dx+\int_1^{-1}    \frac{i}{-1+i y} \, dy+\int_{-1}^1 \frac{1}{x-i} \, dx+\int_{-1}^1 \frac{i}{1+i y} \, dy=2\pi i$$ Is this correct?",,"['integration', 'complex-analysis', 'ordinary-differential-equations']"
85,Integrability of $f(t) =\frac{2^{\frac{it+1}{1.5}}}{2^{\frac{it+1}{2}}} \frac{\Gamma \left( \frac{it+1}{1.5} \right) }{\Gamma \frac{ it+1}{2} }$,Integrability of,f(t) =\frac{2^{\frac{it+1}{1.5}}}{2^{\frac{it+1}{2}}} \frac{\Gamma \left( \frac{it+1}{1.5} \right) }{\Gamma \frac{ it+1}{2} },"Can we show that the following  function is integrable \begin{align} f(t) =\frac{2^{\frac{it+1}{1.5}}}{2^{\frac{it+1}{2}}} \frac{\Gamma \left( \frac{it+1}{1.5} \right) }{\Gamma \left(\frac{ it+1}{2} \right)}, \end{align} where $t \in \mathbb{R}$ and $i=\sqrt{-1}$. That is can we show that  \begin{align} \int_{-\infty}^{\infty} |f(t)| dt<\infty. \end{align} I was wondering if Stirling's approximation can be used, since this is a complex case? Note if Stirling's approximation can be used than \begin{align} f(t) \approx  \sqrt{\frac{1.5}{2}}\frac{2^{\frac{it+1}{1.5}}}{2^{\frac{it+1}{2}}} \frac{ \left( \frac{it+1}{1.5 e} \right)^{\frac{1+it}{1.5}} }{ \left(\frac{ it+1}{2 e} \right)^{\frac{1+it}{2}}}. \end{align} Note that  \begin{align} |f(t)| &\approx \left| \frac{2^{\frac{it+1}{1.5}}}{2^{\frac{it+1}{2}}}\right| \left|  \frac{ \left( \frac{it+1}{1.5 e} \right)^{\frac{1+it}{ 1.5}} }{ \left(\frac{ it+1}{2 e} \right)^{\frac{1+it}{2}}}\right|\\ &=\left| \frac{2^{\frac{it+1}{1.5}}}{2^{\frac{it+1}{2}}}\right| \left|  \frac{ \left( it+1\right)^\frac{1+it}{1.5} }{ \left( it+1 \right)^{\frac{1+it}{2}}}\right|    \left|\frac{ \left(2 e\right)^{\frac{1+it}{2 }}} {(1.5 e)^{\frac{1+it}{ 1.5}}} \right| \end{align} Also we have that  \begin{align} &\left|\frac{2^{\frac{it+1}{1.5}}}{2^{\frac{it+1}{2}}}\right|= 2^{\frac{2}{3}-\frac{1}{2}} \\ & \left|\frac{ \left(2 e\right)^{\frac{1+it}{2 }}} {(1.5 e)^{\frac{1+it}{ 1.5}}} \right| =\frac{ \left(2 e\right)^{\frac{1}{2 }}} {(1.5 e)^{\frac{1}{ 1.5}}} \end{align} So, in the end we if everthing is corect we have to show that  \begin{align} g(t)=\left|  \frac{ \left( it+1\right)^\frac{1+it}{1.5} }{ \left( it+1 \right)^{\frac{1+it}{2}}}\right|  \end{align} is integrable, but I am not sure how to show if the above equation is integrable or not? Any ideas would be greatly appreciated. Thank you. Edit The integrability of $g(t)$ has been shown in one of the answers. My question now is the following: Since we have that \begin{align} |f(t)| =|g(t)| +e  \end{align} and $g(t)$ is integrable does this mean that $f(t)$ is inegrable?  Can we prove that the error term $e$ is also integrable?","Can we show that the following  function is integrable \begin{align} f(t) =\frac{2^{\frac{it+1}{1.5}}}{2^{\frac{it+1}{2}}} \frac{\Gamma \left( \frac{it+1}{1.5} \right) }{\Gamma \left(\frac{ it+1}{2} \right)}, \end{align} where $t \in \mathbb{R}$ and $i=\sqrt{-1}$. That is can we show that  \begin{align} \int_{-\infty}^{\infty} |f(t)| dt<\infty. \end{align} I was wondering if Stirling's approximation can be used, since this is a complex case? Note if Stirling's approximation can be used than \begin{align} f(t) \approx  \sqrt{\frac{1.5}{2}}\frac{2^{\frac{it+1}{1.5}}}{2^{\frac{it+1}{2}}} \frac{ \left( \frac{it+1}{1.5 e} \right)^{\frac{1+it}{1.5}} }{ \left(\frac{ it+1}{2 e} \right)^{\frac{1+it}{2}}}. \end{align} Note that  \begin{align} |f(t)| &\approx \left| \frac{2^{\frac{it+1}{1.5}}}{2^{\frac{it+1}{2}}}\right| \left|  \frac{ \left( \frac{it+1}{1.5 e} \right)^{\frac{1+it}{ 1.5}} }{ \left(\frac{ it+1}{2 e} \right)^{\frac{1+it}{2}}}\right|\\ &=\left| \frac{2^{\frac{it+1}{1.5}}}{2^{\frac{it+1}{2}}}\right| \left|  \frac{ \left( it+1\right)^\frac{1+it}{1.5} }{ \left( it+1 \right)^{\frac{1+it}{2}}}\right|    \left|\frac{ \left(2 e\right)^{\frac{1+it}{2 }}} {(1.5 e)^{\frac{1+it}{ 1.5}}} \right| \end{align} Also we have that  \begin{align} &\left|\frac{2^{\frac{it+1}{1.5}}}{2^{\frac{it+1}{2}}}\right|= 2^{\frac{2}{3}-\frac{1}{2}} \\ & \left|\frac{ \left(2 e\right)^{\frac{1+it}{2 }}} {(1.5 e)^{\frac{1+it}{ 1.5}}} \right| =\frac{ \left(2 e\right)^{\frac{1}{2 }}} {(1.5 e)^{\frac{1}{ 1.5}}} \end{align} So, in the end we if everthing is corect we have to show that  \begin{align} g(t)=\left|  \frac{ \left( it+1\right)^\frac{1+it}{1.5} }{ \left( it+1 \right)^{\frac{1+it}{2}}}\right|  \end{align} is integrable, but I am not sure how to show if the above equation is integrable or not? Any ideas would be greatly appreciated. Thank you. Edit The integrability of $g(t)$ has been shown in one of the answers. My question now is the following: Since we have that \begin{align} |f(t)| =|g(t)| +e  \end{align} and $g(t)$ is integrable does this mean that $f(t)$ is inegrable?  Can we prove that the error term $e$ is also integrable?",,"['real-analysis', 'integration', 'complex-analysis', 'gamma-function']"
86,Raising Complex Numbers to Complex Powers,Raising Complex Numbers to Complex Powers,,"So I was recently introduced to Maclaurin series, which leads to the inevitable proving of $e^{i\pi} = -1$, and from that the discovery of the equation $e^{ix} = i\sin(x) + \cos(x)$. I noted that using these properties, any real number $r$ can be raised to the complex power $a+bi$ by the equation: $$r^a(i\sin(b\ln(r))+\cos(b\ln(r)))$$ I was interested in raising complex numbers to complex powers, and couldn't find anything on the internet about this. Therefore I tried to generalize the equation to accept complex values of $r$, say $c+di$. From there I got to the equation: $$(c+di)^{(a+bi)}=c^a(i\sin(b\ln(\sqrt{c^2+d^2})) + \cos(b\ln(\sqrt{c^2+d^2})))(\cosh(b\arg(c+di)) - \sinh(b\arg(c+di)))$$ Which I got from using the identities: $$\ln(a+bi) = \ln(\sqrt{a^2+b^2})+i\arg(a+bi)$$ $$\sin(a+bi) = \sin(a)\cosh(b)+i\cos(a)\sinh(b)$$ $$\cos(a+bi) = \cos(a)\cosh(b) + i\sin(a)\sinh(b)$$ From here I ran, tediously, the value $(1+i)^{1+i}$ through my formula and through Google, and they matched, which was both surprising and great. This leads to two questions: In general, does generalization of all sub-functions within a function generalize the entire function? Basically, is the approach I took to generalizing my first equation to my second equation an approach I can expect to work in general? How does this, relate, if at all, to the Wolfram Alpha equation given at http://mathworld.wolfram.com/ComplexExponentiation.html ?","So I was recently introduced to Maclaurin series, which leads to the inevitable proving of $e^{i\pi} = -1$, and from that the discovery of the equation $e^{ix} = i\sin(x) + \cos(x)$. I noted that using these properties, any real number $r$ can be raised to the complex power $a+bi$ by the equation: $$r^a(i\sin(b\ln(r))+\cos(b\ln(r)))$$ I was interested in raising complex numbers to complex powers, and couldn't find anything on the internet about this. Therefore I tried to generalize the equation to accept complex values of $r$, say $c+di$. From there I got to the equation: $$(c+di)^{(a+bi)}=c^a(i\sin(b\ln(\sqrt{c^2+d^2})) + \cos(b\ln(\sqrt{c^2+d^2})))(\cosh(b\arg(c+di)) - \sinh(b\arg(c+di)))$$ Which I got from using the identities: $$\ln(a+bi) = \ln(\sqrt{a^2+b^2})+i\arg(a+bi)$$ $$\sin(a+bi) = \sin(a)\cosh(b)+i\cos(a)\sinh(b)$$ $$\cos(a+bi) = \cos(a)\cosh(b) + i\sin(a)\sinh(b)$$ From here I ran, tediously, the value $(1+i)^{1+i}$ through my formula and through Google, and they matched, which was both surprising and great. This leads to two questions: In general, does generalization of all sub-functions within a function generalize the entire function? Basically, is the approach I took to generalizing my first equation to my second equation an approach I can expect to work in general? How does this, relate, if at all, to the Wolfram Alpha equation given at http://mathworld.wolfram.com/ComplexExponentiation.html ?",,"['calculus', 'complex-analysis', 'proof-verification', 'complex-numbers', 'proof-writing']"
87,Prove the existence of answer:,Prove the existence of answer:,,"Let $K$ be closed and convex. also $F:K \subseteq \mathbb R^n \to \mathbb R^n $ be a continuous function.If foe every $x,y \in K$ we have $(x-y)^T(F(x)-F(y))\ge \alpha ||x-y||^2  ;\alpha\gt0 $ we want to prove that there exist a unique $x^*$ such that $F(x^*)^T(x-x^*)\ge 0$. It is obvious proof of uniqueness of solutions.Assuming there are two answers we have $(x^*-x')^T(F(x')-F(x^*))\ge\alpha ||x-y||^2$ Which is a contradiction. But how to prove that there exist answer?","Let $K$ be closed and convex. also $F:K \subseteq \mathbb R^n \to \mathbb R^n $ be a continuous function.If foe every $x,y \in K$ we have $(x-y)^T(F(x)-F(y))\ge \alpha ||x-y||^2  ;\alpha\gt0 $ we want to prove that there exist a unique $x^*$ such that $F(x^*)^T(x-x^*)\ge 0$. It is obvious proof of uniqueness of solutions.Assuming there are two answers we have $(x^*-x')^T(F(x')-F(x^*))\ge\alpha ||x-y||^2$ Which is a contradiction. But how to prove that there exist answer?",,"['real-analysis', 'complex-analysis', 'functional-analysis', 'analysis']"
88,Evaluate $\displaystyle \int_C \frac{1}{4z^2+4z-3}$ where $C=C^{+}_{1}(0)$.,Evaluate  where .,\displaystyle \int_C \frac{1}{4z^2+4z-3} C=C^{+}_{1}(0),"Evaluate $\displaystyle \int_C \frac{1}{4z^2+4z-3}$ where $C=C^{+}_{1}(0)$. I am not sure whether I understand how to use Cauchy's theorem which states that: If $f$ is a analytic in a simply connected domain $D$, and $C$ is a simple closed contour lying in $D$, then $\displaystyle \int_C f(z)dz=0$. (From this also follows the deformation of contours). Anyways, what I have done so far is in decomposing $f(z)=\frac{1}{4z^2+4z-3}=\frac{1}{4}\left(\frac{1}{2z-1}-\frac{1}{2z+3}\right)$. Now, I look at the singularities of $f$ which are $z=\frac{1}{2},-\frac{3}{2}$. My contour is the circle (with positive orientation) centered at $0$ with radius $1$ (so the integral part of $\frac{1}{4}\frac{1}{2z+3}$ just goes to $0$ by Cauchy). I then make a contour $C_{1/2}$ around the singularity $\frac{1}{2}$ so I can apply the deformation theorem. I'm then left with $\frac{1}{4}{2\pi i}=\frac{\pi}{2}i$. But apparently the answer is $\frac{\pi}{4}i$. Where did I go wrong here? Thanks.","Evaluate $\displaystyle \int_C \frac{1}{4z^2+4z-3}$ where $C=C^{+}_{1}(0)$. I am not sure whether I understand how to use Cauchy's theorem which states that: If $f$ is a analytic in a simply connected domain $D$, and $C$ is a simple closed contour lying in $D$, then $\displaystyle \int_C f(z)dz=0$. (From this also follows the deformation of contours). Anyways, what I have done so far is in decomposing $f(z)=\frac{1}{4z^2+4z-3}=\frac{1}{4}\left(\frac{1}{2z-1}-\frac{1}{2z+3}\right)$. Now, I look at the singularities of $f$ which are $z=\frac{1}{2},-\frac{3}{2}$. My contour is the circle (with positive orientation) centered at $0$ with radius $1$ (so the integral part of $\frac{1}{4}\frac{1}{2z+3}$ just goes to $0$ by Cauchy). I then make a contour $C_{1/2}$ around the singularity $\frac{1}{2}$ so I can apply the deformation theorem. I'm then left with $\frac{1}{4}{2\pi i}=\frac{\pi}{2}i$. But apparently the answer is $\frac{\pi}{4}i$. Where did I go wrong here? Thanks.",,['integration']
89,"$\int_0^\infty \frac{\ln(x^2+1)}{x^{\alpha+1}}dx, 0<\alpha <2 $ using Residue and IBP",using Residue and IBP,"\int_0^\infty \frac{\ln(x^2+1)}{x^{\alpha+1}}dx, 0<\alpha <2 ","I am practicing the examples in alfors and I've been stuck in this problem for a quite few days $$\int_0^\infty \frac{\ln(x^2+1)}{x^{\alpha+1}}dx, 0<\alpha <2$$ What I've done so far was \begin{eqnarray*} \int_0^\infty \frac{\ln(x^2+1)}{x^{\alpha+1}}dx\ &=&\frac{1}{-\alpha} \frac{\ln(x^2+1)}{x^{\alpha}}|_0^\infty  +\frac{1}{\alpha} \int_0^\infty \frac{2x}{(x^2+1)x^{\alpha}}dx\\ &=^L& \frac{1}{(-\alpha)^2}[0-\lim_{x\to 0}\frac{2x}{(x^2+1)x^{\alpha +1}}] +\frac{2}{\alpha(1+e^{-i\pi (\alpha-1)})} \int_0^\infty \frac{(1+e^{i\pi (\alpha-1)})}{(x^2+1)x^{\alpha-1}}dx\\ &=&-\frac{1}{\alpha^2}\lim_{x\to 0}\frac{2}{(x^2+1)x^{\alpha}} +\frac{2}{\alpha(1+e^{-i\pi (\alpha-1)})}[\int_0^\infty \frac{1}{(x^2+1)x^{\alpha-1}}dx +\int_0^\infty \frac{e^{-i\pi (\alpha-1)}}{(x^2+1)x^{\alpha-1}}dx]\\ &=&-\frac{1}{\alpha^2}\lim_{x\to 0}\frac{2}{(x^2+1)x^{\alpha}} +\frac{2}{\alpha(1+e^{-i\pi (\alpha-1)})}[\int_0^\infty \frac{1}{(x^2+1)x^{\alpha-1}}dx +\int_0^\infty \frac{1}{(x^2+1)(xe^{i\pi})^{(\alpha-1)}}dx]\\ &=&-\frac{1}{\alpha^2}\lim_{x\to 0}\frac{2}{(x^2+1)x^{\alpha}} +\frac{2}{\alpha(1+e^{-i\pi (\alpha-1)})}[\int_0^\infty \frac{1}{(x^2+1)x^{\alpha-1}}dx +\int_0^\infty \frac{1}{((-x)^2+1)(-x)^{(\alpha-1)}}dx]\\ &=&-\frac{1}{\alpha^2}\lim_{x\to 0}\frac{2}{(x^2+1)x^{\alpha}} +\frac{2}{\alpha(1+e^{-i\pi (\alpha-1)})}[\int_0^\infty \frac{1}{(x^2+1)x^{\alpha-1}}dx +\int_0^{-\infty} \frac{1}{((x)^2+1)x^{\alpha-1}}(-dx)]\\ &=&-\frac{1}{\alpha^2}\lim_{x\to 0}\frac{2}{(x^2+1)x^{\alpha}} +\frac{2}{\alpha(1+e^{-i\pi (\alpha-1)})}\int_{-\infty}^\infty \frac{1}{(x^2+1)x^{\alpha-1}}dx\\ &=&-\frac{1}{\alpha^2}\lim_{x\to 0}\frac{2}{(x^2+1)x^{\alpha}} +\frac{2}{\alpha(1+e^{-i\pi (\alpha-1)})}[\oint-\int_{|z|=R,R\to \infty\\{Im(z>0)}\\Counterclockwise} -\int_{|z|=r,r\to 0\\{Im(z>0)}\\Clockwise}]\frac{1}{(z^2+1)z^{(\alpha-1)}}dz\\ \end{eqnarray*} Then, I'am stucked........ Any help would be appreciated.","I am practicing the examples in alfors and I've been stuck in this problem for a quite few days $$\int_0^\infty \frac{\ln(x^2+1)}{x^{\alpha+1}}dx, 0<\alpha <2$$ What I've done so far was \begin{eqnarray*} \int_0^\infty \frac{\ln(x^2+1)}{x^{\alpha+1}}dx\ &=&\frac{1}{-\alpha} \frac{\ln(x^2+1)}{x^{\alpha}}|_0^\infty  +\frac{1}{\alpha} \int_0^\infty \frac{2x}{(x^2+1)x^{\alpha}}dx\\ &=^L& \frac{1}{(-\alpha)^2}[0-\lim_{x\to 0}\frac{2x}{(x^2+1)x^{\alpha +1}}] +\frac{2}{\alpha(1+e^{-i\pi (\alpha-1)})} \int_0^\infty \frac{(1+e^{i\pi (\alpha-1)})}{(x^2+1)x^{\alpha-1}}dx\\ &=&-\frac{1}{\alpha^2}\lim_{x\to 0}\frac{2}{(x^2+1)x^{\alpha}} +\frac{2}{\alpha(1+e^{-i\pi (\alpha-1)})}[\int_0^\infty \frac{1}{(x^2+1)x^{\alpha-1}}dx +\int_0^\infty \frac{e^{-i\pi (\alpha-1)}}{(x^2+1)x^{\alpha-1}}dx]\\ &=&-\frac{1}{\alpha^2}\lim_{x\to 0}\frac{2}{(x^2+1)x^{\alpha}} +\frac{2}{\alpha(1+e^{-i\pi (\alpha-1)})}[\int_0^\infty \frac{1}{(x^2+1)x^{\alpha-1}}dx +\int_0^\infty \frac{1}{(x^2+1)(xe^{i\pi})^{(\alpha-1)}}dx]\\ &=&-\frac{1}{\alpha^2}\lim_{x\to 0}\frac{2}{(x^2+1)x^{\alpha}} +\frac{2}{\alpha(1+e^{-i\pi (\alpha-1)})}[\int_0^\infty \frac{1}{(x^2+1)x^{\alpha-1}}dx +\int_0^\infty \frac{1}{((-x)^2+1)(-x)^{(\alpha-1)}}dx]\\ &=&-\frac{1}{\alpha^2}\lim_{x\to 0}\frac{2}{(x^2+1)x^{\alpha}} +\frac{2}{\alpha(1+e^{-i\pi (\alpha-1)})}[\int_0^\infty \frac{1}{(x^2+1)x^{\alpha-1}}dx +\int_0^{-\infty} \frac{1}{((x)^2+1)x^{\alpha-1}}(-dx)]\\ &=&-\frac{1}{\alpha^2}\lim_{x\to 0}\frac{2}{(x^2+1)x^{\alpha}} +\frac{2}{\alpha(1+e^{-i\pi (\alpha-1)})}\int_{-\infty}^\infty \frac{1}{(x^2+1)x^{\alpha-1}}dx\\ &=&-\frac{1}{\alpha^2}\lim_{x\to 0}\frac{2}{(x^2+1)x^{\alpha}} +\frac{2}{\alpha(1+e^{-i\pi (\alpha-1)})}[\oint-\int_{|z|=R,R\to \infty\\{Im(z>0)}\\Counterclockwise} -\int_{|z|=r,r\to 0\\{Im(z>0)}\\Clockwise}]\frac{1}{(z^2+1)z^{(\alpha-1)}}dz\\ \end{eqnarray*} Then, I'am stucked........ Any help would be appreciated.",,"['complex-analysis', 'improper-integrals']"
90,Finding the image of extended lines under the function $f(z)=\bigl(\frac{z+1}{z-1}\bigr)^{2}$ using a Möbius transformation,Finding the image of extended lines under the function  using a Möbius transformation,f(z)=\bigl(\frac{z+1}{z-1}\bigr)^{2},"Given the function $f(z)=\bigl(\frac{z+1}{z-1}\bigr)^{2}$ when $f(1)=\infty,f(\infty)=1$, I need to find the image of the extended real line and the extended imaginray line. In order to do so, I worked with the Möbius transformation $\psi(z)=\frac{z+1}{z-1}$. Finding the image of the extended real line was easy- as a Möbius transformation sends extended circles to extended circles in $\overline{\mathbb{C}}$ which are characterized by three points, I found the image of three points on the extended real line to be on the extended real line (from here I'll use e.r.l), so $\psi(z)$ sends the e.r.l to itself.  $f(z)=(\psi(z))^{2}$ thus sends the e.r.l to the positive half of the e.r.l in $\overline{\mathbb{C}}$. Now, for finding the image of the extended imaginary line (e.i.l) in $\overline{\mathbb{C}}$, I used the same reasoning and got: $\psi(i)=\frac{i+1}{i-1}=\frac{-(i+1)^{2}}{2}, \psi(0)=-1, \psi(-i)=\frac{-i+1}{-i-1}=\frac{-(i-1)^{2}}{2}$. So the e.i.l is sent by $\psi$ to a circle going through $\frac{-(i+1)^{2}}{2},\frac{-(i-1)^{2}}{2},-1$. Before I go through the quite tedious work of finding the equation of the circle, a troubling thought came to my mind. Say I have the circle. I need to find the image under $f(z)=(\psi(z))^2$. Should I square the circle?","Given the function $f(z)=\bigl(\frac{z+1}{z-1}\bigr)^{2}$ when $f(1)=\infty,f(\infty)=1$, I need to find the image of the extended real line and the extended imaginray line. In order to do so, I worked with the Möbius transformation $\psi(z)=\frac{z+1}{z-1}$. Finding the image of the extended real line was easy- as a Möbius transformation sends extended circles to extended circles in $\overline{\mathbb{C}}$ which are characterized by three points, I found the image of three points on the extended real line to be on the extended real line (from here I'll use e.r.l), so $\psi(z)$ sends the e.r.l to itself.  $f(z)=(\psi(z))^{2}$ thus sends the e.r.l to the positive half of the e.r.l in $\overline{\mathbb{C}}$. Now, for finding the image of the extended imaginary line (e.i.l) in $\overline{\mathbb{C}}$, I used the same reasoning and got: $\psi(i)=\frac{i+1}{i-1}=\frac{-(i+1)^{2}}{2}, \psi(0)=-1, \psi(-i)=\frac{-i+1}{-i-1}=\frac{-(i-1)^{2}}{2}$. So the e.i.l is sent by $\psi$ to a circle going through $\frac{-(i+1)^{2}}{2},\frac{-(i-1)^{2}}{2},-1$. Before I go through the quite tedious work of finding the equation of the circle, a troubling thought came to my mind. Say I have the circle. I need to find the image under $f(z)=(\psi(z))^2$. Should I square the circle?",,"['complex-analysis', 'mobius-transformation']"
91,Inequality involving Mobius transformation,Inequality involving Mobius transformation,,"I have the following simple-looking inequality I have to show: Let $z, w \in \mathbb D$, where $\mathbb D$ is the open unit disc in $\mathbb C$. Show that  $$\left| \frac{z-w}{1-\overline{z}w} \right| \geq \left| \frac{|z|-|w|}{1-|z||w|} \right|.$$ It looks pretty straightforward, but I just can't seem to get it, and I think I might be missing something obvious. I've tried putting $z=|z|e^{i \alpha}$ and $w=|w|e^{i \beta}$ to get  $$\left| \frac{z-w}{1-\overline{z}w} \right| = \left| \frac{|z|-|w|e^{i \theta}}{1-|z||w|e^{i \theta}} \right|$$ where $\theta = \beta - \alpha$, and can't get much out of this. I've tried squaring both sides etc., and a few other things. If anyone has any ideas, I'd be very grateful, thanks.","I have the following simple-looking inequality I have to show: Let $z, w \in \mathbb D$, where $\mathbb D$ is the open unit disc in $\mathbb C$. Show that  $$\left| \frac{z-w}{1-\overline{z}w} \right| \geq \left| \frac{|z|-|w|}{1-|z||w|} \right|.$$ It looks pretty straightforward, but I just can't seem to get it, and I think I might be missing something obvious. I've tried putting $z=|z|e^{i \alpha}$ and $w=|w|e^{i \beta}$ to get  $$\left| \frac{z-w}{1-\overline{z}w} \right| = \left| \frac{|z|-|w|e^{i \theta}}{1-|z||w|e^{i \theta}} \right|$$ where $\theta = \beta - \alpha$, and can't get much out of this. I've tried squaring both sides etc., and a few other things. If anyone has any ideas, I'd be very grateful, thanks.",,"['complex-analysis', 'complex-numbers']"
92,"Proving uniqueness in $\mathbb{C}$ of function $f$ verifying $\hspace{0.2cm}f(u+v)=f(u)f(v),\forall u,v\in\mathbb{C},f(1)=e$",Proving uniqueness in  of function  verifying,"\mathbb{C} f \hspace{0.2cm}f(u+v)=f(u)f(v),\forall u,v\in\mathbb{C},f(1)=e","I am trying to prove that the complex exponential function  $$exp\colon\mathbb{C} \to \mathbb{C},z \mapsto exp(z):=\sum_{n=0}^{\infty}\frac{z^n}{n!}$$ is the unique continuous function $\hspace{0.2cm} f\colon \mathbb{C} \to \mathbb{C}$ such that $f(u+v)=f(u)f(v), \forall u,v\in\mathbb{C}$ and $f(1)=\sum_{n=0}^{\infty}\frac{1}{n!}$. I have already proved that the exponential satisfies this condition. My difficulties are only in proving the uniqueness. I have tried several ways to prove it, but not a clue how to do it. Notice the definition of the exponential function I am using. Any ideas?","I am trying to prove that the complex exponential function  $$exp\colon\mathbb{C} \to \mathbb{C},z \mapsto exp(z):=\sum_{n=0}^{\infty}\frac{z^n}{n!}$$ is the unique continuous function $\hspace{0.2cm} f\colon \mathbb{C} \to \mathbb{C}$ such that $f(u+v)=f(u)f(v), \forall u,v\in\mathbb{C}$ and $f(1)=\sum_{n=0}^{\infty}\frac{1}{n!}$. I have already proved that the exponential satisfies this condition. My difficulties are only in proving the uniqueness. I have tried several ways to prove it, but not a clue how to do it. Notice the definition of the exponential function I am using. Any ideas?",,['complex-analysis']
93,Showing a complex polynomial is linear,Showing a complex polynomial is linear,,"I've been stuck on the following exercise: Show that if $p(z)$ is a complex polynomial such that $p(z) \in \mathbb R$ if and only if $z\in \mathbb R$ then $p$ is linear. Here is what I have so far: Because $0$ is real it follows that the roots of $p$ are real. From this it is immediate that the coefficients of $p$ are real. So let $p$ be a polynomial with real coefficients of degree $n$. We can write $p(z) = u(x,y) + i v(x,y)$. Then $v(x,y) = 0$ if and only if $y=0$. Since $p$ is real we have $\overline{p(z)} = p(\overline{z})$ hence $$ u(x,y) = u(x,-y)$$ and $$ -v(x,y) = v(x,-y)$$ Can I use any of these observations to make a proof? Please could   someone show me how to prove this?","I've been stuck on the following exercise: Show that if $p(z)$ is a complex polynomial such that $p(z) \in \mathbb R$ if and only if $z\in \mathbb R$ then $p$ is linear. Here is what I have so far: Because $0$ is real it follows that the roots of $p$ are real. From this it is immediate that the coefficients of $p$ are real. So let $p$ be a polynomial with real coefficients of degree $n$. We can write $p(z) = u(x,y) + i v(x,y)$. Then $v(x,y) = 0$ if and only if $y=0$. Since $p$ is real we have $\overline{p(z)} = p(\overline{z})$ hence $$ u(x,y) = u(x,-y)$$ and $$ -v(x,y) = v(x,-y)$$ Can I use any of these observations to make a proof? Please could   someone show me how to prove this?",,['complex-analysis']
94,Why does this integral converge for all $s \in \mathbb C$?,Why does this integral converge for all ?,s \in \mathbb C,"I'm trying to understand the proof of analytic continuation of the Riemann zeta function.  These notes ( http://math.bu.edu/people/jsweinst/Teaching/MA843/TatesThesis.pdf ) have been very helpful.  I'm right at the end of the proof. Here $\omega(x) = \sum\limits_{n =1}^{\infty} e^{-\pi n^2x}$.  To finish, the proof, we need the fact that that last integral converges for all $s \in \mathbb{C}$, and hence $\Lambda(s) = \pi^{-\frac{s}{2}} \Gamma(\frac{s}{2})\zeta(s)$ admits an analytic continuation to the whole complex plane with poles at $s =0$ and $s = 1$.  Is it obvious that this integral always converges?","I'm trying to understand the proof of analytic continuation of the Riemann zeta function.  These notes ( http://math.bu.edu/people/jsweinst/Teaching/MA843/TatesThesis.pdf ) have been very helpful.  I'm right at the end of the proof. Here $\omega(x) = \sum\limits_{n =1}^{\infty} e^{-\pi n^2x}$.  To finish, the proof, we need the fact that that last integral converges for all $s \in \mathbb{C}$, and hence $\Lambda(s) = \pi^{-\frac{s}{2}} \Gamma(\frac{s}{2})\zeta(s)$ admits an analytic continuation to the whole complex plane with poles at $s =0$ and $s = 1$.  Is it obvious that this integral always converges?",,"['complex-analysis', 'number-theory', 'analytic-number-theory']"
95,"$\int^\infty_0\frac{\sin x}{x} \, dx = \frac{1}{2i}\int^\infty_{-\infty} \frac{e^{ix}-1}{x} \, dx$, why?",", why?","\int^\infty_0\frac{\sin x}{x} \, dx = \frac{1}{2i}\int^\infty_{-\infty} \frac{e^{ix}-1}{x} \, dx","How comes this true? $$\int^\infty_0\frac{\sin x} x \, dx = \frac{1}{2i}\int^\infty_{-\infty} \frac{e^{ix}-1} x \, dx$$","How comes this true? $$\int^\infty_0\frac{\sin x} x \, dx = \frac{1}{2i}\int^\infty_{-\infty} \frac{e^{ix}-1} x \, dx$$",,"['integration', 'complex-analysis']"
96,More direct proof of Cauchy's Integral Formula for simply connected domains,More direct proof of Cauchy's Integral Formula for simply connected domains,,"In the books I looked and also in the script of my former Complex Analysis Prof. we proved the Cauchy Integral Formula for simply connected domains and closed curves in the following order: Goursat's Theorem for Triangles for functions continuous everywhere and holomorphic everywhere except at a single point Cauchy's Theorem for convex domains for functions continuous everywhere and holomorphic everywhere except at a single point Cauchy's Integral Formula for convex domains Cauchy's Integral Theorem for null homologous cycles Introduction of homotopic curves and proof of Cauchy's Integral Theorem for those curves. What I really don't like is two things: First this artificial single point in (1) and (2). It is of course nice to have such a weak version of Goursat and Cauchy Integral Theorem, because then the Integral Formula is easy to derive. But as a student is seems very artificial at first to exclude a single point out of the domain of holomorphy, and then see later that this makes sense and is handy.  Second, I don't like to go first over nice domains like convex or starlike domains and afterwards end up in arbitrary domains. So, what I know is the following version of Cauchy's Integral Theorem: Let $\Omega$ be a simply connected domain, let $f$ be holomorphic in $\Omega$ and let $\gamma$ be a closed curve (piecewise $C^1$) in $\Omega$. Then \begin{align*} \int_\gamma f(z) dz=0. \end{align*} I would like to deduce from this the following version of the Cauchy Integral Formula: Let $\Omega$ be a simply connected domain, let $f$ be holomorphic in $\Omega$ and let $\gamma$ be a closed curve (piecewise $C^1$) in $\Omega$. Then \begin{align*} n(\gamma, z)f(z)=\frac{1}{2\pi i}\int_\gamma \frac{f(w)}{w-z} dw,\qquad z\in\Omega\backslash\gamma. \end{align*} Here, $n(\gamma,z)$ is the winding number of $\gamma$ at $z$. The only way i can think of to achieve my goal is to fix some $z\in\Omega\backslash\gamma$ and cut a line connecting $z$ and the boundary of $\Omega$ out of $\Omega$ to obtain a simply connected domain. Then I need to adjust $\gamma$ so that it does not run trough this slit anymore and goes around $z$. After that I must show that this detour goes to zero if I move along the slit sufficiently close. This is certainly a way to go, but I really don't like how technical this gets if one wants to write down a rigorous proof. My question is: Is there a more elegant way? Once again, I don't know that holomorphic functions have a continuous derivative, that they can be developed in a power series expension and so on. I only have the abovementioned version of Cauchy's Theorem.","In the books I looked and also in the script of my former Complex Analysis Prof. we proved the Cauchy Integral Formula for simply connected domains and closed curves in the following order: Goursat's Theorem for Triangles for functions continuous everywhere and holomorphic everywhere except at a single point Cauchy's Theorem for convex domains for functions continuous everywhere and holomorphic everywhere except at a single point Cauchy's Integral Formula for convex domains Cauchy's Integral Theorem for null homologous cycles Introduction of homotopic curves and proof of Cauchy's Integral Theorem for those curves. What I really don't like is two things: First this artificial single point in (1) and (2). It is of course nice to have such a weak version of Goursat and Cauchy Integral Theorem, because then the Integral Formula is easy to derive. But as a student is seems very artificial at first to exclude a single point out of the domain of holomorphy, and then see later that this makes sense and is handy.  Second, I don't like to go first over nice domains like convex or starlike domains and afterwards end up in arbitrary domains. So, what I know is the following version of Cauchy's Integral Theorem: Let $\Omega$ be a simply connected domain, let $f$ be holomorphic in $\Omega$ and let $\gamma$ be a closed curve (piecewise $C^1$) in $\Omega$. Then \begin{align*} \int_\gamma f(z) dz=0. \end{align*} I would like to deduce from this the following version of the Cauchy Integral Formula: Let $\Omega$ be a simply connected domain, let $f$ be holomorphic in $\Omega$ and let $\gamma$ be a closed curve (piecewise $C^1$) in $\Omega$. Then \begin{align*} n(\gamma, z)f(z)=\frac{1}{2\pi i}\int_\gamma \frac{f(w)}{w-z} dw,\qquad z\in\Omega\backslash\gamma. \end{align*} Here, $n(\gamma,z)$ is the winding number of $\gamma$ at $z$. The only way i can think of to achieve my goal is to fix some $z\in\Omega\backslash\gamma$ and cut a line connecting $z$ and the boundary of $\Omega$ out of $\Omega$ to obtain a simply connected domain. Then I need to adjust $\gamma$ so that it does not run trough this slit anymore and goes around $z$. After that I must show that this detour goes to zero if I move along the slit sufficiently close. This is certainly a way to go, but I really don't like how technical this gets if one wants to write down a rigorous proof. My question is: Is there a more elegant way? Once again, I don't know that holomorphic functions have a continuous derivative, that they can be developed in a power series expension and so on. I only have the abovementioned version of Cauchy's Theorem.",,"['complex-analysis', 'cauchy-integral-formula']"
97,Continuous Holomorphic Function and Max Modulus Principle,Continuous Holomorphic Function and Max Modulus Principle,,"Suppose $U$ is a bounded open subset of the plane and $f$ is continuous on the closure of $U$ and holomorphic on $U$. Show that if $C \geq 0$ and $\begin{vmatrix} f(z) \end{vmatrix} \leq C$ for all $z$ on the boundary of $U$, then $\begin{vmatrix} f(z) \end{vmatrix} \leq C$ for $z \in U$. This problem seems to scream Max Modulus Principle, but I am not sure how to use it here. I am not sure how, a non-constant function cannot attain its maximum is even applicable here.","Suppose $U$ is a bounded open subset of the plane and $f$ is continuous on the closure of $U$ and holomorphic on $U$. Show that if $C \geq 0$ and $\begin{vmatrix} f(z) \end{vmatrix} \leq C$ for all $z$ on the boundary of $U$, then $\begin{vmatrix} f(z) \end{vmatrix} \leq C$ for $z \in U$. This problem seems to scream Max Modulus Principle, but I am not sure how to use it here. I am not sure how, a non-constant function cannot attain its maximum is even applicable here.",,"['complex-analysis', 'holomorphic-functions']"
98,Where is $f(x+iy) = x^2y^2$ complex differentiable?,Where is  complex differentiable?,f(x+iy) = x^2y^2,"Here's my proof attempt: $$f(x+iy) = x^2y^2$$ Applying Cauchy Riemann conditions: $$\frac{\partial u}{\partial x} = 2xy^2 = \frac{\partial v}{\partial y} = 0$$ only when $x=0$ or $y=0$. The second condition is: $$\frac{\partial v}{\partial x} = 0 = -\frac{\partial u}{\partial y} = 2yx^2$$ only when $x=0$ or $y=0$. Here, I do not have $x=0$ and $y=0$ at the same time, so what can I say about the set where this function can be complex differentiable? Since the partial derivatives are continuous, this funciton will be differentiable wherever the Cauchy Riemann equations are satisfied.","Here's my proof attempt: $$f(x+iy) = x^2y^2$$ Applying Cauchy Riemann conditions: $$\frac{\partial u}{\partial x} = 2xy^2 = \frac{\partial v}{\partial y} = 0$$ only when $x=0$ or $y=0$. The second condition is: $$\frac{\partial v}{\partial x} = 0 = -\frac{\partial u}{\partial y} = 2yx^2$$ only when $x=0$ or $y=0$. Here, I do not have $x=0$ and $y=0$ at the same time, so what can I say about the set where this function can be complex differentiable? Since the partial derivatives are continuous, this funciton will be differentiable wherever the Cauchy Riemann equations are satisfied.",,"['calculus', 'complex-analysis', 'multivariable-calculus', 'derivatives']"
99,Principal value of complete elliptic integral of third kind,Principal value of complete elliptic integral of third kind,,"Q: How to evaluate the following integral? $$ PV\int_k^1\frac{dt}{(x^2-t^2)\sqrt{(t^2-k^2)(1-t^2)}} $$ where $k<x<1$. For the sake of definiteness we may assume $x=0.8, k=0.5$. Attempt: The substitution $t^2=k^2\cos^2\theta+\sin^2\theta$ can be used to transform the integral into $$ {1\over x^2-1}\int_0^{\pi\over2}\frac{d\theta}{\left(1-\frac{1-k^2}{1-x^2}\sin^2\theta\right)\sqrt{1-(1-k^2)\sin^2\theta}} $$ which can be expressed as the complete elliptic integral of third kind $$ {1\over x^2-1}\Pi\left(\frac{1-k^2}{1-x^2},\sqrt{1-k^2}\right) $$ if $$\frac{1-k^2}{1-x^2}<1\iff x^2<k^2$$ in which case there would not be any singularity in the denominator. But in our case this is not true and the integrand is singular at $$\theta=\arcsin\left(\sqrt{\frac{1-x^2}{1-k^2}}\right)$$ Can someone show me the right contour for this integral and the nuances that come with it?","Q: How to evaluate the following integral? $$ PV\int_k^1\frac{dt}{(x^2-t^2)\sqrt{(t^2-k^2)(1-t^2)}} $$ where $k<x<1$. For the sake of definiteness we may assume $x=0.8, k=0.5$. Attempt: The substitution $t^2=k^2\cos^2\theta+\sin^2\theta$ can be used to transform the integral into $$ {1\over x^2-1}\int_0^{\pi\over2}\frac{d\theta}{\left(1-\frac{1-k^2}{1-x^2}\sin^2\theta\right)\sqrt{1-(1-k^2)\sin^2\theta}} $$ which can be expressed as the complete elliptic integral of third kind $$ {1\over x^2-1}\Pi\left(\frac{1-k^2}{1-x^2},\sqrt{1-k^2}\right) $$ if $$\frac{1-k^2}{1-x^2}<1\iff x^2<k^2$$ in which case there would not be any singularity in the denominator. But in our case this is not true and the integrand is singular at $$\theta=\arcsin\left(\sqrt{\frac{1-x^2}{1-k^2}}\right)$$ Can someone show me the right contour for this integral and the nuances that come with it?",,"['complex-analysis', 'definite-integrals', 'contour-integration', 'elliptic-integrals', 'cauchy-principal-value']"
