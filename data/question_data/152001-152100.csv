,title_raw,title_text,title_latex,body_raw,body_text,body_latex,tags
0,"Action of Calderon-Zygmund operator on $H^{1,\infty}$",Action of Calderon-Zygmund operator on,"H^{1,\infty}","I am trying to understand the theorem in Jean-Lin Journe's book Calderon-Zygmund Operators, Pseudo-Differential Operators and the Cauchy Integral of Calderon . On page 49, the theorem says: Let $T$ be associated to a standard kernel. Then the following are   equivalent: $T$ satisfies: for $Q \subset \mathbb{R}^n$ , $a \in L^\infty(\mathbb{R}^n)$ that is supported in $Q$ , we have $$\int_{\overline{Q}} |Ta|\, dx \leq C||a||_\infty |Q|.$$ Here $Q$ is   a cube and $\overline{Q}$ is the concentric cube with twice the   radius. $T$ is bounded map from $H^{1,\infty}$ into $L^1$ . $T$ is a bounded map from $L_c^\infty$ to BMO. My question lies in the implication from 3 to 2. I understand that for any $\infty$ -atom $a$ ,i.e., $supp(a) \subset Q$ and $||a||_\infty \leq \frac{1}{|Q|}$ , $$||Ta||_1 \leq C.$$ However, I do not see how this implies $||Ta||_1 \leq C||a||_{H^{1,\infty}}$ . More precisely, since $||a||_{H^{1,\infty}} \leq 1$ for this $\infty$ -atom, how do we know that $C \leq C'\cdot||a||_{H^{1,\infty}}$ ?","I am trying to understand the theorem in Jean-Lin Journe's book Calderon-Zygmund Operators, Pseudo-Differential Operators and the Cauchy Integral of Calderon . On page 49, the theorem says: Let be associated to a standard kernel. Then the following are   equivalent: satisfies: for , that is supported in , we have Here is   a cube and is the concentric cube with twice the   radius. is bounded map from into . is a bounded map from to BMO. My question lies in the implication from 3 to 2. I understand that for any -atom ,i.e., and , However, I do not see how this implies . More precisely, since for this -atom, how do we know that ?","T T Q \subset \mathbb{R}^n a \in L^\infty(\mathbb{R}^n) Q \int_{\overline{Q}} |Ta|\, dx \leq C||a||_\infty |Q|. Q \overline{Q} T H^{1,\infty} L^1 T L_c^\infty \infty a supp(a) \subset Q ||a||_\infty \leq \frac{1}{|Q|} ||Ta||_1 \leq C. ||Ta||_1 \leq C||a||_{H^{1,\infty}} ||a||_{H^{1,\infty}} \leq 1 \infty C \leq C'\cdot||a||_{H^{1,\infty}}","['real-analysis', 'functional-analysis', 'analysis', 'harmonic-analysis', 'singular-integrals']"
1,Interchanging the order of summation in a double sequence,Interchanging the order of summation in a double sequence,,"I encountered the following change of summation $\sum_{i=1}^ \infty \sum_{j=1}^ \infty  a_{i,j} = \sum_{j=1}^ \infty \sum_{i=1}^ \infty  a_{i,j} $ , in Ahtreya and Lahiri's ""Measure theory and Probability theory"", page 16, where it says that this is allowed since the summands $a_{i,j}$ are non-negative. I have encountered the same argument in other books of Measure theory, but never saw a proof in the appendix neither a reference of it. From a web search I made, it seems that Fubini's theorem for double integrals in $R^n$ , or, for a measurable function on a product measure space, is proved, and resembles very much my question. However, I cannot find a proof exactly for double sequences, as is my original question. This is quite strange, since this should be considerably easier to prove. Can you kindly point me to one? Could you outline the proof, alternatively? Thanks a lot.","I encountered the following change of summation , in Ahtreya and Lahiri's ""Measure theory and Probability theory"", page 16, where it says that this is allowed since the summands are non-negative. I have encountered the same argument in other books of Measure theory, but never saw a proof in the appendix neither a reference of it. From a web search I made, it seems that Fubini's theorem for double integrals in , or, for a measurable function on a product measure space, is proved, and resembles very much my question. However, I cannot find a proof exactly for double sequences, as is my original question. This is quite strange, since this should be considerably easier to prove. Can you kindly point me to one? Could you outline the proof, alternatively? Thanks a lot.","\sum_{i=1}^ \infty \sum_{j=1}^ \infty  a_{i,j} = \sum_{j=1}^ \infty \sum_{i=1}^ \infty  a_{i,j}  a_{i,j} R^n","['sequences-and-series', 'analysis']"
2,Limit distribution equal to Dirac delta,Limit distribution equal to Dirac delta,,"This is the problem 6.19 from the book Distributions and Operators, Gerd Grubb. I already have done parts (a) and (b). The part (a) of this problem is proving that for $r\in(0,1]$ , the sequence $$\{\frac{1}{2\pi}\sum_{n=-N}^{N}r^{|n|}e^{inx}\}_{N\in\mathbb{N}}$$ converges to a distribution $P_{r}$ in $D'((-\pi,\pi))$ and that $P_{1}=\delta$ . Part (b) is just showing that $r\mapsto P_{r}$ is continuous. Now, for part (c), I have to show that when $r$ converges to $1$ from the left, then $$\int_{-\pi}^{\pi}\frac{1-r^2}{1-2r\cos\theta+r^2}\;\varphi(\theta)\;d\theta$$ converges to $\varphi(0)$ for any $\varphi\in C_{0}^{\infty}((-\pi,\pi))$ . I thought about this for a while but I don't have a clue. Thanks for the help.","This is the problem 6.19 from the book Distributions and Operators, Gerd Grubb. I already have done parts (a) and (b). The part (a) of this problem is proving that for , the sequence converges to a distribution in and that . Part (b) is just showing that is continuous. Now, for part (c), I have to show that when converges to from the left, then converges to for any . I thought about this for a while but I don't have a clue. Thanks for the help.","r\in(0,1] \{\frac{1}{2\pi}\sum_{n=-N}^{N}r^{|n|}e^{inx}\}_{N\in\mathbb{N}} P_{r} D'((-\pi,\pi)) P_{1}=\delta r\mapsto P_{r} r 1 \int_{-\pi}^{\pi}\frac{1-r^2}{1-2r\cos\theta+r^2}\;\varphi(\theta)\;d\theta \varphi(0) \varphi\in C_{0}^{\infty}((-\pi,\pi))","['limits', 'analysis', 'distribution-theory', 'dirac-delta']"
3,Borsuk–Ulam theorem proof using Brouwer degree,Borsuk–Ulam theorem proof using Brouwer degree,,"I wonder if Borsuk–Ulam theorem (if $f:\mathbb{S}^n\rightarrow\mathbb{R}^n$ is continuous, then exists $x_0\in\mathbb{S}^n$ such that $f(x_0)=f(-x_0)$ ) can be sucesfully proved by using the Brouwer degree. My attempt is to find an homotopy from the function $f(x)-f(-x)$ to another suitable one in order to apply the invariance under homotopy of the degree and conclude that the degree of the considered function in a certain open set and in a certain point is not zero (which implies that the function $f(x)-f(-x)$ has a zero.","I wonder if Borsuk–Ulam theorem (if is continuous, then exists such that ) can be sucesfully proved by using the Brouwer degree. My attempt is to find an homotopy from the function to another suitable one in order to apply the invariance under homotopy of the degree and conclude that the degree of the considered function in a certain open set and in a certain point is not zero (which implies that the function has a zero.",f:\mathbb{S}^n\rightarrow\mathbb{R}^n x_0\in\mathbb{S}^n f(x_0)=f(-x_0) f(x)-f(-x) f(x)-f(-x),"['general-topology', 'analysis', 'roots', 'geometric-topology']"
4,Determine the number of point $z \in \mathbb{C}$ such that $(2z+i\overline{z})^3=27i$,Determine the number of point  such that,z \in \mathbb{C} (2z+i\overline{z})^3=27i,"I've just learned complex numbers in Mathematical Analysis 1, and I'm stuck in the following problem: I would like to determine the number of point $z \in \mathbb{C}$ such that $(2z+i\overline{z})^3=27i$ , and solve the following system of equations: $\begin{cases}\begin{matrix} (2z+i\overline{z})^3=27i \\ Re(z)\geq Im(z) \end{matrix}\end{cases}$ . Can someone help me explaining in detail the steps? Thank you very much!","I've just learned complex numbers in Mathematical Analysis 1, and I'm stuck in the following problem: I would like to determine the number of point such that , and solve the following system of equations: . Can someone help me explaining in detail the steps? Thank you very much!",z \in \mathbb{C} (2z+i\overline{z})^3=27i \begin{cases}\begin{matrix} (2z+i\overline{z})^3=27i \\ Re(z)\geq Im(z) \end{matrix}\end{cases},"['real-analysis', 'complex-analysis', 'analysis', 'complex-numbers']"
5,Interpolation inequality on $L^p$ space,Interpolation inequality on  space,L^p,"Let $\Omega$ be a bounded domain in $\mathbb{R}^n$ . If I already know the interpolation inequality that said $$ \|u\|_{L^q(\Omega)} \leq \|u\|^{\lambda}_{L^p(\Omega)} \|u\|^{1-\lambda}_{L^r(\Omega)} $$ for $1 \leq p \leq q \leq r$ satisfying $\frac{1}{q} = \frac{\lambda}{p} + \frac{1-\lambda}{r}$ with $0 \leq \lambda \leq 1$ . How can I derive the following inequality $$ \|u\|_{L^{\frac{2q}{q-1}}(B_1)} \leq \varepsilon \|u\|_{L^{\frac{2n}{n-2}}(B_1)} + C(n,q) \varepsilon^{- \frac{n}{2q-n}} \|u\|_{L^{2}(B_1)} $$ for $2 < \frac{2q}{q-1} < \frac{2n}{n-2}$ , and $B_1$ being the unit ball in $\mathbb{R}^n$ ? I think this is standard, but I do not have a good idea to split the multiplication to fit the power.","Let be a bounded domain in . If I already know the interpolation inequality that said for satisfying with . How can I derive the following inequality for , and being the unit ball in ? I think this is standard, but I do not have a good idea to split the multiplication to fit the power.","\Omega \mathbb{R}^n 
\|u\|_{L^q(\Omega)} \leq \|u\|^{\lambda}_{L^p(\Omega)} \|u\|^{1-\lambda}_{L^r(\Omega)}
 1 \leq p \leq q \leq r \frac{1}{q} = \frac{\lambda}{p} + \frac{1-\lambda}{r} 0 \leq \lambda \leq 1 
\|u\|_{L^{\frac{2q}{q-1}}(B_1)} \leq \varepsilon \|u\|_{L^{\frac{2n}{n-2}}(B_1)} + C(n,q) \varepsilon^{- \frac{n}{2q-n}} \|u\|_{L^{2}(B_1)}
 2 < \frac{2q}{q-1} < \frac{2n}{n-2} B_1 \mathbb{R}^n","['functional-analysis', 'analysis', 'partial-differential-equations', 'sobolev-spaces']"
6,"$\lim_{a\to 0}\|\tau_a f-f\|_\infty=0$, then there is some $f=g$ a.e. such that $g\in BUC(\Bbb R^n,E)$",", then there is some  a.e. such that","\lim_{a\to 0}\|\tau_a f-f\|_\infty=0 f=g g\in BUC(\Bbb R^n,E)","I want to check my proof. Let $\tau_a:E^{\Bbb R^n}\to E^{\Bbb R^n}$ such that $\tau_a f(x)=f(x-a)$ for all $x\in\Bbb R^n$ and any chosen $f\in E^{\Bbb R^n}$ . Then the exercise says Prove that if $\lim_{a\to 0}\|\tau_a f-f\|_\infty=0$ then there is some $g=f$ almost everywhere such that $g$ is bounded and uniformly continuous. Here $\|{\cdot}\|_\infty$ is the seminorm of the space $\mathcal L_\infty(\Bbb R^n,\lambda_n,E)$ , where $\lambda_n$ is the Lebesgue measure and $E$ is a Banach space. My proof: The statement to be proved is not true in it generality. Let $f(x):=x$ (we choose here $E=\Bbb R^n$ ), then clearly $\|\tau_a f-f\|_\infty=|a|$ so $\lim_{a\to 0}\|\tau_af-f\|_\infty=0$ , however $f$ is not essentially bounded, so I assume that we need the assumption that $f\in\mathcal L_\infty(\Bbb R^n,\lambda_n,E)$ . If $f\in\mathcal L_\infty(\Bbb R^n,\lambda_n,E)$ then clearly $f$ is essentially bounded, so there is some $g\in\mathcal L_\infty(\Bbb R^n,\lambda_n,E)$ such that $f=g$ a.e. such that $g$ is bounded. Now suppose that $f$ is not uniformly continuous a.e., then for any chosen null set $N$ we have that $$ \exists \epsilon_0>0,\;\forall \delta>0,\;\exists x,y\in N^\complement:|x-y|<\delta\,\land\,|f(x)-f(y)|\ge\epsilon_0\tag1 $$ Then setting $|x-y|=|a|$ we find that $$ \exists \epsilon_0>0,\;\forall \delta>0,\;\exists a\in\Bbb R^n,\;\exists x_a\in N^\complement:|a|<\delta\;\land\;|\tau_a f(x_a)-f(x_a)|\ge\epsilon_0\tag2 $$ But then we find that $\|\tau_a f-f\|_\infty\ge\epsilon_0$ for arbitrarily small $|a|>0$ in any chosen subset of $\Bbb R^n$ with null complement, what contradicts the assumption that $\lim_{a\to 0}\|\tau_a f-f\|_\infty=0$ , so we conclude that $f$ is also uniformly continuous a.e. Now we need to show that if $f$ is uniformly continuous a.e. then there is some $g=f$ a.e. such that $g$ is uniformly continuous. Let $N$ the null set such that $f$ is uniformly continuous in $N^\complement$ and suppose that $\overline{N^\complement}\neq\Bbb R^n$ , then $(\overline{N^\complement})^\complement$ is open and not empty, and because the Lebesgue measure is massive then $\mu((\overline{N^\complement})^\complement)>0$ what contradicts the completeness of the Lebesgue measure because $(\overline{N^\complement})^\complement\subset N$ , hence $N^\complement$ is dense in $\Bbb R^n$ . Then by a previous result I know that $f|_{N^\complement}$ can be extended continuously on $N$ , so we can conclude that such $g\in BUC(\Bbb R^n,E)$ exists. $\Box$","I want to check my proof. Let such that for all and any chosen . Then the exercise says Prove that if then there is some almost everywhere such that is bounded and uniformly continuous. Here is the seminorm of the space , where is the Lebesgue measure and is a Banach space. My proof: The statement to be proved is not true in it generality. Let (we choose here ), then clearly so , however is not essentially bounded, so I assume that we need the assumption that . If then clearly is essentially bounded, so there is some such that a.e. such that is bounded. Now suppose that is not uniformly continuous a.e., then for any chosen null set we have that Then setting we find that But then we find that for arbitrarily small in any chosen subset of with null complement, what contradicts the assumption that , so we conclude that is also uniformly continuous a.e. Now we need to show that if is uniformly continuous a.e. then there is some a.e. such that is uniformly continuous. Let the null set such that is uniformly continuous in and suppose that , then is open and not empty, and because the Lebesgue measure is massive then what contradicts the completeness of the Lebesgue measure because , hence is dense in . Then by a previous result I know that can be extended continuously on , so we can conclude that such exists.","\tau_a:E^{\Bbb R^n}\to E^{\Bbb R^n} \tau_a f(x)=f(x-a) x\in\Bbb R^n f\in E^{\Bbb R^n} \lim_{a\to 0}\|\tau_a f-f\|_\infty=0 g=f g \|{\cdot}\|_\infty \mathcal L_\infty(\Bbb R^n,\lambda_n,E) \lambda_n E f(x):=x E=\Bbb R^n \|\tau_a f-f\|_\infty=|a| \lim_{a\to 0}\|\tau_af-f\|_\infty=0 f f\in\mathcal L_\infty(\Bbb R^n,\lambda_n,E) f\in\mathcal L_\infty(\Bbb R^n,\lambda_n,E) f g\in\mathcal L_\infty(\Bbb R^n,\lambda_n,E) f=g g f N 
\exists \epsilon_0>0,\;\forall \delta>0,\;\exists x,y\in N^\complement:|x-y|<\delta\,\land\,|f(x)-f(y)|\ge\epsilon_0\tag1
 |x-y|=|a| 
\exists \epsilon_0>0,\;\forall \delta>0,\;\exists a\in\Bbb R^n,\;\exists x_a\in N^\complement:|a|<\delta\;\land\;|\tau_a f(x_a)-f(x_a)|\ge\epsilon_0\tag2
 \|\tau_a f-f\|_\infty\ge\epsilon_0 |a|>0 \Bbb R^n \lim_{a\to 0}\|\tau_a f-f\|_\infty=0 f f g=f g N f N^\complement \overline{N^\complement}\neq\Bbb R^n (\overline{N^\complement})^\complement \mu((\overline{N^\complement})^\complement)>0 (\overline{N^\complement})^\complement\subset N N^\complement \Bbb R^n f|_{N^\complement} N g\in BUC(\Bbb R^n,E) \Box","['analysis', 'proof-verification', 'lebesgue-measure', 'lp-spaces']"
7,Fractional embedding inequality with $L^{\infty}$ norm,Fractional embedding inequality with  norm,L^{\infty},"Here we consider the fractional Sobolev spaces and suppose $u$ is a vector function in $\mathbb R^2$ . Is the following always true? $$\Vert Du \Vert_{L^{\infty}(\mathbb R^2)} \leq C\Vert Du \Vert^{1-\alpha}_{L^{q}(\mathbb R^2)}\Vert D^{\beta}u \Vert^{\alpha}_{L^{q}(\mathbb R^2)}$$ where we can calculate the relation between $\alpha$ , $\beta$ and $q$ by scaling.","Here we consider the fractional Sobolev spaces and suppose is a vector function in . Is the following always true? where we can calculate the relation between , and by scaling.",u \mathbb R^2 \Vert Du \Vert_{L^{\infty}(\mathbb R^2)} \leq C\Vert Du \Vert^{1-\alpha}_{L^{q}(\mathbb R^2)}\Vert D^{\beta}u \Vert^{\alpha}_{L^{q}(\mathbb R^2)} \alpha \beta q,"['real-analysis', 'analysis', 'partial-differential-equations', 'sobolev-spaces', 'fractional-sobolev-spaces']"
8,Linear Independence of Complex Absolute Value Function,Linear Independence of Complex Absolute Value Function,,"Given $n$ different complex numbers $z_1,z_2\cdots z_n$ , $n$ real numbers $a_1,a_2\cdots a_n$ and a constant $C$ , if $$\sum_{i=1}^{n}a_i\left|z-z_i\right|=C$$ for every complex number $z$ in a region $\Omega$ on the complex plane, will it imply that $a_i=C=0\quad\left(i=1,2,\cdots n\right)$ ? i.e. functions $\left|z-z_i\right|$ are linearly independent in the space $\mathbb{R}^\Omega$ ? If not, does there exist a counterexample? I think it’s unlikely to be false, because intuition tells me the equation $\sum_{i=1}^{n}a_i\left|z-z_i\right|=C$ (if non-trivial) always represents a curve in $\Omega$ . I noticed recently that it can be converted into the following question: If $$\sum_{i=1}^n a_i\frac{\left(z-z_i\right)}{\left|z-z_i\right|}=0$$ for all $z\in \Omega$ , prove $a_i=0,\quad i=1,2\cdots n$ . My thought is to pick $z=Z_1,Z_2\cdots Z_n$ such that the matrix $$\left(\frac{\Re\left(Z_i-z_j\right)}{\left|Z_i-z_j\right|}\right)_{n\times n}\quad or \quad\left(\frac{\Im\left(Z_i-z_j\right)}{\left|Z_i-z_j\right|}\right)_{n\times n} $$ has nonzero determinant.","Given different complex numbers , real numbers and a constant , if for every complex number in a region on the complex plane, will it imply that ? i.e. functions are linearly independent in the space ? If not, does there exist a counterexample? I think it’s unlikely to be false, because intuition tells me the equation (if non-trivial) always represents a curve in . I noticed recently that it can be converted into the following question: If for all , prove . My thought is to pick such that the matrix has nonzero determinant.","n z_1,z_2\cdots z_n n a_1,a_2\cdots a_n C \sum_{i=1}^{n}a_i\left|z-z_i\right|=C z \Omega a_i=C=0\quad\left(i=1,2,\cdots n\right) \left|z-z_i\right| \mathbb{R}^\Omega \sum_{i=1}^{n}a_i\left|z-z_i\right|=C \Omega \sum_{i=1}^n a_i\frac{\left(z-z_i\right)}{\left|z-z_i\right|}=0 z\in \Omega a_i=0,\quad i=1,2\cdots n z=Z_1,Z_2\cdots Z_n \left(\frac{\Re\left(Z_i-z_j\right)}{\left|Z_i-z_j\right|}\right)_{n\times n}\quad or \quad\left(\frac{\Im\left(Z_i-z_j\right)}{\left|Z_i-z_j\right|}\right)_{n\times n} ","['linear-algebra', 'complex-analysis', 'analysis']"
9,Complex Square root bound,Complex Square root bound,,"Under what conditions on $a, b$ and the branch cut of the square root does the following hold: $$ |\sqrt{a + b} - \sqrt{a} | \leq C \frac{|b|}{\sqrt{|a| + |b|}} $$ for a universal constant $C > 0$ .  This seems to be true in the real case, but I'm not familiar enough with the subtleties in the complex plane to generalize this.","Under what conditions on and the branch cut of the square root does the following hold: for a universal constant .  This seems to be true in the real case, but I'm not familiar enough with the subtleties in the complex plane to generalize this.","a, b 
|\sqrt{a + b} - \sqrt{a} | \leq C \frac{|b|}{\sqrt{|a| + |b|}}
 C > 0","['complex-analysis', 'analysis', 'inequality']"
10,Turning a product into a sum,Turning a product into a sum,,"Is it possible to change $$\prod_{i=1}^n(1+2a_ib_i),$$ where all elements are contained in an unital associative algebra generated by $a_i,b_i$, $i=1,...,n$, such that $a_ib_i=-b_ia_i$, into a sum which contains the summand $$\sum_{i=1}^n 2a_ib_i?$$ There is no special background for this question it is just about what I am asking. Thank you very much.","Is it possible to change $$\prod_{i=1}^n(1+2a_ib_i),$$ where all elements are contained in an unital associative algebra generated by $a_i,b_i$, $i=1,...,n$, such that $a_ib_i=-b_ia_i$, into a sum which contains the summand $$\sum_{i=1}^n 2a_ib_i?$$ There is no special background for this question it is just about what I am asking. Thank you very much.",,"['abstract-algebra', 'analysis']"
11,"Function $f:[0,1] \to [0,1]$ taking on each value in $[0,1]$ exactly twice",Function  taking on each value in  exactly twice,"f:[0,1] \to [0,1] [0,1]","I want to find a function $f:[0,1] \to [0,1]$ such that $f$ takes on each value in $[0,1]$ exactly twice. I think this means there are an infinite number of discontinuities. Can anyone help me figure this one out? Anyone have any pointers?","I want to find a function $f:[0,1] \to [0,1]$ such that $f$ takes on each value in $[0,1]$ exactly twice. I think this means there are an infinite number of discontinuities. Can anyone help me figure this one out? Anyone have any pointers?",,['real-analysis']
12,How do not generate a $\sigma$-algebra,How do not generate a -algebra,\sigma,"Let  $\mathscr{C}\subset \mathscr{P}(\Omega)$ be  a class of subsets of a nonempty set $\Omega$ containing $\Omega$ and $\varnothing$. Define $\mathscr{C}_0=\mathscr{C}$ and for each  $n\geq 1$ define  $$ \mathscr{C}_{n+1}=\mathscr{C}_n\cup \{A^c:~ A\in \mathscr{C}_n\}\cup \{\bigcup_{i=1}^\infty A_{i}: ~\{A_{i}\}_{i=1}^\infty\subset \mathscr{C}_n\}. $$ Question: I am looking for an example of $\Omega$ such that   $\mathscr{A}(\mathscr{C})=\bigcup_{i=1}^\infty \mathscr{C}_n$ do not equal to $\sigma(\mathscr{C})$ the sigma álgebra generated by $\mathscr{C}$. I first try to pick a $\Omega=\mathbb{R}$ and consider $\mathscr{C}$ the class given by the finite and cofinite sets, however applying the above procedure we get the the class of the contable and co-countable subsets, which is a sigma álgebra.... I know that must be examples that $\mathscr{A}(\mathscr{C})=\bigcup_{i=1}^\infty \mathscr{C}_n$ do not equal to $\sigma(\mathscr{C})$  because we need of an ordinal argument in the construction...","Let  $\mathscr{C}\subset \mathscr{P}(\Omega)$ be  a class of subsets of a nonempty set $\Omega$ containing $\Omega$ and $\varnothing$. Define $\mathscr{C}_0=\mathscr{C}$ and for each  $n\geq 1$ define  $$ \mathscr{C}_{n+1}=\mathscr{C}_n\cup \{A^c:~ A\in \mathscr{C}_n\}\cup \{\bigcup_{i=1}^\infty A_{i}: ~\{A_{i}\}_{i=1}^\infty\subset \mathscr{C}_n\}. $$ Question: I am looking for an example of $\Omega$ such that   $\mathscr{A}(\mathscr{C})=\bigcup_{i=1}^\infty \mathscr{C}_n$ do not equal to $\sigma(\mathscr{C})$ the sigma álgebra generated by $\mathscr{C}$. I first try to pick a $\Omega=\mathbb{R}$ and consider $\mathscr{C}$ the class given by the finite and cofinite sets, however applying the above procedure we get the the class of the contable and co-countable subsets, which is a sigma álgebra.... I know that must be examples that $\mathscr{A}(\mathscr{C})=\bigcup_{i=1}^\infty \mathscr{C}_n$ do not equal to $\sigma(\mathscr{C})$  because we need of an ordinal argument in the construction...",,"['real-analysis', 'analysis', 'measure-theory', 'reference-request', 'lebesgue-measure']"
13,Checking inequality,Checking inequality,,"Let $\pi$ be a given permutation of the integers $\{1,\ldots,n\}$ and let  $$\mathcal{X}=\{x\in\mathbb{R}_{+}^{n} \mid x_{\pi(1)}\geq\cdots\geq x_{\pi(n)},\mathbb{1}^\top x\geq \alpha\},$$ for some $\alpha>0$. Let $a\in\mathbb{R}$, $b\in\mathbb{R}^{n}$ and $Q\in\mathbb{R}^{n\times n}$ symmetric. Is there a way to check if  \begin{align*} a+b^{\top}x+x^{\top}Qx<0,\quad \forall x\in\mathcal{X}, \end{align*} in general? For example, if $Q$ is negative semidefinite, then we can use convex quadratic programming to solve the problem. However, can we do something in the general case? Perhaps the eigendecomposition of $Q$ and the structure of $\mathcal{X}$ might be used.","Let $\pi$ be a given permutation of the integers $\{1,\ldots,n\}$ and let  $$\mathcal{X}=\{x\in\mathbb{R}_{+}^{n} \mid x_{\pi(1)}\geq\cdots\geq x_{\pi(n)},\mathbb{1}^\top x\geq \alpha\},$$ for some $\alpha>0$. Let $a\in\mathbb{R}$, $b\in\mathbb{R}^{n}$ and $Q\in\mathbb{R}^{n\times n}$ symmetric. Is there a way to check if  \begin{align*} a+b^{\top}x+x^{\top}Qx<0,\quad \forall x\in\mathcal{X}, \end{align*} in general? For example, if $Q$ is negative semidefinite, then we can use convex quadratic programming to solve the problem. However, can we do something in the general case? Perhaps the eigendecomposition of $Q$ and the structure of $\mathcal{X}$ might be used.",,"['matrices', 'analysis', 'optimization', 'quadratic-programming']"
14,Extending laws for Riemann integral to Riemann-Stieltjes integral,Extending laws for Riemann integral to Riemann-Stieltjes integral,,"I was reading Terence Tao's notes on Analysis. He says Theorem 13(g) cannot be extended from Riemann integral to Riemann-Stieltjes integral: Most (but not all) of the remaining theory from Week 9 notes then can   be carried over without difficulty, replacing Riemann integrals with   Riemann-Stieltjes integrals and lengths with $\alpha$-length. (There   are a couple results which break down; Theorem 13(g) , Proposition   16, and Proposition 17 are not necessarily true when $\alpha$ is   discontinuous at key places (e.g. if $f$ and $\alpha$ are both   discontinuous at the same point, then $\int_{I}fd\alpha$ is unlikely   to be defined). Source: page 4 of week 10 notes But I do not see why Theorem 13(g) breaks down if we extend it to Riemann-Stieltjes integral. Can anyone explain? Below is the theorem: Theorem 13 (Laws of integration). Let $I$ be a generalized interval,   and let $f: I \rightarrow R$ and $g: I \rightarrow R$ be Riemann   integrable functions on I. ... (g) Let $J$ be a generalized interval containing $I$ (i.e. $I \subseteq J$), and let $F: J \rightarrow R$ be the function $$ F(x) := \left\{                  \begin{array}{ll}                    f(x) & \text{if } x \in I \\                    0    & \text{if } x \notin I                  \end{array}                \right. $$ Then $F$ is Riemann integrable on $J$, and $\int_{J}F = \int_{I}f$. Source: page 14 of week 9 notes","I was reading Terence Tao's notes on Analysis. He says Theorem 13(g) cannot be extended from Riemann integral to Riemann-Stieltjes integral: Most (but not all) of the remaining theory from Week 9 notes then can   be carried over without difficulty, replacing Riemann integrals with   Riemann-Stieltjes integrals and lengths with $\alpha$-length. (There   are a couple results which break down; Theorem 13(g) , Proposition   16, and Proposition 17 are not necessarily true when $\alpha$ is   discontinuous at key places (e.g. if $f$ and $\alpha$ are both   discontinuous at the same point, then $\int_{I}fd\alpha$ is unlikely   to be defined). Source: page 4 of week 10 notes But I do not see why Theorem 13(g) breaks down if we extend it to Riemann-Stieltjes integral. Can anyone explain? Below is the theorem: Theorem 13 (Laws of integration). Let $I$ be a generalized interval,   and let $f: I \rightarrow R$ and $g: I \rightarrow R$ be Riemann   integrable functions on I. ... (g) Let $J$ be a generalized interval containing $I$ (i.e. $I \subseteq J$), and let $F: J \rightarrow R$ be the function $$ F(x) := \left\{                  \begin{array}{ll}                    f(x) & \text{if } x \in I \\                    0    & \text{if } x \notin I                  \end{array}                \right. $$ Then $F$ is Riemann integrable on $J$, and $\int_{J}F = \int_{I}f$. Source: page 14 of week 9 notes",,"['analysis', 'riemann-integration', 'stieltjes-integral']"
15,"Mathematical bases for which $q$ and $\dot{q}$ could be treated as independent variable in $L(q,\dot{q},t)$",Mathematical bases for which  and  could be treated as independent variable in,"q \dot{q} L(q,\dot{q},t)","Mathematical bases for which $q$ and $\dot{q}$ could be treated as independent variable in $L(q,\dot{q},t)$. In Lagrangian mechanics with single degree of freedome $q(t)$ and it's first degree deritative $\dot{q}(t)$ were considered as independent variable. $\displaystyle \frac{\partial \dot{q}}{\partial q}=\frac{\partial q}{\partial t\partial q}=\frac{\partial q}{\partial q\partial t}=\frac{\partial C}{\partial t}=0$ and its general form $\displaystyle \frac{\partial q^{(n)}}{\partial q}=0$ could be easily proven. However, $\displaystyle \frac{\partial q}{\partial \dot{q}}=0$ was not so straight forward. My question was that: Prove the general form $\displaystyle \frac{\partial q}{\partial q^{(n)}}=0$ in Lagrangian equation $L(q,\dot{q},t)$ where $\displaystyle\frac{\partial L}{\partial q}=\frac{d}{dt}(\frac{\partial L}{\partial \dot{q}})$ and $q^{(n)}$ was the $n$ th deritative of $q$.","Mathematical bases for which $q$ and $\dot{q}$ could be treated as independent variable in $L(q,\dot{q},t)$. In Lagrangian mechanics with single degree of freedome $q(t)$ and it's first degree deritative $\dot{q}(t)$ were considered as independent variable. $\displaystyle \frac{\partial \dot{q}}{\partial q}=\frac{\partial q}{\partial t\partial q}=\frac{\partial q}{\partial q\partial t}=\frac{\partial C}{\partial t}=0$ and its general form $\displaystyle \frac{\partial q^{(n)}}{\partial q}=0$ could be easily proven. However, $\displaystyle \frac{\partial q}{\partial \dot{q}}=0$ was not so straight forward. My question was that: Prove the general form $\displaystyle \frac{\partial q}{\partial q^{(n)}}=0$ in Lagrangian equation $L(q,\dot{q},t)$ where $\displaystyle\frac{\partial L}{\partial q}=\frac{d}{dt}(\frac{\partial L}{\partial \dot{q}})$ and $q^{(n)}$ was the $n$ th deritative of $q$.",,['calculus']
16,"In compact embedding Theorem, $u_0$ lies in $W^{1,p}(\Omega)$?","In compact embedding Theorem,  lies in ?","u_0 W^{1,p}(\Omega)","We know that $W^{1,p}(\Omega)\hookrightarrow L^q(\Omega)$ if $\Omega\subset\mathbb R^N$ is a bounded open and $\partial \Omega$ is $C^1$, $1\leq p<N$ and $1\leq q<p^*:=\frac{Np}{N-p}$. Moreover, for every bounded sequence $(u_n)$ in $W^{1,p}(\Omega)$ (unless subsequence), $u_n\rightarrow u_0$ in $L^q(\Omega)$. I want to $u_0\in W^{1,p}(\Omega)$, but I only know that $u_0\in L^q(\Omega)$. Is it true that $u_0\in W^{1,p}(\Omega)$? and how can I prove it? My attempt: Since $W^{1,p}(\Omega)$ is reflexive, we obtain $u_n\rightharpoonup u_0$ in $W^{1,p}(\Omega)$. We conclue with compact embedding. The problem in my attempt is that $W^{1,p}(\Omega)$ is not reflexive for $p=1$.","We know that $W^{1,p}(\Omega)\hookrightarrow L^q(\Omega)$ if $\Omega\subset\mathbb R^N$ is a bounded open and $\partial \Omega$ is $C^1$, $1\leq p<N$ and $1\leq q<p^*:=\frac{Np}{N-p}$. Moreover, for every bounded sequence $(u_n)$ in $W^{1,p}(\Omega)$ (unless subsequence), $u_n\rightarrow u_0$ in $L^q(\Omega)$. I want to $u_0\in W^{1,p}(\Omega)$, but I only know that $u_0\in L^q(\Omega)$. Is it true that $u_0\in W^{1,p}(\Omega)$? and how can I prove it? My attempt: Since $W^{1,p}(\Omega)$ is reflexive, we obtain $u_n\rightharpoonup u_0$ in $W^{1,p}(\Omega)$. We conclue with compact embedding. The problem in my attempt is that $W^{1,p}(\Omega)$ is not reflexive for $p=1$.",,"['functional-analysis', 'analysis', 'partial-differential-equations', 'compactness', 'sobolev-spaces']"
17,"Alternative to ""A course of pure mathematics"" by G.H. Hardy [closed]","Alternative to ""A course of pure mathematics"" by G.H. Hardy [closed]",,"Closed . This question is opinion-based . It is not currently accepting answers. Want to improve this question? Update the question so it can be answered with facts and citations by editing this post . Closed 5 years ago . Improve this question I have got the book ""The theory of functions"" be E.C. Titchmarsh and it states that it is necessary to know the content of Hardys book. Is there a rather quick way to acquire the knowledge in order to read ""The Theory of Functions""?","Closed . This question is opinion-based . It is not currently accepting answers. Want to improve this question? Update the question so it can be answered with facts and citations by editing this post . Closed 5 years ago . Improve this question I have got the book ""The theory of functions"" be E.C. Titchmarsh and it states that it is necessary to know the content of Hardys book. Is there a rather quick way to acquire the knowledge in order to read ""The Theory of Functions""?",,"['analysis', 'reference-request']"
18,Submmersions: $|f(x)|$ do not assume maximal value,Submmersions:  do not assume maximal value,|f(x)|,"I am studying analysis and I have had a lot of uncertainties. For instance, I cannot solve this exercise: If $f:U\rightarrow\mathbb{R}^3$ has class $C^1$ and rank $3$ in all of the points of the open $U\in\mathbb{R^4}$ , show that $|f(x)|$ do not assume maximal value for $x\in U$ . (I guess this is the comand, but I'm so sorry if I did mistakes. My language and the language of the comand is Portuguese) Well. I know that $f$ is a submersion. So, it's an open map. From here can I get the required? If I know that $f$ is an open map, have I that $|f(x)|$ is an open set and so that it has not a maximum? Edit - September, 25 I was taking another look at this question I decided try a formal proof in here: Once the rank of $f$ is maximal (the dominium dimension) and the contradominium has a lower dimension, so $f$ is a submmersion. But the submmersions are opened applications. So, $A=\{f(x);x\in U\}\subset \mathbb{R}^4$ is open. Consider a value $|f(x_0)|,x_0\in U$ and we'll prove that it isn't maximal. Once $A$ is open, there is $\delta>0$ such that $B[f(x_0),\delta]\subset A$ . Taking $f(x_0) $ as a vector, we have $(1+\delta)f(x_0)\in A$ . So, there is $x_1\in U$ such that $f(x_1)=(1+\delta)f(x_0)$ . But $|f(x_1)|=(1+\delta)|f(x_0)|>|f(x_0)|$ and this completes the proof. What do you think? Thanks very much.","I am studying analysis and I have had a lot of uncertainties. For instance, I cannot solve this exercise: If has class and rank in all of the points of the open , show that do not assume maximal value for . (I guess this is the comand, but I'm so sorry if I did mistakes. My language and the language of the comand is Portuguese) Well. I know that is a submersion. So, it's an open map. From here can I get the required? If I know that is an open map, have I that is an open set and so that it has not a maximum? Edit - September, 25 I was taking another look at this question I decided try a formal proof in here: Once the rank of is maximal (the dominium dimension) and the contradominium has a lower dimension, so is a submmersion. But the submmersions are opened applications. So, is open. Consider a value and we'll prove that it isn't maximal. Once is open, there is such that . Taking as a vector, we have . So, there is such that . But and this completes the proof. What do you think? Thanks very much.","f:U\rightarrow\mathbb{R}^3 C^1 3 U\in\mathbb{R^4} |f(x)| x\in U f f |f(x)| f f A=\{f(x);x\in U\}\subset \mathbb{R}^4 |f(x_0)|,x_0\in U A \delta>0 B[f(x_0),\delta]\subset A f(x_0)  (1+\delta)f(x_0)\in A x_1\in U f(x_1)=(1+\delta)f(x_0) |f(x_1)|=(1+\delta)|f(x_0)|>|f(x_0)|","['analysis', 'matrix-rank', 'open-map']"
19,Question about conformal mapping,Question about conformal mapping,,"Let $y_a(t):U \to \mathcal{C}$ be any comformal mappitn taking $U \in  \mathcal{C}$ (simply conected domeain ) into a unit disc $D$ and the a point $a\in U$ into its center, $y_a(a)=0$. Now define a relative diameter of a compact set $K$ with respect to set $U$ as  \begin{align} R(U,K)= \max_{a,t \in K} |y_a(t)|. \end{align} My question: Let $U$ be a disc of radius $R$ centered at the orign and $K$  a disc of radius $r$ centered at the origin where $r<R$.  What is the value of $R(U,K)$? I think $R(U,K)$ is equal to $ \frac{2 r}{R}$. However, I am not sure how to start this problem. Specifically, I am not sure how to characterize the mapping $y_a(t)$. Edit: I would also be happy with an upper bound on $R(U,K)$.","Let $y_a(t):U \to \mathcal{C}$ be any comformal mappitn taking $U \in  \mathcal{C}$ (simply conected domeain ) into a unit disc $D$ and the a point $a\in U$ into its center, $y_a(a)=0$. Now define a relative diameter of a compact set $K$ with respect to set $U$ as  \begin{align} R(U,K)= \max_{a,t \in K} |y_a(t)|. \end{align} My question: Let $U$ be a disc of radius $R$ centered at the orign and $K$  a disc of radius $r$ centered at the origin where $r<R$.  What is the value of $R(U,K)$? I think $R(U,K)$ is equal to $ \frac{2 r}{R}$. However, I am not sure how to start this problem. Specifically, I am not sure how to characterize the mapping $y_a(t)$. Edit: I would also be happy with an upper bound on $R(U,K)$.",,"['complex-analysis', 'analysis']"
20,Problem with open map that is not holomorphic,Problem with open map that is not holomorphic,,"This is the problem: Let $0\in\Omega\subset \mathbb{C}$ connected, open neighborhood of the origin.   Let $f,g:\Omega\to\mathbb{C}$ holomorphic functions such that   $f(0)\neq0\neq g(0).$ Let $h:\Omega\to\mathbb{C}$ to be   $h(z)=f(z)\overline{g(z)}z^a\overline{z}^b$ with $a\neq b,$ positive   integers. Take $U$ a neiberhood of the origin. show that $h(U)$ is a   neighborhood of the origin. Clearly, the only thing I have to prove is that $h(U)$ is open. $h$ is not holomorphic but I've tried to mimic the proof for the open map theorem with no succes. I've tried to some arguments with isolated zeros of $f$ and $g$ but no luck.","This is the problem: Let $0\in\Omega\subset \mathbb{C}$ connected, open neighborhood of the origin.   Let $f,g:\Omega\to\mathbb{C}$ holomorphic functions such that   $f(0)\neq0\neq g(0).$ Let $h:\Omega\to\mathbb{C}$ to be   $h(z)=f(z)\overline{g(z)}z^a\overline{z}^b$ with $a\neq b,$ positive   integers. Take $U$ a neiberhood of the origin. show that $h(U)$ is a   neighborhood of the origin. Clearly, the only thing I have to prove is that $h(U)$ is open. $h$ is not holomorphic but I've tried to mimic the proof for the open map theorem with no succes. I've tried to some arguments with isolated zeros of $f$ and $g$ but no luck.",,"['calculus', 'real-analysis', 'complex-analysis', 'analysis']"
21,What's the Fourier transform of the p-Laplacian operator $\Delta_{p}$?,What's the Fourier transform of the p-Laplacian operator ?,\Delta_{p},"Consider the p-Laplacian operator $\Delta_{p}u:= div(|\nabla u|^{p-2}\nabla u)$, where $1<p<\infty$. We know when $p=2$, it's the standard laplacian operator. So my question is what is the fourier transform of this operator. For general Laplacian, I know $\widehat {\Delta u}(\xi)=|\xi|^2 \hat u(\xi)$. What is $\widehat {\Delta_{p} u}(\xi)$? I couldn't find relevant notes and only find this link: https://mathoverflow.net/questions/228009/backgrounds-of-the-p-laplacian-operator","Consider the p-Laplacian operator $\Delta_{p}u:= div(|\nabla u|^{p-2}\nabla u)$, where $1<p<\infty$. We know when $p=2$, it's the standard laplacian operator. So my question is what is the fourier transform of this operator. For general Laplacian, I know $\widehat {\Delta u}(\xi)=|\xi|^2 \hat u(\xi)$. What is $\widehat {\Delta_{p} u}(\xi)$? I couldn't find relevant notes and only find this link: https://mathoverflow.net/questions/228009/backgrounds-of-the-p-laplacian-operator",,"['real-analysis', 'analysis', 'partial-differential-equations', 'fourier-analysis', 'harmonic-analysis']"
22,Critical values of a map to a Riemann surface,Critical values of a map to a Riemann surface,,"Let $f:X\to S$ be open proper holomorphic map, $X$ a complex manifold and $S$ a Riemann surface. Is it then true that the critical values $C\subset S$ of  $f$ are a discrete supset? So far I only noted that this would be true, if the set of critical points $K\subset X$ was discrete. But I don't know how to prove it. If $X$ was also a Riemann surface, I could apply the identity principle to $f'$, but in the general case I don't know how to proceed. Edit: I know about Sard's theorem. I don't see why it is strong enough though.","Let $f:X\to S$ be open proper holomorphic map, $X$ a complex manifold and $S$ a Riemann surface. Is it then true that the critical values $C\subset S$ of  $f$ are a discrete supset? So far I only noted that this would be true, if the set of critical points $K\subset X$ was discrete. But I don't know how to prove it. If $X$ was also a Riemann surface, I could apply the identity principle to $f'$, but in the general case I don't know how to proceed. Edit: I know about Sard's theorem. I don't see why it is strong enough though.",,"['complex-analysis', 'analysis']"
23,Prove that in Banach space $X$ the following are equivalent for series $\sum_{i\in\mathbb{N}} x_i$,Prove that in Banach space  the following are equivalent for series,X \sum_{i\in\mathbb{N}} x_i,"Prove that in Banach space $X$ the following are equivalent for series $\sum_{i\in\mathbb{N}} x_i$: 1) $\forall\varepsilon>0 \hspace{2mm} \exists $ finite $F\subset\mathbb{N}   $ such that for every finite $F'\subset \mathbb{N}$ with $F\subseteq F'$, $\lVert\sum_{i\in F'}x_i-x\rVert\ < \varepsilon$ 2) For every permutation $\pi$ of $\mathbb{N}$, $\sum_{i\in\mathbb{N}} x_{\pi(i)}$ converges to $x$. For 1)$\Rightarrow$2) Let $\pi$ be a permutation of the natural numbers and $\varepsilon>0$. By the assumption in 1), there exists F with the properties above. For some $n_0$ we have that $F\subseteq\{\pi(0),\pi(1),\dots,\pi(n_0)\}$ and thus $\lVert\sum_{i=1}^nx_i-x\rVert\ < \varepsilon$ whenever $n\geq n_0$ For 2)$\Rightarrow 1)$ let's assume that 1) does not hold, that is $\exists\varepsilon_0>0 \hspace{2mm} \forall $ finite $F\subset\mathbb{N} \hspace{2mm}   \exists F'\subset \mathbb{N}$ with $F\subseteq F'$ and $\lVert\sum_{i\in F'}x_i-x\rVert\ \geq \varepsilon_0$. We have that there is $n_0$ such that $\lVert\sum_{i=1}^{n_0}x_i-x\rVert\ < \varepsilon_0$ (because the series converjes for the identity permutation). By the assumption there is a finite set $F_1\supset F$ with $\lVert\sum_{i\in F_1}x_i-x\rVert\ \geq \varepsilon_0$.  Let's look at the permutation $\pi=F\cup(F_1\setminus F)\cup (N\setminus F)$. Since 2) holds for $\pi$there is a $j_0>\max\{F\}$ such that  $\lVert\sum_{i=0}^{\pi(i)=j_0}x_\pi(i)-x\rVert\ < \varepsilon_0$. Now there is a set $F_2$ such that ... and continue by recursion. I'm not 100% sure if the permutation that I get is such that the series doesn't converge to $x$. And is there a simpler construction to get a contradiction?","Prove that in Banach space $X$ the following are equivalent for series $\sum_{i\in\mathbb{N}} x_i$: 1) $\forall\varepsilon>0 \hspace{2mm} \exists $ finite $F\subset\mathbb{N}   $ such that for every finite $F'\subset \mathbb{N}$ with $F\subseteq F'$, $\lVert\sum_{i\in F'}x_i-x\rVert\ < \varepsilon$ 2) For every permutation $\pi$ of $\mathbb{N}$, $\sum_{i\in\mathbb{N}} x_{\pi(i)}$ converges to $x$. For 1)$\Rightarrow$2) Let $\pi$ be a permutation of the natural numbers and $\varepsilon>0$. By the assumption in 1), there exists F with the properties above. For some $n_0$ we have that $F\subseteq\{\pi(0),\pi(1),\dots,\pi(n_0)\}$ and thus $\lVert\sum_{i=1}^nx_i-x\rVert\ < \varepsilon$ whenever $n\geq n_0$ For 2)$\Rightarrow 1)$ let's assume that 1) does not hold, that is $\exists\varepsilon_0>0 \hspace{2mm} \forall $ finite $F\subset\mathbb{N} \hspace{2mm}   \exists F'\subset \mathbb{N}$ with $F\subseteq F'$ and $\lVert\sum_{i\in F'}x_i-x\rVert\ \geq \varepsilon_0$. We have that there is $n_0$ such that $\lVert\sum_{i=1}^{n_0}x_i-x\rVert\ < \varepsilon_0$ (because the series converjes for the identity permutation). By the assumption there is a finite set $F_1\supset F$ with $\lVert\sum_{i\in F_1}x_i-x\rVert\ \geq \varepsilon_0$.  Let's look at the permutation $\pi=F\cup(F_1\setminus F)\cup (N\setminus F)$. Since 2) holds for $\pi$there is a $j_0>\max\{F\}$ such that  $\lVert\sum_{i=0}^{\pi(i)=j_0}x_\pi(i)-x\rVert\ < \varepsilon_0$. Now there is a set $F_2$ such that ... and continue by recursion. I'm not 100% sure if the permutation that I get is such that the series doesn't converge to $x$. And is there a simpler construction to get a contradiction?",,"['functional-analysis', 'analysis']"
24,"Prob. 6, Exercises 8.14, in Apostol's CALCULUS Vol II: Given a scalar field differentiable at a point ...","Prob. 6, Exercises 8.14, in Apostol's CALCULUS Vol II: Given a scalar field differentiable at a point ...",,"Here is Prob. 6, Exercises 8.14, in the book Calculus Vol. II by Tom M. Apostol, 2nd edition: Given a scalar field differentiable at a point $\mathbf{a}$ in $\mathbb{R}^2$, suppose that $f^\prime(\mathbf{a}; \mathbf{y})=1$ and $f^\prime(\mathbf{a}; \mathbf{z})=2$, where $\mathbf{y} = 2\mathbf{i}+3\mathbf{j}$ and $\mathbf{z} = \mathbf{i} + \mathbf{j}$. Make a sketch showing the set of all points $(x, y)$ for which $f^\prime(\mathbf{a}; x\mathbf{i}+y\mathbf{j})=6$. Also, calculate the gradient $\nabla f(\mathbf{a})$. My Attempt: As $f$ is differentiable at $\mathbf{a}$, so for every $\mathbf{b} \in \mathbb{R}^2$ the directional derivative $f^\prime(\mathbf{a}; \mathbf{b})$ exists and is given by    $$ f^\prime(\mathbf{a}; \mathbf{b}) = \nabla f(\mathbf{a}) \cdot \mathbf{b}.  $$ Now let $(u, v)$ be the gradient vector of $f$ at the point $\mathbf{a}$. Then from the given information we can conclude that    $$ 2u + 3v = 1 \qquad \mbox{ and } \qquad u+v = 2, $$   and upon solving these two equations simultaneously for $u$ and $v$ we get $u = 5$ and $v=-3$. Thus    $$ \nabla f(\mathbf{a}) = (5, -3). $$ Am I right? Thus the set of all the points $(x, y)$ for which $f^\prime(\mathbf{a}; x\mathbf{i} + y\mathbf{j})=6$ is    $$ \left\{ \ (x, y) \in \mathbb{R}^2 \ \colon \ 5x-3y=6 \ \right\}, $$   which is the straight line in the plane through the point (0, -2) and having the slope $5/3$. Am I right?","Here is Prob. 6, Exercises 8.14, in the book Calculus Vol. II by Tom M. Apostol, 2nd edition: Given a scalar field differentiable at a point $\mathbf{a}$ in $\mathbb{R}^2$, suppose that $f^\prime(\mathbf{a}; \mathbf{y})=1$ and $f^\prime(\mathbf{a}; \mathbf{z})=2$, where $\mathbf{y} = 2\mathbf{i}+3\mathbf{j}$ and $\mathbf{z} = \mathbf{i} + \mathbf{j}$. Make a sketch showing the set of all points $(x, y)$ for which $f^\prime(\mathbf{a}; x\mathbf{i}+y\mathbf{j})=6$. Also, calculate the gradient $\nabla f(\mathbf{a})$. My Attempt: As $f$ is differentiable at $\mathbf{a}$, so for every $\mathbf{b} \in \mathbb{R}^2$ the directional derivative $f^\prime(\mathbf{a}; \mathbf{b})$ exists and is given by    $$ f^\prime(\mathbf{a}; \mathbf{b}) = \nabla f(\mathbf{a}) \cdot \mathbf{b}.  $$ Now let $(u, v)$ be the gradient vector of $f$ at the point $\mathbf{a}$. Then from the given information we can conclude that    $$ 2u + 3v = 1 \qquad \mbox{ and } \qquad u+v = 2, $$   and upon solving these two equations simultaneously for $u$ and $v$ we get $u = 5$ and $v=-3$. Thus    $$ \nabla f(\mathbf{a}) = (5, -3). $$ Am I right? Thus the set of all the points $(x, y)$ for which $f^\prime(\mathbf{a}; x\mathbf{i} + y\mathbf{j})=6$ is    $$ \left\{ \ (x, y) \in \mathbb{R}^2 \ \colon \ 5x-3y=6 \ \right\}, $$   which is the straight line in the plane through the point (0, -2) and having the slope $5/3$. Am I right?",,"['calculus', 'real-analysis', 'analysis', 'multivariable-calculus', 'derivatives']"
25,"Prob. 3, Exercises 8.14, in Apostol's CALCULUS Vol. 2: Find the point(s) and the direction(s) of maximum directional derivative of this scalar field","Prob. 3, Exercises 8.14, in Apostol's CALCULUS Vol. 2: Find the point(s) and the direction(s) of maximum directional derivative of this scalar field",,"Here is Prob. 3, Exercises 8.14, in the book Calculus Vol. II by Tom M. Apostol, 2nd edition: Find the points $(x, y)$ and the directions for which the directional derivative of $f(x, y) = 3x^2+y^2$ has its largest value, if $(x, y)$ is restricted to be on the circle $x^2 + y^2 = 1$. My Attempt: We note that, at any point $(x, y) \in \mathbb{R}^2$, we have    $$ \frac{ \partial f (x, y)}{ \partial x} = 6x, \qquad \mbox{ and } \qquad \frac{ \partial f (x, y)}{ \partial y } = 2y. $$   Thus these partial derivatives exist and are continuous everywhere in $\mathbb{R}^2$. Hence $f$ is differentiable everywhere in $\mathbb{R}^2$. Therefore we must have    $$ f^\prime( (x, y); (u, v) ) = \nabla f(x, y) \cdot (u, v) = ( 6x, 2y) \cdot (u, v) = 6xu + 2yv $$   for any point $(x, y) \in \mathbb{R}^2$ and for every direction $(u, v) \in \mathbb{R}^2$. Now at any point $(x, y) \in \mathbb{R}^2$, the derivative $f^\prime( (x, y) ; (u, v) )$ is largest in the direction of the gradient vector $(6x, 2y)$ and this largest value is    $$ \lVert \nabla f(x, y) \rVert^2 = 36x^2 + 4y^2, $$   and, for all points $(x, y) \in \mathbb{R}^2$ for which $x^2 + y^2 = 1$, this largest value is    $$ \lVert \nabla f(x, y) \rVert^2 = 36x^2 + 4y^2 = 36 \left(x^2 + y^2 \right) - 32 y^2 = 36 - 32 y^2, $$   and this last expression attains its maximum value when $y = 0$, that is, at either of the points $(1, 0)$ and $(-1, 0)$. Thus the largest value of $f^\prime( (x, y); (u, v) )$ for any points $(x, y) \in \mathbb{R}^2$ for which $x^2+y^2=1$ is attained at the points $( 1, 0)$ and $(-1, 0)$ in the direction of the vectors    $$\nabla f(1, 0) = ( 6, 0) $$   and    $$\nabla f(-1, 0) = (-6, 0),$$   respectively, and this largest value in either case is $36$. Is my solution correct? Is each and every step in it correct and clear enough? If not, then where are the issues?","Here is Prob. 3, Exercises 8.14, in the book Calculus Vol. II by Tom M. Apostol, 2nd edition: Find the points $(x, y)$ and the directions for which the directional derivative of $f(x, y) = 3x^2+y^2$ has its largest value, if $(x, y)$ is restricted to be on the circle $x^2 + y^2 = 1$. My Attempt: We note that, at any point $(x, y) \in \mathbb{R}^2$, we have    $$ \frac{ \partial f (x, y)}{ \partial x} = 6x, \qquad \mbox{ and } \qquad \frac{ \partial f (x, y)}{ \partial y } = 2y. $$   Thus these partial derivatives exist and are continuous everywhere in $\mathbb{R}^2$. Hence $f$ is differentiable everywhere in $\mathbb{R}^2$. Therefore we must have    $$ f^\prime( (x, y); (u, v) ) = \nabla f(x, y) \cdot (u, v) = ( 6x, 2y) \cdot (u, v) = 6xu + 2yv $$   for any point $(x, y) \in \mathbb{R}^2$ and for every direction $(u, v) \in \mathbb{R}^2$. Now at any point $(x, y) \in \mathbb{R}^2$, the derivative $f^\prime( (x, y) ; (u, v) )$ is largest in the direction of the gradient vector $(6x, 2y)$ and this largest value is    $$ \lVert \nabla f(x, y) \rVert^2 = 36x^2 + 4y^2, $$   and, for all points $(x, y) \in \mathbb{R}^2$ for which $x^2 + y^2 = 1$, this largest value is    $$ \lVert \nabla f(x, y) \rVert^2 = 36x^2 + 4y^2 = 36 \left(x^2 + y^2 \right) - 32 y^2 = 36 - 32 y^2, $$   and this last expression attains its maximum value when $y = 0$, that is, at either of the points $(1, 0)$ and $(-1, 0)$. Thus the largest value of $f^\prime( (x, y); (u, v) )$ for any points $(x, y) \in \mathbb{R}^2$ for which $x^2+y^2=1$ is attained at the points $( 1, 0)$ and $(-1, 0)$ in the direction of the vectors    $$\nabla f(1, 0) = ( 6, 0) $$   and    $$\nabla f(-1, 0) = (-6, 0),$$   respectively, and this largest value in either case is $36$. Is my solution correct? Is each and every step in it correct and clear enough? If not, then where are the issues?",,"['calculus', 'real-analysis', 'analysis', 'multivariable-calculus', 'derivatives']"
26,"Prob. 21, Exercises 8.9, in Apostol's CALCULUS vol. II: If the directional derivative of a scalar field vanishes . . .","Prob. 21, Exercises 8.9, in Apostol's CALCULUS vol. II: If the directional derivative of a scalar field vanishes . . .",,"Here is Prob. 21, Exercises 8.9, in the book Calculus volume II by Tom M. Apostol, 2nd edition: A set $S$ in $\mathbb{R}^n$ is called convex if for every pair of points $\mathbf{a}$ and $\mathbf{a}$ in $S$ the line segment from $\mathbf{a}$ to $\mathbf{b}$ is also in $S$; in other words, $t \mathbf{a} + (1-t) \mathbf{b} \in S$ for each $t$ in the interval $0 \leq t \leq 1$. (a) Prove that every $n$-ball is convex. (b) If $f^\prime ( \mathbf{x}; \mathbf{y} ) = 0$ for every $\mathbf{x}$ in an open convex set $S$ and for every $\mathbf{y}$ in $\mathbb{R}^n$, prove that $f$ is constant on $S$. Part (a) Let $\mathbf{p}$ be a given point in $\mathbb{R}^n$, and let $\delta$ be a given positive real number. Then the $n$-ball $B( \mathbf{p} ; \delta )$ is given by    $$ B( \mathbf{p} ; \delta ) \colon= \{ \ \mathbf{x} \in \mathbb{R}^n \ \colon \ \lVert \mathbf{x} - \mathbf{p} \rVert < \delta \ \}, $$   where    $$ \lVert \mathbf{y} \rVert = \sqrt{ \sum_{i=1}^n \left\lvert y_i \right\rvert^2 } $$   for any point    $$ \mathbf{y} \colon= \left( y_1, \ldots, y_n \right) $$   in $\mathbb{R}^n$. Suppose $\mathbb{a}$ and $\mathbb{b}$ are any two points in $B( \mathbf{p} ; \delta )$ and suppose that $t$ is any real number such that $0 \leq t \leq 1$. Then $\mathbb{a}$ and $\mathbb{b}$ are in $\mathbb{R}^n$, and we also have    $$ \lVert \mathbf{a} - \mathbf{p} \rVert < \delta, \qquad \mbox{ and } \qquad \lVert \mathbf{b} - \mathbf{p} \rVert < \delta. $$   So    $$ \begin{align} \lVert t \mathbf{a} + (1-t) \mathbf{b} - \mathbf{p} \rVert &= \lVert t \mathbf{a} + (1-t) \mathbf{b} - [t + (1-t)] \mathbf{p} \rVert \\ &= \lVert t ( \mathbf{a} - \mathbf{p} ) + (1-t) ( \mathbf{b} - \mathbf{p} ) \rVert \\ &\leq \lVert t ( \mathbf{a} - \mathbf{p} ) \rVert  + \lVert  (1-t) ( \mathbf{b} - \mathbf{p} ) \rVert \\ &= \lvert t \rvert  \lVert  \mathbf{a} - \mathbf{p}  \rVert  + \lvert 1-t \rvert  \lVert  \mathbf{b} - \mathbf{p}  \rVert \\ &=  t \lVert  \mathbf{a} - \mathbf{p}  \rVert  + ( 1-t )  \lVert  \mathbf{b} - \mathbf{p}  \rVert \ [ \mbox{ because } \ t \in [0, 1] \ \mbox{ and } \ 1-t \in [0, 1] \ ] \\ &< t \delta + (1-t) \delta \\ &= \delta, \end{align} $$   which shows that the point $t \mathbf{a} + (1-t) \mathbf{b}$ is also in $B( \mathbf{p}; \delta )$. Hence $B( \mathbf{p}; \delta )$ is convex. Part (b) Suppose that $S$ is an open convex set in $\mathbb{R}^n$, suppose that $f^\prime( \mathbf{x}; \mathbf{y} ) = 0$ for every $\mathbf{x} \in S$ and for every $\mathbf{y} \in \mathbb{R}^n$, and suppose that $\mathbf{a}$ and $\mathbf{b}$ are any two points in $S$. Then, for every $t \in [0, 1]$, the point $t \mathbf{a} + (1-t) \mathbf{b}$ is also in $S$. Now if we put    $$ \mathbf{y} \colon= \mathbf{b} - \mathbf{a}, $$   then, by the mean-value theorem for scalar fields, we can conclude that,  for some real number $\theta \in (0, 1)$, we have    $$ f( \mathbf{b} ) - f( \mathbf{a} ) = f( \mathbf{a} + \mathbf{y} ) - f( \mathbf{a} ) = f^\prime ( \mathbf{a} + \theta \mathbf{y} ; \mathbf{y} ). \tag{1}   $$   Now    $$ \mathbf{a} + \theta \mathbf{y} = \mathbf{a} + \theta (\mathbf{b} - \mathbf{a} ) = \theta \mathbf{b} + (1- \theta) \mathbf{a} \in S, $$   and thus    $$ f^\prime ( \mathbf{a} + \theta \mathbf{y} ; \mathbf{y} ) = 0,  $$   by our hypothesis. Therefore (1) gives    $$  f( \mathbf{b} ) = f( \mathbf{a} ) $$   for any two points $\mathbf{a}$ and $\mathbf{b}$ in $S$. Hence $f$ is constant on set $S$. Is this proof correct? If so, then do we really need our set $S$ to be open in $\mathbb{R}^n$? If not, then where is it that I'm going wrong?","Here is Prob. 21, Exercises 8.9, in the book Calculus volume II by Tom M. Apostol, 2nd edition: A set $S$ in $\mathbb{R}^n$ is called convex if for every pair of points $\mathbf{a}$ and $\mathbf{a}$ in $S$ the line segment from $\mathbf{a}$ to $\mathbf{b}$ is also in $S$; in other words, $t \mathbf{a} + (1-t) \mathbf{b} \in S$ for each $t$ in the interval $0 \leq t \leq 1$. (a) Prove that every $n$-ball is convex. (b) If $f^\prime ( \mathbf{x}; \mathbf{y} ) = 0$ for every $\mathbf{x}$ in an open convex set $S$ and for every $\mathbf{y}$ in $\mathbb{R}^n$, prove that $f$ is constant on $S$. Part (a) Let $\mathbf{p}$ be a given point in $\mathbb{R}^n$, and let $\delta$ be a given positive real number. Then the $n$-ball $B( \mathbf{p} ; \delta )$ is given by    $$ B( \mathbf{p} ; \delta ) \colon= \{ \ \mathbf{x} \in \mathbb{R}^n \ \colon \ \lVert \mathbf{x} - \mathbf{p} \rVert < \delta \ \}, $$   where    $$ \lVert \mathbf{y} \rVert = \sqrt{ \sum_{i=1}^n \left\lvert y_i \right\rvert^2 } $$   for any point    $$ \mathbf{y} \colon= \left( y_1, \ldots, y_n \right) $$   in $\mathbb{R}^n$. Suppose $\mathbb{a}$ and $\mathbb{b}$ are any two points in $B( \mathbf{p} ; \delta )$ and suppose that $t$ is any real number such that $0 \leq t \leq 1$. Then $\mathbb{a}$ and $\mathbb{b}$ are in $\mathbb{R}^n$, and we also have    $$ \lVert \mathbf{a} - \mathbf{p} \rVert < \delta, \qquad \mbox{ and } \qquad \lVert \mathbf{b} - \mathbf{p} \rVert < \delta. $$   So    $$ \begin{align} \lVert t \mathbf{a} + (1-t) \mathbf{b} - \mathbf{p} \rVert &= \lVert t \mathbf{a} + (1-t) \mathbf{b} - [t + (1-t)] \mathbf{p} \rVert \\ &= \lVert t ( \mathbf{a} - \mathbf{p} ) + (1-t) ( \mathbf{b} - \mathbf{p} ) \rVert \\ &\leq \lVert t ( \mathbf{a} - \mathbf{p} ) \rVert  + \lVert  (1-t) ( \mathbf{b} - \mathbf{p} ) \rVert \\ &= \lvert t \rvert  \lVert  \mathbf{a} - \mathbf{p}  \rVert  + \lvert 1-t \rvert  \lVert  \mathbf{b} - \mathbf{p}  \rVert \\ &=  t \lVert  \mathbf{a} - \mathbf{p}  \rVert  + ( 1-t )  \lVert  \mathbf{b} - \mathbf{p}  \rVert \ [ \mbox{ because } \ t \in [0, 1] \ \mbox{ and } \ 1-t \in [0, 1] \ ] \\ &< t \delta + (1-t) \delta \\ &= \delta, \end{align} $$   which shows that the point $t \mathbf{a} + (1-t) \mathbf{b}$ is also in $B( \mathbf{p}; \delta )$. Hence $B( \mathbf{p}; \delta )$ is convex. Part (b) Suppose that $S$ is an open convex set in $\mathbb{R}^n$, suppose that $f^\prime( \mathbf{x}; \mathbf{y} ) = 0$ for every $\mathbf{x} \in S$ and for every $\mathbf{y} \in \mathbb{R}^n$, and suppose that $\mathbf{a}$ and $\mathbf{b}$ are any two points in $S$. Then, for every $t \in [0, 1]$, the point $t \mathbf{a} + (1-t) \mathbf{b}$ is also in $S$. Now if we put    $$ \mathbf{y} \colon= \mathbf{b} - \mathbf{a}, $$   then, by the mean-value theorem for scalar fields, we can conclude that,  for some real number $\theta \in (0, 1)$, we have    $$ f( \mathbf{b} ) - f( \mathbf{a} ) = f( \mathbf{a} + \mathbf{y} ) - f( \mathbf{a} ) = f^\prime ( \mathbf{a} + \theta \mathbf{y} ; \mathbf{y} ). \tag{1}   $$   Now    $$ \mathbf{a} + \theta \mathbf{y} = \mathbf{a} + \theta (\mathbf{b} - \mathbf{a} ) = \theta \mathbf{b} + (1- \theta) \mathbf{a} \in S, $$   and thus    $$ f^\prime ( \mathbf{a} + \theta \mathbf{y} ; \mathbf{y} ) = 0,  $$   by our hypothesis. Therefore (1) gives    $$  f( \mathbf{b} ) = f( \mathbf{a} ) $$   for any two points $\mathbf{a}$ and $\mathbf{b}$ in $S$. Hence $f$ is constant on set $S$. Is this proof correct? If so, then do we really need our set $S$ to be open in $\mathbb{R}^n$? If not, then where is it that I'm going wrong?",,"['real-analysis', 'analysis', 'multivariable-calculus', 'derivatives', 'proof-verification']"
27,Why are in the definition of the Hölder space $C^{k+\gamma}$ only the partial derivatives of order $k$ assumed to be $\gamma$-Hölder continuous?,Why are in the definition of the Hölder space  only the partial derivatives of order  assumed to be -Hölder continuous?,C^{k+\gamma} k \gamma,"Let $d\in\mathbb N$ $\Lambda\subseteq\mathbb R^d$ be open $E$ be a $\mathbb R$-Banach space $k\in\mathbb N_0$ $\gamma\in(0,1]$ The Hölder space $C^{k+\gamma}(\Lambda,E)$ is defined to be $$\left\{f\in C^k(\Lambda,E):\partial^\alpha f\text{ is bounded for all }|\alpha|\le k\text{ and }[\partial^\alpha f]_\gamma<\infty\text{ for all }|\alpha|=k\right\},$$ where $$[f]_\gamma:=\sup_{\substack{x,\:y\:\in\:\Lambda\\x\:\ne\:y}}\frac{\left\|f(x)-f(y)\right\|_E}{|x-y|^\gamma}\;\;\;\text{for }f:\Lambda\to E.$$ Why are only the partial derivatives of order $k$ forced to be $\gamma$-Hölder continuous? That doesn't make sense to me, unless we can show that, for any $f\in C^{k+\gamma}(\Lambda,E)$, $\partial^\alpha f$ is $\gamma$-Hölder continuous for all $|\alpha|<k$. It's clear to me that if $g:\Lambda\to E$ is Fréchet differentiable at $x_0\in\Lambda$, then $g$ is locally Lipschitz continuous at $x_0$. Moreover, if $\Lambda$ is convex, $g$ is everywhere Fréchet differentiable and ${\rm D}g$ is bounded by $C\ge0$, then $g$ is Lipschitz continuous with Lipschitz constant $C$. However, since $\Lambda$ is not assumed to be convex in general, this fact doesn't help.","Let $d\in\mathbb N$ $\Lambda\subseteq\mathbb R^d$ be open $E$ be a $\mathbb R$-Banach space $k\in\mathbb N_0$ $\gamma\in(0,1]$ The Hölder space $C^{k+\gamma}(\Lambda,E)$ is defined to be $$\left\{f\in C^k(\Lambda,E):\partial^\alpha f\text{ is bounded for all }|\alpha|\le k\text{ and }[\partial^\alpha f]_\gamma<\infty\text{ for all }|\alpha|=k\right\},$$ where $$[f]_\gamma:=\sup_{\substack{x,\:y\:\in\:\Lambda\\x\:\ne\:y}}\frac{\left\|f(x)-f(y)\right\|_E}{|x-y|^\gamma}\;\;\;\text{for }f:\Lambda\to E.$$ Why are only the partial derivatives of order $k$ forced to be $\gamma$-Hölder continuous? That doesn't make sense to me, unless we can show that, for any $f\in C^{k+\gamma}(\Lambda,E)$, $\partial^\alpha f$ is $\gamma$-Hölder continuous for all $|\alpha|<k$. It's clear to me that if $g:\Lambda\to E$ is Fréchet differentiable at $x_0\in\Lambda$, then $g$ is locally Lipschitz continuous at $x_0$. Moreover, if $\Lambda$ is convex, $g$ is everywhere Fréchet differentiable and ${\rm D}g$ is bounded by $C\ge0$, then $g$ is Lipschitz continuous with Lipschitz constant $C$. However, since $\Lambda$ is not assumed to be convex in general, this fact doesn't help.",,"['analysis', 'continuity', 'holder-spaces']"
28,$L^2$ equality implies $L^1$ equality or a.e. equality?,equality implies  equality or a.e. equality?,L^2 L^1,"Suppose $f\in L^1 \cap L^2$ and $g\in L^2$. If I know that $f=g$ in $L^2$, then does it follow that $f=g$ in $L^1$ or a.e? Does it follow that $g\in L^1$? More generally, if $f\in L^s \cap L^r, g\in L^s$ and $f=g$ in $L^s$, then does it follow that $g\in L^r$? In particular, I have the Lebesgue measure on $\mathbb R$ in mind but I am also curious about general results. Thanks and regards.","Suppose $f\in L^1 \cap L^2$ and $g\in L^2$. If I know that $f=g$ in $L^2$, then does it follow that $f=g$ in $L^1$ or a.e? Does it follow that $g\in L^1$? More generally, if $f\in L^s \cap L^r, g\in L^s$ and $f=g$ in $L^s$, then does it follow that $g\in L^r$? In particular, I have the Lebesgue measure on $\mathbb R$ in mind but I am also curious about general results. Thanks and regards.",,"['real-analysis', 'integration', 'functional-analysis', 'analysis', 'measure-theory']"
29,Convergence of a Series with Infinitely Many Increasing Terms,Convergence of a Series with Infinitely Many Increasing Terms,,"I'm working on the following problem: Let $\{ c_n \}_{n=1}^{\infty}$ be a sequence such that $c_n = \frac{1}{n^2}$ if $n$ is odd, and $c_n = \frac{1}{n^4}$ if $n$ is even. Prove that (i) $c_{n+1} > c_n$ for infinitely many $n$ and (ii) $\sum_{n=1}^{\infty} (-1)^{n-1} c_n$ converges. My attempt: (i) We claim that when $n$ is even,  $$c_{n+1} > c_n$$ and proceed via induction. First, note that for $n=2$,  $$c_3 = \frac{1}{9} > \frac{1}{16} = c_2.$$ Now, suppose that $c_{k+1} > c_k$ for some even $k$. Then, we prove that  \begin{align*} c_{k+3} &> c_{k+2} \\ \frac{1}{(k+3)^2} &> \frac{1}{(k+2)^4} \\ \end{align*} by showing that  $$(k+2)^4 > (k+3)^2.$$ Accordingly, \begin{align*} (k+2)^4 &= k^4 + 8k^3 + 24k^2 + 32k + 16 \\ &>^{I.H.} (k^2 + 2k + 1) + 8k^3 + 24k^2 + 32k + 16 \\ &= 8k^3 + 25k^2 + 34k + 17. \\ \end{align*} Now, \begin{align*} 8k^3 + 25k^2 + 34k + 17 &> (k+3)^2 \\ \Rightarrow 8k^3 + 24k^2 + 28k + 8 &> 0 \\ \end{align*} holds as $k>0$ and each coefficient of the polynomial is positive. Thus, $c_{n+1} > c_n$ for each even $n$. (ii) To see that $\sum_{n=1}^{\infty} (-1)^{n-1} c_n$ converges, we first note that $$ |(-1)^{n-1} c_n | = c_n \leq \frac{1}{n^2} \qquad \forall n \in \mathbb{N} $$ Further, $\sum_{n=1}^{\infty} \frac{1}{n^2}$ converges via $p$-Test with $p = 2 > 1$, so $$ \sum_{n=1}^{\infty} (-1)^{n-1} c_n $$ converges via the Weierstrass M-test. Is this sound reasoning? The inductive portion seems long, as the result is somewhat intuitive. Is there an shorter way to prove this?","I'm working on the following problem: Let $\{ c_n \}_{n=1}^{\infty}$ be a sequence such that $c_n = \frac{1}{n^2}$ if $n$ is odd, and $c_n = \frac{1}{n^4}$ if $n$ is even. Prove that (i) $c_{n+1} > c_n$ for infinitely many $n$ and (ii) $\sum_{n=1}^{\infty} (-1)^{n-1} c_n$ converges. My attempt: (i) We claim that when $n$ is even,  $$c_{n+1} > c_n$$ and proceed via induction. First, note that for $n=2$,  $$c_3 = \frac{1}{9} > \frac{1}{16} = c_2.$$ Now, suppose that $c_{k+1} > c_k$ for some even $k$. Then, we prove that  \begin{align*} c_{k+3} &> c_{k+2} \\ \frac{1}{(k+3)^2} &> \frac{1}{(k+2)^4} \\ \end{align*} by showing that  $$(k+2)^4 > (k+3)^2.$$ Accordingly, \begin{align*} (k+2)^4 &= k^4 + 8k^3 + 24k^2 + 32k + 16 \\ &>^{I.H.} (k^2 + 2k + 1) + 8k^3 + 24k^2 + 32k + 16 \\ &= 8k^3 + 25k^2 + 34k + 17. \\ \end{align*} Now, \begin{align*} 8k^3 + 25k^2 + 34k + 17 &> (k+3)^2 \\ \Rightarrow 8k^3 + 24k^2 + 28k + 8 &> 0 \\ \end{align*} holds as $k>0$ and each coefficient of the polynomial is positive. Thus, $c_{n+1} > c_n$ for each even $n$. (ii) To see that $\sum_{n=1}^{\infty} (-1)^{n-1} c_n$ converges, we first note that $$ |(-1)^{n-1} c_n | = c_n \leq \frac{1}{n^2} \qquad \forall n \in \mathbb{N} $$ Further, $\sum_{n=1}^{\infty} \frac{1}{n^2}$ converges via $p$-Test with $p = 2 > 1$, so $$ \sum_{n=1}^{\infty} (-1)^{n-1} c_n $$ converges via the Weierstrass M-test. Is this sound reasoning? The inductive portion seems long, as the result is somewhat intuitive. Is there an shorter way to prove this?",,"['sequences-and-series', 'analysis']"
30,"If $\frac{\partial z}{\partial x}=\frac{\partial z}{\partial y}$, then $z\left(x,y\right)$ must be differentiable?","If , then  must be differentiable?","\frac{\partial z}{\partial x}=\frac{\partial z}{\partial y} z\left(x,y\right)","$z=z\left(x,y\right)$ is defined on region $D$, $\frac{\partial z}{\partial x}$ and $\frac{\partial z}{\partial y}$ both exists, and $\frac{\partial z}{\partial x}=\frac{\partial z}{\partial y}$ for all $\left(x,y\right)\in D$. If $z$ is differentiable we can get $z=f\left(x+y\right)$, i.e. z is only dependent on $x+y$. But what if $z$ is not differentiable? Do there exist some cases when $z$ is not differentiable? I guess that $z\left(x,y\right)$ must be differentiable if the conditions above are true, but anyway I cannot prove it. It's probable that I made a wrong guess. Now it remains a problem: if $\frac{\partial z}{\partial x}=\frac{\partial z}{\partial y}$ for all $\left(x,y\right)\in D$ and $z\left(x,x\right)\equiv 0$, then $z\left(x,y\right)\equiv 0$? I think it would be true. But without given ""$z$ is differentiable"", it is hard to prove.","$z=z\left(x,y\right)$ is defined on region $D$, $\frac{\partial z}{\partial x}$ and $\frac{\partial z}{\partial y}$ both exists, and $\frac{\partial z}{\partial x}=\frac{\partial z}{\partial y}$ for all $\left(x,y\right)\in D$. If $z$ is differentiable we can get $z=f\left(x+y\right)$, i.e. z is only dependent on $x+y$. But what if $z$ is not differentiable? Do there exist some cases when $z$ is not differentiable? I guess that $z\left(x,y\right)$ must be differentiable if the conditions above are true, but anyway I cannot prove it. It's probable that I made a wrong guess. Now it remains a problem: if $\frac{\partial z}{\partial x}=\frac{\partial z}{\partial y}$ for all $\left(x,y\right)\in D$ and $z\left(x,x\right)\equiv 0$, then $z\left(x,y\right)\equiv 0$? I think it would be true. But without given ""$z$ is differentiable"", it is hard to prove.",,"['real-analysis', 'analysis', 'multivariable-calculus']"
31,Score-based rating system,Score-based rating system,,"I am looking for a rating system to rate people based on their score on a competition and their previous rating. Codeforces is using a variant of Elo rating system which is pretty cool (you can read about it here ), but it is rating people by their ""rank"", not their ""score"". Has anyone encountered such a system?","I am looking for a rating system to rate people based on their score on a competition and their previous rating. Codeforces is using a variant of Elo rating system which is pretty cool (you can read about it here ), but it is rating people by their ""rank"", not their ""score"". Has anyone encountered such a system?",,"['analysis', 'statistics', 'estimation']"
32,Nice proofs of $\zeta(4) = \frac{\pi^4}{90}$?,Nice proofs of ?,\zeta(4) = \frac{\pi^4}{90},"I know some nice ways to prove that $\zeta(2) = \sum_{n=1}^{\infty} \frac{1}{n^2} = \pi^2/6$.  For example, see Robin Chapman's list or the answers to the question "" Different methods to compute $\sum_{n=1}^{\infty} \frac{1}{n^2}$ ?"" Are there any nice ways to prove that $$\zeta(4) = \sum_{n=1}^{\infty} \frac{1}{n^4} = \frac{\pi^4}{90}?$$ I already know some proofs that give all values of $\zeta(n)$ for positive even integers $n$ (like #7 on Robin Chapman's list or Qiaochu Yuan's answer in the linked question).  I'm not so much interested in those kinds of proofs as I am those that are specifically for $\zeta(4)$. I would be particularly interested in a proof that isn't an adaption of one that $\zeta(2) = \pi^2/6$.","I know some nice ways to prove that $\zeta(2) = \sum_{n=1}^{\infty} \frac{1}{n^2} = \pi^2/6$.  For example, see Robin Chapman's list or the answers to the question "" Different methods to compute $\sum_{n=1}^{\infty} \frac{1}{n^2}$ ?"" Are there any nice ways to prove that $$\zeta(4) = \sum_{n=1}^{\infty} \frac{1}{n^4} = \frac{\pi^4}{90}?$$ I already know some proofs that give all values of $\zeta(n)$ for positive even integers $n$ (like #7 on Robin Chapman's list or Qiaochu Yuan's answer in the linked question).  I'm not so much interested in those kinds of proofs as I am those that are specifically for $\zeta(4)$. I would be particularly interested in a proof that isn't an adaption of one that $\zeta(2) = \pi^2/6$.",,"['calculus', 'real-analysis', 'sequences-and-series', 'complex-analysis', 'riemann-zeta']"
33,Deriving a weak form of Stirling's formula,Deriving a weak form of Stirling's formula,,"I'm reading a book which gives a recipe of sorts to derive the approximations: $$\ln(n!) = \sum_{m=1}^{n} \ln m= n\ln n -n + O(\ln n)$$ I get stuck towards the end of the derivation, here goes: for $m\ge1$, we have $$0 \le \int_m^{m+1}\ln x\, dx - \ln m = \int_m^{m+1}\ln \frac{x}{m}\, dx \le \int_m^{m+1}\frac{x}{m}-1\, dx  = \frac{1}{2m}$$ So far so good. Now we're told to sum for $m = 1, 2, ... n-1$, and use the fact that  primitive of $\ln x$ is $x\ln x - x$. Let's go ahead and do that; by summing we get: $$0\le\int_{1}^{n} \ln x\,dx - \sum_{m=1}^{n-1} \ln m \le \frac{1}{2} \sum_{m=1}^{n-1}\frac{1}{m} $$ and simplifying the integral: $$0\le n\ln n - n + 1 - \sum_{m=1}^{n-1} \ln m \le \frac{1}{2} \sum_{m=1}^{n-1}\frac{1}{m} $$ This is where the instructions stop. Now it feels like we are very close to the result, however I'm unable to conclude. I tried to use the fact that $$\sum_{m=1}^{n-1}\frac{1}{m} \le 1 + \ln(n-1)$$ but to no avail. The problem seems to be that our sums go to $n-1$ but we always have $n\ln n - n$ (Though we do seem to be getting the $n-1$). Any step in the right direction is welcome. EDIT It took a while but I did get it in the end. $$0\le n\ln n - n + 1 - \sum_{m=1}^{n-1} \ln m \le \frac{1}{2} (1+\ln (n-1)) $$ $$0\le n\ln n - n + 1 - \sum_{m=1}^{n-1} \ln m \le 1 + \ln (n-1) $$ $$-1 \le n\ln n - n - \sum_{m=1}^{n-1} \ln m \le \ln (n-1) \le \ln(n)$$ $$-1 - \ln(n) \le n\ln n - n - \sum_{m=1}^{n} \ln m \le 0$$ and finally for $n\ge2$, $1 \le 2\ln 2$, so  $$-3\ln(n) \le n\ln n - n - \sum_{m=1}^{n} \ln m \le 0$$. We can happily conclude : $$\left|n\ln n - n - \sum_{m=1}^{n} \ln m \right| \le 3\ln n$$ Hence $$\ln(n!) = n\ln n -n + O(\ln n)$$ I'm unsure what to do about a question which I've managed to answer so I'll leave it up here.","I'm reading a book which gives a recipe of sorts to derive the approximations: $$\ln(n!) = \sum_{m=1}^{n} \ln m= n\ln n -n + O(\ln n)$$ I get stuck towards the end of the derivation, here goes: for $m\ge1$, we have $$0 \le \int_m^{m+1}\ln x\, dx - \ln m = \int_m^{m+1}\ln \frac{x}{m}\, dx \le \int_m^{m+1}\frac{x}{m}-1\, dx  = \frac{1}{2m}$$ So far so good. Now we're told to sum for $m = 1, 2, ... n-1$, and use the fact that  primitive of $\ln x$ is $x\ln x - x$. Let's go ahead and do that; by summing we get: $$0\le\int_{1}^{n} \ln x\,dx - \sum_{m=1}^{n-1} \ln m \le \frac{1}{2} \sum_{m=1}^{n-1}\frac{1}{m} $$ and simplifying the integral: $$0\le n\ln n - n + 1 - \sum_{m=1}^{n-1} \ln m \le \frac{1}{2} \sum_{m=1}^{n-1}\frac{1}{m} $$ This is where the instructions stop. Now it feels like we are very close to the result, however I'm unable to conclude. I tried to use the fact that $$\sum_{m=1}^{n-1}\frac{1}{m} \le 1 + \ln(n-1)$$ but to no avail. The problem seems to be that our sums go to $n-1$ but we always have $n\ln n - n$ (Though we do seem to be getting the $n-1$). Any step in the right direction is welcome. EDIT It took a while but I did get it in the end. $$0\le n\ln n - n + 1 - \sum_{m=1}^{n-1} \ln m \le \frac{1}{2} (1+\ln (n-1)) $$ $$0\le n\ln n - n + 1 - \sum_{m=1}^{n-1} \ln m \le 1 + \ln (n-1) $$ $$-1 \le n\ln n - n - \sum_{m=1}^{n-1} \ln m \le \ln (n-1) \le \ln(n)$$ $$-1 - \ln(n) \le n\ln n - n - \sum_{m=1}^{n} \ln m \le 0$$ and finally for $n\ge2$, $1 \le 2\ln 2$, so  $$-3\ln(n) \le n\ln n - n - \sum_{m=1}^{n} \ln m \le 0$$. We can happily conclude : $$\left|n\ln n - n - \sum_{m=1}^{n} \ln m \right| \le 3\ln n$$ Hence $$\ln(n!) = n\ln n -n + O(\ln n)$$ I'm unsure what to do about a question which I've managed to answer so I'll leave it up here.",,"['analysis', 'inequality']"
34,"If $F:[a,b] \rightarrow \mathbb{R}$ is not Lipschitz on $[a,b]$ , then there is a point $x_0 \in [a,b]$ with certain properties.","If  is not Lipschitz on  , then there is a point  with certain properties.","F:[a,b] \rightarrow \mathbb{R} [a,b] x_0 \in [a,b]","Suppose that  $F:[a,b] \rightarrow \mathbb{R}$ is not Lipschitz in $[a,b]$, then there exists $x_0 \in  [a,b]$ such that for any $\delta(x_0)>0$(delta neighbourhood of $x_0$) there is a sequence $(x_n)$ in $(x_0-\delta(x_0),x_0+\delta(x_0))$ such that $x_n \rightarrow x_0$ and the sequence $\frac{|F(x_0)-F(x_n)|}{|x_0-x_n|}$ does not converge to any real number $L$. Can somebody help me proving this? I need this to prove something. I have examples: $\sqrt{x}$ for $x \in [0,\infty]$ is not Lipschitz on $[0,\infty]$  and for any $\delta > 0$ neighbourhood of $0$, we can find a sequence $(x_n)$ that converges  to $0$, and the sequence $\frac{|F(x_0)-F(x_n)|}{|x_0-x_n|}$ does not converge to any real number $L$. but, I have trouble in the function $F(x)={xsin(\frac{1}{x})}\, \text{for} \,  0<x\leq 1;F(x)=0\, for \, x=0$. I know that $F(x)$ is not Lipschitz in $[0,1]$ maybe because of $0$. But, I can't find a sequence that converges to $0$ and $\frac{|F(x_0)-F(x_n)|}{|x_0-x_n|}$ does not converge to any real number $L$.","Suppose that  $F:[a,b] \rightarrow \mathbb{R}$ is not Lipschitz in $[a,b]$, then there exists $x_0 \in  [a,b]$ such that for any $\delta(x_0)>0$(delta neighbourhood of $x_0$) there is a sequence $(x_n)$ in $(x_0-\delta(x_0),x_0+\delta(x_0))$ such that $x_n \rightarrow x_0$ and the sequence $\frac{|F(x_0)-F(x_n)|}{|x_0-x_n|}$ does not converge to any real number $L$. Can somebody help me proving this? I need this to prove something. I have examples: $\sqrt{x}$ for $x \in [0,\infty]$ is not Lipschitz on $[0,\infty]$  and for any $\delta > 0$ neighbourhood of $0$, we can find a sequence $(x_n)$ that converges  to $0$, and the sequence $\frac{|F(x_0)-F(x_n)|}{|x_0-x_n|}$ does not converge to any real number $L$. but, I have trouble in the function $F(x)={xsin(\frac{1}{x})}\, \text{for} \,  0<x\leq 1;F(x)=0\, for \, x=0$. I know that $F(x)$ is not Lipschitz in $[0,1]$ maybe because of $0$. But, I can't find a sequence that converges to $0$ and $\frac{|F(x_0)-F(x_n)|}{|x_0-x_n|}$ does not converge to any real number $L$.",,"['calculus', 'real-analysis', 'analysis']"
35,Interchanging limits in a special case,Interchanging limits in a special case,,"In general, one may not interchange limits (eg, $\lim_k f_k(\lim_n x_n)\neq \lim_n\lim_k f_k(x_n)$ in most cases) without requiring that the family $\{f_k\}$ is uniformly continuous. However, can you help me in the following situation? Let $X$ be a compact metric space (think unit interval $[0,1]$ if you prefer), with $f:X\rightarrow X$ a homeomorphism (so, 1-1 continuous with continuous inverse). We do not know that the family $\{f^n\}_{n\in\mathbb{Z}}$ is uniformly continuous, but we do know that there is a convergent sequence $\{f^{k'}\}$ (where by convergent we mean pointwise convergent) and that there is also a convergent sequence $\{f^{n'}(x)\}$ (where here convergence is of a sequence of points in $X$ with its compact metric topology). Are we allowed to do the following: $$ \lim_{k'\rightarrow +\infty} f^{k'}(\lim_{n'\rightarrow+\infty}f^{n'}(x))=\lim_{n'\rightarrow+\infty}\lim_{k'\rightarrow+\infty} f^{k'+n'}(x)= \lim_{n'\rightarrow +\infty} f^{n'}(\lim_{k'\rightarrow+\infty}f^{k'}(x)) $$ I think the answer is 'no', but a counterexmple would be useful. I've also looked at this question , but it's not quite what I'm looking for. Thanks!","In general, one may not interchange limits (eg, $\lim_k f_k(\lim_n x_n)\neq \lim_n\lim_k f_k(x_n)$ in most cases) without requiring that the family $\{f_k\}$ is uniformly continuous. However, can you help me in the following situation? Let $X$ be a compact metric space (think unit interval $[0,1]$ if you prefer), with $f:X\rightarrow X$ a homeomorphism (so, 1-1 continuous with continuous inverse). We do not know that the family $\{f^n\}_{n\in\mathbb{Z}}$ is uniformly continuous, but we do know that there is a convergent sequence $\{f^{k'}\}$ (where by convergent we mean pointwise convergent) and that there is also a convergent sequence $\{f^{n'}(x)\}$ (where here convergence is of a sequence of points in $X$ with its compact metric topology). Are we allowed to do the following: $$ \lim_{k'\rightarrow +\infty} f^{k'}(\lim_{n'\rightarrow+\infty}f^{n'}(x))=\lim_{n'\rightarrow+\infty}\lim_{k'\rightarrow+\infty} f^{k'+n'}(x)= \lim_{n'\rightarrow +\infty} f^{n'}(\lim_{k'\rightarrow+\infty}f^{k'}(x)) $$ I think the answer is 'no', but a counterexmple would be useful. I've also looked at this question , but it's not quite what I'm looking for. Thanks!",,"['calculus', 'analysis', 'limits']"
36,For $A\subset\Bbb R^n$ and $B\subset \Bbb R^m$ show that $\dim_H(A\times B)=\dim_H(A)+\dim_H(B)$,For  and  show that,A\subset\Bbb R^n B\subset \Bbb R^m \dim_H(A\times B)=\dim_H(A)+\dim_H(B),"Im stuck with this exercise For $A\subset\Bbb R^n$ and $B\subset \Bbb R^m$ show that $\dim_H(A\times B)=\dim_H(A)+\dim_H(B)$ where $\dim_H$ is the Hausdorff dimension. I know that when $A$ and $B$ are open the above holds. However Im unable to generalize the result. I tried to relate the following inequalities and identities $$\operatorname{diam}(A)\lor\operatorname{diam}(B)\le\operatorname{diam}(A\times B)\le\operatorname{diam}(A)+\operatorname{diam}(B)\tag1$$ $$\mathcal H_*^r(A)<\infty\implies\mathcal H_*^s(A)=0,\quad\forall s>r\\ \mathcal H_*^r(A)>0\implies\mathcal H_*^s(A)=\infty,\quad\forall s\in[0,r)\tag2$$ $$x,y\in[0,1]\implies xy<x+y\,\text{ and }\, x^{r+s}<x^r,\quad\forall r,s>0\tag3$$ $$\inf A+\inf B=\inf(A+B)\text{ and }\sup A+\sup B=\sup(A+B)\tag4$$ where $\mathcal H_*^s$ is the $s$ -dimensional Hausdorff outer measure and $\rm diam$ is the diameter of a set. By example I find that $$\dim_H(A)+\dim_H(B)=\inf\{r>0:\exists \alpha\in[0,r]\text{ such that }\mathcal H_*^{r-\alpha}(A)+\mathcal H_*^\alpha(B)=0\}\tag5$$ so a line of action is try to relate $(2)$ and $(5)$ in something like $$\mathcal H_*^{r-\alpha}(A)+\mathcal H_*^\alpha(B)=0\implies\mathcal H_*^r(A\times B)=0\tag6$$ using the definition of $\mathcal H_*^s$ , $(1)$ and maybe $(3)$ . However I found nothing. Some help will be appreciated. EDIT: to clarify some things: from the definitions of Hausdorff dimension the statement to be proved can be stated as $$ \begin{align}\dim_H(A)+\dim_H(B)&=\inf\left\{s+t>0:\sup_\epsilon\inf\left\{\sum_{k=0}^\infty a_k^r+b_k^s: a_k,b_k<\epsilon\right\}=0\right\}\\ &=\inf\left\{s+t>0:\sup_\epsilon\inf\left\{\sum_{k=0}^\infty c_k^{r+s}: c_k<\epsilon\right\}=0\right\}\\ &=\sup\left\{s+t\ge0:\sup_\epsilon\inf\left\{\sum_{k=0}^\infty a_k^r+b_k^s: a_k,b_k<\epsilon\right\}=\infty\right\}\\ &=\sup\left\{s+t\ge0:\sup_\epsilon\inf\left\{\sum_{k=0}^\infty c_k^{r+s}: c_k<\epsilon\right\}=\infty\right\}\\ &=\dim_H(A\times B)\end{align}\tag{*} $$ where $a_k,b_k,c_k$ are the diameters of sequences of covers $(A_k),(B_k),(C_k)$ of $A\subset\Bbb R^n$ , $B\subset\Bbb R^m$ and $A\times B\subset\Bbb R^{n+m}$ respectively. This together with $(2)$ seems the way to go, however I can't found appropriate bounds because I can't relate covers $A$ and $B$ with covers of $A\times B$ such that it make possible to find these bounds. At my disposal, in the context where this exercise appear, there is not too many theorems to solve this exercise, by example I dont know the Frostman's lemma that @DavidUlrich state in the comment. I need to solve it from elementary theorems as the stated above. P.S.: I know some more identities related to the Hausdorff dimension, by example that is an increasing function or that $\dim_H(f(A))\le\dim_H(A)$ for $f$ Lipschitz or that $\dim_H(\bigcup_k A_k)=\sup_k\dim_H(A_k)$ .","Im stuck with this exercise For and show that where is the Hausdorff dimension. I know that when and are open the above holds. However Im unable to generalize the result. I tried to relate the following inequalities and identities where is the -dimensional Hausdorff outer measure and is the diameter of a set. By example I find that so a line of action is try to relate and in something like using the definition of , and maybe . However I found nothing. Some help will be appreciated. EDIT: to clarify some things: from the definitions of Hausdorff dimension the statement to be proved can be stated as where are the diameters of sequences of covers of , and respectively. This together with seems the way to go, however I can't found appropriate bounds because I can't relate covers and with covers of such that it make possible to find these bounds. At my disposal, in the context where this exercise appear, there is not too many theorems to solve this exercise, by example I dont know the Frostman's lemma that @DavidUlrich state in the comment. I need to solve it from elementary theorems as the stated above. P.S.: I know some more identities related to the Hausdorff dimension, by example that is an increasing function or that for Lipschitz or that .","A\subset\Bbb R^n B\subset \Bbb R^m \dim_H(A\times B)=\dim_H(A)+\dim_H(B) \dim_H A B \operatorname{diam}(A)\lor\operatorname{diam}(B)\le\operatorname{diam}(A\times B)\le\operatorname{diam}(A)+\operatorname{diam}(B)\tag1 \mathcal H_*^r(A)<\infty\implies\mathcal H_*^s(A)=0,\quad\forall s>r\\
\mathcal H_*^r(A)>0\implies\mathcal H_*^s(A)=\infty,\quad\forall s\in[0,r)\tag2 x,y\in[0,1]\implies xy<x+y\,\text{ and }\, x^{r+s}<x^r,\quad\forall r,s>0\tag3 \inf A+\inf B=\inf(A+B)\text{ and }\sup A+\sup B=\sup(A+B)\tag4 \mathcal H_*^s s \rm diam \dim_H(A)+\dim_H(B)=\inf\{r>0:\exists \alpha\in[0,r]\text{ such that }\mathcal H_*^{r-\alpha}(A)+\mathcal H_*^\alpha(B)=0\}\tag5 (2) (5) \mathcal H_*^{r-\alpha}(A)+\mathcal H_*^\alpha(B)=0\implies\mathcal H_*^r(A\times B)=0\tag6 \mathcal H_*^s (1) (3) 
\begin{align}\dim_H(A)+\dim_H(B)&=\inf\left\{s+t>0:\sup_\epsilon\inf\left\{\sum_{k=0}^\infty a_k^r+b_k^s: a_k,b_k<\epsilon\right\}=0\right\}\\
&=\inf\left\{s+t>0:\sup_\epsilon\inf\left\{\sum_{k=0}^\infty c_k^{r+s}: c_k<\epsilon\right\}=0\right\}\\
&=\sup\left\{s+t\ge0:\sup_\epsilon\inf\left\{\sum_{k=0}^\infty a_k^r+b_k^s: a_k,b_k<\epsilon\right\}=\infty\right\}\\
&=\sup\left\{s+t\ge0:\sup_\epsilon\inf\left\{\sum_{k=0}^\infty c_k^{r+s}: c_k<\epsilon\right\}=\infty\right\}\\
&=\dim_H(A\times B)\end{align}\tag{*}
 a_k,b_k,c_k (A_k),(B_k),(C_k) A\subset\Bbb R^n B\subset\Bbb R^m A\times B\subset\Bbb R^{n+m} (2) A B A\times B \dim_H(f(A))\le\dim_H(A) f \dim_H(\bigcup_k A_k)=\sup_k\dim_H(A_k)","['real-analysis', 'analysis', 'measure-theory', 'dimension-theory-analysis', 'hausdorff-measure']"
37,Inequality of $(|a|^{\ell-2}a- |b|^{\ell-2}b\Big)(a-b) $,Inequality of,(|a|^{\ell-2}a- |b|^{\ell-2}b\Big)(a-b) ,"Letting $\epsilon\geq0$ be small, $a,b\in \mathbb{R}^N$ and $\ell >1$.  Is the following inquality $$\tag{E}\label{E} \Big((\sqrt{\epsilon^2+|a|^2})^{\ell-2}a- (\sqrt{\epsilon^2+|b|^2})^{\ell-2}b\Big)(a-b)\geq 0, $$ true? If $\epsilon=0$, \eqref{E} becomes  $$(|a|^{\ell-2}a- |b|^{\ell-2}b\Big)(a-b)\geq 0, $$ which is true by using the following inequalities:  \begin{align} \left(|a|^{\ell-2}a-|b|^{\ell-2}b\right)\cdot (a-b)\geq 2^{-\ell}|a-b|^2,&\quad  \ell\geq 2,\\ \left(|a|^{\ell-2}a-|b|^{\ell-2}b\right)\cdot (a-b)\geq (\ell-1)|a-b|^2\left(|a|+|b|\right)^{\ell-2}  ,&\quad  1<\ell \leq 2. \end{align}  $a\cdot b$ denotes the usual inner product in $\mathbb{R}^N$. Can any one know if \eqref{E} is true or not? Here my try for the answer: Letting $\epsilon$ small. Denoting by $f$ the function defined as $$f(\mathbf{x})=\frac{1}{\ell}(\epsilon^2+|\mathbf{x}|^2)^{\ell/2}.$$  $f$ is convexe in $\mathbf{x}$. Then, by the convexity proprity we have  $$(f'(\mathbf{x})-f'(\mathbf{y}))\cdot (\mathbf{x}-\mathbf{y})\geq c\|\mathbf{x}-\mathbf{y}\|^2.$$ Then \eqref{E} holds. I don't know there is a mistake or not.","Letting $\epsilon\geq0$ be small, $a,b\in \mathbb{R}^N$ and $\ell >1$.  Is the following inquality $$\tag{E}\label{E} \Big((\sqrt{\epsilon^2+|a|^2})^{\ell-2}a- (\sqrt{\epsilon^2+|b|^2})^{\ell-2}b\Big)(a-b)\geq 0, $$ true? If $\epsilon=0$, \eqref{E} becomes  $$(|a|^{\ell-2}a- |b|^{\ell-2}b\Big)(a-b)\geq 0, $$ which is true by using the following inequalities:  \begin{align} \left(|a|^{\ell-2}a-|b|^{\ell-2}b\right)\cdot (a-b)\geq 2^{-\ell}|a-b|^2,&\quad  \ell\geq 2,\\ \left(|a|^{\ell-2}a-|b|^{\ell-2}b\right)\cdot (a-b)\geq (\ell-1)|a-b|^2\left(|a|+|b|\right)^{\ell-2}  ,&\quad  1<\ell \leq 2. \end{align}  $a\cdot b$ denotes the usual inner product in $\mathbb{R}^N$. Can any one know if \eqref{E} is true or not? Here my try for the answer: Letting $\epsilon$ small. Denoting by $f$ the function defined as $$f(\mathbf{x})=\frac{1}{\ell}(\epsilon^2+|\mathbf{x}|^2)^{\ell/2}.$$  $f$ is convexe in $\mathbf{x}$. Then, by the convexity proprity we have  $$(f'(\mathbf{x})-f'(\mathbf{y}))\cdot (\mathbf{x}-\mathbf{y})\geq c\|\mathbf{x}-\mathbf{y}\|^2.$$ Then \eqref{E} holds. I don't know there is a mistake or not.",,"['calculus', 'real-analysis', 'analysis', 'functions']"
38,Product of two Lebesgue measurable set is measurable,Product of two Lebesgue measurable set is measurable,,"Let $A,B \subset \mathbb{R}$ two bounded and Lebesgue measurable sets. I have to show that $A\times B \subset \mathbb{R}^2$ is measurable and \begin{align*} \lambda(A \times B) = \lambda (A) \cdot \lambda (B) \end{align*} where $\lambda$ is the Lebesgue measure. I tried to show the equality of the measure using the Fubini's Theorem with the indicators function but I'm not sure is the right way and I have no idea how to show that $A \times B$ is measurable. Any suggestions? Thanks in advance!!","Let $A,B \subset \mathbb{R}$ two bounded and Lebesgue measurable sets. I have to show that $A\times B \subset \mathbb{R}^2$ is measurable and \begin{align*} \lambda(A \times B) = \lambda (A) \cdot \lambda (B) \end{align*} where $\lambda$ is the Lebesgue measure. I tried to show the equality of the measure using the Fubini's Theorem with the indicators function but I'm not sure is the right way and I have no idea how to show that $A \times B$ is measurable. Any suggestions? Thanks in advance!!",,"['real-analysis', 'analysis', 'measure-theory', 'lebesgue-integral', 'lebesgue-measure']"
39,Poincare-Wirtinger inequality for $H^{1}(D)$.,Poincare-Wirtinger inequality for .,H^{1}(D),"I have a question about Poincare-Wirtinger inequality for $H^{1}(D)$. Let $D$ is an open subset of $\mathbb{R}^d$. We define $H^{1}(D)$ by \begin{equation*} H^{1}(D)=\{f \in L^{2}(D,m): \frac{\partial f}{\partial x_i} \in L^{2}(D,m),\ 1\le i \le d\}, \end{equation*} where $\partial f/\partial x_i$ is the distributional derivative of $f$ and $m$ is the Lebesgue measure on $D$. $H^{1}(D)$ becomes a Hilbert space with the usual Sobolev norm $\|\cdot\|_{H^{1}(D)}$. In the following, we write $\nabla f$ for $(\partial f/\partial x_1,\ldots \partial f/\partial x_d)$. We also write $\|f\|$ for $(\int_{D}f^2\,dx)^{1/2}$ [Theorem 1] Let $D$ be a connected open subset of $\mathbb{R}^d$ with finite volume. We assume the canonical embedding $H^{1}(D) \subset L^{2}(D,m)$ is compact. Then, there exists a positive constant $C>0$ such that for any $f \in H^{1}(D)$   \begin{equation*} \|f-f_{D}\|\le C \|\nabla f\|, \end{equation*}   where $f_{D}:=m(D)^{-1}\int_{D}f\,dm$ [Proof of Theorem 1] Define $X \subset H^{1}(D)$ by $\{f \in H^{1}(D) : f_{D}=0 \}$. Then, $X$ is a closed subspace of $H^{1}(D)$. It suffices to prove there exists a $C>0$ such that  \begin{equation*} (*)\quad \|f\|\le C \|\nabla f\| \ \text{ for any }f \in X \end{equation*} We prove $(*)$ by contradiction. We assume there exist a $\{f_n\} \subset X$ such that $\|f_n\|>n \|\nabla f_n\|$. We may assume $\|f_n\|_{H^{1}(D)}=1$. As $f_n$ is a bounded sequence in $X$, there exists a subsequence $f_{n_k}$ of $f_n$ and a $f \in X$ such that $f_{n_k} \to f$ weakly in $X$. $f_{n_k}$ is also denoted by $f_n$. By assumption, $f_n \to f$ in $L^{2}(D,m)$. Since  $$1=\|f_n\|_{H^{1}(D)}\text{ and }\|f_n\|>n \|\nabla f_n\|$$ holds, $\|f_n\|\to \underline{1=\|f\|}$ and $\|\nabla f_n\| \to 0$. Since $f_n \to f$ in $L^{2}(D,dx)$ and $\|\nabla f_n\| \to 0$, \begin{equation*} \int_{D}(\nabla f_n,\nabla g)+\int_{D}f_n g\,dx\to \int_{D}fg\,dx. \end{equation*} for any $g \in X$. On the other hand, since $f_n$ convenes to $f$ weakly in $X$, \begin{equation*} \int_{D}(\nabla f_n,\nabla g)dx+\int_{D}f_n g\,dx\to \int_{D} (\nabla f,\nabla g)\,dx+\int_{D}fg\,dx. \end{equation*} Hence $\int_{D}(\nabla f,\nabla g)dx=0$ for any $g \in X$, which implies $\nabla f=0$ and $\Delta f=0$ in weak sense. By Wyle's Lemma $f$ is  smooth on $D$. Hence, $\nabla  f=0$ in classical sense. Since $D$ is connected, $f$ is a some constant. Since $ f \in X$, we have $f=0$, which contradicts to $\underline{1=\|f\|}$. My question and remark From the above proof, we do not know concrete estimates of C. If you know an another proof, please let me know.","I have a question about Poincare-Wirtinger inequality for $H^{1}(D)$. Let $D$ is an open subset of $\mathbb{R}^d$. We define $H^{1}(D)$ by \begin{equation*} H^{1}(D)=\{f \in L^{2}(D,m): \frac{\partial f}{\partial x_i} \in L^{2}(D,m),\ 1\le i \le d\}, \end{equation*} where $\partial f/\partial x_i$ is the distributional derivative of $f$ and $m$ is the Lebesgue measure on $D$. $H^{1}(D)$ becomes a Hilbert space with the usual Sobolev norm $\|\cdot\|_{H^{1}(D)}$. In the following, we write $\nabla f$ for $(\partial f/\partial x_1,\ldots \partial f/\partial x_d)$. We also write $\|f\|$ for $(\int_{D}f^2\,dx)^{1/2}$ [Theorem 1] Let $D$ be a connected open subset of $\mathbb{R}^d$ with finite volume. We assume the canonical embedding $H^{1}(D) \subset L^{2}(D,m)$ is compact. Then, there exists a positive constant $C>0$ such that for any $f \in H^{1}(D)$   \begin{equation*} \|f-f_{D}\|\le C \|\nabla f\|, \end{equation*}   where $f_{D}:=m(D)^{-1}\int_{D}f\,dm$ [Proof of Theorem 1] Define $X \subset H^{1}(D)$ by $\{f \in H^{1}(D) : f_{D}=0 \}$. Then, $X$ is a closed subspace of $H^{1}(D)$. It suffices to prove there exists a $C>0$ such that  \begin{equation*} (*)\quad \|f\|\le C \|\nabla f\| \ \text{ for any }f \in X \end{equation*} We prove $(*)$ by contradiction. We assume there exist a $\{f_n\} \subset X$ such that $\|f_n\|>n \|\nabla f_n\|$. We may assume $\|f_n\|_{H^{1}(D)}=1$. As $f_n$ is a bounded sequence in $X$, there exists a subsequence $f_{n_k}$ of $f_n$ and a $f \in X$ such that $f_{n_k} \to f$ weakly in $X$. $f_{n_k}$ is also denoted by $f_n$. By assumption, $f_n \to f$ in $L^{2}(D,m)$. Since  $$1=\|f_n\|_{H^{1}(D)}\text{ and }\|f_n\|>n \|\nabla f_n\|$$ holds, $\|f_n\|\to \underline{1=\|f\|}$ and $\|\nabla f_n\| \to 0$. Since $f_n \to f$ in $L^{2}(D,dx)$ and $\|\nabla f_n\| \to 0$, \begin{equation*} \int_{D}(\nabla f_n,\nabla g)+\int_{D}f_n g\,dx\to \int_{D}fg\,dx. \end{equation*} for any $g \in X$. On the other hand, since $f_n$ convenes to $f$ weakly in $X$, \begin{equation*} \int_{D}(\nabla f_n,\nabla g)dx+\int_{D}f_n g\,dx\to \int_{D} (\nabla f,\nabla g)\,dx+\int_{D}fg\,dx. \end{equation*} Hence $\int_{D}(\nabla f,\nabla g)dx=0$ for any $g \in X$, which implies $\nabla f=0$ and $\Delta f=0$ in weak sense. By Wyle's Lemma $f$ is  smooth on $D$. Hence, $\nabla  f=0$ in classical sense. Since $D$ is connected, $f$ is a some constant. Since $ f \in X$, we have $f=0$, which contradicts to $\underline{1=\|f\|}$. My question and remark From the above proof, we do not know concrete estimates of C. If you know an another proof, please let me know.",,"['real-analysis', 'analysis', 'proof-verification']"
40,An inequality involving a quasiconvex function with binomial and power terms,An inequality involving a quasiconvex function with binomial and power terms,,"The question is as follows. Let us consider a positive integer number $x \in \{1,2,3,...\}$ and a positive real number $q \in [1,x]$. Show that \begin{equation} \sum_{m=0}^{x-1} \left( \frac{x+2-q}{m+1} -\frac{x+q}{\min(m+q,x)} \right) \binom{x-1}{m} \left ( \frac{x-q}{x+q}\right)^m \geq 0. \end{equation} I've tried to solve the inequality as follows. First, let us denote the LHS as $f(x,q)$ \begin{equation} f(x,q)=\sum_{m=0}^{x-1} \left( \frac{x+2-q}{m+1} -\frac{x+q}{\min(m+q,x)} \right) \binom{x-1}{m} \left ( \frac{x-q}{x+q}\right)^m. \end{equation}  I want to show that $f(x,q) \geq 0$ for $x \in \{1,2,3,...\}$ and any positive real number $q \in [1,x]$. It is easy to verify that $f(x,1)=0$ and $f(x,x)=0$. The first idea I pursued was to try by induction over $x$, while considering $f(x,q)$ with a fixed $q$. In fact, numerical results seem to verify that $f(x+1,q) \geq f(x,q)$ (the inequality is strict for $q > 1$). I tried to prove it by induction but I could not succed as I could not manage the terms $\binom{x-1}{m} \left ( \frac{x-q}{x+q}\right)^m$, which are very difficult to manipolate. The second way I tried was to consider $f(x,q)$ by fixing $x$ and letting $q$ to vary in $[1,x]$. In this case $f(x,q)=0$ at the two extremes of the interval ($q=1$ and $q=x$), while numerical results show that $f(x,q)$ is greater than zero inside the interval, suggesting that $f(x,q)$ is quasiconvex in $q \in [1,x]$. My idea was to consider the different intervals of $q$ given by $q \in [1,2)$, $[2,3)$ (where $f(x,q)$ is differentiable), and show that in each of this interval the function is either convex or concave, while being quasiconcave all over the interval. In fact, by considering $q \in [a,a+1)$, for any integer $a$ in $[1,x]$, we have that \begin{equation} f(x,q)=\sum_{m=0}^{x-a-1} \left( \frac{x+2-q}{m+1} -\frac{x+q}{m+q} \right) \binom{x-1}{m} \left ( \frac{x-q}{x+q}\right)^m + \sum_{m=x-a}^{x-1} \left( \frac{x+2-q}{m+1} -\frac{x+q}{x} \right) \binom{x-1}{m} \left ( \frac{x-q}{x+q}\right)^m . \end{equation}  However, taking the derivate with respect to $q$ is very difficult and I wasn't able to conclude anything after this. To sum up, it looks like that the issues are given by the terms $\binom{x-1}{m} \left ( \frac{x-q}{x+q}\right)^m$, which are very difficult to manage. If someone can help me by providing new directions to look at, pointing me out similar inequalities in the literature which I don't know, or give me new ideas, I would be grateful.","The question is as follows. Let us consider a positive integer number $x \in \{1,2,3,...\}$ and a positive real number $q \in [1,x]$. Show that \begin{equation} \sum_{m=0}^{x-1} \left( \frac{x+2-q}{m+1} -\frac{x+q}{\min(m+q,x)} \right) \binom{x-1}{m} \left ( \frac{x-q}{x+q}\right)^m \geq 0. \end{equation} I've tried to solve the inequality as follows. First, let us denote the LHS as $f(x,q)$ \begin{equation} f(x,q)=\sum_{m=0}^{x-1} \left( \frac{x+2-q}{m+1} -\frac{x+q}{\min(m+q,x)} \right) \binom{x-1}{m} \left ( \frac{x-q}{x+q}\right)^m. \end{equation}  I want to show that $f(x,q) \geq 0$ for $x \in \{1,2,3,...\}$ and any positive real number $q \in [1,x]$. It is easy to verify that $f(x,1)=0$ and $f(x,x)=0$. The first idea I pursued was to try by induction over $x$, while considering $f(x,q)$ with a fixed $q$. In fact, numerical results seem to verify that $f(x+1,q) \geq f(x,q)$ (the inequality is strict for $q > 1$). I tried to prove it by induction but I could not succed as I could not manage the terms $\binom{x-1}{m} \left ( \frac{x-q}{x+q}\right)^m$, which are very difficult to manipolate. The second way I tried was to consider $f(x,q)$ by fixing $x$ and letting $q$ to vary in $[1,x]$. In this case $f(x,q)=0$ at the two extremes of the interval ($q=1$ and $q=x$), while numerical results show that $f(x,q)$ is greater than zero inside the interval, suggesting that $f(x,q)$ is quasiconvex in $q \in [1,x]$. My idea was to consider the different intervals of $q$ given by $q \in [1,2)$, $[2,3)$ (where $f(x,q)$ is differentiable), and show that in each of this interval the function is either convex or concave, while being quasiconcave all over the interval. In fact, by considering $q \in [a,a+1)$, for any integer $a$ in $[1,x]$, we have that \begin{equation} f(x,q)=\sum_{m=0}^{x-a-1} \left( \frac{x+2-q}{m+1} -\frac{x+q}{m+q} \right) \binom{x-1}{m} \left ( \frac{x-q}{x+q}\right)^m + \sum_{m=x-a}^{x-1} \left( \frac{x+2-q}{m+1} -\frac{x+q}{x} \right) \binom{x-1}{m} \left ( \frac{x-q}{x+q}\right)^m . \end{equation}  However, taking the derivate with respect to $q$ is very difficult and I wasn't able to conclude anything after this. To sum up, it looks like that the issues are given by the terms $\binom{x-1}{m} \left ( \frac{x-q}{x+q}\right)^m$, which are very difficult to manage. If someone can help me by providing new directions to look at, pointing me out similar inequalities in the literature which I don't know, or give me new ideas, I would be grateful.",,"['real-analysis', 'analysis', 'functions', 'inequality', 'convex-analysis']"
41,A property for real valued functions,A property for real valued functions,,"Let $f$ be a real valued continuous function over a  compact topological space $X$, say [0, 1], I am looking for a condition on $f$ such that if there exist real valued continuous functions $g_1,...,g_n , f_1,...,f_n$ over $X$ with $f=f_1g_1+...+f_ng_n$, then there exist $i , j\in\{ 1,...n \} $ and a real valued continuous functions $h$ and $k$ over $X$ such that $f=hf_i$ and $f=kg_j$?(that is, if a function is a linear combenition of some functions with functional-coefficients then that function is a multiple of one of them with functional-coefficient)","Let $f$ be a real valued continuous function over a  compact topological space $X$, say [0, 1], I am looking for a condition on $f$ such that if there exist real valued continuous functions $g_1,...,g_n , f_1,...,f_n$ over $X$ with $f=f_1g_1+...+f_ng_n$, then there exist $i , j\in\{ 1,...n \} $ and a real valued continuous functions $h$ and $k$ over $X$ such that $f=hf_i$ and $f=kg_j$?(that is, if a function is a linear combenition of some functions with functional-coefficients then that function is a multiple of one of them with functional-coefficient)",,"['real-analysis', 'functional-analysis', 'analysis', 'functions', 'special-functions']"
42,Derivative of a limit,Derivative of a limit,,"Suppose that we are given a sequence of continuous functions $f_n$ which are non-negative, integrate to $1$ and supported on intervals $[-x_n,x_n]$ with $\sum_{n \geq 1} x_n$ converging where $x_n \geq 0$ for every $n$. If we set $F_n;=f_1 *\cdots *f_n$, where $*$ denotes convolution, is it true that $\lim_{n \to \infty} F_n$ is differentiable?","Suppose that we are given a sequence of continuous functions $f_n$ which are non-negative, integrate to $1$ and supported on intervals $[-x_n,x_n]$ with $\sum_{n \geq 1} x_n$ converging where $x_n \geq 0$ for every $n$. If we set $F_n;=f_1 *\cdots *f_n$, where $*$ denotes convolution, is it true that $\lim_{n \to \infty} F_n$ is differentiable?",,"['real-analysis', 'functional-analysis', 'analysis']"
43,How to deal with the partial inverse of a two-variable function?,How to deal with the partial inverse of a two-variable function?,,"Let $E$ be a Banach space consisting of some kinds of functions from $\mathbf R^n$ to $\mathbf R^m$. Given a two-variable function $f:\mathbf R^m\times\mathbf R^n\to\mathbf R^m$. Suppose that $f\in C(\mathbf R^m;E)$, and for any $y\in\mathbf R^n$, the function $f(\cdot,y):\mathbf R^m\to\mathbf R^m$ is bijective whose inverse is denoted as $g(x,y)=f(\cdot,y)^{-1}(x)$. Suppose also that for any $x\in\mathbf R^m$, $g(x,\cdot)\in E$. Question: Does $g$ also belong to $C(\mathbf R^m;E)$? I think the answer is negative in general. But does that hold true for some special cases such as $E=C^k(\mathbf R^n;\mathbf R^m)$ or $E=C^\alpha(\mathbf R^n;\mathbf R^m)$ or $E=L^p(\mathbf R^n;\mathbf R^m)$? Actually I have no idea to deal with the 'partial' inverse... Could anyone give some hints or comments? TIA...","Let $E$ be a Banach space consisting of some kinds of functions from $\mathbf R^n$ to $\mathbf R^m$. Given a two-variable function $f:\mathbf R^m\times\mathbf R^n\to\mathbf R^m$. Suppose that $f\in C(\mathbf R^m;E)$, and for any $y\in\mathbf R^n$, the function $f(\cdot,y):\mathbf R^m\to\mathbf R^m$ is bijective whose inverse is denoted as $g(x,y)=f(\cdot,y)^{-1}(x)$. Suppose also that for any $x\in\mathbf R^m$, $g(x,\cdot)\in E$. Question: Does $g$ also belong to $C(\mathbf R^m;E)$? I think the answer is negative in general. But does that hold true for some special cases such as $E=C^k(\mathbf R^n;\mathbf R^m)$ or $E=C^\alpha(\mathbf R^n;\mathbf R^m)$ or $E=L^p(\mathbf R^n;\mathbf R^m)$? Actually I have no idea to deal with the 'partial' inverse... Could anyone give some hints or comments? TIA...",,"['calculus', 'real-analysis', 'functional-analysis', 'analysis', 'partial-differential-equations']"
44,"Is the ""composition"" of two dense subsets of functions dense?","Is the ""composition"" of two dense subsets of functions dense?",,"Given $F \subseteq C_C(\mathbb{R}^d, \mathbb{R}^p)$, $F$ is dense in $C_C(\mathbb{R}^d, \mathbb{R}^p)$ in the supremum norm $\|\cdot\|_\infty$. Also given $G \subseteq C_C(\mathbb{R}^p, \mathbb{R}^s)$, $G$ is dense in $C_C(\mathbb{R}^p, \mathbb{R}^s)$ in $\|\cdot\|_\infty$. Is the set $G \circ F := \{g \circ f: g \in G, f \in F, g \circ f \in C_C(\mathbb{R}^d, \mathbb{R}^s)\}$ dense in $C_C(\mathbb{R}^d, \mathbb{R}^s)$? Note that $d, p, s\in \mathbb{N}$ are not necessarily equal. Any help would be much appreciated. Thanks!","Given $F \subseteq C_C(\mathbb{R}^d, \mathbb{R}^p)$, $F$ is dense in $C_C(\mathbb{R}^d, \mathbb{R}^p)$ in the supremum norm $\|\cdot\|_\infty$. Also given $G \subseteq C_C(\mathbb{R}^p, \mathbb{R}^s)$, $G$ is dense in $C_C(\mathbb{R}^p, \mathbb{R}^s)$ in $\|\cdot\|_\infty$. Is the set $G \circ F := \{g \circ f: g \in G, f \in F, g \circ f \in C_C(\mathbb{R}^d, \mathbb{R}^s)\}$ dense in $C_C(\mathbb{R}^d, \mathbb{R}^s)$? Note that $d, p, s\in \mathbb{N}$ are not necessarily equal. Any help would be much appreciated. Thanks!",,"['functional-analysis', 'analysis']"
45,The definition of integral equations (beginner question),The definition of integral equations (beginner question),,"The Fredholm equation of the first kind is  $$ f(x)=\int_a^b K(x,t)\phi(t)\, dt \tag 1 $$ Q1: Does it mean $x$ is a scalar $x\in \mathbb R$? Or is it a function $x:\mathbb R\rightarrow \mathbb R$, i.e. $x(t)$, so explicit we have $$ f(x(t))=\int_a^b K(x(t),t)\phi(t)\, dt \quad ? \tag 2 $$ Q2: And the same question for the Volterra equation of the first kind. Is it  $$ f(x)=\int_a^x K(x,t)\phi(t)\, dt \tag 3 $$ Or  $$ f(x(t))=\int_a^{x(t)} K(x(t),t)\phi(t)\, dt \quad ?\tag 4 $$ Q3: What about the Fredholm equation of the second kind and the Volterra equation of the second kind?","The Fredholm equation of the first kind is  $$ f(x)=\int_a^b K(x,t)\phi(t)\, dt \tag 1 $$ Q1: Does it mean $x$ is a scalar $x\in \mathbb R$? Or is it a function $x:\mathbb R\rightarrow \mathbb R$, i.e. $x(t)$, so explicit we have $$ f(x(t))=\int_a^b K(x(t),t)\phi(t)\, dt \quad ? \tag 2 $$ Q2: And the same question for the Volterra equation of the first kind. Is it  $$ f(x)=\int_a^x K(x,t)\phi(t)\, dt \tag 3 $$ Or  $$ f(x(t))=\int_a^{x(t)} K(x(t),t)\phi(t)\, dt \quad ?\tag 4 $$ Q3: What about the Fredholm equation of the second kind and the Volterra equation of the second kind?",,"['calculus', 'real-analysis', 'functional-analysis', 'analysis']"
46,Can this function be anything but a polynomial function?,Can this function be anything but a polynomial function?,,"If $f\in\mathcal{C}^\infty((0,1),\mathbb{R})$ verifies the following: \begin{equation} \forall x\in(0,1),\,\exists n\in\mathbb{N},\quad f^{(n)}(x)=0 \end{equation} Can $f$ be anything but a polynomial function?","If $f\in\mathcal{C}^\infty((0,1),\mathbb{R})$ verifies the following: \begin{equation} \forall x\in(0,1),\,\exists n\in\mathbb{N},\quad f^{(n)}(x)=0 \end{equation} Can $f$ be anything but a polynomial function?",,['analysis']
47,What are the functions satisfying $f\left(2\sum_{i=0}^{\infty}\frac{a_i}{3^i}\right)=\sum_{i=0}^{\infty}\frac{a_i}{2^i}$,What are the functions satisfying,f\left(2\sum_{i=0}^{\infty}\frac{a_i}{3^i}\right)=\sum_{i=0}^{\infty}\frac{a_i}{2^i},"I am interesting of finding for all monotone functions $f:[0,1]\to [0,1]$ satisfying     the relation    $$f\left(2\sum_{i=1}^{\infty}\frac{a_i}{3^i}\right)=\sum_{i=1}^{\infty}\frac{a_i}{2^i}$$   Where $a_i\in \{0,1\}$ I have tired to see whether I could make some conjecture. Namely, such function must satisfy the point-wise relations $$f(0)=0~~~~~a_i=0~~~~\mbox{for $i\ge 1$}~~~and~~~f(1)=1~~~~~a_i=1~~~~\mbox{for $i\ge 1$}$$ $$f(\frac23)=\frac12~~~~~a_1=1,a_i=0~~~~\mbox{for $i\ge 2$}$$ $$f(\frac13)=\frac12~~~~~a_1=0,a_i=1~~~~\mbox{for $i\ge 2$}$$ continuing this way we can find particular values. But I cannot find a conjecture on this Can any one help?","I am interesting of finding for all monotone functions $f:[0,1]\to [0,1]$ satisfying     the relation    $$f\left(2\sum_{i=1}^{\infty}\frac{a_i}{3^i}\right)=\sum_{i=1}^{\infty}\frac{a_i}{2^i}$$   Where $a_i\in \{0,1\}$ I have tired to see whether I could make some conjecture. Namely, such function must satisfy the point-wise relations $$f(0)=0~~~~~a_i=0~~~~\mbox{for $i\ge 1$}~~~and~~~f(1)=1~~~~~a_i=1~~~~\mbox{for $i\ge 1$}$$ $$f(\frac23)=\frac12~~~~~a_1=1,a_i=0~~~~\mbox{for $i\ge 2$}$$ $$f(\frac13)=\frac12~~~~~a_1=0,a_i=1~~~~\mbox{for $i\ge 2$}$$ continuing this way we can find particular values. But I cannot find a conjecture on this Can any one help?",,"['calculus', 'real-analysis', 'algebra-precalculus', 'analysis', 'functions']"
48,$L^1(\mathbb{R}/\mathbb{Z})$ norm of a trigonometric polynomial of Hardy-Littlewood,norm of a trigonometric polynomial of Hardy-Littlewood,L^1(\mathbb{R}/\mathbb{Z}),"For $x \in \mathbb{R}$ and $N \geq 1$ an integer, let  $$S_N(x) = \sum_{n \leq N}n^{\frac{1}{2} + in}e^{2\pi i n x}. $$  I wonder if anyone knows what is the order of  $$\int_{0}^{1}|S_N(x)|dx.$$  This polynomial was considered long ago by Hardy-Littlewood. It appears for instance in the book Trigonometric Series by Zygmund (if you have access to the book, it's in the 3rd edition, Vol 1, Chapter 5, Section 4, page 197). It follows from Theorem 4.7 there that $S_N(x) = O(N)$ uniformly in $x$. But does anyone know if one can do better in $L^1([0,1])$? Thanks!","For $x \in \mathbb{R}$ and $N \geq 1$ an integer, let  $$S_N(x) = \sum_{n \leq N}n^{\frac{1}{2} + in}e^{2\pi i n x}. $$  I wonder if anyone knows what is the order of  $$\int_{0}^{1}|S_N(x)|dx.$$  This polynomial was considered long ago by Hardy-Littlewood. It appears for instance in the book Trigonometric Series by Zygmund (if you have access to the book, it's in the 3rd edition, Vol 1, Chapter 5, Section 4, page 197). It follows from Theorem 4.7 there that $S_N(x) = O(N)$ uniformly in $x$. But does anyone know if one can do better in $L^1([0,1])$? Thanks!",,"['complex-analysis', 'analysis', 'fourier-analysis', 'fourier-series', 'analytic-number-theory']"
49,Why does this power series defined on the open unit disk take on every complex number infinitely often?,Why does this power series defined on the open unit disk take on every complex number infinitely often?,,"This is a problem from Rudin's Real and complex analysis. Let us take a sequence of integers $\{n_k\}$ with $n_1 >1$ and $n_{k+1} > 2kn_{k}.$ We let  $$f(z) = \sum_{k=1}^\infty 5^k z^{n_k}.$$ It is not too hard to prove that this series converges for $|z|<1,$ and that $f$ has no finite radial limits. However, from what I understand, it is also true that $f$ attains every value $z \in \mathbb{C}$ infinitely often. I am stuck in how to prove this. One idea was to use Rouche's theorem, but that didn't get me too far. Any hint, or solution, is welcome. EDIT Here is an idea, which I do not know if it is correct or not. Let $a \in \mathbb{C}$ and suppose that $f(z) = a$ for only finitely many values $z$ in the open unit disk. Then if we consider $f(z) -a,$ there is a polynomial $P(z)$ with the same zeros as $f(z)-a.$ Then if we let $$g(z) = P(z)/(f(z)-a),$$ $g$ is holomorphic in the open unit disk. If we had that $g$ could be extended to a holomorphic function on some open containing $D^1,$then we would be done, since $g(z) = 0$ on the boundary of the unit disk would contradict the maximal modulus principle. However, it is to me far from clear that $g$ extends. EDIT 2 Maybe it is true that we do not need that $g$ extends. Namely, since $g$ has no zeros inside of $D,$ it is true that $g(0) \neq 0.$So $|g(0)| >0.$ however, since $g(z) \to 0$ on the radial limits, this means that we should be able to derive a contradiction to the maximum modulus principle.","This is a problem from Rudin's Real and complex analysis. Let us take a sequence of integers $\{n_k\}$ with $n_1 >1$ and $n_{k+1} > 2kn_{k}.$ We let  $$f(z) = \sum_{k=1}^\infty 5^k z^{n_k}.$$ It is not too hard to prove that this series converges for $|z|<1,$ and that $f$ has no finite radial limits. However, from what I understand, it is also true that $f$ attains every value $z \in \mathbb{C}$ infinitely often. I am stuck in how to prove this. One idea was to use Rouche's theorem, but that didn't get me too far. Any hint, or solution, is welcome. EDIT Here is an idea, which I do not know if it is correct or not. Let $a \in \mathbb{C}$ and suppose that $f(z) = a$ for only finitely many values $z$ in the open unit disk. Then if we consider $f(z) -a,$ there is a polynomial $P(z)$ with the same zeros as $f(z)-a.$ Then if we let $$g(z) = P(z)/(f(z)-a),$$ $g$ is holomorphic in the open unit disk. If we had that $g$ could be extended to a holomorphic function on some open containing $D^1,$then we would be done, since $g(z) = 0$ on the boundary of the unit disk would contradict the maximal modulus principle. However, it is to me far from clear that $g$ extends. EDIT 2 Maybe it is true that we do not need that $g$ extends. Namely, since $g$ has no zeros inside of $D,$ it is true that $g(0) \neq 0.$So $|g(0)| >0.$ however, since $g(z) \to 0$ on the radial limits, this means that we should be able to derive a contradiction to the maximum modulus principle.",,"['complex-analysis', 'analysis']"
50,"For which $u$ does the derivative $f'(u,0)$ of $f(x,y)=|x|+|y|$ exist?",For which  does the derivative  of  exist?,"u f'(u,0) f(x,y)=|x|+|y|","Let $f: \mathbb{R}^2\to \mathbb{R}$ be defined by setting $f(x,y)=|x|+|y|$ (a) For which vectors $u\ne 0$ does $f'(0; u)$ exist? Evaluate it when it exists. (b) Do $D_1f$ and $D_2f$ exist at $0$? (c) Is $f$ differentiable at $0$? (d) Is $f$ continuous at $0$? I have thought a lot about this problem: For (a), be $u\neq 0, u:=(h,k)$, then $\lim_{t\to 0}\frac{f(0+tu)-f(0)}{t}=\lim_{t\to 0}\frac{f(th,tk)}{t}=\lim_{t\to 0}\frac{|th|+|tk|}{t}=(|h|+|k|)\lim_{t\to 0}\frac{|t|}{t}$ and just as $\lim_{t\to 0}\frac{|t|}{t}$ does not exist, necessarily $h=k=0$, with which no directional derivative exists, this immediately tells us that in (b) neither $D_1f$ and $D_2f$ exist and that $f$ is not differentiable in $(0,0)$. Also clearly $f$ is continuous in $(0,0)$. Is this reasoning correct? Could someone help me by giving me suggestions? Thank you.","Let $f: \mathbb{R}^2\to \mathbb{R}$ be defined by setting $f(x,y)=|x|+|y|$ (a) For which vectors $u\ne 0$ does $f'(0; u)$ exist? Evaluate it when it exists. (b) Do $D_1f$ and $D_2f$ exist at $0$? (c) Is $f$ differentiable at $0$? (d) Is $f$ continuous at $0$? I have thought a lot about this problem: For (a), be $u\neq 0, u:=(h,k)$, then $\lim_{t\to 0}\frac{f(0+tu)-f(0)}{t}=\lim_{t\to 0}\frac{f(th,tk)}{t}=\lim_{t\to 0}\frac{|th|+|tk|}{t}=(|h|+|k|)\lim_{t\to 0}\frac{|t|}{t}$ and just as $\lim_{t\to 0}\frac{|t|}{t}$ does not exist, necessarily $h=k=0$, with which no directional derivative exists, this immediately tells us that in (b) neither $D_1f$ and $D_2f$ exist and that $f$ is not differentiable in $(0,0)$. Also clearly $f$ is continuous in $(0,0)$. Is this reasoning correct? Could someone help me by giving me suggestions? Thank you.",,"['calculus', 'real-analysis', 'analysis', 'multivariable-calculus', 'vector-analysis']"
51,Extension of Bounded Linear Operator,Extension of Bounded Linear Operator,,"Let $X$ and $Y$ be normed spaces and let $W \subset X$ be a linear subspace. Suppose that $T_W \in B(W,Y)$ and that the range of $T_W$ is finite dimensional. I know that $T_W$ can be extended to $T \in B(X,Y)$ with $\|T\|=\|T_W\|$. How about the range?","Let $X$ and $Y$ be normed spaces and let $W \subset X$ be a linear subspace. Suppose that $T_W \in B(W,Y)$ and that the range of $T_W$ is finite dimensional. I know that $T_W$ can be extended to $T \in B(X,Y)$ with $\|T\|=\|T_W\|$. How about the range?",,['functional-analysis']
52,Is there a Hausdorff-Young inequality which applies between a length n sequence and its n Discrete Fourier Transform?,Is there a Hausdorff-Young inequality which applies between a length n sequence and its n Discrete Fourier Transform?,,I have been looking around for generalization of the Hausdorff-Young inequality that can be applied between a length $n$ sequence and its $n$-Discrete Fourier Transform (DFT) but no luck. The n-DFT $X$ of a sequence $x$ of length $n$ is defined as: $$  X_k = \sum\limits_{l=0}^{n-1} x_l \exp\left(\frac{-2\pi j k l}{ n}\right) $$ Any help is appreciated. Regard.,I have been looking around for generalization of the Hausdorff-Young inequality that can be applied between a length $n$ sequence and its $n$-Discrete Fourier Transform (DFT) but no luck. The n-DFT $X$ of a sequence $x$ of length $n$ is defined as: $$  X_k = \sum\limits_{l=0}^{n-1} x_l \exp\left(\frac{-2\pi j k l}{ n}\right) $$ Any help is appreciated. Regard.,,"['analysis', 'fourier-analysis', 'fourier-transform', 'fast-fourier-transform']"
53,Volterra Operator Property,Volterra Operator Property,,"It can be shown that if $V(f) = \int_0^x f(t)\,dt$ denotes the Volterra operator on $C([0,1])$, then the closed subspace $A_b = \{f : f|_{[0,b]} =0\}$ satisfies $ V(A_b) \subset A_b$, that is, $A_b$ is invariant under the Volterra operator. Is it true that $A(A_b) \subset A_b$ whenever $A$ is a bounded operator satisfying $AV=VA$? Edit: Here's a potential solution, however, I'm not sure how to bring it full circle. Any comments would be helpful! : ) Inductively we can show the following three properties: $$ (i) \quad V x^m = \frac{x^{m+1}}{m+1} $$ $$ (ii) \quad V^m e = \frac{x^m}{m!} $$ $$ (iii) \quad V^m f(x) = (x^{m-1} \star f)(x), $$ where $e \equiv 1$ on $[0,1]$. Applying these observations to $A$, we see that $Ax^m = (x^m \star A e)'$, the derivative of the convolution. Given $f \in C[0,1]$ and $\epsilon>0$, we can find a polynomial $p(x) = \sum_{k=0}^ma_k x^k$ such that $||f-p|| < \epsilon/ \lambda$, where $\lambda > 0$ is a constant to be determined later. Then, by the triangle inequality, we must have  $$ ||Vf-Af|| \leq ||Vf-Vp|| + ||Af-Ap|| + ||Vp-Ap|| \leq (M_1+M_2) \epsilon + ||Vp-Ap||. $$ The using linearity and a rule for differentiating a convolution, we get $$ ||Vp-Ap|| \leq m \max |a_k| ||Vx^k - Ax^k|| \leq M_3 ||V x^k - (kx^{k-1} \star Ae)||. $$ My fight is with getting this very last estimate arbitrarily small.","It can be shown that if $V(f) = \int_0^x f(t)\,dt$ denotes the Volterra operator on $C([0,1])$, then the closed subspace $A_b = \{f : f|_{[0,b]} =0\}$ satisfies $ V(A_b) \subset A_b$, that is, $A_b$ is invariant under the Volterra operator. Is it true that $A(A_b) \subset A_b$ whenever $A$ is a bounded operator satisfying $AV=VA$? Edit: Here's a potential solution, however, I'm not sure how to bring it full circle. Any comments would be helpful! : ) Inductively we can show the following three properties: $$ (i) \quad V x^m = \frac{x^{m+1}}{m+1} $$ $$ (ii) \quad V^m e = \frac{x^m}{m!} $$ $$ (iii) \quad V^m f(x) = (x^{m-1} \star f)(x), $$ where $e \equiv 1$ on $[0,1]$. Applying these observations to $A$, we see that $Ax^m = (x^m \star A e)'$, the derivative of the convolution. Given $f \in C[0,1]$ and $\epsilon>0$, we can find a polynomial $p(x) = \sum_{k=0}^ma_k x^k$ such that $||f-p|| < \epsilon/ \lambda$, where $\lambda > 0$ is a constant to be determined later. Then, by the triangle inequality, we must have  $$ ||Vf-Af|| \leq ||Vf-Vp|| + ||Af-Ap|| + ||Vp-Ap|| \leq (M_1+M_2) \epsilon + ||Vp-Ap||. $$ The using linearity and a rule for differentiating a convolution, we get $$ ||Vp-Ap|| \leq m \max |a_k| ||Vx^k - Ax^k|| \leq M_3 ||V x^k - (kx^{k-1} \star Ae)||. $$ My fight is with getting this very last estimate arbitrarily small.",,"['functional-analysis', 'analysis', 'operator-theory', 'compact-operators']"
54,Conditions required for Taylor's theorem for multivariate functions,Conditions required for Taylor's theorem for multivariate functions,,"It's easy to find proofs for the following version of the Taylor's theorem for multivariate functions: Let $U$ be an open neighbourhood of $\mathbf a\in\mathbb R^n$ and $f:U\to\mathbb R^m$ a $C^k$ function. Then $$f(\mathbf a+\mathbf h)=\sum^k_{i=0}{1\over i!}D^if(\mathbf a)\mathbf h^i+o(\|\mathbf h\|^k)$$ where $\mathbf h^i=(\mathbf h,\cdots,\mathbf h)\in(\mathbb R^n)^i$. For example, see http://math.stanford.edu/~conrad/diffgeomPage/handouts/taylor.pdf I remember a long time ago I saw on the internet a proof of this theorem only assuming $f$ is $k$-times differentiable, not assuming it to be $C^k$. I didn't really pay much attention to the proof back then. Now I am again thinking about this question, but I failed to find anywhere this version of Taylor's theorem with the weaker condition despite considerable effort. And I could not prove it or disprove it myself. So I wonder does the above hold under this weaker condition? Could anyone point me to any literature on the subject? It seems plausible that it might be true, since we know that the $k=1$ case is true by definition of being differentiable, and moreover the $n=m=1$ case is also true by the one-dimensional Taylor's theorem. And if this is true, does it also hold if $f$ is just $k$-times differentiable at $\mathbf a$, instead of on the whole of $U$?","It's easy to find proofs for the following version of the Taylor's theorem for multivariate functions: Let $U$ be an open neighbourhood of $\mathbf a\in\mathbb R^n$ and $f:U\to\mathbb R^m$ a $C^k$ function. Then $$f(\mathbf a+\mathbf h)=\sum^k_{i=0}{1\over i!}D^if(\mathbf a)\mathbf h^i+o(\|\mathbf h\|^k)$$ where $\mathbf h^i=(\mathbf h,\cdots,\mathbf h)\in(\mathbb R^n)^i$. For example, see http://math.stanford.edu/~conrad/diffgeomPage/handouts/taylor.pdf I remember a long time ago I saw on the internet a proof of this theorem only assuming $f$ is $k$-times differentiable, not assuming it to be $C^k$. I didn't really pay much attention to the proof back then. Now I am again thinking about this question, but I failed to find anywhere this version of Taylor's theorem with the weaker condition despite considerable effort. And I could not prove it or disprove it myself. So I wonder does the above hold under this weaker condition? Could anyone point me to any literature on the subject? It seems plausible that it might be true, since we know that the $k=1$ case is true by definition of being differentiable, and moreover the $n=m=1$ case is also true by the one-dimensional Taylor's theorem. And if this is true, does it also hold if $f$ is just $k$-times differentiable at $\mathbf a$, instead of on the whole of $U$?",,"['real-analysis', 'analysis', 'multivariable-calculus']"
55,Summation about transcendental equation's positive real roots,Summation about transcendental equation's positive real roots,,"If $a_i$ are positive real roots of transcendental equation$$\left( {\cos x} \right)\left( {\cosh x} \right) + 1 = 0.$$And we have $0<a_1<a_2<a_3<\cdots$, prove:   $$\sum_{i=1}^{\infty}{a_{i}^{-6}\left(\frac{\sin a_i-\sinh a_i}{\cos a_i+\cosh a_i}\right)^2}=\frac{1}{80}.$$ First, we have$${\left( {\sinh {a_i}} \right)^2} = {\left( {\cosh {a_i}} \right)^2} - 1 = \frac{1}{{{{\cos }^2}{a_i}}} - 1 = {\tan ^2}{a_i} \Rightarrow \sinh {a_i} = \left| {\tan {a_i}} \right|.$$ If $\sinh {a_i} = \tan {a_i}$, we obtain$${\left( {\frac{{\sin {a_i} - \sinh {a_i}}}{{\cos {a_i} + \cosh {a_i}}}} \right)^2} = {\left( {\frac{{\sin {a_i} - \tan {a_i}}}{{\cos {a_i} - \frac{1}{{\cos {a_i}}}}}} \right)^2} = {\tan ^2}\frac{{{a_i}}}{2}.$$ If $\sinh {a_i} = -\tan {a_i}$, we obtain$${\left( {\frac{{\sin {a_i} - \sinh {a_i}}}{{\cos {a_i} + \cosh {a_i}}}} \right)^2} = {\left( {\frac{{\sin {a_i} + \tan {a_i}}}{{\cos {a_i} - \frac{1}{{\cos {a_i}}}}}} \right)^2} = {\cot ^2}\frac{{{a_i}}}{2}.$$ In fact, both cases will happen.","If $a_i$ are positive real roots of transcendental equation$$\left( {\cos x} \right)\left( {\cosh x} \right) + 1 = 0.$$And we have $0<a_1<a_2<a_3<\cdots$, prove:   $$\sum_{i=1}^{\infty}{a_{i}^{-6}\left(\frac{\sin a_i-\sinh a_i}{\cos a_i+\cosh a_i}\right)^2}=\frac{1}{80}.$$ First, we have$${\left( {\sinh {a_i}} \right)^2} = {\left( {\cosh {a_i}} \right)^2} - 1 = \frac{1}{{{{\cos }^2}{a_i}}} - 1 = {\tan ^2}{a_i} \Rightarrow \sinh {a_i} = \left| {\tan {a_i}} \right|.$$ If $\sinh {a_i} = \tan {a_i}$, we obtain$${\left( {\frac{{\sin {a_i} - \sinh {a_i}}}{{\cos {a_i} + \cosh {a_i}}}} \right)^2} = {\left( {\frac{{\sin {a_i} - \tan {a_i}}}{{\cos {a_i} - \frac{1}{{\cos {a_i}}}}}} \right)^2} = {\tan ^2}\frac{{{a_i}}}{2}.$$ If $\sinh {a_i} = -\tan {a_i}$, we obtain$${\left( {\frac{{\sin {a_i} - \sinh {a_i}}}{{\cos {a_i} + \cosh {a_i}}}} \right)^2} = {\left( {\frac{{\sin {a_i} + \tan {a_i}}}{{\cos {a_i} - \frac{1}{{\cos {a_i}}}}}} \right)^2} = {\cot ^2}\frac{{{a_i}}}{2}.$$ In fact, both cases will happen.",,"['calculus', 'real-analysis', 'sequences-and-series', 'analysis', 'power-series']"
56,What is a 'localization argument'?,What is a 'localization argument'?,,"While reading an article on nonlinear variational problems, the author talked about the possibility to use localization arguments in order to show some coercivity estimates for a given functional. I figured, that what he meant would be something like restricting the functional, which orginally acted on some Sobolev space $W^{1,2}_0(\Omega)$ over a domain $\Omega$, onto one over $W^{1,2}_0(\Omega')$ with $\Omega' \subset\subset \Omega$ being compactly embedded and ""small/local enough"". My question: Can you specify what is (generally) meant by localization arguments and give a prototypical example of their use? Edit (further information) : My problem at hand is an intermediate step in the minimization of the socalled Yang-Mills functional. For this, consider some matrix-valued vector field $A \in W^{1,2}(B^n, \mathbb{R}^n\otimes \mathfrak{g})$ with values in a Lie algebra $\mathfrak{g}$. For simplicity, one may think of $\mathfrak{g} = \mathfrak{su}(k)$, the vector space of hermitian trace-less $k\times k$-matrices. We think of $A$ as $n$ matrix-valued functions $(A_i)_{i=1,\ldots,n}$. We then consider the functional $E_0(A) = \int_{B^n} |\mathrm{d}A|^2 + |\mathrm{d}^\ast A|^2 = \int_{B^n} |\mathrm{rot}A|^2 + |\mathrm{div}A|^2$. Here, $\mathrm{d}, \mathrm{d}^\ast$ are the exterior differential , respectively codifferential . This functional is coercive and weakly lower semicontinuous and hence posesses a minimizer. Now, we consider the functional $E(A) = \int_{B^n}|\mathrm{d}A + [A,A]|^2 + |\mathrm{d}^\ast A|^2$, where the first term of the integrand is explicitly given by $\frac14 \sum_{ij} |\partial_i A_j - \partial_j A_i + [A_i,A_j]|^2$. The map $K:W^{1,2}(B^n, \mathbb{R}^n\otimes \mathfrak{g}) \rightarrow L^2(B^n, \mathbb{R}^n\otimes\mathbb{R}^n\otimes \mathfrak{g}), A\mapsto [A,A]$ is (by Sobolev multiplication and embedding theorems) a compact map for dimensions $n\leq 3$. Now the questions is, if the existence of a minimizer for $E_0$ and the compactness of the pertubation $K$ can be somehow used to show existence of a minimizer for $E$. By a private suggestion of a professor, I was told that this is possible by localization arguments . Unfortunately, I was not able to ask more specific about that suggestion.","While reading an article on nonlinear variational problems, the author talked about the possibility to use localization arguments in order to show some coercivity estimates for a given functional. I figured, that what he meant would be something like restricting the functional, which orginally acted on some Sobolev space $W^{1,2}_0(\Omega)$ over a domain $\Omega$, onto one over $W^{1,2}_0(\Omega')$ with $\Omega' \subset\subset \Omega$ being compactly embedded and ""small/local enough"". My question: Can you specify what is (generally) meant by localization arguments and give a prototypical example of their use? Edit (further information) : My problem at hand is an intermediate step in the minimization of the socalled Yang-Mills functional. For this, consider some matrix-valued vector field $A \in W^{1,2}(B^n, \mathbb{R}^n\otimes \mathfrak{g})$ with values in a Lie algebra $\mathfrak{g}$. For simplicity, one may think of $\mathfrak{g} = \mathfrak{su}(k)$, the vector space of hermitian trace-less $k\times k$-matrices. We think of $A$ as $n$ matrix-valued functions $(A_i)_{i=1,\ldots,n}$. We then consider the functional $E_0(A) = \int_{B^n} |\mathrm{d}A|^2 + |\mathrm{d}^\ast A|^2 = \int_{B^n} |\mathrm{rot}A|^2 + |\mathrm{div}A|^2$. Here, $\mathrm{d}, \mathrm{d}^\ast$ are the exterior differential , respectively codifferential . This functional is coercive and weakly lower semicontinuous and hence posesses a minimizer. Now, we consider the functional $E(A) = \int_{B^n}|\mathrm{d}A + [A,A]|^2 + |\mathrm{d}^\ast A|^2$, where the first term of the integrand is explicitly given by $\frac14 \sum_{ij} |\partial_i A_j - \partial_j A_i + [A_i,A_j]|^2$. The map $K:W^{1,2}(B^n, \mathbb{R}^n\otimes \mathfrak{g}) \rightarrow L^2(B^n, \mathbb{R}^n\otimes\mathbb{R}^n\otimes \mathfrak{g}), A\mapsto [A,A]$ is (by Sobolev multiplication and embedding theorems) a compact map for dimensions $n\leq 3$. Now the questions is, if the existence of a minimizer for $E_0$ and the compactness of the pertubation $K$ can be somehow used to show existence of a minimizer for $E$. By a private suggestion of a professor, I was told that this is possible by localization arguments . Unfortunately, I was not able to ask more specific about that suggestion.",,"['functional-analysis', 'analysis', 'partial-differential-equations', 'calculus-of-variations']"
57,Holmgren’s uniqueness theorem,Holmgren’s uniqueness theorem,,"I read in a paper the proof of the following inequality: \begin{equation}\label{machtyginer-argument} \int_{0}^{1}|w(y,0)|^2 dy  \leq K \int_{0}^{\tau} |w_y(0,t)|^2 dt \end{equation} where $w(x,t)$ be the solution of 1D Schrodinger equation $iw_t-w_{xx} = 0$. They stated that the inequality could be achieve by using the compactness uniqueness argument. ""By contradiction, suppose that the inequality is not true for any $K >0$, there exist a sequence solution $\{w_n\}$ such that : \begin{equation}\label{subsequence-norm} \|w_n(0)\|_{L_2(0,1)} = 1 \quad \forall n \in \mathbb{N} \end{equation} and  \begin{equation}\label{subsequence-norm-2} \int_{0}^{\tau} |(w_n)_y(0,t)|^2 dt \rightarrow 0 \quad \text{as} \quad n \rightarrow +\infty \end{equation} By above argument, we know that $\{w_n(0)\}$ is bounded in $H_0^1(0,1)$, so $\{w_n\}$ is bounded in $L_{\infty}(0,\tau;H_0^1(0,1)) \bigcap W^{1,\infty}(0,\tau;H^{-1}(0,1))$. Then we can subtract a subsequence (that we still denote by $w_n$) such that \begin{equation} w_n \rightarrow w \quad \text{in} \quad L_{\infty}(0,\tau;H_0^1(0,1)) \quad \text{weak}^* \end{equation} \begin{equation} (w_n)_t \rightarrow w_t \quad \text{in} \quad L_{\infty}(0,\tau;H^{-1}(0,1)) \quad \text{weak}^* \end{equation} The function $w \in L_{\infty}(0,\tau;H_0^1(0,1)) \bigcap W^{1,\infty}(0,\tau;H^{-1}(0,1))$ is clearly a solution of Schrodinger equation. From the compactness of embedding  \begin{equation} L_{\infty}(0,\tau;H_0^1(0,1)) \bigcap W^{1,\infty}(0,\tau;H^{-1}(0,1)) \rightarrow C([0,\tau];L_2(0,1)) \end{equation} and using the above arguments, we deduce $\|w(0)\|_{L_2(0,1)} = 1 \forall n \in \mathbb{N}$ and $w_y(0,t) = 0$ on $[0,\tau]$. From Holmgren's Uniqueness Theorem, that implies $w = 0$. This is a contradiction. "" I do not understand how to apply the Holmgren's Uniqueness Theorem ( https://en.wikipedia.org/wiki/Holmgren%27s_uniqueness_theorem ) in this case. Anyone can help me ?","I read in a paper the proof of the following inequality: \begin{equation}\label{machtyginer-argument} \int_{0}^{1}|w(y,0)|^2 dy  \leq K \int_{0}^{\tau} |w_y(0,t)|^2 dt \end{equation} where $w(x,t)$ be the solution of 1D Schrodinger equation $iw_t-w_{xx} = 0$. They stated that the inequality could be achieve by using the compactness uniqueness argument. ""By contradiction, suppose that the inequality is not true for any $K >0$, there exist a sequence solution $\{w_n\}$ such that : \begin{equation}\label{subsequence-norm} \|w_n(0)\|_{L_2(0,1)} = 1 \quad \forall n \in \mathbb{N} \end{equation} and  \begin{equation}\label{subsequence-norm-2} \int_{0}^{\tau} |(w_n)_y(0,t)|^2 dt \rightarrow 0 \quad \text{as} \quad n \rightarrow +\infty \end{equation} By above argument, we know that $\{w_n(0)\}$ is bounded in $H_0^1(0,1)$, so $\{w_n\}$ is bounded in $L_{\infty}(0,\tau;H_0^1(0,1)) \bigcap W^{1,\infty}(0,\tau;H^{-1}(0,1))$. Then we can subtract a subsequence (that we still denote by $w_n$) such that \begin{equation} w_n \rightarrow w \quad \text{in} \quad L_{\infty}(0,\tau;H_0^1(0,1)) \quad \text{weak}^* \end{equation} \begin{equation} (w_n)_t \rightarrow w_t \quad \text{in} \quad L_{\infty}(0,\tau;H^{-1}(0,1)) \quad \text{weak}^* \end{equation} The function $w \in L_{\infty}(0,\tau;H_0^1(0,1)) \bigcap W^{1,\infty}(0,\tau;H^{-1}(0,1))$ is clearly a solution of Schrodinger equation. From the compactness of embedding  \begin{equation} L_{\infty}(0,\tau;H_0^1(0,1)) \bigcap W^{1,\infty}(0,\tau;H^{-1}(0,1)) \rightarrow C([0,\tau];L_2(0,1)) \end{equation} and using the above arguments, we deduce $\|w(0)\|_{L_2(0,1)} = 1 \forall n \in \mathbb{N}$ and $w_y(0,t) = 0$ on $[0,\tau]$. From Holmgren's Uniqueness Theorem, that implies $w = 0$. This is a contradiction. "" I do not understand how to apply the Holmgren's Uniqueness Theorem ( https://en.wikipedia.org/wiki/Holmgren%27s_uniqueness_theorem ) in this case. Anyone can help me ?",,"['real-analysis', 'analysis', 'partial-differential-equations', 'operator-theory']"
58,"Counter example of the dominated convergence theorem, if the dominating function was not integrable.","Counter example of the dominated convergence theorem, if the dominating function was not integrable.",,"Let $(\Omega, \mathcal{A}, \mu)$ be a measure space and $f_1, f_2, f_3,\dots:\Omega \to \overline{\mathbb{R}}$ be measurable functions such that $$f_n \to f\quad f_n\geq f_{n+1}\geq 0$$ Show that if $f_1$ is integrable then $$\lim_{n\to\infty}\int_\Omega f_n \;d\mu = \int_\Omega f \;d\mu$$ Show also that if $f_1$ is not integrable, this equality need not be true. The result follows immediately from the Dominated Convergence Theorem, if we use $f_1$ as the dominating function. The counterexample is a bit harder. Consider $\Omega = \overline{\mathbb{R}}$ such that $f:\overline{\mathbb{R}}\to\overline{\mathbb{R}}$ and we have a measure space equipped with the Borel sigma-algebra and Lebesgue measure. Let $$f_n(x) = \frac{x}{n}$$ Then $$f_n(x)\to f(x)=0\quad as \quad n\to\infty$$ But $$\int_{\overline{\mathbb{R}}}f_1 \;dm = \int_{\overline{\mathbb{R}}}x \;dm = \infty\neq 0 = \int_{\overline{\mathbb{R}}}f \;dm$$ Does this solution make sense to you?","Let $(\Omega, \mathcal{A}, \mu)$ be a measure space and $f_1, f_2, f_3,\dots:\Omega \to \overline{\mathbb{R}}$ be measurable functions such that $$f_n \to f\quad f_n\geq f_{n+1}\geq 0$$ Show that if $f_1$ is integrable then $$\lim_{n\to\infty}\int_\Omega f_n \;d\mu = \int_\Omega f \;d\mu$$ Show also that if $f_1$ is not integrable, this equality need not be true. The result follows immediately from the Dominated Convergence Theorem, if we use $f_1$ as the dominating function. The counterexample is a bit harder. Consider $\Omega = \overline{\mathbb{R}}$ such that $f:\overline{\mathbb{R}}\to\overline{\mathbb{R}}$ and we have a measure space equipped with the Borel sigma-algebra and Lebesgue measure. Let $$f_n(x) = \frac{x}{n}$$ Then $$f_n(x)\to f(x)=0\quad as \quad n\to\infty$$ But $$\int_{\overline{\mathbb{R}}}f_1 \;dm = \int_{\overline{\mathbb{R}}}x \;dm = \infty\neq 0 = \int_{\overline{\mathbb{R}}}f \;dm$$ Does this solution make sense to you?",,"['analysis', 'measure-theory', 'proof-verification', 'improper-integrals', 'lebesgue-integral']"
59,Estimate a series,Estimate a series,,"I'm stuck on a step in a proof. Does there exist a constant $C$ such that $\sum_{l=0}^{\infty} [1+(t+lb)^2]^{-\alpha}\leq Ct^{-2\alpha}$ uniformly in $t>0$, where $ b>0, \alpha >1$ are fixed.","I'm stuck on a step in a proof. Does there exist a constant $C$ such that $\sum_{l=0}^{\infty} [1+(t+lb)^2]^{-\alpha}\leq Ct^{-2\alpha}$ uniformly in $t>0$, where $ b>0, \alpha >1$ are fixed.",,"['real-analysis', 'sequences-and-series', 'analysis']"
60,"Markov Chain, Doubly stochastic and Supremum","Markov Chain, Doubly stochastic and Supremum",,"Suppose a Discrete-Time Markov Chain has a state space $S = \{0,1,2,\ldots\}$ and a  doubly stochastic transition matrix $P$ (i.e $\sum_{i \in S}P_{ij} = 1, \forall i \in S$) Let $\underline{a} = (a_0,a_1,\ldots)$ be a probability distribution and let $\underline{b} = \underline{a} P$. Prove that (1) $sup\{a_i : i = 0,1,\ldots\} \le sup\{b_i: i = 0,1,\ldots\}$ (2) Define $A:=\{i \in S: b_i =  sup\{b_i: i = 0,1,\ldots\}\}$ and $B := \{i : a_i < sup\{a_i : i = 0,1,\ldots\}\}$. Show that if the inequality in part (1) takes equality, then $A$ is not empty and $P_{ij} = 0$ for any $i \in B$ and $j \in A$ For (1) I know that $sup_{i \in S}(b_i) = sup_{i \in S} (\sum_{j \in S}a_j) P_{ji} \le \sum_{j \in S} sup_{j \in S}(a_j P_{ji}) = sup_{j \in S}(a_j)\sum_{j \in S}  P_{ji} = sup_{j \in S}(a_j)$ But how to show part (2), I think I should start with contradiction, thank you for your help!","Suppose a Discrete-Time Markov Chain has a state space $S = \{0,1,2,\ldots\}$ and a  doubly stochastic transition matrix $P$ (i.e $\sum_{i \in S}P_{ij} = 1, \forall i \in S$) Let $\underline{a} = (a_0,a_1,\ldots)$ be a probability distribution and let $\underline{b} = \underline{a} P$. Prove that (1) $sup\{a_i : i = 0,1,\ldots\} \le sup\{b_i: i = 0,1,\ldots\}$ (2) Define $A:=\{i \in S: b_i =  sup\{b_i: i = 0,1,\ldots\}\}$ and $B := \{i : a_i < sup\{a_i : i = 0,1,\ldots\}\}$. Show that if the inequality in part (1) takes equality, then $A$ is not empty and $P_{ij} = 0$ for any $i \in B$ and $j \in A$ For (1) I know that $sup_{i \in S}(b_i) = sup_{i \in S} (\sum_{j \in S}a_j) P_{ji} \le \sum_{j \in S} sup_{j \in S}(a_j P_{ji}) = sup_{j \in S}(a_j)\sum_{j \in S}  P_{ji} = sup_{j \in S}(a_j)$ But how to show part (2), I think I should start with contradiction, thank you for your help!",,"['probability', 'matrices']"
61,$\limsup\limits_{x\to\infty}(f(x+h)-f(x)) = 0$ and $\liminf\limits_{x\to\infty}(f(x+h)-f(x)e^{-h})\ge0 \implies\lim_{x\to\infty}(f(x+h)-f(x))= 0$?,and ?,\limsup\limits_{x\to\infty}(f(x+h)-f(x)) = 0 \liminf\limits_{x\to\infty}(f(x+h)-f(x)e^{-h})\ge0 \implies\lim_{x\to\infty}(f(x+h)-f(x))= 0,"I have an absolutely continuous function $f:[0,\infty)\to[0,\infty)$ that satisfies $$\limsup_{x\to\infty}(f(x+h)-f(x)) = 0$$ and $$\liminf_{x\to\infty}(f(x+h)-f(x)e^{-h})\ge0$$ for all $h>0$. I need to check if it is true or false that $$\lim_{x\to\infty}(f(x+h)-f(x)) = 0$$ for all $h>0$. zhw. proved that the conjecture is false with only$$\limsup_{x\to\infty}(f(x+h)-f(x)) = 0$$ $\forall h>0$, here . I am wondering that if it is true with the additional condition $$\liminf_{x\to\infty}(f(x+h)-f(x)e^{-h}) \ge0$$ $\forall h>0$. Any hint will be welcome.","I have an absolutely continuous function $f:[0,\infty)\to[0,\infty)$ that satisfies $$\limsup_{x\to\infty}(f(x+h)-f(x)) = 0$$ and $$\liminf_{x\to\infty}(f(x+h)-f(x)e^{-h})\ge0$$ for all $h>0$. I need to check if it is true or false that $$\lim_{x\to\infty}(f(x+h)-f(x)) = 0$$ for all $h>0$. zhw. proved that the conjecture is false with only$$\limsup_{x\to\infty}(f(x+h)-f(x)) = 0$$ $\forall h>0$, here . I am wondering that if it is true with the additional condition $$\liminf_{x\to\infty}(f(x+h)-f(x)e^{-h}) \ge0$$ $\forall h>0$. Any hint will be welcome.",,"['calculus', 'real-analysis', 'analysis']"
62,Flipping the order of an infinite series and assigning it a value?,Flipping the order of an infinite series and assigning it a value?,,"I was recently wondering if there was some method to ""flip"" the series and still assign it a value for divergent series. To illustrate what I mean: $$ S(x,n) = a_0 + a_1 x + a_2 x^2 + \dots + a_n x^n $$ We would like to define the flipped sum series: $$ F(S(x,n)) = a_n  + a_{n-1} x^{1} + \dots + a_0 x^{n} $$ Hence, to do this we begin by: assigning $x \to x^{-1} $ and take $x^n$ common: $$  S(x^{-1},n) = (a_n  + a_{n-1} x^{1} + \dots + a_0 x^{n}) x^{-n}  $$ Multiplying both sides with $x^n$: $$ S(x^{-1},n) x^n =  (a_n  + a_{n-1} x^{1} + \dots + a_0 x^{n})$$ Taking limit $n \to \infty$: $$ \lim_{n \to \infty}S(x^{-1},n) x^n =  \lim_{n \to \infty}(a_n  + a_{n-1} x^{1} + \dots + a_0 x^{n}) $$ Taking the second limit $x \to 1$: $$\lim_{x \to 1 } \lim_{n \to \infty}S(x^{-1},n) x^n = \lim_{n \to \infty} (a_n  + a_{n-1}  + \dots + a_0 )$$ Questions Is the above proof correct? Can this somehow be extended to zeta regularization?","I was recently wondering if there was some method to ""flip"" the series and still assign it a value for divergent series. To illustrate what I mean: $$ S(x,n) = a_0 + a_1 x + a_2 x^2 + \dots + a_n x^n $$ We would like to define the flipped sum series: $$ F(S(x,n)) = a_n  + a_{n-1} x^{1} + \dots + a_0 x^{n} $$ Hence, to do this we begin by: assigning $x \to x^{-1} $ and take $x^n$ common: $$  S(x^{-1},n) = (a_n  + a_{n-1} x^{1} + \dots + a_0 x^{n}) x^{-n}  $$ Multiplying both sides with $x^n$: $$ S(x^{-1},n) x^n =  (a_n  + a_{n-1} x^{1} + \dots + a_0 x^{n})$$ Taking limit $n \to \infty$: $$ \lim_{n \to \infty}S(x^{-1},n) x^n =  \lim_{n \to \infty}(a_n  + a_{n-1} x^{1} + \dots + a_0 x^{n}) $$ Taking the second limit $x \to 1$: $$\lim_{x \to 1 } \lim_{n \to \infty}S(x^{-1},n) x^n = \lim_{n \to \infty} (a_n  + a_{n-1}  + \dots + a_0 )$$ Questions Is the above proof correct? Can this somehow be extended to zeta regularization?",,"['sequences-and-series', 'analysis', 'summation', 'zeta-functions']"
63,Convexity vs Quasi-convexity,Convexity vs Quasi-convexity,,"It is well known that in optimization the concept of quasi-convex function is the following: $f: \Omega\subset \mathbb{R}^n \rightarrow \mathbb{R}$ is a quasi convex function if for every $\alpha\in\mathbb{R}$, we have that $level_{\alpha}(f):=\{x\in\Omega; f(x)\leq \alpha\}$ is a convex set. Every convex function is quasi convex in this sense. Although the converse is not true, as one can see from, for example, the level sets of the Heavyside function. My question: Which additional conditions make a quasi-convex function $f: \Omega\rightarrow \mathbb{R}$ convex? One should ask at least that f is locally Lipschitz, since every convex function satisfies this hypothesis. Thanks for the attention.","It is well known that in optimization the concept of quasi-convex function is the following: $f: \Omega\subset \mathbb{R}^n \rightarrow \mathbb{R}$ is a quasi convex function if for every $\alpha\in\mathbb{R}$, we have that $level_{\alpha}(f):=\{x\in\Omega; f(x)\leq \alpha\}$ is a convex set. Every convex function is quasi convex in this sense. Although the converse is not true, as one can see from, for example, the level sets of the Heavyside function. My question: Which additional conditions make a quasi-convex function $f: \Omega\rightarrow \mathbb{R}$ convex? One should ask at least that f is locally Lipschitz, since every convex function satisfies this hypothesis. Thanks for the attention.",,"['analysis', 'optimization', 'convex-analysis', 'variational-analysis']"
64,Inequality involving $\limsup$ and $\liminf$: $ \liminf(a_{n+1}/a_n) \le \liminf a_n^{1/n} \le \limsup a_n^{1/n} \le \limsup(a_{n+1}/a_n)$,Inequality involving  and :,\limsup \liminf  \liminf(a_{n+1}/a_n) \le \liminf a_n^{1/n} \le \limsup a_n^{1/n} \le \limsup(a_{n+1}/a_n),"This may have been asked before, however I was unable to find any duplicate. This comes from pg. 52 of ""Mathematical Analysis: An Introduction"" by Browder. Problem 14: If $(a_n)$ is a sequence in $\mathbb R$ and $a_n > 0$ for every $n$ . Then show: $$ \liminf\frac{a_{n+1}}{a_n} \le \liminf a_n^{1/n} \le \limsup a_n^{1/n} \le \limsup\frac{a_{n+1}}{a_n}$$ The middle inequality is clear. However I am having a hard time showing the ones on the left and right. (It seems like the approach should be similar for each). This is homework, so it'd be great if someone could give me a hint to get started on at least one of the inequalities. Thanks.","This may have been asked before, however I was unable to find any duplicate. This comes from pg. 52 of ""Mathematical Analysis: An Introduction"" by Browder. Problem 14: If is a sequence in and for every . Then show: The middle inequality is clear. However I am having a hard time showing the ones on the left and right. (It seems like the approach should be similar for each). This is homework, so it'd be great if someone could give me a hint to get started on at least one of the inequalities. Thanks.","(a_n) \mathbb R a_n > 0 n 
\liminf\frac{a_{n+1}}{a_n} \le \liminf a_n^{1/n} \le \limsup a_n^{1/n} \le \limsup\frac{a_{n+1}}{a_n}","['real-analysis', 'analysis', 'inequality', 'limsup-and-liminf']"
65,"$(\lambda I-A)^{-1}C\subset C,\forall \lambda\in\Bbb K:\Re(\lambda)>\|A\|\implies e^{tA}C\subset C,\forall t\in\Bbb R$",,"(\lambda I-A)^{-1}C\subset C,\forall \lambda\in\Bbb K:\Re(\lambda)>\|A\|\implies e^{tA}C\subset C,\forall t\in\Bbb R","This is the exercise 25 in the book Analysis II of Amann and Escher, page 148. Let $C$ a closed and convex subset of $E$ and $A\in\mathcal L(E)$ . Show that these statements are equivalent: $e^{tA}C\subset C$ for all $t\in\Bbb R$ . $(\lambda I-A)^{-1}C\subset C$ for all $\lambda\in\Bbb K$ such that $\Re(\lambda)>\|A\|$ . (Note: here $E$ is a Banach space and $\Bbb K=\Bbb R\text{ or }\Bbb C$ .) I know that $$e^{tA}=\lim_{n\to\infty}\left(I-\frac{t}nA\right)^{-n},\quad (\lambda I-A)^{-1}=\int_0^\infty e^{-s(\lambda I-A)}\mathrm ds$$ What I had tried $(ii)\implies (i)$ If $n>\|A\|$ then the operator $n I-A$ is invertible. Now observe that $$n I-A=n\left(I-\frac1{n}A\right)\implies (n I-A)^{-1}=\frac1{n}\left(I-\frac1{n}A\right)^{-1}\tag1$$ Now if I use recursively the last result I get $$n^{-n}\left(I-\frac1{n}A\right)^{-n}C\subset C$$ for all $n>\|A\|$ . But due to the factor $n^{-n}$ I cant show that $e^AC\subset C$ . Some help will be appreciated, thank you. W.I.P.: from the comments I assume there is some typo or lack of some important information in the exercise so I will try to add some restrictions on it. By example: it can be seen that assuming $E=\Bbb R$ and $A=I$ and $\lambda,t\in\Bbb R$ (in addition to $\Re(\lambda)>\|A\|$ ) and some perfect interval the statement holds. So maybe assuming these conditions for arbitrary real Banach space $E$ the exercise make sense. That is: $\lambda,t\in\Bbb R$ and $C\subset E$ is a closed and convex subset with more than one point in it. Thus, trying to show $(ii)\implies (i)$ we want to find some ""valid"" expression in terms of linear functions $(\lambda I-A)^{-1}$ to prove that $(I-\frac{t}nA)^{-1}C\subset C$ , what will let me show that $e^{tA}C\subset C$ . Because $C$ is convex then $$\frac{\sum_{k=0}^n r_k(\lambda_k I-A)^{-k}}{\sum_{k=0}^n r_k}C\subset C$$ for arbitrary $r_k>0$ and $\lambda_k>\|A\|$ . Then taking $r_k=\frac1{k!}$ and $\lambda_k=m$ , and by the closedness of $C$ we find that $$e^{-1}\lim_{n\to\infty}\sum_{k=0}^n \frac{(m I-A)^{-k}}{k!}C=e^{(m I-A)^{-1}-I}C\subset C$$ However from the last expression it is not so clear how I can prove that $e^{tA}C\subset C$ .","This is the exercise 25 in the book Analysis II of Amann and Escher, page 148. Let a closed and convex subset of and . Show that these statements are equivalent: for all . for all such that . (Note: here is a Banach space and .) I know that What I had tried If then the operator is invertible. Now observe that Now if I use recursively the last result I get for all . But due to the factor I cant show that . Some help will be appreciated, thank you. W.I.P.: from the comments I assume there is some typo or lack of some important information in the exercise so I will try to add some restrictions on it. By example: it can be seen that assuming and and (in addition to ) and some perfect interval the statement holds. So maybe assuming these conditions for arbitrary real Banach space the exercise make sense. That is: and is a closed and convex subset with more than one point in it. Thus, trying to show we want to find some ""valid"" expression in terms of linear functions to prove that , what will let me show that . Because is convex then for arbitrary and . Then taking and , and by the closedness of we find that However from the last expression it is not so clear how I can prove that .","C E A\in\mathcal L(E) e^{tA}C\subset C t\in\Bbb R (\lambda I-A)^{-1}C\subset C \lambda\in\Bbb K \Re(\lambda)>\|A\| E \Bbb K=\Bbb R\text{ or }\Bbb C e^{tA}=\lim_{n\to\infty}\left(I-\frac{t}nA\right)^{-n},\quad (\lambda I-A)^{-1}=\int_0^\infty e^{-s(\lambda I-A)}\mathrm ds (ii)\implies (i) n>\|A\| n I-A n I-A=n\left(I-\frac1{n}A\right)\implies (n I-A)^{-1}=\frac1{n}\left(I-\frac1{n}A\right)^{-1}\tag1 n^{-n}\left(I-\frac1{n}A\right)^{-n}C\subset C n>\|A\| n^{-n} e^AC\subset C E=\Bbb R A=I \lambda,t\in\Bbb R \Re(\lambda)>\|A\| E \lambda,t\in\Bbb R C\subset E (ii)\implies (i) (\lambda I-A)^{-1} (I-\frac{t}nA)^{-1}C\subset C e^{tA}C\subset C C \frac{\sum_{k=0}^n r_k(\lambda_k I-A)^{-k}}{\sum_{k=0}^n r_k}C\subset C r_k>0 \lambda_k>\|A\| r_k=\frac1{k!} \lambda_k=m C e^{-1}\lim_{n\to\infty}\sum_{k=0}^n \frac{(m I-A)^{-k}}{k!}C=e^{(m I-A)^{-1}-I}C\subset C e^{tA}C\subset C","['linear-algebra', 'functional-analysis', 'analysis', 'operator-theory']"
66,"Prob. 9, Chap. 6, in Baby Rudin: Integration by parts for improper integrals","Prob. 9, Chap. 6, in Baby Rudin: Integration by parts for improper integrals",,"Here is Prob. 9, Chap. 6, in the book Principles of Mathematical Analysis by Walter Rudin, 3rd edition: Show that integration by parts can sometimes be applied to the ""improper"" integrals defined in Exercises 7 and 8. (State appropriate hypotheses, formulate a theorem, and prove it.) For instance show that    $$ \int_0^\infty \frac{\cos x}{1+x} \ \mathrm{d} x =  \int_0^\infty \frac{\sin x}{(1+x)^2} \ \mathrm{d} x. $$    Show that one of these integrals converges absolutely , but that the other does not. Here are the links to my Math SE posts on Probs. 7 and 8, Chap. 6 in Baby Rudin, 3rd edition: Prob. 7 (a), Chap. 6, in Baby Rudin: If $f$ is integrable on $[c, 1]$ for every $c>0$, then $\int_0^1 f(x) \ \mathrm{d}x = $ . . . Prob. 7 (b), Chap. 6, in Baby Rudin: Example of a function such that $\lim_{c \to 0+} \int_c^1 f(x) \ \mathrm{d}x$ exists but . . . Prob. 8, Chap. 6, in Baby Rudin: The Integral Test for Convergence of Series And, here is Theorem 6.22 (integration by parts) in Baby Rudin: Suppose $F$ and $G$ are differentiable functions on $[a, b]$, $F^\prime = f \in \mathscr{R}$, and $G^\prime = g \in \mathscr{R}$. Then    $$ \int_a^b F(x) g(x) \ \mathrm{d} x = F(b)G(b) - F(a) G(a) - \int_a^b f(x) G(x) \ \mathrm{d} x. $$ My Attempt: Here I will only be formulating the analog of the result in Prob. 7 (a). Suppose $F$ and $G$ are  real differentiable functions on $(0, 1]$ such that $F^\prime = f \in \mathscr{R}$ and $G^\prime = g \in \mathscr{R}$ on $[c, 1]$ for every $c > 0$. Suppose that $\lim_{x \to 0+} \left[ F(x) G(x) \right]$ exists and is finite. If $f \in \mathscr{R}$ and $g \in \mathscr{R}$ on $[0, 1]$, then we have    $$ \int_0^1 F(x) g(x) \ \mathrm{d} x =  F(1) G(1) - \lim_{c \to 0+} F(c) G(c) \ - \  \int_0^1 f(x) G(x) \ \mathrm{d} x. $$ Is this result correct? If so, is this the result required by Rudin? Now here is my proof: By Theorem 6.22 in Baby Rudin, we see that for any $c$ such that $0 < c < 1$, we have    $$ \int_c^1 F(x) g(x) \ \mathrm{d} x = F(1)G(1) - F(c) G(c) - \int_c^1 f(x) G(x) \ \mathrm{d} x.  $$   Now applying the conclusion of Prob. 7 (a) to this identity we obtain    $$ \begin{align} \int_0^1 F(x) g(x) \ \mathrm{d} x &= \lim_{c \to 0+} \int_c^1 F(x) g(x) \ \mathrm{d} x \\ &= \lim_{c \to 0+} \left( F(1)G(1) - F(c) G(c) - \int_c^1 f(x) G(x) \ \mathrm{d} x \right) \\ &= F(1) G(1) - \lim_{c \to 0+} F(c) G(c) - \lim_{c \to 0+} \int_c^1 f(x) G(x) \ \mathrm{d} x \\ &= F(1) G(1) - \lim_{c \to 0+} F(c) G(c) \  - \  \int_0^1 f(x) G(x) \ \mathrm{d} x,  \end{align} $$   as required. Is my proof correct and rigorous enough for Rudin?","Here is Prob. 9, Chap. 6, in the book Principles of Mathematical Analysis by Walter Rudin, 3rd edition: Show that integration by parts can sometimes be applied to the ""improper"" integrals defined in Exercises 7 and 8. (State appropriate hypotheses, formulate a theorem, and prove it.) For instance show that    $$ \int_0^\infty \frac{\cos x}{1+x} \ \mathrm{d} x =  \int_0^\infty \frac{\sin x}{(1+x)^2} \ \mathrm{d} x. $$    Show that one of these integrals converges absolutely , but that the other does not. Here are the links to my Math SE posts on Probs. 7 and 8, Chap. 6 in Baby Rudin, 3rd edition: Prob. 7 (a), Chap. 6, in Baby Rudin: If $f$ is integrable on $[c, 1]$ for every $c>0$, then $\int_0^1 f(x) \ \mathrm{d}x = $ . . . Prob. 7 (b), Chap. 6, in Baby Rudin: Example of a function such that $\lim_{c \to 0+} \int_c^1 f(x) \ \mathrm{d}x$ exists but . . . Prob. 8, Chap. 6, in Baby Rudin: The Integral Test for Convergence of Series And, here is Theorem 6.22 (integration by parts) in Baby Rudin: Suppose $F$ and $G$ are differentiable functions on $[a, b]$, $F^\prime = f \in \mathscr{R}$, and $G^\prime = g \in \mathscr{R}$. Then    $$ \int_a^b F(x) g(x) \ \mathrm{d} x = F(b)G(b) - F(a) G(a) - \int_a^b f(x) G(x) \ \mathrm{d} x. $$ My Attempt: Here I will only be formulating the analog of the result in Prob. 7 (a). Suppose $F$ and $G$ are  real differentiable functions on $(0, 1]$ such that $F^\prime = f \in \mathscr{R}$ and $G^\prime = g \in \mathscr{R}$ on $[c, 1]$ for every $c > 0$. Suppose that $\lim_{x \to 0+} \left[ F(x) G(x) \right]$ exists and is finite. If $f \in \mathscr{R}$ and $g \in \mathscr{R}$ on $[0, 1]$, then we have    $$ \int_0^1 F(x) g(x) \ \mathrm{d} x =  F(1) G(1) - \lim_{c \to 0+} F(c) G(c) \ - \  \int_0^1 f(x) G(x) \ \mathrm{d} x. $$ Is this result correct? If so, is this the result required by Rudin? Now here is my proof: By Theorem 6.22 in Baby Rudin, we see that for any $c$ such that $0 < c < 1$, we have    $$ \int_c^1 F(x) g(x) \ \mathrm{d} x = F(1)G(1) - F(c) G(c) - \int_c^1 f(x) G(x) \ \mathrm{d} x.  $$   Now applying the conclusion of Prob. 7 (a) to this identity we obtain    $$ \begin{align} \int_0^1 F(x) g(x) \ \mathrm{d} x &= \lim_{c \to 0+} \int_c^1 F(x) g(x) \ \mathrm{d} x \\ &= \lim_{c \to 0+} \left( F(1)G(1) - F(c) G(c) - \int_c^1 f(x) G(x) \ \mathrm{d} x \right) \\ &= F(1) G(1) - \lim_{c \to 0+} F(c) G(c) - \lim_{c \to 0+} \int_c^1 f(x) G(x) \ \mathrm{d} x \\ &= F(1) G(1) - \lim_{c \to 0+} F(c) G(c) \  - \  \int_0^1 f(x) G(x) \ \mathrm{d} x,  \end{align} $$   as required. Is my proof correct and rigorous enough for Rudin?",,"['calculus', 'real-analysis', 'integration', 'analysis', 'definite-integrals']"
67,"Prob. 5, Chap. 6, in Baby Rudin: If $f^2$ is integrable, does it follow that $f$ is integrable too? What about $f^3$?","Prob. 5, Chap. 6, in Baby Rudin: If  is integrable, does it follow that  is integrable too? What about ?",f^2 f f^3,"Here is Prob. 5, Chap. 6, in the book Principles of Mathematical Analysis by Walter Rudin, 3rd edition: Suppose $f$ is a bounded real function on $[a, b]$, and $f^2 \in \mathscr{R}$ on $[a, b]$. Does it follow that $f \in \mathscr{R}$? Does the answer change if we assume that $f^3 \in \mathscr{R}$? My Attempt: The Riemann-integrability of $f^2$ need not imply the integrability of $f$. For example, let $f$ be defined on $\mathbb{R}$ by    $$ f(x) \colon=  \begin{cases} 1 \qquad & \mbox{ if } x \in \mathbb{Q}, \\ -1 \qquad & \mbox{ if } x \not\in \mathbb{Q}. \end{cases}  $$   Then $f^2(x) = 1$ for all $x \in \mathbb{R}$, and so    $$ \int_{-r}^{+r} f(x) \ \mathrm{d} x = 2r $$   for any real number $r > 0$. However, this function $f$ is not integrable on $[a, b]$, where $a$ and $b$ are real numbers such that $a < b$. The details are as follows: If $P$ be any partition of $[a, b]$, then we have    $$ L(P, f) = -(b-a), \ \mbox{ and } \ U(P, f) = b-a, $$   and so    $$ U(P, f) - L(P, f) = 2(b-a) > \varepsilon $$   for any real number $\varepsilon$ such that $0 < \varepsilon < 2(b-a)$, and thus the condition of Theorem 6.6 in Baby Rudin is violated. Is what I've stated above correct? If so, then is my counter-example the right one? And, have I managed to present this argument correctly as well? On the other hand, if $f^3$ is integrable on $[a, b]$, then so is $f$. Am I right? The proof is as follows: As $f$ is a bounded real function on $[a, b]$, so the supremum and infimum of the range of $f$ exist in $\mathbb{R}$. Let us put    $$ M \colon= \sup \left\{ \ f(x) \ \colon \ a \leq x \leq b \ \right\}, \ \mbox{ and } \ m \colon= \inf \left\{ \ f(x) \ \colon \ a \leq x \leq b \ \right\}. $$ Since the function $t \mapsto t^3$ is a continuous one-to-one mapping of $\mathbb{R}$ onto $\mathbb{R}$, therefore it is also a continuous one-to-one mapping of any finite interval $[c, d ]$  onto the interval $\left[c^3, d^3 \right]$. Moreover, the interval $[ c, d ]$  is compact.  So by Theorem 4.17 in Baby Rudin, the function $t \mapsto t^{1/3}$ is a continuous (one-to-one) mapping of $[c^3, d^3 ]$ onto  $[c , d ]$. Now as $f^3 \in \mathscr{R}$ on $[a, b]$, $m^3 \leq f^3 \leq M^3$ on $[a, b]$, and the map $\phi$ defined on $\left[m^3, M^3 \right]$ by    $\phi(t) = t^{1/3}$ is continuous, so by Theorem 6.11 in Baby Rudin the function $h = \phi \circ f^3 $ is also integrable on $[a, b]$. Is my reasoning in this proof correct? If so, then have I presented this proof correctly and lucidly enough too?","Here is Prob. 5, Chap. 6, in the book Principles of Mathematical Analysis by Walter Rudin, 3rd edition: Suppose $f$ is a bounded real function on $[a, b]$, and $f^2 \in \mathscr{R}$ on $[a, b]$. Does it follow that $f \in \mathscr{R}$? Does the answer change if we assume that $f^3 \in \mathscr{R}$? My Attempt: The Riemann-integrability of $f^2$ need not imply the integrability of $f$. For example, let $f$ be defined on $\mathbb{R}$ by    $$ f(x) \colon=  \begin{cases} 1 \qquad & \mbox{ if } x \in \mathbb{Q}, \\ -1 \qquad & \mbox{ if } x \not\in \mathbb{Q}. \end{cases}  $$   Then $f^2(x) = 1$ for all $x \in \mathbb{R}$, and so    $$ \int_{-r}^{+r} f(x) \ \mathrm{d} x = 2r $$   for any real number $r > 0$. However, this function $f$ is not integrable on $[a, b]$, where $a$ and $b$ are real numbers such that $a < b$. The details are as follows: If $P$ be any partition of $[a, b]$, then we have    $$ L(P, f) = -(b-a), \ \mbox{ and } \ U(P, f) = b-a, $$   and so    $$ U(P, f) - L(P, f) = 2(b-a) > \varepsilon $$   for any real number $\varepsilon$ such that $0 < \varepsilon < 2(b-a)$, and thus the condition of Theorem 6.6 in Baby Rudin is violated. Is what I've stated above correct? If so, then is my counter-example the right one? And, have I managed to present this argument correctly as well? On the other hand, if $f^3$ is integrable on $[a, b]$, then so is $f$. Am I right? The proof is as follows: As $f$ is a bounded real function on $[a, b]$, so the supremum and infimum of the range of $f$ exist in $\mathbb{R}$. Let us put    $$ M \colon= \sup \left\{ \ f(x) \ \colon \ a \leq x \leq b \ \right\}, \ \mbox{ and } \ m \colon= \inf \left\{ \ f(x) \ \colon \ a \leq x \leq b \ \right\}. $$ Since the function $t \mapsto t^3$ is a continuous one-to-one mapping of $\mathbb{R}$ onto $\mathbb{R}$, therefore it is also a continuous one-to-one mapping of any finite interval $[c, d ]$  onto the interval $\left[c^3, d^3 \right]$. Moreover, the interval $[ c, d ]$  is compact.  So by Theorem 4.17 in Baby Rudin, the function $t \mapsto t^{1/3}$ is a continuous (one-to-one) mapping of $[c^3, d^3 ]$ onto  $[c , d ]$. Now as $f^3 \in \mathscr{R}$ on $[a, b]$, $m^3 \leq f^3 \leq M^3$ on $[a, b]$, and the map $\phi$ defined on $\left[m^3, M^3 \right]$ by    $\phi(t) = t^{1/3}$ is continuous, so by Theorem 6.11 in Baby Rudin the function $h = \phi \circ f^3 $ is also integrable on $[a, b]$. Is my reasoning in this proof correct? If so, then have I presented this proof correctly and lucidly enough too?",,"['calculus', 'real-analysis', 'integration', 'analysis', 'definite-integrals']"
68,Sequence of continuous functions over a compact set that converges pointwise and monotonically also converges uniformly,Sequence of continuous functions over a compact set that converges pointwise and monotonically also converges uniformly,,"I have the following problem: Let $\{f_n\}_{n = 1}^\infty$ be a sequence of functions such that, for all $n \in \mathbb{N}$,  $f_n:[0,1]  \to [0,1]$ is continuous, and for all $x\in [0,1]$, we have that $f_n(x) \to 0$ and $f_{n+1}(x) \le f_n(x)$. Show that $f_n \to 0$ uniformly. My idea so far is to get for every $n \in \mathbb{N}$ a $c_n \in [0,1]$ such that $f_n(c_n) = \sup_{x \in [0,1]} f_n(c_n)$ and to show that $f_n(c_n) \to 0$. I know that passing to a subsequence I can assume that the $c_n$ converge to some $c \in [0,1]$, and I know that sequence $\{f_n(c_n)\}$ is monotonically decreasing, so it has a limit. My instinct tells me to show that $\lim_{n \to \infty}f_n(c_n) = \lim_{n \to \infty}f_n(c)$, but I haven't been able to do this last step. I feel like I'm close to a solution, but am I really? Does anyone have any idea how to close this proof or of how to do a different proof?","I have the following problem: Let $\{f_n\}_{n = 1}^\infty$ be a sequence of functions such that, for all $n \in \mathbb{N}$,  $f_n:[0,1]  \to [0,1]$ is continuous, and for all $x\in [0,1]$, we have that $f_n(x) \to 0$ and $f_{n+1}(x) \le f_n(x)$. Show that $f_n \to 0$ uniformly. My idea so far is to get for every $n \in \mathbb{N}$ a $c_n \in [0,1]$ such that $f_n(c_n) = \sup_{x \in [0,1]} f_n(c_n)$ and to show that $f_n(c_n) \to 0$. I know that passing to a subsequence I can assume that the $c_n$ converge to some $c \in [0,1]$, and I know that sequence $\{f_n(c_n)\}$ is monotonically decreasing, so it has a limit. My instinct tells me to show that $\lim_{n \to \infty}f_n(c_n) = \lim_{n \to \infty}f_n(c)$, but I haven't been able to do this last step. I feel like I'm close to a solution, but am I really? Does anyone have any idea how to close this proof or of how to do a different proof?",,"['analysis', 'uniform-convergence', 'pointwise-convergence']"
69,Physical System Producing Prime Numbers?,Physical System Producing Prime Numbers?,,"Have you seen this wonderful movie ""Contact"" starring Jodie Foster ? She is basically looking for signals from outer space to discover intelligent life. She eventually finds a source that transmits the sequence of prime numbers and this is considered as an answer to her questions. Everyone is convinced that the signal came from conscious, intelligent beings. My question as a mathematician is, is this absolutely true ? Can we have a non-intelligent physical system producing prime numbers ? I understand that I haven't actually defined the word ""intelligent"" and that's because it is very hard (if not impossible) to define at this point. This is some form of Turing test, no? It could be a program on a computer that does this but then some intelligent life form must have manufactured the computer and coded up the program so are we back to square one ? Thanks a bunch!","Have you seen this wonderful movie ""Contact"" starring Jodie Foster ? She is basically looking for signals from outer space to discover intelligent life. She eventually finds a source that transmits the sequence of prime numbers and this is considered as an answer to her questions. Everyone is convinced that the signal came from conscious, intelligent beings. My question as a mathematician is, is this absolutely true ? Can we have a non-intelligent physical system producing prime numbers ? I understand that I haven't actually defined the word ""intelligent"" and that's because it is very hard (if not impossible) to define at this point. This is some form of Turing test, no? It could be a program on a computer that does this but then some intelligent life form must have manufactured the computer and coded up the program so are we back to square one ? Thanks a bunch!",,"['analysis', 'prime-numbers', 'mathematical-physics']"
70,A proof for a Gidas-Ni-Nirenberg's Theorem in their 1981's paper,A proof for a Gidas-Ni-Nirenberg's Theorem in their 1981's paper,,"I want to ask whether my proof works for a theorem proved in Gidas-Ni-Nirenberg's second paper in 1981 ( MR0634248 ), which is much shorter than the authors' proof. Many thanks for any discussion. Theorem 1' (Page 378) Let $u > 0$ be a $C^2$ solution of $-\Delta u = g(u)$ in $\mathbb{R}^n, n \geq 3$ with $u(x) = O(|x|^{-m})$ at infinity, $m>0$ Assume (i) $g(s) \geq 0$ on $ 0\leq s \leq \|u\|_\infty$ and $g = g_1 + g_2$ with $g_1$ is Lipschitz and $g_2$ is continuous and non-decreasing, (ii) $g(s) = O(s^\alpha)$ near $s=0$ for some $\alpha > \frac{n+1}{m}$ . Then $u$ is radially symmetric about some point in $\mathbb{R}^n$ and $u_r < 0$ where $r$ is the radial coorinate about that point. They shows the symmetry in $x_1$ -axis by the well-known moving plane method. The first step is to show the set $\Lambda := \{ \lambda \in \mathbb{R}: u(x) > u(x^\lambda),  \forall x \in \Sigma_\lambda\}$ contains $[R,\infty)$ for some large $R>0$ . Notations: $x^\lambda$ is the reflection of $x$ with respect to the plane $\{ x_1 = \lambda \}$ . $\Sigma_\lambda = \{ x: x_1 < \lambda\}$ . To obtain such result, they need to show $\int_{\mathbb{R}^n} g(u(y)) dy > 0$ . The authors used 2 pages to prove such thing, which is much longer than mine. (That's why I confused.) My shorter proof: Assume not, then $g(u(y)) = 0$ in $\mathbb{R}^n$ by $g \geq 0$ . So $u$ is harmonic and bounded. The Liouville's theorem and decay condition implies $u \equiv 0$ , which contradicts to the positivity of $u$ .","I want to ask whether my proof works for a theorem proved in Gidas-Ni-Nirenberg's second paper in 1981 ( MR0634248 ), which is much shorter than the authors' proof. Many thanks for any discussion. Theorem 1' (Page 378) Let be a solution of in with at infinity, Assume (i) on and with is Lipschitz and is continuous and non-decreasing, (ii) near for some . Then is radially symmetric about some point in and where is the radial coorinate about that point. They shows the symmetry in -axis by the well-known moving plane method. The first step is to show the set contains for some large . Notations: is the reflection of with respect to the plane . . To obtain such result, they need to show . The authors used 2 pages to prove such thing, which is much longer than mine. (That's why I confused.) My shorter proof: Assume not, then in by . So is harmonic and bounded. The Liouville's theorem and decay condition implies , which contradicts to the positivity of .","u > 0 C^2 -\Delta u = g(u) \mathbb{R}^n, n \geq 3 u(x) = O(|x|^{-m}) m>0 g(s) \geq 0  0\leq s \leq \|u\|_\infty g = g_1 + g_2 g_1 g_2 g(s) = O(s^\alpha) s=0 \alpha > \frac{n+1}{m} u \mathbb{R}^n u_r < 0 r x_1 \Lambda := \{ \lambda \in \mathbb{R}: u(x) > u(x^\lambda),  \forall x \in \Sigma_\lambda\} [R,\infty) R>0 x^\lambda x \{ x_1 = \lambda \} \Sigma_\lambda = \{ x: x_1 < \lambda\} \int_{\mathbb{R}^n} g(u(y)) dy > 0 g(u(y)) = 0 \mathbb{R}^n g \geq 0 u u \equiv 0 u","['analysis', 'partial-differential-equations', 'elliptic-equations', 'maximum-principle']"
71,How can we factor out the maximum value of f'(x) in an integral with an absolute value?,How can we factor out the maximum value of f'(x) in an integral with an absolute value?,,I'm currently trying to understand a proof concerning the error term in the left- and right Riemann sums to approximate a definite integral. What I can't seem to understand is the last three lines of the proof where the author first factors out the maximum value of the derivative of f and sets up an inequality? The second part of my question then is how the author expands the integral to get the squared bracketed terms and the term 1/2. [Proof Image from Research Paper] https://i.sstatic.net/ywGd9.jpg,I'm currently trying to understand a proof concerning the error term in the left- and right Riemann sums to approximate a definite integral. What I can't seem to understand is the last three lines of the proof where the author first factors out the maximum value of the derivative of f and sets up an inequality? The second part of my question then is how the author expands the integral to get the squared bracketed terms and the term 1/2. [Proof Image from Research Paper] https://i.sstatic.net/ywGd9.jpg,,"['calculus', 'integration', 'analysis', 'inequality']"
72,Is this proof about the Riemann integration correct?,Is this proof about the Riemann integration correct?,,"Can someone tell me if is this proof correct? I don't know if it is correct because I did it myself, so.. 😏 If $f\in R$ on $[a,b]$ and $g$ is a monotonous function on $[a,b],$ then there exist $\epsilon \in [a,b]$ such that $$\int_a^bfg=g(a)\int_a^{\epsilon}f+g(b)\int_{\epsilon}^bf.$$ Proof: As $f\in R$ on $[a,b]$ then f is bounded and continuous almost everywhere. Let $\alpha(x)=\int_a^xf(t)dt$. By the fundamental theorem of calculus, $\alpha'(x)=f(x).$ So we have $d\alpha(x)=fd(x)$. Now, by the second mean value theorem for riemann-stieltjes integrals*, we have $$\int_a^bfg=\int_a^bgd(\alpha)=g(a)\int_a^{\epsilon}f+g(b)\int_{\epsilon}^bf$$ This is the $*2nd$ MVT for riemann-stieltjes integrals: Let $f$ be increasing on $[a,b]$, $g$ continuous on $[a,b]$. Then there exist $c\in [a,b]:\int_a^bfdg(x)=f(a)\int_a^cdg(x)+f(b)\int_c^bdg(x)$.","Can someone tell me if is this proof correct? I don't know if it is correct because I did it myself, so.. 😏 If $f\in R$ on $[a,b]$ and $g$ is a monotonous function on $[a,b],$ then there exist $\epsilon \in [a,b]$ such that $$\int_a^bfg=g(a)\int_a^{\epsilon}f+g(b)\int_{\epsilon}^bf.$$ Proof: As $f\in R$ on $[a,b]$ then f is bounded and continuous almost everywhere. Let $\alpha(x)=\int_a^xf(t)dt$. By the fundamental theorem of calculus, $\alpha'(x)=f(x).$ So we have $d\alpha(x)=fd(x)$. Now, by the second mean value theorem for riemann-stieltjes integrals*, we have $$\int_a^bfg=\int_a^bgd(\alpha)=g(a)\int_a^{\epsilon}f+g(b)\int_{\epsilon}^bf$$ This is the $*2nd$ MVT for riemann-stieltjes integrals: Let $f$ be increasing on $[a,b]$, $g$ continuous on $[a,b]$. Then there exist $c\in [a,b]:\int_a^bfdg(x)=f(a)\int_a^cdg(x)+f(b)\int_c^bdg(x)$.",,"['real-analysis', 'integration', 'analysis', 'proof-verification', 'riemann-integration']"
73,"Increment of a functionnal, derivative function","Increment of a functionnal, derivative function",,"Consider a path $x:[0;T] \rightarrow \mathbb{R}^2$ such that  $x(0)=x(T)$ and the dynamic is given by $\dot x = f(x)$. Considering some variations on the intial state $x(0)$ and on the final time $T$, I would like to analyze the equation $x(0)=x(T)$. The answer is  $$ \delta x(0) = d x(T) = \delta x(T) + f(x(T))\,d T. $$ which I don't understand. The increment of $x$ at time $0$ and $T$ wrt $x(0)$ and $T$ is $$  \Delta (x(0),\delta x(0),dT) = \Delta (x(T),\delta x(0),dT). \qquad (*) $$ But $\delta x(0)$ is the linear approximation of $\Delta x(0)$: $$ \Delta (x(0),\delta x(0),dT) = \delta (x(0),\delta x(0),dT) + H.O.T $$ so taking only the first order term in $(*)$  $$ \delta (x(0),\delta x(0),dT) = \delta (x(T),\delta x(0),dT) $$ i.e. $$ \underbrace{\delta (x(0),\delta x(0))}_{\delta x(0)}  =^? \delta (x(T),x(0))+\delta (x(T),dT)=\delta x(T) + dx(T) = \delta x(T) +  f(x(T))dT $$ but it doesn't yield the result First, why  $dx(T) = \delta x(T) + f(x(T))\,dT$? This is taken from the article (see equation (9) ) : [article][1] http://sci-hub.io/10.1109/TAC.1984.1103482","Consider a path $x:[0;T] \rightarrow \mathbb{R}^2$ such that  $x(0)=x(T)$ and the dynamic is given by $\dot x = f(x)$. Considering some variations on the intial state $x(0)$ and on the final time $T$, I would like to analyze the equation $x(0)=x(T)$. The answer is  $$ \delta x(0) = d x(T) = \delta x(T) + f(x(T))\,d T. $$ which I don't understand. The increment of $x$ at time $0$ and $T$ wrt $x(0)$ and $T$ is $$  \Delta (x(0),\delta x(0),dT) = \Delta (x(T),\delta x(0),dT). \qquad (*) $$ But $\delta x(0)$ is the linear approximation of $\Delta x(0)$: $$ \Delta (x(0),\delta x(0),dT) = \delta (x(0),\delta x(0),dT) + H.O.T $$ so taking only the first order term in $(*)$  $$ \delta (x(0),\delta x(0),dT) = \delta (x(T),\delta x(0),dT) $$ i.e. $$ \underbrace{\delta (x(0),\delta x(0))}_{\delta x(0)}  =^? \delta (x(T),x(0))+\delta (x(T),dT)=\delta x(T) + dx(T) = \delta x(T) +  f(x(T))dT $$ but it doesn't yield the result First, why  $dx(T) = \delta x(T) + f(x(T))\,dT$? This is taken from the article (see equation (9) ) : [article][1] http://sci-hub.io/10.1109/TAC.1984.1103482",,"['functional-analysis', 'analysis', 'derivatives']"
74,Application Banach Steinhaus: cannot see my mistake,Application Banach Steinhaus: cannot see my mistake,,"I'm studying Banach Steinhaus theorem in Rudin's functional analysis (page 43-46).  Theorem 2.9 says: If $X$,$Y$ are two topological vector spaces, $K$ is a compact convex set in $Y$, $\Gamma$ is a collection of continuous linear maps $X\to Y$ and the orbits $\Gamma(x)=\{\Lambda(x)|\Lambda \in \Gamma\}$ are boundend subsets of $Y$ for every $x\in K$. Then there is a bounded set $B\subset Y$ such that $\Gamma(K)\subset B$. I understand the proof of the theorem BUT: $K$ is a locally compact Hausdorff space, so by Baire's theorem is of second cathegory. By Banach Steinhaus theorem $\Gamma|_K$ is an equicontinuous collection of linear maps, thus (theorem 2.4)  $\Gamma|_K$ are equibounded. Finally since $K$ is bounded  $\Gamma|_K(K)=\Gamma (K)$ is a bounded subset of $Y$. I presume there is something wrong with this argument since I didn't use the convexity of $K$.","I'm studying Banach Steinhaus theorem in Rudin's functional analysis (page 43-46).  Theorem 2.9 says: If $X$,$Y$ are two topological vector spaces, $K$ is a compact convex set in $Y$, $\Gamma$ is a collection of continuous linear maps $X\to Y$ and the orbits $\Gamma(x)=\{\Lambda(x)|\Lambda \in \Gamma\}$ are boundend subsets of $Y$ for every $x\in K$. Then there is a bounded set $B\subset Y$ such that $\Gamma(K)\subset B$. I understand the proof of the theorem BUT: $K$ is a locally compact Hausdorff space, so by Baire's theorem is of second cathegory. By Banach Steinhaus theorem $\Gamma|_K$ is an equicontinuous collection of linear maps, thus (theorem 2.4)  $\Gamma|_K$ are equibounded. Finally since $K$ is bounded  $\Gamma|_K(K)=\Gamma (K)$ is a bounded subset of $Y$. I presume there is something wrong with this argument since I didn't use the convexity of $K$.",,"['functional-analysis', 'analysis', 'banach-spaces', 'topological-vector-spaces']"
75,Sums of entire functions,Sums of entire functions,,"Suppose $A$ and $B$ are two entire, non-surjective, functions. This means $$ A(z)=e^{f(z)}+c_1 $$ and $$   B(z)=e^{g(z)}+c_2 $$ for some entire functions $f$ and $g$, and some complex constants $c_1$ and $c_2$. When is the sum $A(z)+B(z)$ an surjective entire function? More precisely, is there any condition on $f$ and $g$. Clearly the sum may fail to be surjective, for example when $f(z)=g(z)+c$. But also can be surjective, for example when $f(z)=2g(z)$. A naive conjecture would be that the sum is surjective as long as $f-g$ is not constant. Is that actually true?","Suppose $A$ and $B$ are two entire, non-surjective, functions. This means $$ A(z)=e^{f(z)}+c_1 $$ and $$   B(z)=e^{g(z)}+c_2 $$ for some entire functions $f$ and $g$, and some complex constants $c_1$ and $c_2$. When is the sum $A(z)+B(z)$ an surjective entire function? More precisely, is there any condition on $f$ and $g$. Clearly the sum may fail to be surjective, for example when $f(z)=g(z)+c$. But also can be surjective, for example when $f(z)=2g(z)$. A naive conjecture would be that the sum is surjective as long as $f-g$ is not constant. Is that actually true?",,"['complex-analysis', 'analysis', 'complex-numbers']"
76,"uniform and pointwise convergence of $\sum_{n=1}^{\infty}\frac{1}{n^z}$ , Is the family $\{f_n : n \in \mathbb{N^*} \}$ normal?","uniform and pointwise convergence of  , Is the family  normal?",\sum_{n=1}^{\infty}\frac{1}{n^z} \{f_n : n \in \mathbb{N^*} \},Study the Convergence(pointwise and uniform ) of the sequence $(f_n)_{n \in \mathbb{N^*}}$ Where $$f_n(z)=\sum_{n=1}^{n}\frac{1}{n^z}$$ Is the family $\{f_n : n \in \mathbb{N^*} \}$ normal ? First we study the uniform convergence We must show that $\sum_{n=1}^{\infty} \frac{1}{n^z}$ is uniformly convergence in $\text{Re}\  z \geq 1+ \epsilon$ where $\epsilon >0$ $$\frac{1}{n^z} = \frac{1}{e^{z\  \text {Log}\  n}} =\frac{1}{e^{x\  \text {Log}\  n}}\frac{1}{e^{i y\  \text {Log}\  n}} $$ Now $$|\frac{1}{n^z}| = \frac{1}{n^x} \leq \frac{1}{n^{\epsilon +1}} \ \ \text{if} \ \ x \geq 1 + \epsilon$$ Let $M_n = \sum_{n=1}^{\infty} \frac{1}{n^{\epsilon +1}}$ which is converge So $\sum_{n=1}^{\infty} \frac{1}{n^z} $ converges uniformly if $x \geq 1 + \epsilon$ Moreover since $$\sum_{n=1}^{\infty}|\frac{1}{n^z}|=\sum_{n=1}^{\infty}\frac{1}{n^x} \ \ \text{which converges } \ \ \forall \ x >1$$ So $\sum_{n=1}^{\infty} \frac{1}{n^z}$ is absolutely poinwise convergent for $x > 1$ Finally How to prove that the  family is normal ? Thank you .,Study the Convergence(pointwise and uniform ) of the sequence $(f_n)_{n \in \mathbb{N^*}}$ Where $$f_n(z)=\sum_{n=1}^{n}\frac{1}{n^z}$$ Is the family $\{f_n : n \in \mathbb{N^*} \}$ normal ? First we study the uniform convergence We must show that $\sum_{n=1}^{\infty} \frac{1}{n^z}$ is uniformly convergence in $\text{Re}\  z \geq 1+ \epsilon$ where $\epsilon >0$ $$\frac{1}{n^z} = \frac{1}{e^{z\  \text {Log}\  n}} =\frac{1}{e^{x\  \text {Log}\  n}}\frac{1}{e^{i y\  \text {Log}\  n}} $$ Now $$|\frac{1}{n^z}| = \frac{1}{n^x} \leq \frac{1}{n^{\epsilon +1}} \ \ \text{if} \ \ x \geq 1 + \epsilon$$ Let $M_n = \sum_{n=1}^{\infty} \frac{1}{n^{\epsilon +1}}$ which is converge So $\sum_{n=1}^{\infty} \frac{1}{n^z} $ converges uniformly if $x \geq 1 + \epsilon$ Moreover since $$\sum_{n=1}^{\infty}|\frac{1}{n^z}|=\sum_{n=1}^{\infty}\frac{1}{n^x} \ \ \text{which converges } \ \ \forall \ x >1$$ So $\sum_{n=1}^{\infty} \frac{1}{n^z}$ is absolutely poinwise convergent for $x > 1$ Finally How to prove that the  family is normal ? Thank you .,,"['real-analysis', 'sequences-and-series', 'complex-analysis', 'analysis', 'uniform-convergence']"
77,Density of continuous functions with compact support in weak $L^1$,Density of continuous functions with compact support in weak,L^1,"One can prove that the space of continuous functions with compact support is dense in $L^p$. But is it also dense in $L^{1,\infty}$ (or $L^{p,\infty}$)? I am not able to come up with a counter-example but if there should be one, it would involve $f(x) = \frac{1}{|x|^{n/p}}$ since this does not belong to $L^p$ but is in $L^{p,\infty}$.","One can prove that the space of continuous functions with compact support is dense in $L^p$. But is it also dense in $L^{1,\infty}$ (or $L^{p,\infty}$)? I am not able to come up with a counter-example but if there should be one, it would involve $f(x) = \frac{1}{|x|^{n/p}}$ since this does not belong to $L^p$ but is in $L^{p,\infty}$.",,"['functional-analysis', 'analysis']"
78,Examples of continuous and bounded Frechet derivatives,Examples of continuous and bounded Frechet derivatives,,"Let $C_b^m(U, V)$ denote the space of not necessarily bounded mappings $g\colon U \to V$ that have $m\ge 1$ continuous and bounded Fréchet derivatives of order $1,\dots,m$. I see $C_b^m(U, V)$ reasonably often, which is endowed with the seminorm $|\cdot|_{C_b^m}$ defined as the smallest constant $C\ge0$ such that $$ \sup_{u\in U}\|D^m g(u)(x_1,\dots,x_m)\|_{V}\le C \|x_1\|_U\dotsb\|x_m\|_U\text{ for }x_1,\dots,x_m\in U. $$ Naive examples would be Nemytskii operators. I see if $U$ is a Banach space like $C_b(\overline{\Omega})$, then Nemytskii operator would do. But if $U$ is a Hilbert space, e.g., $L^2$ then differentiability implies that the Nemytskii operator must be generated by affine functions (e.g., Appell--Zabreiko's book, nonlinear superposition operators), which are not interesting. For smooth Sobolev spaces that are Banach algebras, the differentiability is fine but they won't be bounded? I wish to know what are examples of non-trivial (or non-affine) elements in $C_b^m(H,H)$ with $H$ being an (infinite dimensional) Hilbert space.","Let $C_b^m(U, V)$ denote the space of not necessarily bounded mappings $g\colon U \to V$ that have $m\ge 1$ continuous and bounded Fréchet derivatives of order $1,\dots,m$. I see $C_b^m(U, V)$ reasonably often, which is endowed with the seminorm $|\cdot|_{C_b^m}$ defined as the smallest constant $C\ge0$ such that $$ \sup_{u\in U}\|D^m g(u)(x_1,\dots,x_m)\|_{V}\le C \|x_1\|_U\dotsb\|x_m\|_U\text{ for }x_1,\dots,x_m\in U. $$ Naive examples would be Nemytskii operators. I see if $U$ is a Banach space like $C_b(\overline{\Omega})$, then Nemytskii operator would do. But if $U$ is a Hilbert space, e.g., $L^2$ then differentiability implies that the Nemytskii operator must be generated by affine functions (e.g., Appell--Zabreiko's book, nonlinear superposition operators), which are not interesting. For smooth Sobolev spaces that are Banach algebras, the differentiability is fine but they won't be bounded? I wish to know what are examples of non-trivial (or non-affine) elements in $C_b^m(H,H)$ with $H$ being an (infinite dimensional) Hilbert space.",,"['functional-analysis', 'analysis', 'reference-request', 'nonlinear-analysis']"
79,Direct computation of continuous spectrum for differential operator,Direct computation of continuous spectrum for differential operator,,"I have trouble to directly obtain the continuous spectrums of differential operators such as $\pm\frac{d}{dx}, \pm i\frac{d}{dx}, \pm \frac{d^2}{dx^2} \pm \frac{d}{dx}$, etc. I find it easier to calculate directly what the point spectrum is via finding eigenvalues, and the residual spectrum via finding the point spectrum of the adjoint. But there seems to be no equivalent method to do that for the continuous spectrum. One way that I rely on to find the continuous spectrum is to first obtain the inverse of the operator, then shows that it is bounded. Secondly, sometimes the domain is nice enough that I can obtain the result without actually trying. Is there some method to directly obtain the continuous spectrum without relying on fancy theorems or special cases? For a specific example, take any of the above operator and use the domain $C^1_0[0,1]$, or $C^1[0,1]$.","I have trouble to directly obtain the continuous spectrums of differential operators such as $\pm\frac{d}{dx}, \pm i\frac{d}{dx}, \pm \frac{d^2}{dx^2} \pm \frac{d}{dx}$, etc. I find it easier to calculate directly what the point spectrum is via finding eigenvalues, and the residual spectrum via finding the point spectrum of the adjoint. But there seems to be no equivalent method to do that for the continuous spectrum. One way that I rely on to find the continuous spectrum is to first obtain the inverse of the operator, then shows that it is bounded. Secondly, sometimes the domain is nice enough that I can obtain the result without actually trying. Is there some method to directly obtain the continuous spectrum without relying on fancy theorems or special cases? For a specific example, take any of the above operator and use the domain $C^1_0[0,1]$, or $C^1[0,1]$.",,"['functional-analysis', 'analysis', 'operator-theory', 'spectral-theory']"
80,"Prob. 12, Chap. 5 in Baby Rudin: The first, second, and third derivatives of $f(x) = |x|^3$","Prob. 12, Chap. 5 in Baby Rudin: The first, second, and third derivatives of",f(x) = |x|^3,"Here is Prob. 12, Chap. 5 in Principles of Mathematical Analysis by Walter Rudin, 3rd edition: If $f(x) = |x|^3$, compute $f^\prime(x)$, $f^{\prime\prime}(x)$ for all real $x$, and show that $f^{(3)}(0)$ does not exist. My Attempt: We can also write the formula for $f$ as    $$  f(x) = \begin{cases}  x^3 \ \mbox{ if $x > 0$}, \\  0 \ \mbox{ if $x = 0$}, \\  - x^3 \ \mbox{ if $x < 0$}. \end{cases}  $$   Then    $$f^\prime(0) = \lim_{h\to 0} \frac{ f(h) -f(0)}{h} = \lim_{h \to 0} \frac{|h|^3}{h} = \lim_{h \to 0} |h|^2  \mathrm{sgn} (h) = 0.$$    Thus   $$  f^\prime(x) = \begin{cases}  3x^2 \ \mbox{ if $x > 0$}, \\  0 \ \mbox{ if $x = 0$}, \\  - 3x^2 \ \mbox{ if $x < 0$}. \end{cases}  $$   That is,    $$f^\prime(x) = 3x^2 \mathrm{sgn}(x).$$   Now    $$ f^{\prime\prime}(0) = \lim_{h \to 0} \frac{ f^\prime(h) - f^\prime(0)}{h} = \lim_{h \to 0} \frac{ 3h^2 \mathrm{sgn} (h) }{h} = \lim_{h \to 0} 3h \ \mathrm{sgn} (h) = 0.$$   Thus    $$  f^{\prime\prime}(x) = \begin{cases}  6x \ \mbox{ if $x > 0$}, \\  0 \ \mbox{ if $x = 0$}, \\  - 6x^ \ \mbox{ if $x < 0$}. \end{cases}  $$   That is,    $$f^{\prime\prime}(x) = 6x\  \mathrm{sgn} (x).$$   Now    $$f^{(3)}(0) = \lim_{h \to 0} \frac{f^{\prime\prime}(h) - f^{\prime\prime}(0) }{h} = \lim_{h \to 0} 6 \mathrm{sgn}(h), $$   and this limit does not exist, as it is $+1$ as $h \to 0+$ and is $-1$ as $h \to 0-$. Is my calculation correct?","Here is Prob. 12, Chap. 5 in Principles of Mathematical Analysis by Walter Rudin, 3rd edition: If $f(x) = |x|^3$, compute $f^\prime(x)$, $f^{\prime\prime}(x)$ for all real $x$, and show that $f^{(3)}(0)$ does not exist. My Attempt: We can also write the formula for $f$ as    $$  f(x) = \begin{cases}  x^3 \ \mbox{ if $x > 0$}, \\  0 \ \mbox{ if $x = 0$}, \\  - x^3 \ \mbox{ if $x < 0$}. \end{cases}  $$   Then    $$f^\prime(0) = \lim_{h\to 0} \frac{ f(h) -f(0)}{h} = \lim_{h \to 0} \frac{|h|^3}{h} = \lim_{h \to 0} |h|^2  \mathrm{sgn} (h) = 0.$$    Thus   $$  f^\prime(x) = \begin{cases}  3x^2 \ \mbox{ if $x > 0$}, \\  0 \ \mbox{ if $x = 0$}, \\  - 3x^2 \ \mbox{ if $x < 0$}. \end{cases}  $$   That is,    $$f^\prime(x) = 3x^2 \mathrm{sgn}(x).$$   Now    $$ f^{\prime\prime}(0) = \lim_{h \to 0} \frac{ f^\prime(h) - f^\prime(0)}{h} = \lim_{h \to 0} \frac{ 3h^2 \mathrm{sgn} (h) }{h} = \lim_{h \to 0} 3h \ \mathrm{sgn} (h) = 0.$$   Thus    $$  f^{\prime\prime}(x) = \begin{cases}  6x \ \mbox{ if $x > 0$}, \\  0 \ \mbox{ if $x = 0$}, \\  - 6x^ \ \mbox{ if $x < 0$}. \end{cases}  $$   That is,    $$f^{\prime\prime}(x) = 6x\  \mathrm{sgn} (x).$$   Now    $$f^{(3)}(0) = \lim_{h \to 0} \frac{f^{\prime\prime}(h) - f^{\prime\prime}(0) }{h} = \lim_{h \to 0} 6 \mathrm{sgn}(h), $$   and this limit does not exist, as it is $+1$ as $h \to 0+$ and is $-1$ as $h \to 0-$. Is my calculation correct?",,"['calculus', 'real-analysis', 'analysis', 'limits', 'derivatives']"
81,"Prob. 11, Chap. 5 in Baby Rudin: If $f$ is defined in a neighborhood of $x$ and if $f^{\prime\prime}(x)$ exists, then . . .","Prob. 11, Chap. 5 in Baby Rudin: If  is defined in a neighborhood of  and if  exists, then . . .",f x f^{\prime\prime}(x),"Here is Prob. 11, Chap. 5 in the book Principles of Mathematical Analysis by Walter Rudin, 3rd edition: Suppose $f$ is defined in a neighborhood of $x$, and suppose $f^{\prime\prime}(x)$ exists. Show that $$ \lim_{h \to 0} \frac{ f(x+h)+f(x-h)-2f(x)}{h^2} = f^{\prime\prime}(x).$$ Show by an example that the limit may exist even if $f^{\prime\prime}(x)$ does not. Hint: Use Theorem 5.13. My Attempt: As $f{\prime\prime}(x)$ exists, so there is an interval containing $x$ in which $f^\prime$ exists, and $f^\prime$ is continuous at $x$. [What if $x$ is an endpoint of the interval? How can we gaurantee that there is a segment containing $x$ on which $f^\prime$ exists? ] So there is an interval containing $x$ on which $f$ is defined and $f$ is continuous at $x$. Therefore by Theorem 5.13 (i.e. L'Hospital's rule) in Baby Rudin, we obtain   $$ \begin{align} \lim_{h \to 0} \frac{f(x+h) + f(x-h) -2f(x) }{h^2}  &= \lim_{h \to 0} \frac{f^\prime(x+h) - f^\prime(x-h)}{2h} \\ & \ \ \  \mbox{ [ using L'Hospital's rule, } \\ & \ \ \ \ \ \ \mbox{ because $f(x+h)+f(x-h)-2f(x) \to 0$ and $h \to 0$ } \\ & \ \ \ \ \ \ \ \ \ \mbox{ as $h \to 0$ ]} \\ &= \lim_{h \to 0} \frac{ f^\prime(x+h) - f^\prime(x) + f^\prime(x) - f^\prime(x-h) }{2h} \\ &= \lim_{h\to 0} \frac{1}{2} \left( \frac{ f^\prime(x+h) - f^\prime(x) }{h} + \frac{ f^\prime(x-h)-f^\prime(x)}{-h} \right) \\ &= \lim_{h \to 0} \frac{1}{2} \left( \mathrm{R}f^{\prime\prime}(x) + \mathrm{L}f^{\prime\prime}(x)  \right) \\ & \ \ \ \mbox{ [ Here $\mathrm{R}f^{\prime\prime}(x)$ denotes the right-hand derivative of $f$ } \\ & \ \ \ \ \ \ \mbox{ at $x$, and $\mathrm{L}f^{\prime\prime}(x)$ denotes the left-hand derivative; } \\ & \ \ \ \ \ \ \ \ \ \mbox{ we assume of course that here $h \to 0+$ ]} \\ &= f^{\prime\prime}(x). \end{align} $$ Is this reasoning correct? Now let $f$ be the real function defined on $\mathbb{R}^1$ by    $$ f(x) = \begin{cases} x^2 \sin \frac{1}{x} \ \mbox{ for $x \neq 0$} \\ 0 \ \mbox{ for $x = 0$}. \end{cases} $$    Then    $$  \lim_{h \to 0} \frac{ f(h) + f(-h) - 2f(0) }{h^2}  = \lim_{h \to 0} \frac{ f(h) - f(h) }{h^2}  = 0, $$   but    $$ \begin{align} f^{\prime\prime}(0)  &= \lim_{h \to 0} \frac{ f^\prime(h) - f^\prime(0) }{h} \\ &= \lim_{h \to 0} \frac{ \left( 2h \sin \frac{1}{h} - \cos \frac{1}{h} \right) - 0 }{h} \\ &= \lim_{h \to 0} \left( 2 \sin \frac{1}{h} - \frac{\cos \frac{1}{h} }{h}  \right),  \end{align} $$   and the last limit (and hence $f^{\prime\prime}(0)$) of course does not exist. Am I right? However, as Rudin has not yet discussed the trigonometric functions (in a rigorous and systematic manner), I would like to have an example which is within the corpus of the examples dealt with by Rudin up to this point in the book.","Here is Prob. 11, Chap. 5 in the book Principles of Mathematical Analysis by Walter Rudin, 3rd edition: Suppose $f$ is defined in a neighborhood of $x$, and suppose $f^{\prime\prime}(x)$ exists. Show that $$ \lim_{h \to 0} \frac{ f(x+h)+f(x-h)-2f(x)}{h^2} = f^{\prime\prime}(x).$$ Show by an example that the limit may exist even if $f^{\prime\prime}(x)$ does not. Hint: Use Theorem 5.13. My Attempt: As $f{\prime\prime}(x)$ exists, so there is an interval containing $x$ in which $f^\prime$ exists, and $f^\prime$ is continuous at $x$. [What if $x$ is an endpoint of the interval? How can we gaurantee that there is a segment containing $x$ on which $f^\prime$ exists? ] So there is an interval containing $x$ on which $f$ is defined and $f$ is continuous at $x$. Therefore by Theorem 5.13 (i.e. L'Hospital's rule) in Baby Rudin, we obtain   $$ \begin{align} \lim_{h \to 0} \frac{f(x+h) + f(x-h) -2f(x) }{h^2}  &= \lim_{h \to 0} \frac{f^\prime(x+h) - f^\prime(x-h)}{2h} \\ & \ \ \  \mbox{ [ using L'Hospital's rule, } \\ & \ \ \ \ \ \ \mbox{ because $f(x+h)+f(x-h)-2f(x) \to 0$ and $h \to 0$ } \\ & \ \ \ \ \ \ \ \ \ \mbox{ as $h \to 0$ ]} \\ &= \lim_{h \to 0} \frac{ f^\prime(x+h) - f^\prime(x) + f^\prime(x) - f^\prime(x-h) }{2h} \\ &= \lim_{h\to 0} \frac{1}{2} \left( \frac{ f^\prime(x+h) - f^\prime(x) }{h} + \frac{ f^\prime(x-h)-f^\prime(x)}{-h} \right) \\ &= \lim_{h \to 0} \frac{1}{2} \left( \mathrm{R}f^{\prime\prime}(x) + \mathrm{L}f^{\prime\prime}(x)  \right) \\ & \ \ \ \mbox{ [ Here $\mathrm{R}f^{\prime\prime}(x)$ denotes the right-hand derivative of $f$ } \\ & \ \ \ \ \ \ \mbox{ at $x$, and $\mathrm{L}f^{\prime\prime}(x)$ denotes the left-hand derivative; } \\ & \ \ \ \ \ \ \ \ \ \mbox{ we assume of course that here $h \to 0+$ ]} \\ &= f^{\prime\prime}(x). \end{align} $$ Is this reasoning correct? Now let $f$ be the real function defined on $\mathbb{R}^1$ by    $$ f(x) = \begin{cases} x^2 \sin \frac{1}{x} \ \mbox{ for $x \neq 0$} \\ 0 \ \mbox{ for $x = 0$}. \end{cases} $$    Then    $$  \lim_{h \to 0} \frac{ f(h) + f(-h) - 2f(0) }{h^2}  = \lim_{h \to 0} \frac{ f(h) - f(h) }{h^2}  = 0, $$   but    $$ \begin{align} f^{\prime\prime}(0)  &= \lim_{h \to 0} \frac{ f^\prime(h) - f^\prime(0) }{h} \\ &= \lim_{h \to 0} \frac{ \left( 2h \sin \frac{1}{h} - \cos \frac{1}{h} \right) - 0 }{h} \\ &= \lim_{h \to 0} \left( 2 \sin \frac{1}{h} - \frac{\cos \frac{1}{h} }{h}  \right),  \end{align} $$   and the last limit (and hence $f^{\prime\prime}(0)$) of course does not exist. Am I right? However, as Rudin has not yet discussed the trigonometric functions (in a rigorous and systematic manner), I would like to have an example which is within the corpus of the examples dealt with by Rudin up to this point in the book.",,"['calculus', 'real-analysis', 'analysis', 'limits', 'derivatives']"
82,"Prob. 6, Chap. 5 in Baby Rudin: For $f$ continuous for $x\geq 0$ and differentiable for $x>0$ with $f(0)=0$, if $f^\prime$ is ...","Prob. 6, Chap. 5 in Baby Rudin: For  continuous for  and differentiable for  with , if  is ...",f x\geq 0 x>0 f(0)=0 f^\prime,"Here is Prob. 6, Chap. 5 in the book Principles of Mathematical Analysis by Walter Rudin, 3rd edition: Suppose (a) $f$ is continuous for $x \geq 0$ , (b) $f^\prime(x)$ exists for $x>0$ , (c) $f(0)=0$ , (d) $f^\prime$ is monotonically increasing. Put $$ g(x) = \frac{f(x)}{x} \qquad  ( x>0)$$ and prove that $g$ is monotonically increasing. My Attempt: For any $x>0$ , as $f$ is continuous on $[0, x]$ and differentiable on $(0, x)$ , so by the Mean Value Theorem there is a point $p \in (0, x)$ such that $$ f(x) = f(x) - 0 = f(x)-f(0) = (x-0) f^\prime(p) = x f^\prime(p).$$ But as $f^\prime$ is monotonically increasing on $(0, +\infty)$ and as $0 < p < x$ , so we can conclude that $f^\prime(p) \leq f^\prime(x)$ and hence $$f(x) = xf^\prime(p) \leq x f^\prime(x),$$ which implies that $$x f^\prime(x) - f(x) \geq 0 \ \mbox{ for all } \ x > 0.$$ Now we see that, for any $x > 0$ , $$ g^\prime(x) = \frac{f^\prime(x)}{x} - \frac{f(x)}{x^2} =  \frac{ xf^\prime(x) - f(x)}{x^2} \geq 0,$$ which implies that $g$ is monotonically increasing on $(0, +\infty)$ . Is the above proof correct?","Here is Prob. 6, Chap. 5 in the book Principles of Mathematical Analysis by Walter Rudin, 3rd edition: Suppose (a) is continuous for , (b) exists for , (c) , (d) is monotonically increasing. Put and prove that is monotonically increasing. My Attempt: For any , as is continuous on and differentiable on , so by the Mean Value Theorem there is a point such that But as is monotonically increasing on and as , so we can conclude that and hence which implies that Now we see that, for any , which implies that is monotonically increasing on . Is the above proof correct?","f x \geq 0 f^\prime(x) x>0 f(0)=0 f^\prime  g(x) = \frac{f(x)}{x} \qquad  ( x>0) g x>0 f [0, x] (0, x) p \in (0, x)  f(x) = f(x) - 0 = f(x)-f(0) = (x-0) f^\prime(p) = x f^\prime(p). f^\prime (0, +\infty) 0 < p < x f^\prime(p) \leq f^\prime(x) f(x) = xf^\prime(p) \leq x f^\prime(x), x f^\prime(x) - f(x) \geq 0 \ \mbox{ for all } \ x > 0. x > 0  g^\prime(x) = \frac{f^\prime(x)}{x} - \frac{f(x)}{x^2} =  \frac{ xf^\prime(x) - f(x)}{x^2} \geq 0, g (0, +\infty)","['calculus', 'real-analysis', 'analysis', 'derivatives']"
83,"Prob. 4, Chap. 5 in Baby Rudin: The existence of a solution in $(0, 1)$","Prob. 4, Chap. 5 in Baby Rudin: The existence of a solution in","(0, 1)","Here is Prob. 4, Chap. 5 in the book Principles of Mathematical Analysis by Walter Rudin, 3rd edition: If $$ C_0 + \frac{C_1}{2} + \cdots + \frac{C_{n-1}}{n} + \frac{C_n}{n+1} = 0,$$ where $C_0, \ldots, C_n$ are real constants, prove that the equation    $$ C_0 + C_1 x + \cdots + C_{n-1}x^{n-1} + C_n x^n = 0$$    has at least one real root between $0$ and $1$. My Attempt: Let $f$ be the real function defined on $[0, 1]$ by the equation $$f(x) \colon=  C_0 x + \frac{C_1 x^2 }{2} + \cdots + \frac{C_{n-1} x^n}{n} + \frac{C_n x^{n+1} }{n+1}. $$   Then $f(0) = 0$, and also $f(1) = 0$, by our hypothesis. Moreover, $f$ is continuous on $[0, 1]$ and differentiable on $(0, 1)$. So by the Mean Value Theorem there is a point $p \in (0, 1)$ such that    $$ f(1) - f(0) = (1-0) f^\prime(p),$$   that is, there is a point $p \in (0, 1)$ such that    $$ f^\prime(p) =  C_0 + C_1 p + \cdots + C_{n-1} p^{n-1} + C_n p^n = 0,$$    as required. Is this proof correct?","Here is Prob. 4, Chap. 5 in the book Principles of Mathematical Analysis by Walter Rudin, 3rd edition: If $$ C_0 + \frac{C_1}{2} + \cdots + \frac{C_{n-1}}{n} + \frac{C_n}{n+1} = 0,$$ where $C_0, \ldots, C_n$ are real constants, prove that the equation    $$ C_0 + C_1 x + \cdots + C_{n-1}x^{n-1} + C_n x^n = 0$$    has at least one real root between $0$ and $1$. My Attempt: Let $f$ be the real function defined on $[0, 1]$ by the equation $$f(x) \colon=  C_0 x + \frac{C_1 x^2 }{2} + \cdots + \frac{C_{n-1} x^n}{n} + \frac{C_n x^{n+1} }{n+1}. $$   Then $f(0) = 0$, and also $f(1) = 0$, by our hypothesis. Moreover, $f$ is continuous on $[0, 1]$ and differentiable on $(0, 1)$. So by the Mean Value Theorem there is a point $p \in (0, 1)$ such that    $$ f(1) - f(0) = (1-0) f^\prime(p),$$   that is, there is a point $p \in (0, 1)$ such that    $$ f^\prime(p) =  C_0 + C_1 p + \cdots + C_{n-1} p^{n-1} + C_n p^n = 0,$$    as required. Is this proof correct?",,"['calculus', 'real-analysis', 'analysis', 'derivatives']"
84,Is this the odd expansion of the function $f(x)=x$ if $0\leq x<1$ and $f(x)=1$ if $1\leq x<2$?,Is this the odd expansion of the function  if  and  if ?,f(x)=x 0\leq x<1 f(x)=1 1\leq x<2,"We have the function $f(x)=\left\{\begin{matrix} x &, 0\leq x<1 \\  1 &, 1\leq x<2  \end{matrix}\right.$ I want to compute the Fourier sinus series with period 4. Is the odd expansion of the function the following? $h(x)=\left\{\begin{matrix}  -1 &, -2<x<-1 \\  x& , -1<x<0 \\  0 & , x=0,2 \\  x &, 0\leq x<1 \\  1 &, 1\leq x<2  \end{matrix}\right.$ The graph of that function is this: I think that it is wrong, since a periodic function must be the same for each period. But in this case the line segment $y=x$ is getting higher at each interval. Can that be?","We have the function $f(x)=\left\{\begin{matrix} x &, 0\leq x<1 \\  1 &, 1\leq x<2  \end{matrix}\right.$ I want to compute the Fourier sinus series with period 4. Is the odd expansion of the function the following? $h(x)=\left\{\begin{matrix}  -1 &, -2<x<-1 \\  x& , -1<x<0 \\  0 & , x=0,2 \\  x &, 0\leq x<1 \\  1 &, 1\leq x<2  \end{matrix}\right.$ The graph of that function is this: I think that it is wrong, since a periodic function must be the same for each period. But in this case the line segment $y=x$ is getting higher at each interval. Can that be?",,"['analysis', 'fourier-series']"
85,Prove $A\cap f^{-1}(z)\subset \cup_{i=1}^{n} O_i$ if $A\cap f^{-1}(y)\subset \cup_{i=1}^{n} O_i$ and $z$ is close enough to $y$ when $A$ is compact,Prove  if  and  is close enough to  when  is compact,A\cap f^{-1}(z)\subset \cup_{i=1}^{n} O_i A\cap f^{-1}(y)\subset \cup_{i=1}^{n} O_i z y A,"I want to prove Let $A$ be a compact subset of $\mathbb{R}^d$ and $f$ be a continuous function defined on $A$. If $A\cap f^{-1}(y)\subset \cup_{i=1}^{n} O_i$ for a given $y$, where $\{O_i\}_{i=1}^n$ is a collection of open sets, then $A\cap f^{-1}(z)\subset \cup_{i=1}^{n} O_i$ for any $z$ close enough to $y$. I know that the fact $A$ being compact is essential, because I can easily construct a continuous function whose inverse function is NOT continuous so that it can fail the conclusion. (In the above statement, $f$ is not necessarily bijective though.) But I'm not sure how I can make use of this compactness condition. Thanks for any tip.","I want to prove Let $A$ be a compact subset of $\mathbb{R}^d$ and $f$ be a continuous function defined on $A$. If $A\cap f^{-1}(y)\subset \cup_{i=1}^{n} O_i$ for a given $y$, where $\{O_i\}_{i=1}^n$ is a collection of open sets, then $A\cap f^{-1}(z)\subset \cup_{i=1}^{n} O_i$ for any $z$ close enough to $y$. I know that the fact $A$ being compact is essential, because I can easily construct a continuous function whose inverse function is NOT continuous so that it can fail the conclusion. (In the above statement, $f$ is not necessarily bijective though.) But I'm not sure how I can make use of this compactness condition. Thanks for any tip.",,"['real-analysis', 'general-topology', 'analysis', 'measure-theory']"
86,Error in approximation by convolution with a mollifier or a Gaussian,Error in approximation by convolution with a mollifier or a Gaussian,,"Let $f : \mathbb{R} \to \mathbb{R}$ be bounded and supported on some finite non-empty open interval $(a,b)$. For $\delta > 0$, let $\phi_\delta(x) = \delta^{-1}\phi(x/\delta)$ be the standard mollifier. If $f$ is smooth, what is an upper bound for $\|(f \ast \phi_\delta) - f\|_{L_p}$ in terms of $\delta$? I'm especially interested in the case when $p=1$. What in the case when $f$ is only continuous? It would be interesting to know what happens when $\phi(x) = \frac{1}{\sqrt{2\pi}}e^{-x^2/2}$ is the Gaussian as well. I think these things should be well known, but I was not able to find a reference online. Thanks!","Let $f : \mathbb{R} \to \mathbb{R}$ be bounded and supported on some finite non-empty open interval $(a,b)$. For $\delta > 0$, let $\phi_\delta(x) = \delta^{-1}\phi(x/\delta)$ be the standard mollifier. If $f$ is smooth, what is an upper bound for $\|(f \ast \phi_\delta) - f\|_{L_p}$ in terms of $\delta$? I'm especially interested in the case when $p=1$. What in the case when $f$ is only continuous? It would be interesting to know what happens when $\phi(x) = \frac{1}{\sqrt{2\pi}}e^{-x^2/2}$ is the Gaussian as well. I think these things should be well known, but I was not able to find a reference online. Thanks!",,"['analysis', 'convolution', 'approximation-theory']"
87,Help with a question on Taylors Theorem,Help with a question on Taylors Theorem,,"Let $g:R→R$ be a twice differentiable function satisfying $g(0)=1,g'(0)=0$ and $g''(x)−g(x)=0$ for all $x \in \mathbb{R}$ Fix $x \in R$. Show that there exists $M>0$ such that for all natural numbers $n$ and all $\theta$ from 0 to 1   $|g^{(n)}(\theta x)|≤M$ Also, find the coefficients of the Taylor expansion of $g$ about $0$, and prove that this expansion converges to $g(x)$ for all $x \in \mathbb{R}$. I have proved that $g(x)$ has derivatives of all orders, not sure how to use this to proceed with the above question.","Let $g:R→R$ be a twice differentiable function satisfying $g(0)=1,g'(0)=0$ and $g''(x)−g(x)=0$ for all $x \in \mathbb{R}$ Fix $x \in R$. Show that there exists $M>0$ such that for all natural numbers $n$ and all $\theta$ from 0 to 1   $|g^{(n)}(\theta x)|≤M$ Also, find the coefficients of the Taylor expansion of $g$ about $0$, and prove that this expansion converges to $g(x)$ for all $x \in \mathbb{R}$. I have proved that $g(x)$ has derivatives of all orders, not sure how to use this to proceed with the above question.",,"['real-analysis', 'analysis']"
88,"Prove the indicator function is not Riemann integrable on $[0,1]$.",Prove the indicator function is not Riemann integrable on .,"[0,1]","Let $\{p_1, p_2, p_3,...\} = \mathbb{Q}\cap[0,1]$ and $S=\cup_{n=1}^\infty B(p_n, 10^{-n})\cap [0,1]$, where $B(p,r)$ is the ball around the point $p$ with radius $r$. Prove the indicator function, $1_S$, is not Riemann integrable. I think it would be possible to show the set of discontinuities of $1_S$ is of positive measure, which by Lebesgue's criterion would prove the fact. I'm not sure how to do this though. Thoughts?","Let $\{p_1, p_2, p_3,...\} = \mathbb{Q}\cap[0,1]$ and $S=\cup_{n=1}^\infty B(p_n, 10^{-n})\cap [0,1]$, where $B(p,r)$ is the ball around the point $p$ with radius $r$. Prove the indicator function, $1_S$, is not Riemann integrable. I think it would be possible to show the set of discontinuities of $1_S$ is of positive measure, which by Lebesgue's criterion would prove the fact. I'm not sure how to do this though. Thoughts?",,"['real-analysis', 'analysis']"
89,"Point of dense orbit under the action of the tent map on $[0,1]$",Point of dense orbit under the action of the tent map on,"[0,1]","Consider the tent map $f:[0,1] \to [0,1]$, defined by $f(x)=2x$ if $x<0.5$, or $x =0.5$ and $f(x)=2-2x$ if $x>0.5$. Describe a point in $[0,1]$ whose orbit under $f$ is dense in $[0,1]$. This is a question which is part of a course in fractal geometry and chaotic dynamics. I realise that the functions in this equation are the inverse of the equations in an iterated function system which maps $[0,1]$ onto itself (i.e., $[0,1]$ is the attractor). However, I'm not quite sure how to find the points with periodic orbits, and I'm not sure if it is best to examine the original equation or the iterated functions system to find the points.","Consider the tent map $f:[0,1] \to [0,1]$, defined by $f(x)=2x$ if $x<0.5$, or $x =0.5$ and $f(x)=2-2x$ if $x>0.5$. Describe a point in $[0,1]$ whose orbit under $f$ is dense in $[0,1]$. This is a question which is part of a course in fractal geometry and chaotic dynamics. I realise that the functions in this equation are the inverse of the equations in an iterated function system which maps $[0,1]$ onto itself (i.e., $[0,1]$ is the attractor). However, I'm not quite sure how to find the points with periodic orbits, and I'm not sure if it is best to examine the original equation or the iterated functions system to find the points.",,"['real-analysis', 'analysis', 'dynamical-systems', 'chaos-theory']"
90,"Show that the family of all analytic maps $f: B(0,1)\to\{z\in\Bbb C: Re(z)>0\}$ such that $|f(0)|\le1$ is normal.",Show that the family of all analytic maps  such that  is normal.,"f: B(0,1)\to\{z\in\Bbb C: Re(z)>0\} |f(0)|\le1","Show that the family of all analytic maps $f: B(0,1)\to\{z\in\Bbb C: Re(z)>0\}$ such that $|f(0)|\le1$ is normal. I tried to use Azela-Ascoli Theorem. first show for each $z\in B(0,1)$,$f(z)$ has compact closure. Second is to show at each point $z$ the family of functions are equicontinuous. But I do not know how to show $f(z)$ has compact closure for each $z$ and $f$ is equicontinuous. Is this the right direction to prove this question? Could someone kindly help? Thanks!","Show that the family of all analytic maps $f: B(0,1)\to\{z\in\Bbb C: Re(z)>0\}$ such that $|f(0)|\le1$ is normal. I tried to use Azela-Ascoli Theorem. first show for each $z\in B(0,1)$,$f(z)$ has compact closure. Second is to show at each point $z$ the family of functions are equicontinuous. But I do not know how to show $f(z)$ has compact closure for each $z$ and $f$ is equicontinuous. Is this the right direction to prove this question? Could someone kindly help? Thanks!",,"['real-analysis', 'complex-analysis', 'analysis']"
91,To find out the number of components of a given set,To find out the number of components of a given set,,Find the number of connected components of the set $$\left\{x\in \mathbb R : x^3\left(x^2+5x-\frac{65}{3}\right)>70x^2-300x-297\right\}$$ under the usual topology on $\mathbb R$ . I tried to factorize the both right hand side and left hand side but fails. It is difficult to handle the too large constant. What's the actual process to solve this type of problems ? Any hint. ?,Find the number of connected components of the set under the usual topology on . I tried to factorize the both right hand side and left hand side but fails. It is difficult to handle the too large constant. What's the actual process to solve this type of problems ? Any hint. ?,\left\{x\in \mathbb R : x^3\left(x^2+5x-\frac{65}{3}\right)>70x^2-300x-297\right\} \mathbb R,"['real-analysis', 'general-topology', 'analysis', 'metric-spaces', 'connectedness']"
92,Necas inequality on unit ball,Necas inequality on unit ball,,"I want to show the inequality of Necas on the unit ball, knowing that  I proved it for $\mathbb R^d$ using Fourier transform, here it is: Let $B$ be the unit ball of $\mathbb R^d$. Then there is $C>0$ that  depends      only on $B$ such that:       $$\left \| P \right \| _{L_2(B)} \leqslant  C \left \| P \right \| _{\chi(B)}  \quad\forall P \in L_2^{0}(B). $$ with the notation $\\ L_2^{0}(B)=\{P\in L^2(B;\; \int_{B}^{}P(x)dx=0\}$ $\chi(B):=\{P \in H^{-1} ,\nabla P \in (H^{-1}(B))^N\}$ and $\left \| P \right \| _{\chi(B)} = \left \| P \right \|_{H^{-1}(B)}+\left \| \nabla P \right \| _{(H^{-1}(B))^N}$ . There is the idea: let $\eta \in \mathcal D (B)$ be such that $\eta(x) = 1$ for $|x| \leq 1/3$ and $\eta(x) = 0$ for $|x| \geq  1/2$. Then: $$\left \| \eta P \right \| _{L_2(B)} = \left \| \eta P \right \| _{L_2(\mathbb R^d)}\leqslant  C \left \| \eta P \right \| _{\chi(\mathbb R^d)},\;\;\forall P \in L_2^{0}(B).$$ My question is: How can I prove that: $\left \| \eta P \right \| _{\chi(\mathbb R^d)}\leqslant\left \| \eta P \right \| _{\chi(B)}$? thank you for your help.","I want to show the inequality of Necas on the unit ball, knowing that  I proved it for $\mathbb R^d$ using Fourier transform, here it is: Let $B$ be the unit ball of $\mathbb R^d$. Then there is $C>0$ that  depends      only on $B$ such that:       $$\left \| P \right \| _{L_2(B)} \leqslant  C \left \| P \right \| _{\chi(B)}  \quad\forall P \in L_2^{0}(B). $$ with the notation $\\ L_2^{0}(B)=\{P\in L^2(B;\; \int_{B}^{}P(x)dx=0\}$ $\chi(B):=\{P \in H^{-1} ,\nabla P \in (H^{-1}(B))^N\}$ and $\left \| P \right \| _{\chi(B)} = \left \| P \right \|_{H^{-1}(B)}+\left \| \nabla P \right \| _{(H^{-1}(B))^N}$ . There is the idea: let $\eta \in \mathcal D (B)$ be such that $\eta(x) = 1$ for $|x| \leq 1/3$ and $\eta(x) = 0$ for $|x| \geq  1/2$. Then: $$\left \| \eta P \right \| _{L_2(B)} = \left \| \eta P \right \| _{L_2(\mathbb R^d)}\leqslant  C \left \| \eta P \right \| _{\chi(\mathbb R^d)},\;\;\forall P \in L_2^{0}(B).$$ My question is: How can I prove that: $\left \| \eta P \right \| _{\chi(\mathbb R^d)}\leqslant\left \| \eta P \right \| _{\chi(B)}$? thank you for your help.",,"['real-analysis', 'analysis', 'inequality', 'partial-differential-equations', 'sobolev-spaces']"
93,If $E\subseteq \mathbb{R}$ is measurable and $\delta>0$ then there exists open set $U$ s.t. $\delta \mu(U)<\mu(E)$,If  is measurable and  then there exists open set  s.t.,E\subseteq \mathbb{R} \delta>0 U \delta \mu(U)<\mu(E),"In part of the proof of a problem I am trying to solve I need the following fact (assume that $\mu$ is the Lebesgue measure): If $E\subseteq \mathbb{R}$ is measurable and $\delta>0$ then there exists open set $U\subseteq \mathbb{R}$, such that $E\subseteq U$ and $\,$ $\delta \mu(U)<\mu(E)$. I know and have proven the following fact: Suppose $E \subseteq \mathbb{R}$. Then for each $\epsilon>0$ there exists an open set $U\subseteq \mathbb{R}$ such that $E\subseteq U$ and $\mu(U)< \mu(E)+\epsilon$. I am pretty sure I can use the second fact to prove the first fact, but I keep getting a value of $\epsilon$ that is in terms of $\mu(U)$, which isn't good because $U$ should depend on $\epsilon$, not the other way around. Some help?","In part of the proof of a problem I am trying to solve I need the following fact (assume that $\mu$ is the Lebesgue measure): If $E\subseteq \mathbb{R}$ is measurable and $\delta>0$ then there exists open set $U\subseteq \mathbb{R}$, such that $E\subseteq U$ and $\,$ $\delta \mu(U)<\mu(E)$. I know and have proven the following fact: Suppose $E \subseteq \mathbb{R}$. Then for each $\epsilon>0$ there exists an open set $U\subseteq \mathbb{R}$ such that $E\subseteq U$ and $\mu(U)< \mu(E)+\epsilon$. I am pretty sure I can use the second fact to prove the first fact, but I keep getting a value of $\epsilon$ that is in terms of $\mu(U)$, which isn't good because $U$ should depend on $\epsilon$, not the other way around. Some help?",,"['real-analysis', 'analysis', 'measure-theory', 'lebesgue-measure']"
94,Equality on a limit involving solution of heat equation,Equality on a limit involving solution of heat equation,,"I've been trying to solve one exercise on PDEs for a while now and I have everything except from this equality . It would be really useful if this were true, but I can't prove it and I think it should be because I am following some hints and I got here. Here $g$ can be smooth, $g(0)=0$ and that's what we know. The step is $$\lim_\limits{x\rightarrow 0^+}\dfrac{x}{\sqrt{4\pi}}\int\limits_0^{t-\delta}\dfrac{1}{(t-s)^{3/2}}e^{-\frac{x^2}{4(t-s)}}g(s) ds+\lim_\limits{x\rightarrow 0^+}\dfrac{x}{\sqrt{4\pi}}\int\limits_{t-\delta}^t\dfrac{1}{(t-s)^{3/2}}e^{-\frac{x^2}{4(t-s)}}g(s) ds$$ $$= g(t) \lim_\limits{x\rightarrow 0^+}\dfrac{x}{\sqrt{4\pi}}\int\limits_{t-\delta}^t\dfrac{1}{(t-s)^{3/2}}e^{-\frac{x^2}{4(t-s)}}ds$$ So this is what I think: since we have excluded $t$ from the first integral, we can simply substitute $x$ by $0$ and so the first limit is $0$. Now this is not true for the second limit, as the integral explodes close to $0$. But how can $g$ leave the integral? Is it some argument of continuity? I tried integration by parts but I don't seem to go anywhere. Thank you very much for your attention!","I've been trying to solve one exercise on PDEs for a while now and I have everything except from this equality . It would be really useful if this were true, but I can't prove it and I think it should be because I am following some hints and I got here. Here $g$ can be smooth, $g(0)=0$ and that's what we know. The step is $$\lim_\limits{x\rightarrow 0^+}\dfrac{x}{\sqrt{4\pi}}\int\limits_0^{t-\delta}\dfrac{1}{(t-s)^{3/2}}e^{-\frac{x^2}{4(t-s)}}g(s) ds+\lim_\limits{x\rightarrow 0^+}\dfrac{x}{\sqrt{4\pi}}\int\limits_{t-\delta}^t\dfrac{1}{(t-s)^{3/2}}e^{-\frac{x^2}{4(t-s)}}g(s) ds$$ $$= g(t) \lim_\limits{x\rightarrow 0^+}\dfrac{x}{\sqrt{4\pi}}\int\limits_{t-\delta}^t\dfrac{1}{(t-s)^{3/2}}e^{-\frac{x^2}{4(t-s)}}ds$$ So this is what I think: since we have excluded $t$ from the first integral, we can simply substitute $x$ by $0$ and so the first limit is $0$. Now this is not true for the second limit, as the integral explodes close to $0$. But how can $g$ leave the integral? Is it some argument of continuity? I tried integration by parts but I don't seem to go anywhere. Thank you very much for your attention!",,['analysis']
95,"Evaluate $\lim s_n$ if $a_1=1$, $a_{n+1}=\frac1{s_n}-\sqrt2$, $s_n=a_1+\cdots+a_n$","Evaluate  if , ,",\lim s_n a_1=1 a_{n+1}=\frac1{s_n}-\sqrt2 s_n=a_1+\cdots+a_n,"Let, $\{a_n\}$ be a sequence be defined by $a_1=1$ and $\displaystyle a_{n+1}=\frac{1}{a_1+a_2+\cdots +a_n}-\sqrt 2$ , for $n\ge 2$. Then find the sum of the series $\displaystyle \sum_{n=1}^{\infty}a_n$. To find the required sum we have to find out the limit of $\{a_n\}$. But I couldn't find any recurrence relation which helps. Can you give any idea or some hint?","Let, $\{a_n\}$ be a sequence be defined by $a_1=1$ and $\displaystyle a_{n+1}=\frac{1}{a_1+a_2+\cdots +a_n}-\sqrt 2$ , for $n\ge 2$. Then find the sum of the series $\displaystyle \sum_{n=1}^{\infty}a_n$. To find the required sum we have to find out the limit of $\{a_n\}$. But I couldn't find any recurrence relation which helps. Can you give any idea or some hint?",,"['real-analysis', 'sequences-and-series', 'analysis', 'limits']"
96,Complete and incomplete norm in a vector space,Complete and incomplete norm in a vector space,,"Let $X$ be a plain vector space . Since we can always equip $X$ with a norm, Can we always equip $X$ with a complete norm? Can we always equip $X$ with an incomplete norm?","Let be a plain vector space . Since we can always equip with a norm, Can we always equip with a complete norm? Can we always equip with an incomplete norm?",X X X X,"['linear-algebra', 'functional-analysis', 'analysis']"
97,Why this integral $\int_0^te^{41s^2+40s}ds$ is bounded by $\frac{e^{41t^2+40t}}{t}$?,Why this integral  is bounded by ?,\int_0^te^{41s^2+40s}ds \frac{e^{41t^2+40t}}{t},"This is a bound without proof I find in a published paper. Let $g(t)= e^{41t^2+40t}n^{-\frac{1}{6}}$ when $t\le 1$, and $g(t)=\frac{e^{41t^2+40t}}{t}n^{-\frac{1}{6}}$ when $t>1$. $a\pm b$ means $[a-b,a+b]$. How to show $\pm n^{\frac{11}{6}}\int_0^te^{41s^2+40s}ds\pm n^{\frac{5}{3}}\subseteq \pm n^2g(t)$ when $0\le t\le \sqrt{\log n}$? And since this an asymptotic bound, we may assume $n$ is large enough. For $t\le 1$ part, it seems we can use $n^{\frac{11}{6}}\int_0^te^{41s^2+40s}ds\le n^{\frac{11}{6}}e^{41t^2+40t}\cdot1=n^2g(t)$. But I have no idea for larger $t$.","This is a bound without proof I find in a published paper. Let $g(t)= e^{41t^2+40t}n^{-\frac{1}{6}}$ when $t\le 1$, and $g(t)=\frac{e^{41t^2+40t}}{t}n^{-\frac{1}{6}}$ when $t>1$. $a\pm b$ means $[a-b,a+b]$. How to show $\pm n^{\frac{11}{6}}\int_0^te^{41s^2+40s}ds\pm n^{\frac{5}{3}}\subseteq \pm n^2g(t)$ when $0\le t\le \sqrt{\log n}$? And since this an asymptotic bound, we may assume $n$ is large enough. For $t\le 1$ part, it seems we can use $n^{\frac{11}{6}}\int_0^te^{41s^2+40s}ds\le n^{\frac{11}{6}}e^{41t^2+40t}\cdot1=n^2g(t)$. But I have no idea for larger $t$.",,"['calculus', 'integration', 'analysis']"
98,Subderivatives of a convex function on a convex set might not form a convex set,Subderivatives of a convex function on a convex set might not form a convex set,,"The set of subdifferentials on a single point forms a convex set (can be easily shown by using half-space arguments). However, the collection of subdifferentials on a set might not form a convex set, even if the function and set are convex. To make it more concrete, let $f:\mathbb R^n \to \mathbb R$ be a convex function, and $E \subset \mathbb R^n$ be an open convex set. Is there a counterexample showing that $\partial f(E):= \{g | g \in \partial f(x) \text { for some $x \in E$}\}$ is not a convex set? There is an obvious counterexample if convexity condition on $E$ is dropped, by considering $f(x) = |x|$ on $\mathbb R$ with $E = \mathbb R \setminus \{0\}$. However, with convexity condition on a domain, creating a counterexample seems extremely difficult.","The set of subdifferentials on a single point forms a convex set (can be easily shown by using half-space arguments). However, the collection of subdifferentials on a set might not form a convex set, even if the function and set are convex. To make it more concrete, let $f:\mathbb R^n \to \mathbb R$ be a convex function, and $E \subset \mathbb R^n$ be an open convex set. Is there a counterexample showing that $\partial f(E):= \{g | g \in \partial f(x) \text { for some $x \in E$}\}$ is not a convex set? There is an obvious counterexample if convexity condition on $E$ is dropped, by considering $f(x) = |x|$ on $\mathbb R$ with $E = \mathbb R \setminus \{0\}$. However, with convexity condition on a domain, creating a counterexample seems extremely difficult.",,"['real-analysis', 'geometry', 'functional-analysis', 'analysis', 'convex-analysis']"
99,Prove that every non-constant complex power series with infinite radius of convergence is unbounded,Prove that every non-constant complex power series with infinite radius of convergence is unbounded,,"I know and can prove that any non-constant polynomials are unbounded, and then I was wondering whether this is true for power series (like infinite degree polynomials). For expamle this is true for the exponential function (when expressed as a power series). After some research, I know that this is true by Liouville's theorem via the Cauchy's integral formula. But is there any simpler way of proving this result?","I know and can prove that any non-constant polynomials are unbounded, and then I was wondering whether this is true for power series (like infinite degree polynomials). For expamle this is true for the exponential function (when expressed as a power series). After some research, I know that this is true by Liouville's theorem via the Cauchy's integral formula. But is there any simpler way of proving this result?",,"['complex-analysis', 'analysis']"
