,title_raw,title_text,title_latex,body_raw,body_text,body_latex,tags
0,Metal Ball Cage Template Cardinality: A Brilliantly Lazy PROOF,Metal Ball Cage Template Cardinality: A Brilliantly Lazy PROOF,,"N.B. - I'm looking for the simplest way to ascertain the number of templates $T$ (see below) comprising the structure from just one angle alone; that is, I'm sitting down looking up at this thing, and I want a way to compute its cardinality based on the simplest methods, but perhaps relying on some underlying abstract concept, in particular, graph theory, geodesics, topology, algebra, or something totally new. My working theory is that we need only the equation $$V-E+F=2,$$ and the Inclusion-Exclusion Principle. In fact, we might also need the fact that the star is a fraction of the whole, and some other symmetry to rely on to create a system of equations. That is $$\frac{V}{n}-\frac{E}{n}+\frac{F}{n}=2$$ and $$\frac{V}{m}-\frac{E}{m}+\frac{F}{m}=2,$$ where we know the relationship between $m$ and $n$. Actually, that makes no sense... Hmm... UPDATE: I have verified, with sufficient effort, Mr. Narain's proposal that the structure is a snub dodecahedron: There are indeed 60 pieces, however, I'd still like a lazy method using the ideas I've alluded to all throughout this post... I'm at a fancy restaurant, and I saw these template ball lights: http://tinypic.com/r/2co6b9w/5 I'm trying to figure out the number of template pieces, call them $T$; they look like this: http://tinypic.com/r/29cs8cw/5 Here is my approach: Count the number of things that look like this: http://tinypic.com/r/wmnssn/5 Now notice that for each $T$ coming out from the center there are four legs which the rest are connected to, two of which are connected to adjacent $T$'s which are coming out from the same center mentioned before. I feel this problem is one of algebra. My friend here thinks that if you measured the shape of $T$, then you could find it easily with the surface area of a sphere, but she can't seem to work out how to get the number of $T$'s. If you need more photos, let me know. Just in case you don't see it: This appears to be a snub dodecahedron--as was pointed out by one of the commenters--and can be seen in an overlay here: Look at this: I believe these two can be related in a system of equations via the Euler Characteristic--perhaps I'd need a third distinct shape... Here are some statistics: Based on these statistics, here is the template--just in case you want to make one for yourself: If you pay me $50 I'll make a larger one for you out of balsa wood . ^_^ THE BIG IDEA: Assume I am a tree--maybe I'm an African Baobab, and my Baobab friend next to me has this thing dangling motionlessly from her branches. Me being an Baobab, I don't know about snub dodecahedrons, but--for some genetically mutative reason--I know a bit of mathematics. So, now, I'm looking at this thing wondering if I can count how many $T$'s there are (see above) just by noting how the arms of the $T$'s are connected. What is the least amount of data that I need from my single, grounded point of view to ascertain the number of the $T$'s comprising this object?","N.B. - I'm looking for the simplest way to ascertain the number of templates $T$ (see below) comprising the structure from just one angle alone; that is, I'm sitting down looking up at this thing, and I want a way to compute its cardinality based on the simplest methods, but perhaps relying on some underlying abstract concept, in particular, graph theory, geodesics, topology, algebra, or something totally new. My working theory is that we need only the equation $$V-E+F=2,$$ and the Inclusion-Exclusion Principle. In fact, we might also need the fact that the star is a fraction of the whole, and some other symmetry to rely on to create a system of equations. That is $$\frac{V}{n}-\frac{E}{n}+\frac{F}{n}=2$$ and $$\frac{V}{m}-\frac{E}{m}+\frac{F}{m}=2,$$ where we know the relationship between $m$ and $n$. Actually, that makes no sense... Hmm... UPDATE: I have verified, with sufficient effort, Mr. Narain's proposal that the structure is a snub dodecahedron: There are indeed 60 pieces, however, I'd still like a lazy method using the ideas I've alluded to all throughout this post... I'm at a fancy restaurant, and I saw these template ball lights: http://tinypic.com/r/2co6b9w/5 I'm trying to figure out the number of template pieces, call them $T$; they look like this: http://tinypic.com/r/29cs8cw/5 Here is my approach: Count the number of things that look like this: http://tinypic.com/r/wmnssn/5 Now notice that for each $T$ coming out from the center there are four legs which the rest are connected to, two of which are connected to adjacent $T$'s which are coming out from the same center mentioned before. I feel this problem is one of algebra. My friend here thinks that if you measured the shape of $T$, then you could find it easily with the surface area of a sphere, but she can't seem to work out how to get the number of $T$'s. If you need more photos, let me know. Just in case you don't see it: This appears to be a snub dodecahedron--as was pointed out by one of the commenters--and can be seen in an overlay here: Look at this: I believe these two can be related in a system of equations via the Euler Characteristic--perhaps I'd need a third distinct shape... Here are some statistics: Based on these statistics, here is the template--just in case you want to make one for yourself: If you pay me $50 I'll make a larger one for you out of balsa wood . ^_^ THE BIG IDEA: Assume I am a tree--maybe I'm an African Baobab, and my Baobab friend next to me has this thing dangling motionlessly from her branches. Me being an Baobab, I don't know about snub dodecahedrons, but--for some genetically mutative reason--I know a bit of mathematics. So, now, I'm looking at this thing wondering if I can count how many $T$'s there are (see above) just by noting how the arms of the $T$'s are connected. What is the least amount of data that I need from my single, grounded point of view to ascertain the number of the $T$'s comprising this object?",,"['abstract-algebra', 'general-topology', 'graph-theory', 'connectedness', 'geodesic']"
1,Prove that if for all $aba=bab$ then $|G|=1$.,Prove that if for all  then .,aba=bab |G|=1,"Let $G$ be a group such that for all $a,b \in G$ we have $aba=bab$. Prove that $|G|=1$. So I have to show that $G =\left\{e \right\} $. Because for any $a,b \in G$ we have $aba=bab$, let $b=e$. Then $aea=eae$ so $a^2 = a$ hence $a = e$. Because $a$ is any we have $G=\left\{e \right\} $. Does it work?","Let $G$ be a group such that for all $a,b \in G$ we have $aba=bab$. Prove that $|G|=1$. So I have to show that $G =\left\{e \right\} $. Because for any $a,b \in G$ we have $aba=bab$, let $b=e$. Then $aea=eae$ so $a^2 = a$ hence $a = e$. Because $a$ is any we have $G=\left\{e \right\} $. Does it work?",,"['abstract-algebra', 'group-theory']"
2,Finite groups with periodic cohomology,Finite groups with periodic cohomology,,"I'm trying to understand Chapter 12, Section 11 in Cartan + Eilenberg's Homological Algebra , which concerns finite groups with periodic cohomology. Unfortunately I am jumping right to this section in the book (I've been working in Serre's Local Fields , and I'm doing the exercise at the end of Chapter 8, Section 4, which refers to the above section in Cartan + Eilenberg), so I'm a little disconcerted with the notation change (and things like using $(\Pi:1)$ for the order of the group $\Pi$ - what is up with that?) I suppose I have two main questions. How do we know that, given a finite group $G$ with periodic cohomology, say with $${\widehat{H}}{}^n(G,A)\cong\!\!\!{\widehat{H}}{}^{n+q}(G,A)$$ for all $n\in\mathbb{Z}$ for some $q\in\mathbb{N}$, these isomorphisms must be given by cup-producting with a fixed element $g\in\widehat{H}{}^q(G,\mathbb{Z})$? This seems to be an implicit assumption in their investigation, and while I can very well believe that it's true (the cup-product satisfies some universal property, if I understand correctly), I don't see what's barring the isomorphisms from being ""accidental"". The fact that the period $q$ is necessarily even (unless $G$ is trivial in which case $q=1$) seems very mysterious to me. Of course, this is the key property for the exercise I'm doing (defining a generalization of the Herbrand quotient), so I would like to have a firm grasp of why it's true. I can more or less follow the reasoning in Cartan + Eilenberg for why this is true, but it's just a proof by contradiction by making a computation using the cup product, and using the fact that $\mathbb{Z}/2\mathbb{Z}$ has periodic cohomology with even period. Furthermore, I again am not seeing why cup-producting with an element of $\widehat{H}{}^q(G,\mathbb{Z})$ is necessarily involved. So, are there any more intuitive explanations of why group cohomology, if it is periodic, has even period? Are there any references other than Cartan + Eilenberg I can look at for this fact?","I'm trying to understand Chapter 12, Section 11 in Cartan + Eilenberg's Homological Algebra , which concerns finite groups with periodic cohomology. Unfortunately I am jumping right to this section in the book (I've been working in Serre's Local Fields , and I'm doing the exercise at the end of Chapter 8, Section 4, which refers to the above section in Cartan + Eilenberg), so I'm a little disconcerted with the notation change (and things like using $(\Pi:1)$ for the order of the group $\Pi$ - what is up with that?) I suppose I have two main questions. How do we know that, given a finite group $G$ with periodic cohomology, say with $${\widehat{H}}{}^n(G,A)\cong\!\!\!{\widehat{H}}{}^{n+q}(G,A)$$ for all $n\in\mathbb{Z}$ for some $q\in\mathbb{N}$, these isomorphisms must be given by cup-producting with a fixed element $g\in\widehat{H}{}^q(G,\mathbb{Z})$? This seems to be an implicit assumption in their investigation, and while I can very well believe that it's true (the cup-product satisfies some universal property, if I understand correctly), I don't see what's barring the isomorphisms from being ""accidental"". The fact that the period $q$ is necessarily even (unless $G$ is trivial in which case $q=1$) seems very mysterious to me. Of course, this is the key property for the exercise I'm doing (defining a generalization of the Herbrand quotient), so I would like to have a firm grasp of why it's true. I can more or less follow the reasoning in Cartan + Eilenberg for why this is true, but it's just a proof by contradiction by making a computation using the cup product, and using the fact that $\mathbb{Z}/2\mathbb{Z}$ has periodic cohomology with even period. Furthermore, I again am not seeing why cup-producting with an element of $\widehat{H}{}^q(G,\mathbb{Z})$ is necessarily involved. So, are there any more intuitive explanations of why group cohomology, if it is periodic, has even period? Are there any references other than Cartan + Eilenberg I can look at for this fact?",,"['abstract-algebra', 'homology-cohomology', 'group-cohomology']"
3,Examples of classes $\mathcal{C}$ of structures such that every finite group is isomorphic to the automorphism group of a structure in $\mathcal{C}$,Examples of classes  of structures such that every finite group is isomorphic to the automorphism group of a structure in,\mathcal{C} \mathcal{C},"Since it is not the case that every group is the automorphism group of a group (see Is every group the automorphism group of a group? ), it is natural to ask: what are some examples of classes $\mathcal{C}$ of structures such that for each finite group $G$, there exists a structure $C$ of class $\mathcal{C}$ such that $\text{Aut}(C) \cong G$? As discussed in Peter Cameron's Automorphisms of graphs , a class $\mathcal{C}$ of structures is said to be universal if every finite group is the automorphism group of a structure in $\mathcal{C}$. As indicated in this article, the following classes of structures are universal: $\bullet$ The class of graphs ( Frucht's theorem ); $\bullet$ The class of trivalent graphs; $\bullet$ The class of graphs of valency $k$ for fixed $k > 2$; $\bullet$ The class of bipartite graphs; $\bullet$ The class of strongly regular graphs ; $\bullet$ The class of Hamiltonian graphs ; $\bullet$ The class of $k$-connected graphs for $k \in \mathbb{N}$; $\bullet$ The class of $k$-chromatic graphs for $k > 1$; $\bullet$ The class of finite distributive lattices ; $\bullet$ Switching classes of graphs ; $\bullet$ The class of projective planes ; $\bullet$ The class of Steiner triple systems ; and $\bullet$ The class of balanced incomplete block designs . It is also known that: $\bullet$ The class of matroids is universal, as shown in the article On the automorphism group of a matroid ; $\bullet$ The class of finite posets is universal, as shown in the article Automorphism groups of finite posets ; and $\bullet$ The class of complete, connected, locally connected metric spaces of any fixed positive dimension is universal, as discussed in the following link: Automorphism group of a topological space ; $\bullet$ The class of directed acyclic graphs is universal, as discussed in the following link: Can any finite group be realized as the automorphism group of a directed acyclic graph? ; and $\bullet$ The class of finite orthomodular lattices is universal, as proven in the article Every finite group is the automorphism group of some finite orthomodular lattice . Observe that most of the universal classes given above are classes of combinatorial / discrete structures as opposed to algebraic structures defined in terms of binary operations such as monoids and rings, or geometric structures such as manifolds. It is natural to ask: (1) What are some other interesting examples of universal classes of structures? (2) Are there any known examples of universal classes of 'algebraic' structures, i.e. structures endowed with at least one binary operation satisfying certain axioms? Is the class of rings universal? Is the class of monoids universal? Is the class of semigroups universal? (3) Are there any known examples of universal classes of 'geometric' structures, e.g., structures such as smooth manifolds? (4) What are some interesting examples of classes of structures which are known to be non-universal? As shown by Polya, one such example is the class of trees. It is also known that the class of planar graphs is not universal. Also, it is known that any minor-closed class of graphs is not universal.","Since it is not the case that every group is the automorphism group of a group (see Is every group the automorphism group of a group? ), it is natural to ask: what are some examples of classes $\mathcal{C}$ of structures such that for each finite group $G$, there exists a structure $C$ of class $\mathcal{C}$ such that $\text{Aut}(C) \cong G$? As discussed in Peter Cameron's Automorphisms of graphs , a class $\mathcal{C}$ of structures is said to be universal if every finite group is the automorphism group of a structure in $\mathcal{C}$. As indicated in this article, the following classes of structures are universal: $\bullet$ The class of graphs ( Frucht's theorem ); $\bullet$ The class of trivalent graphs; $\bullet$ The class of graphs of valency $k$ for fixed $k > 2$; $\bullet$ The class of bipartite graphs; $\bullet$ The class of strongly regular graphs ; $\bullet$ The class of Hamiltonian graphs ; $\bullet$ The class of $k$-connected graphs for $k \in \mathbb{N}$; $\bullet$ The class of $k$-chromatic graphs for $k > 1$; $\bullet$ The class of finite distributive lattices ; $\bullet$ Switching classes of graphs ; $\bullet$ The class of projective planes ; $\bullet$ The class of Steiner triple systems ; and $\bullet$ The class of balanced incomplete block designs . It is also known that: $\bullet$ The class of matroids is universal, as shown in the article On the automorphism group of a matroid ; $\bullet$ The class of finite posets is universal, as shown in the article Automorphism groups of finite posets ; and $\bullet$ The class of complete, connected, locally connected metric spaces of any fixed positive dimension is universal, as discussed in the following link: Automorphism group of a topological space ; $\bullet$ The class of directed acyclic graphs is universal, as discussed in the following link: Can any finite group be realized as the automorphism group of a directed acyclic graph? ; and $\bullet$ The class of finite orthomodular lattices is universal, as proven in the article Every finite group is the automorphism group of some finite orthomodular lattice . Observe that most of the universal classes given above are classes of combinatorial / discrete structures as opposed to algebraic structures defined in terms of binary operations such as monoids and rings, or geometric structures such as manifolds. It is natural to ask: (1) What are some other interesting examples of universal classes of structures? (2) Are there any known examples of universal classes of 'algebraic' structures, i.e. structures endowed with at least one binary operation satisfying certain axioms? Is the class of rings universal? Is the class of monoids universal? Is the class of semigroups universal? (3) Are there any known examples of universal classes of 'geometric' structures, e.g., structures such as smooth manifolds? (4) What are some interesting examples of classes of structures which are known to be non-universal? As shown by Polya, one such example is the class of trees. It is also known that the class of planar graphs is not universal. Also, it is known that any minor-closed class of graphs is not universal.",,"['abstract-algebra', 'group-theory', 'discrete-mathematics', 'graph-theory']"
4,A group acting on functions of functions of functions,A group acting on functions of functions of functions,,"Given a group acting on a set $X$, there is a standard way to define an action of the group on the set of functions of $X$. This can be extended to the set of functions of functions of $X$ as I show below, and it can also be extended to functions of functions of functions (and of course to higher orders beyond that). However, for the latter (and higher) cases, I can't see a way to write out the action explicitly in terms of function compositions, rather than in terms of the action on ""lower-order"" functions. My question is about whether it's possible to do this. The following paragraphs present the problem in greater detail. Consider a group $G$ acting upon a set $X$. If we consider the set $A$ of functions $a:X\to P$ for some set $P$, there is a natural action of $G$ upon $A$ given by $(g.a)(x) = a(g^{-1}.x)$ for all $g\in G$, $a\in A$, $x\in X$. Here the period '$.$' is used to represent both the action of $G$ upon $X$ and the action of $G$ upon $A$. As a concrete example, let $X$ be the set of faces of a cube and $G$ be its group of rotational symmetries. Then $A$ can be thought of as the set of colourings of the cube's faces, with $P$ being the set of colours. If we write the action of $G$ upon $X$ as $g(x)$ instead of $g.x$ then we can write the action of $G$ upon $F$ as $g.a = a\circ g^{-1}$. This is useful because it allows us to eliminate $x$ from the notation, and allows us to think in terms of function composition rather than the more abstract notion of a group action. Let us now consider the set $B$ of functions of functions of $X$, that is, the set of functions $b:A\to Q$ for some set $Q$. An example might be a functiom that counts the number of blue faces that are adjacent to red faces. We want to define a natural action of $G$ upon $B$. Since we already have an action of $G$ on $A$ we can apply the same trick again and write $(g.b)(a) = b(g^{-1}.a)$, for all $a\in A$, $b\in B$, $g\in G$. In terms of funtion composition this becomes  $(g.b)(a) = b(a\circ g)$, but I can't see an obvious way to eliminate $a$ from the notation as we were able to do with $x$ above. Finally, let us consider the set $C$ of functions $c:B\to R$ for some set $R$. That is, functions of functions of functions of $X$. As before we can write $(g.c)(b) = c(g^{-1}.b)$. However, what I can't see is how to write out this action explicitly in terms of function composition, rather than in terms of the action on $B$. That is, I want to get rid of the '$.$' in the right-hand side of this equation, but I can't see a way to do it. My question is whether it is possible to do this, and if so, how. If it can be done for functions of functions of functions, can it also be done for functions of functions of functions of functions, etc.?","Given a group acting on a set $X$, there is a standard way to define an action of the group on the set of functions of $X$. This can be extended to the set of functions of functions of $X$ as I show below, and it can also be extended to functions of functions of functions (and of course to higher orders beyond that). However, for the latter (and higher) cases, I can't see a way to write out the action explicitly in terms of function compositions, rather than in terms of the action on ""lower-order"" functions. My question is about whether it's possible to do this. The following paragraphs present the problem in greater detail. Consider a group $G$ acting upon a set $X$. If we consider the set $A$ of functions $a:X\to P$ for some set $P$, there is a natural action of $G$ upon $A$ given by $(g.a)(x) = a(g^{-1}.x)$ for all $g\in G$, $a\in A$, $x\in X$. Here the period '$.$' is used to represent both the action of $G$ upon $X$ and the action of $G$ upon $A$. As a concrete example, let $X$ be the set of faces of a cube and $G$ be its group of rotational symmetries. Then $A$ can be thought of as the set of colourings of the cube's faces, with $P$ being the set of colours. If we write the action of $G$ upon $X$ as $g(x)$ instead of $g.x$ then we can write the action of $G$ upon $F$ as $g.a = a\circ g^{-1}$. This is useful because it allows us to eliminate $x$ from the notation, and allows us to think in terms of function composition rather than the more abstract notion of a group action. Let us now consider the set $B$ of functions of functions of $X$, that is, the set of functions $b:A\to Q$ for some set $Q$. An example might be a functiom that counts the number of blue faces that are adjacent to red faces. We want to define a natural action of $G$ upon $B$. Since we already have an action of $G$ on $A$ we can apply the same trick again and write $(g.b)(a) = b(g^{-1}.a)$, for all $a\in A$, $b\in B$, $g\in G$. In terms of funtion composition this becomes  $(g.b)(a) = b(a\circ g)$, but I can't see an obvious way to eliminate $a$ from the notation as we were able to do with $x$ above. Finally, let us consider the set $C$ of functions $c:B\to R$ for some set $R$. That is, functions of functions of functions of $X$. As before we can write $(g.c)(b) = c(g^{-1}.b)$. However, what I can't see is how to write out this action explicitly in terms of function composition, rather than in terms of the action on $B$. That is, I want to get rid of the '$.$' in the right-hand side of this equation, but I can't see a way to do it. My question is whether it is possible to do this, and if so, how. If it can be done for functions of functions of functions, can it also be done for functions of functions of functions of functions, etc.?",,"['abstract-algebra', 'group-theory', 'discrete-mathematics']"
5,Closest cyclotomic integer to a cyclotomic number?,Closest cyclotomic integer to a cyclotomic number?,,"Let's take a cyclotomic field of the form $K=\mathbb{Q}(\zeta_n)$ where $\zeta_p$ is the $n$th root of unity. Then the ring of integers of $K$ is $\mathcal{O}_K= \mathbb{Z}(\zeta_n)$. Is there a generalisation of the rounding function $\left \lfloor \cdot \right \rceil: \mathbb{Q} \to \mathbb{Z}$ to some rounding function $\left \lfloor \cdot \right \rceil_K : K \to \mathcal{O}_K $ for cyclotomic fields that rounds a cyclotomic number to its ""nearest"" cyclotomic integer? EDIT: I found something that might be useful. The following definition comes from https://hal.archives-ouvertes.fr/hal-00632997v1/document : Definition : For any $\eta \in K$, the real number $m_K(\eta)= \min_{z \in \mathcal{O}_K}|N_{K/\mathbb{Q}}(\eta - z)|$ is the Euclidean minimum of $\eta$. Does this give us a generalisation of the rounding function, and if so does the ""rounding"" function only hold for Euclidean domains?","Let's take a cyclotomic field of the form $K=\mathbb{Q}(\zeta_n)$ where $\zeta_p$ is the $n$th root of unity. Then the ring of integers of $K$ is $\mathcal{O}_K= \mathbb{Z}(\zeta_n)$. Is there a generalisation of the rounding function $\left \lfloor \cdot \right \rceil: \mathbb{Q} \to \mathbb{Z}$ to some rounding function $\left \lfloor \cdot \right \rceil_K : K \to \mathcal{O}_K $ for cyclotomic fields that rounds a cyclotomic number to its ""nearest"" cyclotomic integer? EDIT: I found something that might be useful. The following definition comes from https://hal.archives-ouvertes.fr/hal-00632997v1/document : Definition : For any $\eta \in K$, the real number $m_K(\eta)= \min_{z \in \mathcal{O}_K}|N_{K/\mathbb{Q}}(\eta - z)|$ is the Euclidean minimum of $\eta$. Does this give us a generalisation of the rounding function, and if so does the ""rounding"" function only hold for Euclidean domains?",,"['abstract-algebra', 'number-theory', 'algebraic-number-theory', 'cyclotomic-fields']"
6,When is $(x^n-1)/(x-1)$ a prime number?,When is  a prime number?,(x^n-1)/(x-1),"Let $x > 1$ and let $n$ be a prime. I'm wondering if a characterization of this is known. That is, what are sufficient and necessary conditions for  $$ \dfrac{x^n-1}{x-1} = 1 + x + x^2 + \cdots + x^{n-1} $$ to be a prime number? What are these conditions if we restrict $x$ to be a power of a prime? Note that $n$ can not be composite since otherwise it is easy to show that so is $(x^n-1)/(x-1)$. Thanks in advance.","Let $x > 1$ and let $n$ be a prime. I'm wondering if a characterization of this is known. That is, what are sufficient and necessary conditions for  $$ \dfrac{x^n-1}{x-1} = 1 + x + x^2 + \cdots + x^{n-1} $$ to be a prime number? What are these conditions if we restrict $x$ to be a power of a prime? Note that $n$ can not be composite since otherwise it is easy to show that so is $(x^n-1)/(x-1)$. Thanks in advance.",,"['abstract-algebra', 'elementary-number-theory', 'cyclotomic-polynomials']"
7,On solvable quintics and septics,On solvable quintics and septics,,"Here is a nice sufficient (but not necessary) condition on whether a quintic is solvable in radicals or not.  Given, $x^5+10cx^3+10dx^2+5ex+f = 0\tag{1}$ If there is an ordering of its roots such that, $x_1 x_2 + x_2 x_3 + x_3 x_4 + x_4 x_5 + x_5 x_1 - (x_1 x_3 + x_3 x_5 + x_5 x_2 + x_2 x_4 + x_4 x_1) = 0\tag{2}$ or alternatively, its coefficients are related by the quadratic in f , $(c^3 + d^2 - c e) \big((5 c^2 - e)^2 + 16 c d^2\big) = (c^2 d + d e - c f)^2 \tag{3}$ then (1) is solvable.  This also implies that if $c\neq0$, then it has a solvable twin , $x^5+10cx^3+10dx^2+5ex+f' = 0\tag{4}$ where $f'$ is the other root of (3). The Lagrange resolvent are the roots of, $z^4+fz^3+(2c^5-5c^3e-4d^2e+ce^2+2cdf)z^2-c^5fz+c^{10} = 0\tag{5}$ so, $x = z_1^{1/5}+z_2^{1/5}+z_3^{1/5}+z_4^{1/5}\tag{6}$ Two questions though: I. Does the septic (7th deg) analogue, $x_1 x_2 + x_2 x_3 + \dots + x_7 x_1 – (x_1 x_3 + x_3 x_5 + \dots + x_6 x_1) = 0\tag{7}$ imply such a septic is solvable? II. The septic has a $5! = 120$-deg resolvent. While this is next to impossible to explicitly construct, is it feasible to construct just the constant term? Equating it to zero would then imply a family of solvable septics, just like (3) above. More details and examples for (2) like the Emma Lehmer quintic in my blog .","Here is a nice sufficient (but not necessary) condition on whether a quintic is solvable in radicals or not.  Given, $x^5+10cx^3+10dx^2+5ex+f = 0\tag{1}$ If there is an ordering of its roots such that, $x_1 x_2 + x_2 x_3 + x_3 x_4 + x_4 x_5 + x_5 x_1 - (x_1 x_3 + x_3 x_5 + x_5 x_2 + x_2 x_4 + x_4 x_1) = 0\tag{2}$ or alternatively, its coefficients are related by the quadratic in f , $(c^3 + d^2 - c e) \big((5 c^2 - e)^2 + 16 c d^2\big) = (c^2 d + d e - c f)^2 \tag{3}$ then (1) is solvable.  This also implies that if $c\neq0$, then it has a solvable twin , $x^5+10cx^3+10dx^2+5ex+f' = 0\tag{4}$ where $f'$ is the other root of (3). The Lagrange resolvent are the roots of, $z^4+fz^3+(2c^5-5c^3e-4d^2e+ce^2+2cdf)z^2-c^5fz+c^{10} = 0\tag{5}$ so, $x = z_1^{1/5}+z_2^{1/5}+z_3^{1/5}+z_4^{1/5}\tag{6}$ Two questions though: I. Does the septic (7th deg) analogue, $x_1 x_2 + x_2 x_3 + \dots + x_7 x_1 – (x_1 x_3 + x_3 x_5 + \dots + x_6 x_1) = 0\tag{7}$ imply such a septic is solvable? II. The septic has a $5! = 120$-deg resolvent. While this is next to impossible to explicitly construct, is it feasible to construct just the constant term? Equating it to zero would then imply a family of solvable septics, just like (3) above. More details and examples for (2) like the Emma Lehmer quintic in my blog .",,"['abstract-algebra', 'group-theory', 'galois-theory']"
8,Proof $\pi$ is transcendental without symmetric function theory,Proof  is transcendental without symmetric function theory,\pi,"Recently for a bonus homework assignment in my algebra class, I was asked to review the literature and write up a proof that $\pi$ is transcendental. Essentially every source I found (""The Transcendence of $\pi$ "" by Steve Mayer for example) presents the classic proof of Lindemann, which heavily relies on symmetric function theory and in particular the fundamental theorem of elementary symmetric functions. Before this assignment, I did not know symmetric function theory, and by far the biggest difficulty in my solution and write up was understanding this theory and the argument used in the proof (which in my opinion, was not spelled out enough for a beginner to the theory to easily understand the argument in the sources I consulted). Now, symmetric function theory was useful to learn, and I am aware it is very useful in the proof of the Lindemann-Weierstrass theorem, but it begs the question: Is there any proof (preferably understandable to approximately a beginning graduate student) that $\pi$ is transcendental, without using symmetric function theory, and if not, is there a theoretical explanation for why? Edit: Crossposted to MO after 2.5 weeks https://mathoverflow.net/questions/466288/proof-pi-is-transcendental-without-symmetric-function-theory","Recently for a bonus homework assignment in my algebra class, I was asked to review the literature and write up a proof that is transcendental. Essentially every source I found (""The Transcendence of "" by Steve Mayer for example) presents the classic proof of Lindemann, which heavily relies on symmetric function theory and in particular the fundamental theorem of elementary symmetric functions. Before this assignment, I did not know symmetric function theory, and by far the biggest difficulty in my solution and write up was understanding this theory and the argument used in the proof (which in my opinion, was not spelled out enough for a beginner to the theory to easily understand the argument in the sources I consulted). Now, symmetric function theory was useful to learn, and I am aware it is very useful in the proof of the Lindemann-Weierstrass theorem, but it begs the question: Is there any proof (preferably understandable to approximately a beginning graduate student) that is transcendental, without using symmetric function theory, and if not, is there a theoretical explanation for why? Edit: Crossposted to MO after 2.5 weeks https://mathoverflow.net/questions/466288/proof-pi-is-transcendental-without-symmetric-function-theory",\pi \pi \pi,"['abstract-algebra', 'number-theory', 'pi', 'transcendental-numbers']"
9,On a ring $R$ such that every subring of $R$ is an ideal .,On a ring  such that every subring of  is an ideal .,R R,"$\mathbf {The \ Problem \ is}:$ Give an example of a non-commutative ring $R$ (which may or may not contain the identity) such that every subring of $R$ is an ideal . $\mathbf {My \ approach} :$ I found a proof of a problem that if a ring $R$ contains no divisors of $0$ and every subring of $R$ is an ideal, then $R$ is commutative . Again if $R$ has an identity and it satisfies the above stated property, then $R$ is either the ""zero ring"" $\{0\}$ , $\mathbb Z$ or $\mathbb Z_n$ under the criterion that every subring of $R$ must contain the identity of $R .$ And, for any group $(R , +)$ , if we define the multiplication operation such that $ab =0$ for all $a, b$ in $R$ , then also the criterion would have been satisfied without the requirement of having an identity . But, I tried some subrings of the matrix groups but failed .","Give an example of a non-commutative ring (which may or may not contain the identity) such that every subring of is an ideal . I found a proof of a problem that if a ring contains no divisors of and every subring of is an ideal, then is commutative . Again if has an identity and it satisfies the above stated property, then is either the ""zero ring"" , or under the criterion that every subring of must contain the identity of And, for any group , if we define the multiplication operation such that for all in , then also the criterion would have been satisfied without the requirement of having an identity . But, I tried some subrings of the matrix groups but failed .","\mathbf {The \ Problem \ is}: R R \mathbf {My \ approach} : R 0 R R R R \{0\} \mathbb Z \mathbb Z_n R R . (R , +) ab =0 a, b R","['abstract-algebra', 'ring-theory', 'ideals']"
10,Show that $x^3-x^2+8=y^2$ has no integer solution,Show that  has no integer solution,x^3-x^2+8=y^2,"Show that $x^3-x^2+8=y^2$ has no integer solution. I spent several hours on this problem but I couldn't figure out how to solve it. A hint from the professor was to find a proper field where to find the solutions, or use Gaussian integers, but I still can't find the proof. I only showed that $y$ must be even, and I tried to factorize $(x^3+8)$ and equalize the two members with $x^2+y^2$, but nothing came out of it. This problem was given at the Competencia Interuniversitaria Matemática Argentina (CIMA).","Show that $x^3-x^2+8=y^2$ has no integer solution. I spent several hours on this problem but I couldn't figure out how to solve it. A hint from the professor was to find a proper field where to find the solutions, or use Gaussian integers, but I still can't find the proof. I only showed that $y$ must be even, and I tried to factorize $(x^3+8)$ and equalize the two members with $x^2+y^2$, but nothing came out of it. This problem was given at the Competencia Interuniversitaria Matemática Argentina (CIMA).",,"['abstract-algebra', 'number-theory', 'diophantine-equations']"
11,Subgroup of free group on two elements free of rank $6$.,Subgroup of free group on two elements free of rank .,6,"Let $F$ be the free group on two elements $x$, $y$. To make our lives easier, we denote $x^{-1}$ by $X$ and $y^{-1}$ by $Y$. Let $g_i$ for $i = 1, \ldots, 6$ be the following elements:$$g_1 = xyxY,$$$$g_2 = XYXyxYxyXyxYXYXyxYxyxyXYY,$$$$g_3 = XYXyx,$$$$g_4 = YxyXyxYXYXyyx,$$$$g_5 = YxyXYxyxyX,$$$$g_6 = xxYXXy.$$Let $G$ be the subgroup of $F$ generated by the $g_i$. Question. How do I see that $G$ is free of rank $6$?","Let $F$ be the free group on two elements $x$, $y$. To make our lives easier, we denote $x^{-1}$ by $X$ and $y^{-1}$ by $Y$. Let $g_i$ for $i = 1, \ldots, 6$ be the following elements:$$g_1 = xyxY,$$$$g_2 = XYXyxYxyXyxYXYXyxYxyxyXYY,$$$$g_3 = XYXyx,$$$$g_4 = YxyXyxYXYXyyx,$$$$g_5 = YxyXYxyxyX,$$$$g_6 = xxYXXy.$$Let $G$ be the subgroup of $F$ generated by the $g_i$. Question. How do I see that $G$ is free of rank $6$?",,"['abstract-algebra', 'group-theory']"
12,Study of rings of the form $R+I$,Study of rings of the form,R+I,"In my life I saw lots of ways of constructing rings: polynomial rings, quotient rings, localizations, endomorphism rings, rings of fractions, integral closure of a ring, center of a ring, etc... These constructions are very useful to build up exotic counterexamples (for example rings being left noetherian and not right noetherian). In these days I am studying some theory of valuation rings, and my professor showed me a new way of constructing rings. Suppose $R \subseteq S$ is an extension of commutative rings (with unity), and $I$ is an ideal of $S$. Then the following is a subring of $S$:   $$R+I = \{ r+i : r \in R, i \in I\}$$ This construction is used for example to build the ring $\Bbb{Z}+x\Bbb{Q}[[x]]$, which is a local domain of dimension $2$ dominated by $\Bbb{Q}[[x]]$. I found this simple idea very easy to understand and actually it is useful to build up new things. However, I wonder why I never saw it in my life (for example it does not appear in Atiyah-MacDonald, or other books of commutative algebra). My question is: Does this construction have a name? Where can I find some references to study such rings? Is there any reason why it is so poorly considered?","In my life I saw lots of ways of constructing rings: polynomial rings, quotient rings, localizations, endomorphism rings, rings of fractions, integral closure of a ring, center of a ring, etc... These constructions are very useful to build up exotic counterexamples (for example rings being left noetherian and not right noetherian). In these days I am studying some theory of valuation rings, and my professor showed me a new way of constructing rings. Suppose $R \subseteq S$ is an extension of commutative rings (with unity), and $I$ is an ideal of $S$. Then the following is a subring of $S$:   $$R+I = \{ r+i : r \in R, i \in I\}$$ This construction is used for example to build the ring $\Bbb{Z}+x\Bbb{Q}[[x]]$, which is a local domain of dimension $2$ dominated by $\Bbb{Q}[[x]]$. I found this simple idea very easy to understand and actually it is useful to build up new things. However, I wonder why I never saw it in my life (for example it does not appear in Atiyah-MacDonald, or other books of commutative algebra). My question is: Does this construction have a name? Where can I find some references to study such rings? Is there any reason why it is so poorly considered?",,"['abstract-algebra', 'algebraic-geometry', 'ring-theory', 'commutative-algebra']"
13,"The possible degrees of $\mathbb{Q}(a,b)$ in terms of the degrees of $a$ and $b$",The possible degrees of  in terms of the degrees of  and,"\mathbb{Q}(a,b) a b","Let $n,m \geq 1$ be natural numbers. Is there a characterization of those natural numbers $d$ for which there are algebraic numbers $a,b$ of degrees $n,m$ such that $\mathbb{Q}(a,b)$ has degree $d$ over $\mathbb{Q}$? Two necessary conditions are $\mathrm{lcm}(n,m) \mid d$ and $d \leq nm$. (In particular, if $n,m$ are coprime, only $ d=nm$ is possible.) Are they sufficient? Or do we actually have $d \mid nm$? I have chosen $\mathbb{Q}$ just to fix ideas, maybe the same analysis works for any field (of characteristic zero). So an answer treating this more general case is appreciated as well.","Let $n,m \geq 1$ be natural numbers. Is there a characterization of those natural numbers $d$ for which there are algebraic numbers $a,b$ of degrees $n,m$ such that $\mathbb{Q}(a,b)$ has degree $d$ over $\mathbb{Q}$? Two necessary conditions are $\mathrm{lcm}(n,m) \mid d$ and $d \leq nm$. (In particular, if $n,m$ are coprime, only $ d=nm$ is possible.) Are they sufficient? Or do we actually have $d \mid nm$? I have chosen $\mathbb{Q}$ just to fix ideas, maybe the same analysis works for any field (of characteristic zero). So an answer treating this more general case is appreciated as well.",,"['abstract-algebra', 'field-theory', 'extension-field']"
14,In which algebraic theories do 'free' and 'projective' coincide?,In which algebraic theories do 'free' and 'projective' coincide?,,"Free models of algebraic theories are always projective objects in the category of models, but the converse is not always true. For instance, some (actually, all) projective modules are direct summands of free modules, and these need not be free. On the other hand, the two notions do coincide for abelian groups. Hence I'm left wondering: for which kinds of algebraic theories do free and projective coincide?","Free models of algebraic theories are always projective objects in the category of models, but the converse is not always true. For instance, some (actually, all) projective modules are direct summands of free modules, and these need not be free. On the other hand, the two notions do coincide for abelian groups. Hence I'm left wondering: for which kinds of algebraic theories do free and projective coincide?",,"['abstract-algebra', 'category-theory', 'universal-algebra', 'projective-module']"
15,Converse of Chinese Remainder Theorem,Converse of Chinese Remainder Theorem,,"Chinese Remainder Theorem for commutative rings with identity Let $R$ be a commutative ring with identity. If $I, J$ are ideals of $R$ satisfying $I+J=R$, then there is an isomorphism of rings: $$R/(I\cap J) \cong R/I \times R/J.$$ I am interested in the converse of this. I saw the following two cases: Converse V1 If we have positive integers $m, n$ with $(m,n)\neq 1$, then  $$ \mathbb{Z}/mn\mathbb{Z} \not\cong \mathbb{Z}/m\mathbb{Z}\times\mathbb{Z}/n\mathbb{Z}.$$ This one is easy if we consider characteristics. Converse V2 Let $F$ be a field. If we have polynomials $f, g \in F[x]$ with $(f)+(g)\neq F[x]$, then  $$ F[x]/(f(x)g(x))\not\cong F[x]/(f(x))\times F[x]/(g(x)).$$ This one is answered here . The idea is counting number of ideals of both sides. It is easy to see that if we have factorization of ideals into prime ideals (Dedekind Domain), the same idea applies. However, we have this example: Example If $R=\prod_{i=1}^{\infty} \mathbb{Z}$ and $I=J=(0)$, then $I+J\neq R$ and $$ R/(I\cap J) \cong \prod_{i=1}^{\infty} \mathbb{Z} \cong \prod_{i=1}^{\infty} \mathbb{Z}\times \prod_{i=1}^{\infty} \mathbb{Z} \cong R/I\times R/J.$$ Thus, the converse of CRT does not hold in general for ""commutative ring with identity"". We have seen, however, the converse of CRT holds for ""Dedekind Domain"". My question is Quetion Do we have a commutative ring with identity which is not a ""Dedekind Domain"" such that the converse of CRT holds?","Chinese Remainder Theorem for commutative rings with identity Let $R$ be a commutative ring with identity. If $I, J$ are ideals of $R$ satisfying $I+J=R$, then there is an isomorphism of rings: $$R/(I\cap J) \cong R/I \times R/J.$$ I am interested in the converse of this. I saw the following two cases: Converse V1 If we have positive integers $m, n$ with $(m,n)\neq 1$, then  $$ \mathbb{Z}/mn\mathbb{Z} \not\cong \mathbb{Z}/m\mathbb{Z}\times\mathbb{Z}/n\mathbb{Z}.$$ This one is easy if we consider characteristics. Converse V2 Let $F$ be a field. If we have polynomials $f, g \in F[x]$ with $(f)+(g)\neq F[x]$, then  $$ F[x]/(f(x)g(x))\not\cong F[x]/(f(x))\times F[x]/(g(x)).$$ This one is answered here . The idea is counting number of ideals of both sides. It is easy to see that if we have factorization of ideals into prime ideals (Dedekind Domain), the same idea applies. However, we have this example: Example If $R=\prod_{i=1}^{\infty} \mathbb{Z}$ and $I=J=(0)$, then $I+J\neq R$ and $$ R/(I\cap J) \cong \prod_{i=1}^{\infty} \mathbb{Z} \cong \prod_{i=1}^{\infty} \mathbb{Z}\times \prod_{i=1}^{\infty} \mathbb{Z} \cong R/I\times R/J.$$ Thus, the converse of CRT does not hold in general for ""commutative ring with identity"". We have seen, however, the converse of CRT holds for ""Dedekind Domain"". My question is Quetion Do we have a commutative ring with identity which is not a ""Dedekind Domain"" such that the converse of CRT holds?",,"['abstract-algebra', 'ring-theory', 'ideals', 'chinese-remainder-theorem']"
16,Give an example of a noncyclic Abelian group all of whose proper subgroups are cyclic.,Give an example of a noncyclic Abelian group all of whose proper subgroups are cyclic.,,I've tried but I could not find a noncyclic Abelian group all of whose proper subgroups are cyclic. please help me.,I've tried but I could not find a noncyclic Abelian group all of whose proper subgroups are cyclic. please help me.,,"['abstract-algebra', 'abelian-groups', 'cyclic-groups']"
17,Show that a nonabelian group must have at least five distinct elements [closed],Show that a nonabelian group must have at least five distinct elements [closed],,"Closed. This question is off-topic . It is not currently accepting answers. This question is missing context or other details : Please improve the question by providing additional context, which ideally includes your thoughts on the problem and any attempts you have made to solve it. This information helps others identify where you have difficulties and helps them write answers appropriate to your experience level. Closed 7 years ago . Improve this question Show that a nonabelian group must have at least five distinct elements. I just learn abstract algebra by self study. I want help to solve this problem. Just give me a hint.","Closed. This question is off-topic . It is not currently accepting answers. This question is missing context or other details : Please improve the question by providing additional context, which ideally includes your thoughts on the problem and any attempts you have made to solve it. This information helps others identify where you have difficulties and helps them write answers appropriate to your experience level. Closed 7 years ago . Improve this question Show that a nonabelian group must have at least five distinct elements. I just learn abstract algebra by self study. I want help to solve this problem. Just give me a hint.",,"['abstract-algebra', 'group-theory', 'abelian-groups']"
18,Non-unital rings: a few examples,Non-unital rings: a few examples,,"Every ring I've ever heard of is unital, i. e. , contains a (unique) element $a$ such that $xa = ax = x$ for every $x$ in it. However, some rings do not have such an element. What are they? P. S. : one will notice I assumed commutativity. So, for an easier related request, some examples of non-commutative rings would also be appreciated.","Every ring I've ever heard of is unital, i. e. , contains a (unique) element $a$ such that $xa = ax = x$ for every $x$ in it. However, some rings do not have such an element. What are they? P. S. : one will notice I assumed commutativity. So, for an easier related request, some examples of non-commutative rings would also be appreciated.",,"['abstract-algebra', 'ring-theory', 'examples-counterexamples', 'rngs']"
19,Can a non-local ring have only two prime ideals?,Can a non-local ring have only two prime ideals?,,"Can a non-local ring have only two prime ideals? The only way this would be possible is if the ring $R$ had two distinct maximal ideals $\mathfrak{m}$ and $\mathfrak{n}$, and no other prime ideals. I suspect that such a ring exists, though I don't know how to construct it.","Can a non-local ring have only two prime ideals? The only way this would be possible is if the ring $R$ had two distinct maximal ideals $\mathfrak{m}$ and $\mathfrak{n}$, and no other prime ideals. I suspect that such a ring exists, though I don't know how to construct it.",,"['abstract-algebra', 'algebraic-geometry', 'ring-theory', 'commutative-algebra', 'maximal-and-prime-ideals']"
20,Abelian groups axioms with minus in place of plus,Abelian groups axioms with minus in place of plus,,"An abelian group is a set equipped with a binary operation $+$, a unary operation $-$ and a nullary operation (constant) $0$, satisfying certain axioms (associativity, unity, etc). I wonder if it is possible to describe the same structure, that is that of abelian group, using a binary operation $-$ and a constant $0$. I think it is possible by setting $a-b=a+(-b)$ where in the LHS there is the binary $-$ I am defining, while in the RHS we have the usually binary $+$ and the unary ""inverse"" operator $-$. But which should be the axioms for $-$ in order to have an abelian group?","An abelian group is a set equipped with a binary operation $+$, a unary operation $-$ and a nullary operation (constant) $0$, satisfying certain axioms (associativity, unity, etc). I wonder if it is possible to describe the same structure, that is that of abelian group, using a binary operation $-$ and a constant $0$. I think it is possible by setting $a-b=a+(-b)$ where in the LHS there is the binary $-$ I am defining, while in the RHS we have the usually binary $+$ and the unary ""inverse"" operator $-$. But which should be the axioms for $-$ in order to have an abelian group?",,['abstract-algebra']
21,Is there a number $n$ such that there are exactly 1 million abelian groups of order $n$?,Is there a number  such that there are exactly 1 million abelian groups of order ?,n n,"Is there a number $n$ such that there are exactly 1 million abelian groups of order $n$ ? Can anyone please explain. I would yes because numbers are infinitive, and so any number $n$ can be expressed as a direct product of cyclic groups of order $n$ .","Is there a number such that there are exactly 1 million abelian groups of order ? Can anyone please explain. I would yes because numbers are infinitive, and so any number can be expressed as a direct product of cyclic groups of order .",n n n n,"['abstract-algebra', 'group-theory']"
22,In a ring homomorphism we always have $f(1)=1$? [duplicate],In a ring homomorphism we always have ? [duplicate],f(1)=1,"This question already has answers here : Closed 11 years ago . Possible Duplicate: the image of $1$ by a  homomorphism between unitary rings I'm studying the Atiyah's commutative algebra book and I realized that in the beginning of the book, the author says as one of the conditions to a map be a homomorphism is $f(1)=1$, I would like to know if I can left this condition behind, i. e., if $f(x+y)=f(x)+f(y)$ and $f(xy)=f(x)f(y)$ we can have $f(1)=1$. Thanks","This question already has answers here : Closed 11 years ago . Possible Duplicate: the image of $1$ by a  homomorphism between unitary rings I'm studying the Atiyah's commutative algebra book and I realized that in the beginning of the book, the author says as one of the conditions to a map be a homomorphism is $f(1)=1$, I would like to know if I can left this condition behind, i. e., if $f(x+y)=f(x)+f(y)$ and $f(xy)=f(x)f(y)$ we can have $f(1)=1$. Thanks",,"['ring-theory', 'abstract-algebra']"
23,"When we say two fields are isomorphic, does that just mean they are isomophic as rings?","When we say two fields are isomorphic, does that just mean they are isomophic as rings?",,"If we say fields $A$ and $B$ are isomorphic, does that just mean they are isomorphic as rings, or is there something else?","If we say fields and are isomorphic, does that just mean they are isomorphic as rings, or is there something else?",A B,"['abstract-algebra', 'field-theory']"
24,Shorter proof of $R/I$ is a field if and only if $I$ is maximal,Shorter proof of  is a field if and only if  is maximal,R/I I,"Here is a proof I saw somewhere of the fact $R/I$ is a field if and only if $I$ is maximal: $\implies$ Suppose that $R/I$ is a field and $B$ is an ideal of $R$ that properly contains $I$. Let $b \in B$ but $b \notin I$. Then $b + I$ is a nonzero element of $R/I$ and therefore there exists an element $c + I$ such that $(c + I)(b + I) = 1 + I$. Since $b \in B$ we have $bc \in B$. Because $1 + I = (c + I)(b + I) = bc + I$ we have $1 - bc \in I \subset B$. So $1 = (1-bc) + bc \in B$. Hence $B = R$. $\Longleftarrow$ Now suppose $I$ is maximal and let $b \in R$ but $b \notin I$. Consider $B = \{br + a \mid r \in R, a \in I \}$. This is an ideal properly containing $I$. Since $I$ is maximal, $B = R$. Thus $1 = bc + a^\prime$ for some $a^\prime \in I$. Then $1 + I = bc + a^\prime + I = bc + I = (b + I)(c + I)$. I thought this was fairly long so I tried to come up with a shorter proof. Can you tell me if this is right: $\implies$ Assume that $R/I$ is a field and $I$ is not maximal. Then there exists an $x \in R - I = I^c$ that is not a unit (otherwise $I$ would be maximal). Then $x + I$ does not have an inverse hence $R/I$ is not a field. $\Longleftarrow$ Assume $I$ is maximal and $R/I$ is not a field. Then there is an $x$ such that $x + I \neq 0 + I$ does not have an inverse. This $x$ is not in $I$ and $x$ is not a unit. Hence $I \subsetneq I + (x) \subsetneq R$. Which contradicts $I$ being maximal.","Here is a proof I saw somewhere of the fact $R/I$ is a field if and only if $I$ is maximal: $\implies$ Suppose that $R/I$ is a field and $B$ is an ideal of $R$ that properly contains $I$. Let $b \in B$ but $b \notin I$. Then $b + I$ is a nonzero element of $R/I$ and therefore there exists an element $c + I$ such that $(c + I)(b + I) = 1 + I$. Since $b \in B$ we have $bc \in B$. Because $1 + I = (c + I)(b + I) = bc + I$ we have $1 - bc \in I \subset B$. So $1 = (1-bc) + bc \in B$. Hence $B = R$. $\Longleftarrow$ Now suppose $I$ is maximal and let $b \in R$ but $b \notin I$. Consider $B = \{br + a \mid r \in R, a \in I \}$. This is an ideal properly containing $I$. Since $I$ is maximal, $B = R$. Thus $1 = bc + a^\prime$ for some $a^\prime \in I$. Then $1 + I = bc + a^\prime + I = bc + I = (b + I)(c + I)$. I thought this was fairly long so I tried to come up with a shorter proof. Can you tell me if this is right: $\implies$ Assume that $R/I$ is a field and $I$ is not maximal. Then there exists an $x \in R - I = I^c$ that is not a unit (otherwise $I$ would be maximal). Then $x + I$ does not have an inverse hence $R/I$ is not a field. $\Longleftarrow$ Assume $I$ is maximal and $R/I$ is not a field. Then there is an $x$ such that $x + I \neq 0 + I$ does not have an inverse. This $x$ is not in $I$ and $x$ is not a unit. Hence $I \subsetneq I + (x) \subsetneq R$. Which contradicts $I$ being maximal.",,"['abstract-algebra', 'proof-verification', 'ring-theory', 'ideals', 'maximal-and-prime-ideals']"
25,Proving that $\mathbb{Z}[\sqrt{2}]$ is a Euclidean domain,Proving that  is a Euclidean domain,\mathbb{Z}[\sqrt{2}],"We're proving that $\mathbb{Z}[\sqrt{2}]$ is a Euclidean domain, using the norm function $$\nu (a + b\sqrt{2} ) = |a^2 - 2b^2|$$ and the first part says that since $\nu (a + b\sqrt{2} ) = |(a + b\sqrt{2})(a - b\sqrt{2})|$ it's clear that $\nu (xy) = \nu(x) \nu(y)$? ... Can someone please explain to me how this is clear?","We're proving that $\mathbb{Z}[\sqrt{2}]$ is a Euclidean domain, using the norm function $$\nu (a + b\sqrt{2} ) = |a^2 - 2b^2|$$ and the first part says that since $\nu (a + b\sqrt{2} ) = |(a + b\sqrt{2})(a - b\sqrt{2})|$ it's clear that $\nu (xy) = \nu(x) \nu(y)$? ... Can someone please explain to me how this is clear?",,['abstract-algebra']
26,An example of a Ring with many zero divisors,An example of a Ring with many zero divisors,,Is there an example of a commutative ring $R$ with identity such that all its elements distinct from $1$ are zero-divisors? I know that in a finite ring all the elements are units or zero-divisors. Is there a finite ring with the property I've required? Obviosuly I'm requiring that $|R|\geq 3$.,Is there an example of a commutative ring $R$ with identity such that all its elements distinct from $1$ are zero-divisors? I know that in a finite ring all the elements are units or zero-divisors. Is there a finite ring with the property I've required? Obviosuly I'm requiring that $|R|\geq 3$.,,"['abstract-algebra', 'ring-theory']"
27,A normal subgroup that is not a characteristic,A normal subgroup that is not a characteristic,,"In the book I'm study is written: A normal subgroup of a group need not be characteristic. And as an exercise I'm supposed to find an example, it also said that is pretty hard to find one. After trying for two days I wasn't able to find one example. So, I'm asking for an example of a group $G$ with a normal subgroup $H$ that is not a characteristic of $G$. I will add some context because maybe it will clarify why the book said it is difficult to find an example: the main problem with the exercise is that until it the book had only covered Group Definition, Subgroups, Lagrange's Theorem and Homomorphisms. So I'm supposed to find a example with such lack of advanced tools.","In the book I'm study is written: A normal subgroup of a group need not be characteristic. And as an exercise I'm supposed to find an example, it also said that is pretty hard to find one. After trying for two days I wasn't able to find one example. So, I'm asking for an example of a group $G$ with a normal subgroup $H$ that is not a characteristic of $G$. I will add some context because maybe it will clarify why the book said it is difficult to find an example: the main problem with the exercise is that until it the book had only covered Group Definition, Subgroups, Lagrange's Theorem and Homomorphisms. So I'm supposed to find a example with such lack of advanced tools.",,"['abstract-algebra', 'group-theory']"
28,Can we axiomatize a field starting with the binary operations and only “equational” axioms?,Can we axiomatize a field starting with the binary operations and only “equational” axioms?,,"The usual field axioms include the existence of (additive and multiplicative) identities and inverses. Is there a set of field axioms where all axioms are purely equational (see below for what I mean)? The Wikipedia article on fields contained (and still contains, slightly rewritten) an intriguing section on “Alternative axiomatizations”: Because of the relations between the operations, one can alternatively axiomatize a field by explicitly assuming that there are four binary operations (add, subtract, multiply, divide) with axioms relating these, … This is something I'm interested in, and I wonder whether it's true: can I see an example of such a set of axioms? Or prove that one does not exist? Specifically (because whatever Wikipedia is talking about may turn out not to be the thing I want), I'm thinking of a definition something like the following: a field is a set $F$ along with four operations $(+, -, \times, \div)$ satisfying the following axioms (here $a, b, c, d$ denote any elements of $F$): $$\begin{align} a + b &= b + a \\ a + (b + c) &= (a + b) + c \\ a + (b - c) &= (a + b) - c \\ a - (b - c) &= (a - b) + c \\ a + (b - b) &= a \quad \rlap{\text{(maybe we need something like this?)}} \\ a \times b &= b \times a \\ &\dots \end{align}$$ where each axiom is simply an equation (or a term-rewriting rule: if we have an expression of the form on the left, then we can transform it to the one on the right, maybe do these transformations until we get a canonical form), with no axioms of the form “there exist…” (like assuming $0$ or $1$ or additive or multiplicative inverses). If such a system does not result in a field, what's missing? (I'm trying to see whether, by starting with four arbitrary operations defined on a set $S$ and introducing equational constraints on the operations—such as commutativity, associativity, etc.—whether we can finally reach a state where we know these are all the constraints. I know this axiomatization may seem weird, but there do exist weird ones like Tarski's axiomatization of the reals .)","The usual field axioms include the existence of (additive and multiplicative) identities and inverses. Is there a set of field axioms where all axioms are purely equational (see below for what I mean)? The Wikipedia article on fields contained (and still contains, slightly rewritten) an intriguing section on “Alternative axiomatizations”: Because of the relations between the operations, one can alternatively axiomatize a field by explicitly assuming that there are four binary operations (add, subtract, multiply, divide) with axioms relating these, … This is something I'm interested in, and I wonder whether it's true: can I see an example of such a set of axioms? Or prove that one does not exist? Specifically (because whatever Wikipedia is talking about may turn out not to be the thing I want), I'm thinking of a definition something like the following: a field is a set $F$ along with four operations $(+, -, \times, \div)$ satisfying the following axioms (here $a, b, c, d$ denote any elements of $F$): $$\begin{align} a + b &= b + a \\ a + (b + c) &= (a + b) + c \\ a + (b - c) &= (a + b) - c \\ a - (b - c) &= (a - b) + c \\ a + (b - b) &= a \quad \rlap{\text{(maybe we need something like this?)}} \\ a \times b &= b \times a \\ &\dots \end{align}$$ where each axiom is simply an equation (or a term-rewriting rule: if we have an expression of the form on the left, then we can transform it to the one on the right, maybe do these transformations until we get a canonical form), with no axioms of the form “there exist…” (like assuming $0$ or $1$ or additive or multiplicative inverses). If such a system does not result in a field, what's missing? (I'm trying to see whether, by starting with four arbitrary operations defined on a set $S$ and introducing equational constraints on the operations—such as commutativity, associativity, etc.—whether we can finally reach a state where we know these are all the constraints. I know this axiomatization may seem weird, but there do exist weird ones like Tarski's axiomatization of the reals .)",,"['abstract-algebra', 'reference-request', 'field-theory', 'axioms', 'universal-algebra']"
29,Does every infinite group have a maximal subgroup?,Does every infinite group have a maximal subgroup?,,$G$ is an infinite group. Is it necessary true that there exists a subgroup $H$ of $G$ and $H$ is maximal ? Is it possible that there exists such series $H_1 < H_2 < H_3 <\cdots <G $ with the property that for every $H_i$ there exists $H_{i+1}$ such that $H_i < H_{i+1}$ ?,is an infinite group. Is it necessary true that there exists a subgroup of and is maximal ? Is it possible that there exists such series with the property that for every there exists such that ?,G H G H H_1 < H_2 < H_3 <\cdots <G  H_i H_{i+1} H_i < H_{i+1},"['abstract-algebra', 'group-theory', 'infinite-groups']"
30,What's the difference between the center of a group and a normal subgroup?,What's the difference between the center of a group and a normal subgroup?,,"It seems the definition of the center of a group and a normal subgroup are the same so I'm wondering what the difference is between the two? A group $H$ is normal in $G$ iff $Hg=gH$ for all $g \in G$. The center of a group $Z(G) = \{z| \in G$ and for all $g \in G, gz=zg\}$ Those statements seem equivalent to me.","It seems the definition of the center of a group and a normal subgroup are the same so I'm wondering what the difference is between the two? A group $H$ is normal in $G$ iff $Hg=gH$ for all $g \in G$. The center of a group $Z(G) = \{z| \in G$ and for all $g \in G, gz=zg\}$ Those statements seem equivalent to me.",,"['abstract-algebra', 'group-theory']"
31,Alternative proof of $I$ maximal implies $I$ prime,Alternative proof of  maximal implies  prime,I I,"The proof I know to show maximal implies prime for an ideal $I$ in a commutative ring $R$ goes as follows: $I$ maximal $\iff$ $R/I$ is a field $\implies$ $R/I$ is an integral domain $\iff$ $I$ is prime. Surely, it has to be possible to show the same as follows: Let $I$ be maximal and $ab \in I$. Assume $a \notin I$. Then $I$ is properly contained in $I + (a)$ and hence $1 \in I + (a)$ that is, there exists $r \in R$ and $i \in I$ such that $i + ra = 1$. Similarly, one can find $i^\prime$ and $r^\prime$ such that $i^\prime + r^\prime b = 1$. So we get that $ra - r^\prime b \in I$. Now I'm stuck. How can I deduce that either $a$ or $b$ have to be in $I$?","The proof I know to show maximal implies prime for an ideal $I$ in a commutative ring $R$ goes as follows: $I$ maximal $\iff$ $R/I$ is a field $\implies$ $R/I$ is an integral domain $\iff$ $I$ is prime. Surely, it has to be possible to show the same as follows: Let $I$ be maximal and $ab \in I$. Assume $a \notin I$. Then $I$ is properly contained in $I + (a)$ and hence $1 \in I + (a)$ that is, there exists $r \in R$ and $i \in I$ such that $i + ra = 1$. Similarly, one can find $i^\prime$ and $r^\prime$ such that $i^\prime + r^\prime b = 1$. So we get that $ra - r^\prime b \in I$. Now I'm stuck. How can I deduce that either $a$ or $b$ have to be in $I$?",,"['abstract-algebra', 'ring-theory']"
32,Field extension obtained by adjoining a cubic root to the rationals.,Field extension obtained by adjoining a cubic root to the rationals.,,"I hope it's not too long winded, but I prefer to give a short intro hoping for a last chance to go over this in my head and catch any error. Here's a primitive example of a field extension: $\mathbb{Q}(\sqrt 2) = \{a + b\sqrt 2 \;|\; a,b \in \mathbb{Q}\}$.  It's easy to show that it is a commutative additive group with identity $0$.  It's a little more involved to show that once $0$ is taken out (which rules out $a = b = 0$), what's left is a multiplicative group with identity $1$ and multiplicative inverse $$\dfrac{1}{a + b\sqrt 2} = \dfrac{1}{a + b\sqrt 2}\dfrac{a - b\sqrt 2}{a - b\sqrt 2} = \dfrac{a - b\sqrt 2}{a^2 - 2b^2} = \dfrac{a}{a^2 - 2b^2} + \dfrac{- b}{a^2 - 2b^2}\sqrt 2$$ which always exists because $a,b\in\mathbb{Q}$ ensures that the denominator in the above equation can never equal zero and since $a$ and $b$ cannot both be $0$ neither can the inverse, giving us closure.  So $\mathbb{Q}(\sqrt 2)$ is a field. Now we seek to replicate this with $\sqrt[3] 5$. A little bit of algebra will quickly show that if we define $\mathbb{Q}(\sqrt[3]5)$ with elements $a + b\sqrt[3]5$ as before, we'll run into problems with closure when multiplying two such elements.  Instead we define $$\mathbb{Q}(\sqrt[3]5) = \{a + b\sqrt[3]5 + c\sqrt[3]{25} \;|\; a,b,c \in \mathbb{Q}\}$$ Once again, checking that the above set is an additive abelian group is easy. To show that the set minus $0$ is a multiplicative group we need to do some linear algebra: We want to show that given $a,b$ and $c$ (not all three $0$) there exists a unique $x,y$ and $z$ such that $$(a + b\sqrt[3]5 + c\sqrt[3]{25})(x + y\sqrt[3]5 + z\sqrt[3]{25}) = (1 + 0\sqrt[3]5 + 0\sqrt[3]{25})$$ where the right-hand side of the above equation is just $1$, namely the multiplicative identity.  Some tedious algebra allows us to rewrite the left-hand side as  $$(ax + 5cy + 5bz) + (bx + ay + 5cz)\sqrt[3]{5} + (cx + by + az)\sqrt[3]{25} = 1$$ So we can rewrite the above as a system of equations ${\bf A x = b}$ given by $$\begin{pmatrix}a & 5c & 5b \\ b & a & 5c \\ c & b & a\end{pmatrix}\begin{pmatrix}x \\ y \\ z\end{pmatrix} = \begin{pmatrix} 1 \\ 0 \\ 0 \end{pmatrix}$$ This reduces the problem of showing that there always exists a unique multiplicative inverse to one of showing that the above square matrix is invertible (which would guarantee us a unique solution.)  So let's find its determinant: $$\det({\bf A}) = a(a^2 - 5bc) + b(5b^2 - 5ac) + c(25c^2 - 5ab) = a^3 + 5b^3 + 25c^3 - 15abc.$$ Finally, we get to where I'm stuck.  How can we guarantee that the above is always non-zero as long as $a,b$ and $c$ are not all zero?  The notes I'm going over skipped this part and just said that $\mathbb{Q}(\sqrt[3]{5})$ is a field.","I hope it's not too long winded, but I prefer to give a short intro hoping for a last chance to go over this in my head and catch any error. Here's a primitive example of a field extension: $\mathbb{Q}(\sqrt 2) = \{a + b\sqrt 2 \;|\; a,b \in \mathbb{Q}\}$.  It's easy to show that it is a commutative additive group with identity $0$.  It's a little more involved to show that once $0$ is taken out (which rules out $a = b = 0$), what's left is a multiplicative group with identity $1$ and multiplicative inverse $$\dfrac{1}{a + b\sqrt 2} = \dfrac{1}{a + b\sqrt 2}\dfrac{a - b\sqrt 2}{a - b\sqrt 2} = \dfrac{a - b\sqrt 2}{a^2 - 2b^2} = \dfrac{a}{a^2 - 2b^2} + \dfrac{- b}{a^2 - 2b^2}\sqrt 2$$ which always exists because $a,b\in\mathbb{Q}$ ensures that the denominator in the above equation can never equal zero and since $a$ and $b$ cannot both be $0$ neither can the inverse, giving us closure.  So $\mathbb{Q}(\sqrt 2)$ is a field. Now we seek to replicate this with $\sqrt[3] 5$. A little bit of algebra will quickly show that if we define $\mathbb{Q}(\sqrt[3]5)$ with elements $a + b\sqrt[3]5$ as before, we'll run into problems with closure when multiplying two such elements.  Instead we define $$\mathbb{Q}(\sqrt[3]5) = \{a + b\sqrt[3]5 + c\sqrt[3]{25} \;|\; a,b,c \in \mathbb{Q}\}$$ Once again, checking that the above set is an additive abelian group is easy. To show that the set minus $0$ is a multiplicative group we need to do some linear algebra: We want to show that given $a,b$ and $c$ (not all three $0$) there exists a unique $x,y$ and $z$ such that $$(a + b\sqrt[3]5 + c\sqrt[3]{25})(x + y\sqrt[3]5 + z\sqrt[3]{25}) = (1 + 0\sqrt[3]5 + 0\sqrt[3]{25})$$ where the right-hand side of the above equation is just $1$, namely the multiplicative identity.  Some tedious algebra allows us to rewrite the left-hand side as  $$(ax + 5cy + 5bz) + (bx + ay + 5cz)\sqrt[3]{5} + (cx + by + az)\sqrt[3]{25} = 1$$ So we can rewrite the above as a system of equations ${\bf A x = b}$ given by $$\begin{pmatrix}a & 5c & 5b \\ b & a & 5c \\ c & b & a\end{pmatrix}\begin{pmatrix}x \\ y \\ z\end{pmatrix} = \begin{pmatrix} 1 \\ 0 \\ 0 \end{pmatrix}$$ This reduces the problem of showing that there always exists a unique multiplicative inverse to one of showing that the above square matrix is invertible (which would guarantee us a unique solution.)  So let's find its determinant: $$\det({\bf A}) = a(a^2 - 5bc) + b(5b^2 - 5ac) + c(25c^2 - 5ab) = a^3 + 5b^3 + 25c^3 - 15abc.$$ Finally, we get to where I'm stuck.  How can we guarantee that the above is always non-zero as long as $a,b$ and $c$ are not all zero?  The notes I'm going over skipped this part and just said that $\mathbb{Q}(\sqrt[3]{5})$ is a field.",,"['abstract-algebra', 'ring-theory']"
33,Can an integer of the form $4n+3$ written as a sum of two squares?,Can an integer of the form  written as a sum of two squares?,4n+3,"Let $u$ be an integer of the form $4n+3$, where $n$ is a positive integer. Can we find integers $a$ and $b$ such that $u = a^2 + b^2$? If not, how to establish this for a fact?","Let $u$ be an integer of the form $4n+3$, where $n$ is a positive integer. Can we find integers $a$ and $b$ such that $u = a^2 + b^2$? If not, how to establish this for a fact?",,"['abstract-algebra', 'elementary-number-theory', 'sums-of-squares']"
34,Does the inverse of the matrix always rely on the determinant of a matrix?,Does the inverse of the matrix always rely on the determinant of a matrix?,,"I always thought that if the determinant of a matrix $A$ is $0$ then it has no inverse, $(A^{-1})$, until I saw an exercise in Contemporary Abstract Algebra by Gallian. This asks me to prove that the set of the $2\times2$ matrices of the form $$\begin{bmatrix} a&a\\ a&a\\ \end{bmatrix}\,,$$ where $a \neq 0$ and $a \in \mathbb R$. is a group under matrix multiplication. The determinant of the above set of matrices is $0$, but still the inverse exists for each matrix which is nothing but $$\begin{bmatrix} \frac{a}{2} & \frac{a}{2} \\ \frac{a}{2} & \frac{a}{2} \\ \end{bmatrix}\,.$$ How is this possible? What makes these type of matrices escape from satisfying determinants? What's the logic behind that?","I always thought that if the determinant of a matrix $A$ is $0$ then it has no inverse, $(A^{-1})$, until I saw an exercise in Contemporary Abstract Algebra by Gallian. This asks me to prove that the set of the $2\times2$ matrices of the form $$\begin{bmatrix} a&a\\ a&a\\ \end{bmatrix}\,,$$ where $a \neq 0$ and $a \in \mathbb R$. is a group under matrix multiplication. The determinant of the above set of matrices is $0$, but still the inverse exists for each matrix which is nothing but $$\begin{bmatrix} \frac{a}{2} & \frac{a}{2} \\ \frac{a}{2} & \frac{a}{2} \\ \end{bmatrix}\,.$$ How is this possible? What makes these type of matrices escape from satisfying determinants? What's the logic behind that?",,"['abstract-algebra', 'matrices', 'group-theory', 'inverse']"
35,Every non-unit is in some maximal ideal,Every non-unit is in some maximal ideal,,"I am trying to prove that every non-unit of a ring is contained in some maximal ideal. I have reasoned as follows: let $a$ be a non-unit and $M$ a maximal ideal. If $a$ is not contained in any maximal ideal, then the ideal $\langle M, a \rangle$ (that is, the ideal generated by $M$ and $a$) strictly contains $M$, a contradiction. However, there is a detail I'm unsure about. It's easy to see that $\langle a \rangle$ is a proper ideal for any non-unit $a$, but how can I be sure that $\langle M,a \rangle$ is also a proper ideal? Couldn't there, for example, exist some $m \in M$ such that $1=m+a$, so that $\langle M,a \rangle$ is the whole ring?","I am trying to prove that every non-unit of a ring is contained in some maximal ideal. I have reasoned as follows: let $a$ be a non-unit and $M$ a maximal ideal. If $a$ is not contained in any maximal ideal, then the ideal $\langle M, a \rangle$ (that is, the ideal generated by $M$ and $a$) strictly contains $M$, a contradiction. However, there is a detail I'm unsure about. It's easy to see that $\langle a \rangle$ is a proper ideal for any non-unit $a$, but how can I be sure that $\langle M,a \rangle$ is also a proper ideal? Couldn't there, for example, exist some $m \in M$ such that $1=m+a$, so that $\langle M,a \rangle$ is the whole ring?",,"['abstract-algebra', 'ring-theory']"
36,Algebraic objects associated with topological spaces.,Algebraic objects associated with topological spaces.,,"In Algebraic topology we use tools from abstract algebra: we ask question like ""when are two spaces not homeomorphic?"" by associating algebraic objects to them. For example, the fundamental group of a topological space. Let's go one step further and increase the number of binary operations from one to two, such as with rings. So I am curious whether there are other algebraic objects with two binary operations associated to topological spaces. Basically I am curious to know whether rings and fields are also associated to topological spaces, or is it just a vague question to ask?","In Algebraic topology we use tools from abstract algebra: we ask question like ""when are two spaces not homeomorphic?"" by associating algebraic objects to them. For example, the fundamental group of a topological space. Let's go one step further and increase the number of binary operations from one to two, such as with rings. So I am curious whether there are other algebraic objects with two binary operations associated to topological spaces. Basically I am curious to know whether rings and fields are also associated to topological spaces, or is it just a vague question to ask?",,"['abstract-algebra', 'general-topology', 'algebraic-topology']"
37,Understanding the ideal generated by a polynomial,Understanding the ideal generated by a polynomial,,"So my class on ring theory recently began, and I'm having a bit of trouble understanding ideals that are generated by polynomials. For an arbitrary ring, I know the definition of such an ideal, that is, for $R$ a ring and $a \in R$, $(a) = \{ \sum_{i=1}^{n} r_ias_i \mid r_i,s_i \in R \}$. So, then, this set is essentially just all the possible sums of different combinations of $r_ias_i$? Say, for $n=1$, $r_1as_1 \in (a)$, where $r_1,s_1$ just ""run through"" all the elements in $R$? Specifically, I had a previous homework problem concerning the following ring of polynomials: $R = [\mathbb{Z}/2\mathbb{Z}](t)$. I know what this means as a set, but I had trouble understanding this ideal: $f = f(t) = t^2 + t + 1 \in R$ and the ideal being $(f)$. The question was regarding $R/(f)$, which I know is the set $\{r + (f) \mid r \in R \}$, but I didn't even know where to start due to my lack of understanding of $(f)$. Any help would in understanding would be welcomed","So my class on ring theory recently began, and I'm having a bit of trouble understanding ideals that are generated by polynomials. For an arbitrary ring, I know the definition of such an ideal, that is, for $R$ a ring and $a \in R$, $(a) = \{ \sum_{i=1}^{n} r_ias_i \mid r_i,s_i \in R \}$. So, then, this set is essentially just all the possible sums of different combinations of $r_ias_i$? Say, for $n=1$, $r_1as_1 \in (a)$, where $r_1,s_1$ just ""run through"" all the elements in $R$? Specifically, I had a previous homework problem concerning the following ring of polynomials: $R = [\mathbb{Z}/2\mathbb{Z}](t)$. I know what this means as a set, but I had trouble understanding this ideal: $f = f(t) = t^2 + t + 1 \in R$ and the ideal being $(f)$. The question was regarding $R/(f)$, which I know is the set $\{r + (f) \mid r \in R \}$, but I didn't even know where to start due to my lack of understanding of $(f)$. Any help would in understanding would be welcomed",,"['abstract-algebra', 'ring-theory']"
38,An element of a group $G$ is not conjugate to its inverse if $\lvert G\rvert$ is odd,An element of a group  is not conjugate to its inverse if  is odd,G \lvert G\rvert,"Prove that if $G$ is a finite group of odd order, then no $x\in$$G$ , other than $x=1$, is conjugate to its inverse. This question is from Advanced Modern Algebra (exer 2.79) by Joseph J. Rotman. The hint states that if $x$ and $x^{-1}$ are conjugate, how many elements are in $x^{G}$? What I know so far: $\left\lvert x^{G}\right\rvert$ is odd (greater than 1, otherwise it's in the center) and is a divisor of |$G$| |Z($G$)| has a common factor (other than 1) with size of the orbit $\left\lvert x^{G}\right\rvert$ so that the center is not just the identity. This is from the class equation. The centralizer of $x$ has odd size and $\lvert C_{G}(x) \rvert \cdot \lvert x^{G}\rvert=\lvert G\rvert$ I don't see the implication of $x$ and its inverse being conjugate has other than their orbits having the same size and laying in the same conjugacy class. Is the info I have useful? Any help would be appreciated.","Prove that if $G$ is a finite group of odd order, then no $x\in$$G$ , other than $x=1$, is conjugate to its inverse. This question is from Advanced Modern Algebra (exer 2.79) by Joseph J. Rotman. The hint states that if $x$ and $x^{-1}$ are conjugate, how many elements are in $x^{G}$? What I know so far: $\left\lvert x^{G}\right\rvert$ is odd (greater than 1, otherwise it's in the center) and is a divisor of |$G$| |Z($G$)| has a common factor (other than 1) with size of the orbit $\left\lvert x^{G}\right\rvert$ so that the center is not just the identity. This is from the class equation. The centralizer of $x$ has odd size and $\lvert C_{G}(x) \rvert \cdot \lvert x^{G}\rvert=\lvert G\rvert$ I don't see the implication of $x$ and its inverse being conjugate has other than their orbits having the same size and laying in the same conjugacy class. Is the info I have useful? Any help would be appreciated.",,"['abstract-algebra', 'group-theory', 'finite-groups', 'group-actions']"
39,Intuition on non-unique ideal factorization if not integrally closed,Intuition on non-unique ideal factorization if not integrally closed,,"I'm aware of the result that integral domains admit unique factorization of ideals iff they are Dedekind domains. It's clear that $\mathbb{Z}[\sqrt{-3}]$ is not a Dedekind domain, as it is not integrally closed.  I'm having difficulty demonstrating directly that $\mathbb{Z}[\sqrt{-3}]$ does not admit unique factorization of ideals. Obviously we only need one example of an ideal that doesn't have a unique factorization into prime ideals, but a good answer would provide either a process or some intuition in finding such an example.","I'm aware of the result that integral domains admit unique factorization of ideals iff they are Dedekind domains. It's clear that $\mathbb{Z}[\sqrt{-3}]$ is not a Dedekind domain, as it is not integrally closed.  I'm having difficulty demonstrating directly that $\mathbb{Z}[\sqrt{-3}]$ does not admit unique factorization of ideals. Obviously we only need one example of an ideal that doesn't have a unique factorization into prime ideals, but a good answer would provide either a process or some intuition in finding such an example.",,"['abstract-algebra', 'number-theory', 'ring-theory', 'commutative-algebra', 'algebraic-number-theory']"
40,Localization at a prime ideal in $\mathbb{Z}/6\mathbb{Z}$,Localization at a prime ideal in,\mathbb{Z}/6\mathbb{Z},How can we compute the localization of the ring $\mathbb{Z}/6\mathbb{Z}$ at the prime ideal $2\mathbb{Z}/\mathbb{6Z}$? (or how do we see that this localization is an integral domain)?,How can we compute the localization of the ring $\mathbb{Z}/6\mathbb{Z}$ at the prime ideal $2\mathbb{Z}/\mathbb{6Z}$? (or how do we see that this localization is an integral domain)?,,"['abstract-algebra', 'ring-theory']"
41,Why is commutativity optional in multiplication for rings?,Why is commutativity optional in multiplication for rings?,,"More precisely, why is it that all rings are required by the axioms to have commutativity in addition, but are not held to the same axiom regarding multiplication? I know that we have commutative and non-commutative rings depending on whether or not they are commutative in multiplication, but I am wondering why it is that the axioms were defined that way, providing us with this option. I am using this list of axioms, from David Sharpe’s Rings and factorization : Definition 1.3.1. A ring is a non-empty set $R$ which satisfies the following axioms: (1) $R$ has a binary operation denoted by $+$ defined on it; (2) addition is associative, i.e. \begin{align} a + \left(b+c\right) = \left(a+b\right) + c \text{ for all } a, b, c \in R \end{align} (so that we can write $a+b+c$ without brackets); (3) addition is commutative, i.e. \begin{align} a + b = b + a \text{ for all } a, b \in R; \end{align} (4) there is an element denoted by $0$ in $R$ such that \begin{align} 0 + a = a \text{ for all } a \in R \end{align} (there is only one such element because, if $0_1$ and $0_2$ are two such, then $0_1 = 0_1 + 0_2 = 0_2$ and they are the same -- we call $0$ the zero element of $R$ ); (5) for every $a \in R$ , there exists an element $-a \in R$ such that \begin{align} \left(-a\right) + a = 0 \end{align} (there is only one such element for each $a$ , because if $b + a = 0$ and $c + a = 0$ , then \begin{align} b = 0 + b = \left(c + a\right) + b = c + \left(a + b\right) = c + 0 = c; \end{align} we call $-a$ the negative of $a$ ); (6) $R$ has a binary operation denoted by multiplication defined on it; (7) multiplication is associative, i.e. \begin{align} a\left(bc\right) = \left(ab\right)c \text{ for all } a, b, c \in R; \end{align} (8) multiplication is left and right distributive over addition, i.e. \begin{align} a\left(b+c\right) = ab + ac,\ \left(a+b\right)c = ac + bc \text{ for all } a, b, c \in R; \end{align} (9) there is an element denoted by $1$ in $R$ such that $1 \neq 0$ and \begin{align} 1 \cdot a = a \cdot 1 = a \text{ for all } a \in R \end{align} (as for the zero element, there is only one such element, and it is called the identity element of $R$ ).","More precisely, why is it that all rings are required by the axioms to have commutativity in addition, but are not held to the same axiom regarding multiplication? I know that we have commutative and non-commutative rings depending on whether or not they are commutative in multiplication, but I am wondering why it is that the axioms were defined that way, providing us with this option. I am using this list of axioms, from David Sharpe’s Rings and factorization : Definition 1.3.1. A ring is a non-empty set which satisfies the following axioms: (1) has a binary operation denoted by defined on it; (2) addition is associative, i.e. (so that we can write without brackets); (3) addition is commutative, i.e. (4) there is an element denoted by in such that (there is only one such element because, if and are two such, then and they are the same -- we call the zero element of ); (5) for every , there exists an element such that (there is only one such element for each , because if and , then we call the negative of ); (6) has a binary operation denoted by multiplication defined on it; (7) multiplication is associative, i.e. (8) multiplication is left and right distributive over addition, i.e. (9) there is an element denoted by in such that and (as for the zero element, there is only one such element, and it is called the identity element of ).","R R + \begin{align}
a + \left(b+c\right) = \left(a+b\right) + c \text{ for all } a, b, c \in R
\end{align} a+b+c \begin{align}
a + b = b + a \text{ for all } a, b \in R;
\end{align} 0 R \begin{align}
0 + a = a \text{ for all } a \in R
\end{align} 0_1 0_2 0_1 = 0_1 + 0_2 = 0_2 0 R a \in R -a \in R \begin{align}
\left(-a\right) + a = 0
\end{align} a b + a = 0 c + a = 0 \begin{align}
b = 0 + b = \left(c + a\right) + b = c + \left(a + b\right) = c + 0 = c;
\end{align} -a a R \begin{align}
a\left(bc\right) = \left(ab\right)c \text{ for all } a, b, c \in R;
\end{align} \begin{align}
a\left(b+c\right) = ab + ac,\ \left(a+b\right)c = ac + bc
\text{ for all } a, b, c \in R;
\end{align} 1 R 1 \neq 0 \begin{align}
1 \cdot a = a \cdot 1 = a \text{ for all } a \in R
\end{align} R","['abstract-algebra', 'ring-theory', 'commutative-algebra']"
42,There are at least three mutually non-isomorphic rings with $4$ elements?,There are at least three mutually non-isomorphic rings with  elements?,4,Is the following statement is true? There are at least three mutually non-isomorphic rings with $4$ elements. I have no idea or counterexample at the moment. Please help. So far I know about that a group of order $4$ is abelian and there are two non isomorphic groups of order $4$ like $K_4(non cyclic)$ and $\mathbb Z_4(cyclic)$.,Is the following statement is true? There are at least three mutually non-isomorphic rings with $4$ elements. I have no idea or counterexample at the moment. Please help. So far I know about that a group of order $4$ is abelian and there are two non isomorphic groups of order $4$ like $K_4(non cyclic)$ and $\mathbb Z_4(cyclic)$.,,"['abstract-algebra', 'ring-theory']"
43,Is the product of ideals commutative?,Is the product of ideals commutative?,,"My algebra book introduces sum, intersection and product of ideals (in non-commutative rings), and then says that all three operations are commutative and associative, without proof. I see no reasons why the product of ideals should be commutative, but I wasn't able to find a counterexample either.","My algebra book introduces sum, intersection and product of ideals (in non-commutative rings), and then says that all three operations are commutative and associative, without proof. I see no reasons why the product of ideals should be commutative, but I wasn't able to find a counterexample either.",,['abstract-algebra']
44,Can an algebraic structure have indistinguishable elements?,Can an algebraic structure have indistinguishable elements?,,"Sometimes, a topological space has indistinguishable points - we call those spaces non- $T_0$ . But given such a space, we can always identify indistinguishable points, thereby yielding a $T_0$ space. (Technically, we've taken the Kolomogorov quotient ). Does this sort of thing ever happen in abstract algebra? Here's two more examples. A preordered set can have comparable, distinct points - in other words it can fail to be antisymmetric. But that's cool, we can identify comparable points to obtain a partially ordered set. Sometimes a pseudometric space has distinct points that are zero distance apart. But that's okay, we can identify zero-distance points to obtain a metric space. Edit: It would be nice to see a definition of 'indistinguishable' for the elements of arbitrary structures. It would then be a consequence of this more general definition that for an arbitrary preordered set $X$ (order relation $\leq$ ) it holds that $x,y \in X$ are indistinguishable iff $x \leq y$ and $y \leq x$ . Here's an example. Consider the function $f : \mathbb{N} \rightarrow \mathbb{N}$ , with $f(n)=0$ for all $n \in \mathbb{N}$ . The associated notion of indistinguishability for the structure $(\mathbb{N},f)$ should probably be the relation $\sim$ such that $a \sim b$ iff both $a$ and $b$ equal $0$ , or both $a$ and $b$ are distinct from $0$ . Edit2: On the other hand, perhaps it does not make sense to speak of 'the natural notion of indistinguishability in a structure $X$ ' without first situating that structure in a category. After all, if we're going to quotient out by the indistinguishability relation, epimorphisms will probably show up at some point.","Sometimes, a topological space has indistinguishable points - we call those spaces non- . But given such a space, we can always identify indistinguishable points, thereby yielding a space. (Technically, we've taken the Kolomogorov quotient ). Does this sort of thing ever happen in abstract algebra? Here's two more examples. A preordered set can have comparable, distinct points - in other words it can fail to be antisymmetric. But that's cool, we can identify comparable points to obtain a partially ordered set. Sometimes a pseudometric space has distinct points that are zero distance apart. But that's okay, we can identify zero-distance points to obtain a metric space. Edit: It would be nice to see a definition of 'indistinguishable' for the elements of arbitrary structures. It would then be a consequence of this more general definition that for an arbitrary preordered set (order relation ) it holds that are indistinguishable iff and . Here's an example. Consider the function , with for all . The associated notion of indistinguishability for the structure should probably be the relation such that iff both and equal , or both and are distinct from . Edit2: On the other hand, perhaps it does not make sense to speak of 'the natural notion of indistinguishability in a structure ' without first situating that structure in a category. After all, if we're going to quotient out by the indistinguishability relation, epimorphisms will probably show up at some point.","T_0 T_0 X \leq x,y \in X x \leq y y \leq x f : \mathbb{N} \rightarrow \mathbb{N} f(n)=0 n \in \mathbb{N} (\mathbb{N},f) \sim a \sim b a b 0 a b 0 X",['abstract-algebra']
45,The number of ring homomorphisms from $\mathbb{Z}_m$ to $\mathbb{Z}_n$,The number of ring homomorphisms from  to,\mathbb{Z}_m \mathbb{Z}_n,"I face the problem of finding how many non-trivial ring or group homomorphisms there are from $\mathbb{Z}_m$ to $\mathbb{Z}_n$, where $m<n$. Is there any general formula? At the moment, I want to know how many ring homomorphisms there are when $m=12,n=28$. Please help.","I face the problem of finding how many non-trivial ring or group homomorphisms there are from $\mathbb{Z}_m$ to $\mathbb{Z}_n$, where $m<n$. Is there any general formula? At the moment, I want to know how many ring homomorphisms there are when $m=12,n=28$. Please help.",,"['abstract-algebra', 'ring-theory']"
46,Ring homomorphism with $\phi(1_R) \neq1_S$,Ring homomorphism with,\phi(1_R) \neq1_S,"Let $R$ and $S$ be rings with unity $1_R$ and $1_S$ respectively. Let $\phi\colon R\to S$ be a ring homomorphism. Give an example of a non-zero $\phi$ such that $\phi(1_R)\neq 1_S$ In trying to find a non-zero $\phi$ I've done the following observation: Since for $\forall r\in R$ $\phi(r) = \phi(r\times1_R) = \phi(r)\times\phi(1_R)$ we must have that $\phi(1_R)$ is an identity of $\phi(R)$ but not an identity of $S$. We must therefor construct a $\phi$ that is not onto and which have this property. I can't come up with any explicit example though, please help me.","Let $R$ and $S$ be rings with unity $1_R$ and $1_S$ respectively. Let $\phi\colon R\to S$ be a ring homomorphism. Give an example of a non-zero $\phi$ such that $\phi(1_R)\neq 1_S$ In trying to find a non-zero $\phi$ I've done the following observation: Since for $\forall r\in R$ $\phi(r) = \phi(r\times1_R) = \phi(r)\times\phi(1_R)$ we must have that $\phi(1_R)$ is an identity of $\phi(R)$ but not an identity of $S$. We must therefor construct a $\phi$ that is not onto and which have this property. I can't come up with any explicit example though, please help me.",,"['abstract-algebra', 'ring-theory']"
47,Show that ideal is a subring,Show that ideal is a subring,,"I'm experimenting around with ring ideals (perhaps ideals is always for rings, so when speaking of ideals we always refer to these ring subsets?), and my book gives me the definition that an ideal $I$ is a subset of a ring $R$, for which the elements are closed under addition and $ra\in I$ for all $r\in R$ and $a\in I$. Wikipedia say that an ideal is an additive subgroup which I clearly see, but I suspect that $I$ is also a subring to $R$. I cannot find any info on this in my book, but I tried to apply the ""subring criterion"" theorem (let $S$ be a subset of a ring $R$): (i) additive and multiplicative closures, (ii) if $a\in S \implies -a \in S$ and (iii) $S$ contains the identity. This is what I did: (i) addition follows from the definition, but for multiplication let $a,b \in I$ then for any $r_1,r_2 \in R$ we get $r_1ar_2b = (r_1ar_2)b \in I$ because $r_1ar_2 \in R$. Same argument for $a$, thus closure for multiplication. (ii) This holds from definition of a ring. (iii) $I$ is an additive subgroup hence $0\in I$. So, does my ""proof"" hold? Is it really true that an ideal is always a subring? If it is not true, ca anyone illustrate some counter example? Best regards,","I'm experimenting around with ring ideals (perhaps ideals is always for rings, so when speaking of ideals we always refer to these ring subsets?), and my book gives me the definition that an ideal $I$ is a subset of a ring $R$, for which the elements are closed under addition and $ra\in I$ for all $r\in R$ and $a\in I$. Wikipedia say that an ideal is an additive subgroup which I clearly see, but I suspect that $I$ is also a subring to $R$. I cannot find any info on this in my book, but I tried to apply the ""subring criterion"" theorem (let $S$ be a subset of a ring $R$): (i) additive and multiplicative closures, (ii) if $a\in S \implies -a \in S$ and (iii) $S$ contains the identity. This is what I did: (i) addition follows from the definition, but for multiplication let $a,b \in I$ then for any $r_1,r_2 \in R$ we get $r_1ar_2b = (r_1ar_2)b \in I$ because $r_1ar_2 \in R$. Same argument for $a$, thus closure for multiplication. (ii) This holds from definition of a ring. (iii) $I$ is an additive subgroup hence $0\in I$. So, does my ""proof"" hold? Is it really true that an ideal is always a subring? If it is not true, ca anyone illustrate some counter example? Best regards,",,"['abstract-algebra', 'ideals']"
48,$K[X]$-modules are $K$-vector spaces with a linear transformation,-modules are -vector spaces with a linear transformation,K[X] K,"In the Atiyah's book there is this example about $A$ -modules. Let $A=\mathbb{K}[x]$ , where $\mathbb{K}$ is a field. An $A$ -module is a $\mathbb{K}$ -vector space with a linear transformation. Can someone explain this claim to me?","In the Atiyah's book there is this example about -modules. Let , where is a field. An -module is a -vector space with a linear transformation. Can someone explain this claim to me?",A A=\mathbb{K}[x] \mathbb{K} A \mathbb{K},"['abstract-algebra', 'modules']"
49,Can nonzero polynomials vanish identically?,Can nonzero polynomials vanish identically?,,I know that a nonzero single-variable polynomial over a finite field can vanish identically e.g. take the product $\prod_a(x-a)$ for every $a$ in the field. But I know that for an infinite field this cannot happen since a degree $d$ polynomial has at most $d$ roots. My questions are: Why does a nonzero two-variable or higher polynomial over $\mathbb{R}$ not vanish identically? (In this case I know they can't but I don't know why) What about nonzero multivariate polynomials over other infinite fields?,I know that a nonzero single-variable polynomial over a finite field can vanish identically e.g. take the product $\prod_a(x-a)$ for every $a$ in the field. But I know that for an infinite field this cannot happen since a degree $d$ polynomial has at most $d$ roots. My questions are: Why does a nonzero two-variable or higher polynomial over $\mathbb{R}$ not vanish identically? (In this case I know they can't but I don't know why) What about nonzero multivariate polynomials over other infinite fields?,,"['abstract-algebra', 'polynomials', 'field-theory']"
50,Is an ideal also a normal subgroup?,Is an ideal also a normal subgroup?,,"The book I have first goes over group theory. Once it gets to rings and starts discussing subrings along with cosets and factor rings it leaves out some details for brevity and to not repeat what has already been said in the group theory portion. But in this instance, it left something out or perhaps it can be easily derived. It defines an ideal as a subring $H$ of a ring $R$ such that $\forall a \in R$ it is the case that $aH, Ha \subset H$, and it then states this is the analogue of the definition of normal subgroup in the context of group theory and implies a factor ring $R/H$ is created by the existence of some natural homomorphism. Here is where I'm confused. It makes no mentions that the criterion for being a normal subgroup also has to be satisfied in order for this factor ring to be well-defined. Am I correct that the criteron for being a normal subgroup has to also be satsified in order to a (1) be an ideal and (2) in order that a factor ring can be formed?","The book I have first goes over group theory. Once it gets to rings and starts discussing subrings along with cosets and factor rings it leaves out some details for brevity and to not repeat what has already been said in the group theory portion. But in this instance, it left something out or perhaps it can be easily derived. It defines an ideal as a subring $H$ of a ring $R$ such that $\forall a \in R$ it is the case that $aH, Ha \subset H$, and it then states this is the analogue of the definition of normal subgroup in the context of group theory and implies a factor ring $R/H$ is created by the existence of some natural homomorphism. Here is where I'm confused. It makes no mentions that the criterion for being a normal subgroup also has to be satisfied in order for this factor ring to be well-defined. Am I correct that the criteron for being a normal subgroup has to also be satsified in order to a (1) be an ideal and (2) in order that a factor ring can be formed?",,[]
51,Finitely generated modules in exact sequence,Finitely generated modules in exact sequence,,For $A$-modules and homomorphisms $0\to M'\stackrel{u}{\to}M\stackrel{v}{\to}M''\to 0$ is exact. Prove if $M'$ and $M''$ are fintely generated then $M$ is finitely generated.,For $A$-modules and homomorphisms $0\to M'\stackrel{u}{\to}M\stackrel{v}{\to}M''\to 0$ is exact. Prove if $M'$ and $M''$ are fintely generated then $M$ is finitely generated.,,"['abstract-algebra', 'modules', 'exact-sequence']"
52,Spectrum of a field,Spectrum of a field,,Let's $F$ be a field. What is $\operatorname{Spec}(F)$? I know that $\operatorname{Spec}(R)$ for ring $R$ is the set of prime ideals of $R$. But field doesn't have any non-trivial ideals. Thanks a lot!,Let's $F$ be a field. What is $\operatorname{Spec}(F)$? I know that $\operatorname{Spec}(R)$ for ring $R$ is the set of prime ideals of $R$. But field doesn't have any non-trivial ideals. Thanks a lot!,,"['abstract-algebra', 'general-topology', 'definition']"
53,Conjugacy classes of non-Abelian group of order $p^3$,Conjugacy classes of non-Abelian group of order,p^3,"Let $G$ be a non-abelian group of order $p^3$. How many are its conjugacy classes? The conjugacy classes are the orbits of $G$ under conjugation of  $G$ by itself. Since $G$ is non-abelian, its center has order $p$. So the class equation yields $p^3 = p + \sum_{[x]} (G: G_x)$, where $G_x$ is the centralizer of $x$ and the sum is taken over disjoint orbits $[x]$. We can also see that $(G:G_x)$ can only be $p$ or $p^2$. So we will have $p$ orbits of length $1$ and then orbits of length $p$ and $p^2$. Any hints on determining the number of the latter?","Let $G$ be a non-abelian group of order $p^3$. How many are its conjugacy classes? The conjugacy classes are the orbits of $G$ under conjugation of  $G$ by itself. Since $G$ is non-abelian, its center has order $p$. So the class equation yields $p^3 = p + \sum_{[x]} (G: G_x)$, where $G_x$ is the centralizer of $x$ and the sum is taken over disjoint orbits $[x]$. We can also see that $(G:G_x)$ can only be $p$ or $p^2$. So we will have $p$ orbits of length $1$ and then orbits of length $p$ and $p^2$. Any hints on determining the number of the latter?",,"['abstract-algebra', 'group-theory', 'finite-groups']"
54,Prove that a cyclic group with only one generator can have at most 2 elements,Prove that a cyclic group with only one generator can have at most 2 elements,,"Prove that a cyclic group that has only one generator has at most $2$ elements. I want to know if my proof would be valid: Suppose $G$ is a cyclic group and $g$ is its only generator. Let $|G|=n$ where $n>2$ , then we know that $\gcd(n,n-1)=1$ . This implies that $g^{n-1}$ is a generator of $G$ . We have a contradiction, since $g$ is the only generator of $G$ (and $n > 2$ leads to $n-1 \neq 1$ ). Thus $|G|\leq 2$ . I tried to use the fact that generating elements of a group are coprime to the order of the group, thanks.","Prove that a cyclic group that has only one generator has at most elements. I want to know if my proof would be valid: Suppose is a cyclic group and is its only generator. Let where , then we know that . This implies that is a generator of . We have a contradiction, since is the only generator of (and leads to ). Thus . I tried to use the fact that generating elements of a group are coprime to the order of the group, thanks.","2 G g |G|=n n>2 \gcd(n,n-1)=1 g^{n-1} G g G n > 2 n-1 \neq 1 |G|\leq 2","['abstract-algebra', 'gcd-and-lcm', 'cyclic-groups']"
55,Homomorphisms from a unital ring to a ring with no zero divisors preserve unity?,Homomorphisms from a unital ring to a ring with no zero divisors preserve unity?,,"I'm having a bit of trouble with a problem from Hungerford's Algebra concerning ring homomorphisms. Let $f\colon R\to S$ be a homomorphism of rings such that $f(r)\neq 0$ for some nonzero $r\in R$. If $R$ has an identity and $S$ has no zero divisors, then $S$ is a ring with identity $f(1_R)$. I've worked out a bit of an argument. Take $r$ to be as in the problem, so  $$ f(r)=f(1_R)f(r)=f(r)f(1_R)\implies f(r)-f(1_R)f(r)=0_S. $$ This shows that $f(1_R)$ is the identity of $f(R)$, but I'm not sure if that's much use. However, if $1_S$ exists, then from $f(r)-f(1_R)f(r)=1_Sf(r)-f(1_R)f(r)=0_S$ and the distributive law, I would have $$ \bigl(1_S-f(1_R)\bigr)f(r)=0_S\implies 1_S=f(1_R) $$ since $S$ has no zero divisors, and $f(r)\neq 0_S$. But I don't see a way to prove $1_S$ exists, so if it's no bother, I'm hoping to get a hint on how to show that, or perhaps a prod in the right direction if I'm off track. Thank you.","I'm having a bit of trouble with a problem from Hungerford's Algebra concerning ring homomorphisms. Let $f\colon R\to S$ be a homomorphism of rings such that $f(r)\neq 0$ for some nonzero $r\in R$. If $R$ has an identity and $S$ has no zero divisors, then $S$ is a ring with identity $f(1_R)$. I've worked out a bit of an argument. Take $r$ to be as in the problem, so  $$ f(r)=f(1_R)f(r)=f(r)f(1_R)\implies f(r)-f(1_R)f(r)=0_S. $$ This shows that $f(1_R)$ is the identity of $f(R)$, but I'm not sure if that's much use. However, if $1_S$ exists, then from $f(r)-f(1_R)f(r)=1_Sf(r)-f(1_R)f(r)=0_S$ and the distributive law, I would have $$ \bigl(1_S-f(1_R)\bigr)f(r)=0_S\implies 1_S=f(1_R) $$ since $S$ has no zero divisors, and $f(r)\neq 0_S$. But I don't see a way to prove $1_S$ exists, so if it's no bother, I'm hoping to get a hint on how to show that, or perhaps a prod in the right direction if I'm off track. Thank you.",,"['abstract-algebra', 'ring-theory']"
56,Can every finite dimensional division ring be endormorphism ring of some representation？,Can every finite dimensional division ring be endormorphism ring of some representation？,,k is a field，G is a finite group and V is a finite dimensional irreducible representation of G over k， then $End_GV$ is a finite dimensional division ring over k by Schur lemma. Can every finite dimensional division ring over k be obtained in this way?,k is a field，G is a finite group and V is a finite dimensional irreducible representation of G over k， then $End_GV$ is a finite dimensional division ring over k by Schur lemma. Can every finite dimensional division ring over k be obtained in this way?,,['abstract-algebra']
57,Direct sums and direct products,Direct sums and direct products,,This question has been in my head for a while. And today it appears again when I am reading Arveson's book on $C^*$-algebras. He says Countable direct products of Polish spaces are Polish. Countable direct sums of Polish spaces are Polish. But What is the difference between a direct sum and a direct product? It seems to me that these two are the same but Wiki says there are some cases where they are different (without pointing out these cases ). So can somebody clarify the difference between the two and maybe give some examples? Thanks!,This question has been in my head for a while. And today it appears again when I am reading Arveson's book on $C^*$-algebras. He says Countable direct products of Polish spaces are Polish. Countable direct sums of Polish spaces are Polish. But What is the difference between a direct sum and a direct product? It seems to me that these two are the same but Wiki says there are some cases where they are different (without pointing out these cases ). So can somebody clarify the difference between the two and maybe give some examples? Thanks!,,"['abstract-algebra', 'category-theory', 'definition', 'direct-sum', 'direct-product']"
58,What good are free groups?,What good are free groups?,,"In Algebra: Chapter 0 , one learns two definitions of free groups associating with sets. Let $A$ be a set, the free group of $A$, $F(A)$ is the initial object in the category $\mathcal{C}$, where \begin{equation} \operatorname{Obj}(\mathcal{C})=\{A\xrightarrow{g}G\}, \end{equation}where the codomain $G$ are groups, and \begin{equation} \operatorname{Hom}(\mathcal{C})=\{\text{Commutative Diagrams } A\xrightarrow{g_1}G_1\xrightarrow{\phi}G_2\xleftarrow{g_2}A\}  \end{equation} where $\phi$ are group homomorphisms. Also $F(A)$ has the concrete construction with elements being non-redundant words with alphabet $A$, and the group multiplication being juxtaposition and reduction. But what good are free groups? Why are they useful? Wikipedia says they are useful in topology, but does not explain why explicity. Can someone give some examples that an undergraduate can understand? I am asking about examples that can show the usefulness of this abstract construction. So I guess the identification of $\mathbb{Z}=F({a})$ does not really count.","In Algebra: Chapter 0 , one learns two definitions of free groups associating with sets. Let $A$ be a set, the free group of $A$, $F(A)$ is the initial object in the category $\mathcal{C}$, where \begin{equation} \operatorname{Obj}(\mathcal{C})=\{A\xrightarrow{g}G\}, \end{equation}where the codomain $G$ are groups, and \begin{equation} \operatorname{Hom}(\mathcal{C})=\{\text{Commutative Diagrams } A\xrightarrow{g_1}G_1\xrightarrow{\phi}G_2\xleftarrow{g_2}A\}  \end{equation} where $\phi$ are group homomorphisms. Also $F(A)$ has the concrete construction with elements being non-redundant words with alphabet $A$, and the group multiplication being juxtaposition and reduction. But what good are free groups? Why are they useful? Wikipedia says they are useful in topology, but does not explain why explicity. Can someone give some examples that an undergraduate can understand? I am asking about examples that can show the usefulness of this abstract construction. So I guess the identification of $\mathbb{Z}=F({a})$ does not really count.",,"['abstract-algebra', 'group-theory', 'category-theory', 'free-groups']"
59,Show that $x^4-x^2+1$ is irreducible over $\mathbb{Q}$,Show that  is irreducible over,x^4-x^2+1 \mathbb{Q},"My attempts: I cannot apply the Eisenstein's criteria here, because there is no prime number that divides the constant term i.e. $1$ Taking a translation of the form $x \rightarrow x+a$ does not solve this issue either. Next, I tried the mod tests: $\operatorname{mod}2$ doesn't work since $x^4-x^2+1=(x^2+x+1)(x^2-x+1)$, similarly in $\operatorname{mod}3$ $x^4-x^2+1=(x^2+1)^2$. Now I can go on and maybe eventually find a $\operatorname{mod}p$ that works, but that is very time consuming, specially in examinations. So, I'll use the rational root test. The possibilities for roots are $\pm 1$ and it is easy to see that neither is a root. The only possibility left then are quadratic factors, say, $(x^2+ax+b)(x^2+cx+d)=x^4-x^2+1$ This gives me a set of equations $bd=1, a+c=0, b+d+ac=-1$. So either $b=d=1$, in which case $a=\pm \sqrt3 \notin \mathbb{Q}$, or $b=d=-1$, which gives $a=\pm i \notin \mathbb{Q}$. So such factorization is not possible and hence the given polynomial is irreducible. Is this solution correct? Also, is there an easier way to solve this? Thank you.","My attempts: I cannot apply the Eisenstein's criteria here, because there is no prime number that divides the constant term i.e. $1$ Taking a translation of the form $x \rightarrow x+a$ does not solve this issue either. Next, I tried the mod tests: $\operatorname{mod}2$ doesn't work since $x^4-x^2+1=(x^2+x+1)(x^2-x+1)$, similarly in $\operatorname{mod}3$ $x^4-x^2+1=(x^2+1)^2$. Now I can go on and maybe eventually find a $\operatorname{mod}p$ that works, but that is very time consuming, specially in examinations. So, I'll use the rational root test. The possibilities for roots are $\pm 1$ and it is easy to see that neither is a root. The only possibility left then are quadratic factors, say, $(x^2+ax+b)(x^2+cx+d)=x^4-x^2+1$ This gives me a set of equations $bd=1, a+c=0, b+d+ac=-1$. So either $b=d=1$, in which case $a=\pm \sqrt3 \notin \mathbb{Q}$, or $b=d=-1$, which gives $a=\pm i \notin \mathbb{Q}$. So such factorization is not possible and hence the given polynomial is irreducible. Is this solution correct? Also, is there an easier way to solve this? Thank you.",,"['abstract-algebra', 'proof-verification', 'field-theory', 'irreducible-polynomials']"
60,Graphically Organizing the Interrelationships of Basic Algebraic Structures,Graphically Organizing the Interrelationships of Basic Algebraic Structures,,"I have never taken a formal course in Abstract Algebra (yet), but I am interested in learning more about the subject, as I know it is extremely important in Modern Mathematics and a powerful tool beyond (like in, say, Physics). However, in my limited exposure to the subject, I have found the taxonomy of the various Algebraic Structures very difficult to follow. For example, in basic Group Theory, there are groups, but also semigroups, monoids, semilattices, quasigroups, loops, and Abelian groups. I have no difficulty in understanding the definitions of each, but understanding (or maybe more accurately, remembering ) the interrelationships between them is confusing and difficult. I suppose this is largely because many of the names of these structures (like many names in math, unfortunately) are very non-descriptive. I have seen some authors create a flowchart-like graphic to help illustrate these relationships (e.g. a monoid is a semigroup with identity, or equivalently a group without inverses, etc.), and while helpful, they are always rather limited in scope and always deal with a linear progression of the structure hierarchy and do not include structures that ""branch out"" so to speak from the main hierarchy (e.g. quasigroups and semilattices which are special cases of magmas and semigroups respectively, but have no direct inclusive relationship to, say, monoids) So my question is: Are there any robust graphics illustrating the interrelationships between the various algebraic structures? Personally, I'm mainly interested in such graphics for Groups and Rings (especially Rings), but I leave this question open to answers for more advanced structures too (like modules).","I have never taken a formal course in Abstract Algebra (yet), but I am interested in learning more about the subject, as I know it is extremely important in Modern Mathematics and a powerful tool beyond (like in, say, Physics). However, in my limited exposure to the subject, I have found the taxonomy of the various Algebraic Structures very difficult to follow. For example, in basic Group Theory, there are groups, but also semigroups, monoids, semilattices, quasigroups, loops, and Abelian groups. I have no difficulty in understanding the definitions of each, but understanding (or maybe more accurately, remembering ) the interrelationships between them is confusing and difficult. I suppose this is largely because many of the names of these structures (like many names in math, unfortunately) are very non-descriptive. I have seen some authors create a flowchart-like graphic to help illustrate these relationships (e.g. a monoid is a semigroup with identity, or equivalently a group without inverses, etc.), and while helpful, they are always rather limited in scope and always deal with a linear progression of the structure hierarchy and do not include structures that ""branch out"" so to speak from the main hierarchy (e.g. quasigroups and semilattices which are special cases of magmas and semigroups respectively, but have no direct inclusive relationship to, say, monoids) So my question is: Are there any robust graphics illustrating the interrelationships between the various algebraic structures? Personally, I'm mainly interested in such graphics for Groups and Rings (especially Rings), but I leave this question open to answers for more advanced structures too (like modules).",,"['abstract-algebra', 'group-theory', 'ring-theory', 'definition', 'visualization']"
61,Localization of the Integer Ring,Localization of the Integer Ring,,"Let $\mathbb{Z}$ be the ring of integers and let $p$ be a prime, then the $p$-localization of $\mathbb{Z}$ is defined as $\mathbb{Z}_{(p)}=\{\displaystyle\frac{a}{b}|a,b\in\mathbb{Z},p\nmid b\}$. I can understand this definition literally but find it difficult to ""see"" what it really talks about. Would anyone help to shed more light on this definition, or recommend some further readings?","Let $\mathbb{Z}$ be the ring of integers and let $p$ be a prime, then the $p$-localization of $\mathbb{Z}$ is defined as $\mathbb{Z}_{(p)}=\{\displaystyle\frac{a}{b}|a,b\in\mathbb{Z},p\nmid b\}$. I can understand this definition literally but find it difficult to ""see"" what it really talks about. Would anyone help to shed more light on this definition, or recommend some further readings?",,"['abstract-algebra', 'reference-request', 'algebraic-number-theory', 'definition']"
62,Conjugate class in the dihedral group,Conjugate class in the dihedral group,,List all the conjugate classes in the dihedral group of order $2n$ and verify the class equation. The dihedral group is generated by two elements $r$ and $s$. The order of $r$ is two since $r^2=e$ and $s$ is $n$ since $s^n = e$. And I know all elements can be produced as either $s^k$ or $rs^k$. How can I list all the conjugacy class?,List all the conjugate classes in the dihedral group of order $2n$ and verify the class equation. The dihedral group is generated by two elements $r$ and $s$. The order of $r$ is two since $r^2=e$ and $s$ is $n$ since $s^n = e$. And I know all elements can be produced as either $s^k$ or $rs^k$. How can I list all the conjugacy class?,,"['abstract-algebra', 'group-theory']"
63,Is there a distributive law for ideals?,Is there a distributive law for ideals?,,"I'm curious if there is some sort of distributive law for ideals. If $I,J,K$ are ideals in an arbitrary ring, does $I(J+K)=IJ+IK$? The containment ""$\subset$"" is pretty clear I think. But the opposite ontainment doesn't feel like it should work. I couldn't work out a counterexample with ideals in $\mathbb{Z}$ however. So does such an equality always hold or not?","I'm curious if there is some sort of distributive law for ideals. If $I,J,K$ are ideals in an arbitrary ring, does $I(J+K)=IJ+IK$? The containment ""$\subset$"" is pretty clear I think. But the opposite ontainment doesn't feel like it should work. I couldn't work out a counterexample with ideals in $\mathbb{Z}$ however. So does such an equality always hold or not?",,"['abstract-algebra', 'ideals']"
64,Example of a non-Noetherian complete local ring,Example of a non-Noetherian complete local ring,,I was looking for an example of a non-Noetherian complete local commutative ring with $1$. I would appreciate if anyone can point to a reference.,I was looking for an example of a non-Noetherian complete local commutative ring with $1$. I would appreciate if anyone can point to a reference.,,"['abstract-algebra', 'reference-request', 'commutative-algebra']"
65,Index of the center of a group in the group is not a prime number,Index of the center of a group in the group is not a prime number,,The question is to prove index of $Z(G)$ in $G$ is not a prime number. We know that $|G:Z(G)|=|G:C_G (x)||C_G (x):Z(G)|$ where $C_G (x)$ means centralizer of $x \in G$ I want to mention that we do not want to use $G⁄Z(G)$ theorem. We may assume that $|G:Z(G)|=p$ where $p$ is a prime.,The question is to prove index of in is not a prime number. We know that where means centralizer of I want to mention that we do not want to use theorem. We may assume that where is a prime.,Z(G) G |G:Z(G)|=|G:C_G (x)||C_G (x):Z(G)| C_G (x) x \in G G⁄Z(G) |G:Z(G)|=p p,"['abstract-algebra', 'group-theory']"
66,Groups of units of $\mathbb{Z}\left[\frac{1+\sqrt{-3}}{2}\right]$,Groups of units of,\mathbb{Z}\left[\frac{1+\sqrt{-3}}{2}\right],"On page 230 of Dummit and Foote's Abstract Algebra, they say: the units of $\mathbb{Z}\left[\frac{1+\sqrt{-3}}{2}\right]$ are determined by the integers $a,b$ with $a^2+ab+b^2=\pm1$ i.e. with $(2a+b)^2+3b^2=4$, from which is is easy to see the group of units is a group of order $6$ given by $\{\pm1,\pm\rho,\pm\rho^2\}$ where $\rho=\frac{-1+\sqrt{-3}}{2}$. First, why change the characterization of unit from integers solutions of $a^2+ab+b^2=\pm1$ to integers solutions of $(2a+b)^2+3b^2=4$? How did they arrive at their answer?","On page 230 of Dummit and Foote's Abstract Algebra, they say: the units of $\mathbb{Z}\left[\frac{1+\sqrt{-3}}{2}\right]$ are determined by the integers $a,b$ with $a^2+ab+b^2=\pm1$ i.e. with $(2a+b)^2+3b^2=4$, from which is is easy to see the group of units is a group of order $6$ given by $\{\pm1,\pm\rho,\pm\rho^2\}$ where $\rho=\frac{-1+\sqrt{-3}}{2}$. First, why change the characterization of unit from integers solutions of $a^2+ab+b^2=\pm1$ to integers solutions of $(2a+b)^2+3b^2=4$? How did they arrive at their answer?",,"['abstract-algebra', 'ring-theory', 'algebraic-number-theory']"
67,A finite commutative ring with the property that every element can be written as product of two elements is unital,A finite commutative ring with the property that every element can be written as product of two elements is unital,,"I was struggling  for days with this nice problem: Let  $A$  be  a finite commutative  ring such that every element of $A$  can be written as product of two elements of $A$. Show that $A$  has a multiplicative unit  element. I need a hint for this problem, thank you very much.","I was struggling  for days with this nice problem: Let  $A$  be  a finite commutative  ring such that every element of $A$  can be written as product of two elements of $A$. Show that $A$  has a multiplicative unit  element. I need a hint for this problem, thank you very much.",,"['abstract-algebra', 'ring-theory', 'commutative-algebra', 'finite-rings', 'rngs']"
68,Is there an algorithm to check whether a subset generates a group?,Is there an algorithm to check whether a subset generates a group?,,"I’m new to abstract algebra, so recently, I came up with a curious question: Given a group $G$ and a subset $S$ of $G$ , is there a general algorithm to decide whether $\langle S \rangle = G$ ? A special case which I find interesting is when $G = S_n$ (finite symmetric groups), can we still come up with such algorithm to decide this problem? If possible, is the algorithm efficient (i.e in polynomial time)? (I think if the search space $G$ is finite, I might be able to come up with an exponential time brute force algorithm which takes $g_1, g_2 \in S$ and put $g_1g_2, g_2g_1$ back to $S$ repeatedly until there is no more new elements; not sure though if this is correct.) A concrete formulation of this question is more like: “Does the problem of checking whether $S$ generates $G$ in P, EXP, or even decidable or not?” for $G = S_n$ , for finite $G$ , and for general $G$ .","I’m new to abstract algebra, so recently, I came up with a curious question: Given a group and a subset of , is there a general algorithm to decide whether ? A special case which I find interesting is when (finite symmetric groups), can we still come up with such algorithm to decide this problem? If possible, is the algorithm efficient (i.e in polynomial time)? (I think if the search space is finite, I might be able to come up with an exponential time brute force algorithm which takes and put back to repeatedly until there is no more new elements; not sure though if this is correct.) A concrete formulation of this question is more like: “Does the problem of checking whether generates in P, EXP, or even decidable or not?” for , for finite , and for general .","G S G \langle S \rangle = G G = S_n G g_1, g_2 \in S g_1g_2, g_2g_1 S S G G = S_n G G","['abstract-algebra', 'group-theory', 'permutations', 'algorithms', 'computational-complexity']"
69,$p^{th}$ roots of a field with characteristic $p$,roots of a field with characteristic,p^{th} p,"This is problem 10.9 from the book ""Error-Correcting Codes and Finite Fields by Oliver Pretzel"". The Question: Show that in a field of characteristic $p$, any element $\alpha$ has at most one $p$-th root $\beta$ (i.e., an element $\beta\in F$ with $\beta^p = \alpha$). Show further that if $F$ is finite, then every element has exactly one $p$-th root This my attempt at the second part of the question. From Fermat's little theorem $\beta^{{p^n}-1} = 1$, where $p^n$ is the size of the field. Now multiplying both sides by $\beta$ we get $\beta^{p^n} = \beta$. If there is $p$ elements then $n=1$ and we can see this is true for any non-zero element.  For the general case, take the $p$-th root of both sides $\beta^{p^{n-1}} = \beta^{1/p}$ and we know from the multiplicative properties of a field if $\beta$ is a non zero element of the field then any multiple will be. The first part of the question I'm not sure where to begin. Any help would be appreciated.","This is problem 10.9 from the book ""Error-Correcting Codes and Finite Fields by Oliver Pretzel"". The Question: Show that in a field of characteristic $p$, any element $\alpha$ has at most one $p$-th root $\beta$ (i.e., an element $\beta\in F$ with $\beta^p = \alpha$). Show further that if $F$ is finite, then every element has exactly one $p$-th root This my attempt at the second part of the question. From Fermat's little theorem $\beta^{{p^n}-1} = 1$, where $p^n$ is the size of the field. Now multiplying both sides by $\beta$ we get $\beta^{p^n} = \beta$. If there is $p$ elements then $n=1$ and we can see this is true for any non-zero element.  For the general case, take the $p$-th root of both sides $\beta^{p^{n-1}} = \beta^{1/p}$ and we know from the multiplicative properties of a field if $\beta$ is a non zero element of the field then any multiple will be. The first part of the question I'm not sure where to begin. Any help would be appreciated.",,"['abstract-algebra', 'field-theory', 'finite-fields']"
70,Intersection of conjugate subgroups is normal,Intersection of conjugate subgroups is normal,,"Is there a better (more direct or intuitive) proof for this proposition than I have come up with below? I am not sure whether it could be simiplified: Let $G$ be a group with $H \leq G$. Then $K = \bigcap_{g \in G} gHg^{-1}$ is normal in $G$. Let $a \in K$. Then $a \in gHg^{-1}$ for all $g \in G$. Therefore for all $g_1,g \in G$, $g_1ag_1^{-1} \in g_1gHg^{-1}g_1^{-1} = (g_1g)H(g_1g)^{-1}$ and so $g_1ag_1^{-1} \in K$ since $g_1g \in G$. Then $K$ is normal in $G$.","Is there a better (more direct or intuitive) proof for this proposition than I have come up with below? I am not sure whether it could be simiplified: Let $G$ be a group with $H \leq G$. Then $K = \bigcap_{g \in G} gHg^{-1}$ is normal in $G$. Let $a \in K$. Then $a \in gHg^{-1}$ for all $g \in G$. Therefore for all $g_1,g \in G$, $g_1ag_1^{-1} \in g_1gHg^{-1}g_1^{-1} = (g_1g)H(g_1g)^{-1}$ and so $g_1ag_1^{-1} \in K$ since $g_1g \in G$. Then $K$ is normal in $G$.",,"['abstract-algebra', 'group-theory', 'normal-subgroups']"
71,Irreducible polynomial over $\mathbb{Q}$ implies polynomial is irreducible over $\mathbb{Z}$,Irreducible polynomial over  implies polynomial is irreducible over,\mathbb{Q} \mathbb{Z},"Let $f(x) \in \mathbb{Z}[x]$ be a polynomial of degree $\geq 2$. Then choose correct a) if $f(x)$ is irreducible in $ \mathbb{Z}[x] $ then it is irreducible in $ \mathbb{Q}[x] $. b) if $f(x)$ is irreducible in $ \mathbb{Q}[x] $ then it is irreducible in $ \mathbb{Z}[x] $. (1) is definitely true, for (2) $f(x)=2(x^2+2)$ clearly irreducible over $\mathbb{Q}[x]$ But I am confused about whether $f(x)$ is irreducible over $\mathbb{Z}[x]$ or not? According to Gallian, as 2 is non unit in $\mathbb{Z}$, $f(x)$ is reducible over $\mathbb{Z}[x]$, (2) is false. But definition of irreducible polynomial on Wikipedia says a polynomial is reducible if it can be written as product of non constant polynomials hence $f(x)$ is irreducible over $\mathbb{Z}[x]$ accordingly (2) is true .","Let $f(x) \in \mathbb{Z}[x]$ be a polynomial of degree $\geq 2$. Then choose correct a) if $f(x)$ is irreducible in $ \mathbb{Z}[x] $ then it is irreducible in $ \mathbb{Q}[x] $. b) if $f(x)$ is irreducible in $ \mathbb{Q}[x] $ then it is irreducible in $ \mathbb{Z}[x] $. (1) is definitely true, for (2) $f(x)=2(x^2+2)$ clearly irreducible over $\mathbb{Q}[x]$ But I am confused about whether $f(x)$ is irreducible over $\mathbb{Z}[x]$ or not? According to Gallian, as 2 is non unit in $\mathbb{Z}$, $f(x)$ is reducible over $\mathbb{Z}[x]$, (2) is false. But definition of irreducible polynomial on Wikipedia says a polynomial is reducible if it can be written as product of non constant polynomials hence $f(x)$ is irreducible over $\mathbb{Z}[x]$ accordingly (2) is true .",,"['abstract-algebra', 'polynomials']"
72,Maximal Ideals in Ring of Continuous Functions,Maximal Ideals in Ring of Continuous Functions,,"Dummit and Foote, 7.4.33(a) : Let $R$ be the ring of all continuous functions $[0,1] \to \mathbb{R}$ and let $M_c$ be the kernel of evaluation at $c \in [0,1]$, i.e. all $f$ such that $f(c) = 0$. Show that if $M$ is a maximal ideal in $R$ then $M = M_c$ for some $c$. I am aware of proofs that use the compactness of $[0,1]$; however I have been told that a simpler proof exists using only the isomorphism theorems for rings along with basic facts about ideals. I have tried on my own to prove it this way but still have no success; is it possible? (My attempts go something like: since $M$ is maximal, the quotient ring $R/M$ is a field. Then it would be nice to use the fact that $R/M$ has no zero divisors, but I can't quite make it work. Alternatively, consider $M \cap M_c$.)","Dummit and Foote, 7.4.33(a) : Let $R$ be the ring of all continuous functions $[0,1] \to \mathbb{R}$ and let $M_c$ be the kernel of evaluation at $c \in [0,1]$, i.e. all $f$ such that $f(c) = 0$. Show that if $M$ is a maximal ideal in $R$ then $M = M_c$ for some $c$. I am aware of proofs that use the compactness of $[0,1]$; however I have been told that a simpler proof exists using only the isomorphism theorems for rings along with basic facts about ideals. I have tried on my own to prove it this way but still have no success; is it possible? (My attempts go something like: since $M$ is maximal, the quotient ring $R/M$ is a field. Then it would be nice to use the fact that $R/M$ has no zero divisors, but I can't quite make it work. Alternatively, consider $M \cap M_c$.)",,"['abstract-algebra', 'ring-theory', 'continuity', 'maximal-and-prime-ideals']"
73,Are there number systems or rings in which not every number is a product of primes?,Are there number systems or rings in which not every number is a product of primes?,,"I am reading through some number theory and abstract algebra books, and in the number theory books they all prove the theorem which states that every integer is a product of primes (irreducibles). In particular, in Hardy and Wright, they prove that every element in $\mathbb{Z}[i]$ and $\mathbb{Z}[\rho]$ where $\rho:=e^{\frac{2}{3}\pi i}$ is a product of primes, where they define primes to be elements who are only divisible by themselves and the units of their ring. They prove it in a way which uses the norm function ($N(a+bi)=a^2+b^2$), and I believe the proof can be easily adapted to include any euclidean domain. This property (ability for every element to be written as a product of primes, not necessarily unique) seems so inherent to me, so I am wondering: are there any number systems or rings in which not every element can be written as a product of primes? Clarification: I am only talking about the non-units of a ring (that aren't 0). Units are not considered to be prime. $\mathbb{Q}$ satisfies this property because there are no non-units (except 0). Clarification 2: The answer given by Barry Smith was great in that it gave an example of a ring which was not a field but contained no primes so it had nonunit elements that were not products of primes, but I was really looking for a ring which contained primes/irreducibles and also contained elements that could not be expressed as a product of primes/irreducibles. My intuition states that this does not exist straightforwardly from the definition of prime/irreducible. The reason I question my intuition is because my number theory book exploits the norm function, which is a function that not all rings contain.","I am reading through some number theory and abstract algebra books, and in the number theory books they all prove the theorem which states that every integer is a product of primes (irreducibles). In particular, in Hardy and Wright, they prove that every element in $\mathbb{Z}[i]$ and $\mathbb{Z}[\rho]$ where $\rho:=e^{\frac{2}{3}\pi i}$ is a product of primes, where they define primes to be elements who are only divisible by themselves and the units of their ring. They prove it in a way which uses the norm function ($N(a+bi)=a^2+b^2$), and I believe the proof can be easily adapted to include any euclidean domain. This property (ability for every element to be written as a product of primes, not necessarily unique) seems so inherent to me, so I am wondering: are there any number systems or rings in which not every element can be written as a product of primes? Clarification: I am only talking about the non-units of a ring (that aren't 0). Units are not considered to be prime. $\mathbb{Q}$ satisfies this property because there are no non-units (except 0). Clarification 2: The answer given by Barry Smith was great in that it gave an example of a ring which was not a field but contained no primes so it had nonunit elements that were not products of primes, but I was really looking for a ring which contained primes/irreducibles and also contained elements that could not be expressed as a product of primes/irreducibles. My intuition states that this does not exist straightforwardly from the definition of prime/irreducible. The reason I question my intuition is because my number theory book exploits the norm function, which is a function that not all rings contain.",,"['abstract-algebra', 'number-theory', 'prime-numbers']"
74,Infinite coproduct of rings,Infinite coproduct of rings,,"I just learned from Wikipedia that coproduct of two (commutative) rings is given by tensor product over integers, and that coproduct of a family of rings is given by a ""construction analogous to the free product of groups."" Can the tensor product approach be generalized to an arbitrary family of rings? (Infinite tensor product?) I'm a little surprised that coproduct of  commutative rings requires noncommutative structure (free group). Does someone have a reference which explicitly constructs the coproduct?","I just learned from Wikipedia that coproduct of two (commutative) rings is given by tensor product over integers, and that coproduct of a family of rings is given by a ""construction analogous to the free product of groups."" Can the tensor product approach be generalized to an arbitrary family of rings? (Infinite tensor product?) I'm a little surprised that coproduct of  commutative rings requires noncommutative structure (free group). Does someone have a reference which explicitly constructs the coproduct?",,"['abstract-algebra', 'commutative-algebra', 'category-theory']"
75,Cohomology ring of $H^*(A_5;\mathbb Z_2)$?,Cohomology ring of ?,H^*(A_5;\mathbb Z_2),"What is the cohomology ring of $H^*(A_5;\mathbb Z_2)$ ? Here $A_5$ is the alternating group on $5$ letters. I am comfortable with the Lyndon-Hochschild-Serre spectral sequence, and understand how to use it to compute the cohomology of $A_4$ . However, because $A_5$ is simple, there is no extension $1 \to G \to A_5 \to H \to 1$ , so I'm not sure how to use this.","What is the cohomology ring of ? Here is the alternating group on letters. I am comfortable with the Lyndon-Hochschild-Serre spectral sequence, and understand how to use it to compute the cohomology of . However, because is simple, there is no extension , so I'm not sure how to use this.",H^*(A_5;\mathbb Z_2) A_5 5 A_4 A_5 1 \to G \to A_5 \to H \to 1,"['abstract-algebra', 'group-theory', 'algebraic-topology', 'group-cohomology']"
76,Prove that in any GCD domain every irreducible element is prime,Prove that in any GCD domain every irreducible element is prime,,"The proof of the following proposition is not completely clear to me. I get everything up until the bold part and I have a feeling some crucial steps are omitted, can anybody help clear this up? Let $R$ be an integral domain. If every two elements of $R$ have a greatest common divisor, then every irreducible element in $R$ is prime. Proof: Consider an irreducible element $p \in R$ and $x,y \in R$ such that $p\vert xy$. Suppose now that $py$ and $xy$ have a greatest common divisor $z$ in R. We want to conclude from this that $p \vert x$ or $p \vert y$. This is obvious if $xy = 0$, so we may assume that $xy \neq 0$. Then $z \neq 0$. As both $p$ and $y$ divide each of $py$ and $xy$, we have that $z = pu = yv$ for certain $u,v \in R$. Using the cancellation law with $\boldsymbol z \boldsymbol \neq \boldsymbol 0$, we obtain that $\boldsymbol v \boldsymbol \vert \boldsymbol p$ . As $p$ is irreducible, either $v \in R^\times$ (i.e. the set of invertible elements of $R$) or $v \sim p$ (i.e. $Rv = Rp$). If $v \sim p$, then $p \vert x$. If $v \in R^\times$, then $p$ divides $v^{-1} pu = v^{-1} z=y$.","The proof of the following proposition is not completely clear to me. I get everything up until the bold part and I have a feeling some crucial steps are omitted, can anybody help clear this up? Let $R$ be an integral domain. If every two elements of $R$ have a greatest common divisor, then every irreducible element in $R$ is prime. Proof: Consider an irreducible element $p \in R$ and $x,y \in R$ such that $p\vert xy$. Suppose now that $py$ and $xy$ have a greatest common divisor $z$ in R. We want to conclude from this that $p \vert x$ or $p \vert y$. This is obvious if $xy = 0$, so we may assume that $xy \neq 0$. Then $z \neq 0$. As both $p$ and $y$ divide each of $py$ and $xy$, we have that $z = pu = yv$ for certain $u,v \in R$. Using the cancellation law with $\boldsymbol z \boldsymbol \neq \boldsymbol 0$, we obtain that $\boldsymbol v \boldsymbol \vert \boldsymbol p$ . As $p$ is irreducible, either $v \in R^\times$ (i.e. the set of invertible elements of $R$) or $v \sim p$ (i.e. $Rv = Rp$). If $v \sim p$, then $p \vert x$. If $v \in R^\times$, then $p$ divides $v^{-1} pu = v^{-1} z=y$.",,"['abstract-algebra', 'proof-explanation', 'integral-domain']"
77,"Prove that an expression is zero for all sets of distinct $a_1, \dotsc, a_n\in\mathbb{C}$",Prove that an expression is zero for all sets of distinct,"a_1, \dotsc, a_n\in\mathbb{C}","A while ago one of my professors gave the class a problem ""to think about when lying on the beach."" Well, I've been on the beach several times since then to no avail and my curiosity has finally outweighed my desire to solve this personally. The problem is this: Let $a_1, \dotsc, a_n\in\mathbb{C}$ be distinct. Prove that: \begin{equation} \sum_{i=1}^n\prod_{j\neq i}\frac{1}{a_i - a_j} = 0 \end{equation} It's pretty easy, if tedious, to show this for a given $n$ but I'm unsure about how to generalise the result.","A while ago one of my professors gave the class a problem ""to think about when lying on the beach."" Well, I've been on the beach several times since then to no avail and my curiosity has finally outweighed my desire to solve this personally. The problem is this: Let $a_1, \dotsc, a_n\in\mathbb{C}$ be distinct. Prove that: \begin{equation} \sum_{i=1}^n\prod_{j\neq i}\frac{1}{a_i - a_j} = 0 \end{equation} It's pretty easy, if tedious, to show this for a given $n$ but I'm unsure about how to generalise the result.",,"['abstract-algebra', 'complex-numbers']"
78,"Before Abel's proof, what did they used for trying to find the general solution for quintics?","Before Abel's proof, what did they used for trying to find the general solution for quintics?",,"Whenever I read about the history of algebra, I end up with the same conclusion: They solved the general cubic, then the general quartic and then spent lots of years trying to solve the general quintic, which later was proved impossible by Abel and Galois (I guess Ruffini also made contributions to the topic). There's a gap in time, from the first atempts at finding the general solution of the quintic to Abel's proof - what have people tried before Abel's proof? I am curious about what kind of techniques they tried to use for finding the general quintic.","Whenever I read about the history of algebra, I end up with the same conclusion: They solved the general cubic, then the general quartic and then spent lots of years trying to solve the general quintic, which later was proved impossible by Abel and Galois (I guess Ruffini also made contributions to the topic). There's a gap in time, from the first atempts at finding the general solution of the quintic to Abel's proof - what have people tried before Abel's proof? I am curious about what kind of techniques they tried to use for finding the general quintic.",,"['abstract-algebra', 'math-history']"
79,Minimal axioms for a group,Minimal axioms for a group,,"My group theory is very rusty. If I want to just start with left inverses and left identities, must I link the axioms, or can I leave them independent? e.g., is it enough to say ""there exists at least one $e \in G$ s.t. $ea=a$ for all $a \in G$"" and ""for each $a$ in $G$, there exists $a^{-1} \in G$ s.t. $a^{-1}a$ is an identity"", or must I say ""there exists at least one $e \in G$ s.t. ($ea=a$ for all $a \in G$ AND for each $a \in G$ there exists $a^{-1} \in G$ s.t. $a^{-1}a=e$)"". This seems like it must be a FAQ, but I just can't find it. If I don't assume the linkage, I run into problems where I show (for example) that if $a^{-1}a=e_1$, $b=aa^{-1}$, $b^{-1}b=e_2$, then $b=e_2$. That will get me things like $ae_1=a$, but not the general case.","My group theory is very rusty. If I want to just start with left inverses and left identities, must I link the axioms, or can I leave them independent? e.g., is it enough to say ""there exists at least one $e \in G$ s.t. $ea=a$ for all $a \in G$"" and ""for each $a$ in $G$, there exists $a^{-1} \in G$ s.t. $a^{-1}a$ is an identity"", or must I say ""there exists at least one $e \in G$ s.t. ($ea=a$ for all $a \in G$ AND for each $a \in G$ there exists $a^{-1} \in G$ s.t. $a^{-1}a=e$)"". This seems like it must be a FAQ, but I just can't find it. If I don't assume the linkage, I run into problems where I show (for example) that if $a^{-1}a=e_1$, $b=aa^{-1}$, $b^{-1}b=e_2$, then $b=e_2$. That will get me things like $ae_1=a$, but not the general case.",,"['abstract-algebra', 'group-theory']"
80,Does every group have a 'cyclization'?,Does every group have a 'cyclization'?,,"Here's the question: Does every group have a 'cyclization'? That is, let $G$ be a group. Does there necessarily exist a cyclic group $C$ and a surjective homomorphism $\varphi:G\rightarrow C$ such that for every other cyclic group $H$ such that there is a surjective homomorphism $\psi:G\rightarrow H$ then there is a unique homomorphism $\phi:C\rightarrow H$ such that $\psi=\phi\circ \varphi$? This question is inspired by the existence of the abelianization of a group. Here's what I have done: Let $G$ be a group and $C$ a cyclic group such that $\varphi:G\rightarrow C$ is a surjective homomorphism. Since $C$ is cyclic, $C=\langle c\rangle$ for some $c\in C$. Let $c^m$ and $c^n$ be two other elements of $C$. Since $\varphi$ is surjective, there are elements $g$ and $h\in G$ such that $\varphi(g)=c^m$ and $\varphi(h)=c^n$ (assume $m$ and $n$ are non-zero). Thus $$\varphi(h)^{\text{lcm}(m,n)/n}=c^{\text{lcm}(m,n)}=\varphi(g)^{\text{lcm}(m,n)/m}$$ This implies that  $$\varphi\left(g^{\text{lcm}(m,n)/m}h^{-\text{lcm}(m,n)/n}\right)=1_C$$ We can conclude then that  $$N=\langle\;g^{\text{lcm}(m,n)/m}h^{-\text{lcm}(m,n)/n}\;|\;g,h\in G\text{ and }\;m,n\in\Bbb Z-\{0\}\rangle\subseteq\ker\varphi$$ In addition, since $$xg^{\text{lcm}(m,n)/m}h^{-\text{lcm}(m,n)/n} x^{-1}=(xgx^{-1})^{\text{lcm}(m,n)/m}(xhx^{-1})^{-\text{lcm}(m,n)/n}$$ for any $x\in G$, we get that $N$ is a normal subgroup of $G$. Further, we see that $[G,G]\subseteq N$ since  $$xyx^{-1}y^{-1}=(xy)^{\text{lcm}(-1,1)/1}(x^{-1}y^{-1})^{-\text{lcm}(-1,1)/-1}\in N$$ So, I seem to be on the right track. Perhaps $G/N$ is itself cyclic. However, even though I know that $G/N$ is abelian, I have no idea on how to show it is cyclic (if it necessarily is). Stepping back from the problem, it would suffice to show that every abelian group has a cyclization since every map onto a cyclic quotient would have to factor through the abelianization of a group. This seems to be obvious. If $G$ is an abelian group which is finitely generated then the Structure Theorem for Finitely-generated Abelian Groups tells us that $G$ is isomorphic to a group that looks like $$C_{d_1}\times C_{d_2}\times\cdots\times C_{d_n}\times FA_m$$ where $d_1\mid d_2\mid \cdots\mid d_n$, each $C_{d_i}$ represents a cyclic group of order $d_i$, and $FA_m$ represents a free abelian group on $m$ generators. In the absence of a $FA_m$ term, the cyclization would be $C_{d_n}$. In the presence of a $FA_m$ term, the cyclization would be $FA_1$. For an abelian group that is not finitely generated, I believe the cyclization would be the trivial group. But I'm having difficulty with this claim. Maybe there's a counter-example here.","Here's the question: Does every group have a 'cyclization'? That is, let $G$ be a group. Does there necessarily exist a cyclic group $C$ and a surjective homomorphism $\varphi:G\rightarrow C$ such that for every other cyclic group $H$ such that there is a surjective homomorphism $\psi:G\rightarrow H$ then there is a unique homomorphism $\phi:C\rightarrow H$ such that $\psi=\phi\circ \varphi$? This question is inspired by the existence of the abelianization of a group. Here's what I have done: Let $G$ be a group and $C$ a cyclic group such that $\varphi:G\rightarrow C$ is a surjective homomorphism. Since $C$ is cyclic, $C=\langle c\rangle$ for some $c\in C$. Let $c^m$ and $c^n$ be two other elements of $C$. Since $\varphi$ is surjective, there are elements $g$ and $h\in G$ such that $\varphi(g)=c^m$ and $\varphi(h)=c^n$ (assume $m$ and $n$ are non-zero). Thus $$\varphi(h)^{\text{lcm}(m,n)/n}=c^{\text{lcm}(m,n)}=\varphi(g)^{\text{lcm}(m,n)/m}$$ This implies that  $$\varphi\left(g^{\text{lcm}(m,n)/m}h^{-\text{lcm}(m,n)/n}\right)=1_C$$ We can conclude then that  $$N=\langle\;g^{\text{lcm}(m,n)/m}h^{-\text{lcm}(m,n)/n}\;|\;g,h\in G\text{ and }\;m,n\in\Bbb Z-\{0\}\rangle\subseteq\ker\varphi$$ In addition, since $$xg^{\text{lcm}(m,n)/m}h^{-\text{lcm}(m,n)/n} x^{-1}=(xgx^{-1})^{\text{lcm}(m,n)/m}(xhx^{-1})^{-\text{lcm}(m,n)/n}$$ for any $x\in G$, we get that $N$ is a normal subgroup of $G$. Further, we see that $[G,G]\subseteq N$ since  $$xyx^{-1}y^{-1}=(xy)^{\text{lcm}(-1,1)/1}(x^{-1}y^{-1})^{-\text{lcm}(-1,1)/-1}\in N$$ So, I seem to be on the right track. Perhaps $G/N$ is itself cyclic. However, even though I know that $G/N$ is abelian, I have no idea on how to show it is cyclic (if it necessarily is). Stepping back from the problem, it would suffice to show that every abelian group has a cyclization since every map onto a cyclic quotient would have to factor through the abelianization of a group. This seems to be obvious. If $G$ is an abelian group which is finitely generated then the Structure Theorem for Finitely-generated Abelian Groups tells us that $G$ is isomorphic to a group that looks like $$C_{d_1}\times C_{d_2}\times\cdots\times C_{d_n}\times FA_m$$ where $d_1\mid d_2\mid \cdots\mid d_n$, each $C_{d_i}$ represents a cyclic group of order $d_i$, and $FA_m$ represents a free abelian group on $m$ generators. In the absence of a $FA_m$ term, the cyclization would be $C_{d_n}$. In the presence of a $FA_m$ term, the cyclization would be $FA_1$. For an abelian group that is not finitely generated, I believe the cyclization would be the trivial group. But I'm having difficulty with this claim. Maybe there's a counter-example here.",,['abstract-algebra']
81,Finite rings without zero divisors are division rings.,Finite rings without zero divisors are division rings.,,"How can I prove this: Finite rings without zero divisors are division rings. I know how to prove it when the ring has $1$, but I have no idea if my ring needs to have an unity.","How can I prove this: Finite rings without zero divisors are division rings. I know how to prove it when the ring has $1$, but I have no idea if my ring needs to have an unity.",,"['abstract-algebra', 'ring-theory', 'noncommutative-algebra', 'finite-rings']"
82,The intersection of a normal subgroup and Sylow $p$-subgroup,The intersection of a normal subgroup and Sylow -subgroup,p,"Let $G$ be a group and $P\in Syl_p(G)$ , $H$ is normal in $G$ . I want to show that $P\cap H\in Syl_p(H)$ . So I let $P_0\in Syl_p(H)$ . $P\cap H$ is a $p$ subgroup of $H$ , so by Sylow 2nd Theorem, $P\cap H \leq P_0$ . And by Sylow's 2nd and 3rd theorem, I get that there exists $g\in G$ such that $P_0 \leq gPg^{-1}$ . I think I want to prove that $P_0 \leq P\cap H$ next in order to conclude that $P_0=P\cap H$ but got stuck at this part.","Let be a group and , is normal in . I want to show that . So I let . is a subgroup of , so by Sylow 2nd Theorem, . And by Sylow's 2nd and 3rd theorem, I get that there exists such that . I think I want to prove that next in order to conclude that but got stuck at this part.",G P\in Syl_p(G) H G P\cap H\in Syl_p(H) P_0\in Syl_p(H) P\cap H p H P\cap H \leq P_0 g\in G P_0 \leq gPg^{-1} P_0 \leq P\cap H P_0=P\cap H,"['abstract-algebra', 'group-theory', 'finite-groups', 'sylow-theory']"
83,Why is the quotient map $SL_n(\mathbb{Z})$ to $SL_n(\mathbb{Z}/p\mathbb Z)$ is surjective?,Why is the quotient map  to  is surjective?,SL_n(\mathbb{Z}) SL_n(\mathbb{Z}/p\mathbb Z),"Recall that $SL_n(\mathbb{Z})$ is the special linear group, $n\geq 2$, and let $q\geq 2$ be any integer. We have a natural quotient map $$\pi: SL_n(\mathbb{Z})\to SL_n(\mathbb{Z}/q).$$ I remember that this map is surjective (is it correct?). It seems the Chinese Remainder Theorem might be helpful, but I forgot how to prove it. Can anyone give some tips?","Recall that $SL_n(\mathbb{Z})$ is the special linear group, $n\geq 2$, and let $q\geq 2$ be any integer. We have a natural quotient map $$\pi: SL_n(\mathbb{Z})\to SL_n(\mathbb{Z}/q).$$ I remember that this map is surjective (is it correct?). It seems the Chinese Remainder Theorem might be helpful, but I forgot how to prove it. Can anyone give some tips?",,"['abstract-algebra', 'group-theory', 'ring-theory', 'linear-groups']"
84,Category theory without codomains?,Category theory without codomains?,,"A surjection is a function whose range equals its codomain. Thus, the distinction between functions and surjections requires the notion of a codomain. Similarly, a bijection is an injection whose range equals its codomain. So this distinction, too, requires the notion of a codomain. However, codomains seem like a very artificial idea, to myself and many others; and so we avoid using them. Thus, we speak of functions $f:X \rightarrow Y$ and functions $f: X \twoheadrightarrow Y$, the latter being a substitute for the notion of a surjection. And we speak of injections $f:X \rightarrow Y$ and injections $f:X \twoheadrightarrow Y$, the latter being a substitute for the notion of a bijection. That's all well and good, but there's a tension here lurking beneath the surface. The source of the tension is category theory, which stipulates that morphisms have not only domains but also codomains. So unless we can reformulate category theory without codomains, we're sort of trapped. Has anyone managed to do this? If not, here's my very preliminary attempt at doing so. A category* will consist of the following data: a class of objects, a class of isomorphisms ( not homomorphisms), each of which has a unique domain and range; an operation that composes isomorphisms; and two partial order relations defined on the class of objects, namely the relation ""is a subobject of"" and ""is a quotient object of"". So for example, the statement ""$f$ is monomorphism $X \rightarrow Y$"" would be interpreted as ""There exists a subobject of $Y$, call it $B$, such that $f$ is an isomorphism with domain $X$ and range $B$."" And the statement ""$f$ is a morphism $X \twoheadrightarrow Y$"" would be interpreted as ""There exists a quotient object of $X$, call it $A$, such that $f$ is an isomorphism with domain $A$ and range $Y$."" Anyway, I'd be interested in hearing people's opinion on the matter. EDIT. Just to clarify, this approach would appeal to people whom prefer to define functions (etc.) as their graph, rather than as a triple.","A surjection is a function whose range equals its codomain. Thus, the distinction between functions and surjections requires the notion of a codomain. Similarly, a bijection is an injection whose range equals its codomain. So this distinction, too, requires the notion of a codomain. However, codomains seem like a very artificial idea, to myself and many others; and so we avoid using them. Thus, we speak of functions $f:X \rightarrow Y$ and functions $f: X \twoheadrightarrow Y$, the latter being a substitute for the notion of a surjection. And we speak of injections $f:X \rightarrow Y$ and injections $f:X \twoheadrightarrow Y$, the latter being a substitute for the notion of a bijection. That's all well and good, but there's a tension here lurking beneath the surface. The source of the tension is category theory, which stipulates that morphisms have not only domains but also codomains. So unless we can reformulate category theory without codomains, we're sort of trapped. Has anyone managed to do this? If not, here's my very preliminary attempt at doing so. A category* will consist of the following data: a class of objects, a class of isomorphisms ( not homomorphisms), each of which has a unique domain and range; an operation that composes isomorphisms; and two partial order relations defined on the class of objects, namely the relation ""is a subobject of"" and ""is a quotient object of"". So for example, the statement ""$f$ is monomorphism $X \rightarrow Y$"" would be interpreted as ""There exists a subobject of $Y$, call it $B$, such that $f$ is an isomorphism with domain $X$ and range $B$."" And the statement ""$f$ is a morphism $X \twoheadrightarrow Y$"" would be interpreted as ""There exists a quotient object of $X$, call it $A$, such that $f$ is an isomorphism with domain $A$ and range $Y$."" Anyway, I'd be interested in hearing people's opinion on the matter. EDIT. Just to clarify, this approach would appeal to people whom prefer to define functions (etc.) as their graph, rather than as a triple.",,"['functions', 'category-theory', 'abstract-algebra']"
85,When is a group isomorphic to the infinite cyclic group?,When is a group isomorphic to the infinite cyclic group?,,"I am learning algebra and I am a bit confused. Let's say I have a finitely presented group $G$, can anyone tell me if it is possible to find out if $G\cong \mathbb{Z}$? Thanks","I am learning algebra and I am a bit confused. Let's say I have a finitely presented group $G$, can anyone tell me if it is possible to find out if $G\cong \mathbb{Z}$? Thanks",,"['abstract-algebra', 'group-theory', 'combinatorial-group-theory']"
86,"Does the list of ""number of groups of order $n$"" contain every natural number?","Does the list of ""number of groups of order "" contain every natural number?",n,"In other words: For every natural number $m$ , does there always exist an $n$ for which there are exactly $m$ groups of order $n$ up to isomorphism? Or is this an open question in mathematics? If it is an open question, are there any famous conjectures one way or the other? And what progress has been made in answering the question?","In other words: For every natural number , does there always exist an for which there are exactly groups of order up to isomorphism? Or is this an open question in mathematics? If it is an open question, are there any famous conjectures one way or the other? And what progress has been made in answering the question?",m n m n,"['abstract-algebra', 'group-theory', 'finite-groups', 'open-problem', 'groups-enumeration']"
87,Extending Homomorphism into Algebraically Closed Field,Extending Homomorphism into Algebraically Closed Field,,"If we are given a homomorphism $g$ between a field $k$ and an algebraically closed field $\Omega$, and a field $k'$ which is a finite algebraic extension of $k$, how do we extend $g$ to a homomorphism $g'$ from $k'$ to $\Omega$?","If we are given a homomorphism $g$ between a field $k$ and an algebraically closed field $\Omega$, and a field $k'$ which is a finite algebraic extension of $k$, how do we extend $g$ to a homomorphism $g'$ from $k'$ to $\Omega$?",,"['abstract-algebra', 'field-theory']"
88,"Is $(\mathbb{Q},+)$ the direct product of two non-trivial subgroups?",Is  the direct product of two non-trivial subgroups?,"(\mathbb{Q},+)","Is this statement true or false? I am really not having any idea how to prove or a counterexample, please help. Is $(\mathbb{Q},+)$ a direct product of two non-trivial subgroups?","Is this statement true or false? I am really not having any idea how to prove or a counterexample, please help. Is $(\mathbb{Q},+)$ a direct product of two non-trivial subgroups?",,"['abstract-algebra', 'group-theory']"
89,Product of two power series,Product of two power series,,Say if I define a power series over some arbitrary field $F$ as $$a = \sum^{ \infty }_{i = 0} a_{i} X^{i} $$ Then can I say: $$ab = \sum^{ \infty }_{i = 0} \sum^{ \infty }_{j = 0} a_{i} b_{j} X^{i + j} $$,Say if I define a power series over some arbitrary field $F$ as $$a = \sum^{ \infty }_{i = 0} a_{i} X^{i} $$ Then can I say: $$ab = \sum^{ \infty }_{i = 0} \sum^{ \infty }_{j = 0} a_{i} b_{j} X^{i + j} $$,,"['abstract-algebra', 'convergence-divergence', 'power-series']"
90,Euler's remarkable prime-producing polynomial and quadratic UFDs,Euler's remarkable prime-producing polynomial and quadratic UFDs,,"Good example of a polynomial which produces a finite number of primes is: $$x^{2}+x+41$$ which produces primes for every integer $ 0 \leq x \leq 39$. In a paper H. Stark proves the following result: $X_{n}$ (the ring of ""algebraic integers"" in $\mathbb Q(\sqrt{-n}))$ is a principal ideal domain for positive $n$ if and only if  $n = 1,2,3,7,11,19,43,67,163. $  For a reference one can see: Harold Stark, A complete determination of the complex quadratic fields of class-number one , Michigan Math J., 14 (1967) 1-27. Consider in general the polynomial $x^{2}+x + K= (x+ \alpha)(x+ \bar{\alpha})$  which we can factorize where $\alpha$ is given by $$ \alpha = \frac{{1} + \sqrt{1-4K}}{2}, \quad \bar{\alpha} = \frac{1 - \sqrt{1-4K}}{2}.$$ One can get some relationships between polynomials which produce prime in the field $\mathbb Q(\sqrt{-n})$. Question is if a polynomial produces a prime, then will $X_{n}$ as defined above be a PID?","Good example of a polynomial which produces a finite number of primes is: $$x^{2}+x+41$$ which produces primes for every integer $ 0 \leq x \leq 39$. In a paper H. Stark proves the following result: $X_{n}$ (the ring of ""algebraic integers"" in $\mathbb Q(\sqrt{-n}))$ is a principal ideal domain for positive $n$ if and only if  $n = 1,2,3,7,11,19,43,67,163. $  For a reference one can see: Harold Stark, A complete determination of the complex quadratic fields of class-number one , Michigan Math J., 14 (1967) 1-27. Consider in general the polynomial $x^{2}+x + K= (x+ \alpha)(x+ \bar{\alpha})$  which we can factorize where $\alpha$ is given by $$ \alpha = \frac{{1} + \sqrt{1-4K}}{2}, \quad \bar{\alpha} = \frac{1 - \sqrt{1-4K}}{2}.$$ One can get some relationships between polynomials which produce prime in the field $\mathbb Q(\sqrt{-n})$. Question is if a polynomial produces a prime, then will $X_{n}$ as defined above be a PID?",,"['abstract-algebra', 'prime-numbers']"
91,The hookrightarrow and its meaning,The hookrightarrow and its meaning,,Very weirdly I found a $\hookrightarrow $ in my Algebra book just like that. Could someone please just tell me what it means? Sorry for this maybe stupid question in advance but very much appreciate your help!,Very weirdly I found a in my Algebra book just like that. Could someone please just tell me what it means? Sorry for this maybe stupid question in advance but very much appreciate your help!,\hookrightarrow ,"['abstract-algebra', 'notation']"
92,$\textbf Z[\sqrt{pq}]$ is not a UFD if $\left( \frac{q}p \right) = -1$ and $p \equiv 1 \pmod 4$. [duplicate],is not a UFD if  and . [duplicate],\textbf Z[\sqrt{pq}] \left( \frac{q}p \right) = -1 p \equiv 1 \pmod 4,This question already has answers here : Why isn't $\mathbb Z [\sqrt{pq}]$ a factorial domain (2 answers) Closed 5 years ago . Let $p$ and $q$ be primes such that $p \equiv 1 \pmod 4$ and $\left( \frac q p \right) = -1$. Show that $\textbf Z[\sqrt {pq}]$ is not a UFD. I tried some examples like $p=5$ and $q = 2$. But I have no clue about the general case. Any hint?,This question already has answers here : Why isn't $\mathbb Z [\sqrt{pq}]$ a factorial domain (2 answers) Closed 5 years ago . Let $p$ and $q$ be primes such that $p \equiv 1 \pmod 4$ and $\left( \frac q p \right) = -1$. Show that $\textbf Z[\sqrt {pq}]$ is not a UFD. I tried some examples like $p=5$ and $q = 2$. But I have no clue about the general case. Any hint?,,"['abstract-algebra', 'ring-theory', 'algebraic-number-theory', 'unique-factorization-domains']"
93,Is my paper on a number system that allows arithmetic on 3D vectors useful?,Is my paper on a number system that allows arithmetic on 3D vectors useful?,,"I have constructed a number system similar to the quaternions, but with three dimensions, not four, ie vectors of the form $(x, y, z)$. It has fairly well-behaved multiplication and division and every non-zero element has an inverse. My algebra is not commutative, not associative and it only obeys the distributive law on one side. Where can I publish my paper on this number system? My corrected paper is here: http://soler7.com/IFAQ/Ternions.doc Definition of vector multiplication If $v$ and $q$ are two vectors, $v = (a, b, c)$ and $q = (x, y, z)$, then their product, $vq = (a, b, c)(x, y, z)$ is defined to be $$ (ax - by - cz, b|q| + yw, c|q| + zw ), $$ where $|q| = \sqrt{x^2 + y^2 + z^2}$ and $$ w = a + \frac{(by + cz)(x - |q|)}{(y^2 + z^2)} $$ provided $y$ or $z$ is non-zero. If $y$ and $z$ are both zero then $vq = (xa, xb, xc)$. The reciprocal of the vector is given by $$ (x, y, z)^{-1} = |q|^{-2}(x, -y, -z). $$ Proof that this acts as the reciprocal of $q$ on the right. $$ qq^{-1} = (x, y, z)|q|^{-2}(x, -y, -z) = |q|^{-2}(x, y, z)(x, -y, -z) $$ using the definition of multiplication, i.e. $$ (a, b, c)(x, y, z) = (ax - by - cz, b|q| + yw, c|q| + zw), $$ we obtain \begin{align*}   qq^{-1}   &= |q|^{-2} \left(   \begin{gathered}     x^2 + y^2 + z^2, \\   y|q| - y\Bigl(x + \frac{(-y^2 -z^2)(x - |q|)}{y^2 + z^2}\Bigr), \\   z|q| - z\Bigl(x + \frac{(-y^2 -z^2)(x - |q|)}{y^2 + z^2}\Bigr)   \end{gathered}   \right) \\   &= |q|^{-2} \bigl(x^2 + y^2 + z^2,\ y|q| - yx + y(x - |q|),\ z|q| - zx + z(x - |q|)\bigr) \\   &= |q|^{-2}(x^2 + y^2 + z^2, 0, 0) \\   &= (1, 0, 0) \end{align*} as required. Proof that that $|q|^{-2}(x, -y, -z)$ acts as the reciprocal of $q$ on the left. $$ q^{-1}q = |q|^{-2}(x, -y, -z)(x, y, z). $$ Using the definition of multiplication: \begin{align*}   q^{-1}q   &= |q|^{-2}\left(   \begin{gathered}     x^2 + y^2 + z^2, \\   - y|q| + y\Bigl(x + \frac{(- y^2 - z^2)(x - |q|)}{y^2 + z^2}\Bigr), \\   - z|q| + z\Bigl(x + \frac{(- y^2 - z^2)(x - |q|)}{y^2 + z^2}\Bigr)   \end{gathered}   \right) \\   &= |q|^{-2} \bigl(x^2 + y^2 + z^2, -y|q| + yx - y(x - |q|), -z|q| + zx  - z(x - |q|)\bigr) \\   &= (1, 0, 0) \end{align*} as required. So the vector $|q|^{-2}(x, -y, -z)$ behaves correctly as the reciprocal of $(x, y, z)$ on both the left and the right. All non-zero vectors have reciprocals. If $y = z = 0$ but $x$ is non-zero, then the reciprocal of $(x, 0, 0)$ is simply $(1/x, 0, 0)$. Although I am saddened to see that my number system is not associative, I am very grateful to Arctic Tern for pointing this out. The beauty and terror of mathematics is that when you are right, no-one can contradict you, and when you are wrong there is no come-back from that either. All I can do is acknowledge the correctness of everything that Arctic Tern has written. Incidentally, I am very impressed by the level of helpfulness and the positive attitude that is the case here on stackexchange. It is a lovely contrast to the acrimony and pettiness that is so common on other parts of the Net. In particular, I am grateful that people have overlooked my clumsy notation. In the future I promise to use Latex! Geometry may be basic to mathematics and to mathematical intuition, but unfortunately it is my Achilles heel. I had to laboriously work out the products of (1,0,0), (0,1,0) and (0,0,1) to see that (aa)b is not equal to a(ab), just as Arctic Tern asserted. I have to withdraw my comment about wikipedia, as my system is not a near-field. Maybe it is a near-near-field? I have one lingering question, namely where is the error in my faulty proof of associativity? It goes like this: ""The associative law holds for ternion multiplication because, apart from a modulus adjustment, which is clearly associative, it does nothing other than a three-dimensional rotation. Three-dimensional rotations form a group, which means that they are associative."" I agree that someone else has probably discovered the same number system. I have looked, but was unable to find it. Also, what did Arctic Tern mean by ""with only partially retained versions"" in the 5th paragraph under ""some more comments""?","I have constructed a number system similar to the quaternions, but with three dimensions, not four, ie vectors of the form $(x, y, z)$. It has fairly well-behaved multiplication and division and every non-zero element has an inverse. My algebra is not commutative, not associative and it only obeys the distributive law on one side. Where can I publish my paper on this number system? My corrected paper is here: http://soler7.com/IFAQ/Ternions.doc Definition of vector multiplication If $v$ and $q$ are two vectors, $v = (a, b, c)$ and $q = (x, y, z)$, then their product, $vq = (a, b, c)(x, y, z)$ is defined to be $$ (ax - by - cz, b|q| + yw, c|q| + zw ), $$ where $|q| = \sqrt{x^2 + y^2 + z^2}$ and $$ w = a + \frac{(by + cz)(x - |q|)}{(y^2 + z^2)} $$ provided $y$ or $z$ is non-zero. If $y$ and $z$ are both zero then $vq = (xa, xb, xc)$. The reciprocal of the vector is given by $$ (x, y, z)^{-1} = |q|^{-2}(x, -y, -z). $$ Proof that this acts as the reciprocal of $q$ on the right. $$ qq^{-1} = (x, y, z)|q|^{-2}(x, -y, -z) = |q|^{-2}(x, y, z)(x, -y, -z) $$ using the definition of multiplication, i.e. $$ (a, b, c)(x, y, z) = (ax - by - cz, b|q| + yw, c|q| + zw), $$ we obtain \begin{align*}   qq^{-1}   &= |q|^{-2} \left(   \begin{gathered}     x^2 + y^2 + z^2, \\   y|q| - y\Bigl(x + \frac{(-y^2 -z^2)(x - |q|)}{y^2 + z^2}\Bigr), \\   z|q| - z\Bigl(x + \frac{(-y^2 -z^2)(x - |q|)}{y^2 + z^2}\Bigr)   \end{gathered}   \right) \\   &= |q|^{-2} \bigl(x^2 + y^2 + z^2,\ y|q| - yx + y(x - |q|),\ z|q| - zx + z(x - |q|)\bigr) \\   &= |q|^{-2}(x^2 + y^2 + z^2, 0, 0) \\   &= (1, 0, 0) \end{align*} as required. Proof that that $|q|^{-2}(x, -y, -z)$ acts as the reciprocal of $q$ on the left. $$ q^{-1}q = |q|^{-2}(x, -y, -z)(x, y, z). $$ Using the definition of multiplication: \begin{align*}   q^{-1}q   &= |q|^{-2}\left(   \begin{gathered}     x^2 + y^2 + z^2, \\   - y|q| + y\Bigl(x + \frac{(- y^2 - z^2)(x - |q|)}{y^2 + z^2}\Bigr), \\   - z|q| + z\Bigl(x + \frac{(- y^2 - z^2)(x - |q|)}{y^2 + z^2}\Bigr)   \end{gathered}   \right) \\   &= |q|^{-2} \bigl(x^2 + y^2 + z^2, -y|q| + yx - y(x - |q|), -z|q| + zx  - z(x - |q|)\bigr) \\   &= (1, 0, 0) \end{align*} as required. So the vector $|q|^{-2}(x, -y, -z)$ behaves correctly as the reciprocal of $(x, y, z)$ on both the left and the right. All non-zero vectors have reciprocals. If $y = z = 0$ but $x$ is non-zero, then the reciprocal of $(x, 0, 0)$ is simply $(1/x, 0, 0)$. Although I am saddened to see that my number system is not associative, I am very grateful to Arctic Tern for pointing this out. The beauty and terror of mathematics is that when you are right, no-one can contradict you, and when you are wrong there is no come-back from that either. All I can do is acknowledge the correctness of everything that Arctic Tern has written. Incidentally, I am very impressed by the level of helpfulness and the positive attitude that is the case here on stackexchange. It is a lovely contrast to the acrimony and pettiness that is so common on other parts of the Net. In particular, I am grateful that people have overlooked my clumsy notation. In the future I promise to use Latex! Geometry may be basic to mathematics and to mathematical intuition, but unfortunately it is my Achilles heel. I had to laboriously work out the products of (1,0,0), (0,1,0) and (0,0,1) to see that (aa)b is not equal to a(ab), just as Arctic Tern asserted. I have to withdraw my comment about wikipedia, as my system is not a near-field. Maybe it is a near-near-field? I have one lingering question, namely where is the error in my faulty proof of associativity? It goes like this: ""The associative law holds for ternion multiplication because, apart from a modulus adjustment, which is clearly associative, it does nothing other than a three-dimensional rotation. Three-dimensional rotations form a group, which means that they are associative."" I agree that someone else has probably discovered the same number system. I have looked, but was unable to find it. Also, what did Arctic Tern mean by ""with only partially retained versions"" in the 5th paragraph under ""some more comments""?",,"['abstract-algebra', 'rotations', 'quaternions', 'division-algebras']"
94,Do the non-units in a commutative ring form an ideal?,Do the non-units in a commutative ring form an ideal?,,"Do the non-units in a commutative ring form an ideal? The following are my thoughts on this. Have I made any incorrect assumptions? Let $R$ be a commutative ring. Let $a, b \in N$ with $N$ being the set of non-units in $R$. We must show the following to prove $N$ is an ideal - $0 \in N$ $a + b \in N$ $-a \in N$ $ar, ra \in N \ \forall r \in R$ 1. $0 \in N$ I.e. $0$ a non-unit. This is true as $\nexists \ 0^{-1}$ such that $0 \cdot 0^{-1} = 1$ 2. $a + b \in N$ Assume $a + b$ is a unit. Then $\exists \ g \in R$, $g \neq 0$ such that $(a + b)g = 1$ $\implies$ $ag + bg = 1$ For this to be true either $a$ or $b$ must be $0$. Consider the case when $a = 0$. Then we have $bg = 1$. But this is a contradiction as $b$ is a non-unit. Hence $\nexists \ g \in R$ such that $(a + b)g = 1$. Similarly for when $b = 0$. Therefore, $a + b$ is a non-unit. 3. $-a \in N$ I.e. Does there exist $(-a) \in N$ such that $a + (-a) = 0$? Assume $-a$ is a unit. Then $\exists \ g \in R, g \neq$ 0, such that $(-a)g = 1$ Consider $a + (-a) = 0$ Multiplying both sides by $g$ we get $(a + (-a))g = 0 \cdot g$ $ag + (-a)g = 0$ $ag + 1 = 0$ $-ag = 1$ $a(-g) = 1$ But this is a contradiction as $a$ is a non-unit. Hence $(-a)$ is a non-unit. 4. $ar, ra \in N \ \forall r \in R$ Assume $ar$ is a unit. Then $\exists \ g \in R, g \neq$ 0, such that $(ar)g = 1$ I.e. $a(gr) = 1$ But this is a contradiction as $a$ is a non-unit. Hence $ar$ is a non-unit. So, to conclude, the non-units in a commutative ring do form an ideal. Are my workings correct?","Do the non-units in a commutative ring form an ideal? The following are my thoughts on this. Have I made any incorrect assumptions? Let $R$ be a commutative ring. Let $a, b \in N$ with $N$ being the set of non-units in $R$. We must show the following to prove $N$ is an ideal - $0 \in N$ $a + b \in N$ $-a \in N$ $ar, ra \in N \ \forall r \in R$ 1. $0 \in N$ I.e. $0$ a non-unit. This is true as $\nexists \ 0^{-1}$ such that $0 \cdot 0^{-1} = 1$ 2. $a + b \in N$ Assume $a + b$ is a unit. Then $\exists \ g \in R$, $g \neq 0$ such that $(a + b)g = 1$ $\implies$ $ag + bg = 1$ For this to be true either $a$ or $b$ must be $0$. Consider the case when $a = 0$. Then we have $bg = 1$. But this is a contradiction as $b$ is a non-unit. Hence $\nexists \ g \in R$ such that $(a + b)g = 1$. Similarly for when $b = 0$. Therefore, $a + b$ is a non-unit. 3. $-a \in N$ I.e. Does there exist $(-a) \in N$ such that $a + (-a) = 0$? Assume $-a$ is a unit. Then $\exists \ g \in R, g \neq$ 0, such that $(-a)g = 1$ Consider $a + (-a) = 0$ Multiplying both sides by $g$ we get $(a + (-a))g = 0 \cdot g$ $ag + (-a)g = 0$ $ag + 1 = 0$ $-ag = 1$ $a(-g) = 1$ But this is a contradiction as $a$ is a non-unit. Hence $(-a)$ is a non-unit. 4. $ar, ra \in N \ \forall r \in R$ Assume $ar$ is a unit. Then $\exists \ g \in R, g \neq$ 0, such that $(ar)g = 1$ I.e. $a(gr) = 1$ But this is a contradiction as $a$ is a non-unit. Hence $ar$ is a non-unit. So, to conclude, the non-units in a commutative ring do form an ideal. Are my workings correct?",,"['abstract-algebra', 'ring-theory']"
95,number of subgroups index $p$ equals number of subgroups order $p$,number of subgroups index  equals number of subgroups order,p p,"I'm doing an exercise in Dummit's book ""Abstract Algebra"" and stuck for a long time. I think I'm doing in the right way but I can't finish it. Hope someone can help me. I really appreciate it. Let $A$ be a finite abelian group and let $p$ be a prime. Let $A^{p} = \{a^{p}\mid a \in A\}$ and $A_{p} = \{x\mid x^{p} = 1\}$. Prove that $A/A^{p}$ is isomorphic to $A_{p}$, and the number of subgroups of $A$ of order $p$ equals the number of subgroups of $A$ of index $p$. I can prove that $A/A^{p}$ is isomorphic to $A_{p}$, and every subgroups order $p$ of $A$ must be subgroups order $p$ of $A_{p}$. So the number of subgroups order $p$ of $A$ equals number of subgroups order $p$ of $A_{p}$. Moreover because of the previous result, we must have this number equals number of subgroups order $p$ in $A/A^{p}$. So we try to build a bijection from the set of all subgroups order $p$ of $A/A^{p}$ into set of all subgroups index $p$ of $A$. I think that it's possible, because every subgroup $N$ of $A$ is normal and $A/N$ is a group order $p$. Can anyone help me go on in this way to solve this problem. I know there's a solution in Project Crazy Project, but I think that solution is cumbersome and not beautiful.  Thanks","I'm doing an exercise in Dummit's book ""Abstract Algebra"" and stuck for a long time. I think I'm doing in the right way but I can't finish it. Hope someone can help me. I really appreciate it. Let $A$ be a finite abelian group and let $p$ be a prime. Let $A^{p} = \{a^{p}\mid a \in A\}$ and $A_{p} = \{x\mid x^{p} = 1\}$. Prove that $A/A^{p}$ is isomorphic to $A_{p}$, and the number of subgroups of $A$ of order $p$ equals the number of subgroups of $A$ of index $p$. I can prove that $A/A^{p}$ is isomorphic to $A_{p}$, and every subgroups order $p$ of $A$ must be subgroups order $p$ of $A_{p}$. So the number of subgroups order $p$ of $A$ equals number of subgroups order $p$ of $A_{p}$. Moreover because of the previous result, we must have this number equals number of subgroups order $p$ in $A/A^{p}$. So we try to build a bijection from the set of all subgroups order $p$ of $A/A^{p}$ into set of all subgroups index $p$ of $A$. I think that it's possible, because every subgroup $N$ of $A$ is normal and $A/N$ is a group order $p$. Can anyone help me go on in this way to solve this problem. I know there's a solution in Project Crazy Project, but I think that solution is cumbersome and not beautiful.  Thanks",,"['abstract-algebra', 'group-theory', 'finite-groups', 'abelian-groups']"
96,Intuition on group actions,Intuition on group actions,,"I'm trying to get more intuition on this definition: Let $(G,{}\circ{})$ be a group with identity element $e$ .  A group action is a mapping $\varphi : G \times X \to X \,$ such that: \begin{alignat} {}& \forall x \in X, &\varphi(e, x) = x. \tag{1}\\ {}& \forall g,h \in G, \forall x \in X, &\varphi((g\circ h),x) = \varphi(g,\varphi(h,x)). \tag{2} \end{alignat} Is there some kind of intuition behind this definitions ? From the examples I've seen it seems like you always get a bijective mapping $X →X:x↦g(x)$ . Is this the idea ? That $G$ acts on $X$ if all elements of $g$ are bijective mappings from $X→X$ ?",I'm trying to get more intuition on this definition: Let be a group with identity element .  A group action is a mapping such that: Is there some kind of intuition behind this definitions ? From the examples I've seen it seems like you always get a bijective mapping . Is this the idea ? That acts on if all elements of are bijective mappings from ?,"(G,{}\circ{}) e \varphi : G \times X \to X \, \begin{alignat}
{}& \forall x \in X, &\varphi(e, x) = x. \tag{1}\\
{}& \forall g,h \in G, \forall x \in X, &\varphi((g\circ h),x) = \varphi(g,\varphi(h,x)). \tag{2}
\end{alignat} X →X:x↦g(x) G X g X→X","['abstract-algebra', 'group-theory', 'intuition']"
97,Let $n \geq 1$ be an odd integer. Show that $D_{2n}\cong \mathbb{Z}_2 \times D_n$.,Let  be an odd integer. Show that .,n \geq 1 D_{2n}\cong \mathbb{Z}_2 \times D_n,"Let $n \geq 1$ be an odd integer. Show that $D_{2n}\cong \mathbb{Z}_2 \times D_n$ . I define a map $$\phi:D_{2n} \rightarrow \mathbb{Z}_2 \times D_n$$ by $R \mapsto (0,r^{\frac{n+1}{2}})$ and $M \mapsto(1,m)$ . Then I am stuck at showing the map is bijective. By the way, do we need to show that $(0,r^{\frac{n+1}{2}})(1,m)(0,r^{\frac{n+1}{2}})^{-1}=(1,m)$ ? If we can prove this, then what can we conclude?","Let be an odd integer. Show that . I define a map by and . Then I am stuck at showing the map is bijective. By the way, do we need to show that ? If we can prove this, then what can we conclude?","n \geq 1 D_{2n}\cong \mathbb{Z}_2 \times D_n \phi:D_{2n} \rightarrow \mathbb{Z}_2 \times D_n R \mapsto (0,r^{\frac{n+1}{2}}) M \mapsto(1,m) (0,r^{\frac{n+1}{2}})(1,m)(0,r^{\frac{n+1}{2}})^{-1}=(1,m)","['abstract-algebra', 'group-theory', 'finite-groups', 'dihedral-groups', 'direct-product']"
98,Polynomial ring with arbitrarily many variables in ZF,Polynomial ring with arbitrarily many variables in ZF,,"For a given field $k$ and a set $X$ we want to define the ring $k[X]$ of polynomials with $X$ as the set of variables. We do not assume $X$ to be finite. And we want to do this without employing axiom of choice. Informally, the elements of $k[X]$ will be finite sums of monomials of the form $cx_1^{k_1}\dots x_n^{k_n}$ , where each monomial is determined by a coefficient $c\in k$ , finitely many elements $x_1,\dots,x_n\in X$ and the exponents $k_1,\dots,k_n$ , which are positive integers. Addition and multiplication of polynomials from $k[X]$ will be defined in the natural way. However, we also should be able to describe this algebraic structure more formally. Especially if we are trying to use it in some proof in the axiomatic system ZF. In this case it is also important to check that we have not used AC anywhere in the proof. (Using Axiom of Choice can easily be overlooked, especially if somebody is used to work in ZFC rather than ZF, i.e., without the restriction that AC should be avoided.) To explain a bit better what I mean, this is similar to defining the polynomial ring $k[x]$ of polynomials in a single variable $x$ . Informally, we view polynomials as expressions of the form $a_nx^n+\dots+a_1x+a_0$ (with $a_i\in k$ ). And we will also write them in this way. But formally they are sequences of elements of $K$ with finite support. I will also provide below a suggestion how to construct $k[X]$ in ZF. I would be interested in any comments on my approach, but also if there are different ways to do this, I'd be glad to hear about them. This cropped up in a discussion with some colleagues of mine. Transfinite induction and direct limit One colleague suggested the following approach, which clearly uses AC (in the form of the well-ordering theorem ). But he said that this is the construction of $k[X]$ which seems the most natural to him. We take any well-ordering of the set $X=\{x_\beta; \beta<\alpha\}$ . By a transfinite induction we define rings $k_\beta$ for $\beta\le\alpha$ and also an embeddings $k_\beta \hookrightarrow k_{\beta'}$ for any $\beta<\beta'<\alpha$ . The ring $k_\beta$ is supposed to represent the polynomials using only variables $x_\gamma$ for $\gamma\le\beta$ . We put $k_0=k[x_0]$ . Similarly if $\beta$ is a successor ordinal we can define $k_\beta=k_{\beta-1}[x_\beta]$ . If $\beta$ is a limit ordinal, then we can take $k_\beta$ as a direct limit of $k_\gamma$ , $\gamma<\beta$ . Then the ring $k_\alpha$ is $k[X]$ which we wanted to construct. It is not immediately clear to me whether the proof can be simplified in the way that the direct limit can be replaced by union. However, I do not consider this to be an important difference, since using direct limit (especially in such a simple case, with linear order and embeddings) seem to me to be a rather standard approach for this type of constructions. And anybody with enough mathematical maturity to study a proofs of this level will probably not have a problem with the notion of direct limit. The fact that this is indeed a ring (or even integral domain) follows from the fact that these properties are preserved by this simple version of direct limits. (I.e., direct limit based on linearly ordered system of rings with embeddings between them. This does not differ substantially from the proof that union of chain of rings is a ring.) Functions with finite support I have suggested this approach, which is more closely modeled after the case of ring in a single variable. Unless I missed something, this can be done in ZF, i.e., without use of ZFC. Let us first try to definite the set $M$ of all monomials of the form $x_1^{k_1}\dots x_n^{k_n}$ . (I.e., the monomials with the coefficient $1$ .) Every such monomial is uniquely determined by a finite subset $F\subseteq X$ and a function $g: F\mapsto\mathbb N$ , where $\mathbb N=\{1,2,\dots\}$ . Or, if you will, $\mathbb N=\omega\setminus\{0\}$ . (Since we are talking about finite sets, it might be worth mentioning that there are several notions of finite set in ZF . We take the standard one, which is sometimes called Tarski-finite or Kuratowski-finite. This notion of finiteness is well behaved. For our purposes it is important to know that union of finite set of finite sets is again finite and the same is true for Cartesian product.) So we can get $M$ as a set of all pairs $(F,g)$ with the properties described above. Existence of such sets can be defined in ZF in a rather straightforward manner. (All properties of $F$ and $g$ can be described by a formula in a language of set theory. Clearly $F\in\mathcal P(X)$ . Or we can use the set $\mathcal P^{<\omega}(X)$ of finite subsets of $X$ instead. The function $g$ belongs to the set of all functions from such $F$ 's to $\mathbb N$ . For each $F$ we have the set $\mathbb N^F$ consisting of all functions $F\to\mathbb N$ . Then we can simply take the union $G=\bigcup\limits_{F\in\mathcal P(X)} \mathbb N^F$ , based on axiom of union. The we use axiom scheme of specification to get only those pairs from $\mathcal P(X)\times G$ which have the required properties.) Now we have the set $M$ . We want to model somehow the finite sums of elements from $M$ multiplied by a coefficents from $k$ . To this end we simply take the functions from $M$ to $k$ with finite support. So far we have only defined the underlying set $k[X]$ . We still need to define addition, multiplication, verify that this is integral domain. However, any polynomial $p\in k[X]$ only uses finitely many variables, since we have finitely many monomials and each of them only contains finitely many variables. If we are verifying closure under addition or multiplication, or some properties of integral domain such as associativity or distributivity, then any such condition only includes finitely many polynomials and thus we have only finitely many variables. So we can look at this condition as property of polynomials in $k[F]$ , where $F$ is some finite subsets. Assuming we already know that polynomial ring in finitely many variables over a field $k$ is an integral domain, this argument can be used to argue that $k[X]$ is an integral domain, too. The above discussion occurred in connection with the proof of Andreas Blass' result that existence of Hamel basis for vector space over arbitrary fields implies Axiom of Choice. This proof can be found for example in the references below. It is also briefly described in this answer . In this proof the polynomials from $k[X]$ are used. (Then the field $k(X)$ of all rational functions in variables from $X$ is created - in the other words, the quotient field of $k[X]$ . And the proof than uses existence of a Hamel basis of $k(X)$ considered as a vector space over a particular subfield of $k(X)$ .) Unless I missed something, the proofs given there do not discuss whether $k[X]$ can be constructed without AC. Which suggests that the authors considered this point to be simple enough to be filled in by a reader. So I assume that proof of this fact should not be too difficult. (Of course, if you know of another reference for a proof this results which also discusses this issue, I'd be glad to learn about it.) A. Blass: Existence of bases implies the axiom of choice. Contemporary  Mathematics, 31:31–33, 1984. Available on the author's website Theorem 5.4 in L. Halbeisen: Combinatorial Set Theory , Springer, 2012. The book is freely available on the author's website . Theorem 4.44 in H. Herrlich Axiom of choice , Springer, 2006, (Lecture Notes in Mathematics 1876). There are these related questions: Polynomial ring with uncountable indeterminates . Polynomial ring indexed by an arbitrary set. The answers given there can be considered somewhat similar to the approach I suggested above. However, it is not discussed there whether AC was used somewhere in this construction.","For a given field and a set we want to define the ring of polynomials with as the set of variables. We do not assume to be finite. And we want to do this without employing axiom of choice. Informally, the elements of will be finite sums of monomials of the form , where each monomial is determined by a coefficient , finitely many elements and the exponents , which are positive integers. Addition and multiplication of polynomials from will be defined in the natural way. However, we also should be able to describe this algebraic structure more formally. Especially if we are trying to use it in some proof in the axiomatic system ZF. In this case it is also important to check that we have not used AC anywhere in the proof. (Using Axiom of Choice can easily be overlooked, especially if somebody is used to work in ZFC rather than ZF, i.e., without the restriction that AC should be avoided.) To explain a bit better what I mean, this is similar to defining the polynomial ring of polynomials in a single variable . Informally, we view polynomials as expressions of the form (with ). And we will also write them in this way. But formally they are sequences of elements of with finite support. I will also provide below a suggestion how to construct in ZF. I would be interested in any comments on my approach, but also if there are different ways to do this, I'd be glad to hear about them. This cropped up in a discussion with some colleagues of mine. Transfinite induction and direct limit One colleague suggested the following approach, which clearly uses AC (in the form of the well-ordering theorem ). But he said that this is the construction of which seems the most natural to him. We take any well-ordering of the set . By a transfinite induction we define rings for and also an embeddings for any . The ring is supposed to represent the polynomials using only variables for . We put . Similarly if is a successor ordinal we can define . If is a limit ordinal, then we can take as a direct limit of , . Then the ring is which we wanted to construct. It is not immediately clear to me whether the proof can be simplified in the way that the direct limit can be replaced by union. However, I do not consider this to be an important difference, since using direct limit (especially in such a simple case, with linear order and embeddings) seem to me to be a rather standard approach for this type of constructions. And anybody with enough mathematical maturity to study a proofs of this level will probably not have a problem with the notion of direct limit. The fact that this is indeed a ring (or even integral domain) follows from the fact that these properties are preserved by this simple version of direct limits. (I.e., direct limit based on linearly ordered system of rings with embeddings between them. This does not differ substantially from the proof that union of chain of rings is a ring.) Functions with finite support I have suggested this approach, which is more closely modeled after the case of ring in a single variable. Unless I missed something, this can be done in ZF, i.e., without use of ZFC. Let us first try to definite the set of all monomials of the form . (I.e., the monomials with the coefficient .) Every such monomial is uniquely determined by a finite subset and a function , where . Or, if you will, . (Since we are talking about finite sets, it might be worth mentioning that there are several notions of finite set in ZF . We take the standard one, which is sometimes called Tarski-finite or Kuratowski-finite. This notion of finiteness is well behaved. For our purposes it is important to know that union of finite set of finite sets is again finite and the same is true for Cartesian product.) So we can get as a set of all pairs with the properties described above. Existence of such sets can be defined in ZF in a rather straightforward manner. (All properties of and can be described by a formula in a language of set theory. Clearly . Or we can use the set of finite subsets of instead. The function belongs to the set of all functions from such 's to . For each we have the set consisting of all functions . Then we can simply take the union , based on axiom of union. The we use axiom scheme of specification to get only those pairs from which have the required properties.) Now we have the set . We want to model somehow the finite sums of elements from multiplied by a coefficents from . To this end we simply take the functions from to with finite support. So far we have only defined the underlying set . We still need to define addition, multiplication, verify that this is integral domain. However, any polynomial only uses finitely many variables, since we have finitely many monomials and each of them only contains finitely many variables. If we are verifying closure under addition or multiplication, or some properties of integral domain such as associativity or distributivity, then any such condition only includes finitely many polynomials and thus we have only finitely many variables. So we can look at this condition as property of polynomials in , where is some finite subsets. Assuming we already know that polynomial ring in finitely many variables over a field is an integral domain, this argument can be used to argue that is an integral domain, too. The above discussion occurred in connection with the proof of Andreas Blass' result that existence of Hamel basis for vector space over arbitrary fields implies Axiom of Choice. This proof can be found for example in the references below. It is also briefly described in this answer . In this proof the polynomials from are used. (Then the field of all rational functions in variables from is created - in the other words, the quotient field of . And the proof than uses existence of a Hamel basis of considered as a vector space over a particular subfield of .) Unless I missed something, the proofs given there do not discuss whether can be constructed without AC. Which suggests that the authors considered this point to be simple enough to be filled in by a reader. So I assume that proof of this fact should not be too difficult. (Of course, if you know of another reference for a proof this results which also discusses this issue, I'd be glad to learn about it.) A. Blass: Existence of bases implies the axiom of choice. Contemporary  Mathematics, 31:31–33, 1984. Available on the author's website Theorem 5.4 in L. Halbeisen: Combinatorial Set Theory , Springer, 2012. The book is freely available on the author's website . Theorem 4.44 in H. Herrlich Axiom of choice , Springer, 2006, (Lecture Notes in Mathematics 1876). There are these related questions: Polynomial ring with uncountable indeterminates . Polynomial ring indexed by an arbitrary set. The answers given there can be considered somewhat similar to the approach I suggested above. However, it is not discussed there whether AC was used somewhere in this construction.","k X k[X] X X k[X] cx_1^{k_1}\dots x_n^{k_n} c\in k x_1,\dots,x_n\in X k_1,\dots,k_n k[X] k[x] x a_nx^n+\dots+a_1x+a_0 a_i\in k K k[X] k[X] X=\{x_\beta; \beta<\alpha\} k_\beta \beta\le\alpha k_\beta \hookrightarrow k_{\beta'} \beta<\beta'<\alpha k_\beta x_\gamma \gamma\le\beta k_0=k[x_0] \beta k_\beta=k_{\beta-1}[x_\beta] \beta k_\beta k_\gamma \gamma<\beta k_\alpha k[X] M x_1^{k_1}\dots x_n^{k_n} 1 F\subseteq X g: F\mapsto\mathbb N \mathbb N=\{1,2,\dots\} \mathbb N=\omega\setminus\{0\} M (F,g) F g F\in\mathcal P(X) \mathcal P^{<\omega}(X) X g F \mathbb N F \mathbb N^F F\to\mathbb N G=\bigcup\limits_{F\in\mathcal P(X)} \mathbb N^F \mathcal P(X)\times G M M k M k k[X] p\in k[X] k[F] F k k[X] k[X] k(X) X k[X] k(X) k(X) k[X]","['abstract-algebra', 'polynomials', 'ring-theory', 'set-theory', 'axiom-of-choice']"
99,Group with two generators of order 3 is finite,Group with two generators of order 3 is finite,,"A group $G$ is generated by two elements $a$ and $b$ such that for any $g\in G:$ $g^3=e$. Show that $G$ is finite. I don't understand how this is so. I don't know if $G$ is abelian, so I can construct more and more elements of the group of the form $abababababababab...$ which can't be simplified. So how do I know $G$ is finite? (If it was abelian, I could simply list all the elements $e,a,a^2,b,b^2,ab,a^2b,ab^2,a^2b^2$.)","A group $G$ is generated by two elements $a$ and $b$ such that for any $g\in G:$ $g^3=e$. Show that $G$ is finite. I don't understand how this is so. I don't know if $G$ is abelian, so I can construct more and more elements of the group of the form $abababababababab...$ which can't be simplified. So how do I know $G$ is finite? (If it was abelian, I could simply list all the elements $e,a,a^2,b,b^2,ab,a^2b,ab^2,a^2b^2$.)",,"['abstract-algebra', 'finite-groups']"
