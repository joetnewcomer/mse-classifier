,title_raw,title_text,title_latex,body_raw,body_text,body_latex,tags
0,"Is there a bijection from $[1, \infty)$ to half a unit sphere?",Is there a bijection from  to half a unit sphere?,"[1, \infty)","So I'm trying to find a bijection from $[1, \infty)$ to the surface of the unit hemisphere $U$ (without a flat bottom) in spherical coordinates. I define this below: $$U = \{(\theta, \psi).\, \theta \in [0,2\pi) \wedge \psi \in [0,\pi/2)\} - \{(\theta, 0).\, \theta \in (0,2\pi)\}$$ I defined $U$ like this because the coordinates $(0,0)$ and $(1,0)$ for example represent the same point on the surface ( $\theta$ is the annulus and $\psi$ is the polar angle.) What I'm thinking of doing is defining a function $f:[1,\infty) \to U$ where I partition the domain such that each element $e\in P \subseteq [1,\infty)$ where $P$ is a partition can be mapped to a point in one of the unique circular cross-sections of U. $P$ should also have the same number of elements as the number of points in this cross section. I also want $f(1) = (0,0)$ . I'm not sure if wording $f$ like this is sufficient to show that it is a bijection. Could I please have some help?",So I'm trying to find a bijection from to the surface of the unit hemisphere (without a flat bottom) in spherical coordinates. I define this below: I defined like this because the coordinates and for example represent the same point on the surface ( is the annulus and is the polar angle.) What I'm thinking of doing is defining a function where I partition the domain such that each element where is a partition can be mapped to a point in one of the unique circular cross-sections of U. should also have the same number of elements as the number of points in this cross section. I also want . I'm not sure if wording like this is sufficient to show that it is a bijection. Could I please have some help?,"[1, \infty) U U = \{(\theta, \psi).\, \theta \in [0,2\pi) \wedge \psi \in [0,\pi/2)\} - \{(\theta, 0).\, \theta \in (0,2\pi)\} U (0,0) (1,0) \theta \psi f:[1,\infty) \to U e\in P \subseteq [1,\infty) P P f(1) = (0,0) f",['functions']
1,Showing that a specific function is decreasing in one of its arguments,Showing that a specific function is decreasing in one of its arguments,,"Consider the function: $$\lambda(a,b,N) = \frac{N - \frac{a}{1-a^2}(3-a^N)(1-a^N)}{N - \frac{b}{1-b^2}(3-b^N)(1-b^N)},$$ where $N \in \mathbb{N}$ , and $a,b \in [0,1]$ . I want to show that, if $a<b$ , then $\lambda(a,b,N+1) - \lambda(a,b,N)<0$ for any $N$ . I'm >99.9% sure that this is true (based on evaluating this function on a grid of values for $a$ , $b$ , and $N$ ), but I'm finding very hard to prove that analytically.","Consider the function: where , and . I want to show that, if , then for any . I'm >99.9% sure that this is true (based on evaluating this function on a grid of values for , , and ), but I'm finding very hard to prove that analytically.","\lambda(a,b,N) = \frac{N - \frac{a}{1-a^2}(3-a^N)(1-a^N)}{N - \frac{b}{1-b^2}(3-b^N)(1-b^N)}, N \in \mathbb{N} a,b \in [0,1] a<b \lambda(a,b,N+1) - \lambda(a,b,N)<0 N a b N","['algebra-precalculus', 'functions']"
2,Find all function $f:\mathbb Q^+ \rightarrow \mathbb Q^+$ that satisfy $f(x)+f\bigg(\dfrac{1}{x}\bigg)=1$ and $f(2x+1)=\dfrac{f(x)}{2}$,Find all function  that satisfy  and,f:\mathbb Q^+ \rightarrow \mathbb Q^+ f(x)+f\bigg(\dfrac{1}{x}\bigg)=1 f(2x+1)=\dfrac{f(x)}{2},Find all function $f:\mathbb Q^+ \rightarrow \mathbb Q^+$ that satisfy $$f(x)+f\bigg(\dfrac{1}{x}\bigg)=1$$ and $$f(2x+1)=\dfrac{f(x)}{2}$$ for all $x$ in domain of $f$ My Approach: Put $x=1$ in $f(x)+f\bigg(\dfrac{1}{x}\bigg)=1$ $\implies$ $f(1)+f(1)=1$ $\implies$ $f(1)=\dfrac{1}{2}$ Now put $x=2$ in $f(x)+f\bigg(\dfrac{1}{x}\bigg)$ we obtain $f(2)+f(\dfrac{1}{2})=1 \cdots \cdots (1)$ Now using $f(2x+1)=\dfrac{f(x)}{2}\cdots \cdots (2)$ Put $x=\dfrac{1}{2}$ in eqation $(2)$ $\implies$ $f\bigg(\dfrac{1}{2}\bigg)=2f(2)$ Using above result in Equation $(1)$ we obtain $f(2)=\dfrac{1}{3}$ In similar fashion we finally obtain $f(x)=\dfrac{1}{1+x}$ For Integers I want to know Some other method to solve this I also tried using $g(x)=f(x-1)$ like this Help in solving a simple functional equation: $3f(2x+1)=f(x) + 5x$ . But couldn't get desired result Also this question is similar to Showing that $f(x)=\frac x{x+1}$ is the unique function satisfying $f(x)+f\left(\frac1x\right)=1$ and $f(2x)=2f\big(f(x)\big)$,Find all function that satisfy and for all in domain of My Approach: Put in Now put in we obtain Now using Put in eqation Using above result in Equation we obtain In similar fashion we finally obtain For Integers I want to know Some other method to solve this I also tried using like this Help in solving a simple functional equation: $3f(2x+1)=f(x) + 5x$ . But couldn't get desired result Also this question is similar to Showing that $f(x)=\frac x{x+1}$ is the unique function satisfying $f(x)+f\left(\frac1x\right)=1$ and $f(2x)=2f\big(f(x)\big)$,f:\mathbb Q^+ \rightarrow \mathbb Q^+ f(x)+f\bigg(\dfrac{1}{x}\bigg)=1 f(2x+1)=\dfrac{f(x)}{2} x f x=1 f(x)+f\bigg(\dfrac{1}{x}\bigg)=1 \implies f(1)+f(1)=1 \implies f(1)=\dfrac{1}{2} x=2 f(x)+f\bigg(\dfrac{1}{x}\bigg) f(2)+f(\dfrac{1}{2})=1 \cdots \cdots (1) f(2x+1)=\dfrac{f(x)}{2}\cdots \cdots (2) x=\dfrac{1}{2} (2) \implies f\bigg(\dfrac{1}{2}\bigg)=2f(2) (1) f(2)=\dfrac{1}{3} f(x)=\dfrac{1}{1+x} g(x)=f(x-1),"['algebra-precalculus', 'functions', 'functional-equations']"
3,Expressing algorithmic problem as a mathematical problem,Expressing algorithmic problem as a mathematical problem,,"I'm working in an algorithmic problem, which can surely be solved without any math involved, however I'm practicing (translating problems to formal definitions). I've done some baby steps however I need some expert help. algorithmic problem: Given Two stacks a and b , stack a contains some random amounts of numbers and stack b is empty. and given exactly 11 operations , sort the numbers with the lease amount of operations. ~ sa : swap the first 2 elements at the top of stack a . ~ sb : same as above for b . ~ ss : sa and sb at the same time. ~ pa : take the first element of b and put it at the top of a . ~ pb : same as above for b . ~ ra : shift up all elements of stack a by 1 The first element becomes the last one. ~ rb : same as above for b . ~ rr : ra and rb at the same time. ~ rra : shift down all elements of stack a by one The last element becomes the first one. ~ rrb : same as above for b . ~ rrr : rra and rrb at the same time. one cannot compare without comparison and in the algorithmic problem its allowed because its not a movement per se, in algo problem you can compare what you're at, and you're arrive at what you're at by looping, so I thought lets just say cab : compare any or all elements in a and b any time. my baby steps: given a sequence $A$ of $n$ numbers $\left\langle a_{1}, a_{2},    ...a_{n} \right\rangle $ and an empty sequence $B$ . produce a permutations (reordering) of the input $\left\langle a_{1}, a_{2},    ...a_{n} \right\rangle $ so that $\left\langle a_{1} \le  a_{2} \le    a_{n} \right\rangle $ My main problem is how to formalize these operations ( sa , sb ...) in math entities, one idea is functions, but  I'm not that good to complete the formalization. What do you think ?","I'm working in an algorithmic problem, which can surely be solved without any math involved, however I'm practicing (translating problems to formal definitions). I've done some baby steps however I need some expert help. algorithmic problem: Given Two stacks a and b , stack a contains some random amounts of numbers and stack b is empty. and given exactly 11 operations , sort the numbers with the lease amount of operations. ~ sa : swap the first 2 elements at the top of stack a . ~ sb : same as above for b . ~ ss : sa and sb at the same time. ~ pa : take the first element of b and put it at the top of a . ~ pb : same as above for b . ~ ra : shift up all elements of stack a by 1 The first element becomes the last one. ~ rb : same as above for b . ~ rr : ra and rb at the same time. ~ rra : shift down all elements of stack a by one The last element becomes the first one. ~ rrb : same as above for b . ~ rrr : rra and rrb at the same time. one cannot compare without comparison and in the algorithmic problem its allowed because its not a movement per se, in algo problem you can compare what you're at, and you're arrive at what you're at by looping, so I thought lets just say cab : compare any or all elements in a and b any time. my baby steps: given a sequence of numbers and an empty sequence . produce a permutations (reordering) of the input so that My main problem is how to formalize these operations ( sa , sb ...) in math entities, one idea is functions, but  I'm not that good to complete the formalization. What do you think ?","A n \left\langle a_{1}, a_{2},
   ...a_{n} \right\rangle  B \left\langle a_{1}, a_{2},
   ...a_{n} \right\rangle  \left\langle a_{1} \le  a_{2} \le
   a_{n} \right\rangle ","['sequences-and-series', 'functions', 'permutations', 'algorithms', 'sorting']"
4,Proving that an injective function has an inverse,Proving that an injective function has an inverse,,"My textbook defines inverses as follows. f and g are inverses of eachother if and only if $f( g(x) )=x $ for all $x\in$ dom( $g$ ) $g( f(x) )=x $ for all $x\in$ dom( $f$ ) It then proceeds to state that $f$ has an inverse if and only if it is injective (I am aware $f$ technically needs to be surjective as well but my course doesn't bother with that, and I know that any injective function can be made surjective by restricting the codomain) , and I am trying to prove that. I have already proven that injectivity $\Leftarrow$ invertibility as follows. Let $f$ be an invertible function. Then there exists a function $g$ such that $f( g(x) )=x $ for all $x\in$ dom( $g$ ) $g( f(x) )=x $ for all $x\in$ dom( $f$ ) Let $a,b\in$ dom( $f$ ) such that $f(a)=f(b)$ Then $a=g(f(a))=g(f(b))=b$ I now need to prove that injectivity $\implies$ invertibility . Intuitively, it makes sense because if $f$ maps every element of the domain to a unique element in the codomain, then the mapping of every element of the codomain (restricted to image of $f$ ) to the domain must also be a function. I don't know how to mathematically prove this using the given definitions, and would appreciate any hint on how to go about it.","My textbook defines inverses as follows. f and g are inverses of eachother if and only if for all dom( ) for all dom( ) It then proceeds to state that has an inverse if and only if it is injective (I am aware technically needs to be surjective as well but my course doesn't bother with that, and I know that any injective function can be made surjective by restricting the codomain) , and I am trying to prove that. I have already proven that injectivity invertibility as follows. Let be an invertible function. Then there exists a function such that for all dom( ) for all dom( ) Let dom( ) such that Then I now need to prove that injectivity invertibility . Intuitively, it makes sense because if maps every element of the domain to a unique element in the codomain, then the mapping of every element of the codomain (restricted to image of ) to the domain must also be a function. I don't know how to mathematically prove this using the given definitions, and would appreciate any hint on how to go about it.","f( g(x) )=x  x\in g g( f(x) )=x  x\in f f f \Leftarrow f g f( g(x) )=x  x\in g g( f(x) )=x  x\in f a,b\in f f(a)=f(b) a=g(f(a))=g(f(b))=b \implies f f","['functions', 'proof-writing']"
5,Find $f:f(xf(x)+f(y))=f(x)^2+y$,Find,f:f(xf(x)+f(y))=f(x)^2+y,"Find $f:f(xf(x)+f(y))=f(x)^2+y$ Domain and co-domain is real numbers I did the following: Let $s=f(0)$ Then $f(f(y))=s^2+y$ so $f$ is surjective Also, $f(x)=f(y)\implies f(xf(x)+f(y))=f(xf(x)+f(x))\implies x=y$ so $f$ is injective So, $f$ is bijective. Letting $f(x)=0, y=0$ we get $f(f(0))=0 \implies s=f(0)=0$ In fact $f(f(x))=x$ for ALL $x$ Letting $x=1,y=0$ we get $f(f(1))=f(1)^2\implies 1=f(1)^2 \implies f(1)=1$ or $f(1)=-1$ If $f(1)=1$ we let $x=1$ to get $f(y+1)=f(y)+1$ By induction this leads to $f(x)=x$ for all integers If $f(1)=-1$ we let $x=1$ to get $f(y-1)=f(y)+1$ By induction we get $f(x)=-x$ for all integers How to extend the domain over real numbers I don't know, any help would be appreciated","Find Domain and co-domain is real numbers I did the following: Let Then so is surjective Also, so is injective So, is bijective. Letting we get In fact for ALL Letting we get or If we let to get By induction this leads to for all integers If we let to get By induction we get for all integers How to extend the domain over real numbers I don't know, any help would be appreciated","f:f(xf(x)+f(y))=f(x)^2+y s=f(0) f(f(y))=s^2+y f f(x)=f(y)\implies f(xf(x)+f(y))=f(xf(x)+f(x))\implies x=y f f f(x)=0, y=0 f(f(0))=0 \implies s=f(0)=0 f(f(x))=x x x=1,y=0 f(f(1))=f(1)^2\implies 1=f(1)^2 \implies f(1)=1 f(1)=-1 f(1)=1 x=1 f(y+1)=f(y)+1 f(x)=x f(1)=-1 x=1 f(y-1)=f(y)+1 f(x)=-x",['functional-equations']
6,What is the correct way to read $f\circ g$?,What is the correct way to read ?,f\circ g,"Let $f : X \to Y$ and $g : Y \to Z$ be functions. We define the composition $g \circ f : X \to Z$ by $g \circ f(x) = g(f(x))$ for each $x \in X$. I have also heard the composition read out like this: ""The composition of $f$ with $g$ is . . ."" ""Consider the function $g$ composed with $f$ , given by . . ."" ""The function $g$ of $f$ is . . ."" Is this an appropriate way to speak of $g \circ f$? It sometimes happens that I (or my teachers) reverse the order of $f$ and $g$ when describing $g \circ f$ in any of the above ways. Surely, it can't be that both ways are correct. It doesn't cause confusion because the function being talked about is quite straightforward. But I'm still interested in knowing what the ""correct"" way/s to describe $g \circ f$ is/are among the above. I know that some people prefer the functional notation that operates the other way, but this question is not about that scenario.","Let $f : X \to Y$ and $g : Y \to Z$ be functions. We define the composition $g \circ f : X \to Z$ by $g \circ f(x) = g(f(x))$ for each $x \in X$. I have also heard the composition read out like this: ""The composition of $f$ with $g$ is . . ."" ""Consider the function $g$ composed with $f$ , given by . . ."" ""The function $g$ of $f$ is . . ."" Is this an appropriate way to speak of $g \circ f$? It sometimes happens that I (or my teachers) reverse the order of $f$ and $g$ when describing $g \circ f$ in any of the above ways. Surely, it can't be that both ways are correct. It doesn't cause confusion because the function being talked about is quite straightforward. But I'm still interested in knowing what the ""correct"" way/s to describe $g \circ f$ is/are among the above. I know that some people prefer the functional notation that operates the other way, but this question is not about that scenario.",,['functions']
7,On spherical distributions in abstract harmonic analysis,On spherical distributions in abstract harmonic analysis,,"I need a detailed and comprehensive guidance or possibly relevant materials that can help me in my research on spherical distributions on a locally compact group. Jacques Faraut wrote on spherical distributions on hyperbolic spaces in a famous paper in 1978, "" Distributions sphériques sur les espaces hyperboliques..."" Helminck A.G.  and Helminck G.F. did the same in 2002 and 2005. However I am still confused as per the true and explicit picture of a spherical distribution in abstract harmonic analysis.","I need a detailed and comprehensive guidance or possibly relevant materials that can help me in my research on spherical distributions on a locally compact group. Jacques Faraut wrote on spherical distributions on hyperbolic spaces in a famous paper in 1978, "" Distributions sphériques sur les espaces hyperboliques..."" Helminck A.G.  and Helminck G.F. did the same in 2002 and 2005. However I am still confused as per the true and explicit picture of a spherical distribution in abstract harmonic analysis.",,"['group-theory', 'functions', 'reference-request', 'book-recommendation', 'online-resources']"
8,"If $x^2 \geq y^2$, is the same true for their absolute values?","If , is the same true for their absolute values?",x^2 \geq y^2,"Apologies to ask a question that may be extremely obvious. I've got a multi-variable function: $f(x,y) = \sqrt{x^2 - y^2}$ I've been asked to find (and sketch) the domain of this function: Because of the $\sqrt{}$ , we know that $x^2 - y^2 \ge 0$ This means that $x^2 \ge y^2$ Would it be wrong to therefore write the domain as: $D(f) = $ { $(x,y): |x| \ge |y|$ }? The model answer was written as: $D(f) = $ { $x \ge \pm y, x \ge 0$ } ∪ { $x \le \pm y, x \le 0 $ } Would I be right in assuming that the answer was written like this to make sketching the domain easier , and both answers are in fact correct? Any clarity on this would be of great help!","Apologies to ask a question that may be extremely obvious. I've got a multi-variable function: I've been asked to find (and sketch) the domain of this function: Because of the , we know that This means that Would it be wrong to therefore write the domain as: { }? The model answer was written as: { } ∪ { } Would I be right in assuming that the answer was written like this to make sketching the domain easier , and both answers are in fact correct? Any clarity on this would be of great help!","f(x,y) = \sqrt{x^2 - y^2} \sqrt{} x^2 - y^2 \ge 0 x^2 \ge y^2 D(f) =  (x,y): |x| \ge |y| D(f) =  x \ge \pm y, x \ge 0 x \le \pm y, x \le 0 ","['multivariable-calculus', 'functions']"
9,Find the satisfying functions such that $f(x^5)=5f(x)$,Find the satisfying functions such that,f(x^5)=5f(x),Question is same as above. Let me write again. Find the satisfying functions such that $f(x^5)=5f(x)$ Clearly seen that $c\ln x$ satisfy the condition. Question is what else?,Question is same as above. Let me write again. Find the satisfying functions such that Clearly seen that satisfy the condition. Question is what else?,f(x^5)=5f(x) c\ln x,['functions']
10,Doubts in finding an example of a bijective function $f:\mathbb{N} \to \mathbb{Z}$,Doubts in finding an example of a bijective function,f:\mathbb{N} \to \mathbb{Z},"Give an example of a bijective function $f:\mathbb{N} \to \mathbb{Z}$ . I consider the function $f:\mathbb{N} \to \mathbb{Z}$ defined as $$f(n):=\begin{cases} n/2, \ \text{if} \ n \ \text{is even} \\ -(n+1)/2, \ \text{if} \ n \ \text{is odd}\end{cases}$$ I have two questions: one on my work and one more general. For my work: about injectivity, is easy to show that if $n_1$ and $n_2$ are both even or both odd then the implication $f(n_1)=f(n_2) \implies n_1=n_2$ holds; if, without loss of generality, it is $n_1$ even and $n_2$ odd I get that $f(n_1)=f(n_2) \iff n_1=-n_2-1$ which is absurd because $n_1 \ge 0$ and $-n_2-1 <0$ . So I deduced that this case can't occur, because it leads to a contradiction. So, since all the comtemplable cases implies that $f$ is injective, I deduced that $f$ is overall injective. Is this correct? Moreover, I have a ""logic"" doubt: I am not fully convinced why the fact that the case $n_1$ even and $n_2$ odd leads to a contradiction allow us to conclude that $f$ is injective and not to the conclusion that that $f$ is not injective. Is this related to the fact that injectivity is defined as an implication and so we assume $f(n_1)=f(n_2)$ true and it is not that we already know that $f(n_1)=f(n_2)$ is true, and so any contradiction obtained from that the assumption means that $f(n_1)=f(n_2)$ cannot occur and so it is not a contemplable case, so it must be excluded in the study of the injectivity and so, consequently, it doesn't give any information about the injectivity or not injectivity of the function and so this information is only related to all the other possible cases? For the general theory: some authours define $\mathbb{N}$ as the set of positive integers, hence for some authors $0 \notin \mathbb{N}$ ; how things work in this cases? I tried to use a similar function $$g(n):=\begin{cases} n/2, \ \text{if} \ n \ \text{is even} \\ -(n-1)/2, \ \text{if} \ n \ \text{is odd}\end{cases}$$ All works for the most part the same, except for the fact that I get a similar contradiction for $n_1$ even and $n_2$ odd given by the fact that $f(n_1)=f(n_2) \iff n_1+n_2=1$ and, since $n_1$ is even and $n_2$ is odd, this is possible only if $n_1=0$ and $n_2=1$ but $n_1$ can't be $0$ because, in this convention, $0 \notin \mathbb{N}$ . Could this be correct? If this is correct, it is normal that if some mathematical object (like functions) has a property (like injectivity) then this property could be independent of the way we define a certain set like the positive/nonnegative integers? Or this was just a lucky situation?","Give an example of a bijective function . I consider the function defined as I have two questions: one on my work and one more general. For my work: about injectivity, is easy to show that if and are both even or both odd then the implication holds; if, without loss of generality, it is even and odd I get that which is absurd because and . So I deduced that this case can't occur, because it leads to a contradiction. So, since all the comtemplable cases implies that is injective, I deduced that is overall injective. Is this correct? Moreover, I have a ""logic"" doubt: I am not fully convinced why the fact that the case even and odd leads to a contradiction allow us to conclude that is injective and not to the conclusion that that is not injective. Is this related to the fact that injectivity is defined as an implication and so we assume true and it is not that we already know that is true, and so any contradiction obtained from that the assumption means that cannot occur and so it is not a contemplable case, so it must be excluded in the study of the injectivity and so, consequently, it doesn't give any information about the injectivity or not injectivity of the function and so this information is only related to all the other possible cases? For the general theory: some authours define as the set of positive integers, hence for some authors ; how things work in this cases? I tried to use a similar function All works for the most part the same, except for the fact that I get a similar contradiction for even and odd given by the fact that and, since is even and is odd, this is possible only if and but can't be because, in this convention, . Could this be correct? If this is correct, it is normal that if some mathematical object (like functions) has a property (like injectivity) then this property could be independent of the way we define a certain set like the positive/nonnegative integers? Or this was just a lucky situation?","f:\mathbb{N} \to \mathbb{Z} f:\mathbb{N} \to \mathbb{Z} f(n):=\begin{cases} n/2, \ \text{if} \ n \ \text{is even} \\ -(n+1)/2, \ \text{if} \ n \ \text{is odd}\end{cases} n_1 n_2 f(n_1)=f(n_2) \implies n_1=n_2 n_1 n_2 f(n_1)=f(n_2) \iff n_1=-n_2-1 n_1 \ge 0 -n_2-1 <0 f f n_1 n_2 f f f(n_1)=f(n_2) f(n_1)=f(n_2) f(n_1)=f(n_2) \mathbb{N} 0 \notin \mathbb{N} g(n):=\begin{cases} n/2, \ \text{if} \ n \ \text{is even} \\ -(n-1)/2, \ \text{if} \ n \ \text{is odd}\end{cases} n_1 n_2 f(n_1)=f(n_2) \iff n_1+n_2=1 n_1 n_2 n_1=0 n_2=1 n_1 0 0 \notin \mathbb{N}","['analysis', 'functions']"
11,Asymptotic equivalence and kth derivative,Asymptotic equivalence and kth derivative,,"Let $f(x)=\log h(x)$ be a real analytic function on the open set $(0,\infty)$ . My question: If $f(x)\sim g(x)$ as $x\to 1$ where $g(x)$ is real analytic on $(0,\infty)$ . Prove that there is a constant $c$ such that $f^{(k)}(x)\sim c g^{(k)}(x)$ as $x\to 1$ . Next how do we find the value of this constant $c$ ? My try: Since $f(x)\sim g(x)$ as $x\to 1$ , so we have $$\lim_{x\to 1}\frac{f(x)}{g(x)}=1$$ Since $f$ is analytic at $x=1$ so it can be written as a power series $$f(x)=f_0+f_1(x-1)+f_2(x-1)^2+... $$ in an appropriate neighbourhood of $1$ . Also since $g$ is analytic at $x=1$ so it can be written as a power series $$g(x)=g_0+g_1(x-1)+g_2(x-1)^2+... $$ in an appropriate neighbourhood of $1$ . Since $f(x)\sim g(x)$ as $x\to 1$ , so $g_0=f_0$ . Since $f$ and $g$ are analytic, their derivatives are analytic as well and near $x=1$ they are given by the term-wise differentiated power series. Thus we have $$\lim_{x\to 1}\frac{f'(x)}{g'(x)}=\lim_{x\to 1} \frac{f_1+2f_2(x-1)+...}{g_1+2g_2(x-1)+...} $$ So we have $$\lim_{x\to 1}\frac{f'(x)}{g'(x)}=\frac{f_1}{g_1}$$ Similarly we get $$\lim_{x\to 1} \frac{(f(x))^{(k)}}{(g(x))^{(k)}}=\lim_{x\to 1}\frac{\sum_{n=k}^{\infty}\frac{f^{(n)}(1)}{(n-k)!}(x-1)^{n-k}}{ \sum_{n=k}^{\infty}\frac{g^{(n)}(1)}{(n-k)!}(x-1)^{n-k}   } $$ So we have $$\lim_{x\to 1} \frac{(f(x))^{(k)}}{(g(x))^{(k)}}=\frac{f^{(k)}(1)}{g^{(k)}(1)}$$ So taking $c=\frac{f^{(k)}(1)}{g^{(k)}(1)}$ we have $f^{(k)}(x)\sim c g^{(k)}(x)$ as $x\to 1$ . How do I find $f^{(k)}(1)$ given that $f(x)=\log h(x)$ ?","Let be a real analytic function on the open set . My question: If as where is real analytic on . Prove that there is a constant such that as . Next how do we find the value of this constant ? My try: Since as , so we have Since is analytic at so it can be written as a power series in an appropriate neighbourhood of . Also since is analytic at so it can be written as a power series in an appropriate neighbourhood of . Since as , so . Since and are analytic, their derivatives are analytic as well and near they are given by the term-wise differentiated power series. Thus we have So we have Similarly we get So we have So taking we have as . How do I find given that ?","f(x)=\log h(x) (0,\infty) f(x)\sim g(x) x\to 1 g(x) (0,\infty) c f^{(k)}(x)\sim c g^{(k)}(x) x\to 1 c f(x)\sim g(x) x\to 1 \lim_{x\to 1}\frac{f(x)}{g(x)}=1 f x=1 f(x)=f_0+f_1(x-1)+f_2(x-1)^2+...  1 g x=1 g(x)=g_0+g_1(x-1)+g_2(x-1)^2+...  1 f(x)\sim g(x) x\to 1 g_0=f_0 f g x=1 \lim_{x\to 1}\frac{f'(x)}{g'(x)}=\lim_{x\to 1} \frac{f_1+2f_2(x-1)+...}{g_1+2g_2(x-1)+...}  \lim_{x\to 1}\frac{f'(x)}{g'(x)}=\frac{f_1}{g_1} \lim_{x\to 1} \frac{(f(x))^{(k)}}{(g(x))^{(k)}}=\lim_{x\to 1}\frac{\sum_{n=k}^{\infty}\frac{f^{(n)}(1)}{(n-k)!}(x-1)^{n-k}}{ \sum_{n=k}^{\infty}\frac{g^{(n)}(1)}{(n-k)!}(x-1)^{n-k}   }  \lim_{x\to 1} \frac{(f(x))^{(k)}}{(g(x))^{(k)}}=\frac{f^{(k)}(1)}{g^{(k)}(1)} c=\frac{f^{(k)}(1)}{g^{(k)}(1)} f^{(k)}(x)\sim c g^{(k)}(x) x\to 1 f^{(k)}(1) f(x)=\log h(x)","['calculus', 'functions']"
12,if oscillation of a function is zero then the function has a right limit.,if oscillation of a function is zero then the function has a right limit.,,"I was given this question: $$\omega_f(I)=sup_{x,y\in I}{|f(x)-f(y)|}$$ True/False: $$\lim_{\epsilon\rightarrow0}{\omega_f((x_0,x_0+\epsilon))}=0 \iff \lim_{x\rightarrow x_0^+}{f(x)} \in \mathbb{R}$$ I think I proved the direction from right to left: if $\lim_{x\rightarrow x_0^+}{f(x)} =L$ then $$\forall \epsilon_0 \exists \delta_0 : x_0<x<x_0+\delta_0 \rightarrow |f(x)-L|<\epsilon_0$$ Therefore, choose $\epsilon_0/2, \forall x,y\in(x_0,x_0+\delta_0) \rightarrow |f(x)-f(y)|<\epsilon_0 \rightarrow \omega_f(I)\le\epsilon_0$ (triangle inequality) therefore when $\epsilon\le\delta_0$ $\rightarrow \lim_{\epsilon\rightarrow0}{\omega_f((x_0,x_0+\epsilon))}=0$ I tried doing the second direction but I can't determine what limit the function would even have, I also couldn't think of any counterexamples, any help would be greatly appreciated!","I was given this question: True/False: I think I proved the direction from right to left: if then Therefore, choose (triangle inequality) therefore when I tried doing the second direction but I can't determine what limit the function would even have, I also couldn't think of any counterexamples, any help would be greatly appreciated!","\omega_f(I)=sup_{x,y\in I}{|f(x)-f(y)|} \lim_{\epsilon\rightarrow0}{\omega_f((x_0,x_0+\epsilon))}=0 \iff \lim_{x\rightarrow x_0^+}{f(x)} \in \mathbb{R} \lim_{x\rightarrow x_0^+}{f(x)} =L \forall \epsilon_0 \exists \delta_0 : x_0<x<x_0+\delta_0 \rightarrow |f(x)-L|<\epsilon_0 \epsilon_0/2, \forall x,y\in(x_0,x_0+\delta_0) \rightarrow |f(x)-f(y)|<\epsilon_0 \rightarrow \omega_f(I)\le\epsilon_0 \epsilon\le\delta_0 \rightarrow \lim_{\epsilon\rightarrow0}{\omega_f((x_0,x_0+\epsilon))}=0","['real-analysis', 'limits', 'functions']"
13,Asymptotic analysis with affine relations,Asymptotic analysis with affine relations,,I have very little experience with asymptotics so would appreciate some help. I have a function $g$ and I have an asymptotic to a rational function: $g\sim r$ . I guess I can just take the highest powers above and below and so I have $\displaystyle g\sim \frac{1}{N^4}$ but what I actually have is $\displaystyle g\sim \frac{(N-4)!}{N!}$ via: $$1<\dfrac{g(N)}{(N-4)!/N!}<\frac{N-2}{N-3}.$$ Now I have another function $f$ that is related to $g$ by: $$f(N)+(N-3)g(N)=\frac{1}{N(N-1)(N-2)}.$$ Can I figure out the asymptotics for for $f$ from this? Naively I get $f\sim 0$ but that seems a bit iffy. Actually very very iffy because naively putting in the two different asymptotes gives different answers. Any help appreciated.,I have very little experience with asymptotics so would appreciate some help. I have a function and I have an asymptotic to a rational function: . I guess I can just take the highest powers above and below and so I have but what I actually have is via: Now I have another function that is related to by: Can I figure out the asymptotics for for from this? Naively I get but that seems a bit iffy. Actually very very iffy because naively putting in the two different asymptotes gives different answers. Any help appreciated.,g g\sim r \displaystyle g\sim \frac{1}{N^4} \displaystyle g\sim \frac{(N-4)!}{N!} 1<\dfrac{g(N)}{(N-4)!/N!}<\frac{N-2}{N-3}. f g f(N)+(N-3)g(N)=\frac{1}{N(N-1)(N-2)}. f f\sim 0,"['limits', 'functions', 'asymptotics']"
14,Functions that satisfy $f(f(f(x))) = x$ for $f : \mathbb{R} \to \mathbb{R}$,Functions that satisfy  for,f(f(f(x))) = x f : \mathbb{R} \to \mathbb{R},"This is my first post on Math SE soo... I was reading over a thread a while ago that claims the only solution to $f(f(f(x))) = x$ for $f : \mathbb{R} \to \mathbb{R}$ is $f(x) = x$ , but.. I seemed to have found a counterexample: $$f(x) = \frac{1}{1-x}$$ Here, we have $$f(f(f(x))) = \frac{1}{1-\frac{1}{1-\frac{1}{1-x}}} = \frac{1}{1+\frac{1-x}{x}} = \frac{1}{\frac{1}{x}} = x$$ Well... looking over the original problem statement, would we say that my function satisfies $f : \mathbb{R} \to \mathbb{R}$ ? Or would it be something more like $f : \mathbb{R} \setminus \{1\} \to \mathbb{R} \setminus \{0\}$ ?","This is my first post on Math SE soo... I was reading over a thread a while ago that claims the only solution to for is , but.. I seemed to have found a counterexample: Here, we have Well... looking over the original problem statement, would we say that my function satisfies ? Or would it be something more like ?",f(f(f(x))) = x f : \mathbb{R} \to \mathbb{R} f(x) = x f(x) = \frac{1}{1-x} f(f(f(x))) = \frac{1}{1-\frac{1}{1-\frac{1}{1-x}}} = \frac{1}{1+\frac{1-x}{x}} = \frac{1}{\frac{1}{x}} = x f : \mathbb{R} \to \mathbb{R} f : \mathbb{R} \setminus \{1\} \to \mathbb{R} \setminus \{0\},['functions']
15,Proving monotonicity of a equation.,Proving monotonicity of a equation.,,"I am trying to prove the following: Let $f : (a,b) \rightarrow \mathbb{R}$ be a differentiable function and let $x_1,x_2$ be two points in $(a,b).$ Then $f'$ assumes all real values between $f'(x_1)$ and $f'(x_2).$ Hint: Consider $x_1 < x_2$ and a singular value $c$ between $f'(x_1)$ and $f'(x_2),$ set $g(x) = f(x) - cx,$ show that $g(x)$ is not monotonic. My work: WLOG let $x_1 < x_2$ and $f'(x_1) < c < f'(x_2).$ Setting $g(x) = f(x) - cx.$ Then $g'(x) = f'(x) - c$ and thus $g'(x_1) = f'(x_1) - c < 0$ and $g'(x_2) = f'(x_2) - c > 0$ from the inequalities. So $\implies g'(x_1)g'(x_2) < 0.$ My problem: How do I proceed to show that $g(x)$ is not monotonic ? Am I overlooking something ? P.S. I do NOT want a full solution , I want a hint to solve it myself.","I am trying to prove the following: Let be a differentiable function and let be two points in Then assumes all real values between and Hint: Consider and a singular value between and set show that is not monotonic. My work: WLOG let and Setting Then and thus and from the inequalities. So My problem: How do I proceed to show that is not monotonic ? Am I overlooking something ? P.S. I do NOT want a full solution , I want a hint to solve it myself.","f : (a,b) \rightarrow \mathbb{R} x_1,x_2 (a,b). f' f'(x_1) f'(x_2). x_1 < x_2 c f'(x_1) f'(x_2), g(x) = f(x) - cx, g(x) x_1 < x_2 f'(x_1) < c < f'(x_2). g(x) = f(x) - cx. g'(x) = f'(x) - c g'(x_1) = f'(x_1) - c < 0 g'(x_2) = f'(x_2) - c > 0 \implies g'(x_1)g'(x_2) < 0. g(x)","['calculus', 'functions', 'derivatives']"
16,Prove that the limit $\lim_{x\to 0^+}\frac{1}{x}\sin{(\frac{\pi}{x})}$ does not exist,Prove that the limit  does not exist,\lim_{x\to 0^+}\frac{1}{x}\sin{(\frac{\pi}{x})},"I have to prove that $\lim_{x\to 0^+}\frac{1}{x}\sin{\left(\frac{\pi}{x}\right)}$ does not exist. My idea: from the definition of function limit, if I found $x_n\to\infty$ and $y_n\to\infty$ and $x_n,y_n\neq 0$ such that $\frac{1}{x_n}\sin{\left(\frac{\pi}{x_n}\right)}\to l_1$ and $\frac{1}{y_n}\sin{\left(\frac{\pi}{y_n}\right)}\to l_2$ with $l_1\neq l_2$ then I have proved the limit does not exist. I have taken: $x_n=\frac{1}{2n}$ and $y_n=\frac{1}{1/2+2n}$ and so $$\lim_{n\to\infty}\frac{1}{x_n}\sin{\left(\frac{\pi}{x_n}\right)}=\lim_{n\to\infty}2n\sin{(2\pi n)}=0=l_1\\ \lim_{n\to\infty}\frac{1}{y_n}\sin{\left(\frac{\pi}{y_n}\right)}=\lim_{n\to\infty}(1/2+2n)\sin{\left(\frac{\pi}{2}+2\pi n\right)}=\infty=l_2$$ Since $l_1\neq l_2$ then the limit does not exist. Question: my work is right?","I have to prove that does not exist. My idea: from the definition of function limit, if I found and and such that and with then I have proved the limit does not exist. I have taken: and and so Since then the limit does not exist. Question: my work is right?","\lim_{x\to 0^+}\frac{1}{x}\sin{\left(\frac{\pi}{x}\right)} x_n\to\infty y_n\to\infty x_n,y_n\neq 0 \frac{1}{x_n}\sin{\left(\frac{\pi}{x_n}\right)}\to l_1 \frac{1}{y_n}\sin{\left(\frac{\pi}{y_n}\right)}\to l_2 l_1\neq l_2 x_n=\frac{1}{2n} y_n=\frac{1}{1/2+2n} \lim_{n\to\infty}\frac{1}{x_n}\sin{\left(\frac{\pi}{x_n}\right)}=\lim_{n\to\infty}2n\sin{(2\pi n)}=0=l_1\\
\lim_{n\to\infty}\frac{1}{y_n}\sin{\left(\frac{\pi}{y_n}\right)}=\lim_{n\to\infty}(1/2+2n)\sin{\left(\frac{\pi}{2}+2\pi n\right)}=\infty=l_2 l_1\neq l_2","['real-analysis', 'limits', 'functions']"
17,Would this order of operations proposal be effective?,Would this order of operations proposal be effective?,,"Let ~ be a function from R2 to R. Let x (~) y =  ~(x,y). Let priority(P) be a function that maps functions (R2 to R) to integers. Set P(+) = 1, P(*) = 2, P(^)=3 An expression that follows the order of operations is an expression that evaluates subexpressions with the highest priority before evaluating any other subexpression. An expression is well-ordered iff the expression follows the order of operations and for each subexpressions that contain a function with a priority k, the subexpressions are evaluated from left to right. ie 4^3^2 = 262,144 contains an expression, 4^3^2, that follows the order of operations but is not well-ordered since 4^3^2 != (4^3)^2. I couldn't find a better standard than what I saw on Wikipedia. So I don't know if there is a better standard that already exists. I've seen too many memes on reddit of 6÷2(2+1) that I thought it'd be nice to have a reference against claims that 6÷2(2+1)=1.","Let ~ be a function from R2 to R. Let x (~) y =  ~(x,y). Let priority(P) be a function that maps functions (R2 to R) to integers. Set P(+) = 1, P(*) = 2, P(^)=3 An expression that follows the order of operations is an expression that evaluates subexpressions with the highest priority before evaluating any other subexpression. An expression is well-ordered iff the expression follows the order of operations and for each subexpressions that contain a function with a priority k, the subexpressions are evaluated from left to right. ie 4^3^2 = 262,144 contains an expression, 4^3^2, that follows the order of operations but is not well-ordered since 4^3^2 != (4^3)^2. I couldn't find a better standard than what I saw on Wikipedia. So I don't know if there is a better standard that already exists. I've seen too many memes on reddit of 6÷2(2+1) that I thought it'd be nice to have a reference against claims that 6÷2(2+1)=1.",,['functions']
18,If $f'(x) − f'(y) ≤ 3|x − y|$ then show that $|f(x)−f(y)-f'(y)(x-y)|≤\frac{3}{2}(x-y)^2$,If  then show that,f'(x) − f'(y) ≤ 3|x − y| |f(x)−f(y)-f'(y)(x-y)|≤\frac{3}{2}(x-y)^2,"Let $f : \mathbb R \rightarrow \mathbb R $ be a twice differentiable function. Suppose that for all $x, y \in \mathbb R$ the function $f$ satisfies $f'(x) − f'(y) ≤ 3|x − y|$ then show that for all $x$ and $y$ , we must have $$|f(x)−f(y)-f'(y)(x-y)|≤\frac{3}{2}(x-y)^2$$ Setting $y=0$ gives $c-3x≤f'(x)≤c+3x$ , where $c=f'(0)$ $\Rightarrow \int_0^t c-3x≤\int_0^t f'(x)≤\int_0^t c+3x$ $\Rightarrow ct-\frac{3}{2}t^2-c≤f(t)-c_1≤ct+\frac{3}{2}t^2-c$ Where $c_1=f(0)$ . Now let $c_1-c=c_0$ $\Rightarrow ct-\frac{3}{2}t^2+c_0≤f(t)≤ct+\frac{3}{2}t^2+c_0$ For the given expression to be maximum, $f(x)$ must be maximum, $f(y)$ must be minimum and for $x≥y$ $f'(y)$ must also be minimum (we will look at the case $x<y$ separately). Substituting all these values, we get $(cx+\frac{3}{2}x^2+c_0)-(cy-\frac{3}{2}y^2+c_0)-(c-3y)(x-y)$ $\implies \frac{3}{2}(x^2+y^2)+6xy-3y^2$ $\implies \frac{3}{2}(x-y)^2+3y(2x-y)$ Under assumption $x>y$ , for all $y>0$ this expression is clearly $>\frac{3}{2}(x-y)^2$ . So where did I go wrong, and what will be the correct solution?","Let be a twice differentiable function. Suppose that for all the function satisfies then show that for all and , we must have Setting gives , where Where . Now let For the given expression to be maximum, must be maximum, must be minimum and for must also be minimum (we will look at the case separately). Substituting all these values, we get Under assumption , for all this expression is clearly . So where did I go wrong, and what will be the correct solution?","f : \mathbb R \rightarrow \mathbb R  x, y \in \mathbb R f f'(x) − f'(y) ≤ 3|x − y| x y |f(x)−f(y)-f'(y)(x-y)|≤\frac{3}{2}(x-y)^2 y=0 c-3x≤f'(x)≤c+3x c=f'(0) \Rightarrow \int_0^t c-3x≤\int_0^t f'(x)≤\int_0^t c+3x \Rightarrow ct-\frac{3}{2}t^2-c≤f(t)-c_1≤ct+\frac{3}{2}t^2-c c_1=f(0) c_1-c=c_0 \Rightarrow ct-\frac{3}{2}t^2+c_0≤f(t)≤ct+\frac{3}{2}t^2+c_0 f(x) f(y) x≥y f'(y) x<y (cx+\frac{3}{2}x^2+c_0)-(cy-\frac{3}{2}y^2+c_0)-(c-3y)(x-y) \implies \frac{3}{2}(x^2+y^2)+6xy-3y^2 \implies \frac{3}{2}(x-y)^2+3y(2x-y) x>y y>0 >\frac{3}{2}(x-y)^2","['calculus', 'functions', 'inequality', 'solution-verification', 'functional-inequalities']"
19,Expectation - Function using normalising constant and indicator function,Expectation - Function using normalising constant and indicator function,,"$f:\mathbb{R}^2\rightarrow \mathbb{R}^2, by \ f(x,y)=(x^2+y^2,x+y)$ Let $B = f^{-1}([1,4]\ \times \ [-4, \infty)\ \times \ (-2\sqrt2,2\sqrt2))$ Suppose $(X,Y)$ is a r.v. taking values in $B$ with $pdf:f(x,y)=Ce^{-\frac{x^2+y^2}{2}}I_B(x,y)$ - where C is a normalising constant. Find $\mathbb{E}(X), \mathbb{E}^7, \mathbb{E}(|XY|(X^2+Y^2)^{-1})$ I do not understand how to approach this question. I know that for finding the expectation from a random variable, you use $\mathbb{E}(X) = \int_{_-\infty}^\infty xf_X(x)dx$ So would I need to find the marginal distribution of $X$ And find then find it’s expectation? Do I need to find the value of the normalising constant $C$ ? How do I approach expectation when an indicator variable is involved? Do I find the probability of the indicator variable equalling 1 (and hence equalling 0)? Sorry lots of questions, but this problem seems to be more difficult than others I have attempted before.","Let Suppose is a r.v. taking values in with - where C is a normalising constant. Find I do not understand how to approach this question. I know that for finding the expectation from a random variable, you use So would I need to find the marginal distribution of And find then find it’s expectation? Do I need to find the value of the normalising constant ? How do I approach expectation when an indicator variable is involved? Do I find the probability of the indicator variable equalling 1 (and hence equalling 0)? Sorry lots of questions, but this problem seems to be more difficult than others I have attempted before.","f:\mathbb{R}^2\rightarrow \mathbb{R}^2, by \ f(x,y)=(x^2+y^2,x+y) B = f^{-1}([1,4]\ \times \ [-4, \infty)\ \times \ (-2\sqrt2,2\sqrt2)) (X,Y) B pdf:f(x,y)=Ce^{-\frac{x^2+y^2}{2}}I_B(x,y) \mathbb{E}(X), \mathbb{E}^7, \mathbb{E}(|XY|(X^2+Y^2)^{-1}) \mathbb{E}(X) = \int_{_-\infty}^\infty xf_X(x)dx X C","['probability', 'functions', 'probability-distributions', 'continuity', 'expected-value']"
20,Bijection $\mathbb{N} \to \mathbb{N}$,Bijection,\mathbb{N} \to \mathbb{N},"I'm trying to write a ""non-trivial"" (i.e., non-identity) bijection $\mathbb{N} \to \mathbb{N}$ , where I define $\mathbb{N}$ to exclude $0$ . Pictorally, the map I have in mind works, sending odds to evens and evens to odds: $$f(n) =  \begin{cases} n + 1 & & \text{if $n$ is odd} \\  n - 1 & & \text{ if $n$ is even} \end{cases}.  $$ The map is certainly surjective. Let $m \in \mathbb{N}$ . If $m$ is even, $m-1$ is odd and a natural number, and we have $f(m) = (m-1) + 1 = m$ . If $m$ is odd, then $m+1$ is even, and we have $f(m+1) = (m+1)-1 = m$ . Injectivity is tougher to prove because of the case work. Suppose $f(a) = f(b)$ for $a,b \in \mathbb{N}$ . If $a,b$ are both odd, then $a + 1 = b + 1$ , so $a = b$ . If $a,b$ are both even, then $a-1 = b - 1$ , then $a = b$ . I really need to rule out the case where one of $a$ or $b$ is odd and the other is even. Suppose without loss of generality that $a$ is odd and $b$ is even. Then $f(a) = a + 1$ and $f(b) = b - 1$ . So $a + 1 = b - 1$ , so $a = b - 2$ . I don't know how to get a contradiction out of this.","I'm trying to write a ""non-trivial"" (i.e., non-identity) bijection , where I define to exclude . Pictorally, the map I have in mind works, sending odds to evens and evens to odds: The map is certainly surjective. Let . If is even, is odd and a natural number, and we have . If is odd, then is even, and we have . Injectivity is tougher to prove because of the case work. Suppose for . If are both odd, then , so . If are both even, then , then . I really need to rule out the case where one of or is odd and the other is even. Suppose without loss of generality that is odd and is even. Then and . So , so . I don't know how to get a contradiction out of this.","\mathbb{N} \to \mathbb{N} \mathbb{N} 0 f(n) = 
\begin{cases}
n + 1 & & \text{if n is odd} \\ 
n - 1 & & \text{ if n is even}
\end{cases}. 
 m \in \mathbb{N} m m-1 f(m) = (m-1) + 1 = m m m+1 f(m+1) = (m+1)-1 = m f(a) = f(b) a,b \in \mathbb{N} a,b a + 1 = b + 1 a = b a,b a-1 = b - 1 a = b a b a b f(a) = a + 1 f(b) = b - 1 a + 1 = b - 1 a = b - 2","['functions', 'proof-explanation']"
21,Exponential Growth of Functions,Exponential Growth of Functions,,"Let $f,g: \mathbb{N} \longrightarrow [0,+\infty[$ be nondecreasing functions. Then we say that the function $f$ dominates the function $g$ if there are positive integers $A,B$ and $C$ such that $$Af(Br)\geqslant g(r)$$ for every $r\geqslant C$ . Let $h: \mathbb{N} \longrightarrow [0,+\infty[$ be a nondecreasing function. Then $h$ dominates the exponential function if and only if $$\limsup_{x \longrightarrow +\infty} \frac{\log h(r)}{r}>0$$ The questions are: If a function $T$ dominates the exponential function, does not the function $T$ have exponential growth (since the exponential function dominates $T$ ) so they are equivalent under the mutual dominance? Is there any function that strictly dominates the exponential function (the function dominates the exponential function but the exponential function does not dominate it)?","Let be nondecreasing functions. Then we say that the function dominates the function if there are positive integers and such that for every . Let be a nondecreasing function. Then dominates the exponential function if and only if The questions are: If a function dominates the exponential function, does not the function have exponential growth (since the exponential function dominates ) so they are equivalent under the mutual dominance? Is there any function that strictly dominates the exponential function (the function dominates the exponential function but the exponential function does not dominate it)?","f,g: \mathbb{N} \longrightarrow [0,+\infty[ f g A,B C Af(Br)\geqslant g(r) r\geqslant C h: \mathbb{N} \longrightarrow [0,+\infty[ h \limsup_{x \longrightarrow +\infty} \frac{\log h(r)}{r}>0 T T T","['real-analysis', 'calculus', 'analysis', 'functions', 'exponential-function']"
22,Why is Trace function in $\mathbb{F}_{2^n}$ a map $\mathbb{F}_{2^n} \rightarrow \mathbb{F}_2$?,Why is Trace function in  a map ?,\mathbb{F}_{2^n} \mathbb{F}_{2^n} \rightarrow \mathbb{F}_2,"Except from a paper: ( screenshot ) A function $F$ from $\mathbb{F}_{2^{n}}$ to $\mathbb{F}_{2^{n}}$ admits a unique representation, called Univariate Polynomial Representation , over $\mathbb{F}_{2^{n}}$ of degree at most $2^{n}-1$ : $$ F(x)=\sum_{j=0}^{2^{n}-1} \delta_{j} x^{j}, \text { with } \delta_{j} \in \mathbb{F}_{2^{n}} $$ For every integer $j$ consider its binary expansion $\sum_{s=0}^{n-1} j_{s} 2^{s}$ and denote with $w_{2}(j)$ the number of nonzero coefficients (i.e. $\sum_{s=0}^{n-1} j_{s}$ ). The algebraic degree of the function $F$ is the $\max _{j=0, \ldots, 2^{n}-1 / \delta_{j} \neq 0} w_{2}(j)$ . Functions of algebraic degree 1 are called affine and of degree 2 quadratic . Linear functions are affine functions without the constant term and they can be represented as $L(x)=\sum_{j=0}^{n-1} \gamma_{j} x^{2^{j}}$ . A known example of a linear function defined over any dimension $n$ is the Trace function $\operatorname{Tr}(x)=\operatorname{Tr}_{n}(x)=\sum_{i=0}^{n-1} x^{2^{i}}$ , In particular the trace is a Boolean function, i.e. $\operatorname{Tr}: \mathbb{F}_{2^{n}} \rightarrow \mathbb{F}_{2}$ . For $m$ positive divisor of $n$ we use the notation $\operatorname{Tr}^{m}(x)=\sum_{i=0}^{n / m-1} x^{2^{i m}}$ . As you can see, in the paper the trace function $Tr$ is $\mathbb{F}_{2^n} \rightarrow \mathbb{F}_2$ . Shouldn't it be $\mathbb{F}_{2^n} \rightarrow \mathbb{F}_{2^n}\;$ ? I searched a lot over the internet, but couldn't find an explanation. Please help.","Except from a paper: ( screenshot ) A function from to admits a unique representation, called Univariate Polynomial Representation , over of degree at most : For every integer consider its binary expansion and denote with the number of nonzero coefficients (i.e. ). The algebraic degree of the function is the . Functions of algebraic degree 1 are called affine and of degree 2 quadratic . Linear functions are affine functions without the constant term and they can be represented as . A known example of a linear function defined over any dimension is the Trace function , In particular the trace is a Boolean function, i.e. . For positive divisor of we use the notation . As you can see, in the paper the trace function is . Shouldn't it be ? I searched a lot over the internet, but couldn't find an explanation. Please help.","F \mathbb{F}_{2^{n}} \mathbb{F}_{2^{n}} \mathbb{F}_{2^{n}} 2^{n}-1 
F(x)=\sum_{j=0}^{2^{n}-1} \delta_{j} x^{j}, \text { with } \delta_{j} \in \mathbb{F}_{2^{n}}
 j \sum_{s=0}^{n-1} j_{s} 2^{s} w_{2}(j) \sum_{s=0}^{n-1} j_{s} F \max _{j=0, \ldots, 2^{n}-1 / \delta_{j} \neq 0} w_{2}(j) L(x)=\sum_{j=0}^{n-1} \gamma_{j} x^{2^{j}} n \operatorname{Tr}(x)=\operatorname{Tr}_{n}(x)=\sum_{i=0}^{n-1} x^{2^{i}} \operatorname{Tr}: \mathbb{F}_{2^{n}} \rightarrow \mathbb{F}_{2} m n \operatorname{Tr}^{m}(x)=\sum_{i=0}^{n / m-1} x^{2^{i m}} Tr \mathbb{F}_{2^n} \rightarrow \mathbb{F}_2 \mathbb{F}_{2^n} \rightarrow \mathbb{F}_{2^n}\;","['functions', 'finite-fields']"
23,True definition of a function $f$ [duplicate],True definition of a function  [duplicate],f,"This question already has answers here : What is the actual definition of a function? (3 answers) Closed 2 years ago . Thomas Calculus gives the following definition for a function: A function $f$ from a set $D$ to a set $Y$ is a rule that assigns a unique value $f(x)$ in $Y$ to each $x$ in $D$ . I have my misgivings as far as this definition for a mathematical function is concerned. Every $x$ needn't be assigned a unique $f(x)$ from $Y$ . If $x_1, x_2 \in D$ and $f(x_1)=f(x_2)=y \in Y$ which is perfectly valid and $y$ is not unique since the same $y$ gets assigned to both $x_1$ and $x_2$ . Am I wrong in my interpretation?",This question already has answers here : What is the actual definition of a function? (3 answers) Closed 2 years ago . Thomas Calculus gives the following definition for a function: A function from a set to a set is a rule that assigns a unique value in to each in . I have my misgivings as far as this definition for a mathematical function is concerned. Every needn't be assigned a unique from . If and which is perfectly valid and is not unique since the same gets assigned to both and . Am I wrong in my interpretation?,"f D Y f(x) Y x D x f(x) Y x_1, x_2 \in D f(x_1)=f(x_2)=y \in Y y y x_1 x_2","['calculus', 'functions', 'definition', 'relations']"
24,How to find the Closed Form Expression of the function $f$ that outputs the number of ones in the binary expression of a given natural number? [closed],How to find the Closed Form Expression of the function  that outputs the number of ones in the binary expression of a given natural number? [closed],f,"Closed. This question does not meet Mathematics Stack Exchange guidelines . It is not currently accepting answers. Please provide additional context , which ideally explains why the question is relevant to you and our  community. Some forms of context include: background and motivation, relevant definitions, source, possible strategies, your current progress, why the question is interesting or important, etc. Closed 2 years ago . Improve this question I am interested in finding the closed form expression of the number of ones in the binary expression of the given natural number. I have no idea whether it is possible to do so. Any help is appreciated.","Closed. This question does not meet Mathematics Stack Exchange guidelines . It is not currently accepting answers. Please provide additional context , which ideally explains why the question is relevant to you and our  community. Some forms of context include: background and motivation, relevant definitions, source, possible strategies, your current progress, why the question is interesting or important, etc. Closed 2 years ago . Improve this question I am interested in finding the closed form expression of the number of ones in the binary expression of the given natural number. I have no idea whether it is possible to do so. Any help is appreciated.",,"['number-theory', 'functions', 'closed-form', 'binary']"
25,Show that $\sim$ is an equivalence relation.,Show that  is an equivalence relation.,\sim,"Suppose $A,B$ sets and nonempty $X\subseteq A$ . For $f\in B^A$ define $\hat f = f_{|X} \in B^X$ . Define a relation $\sim $ on $B^A$ by $f\sim g \iff \hat f = \hat g$ . Show that $\sim$ is an equivalence relation. If $A,B$ are finite sets, how many equivalence classes are there? How many elements in each class? Denote: $B^A = \{f \text{ a function}\mid f:A\to B\}$ $\hat f = f_{|X} \in B^X$ as function $\hat f(x) = f(x), \forall x\in X$ . To show equivalence relation: $f\sim f \implies \hat f = \hat f \implies \sim$ is reflexive $f\sim g \implies \hat f=\hat g \implies \hat g = \hat f \implies g \sim f \implies \sim$ is symmetric $f\sim g \wedge g \sim h \implies \hat f = \hat g \wedge \hat g = \hat h \implies \hat f = \hat g = \hat h \implies f \sim h \implies \sim $ is transitive. Thus, $\sim$ is an equivalence relation. If $A,B$ are finite sets, then the number of equivalence classes are defined by the number of functions we have $\hat f : X \to B$ ? But how does this make sense? Can someone explain? If this above is the case then we have $|B|^{|X|}$ number of functions $\hat f : X \to B$ and, thus, the number of equivalence classes. (EDIT) We have $A-X$ elements by which two functions $f,g$ differ. Thus the number of functions $f:A-X \to B$ is $|B|^{|A-X|}$ . I am not sure where to proceed from here to characterize the number of elements in each equivalence class.","Suppose sets and nonempty . For define . Define a relation on by . Show that is an equivalence relation. If are finite sets, how many equivalence classes are there? How many elements in each class? Denote: as function . To show equivalence relation: is reflexive is symmetric is transitive. Thus, is an equivalence relation. If are finite sets, then the number of equivalence classes are defined by the number of functions we have ? But how does this make sense? Can someone explain? If this above is the case then we have number of functions and, thus, the number of equivalence classes. (EDIT) We have elements by which two functions differ. Thus the number of functions is . I am not sure where to proceed from here to characterize the number of elements in each equivalence class.","A,B X\subseteq A f\in B^A \hat f = f_{|X} \in B^X \sim  B^A f\sim g \iff \hat f = \hat g \sim A,B B^A = \{f \text{ a function}\mid f:A\to B\} \hat f = f_{|X} \in B^X \hat f(x) = f(x), \forall x\in X f\sim f \implies \hat f = \hat f \implies \sim f\sim g \implies \hat f=\hat g \implies \hat g = \hat f \implies g \sim f \implies \sim f\sim g \wedge g \sim h \implies \hat f = \hat g \wedge \hat g = \hat h \implies \hat f = \hat g = \hat h \implies f \sim h \implies \sim  \sim A,B \hat f : X \to B |B|^{|X|} \hat f : X \to B A-X f,g f:A-X \to B |B|^{|A-X|}","['combinatorics', 'functions', 'solution-verification', 'relations', 'equivalence-relations']"
26,uniformly continuous function $f$ such that $\sum 1/f(n)$ is convergent?,uniformly continuous function  such that  is convergent?,f \sum 1/f(n),"Does there exist a uniformly continuous function $f:[1,\infty)\to \mathbb R$ such that $\sum_{n=1}^\infty 1/f(n)$ is convergent ? I know that $\exists M>0$ such that $|f(x)|< Mx, \forall x\in [1,\infty)$ , so $|1/f(n)|>1/(Mn) ,\forall n \ge 1$ , thus $\sum_{n=1}^\infty |1/f(n)|$ is divergent. But I don't know what happens with $\sum_{n=1}^\infty 1/f(n)$ . Please help","Does there exist a uniformly continuous function such that is convergent ? I know that such that , so , thus is divergent. But I don't know what happens with . Please help","f:[1,\infty)\to \mathbb R \sum_{n=1}^\infty 1/f(n) \exists M>0 |f(x)|< Mx, \forall x\in [1,\infty) |1/f(n)|>1/(Mn) ,\forall n \ge 1 \sum_{n=1}^\infty |1/f(n)| \sum_{n=1}^\infty 1/f(n)","['real-analysis', 'sequences-and-series', 'uniform-continuity']"
27,"Convexity, concavity doubts","Convexity, concavity doubts",,"Let $f$ be a numerical function, $f:D\to\mathbb{R}$ , and we want to algebraically determine if $f$ is convex. According to the definition, $f$ is convex if and only if $\forall~x_1,x_2\in D$ the line segment connecting $\big(x_1,f(x_1)\big)$ and $\big(x_2,f(x_2)\big)$ lies above the graph of $f$ . I have determined the function describing the line: $$g:[x_1,x_2]\to\mathbb{R},~g(x)=\frac{f(x_1)-f(x_2)}{x_1-x_2}x+\frac{x_1f(x_2)-x_2f(x_1)}{x_1-x_2}$$ I have taken an arbitrary $$s\in [x_1,x_2]\Rightarrow s=\lambda x_1+(1-\lambda)x_2,~\lambda\in[0,1]$$ and calculated $$g(s)=\lambda f(x_1)+(1-\lambda)f(x_2)$$ Using the definition the equivalence of convexity becomes $f(s)\le g(s)$ : $$f(\lambda x_1+(1-\lambda)x_2)\le \lambda f(x_1)+(1-\lambda)f(x_2)~\forall~x_1,x_2\in D,\lambda\in[0,1]$$ ... which is the official definition of convexity. For continous functions it is said that it is enough to check the inequality for a fixed $\lambda\in(0,1)$ , but why? If it is true for a single case does it imply it is true for any $\lambda$ ? EDIT1: If I think about it for $f(x)=\sin(x)$ this isn't even true... EDIT2: I think I have misunderstood, my counterexample for $\sin(x)$ isn't valid because it only analyzes a fixed pair $x_1,x_2$ , but for other pairs the inequality isn't true for the same $\lambda$ . Also I have looked over the proof ""midpoint convex, continuous implies convex"". Thanks to everyone for the help.","Let be a numerical function, , and we want to algebraically determine if is convex. According to the definition, is convex if and only if the line segment connecting and lies above the graph of . I have determined the function describing the line: I have taken an arbitrary and calculated Using the definition the equivalence of convexity becomes : ... which is the official definition of convexity. For continous functions it is said that it is enough to check the inequality for a fixed , but why? If it is true for a single case does it imply it is true for any ? EDIT1: If I think about it for this isn't even true... EDIT2: I think I have misunderstood, my counterexample for isn't valid because it only analyzes a fixed pair , but for other pairs the inequality isn't true for the same . Also I have looked over the proof ""midpoint convex, continuous implies convex"". Thanks to everyone for the help.","f f:D\to\mathbb{R} f f \forall~x_1,x_2\in D \big(x_1,f(x_1)\big) \big(x_2,f(x_2)\big) f g:[x_1,x_2]\to\mathbb{R},~g(x)=\frac{f(x_1)-f(x_2)}{x_1-x_2}x+\frac{x_1f(x_2)-x_2f(x_1)}{x_1-x_2} s\in [x_1,x_2]\Rightarrow s=\lambda x_1+(1-\lambda)x_2,~\lambda\in[0,1] g(s)=\lambda f(x_1)+(1-\lambda)f(x_2) f(s)\le g(s) f(\lambda x_1+(1-\lambda)x_2)\le \lambda f(x_1)+(1-\lambda)f(x_2)~\forall~x_1,x_2\in D,\lambda\in[0,1] \lambda\in(0,1) \lambda f(x)=\sin(x) \sin(x) x_1,x_2 \lambda","['functions', 'convexity-inequality']"
28,Complementary Trigonometric relationship for sine and cosine,Complementary Trigonometric relationship for sine and cosine,,"To begin, let's start with the first quadrant in the unit circle: It's easy to see why(due to complementary angles): $$\cos\left(\frac{\pi}{2}-\theta\right) = \sin(\theta)$$ $$\sin\left(\frac{\pi}{2}-\theta\right) = \cos(\theta)$$ But what about Quadrant 2, 3 and 4, how do I visualise and prove these using the unit circle? Q2: $$\sin\left(\frac{\pi}{2}+\theta\right) = ???$$ $$\cos\left(\frac{\pi}{2}+\theta\right) = ???$$ Q3: $$\sin\left(\frac{3\pi}{2}-\theta\right) = ???$$ $$\cos\left(\frac{3\pi}{2}-\theta\right) = ???$$ Q4: $$\sin\left(\frac{3\pi}{2}+\theta\right) = ???$$ $$\cos\left(\frac{3\pi}{2}+\theta\right) = ???$$","To begin, let's start with the first quadrant in the unit circle: It's easy to see why(due to complementary angles): But what about Quadrant 2, 3 and 4, how do I visualise and prove these using the unit circle? Q2: Q3: Q4:",\cos\left(\frac{\pi}{2}-\theta\right) = \sin(\theta) \sin\left(\frac{\pi}{2}-\theta\right) = \cos(\theta) \sin\left(\frac{\pi}{2}+\theta\right) = ??? \cos\left(\frac{\pi}{2}+\theta\right) = ??? \sin\left(\frac{3\pi}{2}-\theta\right) = ??? \cos\left(\frac{3\pi}{2}-\theta\right) = ??? \sin\left(\frac{3\pi}{2}+\theta\right) = ??? \cos\left(\frac{3\pi}{2}+\theta\right) = ???,"['functions', 'trigonometry', 'triangles', 'circles']"
29,Inverse functions and $f(x)=x$,Inverse functions and,f(x)=x,"Is every solution of $f(x)=x$ a solution of $f^{-1}(x) = f(x)$ ? If not, why not? Can we not do the following ? \begin{eqnarray}  f(x)&=&x\\ f(f(x))&=&f(x)=x\\  f^{-1}(x)&=&f(x)\\ \end{eqnarray} Also, what about the converse of this statement? What if a function is invertible (bijective) only in a certain domain?","Is every solution of a solution of ? If not, why not? Can we not do the following ? Also, what about the converse of this statement? What if a function is invertible (bijective) only in a certain domain?","f(x)=x f^{-1}(x) = f(x) \begin{eqnarray}
 f(x)&=&x\\
f(f(x))&=&f(x)=x\\
 f^{-1}(x)&=&f(x)\\
\end{eqnarray}","['functions', 'inverse-function']"
30,What functions with source and target the rational numbers satisfy the intermediate value theorem?,What functions with source and target the rational numbers satisfy the intermediate value theorem?,,"I am curious to find if there is a characterization of the set $S$ of all functions $f: \mathbb{Q} \to \mathbb{Q}$ where for all intervals $[a,b] \subset \mathbb{Q}$ , there exists $x \in [a,b]$ for every $y$ between $f(a)$ and $f(b)$ such that $f(x) = y$ . We see $f(x) = x$ has $f \in S$ since for any $y \in [f(a),f(b)] \subset \mathbb{Q}$ we can choose $x = y$ to have $f(x) = y$ . For a non example, take $f(x) = x^2$ and $[f(0),f(2)] = [0,4]$ . We have $2 \in [0,4]$ , but $f(x) = 2$ has no solutions. Similarly, $f(x) = x^n \not \in S$ for $n > 1$ . From the lemmas I've found so far, I conjecture $S$ is made up of piecewise defined functions of the form $a(x-b)^n+c$ for $a,b,c \in \mathbb{Q}$ and $n \in \{-1,0,1\}$ with appropreate care taken to deal with discontinuities. Here are my questions Do we miss anything restricting to piecewise rational functions $p(x)/q(x)$ ? Supposing $f(x), g(x) \not \in S$ , can we show $f(x) + g(x) \not \in S$ Is this well known appearing in a book somewhere? Is the corresponding question for number fields well known?","I am curious to find if there is a characterization of the set of all functions where for all intervals , there exists for every between and such that . We see has since for any we can choose to have . For a non example, take and . We have , but has no solutions. Similarly, for . From the lemmas I've found so far, I conjecture is made up of piecewise defined functions of the form for and with appropreate care taken to deal with discontinuities. Here are my questions Do we miss anything restricting to piecewise rational functions ? Supposing , can we show Is this well known appearing in a book somewhere? Is the corresponding question for number fields well known?","S f: \mathbb{Q} \to \mathbb{Q} [a,b] \subset \mathbb{Q} x \in [a,b] y f(a) f(b) f(x) = y f(x) = x f \in S y \in [f(a),f(b)] \subset \mathbb{Q} x = y f(x) = y f(x) = x^2 [f(0),f(2)] = [0,4] 2 \in [0,4] f(x) = 2 f(x) = x^n \not \in S n > 1 S a(x-b)^n+c a,b,c \in \mathbb{Q} n \in \{-1,0,1\} p(x)/q(x) f(x), g(x) \not \in S f(x) + g(x) \not \in S","['real-analysis', 'functions', 'continuity', 'rational-numbers', 'rational-functions']"
31,how to obtain the strong convexity inequality,how to obtain the strong convexity inequality,,"I was playing around with the strong convexity definition and got stuck at some point. I was wondering if someone could kindly help me out. We say that function $f$ is strongly convex if $1) f(x) \geq f(y) + \xi^T(x- y) + \frac{\mu}{2}||x-y||^2$ for $\mu >0$ and $\forall x, y \in dom f$ Now, suppose $f$ is convex and let $h(x) = f(x) -\frac{\mu}{2}||x||^2$ which is also convex. If $\xi \in \partial f(y)$ , then $\xi - \mu y \in \partial h(y)$ . Then, I can use the definition of convexity for function $h$ . $h(x) \geq h(y) + (\xi - \mu y)^T (x-y)$ . Now, let us replace $h$ by $ f -\frac{\mu}{2}||x||^2$ . $f(x) -\frac{\mu}{2}||x||^2 \geq f(y)  -\frac{\mu}{2}||y||^2 + (\xi - \mu y)^T (x-y)$ $f(x) \geq f(y)  + \frac{\mu}{2}||x||^2 -\frac{\mu}{2}||y||^2 + \xi^T(x-y) -  \mu y^Tx + \mu ||y||^2$ This is where I got stuck. I was expecting to obtain the inequality (1), however, I am making a mistake somewhere.","I was playing around with the strong convexity definition and got stuck at some point. I was wondering if someone could kindly help me out. We say that function is strongly convex if for and Now, suppose is convex and let which is also convex. If , then . Then, I can use the definition of convexity for function . . Now, let us replace by . This is where I got stuck. I was expecting to obtain the inequality (1), however, I am making a mistake somewhere.","f 1) f(x) \geq f(y) + \xi^T(x- y) + \frac{\mu}{2}||x-y||^2 \mu >0 \forall x, y \in dom f f h(x) = f(x) -\frac{\mu}{2}||x||^2 \xi \in \partial f(y) \xi - \mu y \in \partial h(y) h h(x) \geq h(y) + (\xi - \mu y)^T (x-y) h  f -\frac{\mu}{2}||x||^2 f(x) -\frac{\mu}{2}||x||^2 \geq f(y)  -\frac{\mu}{2}||y||^2 + (\xi - \mu y)^T (x-y) f(x) \geq f(y)  + \frac{\mu}{2}||x||^2 -\frac{\mu}{2}||y||^2 + \xi^T(x-y) -  \mu y^Tx + \mu ||y||^2","['calculus', 'functions', 'inequality', 'convex-analysis', 'convexity-inequality']"
32,Using mappings and bijections for a simple looking probability problem.,Using mappings and bijections for a simple looking probability problem.,,"I came across this simple problem today: Show that the probability of rolling a $14$ is the same whether we use $3$ or $5$ fair dice. It is very easy (but tedious)  to solve with basic probability but I wanted to use something a bit more flash (and hopefully cleaner) Before begging on this puzzle I would like to show you a similar looking puzzle with the method I would like to use to attack the first one. Warm up problem: $A$ rolls three $6$ sided dice and $B$ rolls one $20$ sided die. Show that the probability $A's$ rolls sum greater than $B$ 's roll is equal to the probability $B$ rolls greater than $A$ . We will construct a bijective mapping, $\phi: \mathbb{N}^3 \times \mathbb{N} \to  \mathbb{N}^3 \times \mathbb{N}$ , between winning combinations for $A$ , denoted the set $W_A$ into winning combinations for $B$ , $W_B,$ and thus show the finite sets have the same size. Let $X_1 = \{a_1, a_2, a_3 \} \times\{b_1 \} \in W_A$ denote the rolls that occurred in $A$ winning. In other words: $a_1+a_2+a_3 > b_1$ . Now consider turning over all four dice, (applying $\phi$ )  we now see $ \phi(X_1) = \{7-a_1, 7 -a_2, 7-a_3 \} \times \{ 21-b_1\}  \in W_B$ as $7-a_1 + 7-a_2 + 7 -a_3 = 21 - (a_1+a_2+a_3) < 21 - b_1$ And so there is a one-to-one mapping from $W_A \to W_B$ and $W_B \to W_A$ and so they have equal size and we are done. My problem I cannot help but wonder that there is a very similar method for my puzzle above. Denoting $S_n \subset \mathbb{N}^n$ as combinations of $n$ rolls that sum to $14$ ,I envision it as follows: Let $\{x_1,x_2,x_3 \} \in S_3$ be a combination of rolls that sum to $14$ . We need to show there are $36$ as many rolls in $S_5$ i.e for any $\{x_1,x_2,x_3 \}$ and any $2$ rolls of a dice we can construct a unique element of $S_5$ . I initially had it like this: $\phi: \mathbb{N}^3 \times \mathbb{N}^2 \to \mathbb{N}^5$ by ""absorbing the final two rolls into earlier states"" as required: Consider $S_3 \ni \{6,5,3 \}$ $\times \{2,3 \} \to \{6,2,1,2,3 \}$ as we pushed right to left by taking $2$ from $3$ to give $1$ and then $3$ from $5$ to give $2$ This fails to work however as it is not injective nor subjective. Consider $\{x_1,x_2,x_3\} \times \{6,5\}$ . $\phi$ will always take it to $\{1,1,1,6,5 \}$ . Also $\{6,6\}$ will never be a suitable input for the $\mathbb{N}^2$ part. Can someone think of a nice way to do this?","I came across this simple problem today: Show that the probability of rolling a is the same whether we use or fair dice. It is very easy (but tedious)  to solve with basic probability but I wanted to use something a bit more flash (and hopefully cleaner) Before begging on this puzzle I would like to show you a similar looking puzzle with the method I would like to use to attack the first one. Warm up problem: rolls three sided dice and rolls one sided die. Show that the probability rolls sum greater than 's roll is equal to the probability rolls greater than . We will construct a bijective mapping, , between winning combinations for , denoted the set into winning combinations for , and thus show the finite sets have the same size. Let denote the rolls that occurred in winning. In other words: . Now consider turning over all four dice, (applying )  we now see as And so there is a one-to-one mapping from and and so they have equal size and we are done. My problem I cannot help but wonder that there is a very similar method for my puzzle above. Denoting as combinations of rolls that sum to ,I envision it as follows: Let be a combination of rolls that sum to . We need to show there are as many rolls in i.e for any and any rolls of a dice we can construct a unique element of . I initially had it like this: by ""absorbing the final two rolls into earlier states"" as required: Consider as we pushed right to left by taking from to give and then from to give This fails to work however as it is not injective nor subjective. Consider . will always take it to . Also will never be a suitable input for the part. Can someone think of a nice way to do this?","14 3 5 A 6 B 20 A's B B A \phi: \mathbb{N}^3 \times \mathbb{N} \to  \mathbb{N}^3 \times \mathbb{N} A W_A B W_B, X_1 = \{a_1, a_2, a_3 \} \times\{b_1 \} \in W_A A a_1+a_2+a_3 > b_1 \phi  \phi(X_1) = \{7-a_1, 7 -a_2, 7-a_3 \} \times \{ 21-b_1\}  \in W_B 7-a_1 + 7-a_2 + 7 -a_3 = 21 - (a_1+a_2+a_3) < 21 - b_1 W_A \to W_B W_B \to W_A S_n \subset \mathbb{N}^n n 14 \{x_1,x_2,x_3 \} \in S_3 14 36 S_5 \{x_1,x_2,x_3 \} 2 S_5 \phi: \mathbb{N}^3 \times \mathbb{N}^2 \to \mathbb{N}^5 S_3 \ni \{6,5,3 \} \times \{2,3 \} \to \{6,2,1,2,3 \} 2 3 1 3 5 2 \{x_1,x_2,x_3\} \times \{6,5\} \phi \{1,1,1,6,5 \} \{6,6\} \mathbb{N}^2","['probability', 'functions']"
33,concave function in context of matrices,concave function in context of matrices,,Consider $A \in \mathbb{R}^{n \times k} $ and $x \in \mathbb{R}^n$ . $(AA^T)^{-1}$ is assumed to exist. Define $f(x)= -xAA^T x$ Why is $f$ a concave function? Computing the hessian matrix yields: $ -AA^T$ . At this point I cannot say anything about definiteness. Is there an alternative way of approaching this?,Consider and . is assumed to exist. Define Why is a concave function? Computing the hessian matrix yields: . At this point I cannot say anything about definiteness. Is there an alternative way of approaching this?,A \in \mathbb{R}^{n \times k}  x \in \mathbb{R}^n (AA^T)^{-1} f(x)= -xAA^T x f  -AA^T,"['real-analysis', 'analysis', 'functions', 'derivatives']"
34,Does there exist a non-finite subset X of $\Bbb Q^2$ such that every real valued continuous function on X is bounded.,Does there exist a non-finite subset X of  such that every real valued continuous function on X is bounded.,\Bbb Q^2,"I think so. Because if we think of a non-finite subset X of $\Bbb Q$ such that every real valued continuous function on X is bounded. We can take X to be $\{1/n\} \cup\{0\}$ . Since the function is continuous at 0, it is bounded around $0$ . And the set of all other points is finite, so it is bounded. In a similar manner we can do for $\Bbb Q^2$ . Is my proof okay? If not please explain or give a simple proof","I think so. Because if we think of a non-finite subset X of such that every real valued continuous function on X is bounded. We can take X to be . Since the function is continuous at 0, it is bounded around . And the set of all other points is finite, so it is bounded. In a similar manner we can do for . Is my proof okay? If not please explain or give a simple proof",\Bbb Q \{1/n\} \cup\{0\} 0 \Bbb Q^2,"['real-analysis', 'functions', 'continuity']"
35,Kernel of the polynomial to function map?,Kernel of the polynomial to function map?,,"It's common to think of polynomials as functions, but it was recently brought to my attention that this isn't exactly right. Consider, for example $x$ and $x^7$ in $\mathbb{Z} / 7\mathbb{Z} [x]$ . These are distinct polynomials, but as functions $\mathbb{Z} / 7\mathbb{Z} \to \mathbb{Z} / 7\mathbb{Z}$ they are identical. This made me wonder: how can we describe the kernel of the map $ \varphi : \mathbb{Z} / n\mathbb{Z} [x] \to A $ given by $\varphi(p) = (x \mapsto p(x))$ where \begin{align} A = \{p : \mathbb{Z} / n\mathbb{Z} &\to \mathbb{Z} / n\mathbb{Z} \\&\mid p \text{ is a polynomial with coefficients in } \mathbb{Z} / n\mathbb{Z}\} \end{align} (Is there a more standard notation for $A$ ?) The only thing I know is that when $n$ is prime, $x^n - x \in \ker \varphi$ . This is just Fermat's little theorem. Can we give an explicit characterization of $\ker \varphi$ ? How should we think of elements of $\mathbb{Z} / n\mathbb{Z} [x]$ if not as functions $\mathbb{Z} / n\mathbb{Z} \to \mathbb{Z} / n\mathbb{Z}$ .","It's common to think of polynomials as functions, but it was recently brought to my attention that this isn't exactly right. Consider, for example and in . These are distinct polynomials, but as functions they are identical. This made me wonder: how can we describe the kernel of the map given by where (Is there a more standard notation for ?) The only thing I know is that when is prime, . This is just Fermat's little theorem. Can we give an explicit characterization of ? How should we think of elements of if not as functions .","x x^7 \mathbb{Z} / 7\mathbb{Z} [x] \mathbb{Z} / 7\mathbb{Z} \to \mathbb{Z} / 7\mathbb{Z} 
\varphi : \mathbb{Z} / n\mathbb{Z} [x] \to A
 \varphi(p) = (x \mapsto p(x)) \begin{align}
A = \{p : \mathbb{Z} / n\mathbb{Z} &\to \mathbb{Z} / n\mathbb{Z} \\&\mid p \text{ is a polynomial with coefficients in } \mathbb{Z} / n\mathbb{Z}\}
\end{align} A n x^n - x \in \ker \varphi \ker \varphi \mathbb{Z} / n\mathbb{Z} [x] \mathbb{Z} / n\mathbb{Z} \to \mathbb{Z} / n\mathbb{Z}","['abstract-algebra', 'functions', 'polynomial-rings']"
36,Can you define and elementary $f(x)$ such that $2^{x}<f(f(f(x)))<2^{2^x}$?,Can you define and elementary  such that ?,f(x) 2^{x}<f(f(f(x)))<2^{2^x},Can you define a elementary real-valued function $f$ such that $2^{x} < f(f(f(x))) < 2^{2^x}$ for sufficiently large $x\in \mathbb{R}$ ? I know that there is no elementary function $f$ such that. $f(f(x))=2^{x}$ but is it possible to find an elementary function such that $2^{x}<f(f(f(x)))<2^{2^x}$ for sufficiently large $x\in \mathbb{R}$ ?,Can you define a elementary real-valued function such that for sufficiently large ? I know that there is no elementary function such that. but is it possible to find an elementary function such that for sufficiently large ?,f 2^{x} < f(f(f(x))) < 2^{2^x} x\in \mathbb{R} f f(f(x))=2^{x} 2^{x}<f(f(f(x)))<2^{2^x} x\in \mathbb{R},"['functions', 'exponential-function', 'problem-solving', 'elementary-functions']"
37,Solving $f'(x)=f^{-1}(x)$,Solving,f'(x)=f^{-1}(x),This is a problem that a friend of mine gave to me a while back. I somewhat solved it and paid no attention to it afterwards until I though about it now. Forgive the errors and laziness this was a long time ago and not everything has to be perfect. I assumed $f$ to be a polynomial function of the kind $f(x)=ax^b$ where $f^{-1}(x)= \big(\frac x a\big)^{1/b} $ and found $$f(x)= \varphi^{-\frac{1}{\varphi}-1}(-x)^{\phi} $$ where $\varphi=\frac{1+\sqrt{5}}{2}$ and $\phi=\frac{1-\sqrt{5}}{2}$ my working is in the following images : (I did this in word) I was curious if there were more solutions to just this polynomial? Perhaps a different function ? Don't bother commenting trying to correct my work I am just curious about the above questions. Thank you for your time,This is a problem that a friend of mine gave to me a while back. I somewhat solved it and paid no attention to it afterwards until I though about it now. Forgive the errors and laziness this was a long time ago and not everything has to be perfect. I assumed to be a polynomial function of the kind where and found where and my working is in the following images : (I did this in word) I was curious if there were more solutions to just this polynomial? Perhaps a different function ? Don't bother commenting trying to correct my work I am just curious about the above questions. Thank you for your time,f f(x)=ax^b f^{-1}(x)= \big(\frac x a\big)^{1/b}  f(x)= \varphi^{-\frac{1}{\varphi}-1}(-x)^{\phi}  \varphi=\frac{1+\sqrt{5}}{2} \phi=\frac{1-\sqrt{5}}{2},"['real-analysis', 'ordinary-differential-equations', 'functions']"
38,Non-iterative deterministic function to map an input to a random output in range without repeating,Non-iterative deterministic function to map an input to a random output in range without repeating,,"Apologies if this has been answered before or is impossible, but: Is there a state-independent, non-iterative function that, given an (integer) input (n) and (integer) minimum (min) and maximum (max) values, uniquely maps n to a pseudorandom (integer) output between min and max ? By non-iterative, I mean it wouldn't have to iterate through every number in the set from min to max in order to map n . For example (I do not know LaTeX, sorry): The function is defined as f(n, min, max) f(1, 1, 5) = 5 f(2, 1, 5) = 2 f(3, 1, 5) = 4 f(4, 1, 5) = 1 and f(5, 1, 5) = 3 And similarly for other min and max values.","Apologies if this has been answered before or is impossible, but: Is there a state-independent, non-iterative function that, given an (integer) input (n) and (integer) minimum (min) and maximum (max) values, uniquely maps n to a pseudorandom (integer) output between min and max ? By non-iterative, I mean it wouldn't have to iterate through every number in the set from min to max in order to map n . For example (I do not know LaTeX, sorry): The function is defined as f(n, min, max) f(1, 1, 5) = 5 f(2, 1, 5) = 2 f(3, 1, 5) = 4 f(4, 1, 5) = 1 and f(5, 1, 5) = 3 And similarly for other min and max values.",,"['functions', 'random', 'random-functions']"
39,Proof of: There exists a function which dominates all computable ones,Proof of: There exists a function which dominates all computable ones,,"Let $C \subset \mathbb{N} \times \mathbb{N} $ be the set of all (turing-)computable functions. Then: $$\exists f \in \mathbb{N}^\mathbb{N}: C \subset o(f)$$ I am looking for a proof and the name of a potential statement implying this statement. Personally, I feel this is very counter-intuitive... This statement must be true. If you say that a problem $X \in P$ , you usually say that $$ \exists M \in   \mathcal P. \forall x \in X: \text{M accepts x with step counting function f} \in \mathcal{O(px^n)}$$ Now you can proof that such a problem is decidable by defining a Turing machines which halts after it has done at max $f(n)$ steps. You can now show that this holds true for any function with a computable step counting function.(the complexity class might change, of course) This is however does not hold true for an uncomputable step counting function $g$ . You could try to contradict, saying that for g there is a bigger $f$ , which is computable, such that $g \in \mathcal{O(f(n))}$ . But this is known to be false.","Let be the set of all (turing-)computable functions. Then: I am looking for a proof and the name of a potential statement implying this statement. Personally, I feel this is very counter-intuitive... This statement must be true. If you say that a problem , you usually say that Now you can proof that such a problem is decidable by defining a Turing machines which halts after it has done at max steps. You can now show that this holds true for any function with a computable step counting function.(the complexity class might change, of course) This is however does not hold true for an uncomputable step counting function . You could try to contradict, saying that for g there is a bigger , which is computable, such that . But this is known to be false.","C \subset \mathbb{N} \times \mathbb{N}  \exists f \in \mathbb{N}^\mathbb{N}: C \subset o(f) X \in P  \exists M \in  
\mathcal P. \forall x \in X: \text{M accepts x with step counting function f} \in \mathcal{O(px^n)} f(n) g f g \in \mathcal{O(f(n))}","['functions', 'computer-science', 'turing-machines']"
40,Difference between Sequence of Functions and Multivariable Functions,Difference between Sequence of Functions and Multivariable Functions,,"I already know what is a Multivariable Function, which looks like the following: $$f(x,y)=\frac{x}{x+y}$$ And I just know that a Sequence of Function looks like $$f_n(x)=\frac{x}{x+n}$$ That's confusing to me. Why not, instead, just put n inside of the parentheses $$f(x,n)=\frac{x}{x+n}$$ where $$n \in N^*$$ I wonder what's the difference between those two and can that sub n be considered as a input variable of $f$ ? Thank you StackExchange. <3","I already know what is a Multivariable Function, which looks like the following: And I just know that a Sequence of Function looks like That's confusing to me. Why not, instead, just put n inside of the parentheses where I wonder what's the difference between those two and can that sub n be considered as a input variable of ? Thank you StackExchange. <3","f(x,y)=\frac{x}{x+y} f_n(x)=\frac{x}{x+n} f(x,n)=\frac{x}{x+n} n \in N^* f","['multivariable-calculus', 'functions', 'notation', 'sequence-of-function']"
41,Does a differentiable function exist with $f'(0)=0$ but $f'(x_n) \to \infty$ as $x_n \to 0$,Does a differentiable function exist with  but  as,f'(0)=0 f'(x_n) \to \infty x_n \to 0,"Does a differentiable function $f:\mathbb{R}\to\mathbb{R}$ exist with $f'(0)=0$ where there is a sequence $x_n \to 0$ and $f'(x_n) \to \infty$ ? My first thought was to work with the function $f(x) = x^2\sin(1/x)$ if $x \neq 0$ and $f(x) = 0$ if $x = 0$ . This function is differentiable with derivative $f'(x) = 2x\sin(1/x) - \cos(1/x)$ if $x \neq 0$ and $f'(0) = 0$ if $x = 0$ . My issue now lies with constructing the necessary sequence. Is my thinking correct so far? Can someone help with constructing the sequence if it is possible, or show me where I went wrong in the case that it isn't possible?","Does a differentiable function exist with where there is a sequence and ? My first thought was to work with the function if and if . This function is differentiable with derivative if and if . My issue now lies with constructing the necessary sequence. Is my thinking correct so far? Can someone help with constructing the sequence if it is possible, or show me where I went wrong in the case that it isn't possible?",f:\mathbb{R}\to\mathbb{R} f'(0)=0 x_n \to 0 f'(x_n) \to \infty f(x) = x^2\sin(1/x) x \neq 0 f(x) = 0 x = 0 f'(x) = 2x\sin(1/x) - \cos(1/x) x \neq 0 f'(0) = 0 x = 0,"['real-analysis', 'sequences-and-series', 'functions', 'derivatives']"
42,Estimate $N_p$ norm of $\psi : x \longmapsto \int_{-\infty}^x \phi(t)\ \mathrm dt$,Estimate  norm of,N_p \psi : x \longmapsto \int_{-\infty}^x \phi(t)\ \mathrm dt,"Context and Assumptions : we consider the case of a very standard exercice in Schwartz functions theory: Let's suppose that $\phi$ is a $S(\mathbb{R})$ function, and let's suppose additionally that $\displaystyle\int_{-\infty}^\infty \phi(t)\ \mathrm dt=0$ . In a first time, I demonstrate that the function $\displaystyle \psi\colon\mathbb{R}\ni x\longmapsto \int_{-\infty}^x \phi(t)\ \mathrm dt\in \mathbb{R},$ defines a $S(\mathbb{R})$ function. Then, I am asked to estime $N_p$ norm of $\psi$ (also called $N_p$ semi-norm) in function of $N_p(\phi)$ . For the reminder, it is defined by : $$ N_p(f):=\sum_{\alpha\leq p,\beta \leq p}\sup \vert x^\alpha \partial^\beta f(x) \vert\text{ for all }f \in S(\mathbb{R}).$$ What I have done: $$N_p(\psi)=\sum_{\alpha\leq p,\beta \leq p}\sup \vert x^\alpha \partial^\beta\psi(x) \vert$$ $$= \sum_{\alpha\leq p,1\leq\beta \leq p}\sup \vert x^\alpha \partial^\beta\psi(x) \vert+\sum_{\alpha\leq p}\sup \vert x^\alpha \psi(x) \vert$$ $$=N_p(\phi)-\sum_{\alpha\leq p}\sup \vert x^\alpha \partial^p \phi(x) \vert+\sum_{\alpha\leq p}\sup \vert x^\alpha \psi(x) \vert$$ (I tried to copy it without failing the indexes, but if I did, I tried to isolate $N_p(\phi)$ in the terms using the fact $\psi$ is a primitive of $\phi$ ) I think there is something to find here, what I have written may not be what is waited I believe, I mean a common form should be found, I think, what would be the ""good way"" to Estimate $N_p$ norm of $\psi$ in function of $N_p(\phi)$ .","Context and Assumptions : we consider the case of a very standard exercice in Schwartz functions theory: Let's suppose that is a function, and let's suppose additionally that . In a first time, I demonstrate that the function defines a function. Then, I am asked to estime norm of (also called semi-norm) in function of . For the reminder, it is defined by : What I have done: (I tried to copy it without failing the indexes, but if I did, I tried to isolate in the terms using the fact is a primitive of ) I think there is something to find here, what I have written may not be what is waited I believe, I mean a common form should be found, I think, what would be the ""good way"" to Estimate norm of in function of .","\phi S(\mathbb{R}) \displaystyle\int_{-\infty}^\infty \phi(t)\ \mathrm dt=0 \displaystyle \psi\colon\mathbb{R}\ni x\longmapsto \int_{-\infty}^x \phi(t)\ \mathrm dt\in \mathbb{R}, S(\mathbb{R}) N_p \psi N_p N_p(\phi)  N_p(f):=\sum_{\alpha\leq p,\beta \leq p}\sup \vert x^\alpha \partial^\beta f(x) \vert\text{ for all }f \in S(\mathbb{R}). N_p(\psi)=\sum_{\alpha\leq p,\beta \leq p}\sup \vert x^\alpha \partial^\beta\psi(x) \vert = \sum_{\alpha\leq p,1\leq\beta \leq p}\sup \vert x^\alpha \partial^\beta\psi(x) \vert+\sum_{\alpha\leq p}\sup \vert x^\alpha \psi(x) \vert =N_p(\phi)-\sum_{\alpha\leq p}\sup \vert x^\alpha \partial^p \phi(x) \vert+\sum_{\alpha\leq p}\sup \vert x^\alpha \psi(x) \vert N_p(\phi) \psi \phi N_p \psi N_p(\phi)","['functional-analysis', 'functions', 'schwartz-space']"
43,"Determine the domain of the function $g(x, y, z)=\ln(16-4x^2-4y^2-z^2)$.",Determine the domain of the function .,"g(x, y, z)=\ln(16-4x^2-4y^2-z^2)","By the condition of the logarithm, we have: \begin{align*}      16-4x^2-4y^2-z^2                           &> 0 \\      4x^2+4y^2+z^2                              &<16 \\      \frac{x^2}{4}+\frac{y^2}{4}+\frac{z^2}{16} &< 1 \end{align*} Therefore the domain of $ g $ is the interior of an origin-centered ellipsoid. I think this is the correct solution, I await your comments. If anyone has a different solution or correction of my work I will be grateful.","By the condition of the logarithm, we have: Therefore the domain of is the interior of an origin-centered ellipsoid. I think this is the correct solution, I await your comments. If anyone has a different solution or correction of my work I will be grateful.","\begin{align*}
     16-4x^2-4y^2-z^2                           &> 0 \\
     4x^2+4y^2+z^2                              &<16 \\
     \frac{x^2}{4}+\frac{y^2}{4}+\frac{z^2}{16} &< 1
\end{align*}  g ",['functions']
44,Finding a number that equals another number that references it,Finding a number that equals another number that references it,,"Apologies in advance if I have not formatted this problem correctly. Context : I need to find a way to calculate a number that will equal a service fee applied to a product, taking into account the service fee will also be applied to this number. The system is sort of a middle man between a client and supplier. Currently the the supplier takes the cost of the service charge. We need to be able to allow the client to take this charge. And to do that it it needs to be added as a line item on the invoice, the problem is the service charge will be applied to the line item also as it is calculated using the total transaction value. So in the case above a £2 line item could be added to offset the original service charge, but the 2% would also be applied leaving that (£0.04) unaccounted for. Example : The cost of a product is £100, the an service fee would be (2%) £2. In this case the number couldn't be £2 because a 2% fee would also be applied to the £2 leaving £0.04. When I first looked at this problem I originally thought the value could be: ((Cost of Product) * 0.02) + ((Cost of Product) * 0.02) * 0.02) But this is wrong also as there is still a small amount remaining. Is there an easy way to calculate what the value should be?","Apologies in advance if I have not formatted this problem correctly. Context : I need to find a way to calculate a number that will equal a service fee applied to a product, taking into account the service fee will also be applied to this number. The system is sort of a middle man between a client and supplier. Currently the the supplier takes the cost of the service charge. We need to be able to allow the client to take this charge. And to do that it it needs to be added as a line item on the invoice, the problem is the service charge will be applied to the line item also as it is calculated using the total transaction value. So in the case above a £2 line item could be added to offset the original service charge, but the 2% would also be applied leaving that (£0.04) unaccounted for. Example : The cost of a product is £100, the an service fee would be (2%) £2. In this case the number couldn't be £2 because a 2% fee would also be applied to the £2 leaving £0.04. When I first looked at this problem I originally thought the value could be: ((Cost of Product) * 0.02) + ((Cost of Product) * 0.02) * 0.02) But this is wrong also as there is still a small amount remaining. Is there an easy way to calculate what the value should be?",,['functions']
45,"Let $f(x)$ be twice differentiable function, with $f(x) = x$ has $3$ roots","Let  be twice differentiable function, with  has  roots",f(x) f(x) = x 3,"Let $f(x)$ is twice differentiable increasing  function everywhere such that $f(x) = x$ has $3$ distinct root $\alpha ,\beta$ and $\gamma$ $(\alpha  < \beta  < \gamma )$ . Let $h(x) = \underset{n\to \infty }{\mathop{\lim }}\,\underset{n\,\,\,times}{\mathop{(f(f(...(f(x))))}}\,$ . (1)    If $f’’(x) > 0$ $\forall \,x\,\in \,(-\infty ,\beta )$ , $f”(x) < 0$ $ \forall x\,\in \,(\beta ,\infty ]\,$ and $f''(\beta )=0,\,$ then find $h(x)$ (2)    If $f(x)\ge x\,\forall \,x\,\in \,(-\infty ,\alpha ]\,\cup \,[\gamma ,\infty )$ and $f(x)\le x\,\forall \,x\,\in \,[\beta ,\gamma ]\,$ then find $h(x)$ I tried with a fucntion $k(x) = f(x)-x$ , such that $k(\beta)$ is $0$ . But how to do further.","Let is twice differentiable increasing  function everywhere such that has distinct root and . Let . (1)    If , and then find (2)    If and then find I tried with a fucntion , such that is . But how to do further.","f(x) f(x) = x 3 \alpha ,\beta \gamma (\alpha  < \beta  < \gamma ) h(x) = \underset{n\to \infty }{\mathop{\lim }}\,\underset{n\,\,\,times}{\mathop{(f(f(...(f(x))))}}\, f’’(x) > 0 \forall \,x\,\in \,(-\infty ,\beta ) f”(x) < 0  \forall x\,\in \,(\beta ,\infty ]\, f''(\beta )=0,\, h(x) f(x)\ge x\,\forall \,x\,\in \,(-\infty ,\alpha ]\,\cup \,[\gamma ,\infty ) f(x)\le x\,\forall \,x\,\in \,[\beta ,\gamma ]\, h(x) k(x) = f(x)-x k(\beta) 0","['functions', 'derivatives']"
46,How to prove that a rational function is a polynomial,How to prove that a rational function is a polynomial,,"$\newcommand\Q{\mathbb Q}$ I have a rational function $f(\vec x)$ over $\Q$ in many variables (7 variables). I do not know what $f$ is, but I can evaluate it for random choices of $x_1,\dots,x_7\in \Q$ without $x_i$ for any $i=1,\dots, 7$ . I always get a polynomial over $x_i$ . This indicates to me that $f$ may indeed be a polynomial. If I can evaluate $f$ in this way, is there a smart way to prove that $f$ is a polynomial? Edit : We may assume that we know what the upper bound of the total degrees of the numerator and the denominator are.","I have a rational function over in many variables (7 variables). I do not know what is, but I can evaluate it for random choices of without for any . I always get a polynomial over . This indicates to me that may indeed be a polynomial. If I can evaluate in this way, is there a smart way to prove that is a polynomial? Edit : We may assume that we know what the upper bound of the total degrees of the numerator and the denominator are.","\newcommand\Q{\mathbb Q} f(\vec x) \Q f x_1,\dots,x_7\in \Q x_i i=1,\dots, 7 x_i f f f","['functions', 'polynomials', 'rational-functions']"
47,Range of values of 'a' so that the function has local maxima/minima at given values of x,Range of values of 'a' so that the function has local maxima/minima at given values of x,,"Find the set of all the possible values of $a$ for which the function: $$f(x)=5+(a-2)x+(a-1)x^2-x^3$$ Has a local minimum value at some x<1 and local maximum value at some x>1. I started by taking the derivative first, since the derivative becomes 0 at any local maxima/minima. $$f'(x)=(a-2)+2(a-1)x-3x^2=0$$ I tried to get the roots using quadratic formula which just gives an expression in $a$ which doesn't seem to conclude at any desired result. Also, I don't understand how I can separately apply this condition for $x>1$ and $x<1$ respectively. Rolle's Mean Value Theorem also struck me since I could use it to conclude that $f'(x)$ has a root but I would need an interval for that, to get the condition $f(p)=f(q)$ so that $f'(x)$ has a root between $p$ and $q$ . Clearly, $-1$ and $+1$ would be one of these in each case but what about the other end of the interval? I'm not able to find any other x which gives the same value as $f(-1)$ or $f(+1)$ Please help me out with a way to approach this, or if there's any other better way.","Find the set of all the possible values of for which the function: Has a local minimum value at some x<1 and local maximum value at some x>1. I started by taking the derivative first, since the derivative becomes 0 at any local maxima/minima. I tried to get the roots using quadratic formula which just gives an expression in which doesn't seem to conclude at any desired result. Also, I don't understand how I can separately apply this condition for and respectively. Rolle's Mean Value Theorem also struck me since I could use it to conclude that has a root but I would need an interval for that, to get the condition so that has a root between and . Clearly, and would be one of these in each case but what about the other end of the interval? I'm not able to find any other x which gives the same value as or Please help me out with a way to approach this, or if there's any other better way.",a f(x)=5+(a-2)x+(a-1)x^2-x^3 f'(x)=(a-2)+2(a-1)x-3x^2=0 a x>1 x<1 f'(x) f(p)=f(q) f'(x) p q -1 +1 f(-1) f(+1),"['calculus', 'functions', 'derivatives', 'maxima-minima', 'rolles-theorem']"
48,How to simplify the difference between hard thresholded points that are very close?,How to simplify the difference between hard thresholded points that are very close?,,"A hard thresholding operator $H_k:\mathbb{R}^n\rightarrow \mathbb{R}^n$ is defined as a vector-valued function that maintains the top-k entries of a given vector in an absolute value sense and zero out the rest. As an example $H_2(x)=[-5,0,-3,0]^{\top}$ where $x=[-5,2,-3,1]^{\top}$ and $k=2$ . According to Is hard thresholding operator Lipschitz? , we know that it is not Lipschitz. I have a situation where arguments of hard thresholding operator are very close to each other, that is, $y_1=x$ and $y_2=x+\xi$ where $\xi$ is a random vector in $\mathbb{R}^n$ whose entries are normal random variable with zero mean and variance $\sigma$ which can be reduced as small as possible but not zero. Question : Is there anyway to simplify $||H_k(y_1)-H_k(y_2)||_2$ and have an expression in terms of $y_1, y_2$ ? Would it be possible to impose locally Lipschitzness condition given $y_1$ and $y_2$ ? Can we exploit the bound found in A Tight Bound of Hard Thresholding to find a bound on $||H_k(y_1)-H_k(y_2)||_2$ in terms of $y_1, y_2$ ? Another view : In essence, what I am asking is that when $\mathbb{E}[x+\xi]=x$ , would it be possible to say something about the following: $$ \mathbb{E}[||H_k(x)-H_k(x+\xi)||_2] $$ or $$ ||H_k(x)-\mathbb{E}[H_k(x+\xi)]||_2 $$","A hard thresholding operator is defined as a vector-valued function that maintains the top-k entries of a given vector in an absolute value sense and zero out the rest. As an example where and . According to Is hard thresholding operator Lipschitz? , we know that it is not Lipschitz. I have a situation where arguments of hard thresholding operator are very close to each other, that is, and where is a random vector in whose entries are normal random variable with zero mean and variance which can be reduced as small as possible but not zero. Question : Is there anyway to simplify and have an expression in terms of ? Would it be possible to impose locally Lipschitzness condition given and ? Can we exploit the bound found in A Tight Bound of Hard Thresholding to find a bound on in terms of ? Another view : In essence, what I am asking is that when , would it be possible to say something about the following: or","H_k:\mathbb{R}^n\rightarrow \mathbb{R}^n H_2(x)=[-5,0,-3,0]^{\top} x=[-5,2,-3,1]^{\top} k=2 y_1=x y_2=x+\xi \xi \mathbb{R}^n \sigma ||H_k(y_1)-H_k(y_2)||_2 y_1, y_2 y_1 y_2 ||H_k(y_1)-H_k(y_2)||_2 y_1, y_2 \mathbb{E}[x+\xi]=x 
\mathbb{E}[||H_k(x)-H_k(x+\xi)||_2]
 
||H_k(x)-\mathbb{E}[H_k(x+\xi)]||_2
","['linear-algebra', 'functions', 'inequality', 'lipschitz-functions']"
49,If $2^{2x-1}$ = $(\frac{1}{5})^x$ and $\log 2 = a$ prove that: $x=\frac{a}{a+1}$,If  =  and  prove that:,2^{2x-1} (\frac{1}{5})^x \log 2 = a x=\frac{a}{a+1},"This is my working out: $2^{2x} \times 2^{-1} = (5^{-1})^x$ $\log 2^{2x-1} = \log 5^{-x}$ $2x-1 \log 2 = -x\log5$ $2x-1\times a = -x\log5$ At this point, I got stuck.","This is my working out: At this point, I got stuck.",2^{2x} \times 2^{-1} = (5^{-1})^x \log 2^{2x-1} = \log 5^{-x} 2x-1 \log 2 = -x\log5 2x-1\times a = -x\log5,['functions']
50,Interpolation of $\Pi_\Sigma(N)$,Interpolation of,\Pi_\Sigma(N),"I am asking this question out of curiosity, there is not really a particular reason behind this question. $\Pi_\Sigma$ :- I created this function which I call the Factor-Sum function. I don't know if this function is a popular function used by mathematicians or not, so I am just calling it my own. So I defined, $\Pi_\Sigma(N)= \begin{cases}\sum_{i=1}^{n}{a_i}{p_i}, & \text{if}N=\prod_{i=1}^{n}{p_i}^{a_i} \\N, & \text{if} N=0,1 \\-\Pi_\Sigma(|N|), & \text{if} N<0 \end{cases}$ So this is an odd function which gives the input back when given a prime or $0$ or $1$ and gives the sum of the prime factors when given a composite. For example:- ● $\Pi_\Sigma(13)=13$ ● $\Pi_\Sigma(18)=2+3+3=8$ ● $\Pi_\Sigma(-18)=-8$ Interpolation of $\Pi_\Sigma$ :- So $\operatorname{Dom}(\Pi_\Sigma)=\mathbb{Z}$ I want to extend the domain of $\Pi_\Sigma$ to $\mathbb{R}$ . I have estimated few non-integer values of the function using newton forward formula- $f(a+hu)=f(a)+\sum_{n=1}^{\infty}\frac{u!}{n!(u-n)!}\Delta^nf(a)$ But it doesn't give an accurate or an estimated general formula for $\Pi_\Sigma$ for any real $N$ . I don't want interpolations like Peicewise constant interpolation or Linear interpolation which gives sharp edges in the graph of the function. I want an interpolation of $\Pi_\Sigma$ such that it becomes differentiable everywhere.(Such as polynomial interpolation, spline interpolation) Any help would be appreciated. (I don't really know much about interpolation that's why I need help.)","I am asking this question out of curiosity, there is not really a particular reason behind this question. :- I created this function which I call the Factor-Sum function. I don't know if this function is a popular function used by mathematicians or not, so I am just calling it my own. So I defined, So this is an odd function which gives the input back when given a prime or or and gives the sum of the prime factors when given a composite. For example:- ● ● ● Interpolation of :- So I want to extend the domain of to . I have estimated few non-integer values of the function using newton forward formula- But it doesn't give an accurate or an estimated general formula for for any real . I don't want interpolations like Peicewise constant interpolation or Linear interpolation which gives sharp edges in the graph of the function. I want an interpolation of such that it becomes differentiable everywhere.(Such as polynomial interpolation, spline interpolation) Any help would be appreciated. (I don't really know much about interpolation that's why I need help.)","\Pi_\Sigma \Pi_\Sigma(N)=
\begin{cases}\sum_{i=1}^{n}{a_i}{p_i}, & \text{if}N=\prod_{i=1}^{n}{p_i}^{a_i} \\N, & \text{if} N=0,1 \\-\Pi_\Sigma(|N|), & \text{if} N<0 \end{cases} 0 1 \Pi_\Sigma(13)=13 \Pi_\Sigma(18)=2+3+3=8 \Pi_\Sigma(-18)=-8 \Pi_\Sigma \operatorname{Dom}(\Pi_\Sigma)=\mathbb{Z} \Pi_\Sigma \mathbb{R} f(a+hu)=f(a)+\sum_{n=1}^{\infty}\frac{u!}{n!(u-n)!}\Delta^nf(a) \Pi_\Sigma N \Pi_\Sigma","['functions', 'recreational-mathematics', 'interpolation']"
51,Thurston's 37th way of thinking about the derivative,Thurston's 37th way of thinking about the derivative,,"In Thurston's superb essay On proof and progress in mathematics , he makes this observation: Of course there is always another subtlety to be gleaned, but I would like to at least think that I have absorbed the main intuition behind each element of the above list. However: Differential geometry is not my strong suit, unfortunately, so I have had trouble trying to unravel this even at a formal level. Manifolds and vector bundles themselves I am comfortable with, but with connections and connection forms I have trouble moving between formalism and intuition, and ""Lagrangian section"" is not a term I've come across (though I can find its definition online ). So, I have some questions about Thurston's 37th conception of the derivative: To use Thurston's words: can someone "" translate into precise, formal, and explicit definitions "" making the "" differences start to evaporate "" between 37 and the differential of a smooth map ? What is the intuition behind it - why should the notion of ""Lagrangian section"" appear here, what does it mean (intuitively) when a connection makes the graph of $f$ parallel, etc.? My hope is also for answers that are as accessible to as many people as possible, though of course, any explanation has to assume some level of background knowledge.","In Thurston's superb essay On proof and progress in mathematics , he makes this observation: Of course there is always another subtlety to be gleaned, but I would like to at least think that I have absorbed the main intuition behind each element of the above list. However: Differential geometry is not my strong suit, unfortunately, so I have had trouble trying to unravel this even at a formal level. Manifolds and vector bundles themselves I am comfortable with, but with connections and connection forms I have trouble moving between formalism and intuition, and ""Lagrangian section"" is not a term I've come across (though I can find its definition online ). So, I have some questions about Thurston's 37th conception of the derivative: To use Thurston's words: can someone "" translate into precise, formal, and explicit definitions "" making the "" differences start to evaporate "" between 37 and the differential of a smooth map ? What is the intuition behind it - why should the notion of ""Lagrangian section"" appear here, what does it mean (intuitively) when a connection makes the graph of $f$ parallel, etc.? My hope is also for answers that are as accessible to as many people as possible, though of course, any explanation has to assume some level of background knowledge.",,"['derivatives', 'intuition']"
52,taylor series of $\ln(1+x)$?,taylor series of ?,\ln(1+x),Compute the taylor series of $\ln(1+x)$ I've first computed derivatives (up to  the 4th) of ln(1+x) $f^{'}(x)$ = $\frac{1}{1+x}$ $f^{''}(x) = \frac{-1}{(1+x)^2}$ $f^{'''}(x) = \frac{2}{(1+x)^3}$ $f^{''''}(x) = \frac{-6}{(1+x)^4}$ Therefore the series: $\ln(1+x) = f(a) + \frac{1}{1+a}\frac{x-a}{1!} - \frac{1}{(1+a)^2}\frac{(x-a)^2}{2!} +  \frac{2}{(1+a)^3}\frac{(x-a)^3}{3!} -  \frac{6}{(1+a)^4}\frac{(x-a)^4}{4!} + ...$ But this doesn't seem to be correct. Can anyone please explain why this doesn't work? The supposed correct answers are: $$\ln(1+x) = \int \left(\frac{1}{1+x}\right)dx$$ $$\ln(1+x) = \sum_{k=0}^{\infty} \int (-x)^k dx$$,Compute the taylor series of I've first computed derivatives (up to  the 4th) of ln(1+x) = Therefore the series: But this doesn't seem to be correct. Can anyone please explain why this doesn't work? The supposed correct answers are:,\ln(1+x) f^{'}(x) \frac{1}{1+x} f^{''}(x) = \frac{-1}{(1+x)^2} f^{'''}(x) = \frac{2}{(1+x)^3} f^{''''}(x) = \frac{-6}{(1+x)^4} \ln(1+x) = f(a) + \frac{1}{1+a}\frac{x-a}{1!} - \frac{1}{(1+a)^2}\frac{(x-a)^2}{2!} +  \frac{2}{(1+a)^3}\frac{(x-a)^3}{3!} -  \frac{6}{(1+a)^4}\frac{(x-a)^4}{4!} + ... \ln(1+x) = \int \left(\frac{1}{1+x}\right)dx \ln(1+x) = \sum_{k=0}^{\infty} \int (-x)^k dx,"['derivatives', 'taylor-expansion']"
53,Why aren't integration and differentiation inverses of each other?,Why aren't integration and differentiation inverses of each other?,,"Integration is supposed to be the inverse of differentiation, but the integral of the derivative is not equal to the derivative of the integral: $$\dfrac{\mathrm{d}}{\mathrm{d}x}\left(\int f(x)\mathrm{d}x\right) = f(x) \neq \int\left(\dfrac{\mathrm{d}}{\mathrm{d}x}f(x)\right)\mathrm{d}x$$ For instance: $$\begin{align*} &\dfrac{\mathrm{d}}{\mathrm{d}x}\left(\int 2x+1\;\mathrm{d}x\right) &&= \dfrac{\mathrm{d}}{\mathrm{d}x}\left(x^2+x+C\right) &= 2x+1\\ &\int\left(\dfrac{\mathrm{d}}{\mathrm{d}x}\left(2x+1\right)\right)\mathrm{d}x &&= \int 2\;\mathrm{d}x &= 2x+C\end{align*}$$ Why isn't it defined such that $\dfrac{\mathrm{d}}{\mathrm{d}x}a = \dfrac{\mathrm{d}a}{\mathrm{d}x}$, where $a$ is a constant, and $\int f(x)\;\mathrm{d}x = F(x)$? Then we would have: $$\begin{align*} &\dfrac{\mathrm{d}}{\mathrm{d}x}\left(\int 2x+1\;\mathrm{d}x\right) &&= \dfrac{\mathrm{d}}{\mathrm{d}x}\left(x^2+x\right) &= 2x+1\\ &\int\left(\dfrac{\mathrm{d}}{\mathrm{d}x}\left(2x+1\right)\right)\mathrm{d}x &&= \int \left(2+\dfrac{\mathrm{d1}}{\mathrm{d}x}\right)\;\mathrm{d}x &= 2x+1\end{align*}$$ Then we would have: $$\dfrac{\mathrm{d}}{\mathrm{d}x}\left(\int f(x)\mathrm{d}x\right) = f(x) = \int\left(\dfrac{\mathrm{d}}{\mathrm{d}x}f(x)\right)\mathrm{d}x$$ So what is wrong with my thinking, and why isn't this the used definition?","Integration is supposed to be the inverse of differentiation, but the integral of the derivative is not equal to the derivative of the integral: $$\dfrac{\mathrm{d}}{\mathrm{d}x}\left(\int f(x)\mathrm{d}x\right) = f(x) \neq \int\left(\dfrac{\mathrm{d}}{\mathrm{d}x}f(x)\right)\mathrm{d}x$$ For instance: $$\begin{align*} &\dfrac{\mathrm{d}}{\mathrm{d}x}\left(\int 2x+1\;\mathrm{d}x\right) &&= \dfrac{\mathrm{d}}{\mathrm{d}x}\left(x^2+x+C\right) &= 2x+1\\ &\int\left(\dfrac{\mathrm{d}}{\mathrm{d}x}\left(2x+1\right)\right)\mathrm{d}x &&= \int 2\;\mathrm{d}x &= 2x+C\end{align*}$$ Why isn't it defined such that $\dfrac{\mathrm{d}}{\mathrm{d}x}a = \dfrac{\mathrm{d}a}{\mathrm{d}x}$, where $a$ is a constant, and $\int f(x)\;\mathrm{d}x = F(x)$? Then we would have: $$\begin{align*} &\dfrac{\mathrm{d}}{\mathrm{d}x}\left(\int 2x+1\;\mathrm{d}x\right) &&= \dfrac{\mathrm{d}}{\mathrm{d}x}\left(x^2+x\right) &= 2x+1\\ &\int\left(\dfrac{\mathrm{d}}{\mathrm{d}x}\left(2x+1\right)\right)\mathrm{d}x &&= \int \left(2+\dfrac{\mathrm{d1}}{\mathrm{d}x}\right)\;\mathrm{d}x &= 2x+1\end{align*}$$ Then we would have: $$\dfrac{\mathrm{d}}{\mathrm{d}x}\left(\int f(x)\mathrm{d}x\right) = f(x) = \int\left(\dfrac{\mathrm{d}}{\mathrm{d}x}f(x)\right)\mathrm{d}x$$ So what is wrong with my thinking, and why isn't this the used definition?",,"['derivatives', 'inverse']"
54,"Differentiation, using d or delta","Differentiation, using d or delta",,Are the symbols $d$ and $\delta$ equivalent in expressions like $dy/dx$? Or do they mean something different? Thanks,Are the symbols $d$ and $\delta$ equivalent in expressions like $dy/dx$? Or do they mean something different? Thanks,,"['notation', 'derivatives']"
55,Notation of the second derivative - Where does the $d$ go? [duplicate],Notation of the second derivative - Where does the  go? [duplicate],d,"This question already has answers here : Why is the 2nd derivative written as $\frac{\mathrm d^2y}{\mathrm dx^2}$? (6 answers) Closed 4 months ago . In school, I was taught that we use $\dfrac{du}{dx}$ as a notation for the first derivative of a function $u(x)$ . I was also told that we could use the $d$ just like any variable. After some time we were given the notation for the second derivative and it was explained as follows: $$ \frac{d\left(\frac{du}{dx}\right)}{dx} = \frac{d^2 u}{dx^2} $$ What I do not get here is, if we can use the $d$ as any variable, I would get the following result: $$ \frac{d\left(\frac{du}{dx}\right)}{dx} =\frac{ddu}{dx\,dx} = \frac{d^2 u}{d^2 x^2} $$ Apparently, it is not the same as the notation we were given. A $d$ is missing. I have done some research on this and found some vague comments about ""There are reasons for that, but you do not need to know..."" or ""That is mainly a  notation issue, but you do not need to know further."" So what I am asking for is: Is this really just a notation thing? If so, does this mean we can actually NOT use d like a variable? If not, where does the $d$ go? I found this related question, but it does not really answer my specific question. So I would not see it as a duplicate, but correct me if my search has not been sufficient and there indeed is a similar question out there already.","This question already has answers here : Why is the 2nd derivative written as $\frac{\mathrm d^2y}{\mathrm dx^2}$? (6 answers) Closed 4 months ago . In school, I was taught that we use as a notation for the first derivative of a function . I was also told that we could use the just like any variable. After some time we were given the notation for the second derivative and it was explained as follows: What I do not get here is, if we can use the as any variable, I would get the following result: Apparently, it is not the same as the notation we were given. A is missing. I have done some research on this and found some vague comments about ""There are reasons for that, but you do not need to know..."" or ""That is mainly a  notation issue, but you do not need to know further."" So what I am asking for is: Is this really just a notation thing? If so, does this mean we can actually NOT use d like a variable? If not, where does the go? I found this related question, but it does not really answer my specific question. So I would not see it as a duplicate, but correct me if my search has not been sufficient and there indeed is a similar question out there already.","\dfrac{du}{dx} u(x) d 
\frac{d\left(\frac{du}{dx}\right)}{dx} = \frac{d^2 u}{dx^2}
 d 
\frac{d\left(\frac{du}{dx}\right)}{dx} =\frac{ddu}{dx\,dx} = \frac{d^2 u}{d^2 x^2}
 d d","['derivatives', 'notation']"
56,"Is there any meaning to this ""Super Derivative"" operation I invented?","Is there any meaning to this ""Super Derivative"" operation I invented?",,"Does anyone know anything about the following ""super-derivative"" operation? I just made this up so I don't know where to look, but it appears to have very meaningful properties. An answer to this question could be a reference and explanation, or known similar idea/name, or just any interesting properties or corollaries you can see from the definition here? Is there perhaps a better definition than the one I am using? What is your intuition for what the operator is doing (i.e. is it still in any sense a gradient)? Is there a way to separate the log part out, or remove it? Or is that an essential feature? Definition: I'm using the word ""super-derivative"" but that is a made-up name. Define the ""super-derivative"", operator $S_x^{\alpha}$ , about $\alpha$ , using the derivative type limit equation on the fractional derivative operator $D_x^\alpha$ $$ S_x^{\alpha}  = \lim_{h \to 0} \frac{D^{\alpha+h}_x-D^{\alpha}_x}{h} $$ then for a function $$ S_x^{\alpha} f(x) = \lim_{h \to 0} \frac{D^{\alpha+h}_xf(x)-D^{\alpha}_x f(x)}{h} $$ for example, the [Riemann-Liouville, see appendix] fractional derivative of a power function is $$ D_x^\alpha x^k = \frac{\Gamma(k+1)}{\Gamma(k-\alpha+1)}x^{k-\alpha} $$ and apparently $$ S_x^{\alpha} x^k =  \frac{\Gamma (k+1) x^{k-\alpha} (\psi ^{(0)}(-\alpha+k+1) - \log (x))}{\Gamma (-\alpha+k+1)} = (\psi ^{(0)}(-\alpha+k+1) - \log (x)) D_x^\alpha x^k $$ a nice example of this, the super-derivative of $x$ at $\alpha=1$ is $-\gamma - \log(x)$ , which turns up commonly. I'm wondering if this could be used to describe the series expansions of certain functions that have log or $\gamma$ terms, e.g. BesselK functions, or the Gamma function. Potential relation to Bessel functions : For example, a fundamental function with this kind of series, (the inverse Mellin transform of $\Gamma(s)^2$ ), is $2 K_0(2 \sqrt{x})$ with $$ 2 K_0(2 \sqrt{x}) = (-\log (x)-2 \gamma )+x (-\log (x)-2 \gamma +2)+\frac{1}{4} x^2 (-\log (x)-2 \gamma +3)+\\ +\frac{1}{108} x^3 (-3 \log (x)-6 \gamma +11)+\frac{x^4 (-6 \log (x)-12 \gamma +25)}{3456}+O\left(x^5\right) $$ in the end, taking the super-derivative of polynomials and matching coefficients we find $$ S_x^1[2 \sqrt{x}I_1(2\sqrt{x})] + I_0(2 \sqrt{x})\log(x) = 2K_0(2 \sqrt{x}) $$ which can also potentially be written in terms of linear operators as $$ [2 S_x x D_x + \log(x)]I_0(2 \sqrt{x}) = 2K_0(2 \sqrt{x}) $$ likewise $$ [2 S_x x D_x - \log(x)]J_0(2 \sqrt{x}) = \pi Y_0(2 \sqrt{x}) $$ I like this because it's similar to an eigensystem, but the eigenfunctions swap over. Gamma Function: We can potentially define higher-order derivatives, for example $$ (S_x^{\alpha})^2  = \lim_{h \to 0} \frac{D^{\alpha+h}_x-2 D^{\alpha}_x + D^{\alpha-h}_x}{h^2} $$ and $$ (S_x^{\alpha})^3 = \lim_{h \to 0} \frac{D^{\alpha+3h}_x-3 D^{\alpha+2h}_x + 3 D^{\alpha+h}_x - D^{\alpha}_x}{h^3} $$ this would be needed if there was any hope of explaining the series $$ \Gamma(x) =  \frac{1}{x}-\gamma +\frac{1}{12} \left(6 \gamma ^2+\pi ^2\right)     x+\frac{1}{6} x^2 \left(-\gamma ^3-\frac{\gamma  \pi ^2}{2}+\psi     ^{(2)}(1)\right)+ \\+\frac{1}{24} x^3 \left(\gamma ^4+\gamma ^2 \pi ^2+\frac{3     \pi ^4}{20}-4 \gamma  \psi ^{(2)}(1)\right)+O\left(x^4\right) $$ using the 'super-derivative'. This appears to be $$ \Gamma(x) = [(S^1_x)^0 x]_{x=1} x^{-1} + [(S^1_x)^1 x]_{x=1} x +  \frac{1}{2}[(S^1_x)^2 x]_{x=1} x^2 + \frac{1}{6} [(S^1_x)^3 x]_{x=1} x^3 + \cdots $$ so one could postulate $$ \Gamma(x) = \frac{1}{x}\sum_{k=0}^\infty \frac{1}{k!}[(S^1_x)^k x]_{x=1} x^{k} $$ which I think is quite beautiful. Appendix: I used the following definition for the fractional derivative: $$ D_x^\alpha f(x) = \frac{1}{\Gamma(-\alpha)}\int_0^x (x-t)^{-\alpha-1} f(t) \; dt $$ implemented for example by the Wolfram Mathematica code found here FractionalD[\[Alpha]_, f_, x_, opts___] :=    Integrate[(x - t)^(-\[Alpha] - 1) (f /. x -> t), {t, 0, x},      opts, GenerateConditions -> False]/Gamma[-\[Alpha]]  FractionalD[\[Alpha]_?Positive, f_, x_, opts___] :=  Module[   {m = Ceiling[\[Alpha]]},    If[\[Alpha] \[Element] Integers,      D[f, {x, \[Alpha]}],      D[FractionalD[-(m - \[Alpha]), f, x, opts], {x, m}]   ] ] I'm happy to hear more about other definitions for the fractional operators, and whether they are more suitable.","Does anyone know anything about the following ""super-derivative"" operation? I just made this up so I don't know where to look, but it appears to have very meaningful properties. An answer to this question could be a reference and explanation, or known similar idea/name, or just any interesting properties or corollaries you can see from the definition here? Is there perhaps a better definition than the one I am using? What is your intuition for what the operator is doing (i.e. is it still in any sense a gradient)? Is there a way to separate the log part out, or remove it? Or is that an essential feature? Definition: I'm using the word ""super-derivative"" but that is a made-up name. Define the ""super-derivative"", operator , about , using the derivative type limit equation on the fractional derivative operator then for a function for example, the [Riemann-Liouville, see appendix] fractional derivative of a power function is and apparently a nice example of this, the super-derivative of at is , which turns up commonly. I'm wondering if this could be used to describe the series expansions of certain functions that have log or terms, e.g. BesselK functions, or the Gamma function. Potential relation to Bessel functions : For example, a fundamental function with this kind of series, (the inverse Mellin transform of ), is with in the end, taking the super-derivative of polynomials and matching coefficients we find which can also potentially be written in terms of linear operators as likewise I like this because it's similar to an eigensystem, but the eigenfunctions swap over. Gamma Function: We can potentially define higher-order derivatives, for example and this would be needed if there was any hope of explaining the series using the 'super-derivative'. This appears to be so one could postulate which I think is quite beautiful. Appendix: I used the following definition for the fractional derivative: implemented for example by the Wolfram Mathematica code found here FractionalD[\[Alpha]_, f_, x_, opts___] :=    Integrate[(x - t)^(-\[Alpha] - 1) (f /. x -> t), {t, 0, x},      opts, GenerateConditions -> False]/Gamma[-\[Alpha]]  FractionalD[\[Alpha]_?Positive, f_, x_, opts___] :=  Module[   {m = Ceiling[\[Alpha]]},    If[\[Alpha] \[Element] Integers,      D[f, {x, \[Alpha]}],      D[FractionalD[-(m - \[Alpha]), f, x, opts], {x, m}]   ] ] I'm happy to hear more about other definitions for the fractional operators, and whether they are more suitable.","S_x^{\alpha} \alpha D_x^\alpha 
S_x^{\alpha}  = \lim_{h \to 0} \frac{D^{\alpha+h}_x-D^{\alpha}_x}{h}
 
S_x^{\alpha} f(x) = \lim_{h \to 0} \frac{D^{\alpha+h}_xf(x)-D^{\alpha}_x f(x)}{h}
 
D_x^\alpha x^k = \frac{\Gamma(k+1)}{\Gamma(k-\alpha+1)}x^{k-\alpha}
 
S_x^{\alpha} x^k =  \frac{\Gamma (k+1) x^{k-\alpha} (\psi ^{(0)}(-\alpha+k+1) - \log (x))}{\Gamma (-\alpha+k+1)} = (\psi ^{(0)}(-\alpha+k+1) - \log (x)) D_x^\alpha x^k
 x \alpha=1 -\gamma - \log(x) \gamma \Gamma(s)^2 2 K_0(2 \sqrt{x}) 
2 K_0(2 \sqrt{x}) = (-\log (x)-2 \gamma )+x (-\log (x)-2 \gamma +2)+\frac{1}{4} x^2 (-\log (x)-2 \gamma +3)+\\
+\frac{1}{108} x^3 (-3 \log (x)-6 \gamma +11)+\frac{x^4 (-6 \log (x)-12 \gamma +25)}{3456}+O\left(x^5\right)
 
S_x^1[2 \sqrt{x}I_1(2\sqrt{x})] + I_0(2 \sqrt{x})\log(x) = 2K_0(2 \sqrt{x})
 
[2 S_x x D_x + \log(x)]I_0(2 \sqrt{x}) = 2K_0(2 \sqrt{x})
 
[2 S_x x D_x - \log(x)]J_0(2 \sqrt{x}) = \pi Y_0(2 \sqrt{x})
 
(S_x^{\alpha})^2  = \lim_{h \to 0} \frac{D^{\alpha+h}_x-2 D^{\alpha}_x + D^{\alpha-h}_x}{h^2}
 
(S_x^{\alpha})^3 = \lim_{h \to 0} \frac{D^{\alpha+3h}_x-3 D^{\alpha+2h}_x + 3 D^{\alpha+h}_x - D^{\alpha}_x}{h^3}
 
\Gamma(x) =  \frac{1}{x}-\gamma +\frac{1}{12} \left(6 \gamma ^2+\pi ^2\right)
    x+\frac{1}{6} x^2 \left(-\gamma ^3-\frac{\gamma  \pi ^2}{2}+\psi
    ^{(2)}(1)\right)+
\\+\frac{1}{24} x^3 \left(\gamma ^4+\gamma ^2 \pi ^2+\frac{3
    \pi ^4}{20}-4 \gamma  \psi ^{(2)}(1)\right)+O\left(x^4\right)
 
\Gamma(x) = [(S^1_x)^0 x]_{x=1} x^{-1} + [(S^1_x)^1 x]_{x=1} x +  \frac{1}{2}[(S^1_x)^2 x]_{x=1} x^2 + \frac{1}{6} [(S^1_x)^3 x]_{x=1} x^3 + \cdots
 
\Gamma(x) = \frac{1}{x}\sum_{k=0}^\infty \frac{1}{k!}[(S^1_x)^k x]_{x=1} x^{k}
 
D_x^\alpha f(x) = \frac{1}{\Gamma(-\alpha)}\int_0^x (x-t)^{-\alpha-1} f(t) \; dt
","['derivatives', 'gamma-function', 'bessel-functions', 'fractional-calculus']"
57,Reverse mode differentiation vs. forward mode differentiation - where are the benefits?,Reverse mode differentiation vs. forward mode differentiation - where are the benefits?,,"According to Wikipedia forward mode differentiation is preferred when $f: \mathbb{R}^n \mapsto \mathbb{R}^m$ , m >> n. I cannot see any computational benefits. Let us take simple example: $f(x,y) = sin(xy)$ . We can visualize it as graph with four nodes and 3 edges. Top node is $\sin(xy)$ , node one level below is $xy$ and two initial nodes are $x$ and $y$ . Derivatives on nodes are $\cos(xy)$ , $x$ , and $y$ . For both reverse and forward mode differentiation we have to compute these derivatives. How is reverse mode differentiation is computationally superior here?","According to Wikipedia forward mode differentiation is preferred when , m >> n. I cannot see any computational benefits. Let us take simple example: . We can visualize it as graph with four nodes and 3 edges. Top node is , node one level below is and two initial nodes are and . Derivatives on nodes are , , and . For both reverse and forward mode differentiation we have to compute these derivatives. How is reverse mode differentiation is computationally superior here?","f: \mathbb{R}^n \mapsto \mathbb{R}^m f(x,y) = sin(xy) \sin(xy) xy x y \cos(xy) x y",['derivatives']
58,Why isn't there a contravariant derivative? (Or why are all derivatives covariant?),Why isn't there a contravariant derivative? (Or why are all derivatives covariant?),,"Question: If there exists a covariant derivative, then why doesn't there also exist a ""contravariant derivative""? Why are all or most forms of differentiation ""covariant"", or rather why do all or most forms of differentiation transform covariantly? What aspect of differentiation makes it intrinsically ""covariant"" and intrinsically "" not contravariant""? Why isn't the notion of differentiation agnostic to ""co/contra-variance""? Motivation: To me it is unclear (on an intuitive, i.e. stupid/lazy, level) how notions of differentiation could be restrained to being either ""covariant"" or ""contravariant"", since any notion of differentiation should be linear*, and the dual of any vector space is exactly as linear as the original vector space, i.e. vector space operations in the dual vector space still commute with linear functions and operators, they same way they commute with such linear objects in the original vector space. So to the extent that the notion of linearity is ""agnostic"" to whether we are working with objects from a vector space or from its dual vector space, so I would have expected any notion of differentiation to be similarly ""agnostic"". Perhaps a better word would be ""symmetric"" -- naively, I would have expected that if a notion of ""covariant differentiation"" exists, then a notion of ""contravariant differentiation"" should also exist, because naively I would have expected one to exist if and only if the other exists. However, it appears that no such thing as ""contravariant derivative"" exists ( see here on Math.SE , also these two posts [a] [b] on PhysicsForums), whereas obviously a notion of ""covariant derivative"" is used very frequently and profitably in differential geometry. Even differential operators besides the so-called ""covariant derivative"" seemingly transform covariantly, see this post for a discussion revolving around this property for the gradient. I don't understand why this is the case. (* I think)","Question: If there exists a covariant derivative, then why doesn't there also exist a ""contravariant derivative""? Why are all or most forms of differentiation ""covariant"", or rather why do all or most forms of differentiation transform covariantly? What aspect of differentiation makes it intrinsically ""covariant"" and intrinsically "" not contravariant""? Why isn't the notion of differentiation agnostic to ""co/contra-variance""? Motivation: To me it is unclear (on an intuitive, i.e. stupid/lazy, level) how notions of differentiation could be restrained to being either ""covariant"" or ""contravariant"", since any notion of differentiation should be linear*, and the dual of any vector space is exactly as linear as the original vector space, i.e. vector space operations in the dual vector space still commute with linear functions and operators, they same way they commute with such linear objects in the original vector space. So to the extent that the notion of linearity is ""agnostic"" to whether we are working with objects from a vector space or from its dual vector space, so I would have expected any notion of differentiation to be similarly ""agnostic"". Perhaps a better word would be ""symmetric"" -- naively, I would have expected that if a notion of ""covariant differentiation"" exists, then a notion of ""contravariant differentiation"" should also exist, because naively I would have expected one to exist if and only if the other exists. However, it appears that no such thing as ""contravariant derivative"" exists ( see here on Math.SE , also these two posts [a] [b] on PhysicsForums), whereas obviously a notion of ""covariant derivative"" is used very frequently and profitably in differential geometry. Even differential operators besides the so-called ""covariant derivative"" seemingly transform covariantly, see this post for a discussion revolving around this property for the gradient. I don't understand why this is the case. (* I think)",,"['differential-geometry', 'derivatives', 'riemannian-geometry', 'tensors']"
59,What is the derivative of ${}^xx$,What is the derivative of,{}^xx,"How would one find: $$\frac{\mathrm d}{\mathrm dx}{}^xx?$$ where ${}^ba$ is defined by $${}^ba\stackrel{\mathrm{def}}{=}\underbrace{ a^{a^{\cdot^{\cdot^{\cdot^a}}}}}_{\text{$b$ times}}$$ Work so far The interval that I am working in is $(0, \infty)$. It doesn't make much sense to consider negative numbers. Although there exists no extension to the reals for tetration I am going to assume that it exists. My theory is that it shouldn't change the algebra involved; (correct me if I am wrong). Some visual analysis on the curve and you can see that it diverges to $+\infty$ extremely rapidly. This means that the derivative is going to have similar properties as well. Let $f(x, y):={}^yx$ so we can rewrite our tetration as $f(x, x)$. Now using the definition of the total derivative: $D\;g(x, y)=\partial_xg(x, y)+\partial_yg(x, y)$. This should allow us to differentiate $f$. $$D\;f(x,y)=\frac{\partial}{\partial x}{}^yx+\frac{\partial}{\partial y}{}^yx$$ Let's focus on the first partial derivative $\partial_x{}^yx$. This is just the case of differentiating a finite power tower as $y$ is treated constant. Firstly looking at some examples do derive a general formula for $D\;\;{}^nx$: $$ \begin{array}{c|c} n & D\;\;{}^nx\\ \hline 0 & 0\\ 1 & 1\\ 2 & {}^2x(\log x + 1)\\ 3 & {}^3x\times {}^2x\times x^{-1}(x\log x(\log x + 1)+1) \end{array} $$ It is easy to see that there is some pattern emerging however because of it's recursive nature I could not form a formula to describe it. Edit $$\dfrac{d}{dx}\left(e^{{}^nx \log(x)}\right)={}^{n+1}x\dfrac{d}{dx}\left({}^nx \log(x)\right)={}^{n+1}x\left(({}^nx)' \log(x)+\frac{{}^nx}{x}\right)$$   The recursive formula for the partial was pointed out in comments however an explicit formula would be more useful for this purpose. The second partial derivative is interesting and relies on properties of tetration. I was hoping for it to be similar to exponention such that $$D_y \;x^y=D_y\;e^{y\log x}=e^{y \log x}\log x\;D_y\;y=x^y\log x$$ However I am not sure of an '$e$ for tetration' but I hope it would be something like this: $$D_y \;{}^yx=D_y\;{}^{y\;\text{slog} x}t={}^{y\;\text{slog} x}t\;\text{slog} x\;D_y\;y={}^yx\;\text{slog}\; x$$ Where $\text{slog}$ denotes the super logarithm (slogorithm), an inverse of tetration. Edit This may as well be a possible identity which can easily be applied to the above:   $$\text{slog}\;\left({}^yx\right)=\text{slog}^y\;(x)$$ I am unsure about using slogorithms and tetration in this way and I feel I might just be abusing notation. Work on Tetration I will update this section with more rigorous definitions and properties of tetration. I cannot prove all of them now. For $x\in \Bbb R$ and $n \in \Bbb N$, $${}^nx:=\underbrace{ x^{x^{\cdot^{\cdot^{\cdot^x}}}}}_{\text{$n$ times}}\tag{1}$$ For $x\in\Bbb R$ and $a,b\in\Bbb N$?, $${{}^b({}^ax)={}^{a^b}x\tag{$\not2$}}$$ Through simply algebra you can find that the above is not the case. Update: This is just differentiating the pentation function.","How would one find: $$\frac{\mathrm d}{\mathrm dx}{}^xx?$$ where ${}^ba$ is defined by $${}^ba\stackrel{\mathrm{def}}{=}\underbrace{ a^{a^{\cdot^{\cdot^{\cdot^a}}}}}_{\text{$b$ times}}$$ Work so far The interval that I am working in is $(0, \infty)$. It doesn't make much sense to consider negative numbers. Although there exists no extension to the reals for tetration I am going to assume that it exists. My theory is that it shouldn't change the algebra involved; (correct me if I am wrong). Some visual analysis on the curve and you can see that it diverges to $+\infty$ extremely rapidly. This means that the derivative is going to have similar properties as well. Let $f(x, y):={}^yx$ so we can rewrite our tetration as $f(x, x)$. Now using the definition of the total derivative: $D\;g(x, y)=\partial_xg(x, y)+\partial_yg(x, y)$. This should allow us to differentiate $f$. $$D\;f(x,y)=\frac{\partial}{\partial x}{}^yx+\frac{\partial}{\partial y}{}^yx$$ Let's focus on the first partial derivative $\partial_x{}^yx$. This is just the case of differentiating a finite power tower as $y$ is treated constant. Firstly looking at some examples do derive a general formula for $D\;\;{}^nx$: $$ \begin{array}{c|c} n & D\;\;{}^nx\\ \hline 0 & 0\\ 1 & 1\\ 2 & {}^2x(\log x + 1)\\ 3 & {}^3x\times {}^2x\times x^{-1}(x\log x(\log x + 1)+1) \end{array} $$ It is easy to see that there is some pattern emerging however because of it's recursive nature I could not form a formula to describe it. Edit $$\dfrac{d}{dx}\left(e^{{}^nx \log(x)}\right)={}^{n+1}x\dfrac{d}{dx}\left({}^nx \log(x)\right)={}^{n+1}x\left(({}^nx)' \log(x)+\frac{{}^nx}{x}\right)$$   The recursive formula for the partial was pointed out in comments however an explicit formula would be more useful for this purpose. The second partial derivative is interesting and relies on properties of tetration. I was hoping for it to be similar to exponention such that $$D_y \;x^y=D_y\;e^{y\log x}=e^{y \log x}\log x\;D_y\;y=x^y\log x$$ However I am not sure of an '$e$ for tetration' but I hope it would be something like this: $$D_y \;{}^yx=D_y\;{}^{y\;\text{slog} x}t={}^{y\;\text{slog} x}t\;\text{slog} x\;D_y\;y={}^yx\;\text{slog}\; x$$ Where $\text{slog}$ denotes the super logarithm (slogorithm), an inverse of tetration. Edit This may as well be a possible identity which can easily be applied to the above:   $$\text{slog}\;\left({}^yx\right)=\text{slog}^y\;(x)$$ I am unsure about using slogorithms and tetration in this way and I feel I might just be abusing notation. Work on Tetration I will update this section with more rigorous definitions and properties of tetration. I cannot prove all of them now. For $x\in \Bbb R$ and $n \in \Bbb N$, $${}^nx:=\underbrace{ x^{x^{\cdot^{\cdot^{\cdot^x}}}}}_{\text{$n$ times}}\tag{1}$$ For $x\in\Bbb R$ and $a,b\in\Bbb N$?, $${{}^b({}^ax)={}^{a^b}x\tag{$\not2$}}$$ Through simply algebra you can find that the above is not the case. Update: This is just differentiating the pentation function.",,"['derivatives', 'tetration', 'power-towers']"
60,Derive or differentiate?,Derive or differentiate?,,"When the action is: Taking the derivative what verb should be used? to differentiate to derive I feel that deriving is not the correct word here. In my mind it's more a synonym of deducing . Am I right or has the word derive got the same meaning as differentiate ? Or perhaps differentiate is not a proper English word...? If so, can anyone name a book or article  where the writer(s) (preferably native English speaker(s)) use the word derive to mean differentiate ? Or should we always stick to saying: ""Take the derivative of...""? Edit : So from what I can tell, the phrase:  ""Derive a method for differentiating this function and write down the resulting derivative."", can only have one meaning. XD","When the action is: Taking the derivative what verb should be used? to differentiate to derive I feel that deriving is not the correct word here. In my mind it's more a synonym of deducing . Am I right or has the word derive got the same meaning as differentiate ? Or perhaps differentiate is not a proper English word...? If so, can anyone name a book or article  where the writer(s) (preferably native English speaker(s)) use the word derive to mean differentiate ? Or should we always stick to saying: ""Take the derivative of...""? Edit : So from what I can tell, the phrase:  ""Derive a method for differentiating this function and write down the resulting derivative."", can only have one meaning. XD",,"['derivatives', 'soft-question', 'terminology']"
61,Divergence of the Derivative of the Prime Counting Function,Divergence of the Derivative of the Prime Counting Function,,"On the one hand, the Prime Counting Function $\pi_0(x)$ maybe be written $$ \pi_0(x) = \operatorname{R}(x^1) - \sum_{\rho}\operatorname{R}(x^{\rho}) \tag{1} $$ with $    \operatorname{R}(z) = \sum_{n=1}^{\infty} \frac{ \mu (n)}{n} \operatorname{li}(z^{1/n})$ and $\rho$ running over all the zeros of $\zeta$ function. The derivative of $(1)$ is  $$ \pi_0'(x) = \operatorname{R}'(x) - \sum_{\rho}\operatorname{R}'(x^{\rho}) \tag{2} $$ with $\displaystyle    \operatorname{R}'(x^k) = \sum_{n=1}^{\infty} \frac{ \mu (n)}{n} \frac{x^{k/n-1}}{\log x}$. On the other hand, we have  $$ \pi_1(x)=\sum_{k=1}^{\infty} H(x-p_k),  $$ with $H(\cdot)$ being the Heaviside function. This would result in  (a prime comb) $$ \pi_1'(x)=\sum_{k=1}^\infty \delta(x-p_k).\tag{3} $$ $(3)$ obviously diverges at a prime $p_k$. Is it possible to show that $(2)$ also diverges? Thanks to Raymond there is also a nice gif : $\hskip1.7in$ The lower animated graph is the derivative of the function above, and we see the positions of the primes emerging as Dirac delta-type spikes. The horizontal bar in the middle has been constructed so that the intensity of brightness is related to the absolute value of the derivative. In this way we see the positions of the primes gradually emerge as narrow bands of light. [for reference see Matthew Watkins homepage ].","On the one hand, the Prime Counting Function $\pi_0(x)$ maybe be written $$ \pi_0(x) = \operatorname{R}(x^1) - \sum_{\rho}\operatorname{R}(x^{\rho}) \tag{1} $$ with $    \operatorname{R}(z) = \sum_{n=1}^{\infty} \frac{ \mu (n)}{n} \operatorname{li}(z^{1/n})$ and $\rho$ running over all the zeros of $\zeta$ function. The derivative of $(1)$ is  $$ \pi_0'(x) = \operatorname{R}'(x) - \sum_{\rho}\operatorname{R}'(x^{\rho}) \tag{2} $$ with $\displaystyle    \operatorname{R}'(x^k) = \sum_{n=1}^{\infty} \frac{ \mu (n)}{n} \frac{x^{k/n-1}}{\log x}$. On the other hand, we have  $$ \pi_1(x)=\sum_{k=1}^{\infty} H(x-p_k),  $$ with $H(\cdot)$ being the Heaviside function. This would result in  (a prime comb) $$ \pi_1'(x)=\sum_{k=1}^\infty \delta(x-p_k).\tag{3} $$ $(3)$ obviously diverges at a prime $p_k$. Is it possible to show that $(2)$ also diverges? Thanks to Raymond there is also a nice gif : $\hskip1.7in$ The lower animated graph is the derivative of the function above, and we see the positions of the primes emerging as Dirac delta-type spikes. The horizontal bar in the middle has been constructed so that the intensity of brightness is related to the absolute value of the derivative. In this way we see the positions of the primes gradually emerge as narrow bands of light. [for reference see Matthew Watkins homepage ].",,"['derivatives', 'prime-numbers', 'special-functions', 'analytic-number-theory']"
62,Log of Softmax function Derivative.,Log of Softmax function Derivative.,,"Could someone explain how that derivative was arrived at. According to me, the derivative of $\log(\text{softmax})$ is $$ \nabla\log(\text{softmax}) = \begin{cases} 1-\text{softmax},  & \text{if $i=j$} \\ -\text{softmax}, & \text{if $i \neq j$} \end{cases} $$ Where did that expectation come from? $\phi(s,a)$ is a vector, $\theta$ is also a vector. $\pi(s,a)$ denotes the probability of taking action a in state s.","Could someone explain how that derivative was arrived at. According to me, the derivative of is Where did that expectation come from? is a vector, is also a vector. denotes the probability of taking action a in state s.","\log(\text{softmax}) 
\nabla\log(\text{softmax}) =
\begin{cases}
1-\text{softmax},  & \text{if i=j} \\
-\text{softmax}, & \text{if i \neq j}
\end{cases}
 \phi(s,a) \theta \pi(s,a)","['derivatives', 'machine-learning', 'gradient-descent']"
63,Nth derivative of $x^x$,Nth derivative of,x^x,"A few days ago, I was wondering about a series expansion for $$f(x) = \  x^x$$ so I tried taking a couple derivatives. From these I was able to guess at some formulas for the general coefficients of the first few terms of the nth derivative as well as the last term (where I put the terms of the derivative in the order  $$f^{(n)}(x) = c_1x^x + c_2x^{x-1} + c_3x^{x-2}\ + \ ... \ +\ c_nx^{x-(n-1)}$$ Please let me know if there is a general formula for the coefficients as a function of n. In my own fooling around, I found that $$\ln(x)+1$$ appears quite a bit, so I called this $a$ in my work. Since I would like to center my expansion at $1$, and this expression evaluated to $1$, at $x=0$, I was interested in writing my formulas as functions of $n$ and $a$. I found that you get something of the form (this could very well be completely incorrect):  $$f^{(n)}(x) = ax^x + a^{n-2}{\binom n2}x^{x-1} + a^{n-4}(3{\binom n4}-a{\binom n3})x^{x-2}\\ + a^{n-6}(15{\binom n6}-10{\binom n5}a+2{\binom n4}a^2)x^{x-3}\ + \ ... \ +\ (-1)^n(n-2)! \ x^{x-(n-1)}$$ An interesting property that seemed to recur for the various coefficients was  that once I found a recursion formula for the coefficient in question and plugged in my initial value, it always seemed to be the same as taking the integral of the previous coefficient with respect to $a$. That is to say, if I plugged in the nth coefficient (a polynomial of $a$) into my recursion formula for the $(n+1)$th coefficient, this gave me the same result as integrating. In general, as reflected by the few coefficient formulas above,  the formula for the coefficient of $$x^{x-c}$$ where c is an integer such that $$0\le c<n$$ ""Stabilized"" to a polynomial of a with c terms after the first few applications of the recursion formula. This stabilization occurred when $$n=2c$$ Also, as a direct result of the tentative and incomplete formula I have written above, the first nonzero-value for the coefficient of $$x^{x-c}$$ is $$(-1)^{c+1}(c-1)!$$ and appears in $$f^{(c+1)}(x)$$ If these various properties are indeed true, can anyone explain why this would be the case? I am aware that this function can be expanded using the expansion for $$e^x$$ with $$x\ln(x)$$ in the exponent. This does not strike me as particularly enlightening (let me know if I'm missing something). I am most interested in the coefficients on the binomial coefficients, since these are the only things that stand between me and a general formula for the coefficients (since the form is quite standard). These are: $$1$$ then $$3,-1$$ then $$15, -10,2$$The sum of these coefficients should also give the nth derivative evaluated at zero since $$f(1) = 1$$ and $$a(1) = 1$$ Anyway, please let me know what you think! I am currently stuck due to an inability to brute force my way any further and a complete lack of cleverness. My knowledge of mathematics is quite elementary, so please explain any higher level concepts if they are necessary to explain this problem.","A few days ago, I was wondering about a series expansion for $$f(x) = \  x^x$$ so I tried taking a couple derivatives. From these I was able to guess at some formulas for the general coefficients of the first few terms of the nth derivative as well as the last term (where I put the terms of the derivative in the order  $$f^{(n)}(x) = c_1x^x + c_2x^{x-1} + c_3x^{x-2}\ + \ ... \ +\ c_nx^{x-(n-1)}$$ Please let me know if there is a general formula for the coefficients as a function of n. In my own fooling around, I found that $$\ln(x)+1$$ appears quite a bit, so I called this $a$ in my work. Since I would like to center my expansion at $1$, and this expression evaluated to $1$, at $x=0$, I was interested in writing my formulas as functions of $n$ and $a$. I found that you get something of the form (this could very well be completely incorrect):  $$f^{(n)}(x) = ax^x + a^{n-2}{\binom n2}x^{x-1} + a^{n-4}(3{\binom n4}-a{\binom n3})x^{x-2}\\ + a^{n-6}(15{\binom n6}-10{\binom n5}a+2{\binom n4}a^2)x^{x-3}\ + \ ... \ +\ (-1)^n(n-2)! \ x^{x-(n-1)}$$ An interesting property that seemed to recur for the various coefficients was  that once I found a recursion formula for the coefficient in question and plugged in my initial value, it always seemed to be the same as taking the integral of the previous coefficient with respect to $a$. That is to say, if I plugged in the nth coefficient (a polynomial of $a$) into my recursion formula for the $(n+1)$th coefficient, this gave me the same result as integrating. In general, as reflected by the few coefficient formulas above,  the formula for the coefficient of $$x^{x-c}$$ where c is an integer such that $$0\le c<n$$ ""Stabilized"" to a polynomial of a with c terms after the first few applications of the recursion formula. This stabilization occurred when $$n=2c$$ Also, as a direct result of the tentative and incomplete formula I have written above, the first nonzero-value for the coefficient of $$x^{x-c}$$ is $$(-1)^{c+1}(c-1)!$$ and appears in $$f^{(c+1)}(x)$$ If these various properties are indeed true, can anyone explain why this would be the case? I am aware that this function can be expanded using the expansion for $$e^x$$ with $$x\ln(x)$$ in the exponent. This does not strike me as particularly enlightening (let me know if I'm missing something). I am most interested in the coefficients on the binomial coefficients, since these are the only things that stand between me and a general formula for the coefficients (since the form is quite standard). These are: $$1$$ then $$3,-1$$ then $$15, -10,2$$The sum of these coefficients should also give the nth derivative evaluated at zero since $$f(1) = 1$$ and $$a(1) = 1$$ Anyway, please let me know what you think! I am currently stuck due to an inability to brute force my way any further and a complete lack of cleverness. My knowledge of mathematics is quite elementary, so please explain any higher level concepts if they are necessary to explain this problem.",,"['derivatives', 'exponential-function']"
64,"The difference between $\Delta x$, $\delta x$ and $dx$","The difference between ,  and",\Delta x \delta x dx,"$\Delta x$, $\delta x$ and $dx$ are used when talking about slopes and derivatives. But I don't know what the exact difference is between them.","$\Delta x$, $\delta x$ and $dx$ are used when talking about slopes and derivatives. But I don't know what the exact difference is between them.",,"['derivatives', 'notation']"
65,Proof for $\sin(x) > x - \frac{x^3}{3!}$,Proof for,\sin(x) > x - \frac{x^3}{3!},"They are asking me to prove $$\sin(x) > x - \frac{x^3}{3!},\; \text{for} \, x \, \in \, \mathbb{R}_{+}^{*}.$$ I didn't understand how to approach this kind of problem so here is how I tried: $\sin(x) + x -\frac{x^3}{6} > 0 \\$ then I computed the derivative of that function to determine the critical points. So: $\left(\sin(x) + x -\frac{x^3}{6}\right)' = \cos(x) -1 + \frac{x^2}{2} \\ $ The critical points: $\cos(x) -1 + \frac{x^2}{2} =  0 \\ $ It seems that x = 0 is a critical point.  Since $\left(\cos(x) -1 + \frac{x^2}{2}\right)' =  -\sin(x) + x \\ $ and $-\sin(0) + 0 = 0 \\$ The function has no local minima and maxima. Since the derivative of the function is positive, the function is strictly increasing so the lowest value is f(0) . Since f(0) = 0 and 0 > 0 I proved that $ \sin(x) + x -\frac{x^3}{6} > 0$. I'm not sure if this solution is right. And, in general, how do you tackle this kind of problems?","They are asking me to prove $$\sin(x) > x - \frac{x^3}{3!},\; \text{for} \, x \, \in \, \mathbb{R}_{+}^{*}.$$ I didn't understand how to approach this kind of problem so here is how I tried: $\sin(x) + x -\frac{x^3}{6} > 0 \\$ then I computed the derivative of that function to determine the critical points. So: $\left(\sin(x) + x -\frac{x^3}{6}\right)' = \cos(x) -1 + \frac{x^2}{2} \\ $ The critical points: $\cos(x) -1 + \frac{x^2}{2} =  0 \\ $ It seems that x = 0 is a critical point.  Since $\left(\cos(x) -1 + \frac{x^2}{2}\right)' =  -\sin(x) + x \\ $ and $-\sin(0) + 0 = 0 \\$ The function has no local minima and maxima. Since the derivative of the function is positive, the function is strictly increasing so the lowest value is f(0) . Since f(0) = 0 and 0 > 0 I proved that $ \sin(x) + x -\frac{x^3}{6} > 0$. I'm not sure if this solution is right. And, in general, how do you tackle this kind of problems?",,"['trigonometry', 'derivatives', 'inequality']"
66,Proof derivative equals zero?,Proof derivative equals zero?,,"I know this must be wrong, but I am confused as to where the mathematical fallacy lies. Here is the 'proof': $$f '(x) = \lim_{ h\to0}\frac{f(x+h)-f(x)}{h}$$ L'Hôpital's Rule (The previous limit was $\frac{0}{0}$): $$ f '(x) = \lim_{ h\to 0}\frac{f '(x+h)-f '(x)} {1} $$ Plugging in $h$: $$ f '(x) = f '(x+0)-f '(x) $$ Simplifying: $$ f '(x) = 0 $$ I'm assuming my application of L'Hôpital's rule is fallacious, but it evaluates to an indeterminate form so isn't L'Hôpital's rule still valid?","I know this must be wrong, but I am confused as to where the mathematical fallacy lies. Here is the 'proof': $$f '(x) = \lim_{ h\to0}\frac{f(x+h)-f(x)}{h}$$ L'Hôpital's Rule (The previous limit was $\frac{0}{0}$): $$ f '(x) = \lim_{ h\to 0}\frac{f '(x+h)-f '(x)} {1} $$ Plugging in $h$: $$ f '(x) = f '(x+0)-f '(x) $$ Simplifying: $$ f '(x) = 0 $$ I'm assuming my application of L'Hôpital's rule is fallacious, but it evaluates to an indeterminate form so isn't L'Hôpital's rule still valid?",,"['derivatives', 'fake-proofs']"
67,Are polynomials infinitely many times differentiable?,Are polynomials infinitely many times differentiable?,,"Are polynomials infinitely many times differentiable? If so, does it only mean that at some point we reach 0 and then we keep on getting 0? Thank you!","Are polynomials infinitely many times differentiable? If so, does it only mean that at some point we reach 0 and then we keep on getting 0? Thank you!",,"['derivatives', 'polynomials']"
68,Fractional Derivative Implications/Meaning?,Fractional Derivative Implications/Meaning?,,"I've recently been studying the concept of taking fractional derivatives and antiderivatives, and this question has come to mind: If a first derivative, in Cartesian coordinates, is representative of the function's slope, and the second derivative is representative of its concavity, is there any qualitative relationship between a 1/2 derivative and its original function? Or a 3/2 derivative with its respective function?","I've recently been studying the concept of taking fractional derivatives and antiderivatives, and this question has come to mind: If a first derivative, in Cartesian coordinates, is representative of the function's slope, and the second derivative is representative of its concavity, is there any qualitative relationship between a 1/2 derivative and its original function? Or a 3/2 derivative with its respective function?",,"['derivatives', 'fractional-calculus']"
69,Vector derivation of $x^Tx$,Vector derivation of,x^Tx,"Let $x \in \mathbb{R}^n$ What is $$\frac{\partial}{\partial x} [ x^Tx ]$$ My guess is: $\frac{\partial}{\partial x} [ x^Tx ] = 0$, because $[x^Tx] \in \mathbb{R}^1$, hence a real number as is interpreted as scalar in this derivation.","Let $x \in \mathbb{R}^n$ What is $$\frac{\partial}{\partial x} [ x^Tx ]$$ My guess is: $\frac{\partial}{\partial x} [ x^Tx ] = 0$, because $[x^Tx] \in \mathbb{R}^1$, hence a real number as is interpreted as scalar in this derivation.",,"['derivatives', 'vector-analysis']"
70,Differentiable but not continuously differentiable.,Differentiable but not continuously differentiable.,,"Given $f: \mathbb{R} \rightarrow \mathbb{R}$ defined as $$f(x)=\left\{\begin{array}{cc}x^2\sin\left(\frac{1}{x}\right)&,x\neq 0\\ 0&,x=0\end{array}\right\}.$$ I am trying to prove $f$ is differentiable at $x=0$ but not continuously differentiable there.","Given $f: \mathbb{R} \rightarrow \mathbb{R}$ defined as $$f(x)=\left\{\begin{array}{cc}x^2\sin\left(\frac{1}{x}\right)&,x\neq 0\\ 0&,x=0\end{array}\right\}.$$ I am trying to prove $f$ is differentiable at $x=0$ but not continuously differentiable there.",,['derivatives']
71,Can someone give me a deeper understanding of implicit differentiation?,Can someone give me a deeper understanding of implicit differentiation?,,"I'm doing calculus and I want to be an engineer so I would like to understand the essence of the logic of implicit differentials rather than just memorizing the algorithm. Yes, I could probably memorize it and get a 100% on a test, but it means nothing unless I understand it and can acquire a practical understanding of it. I would really appreciate if someone can enlighten me. What I understand already: I understand how the derivatives of normal functions are found the long way... i.e. $ f'(x)=\frac{f(x+h)-f(x))}{(x+h)-x} $ I understand that the derivative of the above equation is found when we find the limit as h , representing the distance between the two points, approaches 0. I also understand that the chain rule, quotient rule, etc. are just algorithms that speed up the process of finding the derivative. I don't need a proof for that An example problem When we do implicit differential equations such as this one: A ladder is 8.5 m long leaning against a wall, the bottom part of the ladder is 6.0 m from the wall and is sliding away from the wall at a rate of 2.5m/s. $x^2 + y^2 = h^2$ (Pythagorean theorem) (x is x value, y is y value, h is hypot) We can find that y = 6.2m. The derivative is  $2x\frac{\mathrm{d} x}{\mathrm{d} t} + 2y\frac{\mathrm{d} y}{\mathrm{d} t} = 2h\frac{\mathrm{d} h}{\mathrm{d} t}$ What I don't understand: (although I can do them by memorizing the algorithm) Is the implicit differential... How do we relate all the terms to the change in time? I mean how do we know we can make that equation's derivative with respect to t? Would it be possible to make them all with respect to the change in x? If so please demonstrate, I think that would help a lot as my biggest lack of understanding is how to know what the bottom term should be for each derivative term. I really appreciate any help, thanks.","I'm doing calculus and I want to be an engineer so I would like to understand the essence of the logic of implicit differentials rather than just memorizing the algorithm. Yes, I could probably memorize it and get a 100% on a test, but it means nothing unless I understand it and can acquire a practical understanding of it. I would really appreciate if someone can enlighten me. What I understand already: I understand how the derivatives of normal functions are found the long way... i.e. $ f'(x)=\frac{f(x+h)-f(x))}{(x+h)-x} $ I understand that the derivative of the above equation is found when we find the limit as h , representing the distance between the two points, approaches 0. I also understand that the chain rule, quotient rule, etc. are just algorithms that speed up the process of finding the derivative. I don't need a proof for that An example problem When we do implicit differential equations such as this one: A ladder is 8.5 m long leaning against a wall, the bottom part of the ladder is 6.0 m from the wall and is sliding away from the wall at a rate of 2.5m/s. $x^2 + y^2 = h^2$ (Pythagorean theorem) (x is x value, y is y value, h is hypot) We can find that y = 6.2m. The derivative is  $2x\frac{\mathrm{d} x}{\mathrm{d} t} + 2y\frac{\mathrm{d} y}{\mathrm{d} t} = 2h\frac{\mathrm{d} h}{\mathrm{d} t}$ What I don't understand: (although I can do them by memorizing the algorithm) Is the implicit differential... How do we relate all the terms to the change in time? I mean how do we know we can make that equation's derivative with respect to t? Would it be possible to make them all with respect to the change in x? If so please demonstrate, I think that would help a lot as my biggest lack of understanding is how to know what the bottom term should be for each derivative term. I really appreciate any help, thanks.",,"['derivatives', 'implicit-differentiation']"
72,Differentiability in metric spaces,Differentiability in metric spaces,,I have a question in mind:   Why can't we define differentiability in arbitrary metric spaces? Or can we define it really? Please discuss. I only have studied the notion of differentiability in $\mathbb{R}^n$.,I have a question in mind:   Why can't we define differentiability in arbitrary metric spaces? Or can we define it really? Please discuss. I only have studied the notion of differentiability in $\mathbb{R}^n$.,,"['derivatives', 'metric-spaces']"
73,What is the intuition behind a function being 'weakly differentiable'?,What is the intuition behind a function being 'weakly differentiable'?,,"As part of an optimization paper I am reading now, they are talking about a function $g$ being ""weakly differentiable"". I looked it up on the wiki but I do not have enough context to start cracking into it. I understand what it means for a function to be differentiable, (converges to the same answer if taken in the limit from both sides, IIRC), but what does it mean for a function to be weakly differentiable? What would such a function look like? What would it not look like? Thanks!","As part of an optimization paper I am reading now, they are talking about a function $g$ being ""weakly differentiable"". I looked it up on the wiki but I do not have enough context to start cracking into it. I understand what it means for a function to be differentiable, (converges to the same answer if taken in the limit from both sides, IIRC), but what does it mean for a function to be weakly differentiable? What would such a function look like? What would it not look like? Thanks!",,"['partial-differential-equations', 'derivatives']"
74,"Smallest $c$ such that $f'<cf$ holds for all $f$ such that $f,f',f'',f'''>0$ and $f''' \le f.$",Smallest  such that  holds for all  such that  and,"c f'<cf f f,f',f'',f'''>0 f''' \le f.","Let $f: \mathbb{R} \to \mathbb{R}$ be a $C^3$ function such that $f,f',f'',f'''>0$ and $f''' \le f.$ What is the smallest $c$ such that we can guarantee $f'<cf$ ? Since $f(x)=e^x$ works, we must have $c>1.$ On the other hand, I managed to show $c = 1.5^{1/3}$ works. Proof: First, we note $\lim\limits_{x \to -\infty} f'(x) = \lim\limits_{x \to -\infty} f''(x) = 0.$ Now $f''' \le f \Rightarrow (f''^2)' = 2f''f''' \le 2f''f < 2f''f + 2f'^2 = (2ff')',$ so integrating yields $(f''^2)(x) - (f''^2)(x_0) < (2ff')(x)-2ff'(x_0)$ for any $x,x_0.$ Taking $x_0 \to -\infty$ gives us $f''^2 < 2ff'.$ We use $f''' \le f$ again, but multiply by $f'$ instead: $(f'f'')' = f''^2 + f'f''' \le f''^2 + f'f < 3ff' = (1.5f^2)'.$ Integrating, we get $(f'f'')(x) - (f'f'')(x_0) < (1.5f^2)(x) - (1.5f^2)(x_0) < (1.5f^2)(x).$ Take $x_0 \to -\infty$ again to get $f'f'' < 1.5f^2 \Rightarrow (\frac{1}{3}f'^3)' = f'^2f'' < 1.5f^2f' = (0.5f^3)'.$ Integrate and take $x_0 \to -\infty$ for the last time to get $\frac{1}{3}f'^3 < 0.5f^3 \Rightarrow f' < 1.5^{1/3} f.$","Let be a function such that and What is the smallest such that we can guarantee ? Since works, we must have On the other hand, I managed to show works. Proof: First, we note Now so integrating yields for any Taking gives us We use again, but multiply by instead: Integrating, we get Take again to get Integrate and take for the last time to get","f: \mathbb{R} \to \mathbb{R} C^3 f,f',f'',f'''>0 f''' \le f. c f'<cf f(x)=e^x c>1. c = 1.5^{1/3} \lim\limits_{x \to -\infty} f'(x) = \lim\limits_{x \to -\infty} f''(x) = 0. f''' \le f \Rightarrow (f''^2)' = 2f''f''' \le 2f''f < 2f''f + 2f'^2 = (2ff')', (f''^2)(x) - (f''^2)(x_0) < (2ff')(x)-2ff'(x_0) x,x_0. x_0 \to -\infty f''^2 < 2ff'. f''' \le f f' (f'f'')' = f''^2 + f'f''' \le f''^2 + f'f < 3ff' = (1.5f^2)'. (f'f'')(x) - (f'f'')(x_0) < (1.5f^2)(x) - (1.5f^2)(x_0) < (1.5f^2)(x). x_0 \to -\infty f'f'' < 1.5f^2 \Rightarrow (\frac{1}{3}f'^3)' = f'^2f'' < 1.5f^2f' = (0.5f^3)'. x_0 \to -\infty \frac{1}{3}f'^3 < 0.5f^3 \Rightarrow f' < 1.5^{1/3} f.","['derivatives', 'inequality', 'functional-inequalities']"
75,What is an intuition behind total differential in two variables function?,What is an intuition behind total differential in two variables function?,,"As the definition, the total differential of a differentiable function with two variables equal to: $$ dz=\frac{\partial z}{\partial x}dx+\frac{\partial z}{\partial y}dy $$ Since there are innumerable derivable directions , I confuse it now. I have two confusion in follow: Why the total differential equal to sum of just two partial differentials? For a differentiable function, the total differential equal to sum of any two different direction's partial differentials?","As the definition, the total differential of a differentiable function with two variables equal to: $$ dz=\frac{\partial z}{\partial x}dx+\frac{\partial z}{\partial y}dy $$ Since there are innumerable derivable directions , I confuse it now. I have two confusion in follow: Why the total differential equal to sum of just two partial differentials? For a differentiable function, the total differential equal to sum of any two different direction's partial differentials?",,"['differential-geometry', 'derivatives']"
76,Derivative is just speed of change?,Derivative is just speed of change?,,"In school we've been told that derivative of $x^2$ is $2x$ . Also I've read that derivative is simply a speed of value change. So if $$f(x)=x^2$$ then, using simple explanation, derivative of that function would be $$f'(x)=f(x+1)-f(x).$$ Now if we take derivative when $x=2$ we will get $$f'(2)=f(2+1)-f(2)=3^2-2^2=9-4=5.$$ But if we will take conversion rule (from school) which says that $[x^2]'$ is $2x$ then $$f'(x)=2x$$ and if we will put the same point here we will get $$f'(2)=2*2=4$$ so first result gives me $5$ and second result gives me $4$ . And this problem seems to be appearing for every number. Number calculated by simplified interpretation is always bigger by $1$ . And my only guess is that simplified explanation missing something. Or, maybe, I made a mistake somewhere. Can you, please, help me figure it out? Update: I spend 2 days trying to figure this out! Thanks to all of you, guys!!! Now i got it!))))","In school we've been told that derivative of is . Also I've read that derivative is simply a speed of value change. So if then, using simple explanation, derivative of that function would be Now if we take derivative when we will get But if we will take conversion rule (from school) which says that is then and if we will put the same point here we will get so first result gives me and second result gives me . And this problem seems to be appearing for every number. Number calculated by simplified interpretation is always bigger by . And my only guess is that simplified explanation missing something. Or, maybe, I made a mistake somewhere. Can you, please, help me figure it out? Update: I spend 2 days trying to figure this out! Thanks to all of you, guys!!! Now i got it!))))",x^2 2x f(x)=x^2 f'(x)=f(x+1)-f(x). x=2 f'(2)=f(2+1)-f(2)=3^2-2^2=9-4=5. [x^2]' 2x f'(x)=2x f'(2)=2*2=4 5 4 1,['derivatives']
77,How to prove the derivative of position is velocity and of velocity is acceleration?,How to prove the derivative of position is velocity and of velocity is acceleration?,,"How has it been proven that the derivative of position is velocity and the derivative of velocity is acceleration? From Google searching, it seems that everyone just states it as fact without any proof behind it.","How has it been proven that the derivative of position is velocity and the derivative of velocity is acceleration? From Google searching, it seems that everyone just states it as fact without any proof behind it.",,"['derivatives', 'definition', 'physics']"
78,Differentiation of vector norms,Differentiation of vector norms,,"I want to solve the following equation $$\frac{\partial}{\partial {\bf \beta}} \left[||{\bf y}-{\bf X}{\bf \beta}||^2 + ||{\bf \beta}||^2\right] = 0$$ for $\beta$. Here ${\bf y}$ and ${\bf \beta}$ are vectors and ${\bf X}$ is a matrix. I am having trouble  with the part of differentiating the equation. I can split it up into $$\frac{\partial}{\partial {\bf \beta}} ||{\bf y}-{\bf X}{\bf \beta}||^2 + \frac{\partial}{\partial {\bf \beta}}||{\bf \beta}||^2$$ and then use the rule that $$\frac{\partial}{\partial a}||a||^2 = 2a$$ The problem is with the other part. I can use the product rule, but I am still left with $\frac{\partial}{\partial {\bf \beta}}||{\bf y} - {\bf X}{\bf \beta}||^2$.","I want to solve the following equation $$\frac{\partial}{\partial {\bf \beta}} \left[||{\bf y}-{\bf X}{\bf \beta}||^2 + ||{\bf \beta}||^2\right] = 0$$ for $\beta$. Here ${\bf y}$ and ${\bf \beta}$ are vectors and ${\bf X}$ is a matrix. I am having trouble  with the part of differentiating the equation. I can split it up into $$\frac{\partial}{\partial {\bf \beta}} ||{\bf y}-{\bf X}{\bf \beta}||^2 + \frac{\partial}{\partial {\bf \beta}}||{\bf \beta}||^2$$ and then use the rule that $$\frac{\partial}{\partial a}||a||^2 = 2a$$ The problem is with the other part. I can use the product rule, but I am still left with $\frac{\partial}{\partial {\bf \beta}}||{\bf y} - {\bf X}{\bf \beta}||^2$.",,['derivatives']
79,Find $f''(x)$ if $f\circ f'(x) = 4x^2 + 3$,Find  if,f''(x) f\circ f'(x) = 4x^2 + 3,"Can you tell me the solution of this question? If: $f\circ f'(x)=4 x^2 +3$ then what is $f''(x)$ ? This was a question in math test which I just took yesterday. One function satisfying the equation above is $f(x)=x^2+3$ , for which $f'(x)=2x$ and therefore $f''(x)=2$ . We can also see that $f'(x)$ is monotonic in $[0,+\infty)$ , and in $(-\infty,0]$ . What other analytic solutions $f(x)$ exist ? Can we express all analytic solutions $f(x)$ with a few parameters ?","Can you tell me the solution of this question? If: then what is ? This was a question in math test which I just took yesterday. One function satisfying the equation above is , for which and therefore . We can also see that is monotonic in , and in . What other analytic solutions exist ? Can we express all analytic solutions with a few parameters ?","f\circ f'(x)=4 x^2 +3 f''(x) f(x)=x^2+3 f'(x)=2x f''(x)=2 f'(x) [0,+\infty) (-\infty,0] f(x) f(x)","['derivatives', 'functional-equations', 'function-and-relation-composition']"
80,Why is the second derivative of this function a straight line?,Why is the second derivative of this function a straight line?,,"This might be an unusual question but I was wondering why the 2nd derivative of this function is a straight line? I kind of have the feeling this is not that easy to answer. But it kind of struck me that it is exactly linear. Here's a picture: I mean, yes mathematically you can say that it just is as it is, but is there also an intuitive answer to it? Thank you for answering! Regards!","This might be an unusual question but I was wondering why the 2nd derivative of this function is a straight line? I kind of have the feeling this is not that easy to answer. But it kind of struck me that it is exactly linear. Here's a picture: I mean, yes mathematically you can say that it just is as it is, but is there also an intuitive answer to it? Thank you for answering! Regards!",,"['derivatives', 'polynomials', 'curves']"
81,About the derivative of a function defined on rational numbers,About the derivative of a function defined on rational numbers,,"I have found this problem: Let $f : \mathbb{Q} → \mathbb{R}$ with property: $$|f(x) − f(y)| \le (x − y)^2 \tag1$$ for all $x, y \in \mathbb{Q}$. Prove $f$ is constant. My idea is to consider the formal derivative of $f$ like this: $$f'(x) = \lim_{h \rightarrow 0} \frac {f(x + h) - f(x)}{h}, h \in \mathbb{Q} $$ Using (1) it's easy to prove the limit exists and is equals $0$ for all $x \in \mathbb{Q}$. So $f$ has derivative and $f'(x)=0$ for all $x \in \mathbb{Q}$. It follows that $f$ is constant. Unfortunately, it's not that simple because: $$f'\equiv 0 \implies \ f  \ constant \tag 2$$ is a consequence of Mean Value Theorem which is valid only on real intervals. My question: is (2) valid for $f: \mathbb{Q} \rightarrow \mathbb{R}$? Obs. I don't need a proof for the problem.","I have found this problem: Let $f : \mathbb{Q} → \mathbb{R}$ with property: $$|f(x) − f(y)| \le (x − y)^2 \tag1$$ for all $x, y \in \mathbb{Q}$. Prove $f$ is constant. My idea is to consider the formal derivative of $f$ like this: $$f'(x) = \lim_{h \rightarrow 0} \frac {f(x + h) - f(x)}{h}, h \in \mathbb{Q} $$ Using (1) it's easy to prove the limit exists and is equals $0$ for all $x \in \mathbb{Q}$. So $f$ has derivative and $f'(x)=0$ for all $x \in \mathbb{Q}$. It follows that $f$ is constant. Unfortunately, it's not that simple because: $$f'\equiv 0 \implies \ f  \ constant \tag 2$$ is a consequence of Mean Value Theorem which is valid only on real intervals. My question: is (2) valid for $f: \mathbb{Q} \rightarrow \mathbb{R}$? Obs. I don't need a proof for the problem.",,['derivatives']
82,Second (and higher) derivatives of maps between manifolds,Second (and higher) derivatives of maps between manifolds,,"I'm trying to understand derivatives of maps between manifolds, and specifically something I read in Dodson and Poston's Tensor Geometry . I'll try to provide as much background as I can for those without the book (for anyone who has the second edition the relevant section is VII.2... pages 166-168 in my copy). For starters, if $(U, \phi)$ and $(U', \phi')$ are charts on a manifold M (e.g., $\phi: U \subset M \to \mathbb{R}^m$), $u \in U \cap U'$, and $\vec{t}, \vec{t}'$ tangent vectors to $\mathbb{R}^m$, then tangent vectors to $M$ (elements of $T_uM$) are an equivalence class of triples defined by a relation $\sim$ given by $$ (U,\phi,\vec{t}) \sim (U', \phi', \vec{t}') \iff D_{\phi(u)}(\phi' \circ \phi^{-1})\vec{t} = \vec{t}'. $$ If $f : M \to N$ is differentiable at $u$, and $(V, \psi)$ is a chart on $N$ (say $\psi : V \to \mathbb{R}^n$) with $f(u) \in V$, then $$ \begin{align*} (U,\phi,\vec{t}) &\sim (U', \phi', \vec{t}') \\ &\Rightarrow D_{\phi(u)}(\psi \circ f \circ \phi^{-1})\vec{t} = D_{\phi(u)}(\psi \circ f \circ \phi'^{-1})\vec{t}' \\ &\Rightarrow (V,\psi,D_{\phi(u)}(\psi \circ f \circ \phi^{-1})\vec{t}) \sim (V,\psi,D_{\phi(u)}(\psi \circ f \circ \phi'^{-1})\vec{t}') \end{align*} $$ so that $f$ induces a well defined map $$ D_uf : T_uM \to T_{f(u)}N $$ taking the $\sim$ equivalence class of $(U,\phi,\vec{t})$ to that of $(V,\psi,D_{\phi(u)}(\psi \circ f \circ \phi^{-1})\vec{t})$. Note that here $D_{\phi(u)}$ is just the regular derivative... as I understand what's going on, we basically create a map $\psi \circ f \circ \phi^{-1} : \mathbb{R}^m \to \mathbb{R}^n$  and take its derivative. The result (in combination with the arbitrary chart $\psi$) uniquely specifies the tangent vector to $N$ for a given tangent vector to $M$ (answering the question: as we move in some direction on $M$, what direction are we moving on $N$?). The above construction can easily be extended to higher derivatives, by simply taking more derivatives of $\psi \circ f \circ \phi^{-1}$. Dodson and Poston say ""In a similar way we can define higher derivatives, with $D^k_uf \in L^k(T_uM;T_{f(u)}N)$"". $L^k(T_uM;T_{f(u)}N)$ is the space of multilinear mappings taking in $k$ vectors in $T_uM$ and returning a vector in $T_{f(u)}N$. Moving towards my questions. Consider a map $g : U \subset \mathbb{R}^2 \to V \subset S^2$. Following the above, $D^2_u(g)(\vec{t},\vec{t}') : T_u \mathbb{R}^2 \times T_u \mathbb{R}^2 \to T_{g(u)}S^2$, i.e. after feeding it two vectors we get a vector tangent to the sphere. This doesn't seem to be what you get if you take derivatives of $\iota \circ g : \mathbb{R}^2 \to \mathbb{R}^3$, i.e., considering the sphere as embedded in $\mathbb{R}^3$ ($\iota$ being the inclusion map). Fixing $\vec{t}$, after the first derivative we have a field of tangent vectors to the sphere at each point $g(x)$ for $x \in U$, and the second derivative I presume looks at how this field changes with position (yielding vectors no longer necessarily tangent to the sphere, since we're using just a regular and not covariant derivative). My questions : I think my confusion festers somewhere in that last paragraph. Are second derivatives of $g$ and $\iota \circ g$ not the same because the second derivative of the inclusion map is not the inclusion map between the relevant tangent spaces (i.e., chain rule)? Is my picture of the second derivative of $\iota \circ g$ incorrect or misleading? What does $D^2_u(f)$ actually tell us about $f$? Is there some intuitive question that can be formulated like in the first derivative case, which tells us how we move on N under $f$ as we move on M? Any and all help in untying my knotty brain would be greatly appreciated.","I'm trying to understand derivatives of maps between manifolds, and specifically something I read in Dodson and Poston's Tensor Geometry . I'll try to provide as much background as I can for those without the book (for anyone who has the second edition the relevant section is VII.2... pages 166-168 in my copy). For starters, if $(U, \phi)$ and $(U', \phi')$ are charts on a manifold M (e.g., $\phi: U \subset M \to \mathbb{R}^m$), $u \in U \cap U'$, and $\vec{t}, \vec{t}'$ tangent vectors to $\mathbb{R}^m$, then tangent vectors to $M$ (elements of $T_uM$) are an equivalence class of triples defined by a relation $\sim$ given by $$ (U,\phi,\vec{t}) \sim (U', \phi', \vec{t}') \iff D_{\phi(u)}(\phi' \circ \phi^{-1})\vec{t} = \vec{t}'. $$ If $f : M \to N$ is differentiable at $u$, and $(V, \psi)$ is a chart on $N$ (say $\psi : V \to \mathbb{R}^n$) with $f(u) \in V$, then $$ \begin{align*} (U,\phi,\vec{t}) &\sim (U', \phi', \vec{t}') \\ &\Rightarrow D_{\phi(u)}(\psi \circ f \circ \phi^{-1})\vec{t} = D_{\phi(u)}(\psi \circ f \circ \phi'^{-1})\vec{t}' \\ &\Rightarrow (V,\psi,D_{\phi(u)}(\psi \circ f \circ \phi^{-1})\vec{t}) \sim (V,\psi,D_{\phi(u)}(\psi \circ f \circ \phi'^{-1})\vec{t}') \end{align*} $$ so that $f$ induces a well defined map $$ D_uf : T_uM \to T_{f(u)}N $$ taking the $\sim$ equivalence class of $(U,\phi,\vec{t})$ to that of $(V,\psi,D_{\phi(u)}(\psi \circ f \circ \phi^{-1})\vec{t})$. Note that here $D_{\phi(u)}$ is just the regular derivative... as I understand what's going on, we basically create a map $\psi \circ f \circ \phi^{-1} : \mathbb{R}^m \to \mathbb{R}^n$  and take its derivative. The result (in combination with the arbitrary chart $\psi$) uniquely specifies the tangent vector to $N$ for a given tangent vector to $M$ (answering the question: as we move in some direction on $M$, what direction are we moving on $N$?). The above construction can easily be extended to higher derivatives, by simply taking more derivatives of $\psi \circ f \circ \phi^{-1}$. Dodson and Poston say ""In a similar way we can define higher derivatives, with $D^k_uf \in L^k(T_uM;T_{f(u)}N)$"". $L^k(T_uM;T_{f(u)}N)$ is the space of multilinear mappings taking in $k$ vectors in $T_uM$ and returning a vector in $T_{f(u)}N$. Moving towards my questions. Consider a map $g : U \subset \mathbb{R}^2 \to V \subset S^2$. Following the above, $D^2_u(g)(\vec{t},\vec{t}') : T_u \mathbb{R}^2 \times T_u \mathbb{R}^2 \to T_{g(u)}S^2$, i.e. after feeding it two vectors we get a vector tangent to the sphere. This doesn't seem to be what you get if you take derivatives of $\iota \circ g : \mathbb{R}^2 \to \mathbb{R}^3$, i.e., considering the sphere as embedded in $\mathbb{R}^3$ ($\iota$ being the inclusion map). Fixing $\vec{t}$, after the first derivative we have a field of tangent vectors to the sphere at each point $g(x)$ for $x \in U$, and the second derivative I presume looks at how this field changes with position (yielding vectors no longer necessarily tangent to the sphere, since we're using just a regular and not covariant derivative). My questions : I think my confusion festers somewhere in that last paragraph. Are second derivatives of $g$ and $\iota \circ g$ not the same because the second derivative of the inclusion map is not the inclusion map between the relevant tangent spaces (i.e., chain rule)? Is my picture of the second derivative of $\iota \circ g$ incorrect or misleading? What does $D^2_u(f)$ actually tell us about $f$? Is there some intuitive question that can be formulated like in the first derivative case, which tells us how we move on N under $f$ as we move on M? Any and all help in untying my knotty brain would be greatly appreciated.",,"['differential-geometry', 'derivatives', 'manifolds']"
83,Power series converging at $1$ with all derivatives zero,Power series converging at  with all derivatives zero,1,"It is well-known that there are non zero functions with derivatives of all orders at $1$ equal to zero, like $x \mapsto \exp\big(-\frac{1}{(x-1)^2}\big)$ . I'm trying to construct an explicit non zero series with a similar property, that is $F : x \mapsto \sum \limits_{k=0}^{\infty} a_k x^k$ converging for $|x| \le 1$ (including $x=1$ ), such that for all $k \ge 0$ , $F^{(k)}(1) = 0$ .  Here $F^{(k)}(1)$ is defined as the limit (if it exists) of $F^{k}(z)$ when $z \to 1$ for $|z|<1$ . Is this possible with this interpretation? Is it possible if we take a derivative in the radial sense by taking $z$ to be real in the previous limit? A similar question was asked some years ago, but I do not know whether the given solution converges for $|x|=1$ , since it isn't quite explicit: $$\exp\Big(\frac{-1}{(x-1)^2}\Big) \cdot e = 1 - 2 x - x^2 + \dfrac{2}{3} x^3 + \dfrac{13}{6} x^4 + \dfrac{41}{15} x^5 + \ldots$$ Some numerical tests show that the coefficients are not even converging to zero, but I have no way of checking this. Edit 2022/10/11 One additional comment on what I tried for $\exp\big(\frac{1}{x-1}\big)$ , in order to get its Taylor series at $0$ and study its convergence at $1$ . I am replicating Feng Qi's approach : with $B_{n,k}$ the Bell polynomials of the second kind, for $|x|<1$ , \begin{align*} \bigl[e^{1/(x-1)}\bigr]^{(n)} &=\sum_{k=1}^ne^{1/(x-1)}B_{n,k}\biggl(\frac{-1!}{(x-1)^2}, \frac{2!}{(x-1)^3}, \dotsc,\frac{(-1)^{n-k+1}(n-k+1)!}{(x-1)^{n-k+2}}\biggr)\\ &=\sum_{k=1}^ne^{1/(x-1)}B_{n,k}\biggl(\frac{-1!}{(1-x)^2}, \frac{-2!}{(1-x)^3}, \dotsc,\frac{-(n-k+1)!}{(1-x)^{n-k+2}}\biggr)\\ &=e^{1/(x-1)}\sum_{k=1}^n\frac{(-1)^k}{(1-x)^{n+k}}B_{n,k}(1!, 2!, \dotsc,(n-k+1)!)\\ &=e^{1/(x-1)}\sum_{k=1}^n\frac{(-1)^k}{(1-x)^{n+k}}\binom{n}{k}\binom{n-1}{k-1}(n-k)!\\ &\to \frac{1}{e}\sum_{k=1}^n (-1)^k\binom{n}{k}\binom{n-1}{k-1}(n-k)!, \quad x\to0, \end{align*} Consequently, the series expansion of $\exp\big(\frac{1}{x-1}\big)$ is $$\sum \limits_{n=0}^{\infty} \Biggl[\frac{1}{e} \sum_{k=1}^n \frac{(-1)^k}{k!}\binom{n-1}{k-1} \Biggl] x^n$$ Now the last step is to show that the sum of these coefficients converges (for $x=1$ , in order to apply Abel's theorem). This doesn't seem obvious at all, considering that in the general term for the coefficients, the first terms of the sum grow arbitrarily large as $n$ increases.","It is well-known that there are non zero functions with derivatives of all orders at equal to zero, like . I'm trying to construct an explicit non zero series with a similar property, that is converging for (including ), such that for all , .  Here is defined as the limit (if it exists) of when for . Is this possible with this interpretation? Is it possible if we take a derivative in the radial sense by taking to be real in the previous limit? A similar question was asked some years ago, but I do not know whether the given solution converges for , since it isn't quite explicit: Some numerical tests show that the coefficients are not even converging to zero, but I have no way of checking this. Edit 2022/10/11 One additional comment on what I tried for , in order to get its Taylor series at and study its convergence at . I am replicating Feng Qi's approach : with the Bell polynomials of the second kind, for , Consequently, the series expansion of is Now the last step is to show that the sum of these coefficients converges (for , in order to apply Abel's theorem). This doesn't seem obvious at all, considering that in the general term for the coefficients, the first terms of the sum grow arbitrarily large as increases.","1 x \mapsto \exp\big(-\frac{1}{(x-1)^2}\big) F : x \mapsto \sum \limits_{k=0}^{\infty} a_k x^k |x| \le 1 x=1 k \ge 0 F^{(k)}(1) = 0 F^{(k)}(1) F^{k}(z) z \to 1 |z|<1 z |x|=1 \exp\Big(\frac{-1}{(x-1)^2}\Big) \cdot e = 1 - 2 x - x^2 + \dfrac{2}{3} x^3 + \dfrac{13}{6} x^4 + \dfrac{41}{15} x^5 + \ldots \exp\big(\frac{1}{x-1}\big) 0 1 B_{n,k} |x|<1 \begin{align*}
\bigl[e^{1/(x-1)}\bigr]^{(n)}
&=\sum_{k=1}^ne^{1/(x-1)}B_{n,k}\biggl(\frac{-1!}{(x-1)^2}, \frac{2!}{(x-1)^3}, \dotsc,\frac{(-1)^{n-k+1}(n-k+1)!}{(x-1)^{n-k+2}}\biggr)\\
&=\sum_{k=1}^ne^{1/(x-1)}B_{n,k}\biggl(\frac{-1!}{(1-x)^2}, \frac{-2!}{(1-x)^3}, \dotsc,\frac{-(n-k+1)!}{(1-x)^{n-k+2}}\biggr)\\
&=e^{1/(x-1)}\sum_{k=1}^n\frac{(-1)^k}{(1-x)^{n+k}}B_{n,k}(1!, 2!, \dotsc,(n-k+1)!)\\
&=e^{1/(x-1)}\sum_{k=1}^n\frac{(-1)^k}{(1-x)^{n+k}}\binom{n}{k}\binom{n-1}{k-1}(n-k)!\\
&\to \frac{1}{e}\sum_{k=1}^n (-1)^k\binom{n}{k}\binom{n-1}{k-1}(n-k)!, \quad x\to0,
\end{align*} \exp\big(\frac{1}{x-1}\big) \sum \limits_{n=0}^{\infty} \Biggl[\frac{1}{e} \sum_{k=1}^n \frac{(-1)^k}{k!}\binom{n-1}{k-1} \Biggl] x^n x=1 n","['derivatives', 'power-series', 'taylor-expansion']"
84,Integration and differentiation of Fourier series,Integration and differentiation of Fourier series,,"I am interested in the properties of Fourier series under integration and differentiation, and I've noticed a ""strange"" phenomenon. Suppose I have a Fourier series which I Integrate, and suppose that I can integrate it term by term. $$f(x)=a_0+\sum_{n=1}^\infty a_n \cos(nx)+\sum_{n=1}^\infty b_n \sin(nx)$$ $$\int f(x)dx=C+a_0 x+\sum_{n=1}^\infty \frac{a_n}{n} \sin(nx)-\sum_{n=1}^\infty \frac{b_n}{n} \cos(nx) $$ Now because I want the integral to be represented as a Fourier series as well, I use the Fourier series for $x$: $$x=\sum_{n=1}^\infty \frac{2}{\pi}\frac{(-1)^{n+1}}{n}\sin(nx)$$ So I have: $$\int f(x)dx=C+\sum_{n=1}^\infty \left(\frac{2a_0}{\pi}\frac{(-1)^{n+1}}{n}+\frac{a_n}{n}\right)\sin(nx)+\sum_{n=1}^\infty \frac{b_n}{n}\cos(nx)$$ My problem is that upon differentiation (term by term), this series does not appear to give back the original Fourier series, as the constant term has been shifted into the cosine terms. I think my mistake is the step I take converting the $x$ to a Fourier series, because it can't then be differentiated term by term (see this question ). Am I correct? And does this mean the integral of a Fourier series cannot be another Fourier series?","I am interested in the properties of Fourier series under integration and differentiation, and I've noticed a ""strange"" phenomenon. Suppose I have a Fourier series which I Integrate, and suppose that I can integrate it term by term. $$f(x)=a_0+\sum_{n=1}^\infty a_n \cos(nx)+\sum_{n=1}^\infty b_n \sin(nx)$$ $$\int f(x)dx=C+a_0 x+\sum_{n=1}^\infty \frac{a_n}{n} \sin(nx)-\sum_{n=1}^\infty \frac{b_n}{n} \cos(nx) $$ Now because I want the integral to be represented as a Fourier series as well, I use the Fourier series for $x$: $$x=\sum_{n=1}^\infty \frac{2}{\pi}\frac{(-1)^{n+1}}{n}\sin(nx)$$ So I have: $$\int f(x)dx=C+\sum_{n=1}^\infty \left(\frac{2a_0}{\pi}\frac{(-1)^{n+1}}{n}+\frac{a_n}{n}\right)\sin(nx)+\sum_{n=1}^\infty \frac{b_n}{n}\cos(nx)$$ My problem is that upon differentiation (term by term), this series does not appear to give back the original Fourier series, as the constant term has been shifted into the cosine terms. I think my mistake is the step I take converting the $x$ to a Fourier series, because it can't then be differentiated term by term (see this question ). Am I correct? And does this mean the integral of a Fourier series cannot be another Fourier series?",,"['derivatives', 'fourier-analysis', 'indefinite-integrals']"
85,Solve $\sum nx^n$,Solve,\sum nx^n,"I am trying to find a closed form solution for $\sum_{n\ge0} nx^n\text{, where }\lvert x \rvert<1$. This solution makes sense to me: $\sum_{n\ge0} x^n=(1-x)^{-1} \\ \frac{d}{d x} \sum_{n\ge0} x^n = \frac{d}{d x} (1-x)^{-1} \\ \sum_{n\ge0} nx^{n-1} = (1-x)^{-2} \\ x \sum_{n\ge0} n x^{n-1} = x(1-x)^{-2} \\ \sum_{n\ge0} nx^n=\frac x{(1-x)^2}$ However, a book I am reading used the following method: $$\sum_{n\ge0}nx^n=\sum_{n\ge0}x\frac d{dx}x^n= x\frac d{dx}\sum\limits_{n\ge0}x^n=x\frac d{dx}\frac1{1-x}=\frac x{(1-x)^2}$$ This seems closely related to the solution I described above, but I am having difficulty understanding it.  Can someone explain the method being used here?","I am trying to find a closed form solution for $\sum_{n\ge0} nx^n\text{, where }\lvert x \rvert<1$. This solution makes sense to me: $\sum_{n\ge0} x^n=(1-x)^{-1} \\ \frac{d}{d x} \sum_{n\ge0} x^n = \frac{d}{d x} (1-x)^{-1} \\ \sum_{n\ge0} nx^{n-1} = (1-x)^{-2} \\ x \sum_{n\ge0} n x^{n-1} = x(1-x)^{-2} \\ \sum_{n\ge0} nx^n=\frac x{(1-x)^2}$ However, a book I am reading used the following method: $$\sum_{n\ge0}nx^n=\sum_{n\ge0}x\frac d{dx}x^n= x\frac d{dx}\sum\limits_{n\ge0}x^n=x\frac d{dx}\frac1{1-x}=\frac x{(1-x)^2}$$ This seems closely related to the solution I described above, but I am having difficulty understanding it.  Can someone explain the method being used here?",,"['derivatives', 'summation']"
86,Sobolev meets Wiener,Sobolev meets Wiener,,"Even though the Wiener process (Brownian motion) is continuous, it has no derivative at any point. Does it at least have weak derivatives?","Even though the Wiener process (Brownian motion) is continuous, it has no derivative at any point. Does it at least have weak derivatives?",,"['stochastic-processes', 'brownian-motion', 'sobolev-spaces', 'derivatives']"
87,Why does differentiating a polynomial reduce its degree by $1$?,Why does differentiating a polynomial reduce its degree by ?,1,"This may seem a bit silly but I am wondering: can it intuitively be shown that the derivative of a polynomial is precisely 1 degree lower than itself? I understand the basics of calculus enough to appreciate the derivations of derivatives and I also can see why the slope of many rising functions (with many exceptions of course) kinda grow more slowly than the functions themselves (as x grows), but, frankly, I don't have the ""duh"" insight that you want to have. In fact, I feel a bit unconvinced about the whole of calculus itself (though I am still at the very basics). I know that I hardly know anything but do any of you (additional question) know a nice and readable book/books which could reintroduce me to the subject on a more rigorous level?","This may seem a bit silly but I am wondering: can it intuitively be shown that the derivative of a polynomial is precisely 1 degree lower than itself? I understand the basics of calculus enough to appreciate the derivations of derivatives and I also can see why the slope of many rising functions (with many exceptions of course) kinda grow more slowly than the functions themselves (as x grows), but, frankly, I don't have the ""duh"" insight that you want to have. In fact, I feel a bit unconvinced about the whole of calculus itself (though I am still at the very basics). I know that I hardly know anything but do any of you (additional question) know a nice and readable book/books which could reintroduce me to the subject on a more rigorous level?",,"['polynomials', 'derivatives']"
88,Closed form for $n$th derivative of exponential of $f$,Closed form for th derivative of exponential of,n f,What is the closed form for: $$\frac{\partial^n}{\partial x^n}\exp(f(x))=\exp(f(x))\cdot[????]$$,What is the closed form for: $$\frac{\partial^n}{\partial x^n}\exp(f(x))=\exp(f(x))\cdot[????]$$,,"['derivatives', 'closed-form', 'partial-derivative']"
89,"Hessian of log-sum-exp $f(z) = \operatorname{log} \sum_{i=1}^n z_i$, find $\nabla^2f(z)$","Hessian of log-sum-exp , find",f(z) = \operatorname{log} \sum_{i=1}^n z_i \nabla^2f(z),"Let $f(z) = \operatorname{log} \sum_{i=1}^n z_i = \operatorname{log} 1^Tz$. This problem comes from the following famous theorem: My work: The following step is just consider one entry of the Hessian matrix: $$\nabla^2 f(z)_{ij} = \frac{\partial f(z)}{\partial z_i\partial z_j}  = \frac{\partial}{\partial z_i}(\frac{\partial f(z)}{\partial z_j}) = \frac{\partial}{\partial z_i}(\frac{1}{1^Tz})=-\frac{1}{(1^Tz)^2}$$ I have no idea how to obtain the desired form. How to obtain the other term such as $\operatorname{diag(z)}$. Moreover, the numerator here is $1$.","Let $f(z) = \operatorname{log} \sum_{i=1}^n z_i = \operatorname{log} 1^Tz$. This problem comes from the following famous theorem: My work: The following step is just consider one entry of the Hessian matrix: $$\nabla^2 f(z)_{ij} = \frac{\partial f(z)}{\partial z_i\partial z_j}  = \frac{\partial}{\partial z_i}(\frac{\partial f(z)}{\partial z_j}) = \frac{\partial}{\partial z_i}(\frac{1}{1^Tz})=-\frac{1}{(1^Tz)^2}$$ I have no idea how to obtain the desired form. How to obtain the other term such as $\operatorname{diag(z)}$. Moreover, the numerator here is $1$.",,"['derivatives', 'logarithms', 'partial-derivative', 'hessian-matrix']"
90,Intuition behind the derivative of dirac delta function,Intuition behind the derivative of dirac delta function,,"Let me first begin what I mean by saying the intuition behind the "" $\delta'(x)$ "" . For example the smooth approximations of the delta function looks like the following: (Left:the smooth approximation of $\delta(x)$ Right:the smooth approximation of $\delta'(x)$) And by using my intuition I can understand why  $$ \int_{-\infty}^{\infty}f( \bar{x} )\delta(x-\bar{x}) \mathrm{d}\bar{x}=f(x) $$  because I can say that the delta function fires whenever $x=\bar{x}$ and picks up the value of $f(x)$ at that point and when I integrate over all values of x, I get my function f(x) back. In other words it is like building the function $f(x)$ from thin sticks, which has the same hight as the value of the function. (Although I know that this explanation is nowhere near mathematical, it helps me and others to understand -whatever that means- the concept easier.) When I learned about the derivative of the delta function and its following property I was utterly shocked: $$ \int_{-\infty}^{\infty}f(\bar{x})\delta'(x-\bar{x}) \mathrm{d}\bar{x}=f'(x) $$ Because no matter how long I think about the subject I was unable to build a correct intuition about this distribution. My question is this: Can you explain me intuitively why the derivative of the delta function gives arise to a derivative? PS: I know why this is true mathematically (integrating by parts and so on).","Let me first begin what I mean by saying the intuition behind the "" $\delta'(x)$ "" . For example the smooth approximations of the delta function looks like the following: (Left:the smooth approximation of $\delta(x)$ Right:the smooth approximation of $\delta'(x)$) And by using my intuition I can understand why  $$ \int_{-\infty}^{\infty}f( \bar{x} )\delta(x-\bar{x}) \mathrm{d}\bar{x}=f(x) $$  because I can say that the delta function fires whenever $x=\bar{x}$ and picks up the value of $f(x)$ at that point and when I integrate over all values of x, I get my function f(x) back. In other words it is like building the function $f(x)$ from thin sticks, which has the same hight as the value of the function. (Although I know that this explanation is nowhere near mathematical, it helps me and others to understand -whatever that means- the concept easier.) When I learned about the derivative of the delta function and its following property I was utterly shocked: $$ \int_{-\infty}^{\infty}f(\bar{x})\delta'(x-\bar{x}) \mathrm{d}\bar{x}=f'(x) $$ Because no matter how long I think about the subject I was unable to build a correct intuition about this distribution. My question is this: Can you explain me intuitively why the derivative of the delta function gives arise to a derivative? PS: I know why this is true mathematically (integrating by parts and so on).",,"['derivatives', 'intuition', 'distribution-theory', 'dirac-delta']"
91,Why are critical points called critical?,Why are critical points called critical?,,"For a function $y = f(x)$, a number $x_0$ is called $\textit{critical}$ if either $f'(x_0) = 0$ or $f'(x)$ does not exist. Sometimes the term $\textit{stationary}$ is used, but it is by far less popular. My question is Why is the word ""critical"" used in this case as terminology? What makes $x_0$ critical if $f'(x_0) = 0$? Of course the tangent line being horizontal or not being able to draw a tangent line gives local minimums and maximums. So why are maximums and minimums ""critical""? It seems that ""stationary"" is more appropriate. So I am puzzled as to why the latter is less popular.","For a function $y = f(x)$, a number $x_0$ is called $\textit{critical}$ if either $f'(x_0) = 0$ or $f'(x)$ does not exist. Sometimes the term $\textit{stationary}$ is used, but it is by far less popular. My question is Why is the word ""critical"" used in this case as terminology? What makes $x_0$ critical if $f'(x_0) = 0$? Of course the tangent line being horizontal or not being able to draw a tangent line gives local minimums and maximums. So why are maximums and minimums ""critical""? It seems that ""stationary"" is more appropriate. So I am puzzled as to why the latter is less popular.",,"['derivatives', 'terminology']"
92,Only once differentiable,Only once differentiable,,"Is there any example of a real function that is one-time-only differentiable, meaning there is $f'(x)$, but no $f''(x)$? I haven't been able to find any example... Of course it would be preferred if f had a closed-form expression and if it wasn't an integral. Answers on the proposed as duplicate are not quite my case, because in the other post the answer proposed does have a second derivative, just not at 0. I would like the second derivative to exist at no point.","Is there any example of a real function that is one-time-only differentiable, meaning there is $f'(x)$, but no $f''(x)$? I haven't been able to find any example... Of course it would be preferred if f had a closed-form expression and if it wasn't an integral. Answers on the proposed as duplicate are not quite my case, because in the other post the answer proposed does have a second derivative, just not at 0. I would like the second derivative to exist at no point.",,['derivatives']
93,"If a derivative of a continuous function has a limit, must it agree with that limit? [duplicate]","If a derivative of a continuous function has a limit, must it agree with that limit? [duplicate]",,"This question already has answers here : Prove that $f'(a)=\lim_{x\rightarrow a}f'(x)$. (6 answers) Closed 6 years ago . Suppose we have a continuous function $f : \mathbb{R} \to \mathbb{R}$. Suppose also that for a certain point $c$, $\lim_{x \to c} f'(x)$ exists. Must $f'(c)$ exist as well, and be equal to this limit? This isn't quite the same as asking if derivatives are always continuous. The well-known function $f(x) = x^2 \sin (1/x)$ is continuous and differentiable everywhere, but its derivative has no limit at $x = 0$. I'm wondering if the derivative of a continuous function can have a discontinuity where its limit does exist.","This question already has answers here : Prove that $f'(a)=\lim_{x\rightarrow a}f'(x)$. (6 answers) Closed 6 years ago . Suppose we have a continuous function $f : \mathbb{R} \to \mathbb{R}$. Suppose also that for a certain point $c$, $\lim_{x \to c} f'(x)$ exists. Must $f'(c)$ exist as well, and be equal to this limit? This isn't quite the same as asking if derivatives are always continuous. The well-known function $f(x) = x^2 \sin (1/x)$ is continuous and differentiable everywhere, but its derivative has no limit at $x = 0$. I'm wondering if the derivative of a continuous function can have a discontinuity where its limit does exist.",,['derivatives']
94,What is the derivative of max and min functions? [closed],What is the derivative of max and min functions? [closed],,"Closed. This question is off-topic . It is not currently accepting answers. This question is missing context or other details : Please improve the question by providing additional context, which ideally includes your thoughts on the problem and any attempts you have made to solve it. This information helps others identify where you have difficulties and helps them write answers appropriate to your experience level. Closed 9 years ago . Improve this question If I define a function: $f(x) = \max[g(x),h(x)]$ What is $f'(x)$?","Closed. This question is off-topic . It is not currently accepting answers. This question is missing context or other details : Please improve the question by providing additional context, which ideally includes your thoughts on the problem and any attempts you have made to solve it. This information helps others identify where you have difficulties and helps them write answers appropriate to your experience level. Closed 9 years ago . Improve this question If I define a function: $f(x) = \max[g(x),h(x)]$ What is $f'(x)$?",,['derivatives']
95,Evaluate derivative of Lagrange polynomials at construction points,Evaluate derivative of Lagrange polynomials at construction points,,"Assume, that we have points $x_i$ with $i=1,...,N+1$ . We construct the Lagrange basis polynomials as \begin{align} L_j(x) = \prod_{k\not = j} \frac{x-x_k}{x_j-x_k} \end{align} Now according to my computation and the results by Yves Daoust here , the derivative of $L_i$ can be computed as \begin{align} L'_j(x) = L_j(x)\cdot \sum_{k\not = j}\frac{1}{x_k-x_j} \end{align} I try to reproduce the numerical results of a paper, and for this results the authors use the derivative matrix $D$ with $D_{ij} =L_i'(x_j)$ . The authors use the Legendre Gauss Lobatto quadrature points, plotted below. Now I can easily construct the basis polynomials, also plotted below, together with the quadrature points. Given the concrete choice of basis points, the plots suggest, that $L'_j(x_j)=0$ except for the first and last basis function. Additionally it seems, that $L'_j(x_i)\not =0$ for $i\not = j$ . But using the derived formula from above, we obtain \begin{align} L'_j(x_i) = L_j(x_i)\cdot \sum_{k\not = j}\frac{1}{x_k-x_j}=0 \end{align} for $i\not = j$ since $L_j(x_i) =\delta_{ij}$ , which contradicts my observation. I used the following Matlab functions, to construct the matrix $D$ . function y=dl(i,x,z) n = length(x); y = 0; for m=1:n     if not(m==i)         y = y + 1/(x(m)-x(i));     end end size(y) y = y*l(i,x,z);  end    function y=l(i,x,z)  n = length(x); % computes h_i(z)  y = 1; for m=1:n     if not(m==i)         y = y.*(z-x(m))./(x(i)-x(m));     end end end Where dl and l are $L'$ and $L$ . $D$ is then constructed by D = zeros(M+1,M+1); for i=1:M+1     for j=1:M+1         D(i,j) =dl(i,X,X(j));     end end which gives the matrix D =     10.5000         0         0         0         0         0         0          0   -0.0000         0         0         0         0         0          0         0    0.0000         0         0         0         0          0         0         0   -0.0000         0         0         0          0         0         0         0    0.0000         0         0          0         0         0         0         0    0.0000         0          0         0         0         0         0         0  -10.5000 Here I agree with the zeros on the diagonal except for the first and last element. The zeros everywhere else agree with the formula, but they don't agree with my observation. If I use finite differences (which is no solution for the real implementation) in the following way: FD(i,j) =(l(i,X,X(j)+eps)-l(i,X,X(j)-eps))/(2*eps); I obtain the output FD =    -10.5000   -2.4429    0.6253   -0.3125    0.2261   -0.2266    0.5000    14.2016    0.0000   -2.2158    0.9075   -0.6164    0.6022   -1.3174    -5.6690    3.4558    0.0000   -2.0070    1.0664   -0.9613    2.0500     3.2000   -1.5986    2.2667         0   -2.2667    1.5986   -3.2000    -2.0500    0.9613   -1.0664    2.0070   -0.0000   -3.4558    5.6690     1.3174   -0.6022    0.6164   -0.9075    2.2158   -0.0000  -14.2016    -0.5000    0.2266   -0.2261    0.3125   -0.6253    2.4429   10.5000 which is zero on the diagonal (except first and last) and nonzero everywhere else. So here are my questions: Did I make an error in the implementation? Is my formula derived under the assumption that $x$ is not a basis point? Can I modify my code to obtain the results that my finite difference code produces? Is my assumption correct, that rather the results of the finite difference computation are ""correct""? EDIT It seems, like the derivation of the formula goes wrong for $x=x_i$ . If I compute the derivative without simplification, I get \begin{align} L_j'(x) = \sum_{l\not = j} \frac{1}{x_j-x_l}\prod_{m\not = (j,l)} \frac{x-x_m}{x_j-x_m} \end{align} Or in code function y = alternative_dl(j,x,z)  y = 0; n = length(x); for l=1:n     if not(l==j)         k = 1/(x(j)-x(l));         for m=1:n             if not(m==j) && not(m==l)                 k = k*(z-x(m))/(x(j)-x(m));             end         end         y = y + k;     end end  end Which agrees with the finite difference computation. So it seems to me, that simplifying the above formula includes some ""hidden division by zero"" if $x=x_i$ .","Assume, that we have points with . We construct the Lagrange basis polynomials as Now according to my computation and the results by Yves Daoust here , the derivative of can be computed as I try to reproduce the numerical results of a paper, and for this results the authors use the derivative matrix with . The authors use the Legendre Gauss Lobatto quadrature points, plotted below. Now I can easily construct the basis polynomials, also plotted below, together with the quadrature points. Given the concrete choice of basis points, the plots suggest, that except for the first and last basis function. Additionally it seems, that for . But using the derived formula from above, we obtain for since , which contradicts my observation. I used the following Matlab functions, to construct the matrix . function y=dl(i,x,z) n = length(x); y = 0; for m=1:n     if not(m==i)         y = y + 1/(x(m)-x(i));     end end size(y) y = y*l(i,x,z);  end    function y=l(i,x,z)  n = length(x); % computes h_i(z)  y = 1; for m=1:n     if not(m==i)         y = y.*(z-x(m))./(x(i)-x(m));     end end end Where dl and l are and . is then constructed by D = zeros(M+1,M+1); for i=1:M+1     for j=1:M+1         D(i,j) =dl(i,X,X(j));     end end which gives the matrix D =     10.5000         0         0         0         0         0         0          0   -0.0000         0         0         0         0         0          0         0    0.0000         0         0         0         0          0         0         0   -0.0000         0         0         0          0         0         0         0    0.0000         0         0          0         0         0         0         0    0.0000         0          0         0         0         0         0         0  -10.5000 Here I agree with the zeros on the diagonal except for the first and last element. The zeros everywhere else agree with the formula, but they don't agree with my observation. If I use finite differences (which is no solution for the real implementation) in the following way: FD(i,j) =(l(i,X,X(j)+eps)-l(i,X,X(j)-eps))/(2*eps); I obtain the output FD =    -10.5000   -2.4429    0.6253   -0.3125    0.2261   -0.2266    0.5000    14.2016    0.0000   -2.2158    0.9075   -0.6164    0.6022   -1.3174    -5.6690    3.4558    0.0000   -2.0070    1.0664   -0.9613    2.0500     3.2000   -1.5986    2.2667         0   -2.2667    1.5986   -3.2000    -2.0500    0.9613   -1.0664    2.0070   -0.0000   -3.4558    5.6690     1.3174   -0.6022    0.6164   -0.9075    2.2158   -0.0000  -14.2016    -0.5000    0.2266   -0.2261    0.3125   -0.6253    2.4429   10.5000 which is zero on the diagonal (except first and last) and nonzero everywhere else. So here are my questions: Did I make an error in the implementation? Is my formula derived under the assumption that is not a basis point? Can I modify my code to obtain the results that my finite difference code produces? Is my assumption correct, that rather the results of the finite difference computation are ""correct""? EDIT It seems, like the derivation of the formula goes wrong for . If I compute the derivative without simplification, I get Or in code function y = alternative_dl(j,x,z)  y = 0; n = length(x); for l=1:n     if not(l==j)         k = 1/(x(j)-x(l));         for m=1:n             if not(m==j) && not(m==l)                 k = k*(z-x(m))/(x(j)-x(m));             end         end         y = y + k;     end end  end Which agrees with the finite difference computation. So it seems to me, that simplifying the above formula includes some ""hidden division by zero"" if .","x_i i=1,...,N+1 \begin{align}
L_j(x) = \prod_{k\not = j} \frac{x-x_k}{x_j-x_k}
\end{align} L_i \begin{align}
L'_j(x) = L_j(x)\cdot \sum_{k\not = j}\frac{1}{x_k-x_j}
\end{align} D D_{ij} =L_i'(x_j) L'_j(x_j)=0 L'_j(x_i)\not =0 i\not = j \begin{align}
L'_j(x_i) = L_j(x_i)\cdot \sum_{k\not = j}\frac{1}{x_k-x_j}=0
\end{align} i\not = j L_j(x_i) =\delta_{ij} D L' L D x x=x_i \begin{align}
L_j'(x) = \sum_{l\not = j} \frac{1}{x_j-x_l}\prod_{m\not = (j,l)} \frac{x-x_m}{x_j-x_m}
\end{align} x=x_i","['derivatives', 'polynomials', 'numerical-methods', 'matlab', 'interpolation']"
96,Intuition of multivariable chain rule,Intuition of multivariable chain rule,,"I was learning/reviewing the chain rule for multivariable calculus and was wondering why the multivariable calculus chain rule is a function of summation of products of derivatives rather than just product of derivatives, like its single variable counter part. In particular I want to fix how I think of the chain rule and generalize my thoughts/intuition to multiple variables. Usually the way I used to remember the chain rule was by the usual ""trick"" of ""canceling out"" the middle dummy variables i.e. consider y = f(x(t)), then: $$ \frac{dy}{dt} = \frac{dy}{dx} \frac{dx}{dt} $$ and since the dx's cancel out, the chain rule works! Wohooo, super intuitive, easy to remember and even though its not mathematically rigorous, at least it sort of makes sense. However, for multiple variables the equation looks very different. Consider $z = f(x(t), y(t) )$, then its chain rule derivative is: $$ \frac{dz}{dt} = \frac{\partial f}{\partial x} \frac{dx}{dt} + \frac{\partial f}{\partial y} \frac{dy}{dt} $$ Even though there is some of the same ""canceling"" trick, the equation doesn't quite make as much intuitive sense to me or where it came from. So what is the intuition behind this equation? Why is it a summation of products of derivatives? Anyone have a good way of generalizing the intuition from one variable to multiple variables? Or maybe we have to change our intuition in a significant way and that is fine, as long as its more useful for multiple variable calculus! Quick comment, by intuition, I don't necessarily mean analogies to physics, but it can be to conceptual ideas in mathematics. So bringing in explanations say of real analysis and linear algebra that appeal the intuition/concepts of those areas are welcome! We all have different type of intuitions. :)","I was learning/reviewing the chain rule for multivariable calculus and was wondering why the multivariable calculus chain rule is a function of summation of products of derivatives rather than just product of derivatives, like its single variable counter part. In particular I want to fix how I think of the chain rule and generalize my thoughts/intuition to multiple variables. Usually the way I used to remember the chain rule was by the usual ""trick"" of ""canceling out"" the middle dummy variables i.e. consider y = f(x(t)), then: $$ \frac{dy}{dt} = \frac{dy}{dx} \frac{dx}{dt} $$ and since the dx's cancel out, the chain rule works! Wohooo, super intuitive, easy to remember and even though its not mathematically rigorous, at least it sort of makes sense. However, for multiple variables the equation looks very different. Consider $z = f(x(t), y(t) )$, then its chain rule derivative is: $$ \frac{dz}{dt} = \frac{\partial f}{\partial x} \frac{dx}{dt} + \frac{\partial f}{\partial y} \frac{dy}{dt} $$ Even though there is some of the same ""canceling"" trick, the equation doesn't quite make as much intuitive sense to me or where it came from. So what is the intuition behind this equation? Why is it a summation of products of derivatives? Anyone have a good way of generalizing the intuition from one variable to multiple variables? Or maybe we have to change our intuition in a significant way and that is fine, as long as its more useful for multiple variable calculus! Quick comment, by intuition, I don't necessarily mean analogies to physics, but it can be to conceptual ideas in mathematics. So bringing in explanations say of real analysis and linear algebra that appeal the intuition/concepts of those areas are welcome! We all have different type of intuitions. :)",,"['derivatives', 'partial-derivative']"
97,Is there a general formula for estimating the step size h in numerical differentiation formulas?,Is there a general formula for estimating the step size h in numerical differentiation formulas?,,"Using three-point central-difference formula $$ f^{\prime}(x_0)\approx \frac{f(x_0+h)-f(x_0-h)}{2h} $$ and for $f(x)=\exp(x)$ at $x_0=0$ we have $$ \begin{array}{c, l, r}    h & f^{\prime}(0) & error \\    \hline    10^{-01} & 1.0017 & 1.6675\times 10^{-03} \\    10^{-02} & 1 & 1.6667\times 10^{-05} \\    10^{-03} & 1 & 1.6667\times 10^{-07} \\    10^{-04} & 1 & 1.6669\times 10^{-09} \\    10^{-05} & 1 & 1.2102\times 10^{-11} \\    10^{-06} & 1 & -2.6755\times 10^{-11} \\    10^{-07} & 1 & -5.2636\times 10^{-10} \\    10^{-08} & 1 & -6.0775\times 10^{-09} \\    10^{-09} & 1 & 2.7229\times 10^{-08} \\    10^{-10} & 1 & 8.2740\times 10^{-08} \\    10^{-11} & 1 & 8.2740\times 10^{-08} \\    10^{-12} & 1 & 3.3389\times 10^{-05} \\    10^{-13} & 9.9976\times 10^{-01} & -2.4417\times 10^{-04} \\    10^{-14} & 9.9920\times 10^{-01} & -7.9928\times 10^{-04} \\    10^{-15} & 1.0547 & 5.4712\times 10^{-02} \\    10^{-16} & 5.5511\times 10^{-01} & -4.4489\times 10^{-01} \\ \end{array} $$ From $10^{-1}$ down to $10^{-5}$ the results are evident (because the rate of convergence of the three-point central-difference formula is $O(h^2)$). As you see because of the round-off error, the error deteriorate rapidly as $h$ decrease. My question is: Is there a general formula for estimating the step size $h$ in numerical differentiation formulas to get the best result?","Using three-point central-difference formula $$ f^{\prime}(x_0)\approx \frac{f(x_0+h)-f(x_0-h)}{2h} $$ and for $f(x)=\exp(x)$ at $x_0=0$ we have $$ \begin{array}{c, l, r}    h & f^{\prime}(0) & error \\    \hline    10^{-01} & 1.0017 & 1.6675\times 10^{-03} \\    10^{-02} & 1 & 1.6667\times 10^{-05} \\    10^{-03} & 1 & 1.6667\times 10^{-07} \\    10^{-04} & 1 & 1.6669\times 10^{-09} \\    10^{-05} & 1 & 1.2102\times 10^{-11} \\    10^{-06} & 1 & -2.6755\times 10^{-11} \\    10^{-07} & 1 & -5.2636\times 10^{-10} \\    10^{-08} & 1 & -6.0775\times 10^{-09} \\    10^{-09} & 1 & 2.7229\times 10^{-08} \\    10^{-10} & 1 & 8.2740\times 10^{-08} \\    10^{-11} & 1 & 8.2740\times 10^{-08} \\    10^{-12} & 1 & 3.3389\times 10^{-05} \\    10^{-13} & 9.9976\times 10^{-01} & -2.4417\times 10^{-04} \\    10^{-14} & 9.9920\times 10^{-01} & -7.9928\times 10^{-04} \\    10^{-15} & 1.0547 & 5.4712\times 10^{-02} \\    10^{-16} & 5.5511\times 10^{-01} & -4.4489\times 10^{-01} \\ \end{array} $$ From $10^{-1}$ down to $10^{-5}$ the results are evident (because the rate of convergence of the three-point central-difference formula is $O(h^2)$). As you see because of the round-off error, the error deteriorate rapidly as $h$ decrease. My question is: Is there a general formula for estimating the step size $h$ in numerical differentiation formulas to get the best result?",,"['derivatives', 'numerical-methods']"
98,Zeroes of derivatives of high order,Zeroes of derivatives of high order,,"The problem is following. Let $f:(-1,1)\to [-1,1]$ has $n$ derivatives. Prove that there exists a number $\alpha_n$ (independent from $f$) such that condition $|f'(0)|\geq \alpha_n$ implies that equation $f^{(n)}(t)=0$ has at least $n-1$ distinct zeroes on $(-1,1).$ I can prove this for $n=2.$ In this case it is enough to take $\alpha_2>2.$ Unfortunatley, I can't proceed further and prove the general case.","The problem is following. Let $f:(-1,1)\to [-1,1]$ has $n$ derivatives. Prove that there exists a number $\alpha_n$ (independent from $f$) such that condition $|f'(0)|\geq \alpha_n$ implies that equation $f^{(n)}(t)=0$ has at least $n-1$ distinct zeroes on $(-1,1).$ I can prove this for $n=2.$ In this case it is enough to take $\alpha_2>2.$ Unfortunatley, I can't proceed further and prove the general case.",,['derivatives']
99,Minimum point of $x^2+y^2$ given that $x+y=10$,Minimum point of  given that,x^2+y^2 x+y=10,How do you I approach the following question: Find the smallest possible value of $x^2 + y^2$ given that $x + y = 10$ . I can use my common sense and deduce that the minimum value is $5^2 + 5^2 = 50$ . But how do you approach this mathematically? Thanks in advance.,How do you I approach the following question: Find the smallest possible value of given that . I can use my common sense and deduce that the minimum value is . But how do you approach this mathematically? Thanks in advance.,x^2 + y^2 x + y = 10 5^2 + 5^2 = 50,"['derivatives', 'maxima-minima']"
