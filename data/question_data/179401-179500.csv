,title_raw,title_text,title_latex,body_raw,body_text,body_latex,tags
0,Parametrization of two surfaces $\frac{x^2}{a^2}-\frac{y^2}{b^2}-\frac{z^2}{c^2}=1$ and $\frac{x^2}{p}+\frac{y^2}{q}=2z$.,Parametrization of two surfaces  and .,\frac{x^2}{a^2}-\frac{y^2}{b^2}-\frac{z^2}{c^2}=1 \frac{x^2}{p}+\frac{y^2}{q}=2z,"Can someone please help me to parametrize the following surfaces in terms of  hyperbolic(for second it might not be possible but i need some more convenient set of parametric equation than mine ) and trigonometric functions $$\frac{x^2}{a^2}-\frac{y^2}{b^2}-\frac{z^2}{c^2}=1 $$ and  $$\frac{x^2}{p}+\frac{y^2}{q}=2z$$ I have tried to do but the set of parametric equations I got were too complicated as I have to use those in some further calculation which makes the result very ugly.  For first equation the set of parametric equations is: $$x=a\sqrt{1+\frac{u^2}{c^2}}\cos v, \ \ y=b\sqrt{1+\frac{u^2}{c^2}}\sin v \ \ z=u$$ and for second: $$x=\sqrt{2pu} \cos v ,\ \ y=\sqrt{2qu} \sin v, \ \ z=u $$","Can someone please help me to parametrize the following surfaces in terms of  hyperbolic(for second it might not be possible but i need some more convenient set of parametric equation than mine ) and trigonometric functions $$\frac{x^2}{a^2}-\frac{y^2}{b^2}-\frac{z^2}{c^2}=1 $$ and  $$\frac{x^2}{p}+\frac{y^2}{q}=2z$$ I have tried to do but the set of parametric equations I got were too complicated as I have to use those in some further calculation which makes the result very ugly.  For first equation the set of parametric equations is: $$x=a\sqrt{1+\frac{u^2}{c^2}}\cos v, \ \ y=b\sqrt{1+\frac{u^2}{c^2}}\sin v \ \ z=u$$ and for second: $$x=\sqrt{2pu} \cos v ,\ \ y=\sqrt{2qu} \sin v, \ \ z=u $$",,"['multivariable-calculus', 'differential-geometry']"
1,Using Lagrange Multipliers to find the largest possible area of a rectangular box with diagonal length L.,Using Lagrange Multipliers to find the largest possible area of a rectangular box with diagonal length L.,,"I am trying to find the largest possible area of a rectangular box having diagonal length L, by method of Lagrange Multipliers. Here is my approach: Let $f(x,y) = xy$ define the area of a rectangular box with length $x$, width $y$. By Pythagorean Theorem, $L^2 = x^2 + y^2$, so that $L = \sqrt{x^2+y^2}$. Define the constraint function $G(x,y) = \sqrt{x^2 + y^2} - L$ Computing the gradients, $\nabla f(x,y) = (y,x)$ $\nabla G(x,y) = (x(x^2+y^2)^{-1/2}, y(x^2+y^2)^{-1/2})$ Then by method of Lagrange Multipliers, $$ y = \lambda x(x^2 + y^2)^{-1/2} $$ $$ x = \lambda y (x^2+y^2)^{-1/2}$$ It is clear to me that one candidate for a critical point is when $x = y = 0$ (from the gradient of f), and $f(0,0) = 0$. With some algebra you can also determine that $x^2 = y^2$ so that $x = y$. But that's as far as I can go. As I don't have any numerical values, I assume I'm finding a general expression that determines the largest possible area of the box. Any help appreciated.","I am trying to find the largest possible area of a rectangular box having diagonal length L, by method of Lagrange Multipliers. Here is my approach: Let $f(x,y) = xy$ define the area of a rectangular box with length $x$, width $y$. By Pythagorean Theorem, $L^2 = x^2 + y^2$, so that $L = \sqrt{x^2+y^2}$. Define the constraint function $G(x,y) = \sqrt{x^2 + y^2} - L$ Computing the gradients, $\nabla f(x,y) = (y,x)$ $\nabla G(x,y) = (x(x^2+y^2)^{-1/2}, y(x^2+y^2)^{-1/2})$ Then by method of Lagrange Multipliers, $$ y = \lambda x(x^2 + y^2)^{-1/2} $$ $$ x = \lambda y (x^2+y^2)^{-1/2}$$ It is clear to me that one candidate for a critical point is when $x = y = 0$ (from the gradient of f), and $f(0,0) = 0$. With some algebra you can also determine that $x^2 = y^2$ so that $x = y$. But that's as far as I can go. As I don't have any numerical values, I assume I'm finding a general expression that determines the largest possible area of the box. Any help appreciated.",,['multivariable-calculus']
2,Problem with Understanding Proof of the Multivariable Mean Value Theorem,Problem with Understanding Proof of the Multivariable Mean Value Theorem,,"I have some questions concerning a proof of the Mean Value Theorem from Bredon's book that I find rather cryptical. Theorem: Let $f : \mathbb{R}^n \to \mathbb{R}$ be $C^1$. Let $x := (x_1, \dots , x_n)$ and let $x' := (x'_{1}, \dots , x'_n)$. Then there exists a $x^* := (x^*_{1}, \dots , x^*_n)$ on the line segment between $x$ and $x'$ such that $$ f(x) - f(x') = \sum^{n}_{i = 1} \frac{\partial f}{\partial x_i} (x^*) (x - x').$$ Proof: Apply the Mean Value Theorem found in any calculus book to the function $\mathbb{R} \to \mathbb{R}$ defined by $t \mapsto f(tx + (1-t)x')$ and use the Chain Rule:   $$ \frac{d f(tx + (1-t)x')}{dt} \Bigg|_{t = t^*} = % \sum^{n}_{i = 1} \frac{\partial f}{\partial x_i} (x^*) \frac{d (tx_i + (1-t)x'_i)}{dt} \Bigg|_{t = t^*} = % \sum^{n}_{i = 1} \frac{\partial f}{\partial x_i} (x^*) (x_i - x'_i )$$   with $x^* := t^* x + (1 - t^* )x' $. I have the following problems. Why $t \mapsto f(tx + (1-t)x')$ is defined on $\mathbb{R} \to \mathbb{R}$ ? I think it should rather be defined on $[0,1] \to \mathbb{R}$ (i.e., it should be convex and not affine), since we want to capture that $x^* \in f(t^* x + (1 - t^* )x')$ for some $t^* \in [0,1]$. If $t \mapsto f(tx + (1-t)x')$ is defined on $[0,1]$ then I can almost get what happens in the equation. That is, things should work as follows: start from $t \mapsto f(tx + (1-t)x')$ and apply calculus Mean Value Theorem getting \begin{align} \frac{f(x) - f(x')}{?} & = \frac{df}{dt} (t^*) \\ & = \sum^{n}_i \frac{\partial f}{\partial x_i} (x^*). \end{align} Notice that at the denominator on the LHS I put a question mark because I don't see how it should work. We should get $(x - x')$, but I don't really see how. Or better, I see it, if the denominator works as $$ (1x + (1-1)x' - 0x + (1-0)x' ),$$ which is indeed equal to $(x - x')$, but to me this is not clear at all. Indeed it should simply be equal to $1$, because we are acting on the domain of $t \mapsto f(tx + (1-t)x')$, that should be just $[0, 1]$, not on its codomain. [The RHS should be OK, since we have that there is a $t^*$ and then we use it to define $x^*$.] How should this actually work? Any feedback would be greatly appreciated since I am self-taught and I think I never really got how these analysis proofs with differentiation actually work. Thanks a a lot for your time.","I have some questions concerning a proof of the Mean Value Theorem from Bredon's book that I find rather cryptical. Theorem: Let $f : \mathbb{R}^n \to \mathbb{R}$ be $C^1$. Let $x := (x_1, \dots , x_n)$ and let $x' := (x'_{1}, \dots , x'_n)$. Then there exists a $x^* := (x^*_{1}, \dots , x^*_n)$ on the line segment between $x$ and $x'$ such that $$ f(x) - f(x') = \sum^{n}_{i = 1} \frac{\partial f}{\partial x_i} (x^*) (x - x').$$ Proof: Apply the Mean Value Theorem found in any calculus book to the function $\mathbb{R} \to \mathbb{R}$ defined by $t \mapsto f(tx + (1-t)x')$ and use the Chain Rule:   $$ \frac{d f(tx + (1-t)x')}{dt} \Bigg|_{t = t^*} = % \sum^{n}_{i = 1} \frac{\partial f}{\partial x_i} (x^*) \frac{d (tx_i + (1-t)x'_i)}{dt} \Bigg|_{t = t^*} = % \sum^{n}_{i = 1} \frac{\partial f}{\partial x_i} (x^*) (x_i - x'_i )$$   with $x^* := t^* x + (1 - t^* )x' $. I have the following problems. Why $t \mapsto f(tx + (1-t)x')$ is defined on $\mathbb{R} \to \mathbb{R}$ ? I think it should rather be defined on $[0,1] \to \mathbb{R}$ (i.e., it should be convex and not affine), since we want to capture that $x^* \in f(t^* x + (1 - t^* )x')$ for some $t^* \in [0,1]$. If $t \mapsto f(tx + (1-t)x')$ is defined on $[0,1]$ then I can almost get what happens in the equation. That is, things should work as follows: start from $t \mapsto f(tx + (1-t)x')$ and apply calculus Mean Value Theorem getting \begin{align} \frac{f(x) - f(x')}{?} & = \frac{df}{dt} (t^*) \\ & = \sum^{n}_i \frac{\partial f}{\partial x_i} (x^*). \end{align} Notice that at the denominator on the LHS I put a question mark because I don't see how it should work. We should get $(x - x')$, but I don't really see how. Or better, I see it, if the denominator works as $$ (1x + (1-1)x' - 0x + (1-0)x' ),$$ which is indeed equal to $(x - x')$, but to me this is not clear at all. Indeed it should simply be equal to $1$, because we are acting on the domain of $t \mapsto f(tx + (1-t)x')$, that should be just $[0, 1]$, not on its codomain. [The RHS should be OK, since we have that there is a $t^*$ and then we use it to define $x^*$.] How should this actually work? Any feedback would be greatly appreciated since I am self-taught and I think I never really got how these analysis proofs with differentiation actually work. Thanks a a lot for your time.",,"['real-analysis', 'multivariable-calculus', 'proof-explanation']"
3,Why do we find the determinant when finding extrema of multivariable functions?,Why do we find the determinant when finding extrema of multivariable functions?,,"So I know that when we are looking for the relative extrema of a function, f(x,y), we must eventually get the 2nd partial derivatives of f(x,y) (i.e. $f_{xx}(x,y)$, $f_{xy}(x,y)$, $f_{yx}(x,y)$, and $f_{yy}(x,y)$). We then must get the determinant of the matrix $$ \begin{matrix} f_{xx}(x,y) & f_{xy}(x,y) \\ f_{yx}(x,y) & f_{yy}(x,y) \\ \end{matrix} $$ And then we use the determinant to help figure out what we need to know. However, why do we need to find the determinant? What is the reasoning behind this?","So I know that when we are looking for the relative extrema of a function, f(x,y), we must eventually get the 2nd partial derivatives of f(x,y) (i.e. $f_{xx}(x,y)$, $f_{xy}(x,y)$, $f_{yx}(x,y)$, and $f_{yy}(x,y)$). We then must get the determinant of the matrix $$ \begin{matrix} f_{xx}(x,y) & f_{xy}(x,y) \\ f_{yx}(x,y) & f_{yy}(x,y) \\ \end{matrix} $$ And then we use the determinant to help figure out what we need to know. However, why do we need to find the determinant? What is the reasoning behind this?",,"['multivariable-calculus', 'determinant', 'hessian-matrix']"
4,Integrating a function's gradient,Integrating a function's gradient,,"Given the gradient of a function, how do you find the function itself? For a scalar-valued function $f : \mathbb{R}^n \to \mathbb{R}$, the gradient of $f$, denoted $\nabla f : \mathbb{R}^n \to \mathbb{R}^n$, is defined as $\nabla f(x)_i := \frac{\delta f(x)}{\delta x_i}, \quad i=1,\ldots,n.$ For example, for $A \in \mathbb{R}^{n \times n}$ and $b,x \in \mathbb{R}^n$, the gradient of the function $f(x) := \frac{1}{2} (Ax-b)^T(Ax-b)$ is $\nabla f(x) = (Ax-b)^T A.$ Now suppose we are told that the gradient of a scalar-valued function $g$ is $\nabla g(x) = (Ax-b)^T D$ for some diagonal matrix $D \in \mathbb{R}^{n \times n}$. Is there a nice closed-form expression for $g(x)$? These two threads ( here , here ) seem to use guess-and-check. We tried looking at function of the form $g(x) := \frac{1}{2} (Ax-b)^T W (Ax-b)$ for various weighting matrices $W$, but no dice.","Given the gradient of a function, how do you find the function itself? For a scalar-valued function $f : \mathbb{R}^n \to \mathbb{R}$, the gradient of $f$, denoted $\nabla f : \mathbb{R}^n \to \mathbb{R}^n$, is defined as $\nabla f(x)_i := \frac{\delta f(x)}{\delta x_i}, \quad i=1,\ldots,n.$ For example, for $A \in \mathbb{R}^{n \times n}$ and $b,x \in \mathbb{R}^n$, the gradient of the function $f(x) := \frac{1}{2} (Ax-b)^T(Ax-b)$ is $\nabla f(x) = (Ax-b)^T A.$ Now suppose we are told that the gradient of a scalar-valued function $g$ is $\nabla g(x) = (Ax-b)^T D$ for some diagonal matrix $D \in \mathbb{R}^{n \times n}$. Is there a nice closed-form expression for $g(x)$? These two threads ( here , here ) seem to use guess-and-check. We tried looking at function of the form $g(x) := \frac{1}{2} (Ax-b)^T W (Ax-b)$ for various weighting matrices $W$, but no dice.",,"['linear-algebra', 'multivariable-calculus', 'vector-analysis']"
5,Definition of integrability in the extended sense in Spivak's Calculus on manifolds.,Definition of integrability in the extended sense in Spivak's Calculus on manifolds.,,"In page 65 of his book, Spivak is saying $\int_A \varphi . |f|$ exists if 1.$\Phi$ is a partition of unity subordinate to an open cover $O$ of $A \subset  \mathbf{R}^n$, $\varphi \in O$ 2.Discontinuity of $f:A \rightarrow \mathbf{R} $ is measure $0$ 3.$f$ is bounded in some open set around each point of $A$. But I can't understand why it exists. I know in his proof of existance of partition of unity, he actually proved each $\varphi \in O$ has compact support, so we can think above integral as integration on a subset of a rectangle $\prod^n_{i=1}[a_i,b_i]  $. but I think still $A$ should be Jordan measurable since otherwise the integral may not exist.","In page 65 of his book, Spivak is saying $\int_A \varphi . |f|$ exists if 1.$\Phi$ is a partition of unity subordinate to an open cover $O$ of $A \subset  \mathbf{R}^n$, $\varphi \in O$ 2.Discontinuity of $f:A \rightarrow \mathbf{R} $ is measure $0$ 3.$f$ is bounded in some open set around each point of $A$. But I can't understand why it exists. I know in his proof of existance of partition of unity, he actually proved each $\varphi \in O$ has compact support, so we can think above integral as integration on a subset of a rectangle $\prod^n_{i=1}[a_i,b_i]  $. but I think still $A$ should be Jordan measurable since otherwise the integral may not exist.",,"['real-analysis', 'integration', 'multivariable-calculus']"
6,An alternative formula for computing curvature of a curve,An alternative formula for computing curvature of a curve,,"Didn't mean to bother you, but I don't know exactly what is going on here, I'm trying to have a grasp on proving the following formula: $\displaystyle \kappa = \frac{\Vert\dot \gamma \times \ddot \gamma\Vert}{\Vert\dot \gamma\Vert^3}\quad$ where $\kappa$ stands for curvature, and $\gamma$ is a parameterised curve of time $t$. Here I upload an extract of what I'm attending atm. In line $4$, the third identity what has exactly been done by author? Are we allowed to do such things in elemantary calculus? I mean sending $ds$ of $d/ds$ into the numerator of $\displaystyle \frac{d\gamma/dt}{ds/dt}$ and pulling its $dt$ back where $ds$ was, without even touching the denominator?! And the funny thing is after applying this substituting, $d/dt$ became the operator operating only on the numerator, and not the denominator. Am I missing something?! Another proof of this would also be much obliged ;-)","Didn't mean to bother you, but I don't know exactly what is going on here, I'm trying to have a grasp on proving the following formula: $\displaystyle \kappa = \frac{\Vert\dot \gamma \times \ddot \gamma\Vert}{\Vert\dot \gamma\Vert^3}\quad$ where $\kappa$ stands for curvature, and $\gamma$ is a parameterised curve of time $t$. Here I upload an extract of what I'm attending atm. In line $4$, the third identity what has exactly been done by author? Are we allowed to do such things in elemantary calculus? I mean sending $ds$ of $d/ds$ into the numerator of $\displaystyle \frac{d\gamma/dt}{ds/dt}$ and pulling its $dt$ back where $ds$ was, without even touching the denominator?! And the funny thing is after applying this substituting, $d/dt$ became the operator operating only on the numerator, and not the denominator. Am I missing something?! Another proof of this would also be much obliged ;-)",,"['multivariable-calculus', 'differential-geometry', 'curves', 'curvature']"
7,"Critical points of $f(x,y)=\sin(x)+\sin(y)-\sin(x+y)$",Critical points of,"f(x,y)=\sin(x)+\sin(y)-\sin(x+y)","Domain: $0 \le x<\pi$ and $0 \le y<\pi$ After setting the gradient of $f(x,y)$ equal to zero, I obtained the following system: $\cos(x)=\cos(x+y),$  $\cos(y)=\cos(x+y)$ I am not sure how to solve this system but what I tried was: On the given domain, the first equation implies $x=x $ and $y=0$. The second equation implies $y=y $ and $x=0$ so the system is consistent when $(x,y)=(0,0)$? And thus $(0,0)$ is the only critical point in the given domain?","Domain: $0 \le x<\pi$ and $0 \le y<\pi$ After setting the gradient of $f(x,y)$ equal to zero, I obtained the following system: $\cos(x)=\cos(x+y),$  $\cos(y)=\cos(x+y)$ I am not sure how to solve this system but what I tried was: On the given domain, the first equation implies $x=x $ and $y=0$. The second equation implies $y=y $ and $x=0$ so the system is consistent when $(x,y)=(0,0)$? And thus $(0,0)$ is the only critical point in the given domain?",,"['calculus', 'multivariable-calculus', 'trigonometry']"
8,Triple integral x^2 over an ellipsoid,Triple integral x^2 over an ellipsoid,,"I'm having difficulties with an excercise, and having trouble finding resources on the internet that would help me tackle the problem. The exercise: Calculate $$\iiint x^2\ dV$$   over the ellipsoid $(\frac{x}{a})^2+(\frac{y}{b})^2+(\frac{z}{c})^2=1$ I'm not entirely sure how to do the variable change into spherical cordinates. Could someone guide me through the process?","I'm having difficulties with an excercise, and having trouble finding resources on the internet that would help me tackle the problem. The exercise: Calculate $$\iiint x^2\ dV$$   over the ellipsoid $(\frac{x}{a})^2+(\frac{y}{b})^2+(\frac{z}{c})^2=1$ I'm not entirely sure how to do the variable change into spherical cordinates. Could someone guide me through the process?",,['multivariable-calculus']
9,"Is $\{(x,y)\in \mathbb R^2 : y=|x|\} \subseteq \mathbb R^2 $ a smooth manifold ?",Is  a smooth manifold ?,"\{(x,y)\in \mathbb R^2 : y=|x|\} \subseteq \mathbb R^2 ","Is the set $\{(x,y)\in \mathbb R^2 : y=|x|\} \subseteq \mathbb R^2 $ a smooth manifold ? I can only see that if it is a smooth manifold then it is of dimension $1$ . Please help . Thanks in advance","Is the set $\{(x,y)\in \mathbb R^2 : y=|x|\} \subseteq \mathbb R^2 $ a smooth manifold ? I can only see that if it is a smooth manifold then it is of dimension $1$ . Please help . Thanks in advance",,['multivariable-calculus']
10,How to solve this system of equations in Lagrange Multiplier problem,How to solve this system of equations in Lagrange Multiplier problem,,"Find the maximum and minimum values of ${x^{2} + y^{2} + z^{2}}$ subject to the conditions ${\frac{x^{2}}{4} + \frac{y^{2}}{5} + \frac{z^{2}}{25} = 1}$  and ${x + y - z = 0}$. Using Lagrange multiplier method, I got following equations: $$ {2x = \frac{\lambda_{1} x}{2} + \lambda_{2}}$$ $$ {2y = \frac{2 \lambda_{1} y}{5} + \lambda_{2}}$$ $$ {2z = \frac{2 \lambda_{1} z}{25} - \lambda_{2}}$$ $${\frac{x^{2}}{4} + \frac{y^{2}}{5} + \frac{z^{2}}{25} = 1}$$ $${x + y - z = 0}$$ I'm stuck after this. I've tried to solve this system of equations to get critical point many times. Any help will be greatly appreciated. Also is there any other way to approach this problem?","Find the maximum and minimum values of ${x^{2} + y^{2} + z^{2}}$ subject to the conditions ${\frac{x^{2}}{4} + \frac{y^{2}}{5} + \frac{z^{2}}{25} = 1}$  and ${x + y - z = 0}$. Using Lagrange multiplier method, I got following equations: $$ {2x = \frac{\lambda_{1} x}{2} + \lambda_{2}}$$ $$ {2y = \frac{2 \lambda_{1} y}{5} + \lambda_{2}}$$ $$ {2z = \frac{2 \lambda_{1} z}{25} - \lambda_{2}}$$ $${\frac{x^{2}}{4} + \frac{y^{2}}{5} + \frac{z^{2}}{25} = 1}$$ $${x + y - z = 0}$$ I'm stuck after this. I've tried to solve this system of equations to get critical point many times. Any help will be greatly appreciated. Also is there any other way to approach this problem?",,"['multivariable-calculus', 'systems-of-equations', 'lagrange-multiplier', 'discriminant', 'qcqp']"
11,"Volume under sphere with radius 1, center (0, 0, 1) and above the cone $z = \sqrt{x^2+y^2}$","Volume under sphere with radius 1, center (0, 0, 1) and above the cone",z = \sqrt{x^2+y^2},"so I want to find the volume of the body D defined as the region under a sphere with radius 1 with the center in (0, 0, 1) and above the cone given by $z = \sqrt{x^2+y^2}$. The answer should be $\pi$. A hint is included that you should use spherical coordinates. I've started by making a equation for the sphere, $x^2+y^2+(z-1)^2=1$. I used the transformation $(x, y, z) = (\rho\sin\phi\cos\theta, \rho\sin\phi\sin\theta, \rho\cos\phi+1)$. Now I'm struggling on defining the region D. I got that $0\leq\theta\leq2\pi$, but I can't find the bounds for $\phi$ and $\rho$.","so I want to find the volume of the body D defined as the region under a sphere with radius 1 with the center in (0, 0, 1) and above the cone given by $z = \sqrt{x^2+y^2}$. The answer should be $\pi$. A hint is included that you should use spherical coordinates. I've started by making a equation for the sphere, $x^2+y^2+(z-1)^2=1$. I used the transformation $(x, y, z) = (\rho\sin\phi\cos\theta, \rho\sin\phi\sin\theta, \rho\cos\phi+1)$. Now I'm struggling on defining the region D. I got that $0\leq\theta\leq2\pi$, but I can't find the bounds for $\phi$ and $\rho$.",,"['integration', 'multivariable-calculus', 'spherical-coordinates']"
12,Trouble with finding shortest distance between $y=x^2$ and $y=x-1$ with Lagrangian multipliers,Trouble with finding shortest distance between  and  with Lagrangian multipliers,y=x^2 y=x-1,"I've found another thread with a similar question, but none of the answers help with the specific part I'm stuck on. Just to make things simpler, I've used the square of the distance $f(x_1,x_2,y_1,y_2)=(x_1-x_2)^2+(y_1-y_2)^2$, and I have constraints $G_1=x_1^2-y_1$, and $G_2=x_2-y_2-1$. Taking the gradients of $f$, $G_1$, and $G_2$, I have the system of equations $2(x_1-x_2)=2\lambda_1x_1$ $2(y_1-y_2)=-\lambda_1$ $-2(x_1-x_2)=\lambda_2$ $-2(y_1-y_2)=-\lambda_2$ What I've done so far is, I've first observed that the immediate implication of the 2nd and 4th equations is that $\lambda_1=-\lambda_2$. Using this, I cancelled out the 1st and 3rd equations with the substituted value for $\lambda_2$, and got that $2\lambda_1x_1=\lambda_1\implies x_1=\frac{1}{2}$, and by constraint 1 that $y_1=\frac{1}{4}$. From here though, I'm not sure how to pin down the value of $\lambda$, and therefore determine $x_2$ and $y_2$.","I've found another thread with a similar question, but none of the answers help with the specific part I'm stuck on. Just to make things simpler, I've used the square of the distance $f(x_1,x_2,y_1,y_2)=(x_1-x_2)^2+(y_1-y_2)^2$, and I have constraints $G_1=x_1^2-y_1$, and $G_2=x_2-y_2-1$. Taking the gradients of $f$, $G_1$, and $G_2$, I have the system of equations $2(x_1-x_2)=2\lambda_1x_1$ $2(y_1-y_2)=-\lambda_1$ $-2(x_1-x_2)=\lambda_2$ $-2(y_1-y_2)=-\lambda_2$ What I've done so far is, I've first observed that the immediate implication of the 2nd and 4th equations is that $\lambda_1=-\lambda_2$. Using this, I cancelled out the 1st and 3rd equations with the substituted value for $\lambda_2$, and got that $2\lambda_1x_1=\lambda_1\implies x_1=\frac{1}{2}$, and by constraint 1 that $y_1=\frac{1}{4}$. From here though, I'm not sure how to pin down the value of $\lambda$, and therefore determine $x_2$ and $y_2$.",,"['multivariable-calculus', 'optimization']"
13,"Finding the values of n where the function is continuous at (0,0)","Finding the values of n where the function is continuous at (0,0)",,"I have to find the values of $n$ where this limit exists or doesn't exist. $$\lim_{(x,y)->(0,0)}\frac{|x||y|^n}{x^2 + y^2}$$ I have tried to use different paths like $y=mx$ or $y=mx^2$ and such but I don' t know what other strategies to try?","I have to find the values of $n$ where this limit exists or doesn't exist. $$\lim_{(x,y)->(0,0)}\frac{|x||y|^n}{x^2 + y^2}$$ I have tried to use different paths like $y=mx$ or $y=mx^2$ and such but I don' t know what other strategies to try?",,"['calculus', 'limits', 'multivariable-calculus']"
14,Why $\frac{\partial x}{\partial r} \ne\frac{1}{\frac{\partial r}{\partial x}} $? [closed],Why ? [closed],\frac{\partial x}{\partial r} \ne\frac{1}{\frac{\partial r}{\partial x}} ,"Closed . This question needs details or clarity . It is not currently accepting answers. Want to improve this question? Add details and clarify the problem by editing this post . Closed 7 years ago . Improve this question In case of an implicit function $f(x,y)=0$, $ \frac{dy}{dx}= \frac{1}{\frac{dx}{dy}}$ ,  but if $f(x,y)=0 $ such that $ x=g(r,\theta) $ and $y=h(r,\theta)$ then why   $\frac{\partial x}{\partial r} \ne\frac{1}{\frac{\partial r}{\partial x}} $ ? I am a beginner and found it intriguing while converting the coordinates in my mechanical engineering course.","Closed . This question needs details or clarity . It is not currently accepting answers. Want to improve this question? Add details and clarify the problem by editing this post . Closed 7 years ago . Improve this question In case of an implicit function $f(x,y)=0$, $ \frac{dy}{dx}= \frac{1}{\frac{dx}{dy}}$ ,  but if $f(x,y)=0 $ such that $ x=g(r,\theta) $ and $y=h(r,\theta)$ then why   $\frac{\partial x}{\partial r} \ne\frac{1}{\frac{\partial r}{\partial x}} $ ? I am a beginner and found it intriguing while converting the coordinates in my mechanical engineering course.",,['multivariable-calculus']
15,"If $f: \mathbb{R}^{n} \mapsto \mathbb{R}$, can we replace $||Df||$ by $||\nabla f ||$?","If , can we replace  by ?",f: \mathbb{R}^{n} \mapsto \mathbb{R} ||Df|| ||\nabla f ||,"Let $U \in \mathbb{R}^{n}$ be open, and let $f: U \mapsto \mathbb{R}$ be a $C^{1}$ function. We  obviously have $\nabla f$ = $(Df)^{T}$ where $T$ denotes the transpose of the matrix. When are the norms of the gradient of $f$ and the derivative of $f$ equivalent? In other words, is the norm of the gradient is $||\nabla f||$  $\textbf{always}$  equal to the norm of the derivative $||Df$|| ? My question is motivated by the Mean Value Inequality, i.e. $$||f(b) - f(a)|| \leq M ||b - a||$$ where $a, b \in U$ with the segment $\{a+t(b-a): 0 \leq t \leq 1\} \subset U$ and $M = \mbox{max}_{0 \leq t \leq 1} ||Df(a+t(b-a)||$. Can we replace $Df$ with $\nabla f$ in $M$ and write $M =  \mbox{max}_{0 \leq t \leq 1} ||\nabla f(a+t(b-a)||$ ? Another question. In the $||\nabla f||$ formulation, the norm is just the standard Euclidean one. For the Mean Value Inequality, what would be the standard norm used for the computation of $||Df||$ ? Thanks.","Let $U \in \mathbb{R}^{n}$ be open, and let $f: U \mapsto \mathbb{R}$ be a $C^{1}$ function. We  obviously have $\nabla f$ = $(Df)^{T}$ where $T$ denotes the transpose of the matrix. When are the norms of the gradient of $f$ and the derivative of $f$ equivalent? In other words, is the norm of the gradient is $||\nabla f||$  $\textbf{always}$  equal to the norm of the derivative $||Df$|| ? My question is motivated by the Mean Value Inequality, i.e. $$||f(b) - f(a)|| \leq M ||b - a||$$ where $a, b \in U$ with the segment $\{a+t(b-a): 0 \leq t \leq 1\} \subset U$ and $M = \mbox{max}_{0 \leq t \leq 1} ||Df(a+t(b-a)||$. Can we replace $Df$ with $\nabla f$ in $M$ and write $M =  \mbox{max}_{0 \leq t \leq 1} ||\nabla f(a+t(b-a)||$ ? Another question. In the $||\nabla f||$ formulation, the norm is just the standard Euclidean one. For the Mean Value Inequality, what would be the standard norm used for the computation of $||Df||$ ? Thanks.",,"['functional-analysis', 'analysis', 'multivariable-calculus', 'normed-spaces']"
16,"How to compute $\frac{d}{dt} \int_{a(t)}^{b(t)} f(x,t) dx$ [duplicate]",How to compute  [duplicate],"\frac{d}{dt} \int_{a(t)}^{b(t)} f(x,t) dx","This question already has answers here : How do I differentiate this integral? (3 answers) Closed 7 years ago . I want to differentiate $ \int_{a(t)}^{b(t)} f(x,t) dx$ this integral with respect to $t$ $i.e$, \begin{align} \frac{d}{dt} \int_{a(t)}^{b(t)} f(x,t) dx \end{align} I only know for the case of constant $a,b$. For this case how can i differentiatie?","This question already has answers here : How do I differentiate this integral? (3 answers) Closed 7 years ago . I want to differentiate $ \int_{a(t)}^{b(t)} f(x,t) dx$ this integral with respect to $t$ $i.e$, \begin{align} \frac{d}{dt} \int_{a(t)}^{b(t)} f(x,t) dx \end{align} I only know for the case of constant $a,b$. For this case how can i differentiatie?",,"['calculus', 'multivariable-calculus']"
17,"How to show rigorously that $\Omega=\mathbb{R}^3\backslash\{(0,0,z)\ \mid\ z\in\mathbb{R}\}$ isn't simply connected?",How to show rigorously that  isn't simply connected?,"\Omega=\mathbb{R}^3\backslash\{(0,0,z)\ \mid\ z\in\mathbb{R}\}","As indicated in the title, I want to show with outmost possible rigor and without using too advanced techniques that $\Omega=\mathbb{R}^3\backslash\{(0,0,z)\ \mid\ z\in\mathbb{R}\}$ isn't simply connected (which intuitively is obvious). Here is my attempt: Suppose $\Omega$ is simply connected, then for $\alpha,\beta:[0,\pi]\to\mathbb{R}^3$, $\alpha:t\mapsto (\cos t,\sin t,0)$, $\beta:t\mapsto(\cos t,-\sin t,0)$ there exists $F\in C^0([0,\pi]\times[0,1],\Omega)$ (we write $F=(F^1,F^2,F^3)$) such that $F(t,0)=\alpha(t)$ and $F(t,1)=\beta(t)$ for all $t\in[0,\pi]$ as well as $F(0,s)=(1,0,0)$ and $F(\pi,s)=(-1,0,0)$ for all $s\in[0,1]$. As for $s\in[0,1]$ fix we have $F^1(0,s)=1$ and $F(\pi,s)=-1$, there exists $f(s)\in (0,\pi)$ such that $F^1(f(s),s)=0$. If we could prove that there exists such an $f:[0,1]\to(0,\pi)$ such that $f$ is continuous, then we see that $F^2(f(0),0)=1$ and $F^2(f(1),1)=-1$ and thus there would have to be an $s\in(0,1)$ such that $F^2(f(s),s)=0$ and thus $F(f(s),s)\notin\Omega$, contradiction. However, I don't see how to prove the global existence of such an $f$ (intuitively it should exist though). By the inversion theorem, we can conclude that such an inverse exists locally around $s=0$ and $s=1$, but I fail to see how to extend this argument.","As indicated in the title, I want to show with outmost possible rigor and without using too advanced techniques that $\Omega=\mathbb{R}^3\backslash\{(0,0,z)\ \mid\ z\in\mathbb{R}\}$ isn't simply connected (which intuitively is obvious). Here is my attempt: Suppose $\Omega$ is simply connected, then for $\alpha,\beta:[0,\pi]\to\mathbb{R}^3$, $\alpha:t\mapsto (\cos t,\sin t,0)$, $\beta:t\mapsto(\cos t,-\sin t,0)$ there exists $F\in C^0([0,\pi]\times[0,1],\Omega)$ (we write $F=(F^1,F^2,F^3)$) such that $F(t,0)=\alpha(t)$ and $F(t,1)=\beta(t)$ for all $t\in[0,\pi]$ as well as $F(0,s)=(1,0,0)$ and $F(\pi,s)=(-1,0,0)$ for all $s\in[0,1]$. As for $s\in[0,1]$ fix we have $F^1(0,s)=1$ and $F(\pi,s)=-1$, there exists $f(s)\in (0,\pi)$ such that $F^1(f(s),s)=0$. If we could prove that there exists such an $f:[0,1]\to(0,\pi)$ such that $f$ is continuous, then we see that $F^2(f(0),0)=1$ and $F^2(f(1),1)=-1$ and thus there would have to be an $s\in(0,1)$ such that $F^2(f(s),s)=0$ and thus $F(f(s),s)\notin\Omega$, contradiction. However, I don't see how to prove the global existence of such an $f$ (intuitively it should exist though). By the inversion theorem, we can conclude that such an inverse exists locally around $s=0$ and $s=1$, but I fail to see how to extend this argument.",,"['real-analysis', 'general-topology', 'multivariable-calculus', 'connectedness']"
18,Problem in understanding chain rule.,Problem in understanding chain rule.,,"The chain rule for total derivative Assume that $g : \mathbb R^n \longrightarrow \mathbb R^m$ is differentiable at $a \in \mathbb R^n$, with total derivative $Df(a)$ and let $b = g(a)$ and assume that $f : \mathbb R^m \longrightarrow \mathbb R^p$ is differentiable at $b \in \mathbb R^m$, with total derivative $Df(b)$. Then the composition function $h = f \circ g : \mathbb R^n \longrightarrow \mathbb R^p$ is differentiable at $a \in \mathbb R^n$, and the total derivative $Dh(a)$ is given by $Dh(a) = Df(b) \circ Dg(a)$, the composition of the linear functions $Df(b)$ and $Dg(a)$. But I can't relate this concept to the chain rule involving partial derivatives as a linear combination.Please help me. Thank you in advance.","The chain rule for total derivative Assume that $g : \mathbb R^n \longrightarrow \mathbb R^m$ is differentiable at $a \in \mathbb R^n$, with total derivative $Df(a)$ and let $b = g(a)$ and assume that $f : \mathbb R^m \longrightarrow \mathbb R^p$ is differentiable at $b \in \mathbb R^m$, with total derivative $Df(b)$. Then the composition function $h = f \circ g : \mathbb R^n \longrightarrow \mathbb R^p$ is differentiable at $a \in \mathbb R^n$, and the total derivative $Dh(a)$ is given by $Dh(a) = Df(b) \circ Dg(a)$, the composition of the linear functions $Df(b)$ and $Dg(a)$. But I can't relate this concept to the chain rule involving partial derivatives as a linear combination.Please help me. Thank you in advance.",,[]
19,"Proving that the function $f(x,y)=\frac{x^2y}{x^2+y^2}$ has partial derivatives at $0$.",Proving that the function  has partial derivatives at .,"f(x,y)=\frac{x^2y}{x^2+y^2} 0","Let: $$f: (x,y) \mapsto \frac{x^2y}{x^2+y^2}$$ and  $\:f(0,0)=0$ It's partial derivatives are: $$\frac{\partial f}{\partial x} (x,y)=\frac{2xy^3}{(x^2+y^2)^2}$$ $$\frac{\partial f}{\partial y} (x,y)=\frac{x^2(x^2+y^2)}{(x^2+y^2)^2}$$ How can I prove that is has partial derivatives at $0$ and calculate them? Would showing that $f$ is continues at $0$ and stating that the partial derivatives at $0$ are also null suffice? Or should I try showing that the partial derivatives are equal at $0$?","Let: $$f: (x,y) \mapsto \frac{x^2y}{x^2+y^2}$$ and  $\:f(0,0)=0$ It's partial derivatives are: $$\frac{\partial f}{\partial x} (x,y)=\frac{2xy^3}{(x^2+y^2)^2}$$ $$\frac{\partial f}{\partial y} (x,y)=\frac{x^2(x^2+y^2)}{(x^2+y^2)^2}$$ How can I prove that is has partial derivatives at $0$ and calculate them? Would showing that $f$ is continues at $0$ and stating that the partial derivatives at $0$ are also null suffice? Or should I try showing that the partial derivatives are equal at $0$?",,"['multivariable-calculus', 'partial-derivative']"
20,Evaluating a line integral for a vector field and a path C,Evaluating a line integral for a vector field and a path C,,"$$F(x,y)=\langle 2y^2+y^2\cos(xy^2),4xy+2xy\cos(xy^2)\rangle $$ $$r(t)=\langle e^t\sin(t),e^t\cos(t)\rangle,\quad  t\in[0,\pi].$$ I found $$r'(t)= \langle \left[ e^t\sin(t) + e^t\cos(t) \right] \, dt, \left[ e^t \cos(t) - e^t\sin(t)\right] \, dt \rangle .$$ Now I know $\int_0^\pi F\cdot dr$, so I thought I substitute $e^t\sin(t)$ for $x$ and $e^t\cos(t)$ for $y$ and multiply by their respective $r(t)$ value, but the integral gets lengthy and complex very quickly. Is there something I'm missing?","$$F(x,y)=\langle 2y^2+y^2\cos(xy^2),4xy+2xy\cos(xy^2)\rangle $$ $$r(t)=\langle e^t\sin(t),e^t\cos(t)\rangle,\quad  t\in[0,\pi].$$ I found $$r'(t)= \langle \left[ e^t\sin(t) + e^t\cos(t) \right] \, dt, \left[ e^t \cos(t) - e^t\sin(t)\right] \, dt \rangle .$$ Now I know $\int_0^\pi F\cdot dr$, so I thought I substitute $e^t\sin(t)$ for $x$ and $e^t\cos(t)$ for $y$ and multiply by their respective $r(t)$ value, but the integral gets lengthy and complex very quickly. Is there something I'm missing?",,"['calculus', 'multivariable-calculus', 'line-integrals']"
21,What is the definition of multidimensional convex function?,What is the definition of multidimensional convex function?,,"Let's say we have a function $f:\Bbb R^m\to\Bbb R^n$, what does it mean for $f$ to be convex? I stumbled upon this term in article on high dimensional probability theory and I couldn't find a reference for it. PS. I would really appreciate if you can also point me to a nice article or paper about basic theory about multidimensional convex functions.","Let's say we have a function $f:\Bbb R^m\to\Bbb R^n$, what does it mean for $f$ to be convex? I stumbled upon this term in article on high dimensional probability theory and I couldn't find a reference for it. PS. I would really appreciate if you can also point me to a nice article or paper about basic theory about multidimensional convex functions.",,"['probability', 'probability-theory', 'multivariable-calculus', 'convex-analysis', 'convex-optimization']"
22,Closed line integral of conservative field not zero?,Closed line integral of conservative field not zero?,,"show that if $\mathbf{F}(x,y)=\frac{-y\mathbf{i}+x\mathbf{j}}{x^2+y^2}$, then $\oint\mathbf{F}\dot{}d\mathbf{r}=a\pi$ for every simple closed path that encloses the origin. Find the constant $a$. I first calculated the curl of the vector field and it was $\mathbf{0}$. Which means that there exists a scalar field $f$ such that $\mathbf{F}=\nabla f$ So the integral becomes $\oint\mathbf{\nabla }f\dot{}d\mathbf{r}=f(\mathbf{r(a)}-f(\mathbf{r(a)})=0)$ and Hence $a=0$.. But apparently the mark scheme says it should be $a=2$. Any idea where I am going wrong?","show that if $\mathbf{F}(x,y)=\frac{-y\mathbf{i}+x\mathbf{j}}{x^2+y^2}$, then $\oint\mathbf{F}\dot{}d\mathbf{r}=a\pi$ for every simple closed path that encloses the origin. Find the constant $a$. I first calculated the curl of the vector field and it was $\mathbf{0}$. Which means that there exists a scalar field $f$ such that $\mathbf{F}=\nabla f$ So the integral becomes $\oint\mathbf{\nabla }f\dot{}d\mathbf{r}=f(\mathbf{r(a)}-f(\mathbf{r(a)})=0)$ and Hence $a=0$.. But apparently the mark scheme says it should be $a=2$. Any idea where I am going wrong?",,"['multivariable-calculus', 'line-integrals', 'stokes-theorem', 'greens-theorem']"
23,2 variable function only depending on the ratio $x/y$ iff $ \frac{\partial f}{\partial x} x + \frac{\partial f}{\partial y} y = 0$,2 variable function only depending on the ratio  iff,x/y  \frac{\partial f}{\partial x} x + \frac{\partial f}{\partial y} y = 0,"If a differentiable function of two real variables $f(x,y)$ only depends on the ratio $(x/y)$, i.e. $f(x,y)=g(x/y)$ it is easy to see that $$ \frac{\partial f}{\partial x} x + \frac{\partial f}{\partial y} y = 0$$ since $\partial f / \partial x = g'(x/y) (1/y)$ and $\partial f / \partial y = g'(x/y) (-x/y^2)$ and: $$ \frac{\partial f}{\partial x} x + \frac{\partial f}{\partial y} y = g'( x/y ) (x/y - x/y) = 0$$ I suspect the converse is also true, i.e. if $ \frac{\partial f}{\partial x} x + \frac{\partial f}{\partial y} y = 0$ there exists a $g$ such that $f(x,y)=g(x/y)$. Can anyone give me a proof? I don't know how to start. (or a counterexample if I am wrong)","If a differentiable function of two real variables $f(x,y)$ only depends on the ratio $(x/y)$, i.e. $f(x,y)=g(x/y)$ it is easy to see that $$ \frac{\partial f}{\partial x} x + \frac{\partial f}{\partial y} y = 0$$ since $\partial f / \partial x = g'(x/y) (1/y)$ and $\partial f / \partial y = g'(x/y) (-x/y^2)$ and: $$ \frac{\partial f}{\partial x} x + \frac{\partial f}{\partial y} y = g'( x/y ) (x/y - x/y) = 0$$ I suspect the converse is also true, i.e. if $ \frac{\partial f}{\partial x} x + \frac{\partial f}{\partial y} y = 0$ there exists a $g$ such that $f(x,y)=g(x/y)$. Can anyone give me a proof? I don't know how to start. (or a counterexample if I am wrong)",,"['real-analysis', 'analysis', 'multivariable-calculus']"
24,"Showing that $F=\frac{1}{x^2+y^2}\langle-y,x\rangle$ is not path independent.",Showing that  is not path independent.,"F=\frac{1}{x^2+y^2}\langle-y,x\rangle","I thought that path independence and being conservative were tied together so using $\displaystyle F=\left\langle{\frac{-y}{x^2+y^2}, \frac{x}{x^2+y^2}}\right\rangle$ I found $\dfrac{\partial Q}{\partial x} = \dfrac{y^2-x^2}{(x^2-y^2)^2}$ and then $\dfrac{\partial P}{\partial y} = \dfrac{y^2-x^2}{(x^2-y^2)^2}$, also. But they're the same so that means its is conservative, which doesn't show that it's not path independent. Put them into symbolab to be sure I wasn't making a mistake doing the partial derivatives and got the same.","I thought that path independence and being conservative were tied together so using $\displaystyle F=\left\langle{\frac{-y}{x^2+y^2}, \frac{x}{x^2+y^2}}\right\rangle$ I found $\dfrac{\partial Q}{\partial x} = \dfrac{y^2-x^2}{(x^2-y^2)^2}$ and then $\dfrac{\partial P}{\partial y} = \dfrac{y^2-x^2}{(x^2-y^2)^2}$, also. But they're the same so that means its is conservative, which doesn't show that it's not path independent. Put them into symbolab to be sure I wasn't making a mistake doing the partial derivatives and got the same.",,"['multivariable-calculus', 'vectors', 'partial-derivative']"
25,surface integral hard question,surface integral hard question,,"Compute $\int\int_\limits{\gamma}\vec{F}\cdot\bar{n} dS$, where $\vec{F}=[x^{2}yz+xe^{z}]\bar{i}+[x^{2}+y(1-e^{z})]\bar{j}+[2+x^{3}-xyz^{2}]\bar{k}$ $\gamma =x^{2}+y^{2}=(z-1)^{2}  ,0\leq  z\leq 1$ Am not interested only in solving this particular example but in understanding how to work these kind of problems in general so that I can do the rest of problems alone. I need detailed answer , the more detailed the better . Thanks in advance.","Compute $\int\int_\limits{\gamma}\vec{F}\cdot\bar{n} dS$, where $\vec{F}=[x^{2}yz+xe^{z}]\bar{i}+[x^{2}+y(1-e^{z})]\bar{j}+[2+x^{3}-xyz^{2}]\bar{k}$ $\gamma =x^{2}+y^{2}=(z-1)^{2}  ,0\leq  z\leq 1$ Am not interested only in solving this particular example but in understanding how to work these kind of problems in general so that I can do the rest of problems alone. I need detailed answer , the more detailed the better . Thanks in advance.",,"['multivariable-calculus', 'surface-integrals']"
26,Line integrals with triangle vertices,Line integrals with triangle vertices,,"Evaluate the work integral where $F(x,y)=\langle-y,x\rangle$ over a triangle with vertices $A(-2,-2)$, $B(2,-2)$, $C(0,1)$. I am not sure how to approach this problem.  I tried setting $AB(4,0)$, $BC(-2,3)$ and $CA(-2,-3)$ but I am not sure how to proceed. Without using Green's theorem","Evaluate the work integral where $F(x,y)=\langle-y,x\rangle$ over a triangle with vertices $A(-2,-2)$, $B(2,-2)$, $C(0,1)$. I am not sure how to approach this problem.  I tried setting $AB(4,0)$, $BC(-2,3)$ and $CA(-2,-3)$ but I am not sure how to proceed. Without using Green's theorem",,"['multivariable-calculus', 'line-integrals']"
27,Two questions about curves,Two questions about curves,,"How to know if the curve $x^6 + y^6 = x^4y$ is closed ? And how to know that the curve $y^2 = x^2\frac{1 - x}{ 1 + x}$ has a loop ? PS: When I ask this two questions, I'm not considering the usage of a computer program. Another consideration is that I used the ""Multivariable-Calculus"" tag for the questions but I think it's necessary more than that to answer them.","How to know if the curve $x^6 + y^6 = x^4y$ is closed ? And how to know that the curve $y^2 = x^2\frac{1 - x}{ 1 + x}$ has a loop ? PS: When I ask this two questions, I'm not considering the usage of a computer program. Another consideration is that I used the ""Multivariable-Calculus"" tag for the questions but I think it's necessary more than that to answer them.",,['multivariable-calculus']
28,An alternative definition for sets of measure zero,An alternative definition for sets of measure zero,,"A set of measure zero in $\mathbb{R^n}$ is such that for each $a>0$, it can covered by countably many open boxes (hyperrectangles), the sum of the volumes of these boxes being less than $a.$ How can I prove that we can replace open hyperrectangles by open hypercubes or open balls? I tried to cover a rectangle by countable cubes in desired way then cover cubes by countable balls. I can cover cubes by balls but I think first one is so hard!","A set of measure zero in $\mathbb{R^n}$ is such that for each $a>0$, it can covered by countably many open boxes (hyperrectangles), the sum of the volumes of these boxes being less than $a.$ How can I prove that we can replace open hyperrectangles by open hypercubes or open balls? I tried to cover a rectangle by countable cubes in desired way then cover cubes by countable balls. I can cover cubes by balls but I think first one is so hard!",,"['real-analysis', 'measure-theory', 'multivariable-calculus', 'manifolds', 'lebesgue-measure']"
29,How to get the gradient of the trace of a matrix?,How to get the gradient of the trace of a matrix?,,"Like the one I formulate below, $\pi \in \mathbb{R}^n$  $$\frac{\partial \text{trace}\left((X^T\text{diag}(\pi)X)^{-1}\right)}{\partial \pi} = ?$$ I tried to derive, \begin{align*} \text{trace}\left((X^T\text{diag}(\pi)X)^{-1}\right) \\ = \text{trace}\left(\left(\text{diag}(\pi)X\right)^{-1}X^{-T}\right) \\ = \text{trace}\left(X^{-1}\text{diag}(\frac{1}{\pi})X^{-T}\right) \\ = \sum_i\sum_jX^{-1}_{ij}\frac{1}{\pi_j}X^{-T}_{ji} \\ \frac{\partial \sum_i\sum_jX^{-1}_{ij}\frac{1}{\pi_j}X^{-T}_{ji}}{\partial \pi} = X^{-1}\odot X^{-T}\mathbf{1} \frac{\partial 1/\pi}{\partial \pi}\\ = -\frac{1}{\pi^2}X^{-1}\odot X^{-T} \mathbf{1}\\ = -\frac{1}{\pi^2}\text{trace}(X^{-T}X^{-T}) \mathbf{1} \end{align*} Am I right with my derivation?","Like the one I formulate below, $\pi \in \mathbb{R}^n$  $$\frac{\partial \text{trace}\left((X^T\text{diag}(\pi)X)^{-1}\right)}{\partial \pi} = ?$$ I tried to derive, \begin{align*} \text{trace}\left((X^T\text{diag}(\pi)X)^{-1}\right) \\ = \text{trace}\left(\left(\text{diag}(\pi)X\right)^{-1}X^{-T}\right) \\ = \text{trace}\left(X^{-1}\text{diag}(\frac{1}{\pi})X^{-T}\right) \\ = \sum_i\sum_jX^{-1}_{ij}\frac{1}{\pi_j}X^{-T}_{ji} \\ \frac{\partial \sum_i\sum_jX^{-1}_{ij}\frac{1}{\pi_j}X^{-T}_{ji}}{\partial \pi} = X^{-1}\odot X^{-T}\mathbf{1} \frac{\partial 1/\pi}{\partial \pi}\\ = -\frac{1}{\pi^2}X^{-1}\odot X^{-T} \mathbf{1}\\ = -\frac{1}{\pi^2}\text{trace}(X^{-T}X^{-T}) \mathbf{1} \end{align*} Am I right with my derivation?",,"['calculus', 'linear-algebra', 'matrices', 'multivariable-calculus']"
30,An integral with $1/n!$ [duplicate],An integral with  [duplicate],1/n!,This question already has answers here : Volume of $T_n=\{x_i\ge0:x_1+\cdots+x_n\le1\}$ (4 answers) Closed 7 years ago . I was reading a text and it had the following listed as immediate: $\int_{0}^{1}\int_{0}^{1-x_{1}}\cdots \int_{0}^{1-x_{1}-\cdots -x_{n-1}} dx_{n}\cdots dx_{1}=\frac{1}{n!}$ I haven't actually evaluated an integral and done calculus in a while so I'm sorry if this is apparent.  But why is this immediate?,This question already has answers here : Volume of $T_n=\{x_i\ge0:x_1+\cdots+x_n\le1\}$ (4 answers) Closed 7 years ago . I was reading a text and it had the following listed as immediate: $\int_{0}^{1}\int_{0}^{1-x_{1}}\cdots \int_{0}^{1-x_{1}-\cdots -x_{n-1}} dx_{n}\cdots dx_{1}=\frac{1}{n!}$ I haven't actually evaluated an integral and done calculus in a while so I'm sorry if this is apparent.  But why is this immediate?,,"['calculus', 'integration', 'multivariable-calculus']"
31,Proving differentiability for multivariable function without continuous partial derivatives,Proving differentiability for multivariable function without continuous partial derivatives,,"I'm given a function like $(x^2+y^2)sin(1/\sqrt{x^2+y^2})$ that's $0$ at $(x,y)=0$, and the function is obviously not of class $C^1$, is there any way to prove that it's differentiable other than the limit method showing $\exists B,lim_{h\rightarrow 0}(f(a+h)-f(a)-Bh)/||h||=0$ ? Otherwise, is there any particular (general) tricks to proving functions like this are differentiable through the limit method? Note: Proving a scalar function is differentiable at the origin but that its partial derivatives are not continuous at that point. doesn't give everything I'm looking for, because I want to show differentiability everywhere.","I'm given a function like $(x^2+y^2)sin(1/\sqrt{x^2+y^2})$ that's $0$ at $(x,y)=0$, and the function is obviously not of class $C^1$, is there any way to prove that it's differentiable other than the limit method showing $\exists B,lim_{h\rightarrow 0}(f(a+h)-f(a)-Bh)/||h||=0$ ? Otherwise, is there any particular (general) tricks to proving functions like this are differentiable through the limit method? Note: Proving a scalar function is differentiable at the origin but that its partial derivatives are not continuous at that point. doesn't give everything I'm looking for, because I want to show differentiability everywhere.",,"['multivariable-calculus', 'derivatives']"
32,Evaluate triple integral,Evaluate triple integral,,"Evaluate: $$ \int_{0}^{1} \int_{0}^{y} \int_{0}^{x} \frac{2}{(1 - z)^2 \sqrt{z^2 + 1}}\, dz \, dx \, dy $$ I'm starting to learn multivariable calculus and I encounter this problem, but I currently stuck on finding this integral. I only know how to do substitution and integration by parts for one variable and my approach is trying to manipulate $ (1 - z)^2 \sqrt{z^2 + 1} $ so that I can apply the two methods with $ dz $, but I still haven't gotten anywhere.","Evaluate: $$ \int_{0}^{1} \int_{0}^{y} \int_{0}^{x} \frac{2}{(1 - z)^2 \sqrt{z^2 + 1}}\, dz \, dx \, dy $$ I'm starting to learn multivariable calculus and I encounter this problem, but I currently stuck on finding this integral. I only know how to do substitution and integration by parts for one variable and my approach is trying to manipulate $ (1 - z)^2 \sqrt{z^2 + 1} $ so that I can apply the two methods with $ dz $, but I still haven't gotten anywhere.",,"['integration', 'multivariable-calculus']"
33,Multivariable Multiindex Leibniz Rule,Multivariable Multiindex Leibniz Rule,,"Question states: ""Let $f,g: \Bbb R^n \to \Bbb R$ be of class $C^{\infty}$ and let $\alpha \in \Bbb N_0^n$ be a multiindex. Show that $$\partial^\alpha(fg) = \sum_{\beta+\gamma=\alpha}\frac{\alpha!}{\beta!\gamma!}(\partial^\beta f)(\partial^\gamma g)"".$$ I tried solving this using induction. I first proved the case for $\lvert \alpha \rvert = 1$ (or, equivalently, when $\alpha$ has only $1$ in an arbitrary position and all other terms are $0$). However, I couldn't really deduce that if the case is true for $\lvert \alpha \rvert = k$, then $\lvert \alpha \rvert = k+1$ is true. Any help? Also, if there is other any to prove this statement that would be welcome.","Question states: ""Let $f,g: \Bbb R^n \to \Bbb R$ be of class $C^{\infty}$ and let $\alpha \in \Bbb N_0^n$ be a multiindex. Show that $$\partial^\alpha(fg) = \sum_{\beta+\gamma=\alpha}\frac{\alpha!}{\beta!\gamma!}(\partial^\beta f)(\partial^\gamma g)"".$$ I tried solving this using induction. I first proved the case for $\lvert \alpha \rvert = 1$ (or, equivalently, when $\alpha$ has only $1$ in an arbitrary position and all other terms are $0$). However, I couldn't really deduce that if the case is true for $\lvert \alpha \rvert = k$, then $\lvert \alpha \rvert = k+1$ is true. Any help? Also, if there is other any to prove this statement that would be welcome.",,['multivariable-calculus']
34,Multivariable Limit of Rational Function $\frac{x^3y^2}{x^4+y^6}$,Multivariable Limit of Rational Function,\frac{x^3y^2}{x^4+y^6},"I am struggling to show if this does or does not exist: $$\lim_{(x,y) \to (0,0)}\frac{x^3y^2}{x^4+y^6}$$ I usually have no problems with limits like these. This one is throwing me through a loop because the power of $x$ is larger than the power of $y$ in the numerator, but the opposite is true in the denominator. So alot of the usual tricks for doing this kind of limit aren't working. I tried approaching along all sorts of lines of the form $y=kx^a$ and I'm always getting that the limit is $0$. However I am struggling to prove this using the limit definition or the squeeze theorem. The triangle inequality, $2|x||y|\leq x^2 + y^2$, and other tricks aren't working for me. Thanks for the help. NOTE: We have not covered polar coordinates, so the solution shouldn't have to use that.","I am struggling to show if this does or does not exist: $$\lim_{(x,y) \to (0,0)}\frac{x^3y^2}{x^4+y^6}$$ I usually have no problems with limits like these. This one is throwing me through a loop because the power of $x$ is larger than the power of $y$ in the numerator, but the opposite is true in the denominator. So alot of the usual tricks for doing this kind of limit aren't working. I tried approaching along all sorts of lines of the form $y=kx^a$ and I'm always getting that the limit is $0$. However I am struggling to prove this using the limit definition or the squeeze theorem. The triangle inequality, $2|x||y|\leq x^2 + y^2$, and other tricks aren't working for me. Thanks for the help. NOTE: We have not covered polar coordinates, so the solution shouldn't have to use that.",,"['calculus', 'limits', 'multivariable-calculus']"
35,Is work path independent?,Is work path independent?,,"Stewart - Calculus It does not appear that work being change of kinetic energy depends on $C$ or that $F$ being conservative is assumed for the result in the red box. Hence, work is path independent? Or are there some assumptions made that are too advanced for the basic calculus reader?","Stewart - Calculus It does not appear that work being change of kinetic energy depends on $C$ or that $F$ being conservative is assumed for the result in the red box. Hence, work is path independent? Or are there some assumptions made that are too advanced for the basic calculus reader?",,"['calculus', 'linear-algebra', 'integration', 'multivariable-calculus', 'physics']"
36,Estimate the maximum possible error in the approximation - Taylor Polynomials,Estimate the maximum possible error in the approximation - Taylor Polynomials,,"I am given the following example. Estimate the maximum possible error in the approximation. $cos(x) = T_{\displaystyle4,\frac{\displaystyle\pi}{\displaystyle2}}$ for $x \in \left[\frac{\displaystyle\pi}{\displaystyle2} -0.2, \frac{\displaystyle\pi}{\displaystyle2} + 0.2\right]$. $\\$ $R_{\displaystyle4,\frac{\pi}{2}}(x) \le \frac{\displaystyle|-\sin(z)|}{\displaystyle5!} |x-\frac{\pi}{2}|^5$ $\\$ $\Rightarrow R_{\displaystyle4,\frac{\displaystyle\pi}{\displaystyle2}}(x) \le \frac{\displaystyle1}{\displaystyle 5!}(0.2)^5 $ I don't understand how the example went from $|x-\frac{\pi}{2}|^5$ to $(0.2)^5$. Please specify the logical reasoning behind this and exactly how it is achieved. Thank you.","I am given the following example. Estimate the maximum possible error in the approximation. $cos(x) = T_{\displaystyle4,\frac{\displaystyle\pi}{\displaystyle2}}$ for $x \in \left[\frac{\displaystyle\pi}{\displaystyle2} -0.2, \frac{\displaystyle\pi}{\displaystyle2} + 0.2\right]$. $\\$ $R_{\displaystyle4,\frac{\pi}{2}}(x) \le \frac{\displaystyle|-\sin(z)|}{\displaystyle5!} |x-\frac{\pi}{2}|^5$ $\\$ $\Rightarrow R_{\displaystyle4,\frac{\displaystyle\pi}{\displaystyle2}}(x) \le \frac{\displaystyle1}{\displaystyle 5!}(0.2)^5 $ I don't understand how the example went from $|x-\frac{\pi}{2}|^5$ to $(0.2)^5$. Please specify the logical reasoning behind this and exactly how it is achieved. Thank you.",,"['calculus', 'multivariable-calculus']"
37,Volume of a cylindrical wedge,Volume of a cylindrical wedge,,I would like to compute the volume of the following cylindrical wedge (the portion in yellow) using an integral. The plane that cuts the wedge goes through the very bottom of the cylinder leading to an ellipse as the cross section of the wedge. The ellipse has major axis $R$ and minor axis $r$. The ellipse's major axis makes an angle $\theta_{0}$ with the vertical axis/end of the cylinder. We have the following relations $$R\cos\theta = r$$ $$h=R\sin\theta_{0}=r\tan\theta_{0}$$ where $h$ is the height of the cylindrical wedge. How do I think through how to set up an integral for this volume?,I would like to compute the volume of the following cylindrical wedge (the portion in yellow) using an integral. The plane that cuts the wedge goes through the very bottom of the cylinder leading to an ellipse as the cross section of the wedge. The ellipse has major axis $R$ and minor axis $r$. The ellipse's major axis makes an angle $\theta_{0}$ with the vertical axis/end of the cylinder. We have the following relations $$R\cos\theta = r$$ $$h=R\sin\theta_{0}=r\tan\theta_{0}$$ where $h$ is the height of the cylindrical wedge. How do I think through how to set up an integral for this volume?,,"['calculus', 'integration', 'multivariable-calculus', 'volume']"
38,"How to find a trajectory $\sigma(t)$ to represent the ellipse $\{(x,y):4x^{2}+9y^{2}=36\}$?",How to find a trajectory  to represent the ellipse ?,"\sigma(t) \{(x,y):4x^{2}+9y^{2}=36\}","Given the ellipse $\{(x,y):4x^{2}+9y^{2}=36\}$, find a trajectory $\sigma(t)$ which represent it. So far, I have this: The standard equation for an ellipse is: $\frac{x^{2}}{a^{2}}+\frac{y^{2}}{b^{2}}=1$ So, $4x^{2}+9y^{2}=36\}$ $=\left \langle \text{Arithmetic: Divide by}\ 36 \right \rangle$ $\frac{4x^{2}}{36}+\frac{9y^{2}}{36}=\frac{36}{36}$ $=\left \langle \text{Arithmetic} \right \rangle$ $\frac{x^{2}}{9}+\frac{y^{2}}{4}=1$ I don't know how to continue developing the solution, but I think the trajectory could be represented as $(a\cos(t),b\sin(t))$ Some ideas, suggestions to solve this?","Given the ellipse $\{(x,y):4x^{2}+9y^{2}=36\}$, find a trajectory $\sigma(t)$ which represent it. So far, I have this: The standard equation for an ellipse is: $\frac{x^{2}}{a^{2}}+\frac{y^{2}}{b^{2}}=1$ So, $4x^{2}+9y^{2}=36\}$ $=\left \langle \text{Arithmetic: Divide by}\ 36 \right \rangle$ $\frac{4x^{2}}{36}+\frac{9y^{2}}{36}=\frac{36}{36}$ $=\left \langle \text{Arithmetic} \right \rangle$ $\frac{x^{2}}{9}+\frac{y^{2}}{4}=1$ I don't know how to continue developing the solution, but I think the trajectory could be represented as $(a\cos(t),b\sin(t))$ Some ideas, suggestions to solve this?",,['multivariable-calculus']
39,What is an unit area vector?,What is an unit area vector?,,"As the title suggests, What is a unit area vector? I've tried googling but unable to arrive at any satisfactory answers. Any help is appreciated.","As the title suggests, What is a unit area vector? I've tried googling but unable to arrive at any satisfactory answers. Any help is appreciated.",,"['multivariable-calculus', 'differential-geometry', 'physics', 'area', 'electromagnetism']"
40,Graph $y = x^2$ in space,Graph  in space,y = x^2,"I have graphed this equation $y = x^2$ and I got this output: Is it correct? On the other hand, this I what I got in Wolfram|Alpha: How do I can analog/compare these two graphs in such a way I can deduce one from the other?","I have graphed this equation $y = x^2$ and I got this output: Is it correct? On the other hand, this I what I got in Wolfram|Alpha: How do I can analog/compare these two graphs in such a way I can deduce one from the other?",,['multivariable-calculus']
41,"Multivariable limit $\lim_{(x, y) \to (0, 0)} \frac{x \sin(x^2 y^2)}{x^2 + y^2}$ [duplicate]",Multivariable limit  [duplicate],"\lim_{(x, y) \to (0, 0)} \frac{x \sin(x^2 y^2)}{x^2 + y^2}","This question already has answers here : Prove that $x\sqrt{1-x^2} \leq \sin x \leq x$ (3 answers) Closed 7 years ago . I'm learning multivariable calculus, specifically multivariable limits and continuity, and need help to understand the solution to the following problem: Let $$f(x,y) = \begin{cases} \frac{x \sin(x^2 y^2)}{x^2 + y^2}, & (x,y) \neq (0, 0) \\ 0, & (x,y) = (0, 0).  \end{cases}$$ Show that $f$ is continuous at $(0, 0)$ . So we need to show that $$\lim_{(x, y) \to (0, 0)} f(x, y) = f(0, 0)$$ that is $$\lim_{(x, y) \to (0, 0)} \frac{x \sin(x^2 y^2)}{x^2 + y^2} = 0.$$ Solution: (from textbook) We have that $$\left| \frac{x \sin(x^2 y^2)}{x^2 + y^2}  \right| = \frac{\left|x\right| \left|\sin(x^2 y^2)\right|}{x^2 + y^2} \leq \frac{\left|x\right|x^2 y^2}{x^2 + y^2}  \leq \frac{\left|x\right|^3 y^2}{x^2 + y^2} \leq \left|x\right|^3.$$ Question: I don't understand how the author found the upper bound for the sine function. Why does $\left|\sin(x^2y^2)\right| \leq x^2y^2$ ? When I first tried to solve this problem I used $\left|\sin(x^2y^2)\right| \leq 1$ without success. Can someone explein my error?","This question already has answers here : Prove that $x\sqrt{1-x^2} \leq \sin x \leq x$ (3 answers) Closed 7 years ago . I'm learning multivariable calculus, specifically multivariable limits and continuity, and need help to understand the solution to the following problem: Let Show that is continuous at . So we need to show that that is Solution: (from textbook) We have that Question: I don't understand how the author found the upper bound for the sine function. Why does ? When I first tried to solve this problem I used without success. Can someone explein my error?","f(x,y) = \begin{cases} \frac{x \sin(x^2 y^2)}{x^2 + y^2}, & (x,y) \neq (0, 0) \\ 0, & (x,y) = (0, 0).  \end{cases} f (0, 0) \lim_{(x, y) \to (0, 0)} f(x, y) = f(0, 0) \lim_{(x, y) \to (0, 0)} \frac{x \sin(x^2 y^2)}{x^2 + y^2} = 0. \left| \frac{x \sin(x^2 y^2)}{x^2 + y^2}  \right| = \frac{\left|x\right| \left|\sin(x^2 y^2)\right|}{x^2 + y^2} \leq \frac{\left|x\right|x^2 y^2}{x^2 + y^2}  \leq \frac{\left|x\right|^3 y^2}{x^2 + y^2} \leq \left|x\right|^3. \left|\sin(x^2y^2)\right| \leq x^2y^2 \left|\sin(x^2y^2)\right| \leq 1","['calculus', 'limits', 'multivariable-calculus', 'continuity']"
42,Find the curl of $\mathrm a \times ( \mathrm b \times \mathrm r)$,Find the curl of,\mathrm a \times ( \mathrm b \times \mathrm r),"I'm trying to find the curl of $a \times (b \times r)$ where $a$, $b$ are constant vectors and $r = (x,y,z)$. I've worked through the problem to achieve the following answer: $$(\vec a \cdot \vec r) (\nabla \times \vec b) + \nabla (\vec a \cdot \vec r) \times \vec b - (\vec a \cdot \vec b) (\nabla \times \vec r) - \nabla (\vec a \cdot \vec b) \times \vec r$$ I know that the final answer is, $a \times b$, however I'm unsure of the steps in between to get there. Help would be greatly appreciated.","I'm trying to find the curl of $a \times (b \times r)$ where $a$, $b$ are constant vectors and $r = (x,y,z)$. I've worked through the problem to achieve the following answer: $$(\vec a \cdot \vec r) (\nabla \times \vec b) + \nabla (\vec a \cdot \vec r) \times \vec b - (\vec a \cdot \vec b) (\nabla \times \vec r) - \nabla (\vec a \cdot \vec b) \times \vec r$$ I know that the final answer is, $a \times b$, however I'm unsure of the steps in between to get there. Help would be greatly appreciated.",,"['multivariable-calculus', 'derivatives', 'vectors', 'cross-product']"
43,Check if $\vec F$ is a conservative field,Check if  is a conservative field,\vec F,I'm trying to check if $R/r$ is conservative feild where $R=-x\hat i+y\hat j$ and $r=\sqrt{x^2+y^2}$ Attempt: $$R/r=\underbrace{\frac{-x}{\sqrt{x^2+y^2}}}_{M}i+\underbrace{\frac{y}{\sqrt{x^2+y^2}}}_{N}j$$ $$\frac{\partial M}{\partial y}=\frac{xy}{\sqrt{x^2+y^2}(x^2+y^2)}$$ $$\frac{\partial N}{\partial x}=\frac{\color{red}-xy}{\sqrt{x^2+y^2}(x^2+y^2)}$$ Since $$\frac{\partial M}{\partial y}\ne \frac{\partial N}{\partial x}$$ $R/r$ is not conservative field I think that I'm wrong,I'm trying to check if $R/r$ is conservative feild where $R=-x\hat i+y\hat j$ and $r=\sqrt{x^2+y^2}$ Attempt: $$R/r=\underbrace{\frac{-x}{\sqrt{x^2+y^2}}}_{M}i+\underbrace{\frac{y}{\sqrt{x^2+y^2}}}_{N}j$$ $$\frac{\partial M}{\partial y}=\frac{xy}{\sqrt{x^2+y^2}(x^2+y^2)}$$ $$\frac{\partial N}{\partial x}=\frac{\color{red}-xy}{\sqrt{x^2+y^2}(x^2+y^2)}$$ Since $$\frac{\partial M}{\partial y}\ne \frac{\partial N}{\partial x}$$ $R/r$ is not conservative field I think that I'm wrong,,"['multivariable-calculus', 'vector-fields']"
44,Partial integration for smooth functions with compact support,Partial integration for smooth functions with compact support,,"Let $d\in\mathbb N$ $\lambda$ be the Lebesgue measure on $\mathbb R^d$ $\Omega\subseteq\mathbb R^d$ be open Why can we use partial integration to obtain $$\int_\Omega\phi\Delta\psi\;{\rm d}\lambda=-\int_\Omega\nabla\phi\cdot\nabla\psi\;{\rm d}\lambda\tag 1$$ for all $\phi,\psi\in C_c^\infty(\Omega)$? The multi-dimensional partial integration is a special case of the divergence theorem. But the divergence theorem in the version that I know only can be applied on compact subsets $K$ of $\mathbb R^d$ with smooth boundary $\partial K$. Smooth boundary means that for all $p\in\partial K$, there is an open set $U\subseteq\mathbb R^d$ with $p\in U$ such that there is a continuously differentiable $F:U\to\mathbb R$ with $K\cap U=\left\{x\in U:F(x)\le 0\right\}$ and $F'(x)\ne 0$ for all $x\in U$ Let's so what we've got in the situation of $\phi,\psi\in C_c^\infty(\Omega)$. Suppose we can find a compact $K$ with smooth boundary such that $K\subseteq\Omega$ and $\operatorname{supp}\Delta\psi\subset K$. Then it's easy to conclude that $$\int_\Omega\nabla\phi\cdot\nabla\psi+\phi\Delta\psi\;{\rm d}\lambda=\int_K\nabla\phi\cdot\nabla\psi+\phi\Delta\psi\;{\rm d}\lambda=\int_{\partial K}\phi\frac{\partial\psi}{\partial\nu}\;{\rm d}o=0$$ by the divergence theorem, since $\phi$ vanishes at $\partial K$. So, can we always find such a $K$? If not, how can we prove $(1)$ instead?","Let $d\in\mathbb N$ $\lambda$ be the Lebesgue measure on $\mathbb R^d$ $\Omega\subseteq\mathbb R^d$ be open Why can we use partial integration to obtain $$\int_\Omega\phi\Delta\psi\;{\rm d}\lambda=-\int_\Omega\nabla\phi\cdot\nabla\psi\;{\rm d}\lambda\tag 1$$ for all $\phi,\psi\in C_c^\infty(\Omega)$? The multi-dimensional partial integration is a special case of the divergence theorem. But the divergence theorem in the version that I know only can be applied on compact subsets $K$ of $\mathbb R^d$ with smooth boundary $\partial K$. Smooth boundary means that for all $p\in\partial K$, there is an open set $U\subseteq\mathbb R^d$ with $p\in U$ such that there is a continuously differentiable $F:U\to\mathbb R$ with $K\cap U=\left\{x\in U:F(x)\le 0\right\}$ and $F'(x)\ne 0$ for all $x\in U$ Let's so what we've got in the situation of $\phi,\psi\in C_c^\infty(\Omega)$. Suppose we can find a compact $K$ with smooth boundary such that $K\subseteq\Omega$ and $\operatorname{supp}\Delta\psi\subset K$. Then it's easy to conclude that $$\int_\Omega\nabla\phi\cdot\nabla\psi+\phi\Delta\psi\;{\rm d}\lambda=\int_K\nabla\phi\cdot\nabla\psi+\phi\Delta\psi\;{\rm d}\lambda=\int_{\partial K}\phi\frac{\partial\psi}{\partial\nu}\;{\rm d}o=0$$ by the divergence theorem, since $\phi$ vanishes at $\partial K$. So, can we always find such a $K$? If not, how can we prove $(1)$ instead?",,"['analysis', 'multivariable-calculus', 'compactness', 'integration-by-parts']"
45,"Prove that $\lim_{\epsilon \rightarrow 0}\int_{\partial B_\epsilon} (g  n  g  n) ds = 2(0, 0)$",Prove that,"\lim_{\epsilon \rightarrow 0}\int_{\partial B_\epsilon} (g  n  g  n) ds = 2(0, 0)","Suppose $ : \mathbb{R^2}\rightarrow \mathbb{R}$ is any $C^1$ function and let $g:\mathbb{R^2}-\{(0,0)\}\rightarrow \mathbb{R}$ given by $g(x, y) := \ln\sqrt{x^2+y^2}$ Prove that    $\lim_{\epsilon \rightarrow 0}\int_{\partial B_\epsilon} (g  n  g  n) ds = 2(0, 0)$, where $B_\epsilon$ denotes the disk centered at $(0, 0)$ of radius $\epsilon$ and $n$ denotes the outer unit normal to the circle $\partial B_\epsilon.$ I tried to use Green's Theorem, but things got more complicated since I had terms like $\frac{\partial (\nabla g)}{\partial x}$. Is there a simple method to show it? Any help is appreciated.","Suppose $ : \mathbb{R^2}\rightarrow \mathbb{R}$ is any $C^1$ function and let $g:\mathbb{R^2}-\{(0,0)\}\rightarrow \mathbb{R}$ given by $g(x, y) := \ln\sqrt{x^2+y^2}$ Prove that    $\lim_{\epsilon \rightarrow 0}\int_{\partial B_\epsilon} (g  n  g  n) ds = 2(0, 0)$, where $B_\epsilon$ denotes the disk centered at $(0, 0)$ of radius $\epsilon$ and $n$ denotes the outer unit normal to the circle $\partial B_\epsilon.$ I tried to use Green's Theorem, but things got more complicated since I had terms like $\frac{\partial (\nabla g)}{\partial x}$. Is there a simple method to show it? Any help is appreciated.",,"['multivariable-calculus', 'greens-theorem']"
46,What is the definition of a path along a multivariable function?,What is the definition of a path along a multivariable function?,,"I'm taking a class equivalent of Calculus III, and we saw how to prove continuity of a multivariable function. Recently we looked at the following example: \begin{align}  f(x,y) = \begin{cases} \frac{x^{2\alpha}}{x^2 + y^2},  & \text{if ($x$,$y$)} \neq \text{(0,0)} \\[1ex] 0 & \text{if ($x$,$y$) = (0,0)} \end{cases} \end{align} Long story short, we ended up switching to polar coordinates and simplifying down to: $$\lim_{r \to 0} r^{2\alpha - 2} \cdot \cos^{2\alpha}{\theta}$$ Besides the fact that we can now analyze different cases based on the value of $\alpha$, we came to the conclusion that since the point is approached at by infinitely different "" paths "" (depending on the value of $\theta$), the limit doesn't even exist in the first place. So, I get that this limit hinges on $\theta$ to some respects, but isn't it like a constant? I don't get why approaching the same point from different angles would result in no limit existing, does this have to do with the way curves are sometimes used to solve limits? (My visual knowledge on this kind of stuff is very limited, so I have a really hard time imagining how such a path would look like)","I'm taking a class equivalent of Calculus III, and we saw how to prove continuity of a multivariable function. Recently we looked at the following example: \begin{align}  f(x,y) = \begin{cases} \frac{x^{2\alpha}}{x^2 + y^2},  & \text{if ($x$,$y$)} \neq \text{(0,0)} \\[1ex] 0 & \text{if ($x$,$y$) = (0,0)} \end{cases} \end{align} Long story short, we ended up switching to polar coordinates and simplifying down to: $$\lim_{r \to 0} r^{2\alpha - 2} \cdot \cos^{2\alpha}{\theta}$$ Besides the fact that we can now analyze different cases based on the value of $\alpha$, we came to the conclusion that since the point is approached at by infinitely different "" paths "" (depending on the value of $\theta$), the limit doesn't even exist in the first place. So, I get that this limit hinges on $\theta$ to some respects, but isn't it like a constant? I don't get why approaching the same point from different angles would result in no limit existing, does this have to do with the way curves are sometimes used to solve limits? (My visual knowledge on this kind of stuff is very limited, so I have a really hard time imagining how such a path would look like)",,"['limits', 'multivariable-calculus', 'continuity', 'plane-curves']"
47,$f$ satisfies Laplace equation $f_{xx}+f_{yy}=0$ but is not twice continuously differentiable,satisfies Laplace equation  but is not twice continuously differentiable,f f_{xx}+f_{yy}=0,"I know that harmonic function is defined as a real-valued function of $x$ and $y$ such that 1) it is twice continuously differentiable, 2) it satisfies Laplace equation in its domain, $f_{xx}+f_{yy}=0$. I am wondering if the first requirement is necessary if the second one holds. Up to now I can't come up with one example.","I know that harmonic function is defined as a real-valued function of $x$ and $y$ such that 1) it is twice continuously differentiable, 2) it satisfies Laplace equation in its domain, $f_{xx}+f_{yy}=0$. I am wondering if the first requirement is necessary if the second one holds. Up to now I can't come up with one example.",,"['complex-analysis', 'multivariable-calculus']"
48,A discontinuous function with smooth sections,A discontinuous function with smooth sections,,"I am searching for $f : U\rightarrow \mathbb R $ defined in an open square $U$ in $\mathbb R^2$ so that $(0,0) \in U$, $f$ is not continuous at $(0,0)$, for each $x$  the function $y\mapsto f(x,y)$ is smooth ($C^\infty$) and for each $y$ the function $x \mapsto f(x,y)$ is smooth $(C^\infty$). If  this is not the appropriate place for the question, please tell me where to ask for it?","I am searching for $f : U\rightarrow \mathbb R $ defined in an open square $U$ in $\mathbb R^2$ so that $(0,0) \in U$, $f$ is not continuous at $(0,0)$, for each $x$  the function $y\mapsto f(x,y)$ is smooth ($C^\infty$) and for each $y$ the function $x \mapsto f(x,y)$ is smooth $(C^\infty$). If  this is not the appropriate place for the question, please tell me where to ask for it?",,"['multivariable-calculus', 'derivatives']"
49,Directional derivative and lagrange multipliers,Directional derivative and lagrange multipliers,,"Find the points $(x,y)\in \mathbb R^2$ and unit vectors $\vec u$ such   that the directional derivative of $f(x,y)=3x^2+y$ has the maximum   value if $(x,y)$ is in the circle $x^2+y^2=1$ My attempt: I know that the directional derivative is $D_{\vec u}f=\nabla f\cdot \vec u=6xu_1+u_2$ which does not depends on $y$ if $u_1$ is positive then $D_uf$ attains its maximum in $x=1$ and if $u_1$ is negative then $D_uf$ attains its maximum in $x=-1$ then we have two cases 1)$6u_1+u_2$ and 2)$-6u_1+u_2$ then I need to find the points $(u_1,u_2)$ such that $u_1^2+u_2^2=1$ Using Lagrange multipliers I found that $u_1=u_2={\pm {1\over \sqrt{2}}}$  hence $(x,y)=(\pm 1,0)$ and $(u_1,u_2)=(\pm {1\over \sqrt{2}},\pm{1\over \sqrt{2}})$ Is this approach correct? or Am I missing something?","Find the points $(x,y)\in \mathbb R^2$ and unit vectors $\vec u$ such   that the directional derivative of $f(x,y)=3x^2+y$ has the maximum   value if $(x,y)$ is in the circle $x^2+y^2=1$ My attempt: I know that the directional derivative is $D_{\vec u}f=\nabla f\cdot \vec u=6xu_1+u_2$ which does not depends on $y$ if $u_1$ is positive then $D_uf$ attains its maximum in $x=1$ and if $u_1$ is negative then $D_uf$ attains its maximum in $x=-1$ then we have two cases 1)$6u_1+u_2$ and 2)$-6u_1+u_2$ then I need to find the points $(u_1,u_2)$ such that $u_1^2+u_2^2=1$ Using Lagrange multipliers I found that $u_1=u_2={\pm {1\over \sqrt{2}}}$  hence $(x,y)=(\pm 1,0)$ and $(u_1,u_2)=(\pm {1\over \sqrt{2}},\pm{1\over \sqrt{2}})$ Is this approach correct? or Am I missing something?",,"['calculus', 'multivariable-calculus', 'proof-verification', 'optimization', 'lagrange-multiplier']"
50,Calculate the integral $\iint_D (y^2-x^2)^{xy} (x^2+y^2)dxdy$ on a certain region,Calculate the integral  on a certain region,\iint_D (y^2-x^2)^{xy} (x^2+y^2)dxdy,"Let $D$ be the region that's bounded by $xy=a, xy=b, y^2-x^2=1, y=x$ in the first quadrant. Calculate the integral $\iint_D(y^2-x^2)^{xy}(x^2+y^2)dxdy$. Firstly, I was able to show that the boundary of this region is a Jordan curve and that $D$ is bounded. Therefore, I can use either Green's theorem (which I couldn't find a good use for here) or change of variables. So I tried using polar coordinates but that didn't get me anywhere. I also tried the substitution $u=y-x, v=y+x$ such that $x=-\frac{1}{2}u+\frac{1}{2}v, y=\frac{1}{2}u+\frac{1}{2}v$ such that $|J|=\frac{1}{4}$. I had some trouble here showing what the range of integration is using $u,v$, so for now we'll call it $\Omega$. Therefore: $\iint_D (y^2-x^2)^{xy}(x^2+y^2)dxdy=\iint_\Omega(uv)^{\frac{1}{4}v^2-\frac{1}{4}u^2}(\frac{1}{2}u^2+\frac{1}{2}v^2)dudv$. My problems with this particular coordinate substitution are that it's difficult to calculate the range of integration and that the integral itself (regardless of the range) isn't very friendly. I'd like to know if there's some other direction that should be pursued or if there's an easier coordinate substitution. ***Edit: I do believe that the best way to do this is to use the substitution $u=xy, v=y^2-x^2$ because then $a<u<b, 0<v<1$, but I have trouble calculating this substitution's Jacobian. This is due to the fact that it's not easy to write $x,y$ as functions of $u,v$, so if anyone can help with this direction that would be appreciated.","Let $D$ be the region that's bounded by $xy=a, xy=b, y^2-x^2=1, y=x$ in the first quadrant. Calculate the integral $\iint_D(y^2-x^2)^{xy}(x^2+y^2)dxdy$. Firstly, I was able to show that the boundary of this region is a Jordan curve and that $D$ is bounded. Therefore, I can use either Green's theorem (which I couldn't find a good use for here) or change of variables. So I tried using polar coordinates but that didn't get me anywhere. I also tried the substitution $u=y-x, v=y+x$ such that $x=-\frac{1}{2}u+\frac{1}{2}v, y=\frac{1}{2}u+\frac{1}{2}v$ such that $|J|=\frac{1}{4}$. I had some trouble here showing what the range of integration is using $u,v$, so for now we'll call it $\Omega$. Therefore: $\iint_D (y^2-x^2)^{xy}(x^2+y^2)dxdy=\iint_\Omega(uv)^{\frac{1}{4}v^2-\frac{1}{4}u^2}(\frac{1}{2}u^2+\frac{1}{2}v^2)dudv$. My problems with this particular coordinate substitution are that it's difficult to calculate the range of integration and that the integral itself (regardless of the range) isn't very friendly. I'd like to know if there's some other direction that should be pursued or if there's an easier coordinate substitution. ***Edit: I do believe that the best way to do this is to use the substitution $u=xy, v=y^2-x^2$ because then $a<u<b, 0<v<1$, but I have trouble calculating this substitution's Jacobian. This is due to the fact that it's not easy to write $x,y$ as functions of $u,v$, so if anyone can help with this direction that would be appreciated.",,"['calculus', 'integration', 'multivariable-calculus', 'vector-analysis']"
51,Do regions of integration cover *all* possible regions?,Do regions of integration cover *all* possible regions?,,"Now, I'm not real concerned about the integration itself but more about integration regions. Is it true that all possible regions in two-space are covered by the following setup: $$\int^a_b \int^{g_1(x)}_{g_2(x)} f(x,y) dydx$$ Any function for g is allowed. Note that I'm only concerned about the bounds, not f itself. If you could say which functions other than the standard elementary calculus set is needed please specify. My main concern is regions that are not connected. I think any connected single shape is representable this way but about other shapes? For instance two squares not touching? I know this question is pretty broad but it's more about what integral bounds cover and don't cover, and what is in there means. And at what point you MUST use two integrals.","Now, I'm not real concerned about the integration itself but more about integration regions. Is it true that all possible regions in two-space are covered by the following setup: $$\int^a_b \int^{g_1(x)}_{g_2(x)} f(x,y) dydx$$ Any function for g is allowed. Note that I'm only concerned about the bounds, not f itself. If you could say which functions other than the standard elementary calculus set is needed please specify. My main concern is regions that are not connected. I think any connected single shape is representable this way but about other shapes? For instance two squares not touching? I know this question is pretty broad but it's more about what integral bounds cover and don't cover, and what is in there means. And at what point you MUST use two integrals.",,"['calculus', 'multivariable-calculus', 'definite-integrals']"
52,"Help evaluate $\int_0^\infty x\operatorname{erfc}(a + b\ln (x)) \,dx$.",Help evaluate .,"\int_0^\infty x\operatorname{erfc}(a + b\ln (x)) \,dx","I am trying to evaluate  $$ I = \int_0^\infty x\operatorname{erfc}(a + b\ln (x)) \,dx $$ where $a \ge 0$ and $b> 0$. $$ I = \frac{2}{\sqrt{\pi}}\int_0^\infty \int_{a + b\ln (x)}^{\infty} \mathrm{e}^{-u^2}\,du \,dx $$ Some options I have explored towards finding the solution. Use Series Expansion of the exponential function: Since the upper limit of the inner integral is infinite I think using the Maclaurin series of exponential function will not work. Change the order of integration: I think the region of integration is  $x < a + b \ln(x) < \infty$ & $0 < x < \infty$, but I am not sure what it's corresponding region is. I suspect it is  $0 < x < a + b \ln(x)$ & $0 < a + b \ln(x) < \infty$. If that is correct then is the below integral the correct formulation ? $$I = \frac{2}{\sqrt{\pi}}\int_{x = e^{-a/b}}^{x = \infty} \int_{u = 0}^{u = a + b \ln(x)} \mathrm{e}^{-u^2}\,du \,dx$$ Thanks in advance for your help.","I am trying to evaluate  $$ I = \int_0^\infty x\operatorname{erfc}(a + b\ln (x)) \,dx $$ where $a \ge 0$ and $b> 0$. $$ I = \frac{2}{\sqrt{\pi}}\int_0^\infty \int_{a + b\ln (x)}^{\infty} \mathrm{e}^{-u^2}\,du \,dx $$ Some options I have explored towards finding the solution. Use Series Expansion of the exponential function: Since the upper limit of the inner integral is infinite I think using the Maclaurin series of exponential function will not work. Change the order of integration: I think the region of integration is  $x < a + b \ln(x) < \infty$ & $0 < x < \infty$, but I am not sure what it's corresponding region is. I suspect it is  $0 < x < a + b \ln(x)$ & $0 < a + b \ln(x) < \infty$. If that is correct then is the below integral the correct formulation ? $$I = \frac{2}{\sqrt{\pi}}\int_{x = e^{-a/b}}^{x = \infty} \int_{u = 0}^{u = a + b \ln(x)} \mathrm{e}^{-u^2}\,du \,dx$$ Thanks in advance for your help.",,"['calculus', 'integration', 'multivariable-calculus', 'definite-integrals', 'improper-integrals']"
53,Evaluate the volume of the solid defined by $x^2+y^2+z^2 \leq 9$ and $x^2+y^2 \leq 3y$,Evaluate the volume of the solid defined by  and,x^2+y^2+z^2 \leq 9 x^2+y^2 \leq 3y,"I am asked to solve the following problem: Evaluate the volume of the solid defined by $x^2+y^2+z^2 \leq 9$ and   $x^2+y^2 \leq 3y$. I thought about using spherical coordinates: $$ 0 \leq \rho \leq 3\\ 0 \leq \theta \leq 2\pi\\ 0 \leq \phi \leq \frac{\pi}{2} $$ with $\rho^2sin(\phi)$ on the integral, but that didn't work. $$ \int_{0}^{\pi/2} \int_{0}^{2\pi} \int_{0}^{3} \rho^2sin(\phi) \ d\rho d\theta d\phi $$ Where did I go wrong? TEXTBOOK ANSWER: $18 \pi$","I am asked to solve the following problem: Evaluate the volume of the solid defined by $x^2+y^2+z^2 \leq 9$ and   $x^2+y^2 \leq 3y$. I thought about using spherical coordinates: $$ 0 \leq \rho \leq 3\\ 0 \leq \theta \leq 2\pi\\ 0 \leq \phi \leq \frac{\pi}{2} $$ with $\rho^2sin(\phi)$ on the integral, but that didn't work. $$ \int_{0}^{\pi/2} \int_{0}^{2\pi} \int_{0}^{3} \rho^2sin(\phi) \ d\rho d\theta d\phi $$ Where did I go wrong? TEXTBOOK ANSWER: $18 \pi$",,"['calculus', 'multivariable-calculus', 'coordinate-systems', 'multiple-integral']"
54,"Extremum of $f (x,y) := x^2+y^2$ subject to the equality constraint $x+y=3$",Extremum of  subject to the equality constraint,"f (x,y) := x^2+y^2 x+y=3","I had to find the extremum of $z=x^2+y^2$ subject to the constraint $x+y=3$. I used Lagrange multipliers to reach the conclusion that $(1.5,1.5)$ is an extremum point, but had no way of determining whether it's a maximum or a minimum (we did not study the Sylvester criteria). Regardless, intuitively, the most symmetric sum usually gives the largest result, and this is what I used as a justification for the point being a maximum. This is, of course, hardly a mathematical way of showing the correctness of a statement, which is why I ask here what way there is to show it's a maximum in a correct well defined orderly fashion?","I had to find the extremum of $z=x^2+y^2$ subject to the constraint $x+y=3$. I used Lagrange multipliers to reach the conclusion that $(1.5,1.5)$ is an extremum point, but had no way of determining whether it's a maximum or a minimum (we did not study the Sylvester criteria). Regardless, intuitively, the most symmetric sum usually gives the largest result, and this is what I used as a justification for the point being a maximum. This is, of course, hardly a mathematical way of showing the correctness of a statement, which is why I ask here what way there is to show it's a maximum in a correct well defined orderly fashion?",,"['calculus', 'multivariable-calculus', 'lagrange-multiplier']"
55,Changing order of integration- triple integral,Changing order of integration- triple integral,,"Change the order of integration of $$\int_0^6 \int_0^{12-2y}\int_0^{\frac{12-2y-x}{3}} x \, dz \, dx \, dy$$ to $dx\,dy\,dz$ So at first I started with graphing the function, first by looking at the XY plane and then looking at the z function: So first I need to integrate over $x$ so I ""scan"" the function for the all x-axis as $y$ and $z$ varies. so the I get $x$ from $0$ to $\frac{12-2y-x}{3}\Rightarrow x=12-2y-3z$ next I need to look at the ZY plane and integrate first over the Y-axis which goes from $0$ to $\frac{12-2y-x}{3}$ but beacuse we are on the ZY plane $x=0$ and we get $y=\frac{12-3z}{3}$ and last $z$ goes from $0$ to $12$ So overall I got: $$\int_0^{12}\int_0^{\frac{12-3z}{2}}\int_0^{12-2y-3z} x \, dx \, dy \, dz$$ Which is incorrect, where did I get it wrong?","Change the order of integration of to So at first I started with graphing the function, first by looking at the XY plane and then looking at the z function: So first I need to integrate over so I ""scan"" the function for the all x-axis as and varies. so the I get from to next I need to look at the ZY plane and integrate first over the Y-axis which goes from to but beacuse we are on the ZY plane and we get and last goes from to So overall I got: Which is incorrect, where did I get it wrong?","\int_0^6 \int_0^{12-2y}\int_0^{\frac{12-2y-x}{3}} x \, dz \, dx \, dy dx\,dy\,dz x y z x 0 \frac{12-2y-x}{3}\Rightarrow x=12-2y-3z 0 \frac{12-2y-x}{3} x=0 y=\frac{12-3z}{3} z 0 12 \int_0^{12}\int_0^{\frac{12-3z}{2}}\int_0^{12-2y-3z} x \, dx \, dy \, dz",['multivariable-calculus']
56,Polar equation of the curve y = sinx,Polar equation of the curve y = sinx,,I am looking for the polar equation of the following curve given in Cartesian Coordinates. y = sinx Any kind of hint or help is appreciated.,I am looking for the polar equation of the following curve given in Cartesian Coordinates. y = sinx Any kind of hint or help is appreciated.,,"['calculus', 'multivariable-calculus', 'trigonometry', 'polar-coordinates']"
57,Stoke's Theorem Application,Stoke's Theorem Application,,"Problem: By using Stoke's Theorem, deduce that $\int_{C} \mathbf{r} ( \mathbf{r} \cdot d\mathbf{r}) = \int \int _{S} \mathbf{r} \wedge d \mathbf{S}$. Where $C$ is the simple closed curve bounding the smooth surface $S$. My thoughts were applying the relation $$ \nabla \wedge ( \phi \mathbf{u}) = (\nabla \phi) \wedge \mathbf{u} + \phi \nabla \wedge \mathbf{u}$$ with $\mathbf{u}$ as a constant vector. Hence with Stoke's Theorem,  $$ \mathbf{u} \cdot \int \int_{S} -\nabla \phi \wedge d \mathbf{S} =  \mathbf{u} \cdot \int_{C} \phi d \mathbf{r} $$ So if we can some how relate  $$\int \int_{S} -\nabla \phi \wedge d \mathbf{S} = \int_{C} \phi d \mathbf{r} $$ with the original question that would be pretty nice. However, I couldn't really make sense of what the question means $\mathbf{r} ( \mathbf{r} \cdot d \mathbf{r} ) $. Any ideas? Thank you so much.","Problem: By using Stoke's Theorem, deduce that $\int_{C} \mathbf{r} ( \mathbf{r} \cdot d\mathbf{r}) = \int \int _{S} \mathbf{r} \wedge d \mathbf{S}$. Where $C$ is the simple closed curve bounding the smooth surface $S$. My thoughts were applying the relation $$ \nabla \wedge ( \phi \mathbf{u}) = (\nabla \phi) \wedge \mathbf{u} + \phi \nabla \wedge \mathbf{u}$$ with $\mathbf{u}$ as a constant vector. Hence with Stoke's Theorem,  $$ \mathbf{u} \cdot \int \int_{S} -\nabla \phi \wedge d \mathbf{S} =  \mathbf{u} \cdot \int_{C} \phi d \mathbf{r} $$ So if we can some how relate  $$\int \int_{S} -\nabla \phi \wedge d \mathbf{S} = \int_{C} \phi d \mathbf{r} $$ with the original question that would be pretty nice. However, I couldn't really make sense of what the question means $\mathbf{r} ( \mathbf{r} \cdot d \mathbf{r} ) $. Any ideas? Thank you so much.",,"['multivariable-calculus', 'vectors', 'differential-operators', 'stokes-theorem']"
58,Area of a super circle,Area of a super circle,,"I have to find the area enclosed by $x^{2n}+y^{2n}=1, n \in \mathbb{Z}$ in terms of $F(\alpha)=\int_0^{\pi}\frac{dt}{(\sin{t})^{\alpha}}$ I tried the substitution $x=r\cos^{1/n}{\theta}, y=r\sin^{1/n}{\theta}$, but the Jacobian determinant is really horribly ugly, and I don't think this works. I ended up with $\frac{1}{2^{1/n}n} \int_0^{2\pi}\sin^{1/n}({2\theta})(\tan{\theta}+\cot{\theta})d\theta$. I don't see any other reasonable substitutions.","I have to find the area enclosed by $x^{2n}+y^{2n}=1, n \in \mathbb{Z}$ in terms of $F(\alpha)=\int_0^{\pi}\frac{dt}{(\sin{t})^{\alpha}}$ I tried the substitution $x=r\cos^{1/n}{\theta}, y=r\sin^{1/n}{\theta}$, but the Jacobian determinant is really horribly ugly, and I don't think this works. I ended up with $\frac{1}{2^{1/n}n} \int_0^{2\pi}\sin^{1/n}({2\theta})(\tan{\theta}+\cot{\theta})d\theta$. I don't see any other reasonable substitutions.",,['multivariable-calculus']
59,Gradient Of Complex Least Squares,Gradient Of Complex Least Squares,,"I want to find the gradient of $f : \mathbb{C}^{N} \rightarrow \mathbb{R}$, where $$ f(\mathbf{x}) = \frac 1 2 \Vert \mathbf{Ax} - \mathbf{b} \Vert_2^2, $$ and $\mathbf{A}\in\mathbb{C}^{M \times N}$, $\mathbf{x}\in\mathbb{C}^{N}$, $\mathbf{b}\in\mathbb{C}^{M}$. For real matrices, it's straightforward, but I am getting stuck with the complex case: $$ f(\mathbf{x}) = \frac 1 2 ( \mathbf{Ax} - \mathbf{b} )^\mathrm{H} ( \mathbf{Ax} - \mathbf{b} ) = \frac 1 2 ( \mathbf{x}^\mathrm{H} \mathbf{A}^\mathrm{H} \mathbf{Ax} - \mathbf{x}^\mathrm{H} \mathbf{A}^\mathrm{H} \mathbf{b} - \mathbf{b}^\mathrm{H} \mathbf{Ax} + \mathbf{b}^\mathrm{H} \mathbf{b} ). $$ For the first and thirds terms, the gradient is $2 \mathbf{A}^\mathrm{H} \mathbf{Ax}$ and $\mathbf{b}^\mathrm{H} \mathbf{A}$, respectively. For the fourth term it is $\mathbf{0}$. However, it's the gradient of the second term that has me puzzled and I am wondering if there is something fundamental that I have forgotten. How do I account for the conjugation of $\mathbf{x}$ in the gradient? Is that even possible? Any help is greatly appreciated.","I want to find the gradient of $f : \mathbb{C}^{N} \rightarrow \mathbb{R}$, where $$ f(\mathbf{x}) = \frac 1 2 \Vert \mathbf{Ax} - \mathbf{b} \Vert_2^2, $$ and $\mathbf{A}\in\mathbb{C}^{M \times N}$, $\mathbf{x}\in\mathbb{C}^{N}$, $\mathbf{b}\in\mathbb{C}^{M}$. For real matrices, it's straightforward, but I am getting stuck with the complex case: $$ f(\mathbf{x}) = \frac 1 2 ( \mathbf{Ax} - \mathbf{b} )^\mathrm{H} ( \mathbf{Ax} - \mathbf{b} ) = \frac 1 2 ( \mathbf{x}^\mathrm{H} \mathbf{A}^\mathrm{H} \mathbf{Ax} - \mathbf{x}^\mathrm{H} \mathbf{A}^\mathrm{H} \mathbf{b} - \mathbf{b}^\mathrm{H} \mathbf{Ax} + \mathbf{b}^\mathrm{H} \mathbf{b} ). $$ For the first and thirds terms, the gradient is $2 \mathbf{A}^\mathrm{H} \mathbf{Ax}$ and $\mathbf{b}^\mathrm{H} \mathbf{A}$, respectively. For the fourth term it is $\mathbf{0}$. However, it's the gradient of the second term that has me puzzled and I am wondering if there is something fundamental that I have forgotten. How do I account for the conjugation of $\mathbf{x}$ in the gradient? Is that even possible? Any help is greatly appreciated.",,"['multivariable-calculus', 'matrix-calculus', 'several-complex-variables']"
60,Sum of a step function,Sum of a step function,,"Please help me to find the following summation $$\sum_{m=0}^{N} \sum_{n=0}^N \sum_{i=0}^N \sum_{j=0}^N \mathbb{1}(m,n,i,j)$$ where $1(m,n,i,j)= \left\{ \begin{matrix} 1 \qquad m+n=i+j \\0 \qquad \; \; \; \;\; \textrm{otherwise} \end{matrix}\right.$ Thank you very much!","Please help me to find the following summation $$\sum_{m=0}^{N} \sum_{n=0}^N \sum_{i=0}^N \sum_{j=0}^N \mathbb{1}(m,n,i,j)$$ where $1(m,n,i,j)= \left\{ \begin{matrix} 1 \qquad m+n=i+j \\0 \qquad \; \; \; \;\; \textrm{otherwise} \end{matrix}\right.$ Thank you very much!",,"['calculus', 'real-analysis']"
61,Max rectangle area using Lagrange multipliers,Max rectangle area using Lagrange multipliers,,$P=2x+2y$ and we are looking for $S=xy$ so $S$ need to be max $L=xy+\lambda(2x+2y-P)$ $\frac{\partial L}{\partial x}: y+2\lambda=0$ $\frac{\partial L}{\partial y}: x+2\lambda=0$ $\frac{\partial L}{\partial \lambda}: 2x+2y-P=0$ $y+2\lambda=0\Rightarrow y=-2\lambda$ $x+2\lambda=0\Rightarrow x=-2\lambda$ $xy=4\lambda^2$ I know it is a simple question but I want to make sure I understand. So what is the answer? $4\lambda^2$?,$P=2x+2y$ and we are looking for $S=xy$ so $S$ need to be max $L=xy+\lambda(2x+2y-P)$ $\frac{\partial L}{\partial x}: y+2\lambda=0$ $\frac{\partial L}{\partial y}: x+2\lambda=0$ $\frac{\partial L}{\partial \lambda}: 2x+2y-P=0$ $y+2\lambda=0\Rightarrow y=-2\lambda$ $x+2\lambda=0\Rightarrow x=-2\lambda$ $xy=4\lambda^2$ I know it is a simple question but I want to make sure I understand. So what is the answer? $4\lambda^2$?,,"['calculus', 'multivariable-calculus']"
62,Why saddle point occurs when discriminant<0 ? is it always the case?,Why saddle point occurs when discriminant<0 ? is it always the case?,,"Currently I am studying partial derivatives. In the second derivative test   the condition says if discriminant is less than zero there occurs a saddle point, why is this so ? thanks a a lot in advance.","Currently I am studying partial derivatives. In the second derivative test   the condition says if discriminant is less than zero there occurs a saddle point, why is this so ? thanks a a lot in advance.",,['multivariable-calculus']
63,Quasiconvexity (in the sense of Morrey) implies Rank-One convexity,Quasiconvexity (in the sense of Morrey) implies Rank-One convexity,,"I am trying to understand why Quasiconvexity implies Rank-One convexity. In a standard proof of this fact a sequence of functions is constructed, which converges weakly to zero in $W^{1,p}.$ in order to understand why this is useful, I would like to understand why my simpler argument is wrong. Quasiconvexity of a function $f: \mathbb{R}^{m\times d} \rightarrow \mathbb{R}$ means that for any bounded domain $\Omega \subset \mathbb{R}^d:$ $$\int_{\Omega}f(C + \nabla \omega(y))dy \ge | \Omega | f(C), $$ for any $\omega \in W_0^{1,\infty}(\Omega)$. Now let us consider two matrices $A, B$ that are rank one connected, through $A-B = \xi \otimes \eta.$ Wlog we assume that $\eta = e_1 \in \mathbb{R^d}$ (else we rotate everything). We want to prove that $$f (\theta A + (1-\theta)B) \le \theta f(A) + (1-\theta) f(B)$$ So let us define $C = \theta A + (1-\theta)B$ and $ \Omega = (0,1)^d$ and $$ \omega(y) =  \begin{cases} (1-\theta) (x \cdot e_1) \xi, \text{   if   } 0 \le x \cdot e_1 \le \theta \\ \\ -\theta (x \cdot e_1 -1) \xi, \text{   if   } \theta < x \cdot e_1 \le 1 \end{cases} $$ Now we find that the gradient of $\omega$ is: $$\nabla \omega (y) = \begin{cases} (1-\theta)\xi \otimes e_1, \text{   if   } 0 \le x \cdot e_1 \le \theta \\ \\ -\theta \xi \otimes e_1, \text{   if   } \theta < x \cdot e_1 \le 1 \end{cases}$$ So eventually we find: $$\theta f(A) + (1-\theta) f(B) = \int_{\Omega}f(C + \nabla \omega(y))dy \ge f(C)$$ where the latter inequality holds by quasiconvexity. A proof of this fact is given for example by Dacorogna in ""Direct methods for calculus of variations,"" but it is actually more complicated.","I am trying to understand why Quasiconvexity implies Rank-One convexity. In a standard proof of this fact a sequence of functions is constructed, which converges weakly to zero in $W^{1,p}.$ in order to understand why this is useful, I would like to understand why my simpler argument is wrong. Quasiconvexity of a function $f: \mathbb{R}^{m\times d} \rightarrow \mathbb{R}$ means that for any bounded domain $\Omega \subset \mathbb{R}^d:$ $$\int_{\Omega}f(C + \nabla \omega(y))dy \ge | \Omega | f(C), $$ for any $\omega \in W_0^{1,\infty}(\Omega)$. Now let us consider two matrices $A, B$ that are rank one connected, through $A-B = \xi \otimes \eta.$ Wlog we assume that $\eta = e_1 \in \mathbb{R^d}$ (else we rotate everything). We want to prove that $$f (\theta A + (1-\theta)B) \le \theta f(A) + (1-\theta) f(B)$$ So let us define $C = \theta A + (1-\theta)B$ and $ \Omega = (0,1)^d$ and $$ \omega(y) =  \begin{cases} (1-\theta) (x \cdot e_1) \xi, \text{   if   } 0 \le x \cdot e_1 \le \theta \\ \\ -\theta (x \cdot e_1 -1) \xi, \text{   if   } \theta < x \cdot e_1 \le 1 \end{cases} $$ Now we find that the gradient of $\omega$ is: $$\nabla \omega (y) = \begin{cases} (1-\theta)\xi \otimes e_1, \text{   if   } 0 \le x \cdot e_1 \le \theta \\ \\ -\theta \xi \otimes e_1, \text{   if   } \theta < x \cdot e_1 \le 1 \end{cases}$$ So eventually we find: $$\theta f(A) + (1-\theta) f(B) = \int_{\Omega}f(C + \nabla \omega(y))dy \ge f(C)$$ where the latter inequality holds by quasiconvexity. A proof of this fact is given for example by Dacorogna in ""Direct methods for calculus of variations,"" but it is actually more complicated.",,"['real-analysis', 'multivariable-calculus', 'convex-analysis', 'calculus-of-variations']"
64,Double integral $(x-y)^2\sin(x+y)$ on a given parallelogram,Double integral  on a given parallelogram,(x-y)^2\sin(x+y),"Evaluate the following integral $$\iint_{R_{xy}} (x-y)^2\sin(x+y)\,dx\,dy$$ where $R_{xy}$ is the parallelogram with the successive vertices $(\pi,0),(2\pi,\pi),(\pi,2\pi),(0,\pi)$, using $u = x-y, v=x+y$. The Jacobian is $\frac 12$, therefore: $$\iint_{R_{xy}} (x-y)^2\sin(x+y)\,dx\,dy = \frac 12 \int_{-\pi}^{\pi} \int_\pi^{3\pi} u^2\sin v \,dv\,du = 0.$$ The correct answer is $\frac{\pi^4}3$. Any thoughts on the problem? Am I missing something?","Evaluate the following integral $$\iint_{R_{xy}} (x-y)^2\sin(x+y)\,dx\,dy$$ where $R_{xy}$ is the parallelogram with the successive vertices $(\pi,0),(2\pi,\pi),(\pi,2\pi),(0,\pi)$, using $u = x-y, v=x+y$. The Jacobian is $\frac 12$, therefore: $$\iint_{R_{xy}} (x-y)^2\sin(x+y)\,dx\,dy = \frac 12 \int_{-\pi}^{\pi} \int_\pi^{3\pi} u^2\sin v \,dv\,du = 0.$$ The correct answer is $\frac{\pi^4}3$. Any thoughts on the problem? Am I missing something?",,"['multivariable-calculus', 'definite-integrals', 'multiple-integral']"
65,"Is $f(x,y) = \frac{x \sin(y^2)}{x^2+y^2}$ with $f(0,0) =0$ continuous?",Is  with  continuous?,"f(x,y) = \frac{x \sin(y^2)}{x^2+y^2} f(0,0) =0","I want to know whether the following function is continuous or not, but I have no idea how to do this. $$f(x,y)=\begin{cases} \dfrac{x \sin(y^2)}{x^2+y^2}, &\text{if }(x,y)\neq (0,0)\\ 0, &\text{if }(x,y)=(0,0) \end{cases}$$","I want to know whether the following function is continuous or not, but I have no idea how to do this. $$f(x,y)=\begin{cases} \dfrac{x \sin(y^2)}{x^2+y^2}, &\text{if }(x,y)\neq (0,0)\\ 0, &\text{if }(x,y)=(0,0) \end{cases}$$",,['multivariable-calculus']
66,"$f:\mathbb R \to \mathbb R^n$ be such that $G(f):=\{(x,f(x)):x \in \mathbb R\}$ is closed and connected in $\mathbb R^{n+1}$ , is $f$ continuous ?","be such that  is closed and connected in  , is  continuous ?","f:\mathbb R \to \mathbb R^n G(f):=\{(x,f(x)):x \in \mathbb R\} \mathbb R^{n+1} f","Let $f:\mathbb R \to \mathbb R^n$ be a function whose graph $G(f):=\{(x,f(x)):x \in \mathbb R\}$ is closed and connected in $\mathbb R^{n+1}$ , then is $f$ continuous ?","Let $f:\mathbb R \to \mathbb R^n$ be a function whose graph $G(f):=\{(x,f(x)):x \in \mathbb R\}$ is closed and connected in $\mathbb R^{n+1}$ , then is $f$ continuous ?",,"['general-topology', 'analysis']"
67,"Partial derivative of $f(x,y) = \frac{x^3+y^3}{x^2+y^2}$ at $f_1(0,0)$",Partial derivative of  at,"f(x,y) = \frac{x^3+y^3}{x^2+y^2} f_1(0,0)","To be more precise than the title, the function is actually piecewise $$ f(x,y) = \begin{cases} \frac{x^3+y^3}{x^2+y^2} & (x,y) \ne (0,0) \\ 0 & (x,y) = (0,0) \\ \end{cases} $$ I checked that the function is continuous at $(0,0)$, so I then calculated the partial derivative with respect to $x$ as $$ f_1(x,y) = \frac{x^4-2xy^3+3x^2y^2}{(x^2+y^2)^2} \tag{1} $$ This is undefined at $(0,0)$, so I then tried to find the limit around accumulation points. Let $S_1$ be the points on the $x$ axis $$ \lim_{x \to 0} f_1(x,0) = \frac{x^4}{(x^2)^2} = 1 \tag{2} $$ Let $S_2$ be the points on the line $y = x$ $$ \lim_{x \to 0} f_1(x,x) = \frac{x^4-2x^4+3x^4}{(x^2+x^2)^2} = \frac{2x^4}{(2x^2)^2} = \frac{1}{2} \tag{3} $$ So, the limits are different around different accumulation points. That's where I'm confused because the answer should be $1$.","To be more precise than the title, the function is actually piecewise $$ f(x,y) = \begin{cases} \frac{x^3+y^3}{x^2+y^2} & (x,y) \ne (0,0) \\ 0 & (x,y) = (0,0) \\ \end{cases} $$ I checked that the function is continuous at $(0,0)$, so I then calculated the partial derivative with respect to $x$ as $$ f_1(x,y) = \frac{x^4-2xy^3+3x^2y^2}{(x^2+y^2)^2} \tag{1} $$ This is undefined at $(0,0)$, so I then tried to find the limit around accumulation points. Let $S_1$ be the points on the $x$ axis $$ \lim_{x \to 0} f_1(x,0) = \frac{x^4}{(x^2)^2} = 1 \tag{2} $$ Let $S_2$ be the points on the line $y = x$ $$ \lim_{x \to 0} f_1(x,x) = \frac{x^4-2x^4+3x^4}{(x^2+x^2)^2} = \frac{2x^4}{(2x^2)^2} = \frac{1}{2} \tag{3} $$ So, the limits are different around different accumulation points. That's where I'm confused because the answer should be $1$.",,"['calculus', 'multivariable-calculus']"
68,Why is the total differential of a function just the sum of the components?,Why is the total differential of a function just the sum of the components?,,"In a multivariable calculus class, I was taught if $f$ is a function of $x, y, z$, the total differential of the function is: $$df = f_xdx + f_ydy + f_zdz$$ Intuitively, it seems that there are missing terms to me. For example, shouldn't there be some sort of interactive term to account for how $x$ and $y$ change together? Or do these terms just ""go away"" under a technical derivation? Or maybe these terms or never there to begin with? Any help is appreciated!","In a multivariable calculus class, I was taught if $f$ is a function of $x, y, z$, the total differential of the function is: $$df = f_xdx + f_ydy + f_zdz$$ Intuitively, it seems that there are missing terms to me. For example, shouldn't there be some sort of interactive term to account for how $x$ and $y$ change together? Or do these terms just ""go away"" under a technical derivation? Or maybe these terms or never there to begin with? Any help is appreciated!",,"['multivariable-calculus', 'chain-rule']"
69,Implicit Function Theorem Application to 2 Equations,Implicit Function Theorem Application to 2 Equations,,"So I think I understand how to use the Implicit Function theorem to find partial derivatives given one function but I am confused as to how to do this for 2 functions. I'm trying to find $\frac{\partial x}{\partial u}, \frac{\partial x}{\partial v}$ around the point $(1,-1,-1,2)$ given the equations $$x^2+2y^2+u^2+v=6$$$$2x^3+4y^2+u+v^2=9$$ and I thus calculated that $$Df(1,-1,-1,2)=\begin{bmatrix} 2 & -4 & -2 & 1\\6 & -8 & 1 & 4\end{bmatrix}$$ but I'm not sure where to go from here since the equation for the partial of the implicit function in my textbook is only for cases where we have one equation? Thanks in advance for any help! EDIT: So I've looked at the problem a bit further and I'm still confused as to how the derivative for an implicit function works in the case of $2+$ equations? Could anyone give an example with simpler equations so I could then apply it to this system? Thanks again.","So I think I understand how to use the Implicit Function theorem to find partial derivatives given one function but I am confused as to how to do this for 2 functions. I'm trying to find $\frac{\partial x}{\partial u}, \frac{\partial x}{\partial v}$ around the point $(1,-1,-1,2)$ given the equations $$x^2+2y^2+u^2+v=6$$$$2x^3+4y^2+u+v^2=9$$ and I thus calculated that $$Df(1,-1,-1,2)=\begin{bmatrix} 2 & -4 & -2 & 1\\6 & -8 & 1 & 4\end{bmatrix}$$ but I'm not sure where to go from here since the equation for the partial of the implicit function in my textbook is only for cases where we have one equation? Thanks in advance for any help! EDIT: So I've looked at the problem a bit further and I'm still confused as to how the derivative for an implicit function works in the case of $2+$ equations? Could anyone give an example with simpler equations so I could then apply it to this system? Thanks again.",,"['multivariable-calculus', 'implicit-differentiation', 'implicit-function-theorem']"
70,Open immersion pulls back symplectic form to symplectic form?,Open immersion pulls back symplectic form to symplectic form?,,"If $M$ is symplectic, and $f: W \to M$ is an open immersion, i.e. an immersion where $W$ and $M$ have the same dimension, does $f$ necessarily pull back a symplectic form on $M$ to a symplectic form on $W$?","If $M$ is symplectic, and $f: W \to M$ is an open immersion, i.e. an immersion where $W$ and $M$ have the same dimension, does $f$ necessarily pull back a symplectic form on $M$ to a symplectic form on $W$?",,"['multivariable-calculus', 'differential-geometry']"
71,Finding values for integral $\iint_A \frac{dxdy}{|x|^p+|y|^q}$ converges,Finding values for integral  converges,\iint_A \frac{dxdy}{|x|^p+|y|^q},"Given the following integral $$\iint_A \frac{dxdy}{|x|^p+|y|^q}$$ where $A=|x|+|y|>1$. How can one find for which $p$ and $q$ values the integral converges? Since the function is  non-negative it is sufficient to show convergence/divergence on any Jordan exhaustion of $A$ in order to show convergence/divergence on $A$. I tried to use polar coordinates here, but I don't know how to fit it to match $p$ and $q$ so it'll actually be helpful.","Given the following integral $$\iint_A \frac{dxdy}{|x|^p+|y|^q}$$ where $A=|x|+|y|>1$. How can one find for which $p$ and $q$ values the integral converges? Since the function is  non-negative it is sufficient to show convergence/divergence on any Jordan exhaustion of $A$ in order to show convergence/divergence on $A$. I tried to use polar coordinates here, but I don't know how to fit it to match $p$ and $q$ so it'll actually be helpful.",,"['calculus', 'integration', 'multivariable-calculus', 'improper-integrals']"
72,"Surfaces in dimensions higher than $3$, how to compute the normal vector?","Surfaces in dimensions higher than , how to compute the normal vector?",3,"In my book surfaces are defined as maps $\sigma\ \colon A \subseteq \mathbb R^2 \to \mathbb R^n$ ($n \geq 3$). Then the book goes on to define regular surfaces and the normal vector. The latter is defined as the cross product $$N_\sigma(u, v) = \frac{\partial\sigma}{\partial u} \times \frac{\partial\sigma}{\partial v}$$ However, this only holds for $\mathbb R^3$, because the cross product is defined only there. What about higher dimensions? I'm asking because I also have a theorem involving the normal vector and the dimension isn't $3$. EDIT: Here is the theorem I'm talking about. Def Let $\sigma\ \colon K \to \mathbb R^n$ e $\eta\ \colon K' \to \mathbb R^n$ be two regular surfaces. We say that $\sigma$ and $\eta$ differ by a change of variables if: there exist two open sets $A, A' \subseteq \mathbb R^2$ such that $K \subseteq A$ e $K' \subseteq A'$; there exist a change of variables $\alpha\ \colon A' \to A$ of class $C^1$ on $A'$ such that $\alpha(K') = K$; $\eta(s, t) = \sigma(\alpha(s, t))$ for every $(s, t) \in A'$. Theorem If two surfaces differ by a change of variables, then: $\sigma(K) = \eta(K')$; The normal vector of $\sigma$ is $$N_\sigma(\alpha(s, t)) = N_\eta(\alpha(s, t))\det J_\alpha(s, t)$$ The proof is left to the reader. I encountered this problem when trying to prove the above claim. I don't know how to calculate $N_\sigma$! Any hints?","In my book surfaces are defined as maps $\sigma\ \colon A \subseteq \mathbb R^2 \to \mathbb R^n$ ($n \geq 3$). Then the book goes on to define regular surfaces and the normal vector. The latter is defined as the cross product $$N_\sigma(u, v) = \frac{\partial\sigma}{\partial u} \times \frac{\partial\sigma}{\partial v}$$ However, this only holds for $\mathbb R^3$, because the cross product is defined only there. What about higher dimensions? I'm asking because I also have a theorem involving the normal vector and the dimension isn't $3$. EDIT: Here is the theorem I'm talking about. Def Let $\sigma\ \colon K \to \mathbb R^n$ e $\eta\ \colon K' \to \mathbb R^n$ be two regular surfaces. We say that $\sigma$ and $\eta$ differ by a change of variables if: there exist two open sets $A, A' \subseteq \mathbb R^2$ such that $K \subseteq A$ e $K' \subseteq A'$; there exist a change of variables $\alpha\ \colon A' \to A$ of class $C^1$ on $A'$ such that $\alpha(K') = K$; $\eta(s, t) = \sigma(\alpha(s, t))$ for every $(s, t) \in A'$. Theorem If two surfaces differ by a change of variables, then: $\sigma(K) = \eta(K')$; The normal vector of $\sigma$ is $$N_\sigma(\alpha(s, t)) = N_\eta(\alpha(s, t))\det J_\alpha(s, t)$$ The proof is left to the reader. I encountered this problem when trying to prove the above claim. I don't know how to calculate $N_\sigma$! Any hints?",,"['multivariable-calculus', 'differential-geometry', 'surfaces']"
73,What is the most painless way to check whether a $f: \mathbb{R}^n \to \mathbb{R}^n$ and $f : \mathbb{R}^n \to \mathbb{R}^{n\times m}$ is continuous,What is the most painless way to check whether a  and  is continuous,f: \mathbb{R}^n \to \mathbb{R}^n f : \mathbb{R}^n \to \mathbb{R}^{n\times m},"Given $f(x_1,x_2) = \begin{bmatrix} x_1^2 + x_2 \\ x_1 + x_2^2\end{bmatrix}$ and $g(x_1,x_2) = \begin{bmatrix} 2x_1 & 1 \\ 1 &  2 x_2\end{bmatrix}$ In my class I am only taught the $\epsilon-\delta$ method, but oh boy is that method tedious and painful, even for trivial looking functions such as $f(x) = x^{3/2}$ you need divine intervention to fully carry out the proof. There must be a better way, especially given that $f$ and $g$ are obviously continuous elementwise. What is the ""best"" way to show that $f,g$ are continuous?","Given $f(x_1,x_2) = \begin{bmatrix} x_1^2 + x_2 \\ x_1 + x_2^2\end{bmatrix}$ and $g(x_1,x_2) = \begin{bmatrix} 2x_1 & 1 \\ 1 &  2 x_2\end{bmatrix}$ In my class I am only taught the $\epsilon-\delta$ method, but oh boy is that method tedious and painful, even for trivial looking functions such as $f(x) = x^{3/2}$ you need divine intervention to fully carry out the proof. There must be a better way, especially given that $f$ and $g$ are obviously continuous elementwise. What is the ""best"" way to show that $f,g$ are continuous?",,"['calculus', 'multivariable-calculus', 'functions', 'continuity', 'proof-writing']"
74,"Why doesn't the limit $\lim_{(x,y) \rightarrow (0,0)} \frac{ e^{x+y} - x - y}{\sqrt{x^2 + y^2}}$ exist?",Why doesn't the limit  exist?,"\lim_{(x,y) \rightarrow (0,0)} \frac{ e^{x+y} - x - y}{\sqrt{x^2 + y^2}}","Why is this limit non-existant? $\lim_{(x,y) \rightarrow (0,0)} \frac{ e^{x+y} - x - y}{\sqrt{x^2 + y^2}}$ I can't seem to find $2$ different paths that would show it is non-existant.","Why is this limit non-existant? $\lim_{(x,y) \rightarrow (0,0)} \frac{ e^{x+y} - x - y}{\sqrt{x^2 + y^2}}$ I can't seem to find $2$ different paths that would show it is non-existant.",,"['real-analysis', 'analysis', 'limits', 'multivariable-calculus']"
75,Showing that a function is injective,Showing that a function is injective,,"I am trying to show that the following function is injective in some neighborhood of $(0, 0)$: $f:\mathbb R^2 \rightarrow \mathbb R^2$ given by $$f(x, y)=(\sin(x^3)\cosh(y), \cos(x^3)\sinh(y))$$ I tried to use the inverse function theorem but $df(0, 0)$ is not invertible.","I am trying to show that the following function is injective in some neighborhood of $(0, 0)$: $f:\mathbb R^2 \rightarrow \mathbb R^2$ given by $$f(x, y)=(\sin(x^3)\cosh(y), \cos(x^3)\sinh(y))$$ I tried to use the inverse function theorem but $df(0, 0)$ is not invertible.",,['multivariable-calculus']
76,How do I deal with this Diag operator when differentiating with respect to a matrix?,How do I deal with this Diag operator when differentiating with respect to a matrix?,,"This question is an extension of this question . The arbitrary function $B(\cdot)$ is now as specified as follows: $$ B(\mu, \sigma^{2}) = \exp \left(\mu + \frac{1}{2}\sigma^{2} \right). $$ So if I write $$ \begin{align*}   s &= X_{i}\beta + Z_{i}\mu_{i} + \frac{1}{2}\operatorname{dg}\left(Z_{i}\Lambda_{i}Z_{i}^{T} \right)\\   e &= \exp \left(s \right)\\   E &= \operatorname{Diag}\left(e\right) \end{align*} $$ where $\exp$ is applied element wise and $\operatorname{Diag}$ is an operator that creates a diagonal matrix with the elements of the vector inside. The problem is that I need to compute $\frac{\partial^{2} \underline{\ell}}{\partial \operatorname{vech}\left( \Lambda_{i}\right)\partial\mu_{i} }$, I'm stuck with calculating the differential within the $\operatorname{Diag}$ operator. $$ \frac{\partial \underline{\ell} }{\partial \operatorname{vech}\left(\Lambda_{i} \right)} = \frac{1}{2}\left(\operatorname{vec} \left(\Lambda_{i}^{-1} - \Sigma^{-1} - Z_{i}^{T}EZ_{i} \right)\right)^{T}D_{K}. $$ Setting $f= \frac{\partial \underline{\ell} }{\partial \operatorname{vech}\left(\Lambda_{i} \right)}$ and trying to differentiate this again with respect to $\mu_{i}$, $$ \begin{align*}   df &= -\frac{1}{2} \left(\operatorname{vec} \left(Z_{i}^{T} dE Z_{i} \right)\right)^{T}D_{K}\\   dE &= \operatorname{Diag}\left(de \right)\\  &= \operatorname{Diag}\left(e \circ ds \right)\\ &= \operatorname{Diag} \left(e \circ Z_{i}d\mu_{i} \right) \end{align*} $$ the differential is inside the $\operatorname{Diag}$ operator. How do I proceed from here?","This question is an extension of this question . The arbitrary function $B(\cdot)$ is now as specified as follows: $$ B(\mu, \sigma^{2}) = \exp \left(\mu + \frac{1}{2}\sigma^{2} \right). $$ So if I write $$ \begin{align*}   s &= X_{i}\beta + Z_{i}\mu_{i} + \frac{1}{2}\operatorname{dg}\left(Z_{i}\Lambda_{i}Z_{i}^{T} \right)\\   e &= \exp \left(s \right)\\   E &= \operatorname{Diag}\left(e\right) \end{align*} $$ where $\exp$ is applied element wise and $\operatorname{Diag}$ is an operator that creates a diagonal matrix with the elements of the vector inside. The problem is that I need to compute $\frac{\partial^{2} \underline{\ell}}{\partial \operatorname{vech}\left( \Lambda_{i}\right)\partial\mu_{i} }$, I'm stuck with calculating the differential within the $\operatorname{Diag}$ operator. $$ \frac{\partial \underline{\ell} }{\partial \operatorname{vech}\left(\Lambda_{i} \right)} = \frac{1}{2}\left(\operatorname{vec} \left(\Lambda_{i}^{-1} - \Sigma^{-1} - Z_{i}^{T}EZ_{i} \right)\right)^{T}D_{K}. $$ Setting $f= \frac{\partial \underline{\ell} }{\partial \operatorname{vech}\left(\Lambda_{i} \right)}$ and trying to differentiate this again with respect to $\mu_{i}$, $$ \begin{align*}   df &= -\frac{1}{2} \left(\operatorname{vec} \left(Z_{i}^{T} dE Z_{i} \right)\right)^{T}D_{K}\\   dE &= \operatorname{Diag}\left(de \right)\\  &= \operatorname{Diag}\left(e \circ ds \right)\\ &= \operatorname{Diag} \left(e \circ Z_{i}d\mu_{i} \right) \end{align*} $$ the differential is inside the $\operatorname{Diag}$ operator. How do I proceed from here?",,"['linear-algebra', 'multivariable-calculus', 'derivatives', 'matrix-calculus']"
77,Confusion regarding notation of a dual transformation,Confusion regarding notation of a dual transformation,,"I'm reading Spivak's Calculus on Manifolds and in Chapter 4 he defines the dual transformation (although he doesn't call it that) as follows: If $f:V \rightarrow W$ is a linear transformation, a linear transformation $f^* : \mathfrak{I}^k (W) \rightarrow \mathfrak{I}^k (V)$ is defined by $f^* T (v_1, \dots, v_k) = T(f(v_1),\dots,f(v_k))$ for $T \in \mathfrak{I}^k (W)$ and $v_1, \dots, v_k \in V$. Here, $\mathfrak{I}^k (V)$ denotes the set of all $k$-tensors on $V$. My confusion is with the next statement that he makes: It is easy to verify that $f^*(S \otimes T) = f^*S \otimes f^*T$. Here, $S$ would be a $k$-tensor on $W$ and $T$ would be an $l$-tensor on $W$. So, on the L.H.S. $f^*$ denotes the map from $\mathfrak{I}^{k+l} (W)$ to $\mathfrak{I}^{k+l} (V)$. But, on the R.H.S. the first $f^*$ denotes the map from $\mathfrak{I}^k (W)$ to $\mathfrak{I}^k (V)$ and the second $f^*$ denotes the map from $\mathfrak{I}^l (W)$ to $\mathfrak{I}^l (V)$. Is this notation standard, so that the dimension of the space on which $f^*$ acts is to be understood from the context? Since we really have three different functions in this equation, is it still appropriate to say that $f^*$ 'distributes over $\otimes$'?","I'm reading Spivak's Calculus on Manifolds and in Chapter 4 he defines the dual transformation (although he doesn't call it that) as follows: If $f:V \rightarrow W$ is a linear transformation, a linear transformation $f^* : \mathfrak{I}^k (W) \rightarrow \mathfrak{I}^k (V)$ is defined by $f^* T (v_1, \dots, v_k) = T(f(v_1),\dots,f(v_k))$ for $T \in \mathfrak{I}^k (W)$ and $v_1, \dots, v_k \in V$. Here, $\mathfrak{I}^k (V)$ denotes the set of all $k$-tensors on $V$. My confusion is with the next statement that he makes: It is easy to verify that $f^*(S \otimes T) = f^*S \otimes f^*T$. Here, $S$ would be a $k$-tensor on $W$ and $T$ would be an $l$-tensor on $W$. So, on the L.H.S. $f^*$ denotes the map from $\mathfrak{I}^{k+l} (W)$ to $\mathfrak{I}^{k+l} (V)$. But, on the R.H.S. the first $f^*$ denotes the map from $\mathfrak{I}^k (W)$ to $\mathfrak{I}^k (V)$ and the second $f^*$ denotes the map from $\mathfrak{I}^l (W)$ to $\mathfrak{I}^l (V)$. Is this notation standard, so that the dimension of the space on which $f^*$ acts is to be understood from the context? Since we really have three different functions in this equation, is it still appropriate to say that $f^*$ 'distributes over $\otimes$'?",,"['multivariable-calculus', 'notation']"
78,$C$ be curve of intersection of hemisphere $x^2+y^2+z^2=2ax$ and cylinder $x^2+y^2=2bx$ ; to evaluate $\int_C(y^2+z^2)dx+(x^2+z^2)dy+(x^2+y^2)dz$,be curve of intersection of hemisphere  and cylinder  ; to evaluate,C x^2+y^2+z^2=2ax x^2+y^2=2bx \int_C(y^2+z^2)dx+(x^2+z^2)dy+(x^2+y^2)dz,"$C$ be the curve of intersection of the hemisphere $x^2+y^2+z^2=2ax$ and the cylinder $x^2+y^2=2bx$ , where $0<b<a$ ; how to evaluate $\int_C(y^2+z^2)dx+(x^2+z^2)dy+(x^2+y^2)dz$ using Stoke's theorem ? I can't parametrize the curve $C$ nor able to get the surface $S$ . Please help . Thanks in advance","$C$ be the curve of intersection of the hemisphere $x^2+y^2+z^2=2ax$ and the cylinder $x^2+y^2=2bx$ , where $0<b<a$ ; how to evaluate $\int_C(y^2+z^2)dx+(x^2+z^2)dy+(x^2+y^2)dz$ using Stoke's theorem ? I can't parametrize the curve $C$ nor able to get the surface $S$ . Please help . Thanks in advance",,"['multivariable-calculus', 'surface-integrals']"
79,Solving by limit definition,Solving by limit definition,,"To show that $\displaystyle\lim_{(x,y)->(0,0)} \dfrac{5x^2y}{x^2+y^2}=0$ I tried to do this by the limit definition by pluging 0 to l in lim(x,y)->(Xo,Yo) | f(x,y)-l |. Then i got stucked where i couldnt put delta  to get an inequility. Can some one lemme know how to get this inequility?","To show that $\displaystyle\lim_{(x,y)->(0,0)} \dfrac{5x^2y}{x^2+y^2}=0$ I tried to do this by the limit definition by pluging 0 to l in lim(x,y)->(Xo,Yo) | f(x,y)-l |. Then i got stucked where i couldnt put delta  to get an inequility. Can some one lemme know how to get this inequility?",,"['limits', 'multivariable-calculus']"
80,How to find the parametric equation of the intersection of $z=x+y$ with $y=x$?,How to find the parametric equation of the intersection of  with ?,z=x+y y=x,"How to find such intersection? After that, how to find its parametric equation? I need it because I must solve the line integral: $$\int_Y dx + ydy +dz$$ in the curve of the intersection of $z=x+y$ with $y=x$, $z\le 2$, from $(-1,-1,2)$ to $(1,1,2)$","How to find such intersection? After that, how to find its parametric equation? I need it because I must solve the line integral: $$\int_Y dx + ydy +dz$$ in the curve of the intersection of $z=x+y$ with $y=x$, $z\le 2$, from $(-1,-1,2)$ to $(1,1,2)$",,"['calculus', 'integration', 'multivariable-calculus', 'definite-integrals']"
81,Assistance setting up a double integral bounded by the upper nappe of $z^2=x^2+y^2$ and above the sphere $x^2+y^2+z^2=9$,Assistance setting up a double integral bounded by the upper nappe of  and above the sphere,z^2=x^2+y^2 x^2+y^2+z^2=9,"So basically I need to find the volume of a solid bounded below by the upper nappe of the cone $z^2=x^2+y^2$ and bounded above by the sphere $x^2+y^2+z^2=9$ I know it looks like this: But I'm not sure how to setup the double integral to find the volume.  I do understand what my limits of integration will be and that I will need to do some conversions to polar coordinates, but I don't understand how to find the functions to integrate. Any hints or tips are greatly appreciated. EDIT After thinking about the problem a little more, is it a matter of just adding the two functions together? For example: $$\iint \sqrt{x^2+y^2}+\sqrt{9-x^2-y^2}\;dy\;dx$$","So basically I need to find the volume of a solid bounded below by the upper nappe of the cone $z^2=x^2+y^2$ and bounded above by the sphere $x^2+y^2+z^2=9$ I know it looks like this: But I'm not sure how to setup the double integral to find the volume.  I do understand what my limits of integration will be and that I will need to do some conversions to polar coordinates, but I don't understand how to find the functions to integrate. Any hints or tips are greatly appreciated. EDIT After thinking about the problem a little more, is it a matter of just adding the two functions together? For example: $$\iint \sqrt{x^2+y^2}+\sqrt{9-x^2-y^2}\;dy\;dx$$",,"['calculus', 'multivariable-calculus', 'volume']"
82,"Boy's surface, visualization of the preimage of self-intersection locus as graph on projective plane","Boy's surface, visualization of the preimage of self-intersection locus as graph on projective plane",,"For the immersion of the projective plane in $\mathbb{R}^3$ with one triple point, what does the preimage of the self-interaction locus as a graph on a projective plane look like?","For the immersion of the projective plane in $\mathbb{R}^3$ with one triple point, what does the preimage of the self-interaction locus as a graph on a projective plane look like?",,"['general-topology', 'multivariable-calculus']"
83,Double integral - Change of Variables,Double integral - Change of Variables,,"I am trying to evaluate this double integral, but I don't see any good change of variables (I tried polar, but it got really hairy): $$ \iint_D \sqrt{(x-1)^2+y^2} \, dx \, dy $$ given $D = \{(x,y):x^2+y^2 \le 1, y>0\}$","I am trying to evaluate this double integral, but I don't see any good change of variables (I tried polar, but it got really hairy): $$ \iint_D \sqrt{(x-1)^2+y^2} \, dx \, dy $$ given $D = \{(x,y):x^2+y^2 \le 1, y>0\}$",,"['integration', 'multivariable-calculus']"
84,Calculating Surface Integrals,Calculating Surface Integrals,,"How does one calculate the surface integral of a vector field on a surface? I have been tasked with solving surface integral of ${\bf V} = x^2{\bf e_x}+ y^2{\bf e_y}+ z^2 {\bf e_z}$ on the surface of a cube bounding the region $0\le x,y,z \le 1$. Verify result using Divergence Theorem and calculating associated volume integral. How do all these things relate in simple terms please. I can read a definition, but examples would be helpful.","How does one calculate the surface integral of a vector field on a surface? I have been tasked with solving surface integral of ${\bf V} = x^2{\bf e_x}+ y^2{\bf e_y}+ z^2 {\bf e_z}$ on the surface of a cube bounding the region $0\le x,y,z \le 1$. Verify result using Divergence Theorem and calculating associated volume integral. How do all these things relate in simple terms please. I can read a definition, but examples would be helpful.",,"['calculus', 'integration', 'multivariable-calculus', 'vectors', 'surface-integrals']"
85,Line Integrals with Vector Fields,Line Integrals with Vector Fields,,"I am trying to find the line integral of the vector field $F(x,y)=(x^2-y^2)$ x $-2xy$ y from $(0,0)$ to $(1,2)$ along a few different paths. First path is along curve $y=2x^2$ Second path is curve described by $x=t^2$ and $y=2t$ Last path is the path that goes from $(0,0)$ to $(2,0)$ along the x-axis and then along line $(2,0)$ to $(1,2)$. This is what I have so far. For the first one: $$\int F(x,y)\cdot dr=\int ((x^2-y^2)+(-2xy)4t)  dt$$ I said $r(t)=x(t)$ x + $y(t)$ y . Therefore $dr=(1$ x +$4t$ y )dt. This part is algebra and derivatives. Thus by setting $x=t$ and $y=2x^2=2t^2$, we can solve the integral as follows: $$\int_0^1 (t^2-4t^4-16t^4)dt=\frac{-31}{15}$$ For the second one: Same process, but this time, $r(t)=t^2$ x +$2t$ y . Therefore, $dr=(2t$ x +$2$ y )$dt$. Now the integral becomes $$\int F(x,y)\cdot dr=\int ((x^2-y^2)t^2+(-2xy)2)  dt$$ Now with substituting for $x$ and $y$, $$\int_0^1 (2t^5-8t^3+8t^3)dt=\frac{1}{3}$$ For the last one: I broke this up first along the x-axis then along the line $y=-2x+4$. Now we have $$\int F_x dx+\int F \cdot dr=\int_0^1(x^2-y^2)dx+\int_2^1((x^2-y^2)+2xy*-2)dx$$ I found out that $dy=-2$ since the line from $(2,0)$ to $(1,2)$. So for the first integral, y=0, but the second one, $y=-2x+4$. Thus left with $$\int_0^1(x^2-0^2)dx+\int_2^1((x^2-(-2x+4)^2)+2x(-2x+4)*-2)dx=\frac{-11}{3}$$","I am trying to find the line integral of the vector field $F(x,y)=(x^2-y^2)$ x $-2xy$ y from $(0,0)$ to $(1,2)$ along a few different paths. First path is along curve $y=2x^2$ Second path is curve described by $x=t^2$ and $y=2t$ Last path is the path that goes from $(0,0)$ to $(2,0)$ along the x-axis and then along line $(2,0)$ to $(1,2)$. This is what I have so far. For the first one: $$\int F(x,y)\cdot dr=\int ((x^2-y^2)+(-2xy)4t)  dt$$ I said $r(t)=x(t)$ x + $y(t)$ y . Therefore $dr=(1$ x +$4t$ y )dt. This part is algebra and derivatives. Thus by setting $x=t$ and $y=2x^2=2t^2$, we can solve the integral as follows: $$\int_0^1 (t^2-4t^4-16t^4)dt=\frac{-31}{15}$$ For the second one: Same process, but this time, $r(t)=t^2$ x +$2t$ y . Therefore, $dr=(2t$ x +$2$ y )$dt$. Now the integral becomes $$\int F(x,y)\cdot dr=\int ((x^2-y^2)t^2+(-2xy)2)  dt$$ Now with substituting for $x$ and $y$, $$\int_0^1 (2t^5-8t^3+8t^3)dt=\frac{1}{3}$$ For the last one: I broke this up first along the x-axis then along the line $y=-2x+4$. Now we have $$\int F_x dx+\int F \cdot dr=\int_0^1(x^2-y^2)dx+\int_2^1((x^2-y^2)+2xy*-2)dx$$ I found out that $dy=-2$ since the line from $(2,0)$ to $(1,2)$. So for the first integral, y=0, but the second one, $y=-2x+4$. Thus left with $$\int_0^1(x^2-0^2)dx+\int_2^1((x^2-(-2x+4)^2)+2x(-2x+4)*-2)dx=\frac{-11}{3}$$",,"['calculus', 'integration', 'multivariable-calculus', 'vectors', 'line-integrals']"
86,How to prove this assertion? Do I need to use Inverse function Theorem?,How to prove this assertion? Do I need to use Inverse function Theorem?,,"Let $f:R^2\to R$ be a continuously differentiable function. Show that there exist a continuous one-one function $g:[0,1]\to R^2$ such that $f\circ g:[0,1]\to R $ is  constant.","Let $f:R^2\to R$ be a continuously differentiable function. Show that there exist a continuous one-one function $g:[0,1]\to R^2$ such that $f\circ g:[0,1]\to R $ is  constant.",,"['real-analysis', 'multivariable-calculus', 'inverse-function-theorem']"
87,Why are these three integrals all $0$?,Why are these three integrals all ?,0,"I have the following three triple integrals: $$\iiint_S x \sigma \mathrm{d} V$$ $$\iiint_S y \sigma \mathrm{d} V$$ $$\iiint_S z \sigma \mathrm{d} V$$ where $\sigma = k \left(1 - (\rho / a)^3 \right)$, $\rho$ is the distance from the origin in spherical coordinates, and $S$ represents a sphere at the origin with radius $a$. I can easily compute all three integrals to be $0$. However, I'm asked to make an argument based on the symmetry of the sphere, rather than actually calculating the integrals. I could, for example, substitute $x = \rho \cos \theta \sin \phi$ to calculate the first integral as 0, but what specifically about the symmetry of the sphere is causing this to be true?","I have the following three triple integrals: $$\iiint_S x \sigma \mathrm{d} V$$ $$\iiint_S y \sigma \mathrm{d} V$$ $$\iiint_S z \sigma \mathrm{d} V$$ where $\sigma = k \left(1 - (\rho / a)^3 \right)$, $\rho$ is the distance from the origin in spherical coordinates, and $S$ represents a sphere at the origin with radius $a$. I can easily compute all three integrals to be $0$. However, I'm asked to make an argument based on the symmetry of the sphere, rather than actually calculating the integrals. I could, for example, substitute $x = \rho \cos \theta \sin \phi$ to calculate the first integral as 0, but what specifically about the symmetry of the sphere is causing this to be true?",,"['calculus', 'integration', 'multivariable-calculus']"
88,Gradient versus Tangent,Gradient versus Tangent,,"I am in calculus 3 and I have a question on gradient versus tangent. I know that this has been answered several times before: Difference between a Gradient and Tangent But I still have much trouble understanding them. Assume $z = x^2 + y^2$ (a sphere) In the topic of directional derivatives and gradients , gradient is the steepest slope (or, more formally, the direction to move so that z increases most rapidly). AT THE SAME TIME , it is a normal to the level curve (to my understanding, it means when z is fixed at a particular value, a vector that is perpendicular to the tangent of a curve at a particular point. The gradient points to the concave side of the level curve). Combining the two ideas above, I assume it is correct that a normal to the level curve is the same as the direction in which z will increase most rapidly (although I am unsure why it is true). Somehow, then I thought that gradient points to the direction where a tangent line to the 3D object is, because gradient points to the direction of the steepest slope. And then in the following topic of tangent planes, now it tells me that gradient is actually a normal line instead of a tangent line to a 3D object? What a surprise! In the case of 2D, gradient is the slope of a line. therefore, I naturally think that in the case of 3D, gradient is also the slope (or, the vector that points to the steepest slope for z). It is not the case? What is wrong with my thinking?","I am in calculus 3 and I have a question on gradient versus tangent. I know that this has been answered several times before: Difference between a Gradient and Tangent But I still have much trouble understanding them. Assume $z = x^2 + y^2$ (a sphere) In the topic of directional derivatives and gradients , gradient is the steepest slope (or, more formally, the direction to move so that z increases most rapidly). AT THE SAME TIME , it is a normal to the level curve (to my understanding, it means when z is fixed at a particular value, a vector that is perpendicular to the tangent of a curve at a particular point. The gradient points to the concave side of the level curve). Combining the two ideas above, I assume it is correct that a normal to the level curve is the same as the direction in which z will increase most rapidly (although I am unsure why it is true). Somehow, then I thought that gradient points to the direction where a tangent line to the 3D object is, because gradient points to the direction of the steepest slope. And then in the following topic of tangent planes, now it tells me that gradient is actually a normal line instead of a tangent line to a 3D object? What a surprise! In the case of 2D, gradient is the slope of a line. therefore, I naturally think that in the case of 3D, gradient is also the slope (or, the vector that points to the steepest slope for z). It is not the case? What is wrong with my thinking?",,"['multivariable-calculus', 'scalar-fields']"
89,Vector Integration - Intuition,Vector Integration - Intuition,,"I understand that an integral of a scalar valued function can be visualized as ""signed area under the curve"". But what about integration of a vector valued function by its parameter? Is there a visually intuitive geometric explanation for what $\int_a^b \mathbf{r}(t)\, \mathrm{d}t$ represents if $\mathbf{r}(t)$ is a function from, say, $\mathbb{R}$ to $\mathbb{R}^3$?","I understand that an integral of a scalar valued function can be visualized as ""signed area under the curve"". But what about integration of a vector valued function by its parameter? Is there a visually intuitive geometric explanation for what $\int_a^b \mathbf{r}(t)\, \mathrm{d}t$ represents if $\mathbf{r}(t)$ is a function from, say, $\mathbb{R}$ to $\mathbb{R}^3$?",,"['integration', 'multivariable-calculus', 'soft-question', 'intuition']"
90,Product of limits when one is zero and the other does not exist,Product of limits when one is zero and the other does not exist,,"We have two functions $f(x)$ and $g(x)$, such that $\lim_{ x\to 0}f(x)=0$ but $\lim_{x\to 0} g(x)$ does not exist. Would that mean that   $\lim_{x\to 0}f(x)g(x)= 0$, assuming we don't divide by zero anywhere? Also, is this true in the multivariable case, where $f$ is a function of $x$ and $g$ is a function of $y$? In that case, is $\lim_{(x,y)\to(0,0)}=f(x)g(y)=0$?","We have two functions $f(x)$ and $g(x)$, such that $\lim_{ x\to 0}f(x)=0$ but $\lim_{x\to 0} g(x)$ does not exist. Would that mean that   $\lim_{x\to 0}f(x)g(x)= 0$, assuming we don't divide by zero anywhere? Also, is this true in the multivariable case, where $f$ is a function of $x$ and $g$ is a function of $y$? In that case, is $\lim_{(x,y)\to(0,0)}=f(x)g(y)=0$?",,"['calculus', 'limits', 'multivariable-calculus']"
91,Show a function is linear,Show a function is linear,,I'm asked to show that for $x \in \mathbb{R^2}$ that $h:\mathbb{R} \to \mathbb{R}$ defined by $h(t) = f(tx)$ is differentiable by first showing it is linear. The solutions say to use $g(x) = -g(-x)$ but I can't understand why,I'm asked to show that for $x \in \mathbb{R^2}$ that $h:\mathbb{R} \to \mathbb{R}$ defined by $h(t) = f(tx)$ is differentiable by first showing it is linear. The solutions say to use $g(x) = -g(-x)$ but I can't understand why,,"['calculus', 'real-analysis', 'linear-algebra', 'algebra-precalculus', 'multivariable-calculus']"
92,How to show that $\nabla \cdot (\nabla f) = \nabla ^{2} f$ for this function?,How to show that  for this function?,\nabla \cdot (\nabla f) = \nabla ^{2} f,"I've been given a homework question that asks me to show that $\nabla \cdot (\nabla f) = \nabla ^{2} f$, where $f(x,y,z)=xyz-z^{2}sin(y)$. However, $\nabla \cdot (\nabla f)$ is a scalar quantity, whilst $\nabla ^{2} f$ is a vector quantity, so they cannot be equal. What am I missing here? Thanks.","I've been given a homework question that asks me to show that $\nabla \cdot (\nabla f) = \nabla ^{2} f$, where $f(x,y,z)=xyz-z^{2}sin(y)$. However, $\nabla \cdot (\nabla f)$ is a scalar quantity, whilst $\nabla ^{2} f$ is a vector quantity, so they cannot be equal. What am I missing here? Thanks.",,"['calculus', 'multivariable-calculus', 'vectors', 'vector-fields']"
93,Computing the Laplacian,Computing the Laplacian,,"How can I compute the laplacian $\nabla^2 g $ where $ g $($\textbf{r}$) = f(r), where f is a function of a single variable, r = |$\textbf{r}$|. I understand that the laplacian is defined to be $ \frac{\partial^2g}{\partial x_i ^2} $ but I don't really understand the definition of $ \phi $ and how I should apply the chain rule here.","How can I compute the laplacian $\nabla^2 g $ where $ g $($\textbf{r}$) = f(r), where f is a function of a single variable, r = |$\textbf{r}$|. I understand that the laplacian is defined to be $ \frac{\partial^2g}{\partial x_i ^2} $ but I don't really understand the definition of $ \phi $ and how I should apply the chain rule here.",,['multivariable-calculus']
94,finding a vector field when given curl(F),finding a vector field when given curl(F),,"Is there a vector field F such that Curl(F) = ($xy^2$, $yz^2$, $zx^2$)? Explain. Ive been testing it out myself, coordinate by coordinate, and once determining what $F_3$ or $F_1$ or $F_2$ would need to be I realize that i'm pretty sure it is not possible to have a vector field that produces that curl. I am not absolute in my answer and im having a tough time figuring out how to explain this. Any help would be appreciated.","Is there a vector field F such that Curl(F) = ($xy^2$, $yz^2$, $zx^2$)? Explain. Ive been testing it out myself, coordinate by coordinate, and once determining what $F_3$ or $F_1$ or $F_2$ would need to be I realize that i'm pretty sure it is not possible to have a vector field that produces that curl. I am not absolute in my answer and im having a tough time figuring out how to explain this. Any help would be appreciated.",,['multivariable-calculus']
95,"What is the connection between outer product, quadratic form and definiteness?","What is the connection between outer product, quadratic form and definiteness?",,"In proving whether a function is convex or concave, I frequently encounter people who ends their analysis on something the sort: Since $\nabla^2 f(x,y) = \begin{bmatrix} a^2 & b a\\ ab & b^2  \end{bmatrix}$ is an outer product of $\begin{bmatrix} a & b  \end{bmatrix}$ therefore $f(x,y)$ is convex Can someone clarify what is this so special about outer product that we can make quick statements such as this? Is it true that every single hessian that can be expressed as an outer product is positive semidefinite?","In proving whether a function is convex or concave, I frequently encounter people who ends their analysis on something the sort: Since $\nabla^2 f(x,y) = \begin{bmatrix} a^2 & b a\\ ab & b^2  \end{bmatrix}$ is an outer product of $\begin{bmatrix} a & b  \end{bmatrix}$ therefore $f(x,y)$ is convex Can someone clarify what is this so special about outer product that we can make quick statements such as this? Is it true that every single hessian that can be expressed as an outer product is positive semidefinite?",,"['linear-algebra', 'matrices', 'multivariable-calculus', 'optimization']"
96,"Given a surface $z=f(x,y)$ is $\nabla (f(a,b))$ always perpendicular to the surface at the point $(a,b,f(a,b))$",Given a surface  is  always perpendicular to the surface at the point,"z=f(x,y) \nabla (f(a,b)) (a,b,f(a,b))","Given a surface $z=f(x,y)$ is $\nabla (f(a,b))$ always perpendicular to the surface at the point $(a,b,f(a,b))$. I see many problems using this gradient function to find a normal vector to a surface so it is most likely true I guess but I can't seem to come up with an intuitive reason or better still a proof to show me why this is the case. Any help?","Given a surface $z=f(x,y)$ is $\nabla (f(a,b))$ always perpendicular to the surface at the point $(a,b,f(a,b))$. I see many problems using this gradient function to find a normal vector to a surface so it is most likely true I guess but I can't seem to come up with an intuitive reason or better still a proof to show me why this is the case. Any help?",,['multivariable-calculus']
97,When to type a function in bold?,When to type a function in bold?,,"Is it convention to bold any function with more than one output? For example, $\textbf{f}:\textbf{R}^2 \mapsto \textbf{R}^3$ or $f: \textbf{R}^2 \mapsto \textbf{R}^3$ $\textbf{f}(\textbf{x})$ or $f(\textbf{x})$","Is it convention to bold any function with more than one output? For example, $\textbf{f}:\textbf{R}^2 \mapsto \textbf{R}^3$ or $f: \textbf{R}^2 \mapsto \textbf{R}^3$ $\textbf{f}(\textbf{x})$ or $f(\textbf{x})$",,['multivariable-calculus']
98,"If every real valued differentiable function on $X \subset \mathbb R^n$ is bounded , then is it true that $X$ is compact ?","If every real valued differentiable function on  is bounded , then is it true that  is compact ?",X \subset \mathbb R^n X,"If every real valued differentiable function on $X \subset \mathbb R^n$ is bounded , then is it true that $X$ is compact ?","If every real valued differentiable function on $X \subset \mathbb R^n$ is bounded , then is it true that $X$ is compact ?",,"['analysis', 'multivariable-calculus']"
99,Why does the polar coordinate method not work?,Why does the polar coordinate method not work?,,"I tried to calculate the limit $\lim\limits_{(x,y)\rightarrow(0,0)} (x^2+y^2)^x$ By using polar coordinates $ x = r \cdot \cos(\theta)$ $y = r \cdot \sin(\theta)$ resulting in $((r\cdot \cos(\theta))^2+(r\cdot \sin(\theta))^2)^{r\cdot \cos(\theta)}$ $ = (r^2(\cos^2(\theta)+\sin^2(\theta))^{r\cdot\cos(\theta)} =  (r^2)^{r\cdot\cos(\theta)} = r^{2r\cdot \cos(\theta)}$ which for $r \to 0$ is $1$ But the actual limit of $\lim\limits_{(x,y)\rightarrow(0,0)} (x^2+y^2)^x$ is $0$. Why does the polar cooridinate method not work? Thank you in advance.","I tried to calculate the limit $\lim\limits_{(x,y)\rightarrow(0,0)} (x^2+y^2)^x$ By using polar coordinates $ x = r \cdot \cos(\theta)$ $y = r \cdot \sin(\theta)$ resulting in $((r\cdot \cos(\theta))^2+(r\cdot \sin(\theta))^2)^{r\cdot \cos(\theta)}$ $ = (r^2(\cos^2(\theta)+\sin^2(\theta))^{r\cdot\cos(\theta)} =  (r^2)^{r\cdot\cos(\theta)} = r^{2r\cdot \cos(\theta)}$ which for $r \to 0$ is $1$ But the actual limit of $\lim\limits_{(x,y)\rightarrow(0,0)} (x^2+y^2)^x$ is $0$. Why does the polar cooridinate method not work? Thank you in advance.",,"['limits', 'multivariable-calculus', 'polar-coordinates']"
