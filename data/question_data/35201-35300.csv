,title_raw,title_text,title_latex,body_raw,body_text,body_latex,tags
0,Probability remains same Why?,Probability remains same Why?,,A box has $m$ black and $n$ white balls. A ball is drawn at random and put back with $k$ additional balls of the same colour as that of the drawn ball. Now a ball is drawn again. Find the probability that it is a white ball. I got the answer $\frac{n}{m+n}$ . I just want to understand why it's not dependent on $k$ .,A box has black and white balls. A ball is drawn at random and put back with additional balls of the same colour as that of the drawn ball. Now a ball is drawn again. Find the probability that it is a white ball. I got the answer . I just want to understand why it's not dependent on .,m n k \frac{n}{m+n} k,"['probability', 'probability-theory', 'conditional-probability', 'elementary-probability']"
1,expected value of Consecutive numbers [duplicate],expected value of Consecutive numbers [duplicate],,"This question already has answers here : Probability and expectancy problem [closed] (2 answers) Closed 5 years ago . We choose randomly a subset $\mathit S$ of size 25 from the set $\{ 1,2,...,100\}$ , what is the expected value of number of Consecutive numbers in $\mathit S$ ? Consecutive numbers in $\mathit S$ : is a pair $\{ i,i+1\}$ where i $\in$$\mathit S$ and i+1 $\in$$\mathit S$ .","This question already has answers here : Probability and expectancy problem [closed] (2 answers) Closed 5 years ago . We choose randomly a subset of size 25 from the set , what is the expected value of number of Consecutive numbers in ? Consecutive numbers in : is a pair where i and i+1 .","\mathit S \{ 1,2,...,100\} \mathit S \mathit S \{ i,i+1\} \in\mathit S \in\mathit S","['probability', 'expected-value']"
2,Non convergence of a series of random variables,Non convergence of a series of random variables,,"Question: Let $(X_n), n\in\mathbb{N}$ be a sequence of independent r.v.s such that $P(X_n=n^4)=\frac{1}{n^4}$ and $P(X_n=-1)=1-\frac{1}{n^4}$ . Study the a.s. convergence of $S_n=\sum_{i=1}^n X_n$ as $n\rightarrow +\infty$ . My Attempt: I have been simulating the stochastic process $S_n$ in R to understand whether convergence was possible at all but in none of the simulations I performed I obtained a finite value (all of the paths go to -9999). Also, clearly, $\sum_{i=1}^n \operatorname{Var}(S_n)\rightarrow +\infty$ as $n\rightarrow +\infty$ . Can I thus conclude that $S_n$ does NOT converge a.s.? Many thanks in advance for the help!","Question: Let be a sequence of independent r.v.s such that and . Study the a.s. convergence of as . My Attempt: I have been simulating the stochastic process in R to understand whether convergence was possible at all but in none of the simulations I performed I obtained a finite value (all of the paths go to -9999). Also, clearly, as . Can I thus conclude that does NOT converge a.s.? Many thanks in advance for the help!","(X_n), n\in\mathbb{N} P(X_n=n^4)=\frac{1}{n^4} P(X_n=-1)=1-\frac{1}{n^4} S_n=\sum_{i=1}^n X_n n\rightarrow +\infty S_n \sum_{i=1}^n \operatorname{Var}(S_n)\rightarrow +\infty n\rightarrow +\infty S_n","['probability', 'probability-theory', 'convergence-divergence', 'borel-cantelli-lemmas']"
3,The Maximimum Monochromatic k-Cliques for Complete Graph,The Maximimum Monochromatic k-Cliques for Complete Graph,,"Show:  For a complete graph $K_{n}$ , there is a coloring of the edges with $2$ colors, such that the number of monochromatic $k$ -Cliques is a maximum of $\binom{n}{k}2^{1-\binom{k}{2}}$ . I do not know where to begin on this exercise. As a hint, our professor gave us the following pre-exercise, which I have been able to solve: Let $n \in \mathbb N$ , and $\mathcal{K}$ be the set of permutations   possible for a set $\{1,...,n\}$ . Let $\sigma \in \mathcal{K}, $ such   that $\sigma: [n] \to [n]$ is a randomly selected permutation. Find   the probability space, define random variable $X$ as the number of   fixed points and find $\mathbb E[X]$ . How do the two questions fit together? I am lost.","Show:  For a complete graph , there is a coloring of the edges with colors, such that the number of monochromatic -Cliques is a maximum of . I do not know where to begin on this exercise. As a hint, our professor gave us the following pre-exercise, which I have been able to solve: Let , and be the set of permutations   possible for a set . Let such   that is a randomly selected permutation. Find   the probability space, define random variable as the number of   fixed points and find . How do the two questions fit together? I am lost.","K_{n} 2 k \binom{n}{k}2^{1-\binom{k}{2}} n \in \mathbb N \mathcal{K} \{1,...,n\} \sigma \in \mathcal{K},  \sigma: [n] \to [n] X \mathbb E[X]","['probability', 'stochastic-processes', 'permutations']"
4,Why $\mathbb E\left[\sup\frac{|Y_t-Y_s|}{|t-s|^\alpha }\right]<\infty$ imply $(Y_t)_t$ continuous?,Why  imply  continuous?,\mathbb E\left[\sup\frac{|Y_t-Y_s|}{|t-s|^\alpha }\right]<\infty (Y_t)_t,"Let $(Y_t)_t$ a stochastic process s.t. $$\mathbb E\left[\sup_{s,t\in [0,1], s\neq t}\frac{|Y_t-Y_s|}{|t-s|^\alpha }\right]<\infty,$$ with $\alpha >0$ . Why does this implies that $(Y_t)_t$ is continuous a.s. ? Does it come from the fact that if $\mathbb E[X]<\infty$ then $\mathbb P\{X<\infty\}=1$ , and thus $$\mathbb P\left\{\sup_{s,t\in [0,1], s\neq t}\frac{|Y_t-Y_s|}{|t-s|^\alpha }<\infty\right\}=1.$$ Also $$\mathbb P\left\{\sup_{s,t\in [0,1], s\neq t}\frac{|Y_t-Y_s|}{|t-s|^\alpha }<\infty\right\}\leq \mathbb P\left\{\frac{|Y_t-Y_s|}{|t-s|^\alpha}<\infty\right\}=1.$$ 1) How can I continue ? Does it implies that there is $C>0$ s.t. $$\mathbb P\{|Y_t-Y_s|<C|t-s|^\alpha \}=1,$$ or that $$\mathbb P\{\exists C>0: |Y_t-Y_s|\leq C|t-s|^\alpha \}=1 \ \ ?$$ 2) And will it implies that $$\mathbb P\{\lim_{t\to s}|Y_t-Y_s|\}=1 \ \ ?$$ If yes, why ? I don't understand why I can put the limit inside.","Let a stochastic process s.t. with . Why does this implies that is continuous a.s. ? Does it come from the fact that if then , and thus Also 1) How can I continue ? Does it implies that there is s.t. or that 2) And will it implies that If yes, why ? I don't understand why I can put the limit inside.","(Y_t)_t \mathbb E\left[\sup_{s,t\in [0,1], s\neq t}\frac{|Y_t-Y_s|}{|t-s|^\alpha }\right]<\infty, \alpha >0 (Y_t)_t \mathbb E[X]<\infty \mathbb P\{X<\infty\}=1 \mathbb P\left\{\sup_{s,t\in [0,1], s\neq t}\frac{|Y_t-Y_s|}{|t-s|^\alpha }<\infty\right\}=1. \mathbb P\left\{\sup_{s,t\in [0,1], s\neq t}\frac{|Y_t-Y_s|}{|t-s|^\alpha }<\infty\right\}\leq \mathbb P\left\{\frac{|Y_t-Y_s|}{|t-s|^\alpha}<\infty\right\}=1. C>0 \mathbb P\{|Y_t-Y_s|<C|t-s|^\alpha \}=1, \mathbb P\{\exists C>0: |Y_t-Y_s|\leq C|t-s|^\alpha \}=1 \ \ ? \mathbb P\{\lim_{t\to s}|Y_t-Y_s|\}=1 \ \ ?","['probability', 'stochastic-processes']"
5,"If $n$ balls are distributed randomly into $k$ buckets, what is the probability that the first bucket contains $j$ balls? [duplicate]","If  balls are distributed randomly into  buckets, what is the probability that the first bucket contains  balls? [duplicate]",n k j,"This question already has an answer here : What's the explicit formula for Permutations of Subsets of a Multiset? [duplicate] (1 answer) Closed last year . I know that the solution I have is wrong but I'd like to give you my thought process in the hopes that someone can point out the flaw in my thinking: Since there has to be $j$ balls in the first bucket, I remove $j$ balls from $n$ and one bucket from $k$ and then find the number of ways $n-j$ balls can be randomly distributed into $k-1$ buckets instead. Using the stars and bars method where there are $n-j$ stars and $k-2$ bars, we get that there are ${n-j+k-2} \choose {k-2}$ ways to find the number of ways $n-j$ balls can be randomly distributed into $k-1$ buckets. Then I need to find the total number of random ball distributions of all the balls and all the buckets. There are ${n+k-1} \choose {k-1}$ ways to do that using the stars and bars method. Putting it all together, we get that the probability is ${n-j+k-2} \choose {k-2}$ all over ${n+k-1} \choose {k-1}$ . Where did I go wrong? Is the entire reasoning flawed to begin with or can it be recovered with some tweaks? I should note that these balls are indistinguishable. Edit: If I could checkmark everyone's answer I would, but the least I could do is upvote them, thanks everyone for the clarification.","This question already has an answer here : What's the explicit formula for Permutations of Subsets of a Multiset? [duplicate] (1 answer) Closed last year . I know that the solution I have is wrong but I'd like to give you my thought process in the hopes that someone can point out the flaw in my thinking: Since there has to be balls in the first bucket, I remove balls from and one bucket from and then find the number of ways balls can be randomly distributed into buckets instead. Using the stars and bars method where there are stars and bars, we get that there are ways to find the number of ways balls can be randomly distributed into buckets. Then I need to find the total number of random ball distributions of all the balls and all the buckets. There are ways to do that using the stars and bars method. Putting it all together, we get that the probability is all over . Where did I go wrong? Is the entire reasoning flawed to begin with or can it be recovered with some tweaks? I should note that these balls are indistinguishable. Edit: If I could checkmark everyone's answer I would, but the least I could do is upvote them, thanks everyone for the clarification.",j j n k n-j k-1 n-j k-2 {n-j+k-2} \choose {k-2} n-j k-1 {n+k-1} \choose {k-1} {n-j+k-2} \choose {k-2} {n+k-1} \choose {k-1},"['probability', 'combinatorics', 'proof-verification']"
6,"In a shuffled deck, are the probabilities of finding one pair of cards and a second pair of cards independent events?","In a shuffled deck, are the probabilities of finding one pair of cards and a second pair of cards independent events?",,"This question caused a massive fight in a number theory class I was in a few years ago. Class was split, and no one changed their opinion. Curious if it is obvious to outsiders, or if anyone can offer any proof. Course was split with most poker and blackjack players arguing these are dependent events, with most others claiming they were independent and you could simulate relevant events with an infinite deck. Take a standard deck with no jokers. 52 cards. Four aces, four deuces, four threes, and so on. Shuffle the deck. Draw cards until you spot a match, ie, a card with the same numerical value as the last card drawn. Does drawing such a match affect the odds of drawing a subsequent match later in the deck? Clarifying note: I call a match a ""pair"" in the title, but ""match"" is more precise. Drawing three in a row counts as two matches, but most people would not call that two pairs.","This question caused a massive fight in a number theory class I was in a few years ago. Class was split, and no one changed their opinion. Curious if it is obvious to outsiders, or if anyone can offer any proof. Course was split with most poker and blackjack players arguing these are dependent events, with most others claiming they were independent and you could simulate relevant events with an infinite deck. Take a standard deck with no jokers. 52 cards. Four aces, four deuces, four threes, and so on. Shuffle the deck. Draw cards until you spot a match, ie, a card with the same numerical value as the last card drawn. Does drawing such a match affect the odds of drawing a subsequent match later in the deck? Clarifying note: I call a match a ""pair"" in the title, but ""match"" is more precise. Drawing three in a row counts as two matches, but most people would not call that two pairs.",,"['probability', 'combinatorics', 'conditional-probability', 'poker']"
7,Probability Question for 2-Sided Coin (Verification),Probability Question for 2-Sided Coin (Verification),,"Suppose you and your friends have a two-sided coin each. Your coin lands Heads with probability $\frac{1}{6}$ , while your friend's coin lands Heads with probability $\frac{3}{4}$ . The two coins are independent of one another. Suppose you play a game where you both flip your coins once, and if they both land on the same side (i.e., both Heads or both Tails) you get $x from your friend, but if they land on different sides, then your friend gets 2 dollars from you. What is the minimum integer value of $x$ for which your expected total winnings after 3 rounds of this game are positive (i.e., you are expected to make money rather than lose some)? Hint: Let $W$ denote your total winnings after 3 rounds of this game. What values of $W$ are possible? This is what I have so far: Your Coin: Probability of Heads P(H1) = $\frac{1}{6}$ ; P(T1) = $\frac{5}{6}$ Your friends coin: Probability of Heads P(H2) = $\frac{3}{4}$ ; P(T2) = $\frac{1}{4}$ For a given round: $W$ = you win : Both heads or both sides $L$ = you lose : landing on different side P(W) = Probabilty of Both heads or Both tails = P(H1H2) + P(T1T2) = P(H1)P(H2) + P((T1)P(T2) = $\frac{1}{6}$ • $\frac{3}{4}$ • $\frac{5}{6}$ • $\frac{1}{4}$ = $\frac{1}{3}$ P( $L$ ) = Probability of landing different sides = 1- Probabilty of Both heads or Both tails = 1- $\frac{1}{3}$ = $\frac{2}{3}$ P( $W$ ) = $\frac{1}{3}$ P( $L$ ) = $\frac{2}{3}$ The game is played for three rounds : Let $Y$ be the number of rounds you win Then possible values of $Y$ = 0,1,2,3; $W$ : Total winning after three rounds when $Y$ = 0 ; You lose 3 rounds and lose 3*2 = 6$ ; W = 0x - 6 = -6 Y=1 ; You win one round and lose 2 round ; W = 1x - 4 = x - 4 Y=2 ; You win 2 rounds and lose one round : W= 2x -2 Y=3 ; You win all three rounds W = 3x - 0 = 3x If Y is random variable representing number of wins in 3 rounds with probability of winning a round: p= $\frac{1}{3}$ and q =1 -p = $\frac{2}{3}$ ; If expected value of winning after 3 rounds to be positive [ex) x - 4 > 0 = x > 4] Then, when x > 4 then expected value of winning after 3 rounds to be positive when value of x = 5 ; then expected value of winning is x-4 ex) 5-4 = 1 Minimum integer value of x > 4 is 5 Minimum integer value of x = 5 for which you expected total winnings after 3 rounds of this game are positive.","Suppose you and your friends have a two-sided coin each. Your coin lands Heads with probability , while your friend's coin lands Heads with probability . The two coins are independent of one another. Suppose you play a game where you both flip your coins once, and if they both land on the same side (i.e., both Heads or both Tails) you get $x from your friend, but if they land on different sides, then your friend gets 2 dollars from you. What is the minimum integer value of for which your expected total winnings after 3 rounds of this game are positive (i.e., you are expected to make money rather than lose some)? Hint: Let denote your total winnings after 3 rounds of this game. What values of are possible? This is what I have so far: Your Coin: Probability of Heads P(H1) = ; P(T1) = Your friends coin: Probability of Heads P(H2) = ; P(T2) = For a given round: = you win : Both heads or both sides = you lose : landing on different side P(W) = Probabilty of Both heads or Both tails = P(H1H2) + P(T1T2) = P(H1)P(H2) + P((T1)P(T2) = • • • = P( ) = Probability of landing different sides = 1- Probabilty of Both heads or Both tails = 1- = P( ) = P( ) = The game is played for three rounds : Let be the number of rounds you win Then possible values of = 0,1,2,3; : Total winning after three rounds when = 0 ; You lose 3 rounds and lose 3*2 = 6$ ; W = 0x - 6 = -6 Y=1 ; You win one round and lose 2 round ; W = 1x - 4 = x - 4 Y=2 ; You win 2 rounds and lose one round : W= 2x -2 Y=3 ; You win all three rounds W = 3x - 0 = 3x If Y is random variable representing number of wins in 3 rounds with probability of winning a round: p= and q =1 -p = ; If expected value of winning after 3 rounds to be positive [ex) x - 4 > 0 = x > 4] Then, when x > 4 then expected value of winning after 3 rounds to be positive when value of x = 5 ; then expected value of winning is x-4 ex) 5-4 = 1 Minimum integer value of x > 4 is 5 Minimum integer value of x = 5 for which you expected total winnings after 3 rounds of this game are positive.",\frac{1}{6} \frac{3}{4} x W W \frac{1}{6} \frac{5}{6} \frac{3}{4} \frac{1}{4} W L \frac{1}{6} \frac{3}{4} \frac{5}{6} \frac{1}{4} \frac{1}{3} L \frac{1}{3} \frac{2}{3} W \frac{1}{3} L \frac{2}{3} Y Y W Y \frac{1}{3} \frac{2}{3},"['probability', 'probability-theory', 'statistics', 'proof-verification', 'probability-distributions']"
8,Expected number of offers until house is sold,Expected number of offers until house is sold,,"I am selling my house, and have decided to accept the first offering exceeding $K$ dollars. Assuming that offers are independent rv with common distribution $F$ , find the expected number of offers received before I sell the house. Try I call $X$ to be number of offers receiver before house is sold. Suppose we have $n$ such offers and call them $X_1,X_2,...,X_n$ . Therefore, $X = \sum X_i$ . We have that $$ E(X) = E(X_1) + .. + E(X_n) $$ Since all $X_i$ have common distribution $F$ , then $$ E(X_i) = \int\limits_0^K x f(x) $$ where $f = F' $ . So, $$ E(X) = n \int\limits_0^K x f(x) $$ is this correct?","I am selling my house, and have decided to accept the first offering exceeding dollars. Assuming that offers are independent rv with common distribution , find the expected number of offers received before I sell the house. Try I call to be number of offers receiver before house is sold. Suppose we have such offers and call them . Therefore, . We have that Since all have common distribution , then where . So, is this correct?","K F X n X_1,X_2,...,X_n X = \sum X_i  E(X) = E(X_1) + .. + E(X_n)  X_i F  E(X_i) = \int\limits_0^K x f(x)  f = F'   E(X) = n \int\limits_0^K x f(x) ",['probability']
9,What is the probability that some random event won't happen in the next 10 minutes given it happened exactly twice in the last 120 minutes?,What is the probability that some random event won't happen in the next 10 minutes given it happened exactly twice in the last 120 minutes?,,"The title says pretty much all. The event is fully random, it has the same chance any minute, it can happen multiple times at the same minute. How is it possible to calculate that? I made a simulation code in c++, which says 78%, is that good answer? https://gist.github.com/bakaiadam/4f4732f4147fc3a5c68f121bf57b919f Edit: since people say it's not clear what I mean and I'm not really familiar with random distribution types, I tell you the concrete example that I was thinking about: My idea came from seeing group of people passing in a forest path. Let's say i have been there for 2 hours and saw 2 group of people. What is the probability that i will see at least 1 group of people in the next 10 minutes? Obviously there can be more than one group of people passing at the same minute, and obviously their chance  to be there is independent from each other.","The title says pretty much all. The event is fully random, it has the same chance any minute, it can happen multiple times at the same minute. How is it possible to calculate that? I made a simulation code in c++, which says 78%, is that good answer? https://gist.github.com/bakaiadam/4f4732f4147fc3a5c68f121bf57b919f Edit: since people say it's not clear what I mean and I'm not really familiar with random distribution types, I tell you the concrete example that I was thinking about: My idea came from seeing group of people passing in a forest path. Let's say i have been there for 2 hours and saw 2 group of people. What is the probability that i will see at least 1 group of people in the next 10 minutes? Obviously there can be more than one group of people passing at the same minute, and obviously their chance  to be there is independent from each other.",,['probability']
10,Random Gift Giving at a Party - Combinatorics Problem,Random Gift Giving at a Party - Combinatorics Problem,,"Each of $10$ employees brings one (distinct) present to an office party. Each present is given to a randomly selected employee by Santa (an employee can get more than one present). What is the probability that at least two employees receive no presents? Firstly, there are $10^{10}$ total ways to give the $10$ employees the $10$ presents. So this is our denominator. My attempt was to consider the complement and consider the number of ways that either $0$ employees receive no presents (every employee gets a present) or $1$ employee receives no present. Case 1: $0$ employees There are $10$ employees and $10$ presents. So there are $10^{10}$ ways to give the presents. Case 2: $1$ employee Step 1: Decide which employee receives no presents: $10$ possibilities. Step 2: Distribute the $10$ presents to the remaining $9$ employees: $9^{10}$ ways. So the number of ways in which at least $2$ employees receive no presents is: $1-(10^{10}+9^{10}$ ). So my final answer is: $1-\displaystyle\frac{(10^{10}+9^{10})}{10^{10}}$ . However, this answer does not match the answer in my textbook. Which is: $1-\displaystyle\frac{10!-10\times 9 \times \frac{10!}{2!}}{10^{10}}$ Where did my attempt go wrong and how can I correct it?","Each of employees brings one (distinct) present to an office party. Each present is given to a randomly selected employee by Santa (an employee can get more than one present). What is the probability that at least two employees receive no presents? Firstly, there are total ways to give the employees the presents. So this is our denominator. My attempt was to consider the complement and consider the number of ways that either employees receive no presents (every employee gets a present) or employee receives no present. Case 1: employees There are employees and presents. So there are ways to give the presents. Case 2: employee Step 1: Decide which employee receives no presents: possibilities. Step 2: Distribute the presents to the remaining employees: ways. So the number of ways in which at least employees receive no presents is: ). So my final answer is: . However, this answer does not match the answer in my textbook. Which is: Where did my attempt go wrong and how can I correct it?",10 10^{10} 10 10 0 1 0 10 10 10^{10} 1 10 10 9 9^{10} 2 1-(10^{10}+9^{10} 1-\displaystyle\frac{(10^{10}+9^{10})}{10^{10}} 1-\displaystyle\frac{10!-10\times 9 \times \frac{10!}{2!}}{10^{10}},"['probability', 'combinatorics']"
11,"What's the probability of 2 credit card numbers matching, given incomplete data and known expiry date?","What's the probability of 2 credit card numbers matching, given incomplete data and known expiry date?",,"Trying to settle an office argument on a challenging probability scenario, thought I'd see if anyone would like to take a stab: We're trying to determine the probability of 2 credit cards matching amongst a global population given that only the first six and last four digits are known. We also know the expiry date. A couple of rules for each field: First six: We're working strictly with a single card issuer who's range of first six digits is between 222100-272099 or 510000-559999. For simplicity this allows for 100,000 possible combinations. Last four: These for simplicity can be assumed to be completely random. (ignoring Luhn checks) Expiry date: Only valid future dates within a four year time frame. What's the probability of 2 cards in the entire population being the same? So far our best theory by breaking this down into parts is: A = There are 100,000 combinations of the first 6 digits. B = There are 10,000 combinations of the last 4 digits C = There are 48 combinations of the date field. We are looking for the number of clashes in the population. For simplicity we've assumed a population of 1 billion. So we are assuming ultimately that there will be 1 billion events and are looking for the probability of getting 2 identical outcomes from those events: A * B * C = 1,000,000,000x 1/100,000 * 1/10,000 * 1/48 = 1,000,000,000x x = 1/48 So the theory is that there is a 1/48 chance of there being a single clash in 1bn cards.","Trying to settle an office argument on a challenging probability scenario, thought I'd see if anyone would like to take a stab: We're trying to determine the probability of 2 credit cards matching amongst a global population given that only the first six and last four digits are known. We also know the expiry date. A couple of rules for each field: First six: We're working strictly with a single card issuer who's range of first six digits is between 222100-272099 or 510000-559999. For simplicity this allows for 100,000 possible combinations. Last four: These for simplicity can be assumed to be completely random. (ignoring Luhn checks) Expiry date: Only valid future dates within a four year time frame. What's the probability of 2 cards in the entire population being the same? So far our best theory by breaking this down into parts is: A = There are 100,000 combinations of the first 6 digits. B = There are 10,000 combinations of the last 4 digits C = There are 48 combinations of the date field. We are looking for the number of clashes in the population. For simplicity we've assumed a population of 1 billion. So we are assuming ultimately that there will be 1 billion events and are looking for the probability of getting 2 identical outcomes from those events: A * B * C = 1,000,000,000x 1/100,000 * 1/10,000 * 1/48 = 1,000,000,000x x = 1/48 So the theory is that there is a 1/48 chance of there being a single clash in 1bn cards.",,['probability']
12,Probability that a sum of uniformly distributed random variables is large,Probability that a sum of uniformly distributed random variables is large,,"Problem Let $\ell_1 \le \ell_2 \le \dots \ell_n$ be nonnegative real numbers, and $S$ a nonnegative real number that is smaller than the sum of the $\ell_i$. Suppose that for $i = 1, 2, \dots, n$, a number $a_i$ is picked from the interval $[0, \ell_i]$ uniformly at random. What is the probability that $$a_1 + a_2 + \dots + a_n \ge S\text{ ?}$$ Progress If $S > \ell_2 + \ell_3 + \dots + \ell_n$, it seems that the answer is just $$\frac{\left(\ell_1 + \ell_2 + \dots + \ell_n - S \right)^n}{n!\cdot \ell_1\ell_2\cdots \ell_n}.$$ I got this by computing the volume of the associated region, which in this case forms a simplex. I'm not sure what the answer is in the general case however. If there isn't a nice closed form, I'd still like to find an algorithmic approach that could determine the answer quickly.","Problem Let $\ell_1 \le \ell_2 \le \dots \ell_n$ be nonnegative real numbers, and $S$ a nonnegative real number that is smaller than the sum of the $\ell_i$. Suppose that for $i = 1, 2, \dots, n$, a number $a_i$ is picked from the interval $[0, \ell_i]$ uniformly at random. What is the probability that $$a_1 + a_2 + \dots + a_n \ge S\text{ ?}$$ Progress If $S > \ell_2 + \ell_3 + \dots + \ell_n$, it seems that the answer is just $$\frac{\left(\ell_1 + \ell_2 + \dots + \ell_n - S \right)^n}{n!\cdot \ell_1\ell_2\cdots \ell_n}.$$ I got this by computing the volume of the associated region, which in this case forms a simplex. I'm not sure what the answer is in the general case however. If there isn't a nice closed form, I'd still like to find an algorithmic approach that could determine the answer quickly.",,"['probability', 'geometry', 'probability-theory', 'probability-distributions', 'volume']"
13,Average time waiting for bus,Average time waiting for bus,,"There are two buses: first arrives to the bus top every 8 minutes, and second arrives every 12 minutes. What is the average waiting time on a bus stop if we take whatever came first? It is expected that buses arrive at regular intervals, so first arrives every 8 minutes and second arrives every 12 minutes, but it is unknown when did they started. I was thinking in the following way: there is 1/3 probability that second bus arrives at 8-12 minutes, so it is after the first one. In this case probability is 1/3 * 4(average waiting time for the first bus). Then we have 2/3 probability that second bus came at 0-7 minutes, and basically we have two buses that arrive every 8 minutes. In this case I estimated average waiting time as 2 minutes(4 min average waiting time and 2 minutes because there are two 8 minutes buses now) and in this case answer is 2 * 2/3 + 4 * 1/3 = 8/3 = 2 + 2/3. But I'm not sure that in case of two buses there is actually 2 minutes waiting time and not some other number, and also I think that this is a ""standard"" problem that should have standard way of solving. Please guide me to the correct answer.","There are two buses: first arrives to the bus top every 8 minutes, and second arrives every 12 minutes. What is the average waiting time on a bus stop if we take whatever came first? It is expected that buses arrive at regular intervals, so first arrives every 8 minutes and second arrives every 12 minutes, but it is unknown when did they started. I was thinking in the following way: there is 1/3 probability that second bus arrives at 8-12 minutes, so it is after the first one. In this case probability is 1/3 * 4(average waiting time for the first bus). Then we have 2/3 probability that second bus came at 0-7 minutes, and basically we have two buses that arrive every 8 minutes. In this case I estimated average waiting time as 2 minutes(4 min average waiting time and 2 minutes because there are two 8 minutes buses now) and in this case answer is 2 * 2/3 + 4 * 1/3 = 8/3 = 2 + 2/3. But I'm not sure that in case of two buses there is actually 2 minutes waiting time and not some other number, and also I think that this is a ""standard"" problem that should have standard way of solving. Please guide me to the correct answer.",,['probability']
14,$E(X)>E(Y)$ always imply that $P[X>Y]>0$,always imply that,E(X)>E(Y) P[X>Y]>0,"Prove or disprove- ""If $E(X)>E(Y)$ then $P[X>Y]>0$ What I attempted:- I am using contradiction. Suppose $E(X)>E(Y)$. We assume that $P[X>Y]=0$ Now, we have  \begin{equation} \begin{aligned} &P[X>Y]=0\\ \Rightarrow & P[(X-Y)>0]=0 \\ \Rightarrow & P[Z>0]=0   \qquad \mbox{where}\quad Z=X-Y \end{aligned} \end{equation}  The last equation imply that \begin{equation} \begin{aligned} & Z\le 0 \\ \Rightarrow & E(Z)\le 0 \\ \Rightarrow & E(X-Y)\le 0 \\ \Rightarrow & E(X)\le E(Y)\\ \end{aligned} \end{equation} which is a contradiction. Therefore we must have $P[X>Y]>0$","Prove or disprove- ""If $E(X)>E(Y)$ then $P[X>Y]>0$ What I attempted:- I am using contradiction. Suppose $E(X)>E(Y)$. We assume that $P[X>Y]=0$ Now, we have  \begin{equation} \begin{aligned} &P[X>Y]=0\\ \Rightarrow & P[(X-Y)>0]=0 \\ \Rightarrow & P[Z>0]=0   \qquad \mbox{where}\quad Z=X-Y \end{aligned} \end{equation}  The last equation imply that \begin{equation} \begin{aligned} & Z\le 0 \\ \Rightarrow & E(Z)\le 0 \\ \Rightarrow & E(X-Y)\le 0 \\ \Rightarrow & E(X)\le E(Y)\\ \end{aligned} \end{equation} which is a contradiction. Therefore we must have $P[X>Y]>0$",,['probability']
15,How to solve Black Scholes equation directly without using probability,How to solve Black Scholes equation directly without using probability,,"Given constants $r,\sigma, K>0$, considering the Black-Scholes PDE of a Europoean call: \begin{cases} \frac{\partial c}{\partial t}+rs\frac{\partial c}{\partial s}+\frac{1}{2}\sigma^2s^2 \frac{\partial^2 c}{\partial s^2}-rc = 0\\ c(T,s) =(s-K)^+\\ c(t,0) = 0 \end{cases} I learned that we can do the substitution like this: \begin{cases} \tau=T-t\\ u=ce^{r\tau}\\ x=\ln\frac{s}{K}+(r-\frac{1}{2}\sigma^2)(T-t) \end{cases} Set $u=u(\tau,x)$, then $c(t,s)=e^{-r\tau}u(\tau,x)$. Then we have: \begin{gather*} \frac{\partial c}{\partial t} = re^{-r\tau}u+e^{-r\tau}(\frac{\partial u}{\partial \tau} \frac{d \tau}{dt} + \frac{\partial u}{\partial x} \frac{\partial x}{\partial t}) = e^{-r\tau}[ru-\frac{\partial u}{\partial \tau}- (r-\frac{1}{2}\sigma^2)\frac{\partial u}{\partial x}]\\ \frac{\partial c}{\partial s} = e^{-r\tau}\frac{\partial u}{\partial x} \frac{\partial x}{\partial s} = e^{-r\tau}\frac{1}{s}\frac{\partial u}{\partial x}\\ \frac{\partial^2 c}{\partial s^2} = e^{-r\tau}[-\frac{1}{s^2}\frac{\partial u}{\partial x}+\frac{1}{s^2}\frac{\partial^2 u}{\partial x^2}]=e^{-r\tau}\frac{1}{s^2}(\frac{\partial^2 u}{\partial x^2}-\frac{\partial u}{\partial x}) \end{gather*} Thus: \begin{align*} \frac{\partial c}{\partial t}+rs\frac{\partial c}{\partial s}+\frac{1}{2}\sigma^2s^2 \frac{\partial^2 c}{\partial s^2}-rc  &= e^{-r\tau}[-\frac{\partial u}{\partial \tau}- (r-\frac{1}{2}\sigma^2)\frac{\partial u}{\partial x} + r\frac{\partial u}{\partial x} + \frac{1}{2}\sigma^2(\frac{\partial^2 u}{\partial x^2}-\frac{\partial u}{\partial x})]\\ &= e^{-r\tau}[-\frac{\partial u}{\partial \tau} + \frac{1}{2}\sigma^2\frac{\partial^2 u}{\partial x^2}]\\ &=0 \end{align*} Namely we get the heat equation with boundary condition: \begin{cases} \frac{\partial u}{\partial \tau} = \frac{\sigma^2}{2}\frac{\partial^2 u}{\partial x^2}\\ u(0,x)=K(e^x - 1)^+\\ u(\tau, -\infty) = 0 \end{cases} I am stuck here. Since the boundary condition is not integrable, I cannot use Fourier transformation with regard to $x$ here. I know there is a probabilistic way of doing it, by computing the conditional expectation. I want to solve the PDE directly. After getting the heat equation here, what should I do next to solve it? Besides, is there any way to start directly from the PDE of $c$, without change of variable? And, how do we know that change of variable work here? It seems to me very coincidentally the PDE becomes a heat equation in the end. Thank you so much!","Given constants $r,\sigma, K>0$, considering the Black-Scholes PDE of a Europoean call: \begin{cases} \frac{\partial c}{\partial t}+rs\frac{\partial c}{\partial s}+\frac{1}{2}\sigma^2s^2 \frac{\partial^2 c}{\partial s^2}-rc = 0\\ c(T,s) =(s-K)^+\\ c(t,0) = 0 \end{cases} I learned that we can do the substitution like this: \begin{cases} \tau=T-t\\ u=ce^{r\tau}\\ x=\ln\frac{s}{K}+(r-\frac{1}{2}\sigma^2)(T-t) \end{cases} Set $u=u(\tau,x)$, then $c(t,s)=e^{-r\tau}u(\tau,x)$. Then we have: \begin{gather*} \frac{\partial c}{\partial t} = re^{-r\tau}u+e^{-r\tau}(\frac{\partial u}{\partial \tau} \frac{d \tau}{dt} + \frac{\partial u}{\partial x} \frac{\partial x}{\partial t}) = e^{-r\tau}[ru-\frac{\partial u}{\partial \tau}- (r-\frac{1}{2}\sigma^2)\frac{\partial u}{\partial x}]\\ \frac{\partial c}{\partial s} = e^{-r\tau}\frac{\partial u}{\partial x} \frac{\partial x}{\partial s} = e^{-r\tau}\frac{1}{s}\frac{\partial u}{\partial x}\\ \frac{\partial^2 c}{\partial s^2} = e^{-r\tau}[-\frac{1}{s^2}\frac{\partial u}{\partial x}+\frac{1}{s^2}\frac{\partial^2 u}{\partial x^2}]=e^{-r\tau}\frac{1}{s^2}(\frac{\partial^2 u}{\partial x^2}-\frac{\partial u}{\partial x}) \end{gather*} Thus: \begin{align*} \frac{\partial c}{\partial t}+rs\frac{\partial c}{\partial s}+\frac{1}{2}\sigma^2s^2 \frac{\partial^2 c}{\partial s^2}-rc  &= e^{-r\tau}[-\frac{\partial u}{\partial \tau}- (r-\frac{1}{2}\sigma^2)\frac{\partial u}{\partial x} + r\frac{\partial u}{\partial x} + \frac{1}{2}\sigma^2(\frac{\partial^2 u}{\partial x^2}-\frac{\partial u}{\partial x})]\\ &= e^{-r\tau}[-\frac{\partial u}{\partial \tau} + \frac{1}{2}\sigma^2\frac{\partial^2 u}{\partial x^2}]\\ &=0 \end{align*} Namely we get the heat equation with boundary condition: \begin{cases} \frac{\partial u}{\partial \tau} = \frac{\sigma^2}{2}\frac{\partial^2 u}{\partial x^2}\\ u(0,x)=K(e^x - 1)^+\\ u(\tau, -\infty) = 0 \end{cases} I am stuck here. Since the boundary condition is not integrable, I cannot use Fourier transformation with regard to $x$ here. I know there is a probabilistic way of doing it, by computing the conditional expectation. I want to solve the PDE directly. After getting the heat equation here, what should I do next to solve it? Besides, is there any way to start directly from the PDE of $c$, without change of variable? And, how do we know that change of variable work here? It seems to me very coincidentally the PDE becomes a heat equation in the end. Thank you so much!",,"['probability', 'partial-differential-equations', 'finance', 'heat-equation']"
16,"After $10$ inspections with no defect, whats the prob of that number of inspection is no more than $20$?","After  inspections with no defect, whats the prob of that number of inspection is no more than ?",10 20,"I have a problem which I'm not sure how to solve. It goes as follows: A production line has a $5$% defective rate, and its products are inspected one by one until the first defect is found. Given that the first $10$ inspections do not find any defect, what is the probability that the number of inspections is no more than $20$? I tried the following approach: Given that this is a Bernoulli process, I thought that if there were no defective parts encountered in the first 10 inspected products, the next 10 products (till $X_{20}$) will be independent from the past, which means a new Bernoulli process of 10 trials. So, if $S$ is the sum of each $X_i$, then I could calculate its PMF with parameters $p=.05$ and $t=10$, where $t$ is the number of trials: $p_s(1) = \binom{10}{1}(0.05)(0.95)^9$ $p_s(1) = .3151$ Yet this is wrong. I'm not sure if I had to use a conditional approach (because the second round of inspection occurs in the conditional universe where there were no defects in the first $10$) so as to say that: Event A : one is defective in $11$ through $20$ Event B : none is defective in the first $10$ $P(A \cap B) = P(A) P(B)$ In which case: $P(A) = p_s(1) = \binom{10}{1}(0.05)(0.95)^9$ $P(B) = p_s(0) = \binom{10}{0}(0.95)^{10}$","I have a problem which I'm not sure how to solve. It goes as follows: A production line has a $5$% defective rate, and its products are inspected one by one until the first defect is found. Given that the first $10$ inspections do not find any defect, what is the probability that the number of inspections is no more than $20$? I tried the following approach: Given that this is a Bernoulli process, I thought that if there were no defective parts encountered in the first 10 inspected products, the next 10 products (till $X_{20}$) will be independent from the past, which means a new Bernoulli process of 10 trials. So, if $S$ is the sum of each $X_i$, then I could calculate its PMF with parameters $p=.05$ and $t=10$, where $t$ is the number of trials: $p_s(1) = \binom{10}{1}(0.05)(0.95)^9$ $p_s(1) = .3151$ Yet this is wrong. I'm not sure if I had to use a conditional approach (because the second round of inspection occurs in the conditional universe where there were no defects in the first $10$) so as to say that: Event A : one is defective in $11$ through $20$ Event B : none is defective in the first $10$ $P(A \cap B) = P(A) P(B)$ In which case: $P(A) = p_s(1) = \binom{10}{1}(0.05)(0.95)^9$ $P(B) = p_s(0) = \binom{10}{0}(0.95)^{10}$",,['probability']
17,Complement Event,Complement Event,,"Four people are chosen at random. What is the probability that: (a) One of the first three people chosen has their birthday in the same month as the fourth person? The solution given for this question is just the complement of $P(\text{None of the first three in same month as 4th})= 1- (\frac {11}{12})^3 $ I feel that the answer is wrong here because the complement of ""None of the first three in same month as 4th"" doesn't equal to ""exactly one of the first three people chosen has their birth day in the same month"" but rather the complement means that not all of the first three is in the same month as 4th? Therefore, shouldn't the answer be $\binom {3}{1} \frac {1}{12} \cdot\ \frac {11}{12} \cdot\ \frac {11}{12}$ (b) The first and second people chosen have their birthdays in the same month, given that there is some pair of people having their birthdays in the same month? For (b) the answer given was P(first and second in same month)/P(some pair in same month) In particular, the P(some pair in same month) I get that it's easier to use rule of complement to get the answer: $1-(\frac {12}{12} \frac {11}{12}\frac {10}{12}\frac {9}{12} ) $ However, how come my answer here is wrong: $\binom {4}{2} \frac {12}{12} \frac {1}{12} \frac {11}{12} \frac {10}{12}$ My rationale is, there are $4$ items in total, we are picking $2$ at a time to make them a pair thus the combination should be $4C2$. The next step is to figure out the probability. Assume that the 1st person can be any of the month $\frac {12}{12}$. Assuming the 2nd person share the same birthday as the 1st then it should be $\frac {1}{12}$. This completes the first pair therefore, the 3rd person has a probability of $\frac {11}{12}$ and subsequently the 4th person's probability is $\frac {10}{12}$ What is wrong with my reasoning here? Many thanks in advance!","Four people are chosen at random. What is the probability that: (a) One of the first three people chosen has their birthday in the same month as the fourth person? The solution given for this question is just the complement of $P(\text{None of the first three in same month as 4th})= 1- (\frac {11}{12})^3 $ I feel that the answer is wrong here because the complement of ""None of the first three in same month as 4th"" doesn't equal to ""exactly one of the first three people chosen has their birth day in the same month"" but rather the complement means that not all of the first three is in the same month as 4th? Therefore, shouldn't the answer be $\binom {3}{1} \frac {1}{12} \cdot\ \frac {11}{12} \cdot\ \frac {11}{12}$ (b) The first and second people chosen have their birthdays in the same month, given that there is some pair of people having their birthdays in the same month? For (b) the answer given was P(first and second in same month)/P(some pair in same month) In particular, the P(some pair in same month) I get that it's easier to use rule of complement to get the answer: $1-(\frac {12}{12} \frac {11}{12}\frac {10}{12}\frac {9}{12} ) $ However, how come my answer here is wrong: $\binom {4}{2} \frac {12}{12} \frac {1}{12} \frac {11}{12} \frac {10}{12}$ My rationale is, there are $4$ items in total, we are picking $2$ at a time to make them a pair thus the combination should be $4C2$. The next step is to figure out the probability. Assume that the 1st person can be any of the month $\frac {12}{12}$. Assuming the 2nd person share the same birthday as the 1st then it should be $\frac {1}{12}$. This completes the first pair therefore, the 3rd person has a probability of $\frac {11}{12}$ and subsequently the 4th person's probability is $\frac {10}{12}$ What is wrong with my reasoning here? Many thanks in advance!",,"['probability', 'combinatorics']"
18,random variable takes only rational values with probability one,random variable takes only rational values with probability one,,"I have found an old exercise that seems very interesting: let $X_{1},X_{2},...$ be i.i.d. bernoullian random variables with $\mathbb{P}(X_{n}=1) = \mathbb{P}(X_{n}=0) = 1/2$. Define $S_{n} := X_{1} + ... + X_{n}$. It is to show that the random variable $$ M := \sup_\limits{n \in \mathbb{N}}\frac{S_{n}}{n} $$ with probability $1$ only takes rational values in the intervall $(1/2,1]$. Anyone has an idea, how to prove it?","I have found an old exercise that seems very interesting: let $X_{1},X_{2},...$ be i.i.d. bernoullian random variables with $\mathbb{P}(X_{n}=1) = \mathbb{P}(X_{n}=0) = 1/2$. Define $S_{n} := X_{1} + ... + X_{n}$. It is to show that the random variable $$ M := \sup_\limits{n \in \mathbb{N}}\frac{S_{n}}{n} $$ with probability $1$ only takes rational values in the intervall $(1/2,1]$. Anyone has an idea, how to prove it?",,['probability']
19,How to distinguish trial and experiment?,How to distinguish trial and experiment?,,"How to distinguish trial and experiment in probability? I have checked Ross's definition and wikipedia's intro on the definition of them for a while, but not quite get it till now. What is the difference of them? And when it comes to Bernoulli trail or Bernoulli experiment , do we call Bernoulli trial, or Bernoulli trials, or Bernoulli experiment, or Bernoulli experiments?","How to distinguish trial and experiment in probability? I have checked Ross's definition and wikipedia's intro on the definition of them for a while, but not quite get it till now. What is the difference of them? And when it comes to Bernoulli trail or Bernoulli experiment , do we call Bernoulli trial, or Bernoulli trials, or Bernoulli experiment, or Bernoulli experiments?",,"['probability', 'random-variables']"
20,Probability of Following Winning Player,Probability of Following Winning Player,,"Let's say we're playing a game with a total of $n$ people. In each round of the game, two random players $A$ and $B$ are selected. One of $A$ or $B$ is randomly chosen to be the winner, and the other the loser. Without loss of generality, let us assume $A$ wins. Then, $B$ is eliminated from the pool of $n$ contestants, and denoted as a 'follower' of $A$. Furthermore, any previous followers of $B$ also become followers of $A$. $A$ is re-entered into the pool of contestants, and this process is repeated until one person is left and wins the game. Clearly, this last person remaining will have everyone following him. My question is: what is the probability that a player $X$, randomly selected at the beginning of the game, ends up following the overall winner $Y$, before $Y$ wins? I'll say any player is definitionally following themselves at the beginning of the game, to formalize the problem. I think the answer is $\frac{1}{2}$ by symmetry, but this also seems somewhat counter-intuitive. If you're curious, the above question is motivated by the game Fortnite; when another player kills you, you can then watch them play. If another player kills the player who killed you, you then watch the new winner play.","Let's say we're playing a game with a total of $n$ people. In each round of the game, two random players $A$ and $B$ are selected. One of $A$ or $B$ is randomly chosen to be the winner, and the other the loser. Without loss of generality, let us assume $A$ wins. Then, $B$ is eliminated from the pool of $n$ contestants, and denoted as a 'follower' of $A$. Furthermore, any previous followers of $B$ also become followers of $A$. $A$ is re-entered into the pool of contestants, and this process is repeated until one person is left and wins the game. Clearly, this last person remaining will have everyone following him. My question is: what is the probability that a player $X$, randomly selected at the beginning of the game, ends up following the overall winner $Y$, before $Y$ wins? I'll say any player is definitionally following themselves at the beginning of the game, to formalize the problem. I think the answer is $\frac{1}{2}$ by symmetry, but this also seems somewhat counter-intuitive. If you're curious, the above question is motivated by the game Fortnite; when another player kills you, you can then watch them play. If another player kills the player who killed you, you then watch the new winner play.",,"['probability', 'induction']"
21,How can I compute $\mathbb P\{T_n<T_0\}$?,How can I compute ?,\mathbb P\{T_n<T_0\},"Let $(X_n)_{n}$ a random walk over $\mathbb Z$ starting at $0$, i.e. $\mathbb P\{X_0=0\}=1$. I denote $T_k=\inf\{n\geq 1\mid X_n=k\}$. I suppose that $$\mathbb P\{X_{n+1}=X_n+1\mid X_n,...,X_0\}=p\quad \text{and}\quad \mathbb P\{X_{n+1}=X_n-1\mid X_n,...,X_0\}=q.$$ Remark that $q=1-p$. How can I compute $\mathbb P\{T_n<T_0\}$, i.e. the probability to touch $n$ before touching $0$ ? In fact I have problem to interpret $\{T_n<T_0\}$ using $(X_n)_n$. I tired as follow : $\mathbb P\{T_1<T_0\}=\mathbb P\{X_1=1\mid X_0=0\}=p$. $\mathbb P\{T_2<T_0\}=\mathbb P\{X_1=1\mid X_0=0\}+\mathbb P\{X_2=2\mid X_1=1\}=2p$ $\mathbb P\{T_3<T_0\}=\mathbb P\{X_1=1\mid X_0=0\}+\mathbb P\{X_2=2\mid X_1=1\}+\mathbb P\{X_3=3\mid X_2=2\}+\mathbb P\{X_3=2\mid X_2=2\}(\mathbb P\{X_4=3\mid X_3=2\}+\mathbb P\{X_4=1\mid X_3=2\})$ I know that the last one is not clear at all, but I don't know how to interpret that fact that the walker can past many time between 1 and 2 many times before arriving at $3$. Any explanation would be appreciated.","Let $(X_n)_{n}$ a random walk over $\mathbb Z$ starting at $0$, i.e. $\mathbb P\{X_0=0\}=1$. I denote $T_k=\inf\{n\geq 1\mid X_n=k\}$. I suppose that $$\mathbb P\{X_{n+1}=X_n+1\mid X_n,...,X_0\}=p\quad \text{and}\quad \mathbb P\{X_{n+1}=X_n-1\mid X_n,...,X_0\}=q.$$ Remark that $q=1-p$. How can I compute $\mathbb P\{T_n<T_0\}$, i.e. the probability to touch $n$ before touching $0$ ? In fact I have problem to interpret $\{T_n<T_0\}$ using $(X_n)_n$. I tired as follow : $\mathbb P\{T_1<T_0\}=\mathbb P\{X_1=1\mid X_0=0\}=p$. $\mathbb P\{T_2<T_0\}=\mathbb P\{X_1=1\mid X_0=0\}+\mathbb P\{X_2=2\mid X_1=1\}=2p$ $\mathbb P\{T_3<T_0\}=\mathbb P\{X_1=1\mid X_0=0\}+\mathbb P\{X_2=2\mid X_1=1\}+\mathbb P\{X_3=3\mid X_2=2\}+\mathbb P\{X_3=2\mid X_2=2\}(\mathbb P\{X_4=3\mid X_3=2\}+\mathbb P\{X_4=1\mid X_3=2\})$ I know that the last one is not clear at all, but I don't know how to interpret that fact that the walker can past many time between 1 and 2 many times before arriving at $3$. Any explanation would be appreciated.",,"['probability', 'probability-theory', 'random-walk']"
22,Uniform distribution on the unit circle,Uniform distribution on the unit circle,,"Determine the probability density function on the unit circle $U:=\{(x,y)\in\mathbb{R}^2:x^2+y^2 =1\}$ with respect to the Lebesgue measure $\lambda$. Calculate the marginal distributions $f_X$ and $f_Y$ if the random positions on the unit circle in the euclidian coordinate system are interpreted as a two-dimensional random variable $(X,Y)$. Now, if we set $(x,y) = (\cos\theta,\sin\theta)$, the probability density function would be $f(\theta) = \frac{1}{2\pi}$. But I cannot figure out how to determine the marginal distributions then, as this is only a one-dimensional object and with respect to $\lambda$, not $\lambda^2$.","Determine the probability density function on the unit circle $U:=\{(x,y)\in\mathbb{R}^2:x^2+y^2 =1\}$ with respect to the Lebesgue measure $\lambda$. Calculate the marginal distributions $f_X$ and $f_Y$ if the random positions on the unit circle in the euclidian coordinate system are interpreted as a two-dimensional random variable $(X,Y)$. Now, if we set $(x,y) = (\cos\theta,\sin\theta)$, the probability density function would be $f(\theta) = \frac{1}{2\pi}$. But I cannot figure out how to determine the marginal distributions then, as this is only a one-dimensional object and with respect to $\lambda$, not $\lambda^2$.",,"['probability', 'probability-theory', 'uniform-distribution', 'marginal-distribution']"
23,Value at Risk: Coherent risk measure for normal distribution,Value at Risk: Coherent risk measure for normal distribution,,"I know that there are cases where VaR does not satisfy the subadditivity property ( coherent risk measure properties ) for coherent risk measures. But I would like to show that in the case of normal distributions, VaR does satisfy this property and therefore is a coherent risk measure. However, I couldn't find a proof for this question so I wonder how this question could be approached. Thank you!","I know that there are cases where VaR does not satisfy the subadditivity property ( coherent risk measure properties ) for coherent risk measures. But I would like to show that in the case of normal distributions, VaR does satisfy this property and therefore is a coherent risk measure. However, I couldn't find a proof for this question so I wonder how this question could be approached. Thank you!",,"['probability', 'probability-distributions', 'normal-distribution', 'actuarial-science', 'distribution-tails']"
24,Finding the expected value of martingale increments on conditioned on brownian bridge,Finding the expected value of martingale increments on conditioned on brownian bridge,,"I want to show that $\mathbb{E}[W_t - W_s | \mathcal{G}_s] = \frac{t-s}{1-s}(W_1 - W_s)$ for $0 \leq s \leq t$, where $W_t$ is the Weiner process, and $\mathcal{G}_s = \sigma(\mathcal{F}_t^W, \sigma(W_1))$, where $\mathcal{F}_t^W$ is the filtration of $W_t$. This is driving me crazy, I just keep getting the wrong answer. Using the linearity of expectation, we have $\mathbb{E}[W_t - W_s | \mathcal{G}_s]   = \mathbb{E}[W_t| \mathcal{G}_s] - \mathbb{E}[W_s| \mathcal{G}_s]$.  It seems to me that since $\mathcal{F}_s^W \subset \mathcal{G}_s$, we should have $\mathbb{E}[W_s| \mathcal{G}_s] = W_s$ (is this part wrong?) Now we are left with $\mathbb{E}[W_t| \mathcal{G}_s]$. I already know from this post: conditional expected value of a brownian motion that $\mathbb{E}[W_t|W_s] = \frac{t}{s}W_t$ for $0 <t <s$, so since $\sigma(W_1) \subset \mathcal{G}_s$, we have $\mathbb{E}[W_t| \mathcal{G}_s] = tW_1$. But then this gives $\mathbb{E}[W_t - W_s | \mathcal{G}_s] = tW_1 - W_s$, which is wrong. How do I properly deal with the fact that the sigma algebra gives us information about the future and past? Am I supposed to somehow use the fact that the question is only asking about the difference? Hints would be appreciated! Thanks very much in advance.","I want to show that $\mathbb{E}[W_t - W_s | \mathcal{G}_s] = \frac{t-s}{1-s}(W_1 - W_s)$ for $0 \leq s \leq t$, where $W_t$ is the Weiner process, and $\mathcal{G}_s = \sigma(\mathcal{F}_t^W, \sigma(W_1))$, where $\mathcal{F}_t^W$ is the filtration of $W_t$. This is driving me crazy, I just keep getting the wrong answer. Using the linearity of expectation, we have $\mathbb{E}[W_t - W_s | \mathcal{G}_s]   = \mathbb{E}[W_t| \mathcal{G}_s] - \mathbb{E}[W_s| \mathcal{G}_s]$.  It seems to me that since $\mathcal{F}_s^W \subset \mathcal{G}_s$, we should have $\mathbb{E}[W_s| \mathcal{G}_s] = W_s$ (is this part wrong?) Now we are left with $\mathbb{E}[W_t| \mathcal{G}_s]$. I already know from this post: conditional expected value of a brownian motion that $\mathbb{E}[W_t|W_s] = \frac{t}{s}W_t$ for $0 <t <s$, so since $\sigma(W_1) \subset \mathcal{G}_s$, we have $\mathbb{E}[W_t| \mathcal{G}_s] = tW_1$. But then this gives $\mathbb{E}[W_t - W_s | \mathcal{G}_s] = tW_1 - W_s$, which is wrong. How do I properly deal with the fact that the sigma algebra gives us information about the future and past? Am I supposed to somehow use the fact that the question is only asking about the difference? Hints would be appreciated! Thanks very much in advance.",,"['probability', 'stochastic-processes', 'brownian-motion', 'conditional-expectation']"
25,Convergence a.s. $\Longleftrightarrow$ Convergence in P,Convergence a.s.  Convergence in P,\Longleftrightarrow,"Let $\{Y_n\}'s$ be independent r.v.'s, $S_n = \sum^n_{i=1}Y_i$, a. Prove that for arbitrary $\epsilon >0$, $$P\left(\sup_{\{k\geq1\}}|S_k|>4 \epsilon\right) \leq 4\sup_{\{k\geq1\}}P(|S_k|>\epsilon)$$    and for each integer $n$, $\epsilon >0$,    $$P\left(\sup_{\{n\geq k\geq1\}}|S_k|>4 \epsilon\right) \leq 4\sup_{\{n\geq     k\geq1\}}P(|S_k|>\epsilon).$$ b. Use (a) to prove $\sum^{\infty}_{i=1}Y_i$ convergence in probability implies it convergence almost surely. I am very confused about how to solve this question, by my intuition, convergence in probability dose not implies convergence almost surely.","Let $\{Y_n\}'s$ be independent r.v.'s, $S_n = \sum^n_{i=1}Y_i$, a. Prove that for arbitrary $\epsilon >0$, $$P\left(\sup_{\{k\geq1\}}|S_k|>4 \epsilon\right) \leq 4\sup_{\{k\geq1\}}P(|S_k|>\epsilon)$$    and for each integer $n$, $\epsilon >0$,    $$P\left(\sup_{\{n\geq k\geq1\}}|S_k|>4 \epsilon\right) \leq 4\sup_{\{n\geq     k\geq1\}}P(|S_k|>\epsilon).$$ b. Use (a) to prove $\sum^{\infty}_{i=1}Y_i$ convergence in probability implies it convergence almost surely. I am very confused about how to solve this question, by my intuition, convergence in probability dose not implies convergence almost surely.",,"['real-analysis', 'probability', 'probability-theory']"
26,$\mathrm{E}[e^{u X}|\mathcal{A}] = \mathrm{E}[e^{u X}|\mathcal{B}]$ implies equality of conditional distributions,implies equality of conditional distributions,\mathrm{E}[e^{u X}|\mathcal{A}] = \mathrm{E}[e^{u X}|\mathcal{B}],"Let $X$ be a random variable on the probability space $(\Omega, \mathcal{F}, P)$ and $\mathcal{A} \subset \mathcal{B} \subset \mathcal{F}$ be a $\sigma$-subalgebras. I want to prove that if $$ \mathrm{E}[e^{u X}|\mathcal{A}] = \mathrm{E}[e^{u X}|\mathcal{B}] $$ holds for any $u \in \mathbb{C}$ then $P(X\in \Gamma|\mathcal{A}) = P(X\in \Gamma|\mathcal{B})$ for any borel $\Gamma$. Any hints?","Let $X$ be a random variable on the probability space $(\Omega, \mathcal{F}, P)$ and $\mathcal{A} \subset \mathcal{B} \subset \mathcal{F}$ be a $\sigma$-subalgebras. I want to prove that if $$ \mathrm{E}[e^{u X}|\mathcal{A}] = \mathrm{E}[e^{u X}|\mathcal{B}] $$ holds for any $u \in \mathbb{C}$ then $P(X\in \Gamma|\mathcal{A}) = P(X\in \Gamma|\mathcal{B})$ for any borel $\Gamma$. Any hints?",,"['probability', 'probability-theory', 'conditional-expectation', 'moment-generating-functions']"
27,"Is a distribution part of the exponential family if the support depends on the parameter? (namely, $f_\theta (x)=e^{-(x-3\theta)}, 3\theta<x<\infty$)","Is a distribution part of the exponential family if the support depends on the parameter? (namely, )","f_\theta (x)=e^{-(x-3\theta)}, 3\theta<x<\infty","I have trouble verifying is the distribution $$f_\theta (x)=e^{-(x-3\theta)}\qquad\qquad for \ \ 3\theta<x<\infty$$ Is part of the exponential family. In particular, one can rewrite the above as: $$f_\theta (x)=e^{-(x-3\theta)}I_{(3\theta,\infty)}(x)$$ If one would like to write $f_\theta(x)$ as $$f_\theta(x)=c(\theta)h(x)e^{\sum_{j=1}^{p}q_j(\theta)T_j(x)}$$ One should manipulate $I_{(3\theta,\infty)}(x)$ as follows $$I_{(3\theta,\infty)}(x)=e^{log(I_{(3\theta,\infty)}(x))}$$ Now sustituting $$f_\theta(x)=e^{-(x-3\theta)}e^{log(I_{(\theta,\infty)}(x))}=e^{-x+3\theta+log(I_{(3\theta,\infty)}(x))}$$ Now, I find it quite dificult to factorize the function $log(I_{(3\theta,\infty)}(x))$ as a product of two functions that depend only on $x$ and $\theta$ respectively. That may indicate that its not an exponential-family-type distribution; however, I don't know how to prove that I am not smart enough to see it. Thanks in advance for your collaboration.","I have trouble verifying is the distribution $$f_\theta (x)=e^{-(x-3\theta)}\qquad\qquad for \ \ 3\theta<x<\infty$$ Is part of the exponential family. In particular, one can rewrite the above as: $$f_\theta (x)=e^{-(x-3\theta)}I_{(3\theta,\infty)}(x)$$ If one would like to write $f_\theta(x)$ as $$f_\theta(x)=c(\theta)h(x)e^{\sum_{j=1}^{p}q_j(\theta)T_j(x)}$$ One should manipulate $I_{(3\theta,\infty)}(x)$ as follows $$I_{(3\theta,\infty)}(x)=e^{log(I_{(3\theta,\infty)}(x))}$$ Now sustituting $$f_\theta(x)=e^{-(x-3\theta)}e^{log(I_{(\theta,\infty)}(x))}=e^{-x+3\theta+log(I_{(3\theta,\infty)}(x))}$$ Now, I find it quite dificult to factorize the function $log(I_{(3\theta,\infty)}(x))$ as a product of two functions that depend only on $x$ and $\theta$ respectively. That may indicate that its not an exponential-family-type distribution; however, I don't know how to prove that I am not smart enough to see it. Thanks in advance for your collaboration.",,"['probability', 'probability-theory', 'statistics', 'probability-distributions', 'statistical-inference']"
28,3 cards are dealt from a well shuffled deck.,3 cards are dealt from a well shuffled deck.,,"1. Find the chance that none of the cards are hearts. The answer is $\frac{39}{52}$ $\cdot$ $\frac{38}{51}$ $\cdot$  $\frac{37}{50}$ However, why can't we use complement rule here: 1- p(chance that all the cards are hearts)= 1- ($\frac{13}{52}$ $\cdot$ $\frac{12}{51}$ $\cdot$ $\frac{11}{50}$) 2. Find the chance that the cards are not all hearts. The answer given is: 1- ($\frac{13}{52}$ $\cdot$ $\frac{12}{51}$ $\cdot$ $\frac{11}{50}$) here I'm little confused as to why the complement rule was used. for chance that the cards are not all hearts why can't we use: P(1 heart) + P(2 heart) + P(No hearts)= ($\frac{13}{52}$ $\cdot$ $\frac{39}{51}$ $\cdot$  $\frac{38}{50}$) + ($\frac{13}{52}$ $\cdot$ $\frac{12}{51}$ $\cdot$  $\frac{39}{50}$) + ($\frac{39}{52}$ $\cdot$ $\frac{38}{51}$ $\cdot$  $\frac{37}{50}$)","1. Find the chance that none of the cards are hearts. The answer is $\frac{39}{52}$ $\cdot$ $\frac{38}{51}$ $\cdot$  $\frac{37}{50}$ However, why can't we use complement rule here: 1- p(chance that all the cards are hearts)= 1- ($\frac{13}{52}$ $\cdot$ $\frac{12}{51}$ $\cdot$ $\frac{11}{50}$) 2. Find the chance that the cards are not all hearts. The answer given is: 1- ($\frac{13}{52}$ $\cdot$ $\frac{12}{51}$ $\cdot$ $\frac{11}{50}$) here I'm little confused as to why the complement rule was used. for chance that the cards are not all hearts why can't we use: P(1 heart) + P(2 heart) + P(No hearts)= ($\frac{13}{52}$ $\cdot$ $\frac{39}{51}$ $\cdot$  $\frac{38}{50}$) + ($\frac{13}{52}$ $\cdot$ $\frac{12}{51}$ $\cdot$  $\frac{39}{50}$) + ($\frac{39}{52}$ $\cdot$ $\frac{38}{51}$ $\cdot$  $\frac{37}{50}$)",,['probability']
29,Count the probability of a random $20$ bit string,Count the probability of a random  bit string,20,"Consider binary words of length $20$. They are formed in such a way that a $1$ occurs with probability $0.6$, and a $0$ does with probability $0.4$ (diﬀerent places are independent). Also, a “run” is a maximal substring of only ones or only zeros; e.g., $00110001101011111111$ has $8$ runs. I think this question should be solved using Linearity of Expectation from Discrete Math II but It is kind of hard for me to make the connections between probability and counting theories. 1) What is the probability that a random bit string of length $20$ starts in a $0$, has exactly $8$ ones, and has exactly $10$ runs? I worked out that the probability of ""a random bit string of length $20$ starts in a $0$, has exactly $8$ ones"" is equal to $(0.6^8)\cdot(0.4^{11})$ because it starts with $0$ and then there're $19$ bits left. Then I want to use stars and bars theory to count the number of ways we can arrange $8$ ones and $11$ zeros to make it a ""$10$ run"" string. It is like choosing $4$ walls of ones to separate $19$ zeros. I am stuck here because I don't know how to make the connection between runs and the possibility of $o$'s and $1$'s ($0.4/0.6$). 2) What is the probability that a random bit string of length $20$ has exactly $10$ runs? Same idea here, to use the stars and bars theory to separate $4-16$ $1$'s or $0$'s using $4-16$ $0$'s or $1$'s. Ah, I just don't know how to start from here...","Consider binary words of length $20$. They are formed in such a way that a $1$ occurs with probability $0.6$, and a $0$ does with probability $0.4$ (diﬀerent places are independent). Also, a “run” is a maximal substring of only ones or only zeros; e.g., $00110001101011111111$ has $8$ runs. I think this question should be solved using Linearity of Expectation from Discrete Math II but It is kind of hard for me to make the connections between probability and counting theories. 1) What is the probability that a random bit string of length $20$ starts in a $0$, has exactly $8$ ones, and has exactly $10$ runs? I worked out that the probability of ""a random bit string of length $20$ starts in a $0$, has exactly $8$ ones"" is equal to $(0.6^8)\cdot(0.4^{11})$ because it starts with $0$ and then there're $19$ bits left. Then I want to use stars and bars theory to count the number of ways we can arrange $8$ ones and $11$ zeros to make it a ""$10$ run"" string. It is like choosing $4$ walls of ones to separate $19$ zeros. I am stuck here because I don't know how to make the connection between runs and the possibility of $o$'s and $1$'s ($0.4/0.6$). 2) What is the probability that a random bit string of length $20$ has exactly $10$ runs? Same idea here, to use the stars and bars theory to separate $4-16$ $1$'s or $0$'s using $4-16$ $0$'s or $1$'s. Ah, I just don't know how to start from here...",,"['probability', 'combinatorics']"
30,Voting score paradox,Voting score paradox,,"These is a voting system, where a user can place one of the three types of votes: $NEG, NEUT, POS$ Let's call a voting configuration ( VC for short) total number of NEG, NEUT and POS votes left by the users. (E.g. $N_{neg} = 5, N_{neut} = 10, N_{pos} = 1.$) Let's call a score of the voting system a rational number defined as following: $$score = \frac{(-N_{neg} + N_{pos})}{(N_{neg} + N_{neut} + N_{pos})}$$ In the above example score will be: $$score = \frac{(-5 + 1)}{(5 + 10 + 1)}=-\frac{1}{4}$$ Let's call a random voting configuration the one, where number of votes $N_{neg}, N_{neut}, N_{pos}$ is taken from the uniform distribution on $[0, +\infty)$. The overall decision for the VC is defined as follows: $$decision(score) = \begin{cases} NEG, & \mbox{if } score \in [-1, -T] \\ NEUT, & \mbox{if } score \in (-T, T) \\ POS , & \mbox{if } score \in [T, 1] \end{cases}$$ Q: How to choose T so that a randomly picked VC falls into each of the three types with equal probability? P.S. From the practical point of view the task is to choose such a threshold that classes for $NEG, NEUT, POS$ are balanced in terms of how many VCs do fall into them. My interest to this task came up because the first obvious choice of $T=1/3$ leads to the heavily overloaded $NEUT$ class, when analyzing all VCs with number of votes between 10 and 25 (both inclusive): $|VC_{neg}| = 746$ $|VC_{neut}| = 1564$ $|VC_{pos}| = 746$","These is a voting system, where a user can place one of the three types of votes: $NEG, NEUT, POS$ Let's call a voting configuration ( VC for short) total number of NEG, NEUT and POS votes left by the users. (E.g. $N_{neg} = 5, N_{neut} = 10, N_{pos} = 1.$) Let's call a score of the voting system a rational number defined as following: $$score = \frac{(-N_{neg} + N_{pos})}{(N_{neg} + N_{neut} + N_{pos})}$$ In the above example score will be: $$score = \frac{(-5 + 1)}{(5 + 10 + 1)}=-\frac{1}{4}$$ Let's call a random voting configuration the one, where number of votes $N_{neg}, N_{neut}, N_{pos}$ is taken from the uniform distribution on $[0, +\infty)$. The overall decision for the VC is defined as follows: $$decision(score) = \begin{cases} NEG, & \mbox{if } score \in [-1, -T] \\ NEUT, & \mbox{if } score \in (-T, T) \\ POS , & \mbox{if } score \in [T, 1] \end{cases}$$ Q: How to choose T so that a randomly picked VC falls into each of the three types with equal probability? P.S. From the practical point of view the task is to choose such a threshold that classes for $NEG, NEUT, POS$ are balanced in terms of how many VCs do fall into them. My interest to this task came up because the first obvious choice of $T=1/3$ leads to the heavily overloaded $NEUT$ class, when analyzing all VCs with number of votes between 10 and 25 (both inclusive): $|VC_{neg}| = 746$ $|VC_{neut}| = 1564$ $|VC_{pos}| = 746$",,['probability']
31,probability that subsets are successively contained,probability that subsets are successively contained,,"The set A has $n$ elements, and so has $2^n$ subsets.  The subsets are placed into an urn, and $m$ subsets $B_1, \dots, B_m$ are drawn in order at random with replacement from the urn.  (Each subset has probability $\frac{1}{2^n}$.)  What is the probability that $$B_1 \subseteq B_2 \subseteq \dots \subseteq B_m?$$ I'm not really sure how to approach this problem; the only thing I can think of is a counting argument of some sort, but it seems like that would involve a lot of casework about the sizes of the sets.","The set A has $n$ elements, and so has $2^n$ subsets.  The subsets are placed into an urn, and $m$ subsets $B_1, \dots, B_m$ are drawn in order at random with replacement from the urn.  (Each subset has probability $\frac{1}{2^n}$.)  What is the probability that $$B_1 \subseteq B_2 \subseteq \dots \subseteq B_m?$$ I'm not really sure how to approach this problem; the only thing I can think of is a counting argument of some sort, but it seems like that would involve a lot of casework about the sizes of the sets.",,['probability']
32,Why does fair random process lead to unfair result?,Why does fair random process lead to unfair result?,,"Suppose there are 100 people, each one has \$100. Now repeatedly do following: Random choose two people A and B (random means uniform and independent); If A has money, A gives \$1 to B; If A has no money, do nothing; After doing many times, what is the distribution of money among these people? I guess it should be something like uniform distributed, but it is not. Could you tell me why? Why do some people get so much money from others in this fair process? Here is the simulation: n = 100; a = Table[100, {i, 1, n}]; f[a_] := With[{i = RandomInteger[{1, n}], j = RandomInteger[{1, n}]},               If[a[[i]] > 0 && i != j,                ReplacePart[a, {i -> a[[i]] - 1, j -> a[[j]] + 1}], a]          ]; b1 = Nest[f, a, 100]; b2 = Nest[f, b1, 1000]; b3 = Nest[f, b2, 10000]; b4 = Nest[f, b3, 100000]; b1 = Sort[b1]; b2 = Sort[b2]; b3 = Sort[b3]; b4 = Sort[b4]; ListPlot[{b1, b2, b3, b4}, PlotRange -> All] ==== UPDATED #1 Consider 3-player case, it can be described by 2d random walk on a triangular lattice. By counting the number of arrivals on each lattice point in $10^6$ walks, we can split points into two groups: All blue points have almost same arrivals. From this point of view, this problem seems trivial... We can label each blue point with lattice distance from the center, and the ""unfair"" result can be achieved by points with most common lattice distance.","Suppose there are 100 people, each one has \$100. Now repeatedly do following: Random choose two people A and B (random means uniform and independent); If A has money, A gives \$1 to B; If A has no money, do nothing; After doing many times, what is the distribution of money among these people? I guess it should be something like uniform distributed, but it is not. Could you tell me why? Why do some people get so much money from others in this fair process? Here is the simulation: n = 100; a = Table[100, {i, 1, n}]; f[a_] := With[{i = RandomInteger[{1, n}], j = RandomInteger[{1, n}]},               If[a[[i]] > 0 && i != j,                ReplacePart[a, {i -> a[[i]] - 1, j -> a[[j]] + 1}], a]          ]; b1 = Nest[f, a, 100]; b2 = Nest[f, b1, 1000]; b3 = Nest[f, b2, 10000]; b4 = Nest[f, b3, 100000]; b1 = Sort[b1]; b2 = Sort[b2]; b3 = Sort[b3]; b4 = Sort[b4]; ListPlot[{b1, b2, b3, b4}, PlotRange -> All] ==== UPDATED #1 Consider 3-player case, it can be described by 2d random walk on a triangular lattice. By counting the number of arrivals on each lattice point in $10^6$ walks, we can split points into two groups: All blue points have almost same arrivals. From this point of view, this problem seems trivial... We can label each blue point with lattice distance from the center, and the ""unfair"" result can be achieved by points with most common lattice distance.",,"['probability', 'random-walk']"
33,Math behind leap year cycle,Math behind leap year cycle,,"Everybody knows that leap year is year which is divisible by 4. In case of century years, it has to be divisible by 400. The leap year cycle has 400 years (calendar repeat with the same day-date combinations). Can anybody explain me please, the math behind it? How can we prove that leap year cycle has 400 years? Thank you.","Everybody knows that leap year is year which is divisible by 4. In case of century years, it has to be divisible by 400. The leap year cycle has 400 years (calendar repeat with the same day-date combinations). Can anybody explain me please, the math behind it? How can we prove that leap year cycle has 400 years? Thank you.",,"['probability', 'calendar-computations']"
34,Where am I going wrong in interpreting this problem as a gambler's ruin problem?,Where am I going wrong in interpreting this problem as a gambler's ruin problem?,,"I was trying to solve this problem (Strategic Practice Week 3, Homework problem 4 in Harvard's Stat 110 class) , by framing it as a gambler's ruin problem: Calvin and Hobbes play a match consisting of a series of games, where   Calvin has probability $p$ of winning each game (independently) and $q = 1-p$. They   play with a “win by two” rule: the first player to win two games more   than his opponent wins the match. Find the probability that Calvin   wins the match (in terms of $p$), by interpreting the problem as a   gambler's ruin problem. Here's how I approached the problem: Let, $W:$ Calvin wins the match; $D_i:$ (Wins by Calvin) $-$ (Wins by Hobbes) $= i$ $p_i:$ $\Pr($W | $D_i$$)$ Now, by conditioning on the first game, and using the law of total probability, we get: $p_i = p p_{i+1} + qp_{i-1}$, with $p_2 = 1$ (Calvin wins with certainty if the difference is $2$) and $p_{-2}  = 0$ (Calvin loses with certainty if the difference is $-2$). Solving this recurrence relation (which I omit here, since it's mostly algebra gymnastics), we get: $p_i = \dfrac{p^6}{p^8 - q^4}p^i - \dfrac{p^2q^2}{p^8-q^4}(\dfrac{q}{p})^i$ Since both Calvin and Hobbes start with a $0$ difference in wins, what we need to find is $p_0 = \dfrac{p^2(p^4-q^2)}{p^8-q^4}$. However, the answer turns out to be $\dfrac{p^2}{p^2+q^2}$, which can be easily obtained by using the law of total probability and conditioning on the number of wins in the first 2 matches (0 win, 1 win or 2 wins, with the number of wins $X \sim $ Bin($2,p$)). Where am I going wrong with my interpretation of the problem as a gambler's ruin problem?","I was trying to solve this problem (Strategic Practice Week 3, Homework problem 4 in Harvard's Stat 110 class) , by framing it as a gambler's ruin problem: Calvin and Hobbes play a match consisting of a series of games, where   Calvin has probability $p$ of winning each game (independently) and $q = 1-p$. They   play with a “win by two” rule: the first player to win two games more   than his opponent wins the match. Find the probability that Calvin   wins the match (in terms of $p$), by interpreting the problem as a   gambler's ruin problem. Here's how I approached the problem: Let, $W:$ Calvin wins the match; $D_i:$ (Wins by Calvin) $-$ (Wins by Hobbes) $= i$ $p_i:$ $\Pr($W | $D_i$$)$ Now, by conditioning on the first game, and using the law of total probability, we get: $p_i = p p_{i+1} + qp_{i-1}$, with $p_2 = 1$ (Calvin wins with certainty if the difference is $2$) and $p_{-2}  = 0$ (Calvin loses with certainty if the difference is $-2$). Solving this recurrence relation (which I omit here, since it's mostly algebra gymnastics), we get: $p_i = \dfrac{p^6}{p^8 - q^4}p^i - \dfrac{p^2q^2}{p^8-q^4}(\dfrac{q}{p})^i$ Since both Calvin and Hobbes start with a $0$ difference in wins, what we need to find is $p_0 = \dfrac{p^2(p^4-q^2)}{p^8-q^4}$. However, the answer turns out to be $\dfrac{p^2}{p^2+q^2}$, which can be easily obtained by using the law of total probability and conditioning on the number of wins in the first 2 matches (0 win, 1 win or 2 wins, with the number of wins $X \sim $ Bin($2,p$)). Where am I going wrong with my interpretation of the problem as a gambler's ruin problem?",,"['probability', 'gambling']"
35,Expectation on the number of geometric variables needed for their sum to exceed a threshold,Expectation on the number of geometric variables needed for their sum to exceed a threshold,,"Let $X_1, X_2, \ldots,$ be a series of independent random variables such that $X_i\sim Geo(p_i)$, and let $T\in\mathbb N$ be a constant positive integer. Next, define $N\triangleq \min\{{n\in\mathbb N}\mid \sum\limits_{i=1}^nX_i\ge T\}$. How can we find $\mathbb E[N]$? (An upper bound would also work). This seems like a somewhat inverse version of the general version of Wald's equation . Intuitively, I'd like to say that $\mathbb E[N]\approx\min\{{n\in\mathbb N}\mid \sum\limits_{i=1}^n \mathbb E[X_i]\ge T\} = \min\{{n\in\mathbb N}\mid \sum\limits_{i=1}^n 1/p_i \ge T\}$, but I'm looking for a formal argument. If this helps, we can assume that $0.01\ge p_1\ge p_2\ge p_3\ge\ldots$ In his answer, Ian gave a solution that handles two extreme cases - one where $\min\{{n\in\mathbb N}\mid \sum\limits_{i=1}^n 1/p_i \ge T\} = \Theta(T)$ and one where the $p_i$'s decrease exponentially and we can just look for the $i$ such that $p_i^{-1}\approx T$ as a good approximation. However, this approach doesn't seem to work well for what I think are the interesting cases. As we have $p=\prod_{i=1}^d(1-p_i)^{x_i-1}\le \prod_{i=1}^d(1-p_d)^{x_i-1}=(1-p_d)^{T-d}$, this fails in the following example. If we have $p_i = \Theta(1/i)$, then Ian's approach would give an a bound of $\mathbb E[N]\le T/\log T$ (after optimizing $d$), while we can expect $N$ to be of the order of $\sqrt T$. This is a near quadratic gap that this method cannot address.","Let $X_1, X_2, \ldots,$ be a series of independent random variables such that $X_i\sim Geo(p_i)$, and let $T\in\mathbb N$ be a constant positive integer. Next, define $N\triangleq \min\{{n\in\mathbb N}\mid \sum\limits_{i=1}^nX_i\ge T\}$. How can we find $\mathbb E[N]$? (An upper bound would also work). This seems like a somewhat inverse version of the general version of Wald's equation . Intuitively, I'd like to say that $\mathbb E[N]\approx\min\{{n\in\mathbb N}\mid \sum\limits_{i=1}^n \mathbb E[X_i]\ge T\} = \min\{{n\in\mathbb N}\mid \sum\limits_{i=1}^n 1/p_i \ge T\}$, but I'm looking for a formal argument. If this helps, we can assume that $0.01\ge p_1\ge p_2\ge p_3\ge\ldots$ In his answer, Ian gave a solution that handles two extreme cases - one where $\min\{{n\in\mathbb N}\mid \sum\limits_{i=1}^n 1/p_i \ge T\} = \Theta(T)$ and one where the $p_i$'s decrease exponentially and we can just look for the $i$ such that $p_i^{-1}\approx T$ as a good approximation. However, this approach doesn't seem to work well for what I think are the interesting cases. As we have $p=\prod_{i=1}^d(1-p_i)^{x_i-1}\le \prod_{i=1}^d(1-p_d)^{x_i-1}=(1-p_d)^{T-d}$, this fails in the following example. If we have $p_i = \Theta(1/i)$, then Ian's approach would give an a bound of $\mathbb E[N]\le T/\log T$ (after optimizing $d$), while we can expect $N$ to be of the order of $\sqrt T$. This is a near quadratic gap that this method cannot address.",,"['probability', 'probability-distributions', 'random-variables', 'expectation', 'random']"
36,A simple proof of McDiarmid's inequality?,A simple proof of McDiarmid's inequality?,,"$\newcommand{\Ex}{\mathrm{E}} \newcommand{\Pr}{\mathrm{Pr}} \newcommand{\Ind}{\mathbf{1}} \newcommand{\implies}{\Rightarrow}$ Let $f(x_1, \ldots, x_n)$ be a real valued function of $n$ variables, with each $x_i \in \mathcal{X}$, such that: \begin{equation}  \forall (x_1,\ldots,x_{n-1}) \in \mathcal{X}^{n-1},  | f(x_1, \ldots, x_{n-1}, x) - f(x_1, \ldots, x_{n-1}, x') | \leq 1 , \end{equation} where $x, x' \in \mathcal{X}$. Note that McDiarmid's assumes that $f$ is 1-Lipschitz, i.e., the above condition holds if we change any coordinate (and not just the n-th coordinate). The constant 1 is not important, it can be any arbitrary constant. First, I use the Hoeffding's inequality to show that for any arbitrary $(x_1,\ldots,x_{n-1})$: \begin{equation*} \Pr_{X_n}\{ \hat{\Ex}[f(x_1, \ldots, x_{n-1}, X_n)] - \Ex[f(x_1, \ldots, x_{n-1}, X_n)] \geq t \mid (x_1, \ldots, x_{n-1}) \} \leq e^{-2mt^2}, \end{equation*} where $\hat{\Ex}$ is the average of $f(\cdot)$ over $m$ samples of the $X_n$ while fixing the first $n-1$ variables to $x_1, \ldots, x_{n-1}$. Next, I take expectation with respect to $X_1, \ldots, X_{n-1}$, of both sides of the equation to get:  \begin{gather*} \Pr_{X_n}\{ \hat{\Ex}[f(x_1, \ldots, x_{n-1}, X_n)] - \Ex[f(x_1, \ldots, x_{n-1}, X_n)] \geq t \mid (x_1, \ldots, x_{n-1}) \} \leq e^{-2mt^2} \\ \implies \Ex_{X_n}[ \Ind[\hat{\Ex}[f(x_1, \ldots, x_{n-1}, X_n)] - \Ex[f(x_1, \ldots, x_{n-1}, X_n)] \geq t] \mid (x_1, \ldots, x_{n-1}) ] \leq e^{-2mt^2} \\ \implies \Ex_{X_1, \ldots X_{n-1}}[\Ex_{X_n}\{ \Ind[\hat{\Ex}[f(x_1, \ldots, x_{n-1}, X_n)] - \Ex[f(x_1, \ldots, x_{n-1}, X_n)] \geq t]] \mid (x_1, \ldots, x_{n-1}) ] \leq e^{-2mt^2} \\ \implies \Pr_{X_1, \ldots, X_n}\{ \hat{\Ex}[f(X_1, \ldots, X_n)] - \Ex[f(X_1, \ldots, X_n)] \geq t \} \leq e^{-2mt^2}. \end{gather*} Essentially, I have shown concentration of $f(.)$ with respect to all variables by showing concentration with respect to only one variable and without constructing a Doob Martingale as the standard McDiarmid's proof does. The above is definitely wrong but I am not able to see where. Also, is the above proof correct if $\mathcal{X}$ is finite or countably infinite?","$\newcommand{\Ex}{\mathrm{E}} \newcommand{\Pr}{\mathrm{Pr}} \newcommand{\Ind}{\mathbf{1}} \newcommand{\implies}{\Rightarrow}$ Let $f(x_1, \ldots, x_n)$ be a real valued function of $n$ variables, with each $x_i \in \mathcal{X}$, such that: \begin{equation}  \forall (x_1,\ldots,x_{n-1}) \in \mathcal{X}^{n-1},  | f(x_1, \ldots, x_{n-1}, x) - f(x_1, \ldots, x_{n-1}, x') | \leq 1 , \end{equation} where $x, x' \in \mathcal{X}$. Note that McDiarmid's assumes that $f$ is 1-Lipschitz, i.e., the above condition holds if we change any coordinate (and not just the n-th coordinate). The constant 1 is not important, it can be any arbitrary constant. First, I use the Hoeffding's inequality to show that for any arbitrary $(x_1,\ldots,x_{n-1})$: \begin{equation*} \Pr_{X_n}\{ \hat{\Ex}[f(x_1, \ldots, x_{n-1}, X_n)] - \Ex[f(x_1, \ldots, x_{n-1}, X_n)] \geq t \mid (x_1, \ldots, x_{n-1}) \} \leq e^{-2mt^2}, \end{equation*} where $\hat{\Ex}$ is the average of $f(\cdot)$ over $m$ samples of the $X_n$ while fixing the first $n-1$ variables to $x_1, \ldots, x_{n-1}$. Next, I take expectation with respect to $X_1, \ldots, X_{n-1}$, of both sides of the equation to get:  \begin{gather*} \Pr_{X_n}\{ \hat{\Ex}[f(x_1, \ldots, x_{n-1}, X_n)] - \Ex[f(x_1, \ldots, x_{n-1}, X_n)] \geq t \mid (x_1, \ldots, x_{n-1}) \} \leq e^{-2mt^2} \\ \implies \Ex_{X_n}[ \Ind[\hat{\Ex}[f(x_1, \ldots, x_{n-1}, X_n)] - \Ex[f(x_1, \ldots, x_{n-1}, X_n)] \geq t] \mid (x_1, \ldots, x_{n-1}) ] \leq e^{-2mt^2} \\ \implies \Ex_{X_1, \ldots X_{n-1}}[\Ex_{X_n}\{ \Ind[\hat{\Ex}[f(x_1, \ldots, x_{n-1}, X_n)] - \Ex[f(x_1, \ldots, x_{n-1}, X_n)] \geq t]] \mid (x_1, \ldots, x_{n-1}) ] \leq e^{-2mt^2} \\ \implies \Pr_{X_1, \ldots, X_n}\{ \hat{\Ex}[f(X_1, \ldots, X_n)] - \Ex[f(X_1, \ldots, X_n)] \geq t \} \leq e^{-2mt^2}. \end{gather*} Essentially, I have shown concentration of $f(.)$ with respect to all variables by showing concentration with respect to only one variable and without constructing a Doob Martingale as the standard McDiarmid's proof does. The above is definitely wrong but I am not able to see where. Also, is the above proof correct if $\mathcal{X}$ is finite or countably infinite?",,"['probability', 'concentration-of-measure']"
37,Conditional Distribution of The Sum of Two Standard Normal Random Variables,Conditional Distribution of The Sum of Two Standard Normal Random Variables,,"I have a question about conditional Distribution. Let $X,Y\sim N(0,1)$ and independently distributed. Then, let $Z=X+Y$.  Find the p.d.f and c.d.f of $Z\mid(X>0, Y>0)$. The hint here is to use the circular symmetry of joint density of $X$ and $Y$. No integration needed. My attempt: First, we know that $Z\sim N(0,2)$ then the p.d.f of $Z$ is $f_{Z}(z)=\frac{1}{2\sqrt{\pi}}e^{\frac{-z^{2}}{4}},-\infty<z<\infty.$ Next, we have that: $$F_{Z\mid X>0,Y>0}(z\mid x>0,y>0)=P(Z\leq z\mid X>0,Y>0)=\frac{P(Z\leq z, X>0,Y>0)}{P(X>0,Y>0)}.$$ It follows that by independence: \begin{align} & F_{Z\mid X>0,Y>0}(z\mid x>0,y>0) = \frac{P(0<Z\leq z)}{P(X>0)P(Y>0)} \\[10pt] = {} & \frac{F_Z(z)-F_Z(0)}{\frac{1}{4}} = 4F_Z(z)-\frac{4}{2}=4F_Z(z)-2. \end{align} So, the c.d.f is: $F_{Z\mid X>0,Y>0}(z\mid x>0,y>0)=4F_Z(z)-2$. Then, the p.d.f will be $$f_{Z\mid X>0,Y>0}(z\mid x,y)=4f_Z(z)=\frac{2}{\sqrt{\pi}} e^{\frac{-z^{2}}{4}},0<z<\infty.$$ This is what I got; however, I'm pretty sure I did something wrong since when I integrate the p.d.f over the whole region, it does not give me 1. Any suggestion?","I have a question about conditional Distribution. Let $X,Y\sim N(0,1)$ and independently distributed. Then, let $Z=X+Y$.  Find the p.d.f and c.d.f of $Z\mid(X>0, Y>0)$. The hint here is to use the circular symmetry of joint density of $X$ and $Y$. No integration needed. My attempt: First, we know that $Z\sim N(0,2)$ then the p.d.f of $Z$ is $f_{Z}(z)=\frac{1}{2\sqrt{\pi}}e^{\frac{-z^{2}}{4}},-\infty<z<\infty.$ Next, we have that: $$F_{Z\mid X>0,Y>0}(z\mid x>0,y>0)=P(Z\leq z\mid X>0,Y>0)=\frac{P(Z\leq z, X>0,Y>0)}{P(X>0,Y>0)}.$$ It follows that by independence: \begin{align} & F_{Z\mid X>0,Y>0}(z\mid x>0,y>0) = \frac{P(0<Z\leq z)}{P(X>0)P(Y>0)} \\[10pt] = {} & \frac{F_Z(z)-F_Z(0)}{\frac{1}{4}} = 4F_Z(z)-\frac{4}{2}=4F_Z(z)-2. \end{align} So, the c.d.f is: $F_{Z\mid X>0,Y>0}(z\mid x>0,y>0)=4F_Z(z)-2$. Then, the p.d.f will be $$f_{Z\mid X>0,Y>0}(z\mid x,y)=4f_Z(z)=\frac{2}{\sqrt{\pi}} e^{\frac{-z^{2}}{4}},0<z<\infty.$$ This is what I got; however, I'm pretty sure I did something wrong since when I integrate the p.d.f over the whole region, it does not give me 1. Any suggestion?",,"['probability', 'statistics', 'probability-distributions', 'normal-distribution']"
38,A universal bound on expectation $E[X^ke^{-X}]$,A universal bound on expectation,E[X^ke^{-X}],"My friend introduced me to association inequalities for expectation. Namely: If $f$ is monotonically increasing and $g$ is monotonically decreasing, then for any random variable $X$ $$E[f(X)g(X)] \leq E[f(X)]E[g(X)]$$ provided the expectations are well defined. As an example, we can show for $k\geq 1$ $$E[X^ke^{-X}] \leq E[X^k]E[e^{-X}]$$ The Cauchy Schwarz inequality would give a looser bound in this case. Now for the above example, I wondered if it was possible to prove the following stronger claim: Does there exist a universal constant $C_k$ such that for $X$ having ANY distribution, $$E[X^ke^{-X}] \leq C_kE[e^{-X}]$$ ? My understanding as of now is that it does exist but not universally (i.e. not distribution independent). But I couldn't come up with a counter for this? I'd appreciate it if someone could throw some light on this. If necessary we may assume that $X$ is non negative. The reason I'm interested in such a bound is that there are cases where the kth moment of $X$ is infinity but $E[X^ke^{-X}]<\infty$. Update: The question has been successfully answered. However I wanted to make a note that $E[X^ke^{-X}] \leq k^ke^{-k}$ for $X\geq 0$.","My friend introduced me to association inequalities for expectation. Namely: If $f$ is monotonically increasing and $g$ is monotonically decreasing, then for any random variable $X$ $$E[f(X)g(X)] \leq E[f(X)]E[g(X)]$$ provided the expectations are well defined. As an example, we can show for $k\geq 1$ $$E[X^ke^{-X}] \leq E[X^k]E[e^{-X}]$$ The Cauchy Schwarz inequality would give a looser bound in this case. Now for the above example, I wondered if it was possible to prove the following stronger claim: Does there exist a universal constant $C_k$ such that for $X$ having ANY distribution, $$E[X^ke^{-X}] \leq C_kE[e^{-X}]$$ ? My understanding as of now is that it does exist but not universally (i.e. not distribution independent). But I couldn't come up with a counter for this? I'd appreciate it if someone could throw some light on this. If necessary we may assume that $X$ is non negative. The reason I'm interested in such a bound is that there are cases where the kth moment of $X$ is infinity but $E[X^ke^{-X}]<\infty$. Update: The question has been successfully answered. However I wanted to make a note that $E[X^ke^{-X}] \leq k^ke^{-k}$ for $X\geq 0$.",,"['probability', 'expectation']"
39,Calculation problem with Central limit theorem,Calculation problem with Central limit theorem,,"Let $X_1,X_2,\dots\,$ i.i.d random variables with mean zero and variance $1$. Let $S_n=\sum_{i=1}^n X_i\,,n\in \mathbb N.$ Compute the weak limes $\lim_{n\to\infty} \frac1n \sum_{i=1}^n \frac{S_i}{\sqrt n}$ Surely we will have to use the CLT. First I tried to simplify the expression, but I am not sure how to continue here. $$\lim_{n\to\infty} \frac1n \sum_{i=1}^n \frac{S_i}{\sqrt n}=\dots=\lim_{ n\to\infty}\frac{1}{\sqrt n} \frac{nX_1+(n-1)X_2+\dots+X_n}{n}$$ Edit(2) According to the comments, we have to verify Lindberg's condition ( https://en.wikipedia.org/wiki/Lindeberg%27s_condition ) Lindberg's condition: $$\lim_{n\to\infty} \frac{1}{s_n^2} \sum_{k=1}^n E[(X_k - \mu_k)^2 \mathbb 1_{\{\mid X_k - \mu_k \mid > \epsilon s_n \}}=0,\quad \text{for all $\epsilon >0$}$$ Here: $E(S_i) {\overset{\text{$X_i$ i.i.d}}{=}}0$ , $Var(S_i) {\overset{\text{$X_i$ i.i.d}}{=}} \sum Var( X_i) {\overset{\text{$X_i$ i.i.d}}{=}} i$ for all $i=1,2,\dots$ Furthermore $s_n^2= \sum_{i=1}^n \sigma_i^2 =Var(S_1)+Var(S_2)+\dots + Var(S_n)=1+2+\dots +n=\frac{n(n+1)}{2}$. Plugging in: $$\lim_{n\to\infty}\frac{2}{n^2+n}\sum_{k=1}^n E(S_k)^2 1_{\{\mid S_k \mid > \epsilon  {\frac{\sqrt {n^2+n}}{\sqrt 2}}\}}$$  Intuitively this does not seem correct to me. Furthermore I am not sure how to simplify this expression. Some help is welcome and obviously needed!","Let $X_1,X_2,\dots\,$ i.i.d random variables with mean zero and variance $1$. Let $S_n=\sum_{i=1}^n X_i\,,n\in \mathbb N.$ Compute the weak limes $\lim_{n\to\infty} \frac1n \sum_{i=1}^n \frac{S_i}{\sqrt n}$ Surely we will have to use the CLT. First I tried to simplify the expression, but I am not sure how to continue here. $$\lim_{n\to\infty} \frac1n \sum_{i=1}^n \frac{S_i}{\sqrt n}=\dots=\lim_{ n\to\infty}\frac{1}{\sqrt n} \frac{nX_1+(n-1)X_2+\dots+X_n}{n}$$ Edit(2) According to the comments, we have to verify Lindberg's condition ( https://en.wikipedia.org/wiki/Lindeberg%27s_condition ) Lindberg's condition: $$\lim_{n\to\infty} \frac{1}{s_n^2} \sum_{k=1}^n E[(X_k - \mu_k)^2 \mathbb 1_{\{\mid X_k - \mu_k \mid > \epsilon s_n \}}=0,\quad \text{for all $\epsilon >0$}$$ Here: $E(S_i) {\overset{\text{$X_i$ i.i.d}}{=}}0$ , $Var(S_i) {\overset{\text{$X_i$ i.i.d}}{=}} \sum Var( X_i) {\overset{\text{$X_i$ i.i.d}}{=}} i$ for all $i=1,2,\dots$ Furthermore $s_n^2= \sum_{i=1}^n \sigma_i^2 =Var(S_1)+Var(S_2)+\dots + Var(S_n)=1+2+\dots +n=\frac{n(n+1)}{2}$. Plugging in: $$\lim_{n\to\infty}\frac{2}{n^2+n}\sum_{k=1}^n E(S_k)^2 1_{\{\mid S_k \mid > \epsilon  {\frac{\sqrt {n^2+n}}{\sqrt 2}}\}}$$  Intuitively this does not seem correct to me. Furthermore I am not sure how to simplify this expression. Some help is welcome and obviously needed!",,"['probability', 'probability-theory']"
40,"One of the terms in the open form of $(3x^2+2x+y+4z)^{10}$ is randomly chosen, what is the probability that the chosen term contains $x^7$?","One of the terms in the open form of  is randomly chosen, what is the probability that the chosen term contains ?",(3x^2+2x+y+4z)^{10} x^7,"today I've encountered a problem like the following: One of the terms in the open form of $(3x^2+2x+y+4z)^{10}$ is randomly chosen, what is the probability that the chosen term contains $x^7$? My Attempts I've reduced the question to two pieces, -calculating the number of terms with $x^7$ and the number of all terms. I've opened the brackets using multinomials: $$\sum_{k_1,k_2,k_3,k_4=1\\k_1+k_2+k_3+k_4=10}^{10} \dbinom{10}{k_1,k_2,k_3,k_4} (3x^2)^{k_1}\cdot(2x)^{k_2}\cdot y^{k_3}\cdot(4z)^{k_4}$$  To calculate the number of terms with $x^7$ I've used that $2k_1+k_2=7$ easily I got $(0,7),(1,5),(2,3),(3,1)$ as the number of solutions for this, thus $4$ terms with $x^7$. However when it got to calculating the number of all terms It got a little more complicated, As an initial thought $x^2$ allows us to get $x^{20}$ for the max degree and $x^{10}$ for the minimum, I assumed that they can take all the values between firstly and what I got was $\{10,11,12,13,14,15,16,17,18,19,20\}$ thus I said the answer might be $\dfrac{4}{11}$. Though I think that my solution has technical errors, and the answer I gave isn't in the options. What are your suggestions?","today I've encountered a problem like the following: One of the terms in the open form of $(3x^2+2x+y+4z)^{10}$ is randomly chosen, what is the probability that the chosen term contains $x^7$? My Attempts I've reduced the question to two pieces, -calculating the number of terms with $x^7$ and the number of all terms. I've opened the brackets using multinomials: $$\sum_{k_1,k_2,k_3,k_4=1\\k_1+k_2+k_3+k_4=10}^{10} \dbinom{10}{k_1,k_2,k_3,k_4} (3x^2)^{k_1}\cdot(2x)^{k_2}\cdot y^{k_3}\cdot(4z)^{k_4}$$  To calculate the number of terms with $x^7$ I've used that $2k_1+k_2=7$ easily I got $(0,7),(1,5),(2,3),(3,1)$ as the number of solutions for this, thus $4$ terms with $x^7$. However when it got to calculating the number of all terms It got a little more complicated, As an initial thought $x^2$ allows us to get $x^{20}$ for the max degree and $x^{10}$ for the minimum, I assumed that they can take all the values between firstly and what I got was $\{10,11,12,13,14,15,16,17,18,19,20\}$ thus I said the answer might be $\dfrac{4}{11}$. Though I think that my solution has technical errors, and the answer I gave isn't in the options. What are your suggestions?",,"['probability', 'combinatorics', 'contest-math', 'binomial-coefficients', 'multinomial-coefficients']"
41,probability involving rerolling dice,probability involving rerolling dice,,"The question go like the following. You roll one 6 sided dice one time. You get the same money as the number of dots face up. But if you roll a six, you get nothing and re-roll. If your re-roll gets a six again, you reroll again and again. What is the expercted value of money get. I guess i should go like the following $$E(x)=\frac16+\frac26+\frac36+\frac46+\frac56+\frac16 E(x)$$","The question go like the following. You roll one 6 sided dice one time. You get the same money as the number of dots face up. But if you roll a six, you get nothing and re-roll. If your re-roll gets a six again, you reroll again and again. What is the expercted value of money get. I guess i should go like the following $$E(x)=\frac16+\frac26+\frac36+\frac46+\frac56+\frac16 E(x)$$",,['probability']
42,Series involving error function,Series involving error function,,"In a problem of probability I obtained the following summation $$ \sum_{n=0}^\infty [ \mathrm{erf} \ (1+n)k - \mathrm{erf} \ nk ]^2 $$ but I have no idea of how to sum it. I observed (through numerical calculation) that the sum is directly proportional to $k$, at last for $k\sim 10^{-3}$. For large $n$ the value of both error functions is very close to $1$. Then, the 'most important' terms of the sum are the ones of small $n$. Then, I thought, the term corresponding to $n=0$ would give a reasonable approximation to the sum. If $k$ is small, the term for $n=0$ is approximately $k^2$, which is a completely different behavior than the observed. I checked the terms of the summation and many of then are important, therefore we can not obtain the behavior of the summation only from the largest term. How can I obtain at least the behavior of the sum with $k$, or how to obtain an approximate value for the sum?","In a problem of probability I obtained the following summation $$ \sum_{n=0}^\infty [ \mathrm{erf} \ (1+n)k - \mathrm{erf} \ nk ]^2 $$ but I have no idea of how to sum it. I observed (through numerical calculation) that the sum is directly proportional to $k$, at last for $k\sim 10^{-3}$. For large $n$ the value of both error functions is very close to $1$. Then, the 'most important' terms of the sum are the ones of small $n$. Then, I thought, the term corresponding to $n=0$ would give a reasonable approximation to the sum. If $k$ is small, the term for $n=0$ is approximately $k^2$, which is a completely different behavior than the observed. I checked the terms of the summation and many of then are important, therefore we can not obtain the behavior of the summation only from the largest term. How can I obtain at least the behavior of the sum with $k$, or how to obtain an approximate value for the sum?",,"['probability', 'sequences-and-series']"
43,Showing the Clayton Copula is $2-$increasing,Showing the Clayton Copula is increasing,2-,"This is the definition of a bivariate ($2$-dimensional) copula: $C(\mathbf{u}):[0,1]^2 \mapsto [0,1]$ is a bivariate copula if $C(u_{1},0) = 0$ and $C(0,u_{2})=0$; i.e., $C = 0$ if one argument is $0$. $C(u_{1},1) = u_{1}$ and $C(1,u_{2}) = u_{2}$; i.e., the copula reduces to $u_{i}$ if all arguments are $1$ except the $i$th one. $C(\mathbf{u})$ is $2$-increasing - i.e., for each hyperrectangle $B = \prod_{i=1}^{k}[x_{i},y_{i}]$ in $[0,1]^{2}$, the $C$-volume:   $$ \int_{B}dC = \sum_{\mathbf{z} \in \{x_{1},y_{1}\}\times\{x_{2},y_{2}\}} (-1)^{N(\mathbf{z})} C(\mathbf{z}) \geq 0 $$   where $N(\mathbf{z}) = \text{the number of}\,z_{i}=x_{i}$ for $\mathbf{z} \in \{x_{1},y_{1}\}\times \{x_{2},y_{2}\}$ I need to prove that the Clayton Copula, $C(u,v) = \left[\max\{u^{-\theta} + v^{-\theta}-1,0 \}\right]^{-1/\theta}$ for $u,v \in (0,1)$ and $\theta > 0$, is a bonafide bivariate copula. So, far, the only part I am still having trouble with is showing property #3 - namely, that $C$ is what is known as $2-$increasing. For #3 , I have that $\displaystyle \int_{B}dC = C(x_{2},y_{2})-C(x_{2},y_{1})-C(x_{1},y_{2})+C(x_{1},y_{1})$, which after many, many steps of algebra, I got to look like $$ = \frac{x_{2}y_{2}}{\left(y_{2}^{\theta}+x_{2}^{\theta}-x_{2}^{\theta}y_{2}^{\theta} \right)^{1/\theta}} - \frac{x_{2}y_{1}}{\left( y_{1}^{\theta}+x_{2}^{\theta}-x_{2}^{\theta}y_{1}^{\theta} \right)^{1/\theta}} - \frac{x_{1}y_{2}}{\left(y_{2}^{\theta}+x_{1}^{\theta}-x_{1}^{\theta}y_{2}^{\theta} \right)^{1/\theta}}  + \frac{x_{1}y_{1}}{\left( y_{1}^{\theta}+x_{1}^{\theta}-x_{1}^{\theta}y_{1}^{\theta} \right)^{1/\theta}} $$ But, how do I show that this must be $\geq 0$? I thank you ahead of time for your help!","This is the definition of a bivariate ($2$-dimensional) copula: $C(\mathbf{u}):[0,1]^2 \mapsto [0,1]$ is a bivariate copula if $C(u_{1},0) = 0$ and $C(0,u_{2})=0$; i.e., $C = 0$ if one argument is $0$. $C(u_{1},1) = u_{1}$ and $C(1,u_{2}) = u_{2}$; i.e., the copula reduces to $u_{i}$ if all arguments are $1$ except the $i$th one. $C(\mathbf{u})$ is $2$-increasing - i.e., for each hyperrectangle $B = \prod_{i=1}^{k}[x_{i},y_{i}]$ in $[0,1]^{2}$, the $C$-volume:   $$ \int_{B}dC = \sum_{\mathbf{z} \in \{x_{1},y_{1}\}\times\{x_{2},y_{2}\}} (-1)^{N(\mathbf{z})} C(\mathbf{z}) \geq 0 $$   where $N(\mathbf{z}) = \text{the number of}\,z_{i}=x_{i}$ for $\mathbf{z} \in \{x_{1},y_{1}\}\times \{x_{2},y_{2}\}$ I need to prove that the Clayton Copula, $C(u,v) = \left[\max\{u^{-\theta} + v^{-\theta}-1,0 \}\right]^{-1/\theta}$ for $u,v \in (0,1)$ and $\theta > 0$, is a bonafide bivariate copula. So, far, the only part I am still having trouble with is showing property #3 - namely, that $C$ is what is known as $2-$increasing. For #3 , I have that $\displaystyle \int_{B}dC = C(x_{2},y_{2})-C(x_{2},y_{1})-C(x_{1},y_{2})+C(x_{1},y_{1})$, which after many, many steps of algebra, I got to look like $$ = \frac{x_{2}y_{2}}{\left(y_{2}^{\theta}+x_{2}^{\theta}-x_{2}^{\theta}y_{2}^{\theta} \right)^{1/\theta}} - \frac{x_{2}y_{1}}{\left( y_{1}^{\theta}+x_{2}^{\theta}-x_{2}^{\theta}y_{1}^{\theta} \right)^{1/\theta}} - \frac{x_{1}y_{2}}{\left(y_{2}^{\theta}+x_{1}^{\theta}-x_{1}^{\theta}y_{2}^{\theta} \right)^{1/\theta}}  + \frac{x_{1}y_{1}}{\left( y_{1}^{\theta}+x_{1}^{\theta}-x_{1}^{\theta}y_{1}^{\theta} \right)^{1/\theta}} $$ But, how do I show that this must be $\geq 0$? I thank you ahead of time for your help!",,['probability']
44,What is the distribution of real numbers with biased digits?,What is the distribution of real numbers with biased digits?,,"Suppose I have an infinite sequence of biased bits where the probability of $1$ is $2/3$ and the probability of $0$ is $1/3.$ If I view these as the digits in the binary expansion of a real number, then this sequence defines a real number in the interval $[0,1]$. So what kind of distribution does this real number have? Some considerations I have made so far is that the probability between $0.5$ and $1$ should be twice the probability between $0$ and $0.5.$ Similarly the probability between $0.25$ and $0.5$ should be twice the probability between $0$ and $0.25.$ A general way of writing this is recursive relationship is $$F(2x) - F(x) = 2F(x).$$ Adding boundary conditions I get the three equations $$F(0)=0\\ F(1)=1\\ F(2x)=3F(x)$$ which, if viewed as a recurrence relation, has the solution $F(x) = x^{\log_2(3)}$. My question is: Is this really airtight? Setting up these equations and using the solution from a recurrence relation felt a little hand wavy. I can easily verify that $x^{\log_2(3)}$ satisfies the above conditions for real numbers in the interval $[0,1]$, but is this solution unique?","Suppose I have an infinite sequence of biased bits where the probability of $1$ is $2/3$ and the probability of $0$ is $1/3.$ If I view these as the digits in the binary expansion of a real number, then this sequence defines a real number in the interval $[0,1]$. So what kind of distribution does this real number have? Some considerations I have made so far is that the probability between $0.5$ and $1$ should be twice the probability between $0$ and $0.5.$ Similarly the probability between $0.25$ and $0.5$ should be twice the probability between $0$ and $0.25.$ A general way of writing this is recursive relationship is $$F(2x) - F(x) = 2F(x).$$ Adding boundary conditions I get the three equations $$F(0)=0\\ F(1)=1\\ F(2x)=3F(x)$$ which, if viewed as a recurrence relation, has the solution $F(x) = x^{\log_2(3)}$. My question is: Is this really airtight? Setting up these equations and using the solution from a recurrence relation felt a little hand wavy. I can easily verify that $x^{\log_2(3)}$ satisfies the above conditions for real numbers in the interval $[0,1]$, but is this solution unique?",,"['probability', 'probability-distributions', 'recurrence-relations']"
45,Geometric distribution with increasing chance of success,Geometric distribution with increasing chance of success,,"I have the following problem (perhaps it could be seen as a variant of the geometric distribution): I'd like to know the expected number of trials needed to get the (single) special ticket among n tickets. Tickets are drawn without replacement. The distribution of a generalized version of this problem is here: https://math.stackexchange.com/a/436247 The given distribution is: $p_i \prod_{k=1}^{i-1}(1 - p_k)$, where $p_i$ is the probability of success at i-th trial. This looks like a generalized version of the geometric random variable. In my specific case, $p_i = \frac{1}{n-i+1}$, because on each trial, the number of tickets to draw reduces by 1. It might seem trivial, but I'm having trouble to calculate the mean of ""my"" distribution, which would give the expected number of trials until success... This could be a well-known problem already, but I was not able to find a reference for it (I found it's ""inverse"" here: A Diminishing Geometric Distribution ). So, any help is appreciated. EDIT: took out the sentence ""until I'll certainly get the special ticket on the last trial"" because one could infer that $p_n = 1$, implying $p_i = 0$ for $i < n$, which is not the case. EDIT: took out the sentence ""on the 1st trial the chance of selecting the special ticket is $1/n$, on 2nd it is $1/(n-1)$ and so on."" because it can be misleading. See @Karn Watcharasupat's answer and comments below.","I have the following problem (perhaps it could be seen as a variant of the geometric distribution): I'd like to know the expected number of trials needed to get the (single) special ticket among n tickets. Tickets are drawn without replacement. The distribution of a generalized version of this problem is here: https://math.stackexchange.com/a/436247 The given distribution is: $p_i \prod_{k=1}^{i-1}(1 - p_k)$, where $p_i$ is the probability of success at i-th trial. This looks like a generalized version of the geometric random variable. In my specific case, $p_i = \frac{1}{n-i+1}$, because on each trial, the number of tickets to draw reduces by 1. It might seem trivial, but I'm having trouble to calculate the mean of ""my"" distribution, which would give the expected number of trials until success... This could be a well-known problem already, but I was not able to find a reference for it (I found it's ""inverse"" here: A Diminishing Geometric Distribution ). So, any help is appreciated. EDIT: took out the sentence ""until I'll certainly get the special ticket on the last trial"" because one could infer that $p_n = 1$, implying $p_i = 0$ for $i < n$, which is not the case. EDIT: took out the sentence ""on the 1st trial the chance of selecting the special ticket is $1/n$, on 2nd it is $1/(n-1)$ and so on."" because it can be misleading. See @Karn Watcharasupat's answer and comments below.",,"['probability', 'probability-distributions']"
46,"If numbers of heads and tails are independent, then number of tosses $N \sim \mathrm{Poisson}$","If numbers of heads and tails are independent, then number of tosses",N \sim \mathrm{Poisson},"A fair coin is tossed a random number $N$ of times, giving a total $X$ of heads and $Y=N-X$ tails. Show that if $X$ and $Y$ are independent and the generating function $G_N(s)$ of $N$ exists for $s$ in a neighbourhood of $s=1$, then $N$ is Poisson distributed. In other exercises, I showed the converse , which was relatively easy. I'm working through my probability book myself, but my book does not provide an answer. I came this far: Because we have a fair coin, each coin toss follows a $\mathrm{Bernoulli}(\frac12)$ distribution, thus having generating function $G_{X_i}(s)=\frac12+\frac12 s$ for each $i \in \{1,2,\ldots, n\}$. Using the random sum formula I found $G_N(s)=(G_N(\frac12 +\frac12 s))^2$, for the probability generating function for the random sum, because $N=X+Y$, because each coin toss yields either a heads or a tails. The book gives as a hint: use $H(s)=G_N(1-s)$. However, I have no idea how to solve this one. This is also supposed to be one of the most difficult exercises so I'm just curious how this one has to be solved. Any ideas?","A fair coin is tossed a random number $N$ of times, giving a total $X$ of heads and $Y=N-X$ tails. Show that if $X$ and $Y$ are independent and the generating function $G_N(s)$ of $N$ exists for $s$ in a neighbourhood of $s=1$, then $N$ is Poisson distributed. In other exercises, I showed the converse , which was relatively easy. I'm working through my probability book myself, but my book does not provide an answer. I came this far: Because we have a fair coin, each coin toss follows a $\mathrm{Bernoulli}(\frac12)$ distribution, thus having generating function $G_{X_i}(s)=\frac12+\frac12 s$ for each $i \in \{1,2,\ldots, n\}$. Using the random sum formula I found $G_N(s)=(G_N(\frac12 +\frac12 s))^2$, for the probability generating function for the random sum, because $N=X+Y$, because each coin toss yields either a heads or a tails. The book gives as a hint: use $H(s)=G_N(1-s)$. However, I have no idea how to solve this one. This is also supposed to be one of the most difficult exercises so I'm just curious how this one has to be solved. Any ideas?",,['probability']
47,Exit Time of an Interval Brownian Motion - Distribution,Exit Time of an Interval Brownian Motion - Distribution,,"Let $W_t$ be a Brownian motion, fix $a<0<b$ and let $\tau_x=\mathrm{inf}(t\ge0:W_t=x)$. Show there is an $\alpha<1$: $P(\tau_a \wedge \tau_b>n )\le \alpha^n$ for all $n \in \mathbb{N}$. Proof-Idea: Use the distribution of the min and max of the brownian motion and their independence, pray and find an estimate: $$ \begin{eqnarray} P(\tau_a \wedge \tau_b>n ) &=& (1-P(\tau_a\le n ))(1-P(\tau_b\le n))\\ &=&(1-\Phi(\frac{-a}{\sqrt{n}})(1-\Phi(\frac{b}{\sqrt{n}})\\ \end{eqnarray} $$ But i am not able to find an estimation such that this expression is dominated be $\alpha^n$.","Let $W_t$ be a Brownian motion, fix $a<0<b$ and let $\tau_x=\mathrm{inf}(t\ge0:W_t=x)$. Show there is an $\alpha<1$: $P(\tau_a \wedge \tau_b>n )\le \alpha^n$ for all $n \in \mathbb{N}$. Proof-Idea: Use the distribution of the min and max of the brownian motion and their independence, pray and find an estimate: $$ \begin{eqnarray} P(\tau_a \wedge \tau_b>n ) &=& (1-P(\tau_a\le n ))(1-P(\tau_b\le n))\\ &=&(1-\Phi(\frac{-a}{\sqrt{n}})(1-\Phi(\frac{b}{\sqrt{n}})\\ \end{eqnarray} $$ But i am not able to find an estimation such that this expression is dominated be $\alpha^n$.",,"['probability', 'stochastic-processes', 'stochastic-calculus', 'brownian-motion']"
48,Joint PDF from marginal of two variables,Joint PDF from marginal of two variables,,"Sam is a stockbroker with investment and revenue that are not certain. His investment, $X$, each month is a random variable with the PDF in the image. $Y$ is the amount of money he earns in a month and is distributed uniformly between 0 and twice the amount he invested that month. (a) What is the joint PDF $f_{X,Y}(x, y)$? (b) What is the probability that in any given month, Sam makes a profit? (c) Sam continues his job for 10 years. What would be the approximate probability that he makes a profit in at least 63 of the months? Attempted Solution: a) Do I need to differentiate the marginal PDFs of both the variables and add them up. But I don't understand how the marginal PDF graph of $Y$ would look like. It would be a horizontal line with lower bound $0$, and the upper bound $2X$? b) For this I will find $P(Y>X)$, would this be $\frac X{2X}$, since for a profit to be earned $Y$ must be greater than $X$. This would be the upper half of the marginal PDF graph of $Y$. c) Clueless about this.","Sam is a stockbroker with investment and revenue that are not certain. His investment, $X$, each month is a random variable with the PDF in the image. $Y$ is the amount of money he earns in a month and is distributed uniformly between 0 and twice the amount he invested that month. (a) What is the joint PDF $f_{X,Y}(x, y)$? (b) What is the probability that in any given month, Sam makes a profit? (c) Sam continues his job for 10 years. What would be the approximate probability that he makes a profit in at least 63 of the months? Attempted Solution: a) Do I need to differentiate the marginal PDFs of both the variables and add them up. But I don't understand how the marginal PDF graph of $Y$ would look like. It would be a horizontal line with lower bound $0$, and the upper bound $2X$? b) For this I will find $P(Y>X)$, would this be $\frac X{2X}$, since for a profit to be earned $Y$ must be greater than $X$. This would be the upper half of the marginal PDF graph of $Y$. c) Clueless about this.",,"['probability', 'probability-distributions', 'uniform-distribution']"
49,The average of $X_iX_j (i\neq j) $ goes to $\mathbb E(X)^2$ in probability,The average of  goes to  in probability,X_iX_j (i\neq j)  \mathbb E(X)^2,"Suppose $(X_n)_n\geq 1$ are i.i.d. $\mathbb E(X)=\mu$, $\sigma^2= \operatorname{Var}(X)<\infty$. I want to show  $$\frac 1 {n(n-1)} \sum_{1\,\leq\, i,j\,\leq\, n,\,\, i\,\neq\, j} X_iX_j\to \mu^2$$ in probability. Here is what I tried. Applying Strong law of large number we have $$\frac 1 {n^2} \left( \sum_{1\leq i\leq n} X_i\right)^2\to \mu^2$$ in probability. For all $\epsilon>0$, $\mathbb P(|1/n^2(\sum_{1\leq i\leq n} X_i)^2-\mu^2|\geq\epsilon)\to 0 $. Expand this we have  $\mathbb P(|(\sum_{1\leq i,j\leq n, i\neq j} X_iX_j-n(n-1)\mu^2+\sum_{1\leq i\leq n}X_i^2-n\mu^2|\geq n^2\epsilon)\to 0 $. I don't know how to proceed from here. In particular , I'm wondering how to use Var$(X)<\infty$.","Suppose $(X_n)_n\geq 1$ are i.i.d. $\mathbb E(X)=\mu$, $\sigma^2= \operatorname{Var}(X)<\infty$. I want to show  $$\frac 1 {n(n-1)} \sum_{1\,\leq\, i,j\,\leq\, n,\,\, i\,\neq\, j} X_iX_j\to \mu^2$$ in probability. Here is what I tried. Applying Strong law of large number we have $$\frac 1 {n^2} \left( \sum_{1\leq i\leq n} X_i\right)^2\to \mu^2$$ in probability. For all $\epsilon>0$, $\mathbb P(|1/n^2(\sum_{1\leq i\leq n} X_i)^2-\mu^2|\geq\epsilon)\to 0 $. Expand this we have  $\mathbb P(|(\sum_{1\leq i,j\leq n, i\neq j} X_iX_j-n(n-1)\mu^2+\sum_{1\leq i\leq n}X_i^2-n\mu^2|\geq n^2\epsilon)\to 0 $. I don't know how to proceed from here. In particular , I'm wondering how to use Var$(X)<\infty$.",,['probability']
50,How can I prove this using a Combinatorial Proof?,How can I prove this using a Combinatorial Proof?,,"Can someone explain this to me? How can I approach this? Let $k \geq 1$ be an integer and consider a sequence  $n_1,n_2,\ldots,n_k$ of positive integers. Use a combinatorial proof  to show that. $$  {{n_1} \choose 2} + {{n_2} \choose 2} + \cdots + {{n_k} \choose 2}            \leq     {{n_1 + n_2 + \cdots + n_k} \choose 2}$$ Consider the complete graph on $n_i$ vertices. How many edges does this graph have?","Can someone explain this to me? How can I approach this? Let $k \geq 1$ be an integer and consider a sequence  $n_1,n_2,\ldots,n_k$ of positive integers. Use a combinatorial proof  to show that. $$  {{n_1} \choose 2} + {{n_2} \choose 2} + \cdots + {{n_k} \choose 2}            \leq     {{n_1 + n_2 + \cdots + n_k} \choose 2}$$ Consider the complete graph on $n_i$ vertices. How many edges does this graph have?",,"['probability', 'combinations', 'proof-explanation']"
51,Conditional expectation of exponential random variable,Conditional expectation of exponential random variable,,"Let $X$ be an exponential random variable with parameter $\lambda$. For any fixed $s>0$, I would like to compute $E(X\vert X>s)$. I have read in multiple places that this conditional expectation is equal to $1/\lambda +s$, however, the definition of a conditional expectation implies that $E(E(X\vert X>s))=E(X)=1/\lambda\ne E(1/\lambda +s)=1/\lambda +s$. (For any $\sigma$-field $\mathcal{F}$, it is a fact that $E(E(X\vert\mathcal{F}))=E(X)$, so here we just have $\mathcal{F}=\sigma(\{X>s\})=\{\emptyset, \Omega, \{X>s\}, \{X\le s\}\}.)$ Why is there a discrepancy here? There shouldn't be...I'm probably missing something, but I don't know what. EDIT: I suppose the discrepancy could be if $E[\cdot \vert X>s]$ was meant to be the expectation of $X$ with respect to the probability measure $P(\cdot \vert X>s)$. However, does one know what the conditional expectation $E[X\vert X>s]$ should be as a RV?","Let $X$ be an exponential random variable with parameter $\lambda$. For any fixed $s>0$, I would like to compute $E(X\vert X>s)$. I have read in multiple places that this conditional expectation is equal to $1/\lambda +s$, however, the definition of a conditional expectation implies that $E(E(X\vert X>s))=E(X)=1/\lambda\ne E(1/\lambda +s)=1/\lambda +s$. (For any $\sigma$-field $\mathcal{F}$, it is a fact that $E(E(X\vert\mathcal{F}))=E(X)$, so here we just have $\mathcal{F}=\sigma(\{X>s\})=\{\emptyset, \Omega, \{X>s\}, \{X\le s\}\}.)$ Why is there a discrepancy here? There shouldn't be...I'm probably missing something, but I don't know what. EDIT: I suppose the discrepancy could be if $E[\cdot \vert X>s]$ was meant to be the expectation of $X$ with respect to the probability measure $P(\cdot \vert X>s)$. However, does one know what the conditional expectation $E[X\vert X>s]$ should be as a RV?",,"['probability', 'probability-theory', 'conditional-expectation']"
52,Conditional Probability of Good vs Bad Drivers,Conditional Probability of Good vs Bad Drivers,,"I have a problem I'm working on that I'm not quite getting. The problem starts as such: Suppose that there are two types of drivers: good drivers and bad drivers. Let G be the event that a particular man is a good driver, A be the event that he gets into a car accident next year, and B be the event that he gets into a car accident the following year. Let P(G) = g and P(A|G) = P(B|G) = p1,P(A|G^c) = P(B|G^c) = p2, with p1 < p2. Suppose that given the information of whether or not the man is a good driver, A and B are independent. (for simplicity and to avoid being morbid, assume that the accidents being considered are minor and wouldn’t make the man unable to drive). (a) Explain intuitively whether or not A and B are independent. (b) Find P(G|A^c) (c) Find P(B|A^c) I am looking for hints on how to start or think about this problem, as I'm quite stuck at the moment. My work is as follows thus far: A) If a driver gets into a car crash in the first year (meaning event A occurs, this means that it is more likely that the driver is a bad driver. Hence, it is more likely that event B occurs, meaning they are not independent. B) I immediately expand our desired probability as follows: $$P(G|A^c) = \frac{P(A^c|G)P(G)}{P(A^c)}$$ Using LOTP, we can expand the denominator as such: $$P(G|A^c) = \frac{P(A^c|G)*g}{P(A^c|G)*g + P(A^c|G^c)P(G^c)}$$ Any hints?","I have a problem I'm working on that I'm not quite getting. The problem starts as such: Suppose that there are two types of drivers: good drivers and bad drivers. Let G be the event that a particular man is a good driver, A be the event that he gets into a car accident next year, and B be the event that he gets into a car accident the following year. Let P(G) = g and P(A|G) = P(B|G) = p1,P(A|G^c) = P(B|G^c) = p2, with p1 < p2. Suppose that given the information of whether or not the man is a good driver, A and B are independent. (for simplicity and to avoid being morbid, assume that the accidents being considered are minor and wouldn’t make the man unable to drive). (a) Explain intuitively whether or not A and B are independent. (b) Find P(G|A^c) (c) Find P(B|A^c) I am looking for hints on how to start or think about this problem, as I'm quite stuck at the moment. My work is as follows thus far: A) If a driver gets into a car crash in the first year (meaning event A occurs, this means that it is more likely that the driver is a bad driver. Hence, it is more likely that event B occurs, meaning they are not independent. B) I immediately expand our desired probability as follows: $$P(G|A^c) = \frac{P(A^c|G)P(G)}{P(A^c)}$$ Using LOTP, we can expand the denominator as such: $$P(G|A^c) = \frac{P(A^c|G)*g}{P(A^c|G)*g + P(A^c|G^c)P(G^c)}$$ Any hints?",,"['probability', 'bayesian']"
53,probabilities on multiple choice questions,probabilities on multiple choice questions,,"I am setting a series of multiple choice questions. I have a range of option answers from 2 to 7. Some questions only have a single correct answer and others have more.  I can tell the person taking the test what number are correct, or I can just tell them to 'select all that are correct'. Is there a difference in the probability of someone guessing correctly between the two scenarios? I am, by the way, absolutely clueless at maths so please dumb your answer down to the lowest common denominator! eg Yes- if you tell them there are two correct answers it is xx:yy and if you don't tell them it is aa:bb. Thank you.","I am setting a series of multiple choice questions. I have a range of option answers from 2 to 7. Some questions only have a single correct answer and others have more.  I can tell the person taking the test what number are correct, or I can just tell them to 'select all that are correct'. Is there a difference in the probability of someone guessing correctly between the two scenarios? I am, by the way, absolutely clueless at maths so please dumb your answer down to the lowest common denominator! eg Yes- if you tell them there are two correct answers it is xx:yy and if you don't tell them it is aa:bb. Thank you.",,['probability']
54,Probability function for distance d from the center of a point picked at random in a unit disk,Probability function for distance d from the center of a point picked at random in a unit disk,,"Assume there is a unit disk with radius = 1 and centered at $C$. Randomly and uniformly pick a point $P$ in the disk. What is the expected distance between $C$ and $P$? Solution: Since $P$ is $\bf{Uniformly Distributed}$, we know the probability is $\frac{1}{\pi}$, use polar coordinates substitution $x = r\cos{\theta}$ and $y = r\sin{\theta}$, we know $E[\sqrt{x^2+y^2}] = \frac{1}{\pi}\int_{0}^{2\pi}\int_0^1r*rdrd\theta = \frac{2}{3} $ Here is the problem. How do we generate a uniformly randomly distributed $P$ in real life? At my original thinking, there are two independent variables, $\theta$ and $r$, every point in the disk can be represented by these two variables. Thus, we uniformly pick an angle from $[0, 2\pi)$ and distance from $[0, 1)$. But in this way, the probability of choosing a point becomes $\frac{1}{2\pi}*1=\frac{1}{2\pi}$ , which is different from $\frac{1}{\pi}$ that I claimed before. Also, in this setup, the expected distance from any point to the center becomes $\frac{1}{2}$, since it is $[0, 1)$ uniform distribution. This contradiction gives me trouble and I can only conclude that the distance probability isn't uniform $[0, 1)$ distributed. Actually, from this link http://mathworld.wolfram.com/DiskPointPicking.html it actually says that ""The probability function for distance d from the center of a point picked at random in a unit disk is $P(d) = 2d$. Indeed, if this is the probability function for distance, the expectation is easy to calculate, $\int_0^1 2rdr=\frac{2}{3}$, which is the same as before. Also, the total probability becomes $\int_0^{2\pi} \int_0^1 2r(\frac{1}{2\pi})rdrd\theta = 1$. I know why $dxdy = rdrd\theta$ when transforming $x,y$ to $r, \theta$, but it is not that easy to imagine the distance is not uniformly distributed. Can someone give an easy to understand explanation? Follow-up question, what if the shape of object is more complicated? As an example, if on x-y plane, I draw a equilateral triangle and be asked to uniformly pick a point inside the triangle, how to do it? Previously, I was thinking use rotation matrix. Give the vector representation of two sides, each decides an angle (uniformly between $[0,\frac{\pi}{3}]]$ to rotate. But now I'm very worried that this way, it cannot generate really uniformed distributed points. What if the triangle is not symmetric?","Assume there is a unit disk with radius = 1 and centered at $C$. Randomly and uniformly pick a point $P$ in the disk. What is the expected distance between $C$ and $P$? Solution: Since $P$ is $\bf{Uniformly Distributed}$, we know the probability is $\frac{1}{\pi}$, use polar coordinates substitution $x = r\cos{\theta}$ and $y = r\sin{\theta}$, we know $E[\sqrt{x^2+y^2}] = \frac{1}{\pi}\int_{0}^{2\pi}\int_0^1r*rdrd\theta = \frac{2}{3} $ Here is the problem. How do we generate a uniformly randomly distributed $P$ in real life? At my original thinking, there are two independent variables, $\theta$ and $r$, every point in the disk can be represented by these two variables. Thus, we uniformly pick an angle from $[0, 2\pi)$ and distance from $[0, 1)$. But in this way, the probability of choosing a point becomes $\frac{1}{2\pi}*1=\frac{1}{2\pi}$ , which is different from $\frac{1}{\pi}$ that I claimed before. Also, in this setup, the expected distance from any point to the center becomes $\frac{1}{2}$, since it is $[0, 1)$ uniform distribution. This contradiction gives me trouble and I can only conclude that the distance probability isn't uniform $[0, 1)$ distributed. Actually, from this link http://mathworld.wolfram.com/DiskPointPicking.html it actually says that ""The probability function for distance d from the center of a point picked at random in a unit disk is $P(d) = 2d$. Indeed, if this is the probability function for distance, the expectation is easy to calculate, $\int_0^1 2rdr=\frac{2}{3}$, which is the same as before. Also, the total probability becomes $\int_0^{2\pi} \int_0^1 2r(\frac{1}{2\pi})rdrd\theta = 1$. I know why $dxdy = rdrd\theta$ when transforming $x,y$ to $r, \theta$, but it is not that easy to imagine the distance is not uniformly distributed. Can someone give an easy to understand explanation? Follow-up question, what if the shape of object is more complicated? As an example, if on x-y plane, I draw a equilateral triangle and be asked to uniformly pick a point inside the triangle, how to do it? Previously, I was thinking use rotation matrix. Give the vector representation of two sides, each decides an angle (uniformly between $[0,\frac{\pi}{3}]]$ to rotate. But now I'm very worried that this way, it cannot generate really uniformed distributed points. What if the triangle is not symmetric?",,"['probability', 'simulation']"
55,Explain whether or not the three probabilities add to unity. Combinations with replacement?,Explain whether or not the three probabilities add to unity. Combinations with replacement?,,Problem (1): A storage contains 200 computers; 5 are defective and 195 are fine. Two computers are selected randomly with replacement . Calculate the probability that neither computer is defective (correct?) $$\frac {\binom {195+2-1}{2}}{\binom {200+2-1}{2}}$$ Calculate the probability that exactly one computer is defective (correct?) $$\frac {\binom {5+1-1}{1}\binom {195+1-1}{1}}{\binom {200+2-1}{2}}$$ Calculate the probability that both computers are defective (correct?) $$\frac {\binom {5+2-1}{2}}{\binom {200+2-1}{2}}$$ Explain whether or not the three probabilities you have just calculated should add to unity Question 4 I don't really understand. Problem (2): ... without replacement. Calculate the probability that neither computer is defective $$\frac {\binom {195}{2}}{\binom {200}{2}}$$ Calculate the probability that exactly one computer is defective $$\frac {\binom {5}{1}\binom {195}{1}}{\binom {200}{2}}$$ Calculate the probability that both computers are defective $$\frac {\binom {5}{2}}{\binom {200}{2}}$$ Explain whether or not the three probabilities you have just calculated should add to unity All three add to 1. Should these three add to unity because of their mutual exclusiveness and exhaustiveness? Please clarify my thoughts. Edited: Much thanks for every single answer and comment! In the mean time I came up with the tree diagram to visualize the solutions:,Problem (1): A storage contains 200 computers; 5 are defective and 195 are fine. Two computers are selected randomly with replacement . Calculate the probability that neither computer is defective (correct?) $$\frac {\binom {195+2-1}{2}}{\binom {200+2-1}{2}}$$ Calculate the probability that exactly one computer is defective (correct?) $$\frac {\binom {5+1-1}{1}\binom {195+1-1}{1}}{\binom {200+2-1}{2}}$$ Calculate the probability that both computers are defective (correct?) $$\frac {\binom {5+2-1}{2}}{\binom {200+2-1}{2}}$$ Explain whether or not the three probabilities you have just calculated should add to unity Question 4 I don't really understand. Problem (2): ... without replacement. Calculate the probability that neither computer is defective $$\frac {\binom {195}{2}}{\binom {200}{2}}$$ Calculate the probability that exactly one computer is defective $$\frac {\binom {5}{1}\binom {195}{1}}{\binom {200}{2}}$$ Calculate the probability that both computers are defective $$\frac {\binom {5}{2}}{\binom {200}{2}}$$ Explain whether or not the three probabilities you have just calculated should add to unity All three add to 1. Should these three add to unity because of their mutual exclusiveness and exhaustiveness? Please clarify my thoughts. Edited: Much thanks for every single answer and comment! In the mean time I came up with the tree diagram to visualize the solutions:,,"['probability', 'combinatorics']"
56,proof of product rule on conditonal probabilitiy,proof of product rule on conditonal probabilitiy,,"From the basic product rule on conditional probability, we know the following: p(x,y) = P(x|y)P(y). But I cannot understand this formula: p(x,y|z) = p(x|y,z)p(y|z). I have tried to prove this as: p(x,y|z) = p(x|y|z)p(y|z) But i am confused on p(x|y|z)[don't know this notation exists or not.]. And if p(x|y|z) exists then again confused why p(x|y|z) = p(x|y,z)","From the basic product rule on conditional probability, we know the following: p(x,y) = P(x|y)P(y). But I cannot understand this formula: p(x,y|z) = p(x|y,z)p(y|z). I have tried to prove this as: p(x,y|z) = p(x|y|z)p(y|z) But i am confused on p(x|y|z)[don't know this notation exists or not.]. And if p(x|y|z) exists then again confused why p(x|y|z) = p(x|y,z)",,"['probability', 'statistics']"
57,Monotone class theorem and measurability of random variables,Monotone class theorem and measurability of random variables,,"Consider a family $f_i$, $i \in I$, of mappings of a set $\Omega$ into measurable spaces $(E_i,\mathcal{E}_i)$. We assume that for each $i \in I$ there is a subclass $\mathcal{N}_i$ of $\mathcal{E}_i$ , closed under finite intersections and such that $\sigma(\mathcal{N}_i) = \mathcal{E}_i$. Let  $$ \mathcal{N} := \bigg\{ \bigcap_{j \in J} f_j^{-1}(A_j) \colon A_j \in \mathcal{N}_j, J \subseteq I \text{ finite}\bigg\}, $$ then by the Monotone Class Theorem we have $\sigma(\mathcal{N}) = \sigma(f_i \colon i \in I)$. $\qquad$ $(\ast)$ I don't see why $(\ast)$ is an immediate consequence of the Monotone Class Theorem , which states the following: Monotone Class Theorem. Let $\mathcal{M}$ be a collection of subsets of a set $\Omega$ such that $\Omega \in \mathcal{M}$, if $A,B \in \mathcal{M}$ and $A \subseteq B$, then $B \setminus A \in \mathcal{M}$, if $\{A_n\}_{n \geq 1}$ is an increasing sequence of elements of $\mathcal{M}$, then $\bigcup_{n \geq 1} A_n \in \mathcal{M}$. If $\mathcal{F} \subseteq \mathcal{M}$, where $\mathcal{F}$ is closed under finite intersections, then $\sigma(\mathcal{F}) \subseteq \mathcal{M}$. My attempt of proof of $(\ast)$: I know that $\sigma(f_i \colon i \in I) = \sigma(\mathcal{G})$, where $$ \mathcal{G} = \bigcup_{i \in I} f_i^{-1}(\mathcal{N}_i). $$ At this point we only need that $\sigma(\mathcal{N}_i) = \mathcal{E}_i$ but we don't use that $\mathcal{N}_i$ is closed under finite intersections. Now we want to show $\sigma(\mathcal{N}) = \sigma(\mathcal{G})$. It is clear that $\mathcal{N} \subseteq \sigma(\mathcal{G})$ and hence $\sigma(\mathcal{N}) \subseteq \sigma(\mathcal{G})$. Thus it remains to show that $\mathcal{G} \subseteq \sigma(\mathcal{N})$. At this point I got stuck and I don't see how to use the Monotone Class Theorem .","Consider a family $f_i$, $i \in I$, of mappings of a set $\Omega$ into measurable spaces $(E_i,\mathcal{E}_i)$. We assume that for each $i \in I$ there is a subclass $\mathcal{N}_i$ of $\mathcal{E}_i$ , closed under finite intersections and such that $\sigma(\mathcal{N}_i) = \mathcal{E}_i$. Let  $$ \mathcal{N} := \bigg\{ \bigcap_{j \in J} f_j^{-1}(A_j) \colon A_j \in \mathcal{N}_j, J \subseteq I \text{ finite}\bigg\}, $$ then by the Monotone Class Theorem we have $\sigma(\mathcal{N}) = \sigma(f_i \colon i \in I)$. $\qquad$ $(\ast)$ I don't see why $(\ast)$ is an immediate consequence of the Monotone Class Theorem , which states the following: Monotone Class Theorem. Let $\mathcal{M}$ be a collection of subsets of a set $\Omega$ such that $\Omega \in \mathcal{M}$, if $A,B \in \mathcal{M}$ and $A \subseteq B$, then $B \setminus A \in \mathcal{M}$, if $\{A_n\}_{n \geq 1}$ is an increasing sequence of elements of $\mathcal{M}$, then $\bigcup_{n \geq 1} A_n \in \mathcal{M}$. If $\mathcal{F} \subseteq \mathcal{M}$, where $\mathcal{F}$ is closed under finite intersections, then $\sigma(\mathcal{F}) \subseteq \mathcal{M}$. My attempt of proof of $(\ast)$: I know that $\sigma(f_i \colon i \in I) = \sigma(\mathcal{G})$, where $$ \mathcal{G} = \bigcup_{i \in I} f_i^{-1}(\mathcal{N}_i). $$ At this point we only need that $\sigma(\mathcal{N}_i) = \mathcal{E}_i$ but we don't use that $\mathcal{N}_i$ is closed under finite intersections. Now we want to show $\sigma(\mathcal{N}) = \sigma(\mathcal{G})$. It is clear that $\mathcal{N} \subseteq \sigma(\mathcal{G})$ and hence $\sigma(\mathcal{N}) \subseteq \sigma(\mathcal{G})$. Thus it remains to show that $\mathcal{G} \subseteq \sigma(\mathcal{N})$. At this point I got stuck and I don't see how to use the Monotone Class Theorem .",,"['probability', 'probability-theory', 'measure-theory', 'random-variables', 'monotone-class-theorem']"
58,Expected length of runs in a binary string,Expected length of runs in a binary string,,"Question This is a math of the Introduction to Probability Models (11th edition) written by Sheldon M Ross. Runs of 0s or 1s follow a geometric distribution. The solution I found in the solution manual: Answer Here conditioning has been applied on the first bit. So E[L1|X = 0] should be (1/p) - 1 , as the expected value of a geometric random variable is 1/p (where p is the probability of success). Here the expected length of first run given that first bit is zero is 1/p ""including the first 1 after the run of zeroes"". So I have written (1/p) - 1 , excluding the first 1 after the run of zeroes. But in the solution manual, it is (1/p). Am I missing something? If I think somewhat different (not conditioning on the first bit), the first run can contain either all os or all 1s. If it contains all 0s, the expected length should be (1/p) - 1 or (1-p)/p , and if it contains all 1s, the expected length should be 1/(1-p) - 1 or p/(1-p) . In the first case, the success of a geometric random variable is characterized by the presence of a 1 after the first run ends. Same type of logic applies for the run of 1s. Then my solution matches with the solution of the picture. Can anyone please point out what I am missing?","Question This is a math of the Introduction to Probability Models (11th edition) written by Sheldon M Ross. Runs of 0s or 1s follow a geometric distribution. The solution I found in the solution manual: Answer Here conditioning has been applied on the first bit. So E[L1|X = 0] should be (1/p) - 1 , as the expected value of a geometric random variable is 1/p (where p is the probability of success). Here the expected length of first run given that first bit is zero is 1/p ""including the first 1 after the run of zeroes"". So I have written (1/p) - 1 , excluding the first 1 after the run of zeroes. But in the solution manual, it is (1/p). Am I missing something? If I think somewhat different (not conditioning on the first bit), the first run can contain either all os or all 1s. If it contains all 0s, the expected length should be (1/p) - 1 or (1-p)/p , and if it contains all 1s, the expected length should be 1/(1-p) - 1 or p/(1-p) . In the first case, the success of a geometric random variable is characterized by the presence of a 1 after the first run ends. Same type of logic applies for the run of 1s. Then my solution matches with the solution of the picture. Can anyone please point out what I am missing?",,['probability']
59,Exercise on Conditional probability,Exercise on Conditional probability,,"I'm doing the following exercise: You have 2 boxs filled with balls. In the first box you have 30% white, 30% black, 20% green and 20% blu balls. In the second box you have 25% white, 25% black, 25% green and 25% blu balls.(I assume that the quantity of balls are the same, for example 100 in the first one and 100 in the second one) Calculate: The probability of have extracted a blue ball, from a random box if you extracted a blue ball, what is the probability that you extracted it from the first box? If you take a random ball from the first box and a random ball from the second box, what is the probability that the ball are the same? That's what I did: 1) $B=$probability of a blu ball, $A$=first box, $\neg A$= second box $$P(B)=P(B|A)P(A)+P(B|\neg A)P(\neg A)=20\%*\frac{1}{2}+25\%*\frac{1}{2}=22.5\%$$ 2) $B=$probability of a blu ball, $A=$first box $$P(A|B)=\frac{P(B|A)P(A)}{P(B)}=\frac{20\%*\frac{1}{2}}{22.5\%}=0.88$$ I think that what I did until now is right. But know I dont know how to calculate the third point.  Can you give me some hits? EDIT: I just noticed that on the 2nd point if I switch the box A with the box ¬A I get 0.55 (for the first) 0.88 (for the second). Isn't it strange? in the first box you have less of that kind of ball (20%) in the second you have more(25%). Shouldn't I get an highter probability if I use ¬A instead of A?","I'm doing the following exercise: You have 2 boxs filled with balls. In the first box you have 30% white, 30% black, 20% green and 20% blu balls. In the second box you have 25% white, 25% black, 25% green and 25% blu balls.(I assume that the quantity of balls are the same, for example 100 in the first one and 100 in the second one) Calculate: The probability of have extracted a blue ball, from a random box if you extracted a blue ball, what is the probability that you extracted it from the first box? If you take a random ball from the first box and a random ball from the second box, what is the probability that the ball are the same? That's what I did: 1) $B=$probability of a blu ball, $A$=first box, $\neg A$= second box $$P(B)=P(B|A)P(A)+P(B|\neg A)P(\neg A)=20\%*\frac{1}{2}+25\%*\frac{1}{2}=22.5\%$$ 2) $B=$probability of a blu ball, $A=$first box $$P(A|B)=\frac{P(B|A)P(A)}{P(B)}=\frac{20\%*\frac{1}{2}}{22.5\%}=0.88$$ I think that what I did until now is right. But know I dont know how to calculate the third point.  Can you give me some hits? EDIT: I just noticed that on the 2nd point if I switch the box A with the box ¬A I get 0.55 (for the first) 0.88 (for the second). Isn't it strange? in the first box you have less of that kind of ball (20%) in the second you have more(25%). Shouldn't I get an highter probability if I use ¬A instead of A?",,"['probability', 'probability-theory']"
60,Are probability distributions for sample or population?,Are probability distributions for sample or population?,,"When we take a sample, does the relative frequencies of sample  follow sample probability distributions or the population probability distribution ? Also, why does events have to follow a certain distribution ? What natural laws force them to follow those underlying distribution ?","When we take a sample, does the relative frequencies of sample  follow sample probability distributions or the population probability distribution ? Also, why does events have to follow a certain distribution ? What natural laws force them to follow those underlying distribution ?",,"['probability', 'probability-theory', 'statistics', 'probability-distributions', 'statistical-inference']"
61,Odds of rolling $K$ or fewer of a particular die face with $N$ dice.,Odds of rolling  or fewer of a particular die face with  dice.,K N,"I have a real world problem that boils down to this: I have $S$ sensors that each send a message once every two minutes to the server.  Each sensor sends its data at a time uniformly at random within the two minute period, and is independent of all other sensors. The server can only receive $R$ messages per second (if more are sent errors result) How large must $R$ be to ensure that no more than $R$ messages are received within one second 99.995% of the time? Equivalently, if I roll $S$ dice, each with 120 different faces, how large must $R$ be so that 99.995% of the time no more than $R$ of any given face comes up?","I have a real world problem that boils down to this: I have $S$ sensors that each send a message once every two minutes to the server.  Each sensor sends its data at a time uniformly at random within the two minute period, and is independent of all other sensors. The server can only receive $R$ messages per second (if more are sent errors result) How large must $R$ be to ensure that no more than $R$ messages are received within one second 99.995% of the time? Equivalently, if I roll $S$ dice, each with 120 different faces, how large must $R$ be so that 99.995% of the time no more than $R$ of any given face comes up?",,"['probability', 'combinatorics', 'dice']"
62,weighted sum of independent indicator random variables,weighted sum of independent indicator random variables,,"I am trying to prove the following claim: Let $v \in \mathbb R_{\ge 0}^n $ such that $||v||_1 = 1$ (i.e. $\sum_{i=1}^n  v_i = 1$) and let $w \in \{0, 1\}^n $ be a random vector so that   each $w_i$ is $1$ with probability $ \frac{1}{3} $ and $0$ with   probability $\frac{2}{3}$ with all choices being independent. Then $\mathrm {Pr}(w \cdot v \ge 1/3) \ge \frac{1}{3}$ I know by linearity of expectation that $E[w \cdot v] = 1/3$, so it seems very intuitive. The result is obvious for $v=(1, 0, 0, \ldots ,0)$ but I am not sure how to prove it for the general case.","I am trying to prove the following claim: Let $v \in \mathbb R_{\ge 0}^n $ such that $||v||_1 = 1$ (i.e. $\sum_{i=1}^n  v_i = 1$) and let $w \in \{0, 1\}^n $ be a random vector so that   each $w_i$ is $1$ with probability $ \frac{1}{3} $ and $0$ with   probability $\frac{2}{3}$ with all choices being independent. Then $\mathrm {Pr}(w \cdot v \ge 1/3) \ge \frac{1}{3}$ I know by linearity of expectation that $E[w \cdot v] = 1/3$, so it seems very intuitive. The result is obvious for $v=(1, 0, 0, \ldots ,0)$ but I am not sure how to prove it for the general case.",,"['probability', 'probability-distributions']"
63,Binomial coefficients equality or maybe probability,Binomial coefficients equality or maybe probability,,"Let $m,n$ be positive integers.  Evaluate the following expression: $$ F(m,n) = \sum\limits_{i=0}^n\frac{\binom{m+i}{i}}{2^{m+i+1}}+ \sum\limits_{i=0}^m\frac{\binom{n+i}{i}}{2^{n+i+1}}. $$ Calclulations give the hypothesis that $$F(m,n)=1,$$ for all positive integers $m,n$. Also if $m=n$, then $$ F(m,m) = \sum\limits_{i=0}^m\frac{\binom{m+i}{i}}{2^{m+i}} = \sum\limits_{i=0}^m\frac{\binom{m+i}{m}}{2^{m+i}}.  $$ The numerator of every summand is equal to the number of $m$-subsets in $m+i$-set and denominator is equal to the number of subsets in $m+i$-set. So, I think it maybe the key to solution.","Let $m,n$ be positive integers.  Evaluate the following expression: $$ F(m,n) = \sum\limits_{i=0}^n\frac{\binom{m+i}{i}}{2^{m+i+1}}+ \sum\limits_{i=0}^m\frac{\binom{n+i}{i}}{2^{n+i+1}}. $$ Calclulations give the hypothesis that $$F(m,n)=1,$$ for all positive integers $m,n$. Also if $m=n$, then $$ F(m,m) = \sum\limits_{i=0}^m\frac{\binom{m+i}{i}}{2^{m+i}} = \sum\limits_{i=0}^m\frac{\binom{m+i}{m}}{2^{m+i}}.  $$ The numerator of every summand is equal to the number of $m$-subsets in $m+i$-set and denominator is equal to the number of subsets in $m+i$-set. So, I think it maybe the key to solution.",,"['probability', 'combinatorics', 'binomial-coefficients']"
64,Question about the relation between Expectation and Covariance,Question about the relation between Expectation and Covariance,,"I have a question regarding the relationship between the Expectation $E(X)$ and Covariance $Cov(X, Y)$. For reference, Wolfram MathWorld defines Expectation for a single discrete random variable as: $$E(f(X)) = \sum_{x}f(x)P(x)\qquad\qquad\qquad \cdots\qquad(i)$$ and Covariance of two discrete random variables as: $$Cov(X, Y) = E(XY) - E(X)E(Y)\qquad \cdots\qquad(ii)$$ But it also states the Covariance explicitly as: $$Cov(X, Y) = \sum_{i=0}^N\frac{(x_i - x_a)(y_i - y_a)}{N}\qquad \cdots\qquad(iii)$$ How do you get (iii) from (i) and (ii)? What happened to the P(x) and P(y)?","I have a question regarding the relationship between the Expectation $E(X)$ and Covariance $Cov(X, Y)$. For reference, Wolfram MathWorld defines Expectation for a single discrete random variable as: $$E(f(X)) = \sum_{x}f(x)P(x)\qquad\qquad\qquad \cdots\qquad(i)$$ and Covariance of two discrete random variables as: $$Cov(X, Y) = E(XY) - E(X)E(Y)\qquad \cdots\qquad(ii)$$ But it also states the Covariance explicitly as: $$Cov(X, Y) = \sum_{i=0}^N\frac{(x_i - x_a)(y_i - y_a)}{N}\qquad \cdots\qquad(iii)$$ How do you get (iii) from (i) and (ii)? What happened to the P(x) and P(y)?",,"['probability', 'statistics', 'expectation', 'covariance']"
65,Which fallacy is this?,Which fallacy is this?,,"Assume that all people are either right-handed or left-handed, and likewise either right-footed or left-footed. 90% of people are right-handed. 90% of right-handed people are right-footed, but only 50% of left-handed people are left-footed as well. Which is more common: left-handedness or left-footedness? STOP here, and have a go at answering that question first before continuing. And then read on: The answer is easy enough to get if you calculate it. Yet it feels counter-intuitive. Two people I tested this on (another used maths and got it right) assumed that because right-footedness is so dominant among right-handers and common among left-handers, that right-footedness should be even more common than right-handedness. There's some sort of fallacy at work here: any idea of what it is?","Assume that all people are either right-handed or left-handed, and likewise either right-footed or left-footed. 90% of people are right-handed. 90% of right-handed people are right-footed, but only 50% of left-handed people are left-footed as well. Which is more common: left-handedness or left-footedness? STOP here, and have a go at answering that question first before continuing. And then read on: The answer is easy enough to get if you calculate it. Yet it feels counter-intuitive. Two people I tested this on (another used maths and got it right) assumed that because right-footedness is so dominant among right-handers and common among left-handers, that right-footedness should be even more common than right-handedness. There's some sort of fallacy at work here: any idea of what it is?",,"['probability', 'percentages']"
66,"Can one determine the joint distribution of $(X,Y)$ from the probability densities of $X$, $Y$, and $X+Y$?","Can one determine the joint distribution of  from the probability densities of , , and ?","(X,Y) X Y X+Y","Can one determine the joint distribution of $(X,Y)$ from the probability densities of $X$, $Y$, and $X+Y$? Here, $X$ and $Y$ are random variables from a sample space $(\Omega, \mathbb{P}) \to \mathbb{R}$. This is NOT a homework question.","Can one determine the joint distribution of $(X,Y)$ from the probability densities of $X$, $Y$, and $X+Y$? Here, $X$ and $Y$ are random variables from a sample space $(\Omega, \mathbb{P}) \to \mathbb{R}$. This is NOT a homework question.",,['probability']
67,Queuing with impatience,Queuing with impatience,,"I am considering simple M/M/1 queue with customer impatience. Customer waits and leaves the system if the delay before service is more than an exponential  wait time. Arrival rate is $\lambda$, service rate is $\mu$, and abandonment rate due to delay is $\lambda_W$.  I assume state probabilities $p_n$ are known. My problem is to find expected wait time given that customer receives service. My approach to the problem is very simple: if customer finds one customer in the system average queueing delay is  $E[S_1 | S_1 < W]$ = $\frac{1}{\mu + \lambda_w}$, where $S_1$ is the random variable of service time (~exp($\mu$)), and $W$ is the exponential delay. Similarly, if customer finds 2 customer in the system average queueing delay is the conditional expectation $E[S_1 + S_2 |S_1 + S_2 < W ] = \frac{1}{\mu + \lambda_w} + \frac{1}{\mu + 2\lambda_w}$ If customer finds $n$ customer  in queue and successfully receives service, total queuing delay until service time begins is the sum of $n$ independent exponentials with rates $\mu +\lambda_w$, $\mu+2\lambda_w$, $\mu +3\lambda_w$...$\mu+n\lambda_w$. Thus, the expectation of queueing delay is $\frac{1}{\mu +\lambda_w}+...+\frac{1}{\mu + n \lambda_w}$ Thus, average delay $\overline{W}_q$  conditioning on states is $\overline{W}_q = \sum_{i=1}^{\infty} E[S_n| S_n <W] p_n $. The result I obtained from this approach does not agree with simulation results. I don`t know where I am doing wrong?","I am considering simple M/M/1 queue with customer impatience. Customer waits and leaves the system if the delay before service is more than an exponential  wait time. Arrival rate is $\lambda$, service rate is $\mu$, and abandonment rate due to delay is $\lambda_W$.  I assume state probabilities $p_n$ are known. My problem is to find expected wait time given that customer receives service. My approach to the problem is very simple: if customer finds one customer in the system average queueing delay is  $E[S_1 | S_1 < W]$ = $\frac{1}{\mu + \lambda_w}$, where $S_1$ is the random variable of service time (~exp($\mu$)), and $W$ is the exponential delay. Similarly, if customer finds 2 customer in the system average queueing delay is the conditional expectation $E[S_1 + S_2 |S_1 + S_2 < W ] = \frac{1}{\mu + \lambda_w} + \frac{1}{\mu + 2\lambda_w}$ If customer finds $n$ customer  in queue and successfully receives service, total queuing delay until service time begins is the sum of $n$ independent exponentials with rates $\mu +\lambda_w$, $\mu+2\lambda_w$, $\mu +3\lambda_w$...$\mu+n\lambda_w$. Thus, the expectation of queueing delay is $\frac{1}{\mu +\lambda_w}+...+\frac{1}{\mu + n \lambda_w}$ Thus, average delay $\overline{W}_q$  conditioning on states is $\overline{W}_q = \sum_{i=1}^{\infty} E[S_n| S_n <W] p_n $. The result I obtained from this approach does not agree with simulation results. I don`t know where I am doing wrong?",,"['probability', 'probability-distributions', 'expectation', 'conditional-expectation', 'queueing-theory']"
68,Moment Generating Function with Taylor Series,Moment Generating Function with Taylor Series,,"I'm studying moment generating function(MGF). In the video it says The MGF of a random variables(r.v.s.) is $M_x(t) = E (e^{tx})$ and for discrete r.v.s. is $M_x(t) = \sum_x e^{tx} P(X=x)$ I do understand that, for example the MGF of Bernoulli (X~Bern(p)) is $M(t) = E(e^{tX}) = P(X=1)*e^t + P(X=0) * 1 = pe^t + q$ What I do not understand is from the book ""Introduction to probability"" by Joseph K. Blitzstein and Jessica Hwang page 256 talks about the topic of ""Moments via derivatives of the MGF"" it says. ""Given the MGF of X, we can get the nth moment of X by evaluating the nth derivative of the MGF at $0: E(X^n) = M^{(n)}(0)$ This can be seen by noting that the Taylor expansion of M(t) about 0 is $$M(t) = \sum_{n=0}^\infty M^{(n)}(0) \frac{t^n}{n!}$$ While on the other hand we also have $$M(t) = E(e^{tX}) = E(\sum_{n=0}^\infty X^n \frac{t^n}{n!})$$ "" How to go from $E(e^{tX}) $ to $ E(\sum_{n=0}^\infty X^n \frac{t^n}{n!})$ ??? I only know taylor series around 0 (Maclaurin series) of $e^x = \sum _{n=0} ^\infty \frac{x^n}{n!} $ but in this case it is $e^{tX}$ which also involve the r.v.s. X. How to calculate that??? how to take derivative?","I'm studying moment generating function(MGF). In the video it says The MGF of a random variables(r.v.s.) is $M_x(t) = E (e^{tx})$ and for discrete r.v.s. is $M_x(t) = \sum_x e^{tx} P(X=x)$ I do understand that, for example the MGF of Bernoulli (X~Bern(p)) is $M(t) = E(e^{tX}) = P(X=1)*e^t + P(X=0) * 1 = pe^t + q$ What I do not understand is from the book ""Introduction to probability"" by Joseph K. Blitzstein and Jessica Hwang page 256 talks about the topic of ""Moments via derivatives of the MGF"" it says. ""Given the MGF of X, we can get the nth moment of X by evaluating the nth derivative of the MGF at $0: E(X^n) = M^{(n)}(0)$ This can be seen by noting that the Taylor expansion of M(t) about 0 is $$M(t) = \sum_{n=0}^\infty M^{(n)}(0) \frac{t^n}{n!}$$ While on the other hand we also have $$M(t) = E(e^{tX}) = E(\sum_{n=0}^\infty X^n \frac{t^n}{n!})$$ "" How to go from $E(e^{tX}) $ to $ E(\sum_{n=0}^\infty X^n \frac{t^n}{n!})$ ??? I only know taylor series around 0 (Maclaurin series) of $e^x = \sum _{n=0} ^\infty \frac{x^n}{n!} $ but in this case it is $e^{tX}$ which also involve the r.v.s. X. How to calculate that??? how to take derivative?",,"['probability', 'sequences-and-series', 'taylor-expansion', 'moment-generating-functions']"
69,How would you discover the normal distribution?,How would you discover the normal distribution?,,"What is the simplest or easiest or most clear way that a mathematician could discover the normal distribution and the central limit theorem? The derivation does not need to be rigorous.  (Much of calculus was discovered and understood clearly before rigorous proofs were provided, and maybe a similar thing is true for the central limit theorem.) It's ok if the answer is not historically accurate.  (But I'd also be interested in knowing how the normal distribution was discovered historically.) Is there a viewpoint that makes the central limit theorem intuitive or ""obvious""?","What is the simplest or easiest or most clear way that a mathematician could discover the normal distribution and the central limit theorem? The derivation does not need to be rigorous.  (Much of calculus was discovered and understood clearly before rigorous proofs were provided, and maybe a similar thing is true for the central limit theorem.) It's ok if the answer is not historically accurate.  (But I'd also be interested in knowing how the normal distribution was discovered historically.) Is there a viewpoint that makes the central limit theorem intuitive or ""obvious""?",,['probability']
70,Martingales composed with stopping times,Martingales composed with stopping times,,"Let $(\Omega, \mathcal{F}, (\mathcal{F}_t),P)$ be a filtered probability space and $M_{t}$ be a continuous-time martingale. Suppose $\tau: \Omega \to \mathbb{R}$ is a stopping time with respect to $\mathcal{F}_t$. One often sees the composed functions $M_\tau : \Omega \to \mathbb{R}$, given by $M_{\tau}(\omega) = M_{\tau(\omega)}(\omega)$ being treated as random variables, but why are they measurable? Thanks.","Let $(\Omega, \mathcal{F}, (\mathcal{F}_t),P)$ be a filtered probability space and $M_{t}$ be a continuous-time martingale. Suppose $\tau: \Omega \to \mathbb{R}$ is a stopping time with respect to $\mathcal{F}_t$. One often sees the composed functions $M_\tau : \Omega \to \mathbb{R}$, given by $M_{\tau}(\omega) = M_{\tau(\omega)}(\omega)$ being treated as random variables, but why are they measurable? Thanks.",,"['probability', 'martingales']"
71,Simple yet confusing probability problem.,Simple yet confusing probability problem.,,A bag contains x green candies and y red candies. A candy is selected at random from the bag and it's color is noted. It is then replaced into the bag with an additional ten candies of the same color. A second candy is then randomly chosen. Find the probability that the second candy is red. I think that the answer is y/(x+y+10) but am unsure. Also where would you recommend to get more practice on probality problems,A bag contains x green candies and y red candies. A candy is selected at random from the bag and it's color is noted. It is then replaced into the bag with an additional ten candies of the same color. A second candy is then randomly chosen. Find the probability that the second candy is red. I think that the answer is y/(x+y+10) but am unsure. Also where would you recommend to get more practice on probality problems,,['probability']
72,Do probabilities imply a static underlying system?,Do probabilities imply a static underlying system?,,"I'm trying to wrap my head around how to interpret probabilities. Specifically, I work in sociolinguistics where language items (e.g. presence or absence of R at the end of words in English) are treated as variables whose realizations are associated with extra-linguistic variables. One takes a corpus of speech, finds all the tokens of a particular variable, determines the relative frequencies for the realizations of that variable and analyzes it according to various social variables (e.g. age of speakers, sex of speakers, etc.). The relative frequencies are typically converted to probabilities which are thought by some to represent the grammatical rules of the speakers in the corpus. The problem I have with this is at the point where relative frequencies are converted to probabilities. Language is thought to be constantly changing, but to me, probabilities seem to suggest that there's a static underlying system that generates the data that's being observed. A simple example: if one measures how many nights in a row the moon comes out, one can calculate a relative frequency from that and then go further and calculate a probability that it will come out on any given night. That probability will likely be 100% because what's actually being observed is a static system where the moon is orbiting the Earth according to physical laws. A probability in this case seems to be simply a calculation of an observation of the results produced by a law in action that we have not yet identified. If this is an accurate description of what probabilities actually represent, then does it makes sense to calculate them when working with a system that is constantly changing, e.g. language and its grammatical rules? A grammatical rule that says whether speakers of a particular dialect of English should pronounce an R at the end of words or not is not static; it's expected to change. If a group of people produce an R for 65% of all tokens, that may very well mean that they used to not produce an R but are starting to produce it more and more. I'm not exactly sure that there's utility in converting that relative frequency into a probability if the relative frequency simply represents the state the system was in at the time and not a static system where every single speaker makes sure to produce R 65% of the time and not the rest of the time. If my question points to an ongoing debate in probability theory as opposed to something that's been settled among mathematicians, what are the major arguments for how to interpret probabilities? Are there important articles in the literature that I could read to understand how mathematicians approach this topic?","I'm trying to wrap my head around how to interpret probabilities. Specifically, I work in sociolinguistics where language items (e.g. presence or absence of R at the end of words in English) are treated as variables whose realizations are associated with extra-linguistic variables. One takes a corpus of speech, finds all the tokens of a particular variable, determines the relative frequencies for the realizations of that variable and analyzes it according to various social variables (e.g. age of speakers, sex of speakers, etc.). The relative frequencies are typically converted to probabilities which are thought by some to represent the grammatical rules of the speakers in the corpus. The problem I have with this is at the point where relative frequencies are converted to probabilities. Language is thought to be constantly changing, but to me, probabilities seem to suggest that there's a static underlying system that generates the data that's being observed. A simple example: if one measures how many nights in a row the moon comes out, one can calculate a relative frequency from that and then go further and calculate a probability that it will come out on any given night. That probability will likely be 100% because what's actually being observed is a static system where the moon is orbiting the Earth according to physical laws. A probability in this case seems to be simply a calculation of an observation of the results produced by a law in action that we have not yet identified. If this is an accurate description of what probabilities actually represent, then does it makes sense to calculate them when working with a system that is constantly changing, e.g. language and its grammatical rules? A grammatical rule that says whether speakers of a particular dialect of English should pronounce an R at the end of words or not is not static; it's expected to change. If a group of people produce an R for 65% of all tokens, that may very well mean that they used to not produce an R but are starting to produce it more and more. I'm not exactly sure that there's utility in converting that relative frequency into a probability if the relative frequency simply represents the state the system was in at the time and not a static system where every single speaker makes sure to produce R 65% of the time and not the rest of the time. If my question points to an ongoing debate in probability theory as opposed to something that's been settled among mathematicians, what are the major arguments for how to interpret probabilities? Are there important articles in the literature that I could read to understand how mathematicians approach this topic?",,"['probability', 'probability-theory']"
73,Expected Value and Variance of a Markov Chain,Expected Value and Variance of a Markov Chain,,Say I have a transition matrix $Q = \begin{bmatrix}     1-p       & p \\     p       & 1-p  \end{bmatrix}$ where $0 < p < 1$ for a two state system with states $-1$ and $1.$ Define $X_i$ to be the value of the markov chain at time $i$ (so either $-1$ or $1$). If $\bar{X_i} = \frac{1}{n}\sum_{i = 1}^n X_i$ what is the $\mathbb{E}[\bar{X_i}]$ and $Var[\bar{X_i}]$? I've started off by tackling $\mathbb{E}[\bar{X_i}]$ but it seems to me that this answer depends on whether $n$ is even or odd. Note since this is a Markov chain there is not pairwise independence.,Say I have a transition matrix $Q = \begin{bmatrix}     1-p       & p \\     p       & 1-p  \end{bmatrix}$ where $0 < p < 1$ for a two state system with states $-1$ and $1.$ Define $X_i$ to be the value of the markov chain at time $i$ (so either $-1$ or $1$). If $\bar{X_i} = \frac{1}{n}\sum_{i = 1}^n X_i$ what is the $\mathbb{E}[\bar{X_i}]$ and $Var[\bar{X_i}]$? I've started off by tackling $\mathbb{E}[\bar{X_i}]$ but it seems to me that this answer depends on whether $n$ is even or odd. Note since this is a Markov chain there is not pairwise independence.,,"['probability', 'markov-chains']"
74,The probability that there will be 3 cloves with four leaves?,The probability that there will be 3 cloves with four leaves?,,"Suppose there is $1$% of cloves with four leaves. We pick $100$ cloves. Let $X$ denote the event a clove has four leaves . What's the probability of having 4 cloves? I am wondering which method should I use? 1st possibility: The first such clove has a probability of being picked of $\frac{1}{100}$, the second $\frac{3}{99}$ the third $\frac{2}{98}$ and the last one $\frac{1}{97}$. By multiplying these, I could get the final probability of getting $4$ such cloves. Is this correct? 2nd possibility: My second thought would have been to use the Binomial distribution, although I am unsure I can use it, as we don't repeat the experience several times, we just pick $100$ cloves and see whether it has $4$ four-leave cloves. Any help will be appreciated.","Suppose there is $1$% of cloves with four leaves. We pick $100$ cloves. Let $X$ denote the event a clove has four leaves . What's the probability of having 4 cloves? I am wondering which method should I use? 1st possibility: The first such clove has a probability of being picked of $\frac{1}{100}$, the second $\frac{3}{99}$ the third $\frac{2}{98}$ and the last one $\frac{1}{97}$. By multiplying these, I could get the final probability of getting $4$ such cloves. Is this correct? 2nd possibility: My second thought would have been to use the Binomial distribution, although I am unsure I can use it, as we don't repeat the experience several times, we just pick $100$ cloves and see whether it has $4$ four-leave cloves. Any help will be appreciated.",,['probability']
75,"Let $Z\sim N(0,1)$ be a random variable, then $E[\max\{Z,0\}]$ is?","Let  be a random variable, then  is?","Z\sim N(0,1) E[\max\{Z,0\}]","Let $Z\sim N(0,1)$ be a random variable, then $E[\max\{Z,0\}]$ is ? $\frac{1}{\sqrt{\pi}}$ $\sqrt{\frac{2}{\pi}}$ $\frac{1}{\sqrt{2\pi}}$ $\frac{1}{\pi}$ I know that $E[\max\{Z,Y\}]=\iint \max(z,y)f(z,y) \, dz \, dy$. How to use this in this case, since one variable is zero. Any hint would be helpful. Thanks.","Let $Z\sim N(0,1)$ be a random variable, then $E[\max\{Z,0\}]$ is ? $\frac{1}{\sqrt{\pi}}$ $\sqrt{\frac{2}{\pi}}$ $\frac{1}{\sqrt{2\pi}}$ $\frac{1}{\pi}$ I know that $E[\max\{Z,Y\}]=\iint \max(z,y)f(z,y) \, dz \, dy$. How to use this in this case, since one variable is zero. Any hint would be helpful. Thanks.",,"['probability', 'normal-distribution']"
76,Rolling Dice Probability Question,Rolling Dice Probability Question,,"Rolling a fair dice $7$ times, what is the probability that the total sum is divisible by $3$, and no odd value was rolled? I know that there are $3^6$ possible ways for the first $6$ dice to give only even values, but how many of them are divisible by $3$ ?","Rolling a fair dice $7$ times, what is the probability that the total sum is divisible by $3$, and no odd value was rolled? I know that there are $3^6$ possible ways for the first $6$ dice to give only even values, but how many of them are divisible by $3$ ?",,['probability']
77,"Simple Random Walk, proving variance of the walk at a stopping time","Simple Random Walk, proving variance of the walk at a stopping time",,So the problem is that $S_N$ is a symmetric random walk and $T$ is a bounded stopping time. I have to show that the variance $$\text{Var}\left(S_T\right) = \mathbb{E}\left(S_T^2\right) = \mathbb{E}(T)$$ where $\mathbb{E}$ is the expectation. I know how to show that  $$\text{Var}\left(S_T\right) = \mathbb{E}\left(S_T^2\right)$$  but not sure how to show that this equals $\mathbb{E}(T)$? Any hints or help is much appreciated!,So the problem is that $S_N$ is a symmetric random walk and $T$ is a bounded stopping time. I have to show that the variance $$\text{Var}\left(S_T\right) = \mathbb{E}\left(S_T^2\right) = \mathbb{E}(T)$$ where $\mathbb{E}$ is the expectation. I know how to show that  $$\text{Var}\left(S_T\right) = \mathbb{E}\left(S_T^2\right)$$  but not sure how to show that this equals $\mathbb{E}(T)$? Any hints or help is much appreciated!,,"['probability', 'expectation', 'random-walk', 'stopping-times', 'variance']"
78,Is probability that $x<2$ equal to the probability that $x^2<4$ given $1<x<3$?,Is probability that  equal to the probability that  given ?,x<2 x^2<4 1<x<3,"Assuming $x$ is a real number uniformly distributed over the interval $(1,3).$ so $x^2$ is also uniformly distributed over the interval $(1,9)${As for every $x=a\in (1,3) $ there exists $x^2=a^2\in (1,9)$}. Probability that $x<2$ would be $\frac{1}{2}$ as $x$ can be in $(1,2)$ where sample set of $x$ is $(1,3)$, while probability that $x^2<4$ is $\frac{3}{8}$ as $x^2$ can be in $(1,4)$ where sample set of $x^2$ is $(1,9).$ So why is the probability that $x<2$ different from $x^2<4$ if both are identical?","Assuming $x$ is a real number uniformly distributed over the interval $(1,3).$ so $x^2$ is also uniformly distributed over the interval $(1,9)${As for every $x=a\in (1,3) $ there exists $x^2=a^2\in (1,9)$}. Probability that $x<2$ would be $\frac{1}{2}$ as $x$ can be in $(1,2)$ where sample set of $x$ is $(1,3)$, while probability that $x^2<4$ is $\frac{3}{8}$ as $x^2$ can be in $(1,4)$ where sample set of $x^2$ is $(1,9).$ So why is the probability that $x<2$ different from $x^2<4$ if both are identical?",,"['probability', 'probability-theory']"
79,Simultaneous vs. Successive Choosing,Simultaneous vs. Successive Choosing,,"I will illustrate my question with an example.  Suppose that there are $14$ balls in a box of which 8 are Blue, $4$ are Yellow, and $2$ are Orange.  Suppose we pick two balls from the box and we want to find the probability table of the possible combinations.  In this case the combinations (macrostates) possible are:  $$OO, OY, OB, YY, YB, BB.$$ Now lets separate the problem into two cases. $1)$We pick the balls simultaneously. $2)$We pick the balls one after the other. In the first case the order of choosing does not matter so, for example, in the macrostate $OY$ it seems that there is only one microstate contributing - $OY$.  However in case $2)$ for the same macrostate $OY$ is it correct to say that there are now two microstates contributing $OY,YO$?","I will illustrate my question with an example.  Suppose that there are $14$ balls in a box of which 8 are Blue, $4$ are Yellow, and $2$ are Orange.  Suppose we pick two balls from the box and we want to find the probability table of the possible combinations.  In this case the combinations (macrostates) possible are:  $$OO, OY, OB, YY, YB, BB.$$ Now lets separate the problem into two cases. $1)$We pick the balls simultaneously. $2)$We pick the balls one after the other. In the first case the order of choosing does not matter so, for example, in the macrostate $OY$ it seems that there is only one microstate contributing - $OY$.  However in case $2)$ for the same macrostate $OY$ is it correct to say that there are now two microstates contributing $OY,YO$?",,"['probability', 'probability-theory']"
80,How to determine the average number of dice showing the most common value?,How to determine the average number of dice showing the most common value?,,"Suppose the situation that tossing $n$ dice simultaneously. Each die has $k$ faces. How can I determine the average number of dice showing the most common value among them? For example, $n=10$ and $k=3$, the result of tossing is 1-3-2-1-1-1-2-1-2-3, respectively. The most common value is ""1"" with 5 dice. Trying again, the result is 3-2-3-2-1-3-3-3-3-3. The most common value is ""3"" with 7 dice. Try in this way 1,000 times and average the number of dice showing the most common value. That is $\frac{{5 + 7 +  \cdots }}{{1,000}}$. Do there already exist any function (with parameter $n$, $k$) to determine the average? Note that the number of dice showing ""i"" is not independent of the number of dice showing ""j"" with $i\ne j$ that is because the total number of dice is fixed. I cannot solve this problem because the independent assumption cannot be used. Thank you.","Suppose the situation that tossing $n$ dice simultaneously. Each die has $k$ faces. How can I determine the average number of dice showing the most common value among them? For example, $n=10$ and $k=3$, the result of tossing is 1-3-2-1-1-1-2-1-2-3, respectively. The most common value is ""1"" with 5 dice. Trying again, the result is 3-2-3-2-1-3-3-3-3-3. The most common value is ""3"" with 7 dice. Try in this way 1,000 times and average the number of dice showing the most common value. That is $\frac{{5 + 7 +  \cdots }}{{1,000}}$. Do there already exist any function (with parameter $n$, $k$) to determine the average? Note that the number of dice showing ""i"" is not independent of the number of dice showing ""j"" with $i\ne j$ that is because the total number of dice is fixed. I cannot solve this problem because the independent assumption cannot be used. Thank you.",,"['probability', 'combinatorics', 'average', 'means']"
81,Suspect unfair die,Suspect unfair die,,If I claim to have a fair die that rolls 1-6 uniformly but my die actually only rolls 1-5 uniformly (and never produces a 6) how many rolls would you need to see before you had over 50% confidence that I was messing with you?,If I claim to have a fair die that rolls 1-6 uniformly but my die actually only rolls 1-5 uniformly (and never produces a 6) how many rolls would you need to see before you had over 50% confidence that I was messing with you?,,['probability']
82,Joint distribution between sum and a component of its sum,Joint distribution between sum and a component of its sum,,"Let $X$ and $N$ be independent uniform random variables on $[0,1]$. Define, \begin{equation} Y=X+N \end{equation} I am interested in computing the joint distribution $P_{XY}$. I have the following tried from my side. \begin{equation} P_{XY}(x,y)=P_{X}P_{Y|X}(x,y)=P_{Y|X}(x,y)=P_{N}(y-x) \end{equation} Then, $P_{XY}(x,y)=1$ for $(x,y)\in S:=\{ (x,y): y\ge x, y \le 1+x \}$, defines the distribution. But I see that,  \begin{equation} \iint_S P_{XY}= \frac{1}{2} \neq 1  \end{equation} Where am I wrong? Thank you","Let $X$ and $N$ be independent uniform random variables on $[0,1]$. Define, \begin{equation} Y=X+N \end{equation} I am interested in computing the joint distribution $P_{XY}$. I have the following tried from my side. \begin{equation} P_{XY}(x,y)=P_{X}P_{Y|X}(x,y)=P_{Y|X}(x,y)=P_{N}(y-x) \end{equation} Then, $P_{XY}(x,y)=1$ for $(x,y)\in S:=\{ (x,y): y\ge x, y \le 1+x \}$, defines the distribution. But I see that,  \begin{equation} \iint_S P_{XY}= \frac{1}{2} \neq 1  \end{equation} Where am I wrong? Thank you",,['probability']
83,Doubt in Pratt's Lemma,Doubt in Pratt's Lemma,,"Pratt's Lemma: Let $X_n \leq Y_n \leq Z_n, n=1,2 \cdots$. Suppose with probability $1$, $X_n\rightarrow X, Y_n\rightarrow Y \ \rm{and}  \ Z_n\rightarrow Z $. Show that if $E[Z_n]\rightarrow E[Z]$ and $E[X_n]\rightarrow E[X]$, then $E[Y_n]\rightarrow E[Y]$. Proof: $Z_n-Y_n\geq 0$ and $Y_n-X_n\geq 0$, By Fatou's Lemma, $E(Z-Y)\leq \liminf E(Z_n-Y_n)$, hence $\limsup E(Y_n)\leq E(Y)$. Similary,$E(Y)\leq \liminf E(Y_n)$. Thus, $\lim E(Y_n)=E(Y)$. What I don't understand is that $X$ and $Z$ are not given to be integrable, so how can we cancel $E(Z)$ and $E(X)$?","Pratt's Lemma: Let $X_n \leq Y_n \leq Z_n, n=1,2 \cdots$. Suppose with probability $1$, $X_n\rightarrow X, Y_n\rightarrow Y \ \rm{and}  \ Z_n\rightarrow Z $. Show that if $E[Z_n]\rightarrow E[Z]$ and $E[X_n]\rightarrow E[X]$, then $E[Y_n]\rightarrow E[Y]$. Proof: $Z_n-Y_n\geq 0$ and $Y_n-X_n\geq 0$, By Fatou's Lemma, $E(Z-Y)\leq \liminf E(Z_n-Y_n)$, hence $\limsup E(Y_n)\leq E(Y)$. Similary,$E(Y)\leq \liminf E(Y_n)$. Thus, $\lim E(Y_n)=E(Y)$. What I don't understand is that $X$ and $Z$ are not given to be integrable, so how can we cancel $E(Z)$ and $E(X)$?",,"['probability', 'probability-theory', 'measure-theory']"
84,Why is this expression for $1/e$ so similar to the definition of $e$?,Why is this expression for  so similar to the definition of ?,1/e e,"Consider this probability thought experiment: A barrel is filled with $n$ grains of rice. All grains are white, except for a single grain of brown rice. Pick a grain at random from the barrel. Observe it and put it back in the barrel. Do this $n$ times. For very large values of $n$, what is the probability that you never picked the grain of brown rice? On any given iteration of this procedure, the probability of picking the brown grain is $P_{brown}=\frac{1}{n}$. So the probability of picking a white grain on a given iteration is $1-\frac{1}{n}$. Therefore the probability of picking a white grain $n$ times is: $$ P_n=(1-\frac{1}{n})^n $$ And for very large $n$, we get: $$ P = \lim_{n \to \infty}{(1-\frac{1}{n})^n} $$ Empirically, it's easy to see that this limit is equal to $\frac{1}{e}$. For example, we can plug in $n=999,999,999$ to get: $$ \frac{1}{P_{999,999,999}} = (1-\frac{1}{999,999,999})^{-999,999,999} $$ This equals equals 2.7182817629 , which is within $10^{-7}$ of $e$. Now, notice how similar is the limit mentioned above to one of the most popular definitions of $e$: $$ e \equiv \lim_{n \to \infty}{(1+\frac{1}{n})^n} $$ The only difference is that the ""$-$"" is now a ""$+$"". Here's the question my waste bin and I have been struggling with: how can one prove that $$ \lim_{n \to \infty}{P_n} = e^{-1} $$ or, equivalently, that $$ \lim_{n \to\ \infty}{(1-\frac{1}{n})^n} = \frac{1}{\lim_{n \to \infty}{(1+\frac{1}{n})^n}} $$","Consider this probability thought experiment: A barrel is filled with $n$ grains of rice. All grains are white, except for a single grain of brown rice. Pick a grain at random from the barrel. Observe it and put it back in the barrel. Do this $n$ times. For very large values of $n$, what is the probability that you never picked the grain of brown rice? On any given iteration of this procedure, the probability of picking the brown grain is $P_{brown}=\frac{1}{n}$. So the probability of picking a white grain on a given iteration is $1-\frac{1}{n}$. Therefore the probability of picking a white grain $n$ times is: $$ P_n=(1-\frac{1}{n})^n $$ And for very large $n$, we get: $$ P = \lim_{n \to \infty}{(1-\frac{1}{n})^n} $$ Empirically, it's easy to see that this limit is equal to $\frac{1}{e}$. For example, we can plug in $n=999,999,999$ to get: $$ \frac{1}{P_{999,999,999}} = (1-\frac{1}{999,999,999})^{-999,999,999} $$ This equals equals 2.7182817629 , which is within $10^{-7}$ of $e$. Now, notice how similar is the limit mentioned above to one of the most popular definitions of $e$: $$ e \equiv \lim_{n \to \infty}{(1+\frac{1}{n})^n} $$ The only difference is that the ""$-$"" is now a ""$+$"". Here's the question my waste bin and I have been struggling with: how can one prove that $$ \lim_{n \to \infty}{P_n} = e^{-1} $$ or, equivalently, that $$ \lim_{n \to\ \infty}{(1-\frac{1}{n})^n} = \frac{1}{\lim_{n \to \infty}{(1+\frac{1}{n})^n}} $$",,"['calculus', 'probability']"
85,What is the probability that a quadrilateral inside a square is convex?,What is the probability that a quadrilateral inside a square is convex?,,"Need probability that ABCD is convex if the points have coordinates $(x_1,y_1)(x_2,y_2)(x_3,y_3)(x_4,y_4)$ where $x_i,y_i\in[0,1]$ are uniformly equiprobable. It would be same as the expected value of the green area but I can't get it... I guess Monte Carlo would do the job","Need probability that ABCD is convex if the points have coordinates $(x_1,y_1)(x_2,y_2)(x_3,y_3)(x_4,y_4)$ where $x_i,y_i\in[0,1]$ are uniformly equiprobable. It would be same as the expected value of the green area but I can't get it... I guess Monte Carlo would do the job",,"['probability', 'geometric-probability']"
86,Is $\mathbb{E}[Y|X]$ a random variable?,Is  a random variable?,\mathbb{E}[Y|X],"If X and Y are random variables, is $$\mathbb E[Y|X]$$ a random variable also? Intuitively it seems to be the case, but I can't explain why.","If X and Y are random variables, is $$\mathbb E[Y|X]$$ a random variable also? Intuitively it seems to be the case, but I can't explain why.",,['probability']
87,"Deck of Cards Probability: Two lost cards, one drawn—what is its suit?","Deck of Cards Probability: Two lost cards, one drawn—what is its suit?",,I'm having trouble with a probability problem and would love for an explanation. The problem is as follows: Two cards from an ordinary deck of 52 cards are missing. What is the probability that a random card drawn from the deck is a spade? For whatever reason I can't wrap my head around the change in probability from the random removal—best I've got is 52-choose-50.,I'm having trouble with a probability problem and would love for an explanation. The problem is as follows: Two cards from an ordinary deck of 52 cards are missing. What is the probability that a random card drawn from the deck is a spade? For whatever reason I can't wrap my head around the change in probability from the random removal—best I've got is 52-choose-50.,,['probability']
88,"Probability ""Paradox"" I've been thinking of","Probability ""Paradox"" I've been thinking of",,"Last night I thought about a probablity ""paradox"" that I would like if someone could clarify for me. I couldn't find this version of it anywhere (I thought about it myself) so I hope it isn't something that has been answered before. The paradox goes like this: suppose we have 2 friends, Alice and Bob, and we let them play a game:  the ""game master"" puts a note with a natural number on Alice's forehead, and the same goes for Bob (Those two numbers can be equal, or different). Each of them is requested to state a natural number that is larger (or equals to) the number on their forehead. They win if at least one of them is correct. In the first version of the game, they are both in seperate rooms, and don't know anything about their friend in the other room. In the second version of the game, they are sitting in front of each other, and can see the number on their friend's forehead. Now, intuitively, there should be no difference between their chances of winning the game in both versions, because the number on their friend's forehead gives them absolutly no information about their number . But this is not true. In the second vesrion, they can use the simple strategy of just stating the number on their friend's forehead. It assures winning because one of the two numbers is larger (or equal to) the other one. However, I think it is quite obvious that there is nothing that they can do in the first version. I don't know a lot about probability, but intuitively I believe their winning chance is actually $0$ (is it even something that makes sense?) - stating ""$1$"" isn't really better than stating ""$10^{10^{100}}$"" in any way. So the paradox is: what exactly is the extra information they recieved in the seocnd version? How does this make sense? I'd love to hear your explanations. I think it is a beautiful paradox but I can't think of anything convincing to say about it. Thank you for your time reading the question!","Last night I thought about a probablity ""paradox"" that I would like if someone could clarify for me. I couldn't find this version of it anywhere (I thought about it myself) so I hope it isn't something that has been answered before. The paradox goes like this: suppose we have 2 friends, Alice and Bob, and we let them play a game:  the ""game master"" puts a note with a natural number on Alice's forehead, and the same goes for Bob (Those two numbers can be equal, or different). Each of them is requested to state a natural number that is larger (or equals to) the number on their forehead. They win if at least one of them is correct. In the first version of the game, they are both in seperate rooms, and don't know anything about their friend in the other room. In the second version of the game, they are sitting in front of each other, and can see the number on their friend's forehead. Now, intuitively, there should be no difference between their chances of winning the game in both versions, because the number on their friend's forehead gives them absolutly no information about their number . But this is not true. In the second vesrion, they can use the simple strategy of just stating the number on their friend's forehead. It assures winning because one of the two numbers is larger (or equal to) the other one. However, I think it is quite obvious that there is nothing that they can do in the first version. I don't know a lot about probability, but intuitively I believe their winning chance is actually $0$ (is it even something that makes sense?) - stating ""$1$"" isn't really better than stating ""$10^{10^{100}}$"" in any way. So the paradox is: what exactly is the extra information they recieved in the seocnd version? How does this make sense? I'd love to hear your explanations. I think it is a beautiful paradox but I can't think of anything convincing to say about it. Thank you for your time reading the question!",,"['probability', 'soft-question', 'intuition', 'paradoxes']"
89,Conditional Probability: Two die randomly chosen with red and blue faces,Conditional Probability: Two die randomly chosen with red and blue faces,,"I have been going through and doing some (non-assessed) homework questions, but am getting really stuck on conditional probability. The following problem is one that I simply cannot get my head around. Question: Die A has four red and two blue faces, and die B has two red and four blue faces. One of the dice is selected at random for use. i). What is the probability of red being thrown? ii). If the first two throws resulted in red, what is the probability of red for the third throw? I was able to get i), no trouble, it came out as 1/2. The second part has me entirely lost though.","I have been going through and doing some (non-assessed) homework questions, but am getting really stuck on conditional probability. The following problem is one that I simply cannot get my head around. Question: Die A has four red and two blue faces, and die B has two red and four blue faces. One of the dice is selected at random for use. i). What is the probability of red being thrown? ii). If the first two throws resulted in red, what is the probability of red for the third throw? I was able to get i), no trouble, it came out as 1/2. The second part has me entirely lost though.",,['probability']
90,Probability that a player wins a card guessing game,Probability that a player wins a card guessing game,,"$ \textbf{Question:} $ A card game consists of $ n $ cards $ (n \ge 1), $ one of which is a special card. The cards are shuffled randomly and then turned over one at a time. At any time, a player must guess whether the current card is the special card before it is revealed. The player wins when he correctly guesses the special card. What is the probability that the player wins the game? I approach this problem by letting $ E $ be the event that the current card is a special card and $ F $ be the event that the player will guess the current card is the special card, so finding the probability that the player wins the game means finding the probability that both $ E $ and $ F $ occur. Now I have $ \displaystyle P(EF) = P(E)P(F|E) $ with $ \displaystyle P(E) = \frac{1}{n}, $ but I don't know how to compute $ P(F|E). $ It seems reasonable (to me) that $ P(F|E) = 1 $ since once the player knows apriori that the current card is the special card, he will just make the guess. Suppose that the problem provides an extra information that at any time, the probability that the player guesses the current card is the special card is $ 30\% $ (meaning $ P(F) = 0.3), $ will that change the value of $ P(F|E) $ from $ 1 $ to $ 0.3? $","$ \textbf{Question:} $ A card game consists of $ n $ cards $ (n \ge 1), $ one of which is a special card. The cards are shuffled randomly and then turned over one at a time. At any time, a player must guess whether the current card is the special card before it is revealed. The player wins when he correctly guesses the special card. What is the probability that the player wins the game? I approach this problem by letting $ E $ be the event that the current card is a special card and $ F $ be the event that the player will guess the current card is the special card, so finding the probability that the player wins the game means finding the probability that both $ E $ and $ F $ occur. Now I have $ \displaystyle P(EF) = P(E)P(F|E) $ with $ \displaystyle P(E) = \frac{1}{n}, $ but I don't know how to compute $ P(F|E). $ It seems reasonable (to me) that $ P(F|E) = 1 $ since once the player knows apriori that the current card is the special card, he will just make the guess. Suppose that the problem provides an extra information that at any time, the probability that the player guesses the current card is the special card is $ 30\% $ (meaning $ P(F) = 0.3), $ will that change the value of $ P(F|E) $ from $ 1 $ to $ 0.3? $",,[]
91,Can every factorization be represented by a Bayesian network?,Can every factorization be represented by a Bayesian network?,,"A Bayesian network is defined as a directed acyclic graph with a set of random variables as its nodes, and it satisfies two axioms, 1)  Root nodes (nodes without parents) are independent. 2)  Given a variable $X$ in the network, denote its parents (adjacent nodes with inbound edges to $X$) as $p(X)$. A RV $X$ is conditionally independent from all other RVs on $p(X)$. For example, A Bayesian network represents a factorization of joint distributions. In the following, $\cal p(X)$ is the set of all parents of $X$ in the network. My question is the converse of the above statement , i.e. can every factorization be represented by a Bayesian network ? If not, can anyone help provide a counter example? Thank you!","A Bayesian network is defined as a directed acyclic graph with a set of random variables as its nodes, and it satisfies two axioms, 1)  Root nodes (nodes without parents) are independent. 2)  Given a variable $X$ in the network, denote its parents (adjacent nodes with inbound edges to $X$) as $p(X)$. A RV $X$ is conditionally independent from all other RVs on $p(X)$. For example, A Bayesian network represents a factorization of joint distributions. In the following, $\cal p(X)$ is the set of all parents of $X$ in the network. My question is the converse of the above statement , i.e. can every factorization be represented by a Bayesian network ? If not, can anyone help provide a counter example? Thank you!",,"['probability', 'bayesian', 'bayesian-network']"
92,How to physically model/construct a biased coin?,How to physically model/construct a biased coin?,,"A perfectly unbiased coin is one that has the same probability for heads and tails (i.e., 50%/50%). A perfectly biased coin is one that has (as the name suggests) different probabilities for head than for tails. The design of a perfectly unbiased coin is pretty straightforward: a cylinder with height << radius (h << r). However, I'm wondering how the design for a perfectly biased coin would be. Although I'm not a mathematician, I can intuitively think that a perfectly biased coin (with 60% for heads and 40% for heads) would have the heads circle surface area 60% higher than that of the tails' circle surface area. I don't know how true this is and, if true, I'd like to reach the same conclusion using a mathematical approach. Would this problem be much more difficult if we had, for example, a perfectly biased dice (i.e. cube)? How can I start constructing a mathematical model for biased, throwable, generic objects?","A perfectly unbiased coin is one that has the same probability for heads and tails (i.e., 50%/50%). A perfectly biased coin is one that has (as the name suggests) different probabilities for head than for tails. The design of a perfectly unbiased coin is pretty straightforward: a cylinder with height << radius (h << r). However, I'm wondering how the design for a perfectly biased coin would be. Although I'm not a mathematician, I can intuitively think that a perfectly biased coin (with 60% for heads and 40% for heads) would have the heads circle surface area 60% higher than that of the tails' circle surface area. I don't know how true this is and, if true, I'd like to reach the same conclusion using a mathematical approach. Would this problem be much more difficult if we had, for example, a perfectly biased dice (i.e. cube)? How can I start constructing a mathematical model for biased, throwable, generic objects?",,"['probability', 'probability-theory', 'dice']"
93,Difficulty understanding step in Kac's proof of Feynman-Kac Theorem,Difficulty understanding step in Kac's proof of Feynman-Kac Theorem,,"I am trying to understand a proof of the Feynman-Kac Theorem , as set out in Mark Kac's 1949 paper 'On Distributions of Certain Wiener Functionals' . Kac defines a series of independent and identically distributed, discrete random variables $\left( X_i\right)_{i\in\mathbb N}$, each of which has value either 1 or $-1$ with equal probability. On page 6 he makes the following statement, without providing any justification: $$\mathbb{P}\left\{\sum_{i=1}^k X_i=m\right\}=\frac{1}{2\pi}\int_0^{2\pi}e^{-im\xi}\cos^k\xi\ d\xi$$ for any $m\in\mathbb Z$. I am totally stumped as to how this formula is justified. The left-hand side should be fairly simple as, if my calculations are correct, it is equal to  $$\mathbb{P}\left\{2M_k^{0.5}-k=m\right\}$$ where $M_k^p$ is a binomial random variable that is the result of $k$ trials with probability of success $p$. The integral on the right looks like it will have a complex result, and hence not be equal to that on the left. Complex numbers and integrals have played no part in the paper up to that point, so their sudden introduction is a complete surprise. Also, while I could not integrate the expression, Wolfram Alpha says the indefinite integral is (replacing $\xi$ by $x$): $$i\frac{2^{-k}(e^{-ix}+e^{ix})(1+e^{2ix})^{-k}e^{-imx}{}_2F_1(-k,-\frac k2-\frac m2;-\frac k2-\frac m2+1;-e^{-2ix})}{k+m} $$ where the ${}_2F_1$ item is the hypergeometric function. For $x=0$ this is $$i\frac{2^{-k}\cdot 2^k\cdot2^{-k}\cdot 1\cdot {}_2F_1(-k,-\frac k2-\frac m2;-\frac k2-\frac m2+1;-1)}{k+m} $$which is $$i\frac{2^{-k}\cdot {}_2F_1(-k,-\frac k2-\frac m2;-\frac k2-\frac m2+1;-1)}{k+m} $$which is $$i\cdot\frac{2^{-k}}b\cdot {}_2F_1(-k,-b;1-b;-1) $$where $b\equiv (k+m)/2$. The same value should be obtained for $x=2\pi$, so it seems to me that, based on that, the definite integral should be zero. I am stuck in a situation where, not only can I see neither motivation nor justification for Kac's introduction of the formula on the RHS, but on my (quite possibly faulty) calculations, that step seems to lead to an impossible result. I would be very grateful to anybody that can help me understand this. Thank you very much.","I am trying to understand a proof of the Feynman-Kac Theorem , as set out in Mark Kac's 1949 paper 'On Distributions of Certain Wiener Functionals' . Kac defines a series of independent and identically distributed, discrete random variables $\left( X_i\right)_{i\in\mathbb N}$, each of which has value either 1 or $-1$ with equal probability. On page 6 he makes the following statement, without providing any justification: $$\mathbb{P}\left\{\sum_{i=1}^k X_i=m\right\}=\frac{1}{2\pi}\int_0^{2\pi}e^{-im\xi}\cos^k\xi\ d\xi$$ for any $m\in\mathbb Z$. I am totally stumped as to how this formula is justified. The left-hand side should be fairly simple as, if my calculations are correct, it is equal to  $$\mathbb{P}\left\{2M_k^{0.5}-k=m\right\}$$ where $M_k^p$ is a binomial random variable that is the result of $k$ trials with probability of success $p$. The integral on the right looks like it will have a complex result, and hence not be equal to that on the left. Complex numbers and integrals have played no part in the paper up to that point, so their sudden introduction is a complete surprise. Also, while I could not integrate the expression, Wolfram Alpha says the indefinite integral is (replacing $\xi$ by $x$): $$i\frac{2^{-k}(e^{-ix}+e^{ix})(1+e^{2ix})^{-k}e^{-imx}{}_2F_1(-k,-\frac k2-\frac m2;-\frac k2-\frac m2+1;-e^{-2ix})}{k+m} $$ where the ${}_2F_1$ item is the hypergeometric function. For $x=0$ this is $$i\frac{2^{-k}\cdot 2^k\cdot2^{-k}\cdot 1\cdot {}_2F_1(-k,-\frac k2-\frac m2;-\frac k2-\frac m2+1;-1)}{k+m} $$which is $$i\frac{2^{-k}\cdot {}_2F_1(-k,-\frac k2-\frac m2;-\frac k2-\frac m2+1;-1)}{k+m} $$which is $$i\cdot\frac{2^{-k}}b\cdot {}_2F_1(-k,-b;1-b;-1) $$where $b\equiv (k+m)/2$. The same value should be obtained for $x=2\pi$, so it seems to me that, based on that, the definite integral should be zero. I am stuck in a situation where, not only can I see neither motivation nor justification for Kac's introduction of the formula on the RHS, but on my (quite possibly faulty) calculations, that step seems to lead to an impossible result. I would be very grateful to anybody that can help me understand this. Thank you very much.",,"['probability', 'ordinary-differential-equations']"
94,Transformation of random variables that preserves the distribution,Transformation of random variables that preserves the distribution,,"Suppose we have a random variable $X$ with distribution $F_X$. Let $X_1$ and $X_2$ be two independent copies of $X$. My question: can we find a transformation $Z=g(X_1,X_2)$ such that the distribution of $Z$ is given by $F_X$? For example, if $X$ is a standard normal then such a transformation is given by $g(X_1,X_2)=0.25 X_1+0.25X_2$. Another example is we have $X={-1,1}$ equally likely. Then the transformation is  $g(X_1,X_2)=X_1X_2$. Note, a  trivial answer to this question is $g(X_1,X_2)=X_1$.  However, I would like to find something more interesting. I started with the usual set up but didn't get anywhere \begin{align} F_Z(z)=\mathbb P [ g(X_1,X_2) \le z] \end{align}","Suppose we have a random variable $X$ with distribution $F_X$. Let $X_1$ and $X_2$ be two independent copies of $X$. My question: can we find a transformation $Z=g(X_1,X_2)$ such that the distribution of $Z$ is given by $F_X$? For example, if $X$ is a standard normal then such a transformation is given by $g(X_1,X_2)=0.25 X_1+0.25X_2$. Another example is we have $X={-1,1}$ equally likely. Then the transformation is  $g(X_1,X_2)=X_1X_2$. Note, a  trivial answer to this question is $g(X_1,X_2)=X_1$.  However, I would like to find something more interesting. I started with the usual set up but didn't get anywhere \begin{align} F_Z(z)=\mathbb P [ g(X_1,X_2) \le z] \end{align}",,"['probability', 'probability-theory', 'random-variables']"
95,How exactly is the St Petersburg Paradox giving bounded payoff in average-of-N-trials?,How exactly is the St Petersburg Paradox giving bounded payoff in average-of-N-trials?,,"I understand why the expected value of the St Petersburg Paradox is algebraically infinite, but intuition tells me that in practice any given round of the game will not go on multiplying the pot for an infinite number of steps, so I am attempting a more nuanced analysis. I wrote a computer program to measure the actual payoff in practice. In pseudocode, the linked algorithm is basically for each trial     pot = 2     if (coin toss is tails)         end this trial with no winnings     while (coin toss is heads)         pot <-- pot x 2     winnings <-- pot end trial loop print winnings / number of trials Here are the raw results TRIALS      AVERAGE PAYOFF (one column per run of the program) 10          2.6     6.8     3.8     2.4     0.6     3.2 100         14.02   6.44    9.8     5.5     5.54    6.3 1000        7.494   9.516   9.254   4.162   4.676   4.36 10000       6.864   7.362   217.462 11.3302 5.722   13.2424 100000      19.248  9.78776 14.4392 15.9848 14.1321 14.9646 1000000     26.0934 10.4752 13.1372 15.6501 10.6382 9.93312 10000000    12.3089 12.9838 11.7851 17.3922 12.9416 27.9271 100000000   23.467  14.6506 15.1155 16.2025 12.4644 15.5596 1000000000  18.4466 13.9933 16.7371 14.888  15.5726 Let's assume the random number generator is behaving as documented and there is no bias in the program. Note that the numbers involved are well within the limits of numerical stability for a FPU. Apart from one outlier, where apparently sufficiently many trials in the 10,000 case were lengthy enough to move the average, the average payoff seems to be less than 30. This result is remarkably consistent. Now, the analytical response to this would be ""a long trial is exponentially unlikely to happen but produces an exponentially increasing payoff if you wait long enough until you see it "". However, unless you play the game an infinite number of times , you will not see an infinite payoff in practice. My next observation is that as the number of trials increases the average payoff over all trials seems to stabilise at around 15. (If you stick to taking an average of, say, 10 trials, then you soon start to see larger average payoffs as you re-run the program, dropping off exponentially as you would expect). What I'm getting at is that, from an economic and decision-theoretic point of view, it appears to be demonstrably irrational to put down more than about $15,000,000,000 to play the game 1,000,000,000 times. Intuitively we could say that, in the long run, one expects that most games are short and this constrains the average payoff in practice; the algebraic limit doesn't matter because we never actually get there. How can we quantify this notion? How can we derive this apparently stable practical-limit-of-the-average-over-many-trials, which seems to be about 15?","I understand why the expected value of the St Petersburg Paradox is algebraically infinite, but intuition tells me that in practice any given round of the game will not go on multiplying the pot for an infinite number of steps, so I am attempting a more nuanced analysis. I wrote a computer program to measure the actual payoff in practice. In pseudocode, the linked algorithm is basically for each trial     pot = 2     if (coin toss is tails)         end this trial with no winnings     while (coin toss is heads)         pot <-- pot x 2     winnings <-- pot end trial loop print winnings / number of trials Here are the raw results TRIALS      AVERAGE PAYOFF (one column per run of the program) 10          2.6     6.8     3.8     2.4     0.6     3.2 100         14.02   6.44    9.8     5.5     5.54    6.3 1000        7.494   9.516   9.254   4.162   4.676   4.36 10000       6.864   7.362   217.462 11.3302 5.722   13.2424 100000      19.248  9.78776 14.4392 15.9848 14.1321 14.9646 1000000     26.0934 10.4752 13.1372 15.6501 10.6382 9.93312 10000000    12.3089 12.9838 11.7851 17.3922 12.9416 27.9271 100000000   23.467  14.6506 15.1155 16.2025 12.4644 15.5596 1000000000  18.4466 13.9933 16.7371 14.888  15.5726 Let's assume the random number generator is behaving as documented and there is no bias in the program. Note that the numbers involved are well within the limits of numerical stability for a FPU. Apart from one outlier, where apparently sufficiently many trials in the 10,000 case were lengthy enough to move the average, the average payoff seems to be less than 30. This result is remarkably consistent. Now, the analytical response to this would be ""a long trial is exponentially unlikely to happen but produces an exponentially increasing payoff if you wait long enough until you see it "". However, unless you play the game an infinite number of times , you will not see an infinite payoff in practice. My next observation is that as the number of trials increases the average payoff over all trials seems to stabilise at around 15. (If you stick to taking an average of, say, 10 trials, then you soon start to see larger average payoffs as you re-run the program, dropping off exponentially as you would expect). What I'm getting at is that, from an economic and decision-theoretic point of view, it appears to be demonstrably irrational to put down more than about $15,000,000,000 to play the game 1,000,000,000 times. Intuitively we could say that, in the long run, one expects that most games are short and this constrains the average payoff in practice; the algebraic limit doesn't matter because we never actually get there. How can we quantify this notion? How can we derive this apparently stable practical-limit-of-the-average-over-many-trials, which seems to be about 15?",,"['probability', 'statistics', 'game-theory', 'decision-theory']"
96,"Bayes Theorem problem, from Finan #9.4: $P(A\mid B ∩ C)$","Bayes Theorem problem, from Finan #9.4:",P(A\mid B ∩ C),"The Problem: You are given $\Pr(A) = 2/5, \Pr(A ∪ B) = 3/5, \Pr(B\mid A) = 1/4, \Pr(C\mid B) = 1/3,$ and $\Pr(C\mid A ∩ B) = 1/2$. Find $\Pr(A\mid B ∩ C)$. My work: I know that $\Pr(A\mid B) \Pr(B) = \Pr(B\mid A) \Pr(A) = \Pr(A \cap B)$ . To solve the problem I need to solve   $\Pr(A\mid B ∩ C) = \frac{\Pr(A \cap B \cap C) }{\Pr(B \cap C)}$ I worked out that $\Pr(B \cap C) = \Pr(C\mid B) \Pr(B) = 1/3 \cdot \Pr(B) = \Pr(B\mid C)\Pr(C)$ So, now I want to find either $\Pr(B)$ or $\Pr(B\mid C)$ & $\Pr(C)$. Then, I found that $\Pr(A \cap B \cap C) = 1/20$ by working out that $\Pr(C\mid A \cap B) = \frac{\Pr(A \cap B \cap C) }{\Pr(A \cap B)} = 1/2$ I've been trying to go further than this, but end up with no new information. I'm not sure how knowing $\Pr(A ∪ B) = 3/5$ would help. Any suggestions?","The Problem: You are given $\Pr(A) = 2/5, \Pr(A ∪ B) = 3/5, \Pr(B\mid A) = 1/4, \Pr(C\mid B) = 1/3,$ and $\Pr(C\mid A ∩ B) = 1/2$. Find $\Pr(A\mid B ∩ C)$. My work: I know that $\Pr(A\mid B) \Pr(B) = \Pr(B\mid A) \Pr(A) = \Pr(A \cap B)$ . To solve the problem I need to solve   $\Pr(A\mid B ∩ C) = \frac{\Pr(A \cap B \cap C) }{\Pr(B \cap C)}$ I worked out that $\Pr(B \cap C) = \Pr(C\mid B) \Pr(B) = 1/3 \cdot \Pr(B) = \Pr(B\mid C)\Pr(C)$ So, now I want to find either $\Pr(B)$ or $\Pr(B\mid C)$ & $\Pr(C)$. Then, I found that $\Pr(A \cap B \cap C) = 1/20$ by working out that $\Pr(C\mid A \cap B) = \frac{\Pr(A \cap B \cap C) }{\Pr(A \cap B)} = 1/2$ I've been trying to go further than this, but end up with no new information. I'm not sure how knowing $\Pr(A ∪ B) = 3/5$ would help. Any suggestions?",,"['probability', 'bayes-theorem', 'actuarial-science']"
97,How to apply Chernoff's bound when variables are not independent,How to apply Chernoff's bound when variables are not independent,,"Let $X = \sum_{i=1}^n{X_i}$, for Bernoulli random variables $X_i$ which are not necessarily independent. However, assume that conditioned on any possible values for the other variables, the probability that $X_i = 1$ is at most $p$. I would like to say that if $Y = Y_i$ is the sum of $n$ independent Bernoulli variables with $\Pr(Y_i = 1)=p$, then for any $q>p$, $$ \Pr(X>qn) \leq \Pr(Y>qn). $$ However, I don't know how to justify this. Is it true? What's the proof?","Let $X = \sum_{i=1}^n{X_i}$, for Bernoulli random variables $X_i$ which are not necessarily independent. However, assume that conditioned on any possible values for the other variables, the probability that $X_i = 1$ is at most $p$. I would like to say that if $Y = Y_i$ is the sum of $n$ independent Bernoulli variables with $\Pr(Y_i = 1)=p$, then for any $q>p$, $$ \Pr(X>qn) \leq \Pr(Y>qn). $$ However, I don't know how to justify this. Is it true? What's the proof?",,['probability']
98,Calculating large exponential probabilities,Calculating large exponential probabilities,,"Earlier today there was Youtube video attempting to solve a problem for a certain game. In it he tries to calculate the probability of certain events happening which narrows down to this equation: $P(r) = 1-(1-10^{-1014})^{10^{155}}$ Unfortunately, there isn't a trivial way to calculate the percentage form of the answer due to very large exponents, even with a machine. This is why he was unable to come up with an percentage himself. So how does one approximate or calculate a percent form of large exponent probabilities similar to this ? Correction : The original value I posted was $P(r) = (1-10^{-1014})^{10^{155}}$, this was different the the actual formula used in the video which has been corrected to the one above, but doesn't effect the question.","Earlier today there was Youtube video attempting to solve a problem for a certain game. In it he tries to calculate the probability of certain events happening which narrows down to this equation: $P(r) = 1-(1-10^{-1014})^{10^{155}}$ Unfortunately, there isn't a trivial way to calculate the percentage form of the answer due to very large exponents, even with a machine. This is why he was unable to come up with an percentage himself. So how does one approximate or calculate a percent form of large exponent probabilities similar to this ? Correction : The original value I posted was $P(r) = (1-10^{-1014})^{10^{155}}$, this was different the the actual formula used in the video which has been corrected to the one above, but doesn't effect the question.",,"['probability', 'algorithms', 'approximation']"
99,What is the probability that all $n$ colors are selected in $m$ trials?,What is the probability that all  colors are selected in  trials?,n m,"I have a concrete problem, say, there are $n$ different balls ($n$ different colors to distinguish them), each ball will be selected uniformly at random. The way I choose a ball is that I randomly get a ball, and write down its color．Then I put  this ball back to the collection and choose a ball again. It won't stop until I get $m$ recorded colors. Suppose $m \geq n$. How many ways are there such that all $n$ colors were selected? And what is the probability that all $n$ different colors are recorded in the consecutive $m$ selections? My guts tell me there are $n^m$ combinations in total, but my rusty math halts me there.  Thank you in advance.","I have a concrete problem, say, there are $n$ different balls ($n$ different colors to distinguish them), each ball will be selected uniformly at random. The way I choose a ball is that I randomly get a ball, and write down its color．Then I put  this ball back to the collection and choose a ball again. It won't stop until I get $m$ recorded colors. Suppose $m \geq n$. How many ways are there such that all $n$ colors were selected? And what is the probability that all $n$ different colors are recorded in the consecutive $m$ selections? My guts tell me there are $n^m$ combinations in total, but my rusty math halts me there.  Thank you in advance.",,"['probability', 'combinatorics']"
