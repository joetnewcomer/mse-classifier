,title_raw,title_text,title_latex,body_raw,body_text,body_latex,tags
0,Independence of the spacing of order statistics characterizes exponential distribution?,Independence of the spacing of order statistics characterizes exponential distribution?,,"The question is: Let $Y_1 < Y_2$ be the order statistics of a random sample of size $2$ from a distribution of the continuous type which has p.d.f $f(x)>0$ provided $x \geq 0$ , and $0$ elsewhere. Show that the independence of $Z_1 = Y_1$ and $Z_2=Y_2-Y_1$ characterizes the exponential distribution. This is as far as I went: The joint distribution of the order statistics : $$f(y_1,y_2)=2f(y_1)f(y_2), 0 < y_1 < y_2$$ . The Jacobian of change of variable is $1$ , thus $$g(z_1,z_2)=f(z_1,z_1+z_2)=2f(z_1)f(z_1+z_2)$$ . $Z_1$ and $Z_2$ are independent if and only if $$g(z_1,z_2)=g(z_1)g(z_2)$$ Not sure how to continue, can anyone please help? Edit: I found a similar question: Independence of spacing of order statistics of exponential distribution My question is just the reverse of the problem.","The question is: Let be the order statistics of a random sample of size from a distribution of the continuous type which has p.d.f provided , and elsewhere. Show that the independence of and characterizes the exponential distribution. This is as far as I went: The joint distribution of the order statistics : . The Jacobian of change of variable is , thus . and are independent if and only if Not sure how to continue, can anyone please help? Edit: I found a similar question: Independence of spacing of order statistics of exponential distribution My question is just the reverse of the problem.","Y_1 < Y_2 2 f(x)>0 x \geq 0 0 Z_1 = Y_1 Z_2=Y_2-Y_1 f(y_1,y_2)=2f(y_1)f(y_2), 0 < y_1 < y_2 1 g(z_1,z_2)=f(z_1,z_1+z_2)=2f(z_1)f(z_1+z_2) Z_1 Z_2 g(z_1,z_2)=g(z_1)g(z_2)","['probability', 'statistics', 'exponential-distribution', 'order-statistics', 'gamma-distribution']"
1,"If $X+Y$ follows exponential distribution with parameter $2 \lambda$, is it necessary $X$ and $Y$ follow exponential with parameter $\lambda$?","If  follows exponential distribution with parameter , is it necessary  and  follow exponential with parameter ?",X+Y 2 \lambda X Y \lambda,"Let $Z=X+Y$ , $Z \backsim exp(2 \lambda)$ , with $X, Y$ i.i.d. Then, $$f_Z(Z=z)=2 \lambda e^{-2 \lambda(x+y)}$$ By the convolution function, $$f_Z(Z=z) = \int\limits_{- \infty}^{\infty}f_Y(z-x)f_X(x)dx $$ $$ \implies 2 \lambda e^{-2 \lambda(x+y)} = \int\limits_{- \infty}^{\infty}f_Y(z-x)f_X(x)dx$$ Differentiating both sides, $$-4 {\lambda}^2e^{-2 \lambda (x+y)} = f_Y(z-x)f_X(x)$$ Since $Y=Z-X$ and $X, Y$ are independent, $f_Y(z-x)f_X(x) = f(x,y)$ , the joint pdf. Then, $$f_X(x)=\int\limits_{- \infty}^{\infty}f(x,y)dy$$ $$f_X(x)=\int\limits_0^{\infty}-4 {\lambda}^2e^{-2 \lambda (x+y)}$$ $$f_X(x)= -4 {\lambda}^2e^{-2 \lambda x}\int\limits_0^{\infty}e^{-2 \lambda y}dy$$ Put $2 \lambda y = u,$ then, $dy = \frac{1}{2 \lambda}du$ $$f_X(x)= -4 {\lambda}^2e^{-2 \lambda x}.\frac{1}{2 \lambda}$$ $$f_X(x) = -2{\lambda}e^{-2 \lambda x}$$ What am I doing wrong that my marginal function for $X$ is not integrating to $1$ ? More importantly, is the general approach that I have taken to solving this problem correct?","Let , , with i.i.d. Then, By the convolution function, Differentiating both sides, Since and are independent, , the joint pdf. Then, Put then, What am I doing wrong that my marginal function for is not integrating to ? More importantly, is the general approach that I have taken to solving this problem correct?","Z=X+Y Z \backsim exp(2 \lambda) X, Y f_Z(Z=z)=2 \lambda e^{-2 \lambda(x+y)} f_Z(Z=z) = \int\limits_{- \infty}^{\infty}f_Y(z-x)f_X(x)dx   \implies 2 \lambda e^{-2 \lambda(x+y)} = \int\limits_{- \infty}^{\infty}f_Y(z-x)f_X(x)dx -4 {\lambda}^2e^{-2 \lambda (x+y)} = f_Y(z-x)f_X(x) Y=Z-X X, Y f_Y(z-x)f_X(x) = f(x,y) f_X(x)=\int\limits_{- \infty}^{\infty}f(x,y)dy f_X(x)=\int\limits_0^{\infty}-4 {\lambda}^2e^{-2 \lambda (x+y)} f_X(x)= -4 {\lambda}^2e^{-2 \lambda x}\int\limits_0^{\infty}e^{-2 \lambda y}dy 2 \lambda y = u, dy = \frac{1}{2 \lambda}du f_X(x)= -4 {\lambda}^2e^{-2 \lambda x}.\frac{1}{2 \lambda} f_X(x) = -2{\lambda}e^{-2 \lambda x} X 1","['statistics', 'probability-distributions', 'solution-verification']"
2,How to find a point estimate for a given random sample of exponential distribution given the sample variance and four out of five sample values?,How to find a point estimate for a given random sample of exponential distribution given the sample variance and four out of five sample values?,,"Let $\left(x_1,x_2,x_3,x_4,x_5\right)$ be the observed values of a random sample of size $5$ from an exponential distribution with parameter $\beta$ . Out of five observed values four are given as $x_1=2$ , $x_2=4$ , $x_3=5$ , $x_4=5$ and if the sample variance is $s^2=1.5$ , then a point estimate of $\beta$ is? In this question a valid method of solving would be to assume a value $x$ for $x_5$ and finding sample variance with one equation and one variable. However, this neglects the first statement about the distribution that is being used and fails to take advantage of the properties that come along with the distribution. What is the significance of that first statement and how does it play a role in the solution?","Let be the observed values of a random sample of size from an exponential distribution with parameter . Out of five observed values four are given as , , , and if the sample variance is , then a point estimate of is? In this question a valid method of solving would be to assume a value for and finding sample variance with one equation and one variable. However, this neglects the first statement about the distribution that is being used and fails to take advantage of the properties that come along with the distribution. What is the significance of that first statement and how does it play a role in the solution?","\left(x_1,x_2,x_3,x_4,x_5\right) 5 \beta x_1=2 x_2=4 x_3=5 x_4=5 s^2=1.5 \beta x x_5","['statistics', 'variance', 'exponential-distribution', 'parameter-estimation']"
3,MLE of two variables,MLE of two variables,,"Suppose $X$ and $Y$ are random variables and let $x_1, ..., x_n$ be observed values from a random sample of $X$ . Assume that $Y_i = 2\alpha x_i + \alpha + \beta_i$ where $\alpha$ is unknown and $\beta_1, ..., \beta_n$ are iid. $N(0, \sigma^2)$ with $\sigma^2$ being unknown (Equivalently assume that the conditional expectation of $Y$ depends linearly on $X$ and that the slope of the line is double the y-intercept). (i) Determine the maximum likelihood estimators for $\alpha$ and $\sigma^2$ . (ii) You take $3$ samples and observe $(x_1, y_1) = (1, 4)$ , $(x_2, y_2) = (0, 1)$ , and $(x_3, y_3) = (3, 6)$ . Find the point estimate for $\alpha$ using the MLE you found in (i). (iii) What are the residuals of this model and what do they measure? Hint: The MLE of $\sigma^2$ is the average of the squares of the residuals. My attempt: (i) I used this likelihood function $$L(\alpha, \sigma^2) = \prod_{i=1}^{n} \frac{1}{\sqrt{2\pi \sigma^2}} exp(\sum_{i=1}^{n}\frac{-(y_i - 2\alpha x_i - \alpha)^2}{2\sigma^2})$$ The negative of the natural log of this is $$-ln(L(\alpha, \sigma^2)) = \frac{n}{2}\ln(2\pi \sigma^2) + \sum_{i=1}^{n}\frac{(y_i - 2\alpha x_i - \alpha)^2}{2\sigma^2}$$ We let the latter half of the RHS be another function $G$ so that: $$G(\alpha) = \sum_{i=1}^{n}\frac{(y_i - 2\alpha x_i - \alpha)^2}{2\sigma^2}$$ Minimizing this w.r.t. $\alpha$ gives $$G' = \sum_{i=1}^{n} 2(y_i - 2 \alpha x_i - \alpha)(-2x_i - 1) = 0$$ And so the MLE for $\alpha$ is $$\hat{\alpha} =  \frac{\sum_{i = 1}^{n}4Y_ix_i + 2Y_i}{\sum_{i = 1}^{n}8x_i^2 + 8x_i + 2}$$ We check the second partial derivative: $$G'' = \sum_{i=1}^{n} 8x_i^2 + 8x_i + 2$$ This is greater than $0$ so it is a minimum. Now, minimizing $-\ln(L)$ w.r.t. $\sigma^2$ gives $$-ln(L(\alpha, \sigma^2))' = \frac{n}{2\sigma^2} - \frac{1}{2\sigma^4}\sum_{i=1}^{n}(y_i - 2\alpha x_i - \alpha)^2 = 0$$ And so the MLE for $\sigma^2$ is $$\hat{\sigma}^2 =  \frac{1}{n}\sum_{i = 1}^{n} (y_i - 2\alpha x_i - \alpha)^2$$ We check the second partial: $$-ln(L(\alpha, \sigma^2))'' = \frac{-n}{2\sigma^4} + \frac{1}{\sigma^6}\sum_{i=1}^{n}(y_i - 2\alpha x_i - \alpha)^2$$ This is greater than $0$ $\iff$ $\sum_{i=1}^{n}(y_i - 2\alpha x_i - \alpha)^2 > n \sigma^2$ However, I am not sure how to show that this is true. (ii) The MLE from (i) was $$\hat{\alpha} =  \frac{\sum_{i = 1}^{n}4Y_ix_i + 2Y_i}{\sum_{i = 1}^{n}8x_i^2 + 8x_i + 2}$$ So $$\alpha = \frac{(4 \cdot 4 \cdot 1 + 2\cdot 4) + (4\cdot 1\cdot 0 + 2\cdot 1) + (4\cdot 6\cdot 3 + 2\cdot 6)}{(8 \cdot 1^2 + 8\cdot 1 + 2) + (8 \cdot 0^2 + 8 \cdot 0 + 2) + (8 \cdot 3^2 + 8\cdot 3 + 2) } = \frac{110}{118}$$ (iii) Going by the hint, the residual is $Y_i - 2\alpha x_i - \alpha$ . It is the difference between the actual and observed data, and is used to determine whether a line or curve is appropriate for the data. Any assistance especially with the last part of (i) is appreciated. Was my explanation about residuals correct?","Suppose and are random variables and let be observed values from a random sample of . Assume that where is unknown and are iid. with being unknown (Equivalently assume that the conditional expectation of depends linearly on and that the slope of the line is double the y-intercept). (i) Determine the maximum likelihood estimators for and . (ii) You take samples and observe , , and . Find the point estimate for using the MLE you found in (i). (iii) What are the residuals of this model and what do they measure? Hint: The MLE of is the average of the squares of the residuals. My attempt: (i) I used this likelihood function The negative of the natural log of this is We let the latter half of the RHS be another function so that: Minimizing this w.r.t. gives And so the MLE for is We check the second partial derivative: This is greater than so it is a minimum. Now, minimizing w.r.t. gives And so the MLE for is We check the second partial: This is greater than However, I am not sure how to show that this is true. (ii) The MLE from (i) was So (iii) Going by the hint, the residual is . It is the difference between the actual and observed data, and is used to determine whether a line or curve is appropriate for the data. Any assistance especially with the last part of (i) is appreciated. Was my explanation about residuals correct?","X Y x_1, ..., x_n X Y_i = 2\alpha x_i + \alpha + \beta_i \alpha \beta_1, ..., \beta_n N(0, \sigma^2) \sigma^2 Y X \alpha \sigma^2 3 (x_1, y_1) = (1, 4) (x_2, y_2) = (0, 1) (x_3, y_3) = (3, 6) \alpha \sigma^2 L(\alpha, \sigma^2) = \prod_{i=1}^{n} \frac{1}{\sqrt{2\pi \sigma^2}} exp(\sum_{i=1}^{n}\frac{-(y_i - 2\alpha x_i - \alpha)^2}{2\sigma^2}) -ln(L(\alpha, \sigma^2)) = \frac{n}{2}\ln(2\pi \sigma^2) + \sum_{i=1}^{n}\frac{(y_i - 2\alpha x_i - \alpha)^2}{2\sigma^2} G G(\alpha) = \sum_{i=1}^{n}\frac{(y_i - 2\alpha x_i - \alpha)^2}{2\sigma^2} \alpha G' = \sum_{i=1}^{n} 2(y_i - 2 \alpha x_i - \alpha)(-2x_i - 1) = 0 \alpha \hat{\alpha} =  \frac{\sum_{i = 1}^{n}4Y_ix_i + 2Y_i}{\sum_{i = 1}^{n}8x_i^2 + 8x_i + 2} G'' = \sum_{i=1}^{n} 8x_i^2 + 8x_i + 2 0 -\ln(L) \sigma^2 -ln(L(\alpha, \sigma^2))' = \frac{n}{2\sigma^2} - \frac{1}{2\sigma^4}\sum_{i=1}^{n}(y_i - 2\alpha x_i - \alpha)^2 = 0 \sigma^2 \hat{\sigma}^2 =  \frac{1}{n}\sum_{i = 1}^{n} (y_i - 2\alpha x_i - \alpha)^2 -ln(L(\alpha, \sigma^2))'' = \frac{-n}{2\sigma^4} + \frac{1}{\sigma^6}\sum_{i=1}^{n}(y_i - 2\alpha x_i - \alpha)^2 0 \iff \sum_{i=1}^{n}(y_i - 2\alpha x_i - \alpha)^2 > n \sigma^2 \hat{\alpha} =  \frac{\sum_{i = 1}^{n}4Y_ix_i + 2Y_i}{\sum_{i = 1}^{n}8x_i^2 + 8x_i + 2} \alpha = \frac{(4 \cdot 4 \cdot 1 + 2\cdot 4) + (4\cdot 1\cdot 0 + 2\cdot 1) + (4\cdot 6\cdot 3 + 2\cdot 6)}{(8 \cdot 1^2 + 8\cdot 1 + 2) + (8 \cdot 0^2 + 8 \cdot 0 + 2) + (8 \cdot 3^2 + 8\cdot 3 + 2) } = \frac{110}{118} Y_i - 2\alpha x_i - \alpha","['statistics', 'solution-verification', 'regression', 'maximum-likelihood', 'parameter-estimation']"
4,Showing the data processing inequality,Showing the data processing inequality,,"Let $X \sim P_\theta$ for some distribution $P_\theta$ parametrized by $\theta \in \Theta \subset \mathbb R$ and $Y \sim Q(\cdot | X)$ for some distribution $Q$ . Assume that $P_\theta$ has a density $p_\theta$ , with respect to some ground measure $\mu$ and $Q(\cdot|X)$ has a density $q(\cdot | X)$ with respect to the same $\mu$ for every $X$ . Show that the Fisher information of $X$ is at least as large as that of $Y$ : $$\mathbb E\left [ \frac{\int q(Y | X=x) \dot p_\theta(x)dx}{\int q(Y | X=x) p_\theta(x)dx} \right ]^2 \leq \mathbb E\left[\frac{\dot p_\theta(X)}{p_\theta(X)}\right]^2$$ where $\dot p_\theta(x)$ is the first derivative of the density $p_\theta$ with respect to $\theta$ . I'm looking for hints for solving this problem (preferably vague hints that will allow me to still solve the problem myself). What I've tried The inequality makes some sense to me intuitively but I'm having trouble understanding why it's true. I would usually first try to show a pointwise inequality, but that won't work here since the expectations are over different variables. I also tried expressing the expectations as integrals and switching the order of integration, but that didn't give me any new insights. My instinct is to apply Cauchy-Schwarz or Jensen's inequality, but I haven't figured out the right way to apply them here.","Let for some distribution parametrized by and for some distribution . Assume that has a density , with respect to some ground measure and has a density with respect to the same for every . Show that the Fisher information of is at least as large as that of : where is the first derivative of the density with respect to . I'm looking for hints for solving this problem (preferably vague hints that will allow me to still solve the problem myself). What I've tried The inequality makes some sense to me intuitively but I'm having trouble understanding why it's true. I would usually first try to show a pointwise inequality, but that won't work here since the expectations are over different variables. I also tried expressing the expectations as integrals and switching the order of integration, but that didn't give me any new insights. My instinct is to apply Cauchy-Schwarz or Jensen's inequality, but I haven't figured out the right way to apply them here.",X \sim P_\theta P_\theta \theta \in \Theta \subset \mathbb R Y \sim Q(\cdot | X) Q P_\theta p_\theta \mu Q(\cdot|X) q(\cdot | X) \mu X X Y \mathbb E\left [ \frac{\int q(Y | X=x) \dot p_\theta(x)dx}{\int q(Y | X=x) p_\theta(x)dx} \right ]^2 \leq \mathbb E\left[\frac{\dot p_\theta(X)}{p_\theta(X)}\right]^2 \dot p_\theta(x) p_\theta \theta,"['probability', 'statistics', 'cauchy-schwarz-inequality', 'jensen-inequality']"
5,Finding the $p$-value of a test with exponential data,Finding the -value of a test with exponential data,p,"My problem Assume that $X$ is exponentially distributed such that $X\in\operatorname{Exp}(\lambda)$ . Let $H_0$ be $\lambda=3$ . We want to test $H_0$ against the alternative $H_1$ : $\lambda=1$ and reject $H_0$ if we observe a great value $x$ for $X$ . Assume that we have observed $x = 0.30$ . Determine the p-value of the test. My attempt of solving the solution $X \in\operatorname{Exp}(3)$ p-value $= P(X \geq 0.30) = 0.40657$ My question I think these problems are very difficult, because I am always unsure if I have calculated them correctly. How do I check that my solution is correct?","My problem Assume that is exponentially distributed such that . Let be . We want to test against the alternative : and reject if we observe a great value for . Assume that we have observed . Determine the p-value of the test. My attempt of solving the solution p-value My question I think these problems are very difficult, because I am always unsure if I have calculated them correctly. How do I check that my solution is correct?",X X\in\operatorname{Exp}(\lambda) H_0 \lambda=3 H_0 H_1 \lambda=1 H_0 x X x = 0.30 X \in\operatorname{Exp}(3) = P(X \geq 0.30) = 0.40657,"['probability', 'statistics', 'hypothesis-testing', 'exponential-distribution', 'p-value']"
6,Finding ACF of MA Process,Finding ACF of MA Process,,"trying to solve a general problem found in my textbook. I have the following MA Process, $X_t = \epsilon_t +0.8\epsilon_{t-1} + 0.4\epsilon_{t-12}+0.32\epsilon_{t-13} $ I simply want to calculate the autocorrelations of $X_t$ for lags k = 0 and k = 12 Here's what i've done so far: For $K = 0$ $E(X_t\epsilon_t) = \epsilon_t^2=1$ and for $K = 12$ $E(X_t\epsilon_{t-12}) = ? $ I think I am on the right path but would appreciate anyone who can guide me to the finish line! Thank you :D","trying to solve a general problem found in my textbook. I have the following MA Process, I simply want to calculate the autocorrelations of for lags k = 0 and k = 12 Here's what i've done so far: For and for I think I am on the right path but would appreciate anyone who can guide me to the finish line! Thank you :D",X_t = \epsilon_t +0.8\epsilon_{t-1} + 0.4\epsilon_{t-12}+0.32\epsilon_{t-13}  X_t K = 0 E(X_t\epsilon_t) = \epsilon_t^2=1 K = 12 E(X_t\epsilon_{t-12}) = ? ,"['probability', 'statistics']"
7,a.s. convergence and Glivenko-Cantelli theorem,a.s. convergence and Glivenko-Cantelli theorem,,"Assume that we have a sample $X_1, X_2, \dots, X_n$ is a sample from, for example, Poisson distribution with probability mass function $p_{i}$ and cdf $\mathbb{F}$ . Then, for $i\in\mathbb{N}$ let $$ \bar{p}_{n,i} = \frac{\sum_{j=1}^{n} 1 \{ X_j = i \}}{n} $$ is empirical estimator. The Glivenko-Cantelli theorem says: $$ \|\bar{\mathbb{F}}_n - F\|_\infty \overset{\text{a.s.}}{\to} 0 $$ where $\bar{\mathbb{F}}_n$ is empirical cdf. The queston: assume that $\tilde{p}_n$ is another strongly consistent estimator of $p$ , i.e. $\tilde{p}_{n}$ converges a.s. to $p$ in $l_{2}$ norm. Note, in $l_{2}$ , not point-wise. Is then $$ \|\tilde{\mathbb{F}}_n - F\|_\infty \overset{\text{a.s.}}{\to} 0 $$ where $\tilde{\mathbb{F}}_n$ is cdf for the estimator $\tilde{p}_n$ ?","Assume that we have a sample is a sample from, for example, Poisson distribution with probability mass function and cdf . Then, for let is empirical estimator. The Glivenko-Cantelli theorem says: where is empirical cdf. The queston: assume that is another strongly consistent estimator of , i.e. converges a.s. to in norm. Note, in , not point-wise. Is then where is cdf for the estimator ?","X_1, X_2, \dots, X_n p_{i} \mathbb{F} i\in\mathbb{N} 
\bar{p}_{n,i} = \frac{\sum_{j=1}^{n} 1 \{ X_j = i \}}{n}
 
\|\bar{\mathbb{F}}_n - F\|_\infty \overset{\text{a.s.}}{\to} 0
 \bar{\mathbb{F}}_n \tilde{p}_n p \tilde{p}_{n} p l_{2} l_{2} 
\|\tilde{\mathbb{F}}_n - F\|_\infty \overset{\text{a.s.}}{\to} 0
 \tilde{\mathbb{F}}_n \tilde{p}_n","['functional-analysis', 'probability-theory', 'measure-theory', 'statistics', 'convergence-divergence']"
8,Ordinary Least Squares Estimate,Ordinary Least Squares Estimate,,"In this paper ( https://www.stat.berkeley.edu/~brill/Papers/lehmannfest.pdf ), the author claims that (and more generally elsewhere I have seen that it is usually written as that) the Ordinary Least Squares estimate for the classical problem $y_i = \beta_0 + \beta_1^T x_i$ plus some noise (where $\beta_1, x_i$ are $p$ -dimensional vectors) satisfies: $$\hat{\beta_1} = \left( \sum_{i=1}^n (x_i - \bar{x}) (x_i - \bar{x})^T \right)^{-1} \left( \sum_{i=1}^n y_i (x_i - \bar{x}) \right) $$ Nevertheless, what I know ""in the modern days"" as the classical ordinary least squares estimate is the following quantity, see e.g. https://en.wikipedia.org/wiki/Linear_least_squares : $$\hat{\beta} = \left( \sum_{i=1}^n x_i x_i^T \right)^{-1} \left( \sum_{i=1}^n y_i x_i \right) $$ However, preliminary calculations of mine do not indicate that the two above quantities are equal, as they should be for the paper to hold. Does anyone have any idea why they are equal, if they are? Maybe it has to somehow do with the conditions of the least squares estimate? What am I missing?","In this paper ( https://www.stat.berkeley.edu/~brill/Papers/lehmannfest.pdf ), the author claims that (and more generally elsewhere I have seen that it is usually written as that) the Ordinary Least Squares estimate for the classical problem plus some noise (where are -dimensional vectors) satisfies: Nevertheless, what I know ""in the modern days"" as the classical ordinary least squares estimate is the following quantity, see e.g. https://en.wikipedia.org/wiki/Linear_least_squares : However, preliminary calculations of mine do not indicate that the two above quantities are equal, as they should be for the paper to hold. Does anyone have any idea why they are equal, if they are? Maybe it has to somehow do with the conditions of the least squares estimate? What am I missing?","y_i = \beta_0 + \beta_1^T x_i \beta_1, x_i p \hat{\beta_1} = \left( \sum_{i=1}^n (x_i - \bar{x}) (x_i - \bar{x})^T \right)^{-1} \left( \sum_{i=1}^n y_i (x_i - \bar{x}) \right)  \hat{\beta} = \left( \sum_{i=1}^n x_i x_i^T \right)^{-1} \left( \sum_{i=1}^n y_i x_i \right) ","['linear-algebra', 'statistics', 'regression', 'linear-regression']"
9,Why does a standard normal with $\frac{z^2}{2}=w$ equal $\Gamma(\frac{1}{2})$?,Why does a standard normal with  equal ?,\frac{z^2}{2}=w \Gamma(\frac{1}{2}),"In Berger, Casella (2002) I read that the standard normal pdf, given by $$ \int_0^\infty \exp\left(-\frac{z^2}{2}\right)\,dz$$ is equal to the gamma function $\Gamma(\alpha)$ evaluated in $\alpha=\frac{1}{2}$ when $w=\frac{1}{2}z^2$ , which results in: $$\Gamma\left(\frac{1}{2}\right) =\int^\infty_0w^{-\frac{1}{2}}e^{-w}dw=\sqrt{\pi}.$$ However, when I do my calculations and substitute $dz=\frac{dw}{\sqrt{2w}}$ I obtain a slightly different result, that is: $$2^{-\frac{1}{2}}\int^\infty_0w^{-\frac{1}{2}}e^{-w}dw.$$ Can somebody tell me what I missed?","In Berger, Casella (2002) I read that the standard normal pdf, given by is equal to the gamma function evaluated in when , which results in: However, when I do my calculations and substitute I obtain a slightly different result, that is: Can somebody tell me what I missed?"," \int_0^\infty \exp\left(-\frac{z^2}{2}\right)\,dz \Gamma(\alpha) \alpha=\frac{1}{2} w=\frac{1}{2}z^2 \Gamma\left(\frac{1}{2}\right) =\int^\infty_0w^{-\frac{1}{2}}e^{-w}dw=\sqrt{\pi}. dz=\frac{dw}{\sqrt{2w}} 2^{-\frac{1}{2}}\int^\infty_0w^{-\frac{1}{2}}e^{-w}dw.","['statistics', 'normal-distribution', 'statistical-inference', 'gamma-function']"
10,How to calculate the lower and upper bound of the error in estimating a ratio with low sample size?,How to calculate the lower and upper bound of the error in estimating a ratio with low sample size?,,"I have the following problem, I am trying to estimate the conversion rate of product sales online, my data is simple, I have the number of clicks and the number of sales for each product. Sales are sparse so there are few sales per product. how can I estimate the upper and lower bound of the error on each product? For example, if I have 21 clicks and 1 sale, the conversion rate is 1/21=0.07, I want to be able to add a lower and upper bound on the error of this estimation. Thank you","I have the following problem, I am trying to estimate the conversion rate of product sales online, my data is simple, I have the number of clicks and the number of sales for each product. Sales are sparse so there are few sales per product. how can I estimate the upper and lower bound of the error on each product? For example, if I have 21 clicks and 1 sale, the conversion rate is 1/21=0.07, I want to be able to add a lower and upper bound on the error of this estimation. Thank you",,"['statistics', 'estimation', 'descriptive-statistics']"
11,Linear regression with two possible slopes,Linear regression with two possible slopes,,"Let's say, I have a dataset with $X$ and $Y$ values. $X$ represents the monthly average temperature and $Y$ represents the money spent on utilities. My underlying hypothesis is that the heating energy (and utility bill) will be proportional to the average monthly temperature, but depending upon whether the house has gas or electric heating, the slope, $ ^\circ C$ , will be different. How can I use linear regression to extract out these two slopes? If I just do a simple linear regression with $X$ and $Y$ , I will only get a single slope that will represent the average $^\circ C$ between gas and electric heating. If I do a scatter plot, it's quite easy to see distinct linear relations (as shown in figure below), but I am lost in terms of how to extract the two slopes.","Let's say, I have a dataset with and values. represents the monthly average temperature and represents the money spent on utilities. My underlying hypothesis is that the heating energy (and utility bill) will be proportional to the average monthly temperature, but depending upon whether the house has gas or electric heating, the slope, , will be different. How can I use linear regression to extract out these two slopes? If I just do a simple linear regression with and , I will only get a single slope that will represent the average between gas and electric heating. If I do a scatter plot, it's quite easy to see distinct linear relations (as shown in figure below), but I am lost in terms of how to extract the two slopes.",X Y X Y  ^\circ C X Y ^\circ C,"['statistics', 'regression', 'linear-regression']"
12,"$f(x|\theta)=\frac{\theta}{x^2}I_{(\theta,\infty)}(x)\hspace{0.5cm}\theta\in\mathbb{R}^+$ UMVUE for $h(\theta)=\theta$",UMVUE for,"f(x|\theta)=\frac{\theta}{x^2}I_{(\theta,\infty)}(x)\hspace{0.5cm}\theta\in\mathbb{R}^+ h(\theta)=\theta","Let  X a random sample with density function $$f(x|\theta)=\frac{\theta}{x^2}I_{(\theta,\infty)}(x)\hspace{0.5cm}\theta\in\mathbb{R}^+$$ Find the UMVUE for $h(\theta)=\theta$ $$f_\theta(\vec{x})=\theta^nI_{(0,x_{(1)})}(\theta)\prod_{i=1}^n\frac {1}{x_i^2}$$ Then $$T(X)=X_{(1)}$$ is sufficient for $\theta$ I am trying to prove that  T is a complete statistic $$F_\theta(x)\,dx= \int_\theta^x f_\theta(x)\,dx = \int_\theta^x \frac{\theta}{x^2}\,dx=\left[-\frac{\theta}{x}\right]_\theta^x=\frac{x-\theta}{x}$$ Then $$F_{X_{(1)}}(y)=1-[1-F_\theta(y)]^n=1-\left[\frac{\theta}{y}\right]^n$$ Then $$f_{X_{(1)}}(y)=n\theta^ny^{-n-1}$$ Let $g(t)$ $$E[g(t)]=\int_\theta^{\infty}g(t)f_{X_{(1)}}=\int_\theta^{\infty}g(t)n\theta^nt^{-n-1}\,dt=n\theta^n\int_\theta^\infty g(t)t^{-n-1}\,dt $$ Then $$0=n\theta^n\int_\theta^\infty g(t)t^{-n-1}\,dt$$ $$0={n^2\theta^{n-1}\int_\theta^\infty g(t)y^{-n-1}\,dt}+n\theta^n\frac{d}{d\theta}\left[\int_\theta^\infty g(t)t^{-n-1}\,dt\right]$$ It follows that $\int_0^\theta g(t)t^{{-n-1}}\;dt=0$ Then $$n\theta^n\frac{d}{d\theta}\left[\int_\theta^\infty g(t)t^{-n-1}\,dt\right]=0$$ I am stuck in this point ... I thougt T was complete but now I am not sure, can you help me please Thanks","Let  X a random sample with density function Find the UMVUE for Then is sufficient for I am trying to prove that  T is a complete statistic Then Then Let Then It follows that Then I am stuck in this point ... I thougt T was complete but now I am not sure, can you help me please Thanks","f(x|\theta)=\frac{\theta}{x^2}I_{(\theta,\infty)}(x)\hspace{0.5cm}\theta\in\mathbb{R}^+ h(\theta)=\theta f_\theta(\vec{x})=\theta^nI_{(0,x_{(1)})}(\theta)\prod_{i=1}^n\frac
{1}{x_i^2} T(X)=X_{(1)} \theta F_\theta(x)\,dx= \int_\theta^x f_\theta(x)\,dx = \int_\theta^x \frac{\theta}{x^2}\,dx=\left[-\frac{\theta}{x}\right]_\theta^x=\frac{x-\theta}{x} F_{X_{(1)}}(y)=1-[1-F_\theta(y)]^n=1-\left[\frac{\theta}{y}\right]^n f_{X_{(1)}}(y)=n\theta^ny^{-n-1} g(t) E[g(t)]=\int_\theta^{\infty}g(t)f_{X_{(1)}}=\int_\theta^{\infty}g(t)n\theta^nt^{-n-1}\,dt=n\theta^n\int_\theta^\infty g(t)t^{-n-1}\,dt  0=n\theta^n\int_\theta^\infty g(t)t^{-n-1}\,dt 0={n^2\theta^{n-1}\int_\theta^\infty g(t)y^{-n-1}\,dt}+n\theta^n\frac{d}{d\theta}\left[\int_\theta^\infty g(t)t^{-n-1}\,dt\right] \int_0^\theta g(t)t^{{-n-1}}\;dt=0 n\theta^n\frac{d}{d\theta}\left[\int_\theta^\infty g(t)t^{-n-1}\,dt\right]=0","['statistics', 'statistical-inference']"
13,Partial likelihood in Cox's proportional hazards model,Partial likelihood in Cox's proportional hazards model,,"I'm reading about Cox's proportional hazards approach to (continuous) survival analysis and I'm finding it difficult to understand his argument for the derivation of the partial likelihood in his 1972 paper . He states on page 6 of the pdf in the link that the probability to observe a failure at time $t_{(i)}$ on the individual $i$ (given that there is exactly one failure at $t_{(i)}$ ) is equal to $$\frac{\exp(z_{(i)}\beta)}{\sum_{l\in R(t_{(l)})}\exp(z_{(l)}\beta)}.$$ Now I understand that the baseline hazard in the denominator is supposed to cancel against the baseline hazardin the numerator. However, the denominator itself surprises me, because it seems that this denominator is associated to the expected number of failures, rather than the probability to observe exactly one failure (which is what I would expect from the conditional probability that this partial likelihood is supposed to describe). I'm wondering if, instead, I should view this partial likelihood as some approximation to the exact problem (and if so, why this approximation is justified). Also, I'm curious if it's still necessary with modern computers to actually split the full likelihood into partial likelihoods or if this is something that was mostly useful in the 70s.","I'm reading about Cox's proportional hazards approach to (continuous) survival analysis and I'm finding it difficult to understand his argument for the derivation of the partial likelihood in his 1972 paper . He states on page 6 of the pdf in the link that the probability to observe a failure at time on the individual (given that there is exactly one failure at ) is equal to Now I understand that the baseline hazard in the denominator is supposed to cancel against the baseline hazardin the numerator. However, the denominator itself surprises me, because it seems that this denominator is associated to the expected number of failures, rather than the probability to observe exactly one failure (which is what I would expect from the conditional probability that this partial likelihood is supposed to describe). I'm wondering if, instead, I should view this partial likelihood as some approximation to the exact problem (and if so, why this approximation is justified). Also, I'm curious if it's still necessary with modern computers to actually split the full likelihood into partial likelihoods or if this is something that was mostly useful in the 70s.",t_{(i)} i t_{(i)} \frac{\exp(z_{(i)}\beta)}{\sum_{l\in R(t_{(l)})}\exp(z_{(l)}\beta)}.,"['statistics', 'statistical-inference', 'maximum-likelihood']"
14,Why are the most likelihood estimators often very intuitive?,Why are the most likelihood estimators often very intuitive?,,"This is likely a very vague and ""wish-washy"" question but I wanted to ask it on here. I have recently began studying mathematical statistics and have been working with the MLE estimators for a while now. There are of course a few exceptions of course but the MLE always seem to be the empirical average, empirical variance or another obvious and intuitive sum that one would guess for the parameter in question. For example when considering a sequence of IID normal random variables we have the MLE for the mean = $ \hat{\mu} =\frac{1}{n}\sum\limits_{i=0}^nX_i$ and $\hat{\sigma} = \frac{1}{2}\sum\limits_{i=0}^n(X_i - \hat{\mu})^2$ Would someone be able to provide some intuition as to this ""rule of thumb"". Maybe I have just been looking too closely into this and its actually very obvious. It does feel natural I just cant seem to explain it at least somewhat rigorously. Thanks :)","This is likely a very vague and ""wish-washy"" question but I wanted to ask it on here. I have recently began studying mathematical statistics and have been working with the MLE estimators for a while now. There are of course a few exceptions of course but the MLE always seem to be the empirical average, empirical variance or another obvious and intuitive sum that one would guess for the parameter in question. For example when considering a sequence of IID normal random variables we have the MLE for the mean = and Would someone be able to provide some intuition as to this ""rule of thumb"". Maybe I have just been looking too closely into this and its actually very obvious. It does feel natural I just cant seem to explain it at least somewhat rigorously. Thanks :)", \hat{\mu} =\frac{1}{n}\sum\limits_{i=0}^nX_i \hat{\sigma} = \frac{1}{2}\sum\limits_{i=0}^n(X_i - \hat{\mu})^2,"['probability', 'statistics', 'probability-distributions', 'statistical-inference', 'maximum-likelihood']"
15,Finding moment estimator and its asymptotic distribution,Finding moment estimator and its asymptotic distribution,,"I got a question: We let $X$ and $Y$ be independent random variables with $X$ Poisson distributed with mean $\lambda$ and $Y$ exponentially distributed with rate $\lambda>0$ and we let $(X_1,Y_1),\ldots,(X_n,Y_n)$ be a sample from this distribution. I have to find the moment estimator $\hat{\lambda}$ based on the statistic $t(x,y)=x-y$ and the sample $(X_1,Y_1),\ldots,(X_n,Y_n)$ and find the moment estimator's asymptotic distribution. Can anyone help me? My thoughts so far is that: To find the asymtotic distribution I think I can use that $V(t(x,y))/m'(\lambda)^2$ , but how do I find $m(\lambda)$ ? And can I then find moment estimator by solving $\lambda$ in $m(\lambda)$ ?","I got a question: We let and be independent random variables with Poisson distributed with mean and exponentially distributed with rate and we let be a sample from this distribution. I have to find the moment estimator based on the statistic and the sample and find the moment estimator's asymptotic distribution. Can anyone help me? My thoughts so far is that: To find the asymtotic distribution I think I can use that , but how do I find ? And can I then find moment estimator by solving in ?","X Y X \lambda Y \lambda>0 (X_1,Y_1),\ldots,(X_n,Y_n) \hat{\lambda} t(x,y)=x-y (X_1,Y_1),\ldots,(X_n,Y_n) V(t(x,y))/m'(\lambda)^2 m(\lambda) \lambda m(\lambda)","['probability-theory', 'statistics', 'poisson-distribution', 'moment-problem']"
16,On the Wigner semicircle distribution,On the Wigner semicircle distribution,,"Question Let $(X, Y)$ be a jointly continuous pair of random variables such that the marginal density of $X$ is $$f_X(x) = \frac 1 {2\pi} \sqrt{4 - x^2}, \quad -2 < x < 2$$ (also known as the Wigner semicircle distribution) and such that the the conditional distribution of $Y$ given $X = x$ is uniform on the interval $[-\sqrt{4 - x^2}, \sqrt{4 - x^2}]$ . $(a)\quad$ Give a simplified expression for the joint density of $X$ and $Y$ , specifying clearly the set of $x$ and $y$ values for which the joint density is non-zero. $(b)\quad$ How would you describe this joint distribution in words? $(c)\quad$ Hence, or otherwise, show that $Y$ has the same distribution as $X$ . My working $(a)$ $$f_{Y \mid X}(y \mid x) = \frac 1 {2\sqrt{4 - x^2}}$$ $$\begin{aligned} \implies f_{X, Y}(x, y) & = f_{Y \mid X}(y \mid x)f_X(x) \\[1 mm] & = \left(\frac 1 {2\sqrt{4 - x^2}}\right)\left(\frac 1 {2\pi} \sqrt{4 - x^2}\right) \\[1 mm] & = \frac 1 {4\pi}, \quad -2 < x < 2\ \mathrm{and}\ -\sqrt{4 - x^2} < y < \sqrt{4 - x^2} \end{aligned}$$ $(b)\quad$ I would say that “the joint distribution maps being in a circle of radius $2$ about the origin with probability $\frac 1 {4\pi}$ ”, but I am not sure if this is mathematically correct. $(c)\quad$ I would say that ""since the joint distribution of $X$ and $Y$ maps a full circle of radius $2$ about the origin and the marginal distribution of $X$ maps a semicircle of radius $2$ about the origin, it follows that the marginal distribution of $Y$ also maps a semicircle of radius $2$ about the origin, implying that $Y$ has the same distribution as $X$ "", but again I am not sure if this is entirely correct. If I am wrong anywhere, please do point it out and explain why :)","Question Let be a jointly continuous pair of random variables such that the marginal density of is (also known as the Wigner semicircle distribution) and such that the the conditional distribution of given is uniform on the interval . Give a simplified expression for the joint density of and , specifying clearly the set of and values for which the joint density is non-zero. How would you describe this joint distribution in words? Hence, or otherwise, show that has the same distribution as . My working I would say that “the joint distribution maps being in a circle of radius about the origin with probability ”, but I am not sure if this is mathematically correct. I would say that ""since the joint distribution of and maps a full circle of radius about the origin and the marginal distribution of maps a semicircle of radius about the origin, it follows that the marginal distribution of also maps a semicircle of radius about the origin, implying that has the same distribution as "", but again I am not sure if this is entirely correct. If I am wrong anywhere, please do point it out and explain why :)","(X, Y) X f_X(x) = \frac 1 {2\pi} \sqrt{4 - x^2}, \quad -2 < x < 2 Y X = x [-\sqrt{4 - x^2}, \sqrt{4 - x^2}] (a)\quad X Y x y (b)\quad (c)\quad Y X (a) f_{Y \mid X}(y \mid x) = \frac 1 {2\sqrt{4 - x^2}} \begin{aligned}
\implies f_{X, Y}(x, y) & = f_{Y \mid X}(y \mid x)f_X(x)
\\[1 mm] & = \left(\frac 1 {2\sqrt{4 - x^2}}\right)\left(\frac 1 {2\pi} \sqrt{4 - x^2}\right)
\\[1 mm] & = \frac 1 {4\pi}, \quad -2 < x < 2\ \mathrm{and}\ -\sqrt{4 - x^2} < y < \sqrt{4 - x^2}
\end{aligned} (b)\quad 2 \frac 1 {4\pi} (c)\quad X Y 2 X 2 Y 2 Y X","['probability', 'statistics', 'probability-distributions', 'random-variables', 'density-function']"
17,"Covariance of [$X_{(1)}$, $X_{(n)}$] from $\operatorname{Unif}(a,b)$","Covariance of [, ] from","X_{(1)} X_{(n)} \operatorname{Unif}(a,b)","I would like to know how to calculate the Covariance between the minimum and maximum order statistics from an arbitrary uniform distribution. I am trying to fill in a gap between the answers here: Question 1 and here: Question 2 by taking the approach of finding $E[XY]-E[X]E[Y]$ where $X=X_{(1)},Y=X_{(n)}$ . I have already determined $E[X]=\frac{na+b}{b-a}$ and $E[Y]=\frac{a+nb}{b-a}$ . I am using the following to calculate $E[XY]$ : $$E[XY]=\int_a^b\int_a^bxyf_{X,Y}(x,y)dxdy$$ Using the above formula with i=1 and j=n, $$f_{X,Y}(x,y)=\frac{n!}{(n-2)!}f_X(x)f_X(y)[F_X(y)-F_X(x)]^{n-2}$$ Now $f_X(x)=\frac{1}{b-a}$ and $F_X(x)=\frac{x-a}{b-a},a<x<b$ . This gives: $$E[XY]=\int_a^b\int_a^bxy\frac{1}{(b-a)^2}[\frac{y-x}{b-a}]^{n-2}dxdy$$ $$=\int_a^b\int_a^bxy(b-a)^{-n}[y-x]^{n-2}dxdy$$ Calculating this double integral is where I am stuck. In the $\operatorname{Unif}(0,1)$ case I have seen Beta functions used to solve it, but I am not well-versed enough to do so outside of the standard uniform. Any advice is welcome, thank you!","I would like to know how to calculate the Covariance between the minimum and maximum order statistics from an arbitrary uniform distribution. I am trying to fill in a gap between the answers here: Question 1 and here: Question 2 by taking the approach of finding where . I have already determined and . I am using the following to calculate : Using the above formula with i=1 and j=n, Now and . This gives: Calculating this double integral is where I am stuck. In the case I have seen Beta functions used to solve it, but I am not well-versed enough to do so outside of the standard uniform. Any advice is welcome, thank you!","E[XY]-E[X]E[Y] X=X_{(1)},Y=X_{(n)} E[X]=\frac{na+b}{b-a} E[Y]=\frac{a+nb}{b-a} E[XY] E[XY]=\int_a^b\int_a^bxyf_{X,Y}(x,y)dxdy f_{X,Y}(x,y)=\frac{n!}{(n-2)!}f_X(x)f_X(y)[F_X(y)-F_X(x)]^{n-2} f_X(x)=\frac{1}{b-a} F_X(x)=\frac{x-a}{b-a},a<x<b E[XY]=\int_a^b\int_a^bxy\frac{1}{(b-a)^2}[\frac{y-x}{b-a}]^{n-2}dxdy =\int_a^b\int_a^bxy(b-a)^{-n}[y-x]^{n-2}dxdy \operatorname{Unif}(0,1)","['integration', 'statistics', 'probability-distributions']"
18,Change of discrete random variables,Change of discrete random variables,,"I just read this: Let X be a discrete random variable whose probability function is $f(x)$ . Suppose that a discrete random variable U is defined in terms of X by $U = \phi(X)$ , where to each value of X there corresponds one and only one value of U and conversely, so that $X = \psi (U)$ . Then the probability function for U is given by $g(u) = f [\psi(u)]$ . Is this correct? This is from Schaum's  Probability and Statistics 4th edition pg42. Can any of you please help me understand this? I am sorry I am a bit weak in Probability. Thanks!","I just read this: Let X be a discrete random variable whose probability function is . Suppose that a discrete random variable U is defined in terms of X by , where to each value of X there corresponds one and only one value of U and conversely, so that . Then the probability function for U is given by . Is this correct? This is from Schaum's  Probability and Statistics 4th edition pg42. Can any of you please help me understand this? I am sorry I am a bit weak in Probability. Thanks!",f(x) U = \phi(X) X = \psi (U) g(u) = f [\psi(u)],"['probability', 'probability-theory', 'statistics']"
19,Ask linking the Poisson and gamma families,Ask linking the Poisson and gamma families,,"Let $X_1,..,X_n$ be a random sample from a Poisson population with parameter $\lambda$ and define $Y=\Sigma X_i$ . Y is sufficient for $\lambda$ and $Y \sim Poisson(n\lambda)$ . Now $Y=y_0$ is observed, one equation is $$\Sigma_{k=0}^{y_0} e^{-n\lambda} \frac{(n\lambda)^k}{k!}=\frac{\alpha}{2}$$ Recall the identity linking the Poisson and gamma families: If X is a gamma( $\alpha, \beta$ ) random variable, where $\alpha$ is an integer, then for any x, P(X $\le$ x)=P(Y $\ge \alpha$ ), where $Y \sim Poisson(x/\beta)$ . We can write (remembering that $y_0$ is the observed value of Y): $$\frac{\alpha}{2}=\Sigma_{k=0}^{y_0} e^{-n\lambda} \frac{(n\lambda)^k}{k!}= P(Y \le y_0|\lambda)= P(\chi^2_{2(y_0+1)}>2n\lambda)$$ . I didn't get the above equality. My attempt is: $$\frac{\alpha}{2}=\Sigma_{k=0}^{y_0} e^{-n\lambda} \frac{(n\lambda)^k}{k!}= P(Y \le y_0|\lambda)=1-P(Y \ge y_0+1|\lambda)= 1-P(X \le x) =P(X>x)$$ , where x is gamma( $y_0+1,\frac{x}{n\lambda}$ ). Because I think $x/\beta = n\lambda$ . I don't know how to do next. And I don't know my attempt is correct or not. I also know the identity that $\chi^2_v=gamma(v/2,2)$ Background of this question:","Let be a random sample from a Poisson population with parameter and define . Y is sufficient for and . Now is observed, one equation is Recall the identity linking the Poisson and gamma families: If X is a gamma( ) random variable, where is an integer, then for any x, P(X x)=P(Y ), where . We can write (remembering that is the observed value of Y): . I didn't get the above equality. My attempt is: , where x is gamma( ). Because I think . I don't know how to do next. And I don't know my attempt is correct or not. I also know the identity that Background of this question:","X_1,..,X_n \lambda Y=\Sigma X_i \lambda Y \sim Poisson(n\lambda) Y=y_0 \Sigma_{k=0}^{y_0} e^{-n\lambda} \frac{(n\lambda)^k}{k!}=\frac{\alpha}{2} \alpha, \beta \alpha \le \ge \alpha Y \sim Poisson(x/\beta) y_0 \frac{\alpha}{2}=\Sigma_{k=0}^{y_0} e^{-n\lambda} \frac{(n\lambda)^k}{k!}=
P(Y \le y_0|\lambda)=
P(\chi^2_{2(y_0+1)}>2n\lambda) \frac{\alpha}{2}=\Sigma_{k=0}^{y_0} e^{-n\lambda} \frac{(n\lambda)^k}{k!}=
P(Y \le y_0|\lambda)=1-P(Y \ge y_0+1|\lambda)=
1-P(X \le x)
=P(X>x) y_0+1,\frac{x}{n\lambda} x/\beta = n\lambda \chi^2_v=gamma(v/2,2)","['probability', 'statistics', 'probability-distributions']"
20,"Prove $P(G\mid E_1) > P(G)$ and $P(G\mid E_2) > P(G)$, but $P(G\mid E_1, E_2) < P(G)$","Prove  and , but","P(G\mid E_1) > P(G) P(G\mid E_2) > P(G) P(G\mid E_1, E_2) < P(G)","I have a really simple question. The problem is as below. Let $G$ be the event that a certain individual is guilty of a certain robbery. In gathering evidence, it is learned that an event $E_1$ occurred, and a little later it is also learned that another event $E_2$ also occurred. (a) Is it possible that individually, these pieces of evidence increase the chance of guilt (so $P (G\mid E_1 ) > P (G)$ and $P (G\mid E_2 ) > P (G))$ , but together they decrease the chance of guilt (so $P(G\mid E_1,E_2) < P(G)$ )? The solution is: Yes, this is possible. In fact, it is possible to have two events which separately provide evidence in favor of $G$ , yet which together preclude $G$ ! For example, suppose that the crime was committed between $1$ pm and $3$ pm on a certain day. Let $E_1$ be the event that the suspect was at a nearby co↵eeshop from $1$ pm to $2$ pm that day, and let $E_2$ be the event that the suspect was at the nearby coffeeshop from $2$ pm to $3$ pm that day. Then $P(G\mid E_1) > P(G), P(G\mid E_2) > P(G)$ (assuming that being in the vicinity helps show that the suspect had the opportunity to commit the crime), yet $P(G\mid E_1,E_2) < P(G)$ (as being in the coffeehouse from $1$ pm to $3$ pm gives the suspect an alibi for the full time). I can't understand why the intersection of $E_1$ and $E_2$ is being from $1$ pm to $3$ pm. Isn't it an empty one since there is no intersection between the event from $1$ pm to $2$ pm and the event from $2$ pm to $3$ pm?","I have a really simple question. The problem is as below. Let be the event that a certain individual is guilty of a certain robbery. In gathering evidence, it is learned that an event occurred, and a little later it is also learned that another event also occurred. (a) Is it possible that individually, these pieces of evidence increase the chance of guilt (so and , but together they decrease the chance of guilt (so )? The solution is: Yes, this is possible. In fact, it is possible to have two events which separately provide evidence in favor of , yet which together preclude ! For example, suppose that the crime was committed between pm and pm on a certain day. Let be the event that the suspect was at a nearby co↵eeshop from pm to pm that day, and let be the event that the suspect was at the nearby coffeeshop from pm to pm that day. Then (assuming that being in the vicinity helps show that the suspect had the opportunity to commit the crime), yet (as being in the coffeehouse from pm to pm gives the suspect an alibi for the full time). I can't understand why the intersection of and is being from pm to pm. Isn't it an empty one since there is no intersection between the event from pm to pm and the event from pm to pm?","G E_1 E_2 P (G\mid E_1 ) > P (G) P (G\mid E_2 ) > P (G)) P(G\mid E_1,E_2) < P(G) G G 1 3 E_1 1 2 E_2 2 3 P(G\mid E_1) > P(G), P(G\mid E_2) > P(G) P(G\mid E_1,E_2) < P(G) 1 3 E_1 E_2 1 3 1 2 2 3","['probability', 'statistics', 'conditional-probability']"
21,Distribution of minimum of Uniform products [closed],Distribution of minimum of Uniform products [closed],,"Closed. This question does not meet Mathematics Stack Exchange guidelines . It is not currently accepting answers. Please provide additional context , which ideally explains why the question is relevant to you and our  community. Some forms of context include: background and motivation, relevant definitions, source, possible strategies, your current progress, why the question is interesting or important, etc. Closed 3 years ago . Improve this question I've been stuck on this question for a while. { $U_{i}$ iid, Uniform(0,1) $V_{n} = \prod_{1}^{n} U_i$ , n = 1, 2,... $N = \min \{k: V_k < 0.1\} $ Find the distribution of $N$ I know that this is supposed to be related somehow to a poisson distribution based off of other things that I have learnt, but that was with the summation of uniform distributions, not the product. I honestly just don't know where to begin. Any help would be very much appreciated!","Closed. This question does not meet Mathematics Stack Exchange guidelines . It is not currently accepting answers. Please provide additional context , which ideally explains why the question is relevant to you and our  community. Some forms of context include: background and motivation, relevant definitions, source, possible strategies, your current progress, why the question is interesting or important, etc. Closed 3 years ago . Improve this question I've been stuck on this question for a while. { iid, Uniform(0,1) , n = 1, 2,... Find the distribution of I know that this is supposed to be related somehow to a poisson distribution based off of other things that I have learnt, but that was with the summation of uniform distributions, not the product. I honestly just don't know where to begin. Any help would be very much appreciated!",U_{i} V_{n} = \prod_{1}^{n} U_i N = \min \{k: V_k < 0.1\}  N,"['probability', 'statistics', 'probability-distributions', 'uniform-distribution', 'poisson-distribution']"
22,Covariance Matrix of two random variables,Covariance Matrix of two random variables,,Can two random variables $X$ and $Y$ have the covariance matrix $\begin{pmatrix} 0 & -1 \\ -1 & 0 \end{pmatrix}? $ The matrix is positive definite if $xy<0$ . Does this imply that it could be the convariance matrix for some $X$ and $Y$ ?,Can two random variables and have the covariance matrix The matrix is positive definite if . Does this imply that it could be the convariance matrix for some and ?,X Y \begin{pmatrix} 0 & -1 \\ -1 & 0 \end{pmatrix}?  xy<0 X Y,"['probability', 'statistics', 'random-variables', 'covariance']"
23,Question about Regression,Question about Regression,,"I am solving a problem but I am having a hard time understanding the terminology. I have not been exposed to much statistics/probability before so please bear with me. I am considering a regression model $Y_i = f(x_i)+\epsilon_i$ for $i=1,2,\dots n$ . We are given that $x_i$ are independently and identically distributed in [0,1] and that $\epsilon_i$ are independently and identically distributed normal random variables with standard deviation $\sigma$ . We are of course given $\sigma$ . So the first thing I want to do is generate some random samples $\{(x_j,Y_j)\}$ , but I am unsure how to do this. I feel like I am not understanding how to generate my $x_i$ and $\epsilon_i$ , but once knowing this it should become simple I believe. I know this may be simple, but any help is much appreciated.","I am solving a problem but I am having a hard time understanding the terminology. I have not been exposed to much statistics/probability before so please bear with me. I am considering a regression model for . We are given that are independently and identically distributed in [0,1] and that are independently and identically distributed normal random variables with standard deviation . We are of course given . So the first thing I want to do is generate some random samples , but I am unsure how to do this. I feel like I am not understanding how to generate my and , but once knowing this it should become simple I believe. I know this may be simple, but any help is much appreciated.","Y_i = f(x_i)+\epsilon_i i=1,2,\dots n x_i \epsilon_i \sigma \sigma \{(x_j,Y_j)\} x_i \epsilon_i","['statistics', 'regression']"
24,Understanding Rayleigh distribution,Understanding Rayleigh distribution,,"I am an aerospace enthusiast. I have obtained wind speed data that's been gathered and published by NCEP/NCAR. It provides wind speed data for every $2.5^\circ$ increment in latitude and longitude for 17 altitude levels. I want to predict the upper bound of the wind speed that a given location and altitude will face with a confidence percentage of 99. Wind speed data is usually modelled as a Rayleigh distribution. How can I find this upper bound? I have tried something along these lines, please let me know if I'm going about it the right way. The probability of a wind exceeding $v_i$ for each day ( $d$ ), altitude ( $h$ ), latitude ( $\Phi$ ) and longitude ( $\lambda$ ) as $P(d, h, \phi, \lambda)_i = e^{-\frac{\pi}{4}\left(\frac{v^2}{\mu^2}\right)}$ The percent availability (i.e., the percent of the time that the wind is less than $v_i$ ) is, $\bar{P}(d,h,\phi,\lambda)_i = 1 - P(d,h,\phi,\lambda)_i \times 100%$ So the desired station keeping probability if set at 99%, we need to find $v_i$ that satisfies, $0.99 = \frac{\sum_{d=1}^{365} P(d,h,\phi,\lambda)_i}{365}$ i.e., taking the mean of the probability over 365 days. Is that right?","I am an aerospace enthusiast. I have obtained wind speed data that's been gathered and published by NCEP/NCAR. It provides wind speed data for every increment in latitude and longitude for 17 altitude levels. I want to predict the upper bound of the wind speed that a given location and altitude will face with a confidence percentage of 99. Wind speed data is usually modelled as a Rayleigh distribution. How can I find this upper bound? I have tried something along these lines, please let me know if I'm going about it the right way. The probability of a wind exceeding for each day ( ), altitude ( ), latitude ( ) and longitude ( ) as The percent availability (i.e., the percent of the time that the wind is less than ) is, So the desired station keeping probability if set at 99%, we need to find that satisfies, i.e., taking the mean of the probability over 365 days. Is that right?","2.5^\circ v_i d h \Phi \lambda P(d, h, \phi, \lambda)_i = e^{-\frac{\pi}{4}\left(\frac{v^2}{\mu^2}\right)} v_i \bar{P}(d,h,\phi,\lambda)_i = 1 - P(d,h,\phi,\lambda)_i \times 100% v_i 0.99 = \frac{\sum_{d=1}^{365} P(d,h,\phi,\lambda)_i}{365}","['probability', 'statistics', 'random-variables']"
25,MLE and biasedness,MLE and biasedness,,"Question: Let $X_1, ..., X_n \sim \text{Exp}(\lambda)$ where $E(X) = \lambda$ . Find MLE for $\log(\lambda)$ , and show if it is biased or not. From the description, $f_X(x) = \lambda^{-1}\exp(-\lambda^{-1}x)$ I know that $\bar{X}$ is the MLE for $\lambda$ , and since $\log$ is a bijection, and overall a ""good"" function, invariance of MLE applies. So MLE of $\log(\lambda) = \log$ of MLE of $\lambda = \log(\bar{X})$ . I know $\bar{X}$ is an unbiased estimator of $\lambda$ , and I doubt $E(\log(\bar{X})) = \log(\lambda)$ because of Jensen's inequality. Is there a better way of proving this (preferably not calculating the integral), especially when Jensen's inequality doesn't apply? In addition, the follow-up question asks me to show that MLE of $\log(\lambda)$ is ""CAN"" for $\log(\lambda)$ , and then identify its asymptotic normal variance. I suppose ""AN"" in ""CAN"" means asymptotic normal. Does any one know what the ""C"" might stand for? Also, how do I calculate its asymptotic normal variance? Thanks!","Question: Let where . Find MLE for , and show if it is biased or not. From the description, I know that is the MLE for , and since is a bijection, and overall a ""good"" function, invariance of MLE applies. So MLE of of MLE of . I know is an unbiased estimator of , and I doubt because of Jensen's inequality. Is there a better way of proving this (preferably not calculating the integral), especially when Jensen's inequality doesn't apply? In addition, the follow-up question asks me to show that MLE of is ""CAN"" for , and then identify its asymptotic normal variance. I suppose ""AN"" in ""CAN"" means asymptotic normal. Does any one know what the ""C"" might stand for? Also, how do I calculate its asymptotic normal variance? Thanks!","X_1, ..., X_n \sim \text{Exp}(\lambda) E(X) = \lambda \log(\lambda) f_X(x) = \lambda^{-1}\exp(-\lambda^{-1}x) \bar{X} \lambda \log \log(\lambda) = \log \lambda = \log(\bar{X}) \bar{X} \lambda E(\log(\bar{X})) = \log(\lambda) \log(\lambda) \log(\lambda)","['probability-theory', 'statistics', 'maximum-likelihood', 'parameter-estimation']"
26,What is the pdf of Z = XY?,What is the pdf of Z = XY?,,"Consider two random variables $X$ and $Y$ with joint pdf $f_{XY}(x,y)$ . Determine the pdf of $Z = XY$ My friend said he solved this using a Jacobian matrix. I'm not well versed  in that topic so I tried solving it the way I solved similar problems. Here is the hyperbola formed when $Z = 2$ : From this figure, I can separate my original probability into the probability that $X > 0$ and the probability that $X < 0$ : $$ P(Z\leq z)=P(XY\leq z)= P(Y\leq \frac{z}{X}\big|X>0)P(X>0)+P(Y\geq \frac{z}{X}\big|X<0)P(X<0)= $$ $$  \int\limits_{0}^{\infty}\int\limits_{-\infty}^{\frac{z}{x}}f_{X,Y}(x,y)dydx +  \int\limits_{-\infty}^{0}\int\limits_{\frac{z}{x}}^{\infty}f_{X,Y}(x,y)dydx $$ I'm not sure how to proceed from here. Do I need to take partial derivatives of each integral because it is a joint pdf? I'm trying to solve this problem using this CDF -> to PDF method because that conceptually makes sense to me. Is it possible to solve it this way?","Consider two random variables and with joint pdf . Determine the pdf of My friend said he solved this using a Jacobian matrix. I'm not well versed  in that topic so I tried solving it the way I solved similar problems. Here is the hyperbola formed when : From this figure, I can separate my original probability into the probability that and the probability that : I'm not sure how to proceed from here. Do I need to take partial derivatives of each integral because it is a joint pdf? I'm trying to solve this problem using this CDF -> to PDF method because that conceptually makes sense to me. Is it possible to solve it this way?","X Y f_{XY}(x,y) Z = XY Z = 2 X > 0 X < 0 
P(Z\leq z)=P(XY\leq z)=
P(Y\leq \frac{z}{X}\big|X>0)P(X>0)+P(Y\geq \frac{z}{X}\big|X<0)P(X<0)=
 
 \int\limits_{0}^{\infty}\int\limits_{-\infty}^{\frac{z}{x}}f_{X,Y}(x,y)dydx +  \int\limits_{-\infty}^{0}\int\limits_{\frac{z}{x}}^{\infty}f_{X,Y}(x,y)dydx
","['calculus', 'probability', 'statistics']"
27,Finding outliers using interquartile range,Finding outliers using interquartile range,,"I am trying to find outliers using interquartile range and is having a few problems. I have read this article to understand how to find outliers and understood most of it. Now I am trying to apply this method in a program. But it seems that the data I am using doesn't work with this method. The data I am using is more than 4000 rows of data and can be found in this link . The minimum value is: 951,723112057644 The maximum value is: 1588,93458298046 Q1 Median is: 1273,39127623714 Q3 Median is: 1273,52543277336 IQR is: 0,13415653622 With the above result my inner fence range is between 1273,19004143281018 and 1273,726667577769002. Then it means that I have a lot of data are candidates to removed as outliers. I would call them outliers. The data set that I use have a lot of data between 1273 and 1274, I believe it's more than 50% that are between this range. Is using IQR a suitable method to find outliers, are there any other method that I should use instead?","I am trying to find outliers using interquartile range and is having a few problems. I have read this article to understand how to find outliers and understood most of it. Now I am trying to apply this method in a program. But it seems that the data I am using doesn't work with this method. The data I am using is more than 4000 rows of data and can be found in this link . The minimum value is: 951,723112057644 The maximum value is: 1588,93458298046 Q1 Median is: 1273,39127623714 Q3 Median is: 1273,52543277336 IQR is: 0,13415653622 With the above result my inner fence range is between 1273,19004143281018 and 1273,726667577769002. Then it means that I have a lot of data are candidates to removed as outliers. I would call them outliers. The data set that I use have a lot of data between 1273 and 1274, I believe it's more than 50% that are between this range. Is using IQR a suitable method to find outliers, are there any other method that I should use instead?",,['statistics']
28,Find value of test statistic given significance and unknown mean amount,Find value of test statistic given significance and unknown mean amount,,"""Suppose that in past years the average price per square metre for warehouses in Canada has been 347.46 dollars. A national real estate investor wants to determine whether that figure has changed now. The investor hires a researcher who randomly samples 48 warehouses that are for sale across Canada and finds that the mean price per square foot is 339.80 dollars, with a standard deviation of $13.89. Assume that prices of warehouse area are normally distributed in population. If the researcher uses a 5% level of significance, what statistical conclusion can be reached?"" Attempt: n=48 Xbar= $339.80 o=$13.89 a=0.05 (xbar-u)/(o/rootn)=3.82=test statistic Reject null hypothesis The numerical answer is wrong so what mistake did I make? Whats the correct equation to solve for test equation.","""Suppose that in past years the average price per square metre for warehouses in Canada has been 347.46 dollars. A national real estate investor wants to determine whether that figure has changed now. The investor hires a researcher who randomly samples 48 warehouses that are for sale across Canada and finds that the mean price per square foot is 339.80 dollars, with a standard deviation of $13.89. Assume that prices of warehouse area are normally distributed in population. If the researcher uses a 5% level of significance, what statistical conclusion can be reached?"" Attempt: n=48 Xbar= $339.80 o=$13.89 a=0.05 (xbar-u)/(o/rootn)=3.82=test statistic Reject null hypothesis The numerical answer is wrong so what mistake did I make? Whats the correct equation to solve for test equation.",,"['statistics', 'hypothesis-testing']"
29,Is there something wrong with this explanation of Chebyshev's Inequality?,Is there something wrong with this explanation of Chebyshev's Inequality?,,"So I was revising (more like re-learning coz I suck) Chebyshev's Inequality using this document . Specifically, I am referring to Example $7$ on Page $3$ , which I shall reproduce below. Question A coin is weighted so that the probability of landing on heads is $0.2$ . Suppose the coin is flipped $20$ times. Using Chebyshev's Inequality, find a bound for the probability it lands on heads at least $16$ times. Answer \begin{align} P(X \geq 16) & = P(0 \leq X \leq 16) \\[5 mm] & = P(-8 \leq X \leq 16) \\[5 mm] & = P(|X - 4| \geq 12) \\[5 mm] & \leq \frac {Var(X)} {12^2} \\[5 mm] & = \frac {(20)(0.2)(0.8)} {144} \\[5 mm] & = \frac 1 {45} \end{align} I understand if we skip the first jump from $P(X \geq 16)$ to $P(|X - 4| \geq 12)$ because, as suggested in the document, $X \geq 0$ . However, what I cannot get is how the first equality makes sense. In particular, how does $P(X \geq 16)$ equal $P(0 \leq X \leq 16)$ ? Also, how does $P(-8 \leq X \leq 16)$ equal $P(|X - 4| \geq 12)$ ? I know the answer is correct but I cannot help but feel that the proof is not entirely mathematically right. Anyone who can explain why the first $3$ equalities make sense please do tell.","So I was revising (more like re-learning coz I suck) Chebyshev's Inequality using this document . Specifically, I am referring to Example on Page , which I shall reproduce below. Question A coin is weighted so that the probability of landing on heads is . Suppose the coin is flipped times. Using Chebyshev's Inequality, find a bound for the probability it lands on heads at least times. Answer I understand if we skip the first jump from to because, as suggested in the document, . However, what I cannot get is how the first equality makes sense. In particular, how does equal ? Also, how does equal ? I know the answer is correct but I cannot help but feel that the proof is not entirely mathematically right. Anyone who can explain why the first equalities make sense please do tell.","7 3 0.2 20 16 \begin{align}
P(X \geq 16) & =
P(0 \leq X \leq 16)
\\[5 mm] & =
P(-8 \leq X \leq 16)
\\[5 mm] & =
P(|X - 4| \geq 12)
\\[5 mm] & \leq
\frac {Var(X)} {12^2}
\\[5 mm] & =
\frac {(20)(0.2)(0.8)} {144}
\\[5 mm] & =
\frac 1 {45}
\end{align} P(X \geq 16) P(|X - 4| \geq 12) X \geq 0 P(X \geq 16) P(0 \leq X \leq 16) P(-8 \leq X \leq 16) P(|X - 4| \geq 12) 3","['probability', 'probability-theory', 'statistics', 'probability-distributions']"
30,Expectations of mean squared error,Expectations of mean squared error,,"I am currently studying the expected mean squared error and the derivations of this are as follows: \begin{align} &E[(y_i - \hat{y}_0 | x_0]\\ &=E [ (f(x) + \epsilon_i - \hat{f}(x_i))^2 | x_0]\\ &=E [(f(x) - \hat{f}(x_i))^2|x_0] + 2 E [(f(x_i)-\hat{f}(x_i))\epsilon_i | x_0] + E[\epsilon_0^2 |x_0]\\ \end{align} From which we can further get the bias and variance. But I do not understand the following, what rule is used in order to rewrite it using the following part: $2 E [(f(x_i)-\hat{f}(x_i))\epsilon_i | x_0]$ ?","I am currently studying the expected mean squared error and the derivations of this are as follows: From which we can further get the bias and variance. But I do not understand the following, what rule is used in order to rewrite it using the following part: ?","\begin{align}
&E[(y_i - \hat{y}_0 | x_0]\\
&=E [ (f(x) + \epsilon_i - \hat{f}(x_i))^2 | x_0]\\
&=E [(f(x) - \hat{f}(x_i))^2|x_0] + 2 E [(f(x_i)-\hat{f}(x_i))\epsilon_i | x_0] + E[\epsilon_0^2 |x_0]\\
\end{align} 2 E [(f(x_i)-\hat{f}(x_i))\epsilon_i | x_0]","['statistics', 'expected-value', 'mean-square-error']"
31,Hypothesis testing - When to subtract one during type I and type II testing?,Hypothesis testing - When to subtract one during type I and type II testing?,,"I'm currently studying statistics and I'm reviewing notes I took during a class last week. However, there's something I'm confused about. My professor subtracted 1 from the ""successes"" and I can't seem to figure out why. Essentially, the scenario was like this: Someone flipped a coin 16 times. $$H_0:p=0.5\text{ v.s. }H_a:p=0.55$$ Test 1: Reject $p = 0.50$ if 10 or more heads are observed out of 16. $\text{Pr}(X \ge  10 \text{ when } p= 0.5)$ where X is a binomial with $n = 16$ and $p = 0.50$ . This is the part I don't understand. When using R, he did the following: 1 - pbinom(9, 16, 0.5) Where did the 9 come from? Why did he subtract 1 from the initial 10 tosses? If I was in a scenario where I had: $$\text{Pr}(X \le 15 \text{ when } p = 0.50)$$ Would I still subtract 1? Would the R solution be pbinom(15, 16, 0.50) or pbinom(14, 16, 0.50) ? How do we decide when to subtract 1 like this? ( Sorry if my formatting is bad! I've never posted math equations like this to these forums and I can't seem to figure it out)","I'm currently studying statistics and I'm reviewing notes I took during a class last week. However, there's something I'm confused about. My professor subtracted 1 from the ""successes"" and I can't seem to figure out why. Essentially, the scenario was like this: Someone flipped a coin 16 times. Test 1: Reject if 10 or more heads are observed out of 16. where X is a binomial with and . This is the part I don't understand. When using R, he did the following: 1 - pbinom(9, 16, 0.5) Where did the 9 come from? Why did he subtract 1 from the initial 10 tosses? If I was in a scenario where I had: Would I still subtract 1? Would the R solution be pbinom(15, 16, 0.50) or pbinom(14, 16, 0.50) ? How do we decide when to subtract 1 like this? ( Sorry if my formatting is bad! I've never posted math equations like this to these forums and I can't seem to figure it out)",H_0:p=0.5\text{ v.s. }H_a:p=0.55 p = 0.50 \text{Pr}(X \ge  10 \text{ when } p= 0.5) n = 16 p = 0.50 \text{Pr}(X \le 15 \text{ when } p = 0.50),"['statistics', 'hypothesis-testing']"
32,Reference algorithm/formula for the distribution of the median of random variables?,Reference algorithm/formula for the distribution of the median of random variables?,,"The distribution of the mean of two random variables can be calculated using a convolution. I have a collection of $n$ independent random variables each with PDFs that are simple functions on $[0,1]$ . I would like to know the exact distribution of the median of these variables. I understand there is a central limit theorem for the distribution of the sample median for i.i.d variables, but I don't have that assumption here. I also see that there's a way to get a formula for discrete random variables. Is there a reference for continuous random variables?","The distribution of the mean of two random variables can be calculated using a convolution. I have a collection of independent random variables each with PDFs that are simple functions on . I would like to know the exact distribution of the median of these variables. I understand there is a central limit theorem for the distribution of the sample median for i.i.d variables, but I don't have that assumption here. I also see that there's a way to get a formula for discrete random variables. Is there a reference for continuous random variables?","n [0,1]","['statistics', 'reference-request', 'random-variables', 'order-statistics', 'median']"
33,Hypothesis testing in elections,Hypothesis testing in elections,,"The following is a problem I am working on for my stats class regarding hypothesis testing: I have a random sample $X_1,...,X_n$ of voters who either voted for Candidate A ( $X_1=1$ ) or for Candidate B ( $X_i=0$ ), with probability of voting for A denoted as $p$ . I want to test the hypothesis that A will not lose the election: $H_0:p\geq 0.5$ vs. $H_1:p<0.5$ . I set the null parameter space $\Theta_0=[0.5,1]$ and the power test to $\beta_\psi(p)=\mathbb{P}_p(T\leq c)=\sum_{k=0}^c{n\choose k}p^k(1-p)^{n-k}$ , observing that for $c\in\{0,...,n-1\}$ , the power test is strictly decreasing in $p$ and that size of $\psi$ is thus $\beta_\psi(0.5)$ . Now, I want to find $c$ so that the power test is at level 5% with the largest possible power, at sample size $n=100$ . Is it correct for me to assume that I must find the largest $c$ such that $\beta_\psi(0.5)\leq 0.05$ ? I do intuit that this is because of the null hypothesis $H_0:p\geq 0.5$ and $c$ being in the ""same direction"" as $p$ (number of A voters in the sample is used to ""estimate"" the probability of voting A, which we want to increase to the minimal degree to satisfy the level- $\alpha$ test; the ""increasing"" being to control type I error and the ""to the minimal degree"" being to minimize type II error). Could somebody please verify this? If there are any mistakes or a more rigorous explanation in what I said, please let me know.","The following is a problem I am working on for my stats class regarding hypothesis testing: I have a random sample of voters who either voted for Candidate A ( ) or for Candidate B ( ), with probability of voting for A denoted as . I want to test the hypothesis that A will not lose the election: vs. . I set the null parameter space and the power test to , observing that for , the power test is strictly decreasing in and that size of is thus . Now, I want to find so that the power test is at level 5% with the largest possible power, at sample size . Is it correct for me to assume that I must find the largest such that ? I do intuit that this is because of the null hypothesis and being in the ""same direction"" as (number of A voters in the sample is used to ""estimate"" the probability of voting A, which we want to increase to the minimal degree to satisfy the level- test; the ""increasing"" being to control type I error and the ""to the minimal degree"" being to minimize type II error). Could somebody please verify this? If there are any mistakes or a more rigorous explanation in what I said, please let me know.","X_1,...,X_n X_1=1 X_i=0 p H_0:p\geq 0.5 H_1:p<0.5 \Theta_0=[0.5,1] \beta_\psi(p)=\mathbb{P}_p(T\leq c)=\sum_{k=0}^c{n\choose k}p^k(1-p)^{n-k} c\in\{0,...,n-1\} p \psi \beta_\psi(0.5) c n=100 c \beta_\psi(0.5)\leq 0.05 H_0:p\geq 0.5 c p \alpha","['statistics', 'statistical-inference', 'hypothesis-testing', 'log-likelihood']"
34,Find the asymptotic distribution of δ,Find the asymptotic distribution of δ,,"Let $X_1, \ldots, X_n$ i.i.d. from the $\mathrm{Bernoulli}(p)$ distribution. Then $X = X_1 +\ldots+X_n$ follows the $\mathrm{Binomial}(n,p)$ distribution. We wish to estimate $g(p)=p(1-p)$ whose UMVUE is $$\delta=\frac{X(n−X)}{n(n−1)} $$ Find the asymptotic distribution of $\delta$ . I tried: let $$ \delta^*=\frac{X(n-X))}{n^2}=\frac{X}{n}-\left(\frac{X}{n}\right)^2=\frac{\sum_{i=0}^n X_i}{n}-\left(\frac{\sum_{i=0}^n X_i}{n}\right)^2=\bar X-(\bar X)^2 $$ so, $$ \delta=\frac{X(n-X)}{n(n-1)}=\frac{n}{n-1}\big(\bar X - (\bar X)^2\big) $$ Let, $$h(\mu) = \left(\frac{n}{n-1}\right)\big(\mu-(\mu)^2\big) $$ So, $$h^\prime(\mu)=\frac{n}{n-1}-2\mu\left(\frac{n}{n-1}\right)$$ hence, the asymptotic distribution of $\delta$ is $$ \sqrt n {\big(h(\bar x) - h(\mu)\big)}\xrightarrow{L} N\big(0, (h^\prime(\mu))^2\sigma^2\big)=N\big(0, \big(\frac{n}{n-1}-2\mu(\frac{n}{n-1})\big)^2\big(np(1-p)\big)^2\big) $$ By the Delta method. But, to use the Delta method, I must prove $\delta$ and $g(p)$ are consistent. How do I prove it? Thank you very much!","Let i.i.d. from the distribution. Then follows the distribution. We wish to estimate whose UMVUE is Find the asymptotic distribution of . I tried: let so, Let, So, hence, the asymptotic distribution of is By the Delta method. But, to use the Delta method, I must prove and are consistent. How do I prove it? Thank you very much!","X_1, \ldots, X_n \mathrm{Bernoulli}(p) X = X_1 +\ldots+X_n \mathrm{Binomial}(n,p) g(p)=p(1-p) \delta=\frac{X(n−X)}{n(n−1)}
 \delta 
\delta^*=\frac{X(n-X))}{n^2}=\frac{X}{n}-\left(\frac{X}{n}\right)^2=\frac{\sum_{i=0}^n X_i}{n}-\left(\frac{\sum_{i=0}^n X_i}{n}\right)^2=\bar X-(\bar X)^2
 
\delta=\frac{X(n-X)}{n(n-1)}=\frac{n}{n-1}\big(\bar X - (\bar X)^2\big)
 h(\mu) = \left(\frac{n}{n-1}\right)\big(\mu-(\mu)^2\big)  h^\prime(\mu)=\frac{n}{n-1}-2\mu\left(\frac{n}{n-1}\right) \delta 
\sqrt n {\big(h(\bar x) - h(\mu)\big)}\xrightarrow{L} N\big(0, (h^\prime(\mu))^2\sigma^2\big)=N\big(0, \big(\frac{n}{n-1}-2\mu(\frac{n}{n-1})\big)^2\big(np(1-p)\big)^2\big)
 \delta g(p)",['statistics']
35,What is the autocovariance of $x_t = U_1 \sin(2 \pi \omega_0 t) + U_2 \cos(2 \pi \omega_0 t)$,What is the autocovariance of,x_t = U_1 \sin(2 \pi \omega_0 t) + U_2 \cos(2 \pi \omega_0 t),"A time series with a periodic component can be constructed from $$x_t=U_1\sin(2\pi \omega_0t)+U_2\cos(2\pi \omega_0t),$$ where $U_1$ and $U_2$ are independent random variables with zero means and $$E(U_1^2)=E(U_2^2)=\sigma^2$$ Show this series is weakly stationary with autocovariance $\gamma(h) = \sigma^2 \cos(2 \pi \omega_0 h)$ . So I set $p = 2\pi\omega_0$ I have that the mean function is $E(x_t) = E(U_1 \sin(pt)) + E(U_2 \cos(pt)) = 0$ which is not dependent on $t$ What I have for covariance is based on the following property: If the random variables $U=\sum^m_{j=1}a_jX_j$ and $V=\sum^r_{k=1}b_kY_k$ are linear combinations of (finite variance) random variables ${X_j}$ and ${Y_k}$ , respectively, then $cov(U,V)=\sum^m_{j=1}\sum^r_{k=1}a_jb_k\operatorname{cov}((X_j,Y_k)$ Furthermore, $\operatorname{var}((U)=\operatorname{cov}((U,U)$ Then setting $U = U_1 \sin(p(t+h)) + U_2 \cos(p(t+h))$ and $V = U_1 \sin(p(t)) + U_2 \cos(p(t))$ with $a_1 = \sin(p(t+h))$ and $a_2 = \cos(p(t+h))$ and $b_1 = \sin(pt)$ and $b_2 = \cos(pt)$ and $X_1 = Y_1 = U_1$ and $X_2 = Y_2 = U_2$ And noting that $\operatorname{cov}((U_1, U_2) = 0$ and $\operatorname{var}(U_1) = \operatorname{var}(U_2) = \sigma^2$ we have $\gamma(h) = \operatorname{cov}(U, V) $ $= \sigma^2 [\sin(p(t+h))\sin(pt) + \cos(p(t+h)) \cos(pt)]$ see rest of solution below","A time series with a periodic component can be constructed from where and are independent random variables with zero means and Show this series is weakly stationary with autocovariance . So I set I have that the mean function is which is not dependent on What I have for covariance is based on the following property: If the random variables and are linear combinations of (finite variance) random variables and , respectively, then Furthermore, Then setting and with and and and and and And noting that and we have see rest of solution below","x_t=U_1\sin(2\pi \omega_0t)+U_2\cos(2\pi \omega_0t), U_1 U_2 E(U_1^2)=E(U_2^2)=\sigma^2 \gamma(h) = \sigma^2 \cos(2 \pi \omega_0 h) p = 2\pi\omega_0 E(x_t) = E(U_1 \sin(pt)) + E(U_2 \cos(pt)) = 0 t U=\sum^m_{j=1}a_jX_j V=\sum^r_{k=1}b_kY_k {X_j} {Y_k} cov(U,V)=\sum^m_{j=1}\sum^r_{k=1}a_jb_k\operatorname{cov}((X_j,Y_k) \operatorname{var}((U)=\operatorname{cov}((U,U) U = U_1 \sin(p(t+h)) + U_2 \cos(p(t+h)) V = U_1 \sin(p(t)) + U_2 \cos(p(t)) a_1 = \sin(p(t+h)) a_2 = \cos(p(t+h)) b_1 = \sin(pt) b_2 = \cos(pt) X_1 = Y_1 = U_1 X_2 = Y_2 = U_2 \operatorname{cov}((U_1, U_2) = 0 \operatorname{var}(U_1) = \operatorname{var}(U_2) = \sigma^2 \gamma(h) = \operatorname{cov}(U, V)  = \sigma^2 [\sin(p(t+h))\sin(pt) + \cos(p(t+h)) \cos(pt)]","['statistics', 'time-series', 'stationary-processes']"
36,Markov chain exercise 1,Markov chain exercise 1,,"I have the transition matrix $$P= \begin{pmatrix} 1 & 0 & 0\\ 0 & 0 & 1\\ \frac14 & \frac14 & \frac12 \end{pmatrix}$$ and I have to determine the period of each state, compute $f_{2,2}{(n)}$ for each $n\geq 1$ and compute $p_2(X_n=3)$ I think $f_{2,2}{(n)} = p_{(2,3)}p_{(3,2)}=1·\frac14=\frac14$ but I am not sure and the last part I have $p_i(X_n=j)=p_{i,j}{(n)}$ but I don't know how to do it thank you","I have the transition matrix and I have to determine the period of each state, compute for each and compute I think but I am not sure and the last part I have but I don't know how to do it thank you","P= \begin{pmatrix}
1 & 0 & 0\\
0 & 0 & 1\\
\frac14 & \frac14 & \frac12
\end{pmatrix} f_{2,2}{(n)} n\geq 1 p_2(X_n=3) f_{2,2}{(n)} = p_{(2,3)}p_{(3,2)}=1·\frac14=\frac14 p_i(X_n=j)=p_{i,j}{(n)}","['probability-theory', 'statistics', 'stochastic-processes', 'markov-chains', 'risk-assessment']"
37,Conditional expectation given sum,Conditional expectation given sum,,"There are two random variables $X$ and $Y$ , both of them distributed standard normal. Based on $X$ and $Y$ , define $Z = X + a  Y$ for $a$ a known parameter. I'm interested in $E(X | Z)$ . It seems to me that a starting point could be the approach taken here . So I start with: $ Z = E( X | Z) + a E(Y | Z) $ and then I write out the expectation terms explicitly, hoping to find an easy substitution: $ E(X | Z) = \frac{\int_{-\infty}^{\infty}x f(x)f(\frac{Z-x}{a})dx}{\int_{-\infty}^{\infty} f(x)f(\frac{Z-x}{a})dx} $ and $ E(Y | Z) = \frac{\int_{-\infty}^{\infty}y f(y)f(Z-ay)dy}{\int_{-\infty}^{\infty} f(y)f(Z-ay)dy} $ But here I'm stuck, I can't find an substition that would allow me to express $E(Y|Z)$ in terms of $E(X|Z)$ , in order to plug in above. So how to continue from here? Edit: For $a =1$ , one can simply use $E(X|Z) = E(Y | Z)$ , plug in above, and obtain $E(X|Z) = Z/2$ . I conjecture that in general, it holds that $$E(X|Z) = \frac{Z}{1 + a^2}.$$ This is supported by several numerical simulations I made. This would require showing that $E(Y|Z) = a E(X | Z)$ . In case substituting in the integral does not help, how else could I show that?","There are two random variables and , both of them distributed standard normal. Based on and , define for a known parameter. I'm interested in . It seems to me that a starting point could be the approach taken here . So I start with: and then I write out the expectation terms explicitly, hoping to find an easy substitution: and But here I'm stuck, I can't find an substition that would allow me to express in terms of , in order to plug in above. So how to continue from here? Edit: For , one can simply use , plug in above, and obtain . I conjecture that in general, it holds that This is supported by several numerical simulations I made. This would require showing that . In case substituting in the integral does not help, how else could I show that?",X Y X Y Z = X + a  Y a E(X | Z)  Z = E( X | Z) + a E(Y | Z)   E(X | Z) = \frac{\int_{-\infty}^{\infty}x f(x)f(\frac{Z-x}{a})dx}{\int_{-\infty}^{\infty} f(x)f(\frac{Z-x}{a})dx}   E(Y | Z) = \frac{\int_{-\infty}^{\infty}y f(y)f(Z-ay)dy}{\int_{-\infty}^{\infty} f(y)f(Z-ay)dy}  E(Y|Z) E(X|Z) a =1 E(X|Z) = E(Y | Z) E(X|Z) = Z/2 E(X|Z) = \frac{Z}{1 + a^2}. E(Y|Z) = a E(X | Z),"['probability', 'statistics', 'conditional-expectation']"
38,Prove subset sum of squares errors are less than sum of squares error of the original set,Prove subset sum of squares errors are less than sum of squares error of the original set,,"As part of a proof I'd like to show if $R_1 \cup R_2 \subseteq R_3$ , then $SSE(R_1) + SSE(R_2) \leq SSE(R_3)$ where SSE is the sum of squared deviations from the mean of each group.  I'm assuming the values of the group are real or rational numbers if that makes a difference. I know that since the mean minimizes the sum of squared errors, then each group must be minimized, I'm stuck on how to leverage the fact that the groups are subsets to show that $SSE(R_1 \cup R_2) \geq SSE(R_1) + SSE(R_2)$ .  Is there a basic piece of theory I am missing?  Any pointers would be appreciated!","As part of a proof I'd like to show if , then where SSE is the sum of squared deviations from the mean of each group.  I'm assuming the values of the group are real or rational numbers if that makes a difference. I know that since the mean minimizes the sum of squared errors, then each group must be minimized, I'm stuck on how to leverage the fact that the groups are subsets to show that .  Is there a basic piece of theory I am missing?  Any pointers would be appreciated!",R_1 \cup R_2 \subseteq R_3 SSE(R_1) + SSE(R_2) \leq SSE(R_3) SSE(R_1 \cup R_2) \geq SSE(R_1) + SSE(R_2),"['statistics', 'proof-writing', 'sums-of-squares']"
39,"Question regarding a ""Without loss of generality"" statement in proof of optimal mean-square estimator.","Question regarding a ""Without loss of generality"" statement in proof of optimal mean-square estimator.",,"The following question is from a statement in a proof (p. 287 in Shiryaev I Third edition) that the conditional expectation of $\eta \mid \xi=x $ is the optimal estimator of $\eta$ in terms of $\xi$ . Given a pair of random variables $( \eta, \xi )$ , we may try to find a Borel function $\phi$ such that $\phi(\xi)$ estimates $\eta$ . The optimal such estimator is the one that minimizes the mean-squared error. There is a theorem that states that if $\mathrm{E} \eta^2 < \infty$ then $$ \phi(x) = \mathrm E (\eta \mid \xi = x ) $$ can be taken as the optimal estimator. In the first line of the proof it is stated that: ""Without loss of generality we may consider only estimators $\phi(\xi)$ for which $\mathrm \phi^2(\xi) < \infty$ ."" I have a hard time figuring out what is meant with this? I can see that, if $\phi ^2 (\xi ) = \infty $ , then $$ \mathrm E \left[ \eta - \phi (\xi) \right] ^2 = \mathrm E \eta^2 - 2 \mathrm E \eta \phi(\xi) + \mathrm E \phi^2(\xi) = \infty $$ (assuming $\mathrm E \phi(\xi) < \infty$ and that we have defined addition with extended real numbers $r < \infty$ such that $r + \infty = \infty$ ), and if there is no better estimator then perhaps the term ""optimal"" does not give much information? I have included a screenshot of the whole section below.","The following question is from a statement in a proof (p. 287 in Shiryaev I Third edition) that the conditional expectation of is the optimal estimator of in terms of . Given a pair of random variables , we may try to find a Borel function such that estimates . The optimal such estimator is the one that minimizes the mean-squared error. There is a theorem that states that if then can be taken as the optimal estimator. In the first line of the proof it is stated that: ""Without loss of generality we may consider only estimators for which ."" I have a hard time figuring out what is meant with this? I can see that, if , then (assuming and that we have defined addition with extended real numbers such that ), and if there is no better estimator then perhaps the term ""optimal"" does not give much information? I have included a screenshot of the whole section below.","\eta \mid \xi=x  \eta \xi ( \eta, \xi ) \phi \phi(\xi) \eta \mathrm{E} \eta^2 < \infty 
\phi(x) = \mathrm E (\eta \mid \xi = x )
 \phi(\xi) \mathrm \phi^2(\xi) < \infty \phi ^2 (\xi ) = \infty  
\mathrm E \left[ \eta - \phi (\xi) \right] ^2 = \mathrm E \eta^2 - 2 \mathrm E \eta \phi(\xi) + \mathrm E \phi^2(\xi) = \infty
 \mathrm E \phi(\xi) < \infty r < \infty r + \infty = \infty","['probability-theory', 'statistics', 'parameter-estimation']"
40,Find $P(X_{(1)} < \mu < X_{(4)})$,Find,P(X_{(1)} < \mu < X_{(4)}),"Let $X_1, X_2, X3, X4$ be iid random variables from a normal distribution with a mean of $\mu$ . Find $P(X_{(1)} < \mu < X_{(4)})$ My try: $P(X_{(1)} < \mu < X_{(4)})$ = $P(\mu < X_{(4)})$ - $P(\mu < X_{(1)})$ And I know how to get the distributions of $X_{(1)}$ and $X_{(4)}$ . But I think there must be a simpler way to find $P(X_{(1)} < \mu < X_{(4)})$ . Any suggestions would be great!",Let be iid random variables from a normal distribution with a mean of . Find My try: = - And I know how to get the distributions of and . But I think there must be a simpler way to find . Any suggestions would be great!,"X_1, X_2, X3, X4 \mu P(X_{(1)} < \mu < X_{(4)}) P(X_{(1)} < \mu < X_{(4)}) P(\mu < X_{(4)}) P(\mu < X_{(1)}) X_{(1)} X_{(4)} P(X_{(1)} < \mu < X_{(4)})","['probability', 'statistics', 'probability-distributions', 'normal-distribution']"
41,Showing equivalence of expressions for the joint probability of two events,Showing equivalence of expressions for the joint probability of two events,,"If I have random variable $v$ distributed by CDF $G$ on $[\underline{v},\bar{v}]$ , I know that $Pr(v\geq a-x \hspace{1mm} and \hspace{1mm} v\geq a)=Pr(v\geq a-x | v  \geq a)Pr(v\geq a)= Pr(v\geq a)$ , where $x$ is non-negative. But I cannot seem to get the same answer if instead I write $Pr(v\geq a | v\geq a-x)Pr(v\geq a-x)$ . These two should clearly be equivalent. Thank you.","If I have random variable distributed by CDF on , I know that , where is non-negative. But I cannot seem to get the same answer if instead I write . These two should clearly be equivalent. Thank you.","v G [\underline{v},\bar{v}] Pr(v\geq a-x \hspace{1mm} and \hspace{1mm} v\geq a)=Pr(v\geq a-x | v  \geq a)Pr(v\geq a)= Pr(v\geq a) x Pr(v\geq a | v\geq a-x)Pr(v\geq a-x)","['probability', 'statistics']"
42,Expected Value of MSR in General Linear Model,Expected Value of MSR in General Linear Model,,"The setting of my problem is as follows: Consider the general linear model $Y = X\boldsymbol{\beta} + \epsilon$ , where $\beta = \begin{bmatrix} \beta_0 \\ \boldsymbol{\beta_1} \end{bmatrix}$ and $X = \begin{bmatrix} \mathbf{1} & \mathbf{X_1} \end{bmatrix}$ , $\epsilon_i$ are independent with mean zero and variance $\sigma^2$ . Consider the case where at least one element of $\boldsymbol{\beta_1}$ is not equal to zero. Prove that $\mathbb{E}[MSR]>\sigma^2$ . What I did so far: $SSR = \sum(\hat{Y_i}-\bar{Y})^2 = \hat{Y}^T\hat{Y}-n\bar{Y}^2=Y^THY-n\bar{Y}^2$ where $H = X(X^TX)^{-1}X^T$ . Using the identity $\mathbb{E}[Y^THY]=\mathbb{E}[Y]^TH\mathbb{E}[Y]+tr[H*Var(Y)]$ I was able to get: $\mathbb{E}[SSR] = \beta^TX^TX\beta+p\sigma^2-n\bar{Y}^2$ This is where I am stuck. It would suffice to show $\beta^TX^TX\beta-n\bar{Y}^2>0$ , for then $MSR = \frac{SSR}{p-1} > \frac{p\sigma^2}{p-1} > \sigma^2$ , but I cannot seem to figure out how to do that. Your help is much appreciated! $\mathbf{EDIT}$ My initial attempt was incorrect: I moved $\bar{Y}^2$ past the expectation, which is obviously wrong. Now, I attempted the problem again and I think it may have worked: $$SSR = \sum(\hat{Y_i}-\bar{Y})^2 = \hat{Y}^T\hat{Y}-n\bar{Y}^2=Y^THY-n\bar{Y}^2$$ $\mathbb{E}[SSR] = \mathbb{E}[Y^THY-n\bar{Y}^2] = \mathbb{E}[Y^THY] -n\mathbb{E}[\bar{Y}^2]$ $=\mathbb{E}[Y]^TH\mathbb{E}[Y]+tr[H*Var(Y)] - n\mathbb{E}[\bar{Y}^2]$ $=\boldsymbol{\beta}^TX^T(X(X^TX)^{-1}X^T)X\boldsymbol{\beta} + p\sigma^2- n\mathbb{E}[\bar{Y}^2]$ (using $tr(H) = p$ ) $=\boldsymbol{\beta}^TX^TX\boldsymbol{\beta}+p\sigma^2- n\mathbb{E}[\bar{Y}^2] = \mathbb{E}[Y]^T\mathbb{E}[Y]+p\sigma^2- n\mathbb{E}[\bar{Y}^2]$ $=\sum{\mathbb{E}[Y_i]^2} - \frac{1}{n}\mathbb{E}[(\sum{Y_i})^2]+p\sigma^2$ $=\sum{\mathbb{E}[Y_i]^2} - \frac{1}{n}\big(\mathbb{E}[(\sum{Y_i})^2]-\mathbb{E}[\sum{Y_i}]^2\big) - \frac{1}{n}\mathbb{E}[\sum{Y_i}]^2+p\sigma^2$ $=\sum{\mathbb{E}[Y_i]^2} - \frac{1}{n}Var(\sum{Y_i}) - \frac{1}{n}\big(\sum{\mathbb{E}[Y_i]}\big)^2+p\sigma^2$ Since the $Y_i$ are independent: $=\sum{\mathbb{E}[Y_i]^2} - \frac{1}{n}\big(\sum{\mathbb{E}[Y_i]}\big)^2+(p-1)\sigma^2$ Now, we can simply use the fact that $\sum{x_i^2} \ge n\bar{x}^2$ with equality iff $\sum{(x_i-\bar{x})^2}=0$ , by taking $x_i=\mathbb{E}[Y_i]$ (for a proof see How can I prove that the mean of squared data points is greater than the square of the mean of the data points? ): $$\mathbb{E}[SSR]\ge(p-1)\sigma^2$$ with equality iff $\mathbb{E}[Y_i]=const$ . Assuming uniqueness of the OLS estimates, if at least one element of $\boldsymbol\beta_1$ is non-zero this gives: $$\mathbb{E}[MSR] = \frac{1}{p-1}\mathbb{E}[SSR] > \sigma^2$$","The setting of my problem is as follows: Consider the general linear model , where and , are independent with mean zero and variance . Consider the case where at least one element of is not equal to zero. Prove that . What I did so far: where . Using the identity I was able to get: This is where I am stuck. It would suffice to show , for then , but I cannot seem to figure out how to do that. Your help is much appreciated! My initial attempt was incorrect: I moved past the expectation, which is obviously wrong. Now, I attempted the problem again and I think it may have worked: (using ) Since the are independent: Now, we can simply use the fact that with equality iff , by taking (for a proof see How can I prove that the mean of squared data points is greater than the square of the mean of the data points? ): with equality iff . Assuming uniqueness of the OLS estimates, if at least one element of is non-zero this gives:",Y = X\boldsymbol{\beta} + \epsilon \beta = \begin{bmatrix} \beta_0 \\ \boldsymbol{\beta_1} \end{bmatrix} X = \begin{bmatrix} \mathbf{1} & \mathbf{X_1} \end{bmatrix} \epsilon_i \sigma^2 \boldsymbol{\beta_1} \mathbb{E}[MSR]>\sigma^2 SSR = \sum(\hat{Y_i}-\bar{Y})^2 = \hat{Y}^T\hat{Y}-n\bar{Y}^2=Y^THY-n\bar{Y}^2 H = X(X^TX)^{-1}X^T \mathbb{E}[Y^THY]=\mathbb{E}[Y]^TH\mathbb{E}[Y]+tr[H*Var(Y)] \mathbb{E}[SSR] = \beta^TX^TX\beta+p\sigma^2-n\bar{Y}^2 \beta^TX^TX\beta-n\bar{Y}^2>0 MSR = \frac{SSR}{p-1} > \frac{p\sigma^2}{p-1} > \sigma^2 \mathbf{EDIT} \bar{Y}^2 SSR = \sum(\hat{Y_i}-\bar{Y})^2 = \hat{Y}^T\hat{Y}-n\bar{Y}^2=Y^THY-n\bar{Y}^2 \mathbb{E}[SSR] = \mathbb{E}[Y^THY-n\bar{Y}^2] = \mathbb{E}[Y^THY] -n\mathbb{E}[\bar{Y}^2] =\mathbb{E}[Y]^TH\mathbb{E}[Y]+tr[H*Var(Y)] - n\mathbb{E}[\bar{Y}^2] =\boldsymbol{\beta}^TX^T(X(X^TX)^{-1}X^T)X\boldsymbol{\beta} + p\sigma^2- n\mathbb{E}[\bar{Y}^2] tr(H) = p =\boldsymbol{\beta}^TX^TX\boldsymbol{\beta}+p\sigma^2- n\mathbb{E}[\bar{Y}^2] = \mathbb{E}[Y]^T\mathbb{E}[Y]+p\sigma^2- n\mathbb{E}[\bar{Y}^2] =\sum{\mathbb{E}[Y_i]^2} - \frac{1}{n}\mathbb{E}[(\sum{Y_i})^2]+p\sigma^2 =\sum{\mathbb{E}[Y_i]^2} - \frac{1}{n}\big(\mathbb{E}[(\sum{Y_i})^2]-\mathbb{E}[\sum{Y_i}]^2\big) - \frac{1}{n}\mathbb{E}[\sum{Y_i}]^2+p\sigma^2 =\sum{\mathbb{E}[Y_i]^2} - \frac{1}{n}Var(\sum{Y_i}) - \frac{1}{n}\big(\sum{\mathbb{E}[Y_i]}\big)^2+p\sigma^2 Y_i =\sum{\mathbb{E}[Y_i]^2} - \frac{1}{n}\big(\sum{\mathbb{E}[Y_i]}\big)^2+(p-1)\sigma^2 \sum{x_i^2} \ge n\bar{x}^2 \sum{(x_i-\bar{x})^2}=0 x_i=\mathbb{E}[Y_i] \mathbb{E}[SSR]\ge(p-1)\sigma^2 \mathbb{E}[Y_i]=const \boldsymbol\beta_1 \mathbb{E}[MSR] = \frac{1}{p-1}\mathbb{E}[SSR] > \sigma^2,"['statistics', 'expected-value', 'statistical-inference', 'regression', 'mathematical-modeling']"
43,How to compute probability of a bootstrap sample,How to compute probability of a bootstrap sample,,"The Question Consider the  samples $\{1, 3, 4, 6\}$ from some distribution. a) For one random bootstrap sample, find the probability that the mean is $1$ . b) For one random bootstrap sample, find the probability that the maximum is $6$ . c) For one random bootstrap sample, find the probability that exactly two elements in the sample are less than $2$ . My Understanding We just started to learn the bootstrap in class and I came across this question. I'm a little confused, as I feel like this question is too easy, as the mean of any sample with those numbers is always $3.5$ so a) is $0$ . The maximum will always be $6$ so b) is $1$ . And $2$ of the numbers cannot be less than $2$ . So c) is $0.$ Is there something major that I'm missing?","The Question Consider the  samples from some distribution. a) For one random bootstrap sample, find the probability that the mean is . b) For one random bootstrap sample, find the probability that the maximum is . c) For one random bootstrap sample, find the probability that exactly two elements in the sample are less than . My Understanding We just started to learn the bootstrap in class and I came across this question. I'm a little confused, as I feel like this question is too easy, as the mean of any sample with those numbers is always so a) is . The maximum will always be so b) is . And of the numbers cannot be less than . So c) is Is there something major that I'm missing?","\{1, 3, 4, 6\} 1 6 2 3.5 0 6 1 2 2 0.","['probability', 'statistics', 'bootstrap-sampling']"
44,"What is is it meant by ""analytically derive the expected value of an estimator?"" in statistics?","What is is it meant by ""analytically derive the expected value of an estimator?"" in statistics?",,"Good evening, everyone. I am new to statistics, so I wanted to see if I could get guidance with this problem: Consider a variable Y which follows a distribution with mean µ and variance σ2 in the population. Suppose we take a random sample of Y of size n, and propose to estimate the mean of Y from this dataset using two potential new estimators, the Addition Estimator and the First-ten Estimator, which are defined as follows: The Addition Estimator: Take the sample mean, and add on $\frac{20}{n}$ The First-ten Estimator: Select only the first ten observations in the sample, and take the sample mean of these observations For the sake of simplicity, we will assume that we always take samples containing more than ten obser- vations. Remember that the sample mean can be calculated in the following manner: $µ = ∑_n \frac{Yn}{n} $ Analytically derive the expected value of both estimators. Based on these results, is each estimator biased or unbiased? Using your knowledge of limits, intuitively explain what you think will happen to the bias (that is, the degree to which E[θ] deviates from θ) as n approaches infinity I am not understanding what it means to ""analitically derive de expected value"" if someone could explain me what it means and how to do it, I would be very thankful. Thank you again!","Good evening, everyone. I am new to statistics, so I wanted to see if I could get guidance with this problem: Consider a variable Y which follows a distribution with mean µ and variance σ2 in the population. Suppose we take a random sample of Y of size n, and propose to estimate the mean of Y from this dataset using two potential new estimators, the Addition Estimator and the First-ten Estimator, which are defined as follows: The Addition Estimator: Take the sample mean, and add on The First-ten Estimator: Select only the first ten observations in the sample, and take the sample mean of these observations For the sake of simplicity, we will assume that we always take samples containing more than ten obser- vations. Remember that the sample mean can be calculated in the following manner: Analytically derive the expected value of both estimators. Based on these results, is each estimator biased or unbiased? Using your knowledge of limits, intuitively explain what you think will happen to the bias (that is, the degree to which E[θ] deviates from θ) as n approaches infinity I am not understanding what it means to ""analitically derive de expected value"" if someone could explain me what it means and how to do it, I would be very thankful. Thank you again!",\frac{20}{n} µ = ∑_n \frac{Yn}{n} ,['statistics']
45,"Show that the statistic $T(x_1,...,x_n)=\sum_{i=1}^nx_i^2$ is complete.",Show that the statistic  is complete.,"T(x_1,...,x_n)=\sum_{i=1}^nx_i^2","Let $X_1,...,X_n$ be iid random sample form $N(\theta,c\theta)$ , where $c$ is a known constant. Show that the statistic $T(x_1,...,x_n)=\sum_{i=1}^nx_i^2$ is complete. In other words I have to show that if $$E[g(T)]=0\quad \Longrightarrow \quad g(T)=0 \quad \forall t$$ I'm kind of stuck with the distribution of $T$ because $x_i^2$ isn´t a gaussian distribution. Any suggestions on how to approach this problem would be great!","Let be iid random sample form , where is a known constant. Show that the statistic is complete. In other words I have to show that if I'm kind of stuck with the distribution of because isn´t a gaussian distribution. Any suggestions on how to approach this problem would be great!","X_1,...,X_n N(\theta,c\theta) c T(x_1,...,x_n)=\sum_{i=1}^nx_i^2 E[g(T)]=0\quad \Longrightarrow \quad g(T)=0 \quad \forall t T x_i^2","['probability', 'statistics', 'statistical-inference']"
46,Probability of CDC data,Probability of CDC data,,"All of this is my work, for (c) I believe its $1 - pbinom(144, 138, 2.9)$ which yields $.0192 $ For (d) I know that when D is negative, that would mean the woman is taller than the man, so I think it is just $ 69.2/63.8$ ? Please correct me if I am wrong, Thank you in advance!","All of this is my work, for (c) I believe its which yields For (d) I know that when D is negative, that would mean the woman is taller than the man, so I think it is just ? Please correct me if I am wrong, Thank you in advance!","1 - pbinom(144, 138, 2.9) .0192   69.2/63.8","['probability', 'statistics']"
47,How do I find the probability of getting an average when selecting from a normal distribution?,How do I find the probability of getting an average when selecting from a normal distribution?,,"If I have that scores follow a normal distribution, I know that the probability of selecting any person at random and them getting a score between 1 and 2 sd's above the mean is 13.6%. If I randomly select 4 people what is the probability that their scores will average to a value between 1 and 2 sd's above the mean? I dont care where any of their individual scores are only that they average to a value between 1 and 2 sds above the mean.","If I have that scores follow a normal distribution, I know that the probability of selecting any person at random and them getting a score between 1 and 2 sd's above the mean is 13.6%. If I randomly select 4 people what is the probability that their scores will average to a value between 1 and 2 sd's above the mean? I dont care where any of their individual scores are only that they average to a value between 1 and 2 sds above the mean.",,"['probability', 'statistics', 'normal-distribution']"
48,Simplify this statistical average,Simplify this statistical average,,"I have a quantity $\tau$ given by: $$ \frac{1}{\tau} = \frac{1}{\tau_1}+\frac{1}{\tau_2}+\frac{1}{\tau_3} $$ where $\tau_1$ , $\tau_2$ and $\tau_3$ are some constituent quantities. Now these $\tau$ 's are function of a variable $x$ , but the ensemble averages of $\tau_i$ are known, given by: $$ \frac{\langle \tau_i^2\rangle}{\langle \tau_i\rangle^2} = \alpha_i $$ Is there a way to express or simplify the ensemble average of total $\tau$ in terms of $\alpha_i$ 's: $$ \frac{\langle \tau^2\rangle}{\langle \tau\rangle^2}  $$ If not, what would be the necessary information needed w.r.t. the actual distribution of $\tau_i$ with $x$ .","I have a quantity given by: where , and are some constituent quantities. Now these 's are function of a variable , but the ensemble averages of are known, given by: Is there a way to express or simplify the ensemble average of total in terms of 's: If not, what would be the necessary information needed w.r.t. the actual distribution of with .","\tau 
\frac{1}{\tau} = \frac{1}{\tau_1}+\frac{1}{\tau_2}+\frac{1}{\tau_3}
 \tau_1 \tau_2 \tau_3 \tau x \tau_i 
\frac{\langle \tau_i^2\rangle}{\langle \tau_i\rangle^2} = \alpha_i
 \tau \alpha_i 
\frac{\langle \tau^2\rangle}{\langle \tau\rangle^2} 
 \tau_i x","['statistics', 'random-variables', 'average']"
49,Variance of a single random variable with two terms.,Variance of a single random variable with two terms.,,"A random variable X, representing the time until some event occurs, has the following pdf. $$(14/99)e^{-0.5x}+(85/99)e^{-0.25x}$$ . Using integration, I get the expectation as 14.303 via $\int_0^\infty xf(x)dx$ , and a variance of 463.453 via $\int_0^\infty (x-\overline{x})^2f(x)dx$ . I don't understand why the variance is so high. I also computed the median, which was much higher than the mean. When I try and compute the variance using $Var(X)=E(X^2)-(E(X))^2$ I get a negative value. 2) Why is there a mismatch between using these 2 methods?","A random variable X, representing the time until some event occurs, has the following pdf. . Using integration, I get the expectation as 14.303 via , and a variance of 463.453 via . I don't understand why the variance is so high. I also computed the median, which was much higher than the mean. When I try and compute the variance using I get a negative value. 2) Why is there a mismatch between using these 2 methods?",(14/99)e^{-0.5x}+(85/99)e^{-0.25x} \int_0^\infty xf(x)dx \int_0^\infty (x-\overline{x})^2f(x)dx Var(X)=E(X^2)-(E(X))^2,['statistics']
50,MLE of difference is difference of MLEs,MLE of difference is difference of MLEs,,"Suppose a distribution has two parameters $\alpha$ and $\beta$ , and let the maximum likelihood estimators for these parameters be $\hat{\alpha}$ and $\hat \beta$ . Is the maximum likelihood estimator for $\alpha-\beta$ necessarily equal to $\hat{\alpha}-\hat{\beta}?$ We cannot assume the independence of $\hat \alpha$ and $\hat \beta$ since they are both functions of the same data set.","Suppose a distribution has two parameters and , and let the maximum likelihood estimators for these parameters be and . Is the maximum likelihood estimator for necessarily equal to We cannot assume the independence of and since they are both functions of the same data set.",\alpha \beta \hat{\alpha} \hat \beta \alpha-\beta \hat{\alpha}-\hat{\beta}? \hat \alpha \hat \beta,"['statistics', 'maximum-likelihood']"
51,Understanding Corollary 2.3.2 from Bickel's Mathematical Statistics.,Understanding Corollary 2.3.2 from Bickel's Mathematical Statistics.,,"I am trying to understand the Corollary 2.3.2 from Bickel's book mathematical statistics . The corollary says Consider the exponential family $$p(x,\theta)=h(x)\exp\left\{\sum_{j=1}^kc_j(\theta)T_j(x)-B(\theta)\right\},x\in\mathcal{X},\theta\in\Theta.$$ Let $C^0$ denote the interior of the range of $(c_1(\theta),c_2(\theta),\dots,c_k(\theta))^T$ and let $x$ be the observed data. If the equations $$E_\theta T_j(X)=T_j(x),j=1,\dots,k$$ have a solution $C(\hat{\theta})\in C^0,$ then it is the unique MLE of $\theta$ . I think the conclusion of this corollary is useful, as it is not easy to show the existence and uniqueness of a curved exponential family in general. However, the book does not given any example of using this corollary. I am trying to understand this corollary. Can someone give an example showing the usefulness of it?","I am trying to understand the Corollary 2.3.2 from Bickel's book mathematical statistics . The corollary says Consider the exponential family Let denote the interior of the range of and let be the observed data. If the equations have a solution then it is the unique MLE of . I think the conclusion of this corollary is useful, as it is not easy to show the existence and uniqueness of a curved exponential family in general. However, the book does not given any example of using this corollary. I am trying to understand this corollary. Can someone give an example showing the usefulness of it?","p(x,\theta)=h(x)\exp\left\{\sum_{j=1}^kc_j(\theta)T_j(x)-B(\theta)\right\},x\in\mathcal{X},\theta\in\Theta. C^0 (c_1(\theta),c_2(\theta),\dots,c_k(\theta))^T x E_\theta T_j(X)=T_j(x),j=1,\dots,k C(\hat{\theta})\in C^0, \theta","['probability-theory', 'statistics', 'maximum-likelihood']"
52,Can I use a z test to compare two samples if there is a significant difference in the size? [closed],Can I use a z test to compare two samples if there is a significant difference in the size? [closed],,"Closed . This question needs details or clarity . It is not currently accepting answers. Want to improve this question? Add details and clarify the problem by editing this post . Closed 3 years ago . Improve this question I have to compare two unequal size samples from a population of 53k. One has a proportion of 56% and size of 50k and the other has a proportion of 50% and size of 3k. Can I do a two proportion z test to compare to make a case that the proportion of sample 1 is greater than sample 2? Does the difference in the sample size pose an issue? Some more info: Here is the situation: Lets say I had 53k people and 50k received treatment A and 3k received treatment B. From the 3k, 50% showed positive results and from the 50k, 56% showed positive effect. How can I conclude that my treatment A was more effective than treatment B? I am looking for a way to make that case.","Closed . This question needs details or clarity . It is not currently accepting answers. Want to improve this question? Add details and clarify the problem by editing this post . Closed 3 years ago . Improve this question I have to compare two unequal size samples from a population of 53k. One has a proportion of 56% and size of 50k and the other has a proportion of 50% and size of 3k. Can I do a two proportion z test to compare to make a case that the proportion of sample 1 is greater than sample 2? Does the difference in the sample size pose an issue? Some more info: Here is the situation: Lets say I had 53k people and 50k received treatment A and 3k received treatment B. From the 3k, 50% showed positive results and from the 50k, 56% showed positive effect. How can I conclude that my treatment A was more effective than treatment B? I am looking for a way to make that case.",,"['probability', 'statistics']"
53,Non-uniqueness of MLE of multivariate Laplace distribution?,Non-uniqueness of MLE of multivariate Laplace distribution?,,"Suppose $\{X\}_{i=1}^n\overset{i.i.d}{\sim}X$ , and $X\in\mathbb{R}^d$ has density, $$f_{\theta}(x)=c\exp\left\{-||x-\theta||\right\},\theta\in\mathbb{R}^d,$$ where $||\cdot||$ denotes the Euclidean norm. Show that the MLE $\hat{\theta}$ exists but is not unique when $n$ is even. I know how to prove that the MLE $\hat{\theta}$ exists, by noticing $\underset{\theta\rightarrow\partial\mathbb{R}^d}{\lim} \log f(\theta)=-\infty$ , however I don't know how to prove it is not unique. I understand that the log-likelihood function, $l(\theta)=-\sum_{i=1}^n||x_i-\theta||$ is a convex, but not strictly convex, function, but this does not guarantee the nonuniqueness. I also tried taking the first and second gradient of $l(\theta)$ . But even I have a non-negative definite $l(\theta)$ , I still don't have the non-uniqueness... ---Update--- The original question is here. This is from Mathematical Statistics (Bickel).","Suppose , and has density, where denotes the Euclidean norm. Show that the MLE exists but is not unique when is even. I know how to prove that the MLE exists, by noticing , however I don't know how to prove it is not unique. I understand that the log-likelihood function, is a convex, but not strictly convex, function, but this does not guarantee the nonuniqueness. I also tried taking the first and second gradient of . But even I have a non-negative definite , I still don't have the non-uniqueness... ---Update--- The original question is here. This is from Mathematical Statistics (Bickel).","\{X\}_{i=1}^n\overset{i.i.d}{\sim}X X\in\mathbb{R}^d f_{\theta}(x)=c\exp\left\{-||x-\theta||\right\},\theta\in\mathbb{R}^d, ||\cdot|| \hat{\theta} n \hat{\theta} \underset{\theta\rightarrow\partial\mathbb{R}^d}{\lim} \log f(\theta)=-\infty l(\theta)=-\sum_{i=1}^n||x_i-\theta|| l(\theta) l(\theta)","['statistics', 'multivariable-calculus', 'optimization', 'maximum-likelihood']"
54,Curve of number of attempts to gain a result with probability p,Curve of number of attempts to gain a result with probability p,,"Recently, I've been reinvigorated to get a better intuition for probability and statistics by its several simultaneously vexing and intriguing paradoxes. One of which I've discovered through my interest in programming. I have a program that will generate a random number between 0 and 1 over and over again until it is greater than some number N, counting the number of times it does this. It then keeps a list of each possible number of tries it could have taken and counts them up. In essence, it's repeating random chance 1-N until it gets the result and telling me how many times the program was able to get it in X attempts. Now, originally I was expecting a bell curve around the number 1/p where p would be 1-N, or just the probability. However, I was completely surprised to find out that the most common result was only 1, and it trails off in probability from there! The average number of attempts is still 100, I suppose, but it's simply baffling to me that the graph comes out this way. For an example of why it seems paradoxical to me, take the case of N = 0.99, so P = 0.01 or 1 in 100. It will be more likely that you have a result before fifty attempts than it will be that you have a result between 75 to 125 attempts. I don't know if anyone here is able to shed light on the topic, as it may just come down to poor intuition, but I felt like it was striking enough to post a question regarding it. Below I will post a graph of my results from 10,000,000 trials where X is the number of attempts before getting the result, and Y is how many times it took exactly that number of attempts.","Recently, I've been reinvigorated to get a better intuition for probability and statistics by its several simultaneously vexing and intriguing paradoxes. One of which I've discovered through my interest in programming. I have a program that will generate a random number between 0 and 1 over and over again until it is greater than some number N, counting the number of times it does this. It then keeps a list of each possible number of tries it could have taken and counts them up. In essence, it's repeating random chance 1-N until it gets the result and telling me how many times the program was able to get it in X attempts. Now, originally I was expecting a bell curve around the number 1/p where p would be 1-N, or just the probability. However, I was completely surprised to find out that the most common result was only 1, and it trails off in probability from there! The average number of attempts is still 100, I suppose, but it's simply baffling to me that the graph comes out this way. For an example of why it seems paradoxical to me, take the case of N = 0.99, so P = 0.01 or 1 in 100. It will be more likely that you have a result before fifty attempts than it will be that you have a result between 75 to 125 attempts. I don't know if anyone here is able to shed light on the topic, as it may just come down to poor intuition, but I felt like it was striking enough to post a question regarding it. Below I will post a graph of my results from 10,000,000 trials where X is the number of attempts before getting the result, and Y is how many times it took exactly that number of attempts.",,"['probability', 'statistics', 'intuition', 'curves']"
55,How do I prove that a statistic is a pivot?,How do I prove that a statistic is a pivot?,,"In this example I have a sample out of a distribution with density $P_{\theta}(x) = 2x^{\theta} e^{−\theta x^2} I_{x \ge 0}$ . I know that for all $i \ge 1$ we have $X_i^2 \sim Exp(\theta)$ . If $\theta = 1$ , then $T_1 = 2\sum_{1 \le i \le n}X_i^2 \sim X_{2n}^2$ . And now I have to show that $T_{\theta} = 2\sum_{1 \le i \le n} \theta X_i^2$ is a pivot. A pivot is a function of which the probability function does not depend on $\theta$ . But how do I show that $T_{\theta}$ does not depend on $\theta$ ? Btw: I tried to plug it first in LaTeX, so that it's easier to read. But that doesn't work.","In this example I have a sample out of a distribution with density . I know that for all we have . If , then . And now I have to show that is a pivot. A pivot is a function of which the probability function does not depend on . But how do I show that does not depend on ? Btw: I tried to plug it first in LaTeX, so that it's easier to read. But that doesn't work.",P_{\theta}(x) = 2x^{\theta} e^{−\theta x^2} I_{x \ge 0} i \ge 1 X_i^2 \sim Exp(\theta) \theta = 1 T_1 = 2\sum_{1 \le i \le n}X_i^2 \sim X_{2n}^2 T_{\theta} = 2\sum_{1 \le i \le n} \theta X_i^2 \theta T_{\theta} \theta,['statistics']
56,Generating function of total sales on a given day,Generating function of total sales on a given day,,"Given the problem below: The number of customers that shop at Elmo’s World of Food on a given day is geometrically distributed with parameter $p_1$ . The number of items a customer buys at Elmo’s is geometrically distributed with parameter $p_2$ . The distribution of the cost of each item is exponentially distributed with parameter $\lambda$ . Assume that the amount customers buy and the amount of prices of items are independent. Let $X$ denote the total sales on a given day. Find the generating function of $X$ . In my attempt on solving this I let $N_c$ denote the number of customers that shop on a given fay, $N_i$ denote the number of items a customer buys, and $C$ cost of each item. Then we can write $X = \sum_{k=1}^{N_c} N_i\cdot C$ . To find the generating function I used the MGF: $E[e^{tX}]$ and to compute it I conditioned on $N_c$ first and then on $C$ . However, my work became very tedious and I'm stuck at simplifying the expression I got. Therefore, I was wondering if there is a different approach I could use in finding the generating function of $X$ ? This is just a practice problem as I study for my exam.","Given the problem below: The number of customers that shop at Elmo’s World of Food on a given day is geometrically distributed with parameter . The number of items a customer buys at Elmo’s is geometrically distributed with parameter . The distribution of the cost of each item is exponentially distributed with parameter . Assume that the amount customers buy and the amount of prices of items are independent. Let denote the total sales on a given day. Find the generating function of . In my attempt on solving this I let denote the number of customers that shop on a given fay, denote the number of items a customer buys, and cost of each item. Then we can write . To find the generating function I used the MGF: and to compute it I conditioned on first and then on . However, my work became very tedious and I'm stuck at simplifying the expression I got. Therefore, I was wondering if there is a different approach I could use in finding the generating function of ? This is just a practice problem as I study for my exam.",p_1 p_2 \lambda X X N_c N_i C X = \sum_{k=1}^{N_c} N_i\cdot C E[e^{tX}] N_c C X,"['probability', 'probability-theory', 'statistics', 'probability-distributions', 'conditional-probability']"
57,Prove lower bound of binomial distribution near mean,Prove lower bound of binomial distribution near mean,,"Let $0<p<1 / 2$ be fixed independently of $n,$ and let $X_{1}, \ldots, X_{n}$ be iid copies of a Bernoulli random variable that equals 1 with probability $p,$ thus $\mu_{i}=p$ and $\sigma_{i}^{2}=p(1-p),$ and so $\mu=n p$ and $\sigma^{2}=n p(1-p) .$ Using Stirling's formula  show that $$ \mathbf{P}\left(\left|S_{n}-\mu\right| \geq \lambda \sigma\right) \geq c \exp \left(-C \lambda^{2}\right) $$ for some absolute constants $C, c>0$ and all $\lambda \leq c \sigma .$ What happens when $\lambda$ is much larger than $\sigma ?$ Attempt: By substituting in Stirling formula, I have the following result, which in itself is a lower bound for binomial tail: $\begin{aligned} P(\operatorname{Bin}(n, p)=&k)=\left(\begin{array}{l}n \\ k\end{array}\right) p^{k}(1-p)^{k} \\ &=\sqrt{\frac{n}{2 \pi k(n-k)}} \exp \left(o(1)+k \log \left(\frac{p}{1-p} \cdot \frac{n-k}{k}\right)+n \log \left(\frac{(1-p) n}{n-k}\right)\right) \\ &=\sqrt{\frac{n}{2 \pi k(n-k)}} \exp \left(o(1)-n\left(D_{K L}(k / n \| p)\right)\right) \end{aligned}$ Let $k=n p+\lambda \sqrt{n(1-p) p}$ where $\lambda=o\left(n^{\frac{1}{6}}\right) .$ Then, we can continue our approximation $$ \begin{aligned} =\sqrt{\frac{n}{2 \pi k(n-k)}} & \exp \left(o(1)+k \log \left(\frac{n-\lambda \sqrt{\frac{n p}{1-p}}}{n+\lambda \sqrt{\frac{n(1-p)}{p}}}\right)+n \log \left(\frac{n}{n-\lambda \sqrt{\frac{n p}{1-p}}}\right)\right) \\ &=\sqrt{\frac{n}{2 \pi k(n-k)}} \exp (o(1)\\ &+(n p+\lambda \sqrt{n(1-p) p})\left(-\frac{\lambda}{\sqrt{n}}\left(\sqrt{\frac{p}{1-p}}+\sqrt{\frac{1-p}{p}}\right)+\frac{1}{2} \frac{\lambda^{2}}{n}\left(\frac{1}{p}-\frac{1}{1-p}\right)\right.\\ &\left.\left.+O\left(\frac{\lambda^{3}}{n^{3 / 2}}\right)\right)+n\left(-\lambda \sqrt{\frac{p}{1-p}} \frac{1}{\sqrt{n}}-\frac{\lambda^{2}}{2} \frac{p}{1-p} \frac{1}{n}+o\left(\frac{\lambda^{3}}{n^{3 / 2}}\right)\right)\right) \\ &=\sqrt{\frac{n}{2 \pi k(n-k)}} \exp \left(o(1)-\frac{1}{2} \lambda^{2}\right) \end{aligned} $$ using Taylor's expansion. Now I'm stuck. I'm trying to figure out how to get rid of $\sqrt{\frac{n}{2 \pi k(n-k)}}\sim O(1/\sqrt{n})$ in the front by summing up other terms from $k$ to $n$ but I don't know how to do that. Plus, I assumed $\lambda=o(n^{1/6})$ which is not part of the question. I saw most posts on this topic on this site. Don't think there is an answer to this particular form.","Let be fixed independently of and let be iid copies of a Bernoulli random variable that equals 1 with probability thus and and so and Using Stirling's formula  show that for some absolute constants and all What happens when is much larger than Attempt: By substituting in Stirling formula, I have the following result, which in itself is a lower bound for binomial tail: Let where Then, we can continue our approximation using Taylor's expansion. Now I'm stuck. I'm trying to figure out how to get rid of in the front by summing up other terms from to but I don't know how to do that. Plus, I assumed which is not part of the question. I saw most posts on this topic on this site. Don't think there is an answer to this particular form.","0<p<1 / 2 n, X_{1}, \ldots, X_{n} p, \mu_{i}=p \sigma_{i}^{2}=p(1-p), \mu=n p \sigma^{2}=n p(1-p) . 
\mathbf{P}\left(\left|S_{n}-\mu\right| \geq \lambda \sigma\right) \geq c \exp \left(-C \lambda^{2}\right)
 C, c>0 \lambda \leq c \sigma . \lambda \sigma ? \begin{aligned} P(\operatorname{Bin}(n, p)=&k)=\left(\begin{array}{l}n \\ k\end{array}\right) p^{k}(1-p)^{k} \\ &=\sqrt{\frac{n}{2 \pi k(n-k)}} \exp \left(o(1)+k \log \left(\frac{p}{1-p} \cdot \frac{n-k}{k}\right)+n \log \left(\frac{(1-p) n}{n-k}\right)\right) \\ &=\sqrt{\frac{n}{2 \pi k(n-k)}} \exp \left(o(1)-n\left(D_{K L}(k / n \| p)\right)\right) \end{aligned} k=n p+\lambda \sqrt{n(1-p) p} \lambda=o\left(n^{\frac{1}{6}}\right) . 
\begin{aligned}
=\sqrt{\frac{n}{2 \pi k(n-k)}} & \exp \left(o(1)+k \log \left(\frac{n-\lambda \sqrt{\frac{n p}{1-p}}}{n+\lambda \sqrt{\frac{n(1-p)}{p}}}\right)+n \log \left(\frac{n}{n-\lambda \sqrt{\frac{n p}{1-p}}}\right)\right) \\
&=\sqrt{\frac{n}{2 \pi k(n-k)}} \exp (o(1)\\
&+(n p+\lambda \sqrt{n(1-p) p})\left(-\frac{\lambda}{\sqrt{n}}\left(\sqrt{\frac{p}{1-p}}+\sqrt{\frac{1-p}{p}}\right)+\frac{1}{2} \frac{\lambda^{2}}{n}\left(\frac{1}{p}-\frac{1}{1-p}\right)\right.\\
&\left.\left.+O\left(\frac{\lambda^{3}}{n^{3 / 2}}\right)\right)+n\left(-\lambda \sqrt{\frac{p}{1-p}} \frac{1}{\sqrt{n}}-\frac{\lambda^{2}}{2} \frac{p}{1-p} \frac{1}{n}+o\left(\frac{\lambda^{3}}{n^{3 / 2}}\right)\right)\right) \\
&=\sqrt{\frac{n}{2 \pi k(n-k)}} \exp \left(o(1)-\frac{1}{2} \lambda^{2}\right)
\end{aligned}
 \sqrt{\frac{n}{2 \pi k(n-k)}}\sim O(1/\sqrt{n}) k n \lambda=o(n^{1/6})","['probability', 'combinatorics', 'statistics', 'binomial-distribution']"
58,Simulating Brownian motion in R. Is this correct?,Simulating Brownian motion in R. Is this correct?,,"I have a Weiner process $\{W(t)\}_{t\ge0}$ with $\sigma^2=\text{Var}(W(1))=1$ . For a real constant $\epsilon>0$ consider the differential ratio process $\Delta_\epsilon=\{\Delta_\epsilon(t)\}_{r>0}$ given by \begin{equation} \Delta_\epsilon(t) = \frac{W(t+\epsilon)-W(t)}{\epsilon}, \quad \text{for} \ t>0. \end{equation} I want to simulate a sample path of $\{\Delta_\epsilon(t)\}_{t\in(0,1]}$ for small $\epsilon>0$ in Rstudio. Simulating a basic Weinerprocess/Brownian motion is easy in R, one can do it by the function rweiner() or by plotting the cumulative sum of standard normally distributed variables. The part that confuses me is how to simulate the $W(t+\epsilon)$ . What I've done is used rwiener() and simply passed the end parameter to $1+\epsilon$ . epsilon = 10e-1 weinerEps = rwiener(end = 1+epsilon, frequency = 1000) weiner = rwiener(end = 1, frequency = 1000)  Delta = (wienerEps - wiener)/epsilon  plot(Delta , type=""l"") I'm not sure this is correct though.","I have a Weiner process with . For a real constant consider the differential ratio process given by I want to simulate a sample path of for small in Rstudio. Simulating a basic Weinerprocess/Brownian motion is easy in R, one can do it by the function rweiner() or by plotting the cumulative sum of standard normally distributed variables. The part that confuses me is how to simulate the . What I've done is used rwiener() and simply passed the end parameter to . epsilon = 10e-1 weinerEps = rwiener(end = 1+epsilon, frequency = 1000) weiner = rwiener(end = 1, frequency = 1000)  Delta = (wienerEps - wiener)/epsilon  plot(Delta , type=""l"") I'm not sure this is correct though.","\{W(t)\}_{t\ge0} \sigma^2=\text{Var}(W(1))=1 \epsilon>0 \Delta_\epsilon=\{\Delta_\epsilon(t)\}_{r>0} \begin{equation}
\Delta_\epsilon(t) = \frac{W(t+\epsilon)-W(t)}{\epsilon}, \quad \text{for} \ t>0.
\end{equation} \{\Delta_\epsilon(t)\}_{t\in(0,1]} \epsilon>0 W(t+\epsilon) 1+\epsilon","['statistics', 'stochastic-processes', 'brownian-motion']"
59,Determine the maximum value of a so that $P(S_{50}≥a)≥0.999$,Determine the maximum value of a so that,P(S_{50}≥a)≥0.999,"A toymaker makes $Xn$ specialized toy parts on day n, where $Xn$ are independent and identically distributed random variables with mean 6 and variance 9. Let $Sn$ be the total number of specialized parts produced from day 1 to day n. Using central limit theorem, determine the total number of parts, a, the said factory can guarantee to produce by day 50 with at least 99.9% certainty, i.e. determine the maximum value of a so that $P(S_{50}≥a)≥0.999$ . The maximum value must be a whole number. So I used the formula $Zn = \frac{\sigma - \mu}{\sigma \sqrt n}  $ and from z-table, $Zn = -3.10$ where, $\sigma = \frac{Zn \sigma}{\sqrt n} + n \mu = \frac{(-3.10)(3)}{\sqrt 50} + (50)(6) = 298.69 \approx 299$ However my answer was incorrect. Any help would be appreciated.","A toymaker makes specialized toy parts on day n, where are independent and identically distributed random variables with mean 6 and variance 9. Let be the total number of specialized parts produced from day 1 to day n. Using central limit theorem, determine the total number of parts, a, the said factory can guarantee to produce by day 50 with at least 99.9% certainty, i.e. determine the maximum value of a so that . The maximum value must be a whole number. So I used the formula and from z-table, where, However my answer was incorrect. Any help would be appreciated.",Xn Xn Sn P(S_{50}≥a)≥0.999 Zn = \frac{\sigma - \mu}{\sigma \sqrt n}   Zn = -3.10 \sigma = \frac{Zn \sigma}{\sqrt n} + n \mu = \frac{(-3.10)(3)}{\sqrt 50} + (50)(6) = 298.69 \approx 299,"['probability', 'statistics', 'variance']"
60,Formula to work out likely number of unique items,Formula to work out likely number of unique items,,"There is an unknown quantity of unique prizes. Each time you play the game, you get a prize. The likelihood of getting any type of prize X is equal to getting the likelihood of any other type, and winning a prize does not decrease the likelihood that that type will be won in future. I won 14 prizes. Where '1' represents a type of prize I have never won before, and '0' represents winning a type of prize I have won before, I got the following ordered list: (1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0) I know that each time I play the game, the probability of getting a prize that I have not won before is (N-a)/N where a is the number of unique prizes I have already won. What formula can I use to determine what the most likely number of prizes is?","There is an unknown quantity of unique prizes. Each time you play the game, you get a prize. The likelihood of getting any type of prize X is equal to getting the likelihood of any other type, and winning a prize does not decrease the likelihood that that type will be won in future. I won 14 prizes. Where '1' represents a type of prize I have never won before, and '0' represents winning a type of prize I have won before, I got the following ordered list: (1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0) I know that each time I play the game, the probability of getting a prize that I have not won before is (N-a)/N where a is the number of unique prizes I have already won. What formula can I use to determine what the most likely number of prizes is?",,"['statistics', 'probability-distributions', 'statistical-inference']"
61,Minimizing RSS for model with missing observations. Dummy variable vs Dropping observations,Minimizing RSS for model with missing observations. Dummy variable vs Dropping observations,,"Suppose that the relationship: $Y_i = β_1 + β_2*X_i + u_i$ is being fitted and that the value of X is missing for some observations. One way of handling the missing values problem is to drop those observations. Another is to set X = 0 for the missing observations and include a dummy variable D defined to be equal to 1 if X is missing, 0 otherwise. Demonstrate that the two methods must yield the same estimates of β_1 and β_2. Write down an expression for RSS using the second approach, decompose it into the RSS for observations with X present and RSS for observations with X missing, and determine how the resulting expression is related to RSS when the missing value observations are dropped. My attempt: Suppose we have n total observations and the first k of them are not missing, and all the rest are missing. So we have model 1 where missing x_i will be 0 and model 2 where d_i is 1 if x_i is missing, and 0 otherwise: We can see that in RSS_1 and RSS_2 second terms are the same. And the textbook, from which I took this task says that β_1 and β_2 which are minimizing RSS for two models will be the same. Namely, this will be β_1 and β_2 which could be obtained by minimizing the first term of RSS_2, but I have no idea why. Is this because the first term of RSS_1 does not depend on x_i?","Suppose that the relationship: is being fitted and that the value of X is missing for some observations. One way of handling the missing values problem is to drop those observations. Another is to set X = 0 for the missing observations and include a dummy variable D defined to be equal to 1 if X is missing, 0 otherwise. Demonstrate that the two methods must yield the same estimates of β_1 and β_2. Write down an expression for RSS using the second approach, decompose it into the RSS for observations with X present and RSS for observations with X missing, and determine how the resulting expression is related to RSS when the missing value observations are dropped. My attempt: Suppose we have n total observations and the first k of them are not missing, and all the rest are missing. So we have model 1 where missing x_i will be 0 and model 2 where d_i is 1 if x_i is missing, and 0 otherwise: We can see that in RSS_1 and RSS_2 second terms are the same. And the textbook, from which I took this task says that β_1 and β_2 which are minimizing RSS for two models will be the same. Namely, this will be β_1 and β_2 which could be obtained by minimizing the first term of RSS_2, but I have no idea why. Is this because the first term of RSS_1 does not depend on x_i?",Y_i = β_1 + β_2*X_i + u_i,"['statistics', 'least-squares']"
62,"Calculating $P\{\max(-x_{(1)},x_{(n)})\leq x \}$ where $X_{i}$ are i.i.d. from uniform distribution",Calculating  where  are i.i.d. from uniform distribution,"P\{\max(-x_{(1)},x_{(n)})\leq x \} X_{i}","Let $X_{i}$ be i.i.d. from uniform distribution $[-\theta,2\theta]$ . I am interested in if the following is true: $P\{\max(-x_{(1)},x_{(n)})\leq x \}= P\{-x_{(1)}\leq x , x_{(n)}\leq x \}\\ = P\{-x\leq x_{(1)} , x_{(n)}\leq x \}\\= P\{-x\leq x_{1} \leq x, ... ,-x\leq x_{(n)}\leq x \}\\= P\{-x\leq x_{1} \leq x \} ... P\{-x\leq x_{n} \leq x \}\\=(\int_{-x}^{x}\frac{1}{3\theta})^{n} $",Let be i.i.d. from uniform distribution . I am interested in if the following is true:,"X_{i} [-\theta,2\theta] P\{\max(-x_{(1)},x_{(n)})\leq x \}= P\{-x_{(1)}\leq x , x_{(n)}\leq x \}\\ = P\{-x\leq x_{(1)} , x_{(n)}\leq x \}\\= P\{-x\leq x_{1} \leq x, ... ,-x\leq x_{(n)}\leq x \}\\= P\{-x\leq x_{1} \leq x \} ... P\{-x\leq x_{n} \leq x \}\\=(\int_{-x}^{x}\frac{1}{3\theta})^{n} ","['probability', 'statistics', 'order-statistics']"
63,Convergence in Distribution ($Y \mid N = n)$ is $\chi^2_{2n}$ and $N$ is Poisson($\theta$)),Convergence in Distribution ( is  and  is Poisson()),Y \mid N = n) \chi^2_{2n} N \theta,"Suppose I have a hierarchical model like the following: $Y\mid N=n$ is governed by a chi-square distribution with $2n$ degrees of freedom, and $\theta$ is governed by a Poisson distribution. I want to show that as $\theta \rightarrow \infty$ , $\frac{Y - E[Y]}{\sqrt{\operatorname{Var}(Y)}} \rightarrow$ standard normal in distribution. I know that to show convergence in distribution, I have to show that the pdf (or cdf) $$\lim_{n \rightarrow \infty} f_{X_n}(x) = f_X(x),$$ but I am unsure how to go about this. How could I show that $\frac{Y - E[Y]}{\sqrt{\operatorname{Var}(Y)}} \rightarrow$ standard normal in distribution?","Suppose I have a hierarchical model like the following: is governed by a chi-square distribution with degrees of freedom, and is governed by a Poisson distribution. I want to show that as , standard normal in distribution. I know that to show convergence in distribution, I have to show that the pdf (or cdf) but I am unsure how to go about this. How could I show that standard normal in distribution?","Y\mid N=n 2n \theta \theta \rightarrow \infty \frac{Y - E[Y]}{\sqrt{\operatorname{Var}(Y)}} \rightarrow \lim_{n \rightarrow \infty} f_{X_n}(x) = f_X(x), \frac{Y - E[Y]}{\sqrt{\operatorname{Var}(Y)}} \rightarrow","['probability', 'probability-theory', 'statistics', 'probability-distributions', 'convergence-divergence']"
64,"$X_1,...,X_n \text{ iid }X_1\sim \operatorname{Ge}\big( \theta\big)\implies \mathbf X =(X_1,...,X_n)^T \in \text{ Exponential Family}$",,"X_1,...,X_n \text{ iid }X_1\sim \operatorname{Ge}\big( \theta\big)\implies \mathbf X =(X_1,...,X_n)^T \in \text{ Exponential Family}","$$\bullet \text{ Let} \quad X_1,X_2,...,X_n \text{ iid }X_1\sim \operatorname{Ge}\big( \theta\big)$$ $$\text{Goal: } \mathbf X =(X_1,...,X_n)^T \in \text{ Exponential Family of distributions.}$$ $$\text{ Its enough to show that :}$$ $$\exists \phi \in \Phi \subseteq \mathbb R^k,k\in\mathbb N, \text{parametric space s.t pmf:}$$ $$f(\mathbf X;\phi)=h(x)\cdot \exp{\bigg\{ \big< s(\mathbf X),\phi \big>_{\mathbb R^k} -K(\phi)\bigg\}},\mathbf X\in \mathcal X ,\phi\in\Phi . $$ $$\text{Note : $\big< \cdot \big>$ Inner product on  $\mathbb R^k$}.$$ $$\text{pmf of Geometric distr: }\Omega =\mathbb N,\operatorname{P}\big( X=x\big)=f_X(x)=(1-\theta )^{x-1}\theta .$$ $$\text{cdf: }F_X(x)=\operatorname{P}\big( X\le x \big)=1-(1-\theta )^x $$ $\text{So,}$ $$F_{\mathbf X}(\mathbf x)=\operatorname{P}\big(X_1\le x_1,X_2\le x_2,...,X_n\le x_n \big)=$$ $$ \color{black}{\underbrace{ \operatorname{P}\big(X_1\le x_1\big) \operatorname{P}\big(X_2\le x_2\big)\cdots \operatorname{P}\big( X_n\le x_n \big)=   }_{\text{independent}  }}$$ $$=\color{black}{\underbrace{ \operatorname{P}\bigg(X_1\le x_1 \bigg)^n   }_{ \text{identically }}}\implies F_{\mathbf X}'(\mathbf x)=n\operatorname{P}\big( X_1\le x_1\big)^{n-1}\cdot f_{X_1}(x_1) $$ $$=n\big( 1-(1-\theta )^{x_1}\big)^{n-1}\cdot \theta \big( 1-\theta \big)^{x_1-1}=f_{\mathbf X}(\mathbf x)$$ So, how can i continue the proof from this point? This is as far as  i can get... Any ideas? Thank you.","So, how can i continue the proof from this point? This is as far as  i can get... Any ideas? Thank you.","\bullet \text{ Let} \quad X_1,X_2,...,X_n \text{ iid }X_1\sim \operatorname{Ge}\big( \theta\big) \text{Goal: } \mathbf X =(X_1,...,X_n)^T \in \text{ Exponential Family of distributions.} \text{ Its enough to show that :} \exists \phi \in \Phi \subseteq \mathbb R^k,k\in\mathbb N, \text{parametric space s.t pmf:} f(\mathbf X;\phi)=h(x)\cdot \exp{\bigg\{ \big< s(\mathbf X),\phi \big>_{\mathbb R^k} -K(\phi)\bigg\}},\mathbf X\in \mathcal X ,\phi\in\Phi .  \text{Note : \big< \cdot \big> Inner product on  \mathbb R^k}. \text{pmf of Geometric distr: }\Omega =\mathbb N,\operatorname{P}\big( X=x\big)=f_X(x)=(1-\theta )^{x-1}\theta . \text{cdf: }F_X(x)=\operatorname{P}\big( X\le x \big)=1-(1-\theta )^x  \text{So,} F_{\mathbf X}(\mathbf x)=\operatorname{P}\big(X_1\le x_1,X_2\le x_2,...,X_n\le x_n \big)=  \color{black}{\underbrace{ \operatorname{P}\big(X_1\le x_1\big) \operatorname{P}\big(X_2\le x_2\big)\cdots \operatorname{P}\big( X_n\le x_n \big)=   }_{\text{independent}  }} =\color{black}{\underbrace{ \operatorname{P}\bigg(X_1\le x_1 \bigg)^n   }_{ \text{identically }}}\implies F_{\mathbf X}'(\mathbf x)=n\operatorname{P}\big( X_1\le x_1\big)^{n-1}\cdot f_{X_1}(x_1)  =n\big( 1-(1-\theta )^{x_1}\big)^{n-1}\cdot \theta \big( 1-\theta \big)^{x_1-1}=f_{\mathbf X}(\mathbf x)","['probability-theory', 'statistics', 'probability-distributions', 'statistical-inference', 'exponential-distribution']"
65,Averages clarifications!,Averages clarifications!,,If i know an average such as ( catch an average of one rat every 39 minutes ) what is the average number of rats caught in 10 hours ? is it as simple as dividing 600/39 (600 = 10 hours in minutes) and multiplying this value by number of rats caught in 39 minutes ? in this case 15.38 rats caught in 10 hours.,If i know an average such as ( catch an average of one rat every 39 minutes ) what is the average number of rats caught in 10 hours ? is it as simple as dividing 600/39 (600 = 10 hours in minutes) and multiplying this value by number of rats caught in 39 minutes ? in this case 15.38 rats caught in 10 hours.,,['statistics']
66,Why is the first eigenvalue on the principal component always superior to the second eigenvalue?,Why is the first eigenvalue on the principal component always superior to the second eigenvalue?,,"I'm learning PCA and I wanted to know why the first eigenvalue on the principal component is always superior at second eigenvalue. Thanks for helping me Ps:  I tried to unterstand before asking this question but to be honest, I had difficult to unterstand what I was reading on book. I would like just an easy answer who can allow me to unterstand it better","I'm learning PCA and I wanted to know why the first eigenvalue on the principal component is always superior at second eigenvalue. Thanks for helping me Ps:  I tried to unterstand before asking this question but to be honest, I had difficult to unterstand what I was reading on book. I would like just an easy answer who can allow me to unterstand it better",,"['matrices', 'statistics', 'eigenvalues-eigenvectors', 'principal-component-analysis']"
67,"In 1000 tosses of a coin, 560 heads appear. Is it reasonable to assume the coin is fair?","In 1000 tosses of a coin, 560 heads appear. Is it reasonable to assume the coin is fair?",,"So the question is: In 1000 tosses of a coin, 560 heads appear. Is it reasonable to assume the coin is fair? Let $Y = \sum_{i=1}^{1000} X_i = $ number of heads. I chose $H_0: p = 0.5$ and $H_1: p > 0.5$ . The $p$ -value is: $$P(Y\geq 560) = \sum_{y=560}^{1000}\binom{1000}{y}(0.5)^y(0.5)^{1000-y}$$ $$= 0.00008252494$$ Then I try to find $\alpha$ using $\frac{Y}{n} = \overline{X}$ as the estimator for $p$ : $$P(Y \geq 560) = P(\overline{X} \geq 0.56)$$ $$= P(\frac{\sqrt{n}(\overline{X} - p_0)}{p(1-p)} \geq \frac{\sqrt{1000}(0.56 - 0.5)}{0.5*0.5}$$ $$= P(Z\geq 7.59)$$ That Z-score seems incredibly high and I don't think they're even included in any Z-tables. Am I on the right track here or have I missed something?","So the question is: In 1000 tosses of a coin, 560 heads appear. Is it reasonable to assume the coin is fair? Let number of heads. I chose and . The -value is: Then I try to find using as the estimator for : That Z-score seems incredibly high and I don't think they're even included in any Z-tables. Am I on the right track here or have I missed something?",Y = \sum_{i=1}^{1000} X_i =  H_0: p = 0.5 H_1: p > 0.5 p P(Y\geq 560) = \sum_{y=560}^{1000}\binom{1000}{y}(0.5)^y(0.5)^{1000-y} = 0.00008252494 \alpha \frac{Y}{n} = \overline{X} p P(Y \geq 560) = P(\overline{X} \geq 0.56) = P(\frac{\sqrt{n}(\overline{X} - p_0)}{p(1-p)} \geq \frac{\sqrt{1000}(0.56 - 0.5)}{0.5*0.5} = P(Z\geq 7.59),"['statistics', 'hypothesis-testing']"
68,Why is noncentral t-distribution for sample size determination for small sample?,Why is noncentral t-distribution for sample size determination for small sample?,,"For sample size determination for small sample, population distribution of which is normal, the test statistic $$T = \frac{\bar{x}-\mu_0}{S/\sqrt{n}}, ~~~\text{where $\mu_0$ is null value}$$ is considered under the assumption of the alternative hypothesis $\mu = \mu' \gt \mu_0$ . Since $T$ follows noncentral $t$ distribution, chart or table is used to determine sample size $n$ with desired type II error. The above is typical explanation available in statistics text books. Now I'm curious what's wrong with the following reasoning: If we assume that the alternative hypothesis $μ=μ′$ is correct $T = \frac{\bar{x} - \mu'}{S/\sqrt{n}}$ should follow $t$ -distribution by the same argument that $T = \frac{\bar{x} - \mu_0}{S/\sqrt{n}}$ follows $t$ -distribution when $μ=μ_0$ . Then we can proceed on to calculate $$ Pr(\frac{\bar{x}-\mu'}{S/\sqrt{n}} \leq \frac{\mu_0 + t_{\alpha, n-1}s/\sqrt{n} - \mu'}{s/\sqrt{n}}) = \beta(\mu')~~~\text{(*)} $$ to determine sample size n that satisfies the required $\beta$ . noncentral t-distribution cannot be involved at all. Specifically, if my assumption that $(\bar{x}-\mu')/(s/\sqrt{n})$ follows t distribution is correct, we can simply set: $$ \frac{\mu_0 + t_{\alpha, n-1}s/\sqrt{n} - \mu'}{s/\sqrt{n}} = t_{\beta(\mu')} $$ and solve for $n$ . No need to summon noncentral t distribution, no need to use $\beta$ curve to find $n$ ================================================================= With help of @BruceET, I found my silly mistake. The equation to determine a rejection region is $$ Pr(\frac{\bar{x} - \mu_0}{S/\sqrt{n}} > t_{\alpha, n-1}) = \alpha $$ From this I wrongly thought the rejection region in terms of $\bar{x}$ is $$ \bar{x} > \mu_0 + t_{\alpha, n-1}\frac{s}{\sqrt{n}} $$ This is wrong because $s$ itself is a random variable and therefore cannot be included in the bounds. If it were, I could have argued the eq.(*) could be used to calculate power for the alternative hypothesis.","For sample size determination for small sample, population distribution of which is normal, the test statistic is considered under the assumption of the alternative hypothesis . Since follows noncentral distribution, chart or table is used to determine sample size with desired type II error. The above is typical explanation available in statistics text books. Now I'm curious what's wrong with the following reasoning: If we assume that the alternative hypothesis is correct should follow -distribution by the same argument that follows -distribution when . Then we can proceed on to calculate to determine sample size n that satisfies the required . noncentral t-distribution cannot be involved at all. Specifically, if my assumption that follows t distribution is correct, we can simply set: and solve for . No need to summon noncentral t distribution, no need to use curve to find ================================================================= With help of @BruceET, I found my silly mistake. The equation to determine a rejection region is From this I wrongly thought the rejection region in terms of is This is wrong because itself is a random variable and therefore cannot be included in the bounds. If it were, I could have argued the eq.(*) could be used to calculate power for the alternative hypothesis.","T = \frac{\bar{x}-\mu_0}{S/\sqrt{n}}, ~~~\text{where \mu_0 is null value} \mu = \mu' \gt \mu_0 T t n μ=μ′ T = \frac{\bar{x} - \mu'}{S/\sqrt{n}} t T = \frac{\bar{x} - \mu_0}{S/\sqrt{n}} t μ=μ_0 
Pr(\frac{\bar{x}-\mu'}{S/\sqrt{n}} \leq \frac{\mu_0 + t_{\alpha, n-1}s/\sqrt{n} - \mu'}{s/\sqrt{n}}) = \beta(\mu')~~~\text{(*)}
 \beta (\bar{x}-\mu')/(s/\sqrt{n}) 
\frac{\mu_0 + t_{\alpha, n-1}s/\sqrt{n} - \mu'}{s/\sqrt{n}} = t_{\beta(\mu')}
 n \beta n 
Pr(\frac{\bar{x} - \mu_0}{S/\sqrt{n}} > t_{\alpha, n-1}) = \alpha
 \bar{x} 
\bar{x} > \mu_0 + t_{\alpha, n-1}\frac{s}{\sqrt{n}}
 s","['statistics', 'statistical-inference']"
69,"Why do elliptical copula densities contain $x_1$ and $x_2$, but Archimedean copula densities contain $u_1$ and $u_2$?","Why do elliptical copula densities contain  and , but Archimedean copula densities contain  and ?",x_1 x_2 u_1 u_2,"$$c\left(u_{1}, u_{2}\right)=\frac{1}{\sqrt{1-\rho_{12}^{2}}} \exp \left\{-\frac{\rho_{12}^{2}\left(x_{1}^{2}+x_{2}^{2}\right)-2 \rho_{12} x_{1} x_{2}}{2\left(1-\rho_{12}^{2}\right)}\right\}$$ is the copula density of the Gaussian copula, which is an elliptical copula , where $\rho_{12}$ is the parameter of the copula, $x_{1}=\Phi^{-1}\left(u_{1}\right), x_{2}=$ $\Phi^{-1}\left(u_{2}\right)$ and $\Phi^{-1}(\cdot)$ is the inverse of the standard univariate Gaussian distribution function. $$ c\left(u_{1}, u_{2}\right)=\left(1+\delta_{12}\right)\left(u_{1} \cdot u_{2}\right)^{-1-\delta_{12}} \times\left(u_{1}^{-\delta_{12}}+u_{2}^{-\delta_{12}}-1\right)^{-1 / \delta_{12}-2} $$ is the copula density of the Clayton copula, which is an Archimedean copula , where $0<\delta_{12}<\infty$ is a parameter controlling the dependence. Perfect dependence is obtained when $\delta_{12} \rightarrow \infty,$ while $\delta_{12} \rightarrow 0$ implies independence. Question As you can see the elliptical copula density, despite being a function of $x_1$ and $x_2$ , has the actual random variables $u_1$ and $u_2$ on the right hand-side, whereas the Archimedean copula densities do not have $x_1$ and $x_2$ on the right hand side, but instead what we would expect from the function: $u_1$ and $u_2$ . This property seems to be a shared between all elliptical versus Archimedean copula densities. Why? Source of formulas: Aas et al 2009 ""Pair-copula constructions of multiple dependence""","is the copula density of the Gaussian copula, which is an elliptical copula , where is the parameter of the copula, and is the inverse of the standard univariate Gaussian distribution function. is the copula density of the Clayton copula, which is an Archimedean copula , where is a parameter controlling the dependence. Perfect dependence is obtained when while implies independence. Question As you can see the elliptical copula density, despite being a function of and , has the actual random variables and on the right hand-side, whereas the Archimedean copula densities do not have and on the right hand side, but instead what we would expect from the function: and . This property seems to be a shared between all elliptical versus Archimedean copula densities. Why? Source of formulas: Aas et al 2009 ""Pair-copula constructions of multiple dependence""","c\left(u_{1}, u_{2}\right)=\frac{1}{\sqrt{1-\rho_{12}^{2}}} \exp \left\{-\frac{\rho_{12}^{2}\left(x_{1}^{2}+x_{2}^{2}\right)-2 \rho_{12} x_{1} x_{2}}{2\left(1-\rho_{12}^{2}\right)}\right\} \rho_{12} x_{1}=\Phi^{-1}\left(u_{1}\right), x_{2}= \Phi^{-1}\left(u_{2}\right) \Phi^{-1}(\cdot) 
c\left(u_{1}, u_{2}\right)=\left(1+\delta_{12}\right)\left(u_{1} \cdot u_{2}\right)^{-1-\delta_{12}} \times\left(u_{1}^{-\delta_{12}}+u_{2}^{-\delta_{12}}-1\right)^{-1 / \delta_{12}-2}
 0<\delta_{12}<\infty \delta_{12} \rightarrow \infty, \delta_{12} \rightarrow 0 x_1 x_2 u_1 u_2 x_1 x_2 u_1 u_2","['statistics', 'marginal-distribution', 'ellipsoids', 'marginal-probability', 'copula']"
70,Expectation of sampling integers from a reciprocal distribution without replacement,Expectation of sampling integers from a reciprocal distribution without replacement,,"I have a reciprocal distribution with PDF $$\frac{1}{x\ ln{N}}$$ I sample $k$ integers from this distribution in the range $[1,N]$ without replacement. I need to determine the average (expected) number of each integer in $[1,N]$ in my sample. I came across the multivariate Wallenius' noncentral hypergeometric distribution, which deals with sampling weighted colours of ball from an urn without replacement in sequence. The distribution describes the expected number of each colour $i$ as $\mu_i$ in a vector $\mathbf\mu$ , which can be found by solving the following system of non-linear equations $$\left(1-\frac{\mu_1}{m_1}\right)^\frac{1}{\omega_1} = \left(1-\frac{\mu_2}{m_2}\right)^\frac{1}{\omega_2} = \cdots = \left(1-\frac{\mu_c}{m_c}\right)^\frac{1}{\omega_c}$$ $$\sum_{i=1}^c \mu_i=n$$ For my use case, $c=N$ , $m_i=1$ , $n=k$ and $\omega_i=\frac{1}{i\ lnN}$ , so the equations become $$\left(1-\mu_1\right)^{lnN} = \left(1-\mu_2\right)^{2\ lnN} = \cdots = \left(1-\mu_N\right)^{N\ lnN} \tag{1}$$ $$\sum_{i=1}^N \mu_i=k \tag{2}$$ The more general Wallenius' mean is normally approximated through e.g. Newton-Raphson, so I'm hoping that the narrowing of the equations makes them directly solvable. My work so far is as follows: We can rewrite $(1)$ to put $\mu_i$ in terms of $\mu_j$ $$(1-\mu_i)^{i\ lnN}=(1-\mu_j)^{j\ lnN}$$ Using the identity $a^b=e^{b\ lna}$ $$e^{i\ lnN\ ln(1-\mu_i)}=e^{j\ lnN\ ln(1-\mu_j)}$$ $$i\ lnN\ ln(1-\mu_i)=j\ lnN\ ln(1-\mu_j)$$ $$ln(1-\mu_i)=\frac{j}{i}\ ln(1-\mu_j),\ \ \ N>1$$ $$\mu_i=1-(1-\mu_j)^{j/i}\tag{3}$$ We can then repeatedly substitute $(3)$ into the summation $(2)$ to obtain a formula only in terms of $\mu_i$ $$ \begin{aligned} k&=\mu_1+\mu_2+\cdots+\mu_N\\ &=(1-(1-\mu_i)^{i/1})+(1-(1-\mu_i)^{i/2})+\cdots+(1-(1-\mu_i)^{i/N})\\ &=N-\sum_{j=1}^N (1-\mu_i)^{i/j} \end{aligned} $$ Therefore $$N-k=\sum_{j=1}^N (1-\mu_i)^{i/j}$$ However, I do not know how to proceed. Can this be rearranged for $\mu_i$ ? Am I overcomplicating things?","I have a reciprocal distribution with PDF I sample integers from this distribution in the range without replacement. I need to determine the average (expected) number of each integer in in my sample. I came across the multivariate Wallenius' noncentral hypergeometric distribution, which deals with sampling weighted colours of ball from an urn without replacement in sequence. The distribution describes the expected number of each colour as in a vector , which can be found by solving the following system of non-linear equations For my use case, , , and , so the equations become The more general Wallenius' mean is normally approximated through e.g. Newton-Raphson, so I'm hoping that the narrowing of the equations makes them directly solvable. My work so far is as follows: We can rewrite to put in terms of Using the identity We can then repeatedly substitute into the summation to obtain a formula only in terms of Therefore However, I do not know how to proceed. Can this be rearranged for ? Am I overcomplicating things?","\frac{1}{x\ ln{N}} k [1,N] [1,N] i \mu_i \mathbf\mu \left(1-\frac{\mu_1}{m_1}\right)^\frac{1}{\omega_1}
= \left(1-\frac{\mu_2}{m_2}\right)^\frac{1}{\omega_2}
= \cdots
= \left(1-\frac{\mu_c}{m_c}\right)^\frac{1}{\omega_c} \sum_{i=1}^c \mu_i=n c=N m_i=1 n=k \omega_i=\frac{1}{i\ lnN} \left(1-\mu_1\right)^{lnN}
= \left(1-\mu_2\right)^{2\ lnN}
= \cdots
= \left(1-\mu_N\right)^{N\ lnN} \tag{1} \sum_{i=1}^N \mu_i=k \tag{2} (1) \mu_i \mu_j (1-\mu_i)^{i\ lnN}=(1-\mu_j)^{j\ lnN} a^b=e^{b\ lna} e^{i\ lnN\ ln(1-\mu_i)}=e^{j\ lnN\ ln(1-\mu_j)} i\ lnN\ ln(1-\mu_i)=j\ lnN\ ln(1-\mu_j) ln(1-\mu_i)=\frac{j}{i}\ ln(1-\mu_j),\ \ \ N>1 \mu_i=1-(1-\mu_j)^{j/i}\tag{3} (3) (2) \mu_i 
\begin{aligned}
k&=\mu_1+\mu_2+\cdots+\mu_N\\
&=(1-(1-\mu_i)^{i/1})+(1-(1-\mu_i)^{i/2})+\cdots+(1-(1-\mu_i)^{i/N})\\
&=N-\sum_{j=1}^N (1-\mu_i)^{i/j}
\end{aligned}
 N-k=\sum_{j=1}^N (1-\mu_i)^{i/j} \mu_i","['statistics', 'probability-distributions', 'systems-of-equations', 'sampling']"
71,$MSE(θ_1) = MSE(θ_2) = 0.06$?,?,MSE(θ_1) = MSE(θ_2) = 0.06,"We offer two estimators for the average concentration $μ$ of lead in the atmosphere of a region of Quebec where factories manufacturing dyes are located. The first estimator $θ_1$ has a bias equal to 0.2 and a variance of 0.02. The second estimator $θ_2$ is unbiased and has a variance equal to 0.06. Which of the following is true? $θ_1$ is better than $θ_2$ to estimate $μ$ . $θ_2$ is better than $θ_1$ to estimate $μ$ . None of the above is true I have some difficulties to find the right one. If I use the mean square error (MSE), then $MSE(θ_1) = MSE(θ_2) = 0.06$ . So does the the choice 3 is the right one?","We offer two estimators for the average concentration of lead in the atmosphere of a region of Quebec where factories manufacturing dyes are located. The first estimator has a bias equal to 0.2 and a variance of 0.02. The second estimator is unbiased and has a variance equal to 0.06. Which of the following is true? is better than to estimate . is better than to estimate . None of the above is true I have some difficulties to find the right one. If I use the mean square error (MSE), then . So does the the choice 3 is the right one?",μ θ_1 θ_2 θ_1 θ_2 μ θ_2 θ_1 μ MSE(θ_1) = MSE(θ_2) = 0.06,"['statistics', 'estimation', 'parameter-estimation']"
72,A question on conditional probability (Cambridge Admissions Exam),A question on conditional probability (Cambridge Admissions Exam),,"I was solving the following problem and I got to part (b). I thought of approaching this the following way: There are two types of cartons, one with a 60 per cent probability of being selected and the other with 40 per cent. I split it intwo scenarios and I calculated each of the conditional probabilities and then added them together (alongside with the probability of each carton being selected). Here is what I have done: First consider the case where he carton is of skimmed milk. $$P(X>500 | X<505)= \frac{P(500 < X <505)}{P(X<505)}=\frac{b-\frac{1}{2}}{b}=\frac{2b-1}{2b}$$ Case for full-fat milk $$P(X>500 | X<505)= \frac{P(500 < X <505)}{P(X<505)}=\frac{b-a}{a}$$ Thus the total probability must be $$=\frac{6}{10} \frac{2b-1}{2b} + \frac{4}{10} \frac{b-a}{a} $$ However, this is wrong according to the markscheme given here: My question: Why is it wrong to consider the different cases separately for part (i)?","I was solving the following problem and I got to part (b). I thought of approaching this the following way: There are two types of cartons, one with a 60 per cent probability of being selected and the other with 40 per cent. I split it intwo scenarios and I calculated each of the conditional probabilities and then added them together (alongside with the probability of each carton being selected). Here is what I have done: First consider the case where he carton is of skimmed milk. Case for full-fat milk Thus the total probability must be However, this is wrong according to the markscheme given here: My question: Why is it wrong to consider the different cases separately for part (i)?",P(X>500 | X<505)= \frac{P(500 < X <505)}{P(X<505)}=\frac{b-\frac{1}{2}}{b}=\frac{2b-1}{2b} P(X>500 | X<505)= \frac{P(500 < X <505)}{P(X<505)}=\frac{b-a}{a} =\frac{6}{10} \frac{2b-1}{2b} + \frac{4}{10} \frac{b-a}{a} ,"['probability', 'statistics']"
73,Transportation-information inequalities (concentration inequalities),Transportation-information inequalities (concentration inequalities),,"The following is one example of a transportation-information inequality,  which show connections between optimal transport theory and information theory : $$W_1(\nu,\mu) \leq \left[ 2\sigma^2 D_{KL}(\nu\Vert\mu) \right]^\frac{1}{2} $$ $W_1$ is the Wasserstein distance found in optimal transport theory, and $D_{KL}$ is the Kullback-Leibler (KL) divergence found in information theory. ( Source ) What other transportation-information inequalities are out there?","The following is one example of a transportation-information inequality,  which show connections between optimal transport theory and information theory : is the Wasserstein distance found in optimal transport theory, and is the Kullback-Leibler (KL) divergence found in information theory. ( Source ) What other transportation-information inequalities are out there?","W_1(\nu,\mu) \leq \left[ 2\sigma^2 D_{KL}(\nu\Vert\mu) \right]^\frac{1}{2}  W_1 D_{KL}","['statistics', 'probability-distributions', 'information-theory', 'concentration-of-measure', 'optimal-transport']"
74,Conditional Probabilities defective items,Conditional Probabilities defective items,,"A company sends 30% of its product to Client A and 70% to Client B. Client A reports that 5% of the products it received are defective, whereas Client B reports that 4% of products received are defective. The defective products are returned back to the company. What is the probability of a returned defective product coming from client B? probability of an item being sent to customer A  = $P(C_a) =  0.30$ & $P(D|C_a) = 0.05$ probability of an item being sent to customer B  = $P(C_b) =  0.70$ & $P(D|C_b) = 0.04$ $P$ (Returned) = $P$ (Defective)= $P(D)$ = $0.3 \times 0.05 + 0.7\times 0.04 = 0.043$ My thinking is that I need to find $P$ ( $C_b$ | Defective ) $= P( C_b | D )$ $P( C_b | D ) = \frac{P(D \,\cap \,C_b)}{P(D)} = \frac{P(D \,\cap \,C_b)}{0.043}\;$ so I need to find $P(D \,\cap \,C_b)$ $P(D|C_b) = \frac{P(D \,\cap \,C_b)}{P(C_b)}\;$ so $0.04 = \frac{P(D \,\cap \,C_b)}{0.70}\;$ which implies $P(D \,\cap \,C_b) = 0.028$ $P( C_b | D ) = \frac{0.028}{0.043} = \frac{28}{43}$ however I feel my answer may be wrong. Am i on the right track?","A company sends 30% of its product to Client A and 70% to Client B. Client A reports that 5% of the products it received are defective, whereas Client B reports that 4% of products received are defective. The defective products are returned back to the company. What is the probability of a returned defective product coming from client B? probability of an item being sent to customer A  = & probability of an item being sent to customer B  = & (Returned) = (Defective)= = My thinking is that I need to find ( | Defective ) so I need to find so which implies however I feel my answer may be wrong. Am i on the right track?","P(C_a) =  0.30 P(D|C_a) = 0.05 P(C_b) =  0.70 P(D|C_b) = 0.04 P P P(D) 0.3 \times 0.05 + 0.7\times 0.04 = 0.043 P C_b = P( C_b | D ) P( C_b | D ) = \frac{P(D \,\cap \,C_b)}{P(D)} = \frac{P(D \,\cap \,C_b)}{0.043}\; P(D \,\cap \,C_b) P(D|C_b) = \frac{P(D \,\cap \,C_b)}{P(C_b)}\; 0.04 = \frac{P(D \,\cap \,C_b)}{0.70}\; P(D \,\cap \,C_b) = 0.028 P( C_b | D ) = \frac{0.028}{0.043} = \frac{28}{43}","['probability', 'statistics', 'conditional-probability']"
75,"Likelihood function of $n$ for $X\sim \text{Bin}(n,0.5)$",Likelihood function of  for,"n X\sim \text{Bin}(n,0.5)","I have to find the likelihood function for $X\sim \text{Bin}(n,0.5)$ where we know that $x=14$ and so $n \ge 14$ . Several trials were conducted counting successes (14 of which there were) but the number of trials was forgotten, so we need to estimate $n$ . I know that the pmf of a Binomial random variable is $$P(X=x)=\binom{n}{x}\cdot p^x \cdot(1-p)^{n-x}$$ and in this case we have $$P(X=14)=\binom{n}{14}\cdot 0.5^{14} \cdot(1-0.5)^{n-14}=\binom{n}{14}\cdot0.5^{n}\,,$$ however I'm not sure this would be useful since for $n \ge 14$ , the function seems to increase at an exponential rate that doesn't appear to have a maximum. So I'm not sure using the pmf is appropriate - does anyone have any other ideas? Any help would be greatly appreciated.","I have to find the likelihood function for where we know that and so . Several trials were conducted counting successes (14 of which there were) but the number of trials was forgotten, so we need to estimate . I know that the pmf of a Binomial random variable is and in this case we have however I'm not sure this would be useful since for , the function seems to increase at an exponential rate that doesn't appear to have a maximum. So I'm not sure using the pmf is appropriate - does anyone have any other ideas? Any help would be greatly appreciated.","X\sim \text{Bin}(n,0.5) x=14 n \ge 14 n P(X=x)=\binom{n}{x}\cdot p^x \cdot(1-p)^{n-x} P(X=14)=\binom{n}{14}\cdot 0.5^{14} \cdot(1-0.5)^{n-14}=\binom{n}{14}\cdot0.5^{n}\,, n \ge 14","['statistics', 'probability-distributions', 'binomial-distribution', 'parameter-estimation']"
76,Estimating $|\mu_1 - \mu_2|$ in a mixture of two Gaussians with known variance.,Estimating  in a mixture of two Gaussians with known variance.,|\mu_1 - \mu_2|,"Consider $n$ samples denoted with $X^n = X_1, \ldots, X_n$ from mixture of two Gaussians with equal variance and equal mixture proportions: $X^n \sim 1/2 \mathcal{N}(\mu_1, 1) + 1/2 \mathcal{N}(\mu_2, 1)$ . Now the goal is to estimate the difference in means $|\mu_1 - \mu_2|$ from this $n$ samples. One idea is to rank them from lowest to highest, and take the difference between the 1/4th highest and lowest sample point. But how accurate is this estimate, and is there a better way?","Consider samples denoted with from mixture of two Gaussians with equal variance and equal mixture proportions: . Now the goal is to estimate the difference in means from this samples. One idea is to rank them from lowest to highest, and take the difference between the 1/4th highest and lowest sample point. But how accurate is this estimate, and is there a better way?","n X^n = X_1, \ldots, X_n X^n \sim 1/2 \mathcal{N}(\mu_1, 1) + 1/2 \mathcal{N}(\mu_2, 1) |\mu_1 - \mu_2| n","['statistics', 'order-statistics', 'gaussian']"
77,Non-homogeneous Poisson process first occurrence distribution,Non-homogeneous Poisson process first occurrence distribution,,"I am trying to do the following problem in order to prepare for my Stochastic Processes exam: Problem statement: Consider $N(t)$ to be a non homogeneous Poisson process with rate $\lambda(t)$ over the interval [0, a]. Let $T_{1}$ denote the time for the first event to occur. Find the distribution of $T_{1}$ given that $N(a) = 1$ . Here, $N(t)$ denotes the number of occurrences up until time t. -> What have i tried so far? I started by investigating the following: $$P(T_{1} > t) = P(N(T_{1})=0)$$ Which, since we have a non homogeneous Poisson process, turns out to be: $$\frac{\left[\int_{0}^{T_{1}} \lambda(s) d s)\right]^{0}e^{-\int_{0}^{T_{1}} \lambda(s)ds}}{0 {\displaystyle !\,}} = e^{-\int_{0}^{T_{1}} \lambda(s)ds}$$ Finally: $$P\left(N\left(T_{1}\right)=0\right) = e^{-\int_{0}^{T_{1}} \lambda(s) d s}$$ Now, i tried to use the remaining information to finish the problem. However, i got stuck . I don't know how to proceed in regards to conditioning the variables ( I dont know if that is indeed the path to solve the problem ). I tried doing: $$P\left(N\left(T_{1}\right)=0 \mid N(a)=1\right) = \frac{P\left(N\left(T_{1}\right)=0, N(a)=1\right)}{P(N(a)=1)}$$ Now, i tried a few things, but i dont know if that is indeed correct. Knowing that $N(T) = 0$ and $N(a) = 1$ , one can infer that $T \le a$ , otherwise $N(T)$ would not equal zero, as one event would have ocurred. This information can be translated into: $$P(N(a)=1) = e^{-\int_{a}^{T_{1}} \lambda(s) d s}$$ Now, if i could expand the below term: $$P\left(N\left(T_{1}\right)=0, N(a)=1\right)$$ into something useful, i could start manipulating some terms. What do you guys think of my approach? Any help is appreciated. Thanks in advance!","I am trying to do the following problem in order to prepare for my Stochastic Processes exam: Problem statement: Consider to be a non homogeneous Poisson process with rate over the interval [0, a]. Let denote the time for the first event to occur. Find the distribution of given that . Here, denotes the number of occurrences up until time t. -> What have i tried so far? I started by investigating the following: Which, since we have a non homogeneous Poisson process, turns out to be: Finally: Now, i tried to use the remaining information to finish the problem. However, i got stuck . I don't know how to proceed in regards to conditioning the variables ( I dont know if that is indeed the path to solve the problem ). I tried doing: Now, i tried a few things, but i dont know if that is indeed correct. Knowing that and , one can infer that , otherwise would not equal zero, as one event would have ocurred. This information can be translated into: Now, if i could expand the below term: into something useful, i could start manipulating some terms. What do you guys think of my approach? Any help is appreciated. Thanks in advance!","N(t) \lambda(t) T_{1} T_{1} N(a) = 1 N(t) P(T_{1} > t) = P(N(T_{1})=0) \frac{\left[\int_{0}^{T_{1}} \lambda(s) d s)\right]^{0}e^{-\int_{0}^{T_{1}} \lambda(s)ds}}{0 {\displaystyle !\,}} = e^{-\int_{0}^{T_{1}} \lambda(s)ds} P\left(N\left(T_{1}\right)=0\right) = e^{-\int_{0}^{T_{1}} \lambda(s) d s} P\left(N\left(T_{1}\right)=0 \mid N(a)=1\right) = \frac{P\left(N\left(T_{1}\right)=0, N(a)=1\right)}{P(N(a)=1)} N(T) = 0 N(a) = 1 T \le a N(T) P(N(a)=1) = e^{-\int_{a}^{T_{1}} \lambda(s) d s} P\left(N\left(T_{1}\right)=0, N(a)=1\right)","['probability', 'statistics', 'stochastic-processes', 'poisson-distribution', 'poisson-process']"
78,Is Gower's Distance a metric?,Is Gower's Distance a metric?,,"A novice here My previous question was closed due to inadequate details So here I've added more details A metric should basically satisfy 3 properties Distance is equal to zero if and only if $x$ is equal to $y$ ( $d(x,y)=0 ⇔ x=y$ )) Distance from $x$ to $y$ is the same as distance from $y$ to $x$ ( $d(x,y)=d(y,x)$ ) Distance should satisfy the triangular inequality ( $d(x,y)\leq d(x,z) +d(z,y)$ ) I already know that Gower's distance satisfy the first 2 properties to be a metric, but I want to know whether it satisfies the triangular inequality property. The reason I want to know this, is because all metric spaces are Hausdorff spaces, and I want use the Gowers distance in order to find the Hausdorff distance for 2 sets of points. In my case, a point contains data of mixed types (logical, categorical & numeral), and therefore I have to use the Gowers distance. Any help would be appreciated. Thank You! Edit: According to a suggestion on the comments, here is the formal definition of a metric A metric on a set X is a function (called the distance function or simply distance) $d : X × X → R$ (where R is the set of real numbers). For all $ x, y, z $ in $X$ , this function is required to satisfy the following conditions: $d(x, y) ≥ 0$ (non-negativity) $d(x, y) = 0$ if and only if $x = y $ $d(x, y) = d(y, x) $ (symmetry) $d(x, z) ≤ d(x, y) + d(y, z) $ Note that the first condition is implied by the others.","A novice here My previous question was closed due to inadequate details So here I've added more details A metric should basically satisfy 3 properties Distance is equal to zero if and only if is equal to ( )) Distance from to is the same as distance from to ( ) Distance should satisfy the triangular inequality ( ) I already know that Gower's distance satisfy the first 2 properties to be a metric, but I want to know whether it satisfies the triangular inequality property. The reason I want to know this, is because all metric spaces are Hausdorff spaces, and I want use the Gowers distance in order to find the Hausdorff distance for 2 sets of points. In my case, a point contains data of mixed types (logical, categorical & numeral), and therefore I have to use the Gowers distance. Any help would be appreciated. Thank You! Edit: According to a suggestion on the comments, here is the formal definition of a metric A metric on a set X is a function (called the distance function or simply distance) (where R is the set of real numbers). For all in , this function is required to satisfy the following conditions: (non-negativity) if and only if (symmetry) Note that the first condition is implied by the others.","x y d(x,y)=0 ⇔ x=y x y y x d(x,y)=d(y,x) d(x,y)\leq d(x,z) +d(z,y) d : X × X → R  x, y, z  X d(x, y) ≥ 0 d(x, y) = 0 x = y  d(x, y) = d(y, x)  d(x, z) ≤ d(x, y) + d(y, z) ","['statistics', 'metric-spaces', 'hausdorff-distance', 'mixing-variables']"
79,Annual variance from monthly data (returns),Annual variance from monthly data (returns),,"I can't determine how we establish equality between the two bracketed expressions in the image below. The square of the sum is being equated with the sum of the squares, which in general isn't true. What am I missing? Thank you.","I can't determine how we establish equality between the two bracketed expressions in the image below. The square of the sum is being equated with the sum of the squares, which in general isn't true. What am I missing? Thank you.",,"['statistics', 'finance', 'variance']"
80,Hypothesis testing for a Uniform distribution with both parameters unknown,Hypothesis testing for a Uniform distribution with both parameters unknown,,"I need some help, I need to make a hypothesis testing for a Uniform distribution with both parameters $(a,b)$ unknown for $\begin{align}  H_{0}:b-a>2    \end{align}$ So. I know that I need a minimal sufficient statistic which is going to be a vector $(X_{(1)},X_{(n)})$ , where $X_{(1)}=\min\{X1,...,X_{n} \}$ and $X_{(n)}=\max\{X1,...,X_{n} \}$ , Is my reject region going to be $RR_{\alpha}=\{x_{1},...x_{n}:X_{(n)}-X_{(1)}<C_{\alpha} \}$ for a $0<C_{\alpha}<1$ ? How can I make the hypothesis testing on which the probability error type 1 and 2 don't exceed $\alpha$ and $\beta$ , where $\mathbb{P}(E.T.1)=\mathbb{P}$ (Reject $H_{0}$ | $H_{0}$ is true)= $\mathbb{P}(X_{(n)}-X_{(1)}\leq C_{\alpha}|b-a>2)$ ? Thanks","I need some help, I need to make a hypothesis testing for a Uniform distribution with both parameters unknown for So. I know that I need a minimal sufficient statistic which is going to be a vector , where and , Is my reject region going to be for a ? How can I make the hypothesis testing on which the probability error type 1 and 2 don't exceed and , where (Reject | is true)= ? Thanks","(a,b) \begin{align}  H_{0}:b-a>2    \end{align} (X_{(1)},X_{(n)}) X_{(1)}=\min\{X1,...,X_{n} \} X_{(n)}=\max\{X1,...,X_{n} \} RR_{\alpha}=\{x_{1},...x_{n}:X_{(n)}-X_{(1)}<C_{\alpha} \} 0<C_{\alpha}<1 \alpha \beta \mathbb{P}(E.T.1)=\mathbb{P} H_{0} H_{0} \mathbb{P}(X_{(n)}-X_{(1)}\leq C_{\alpha}|b-a>2)","['statistics', 'hypothesis-testing']"
81,Tutors correcting tests - Confidence intervals,Tutors correcting tests - Confidence intervals,,"At the end of the semester, two tutors Albert and Ben are correcting an exam with $10$ tasks. They share the $100$ written exams and measure the time needed to correct a task in minutes. The difference $x_i$ of the correction times (Ben's time $-$ Albert's time) for task $i$ is given in the following table: The sample mean $\bar{x} = 4.4$ and the sample standard deviation $\bar{\sigma} = 6.82$ . We assume that the values $x_1, x_2, ..., x_{10}$ are realizations of $10$ independent and identically normally distributed random variables. For the significance level $\alpha = 0.05$ , find a confidence interval for the difference $x_i$ and determine the acceptance region for $\bar{x}.$ Since the population standard deviation $\sigma$ is not given, we will use the $t-$ distribution (or Student- $t$ -distribution) to find the confidence interval for the population mean $\mu$ . First we calculate our acceptance thresholds $t_c$ and $-t_c$ : Since we know that $\alpha = 0.05$ , the area of the region right to $t_c$ $= 0.025 = $ the area left to $-t_c$ . We also know that we have $n-1 = 10-1 = 9$ degrees of freedom. Using the $t-$ distribution values table, we find $t_c = 2.26$ and $-t_c = -2.26.$ Now we find our test statistic $T_s$ : $T_s = \dfrac{\bar{x} - \mu}{\dfrac{\bar{\sigma}}{\sqrt{n}}}$ $= \dfrac{4.4 - \mu}{\dfrac{6.82}{\sqrt{10}}}$ . We know that $P(-t_c \leq T_s \leq t_c) = 1- \alpha = 0.95.$ Substituting then gives us: $$\bar{x} - t_c \cdot \dfrac{\bar{\sigma}}{\sqrt{n}} \leq \mu \leq \bar{x} + t_c \cdot \dfrac{\bar{\sigma}}{\sqrt{n}}$$ $$4.4 -2.26 \cdot \dfrac{6.82}{\sqrt{10}} \leq \mu \leq 4.4 +2.26 \cdot \dfrac{6.82}{\sqrt{10}}$$ $$-0.474 \leq \mu \leq 9.274$$ So we know that $-0.474 \leq \mu \leq 9.274$ with $95\%$ confidence. The acceptance region for $\bar{x}$ would be $[-t_c \cdot \dfrac{\bar{\sigma}}{\sqrt{n}}, t_c \cdot \dfrac{\bar{\sigma}}{\sqrt{n}}] = [-4.874, 4.874].$ Did I do this correctly? I'm very unsure about my work and don't know how to interpret the negative values in the confidence interval.","At the end of the semester, two tutors Albert and Ben are correcting an exam with tasks. They share the written exams and measure the time needed to correct a task in minutes. The difference of the correction times (Ben's time Albert's time) for task is given in the following table: The sample mean and the sample standard deviation . We assume that the values are realizations of independent and identically normally distributed random variables. For the significance level , find a confidence interval for the difference and determine the acceptance region for Since the population standard deviation is not given, we will use the distribution (or Student- -distribution) to find the confidence interval for the population mean . First we calculate our acceptance thresholds and : Since we know that , the area of the region right to the area left to . We also know that we have degrees of freedom. Using the distribution values table, we find and Now we find our test statistic : . We know that Substituting then gives us: So we know that with confidence. The acceptance region for would be Did I do this correctly? I'm very unsure about my work and don't know how to interpret the negative values in the confidence interval.","10 100 x_i - i \bar{x} = 4.4 \bar{\sigma} = 6.82 x_1, x_2, ..., x_{10} 10 \alpha = 0.05 x_i \bar{x}. \sigma t- t \mu t_c -t_c \alpha = 0.05 t_c = 0.025 =  -t_c n-1 = 10-1 = 9 t- t_c = 2.26 -t_c = -2.26. T_s T_s = \dfrac{\bar{x} - \mu}{\dfrac{\bar{\sigma}}{\sqrt{n}}} = \dfrac{4.4 - \mu}{\dfrac{6.82}{\sqrt{10}}} P(-t_c \leq T_s \leq t_c) = 1- \alpha = 0.95. \bar{x} - t_c \cdot \dfrac{\bar{\sigma}}{\sqrt{n}} \leq \mu \leq \bar{x} + t_c \cdot \dfrac{\bar{\sigma}}{\sqrt{n}} 4.4 -2.26 \cdot \dfrac{6.82}{\sqrt{10}} \leq \mu \leq 4.4 +2.26 \cdot \dfrac{6.82}{\sqrt{10}} -0.474 \leq \mu \leq 9.274 -0.474 \leq \mu \leq 9.274 95\% \bar{x} [-t_c \cdot \dfrac{\bar{\sigma}}{\sqrt{n}}, t_c \cdot \dfrac{\bar{\sigma}}{\sqrt{n}}] = [-4.874, 4.874].","['probability', 'statistics', 'statistical-inference', 'confidence-interval']"
82,Controversial probability calculation regarding Thai lotto incident,Controversial probability calculation regarding Thai lotto incident,,"At 1st September 2020, the number ""999997"" was picked for the first prize in Thailand's government lotto. The consecutive repeating of the number ""9"" caused extensive controversial discussion whether the lotto machine was working properly or not, some even claim that this incident proved that the government was cheating. Note for the lotto drawing method. A six-digit number will be randomly picked from the set of 000000, ..., 999999 for the 1st prize by using 6 staffs each draw a number 0 - 9 from their corresponding machines. To simplify the problem, I will consider the 1st prize number ""999999"" instead of ""999997"" in this question. Commonly, most people know that every number has equal probability of $1/1000000$ . Let me define the mathematical statement for this. Statement 1: Randomly drawing a number $n$ from the set of six-digit numbers $000000, ..., 999999$ , the probability of $n$ being any specific number in the set is $1/1000000$ Now, the problem arises when someone proposes the following statement. Statement 2: Let $A$ be a set {000000, 111111, 222222, ..., 999999}, The probability of $n$ being a member of $A$ is $10/1000000$ . On one side, people use Statement 1 to explain that the number ""999999"" being drawn is as usual as any familiar number such as ""326648"", ""863439"", ... On the other side, people use Statement 2 to claim that the number ""999999"" being drawn is ""unusual"" as it has only $10/1000000$ probability to draw this kind of number. I got some feeling that latter claim using Statement 2 has something wrong because if I let the set $A$ being a set of my any desired 10 numbers such as {123456, 443253, 857342, ...}, I could claim that any number is unusual. But I cannot explain it clearly enough to convince the people who believe this claim. Please help me see if there is some mathematical explanation behind this conflict, which can explain why the claim using Statement 2 is invalid and why people find it difficult to figure it out spontaneously.","At 1st September 2020, the number ""999997"" was picked for the first prize in Thailand's government lotto. The consecutive repeating of the number ""9"" caused extensive controversial discussion whether the lotto machine was working properly or not, some even claim that this incident proved that the government was cheating. Note for the lotto drawing method. A six-digit number will be randomly picked from the set of 000000, ..., 999999 for the 1st prize by using 6 staffs each draw a number 0 - 9 from their corresponding machines. To simplify the problem, I will consider the 1st prize number ""999999"" instead of ""999997"" in this question. Commonly, most people know that every number has equal probability of . Let me define the mathematical statement for this. Statement 1: Randomly drawing a number from the set of six-digit numbers , the probability of being any specific number in the set is Now, the problem arises when someone proposes the following statement. Statement 2: Let be a set {000000, 111111, 222222, ..., 999999}, The probability of being a member of is . On one side, people use Statement 1 to explain that the number ""999999"" being drawn is as usual as any familiar number such as ""326648"", ""863439"", ... On the other side, people use Statement 2 to claim that the number ""999999"" being drawn is ""unusual"" as it has only probability to draw this kind of number. I got some feeling that latter claim using Statement 2 has something wrong because if I let the set being a set of my any desired 10 numbers such as {123456, 443253, 857342, ...}, I could claim that any number is unusual. But I cannot explain it clearly enough to convince the people who believe this claim. Please help me see if there is some mathematical explanation behind this conflict, which can explain why the claim using Statement 2 is invalid and why people find it difficult to figure it out spontaneously.","1/1000000 n 000000, ..., 999999 n 1/1000000 A n A 10/1000000 10/1000000 A","['probability', 'statistics', 'lotteries']"
83,Finding the conditional probabilities of a latent dirichlet allocation model,Finding the conditional probabilities of a latent dirichlet allocation model,,"Let's say I'm defining a LDA as the following: For each doc $m$ : Sample topic probabilities $\theta_m \sim Dirichlet(\alpha)$ For each word $n$ : Sample a topic $z_{mn} \sim Multinomial(\theta_m)$ Sample a word $w_{mn} \sim Multinomial(\beta)$ where $\alpha, \beta$ are fixed hyperparameters. Now, to do this, I'm going to use an EM algorithm. For the 'E' step, given $\alpha$ and $\beta$ , I infer $z_{mn}$ for all $n$ and $\theta_m$ for all $m$ given $w, \alpha, \beta$ using Gibbs sampling. Equivalently, I'm trying to find $p(z, \theta | w, \alpha, \beta) = \frac{p(z, \theta, w | \alpha, \beta)}{p(w|\alpha, \beta)}$ using Gibbs sampling. At a high level, I understand what Gibbs Sampling is and what this LDA model does. We assigns topics to each word and document (assuming all others are correct). We do repeat this process in a chain until we maximize the probability of each $m$ and $n$ belonging to a particular category. However, I'm having trouble representing the conditional probabilities of the Gibbs Sampler for this model. Where do I even start and what am I looking for when asked to find the conditional probabilities of this sampler?","Let's say I'm defining a LDA as the following: For each doc : Sample topic probabilities For each word : Sample a topic Sample a word where are fixed hyperparameters. Now, to do this, I'm going to use an EM algorithm. For the 'E' step, given and , I infer for all and for all given using Gibbs sampling. Equivalently, I'm trying to find using Gibbs sampling. At a high level, I understand what Gibbs Sampling is and what this LDA model does. We assigns topics to each word and document (assuming all others are correct). We do repeat this process in a chain until we maximize the probability of each and belonging to a particular category. However, I'm having trouble representing the conditional probabilities of the Gibbs Sampler for this model. Where do I even start and what am I looking for when asked to find the conditional probabilities of this sampler?","m \theta_m \sim Dirichlet(\alpha) n z_{mn} \sim Multinomial(\theta_m) w_{mn} \sim Multinomial(\beta) \alpha, \beta \alpha \beta z_{mn} n \theta_m m w, \alpha, \beta p(z, \theta | w, \alpha, \beta) = \frac{p(z, \theta, w | \alpha, \beta)}{p(w|\alpha, \beta)} m n","['probability', 'statistics', 'expected-value', 'statistical-inference', 'machine-learning']"
84,What is the difference between standard deviation and variance?,What is the difference between standard deviation and variance?,,"I have been learning discrete probability and know that variance and sd are both measures of spread. I also know that sd is just the variance squared. But I don't know the difference between them. Do they measure different types of spread? Also, I know that the expected value is used in terms of a full census so the mean is called the expected value. In a sample, however, the mean is just called the mean and is only an approximation of $\mu$ (E(x)). Similarly, $S$ is an approximation of $\sigma$ . But is there such thing as an approximation of $Var(x)$ in a sample?","I have been learning discrete probability and know that variance and sd are both measures of spread. I also know that sd is just the variance squared. But I don't know the difference between them. Do they measure different types of spread? Also, I know that the expected value is used in terms of a full census so the mean is called the expected value. In a sample, however, the mean is just called the mean and is only an approximation of (E(x)). Similarly, is an approximation of . But is there such thing as an approximation of in a sample?",\mu S \sigma Var(x),['statistics']
85,Convergence Rate of Sample Variance,Convergence Rate of Sample Variance,,"Question: Given $X_1,X_2,...$ as an sequence of iid distributed random variables with $E(X_i)=0 $ and $V(X_i)=σ^2$ and the fourth order moment $E(X_i^4)<\infty$ . Show that: $\sqrt{n}(S_n^2-\sigma^2)\xrightarrow{d}N(0,E[(X_i^2-\sigma^2)^2])$ , where $S_n^2$ is the sample variance. I am sure that we have to employ the fact that $S^2_n\xrightarrow{p} \sigma^2$ together with the Central Limit Theorem. But I still can not figure out the exact proof of this problem.","Question: Given as an sequence of iid distributed random variables with and and the fourth order moment . Show that: , where is the sample variance. I am sure that we have to employ the fact that together with the Central Limit Theorem. But I still can not figure out the exact proof of this problem.","X_1,X_2,... E(X_i)=0  V(X_i)=σ^2 E(X_i^4)<\infty \sqrt{n}(S_n^2-\sigma^2)\xrightarrow{d}N(0,E[(X_i^2-\sigma^2)^2]) S_n^2 S^2_n\xrightarrow{p} \sigma^2","['probability-theory', 'statistics', 'central-limit-theorem', 'probability-limit-theorems', 'law-of-large-numbers']"
86,Is the formula for standard error for the slope of a linear regression with intercept the same as without?,Is the formula for standard error for the slope of a linear regression with intercept the same as without?,,If we are given sets $X$ and $Y$ . The standart error formula for $\alpha$ coefficient of the regrssion $\hat{y} = \alpha x + \beta$ is $$ \frac{\sum{(y_i -\hat{y})^2}/(n-2)}{\sqrt{\sum(x_i-\bar{x})^2}}$$ How the formula for $S.E(\alpha)$ would change if we considered $\hat{y} = \alpha x$,If we are given sets and . The standart error formula for coefficient of the regrssion is How the formula for would change if we considered,X Y \alpha \hat{y} = \alpha x + \beta  \frac{\sum{(y_i -\hat{y})^2}/(n-2)}{\sqrt{\sum(x_i-\bar{x})^2}} S.E(\alpha) \hat{y} = \alpha x,"['statistics', 'parameter-estimation', 'linear-regression']"
87,Is there a meaning to $\mathrm{e}^{H(p_{i})}$ or $2^{H(p_{i})}$?,Is there a meaning to  or ?,\mathrm{e}^{H(p_{i})} 2^{H(p_{i})},"In my research I find an equation featuring the ""exponential entropy"" term $\mathrm{e}^{H(p_{i})}$ and I wonder if it has a specific meaning. I have only found rare references to that term (usually in terms of dispersion or ""spread of the distribution"") so I'm looking for more insights. I work with natural logarithms and in my case the entropy is Shannon's: $H(p_{i})=-\sum{ p_{i}\ln p_{i}}$ ... My question is: what is $\mathrm{e}^{H(p_{i})}$ ? Note: I assume that the same question would arise if I were to work in log-base 2... So  is there a meaning to $2^{H(p_{i})}$ when entropy is now defined by $H(p_{i})=-\sum{p_{i}\log_{2} p_{i}}$ ?","In my research I find an equation featuring the ""exponential entropy"" term and I wonder if it has a specific meaning. I have only found rare references to that term (usually in terms of dispersion or ""spread of the distribution"") so I'm looking for more insights. I work with natural logarithms and in my case the entropy is Shannon's: ... My question is: what is ? Note: I assume that the same question would arise if I were to work in log-base 2... So  is there a meaning to when entropy is now defined by ?",\mathrm{e}^{H(p_{i})} H(p_{i})=-\sum{ p_{i}\ln p_{i}} \mathrm{e}^{H(p_{i})} 2^{H(p_{i})} H(p_{i})=-\sum{p_{i}\log_{2} p_{i}},"['statistics', 'entropy', 'descriptive-statistics', 'statistical-mechanics']"
88,"Are the remaining samples still independent, identically distributed (IID) after removing the maximum value of the IID samples?","Are the remaining samples still independent, identically distributed (IID) after removing the maximum value of the IID samples?",,"$X_1, X_2, \cdots, X_N$ are independent identically distributed (IID) random variables and $Y_1$ , $Y_2, \cdots, Y_{N-1}$ are the remaining after removing the maximum value of $X_k, k=1, \cdots, N .$ Is this assumption true that $Y_1$ , $Y_2, \cdots, Y_{N-1}$ are IID?","are independent identically distributed (IID) random variables and , are the remaining after removing the maximum value of Is this assumption true that , are IID?","X_1, X_2, \cdots, X_N Y_1 Y_2, \cdots, Y_{N-1} X_k, k=1, \cdots, N . Y_1 Y_2, \cdots, Y_{N-1}","['statistics', 'probability-distributions', 'order-statistics', 'statistical-mechanics']"
89,"How should I state the null and alternative hypotheses, when the alternative speaks in favour of the null one?","How should I state the null and alternative hypotheses, when the alternative speaks in favour of the null one?",,"The question is as follows: More than 50% of all people usually drink coffee before breakfast. To check this claim 100 people were chosen randomly and 60 of them declared to have a coffee before breakfast. Clearly the sample taken confirms that the null hypothesis is correct. Moreover, it is said alternative hypothesis never contains <=, >= or = operators, so even if I set H 0 : p > 0.5, I shouldn't do H a : p <= 0.5, which would contradict the sample taken. How should I deal with this problem? Or could that be an error in the question?","The question is as follows: More than 50% of all people usually drink coffee before breakfast. To check this claim 100 people were chosen randomly and 60 of them declared to have a coffee before breakfast. Clearly the sample taken confirms that the null hypothesis is correct. Moreover, it is said alternative hypothesis never contains <=, >= or = operators, so even if I set H 0 : p > 0.5, I shouldn't do H a : p <= 0.5, which would contradict the sample taken. How should I deal with this problem? Or could that be an error in the question?",,"['statistics', 'hypothesis-testing']"
90,Cramér–Rao bound on estimating the parameters of an impulse,Cramér–Rao bound on estimating the parameters of an impulse,,"Given a noisy discrete-time complex signal that is the sum of an impulse at some time, $t_0$ , (with amplitude, $a_0 e^{i \phi_0}$ ) and additive white Gaussian noise, what is the Cramér–Rao lower bound on the variance of an unbiased estimator of $t_0, a_0, \phi_0$ ? If I have a discrete-time signal of $N$ samples (let $N$ be even for simplicity), $z_n$ , as described above, if you took the discrete Fourier transform, you would get: $$ Z_n = a_0 \exp\left(\frac{-2\pi i t_0 n}{N} + i\phi_0\right) + \mathcal{CN}(0, 2 N \sigma^2)$$ where $t_0$ is the time of the impulse in the time-domain (and the parameter to be estimated), $a_0$ is some complex amplitude of this impulse, $n = -\frac{N}{2}, ... \frac{N}{2} - 1$ , and $i$ is the imaginary unit. Here I have assumed a sampling frequency of $1$ without loss of generality. The additive complex Gaussian noise, $\mathcal{CN}(0, 2 N \sigma^2)$ , is a complex random variable where both the real and imaginary parts follow a $\mathcal{N}(0, N \sigma^2)$ distribution each. The factor of $N$ in the variance of the additive noise accounts for the normalization factor in the inverse Discrete Fourier Transform, ensuring a constant noise variance in the time-domain. $a_0 > 0, t_0 \in [0, N], \phi_0 \in [-\pi, \pi)$ are real parameters that describe the impulse in the time-domain. Intuitively, it seems to me that if we take the discrete-time Fourier transform $$ f(t) = \frac{1}{N} \sum_{n} Z_n \exp\left(\frac{2 \pi i t n}{N}\right) $$ then an unbiased estimator of $t_0$ is $$\hat{t} = \underset{t}{\operatorname{argmax}} \left|f(t) \right| $$ and $a_0$ and $\phi_0$ can also be similarly estimated via $f(\hat{t}) = \hat{a} e^{i \hat{\phi}}$ . I have a hunch that this should be a maximum-likelihood estimator and should achieve the Cramér–Rao lower bound. To determine the Cramér–Rao lower bounds, we need to derive the likelihood function. Let, $$ p_n = a \cos\left(-2\pi t \frac{n}{N} + \phi\right) \\ q_n = a \sin\left(-2\pi t \frac{n}{N} + \phi\right)$$ With $Z_n = X_n + i Y_n$ , we have $$X_n = a_0 \cos\left(-2\pi t_0 \frac{n}{N} + \phi_0\right) + \mathcal{N}(0, N\sigma^2) \\ Y_n = a_0 \sin\left(-2\pi t_0 \frac{n}{N} + \phi_0\right) + \mathcal{N}(0, N\sigma^2)$$ Then, the likelihood function is $$ \mathcal{L}(\boldsymbol{Z}) = \left(\frac{1}{2 \pi N \sigma^2}\right)^N \exp\left[-\frac{1}{2 N \sigma^2} \sum_n\left((X_n - p_n)^2 + (Y_n - q_n)^2\right)\right]$$ Now, I must derive a $3 \times 3$ Fisher Information matrix for three unknown parameters, $a_0, t_0, \phi_0$ , using this likelihood function and invert it to get the lower bound on the variance of an unbiased estimator for the impulse's parameters. This is where I am stuck. I have no idea how to proceed in this case.","Given a noisy discrete-time complex signal that is the sum of an impulse at some time, , (with amplitude, ) and additive white Gaussian noise, what is the Cramér–Rao lower bound on the variance of an unbiased estimator of ? If I have a discrete-time signal of samples (let be even for simplicity), , as described above, if you took the discrete Fourier transform, you would get: where is the time of the impulse in the time-domain (and the parameter to be estimated), is some complex amplitude of this impulse, , and is the imaginary unit. Here I have assumed a sampling frequency of without loss of generality. The additive complex Gaussian noise, , is a complex random variable where both the real and imaginary parts follow a distribution each. The factor of in the variance of the additive noise accounts for the normalization factor in the inverse Discrete Fourier Transform, ensuring a constant noise variance in the time-domain. are real parameters that describe the impulse in the time-domain. Intuitively, it seems to me that if we take the discrete-time Fourier transform then an unbiased estimator of is and and can also be similarly estimated via . I have a hunch that this should be a maximum-likelihood estimator and should achieve the Cramér–Rao lower bound. To determine the Cramér–Rao lower bounds, we need to derive the likelihood function. Let, With , we have Then, the likelihood function is Now, I must derive a Fisher Information matrix for three unknown parameters, , using this likelihood function and invert it to get the lower bound on the variance of an unbiased estimator for the impulse's parameters. This is where I am stuck. I have no idea how to proceed in this case.","t_0 a_0 e^{i \phi_0} t_0, a_0, \phi_0 N N z_n  Z_n = a_0 \exp\left(\frac{-2\pi i t_0 n}{N} + i\phi_0\right) + \mathcal{CN}(0, 2 N \sigma^2) t_0 a_0 n = -\frac{N}{2}, ... \frac{N}{2} - 1 i 1 \mathcal{CN}(0, 2 N \sigma^2) \mathcal{N}(0, N \sigma^2) N a_0 > 0, t_0 \in [0, N], \phi_0 \in [-\pi, \pi)  f(t) = \frac{1}{N} \sum_{n} Z_n \exp\left(\frac{2 \pi i t n}{N}\right)  t_0 \hat{t} = \underset{t}{\operatorname{argmax}} \left|f(t) \right|  a_0 \phi_0 f(\hat{t}) = \hat{a} e^{i \hat{\phi}}  p_n = a \cos\left(-2\pi t \frac{n}{N} + \phi\right) \\
q_n = a \sin\left(-2\pi t \frac{n}{N} + \phi\right) Z_n = X_n + i Y_n X_n = a_0 \cos\left(-2\pi t_0 \frac{n}{N} + \phi_0\right) + \mathcal{N}(0, N\sigma^2) \\
Y_n = a_0 \sin\left(-2\pi t_0 \frac{n}{N} + \phi_0\right) + \mathcal{N}(0, N\sigma^2)  \mathcal{L}(\boldsymbol{Z}) = \left(\frac{1}{2 \pi N \sigma^2}\right)^N \exp\left[-\frac{1}{2 N \sigma^2} \sum_n\left((X_n - p_n)^2 + (Y_n - q_n)^2\right)\right] 3 \times 3 a_0, t_0, \phi_0","['statistics', 'signal-processing', 'estimation', 'log-likelihood', 'fisher-information']"
91,SIR model exact solution,SIR model exact solution,,"I try to understand how the exact solution for infectious from this article works, in order to prove that we cannot have ""two waves of infectious"" with fixed parameters. The exact solution for the infected is $ i(t)=\dfrac{\lambda}{\beta+\lambda \left(  \dfrac{\lambda-i_0\beta}{\lambda i_0 e^{\dfrac{\beta (s_0 + i_0 -1)}{\mu}}} )\right) \cdot e^{-\lambda t + \dfrac{\beta (s_0+i_0-1)}{\mu}} }$ I'm confused because it seems to me that $\beta,\lambda,\mu,i_0,s_0$ are fixed so the only term not fixed is $e^{-\lambda t +\dfrac{\beta (s_0+i_0-1)}{\mu}}$ according to $t$ . But I feel that am wrong because the graph of inctious is as a wave, not as an exponential function. My question is about where am I wrong and do you have any clue in order to prove that simple fixed SIR model infection has only one wave ? edit: typo in exponential, missed ""+""","I try to understand how the exact solution for infectious from this article works, in order to prove that we cannot have ""two waves of infectious"" with fixed parameters. The exact solution for the infected is I'm confused because it seems to me that are fixed so the only term not fixed is according to . But I feel that am wrong because the graph of inctious is as a wave, not as an exponential function. My question is about where am I wrong and do you have any clue in order to prove that simple fixed SIR model infection has only one wave ? edit: typo in exponential, missed ""+"""," i(t)=\dfrac{\lambda}{\beta+\lambda \left(  \dfrac{\lambda-i_0\beta}{\lambda i_0 e^{\dfrac{\beta (s_0 + i_0 -1)}{\mu}}} )\right) \cdot e^{-\lambda t + \dfrac{\beta (s_0+i_0-1)}{\mu}} } \beta,\lambda,\mu,i_0,s_0 e^{-\lambda t +\dfrac{\beta (s_0+i_0-1)}{\mu}} t","['statistics', 'multivariable-calculus', 'proof-writing']"
92,Percentage of contribution to the average,Percentage of contribution to the average,,I want to calculate how an individual Business Unit (BU) of a company contributed to the overall Average  Salary of the company. Average Salary is defined as the average (arithmetic mean) salary of the employees of the company. Suppose the BU-wise breakup of the salaries and the count of employees of the company is as follows: $$\begin{array}{|c|c|c|}  \hline \textbf{BU} & \textbf{Total Salary (in thousands)} & \textbf{No of Employees}& \textbf{Average Salary (in thousands)} \\ \hline \text{A} & 500 & 8 & 62.5 \\ \hline \text{B} & 700 & 10 &  70\\ \hline \text{C} & 630 & 7 &  90\\ \hline \end{array}$$ I can calculate the average salary of an employee for the company by : (500 + 700 + 630)/(8 + 10 +7) = 73.2 I want to calculate how much did each BU contribute to this average. Is there a way to calculate the individual contribution percentages of each BU?,I want to calculate how an individual Business Unit (BU) of a company contributed to the overall Average  Salary of the company. Average Salary is defined as the average (arithmetic mean) salary of the employees of the company. Suppose the BU-wise breakup of the salaries and the count of employees of the company is as follows: I can calculate the average salary of an employee for the company by : (500 + 700 + 630)/(8 + 10 +7) = 73.2 I want to calculate how much did each BU contribute to this average. Is there a way to calculate the individual contribution percentages of each BU?,"\begin{array}{|c|c|c|} 
\hline
\textbf{BU} & \textbf{Total Salary (in thousands)} & \textbf{No of Employees}& \textbf{Average Salary (in thousands)} \\ \hline
\text{A} & 500 & 8 & 62.5 \\ \hline
\text{B} & 700 & 10 &  70\\ \hline
\text{C} & 630 & 7 &  90\\ \hline
\end{array}","['algebra-precalculus', 'statistics', 'fractions', 'average', 'percentages']"
93,Show that $|\operatorname{median}(X) - \operatorname{mean}(x)| \leq \sigma_X$ [duplicate],Show that  [duplicate],|\operatorname{median}(X) - \operatorname{mean}(x)| \leq \sigma_X,"This question already has answers here : Distance between mean and median (2 answers) Closed 3 years ago . Let $m$ denote median and $\bar{x}$ denote mean and $\sigma$ denote the standard deviation, I want to show that $|m - \bar{x}| \leq \sigma$ . Since the LHS and RHS are both positive, we can prove $(m - \bar{x})^2 \leq \sigma^2$ instead. Here is what I've attempted with my first approach: \begin{align} & |m - \bar{x}|^2 = |\bar{x} - m|^2 \\ = {} & \left|\sum_i (\frac{1}{n}x_i) - m\right|^2 \\ = {} & \left|\sum_i (\frac{1}{n}x_i) - \frac{n}{n}m\right|^2 \\ = {} & \left|\frac{1}{n}\sum_i x_i - m\right|^2 \\ = {} & \left(\frac{1}{n}\sum_i x_i - m\right)^2 \end{align} Also, $$ \sigma^2 = \frac{1}{n}\sum_i (x_i - \bar{x})^2 $$ I don't see an easy way to show that this quantity is $\geq$ than the previous quantity. Is this in the right direction? The standard deviation and mean are related, but it's not clear to me how the median relates to either one. I'm not sure if this is relevant, but I also know that the minimizers for the following are the mean and median, respectively $$ \bar{x} = \arg \min_y \sum_i (x_i - y)^2 \\ m = \arg \min_y \sum_i \left| x_i - y \right| \\ $$","This question already has answers here : Distance between mean and median (2 answers) Closed 3 years ago . Let denote median and denote mean and denote the standard deviation, I want to show that . Since the LHS and RHS are both positive, we can prove instead. Here is what I've attempted with my first approach: Also, I don't see an easy way to show that this quantity is than the previous quantity. Is this in the right direction? The standard deviation and mean are related, but it's not clear to me how the median relates to either one. I'm not sure if this is relevant, but I also know that the minimizers for the following are the mean and median, respectively","m \bar{x} \sigma |m - \bar{x}| \leq \sigma (m - \bar{x})^2 \leq \sigma^2 \begin{align}
& |m - \bar{x}|^2 = |\bar{x} - m|^2 \\
= {} & \left|\sum_i (\frac{1}{n}x_i) - m\right|^2 \\
= {} & \left|\sum_i (\frac{1}{n}x_i) - \frac{n}{n}m\right|^2 \\
= {} & \left|\frac{1}{n}\sum_i x_i - m\right|^2 \\
= {} & \left(\frac{1}{n}\sum_i x_i - m\right)^2
\end{align} 
\sigma^2 = \frac{1}{n}\sum_i (x_i - \bar{x})^2
 \geq 
\bar{x} = \arg \min_y \sum_i (x_i - y)^2 \\
m = \arg \min_y \sum_i \left| x_i - y \right| \\
","['statistics', 'proof-writing', 'standard-deviation', 'means', 'median']"
94,Is There a Connection Between Minimum $ {L}_{1} $ Norm Solution and LASSO?,Is There a Connection Between Minimum  Norm Solution and LASSO?, {L}_{1} ,"I am reading a book about sparsity Statistical Learning with Sparsity: The Lasso and Generalizations . I want to know the relationship between the following two optimization problem: $$\min_{\beta} \| \beta\|_{1} \;  s.t. X\beta=y$$ and $$\arg \min_{\beta}  \frac{1}{2n}\|X\beta-y \|_{2}^{2}+\lambda \| \beta\|_{1}$$ where $X \in \mathbb{R}^{n\times d}$ , $y\in\mathbb{R}^{n}$ , $\beta \in\mathbb{R}^{d}$ , d>n and $\lambda > 0$ . Can we say that Lasso is the relaxed version of the minimum $L_{1}$ norm solution?","I am reading a book about sparsity Statistical Learning with Sparsity: The Lasso and Generalizations . I want to know the relationship between the following two optimization problem: and where , , , d>n and . Can we say that Lasso is the relaxed version of the minimum norm solution?",\min_{\beta} \| \beta\|_{1} \;  s.t. X\beta=y \arg \min_{\beta}  \frac{1}{2n}\|X\beta-y \|_{2}^{2}+\lambda \| \beta\|_{1} X \in \mathbb{R}^{n\times d} y\in\mathbb{R}^{n} \beta \in\mathbb{R}^{d} \lambda > 0 L_{1},"['statistics', 'optimization', 'convex-optimization', 'machine-learning', 'sparsity']"
95,Normal distribution sample,Normal distribution sample,,"Since I'am beginner in statistics I'm stuck in simple exercise so will appreciate any help. I have mean, standard deviation and probability p(x) and need to get x. Here is the Exercise The patient recovery time from a particular surgical procedure is normally distributed with a mean of 5.3 days and a standard deviation of 2.1 days The 90th percentile for recovery times is? I know that it's possible to get x from probability formula but I was wondering if there is easier way to get it.","Since I'am beginner in statistics I'm stuck in simple exercise so will appreciate any help. I have mean, standard deviation and probability p(x) and need to get x. Here is the Exercise The patient recovery time from a particular surgical procedure is normally distributed with a mean of 5.3 days and a standard deviation of 2.1 days The 90th percentile for recovery times is? I know that it's possible to get x from probability formula but I was wondering if there is easier way to get it.",,"['statistics', 'probability-distributions', 'normal-distribution', 'gaussian-integral']"
96,How to Make Meaningful Conclusions?,How to Make Meaningful Conclusions?,,"I recently appeared for an Interview for my college and I was asked the following question. The Interviewer said that this question was a Data Science question. He asked the same question to a friend of mine as well. The question- Suppose 7.5% of the population has a certain Bone Disease. During COVID pandemic you go to a hospital and see the records. 25% of the COVID Infected patients also had the Bone Disease. Can we say for sure if the Bone Disease is a symptom of COVID-19? My Reponse- I said No, and explained it as it's not necessary that COVID-19 is causing these symptoms, it could very well be possible that the 7.5% of the country's population which already had the disease is more susceptible to the virus due to lowered immunity. Hence making conclusions is not possible. Then the interviewer asked me How can we be sure if it is a symptom or not? I replied saying we can go to more Hospitals, collect more data and see if it correlates everywhere. The Interviewer then said If we have the same results everywhere will you conclude it's a symptom? I had no good answer but I replied that Just correlation of data is not sufficient, we also need to check if the people who have COVID-19 had the bone disease prior to getting infected or not. See if that percentage also correlates and stuff. Here he stopped questioning however I couldn't judge If I was right or wrong. I am in Grade-12 so I have no experience in Data Science as such. I do know a fair bit of statistics however I have never solved such questions. Can someone provide me insights on how to solve such questions and make meaningful conclusions? I have asked the same question on Data Science SE however i noticed the other questions there were quite different so I wasn't sure if this question is appropriate there. If there are any better SE suggestions do comment them.","I recently appeared for an Interview for my college and I was asked the following question. The Interviewer said that this question was a Data Science question. He asked the same question to a friend of mine as well. The question- Suppose 7.5% of the population has a certain Bone Disease. During COVID pandemic you go to a hospital and see the records. 25% of the COVID Infected patients also had the Bone Disease. Can we say for sure if the Bone Disease is a symptom of COVID-19? My Reponse- I said No, and explained it as it's not necessary that COVID-19 is causing these symptoms, it could very well be possible that the 7.5% of the country's population which already had the disease is more susceptible to the virus due to lowered immunity. Hence making conclusions is not possible. Then the interviewer asked me How can we be sure if it is a symptom or not? I replied saying we can go to more Hospitals, collect more data and see if it correlates everywhere. The Interviewer then said If we have the same results everywhere will you conclude it's a symptom? I had no good answer but I replied that Just correlation of data is not sufficient, we also need to check if the people who have COVID-19 had the bone disease prior to getting infected or not. See if that percentage also correlates and stuff. Here he stopped questioning however I couldn't judge If I was right or wrong. I am in Grade-12 so I have no experience in Data Science as such. I do know a fair bit of statistics however I have never solved such questions. Can someone provide me insights on how to solve such questions and make meaningful conclusions? I have asked the same question on Data Science SE however i noticed the other questions there were quite different so I wasn't sure if this question is appropriate there. If there are any better SE suggestions do comment them.",,"['statistics', 'soft-question', 'data-analysis']"
97,"Profesor used this property in a statistics class, first that I've seen this [duplicate]","Profesor used this property in a statistics class, first that I've seen this [duplicate]",,"This question already has answers here : Simplify $\sum (x_i- \mu)^2$ (2 answers) Closed 3 years ago . $$ \sum_{i=1}^{n}\left(X_{i}-\mu\right)^{2}=\sum_{i=1}^{n}\left(X_{i}-\bar{X}\right)^{2}+n(\bar{X}-\mu)^{2} $$ My professor didn't prove this, but said it is easy to. I am confused because I can't seem to get it. Would appreciate it if someone could help. I am new to statistics, please bear with me if I make stupid statements.","This question already has answers here : Simplify $\sum (x_i- \mu)^2$ (2 answers) Closed 3 years ago . My professor didn't prove this, but said it is easy to. I am confused because I can't seem to get it. Would appreciate it if someone could help. I am new to statistics, please bear with me if I make stupid statements.","
\sum_{i=1}^{n}\left(X_{i}-\mu\right)^{2}=\sum_{i=1}^{n}\left(X_{i}-\bar{X}\right)^{2}+n(\bar{X}-\mu)^{2}
",['statistics']
98,Another extension of mutual information to multiple variables,Another extension of mutual information to multiple variables,,"The mutual information can be expressed as $$ I(X;Y) = H(X) + H(Y) - H(X, Y) $$ And now I encounter the following expression, which seems to be an extension of mutual information: $$ F(X_1;\cdots;X_N) = \sum_{i=1}^N H(X_i) - H(X_1, X_2, \cdots, X_N) $$ However, I know the definition of multivariate mutual information, where $$ I(X_1;\cdots;X_N) = -\sum _{T\subseteq \{X_1,\ldots ,X_N\}}(-1)^{|T|}H(T) $$ I have two questions, Why $I$ is the commonly used extension of mutual information, rather than $F$ ? From my view, $F$ is more like an ""information gain"" because $F$ is non-negative.  And $I$ can be positive and negative, confusing me what it stands for. Is there any interpretation of $F$ ? Or any studies about properties? I didn't have much knowledge about information theory, and I appreciate any kindly help.","The mutual information can be expressed as And now I encounter the following expression, which seems to be an extension of mutual information: However, I know the definition of multivariate mutual information, where I have two questions, Why is the commonly used extension of mutual information, rather than ? From my view, is more like an ""information gain"" because is non-negative.  And can be positive and negative, confusing me what it stands for. Is there any interpretation of ? Or any studies about properties? I didn't have much knowledge about information theory, and I appreciate any kindly help.","
I(X;Y) = H(X) + H(Y) - H(X, Y)
 
F(X_1;\cdots;X_N) = \sum_{i=1}^N H(X_i) - H(X_1, X_2, \cdots, X_N)
 
I(X_1;\cdots;X_N) = -\sum _{T\subseteq \{X_1,\ldots ,X_N\}}(-1)^{|T|}H(T)
 I F F F I F","['probability', 'statistics', 'information-theory', 'entropy']"
99,MSE for MLE of normal distribution's ${\sigma}^2$,MSE for MLE of normal distribution's,{\sigma}^2,"So I've known $MLE$ for ${\sigma}^2$ is $\hat{{\sigma}^2}=\frac{1}{n}\sum_{i=1}^{n} (X_{i} -\bar{X})^2$ , and I'm looking for $MSE$ of $\hat{{\sigma}^2}$ . But I'm having trouble to get the result. What I tried goes like below: By definition, $MSE$ = $E[(\hat{{\sigma}^2}$ - ${\sigma}^2$ ) $^2$ ], which is = $Var(\hat{{\sigma}^2} - {\sigma}^2)+(E(\hat{{\sigma}^2} - {\sigma}^2))^2$ = $Var(\hat{{\sigma}^2})-Var({{\sigma}^2})+(E(\hat{{\sigma}^2} - {\sigma}^2))^2$ . From here, I tried to find $Var(\hat{{\sigma}^2})$ , which is = $Var(\frac{1}{n}\sum_{i=1}^{n} (X_{i} -\bar{X})^2$ ) = $\frac{1}{n^2}Var(\sum_{i=1}^{n} X_{i}^2 -n\bar{X}^2)$ = $\frac{1}{n^2}(\sum_{i=1}^{n} Var (X_{i}^2) -n^2Var(\bar{X}^2))$ But I'm not sure how to get $Var (X_{i}^2)$ and $Var(\bar{X}^2)$ . I tried $Var (X_{i}^2)$ = $E(X_i^4) - (E(X_i^2))^2$ , But I'm not quite sure what $E(X_i^4)$ would be. Could anyone help me with this? Am I on the correct path to solve this? Thanks in advance!","So I've known for is , and I'm looking for of . But I'm having trouble to get the result. What I tried goes like below: By definition, = - ) ], which is = = . From here, I tried to find , which is = ) = = But I'm not sure how to get and . I tried = , But I'm not quite sure what would be. Could anyone help me with this? Am I on the correct path to solve this? Thanks in advance!",MLE {\sigma}^2 \hat{{\sigma}^2}=\frac{1}{n}\sum_{i=1}^{n} (X_{i} -\bar{X})^2 MSE \hat{{\sigma}^2} MSE E[(\hat{{\sigma}^2} {\sigma}^2 ^2 Var(\hat{{\sigma}^2} - {\sigma}^2)+(E(\hat{{\sigma}^2} - {\sigma}^2))^2 Var(\hat{{\sigma}^2})-Var({{\sigma}^2})+(E(\hat{{\sigma}^2} - {\sigma}^2))^2 Var(\hat{{\sigma}^2}) Var(\frac{1}{n}\sum_{i=1}^{n} (X_{i} -\bar{X})^2 \frac{1}{n^2}Var(\sum_{i=1}^{n} X_{i}^2 -n\bar{X}^2) \frac{1}{n^2}(\sum_{i=1}^{n} Var (X_{i}^2) -n^2Var(\bar{X}^2)) Var (X_{i}^2) Var(\bar{X}^2) Var (X_{i}^2) E(X_i^4) - (E(X_i^2))^2 E(X_i^4),"['statistics', 'maximum-likelihood']"
