,title_raw,title_text,title_latex,body_raw,body_text,body_latex,tags
0,A Friend of a Friend: Transitivity in Large Random Graphs,A Friend of a Friend: Transitivity in Large Random Graphs,,"Suppose we have a network $G$ of $n$ people, who share $M$ connections. Define the transitivity index $\tau(G)$ as the count of all possible threesomes having $3$ connections divided by the count of all possible threesomes having $2$ or $3$ connections (provided that these counts are not both zero, otherwise $\tau(G)=1$). Suppose that the connections are distributed uniformly randomly (conditional on there being $M$ such connections). More precisely: Let $N\equiv\binom{n}{2}$ and let the model $\mathcal G(n,M)$ consist of all $\binom{N}{M}$ graphs, having vertices $\{1,2,\ldots,\ n\}$ and $M$ edges, in which every such graph is realised with probability $\binom{N}{M}^{-1}$. Define the transitivity distribution implicitly by $G\sim\mathcal G(n,M)\Rightarrow\tau(G)\sim\mathrm{TR}(n,M)$. In $\mathcal G(n,M)$, what can be said about the transitivity distribution $\mathrm{TR}(n,M)$ for large $n$ and $M$? Edit I am particularly interested in quantiles of $\mathrm{TR}(n,M)$. For example, for $(n,M)=(100,150)$, the order statistic $\tau_{4750:5000}$ was about $0.054$ in simulation. Enumeration is not an option. The number of automorphism classes containing graphs with vertex set $\{1,2,\ldots,n\}$ and size $M$ is for moderate $(n,M)=(50,75)$ at least $5\cdot10^{56}$. But, of course, I am hoping that an analytical approximation is available, even if it only works well beyond say $(n,M)=(10^5,10^6)$. Note The fabulously useful answer provided by joriki may not work for $M<{{n}\over{2}}$, since the denominator then has mass at $0$, which is contra the assumption in Approximations for Mean and Variance of a Ratio . However, these cases are not at all as interesting with regards to (the) applications (I have in mind).","Suppose we have a network $G$ of $n$ people, who share $M$ connections. Define the transitivity index $\tau(G)$ as the count of all possible threesomes having $3$ connections divided by the count of all possible threesomes having $2$ or $3$ connections (provided that these counts are not both zero, otherwise $\tau(G)=1$). Suppose that the connections are distributed uniformly randomly (conditional on there being $M$ such connections). More precisely: Let $N\equiv\binom{n}{2}$ and let the model $\mathcal G(n,M)$ consist of all $\binom{N}{M}$ graphs, having vertices $\{1,2,\ldots,\ n\}$ and $M$ edges, in which every such graph is realised with probability $\binom{N}{M}^{-1}$. Define the transitivity distribution implicitly by $G\sim\mathcal G(n,M)\Rightarrow\tau(G)\sim\mathrm{TR}(n,M)$. In $\mathcal G(n,M)$, what can be said about the transitivity distribution $\mathrm{TR}(n,M)$ for large $n$ and $M$? Edit I am particularly interested in quantiles of $\mathrm{TR}(n,M)$. For example, for $(n,M)=(100,150)$, the order statistic $\tau_{4750:5000}$ was about $0.054$ in simulation. Enumeration is not an option. The number of automorphism classes containing graphs with vertex set $\{1,2,\ldots,n\}$ and size $M$ is for moderate $(n,M)=(50,75)$ at least $5\cdot10^{56}$. But, of course, I am hoping that an analytical approximation is available, even if it only works well beyond say $(n,M)=(10^5,10^6)$. Note The fabulously useful answer provided by joriki may not work for $M<{{n}\over{2}}$, since the denominator then has mass at $0$, which is contra the assumption in Approximations for Mean and Variance of a Ratio . However, these cases are not at all as interesting with regards to (the) applications (I have in mind).",,"['probability', 'combinatorics', 'graph-theory']"
1,Expected Value of random variable raised to another random variable,Expected Value of random variable raised to another random variable,,"If $X$ and $Y$ are continuous random variables uniformly distributed over $[0,1]$, find $E(X^Y)$. My first thought was that $E(X^Y) = E(X)^{E(Y)}$, but through simulation I found that this was not the case. I then tried using a double integral from $0$ to $1$ with respect to $x$ and $y$, but doing so results in a $1/\log$ term which cannot be evaluated at $0$ or $1$. I'm not sure what to try next","If $X$ and $Y$ are continuous random variables uniformly distributed over $[0,1]$, find $E(X^Y)$. My first thought was that $E(X^Y) = E(X)^{E(Y)}$, but through simulation I found that this was not the case. I then tried using a double integral from $0$ to $1$ with respect to $x$ and $y$, but doing so results in a $1/\log$ term which cannot be evaluated at $0$ or $1$. I'm not sure what to try next",,['probability']
2,Intuitive explanations for Gaussian distribution function and mahalanobis distance,Intuitive explanations for Gaussian distribution function and mahalanobis distance,,"I was wondering If anyone could give intuitive explanations for the multivariate Gaussian distribution function and mahalanobis distance? My professor didn't explain these in probability class, they were only defined... Where did the formula come from? Why is the Gaussian function the way it is? Is there a way to intuitively explain mahalanobis distance? Thank you for any support!!","I was wondering If anyone could give intuitive explanations for the multivariate Gaussian distribution function and mahalanobis distance? My professor didn't explain these in probability class, they were only defined... Where did the formula come from? Why is the Gaussian function the way it is? Is there a way to intuitively explain mahalanobis distance? Thank you for any support!!",,['probability']
3,Rephrasing a Convergence Result to make use of the Borel-Cantelli Lemma,Rephrasing a Convergence Result to make use of the Borel-Cantelli Lemma,,"Let $X_n$ be a sequence of non-negative iid random variables. Is it true that the condition, $$\limsup_{n\rightarrow\infty} \frac{X_n}{n} = \infty \text{ almost surely}$$ is equivalent to the condition, $$\mathbb{P} \Bigg(  \limsup_{n\rightarrow\infty} \Big\{ {\frac{X_n}{n} \geq x} \Big\} \Bigg) = 1 \ \text{ for all } \ x > 0$$ I am wondering because I would like to rephrase the initial condition so as to make use of the Borel Cantelli lemma.","Let $X_n$ be a sequence of non-negative iid random variables. Is it true that the condition, $$\limsup_{n\rightarrow\infty} \frac{X_n}{n} = \infty \text{ almost surely}$$ is equivalent to the condition, $$\mathbb{P} \Bigg(  \limsup_{n\rightarrow\infty} \Big\{ {\frac{X_n}{n} \geq x} \Big\} \Bigg) = 1 \ \text{ for all } \ x > 0$$ I am wondering because I would like to rephrase the initial condition so as to make use of the Borel Cantelli lemma.",,"['probability', 'measure-theory', 'probability-theory']"
4,Combinatorics: likelihood of a uniform draw,Combinatorics: likelihood of a uniform draw,,"An urn contains 10 kinds of pebbles, and 100 pebbles of each kind. We draw 100 pebbles (without replacement). What is the probability that we get between 8 and 12 pebbles of each kind?","An urn contains 10 kinds of pebbles, and 100 pebbles of each kind. We draw 100 pebbles (without replacement). What is the probability that we get between 8 and 12 pebbles of each kind?",,"['probability', 'combinatorics']"
5,Clarification: Viewing $\mathbb{R}^n$ as a probabilistic state space,Clarification: Viewing  as a probabilistic state space,\mathbb{R}^n,"In this MathOverflow post on visualizing high-dimensional spaces, Terry Tao states that ""the fact that most of the mass of a unit ball in high dimensions lurks near the boundary of the ball can be interpreted as a manifestation of the law of large numbers, using the interpretation of a high-dimensional vector space as the state space for a large number of trials of a random variable."" What exactly does he mean? -- The only interpretation I've come up with is the following: Observing n trials of a random variable (for instance $X \sim U(0,1)$) is equivalent to picking a random point inside an n -dimensional unit cube. For large n , most of the volume of the n -dimensional unit cube is near the edges. Thus our point is likely to be near an edge of the cube, which means one of our trials has a large value. This shows some law of rare events: ""given enough trials, a rare event will occur"". However, this interpretation must use an n -cube rather than an n -sphere because the trials are independent. Additionally, our conclusion is not the law of large numbers, which states that the average of our trials will converge to the expected value for large n . Can someone provide a correct interpretation of Tao's comment? Thanks!","In this MathOverflow post on visualizing high-dimensional spaces, Terry Tao states that ""the fact that most of the mass of a unit ball in high dimensions lurks near the boundary of the ball can be interpreted as a manifestation of the law of large numbers, using the interpretation of a high-dimensional vector space as the state space for a large number of trials of a random variable."" What exactly does he mean? -- The only interpretation I've come up with is the following: Observing n trials of a random variable (for instance $X \sim U(0,1)$) is equivalent to picking a random point inside an n -dimensional unit cube. For large n , most of the volume of the n -dimensional unit cube is near the edges. Thus our point is likely to be near an edge of the cube, which means one of our trials has a large value. This shows some law of rare events: ""given enough trials, a rare event will occur"". However, this interpretation must use an n -cube rather than an n -sphere because the trials are independent. Additionally, our conclusion is not the law of large numbers, which states that the average of our trials will converge to the expected value for large n . Can someone provide a correct interpretation of Tao's comment? Thanks!",,"['probability', 'vector-spaces']"
6,Strategy in an urn problem with two urns,Strategy in an urn problem with two urns,,"We consider two urns each containing $N$ balls. One of the urns has white balls only. The other has $k$ black balls and thus $N-k$ white balls. Both $N$ and $k$ are known, and $k>0$. What would be the best ball extraction strategy (without returning an extracted ball to any of the urns) if one wants to find out, which is the urn with the black balls? Best strategy is understood in the sense of reducing the expected number of extracted balls.","We consider two urns each containing $N$ balls. One of the urns has white balls only. The other has $k$ black balls and thus $N-k$ white balls. Both $N$ and $k$ are known, and $k>0$. What would be the best ball extraction strategy (without returning an extracted ball to any of the urns) if one wants to find out, which is the urn with the black balls? Best strategy is understood in the sense of reducing the expected number of extracted balls.",,"['probability', 'combinatorics']"
7,Probability of being late,Probability of being late,,"I would like help with correcting my solution for the following problem: Adam, Bob and Clare have made an appointment at 5 PM. Adam is never late. The probability that none of them is late is 0.4. If at least one of them is late, the probability Clare will be amongst those who are late is 0.6. If it is known Clare will be late, the probability she will be the only one who is late is 5/6. The questions are: a. What is the probability only Bob will be late? b. If it is known exactly two people will arrive on time, what is the probability Clare is the one who is late? Attempted solution I marked $A$ to mean Adam is late, $B$ to mean Bob is late and $C$ to mean Clare. I got the following information from the question: $P(A)=0; P(A^c\cap B^c\cap C^c)=0.4; P(C|A\cup B\cup C)=0.6; P(B^c\cap A^c|C)=5/6$ And also: $P(A^c)=1; P(A^\cup B\cup C)=1-P(A^c\cap B^c\cap C^c)=0.6$ $P(C)=P(C|A\cup B\cup C)*P(A\cup B\cup C)=0.36$ $P(B^c\cap C)=P(B^c|C)P(C)=P(B^c\cap A^c|C)P(C)=5/6*0.36=0.3$ $P(B^c)=P(B^c\cap C)+P(B^c\cap C^c)=0.3+0.4=0.7$ Using this I calculated a. like this: a. $P(A^c\cap B\cap C^c)=P(B\cap C^c)=1-P(B^c\cup C)=1-(P(B^c)+P(C)-P(B^c\cap C))=1-(0.7+0.36-0.3)=0.24$ And b. like this: b. Adam is never late, so he'll always arrive on time and be one of the group which isn't late, so the probability is: $P(C|(A^c\cap B^c)\cup (A^c\cap C^c))=P(C|B^c\cup C^c)=P(C\cap (B^c\cup C^c))/P(B^c\cup C^c)=$ $P(C\cap B^c)/P(B^c\cup C^c)=0.3/(0.7+(1-0.36)-0.4)$ (The last step uses the inclusion-exclusion principle on $P(B^c\cup C^c)$ ) I know from my lecturer that at least the answer on b. is incorrect (I'm not sure about a.). But I just can't find my error(s)! I'd really appreciate if someone could point out where I went wrong. Thanks!","I would like help with correcting my solution for the following problem: Adam, Bob and Clare have made an appointment at 5 PM. Adam is never late. The probability that none of them is late is 0.4. If at least one of them is late, the probability Clare will be amongst those who are late is 0.6. If it is known Clare will be late, the probability she will be the only one who is late is 5/6. The questions are: a. What is the probability only Bob will be late? b. If it is known exactly two people will arrive on time, what is the probability Clare is the one who is late? Attempted solution I marked to mean Adam is late, to mean Bob is late and to mean Clare. I got the following information from the question: And also: Using this I calculated a. like this: a. And b. like this: b. Adam is never late, so he'll always arrive on time and be one of the group which isn't late, so the probability is: (The last step uses the inclusion-exclusion principle on ) I know from my lecturer that at least the answer on b. is incorrect (I'm not sure about a.). But I just can't find my error(s)! I'd really appreciate if someone could point out where I went wrong. Thanks!",A B C P(A)=0; P(A^c\cap B^c\cap C^c)=0.4; P(C|A\cup B\cup C)=0.6; P(B^c\cap A^c|C)=5/6 P(A^c)=1; P(A^\cup B\cup C)=1-P(A^c\cap B^c\cap C^c)=0.6 P(C)=P(C|A\cup B\cup C)*P(A\cup B\cup C)=0.36 P(B^c\cap C)=P(B^c|C)P(C)=P(B^c\cap A^c|C)P(C)=5/6*0.36=0.3 P(B^c)=P(B^c\cap C)+P(B^c\cap C^c)=0.3+0.4=0.7 P(A^c\cap B\cap C^c)=P(B\cap C^c)=1-P(B^c\cup C)=1-(P(B^c)+P(C)-P(B^c\cap C))=1-(0.7+0.36-0.3)=0.24 P(C|(A^c\cap B^c)\cup (A^c\cap C^c))=P(C|B^c\cup C^c)=P(C\cap (B^c\cup C^c))/P(B^c\cup C^c)= P(C\cap B^c)/P(B^c\cup C^c)=0.3/(0.7+(1-0.36)-0.4) P(B^c\cup C^c),['probability']
8,A Inequality between Bhattacharyya distance and KL divergence,A Inequality between Bhattacharyya distance and KL divergence,,"I was trying to solve the following problem. But I dont know how to proceed. I would be really grateful if anybody would point me in the right direction. Let $P = (p_1,p_2, \cdots, p_n)$ be a probability vector (That is $\sum p_i = 1$ and $p_i  \geq 0$ ). Let $Q = (q_1,q_2, \cdots, q_n)$ be a permutation of the vector $P$ . If $I = \frac14 \left(\sum p_i \log \frac{p_i}{q_i}\right) + \frac14 \left(\sum q_i \log \frac{q_i}{p_i}\right)$ (involves the Kullback - Leibler divergence ) And $Z = \sum \sqrt{p_iq_i}$ (called the Bhattacharyya distance ) Prove that $I^2 + Z^2 \leq 1$ Thank you",I was trying to solve the following problem. But I dont know how to proceed. I would be really grateful if anybody would point me in the right direction. Let be a probability vector (That is and ). Let be a permutation of the vector . If (involves the Kullback - Leibler divergence ) And (called the Bhattacharyya distance ) Prove that Thank you,"P = (p_1,p_2, \cdots, p_n) \sum p_i = 1 p_i  \geq 0 Q = (q_1,q_2, \cdots, q_n) P I = \frac14 \left(\sum p_i \log \frac{p_i}{q_i}\right) + \frac14 \left(\sum q_i \log \frac{q_i}{p_i}\right) Z = \sum \sqrt{p_iq_i} I^2 + Z^2 \leq 1","['probability', 'inequality']"
9,Probability of picking all elements in a set,Probability of picking all elements in a set,,"I was studying the birthday paradox and got curious about a related, but slightly different problem. Lets say I have a set S, that has n unique elements. If I randomly pick k elements from the set (assuming equal probability of picking any element), the probability that I'll have picked all the elements in the set, when k=n is n!/n^n. Now, what would be the value of k for probability of picking all elements in the set to be > 0.9 (or some constant). A simpler variation would be - how many times should I flip a coin to have a probability > 0.9 to have thrown heads and tails at least once.","I was studying the birthday paradox and got curious about a related, but slightly different problem. Lets say I have a set S, that has n unique elements. If I randomly pick k elements from the set (assuming equal probability of picking any element), the probability that I'll have picked all the elements in the set, when k=n is n!/n^n. Now, what would be the value of k for probability of picking all elements in the set to be > 0.9 (or some constant). A simpler variation would be - how many times should I flip a coin to have a probability > 0.9 to have thrown heads and tails at least once.",,"['probability', 'combinatorics']"
10,Probability that two elements commute in a noncommutative simple finite group,Probability that two elements commute in a noncommutative simple finite group,,"Good afternoon ! Let $G$ a finite non-abelian group. Let $p_G$ the probability that two elements randomly chosen commute. It is well known that : $$p_G \leqslant \frac{5}{8}$$ The upper bound is optimal (if we consider for example $\mathbb{D_4}$ the dihedral group of the square or the quaternion group $\mathbb{H_8}$ ), and there is no interesting lower bound because of the symmetric group $S_n$ for example. We can even show with Burnside's lemma that : $$p_G = \frac{k_G}{|G|}$$ With $k_G$ the number of conjugacy classes in $G$ . I've seen that if moreover we suppose $G$ simple (i.e $G$ has only trivial normal subgroups), we have : $$p_G \leqslant \frac{1}{12}$$ I would love to prove it. What I tried so far : It is sufficient to show that if $k_G \geqslant \frac{|G|}{12}$ , then $k_G = \frac{|G|}{12} = 5$ . There are $k_G$ irreducible representations of degree $d_1, d_2, \dots, d_{k_G}$ , with $d_1 = 1$ (trivial representation), $d_1 \leqslant \dots \leqslant d_{k_G}$ and $d_1^2 + \dots + d_{k_G}^2 = |G|$ . $G$ is simple so the only irreducible representation of degree 1 is the trivial representation. We can show that : $$d_2 = 3$$ Indeed G is simple so it has no irreducible representations of degree 2. Furthermore if $d_2 \geqslant 4$ then $|G| \geqslant 1 + 16(k_G - 1)$ , so by the inequality $k_G \geqslant \frac{|G|}{12}$ we have $|G| \leqslant 15$ , which is impossible because the smallest simple non-abelian subgroup is $A_5$ , and $|A_5| = 60 > 15$ . From now I don't know how to conclude. We have : $$12k_G \geqslant |G|= 1 + 3^2 + \dots + d_{k_G}^2$$ And I don't know what to say. Any help ? :(","Good afternoon ! Let a finite non-abelian group. Let the probability that two elements randomly chosen commute. It is well known that : The upper bound is optimal (if we consider for example the dihedral group of the square or the quaternion group ), and there is no interesting lower bound because of the symmetric group for example. We can even show with Burnside's lemma that : With the number of conjugacy classes in . I've seen that if moreover we suppose simple (i.e has only trivial normal subgroups), we have : I would love to prove it. What I tried so far : It is sufficient to show that if , then . There are irreducible representations of degree , with (trivial representation), and . is simple so the only irreducible representation of degree 1 is the trivial representation. We can show that : Indeed G is simple so it has no irreducible representations of degree 2. Furthermore if then , so by the inequality we have , which is impossible because the smallest simple non-abelian subgroup is , and . From now I don't know how to conclude. We have : And I don't know what to say. Any help ? :(","G p_G p_G \leqslant \frac{5}{8} \mathbb{D_4} \mathbb{H_8} S_n p_G = \frac{k_G}{|G|} k_G G G G p_G \leqslant \frac{1}{12} k_G \geqslant \frac{|G|}{12} k_G = \frac{|G|}{12} = 5 k_G d_1, d_2, \dots, d_{k_G} d_1 = 1 d_1 \leqslant \dots \leqslant d_{k_G} d_1^2 + \dots + d_{k_G}^2 = |G| G d_2 = 3 d_2 \geqslant 4 |G| \geqslant 1 + 16(k_G - 1) k_G \geqslant \frac{|G|}{12} |G| \leqslant 15 A_5 |A_5| = 60 > 15 12k_G \geqslant |G|= 1 + 3^2 + \dots + d_{k_G}^2","['probability', 'group-theory', 'finite-groups', 'simple-groups']"
11,"Coin flip puzzle, prove $P(X>Y) > P(Y>X)$","Coin flip puzzle, prove",P(X>Y) > P(Y>X),"I recently encountered the following riddle: Let a fair coin be flipped $N > 2$ times. Player A gets a point every time there are two heads in a row, and player B gets a point every time a tail is followed by a head, the player with more points wins. Thus, for example, for the sequence THHH , player A wins with 2 vs. 1 points while in the sequence THHTHH , neither wins, as they both score two points. Clearly, the expected number of points is equal, as the sequences HH and TH are both equally likely. Now the riddle itself: Are player A's chances of winning higher, lower or equal to that of player B? I found it surprising that player A's chances of winning are lower than that of player B's. Though there are intuitive explanations, I'd like to rigorously prove this.","I recently encountered the following riddle: Let a fair coin be flipped times. Player A gets a point every time there are two heads in a row, and player B gets a point every time a tail is followed by a head, the player with more points wins. Thus, for example, for the sequence THHH , player A wins with 2 vs. 1 points while in the sequence THHTHH , neither wins, as they both score two points. Clearly, the expected number of points is equal, as the sequences HH and TH are both equally likely. Now the riddle itself: Are player A's chances of winning higher, lower or equal to that of player B? I found it surprising that player A's chances of winning are lower than that of player B's. Though there are intuitive explanations, I'd like to rigorously prove this.",N > 2,"['probability', 'probability-distributions', 'puzzle']"
12,Insurance company with claims following a Poisson Process. Calculate the probability that the capital is always positive throughout the first 4 days.,Insurance company with claims following a Poisson Process. Calculate the probability that the capital is always positive throughout the first 4 days.,,"Suppose that claims are made to an insurance company according to a Poisson process with rate $10$ per day. The amount of a claim is a random variable that has an exponential distribution with mean $1,000$ dollars. The insurance company receives payments continuously in time at a constant rate of $11,000$ dollars per day. Starting with zero initial capital, find the probability that the firm’s capital is always positive throughout its first $4$ days. (Book: Simulation by Sheldon Ross, Chapter 7 Problem 11) I initially simulated this to approximate the answer as around $13.5$ % but I want to find an exact form. I note that the distribution of the total amount $x$ claimed after $d$ days can be represented with a Gamma( $10d,1000$ ) distribution: $$F(x) = \int_0^x \frac{1}{(1000)^{10d}\Gamma(10d)}t^{10d-1}e^{-\frac{t}{1000}}dt$$ For example, since the insurance company receives $11,000$ dollars after $1$ day, $F(11000)\approx 0.6595$ , which means that after $1$ day, the probability of having a negative capital is $1-0.6595 = 0.3405$ . However, this doesn't account for the firm once having a negative capital within the day, even if at the end of the day, the capital is positive. I'm having trouble finding how to deal with this part. My thinking is to find $$\epsilon \cdot P(\text{positive capital on days } [0,\epsilon]) + \epsilon \cdot P(\text{positive capital on } [\epsilon, 2\epsilon] \text{ given positive capital on } [0,\epsilon]) + \epsilon \cdot P(\text{positive capital on } [2\epsilon, 3\epsilon] \text{ given positive capital on } [0,2\epsilon]) + ...$$ but I run into a dead end here since finding $P(\text{positive capital on } [\epsilon, 2\epsilon])$ would require knowing the capital I start with after $\epsilon$ days.","Suppose that claims are made to an insurance company according to a Poisson process with rate per day. The amount of a claim is a random variable that has an exponential distribution with mean dollars. The insurance company receives payments continuously in time at a constant rate of dollars per day. Starting with zero initial capital, find the probability that the firm’s capital is always positive throughout its first days. (Book: Simulation by Sheldon Ross, Chapter 7 Problem 11) I initially simulated this to approximate the answer as around % but I want to find an exact form. I note that the distribution of the total amount claimed after days can be represented with a Gamma( ) distribution: For example, since the insurance company receives dollars after day, , which means that after day, the probability of having a negative capital is . However, this doesn't account for the firm once having a negative capital within the day, even if at the end of the day, the capital is positive. I'm having trouble finding how to deal with this part. My thinking is to find but I run into a dead end here since finding would require knowing the capital I start with after days.","10 1,000 11,000 4 13.5 x d 10d,1000 F(x) = \int_0^x \frac{1}{(1000)^{10d}\Gamma(10d)}t^{10d-1}e^{-\frac{t}{1000}}dt 11,000 1 F(11000)\approx 0.6595 1 1-0.6595 = 0.3405 \epsilon \cdot P(\text{positive capital on days } [0,\epsilon]) + \epsilon \cdot P(\text{positive capital on } [\epsilon, 2\epsilon] \text{ given positive capital on } [0,\epsilon]) + \epsilon \cdot P(\text{positive capital on } [2\epsilon, 3\epsilon] \text{ given positive capital on } [0,2\epsilon]) + ... P(\text{positive capital on } [\epsilon, 2\epsilon]) \epsilon","['probability', 'gamma-function', 'poisson-distribution', 'poisson-process', 'gamma-distribution']"
13,Proof of Kolmogorov's $0$-$1$ Law in Shiryaev,Proof of Kolmogorov's - Law in Shiryaev,0 1,"I know this theorem has been discussed in a lot of posts on this site however I was unable to find an answer that deals with the particular proof I am trying to understand. I'm interested in the proof of Kolmogorov's $0$ - $1$ law given in the book "" Probability by Albert Shiryaev "". The statement is as follows, Theorem: Let $\xi_1,\xi_2,\ldots$ be a sequence of independent random variables and let $A \in \cap_{n=1}^\infty \mathcal{F}_n^{\infty}$ , i.e an element of the tail $\sigma$ -algebra. Then $\mathbb{P}(A)=0$ or $1$ . We define $\mathcal{F}_n^k = \sigma(\xi_n,\xi_{n+1},\ldots,\xi_{k})$ with the convention $\mathcal{F}_n^{\infty}=\sigma(\xi_n,\xi_{n+1},\ldots)$ . Proof: Clearly we have $A \in \mathcal{F}_1^\infty=\sigma(\xi_1,\xi_{2},\ldots)=\sigma(\cup_n \mathcal{F}_1^n)$ . We can find sets $A_n \in \mathcal{F}_1^n$ , $m\geq1$ , such that $\mathbb{P}(A \Delta A_n) \to 0$ as $n \to \infty$ . Hence, $$\mathbb{P}(A_n) \to \mathbb{P}(A), \quad \mathbb{P}(A_n \cap A) \to \mathbb{P}(A) .$$ But as $A \in \cap_{n=1}^\infty \mathcal{F}_n^{\infty}$ , the events $A_n$ and $A$ are independent for every $n \geq 1$ . Hence it follows that $\mathbb{P}(A)=\mathbb{P}(A)^2$ , which completes the proof. Now first of all I presume the $m$ is a typo and this should be an $n$ . The only issue I'm having with this proof is the claim that you can approximate the event $A$ by an event in $\mathcal{F}_1^n$ , and by approximate I mean in the sense of the pseudo metric defined by the symmetric difference operation. Shiryaev has an exercise in a previous chapter which he refers to in this proof that says given a probability space $(\Omega,\mathcal{F},\mathbb{P})$ and an algebra $\mathcal{A}$ such that $\sigma(\mathcal{A})=\mathcal{F}$ , for any $B \in \mathcal{F}$ and $\epsilon>0$ there exists a set $A \in \mathcal{A}$ such that, $$\mathbb{P}(B \Delta A) \leq \epsilon. $$ In the proof above the set in which we are taking the $A_n$ 's are one, not necessarily an algebra as unions of $\sigma$ -algebras need not be an algebra and secondly they do not even generate the $\sigma$ -algebra that $A$ belongs to, i.e $\mathcal{F}_1^\infty$ . So my question is why we can apply this result ? Just to be 100% clear the proof is transcribed correctly, i.e the possible typo of $m\geq1$ does appear in the book.","I know this theorem has been discussed in a lot of posts on this site however I was unable to find an answer that deals with the particular proof I am trying to understand. I'm interested in the proof of Kolmogorov's - law given in the book "" Probability by Albert Shiryaev "". The statement is as follows, Theorem: Let be a sequence of independent random variables and let , i.e an element of the tail -algebra. Then or . We define with the convention . Proof: Clearly we have . We can find sets , , such that as . Hence, But as , the events and are independent for every . Hence it follows that , which completes the proof. Now first of all I presume the is a typo and this should be an . The only issue I'm having with this proof is the claim that you can approximate the event by an event in , and by approximate I mean in the sense of the pseudo metric defined by the symmetric difference operation. Shiryaev has an exercise in a previous chapter which he refers to in this proof that says given a probability space and an algebra such that , for any and there exists a set such that, In the proof above the set in which we are taking the 's are one, not necessarily an algebra as unions of -algebras need not be an algebra and secondly they do not even generate the -algebra that belongs to, i.e . So my question is why we can apply this result ? Just to be 100% clear the proof is transcribed correctly, i.e the possible typo of does appear in the book.","0 1 \xi_1,\xi_2,\ldots A \in \cap_{n=1}^\infty \mathcal{F}_n^{\infty} \sigma \mathbb{P}(A)=0 1 \mathcal{F}_n^k = \sigma(\xi_n,\xi_{n+1},\ldots,\xi_{k}) \mathcal{F}_n^{\infty}=\sigma(\xi_n,\xi_{n+1},\ldots) A \in \mathcal{F}_1^\infty=\sigma(\xi_1,\xi_{2},\ldots)=\sigma(\cup_n \mathcal{F}_1^n) A_n \in \mathcal{F}_1^n m\geq1 \mathbb{P}(A \Delta A_n) \to 0 n \to \infty \mathbb{P}(A_n) \to \mathbb{P}(A), \quad \mathbb{P}(A_n \cap A) \to \mathbb{P}(A) . A \in \cap_{n=1}^\infty \mathcal{F}_n^{\infty} A_n A n \geq 1 \mathbb{P}(A)=\mathbb{P}(A)^2 m n A \mathcal{F}_1^n (\Omega,\mathcal{F},\mathbb{P}) \mathcal{A} \sigma(\mathcal{A})=\mathcal{F} B \in \mathcal{F} \epsilon>0 A \in \mathcal{A} \mathbb{P}(B \Delta A) \leq \epsilon.  A_n \sigma \sigma A \mathcal{F}_1^\infty m\geq1","['probability', 'probability-theory']"
14,Probability of winning a coin toss game,Probability of winning a coin toss game,,"This may seem like a straightforward probability question but it becomes complicated quickly. I have tried a lot of paths to a solution and would appreciate your help. It is not for school or work. I flip a coin. If it's heads, I win. But if it's tails, then I need two heads in a row to win. If I then flip another tails, at any time before winning, then I need three heads in a row to win, and so on. The number of heads I need to win is always T+1, where T is the cumulative number of tails flipped during the game. I want to find the overall probability of winning. It is easy to do so numerically. The answer is about 71%. But I want an exact expression, a recursive function to sum the series of probabilities over infinite flips. I imagine it's an infinite recursive product but I've yet to find an answer. Any help would be infinitely appreciated.","This may seem like a straightforward probability question but it becomes complicated quickly. I have tried a lot of paths to a solution and would appreciate your help. It is not for school or work. I flip a coin. If it's heads, I win. But if it's tails, then I need two heads in a row to win. If I then flip another tails, at any time before winning, then I need three heads in a row to win, and so on. The number of heads I need to win is always T+1, where T is the cumulative number of tails flipped during the game. I want to find the overall probability of winning. It is easy to do so numerically. The answer is about 71%. But I want an exact expression, a recursive function to sum the series of probabilities over infinite flips. I imagine it's an infinite recursive product but I've yet to find an answer. Any help would be infinitely appreciated.",,"['probability', 'statistics']"
15,Find the limiting distribution of $\bar{X}=\frac{\sum X_i}{n}$.,Find the limiting distribution of .,\bar{X}=\frac{\sum X_i}{n},"Let $X_1,\dots, X_n$ be identically distributed with mean $E[X_1]=\mu$ and $\operatorname{Var}[X_1]=\sigma^2$ . Assume that $\operatorname{Cov}(X_k, X_{k+1})\neq 0$ for $k=1,\dots, n$ but $\operatorname{Cov}(X_k, X_l)=0$ for $|k-l|\ge 2$ . Find the limiting distribution of $\bar{X}=\displaystyle{\frac{\sum X_i}{n}}$ . I cannot use the Central limit distribution. Is there another method to do that?",Let be identically distributed with mean and . Assume that for but for . Find the limiting distribution of . I cannot use the Central limit distribution. Is there another method to do that?,"X_1,\dots, X_n E[X_1]=\mu \operatorname{Var}[X_1]=\sigma^2 \operatorname{Cov}(X_k, X_{k+1})\neq 0 k=1,\dots, n \operatorname{Cov}(X_k, X_l)=0 |k-l|\ge 2 \bar{X}=\displaystyle{\frac{\sum X_i}{n}}","['probability', 'statistics']"
16,Calculating expected value of choosing red ball from 5 bins of different sizes,Calculating expected value of choosing red ball from 5 bins of different sizes,,MY ATTEMPT AT SOLUTION: $P(win)=P(draw.at.least.3. red)=P(draw 3 R)+P(draw 4 R) + P(draw 5 R)$ Using the law of total probability: $=P(bin1)*P(draw.3|bin1)+P(bin2)*P(draw.3|bin2)+P(bin3)*P(draw.3|bin3)+P(bin4)*P(draw.3|bin4)+P(bin5)*P(draw.3|bin5)+P(bin1)*P(draw.4|bin1)+P(bin2)*P(draw.4|bin2)+P(bin3)*P(draw.4|bin3)+P(bin4)*P(draw.4|bin4)+P(bin5)*P(draw.4|bin5)+P(bin1)*P(draw.5|bin1)+P(bin2)*P(draw.5|bin2)+P(bin3)*P(draw.5|bin3)+P(bin4)*P(draw.5|bin4)+P(bin5)*P(draw.5|bin5)$ Factoring this all out I simplify to get: $P(bin1)*(P(draw.3|bin1)+P(draw.4|bin1)+P(draw.5|bin1)) +P(bin2)*(P(draw.3|bin2)+P(draw.4|bin2)+P(draw.5|bin2)) +P(bin3)*(P(draw.3|bin3)+P(draw.4|bin3)+P(draw.5|bin3) +P(bin4)*(P(draw.3|bin4)+P(draw.4|bin4)+P(draw.5|bin4)) +P(bin5)*(P(draw.3|bin5)+P(draw.4|bin5)+P(draw.5|bin5))$ Since we are given the ratios of the sizes of the bins I tried to find the probability that way. So bin 5 is the smallest. $P(bin1)=5*P(bin5)$ $P(bin2)=4*P(bin5)$ $P(bin3)=3*P(bin5)$ $P(bin4)=2*P(bin5)$ $P(bin5)=1*P(bin5)$ since the probabilities of all the bins need to add to 1 we have $1=15*P(bin5) \implies P(bin5)=\frac{1}{15}$ Then I can calculate the remainder probabilities of bins. $P(bin1)=1/3$ $P(bin2)=4/15$ $P(bin3)=1/5$ $P(bin4)=2/15$ $P(bin5)=1/15$ Then calculating $P(draw.3|bin1)=\frac{{12\choose 3}{13\choose 2}}{{25 \choose 5}}$ So I did these calculations for the remainder probabilities and got something around 0.58. So the probability of winning 10 is 0.58 and the probability of losing $p$ is 0.42. So then I set $E[X]=0$ and solved for how much you would lose if playing the game. $10(0.58)+p(0.42)=0$ $p=-13.8$ This makes no sense. I'm losing more than I would win by playing the game? I have a feeling I did this question all wrong. Can someone please help me with the solution?,MY ATTEMPT AT SOLUTION: Using the law of total probability: Factoring this all out I simplify to get: Since we are given the ratios of the sizes of the bins I tried to find the probability that way. So bin 5 is the smallest. since the probabilities of all the bins need to add to 1 we have Then I can calculate the remainder probabilities of bins. Then calculating So I did these calculations for the remainder probabilities and got something around 0.58. So the probability of winning 10 is 0.58 and the probability of losing is 0.42. So then I set and solved for how much you would lose if playing the game. This makes no sense. I'm losing more than I would win by playing the game? I have a feeling I did this question all wrong. Can someone please help me with the solution?,"P(win)=P(draw.at.least.3. red)=P(draw 3 R)+P(draw 4 R) + P(draw 5 R) =P(bin1)*P(draw.3|bin1)+P(bin2)*P(draw.3|bin2)+P(bin3)*P(draw.3|bin3)+P(bin4)*P(draw.3|bin4)+P(bin5)*P(draw.3|bin5)+P(bin1)*P(draw.4|bin1)+P(bin2)*P(draw.4|bin2)+P(bin3)*P(draw.4|bin3)+P(bin4)*P(draw.4|bin4)+P(bin5)*P(draw.4|bin5)+P(bin1)*P(draw.5|bin1)+P(bin2)*P(draw.5|bin2)+P(bin3)*P(draw.5|bin3)+P(bin4)*P(draw.5|bin4)+P(bin5)*P(draw.5|bin5) P(bin1)*(P(draw.3|bin1)+P(draw.4|bin1)+P(draw.5|bin1))
+P(bin2)*(P(draw.3|bin2)+P(draw.4|bin2)+P(draw.5|bin2))
+P(bin3)*(P(draw.3|bin3)+P(draw.4|bin3)+P(draw.5|bin3)
+P(bin4)*(P(draw.3|bin4)+P(draw.4|bin4)+P(draw.5|bin4))
+P(bin5)*(P(draw.3|bin5)+P(draw.4|bin5)+P(draw.5|bin5)) P(bin1)=5*P(bin5) P(bin2)=4*P(bin5) P(bin3)=3*P(bin5) P(bin4)=2*P(bin5) P(bin5)=1*P(bin5) 1=15*P(bin5) \implies P(bin5)=\frac{1}{15} P(bin1)=1/3 P(bin2)=4/15 P(bin3)=1/5 P(bin4)=2/15 P(bin5)=1/15 P(draw.3|bin1)=\frac{{12\choose 3}{13\choose 2}}{{25 \choose 5}} p E[X]=0 10(0.58)+p(0.42)=0 p=-13.8",['probability']
17,Almost Surely Convergence of a Series of Random Variables,Almost Surely Convergence of a Series of Random Variables,,"I am trying to solve this problem: Let $(X_n)_{n\in \mathbb{N}}$ be a sequence of independent random variables on the probability space $(\Omega , \mathcal{F},  \mathbb{P})$ and they all have uniform distribution on $[0,1]$ . Calculate $E(X_1 \cdots X_n)$ for each $n\in \mathbb{N}$ . Deduce that the series $\sum_{n=1}^\infty X_1\cdots X_n$ converges almost surely to a finite sum. These are my thoughts on the first question: Let $f(x)$ be the density function of uniform distribution on $[0,1]$ . Then $$E(X_1\cdots X_n) = \int_0^1 \cdots \int_0^1 x_1\cdots x_nf(x_1)\cdots f(x_n)dx_1\cdots dx_n.$$ Now, since $f(x)=1$ for $x\in [0,1]$ and by Fubini-Tonelli theorem we have $$E(X_1\cdots X_n) = \int_0^1 x_1dx_1 \cdots \int_0^1x_ndx_n = \left(\frac{1}{2}\right)^n.$$ For the second part, I defined $$Y_n := X_1\cdots X_n.$$ Since the expectation of $Y_n$ 's converges to $0$ , we say that their outcomes go to zero on average. Hence it makes sense if we can find some constant $c\in \mathbb{R}$ such that $$\mathbb{P}\left(\sum_{n=1}^\infty Y_n = c\right) = 1.$$ But I do not know how can I prove the existence of the constant $c$ mathematically. I appreciate any help.","I am trying to solve this problem: Let be a sequence of independent random variables on the probability space and they all have uniform distribution on . Calculate for each . Deduce that the series converges almost surely to a finite sum. These are my thoughts on the first question: Let be the density function of uniform distribution on . Then Now, since for and by Fubini-Tonelli theorem we have For the second part, I defined Since the expectation of 's converges to , we say that their outcomes go to zero on average. Hence it makes sense if we can find some constant such that But I do not know how can I prove the existence of the constant mathematically. I appreciate any help.","(X_n)_{n\in \mathbb{N}} (\Omega , \mathcal{F},
 \mathbb{P}) [0,1] E(X_1 \cdots X_n) n\in \mathbb{N} \sum_{n=1}^\infty X_1\cdots X_n f(x) [0,1] E(X_1\cdots X_n) = \int_0^1 \cdots \int_0^1 x_1\cdots x_nf(x_1)\cdots f(x_n)dx_1\cdots dx_n. f(x)=1 x\in [0,1] E(X_1\cdots X_n) = \int_0^1 x_1dx_1 \cdots \int_0^1x_ndx_n = \left(\frac{1}{2}\right)^n. Y_n := X_1\cdots X_n. Y_n 0 c\in \mathbb{R} \mathbb{P}\left(\sum_{n=1}^\infty Y_n = c\right) = 1. c","['probability', 'probability-theory', 'probability-distributions', 'convergence-divergence', 'random-variables']"
18,1956 Miklos Schweitzer Counting - Problem 10,1956 Miklos Schweitzer Counting - Problem 10,,"Source : 1956 Miklos Schweitzer Contest (a Hungarian undergraduate open-note math contest) Problem 10 In an urn there are balls of $N$ different colours, $n$ balls of each colour. Balls are drawn and not replaced until one of the colours turns up twice; denote by $V_{N,n} $ the number of the balls drawn and by $M_{N,n}$ the expectation of the random variable $v_{N,n}$ . Find the limit distribution of the random variable $\frac{V_{N,n}}{M_{N,n}}$ if $N \to \infty$ and $n$ is a fixed number. Attempt (wrong) : My thought process is that I'd 1. find the distribution of $V_{N,n}$ , 2. compute $\mathbb{E}[V_{N,n}]$ , and 3. apply a change of variables to get the distribution in question $Y=\frac{V_{N,n}}{\mathbb{E}[V_{N,n}]}$ , and 4. justify the limiting distribution (I assume not too bad if we fix $n$ ). We can justify the density by counting. If we desire $P(V_{N,n}=k)$ , then this event occurs if we select $k-1$ colors from $N$ possibilities, $\binom{N}{k-1}$ , and we select one ball from each of the $k-1$ colors for the first $k-1$ draws, $\binom{n}{1}^{k-1}$ , and finally we select the last ball to be one of the $k-1$ colors already chosen, $\binom{k-1}{1}$ . In summary, we have for $k\in\{2,\dots,N+1\}$ $$\mathbb{P}\left(V_{N,n}=k\right) = \frac{\binom{N}{k-1}n^{k-1}(k-1)}{\binom{Nn}{k}}.$$ From here, I thought oh maybe I'd apply some combinatorial identity like hockey-stick, Pascal's, the binomial theorem, look at generating functions or something similar to compute the expectation: $$\mathbb{E}\left[V_{N,n}\right]=\sum\limits_{k=2}^{N+1} \frac{\binom{N}{k-1}n^{k-1}(k-1)}{\binom{Nn}{k}}\cdot k.$$ However, I have no idea how to deal with the bottom - I assume I could apply Stirling's to get an approximation, but is there something exact here? Even if step 2 failed, I try to continue - by change of variables, we have $$\mathbb{P}\left(Y=y\right)=\mathbb{P}\left(V_{N,n}=\mathbb{E}[V_{N,n}]y\right)\cdot\mathbb{E}[V_{N,n}]=\frac{\binom{N}{\mathbb{E}[V_{N,n}]y-1}n^{\mathbb{E}[V_{N,n}]y-1}(\mathbb{E}[V_{N,n}]y-1)}{\binom{Nn}{\mathbb{E}[V_{N,n}]y}}\cdot \mathbb{E}[V_{N,n}].$$ I can obviously not obtain a closed form without finding the expectation; however, it is evident that $\mathbb{E}[V_{N,n}]\propto N$ and $\mathbb{E}[V_{N,n}]\propto n^{-1}$ . Thus as $N\rightarrow\infty$ , ... uh never mind, there's nothing I think we could deduce. Edit 1 (old method) : With Raskolnikov's help, I now have, after some manipulation, $$\mathbb{E}[V_{N,n}]=n(n-1)\sum\limits_{k=2}^N \frac{\binom{N}{k-1}n^{k-2}(k-1)}{\binom{Nn}{k}}+\frac{n^{N-1}(N+1)}{\binom{Nn-1}{N-1}}.$$ This looks a little more interesting - I really wanted to get the sum on the left to look like the derivative of the sum if I differentiate with respect to $n$ , but the denominator is still troubling me. I thought maybe Vandermonde's would be useful $$\binom{Nn}{k}=\sum\limits_{k_1+\cdots+k_n=k} \binom{N}{k_1}\cdots\binom{N}{k_n},$$ but that also seems to lead to a dead end I think. Edit 2 (new method) : Okay, with Raskolnikov's new method, I think it suffices to show the distribution of $V_{N,n}$ is exponential for some rate $r\in(0,\infty)$ . With this approach, I tried the following (setting $n=2$ WLOG) $$\mathbb{P}(V_{N,n}>k)=\frac{2^{k-1}\binom{N-1}{k-1}}{\binom{2N-1}{k-1}}$$ $$= 2^k\frac{(2N-k)\cdots(N-k+1)}{2N\cdots(N+1)}$$ $$= 2^k\left(1-\frac{k}{2N}\right)\cdots\left(1-\frac{k}{N+1}\right)$$ $$=^*\left(1-\frac{rk}{N}\right)^N\rightarrow e^{-rk}\;\;\text{as}\;\; N\rightarrow\infty$$ but I'm blanking on the choice of $r$ to get $=^*$ to hold (or if it's even possible). Question : I would appreciate some help for evaluating the limit in the new edited section. Thanks!","Source : 1956 Miklos Schweitzer Contest (a Hungarian undergraduate open-note math contest) Problem 10 In an urn there are balls of different colours, balls of each colour. Balls are drawn and not replaced until one of the colours turns up twice; denote by the number of the balls drawn and by the expectation of the random variable . Find the limit distribution of the random variable if and is a fixed number. Attempt (wrong) : My thought process is that I'd 1. find the distribution of , 2. compute , and 3. apply a change of variables to get the distribution in question , and 4. justify the limiting distribution (I assume not too bad if we fix ). We can justify the density by counting. If we desire , then this event occurs if we select colors from possibilities, , and we select one ball from each of the colors for the first draws, , and finally we select the last ball to be one of the colors already chosen, . In summary, we have for From here, I thought oh maybe I'd apply some combinatorial identity like hockey-stick, Pascal's, the binomial theorem, look at generating functions or something similar to compute the expectation: However, I have no idea how to deal with the bottom - I assume I could apply Stirling's to get an approximation, but is there something exact here? Even if step 2 failed, I try to continue - by change of variables, we have I can obviously not obtain a closed form without finding the expectation; however, it is evident that and . Thus as , ... uh never mind, there's nothing I think we could deduce. Edit 1 (old method) : With Raskolnikov's help, I now have, after some manipulation, This looks a little more interesting - I really wanted to get the sum on the left to look like the derivative of the sum if I differentiate with respect to , but the denominator is still troubling me. I thought maybe Vandermonde's would be useful but that also seems to lead to a dead end I think. Edit 2 (new method) : Okay, with Raskolnikov's new method, I think it suffices to show the distribution of is exponential for some rate . With this approach, I tried the following (setting WLOG) but I'm blanking on the choice of to get to hold (or if it's even possible). Question : I would appreciate some help for evaluating the limit in the new edited section. Thanks!","N n V_{N,n}  M_{N,n} v_{N,n} \frac{V_{N,n}}{M_{N,n}} N \to \infty n V_{N,n} \mathbb{E}[V_{N,n}] Y=\frac{V_{N,n}}{\mathbb{E}[V_{N,n}]} n P(V_{N,n}=k) k-1 N \binom{N}{k-1} k-1 k-1 \binom{n}{1}^{k-1} k-1 \binom{k-1}{1} k\in\{2,\dots,N+1\} \mathbb{P}\left(V_{N,n}=k\right) = \frac{\binom{N}{k-1}n^{k-1}(k-1)}{\binom{Nn}{k}}. \mathbb{E}\left[V_{N,n}\right]=\sum\limits_{k=2}^{N+1} \frac{\binom{N}{k-1}n^{k-1}(k-1)}{\binom{Nn}{k}}\cdot k. \mathbb{P}\left(Y=y\right)=\mathbb{P}\left(V_{N,n}=\mathbb{E}[V_{N,n}]y\right)\cdot\mathbb{E}[V_{N,n}]=\frac{\binom{N}{\mathbb{E}[V_{N,n}]y-1}n^{\mathbb{E}[V_{N,n}]y-1}(\mathbb{E}[V_{N,n}]y-1)}{\binom{Nn}{\mathbb{E}[V_{N,n}]y}}\cdot \mathbb{E}[V_{N,n}]. \mathbb{E}[V_{N,n}]\propto N \mathbb{E}[V_{N,n}]\propto n^{-1} N\rightarrow\infty \mathbb{E}[V_{N,n}]=n(n-1)\sum\limits_{k=2}^N \frac{\binom{N}{k-1}n^{k-2}(k-1)}{\binom{Nn}{k}}+\frac{n^{N-1}(N+1)}{\binom{Nn-1}{N-1}}. n \binom{Nn}{k}=\sum\limits_{k_1+\cdots+k_n=k} \binom{N}{k_1}\cdots\binom{N}{k_n}, V_{N,n} r\in(0,\infty) n=2 \mathbb{P}(V_{N,n}>k)=\frac{2^{k-1}\binom{N-1}{k-1}}{\binom{2N-1}{k-1}} = 2^k\frac{(2N-k)\cdots(N-k+1)}{2N\cdots(N+1)} = 2^k\left(1-\frac{k}{2N}\right)\cdots\left(1-\frac{k}{N+1}\right) =^*\left(1-\frac{rk}{N}\right)^N\rightarrow e^{-rk}\;\;\text{as}\;\; N\rightarrow\infty r =^*","['probability', 'combinatorics']"
19,Integrating a function of a Markov chain on a random length trajectory,Integrating a function of a Markov chain on a random length trajectory,,"Suppose that $(X_k)_{k\geq 0}$ is a Markov chain in a finite state space $\mathcal{S}$ , with a known transition matrix $\mathbf{P}$ , and let $f:\mathcal{S}\to\mathbb{R}$ be a known function that assigns a real value $f(s)$ to each state in $s\in\mathcal{S}$ . If $n$ is a fixed integer and $X_0 = s_0\in\mathcal{S}$ with probability one, then we can easily compute the sum of $f$ along a trajectory of length $n$ , given by $\mathbf{E}[\sum_{i=0}^{n}f(X_i)]$ . This can be done by computing each term separately and integrating $f$ with respect to the law of $X_i$ , which can be expressed using the transition matrix $\mathbf{P}$ and its powers. I am trying to solve an extension of this problem, where we replace the fixed $n$ by a random integer $N$ that depends on the chain in the following way. Instead of having a trajectory of fixed length $n$ in the sum, I want certain states to be able to increase the length of the trajectory when visited. To formalize this idea, I define a map $m:\mathcal{S}\to\mathbb{N}\cup\{0\}$ with $m(s_0)\geq 1$ , where $m(s)$ represents the increase in trajectory length brought by visiting the state $s \in \mathcal{S}$ , and I define the process $(M_k)_{k\geq 0}$ that will keep track of the remaining trajectory length. We set $M_0 = m(s_0) \geq 1$ , which is the initial trajectory length, and let $M_k = M_{k-1} + m(X_k) - 1$ . With each time step and visited state $s$ , the remaining trajectory length decreases by $1$ , and increases by $m(s)$ . The summation of $f$ along the trajectory stops when $M_k$ reaches zero, i.e. we define $N = \inf\{k\geq 0 : M_k = 0\}$ and are interested in computing $\mathbf{E}[\sum_{i=0}^{N}f(X_i)]$ . To exclude the possibility that $\mathbf{P}(N=\infty)>0$ , I assume that there exists one and only one absorbing state $s_A\in\mathcal{S}$ , such that $m(s_A)=0$ . This makes $N$ almost surely finite. However, if there exists a state $s\neq s_A$ with $m(s)=0$ , it can happen that $X_N \neq s_A$ . Is there a hope to analytically calculate $\mathbb{E}[\sum_{i=0}^N f(X_i)]$ for a general choice of $\mathcal{S}, \mathbf{P}, f$ and $m$ that satisfy the above properties ? The only idea I have is to partition the space according to the value of $N$ , but then the difficulty shifts to computing the quantities $\mathbb{P}(N=n)$ .","Suppose that is a Markov chain in a finite state space , with a known transition matrix , and let be a known function that assigns a real value to each state in . If is a fixed integer and with probability one, then we can easily compute the sum of along a trajectory of length , given by . This can be done by computing each term separately and integrating with respect to the law of , which can be expressed using the transition matrix and its powers. I am trying to solve an extension of this problem, where we replace the fixed by a random integer that depends on the chain in the following way. Instead of having a trajectory of fixed length in the sum, I want certain states to be able to increase the length of the trajectory when visited. To formalize this idea, I define a map with , where represents the increase in trajectory length brought by visiting the state , and I define the process that will keep track of the remaining trajectory length. We set , which is the initial trajectory length, and let . With each time step and visited state , the remaining trajectory length decreases by , and increases by . The summation of along the trajectory stops when reaches zero, i.e. we define and are interested in computing . To exclude the possibility that , I assume that there exists one and only one absorbing state , such that . This makes almost surely finite. However, if there exists a state with , it can happen that . Is there a hope to analytically calculate for a general choice of and that satisfy the above properties ? The only idea I have is to partition the space according to the value of , but then the difficulty shifts to computing the quantities .","(X_k)_{k\geq 0} \mathcal{S} \mathbf{P} f:\mathcal{S}\to\mathbb{R} f(s) s\in\mathcal{S} n X_0 = s_0\in\mathcal{S} f n \mathbf{E}[\sum_{i=0}^{n}f(X_i)] f X_i \mathbf{P} n N n m:\mathcal{S}\to\mathbb{N}\cup\{0\} m(s_0)\geq 1 m(s) s \in \mathcal{S} (M_k)_{k\geq 0} M_0 = m(s_0) \geq 1 M_k = M_{k-1} + m(X_k) - 1 s 1 m(s) f M_k N = \inf\{k\geq 0 : M_k = 0\} \mathbf{E}[\sum_{i=0}^{N}f(X_i)] \mathbf{P}(N=\infty)>0 s_A\in\mathcal{S} m(s_A)=0 N s\neq s_A m(s)=0 X_N \neq s_A \mathbb{E}[\sum_{i=0}^N f(X_i)] \mathcal{S}, \mathbf{P}, f m N \mathbb{P}(N=n)","['probability', 'probability-theory', 'random-variables', 'markov-chains']"
20,Expected number of rolls until all dice are removed,Expected number of rolls until all dice are removed,,"There are $n$ fair dice. They are all tossed every time except the dice that are removed. A dice is removed if $3$ is rolled. What is the expected number of rolls? Any help would be appreciated. My attempt: Consider the case of two dice. The process ends on step one with probability $\frac{1}{36}$ , if one of the dice rolls up $6$ and other doesn't the on average $\frac{10}{36}(1+6)$ more steps are needed. Finally if $6$ doesn't roll up on both of the rolls then $\frac{25}{36}(z+1)$ rolls will be needed where $z$ is the expected number of rolls until both dice are removed. Thus $$z= \frac{1}{36}+\frac{10}{36}(1+6)+\frac{25}{36}(z+1) $$ Is this correct? If it is I can extend it recursively for $n$ dice.","There are fair dice. They are all tossed every time except the dice that are removed. A dice is removed if is rolled. What is the expected number of rolls? Any help would be appreciated. My attempt: Consider the case of two dice. The process ends on step one with probability , if one of the dice rolls up and other doesn't the on average more steps are needed. Finally if doesn't roll up on both of the rolls then rolls will be needed where is the expected number of rolls until both dice are removed. Thus Is this correct? If it is I can extend it recursively for dice.",n 3 \frac{1}{36} 6 \frac{10}{36}(1+6) 6 \frac{25}{36}(z+1) z z= \frac{1}{36}+\frac{10}{36}(1+6)+\frac{25}{36}(z+1)  n,"['probability', 'combinatorics', 'expected-value']"
21,"Computing the expected size of the largest connected component in a ""hitomezashi graph"" (described in the question body)","Computing the expected size of the largest connected component in a ""hitomezashi graph"" (described in the question body)",,"A while ago there was a numberphile video about a certain graph you can build based on Hitomezashi Sashiko , a kind of decorative mending. Intuitively, we alternate putting walls (originally stitches) in each column/row, which separate the integer lattice into components. $s_1$ governs the positions of the vertical walls and $s_2$ governs the positions of the horizontal walls. A $0$ tells us to start ""at the edge"" and a $1$ tells us to start ""off the edge"". For example, choosing $s_1 = 01101$ and $s_2 = 10110$ gives the following stitch pattern. If we think of each unit square as a vertex, with adjacent cells connected whenever there isn't a wall separating them, we get a graph, $H(s_1,s_2)$ : If it's still unclear how the walls work, it's made clear in the first few minutes of the linked numberphile video. For those looking for a precise description, formally, we build a graph $H(s_1, s_2)$ given two binary strings (say of length $n_1$ and $n_2$ ) as follows $[n_1] \times [n_2]$ is the vertex set ( $0$ indexed) $(x,y) \sim (x+1,y)$ whenever $y \not \equiv s_1[x+1] \pmod{2}$ $(x,y) \sim (x,y+1)$ whenever $x \not \equiv s_2[y+1] \pmod{2}$ I also have a demo on my blog where you can input binary strings and it will output a picture of the graph. Now, if we do this with longer binary strings, we get some quite intricate pictures: and there are some natural questions to ask. I've put a fair amount of my thoughts about these problems in a different blog post , but here is the one I'm primarily interested in: Say we (uniformly) randomly choose two binary strings of a fixed length $n$ . What is the expected size of the largest connected component of $H(s_1, s_2)$ as a function of $n$ ? I'm not much of a probabilistic combinatorialist, so I pretty quickly exhausted my personal bag of tricks for attacking these kinds of problems. But this feels like something that somebody knows how to answer (and ideally, something somebody would enjoy answering ^_^). I recognize this is probably hard to answer, so I'm open to partial progress. In particular, I wrote some sage code to get some data, and here's a graph of the average maximum region size (across a few hundred samples) as a function of $n$ : The blue curve is the polynomial of best fit, which turns out to be $\approx 1.95 n^{1.38}$ This brings us to some (hopefully easier) problems: Can we show that the expected size of the largest component is $o(n^2)$ ? What about $o(n^{1.5})$ ? Is it possible to pin down the exponent exactly? Can we get lower bounds too? You can find more of my thoughts, as well as some code for simulating these things yourself if you're interested, in my blog post here . I'm open to hearing any thoughts that people have about this, because I really have no idea how to proceed. Edit (Jan 5): Based on the new data from Daniel Mathias, it seems a good conjecture is $\frac{8}{3} n^{4/3}$ . I've added a $50$ rep bounty as thanks, and afterwards I'll add a $100$ rep bounty for a proof of this conjecture (or some substantial progress). I need to wait $24$ hours before I can post that second bounty, though. Thanks in advance! ^_^","A while ago there was a numberphile video about a certain graph you can build based on Hitomezashi Sashiko , a kind of decorative mending. Intuitively, we alternate putting walls (originally stitches) in each column/row, which separate the integer lattice into components. governs the positions of the vertical walls and governs the positions of the horizontal walls. A tells us to start ""at the edge"" and a tells us to start ""off the edge"". For example, choosing and gives the following stitch pattern. If we think of each unit square as a vertex, with adjacent cells connected whenever there isn't a wall separating them, we get a graph, : If it's still unclear how the walls work, it's made clear in the first few minutes of the linked numberphile video. For those looking for a precise description, formally, we build a graph given two binary strings (say of length and ) as follows is the vertex set ( indexed) whenever whenever I also have a demo on my blog where you can input binary strings and it will output a picture of the graph. Now, if we do this with longer binary strings, we get some quite intricate pictures: and there are some natural questions to ask. I've put a fair amount of my thoughts about these problems in a different blog post , but here is the one I'm primarily interested in: Say we (uniformly) randomly choose two binary strings of a fixed length . What is the expected size of the largest connected component of as a function of ? I'm not much of a probabilistic combinatorialist, so I pretty quickly exhausted my personal bag of tricks for attacking these kinds of problems. But this feels like something that somebody knows how to answer (and ideally, something somebody would enjoy answering ^_^). I recognize this is probably hard to answer, so I'm open to partial progress. In particular, I wrote some sage code to get some data, and here's a graph of the average maximum region size (across a few hundred samples) as a function of : The blue curve is the polynomial of best fit, which turns out to be This brings us to some (hopefully easier) problems: Can we show that the expected size of the largest component is ? What about ? Is it possible to pin down the exponent exactly? Can we get lower bounds too? You can find more of my thoughts, as well as some code for simulating these things yourself if you're interested, in my blog post here . I'm open to hearing any thoughts that people have about this, because I really have no idea how to proceed. Edit (Jan 5): Based on the new data from Daniel Mathias, it seems a good conjecture is . I've added a rep bounty as thanks, and afterwards I'll add a rep bounty for a proof of this conjecture (or some substantial progress). I need to wait hours before I can post that second bounty, though. Thanks in advance! ^_^","s_1 s_2 0 1 s_1 = 01101 s_2 = 10110 H(s_1,s_2) H(s_1, s_2) n_1 n_2 [n_1] \times [n_2] 0 (x,y) \sim (x+1,y) y \not \equiv s_1[x+1] \pmod{2} (x,y) \sim (x,y+1) x \not \equiv s_2[y+1] \pmod{2} n H(s_1, s_2) n n \approx 1.95 n^{1.38} o(n^2) o(n^{1.5}) \frac{8}{3} n^{4/3} 50 100 24","['probability', 'combinatorics', 'graph-theory']"
22,Entropy of bivariate negative binomial distribution,Entropy of bivariate negative binomial distribution,,"The probability mass function (PMF) of a bivariate negative binomial distribution [1] is given by: $$P(X=x, Y=y) = \frac{(a + x + y - 1)!}{(a-1)! x! y!} p_0^a p_1^x p_2^y $$ where $a, p_0, p_1, p_2 > 0$ and $p_0 + p_1 + p_2 = 1$ . I would like to calculate the entropy $$H(x, y) = - \sum_{x=0}^{\infty}\sum_{y=0}^{\infty} P(x, y) \log P(x, y)$$ of this distribution. After some arithmetic manipulation, I arrived at the following expression: \begin{multline} H(x, y) = - a \log p_0 - \mathbb{E}[x] \log(p_1) - \mathbb{E}[y] \log(p_2) + \log\big((a-1)!\big) \\ - \mathbb{E}\big[\log\big((a + x + y - 1)!\big)\big] + \mathbb{E}[\log(x!)] + \mathbb{E}[\log(y!)]\end{multline} The terms $\mathbb{E}[\log(x!)]$ and $\mathbb{E}[\log(y!)]$ can be calculated by solving a definite integral in the interval $[0, 1]$ as shown in [2] (equation 23). However, it is not clear to me how the same approach can be extended for the computation of expectation of the log factorial $\mathbb{E}\big[\log\big((a + x + y - 1)!\big)\big]$ over the joint. Can anyone shed some light on this? References: [1] Dunn 1967, Characterization of the Bivariate Negative Binomial Distribution ( pdf ) [2] Cheraghchi 2018, Expressions for the Entropy of Binomial-Type Distributions ( pdf )","The probability mass function (PMF) of a bivariate negative binomial distribution [1] is given by: where and . I would like to calculate the entropy of this distribution. After some arithmetic manipulation, I arrived at the following expression: The terms and can be calculated by solving a definite integral in the interval as shown in [2] (equation 23). However, it is not clear to me how the same approach can be extended for the computation of expectation of the log factorial over the joint. Can anyone shed some light on this? References: [1] Dunn 1967, Characterization of the Bivariate Negative Binomial Distribution ( pdf ) [2] Cheraghchi 2018, Expressions for the Entropy of Binomial-Type Distributions ( pdf )","P(X=x, Y=y) = \frac{(a + x + y - 1)!}{(a-1)! x! y!} p_0^a p_1^x p_2^y  a, p_0, p_1, p_2 > 0 p_0 + p_1 + p_2 = 1 H(x, y) = - \sum_{x=0}^{\infty}\sum_{y=0}^{\infty} P(x, y) \log P(x, y) \begin{multline}
H(x, y) = - a \log p_0 - \mathbb{E}[x] \log(p_1) - \mathbb{E}[y] \log(p_2) + \log\big((a-1)!\big) \\
- \mathbb{E}\big[\log\big((a + x + y - 1)!\big)\big] + \mathbb{E}[\log(x!)] + \mathbb{E}[\log(y!)]\end{multline} \mathbb{E}[\log(x!)] \mathbb{E}[\log(y!)] [0, 1] \mathbb{E}\big[\log\big((a + x + y - 1)!\big)\big]","['probability', 'statistics', 'probability-distributions', 'entropy']"
23,Analysis of a calculation of expected number of collisions in hashing,Analysis of a calculation of expected number of collisions in hashing,,"For a formal problem statement, I quote from the text Introduction to Algorithms by Cormen et. al Suppose we use a hash function $h$ to hash $n$ distinct keys into an array $T$ of length $m$ . Assuming simple uniform hashing, what is the expected number of collisions? More precisely, what is the expected cardinality of $\{\{k, l\} : k\neq l \text{ and } h(k)= h(l)\}$ ? Quite intuitively I proceeded as follows: Let $X$ be the random variable indicating the number of collisions. Let us define an indicator random variable $X_{ij}$ which indicates whether the $i$ th element ( $e_i$ ) already in the table collides with the $j$ th element ( $e_j$ ) when the $j$ th element is to be inserted. $Pr\{h(e_i)=h(e_j)\}=\frac{1}{m}$ . So $E[X_{ij}]=\frac{1}{m}$ . So based on that we have: $$X= \sum_{i=1}^n \sum_{j=i+1}^n X_{ij}$$ Taking expectation on both sides we have: $$E[X]= E\left[\sum_{i=1}^n \sum_{j=i+1}^n X_{ij}\right]$$ Using linearity of expectation: $$E[X]= \sum_{i=1}^n \sum_{j=i+1}^n E[X_{ij}]$$ But, $E[X_{ij}]=\frac{1}{m}$ , which is already found above. So, $$E[X]= \sum_{i=1}^n \sum_{j=i+1}^n \frac{1}{m}$$ $$=\frac{n(n-1)}{2m}$$ Below is how my peer approached: Let $X$ be the random variable indicating the number of collisions. Let $X_i$ be the indicator random variable indicating collision during the $i$ th insertion of the element, $i=1,2,3,..,n$ So, $$X=\sum_{i=1}^n X_i$$ taking expectation on both sides, we have: $$E[X]=E\left[\sum_{i=1}^n X_i\right]$$ Using linearity of expectation: $$E[X]=\sum_{i=1}^n E[X_i]$$ Now let us find $Pr\{X_i=1\}$ then by the property of indicator random variable we shall have $E[X_i]=Pr\{X_i=1\}$ . Probability that there is collision during the first insertion = $0$ [First element is inserted without any collision.] Probability that there is collision during the second insertion= $\frac{1}{m}$ [Assuming open addressing, $1$ slot is already occupied.] Probability that there is collision during the third insertion= $\frac{2}{m}$ [Assuming open addressing, $2$ slots are already occupied.] . . . Probability that there is collision during the $i$ th insertion= $\frac{i-1}{m}$ [Assuming open addressing, $i-1$ slots are already occupied.] So, $$E[X]=\sum_{i=1}^n \frac{i-1}{m}=\frac{n(n-1)}{2m}$$ Though the final answer obtained by my peer is same as that of mine, but I guess there are quite a lot of issues with the approach. First it assumes open addressing, while my approach is a generalized one. Secondly, as per the formal problem statement given in the text, in the worst case if all elements hash to the same slot then we shall have $X=\binom{n}{2}$ . So the spectrum of $X$ is , $X=0,1,2,..,\binom{n}{2}$ But the spectrum of $X$ as per my peer's method is $X=0,1,2..,n$ . Is my peer's method actually correct?","For a formal problem statement, I quote from the text Introduction to Algorithms by Cormen et. al Suppose we use a hash function to hash distinct keys into an array of length . Assuming simple uniform hashing, what is the expected number of collisions? More precisely, what is the expected cardinality of ? Quite intuitively I proceeded as follows: Let be the random variable indicating the number of collisions. Let us define an indicator random variable which indicates whether the th element ( ) already in the table collides with the th element ( ) when the th element is to be inserted. . So . So based on that we have: Taking expectation on both sides we have: Using linearity of expectation: But, , which is already found above. So, Below is how my peer approached: Let be the random variable indicating the number of collisions. Let be the indicator random variable indicating collision during the th insertion of the element, So, taking expectation on both sides, we have: Using linearity of expectation: Now let us find then by the property of indicator random variable we shall have . Probability that there is collision during the first insertion = [First element is inserted without any collision.] Probability that there is collision during the second insertion= [Assuming open addressing, slot is already occupied.] Probability that there is collision during the third insertion= [Assuming open addressing, slots are already occupied.] . . . Probability that there is collision during the th insertion= [Assuming open addressing, slots are already occupied.] So, Though the final answer obtained by my peer is same as that of mine, but I guess there are quite a lot of issues with the approach. First it assumes open addressing, while my approach is a generalized one. Secondly, as per the formal problem statement given in the text, in the worst case if all elements hash to the same slot then we shall have . So the spectrum of is , But the spectrum of as per my peer's method is . Is my peer's method actually correct?","h n T m \{\{k, l\} : k\neq l \text{ and } h(k)= h(l)\} X X_{ij} i e_i j e_j j Pr\{h(e_i)=h(e_j)\}=\frac{1}{m} E[X_{ij}]=\frac{1}{m} X= \sum_{i=1}^n \sum_{j=i+1}^n X_{ij} E[X]= E\left[\sum_{i=1}^n \sum_{j=i+1}^n X_{ij}\right] E[X]= \sum_{i=1}^n \sum_{j=i+1}^n E[X_{ij}] E[X_{ij}]=\frac{1}{m} E[X]= \sum_{i=1}^n \sum_{j=i+1}^n \frac{1}{m} =\frac{n(n-1)}{2m} X X_i i i=1,2,3,..,n X=\sum_{i=1}^n X_i E[X]=E\left[\sum_{i=1}^n X_i\right] E[X]=\sum_{i=1}^n E[X_i] Pr\{X_i=1\} E[X_i]=Pr\{X_i=1\} 0 \frac{1}{m} 1 \frac{2}{m} 2 i \frac{i-1}{m} i-1 E[X]=\sum_{i=1}^n \frac{i-1}{m}=\frac{n(n-1)}{2m} X=\binom{n}{2} X X=0,1,2,..,\binom{n}{2} X X=0,1,2..,n","['probability', 'probability-theory', 'hash-function']"
24,Probability of drawing an ace from a modified deck,Probability of drawing an ace from a modified deck,,"Given the following question, I have a deck of $52$ cards. I shuffle the deck, and drop $20$ cards randomly into a shredder. You then draw two cards from what remains. What is the probability that they are both aces? A lot of similar questions have been asked (with good answers). For example, ace in the 10th card. I know that the answer to my question is $\frac{4}{52}\cdot \frac{3}{51}$ , but I am having A LOT OF TROUBLE internalising the answer. Intuitively, why is it that randomly removing cards does not effect the probability of drawing aces? To be more specific: Shouldn't we condition for the fact that some aces were shredded? I find it very surprising that the answer remains the same after we condition for the fact that $0$ to $4$ aces could be shredded. If we added cards to the deck instead, does the same logic hold? If I were to randomly add $20$ cards from a shuffled deck to an existing deck and shuffle it, is the probability of drawing an ace still $4/52$ ? Or should I factor in the probability that aces were carried over to the deck? There was a similar question Combining random cards from two decks and calculating probability . Any help will be much appreciated.","Given the following question, I have a deck of cards. I shuffle the deck, and drop cards randomly into a shredder. You then draw two cards from what remains. What is the probability that they are both aces? A lot of similar questions have been asked (with good answers). For example, ace in the 10th card. I know that the answer to my question is , but I am having A LOT OF TROUBLE internalising the answer. Intuitively, why is it that randomly removing cards does not effect the probability of drawing aces? To be more specific: Shouldn't we condition for the fact that some aces were shredded? I find it very surprising that the answer remains the same after we condition for the fact that to aces could be shredded. If we added cards to the deck instead, does the same logic hold? If I were to randomly add cards from a shuffled deck to an existing deck and shuffle it, is the probability of drawing an ace still ? Or should I factor in the probability that aces were carried over to the deck? There was a similar question Combining random cards from two decks and calculating probability . Any help will be much appreciated.",52 20 \frac{4}{52}\cdot \frac{3}{51} 0 4 20 4/52,"['probability', 'combinatorics', 'card-games']"
25,How to toss a coin in your head. [duplicate],How to toss a coin in your head. [duplicate],,"This question already has answers here : How to mentally flip a coin? (10 answers) Closed 2 years ago . The community reviewed whether to reopen this question 2 years ago and left it closed: Original close reason(s) were not resolved This is quite a soft question but hopefully, this community has some nice answers. What is a good way to ""toss a coin"" in your head? That is an ""algorithm"" to generate heads $50/50$ of the time roughly that is hard for you to influence without thinking too deeply. I am looking for little mathematical tricks to ""randomise"" and scramble ones ability to select a handful or manageable numbers. In a way that is hard to forsee, predict or influence like the listed below. I am not looking for ways to manipulate real-world data like freckles, hair, second hands, books or digits of pi. My methods so far Mod 3: I think of the first two digit numbers that come into my head, multiply them together and if the result is $1$ mod $3$ I say heads, if $2$ mod $3$ I say tails and if $0$ mod $3$ I go again. This seems to work quite well but now I know how the game works it is quite hard to not overinfluence my choices without picking very large numbers. Collatz down I think of a number quickly and then perform the collatz operations on it, halfing if even and tripling then adding one if odd. If it takes an odd number of steps to reach $1$ I say heads and if even I say tails. I worry this doesn't yield heads $50\%$ of the time however. Penultimate digit I think of two $2$ digit numbers and multiply them together. If the tens digit is even I say heads, if it is odd I say tails. Can someone think of other quick methods that are hard to influence?","This question already has answers here : How to mentally flip a coin? (10 answers) Closed 2 years ago . The community reviewed whether to reopen this question 2 years ago and left it closed: Original close reason(s) were not resolved This is quite a soft question but hopefully, this community has some nice answers. What is a good way to ""toss a coin"" in your head? That is an ""algorithm"" to generate heads of the time roughly that is hard for you to influence without thinking too deeply. I am looking for little mathematical tricks to ""randomise"" and scramble ones ability to select a handful or manageable numbers. In a way that is hard to forsee, predict or influence like the listed below. I am not looking for ways to manipulate real-world data like freckles, hair, second hands, books or digits of pi. My methods so far Mod 3: I think of the first two digit numbers that come into my head, multiply them together and if the result is mod I say heads, if mod I say tails and if mod I go again. This seems to work quite well but now I know how the game works it is quite hard to not overinfluence my choices without picking very large numbers. Collatz down I think of a number quickly and then perform the collatz operations on it, halfing if even and tripling then adding one if odd. If it takes an odd number of steps to reach I say heads and if even I say tails. I worry this doesn't yield heads of the time however. Penultimate digit I think of two digit numbers and multiply them together. If the tens digit is even I say heads, if it is odd I say tails. Can someone think of other quick methods that are hard to influence?",50/50 1 3 2 3 0 3 1 50\% 2,"['probability', 'soft-question']"
26,"Modified coupon collector's problem, where you can trade off excess coupons","Modified coupon collector's problem, where you can trade off excess coupons",,"I am struggling with a variation of the coupon collector's problem. Suppose that we need to collect $n$ distinct coupons by buying boxes of toys, where each box contains one coupon with uniform probability. The manufacturer however allows customers to trade any ten coupons (not necessarily the same) for any one coupon. Now suppose that my strategy is to keep buying boxes and hoarding coupons, until I can trade all my excess coupons to get an entire set of $n$ distinct coupons. What is then the expectation of the number of boxes that I need to buy? To be more precise, let $X_i$ be the number of unique coupons after buying $i$ boxes, and let $Y_i = i - X_i$ . Let $I = \min\{i\in \mathbb{N}:X_i + \frac{Y_i}{10} \geq n\}$ . I am then asking for $\mathbb{E}(I)$ . (Edit: cleaned up the notations in the last part)","I am struggling with a variation of the coupon collector's problem. Suppose that we need to collect distinct coupons by buying boxes of toys, where each box contains one coupon with uniform probability. The manufacturer however allows customers to trade any ten coupons (not necessarily the same) for any one coupon. Now suppose that my strategy is to keep buying boxes and hoarding coupons, until I can trade all my excess coupons to get an entire set of distinct coupons. What is then the expectation of the number of boxes that I need to buy? To be more precise, let be the number of unique coupons after buying boxes, and let . Let . I am then asking for . (Edit: cleaned up the notations in the last part)",n n X_i i Y_i = i - X_i I = \min\{i\in \mathbb{N}:X_i + \frac{Y_i}{10} \geq n\} \mathbb{E}(I),"['probability', 'coupon-collector']"
27,Distribution of Sum of Sample Mean and Sample Variance from a Normal Population.,Distribution of Sum of Sample Mean and Sample Variance from a Normal Population.,,"Let $X_i\sim^{iid} N(\mu, \sigma^2)$ . Let $\bar X$ and $S^2$ denote the usual, resp, sample mean and sample variance. What is the distribution of $\bar X+S^2$ ? Since we know that the sample mean and sample variance are independent in a Normal population, I guess my question would be equivalent to asking what is the resulting distribution of independent Normal distribution and a Chi-Squared distribution.","Let . Let and denote the usual, resp, sample mean and sample variance. What is the distribution of ? Since we know that the sample mean and sample variance are independent in a Normal population, I guess my question would be equivalent to asking what is the resulting distribution of independent Normal distribution and a Chi-Squared distribution.","X_i\sim^{iid} N(\mu, \sigma^2) \bar X S^2 \bar X+S^2","['probability', 'statistics', 'probability-distributions', 'normal-distribution', 'sampling']"
28,Coupon collectors derivation,Coupon collectors derivation,,"Yesterday, a user posted the following derivation of the coupon collectors problem but unfortunately deleted his question: There are $n$ distinct characters one could obtain from a card package, whereby each card package you buy (one such package contains exactly one card) contains each of those characters with probability $\frac{1}{n}$ . Compute the expected number of card packages you need to buy so that you have each of the $n$ distinct characters at least once. Let $\Omega$ be our sample space and $X\colon \Omega\to \mathbb{R}$ the random variable that counts the number of card packages we have to buy in order to obtain each of the $n$ distinct characters at least once. Moreover, let $X_i$ be an indicator variable with $$ X_i = \begin{cases} 	1, \ \text{the $i$th character we obtain that is distinct to all 	others we have collected so far}\\ 	0, \ \text{otherwise} \end{cases} $$ Now we find that $P(X_i) = 1-\frac{i-1}{n} = \frac{n-i+1}{n}$ since the probability that we get one of the $i-1$ characters that we have already is $i-\frac{1}{n}$ . Moreover, it's clear that $\Omega= X_1 \uplus X_1^c$ where $X_1^c$ denotes the complementary event of $X_1$ . Now I used the total expectation theorem to obtain \begin{align*} 	\mathbb{E}[X] = (\mathbb{E}[X|X_1] + 1)\cdot P(X_1) +  	(\mathbb{E}[X|X_1^{c}] + 1) \cdot P(X_1^{c}) .\end{align*} whereby the $+1$ denotes the fact that we have bought a card package. We notice that $\mathbb{E}[X|X_i^{c}] = \mathbb{E}[X]$ since we haven't obtained any new card. With this we obtain \begin{align*} 	&\mathbb{E}[X] = \mathbb{E}[X|X_1]\cdot P(X_1) + P(X_1) 	+ \mathbb{E}[X]\cdot  P(X_1^{c})+ P(X_1^{c}) 	\\[15pt] 	\iff &\mathbb{E}[X]\cdot (1-P(X_1^{c})) = \mathbb{E}[X|X_1]\cdot P(X_1) + P(X_1) 	+ P(X_1^{c}) .\end{align*} Since $P(X_1^{c}) = 1-P(X_1)$ this becomes \begin{align*} 	\mathbb{E}[X] \cdot P(X_1) = \mathbb{E}[X|X_1] \cdot P(X_1) + 1 	\iff \mathbb{E}[X] = \mathbb{E}[X|X_1] + \frac{1}{P(X_1)} 	= E[X|X_1] + \frac{n}{1} .\end{align*} We can repeat this procedure since $X_2 \uplus X_2^{c}$ is again a disjoint partition of our sample space so we obtain \begin{align*} 	\mathbb{E}[X] = \mathbb{E}[X|X_1, X_2] + \frac{n}{2} + \frac{n}{1} .\end{align*} I would now argue inductively that \begin{align*} 	\mathbb{E}[X] = \mathbb{E}[X|X_1, X_2, \ldots, X_{i}] + \sum_{k = 1}^{i} \frac{n}{i} .\end{align*} so in the end one has \begin{align*} 	\mathbb{E}[X] = \sum_{k =  1}^{n} \frac{n}{k} =  n\sum_{k = 1}^{n}\frac{1}{k} \end{align*} since the ""recursion"" ends with $\mathbb{E}[X|X_1, \ldots, X_n] = 0$ the ""recursion"" ends. My problem is the $+1$ he has in the application of the total expectation, which is intuitively clear to me but how would one explain this formally since the total expectation theorem actually would look like $$ \mathbb{E}[X] = \mathbb{E}[X|X_1] \cdot P(X_1) + \mathbb{E}[X|X_1^{c}] \cdot (1-P(X_1)) $$ without any $1$ 's, so how could one make this more rigorous? How does one explain $\mathbb{E}[X] = \mathbb{E}[X|X_1^{c}]$ rigorously? Edit: I think it suffices for the latter to just say that since $X_1^c$ means that we haven't obtained any new card this doesn't influence the expected value of $X$ .","Yesterday, a user posted the following derivation of the coupon collectors problem but unfortunately deleted his question: There are distinct characters one could obtain from a card package, whereby each card package you buy (one such package contains exactly one card) contains each of those characters with probability . Compute the expected number of card packages you need to buy so that you have each of the distinct characters at least once. Let be our sample space and the random variable that counts the number of card packages we have to buy in order to obtain each of the distinct characters at least once. Moreover, let be an indicator variable with Now we find that since the probability that we get one of the characters that we have already is . Moreover, it's clear that where denotes the complementary event of . Now I used the total expectation theorem to obtain whereby the denotes the fact that we have bought a card package. We notice that since we haven't obtained any new card. With this we obtain Since this becomes We can repeat this procedure since is again a disjoint partition of our sample space so we obtain I would now argue inductively that so in the end one has since the ""recursion"" ends with the ""recursion"" ends. My problem is the he has in the application of the total expectation, which is intuitively clear to me but how would one explain this formally since the total expectation theorem actually would look like without any 's, so how could one make this more rigorous? How does one explain rigorously? Edit: I think it suffices for the latter to just say that since means that we haven't obtained any new card this doesn't influence the expected value of .","n \frac{1}{n} n \Omega X\colon \Omega\to \mathbb{R} n X_i 
X_i = \begin{cases}
	1, \ \text{the ith character we obtain that is distinct to all
	others we have collected so far}\\
	0, \ \text{otherwise}
\end{cases}
 P(X_i) = 1-\frac{i-1}{n} = \frac{n-i+1}{n} i-1 i-\frac{1}{n} \Omega= X_1 \uplus X_1^c X_1^c X_1 \begin{align*}
	\mathbb{E}[X] = (\mathbb{E}[X|X_1] + 1)\cdot P(X_1) + 
	(\mathbb{E}[X|X_1^{c}] + 1) \cdot P(X_1^{c})
.\end{align*} +1 \mathbb{E}[X|X_i^{c}] = \mathbb{E}[X] \begin{align*}
	&\mathbb{E}[X] = \mathbb{E}[X|X_1]\cdot P(X_1) + P(X_1)
	+ \mathbb{E}[X]\cdot  P(X_1^{c})+ P(X_1^{c})
	\\[15pt]
	\iff &\mathbb{E}[X]\cdot (1-P(X_1^{c})) = \mathbb{E}[X|X_1]\cdot P(X_1) + P(X_1)
	+ P(X_1^{c})
.\end{align*} P(X_1^{c}) = 1-P(X_1) \begin{align*}
	\mathbb{E}[X] \cdot P(X_1) = \mathbb{E}[X|X_1] \cdot P(X_1) + 1
	\iff \mathbb{E}[X] = \mathbb{E}[X|X_1] + \frac{1}{P(X_1)}
	= E[X|X_1] + \frac{n}{1}
.\end{align*} X_2 \uplus X_2^{c} \begin{align*}
	\mathbb{E}[X] = \mathbb{E}[X|X_1, X_2] + \frac{n}{2} + \frac{n}{1}
.\end{align*} \begin{align*}
	\mathbb{E}[X] = \mathbb{E}[X|X_1, X_2, \ldots, X_{i}] + \sum_{k = 1}^{i} \frac{n}{i}
.\end{align*} \begin{align*}
	\mathbb{E}[X] = \sum_{k =  1}^{n} \frac{n}{k} =  n\sum_{k = 1}^{n}\frac{1}{k}
\end{align*} \mathbb{E}[X|X_1, \ldots, X_n] = 0 +1 
\mathbb{E}[X] = \mathbb{E}[X|X_1] \cdot P(X_1) + \mathbb{E}[X|X_1^{c}]
\cdot (1-P(X_1))
 1 \mathbb{E}[X] = \mathbb{E}[X|X_1^{c}] X_1^c X","['probability', 'combinatorics', 'solution-verification', 'coupon-collector']"
29,Rigorous justification for this conditional probability inequality?,Rigorous justification for this conditional probability inequality?,,"Let $(\Omega,\mathcal F,P)$ be a probability space. Consider a collection of bounded real random variables $X(\gamma)$ , for $\gamma\in[0,1]$ , defined on this probability space. Let $(\gamma_i)_{i=1}^\infty$ be a sequence of iid random variables taking values in $[0,1]$ . The family $\{\gamma_i : i \in \mathbb{N}\}$ is assumed to be independent of the family $\{X(\gamma) : \gamma \in [0,1]\}$ . Consider the following inequality for a given $\delta>0$ : $$\sup_{i\in\mathbb{N}}P\bigg(\Big|X(\gamma_i)-E[X(\gamma_i)|\sigma(\gamma_i)]\Big|>\delta \bigg | \sigma(\gamma_i) \bigg)\leq \sup_{\gamma \in [0,1]}P\bigg(\Big|X(\gamma)-E[X(\gamma)]\Big|>\delta \bigg) .$$ It seems to me that this inequality is true, since once we condition on $\gamma_i$ , the $i$ th probability on the left must appear on the right as well. But how to show it rigorously? Any help on this is very appreciated. EDIT: In fact I would be happy if someone could just give a rigorous proof of the statement $$E[X(\gamma_i)|\sigma(\gamma_i)(\omega)=E[X(\gamma_i(\omega)) ,\quad \omega\in\Omega.$$","Let be a probability space. Consider a collection of bounded real random variables , for , defined on this probability space. Let be a sequence of iid random variables taking values in . The family is assumed to be independent of the family . Consider the following inequality for a given : It seems to me that this inequality is true, since once we condition on , the th probability on the left must appear on the right as well. But how to show it rigorously? Any help on this is very appreciated. EDIT: In fact I would be happy if someone could just give a rigorous proof of the statement","(\Omega,\mathcal F,P) X(\gamma) \gamma\in[0,1] (\gamma_i)_{i=1}^\infty [0,1] \{\gamma_i : i \in \mathbb{N}\} \{X(\gamma) : \gamma \in [0,1]\} \delta>0 \sup_{i\in\mathbb{N}}P\bigg(\Big|X(\gamma_i)-E[X(\gamma_i)|\sigma(\gamma_i)]\Big|>\delta \bigg | \sigma(\gamma_i) \bigg)\leq \sup_{\gamma \in [0,1]}P\bigg(\Big|X(\gamma)-E[X(\gamma)]\Big|>\delta \bigg) . \gamma_i i E[X(\gamma_i)|\sigma(\gamma_i)(\omega)=E[X(\gamma_i(\omega)) ,\quad \omega\in\Omega.","['probability', 'probability-theory', 'measure-theory', 'conditional-probability', 'conditional-expectation']"
30,Physical disasters and probability,Physical disasters and probability,,"An urban area is susceptible to one earthquake per year with probability $30 \%$ and two with probability $5 \%$ , also three earthquakes per year is impossible. The same area is also susceptible to floods, which may result from heavy rain (a fact which occurs with probability $100 \%$ within a year) or from a Dam failure due to the earthquake (event occurring with probability $25 \%$ in a year). The two events (floods from rain and floods from dam damage) are considered to be independent. Calculate the probability of a flood in a year (for the specific area). I know it can be solved by using Bayes theorem but am not sure on how to use it. $P(\text{heavy_rain}) = 1$ $P(\text{dam_failure_from_earthquake}) = 0.25$ $P(\text{1_earthquake}) = 0.3$ $P(\text{2_earthquakes}) = 0.05$ $P(\text{3_earthquakes}) = 0.00$ $P(\text{floods}) = P(\text{floods_from_rain})+P(\text{floods_from_dam})$ $P(\text{floods_from_rain}) = 1$ but I don't know how to continue.","An urban area is susceptible to one earthquake per year with probability and two with probability , also three earthquakes per year is impossible. The same area is also susceptible to floods, which may result from heavy rain (a fact which occurs with probability within a year) or from a Dam failure due to the earthquake (event occurring with probability in a year). The two events (floods from rain and floods from dam damage) are considered to be independent. Calculate the probability of a flood in a year (for the specific area). I know it can be solved by using Bayes theorem but am not sure on how to use it. but I don't know how to continue.",30 \% 5 \% 100 \% 25 \% P(\text{heavy_rain}) = 1 P(\text{dam_failure_from_earthquake}) = 0.25 P(\text{1_earthquake}) = 0.3 P(\text{2_earthquakes}) = 0.05 P(\text{3_earthquakes}) = 0.00 P(\text{floods}) = P(\text{floods_from_rain})+P(\text{floods_from_dam}) P(\text{floods_from_rain}) = 1,"['probability', 'bayes-theorem']"
31,Why do my Stack Exchange reps follow a power law?,Why do my Stack Exchange reps follow a power law?,,"I noticed a pattern while looking at my network profile the other day, and I'm wondering if it's a fluke, or if there is something deep to it. My reps for my top five Stack Exchange communities follow a inverse-square power law pretty neatly.  (Red dots are data; blue curve is a perfect inverse-square law.) The ratio of my top SE rep to subsequent ones is 1, 4.02, 8.86, 16.58, 21.15.  All but the last are within a few percent of the expected 1, 4, 9, 16, 25 for an inverse-square law.  I'm not a statistician, so I have the following questions: How can I go about testing whether this is a fluke or not? Is there a universality argument for why this power law might arise? Some remarks: If you go further down my list of SE reps, the power law disappears, because it mostly consists of sites I've only visited or maybe posted once on.  All power laws have a cutoff, though, so this is unsurprising. I've looked at some other user profiles, and there seem to be different classes of users.  Many are only active on one site, and there is no power law there.  Others are active on multiple sites and do seem to have superficially similar statistics to mine, but I don't know enough to test whether any of this is statistically significant.","I noticed a pattern while looking at my network profile the other day, and I'm wondering if it's a fluke, or if there is something deep to it. My reps for my top five Stack Exchange communities follow a inverse-square power law pretty neatly.  (Red dots are data; blue curve is a perfect inverse-square law.) The ratio of my top SE rep to subsequent ones is 1, 4.02, 8.86, 16.58, 21.15.  All but the last are within a few percent of the expected 1, 4, 9, 16, 25 for an inverse-square law.  I'm not a statistician, so I have the following questions: How can I go about testing whether this is a fluke or not? Is there a universality argument for why this power law might arise? Some remarks: If you go further down my list of SE reps, the power law disappears, because it mostly consists of sites I've only visited or maybe posted once on.  All power laws have a cutoff, though, so this is unsurprising. I've looked at some other user profiles, and there seem to be different classes of users.  Many are only active on one site, and there is no power law there.  Others are active on multiple sites and do seem to have superficially similar statistics to mine, but I don't know enough to test whether any of this is statistically significant.",,"['probability', 'statistics', 'applications', 'probability-limit-theorems']"
32,Is this Markov chain recurrent or transient?,Is this Markov chain recurrent or transient?,,"A Markov chain $X_n$ $(n\ge1)$ with state space S=0,1,2,3,... has the following transition probabilities: $$p_{i,i+1}=\frac{(i+1)^2}{2i^2+2i+1},\ p_{i,i-1}=\frac{i^2}{2i^2+2i+1}, \ p_{0,1}=1, \ and \ i\ge 1$$ I am trying to find if it is recurrent or transient. What I have is the following: Obviously, $p_{i,i+1}$ and $p_{i,i-1}$ depends on $i$ , it means that each state will have a different probability. For example, when $i=1$ , then $p_{1,2}=4/5$ and $p_{1,0}=1/5$ . So, it seems that $p_{i,i+1}$ is decreasing as $p_{i,i-1}$ increases on each step. $$T=\pmatrix{0&1&0&0&...&0&0&0\\1/5&0&4/5&0&...&0&0&0\\0&4/13&0&9/13&...&0&0&0\\\vdots&\vdots&\vdots&\vdots&\ddots&\vdots&\vdots&\vdots\\0&0&0&0&...&1/2&0&1/2}_{nxn}$$ According to wiki , a state i is recurrent if and only if the expected number of visits to i is infinite: $$ \sum_{n=0}^{\infty}p_{ii}^{(n)}=\infty\\ p_{ii}=Pr(X_n=i|X_0=0)\ for\ n\ge 1$$ for a recurrent state i, the mean hitting time is defined as: $$M_i=E\left[ T_i \right]=\sum_{n=1}^{\infty}nf_{ii}^{(n)}$$ State i is positive recurrent (or non-null persistent) if Mi is finite; otherwise, state i is null recurrent (or null persistent). If this Markov chain had a finite state space: I think that I should be positive recurrent because it is possible to get to any state from any state. How does the infinite state space change this fact? I haven't found good information about it. What about this? If we start from zero, it seems that the MC may return to zero almost surely with probability 1. Thank you!","A Markov chain with state space S=0,1,2,3,... has the following transition probabilities: I am trying to find if it is recurrent or transient. What I have is the following: Obviously, and depends on , it means that each state will have a different probability. For example, when , then and . So, it seems that is decreasing as increases on each step. According to wiki , a state i is recurrent if and only if the expected number of visits to i is infinite: for a recurrent state i, the mean hitting time is defined as: State i is positive recurrent (or non-null persistent) if Mi is finite; otherwise, state i is null recurrent (or null persistent). If this Markov chain had a finite state space: I think that I should be positive recurrent because it is possible to get to any state from any state. How does the infinite state space change this fact? I haven't found good information about it. What about this? If we start from zero, it seems that the MC may return to zero almost surely with probability 1. Thank you!","X_n (n\ge1) p_{i,i+1}=\frac{(i+1)^2}{2i^2+2i+1},\ p_{i,i-1}=\frac{i^2}{2i^2+2i+1}, \ p_{0,1}=1, \ and \ i\ge 1 p_{i,i+1} p_{i,i-1} i i=1 p_{1,2}=4/5 p_{1,0}=1/5 p_{i,i+1} p_{i,i-1} T=\pmatrix{0&1&0&0&...&0&0&0\\1/5&0&4/5&0&...&0&0&0\\0&4/13&0&9/13&...&0&0&0\\\vdots&\vdots&\vdots&\vdots&\ddots&\vdots&\vdots&\vdots\\0&0&0&0&...&1/2&0&1/2}_{nxn}  \sum_{n=0}^{\infty}p_{ii}^{(n)}=\infty\\ p_{ii}=Pr(X_n=i|X_0=0)\ for\ n\ge 1 M_i=E\left[ T_i \right]=\sum_{n=1}^{\infty}nf_{ii}^{(n)}","['probability', 'markov-chains', 'ergodic-theory']"
33,What does the limit of the characteristic function $\varphi(x)\xrightarrow{\lvert x\rvert\rightarrow \infty} 0$ tell us about $\mu(\{a\})$?,What does the limit of the characteristic function  tell us about ?,\varphi(x)\xrightarrow{\lvert x\rvert\rightarrow \infty} 0 \mu(\{a\}),"If we know that $\varphi(x)\xrightarrow{\lvert x\rvert\rightarrow \infty} 0$ for the characteristic function $\varphi$ of the probability measure $\mu$ on $\mathbb{R}$ , what does that tell us about $\mu(\{a\})$ for any $a$ ? Because of the identity $$\mu(\{a\}) = \lim_{T\rightarrow\infty}\frac 1{2T}\int_{-T}^Te^{-ita}\varphi(t) dt$$ I think it should be $\mu(\{a\}) = 0$ for all $a$ . But I don't know how to formalize this idea, if it's true. Edit: A TA confirmed it is true that under the assumption $\mu(\{a\})=0$ for all $a$ , but I still don't even know how to start a possible proof/what the key insight is. Edit2: I have successfully shown that $$\lim_{T\rightarrow\infty}\frac 1{2T}\int_{-T}^T\varphi^2(t)dt = \sum_{x\in\mathbb{R}}(\mu(x))^2$$ But I need to show that the LHS is $0$ . It is not trivial to show that the integral is even finite in the limit, how can I show that the LHS is finite?","If we know that for the characteristic function of the probability measure on , what does that tell us about for any ? Because of the identity I think it should be for all . But I don't know how to formalize this idea, if it's true. Edit: A TA confirmed it is true that under the assumption for all , but I still don't even know how to start a possible proof/what the key insight is. Edit2: I have successfully shown that But I need to show that the LHS is . It is not trivial to show that the integral is even finite in the limit, how can I show that the LHS is finite?",\varphi(x)\xrightarrow{\lvert x\rvert\rightarrow \infty} 0 \varphi \mu \mathbb{R} \mu(\{a\}) a \mu(\{a\}) = \lim_{T\rightarrow\infty}\frac 1{2T}\int_{-T}^Te^{-ita}\varphi(t) dt \mu(\{a\}) = 0 a \mu(\{a\})=0 a \lim_{T\rightarrow\infty}\frac 1{2T}\int_{-T}^T\varphi^2(t)dt = \sum_{x\in\mathbb{R}}(\mu(x))^2 0,"['probability', 'probability-theory', 'measure-theory', 'characteristic-functions']"
34,Can we rely on Confidence Intervals?,Can we rely on Confidence Intervals?,,"Suppose the mean is in (7.6,8.4) with 95% confidence. I understand that this means 95% of the confidence intervals from different samples will contain the population mean. But, what is the significance of this particular interval on its own . Since I am sure that 95% of sampling intervals will contain the mean can I be somewhat certain that this interval is one of them? If not, how is this interval useful at all to me? In other words, how sure can I be that mean is in (7.6,8.4) and if I can't be sure then what's the use of this?","Suppose the mean is in (7.6,8.4) with 95% confidence. I understand that this means 95% of the confidence intervals from different samples will contain the population mean. But, what is the significance of this particular interval on its own . Since I am sure that 95% of sampling intervals will contain the mean can I be somewhat certain that this interval is one of them? If not, how is this interval useful at all to me? In other words, how sure can I be that mean is in (7.6,8.4) and if I can't be sure then what's the use of this?",,"['probability', 'statistics', 'intuition', 'means', 'confidence-interval']"
35,Probability/Combinatorics,Probability/Combinatorics,,"The Question is: An urn contains 6 black and an unknown number ( $\le6$ ) of white balls. Three balls are drawn one by one without replacement and all of them are found to be white. What is the probability that the next ball drawn is black? I propose the following solution: We calculate the probability that there are r white balls in the urn using Bayes' Theorem. So this will be: $$ f(r) = \frac{\frac{{r}\choose{3}}{{6+r}\choose{3}}}{\sum_{r=3}^6\frac{{r}\choose{3}}{{6+r}\choose{3}} }$$ where f(r) denotes the probability that their are r white balls. Now, if their were r white balls, the probability that the next draw will be black is $g(r) = \frac{6}{6 + (r-3)} = \frac{6}{3+r}$ , because 3 white balls have already been picked. Now to get the total probability that the next ball drawn is black can be given by: $$ \sum_{r=3}^6 f(r)\cdot g(r)$$ Is this approach correct? The answer obtained (I calculated using a python script) does match in decimal value with one of the options ( $\frac{677}{909})$ which is equal to 0.744. If it is correct, I would like to know if there is a more elegant/less calculative way of approaching this question. Thanks!","The Question is: An urn contains 6 black and an unknown number ( ) of white balls. Three balls are drawn one by one without replacement and all of them are found to be white. What is the probability that the next ball drawn is black? I propose the following solution: We calculate the probability that there are r white balls in the urn using Bayes' Theorem. So this will be: where f(r) denotes the probability that their are r white balls. Now, if their were r white balls, the probability that the next draw will be black is , because 3 white balls have already been picked. Now to get the total probability that the next ball drawn is black can be given by: Is this approach correct? The answer obtained (I calculated using a python script) does match in decimal value with one of the options ( which is equal to 0.744. If it is correct, I would like to know if there is a more elegant/less calculative way of approaching this question. Thanks!",\le6  f(r) = \frac{\frac{{r}\choose{3}}{{6+r}\choose{3}}}{\sum_{r=3}^6\frac{{r}\choose{3}}{{6+r}\choose{3}} } g(r) = \frac{6}{6 + (r-3)} = \frac{6}{3+r}  \sum_{r=3}^6 f(r)\cdot g(r) \frac{677}{909}),"['probability', 'combinatorics']"
36,Probability of majority vote to be correct,Probability of majority vote to be correct,,"Let $X$ be a random variable taking values from $[k] = \{1, 2, ..., k\}$ with probabilities $p_1, ..., p_k$ , respectively. Suppose that $X$ is slightly more likely to be 1: there exists some $\epsilon > 0$ such that for all $1 < i \leq k$ , $p_1 - p_i \geq \epsilon$ . Now, suppose we have $n$ independent copies of $X$ : $X_1, X_2, ..., X_n$ . For each $j \in [k]$ , define the random variable $Y_j$ to be the ""number of votes"" for $j$ : $Y_j := |\{t \in [n] : X_t = j\}|$ . Define the majority random variable $M$ to be the ""winner candidate"", i.e. the arg-max of $Y_1,...,Y_k$ (if there is more than a single maximizer, $M$ equals one of them arbitrarily. In order to make $M$ well defined, assume it equals the smallest such index). I want to bound the probability that $M \neq 1$ . For $k=2$ the problem is well-known and an exponential bound is not difficult to obtain. My attempt I am not sure about it at all, but this is what I tried. From the union bound, $$\Pr[M \neq 1] \leq \Pr[\exists i\neq 1: Y_i > Y_1] \leq \sum_{i=2}^k \Pr[Y_i > Y_1] \text{ ,}$$ And by the law of total probability, $$\Pr[Y_i > Y_1] = \sum_{t=0}^n \Pr[Y_i >Y_1 | Y_i + Y_1 = t]\Pr[Y_1 + Y_i = t]$$ Now $\Pr[Y_i+Y_1 = t]$ is like Binomial random variable with success probability $p_1 + p_i$ , which is smaller than $2p_1 -\epsilon$ by the assumption on $X$ . Thus, $\Pr[Y_1 + Y_i = t] \leq {n \choose t}(2p_1 - \epsilon)^t (1-2p_1 + \epsilon)^{n-t}$ . Furthermore, $\Pr[Y_i >Y_1 | Y_i + Y_1 = t] = \Pr[Y_1 \leq t/2 - 1 | Y_1 + Y_i = t]$ . I think that this is like asking what is the probability that a Binomial random variable $B(t, p_1)$ is smaller than $t/2$ . I can bound it using Hoeffding's inequality: $$ \Pr[Y_i >Y_1 | Y_i + Y_1 = t] \leq e^{-2t(p_1 - 1/2)^2}.$$ Then I can combine the two results and conclude that $$\Pr[M \neq 1] \leq (k-1)  \sum_{t=0}^n e^{-2t(p_1 - 1/2)^2} {n \choose t}(2p_1 - \epsilon)^t (1-2p_1 + \epsilon)^{n-t}. $$ My issue with this solution (beyond just not being sure if that's right) is that if $p_1 = 1/2$ I would expect the majority to be $1$ with overwhelming probability, but this bound does not capture this behaviour, which makes me trust it even less.","Let be a random variable taking values from with probabilities , respectively. Suppose that is slightly more likely to be 1: there exists some such that for all , . Now, suppose we have independent copies of : . For each , define the random variable to be the ""number of votes"" for : . Define the majority random variable to be the ""winner candidate"", i.e. the arg-max of (if there is more than a single maximizer, equals one of them arbitrarily. In order to make well defined, assume it equals the smallest such index). I want to bound the probability that . For the problem is well-known and an exponential bound is not difficult to obtain. My attempt I am not sure about it at all, but this is what I tried. From the union bound, And by the law of total probability, Now is like Binomial random variable with success probability , which is smaller than by the assumption on . Thus, . Furthermore, . I think that this is like asking what is the probability that a Binomial random variable is smaller than . I can bound it using Hoeffding's inequality: Then I can combine the two results and conclude that My issue with this solution (beyond just not being sure if that's right) is that if I would expect the majority to be with overwhelming probability, but this bound does not capture this behaviour, which makes me trust it even less.","X [k] = \{1, 2, ..., k\} p_1, ..., p_k X \epsilon > 0 1 < i \leq k p_1 - p_i \geq \epsilon n X X_1, X_2, ..., X_n j \in [k] Y_j j Y_j := |\{t \in [n] : X_t = j\}| M Y_1,...,Y_k M M M \neq 1 k=2 \Pr[M \neq 1] \leq \Pr[\exists i\neq 1: Y_i > Y_1] \leq \sum_{i=2}^k \Pr[Y_i > Y_1] \text{ ,} \Pr[Y_i > Y_1] = \sum_{t=0}^n \Pr[Y_i >Y_1 | Y_i + Y_1 = t]\Pr[Y_1 + Y_i = t] \Pr[Y_i+Y_1 = t] p_1 + p_i 2p_1 -\epsilon X \Pr[Y_1 + Y_i = t] \leq {n \choose t}(2p_1 - \epsilon)^t (1-2p_1 + \epsilon)^{n-t} \Pr[Y_i >Y_1 | Y_i + Y_1 = t] = \Pr[Y_1 \leq t/2 - 1 | Y_1 + Y_i = t] B(t, p_1) t/2  \Pr[Y_i >Y_1 | Y_i + Y_1 = t] \leq e^{-2t(p_1 - 1/2)^2}. \Pr[M \neq 1] \leq (k-1)  \sum_{t=0}^n e^{-2t(p_1 - 1/2)^2} {n \choose t}(2p_1 - \epsilon)^t (1-2p_1 + \epsilon)^{n-t}.  p_1 = 1/2 1","['probability', 'probability-theory', 'random-variables', 'binomial-coefficients', 'binomial-distribution']"
37,When you can look through a forest?,When you can look through a forest?,,"Yesterday I was coming back home by train through various forests and I realised, that through some of them one can see to the other side and through some of them not. Can this be formalised? That is, what is the chance of seeing what is on the other side of the forest depending on the size of the forest, average number of trees per unit area and the diameter of their trunk?","Yesterday I was coming back home by train through various forests and I realised, that through some of them one can see to the other side and through some of them not. Can this be formalised? That is, what is the chance of seeing what is on the other side of the forest depending on the size of the forest, average number of trees per unit area and the diameter of their trunk?",,"['probability', 'poisson-distribution']"
38,The conditional probability and the expected value for a sequence of independent variables,The conditional probability and the expected value for a sequence of independent variables,,"$X_1, X_2, \dots $ is sequences of independent random variables with a value in $\{ 0,1\}$ s.t $p(X_i=1)=p,~~ \text{for}~~ i \geq 1 $ where $p \in (0,1)$ . Assume now we have another sequence $(Y_i )_{i \geq 1}$ of independent random variables with a value in $\{ 0,1\}$ such that $ p (Y_i = 1) = q~$ and also $ q  \in (0, 1).$ Knowing that $U_n = \sum_{i=1}^n X_i$ , $V_n = \sum_{i=1}^n X_i Y_i$ and $N = \inf\{n \geq 0, V_{n+1} = 1\}$ , I would like to find $E(U_n)$ and the probability $p(U_n = i / N=n )$ . I just see let and assume, I don't know from where to start so any kind of help would be appreciated. From the comments below, $U_n$ distirbute Binomial $B(n,p)$ , the same distribution for $V_n$ as $B(n,pq)$ and $N$ distributes geometric $(pq)$ . How could I use all of this to find the conditional probability? \begin{eqnarray} p(U_n = i / N=n ) &=& \frac{p(U_n = i, N=n)}{p(N=n)} ~~\text{but}~~p(U_n = i, N=n)=unknown \nonumber \\ &=& \frac{p(N=n/ U_n = i)p(U_n=i)}{p(N=n)}~~ \text{how to find}~~ p(N=n/ U_n = i)p(U_n=i) \end{eqnarray}","is sequences of independent random variables with a value in s.t where . Assume now we have another sequence of independent random variables with a value in such that and also Knowing that , and , I would like to find and the probability . I just see let and assume, I don't know from where to start so any kind of help would be appreciated. From the comments below, distirbute Binomial , the same distribution for as and distributes geometric . How could I use all of this to find the conditional probability?","X_1, X_2, \dots  \{ 0,1\} p(X_i=1)=p,~~ \text{for}~~ i \geq 1  p \in (0,1) (Y_i )_{i \geq 1} \{ 0,1\}  p (Y_i = 1) = q~  q  \in (0, 1). U_n = \sum_{i=1}^n X_i V_n = \sum_{i=1}^n X_i Y_i N = \inf\{n \geq 0, V_{n+1} = 1\} E(U_n) p(U_n = i / N=n ) U_n B(n,p) V_n B(n,pq) N (pq) \begin{eqnarray}
p(U_n = i / N=n ) &=& \frac{p(U_n = i, N=n)}{p(N=n)} ~~\text{but}~~p(U_n = i, N=n)=unknown \nonumber \\
&=& \frac{p(N=n/ U_n = i)p(U_n=i)}{p(N=n)}~~ \text{how to find}~~ p(N=n/ U_n = i)p(U_n=i)
\end{eqnarray}","['probability', 'probability-theory', 'probability-distributions', 'conditional-probability', 'expected-value']"
39,The conditional probability when some random variables are independent,The conditional probability when some random variables are independent,,"For $n \geq 1$ ,  let $X_1, X_2, \dots, X_n$ be a sequence of independent random variables each with a value equals $1$ or $0$ with probability $p$ and $q$ , respectively. If $N = \inf \{n \geq 0, Z_{n+1} = 1 \}$ such that $Z_n = \sum_{i=1}^n X_i Y_i$ and $Y_1, \dots, Y_n$ is a sequence of random variables defined exactly as the first sequence above ""sequence of $X_i$ "" and independent of it. Show that: $1.~~ p(\cap_{i=1}^n  (X_i=x_i) /N=n) = \prod_{i=1}^n p(X_i=x_i /N=n).$ $2.~~ \forall i \in[1,n],~~p(X_i = 1/ N=n) = p(X_i=1 /X_iY_i=0) = \frac{p(1-q)}{1-pq}.$ $\Longrightarrow$ I tried with the first part as \begin{eqnarray} p(\cap_{i=1}^n  (X_i=x_i) /N=n) &=& p((X_1=x_1 \cap X_2=x_2 \dots \cap X_n=x_n)/N=n)\\ &=& \frac{p((X_1=x_1 \cap X_2=x_2 \dots \cap X_n=x_n) \cap N=n)}{p(N=n)}\\ \text{after that I should say }\\ &=& p((X_1=x_1) /N=n) p((X_2=x_2) /N=n) \dots p((X_n=x_n) /N=n)\\ &=&  \prod_{i=1}^n p(X_i=x_i /N=n) \end{eqnarray} But I don't know how to get this. I know that the sequence is independent but I don't see how I can use this property to arrive to the last two lines. $\Longrightarrow$ For the second part, we know $N \sim Geometric(pq)$ , $Z_n \sim \text{Binomial}(n,pq)$ and $p(X_i =1)=p$ then \begin{eqnarray} p(X_i = 1/ N=n) &=& \frac{p(X_i=1 \cap N=n)}{p(N=n)}\\ &=& \vdots \end{eqnarray} and I don't know also how to continue from here. I really appreciate any help or hint","For ,  let be a sequence of independent random variables each with a value equals or with probability and , respectively. If such that and is a sequence of random variables defined exactly as the first sequence above ""sequence of "" and independent of it. Show that: I tried with the first part as But I don't know how to get this. I know that the sequence is independent but I don't see how I can use this property to arrive to the last two lines. For the second part, we know , and then and I don't know also how to continue from here. I really appreciate any help or hint","n \geq 1 X_1, X_2, \dots, X_n 1 0 p q N = \inf \{n \geq 0, Z_{n+1} = 1 \} Z_n = \sum_{i=1}^n X_i Y_i Y_1, \dots, Y_n X_i 1.~~ p(\cap_{i=1}^n  (X_i=x_i) /N=n) = \prod_{i=1}^n p(X_i=x_i /N=n). 2.~~ \forall i \in[1,n],~~p(X_i = 1/ N=n) = p(X_i=1 /X_iY_i=0) = \frac{p(1-q)}{1-pq}. \Longrightarrow \begin{eqnarray}
p(\cap_{i=1}^n  (X_i=x_i) /N=n) &=& p((X_1=x_1 \cap X_2=x_2 \dots \cap X_n=x_n)/N=n)\\
&=& \frac{p((X_1=x_1 \cap X_2=x_2 \dots \cap X_n=x_n) \cap N=n)}{p(N=n)}\\
\text{after that I should say }\\
&=& p((X_1=x_1) /N=n) p((X_2=x_2) /N=n) \dots p((X_n=x_n) /N=n)\\
&=&  \prod_{i=1}^n p(X_i=x_i /N=n)
\end{eqnarray} \Longrightarrow N \sim Geometric(pq) Z_n \sim \text{Binomial}(n,pq) p(X_i =1)=p \begin{eqnarray}
p(X_i = 1/ N=n) &=& \frac{p(X_i=1 \cap N=n)}{p(N=n)}\\
&=& \vdots
\end{eqnarray}","['probability', 'probability-distributions']"
40,Records and covers,Records and covers,,"Juan is challenging his friend Thomas with the following: Juan has 5 LP vinyl records, each of them of a different band, and asks Thomas to match them with its respective band name, by showing him only the cover photo, not the names etc. Thomas will make a first guess and Juan will reveal him the number of correct matchings but without telling him which ones are correct. If not all his matchings are correct, he has the right to do a second guess, by changing as many as he wants. If he correctly guesses all 5 in these two attempts, then Juan will buy him a nice dinner. What is the probability for Thomas to win the dinner, in each of the below 3 cases: 1) He can recognize the 3 covers but not the other 2. 2) He can recognize the 2 covers but not the other 3. 3) He knows the matching of 2 covers with 2 of the bands but not which is which. Same also for 2 more covers. For example, he knows that cover1 and cover2 correspond to Band1 and Band2 but not which is which, and also he knows that cover3 and cover4 correspond to Band3 and Band4 but not which is which. I can easily tell that the requested probability for 1) is 100%: If he knows 123 and he guesses 45, Juan will reply him “3” as the correct number of matchings. Since Thomas knows 123 are correct, he will say 12354 and this will be correct. For number 3): Thomas must guess 12 or 21, 34 or 43 and he can deduce 5.  We have 4 cases but with the 2nd attempt, in essence we have 6: Suppose the correct choices are 21345: If he says 21345, Juan replies “5” and we are done. If he says 12435, Juan replies “1” and we are also done because Thomas knows it is 21345. If he says 12345, Juan replies “3”. Thomas doesn’t know which are the correct two (he only knows 5). Therefore in the second attempt, he can either say 21345, (correct) or 12435 which is wrong. Same also for 21435: In the second attempt he can either say 21345 (correct) or 12435 (wrong). So the overall probability is 4/6? Is this correct? Can you also help me out with the 2nd case? I started examining the cases but it’s very confusing! Many thanks!","Juan is challenging his friend Thomas with the following: Juan has 5 LP vinyl records, each of them of a different band, and asks Thomas to match them with its respective band name, by showing him only the cover photo, not the names etc. Thomas will make a first guess and Juan will reveal him the number of correct matchings but without telling him which ones are correct. If not all his matchings are correct, he has the right to do a second guess, by changing as many as he wants. If he correctly guesses all 5 in these two attempts, then Juan will buy him a nice dinner. What is the probability for Thomas to win the dinner, in each of the below 3 cases: 1) He can recognize the 3 covers but not the other 2. 2) He can recognize the 2 covers but not the other 3. 3) He knows the matching of 2 covers with 2 of the bands but not which is which. Same also for 2 more covers. For example, he knows that cover1 and cover2 correspond to Band1 and Band2 but not which is which, and also he knows that cover3 and cover4 correspond to Band3 and Band4 but not which is which. I can easily tell that the requested probability for 1) is 100%: If he knows 123 and he guesses 45, Juan will reply him “3” as the correct number of matchings. Since Thomas knows 123 are correct, he will say 12354 and this will be correct. For number 3): Thomas must guess 12 or 21, 34 or 43 and he can deduce 5.  We have 4 cases but with the 2nd attempt, in essence we have 6: Suppose the correct choices are 21345: If he says 21345, Juan replies “5” and we are done. If he says 12435, Juan replies “1” and we are also done because Thomas knows it is 21345. If he says 12345, Juan replies “3”. Thomas doesn’t know which are the correct two (he only knows 5). Therefore in the second attempt, he can either say 21345, (correct) or 12435 which is wrong. Same also for 21435: In the second attempt he can either say 21345 (correct) or 12435 (wrong). So the overall probability is 4/6? Is this correct? Can you also help me out with the 2nd case? I started examining the cases but it’s very confusing! Many thanks!",,['probability']
41,"If $U\sim\chi_{m}^2$ independently of $V\sim\chi_n^2$ then prove that $\frac{V}{U+V}\sim\beta\left(\frac n2,\frac m2\right)$",If  independently of  then prove that,"U\sim\chi_{m}^2 V\sim\chi_n^2 \frac{V}{U+V}\sim\beta\left(\frac n2,\frac m2\right)","If $U\sim\chi_{m}^2,V\sim\chi_n^2$ and $U,V$ are independent then prove that $\frac{V}{U+V}\sim\beta\left(\frac n2,\frac m2\right)$ The joint pdf of $U$ and $V$ is, \begin{align} f_{UV}(u,v)&=\frac{1}{2^{\frac m2}\Gamma\left(\frac m2\right)}u^{\frac m2-1}e^{-\frac u2}\frac{1}{2^{\frac n2}\Gamma\left(\frac n2\right)}v^{\frac n2-1}e^{-\frac u2}\\ &=\frac{1}{2^{\frac{m+n}{2}}\Gamma\left(\frac m2\right)\Gamma\left(\frac n2\right)}u^{\frac m2-1}v^{\frac n2-1}e^{-\frac12(u+v)} \end{align} Now let $Y=\frac{V}{U+V}$ then CDF of $Y$ is, \begin{align} F_Y(y)&=\mathbb P(Y\le y)\\ &=\mathbb P\left(\frac{V}{U+V}\le y\right)\\ &=\mathbb P\left(\frac VU\le \left(\frac{y}{1-y}\right)\right)\\ &=\mathbb P\left(V\le \left(\frac{y}{1-y}\right)U\right)\\ &=\int_{u=0}^{\infty}\int_{v=0}^{\left(\frac{y}{1-y}\right)u}\frac{1}{2^{\frac{m+n}{2}}\Gamma\left(\frac m2\right)\Gamma\left(\frac n2\right)}u^{\frac m2-1}v^{\frac n2-1}e^{-\frac12(u+v)}\:dv\:du \end{align} Now we can get $f(y)$ using Leibniz integral rule, \begin{align} f_y(y)&=\frac{1}{2^{\frac{m+n}{2}}\Gamma\left(\frac m2\right)\Gamma\left(\frac n2\right)}\underbrace{\int_{u=0}^{\infty}\frac{u}{(1-y)^2}u^{\frac m2-1}{\left(\frac{yu}{1-y}\right)}^{\frac n2-1}e^{-\frac12\left(u+\frac{yu}{1-y}\right)}\:du}_{I} \end{align} But it seems I am far away to $\beta\left(\frac n2,\frac m2\right)$ . Is there other way to proof it $?$ Any hint or solution will be appreciated. Update : [For seek of completeness]Using @NCh answer, Replace $t=u\left(\frac{1}{2(1-y)}\right)$ , $u=2(1-y)t$ , $du=2(1-y)\,dt$ : $$ I=\frac{y^{\frac{n}2-1}}{(1-y)^{\frac{n}{2}+1}}\cdot 2^{\frac{n+m}{2}}(1-y)^\frac{n+m}{2}\underbrace{\int_{t=0}^{\infty}t^{\frac{n+m}{2}-1}e^{-t}\:dt}_{\Gamma\left(\frac{n+m}{2}\right)}$$ $$f_Y(y)=\frac{\Gamma\left(\frac{n+m}{2}\right)}{\Gamma\left(\frac m2\right)\Gamma\left(\frac n2\right)}y^{\frac{n}2-1}(1-y)^{\frac m2-1}$$ Hence $f_Y(y)\sim \beta\left(\frac n2,\frac m2\right)$","If and are independent then prove that The joint pdf of and is, Now let then CDF of is, Now we can get using Leibniz integral rule, But it seems I am far away to . Is there other way to proof it Any hint or solution will be appreciated. Update : [For seek of completeness]Using @NCh answer, Replace , , : Hence","U\sim\chi_{m}^2,V\sim\chi_n^2 U,V \frac{V}{U+V}\sim\beta\left(\frac n2,\frac m2\right) U V \begin{align}
f_{UV}(u,v)&=\frac{1}{2^{\frac m2}\Gamma\left(\frac m2\right)}u^{\frac m2-1}e^{-\frac u2}\frac{1}{2^{\frac n2}\Gamma\left(\frac n2\right)}v^{\frac n2-1}e^{-\frac u2}\\
&=\frac{1}{2^{\frac{m+n}{2}}\Gamma\left(\frac m2\right)\Gamma\left(\frac n2\right)}u^{\frac m2-1}v^{\frac n2-1}e^{-\frac12(u+v)}
\end{align} Y=\frac{V}{U+V} Y \begin{align}
F_Y(y)&=\mathbb P(Y\le y)\\
&=\mathbb P\left(\frac{V}{U+V}\le y\right)\\
&=\mathbb P\left(\frac VU\le \left(\frac{y}{1-y}\right)\right)\\
&=\mathbb P\left(V\le \left(\frac{y}{1-y}\right)U\right)\\
&=\int_{u=0}^{\infty}\int_{v=0}^{\left(\frac{y}{1-y}\right)u}\frac{1}{2^{\frac{m+n}{2}}\Gamma\left(\frac m2\right)\Gamma\left(\frac n2\right)}u^{\frac m2-1}v^{\frac n2-1}e^{-\frac12(u+v)}\:dv\:du
\end{align} f(y) \begin{align}
f_y(y)&=\frac{1}{2^{\frac{m+n}{2}}\Gamma\left(\frac m2\right)\Gamma\left(\frac n2\right)}\underbrace{\int_{u=0}^{\infty}\frac{u}{(1-y)^2}u^{\frac m2-1}{\left(\frac{yu}{1-y}\right)}^{\frac n2-1}e^{-\frac12\left(u+\frac{yu}{1-y}\right)}\:du}_{I}
\end{align} \beta\left(\frac n2,\frac m2\right) ? t=u\left(\frac{1}{2(1-y)}\right) u=2(1-y)t du=2(1-y)\,dt 
I=\frac{y^{\frac{n}2-1}}{(1-y)^{\frac{n}{2}+1}}\cdot 2^{\frac{n+m}{2}}(1-y)^\frac{n+m}{2}\underbrace{\int_{t=0}^{\infty}t^{\frac{n+m}{2}-1}e^{-t}\:dt}_{\Gamma\left(\frac{n+m}{2}\right)} f_Y(y)=\frac{\Gamma\left(\frac{n+m}{2}\right)}{\Gamma\left(\frac m2\right)\Gamma\left(\frac n2\right)}y^{\frac{n}2-1}(1-y)^{\frac m2-1} f_Y(y)\sim \beta\left(\frac n2,\frac m2\right)","['probability', 'integration', 'probability-distributions', 'density-function']"
42,A little game with rolling a fair dice,A little game with rolling a fair dice,,"You play the following game: you roll a fair dice then either you stop rolling and take the sum of the rolled numbers so far or you continue rolling the dice. Any time if a 1 is rolled you lose all your money, and has no option to continue the game. The strategy that you follow is that you wait until the cumulated price reaches a given level and then you stop. What should be this level in order to maximize the expected value of the prize? My answer is 20...is it correct?","You play the following game: you roll a fair dice then either you stop rolling and take the sum of the rolled numbers so far or you continue rolling the dice. Any time if a 1 is rolled you lose all your money, and has no option to continue the game. The strategy that you follow is that you wait until the cumulated price reaches a given level and then you stop. What should be this level in order to maximize the expected value of the prize? My answer is 20...is it correct?",,"['probability', 'optimization', 'expected-value', 'dice']"
43,What is a martingale measure? And in particular what is a $L^2(P)$ valued sigma-finite measure?,What is a martingale measure? And in particular what is a  valued sigma-finite measure?,L^2(P),"My text book defines a Martingale measure in the following way: Let $(\Omega, \mathscr F, \{\mathscr F_t\}_{t \ge 0}, P)$ be a filtered probability space. A Processes $\{M_t(A)\}_{t \ge 0, \; A\in \mathscr B(\mathbb R^n) }$ is a martingale measure with respect to the filtration $\{\mathscr F_t\}_{t\ge 0}$ if: 1: $M_0(A)=0$ for any $A \in \mathscr B(\mathbb R^n)$ 2: If $t\ge 0 $ then $M_t$ is a sigma finite $L^2(P)$ valued signed measure 3: For all $A \in \mathscr B(\mathbb R^n)$ , $\{M_t(A)\}_{t \ge 0}$ is a $0$ -mean martingale with respect to the filtration $\mathscr F$ . I have a question about the meaning of (2). How should I think of this? For fixed $t \ge 0$ , is $M_t(A)$ the measure of $A$ for any $A \in \mathscr B( \mathbb R^n)$ ? Or is $E\big[\big(M_t(A)\big)^2\big]$ the measure of $A$ ? Also what woud it mean to be a sigma finite $L^2(P)$ valued measure? Does that mean that we can find a collection of countable sets, $\{B_n\}$ , such that these sets cover $\mathbb R^n$ and that $E\big[\big(M_t(B_n)\big)^2\big] < \infty $ ?","My text book defines a Martingale measure in the following way: Let be a filtered probability space. A Processes is a martingale measure with respect to the filtration if: 1: for any 2: If then is a sigma finite valued signed measure 3: For all , is a -mean martingale with respect to the filtration . I have a question about the meaning of (2). How should I think of this? For fixed , is the measure of for any ? Or is the measure of ? Also what woud it mean to be a sigma finite valued measure? Does that mean that we can find a collection of countable sets, , such that these sets cover and that ?","(\Omega, \mathscr F, \{\mathscr F_t\}_{t \ge 0}, P) \{M_t(A)\}_{t \ge 0, \; A\in \mathscr B(\mathbb R^n) } \{\mathscr F_t\}_{t\ge 0} M_0(A)=0 A \in \mathscr B(\mathbb R^n) t\ge 0  M_t L^2(P) A \in \mathscr B(\mathbb R^n) \{M_t(A)\}_{t \ge 0} 0 \mathscr F t \ge 0 M_t(A) A A \in \mathscr B( \mathbb R^n) E\big[\big(M_t(A)\big)^2\big] A L^2(P) \{B_n\} \mathbb R^n E\big[\big(M_t(B_n)\big)^2\big] < \infty ","['probability', 'probability-theory', 'stochastic-processes', 'martingales']"
44,What is the average size of an island? [closed],What is the average size of an island? [closed],,"Closed. This question does not meet Mathematics Stack Exchange guidelines . It is not currently accepting answers. Please provide additional context , which ideally explains why the question is relevant to you and our  community. Some forms of context include: background and motivation, relevant definitions, source, possible strategies, your current progress, why the question is interesting or important, etc. Closed 4 years ago . Improve this question If you have a square grid, and each square* has probability $n$ of being ground. If the other squares are water, what is the average area of an island? If $n$ is small then the average island would have an area of about $1$ . With large values of $n$ , the average size of islands is really big. Also diagonally touching squares don’t count as being the same island. Unless of course they connect somewhere else. *Each with an area of $1$ .","Closed. This question does not meet Mathematics Stack Exchange guidelines . It is not currently accepting answers. Please provide additional context , which ideally explains why the question is relevant to you and our  community. Some forms of context include: background and motivation, relevant definitions, source, possible strategies, your current progress, why the question is interesting or important, etc. Closed 4 years ago . Improve this question If you have a square grid, and each square* has probability of being ground. If the other squares are water, what is the average area of an island? If is small then the average island would have an area of about . With large values of , the average size of islands is really big. Also diagonally touching squares don’t count as being the same island. Unless of course they connect somewhere else. *Each with an area of .",n n 1 n 1,"['probability', 'percolation']"
45,How Do You Call The Probability That An Outcome Is By Chance,How Do You Call The Probability That An Outcome Is By Chance,,"I can't find an answer to my question because I don't know what to search for, i.e. the terminology. To explain the word I'm looking for, let me start with an example. Say we know that the probability for having heads up after a coin flip is 50%. Now say I have a theory that if you flip the coin a certain way, it increases your chance of having the coin land with heads up. I try the theory 100 times (flipping the coin), and have heads up 60% of the time. Now I want to have a metric that meaningfully represents how likely it is that my technique actually increased the the heads-probability, vs. it just being chance. What do you call this metric? I'm naturally interested in the maths behind this question, but am rather asking for the terminology only to avoid duplicate questions. Optionally, if this is a ""new"" question, feel free to also include the maths.","I can't find an answer to my question because I don't know what to search for, i.e. the terminology. To explain the word I'm looking for, let me start with an example. Say we know that the probability for having heads up after a coin flip is 50%. Now say I have a theory that if you flip the coin a certain way, it increases your chance of having the coin land with heads up. I try the theory 100 times (flipping the coin), and have heads up 60% of the time. Now I want to have a metric that meaningfully represents how likely it is that my technique actually increased the the heads-probability, vs. it just being chance. What do you call this metric? I'm naturally interested in the maths behind this question, but am rather asking for the terminology only to avoid duplicate questions. Optionally, if this is a ""new"" question, feel free to also include the maths.",,"['probability', 'probability-distributions']"
46,Using a gamblers race to approximate $\pi$,Using a gamblers race to approximate,\pi,"Imagine two wealthy gamblers start tossing their own separate fair coins, winning 1\$ on heads and losing 1\$ on tails. Both start at 0\$ and have infinite bank balances. Both of them want to get to k\$. What is the probability that they will both reach their targets on the same toss? Based on this question we see that the answers to all such questions take the form $A+\frac{B}{\pi}$ . Here, we're strictly focusing on draws. First, we know that the stopping time (probability he'll reach his target first time on toss $2t+k$ ) for any one gambler is given by: $$a_k(t) = \frac{k}{k+t}\frac{{2t+k-1 \choose t}}{2^{2t+k}}$$ Now, the probability of an overall draw is simply the probability both will reach their target on toss $2t+k$ , summed over all possible values of $t$ . $$D_k = \sum\limits_{t=0}^{\infty} \left(\frac{k}{k+t}\frac{{2t+k-1 \choose t}}{2^{2t+k}}\right)^2$$ Mathematica can't find a nice closed form expression for the summation above (if you can, I'll be very impressed indeed). However, plugging various values of $k$ in we find that the answer always takes the form: $$D_k = \frac{E_k}{F_k \pi} - G_k$$ Where $E_k$ , $F_k$ and $G_k$ are all integers. Plugging the first few values of $G_k$ (calculated using Mathematica) into OEIS, I got the sequence A002002 . This means that: $$G_k = \sum\limits_{l=0}^{k-1} {k \choose l+1} {k+l \choose l}$$ However, I haven't been able to find corresponding expressions for $E_k$ and $F_k$ . Can anyone help with this? We can use it to get an expression for $\pi$ since $D_{\infty} = 0$ Here are the first few expressions for $D_k$ . $$D_1 = \frac{4}{\pi}-1$$ $$D_2 = \frac{16}{\pi}-5$$ $$D_3 = \frac{236}{3 \pi}-25$$ $$D_4 = \frac{1216}{3 \pi} - 129$$ $$D_5 = \frac{32092}{15 \pi} - 681$$ $$D_6 = \frac{172144}{15 \pi} - 3653$$ $$D_7 = \frac{1307924}{21 \pi} - 19825$$ $$D_8 = \frac{7161088}{21 \pi} - 108545$$ $$D_9 = \frac{592194476}{315 \pi} - 598417$$ $$D_{10} = \frac{3282949168}{315 \pi} - 3317445$$ $$D_{11} = \frac{40221561524}{693 \pi} - 18474633$$ $$D_{12} = \frac{224841634624}{693 \pi} - 103274625$$ Unfortunately, Mathematica gives up providing nice expressions in terms of $\pi$ , $D_{13}$ onwards. The idea is courtesy /u/boyobo on this reddit thread .","Imagine two wealthy gamblers start tossing their own separate fair coins, winning 1\$ on heads and losing 1\$ on tails. Both start at 0\$ and have infinite bank balances. Both of them want to get to k\$. What is the probability that they will both reach their targets on the same toss? Based on this question we see that the answers to all such questions take the form . Here, we're strictly focusing on draws. First, we know that the stopping time (probability he'll reach his target first time on toss ) for any one gambler is given by: Now, the probability of an overall draw is simply the probability both will reach their target on toss , summed over all possible values of . Mathematica can't find a nice closed form expression for the summation above (if you can, I'll be very impressed indeed). However, plugging various values of in we find that the answer always takes the form: Where , and are all integers. Plugging the first few values of (calculated using Mathematica) into OEIS, I got the sequence A002002 . This means that: However, I haven't been able to find corresponding expressions for and . Can anyone help with this? We can use it to get an expression for since Here are the first few expressions for . Unfortunately, Mathematica gives up providing nice expressions in terms of , onwards. The idea is courtesy /u/boyobo on this reddit thread .",A+\frac{B}{\pi} 2t+k a_k(t) = \frac{k}{k+t}\frac{{2t+k-1 \choose t}}{2^{2t+k}} 2t+k t D_k = \sum\limits_{t=0}^{\infty} \left(\frac{k}{k+t}\frac{{2t+k-1 \choose t}}{2^{2t+k}}\right)^2 k D_k = \frac{E_k}{F_k \pi} - G_k E_k F_k G_k G_k G_k = \sum\limits_{l=0}^{k-1} {k \choose l+1} {k+l \choose l} E_k F_k \pi D_{\infty} = 0 D_k D_1 = \frac{4}{\pi}-1 D_2 = \frac{16}{\pi}-5 D_3 = \frac{236}{3 \pi}-25 D_4 = \frac{1216}{3 \pi} - 129 D_5 = \frac{32092}{15 \pi} - 681 D_6 = \frac{172144}{15 \pi} - 3653 D_7 = \frac{1307924}{21 \pi} - 19825 D_8 = \frac{7161088}{21 \pi} - 108545 D_9 = \frac{592194476}{315 \pi} - 598417 D_{10} = \frac{3282949168}{315 \pi} - 3317445 D_{11} = \frac{40221561524}{693 \pi} - 18474633 D_{12} = \frac{224841634624}{693 \pi} - 103274625 \pi D_{13},"['probability', 'combinatorics', 'summation', 'binomial-coefficients', 'pi']"
47,Choosing the best or second best secretary,Choosing the best or second best secretary,,"We have a hall with $N$ secretaries.  We choose randomly one secretary at a time and interview them.  After the interview either we hire the secretary (and all others go home) or move to the next one (we can't go back and hire them later). The interviewer wants to hire either the best or second best secretary with the highest probability My approach: I know we should use this strategy: after we interview the first $aN$ candidates (which we don't hire), then we choose the first one that's better than the previous $aN$ candidates. I am just stuck knowing what is the value of $a$ .  What is the probability of this value? and how we get to it? I have seen the ""Secretary Problem"" where we want to choose the best one, the difference here is that we are satisfied with the best or second best one equally","We have a hall with secretaries.  We choose randomly one secretary at a time and interview them.  After the interview either we hire the secretary (and all others go home) or move to the next one (we can't go back and hire them later). The interviewer wants to hire either the best or second best secretary with the highest probability My approach: I know we should use this strategy: after we interview the first candidates (which we don't hire), then we choose the first one that's better than the previous candidates. I am just stuck knowing what is the value of .  What is the probability of this value? and how we get to it? I have seen the ""Secretary Problem"" where we want to choose the best one, the difference here is that we are satisfied with the best or second best one equally",N aN aN a,['probability']
48,"Suppose $X \sim Bin(n,p)$ and $Y \sim Bin(n,1-p)$, how is $X+Y$ distributed?","Suppose  and , how is  distributed?","X \sim Bin(n,p) Y \sim Bin(n,1-p) X+Y","Suppose $X \sim Bin(n,p)$ and $Y \sim Bin(n,1-p)$, How is $X+Y$ distributed? I know that for independent variables one can do the same as: Sum of two independent binomial variables Furthermore i have seen this post: Addition of two Binomial Distribution However $X$ and $Y$ do not necessarily need to be independent of each other. Backstory: I am trying to calculate the entropy of some $Z = X_1 + X_2 + \dots + X_n$ where each $X_i$ is either $Bin(n,p)$ or $Bin(n,1-p)$ depending on its parent node. For example lets take a graph that has only outgoing edges of degree 2 beginning from some source $X_0 \sim (\frac{1}{2},\frac{1}{2})$. If we compare layer $2$ and layer $3$, we sent $2^3$ nodes to $2^4$ nodes. The probability for a set of child nodes to get certain states is $Bin(n,p)$ when the parent has state $1$ and $Bin(n,1-p)$ if the parent has state $-1$. We proceed this until we reached some threshold layer $d$. What is the probability distribution of $Z = \sum X_i^{(d)}$","Suppose $X \sim Bin(n,p)$ and $Y \sim Bin(n,1-p)$, How is $X+Y$ distributed? I know that for independent variables one can do the same as: Sum of two independent binomial variables Furthermore i have seen this post: Addition of two Binomial Distribution However $X$ and $Y$ do not necessarily need to be independent of each other. Backstory: I am trying to calculate the entropy of some $Z = X_1 + X_2 + \dots + X_n$ where each $X_i$ is either $Bin(n,p)$ or $Bin(n,1-p)$ depending on its parent node. For example lets take a graph that has only outgoing edges of degree 2 beginning from some source $X_0 \sim (\frac{1}{2},\frac{1}{2})$. If we compare layer $2$ and layer $3$, we sent $2^3$ nodes to $2^4$ nodes. The probability for a set of child nodes to get certain states is $Bin(n,p)$ when the parent has state $1$ and $Bin(n,1-p)$ if the parent has state $-1$. We proceed this until we reached some threshold layer $d$. What is the probability distribution of $Z = \sum X_i^{(d)}$",,"['probability', 'information-theory']"
49,Random tree generation probability problem,Random tree generation probability problem,,"Given a tree-structure with a root node, the node can either get zero, one or two children, all with the same probability of 1/3. The children also has the same probability of getting zero, one or two children, which is 1/3. Recursively the algorithm creates the tree. What is the probability that the tree has more than N nodes in total? Came across this question after programming a random tree generator and got really curious, would be cool if someone knew the answer.","Given a tree-structure with a root node, the node can either get zero, one or two children, all with the same probability of 1/3. The children also has the same probability of getting zero, one or two children, which is 1/3. Recursively the algorithm creates the tree. What is the probability that the tree has more than N nodes in total? Came across this question after programming a random tree generator and got really curious, would be cool if someone knew the answer.",,"['probability', 'probability-theory', 'programming']"
50,How to derive approximation result from Levy 0-1 law?,How to derive approximation result from Levy 0-1 law?,,"Let $(\Omega, \mathcal{F}, P)$ be a probability space. Let $\mathcal{E}_1, \mathcal{E}_2,...$ be a sequence of finite, measurable partitions of $\Omega$ such that each $\mathcal{E}_{n+1}$ is a refinement of $\mathcal{E}_n$, and assume that $\sigma(\cup_n \mathcal{E}_n) = \mathcal{F}$. Let $E_n(\omega)$ be the cell of $\mathcal{E}_n$ that contains $\omega$. For all $A$ and almost all $\omega$, the Levy 0-1 law says that $$P(A \mid E_n(\omega)) \to \mathbf{1}_A(\omega)\tag{1}$$ In contemporary probability textbooks, this result is usually presented as an immediate consequence of the martingale convergence theorem for conditional expectations. Interestingly, I have recently discovered a much more elementary proof of this result in Halmos's Measure Theory (Theorem 49.B). Besides the basic properties of conditional probability, the only result that Halmos's proof appeals to is the standard fact that measurable sets can be approximated by sets in a generating algebra. In our context, for all $A \in \mathcal{F}$ and all $\epsilon > 0$, there exists $n$ and $E_1,...,E_k \in \mathcal{E}_n$ such that $$P(A \triangle E)< \epsilon, \tag{2}$$ where $E=E_1 \cup ... \cup E_k$. From (2), Halmos's straightforwardly derives (1), which leads to my question: Assuming (1), is there a straightforward way to derive (2)? We might start by noticing that $P(A \mid E_n) \to \mathbf{1}_A$ a.s. iff $P(A^c \mid E_n) \to \mathbf{1}_{A^c}$ a.s. and for all $E \in \sigma(\mathcal{E}_n)$ $$P(A \triangle E) = P(A \cap E^c) + P(A^c \cap E) = \int_{E^c}P(A \mid E_n)dP + \int_E P(A^c \mid E_n)dP.$$ Then I thought about trying something like, let $F_n = \{\omega: P(A^c \mid E_n(\omega)) < 1- \epsilon\} \in \sigma(\mathcal{E}_n)$ so that $F_n^c = \{\omega: P(A \mid E_n(\omega)) \leq \epsilon\}$. Then, $$P(A \triangle F_n) \leq \epsilon P(F_n^c) + (1-\epsilon) P(F_n).$$ But I'm stuck here and not sure this is going to work. Any hints are appreciated.","Let $(\Omega, \mathcal{F}, P)$ be a probability space. Let $\mathcal{E}_1, \mathcal{E}_2,...$ be a sequence of finite, measurable partitions of $\Omega$ such that each $\mathcal{E}_{n+1}$ is a refinement of $\mathcal{E}_n$, and assume that $\sigma(\cup_n \mathcal{E}_n) = \mathcal{F}$. Let $E_n(\omega)$ be the cell of $\mathcal{E}_n$ that contains $\omega$. For all $A$ and almost all $\omega$, the Levy 0-1 law says that $$P(A \mid E_n(\omega)) \to \mathbf{1}_A(\omega)\tag{1}$$ In contemporary probability textbooks, this result is usually presented as an immediate consequence of the martingale convergence theorem for conditional expectations. Interestingly, I have recently discovered a much more elementary proof of this result in Halmos's Measure Theory (Theorem 49.B). Besides the basic properties of conditional probability, the only result that Halmos's proof appeals to is the standard fact that measurable sets can be approximated by sets in a generating algebra. In our context, for all $A \in \mathcal{F}$ and all $\epsilon > 0$, there exists $n$ and $E_1,...,E_k \in \mathcal{E}_n$ such that $$P(A \triangle E)< \epsilon, \tag{2}$$ where $E=E_1 \cup ... \cup E_k$. From (2), Halmos's straightforwardly derives (1), which leads to my question: Assuming (1), is there a straightforward way to derive (2)? We might start by noticing that $P(A \mid E_n) \to \mathbf{1}_A$ a.s. iff $P(A^c \mid E_n) \to \mathbf{1}_{A^c}$ a.s. and for all $E \in \sigma(\mathcal{E}_n)$ $$P(A \triangle E) = P(A \cap E^c) + P(A^c \cap E) = \int_{E^c}P(A \mid E_n)dP + \int_E P(A^c \mid E_n)dP.$$ Then I thought about trying something like, let $F_n = \{\omega: P(A^c \mid E_n(\omega)) < 1- \epsilon\} \in \sigma(\mathcal{E}_n)$ so that $F_n^c = \{\omega: P(A \mid E_n(\omega)) \leq \epsilon\}$. Then, $$P(A \triangle F_n) \leq \epsilon P(F_n^c) + (1-\epsilon) P(F_n).$$ But I'm stuck here and not sure this is going to work. Any hints are appreciated.",,"['probability', 'measure-theory', 'probability-limit-theorems']"
51,What is the distribution of the results of a round robin tournament? What is the distribution of the number of winners?,What is the distribution of the results of a round robin tournament? What is the distribution of the number of winners?,,"With the ATP tour finals tennis tournament going on right now, I got to thinking about round robin tournaments. I learned upon searching that in graph theory the tournament models a round robin, in which each player plays every other player exactly once, and either wins or loses (no draws). But after some searching I was not able to find answers to the questions I posed in the title. Suppose there are $n$ players in a tournament. The result of a tournament is described as a graph in which each pair of vertices is connected by exactly one directed edge, where an edge going from vertex $i$ to vertex $j$ indicates that player $i$ beat player $j$. Call $g$ the number of edges in the graph, which reflects the number of games played in the tournament. Of course, $g = n(n-1)/2$, the triangular number of $n-1$ (imagine the cells above the diagonal of a tournament crosstable). Clearly, there are $2^g$ possible results of a tournament. Suppose every player in the tournament is equally skilled, so that every result is equally likely. For a given result, we can count the wins of each player and rank the win counts in a non-decreasing sequence $s = (s_1,\ldots,s_n)$ where $0 \leq s_1 \leq \cdots \leq s_n$. My first question is: what in general would be the probability distribution for $s$? Relatedly, what would be the expectation for each $s_i$? To get an intuition for this last question, I simulated 10,000 instances of a round robin tournament of 100 equally skilled players. The plot below shows the average of $s$, where the x axis reflects the index $i$ and the y axis the number of wins $s_i$. Now let the number of winners of the tournament be denoted $ w = |\{s_i : s_i = \max s\}|$. What is the probability that $w=1$, that is, that the tournament has an outright winner? In general, what is the probability distribution of $w$? Using the data from the same simulation as above, I plotted the relative frequencies of $w$ values: . Does anyone know of any results that bear on these questions? When I thought of these questions, I figured the answers would be standard results, but after a moderate amount of looking I haven't come across them.","With the ATP tour finals tennis tournament going on right now, I got to thinking about round robin tournaments. I learned upon searching that in graph theory the tournament models a round robin, in which each player plays every other player exactly once, and either wins or loses (no draws). But after some searching I was not able to find answers to the questions I posed in the title. Suppose there are $n$ players in a tournament. The result of a tournament is described as a graph in which each pair of vertices is connected by exactly one directed edge, where an edge going from vertex $i$ to vertex $j$ indicates that player $i$ beat player $j$. Call $g$ the number of edges in the graph, which reflects the number of games played in the tournament. Of course, $g = n(n-1)/2$, the triangular number of $n-1$ (imagine the cells above the diagonal of a tournament crosstable). Clearly, there are $2^g$ possible results of a tournament. Suppose every player in the tournament is equally skilled, so that every result is equally likely. For a given result, we can count the wins of each player and rank the win counts in a non-decreasing sequence $s = (s_1,\ldots,s_n)$ where $0 \leq s_1 \leq \cdots \leq s_n$. My first question is: what in general would be the probability distribution for $s$? Relatedly, what would be the expectation for each $s_i$? To get an intuition for this last question, I simulated 10,000 instances of a round robin tournament of 100 equally skilled players. The plot below shows the average of $s$, where the x axis reflects the index $i$ and the y axis the number of wins $s_i$. Now let the number of winners of the tournament be denoted $ w = |\{s_i : s_i = \max s\}|$. What is the probability that $w=1$, that is, that the tournament has an outright winner? In general, what is the probability distribution of $w$? Using the data from the same simulation as above, I plotted the relative frequencies of $w$ values: . Does anyone know of any results that bear on these questions? When I thought of these questions, I figured the answers would be standard results, but after a moderate amount of looking I haven't come across them.",,"['probability', 'statistics', 'graph-theory']"
52,"Working with normal distributions, how large can noise be before data becomes inaccurate?","Working with normal distributions, how large can noise be before data becomes inaccurate?",,"I'm measuring a characteristic of a device that has a normal distribution ($0$ mean and std dev of $\sigma_M$). There is, however, noise in the measurement process, which also has a normal distribution ($0$ mean and std dev of $\sigma_N$). I can measure this noise independently. I can estimate the device's true characteristic (without noise) as $\sigma_D = \sqrt{\sigma_M^2 - \sigma_N^2}$. To be compliant with a specific spec, $\sigma_D$ must be less than $L$. If the noise is small compared to the measured value, I have high confidence in my data. But my confidence drops as the noise approaches the measured value. In the extreme case, if $\sigma_M = \sigma_N$, my estimation returns $\sigma_D = 0$, indicating that I've reached the noise floor of my equipment (I think that's the correct interpretation, but let me know if not). My question is, how close can $\sigma_N$ be to $\sigma_M$ such that $\sigma_D$ is still ""accurate""? I don't want to report a value of $\sigma_D$ that contains too much error. Rather, I'd like to report some lower bound for $\sigma_D$ once $\sigma_N$ becomes too close to $\sigma_M$. Any light you can shed to help me define that lower bound would be much appreciated. UPDATE 1 To clarify, I measure $(1)$ the device with noise and, separately at a later time, $(2)$ the noise (without the device). The distribution of noise measured directly (without the device) can be assumed to also exist when the device is measured with noise, and that it is the ONLY noise present when the device is measured with noise. UPDATE 2 Is the following statistically meaningful as a condition where $\sigma_D$ is inaccurate: $$ \sigma_M - \sigma_N < \dfrac{1.96}{\sqrt{n}}(\sigma_M + \sigma_N) \;, $$ where $n$ is the number of samples used to compute the sigmas?","I'm measuring a characteristic of a device that has a normal distribution ($0$ mean and std dev of $\sigma_M$). There is, however, noise in the measurement process, which also has a normal distribution ($0$ mean and std dev of $\sigma_N$). I can measure this noise independently. I can estimate the device's true characteristic (without noise) as $\sigma_D = \sqrt{\sigma_M^2 - \sigma_N^2}$. To be compliant with a specific spec, $\sigma_D$ must be less than $L$. If the noise is small compared to the measured value, I have high confidence in my data. But my confidence drops as the noise approaches the measured value. In the extreme case, if $\sigma_M = \sigma_N$, my estimation returns $\sigma_D = 0$, indicating that I've reached the noise floor of my equipment (I think that's the correct interpretation, but let me know if not). My question is, how close can $\sigma_N$ be to $\sigma_M$ such that $\sigma_D$ is still ""accurate""? I don't want to report a value of $\sigma_D$ that contains too much error. Rather, I'd like to report some lower bound for $\sigma_D$ once $\sigma_N$ becomes too close to $\sigma_M$. Any light you can shed to help me define that lower bound would be much appreciated. UPDATE 1 To clarify, I measure $(1)$ the device with noise and, separately at a later time, $(2)$ the noise (without the device). The distribution of noise measured directly (without the device) can be assumed to also exist when the device is measured with noise, and that it is the ONLY noise present when the device is measured with noise. UPDATE 2 Is the following statistically meaningful as a condition where $\sigma_D$ is inaccurate: $$ \sigma_M - \sigma_N < \dfrac{1.96}{\sqrt{n}}(\sigma_M + \sigma_N) \;, $$ where $n$ is the number of samples used to compute the sigmas?",,"['probability', 'probability-theory', 'statistics', 'normal-distribution']"
53,Quickly estimating a probability,Quickly estimating a probability,,"Suppose I have five bins into which I want to place 15 balls. The bins have capacities $2$, $2$, $3$, $3$, and $7.$ I place the balls one at a time in the bins, randomly and uniformly amongst the bins that are not full (so for example, if after placing four balls, both of the bins with capacity $2$ are already full, the next ball is placed with probability $1/3$ in each of the remaining three bins). My question is if there is a an efficient way to estimate the probability that the bin with capacity $7$ is full at the end of this process (it would be great if the technique generalizes in the obvious way).","Suppose I have five bins into which I want to place 15 balls. The bins have capacities $2$, $2$, $3$, $3$, and $7.$ I place the balls one at a time in the bins, randomly and uniformly amongst the bins that are not full (so for example, if after placing four balls, both of the bins with capacity $2$ are already full, the next ball is placed with probability $1/3$ in each of the remaining three bins). My question is if there is a an efficient way to estimate the probability that the bin with capacity $7$ is full at the end of this process (it would be great if the technique generalizes in the obvious way).",,"['probability', 'estimation']"
54,Probability of rolling a value a certain number of times in a certain number of rolls,Probability of rolling a value a certain number of times in a certain number of rolls,,"I am familiar with the formula to calculate the probability of rolling a certain number at least once given $x$ rolls of a die with $y$ sides, that is: $$P_1(x, y) = 1 - \left(1 - \frac{1}{y}\right)^x$$ where the subscript $1$ indicates that the number should be rolled at least once. For example, the chance of rolling a 6 in 6 rolls of a D6 would be $P_1(6, 6) = 1 - \left(1 - \frac{1}{6}\right)^6 \approx 0.665$. This got me thinking what the probability would be to roll a number at least $n$ times given the same die conditions. I manually worked out the probability for n = 2: $$P_2(x, y) = \frac{x}{y}-\left(1 - \left(1 - \frac{1}{y}\right)^x\right)$$ This does work even if only two dice are rolled, in which case the probability should simply be $(\frac{1}{y})^2$, and it is. For a D20, $P_2(2, 20) = (\frac{1}{20})^2 = \frac{1}{400}$. After that I tried to figure out how to represent $P_3(x, y)$ but unfortunately I was unable to do so. Previously, I was effectively considering a binomial coefficient in geometric terms. Using $P_n(3, y)$ as an example and looking for rolls of 20 for simplicity's sake, I considered a cube of side length $y$ divided into $y^3$ unit cubes. For $P_1$ I took the face cubes, subtracted the edge cubes, and added back the corner to give me the number of rolls in which a 20 appeared. For $P_2$, the formula was edge cubes $-$ corner for all rolls in which two 20s appeared. I know this all involves binomial coefficient, but I never really took a proper stats class so my knowledge of its application is somewhat limited. To find a general case for $P_3$, I would have to consider a 4-cube which I tried and failed. I'm sure this could be done easily using binomial coefficient, but that's sort of why I'm asking about this. I'm fairly sure that my expression for $P_2$ only has that extra $\frac{x}{y}$ term because $\binom{3}{1}$ and $\binom{3}{2}$ happen to equal 3 and therefore could be algebraically reorganized. My question is this: Is there a general case formula for $P_n(x, y)$ that represents the probability of rolling a number $n$ times given $x$ rolls of a $y$-sided die. Additionally, would said formula be different if the requirement was to roll one number $n_1$ times and another number $n_2$ times and so on? Is there a different general case for $P_{{n_1}, {n_2}, ...}(x, y)$?","I am familiar with the formula to calculate the probability of rolling a certain number at least once given $x$ rolls of a die with $y$ sides, that is: $$P_1(x, y) = 1 - \left(1 - \frac{1}{y}\right)^x$$ where the subscript $1$ indicates that the number should be rolled at least once. For example, the chance of rolling a 6 in 6 rolls of a D6 would be $P_1(6, 6) = 1 - \left(1 - \frac{1}{6}\right)^6 \approx 0.665$. This got me thinking what the probability would be to roll a number at least $n$ times given the same die conditions. I manually worked out the probability for n = 2: $$P_2(x, y) = \frac{x}{y}-\left(1 - \left(1 - \frac{1}{y}\right)^x\right)$$ This does work even if only two dice are rolled, in which case the probability should simply be $(\frac{1}{y})^2$, and it is. For a D20, $P_2(2, 20) = (\frac{1}{20})^2 = \frac{1}{400}$. After that I tried to figure out how to represent $P_3(x, y)$ but unfortunately I was unable to do so. Previously, I was effectively considering a binomial coefficient in geometric terms. Using $P_n(3, y)$ as an example and looking for rolls of 20 for simplicity's sake, I considered a cube of side length $y$ divided into $y^3$ unit cubes. For $P_1$ I took the face cubes, subtracted the edge cubes, and added back the corner to give me the number of rolls in which a 20 appeared. For $P_2$, the formula was edge cubes $-$ corner for all rolls in which two 20s appeared. I know this all involves binomial coefficient, but I never really took a proper stats class so my knowledge of its application is somewhat limited. To find a general case for $P_3$, I would have to consider a 4-cube which I tried and failed. I'm sure this could be done easily using binomial coefficient, but that's sort of why I'm asking about this. I'm fairly sure that my expression for $P_2$ only has that extra $\frac{x}{y}$ term because $\binom{3}{1}$ and $\binom{3}{2}$ happen to equal 3 and therefore could be algebraically reorganized. My question is this: Is there a general case formula for $P_n(x, y)$ that represents the probability of rolling a number $n$ times given $x$ rolls of a $y$-sided die. Additionally, would said formula be different if the requirement was to roll one number $n_1$ times and another number $n_2$ times and so on? Is there a different general case for $P_{{n_1}, {n_2}, ...}(x, y)$?",,"['probability', 'statistics', 'binomial-coefficients', 'binomial-theorem', 'dice']"
55,Probability that 3 points are collinear.,Probability that 3 points are collinear.,,"If we select three independent and random points $A$,$B$ and $C$ in a plane, what shall be the probability that they are collinear? Actually, this problem was asked to my friend in an interview, he applied common sense and approached in the following way : since any two, say $A$ and $B$ lie on a line, $C$ either lies on the line joining $A$ and $B$ or anywhere else in the plane.Now since the plane is so vast compared to the line (here comes the argument) the probability must be tending to 0. But the Mathematics professor (interviewing him) said that as both, the line and the plane stretch to infinity we cannot compare them.Therefore there are only two possibilities - $C$ will either lie on the line or on the plane.So, probability is equally distributed.Thus answer will be 0.5. The whole argument lies within the point Whether two infinities can be compared or not . Although he also wasn't satisfied but he couldn't argue further because he didn't knew two Infinities can be compared!(For example the set of all Real Numbers is strictly bigger than set of all Natural Numbers which can be proved by Cantor's Diagonal Argument) We can compare cardinality of two infinities and tell which one is relatively bigger and I couldn't find any One-One and On-to function for the points of line and the plane and I am pretty sure I wouldn't find any.(To explain that they are equal) But as the interviewer himself was a Mathematics professor, I couldn't simply neglect his answer. EDIT : I previously had accepted the answer given by sds , but recently I came to know that, the cardinality of the set containing all the points of the plane, and the set containing all the points on the line is, infact equal, and hence we cann't say that plane is ""vast"" as compare to line. So, I am again confused.","If we select three independent and random points $A$,$B$ and $C$ in a plane, what shall be the probability that they are collinear? Actually, this problem was asked to my friend in an interview, he applied common sense and approached in the following way : since any two, say $A$ and $B$ lie on a line, $C$ either lies on the line joining $A$ and $B$ or anywhere else in the plane.Now since the plane is so vast compared to the line (here comes the argument) the probability must be tending to 0. But the Mathematics professor (interviewing him) said that as both, the line and the plane stretch to infinity we cannot compare them.Therefore there are only two possibilities - $C$ will either lie on the line or on the plane.So, probability is equally distributed.Thus answer will be 0.5. The whole argument lies within the point Whether two infinities can be compared or not . Although he also wasn't satisfied but he couldn't argue further because he didn't knew two Infinities can be compared!(For example the set of all Real Numbers is strictly bigger than set of all Natural Numbers which can be proved by Cantor's Diagonal Argument) We can compare cardinality of two infinities and tell which one is relatively bigger and I couldn't find any One-One and On-to function for the points of line and the plane and I am pretty sure I wouldn't find any.(To explain that they are equal) But as the interviewer himself was a Mathematics professor, I couldn't simply neglect his answer. EDIT : I previously had accepted the answer given by sds , but recently I came to know that, the cardinality of the set containing all the points of the plane, and the set containing all the points on the line is, infact equal, and hence we cann't say that plane is ""vast"" as compare to line. So, I am again confused.",,"['probability', 'infinite-groups']"
56,"For two random variables $X_1, X_2$, is it always necessarily the case that $E(e^{X_2}\mid e^{X_1}) = E(e^{X_2}\mid X_1)$?","For two random variables , is it always necessarily the case that ?","X_1, X_2 E(e^{X_2}\mid e^{X_1}) = E(e^{X_2}\mid X_1)","For two random variables $X_1, X_2$, is it always necessarily the case that $E \left(e^{X_2}\mid e^{X_1}\right) = E\left(e^{X_2}\mid X_1\right)$? If not, in what cases are they like so? An explanation I read in a book is that $X_1$ is increasing in $e^{X_1}$ but that doesn't make sense to me.","For two random variables $X_1, X_2$, is it always necessarily the case that $E \left(e^{X_2}\mid e^{X_1}\right) = E\left(e^{X_2}\mid X_1\right)$? If not, in what cases are they like so? An explanation I read in a book is that $X_1$ is increasing in $e^{X_1}$ but that doesn't make sense to me.",,"['probability', 'probability-theory', 'exponential-function', 'conditional-expectation']"
57,$f(P)=f(Q)$ implies that $P=Q$,implies that,f(P)=f(Q) P=Q,"Let $(X,\mathbb{H})$ and $(Y,\mathbb{F})$ be two measurable spaces. Assume that $P$ and $Q$ be probability measures on $(X,\mathbb{H})$ and that $f:X\to Y$ is a $\mathbb{H}/\mathbb{F}$-measurable mapping. What are the weakest (alternatively some weak) conditions on $f$ for which $$ f(P)=f(Q) \implies P=Q, $$ holds? Here $f(P)$ and $f(Q)$ are push-forward measures. If we work under the assumption that $X$ and $Y$ are metric spaces and $P,Q$ are Borel measures then it is sufficient to say that $f$ is a homeomorphism, but what does this translate to when we have no topology on $X$ and $Y$ only $\sigma$-algebras.","Let $(X,\mathbb{H})$ and $(Y,\mathbb{F})$ be two measurable spaces. Assume that $P$ and $Q$ be probability measures on $(X,\mathbb{H})$ and that $f:X\to Y$ is a $\mathbb{H}/\mathbb{F}$-measurable mapping. What are the weakest (alternatively some weak) conditions on $f$ for which $$ f(P)=f(Q) \implies P=Q, $$ holds? Here $f(P)$ and $f(Q)$ are push-forward measures. If we work under the assumption that $X$ and $Y$ are metric spaces and $P,Q$ are Borel measures then it is sufficient to say that $f$ is a homeomorphism, but what does this translate to when we have no topology on $X$ and $Y$ only $\sigma$-algebras.",,"['probability', 'general-topology', 'probability-theory', 'measure-theory']"
58,Optimal strategy: Two Player Gambling Die Game,Optimal strategy: Two Player Gambling Die Game,,"This is a simple 2 player game on which each player has an individual ""pool"" of finite money or points, and every round they must decide how many points they want to risk for a chance to get a directly proportional reward. I've been trying to solve this problem for a while, but I still know very little about game theory and I can't seem to even find the proper place to start. I'd be thankful if anyone could help me. The Game: Two players, A and B, start the game with \$500 each. In every round, a single, 6 faced die is tossed. Both players have to gamble an integer value from \$1 to \$99 (inclusive). If the die shows the number one, both players get their respective bets back and earn 5x the amount they gambled. All earnings are added to the amount available for gambling. If the die rolls any other number, both players lose the amount they gambled. The amount of money remaining for each player is revealed to their opponent at the end of every round. The game ends when one of the players has no money left, or after 1000 rounds. The winner is the player with the most money. Example: Round 1 begins Player A bets \$1, Player B bets \$2 Die rolls a 5 Player A now has \$499, Player B now has \$498 Round 1 ends Round 2 begins Player A bets \$1, Player B bets \$2 Die rolls a 1 Player A gets his \$1 back, plus another \$5. He now has \$504. Player B gets her \$2 back, plus another \$10. She now has \$508. Round 2 ends What is the best strategy the players can play to win the game? Any help is appreciated.","This is a simple 2 player game on which each player has an individual ""pool"" of finite money or points, and every round they must decide how many points they want to risk for a chance to get a directly proportional reward. I've been trying to solve this problem for a while, but I still know very little about game theory and I can't seem to even find the proper place to start. I'd be thankful if anyone could help me. The Game: Two players, A and B, start the game with \$500 each. In every round, a single, 6 faced die is tossed. Both players have to gamble an integer value from \$1 to \$99 (inclusive). If the die shows the number one, both players get their respective bets back and earn 5x the amount they gambled. All earnings are added to the amount available for gambling. If the die rolls any other number, both players lose the amount they gambled. The amount of money remaining for each player is revealed to their opponent at the end of every round. The game ends when one of the players has no money left, or after 1000 rounds. The winner is the player with the most money. Example: Round 1 begins Player A bets \$1, Player B bets \$2 Die rolls a 5 Player A now has \$499, Player B now has \$498 Round 1 ends Round 2 begins Player A bets \$1, Player B bets \$2 Die rolls a 1 Player A gets his \$1 back, plus another \$5. He now has \$504. Player B gets her \$2 back, plus another \$10. She now has \$508. Round 2 ends What is the best strategy the players can play to win the game? Any help is appreciated.",,"['probability', 'game-theory']"
59,Solving birthday problem without complement,Solving birthday problem without complement,,"I'm trying to find the probability of at least 2 people in a room of 4 sharing the same birthday (without using complements). I began by breaking the problem down into 4 cases: Let E = the event that at least 2 people share the same birthday in a room of 4. Our sample size: $365^4$ Case 1: 4 people share the same birthday: 365 ways Case 2: 3 people share the same birthday, 1 distinct birthday: $365 \cdot 364 \cdot C(4,3)$ Case 3: 2 people share a birthday, another 2 people share some other birthday: $365 \cdot 364 \cdot \frac{C(4,2)}{2}$ Case 4: 2 people share same birthday, 2 distinct birthdays: $365 \cdot 364 \cdot 363 \cdot C(4,2) \cdot 2$ After adding up all the cases and dividing by the sample size to find probability the answer had an over-count. I checked my answer by doing $$P(E) = 1- \frac{365 \cdot 364 \cdot 363 \cdot 362}{365^4}$$ Where did I have an over-count? Thank you! Here is an example that works with n = 3 people and at least 2 people share same birthday. Case 1: 3 people share same birthday: 365 Case 2: 2 Same birthdays, 1 different: $365 \cdot 364 \cdot \binom{3}{2}$ $$P(E) = \frac{365 + (365 \cdot 364 \cdot \binom{3}{2})}{365^3} \equiv 1 - \frac{365 \cdot 364 \cdot 363}{365^3}$$ Those are both equivalent answers because in the complement we're subtracting away the event that all birthdays are distinct.","I'm trying to find the probability of at least 2 people in a room of 4 sharing the same birthday (without using complements). I began by breaking the problem down into 4 cases: Let E = the event that at least 2 people share the same birthday in a room of 4. Our sample size: $365^4$ Case 1: 4 people share the same birthday: 365 ways Case 2: 3 people share the same birthday, 1 distinct birthday: $365 \cdot 364 \cdot C(4,3)$ Case 3: 2 people share a birthday, another 2 people share some other birthday: $365 \cdot 364 \cdot \frac{C(4,2)}{2}$ Case 4: 2 people share same birthday, 2 distinct birthdays: $365 \cdot 364 \cdot 363 \cdot C(4,2) \cdot 2$ After adding up all the cases and dividing by the sample size to find probability the answer had an over-count. I checked my answer by doing $$P(E) = 1- \frac{365 \cdot 364 \cdot 363 \cdot 362}{365^4}$$ Where did I have an over-count? Thank you! Here is an example that works with n = 3 people and at least 2 people share same birthday. Case 1: 3 people share same birthday: 365 Case 2: 2 Same birthdays, 1 different: $365 \cdot 364 \cdot \binom{3}{2}$ $$P(E) = \frac{365 + (365 \cdot 364 \cdot \binom{3}{2})}{365^3} \equiv 1 - \frac{365 \cdot 364 \cdot 363}{365^3}$$ Those are both equivalent answers because in the complement we're subtracting away the event that all birthdays are distinct.",,['probability']
60,How to compute the expected number of duplicates,How to compute the expected number of duplicates,,"Consider $n$ objects each with an associated probability $p_i$, $i\in\{1,\dots,n\}$.  If I sample objects  $k$ times independently with replacement according to the probability distribution defined by the $p_i$, how does one compute the expected number of times you sample an object you have sampled before? We can assume that $n > k$.","Consider $n$ objects each with an associated probability $p_i$, $i\in\{1,\dots,n\}$.  If I sample objects  $k$ times independently with replacement according to the probability distribution defined by the $p_i$, how does one compute the expected number of times you sample an object you have sampled before? We can assume that $n > k$.",,['probability']
61,On the variance proxy of a positive (and bounded) sub-Gaussian variable,On the variance proxy of a positive (and bounded) sub-Gaussian variable,,"Consider a random variable $X \ge 0$ which takes values in an interval $[0, b]$, and further $$ \text{P}(X \ge t) \le C \exp\left(\frac{-t^{2}}{B}\right), \quad \forall t \ge 0, $$ for given constants $C \gg 1$ and $B >0$. Since $X$ is bounded, it is a sub-Gaussian variable, and its variance proxy can be upper bounded by $O\left((b-0)^{2}\right)$ based on the length of the interval. Q1: First, a clarification on the definition: if we temporarily ignore the fact that $X$ is bounded (but taking into account that $X \ge 0$), then is the above tail bound enough to say that $X$ is sub-Gaussian? (E.g., does the value of $C$ matter?) Q2: Using the tail bound, is it possible to get a better upper bound on the variance proxy? In particular, I saw a claim that based on the above tail bound, the moments of $X-\mathbb{E}[X]$ can upper be bounded by those of a Gaussian with variance $O(B \sqrt{\log{C}})$. Is that true? Edit : To bound all moments of $X-\mathbb{E}[X]$ by those of a Gaussian with variance $\gamma$, I would need to show that $X-\mathbb{E}[X]$ is sub-Gaussian with variance proxy $\gamma > 0 $, i.e. , that $\mathbb{E}[e^{s(X-\mathbb{E}[X])}] \le e^{s^{2}\gamma/2}$. Motivated by Michael's answer, which gives an upper bound on the variance $\sigma^{2}$ of $X$, we could put the question this way: is there a straightforward connection between $\gamma$ and  $\sigma^{2}$? I see a related question here: Bound variance proxy of a subGaussian random variable by its variance","Consider a random variable $X \ge 0$ which takes values in an interval $[0, b]$, and further $$ \text{P}(X \ge t) \le C \exp\left(\frac{-t^{2}}{B}\right), \quad \forall t \ge 0, $$ for given constants $C \gg 1$ and $B >0$. Since $X$ is bounded, it is a sub-Gaussian variable, and its variance proxy can be upper bounded by $O\left((b-0)^{2}\right)$ based on the length of the interval. Q1: First, a clarification on the definition: if we temporarily ignore the fact that $X$ is bounded (but taking into account that $X \ge 0$), then is the above tail bound enough to say that $X$ is sub-Gaussian? (E.g., does the value of $C$ matter?) Q2: Using the tail bound, is it possible to get a better upper bound on the variance proxy? In particular, I saw a claim that based on the above tail bound, the moments of $X-\mathbb{E}[X]$ can upper be bounded by those of a Gaussian with variance $O(B \sqrt{\log{C}})$. Is that true? Edit : To bound all moments of $X-\mathbb{E}[X]$ by those of a Gaussian with variance $\gamma$, I would need to show that $X-\mathbb{E}[X]$ is sub-Gaussian with variance proxy $\gamma > 0 $, i.e. , that $\mathbb{E}[e^{s(X-\mathbb{E}[X])}] \le e^{s^{2}\gamma/2}$. Motivated by Michael's answer, which gives an upper bound on the variance $\sigma^{2}$ of $X$, we could put the question this way: is there a straightforward connection between $\gamma$ and  $\sigma^{2}$? I see a related question here: Bound variance proxy of a subGaussian random variable by its variance",,"['probability', 'probability-theory', 'distribution-tails']"
62,Calculate the non-server's advantage in a win-by-two game,Calculate the non-server's advantage in a win-by-two game,,"In volleyball, a point is awarded for every ""rally"" and a game is won by the first team to get to $N$ points, but you must win a game (sometimes called a ""set"") by two or more points. (If one team gets to $N$ points but the other team has $N-1$ points, play continues until one team is ahead by two.) The team that has won the last point always serves to the next point. It is well known that a high levels of play, the serving side is at a disadvantage in the sense of having a lower probability of winning that point (since the receiving side has the first opportunity to ""attack""). Let us say two teams are evenly matched, and the probability of the serving side winning a given point is $p$ with $0<p<\frac12$. It might be thought that since you need to win a game by two points, there will be an even number of points played in any sufficiently close game, and the disadvantage of having to serve first will even out.  This is not the case.  For example, if $N=2$ (that is, you start out even and the first team to be ahead by two points will win), the probability of the initial server winning is $$ S_{22} = \frac{1}{3-2p} $$ Here I have introduced a notation:  $S_{ab}$ is the probability of the server winning if the server needs at least $a$ more points, and the non-server needs at least $b$ more points.  So for example, in a game to $N=21$, if the score is $20$ serving to $19$, that position would be represented by $S_{13}$. My question is, for $k>2$, what is $S_{kk}$? That is, in a game to $N=k$, what is the probability that the starting server will win (and thus how much of a disadvantage is the first serve)?  I would guess that this probability can be obtained in closed form, but if not, I'd like to see the asymptotic behavior of $\frac12-S_{kk}$. Calculation of $S_{22}$: $$S_{22} = p S_{13} + (1-p) ( (1-S_{13}) \\ S_{13} = p + (1-p) (1-S_{22} ) $$ Take the value of $S_{13}$ from the second equation and plug it into the first.  Group terms to get $$ S_{22}(3p-2p^2) = p $$","In volleyball, a point is awarded for every ""rally"" and a game is won by the first team to get to $N$ points, but you must win a game (sometimes called a ""set"") by two or more points. (If one team gets to $N$ points but the other team has $N-1$ points, play continues until one team is ahead by two.) The team that has won the last point always serves to the next point. It is well known that a high levels of play, the serving side is at a disadvantage in the sense of having a lower probability of winning that point (since the receiving side has the first opportunity to ""attack""). Let us say two teams are evenly matched, and the probability of the serving side winning a given point is $p$ with $0<p<\frac12$. It might be thought that since you need to win a game by two points, there will be an even number of points played in any sufficiently close game, and the disadvantage of having to serve first will even out.  This is not the case.  For example, if $N=2$ (that is, you start out even and the first team to be ahead by two points will win), the probability of the initial server winning is $$ S_{22} = \frac{1}{3-2p} $$ Here I have introduced a notation:  $S_{ab}$ is the probability of the server winning if the server needs at least $a$ more points, and the non-server needs at least $b$ more points.  So for example, in a game to $N=21$, if the score is $20$ serving to $19$, that position would be represented by $S_{13}$. My question is, for $k>2$, what is $S_{kk}$? That is, in a game to $N=k$, what is the probability that the starting server will win (and thus how much of a disadvantage is the first serve)?  I would guess that this probability can be obtained in closed form, but if not, I'd like to see the asymptotic behavior of $\frac12-S_{kk}$. Calculation of $S_{22}$: $$S_{22} = p S_{13} + (1-p) ( (1-S_{13}) \\ S_{13} = p + (1-p) (1-S_{22} ) $$ Take the value of $S_{13}$ from the second equation and plug it into the first.  Group terms to get $$ S_{22}(3p-2p^2) = p $$",,"['probability', 'asymptotics']"
63,Independence of the data and the parameter in Machine Learning,Independence of the data and the parameter in Machine Learning,,"In one of the lectures, prof. Nando de Freitas explains the use of Bayesian rule to logistic regression. Here's the video and the slides . In particular, on slide 10 (around 34:50 on the video) NdF writes the posterior as following: $$p(\theta \mid X,y)=\frac{p(y \mid X,\theta)p(\theta)}{p(y \mid X)}$$ where $(X, y)$ are the observed data $D$ and $\theta$ is the parameter of the model. (1) Strict application of Bayesian rule gives a slightly different equation: $$p(\theta \mid X,y)=\frac{p(y \mid X,\theta)p(\theta\mid X)}{p(y \mid X)}$$ $X$ is just ignored from the condition to form a prior. For me it's not obvious that $X$ and $\theta$ are independent. Why is it true then? (2) NdF repeats a similar reasoning on the next slide: $$p(y_{n+1}\mid x_{n+1}, D) =$$ $$\int{p(y_{n+1}, \theta \mid x_{n+1}, D)}d\theta=$$ $$\int{p(y_{n+1}\mid \theta, x_{n+1}, D)} p(\theta \mid x_{n+1}, D)d\theta=$$ $$\int{p(y_{n+1}\mid \theta, x_{n+1})} p(\theta\mid D)d\theta$$ In the last equation two conditions disappear, $D$ and $x_{n+1}$. The argument is as follows (around 40:20 on the video): $\theta$ already contains the information about $D$, hence $D$ is redundant. Plus $x_{n+1}$ doesn't give any information to the posterior, hence $x_{n+1}$ is redundant. I don't quite understand this reasoning and the nature of $\theta$ as a random variable. The dependence of $\theta$ and $x$ is not straightforward, but it looks like to compute $p(\theta \mid x)$ we need to marginalize over all $y$. Would appreciate if someone explains the intuition behind it.","In one of the lectures, prof. Nando de Freitas explains the use of Bayesian rule to logistic regression. Here's the video and the slides . In particular, on slide 10 (around 34:50 on the video) NdF writes the posterior as following: $$p(\theta \mid X,y)=\frac{p(y \mid X,\theta)p(\theta)}{p(y \mid X)}$$ where $(X, y)$ are the observed data $D$ and $\theta$ is the parameter of the model. (1) Strict application of Bayesian rule gives a slightly different equation: $$p(\theta \mid X,y)=\frac{p(y \mid X,\theta)p(\theta\mid X)}{p(y \mid X)}$$ $X$ is just ignored from the condition to form a prior. For me it's not obvious that $X$ and $\theta$ are independent. Why is it true then? (2) NdF repeats a similar reasoning on the next slide: $$p(y_{n+1}\mid x_{n+1}, D) =$$ $$\int{p(y_{n+1}, \theta \mid x_{n+1}, D)}d\theta=$$ $$\int{p(y_{n+1}\mid \theta, x_{n+1}, D)} p(\theta \mid x_{n+1}, D)d\theta=$$ $$\int{p(y_{n+1}\mid \theta, x_{n+1})} p(\theta\mid D)d\theta$$ In the last equation two conditions disappear, $D$ and $x_{n+1}$. The argument is as follows (around 40:20 on the video): $\theta$ already contains the information about $D$, hence $D$ is redundant. Plus $x_{n+1}$ doesn't give any information to the posterior, hence $x_{n+1}$ is redundant. I don't quite understand this reasoning and the nature of $\theta$ as a random variable. The dependence of $\theta$ and $x$ is not straightforward, but it looks like to compute $p(\theta \mid x)$ we need to marginalize over all $y$. Would appreciate if someone explains the intuition behind it.",,"['probability', 'probability-theory', 'machine-learning', 'bayes-theorem']"
64,Infinitesimal generator of Brownian motion with additional jumps,Infinitesimal generator of Brownian motion with additional jumps,,"A compound Poisson process is a jump process with two parameters, the rate of the jumps $\lambda$ and the distribution of the jumps $\mu$ ($\mu$ is a probability measure on $\mathbb{R}$). The infinitesimal generator of this process is given by: $$A_{\lambda,\mu}f(x) = \lambda\int_\mathbb{R} (f(x+z)-f(x))\mu(dz)$$ What happen if I take a brownian motion with generator $\frac{1}{2}f''$ and construct the process with generator $\frac{1}{2}f''+A_{\lambda,\mu}$. Is this process the sum of a brownian motion with an a independent compound poisson process with parameters $\lambda, \mu$? If the answer is yes, How we can generalize this notion? for example, If we take a general process with generator $L$ and we construct the process with generator $L+A$ is this process the sum of a $L$-process and a independent $A$-process. I think that total generality it is not true because the $L$-process can have jumps with rate dependent on the state. Any help will be appreciated!","A compound Poisson process is a jump process with two parameters, the rate of the jumps $\lambda$ and the distribution of the jumps $\mu$ ($\mu$ is a probability measure on $\mathbb{R}$). The infinitesimal generator of this process is given by: $$A_{\lambda,\mu}f(x) = \lambda\int_\mathbb{R} (f(x+z)-f(x))\mu(dz)$$ What happen if I take a brownian motion with generator $\frac{1}{2}f''$ and construct the process with generator $\frac{1}{2}f''+A_{\lambda,\mu}$. Is this process the sum of a brownian motion with an a independent compound poisson process with parameters $\lambda, \mu$? If the answer is yes, How we can generalize this notion? for example, If we take a general process with generator $L$ and we construct the process with generator $L+A$ is this process the sum of a $L$-process and a independent $A$-process. I think that total generality it is not true because the $L$-process can have jumps with rate dependent on the state. Any help will be appreciated!",,"['probability', 'probability-theory', 'markov-process']"
65,Weighted War - Game of Mind and Probability,Weighted War - Game of Mind and Probability,,"Game Weighted War is a game   of bidding, where: Both players have cards valued from $1$ to $11$ in their hands There is a third pile of cards from $1$ to $11$ face down on the table and shuffled, with one random card being removed from it at the   beginning of the game Each beginning of a turn one random card from the table pile is turned face up, and the players offer one of their own cards face   down. When both players decided on their bid card, their cards are flipped and the higher value takes the table card. The bid cards are   put aside, and new turn begins. If the bid cards are equal, they are put aside and they start a next turn by flipping the next table card. This turn they bid for both   cards. If the equal value repeats, they continue to add table cards to   bid pile until someone wins it. (If both players run out of bidding cards and the bid pile hasn't been won yet, it goes to no one and stays aside.) When all cards are won, players count their points by adding the values of the table cards they won. The winner is one with more   points. I'm wondering what would be the optimal strategy that maximizes your chances of winning and minimizes your chances of losing? I could find barely anything on this game online. One trivial things is that playing $1$ doesn't make sense since there is one card less at the table than in your hand. Also, playing a random card won't be any good for you since cases like bidding $11$ for a low valued card will rarely have any good effects for you. The video also mentions that they found that it's the best to play the same valued card as the table card but I couldn't find any proof of that. A Counter to that would be playing one card higher, and counter to that would be occasionally sacrificing some low cards to gain advantage in end game. Anyway, I'm also interested in how much luck has an effect here. Pattern I attempted to develop the optimal strategy for when there are   only $2$, $3$ or $4$ cards in hopes of helping me to find a strategy for the $11$ card game. Assuming both players use the optimal strategy: For $2$ card case, there is no point in playing $1$ so both play $2$ and end up in a draw . For $3$ card case, a draw is forced $\frac{2}{3}$ times, and $\frac{1}{3}$ times happens when the first table card is $2$, then both players have equal chance of either winning , losing or ending up in a draw again. That depends on the table card that was removed at the beginning of the game. For $4$ card case, a draw is forced $\frac{3}{4}$ times, and $\frac{1}{4}$ times happens when the first table card is $4$, draw should also be forced since $4$ is the best choice to play for both players, but if a $4$ is countered with a $2$ then both players have again equal chance of either winning , losing or ending up in a draw if they continue to play perfectly.  ($3$ always beats $2$ and $4$ always beats $3$, if the rest of choices are best possible from both players.) That means the safest option in this $\frac{1}{4}$ case is $4$ and also the best option against a random play; but that always results in a draw if both players play perfectly. Thus, if both perfect players play a set of games until the match is resolved, and observe that they both keep drawing with a $4$, one might attempt to break the draw chain by playing $2$ and give both players an equal chance to resolve the match without needing to worry that the opponent might suddenly play $3$, making that the optimal strategy? But if other can predict your $2$ and counter with a $3$ he beats your method. That's why I'm not sure about the strategy for $4$ case if players are going for a win rather than accepting the draw as the best way of minimizing their losing chances, so I decided to post a separate question . All in all, I have solved these $2,3,4$ cases by observing each   possible game state to determine what would be the optimal play. If   the pattern holds, the original game of $11$ cards should have a   optimal strategy which if used by both players, always results in a   draw and/or equal chances for both players to win , lose or end   up in a draw . But I still don't know how to create a general optimal strategy other   than evaluating all possible states by brute force. I wonder If this   can be solved by a optimal set of rules other than a brute force   approach? (Either yes or no, I would need a proof of the solution)","Game Weighted War is a game   of bidding, where: Both players have cards valued from $1$ to $11$ in their hands There is a third pile of cards from $1$ to $11$ face down on the table and shuffled, with one random card being removed from it at the   beginning of the game Each beginning of a turn one random card from the table pile is turned face up, and the players offer one of their own cards face   down. When both players decided on their bid card, their cards are flipped and the higher value takes the table card. The bid cards are   put aside, and new turn begins. If the bid cards are equal, they are put aside and they start a next turn by flipping the next table card. This turn they bid for both   cards. If the equal value repeats, they continue to add table cards to   bid pile until someone wins it. (If both players run out of bidding cards and the bid pile hasn't been won yet, it goes to no one and stays aside.) When all cards are won, players count their points by adding the values of the table cards they won. The winner is one with more   points. I'm wondering what would be the optimal strategy that maximizes your chances of winning and minimizes your chances of losing? I could find barely anything on this game online. One trivial things is that playing $1$ doesn't make sense since there is one card less at the table than in your hand. Also, playing a random card won't be any good for you since cases like bidding $11$ for a low valued card will rarely have any good effects for you. The video also mentions that they found that it's the best to play the same valued card as the table card but I couldn't find any proof of that. A Counter to that would be playing one card higher, and counter to that would be occasionally sacrificing some low cards to gain advantage in end game. Anyway, I'm also interested in how much luck has an effect here. Pattern I attempted to develop the optimal strategy for when there are   only $2$, $3$ or $4$ cards in hopes of helping me to find a strategy for the $11$ card game. Assuming both players use the optimal strategy: For $2$ card case, there is no point in playing $1$ so both play $2$ and end up in a draw . For $3$ card case, a draw is forced $\frac{2}{3}$ times, and $\frac{1}{3}$ times happens when the first table card is $2$, then both players have equal chance of either winning , losing or ending up in a draw again. That depends on the table card that was removed at the beginning of the game. For $4$ card case, a draw is forced $\frac{3}{4}$ times, and $\frac{1}{4}$ times happens when the first table card is $4$, draw should also be forced since $4$ is the best choice to play for both players, but if a $4$ is countered with a $2$ then both players have again equal chance of either winning , losing or ending up in a draw if they continue to play perfectly.  ($3$ always beats $2$ and $4$ always beats $3$, if the rest of choices are best possible from both players.) That means the safest option in this $\frac{1}{4}$ case is $4$ and also the best option against a random play; but that always results in a draw if both players play perfectly. Thus, if both perfect players play a set of games until the match is resolved, and observe that they both keep drawing with a $4$, one might attempt to break the draw chain by playing $2$ and give both players an equal chance to resolve the match without needing to worry that the opponent might suddenly play $3$, making that the optimal strategy? But if other can predict your $2$ and counter with a $3$ he beats your method. That's why I'm not sure about the strategy for $4$ case if players are going for a win rather than accepting the draw as the best way of minimizing their losing chances, so I decided to post a separate question . All in all, I have solved these $2,3,4$ cases by observing each   possible game state to determine what would be the optimal play. If   the pattern holds, the original game of $11$ cards should have a   optimal strategy which if used by both players, always results in a   draw and/or equal chances for both players to win , lose or end   up in a draw . But I still don't know how to create a general optimal strategy other   than evaluating all possible states by brute force. I wonder If this   can be solved by a optimal set of rules other than a brute force   approach? (Either yes or no, I would need a proof of the solution)",,"['probability', 'recreational-mathematics', 'game-theory']"
66,Bridges across a tiled floor,Bridges across a tiled floor,,"A few years back, a friend of mine did a seminar on ""Bridges across a tiled floor"".  A ""bridge"" was defined as a row or column of an $n \times n$ binary matrix consisting entirely of $1$'s, for example the third column and fourth row of \begin{bmatrix} 1&0&1&0 \\ 0&0&1&0 \\ 0&1&1&1 \\ 1&1&1&1 \end{bmatrix} The problem is to find the probability of selecting an $n\times n$ binary matrix with at least one bridge, when selecting from all $n\times n$ binary matrices.  My friend made an algorithm using Markov chains for calculating it for a given $n$, but we never found a closed formula.  I was wondering if there was a simple approach, or if anyone knows how to find the solution. I made several attempts.  My first attempt was to try a purely combinatorial solution, but the interconnectivity made it a bit ridiculous.  I tried to solve the complementary problem by placing $0$'s on the main diagonal, permuting them, and considering all other choices for the other entries, but this resulted in multiple ways of attaining the same matrix.  I tried solving the simpler problems of only column bridges or row bridges, which had simple solutions, but combining them proved difficult.  And most recently (which I haven't fully fleshed out), I tried setting up a recursive relationship from the $n-1$ case to the $n$ case. Any insight would be greatly appreciated.","A few years back, a friend of mine did a seminar on ""Bridges across a tiled floor"".  A ""bridge"" was defined as a row or column of an $n \times n$ binary matrix consisting entirely of $1$'s, for example the third column and fourth row of \begin{bmatrix} 1&0&1&0 \\ 0&0&1&0 \\ 0&1&1&1 \\ 1&1&1&1 \end{bmatrix} The problem is to find the probability of selecting an $n\times n$ binary matrix with at least one bridge, when selecting from all $n\times n$ binary matrices.  My friend made an algorithm using Markov chains for calculating it for a given $n$, but we never found a closed formula.  I was wondering if there was a simple approach, or if anyone knows how to find the solution. I made several attempts.  My first attempt was to try a purely combinatorial solution, but the interconnectivity made it a bit ridiculous.  I tried to solve the complementary problem by placing $0$'s on the main diagonal, permuting them, and considering all other choices for the other entries, but this resulted in multiple ways of attaining the same matrix.  I tried solving the simpler problems of only column bridges or row bridges, which had simple solutions, but combining them proved difficult.  And most recently (which I haven't fully fleshed out), I tried setting up a recursive relationship from the $n-1$ case to the $n$ case. Any insight would be greatly appreciated.",,"['probability', 'combinatorics', 'matrices']"
67,Modified gambler's ruin problem: quit when going bankruptcy or losing $k$ dollars in all,Modified gambler's ruin problem: quit when going bankruptcy or losing  dollars in all,k,"In each round, the gambler either wins and earns 1 dollar, or loses 1 dollar. The winning probability in each round is $p<1/2$. The gambler initially has $a$ dollars. He quits the game when he has no money, or he has lost $k>a$ rounds in all by this time, no matter how many rounds he wins. (For example, if $a=2$, $k=3$, and the sequence is +1,+1,+1,-1,+1,-1,-1, he quits now.) What is his expected exit time? What confuses me is the dependence between these two events. I know the generating function of the exit time in the standard gambler's ruin problem, and the duration until the gambler loses $k$ dollars in all is a negative binomial random variable. But these two stopping times are dependent. I was wondering if anyone could give me some hint. Thanks a lot! Update: From Ross Millikan's hint: how to calculate the probability that the wealth is $b$ at the end of round $2k-a$, given that the game is not over?","In each round, the gambler either wins and earns 1 dollar, or loses 1 dollar. The winning probability in each round is $p<1/2$. The gambler initially has $a$ dollars. He quits the game when he has no money, or he has lost $k>a$ rounds in all by this time, no matter how many rounds he wins. (For example, if $a=2$, $k=3$, and the sequence is +1,+1,+1,-1,+1,-1,-1, he quits now.) What is his expected exit time? What confuses me is the dependence between these two events. I know the generating function of the exit time in the standard gambler's ruin problem, and the duration until the gambler loses $k$ dollars in all is a negative binomial random variable. But these two stopping times are dependent. I was wondering if anyone could give me some hint. Thanks a lot! Update: From Ross Millikan's hint: how to calculate the probability that the wealth is $b$ at the end of round $2k-a$, given that the game is not over?",,"['probability', 'stochastic-processes', 'markov-chains', 'random-walk']"
68,Average shortest distance between a circle and a random point lying in it,Average shortest distance between a circle and a random point lying in it,,"What is the average shortest distance between the circle $(x-a)^2+(y-b)^2=r^2$ and a random point lying in it? This question is just idle curiosity. Basically, it's the same as finding the difference between its radius and the average distance between the random point and its center. Let $D$ denote the shortest distance between the circle $(x-a)^2+(y-b)^2=r^2$ and the random point $P(X,Y)$, then \begin{equation} D=r-\sqrt{(X-a)^2+(Y-b)^2} \end{equation} We may assume $X$ and $Y$ are independently uniformly distributed in $(0,a)$ and $(0,b)$, respectively. Then its joint pdf is \begin{equation} f_{X,Y}(x,y)=f_X(x)\cdot f_Y(y)=\frac{1}{ab} \end{equation} Hence the average of $D$ is \begin{equation} E[D]=\int_0^b\int_0^a d\ f_{X,Y}(x,y)\ dx\ dy=r-\frac{1}{ab}\int_0^b\int_0^a \sqrt{(x-a)^2+(y-b)^2}\ dx\ dy \end{equation} Is my approach correct? If not, how does one find the correct $E[D]$?","What is the average shortest distance between the circle $(x-a)^2+(y-b)^2=r^2$ and a random point lying in it? This question is just idle curiosity. Basically, it's the same as finding the difference between its radius and the average distance between the random point and its center. Let $D$ denote the shortest distance between the circle $(x-a)^2+(y-b)^2=r^2$ and the random point $P(X,Y)$, then \begin{equation} D=r-\sqrt{(X-a)^2+(Y-b)^2} \end{equation} We may assume $X$ and $Y$ are independently uniformly distributed in $(0,a)$ and $(0,b)$, respectively. Then its joint pdf is \begin{equation} f_{X,Y}(x,y)=f_X(x)\cdot f_Y(y)=\frac{1}{ab} \end{equation} Hence the average of $D$ is \begin{equation} E[D]=\int_0^b\int_0^a d\ f_{X,Y}(x,y)\ dx\ dy=r-\frac{1}{ab}\int_0^b\int_0^a \sqrt{(x-a)^2+(y-b)^2}\ dx\ dy \end{equation} Is my approach correct? If not, how does one find the correct $E[D]$?",,"['probability', 'probability-theory', 'probability-distributions', 'random-variables', 'geometric-probability']"
69,What does probability really mean?,What does probability really mean?,,"I apologize in advance if the question is a bit off-topic and not strictly mathematical. To be clear, I'm talking about classical probability which is defined like this: Given a finite sample space $S$, and a subset of $S$ which we call event $E$, the probability $P(E)$ of event $E$ is $$P(E)=\dfrac{N(E)}{N(S)}$$ Where $N$ denotes the number of the elements of a set. I wanna understand what notion we try to capture when we define probability. Bernoulli, Laplace and others certainly had a certain concept in mind that they wanted to describe and hence they formalized it into this definition. To explain my point, consider the following example: We have a set of data, say, a bunch of real numbers and we want to represent them using a single real number. So the notion we wanna capture here is finding a real number that is the best representative to a group of real numbers. It turns out the mean of those numbers is the best representative to them in the sense that it minimizes the sum of the square of differences(distances) between the mean and every real number in that set of data. So what notion we wanna capture by defining probability? I was told that if the probability of an event $E$ in an experiment is say, $0.3$ then if the experiment is performed $n$ number of times, $0.3n$ of times event $E$ will happen. So this is the notion $P(E)$ supposed to capture. This definition, as far as capturing this concept makes perfect sense to me in the following: -If an event has a $P(E)=0$ then it will never happen. -If an event has a $P(E)=1$ then it will always happen, or if we perform the experiment $n$ times, $E$ will occur $n$ times. However for $P(E)$ that has a value between 0 and 1, I don't know how it works. For example if we tossed a coin 10 times, it's not guaranteed at all that half the tosses will give you head, even worse, all the tosses can sometimes give you tails. So what's going on here? When I asked for the justification that $P(E)$ really captures this notion, I read this is justified by the law of large numbers: You perform an experiment certain number of times, say for example tossing a fair coin. When we calculate the probability of a toss being head, we assign the number $0.5$ to that event. This means(According to my understanding of the law of large numbers) that as the number of tossing the coin gets arbitrarily large(number of trials approaching infinity), Heads will make up $0.5$ of the total number of coins tossed, or a number that's very close to $0.5$ and as the number of trials increase, it will approach $0.5$. However there's something circular about this: When I ask what does ""fair"" coin mean? It's a coin that, as we toss it arbitrarily large number of times, the number of its heads will approach $0.5$. So can someone clear it up for me, and justify why $P(E)$ captures what it captures?","I apologize in advance if the question is a bit off-topic and not strictly mathematical. To be clear, I'm talking about classical probability which is defined like this: Given a finite sample space $S$, and a subset of $S$ which we call event $E$, the probability $P(E)$ of event $E$ is $$P(E)=\dfrac{N(E)}{N(S)}$$ Where $N$ denotes the number of the elements of a set. I wanna understand what notion we try to capture when we define probability. Bernoulli, Laplace and others certainly had a certain concept in mind that they wanted to describe and hence they formalized it into this definition. To explain my point, consider the following example: We have a set of data, say, a bunch of real numbers and we want to represent them using a single real number. So the notion we wanna capture here is finding a real number that is the best representative to a group of real numbers. It turns out the mean of those numbers is the best representative to them in the sense that it minimizes the sum of the square of differences(distances) between the mean and every real number in that set of data. So what notion we wanna capture by defining probability? I was told that if the probability of an event $E$ in an experiment is say, $0.3$ then if the experiment is performed $n$ number of times, $0.3n$ of times event $E$ will happen. So this is the notion $P(E)$ supposed to capture. This definition, as far as capturing this concept makes perfect sense to me in the following: -If an event has a $P(E)=0$ then it will never happen. -If an event has a $P(E)=1$ then it will always happen, or if we perform the experiment $n$ times, $E$ will occur $n$ times. However for $P(E)$ that has a value between 0 and 1, I don't know how it works. For example if we tossed a coin 10 times, it's not guaranteed at all that half the tosses will give you head, even worse, all the tosses can sometimes give you tails. So what's going on here? When I asked for the justification that $P(E)$ really captures this notion, I read this is justified by the law of large numbers: You perform an experiment certain number of times, say for example tossing a fair coin. When we calculate the probability of a toss being head, we assign the number $0.5$ to that event. This means(According to my understanding of the law of large numbers) that as the number of tossing the coin gets arbitrarily large(number of trials approaching infinity), Heads will make up $0.5$ of the total number of coins tossed, or a number that's very close to $0.5$ and as the number of trials increase, it will approach $0.5$. However there's something circular about this: When I ask what does ""fair"" coin mean? It's a coin that, as we toss it arbitrarily large number of times, the number of its heads will approach $0.5$. So can someone clear it up for me, and justify why $P(E)$ captures what it captures?",,['probability']
70,"Probability of picking 3 numbers in sequence, with k random picks","Probability of picking 3 numbers in sequence, with k random picks",,"I am struggling a bit with this problem, I think I am somehow close but I miss something. Let's assume we have numbers from $1$ to $30$, we pick $5$ random numbers (with repetition) and we want to know the probability to pick at least $3$ numbers in sequence. Denominator : The total number of combinations without repetition should be $$30!/(5!*(30-5)!)$$ and with repetition: ( 5 5 6 7 8 ) is a valid sequence, this is what i am actually interested in $$(30+5-1)!/(5!*(30-1)!)$$ Numerator : I should get the ways to pick $5$ numbers where at least $3$ are in a sequence. There are $28$ ways to pick $3$ numbers in a row, if we just would pick $3$, but with $5$ I am having the problem. My approach is $28$ $+$ the way to pick the other $2$ numbers, that is $C(30+2-1,2)$ The flaw here I think is that I think I am counting twice some combinations. How can I get the number of combination of picking $3$ consecutive numbers with $5$ picks? Edit: To clarify repetition is allowed, so $5\; 5\; 6\; 7\; 8$ is a valid pick. Order is not important, so $5$ $5$ $6$ $7$ $8$ is the same as $6$ $7$ $8$ $5$ $5$.","I am struggling a bit with this problem, I think I am somehow close but I miss something. Let's assume we have numbers from $1$ to $30$, we pick $5$ random numbers (with repetition) and we want to know the probability to pick at least $3$ numbers in sequence. Denominator : The total number of combinations without repetition should be $$30!/(5!*(30-5)!)$$ and with repetition: ( 5 5 6 7 8 ) is a valid sequence, this is what i am actually interested in $$(30+5-1)!/(5!*(30-1)!)$$ Numerator : I should get the ways to pick $5$ numbers where at least $3$ are in a sequence. There are $28$ ways to pick $3$ numbers in a row, if we just would pick $3$, but with $5$ I am having the problem. My approach is $28$ $+$ the way to pick the other $2$ numbers, that is $C(30+2-1,2)$ The flaw here I think is that I think I am counting twice some combinations. How can I get the number of combination of picking $3$ consecutive numbers with $5$ picks? Edit: To clarify repetition is allowed, so $5\; 5\; 6\; 7\; 8$ is a valid pick. Order is not important, so $5$ $5$ $6$ $7$ $8$ is the same as $6$ $7$ $8$ $5$ $5$.",,['probability']
71,What is the Probability that a Magic Coin will come up Heads at least 7 times in 10 throws?,What is the Probability that a Magic Coin will come up Heads at least 7 times in 10 throws?,,"Please consider the following problem which came up when I was thinking about modeling options. Problem: Suppose we have a magic two-sided coin. The probability it comes up heads in a given throw is normally $0.5$. However, if it has come up heads three times in the last three throws then the probability that it will come up heads in the next throw drops to $0.25$. In addition, if it has come up tails three times in the last three throws then the probability that it will come up heads in the next throw is $0.75$. For the first three throws of the coin the probability it will come up heads is $0.5$. If I throw this coin $10$ times, what is the probability that I will get at least $7$ heads? Here is my attempt to solve the problem. Let $E(c,h,t)$ be a function which tells us the expected number of heads in $c$ throws given that we are on a run of $t$ heads and a run of $h$ heads. This means that $t$ will equal 0 or $h$ will be zero. They will both be zero at the start. For example, if the last three throws were heads, tails, tails then $t$ would be $2$ and $h$ would be $0$. \begin{eqnarray*} E(0,t,h) &=& 0 \\ E(1,0,t) &=& 0.5 \\ E(2,0,t) &=& 1 \\ E(3,0,t) &=& 1.5 \\ E(2,1,t) &=& 1.5 \\ E(2,2,t) &=& .5(1+E(1,3,t) + 0.5(E(1,0,1)) \\ E(1,3,t) &=& 0.25 \\ E(2,2,t) &=& .5(1+0.25) + 0.5(.5) = .5(1.25) + .25 = 1.125  \\ E(4,0,t) &=& .5(1 + E(3,1,0)) + .5(E(3,0,1) \\ \end{eqnarray*} It seems to me that there should be an easier way to do this problem. I am hoping that somebody could point out a useful theorem that would make the problem much easier to solve. Bob","Please consider the following problem which came up when I was thinking about modeling options. Problem: Suppose we have a magic two-sided coin. The probability it comes up heads in a given throw is normally $0.5$. However, if it has come up heads three times in the last three throws then the probability that it will come up heads in the next throw drops to $0.25$. In addition, if it has come up tails three times in the last three throws then the probability that it will come up heads in the next throw is $0.75$. For the first three throws of the coin the probability it will come up heads is $0.5$. If I throw this coin $10$ times, what is the probability that I will get at least $7$ heads? Here is my attempt to solve the problem. Let $E(c,h,t)$ be a function which tells us the expected number of heads in $c$ throws given that we are on a run of $t$ heads and a run of $h$ heads. This means that $t$ will equal 0 or $h$ will be zero. They will both be zero at the start. For example, if the last three throws were heads, tails, tails then $t$ would be $2$ and $h$ would be $0$. \begin{eqnarray*} E(0,t,h) &=& 0 \\ E(1,0,t) &=& 0.5 \\ E(2,0,t) &=& 1 \\ E(3,0,t) &=& 1.5 \\ E(2,1,t) &=& 1.5 \\ E(2,2,t) &=& .5(1+E(1,3,t) + 0.5(E(1,0,1)) \\ E(1,3,t) &=& 0.25 \\ E(2,2,t) &=& .5(1+0.25) + 0.5(.5) = .5(1.25) + .25 = 1.125  \\ E(4,0,t) &=& .5(1 + E(3,1,0)) + .5(E(3,0,1) \\ \end{eqnarray*} It seems to me that there should be an easier way to do this problem. I am hoping that somebody could point out a useful theorem that would make the problem much easier to solve. Bob",,"['probability', 'statistics', 'stochastic-processes']"
72,Entropy of matrix vector product,Entropy of matrix vector product,,"Consider a random $n$ by $n$ matrix $A$ whose entries are chosen from $\{0,1\}$ and a random $n$ dimensional vector $x$ whose entries are also chosen from $\{0,1\}$. Assume $n$ is large. What is the (base 2) Shannon entropy of $Ax$? That is, can we give a large $n$ approximation for $H(Ax)$? It feels like $H(Ax)$ should be at least $n$ as that is the entropy of $x$ and $A$ is very likely to be non-singular. We also know $H(Ax) \leq n \log_2{n}$ as we can encode $Ax$ in $n \log_2{n}$ bits (the entries of $Ax$ are no larger than $n$). Is the entropy of the form $n$ or of the form $n\log_2{n}$ or something in between?","Consider a random $n$ by $n$ matrix $A$ whose entries are chosen from $\{0,1\}$ and a random $n$ dimensional vector $x$ whose entries are also chosen from $\{0,1\}$. Assume $n$ is large. What is the (base 2) Shannon entropy of $Ax$? That is, can we give a large $n$ approximation for $H(Ax)$? It feels like $H(Ax)$ should be at least $n$ as that is the entropy of $x$ and $A$ is very likely to be non-singular. We also know $H(Ax) \leq n \log_2{n}$ as we can encode $Ax$ in $n \log_2{n}$ bits (the entries of $Ax$ are no larger than $n$). Is the entropy of the form $n$ or of the form $n\log_2{n}$ or something in between?",,['probability']
73,(CLT) Number of rolls of two fair dice to be 90% certain that the percentage of times they show the same face is between 5/36 and 7/36,(CLT) Number of rolls of two fair dice to be 90% certain that the percentage of times they show the same face is between 5/36 and 7/36,,"How often do you have to roll two fair six-sided dice to be 90% certain that the percentage of times they show the same face is between 5/36 and 7/36? I was thinking, to apply the central limit theorem, between the two bounds, but I have no idea how to setup it. First of all thanks to Henry for your answer. My professor said, First : From the statement of the problem one must know, how to apply the   correction factor to correctly have the limit in which one evaluate   the normal, and find the correct n. Second: The distribution is binomial and it will be impossible have a   greater value (%) for 486 than 487. I have three question:  1) How Henry obtain (5/36n)^(1/2) for the standard deviation. 2) Where is my mistake evaluating the probability of 486 and 487. 3) How to solve it using the CLT. Thanks.","How often do you have to roll two fair six-sided dice to be 90% certain that the percentage of times they show the same face is between 5/36 and 7/36? I was thinking, to apply the central limit theorem, between the two bounds, but I have no idea how to setup it. First of all thanks to Henry for your answer. My professor said, First : From the statement of the problem one must know, how to apply the   correction factor to correctly have the limit in which one evaluate   the normal, and find the correct n. Second: The distribution is binomial and it will be impossible have a   greater value (%) for 486 than 487. I have three question:  1) How Henry obtain (5/36n)^(1/2) for the standard deviation. 2) Where is my mistake evaluating the probability of 486 and 487. 3) How to solve it using the CLT. Thanks.",,"['probability', 'probability-theory', 'dice']"
74,Measuring $\pi$ by throwing darts,Measuring  by throwing darts,\pi,"I want to give an approximation of $\pi$ in this way:  I inscribe a circle in a square then I throw darts at random on the square from far away. If the darts falling on the square are $n$ and the darts falling on the circle are $m<n$ I approximate $\pi$ with $4 \frac{m}{n}$. Suppose I want an approximation such that $|4\frac{m}{n}-\pi|<0,0001$. How can I quantify how many shots I have to do at least (before doing the experiment)? I know that the strong law of large numbers tells me that $P(\lim_{n\to\infty} 4\frac{m}{n}=\pi)=1$, but I can't do an infinite number of shots so I try with the weak law: $\lim_{n\to\infty}P(|4\frac{m}{n}-\pi|<0,0001)=1$. Again, this seems to be unhelpful to my cause. Maybe I could take the compromise that I want an approximation such that $|4\frac{m}{n}-\pi|<0,0001$ with a probability greater than $0,95$, but even if I succeed in this goal, nothing assures me that in $5\%$ of cases, the approximation obtained is such that $|4\frac{m}{n}-\pi|>3$. What your approach to this problem would be?","I want to give an approximation of $\pi$ in this way:  I inscribe a circle in a square then I throw darts at random on the square from far away. If the darts falling on the square are $n$ and the darts falling on the circle are $m<n$ I approximate $\pi$ with $4 \frac{m}{n}$. Suppose I want an approximation such that $|4\frac{m}{n}-\pi|<0,0001$. How can I quantify how many shots I have to do at least (before doing the experiment)? I know that the strong law of large numbers tells me that $P(\lim_{n\to\infty} 4\frac{m}{n}=\pi)=1$, but I can't do an infinite number of shots so I try with the weak law: $\lim_{n\to\infty}P(|4\frac{m}{n}-\pi|<0,0001)=1$. Again, this seems to be unhelpful to my cause. Maybe I could take the compromise that I want an approximation such that $|4\frac{m}{n}-\pi|<0,0001$ with a probability greater than $0,95$, but even if I succeed in this goal, nothing assures me that in $5\%$ of cases, the approximation obtained is such that $|4\frac{m}{n}-\pi|>3$. What your approach to this problem would be?",,"['probability', 'probability-theory', 'probability-limit-theorems', 'law-of-large-numbers']"
75,Joint pdf of discrete and continuous random variables,Joint pdf of discrete and continuous random variables,,"Consider two independent random variables $X$ and $Y$, where $X$ is uniformly distributed on the interval $[0,1]$ and $Y$ is uniformly distributed on the set $\{0,1\}$. Thus, the cdfs are given by  $F_X(x) = \begin{cases} 0 & x <0\\ x & 0 \leq x <1\\ 1 &  else \end{cases}$ and $F_Y(y) =  \begin{cases} 0 & y <0\\ 1/2 & 0 \leq y <1\\ 1 &  else. \end{cases}$ Consider the random variable $Z = (X,Y)$ with cdf is given by $F_Z(x,y) =  \begin{cases} 0 & y<0\\  F_X(x)/2& 0 \leq y < 1\\ F_X(x) & else. \end{cases}$ Now, what I'm interested in is a pdf of $Z$. Is $ f_Z(x,y) =  \begin{cases} 0 & y<0\\ f_X(x)/2& 0 \leq y < 1\\ f_X(x) & else. \end{cases} $ the pdf of $Z$? More generally, I'm interested in the joint pdf of independent random variables, one of which is continuous and the others (possibly more than one) are discrete. If correct, can the above be applied in this case? Thank you for your help.","Consider two independent random variables $X$ and $Y$, where $X$ is uniformly distributed on the interval $[0,1]$ and $Y$ is uniformly distributed on the set $\{0,1\}$. Thus, the cdfs are given by  $F_X(x) = \begin{cases} 0 & x <0\\ x & 0 \leq x <1\\ 1 &  else \end{cases}$ and $F_Y(y) =  \begin{cases} 0 & y <0\\ 1/2 & 0 \leq y <1\\ 1 &  else. \end{cases}$ Consider the random variable $Z = (X,Y)$ with cdf is given by $F_Z(x,y) =  \begin{cases} 0 & y<0\\  F_X(x)/2& 0 \leq y < 1\\ F_X(x) & else. \end{cases}$ Now, what I'm interested in is a pdf of $Z$. Is $ f_Z(x,y) =  \begin{cases} 0 & y<0\\ f_X(x)/2& 0 \leq y < 1\\ f_X(x) & else. \end{cases} $ the pdf of $Z$? More generally, I'm interested in the joint pdf of independent random variables, one of which is continuous and the others (possibly more than one) are discrete. If correct, can the above be applied in this case? Thank you for your help.",,"['probability', 'probability-distributions', 'random-variables']"
76,Proof that $2^n-(n+1) $ equations are necessary to establish the independence of n events.,Proof that  equations are necessary to establish the independence of n events.,2^n-(n+1) ,"Suppose $A_1,A_2,\cdots,A_n$ are $n$ events, we say that they are all independent if for all $\{i_1,\cdots, i_m\}\subset \{1,2,\cdots,n\}$(where $m\ge 2$), we have $$\mathrm{Pr}[A_{i_1}\cap A_{i_2}\cdots\cap A_{i_m}]=\mathrm{Pr}[A_{i_1}]\times\mathrm{Pr}[A_{i_2}]\cdots\times\mathrm{Pr}[A_{i_m}].$$ One can easily observe that there are $2^n-(n+1)$ relations, my question is that if these relations are all necessary to establish the independence of $A_i$s?(i.e. can one of the relation be derived from any of other relations?) I think this is necessary, but I don't know how to prove.","Suppose $A_1,A_2,\cdots,A_n$ are $n$ events, we say that they are all independent if for all $\{i_1,\cdots, i_m\}\subset \{1,2,\cdots,n\}$(where $m\ge 2$), we have $$\mathrm{Pr}[A_{i_1}\cap A_{i_2}\cdots\cap A_{i_m}]=\mathrm{Pr}[A_{i_1}]\times\mathrm{Pr}[A_{i_2}]\cdots\times\mathrm{Pr}[A_{i_m}].$$ One can easily observe that there are $2^n-(n+1)$ relations, my question is that if these relations are all necessary to establish the independence of $A_i$s?(i.e. can one of the relation be derived from any of other relations?) I think this is necessary, but I don't know how to prove.",,"['probability', 'combinatorics', 'independence']"
77,No Adjacency Combinatorics Problem via Generating Function,No Adjacency Combinatorics Problem via Generating Function,,"I would like to find the generating function solution for the following combinatorics/probability problem. I have a combinatorial solution and the generating function deduced thereof. But I can not fathom the direct interpretation of it. I would like to reverse the current solving process and directly deduce the following or some other generating function, then get the relevant coefficient as the solution. In a stack of $W$ white cards, $G$ green cards and $R$ red cards, what is the number of arrangement for no green card is next to a red card? One solution is to view the $W$ white cards as dividers and leaving $W+1$ slots in between them together with both ends. For each $r\in \{1,2,\dots,R\}$ and $g\in\{1,2,\dots, G\}$, designate non-overlappingly $r$ slots for inserting the red cards and $g$ slots for the green cards. There are ${W+1\choose r,g}$ combinations. For each of these combinations, we insert $R$ red cards into the previously designated $r$ red slots, and there are ${R-1\choose r-1}$ ways of insertion. Do the same for the $g$ green cards and there are ${G-1\choose g-1}$ ways of insertion. Multiplying them and summing over $r$ and $g$, we arrive at the desired total number of arrangements: $$\sum_{r=1}^R\sum_{g=1}^G{W+1\choose r,g}{R-1\choose R-r}{G-1\choose G-g}.$$ But this is the coefficient of $x^Ry^G$ in $(1+x+y)^{W+1}(1+x)^{R-1}(1+y)^{G-1}$. Now how do I deduce this generating function directly from the problem?","I would like to find the generating function solution for the following combinatorics/probability problem. I have a combinatorial solution and the generating function deduced thereof. But I can not fathom the direct interpretation of it. I would like to reverse the current solving process and directly deduce the following or some other generating function, then get the relevant coefficient as the solution. In a stack of $W$ white cards, $G$ green cards and $R$ red cards, what is the number of arrangement for no green card is next to a red card? One solution is to view the $W$ white cards as dividers and leaving $W+1$ slots in between them together with both ends. For each $r\in \{1,2,\dots,R\}$ and $g\in\{1,2,\dots, G\}$, designate non-overlappingly $r$ slots for inserting the red cards and $g$ slots for the green cards. There are ${W+1\choose r,g}$ combinations. For each of these combinations, we insert $R$ red cards into the previously designated $r$ red slots, and there are ${R-1\choose r-1}$ ways of insertion. Do the same for the $g$ green cards and there are ${G-1\choose g-1}$ ways of insertion. Multiplying them and summing over $r$ and $g$, we arrive at the desired total number of arrangements: $$\sum_{r=1}^R\sum_{g=1}^G{W+1\choose r,g}{R-1\choose R-r}{G-1\choose G-g}.$$ But this is the coefficient of $x^Ry^G$ in $(1+x+y)^{W+1}(1+x)^{R-1}(1+y)^{G-1}$. Now how do I deduce this generating function directly from the problem?",,"['probability', 'combinatorics', 'generating-functions']"
78,Optimizing screening sample size,Optimizing screening sample size,,"Consider a machine like so: Every time I activate it, the machine generates a ""sufficiently large"" pool of plastic balls for me. Each ball is made out of colored plastic and painted over with white paint. I do not see the balls being created, and the white coat makes it impossible to tell what color the balls are inside. The machine chooses red, green or blue plastic for each ball randomly and with equal probability of each. I can only activate the machine once per day. The only way to determine the true color of a ball is to use a chemical stripping process that takes a whole day. Luckily, you can do more than one ball in parallel, but there is still an extra cost per-ball (although doing $2$ balls in one strip is still much better than doing $1$ ball each in $2$ strips). My objective is to obtain $1$ ball of every color. To do this, I am using the following strategy: Activate the machine, getting a large pool of balls. Pick a small number of balls to ""screen"" (for instance, $5$). Take this ""screening set"", strip them all in one go, take one ball of each color and discard the rest. At the end of this, I will probably get one of each ball, but there is a chance that I might not get every kind. If that happens, I will have to go back and screen some more balls, which will cost me an extra day - but time is money and I am very impatient. Therefore, I don't want to screen too few balls at once (the extreme case is screening 1 ball at a time, which will take forever). On the other hand, stripping balls costs money, so I don't want to strip more balls than I have to. For example, screening a whole pool of $1000$ balls when there are only $3$ colors will almost guarantee success, but I'm not really getting much more assurance than a smaller screen (e.g. $10$ balls) while it costs me $100$ times more. Obviously, if I want $100\%$ probability to catch all my balls in the first screen, I need to screen an infinite number of them. However, if I can accept a $95\%$ probability of needing more than one screen, then there is a finite and small number that will accomplish this. For $3$ colors with equal probability, the chances of needing a second screen after screening $n$ balls are: $$ 3\cdot\left(\frac{1}{3}\right)^n + 3\cdot\left(\frac{2}{3}\right)^n  $$ If I want $95\%$ or more confidence, I set this to: $$ 3\cdot\left(\frac{1}{3}\right)^n + 3\cdot\left(\frac{2}{3}\right)^n \le 0.05 $$ Then solve for $n$. (in this case it comes out $2.68$, so I would screen $3$ balls at a time) Is it possible to generalize this for any desired minimum confidence $p$ of getting all colors on each try, $q$ colors of equal probability, and derive a function of $q$ and $p$ that gives $n$? If you are curious, I am trying to figure out how many colonies to screen after a single transformation with a pooled sample. The machine producing a number of balls represents colonies you get from one transformation, the balls themselves are strains of bacteria, the color represents the plasmid carried by each strain, and the paint stripping process represents extracting and sequencing the plasmid.","Consider a machine like so: Every time I activate it, the machine generates a ""sufficiently large"" pool of plastic balls for me. Each ball is made out of colored plastic and painted over with white paint. I do not see the balls being created, and the white coat makes it impossible to tell what color the balls are inside. The machine chooses red, green or blue plastic for each ball randomly and with equal probability of each. I can only activate the machine once per day. The only way to determine the true color of a ball is to use a chemical stripping process that takes a whole day. Luckily, you can do more than one ball in parallel, but there is still an extra cost per-ball (although doing $2$ balls in one strip is still much better than doing $1$ ball each in $2$ strips). My objective is to obtain $1$ ball of every color. To do this, I am using the following strategy: Activate the machine, getting a large pool of balls. Pick a small number of balls to ""screen"" (for instance, $5$). Take this ""screening set"", strip them all in one go, take one ball of each color and discard the rest. At the end of this, I will probably get one of each ball, but there is a chance that I might not get every kind. If that happens, I will have to go back and screen some more balls, which will cost me an extra day - but time is money and I am very impatient. Therefore, I don't want to screen too few balls at once (the extreme case is screening 1 ball at a time, which will take forever). On the other hand, stripping balls costs money, so I don't want to strip more balls than I have to. For example, screening a whole pool of $1000$ balls when there are only $3$ colors will almost guarantee success, but I'm not really getting much more assurance than a smaller screen (e.g. $10$ balls) while it costs me $100$ times more. Obviously, if I want $100\%$ probability to catch all my balls in the first screen, I need to screen an infinite number of them. However, if I can accept a $95\%$ probability of needing more than one screen, then there is a finite and small number that will accomplish this. For $3$ colors with equal probability, the chances of needing a second screen after screening $n$ balls are: $$ 3\cdot\left(\frac{1}{3}\right)^n + 3\cdot\left(\frac{2}{3}\right)^n  $$ If I want $95\%$ or more confidence, I set this to: $$ 3\cdot\left(\frac{1}{3}\right)^n + 3\cdot\left(\frac{2}{3}\right)^n \le 0.05 $$ Then solve for $n$. (in this case it comes out $2.68$, so I would screen $3$ balls at a time) Is it possible to generalize this for any desired minimum confidence $p$ of getting all colors on each try, $q$ colors of equal probability, and derive a function of $q$ and $p$ that gives $n$? If you are curious, I am trying to figure out how many colonies to screen after a single transformation with a pooled sample. The machine producing a number of balls represents colonies you get from one transformation, the balls themselves are strains of bacteria, the color represents the plasmid carried by each strain, and the paint stripping process represents extracting and sequencing the plasmid.",,"['probability', 'binomial-distribution']"
79,"Casino turns 50% of your losses into ""free play"", are odds in your favor?","Casino turns 50% of your losses into ""free play"", are odds in your favor?",,"As a limited-time promotion, if you gamble during your first week at this casino, and you suffer a net loss of money, the casino will give you half of your losses (up to a certain amount) as ""free play"", or cash only usable for more gambling. It seems to me that for a simple game of slot machines, where the payback is mandated to be $95\%$ (avg) of what you bet, this tips the odds in your favor. What is an optimal/very good strategy (highest % average return) using slot machines (or anything) for taking advantage of this scenario? Are the odds in your favor with this strategy? Consider a machine with $95\%$ payout and a $50/50$ binary outcome (lose it all or not): It seems to me the simplest strategy, finding a machine that takes $1.00$ and has a $50\%$ chance of paying $1.90$ and $50\%$ chance of paying $0.00$, is in one's favor, since the half the time you would lose, you can get half of it back and bet $0.50$ for a $50/50$ payout of $0.95$ or $0$ (on a bet of $0.50$), for a total average outcome of $1.90\times0.5 + 0.95\times0.25 + 0\times0.25 = 1.1875$ on a $1.00$ bet, or a profit of $0.1875$. I know one can solve the problem as a generalized binary (money or no money) two-action case (just like above) given a mandated payout of $95\%$ and some probability of winning (like $50\%$ above), and maximize the average outcome. Losses incurred with ""free play"" are not $50\%$ redeemable like with the original funds.","As a limited-time promotion, if you gamble during your first week at this casino, and you suffer a net loss of money, the casino will give you half of your losses (up to a certain amount) as ""free play"", or cash only usable for more gambling. It seems to me that for a simple game of slot machines, where the payback is mandated to be $95\%$ (avg) of what you bet, this tips the odds in your favor. What is an optimal/very good strategy (highest % average return) using slot machines (or anything) for taking advantage of this scenario? Are the odds in your favor with this strategy? Consider a machine with $95\%$ payout and a $50/50$ binary outcome (lose it all or not): It seems to me the simplest strategy, finding a machine that takes $1.00$ and has a $50\%$ chance of paying $1.90$ and $50\%$ chance of paying $0.00$, is in one's favor, since the half the time you would lose, you can get half of it back and bet $0.50$ for a $50/50$ payout of $0.95$ or $0$ (on a bet of $0.50$), for a total average outcome of $1.90\times0.5 + 0.95\times0.25 + 0\times0.25 = 1.1875$ on a $1.00$ bet, or a profit of $0.1875$. I know one can solve the problem as a generalized binary (money or no money) two-action case (just like above) given a mandated payout of $95\%$ and some probability of winning (like $50\%$ above), and maximize the average outcome. Losses incurred with ""free play"" are not $50\%$ redeemable like with the original funds.",,"['probability', 'optimization', 'gambling']"
80,Intuition behind measurable random variables and $\sigma$-algebra,Intuition behind measurable random variables and -algebra,\sigma,"I've been trying to understand $\sigma$-algebras and how it encodes information in context of filtration. While certain parts seem clear and logical, I can't say I get the whole picture. I'll try to explain the counter-intuition I get with the classical example of the coin tossing : the probability space $\Omega = \{ HH, HT, TH, TT \}$ and a r.v. $X(\omega)$ equal to the number of heads. At times $0$, $1$ and $2$ the available information is represented using $\sigma$-algebras $\mathcal{F}_0=\{\emptyset,\Omega\}$, $\mathcal{F}_1=\{\emptyset, \Omega, \{HH,HT\},\{TH,TT\}\}$ and $\mathcal{F}_2=\{\emptyset, \Omega,\{HH,HT\},\{TH,TT\},\{HH\},\{HT\},\{TH\},\{TT\}\}$. One can notice that $X(\omega)$ is not measurable with respect to $\mathcal{F}_0$ and $\mathcal{F}_1$, because $X^{-1}((\frac{3}{2}; +\infty))=\{HH\}$. To me it is quite surprising: intuitively $X$ makes perfect sense at all times. In particular it has an expected value at time $0$, which I interpret as that the probability and value of all outcomes $\{\omega\}$ can be computed. How do you think of a non-measurable function? Here's another way of expressing the same confusion. The most natural choice of $\sigma$-algebra in a finite discrete case is $\mathcal{F}=2^\Omega$, and it is implicitly used in all elementary probability problems. However, this choice of $\mathcal{F}$ does not reflect the fact that some information is known or unknown, conditional probability does. Does it mean that the statement ""$\sigma$-algebra is known information"" make sense only in conditioning? Why is it convenient then?","I've been trying to understand $\sigma$-algebras and how it encodes information in context of filtration. While certain parts seem clear and logical, I can't say I get the whole picture. I'll try to explain the counter-intuition I get with the classical example of the coin tossing : the probability space $\Omega = \{ HH, HT, TH, TT \}$ and a r.v. $X(\omega)$ equal to the number of heads. At times $0$, $1$ and $2$ the available information is represented using $\sigma$-algebras $\mathcal{F}_0=\{\emptyset,\Omega\}$, $\mathcal{F}_1=\{\emptyset, \Omega, \{HH,HT\},\{TH,TT\}\}$ and $\mathcal{F}_2=\{\emptyset, \Omega,\{HH,HT\},\{TH,TT\},\{HH\},\{HT\},\{TH\},\{TT\}\}$. One can notice that $X(\omega)$ is not measurable with respect to $\mathcal{F}_0$ and $\mathcal{F}_1$, because $X^{-1}((\frac{3}{2}; +\infty))=\{HH\}$. To me it is quite surprising: intuitively $X$ makes perfect sense at all times. In particular it has an expected value at time $0$, which I interpret as that the probability and value of all outcomes $\{\omega\}$ can be computed. How do you think of a non-measurable function? Here's another way of expressing the same confusion. The most natural choice of $\sigma$-algebra in a finite discrete case is $\mathcal{F}=2^\Omega$, and it is implicitly used in all elementary probability problems. However, this choice of $\mathcal{F}$ does not reflect the fact that some information is known or unknown, conditional probability does. Does it mean that the statement ""$\sigma$-algebra is known information"" make sense only in conditioning? Why is it convenient then?",,"['probability', 'measure-theory', 'probability-theory']"
81,The Lost Boarding Pass Advanced,The Lost Boarding Pass Advanced,,"The Lost Boarding Pass is a famous puzzle as follows: On a sold out flight, $100$ people line up to board the plane. The first passenger in the line has lost his boarding pass, but was allowed in, regardless. He takes a random seat. Each subsequent passenger takes his or her assigned seat if available, or a random unoccupied seat, otherwise. What is the probability that the last passenger to board the plane finds his seat unoccupied? It is not difficult to prove that the answer is $\frac 12$. After seeing this I wanted to find the chance that the $k$ person to enter would end up at his spot. this resulted in the next function: If $k\neq 1$ then: $P(n,k) = \dfrac{n-k+1}{n-k+2}$, $n$ being the number of people. and for $k=1$: $P(n,1) = \frac 1n$ Now I'm interested in the chance that a number of people would sit at their place. (If it helps I know the chance of the k person to enter to sit at the l one's spot if $l=1$ or $l>k$ is $\dfrac{1}{(n-k)(n-k+2)}$) Thank you in advance.","The Lost Boarding Pass is a famous puzzle as follows: On a sold out flight, $100$ people line up to board the plane. The first passenger in the line has lost his boarding pass, but was allowed in, regardless. He takes a random seat. Each subsequent passenger takes his or her assigned seat if available, or a random unoccupied seat, otherwise. What is the probability that the last passenger to board the plane finds his seat unoccupied? It is not difficult to prove that the answer is $\frac 12$. After seeing this I wanted to find the chance that the $k$ person to enter would end up at his spot. this resulted in the next function: If $k\neq 1$ then: $P(n,k) = \dfrac{n-k+1}{n-k+2}$, $n$ being the number of people. and for $k=1$: $P(n,1) = \frac 1n$ Now I'm interested in the chance that a number of people would sit at their place. (If it helps I know the chance of the k person to enter to sit at the l one's spot if $l=1$ or $l>k$ is $\dfrac{1}{(n-k)(n-k+2)}$) Thank you in advance.",,['probability']
82,Probability question - tagging birds and then determining total number based on how many are tagged,Probability question - tagging birds and then determining total number based on how many are tagged,,"I tag 10 birds, and then release them into the wild. I decide to catch the birds until I catch 10 birds that I haven't tagged yet. If I catch 12 birds, what is an estimate for the total number of birds in the wild? My preliminary guess is 10/2 * 12 = 60 birds, but I have a feeling its more complicated than that.","I tag 10 birds, and then release them into the wild. I decide to catch the birds until I catch 10 birds that I haven't tagged yet. If I catch 12 birds, what is an estimate for the total number of birds in the wild? My preliminary guess is 10/2 * 12 = 60 birds, but I have a feeling its more complicated than that.",,['probability']
83,Markov Property Confusion,Markov Property Confusion,,"I feel like I'm being very dense/employing some sort of circular reasoning, but I'm having trouble understanding the Markov Property. According to Durrett (ISBN-10:1461436141), $X_n$ is a Markov chain with transition matrix $p(i,j)$ if for any $j, i, i_{n-1}, \ldots, i_0$: $$P(X_{n+1}=j \mid X_n=i, X_{n-1}=i_{n-1}, \ldots, X_0=i_0)=p(i,j)$$ It seems to me that this property can never be satisfied. For example, suppose the transition matrix: \begin{array}{c|ccc}   & a & b & c \\ \hline a & 0 & 1 & 0 \\ b & 0.5 & 0 & 0.5 \\ c & 0 & 1 & 0  \end{array} which is graphically represented as: $P(X_3=c \vert X_2=b, X_1=b)=0 \neq p(b,c)=0.5$.  I can similarly do this to any chain and have every probability be equal to $0$. Is this because I'm conditioning on an event with 0 probability?  Or is it because I should be saying $$\sum_{x \in \{a,b,c\}} P(X_3=c \mid X_2=b, X_1=x)=0.5=p(b,c)$$ Or am I just thinking about it completely wrong?","I feel like I'm being very dense/employing some sort of circular reasoning, but I'm having trouble understanding the Markov Property. According to Durrett (ISBN-10:1461436141), $X_n$ is a Markov chain with transition matrix $p(i,j)$ if for any $j, i, i_{n-1}, \ldots, i_0$: $$P(X_{n+1}=j \mid X_n=i, X_{n-1}=i_{n-1}, \ldots, X_0=i_0)=p(i,j)$$ It seems to me that this property can never be satisfied. For example, suppose the transition matrix: \begin{array}{c|ccc}   & a & b & c \\ \hline a & 0 & 1 & 0 \\ b & 0.5 & 0 & 0.5 \\ c & 0 & 1 & 0  \end{array} which is graphically represented as: $P(X_3=c \vert X_2=b, X_1=b)=0 \neq p(b,c)=0.5$.  I can similarly do this to any chain and have every probability be equal to $0$. Is this because I'm conditioning on an event with 0 probability?  Or is it because I should be saying $$\sum_{x \in \{a,b,c\}} P(X_3=c \mid X_2=b, X_1=x)=0.5=p(b,c)$$ Or am I just thinking about it completely wrong?",,"['probability', 'probability-theory', 'stochastic-processes', 'markov-chains', 'markov-process']"
84,Joint probability distribution of sum and product of two random variables,Joint probability distribution of sum and product of two random variables,,"Let $X$ and $Y$ be two discrete random variables. I know the joint probability distribution of the vector $(X,Y)$, namely $P(X = x, Y = y)$ for all $x$ and $y$ in the sample spaces $\Omega_X$ and $\Omega_Y$, respectively. Using the joint distribution, I discovered that $X$ and $Y$ are conditionally dependent. Now let $U := X + Y$ and $V := X \cdot Y$ be two random variables. I need to calculate the distribution of both $U$ and $V$. Is it correct to calculate the distribution of $U$ as \begin{align}   P(U = u) = \sum_{x \in \Omega_X, y \in \Omega_Y \text{ such that } x \cdot y = u} P(X = x, Y = y), \end{align} even though $X$ and $Y$ are conditionally dependent? Furthermore I need to calculate the joint distribution of the probabilty vector $(U,V)$. I know that the distribution can be calculated as \begin{align}   P(U = u, V = v) = P(U = u) \cdot P(V = v), \end{align} if $U$ and $V$ are conditionally independent. However, to show that $U$ and $V$ are conditionally independent, I would look at the joint distribution, which I have to calculate. How does one calculate the joint distribution of $(U,V)$, if the variables are conditionally dependent? Furthermore, is there any way to find out whether $U$ and $V$ are conditionally dependent or not, without using the joint probability distribution? Since $U$ and $V$ are defined as sum and product of two random variables (which are conditionally dependent), I have the feeling that $U$ and $V$ are conditionally dependent.","Let $X$ and $Y$ be two discrete random variables. I know the joint probability distribution of the vector $(X,Y)$, namely $P(X = x, Y = y)$ for all $x$ and $y$ in the sample spaces $\Omega_X$ and $\Omega_Y$, respectively. Using the joint distribution, I discovered that $X$ and $Y$ are conditionally dependent. Now let $U := X + Y$ and $V := X \cdot Y$ be two random variables. I need to calculate the distribution of both $U$ and $V$. Is it correct to calculate the distribution of $U$ as \begin{align}   P(U = u) = \sum_{x \in \Omega_X, y \in \Omega_Y \text{ such that } x \cdot y = u} P(X = x, Y = y), \end{align} even though $X$ and $Y$ are conditionally dependent? Furthermore I need to calculate the joint distribution of the probabilty vector $(U,V)$. I know that the distribution can be calculated as \begin{align}   P(U = u, V = v) = P(U = u) \cdot P(V = v), \end{align} if $U$ and $V$ are conditionally independent. However, to show that $U$ and $V$ are conditionally independent, I would look at the joint distribution, which I have to calculate. How does one calculate the joint distribution of $(U,V)$, if the variables are conditionally dependent? Furthermore, is there any way to find out whether $U$ and $V$ are conditionally dependent or not, without using the joint probability distribution? Since $U$ and $V$ are defined as sum and product of two random variables (which are conditionally dependent), I have the feeling that $U$ and $V$ are conditionally dependent.",,"['probability', 'probability-distributions']"
85,Converge to Brownian Motion problem,Converge to Brownian Motion problem,,"Consider the following sequence of SDEs: $dX^n_t = \sin(nX^n_t)dt + dW_t, X^n_0 = 0\,\,\,$ Show that the solutions $X^n$ converge in finite dimensional distribution to Brownian Motion. I have been working on this for a long time and can't get anywhere. Any help would be appreciated. Thank you.","Consider the following sequence of SDEs: $dX^n_t = \sin(nX^n_t)dt + dW_t, X^n_0 = 0\,\,\,$ Show that the solutions $X^n$ converge in finite dimensional distribution to Brownian Motion. I have been working on this for a long time and can't get anywhere. Any help would be appreciated. Thank you.",,"['probability', 'probability-theory', 'stochastic-processes', 'stochastic-calculus', 'brownian-motion']"
86,Alternative Monty Hall Problem,Alternative Monty Hall Problem,,"So the typical set up for Monty Hall problem, there are 3 doors where 2 have goats and 1 has a car. I, the contestant, get to randomly guess a door looking to get the one with the car, after this the host will open a door that will always be a goat. Thus out the two doors that are left, I have to choose to stay with the original door I chose or switch to the other door. As many analysis of this problem have been done, switching my choice gives me a higher probability of winning. This largely has to do with the fact that since the host always reveals a goat the asking of whether to stay or not, is the same as did you guess right or not, and you have $\frac{2}{3}$ of wrong so you should switch Now it seems, this ""strange"" result largely has to do with the fact that the host always reveals a goat. But what if alternatively you had this situation You are given 3 doors, 2 with a goat and 1 with a car. You randomly choose a door (looking to get one with the car). The host will randomly choose to reveal what is behind one of the 2 doors you haven't chosen. Given that he reveals goat, what is the probability of getting car if you chose to stay with choice? My analysis of this problem goes as follows: Let $D$ be event the door I guessed has car and Let $G$ reprsent the event that host reveals a goat thus what I want to calculate is $P(D|G)$ with this I have $$P(D|G)=\frac{P(D\cap G)}{P(G)}=\frac{P(G|D)P(D)}{P(G|D)P(D)+P(G|D^{c})P(D^{c})}=\frac{1\left(\frac{1}{3}\right)}{1\left(\frac{1}{3}\right)+\frac{1}{2}\left(\frac{2}{3}\right)}=\frac{1}{2}$$ So it seems it doesn't matter if I choose to switch or not, and this is the result most people come up with when first thinking of problem. Question : First is my analysis correct for this problem? Second, is it true in general that if you guess out of $n$ doors and host reveals $k$ doors that all have goats, will the probability that car is behind door you choose just $\dfrac{1}{n-k}$? UPDATE So I ended up asking my statistics/probability professor about this question and he said the result I got was correct. He explained that the reasoning that the Monty Hall problem inherently causes confusion is because many don't notice the only randomness in the original problem is just in your choice while hosts choice of door is deterministic. Now the problem I asked now has two sets of randomness, your original choice of door and the hosts choice thus the problems are inherently different","So the typical set up for Monty Hall problem, there are 3 doors where 2 have goats and 1 has a car. I, the contestant, get to randomly guess a door looking to get the one with the car, after this the host will open a door that will always be a goat. Thus out the two doors that are left, I have to choose to stay with the original door I chose or switch to the other door. As many analysis of this problem have been done, switching my choice gives me a higher probability of winning. This largely has to do with the fact that since the host always reveals a goat the asking of whether to stay or not, is the same as did you guess right or not, and you have $\frac{2}{3}$ of wrong so you should switch Now it seems, this ""strange"" result largely has to do with the fact that the host always reveals a goat. But what if alternatively you had this situation You are given 3 doors, 2 with a goat and 1 with a car. You randomly choose a door (looking to get one with the car). The host will randomly choose to reveal what is behind one of the 2 doors you haven't chosen. Given that he reveals goat, what is the probability of getting car if you chose to stay with choice? My analysis of this problem goes as follows: Let $D$ be event the door I guessed has car and Let $G$ reprsent the event that host reveals a goat thus what I want to calculate is $P(D|G)$ with this I have $$P(D|G)=\frac{P(D\cap G)}{P(G)}=\frac{P(G|D)P(D)}{P(G|D)P(D)+P(G|D^{c})P(D^{c})}=\frac{1\left(\frac{1}{3}\right)}{1\left(\frac{1}{3}\right)+\frac{1}{2}\left(\frac{2}{3}\right)}=\frac{1}{2}$$ So it seems it doesn't matter if I choose to switch or not, and this is the result most people come up with when first thinking of problem. Question : First is my analysis correct for this problem? Second, is it true in general that if you guess out of $n$ doors and host reveals $k$ doors that all have goats, will the probability that car is behind door you choose just $\dfrac{1}{n-k}$? UPDATE So I ended up asking my statistics/probability professor about this question and he said the result I got was correct. He explained that the reasoning that the Monty Hall problem inherently causes confusion is because many don't notice the only randomness in the original problem is just in your choice while hosts choice of door is deterministic. Now the problem I asked now has two sets of randomness, your original choice of door and the hosts choice thus the problems are inherently different",,"['probability', 'conditional-probability', 'monty-hall']"
87,Placing a circle in a square lattice,Placing a circle in a square lattice,,"Two part question. Consider the square lattice $\mathbb{Z}^2$ : Imagine you are going to place a circle of radius $r$ somewhere in $\mathbb{R}^2$ . Question 1: What is the radius of the largest circle that cannot be placed anywhere in $\mathbb{R}^2$ without overlapping or containing any of the points in $\mathbb{Z}^2$ ? I'm fairly certain the answer is $r < \frac{\sqrt{2}}{2}$ . If you place a circle with center, for example $\left(\frac{1}{2}, \frac{1}{2}\right)$ , computing the radius that would touch the four corners is quite easy, but I don't know how to prove that this is the best place to do it, as intuitively obvious as it might be. Question 2: A circle with radius $r$ , such that $0 < r \leq \frac{\sqrt{2}}{2}$ is randomly placed somewhere in $\mathbb{R}^2$ . What is the probability, as a function of $r$ , that the circle contains or overlaps at least one point in the lattice? My suspicion is to try to only consider a limited subset of the 2D plane, e.g. $-1 < x < 1$ and $-1 < y < 1$ , and then do something with ratios of the area of the circle and the area of the limited region, but I'm not exactly sure what I would do with that. Obviously the probability is 1 for all $r \geq \frac{\sqrt{2}}{2}$ , because of question 1.","Two part question. Consider the square lattice : Imagine you are going to place a circle of radius somewhere in . Question 1: What is the radius of the largest circle that cannot be placed anywhere in without overlapping or containing any of the points in ? I'm fairly certain the answer is . If you place a circle with center, for example , computing the radius that would touch the four corners is quite easy, but I don't know how to prove that this is the best place to do it, as intuitively obvious as it might be. Question 2: A circle with radius , such that is randomly placed somewhere in . What is the probability, as a function of , that the circle contains or overlaps at least one point in the lattice? My suspicion is to try to only consider a limited subset of the 2D plane, e.g. and , and then do something with ratios of the area of the circle and the area of the limited region, but I'm not exactly sure what I would do with that. Obviously the probability is 1 for all , because of question 1.","\mathbb{Z}^2 r \mathbb{R}^2 \mathbb{R}^2 \mathbb{Z}^2 r < \frac{\sqrt{2}}{2} \left(\frac{1}{2}, \frac{1}{2}\right) r 0 < r \leq \frac{\sqrt{2}}{2} \mathbb{R}^2 r -1 < x < 1 -1 < y < 1 r \geq \frac{\sqrt{2}}{2}","['probability', 'circles', 'integer-lattices']"
88,Expected value of sock pairs,Expected value of sock pairs,,"Suppose that $N$ pairs of socks are put in a washing machine, with each sock having one mate. If the washing machine randomly eats socks, and at the end of the wash returns a random number $K$ of socks where $0 \leq K \leq 2N$, where each $K$ is equally probable, what is the expected number of complete pairs of returned socks? Just from working out the first few values of $N$, I conjecture that the answer is $N/3$, but I am not sure how to prove it for all values of $N$. Just to be clear, this is the expected value for an infinite number of trials where each $K$ is equally probable, not the expected value for an infinite number of trials where $K$ is fixed, whereupon the answer is $\displaystyle{K \choose 2}/(2N-1)$.","Suppose that $N$ pairs of socks are put in a washing machine, with each sock having one mate. If the washing machine randomly eats socks, and at the end of the wash returns a random number $K$ of socks where $0 \leq K \leq 2N$, where each $K$ is equally probable, what is the expected number of complete pairs of returned socks? Just from working out the first few values of $N$, I conjecture that the answer is $N/3$, but I am not sure how to prove it for all values of $N$. Just to be clear, this is the expected value for an infinite number of trials where each $K$ is equally probable, not the expected value for an infinite number of trials where $K$ is fixed, whereupon the answer is $\displaystyle{K \choose 2}/(2N-1)$.",,['probability']
89,"The PMF of the larger of two numbers selected at random from $1,\dots,12$",The PMF of the larger of two numbers selected at random from,"1,\dots,12","Two balls are chosen at random from a box containing 12 balls, numbered 1;2; : : : ;12. Let X be the   larger of the two numbers obtained. Compute the PMF of X, if the sampling is done (a) without replacement; (b) with replacement I understand the numerator for both cases. In case 1, we have 1 ball x such that X=x and x-1 balls smaller than x, so we have 1(x-1) = x-1. In case two, we have x choices for a ball, then x-1 choices, so x+x-1 = 2x-1. The denominator is troubling me and it's a core probability concept I never understood In case 1, we have 12 options for the first choice and 11 for the second. Therefore, the total number of possibilities should be 12*11 = 132. But it's not, it's half of that, which is 66 In case 2, we have 12 options for the first choice and 12 for the second. Total possibilities is 12*12 = 144. This is correct Why am I right in the 2nd case but wrong in the first? We had a similar question on our midterm: We have 7 unique children and 20 identical cookies. How many ways can we distribute the cookies such that each child gets one? I thought if each child gets one, there are 13 left. Each of those 13 cookies can go to one of 7 children, so the answer should be 7^13. After learning the stars and stripes method, I realize the correct calculation leads to 19C7. I'm wondering WHY it is that my calculation is wrong, why there are two possible calculations, and what my calculation represents. It is this concept that I never understood that is still giving me trouble. I can do PMF and density functions, but I have trouble with this simplest concept","Two balls are chosen at random from a box containing 12 balls, numbered 1;2; : : : ;12. Let X be the   larger of the two numbers obtained. Compute the PMF of X, if the sampling is done (a) without replacement; (b) with replacement I understand the numerator for both cases. In case 1, we have 1 ball x such that X=x and x-1 balls smaller than x, so we have 1(x-1) = x-1. In case two, we have x choices for a ball, then x-1 choices, so x+x-1 = 2x-1. The denominator is troubling me and it's a core probability concept I never understood In case 1, we have 12 options for the first choice and 11 for the second. Therefore, the total number of possibilities should be 12*11 = 132. But it's not, it's half of that, which is 66 In case 2, we have 12 options for the first choice and 12 for the second. Total possibilities is 12*12 = 144. This is correct Why am I right in the 2nd case but wrong in the first? We had a similar question on our midterm: We have 7 unique children and 20 identical cookies. How many ways can we distribute the cookies such that each child gets one? I thought if each child gets one, there are 13 left. Each of those 13 cookies can go to one of 7 children, so the answer should be 7^13. After learning the stars and stripes method, I realize the correct calculation leads to 19C7. I'm wondering WHY it is that my calculation is wrong, why there are two possible calculations, and what my calculation represents. It is this concept that I never understood that is still giving me trouble. I can do PMF and density functions, but I have trouble with this simplest concept",,"['probability', 'probability-distributions']"
90,Sum of average reciprocal of which random variable converges to a Cauchy distribution?,Sum of average reciprocal of which random variable converges to a Cauchy distribution?,,"If $(X_n)_{n\in\mathbb{N}}$ are independent identically distributed random variables with density $f$ even, continuous in $0$ and such that $f(0)>0$, then $$\frac{1}{n}\left(\frac{1}{X_1}+\dots + \frac{1}{X_n}\right)\xrightarrow{d}Z$$ With $Z$ a r.v. with Cauchy distribution.","If $(X_n)_{n\in\mathbb{N}}$ are independent identically distributed random variables with density $f$ even, continuous in $0$ and such that $f(0)>0$, then $$\frac{1}{n}\left(\frac{1}{X_1}+\dots + \frac{1}{X_n}\right)\xrightarrow{d}Z$$ With $Z$ a r.v. with Cauchy distribution.",,"['probability', 'probability-theory', 'random-variables']"
91,The distribution of the null space of a random Gaussian matrix,The distribution of the null space of a random Gaussian matrix,,"Each element of a `fat' matrix is i.i.d standard normal distribution,  is the distribution of the element in its null space still normal?   For example,  $A$ is a $2\times 3$ matrix, each element of $A$ is normal distribution.  Obviously, there exists a $3\times 1$ vector $x$ that satisfies $Ax=0$.  So what is the distribution of the element in $x$?","Each element of a `fat' matrix is i.i.d standard normal distribution,  is the distribution of the element in its null space still normal?   For example,  $A$ is a $2\times 3$ matrix, each element of $A$ is normal distribution.  Obviously, there exists a $3\times 1$ vector $x$ that satisfies $Ax=0$.  So what is the distribution of the element in $x$?",,"['probability', 'matrices']"
92,Conditional probability combining discrete and continuous variables,Conditional probability combining discrete and continuous variables,,"Let $A$ and $B$ be some continuous random variables. We proceed as follows: we take a coin with bias $b$ and flip it. If heads, we inspect $A$, if tails we inspect $B$. Call this resulting random variable $C$. Now say I can observe $C$ and want to figure out if the coin was heads or tails, i.e. I want to compute $\Pr[$head$|C = c]$ Everywhere I have looked, the definition is either for purely discrete or purely continuous random variables, I have not found a rigorous way of combining the two types of variables. One thought I had was to approximate the coin toss with a continuous random variable $K$ with pdf: $k(x) = b, \mbox{ for } x \in [-1, 0]   $ $k(x) = 1-b, \mbox{ for } x \in [0,1]$ Then one could compute the joint density function for $K$ and $C$ and compute $\Pr[K < 0 | C = c]$ from there. But this looks clunky and ugly. Is there a better way?","Let $A$ and $B$ be some continuous random variables. We proceed as follows: we take a coin with bias $b$ and flip it. If heads, we inspect $A$, if tails we inspect $B$. Call this resulting random variable $C$. Now say I can observe $C$ and want to figure out if the coin was heads or tails, i.e. I want to compute $\Pr[$head$|C = c]$ Everywhere I have looked, the definition is either for purely discrete or purely continuous random variables, I have not found a rigorous way of combining the two types of variables. One thought I had was to approximate the coin toss with a continuous random variable $K$ with pdf: $k(x) = b, \mbox{ for } x \in [-1, 0]   $ $k(x) = 1-b, \mbox{ for } x \in [0,1]$ Then one could compute the joint density function for $K$ and $C$ and compute $\Pr[K < 0 | C = c]$ from there. But this looks clunky and ugly. Is there a better way?",,"['probability', 'conditional-probability']"
93,Mana Maximization (Hearthstone),Mana Maximization (Hearthstone),,"I recently started playing Hearthstone and a statistic / probability question came up my mind. Here's a quick breakdown: The game is a turn-based card game which involves ""points"" that you can used called Mana. The amount of Mana each player gets start with 1 (in the 1st turn), 2 (in the 2nd turn) up to 10 (in the 10th turn), then 10 for any turn after that. Each card has a specific Mana costs (also range from 1 to 10) associates with it and each card deck has 30 cards total. Each player gets 3 card to start, with the person going 2nd (since it's a turn-based game) gaining an additional card and an additional freebie Mana that can only be used once. The players have special powers which costs 2 Mana regardless of the hero that they use, and the hero power can be used each turn. Assuming an average game length of 15 to 25 turns, my question is, what combination of cards in terms of Mana cost would maximize expected Mana spendage (i.e. always spend all of your mana at the end of each turn)? Also, the cards you draw are at random (in case that was not clear). I apologize if the explanation above is confusing or unclear; I've only started playing the game a few days ago. I will check regularly to see if I can clarify any of the game mechanics to anyone.","I recently started playing Hearthstone and a statistic / probability question came up my mind. Here's a quick breakdown: The game is a turn-based card game which involves ""points"" that you can used called Mana. The amount of Mana each player gets start with 1 (in the 1st turn), 2 (in the 2nd turn) up to 10 (in the 10th turn), then 10 for any turn after that. Each card has a specific Mana costs (also range from 1 to 10) associates with it and each card deck has 30 cards total. Each player gets 3 card to start, with the person going 2nd (since it's a turn-based game) gaining an additional card and an additional freebie Mana that can only be used once. The players have special powers which costs 2 Mana regardless of the hero that they use, and the hero power can be used each turn. Assuming an average game length of 15 to 25 turns, my question is, what combination of cards in terms of Mana cost would maximize expected Mana spendage (i.e. always spend all of your mana at the end of each turn)? Also, the cards you draw are at random (in case that was not clear). I apologize if the explanation above is confusing or unclear; I've only started playing the game a few days ago. I will check regularly to see if I can clarify any of the game mechanics to anyone.",,"['probability', 'statistics', 'optimization']"
94,Probability a pair of pairs of rows have the same vector sum,Probability a pair of pairs of rows have the same vector sum,,"Let $X$ be a random square $n$ by $n$ matrix with $X_{i,j} \in \{0,1\}$.  What is the probability that there is a distinct pair of pairs of rows which have the same vector sum? If you add two rows elementwise then each entry has $0$ with probability $1/4$, $2$ with probability $1/4$ and $1$ with probability $1/2$ and they are independent.   So if the two pairs are disjoint the probability they are the same is $(1/4^2 + 1/2^2+1/4^2)^n = (3/8)^n$.  If the two pairs have one row in common the probability is $1/2^n$. How can you use these facts to give the final probability? Clarification: If we let $r_i$ be row $i$ then I want $i,j,k,\ell$ such that $r_i+r_j = r_k+r_{\ell}$ with rule 1)  that we can't have both $i\in \{k,\ell\}$ and  $j \in \{k,\ell\}$ and rule 2) that both $i \ne j$ and  $k \ne \ell$ hold.","Let $X$ be a random square $n$ by $n$ matrix with $X_{i,j} \in \{0,1\}$.  What is the probability that there is a distinct pair of pairs of rows which have the same vector sum? If you add two rows elementwise then each entry has $0$ with probability $1/4$, $2$ with probability $1/4$ and $1$ with probability $1/2$ and they are independent.   So if the two pairs are disjoint the probability they are the same is $(1/4^2 + 1/2^2+1/4^2)^n = (3/8)^n$.  If the two pairs have one row in common the probability is $1/2^n$. How can you use these facts to give the final probability? Clarification: If we let $r_i$ be row $i$ then I want $i,j,k,\ell$ such that $r_i+r_j = r_k+r_{\ell}$ with rule 1)  that we can't have both $i\in \{k,\ell\}$ and  $j \in \{k,\ell\}$ and rule 2) that both $i \ne j$ and  $k \ne \ell$ hold.",,"['probability', 'combinatorics']"
95,What is the probability that the square $U$ is inside the square $S$,What is the probability that the square  is inside the square,U S,Given a square $S$ with size $1 \times 1$ . Two randomly selected points $A$ and $B$ are inside the square. Let $U$ be a square with diagonal $AB$ . How to find out the probability $P$ ( $U$ is inside $S$ ).,Given a square with size . Two randomly selected points and are inside the square. Let be a square with diagonal . How to find out the probability ( is inside ).,S 1 \times 1 A B U AB P U S,['probability']
96,Sum of N i.i.d. random variables,Sum of N i.i.d. random variables,,"Suppose I have a random variable $X_i$ with pdf $$X_i = \begin{cases}1 & P(X_i=1)=p\\-1 & P(X_i=-1)=q\\0 & P(X_i=0)=1-p-q\end{cases}$$ What is the pdf of sum of $N$ such i.i.d. random variables, i.e. $$X = X_1+X_2+\dots+X_N$$","Suppose I have a random variable $X_i$ with pdf $$X_i = \begin{cases}1 & P(X_i=1)=p\\-1 & P(X_i=-1)=q\\0 & P(X_i=0)=1-p-q\end{cases}$$ What is the pdf of sum of $N$ such i.i.d. random variables, i.e. $$X = X_1+X_2+\dots+X_N$$",,['probability']
97,Colored balls puzzle,Colored balls puzzle,,Imagine you have $n$ balls in a bag that are colored from $1$ to $n$.  At each turn you take two balls at random out that have different colors and color one the color of the other. You then put them both back in the bag.  What is the expected number of turns before all the balls have the same color?,Imagine you have $n$ balls in a bag that are colored from $1$ to $n$.  At each turn you take two balls at random out that have different colors and color one the color of the other. You then put them both back in the bag.  What is the expected number of turns before all the balls have the same color?,,"['probability', 'combinatorics', 'puzzle']"
98,Conditional Probability Given N=n,Conditional Probability Given N=n,,"suppose that $N$ is a Poisson$(μ)$ random variable. Given $N=n$, random variables $X_1,X_2,X_3,\cdots,X_n$ are independent with uniform∼$(0,1)$ distribution. So there are a random number of $X$'s. (a) Given $N=n$ what is the probability that all the $X$'s are less than $t$? So I set up the problem as: $P(X<t\mid N=n)=\frac{P(X<t,N=n)}{P(N=n)}$ How do I compute this if it's correct, or what do I do next? (b) What is the (unconditional) probability that all the $X$'s are less than $t$? No idea how to start this one.","suppose that $N$ is a Poisson$(μ)$ random variable. Given $N=n$, random variables $X_1,X_2,X_3,\cdots,X_n$ are independent with uniform∼$(0,1)$ distribution. So there are a random number of $X$'s. (a) Given $N=n$ what is the probability that all the $X$'s are less than $t$? So I set up the problem as: $P(X<t\mid N=n)=\frac{P(X<t,N=n)}{P(N=n)}$ How do I compute this if it's correct, or what do I do next? (b) What is the (unconditional) probability that all the $X$'s are less than $t$? No idea how to start this one.",,"['probability', 'probability-theory', 'probability-distributions', 'conditional-probability']"
99,Measure on a separable Hilbert space,Measure on a separable Hilbert space,,"Let $H$ be a real separable Hilbert space. Is it true that there exist a probability space $(\Omega, \mu)$ and a measurable function $\pi\colon \Omega \to H$ such that for any $h \in H$ we have $$ e^{-\lVert h\rVert^2}=\int_\Omega e^{i\langle h,\pi(\omega)\rangle}d\mu(\omega) \ \ ? $$","Let $H$ be a real separable Hilbert space. Is it true that there exist a probability space $(\Omega, \mu)$ and a measurable function $\pi\colon \Omega \to H$ such that for any $h \in H$ we have $$ e^{-\lVert h\rVert^2}=\int_\Omega e^{i\langle h,\pi(\omega)\rangle}d\mu(\omega) \ \ ? $$",,"['probability', 'measure-theory', 'reference-request', 'probability-theory', 'hilbert-spaces']"
