,title_raw,title_text,title_latex,body_raw,body_text,body_latex,tags
0,Inital value problem,Inital value problem,,"Let A be a constant matrix. Suppose u(t) solves the inital value   problem $\dot u = Au$, $u(0) = b$. Prove that the solution to the   inital value problem $\dot u = Au$, $u(t_0) = b$ is equal to $\hat u =  u(t -t_0)$. How are the solution trajectories related? I am not good with proofs. Never have I done it before prior of taking this differential equation class. Can anyone show me how to prove this?","Let A be a constant matrix. Suppose u(t) solves the inital value   problem $\dot u = Au$, $u(0) = b$. Prove that the solution to the   inital value problem $\dot u = Au$, $u(t_0) = b$ is equal to $\hat u =  u(t -t_0)$. How are the solution trajectories related? I am not good with proofs. Never have I done it before prior of taking this differential equation class. Can anyone show me how to prove this?",,['ordinary-differential-equations']
1,Is this constant of integration necessary at this step?,Is this constant of integration necessary at this step?,,"I came across a differential equation: $$\frac{dy}{dx}=\frac{\sin(\log x)}{\log y}$$. Here is what I tried to do: I transformed it into this form $$\log y dy=\sin(\log x)dx$$ i.e. $$\int \log y dy=\int \sin(\log x)dx\dots(2)$$ and after that I used integration by parts to finish off the problem. However,I was told by my teacher that it should instead be $$\int \log y dy=\int \sin(\log x)dx+C$$ where $C$ is a constant of a integration.I argued that the integration had not yet been carried out and so there was no need for the constant.(and I was told it $had$ to be there.) Can anyone please convince me why my teacher is right and I wrong? Thanks.","I came across a differential equation: $$\frac{dy}{dx}=\frac{\sin(\log x)}{\log y}$$. Here is what I tried to do: I transformed it into this form $$\log y dy=\sin(\log x)dx$$ i.e. $$\int \log y dy=\int \sin(\log x)dx\dots(2)$$ and after that I used integration by parts to finish off the problem. However,I was told by my teacher that it should instead be $$\int \log y dy=\int \sin(\log x)dx+C$$ where $C$ is a constant of a integration.I argued that the integration had not yet been carried out and so there was no need for the constant.(and I was told it $had$ to be there.) Can anyone please convince me why my teacher is right and I wrong? Thanks.",,['integration']
2,How to solve the differential equation $u_k(z)=-2\cfrac{\partial}{\partial z }(\cfrac{u_{k+1}(z)}{z})$?,How to solve the differential equation ?,u_k(z)=-2\cfrac{\partial}{\partial z }(\cfrac{u_{k+1}(z)}{z}),"$$e^{z\sqrt{1-t}}=\sum \limits_{k=0}^\infty \frac{u_k(z)t^k}{k!}$$ $$\frac{\partial}{\partial t }(e^{z\sqrt{1-t}})=\frac{\partial}{\partial t }(\sum \limits_{k=0}^\infty \frac{u_k(z)t^k}{k!})$$ $$\frac{-z}{2\sqrt{1-t} }e^{z\sqrt{1-t}}=\sum \limits_{k=1}^\infty \frac{u_k(z)t^{k-1}}{{(k-1)}!}=\sum \limits_{k=0}^\infty \frac{u_{k+1}(z)t^{k}}{{k}!}$$ $$\frac{-1}{2\sqrt{1-t} }e^{z\sqrt{1-t}}=\sum \limits_{k=0}^\infty \frac{u_{k+1}(z)}{z}\frac{t^{k}}{{k}!}$$ $$\frac{\partial}{\partial z }(\frac{-1}{2\sqrt{1-t} }e^{z\sqrt{1-t}})=\frac{\partial}{\partial z }(\sum \limits_{k=0}^\infty \frac{u_{k+1}(z)}{z}\frac{t^{k}}{{k}!})$$ $$\frac{-e^{z\sqrt{1-t}}}{2}=\sum \limits_{k=0}^\infty \frac{\partial}{\partial z }(\frac{u_{k+1}(z)}{z})\frac{t^{k}}{{k}!}$$ $$\sum \limits_{k=0}^\infty \frac{u_k(z)t^k}{k!}=-2\sum \limits_{k=0}^\infty \frac{\partial}{\partial z }(\frac{u_{k+1}(z)}{z})\frac{t^{k}}{{k}!}$$ $$u_k(z)=-2\frac{\partial}{\partial z }(\frac{u_{k+1}(z)}{z})$$ for $t=0$,$u_0(z)=e^z$ How to find the general formula of $u_k(z)$ ? I would like to learn the methods to solve such differential equations. Thanks a lot for answers. EDIT: We can find $u_1(z)$ as shown below $$\frac{\partial}{\partial t }(e^{z\sqrt{1-t}})=\frac{-z}{2\sqrt{1-t} }e^{z\sqrt{1-t}}=\sum \limits_{k=1}^\infty \frac{u_k(z)t^{k-1}}{{(k-1)}!}$$ for $t=0$, $u_1(z)=\cfrac{-ze^{z}}{2} $ If we continue to derivate in such way, we can find all $u_k(z)$ but it seems long method. I am looking for easier method.","$$e^{z\sqrt{1-t}}=\sum \limits_{k=0}^\infty \frac{u_k(z)t^k}{k!}$$ $$\frac{\partial}{\partial t }(e^{z\sqrt{1-t}})=\frac{\partial}{\partial t }(\sum \limits_{k=0}^\infty \frac{u_k(z)t^k}{k!})$$ $$\frac{-z}{2\sqrt{1-t} }e^{z\sqrt{1-t}}=\sum \limits_{k=1}^\infty \frac{u_k(z)t^{k-1}}{{(k-1)}!}=\sum \limits_{k=0}^\infty \frac{u_{k+1}(z)t^{k}}{{k}!}$$ $$\frac{-1}{2\sqrt{1-t} }e^{z\sqrt{1-t}}=\sum \limits_{k=0}^\infty \frac{u_{k+1}(z)}{z}\frac{t^{k}}{{k}!}$$ $$\frac{\partial}{\partial z }(\frac{-1}{2\sqrt{1-t} }e^{z\sqrt{1-t}})=\frac{\partial}{\partial z }(\sum \limits_{k=0}^\infty \frac{u_{k+1}(z)}{z}\frac{t^{k}}{{k}!})$$ $$\frac{-e^{z\sqrt{1-t}}}{2}=\sum \limits_{k=0}^\infty \frac{\partial}{\partial z }(\frac{u_{k+1}(z)}{z})\frac{t^{k}}{{k}!}$$ $$\sum \limits_{k=0}^\infty \frac{u_k(z)t^k}{k!}=-2\sum \limits_{k=0}^\infty \frac{\partial}{\partial z }(\frac{u_{k+1}(z)}{z})\frac{t^{k}}{{k}!}$$ $$u_k(z)=-2\frac{\partial}{\partial z }(\frac{u_{k+1}(z)}{z})$$ for $t=0$,$u_0(z)=e^z$ How to find the general formula of $u_k(z)$ ? I would like to learn the methods to solve such differential equations. Thanks a lot for answers. EDIT: We can find $u_1(z)$ as shown below $$\frac{\partial}{\partial t }(e^{z\sqrt{1-t}})=\frac{-z}{2\sqrt{1-t} }e^{z\sqrt{1-t}}=\sum \limits_{k=1}^\infty \frac{u_k(z)t^{k-1}}{{(k-1)}!}$$ for $t=0$, $u_1(z)=\cfrac{-ze^{z}}{2} $ If we continue to derivate in such way, we can find all $u_k(z)$ but it seems long method. I am looking for easier method.",,"['ordinary-differential-equations', 'recurrence-relations', 'generating-functions']"
3,Solving differential Equations by substitution,Solving differential Equations by substitution,,"An example in my Differential Equations textbook shows how to solve the homogenous differential equation $$ (x^2+y^2)\,dx +(x^2-xy)\,dy=0 $$ by substituting $y$ with $ux$, which I am trying to understand. The book explains that the reason we do this is so that $dy$ will equal $u\,dx + x\,du$. The answer says that after substitution, the equation becomes $$(x-ux)\,dx + x(u\,dx + x\,du) = 0 $$  and then $$ dx + x\,du = 0$$ My question is, how did it get to $dx+x\,du =0$? Is it a typo or am I missing something? I think it should be $ x\,dx + x\,du$ and then $dx + du$ and then $x + u$ and eventually $x + y/x$. However, the textbook says the answer is $x\ln(x)+y=cx$. What am I missing?","An example in my Differential Equations textbook shows how to solve the homogenous differential equation $$ (x^2+y^2)\,dx +(x^2-xy)\,dy=0 $$ by substituting $y$ with $ux$, which I am trying to understand. The book explains that the reason we do this is so that $dy$ will equal $u\,dx + x\,du$. The answer says that after substitution, the equation becomes $$(x-ux)\,dx + x(u\,dx + x\,du) = 0 $$  and then $$ dx + x\,du = 0$$ My question is, how did it get to $dx+x\,du =0$? Is it a typo or am I missing something? I think it should be $ x\,dx + x\,du$ and then $dx + du$ and then $x + u$ and eventually $x + y/x$. However, the textbook says the answer is $x\ln(x)+y=cx$. What am I missing?",,['ordinary-differential-equations']
4,Numerical solution of fractional integro-diffrential equ. using collocation method?,Numerical solution of fractional integro-diffrential equ. using collocation method?,,"problem comes from  ""Numerical solution of fractional integro-differential , equations by collocation method , E.A. Rawashdeh, Department of Mathematics, Yarmouk University, Irbid 21110, Jordan"" $D^qy(t)=p(t)y(t)+f(t)+\int_{0}^{1}{K(t,s)y(s)\,ds} , t\in I=[0,1]$ I want to create a maple code to check if the results in given article is valid or not but I do not have any idea about collocation method! any reference to collocation method solution are welcome!","problem comes from  ""Numerical solution of fractional integro-differential , equations by collocation method , E.A. Rawashdeh, Department of Mathematics, Yarmouk University, Irbid 21110, Jordan"" $D^qy(t)=p(t)y(t)+f(t)+\int_{0}^{1}{K(t,s)y(s)\,ds} , t\in I=[0,1]$ I want to create a maple code to check if the results in given article is valid or not but I do not have any idea about collocation method! any reference to collocation method solution are welcome!",,"['ordinary-differential-equations', 'numerical-methods', 'integral-equations', 'maple', 'integro-differential-equations']"
5,About the Legendre differential equation,About the Legendre differential equation,,Consider the Legendre differential equation $$ (1-x^2) y'' - 2xy' + n(n+1)y = 0 $$ Then its solution is given by $$ y = c_1 P_n (x) + \text{an infinite series} $$ In fact $y = c_1 P_n (x) + c_2 Q_n (x) $ where $P_n$ is Legendre polynomials and $Q_n $ is Legendre function of the second kind. Here I want to prove that 'an infinite series' above can be written by $c_2 Q_n (x)$ for some constant $c_2$.,Consider the Legendre differential equation $$ (1-x^2) y'' - 2xy' + n(n+1)y = 0 $$ Then its solution is given by $$ y = c_1 P_n (x) + \text{an infinite series} $$ In fact $y = c_1 P_n (x) + c_2 Q_n (x) $ where $P_n$ is Legendre polynomials and $Q_n $ is Legendre function of the second kind. Here I want to prove that 'an infinite series' above can be written by $c_2 Q_n (x)$ for some constant $c_2$.,,"['sequences-and-series', 'ordinary-differential-equations', 'special-functions']"
6,How can I bound this equation?,How can I bound this equation?,,"This is a problem I stuck in Berkeley Problems in Mathematics, fall 1983: Let $x(t)=(x_{1}(t)...x_{n}(t))$ be a differentiable function from $\mathbb{R}$ to $\mathbb{R}^{n}$. It satisfies a differential equation of the form $$x'(t)=f(x(t))$$ where $f:\mathbb{R}^{n}\rightarrow \mathbb{R}$ is a continuous function. Assuming that $f$ satisfies the condition $$\langle f(y),y\rangle\le |y|^{2}$$ derive an inequality showing that $|x(t)|$ grows at most exponentially. I am thinking about decompoising $f(y)$ into $y^{T}$ and $y$ directions, but this does not allow me to integrate $x'(t)$ from $t_{0}$ to $t$. If I can show $|f(y)|\le K|y|$ then the inequality would be immediate; but this is false as $f(y)$ can be orthogonal to $y$ at every point $y=x(t)$ and have arbitrarily large norm. So I do not know how to solve this in an elegant way. I can solve it in the most radical case that $x'(t)\cdot x(t)=0\forall t$ and $x'(t)=K(t)x(t)$, but I do not know how to deal with arbitrarily curves.","This is a problem I stuck in Berkeley Problems in Mathematics, fall 1983: Let $x(t)=(x_{1}(t)...x_{n}(t))$ be a differentiable function from $\mathbb{R}$ to $\mathbb{R}^{n}$. It satisfies a differential equation of the form $$x'(t)=f(x(t))$$ where $f:\mathbb{R}^{n}\rightarrow \mathbb{R}$ is a continuous function. Assuming that $f$ satisfies the condition $$\langle f(y),y\rangle\le |y|^{2}$$ derive an inequality showing that $|x(t)|$ grows at most exponentially. I am thinking about decompoising $f(y)$ into $y^{T}$ and $y$ directions, but this does not allow me to integrate $x'(t)$ from $t_{0}$ to $t$. If I can show $|f(y)|\le K|y|$ then the inequality would be immediate; but this is false as $f(y)$ can be orthogonal to $y$ at every point $y=x(t)$ and have arbitrarily large norm. So I do not know how to solve this in an elegant way. I can solve it in the most radical case that $x'(t)\cdot x(t)=0\forall t$ and $x'(t)=K(t)x(t)$, but I do not know how to deal with arbitrarily curves.",,['ordinary-differential-equations']
7,Differential Inequality Help,Differential Inequality Help,,"I have the inequality $f''(x)x + f'(x) \leq 0$ Also, $f''(x)<0$ and $f'(x)>0$ and $x \in R^+$. And I need to figure out when it is true. I know it is a fairly general question, but I couldn't find any information in several textbooks I have skimmed. Also, I am not sure if integrating would require a sign reversal or not, so I cant go ahead and try to manipulate it my self. Any help or mention of a helpful source would be much appreciated. edit: forgot to mention $f(x)\geq 0$ for every $x \in R^+$","I have the inequality $f''(x)x + f'(x) \leq 0$ Also, $f''(x)<0$ and $f'(x)>0$ and $x \in R^+$. And I need to figure out when it is true. I know it is a fairly general question, but I couldn't find any information in several textbooks I have skimmed. Also, I am not sure if integrating would require a sign reversal or not, so I cant go ahead and try to manipulate it my self. Any help or mention of a helpful source would be much appreciated. edit: forgot to mention $f(x)\geq 0$ for every $x \in R^+$",,"['ordinary-differential-equations', 'inequality']"
8,"Deciding on what solution, $x''=\frac{1}{x}$","Deciding on what solution,",x''=\frac{1}{x},"I am trying to solve the following: Let $x(t)$ be a solution of $x''=\frac{1}{x}$ satisfying   $x(0)=1,x'(0)=2$. Let $t_0$ be the time when $x(t_0)=3$, find   $x'(t_0)$. I got that $x'(t_0)=\pm\sqrt{4+2ln(3)}$. I believe I should take the $+$ sign solution, how can I argue that ? Help is appriciated!","I am trying to solve the following: Let $x(t)$ be a solution of $x''=\frac{1}{x}$ satisfying   $x(0)=1,x'(0)=2$. Let $t_0$ be the time when $x(t_0)=3$, find   $x'(t_0)$. I got that $x'(t_0)=\pm\sqrt{4+2ln(3)}$. I believe I should take the $+$ sign solution, how can I argue that ? Help is appriciated!",,['ordinary-differential-equations']
9,how to solve differential equation $y^4 = k^2 (y^2 + y'^2\csc^2\alpha)$?,how to solve differential equation ?,y^4 = k^2 (y^2 + y'^2\csc^2\alpha),"What's the solution of the differential equation $y^4 = k^2 (y^2 + y'^2\csc^2\alpha)$, where $y$ is a function of $x$ and $\alpha$ is a constant? Polynomial solutions don't seem to work, because the LHS will always have higher degree than the RHS. Solutions of the form $A\cos(x\sin\alpha)+b\sin(x\sin\alpha)$ don't work either, but maybe something similar does? This comes from the 1st integral of the Euler-Lagrange equation for the functional $\int{y^2 + y'^2\csc^2\alpha)^{1/2}}dx$, which is the arc-length of a curve $r(\theta)$ on a cone with interior angle $2\alpha$, where $y=r$ and $x=\theta$. Perhaps there's a more useful way of using the Euler-Lagrange equation, giving an ODE whose solution is obvious?","What's the solution of the differential equation $y^4 = k^2 (y^2 + y'^2\csc^2\alpha)$, where $y$ is a function of $x$ and $\alpha$ is a constant? Polynomial solutions don't seem to work, because the LHS will always have higher degree than the RHS. Solutions of the form $A\cos(x\sin\alpha)+b\sin(x\sin\alpha)$ don't work either, but maybe something similar does? This comes from the 1st integral of the Euler-Lagrange equation for the functional $\int{y^2 + y'^2\csc^2\alpha)^{1/2}}dx$, which is the arc-length of a curve $r(\theta)$ on a cone with interior angle $2\alpha$, where $y=r$ and $x=\theta$. Perhaps there's a more useful way of using the Euler-Lagrange equation, giving an ODE whose solution is obvious?",,"['ordinary-differential-equations', 'calculus-of-variations', 'geodesic']"
10,"Maximizing $f(x,y)$",Maximizing,"f(x,y)","Could somebody please shed some light on this problem? Let $x,y \in \mathbb R$, we wish to maximize $f(x,y)=\frac{x^2-y^2}{(x^2+y^2)^2}$ by finding suitable values of $x,y$. Setting $\partial f\over \partial x$ and $\partial f \over \partial y$ as $0$ gives $x(x^2-3y^2)=0=y(y^2-3x^2)$ but these give $x=y=0$ which is not acceptable! Any ideas? Thank you.","Could somebody please shed some light on this problem? Let $x,y \in \mathbb R$, we wish to maximize $f(x,y)=\frac{x^2-y^2}{(x^2+y^2)^2}$ by finding suitable values of $x,y$. Setting $\partial f\over \partial x$ and $\partial f \over \partial y$ as $0$ gives $x(x^2-3y^2)=0=y(y^2-3x^2)$ but these give $x=y=0$ which is not acceptable! Any ideas? Thank you.",,"['ordinary-differential-equations', 'optimization']"
11,Does the asymptotic stability of a differential equation imply uniform boundedness?,Does the asymptotic stability of a differential equation imply uniform boundedness?,,"Consider the system of differential equations $$\dot{x}(t) = f(t,x)$$ where $x \in \mathbb{R}^n$ and $f$ is a $C^{\infty}$ function. Suppose that every trajectory which begins in the closed unit ball tends to zero as $t \rightarrow \infty$. Does it follow that there exists a ball around the origin $\mathcal{B}$ such that every trajectory that begins in the unit ball stays in $\mathcal{B}$? My natural inclination is to think the answer ought to be yes and one should be able to show this by picking a convergent subsequence somehow. I'm having trouble making this work, however. Here is where I am stuck. Supposing $x_i(t)$ is a trajectory that begins from $x_i(0)$ at $t=0$ and has distance at least $i$ away from the origin at some later time; and supposing $\lim_{i \rightarrow \infty} x_i(0) = x$; then I can't see how to derive a contradiction between the fact that the trajectory beginning from $x$ approaches zero, and is consequently bounded, and the the fact that $||x_i(t_i)||_2 \geq i$ for some $t_i$. If each $t_i$ was below some $T$ a contradiction is easy to obtain, but how to deal with $t_i$'s which blow up? Finally, this is not homework.","Consider the system of differential equations $$\dot{x}(t) = f(t,x)$$ where $x \in \mathbb{R}^n$ and $f$ is a $C^{\infty}$ function. Suppose that every trajectory which begins in the closed unit ball tends to zero as $t \rightarrow \infty$. Does it follow that there exists a ball around the origin $\mathcal{B}$ such that every trajectory that begins in the unit ball stays in $\mathcal{B}$? My natural inclination is to think the answer ought to be yes and one should be able to show this by picking a convergent subsequence somehow. I'm having trouble making this work, however. Here is where I am stuck. Supposing $x_i(t)$ is a trajectory that begins from $x_i(0)$ at $t=0$ and has distance at least $i$ away from the origin at some later time; and supposing $\lim_{i \rightarrow \infty} x_i(0) = x$; then I can't see how to derive a contradiction between the fact that the trajectory beginning from $x$ approaches zero, and is consequently bounded, and the the fact that $||x_i(t_i)||_2 \geq i$ for some $t_i$. If each $t_i$ was below some $T$ a contradiction is easy to obtain, but how to deal with $t_i$'s which blow up? Finally, this is not homework.",,['ordinary-differential-equations']
12,"Range of a linear transformation on $C[0,1]$",Range of a linear transformation on,"C[0,1]","Consider $T : C[0,1] \longrightarrow C[0,1]$ defined by $T(f(x)) = f^′(x)−f(x)$. I like this linear transformation because it's null space is functions of the form $ce^x$ for $c \in \mathbb{R}$ on the interval $[0,1]$. The range, on the other hand, I'm not quite as certain about. We are looking at all functions that can be written as a difference of a continuous function and it's derivative. Is this all of $C[0,1]$? How can I convince myself of this?","Consider $T : C[0,1] \longrightarrow C[0,1]$ defined by $T(f(x)) = f^′(x)−f(x)$. I like this linear transformation because it's null space is functions of the form $ce^x$ for $c \in \mathbb{R}$ on the interval $[0,1]$. The range, on the other hand, I'm not quite as certain about. We are looking at all functions that can be written as a difference of a continuous function and it's derivative. Is this all of $C[0,1]$? How can I convince myself of this?",,"['linear-algebra', 'ordinary-differential-equations']"
13,Solving a kind of differential equation,Solving a kind of differential equation,,"Is it possible to solve the following differential equation: $g: \Bbb{R} \to \Bbb{R}$, $$ g'(a)=a\cdot g(a-1),\ g(0)=\frac{1}{2}$$ I can't find any method for ordinary differential equations which works here.","Is it possible to solve the following differential equation: $g: \Bbb{R} \to \Bbb{R}$, $$ g'(a)=a\cdot g(a-1),\ g(0)=\frac{1}{2}$$ I can't find any method for ordinary differential equations which works here.",,['ordinary-differential-equations']
14,Why can you determine the stability of a system by taking the eigenvalues of the Jacobian?,Why can you determine the stability of a system by taking the eigenvalues of the Jacobian?,,Why can you determine the stability of a system by taking the eigenvalues of the Jacobian?  I know it's an elementary question but it's been a while. Thank you!,Why can you determine the stability of a system by taking the eigenvalues of the Jacobian?  I know it's an elementary question but it's been a while. Thank you!,,"['ordinary-differential-equations', 'dynamical-systems']"
15,Question Regarding Linear ODE-Trouble Using Integrating Factor,Question Regarding Linear ODE-Trouble Using Integrating Factor,,"Consider the equation $$\frac{dy}{dx} + 5y = e^{2x}$$ One method of attack as far as i know is to multiply both sides by $e^{5x}$.This gives $$e^{5x}\frac{dy}{dx} + y5e^{5x} = e^{2x}e^{5x} = e^{7x}$$ We now find that the LHS is,in fact,the derivative of $ye^{5x}$. $$\therefore \frac{d}{dx}(ye^{5x}) = e^{7x}$$ Now what do i do?Integrate this way$$\int\frac{d}{dx}(ye^{5x}) = \int e^{7x} ?$$","Consider the equation $$\frac{dy}{dx} + 5y = e^{2x}$$ One method of attack as far as i know is to multiply both sides by $e^{5x}$.This gives $$e^{5x}\frac{dy}{dx} + y5e^{5x} = e^{2x}e^{5x} = e^{7x}$$ We now find that the LHS is,in fact,the derivative of $ye^{5x}$. $$\therefore \frac{d}{dx}(ye^{5x}) = e^{7x}$$ Now what do i do?Integrate this way$$\int\frac{d}{dx}(ye^{5x}) = \int e^{7x} ?$$",,"['ordinary-differential-equations', 'integration']"
16,Does this Laplace transform exist?,Does this Laplace transform exist?,,"I had a final in differential equations with the first question being: ""1. Does the Laplace transform of $\displaystyle \frac{1}{(1+t)}$ exist? Why or why not?"" and number 2 was ""2. If number one was true, then what is this transform?"" At first I thought it was true because the definition of the Laplace says that it must be of exponential order (it is I believe) and must be piece wise continuous from $[0,\infty)$. That equation satisfies both of those properties, but It's not defined or has complex components I'm reading now? Can somebody set me straight on this problem.","I had a final in differential equations with the first question being: ""1. Does the Laplace transform of $\displaystyle \frac{1}{(1+t)}$ exist? Why or why not?"" and number 2 was ""2. If number one was true, then what is this transform?"" At first I thought it was true because the definition of the Laplace says that it must be of exponential order (it is I believe) and must be piece wise continuous from $[0,\infty)$. That equation satisfies both of those properties, but It's not defined or has complex components I'm reading now? Can somebody set me straight on this problem.",,"['ordinary-differential-equations', 'transformation', 'laplace-transform']"
17,An application of Gronwall's lemma,An application of Gronwall's lemma,,"I've come across a creative use of Gronwall's lemma which I would like to submit to the community. I suspect that the argument, while leading to a correct conclusion, is somewhat flawed. We have a continuous mapping $g \colon \mathbb{R}\to \mathbb{R}$ such that $$\tag{1} \forall \varepsilon>0\ \exists \delta(\varepsilon)>0\ \text{s.t.}\ \lvert x \rvert \le \delta(\varepsilon) \Rightarrow \lvert g(x) \rvert \le \varepsilon \lvert x \rvert$$ and a continuous trajectory $x\colon [0, +\infty) \to \mathbb{R}$ such that $$\tag{2} e^{\alpha t}\lvert x(t)\rvert \le \lvert x_0\rvert+\int_0^t e^{\alpha s}\lvert g(x(s))\rvert\, ds. $$ Here $x_0=x(0)$ is the initial datum, which we may choose small as we wish, but $\alpha >0$ is a fixed constant that we cannot alter in any way. Now comes the point. Fix $\varepsilon>0$. The lecturer says: Suppose   we can apply (1) for all times $t \ge 0$. Then inserting (1) in (2)   we get $$e^{\alpha t}\lvert x(t) \rvert \le \lvert x_0\rvert + \varepsilon \int_0^t e^{\alpha s} \lvert x(s)\rvert \, ds$$ and from Gronwall's lemma we infer $$\tag{3} \lvert x(t)\rvert \le  e^{(\varepsilon - \alpha)t}\lvert x_0\rvert.$$ So if $\varepsilon <\alpha$ and $\lvert x_0 \rvert < \delta(\varepsilon)$, $\lvert x(s) \rvert$ is small at all times and   our use of (1) is justified . We conclude that inequality (3) holds. Does this argument look correct to you? I believe that the conclusion is correct, but that it requires more careful treatment. Thank you.","I've come across a creative use of Gronwall's lemma which I would like to submit to the community. I suspect that the argument, while leading to a correct conclusion, is somewhat flawed. We have a continuous mapping $g \colon \mathbb{R}\to \mathbb{R}$ such that $$\tag{1} \forall \varepsilon>0\ \exists \delta(\varepsilon)>0\ \text{s.t.}\ \lvert x \rvert \le \delta(\varepsilon) \Rightarrow \lvert g(x) \rvert \le \varepsilon \lvert x \rvert$$ and a continuous trajectory $x\colon [0, +\infty) \to \mathbb{R}$ such that $$\tag{2} e^{\alpha t}\lvert x(t)\rvert \le \lvert x_0\rvert+\int_0^t e^{\alpha s}\lvert g(x(s))\rvert\, ds. $$ Here $x_0=x(0)$ is the initial datum, which we may choose small as we wish, but $\alpha >0$ is a fixed constant that we cannot alter in any way. Now comes the point. Fix $\varepsilon>0$. The lecturer says: Suppose   we can apply (1) for all times $t \ge 0$. Then inserting (1) in (2)   we get $$e^{\alpha t}\lvert x(t) \rvert \le \lvert x_0\rvert + \varepsilon \int_0^t e^{\alpha s} \lvert x(s)\rvert \, ds$$ and from Gronwall's lemma we infer $$\tag{3} \lvert x(t)\rvert \le  e^{(\varepsilon - \alpha)t}\lvert x_0\rvert.$$ So if $\varepsilon <\alpha$ and $\lvert x_0 \rvert < \delta(\varepsilon)$, $\lvert x(s) \rvert$ is small at all times and   our use of (1) is justified . We conclude that inequality (3) holds. Does this argument look correct to you? I believe that the conclusion is correct, but that it requires more careful treatment. Thank you.",,"['analysis', 'ordinary-differential-equations', 'inequality']"
18,Second derivative using implicit differentiation with respect to $x$ of $x = \sin y + \cos y$,Second derivative using implicit differentiation with respect to  of,x x = \sin y + \cos y,I am running into trouble with this question: I get as far as $$1 = \cos y\frac{dy}{dx} - \sin y\frac{dy}{dx}$$ $$1 = \frac{dy}{dx} (\cos y - \sin y)$$ $$\frac{dy}{dx} = \frac{1}{\cos y-\sin y}$$ Second derivative: Unsure how to continue here,I am running into trouble with this question: I get as far as $$1 = \cos y\frac{dy}{dx} - \sin y\frac{dy}{dx}$$ $$1 = \frac{dy}{dx} (\cos y - \sin y)$$ $$\frac{dy}{dx} = \frac{1}{\cos y-\sin y}$$ Second derivative: Unsure how to continue here,,"['ordinary-differential-equations', 'implicit-differentiation']"
19,Nonlinear differential equation type,Nonlinear differential equation type,,Which method should I use for solving equation $\sqrt{1-x^2}dy + \sqrt{1-y^2}dx = 0$ ?,Which method should I use for solving equation $\sqrt{1-x^2}dy + \sqrt{1-y^2}dx = 0$ ?,,['ordinary-differential-equations']
20,Problem solving a couple of ODEs,Problem solving a couple of ODEs,,"First I'm trying to make this equation exact  $$ \frac{\sin y}{x} dx + (\frac{y}{x} \cos y - \frac{\sin y}{y} ) dy = 0 $$ The problem says to use to use an integrating factor $ u(x,y)=h(\frac{x}{y}) $. To make integrating a little easier I first did a variable change using $v=\frac{x}{y} $, but that didn't help much when I had to derive it. In general I-ve been having trouble solving most problems that require an integrating factor that involes both variables, unless it's of the form $ u(x,y)=x^a y^b $ with a and b integers to be determined. I think there's some ""simple"" way to solve this I'm not seeing, or knew and can't remember. If anyone has any idea, please share. Now on to the second problem. I need to find a solution for $$ y'' -y' +e^2x y = 0$$ As a note it says to consider the variable change $ x = \ln t$. By doing this I get  $ y'' - y' +t y = 0 $, which I'm not too sure how to solve. I gather that something's missing since as that is right now $y'= \frac{dy}{dx} $, and I need it w.r.t.  $t$. So, $$y'= \frac{dy}{dx} = \frac{dy}{dt} \frac{dt}{dx} = y' \frac{1}{t} $$ $$ y'' = \frac{d^2 y}{dx^2} = \frac{dy'}{dx} = \frac{dy'}{dt} \frac{dt}{dx} = y'' \frac{1}{t^2} $$ Putting all this together back in the equation I get $$ \frac{y''}{t^2} - \frac{y'}{t} + ty = 0 $$ Now All I can really think about this is adding everything together so $$\frac{y'' - ty' +t^3 y}{t^2} = 0 $$ $$ y'' - t y' + t^3 y = 0 $$ Again, I'm probably not doing something right here, but if I am, I'm not sure what would be a suitable $y(t)$ to try, since it can't be $t^a$, $e^{at}$ and so on. Again, any ideas would be more than welcomed. EDIT: Regarding the second problem, with a little help from Gerry I got (Assuming $\frac{d^2y}{dx^2} = t^2 \frac{d^2y}{dt^2} $ which I think is right) $$ t^2 y'' - t y' + ty = 0 $$ $$ ty'' - y' + y = 0 $$ Either $ \frac{d^2y}{dx^2} = t \frac{d^2y}{dt^2}$ so that I don-t have any $t$ laying around, or there's something else that I'm not getting.","First I'm trying to make this equation exact  $$ \frac{\sin y}{x} dx + (\frac{y}{x} \cos y - \frac{\sin y}{y} ) dy = 0 $$ The problem says to use to use an integrating factor $ u(x,y)=h(\frac{x}{y}) $. To make integrating a little easier I first did a variable change using $v=\frac{x}{y} $, but that didn't help much when I had to derive it. In general I-ve been having trouble solving most problems that require an integrating factor that involes both variables, unless it's of the form $ u(x,y)=x^a y^b $ with a and b integers to be determined. I think there's some ""simple"" way to solve this I'm not seeing, or knew and can't remember. If anyone has any idea, please share. Now on to the second problem. I need to find a solution for $$ y'' -y' +e^2x y = 0$$ As a note it says to consider the variable change $ x = \ln t$. By doing this I get  $ y'' - y' +t y = 0 $, which I'm not too sure how to solve. I gather that something's missing since as that is right now $y'= \frac{dy}{dx} $, and I need it w.r.t.  $t$. So, $$y'= \frac{dy}{dx} = \frac{dy}{dt} \frac{dt}{dx} = y' \frac{1}{t} $$ $$ y'' = \frac{d^2 y}{dx^2} = \frac{dy'}{dx} = \frac{dy'}{dt} \frac{dt}{dx} = y'' \frac{1}{t^2} $$ Putting all this together back in the equation I get $$ \frac{y''}{t^2} - \frac{y'}{t} + ty = 0 $$ Now All I can really think about this is adding everything together so $$\frac{y'' - ty' +t^3 y}{t^2} = 0 $$ $$ y'' - t y' + t^3 y = 0 $$ Again, I'm probably not doing something right here, but if I am, I'm not sure what would be a suitable $y(t)$ to try, since it can't be $t^a$, $e^{at}$ and so on. Again, any ideas would be more than welcomed. EDIT: Regarding the second problem, with a little help from Gerry I got (Assuming $\frac{d^2y}{dx^2} = t^2 \frac{d^2y}{dt^2} $ which I think is right) $$ t^2 y'' - t y' + ty = 0 $$ $$ ty'' - y' + y = 0 $$ Either $ \frac{d^2y}{dx^2} = t \frac{d^2y}{dt^2}$ so that I don-t have any $t$ laying around, or there's something else that I'm not getting.",,['ordinary-differential-equations']
21,Stuck on Greens Function question,Stuck on Greens Function question,,"I am stuck on a possibly trivial question.  I have the Greens function for the equation $$p_0(x)y''+p_1(x)y'+p_2(x)y=0$$ with the boundary value $y(\alpha)=y(\beta)=0$ and I need to solve the equation $$p_0(x)y''+p_1(x)y'+p_2(x)y=r(x)$$ with the boundary conditions $y(\alpha)=0, y(\beta) = A \not =0$ I know that if the boundary conditions of the second was the same as the first, then I could do $$y(x)=\int_{\alpha}^{\beta} G(x,t) r(t) dt$$ But what does one do with the different boundary conditions?","I am stuck on a possibly trivial question.  I have the Greens function for the equation $$p_0(x)y''+p_1(x)y'+p_2(x)y=0$$ with the boundary value $y(\alpha)=y(\beta)=0$ and I need to solve the equation $$p_0(x)y''+p_1(x)y'+p_2(x)y=r(x)$$ with the boundary conditions $y(\alpha)=0, y(\beta) = A \not =0$ I know that if the boundary conditions of the second was the same as the first, then I could do $$y(x)=\int_{\alpha}^{\beta} G(x,t) r(t) dt$$ But what does one do with the different boundary conditions?",,['ordinary-differential-equations']
22,Determining the solutions of a differential equation which lie on a line,Determining the solutions of a differential equation which lie on a line,,"Given the differential equation: $$\dot{x} = y-x^2, \;\; \dot{y} = -x+y^2$$ I have to find the solutions of this differential equation which move/lie on a line. I am not quite sure how to handle this problem, I started by writing $y = mx + q$, so: $$\dot{y} = m \dot{x} = m (y-x^2) = -x+y^2$$ Solving this equation, I eventually arrived at $y = -x-1$. Now, a friend of mine told me this already is the solution, but I think it is only the line on which the solutions of the differential equation move. If so, how can I proceed in order to determine the solutions? Thanks for any answers in advance.","Given the differential equation: $$\dot{x} = y-x^2, \;\; \dot{y} = -x+y^2$$ I have to find the solutions of this differential equation which move/lie on a line. I am not quite sure how to handle this problem, I started by writing $y = mx + q$, so: $$\dot{y} = m \dot{x} = m (y-x^2) = -x+y^2$$ Solving this equation, I eventually arrived at $y = -x-1$. Now, a friend of mine told me this already is the solution, but I think it is only the line on which the solutions of the differential equation move. If so, how can I proceed in order to determine the solutions? Thanks for any answers in advance.",,['ordinary-differential-equations']
23,Find optimal control for cooling a cup of coffee,Find optimal control for cooling a cup of coffee,,"I have the following problem: Exercise 2. - A cup of coffee is initially at 100°C, and we want to lower its temperature to 0°C as quickly as possible by adding a fixed amount of milk. If $x(t)$ is the temperature of the coffee and milk mixture, the cooling law is given by $$ x'(t) = -x(t) - 25u(t) - \frac{1}{4}x(t)u(t),$$ where $u(t)$ is what is being added of milk, restricted to $0 \leq u(t) \leq 1$ and also $$ \int_0^T u(t)\,dt = 1, $$ a) Reason that the optimal control should be of the form $$ u(t) = \begin{cases}         0, & 0 < t < t_0, \\         1, & t_0 < t < T,     \end{cases}     $$ for a certain $t_0$ .  b) Taking into account the previous section, find the optimal control. Using a) it easy to compute $t_0$ and $T$ , for which I obtained $t_0=0.697$ and $T=1.697$ . But I don't know how to proceed with section a). I have the constraints $x(0)=100$ and $x(T)=0$ , the Hamiltonian of the system is $$H=u+p(-x-25u-xu/4),$$ so the equation for $p$ is on the form $$p'=-H_x=p(1+u/4),$$ but since $u$ also depends on $t$ I don't know how to  continue. Any help will be very appreciated. Thanks in advance! EDITED: I managed to prove the following : We define the Hamiltonian (H) for our system: $$ H = \lambda(t) \left(-x(t) - 25u(t) - \frac{1}{4}x(t)u(t)\right) + \mu u(t), $$ where $\lambda(t)$ is the constant (or adjoint cost) multiplier and $\mu$ is the multiplier associated with the constraint $\int_0^T u(t) \, dt = 1$ . The maximum principle states that the optimal control $u^*(t)$ maximizes the Hamiltonian $H$ at each time instant $t$ : $$ u^*(t) = \arg\max_{0 \leq u \leq 1} H. $$ We calculate the derivative of the Hamiltonian with respect to $u$ $$ \frac{\partial H}{\partial u} = \lambda(t) \left(-25 - \frac{1}{4}x(t)\right) + \mu. $$ The decision of whether $u(t) = 0$ or $u(t) = 1$ at a given moment depends on the sign of the derivative $$ \frac{\partial H}{\partial u} = \lambda(t) \left(-25 - \frac{1}{4}x(t)\right) + \mu $$ $$\frac{\partial H}{\partial u} > 0 \implies u^*(t) = 1$$ $$\frac{\partial H}{\partial u} < 0\implies u^*(t) = 0$$ I'm not really sure about the last two statements. However I don't know how to conclude the problem. Please help. Thanks in advance!","I have the following problem: Exercise 2. - A cup of coffee is initially at 100°C, and we want to lower its temperature to 0°C as quickly as possible by adding a fixed amount of milk. If is the temperature of the coffee and milk mixture, the cooling law is given by where is what is being added of milk, restricted to and also a) Reason that the optimal control should be of the form for a certain .  b) Taking into account the previous section, find the optimal control. Using a) it easy to compute and , for which I obtained and . But I don't know how to proceed with section a). I have the constraints and , the Hamiltonian of the system is so the equation for is on the form but since also depends on I don't know how to  continue. Any help will be very appreciated. Thanks in advance! EDITED: I managed to prove the following : We define the Hamiltonian (H) for our system: where is the constant (or adjoint cost) multiplier and is the multiplier associated with the constraint . The maximum principle states that the optimal control maximizes the Hamiltonian at each time instant : We calculate the derivative of the Hamiltonian with respect to The decision of whether or at a given moment depends on the sign of the derivative I'm not really sure about the last two statements. However I don't know how to conclude the problem. Please help. Thanks in advance!","x(t) 
x'(t) = -x(t) - 25u(t) - \frac{1}{4}x(t)u(t), u(t) 0 \leq u(t) \leq 1 
\int_0^T u(t)\,dt = 1,
 
u(t) = \begin{cases}
        0, & 0 < t < t_0, \\
        1, & t_0 < t < T,
    \end{cases}
     t_0 t_0 T t_0=0.697 T=1.697 x(0)=100 x(T)=0 H=u+p(-x-25u-xu/4), p p'=-H_x=p(1+u/4), u t  H = \lambda(t) \left(-x(t) - 25u(t) - \frac{1}{4}x(t)u(t)\right) + \mu u(t),  \lambda(t) \mu \int_0^T u(t) \, dt = 1 u^*(t) H t  u^*(t) = \arg\max_{0 \leq u \leq 1} H.  u  \frac{\partial H}{\partial u} = \lambda(t) \left(-25 - \frac{1}{4}x(t)\right) + \mu.  u(t) = 0 u(t) = 1  \frac{\partial H}{\partial u} = \lambda(t) \left(-25 - \frac{1}{4}x(t)\right) + \mu  \frac{\partial H}{\partial u} > 0 \implies u^*(t) = 1 \frac{\partial H}{\partial u} < 0\implies u^*(t) = 0","['ordinary-differential-equations', 'optimization', 'optimal-control']"
24,Determining values of a parameter for which an initial value problem has a polynomial solution,Determining values of a parameter for which an initial value problem has a polynomial solution,,"Given the following IVP: \begin{cases} \ddot{y}-3t\dot{y}-ky=0\\ y(0)=1; \; \dot{y}(0)=0 \end{cases} I want to determine the possible values of $k$ in order to obtain a polynomial solution; if $p(t)=a_0+a_1t+ \dots + t^n$ would be a solution; then $a_0=1; a_1=0$ ; so I'd get $p(t)=1+ a_2t^2+ \dots + t^n$ and I also have that: $$(-3n-k)t^n=0 \iff k=-3n$$ But now I just could find solutions for $k=-6n$ , i.e., for $\ddot{y}-3t\dot{y}+6ny=0$ with a polynomial of degree $2n$ as solution but I can't find a way to reason why shouldn't exist a polynomial solution for $k=-3-6n$ .  Any suggestions?","Given the following IVP: I want to determine the possible values of in order to obtain a polynomial solution; if would be a solution; then ; so I'd get and I also have that: But now I just could find solutions for , i.e., for with a polynomial of degree as solution but I can't find a way to reason why shouldn't exist a polynomial solution for .  Any suggestions?","\begin{cases}
\ddot{y}-3t\dot{y}-ky=0\\
y(0)=1; \; \dot{y}(0)=0
\end{cases} k p(t)=a_0+a_1t+ \dots + t^n a_0=1; a_1=0 p(t)=1+ a_2t^2+ \dots + t^n (-3n-k)t^n=0 \iff k=-3n k=-6n \ddot{y}-3t\dot{y}+6ny=0 2n k=-3-6n","['ordinary-differential-equations', 'polynomials']"
25,$f(x)=\sqrt{\frac{x-1}{x-3}}$ and $g(1)=e$ which of the following options is/are correct?,and  which of the following options is/are correct?,f(x)=\sqrt{\frac{x-1}{x-3}} g(1)=e,"Multiple Choice Question : In a question, a student was given to find the derivative of the product of two functions $'f'$ and $'g'$ . The student by mistake thought $(fg)'=f'\cdot g'$ for this question and co-incidentally got the correct answer. Given that $f(x)=\sqrt{\frac{x-1}{x-3}}$ & $g(1)=e$ which of the following options is/are correct? (a) $g(x)$ is discontinuous at one point. (b) $\lim_{x\to \infty} g(x)=1$ (c) $\lim_{x\to \alpha} \frac{x^3-8}{x-2}=12$ where $\alpha$ is number of point of discontinuity of $g(x)$ (d) $g(x)$ is discontinuous at two points. My Solution : With $f(x)=\sqrt{\frac{x-1}{x-3}}$ , we can see $f(x)$ is discontinuous at $x=3$ and $f'(x)=\frac{-1}{(x-3)\left(\sqrt{(x-1)(x-3)}\right)}$ so $f'(x)$ is discontinuous at $x=1,3$ Now according to question $f'(x)g'(x)=f(x)g'(x)+g(x)f'(x)$ $\implies g'(x)\left(f'(x)-f(x)\right)=g(x)f'(x)$ $\implies g'(x)\left(\frac{-1}{(x-3)\left(\sqrt{(x-1)(x-3)}\right)}-\sqrt{\frac{x-1}{x-3}}\right)=g(x)\left(\frac{-1}{(x-3)\left(\sqrt{(x-1)(x-3)}\right)}\right)$ $\implies g'(x)\left(x^2-4x+4\right)=g(x)$ After solving above differential equation and setting $g(1)=e$ , I am getting $g(x)=e^{\frac{1}{2-x}}$ which is discontinuous at $x=2$ My Doubt : (i) Since $f'(x)$ is discontinuous at $x=1,x=3$ so I think $g(x)$ should be discontinuous at $x=3$ too. because We are getting $g(x)$ after solving $f'(x)$ (ii) Is $f'(x)$ is discontinuous at $x=1,3$ correct or should I say it is discontinuous at $x=1$ because $x=3$ is not in domain of $f(x)$","Multiple Choice Question : In a question, a student was given to find the derivative of the product of two functions and . The student by mistake thought for this question and co-incidentally got the correct answer. Given that & which of the following options is/are correct? (a) is discontinuous at one point. (b) (c) where is number of point of discontinuity of (d) is discontinuous at two points. My Solution : With , we can see is discontinuous at and so is discontinuous at Now according to question After solving above differential equation and setting , I am getting which is discontinuous at My Doubt : (i) Since is discontinuous at so I think should be discontinuous at too. because We are getting after solving (ii) Is is discontinuous at correct or should I say it is discontinuous at because is not in domain of","'f' 'g' (fg)'=f'\cdot g' f(x)=\sqrt{\frac{x-1}{x-3}} g(1)=e g(x) \lim_{x\to \infty} g(x)=1 \lim_{x\to \alpha} \frac{x^3-8}{x-2}=12 \alpha g(x) g(x) f(x)=\sqrt{\frac{x-1}{x-3}} f(x) x=3 f'(x)=\frac{-1}{(x-3)\left(\sqrt{(x-1)(x-3)}\right)} f'(x) x=1,3 f'(x)g'(x)=f(x)g'(x)+g(x)f'(x) \implies g'(x)\left(f'(x)-f(x)\right)=g(x)f'(x) \implies g'(x)\left(\frac{-1}{(x-3)\left(\sqrt{(x-1)(x-3)}\right)}-\sqrt{\frac{x-1}{x-3}}\right)=g(x)\left(\frac{-1}{(x-3)\left(\sqrt{(x-1)(x-3)}\right)}\right) \implies g'(x)\left(x^2-4x+4\right)=g(x) g(1)=e g(x)=e^{\frac{1}{2-x}} x=2 f'(x) x=1,x=3 g(x) x=3 g(x) f'(x) f'(x) x=1,3 x=1 x=3 f(x)","['calculus', 'ordinary-differential-equations', 'derivatives', 'solution-verification', 'continuity']"
26,How does one do this translation?,How does one do this translation?,,"Good day! I am having a bit of trouble understanding the part where the paper said ""Then a simple translation in $x$ can absorb the linear term $\langle b,x \rangle$ into the quadratic form."" Am I correct in under standing that $f$ , after translation, would become something like $f(x) = (x-x_0)^TA(x-x_0)$ ? I tried doing this translation for $x \in \mathbb{R}^2$ where $x = [x_1 \hspace{4pt} x_2]^T$ , $b = [b_1 \hspace{4pt} b_2]^T$ , $A = \begin{bmatrix} a_1 & a_2 \\ a_3 & a_4 \end{bmatrix}$ . I expanded this and it turned out pretty gnarly in the middle and I cannot do completion of squares anymore. Is there an easier way to do this? Thank you!","Good day! I am having a bit of trouble understanding the part where the paper said ""Then a simple translation in can absorb the linear term into the quadratic form."" Am I correct in under standing that , after translation, would become something like ? I tried doing this translation for where , , . I expanded this and it turned out pretty gnarly in the middle and I cannot do completion of squares anymore. Is there an easier way to do this? Thank you!","x \langle b,x \rangle f f(x) = (x-x_0)^TA(x-x_0) x \in \mathbb{R}^2 x = [x_1 \hspace{4pt} x_2]^T b = [b_1 \hspace{4pt} b_2]^T A = \begin{bmatrix} a_1 & a_2 \\ a_3 & a_4 \end{bmatrix}","['linear-algebra', 'ordinary-differential-equations']"
27,Prove that two integrating factors define a solution.,Prove that two integrating factors define a solution.,,"I've been toiling away at this proof problem from Chapter 2.4 ending exercises of Differential Equations 3rd ed by Shepley L. Ross, but to no avail. Show that if $\mu (x, y)$ and $v(x, y)$ are integrating factors of $$ M(x, y)\ dx + N(x, y)\ dy = 0 $$ such that $\mu (x, y) / v(x, y)$ is not constant, then $$ \mu (x, y) = cv(x, y) $$ is a solution of the differential equation for every constant $c$ . My approach was that, iff $ \mu (x, y) / v(x, y) = c $ defines a solution for the differential equation then we must have $$ \frac{ \partial }{ \partial x } \left( \frac {u}{v} \right) = M(x, y) $$ $$ \frac{ \partial }{ \partial y } \left( \frac {u}{v} \right) = N(x, y) $$ and therefore must set out to prove it. On the outset, we have $$ \frac{ \partial }{ \partial y }(\mu M) = \frac{ \partial }{ \partial x }(\mu N) $$ $$ \frac{ \partial }{ \partial y }(v M) = \frac{ \partial }{ \partial y }(v M) $$ A bit of simplification later $$ \mu \left( \frac{ \partial M }{ \partial y } - \frac{ \partial N }{ \partial x } \right) = N(x, y) \frac{ \partial \mu }{ \partial x } - M(x, y) \frac{ \partial \mu }{ \partial y } $$ $$ v \left( \frac{ \partial M }{ \partial y } - \frac{ \partial N }{ \partial x } \right) = N(x, y) \frac{ \partial v }{ \partial x } - M(x, y) \frac{ \partial v }{ \partial y } $$ Doing the obvious and dividing, we have $$ \frac{u}{v} = \frac{ N(x, y) \displaystyle { \frac{ \partial \mu }{ \partial x } - M(x, y) \frac{ \partial \mu }{ \partial y } } }{ N(x, y) \displaystyle{ \frac{ \partial v }{ \partial x } - M(x, y) \frac{ \partial v }{ \partial y } } } $$ At this point, as I did not have any other leads, I decided to try and calculate the x and y partial derivatives. Starting with a deep breath, I set out onto a journey that lasted a good while but did not bring fruitful results. Enough terms did not cancel out and I am out of ideas, please HELP.","I've been toiling away at this proof problem from Chapter 2.4 ending exercises of Differential Equations 3rd ed by Shepley L. Ross, but to no avail. Show that if and are integrating factors of such that is not constant, then is a solution of the differential equation for every constant . My approach was that, iff defines a solution for the differential equation then we must have and therefore must set out to prove it. On the outset, we have A bit of simplification later Doing the obvious and dividing, we have At this point, as I did not have any other leads, I decided to try and calculate the x and y partial derivatives. Starting with a deep breath, I set out onto a journey that lasted a good while but did not bring fruitful results. Enough terms did not cancel out and I am out of ideas, please HELP.","\mu (x, y) v(x, y)  M(x, y)\ dx + N(x, y)\ dy = 0  \mu (x, y) / v(x, y)  \mu (x, y) = cv(x, y)  c  \mu (x, y) / v(x, y) = c   \frac{ \partial }{ \partial x } \left( \frac {u}{v} \right) = M(x, y)   \frac{ \partial }{ \partial y } \left( \frac {u}{v} \right) = N(x, y)   \frac{ \partial }{ \partial y }(\mu M) = \frac{ \partial }{ \partial x }(\mu N)   \frac{ \partial }{ \partial y }(v M) = \frac{ \partial }{ \partial y }(v M)   \mu \left( \frac{ \partial M }{ \partial y } - \frac{ \partial N }{ \partial x } \right) = N(x, y) \frac{ \partial \mu }{ \partial x } - M(x, y) \frac{ \partial \mu }{ \partial y }   v \left( \frac{ \partial M }{ \partial y } - \frac{ \partial N }{ \partial x } \right) = N(x, y) \frac{ \partial v }{ \partial x } - M(x, y) \frac{ \partial v }{ \partial y }   \frac{u}{v} = \frac{ N(x, y) \displaystyle { \frac{ \partial \mu }{ \partial x } - M(x, y) \frac{ \partial \mu }{ \partial y } } }{ N(x, y) \displaystyle{ \frac{ \partial v }{ \partial x } - M(x, y) \frac{ \partial v }{ \partial y } } } ","['ordinary-differential-equations', 'integrating-factor']"
28,On the solutions of $y''=yP(x)$ or $y'+y^2=P(x)$ [duplicate],On the solutions of  or  [duplicate],y''=yP(x) y'+y^2=P(x),"This question already has answers here : Finding an analytical solution to a particular class of second order ODEs (2 answers) Closed 2 months ago . Are there any general solutions to $y''=P(x)y$ ? A change of variables $y=e^u$ implies $y''=(u''+(u')^2)e^u $ and $u''+(u')^2=P(x)$ . Therefore, $y''=P(x)y$ reduces to $w'+w^2=P(x)$ , but I was not able to solve it (neither did WolframAlpha ). If it helps, $P(x)=\frac{Ax^2+B}{(x^2+a^2)^2}$ , but I am also interested in general solutions. Motivation These differential equation arise during finding a closed-form for the following series: $$ \sum_{n=1}^\infty \frac{(3n)!}{(2n)!n!}x^n \quad ,\quad \sum_{n=1}^\infty \frac{(3n)!}{n!n!n!}x^n \quad ,\quad \sum_{n=1}^\infty \frac{(6n)!}{n!(2n)!(3n)!}x^n \quad ,\quad \sum_{n=1}^\infty \frac{(4n)!}{n!n!(2n)!}x^n. $$ Alternatively, it would be equally interesting if any of the above summations have closed-forms.","This question already has answers here : Finding an analytical solution to a particular class of second order ODEs (2 answers) Closed 2 months ago . Are there any general solutions to ? A change of variables implies and . Therefore, reduces to , but I was not able to solve it (neither did WolframAlpha ). If it helps, , but I am also interested in general solutions. Motivation These differential equation arise during finding a closed-form for the following series: Alternatively, it would be equally interesting if any of the above summations have closed-forms.","y''=P(x)y y=e^u y''=(u''+(u')^2)e^u  u''+(u')^2=P(x) y''=P(x)y w'+w^2=P(x) P(x)=\frac{Ax^2+B}{(x^2+a^2)^2} 
\sum_{n=1}^\infty \frac{(3n)!}{(2n)!n!}x^n
\quad ,\quad
\sum_{n=1}^\infty \frac{(3n)!}{n!n!n!}x^n
\quad ,\quad
\sum_{n=1}^\infty \frac{(6n)!}{n!(2n)!(3n)!}x^n
\quad ,\quad
\sum_{n=1}^\infty \frac{(4n)!}{n!n!(2n)!}x^n.
","['calculus', 'ordinary-differential-equations']"
29,How does one solve this ODE using a differential operator and a series expansion?,How does one solve this ODE using a differential operator and a series expansion?,,"I am solving a fairly basic non-homogeneous ODE, but I wanted to try using the differential operator, and a series expansion in order to find the particular solution. I am fairly convinced it is possible to solve an ODE using this method, however I an unable to understand how. This is what I have done thus far: $$ \begin{align*} 2y' + y &= \cos x \\ \end{align*} $$ $$ \begin{align*} (2D+1)y &= \cos x \\ \end{align*} $$ $$ \begin{align*} y &= \frac{\cos x}{2D+1} \\ \end{align*} $$ $$ \begin{align*} y &= \sum_{n=0}^\infty (2D)^{2n} \cdot (1 - 2D) \cdot \cos x \\ \end{align*} $$ $$ \begin{align*} y &= \sum_{n=0}^\infty (2D)^{2n} \cdot (\cos x + 2\sin x) \end{align*} $$ How do I proceed from here? I cannot spot any mistake I have made in anything I have done leading up to it, but trying to proceed seems almost impossible. Have I completely misunderstood how derivative operators work? I tried to solve using maclaurin series as well: $$ \begin{align*} 2y' + y &= \cos x \\ \end{align*} $$ $$ \begin{align*} (2D+1)y &= \cos x \\ \end{align*} $$ $$ \begin{align*} y &= \frac{\cos x}{2D+1} \\ \end{align*} $$ $$ y= \sum_{n=0}^{\infty} \frac{f^{(n)}(0)}{n!} D^n \cdot(\cos x) $$ If the non-homogeneous function was a polynomial $x^n$ I could proceed by setting the upper bound as $n$ and evaluating (since subsequent terms have a differential operator of higher order than $n$ which will therefore annihilate the polynomial). But once again I do not know how to proceed from here for $\cos(x)$ since the derivative is cyclical. I am aware of the Exponential response formula , but I want to try solve without it.","I am solving a fairly basic non-homogeneous ODE, but I wanted to try using the differential operator, and a series expansion in order to find the particular solution. I am fairly convinced it is possible to solve an ODE using this method, however I an unable to understand how. This is what I have done thus far: How do I proceed from here? I cannot spot any mistake I have made in anything I have done leading up to it, but trying to proceed seems almost impossible. Have I completely misunderstood how derivative operators work? I tried to solve using maclaurin series as well: If the non-homogeneous function was a polynomial I could proceed by setting the upper bound as and evaluating (since subsequent terms have a differential operator of higher order than which will therefore annihilate the polynomial). But once again I do not know how to proceed from here for since the derivative is cyclical. I am aware of the Exponential response formula , but I want to try solve without it.","
\begin{align*}
2y' + y &= \cos x \\
\end{align*}
 
\begin{align*}
(2D+1)y &= \cos x \\
\end{align*}
 
\begin{align*}
y &= \frac{\cos x}{2D+1} \\
\end{align*}
 
\begin{align*}
y &= \sum_{n=0}^\infty (2D)^{2n} \cdot (1 - 2D) \cdot \cos x \\
\end{align*}
 
\begin{align*}
y &= \sum_{n=0}^\infty (2D)^{2n} \cdot (\cos x + 2\sin x)
\end{align*}
 
\begin{align*}
2y' + y &= \cos x \\
\end{align*}
 
\begin{align*}
(2D+1)y &= \cos x \\
\end{align*}
 
\begin{align*}
y &= \frac{\cos x}{2D+1} \\
\end{align*}
 
y= \sum_{n=0}^{\infty} \frac{f^{(n)}(0)}{n!} D^n \cdot(\cos x)
 x^n n n \cos(x)","['sequences-and-series', 'ordinary-differential-equations', 'derivatives']"
30,Solving this differential equation using method of characteristics,Solving this differential equation using method of characteristics,,"We have the following differential equation: $$u_xx-2u_xy+u_yy+9u_x+9u_y-9u=0$$ The goal is to solve with the method of characteristics, in other words to simplify, to give the canonical form. But I am not sure how to solve it. Here is my thoughts: Write down the characteristic equations. Solve the characteristic equations. Use the solutions to rewrite the original PDE in canonical form. For 1, here is my attempt $\frac{dx}{dt}=1,\quad\frac{dy}{dt}=-2,\quad\frac{du}{dt}=9$ , but I am not entirely sure and don't know how to proceed further.","We have the following differential equation: The goal is to solve with the method of characteristics, in other words to simplify, to give the canonical form. But I am not sure how to solve it. Here is my thoughts: Write down the characteristic equations. Solve the characteristic equations. Use the solutions to rewrite the original PDE in canonical form. For 1, here is my attempt , but I am not entirely sure and don't know how to proceed further.","u_xx-2u_xy+u_yy+9u_x+9u_y-9u=0 \frac{dx}{dt}=1,\quad\frac{dy}{dt}=-2,\quad\frac{du}{dt}=9","['ordinary-differential-equations', 'derivatives', 'partial-differential-equations', 'characteristics']"
31,Prove that a particle moves in a plane,Prove that a particle moves in a plane,,"Let's say $r \times v = const$ and $r \times a = 0$ . We should prove that the particle moves in a plane. I thought to approach it from a differential equation perspective, where we know from the second equation that $a=  r''  = kr$ , which means that $r(t) = c_1e^{kt}+c_2e^{-kt}$ . If $k$ would not be a scalar, we would get that $v$ is a scalar, so it seems wrong, and the only solution is of the form $r(t) = (t, c_1e^{kt}+c_2e^{-kt})$ . Is there a way to prove that with differential equations, or should I stick instead to a different method (e.g. like here )?","Let's say and . We should prove that the particle moves in a plane. I thought to approach it from a differential equation perspective, where we know from the second equation that , which means that . If would not be a scalar, we would get that is a scalar, so it seems wrong, and the only solution is of the form . Is there a way to prove that with differential equations, or should I stick instead to a different method (e.g. like here )?","r \times v = const r \times a = 0 a=  r''  = kr r(t) = c_1e^{kt}+c_2e^{-kt} k v r(t) = (t, c_1e^{kt}+c_2e^{-kt})","['linear-algebra', 'ordinary-differential-equations', 'solution-verification']"
32,Existence of center manifold,Existence of center manifold,,"I've been working on the following exercise: Prove that the system $$ \begin{cases} \dot{x} = -x^3,\\ \dot{y} =  -y + x^2 \end{cases} $$ has no analytic center manifold (supposed in the following way $y = h(x) = a_2x^2 + a_3x^3 + \cdots$ , then $a_{2n+1} = 0, n \geq 1, a_2 = 1, a_{n+2} = na_n, n \geq 2$ ). Is the manifold $C^{\infty}$ ? It's based on the example 2.5, page 315 of the book 'Methods in Bifurcation Theory' by Chow and Hale. How should I find the center manifold or even prove there is none? The book says it is not difficult to see but I am struggling. Thank you for your help!","I've been working on the following exercise: Prove that the system has no analytic center manifold (supposed in the following way , then ). Is the manifold ? It's based on the example 2.5, page 315 of the book 'Methods in Bifurcation Theory' by Chow and Hale. How should I find the center manifold or even prove there is none? The book says it is not difficult to see but I am struggling. Thank you for your help!","
\begin{cases}
\dot{x} = -x^3,\\
\dot{y} =  -y + x^2
\end{cases}
 y = h(x) = a_2x^2 + a_3x^3 + \cdots a_{2n+1} = 0, n \geq 1, a_2 = 1, a_{n+2} = na_n, n \geq 2 C^{\infty}","['ordinary-differential-equations', 'dynamical-systems', 'bifurcation']"
33,Showing that every plane curve with constant curvature is a circle by solving the Frenet differential equation,Showing that every plane curve with constant curvature is a circle by solving the Frenet differential equation,,"I recently saw the following example that shows that plane curves with constant curvature are circles: Let $\kappa \neq 0$ be constant and $s \in \mathbb{R}$ arbitrary, then by the Frenet Matrix-equation we get $E(s)=exp \biggl(s \begin{pmatrix} 0 & -\kappa \\  -\kappa & 0  \end{pmatrix}\biggr)=\begin{pmatrix} cos(\kappa s) & sin(\kappa s) \\  -sin(\kappa s) & cos(\kappa s)  \end{pmatrix} $ Now my question is regarding how to get to that result. I know that the Frenet Matrix Equation (for plane curves) is $\begin{pmatrix} e_{11}'(s) & e_{12}'(s) \\  e_{21}'(s) & e_{22}'(s)  \end{pmatrix}=\begin{pmatrix} 0 & -\kappa \\  -\kappa & 0  \end{pmatrix} \cdot \begin{pmatrix} e_{11}(s) & e_{12}(s) \\  e_{21}(s) & e_{22}(s) \end{pmatrix}$ Also by googling I now know that the matrix exponential for some $n \times n$ matrix $A$ as $e^{A}=\sum_{k=0}^{\infty}\frac{A^k}{k!}$ . I do know that for some function $f$ , the equation $f'(x)=c f(x)$ has the solution, $C_1 e^{cx}$ , but I don't understand the ""matrix version"". I am not very familiar with differential equations.","I recently saw the following example that shows that plane curves with constant curvature are circles: Let be constant and arbitrary, then by the Frenet Matrix-equation we get Now my question is regarding how to get to that result. I know that the Frenet Matrix Equation (for plane curves) is Also by googling I now know that the matrix exponential for some matrix as . I do know that for some function , the equation has the solution, , but I don't understand the ""matrix version"". I am not very familiar with differential equations.","\kappa \neq 0 s \in \mathbb{R} E(s)=exp \biggl(s \begin{pmatrix}
0 & -\kappa \\ 
-\kappa & 0 
\end{pmatrix}\biggr)=\begin{pmatrix}
cos(\kappa s) & sin(\kappa s) \\ 
-sin(\kappa s) & cos(\kappa s) 
\end{pmatrix}
 \begin{pmatrix}
e_{11}'(s) & e_{12}'(s) \\ 
e_{21}'(s) & e_{22}'(s) 
\end{pmatrix}=\begin{pmatrix}
0 & -\kappa \\ 
-\kappa & 0 
\end{pmatrix} \cdot \begin{pmatrix}
e_{11}(s) & e_{12}(s) \\ 
e_{21}(s) & e_{22}(s)
\end{pmatrix} n \times n A e^{A}=\sum_{k=0}^{\infty}\frac{A^k}{k!} f f'(x)=c f(x) C_1 e^{cx}","['ordinary-differential-equations', 'differential-geometry']"
34,Linear ODEs on random orthogonal matrices,Linear ODEs on random orthogonal matrices,,"I am working on a music synthesizer project. We are just kind of playing around trying to get an interesting way to vary N parameters with a nice balance of predictability and surprise.  I had an idea to choose a random orthogonal (complex) NxN matrix $\mathbf{A}$ , and then solve the linear ODE $y' = \mathbf{A}y$ (using Euler's method).  But something strange happens -- there are a few seconds of interesting semi-chaos, but then things quickly seem to start to settle into a very simple periodic pattern, with all parameters getting stuck in lock-step with each other, all with the same period and even most with the same phase. My question is, is this behavior some interesting property of ODEs on random matrices, or should this not be happening and my solver has some problem?  What is the typical behavior of $e^{\mathbf{A}t}$ for $\mathbf{A}$ random and orthogonal? Possibly important detail(?):  after each step of Euler's method, I normalize the vector (otherwise I start getting exponential blowup from rounding errors). $\mathbf{A}$ is orthogonal so in a perfect world this wouldn't do anything. But I'm intuitively worried this normalization step is causing some damping of the system that is suppressing its interesting dynamics.","I am working on a music synthesizer project. We are just kind of playing around trying to get an interesting way to vary N parameters with a nice balance of predictability and surprise.  I had an idea to choose a random orthogonal (complex) NxN matrix , and then solve the linear ODE (using Euler's method).  But something strange happens -- there are a few seconds of interesting semi-chaos, but then things quickly seem to start to settle into a very simple periodic pattern, with all parameters getting stuck in lock-step with each other, all with the same period and even most with the same phase. My question is, is this behavior some interesting property of ODEs on random matrices, or should this not be happening and my solver has some problem?  What is the typical behavior of for random and orthogonal? Possibly important detail(?):  after each step of Euler's method, I normalize the vector (otherwise I start getting exponential blowup from rounding errors). is orthogonal so in a perfect world this wouldn't do anything. But I'm intuitively worried this normalization step is causing some damping of the system that is suppressing its interesting dynamics.",\mathbf{A} y' = \mathbf{A}y e^{\mathbf{A}t} \mathbf{A} \mathbf{A},"['ordinary-differential-equations', 'random-matrices']"
35,Generalized singular differential equation,Generalized singular differential equation,,"I'm interested to understand what is the general solution for the differential equation $$x^{n}\frac{d^{m}y}{dx^{m}}\,=\,0$$ where $n,m\in\mathbb{N}$ . If $m=1$ , then the solution can be found in Kanwal's generalized function book and is given by: $$y\,=\,c_{1}\,+\,c_{2}\Theta(x)\,+\,c_{3}\delta(x)\,+\,c_{4}\delta^{\prime}(x)\,+\,\ldots\,+\,c_{n+1}\delta^{(n-2)}(x)$$ where $\delta^{(n-2)}(x)$ denotes a Dirac delta function differentiated $n-2$ times. How can I generalize this result?","I'm interested to understand what is the general solution for the differential equation where . If , then the solution can be found in Kanwal's generalized function book and is given by: where denotes a Dirac delta function differentiated times. How can I generalize this result?","x^{n}\frac{d^{m}y}{dx^{m}}\,=\,0 n,m\in\mathbb{N} m=1 y\,=\,c_{1}\,+\,c_{2}\Theta(x)\,+\,c_{3}\delta(x)\,+\,c_{4}\delta^{\prime}(x)\,+\,\ldots\,+\,c_{n+1}\delta^{(n-2)}(x) \delta^{(n-2)}(x) n-2","['ordinary-differential-equations', 'distribution-theory']"
36,Are vector fields topologically equivalent when their limitings sets coincide,Are vector fields topologically equivalent when their limitings sets coincide,,"Let $M$ be a compact manifold and we are given two VFs $F,F' \in \mathcal{X}(M)$ . We define a DS by $$\dot{x}=F(x)$$ and equivalently for $F'$ . The $\omega$ -limiting set is given by $$\omega(f,x)=\bigcap_{s \in \mathbb{R}} \overline{\{\varphi(x, t): t>s\}}$$ The two vector fields are topologically equivalent , denoted by $F \simeq F' \, $ if there exists a homeomorphism $h: U \rightarrow U$ , mapping orbits of the first system onto orbits of the second system, i.e. $$\forall t \in \mathbb{R}, \ \forall x \in U: \quad \phi^{F}_t(x)=h^{-1} \circ \phi^{F'}_{\tau} \circ h(x),$$ with $ \tau: U \times \mathbb{R} \rightarrow \mathbb{R}, \quad \frac{\partial \tau(x,t)}{\partial t} >0 \quad \forall x \in U $ . This means the time direction of the orbits is preserved. My question is the following: Given that alle limiting sets are equal, $\forall x \in U: \ \omega(F,x)=\omega(F',x)$ , can we conclude that $F$ and $F'$ are topologically equivalent?","Let be a compact manifold and we are given two VFs . We define a DS by and equivalently for . The -limiting set is given by The two vector fields are topologically equivalent , denoted by if there exists a homeomorphism , mapping orbits of the first system onto orbits of the second system, i.e. with . This means the time direction of the orbits is preserved. My question is the following: Given that alle limiting sets are equal, , can we conclude that and are topologically equivalent?","M F,F' \in \mathcal{X}(M) \dot{x}=F(x) F' \omega \omega(f,x)=\bigcap_{s \in \mathbb{R}} \overline{\{\varphi(x, t): t>s\}} F \simeq F' \,  h: U \rightarrow U \forall t \in \mathbb{R}, \ \forall x \in U: \quad \phi^{F}_t(x)=h^{-1} \circ \phi^{F'}_{\tau} \circ h(x),  \tau: U \times \mathbb{R} \rightarrow \mathbb{R}, \quad \frac{\partial \tau(x,t)}{\partial t} >0 \quad \forall x \in U  \forall x \in U: \ \omega(F,x)=\omega(F',x) F F'","['calculus', 'ordinary-differential-equations', 'differential-geometry', 'dynamical-systems']"
37,"$\frac{dy}{dx}=\frac{7}{3}y^{\frac{4}{7}},y(x_0\neq0)=0$ has a unique solution?",has a unique solution?,"\frac{dy}{dx}=\frac{7}{3}y^{\frac{4}{7}},y(x_0\neq0)=0","$$\frac{dy}{dx}=\frac{7}{3}y^{\frac{4}{7}},\quad y(x_0\neq0)=0$$ is simply solved with $$\int y^{-\frac{4}{7}}dy=\frac{7}{3}\int dx\implies y(x)=(x-x_0)^{\frac{7}{3}}.$$ My introductory textbook says this ODE has a unique solution. But as far as I can see, in addition to the one displayed above, also $y(x)=0$ solves the same ODE. What do I not understand about ""unique solution"" here, or how am I mistaken?","is simply solved with My introductory textbook says this ODE has a unique solution. But as far as I can see, in addition to the one displayed above, also solves the same ODE. What do I not understand about ""unique solution"" here, or how am I mistaken?","\frac{dy}{dx}=\frac{7}{3}y^{\frac{4}{7}},\quad y(x_0\neq0)=0 \int y^{-\frac{4}{7}}dy=\frac{7}{3}\int dx\implies y(x)=(x-x_0)^{\frac{7}{3}}. y(x)=0",['ordinary-differential-equations']
38,"A specific ""gaussian"" integral","A specific ""gaussian"" integral",,"I wish to compute the following function $$\forall\ \mathbf{b}\in \mathbb{R}^3,\quad V(\mathbf{b}) := \int_{\mathbb{R}^{3}} \frac{ e^{-\alpha \mathbf{k}^2 + \mathbf{b}\cdot \mathbf{k} } }{ \sqrt{\mathbf{k}^2+m^2}}\,  d^{3} \mathbf{k} \quad \text{where}\enspace \mathbf{k}^2:= \lVert \mathbf{k}\rVert^2 $$ I thought of looking for an ODE it would satisfy, but to no avail up to now. A change to spherical coordinates with "" $\mathbf{b}$ "" as the $z$ axis yields $$\begin{aligned} & \iiint \frac{ e^{-\alpha\hspace{.5pt} k^2\hspace{.5pt} + \hspace{.5pt}b\, k\hspace{.5pt} \cos \theta } }{ \sqrt{k^2+m^2} }\, k^2 \hspace{.7pt} \sin \theta\,  d k\, d\theta\, d\varphi = 2\hspace{.7pt}\pi \int_0^{+\infty} \frac{ k^2\, e^{-\alpha\hspace{.5pt} k^2\hspace{.5pt} } }{ \sqrt{k^2+m^2} }\, \left[\frac{e^{\hspace{.5pt} b\, k\hspace{.5pt} \cos \theta}}{- \hspace{.5pt} b\, k}\right]_0^{\pi}\, dk \quad \text{where}\enspace k:= \lVert \mathbf{k}\rVert,\ b:= \lVert \mathbf{b}\rVert \\ &= \frac{2\hspace{.7pt} \pi}{b} \int_0^{+\infty} \frac{ k\left( e^{-\alpha\hspace{.5pt} k^2\hspace{.5pt} + b\hspace{.3pt} k } - e^{-\alpha\hspace{.5pt} k^2\hspace{.5pt} - b\hspace{.3pt} k } \right)}{ \sqrt{k^2+m^2}}\, dk \end{aligned}$$ Inserting then the following definition of Hermite polynomials : $\quad \displaystyle e^{2\hspace{.3pt}x\hspace{.3pt}t - t^2} = \sum_{n=0}^{+\infty} H_n(x)\, \frac{t^n}{n!} $ with $t:= \sqrt{\alpha}\, k,\ x := \frac{b}{2\sqrt{\alpha}} $ $$ \begin{aligned}  V(b) &= \frac{2\hspace{.7pt} \pi}{b} \int_0^{+\infty} \frac{k}{ \sqrt{k^2+m^2}}  \sum_{n=0}^{+\infty} \left( H_n\left(\frac{b}{2\sqrt{\alpha}}\right) - H_n\left(\frac{-\, b}{2\sqrt{\alpha}}\right) \right) \frac{\left(\sqrt{\alpha}\, k\right)^n}{n!} \, dk \\ &=  \frac{2\hspace{.7pt} \pi}{b} \sum_{n=0}^{+\infty} H_n\left(\frac{b}{2\sqrt{\alpha}}\right) \big( 1 + (-1)^{n+1}\big) \frac{\left(\sqrt{\alpha}\right)^n}{n!}  \int_0^{+\infty} \frac{k^{n+1}}{\sqrt{k^2+m^2}} \, dk \end{aligned}$$ The last integral formally looks like a Beta function after the following change of variable $\genfrac{[}{]}{0pt}{0}{l:= \frac{k^2}{m^2}}{dl = \frac{2}{m^2}\, k\, dk}$ $$ \int_0^{+\infty} \frac{k^{n+1}}{m \sqrt{k^2/m^2 + 1}} \, dk = \frac{m}{2}\int_0^{+\infty} \frac{m^n\, l^{\frac{n}{2}}}{\sqrt{l+1}} \, dl  = \frac{m^{n+1}}{2} B\left(\frac{n}{2}+ 1 , -\frac{n+1}{2}\right)$$ Indeed, the integral is bluntly divergent and one should not have permuted sum and integral... Question: can $V$ be expressed in terms of special functions? or as a power series in $b$ ? (in fact I did found one not for $\lVert \mathbf{b}\rVert$ , but as a function of each component $b_1, b_2, b_3$ and it is desperately involved... so I still keep on looking for something more reasonnable...) I also tried integration by parts on $$V(b) = \frac{2\hspace{.7pt} \pi}{b} \int_0^{+\infty} \frac{ k }{ \sqrt{k^2+m^2}} \times e^{-\alpha\hspace{.5pt} k^2 }\, \sinh(b\hspace{.3pt} k)\, dk$$ and complex integration... EDIT: I meant PDE above... in fact I've just found an ODE for $$U(b):= \int_0^{+\infty} \frac{ e^{-\alpha\hspace{.5pt} k^2 }\, \cosh(b\hspace{.3pt} k) }{ \sqrt{k^2+m^2}}\, dk$$ Indeed $$ \begin{aligned} U'(b) & = \int_0^{+\infty} \frac{ k\, e^{-\alpha\hspace{.5pt} k^2 }\, \sinh(b\hspace{.3pt} k) }{ \sqrt{k^2+m^2}}\, dk\\ (\text{Int. by part}) &= \left[ \sqrt{k^2+m^2}\, e^{-\alpha\hspace{.5pt} k^2 }\, \sinh(b\hspace{.3pt} k) \right]_0^{+\infty} - \int_0^{+\infty} \sqrt{k^2+m^2} \left[ (- 2 \alpha k) e^{-\alpha\hspace{.5pt} k^2 }\, \sinh(b\hspace{.3pt} k) +b\, e^{-\alpha\hspace{.5pt} k^2 }\, \cosh(b\hspace{.3pt} k) \right]\, dk \end{aligned}$$ The ""boundary term"" in the integral by part vanishes, and the other can be expressed in terms of the following: $$ U''(b) = \int_0^{+\infty} \frac{k^2 e^{-\alpha\hspace{.5pt} k^2 }\, \cosh(b\hspace{.3pt} k) }{ \sqrt{k^2+m^2}}\, dk = \int_0^{+\infty} \frac{\big( k^2 +m^2 - m^2\big) e^{-\alpha\hspace{.5pt} k^2 }\, \cosh(b\hspace{.3pt} k) }{ \sqrt{k^2+m^2}}\, dk $$ Then consider also $U'''(b) +m^2 U'(b)$ . Shake everything, and you  should find a linear ODE... with non constant coeff, so there will still be work...","I wish to compute the following function I thought of looking for an ODE it would satisfy, but to no avail up to now. A change to spherical coordinates with "" "" as the axis yields Inserting then the following definition of Hermite polynomials : with The last integral formally looks like a Beta function after the following change of variable Indeed, the integral is bluntly divergent and one should not have permuted sum and integral... Question: can be expressed in terms of special functions? or as a power series in ? (in fact I did found one not for , but as a function of each component and it is desperately involved... so I still keep on looking for something more reasonnable...) I also tried integration by parts on and complex integration... EDIT: I meant PDE above... in fact I've just found an ODE for Indeed The ""boundary term"" in the integral by part vanishes, and the other can be expressed in terms of the following: Then consider also . Shake everything, and you  should find a linear ODE... with non constant coeff, so there will still be work...","\forall\ \mathbf{b}\in \mathbb{R}^3,\quad V(\mathbf{b}) := \int_{\mathbb{R}^{3}} \frac{ e^{-\alpha \mathbf{k}^2 + \mathbf{b}\cdot \mathbf{k} } }{ \sqrt{\mathbf{k}^2+m^2}}\,  d^{3} \mathbf{k} \quad \text{where}\enspace \mathbf{k}^2:= \lVert \mathbf{k}\rVert^2  \mathbf{b} z \begin{aligned}
& \iiint \frac{ e^{-\alpha\hspace{.5pt} k^2\hspace{.5pt} + \hspace{.5pt}b\, k\hspace{.5pt} \cos \theta } }{ \sqrt{k^2+m^2} }\, k^2 \hspace{.7pt} \sin \theta\,  d k\, d\theta\, d\varphi = 2\hspace{.7pt}\pi \int_0^{+\infty} \frac{ k^2\, e^{-\alpha\hspace{.5pt} k^2\hspace{.5pt} } }{ \sqrt{k^2+m^2} }\, \left[\frac{e^{\hspace{.5pt} b\, k\hspace{.5pt} \cos \theta}}{- \hspace{.5pt} b\, k}\right]_0^{\pi}\, dk \quad \text{where}\enspace k:= \lVert \mathbf{k}\rVert,\ b:= \lVert \mathbf{b}\rVert \\
&= \frac{2\hspace{.7pt} \pi}{b} \int_0^{+\infty} \frac{ k\left( e^{-\alpha\hspace{.5pt} k^2\hspace{.5pt} + b\hspace{.3pt} k } - e^{-\alpha\hspace{.5pt} k^2\hspace{.5pt} - b\hspace{.3pt} k } \right)}{ \sqrt{k^2+m^2}}\, dk
\end{aligned} \quad \displaystyle e^{2\hspace{.3pt}x\hspace{.3pt}t - t^2} = \sum_{n=0}^{+\infty} H_n(x)\, \frac{t^n}{n!}  t:= \sqrt{\alpha}\, k,\ x := \frac{b}{2\sqrt{\alpha}}   \begin{aligned}
 V(b) &= \frac{2\hspace{.7pt} \pi}{b} \int_0^{+\infty} \frac{k}{ \sqrt{k^2+m^2}}  \sum_{n=0}^{+\infty} \left( H_n\left(\frac{b}{2\sqrt{\alpha}}\right) - H_n\left(\frac{-\, b}{2\sqrt{\alpha}}\right) \right) \frac{\left(\sqrt{\alpha}\, k\right)^n}{n!} \, dk \\
&=  \frac{2\hspace{.7pt} \pi}{b} \sum_{n=0}^{+\infty} H_n\left(\frac{b}{2\sqrt{\alpha}}\right) \big( 1 + (-1)^{n+1}\big) \frac{\left(\sqrt{\alpha}\right)^n}{n!}  \int_0^{+\infty} \frac{k^{n+1}}{\sqrt{k^2+m^2}} \, dk
\end{aligned} \genfrac{[}{]}{0pt}{0}{l:= \frac{k^2}{m^2}}{dl = \frac{2}{m^2}\, k\, dk}  \int_0^{+\infty} \frac{k^{n+1}}{m \sqrt{k^2/m^2 + 1}} \, dk = \frac{m}{2}\int_0^{+\infty} \frac{m^n\, l^{\frac{n}{2}}}{\sqrt{l+1}} \, dl  = \frac{m^{n+1}}{2} B\left(\frac{n}{2}+ 1 , -\frac{n+1}{2}\right) V b \lVert \mathbf{b}\rVert b_1, b_2, b_3 V(b) = \frac{2\hspace{.7pt} \pi}{b} \int_0^{+\infty} \frac{ k }{ \sqrt{k^2+m^2}} \times e^{-\alpha\hspace{.5pt} k^2 }\, \sinh(b\hspace{.3pt} k)\, dk U(b):= \int_0^{+\infty} \frac{ e^{-\alpha\hspace{.5pt} k^2 }\, \cosh(b\hspace{.3pt} k) }{ \sqrt{k^2+m^2}}\, dk  \begin{aligned}
U'(b) & = \int_0^{+\infty} \frac{ k\, e^{-\alpha\hspace{.5pt} k^2 }\, \sinh(b\hspace{.3pt} k) }{ \sqrt{k^2+m^2}}\, dk\\
(\text{Int. by part}) &= \left[ \sqrt{k^2+m^2}\, e^{-\alpha\hspace{.5pt} k^2 }\, \sinh(b\hspace{.3pt} k) \right]_0^{+\infty} - \int_0^{+\infty} \sqrt{k^2+m^2} \left[ (- 2 \alpha k) e^{-\alpha\hspace{.5pt} k^2 }\, \sinh(b\hspace{.3pt} k) +b\, e^{-\alpha\hspace{.5pt} k^2 }\, \cosh(b\hspace{.3pt} k) \right]\, dk
\end{aligned}  U''(b) = \int_0^{+\infty} \frac{k^2 e^{-\alpha\hspace{.5pt} k^2 }\, \cosh(b\hspace{.3pt} k) }{ \sqrt{k^2+m^2}}\, dk = \int_0^{+\infty} \frac{\big( k^2 +m^2 - m^2\big) e^{-\alpha\hspace{.5pt} k^2 }\, \cosh(b\hspace{.3pt} k) }{ \sqrt{k^2+m^2}}\, dk  U'''(b) +m^2 U'(b)","['ordinary-differential-equations', 'improper-integrals', 'analytic-functions', 'gaussian-integral']"
39,"How to solve the differential equation $dx/dy = x^2 y\,$?",How to solve the differential equation ?,"dx/dy = x^2 y\,","How to solve? I have gotten $$  y\,dy = \frac{dx}{x^2}  $$ then taking integrals $$  \int y\, dy = \int \frac{dx}{x^2}  $$ and solving: $$  y = \sqrt{-2x^{-1} + C}  $$ but it's not the correct answer.",How to solve? I have gotten then taking integrals and solving: but it's not the correct answer.," 
y\,dy = \frac{dx}{x^2} 
  
\int y\, dy = \int \frac{dx}{x^2} 
  
y = \sqrt{-2x^{-1} + C} 
","['calculus', 'integration', 'ordinary-differential-equations', 'analysis']"
40,$\frac{\partial \psi}{\partial \mu}$ where $\psi$ is the solution of $\ddot{x}+\tanh{(2\dot{x}+x)}=\mu e^t$,where  is the solution of,\frac{\partial \psi}{\partial \mu} \psi \ddot{x}+\tanh{(2\dot{x}+x)}=\mu e^t,"I am asked to find $\frac{\partial \psi}{\partial \mu}(t, t_0, x_0, \dot{x}_0, µ) \ $ at the point $ \ (t, t_0, x_0, \dot{x}_0, µ) = (t, 0, 0, 0, 0)$ where $\psi$ is the solution of $\ddot{x}+\tanh{(2\dot{x}+x)}=\mu e^t \ $ . I transformed the equation into $$ \begin{cases} & \dot{x}_1=x_2 \newline & \dot{x}_2=-\tanh{(2x_2+x_1)}+\mu e^t \end{cases} $$ So $\frac{\partial \psi}{\partial \mu}$ is the solution of $\dot{X}=AX+b(t)$ , $X(0)= \begin{pmatrix} 0 \newline 0 \end{pmatrix} $ where $A(t)=Df =  \begin{pmatrix} 0 & 1\newline -\frac{1}{cosh^2{(2x_2+x_1)}} & -\frac{2}{cosh^2{(2x_2+x_1)}} \end{pmatrix} $ evaluated in a solution of the system (with $\mu = 0$ in this case) but I don't know how to get a solution (I didn't see any similar problem in class before). How can i get a solution? Or is there any other way to find the partial derivative?","I am asked to find at the point where is the solution of . I transformed the equation into So is the solution of , where evaluated in a solution of the system (with in this case) but I don't know how to get a solution (I didn't see any similar problem in class before). How can i get a solution? Or is there any other way to find the partial derivative?","\frac{\partial \psi}{\partial \mu}(t, t_0, x_0, \dot{x}_0, µ) \   \ (t, t_0, x_0, \dot{x}_0, µ) = (t, 0, 0, 0, 0) \psi \ddot{x}+\tanh{(2\dot{x}+x)}=\mu e^t \  
\begin{cases}
& \dot{x}_1=x_2 \newline
& \dot{x}_2=-\tanh{(2x_2+x_1)}+\mu e^t
\end{cases}
 \frac{\partial \psi}{\partial \mu} \dot{X}=AX+b(t) X(0)=
\begin{pmatrix}
0 \newline
0
\end{pmatrix}
 A(t)=Df = 
\begin{pmatrix}
0 & 1\newline
-\frac{1}{cosh^2{(2x_2+x_1)}} & -\frac{2}{cosh^2{(2x_2+x_1)}}
\end{pmatrix}
 \mu = 0","['ordinary-differential-equations', 'numerical-methods']"
41,What is the difference between the Auxiliary Equation and the Characteristic Equation? (ODEs),What is the difference between the Auxiliary Equation and the Characteristic Equation? (ODEs),,"When learning to solve homogeneous ODEs with constant coefficients a, b, and c, the resulting polynomial ( $ar^2 + br + c$ ) is referred to in my text as the auxiliary equation . However, in the next section on solving homogeneous ODEs with variable coefficients (specifically dealing with an ODE of the form $at^2y'' + bty' + cy = 0$ ), the resulting polynomial [ $ar^2 + (b-a)r + c$ ] is referred to as the characteristic equation . What--if any--is the primary difference/distinction being made in the names of the two polynomials in these applications?","When learning to solve homogeneous ODEs with constant coefficients a, b, and c, the resulting polynomial ( ) is referred to in my text as the auxiliary equation . However, in the next section on solving homogeneous ODEs with variable coefficients (specifically dealing with an ODE of the form ), the resulting polynomial [ ] is referred to as the characteristic equation . What--if any--is the primary difference/distinction being made in the names of the two polynomials in these applications?",ar^2 + br + c at^2y'' + bty' + cy = 0 ar^2 + (b-a)r + c,['ordinary-differential-equations']
42,Confusing notation when substituting differential equation,Confusing notation when substituting differential equation,,"Let's say I have a differential equation $$ \frac{dy(x)}{dx}=f(y,x) \tag{1} $$ Then, I notice that this would be much easier to solve if $x$ is substituted via $z=ax+b$ . Is it correct to write it like this: $$ \frac{dy(\frac{z-b}{a})}{dz/a}=f(y,\frac{z-b}{a})  \tag{2} $$ It feels like I have forgotten the chain rule here. The expression on the left hand side still looks like something I can evaluate using the chain rule: $$ \frac{dy(\frac{z-b}{a})}{dz/a} = a \left.\frac{dy(g)}{dg}\right|_{g=\frac{z-b}{a}} \frac{dg}{dz} = \left.\frac{dy(g)}{dg}\right|_{g=\frac{z-b}{a}}  \tag{3} $$ So that the system becomes $$ \left.\frac{dy(g)}{dg}\right|_{g=\frac{z-b}{a}} = f(y,\frac{z-b}{a}) \tag{4}$$ or again $$ \frac{dy(g)}{dg} = f(y,g) \tag{5}$$ Where is the problem in the notation here? I think in (2), the way it is written, I am not supposed to apply the chain rule but rather replace $y$ with some other function which then hides the full dependence on $z$ . But can I write this down more cleanly?","Let's say I have a differential equation Then, I notice that this would be much easier to solve if is substituted via . Is it correct to write it like this: It feels like I have forgotten the chain rule here. The expression on the left hand side still looks like something I can evaluate using the chain rule: So that the system becomes or again Where is the problem in the notation here? I think in (2), the way it is written, I am not supposed to apply the chain rule but rather replace with some other function which then hides the full dependence on . But can I write this down more cleanly?"," \frac{dy(x)}{dx}=f(y,x) \tag{1}  x z=ax+b  \frac{dy(\frac{z-b}{a})}{dz/a}=f(y,\frac{z-b}{a})  \tag{2}   \frac{dy(\frac{z-b}{a})}{dz/a} = a \left.\frac{dy(g)}{dg}\right|_{g=\frac{z-b}{a}} \frac{dg}{dz} = \left.\frac{dy(g)}{dg}\right|_{g=\frac{z-b}{a}}  \tag{3}   \left.\frac{dy(g)}{dg}\right|_{g=\frac{z-b}{a}} = f(y,\frac{z-b}{a}) \tag{4}  \frac{dy(g)}{dg} = f(y,g) \tag{5} y z","['ordinary-differential-equations', 'notation', 'substitution', 'chain-rule']"
43,Finding the general solution of a linear differential equation when there is just y' given,Finding the general solution of a linear differential equation when there is just y' given,,"The formulation of the problem is as following: $$(\cos x) * y' = \cos x + 2\sin x$$ Since this is a linear differential of the first order, and I'm looking for a general solution, the aim is to find $P(x)$ and $Q(x)$ . Since there is just $y'$ , I thought $P(x)$ would be zero. However, the official solution of the equation is $$y=(1/(\cos x)^2)(C + x/2 + \sin 2x/4) $$ In no way can I get this solution if $P(x)$ is $0$ . But what is it then? I tried getting $y$ through integration of the left side by $dx$ , and then plugging in $x$ derived from that equation into the original linear differential equation, but that is not giving me the requested solution either. I would appreciate any help provided.","The formulation of the problem is as following: Since this is a linear differential of the first order, and I'm looking for a general solution, the aim is to find and . Since there is just , I thought would be zero. However, the official solution of the equation is In no way can I get this solution if is . But what is it then? I tried getting through integration of the left side by , and then plugging in derived from that equation into the original linear differential equation, but that is not giving me the requested solution either. I would appreciate any help provided.","(\cos x) * y' = \cos x + 2\sin x P(x) Q(x) y' P(x) y=(1/(\cos x)^2)(C + x/2 + \sin 2x/4)
 P(x) 0 y dx x",['ordinary-differential-equations']
44,Is it always okay to substitute $a^2$ in place of $D^2$ in the inverse differential operator of cosh ax?,Is it always okay to substitute  in place of  in the inverse differential operator of cosh ax?,a^2 D^2,"In class, we were told that it was okay to make the following substitution while trying to solve a particular integral of a nonhomogeneous linear ODE, if $f(D)$ contains only even powers of $D$ , and $f(a) \neq 0$ $$\frac{1}{f(D)}\cosh ax = \frac{1}{f(a)}\cosh ax$$ But in some examples, we only replaced $D^2$ with $a^2$ , while keeping odd powers. For example, the general solution of the following equation $$\left(D^2-2D-3\right)y=2\cosh 3x$$ is $$y=c_1e^{3x}+c_2e^{-x}+\frac{xe^{3x}}{4}+\frac{1}{12}e^{-3x}$$ But our instructor replaced $D^2$ with $3^2$ while he was solving for the particular solution, which seems to give an inaccurate result $$y_p=\frac{1}{\left(D^2-2D-3\right)}2\cosh 3x = \frac{-1}{2}\frac{1}{\left(D-3\right)}\left(e^{3x}+e^{-3x}\right) = \frac{-xe^{3x}}{2}+\frac{e^{-3x}}{12}$$ So, my questions is, when is it really okay to make this substitution without getting an erroneous result? and does the same go for replacing $D^2$ with $-a^2$ if the image of the linear differential operator is $\sin ax$ or $\cos ax$ ?","In class, we were told that it was okay to make the following substitution while trying to solve a particular integral of a nonhomogeneous linear ODE, if contains only even powers of , and But in some examples, we only replaced with , while keeping odd powers. For example, the general solution of the following equation is But our instructor replaced with while he was solving for the particular solution, which seems to give an inaccurate result So, my questions is, when is it really okay to make this substitution without getting an erroneous result? and does the same go for replacing with if the image of the linear differential operator is or ?",f(D) D f(a) \neq 0 \frac{1}{f(D)}\cosh ax = \frac{1}{f(a)}\cosh ax D^2 a^2 \left(D^2-2D-3\right)y=2\cosh 3x y=c_1e^{3x}+c_2e^{-x}+\frac{xe^{3x}}{4}+\frac{1}{12}e^{-3x} D^2 3^2 y_p=\frac{1}{\left(D^2-2D-3\right)}2\cosh 3x = \frac{-1}{2}\frac{1}{\left(D-3\right)}\left(e^{3x}+e^{-3x}\right) = \frac{-xe^{3x}}{2}+\frac{e^{-3x}}{12} D^2 -a^2 \sin ax \cos ax,"['linear-algebra', 'ordinary-differential-equations', 'differential-operators']"
45,Solve the following differential equation: $\frac{d^2y}{dx^2}+a^2y=\cos ax.$,Solve the following differential equation:,\frac{d^2y}{dx^2}+a^2y=\cos ax.,"Solve the following differential equation: $\frac{d^2y}{dx^2}+a^2y=\cos ax.$ The solution given is as follows: The complementary function is $c_1\cos ax + c_2\sin ax$ ; the particular integral is $\frac{1}{D^2+a^2}(\cos ax)=\frac{1}{-a^2+a^2}\cos ax$ ; and thus the method fails. In this case, change $a$ to $a+h;$ this gives for the value of the particular integral, $\frac{1}{ D^2 + a^2}\ cos(a + h)x$ ; this expression, on the application of the principle above and the expansion of the operand by Taylor's series, becomes $$\frac{1}{ -(a+h)^2 + a^2}(\cos ax - \sin ax. hx - \cos ax\frac{h^2x^2}{1.2}+...).$$ The first term is already contained in the complementary function, and hence need not be regarded here; the particular integral will accordingly be written $$\begin{align}\frac{1}{2a+h}(x\sin ax + \frac{hx^2}{1.2}\cos {ax} +\space \text{terms with higher powers of h}\space)\end{align}$$ on making $h$ approach zero, this reduces to $$\frac{x\sin ax}{2a}.$$ The complete integral is $y= c_1\cos ax + c_2\sin ax + \frac{x\sin ax}{2a}.$ Can anyone please help me, understand the Taylor Series expansion for $\cos(a+h)x$ ? They are expanding $f(x)$ about which point ? (I think there's a lack of clarity in here). Neither do I understand, the rest part after that expansion. I am not quite getting it...","Solve the following differential equation: The solution given is as follows: The complementary function is ; the particular integral is ; and thus the method fails. In this case, change to this gives for the value of the particular integral, ; this expression, on the application of the principle above and the expansion of the operand by Taylor's series, becomes The first term is already contained in the complementary function, and hence need not be regarded here; the particular integral will accordingly be written on making approach zero, this reduces to The complete integral is Can anyone please help me, understand the Taylor Series expansion for ? They are expanding about which point ? (I think there's a lack of clarity in here). Neither do I understand, the rest part after that expansion. I am not quite getting it...",\frac{d^2y}{dx^2}+a^2y=\cos ax. c_1\cos ax + c_2\sin ax \frac{1}{D^2+a^2}(\cos ax)=\frac{1}{-a^2+a^2}\cos ax a a+h; \frac{1}{ D^2 + a^2}\ cos(a + h)x \frac{1}{ -(a+h)^2 + a^2}(\cos ax - \sin ax. hx - \cos ax\frac{h^2x^2}{1.2}+...). \begin{align}\frac{1}{2a+h}(x\sin ax + \frac{hx^2}{1.2}\cos {ax} +\space \text{terms with higher powers of h}\space)\end{align} h \frac{x\sin ax}{2a}. y= c_1\cos ax + c_2\sin ax + \frac{x\sin ax}{2a}. \cos(a+h)x f(x),"['ordinary-differential-equations', 'proof-explanation']"
46,Can't get the coefficient of a particular solution for a Differential Equation,Can't get the coefficient of a particular solution for a Differential Equation,,"I am trying to solve a differential equation that goes as follows: $$y'' - \left( a + 1 \right) \cdot y' + a \cdot y = 2 \cdot \cosh(a \cdot x),\, a \in \mathbb{R}$$ I have progressed enough and now trying to get the particular solution for $e^{ax}$ , using the form $y_p = Ae^{ax}$ . The problem is that putting it in the equation gives $0 = e^{ax}$ , making me unable to get $A$ to begin with. I suspect that I've done something wrong along the road but can't determine it. I appreciate any help I can get.","I am trying to solve a differential equation that goes as follows: I have progressed enough and now trying to get the particular solution for , using the form . The problem is that putting it in the equation gives , making me unable to get to begin with. I suspect that I've done something wrong along the road but can't determine it. I appreciate any help I can get.","y'' - \left( a + 1 \right) \cdot y' + a \cdot y = 2 \cdot \cosh(a \cdot x),\, a \in \mathbb{R} e^{ax} y_p = Ae^{ax} 0 = e^{ax} A",['ordinary-differential-equations']
47,Stability of the SIR epidemic model — Jacobian is singular,Stability of the SIR epidemic model — Jacobian is singular,,"The SIR epidemic model presents three differential equations for three time-dependent variables — $S(t)$ , $I(t)$ , $R(t)$ . \begin{aligned} \frac{dS}{dt} &= -\beta SI\\ \frac{dI}{dt} &= \beta SI - \gamma I\\ \frac{dR}{dt} &= \gamma I\\ \end{aligned} For the SIR model, I calculate the Jacobian matrix $J$ , as follows \begin{equation*} J =  \begin{pmatrix} \frac{\partial S}{\partial S} & \frac{\partial S}{\partial I} & \frac{\partial S}{\partial R} \\ \frac{\partial I}{\partial S} & \frac{\partial I}{\partial I} & \frac{\partial I}{\partial R} \\ \frac{\partial R}{\partial S} & \frac{\partial R}{\partial I} & \frac{\partial R}{\partial R} \end{pmatrix} = \begin{pmatrix} - \beta I & - \beta S & 0 \\ \beta I & \beta S -\gamma & 0 \\ 0 & \gamma & 0 \end{pmatrix} \end{equation*} In this case, the characteristic polynomial is given by $$(-\lambda) \left( \lambda^2+(\beta I-\beta S+\gamma)\lambda+\beta I\gamma \right)$$ To determine the stability of the disease-free equilibrium we substitute $S = 1$ and $I = 0$ for $S$ and $I$ . Hence, the characteristic polynomial becomes $$-\lambda^2(\lambda-\beta+\gamma)$$ This has three roots: $\lambda_1$ , $\lambda_2 =0$ and $\lambda_3=\beta-\gamma$ . The last eigenvalue to be negative is required that $\beta<\gamma$ . How can I determine the stability if two of the three eigenvalues are $0$ ? I have seen some solutions for $2 \times 2$ matrix, but I don't understand them. Is there any easy reason for being stable or unstable?","The SIR epidemic model presents three differential equations for three time-dependent variables — , , . For the SIR model, I calculate the Jacobian matrix , as follows In this case, the characteristic polynomial is given by To determine the stability of the disease-free equilibrium we substitute and for and . Hence, the characteristic polynomial becomes This has three roots: , and . The last eigenvalue to be negative is required that . How can I determine the stability if two of the three eigenvalues are ? I have seen some solutions for matrix, but I don't understand them. Is there any easy reason for being stable or unstable?","S(t) I(t) R(t) \begin{aligned}
\frac{dS}{dt} &= -\beta SI\\
\frac{dI}{dt} &= \beta SI - \gamma I\\
\frac{dR}{dt} &= \gamma I\\
\end{aligned} J \begin{equation*}
J = 
\begin{pmatrix}
\frac{\partial S}{\partial S} & \frac{\partial S}{\partial I} & \frac{\partial S}{\partial R} \\
\frac{\partial I}{\partial S} & \frac{\partial I}{\partial I} & \frac{\partial I}{\partial R} \\
\frac{\partial R}{\partial S} & \frac{\partial R}{\partial I} & \frac{\partial R}{\partial R}
\end{pmatrix}
=
\begin{pmatrix}
- \beta I & - \beta S & 0 \\
\beta I & \beta S -\gamma & 0 \\
0 & \gamma & 0
\end{pmatrix}
\end{equation*} (-\lambda) \left( \lambda^2+(\beta I-\beta S+\gamma)\lambda+\beta I\gamma \right) S = 1 I = 0 S I -\lambda^2(\lambda-\beta+\gamma) \lambda_1 \lambda_2 =0 \lambda_3=\beta-\gamma \beta<\gamma 0 2 \times 2","['ordinary-differential-equations', 'eigenvalues-eigenvectors', 'dynamical-systems', 'stability-in-odes', 'epidemiology']"
48,Is there a way to show that this ode system is asymptotically stable?,Is there a way to show that this ode system is asymptotically stable?,,"Suppose we have $$\dot{x} = -\frac{x}{y+a} $$ $$\dot{y} = -y$$ for $a>0$ , Is the above system asymptotically stable? Now, I know that we can solve for $y$ as $$y = y(0) e^{-t}$$ and we can choose for the $x$ part of the system, a Lyapunov function as $V(x) = 0.5 x^2$ and thus obtain the derivative: $$\dot{V} = -\frac{x^2}{y(0) e^{-t} + a}$$ which $\lim\limits_{t \rightarrow \infty} \dot{V} =  -\frac{x^2}{a}$ . But, I don't think this is correct. Is there a way to prove the $x,y$ system is stable, just by finding a positive definite Lyapunov function and proving that its derivative is negative definite?","Suppose we have for , Is the above system asymptotically stable? Now, I know that we can solve for as and we can choose for the part of the system, a Lyapunov function as and thus obtain the derivative: which . But, I don't think this is correct. Is there a way to prove the system is stable, just by finding a positive definite Lyapunov function and proving that its derivative is negative definite?","\dot{x} = -\frac{x}{y+a}  \dot{y} = -y a>0 y y = y(0) e^{-t} x V(x) = 0.5 x^2 \dot{V} = -\frac{x^2}{y(0) e^{-t} + a} \lim\limits_{t \rightarrow \infty} \dot{V} =  -\frac{x^2}{a} x,y","['ordinary-differential-equations', 'stability-in-odes', 'lyapunov-functions']"
49,"Proof of Variation of Parameters for Linear, 2nd Order ODE","Proof of Variation of Parameters for Linear, 2nd Order ODE",,"For the Linear, 2nd Order ODE $$\frac{d^2y}{dx^2} + P(x)\frac{dy}{dx} + Q(x)y = F(x)\text{,}$$ I've been asked to show that the equation's particular solution can be written $$y_p(x) = y_2(x)\int^{x}\frac{y_1(s)F(s)}{W(y_1(s),y_2(s))}ds - y_1(x)\int^{x}\frac{y_2(s)F(s)}{W(y_1(s),y_2(s))}ds\text{.}$$ It was given to start with $y_p(x) = y_1(x)v(x)$ and develop a 1st order ODE for $v'(x)$ . I started out by computing the 1st and 2nd order derivatives of $y_p(x)$ and substituting that into $\frac{d^2y}{dx^2} + P(x)\frac{dy}{dx} + Q(x)y = F(x)\text{.}$ This yields $$y_1\frac{d^2v}{dx^2} + 2\frac{dy_1}{dx}\frac{dv}{dx} + \frac{d^2y_1}{dx^2}v + P(x)y_1\frac{dv}{dx} + P(x)\frac{dy_1}{dx}v + Q(x)y_1v = F(x)\text{.}$$ I then multiplied both sides by $y_1$ to get $$y_1^2\frac{d^2v}{dx} + 2\frac{dy_1}{dx}\frac{dv}{dx}y_1 +vy_1\left[\frac{d^2y_1}{dx} + P(x)\frac{dy_1}{dx} + Q(x)y_1\right] + P(x)y_1^2\frac{dv}{dx} = y_1F(x)\text{.}$$ Since $y_1$ is a solution of the homogeneous equation $\left(Ly=0\right)$ , the expression inside the brackets vanishes, and what's left can be written as $$\frac{d}{dx}\left[y_1^2\frac{dv}{dx}\right] + P(x)y_1^2\frac{dv}{dx} = y_1F(x)\text{.}$$ Now I multiplied both sides by the integrating factor $e^{\int^{x}P(s)ds}$ , leading to $$\frac{d}{dx}\left[y_1^2\frac{dv}{dx}e^{\int^{x}P(s)ds}\right] = y_1F(x)e^{\int^{x}P(s)ds}\text{.}$$ Notice that $e^{\int^{x}P(s)ds} = \frac{1}{W(y_1(x),y_2(x))}\text{,}$ so the equation becomes $$\frac{d}{dx}\left[\frac{y_1^2}{W(y_1(x),y_2(x))}\frac{dv}{dx}\right] = \frac{y_1F(x)}{W(y_1(x),y_2(x))}$$ Integrating both sides: $$\frac{y_1^2}{W(y_1(x),y_2(x))}\frac{dv}{dx} = \int^{x}\frac{y_1(s)F(s)}{W(y_1(s),y_2(s))}ds$$ or $$\frac{dv}{dx} = \frac{W(y_1(x),y_2(x))}{y_1^2}\int^{x}\frac{y_1(s)F(s)}{W(y_1(s),y_2(s))}ds = \frac{d}{dx}\left[\frac{y_2}{y_1}\right]\int^{x}\frac{y_1(s)F(s)}{W(y_1(s),y_2(s))}ds$$ This is beginning to look very similar to $y_p(x)$ , but after integrating, I would need the equation to be $$v = \frac{y_2(x)}{y_1(x)}\int^{x}\frac{y_1(s)F(s)}{W(y_1(s),y_2(s))}ds - \int^{x}\frac{y_2(s)F(s)}{W(y_1(s),y_2(s))}ds$$ How do I make this leap? What am I missing?","For the Linear, 2nd Order ODE I've been asked to show that the equation's particular solution can be written It was given to start with and develop a 1st order ODE for . I started out by computing the 1st and 2nd order derivatives of and substituting that into This yields I then multiplied both sides by to get Since is a solution of the homogeneous equation , the expression inside the brackets vanishes, and what's left can be written as Now I multiplied both sides by the integrating factor , leading to Notice that so the equation becomes Integrating both sides: or This is beginning to look very similar to , but after integrating, I would need the equation to be How do I make this leap? What am I missing?","\frac{d^2y}{dx^2} + P(x)\frac{dy}{dx} + Q(x)y = F(x)\text{,} y_p(x) = y_2(x)\int^{x}\frac{y_1(s)F(s)}{W(y_1(s),y_2(s))}ds - y_1(x)\int^{x}\frac{y_2(s)F(s)}{W(y_1(s),y_2(s))}ds\text{.} y_p(x) = y_1(x)v(x) v'(x) y_p(x) \frac{d^2y}{dx^2} + P(x)\frac{dy}{dx} + Q(x)y = F(x)\text{.} y_1\frac{d^2v}{dx^2} + 2\frac{dy_1}{dx}\frac{dv}{dx} + \frac{d^2y_1}{dx^2}v + P(x)y_1\frac{dv}{dx} + P(x)\frac{dy_1}{dx}v + Q(x)y_1v = F(x)\text{.} y_1 y_1^2\frac{d^2v}{dx} + 2\frac{dy_1}{dx}\frac{dv}{dx}y_1 +vy_1\left[\frac{d^2y_1}{dx} + P(x)\frac{dy_1}{dx} + Q(x)y_1\right] + P(x)y_1^2\frac{dv}{dx} = y_1F(x)\text{.} y_1 \left(Ly=0\right) \frac{d}{dx}\left[y_1^2\frac{dv}{dx}\right] + P(x)y_1^2\frac{dv}{dx} = y_1F(x)\text{.} e^{\int^{x}P(s)ds} \frac{d}{dx}\left[y_1^2\frac{dv}{dx}e^{\int^{x}P(s)ds}\right] = y_1F(x)e^{\int^{x}P(s)ds}\text{.} e^{\int^{x}P(s)ds} = \frac{1}{W(y_1(x),y_2(x))}\text{,} \frac{d}{dx}\left[\frac{y_1^2}{W(y_1(x),y_2(x))}\frac{dv}{dx}\right] = \frac{y_1F(x)}{W(y_1(x),y_2(x))} \frac{y_1^2}{W(y_1(x),y_2(x))}\frac{dv}{dx} = \int^{x}\frac{y_1(s)F(s)}{W(y_1(s),y_2(s))}ds \frac{dv}{dx} = \frac{W(y_1(x),y_2(x))}{y_1^2}\int^{x}\frac{y_1(s)F(s)}{W(y_1(s),y_2(s))}ds = \frac{d}{dx}\left[\frac{y_2}{y_1}\right]\int^{x}\frac{y_1(s)F(s)}{W(y_1(s),y_2(s))}ds y_p(x) v = \frac{y_2(x)}{y_1(x)}\int^{x}\frac{y_1(s)F(s)}{W(y_1(s),y_2(s))}ds - \int^{x}\frac{y_2(s)F(s)}{W(y_1(s),y_2(s))}ds",['ordinary-differential-equations']
50,Get solution to $y'=\frac{x-y-3}{x+y}$ that is not limited in domain?,Get solution to  that is not limited in domain?,y'=\frac{x-y-3}{x+y},"I need to solve the diffeq $$y'=\frac{x-y-3}{x+y}$$ I proceed as follows. Let $$x(t)=t+\frac32,\qquad \frac{\text{d}x}{\text{d}t} = 1,\qquad \text{d}x=\text{d}t$$ $$y(v(t))=v(t)-\frac32,\qquad \frac{\text{d}y}{\text{d}v} = 1,\qquad \text{d}y=\text{d}v$$ I make the substitution to get $$v' = \frac{t-v}{t+v}$$ Now, let's set $v(t) = t \cdot u(t)$ . This yields $$tu'+u = \frac{1-u}{1+u}$$ when we perform the substitution. The equation is now separable so we integrate to get $$u' = \frac{-u^2 - 2u + 1}{t(u+1)}\Longleftrightarrow \frac{u+1}{-u^2 -2u+1} \cdot u' = \frac{1}{t}$$ $$\implies -\frac12\ln\left(\left|-u^2-2u+1\right|\right) = \ln(|t|) + C$$ I now solve for $u$ , so we have $$\begin{align}     -\frac12\ln\left(\left|-u^2-2u+1\right|\right) &= \ln(|t|) + C\\\\     \ln\left(\left|-u^2-2u+1\right|\right) &= -2\ln(|t|) + C\\\\     \left|-u^2-2u+1\right| &= \exp\left(\ln(|t|^{-2}) + C\right)\\\\     u^2+2u-1+2 &= 2\pm  C|t|^{-2}\\\\     (u+1)^2 &= 2\pm C|t|^{-2}\\\\     u &= -1+\sqrt{2\pm C|t|^{-2}} \end{align}$$ $$\implies v = -t+t\sqrt{2\pm Ct^{-2}}$$ $$\implies y+\frac32 = -\left(x-\frac32\right)+\left(x-\frac32\right)\sqrt{2\pm C\left(x-\frac32\right)^{-2}}$$ $$\implies y= -x+\left(x-\frac32\right)\sqrt{2\pm C\left(x-\frac32\right)^{-2}}$$ When I plot this in desmos, this is the graph I get Obviously, this is missing a chunk of the domain as the slope field of the diffeq produces solutions which extend the endpoints here out. Wolfram alpha however, gives the solutions $$y=-x\pm\sqrt{C+x^2+2\left(\frac{x^2}2-3x\right)}$$ As seen here, these solutions successfully capture all of the domain as we vary $C$ . What did I mess up in my solution to arrive at a $y(x)$ that is missing a chunk of the domain? (and how do i get wolfram alpha's solution?) Help is appreciated! :)","I need to solve the diffeq I proceed as follows. Let I make the substitution to get Now, let's set . This yields when we perform the substitution. The equation is now separable so we integrate to get I now solve for , so we have When I plot this in desmos, this is the graph I get Obviously, this is missing a chunk of the domain as the slope field of the diffeq produces solutions which extend the endpoints here out. Wolfram alpha however, gives the solutions As seen here, these solutions successfully capture all of the domain as we vary . What did I mess up in my solution to arrive at a that is missing a chunk of the domain? (and how do i get wolfram alpha's solution?) Help is appreciated! :)","y'=\frac{x-y-3}{x+y} x(t)=t+\frac32,\qquad \frac{\text{d}x}{\text{d}t} = 1,\qquad \text{d}x=\text{d}t y(v(t))=v(t)-\frac32,\qquad \frac{\text{d}y}{\text{d}v} = 1,\qquad \text{d}y=\text{d}v v' = \frac{t-v}{t+v} v(t) = t \cdot u(t) tu'+u = \frac{1-u}{1+u} u' = \frac{-u^2 - 2u + 1}{t(u+1)}\Longleftrightarrow \frac{u+1}{-u^2 -2u+1} \cdot u' = \frac{1}{t} \implies -\frac12\ln\left(\left|-u^2-2u+1\right|\right) = \ln(|t|) + C u \begin{align}
    -\frac12\ln\left(\left|-u^2-2u+1\right|\right) &= \ln(|t|) + C\\\\
    \ln\left(\left|-u^2-2u+1\right|\right) &= -2\ln(|t|) + C\\\\
    \left|-u^2-2u+1\right| &= \exp\left(\ln(|t|^{-2}) + C\right)\\\\
    u^2+2u-1+2 &= 2\pm  C|t|^{-2}\\\\
    (u+1)^2 &= 2\pm C|t|^{-2}\\\\
    u &= -1+\sqrt{2\pm C|t|^{-2}}
\end{align} \implies v = -t+t\sqrt{2\pm Ct^{-2}} \implies y+\frac32 = -\left(x-\frac32\right)+\left(x-\frac32\right)\sqrt{2\pm C\left(x-\frac32\right)^{-2}} \implies y= -x+\left(x-\frac32\right)\sqrt{2\pm C\left(x-\frac32\right)^{-2}} y=-x\pm\sqrt{C+x^2+2\left(\frac{x^2}2-3x\right)} C y(x)","['calculus', 'ordinary-differential-equations']"
51,If $M_x \neq N_y$ then is the differential not exact?,If  then is the differential not exact?,M_x \neq N_y,"I have to determine the validity of the following statement: Suppose you have a given differential $M(x,y)dx+N(x,y)dy$ . If $M_x\neq N_y$ then the differential is not exact. My answer is that the statement is false since $M(x,y)$ and $N(x,y)$ have to be continuous with continuous first partials, then the differential is exact if $\frac{\partial M}{\partial y}=\frac{\partial N}{\partial x}$ $M_x$ and $N_y$ are kind of irrelevant as far as I can see for determining whether the differential is exact. Am I correct? Also, I was wondering if the continuity part of the theorem plays a role at all in this answer.","I have to determine the validity of the following statement: Suppose you have a given differential . If then the differential is not exact. My answer is that the statement is false since and have to be continuous with continuous first partials, then the differential is exact if and are kind of irrelevant as far as I can see for determining whether the differential is exact. Am I correct? Also, I was wondering if the continuity part of the theorem plays a role at all in this answer.","M(x,y)dx+N(x,y)dy M_x\neq N_y M(x,y) N(x,y) \frac{\partial M}{\partial y}=\frac{\partial N}{\partial x} M_x N_y",['ordinary-differential-equations']
52,"Determine $f:\mathbb{R}\to\mathbb{R}$ derivable such that : $\forall x\in\mathbb{R}$, $f'(x)+f(x)=f(0)+f(1). $","Determine  derivable such that : ,",f:\mathbb{R}\to\mathbb{R} \forall x\in\mathbb{R} f'(x)+f(x)=f(0)+f(1). ,"I want to determine the functions $f:\mathbb{R}\to\mathbb{R}$ derivable such that $$  \forall x\in\mathbb{R}, f'(x)+f(x)=f(0)+f(1).\tag{$*$} $$ For that, if $f$ a differentiable function, check $(*)$ . We set $C=f(0)+f(1)$ , so $f$ is a solution of $y'+y=C$ . The solutions of this equation are the functions $x\mapsto C+De^{−x}$ with $D\in\mathbb{R}$ . Therefore, there exists $D\in\mathbb{R}$ such that $f(x)= C+De^{−x}$ . Then $$ C=f(0)+f(1)=2C+D(1+e^{-1})\Longleftrightarrow D=-\frac{C}{1+e^{-1}}. $$ Hence $f(x)= C\left(1-\frac{1}{1+e^{-1}}\right)e^{−x}$ . But, I see in an indication that the solutions are of the form $$ f(x)=−\lambda(1+e^{−1})+\lambda e^{-x}\text{ with }\lambda\in\mathbb{R} $$ please where is the problem?","I want to determine the functions derivable such that For that, if a differentiable function, check . We set , so is a solution of . The solutions of this equation are the functions with . Therefore, there exists such that . Then Hence . But, I see in an indication that the solutions are of the form please where is the problem?","f:\mathbb{R}\to\mathbb{R} 
 \forall x\in\mathbb{R}, f'(x)+f(x)=f(0)+f(1).\tag{*}
 f (*) C=f(0)+f(1) f y'+y=C x\mapsto C+De^{−x} D\in\mathbb{R} D\in\mathbb{R} f(x)= C+De^{−x} 
C=f(0)+f(1)=2C+D(1+e^{-1})\Longleftrightarrow D=-\frac{C}{1+e^{-1}}.
 f(x)= C\left(1-\frac{1}{1+e^{-1}}\right)e^{−x} 
f(x)=−\lambda(1+e^{−1})+\lambda e^{-x}\text{ with }\lambda\in\mathbb{R}
","['ordinary-differential-equations', 'functions', 'derivatives']"
53,Green's Function and Zero Modes of Differential Operators,Green's Function and Zero Modes of Differential Operators,,"I have this question from reading QFT textbooks. Consider harmonic oscillator as a simpler model for differential operators. The Lagrangian is given by $$L=\frac{1}{2}\dot{x}^{2}-\frac{1}{2}\omega^{2}x^{2}.$$ After integration by parts, one can express the classical action as $$S[x;j]=\int_{-\infty}^{\infty}dt\left[\frac{1}{2}x(t)\left(-\frac{d^{2}}{dt^{2}}-\omega^{2}\right)x(t)+x(t)j(t)\right],$$ where $j(t)$ is an external source. One can define the following second order differential operator $$L(t,s)\equiv\left(\frac{d^{2}}{dt^{2}}+\omega^{2}\right)\delta(t-s),$$ where $\delta(t-s)$ is the Dirac delta function . Using this notation, one can write the classical action in a compact form $$S[x;j]=-\int_{-\infty}^{\infty}\int_{-\infty}^{\infty}dtds\frac{1}{2}x(t)L(t,s)x(s)+\int_{-\infty}^{\infty}dtx(t)j(t)\equiv-\frac{1}{2}\langle x,Lx\rangle+\langle j,x\rangle.$$ In QFT, one defines the Greens's function of the differential operator $L$ as its ""right inverse"", i.e. $$\left(\frac{d^{2}}{dt^{2}}+\omega^{2}\right)G(t-s)=\delta(t-s),$$ where $\delta(t-s)$ is treated as the ""continuous generalization"" of the Kronecker delta symbol . In our compact notation, this is just $$LG=𝟙.$$ Then, it is well-known that the equation of motion $$\ddot{x}(t)+\omega^{2}x(t)=j(t) \tag{1}$$ has a unique solution $$x(t)=x_{0}(t)+\int_{-\infty}^{\infty}dsG(t-s)j(s), \tag{$\star$}$$ where $x_{0}$ is the unique solution of the homogeneous ODE $$\ddot{x}(t)+\omega^{2}x(t)=0. \tag{2}$$ If we use the compact notation introduced above, the solution ( $\star$ ) is very easy to understand: $$Lx=Lx_{0}+LGj=LGj=𝟙j=j,$$ which is precisely equation (1). However, there is a problem. In linear algebra, if an operator has an inverse, then it cannot have zero-eigenmodes. But from equation (2), it is clear that $x_{0}$ is a zero mode of the linear operator $L$ . Then, we cannot define its Green's function $G(t-s)$ , which is absurd. The same problem arises when we are dealing with QFT in higher dimensions, and $L$ becomes a second order partial differential operator. What's going on? Where did I make mistakes?","I have this question from reading QFT textbooks. Consider harmonic oscillator as a simpler model for differential operators. The Lagrangian is given by After integration by parts, one can express the classical action as where is an external source. One can define the following second order differential operator where is the Dirac delta function . Using this notation, one can write the classical action in a compact form In QFT, one defines the Greens's function of the differential operator as its ""right inverse"", i.e. where is treated as the ""continuous generalization"" of the Kronecker delta symbol . In our compact notation, this is just Then, it is well-known that the equation of motion has a unique solution where is the unique solution of the homogeneous ODE If we use the compact notation introduced above, the solution ( ) is very easy to understand: which is precisely equation (1). However, there is a problem. In linear algebra, if an operator has an inverse, then it cannot have zero-eigenmodes. But from equation (2), it is clear that is a zero mode of the linear operator . Then, we cannot define its Green's function , which is absurd. The same problem arises when we are dealing with QFT in higher dimensions, and becomes a second order partial differential operator. What's going on? Where did I make mistakes?","L=\frac{1}{2}\dot{x}^{2}-\frac{1}{2}\omega^{2}x^{2}. S[x;j]=\int_{-\infty}^{\infty}dt\left[\frac{1}{2}x(t)\left(-\frac{d^{2}}{dt^{2}}-\omega^{2}\right)x(t)+x(t)j(t)\right], j(t) L(t,s)\equiv\left(\frac{d^{2}}{dt^{2}}+\omega^{2}\right)\delta(t-s), \delta(t-s) S[x;j]=-\int_{-\infty}^{\infty}\int_{-\infty}^{\infty}dtds\frac{1}{2}x(t)L(t,s)x(s)+\int_{-\infty}^{\infty}dtx(t)j(t)\equiv-\frac{1}{2}\langle x,Lx\rangle+\langle j,x\rangle. L \left(\frac{d^{2}}{dt^{2}}+\omega^{2}\right)G(t-s)=\delta(t-s), \delta(t-s) LG=𝟙. \ddot{x}(t)+\omega^{2}x(t)=j(t) \tag{1} x(t)=x_{0}(t)+\int_{-\infty}^{\infty}dsG(t-s)j(s), \tag{\star} x_{0} \ddot{x}(t)+\omega^{2}x(t)=0. \tag{2} \star Lx=Lx_{0}+LGj=LGj=𝟙j=j, x_{0} L G(t-s) L","['ordinary-differential-equations', 'partial-differential-equations', 'greens-function', 'quantum-field-theory']"
54,How to solve nonlinear differential equation system?,How to solve nonlinear differential equation system?,,"I'm trying to solve this problem about nonlinear differential system: Consider the first order nonlinear differential equation system given by $$ \left\{ \begin{array}{}      x'& = \ 1-x-y   \\ y'& = \ x(y^2-1)(1-x-y). \end{array} \right.     \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \   (*) $$ a) Calculate the equilibrium points of the system. b) Find the solution that passes through the point $(0,2)$ and determine its orbit. The item a) its clear for me. I find that all points of the line $y=-x+1$ are equilibrium points of $(*)$ . My problem comes with item b). I don't really know how to solve the problem, because since it is a non-linear problem I don't know how to find the solution :( In what way does it affect that the solution must pass through the point $(0,2)$ ? Any help is welcome, thank you!!","I'm trying to solve this problem about nonlinear differential system: Consider the first order nonlinear differential equation system given by a) Calculate the equilibrium points of the system. b) Find the solution that passes through the point and determine its orbit. The item a) its clear for me. I find that all points of the line are equilibrium points of . My problem comes with item b). I don't really know how to solve the problem, because since it is a non-linear problem I don't know how to find the solution :( In what way does it affect that the solution must pass through the point ? Any help is welcome, thank you!!","
\left\{
\begin{array}{}
     x'& = \ 1-x-y
  \\ y'& = \ x(y^2-1)(1-x-y).
\end{array}
\right.     \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \   (*)
 (0,2) y=-x+1 (*) (0,2)","['ordinary-differential-equations', 'dynamical-systems', 'nonlinear-dynamics']"
55,(Uniqueness of) solution to $f'(t)=-\int_0^{f(t)}\!\mathrm dg(x)$,(Uniqueness of) solution to,f'(t)=-\int_0^{f(t)}\!\mathrm dg(x),Let $g$ be a measure over the unit interval. Can we show that $f'(t)=-\int_0^{f(t)}\!\mathrm d g(x)$ has a unique nonzero solution $f$ ? (The boundary condition is that $f(0)>0$ and there is some $s>0$ with $f(t)=0$ for $t> s$ .),Let be a measure over the unit interval. Can we show that has a unique nonzero solution ? (The boundary condition is that and there is some with for .),g f'(t)=-\int_0^{f(t)}\!\mathrm d g(x) f f(0)>0 s>0 f(t)=0 t> s,"['ordinary-differential-equations', 'derivatives', 'partial-differential-equations']"
56,How to solve the ordinary differential Equations? [closed],How to solve the ordinary differential Equations? [closed],,"Closed. This question does not meet Mathematics Stack Exchange guidelines . It is not currently accepting answers. Please provide additional context , which ideally explains why the question is relevant to you and our  community. Some forms of context include: background and motivation, relevant definitions, source, possible strategies, your current progress, why the question is interesting or important, etc. Closed 1 year ago . Improve this question \begin{equation} \begin{aligned} \frac{\mathrm{d} x_1}{\mathrm{d} t} & =-x_1+x_2\\ \frac{\mathrm{d} x_2}{\mathrm{d} t} & =x_1\cos t -x_2 \end{aligned} \label{eq:3.6} \end{equation} At the beginning, I prove the zero solution is stable, then I used Maltab ode45 function test more than 10 different initial points, they all convergenced to zero, so I guess the zero solution asymptotically stable, But I can't prove it. So I want to know how to solve the ODEs.","Closed. This question does not meet Mathematics Stack Exchange guidelines . It is not currently accepting answers. Please provide additional context , which ideally explains why the question is relevant to you and our  community. Some forms of context include: background and motivation, relevant definitions, source, possible strategies, your current progress, why the question is interesting or important, etc. Closed 1 year ago . Improve this question At the beginning, I prove the zero solution is stable, then I used Maltab ode45 function test more than 10 different initial points, they all convergenced to zero, so I guess the zero solution asymptotically stable, But I can't prove it. So I want to know how to solve the ODEs.","\begin{equation}
\begin{aligned}
\frac{\mathrm{d} x_1}{\mathrm{d} t} & =-x_1+x_2\\
\frac{\mathrm{d} x_2}{\mathrm{d} t} & =x_1\cos t -x_2
\end{aligned}
\label{eq:3.6}
\end{equation}","['ordinary-differential-equations', 'dynamical-systems']"
57,Gradient flows: is this a typo in a discretization scheme?,Gradient flows: is this a typo in a discretization scheme?,,"I'm reading below theorem from this lecture note. Theorem 4.5. Let $H$ be a Hilbert space over $\mathbb{R}$ . If $\varphi: H \rightarrow \mathbb{R}$ is differentiable and convex, then for every $u \in H$ there exists a unique $y:[0, \infty) \rightarrow H$ such that $$ \begin{aligned} & y^{\prime}(t)=-\nabla \varphi(y(t)), \quad t \geq 0, \\ & y(0)=u. \end{aligned} $$ Idea of proof. Fix $h>0$ . Discretize the differential equation with Euler's implicit scheme, $$ \frac{y((n+1) h)-y(n h)}{h}=-\nabla \varphi(\color{red}{y(n h)}) . $$ Then, with $y_n=y(n h)$ , $$ y_{n+1}+h \nabla \varphi\left(\color{red}{y_{n+1}}\right)=y_n, $$ so that $$ y_{n+1}=(I+h \nabla \varphi)^{-1}\left(y_n\right) . $$ One of the difficulties is to show that the function $x \mapsto x+h \nabla \varphi(x)$ is invertible. Then $$ J_h(x):=(I+h \nabla \varphi)^{-1}(x), \quad x \in H, $$ is called the resolvent associated to $\nabla \varphi$ . We obtain $$ y_n=J_h^n(u), $$ and this is hoped to be a good approximation for $y(n h)$ . The next steps are to show that $$ y(t):=\lim _{k \rightarrow \infty} J_{t / k}^n(u) $$ exists and that the function $y$ thus defined is the unique solution. My understanding It seems to me $$ \frac{y((n+1) h)-y(n h)}{h}=-\nabla \varphi(\color{red}{y(n h)}) $$ implies $y_{n+1}+h \nabla \varphi\left(\color{red}{y_{n}}\right)=y_n$ , and not $y_{n+1}+h \nabla \varphi\left(\color{red}{y_{n+1}}\right)=y_n$ as written in the lecture note. Then it should be $y_{n}=(I-h \nabla \varphi)^{-1}\left(y_{n+1}\right)$ . Could you confirm if my understanding is correct, or I miss something else?","I'm reading below theorem from this lecture note. Theorem 4.5. Let be a Hilbert space over . If is differentiable and convex, then for every there exists a unique such that Idea of proof. Fix . Discretize the differential equation with Euler's implicit scheme, Then, with , so that One of the difficulties is to show that the function is invertible. Then is called the resolvent associated to . We obtain and this is hoped to be a good approximation for . The next steps are to show that exists and that the function thus defined is the unique solution. My understanding It seems to me implies , and not as written in the lecture note. Then it should be . Could you confirm if my understanding is correct, or I miss something else?","H \mathbb{R} \varphi: H \rightarrow \mathbb{R} u \in H y:[0, \infty) \rightarrow H 
\begin{aligned}
& y^{\prime}(t)=-\nabla \varphi(y(t)), \quad t \geq 0, \\
& y(0)=u.
\end{aligned}
 h>0 
\frac{y((n+1) h)-y(n h)}{h}=-\nabla \varphi(\color{red}{y(n h)}) .
 y_n=y(n h) 
y_{n+1}+h \nabla \varphi\left(\color{red}{y_{n+1}}\right)=y_n,
 
y_{n+1}=(I+h \nabla \varphi)^{-1}\left(y_n\right) .
 x \mapsto x+h \nabla \varphi(x) 
J_h(x):=(I+h \nabla \varphi)^{-1}(x), \quad x \in H,
 \nabla \varphi 
y_n=J_h^n(u),
 y(n h) 
y(t):=\lim _{k \rightarrow \infty} J_{t / k}^n(u)
 y 
\frac{y((n+1) h)-y(n h)}{h}=-\nabla \varphi(\color{red}{y(n h)})
 y_{n+1}+h \nabla \varphi\left(\color{red}{y_{n}}\right)=y_n y_{n+1}+h \nabla \varphi\left(\color{red}{y_{n+1}}\right)=y_n y_{n}=(I-h \nabla \varphi)^{-1}\left(y_{n+1}\right)","['ordinary-differential-equations', 'derivatives', 'hilbert-spaces', 'gradient-flows']"
58,Existance of Linearly Independent generalized eigenvectors,Existance of Linearly Independent generalized eigenvectors,,"Is it true that all $n\times n$ matrices $A$ admit $n$ generalized eigenvectors? Are they all linearly independent? Intuitively I know it's true over complex numbers but I could not find a rigorous proof. I am doing linear systems of differential equations and I want to prove that for the system $x'=Ax$ it is always possible to find $n$ linearly independent solutions of the form: $$ x=\sum_{i=1}^n \xi_ie^{\lambda t}t^{n-i} $$ Where $\xi_i$ is a generalized evigenvector of rank $i$ with associated eigenvalue $\lambda$ . I need to ensure that if $\lambda$ is a repeated eigenvalue of multiplicity $m$ then $m$ linearly independent solutions are associated with it. For insufficient eigenvalues, existence of higher rank generalized eigenvectors guarantees extra solutions because of the following property: $$ A\xi_i=\xi_{i-1}+\lambda\xi_i \quad\text{which is analogous to}\quad [t^ne^{\lambda t}]'=t^{n-1}e^{\lambda t}+\lambda t^ne^{\lambda t} $$ Geometrically, if the generalized eigenvectors are linearly independent, then we can express every point in terms of generalized eigenvectors. This explains why the solution is is a degenerate node that curves towards the eigendirection.","Is it true that all matrices admit generalized eigenvectors? Are they all linearly independent? Intuitively I know it's true over complex numbers but I could not find a rigorous proof. I am doing linear systems of differential equations and I want to prove that for the system it is always possible to find linearly independent solutions of the form: Where is a generalized evigenvector of rank with associated eigenvalue . I need to ensure that if is a repeated eigenvalue of multiplicity then linearly independent solutions are associated with it. For insufficient eigenvalues, existence of higher rank generalized eigenvectors guarantees extra solutions because of the following property: Geometrically, if the generalized eigenvectors are linearly independent, then we can express every point in terms of generalized eigenvectors. This explains why the solution is is a degenerate node that curves towards the eigendirection.","n\times n A n x'=Ax n 
x=\sum_{i=1}^n \xi_ie^{\lambda t}t^{n-i}
 \xi_i i \lambda \lambda m m 
A\xi_i=\xi_{i-1}+\lambda\xi_i \quad\text{which is analogous to}\quad [t^ne^{\lambda t}]'=t^{n-1}e^{\lambda t}+\lambda t^ne^{\lambda t}
","['linear-algebra', 'ordinary-differential-equations']"
59,Expression for $ \left( \frac{d^2}{dx^2} + \frac{c}{x} \frac{d}{dx} \right)^k \phi(x)$,Expression for, \left( \frac{d^2}{dx^2} + \frac{c}{x} \frac{d}{dx} \right)^k \phi(x),"Consider the following differential operator \begin{align} D_c= \frac{d^2}{dx^2} + \frac{c}{x}  \frac{d}{dx} \end{align} defined on $x>0$ for some given non-zero constant $c$ . Let $\phi(x)=\exp(-x^2/2)$ be the Gaussian function. For every positive integer $k$ , we are interested in characterizing \begin{align} D^k_c \phi(x) \end{align} i.e., repeated application of this transform. What I did First, note that \begin{align}  \frac{d^k}{dx^k} \phi(x)= (-1)^k \operatorname{He}_k(x) \phi(x) \end{align} where $\operatorname{He}_k(x)$ is the Hermite polynomial. Here is the expression for the first few expressions \begin{align} D_c \phi(x)&=  \operatorname{He}_2(x) \phi(x)- \frac{c}{x}  \operatorname{He}_1(x) \phi(x)\\ &= \left(\operatorname{He}_2(x) - c \right) \phi(x) \end{align} and (if correct) \begin{align} D_c^2 \phi(x)= \left( \operatorname{He}_2(x)^2-c \operatorname{He}_2(x)-4  \operatorname{He}_1^2(x)+2 + c(2-( \operatorname{He}_2(x)-c))  \right) \phi(x) \end{align} So the expression appears to be of the form \begin{align} D_c^k \phi(x)= \text{Poly}(2k) \phi(x) \end{align} where $\text{Poly}(2k)$ is a polynomial of degree $2k$ .  But I couldn't really find the expression for the polynomial. One more thing this operator looks like Bessel operator but is slightly different.","Consider the following differential operator defined on for some given non-zero constant . Let be the Gaussian function. For every positive integer , we are interested in characterizing i.e., repeated application of this transform. What I did First, note that where is the Hermite polynomial. Here is the expression for the first few expressions and (if correct) So the expression appears to be of the form where is a polynomial of degree .  But I couldn't really find the expression for the polynomial. One more thing this operator looks like Bessel operator but is slightly different.","\begin{align}
D_c= \frac{d^2}{dx^2} + \frac{c}{x}  \frac{d}{dx}
\end{align} x>0 c \phi(x)=\exp(-x^2/2) k \begin{align}
D^k_c \phi(x)
\end{align} \begin{align}
 \frac{d^k}{dx^k} \phi(x)= (-1)^k \operatorname{He}_k(x) \phi(x)
\end{align} \operatorname{He}_k(x) \begin{align}
D_c \phi(x)&=  \operatorname{He}_2(x) \phi(x)- \frac{c}{x}  \operatorname{He}_1(x) \phi(x)\\
&= \left(\operatorname{He}_2(x) - c \right) \phi(x)
\end{align} \begin{align}
D_c^2 \phi(x)= \left( \operatorname{He}_2(x)^2-c \operatorname{He}_2(x)-4  \operatorname{He}_1^2(x)+2 + c(2-( \operatorname{He}_2(x)-c))  \right) \phi(x)
\end{align} \begin{align}
D_c^k \phi(x)= \text{Poly}(2k) \phi(x)
\end{align} \text{Poly}(2k) 2k","['ordinary-differential-equations', 'derivatives', 'hermite-polynomials']"
60,Solve $y''-y'-6y=e^{3t}+5$,Solve,y''-y'-6y=e^{3t}+5,"Solve the following initial value problem $$y'' - y' - 6y = e^{3t} + 5, \quad y(0) = 0, \quad y'(0) = 0 $$ I used the Laplace transform and got $$ Y(s) \left(s^2-s-6\right)=\dfrac 1{s-3}+\dfrac 5s $$ then brought it to the other side $$Y(s)=\dfrac 1{(s-3)(s^2-s-6)}+\dfrac 5{s(s^2-s-6)}$$ Taking user577215664's advice, I got $$s^2−s−6=(s+2)(s−3)$$ so i split it into partial fractions $$\dfrac A{(s-3)}+\dfrac B{(s-3)^2}+\dfrac C{(s+2)}$$ and solving it i got in part A $$ A=-\dfrac1{17}, \qquad B = C = \dfrac1{17} $$ and in part b i got $$ B=1\dfrac4{6}, \qquad A = C = -\dfrac5{6} $$ so end up having $$-\dfrac1{17(s-3)}+\dfrac1{17(s-3)^2}+\dfrac1{17(s+2)}+-\dfrac5{6(s)}+\dfrac{10}{6(s-3)}+-\dfrac5{6(s+2)}$$ $$\dfrac1{17}e^{3t}+\dfrac1{17}te^{3t}+\dfrac1{17}e^{-2t}+-\dfrac5{6}+\dfrac{10}{6}e^{3t}+-\dfrac5{6}e^{-2t}$$ which can be simplified to $$\dfrac{82}{51}e^{3t}+\dfrac{79}{102}e^{-2t}+\dfrac1{17}te^{3t}+\dfrac5{6}$$ is this correct?","Solve the following initial value problem I used the Laplace transform and got then brought it to the other side Taking user577215664's advice, I got so i split it into partial fractions and solving it i got in part A and in part b i got so end up having which can be simplified to is this correct?","y'' - y' - 6y = e^{3t} + 5, \quad y(0) = 0, \quad y'(0) = 0   Y(s) \left(s^2-s-6\right)=\dfrac 1{s-3}+\dfrac 5s  Y(s)=\dfrac 1{(s-3)(s^2-s-6)}+\dfrac 5{s(s^2-s-6)} s^2−s−6=(s+2)(s−3) \dfrac A{(s-3)}+\dfrac B{(s-3)^2}+\dfrac C{(s+2)}  A=-\dfrac1{17}, \qquad B = C = \dfrac1{17}   B=1\dfrac4{6}, \qquad A = C = -\dfrac5{6}  -\dfrac1{17(s-3)}+\dfrac1{17(s-3)^2}+\dfrac1{17(s+2)}+-\dfrac5{6(s)}+\dfrac{10}{6(s-3)}+-\dfrac5{6(s+2)} \dfrac1{17}e^{3t}+\dfrac1{17}te^{3t}+\dfrac1{17}e^{-2t}+-\dfrac5{6}+\dfrac{10}{6}e^{3t}+-\dfrac5{6}e^{-2t} \dfrac{82}{51}e^{3t}+\dfrac{79}{102}e^{-2t}+\dfrac1{17}te^{3t}+\dfrac5{6}","['ordinary-differential-equations', 'laplace-transform', 'initial-value-problems', 'inverse-laplace']"
61,Solving Recurrence Relation with Exponential Relationship,Solving Recurrence Relation with Exponential Relationship,,"I've come across a recurrence relationship in a modeling problem given by the following: $$ a_{n+1} = a_{n} + \gamma a_{n} e^{-a_{n}^2} \qquad a_0, \gamma > 0. $$ My intuition tells me that the sequence has an accumulation point as a result of the exponential decay, however, I'm struggling to find a way to show this formally. Does anyone have any resources or means of solving this kind of relation or finding an upper bound?","I've come across a recurrence relationship in a modeling problem given by the following: My intuition tells me that the sequence has an accumulation point as a result of the exponential decay, however, I'm struggling to find a way to show this formally. Does anyone have any resources or means of solving this kind of relation or finding an upper bound?"," a_{n+1} = a_{n} + \gamma a_{n} e^{-a_{n}^2} \qquad a_0, \gamma > 0. ","['ordinary-differential-equations', 'recurrence-relations']"
62,non-linear system of a ODE,non-linear system of a ODE,,"Let $$y'=A(t)y(t)+b(t,y(t)), \qquad y(0)=y_0$$ and $U:=U_\epsilon(0)$ so that there exists $C\geq 0$ with $\|b(t,y)\|\leq C\|y\|$ for all $y\in U$ , $t \in \mathbb R$ and $y^TA(t)y\leq -\lambda \|y\|^2$ for all $t \in \mathbb R$ and a $\lambda>C$ . Prove that $y_0\in U \implies y(t) \in U$ for all $t>0$ . I tried to show $\|y(t)-0\| = \|y_0 + \int_0^t y'(s)ds\| \leq \dots \leq \epsilon$ but this does not lead to the goal.","Let and so that there exists with for all , and for all and a . Prove that for all . I tried to show but this does not lead to the goal.","y'=A(t)y(t)+b(t,y(t)), \qquad y(0)=y_0 U:=U_\epsilon(0) C\geq 0 \|b(t,y)\|\leq C\|y\| y\in U t \in \mathbb R y^TA(t)y\leq -\lambda \|y\|^2 t \in \mathbb R \lambda>C y_0\in U \implies y(t) \in U t>0 \|y(t)-0\| = \|y_0 + \int_0^t y'(s)ds\| \leq \dots \leq \epsilon","['real-analysis', 'ordinary-differential-equations', 'analysis']"
63,Proving that these solutions are formally solving these differential equations: $x'' = -\text{sgn}(x')$ and $y'' = \sqrt{|y'|}$,Proving that these solutions are formally solving these differential equations:  and,x'' = -\text{sgn}(x') y'' = \sqrt{|y'|},"Please take a look also to the comments section, here, and in other people answers, since there are extended what are my apprehensions about the validity of the found answers. I have found these two solutions to the following differential equations by playing on Wolfram-Alpha, and I would like to prove that they are formally solutions of each equation: $$x'' = -\text{sgn}(x'),\quad \,x(0)=2,\,x'(0)=-2 \quad \Rightarrow \quad x(t) = \frac{1}{2}\left(1-\frac{t}{2}+\left|1-\frac{t}{2} \right|\right)^2\quad\text{(Eq. 1)}$$ $$y'' = \sqrt{|y'|},\quad \,y(0)=\frac{2}{3},\,y'(0)=-1 \quad \Rightarrow \quad y(t) = \frac{1}{12}\left(1-\frac{t}{2}+\left|1-\frac{t}{2} \right|\right)^3\quad\text{(Eq. 2)}$$ Notice that Wolfram-Alpha don't show close-form solutions for neither these equations as can be seen here and here . The following notation is going to be used: $\text{sgn}(t)$ is the Sign function , which fulfills that $\frac{\partial}{\partial t}\left(|t| \right) = \text{sgn}(t)$ and $\frac{\partial}{\partial t}\left(\text{sgn}(t) \right) = 2\ \delta(t)$ $\theta(t)$ is the Heaviside step function that fulfills $\frac{\partial}{\partial t}\left(\theta(t) \right) = \delta(t)$ and $\theta(t) = \frac{1}{2}\left(1+\text{sgn}(t) \right)$ $\delta(t)$ is the Dirac delta function that fulfills $\int_{-\infty}^\infty f(t)\delta(t)\,dt=f(0)$ with $\int_{-\infty}^\infty \delta(t)\,dt=1$ These definitions could be problematic, as it will reviewed later. Motivation Recently I figure out that no non-piecewise power series could have a finite extinction time due the Identity theorem , and found on this paper that a differential equation could have a solution  that achieve a finite extinction time if and only if its nonlinear and have a singular solutions , so it must fail to fulfill uniqueness of solutions, but luckily the paper explains that within the initial conditions time and the finite extinction time uniqueness is still hold. Since my intuition tells that classic mechanics system should achieve a finite extinction time where the movement due the system dynamics dies (as opposite of random thermal noise, which nature is external to the system dynamics as it were a random forcing force), so I started to look for some physics' examples, without finding many (I even tried to made them as in here and here ), and those I found ( here ) were quite hard to understand ( at least for me, I'm an electrician, nor a physicists neither a mathematician ). But a few days ago, someone in a Youtube comment named @siguc explains me the following: ""How about the motion of a brick on a horizontal surface with constant friction between the brick and the surface? Assuming the brick moves along the surface at $t=0$ , it'll stop eventually. Newton's law: $x''=-k\ g\ \text{sgn}(x')$ , where $g$ is $9.8\,\frac{m}{s^2}$ and $k$ is the friction coef."". So I started to googling about this problem founding terms as Coulomb damping and the Stribeck curve , but the only place I found the same brick problem was in this Wiki page and no close-form solutions were shown. Since the system were simple enough to be understood by me, I start by trying to see if I could find a solution to the simplest case by myself, so assuming here that $k\ g = 1$ , I got the $\text{(Eq. 1)}$ . Later I will explain why I choose those arbitrary initial conditions. What I have done so far On previous question I have found: In this question another person (@KBS) proves on his answer that the solution I found could be formally a solution to $$u' = -\text{sgn}(u) \sqrt{|u|},\quad \,u(0)=1 \quad \Rightarrow \quad u(t) = \frac{1}{4}\left(1-\frac{t}{2}+\left|1-\frac{t}{2} \right|\right)^2\qquad\text{(Eq. 3)}$$ Later I found here that for a positive finite extinction time $T>0$ and a positive initial conditions $v(0)>0$ that determines $T(v(0))$ , one can have that: $$\begin{array}{r c l} v' = -\text{sgn}(v) \sqrt[n]{|v|},\quad \,v(0)>0 \quad \Rightarrow \quad v(t) & = & \left[\frac{n-1}{n}\left(T-t\right)\right]^{\frac{n}{n-1}}\theta\left(T-t\right) \\ &\overset{!}{\equiv}& v(0)\cdot \left[\frac{1}{2}\left(1-\frac{t}{T}+\left|1-\frac{t}{T} \right|\right)\right]^{\frac{n}{n-1}}, \\ & & v(0) =  \left[\frac{n-1}{n}\cdot T\right]^{\frac{n}{n-1}}\qquad\text{(Eq. 4)} \end{array}$$ I have found these results in a laissez-faire way and not $100\%$ rigorously: I know that some definitions at the beginning have issues in the edge points, like the Heaviside function having or not a transition value of $1/2$ at $t=0$ , which I have ignored, so I am using things like $\theta(t) = (\theta(t))^n$ which have some weird consequences, as in (Eq. 4) where the exclamation point is evidenced on the equivalence could be flawed. I have preferred the version with the absolute value function trying to be as far as possible of having derivatives of the Dirac Delta function $\delta'(t)$ which I don't know how to handle them. Also has the consequence that if I evaluate the differential equation some issues happened at these edge points , like the rising of Dirac delta functions which broke the differential equation equality, but since it only happened on a zero-measure point, and the solutions doesn't have those problems, I believe they are valid as solutions. With this laissez-faire approach I was able to just test some solutions of the form $x(t) = a\cdot(T-t)^b$ similar to the found for (Eq. 3) and (Eq. 4) and made them fit the differential equation, since from the mentioned paper I believe beforehand that there was an existent singular solution that could achieve a finite extinction time. Since these special functions $\delta(t),\,\text{sgn}(t),\,\theta(t)$ are in reality distributions, which theory I know almost nothing, I would like to know if its possible to prove in a rigorous way they are indeed solutions of those differential equations. BONUS TRACK: Do you believe it is possible to find a general solution for $x''=-k\ g\ \text{sgn}(x')$ using (Eq. 1)? At first I was concern about having an initial condition for the speed $x'(0)<0$ , but I think now it makes sense since the system starts with an initial speed $|x'(0)|$ but through its dynamics it must start loosing speed immediately since friction is the only external force present in the sliding brick system. PS: on other question like this people have attacked the intuition of having solutions that achieve a finite ending time, being valid opinions, please keep it out of the discussion here, since I am trying to explore these kind of solutions as alternative - instead, feel free to extend the discussion in the mentioned question which is more suitable for it. Added later About my worries, as example, in Wikipedia the Heaviside step function is defines as $\theta(0)=\frac{1}{2}$ which could generate issues since I have assumed as true that $\theta(t)=(\theta(t))^n$ . Also, I am using that $$\frac{1}{2}\left(x +|x|\right) = \frac{1}{2}|x|\left(1+\frac{x}{|x|}\right) = \frac{1}{2}|x|\left(1+\text{sgn(x)}\right)=|x|\ \theta(x)$$ , but if instead I take as factor the other term I get: $$\frac{1}{2}\left(x +|x|\right) = \frac{1}{2}x\left(1+\frac{|x|}{x}\right) = \frac{1}{2}x\left(1+\frac{1}{\text{sgn(x)}}\right)$$ And since in Wikipedia defined the sign function as having $\text{sgn}(0) = 0$ then the term $\frac{1}{\text{sgn(x)}}$ hidden a division by zero, which is obviously a big issue. These are examples of why I am worried about the validity, this sames issues also made struggles with the definitions at the beginning. Hope you could elaborate why is not a problem if it really is not. An attempt for the bonus track If I used the assumption than $kg>0$ , which make sense with the physics problem the equation is coming from, and I used the change of variable $x' = kg z$ , the equation could become: $$\begin{array}{rcl} -\frac{x''}{kg} & = & \text{sgn}(x') \\ \iff -\frac{kgz'}{kg} & = & \text{sgn}(kg\ z) = \frac{kg\ z}{|kg\ z|} \overset{\text{since}\ kg>0}{=}\frac{kg\ z}{|kg|\ |z|} =\frac{z}{|z|} = \text{sgn}(z) \\ \Rightarrow z' & = & -\text{sgn}(z) \quad\text{which is equivalent to (Eq. 1)}\end{array}$$ With this, we have that the answer to: $$x''=-k\ g\ \text{sgn}(x'), \quad x(0) = 2kg,\,\,x'(0)=-2kg \Rightarrow x(t) = \frac{kg}{2}\left(1-\frac{t}{2}+\left|1-\frac{t}{2} \right|\right)^2$$ So for a general finite extinction time $T>0$ and constants such as $kg>0$ then an answer could be: $$ \begin{array}{l} x''=-k\ g\ \text{sgn}(x'), \qquad x(0) = T^2\frac{kg}{2}>0,\,\,x'(0)=-Tkg<0 \\ \Rightarrow x(t) = T^2\frac{kg}{8}\left(1-\frac{t}{T}+\left|1-\frac{t}{T} \right|\right)^2 = \frac{kg}{2}\left(T-t\right)^2\theta(T-t) \quad \text{(Eq. 6)} \end{array}$$ Does (Ec. 6) make sense-full as closed-form solution for the problem? I have used that $s(t) = \int -(T-t)\ dt\cdot\theta(T-t) = \left[\frac{1}{2}(T-t)^2 + C_0\right]\theta(T-t)$ such as: $$s'(t) = -(T-t)\theta(T-t)\quad + \underbrace{\left[\frac{1}{2}(T-t)^2 + C_0\right]\delta(T-t)}_{C_0 \equiv 0,\,\text{so all the expression could be zero by}\,x\delta(x) = 0}$$ It is still a general solution? Another example of the weird issues I have found, is that if in Wolfram Alpha, as example, I try to solve the equation (Eq. 1) with the founded solution of (Eq. 6) as: $$\frac{1}{kg}\frac{\partial^2}{\partial t^2}\left(\frac{kg}{2}\left(T-t\right)^2\theta(T-t)\right)+\text{sgn}\left(\frac{\partial}{\partial t}\left(\frac{kg}{2}\left(T-t\right)^2\theta(T-t)\right)\right)$$ I will find a mess as is shown here . But if instead I solve the less rigorous equation: $$\left(\frac{1}{kg}\frac{\partial^2}{\partial t^2}\left(\frac{kg}{2}\left(T-t\right)^2\right)+\text{sgn}\left(\frac{\partial}{\partial t}\left(\frac{kg}{2}\left(T-t\right)^2\right)\right)\right)\cdot\theta(T-t)$$ it shows now that the proposed solution solves the equation (except in one isolated point $t=T$ , which is also werid), and I find strange how and why the $\theta(t)$ function can cross through operations as it where a ghost (hope you understand what I am trying to say). Any source to some place where these properties are explain will be fantastic, Thanks.","Please take a look also to the comments section, here, and in other people answers, since there are extended what are my apprehensions about the validity of the found answers. I have found these two solutions to the following differential equations by playing on Wolfram-Alpha, and I would like to prove that they are formally solutions of each equation: Notice that Wolfram-Alpha don't show close-form solutions for neither these equations as can be seen here and here . The following notation is going to be used: is the Sign function , which fulfills that and is the Heaviside step function that fulfills and is the Dirac delta function that fulfills with These definitions could be problematic, as it will reviewed later. Motivation Recently I figure out that no non-piecewise power series could have a finite extinction time due the Identity theorem , and found on this paper that a differential equation could have a solution  that achieve a finite extinction time if and only if its nonlinear and have a singular solutions , so it must fail to fulfill uniqueness of solutions, but luckily the paper explains that within the initial conditions time and the finite extinction time uniqueness is still hold. Since my intuition tells that classic mechanics system should achieve a finite extinction time where the movement due the system dynamics dies (as opposite of random thermal noise, which nature is external to the system dynamics as it were a random forcing force), so I started to look for some physics' examples, without finding many (I even tried to made them as in here and here ), and those I found ( here ) were quite hard to understand ( at least for me, I'm an electrician, nor a physicists neither a mathematician ). But a few days ago, someone in a Youtube comment named @siguc explains me the following: ""How about the motion of a brick on a horizontal surface with constant friction between the brick and the surface? Assuming the brick moves along the surface at , it'll stop eventually. Newton's law: , where is and is the friction coef."". So I started to googling about this problem founding terms as Coulomb damping and the Stribeck curve , but the only place I found the same brick problem was in this Wiki page and no close-form solutions were shown. Since the system were simple enough to be understood by me, I start by trying to see if I could find a solution to the simplest case by myself, so assuming here that , I got the . Later I will explain why I choose those arbitrary initial conditions. What I have done so far On previous question I have found: In this question another person (@KBS) proves on his answer that the solution I found could be formally a solution to Later I found here that for a positive finite extinction time and a positive initial conditions that determines , one can have that: I have found these results in a laissez-faire way and not rigorously: I know that some definitions at the beginning have issues in the edge points, like the Heaviside function having or not a transition value of at , which I have ignored, so I am using things like which have some weird consequences, as in (Eq. 4) where the exclamation point is evidenced on the equivalence could be flawed. I have preferred the version with the absolute value function trying to be as far as possible of having derivatives of the Dirac Delta function which I don't know how to handle them. Also has the consequence that if I evaluate the differential equation some issues happened at these edge points , like the rising of Dirac delta functions which broke the differential equation equality, but since it only happened on a zero-measure point, and the solutions doesn't have those problems, I believe they are valid as solutions. With this laissez-faire approach I was able to just test some solutions of the form similar to the found for (Eq. 3) and (Eq. 4) and made them fit the differential equation, since from the mentioned paper I believe beforehand that there was an existent singular solution that could achieve a finite extinction time. Since these special functions are in reality distributions, which theory I know almost nothing, I would like to know if its possible to prove in a rigorous way they are indeed solutions of those differential equations. BONUS TRACK: Do you believe it is possible to find a general solution for using (Eq. 1)? At first I was concern about having an initial condition for the speed , but I think now it makes sense since the system starts with an initial speed but through its dynamics it must start loosing speed immediately since friction is the only external force present in the sliding brick system. PS: on other question like this people have attacked the intuition of having solutions that achieve a finite ending time, being valid opinions, please keep it out of the discussion here, since I am trying to explore these kind of solutions as alternative - instead, feel free to extend the discussion in the mentioned question which is more suitable for it. Added later About my worries, as example, in Wikipedia the Heaviside step function is defines as which could generate issues since I have assumed as true that . Also, I am using that , but if instead I take as factor the other term I get: And since in Wikipedia defined the sign function as having then the term hidden a division by zero, which is obviously a big issue. These are examples of why I am worried about the validity, this sames issues also made struggles with the definitions at the beginning. Hope you could elaborate why is not a problem if it really is not. An attempt for the bonus track If I used the assumption than , which make sense with the physics problem the equation is coming from, and I used the change of variable , the equation could become: With this, we have that the answer to: So for a general finite extinction time and constants such as then an answer could be: Does (Ec. 6) make sense-full as closed-form solution for the problem? I have used that such as: It is still a general solution? Another example of the weird issues I have found, is that if in Wolfram Alpha, as example, I try to solve the equation (Eq. 1) with the founded solution of (Eq. 6) as: I will find a mess as is shown here . But if instead I solve the less rigorous equation: it shows now that the proposed solution solves the equation (except in one isolated point , which is also werid), and I find strange how and why the function can cross through operations as it where a ghost (hope you understand what I am trying to say). Any source to some place where these properties are explain will be fantastic, Thanks.","x'' = -\text{sgn}(x'),\quad \,x(0)=2,\,x'(0)=-2 \quad \Rightarrow \quad x(t) = \frac{1}{2}\left(1-\frac{t}{2}+\left|1-\frac{t}{2} \right|\right)^2\quad\text{(Eq. 1)} y'' = \sqrt{|y'|},\quad \,y(0)=\frac{2}{3},\,y'(0)=-1 \quad \Rightarrow \quad y(t) = \frac{1}{12}\left(1-\frac{t}{2}+\left|1-\frac{t}{2} \right|\right)^3\quad\text{(Eq. 2)} \text{sgn}(t) \frac{\partial}{\partial t}\left(|t| \right) = \text{sgn}(t) \frac{\partial}{\partial t}\left(\text{sgn}(t) \right) = 2\ \delta(t) \theta(t) \frac{\partial}{\partial t}\left(\theta(t) \right) = \delta(t) \theta(t) = \frac{1}{2}\left(1+\text{sgn}(t) \right) \delta(t) \int_{-\infty}^\infty f(t)\delta(t)\,dt=f(0) \int_{-\infty}^\infty \delta(t)\,dt=1 t=0 x''=-k\ g\ \text{sgn}(x') g 9.8\,\frac{m}{s^2} k k\ g = 1 \text{(Eq. 1)} u' = -\text{sgn}(u) \sqrt{|u|},\quad \,u(0)=1 \quad \Rightarrow \quad u(t) = \frac{1}{4}\left(1-\frac{t}{2}+\left|1-\frac{t}{2} \right|\right)^2\qquad\text{(Eq. 3)} T>0 v(0)>0 T(v(0)) \begin{array}{r c l} v' = -\text{sgn}(v) \sqrt[n]{|v|},\quad \,v(0)>0 \quad \Rightarrow \quad v(t) & = & \left[\frac{n-1}{n}\left(T-t\right)\right]^{\frac{n}{n-1}}\theta\left(T-t\right) \\ &\overset{!}{\equiv}& v(0)\cdot \left[\frac{1}{2}\left(1-\frac{t}{T}+\left|1-\frac{t}{T} \right|\right)\right]^{\frac{n}{n-1}}, \\ & & v(0) =  \left[\frac{n-1}{n}\cdot T\right]^{\frac{n}{n-1}}\qquad\text{(Eq. 4)} \end{array} 100\% 1/2 t=0 \theta(t) = (\theta(t))^n \delta'(t) x(t) = a\cdot(T-t)^b \delta(t),\,\text{sgn}(t),\,\theta(t) x''=-k\ g\ \text{sgn}(x') x'(0)<0 |x'(0)| \theta(0)=\frac{1}{2} \theta(t)=(\theta(t))^n \frac{1}{2}\left(x +|x|\right) = \frac{1}{2}|x|\left(1+\frac{x}{|x|}\right) = \frac{1}{2}|x|\left(1+\text{sgn(x)}\right)=|x|\ \theta(x) \frac{1}{2}\left(x +|x|\right) = \frac{1}{2}x\left(1+\frac{|x|}{x}\right) = \frac{1}{2}x\left(1+\frac{1}{\text{sgn(x)}}\right) \text{sgn}(0) = 0 \frac{1}{\text{sgn(x)}} kg>0 x' = kg z \begin{array}{rcl}
-\frac{x''}{kg} & = & \text{sgn}(x') \\
\iff -\frac{kgz'}{kg} & = & \text{sgn}(kg\ z) = \frac{kg\ z}{|kg\ z|} \overset{\text{since}\ kg>0}{=}\frac{kg\ z}{|kg|\ |z|} =\frac{z}{|z|} = \text{sgn}(z) \\
\Rightarrow z' & = & -\text{sgn}(z) \quad\text{which is equivalent to (Eq. 1)}\end{array} x''=-k\ g\ \text{sgn}(x'), \quad x(0) = 2kg,\,\,x'(0)=-2kg \Rightarrow x(t) = \frac{kg}{2}\left(1-\frac{t}{2}+\left|1-\frac{t}{2} \right|\right)^2 T>0 kg>0  \begin{array}{l} x''=-k\ g\ \text{sgn}(x'), \qquad x(0) = T^2\frac{kg}{2}>0,\,\,x'(0)=-Tkg<0 \\ \Rightarrow x(t) = T^2\frac{kg}{8}\left(1-\frac{t}{T}+\left|1-\frac{t}{T} \right|\right)^2 = \frac{kg}{2}\left(T-t\right)^2\theta(T-t) \quad \text{(Eq. 6)} \end{array} s(t) = \int -(T-t)\ dt\cdot\theta(T-t) = \left[\frac{1}{2}(T-t)^2 + C_0\right]\theta(T-t) s'(t) = -(T-t)\theta(T-t)\quad + \underbrace{\left[\frac{1}{2}(T-t)^2 + C_0\right]\delta(T-t)}_{C_0 \equiv 0,\,\text{so all the expression could be zero by}\,x\delta(x) = 0} \frac{1}{kg}\frac{\partial^2}{\partial t^2}\left(\frac{kg}{2}\left(T-t\right)^2\theta(T-t)\right)+\text{sgn}\left(\frac{\partial}{\partial t}\left(\frac{kg}{2}\left(T-t\right)^2\theta(T-t)\right)\right) \left(\frac{1}{kg}\frac{\partial^2}{\partial t^2}\left(\frac{kg}{2}\left(T-t\right)^2\right)+\text{sgn}\left(\frac{\partial}{\partial t}\left(\frac{kg}{2}\left(T-t\right)^2\right)\right)\right)\cdot\theta(T-t) t=T \theta(t)","['ordinary-differential-equations', 'mathematical-physics', 'distribution-theory', 'singular-solution', 'finite-duration']"
64,How to solve a Fredholm equation with known $\lambda$?,How to solve a Fredholm equation with known ?,\lambda,"I have the Fredholm equation, $$\phi(x)=\sin x+\lambda\int_0^\pi\cos(x/2-3y)\phi(y)dy$$ and would like to solve it. First, I found  using the precondition for contraction of the Fredholm operator: $$|\lambda|<\frac{1}{M(b-a)}$$ That $|\lambda|<\sqrt{2}$ to give a contraction. So I insert 1 as a value for $\lambda$ and get: $$\phi(x)=\sin x+\int_0^\pi\cos(x/2-3y)\phi(y)dy$$ I solved this numerically using Mathematica. The solution is $\phi(x)=\sin(x)$ . However, how can I solve this ""by hand""? Thanks","I have the Fredholm equation, and would like to solve it. First, I found  using the precondition for contraction of the Fredholm operator: That to give a contraction. So I insert 1 as a value for and get: I solved this numerically using Mathematica. The solution is . However, how can I solve this ""by hand""? Thanks",\phi(x)=\sin x+\lambda\int_0^\pi\cos(x/2-3y)\phi(y)dy |\lambda|<\frac{1}{M(b-a)} |\lambda|<\sqrt{2} \lambda \phi(x)=\sin x+\int_0^\pi\cos(x/2-3y)\phi(y)dy \phi(x)=\sin(x),"['ordinary-differential-equations', 'integral-equations']"
65,Can't $f(x)$ be negative here?,Can't  be negative here?,f(x),"The following question was asked in JEE Advanced 2020. Question: Let $b$ be a nonzero real number. Suppose $f:\mathbb R\to\mathbb R$ is a differentiable function such that $f(0)=1$ . If the derivative $f'$ of $f$ satisfies the equation $$f'(x)=\frac{f(x)}{b^2+x^2},$$ for all $x\in\mathbb R$ , then which of the following statements is/are True? A) If $b\gt0$ , then $f$ is an increasing function. B) If $b\lt0$ , then $f$ is a decreasing function. C) $f(x)f(-x)=1$ for all $x\in\mathbb R$ D) $f(x)-f(-x)=0$ for all $x\in\mathbb R$ My Attempt: Positive or negative nature of $f'(x)$ is dependent on the positive or negative nature of $f(x),$ not $b$ . Solving the given differential equation, we get, $$\ln|f(x)|=\frac1{|b|}\tan^{-1}\left(\frac x{|b|}\right)$$ (Constant of integration is zero here) Therefore, $$|f(x)|=e^{\frac1{|b|}\tan^{-1}\left(\frac x{|b|}\right)}\\ \implies f(x)=\pm e^{\frac1{|b|}\tan^{-1}\left(\frac x{|b|}\right)} $$ I can see that option C) is correct. Option D) is incorrect. How to decide for option A) and B)? Official Answer is A,C.","The following question was asked in JEE Advanced 2020. Question: Let be a nonzero real number. Suppose is a differentiable function such that . If the derivative of satisfies the equation for all , then which of the following statements is/are True? A) If , then is an increasing function. B) If , then is a decreasing function. C) for all D) for all My Attempt: Positive or negative nature of is dependent on the positive or negative nature of not . Solving the given differential equation, we get, (Constant of integration is zero here) Therefore, I can see that option C) is correct. Option D) is incorrect. How to decide for option A) and B)? Official Answer is A,C.","b f:\mathbb R\to\mathbb R f(0)=1 f' f f'(x)=\frac{f(x)}{b^2+x^2}, x\in\mathbb R b\gt0 f b\lt0 f f(x)f(-x)=1 x\in\mathbb R f(x)-f(-x)=0 x\in\mathbb R f'(x) f(x), b \ln|f(x)|=\frac1{|b|}\tan^{-1}\left(\frac x{|b|}\right) |f(x)|=e^{\frac1{|b|}\tan^{-1}\left(\frac x{|b|}\right)}\\ \implies f(x)=\pm e^{\frac1{|b|}\tan^{-1}\left(\frac x{|b|}\right)} ","['calculus', 'ordinary-differential-equations', 'functions', 'contest-math']"
66,Stationary solutions of the Cauchy PDE problem,Stationary solutions of the Cauchy PDE problem,,"Consider $$\partial_{t}u -y\partial_{x}u +x\partial_{y}u = 0$$ in $t>0, (x,y) \in \mathbb{R}^{2}$ , with initial condition $$ u(0,x,y) =  u_{0}(x,y) $$ for $(x,y) \in \mathbb{R}^{2}$ . Find the stationary solutions (no dependence on $t$ ) and discuss their physical intepretation. My attempt: Suppose a solution $u$ is stationary. Then $\partial_{t}u = 0$ , so the PDE becomes $$-y\partial_{x}u +x\partial_{y}u = 0.$$ We can solve by using the method of characteristics which gives us the equations: \begin{align} \frac{dx}{ds} &= -y, \\ \frac{dy}{ds} &= x, \\ \frac{du}{ds} &= 0. \end{align} From here I am unsure. The first two equations can be solved to give $x=A\cos(s) +B\sin(s)$ and $y=C\cos(s) +D\sin(s)$ , and we also get that $u(s) = \rm constant$ . How can we proceed?","Consider in , with initial condition for . Find the stationary solutions (no dependence on ) and discuss their physical intepretation. My attempt: Suppose a solution is stationary. Then , so the PDE becomes We can solve by using the method of characteristics which gives us the equations: From here I am unsure. The first two equations can be solved to give and , and we also get that . How can we proceed?","\partial_{t}u -y\partial_{x}u +x\partial_{y}u = 0 t>0, (x,y) \in \mathbb{R}^{2}  u(0,x,y) =
 u_{0}(x,y)  (x,y) \in \mathbb{R}^{2} t u \partial_{t}u = 0 -y\partial_{x}u +x\partial_{y}u = 0. \begin{align}
\frac{dx}{ds} &= -y, \\
\frac{dy}{ds} &= x, \\
\frac{du}{ds} &= 0.
\end{align} x=A\cos(s) +B\sin(s) y=C\cos(s) +D\sin(s) u(s) = \rm constant","['ordinary-differential-equations', 'partial-differential-equations', 'continuity', 'characteristics']"
67,Solving $\frac{dy}{dx} = \frac{xy+3x-2y+6}{xy-3x-2y+6}$,Solving,\frac{dy}{dx} = \frac{xy+3x-2y+6}{xy-3x-2y+6},I'm stuck with this problem... $$\frac{\operatorname{d}y}{\operatorname{d}x} = \frac{xy+3x-2y+6}{xy-3x-2y+6}$$ I have tried separating variables in the following way: $$\frac{\operatorname{d}y}{\operatorname{d}x} = \frac{x(y+3)2(-y+3)}{y(x-2)3(-x+2)}$$ $$\left(\frac{y+3 \cdot 2(-y+3)}{y}\right)\operatorname{d}y = \left(\frac{x-2 \cdot 3(-x+2)}{x}\right)\operatorname{d}x$$ $$\left(\frac{-y+9}{y}\right) \operatorname{d}y = \left(\frac{-2x+4}{x}\right) \operatorname{d}x$$ Then integrating both sides I end up with $$y=2x+\ln\left(\frac{y^9}{x^4}\right)+C$$ I'd appreciate very much if someone give me the right answer because I don't know if my solution is correct or not. Thanks in advance,I'm stuck with this problem... I have tried separating variables in the following way: Then integrating both sides I end up with I'd appreciate very much if someone give me the right answer because I don't know if my solution is correct or not. Thanks in advance,\frac{\operatorname{d}y}{\operatorname{d}x} = \frac{xy+3x-2y+6}{xy-3x-2y+6} \frac{\operatorname{d}y}{\operatorname{d}x} = \frac{x(y+3)2(-y+3)}{y(x-2)3(-x+2)} \left(\frac{y+3 \cdot 2(-y+3)}{y}\right)\operatorname{d}y = \left(\frac{x-2 \cdot 3(-x+2)}{x}\right)\operatorname{d}x \left(\frac{-y+9}{y}\right) \operatorname{d}y = \left(\frac{-2x+4}{x}\right) \operatorname{d}x y=2x+\ln\left(\frac{y^9}{x^4}\right)+C,"['calculus', 'ordinary-differential-equations']"
68,Finding the ODE form of impulse response function $h(t)=e^{-at}H(t)t^n$,Finding the ODE form of impulse response function,h(t)=e^{-at}H(t)t^n,"Consider a linear system that has a known impulse response function $h(t)=e^{-at}H(t)(at)^n$ where $a$ and $n$ are known coefficients and $H(t)$ is the Heaviside function. If there is an external force applied to the system, $x(t)$ , the output of the system can be written as a convolution $$ y(t) = \int_{-\infty}^\infty h(t - \tau) x(\tau)\ d\tau. $$ My question is for non-integer $n > 0$ , how can we write the system in the first order ODE format? That is, $$ \frac{d \mathbf{s}}{dt} = \mathbf{f}(\mathbf{s}, x) $$ for some states $\mathbf{s}\in\mathbb{R}^m$ and some function $\mathbf{f}(\cdot)$ , and the output is determined by $$ y = g(\mathbf{s}) $$ with some function $g(\cdot)$ . What I have done For integer $n$ , I know how to solve it. First, take the simplest example where $n = 0$ . The output $y$ can be written as $$ y(t) = \int_{-\infty}^\infty e^{-a(t-\tau)}H(t - \tau) x(\tau) d\tau $$ where the derivative is $$ \frac{dy}{dt}(t) = \int_{-\infty}^\infty \left[-a H(t - \tau) + \delta(t-\tau)\right] e^{-a(t-\tau)} x(\tau) d\tau $$ with $\delta(\cdot)$ as the Dirac delta function, and can be simplified into $$ \frac{dy}{dt}(t) =-a y(t) + x(t) $$ So in the case of $n = 0$ , we can write $ds/dt = f(s, x) = -a s + x$ with the output $y = g(s) = s$ . The similar procedure can be repeated for $n = 1$ , but in this case we need to differentiate $y$ twice, and get $$ \frac{d^2y}{dt^2}(t) =-2 a \frac{dy}{dt}(t) - a^2 y(t) + 2 a x(t). $$ With the equation above, we can write the first order ODE form by choosing the states $\mathbf{s} = (dy/dt, y)$ . Similar procedures can be done for integer $n \geq 0$ . However, it is unclear to me how to get such equation with non-integer $n$ .","Consider a linear system that has a known impulse response function where and are known coefficients and is the Heaviside function. If there is an external force applied to the system, , the output of the system can be written as a convolution My question is for non-integer , how can we write the system in the first order ODE format? That is, for some states and some function , and the output is determined by with some function . What I have done For integer , I know how to solve it. First, take the simplest example where . The output can be written as where the derivative is with as the Dirac delta function, and can be simplified into So in the case of , we can write with the output . The similar procedure can be repeated for , but in this case we need to differentiate twice, and get With the equation above, we can write the first order ODE form by choosing the states . Similar procedures can be done for integer . However, it is unclear to me how to get such equation with non-integer .","h(t)=e^{-at}H(t)(at)^n a n H(t) x(t) 
y(t) = \int_{-\infty}^\infty h(t - \tau) x(\tau)\ d\tau.
 n > 0 
\frac{d \mathbf{s}}{dt} = \mathbf{f}(\mathbf{s}, x)
 \mathbf{s}\in\mathbb{R}^m \mathbf{f}(\cdot) 
y = g(\mathbf{s})
 g(\cdot) n n = 0 y 
y(t) = \int_{-\infty}^\infty e^{-a(t-\tau)}H(t - \tau) x(\tau) d\tau
 
\frac{dy}{dt}(t) = \int_{-\infty}^\infty \left[-a H(t - \tau) + \delta(t-\tau)\right] e^{-a(t-\tau)} x(\tau) d\tau
 \delta(\cdot) 
\frac{dy}{dt}(t) =-a y(t) + x(t)
 n = 0 ds/dt = f(s, x) = -a s + x y = g(s) = s n = 1 y 
\frac{d^2y}{dt^2}(t) =-2 a \frac{dy}{dt}(t) - a^2 y(t) + 2 a x(t).
 \mathbf{s} = (dy/dt, y) n \geq 0 n","['integration', 'ordinary-differential-equations', 'control-theory', 'linear-control']"
69,ODE with inequality constraint,ODE with inequality constraint,,"I have the Lagrangian $L(u,u';z)$ where $D$ is the spacetime dimension, $m_0$ is some initial mass (constant), and $C$ is another constant. $$L = \frac{1}{z^{D-1}}\sqrt{-u'(z) \left( f(u,z) u'(z) + 2\right)}$$ where $$f(u,z) = 1 - m(u) z^{D}, \qquad m(u) = \left(\frac{D}{C u(z) + D\;m_0^{-1/D} }\right)^D, \qquad \frac{du}{dz} = u'$$ The Euler-Lagrange equation leads to a 2nd-order ODE (most likely nonlinear), $$\frac{d}{dz} \frac{\partial L}{\partial u'} = \frac{\partial L}{\partial u} \quad \rightarrow \quad \mathbb{2nd -order \: ODE}$$ with boundary conditions $u(0) = a$ and $u'(z_s) = \infty$ , where $a$ is some fixed number while $z_s$ is a parameter that I want to vary in the end to see how $u(z)$ changes. Typically, the boundary points of u(z) are specified at some known boundary $z$ points. Here, I only know for sure that one endpoint is at $z=0$ but the other is at $z=z_s$ for which $z_s$ can be varied to study how $u(z)$ changes, specifically I want to find which $z_s$ will give a minimum solution for $u(z)$ , i.e. you could say that the minimum out of the set of minimum solutions that came from the E-L equation for a set of specified $z_s$ . In addition to the 2nd-order ODE, there is also an inequality constraint, $$\frac{D}{C} \left( z_s - m_0^{-1/D}\right) > u(z)$$ I have scanned the web for some information about this including adding a slack and converting an inequality to an equality constraint. However, material is quite scarce when it comes to ODEs, or maybe I'm just ignorant of some things. When thinking about constraints, the Lagrange multiplier immediately comes to mind, however, for the E-L equation the multiplier method does not work for inequality constraints. My questions are, Suppose I choose a particular $z_s$ to completely fix the bc condition, how can I solve an ODE with inequality constraint? Is there a more general method that incorporates the variability of $z_s$ ? *I can work with Mathematica and have some basic understanding of say, the shooting method.","I have the Lagrangian where is the spacetime dimension, is some initial mass (constant), and is another constant. where The Euler-Lagrange equation leads to a 2nd-order ODE (most likely nonlinear), with boundary conditions and , where is some fixed number while is a parameter that I want to vary in the end to see how changes. Typically, the boundary points of u(z) are specified at some known boundary points. Here, I only know for sure that one endpoint is at but the other is at for which can be varied to study how changes, specifically I want to find which will give a minimum solution for , i.e. you could say that the minimum out of the set of minimum solutions that came from the E-L equation for a set of specified . In addition to the 2nd-order ODE, there is also an inequality constraint, I have scanned the web for some information about this including adding a slack and converting an inequality to an equality constraint. However, material is quite scarce when it comes to ODEs, or maybe I'm just ignorant of some things. When thinking about constraints, the Lagrange multiplier immediately comes to mind, however, for the E-L equation the multiplier method does not work for inequality constraints. My questions are, Suppose I choose a particular to completely fix the bc condition, how can I solve an ODE with inequality constraint? Is there a more general method that incorporates the variability of ? *I can work with Mathematica and have some basic understanding of say, the shooting method.","L(u,u';z) D m_0 C L = \frac{1}{z^{D-1}}\sqrt{-u'(z) \left( f(u,z) u'(z) + 2\right)} f(u,z) = 1 - m(u) z^{D}, \qquad m(u) = \left(\frac{D}{C u(z) + D\;m_0^{-1/D} }\right)^D, \qquad \frac{du}{dz} = u' \frac{d}{dz} \frac{\partial L}{\partial u'} = \frac{\partial L}{\partial u} \quad \rightarrow \quad \mathbb{2nd -order \: ODE} u(0) = a u'(z_s) = \infty a z_s u(z) z z=0 z=z_s z_s u(z) z_s u(z) z_s \frac{D}{C} \left( z_s - m_0^{-1/D}\right) > u(z) z_s z_s","['ordinary-differential-equations', 'boundary-value-problem', 'numerical-optimization']"
70,Hypothesis of Grönwall's inequality (Evans version),Hypothesis of Grönwall's inequality (Evans version),,"I hope this question has not alredy been asked: unfortunately there are many versions of this inequality and is difficult to find exactly the version I'm looking for. In Partial Differential Equations, Evans (Appendix B2), there is the proof of the following inequality: Grönwall's inequality: Let $\eta$ be a nonnegative, absolutely continuous function on $[0,T]$ , which satisfies: $$\eta'(t)\leq\phi(t)\eta(t)+\psi(t) \qquad a.e. \ t$$ where $\phi,\psi$ are nonnegative summable functions on $[0,T]$ , then: $$\eta(t)\leq e^{\int_0^t\phi(s)\ ds}\Big[\eta(0)+\int_0^t\psi(s) \ ds\Big] \qquad \forall t\in[0,T]$$ Proof: $$\frac{d}{ds}\Big[\eta(s)e^{-\int_0^s\phi(r)\ dr}\Big]=e^{-\int_0^s\phi(r)\ dr}[\eta'(s)-\phi(s)\eta(s)]\leq e^{-\int_0^s\phi(r)\ dr}\psi(s)$$ By integration on $[0,t]$ , with $t\in[0,T]$ , we have: $$\eta(t) e^{-\int_0^t\phi(r)\ dr}\leq \eta(0)+\int_0^t e^{-\int_0^s\phi(r)\ dr}\psi(s) \ ds\leq \eta(0)+\int_0^t\psi(s) \ ds$$ where the last inequality follows by the nonnegativeness of $\phi,\psi$ . I can't understand how the nonnegativeness of $\eta$ is used in the proof. Is this hypothesis not necessary? I need to use it for a positive $\eta$ so this hypothesis is not a problem. It is just to be sure that I am not missing something important in the proof of the inequality. Thank you","I hope this question has not alredy been asked: unfortunately there are many versions of this inequality and is difficult to find exactly the version I'm looking for. In Partial Differential Equations, Evans (Appendix B2), there is the proof of the following inequality: Grönwall's inequality: Let be a nonnegative, absolutely continuous function on , which satisfies: where are nonnegative summable functions on , then: Proof: By integration on , with , we have: where the last inequality follows by the nonnegativeness of . I can't understand how the nonnegativeness of is used in the proof. Is this hypothesis not necessary? I need to use it for a positive so this hypothesis is not a problem. It is just to be sure that I am not missing something important in the proof of the inequality. Thank you","\eta [0,T] \eta'(t)\leq\phi(t)\eta(t)+\psi(t) \qquad a.e. \ t \phi,\psi [0,T] \eta(t)\leq e^{\int_0^t\phi(s)\ ds}\Big[\eta(0)+\int_0^t\psi(s) \ ds\Big] \qquad \forall t\in[0,T] \frac{d}{ds}\Big[\eta(s)e^{-\int_0^s\phi(r)\ dr}\Big]=e^{-\int_0^s\phi(r)\ dr}[\eta'(s)-\phi(s)\eta(s)]\leq e^{-\int_0^s\phi(r)\ dr}\psi(s) [0,t] t\in[0,T] \eta(t) e^{-\int_0^t\phi(r)\ dr}\leq \eta(0)+\int_0^t e^{-\int_0^s\phi(r)\ dr}\psi(s) \ ds\leq \eta(0)+\int_0^t\psi(s) \ ds \phi,\psi \eta \eta","['real-analysis', 'ordinary-differential-equations', 'inequality', 'partial-differential-equations']"
71,"Solving the pde $f_x(x,y)+f_y(x,y)=0$ given $f(x,0)=\sin x$",Solving the pde  given,"f_x(x,y)+f_y(x,y)=0 f(x,0)=\sin x","Suppose, $f:\mathbb{R}^2\to\mathbb{R}$ is a differentiable function such that, $f_x(x,y)+f_y(x,y)=0$ for all $(x,y)\in \mathbb{R}^2.$ If $f(x,0)=\sin x$ then, find $f(0,y).$ A quick look reveals that $f(x,y)=\sin(x-y) $ is a solution, so $f(0,y)=-\sin y,$ but I got this only by inspection. I am not sure whether this is the only solution to the above pde, I am new to this area, so please help me.","Suppose, is a differentiable function such that, for all If then, find A quick look reveals that is a solution, so but I got this only by inspection. I am not sure whether this is the only solution to the above pde, I am new to this area, so please help me.","f:\mathbb{R}^2\to\mathbb{R} f_x(x,y)+f_y(x,y)=0 (x,y)\in \mathbb{R}^2. f(x,0)=\sin x f(0,y). f(x,y)=\sin(x-y)  f(0,y)=-\sin y,","['ordinary-differential-equations', 'partial-differential-equations']"
72,Can one prove a connection between the order of the polynomials in a differential equation and the roots of the characteristic equation?,Can one prove a connection between the order of the polynomials in a differential equation and the roots of the characteristic equation?,,"I am teaching myself how to solve differential equations and have now gone through a section on non-homogeneous second order linear equations. I have the following problem to solve, which I have been stuck on for a few days: Assume that the differential equation $ y''+ay'+by=p(x)e^{kx}$ has a solution $q(x)e^{kx}$ , where both $p(x)$ and $q(x)$ are polynomials. Prove the following: If k is not a root of the characteristic equation, then the degree of q equals the degree of p If k is a simple root of the characteristic equation, then the degree of q is one higher than the degree of p If k is a double root of the characteristic equation, then the degree of q is two higher than the degree of p I have tried substituding y for a guessed solution of the form $y=ze^{kx}$ where z is a polynomial and expanded the differential equation, but this has brought me nowhere. I honestly have no idea how to go forward with this. Can anyone think of a strategy?","I am teaching myself how to solve differential equations and have now gone through a section on non-homogeneous second order linear equations. I have the following problem to solve, which I have been stuck on for a few days: Assume that the differential equation has a solution , where both and are polynomials. Prove the following: If k is not a root of the characteristic equation, then the degree of q equals the degree of p If k is a simple root of the characteristic equation, then the degree of q is one higher than the degree of p If k is a double root of the characteristic equation, then the degree of q is two higher than the degree of p I have tried substituding y for a guessed solution of the form where z is a polynomial and expanded the differential equation, but this has brought me nowhere. I honestly have no idea how to go forward with this. Can anyone think of a strategy?", y''+ay'+by=p(x)e^{kx} q(x)e^{kx} p(x) q(x) y=ze^{kx},['ordinary-differential-equations']
73,Is this shock/rarefaction problem solved correctly?,Is this shock/rarefaction problem solved correctly?,,"So I'm coursing a subject in PDE and the theory for these kind of problems is quite scarce, and I don't seem to find many solved problems online. The problem is stated as follows: Analyse the following shock-fitting problem. Draw the solution u(x, t) for several times. \begin{equation}  \begin{cases}   u_t + uu_x = 0, & t>0, \ \ x\in \mathbb{R} \\   u(x, 0) =    \begin{cases}      1, \\     -1, \\      0,     \end{cases}    &     \begin{aligned}     x < 0 \\     0 < x < 1 \\     x> 1    \end{aligned}  \end{cases} \end{equation} Notation: The equation can be written as $u_t + c(u)u_x = 0$ where $c(u) = u$ . Solution: Let $\Phi(x) := u(x,0)$ . We can rewrite the system as: \begin{cases} \frac{dX}{dt} = U, & X(0) = \xi \\ \frac{dU}{dt} = 0, & U(0) = \phi(\xi) \end{cases} From here, we see that \begin{equation} U(t) = U(0) = \Phi(\xi) =  \begin{cases}  1, &  \xi < 0 \\ -1, & 0 < \xi < 1 \\ 0, & \xi > 1 \end{cases} \end{equation} So, it's easy to figure out \begin{equation} X'(t) = \Phi(\xi) \Rightarrow X(t) = \Phi(\xi)t + \xi =  \begin{cases}  2t + \xi, &  \xi < 0 \\ -2t + \xi, & 0 < \xi < 1 \\ \xi, & \xi > 1 \end{cases} \end{equation} Now, the case for $\xi = 0$ . We introduce the flux $q$ and identify $q'(u) = c(u) = u$ , so $q(u) = \frac{u^2}{2}. We then can find the shock wave \begin{equation}  \frac{ds}{dt} = \frac{[q]}{[u]} = \frac{1}{2} \cdot \frac{1^2-(-1)^2}{1-(-1)} = 0 \Rightarrow s(t) = 0 \end{equation} For the case $\xi = 1$ \begin{equation}  c(u) = c^{-1}(u) = u \Rightarrow u(x,t) = \frac{x-1}{t} \end{equation} The final solution then would be \begin{equation} u(x,t) =  \begin{cases} 1, & x<0 \\ -1, & 0<x<1 \\ \frac{x-1}{t}, & 1<x<t \\ 0, & x \geq t \end{cases} \end{equation} I feel very insecure about this answer. If it is indeed right, I'd also appreciate to know why . Thanks in advance. EDIT: Also, are there any resources where I can learn about this kind of problems?","So I'm coursing a subject in PDE and the theory for these kind of problems is quite scarce, and I don't seem to find many solved problems online. The problem is stated as follows: Analyse the following shock-fitting problem. Draw the solution u(x, t) for several times. Notation: The equation can be written as where . Solution: Let . We can rewrite the system as: From here, we see that So, it's easy to figure out Now, the case for . We introduce the flux and identify , so $q(u) = \frac{u^2}{2}. We then can find the shock wave For the case The final solution then would be I feel very insecure about this answer. If it is indeed right, I'd also appreciate to know why . Thanks in advance. EDIT: Also, are there any resources where I can learn about this kind of problems?","\begin{equation}
 \begin{cases}
  u_t + uu_x = 0, & t>0, \ \ x\in \mathbb{R} \\
  u(x, 0) =
   \begin{cases}
     1, \\
    -1, \\
     0, 
   \end{cases}
   & 
   \begin{aligned}
    x < 0 \\
    0 < x < 1 \\
    x> 1
   \end{aligned}
 \end{cases}
\end{equation} u_t + c(u)u_x = 0 c(u) = u \Phi(x) := u(x,0) \begin{cases}
\frac{dX}{dt} = U, & X(0) = \xi \\
\frac{dU}{dt} = 0, & U(0) = \phi(\xi)
\end{cases} \begin{equation}
U(t) = U(0) = \Phi(\xi) = 
\begin{cases} 
1, &  \xi < 0 \\
-1, & 0 < \xi < 1 \\
0, & \xi > 1
\end{cases}
\end{equation} \begin{equation}
X'(t) = \Phi(\xi) \Rightarrow X(t) = \Phi(\xi)t + \xi = 
\begin{cases} 
2t + \xi, &  \xi < 0 \\
-2t + \xi, & 0 < \xi < 1 \\
\xi, & \xi > 1
\end{cases}
\end{equation} \xi = 0 q q'(u) = c(u) = u \begin{equation}
 \frac{ds}{dt} = \frac{[q]}{[u]} = \frac{1}{2} \cdot \frac{1^2-(-1)^2}{1-(-1)} = 0 \Rightarrow s(t) = 0
\end{equation} \xi = 1 \begin{equation}
 c(u) = c^{-1}(u) = u \Rightarrow u(x,t) = \frac{x-1}{t}
\end{equation} \begin{equation}
u(x,t) = 
\begin{cases}
1, & x<0 \\
-1, & 0<x<1 \\
\frac{x-1}{t}, & 1<x<t \\
0, & x \geq t
\end{cases}
\end{equation}","['real-analysis', 'functional-analysis', 'ordinary-differential-equations', 'partial-differential-equations', 'wave-equation']"
74,Determine if the following autonomous differential equation has periodic solutions,Determine if the following autonomous differential equation has periodic solutions,,"Let $\begin{cases} x'=-y\\ y'=x+y^{2} \end{cases}$ be an autonomous system. Determine whether or not is has periodic solutions. Hint: Consider the differential equations of the orbit: $\frac{dy}{dx}=-\frac{x}{y}-y$ I just started learning about about autonomous systems, and I have no idea how to approach this. I assume I need to use Poincare-Bendixon but I don't see how to use the hint (or even understand it). Bendixon's criteria doesn't help either. Any help would be much appreciated.","Let be an autonomous system. Determine whether or not is has periodic solutions. Hint: Consider the differential equations of the orbit: I just started learning about about autonomous systems, and I have no idea how to approach this. I assume I need to use Poincare-Bendixon but I don't see how to use the hint (or even understand it). Bendixon's criteria doesn't help either. Any help would be much appreciated.","\begin{cases}
x'=-y\\
y'=x+y^{2}
\end{cases} \frac{dy}{dx}=-\frac{x}{y}-y","['real-analysis', 'ordinary-differential-equations', 'analysis', 'dynamical-systems']"
75,What is wrong with the equations I have set?,What is wrong with the equations I have set?,,"I have been learning calculus on Khan Academy, and came to differential calculus on BC Calculus. The problem says: ""Each month the balance B on Harper's loan increases by 22% and decreases by 250 dollars"". We have to find the equation that describes the relationship best. No problem there. It is logical that: $\frac{\partial B}{\partial t}= 0.22B - 250$ But, also I think that this is correct too: $B_{t} = 1.22B_{t-1} - 250$ however taking derivative of $B^t$ I don't think we get $\frac{\partial B}{\partial t}$ like above, but $1.22B^{'}$ , which are not the same. What am I missing?","I have been learning calculus on Khan Academy, and came to differential calculus on BC Calculus. The problem says: ""Each month the balance B on Harper's loan increases by 22% and decreases by 250 dollars"". We have to find the equation that describes the relationship best. No problem there. It is logical that: But, also I think that this is correct too: however taking derivative of I don't think we get like above, but , which are not the same. What am I missing?",\frac{\partial B}{\partial t}= 0.22B - 250 B_{t} = 1.22B_{t-1} - 250 B^t \frac{\partial B}{\partial t} 1.22B^{'},"['calculus', 'ordinary-differential-equations']"
76,Geodesics on a pseudosphere,Geodesics on a pseudosphere,,"I am trying to show the following: Let \begin{equation} \gamma(t) = \begin{pmatrix} \frac{1}{t} \\ \sqrt{1 - \frac{1}{t^2}} + \cosh^{-1}(t) \end{pmatrix} \end{equation} for $t \geq 1$ be the unit-speed parametrization of the tractrix and \begin{equation} \sigma(u,v) = \begin{pmatrix} \frac{1}{u} \cos v \\ \frac{1}{u} \sin v \\ \sqrt{1 - \frac{1}{u^2}} + \cosh^{-1}(u) \end{pmatrix} \end{equation} the surface of revolution obtained by rotating the tractrix around the z-axis. Now I want to show that the geodesics on the pseudosphere (tractroid in this case) are represented in the $(u,v)$ -coordinates by arcs of circles centered on the $v$ -axis. The first thing that I noted is that the curve is not unit-speed parametrized. So I calculated the coefficients of the first fundamental form (for a surface patches) \begin{align*}   E(u,v) &= \dot{f}^2(u) + \dot{g}^2(u) = \frac{3 + u^2}{u^2(u^2 - 1)}, \\   F(u,v) &= \langle \sigma_u(u,v), \sigma_v(u,v) \rangle = 0, \\   G(u,v) &= \frac{1}{u^2} = f^2(u) \end{align*} and now the geodesic equations are \begin{align*} \frac{d}{dt}\left( E \dot{u} + F \dot{v} \right)  &= \frac{1}{2}\left( E_u \dot{u}^2 + 2 F_u \dot{u} \dot{v} + G_u \dot{v}^2 \right), \\ \frac{d}{dt}\left( F \dot{u} + G \dot{v} \right) &= \frac{1}{2}\left( E_v \dot{u}^2 + 2 F_v \dot{u} \dot{v} + G_v \dot{v}^2 \right) \end{align*} therefore \begin{align*}  \frac{d}{dt} \left(\frac{3 + u^2}{u^2(u^2 - 1)}  \dot{u} \right) &= -\frac{4 u \dot{u}^2 }{(u^2 - 1)^2} - \frac{\dot{v}^2 - 3 \dot{u}^2}{u^3}, \\ \frac{d}{dt}\left( \frac{1}{u^2}\dot{v} \right) &= 0 \end{align*} solving the second gives \begin{equation*}   \dot{v} = C u^2 \end{equation*} which can also be obtained by clairauts theorem where $\psi$ is the angle between $\dot{\gamma}$ and the meridians of the surface \begin{equation*}   \dot{v} = \frac{\sin \psi}{f} = u \sin \psi \end{equation*} and $f \sin \psi$ being constant gives \begin{equation*}   f^2 \dot{v} = \frac{1}{u^2} \dot{v} = f \sin \psi \stackrel{!}{=} \mbox{const}. \end{equation*} Plugging this $\dot{v}$ into the first geodesic equation leaves us with \begin{equation*}   \frac{d}{dt} \left(\frac{3 + u^2}{u^2(u^2 - 1)}  \dot{u} \right) = \frac{\dot{u}\left(3 - \frac{4 u^4}{(u^2 - 1)^2} \right)}{u^3} - C^2 u. \end{equation*} From this I can't figure out how the geodesics are arcs of circles centered on the $v$ -axis. So I tried using the fact that \begin{equation} I(\dot{\gamma},\dot{\gamma}) \stackrel{!}{=} 1 \end{equation} which gives \begin{equation} E \dot{u}^2 +  G \dot{v}^2 = 1 \end{equation} or \begin{equation} \dot{u}^2 \frac{3 + u^2}{u^2 - 1} +  C^2 u^4 = u^2 \end{equation} thus \begin{equation} \dot{u} = \pm \frac{\sqrt{u^2(u^2C^2 - 1)}}{\sqrt{ \frac{3 + u^2}{1 - u^2} }}. \end{equation} From this I would get an expression for $\frac{\dot{v}}{\dot{u}}$ hence by separation of variables $(v - v_0) = (\ldots)$ where the right hand side involves an elliptic integral. I would expect that from $\frac{\dot{v}}{\dot{u}} = F(u)$ where $F$ is some function depending only on $u$ , I can use separation of variables to get something like \begin{equation} (v-v_0)^2 + u^2 = \frac{1}{C} \end{equation} but from the geodesic equation and the ODE it is not possible to obtain such a form. If I would switch the sign from $\cosh^{-1}(t)$ to $-\cosh^{-1}(t)$ this would work. I would be thankful if anybody can give me a hint.","I am trying to show the following: Let for be the unit-speed parametrization of the tractrix and the surface of revolution obtained by rotating the tractrix around the z-axis. Now I want to show that the geodesics on the pseudosphere (tractroid in this case) are represented in the -coordinates by arcs of circles centered on the -axis. The first thing that I noted is that the curve is not unit-speed parametrized. So I calculated the coefficients of the first fundamental form (for a surface patches) and now the geodesic equations are therefore solving the second gives which can also be obtained by clairauts theorem where is the angle between and the meridians of the surface and being constant gives Plugging this into the first geodesic equation leaves us with From this I can't figure out how the geodesics are arcs of circles centered on the -axis. So I tried using the fact that which gives or thus From this I would get an expression for hence by separation of variables where the right hand side involves an elliptic integral. I would expect that from where is some function depending only on , I can use separation of variables to get something like but from the geodesic equation and the ODE it is not possible to obtain such a form. If I would switch the sign from to this would work. I would be thankful if anybody can give me a hint.","\begin{equation}
\gamma(t) = \begin{pmatrix}
\frac{1}{t} \\
\sqrt{1 - \frac{1}{t^2}} + \cosh^{-1}(t)
\end{pmatrix}
\end{equation} t \geq 1 \begin{equation}
\sigma(u,v) = \begin{pmatrix}
\frac{1}{u} \cos v \\
\frac{1}{u} \sin v \\
\sqrt{1 - \frac{1}{u^2}} + \cosh^{-1}(u)
\end{pmatrix}
\end{equation} (u,v) v \begin{align*}
  E(u,v) &= \dot{f}^2(u) + \dot{g}^2(u) = \frac{3 + u^2}{u^2(u^2 - 1)}, \\
  F(u,v) &= \langle \sigma_u(u,v), \sigma_v(u,v) \rangle = 0, \\
  G(u,v) &= \frac{1}{u^2} = f^2(u)
\end{align*} \begin{align*}
\frac{d}{dt}\left( E \dot{u} + F \dot{v} \right)  &= \frac{1}{2}\left( E_u \dot{u}^2 + 2 F_u \dot{u} \dot{v} + G_u \dot{v}^2 \right), \\
\frac{d}{dt}\left( F \dot{u} + G \dot{v} \right) &= \frac{1}{2}\left( E_v \dot{u}^2 + 2 F_v \dot{u} \dot{v} + G_v \dot{v}^2 \right)
\end{align*} \begin{align*}
 \frac{d}{dt} \left(\frac{3 + u^2}{u^2(u^2 - 1)}  \dot{u} \right) &= -\frac{4 u \dot{u}^2 }{(u^2 - 1)^2} - \frac{\dot{v}^2 - 3 \dot{u}^2}{u^3}, \\
\frac{d}{dt}\left( \frac{1}{u^2}\dot{v} \right) &= 0
\end{align*} \begin{equation*}
  \dot{v} = C u^2
\end{equation*} \psi \dot{\gamma} \begin{equation*}
  \dot{v} = \frac{\sin \psi}{f} = u \sin \psi
\end{equation*} f \sin \psi \begin{equation*}
  f^2 \dot{v} = \frac{1}{u^2} \dot{v} = f \sin \psi \stackrel{!}{=} \mbox{const}.
\end{equation*} \dot{v} \begin{equation*}
  \frac{d}{dt} \left(\frac{3 + u^2}{u^2(u^2 - 1)}  \dot{u} \right) = \frac{\dot{u}\left(3 - \frac{4 u^4}{(u^2 - 1)^2} \right)}{u^3} - C^2 u.
\end{equation*} v \begin{equation}
I(\dot{\gamma},\dot{\gamma}) \stackrel{!}{=} 1
\end{equation} \begin{equation}
E \dot{u}^2 +  G \dot{v}^2 = 1
\end{equation} \begin{equation}
\dot{u}^2 \frac{3 + u^2}{u^2 - 1} +  C^2 u^4 = u^2
\end{equation} \begin{equation}
\dot{u} = \pm \frac{\sqrt{u^2(u^2C^2 - 1)}}{\sqrt{ \frac{3 + u^2}{1 - u^2} }}.
\end{equation} \frac{\dot{v}}{\dot{u}} (v - v_0) = (\ldots) \frac{\dot{v}}{\dot{u}} = F(u) F u \begin{equation}
(v-v_0)^2 + u^2 = \frac{1}{C}
\end{equation} \cosh^{-1}(t) -\cosh^{-1}(t)","['real-analysis', 'ordinary-differential-equations', 'differential-geometry']"
77,Infinite variation of Brownian motion and Continuity,Infinite variation of Brownian motion and Continuity,,"Let $C>0$ be a constant. Brownian motion is Hölder continuous for $\alpha=1/2$ : $$| B(t+h) - B(t) | \leq C \sqrt{h \log(1/h)} \leq C h^\alpha,$$ for every sufficiently small $h$ . But Brownian motion is of unbounded variation: $$\sup \sum_{j}^k |B(t_j) - B(t_{j-1})| < \infty.$$ Consider a stochastic differential equation $$dX_t = X_t dt + \alpha B_t,$$ where $\alpha>0$ and $B_t$ is Brownian motion. The Euler-Maruyama simulation gives $$X_{i+1} = X_{i} + X_{i}\Delta t + \alpha \cdot (B_{t_{i+1}} - B_{t_i}).$$ Is the diffusion term of the simulated equation $X_{i+1}$ then bounded w.r.t the Hölder continuity although Brownian motion is of infinite variation? Can we define an upper bound for the increment $B_{t_{i+1}} - B_{t_i}$ ?",Let be a constant. Brownian motion is Hölder continuous for : for every sufficiently small . But Brownian motion is of unbounded variation: Consider a stochastic differential equation where and is Brownian motion. The Euler-Maruyama simulation gives Is the diffusion term of the simulated equation then bounded w.r.t the Hölder continuity although Brownian motion is of infinite variation? Can we define an upper bound for the increment ?,"C>0 \alpha=1/2 | B(t+h) - B(t) | \leq C \sqrt{h \log(1/h)} \leq C h^\alpha, h \sup \sum_{j}^k |B(t_j) - B(t_{j-1})| < \infty. dX_t = X_t dt + \alpha B_t, \alpha>0 B_t X_{i+1} = X_{i} + X_{i}\Delta t + \alpha \cdot (B_{t_{i+1}} - B_{t_i}). X_{i+1} B_{t_{i+1}} - B_{t_i}","['linear-algebra', 'ordinary-differential-equations', 'stochastic-processes', 'stochastic-differential-equations']"
78,Finding basis for the differential equation,Finding basis for the differential equation,,"So the question is to find a basis for the solution of following differential equation that and when $y(0)=3,y'(0)=2$ what is the solution $y'' + 2y' + y = 0$ So my attempt was to put it as auxiliary polynomial,which $p(t) = t^2 +2t +1$ $=(t+1)^2$ Hence zeros are -1 and -1, so that $\{e^{-t},te^{-t}\}$ So the solution is that $y(t)= b_1e^{-t} + b_2te^{-t}$ $y(0)= b_1e^{-0} + b_2(0)e^{-0}$ $=b_1 = 3$ $y'(t)= -b_1e^{-t} + b_2e^{-t} - b_2te^{-t}$ $=-b_1e^{-t} + b_2(1-t)e^{-t}$ $y'(0)= -b_1e^{-0} + b_2(1-0)e^{-0}$ $=b_2-b_1=2$ , which $b_2 = 5$ Hence, the solution $y(t)= 3e^{-t} + 5te^{-t}$ Am I doing it right? New to linear Algebra","So the question is to find a basis for the solution of following differential equation that and when what is the solution So my attempt was to put it as auxiliary polynomial,which Hence zeros are -1 and -1, so that So the solution is that , which Hence, the solution Am I doing it right? New to linear Algebra","y(0)=3,y'(0)=2 y'' + 2y' + y = 0 p(t) = t^2 +2t +1 =(t+1)^2 \{e^{-t},te^{-t}\} y(t)= b_1e^{-t} + b_2te^{-t} y(0)= b_1e^{-0} + b_2(0)e^{-0} =b_1 = 3 y'(t)= -b_1e^{-t} + b_2e^{-t} - b_2te^{-t} =-b_1e^{-t} + b_2(1-t)e^{-t} y'(0)= -b_1e^{-0} + b_2(1-0)e^{-0} =b_2-b_1=2 b_2 = 5 y(t)= 3e^{-t} + 5te^{-t}","['ordinary-differential-equations', 'solution-verification']"
79,Importance of finitude in the Poincaré-Bendixson theorem,Importance of finitude in the Poincaré-Bendixson theorem,,"I have a question regarding the Poincaré-Bendixson theorem: I understand everything, but I'm looking for an example that shows the importance of the finitude of the singular points within the theorem. What happens when we have an infinite number (countable or uncountable) of singular points? Until now I have only been able to find examples that teach me or show me how this theorem works but I would like to see an example about the importance of the finitude of said singular points.","I have a question regarding the Poincaré-Bendixson theorem: I understand everything, but I'm looking for an example that shows the importance of the finitude of the singular points within the theorem. What happens when we have an infinite number (countable or uncountable) of singular points? Until now I have only been able to find examples that teach me or show me how this theorem works but I would like to see an example about the importance of the finitude of said singular points.",,"['ordinary-differential-equations', 'dynamical-systems']"
80,Solve $y’’ – 4y’ + 5y = 4e^{2x}\sin(x)$ using $\mathcal D$ operator,Solve  using  operator,y’’ – 4y’ + 5y = 4e^{2x}\sin(x) \mathcal D,"Hello – I am working through the following question and get stuck at step 6. Could someone please advise in simple terms which I can hopefully understand. Thanks $$y'' – 4y' + 5y = 4e^{2x}\sin(x)$$ Step one – Order equation so that differential operator is in front of the RHS of the equation $\newcommand{\D}{\mathcal D}$ $$1 = \frac 1 {\D^2 – 4\D + 5}  \cdot 4e^{2x}\sin(x)$$ Step two – move constant and exponential in front of the $\D$ operator $$1 = 4e^{2x}\cdot \frac 1 {\D^2 – 4\D + 5}\cdot \sin(x)$$ Step three – calculate $a$ Because of $e^{2x}$ , $a = 2$ , and because of $\sin(x)$ , $a = 2 + i$ . Step four – insert $a$ into the $\D$ operator and then calculate to see if it equals zero \begin{align} 1 &= 4e^{2x}\cdot \frac 1 {(2+i)^2 – 4(2+i) + 5}\cdot \sin(x) \\ &= 4e^{2x}\cdot \frac 1 {(4+4i+4-8-4i+5)}\cdot\sin(x) \\ &= 4e^{2x}\cdot \frac 1 {(0)}\cdot \sin(x) \end{align} Step 5 – because there is a zero, note $a = 2$ therefore make it $\D+2$ \begin{align} 1 &= 4e^{2x}\cdot \frac 1 {(\D + 2)^2 – 4(\D + 2) + 5}\cdot \sin(x) \\ &= 4e^{2x}\cdot\frac 1 {\D^2 + 1}\cdot\sin(x) \end{align} What do I do for step 6? Please explain in simply terms and assume my calculus knowledge is low.","Hello – I am working through the following question and get stuck at step 6. Could someone please advise in simple terms which I can hopefully understand. Thanks Step one – Order equation so that differential operator is in front of the RHS of the equation Step two – move constant and exponential in front of the operator Step three – calculate Because of , , and because of , . Step four – insert into the operator and then calculate to see if it equals zero Step 5 – because there is a zero, note therefore make it What do I do for step 6? Please explain in simply terms and assume my calculus knowledge is low.","y'' – 4y' + 5y = 4e^{2x}\sin(x) \newcommand{\D}{\mathcal D} 1 = \frac 1 {\D^2 – 4\D + 5}  \cdot 4e^{2x}\sin(x) \D 1 = 4e^{2x}\cdot \frac 1 {\D^2 – 4\D + 5}\cdot \sin(x) a e^{2x} a = 2 \sin(x) a = 2 + i a \D \begin{align}
1 &= 4e^{2x}\cdot \frac 1 {(2+i)^2 – 4(2+i) + 5}\cdot \sin(x)
\\
&= 4e^{2x}\cdot \frac 1 {(4+4i+4-8-4i+5)}\cdot\sin(x)
\\
&= 4e^{2x}\cdot \frac 1 {(0)}\cdot \sin(x)
\end{align} a = 2 \D+2 \begin{align}
1 &= 4e^{2x}\cdot \frac 1 {(\D + 2)^2 – 4(\D + 2) + 5}\cdot \sin(x)
\\
&= 4e^{2x}\cdot\frac 1 {\D^2 + 1}\cdot\sin(x)
\end{align}","['calculus', 'ordinary-differential-equations']"
81,Series solution ODE,Series solution ODE,,"I am trying to solve the ODE $y''+y=0$ using a series solution. Here is what I have done so far. Assume the solution is of the form $y(x)=\sum_{k=0}^\infty a_kx^k$ . Then the derivatives are $y'(x)=\sum_{k=1}^\infty ka_kx^{k-1}$ and $y''(x)=\sum_{k=2}^\infty k(k-1)a_{k}x^{k-2}$ I then combine these into the series $$\sum_{n=0}^\infty [(n+2)(n+1)a_{n+2}+a_n]x^n = 0$$ Would I then say that $\forall n\in \mathbb{N}$ $(n+2)(n+1)a_{n+2}+a_n=0$ and solve for $a_n$ ? My question is: If a power series equals zero, do all the terms have to necessarily be zero?","I am trying to solve the ODE using a series solution. Here is what I have done so far. Assume the solution is of the form . Then the derivatives are and I then combine these into the series Would I then say that and solve for ? My question is: If a power series equals zero, do all the terms have to necessarily be zero?",y''+y=0 y(x)=\sum_{k=0}^\infty a_kx^k y'(x)=\sum_{k=1}^\infty ka_kx^{k-1} y''(x)=\sum_{k=2}^\infty k(k-1)a_{k}x^{k-2} \sum_{n=0}^\infty [(n+2)(n+1)a_{n+2}+a_n]x^n = 0 \forall n\in \mathbb{N} (n+2)(n+1)a_{n+2}+a_n=0 a_n,"['calculus', 'ordinary-differential-equations', 'analysis', 'power-series']"
82,Geometrical insights on differential equations,Geometrical insights on differential equations,,"Hi: I am researching about relationships between Differential Geometry and Differential Equations. I am looking for examples and references of the use of geometric concepts to solve or analyze differential equations . For example, in Differential Equations With Applications and Historical Notes the author relates the solution to the Brachistochrone with the Snell law and the behavior of light, providing a very valuable intuition of the form of the solution. I know a lot of geometric concepts are used in this field: orbits, symmetries... I am looking for especially creative or illuminating examples, especially if they had historical significancy. For example, in this question a geometric non-obvious intuition for an important theorem in statistics is provided. Another example is the counterexample to Poincaré-Bendixson Theorem counterexample in the torus, where the geometry of the torus is used to construct non-periodic yet recurrent orbits. Examples and references of relationships in the other direction are also welcome: for example, Picard–Lindelöf theorem can be used to prove every spatial curve is uniquely determined (up to rigid movement) by its curvature and torsion. Thanks in advance.","Hi: I am researching about relationships between Differential Geometry and Differential Equations. I am looking for examples and references of the use of geometric concepts to solve or analyze differential equations . For example, in Differential Equations With Applications and Historical Notes the author relates the solution to the Brachistochrone with the Snell law and the behavior of light, providing a very valuable intuition of the form of the solution. I know a lot of geometric concepts are used in this field: orbits, symmetries... I am looking for especially creative or illuminating examples, especially if they had historical significancy. For example, in this question a geometric non-obvious intuition for an important theorem in statistics is provided. Another example is the counterexample to Poincaré-Bendixson Theorem counterexample in the torus, where the geometry of the torus is used to construct non-periodic yet recurrent orbits. Examples and references of relationships in the other direction are also welcome: for example, Picard–Lindelöf theorem can be used to prove every spatial curve is uniquely determined (up to rigid movement) by its curvature and torsion. Thanks in advance.",,"['ordinary-differential-equations', 'differential-geometry', 'partial-differential-equations', 'soft-question', 'differential-topology']"
83,Find the solutions of : $(x^2-2)x^2y''-(x^2+2)xy'+(x^2+2)y=0$,Find the solutions of :,(x^2-2)x^2y''-(x^2+2)xy'+(x^2+2)y=0,"Find the solutions of : $(x^2-2)x^2y''-(x^2+2)xy'+(x^2+2)y=0$ I have to find infinite series form solutions, $y=\sum a_nx^{n+r}.$ I got the indicial equation : $r^2-1=0 \implies r_1=-1, r_2=1$ I got the general formula while $r=1$ is $a_{k+2}=\frac{k^2}{2(k+2)^2+4(k+2)}a_k$ Then , one solution is $y_2(x)=x^1\sum a_kx^k$ when I get $a_k$ using $a_{k+2}=\frac{k^2}{2(k+2)^2+4(k+2)} , k>0$ . How can I found the other solution when $r=-1$ ? Thanks !","Find the solutions of : I have to find infinite series form solutions, I got the indicial equation : I got the general formula while is Then , one solution is when I get using . How can I found the other solution when ? Thanks !","(x^2-2)x^2y''-(x^2+2)xy'+(x^2+2)y=0 y=\sum a_nx^{n+r}. r^2-1=0 \implies r_1=-1, r_2=1 r=1 a_{k+2}=\frac{k^2}{2(k+2)^2+4(k+2)}a_k y_2(x)=x^1\sum a_kx^k a_k a_{k+2}=\frac{k^2}{2(k+2)^2+4(k+2)} , k>0 r=-1",['ordinary-differential-equations']
84,Construct a function $f$ with $f'-af$ is odd (a>0),Construct a function  with  is odd (a>0),f f'-af,"Let $a>0$ , I am trying to construct a function $f$ such that $f'-af$ is odd. i.e \begin{align*} f'(-x)-af(-x)=-f'(x)+af(x) \end{align*} By direct computation, we have \begin{align*} \frac{d}{dx}(f(x)-f(-x))+a(f(x)-f(-x))=0 \end{align*} Solving the ODE, I pick $f(x)-f(-x)=-e^{ax}$ . i.e \begin{align*} f(x)=-e^{ax}+f(-x) \end{align*} But I got stuck here. Any help would be appreciated.","Let , I am trying to construct a function such that is odd. i.e By direct computation, we have Solving the ODE, I pick . i.e But I got stuck here. Any help would be appreciated.","a>0 f f'-af \begin{align*}
f'(-x)-af(-x)=-f'(x)+af(x)
\end{align*} \begin{align*}
\frac{d}{dx}(f(x)-f(-x))+a(f(x)-f(-x))=0
\end{align*} f(x)-f(-x)=-e^{ax} \begin{align*}
f(x)=-e^{ax}+f(-x)
\end{align*}","['ordinary-differential-equations', 'even-and-odd-functions']"
85,Determine affine linear f $\in C^1(R)$ such that the differential form is exact,Determine affine linear f  such that the differential form is exact,\in C^1(R),"Determine affine linear function $f$ such that the differential form $\omega$ is exact, where $$ \omega (x,y) = (6x^2y+2y^3+2yf(x^2+y^2))dx + (3y^2-2xf(x^2+y^2))dy. $$ I started the exercise but at some point I can't continue. I calculated the partial derivatives and imposed them to be equal. $$ \begin{split} \frac{\partial}{\partial y}F_1  & = 6x^2+6y^2+2f(x^2+y^2)+4y^2f'(x^2+y^2) \\ & = -2f(x^2+y^2)-4x^2f'(x^2+y^2) = \frac{\partial}{\partial x}F_2 \end{split} $$ Doing the math I found $$ 6(x^2+y^2)+4f(x^2+y^2)+4(x^2+y^2)f'(x^2+y^2)= 0 $$ The problem is in solving this differential equation. I have placed $x^2+y^2 = s$ so $$ 6(s)+4f(s)+4(s)f'(s)= 0 $$ can someone help me?","Determine affine linear function such that the differential form is exact, where I started the exercise but at some point I can't continue. I calculated the partial derivatives and imposed them to be equal. Doing the math I found The problem is in solving this differential equation. I have placed so can someone help me?","f \omega 
\omega (x,y) = (6x^2y+2y^3+2yf(x^2+y^2))dx + (3y^2-2xf(x^2+y^2))dy.
 
\begin{split}
\frac{\partial}{\partial y}F_1  & = 6x^2+6y^2+2f(x^2+y^2)+4y^2f'(x^2+y^2) \\
& = -2f(x^2+y^2)-4x^2f'(x^2+y^2) = \frac{\partial}{\partial x}F_2
\end{split}
 
6(x^2+y^2)+4f(x^2+y^2)+4(x^2+y^2)f'(x^2+y^2)= 0
 x^2+y^2 = s 
6(s)+4f(s)+4(s)f'(s)= 0
","['real-analysis', 'ordinary-differential-equations', 'differential-forms']"
86,Give a differential equation to which the 2-parameter family is a family of solutions,Give a differential equation to which the 2-parameter family is a family of solutions,,The given 2-parameter family is $$ y= e^{5x}(A+B\sin(2x))$$ Here's what I have so far: $$y'=e^{5x}(5B\sin(2x)+2B\cos(2x)+5A)$$ $$ y''=e^{5x}(21B\sin(2x)+20B\cos(2x)+25A)$$ $$10\cdot 2Be^{5x}\cos(2x)=10y'-10e^{5x}(5B\sin(2x)+5A)=10y'-50y$$ I'm trying to do the same with $y''$ but I get stuck because they have no common factor: $$20Be^{5x}\cos(2x)=y''-e^{5x}(21B\sin(2x)+25A)$$ How do I proceed?,The given 2-parameter family is Here's what I have so far: I'm trying to do the same with but I get stuck because they have no common factor: How do I proceed?, y= e^{5x}(A+B\sin(2x)) y'=e^{5x}(5B\sin(2x)+2B\cos(2x)+5A)  y''=e^{5x}(21B\sin(2x)+20B\cos(2x)+25A) 10\cdot 2Be^{5x}\cos(2x)=10y'-10e^{5x}(5B\sin(2x)+5A)=10y'-50y y'' 20Be^{5x}\cos(2x)=y''-e^{5x}(21B\sin(2x)+25A),[]
87,Contradictory results from using orthogonality when solving an ODE with Fourier series.,Contradictory results from using orthogonality when solving an ODE with Fourier series.,,"I am trying to solve the following ordinary differential equation by assuming that the function $f(x)$ is in the form of a Fourier series, in the bounds $-L < x < L$ . $$ \frac{\mathrm{d}f}{\mathrm{d}x} = f(x) \qquad f(x) = \frac{A_0}{2} + \sum_{n=1}^\infty \left[A_n\cos\left(\frac{n\pi x}{L}\right) + B_n\sin\left(\frac{n\pi x}{L}\right)\right] $$ My solution involves first determining the derivative $\frac{\mathrm{d}f}{\mathrm{d}x}$ from the assumed Fourier series form of the solution and then substituting into the original equation. $$ \frac{\pi}{L}\sum_{n=1}^\infty n\left[-A_n\sin\left(\frac{n\pi x}{L}\right) + B_n\cos\left(\frac{n\pi x}{L}\right)\right] = \frac{A_0}{2} + \sum_{n=1}^\infty \left[A_n\cos\left(\frac{n\pi x}{L}\right) + B_n\sin\left(\frac{n\pi x}{L}\right)\right] $$ From here I apply orthogonality with $\cos\left(\frac{n\pi x}{L}\right)$ and $\sin\left(\frac{n\pi x}{L}\right)$ with weight function $r(x) = 1$ between $-L < x < L$ , the necessary integral solutions shown below. $$ \int_{-L}^L\!\cos\left(\frac{n\pi x}{L}\right)\cos\left(\frac{m\pi x}{L}\right)\,\mathrm{d}x = \delta_{nm}L \qquad \int_{-L}^L\!\sin\left(\frac{n\pi x}{L}\right)\sin\left(\frac{m\pi x}{L}\right)\,\mathrm{d}x = \delta_{nm}L $$ $$ \int_{-L}^L\!\sin\left(\frac{n\pi x}{L}\right)\cos\left(\frac{m\pi x}{L}\right)\,\mathrm{d}x = 0 \qquad \int_{-L}^L\!\cos\left(\frac{n\pi x}{L}\right)\sin\left(\frac{m\pi x}{L}\right)\,\mathrm{d}x = 0 $$ $$ \int_{-L}^L\!\cos\left(\frac{n\pi x}{L}\right)\,\mathrm{d}x = 0 \qquad \int_{-L}^L\!\sin\left(\frac{n\pi x}{L}\right)\,\mathrm{d}x = 0 $$ Applying these orthogonality conditions to the original equation $$ \text{orthogonality with} \quad \cos\left(\frac{n\pi x}{L}\right), \quad \frac{\pi}{L}\sum_{n=1}^\infty n\left[0 + L\delta_{nm}B_n\right] = 0 + \sum_{n=1}^\infty\left[L\delta_{nm}A_n + 0\right] $$ $$ n\pi B_n = LA_n \qquad B_n = \frac{L}{n\pi}A_n $$ $$ \text{orthogonality with} \quad \sin\left(\frac{n\pi x}{L}\right), \quad \frac{\pi}{L}\sum_{n=1}^\infty n\left[-L\delta_{nm}A_n + 0\right] = 0 + \sum_{n=1}^\infty\left[0 + L\delta_{nm}B_n\right] $$ $$ -n\pi A_n = LB_n \qquad B_n = -\frac{n \pi}{L}A_n $$ These results are contradictory, or to be more specific orthogonality with the cosine seems to give an incorrect result, since derivation of the Fourier series coefficients for the solution to this differential equation, $f(x) = Ae^x$ , shows that $B_n = -\frac{n \pi}{L}A_n$ . Have I made a mistake in my working or am I not understanding something about orthogonality? I may have made some mistakes in typing this up, I have double checked everything but I cannot be sure. I have however done this derivation through many times with pen and paper and always come to these contradictory results.","I am trying to solve the following ordinary differential equation by assuming that the function is in the form of a Fourier series, in the bounds . My solution involves first determining the derivative from the assumed Fourier series form of the solution and then substituting into the original equation. From here I apply orthogonality with and with weight function between , the necessary integral solutions shown below. Applying these orthogonality conditions to the original equation These results are contradictory, or to be more specific orthogonality with the cosine seems to give an incorrect result, since derivation of the Fourier series coefficients for the solution to this differential equation, , shows that . Have I made a mistake in my working or am I not understanding something about orthogonality? I may have made some mistakes in typing this up, I have double checked everything but I cannot be sure. I have however done this derivation through many times with pen and paper and always come to these contradictory results.","f(x) -L < x < L  \frac{\mathrm{d}f}{\mathrm{d}x} = f(x) \qquad f(x) = \frac{A_0}{2} + \sum_{n=1}^\infty \left[A_n\cos\left(\frac{n\pi x}{L}\right) + B_n\sin\left(\frac{n\pi x}{L}\right)\right]  \frac{\mathrm{d}f}{\mathrm{d}x}  \frac{\pi}{L}\sum_{n=1}^\infty n\left[-A_n\sin\left(\frac{n\pi x}{L}\right) + B_n\cos\left(\frac{n\pi x}{L}\right)\right] = \frac{A_0}{2} + \sum_{n=1}^\infty \left[A_n\cos\left(\frac{n\pi x}{L}\right) + B_n\sin\left(\frac{n\pi x}{L}\right)\right]  \cos\left(\frac{n\pi x}{L}\right) \sin\left(\frac{n\pi x}{L}\right) r(x) = 1 -L < x < L  \int_{-L}^L\!\cos\left(\frac{n\pi x}{L}\right)\cos\left(\frac{m\pi x}{L}\right)\,\mathrm{d}x = \delta_{nm}L \qquad \int_{-L}^L\!\sin\left(\frac{n\pi x}{L}\right)\sin\left(\frac{m\pi x}{L}\right)\,\mathrm{d}x = \delta_{nm}L   \int_{-L}^L\!\sin\left(\frac{n\pi x}{L}\right)\cos\left(\frac{m\pi x}{L}\right)\,\mathrm{d}x = 0 \qquad \int_{-L}^L\!\cos\left(\frac{n\pi x}{L}\right)\sin\left(\frac{m\pi x}{L}\right)\,\mathrm{d}x = 0   \int_{-L}^L\!\cos\left(\frac{n\pi x}{L}\right)\,\mathrm{d}x = 0 \qquad \int_{-L}^L\!\sin\left(\frac{n\pi x}{L}\right)\,\mathrm{d}x = 0   \text{orthogonality with} \quad \cos\left(\frac{n\pi x}{L}\right), \quad \frac{\pi}{L}\sum_{n=1}^\infty n\left[0 + L\delta_{nm}B_n\right] = 0 + \sum_{n=1}^\infty\left[L\delta_{nm}A_n + 0\right]   n\pi B_n = LA_n \qquad B_n = \frac{L}{n\pi}A_n   \text{orthogonality with} \quad \sin\left(\frac{n\pi x}{L}\right), \quad \frac{\pi}{L}\sum_{n=1}^\infty n\left[-L\delta_{nm}A_n + 0\right] = 0 + \sum_{n=1}^\infty\left[0 + L\delta_{nm}B_n\right]   -n\pi A_n = LB_n \qquad B_n = -\frac{n \pi}{L}A_n  f(x) = Ae^x B_n = -\frac{n \pi}{L}A_n","['ordinary-differential-equations', 'fourier-analysis', 'fourier-series']"
88,"Truncation error, finite differences","Truncation error, finite differences",,"Consider the following FDM problem: Find $u$ such that $$ -u^{\prime \prime}(x)+b(x) u^{\prime}(x)+c(x) u(x)=f(x) ~~\text { in }(0,1), $$ and conditions $u(0) = u(1) = 0$ , where $$ b(x)=x^{2}, \qquad c(x)=1+x, \qquad f(x)=-2+13 x^{2}+3 x^{3}-x^{4}-5 x^{5}. $$ I am trying to find an upper bound on the local truncation error using finite difference method (forward, backward, and centered). So far, I have the following: Approximate $u\left(x_{i}\right)$ respectively by $U_{i}, V_{i}$ , and $W_{i}$ , where $U_{i}$ is the solution of the finite-difference scheme: $$ \frac{1}{h^{2}}\left[-U_{i-1}+2 U_{i}-U_{i+1}\right]+\frac{b_{i}}{h}\left[U_{i}-U_{i-1}\right]+c_{i} U_{i}=f_{i},  \tag3$$ $V_{i}$ is the solution of $$ \frac{1}{h^{2}}\left[-V_{i-1}+2 V_{i}-V_{i+1}\right]+\frac{b_{i}}{h}\left[V_{i+1}-V_{i}\right]+c_{i} V_{i}=f_{i},  \tag4$$ and $W_{i}$ is the solution of $$ \frac{1}{h^{2}}\left[-W_{i-1}+2 W_{i}-W_{i+1}\right]+\frac{b_{i}}{2 h}\left[W_{i+1}-W_{i-1}\right]+c_{i} W_{i}=f_{i},  \tag5$$ with $U_{0}=V_{0}=W_{0}=U_{N+1}=V_{N+1}=W_{N+1}=0$ . I know that I have to use Taylor's formula, but don't know how find the upper bound on the local truncation error for (3), (4), and (5), or even how to derive the truncation error. I am also trying to find the order. (3) has order O(h2), and the other two have order O(h) - is this correct? Can anyone help me here?","Consider the following FDM problem: Find such that and conditions , where I am trying to find an upper bound on the local truncation error using finite difference method (forward, backward, and centered). So far, I have the following: Approximate respectively by , and , where is the solution of the finite-difference scheme: is the solution of and is the solution of with . I know that I have to use Taylor's formula, but don't know how find the upper bound on the local truncation error for (3), (4), and (5), or even how to derive the truncation error. I am also trying to find the order. (3) has order O(h2), and the other two have order O(h) - is this correct? Can anyone help me here?","u  -u^{\prime \prime}(x)+b(x) u^{\prime}(x)+c(x) u(x)=f(x) ~~\text { in }(0,1),  u(0) = u(1) = 0  b(x)=x^{2}, \qquad c(x)=1+x, \qquad f(x)=-2+13 x^{2}+3 x^{3}-x^{4}-5 x^{5}.  u\left(x_{i}\right) U_{i}, V_{i} W_{i} U_{i} 
\frac{1}{h^{2}}\left[-U_{i-1}+2 U_{i}-U_{i+1}\right]+\frac{b_{i}}{h}\left[U_{i}-U_{i-1}\right]+c_{i} U_{i}=f_{i},
 \tag3 V_{i} 
\frac{1}{h^{2}}\left[-V_{i-1}+2 V_{i}-V_{i+1}\right]+\frac{b_{i}}{h}\left[V_{i+1}-V_{i}\right]+c_{i} V_{i}=f_{i},
 \tag4 W_{i} 
\frac{1}{h^{2}}\left[-W_{i-1}+2 W_{i}-W_{i+1}\right]+\frac{b_{i}}{2 h}\left[W_{i+1}-W_{i-1}\right]+c_{i} W_{i}=f_{i},
 \tag5 U_{0}=V_{0}=W_{0}=U_{N+1}=V_{N+1}=W_{N+1}=0","['ordinary-differential-equations', 'derivatives', 'numerical-methods', 'finite-differences', 'finite-difference-methods']"
89,Is there an intuition behind the Euler Lagrange equation?,Is there an intuition behind the Euler Lagrange equation?,,I am taking calculus of variations at the moment and I am curious if there is a visualization of why the EL must be satisfied for all extremals. At first glance it's hard for me to relate: $$\frac{d}{dx}(\frac{\partial L}{\partial y'}) = \frac{\partial L}{\partial y}$$ With a geometric/visual intuition.,I am taking calculus of variations at the moment and I am curious if there is a visualization of why the EL must be satisfied for all extremals. At first glance it's hard for me to relate: With a geometric/visual intuition.,\frac{d}{dx}(\frac{\partial L}{\partial y'}) = \frac{\partial L}{\partial y},"['calculus', 'ordinary-differential-equations', 'calculus-of-variations', 'euler-lagrange-equation']"
90,Can 2 different ODE's have the same set of solutions?,Can 2 different ODE's have the same set of solutions?,,"If I have two differents linear ODE's: \begin{equation} x'' + p(t)x' + q(t)x = f(t) . \quad p,q \in C(I,\infty). \\ x'' + j(t)x' + g(t)x = h(t). \quad j,g \in C(I,\infty). \end{equation} Coul they have exactly the same set of solutions? And if we have two different non-linear ODE's could they?",If I have two differents linear ODE's: Coul they have exactly the same set of solutions? And if we have two different non-linear ODE's could they?,"\begin{equation} x'' + p(t)x' + q(t)x = f(t) . \quad p,q \in C(I,\infty). \\ x'' + j(t)x' + g(t)x = h(t). \quad j,g \in C(I,\infty).
\end{equation}","['real-analysis', 'ordinary-differential-equations']"
91,Euclidean norm of a solution of matrix differential equation,Euclidean norm of a solution of matrix differential equation,,"I try to solve a problem I have found in a book where I'm asked to find the solution to a differential equation of the form $$ x'=Ax $$ where $A$ is a matrix. The answer is the exponential of $e^{At}$ . Then I'm stuck with the second request that is to prove that $\|x(t)\|_2$ is increasing (as a function of $t$ ) by considering $A+A^T$ . What information can give $A+A^T$ ? I'm not posting the expression for $A$ because I'm looking for a general answer. Thanks in advance for any suggestion. Thanks to the suggestion given, one can observe that the square of the norm is increasing if and only if the norm is increasing but the transpose of $x(t)$ is $e^{A^Tt}$ . So $$ \langle x(t),x^T(t)\rangle=\langle e^{At},e^{A^Tt}\rangle=e^{(A^T+A)t} $$ and if I prove that $A^T+A$ is positive definite I have concluded. It's all correct?","I try to solve a problem I have found in a book where I'm asked to find the solution to a differential equation of the form where is a matrix. The answer is the exponential of . Then I'm stuck with the second request that is to prove that is increasing (as a function of ) by considering . What information can give ? I'm not posting the expression for because I'm looking for a general answer. Thanks in advance for any suggestion. Thanks to the suggestion given, one can observe that the square of the norm is increasing if and only if the norm is increasing but the transpose of is . So and if I prove that is positive definite I have concluded. It's all correct?","
x'=Ax
 A e^{At} \|x(t)\|_2 t A+A^T A+A^T A x(t) e^{A^Tt} 
\langle x(t),x^T(t)\rangle=\langle e^{At},e^{A^Tt}\rangle=e^{(A^T+A)t}
 A^T+A","['linear-algebra', 'ordinary-differential-equations', 'systems-of-equations']"
92,Difficulty in evaluating $\lim_{t\to x}\frac{t^2f(x)-x^2f(t)}{t-x}$,Difficulty in evaluating,\lim_{t\to x}\frac{t^2f(x)-x^2f(t)}{t-x},"We have the differentiable function $f(x)$ , on interval $(0,\infty)$ such that $f(1)=1$ and $\lim_{t\to x}\frac{t^2f(x)-x^2f(t)}{t-x}=1\,\,\forall x>0$ , so, we need to find $f(x)$ . Applying L'Hopital's rule and substituting $t\rightarrow x$ quickly simplified the equation to: $$2xf(x)-x^2f'(x)=1$$ Now, this equation is probably some type of differential equation, and further, I can't solve it and am stuck here. I haven't studied much of differential equations so far. Please help in solving this equation or suggest some other method. Thanks in advance.","We have the differentiable function , on interval such that and , so, we need to find . Applying L'Hopital's rule and substituting quickly simplified the equation to: Now, this equation is probably some type of differential equation, and further, I can't solve it and am stuck here. I haven't studied much of differential equations so far. Please help in solving this equation or suggest some other method. Thanks in advance.","f(x) (0,\infty) f(1)=1 \lim_{t\to x}\frac{t^2f(x)-x^2f(t)}{t-x}=1\,\,\forall x>0 f(x) t\rightarrow x 2xf(x)-x^2f'(x)=1","['calculus', 'ordinary-differential-equations', 'limits']"
93,Convert ODE to a form of Bessel differential equation,Convert ODE to a form of Bessel differential equation,,"I'm working on the solution of the equation $$\tan^2u\partial^2_u y_2 + (2+\tan^2u)\tan u \partial_u y_2 -a^2\lambda_2y_2 - n^2(1+\cot^2u)y_2 = 0.$$ It is possible to write the above equation in terms of the Sturm-Liouville operator by just multiplying the equation by $\cos u$ : \begin{align*} \sin u \tan u \partial^2_u y_2 + (\cos u \tan u + \sec^2 u \sin u)\partial_u y_2 - \cos u(a^2\lambda_2 + n^2(1+\cot^2u))y_2 = 0\\ \implies \partial_u (\sin u \tan u \partial_u y_2) - \cos u (a^2\lambda_2 + n^2(1+\cot^2u))y_2 = 0. \end{align*} Now, let $\eta = \sin u$ , hence, $$ \frac{\partial}{\partial u} = \frac{\partial\eta}{\partial u}\frac{\partial}{\partial\eta} = \cos u \frac{\partial}{\partial\eta}. $$ The last equation becomes \begin{align*} \cos u \partial_\eta \left(\eta^2\partial_\eta y_2\right) - \cos u (a^2\lambda_2y_2 + n^2(1+\cot^2u))y_2 &= 0\\ \implies \left(\eta^2\partial_\eta y_2\right) -\left(a^2\lambda_2 + n^2\left(1+\frac{\cos^2 u}{\sin^2 u}\right)\right)y_2 &= 0, \end{align*} but, since $\sin u = \eta,$ it implies that $\cos u = \sqrt{1-\eta^2}$ by trigonometric relations. Thus, \begin{align*} \partial_\eta \left(\eta^2\partial_\eta y_2\right) -\left(a^2\lambda_2 + n^2\left(1+\frac{1-\eta^2}{\eta^2}\right)\right)y_2 &= 0\\ \implies \partial_\eta \left(\eta^2\partial_\eta y_2\right) -\left(a^2\lambda_2 +\frac{n^2}{\eta^2}\right)y_2 &= 0 \end{align*} Now, the problem is reduced to solve this last ODE, which I can see something similar to the associated Legendre ODE or Bessel ODE. The last one seems to be more plausible when we apply the change of variables $x=n/\eta$ . By making this change, the equation becomes $$ x^2\partial^2_x y - (a^2\lambda_2 + x^2)y = 0$$ I want to reduce the equation to a known one to avoid the Frobenius method, but if it is not possible, I'm ok with that. If possible, how can I write this last equation as a Bessel-type ODE? Any help/idea of substitution will be very appreciated. Thanks in advance","I'm working on the solution of the equation It is possible to write the above equation in terms of the Sturm-Liouville operator by just multiplying the equation by : Now, let , hence, The last equation becomes but, since it implies that by trigonometric relations. Thus, Now, the problem is reduced to solve this last ODE, which I can see something similar to the associated Legendre ODE or Bessel ODE. The last one seems to be more plausible when we apply the change of variables . By making this change, the equation becomes I want to reduce the equation to a known one to avoid the Frobenius method, but if it is not possible, I'm ok with that. If possible, how can I write this last equation as a Bessel-type ODE? Any help/idea of substitution will be very appreciated. Thanks in advance","\tan^2u\partial^2_u y_2 + (2+\tan^2u)\tan u \partial_u y_2 -a^2\lambda_2y_2 - n^2(1+\cot^2u)y_2 = 0. \cos u \begin{align*}
\sin u \tan u \partial^2_u y_2 + (\cos u \tan u + \sec^2 u \sin u)\partial_u y_2 - \cos u(a^2\lambda_2 + n^2(1+\cot^2u))y_2 = 0\\
\implies \partial_u (\sin u \tan u \partial_u y_2) - \cos u (a^2\lambda_2 + n^2(1+\cot^2u))y_2 = 0.
\end{align*} \eta = \sin u 
\frac{\partial}{\partial u} = \frac{\partial\eta}{\partial u}\frac{\partial}{\partial\eta} = \cos u \frac{\partial}{\partial\eta}.
 \begin{align*}
\cos u \partial_\eta \left(\eta^2\partial_\eta y_2\right) - \cos u (a^2\lambda_2y_2 + n^2(1+\cot^2u))y_2 &= 0\\
\implies \left(\eta^2\partial_\eta y_2\right) -\left(a^2\lambda_2 + n^2\left(1+\frac{\cos^2 u}{\sin^2 u}\right)\right)y_2 &= 0,
\end{align*} \sin u = \eta, \cos u = \sqrt{1-\eta^2} \begin{align*}
\partial_\eta \left(\eta^2\partial_\eta y_2\right) -\left(a^2\lambda_2 + n^2\left(1+\frac{1-\eta^2}{\eta^2}\right)\right)y_2 &= 0\\
\implies \partial_\eta \left(\eta^2\partial_\eta y_2\right) -\left(a^2\lambda_2 +\frac{n^2}{\eta^2}\right)y_2 &= 0
\end{align*} x=n/\eta  x^2\partial^2_x y - (a^2\lambda_2 + x^2)y = 0","['ordinary-differential-equations', 'mathematical-physics', 'bessel-functions', 'legendre-polynomials']"
94,How to solve $0=\dot{y}-\dot{y}^2-y-t^2$?,How to solve ?,0=\dot{y}-\dot{y}^2-y-t^2,"I was messing around with some differential equations(long story involving a game of snake, a game of minesweeper, chemical bonds, a basketball, and a friend) when I made a mistake and got the equation $0=\dot{y}-\dot{y}^2-y-t^2$ . However, this seemed like a much more interesting equation than what I should've got but I couldn't figure out how to solve this which brought me here. Could someone give me a hint or point me towards some reading or techniques that could help me solve it?  Thanks. I apologize if I violated any customs of MSE or rules, I'm not familiar.","I was messing around with some differential equations(long story involving a game of snake, a game of minesweeper, chemical bonds, a basketball, and a friend) when I made a mistake and got the equation . However, this seemed like a much more interesting equation than what I should've got but I couldn't figure out how to solve this which brought me here. Could someone give me a hint or point me towards some reading or techniques that could help me solve it?  Thanks. I apologize if I violated any customs of MSE or rules, I'm not familiar.",0=\dot{y}-\dot{y}^2-y-t^2,['ordinary-differential-equations']
95,Solving a nonhomogenous system of eqns with one eigenvalue,Solving a nonhomogenous system of eqns with one eigenvalue,,"I have the system: $\left[\begin{array}{@{}c@{}}     x' \\            y'     \end{array} \right]= \left[\begin{array}{@{}c@{}}     3&2 \\     -2 & -1      \end{array} \right]\left[\begin{array}{@{}c@{}}     x' \\            y'     \end{array} \right]+\left[\begin{array}{@{}c@{}}     2e^{-t} \\           e^{-t}     \end{array} \right]$ Which I should solve using the fundamental matrix. So I start with obtaining the homogenous solution: I find the eigenvalues; \begin{pmatrix}     3-\lambda&2 \\     -2 & -1-\lambda  \end{pmatrix} which gives the determinant: $\lambda^2-2\lambda+1=0$ . Thus $\lambda_1=1$ . Plugging that in the matrix in the original equation, I get that x=y. So a solution to the homogenous system would be: $y_h=e^{t}\left[\begin{array}{@{}c@{}}     1 \\           1     \end{array} \right]$ Since there is no second solution to the determinant, I would ideally form the fundamental matrix: \begin{pmatrix}  e^{t} & e^0 \\     e^{t} & e^0  \end{pmatrix} but this is to no avail. So how do I find the solution of this nonhomogenous system using the fundamental matrix with one eigenvalue? Thanks UPDATE: I set up the generalized eigenvector formula \begin{equation} v_2(A-\lambda I)=v_2 \begin{pmatrix}      3-\lambda&2 \\     -2 & -1-\lambda  \end{pmatrix}=v_1 \end{equation} \begin{equation} v_2(A-\lambda I)=v_1= \begin{vmatrix}      3-\lambda&2 & | 1 \\     -2 & -1-\lambda & |-1   \end{vmatrix} \end{equation} I now get as given by Moo, with Gaussian elimination, the matrix: \begin{equation} \begin{vmatrix}      1 &1 & | 1/2 \\     0 & 0 & |0   \end{vmatrix} \end{equation} and have the second eigenvector: $e_2=e^{t}\left[\begin{array}{@{}c@{}}     \frac{1}{2} \\           0     \end{array} \right]$ . So the homogeneous solution is: \begin{equation} y_h=e^{\lambda_1 t}e_1+e^{\lambda_2t}e_2=e^{t}\left[\begin{array}{@{}c@{}}     1 \\           -1     \end{array} \right]+e^{t}\left[\begin{array}{@{}c@{}}     \frac{1}{2} \\           0     \end{array} \right] \end{equation} At this stage, it remains to find the particular solution. We know that it must be in the form of: \begin{equation} y_p=Ce^{-t} \end{equation} and thus the general solution is: \begin{equation} y_p=y_h+Ce^{-t}=e^{t}\left[\begin{array}{@{}c@{}}     1 \\           -1     \end{array} \right]+e^{t}\left[\begin{array}{@{}c@{}}     \frac{1}{2} \\           0     \end{array} \right]+Ce^{-t} \end{equation} But can this be said?","I have the system: Which I should solve using the fundamental matrix. So I start with obtaining the homogenous solution: I find the eigenvalues; which gives the determinant: . Thus . Plugging that in the matrix in the original equation, I get that x=y. So a solution to the homogenous system would be: Since there is no second solution to the determinant, I would ideally form the fundamental matrix: but this is to no avail. So how do I find the solution of this nonhomogenous system using the fundamental matrix with one eigenvalue? Thanks UPDATE: I set up the generalized eigenvector formula I now get as given by Moo, with Gaussian elimination, the matrix: and have the second eigenvector: . So the homogeneous solution is: At this stage, it remains to find the particular solution. We know that it must be in the form of: and thus the general solution is: But can this be said?","\left[\begin{array}{@{}c@{}}
    x' \\
           y'
    \end{array} \right]= \left[\begin{array}{@{}c@{}}
    3&2 \\
    -2 & -1 
    \end{array} \right]\left[\begin{array}{@{}c@{}}
    x' \\
           y'
    \end{array} \right]+\left[\begin{array}{@{}c@{}}
    2e^{-t} \\
          e^{-t}
    \end{array} \right] \begin{pmatrix}
    3-\lambda&2 \\
    -2 & -1-\lambda 
\end{pmatrix} \lambda^2-2\lambda+1=0 \lambda_1=1 y_h=e^{t}\left[\begin{array}{@{}c@{}}
    1 \\
          1
    \end{array} \right] \begin{pmatrix}
 e^{t} & e^0 \\
    e^{t} & e^0 
\end{pmatrix} \begin{equation}
v_2(A-\lambda I)=v_2
\begin{pmatrix}
     3-\lambda&2 \\
    -2 & -1-\lambda 
\end{pmatrix}=v_1
\end{equation} \begin{equation}
v_2(A-\lambda I)=v_1=
\begin{vmatrix}
     3-\lambda&2 & | 1 \\
    -2 & -1-\lambda & |-1  
\end{vmatrix}
\end{equation} \begin{equation}
\begin{vmatrix}
     1 &1 & | 1/2 \\
    0 & 0 & |0  
\end{vmatrix}
\end{equation} e_2=e^{t}\left[\begin{array}{@{}c@{}}
    \frac{1}{2} \\
          0
    \end{array} \right] \begin{equation}
y_h=e^{\lambda_1 t}e_1+e^{\lambda_2t}e_2=e^{t}\left[\begin{array}{@{}c@{}}
    1 \\
          -1
    \end{array} \right]+e^{t}\left[\begin{array}{@{}c@{}}
    \frac{1}{2} \\
          0
    \end{array} \right]
\end{equation} \begin{equation}
y_p=Ce^{-t}
\end{equation} \begin{equation}
y_p=y_h+Ce^{-t}=e^{t}\left[\begin{array}{@{}c@{}}
    1 \\
          -1
    \end{array} \right]+e^{t}\left[\begin{array}{@{}c@{}}
    \frac{1}{2} \\
          0
    \end{array} \right]+Ce^{-t}
\end{equation}","['ordinary-differential-equations', 'eigenvalues-eigenvectors', 'fundamental-solution']"
96,How does this expression follow algebraically from the last one? (continued),How does this expression follow algebraically from the last one? (continued),,"Continuing from here: How does this expression follow algebraically from the last one? The ""new"" system: \begin{align*} \dot S &= \Lambda - (\beta_1 S I_2 +\beta_2 S J+ \beta_3 S A )-\mu S \\ \dot I_1 &= p\beta_1 S I_2  +q\beta_2 S J +r \beta_3 S A +\xi_1 J -b_1 I_1\\ \dot I_2 &= (1-P)\beta_1 S I_2  +(1-q)\beta_2 S J+(1-r) \beta_3 S A +\epsilon I_1 +\xi_2 J -b_2 I_2\\ \dot J &= p_1 I_2 -b_3 J\\ \dot A &= p_2 J - b_4 A \end{align*} Reproduction number: \begin{align} \mathcal{R}_0 &= \frac{\Lambda\left[ \beta_1 b_3 b_4 \left( \epsilon p +b_1(\left( 1-p\right)\right)+\beta_2 p_1 b_4 \left( \epsilon q +b_1(\left( 1-q\right)\right)+\beta_3 p_1 p_2 \left( \epsilon r +b_1(\left( 1-r\right)\right) \right]}{\mu b_4 \left[ b_1 b_2 b_3 - p_1\left( \epsilon \xi_1 +b_1 \xi_2 \right) \right] } \end{align} Equilibrium point: \begin{align} S^* &= \frac{\Lambda}{\mu \mathcal{R}_0}\\[2ex] I_1^*  &= \frac{1}{b_1}\left[ p \beta_1 \frac{\Lambda b_3 b_4}{\left(\beta_1 b_3 b_4+\beta_2 p_1 b_4+\beta_3 p_1 p_2 \right)J^* +\mu b_4 p_1}\right.\\[1ex] &\quad\;\;+\left. q \beta_2 \frac{\Lambda b_4 p_1}{\left(\beta_1 b_3 b_4+\beta_2 p_1 b_4+\beta_3 p_1 p_2 \right)J^* +\mu b_4 p_1}\right.\\[1ex] &\quad\;\;+\left. r \beta_3 \frac{\Lambda p_1 p_2}{\left(\beta_1 b_3 b_4+\beta_2 p_1 b_4+\beta_3 p_1 p_2 \right)J^* +\mu b_4 p_1}+\xi_1\right]J^*\\[2ex] I_2^* &= \frac{b_3}{p_1}J^*\\[2ex] J^*&= \frac{\mu b_4  p_1 }{\beta_1 b_3 b_4 +\beta_2 p_1 b_4 +\beta_3 p_1 p_2}\left(\mathcal{R}_0-1\right)\\[2ex] A^*&=\frac{p_2}{b_4}J^* \end{align} Theorem: If $p=q=r$ and $\mathcal{R}_0 >1$ then the above equilibrium point is globally stable. To prove this: Define the following Lyapunov function \begin{align} V &= S-S^* \ln S+ B\left( I_1-{I_1}^* \ln I_1\right) +C\left(I_2-{I_2}^* \ln I_2\right) + D\left( J -J^* \ln J\right)\\ &+E\left(A-A^* \ln A\right). \end{align} The derivative of $V$ \begin{align*} \dot V &=\left(1-\frac{S^*}{S}\right) \dot S + B\left(1-\frac{{I_1}^*}{I_1} \right)\dot I_1  + C\left(1-\frac{{I_2}^*}{I_2}\right)\dot I_2+D\left(1-\frac{J^*}{J} \right)\dot J\\[1ex]  &+ E\left(1-\frac{A^*}{A}\right) \dot A \end{align*} where: \begin{align*} B &= \frac{\epsilon}{\epsilon p +b_1(1-p)}\\[1ex] C&= \frac{b_1}{\epsilon p +b_1(1-p)}\\[1ex] D &= \frac{b_1 b_2}{p_1[\epsilon p +b_1(1-p)]} -\frac{\beta_1 S^*}{p_1}\\[1ex] E &= \frac{\beta_3 S^*}{b4} \end{align*} We get to the following: \begin{align*} \dot V &= -\mu S^* \frac{\left(1-x\right)^2}{x}+ \left[ \beta_1 S^* {I_2}^*+\beta_2 S^* J^*+  \beta_3 S^* A^* +B p\beta_1 S^* {I_2}^* + B q \beta_2 S^* J^*\right.\\[1ex] &+\left. B r \beta_3 S^* A^*+ B \xi_1 J^*+ C(1-p)\beta_1 S^* {I_2}^*+C(1-q)\beta_2 S^* J^*+C(1-r)\beta_3 S^* A^*\right.\\[1ex] &+\left. C \epsilon {I_1}^*+C \xi_2 J^*+D p_1 {I_2}^*+ E p_2 J^* \right]-x\left[ C(1-p)\beta_1 S^* {I_2}^* \right] -\frac{xz}{y}B p\beta_1 S^* {I_2}^*\\[1ex] & -\frac{xu}{y}B q \beta_2 S^* J^* -\frac{xv}{y}B r \beta_3 S^* A^*-\frac{u}{y}B \xi_1 J^* - \frac{xu}{z}C(1-q)\beta_2 S^* J^*\\[1ex] &- \frac{xv}{z}C(1-r)\beta_3 S^* A^* - \frac{y}{z}C \epsilon {I_1}^* - \frac{u}{z}C \xi_2 J^*-\frac{z}{u}D p_1  {I_2}^*-\frac{u}{v}E p_2 J^*\\[1ex] &- \frac{1}{x}\left[\beta_1 S^* {I_2}^*+ \beta_2 S^* J^*+\beta_3 S^* A^*\right]\\[2ex] \end{align*} How do we get this expression to the 'final form' like in the question I referred at the beginning of this question?","Continuing from here: How does this expression follow algebraically from the last one? The ""new"" system: Reproduction number: Equilibrium point: Theorem: If and then the above equilibrium point is globally stable. To prove this: Define the following Lyapunov function The derivative of where: We get to the following: How do we get this expression to the 'final form' like in the question I referred at the beginning of this question?","\begin{align*}
\dot S &= \Lambda - (\beta_1 S I_2 +\beta_2 S J+ \beta_3 S A )-\mu S \\
\dot I_1 &= p\beta_1 S I_2  +q\beta_2 S J +r \beta_3 S A +\xi_1 J -b_1 I_1\\
\dot I_2 &= (1-P)\beta_1 S I_2  +(1-q)\beta_2 S J+(1-r) \beta_3 S A +\epsilon I_1 +\xi_2 J -b_2 I_2\\
\dot J &= p_1 I_2 -b_3 J\\
\dot A &= p_2 J - b_4 A
\end{align*} \begin{align}
\mathcal{R}_0 &= \frac{\Lambda\left[ \beta_1 b_3 b_4 \left( \epsilon p +b_1(\left( 1-p\right)\right)+\beta_2 p_1 b_4 \left( \epsilon q +b_1(\left( 1-q\right)\right)+\beta_3 p_1 p_2 \left( \epsilon r +b_1(\left( 1-r\right)\right) \right]}{\mu b_4 \left[ b_1 b_2 b_3 - p_1\left( \epsilon \xi_1 +b_1 \xi_2 \right) \right] }
\end{align} \begin{align}
S^* &= \frac{\Lambda}{\mu \mathcal{R}_0}\\[2ex]
I_1^*  &= \frac{1}{b_1}\left[ p \beta_1 \frac{\Lambda b_3 b_4}{\left(\beta_1 b_3 b_4+\beta_2 p_1 b_4+\beta_3 p_1 p_2 \right)J^* +\mu b_4 p_1}\right.\\[1ex]
&\quad\;\;+\left. q \beta_2 \frac{\Lambda b_4 p_1}{\left(\beta_1 b_3 b_4+\beta_2 p_1 b_4+\beta_3 p_1 p_2 \right)J^* +\mu b_4 p_1}\right.\\[1ex]
&\quad\;\;+\left. r \beta_3 \frac{\Lambda p_1 p_2}{\left(\beta_1 b_3 b_4+\beta_2 p_1 b_4+\beta_3 p_1 p_2 \right)J^* +\mu b_4 p_1}+\xi_1\right]J^*\\[2ex]
I_2^* &= \frac{b_3}{p_1}J^*\\[2ex]
J^*&= \frac{\mu b_4  p_1 }{\beta_1 b_3 b_4 +\beta_2 p_1 b_4 +\beta_3 p_1 p_2}\left(\mathcal{R}_0-1\right)\\[2ex]
A^*&=\frac{p_2}{b_4}J^*
\end{align} p=q=r \mathcal{R}_0 >1 \begin{align}
V &= S-S^* \ln S+ B\left( I_1-{I_1}^* \ln I_1\right) +C\left(I_2-{I_2}^* \ln I_2\right) + D\left( J -J^* \ln J\right)\\
&+E\left(A-A^* \ln A\right).
\end{align} V \begin{align*}
\dot V &=\left(1-\frac{S^*}{S}\right) \dot S + B\left(1-\frac{{I_1}^*}{I_1} \right)\dot I_1  + C\left(1-\frac{{I_2}^*}{I_2}\right)\dot I_2+D\left(1-\frac{J^*}{J} \right)\dot J\\[1ex] 
&+ E\left(1-\frac{A^*}{A}\right) \dot A
\end{align*} \begin{align*}
B &= \frac{\epsilon}{\epsilon p +b_1(1-p)}\\[1ex]
C&= \frac{b_1}{\epsilon p +b_1(1-p)}\\[1ex]
D &= \frac{b_1 b_2}{p_1[\epsilon p +b_1(1-p)]} -\frac{\beta_1 S^*}{p_1}\\[1ex]
E &= \frac{\beta_3 S^*}{b4}
\end{align*} \begin{align*}
\dot V &= -\mu S^* \frac{\left(1-x\right)^2}{x}+ \left[ \beta_1 S^* {I_2}^*+\beta_2 S^* J^*+  \beta_3 S^* A^* +B p\beta_1 S^* {I_2}^* + B q \beta_2 S^* J^*\right.\\[1ex]
&+\left. B r \beta_3 S^* A^*+ B \xi_1 J^*+ C(1-p)\beta_1 S^* {I_2}^*+C(1-q)\beta_2 S^* J^*+C(1-r)\beta_3 S^* A^*\right.\\[1ex]
&+\left. C \epsilon {I_1}^*+C \xi_2 J^*+D p_1 {I_2}^*+ E p_2 J^* \right]-x\left[ C(1-p)\beta_1 S^* {I_2}^* \right] -\frac{xz}{y}B p\beta_1 S^* {I_2}^*\\[1ex]
& -\frac{xu}{y}B q \beta_2 S^* J^* -\frac{xv}{y}B r \beta_3 S^* A^*-\frac{u}{y}B \xi_1 J^* - \frac{xu}{z}C(1-q)\beta_2 S^* J^*\\[1ex]
&- \frac{xv}{z}C(1-r)\beta_3 S^* A^* - \frac{y}{z}C \epsilon {I_1}^* - \frac{u}{z}C \xi_2 J^*-\frac{z}{u}D p_1  {I_2}^*-\frac{u}{v}E p_2 J^*\\[1ex]
&- \frac{1}{x}\left[\beta_1 S^* {I_2}^*+ \beta_2 S^* J^*+\beta_3 S^* A^*\right]\\[2ex]
\end{align*}",['ordinary-differential-equations']
97,Can we use the Lambert W solution $y=-3W(K_2x^{-4/3})$ instead of $y =-3W(\frac 1 3\sqrt[3]{-\frac{K_1}{x^4}})$ if we choose an appropriate constant?,Can we use the Lambert W solution  instead of  if we choose an appropriate constant?,y=-3W(K_2x^{-4/3}) y =-3W(\frac 1 3\sqrt[3]{-\frac{K_1}{x^4}}),"During the process of solving the separable differential equation $4y - x(y-3)y' = 0$ , our solution acquires a constant when we go from $\frac 4 x = \frac{y-3}{y}y'$ to $\ln x + C_1 = y - 3 \ln y$ . We then do: $$ e^{-\frac{4}{3}(\ln x) -\frac{C_1}{3}} = e^{\ln y - \frac y 3}$$ $$ x^{-\frac 4 3}e^{-\frac{C_1}3}= ye^{-\frac y 3}$$ $$-\frac 1 3 x^{-\frac 4 3}e^{-\frac{C_1}3}= -\frac y 3 e^{-\frac y 3}$$ I substitute $- \frac 1 3 e^{\frac {C_1}3}$ by the constant $K_1$ to get $$ K_1x^{-\frac 4 3}=-\frac y 3 e^{-\frac y 3}$$ applying the Lambert W function I get: $$-3W(K_1x^{-\frac 4 3}) = y$$ Instead, wolframalpha suggests me the solution $y = -3W\left( \frac 1 3\sqrt[3]{-\frac{K_2}{x^4}}\right)$ . My question is: is the process through which I substitute in the constant $K_1$ legitimate? Since otherwise, I'm unable to determine why my answer doesn't correspond to the wolframalpha one. The reason why I thought I was allowed to substitute $K_1$ was because, in the step where we go from $\frac 4 x = \frac{y-3}{y}y'$ to $\ln x + C_1 = y - 3 \ln y$ , we obviously see that taking the derivative of any constant makes our equation true, and therefore we would be free to choose whichever $K_1$ makes our equation look nice. But is this reasoning correct, given that wolframalpha provides a more complicated solution?","During the process of solving the separable differential equation , our solution acquires a constant when we go from to . We then do: I substitute by the constant to get applying the Lambert W function I get: Instead, wolframalpha suggests me the solution . My question is: is the process through which I substitute in the constant legitimate? Since otherwise, I'm unable to determine why my answer doesn't correspond to the wolframalpha one. The reason why I thought I was allowed to substitute was because, in the step where we go from to , we obviously see that taking the derivative of any constant makes our equation true, and therefore we would be free to choose whichever makes our equation look nice. But is this reasoning correct, given that wolframalpha provides a more complicated solution?",4y - x(y-3)y' = 0 \frac 4 x = \frac{y-3}{y}y' \ln x + C_1 = y - 3 \ln y  e^{-\frac{4}{3}(\ln x) -\frac{C_1}{3}} = e^{\ln y - \frac y 3}  x^{-\frac 4 3}e^{-\frac{C_1}3}= ye^{-\frac y 3} -\frac 1 3 x^{-\frac 4 3}e^{-\frac{C_1}3}= -\frac y 3 e^{-\frac y 3} - \frac 1 3 e^{\frac {C_1}3} K_1  K_1x^{-\frac 4 3}=-\frac y 3 e^{-\frac y 3} -3W(K_1x^{-\frac 4 3}) = y y = -3W\left( \frac 1 3\sqrt[3]{-\frac{K_2}{x^4}}\right) K_1 K_1 \frac 4 x = \frac{y-3}{y}y' \ln x + C_1 = y - 3 \ln y K_1,"['integration', 'ordinary-differential-equations', 'lambert-w']"
98,Proving an operator is invertible and find inverse,Proving an operator is invertible and find inverse,,"I been having trouble with the following question. This is a homework problem, so I just ned a bit of a push in the right direction. Suppose we have $E_1=\{f\in C^1([0,1])\,|\,f(0)=0\}$ with the norm $$ \|f\|=\sup_{x\in[0,1]}f(x)+\sup_{x\in[0,1]}f'(x) $$ And $E_2=C([0,1])$ with the usual norm. Then, prove the operator $A$ given by $$ (Af)(x)=f'(x)-f(x) $$ is invertible. Firstly, the space isn't Banach, so that rules out using a lot of the powerful theorems that I've learnt. To prove invertibility, the only way forward I can see is demonstrating the inequality $$||Af||\geq m||f||$$ For some $m>0$ . However, I'm having trouble working around the supremums to get this to work. Then, to find the inverse, I've been told I should use the integrating factor method to solve the ODE. I haven't taken any applied math courses and know nothing about ODEs, so I'm struggling to see how to apply this to solve for the inverse operator. Some help here would be fantastic. Thanks!","I been having trouble with the following question. This is a homework problem, so I just ned a bit of a push in the right direction. Suppose we have with the norm And with the usual norm. Then, prove the operator given by is invertible. Firstly, the space isn't Banach, so that rules out using a lot of the powerful theorems that I've learnt. To prove invertibility, the only way forward I can see is demonstrating the inequality For some . However, I'm having trouble working around the supremums to get this to work. Then, to find the inverse, I've been told I should use the integrating factor method to solve the ODE. I haven't taken any applied math courses and know nothing about ODEs, so I'm struggling to see how to apply this to solve for the inverse operator. Some help here would be fantastic. Thanks!","E_1=\{f\in C^1([0,1])\,|\,f(0)=0\} 
\|f\|=\sup_{x\in[0,1]}f(x)+\sup_{x\in[0,1]}f'(x)
 E_2=C([0,1]) A 
(Af)(x)=f'(x)-f(x)
 ||Af||\geq m||f|| m>0","['functional-analysis', 'ordinary-differential-equations', 'analysis', 'operator-theory']"
99,Inconsistency when solving IVP using Laplace Transform with Dirac Delta,Inconsistency when solving IVP using Laplace Transform with Dirac Delta,,"solving $\dot{x}(t) + x(t) = \delta (t) $ Using Laplace transform for $x(0) = 1$ , we get: $sX(s)-1 + X(s) = 1$ $X(s) = \frac{2}{s+1}$ so, $x(t) = 2e^{-t}$ However, evaluating at t=0, $x(0) = 2 \neq 1$ This disagrees with the initial condition. What went wrong here?","solving Using Laplace transform for , we get: so, However, evaluating at t=0, This disagrees with the initial condition. What went wrong here?",\dot{x}(t) + x(t) = \delta (t)  x(0) = 1 sX(s)-1 + X(s) = 1 X(s) = \frac{2}{s+1} x(t) = 2e^{-t} x(0) = 2 \neq 1,"['ordinary-differential-equations', 'laplace-transform', 'dirac-delta', 'initial-value-problems']"
