,title_raw,title_text,title_latex,body_raw,body_text,body_latex,tags
0,Proof that the real vector space of $C^\infty$ functions with $f''(x) + f(x) = 0$ is two-dimensional,Proof that the real vector space of  functions with  is two-dimensional,C^\infty f''(x) + f(x) = 0,"Prove that the real vector space of $C^\infty$ functions $f$ such that $$f''(x) + f(x) = 0$$ is two-dimensional. I feel like there must be an obvious answer to this question, but I don't know what it is. In particular, I can see that $\sin(x)$ and $\cos(x)$ are linearly independent, but how do I know that they span the space? If we restrict the problem to analytic functions, then two-dimensionality is easy: once you know the values of $f(0)$ and $f'(0)$ , the rest of the Maclaurin series follows from the identity $f^{(n+2)}(0) = -f^{(n)}(0)$ for all $n$ . But how do we know there are no non-analytic solutions? Before you ask: this isn't quite a homework question. I'm going to present this example as an algebra TA, and I'd like some confidence that my answer is actually rue.","Prove that the real vector space of functions such that is two-dimensional. I feel like there must be an obvious answer to this question, but I don't know what it is. In particular, I can see that and are linearly independent, but how do I know that they span the space? If we restrict the problem to analytic functions, then two-dimensionality is easy: once you know the values of and , the rest of the Maclaurin series follows from the identity for all . But how do we know there are no non-analytic solutions? Before you ask: this isn't quite a homework question. I'm going to present this example as an algebra TA, and I'd like some confidence that my answer is actually rue.",C^\infty f f''(x) + f(x) = 0 \sin(x) \cos(x) f(0) f'(0) f^{(n+2)}(0) = -f^{(n)}(0) n,"['real-analysis', 'linear-algebra', 'ordinary-differential-equations', 'vector-spaces', 'linear-transformations']"
1,Are all derivatives either Lebesgue integrable or improper Riemann integrable?,Are all derivatives either Lebesgue integrable or improper Riemann integrable?,,"The function $f:[-1,1] \to \mathbb{R}$ given by $f(x) = x^2\sin\left(\frac{1}{x^2}\right)$ is an example of a function whose derivative is not Lebesgue integrable on $[-1,1]$ but is improper Riemann integrable. Volterra constructed a function $f:[0,1] \to \mathbb{R}$ whose derivative is bounded (hence Lebesgue integrable) but not Riemann integrable (proper or improper). Is there a differentiable function $f:[a,b] \to \mathbb{R}$ such that $f'(x)$ is neither improper Riemann integrable nor Lebesgue integrable on $[a,b]$? Such a derivative is Henstock窶適urzweil integrable , since all derivatives happen to be Henstock窶適urzweil integrable. The example here unfortunately does not work. It has jump discontinuities, hence isn't a derivative.","The function $f:[-1,1] \to \mathbb{R}$ given by $f(x) = x^2\sin\left(\frac{1}{x^2}\right)$ is an example of a function whose derivative is not Lebesgue integrable on $[-1,1]$ but is improper Riemann integrable. Volterra constructed a function $f:[0,1] \to \mathbb{R}$ whose derivative is bounded (hence Lebesgue integrable) but not Riemann integrable (proper or improper). Is there a differentiable function $f:[a,b] \to \mathbb{R}$ such that $f'(x)$ is neither improper Riemann integrable nor Lebesgue integrable on $[a,b]$? Such a derivative is Henstock窶適urzweil integrable , since all derivatives happen to be Henstock窶適urzweil integrable. The example here unfortunately does not work. It has jump discontinuities, hence isn't a derivative.",,"['real-analysis', 'integration', 'derivatives']"
2,"How can I show that the closed unit ball in $L^1([0,1])$ is not compact?",How can I show that the closed unit ball in  is not compact?,"L^1([0,1])","Consider the following subset of $L^1([0,1])$, $S=\left\{f\in L^1([0,1]):{\|f\|}_1\leq1\right\}$. Prove that $S$ is not compact. Should I start with an open cover and prove that it has no finite subcover or find a convergent sequence that has no convergent sub-sequence? I am quite confused.","Consider the following subset of $L^1([0,1])$, $S=\left\{f\in L^1([0,1]):{\|f\|}_1\leq1\right\}$. Prove that $S$ is not compact. Should I start with an open cover and prove that it has no finite subcover or find a convergent sequence that has no convergent sub-sequence? I am quite confused.",,"['real-analysis', 'general-topology', 'compactness', 'lp-spaces']"
3,If $\sum a_n$ converges then $\liminf na_n=0$,If  converges then,\sum a_n \liminf na_n=0,"I am trying to prove the following statement: Let $(a_n)_{n\in\mathbb N}$ be a positive sequence such that the series $\sum a_n$ is convergent. Prove that $$\liminf_{n}na_n=0$$ Now, if $(a_n)_{n\in \mathbb N}$ is a real valued sequence, then $\liminf a_n=\alpha$ if and only if the following two conditions hold: If $\beta \in \mathbb R \cup \{\pm \infty\}$ is such that $\beta<\alpha$ there exists $n_0\in \mathbb N$ such that $a_n>\beta$ for every $n>n_0$ There exists a subsequence $(a_{\varphi (k)})_{k\in \mathbb N}$ of $(a_n)_{n\in \mathbb N}$ that converges to $\alpha$. The first one is trivial since $(na_n)_{n\in \mathbb N}$ is positive, so the problem basically boils down to constructing a subsequence on $na_n$ that converges to zero. The given hypothesis implies that $a_n\to 0$ and that $a_n<\frac{1}{n}$ eventually, but I cannot seem to be able to use these bits of information to construct a subsequence that converges to $0$. I also tried by contradiction, assuming that $\liminf na_n\neq 0$, but did not get very far. This brings me to the two following questions: Is it possible to explicitly construct a subsequence of $(na_n)_{n\in \mathbb N}$ that converges to zero? How can I prove the given statement?","I am trying to prove the following statement: Let $(a_n)_{n\in\mathbb N}$ be a positive sequence such that the series $\sum a_n$ is convergent. Prove that $$\liminf_{n}na_n=0$$ Now, if $(a_n)_{n\in \mathbb N}$ is a real valued sequence, then $\liminf a_n=\alpha$ if and only if the following two conditions hold: If $\beta \in \mathbb R \cup \{\pm \infty\}$ is such that $\beta<\alpha$ there exists $n_0\in \mathbb N$ such that $a_n>\beta$ for every $n>n_0$ There exists a subsequence $(a_{\varphi (k)})_{k\in \mathbb N}$ of $(a_n)_{n\in \mathbb N}$ that converges to $\alpha$. The first one is trivial since $(na_n)_{n\in \mathbb N}$ is positive, so the problem basically boils down to constructing a subsequence on $na_n$ that converges to zero. The given hypothesis implies that $a_n\to 0$ and that $a_n<\frac{1}{n}$ eventually, but I cannot seem to be able to use these bits of information to construct a subsequence that converges to $0$. I also tried by contradiction, assuming that $\liminf na_n\neq 0$, but did not get very far. This brings me to the two following questions: Is it possible to explicitly construct a subsequence of $(na_n)_{n\in \mathbb N}$ that converges to zero? How can I prove the given statement?",,"['real-analysis', 'sequences-and-series', 'limits']"
4,Defining the factorial of a real number,Defining the factorial of a real number,,"I'm curious, how is the factorial of a real number defined?  Intuitively, it should be: $x! = 0$ if $x \leq 1$ $x! = \infty$ if $x >1$ Since it would be the product of all real numbers preceding it, however, when I plug $\pi!$ into my calculator, I get an actual value: $7.18808272898$ How is that value determined?","I'm curious, how is the factorial of a real number defined?  Intuitively, it should be: $x! = 0$ if $x \leq 1$ $x! = \infty$ if $x >1$ Since it would be the product of all real numbers preceding it, however, when I plug $\pi!$ into my calculator, I get an actual value: $7.18808272898$ How is that value determined?",,['real-analysis']
5,What is the value of $(-1)^{\frac {2}{2}}$?,What is the value of ?,(-1)^{\frac {2}{2}},"When working with real numbers, should we interpret it as $$\left( (-1)^{2} \right) ^{\frac 12}=1$$ or rather as $$\left( (-1)^{{\frac 12}} \right) ^2= \text {not real/undefined}$$ or rather as $$(-1)^{\ 1} = -1$$ What about when we work with complex numbers? EDIT: The reason why I called the second case ""undefined"" is because when we are only working with reals, $(-1)^{\frac 12}$ is not real. I don't think we can assume that further algebraic manipulations that work on real numbers also work on our $(-1)^{\frac 12}$.","When working with real numbers, should we interpret it as $$\left( (-1)^{2} \right) ^{\frac 12}=1$$ or rather as $$\left( (-1)^{{\frac 12}} \right) ^2= \text {not real/undefined}$$ or rather as $$(-1)^{\ 1} = -1$$ What about when we work with complex numbers? EDIT: The reason why I called the second case ""undefined"" is because when we are only working with reals, $(-1)^{\frac 12}$ is not real. I don't think we can assume that further algebraic manipulations that work on real numbers also work on our $(-1)^{\frac 12}$.",,"['real-analysis', 'algebra-precalculus', 'complex-numbers', 'exponentiation']"
6,Are $C(\mathbb{R})$ and $D(\mathbb{R})$ isomorphic or not?,Are  and  isomorphic or not?,C(\mathbb{R}) D(\mathbb{R}),"Let's denote ring of all continuous functions and differentiable functions from $\mathbb{R}$ to $\mathbb{R}$ by $C(\mathbb{R})$ and $D(\mathbb{R})$, respectively. I want to know whether these rings are isomorphic or not. From the first part of this answer it is clear that if there exists a ring monomorphism which sends 1 to itself then it is identity on the constant functions. But I can not find anything else. Please help. Thank you.","Let's denote ring of all continuous functions and differentiable functions from $\mathbb{R}$ to $\mathbb{R}$ by $C(\mathbb{R})$ and $D(\mathbb{R})$, respectively. I want to know whether these rings are isomorphic or not. From the first part of this answer it is clear that if there exists a ring monomorphism which sends 1 to itself then it is identity on the constant functions. But I can not find anything else. Please help. Thank you.",,"['real-analysis', 'abstract-algebra', 'commutative-algebra']"
7,Use of inequality $1 - \cos (x) \leq x^2 /2$,Use of inequality,1 - \cos (x) \leq x^2 /2,"In this answer the value of $1 - \cos(x)$ has to be evaluated in order to find its upper limit, if it exists. In particular, $x = 2 \pi / n$. The answer is related to the length of a side of a regular $n$-gon inscribed into a unit-radius circumference; because the perimeter of the $n$-gon is always less than $2 \pi$, the single side must always be less than $2 \pi / n$. The inequality $$1 - \cos (x) \leq \displaystyle \frac{x^2}{2}$$ (1) is used and the proof is completed with $$2(1 - \cos(x)) \leq (2 \pi / n)^2$$ $$\sqrt{2(1 - \cos(x))} \leq 2 \pi / n$$ But it is well known that the cosine is a function $f(x) \in [-1;1]$, so $1 - \cos (x) \in [0,2]$. By using this information, we would obtain $$1 - \cos (x) \leq 2$$ (2) The proof would provide $$2(1 - \cos(x)) \leq 4$$ $$\sqrt{2(1 - \cos(x))} \leq 2$$ which is a completely different result. Why in that case it is preferable to use (1) instead of (2)? How to choose when it is convenient to use (1) and when to use (2) in a proof?","In this answer the value of $1 - \cos(x)$ has to be evaluated in order to find its upper limit, if it exists. In particular, $x = 2 \pi / n$. The answer is related to the length of a side of a regular $n$-gon inscribed into a unit-radius circumference; because the perimeter of the $n$-gon is always less than $2 \pi$, the single side must always be less than $2 \pi / n$. The inequality $$1 - \cos (x) \leq \displaystyle \frac{x^2}{2}$$ (1) is used and the proof is completed with $$2(1 - \cos(x)) \leq (2 \pi / n)^2$$ $$\sqrt{2(1 - \cos(x))} \leq 2 \pi / n$$ But it is well known that the cosine is a function $f(x) \in [-1;1]$, so $1 - \cos (x) \in [0,2]$. By using this information, we would obtain $$1 - \cos (x) \leq 2$$ (2) The proof would provide $$2(1 - \cos(x)) \leq 4$$ $$\sqrt{2(1 - \cos(x))} \leq 2$$ which is a completely different result. Why in that case it is preferable to use (1) instead of (2)? How to choose when it is convenient to use (1) and when to use (2) in a proof?",,"['calculus', 'real-analysis', 'trigonometry', 'inequality', 'proof-verification']"
8,"Is $\sin^2x$ uniformly continuous on$x\in [0,\infty]$",Is  uniformly continuous on,"\sin^2x x\in [0,\infty]","I have the question that is $sin^2x$ uniformly continuous on $x \in [0,\infty]$ ? My approach: Let $\left|x-y\right|<\delta$ we have:- $$\left|sin^2x-sin^2y\right|=\left|(\sin x+\sin y)(sin x-sin y)\right|$$ $$=4\left|\sin\left(\frac{x+y}{2}\right)\cos\left(\frac{x-y}{2}\right)\cos\left(\frac{x+y}{2}\right)\sin \left(\frac{x-y}{2}\right)\right|\lt4\left|\sin \left(\frac{x+y}{2}\right)\sin\left(\frac{x-y}{2}\right)\right|\tag{1}$$  $$\lt \left|(x-y)(x+y)\right|<\left|(x+y)\right|\delta,$$which is dependent on $x$ so $\sin^2\!x$ is not uniformly continuous. Is this solution correct or not? I have some doubt about validity of inequality $(1)$ also, if it is correct then why?","I have the question that is $sin^2x$ uniformly continuous on $x \in [0,\infty]$ ? My approach: Let $\left|x-y\right|<\delta$ we have:- $$\left|sin^2x-sin^2y\right|=\left|(\sin x+\sin y)(sin x-sin y)\right|$$ $$=4\left|\sin\left(\frac{x+y}{2}\right)\cos\left(\frac{x-y}{2}\right)\cos\left(\frac{x+y}{2}\right)\sin \left(\frac{x-y}{2}\right)\right|\lt4\left|\sin \left(\frac{x+y}{2}\right)\sin\left(\frac{x-y}{2}\right)\right|\tag{1}$$  $$\lt \left|(x-y)(x+y)\right|<\left|(x+y)\right|\delta,$$which is dependent on $x$ so $\sin^2\!x$ is not uniformly continuous. Is this solution correct or not? I have some doubt about validity of inequality $(1)$ also, if it is correct then why?",,"['real-analysis', 'calculus', 'continuity', 'uniform-continuity']"
9,Show that every sequence in $\mathbb{R}$ has a monotone subsequence,Show that every sequence in  has a monotone subsequence,\mathbb{R},"So I would like some hints as to how to proceed on this problem as I am stuck with a particular case. I divided this proof up into 2 cases, where the sequence is convergent and the sequence is divergent.I am somewhat confident that I have shown this for the case where the sequence is convergent, and I am currently stumped on the case for divergent sequences. My intuition tells me that because the sequence is divergent, I can pick some $p\in \mathbb{R}$ and consider the subsequence of points which move farther and farther away from $p$, and that will give me my monotone subsequence. But in negating the definition of convergence I am only guaranteed that there exists some $\epsilon>0$ such that for any $N\in \mathbb{N}$, there exists an $n>N$ such that $|x_n-p|\geq \epsilon$, so these point can fluctuate between being exactly within $\epsilon$ of $p$ or more than $\epsilon$, and I don't have enough info about the behavior of the sequence to assure myself of a monotone subsequence. So how can I proceed from here (assuming that I am on the right track)? Or is there a better route that I can take which does not involve a proof split into cases? Thank you.","So I would like some hints as to how to proceed on this problem as I am stuck with a particular case. I divided this proof up into 2 cases, where the sequence is convergent and the sequence is divergent.I am somewhat confident that I have shown this for the case where the sequence is convergent, and I am currently stumped on the case for divergent sequences. My intuition tells me that because the sequence is divergent, I can pick some $p\in \mathbb{R}$ and consider the subsequence of points which move farther and farther away from $p$, and that will give me my monotone subsequence. But in negating the definition of convergence I am only guaranteed that there exists some $\epsilon>0$ such that for any $N\in \mathbb{N}$, there exists an $n>N$ such that $|x_n-p|\geq \epsilon$, so these point can fluctuate between being exactly within $\epsilon$ of $p$ or more than $\epsilon$, and I don't have enough info about the behavior of the sequence to assure myself of a monotone subsequence. So how can I proceed from here (assuming that I am on the right track)? Or is there a better route that I can take which does not involve a proof split into cases? Thank you.",,"['real-analysis', 'sequences-and-series']"
10,Integral in n-dimensional spherical coordinates,Integral in n-dimensional spherical coordinates,,"I have to calculate the following integral: $\int_{B_1(0)} \frac{1}{|x|^m}  dx $ where $x \in \mathbb{R}^d$ and  $B_1(0)$ is a $d$ dimensional ball around origin with radius equal to $1$. I know I can use $\int_{R^d}f(x)dx=\int_{S^{d-1}}\left(\int_{0}^{\infty} f(r \gamma) r^{d-1} dr \right) d \sigma(\gamma)$, where $r=|x|$ and $\gamma=\frac{x}{|x|}$. But, I don't know how to apply it on my problem and obtain the results.","I have to calculate the following integral: $\int_{B_1(0)} \frac{1}{|x|^m}  dx $ where $x \in \mathbb{R}^d$ and  $B_1(0)$ is a $d$ dimensional ball around origin with radius equal to $1$. I know I can use $\int_{R^d}f(x)dx=\int_{S^{d-1}}\left(\int_{0}^{\infty} f(r \gamma) r^{d-1} dr \right) d \sigma(\gamma)$, where $r=|x|$ and $\gamma=\frac{x}{|x|}$. But, I don't know how to apply it on my problem and obtain the results.",,"['real-analysis', 'integration', 'polar-coordinates']"
11,"Creating an increasing function $g$ given a continuous function $f$, such that $|f(x)-f(y)| \leq |g(x)-g(y)|$","Creating an increasing function  given a continuous function , such that",g f |f(x)-f(y)| \leq |g(x)-g(y)|,"Given a continuous function $f$ over an interval, must there exist a continuous, increasing function $g$ such that for all $x,y$ $$|f(x)-f(y)| \leq |g(x)-g(y)|$$ I've tried assuming the opposite, that all $g$'s fail for some pair $x,y$ to reach a contradiction, since for every pair $x,y$, there is some $g$ that satisfies the inequality. Similarly for a finite number of pairs $x,y$ there is always a $g$ to satisfy. But I haven't gotten far with that nor used the continuity of $f$. So I tried using the $\varepsilon$-$\delta$ definition on $f$ around a point since I might be able to connect $g$ to the values of $\varepsilon$, but I haven't found a way to do that either. In the end, if a $g$ does exist (which I'm somewhat convinced is true), I'd like to find the ""minimal"" $g_0$ such that for all satisfying $g$'s and $x,y$ $$|g_0(x)-g_0(y)| \leq |g(x)-g(y)|$$ For example, $f=x^2$ and $g_0=x|x|$. Of course if there's a $g_0$, any $g_0+C$ is another one. Thank you.","Given a continuous function $f$ over an interval, must there exist a continuous, increasing function $g$ such that for all $x,y$ $$|f(x)-f(y)| \leq |g(x)-g(y)|$$ I've tried assuming the opposite, that all $g$'s fail for some pair $x,y$ to reach a contradiction, since for every pair $x,y$, there is some $g$ that satisfies the inequality. Similarly for a finite number of pairs $x,y$ there is always a $g$ to satisfy. But I haven't gotten far with that nor used the continuity of $f$. So I tried using the $\varepsilon$-$\delta$ definition on $f$ around a point since I might be able to connect $g$ to the values of $\varepsilon$, but I haven't found a way to do that either. In the end, if a $g$ does exist (which I'm somewhat convinced is true), I'd like to find the ""minimal"" $g_0$ such that for all satisfying $g$'s and $x,y$ $$|g_0(x)-g_0(y)| \leq |g(x)-g(y)|$$ For example, $f=x^2$ and $g_0=x|x|$. Of course if there's a $g_0$, any $g_0+C$ is another one. Thank you.",,"['real-analysis', 'functions', 'continuity']"
12,"How do you prove the set $G = \{(x, f(x)) \mid x \in \mathbb R\}$ is closed? [duplicate]",How do you prove the set  is closed? [duplicate],"G = \{(x, f(x)) \mid x \in \mathbb R\}","This question already has answers here : Graph of a continuous function is closed (2 answers) Closed 9 years ago . Let $f : \mathbb R \to \mathbb R$ continuous. Prove that graph $G = \{(x, f(x)) \mid x \in \mathbb R\}$ is closed. I'm a little confused on how to prove $G$ is closed. I get the general strategy is to show that every arbitrary convergent sequence in $G$ converges to a point in $G$. Here is what I tried so far: Let $x_k$ be a sequence which converges to $x$. Since $f$ is continuous, this implies that $f(x_k)$ converges to $f(x)$. At this point, can you say every $(x_k, f(x_k))$ converges to a $(x, f(x))$, so $G$ is closed?","This question already has answers here : Graph of a continuous function is closed (2 answers) Closed 9 years ago . Let $f : \mathbb R \to \mathbb R$ continuous. Prove that graph $G = \{(x, f(x)) \mid x \in \mathbb R\}$ is closed. I'm a little confused on how to prove $G$ is closed. I get the general strategy is to show that every arbitrary convergent sequence in $G$ converges to a point in $G$. Here is what I tried so far: Let $x_k$ be a sequence which converges to $x$. Since $f$ is continuous, this implies that $f(x_k)$ converges to $f(x)$. At this point, can you say every $(x_k, f(x_k))$ converges to a $(x, f(x))$, so $G$ is closed?",,"['real-analysis', 'general-topology']"
13,Prove that $\sum_{n=0}^\infty \frac{(-1)^n}{3n+1} = \frac{\pi}{3\sqrt{3}}+\frac{\log 2}{3}$,Prove that,\sum_{n=0}^\infty \frac{(-1)^n}{3n+1} = \frac{\pi}{3\sqrt{3}}+\frac{\log 2}{3},Prove that $$\sum_{n=0}^\infty \frac{(-1)^n}{3n+1} = \frac{\pi}{3\sqrt{3}}+\frac{\log 2}{3}$$ I tried to look at $$ f_n(x) = \sum_{n=0}^\infty \frac{(-1)^n}{3n+1} x^n $$ And maybe taking it's derivative but it didn't work out well. Any ideas?,Prove that $$\sum_{n=0}^\infty \frac{(-1)^n}{3n+1} = \frac{\pi}{3\sqrt{3}}+\frac{\log 2}{3}$$ I tried to look at $$ f_n(x) = \sum_{n=0}^\infty \frac{(-1)^n}{3n+1} x^n $$ And maybe taking it's derivative but it didn't work out well. Any ideas?,,"['real-analysis', 'calculus', 'sequences-and-series', 'summation', 'pi']"
14,a custom designed cutoff function whose derivative is bounded above.,a custom designed cutoff function whose derivative is bounded above.,,I am trying to find a $C^\infty$ function $\phi(t)$ with the following  properties. $\phi(t) =1$ for $\lvert t \rvert \le 1$ $\phi(t)$=0 for $t \geq 2$ $\lvert \phi'(t) \rvert \le 2 $ I have tried the obvious function $\exp\left(\frac{1}{t^2-1} \right)$ as follows. $$ \phi(t)=\begin{cases} 1 & \mbox{if} & \lvert t \rvert \le 1\\ \exp\left(\frac{1}{t^2-1}\right) & \mbox{if} & -2 < t < -1 \; \mbox{or} \; 1 < t < 2 \\ 0 & \mbox{if}& \lvert t \rvert \geq 2 \end{cases} $$ But I can't seem to get the required bound on the derivative of $\phi(t)$. Question ?: What am I doing wrong here?. How can I adjust/improve this to get the bound I want?. Any suggestions?,I am trying to find a $C^\infty$ function $\phi(t)$ with the following  properties. $\phi(t) =1$ for $\lvert t \rvert \le 1$ $\phi(t)$=0 for $t \geq 2$ $\lvert \phi'(t) \rvert \le 2 $ I have tried the obvious function $\exp\left(\frac{1}{t^2-1} \right)$ as follows. $$ \phi(t)=\begin{cases} 1 & \mbox{if} & \lvert t \rvert \le 1\\ \exp\left(\frac{1}{t^2-1}\right) & \mbox{if} & -2 < t < -1 \; \mbox{or} \; 1 < t < 2 \\ 0 & \mbox{if}& \lvert t \rvert \geq 2 \end{cases} $$ But I can't seem to get the required bound on the derivative of $\phi(t)$. Question ?: What am I doing wrong here?. How can I adjust/improve this to get the bound I want?. Any suggestions?,,"['real-analysis', 'functional-analysis', 'partial-differential-equations']"
15,Functional equation and fixed points,Functional equation and fixed points,,"Let $f$ be strictly increasing and such that $f(x)+f^{-1}(x)+1=e^x$. Is it true that $f$ has at most one fixed point? I am told the answer is yes, but I am having trouble proving it. It's obvious that it must have at most two, but why can it not have two?","Let $f$ be strictly increasing and such that $f(x)+f^{-1}(x)+1=e^x$. Is it true that $f$ has at most one fixed point? I am told the answer is yes, but I am having trouble proving it. It's obvious that it must have at most two, but why can it not have two?",,"['calculus', 'real-analysis', 'functional-equations']"
16,Evaluating the integral $\int_0^\infty \frac{x \sin rx }{a^2+x^2} dx$ using only real analysis,Evaluating the integral  using only real analysis,\int_0^\infty \frac{x \sin rx }{a^2+x^2} dx,"Calculate the integral$$ \int_0^\infty \frac{x \sin rx }{a^2+x^2} dx=\frac{1}{2}\int_{-\infty}^\infty \frac{x \sin rx }{a^2+x^2} dx,\quad a,r \in \mathbb{R}. $$  Edit: I was able to solve the integral using complex analysis, and now I want to try and solve it using only real analysis techniques.","Calculate the integral$$ \int_0^\infty \frac{x \sin rx }{a^2+x^2} dx=\frac{1}{2}\int_{-\infty}^\infty \frac{x \sin rx }{a^2+x^2} dx,\quad a,r \in \mathbb{R}. $$  Edit: I was able to solve the integral using complex analysis, and now I want to try and solve it using only real analysis techniques.",,"['calculus', 'real-analysis']"
17,Continuity proof for exponential,Continuity proof for exponential,,Show that $f(x) = e^x$ is continuous using the epsilon-delta definition. I can't seem to write down anything meaningful...,Show that $f(x) = e^x$ is continuous using the epsilon-delta definition. I can't seem to write down anything meaningful...,,"['real-analysis', 'continuity', 'exponential-function']"
18,Inequality for the p norm of a convolution,Inequality for the p norm of a convolution,,"Let $1\le p < \infty$ . Let $g \in L^p(\Bbb R^n)$ , and $f \in L^1(\Bbb R^n)$ . Show: $\|f*g\|_p \le \|f\|_1\|g\|_p$ , where $*$ indicates the convolution defined by $$ f*g (x) := \int f(x-t) g(t)dt.$$ Here is a relevant hint, apparently: Let $q$ be the conjugate of $p$ . Note that: $$|f*g| \le \int [|g(t)||f(x-t)|^{1/p}]|f(x-t)|^{1/q}dt,$$ then apply Holder's inequality. I did exactly as the hint suggested, and ended up with what looked like a meaningless inequality compared to the final answer. I even took the $p$ power of both sides. Any help?","Let . Let , and . Show: , where indicates the convolution defined by Here is a relevant hint, apparently: Let be the conjugate of . Note that: then apply Holder's inequality. I did exactly as the hint suggested, and ended up with what looked like a meaningless inequality compared to the final answer. I even took the power of both sides. Any help?","1\le p < \infty g \in L^p(\Bbb R^n) f \in L^1(\Bbb R^n) \|f*g\|_p \le \|f\|_1\|g\|_p *  f*g (x) := \int f(x-t) g(t)dt. q p |f*g| \le \int [|g(t)||f(x-t)|^{1/p}]|f(x-t)|^{1/q}dt, p","['real-analysis', 'measure-theory', 'convolution']"
19,"if derivative vanishes in a path connected set, then $f$ is constant on that set?","if derivative vanishes in a path connected set, then  is constant on that set?",f,"Let $f: \mathbb{R}^d \to \mathbb{R}^m $ be a function sucht that $Df(a) = 0 $ for all $a \in U \subseteq \mathbb{R}^d$. Also, suppose $U$ is open and path-connected. Question: IS $f$ constant on $U$ ??","Let $f: \mathbb{R}^d \to \mathbb{R}^m $ be a function sucht that $Df(a) = 0 $ for all $a \in U \subseteq \mathbb{R}^d$. Also, suppose $U$ is open and path-connected. Question: IS $f$ constant on $U$ ??",,['calculus']
20,Real Analysis and Statistics,Real Analysis and Statistics,,"What level of real analysis do you think is desirable for the study of statistics? I know that for many statisticians with applied focus, rigorous mathematics tend to give them a headache and I am not far off. I have always considered analysis a little annoying because of its many self-evident results. I will study it nevertheless if it can make you a better statistician. What is your opinion? Thank you.","What level of real analysis do you think is desirable for the study of statistics? I know that for many statisticians with applied focus, rigorous mathematics tend to give them a headache and I am not far off. I have always considered analysis a little annoying because of its many self-evident results. I will study it nevertheless if it can make you a better statistician. What is your opinion? Thank you.",,"['real-analysis', 'probability', 'statistics', 'probability-theory', 'soft-question']"
21,Intuition for Lebesgue integration,Intuition for Lebesgue integration,,"I have started doing Lebesgue integration and I just want to clarify one thing to start with. Very loosely speaking, with Riemann integration we partition the domain into $n$ intervals, and then we calculate the area of the $n$ rectangles formed by a base of width $n$ and a height of where each rectangle 'hits' the function. Summing these rectangles gives us an approximation of the area of the graph under the function. Then as we let $n \to \infty$ this approximation increase in accuracy and equals the function when we take the limit. Now with Lebesgue integration do we follow the same process of partitioning (the range this time) into $n$ intervals and then letting $n \to \infty$ giving us smaller and smaller intervals which implies the approximation to the function keeps increasing? Leaving aside the concept of having sets that are measurable on the domain...I am simply wondering is process of considering intervals of decreasing size the same as with Riemann integration?","I have started doing Lebesgue integration and I just want to clarify one thing to start with. Very loosely speaking, with Riemann integration we partition the domain into $n$ intervals, and then we calculate the area of the $n$ rectangles formed by a base of width $n$ and a height of where each rectangle 'hits' the function. Summing these rectangles gives us an approximation of the area of the graph under the function. Then as we let $n \to \infty$ this approximation increase in accuracy and equals the function when we take the limit. Now with Lebesgue integration do we follow the same process of partitioning (the range this time) into $n$ intervals and then letting $n \to \infty$ giving us smaller and smaller intervals which implies the approximation to the function keeps increasing? Leaving aside the concept of having sets that are measurable on the domain...I am simply wondering is process of considering intervals of decreasing size the same as with Riemann integration?",,"['calculus', 'real-analysis', 'integration', 'lebesgue-integral']"
22,Prove that every unbounded sequence contains a monotone subsequence that diverges to inifnity.,Prove that every unbounded sequence contains a monotone subsequence that diverges to inifnity.,,"I think I have the basic framework for this proof, but I am having trouble putting everything together in a convincing way. I plan on showing that by the definition, an unbounded sequence $(a_n)$ has infinitely many $n$ such that $a_n>M$ where $M>0$, for some natural number $n$. I'm not sure how it is implied that there exists some $a_{n_k}$ such that $a_{n_k}>M$ for some natural number $k$.  Let me know if I'm on the right track, and please help me string these ideas together!","I think I have the basic framework for this proof, but I am having trouble putting everything together in a convincing way. I plan on showing that by the definition, an unbounded sequence $(a_n)$ has infinitely many $n$ such that $a_n>M$ where $M>0$, for some natural number $n$. I'm not sure how it is implied that there exists some $a_{n_k}$ such that $a_{n_k}>M$ for some natural number $k$.  Let me know if I'm on the right track, and please help me string these ideas together!",,['real-analysis']
23,"How to prove the limit of a sequence using ""$\epsilon-N$""","How to prove the limit of a sequence using """"",\epsilon-N,"I think I have a proper understanding of the general procedure, but I'm having difficulty manipulating my inequality so that I can isolate $n$ by itself. Sadly I wasn't given many examples to model my answer on. Prove that $\displaystyle\lim_{n\to\infty}\frac{n+1}{n^2+1}=0$ So I'm given $L=0$. I then look at the inequality $$\left| \frac{n+1}{n^2+1}-0\right|<\epsilon$$ but I have no idea how to isolate $n$. The best I can come up with, which may be the right idea, is to use another function $f$ such that $$\left|\frac{n+1}{n^2+1}\right|<f<\epsilon$$ and then work with that. But my idea of using $f=\lvert n+1\rvert$ seems to have me a bit stuck too.","I think I have a proper understanding of the general procedure, but I'm having difficulty manipulating my inequality so that I can isolate $n$ by itself. Sadly I wasn't given many examples to model my answer on. Prove that $\displaystyle\lim_{n\to\infty}\frac{n+1}{n^2+1}=0$ So I'm given $L=0$. I then look at the inequality $$\left| \frac{n+1}{n^2+1}-0\right|<\epsilon$$ but I have no idea how to isolate $n$. The best I can come up with, which may be the right idea, is to use another function $f$ such that $$\left|\frac{n+1}{n^2+1}\right|<f<\epsilon$$ and then work with that. But my idea of using $f=\lvert n+1\rvert$ seems to have me a bit stuck too.",,"['calculus', 'real-analysis', 'limits', 'epsilon-delta']"
24,To show that function is constant [duplicate],To show that function is constant [duplicate],,"This question already has answers here : Function on $[a,b]$ that satisfies a Hﾃｶlder condition of order $\alpha > 1 $ is constant (2 answers) Closed 6 years ago . Let $f$ be defined on $\mathbb{R}$ and suppose that |$f(x)$ - $f(y)$| $\leq$ $(x-y)^2$ $x,y \in\mathbb{R}$. Here I have to show that $f$ is a constant function. I think I have to show that $f'(x)$ = 0 for all $x$. But I don't know from where to start this. I tried taking it as (|$f(x)$ - $f(y)$|/|$x$-$y$|) $\leq$ |$x$ - $y$|. Am I right in doing so? Any hint or suggestion will be helpful. Thanks","This question already has answers here : Function on $[a,b]$ that satisfies a Hﾃｶlder condition of order $\alpha > 1 $ is constant (2 answers) Closed 6 years ago . Let $f$ be defined on $\mathbb{R}$ and suppose that |$f(x)$ - $f(y)$| $\leq$ $(x-y)^2$ $x,y \in\mathbb{R}$. Here I have to show that $f$ is a constant function. I think I have to show that $f'(x)$ = 0 for all $x$. But I don't know from where to start this. I tried taking it as (|$f(x)$ - $f(y)$|/|$x$-$y$|) $\leq$ |$x$ - $y$|. Am I right in doing so? Any hint or suggestion will be helpful. Thanks",,['real-analysis']
25,"Proving that the Fourier Basis is complete for C(R/$2*\pi$ , C) with $L^2$ norm","Proving that the Fourier Basis is complete for C(R/ , C) with  norm",2*\pi L^2,"Let $H$ be the inner product space = $\{f: \mathbb{R} \to \mathbb{C} \mid f \text{ is continuous and has period }2 \pi\}$ where the inner product is: $$\langle f,g \rangle = \int_{0}^{2\pi}f(t)\overline{g(t)} dt $$  How do I prove that for $n \in \mathbb{Z}$ and $e_n(t)=\dfrac{1}{\sqrt{2\cdot\pi}} e^{i n  t}$, $(e_n)_{n \in N}$ is a basis of $H$ (that it is orthonormal is easy).","Let $H$ be the inner product space = $\{f: \mathbb{R} \to \mathbb{C} \mid f \text{ is continuous and has period }2 \pi\}$ where the inner product is: $$\langle f,g \rangle = \int_{0}^{2\pi}f(t)\overline{g(t)} dt $$  How do I prove that for $n \in \mathbb{Z}$ and $e_n(t)=\dfrac{1}{\sqrt{2\cdot\pi}} e^{i n  t}$, $(e_n)_{n \in N}$ is a basis of $H$ (that it is orthonormal is easy).",,"['real-analysis', 'functional-analysis']"
26,"Function Spaces, why is the space of continuous functions (without necessarily differentiability) not important?","Function Spaces, why is the space of continuous functions (without necessarily differentiability) not important?",,"The space $C^0$ denotes the set of continuous and differentiable functions, the space $C^1$ the set of the continuous and differentiable functions which have a continuous and differentiable first derivative and so on. But it is well known that there are functions which are continuous, but not differentiable. So why there are no spaces for them, I mean a space of the continuous functions, and a space for the functions which have just continuous derivative (not necessarily differentiable too)?","The space $C^0$ denotes the set of continuous and differentiable functions, the space $C^1$ the set of the continuous and differentiable functions which have a continuous and differentiable first derivative and so on. But it is well known that there are functions which are continuous, but not differentiable. So why there are no spaces for them, I mean a space of the continuous functions, and a space for the functions which have just continuous derivative (not necessarily differentiable too)?",,"['real-analysis', 'general-topology', 'functional-analysis']"
27,Any countable $A \subseteq \mathbb{R}$ satisfies $(x+A) \cap A = \emptyset$ for some $x$,Any countable  satisfies  for some,A \subseteq \mathbb{R} (x+A) \cap A = \emptyset x,"I have to prove that if $A \subseteq \mathbb{R}$ is countable, then    $\exists x \in \mathbb{R}\, (x+A) \cap A = \emptyset, $ where $x+A$ denotes the set $\{x + a \mid a \in A\}$. I can see why this is true for some specific subsets (like the set of rationals or the set of algebraic numbers), but the general approach eludes me. Any hints would be appreciated.","I have to prove that if $A \subseteq \mathbb{R}$ is countable, then    $\exists x \in \mathbb{R}\, (x+A) \cap A = \emptyset, $ where $x+A$ denotes the set $\{x + a \mid a \in A\}$. I can see why this is true for some specific subsets (like the set of rationals or the set of algebraic numbers), but the general approach eludes me. Any hints would be appreciated.",,['real-analysis']
28,Showing a function of two variables is measurable,Showing a function of two variables is measurable,,"Let f(x,y) be a function defined on the unit square $0\leq x\leq1$,  $0\leq y\leq1$ that is continuous on each variable separately. Is f a measurable function of (x,y)? I think I need to look at the pre-images of f, and I need to use the fact that it is continuous. Maybe I can use the epsilon-delta definition of continuous functions?","Let f(x,y) be a function defined on the unit square $0\leq x\leq1$,  $0\leq y\leq1$ that is continuous on each variable separately. Is f a measurable function of (x,y)? I think I need to look at the pre-images of f, and I need to use the fact that it is continuous. Maybe I can use the epsilon-delta definition of continuous functions?",,['real-analysis']
29,"There's a real between any two rationals, a rational between any two reals, but more reals than rationals?","There's a real between any two rationals, a rational between any two reals, but more reals than rationals?",,"The following statements are all true: Between any two rational numbers, there is a real number (for example, their average). Between any two real numbers, there is a rational number (see this proof of that fact , for example). There are strictly more real numbers than rational numbers. While I accept each of these as true, the third statement seems troubling in light of the first two.  It seems like there should be some way to find a bijection between reals and rationals given the first two properties. I understand that in-between each pair of rationals there are infinitely many reals (in fact, I think there's $2^{\aleph_0}$ of them), but given that this is true it seems like there should also be in turn a large number of rationals between all of those reals. Is there a good conceptual or mathematical justification for why the third statement is tue given that the first two are as well? Thanks!  This has been bothering me for quite some time.","The following statements are all true: Between any two rational numbers, there is a real number (for example, their average). Between any two real numbers, there is a rational number (see this proof of that fact , for example). There are strictly more real numbers than rational numbers. While I accept each of these as true, the third statement seems troubling in light of the first two.  It seems like there should be some way to find a bijection between reals and rationals given the first two properties. I understand that in-between each pair of rationals there are infinitely many reals (in fact, I think there's $2^{\aleph_0}$ of them), but given that this is true it seems like there should also be in turn a large number of rationals between all of those reals. Is there a good conceptual or mathematical justification for why the third statement is tue given that the first two are as well? Thanks!  This has been bothering me for quite some time.",,"['real-analysis', 'infinity']"
30,"Haar's base for $L^2[0,1]$",Haar's base for,"L^2[0,1]","$\newcommand{\span}{\operatorname{span}}$ Define $e_{0,0}\equiv 1$, and for all $n\in \mathbb{N}$ $$e_{n,k}=\begin{cases} 2^{n/2} &\text{if } \frac{k-1}{2^n}\leq x\lt \frac{k-\frac{1}{2}}{2^n}\\ -2^{n/2}&\text{if } \frac{k-\frac{1}{2}}{2^n}\leq x\lt \frac{k}{2^n}\\ 0 &\text{otherwise} \end{cases}$$ for $k=1,\ldots,2^n$. Let $$H:=\{e_{n,k}:n,k\in \mathbb{N}\}.$$ I want to prove that $H$ is a Hilbert's base for $L^2[0,1]$ with the usal inner product. In order to prove this we must show that $H$ is orthonormal and that $\span(H)$ is dense in $L^2[0,1]$. Here is a good place to begin to see the orthonormality. For the second thing I have the following exercise: Let $f\in H^{\bot}$, i.e. $f$ is such that for all $n\in \mathbb{N}$ $$\int_0^1 f(x)e_{n,k}(x)dx=0,$$ for $k=1,\ldots,2^n$. Show that for all $n\in \mathbb{N}$ $$\int_0^1f\cdot 1_{[0,k/2^{n})}=0,$$ $k=1,\ldots,2^n$. Conclude that $f\equiv 0$. The exercise show that $(\overline{\span(H)})^{\bot}=\{0\}$ and then the density follows. And here is where I'm stuck. I wish it $f$ were continuous function, but $f$ is square integrable only. If the notation is not clear, just tell me and I'll fix it. Thanks for your help.","$\newcommand{\span}{\operatorname{span}}$ Define $e_{0,0}\equiv 1$, and for all $n\in \mathbb{N}$ $$e_{n,k}=\begin{cases} 2^{n/2} &\text{if } \frac{k-1}{2^n}\leq x\lt \frac{k-\frac{1}{2}}{2^n}\\ -2^{n/2}&\text{if } \frac{k-\frac{1}{2}}{2^n}\leq x\lt \frac{k}{2^n}\\ 0 &\text{otherwise} \end{cases}$$ for $k=1,\ldots,2^n$. Let $$H:=\{e_{n,k}:n,k\in \mathbb{N}\}.$$ I want to prove that $H$ is a Hilbert's base for $L^2[0,1]$ with the usal inner product. In order to prove this we must show that $H$ is orthonormal and that $\span(H)$ is dense in $L^2[0,1]$. Here is a good place to begin to see the orthonormality. For the second thing I have the following exercise: Let $f\in H^{\bot}$, i.e. $f$ is such that for all $n\in \mathbb{N}$ $$\int_0^1 f(x)e_{n,k}(x)dx=0,$$ for $k=1,\ldots,2^n$. Show that for all $n\in \mathbb{N}$ $$\int_0^1f\cdot 1_{[0,k/2^{n})}=0,$$ $k=1,\ldots,2^n$. Conclude that $f\equiv 0$. The exercise show that $(\overline{\span(H)})^{\bot}=\{0\}$ and then the density follows. And here is where I'm stuck. I wish it $f$ were continuous function, but $f$ is square integrable only. If the notation is not clear, just tell me and I'll fix it. Thanks for your help.",,"['real-analysis', 'measure-theory', 'functional-analysis', 'hilbert-spaces']"
31,Find supremum of an integral,Find supremum of an integral,,"Let $F$ be the set of all continuous functions $f : [1, 3] \to [-1, 1]$ such that $\int_{1}^{3} f(x) = 0.$ Then, find $$ \sup \int_{1}^{3} \frac{f(x)}{x} dx,$$ where $f \in F$ . Now, my first thought was to try Cauchy - Schwarz inequality, but it can only be applied when all terms are positive, but $f(x)$ is not always positive. Then, I tried to bound $f(x)$ using range of $f$ . So, $ -1 \le f(x) \le 1, \forall x \in [1,3] \implies \frac{-1}{x} \le \frac{f(x)}{x} \le \frac{1}{x}, \forall x \in [1,3]$ . We can multiply with $\frac{1}{x}$ since it is defined in the domain $[1,3]$ . Then, we integrate, which we can, since all three of them are integrable on $[1,3]$ . Since we need to find an upper bound we can ignore the left inequality and proceed as follows: $$\int_{1}^{3} \frac{f(x)}{x} dx \le \int_{1}^{3} \frac{1}{x} dx = \ln3.$$ Now, I have no idea how to show it is the least upper bound (I tried using the definition of LUB, assuming another arbitrary upper bound and showing that $\ln3$ is smaller, but it didn't work). Also, I suspect it may not be the LUB at all. Where am I going wrong?","Let be the set of all continuous functions such that Then, find where . Now, my first thought was to try Cauchy - Schwarz inequality, but it can only be applied when all terms are positive, but is not always positive. Then, I tried to bound using range of . So, . We can multiply with since it is defined in the domain . Then, we integrate, which we can, since all three of them are integrable on . Since we need to find an upper bound we can ignore the left inequality and proceed as follows: Now, I have no idea how to show it is the least upper bound (I tried using the definition of LUB, assuming another arbitrary upper bound and showing that is smaller, but it didn't work). Also, I suspect it may not be the LUB at all. Where am I going wrong?","F f : [1, 3] \to [-1, 1] \int_{1}^{3} f(x) = 0.  \sup \int_{1}^{3} \frac{f(x)}{x} dx, f \in F f(x) f(x) f  -1 \le f(x) \le 1, \forall x \in [1,3] \implies \frac{-1}{x} \le \frac{f(x)}{x} \le \frac{1}{x}, \forall x \in [1,3] \frac{1}{x} [1,3] [1,3] \int_{1}^{3} \frac{f(x)}{x} dx \le \int_{1}^{3} \frac{1}{x} dx = \ln3. \ln3","['real-analysis', 'inequality', 'definite-integrals', 'supremum-and-infimum', 'upper-lower-bounds']"
32,"Strategies for solving the infinite integral $\int_0^\infty \frac{k}{k^2+\alpha^2} \, J_0(k) J_0(kr) \, \mathrm{d}k $ where $r \ge 0$ and $\alpha > 0$",Strategies for solving the infinite integral  where  and,"\int_0^\infty \frac{k}{k^2+\alpha^2} \, J_0(k) J_0(kr) \, \mathrm{d}k  r \ge 0 \alpha > 0","While conducting an analysis of an inverse Fourier transform stemming from a fluid mechanics problem related to flows in porous media, I encountered the following infinite integral: $$ f(r) = \int_0^\infty \frac{k}{k^2+\alpha^2} \, J_0(k) J_0(kr) \, \mathrm{d}k \, ,  $$ where $r \ge 0$ and $\alpha > 0$ . It's evident that the integrand behaves as $\mathcal{O}\left( k^{-2} \right)$ as $k \to \infty$ , indicating the convergence of the integral. My attempted approach involved utilizing the classical expression of the zeroth-order Bessel function: $$ J_0(u) = \frac{1}{2\pi} \int_0^{2\pi} \exp \left(-iu \sin t \right) \, \mathrm{d} t $$ I applied this expression to one as well as to both Bessel functions and attempted to evaluate them. Unfortunately, my efforts have not led to a successful result. If anyone here can offer guidance or provide hints that might assist in evaluating this integral, I would greatly appreciate it. E D I T Based on numerical tests, it appears that for $\alpha = 1$ , $f(r)$ exhibits a proportionality to the modified Bessel function of the second kind, $K_0(r)$ . The approximate proportionality coefficient appears to be around $1.2660658\dots$ . For other values of $\alpha$ , the trend is less discernible. It's possible that the behavior is a combination of multiple modified Bessel functions of the second kind, but this is a speculative hypothesis derived from intuition and not necessarily a reflection of reality.","While conducting an analysis of an inverse Fourier transform stemming from a fluid mechanics problem related to flows in porous media, I encountered the following infinite integral: where and . It's evident that the integrand behaves as as , indicating the convergence of the integral. My attempted approach involved utilizing the classical expression of the zeroth-order Bessel function: I applied this expression to one as well as to both Bessel functions and attempted to evaluate them. Unfortunately, my efforts have not led to a successful result. If anyone here can offer guidance or provide hints that might assist in evaluating this integral, I would greatly appreciate it. E D I T Based on numerical tests, it appears that for , exhibits a proportionality to the modified Bessel function of the second kind, . The approximate proportionality coefficient appears to be around . For other values of , the trend is less discernible. It's possible that the behavior is a combination of multiple modified Bessel functions of the second kind, but this is a speculative hypothesis derived from intuition and not necessarily a reflection of reality.","
f(r) = \int_0^\infty \frac{k}{k^2+\alpha^2} \, J_0(k) J_0(kr) \, \mathrm{d}k \, , 
 r \ge 0 \alpha > 0 \mathcal{O}\left( k^{-2} \right) k \to \infty 
J_0(u) = \frac{1}{2\pi} \int_0^{2\pi} \exp \left(-iu \sin t \right) \, \mathrm{d} t
 \alpha = 1 f(r) K_0(r) 1.2660658\dots \alpha","['real-analysis', 'calculus', 'integration', 'improper-integrals', 'bessel-functions']"
33,Harmonic series multiplied by $-2$ every third term converges to $\ln 3$,Harmonic series multiplied by  every third term converges to,-2 \ln 3,"Prove that the series $$\sum_{n=1}^\infty a_n = 1+\frac 1 2 -2\cdot \frac 1 3+\frac 1 4 + \frac 1 5 - 2\cdot \frac 1 6 + \cdots $$ converges to $\ln (3)$ . Inserting parentheses every three terms (inserting parentheses of bounded lengths does not damage convergence), we have $$ \begin {align} \sum_{n=1}^\infty a_n  &= \sum_{k=1}^\infty  \Big( \frac 1 {3k-2} + \frac 1 {3k-1} - \frac 2 {3k} \Big) \\ &=\sum_{k=1}^\infty  \frac {9k-4} {27k^3-27k^2+6k}, \end{align} $$ so the series converges by comparing with $\sum 1/k^2$ . However, I don't see a direct approach to showing that it converges to $\ln (3)$ . Is it possible to do so with Riemann sums? Or using power series of $\ln (2\cdot x/2) = \ln (2) + \ln(x/2)$ ? EDIT: What I refer to by saying ""inserting parentheses of bounded length"" is the following theorem: Let $(a_n)_{n=1}^\infty$ a sequence tending to $0$ . Let $(n_k)_{k=0}^{\infty}$ a strictly ascending sequence of nonnegative integers such that $n_0=0$ , and define the regrouping $b_k=\sum_{n=n_{k-1}+1}^{n_k}a_n$ for all $k\ge 1$ . Then, if $(\Delta n_k)_{k=1}^\infty = (n_k - n_{k-1})_{k=1}^\infty$ is a bounded sequence, the series $\sum_{n=1}^\infty a_n$ converges if and only if $\sum_{k=1}^\infty b_k$ converges, and in this case, both tend to the same limit. (And moreover, this is true also in the more general case in which $\sum_{n=n_{k-1}+1}^{n_k} |a_n| $ tends to $0$ as $k\to \infty$ ).","Prove that the series converges to . Inserting parentheses every three terms (inserting parentheses of bounded lengths does not damage convergence), we have so the series converges by comparing with . However, I don't see a direct approach to showing that it converges to . Is it possible to do so with Riemann sums? Or using power series of ? EDIT: What I refer to by saying ""inserting parentheses of bounded length"" is the following theorem: Let a sequence tending to . Let a strictly ascending sequence of nonnegative integers such that , and define the regrouping for all . Then, if is a bounded sequence, the series converges if and only if converges, and in this case, both tend to the same limit. (And moreover, this is true also in the more general case in which tends to as ).","\sum_{n=1}^\infty a_n = 1+\frac 1 2 -2\cdot \frac 1 3+\frac 1 4 + \frac 1 5 - 2\cdot \frac 1 6 + \cdots  \ln (3)  \begin {align} \sum_{n=1}^\infty a_n  &= \sum_{k=1}^\infty  \Big( \frac 1 {3k-2} + \frac 1 {3k-1} - \frac 2 {3k} \Big) \\
&=\sum_{k=1}^\infty  \frac {9k-4} {27k^3-27k^2+6k}, \end{align}
 \sum 1/k^2 \ln (3) \ln (2\cdot x/2) = \ln (2) + \ln(x/2) (a_n)_{n=1}^\infty 0 (n_k)_{k=0}^{\infty} n_0=0 b_k=\sum_{n=n_{k-1}+1}^{n_k}a_n k\ge 1 (\Delta n_k)_{k=1}^\infty = (n_k - n_{k-1})_{k=1}^\infty \sum_{n=1}^\infty a_n \sum_{k=1}^\infty b_k \sum_{n=n_{k-1}+1}^{n_k} |a_n|  0 k\to \infty","['real-analysis', 'sequences-and-series', 'limits', 'convergence-divergence', 'harmonic-numbers']"
34,The main idea behind Big O notation,The main idea behind Big O notation,,"Well, when we use Big O notation we never know even approximate number of steps of a given algorithm, right? For example, if we have $O(n)$ algorithm, then we don't know how fast this algorithm itself is: its exact number of steps may be $10n$ or even $10^{100}n$ . Thus we just put this algorithm in particular ""class of functions"" $O(n)$ . The reason for doing that is for us to be able to compare algorithms from different ""classes"". For example, we always know that $O(n)$ is going to be slower than $O(\log_2 n)$ for large $n$ , because $\lim_{n \to +\infty} \frac{\log_2 n}{n} = 0$ . But if we are told that there are two different $O(n)$ algorithms, then we can't say which one is going to be faster, right? Moreover, if we are given a single $O(n)$ algorithm, we can't say how fast it is (because it may take $10n$ or even $10^{100}n$ steps and still be $O(n)$ ). Is it the main and the only idea behind Big O notation? Or is there something else that I missed? I've already googled it million times and still have this confusion in my head. So, please, check my understanding and say if anything is wrong. Thanks in advance!","Well, when we use Big O notation we never know even approximate number of steps of a given algorithm, right? For example, if we have algorithm, then we don't know how fast this algorithm itself is: its exact number of steps may be or even . Thus we just put this algorithm in particular ""class of functions"" . The reason for doing that is for us to be able to compare algorithms from different ""classes"". For example, we always know that is going to be slower than for large , because . But if we are told that there are two different algorithms, then we can't say which one is going to be faster, right? Moreover, if we are given a single algorithm, we can't say how fast it is (because it may take or even steps and still be ). Is it the main and the only idea behind Big O notation? Or is there something else that I missed? I've already googled it million times and still have this confusion in my head. So, please, check my understanding and say if anything is wrong. Thanks in advance!",O(n) 10n 10^{100}n O(n) O(n) O(\log_2 n) n \lim_{n \to +\infty} \frac{\log_2 n}{n} = 0 O(n) O(n) 10n 10^{100}n O(n),"['real-analysis', 'asymptotics', 'big-picture']"
35,Why doesn't nonexistence of $\lim_{x \to \infty^+}$ and $\lim_{x \to -\infty^-}$ cause limits at infinity to be undefined?,Why doesn't nonexistence of  and  cause limits at infinity to be undefined?,\lim_{x \to \infty^+} \lim_{x \to -\infty^-},"One criterion for checking existence of limits is to check that and one-sided limits from left and right exist and agree: (Theorem) Let $f$ be a real-valued function. One-sided limits of $f$ as $x$ approaches $a$ from the left and right exist and equal $L$ : $$ \lim_{x \to a^-} f(x) = \lim_{x \to a^+} f(x) = L $$ if and only if the two-sided limit of $f$ at $a$ exists and also equals $L$ , $$\lim_{x \to a} f(x) = L.$$ The contrapositive of this statement can be used to conclude that a limit does not exist: (Contrapositive) Let $f$ be a real-valued function. One-sided limits of $f$ as $x$ approaches $a$ from the left and right do not exist or do not agree if and only if the limit of $f$ at $a$ does not exist. When applying this to a variety of contexts, you can come up with some pretty weird examples and weird results. In the above figure, it seems to me that $\lim_{x \to 3} f(x)$ does not exist since $\lim_{x \to 3^+} f(x)$ does not exist. $\lim_{x \to 4} f(x)$ does not exist since $\lim_{x \to 4^-} f(x)$ and $\lim_{x \to 4^-} f(x)$ do not exist. $\lim_{x \to 5} f(x)$ does not exist since $\lim_{x \to 5^-} f(x)$ does not exist. A bit more controversial is if you apply the same to the limits at infinity, which would stand to reason that $\lim_{x \to \infty} f(x)$ does not exist since $\lim_{x \to \infty^+} f(x)$ does not exist. $\lim_{x \to -\infty} f(x)$ does not exist since $\lim_{x \to \infty^-} f(x)$ does not exist. However to contradict the above, many people would write $\lim_{x \to \infty} f(x)=2$ and $\lim_{x \to -\infty} f(x)=-1$ . TLDR; why does the nonexistence of left and right limits not cause limits at infinity to be undefined but does cause the limit at $x=3$ , $x=4$ , and $x=5$ to not exist? Real analysis answers are welcome. Edit Thanks to answers from Troposphere and Joe, I worked out some more careful definitions and theorems: Definition (Limit) Let $f$ be a function whose domain is a subset of $\mathbb{R}$ , and let $a, L \in \mathbb{R}$ . We say that the limit of $f$ as $x$ approaches $a$ is $L$ , \begin{equation*} \lim_{x \to a} f(x)=L \end{equation*} to mean that $a$ is an accumulation point of $\textrm{dom}(f)$ and for any $\epsilon > 0$ , there exists $\delta >0$ such that if $x \in \textrm{dom}(f)$ is within $\delta$ of $a$ (with $x \ne a$ ), then $f(x)$ is within $\epsilon$ of $L$ : \begin{equation*} 0 < |x-a| < \delta \quad \rightarrow \quad 0 < |f(x)-L| < \epsilon. \end{equation*} Definition (One-Sided Limits) Let $f$ be a function whose domain is a subset of $\mathbb{R}$ , and let $a, L \in \mathbb{R}$ .: We say the limit of $f$ as $x$ approaches $a$ from the left is $L$ , $$\lim_{x \to a^-} f(x)=L,$$ to mean that $a$ is an accumulation point of $\textrm{dom}(f) \cap (-\infty,a]$ and for any $\epsilon > 0$ , there exists $\delta >0$ such that if $x$ is within $\delta$ of $a$ (for $x < a$ ), then $f(x)$ is within $\epsilon$ of $L$ : \begin{equation*} 0 < a-x < \delta \quad \rightarrow \quad 0 < |f(x)-L| < \epsilon. \end{equation*} We say the limit of $f$ as $x$ approaches $a$ from the right is $L$ , \begin{equation*} \lim_{x \to a^+} f(x)=L, \end{equation*} to mean that $a$ is an accumulation point of $\textrm{dom}(f) \cap [a,\infty)$ and for any $\epsilon > 0$ , there exists $\delta >0$ such that if $x$ is within $\delta$ of $a$ (for $x > a$ ), then $f(x)$ is within $\epsilon$ of $L$ : \begin{equation*} 0 < x-a < \delta \quad \rightarrow \quad 0 < |f(x)-L| < \epsilon. \end{equation*} Theorem (One-Sided and Two-Sided Limits Relationship) Let $f$ be a function whose domain is a subset of $\mathbb{R}$ , and let $a, L \in \mathbb{R}$ . Suppose a is an accumulation point of dom(f) and that $f$ is defined everywhere in some punctured neighborhood of $a$ . Then One sided limits of $f$ from left and right at $a$ exist and equal $L$ , $$\lim_{x \to a^-} f(x) = L \textrm{ and }\lim_{x \to a^+} f(x) = L,$$ if and only if the two-sided limit of $f$ at $a$ exists and also equals $L$ : $$\lim_{x \to a} f(x) = L.$$ Contrapositive (One-Sided and Two-Sided Limits Relationship) Let $f$ be a function whose domain is a subset of $\mathbb{R}$ , and let $a, L \in \mathbb{R}$ . Suppose a is an accumulation point of dom(f) and that $f$ is defined everywhere in some punctured neighborhood of $a$ . Then At least one of the one sided limits of $f$ from left and right at $a$ does not exist or do not equal $L$ : $$\lim_{x \to a^-} f(x) \ne L \textrm{ or }\lim_{x \to a^+} f(x) \ne L,$$ if and only if the two-sided limit of $f$ at $a$ does not exist or does not equal $L$ : $$\lim_{x \to a} f(x) \ne L.$$ Under these definitions and theorems, I think (hope) we get the conclusions we expect: $\lim_{x \to 3} f(x)$ exists $\lim_{x \to 5} f(x)$ exists. and $\lim_{x \to 3^+} f(x)$ DNE. $\lim_{x \to 4} f(x)$ DNE. $\lim_{x \to 5^-} f(x)$ DNE.","One criterion for checking existence of limits is to check that and one-sided limits from left and right exist and agree: (Theorem) Let be a real-valued function. One-sided limits of as approaches from the left and right exist and equal : if and only if the two-sided limit of at exists and also equals , The contrapositive of this statement can be used to conclude that a limit does not exist: (Contrapositive) Let be a real-valued function. One-sided limits of as approaches from the left and right do not exist or do not agree if and only if the limit of at does not exist. When applying this to a variety of contexts, you can come up with some pretty weird examples and weird results. In the above figure, it seems to me that does not exist since does not exist. does not exist since and do not exist. does not exist since does not exist. A bit more controversial is if you apply the same to the limits at infinity, which would stand to reason that does not exist since does not exist. does not exist since does not exist. However to contradict the above, many people would write and . TLDR; why does the nonexistence of left and right limits not cause limits at infinity to be undefined but does cause the limit at , , and to not exist? Real analysis answers are welcome. Edit Thanks to answers from Troposphere and Joe, I worked out some more careful definitions and theorems: Definition (Limit) Let be a function whose domain is a subset of , and let . We say that the limit of as approaches is , to mean that is an accumulation point of and for any , there exists such that if is within of (with ), then is within of : Definition (One-Sided Limits) Let be a function whose domain is a subset of , and let .: We say the limit of as approaches from the left is , to mean that is an accumulation point of and for any , there exists such that if is within of (for ), then is within of : We say the limit of as approaches from the right is , to mean that is an accumulation point of and for any , there exists such that if is within of (for ), then is within of : Theorem (One-Sided and Two-Sided Limits Relationship) Let be a function whose domain is a subset of , and let . Suppose a is an accumulation point of dom(f) and that is defined everywhere in some punctured neighborhood of . Then One sided limits of from left and right at exist and equal , if and only if the two-sided limit of at exists and also equals : Contrapositive (One-Sided and Two-Sided Limits Relationship) Let be a function whose domain is a subset of , and let . Suppose a is an accumulation point of dom(f) and that is defined everywhere in some punctured neighborhood of . Then At least one of the one sided limits of from left and right at does not exist or do not equal : if and only if the two-sided limit of at does not exist or does not equal : Under these definitions and theorems, I think (hope) we get the conclusions we expect: exists exists. and DNE. DNE. DNE.","f f x a L 
\lim_{x \to a^-} f(x) = \lim_{x \to a^+} f(x) = L
 f a L \lim_{x \to a} f(x) = L. f f x a f a \lim_{x \to 3} f(x) \lim_{x \to 3^+} f(x) \lim_{x \to 4} f(x) \lim_{x \to 4^-} f(x) \lim_{x \to 4^-} f(x) \lim_{x \to 5} f(x) \lim_{x \to 5^-} f(x) \lim_{x \to \infty} f(x) \lim_{x \to \infty^+} f(x) \lim_{x \to -\infty} f(x) \lim_{x \to \infty^-} f(x) \lim_{x \to \infty} f(x)=2 \lim_{x \to -\infty} f(x)=-1 x=3 x=4 x=5 f \mathbb{R} a, L \in \mathbb{R} f x a L \begin{equation*} \lim_{x \to a} f(x)=L \end{equation*} a \textrm{dom}(f) \epsilon > 0 \delta >0 x \in \textrm{dom}(f) \delta a x \ne a f(x) \epsilon L \begin{equation*} 0 < |x-a| < \delta \quad \rightarrow \quad 0 < |f(x)-L| < \epsilon.
\end{equation*} f \mathbb{R} a, L \in \mathbb{R} f x a L \lim_{x \to a^-} f(x)=L, a \textrm{dom}(f) \cap (-\infty,a] \epsilon > 0 \delta >0 x \delta a x < a f(x) \epsilon L \begin{equation*} 0 < a-x < \delta \quad \rightarrow \quad 0 < |f(x)-L| < \epsilon. \end{equation*} f x a L \begin{equation*} \lim_{x \to a^+} f(x)=L, \end{equation*} a \textrm{dom}(f) \cap [a,\infty) \epsilon > 0 \delta >0 x \delta a x > a f(x) \epsilon L \begin{equation*} 0 < x-a < \delta \quad \rightarrow \quad 0 < |f(x)-L| < \epsilon. \end{equation*} f \mathbb{R} a, L \in \mathbb{R} f a f a L \lim_{x \to a^-} f(x) = L \textrm{ and }\lim_{x \to a^+} f(x) = L, f a L \lim_{x \to a} f(x) = L. f \mathbb{R} a, L \in \mathbb{R} f a f a L \lim_{x \to a^-} f(x) \ne L \textrm{ or }\lim_{x \to a^+} f(x) \ne L, f a L \lim_{x \to a} f(x) \ne L. \lim_{x \to 3} f(x) \lim_{x \to 5} f(x) \lim_{x \to 3^+} f(x) \lim_{x \to 4} f(x) \lim_{x \to 5^-} f(x)","['real-analysis', 'calculus', 'limits', 'definition', 'infinity']"
36,Are there any elementary functions $\beta(x)$ that follows this integral $\int_{y-1}^{y} \beta(x) dx =\cos(y)$,Are there any elementary functions  that follows this integral,\beta(x) \int_{y-1}^{y} \beta(x) dx =\cos(y),Are there any simple functions $\beta(x)$ that follows this integral $$\int_{y-1}^{y} \beta(x) dx =\cos(y)$$ I think there is an infinite amount of solutions that are continuous everywhere but how can I find one that only uses elementary functions?,Are there any simple functions that follows this integral I think there is an infinite amount of solutions that are continuous everywhere but how can I find one that only uses elementary functions?,\beta(x) \int_{y-1}^{y} \beta(x) dx =\cos(y),"['real-analysis', 'calculus']"
37,How come the $\epsilon-\delta$ definition of continuity is preferred over the sequential definition of continuity?,How come the  definition of continuity is preferred over the sequential definition of continuity?,\epsilon-\delta,"I've personally found things are a lot easier to prove using the sequences rather than the $\epsilon-\delta$ method. I do understand the $\epsilon-\delta$ is more intuitive, but it's quite difficult to prove a function is continuous on $x_0 \in D$ since it implies finding a function that takes $\epsilon$ and $x_0$ as arguments and outputs some $\delta>0$ , or at least proving such a function exists. This issue is even more prevalent when proving a function is uniformly continuous. On all of my homework problems involving uniform continuity, I always tried to use the $\epsilon-\delta$ definition, yet I always failed. When searching for the solution, the proofs are quite confusing and complex. I would then switch to the sequence definition and would succeed in solving the problem quickly. My professor says that the mathematics community nonetheless refers the $\epsilon-\delta$ definition, so I am curious why when it's a more difficult path. Here are the definitions. For a function $f:D \rightarrow \mathbb{R}$ , Continuity : $f$ is continuous at $x_0 \in D$ if for any sequence $(x_n)$ in $D$ where $\lim_{n \rightarrow \infty} x_n=x_0$ , it follows $\lim_{n \rightarrow \infty} f(x_n)=f(x_0)$ . Uniform continuity: $f$ is uniformly continuous if for any sequences $(u_n),(v_n)$ in $D$ where $\lim_{n \rightarrow \infty} (u_n-v_n)=0$ , it follows $\lim_{n \rightarrow \infty}(f(u_n)-f(v_n))=0$ . An instance of this is proving the function $f:[0,1) \rightarrow \infty$ where $f(x)=\frac{1}{1-x}$ is not uniformly continuous. I found it very difficult to prove this using the $\epsilon-\delta$ definition, and the proofs I found online were quite confusing and there's definitely no way I would've come across them by myself. The sequential definition however was very simple. I simply used the sequences $u_n=1-\frac{1}{n^2}$ and $v_n=1-\frac{1}{n}$ . The differences of the sequences converge to 0, and the differences of the image of the sequences diverge to $\infty$ . $0 \not = \infty$ and therefore $f$ is not uniformly continuous. Another example is showing that any continuous function whose domain is a closed interval is uniformly continuous. There's no way I could come up with a way to prove this using the $\epsilon-\delta$ definition, but I definitely could using the sequence definition. I may be wrong, but I believe I read once that for some metric spaces, continuity and sequential continuity are not equivalent. I don't know much about that matter though, but would love to learn about it.","I've personally found things are a lot easier to prove using the sequences rather than the method. I do understand the is more intuitive, but it's quite difficult to prove a function is continuous on since it implies finding a function that takes and as arguments and outputs some , or at least proving such a function exists. This issue is even more prevalent when proving a function is uniformly continuous. On all of my homework problems involving uniform continuity, I always tried to use the definition, yet I always failed. When searching for the solution, the proofs are quite confusing and complex. I would then switch to the sequence definition and would succeed in solving the problem quickly. My professor says that the mathematics community nonetheless refers the definition, so I am curious why when it's a more difficult path. Here are the definitions. For a function , Continuity : is continuous at if for any sequence in where , it follows . Uniform continuity: is uniformly continuous if for any sequences in where , it follows . An instance of this is proving the function where is not uniformly continuous. I found it very difficult to prove this using the definition, and the proofs I found online were quite confusing and there's definitely no way I would've come across them by myself. The sequential definition however was very simple. I simply used the sequences and . The differences of the sequences converge to 0, and the differences of the image of the sequences diverge to . and therefore is not uniformly continuous. Another example is showing that any continuous function whose domain is a closed interval is uniformly continuous. There's no way I could come up with a way to prove this using the definition, but I definitely could using the sequence definition. I may be wrong, but I believe I read once that for some metric spaces, continuity and sequential continuity are not equivalent. I don't know much about that matter though, but would love to learn about it.","\epsilon-\delta \epsilon-\delta x_0 \in D \epsilon x_0 \delta>0 \epsilon-\delta \epsilon-\delta f:D \rightarrow \mathbb{R} f x_0 \in D (x_n) D \lim_{n \rightarrow \infty} x_n=x_0 \lim_{n \rightarrow \infty} f(x_n)=f(x_0) f (u_n),(v_n) D \lim_{n \rightarrow \infty} (u_n-v_n)=0 \lim_{n \rightarrow \infty}(f(u_n)-f(v_n))=0 f:[0,1) \rightarrow \infty f(x)=\frac{1}{1-x} \epsilon-\delta u_n=1-\frac{1}{n^2} v_n=1-\frac{1}{n} \infty 0 \not = \infty f \epsilon-\delta","['real-analysis', 'continuity', 'uniform-continuity']"
38,Conjecture: $\sum\limits_{n\geq0}\left(\frac12\right)^n\prod\limits_{k=1}^{n}\frac{2n-2k+1}{2n-2k+2}=\sqrt2$ [duplicate],Conjecture:  [duplicate],\sum\limits_{n\geq0}\left(\frac12\right)^n\prod\limits_{k=1}^{n}\frac{2n-2k+1}{2n-2k+2}=\sqrt2,"This question already has answers here : Summing the power series $\sum\limits_{n=0}^\infty (-1)^n \frac{x^{2n+1}}{2n+1}\prod\limits_{k=1}^n\frac{2k-1}{2k} $ (5 answers) Closed 5 years ago . I am trying to solve a problem I made form myself: proving that $$\sum_{n\geq0}\left(\frac12\right)^n\prod_{k=1}^{n}\frac{2n-2k+1}{2n-2k+2}=\sqrt2$$ The highly accurate powers of Desmos seem to confirm my hunch. But how do I prove it? I was just messing around with products and generating functions, and then I noticed that the numerical value of the sum in question was suspiciously similar to $\sqrt2$ , so I conjectured the result. Unfortunately I found this completely by accident and have absolutely no idea of how to prove it. Feeble attempt: Define $$S(x)=\sum_{n\geq0}x^n\prod_{k=1}^{n}\frac{2n-2k+1}{2n-2k+2}$$ Which Wolfram says is equal to $$S(x)=\sum_{n\geq0}x^n\frac{(1/2-n)_n}{(-n)_n}$$ With $\displaystyle (x)_n=\frac{\Gamma(x+n)}{\Gamma(x)}$ . But that doesn't really make sense because $\Gamma(0)$ is undefined. So all in all I'm just confused. Could I have some help? Edit: According to the comments, it suffices to prove that $$\sum_{n\geq1}\left(\frac12\right)^n\prod_{k=1}^{n}\frac{2n-2k+1}{2n-2k+2}=\sqrt2-1$$","This question already has answers here : Summing the power series $\sum\limits_{n=0}^\infty (-1)^n \frac{x^{2n+1}}{2n+1}\prod\limits_{k=1}^n\frac{2k-1}{2k} $ (5 answers) Closed 5 years ago . I am trying to solve a problem I made form myself: proving that The highly accurate powers of Desmos seem to confirm my hunch. But how do I prove it? I was just messing around with products and generating functions, and then I noticed that the numerical value of the sum in question was suspiciously similar to , so I conjectured the result. Unfortunately I found this completely by accident and have absolutely no idea of how to prove it. Feeble attempt: Define Which Wolfram says is equal to With . But that doesn't really make sense because is undefined. So all in all I'm just confused. Could I have some help? Edit: According to the comments, it suffices to prove that",\sum_{n\geq0}\left(\frac12\right)^n\prod_{k=1}^{n}\frac{2n-2k+1}{2n-2k+2}=\sqrt2 \sqrt2 S(x)=\sum_{n\geq0}x^n\prod_{k=1}^{n}\frac{2n-2k+1}{2n-2k+2} S(x)=\sum_{n\geq0}x^n\frac{(1/2-n)_n}{(-n)_n} \displaystyle (x)_n=\frac{\Gamma(x+n)}{\Gamma(x)} \Gamma(0) \sum_{n\geq1}\left(\frac12\right)^n\prod_{k=1}^{n}\frac{2n-2k+1}{2n-2k+2}=\sqrt2-1,"['real-analysis', 'sequences-and-series', 'combinatorics', 'closed-form']"
39,On $\int_0^{2\pi}e^{\cos2x}\cos(\sin2x)\ \mathrm{d}x=2\pi$,On,\int_0^{2\pi}e^{\cos2x}\cos(\sin2x)\ \mathrm{d}x=2\pi,"Here's my attempt at an integral I found on this site. $$\int_0^{2\pi}e^{\cos2x}\cos(\sin2x)\ \mathrm{d}x=2\pi$$ I'm not asking for a proof, I just want to know where I messed up Recall that, for all $x$ , $$e^x=\sum_{n\geq0}\frac{x^n}{n!}$$ And $$\cos x=\sum_{n\geq0}(-1)^n\frac{x^{2n}}{(2n)!}$$ Hence we have that $$ \begin{align} \int_0^{2\pi}e^{\cos2x}\cos(\sin2x)\ \mathrm{d}x=&\int_0^{2\pi}\bigg(\sum_{n\geq0}\frac{\cos^n2x}{n!}\bigg)\bigg(\sum_{m\geq0}(-1)^m\frac{\sin^{2m}2x}{(2m)!}\bigg)\mathrm{d}x\\ =&\sum_{n,m\geq0}\frac{(-1)^m}{n!(2m)!}\int_0^{2\pi}\cos(2x)^n\sin(2x)^{2m}\mathrm{d}x\\ =&\frac12\sum_{n,m\geq0}\frac{(-1)^m}{n!(2m)!}\int_0^{4\pi}\cos(t)^n\sin(t)^{2m}\mathrm{d}t\\ \end{align} $$ The final integral is related to the incomplete beta function, defined as $$B(x;a,b)=\int_0^x u^{a-1}(1-u)^{b-1}\mathrm{d}u$$ If we define $$I(x;a,b)=\int_0^x\sin(t)^a\cos(t)^b\mathrm{d}t$$ We can make the substitution $\sin^2t=u$ , which gives $$ \begin{align} I(x;a,b)=&\frac12\int_0^{\sin^2x}u^{a/2}(1-u)^{b/2}u^{-1/2}(1-u)^{-1/2}\mathrm{d}u\\ =&\frac12\int_0^{\sin^2x}u^{\frac{a-1}2}(1-u)^{\frac{b-1}2}\mathrm{d}u\\ =&\frac12\int_0^{\sin^2x}u^{\frac{a+1}2-1}(1-u)^{\frac{b+1}2-1}\mathrm{d}u\\ =&\frac12B\bigg(\sin^2x;\frac{a+1}2,\frac{b+1}2\bigg)\\ \end{align} $$ Hence we have a form of our final integral: $$ \begin{align} I(4\pi;2m,n)=&\frac12B\bigg(\sin^24\pi;\frac{2m+1}2,\frac{n+1}2\bigg)\\ =&\frac12B\bigg(0;\frac{2m+1}2,\frac{n+1}2\bigg)\\ =&\frac12\int_0^0t^{\frac{2m-1}2}(1-t)^{\frac{n-1}2}\mathrm{d}t\\ =&\,0 \end{align} $$ Which implies that $$\int_0^{2\pi}e^{\cos2x}\cos(\sin2x)\ \mathrm{d}x=0$$ Which is totally wrong. But as far as I can tell, I haven't broken any rules. Where's my error, and how do I fix it? Thanks.","Here's my attempt at an integral I found on this site. I'm not asking for a proof, I just want to know where I messed up Recall that, for all , And Hence we have that The final integral is related to the incomplete beta function, defined as If we define We can make the substitution , which gives Hence we have a form of our final integral: Which implies that Which is totally wrong. But as far as I can tell, I haven't broken any rules. Where's my error, and how do I fix it? Thanks.","\int_0^{2\pi}e^{\cos2x}\cos(\sin2x)\ \mathrm{d}x=2\pi x e^x=\sum_{n\geq0}\frac{x^n}{n!} \cos x=\sum_{n\geq0}(-1)^n\frac{x^{2n}}{(2n)!} 
\begin{align}
\int_0^{2\pi}e^{\cos2x}\cos(\sin2x)\ \mathrm{d}x=&\int_0^{2\pi}\bigg(\sum_{n\geq0}\frac{\cos^n2x}{n!}\bigg)\bigg(\sum_{m\geq0}(-1)^m\frac{\sin^{2m}2x}{(2m)!}\bigg)\mathrm{d}x\\
=&\sum_{n,m\geq0}\frac{(-1)^m}{n!(2m)!}\int_0^{2\pi}\cos(2x)^n\sin(2x)^{2m}\mathrm{d}x\\
=&\frac12\sum_{n,m\geq0}\frac{(-1)^m}{n!(2m)!}\int_0^{4\pi}\cos(t)^n\sin(t)^{2m}\mathrm{d}t\\
\end{align}
 B(x;a,b)=\int_0^x u^{a-1}(1-u)^{b-1}\mathrm{d}u I(x;a,b)=\int_0^x\sin(t)^a\cos(t)^b\mathrm{d}t \sin^2t=u 
\begin{align}
I(x;a,b)=&\frac12\int_0^{\sin^2x}u^{a/2}(1-u)^{b/2}u^{-1/2}(1-u)^{-1/2}\mathrm{d}u\\
=&\frac12\int_0^{\sin^2x}u^{\frac{a-1}2}(1-u)^{\frac{b-1}2}\mathrm{d}u\\
=&\frac12\int_0^{\sin^2x}u^{\frac{a+1}2-1}(1-u)^{\frac{b+1}2-1}\mathrm{d}u\\
=&\frac12B\bigg(\sin^2x;\frac{a+1}2,\frac{b+1}2\bigg)\\
\end{align}
 
\begin{align}
I(4\pi;2m,n)=&\frac12B\bigg(\sin^24\pi;\frac{2m+1}2,\frac{n+1}2\bigg)\\
=&\frac12B\bigg(0;\frac{2m+1}2,\frac{n+1}2\bigg)\\
=&\frac12\int_0^0t^{\frac{2m-1}2}(1-t)^{\frac{n-1}2}\mathrm{d}t\\
=&\,0
\end{align}
 \int_0^{2\pi}e^{\cos2x}\cos(\sin2x)\ \mathrm{d}x=0","['real-analysis', 'integration', 'special-functions']"
40,Prove that $e^x \cos (\sqrt{x^2+1}) \leq 1$,Prove that,e^x \cos (\sqrt{x^2+1}) \leq 1,"I would like to prove that : $$\forall x \in [0,1], e^x\cos(\sqrt{x^2+1}) \leq 1$$ When plotting the graph this inequality is not sharp at all and we even have : $$\forall x \in [0,1], e^x \cos(\sqrt{x^2+1}) \leq 0.8$$ I tried several things such has : Calculating the derivative and try to apply the mean value theorem to get an upper bound, but the derivative is hard to manipulate and it doesn窶冲 seem I am getting something. Moreover trying something on convexity but once again this is difficult due to the horrible looking of the derivative. I am very interested in sharper upper bound, even if I can窶冲 manage to prove the inequality for $1$ ...","I would like to prove that : When plotting the graph this inequality is not sharp at all and we even have : I tried several things such has : Calculating the derivative and try to apply the mean value theorem to get an upper bound, but the derivative is hard to manipulate and it doesn窶冲 seem I am getting something. Moreover trying something on convexity but once again this is difficult due to the horrible looking of the derivative. I am very interested in sharper upper bound, even if I can窶冲 manage to prove the inequality for ...","\forall x \in [0,1], e^x\cos(\sqrt{x^2+1}) \leq 1 \forall x \in [0,1], e^x \cos(\sqrt{x^2+1}) \leq 0.8 1","['calculus', 'real-analysis', 'inequality']"
41,If $\frac {a_{(n+1)} }{a_n} < \frac {n^2}{(n+1)^2}$ and if $a_n > 0$ for all n . $\sum a_n$ converges or not,If  and if  for all n .  converges or not,\frac {a_{(n+1)} }{a_n} < \frac {n^2}{(n+1)^2} a_n > 0 \sum a_n,If $\frac {a_{(n+1)} }{a_n} < \frac {n^2}{(n+1)^2}$ and if $a_n > 0$ for all n . Then what can we say about the series $\sum a_n$? Can anyone please help me by giving some hints?  I can not find any counter example to show that the series may not converge. Is it true?,If $\frac {a_{(n+1)} }{a_n} < \frac {n^2}{(n+1)^2}$ and if $a_n > 0$ for all n . Then what can we say about the series $\sum a_n$? Can anyone please help me by giving some hints?  I can not find any counter example to show that the series may not converge. Is it true?,,"['real-analysis', 'sequences-and-series']"
42,Are there sequence $(u_n)\subset \mathbb R^+$ s.t. $\sum_{n=1}^\infty u_n<\infty $ but $nu_n\not\to 0$?,Are there sequence  s.t.  but ?,(u_n)\subset \mathbb R^+ \sum_{n=1}^\infty u_n<\infty  nu_n\not\to 0,"I have an exercise that ask me to prove that if $(u_n)$ is decreasing, $u_n\geq 0$ for all $n$ and $\sum_{n=1}^\infty u_n$ converge, then $nu_n\to 0$. I proved it, but for me this result in fact correct even if we remove $(u_n)$ decreasing. Because I have in mind that $1/n$ is the quickest speed of non convergence, i.e. if $\sum_{n}v_n$ converge, it will have $\alpha $ s.t. $v_n\leq \frac{1}{n^\alpha }<\frac{1}{n}$ for all $n$ for a certain $n$. But if we impose $u_n$ decreasing, maybe my imagination is wrong. So is there a positive sequence s.t. $\sum_{n=1}^\infty u_n$ converge but $nu_n\not\to 0$ ?","I have an exercise that ask me to prove that if $(u_n)$ is decreasing, $u_n\geq 0$ for all $n$ and $\sum_{n=1}^\infty u_n$ converge, then $nu_n\to 0$. I proved it, but for me this result in fact correct even if we remove $(u_n)$ decreasing. Because I have in mind that $1/n$ is the quickest speed of non convergence, i.e. if $\sum_{n}v_n$ converge, it will have $\alpha $ s.t. $v_n\leq \frac{1}{n^\alpha }<\frac{1}{n}$ for all $n$ for a certain $n$. But if we impose $u_n$ decreasing, maybe my imagination is wrong. So is there a positive sequence s.t. $\sum_{n=1}^\infty u_n$ converge but $nu_n\not\to 0$ ?",,"['real-analysis', 'sequences-and-series']"
43,Upper bound on $\sum_{n=1}^\infty q^{(n^2)}$,Upper bound on,\sum_{n=1}^\infty q^{(n^2)},"Let $q\in \mathbb R$ with $|q|<1$. I am looking for an upper bound of the converging series $$ \sum_{n=1}^\infty q^{(n^2)} $$ with respect to $q$. Using the inequality $q^{(n^2)} \le q^{2n-1}$, one gets $$ \sum_{n=1}^\infty q^{(n^2)} \le q^{-1} \sum_{n=1}^\infty q^{2n}   = q\sum_{n=0}^\infty q^{2n} = \frac{q}{1-q^2} $$ Numerical experiments say that this estimate does not reflect the behavior of the sum for $q\to1$. They suggests an upper bound in the order of $\frac1{\sqrt{1-q^2}}$. Is it possible to prove this? Preferably with elementary arguments?","Let $q\in \mathbb R$ with $|q|<1$. I am looking for an upper bound of the converging series $$ \sum_{n=1}^\infty q^{(n^2)} $$ with respect to $q$. Using the inequality $q^{(n^2)} \le q^{2n-1}$, one gets $$ \sum_{n=1}^\infty q^{(n^2)} \le q^{-1} \sum_{n=1}^\infty q^{2n}   = q\sum_{n=0}^\infty q^{2n} = \frac{q}{1-q^2} $$ Numerical experiments say that this estimate does not reflect the behavior of the sum for $q\to1$. They suggests an upper bound in the order of $\frac1{\sqrt{1-q^2}}$. Is it possible to prove this? Preferably with elementary arguments?",,"['real-analysis', 'sequences-and-series', 'power-series']"
44,Prove that $A$ is dense if and only if any non-empty open set in $X$ has an intersection with $A$,Prove that  is dense if and only if any non-empty open set in  has an intersection with,A X A,"Let $(X,d)$ be a metric space and let $A \subseteq X$. Prove that $A$ is dense if and only if any non-empty  open set in $X$ has an intersection with $A$ In my proof $\text{Cl}(A)$ denotes the closure of $A$. I need to prove, equivalently, that $\forall O$ non-empty open subset of $X$: $$\text{Cl}(A)=X\Longleftrightarrow O\cap A\neq \emptyset$$ I feel my attempt has some logical errors, and I kindly request criticism and insight. If my attempt is entirely wrong, please help me with alternative ways. Thank you very much. Here is my attempt: PART 1 ($\Longrightarrow$): We have $\text{Cl}(A) = X$, therefore by definition of the closure: $$\forall a\in X:\forall N\in N(a), N\cap A\neq\emptyset$$ Every neighborhood of every point of $X$ intersects $A$. Therefore one could add, every open neighborhood of every point of $X$ intersects $A$. Let $N_O(a)\subseteq N(a)$ be the set of open neighborhoods of $A$, then by definition: $$\forall a\in X: \forall O\in N_O(a),O\cap A\neq\emptyset$$ Therefore, every non-empty open neighborhood of every point of $X$ intersects  $A$. In other words, every non-empty open subset of $X$ intersects  $A$. PART 2 ($\Longleftarrow$): We have $\forall O \subseteq X$ that $O \cap A \neq \emptyset$. ($O$ is non-empty and open) Let $a \in O$. Assume $\text{Cl}(A) \neq X$, therefore $\text{Cl}(A)^\text{C} \neq \emptyset$. By negating the definition of the closure of $A$, we get $\forall a\in \text{Cl}(A)^\text{C}$: $$\exists N \in N(a): N\cap A = \emptyset$$ $N$ is a neighborhood for $a$, therefore $\exists O$ open: $a \in O \subseteq N$. Therefore, $O \cap A = \emptyset$ but $O \cap A \neq \emptyset$. Contradiction , therefore $\text{Cl}(A) = X$.","Let $(X,d)$ be a metric space and let $A \subseteq X$. Prove that $A$ is dense if and only if any non-empty  open set in $X$ has an intersection with $A$ In my proof $\text{Cl}(A)$ denotes the closure of $A$. I need to prove, equivalently, that $\forall O$ non-empty open subset of $X$: $$\text{Cl}(A)=X\Longleftrightarrow O\cap A\neq \emptyset$$ I feel my attempt has some logical errors, and I kindly request criticism and insight. If my attempt is entirely wrong, please help me with alternative ways. Thank you very much. Here is my attempt: PART 1 ($\Longrightarrow$): We have $\text{Cl}(A) = X$, therefore by definition of the closure: $$\forall a\in X:\forall N\in N(a), N\cap A\neq\emptyset$$ Every neighborhood of every point of $X$ intersects $A$. Therefore one could add, every open neighborhood of every point of $X$ intersects $A$. Let $N_O(a)\subseteq N(a)$ be the set of open neighborhoods of $A$, then by definition: $$\forall a\in X: \forall O\in N_O(a),O\cap A\neq\emptyset$$ Therefore, every non-empty open neighborhood of every point of $X$ intersects  $A$. In other words, every non-empty open subset of $X$ intersects  $A$. PART 2 ($\Longleftarrow$): We have $\forall O \subseteq X$ that $O \cap A \neq \emptyset$. ($O$ is non-empty and open) Let $a \in O$. Assume $\text{Cl}(A) \neq X$, therefore $\text{Cl}(A)^\text{C} \neq \emptyset$. By negating the definition of the closure of $A$, we get $\forall a\in \text{Cl}(A)^\text{C}$: $$\exists N \in N(a): N\cap A = \emptyset$$ $N$ is a neighborhood for $a$, therefore $\exists O$ open: $a \in O \subseteq N$. Therefore, $O \cap A = \emptyset$ but $O \cap A \neq \emptyset$. Contradiction , therefore $\text{Cl}(A) = X$.",,"['real-analysis', 'general-topology', 'analysis', 'metric-spaces']"
45,Additive and multiplicative function.,Additive and multiplicative function.,,"We have $f:\mathbb{R} \to \mathbb{R}$ and we know that $f$ is both additive: $f(x+y)=f(x)+f(y)$ and multiplicative: $f(xy)=f(x)f(y)$ and I found out that this means that $f(x)=0$ for any $x$ or $f(x)=x$ for any $x$ but I don't know how to prove it. Can you help me? I know that if $f$ is continuous or monotone we can show what we want only from the first relation, is this somehow related to the second relation?","We have $f:\mathbb{R} \to \mathbb{R}$ and we know that $f$ is both additive: $f(x+y)=f(x)+f(y)$ and multiplicative: $f(xy)=f(x)f(y)$ and I found out that this means that $f(x)=0$ for any $x$ or $f(x)=x$ for any $x$ but I don't know how to prove it. Can you help me? I know that if $f$ is continuous or monotone we can show what we want only from the first relation, is this somehow related to the second relation?",,"['calculus', 'real-analysis', 'functions']"
46,Help with $ \lim\limits_{t \to 0} \int_{-1}^1 \frac{t}{t^2+x^2} f(x)\ dx$ [closed],Help with  [closed], \lim\limits_{t \to 0} \int_{-1}^1 \frac{t}{t^2+x^2} f(x)\ dx,"Closed. This question does not meet Mathematics Stack Exchange guidelines . It is not currently accepting answers. Please provide additional context , which ideally explains why the question is relevant to you and our  community. Some forms of context include: background and motivation, relevant definitions, source, possible strategies, your current progress, why the question is interesting or important, etc. Closed 5 years ago . Improve this question Show that if $f$ is continuous on $[-1,1]$, then $$ \lim_{t \to 0} \int_{-1}^{1}\frac{t}{t^2+x^2}f(x)\,dx=\pi f(0) $$ Any hints?","Closed. This question does not meet Mathematics Stack Exchange guidelines . It is not currently accepting answers. Please provide additional context , which ideally explains why the question is relevant to you and our  community. Some forms of context include: background and motivation, relevant definitions, source, possible strategies, your current progress, why the question is interesting or important, etc. Closed 5 years ago . Improve this question Show that if $f$ is continuous on $[-1,1]$, then $$ \lim_{t \to 0} \int_{-1}^{1}\frac{t}{t^2+x^2}f(x)\,dx=\pi f(0) $$ Any hints?",,['real-analysis']
47,Do these conditions imply function is continuous?,Do these conditions imply function is continuous?,,"I have two separate conditions: a) $\lim\limits_{h\to 0}|f(x+h)-f(x-h)|=0$ for every $x \in \Bbb R$ and b) $\lim_\limits{h\to 0}|f(x+h)+f(x-h)-2f(x)|=0$  for every $x \in \Bbb R$. My question is do the each of them imply $f$ is continuous?  $f(x)$ is said to be continuous at $x_0$ if $\lim_\limits{x\to x_0} f(x) = f(x_0) = c$ For a) it seems correct but I don't know how to prove it. For b), it seems wrong but I can't think of a counterexample.","I have two separate conditions: a) $\lim\limits_{h\to 0}|f(x+h)-f(x-h)|=0$ for every $x \in \Bbb R$ and b) $\lim_\limits{h\to 0}|f(x+h)+f(x-h)-2f(x)|=0$  for every $x \in \Bbb R$. My question is do the each of them imply $f$ is continuous?  $f(x)$ is said to be continuous at $x_0$ if $\lim_\limits{x\to x_0} f(x) = f(x_0) = c$ For a) it seems correct but I don't know how to prove it. For b), it seems wrong but I can't think of a counterexample.",,"['calculus', 'real-analysis']"
48,Why Cauchy's definition of infinitesimal is not widely used?,Why Cauchy's definition of infinitesimal is not widely used?,,"Cauchy defined infinitesimal as a variable or a function tending to zero, or as a null sequence. While I found the definition is not so popular and nearly discarded in math according to the following statement. (1). Infinitesimal entry in  Wikipedia : Some older textbooks use the term ""infinitesimal"" to refer to a variable or a function tending to zero Why textbooks involved with the definition is said to be old ? (2). Robert Goldblatt, Lectures on the Hyperreals: An Introduction to Nonstandard Analysis, P15 (His = Cauchy's) Why says 'Even'? (3). Abraham Robinson, Non-standard analysis, P276 why Cauchy's definition of infinitesimal, along with his 'basic approach' was superseded? Besides, I found most of the Real analysis or Calculus textbooks, such as Principles of mathematical analysis(Rudin) and Introduction to Calculus and Analysis(Richard Courant , Fritz John), don't introduce Cauchy's definition of infinitesimal, Why ? Why Cauchy's definition of infinitesimal was unpopular and not widely used, and nearly discarded? P.S. I refered some papers still cannot find the answer.","Cauchy defined infinitesimal as a variable or a function tending to zero, or as a null sequence. While I found the definition is not so popular and nearly discarded in math according to the following statement. (1). Infinitesimal entry in  Wikipedia : Some older textbooks use the term ""infinitesimal"" to refer to a variable or a function tending to zero Why textbooks involved with the definition is said to be old ? (2). Robert Goldblatt, Lectures on the Hyperreals: An Introduction to Nonstandard Analysis, P15 (His = Cauchy's) Why says 'Even'? (3). Abraham Robinson, Non-standard analysis, P276 why Cauchy's definition of infinitesimal, along with his 'basic approach' was superseded? Besides, I found most of the Real analysis or Calculus textbooks, such as Principles of mathematical analysis(Rudin) and Introduction to Calculus and Analysis(Richard Courant , Fritz John), don't introduce Cauchy's definition of infinitesimal, Why ? Why Cauchy's definition of infinitesimal was unpopular and not widely used, and nearly discarded? P.S. I refered some papers still cannot find the answer.",,"['calculus', 'real-analysis', 'math-history', 'nonstandard-analysis', 'infinitesimals']"
49,Continuity in a compact metric space.,Continuity in a compact metric space.,,"Let $(X,d)$ be a compact metric space and let $f, g: X \rightarrow \mathbb{R}$ be continuous such that $$f(x) \neq g(x), \forall x\in X.$$   Show that there exists an $\epsilon$ such that $$|f(x) - g(x)| \geq \epsilon, \forall x \in X.$$ I'm assuming he means $\epsilon > 0$. Well, suppose to the contrary that for all $\epsilon > 0$, there exists an $x' \in X$ such that $|f(x') - g(x')| < \epsilon.$ Since $f(x')$ and $g(x')$ are fixed values, we must have $f(x') = g(x')$, a contradiction. Seems uh... too easy? I didn't even have to use continuity or compactness? So seems wrong? (I'm really sick, so terrible at math this week, but is this right?)","Let $(X,d)$ be a compact metric space and let $f, g: X \rightarrow \mathbb{R}$ be continuous such that $$f(x) \neq g(x), \forall x\in X.$$   Show that there exists an $\epsilon$ such that $$|f(x) - g(x)| \geq \epsilon, \forall x \in X.$$ I'm assuming he means $\epsilon > 0$. Well, suppose to the contrary that for all $\epsilon > 0$, there exists an $x' \in X$ such that $|f(x') - g(x')| < \epsilon.$ Since $f(x')$ and $g(x')$ are fixed values, we must have $f(x') = g(x')$, a contradiction. Seems uh... too easy? I didn't even have to use continuity or compactness? So seems wrong? (I'm really sick, so terrible at math this week, but is this right?)",,"['real-analysis', 'proof-verification']"
50,Existence of nowhere differentiable functions,Existence of nowhere differentiable functions,,"I have troubles with one step of my solution to this problem: Show that there exists a continuous function $f:[0,1]\to \mathbb{R}$ which is not differentiable at any point. Hint: Consider $X=C([0,1],\mathbb{R})$ and $$U_n:=\left\{f\in X\sup\limits_{0<|h|\leq 1/n}\left|\frac{f(t+h)-f(t)}{h}\right|>n\quad \forall t\in[0,1]\right\}$$ I'm able to prove that $U_n$ is open and I know that if I can show that all $U_n$ are dense in $C([0,1])$ I can apply the Baire category theorem to show that there exist such a function. Unfortunately I have no idea how I should prove that all $U_n$ are dense in $C[0,1]$? So if someone could tell me how I prove that the $U_n$ are dense, then I would be very happy.","I have troubles with one step of my solution to this problem: Show that there exists a continuous function $f:[0,1]\to \mathbb{R}$ which is not differentiable at any point. Hint: Consider $X=C([0,1],\mathbb{R})$ and $$U_n:=\left\{f\in X\sup\limits_{0<|h|\leq 1/n}\left|\frac{f(t+h)-f(t)}{h}\right|>n\quad \forall t\in[0,1]\right\}$$ I'm able to prove that $U_n$ is open and I know that if I can show that all $U_n$ are dense in $C([0,1])$ I can apply the Baire category theorem to show that there exist such a function. Unfortunately I have no idea how I should prove that all $U_n$ are dense in $C[0,1]$? So if someone could tell me how I prove that the $U_n$ are dense, then I would be very happy.",,['real-analysis']
51,"Can the topology $\tau = \{\emptyset, \{a\}, \{a,b\}\}$ be induced by some metric?",Can the topology  be induced by some metric?,"\tau = \{\emptyset, \{a\}, \{a,b\}\}","I'm just beginning to learn about topological spaces, and in my notes a few examples of topological spaces are given. One of them is $\tau = \{\emptyset, \{a\}, \{a,b\}\}$ on the set $X = \{a,b\}$. I can clearly see how this meets my definition of a topology, namely that $\emptyset, X \in T$; $\bigcup_{\lambda \in \Lambda} T_{\lambda} \in \tau$ whenever $\{T_{\lambda}\}_{\lambda \in \Lambda} \subseteq \tau$; $\bigcap_{k=1}^{n} T_{k} \in \tau$ whenever $\{T_{k}\}_{k=1}^{n} \subseteq \tau$. My question: I was wondering if this could be a topology induced by some metric $d$ on $X$, however. Or more generally, how one could determine whether a particular topology is possibly induced by a metric. I have seen that the discrete topology on any $X \neq \emptyset$ given by $\tau = \mathcal{P}(X)$ is induced by the discrete metric $\mu$, for instance, which has motivated my question as to whether some of the other topologies introduced may be metric-induced as well. I attempted to think of some metric which would induce my above topology, but have thus far been unsuccessful - but also not sure how I might prove that one cannot exist.","I'm just beginning to learn about topological spaces, and in my notes a few examples of topological spaces are given. One of them is $\tau = \{\emptyset, \{a\}, \{a,b\}\}$ on the set $X = \{a,b\}$. I can clearly see how this meets my definition of a topology, namely that $\emptyset, X \in T$; $\bigcup_{\lambda \in \Lambda} T_{\lambda} \in \tau$ whenever $\{T_{\lambda}\}_{\lambda \in \Lambda} \subseteq \tau$; $\bigcap_{k=1}^{n} T_{k} \in \tau$ whenever $\{T_{k}\}_{k=1}^{n} \subseteq \tau$. My question: I was wondering if this could be a topology induced by some metric $d$ on $X$, however. Or more generally, how one could determine whether a particular topology is possibly induced by a metric. I have seen that the discrete topology on any $X \neq \emptyset$ given by $\tau = \mathcal{P}(X)$ is induced by the discrete metric $\mu$, for instance, which has motivated my question as to whether some of the other topologies introduced may be metric-induced as well. I attempted to think of some metric which would induce my above topology, but have thus far been unsuccessful - but also not sure how I might prove that one cannot exist.",,"['real-analysis', 'general-topology', 'metric-spaces']"
52,Rearrangement of series in Banach space and absolute convergence,Rearrangement of series in Banach space and absolute convergence,,"Let $X$ be a Banach space, and assume $\sum_{i=1}^\infty x_i$ and any of its rearrangements are convergent to the same value; do we have the conclusion that $\sum_{i=1}^\infty \|x_i\|$ is also convergent? When $X=\mathbb{R}$ I think it is true, but for general Banach spaces, is it true?","Let $X$ be a Banach space, and assume $\sum_{i=1}^\infty x_i$ and any of its rearrangements are convergent to the same value; do we have the conclusion that $\sum_{i=1}^\infty \|x_i\|$ is also convergent? When $X=\mathbb{R}$ I think it is true, but for general Banach spaces, is it true?",,"['real-analysis', 'sequences-and-series', 'banach-spaces']"
53,How do I show that a contraction mapping in a metric space is continuous?,How do I show that a contraction mapping in a metric space is continuous?,,"I start out by letting $V$ be an arbitrary open set in $X$.  Then  $$ f^{-1}(V) = \{x\in X\mid f(x) \in B_\epsilon(f(a))\}. $$ This can be re-written as: $$ f^{-1}(V) = \{x\in X\mid d(f(a), f(x)) < \epsilon \}. $$  I realize that contraction mappings have an $0<r<1$ such that  $$ d(f(x_1), f(x_2)) \leq r\cdot d(x_1,x_2),\quad \forall x_1,x_2 \in X. $$  I construct an open ball  $$ B_{\frac{\delta}{r}}(a) = \{x\in X\mid r\cdot d(a, x) \lt \delta \} $$ but from here I'm unsure as to how to show that $f^{-1}(V)$ is open.","I start out by letting $V$ be an arbitrary open set in $X$.  Then  $$ f^{-1}(V) = \{x\in X\mid f(x) \in B_\epsilon(f(a))\}. $$ This can be re-written as: $$ f^{-1}(V) = \{x\in X\mid d(f(a), f(x)) < \epsilon \}. $$  I realize that contraction mappings have an $0<r<1$ such that  $$ d(f(x_1), f(x_2)) \leq r\cdot d(x_1,x_2),\quad \forall x_1,x_2 \in X. $$  I construct an open ball  $$ B_{\frac{\delta}{r}}(a) = \{x\in X\mid r\cdot d(a, x) \lt \delta \} $$ but from here I'm unsure as to how to show that $f^{-1}(V)$ is open.",,"['real-analysis', 'general-topology', 'metric-spaces']"
54,"Real Analysis, Folland Problem 1.3.15 Measures","Real Analysis, Folland Problem 1.3.15 Measures",,"Given a measure $\mu$ on $(X,M)$ , define $\mu_0$ on $M$ by $$\mu_0(E) = \sup\{\mu(F): F\subset E \ \text{and} \ \mu(F) < \infty\}$$ a.) $\mu_0$ is a semifinite measure. It is called the semifinite part of $\mu$ . b.) If $\mu$ is semifinite, then $\mu = \mu_0$ (Use Exercise 14) c.) There is a measure $\nu$ on $M$ (in general, not unique) which assumes only the values of $0$ and $\infty$ such that $\mu = \mu_0 + \nu$ . The proof of Exercise 14 can be found here For a.) I believe we need to show that $\mu_0(E) = \infty$ . Perhaps we can do this by proof of contradiction, similarly to what the proof in the link above does (actually it seems identical). For b.) I am not sure how we can show $\mu = \mu_0$ I think maybe we have to show by construction that $\mu \leq \mu_0$ and $\mu \geq \mu_0$ For c.) I haven't any idea for. Any suggestions is greatly appreciated.","Given a measure on , define on by a.) is a semifinite measure. It is called the semifinite part of . b.) If is semifinite, then (Use Exercise 14) c.) There is a measure on (in general, not unique) which assumes only the values of and such that . The proof of Exercise 14 can be found here For a.) I believe we need to show that . Perhaps we can do this by proof of contradiction, similarly to what the proof in the link above does (actually it seems identical). For b.) I am not sure how we can show I think maybe we have to show by construction that and For c.) I haven't any idea for. Any suggestions is greatly appreciated.","\mu (X,M) \mu_0 M \mu_0(E) = \sup\{\mu(F): F\subset E \ \text{and} \ \mu(F) < \infty\} \mu_0 \mu \mu \mu = \mu_0 \nu M 0 \infty \mu = \mu_0 + \nu \mu_0(E) = \infty \mu = \mu_0 \mu \leq \mu_0 \mu \geq \mu_0","['real-analysis', 'measure-theory']"
55,challenge problem: show this sequence is convergent.,challenge problem: show this sequence is convergent.,,"had this difficult question from a textbook, and I haven't been able to figure out the solution. say we have a sequence of bounded real numbers $a_n$ such that $2a_n \leq a_{n-1} + a_{n+1} \forall n\in\mathbb{N}$. Show that this sequence converges. What ive tried: I did some algebra on it then tried to use the cauchy criterion, since we don't know the limit, but we know the relation between terms. also tried moving things around and reindexing and using boundedness. but havent been able to come up with anything there either. since bounded and monotone converges. any help would be appreciated","had this difficult question from a textbook, and I haven't been able to figure out the solution. say we have a sequence of bounded real numbers $a_n$ such that $2a_n \leq a_{n-1} + a_{n+1} \forall n\in\mathbb{N}$. Show that this sequence converges. What ive tried: I did some algebra on it then tried to use the cauchy criterion, since we don't know the limit, but we know the relation between terms. also tried moving things around and reindexing and using boundedness. but havent been able to come up with anything there either. since bounded and monotone converges. any help would be appreciated",,"['real-analysis', 'sequences-and-series', 'cauchy-sequences']"
56,Continuous bijection on open interval is homeomorphism,Continuous bijection on open interval is homeomorphism,,"Suppose that $a,b\in\mathbb R$ with $a<b$ and that $f:(a,b)\to\mathbb R$ is continuous and bijective. I would like to prove that $f$ is a homeomorphism using elementary methods (no resort to invariance of domain, for instance). To my surprise, I have not found a straightforward, standard (textbook) reference for this result. If you know any or you have a clever hint in mind, I would be grateful if you could share it.","Suppose that $a,b\in\mathbb R$ with $a<b$ and that $f:(a,b)\to\mathbb R$ is continuous and bijective. I would like to prove that $f$ is a homeomorphism using elementary methods (no resort to invariance of domain, for instance). To my surprise, I have not found a straightforward, standard (textbook) reference for this result. If you know any or you have a clever hint in mind, I would be grateful if you could share it.",,"['real-analysis', 'general-topology', 'reference-request']"
57,Is Lebesgue measure translation invariant?,Is Lebesgue measure translation invariant?,,"I am trying to prove that the Lebesgue measure is translation-invariant. Namely, given a set $X\subseteq\mathbb{R}$ , I'd like to show $X + y$ is measurable and $\mathit{m}(X + y) = \mathit{m}(X)$ . Namely, that the measures -- not the outter measures alone -- agree. I am mostly stuck on demonstrating that the translation $X + y$ is measurable to begin with. Any ideas?","I am trying to prove that the Lebesgue measure is translation-invariant. Namely, given a set , I'd like to show is measurable and . Namely, that the measures -- not the outter measures alone -- agree. I am mostly stuck on demonstrating that the translation is measurable to begin with. Any ideas?",X\subseteq\mathbb{R} X + y \mathit{m}(X + y) = \mathit{m}(X) X + y,"['real-analysis', 'analysis', 'measure-theory']"
58,Is every closure of a metric space a completion?,Is every closure of a metric space a completion?,,"I know that every completion is a closure of a metric space, since every convergent sequence is cauchy and  and the limit of that sequence will exist within the completion. At the same time, from my understanding, every cauchy sequence will bunch closer together and get arbitrarily close to something, but it is just a question as to whether or not that element it gets closer to actually exists in the space. This leads me to the question as to whether every closure of a metric space is a completion, because we would just be adding the limits to sequences which exist outside of the original space, including the limits of nonconvergent cauchy sequences. So is there an example of a closure which is not a completion? Or are these notions  equivalent?","I know that every completion is a closure of a metric space, since every convergent sequence is cauchy and  and the limit of that sequence will exist within the completion. At the same time, from my understanding, every cauchy sequence will bunch closer together and get arbitrarily close to something, but it is just a question as to whether or not that element it gets closer to actually exists in the space. This leads me to the question as to whether every closure of a metric space is a completion, because we would just be adding the limits to sequences which exist outside of the original space, including the limits of nonconvergent cauchy sequences. So is there an example of a closure which is not a completion? Or are these notions  equivalent?",,"['real-analysis', 'cauchy-sequences', 'complete-spaces']"
59,Bounded derivative implies uniform continuity on an open interval,Bounded derivative implies uniform continuity on an open interval,,"Suppose $f : (a,b) \to \mathbb{R}$  such that $f'$ exists and is bounded on $(a,b)$. Then is $f$ uniformly continuous? I have a hint to use the mean value theorem, but I'm not sure I can apply it to an open interval like this?  Does the bounded derivative imply that $f$ is continuous on $[a,b]$ somehow?","Suppose $f : (a,b) \to \mathbb{R}$  such that $f'$ exists and is bounded on $(a,b)$. Then is $f$ uniformly continuous? I have a hint to use the mean value theorem, but I'm not sure I can apply it to an open interval like this?  Does the bounded derivative imply that $f$ is continuous on $[a,b]$ somehow?",,"['real-analysis', 'analysis']"
60,Possible mistake in Royden and Fitzpatrick's book Real Analysis,Possible mistake in Royden and Fitzpatrick's book Real Analysis,,"Proposition 19(v) of Section 1.5 page 23 in the book Real Analysis by Royden and Fitzpatrick, 4th edition (see link ), says: If $a_n \le b_n$ for all $n$, then $\lim\sup a_n \le \lim \inf b_n$. However one can find a counter-example for a sequence $a_n = b_n$ which has different limsup and liminf. Is it a mistake, or am I missing something obvious?","Proposition 19(v) of Section 1.5 page 23 in the book Real Analysis by Royden and Fitzpatrick, 4th edition (see link ), says: If $a_n \le b_n$ for all $n$, then $\lim\sup a_n \le \lim \inf b_n$. However one can find a counter-example for a sequence $a_n = b_n$ which has different limsup and liminf. Is it a mistake, or am I missing something obvious?",,['real-analysis']
61,Prove that $f$ has a fixed point.,Prove that  has a fixed point.,f,"Let $f:[0,\infty [\to[0,\infty [$ continuous such that $$\lim_{t\to\infty }\frac{f(t)}{t}=\ell\in[0,1).$$ Prove that $f$ has a fixed point, i.e. there is an $x\geq 0$ such that $f(x)=x$. I don't really know how to solve this problem. My first intension was to use Brouwer, but it's only useable on a compact. After I tried by induction but with no success.","Let $f:[0,\infty [\to[0,\infty [$ continuous such that $$\lim_{t\to\infty }\frac{f(t)}{t}=\ell\in[0,1).$$ Prove that $f$ has a fixed point, i.e. there is an $x\geq 0$ such that $f(x)=x$. I don't really know how to solve this problem. My first intension was to use Brouwer, but it's only useable on a compact. After I tried by induction but with no success.",,['real-analysis']
62,Need Suggestions for beginner who is in transition period from computational calculus to rigorous proofy Analysis,Need Suggestions for beginner who is in transition period from computational calculus to rigorous proofy Analysis,,"I have completed basic calculus 1,2,3 courses, Linear Algebra, etc. I have not, however, got into rigorous Analysis yet, which I am planning to do now. I have three books in mind. They are : Terence Tao - Real Analysis Apostol Mathematical Analysis G.H Hardy's Course of Pure Math I HAVE NEVER BEEN INTO RIGOROUS PROOFS EVER BEFORE.....I DID CALCULUS AS COMPUTATIONAL SUBJECT MAINLY . However, I am determined to learn Real analysis. I can also use other subjects in conjunction with real analysis like Modern Algebra, etc or I can do real analysis first and then after? I AM TO DO SELF STUDY AT HOME ONLY. I DO NOT HAVE MONEY TO PAY MY TUTION FEES OF COLLEGE Anyone who has gone through these books, kindly help me with selection or whether these books can be used simultaneously and stuff like that. I will be glad. Thanks","I have completed basic calculus 1,2,3 courses, Linear Algebra, etc. I have not, however, got into rigorous Analysis yet, which I am planning to do now. I have three books in mind. They are : Terence Tao - Real Analysis Apostol Mathematical Analysis G.H Hardy's Course of Pure Math I HAVE NEVER BEEN INTO RIGOROUS PROOFS EVER BEFORE.....I DID CALCULUS AS COMPUTATIONAL SUBJECT MAINLY . However, I am determined to learn Real analysis. I can also use other subjects in conjunction with real analysis like Modern Algebra, etc or I can do real analysis first and then after? I AM TO DO SELF STUDY AT HOME ONLY. I DO NOT HAVE MONEY TO PAY MY TUTION FEES OF COLLEGE Anyone who has gone through these books, kindly help me with selection or whether these books can be used simultaneously and stuff like that. I will be glad. Thanks",,"['real-analysis', 'reference-request', 'soft-question', 'book-recommendation']"
63,Wave equation $u_{xx}+u_{xt}- u_{tt}=0$,Wave equation,u_{xx}+u_{xt}- u_{tt}=0,"Does anybody know how we can solve the equation $u_{xx}+  u_{xt}-  u_{tt}=0$ with $u(x,0):=g(x)$ and $u_t(x,0):=h(x)?$ I mean it is known how to do this for the wave equation see here but I don't know how to do this in the more general case with the mixed term in it.","Does anybody know how we can solve the equation $u_{xx}+  u_{xt}-  u_{tt}=0$ with $u(x,0):=g(x)$ and $u_t(x,0):=h(x)?$ I mean it is known how to do this for the wave equation see here but I don't know how to do this in the more general case with the mixed term in it.",,"['calculus', 'real-analysis', 'analysis', 'partial-differential-equations', 'wave-equation']"
64,$\sum_1^{\infty} \frac{(p+1)(p+2)(p+3)...(p+n)}{(q+1)(q+2)(q+3)...(q+n)}$ convergence,convergence,\sum_1^{\infty} \frac{(p+1)(p+2)(p+3)...(p+n)}{(q+1)(q+2)(q+3)...(q+n)},"I need to determine for which values of $p$ and $q$, both greater than $0$, the following series converges: $$\sum_1^{\infty} \frac{(p+1)(p+2)(p+3)...(p+n)}{(q+1)(q+2)(q+3)...(q+n)}$$ I've tried using the ratio test, comparison test, and I've also tried partial fraction decomposition but I can't get to anything. Could you give me a hint on how to solve this? Any help will be appreciated","I need to determine for which values of $p$ and $q$, both greater than $0$, the following series converges: $$\sum_1^{\infty} \frac{(p+1)(p+2)(p+3)...(p+n)}{(q+1)(q+2)(q+3)...(q+n)}$$ I've tried using the ratio test, comparison test, and I've also tried partial fraction decomposition but I can't get to anything. Could you give me a hint on how to solve this? Any help will be appreciated",,"['real-analysis', 'sequences-and-series']"
65,Why is a double exponential function faster than $x!$?,Why is a double exponential function faster than ?,x!,"Reading this wikipedia article I found an interesting sentence: Factorials grow faster than exponential functions, but much slower than double-exponential functions. The author doesn't provide a link let alone a proof of that fact and I don't find it obvious. After all, factorial functions grow really fast but the author says they grow much slower than double exponentials. Does anyone know a proof that $$\lim_{x \to \infty} \frac{e^{e^{x}}}{x!} = \infty$$","Reading this wikipedia article I found an interesting sentence: Factorials grow faster than exponential functions, but much slower than double-exponential functions. The author doesn't provide a link let alone a proof of that fact and I don't find it obvious. After all, factorial functions grow really fast but the author says they grow much slower than double exponentials. Does anyone know a proof that $$\lim_{x \to \infty} \frac{e^{e^{x}}}{x!} = \infty$$",,"['real-analysis', 'limits']"
66,"Two sequences, same set, different limit","Two sequences, same set, different limit",,"I recently saw a user write a sequence as $\{x_n\}_{n=1}^{\infty}$. This is generally bad notation, since it could lead people to think of the sequence as a set rather than as a sequence, although we (hopefully) all know what they mean. However, this raises the question: when does the set $\{x_n: n \in \mathbb{N}\}$ determine the limit $\lim_{n \rightarrow \infty} x_n$? Clearly, when the set is finite, it can't, e.g. $(1, 0, 0, 0, ...)$ and $(0, 1, 1, 1, ...)$. But if not? More precisely: Let $(x_n)_{n=1}^{\infty}$ and $(y_n)_{n=1}^{\infty}$ be real sequences such that: $\lim \limits_{n \rightarrow \infty} x_n$ and $\lim \limits_{n \rightarrow \infty} y_n$ both exist $\{x_n: n \in \mathbb{N}\} = \{y_n: n \in \mathbb{N}\} =: \mathcal{Z}$ for all $z \in \mathcal{Z}$, the sets $\{n: x_n = z\}$ and $\{n: y_n = z\}$ are both finite. Then do we always have $\lim \limits_{n \rightarrow \infty} x_n = \lim \limits_{n \rightarrow \infty} y_n$?","I recently saw a user write a sequence as $\{x_n\}_{n=1}^{\infty}$. This is generally bad notation, since it could lead people to think of the sequence as a set rather than as a sequence, although we (hopefully) all know what they mean. However, this raises the question: when does the set $\{x_n: n \in \mathbb{N}\}$ determine the limit $\lim_{n \rightarrow \infty} x_n$? Clearly, when the set is finite, it can't, e.g. $(1, 0, 0, 0, ...)$ and $(0, 1, 1, 1, ...)$. But if not? More precisely: Let $(x_n)_{n=1}^{\infty}$ and $(y_n)_{n=1}^{\infty}$ be real sequences such that: $\lim \limits_{n \rightarrow \infty} x_n$ and $\lim \limits_{n \rightarrow \infty} y_n$ both exist $\{x_n: n \in \mathbb{N}\} = \{y_n: n \in \mathbb{N}\} =: \mathcal{Z}$ for all $z \in \mathcal{Z}$, the sets $\{n: x_n = z\}$ and $\{n: y_n = z\}$ are both finite. Then do we always have $\lim \limits_{n \rightarrow \infty} x_n = \lim \limits_{n \rightarrow \infty} y_n$?",,"['real-analysis', 'sequences-and-series']"
67,Ratio test with limsup vs lim,Ratio test with limsup vs lim,,"Could I prove that the ratio test still works using $\limsup(\frac{a_{n+1}}{a_n})$ instead of $\lim(\frac{a_{n+1}}{a_n})$? I think for $\limsup<1$ I could show that for $\epsilon>0, N>1 \limsup(\frac{a_{n+1}}{a_n})<1-\epsilon.$ From there I can solve that $\lvert a_k\rvert<(1-\epsilon)^{k-N}\lvert a_N\rvert$ for $k>N$. Thus, by comparison test the left-hand side will converge absolutely. Is this enough to show I can use the ratio test with $\limsup$?","Could I prove that the ratio test still works using $\limsup(\frac{a_{n+1}}{a_n})$ instead of $\lim(\frac{a_{n+1}}{a_n})$? I think for $\limsup<1$ I could show that for $\epsilon>0, N>1 \limsup(\frac{a_{n+1}}{a_n})<1-\epsilon.$ From there I can solve that $\lvert a_k\rvert<(1-\epsilon)^{k-N}\lvert a_N\rvert$ for $k>N$. Thus, by comparison test the left-hand side will converge absolutely. Is this enough to show I can use the ratio test with $\limsup$?",,"['real-analysis', 'convergence-divergence', 'limsup-and-liminf', 'ratio']"
68,"Find sequence of differentiable functions $f_n$ on $\mathbb{R}$ that converge uniformly, but $f'_n$ converges only pointwise","Find sequence of differentiable functions  on  that converge uniformly, but  converges only pointwise",f_n \mathbb{R} f'_n,"Question: Find a sequence of differentiable functions $f_n$ on $\mathbb{R}$ that converge uniformly to a differentiable function $f$, such that $f'_n$ converges pointwise but not uniformly to $f'$. Attempt: I have tried a number of possibilities, such as $f_n=x^n$ or $f_n=\frac{x^n}{n}$ but I don't know what the right approach is to construct the function. I am initially thinking that it's easiest to construct such a sequence of functions on the interval $[0,1]$ so that in the limit of $n$, part of the function goes to $0$ and the other part goes to $1$. However, this would make the resulting $f$ non-differentiable.","Question: Find a sequence of differentiable functions $f_n$ on $\mathbb{R}$ that converge uniformly to a differentiable function $f$, such that $f'_n$ converges pointwise but not uniformly to $f'$. Attempt: I have tried a number of possibilities, such as $f_n=x^n$ or $f_n=\frac{x^n}{n}$ but I don't know what the right approach is to construct the function. I am initially thinking that it's easiest to construct such a sequence of functions on the interval $[0,1]$ so that in the limit of $n$, part of the function goes to $0$ and the other part goes to $1$. However, this would make the resulting $f$ non-differentiable.",,['real-analysis']
69,"If $\sum_{n=1}^\infty a_n$ is a convergent series of positive real numbers, then so is $\sum_{n=1}^\infty a_n^{n/({n+1})}$","If  is a convergent series of positive real numbers, then so is",\sum_{n=1}^\infty a_n \sum_{n=1}^\infty a_n^{n/({n+1})},"This is the $1988$ Putnam $B4$ Problem: Prove that if $\sum_{n=1}^\infty a_n$ is a convergent series of positive real numbers, then so is $\sum_{n=1}^\infty a_n^{n/({n+1})}$. My problem lies in figuring out what to do in the case that $0\lt a_n \lt 1$ for all $n$. I imagine that it must have something to do with the limit comparison test. Any hints would be greatly appreciated.","This is the $1988$ Putnam $B4$ Problem: Prove that if $\sum_{n=1}^\infty a_n$ is a convergent series of positive real numbers, then so is $\sum_{n=1}^\infty a_n^{n/({n+1})}$. My problem lies in figuring out what to do in the case that $0\lt a_n \lt 1$ for all $n$. I imagine that it must have something to do with the limit comparison test. Any hints would be greatly appreciated.",,"['calculus', 'real-analysis', 'sequences-and-series', 'convergence-divergence', 'contest-math']"
70,How are the elementary arithmetics defined?,How are the elementary arithmetics defined?,,"In the book Principles of Mathematical Analysis by Rudin, I read that ""a < b"" is defined this way: if b - a is positive, then a < b or b > a. Then some questions arose to me: we know that ""minus"" is a reverse operation of ""plus"" since a - b = a + (-b), but how is the elementary arithmetic ""a + b"" is defined, and how about ""a * b""? Will there be any differences between the definitions with regard to the real number system, rational number system, and natural number system?","In the book Principles of Mathematical Analysis by Rudin, I read that ""a < b"" is defined this way: if b - a is positive, then a < b or b > a. Then some questions arose to me: we know that ""minus"" is a reverse operation of ""plus"" since a - b = a + (-b), but how is the elementary arithmetic ""a + b"" is defined, and how about ""a * b""? Will there be any differences between the definitions with regard to the real number system, rational number system, and natural number system?",,"['real-analysis', 'arithmetic', 'real-numbers', 'natural-numbers', 'foundations']"
71,Prove that $\lim_{n\to\infty} \left( \frac{ g_n^{\gamma}}{\gamma^{g_n}} \right)^{2n} = \frac{e}{\gamma}$,Prove that,\lim_{n\to\infty} \left( \frac{ g_n^{\gamma}}{\gamma^{g_n}} \right)^{2n} = \frac{e}{\gamma},"Put $g_n = 1 + \frac{1}{2} + \dotsb + \frac{1}{n} - \log(n)$ . Prove that $$\lim_{n\to\infty} \left( \frac{ g_n^{\gamma}}{\gamma^{g_n}} \right)^{2n} = \frac{e}{\gamma},$$ where $\gamma$ is the Euler-Mascheroni constant .",Put . Prove that where is the Euler-Mascheroni constant .,"g_n = 1 + \frac{1}{2} + \dotsb + \frac{1}{n} - \log(n) \lim_{n\to\infty} \left( \frac{ g_n^{\gamma}}{\gamma^{g_n}} \right)^{2n} = \frac{e}{\gamma}, \gamma","['calculus', 'real-analysis', 'sequences-and-series', 'limits']"
72,Examples of differentiable functions that are not of bounded variation,Examples of differentiable functions that are not of bounded variation,,"It is known that a function with bounded variation is differentiable almost everywhere. There are also functions with unbounded variation that are differentiable almost everywhere (e.g. take $f:[0,1]\rightarrow\mathbb{R}$ with $f(0)=0$ and $f(x)=1/x$ otherwise). But does there exist a function on $[0,1]$ with unbounded variation that is everywhere differentiable?","It is known that a function with bounded variation is differentiable almost everywhere. There are also functions with unbounded variation that are differentiable almost everywhere (e.g. take $f:[0,1]\rightarrow\mathbb{R}$ with $f(0)=0$ and $f(x)=1/x$ otherwise). But does there exist a function on $[0,1]$ with unbounded variation that is everywhere differentiable?",,['real-analysis']
73,Cartesian product of dense sets is dense?,Cartesian product of dense sets is dense?,,"If $Q_i$ is dense in $X_i$, where $X_i$ is a topological space and $i \in I$ being an arbitrary family of indices. Does this imply that $\prod Q_i$ is dense in $\prod X_i$?","If $Q_i$ is dense in $X_i$, where $X_i$ is a topological space and $i \in I$ being an arbitrary family of indices. Does this imply that $\prod Q_i$ is dense in $\prod X_i$?",,['calculus']
74,"$\lim_{n \rightarrow\infty} ~ x_{n+1} - x_n= c , c > 0$ . Then, is $\{x_n/n\}$ convergent?",". Then, is  convergent?","\lim_{n \rightarrow\infty} ~ x_{n+1} - x_n= c , c > 0 \{x_n/n\}","If $\{x_n\}$ is a sequence which satisfies $\lim_{n \rightarrow\infty} ~ x_{n+1} - x_n= c$  where $c$ is a real positive number. Then what can be said about the convergence/ divergence, boundedness/ unboundedness of $\{x_n/n\}$. Attempt: $\lim_{n \rightarrow\infty} ~ x_{n+1} - x_n= c$ where $c >0$ => $x_n$ is unbounded and divergent. However, I am stuck on how to relate this to convergence/divergence of $x_n/n$ . Thanks for the help.","If $\{x_n\}$ is a sequence which satisfies $\lim_{n \rightarrow\infty} ~ x_{n+1} - x_n= c$  where $c$ is a real positive number. Then what can be said about the convergence/ divergence, boundedness/ unboundedness of $\{x_n/n\}$. Attempt: $\lim_{n \rightarrow\infty} ~ x_{n+1} - x_n= c$ where $c >0$ => $x_n$ is unbounded and divergent. However, I am stuck on how to relate this to convergence/divergence of $x_n/n$ . Thanks for the help.",,"['real-analysis', 'sequences-and-series']"
75,"$\forall x \in \mathbb{R}$, there exits $\delta$ such that $(x-\delta,x+\delta) \cap A$ is countable. Prove that $A$ is countable.",", there exits  such that  is countable. Prove that  is countable.","\forall x \in \mathbb{R} \delta (x-\delta,x+\delta) \cap A A","As stated in the title. At the first glance I think the approach can be constructing an injection from $A$ to $\mathbb Q$, since obviously $\mathbb Q$ is a set that satisfies such condition. However I have no idea on how to get such injection. Any hints would be appreciated.","As stated in the title. At the first glance I think the approach can be constructing an injection from $A$ to $\mathbb Q$, since obviously $\mathbb Q$ is a set that satisfies such condition. However I have no idea on how to get such injection. Any hints would be appreciated.",,['real-analysis']
76,Compute $\sum\arctan\frac1{3^n}$,Compute,\sum\arctan\frac1{3^n},"Find the Closed-form expression of the sum $$ \sum_{n=1}^{\infty}\arctan\left(1 \over 3^{n}\right) $$ Notice that $\arctan\left(x\right) < x,\forall\ x > 0$, so $\sum_{n = 1}^{\infty}\arctan\left(1 \over 3^{n}\right) < {1 \over 2}$.","Find the Closed-form expression of the sum $$ \sum_{n=1}^{\infty}\arctan\left(1 \over 3^{n}\right) $$ Notice that $\arctan\left(x\right) < x,\forall\ x > 0$, so $\sum_{n = 1}^{\infty}\arctan\left(1 \over 3^{n}\right) < {1 \over 2}$.",,"['calculus', 'real-analysis']"
77,The weak$^*$ topology on $X^*$ is not first countable if $X$ has uncountable dimension.,The weak topology on  is not first countable if  has uncountable dimension.,^* X^* X,"I learnt without proof that if $X$ is a normed space of uncountable dimension, then the weak* topology on $X^*$ is not first countable. Can anyone point out how I should go about proving it? I tried the following: Suppose that the weak* topology on $X^*$ is first countable. Then we may take a sequence $(x_n)$ such that  $$ V_n=\big\{x^*\in X^*: \lvert x^*(x_i)\rvert<1, \,\,\text{for all}\,\, i=1, 2, \dots, n\big\} $$  gives a (descending sequence of) countable base at $0\in X^*$. Because I want to find a contradiction, I guess I should try to show that $(x_n)$ span $X$. Is the above attempt getting anywhere?","I learnt without proof that if $X$ is a normed space of uncountable dimension, then the weak* topology on $X^*$ is not first countable. Can anyone point out how I should go about proving it? I tried the following: Suppose that the weak* topology on $X^*$ is first countable. Then we may take a sequence $(x_n)$ such that  $$ V_n=\big\{x^*\in X^*: \lvert x^*(x_i)\rvert<1, \,\,\text{for all}\,\, i=1, 2, \dots, n\big\} $$  gives a (descending sequence of) countable base at $0\in X^*$. Because I want to find a contradiction, I guess I should try to show that $(x_n)$ span $X$. Is the above attempt getting anywhere?",,"['real-analysis', 'functional-analysis', 'normed-spaces']"
78,"Are convex function from a convex, bounded and closed set in $\mathbb{R}^n$ continuous?","Are convex function from a convex, bounded and closed set in  continuous?",\mathbb{R}^n,"If I have a convex function $f:A\to \mathbb{R}$, where $A$ is a convex, bounded and closed set in $\mathbb{R}^n$, for example $A:=\{x\in\mathbb{R}^n:\|x\|\le 1\}$ the unit ball. Does this imply that $f$ is continuous? I've searched the web and didn't found a theorem for this setting (or which is applicable in this case). If the statement is true, a reference would be appreciated.","If I have a convex function $f:A\to \mathbb{R}$, where $A$ is a convex, bounded and closed set in $\mathbb{R}^n$, for example $A:=\{x\in\mathbb{R}^n:\|x\|\le 1\}$ the unit ball. Does this imply that $f$ is continuous? I've searched the web and didn't found a theorem for this setting (or which is applicable in this case). If the statement is true, a reference would be appreciated.",,"['real-analysis', 'convex-analysis']"
79,Show given any $x\in\mathbb{R}$ show there exists a unique $n\in\mathbb{Z}$ such that $n-1\leq x <n$.,Show given any  show there exists a unique  such that .,x\in\mathbb{R} n\in\mathbb{Z} n-1\leq x <n,Show given any $x\in\mathbb{R}$ show there exists a unique $n\in\mathbb{Z}$ such that $n-1\leq x <n$. I already know that there exists a $n\in\mathbb{N}$ such that $x<n$ by the archimedean property. But to prove that $n-1\leq x$ part I'm not sure. I think I have to create some set such that $n-1$ is a lower bound. But I'm not sure.,Show given any $x\in\mathbb{R}$ show there exists a unique $n\in\mathbb{Z}$ such that $n-1\leq x <n$. I already know that there exists a $n\in\mathbb{N}$ such that $x<n$ by the archimedean property. But to prove that $n-1\leq x$ part I'm not sure. I think I have to create some set such that $n-1$ is a lower bound. But I'm not sure.,,['real-analysis']
80,Borel set preserved by continuous map,Borel set preserved by continuous map,,"Let $f:\mathbb{R}^m\rightarrow\mathbb{R}^n$ be a continuous map. Show that if $A$ is a Borel subset of $\mathbb{R}^n$, then $f^{-1}(A)$ is a Borel subset of $\mathbb{R}^m$. I know that for $A$ open subset of $\mathbb{R}^n$, then $f^{-1}(A)$ is open. Likewise for closed. A Borel subset is a countable unions/intersections of these sets, done repeatedly. From those facts, combined with $f^{-1}(A\cup B)=f^{-1}(A)\cup f^{-1}(B)$ and $f^{-1}(A\cap B)=f^{-1}(A)\cap f^{-1}(B)$, the statement should be intuitively true. But to do it rigorously, I want to write $A$ as open and closed sets. But since the countable union/intersection can be done repeatedly, I don't know how I can write it that way.","Let $f:\mathbb{R}^m\rightarrow\mathbb{R}^n$ be a continuous map. Show that if $A$ is a Borel subset of $\mathbb{R}^n$, then $f^{-1}(A)$ is a Borel subset of $\mathbb{R}^m$. I know that for $A$ open subset of $\mathbb{R}^n$, then $f^{-1}(A)$ is open. Likewise for closed. A Borel subset is a countable unions/intersections of these sets, done repeatedly. From those facts, combined with $f^{-1}(A\cup B)=f^{-1}(A)\cup f^{-1}(B)$ and $f^{-1}(A\cap B)=f^{-1}(A)\cap f^{-1}(B)$, the statement should be intuitively true. But to do it rigorously, I want to write $A$ as open and closed sets. But since the countable union/intersection can be done repeatedly, I don't know how I can write it that way.",,['real-analysis']
81,How to prove $\lim\limits_{n\to\infty} (n+1)^{1/n} = 1$,How to prove,\lim\limits_{n\to\infty} (n+1)^{1/n} = 1,"We know that $\lim\limits_{n\to\infty}n^{1/n} = 1$. Using this, how can we prove that $\lim\limits_{n\to\infty} (n+1)^{1/n} = 1$? Recalling the proof of the former limit, I was able to modify it to prove the latter limit. But I was wondering if we could just use the limit we have already calculated to prove this limit which is related to it.","We know that $\lim\limits_{n\to\infty}n^{1/n} = 1$. Using this, how can we prove that $\lim\limits_{n\to\infty} (n+1)^{1/n} = 1$? Recalling the proof of the former limit, I was able to modify it to prove the latter limit. But I was wondering if we could just use the limit we have already calculated to prove this limit which is related to it.",,"['calculus', 'real-analysis', 'sequences-and-series', 'limits']"
82,Homeomorphism between two Cantor sets,Homeomorphism between two Cantor sets,,Let $K_{1}$ and $K_{2}$ two Cantor sets. Prove that they're homeomorphics. How I begin the demonstration? I just need an idea. Thanks!,Let $K_{1}$ and $K_{2}$ two Cantor sets. Prove that they're homeomorphics. How I begin the demonstration? I just need an idea. Thanks!,,"['real-analysis', 'general-topology']"
83,Proof of Egoroff's Theorem,Proof of Egoroff's Theorem,,"Let $\{f_n \}$ be a sequence of measurable functions, $f_n \to f$ $\mu$-a.e. on a measurable set $E$, $\mu(E) < \infty$. Let $\epsilon>0$ be given. Then $\forall \space n \in \mathbb{N} \space \exists A_n \subset E$ with $\mu(A_n) <\frac{\epsilon}{2^n}$ and $\exists N_n$ such that $\forall \space x \notin A_n$ and $k \ge N_n \space |f_k(x) - f(x)| < \epsilon$. That is: if we define $A = \cup_{n=1}^{\infty} A_n$ with $\mu(A) < \epsilon $ then ${f_n}$ converges uniformly on $E \setminus A$. $\mathbf{Proof}$: (taken from Royden's Real Analysis) Let $A = \cup_{n=1}^{\infty} A_n \Rightarrow A \subset E$ and $\mu(A) < \sum_{n=1}^{\infty} \frac{\epsilon}{2^n} = \epsilon. \mathbf{Q1}$. choose $n_0$ such that $\frac{1}{n_0} < \epsilon$. If $x \notin A$ and $k \ge N_{n_0}$ we have $\space |f_k(x) - f(x)| < \frac{1}{n_0} < \epsilon \space$. $ \square$ $\mathbf{Q1}$: First off, I don't see how $\sum_{n=1}^{\infty} \frac{\epsilon}{2^n} = \epsilon$. It's a geometric series: $ \epsilon \sum_{n=1}^{\infty} \frac{1}{2^n} = \epsilon \frac{1}{1-\frac12} = 2 \epsilon$. Am I wrong? $\mathbf{Q2}$: The idea behind Egoroff is in order to turn almost sure convergence into uniform convergence on $E$ we only need to take away a really small set, right? Interestingly, as $\epsilon \to 0$ (that is $f_n$ is getting closer to $f$), the measure of the set $A$ is getting proportionally smaller ($\mu(A) \to 0$). So are we ultimately taking away a set of zero measure?","Let $\{f_n \}$ be a sequence of measurable functions, $f_n \to f$ $\mu$-a.e. on a measurable set $E$, $\mu(E) < \infty$. Let $\epsilon>0$ be given. Then $\forall \space n \in \mathbb{N} \space \exists A_n \subset E$ with $\mu(A_n) <\frac{\epsilon}{2^n}$ and $\exists N_n$ such that $\forall \space x \notin A_n$ and $k \ge N_n \space |f_k(x) - f(x)| < \epsilon$. That is: if we define $A = \cup_{n=1}^{\infty} A_n$ with $\mu(A) < \epsilon $ then ${f_n}$ converges uniformly on $E \setminus A$. $\mathbf{Proof}$: (taken from Royden's Real Analysis) Let $A = \cup_{n=1}^{\infty} A_n \Rightarrow A \subset E$ and $\mu(A) < \sum_{n=1}^{\infty} \frac{\epsilon}{2^n} = \epsilon. \mathbf{Q1}$. choose $n_0$ such that $\frac{1}{n_0} < \epsilon$. If $x \notin A$ and $k \ge N_{n_0}$ we have $\space |f_k(x) - f(x)| < \frac{1}{n_0} < \epsilon \space$. $ \square$ $\mathbf{Q1}$: First off, I don't see how $\sum_{n=1}^{\infty} \frac{\epsilon}{2^n} = \epsilon$. It's a geometric series: $ \epsilon \sum_{n=1}^{\infty} \frac{1}{2^n} = \epsilon \frac{1}{1-\frac12} = 2 \epsilon$. Am I wrong? $\mathbf{Q2}$: The idea behind Egoroff is in order to turn almost sure convergence into uniform convergence on $E$ we only need to take away a really small set, right? Interestingly, as $\epsilon \to 0$ (that is $f_n$ is getting closer to $f$), the measure of the set $A$ is getting proportionally smaller ($\mu(A) \to 0$). So are we ultimately taking away a set of zero measure?",,"['real-analysis', 'measure-theory']"
84,Finite at every point but unbounded on every interval,Finite at every point but unbounded on every interval,,Is is possible that a function $f$ is finite at every point but unbounded on every interval? What if f is measurable?,Is is possible that a function $f$ is finite at every point but unbounded on every interval? What if f is measurable?,,"['real-analysis', 'measure-theory', 'functions']"
85,A multiple integral question,A multiple integral question,,Proving that $$\lim_{n\to\infty}\underbrace{\int_0^1 \int_0^1 \cdots \int_0^1}_{n \text{ times}}\frac{1}{(x_1\cdot x_2\cdots x_n)^2+1} \mathrm{d}x_1\cdot\mathrm{d}x_2\cdots\mathrm{d}x_n=1$$,Proving that $$\lim_{n\to\infty}\underbrace{\int_0^1 \int_0^1 \cdots \int_0^1}_{n \text{ times}}\frac{1}{(x_1\cdot x_2\cdots x_n)^2+1} \mathrm{d}x_1\cdot\mathrm{d}x_2\cdots\mathrm{d}x_n=1$$,,"['calculus', 'real-analysis', 'limits', 'integration', 'definite-integrals']"
86,Condition for $\displaystyle \lim_{x\to 0}f(x)=\lim_{n\to\infty}f\left(\frac{1}{n}\right)$?,Condition for ?,\displaystyle \lim_{x\to 0}f(x)=\lim_{n\to\infty}f\left(\frac{1}{n}\right),Let $f:{\Bbb R}\to{\Bbb R}$. Is there a courterexample for the following equality or is it always true? $$\lim_{x\to 0}f(x)=\lim_{n\to\infty}f\left(\frac{1}{n}\right)$$ What I think is that one might need a non-continuous function since this is always true for a continuous function. Would $1_{\Bbb Q}$ work? Are there any other counterexamples?,Let $f:{\Bbb R}\to{\Bbb R}$. Is there a courterexample for the following equality or is it always true? $$\lim_{x\to 0}f(x)=\lim_{n\to\infty}f\left(\frac{1}{n}\right)$$ What I think is that one might need a non-continuous function since this is always true for a continuous function. Would $1_{\Bbb Q}$ work? Are there any other counterexamples?,,[]
87,Compute $\lim_{n\to\infty}(n-(\arccos(1/n)+\cdots+\arccos(n/n)))$,Compute,\lim_{n\to\infty}(n-(\arccos(1/n)+\cdots+\arccos(n/n))),"How would you compute $$\lim_{n\to\infty}(n-(\arccos(1/n)+\cdots+\arccos(n/n)))?$$ If we choose $n=10000$ and compute it with W|A, we get $0.78833$ that is suspiciously close to $\pi/4.$ I also discussed the limit in the chatroom. Thanks!","How would you compute $$\lim_{n\to\infty}(n-(\arccos(1/n)+\cdots+\arccos(n/n)))?$$ If we choose $n=10000$ and compute it with W|A, we get $0.78833$ that is suspiciously close to $\pi/4.$ I also discussed the limit in the chatroom. Thanks!",,"['calculus', 'real-analysis', 'limits']"
88,"$f(x)=1/x$ over $[1, \infty)$ is not Lebesgue integrable",over  is not Lebesgue integrable,"f(x)=1/x [1, \infty)","How does one show that $\chi_{[1, \infty)}1/x$ is not (Lebesgue) integrable? What I could think of is as follows: Letting $f(x)=1/x$ (defined for $x\geq 1$), define  $$ f_n(x)=f\chi_{[1, n)}(x). $$ Each $f_n$ is, therefore, Riemann integrable on $[1, n)$ with value $\ln n$, hence integrable there. As $0\leq f_n\nearrow f$ on $[1, \infty)$, the monotone increasing theorem says  $$ \int_{[1, \infty)}f_n\nearrow\int_{[1, \infty)} f $$  and so $\int_{[1, \infty)} f=\infty$ since $\ln n\nearrow\infty$. Is there a more obvious reason why the given integral isn't finite? It seems that my method needs quite some modification if we go to $n$-dimensional integrals of  $$ f(x)=\frac{1}{|x|}\chi_{|x|>1}. $$","How does one show that $\chi_{[1, \infty)}1/x$ is not (Lebesgue) integrable? What I could think of is as follows: Letting $f(x)=1/x$ (defined for $x\geq 1$), define  $$ f_n(x)=f\chi_{[1, n)}(x). $$ Each $f_n$ is, therefore, Riemann integrable on $[1, n)$ with value $\ln n$, hence integrable there. As $0\leq f_n\nearrow f$ on $[1, \infty)$, the monotone increasing theorem says  $$ \int_{[1, \infty)}f_n\nearrow\int_{[1, \infty)} f $$  and so $\int_{[1, \infty)} f=\infty$ since $\ln n\nearrow\infty$. Is there a more obvious reason why the given integral isn't finite? It seems that my method needs quite some modification if we go to $n$-dimensional integrals of  $$ f(x)=\frac{1}{|x|}\chi_{|x|>1}. $$",,"['real-analysis', 'measure-theory', 'lebesgue-integral']"
89,Infinite Series Manipulations,Infinite Series Manipulations,,"Is there any comprehensive list (books, online, ...) of rules for manipulating infinite series (partial sums) to find convergence of a sum? Often authors use some ""trick"" to compute an infinite series. Following this trick is always a disclaimer, such as ""adding infinite sequences is not the same as adding discrete values so the usual rules of algebra will not work."" Huh? Then how am I supposed to learn what I can and cannot do when all I have been shown is a trick that works in a particular case? For example, \begin{align*} \sum_{n=1}^{\infty}\frac{1}{n(n+1)}\rightarrow S_{n}&=\sum_{k=1}^{n}\frac{1}{k(k+1)}\\\ &=\frac{1}{1\cdot 2}+\frac{1}{2\cdot 3}+\frac{1}{3\cdot 4}+\frac{1}{n(n+1)}\\\ &=\;  \sum_{k=1}^{n}(\frac{1}{k}-\frac{1}{k+1})\\ &=(1-\frac{1}{2})+(\frac{1}{2}-\frac{1}{3})+(\frac{1}{3}-\frac{1}{4})+\cdots+(\frac{1}{n}-\frac{1}{n+1})\\\ &=1-\frac{1}{n+1}\rightarrow \lim_{n\rightarrow \infty}\left(1-\frac{1}{n+1}\right)=1-0=1. \end{align*} OK, I see the author used partial fractions expansion followed by grouping like-terms. However, elsewhere I'll find that grouping like-terms will lead you to the wrong answer. Such is the case with this infinite sum: $$ S_{n}=1-1+1-1+1+\cdots \stackrel{?}{\Longrightarrow} (1-1)+(1-1)+\cdots=0 $$ $$ S_{n}=1-1+1-1+1+\cdots\stackrel{?}{\Longrightarrow} 1+(-1+1)+(-1+1)+\cdots=1 $$ ...So does this mean infinite series are not associative? Or are they? What are the invariant properties of infinite series that can be confidently used when manipulating an infinite series? FOLLOW UP: Thanks for the great answers so far, yet they have led me to question the practicality of writing in summation form. Is it not more useful to simply write the infinite sum as its corresponding infinite partial sum sequence? E.g.: Why is the fourier series written as an infinite sum? I realize it can be used to create a continuous analog for many types of discontinuous functions, and yet I feel I gain nothing when I write out the fourier series to say, a solution to a partial differential equation, because Idk how to evaluate the output to a particular input since its defined by an infinite sum of sin's and cosines. (Besides finding the partial sum sequence) Is there not a more direct way?","Is there any comprehensive list (books, online, ...) of rules for manipulating infinite series (partial sums) to find convergence of a sum? Often authors use some ""trick"" to compute an infinite series. Following this trick is always a disclaimer, such as ""adding infinite sequences is not the same as adding discrete values so the usual rules of algebra will not work."" Huh? Then how am I supposed to learn what I can and cannot do when all I have been shown is a trick that works in a particular case? For example, OK, I see the author used partial fractions expansion followed by grouping like-terms. However, elsewhere I'll find that grouping like-terms will lead you to the wrong answer. Such is the case with this infinite sum: ...So does this mean infinite series are not associative? Or are they? What are the invariant properties of infinite series that can be confidently used when manipulating an infinite series? FOLLOW UP: Thanks for the great answers so far, yet they have led me to question the practicality of writing in summation form. Is it not more useful to simply write the infinite sum as its corresponding infinite partial sum sequence? E.g.: Why is the fourier series written as an infinite sum? I realize it can be used to create a continuous analog for many types of discontinuous functions, and yet I feel I gain nothing when I write out the fourier series to say, a solution to a partial differential equation, because Idk how to evaluate the output to a particular input since its defined by an infinite sum of sin's and cosines. (Besides finding the partial sum sequence) Is there not a more direct way?","\begin{align*}
\sum_{n=1}^{\infty}\frac{1}{n(n+1)}\rightarrow S_{n}&=\sum_{k=1}^{n}\frac{1}{k(k+1)}\\\
&=\frac{1}{1\cdot 2}+\frac{1}{2\cdot 3}+\frac{1}{3\cdot 4}+\frac{1}{n(n+1)}\\\
&=\;  \sum_{k=1}^{n}(\frac{1}{k}-\frac{1}{k+1})\\
&=(1-\frac{1}{2})+(\frac{1}{2}-\frac{1}{3})+(\frac{1}{3}-\frac{1}{4})+\cdots+(\frac{1}{n}-\frac{1}{n+1})\\\
&=1-\frac{1}{n+1}\rightarrow \lim_{n\rightarrow \infty}\left(1-\frac{1}{n+1}\right)=1-0=1.
\end{align*} 
S_{n}=1-1+1-1+1+\cdots \stackrel{?}{\Longrightarrow} (1-1)+(1-1)+\cdots=0
 
S_{n}=1-1+1-1+1+\cdots\stackrel{?}{\Longrightarrow} 1+(-1+1)+(-1+1)+\cdots=1
","['calculus', 'real-analysis', 'sequences-and-series', 'online-resources']"
90,"If a function is uniformly continuous on $(-\infty,-1]$ and $[-1,\infty)$ is it uniformly continuous on $\mathbb{R}$ [duplicate]",If a function is uniformly continuous on  and  is it uniformly continuous on  [duplicate],"(-\infty,-1] [-1,\infty) \mathbb{R}","This question already has answers here : Uniform continuity on $[a,b]$ and $ [b,c]$ $\implies$ uniform continuity on $[a,c]$. (2 answers) Closed 10 years ago . Is it correct to say that if $f(x)$ is uniformly continuous on $(-\infty,-1]$ and $[-1,\infty)$, then it is uniformly continuous on $\mathbb{R}$? I don't think this is true but cannot think of a counterexample. Below there is an example of where I want to use this. Thanks for any help Prove that $f(x)=|x|^{\frac{1}{2}}$ is uniformly continuous on $\mathbb{R}$ Proof. As $|x|^{\frac 12}$ is differentiable on $(-\infty,-1]$ and $[1,\infty)$ with the derivatives bounded then it is uniformly continuous on these intervals. It is also continuous on $[-1,1]$ and so it is uniformly continuous. It is therefore continuous on $\mathbb{R}$.","This question already has answers here : Uniform continuity on $[a,b]$ and $ [b,c]$ $\implies$ uniform continuity on $[a,c]$. (2 answers) Closed 10 years ago . Is it correct to say that if $f(x)$ is uniformly continuous on $(-\infty,-1]$ and $[-1,\infty)$, then it is uniformly continuous on $\mathbb{R}$? I don't think this is true but cannot think of a counterexample. Below there is an example of where I want to use this. Thanks for any help Prove that $f(x)=|x|^{\frac{1}{2}}$ is uniformly continuous on $\mathbb{R}$ Proof. As $|x|^{\frac 12}$ is differentiable on $(-\infty,-1]$ and $[1,\infty)$ with the derivatives bounded then it is uniformly continuous on these intervals. It is also continuous on $[-1,1]$ and so it is uniformly continuous. It is therefore continuous on $\mathbb{R}$.",,['real-analysis']
91,Tricky: Convergence,Tricky: Convergence,,"This question is marked as tricky: I have a sequence of continuous functions $(f_n)$ s.t. $f_n$ converges uniformly to the continuous $F$ on $[a,b]$ and a sequence $(x_k)\subset[a,b]$ s.t. $x_k\to x$. I wish to show that $f_k(x_k)\to F(x)$. My thoughts: \begin{align*}\lim_{k\to\infty} [f_k(x_k)-F(x_k)] &= \lim_{k\to\infty} f_k(x_k)-\lim_{k\to\infty} F(x_k)&\\ &= F(\lim_{k\to\infty} x_k)-F(x)&\mbox{ by continuity of F}\\ &=F(x)-F(x)=0&\mbox{ by continuity of } F. \end{align*} I am sure that there is something worng with my argument since it does not seem 'tricky'. I must have stepped into some trap. Could someone please help me out?","This question is marked as tricky: I have a sequence of continuous functions $(f_n)$ s.t. $f_n$ converges uniformly to the continuous $F$ on $[a,b]$ and a sequence $(x_k)\subset[a,b]$ s.t. $x_k\to x$. I wish to show that $f_k(x_k)\to F(x)$. My thoughts: \begin{align*}\lim_{k\to\infty} [f_k(x_k)-F(x_k)] &= \lim_{k\to\infty} f_k(x_k)-\lim_{k\to\infty} F(x_k)&\\ &= F(\lim_{k\to\infty} x_k)-F(x)&\mbox{ by continuity of F}\\ &=F(x)-F(x)=0&\mbox{ by continuity of } F. \end{align*} I am sure that there is something worng with my argument since it does not seem 'tricky'. I must have stepped into some trap. Could someone please help me out?",,"['real-analysis', 'analysis']"
92,Bounded linear operators that commute with translation,Bounded linear operators that commute with translation,,"I'm trying to read Elias Stein's ""Singular Integrals"" book, and in the beginning of the second chapter, he states two results classifying bounded linear operators that commute (on $L^1$ and $L^2$ respectively). The first one reads: Let $T: L^1(\mathbb{R}^n) \to L^1(\mathbb{R}^n)$ be a bounded linear transformation. Then $T$ commutes with translations if and only if there exists a measure $\mu$ in the dual of $C_0(\mathbb{R}^n)$ (continuous functions vanishing at infinity), s.t. $T(f) = f \ast \mu$ for every $f \in L^1(\mathbb{R}^n)$. It is also true that $\|T\|=\|\mu\|$. The second one says: Let $T:L^2(\mathbb{R}^n) \to L^2(\mathbb{R}^n)$ be bounded and linear. Then $T$ commutes with translation if and only if there exists a bounded measurable function $m(y)$ so that $(T\hat{f})(y) = m(y) \hat{f}(y)$ for all $f \in L^2(\mathbb{R}^n)$. It is also true that $\|T\|=\|m\|_\infty$. I was wondering if anyone had a reference to a proof of these two results or could explain why they are true.","I'm trying to read Elias Stein's ""Singular Integrals"" book, and in the beginning of the second chapter, he states two results classifying bounded linear operators that commute (on $L^1$ and $L^2$ respectively). The first one reads: Let $T: L^1(\mathbb{R}^n) \to L^1(\mathbb{R}^n)$ be a bounded linear transformation. Then $T$ commutes with translations if and only if there exists a measure $\mu$ in the dual of $C_0(\mathbb{R}^n)$ (continuous functions vanishing at infinity), s.t. $T(f) = f \ast \mu$ for every $f \in L^1(\mathbb{R}^n)$. It is also true that $\|T\|=\|\mu\|$. The second one says: Let $T:L^2(\mathbb{R}^n) \to L^2(\mathbb{R}^n)$ be bounded and linear. Then $T$ commutes with translation if and only if there exists a bounded measurable function $m(y)$ so that $(T\hat{f})(y) = m(y) \hat{f}(y)$ for all $f \in L^2(\mathbb{R}^n)$. It is also true that $\|T\|=\|m\|_\infty$. I was wondering if anyone had a reference to a proof of these two results or could explain why they are true.",,"['real-analysis', 'harmonic-analysis']"
93,How can one show that the topology of convergence in measure is separable?,How can one show that the topology of convergence in measure is separable?,,"Let $X$ be a polish space equiped with the borel sigma-algebra and a probability measure $\mu$. How can one show that the set of all borel measurable functions $f:X\rightarrow R $  ($R$ being the real numbers), where two a.e. equal functions are identified, equiped with the topology of convegence in measure is separable?","Let $X$ be a polish space equiped with the borel sigma-algebra and a probability measure $\mu$. How can one show that the set of all borel measurable functions $f:X\rightarrow R $  ($R$ being the real numbers), where two a.e. equal functions are identified, equiped with the topology of convegence in measure is separable?",,"['real-analysis', 'measure-theory', 'functional-analysis']"
94,what functions or classes of functions are Riemann non-integrable but Lebesgue integrable,what functions or classes of functions are Riemann non-integrable but Lebesgue integrable,,"I am wondering if there are some other examples of Riemann non-integrable but Lebesgue integrable, besides the well-known Dirichlet function. Thanks.","I am wondering if there are some other examples of Riemann non-integrable but Lebesgue integrable, besides the well-known Dirichlet function. Thanks.",,['real-analysis']
95,Limit of $S(n) = \sum_{k=1}^{\infty} \left(1 - \prod_{j=1}^{n-1}\left(1-\frac{j}{2^k}\right)\right)$ - Part II,Limit of  - Part II,S(n) = \sum_{k=1}^{\infty} \left(1 - \prod_{j=1}^{n-1}\left(1-\frac{j}{2^k}\right)\right),"This is a follow up of Limit of $S(n) = \sum_{k=1}^{\infty} \left(1 - \prod_{j=1}^{n-1}\left(1-\frac{j}{2^k}\right)\right)$ More details can be found in the above thread. Let $S(n) = \displaystyle \sum_{k=1}^{\infty} \left(1 - \prod_{j=1}^{n-1}\left(1-\frac{j}{2^k}\right)\right)$ Mike has proved that $S(n)$ in fact diverges at-least faster than $\log_2(\lfloor n-1 \rfloor)$. Now based on what Mike has worked this conjectures arises: $\displaystyle \lim_{n \rightarrow \infty} (2 \log_{2}(n) - S(n)) = \alpha$. Also, can $\alpha$ be expressed in terms of other familiar constants. $\frac{\pi \gamma}{e}$ seems to be a close guess. The numerical evidence seem to suggest they are true.  For example, we have the following graph of $2 \log_2 n - S(n)$ for $n \leq 300$. (More numerical evidence: The value of $2 \log_2 n - S(n)$, is, for $n = 1000$, $2000$, and $3000$, respectively, $0.667734$, $0.667494$, and $0.667413$.) An alternative expression for $S(n)$ was worked out by Moron in the previously-mentioned question: $$S(n) = - \sum_{k=1}^{n-1} \frac{s(n,k)}{2^{n-k}-1},$$  where $s(n,k)$ is a Stirling number of the first kind .","This is a follow up of Limit of $S(n) = \sum_{k=1}^{\infty} \left(1 - \prod_{j=1}^{n-1}\left(1-\frac{j}{2^k}\right)\right)$ More details can be found in the above thread. Let $S(n) = \displaystyle \sum_{k=1}^{\infty} \left(1 - \prod_{j=1}^{n-1}\left(1-\frac{j}{2^k}\right)\right)$ Mike has proved that $S(n)$ in fact diverges at-least faster than $\log_2(\lfloor n-1 \rfloor)$. Now based on what Mike has worked this conjectures arises: $\displaystyle \lim_{n \rightarrow \infty} (2 \log_{2}(n) - S(n)) = \alpha$. Also, can $\alpha$ be expressed in terms of other familiar constants. $\frac{\pi \gamma}{e}$ seems to be a close guess. The numerical evidence seem to suggest they are true.  For example, we have the following graph of $2 \log_2 n - S(n)$ for $n \leq 300$. (More numerical evidence: The value of $2 \log_2 n - S(n)$, is, for $n = 1000$, $2000$, and $3000$, respectively, $0.667734$, $0.667494$, and $0.667413$.) An alternative expression for $S(n)$ was worked out by Moron in the previously-mentioned question: $$S(n) = - \sum_{k=1}^{n-1} \frac{s(n,k)}{2^{n-k}-1},$$  where $s(n,k)$ is a Stirling number of the first kind .",,"['number-theory', 'real-analysis']"
96,Proving $\sum_{i=m+1}^\infty \frac{m!^{i/m}}{i!}<1$.,Proving .,\sum_{i=m+1}^\infty \frac{m!^{i/m}}{i!}<1,"One of this days, the user @AspiringMat was trying to prove that, for any integer $m\ge 1$ , $$\sum_{i=m+1}^\infty \frac{m!^{i/m}}{i!}<1.$$ and asked for help here on MSE. I've spent too much of my time attempting to solve this. However when I finally got to it, I realized he deleted the question ! I really don't want all my effort to be wasted, so... I'm asking and answering his question once again here. My solution is really messed up, so feel free to post your own as well.","One of this days, the user @AspiringMat was trying to prove that, for any integer , and asked for help here on MSE. I've spent too much of my time attempting to solve this. However when I finally got to it, I realized he deleted the question ! I really don't want all my effort to be wasted, so... I'm asking and answering his question once again here. My solution is really messed up, so feel free to post your own as well.",m\ge 1 \sum_{i=m+1}^\infty \frac{m!^{i/m}}{i!}<1.,"['real-analysis', 'sequences-and-series', 'taylor-expansion']"
97,How to compute $\displaystyle\lim_{n\to\infty} \frac1{n+1}\sum_{k=1}^n \left|X+\frac kn\right|-\left|X-\frac kn\right|$?,How to compute ?,\displaystyle\lim_{n\to\infty} \frac1{n+1}\sum_{k=1}^n \left|X+\frac kn\right|-\left|X-\frac kn\right|,"I was just playing around with the real absolute value, trying to build something smooth (for no particular reason). After some experimentation I got to the sequence $(f_n)_n$ given by $$f_n(X) := \frac1{n+1}\sum_{k=1}^n \left|X+\frac kn\right|-\left|X-\frac kn\right|$$ This sequence got my attention for its graphs (in fact, I build it so its graphs would behave like this): I wonder how to compute $f(X) = \displaystyle\lim_{n\to\infty} f_n(X)$ . I haven't been able to solve this not even with the help of Wolfram Mathematica. Is $f$ really smooth? Of course, I'm only interested in values in $(-1, 1)$ for $X$ .","I was just playing around with the real absolute value, trying to build something smooth (for no particular reason). After some experimentation I got to the sequence given by This sequence got my attention for its graphs (in fact, I build it so its graphs would behave like this): I wonder how to compute . I haven't been able to solve this not even with the help of Wolfram Mathematica. Is really smooth? Of course, I'm only interested in values in for .","(f_n)_n f_n(X) := \frac1{n+1}\sum_{k=1}^n \left|X+\frac kn\right|-\left|X-\frac kn\right| f(X) = \displaystyle\lim_{n\to\infty} f_n(X) f (-1, 1) X","['real-analysis', 'limits', 'absolute-value', 'smooth-functions']"
98,"Closed form for Bessel type integral $\int_0^\infty J_0 (rx) \,\frac{e^{-z\sqrt{x^2+a^2}}}{\sqrt{x^2+a^2}} \, {\rm d}x$",Closed form for Bessel type integral,"\int_0^\infty J_0 (rx) \,\frac{e^{-z\sqrt{x^2+a^2}}}{\sqrt{x^2+a^2}} \, {\rm d}x","Following this link , Zacky calculated the integral $$\int_0^\infty x{J}_0(rx) \,\frac{e^{-z\sqrt{x^2+a^2}}}{\sqrt{x^2+a^2}}\, {\rm d}x=\frac{e^{-a\sqrt{r^2+z^2}}}{\sqrt{r^2+z^2}} \,.$$ It is a special case of the more general Bessel type integrals found in Watson . I'm wondering if also a closed form expression for $$\int_0^\infty {J}_0(rx) \,\frac{e^{-z\sqrt{x^2+a^2}}}{\sqrt{x^2+a^2}} \, {\rm d}x$$ can be found. I tried the method Zacky applied, but it doesn't seem to work, because the appearance of error-functions complicate matters. I also tried the approach from Watson, but only converted the improper integral into a definite integral. Equation 2f (Watson, p.416) gives, while omitting the factor $t^{\mu+1}$ , for $\nu=1/2$ and $\mu=0$ $$\sqrt{\frac{\pi}{2a}}\int_0^\infty J_0(bt) \, \frac{e^{-a\sqrt{t^2+z^2}}}{\sqrt{t^2+z^2}} \, {\rm d}t = \frac12 \int_0^\infty {\rm d}u \int_0^\infty {\rm d}t \, J_0(bt) \, u^{-3/2}\, e^{-\frac{a}{2}\left(u+\frac{t^2+z^2}{u}\right)} \\ =\frac14 \sqrt{\frac{2\pi}{a}} \int_0^\infty \frac{{\rm d}u}{u} \, I_0\left(\frac{b^2u}{4a}\right) \, e^{-\frac{(2a^2+b^2)u}{4a}-\frac{z^2a}{2u}} $$ where the $t$ -integral can be found here . Substituting $v=\frac{b^2u}{4a}$ then gives $$=\frac14 \sqrt{\frac{2\pi}{a}} \int_0^\infty \frac{{\rm d}v}{v} \, I_0(v) \, e^{-\frac{(2a^2+b^2)v}{b^2}-\frac{b^2z^2}{8v}} \, .$$ Using the representation $$I_0(v)=\frac1\pi \int_0^\pi {\rm d}\theta \, e^{-v\cos(\theta)}$$ finally yields $$=\frac{1}{\sqrt{8\pi a}} \int_0^\pi {\rm d}\theta \int_0^\infty \frac{{\rm d}v}{v} \, e^{-\frac{\left(a^2+b^2\cos^2(\theta/2)\right)2v}{b^2}-\frac{b^2z^2}{8v}} \\ \stackrel{w=\frac{\left(a^2+b^2\cos^2(\theta/2)\right)2v}{b^2}}{=} \frac{1}{\sqrt{8\pi a}} \int_0^\pi {\rm d}\theta \int_0^\infty \frac{{\rm d}w}{w} \, e^{-w-\frac{\left(z\sqrt{a^2+b^2\cos^2(\theta/2)}\right)^2}{4w}} \\ = \frac{1}{\sqrt{2\pi a}} \int_0^\pi {\rm d}\theta \, K_0\left(z\sqrt{a^2+b^2\cos^2(\theta/2)}\right)$$ resorting to this integral representation of the Bessel-K-function .","Following this link , Zacky calculated the integral It is a special case of the more general Bessel type integrals found in Watson . I'm wondering if also a closed form expression for can be found. I tried the method Zacky applied, but it doesn't seem to work, because the appearance of error-functions complicate matters. I also tried the approach from Watson, but only converted the improper integral into a definite integral. Equation 2f (Watson, p.416) gives, while omitting the factor , for and where the -integral can be found here . Substituting then gives Using the representation finally yields resorting to this integral representation of the Bessel-K-function .","\int_0^\infty x{J}_0(rx) \,\frac{e^{-z\sqrt{x^2+a^2}}}{\sqrt{x^2+a^2}}\, {\rm d}x=\frac{e^{-a\sqrt{r^2+z^2}}}{\sqrt{r^2+z^2}} \,. \int_0^\infty {J}_0(rx) \,\frac{e^{-z\sqrt{x^2+a^2}}}{\sqrt{x^2+a^2}} \, {\rm d}x t^{\mu+1} \nu=1/2 \mu=0 \sqrt{\frac{\pi}{2a}}\int_0^\infty J_0(bt) \, \frac{e^{-a\sqrt{t^2+z^2}}}{\sqrt{t^2+z^2}} \, {\rm d}t = \frac12 \int_0^\infty {\rm d}u \int_0^\infty {\rm d}t \, J_0(bt) \, u^{-3/2}\, e^{-\frac{a}{2}\left(u+\frac{t^2+z^2}{u}\right)} \\
=\frac14 \sqrt{\frac{2\pi}{a}} \int_0^\infty \frac{{\rm d}u}{u} \, I_0\left(\frac{b^2u}{4a}\right) \, e^{-\frac{(2a^2+b^2)u}{4a}-\frac{z^2a}{2u}}
 t v=\frac{b^2u}{4a} =\frac14 \sqrt{\frac{2\pi}{a}} \int_0^\infty \frac{{\rm d}v}{v} \, I_0(v) \, e^{-\frac{(2a^2+b^2)v}{b^2}-\frac{b^2z^2}{8v}} \, . I_0(v)=\frac1\pi \int_0^\pi {\rm d}\theta \, e^{-v\cos(\theta)} =\frac{1}{\sqrt{8\pi a}} \int_0^\pi {\rm d}\theta \int_0^\infty \frac{{\rm d}v}{v} \, e^{-\frac{\left(a^2+b^2\cos^2(\theta/2)\right)2v}{b^2}-\frac{b^2z^2}{8v}} \\
\stackrel{w=\frac{\left(a^2+b^2\cos^2(\theta/2)\right)2v}{b^2}}{=} \frac{1}{\sqrt{8\pi a}} \int_0^\pi {\rm d}\theta \int_0^\infty \frac{{\rm d}w}{w} \, e^{-w-\frac{\left(z\sqrt{a^2+b^2\cos^2(\theta/2)}\right)^2}{4w}} \\
= \frac{1}{\sqrt{2\pi a}} \int_0^\pi {\rm d}\theta \, K_0\left(z\sqrt{a^2+b^2\cos^2(\theta/2)}\right)","['real-analysis', 'integration', 'analysis', 'improper-integrals']"
99,Rank 1 operator on an infinite dimensional vector space number of eigenvalues.,Rank 1 operator on an infinite dimensional vector space number of eigenvalues.,,"Does a rank 1 bounded operator from $\mathscr{K}:L^2([0,1])\to L^2([0,1])$ have at most 1 non-zero eigenvalue? The reason this is not obvious to me is that $L^2([0,1])$ is infinite dimensional. In general, is rank a bound on the number of eigenvalues?","Does a rank 1 bounded operator from have at most 1 non-zero eigenvalue? The reason this is not obvious to me is that is infinite dimensional. In general, is rank a bound on the number of eigenvalues?","\mathscr{K}:L^2([0,1])\to L^2([0,1]) L^2([0,1])","['real-analysis', 'linear-algebra', 'functional-analysis', 'analysis']"
