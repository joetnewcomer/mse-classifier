,title_raw,title_text,title_latex,body_raw,body_text,body_latex,tags
0,Is the Cantor set made of interval endpoints?,Is the Cantor set made of interval endpoints?,,"The Cantor set is closed, so its complement is open.  So the complement can be written as a countable union of disjoint open intervals.  Why can we not just enumerate all endpoints of the countably many intervals, and conclude the Cantor set is countable?","The Cantor set is closed, so its complement is open.  So the complement can be written as a countable union of disjoint open intervals.  Why can we not just enumerate all endpoints of the countably many intervals, and conclude the Cantor set is countable?",,"['real-analysis', 'general-topology', 'elementary-set-theory', 'cantor-set']"
1,Monotone+continuous but not differentiable,Monotone+continuous but not differentiable,,Is there a continuous and monotone function that's nowhere differentiable ?,Is there a continuous and monotone function that's nowhere differentiable ?,,['real-analysis']
2,Ramanujan's First Letter to Hardy and the Number of $3$-Smooth Integers,Ramanujan's First Letter to Hardy and the Number of -Smooth Integers,3,"A positive integer is $B$-smooth if and only if all of its prime divisors are less than or equal to a positive real $B$. For example, the $3$-smooth integers are of the form $2^{a} 3^{b}$ with non-negative exponents $a$ and $b$, and those integers less than or equal to $20$ are $\{1,2,3,4,6,8,9,12,16,18\}$. In Ramanujan's first letter to G. H. Hardy, Ramanujan emphatically quotes (without proof) his result on the number of $3$-smooth integers less than or equal to $N > 1$, \begin{eqnarray} \frac{\log 2 N \ \log 3N}{2 \log 2 \ \log 3}. \end{eqnarray} This is an amazingly accurate approximation, as it differs from the exact value by less than 3 for the first $2^{1000} \approx 1.07 \times 10^{301}$ integers, as shown by Pillai. Question : Knowing full well that Ramanujan only gave proofs of his own claims while working in England, I wonder if a proof of this particular estimate appears somewhere in the literature. Is this problem still open? If not, what is a reference discussing its proof? Thanks!","A positive integer is $B$-smooth if and only if all of its prime divisors are less than or equal to a positive real $B$. For example, the $3$-smooth integers are of the form $2^{a} 3^{b}$ with non-negative exponents $a$ and $b$, and those integers less than or equal to $20$ are $\{1,2,3,4,6,8,9,12,16,18\}$. In Ramanujan's first letter to G. H. Hardy, Ramanujan emphatically quotes (without proof) his result on the number of $3$-smooth integers less than or equal to $N > 1$, \begin{eqnarray} \frac{\log 2 N \ \log 3N}{2 \log 2 \ \log 3}. \end{eqnarray} This is an amazingly accurate approximation, as it differs from the exact value by less than 3 for the first $2^{1000} \approx 1.07 \times 10^{301}$ integers, as shown by Pillai. Question : Knowing full well that Ramanujan only gave proofs of his own claims while working in England, I wonder if a proof of this particular estimate appears somewhere in the literature. Is this problem still open? If not, what is a reference discussing its proof? Thanks!",,"['number-theory', 'real-analysis', 'reference-request', 'analytic-number-theory']"
3,Understanding the assumptions in the Reverse Fatou's Lemma,Understanding the assumptions in the Reverse Fatou's Lemma,,"Fatou's Lemma says the following: If $(f_n)$ is a sequence of extended real-valued, nonnegative, measurable functions defined on a measure space $\left(\mathbf{X},\mathcal{X},\mu\right)$, then   $$ \int\lim\inf f_n d\mu \leq \lim\inf \int f_n d\mu. $$ In the statement of the Reverse Fatou's Lemma there's an addtional requirement that the given sequence be dominated by an integrable function.  I'm interested in understanding what breaks down if this condition is not satisfied.  For the sake of clarity and notation, here's the statement of the Reverse Fatou's Lemma: Let $(f_n)$ be a sequence of extended real-valued functions defined on a measure space $\left(\mathbf{X},\mathcal{X},\mu\right)$.  If there exists an integrable function $g$ on $\mathbf{X}$ such that $f_n \leq g$ for all $n$, then   $$ \lim\sup\int f_n d\mu \leq \int\lim\sup f_n d\mu. $$ Again, I'm curious to know what happens if this additional condition that the sequence be dominated is not satisfied.  In the proofs that I've seen of the Reverse Fatou's Lemma they've all taken advantage of the fact that the functions are dominated, but I just don't see why there can't be a proof of the inequality that doesn't use this assumption. My interest was further piqued by the following problem I came across in Bartle's Elements of Integration and Lebesgue Measure: Let $(f_n)$ be a sequence of extended real-valued, nonnegative functions defined on $\left(\mathbf{X},\mathcal{X},\mu\right)$, $f_n \to f$, and let $\int f d\mu =\lim \int f_n d\mu < \infty.$  Show that for any $E \in \mathcal{X},$ $$\int_E f d\mu =\lim \int_E f_n d\mu.$$ I was able to prove this through two applications of Fatou's Lemma and use of the nice identity $\lim\sup(-f_n) =-\lim\inf(f_n)$.  But there was another proof I abandoned after I failed to prove that the Reverse Fatou's Lemma held with the given hypotheses. Any insight is much appreciated.","Fatou's Lemma says the following: If $(f_n)$ is a sequence of extended real-valued, nonnegative, measurable functions defined on a measure space $\left(\mathbf{X},\mathcal{X},\mu\right)$, then   $$ \int\lim\inf f_n d\mu \leq \lim\inf \int f_n d\mu. $$ In the statement of the Reverse Fatou's Lemma there's an addtional requirement that the given sequence be dominated by an integrable function.  I'm interested in understanding what breaks down if this condition is not satisfied.  For the sake of clarity and notation, here's the statement of the Reverse Fatou's Lemma: Let $(f_n)$ be a sequence of extended real-valued functions defined on a measure space $\left(\mathbf{X},\mathcal{X},\mu\right)$.  If there exists an integrable function $g$ on $\mathbf{X}$ such that $f_n \leq g$ for all $n$, then   $$ \lim\sup\int f_n d\mu \leq \int\lim\sup f_n d\mu. $$ Again, I'm curious to know what happens if this additional condition that the sequence be dominated is not satisfied.  In the proofs that I've seen of the Reverse Fatou's Lemma they've all taken advantage of the fact that the functions are dominated, but I just don't see why there can't be a proof of the inequality that doesn't use this assumption. My interest was further piqued by the following problem I came across in Bartle's Elements of Integration and Lebesgue Measure: Let $(f_n)$ be a sequence of extended real-valued, nonnegative functions defined on $\left(\mathbf{X},\mathcal{X},\mu\right)$, $f_n \to f$, and let $\int f d\mu =\lim \int f_n d\mu < \infty.$  Show that for any $E \in \mathcal{X},$ $$\int_E f d\mu =\lim \int_E f_n d\mu.$$ I was able to prove this through two applications of Fatou's Lemma and use of the nice identity $\lim\sup(-f_n) =-\lim\inf(f_n)$.  But there was another proof I abandoned after I failed to prove that the Reverse Fatou's Lemma held with the given hypotheses. Any insight is much appreciated.",,"['real-analysis', 'measure-theory', 'integration']"
4,Does there exist positive rational $s$ for which $\zeta(s)$ is a positive integer?,Does there exist positive rational  for which  is a positive integer?,s \zeta(s),"Does there exist positive rational $s$ for which the Riemann Zeta function $\zeta(s) \in N$ or equivalently, does there exist finite positive integers $\ell,m$ and $n$ such that $$\zeta\left(1+\dfrac{\ell}{m}\right) = n$$ Update: 23-Mar-2023 : New and faster code using hash map. Using this code and the method described in my answer below, I have been able to show that if there is a solution then $l > 1.7\times 10^5$ . def get_n_value(a,b):     c_b = (b/a).n(prec = prec)     return max(2, -1 + floor(0.07281584548367672486058637587/(eg - c_b)))  def get_c_value(m):     i   = 1     c_m = 0     while (i <= itr):         c_m = (m + c_m - zeta(1 + 1/(m - 1 + c_m))).n(prec = prec)         i = i + 1     return c_m  def get_next_b(c_n1,b):     b_prev = b     b = 1 + floor(c_n1*a)     if b <= b_prev:         b = b_prev + 1     if gcd(a,b) > 1:         b = b + 1      return b  a = 1  step = 10^1 target = a + step - 1 sd = 0  prec = 1500 n_max = 0  eg = (1 - euler_gamma).n(prec = prec) itr = 50 # c_dict = {} c_2 = get_c_value(2)    while True:     c_n1 = c_2     b = 1 + floor(c_2*a)     b_max = floor(eg*a)     depth = 0          while(b <= b_max):         if(gcd(b,a) == 1):             found = False             test = b/a.n(prec = prec)             n = get_n_value(a,b)                          if n in c_dict:                 c_n = c_dict.get(n)             else:                 c_n = get_c_value(n)                 c_dict[n] = c_n                              while found == False:                              if (n+1) in c_dict:                     c_n1 = c_dict.get(n+1)                 else:                     c_n1 = get_c_value(n+1)                     c_dict[n+1] = c_n1                  if c_n < test and test < c_n1:                     found = True                     depth  = depth + 1                      if (n > n_max):                         n_max = n                         # print(""Maximum n is at:"", a, b, n_max)                                              # print('found',a,b,'witness =',n)                     break                 else:                     c_n = c_n1                     n = n + 1                                  b = get_next_b(c_n1,b)                          if(b > b_max):                 break         else:             b = get_next_b(c_n1,b)             if(b > b_max):                 break      sd = sd + depth          if a == target:         l = len(c_dict)         print(a,'dict', l,l/a.n(), 'dep', depth, 'sd',sd, sd/a.n())         target = target + step      a = a + 1","Does there exist positive rational for which the Riemann Zeta function or equivalently, does there exist finite positive integers and such that Update: 23-Mar-2023 : New and faster code using hash map. Using this code and the method described in my answer below, I have been able to show that if there is a solution then . def get_n_value(a,b):     c_b = (b/a).n(prec = prec)     return max(2, -1 + floor(0.07281584548367672486058637587/(eg - c_b)))  def get_c_value(m):     i   = 1     c_m = 0     while (i <= itr):         c_m = (m + c_m - zeta(1 + 1/(m - 1 + c_m))).n(prec = prec)         i = i + 1     return c_m  def get_next_b(c_n1,b):     b_prev = b     b = 1 + floor(c_n1*a)     if b <= b_prev:         b = b_prev + 1     if gcd(a,b) > 1:         b = b + 1      return b  a = 1  step = 10^1 target = a + step - 1 sd = 0  prec = 1500 n_max = 0  eg = (1 - euler_gamma).n(prec = prec) itr = 50 # c_dict = {} c_2 = get_c_value(2)    while True:     c_n1 = c_2     b = 1 + floor(c_2*a)     b_max = floor(eg*a)     depth = 0          while(b <= b_max):         if(gcd(b,a) == 1):             found = False             test = b/a.n(prec = prec)             n = get_n_value(a,b)                          if n in c_dict:                 c_n = c_dict.get(n)             else:                 c_n = get_c_value(n)                 c_dict[n] = c_n                              while found == False:                              if (n+1) in c_dict:                     c_n1 = c_dict.get(n+1)                 else:                     c_n1 = get_c_value(n+1)                     c_dict[n+1] = c_n1                  if c_n < test and test < c_n1:                     found = True                     depth  = depth + 1                      if (n > n_max):                         n_max = n                         # print(""Maximum n is at:"", a, b, n_max)                                              # print('found',a,b,'witness =',n)                     break                 else:                     c_n = c_n1                     n = n + 1                                  b = get_next_b(c_n1,b)                          if(b > b_max):                 break         else:             b = get_next_b(c_n1,b)             if(b > b_max):                 break      sd = sd + depth          if a == target:         l = len(c_dict)         print(a,'dict', l,l/a.n(), 'dep', depth, 'sd',sd, sd/a.n())         target = target + step      a = a + 1","s \zeta(s) \in N \ell,m n \zeta\left(1+\dfrac{\ell}{m}\right) = n l > 1.7\times 10^5","['real-analysis', 'number-theory', 'prime-numbers', 'analytic-number-theory', 'riemann-zeta']"
5,Calculating $\lim_{n\to\infty}\sqrt{n}\sin(\sin...(\sin(x)..)$,Calculating,\lim_{n\to\infty}\sqrt{n}\sin(\sin...(\sin(x)..),"I was asked today by a friend to calculate a limit and I am having trouble with the question. Denote $\sin_{1}:=\sin$ and for $n>1$ define $\sin_{n}=\sin(\sin_{n-1})$. Calculate $\lim_{n\to\infty}\sqrt{n}\sin_{n}(x)$ for $x\in\mathbb{R}$ (the answer should be a function of $x$ ). My thoughts: It is sufficient to find the limit for $x\in[0,2\pi]$ , and it is easy to find the limit at $0,2\pi$ so we need to find the limit for $x\in(0,2\pi)$. If $[a,b]\subset(0,\pi)$ or $[a,b]\subset(\pi,2\pi)$ we have it that then $$\max_{x\in[a,b]}|\sin'(x)|=\max_{x\in[a,b]}|\cos(x)|<\lambda\leq1$$ hence the map $\sin(x)$ is a contracting map. We know there is a unique fixed-point but since $0$ is such a point I deduce that for any $x\in(0,2\pi)$ s.t $x\neq\pi$ we have it that $$\lim_{n\to\infty}\sin_{n}(x)=0$$ So I have a limit of the form ""$0\cdot\infty$"" and I can't figure out any way on how to tackle it. Can someone please suggest a way to find that limit ? Note: I am unsure about the tags, please change them if you see fit.","I was asked today by a friend to calculate a limit and I am having trouble with the question. Denote $\sin_{1}:=\sin$ and for $n>1$ define $\sin_{n}=\sin(\sin_{n-1})$. Calculate $\lim_{n\to\infty}\sqrt{n}\sin_{n}(x)$ for $x\in\mathbb{R}$ (the answer should be a function of $x$ ). My thoughts: It is sufficient to find the limit for $x\in[0,2\pi]$ , and it is easy to find the limit at $0,2\pi$ so we need to find the limit for $x\in(0,2\pi)$. If $[a,b]\subset(0,\pi)$ or $[a,b]\subset(\pi,2\pi)$ we have it that then $$\max_{x\in[a,b]}|\sin'(x)|=\max_{x\in[a,b]}|\cos(x)|<\lambda\leq1$$ hence the map $\sin(x)$ is a contracting map. We know there is a unique fixed-point but since $0$ is such a point I deduce that for any $x\in(0,2\pi)$ s.t $x\neq\pi$ we have it that $$\lim_{n\to\infty}\sin_{n}(x)=0$$ So I have a limit of the form ""$0\cdot\infty$"" and I can't figure out any way on how to tackle it. Can someone please suggest a way to find that limit ? Note: I am unsure about the tags, please change them if you see fit.",,"['real-analysis', 'limits', 'numerical-methods']"
6,Given $y_n=(1+\frac{1}{n})^{n+1}$ show that $\lbrace y_n \rbrace$ is a decreasing sequence,Given  show that  is a decreasing sequence,y_n=(1+\frac{1}{n})^{n+1} \lbrace y_n \rbrace,"Given  $$ y_n=\left(1+\frac{1}{n}\right)^{n+1}\hspace{-6mm},\qquad n \in \mathbb{N}, \quad n \geq 1. $$ Show that $\lbrace y_n \rbrace$ is a decreasing sequence. Anyone can help ?  I consider the ratio $\frac{y_{n+1}}{y_n}$ but I got stuck.","Given  $$ y_n=\left(1+\frac{1}{n}\right)^{n+1}\hspace{-6mm},\qquad n \in \mathbb{N}, \quad n \geq 1. $$ Show that $\lbrace y_n \rbrace$ is a decreasing sequence. Anyone can help ?  I consider the ratio $\frac{y_{n+1}}{y_n}$ but I got stuck.",,"['real-analysis', 'sequences-and-series']"
7,Can we express any positive real number with arbitrary precision using a ratio of two prime numbers? [duplicate],Can we express any positive real number with arbitrary precision using a ratio of two prime numbers? [duplicate],,"This question already has an answer here : Are fractions with prime numerator and denominator dense? (1 answer) Closed 6 years ago . Although the question might look trivial at first because there are infinitely many prime numbers and for example the ratio of two near prime numbers tends to $1$ at infinity, there is still a point that is missing. If we define the set $S=\{\, \frac{p_{i}}{p_{j}}\mid i,j\in\Bbb N\,\}$ where $p_i$ is prime number $i$, is it dense in the set of non-negative reals? If it is, as much as it looks (prevalently) obvious, I am not sure about which precise property is ensuring this, as neither the infinitude of primes nor the limit of ratio of two successive primes reaching $1$ (which is a theorem on its own) looks sufficient individually. I could imagine something like: the rational set has this property and we can replace each rational number with a ratio of two primes to any desired precision. But, can we? Maybe I am missing something, but it is not obvious whichever way I look at it. Theorem that is expected is like: If $\frac{r_{1}}{s_{1}}>\frac{r_{2}}{s_{2}} > 0$ then there are always two prime numbers $p_{m}$ and $p_{n}$ so that $\frac{r_{1}}{s_{1}}>\frac{p_{m}}{p_{n}}>\frac{r_{2}}{s_{2}}$ if that is to work.","This question already has an answer here : Are fractions with prime numerator and denominator dense? (1 answer) Closed 6 years ago . Although the question might look trivial at first because there are infinitely many prime numbers and for example the ratio of two near prime numbers tends to $1$ at infinity, there is still a point that is missing. If we define the set $S=\{\, \frac{p_{i}}{p_{j}}\mid i,j\in\Bbb N\,\}$ where $p_i$ is prime number $i$, is it dense in the set of non-negative reals? If it is, as much as it looks (prevalently) obvious, I am not sure about which precise property is ensuring this, as neither the infinitude of primes nor the limit of ratio of two successive primes reaching $1$ (which is a theorem on its own) looks sufficient individually. I could imagine something like: the rational set has this property and we can replace each rational number with a ratio of two primes to any desired precision. But, can we? Maybe I am missing something, but it is not obvious whichever way I look at it. Theorem that is expected is like: If $\frac{r_{1}}{s_{1}}>\frac{r_{2}}{s_{2}} > 0$ then there are always two prime numbers $p_{m}$ and $p_{n}$ so that $\frac{r_{1}}{s_{1}}>\frac{p_{m}}{p_{n}}>\frac{r_{2}}{s_{2}}$ if that is to work.",,['real-analysis']
8,Inverse/Implicit Function Theorem Reasons?,Inverse/Implicit Function Theorem Reasons?,,"I watched an ICTP lecture on elementary real analysis & the lecturer went to great pains to emphasize the importance of the intermediate value theorem because it is  what generalizes to higher dimensions via connectedness & how Bolzano-Weierstrass  generalizes to metric spaces & how just a few results following from these are what really matters. Regardless of how true that is (I think he's biased because he is a  functional analyst (analysist?)) I found that bit of intuition & motivation extremely clarifying & I'd read a lot of links, wiki's etc... just looking for that kind of  motivation but it was nowhere to be found beforehand. Similarly, I've read a lot about the implicit/inverse function theorems but I just don't understand the reasons for putting so much focus on them, but that's because  I don't really know what they are saying & that's partly because I don't understand what you need to know to really appreciate these theorems. I guess what I'm asking for is an insightful & human explanation of what these theories are, what you need to know to lead up to them, why you need to know that certain material & why these things are so powerful (for instance, I believe you can use these to prove the Lagrange Multipliers theorem, though I don't know why it has anything to do with it, I also know that knowing one means you can prove the other & that it doesn't matter which direction you approach from, but again I don't appreciate why that is). I'm more interested in the surrounding theory, i.e. what you need to know, why you  need to know that & why it is so important, than what the theorem says so I'd much rather prefer to have the motivation so that I could prove this myself. (Yes I've read the wiki page, the threads on this site, the many articles apparently trying to motivate it, I just feel I haven't read anything that directly quells my aforementioned concerns so I think that justifies the thread). Thanks for your time.","I watched an ICTP lecture on elementary real analysis & the lecturer went to great pains to emphasize the importance of the intermediate value theorem because it is  what generalizes to higher dimensions via connectedness & how Bolzano-Weierstrass  generalizes to metric spaces & how just a few results following from these are what really matters. Regardless of how true that is (I think he's biased because he is a  functional analyst (analysist?)) I found that bit of intuition & motivation extremely clarifying & I'd read a lot of links, wiki's etc... just looking for that kind of  motivation but it was nowhere to be found beforehand. Similarly, I've read a lot about the implicit/inverse function theorems but I just don't understand the reasons for putting so much focus on them, but that's because  I don't really know what they are saying & that's partly because I don't understand what you need to know to really appreciate these theorems. I guess what I'm asking for is an insightful & human explanation of what these theories are, what you need to know to lead up to them, why you need to know that certain material & why these things are so powerful (for instance, I believe you can use these to prove the Lagrange Multipliers theorem, though I don't know why it has anything to do with it, I also know that knowing one means you can prove the other & that it doesn't matter which direction you approach from, but again I don't appreciate why that is). I'm more interested in the surrounding theory, i.e. what you need to know, why you  need to know that & why it is so important, than what the theorem says so I'd much rather prefer to have the motivation so that I could prove this myself. (Yes I've read the wiki page, the threads on this site, the many articles apparently trying to motivate it, I just feel I haven't read anything that directly quells my aforementioned concerns so I think that justifies the thread). Thanks for your time.",,[]
9,How local is the information of a derivative?,How local is the information of a derivative?,,"I have read it a thousand times: ""you only need local information to compute derivatives."" To be more precise: when you take a derivative, in say point $a$, what you are essentially doing is taking a limit, so you only need to look at the open region $ (a-\delta,a+\delta) $. Taylor's theorem seems to contradict this: from the derivatives in just one point, you can reconstruct the whole function within its radius of convergence (which can be infinity). For example, consider the function: $f: \mathbb{R} \rightarrow \mathbb{R}:x\mapsto \left\{      \begin{array}{lr}        x+3\pi/2:& x \leq-3\pi/2 \\        \cos(x): & -3\pi/2\leq x \leq3\pi/2\\        x+3\pi/2& : x\leq-3\pi/2      \end{array}    \right.\\$ Wolfram Alpha tells me that $D^{100}f(0)=\cos(0)$... This should give us more than enough information to get a Taylor expansion that converges beyond the point where $f$ is the $\cos$ function ($R=\infty$ for $\cos$ so eventually we have to get there) ... Let me put it this way: Look at the limiting case. All you need to have for a Taylor expansion that converges over all the reals is all the derivatives in 0. This would give you the exact same Taylor expansion as you'd get for the cosine function, while the function from which we took the derivatives is clearly not the cosine function over all the reals. So my question is: Is Wolfram Alpha wrong? If it is right, why does this seem to violate Taylors theorem? If it's wrong, is that because the local region of the domain you need to compute the nth derivative grows with n? Edit 1 : en.m.wikipedia.org/wiki/Taylor%27s_theorem. The most basic version of Taylors theorem for one variable does not mention analyticity, and it's easy to prove that the ""remainder"" goes to zero as you take more and more derivatives, so that f(x) is determined at any x by the derivatives of f in 0.","I have read it a thousand times: ""you only need local information to compute derivatives."" To be more precise: when you take a derivative, in say point $a$, what you are essentially doing is taking a limit, so you only need to look at the open region $ (a-\delta,a+\delta) $. Taylor's theorem seems to contradict this: from the derivatives in just one point, you can reconstruct the whole function within its radius of convergence (which can be infinity). For example, consider the function: $f: \mathbb{R} \rightarrow \mathbb{R}:x\mapsto \left\{      \begin{array}{lr}        x+3\pi/2:& x \leq-3\pi/2 \\        \cos(x): & -3\pi/2\leq x \leq3\pi/2\\        x+3\pi/2& : x\leq-3\pi/2      \end{array}    \right.\\$ Wolfram Alpha tells me that $D^{100}f(0)=\cos(0)$... This should give us more than enough information to get a Taylor expansion that converges beyond the point where $f$ is the $\cos$ function ($R=\infty$ for $\cos$ so eventually we have to get there) ... Let me put it this way: Look at the limiting case. All you need to have for a Taylor expansion that converges over all the reals is all the derivatives in 0. This would give you the exact same Taylor expansion as you'd get for the cosine function, while the function from which we took the derivatives is clearly not the cosine function over all the reals. So my question is: Is Wolfram Alpha wrong? If it is right, why does this seem to violate Taylors theorem? If it's wrong, is that because the local region of the domain you need to compute the nth derivative grows with n? Edit 1 : en.m.wikipedia.org/wiki/Taylor%27s_theorem. The most basic version of Taylors theorem for one variable does not mention analyticity, and it's easy to prove that the ""remainder"" goes to zero as you take more and more derivatives, so that f(x) is determined at any x by the derivatives of f in 0.",,"['real-analysis', 'taylor-expansion']"
10,A closed form for a lot of integrals on the logarithm,A closed form for a lot of integrals on the logarithm,,"One problem that has been bugging me all this summer is as follows: a) Calculate $$I_3=\int_{0}^{1}\int_{0}^{1}\int_{0}^{1} \ln{(1-x)} \ln{(1-xy)} \ln{(1-xyz)} \,\mathrm{d}x\, \mathrm{d}y\, \mathrm{d}z.$$ b) More generally, let $n \ge 1$ be an integer. Calculate, if possible, in terms of well known constants (find a closed form) this multiple logarithmic integral: $$I_n=\int_{[0,1]^n} \ln{(1-x_1)}\ln{(1-x_1x_2)}\cdots\ln{(1-x_1x_2 \cdots x_n)}\,\mathrm{d}^nx.$$ My attempt so far is that I have got $I_1=-1$ and $I_2=3-2\zeta(3)$.","One problem that has been bugging me all this summer is as follows: a) Calculate $$I_3=\int_{0}^{1}\int_{0}^{1}\int_{0}^{1} \ln{(1-x)} \ln{(1-xy)} \ln{(1-xyz)} \,\mathrm{d}x\, \mathrm{d}y\, \mathrm{d}z.$$ b) More generally, let $n \ge 1$ be an integer. Calculate, if possible, in terms of well known constants (find a closed form) this multiple logarithmic integral: $$I_n=\int_{[0,1]^n} \ln{(1-x_1)}\ln{(1-x_1x_2)}\cdots\ln{(1-x_1x_2 \cdots x_n)}\,\mathrm{d}^nx.$$ My attempt so far is that I have got $I_1=-1$ and $I_2=3-2\zeta(3)$.",,"['real-analysis', 'sequences-and-series', 'definite-integrals', 'closed-form', 'polylogarithm']"
11,"Is the image of a Borel subset of $[0,1]$ under a differentiable map still a Borel set?",Is the image of a Borel subset of  under a differentiable map still a Borel set?,"[0,1]","Let $f:[0,1]\to[0,1]$ be a continuous function such that its derivative $f'$ exists on $(0,1)$. Inspired by a similar question of myself here , I want to ask: If $E\subset[0,1]$ is a Borel set, is $f(E)$ still a Borel set? Remark: It is known that $f$ maps sets of Lebesgue measure zero to sets of Lebesgue measure zero(for example, one may refer to this post ), so it follows easily that $f$ maps Lebesgue measurable sets to Lebesgue measurable sets. Even if $f$ is assumed to be $C^1$, the answer is unclear to me. As pointed by George Lowther in a comment in the same post , it is well known that for the natural projection $p:\mathbb R^2\to \mathbb R$, $p(x,y)=x$, which is clearly real analytic, the image of  a Borel set of $\mathbb R^2$ under $p$ may not be a Borel set in $\mathbb R$. More details can be found here . It follows that for a high dimensional analog of the question, i.e. for $f:[0,1]^n\to\mathbb R^m$, $n\ge 2$, even when $f$ is real analytic, the answer is negative . Any hint or suggestion is appreciated. Thanks in advance.","Let $f:[0,1]\to[0,1]$ be a continuous function such that its derivative $f'$ exists on $(0,1)$. Inspired by a similar question of myself here , I want to ask: If $E\subset[0,1]$ is a Borel set, is $f(E)$ still a Borel set? Remark: It is known that $f$ maps sets of Lebesgue measure zero to sets of Lebesgue measure zero(for example, one may refer to this post ), so it follows easily that $f$ maps Lebesgue measurable sets to Lebesgue measurable sets. Even if $f$ is assumed to be $C^1$, the answer is unclear to me. As pointed by George Lowther in a comment in the same post , it is well known that for the natural projection $p:\mathbb R^2\to \mathbb R$, $p(x,y)=x$, which is clearly real analytic, the image of  a Borel set of $\mathbb R^2$ under $p$ may not be a Borel set in $\mathbb R$. More details can be found here . It follows that for a high dimensional analog of the question, i.e. for $f:[0,1]^n\to\mathbb R^m$, $n\ge 2$, even when $f$ is real analytic, the answer is negative . Any hint or suggestion is appreciated. Thanks in advance.",,['real-analysis']
12,Applications of model theory to analysis,Applications of model theory to analysis,,"Some of the more organic theories considered in model theory (other than set theory, which, from what I've seen, seems to be quite distinct from ""mainstream"" model theory) are those which arise from algebraic structures (theories of abstract groups, rings, fields) and real and complex analysis (theories of expansions of real and complex fields, and sometimes both). While relationships with algebra seem quite apparent, I wonder what are some interesting results in real and complex analysis that have nice model-theoretical proofs (or better yet, only model-theoretical proofs are known!)? Of course, there's nonstandard analysis, but I hope to see some different examples. That said, I wouldn't mind seeing a particularly interesting application of nonstandard analysis. :) I hope the question is at least a little interesting. I have only the very basic knowledge of model theory of that type (and the same applies to nonstandard analysis), so it may seem a little naive, but I got curious, hence the question.","Some of the more organic theories considered in model theory (other than set theory, which, from what I've seen, seems to be quite distinct from ""mainstream"" model theory) are those which arise from algebraic structures (theories of abstract groups, rings, fields) and real and complex analysis (theories of expansions of real and complex fields, and sometimes both). While relationships with algebra seem quite apparent, I wonder what are some interesting results in real and complex analysis that have nice model-theoretical proofs (or better yet, only model-theoretical proofs are known!)? Of course, there's nonstandard analysis, but I hope to see some different examples. That said, I wouldn't mind seeing a particularly interesting application of nonstandard analysis. :) I hope the question is at least a little interesting. I have only the very basic knowledge of model theory of that type (and the same applies to nonstandard analysis), so it may seem a little naive, but I got curious, hence the question.",,"['real-analysis', 'complex-analysis', 'soft-question', 'model-theory', 'applications']"
13,Product $\left(\frac31\right)^{1/2}\cdot\left(\frac75\right)^{1/6}\cdot \left(\frac{11}9\right)^{1/10}\cdot \left(\frac{15}{13}\right)^{1/14}\cdots$,Product,\left(\frac31\right)^{1/2}\cdot\left(\frac75\right)^{1/6}\cdot \left(\frac{11}9\right)^{1/10}\cdot \left(\frac{15}{13}\right)^{1/14}\cdots,"Recently I rediscovered by a new method a family of infinite products I obtained years ago, and one of the examples you may find below, $$\left(\frac{3}{1}\right)^{1/2}\cdot\left(\frac{7}{5}\right)^{1/6}\cdot \left(\frac{11}{9}\right)^{1/10}\cdot \left(\frac{15}{13}\right)^{1/14}\cdots=\exp\bigg(\frac{\pi}{4}\int_0^{\pi/4}\frac{\tan(x)}{x}\textrm{d}{x}\biggr).$$ I wonder if such results are known in the mathematical literature, or anything similar to the example stated above. If yes, I'd be glad to receive information about the related literature. Supplementary question: Find the infinite product representations of  $$\exp\bigg(\frac{\pi}{4}\int_0^{\pi/4}\frac{\tan^2(x)}{x}\textrm{d}{x}\biggr), \ \exp\bigg(\frac{\pi}{4}\int_0^{\pi/4}\frac{\tan^3(x)}{x}\textrm{d}{x}\biggr), \ \exp\bigg(\frac{\pi}{4}\int_0^{\pi/4}\frac{\tan^4(x)}{x}\textrm{d}{x}\biggr)$$","Recently I rediscovered by a new method a family of infinite products I obtained years ago, and one of the examples you may find below, $$\left(\frac{3}{1}\right)^{1/2}\cdot\left(\frac{7}{5}\right)^{1/6}\cdot \left(\frac{11}{9}\right)^{1/10}\cdot \left(\frac{15}{13}\right)^{1/14}\cdots=\exp\bigg(\frac{\pi}{4}\int_0^{\pi/4}\frac{\tan(x)}{x}\textrm{d}{x}\biggr).$$ I wonder if such results are known in the mathematical literature, or anything similar to the example stated above. If yes, I'd be glad to receive information about the related literature. Supplementary question: Find the infinite product representations of  $$\exp\bigg(\frac{\pi}{4}\int_0^{\pi/4}\frac{\tan^2(x)}{x}\textrm{d}{x}\biggr), \ \exp\bigg(\frac{\pi}{4}\int_0^{\pi/4}\frac{\tan^3(x)}{x}\textrm{d}{x}\biggr), \ \exp\bigg(\frac{\pi}{4}\int_0^{\pi/4}\frac{\tan^4(x)}{x}\textrm{d}{x}\biggr)$$",,"['calculus', 'real-analysis', 'integration', 'sequences-and-series', 'special-functions']"
14,Evaluate $\sum\limits_{k=1}^{\infty} \frac{k^2-1}{k^4+k^2+1}$,Evaluate,\sum\limits_{k=1}^{\infty} \frac{k^2-1}{k^4+k^2+1},"How to find $$\sum_{k=1}^{\infty} \frac{k^2-1}{k^4+k^2+1}$$ I try something like this: $$\begin{align*}\sum_{k=1}^{\infty} \frac{k^2-1}{k^4+k^2+1}=\sum_{k=1}^{\infty}\frac{k^2}{k^4+k^2+1}-\sum_{k=1}^{\infty}\frac{1}{k^4+k^2+1}.\end{align*}$$ Using fact that $$\sum_{k = 1}^{n}{\frac{1}{k^4+k^2+1}}=\frac{1}{2}\cdot\frac{n+1}{n^2+n+1}+\frac{1}{2}\cdot\sum_{k = 1}^{n-1}{\frac{1}{k^2+k+1}}$$ we find that $$\begin{align*}\sum_{k=1}^{\infty}\frac{1}{k^4+k^2+1} &=\frac{1}{2}\cdot\sum_{k=1}^{\infty}{\frac{1}{k^2+k+1}}\\ &=\frac{1}{6}\left(\sqrt{3}\pi \tanh{\left(\frac{\sqrt{3}\pi}{2}\right)}-1\right).\end{align*}$$ But I don't know how to find $\displaystyle\sum_{k=1}^{\infty}\frac{k^2}{k^4+k^2+1}.$ If someone want to know how to evaluate $\displaystyle\sum_{k=0}^{\infty}\frac{1}{k^2+k+1}$: First, $$\displaystyle\sum_{k=0}^{\infty} \frac{1}{k^2+k+1}=\sum_{k=0}^{\infty}{\frac{1}{\left(k+\frac{1}{2}\right)^2+\left(\frac{\sqrt{3}}{2}\right)^2}}.$$ Now, using ""well-know"" formula $$\displaystyle\cos(\phi)=\prod_{k=0}^{\infty}{\left( 1-\frac{4\phi^2}{(2k+1)^2\pi^2}\right)}$$ we find that $$\displaystyle\log (\cos(\phi))=\sum_{k=0}^{\infty}{\log\left( 1-\frac{4\phi^2}{(2k+1)^2\pi^2}\right)}$$ and then we attack with $\dfrac{d}{d\phi}$ and find $$\displaystyle\tan(\phi)=\sum_{k=0}^{\infty}{\frac{8\phi}{(2k+1)^2\pi^2-4\phi^2}}.$$ Let $\phi=\pi\alpha\cdot i$, then we get $$\displaystyle\tan(\pi\alpha\cdot i)=i\cdot\tanh(\pi\alpha)=i\cdot\sum_{k=0}^{\infty}{\frac{8\pi\alpha}{(2k+1)^2\pi^2+4\pi^2\alpha^2}}=\frac{2\alpha i}{\pi}\cdot\sum_{k=0}^{\infty}{\frac{1}{\left(k+\frac{1}{2}\right)^2+\alpha^2}}.$$ So, we find that $$\displaystyle\sum_{k=0}^{\infty}{\frac{1}{\left(k+\frac{1}{2}\right)^2+\alpha^2}}=\frac{\pi}{2\alpha}\cdot\tanh(\pi\alpha).$$ Let $ \alpha=\dfrac{\sqrt{3}}{2}.$ We get $$\displaystyle\sum_{k=0}^{\infty}{\frac{1}{\left(k+\frac{1}{2}\right)^2+\left(\frac{\sqrt{3}}{2}\right)^2}}=\frac{\sqrt{3}\pi}{3}\cdot\tanh\left(\frac{\sqrt{3}\pi}{2}\right)$$ or $$\displaystyle\sum_{k=0}^{\infty} \frac{1}{k^2+k+1}=\frac{\sqrt{3}\pi}{3}\cdot\tanh\left(\frac{\sqrt{3}\pi}{2}\right).$$","How to find $$\sum_{k=1}^{\infty} \frac{k^2-1}{k^4+k^2+1}$$ I try something like this: $$\begin{align*}\sum_{k=1}^{\infty} \frac{k^2-1}{k^4+k^2+1}=\sum_{k=1}^{\infty}\frac{k^2}{k^4+k^2+1}-\sum_{k=1}^{\infty}\frac{1}{k^4+k^2+1}.\end{align*}$$ Using fact that $$\sum_{k = 1}^{n}{\frac{1}{k^4+k^2+1}}=\frac{1}{2}\cdot\frac{n+1}{n^2+n+1}+\frac{1}{2}\cdot\sum_{k = 1}^{n-1}{\frac{1}{k^2+k+1}}$$ we find that $$\begin{align*}\sum_{k=1}^{\infty}\frac{1}{k^4+k^2+1} &=\frac{1}{2}\cdot\sum_{k=1}^{\infty}{\frac{1}{k^2+k+1}}\\ &=\frac{1}{6}\left(\sqrt{3}\pi \tanh{\left(\frac{\sqrt{3}\pi}{2}\right)}-1\right).\end{align*}$$ But I don't know how to find $\displaystyle\sum_{k=1}^{\infty}\frac{k^2}{k^4+k^2+1}.$ If someone want to know how to evaluate $\displaystyle\sum_{k=0}^{\infty}\frac{1}{k^2+k+1}$: First, $$\displaystyle\sum_{k=0}^{\infty} \frac{1}{k^2+k+1}=\sum_{k=0}^{\infty}{\frac{1}{\left(k+\frac{1}{2}\right)^2+\left(\frac{\sqrt{3}}{2}\right)^2}}.$$ Now, using ""well-know"" formula $$\displaystyle\cos(\phi)=\prod_{k=0}^{\infty}{\left( 1-\frac{4\phi^2}{(2k+1)^2\pi^2}\right)}$$ we find that $$\displaystyle\log (\cos(\phi))=\sum_{k=0}^{\infty}{\log\left( 1-\frac{4\phi^2}{(2k+1)^2\pi^2}\right)}$$ and then we attack with $\dfrac{d}{d\phi}$ and find $$\displaystyle\tan(\phi)=\sum_{k=0}^{\infty}{\frac{8\phi}{(2k+1)^2\pi^2-4\phi^2}}.$$ Let $\phi=\pi\alpha\cdot i$, then we get $$\displaystyle\tan(\pi\alpha\cdot i)=i\cdot\tanh(\pi\alpha)=i\cdot\sum_{k=0}^{\infty}{\frac{8\pi\alpha}{(2k+1)^2\pi^2+4\pi^2\alpha^2}}=\frac{2\alpha i}{\pi}\cdot\sum_{k=0}^{\infty}{\frac{1}{\left(k+\frac{1}{2}\right)^2+\alpha^2}}.$$ So, we find that $$\displaystyle\sum_{k=0}^{\infty}{\frac{1}{\left(k+\frac{1}{2}\right)^2+\alpha^2}}=\frac{\pi}{2\alpha}\cdot\tanh(\pi\alpha).$$ Let $ \alpha=\dfrac{\sqrt{3}}{2}.$ We get $$\displaystyle\sum_{k=0}^{\infty}{\frac{1}{\left(k+\frac{1}{2}\right)^2+\left(\frac{\sqrt{3}}{2}\right)^2}}=\frac{\sqrt{3}\pi}{3}\cdot\tanh\left(\frac{\sqrt{3}\pi}{2}\right)$$ or $$\displaystyle\sum_{k=0}^{\infty} \frac{1}{k^2+k+1}=\frac{\sqrt{3}\pi}{3}\cdot\tanh\left(\frac{\sqrt{3}\pi}{2}\right).$$",,"['calculus', 'real-analysis', 'sequences-and-series']"
15,Is there a function whose autoconvolution is its square? $g^2(x) = g*g (x)$,Is there a function whose autoconvolution is its square?,g^2(x) = g*g (x),"I am looking for a function over the real line, $g$ , with $g*g = g^2$ (or a proof that such a function doesn't exist on some space like $L_1 \cap L_2$ or $L_1 \cap L_\infty$ ). This relation can't hold in a non-trivial way over any finite space, by that I mean that if $f$ is a probability density function, then $fg^2 = fg * fg$ implies by integration that $E_f[g^2] = E_f[g]^2$ , so $g$ would have to have $0$ variance. Furthermore, this relation can't hold on the positive real line since $e^{-x}$ is a probability distribution with $e^{-x}(g*g) = (e^{-x}g)* (e^{-x}g)$ , which would again mean that $g$ has $0$ variance, forcing it to be zero. Alternatively, one could hope for a power series for $g$ and use the fact that $h_n \equiv x^{n-1}/(n-1)!$ has $h_n * h_m = h_{n+m}$ over the positive real line to derive that the power series of $g^2$ would have to be zero. None of the tricks in this paragraph seem to work over the entire real line, however. Instead, Fourier transforming $g^2 = g*g$ reveals the same equation in Fourier space: $\hat{g}^2 = \hat{g^2} = \hat{g} * \hat{g}$ . If we assume $g$ and $g^2$ have finite moments, this lets us relate them by differentiating $\hat{g}^2$ repeatedly to reveal the binomial-type formula $E_{g^2}[x^n] = \Sigma_{k = 0}^n {n \choose k}E_g[x^k]E_g[x^{n-k}]$ , where $E_g[x]$ means $\int_{-\infty}^\infty dx g x$ . This is quite the strange condition to me, but it doesn't seem to lead to any obvious contradictions. It says that the cumulants of $g^2$ are twice that of $g$ . If we expand $g = \sum_{n=0} a_n \psi_n$ in terms of Hermite-Gauss functions $\psi_n(x) \equiv H_n(x)e^{-x^2/2}$ and use the convolution theorem, the fact that $F[\psi_n] = (-i)^n \psi_n$ , and orthonormality of the $\psi_n$ , we can derive the fact that the 4 mod 4 families of modes $\{\psi_n:n = k \hspace{.2cm}\text{mod} 4\}$ in $g^2$ result from the products over modes in $g$ whose index sums to $k \hspace{.1cm} \text{mod} 4$ : $\langle \psi_k , g^2\rangle = \sum_{n+m = k\text{mod 4}} a_n a_m <\psi_n \psi_m , \psi_k> $ for any $k$ . This is opposed to the usual situation where $n+m = k+2$ modes are also included in the sum. This says that the index of $g$ 's modes don't mix mod 4 after squaring. This condition is difficult to work with because there is not a nice way of dealing with $<\psi_n \psi_m, \psi_k>$ . If there was an extra $e^{x^2/2}$ inside that inner product, it would let us use the tripple product formula for $H_n(x)$ which has a nice form where only finitely many terms pop out (unlike the seemingly all-to-all coupling of the $\psi$ product). A solution of $g^2 = g*g$ would lead to solutions of $g^2 = \lambda g*g$ for any $\lambda$ by using $g(x/\lambda)*g(x/\lambda) (\lambda x) = \lambda g*g(x)$ . Furthermore, using $(e^{\lambda x}g)^2 = e^{2 \lambda x} g*g = e^{\lambda x} (e^{\lambda x}g)*(e^{\lambda x})$ would give us a solution to $g^2 = e^{\lambda x} g*g$ for any $\lambda$ . Substituting $g(x) \rightarrow g(x+\delta)$ gives us solutions to translated equations $g^2(x) = g*g (x+\delta)$ . Unfortunately, none of these transformations have made the solution to the problem obvious. The problem becomes trivial if we are allowed to rescale like $g(x)^2 = g*g(\sqrt{2} x) \sqrt{2}$ , as its just a Gaussian! Herein lies the reason why I suspect that there is no solution to $g^2 = g*g$ . The left-hand side tightens $g$ , whereas the right-hand side spreads $g$ out. Interestingly enough, there are solutions to $g = g*g$ (sinc) as well as $g^{1/2} = g*g$ (again Gaussian), but the homogeneous version of the problem seems more elusive. I have read some papers about solving convolutional equations, but they as far as I have seen have either been over a finite space or have not had the non-linearity present here. I have also tried taking a fractional Fourier transform with angle $-\pi/4$ to convert $g^2 = g*g$ into $g_{-\pi/4}*_{-\pi/4}g_{-\pi/4} = g_{-\pi/4}*_{\pi/4} g_{-\pi/4}$ , which leads to another two dimensional integral which is easy to write down but not solve. I thought of doing a fractional Fourier transform about a very small angle in order to make the convolution $g*g$ less delocalized as well as the product $g*g$ smoother. This didn't really lead to anything helpful. I tried thinking up of a scheme to find better and better approximations for $g$ , as I would be happy even with an implicit answer or a numerical method to plot $g$ . The obvious candidate $g = \sqrt{g*g}$ has the problem of there being cycles (Gaussians) as well as convolutions being difficult to compute over the whole real line. If one thinks of $g^2-g*g$ as a Lagrangian and tries to do a path integral of $e^{-|g^2-g*g|}$ over configurations of $g$ with some given $||g||$ , then I think this corresponds to a badly non-local field theory. Yikes! As a final note, the problem can be restated as finding a function where the act of squaring commutes with taking a Fourier transform: $\hat{g}^2 = \hat{g^2}$ . This would be solved (sufficient but not necessary) by a function who is its own Fourier transform $g = \hat{g}$ , and whose square is its own Fourier transform $g^2 = \hat{g^2}$ . But again we run into the problem where there is no good basis in which to to do both the product and the Fourier transform. Polynomials multiply well but Fourier transform/convolve poorly, vice versa for Hermite Gaussians (etc etc with seemingly every choice of basis). Any help approaching this problem would be greatly appreciated, as I am seriously losing my mind over it! Edit: There is also a simple asymptotic argument which shows that if g is upper and lower bounded by ~ $x^{-n}$ at infinity for some n then it can't solve $g^2 = g*g$ . The reason is that the limit of $x^n g^2$ is $0$ , but the limit of $x^n g*g$ would be $\int_{-\infty}^\infty dx g(x)$ , so this would have to be zero (forcing the integral of $g^2$ to be zero as well). However such an argument fails if $g$ decays faster than any polynomial, or if it repeatedly crosses $0$ as $x$ goes off to infinity. Another edit: I believe that the answer is yes and unique (but trivial) for g taken to be a discrete vector with elements over $Z$ . This is because Fourier transforming the problem moves back to a compact space where the variance argument shows that the only solution is constant, meaning that the only solution for such a $g$ is $g_i = \delta_i$ .","I am looking for a function over the real line, , with (or a proof that such a function doesn't exist on some space like or ). This relation can't hold in a non-trivial way over any finite space, by that I mean that if is a probability density function, then implies by integration that , so would have to have variance. Furthermore, this relation can't hold on the positive real line since is a probability distribution with , which would again mean that has variance, forcing it to be zero. Alternatively, one could hope for a power series for and use the fact that has over the positive real line to derive that the power series of would have to be zero. None of the tricks in this paragraph seem to work over the entire real line, however. Instead, Fourier transforming reveals the same equation in Fourier space: . If we assume and have finite moments, this lets us relate them by differentiating repeatedly to reveal the binomial-type formula , where means . This is quite the strange condition to me, but it doesn't seem to lead to any obvious contradictions. It says that the cumulants of are twice that of . If we expand in terms of Hermite-Gauss functions and use the convolution theorem, the fact that , and orthonormality of the , we can derive the fact that the 4 mod 4 families of modes in result from the products over modes in whose index sums to : for any . This is opposed to the usual situation where modes are also included in the sum. This says that the index of 's modes don't mix mod 4 after squaring. This condition is difficult to work with because there is not a nice way of dealing with . If there was an extra inside that inner product, it would let us use the tripple product formula for which has a nice form where only finitely many terms pop out (unlike the seemingly all-to-all coupling of the product). A solution of would lead to solutions of for any by using . Furthermore, using would give us a solution to for any . Substituting gives us solutions to translated equations . Unfortunately, none of these transformations have made the solution to the problem obvious. The problem becomes trivial if we are allowed to rescale like , as its just a Gaussian! Herein lies the reason why I suspect that there is no solution to . The left-hand side tightens , whereas the right-hand side spreads out. Interestingly enough, there are solutions to (sinc) as well as (again Gaussian), but the homogeneous version of the problem seems more elusive. I have read some papers about solving convolutional equations, but they as far as I have seen have either been over a finite space or have not had the non-linearity present here. I have also tried taking a fractional Fourier transform with angle to convert into , which leads to another two dimensional integral which is easy to write down but not solve. I thought of doing a fractional Fourier transform about a very small angle in order to make the convolution less delocalized as well as the product smoother. This didn't really lead to anything helpful. I tried thinking up of a scheme to find better and better approximations for , as I would be happy even with an implicit answer or a numerical method to plot . The obvious candidate has the problem of there being cycles (Gaussians) as well as convolutions being difficult to compute over the whole real line. If one thinks of as a Lagrangian and tries to do a path integral of over configurations of with some given , then I think this corresponds to a badly non-local field theory. Yikes! As a final note, the problem can be restated as finding a function where the act of squaring commutes with taking a Fourier transform: . This would be solved (sufficient but not necessary) by a function who is its own Fourier transform , and whose square is its own Fourier transform . But again we run into the problem where there is no good basis in which to to do both the product and the Fourier transform. Polynomials multiply well but Fourier transform/convolve poorly, vice versa for Hermite Gaussians (etc etc with seemingly every choice of basis). Any help approaching this problem would be greatly appreciated, as I am seriously losing my mind over it! Edit: There is also a simple asymptotic argument which shows that if g is upper and lower bounded by ~ at infinity for some n then it can't solve . The reason is that the limit of is , but the limit of would be , so this would have to be zero (forcing the integral of to be zero as well). However such an argument fails if decays faster than any polynomial, or if it repeatedly crosses as goes off to infinity. Another edit: I believe that the answer is yes and unique (but trivial) for g taken to be a discrete vector with elements over . This is because Fourier transforming the problem moves back to a compact space where the variance argument shows that the only solution is constant, meaning that the only solution for such a is .","g g*g = g^2 L_1 \cap L_2 L_1 \cap L_\infty f fg^2 = fg * fg E_f[g^2] = E_f[g]^2 g 0 e^{-x} e^{-x}(g*g) = (e^{-x}g)* (e^{-x}g) g 0 g h_n \equiv x^{n-1}/(n-1)! h_n * h_m = h_{n+m} g^2 g^2 = g*g \hat{g}^2 = \hat{g^2} = \hat{g} * \hat{g} g g^2 \hat{g}^2 E_{g^2}[x^n] = \Sigma_{k = 0}^n {n \choose k}E_g[x^k]E_g[x^{n-k}] E_g[x] \int_{-\infty}^\infty dx g x g^2 g g = \sum_{n=0} a_n \psi_n \psi_n(x) \equiv H_n(x)e^{-x^2/2} F[\psi_n] = (-i)^n \psi_n \psi_n \{\psi_n:n = k \hspace{.2cm}\text{mod} 4\} g^2 g k \hspace{.1cm} \text{mod} 4 \langle \psi_k , g^2\rangle = \sum_{n+m = k\text{mod 4}} a_n a_m <\psi_n \psi_m , \psi_k>  k n+m = k+2 g <\psi_n \psi_m, \psi_k> e^{x^2/2} H_n(x) \psi g^2 = g*g g^2 = \lambda g*g \lambda g(x/\lambda)*g(x/\lambda) (\lambda x) = \lambda g*g(x) (e^{\lambda x}g)^2 = e^{2 \lambda x} g*g = e^{\lambda x} (e^{\lambda x}g)*(e^{\lambda x}) g^2 = e^{\lambda x} g*g \lambda g(x) \rightarrow g(x+\delta) g^2(x) = g*g (x+\delta) g(x)^2 = g*g(\sqrt{2} x) \sqrt{2} g^2 = g*g g g g = g*g g^{1/2} = g*g -\pi/4 g^2 = g*g g_{-\pi/4}*_{-\pi/4}g_{-\pi/4} = g_{-\pi/4}*_{\pi/4} g_{-\pi/4} g*g g*g g g g = \sqrt{g*g} g^2-g*g e^{-|g^2-g*g|} g ||g|| \hat{g}^2 = \hat{g^2} g = \hat{g} g^2 = \hat{g^2} x^{-n} g^2 = g*g x^n g^2 0 x^n g*g \int_{-\infty}^\infty dx g(x) g^2 g 0 x Z g g_i = \delta_i","['real-analysis', 'fourier-analysis', 'fourier-transform', 'convolution', 'nonlinear-analysis']"
16,Can every continuous function be continuously ''transformed'' into a differentiable function?,Can every continuous function be continuously ''transformed'' into a differentiable function?,,"Can every continuous function $f(x)$ from $\mathbb{R}\to \mathbb{R}$ be continuously ""transformed"" into a differentiable function? More precisely is  there always  a continuous (non constant) $g(x)$ such that $g(f(x))$ is differentiable? This seems to hold for simple functions, for instance the function $f(x)=|x|$ can be transformed into a differentiable function by the function $g(x)=x^2$ . If $g(x)$ is additionally required to be increasing everywhere and differentiable then the answer seems to be no by the inverse function theorem, because of the existence of continuous nowhere differentiable functions.","Can every continuous function from be continuously ""transformed"" into a differentiable function? More precisely is  there always  a continuous (non constant) such that is differentiable? This seems to hold for simple functions, for instance the function can be transformed into a differentiable function by the function . If is additionally required to be increasing everywhere and differentiable then the answer seems to be no by the inverse function theorem, because of the existence of continuous nowhere differentiable functions.",f(x) \mathbb{R}\to \mathbb{R} g(x) g(f(x)) f(x)=|x| g(x)=x^2 g(x),"['real-analysis', 'examples-counterexamples']"
17,Exponential integral $ \int_0^\infty \frac{x^t}{\Gamma(t+1)}\text dt$,Exponential integral, \int_0^\infty \frac{x^t}{\Gamma(t+1)}\text dt,"Now since the sum $$ \sum_{n=0}^\infty \frac{x^n}{n!},\quad x\in\Bbb R, $$ does have some relatively nice properties, is the same true for its analogues integral? If we take the gamma function to be a generalisation of the factorial with $\Gamma(n+1) = n!$, an obvious analogues integral formula would be $$ \int_0^\infty \frac{x^t}{\Gamma(t+1)}\text dt,\quad x\ge 0. $$ Does this integral have any similar, nice properties?","Now since the sum $$ \sum_{n=0}^\infty \frac{x^n}{n!},\quad x\in\Bbb R, $$ does have some relatively nice properties, is the same true for its analogues integral? If we take the gamma function to be a generalisation of the factorial with $\Gamma(n+1) = n!$, an obvious analogues integral formula would be $$ \int_0^\infty \frac{x^t}{\Gamma(t+1)}\text dt,\quad x\ge 0. $$ Does this integral have any similar, nice properties?",,"['real-analysis', 'integration', 'improper-integrals', 'special-functions', 'integral-transforms']"
18,Lebesgue density strictly between 0 and 1,Lebesgue density strictly between 0 and 1,,"I am having trouble with the following problem: Let $A\subseteq \mathbb{R}$ be measurable, with $\mu(A)>0$ and $\mu(\mathbb{R}\backslash A)>0$. Then how do I show that there exists $x\in \mathbb{R}$ such that $$\lim_{\varepsilon\to 0} \frac{\mu(B_\varepsilon(x) \cap A)}{\mu(B_\varepsilon(x))}$$ is $\alpha$, where $\alpha\neq 0,1$? I know from Lebesgue density theorem that the limit is $1$ for a.e. $x\in A$ and $0$ for a.e. $x\in A^c$. But I don't know how to show that given $A$, we can always find a point which makes the limit not equal to 0 or 1. Please help.","I am having trouble with the following problem: Let $A\subseteq \mathbb{R}$ be measurable, with $\mu(A)>0$ and $\mu(\mathbb{R}\backslash A)>0$. Then how do I show that there exists $x\in \mathbb{R}$ such that $$\lim_{\varepsilon\to 0} \frac{\mu(B_\varepsilon(x) \cap A)}{\mu(B_\varepsilon(x))}$$ is $\alpha$, where $\alpha\neq 0,1$? I know from Lebesgue density theorem that the limit is $1$ for a.e. $x\in A$ and $0$ for a.e. $x\in A^c$. But I don't know how to show that given $A$, we can always find a point which makes the limit not equal to 0 or 1. Please help.",,"['real-analysis', 'analysis', 'measure-theory', 'lebesgue-measure']"
19,Evaluating $\sqrt{1 + \sqrt{2 + \sqrt{4 + \sqrt{8 + \ldots}}}}$,Evaluating,\sqrt{1 + \sqrt{2 + \sqrt{4 + \sqrt{8 + \ldots}}}},"Inspired by Ramanujan's problem and solution of $\sqrt{1 + 2\sqrt{1 + 3\sqrt{1 + \ldots}}}$, I decided to attempt evaluating the infinite radical $$ \sqrt{1 + \sqrt{2 + \sqrt{4 + \sqrt{8 + \ldots}}}} $$ Taking a cue from Ramanujan's solution method, I defined a function $f(x)$ such that $$ f(x) = \sqrt{2^x + \sqrt{2^{x+1} + \sqrt{2^{x+2} + \sqrt{2^{x+3} + \ldots}}}} $$ We can see that $$\begin{align} f(0) &= \sqrt{2^0 + \sqrt{2^1 + \sqrt{2^2 + \sqrt{2^3 + \ldots}}}} \\ &= \sqrt{1 + \sqrt{2 + \sqrt{4 + \sqrt{8 + \ldots}}}} \end{align}$$ And we begin solving by $$\begin{align} f(x) &= \sqrt{2^x + \sqrt{2^{x+1} + \sqrt{2^{x+2} + \sqrt{2^{x+3} + \ldots}}}} \\ f(x)^2 &= 2^x + \sqrt{2^{x+1} + \sqrt{2^{x+2} + \sqrt{2^{x+3} + \ldots}}} \\ &= 2^x + f(x + 1) \\ f(x + 1) &= f(x)^2 - 2^x \end{align}$$ At this point I find myself stuck, as I have little experience with recurrence relations. How would this recurrence relation be solved? Would the method extend easily to $$\begin{align} f_n(x) &= \sqrt{n^x + \sqrt{n^{x+1} + \sqrt{n^{x+2} + \sqrt{n^{x+3} + \ldots}}}} \\ f_n(x)^2 &= n^x + f_n(x + 1)~\text ? \end{align}$$","Inspired by Ramanujan's problem and solution of $\sqrt{1 + 2\sqrt{1 + 3\sqrt{1 + \ldots}}}$, I decided to attempt evaluating the infinite radical $$ \sqrt{1 + \sqrt{2 + \sqrt{4 + \sqrt{8 + \ldots}}}} $$ Taking a cue from Ramanujan's solution method, I defined a function $f(x)$ such that $$ f(x) = \sqrt{2^x + \sqrt{2^{x+1} + \sqrt{2^{x+2} + \sqrt{2^{x+3} + \ldots}}}} $$ We can see that $$\begin{align} f(0) &= \sqrt{2^0 + \sqrt{2^1 + \sqrt{2^2 + \sqrt{2^3 + \ldots}}}} \\ &= \sqrt{1 + \sqrt{2 + \sqrt{4 + \sqrt{8 + \ldots}}}} \end{align}$$ And we begin solving by $$\begin{align} f(x) &= \sqrt{2^x + \sqrt{2^{x+1} + \sqrt{2^{x+2} + \sqrt{2^{x+3} + \ldots}}}} \\ f(x)^2 &= 2^x + \sqrt{2^{x+1} + \sqrt{2^{x+2} + \sqrt{2^{x+3} + \ldots}}} \\ &= 2^x + f(x + 1) \\ f(x + 1) &= f(x)^2 - 2^x \end{align}$$ At this point I find myself stuck, as I have little experience with recurrence relations. How would this recurrence relation be solved? Would the method extend easily to $$\begin{align} f_n(x) &= \sqrt{n^x + \sqrt{n^{x+1} + \sqrt{n^{x+2} + \sqrt{n^{x+3} + \ldots}}}} \\ f_n(x)^2 &= n^x + f_n(x + 1)~\text ? \end{align}$$",,"['real-analysis', 'algebra-precalculus', 'recurrence-relations', 'nested-radicals']"
20,"Is this function decreasing on $(0,1)$?",Is this function decreasing on ?,"(0,1)","While doing some research I got stuck trying to prove that the following function is decreasing $$f(k):= k K(k) \sinh \left(\frac{\pi}{2} \frac{K(\sqrt{1-k^2})}{K(k)}\right)$$ for $k \in (0,1)$. Here $K$ is the Complete elliptic integral of the first kind , defined by $$K(k):= \int_{0}^{1} \frac{dt}{\sqrt{1-t^2} \sqrt{1-k^2t^2}}.$$ This seems to be true, as the graph below suggests : I really don't know much about elliptic integrals, so perhaps someone here can give some insight. Any relevant reference on elliptic integrals of the first kind is welcome. Thank you, Malik EDIT (2012-07-09) : Using J.M.'s suggestion to rewrite the function $f(k)$ as $$f(k) = kK(k) \frac{1-q(k)}{2 \sqrt{q(k)}}$$ and using the derivative formulas $$K'(k) = \frac{E(k)}{k(1-k^2)} - \frac{K(k)}{k},$$ $$q'(k)=\frac{\pi^2}{2} \frac{q(k)} { K(k)^2 (1-k^2)k}$$ where $E(k)$ is the Complete elliptic integral of the second kind, I was able to calculate $f'(k)$ and reduce the problem to showing that the following function is negative for $k \in (0,1)$ : $$g(k):= 4(1-q(k))K(k)E(k) - \pi^2 (1+q(k)).$$ Below is the graph of $g$ obtained with Maple : EDIT (19-07-2012) I asked the question on MathOverflow !","While doing some research I got stuck trying to prove that the following function is decreasing $$f(k):= k K(k) \sinh \left(\frac{\pi}{2} \frac{K(\sqrt{1-k^2})}{K(k)}\right)$$ for $k \in (0,1)$. Here $K$ is the Complete elliptic integral of the first kind , defined by $$K(k):= \int_{0}^{1} \frac{dt}{\sqrt{1-t^2} \sqrt{1-k^2t^2}}.$$ This seems to be true, as the graph below suggests : I really don't know much about elliptic integrals, so perhaps someone here can give some insight. Any relevant reference on elliptic integrals of the first kind is welcome. Thank you, Malik EDIT (2012-07-09) : Using J.M.'s suggestion to rewrite the function $f(k)$ as $$f(k) = kK(k) \frac{1-q(k)}{2 \sqrt{q(k)}}$$ and using the derivative formulas $$K'(k) = \frac{E(k)}{k(1-k^2)} - \frac{K(k)}{k},$$ $$q'(k)=\frac{\pi^2}{2} \frac{q(k)} { K(k)^2 (1-k^2)k}$$ where $E(k)$ is the Complete elliptic integral of the second kind, I was able to calculate $f'(k)$ and reduce the problem to showing that the following function is negative for $k \in (0,1)$ : $$g(k):= 4(1-q(k))K(k)E(k) - \pi^2 (1+q(k)).$$ Below is the graph of $g$ obtained with Maple : EDIT (19-07-2012) I asked the question on MathOverflow !",,"['real-analysis', 'reference-request', 'special-functions', 'elliptic-integrals']"
21,How to solve $\dot{x} = \frac{f(x)}{\|f(x)\|}$?,How to solve ?,\dot{x} = \frac{f(x)}{\|f(x)\|},"How to solve the following ODE? $$\dot{x} = \frac{f(x)}{\|f(x)\|},$$ where $x : \mathbb{R} \to \mathbb{R}^n$ , i.e., $x(t)$ is the trajectory. The right-hand side $f : \mathbb{R}^n \to \mathbb{R}^n$ is a continuously differentiable function with respect to $x$ . $\|\cdot\|$ is any vector norm. I think the right-hand side of the ODE is not Lipschitz continuous. For example, let's take $n=1$ and $f(x)=-x$ , then the right-hand side $-x/|x|$ is not even continuous. In this case, the theorem of existence and uniqueness of a solution cannot be applied. Then how to analyze the existence of the solution? Is there finite escape time?","How to solve the following ODE? where , i.e., is the trajectory. The right-hand side is a continuously differentiable function with respect to . is any vector norm. I think the right-hand side of the ODE is not Lipschitz continuous. For example, let's take and , then the right-hand side is not even continuous. In this case, the theorem of existence and uniqueness of a solution cannot be applied. Then how to analyze the existence of the solution? Is there finite escape time?","\dot{x} = \frac{f(x)}{\|f(x)\|}, x : \mathbb{R} \to \mathbb{R}^n x(t) f : \mathbb{R}^n \to \mathbb{R}^n x \|\cdot\| n=1 f(x)=-x -x/|x|","['real-analysis', 'ordinary-differential-equations', 'dynamical-systems', 'continuity', 'gradient-flows']"
22,A subadditive bijection on the positive reals,A subadditive bijection on the positive reals,,"Question. Does there exist a subadditive bijection $f$ of the positive reals $(0,\infty)$ such that $$ \liminf_{x\to 0^+}f(x)=0 \,\,\,\text{ and }\,\,\,\limsup_{x\to 0^+}f(x)=1\,? $$ Ps1. I guess the answer is no. However, it is affirmative if we replace ""bijective"" by ""injective"" by the following example: $f(x)=x$ if $x$ is rational, otherwise $f(x)=x+1$ . Ps2. The answer is also affirmative if we replace $$\limsup_{x\to 0^+}f(x)=1$$ with $$\limsup_{x\to 0^+}f(x)\neq 0.$$ Indeed, let $(x_i: i \in I)$ be an Hamel basis of the vector space $\mathbf{R}$ over $\mathbf{Q}$ . Fix a nontrivial permutation $\sigma$ of $I$ . For each $r=\sum_j \lambda_jx_j>0$ define $f(r)=|g(r)|$ , where $g(r)=\sum_j \lambda_j x_{\sigma(j)}$ ( thanks i707107 ). Note that $g$ is an additive bijection, hence $$ f(x+y)=|g(x)+g(y)| \le f(x)+f(y)$$ for all $x,y>0$ , so $f$ is subadditive. Now suppose $f(x)=f(y)$ with $x,y>0$ . Then either $g(x)=g(y)$ so $x=y$ , or $g(x)=-g(y)$ so each coefficient of $\lambda_j$ (in $x$ ) is the inverse of the corresponding (in $y$ ), so $x=-y$ : if $x,y>0$ , then $f(x)=f(y)$ implies $x=y$ so $f$ is injective. About surjectivity, fix $x=\sum_{j} \lambda_j x_j$ and define $y=\sum_j \lambda_j x_{\sigma^{-1}(j)}$ . Hence $z:=\max(y,-y)>0$ and $f(z)=x$ ; so $f$ is subjective. Finally, it is known that $g$ has a graph dense in $\mathbf{R}^2$ , from which it holds also $\liminf_{x\to 0^+}f(x)=0$ and $\limsup_{x\to 0^+}f(x)=\infty$ .","Question. Does there exist a subadditive bijection of the positive reals such that Ps1. I guess the answer is no. However, it is affirmative if we replace ""bijective"" by ""injective"" by the following example: if is rational, otherwise . Ps2. The answer is also affirmative if we replace with Indeed, let be an Hamel basis of the vector space over . Fix a nontrivial permutation of . For each define , where ( thanks i707107 ). Note that is an additive bijection, hence for all , so is subadditive. Now suppose with . Then either so , or so each coefficient of (in ) is the inverse of the corresponding (in ), so : if , then implies so is injective. About surjectivity, fix and define . Hence and ; so is subjective. Finally, it is known that has a graph dense in , from which it holds also and .","f (0,\infty) 
\liminf_{x\to 0^+}f(x)=0 \,\,\,\text{ and }\,\,\,\limsup_{x\to 0^+}f(x)=1\,?
 f(x)=x x f(x)=x+1 \limsup_{x\to 0^+}f(x)=1 \limsup_{x\to 0^+}f(x)\neq 0. (x_i: i \in I) \mathbf{R} \mathbf{Q} \sigma I r=\sum_j \lambda_jx_j>0 f(r)=|g(r)| g(r)=\sum_j \lambda_j x_{\sigma(j)} g 
f(x+y)=|g(x)+g(y)| \le f(x)+f(y) x,y>0 f f(x)=f(y) x,y>0 g(x)=g(y) x=y g(x)=-g(y) \lambda_j x y x=-y x,y>0 f(x)=f(y) x=y f x=\sum_{j} \lambda_j x_j y=\sum_j \lambda_j x_{\sigma^{-1}(j)} z:=\max(y,-y)>0 f(z)=x f g \mathbf{R}^2 \liminf_{x\to 0^+}f(x)=0 \limsup_{x\to 0^+}f(x)=\infty","['real-analysis', 'limsup-and-liminf']"
23,Calculating the limit $\lim((n!)^{1/n})$,Calculating the limit,\lim((n!)^{1/n}),"Find $\lim_{n\to\infty} ((n!)^{1/n})$. The question seemed rather simple at first, and then I realized I was not sure how to properly deal with this at all. My attempt: take the logarithm,  $$\lim_{n\to\infty} \ln((n!)^{1/n}) =  \lim_{n\to\infty} (1/n)\ln(n!) =  \lim_{n\to\infty} (\ln(n!)/n)$$ Applying L'hopital's rule:  $$\lim_{n\to\infty} [n! (-\gamma + \sum(1/k))]/n! = \lim_{n\to\infty} (-\gamma + \sum(1/k))=  \lim_{n\to\infty} (-(\lim(\sum(1/k) - \ln(n)) + \sum(1/k))  = \lim_{n\to\infty} (\ln(n) + \sum(1/k)-\sum(1/k)  = \lim_{n\to\infty} (\ln(n))$$ I proceeded to expand the $\ln(n)$ out into Maclaurin form $$\lim_{n\to\infty} (n + (n^2/2)+...) = \infty$$ Since I $\ln$'ed in the beginning, I proceeded to e the infinity  $$= e^\infty  = \infty$$ So am I write in how I approached this or am I just not on the right track? I know it diverges, I was just wanted to try my best to explicitly show it.","Find $\lim_{n\to\infty} ((n!)^{1/n})$. The question seemed rather simple at first, and then I realized I was not sure how to properly deal with this at all. My attempt: take the logarithm,  $$\lim_{n\to\infty} \ln((n!)^{1/n}) =  \lim_{n\to\infty} (1/n)\ln(n!) =  \lim_{n\to\infty} (\ln(n!)/n)$$ Applying L'hopital's rule:  $$\lim_{n\to\infty} [n! (-\gamma + \sum(1/k))]/n! = \lim_{n\to\infty} (-\gamma + \sum(1/k))=  \lim_{n\to\infty} (-(\lim(\sum(1/k) - \ln(n)) + \sum(1/k))  = \lim_{n\to\infty} (\ln(n) + \sum(1/k)-\sum(1/k)  = \lim_{n\to\infty} (\ln(n))$$ I proceeded to expand the $\ln(n)$ out into Maclaurin form $$\lim_{n\to\infty} (n + (n^2/2)+...) = \infty$$ Since I $\ln$'ed in the beginning, I proceeded to e the infinity  $$= e^\infty  = \infty$$ So am I write in how I approached this or am I just not on the right track? I know it diverges, I was just wanted to try my best to explicitly show it.",,"['real-analysis', 'limits', 'factorial', 'radicals']"
24,Find the limit $\lim \limits_{n\to \infty }\cos \left(\pi\sqrt{n^{2}-n} \right)$,Find the limit,\lim \limits_{n\to \infty }\cos \left(\pi\sqrt{n^{2}-n} \right),"I'd love your help with finding the following limit: $$\lim_{n\to \infty }\cos (\pi\sqrt{n^{2}-n}).$$ I was asked to find this limit, but honestly I believe that it doesn't exist. According to Heine Theorem of limit of functions, I can choose two sequences: $x_{k}=2\pi k$ and $y_{k}=2\pi k+\pi$ and notice that when I apply the function on both of them, I'll get -1 and 1, respectively. Am I right? Thank you again.","I'd love your help with finding the following limit: $$\lim_{n\to \infty }\cos (\pi\sqrt{n^{2}-n}).$$ I was asked to find this limit, but honestly I believe that it doesn't exist. According to Heine Theorem of limit of functions, I can choose two sequences: $x_{k}=2\pi k$ and $y_{k}=2\pi k+\pi$ and notice that when I apply the function on both of them, I'll get -1 and 1, respectively. Am I right? Thank you again.",,"['calculus', 'real-analysis']"
25,"Is the space $C[0,1]$ complete?",Is the space  complete?,"C[0,1]","In order to prove $C[0,1]$ is complete, my functional analysis book says: It is only necessary to show that every Cauchy sequence in $C[0,1]$ has a limit. It goes on by supposing $\{x_n\}$ is a Cauchy sequence in $C[0,1]$ . Then for each fixed $t \in [0,1]$ $|x_n(t)-x_m(t)| \le \|x_n - x_m\| \to 0$ , so $\{x_n(t)\}$ is a Cauchy sequence of real numbers. I guess this inequality makes sense, since difference of function sequences would be larger than pointwise difference (at a specific $t$ ) of those functions.. But I did not understand how the author could conclude the sequence $\{x_n(t)\}$ is Cauchy. Second question: the explanation continues, ""Since the set of real numbers is complete, there is a real number $x(t)$ to which the squence converges; $x_n(t) \to x(t)$ "". I know set of real numbers is complete, but how can $\{x_n(t)\}$ represent the entire set of real numbers?","In order to prove is complete, my functional analysis book says: It is only necessary to show that every Cauchy sequence in has a limit. It goes on by supposing is a Cauchy sequence in . Then for each fixed , so is a Cauchy sequence of real numbers. I guess this inequality makes sense, since difference of function sequences would be larger than pointwise difference (at a specific ) of those functions.. But I did not understand how the author could conclude the sequence is Cauchy. Second question: the explanation continues, ""Since the set of real numbers is complete, there is a real number to which the squence converges; "". I know set of real numbers is complete, but how can represent the entire set of real numbers?","C[0,1] C[0,1] \{x_n\} C[0,1] t \in [0,1] |x_n(t)-x_m(t)| \le \|x_n - x_m\| \to 0 \{x_n(t)\} t \{x_n(t)\} x(t) x_n(t) \to x(t) \{x_n(t)\}","['real-analysis', 'functional-analysis', 'convergence-divergence']"
26,Is indefinite integration non-linear?,Is indefinite integration non-linear?,,"Let us consider this small problem: $$ \int0\;dx = 0\cdot\int1\;dx = 0\cdot(x+c) = 0 \tag1 $$ $$ \frac{dc}{dx} = 0 \qquad\iff\qquad \int 0\;dx = c, \qquad\forall c\in\mathbb{R} \tag2 $$ These are two conflicting results. Based on this other question, Sam Dehority's answer seems to indicate: $$ \int\alpha f(x)\;dx\neq\alpha\int f(x)\;dx,\qquad\forall\alpha\in\mathbb{R} \tag3 $$ However, this clearly implies that indefinite integration is nonlinear, since a linear operator $P$ must satisfy $P(\alpha f) = \alpha Pf, \forall\alpha\in\mathbb{R}$, including $\alpha=0$. After all, a linear combination of elements of a vector space $V$ may have zero valued scalars: $f = \alpha g + \beta h, \forall\alpha,\beta\in\mathbb{R}$ and $g, h\in V$. This all seems to corroborate that zero is not excluded when it comes to possible scalars of linear operators. To take two examples, both matrix operators in linear algebra and derivative operators are linear, even when the scalar is zero. In a matrix case for instance, let the operator $A$ operate a vector: $A\vec{x} = \vec{y}$. Now: $A(\alpha\vec{x}) = \alpha A\vec{x} = \alpha\vec{y}$. This works even for $\alpha = 0$. Why is $(3)$ true? Can someone prove it formally? If $(3)$ is false, how do we fix $(1)$ and $(2)$? When exactly does the following equality hold (formal proof)? $$ \int\alpha f(x)\;dx  = \alpha\int f(x)\;dx,\qquad\forall\alpha\in\mathbb{R} $$ I would appreciate formal answers and proofs.","Let us consider this small problem: $$ \int0\;dx = 0\cdot\int1\;dx = 0\cdot(x+c) = 0 \tag1 $$ $$ \frac{dc}{dx} = 0 \qquad\iff\qquad \int 0\;dx = c, \qquad\forall c\in\mathbb{R} \tag2 $$ These are two conflicting results. Based on this other question, Sam Dehority's answer seems to indicate: $$ \int\alpha f(x)\;dx\neq\alpha\int f(x)\;dx,\qquad\forall\alpha\in\mathbb{R} \tag3 $$ However, this clearly implies that indefinite integration is nonlinear, since a linear operator $P$ must satisfy $P(\alpha f) = \alpha Pf, \forall\alpha\in\mathbb{R}$, including $\alpha=0$. After all, a linear combination of elements of a vector space $V$ may have zero valued scalars: $f = \alpha g + \beta h, \forall\alpha,\beta\in\mathbb{R}$ and $g, h\in V$. This all seems to corroborate that zero is not excluded when it comes to possible scalars of linear operators. To take two examples, both matrix operators in linear algebra and derivative operators are linear, even when the scalar is zero. In a matrix case for instance, let the operator $A$ operate a vector: $A\vec{x} = \vec{y}$. Now: $A(\alpha\vec{x}) = \alpha A\vec{x} = \alpha\vec{y}$. This works even for $\alpha = 0$. Why is $(3)$ true? Can someone prove it formally? If $(3)$ is false, how do we fix $(1)$ and $(2)$? When exactly does the following equality hold (formal proof)? $$ \int\alpha f(x)\;dx  = \alpha\int f(x)\;dx,\qquad\forall\alpha\in\mathbb{R} $$ I would appreciate formal answers and proofs.",,"['calculus', 'real-analysis', 'integration', 'indefinite-integrals']"
27,Why Are the Reals Uncountable?,Why Are the Reals Uncountable?,,"Let us start by clarifying this a bit.  I am aware of some proofs that irrationals/reals are uncountable .  My issue comes by way of some properties of the reals. These issues can be summed up by the combination of the following questions: Is it true that between any two rationals one may find at least one irrational? Is it true that between any two irrationals one may find at least one rational? Why are the reals uncountable? I've been talking with a friend about why the answer of these three questions can be the case when they somewhat seem to contradict each other.  I seek clarification on the subject.  Herein lies a summary of the discussion: Person A: By  way of Cantor Diagonalization it can be shown that the reals are uncountable. Person B: But is it not also the case that one may find at least one rational between any two irrationals and vice versa? Person A: That seems logical, I can't pose a counterexample... but why does that matter? Person B: Wouldn't that imply that for every irrational there is a corresponding rational?  And from this the Reals would be equivalent to 2 elements for every element of the rationals? Person A: That implies that the Reals are countable, but we have already shown that they weren't... where is the hole in our reasoning? And so I pose it to you... where is the hole in our reasoning?","Let us start by clarifying this a bit.  I am aware of some proofs that irrationals/reals are uncountable .  My issue comes by way of some properties of the reals. These issues can be summed up by the combination of the following questions: Is it true that between any two rationals one may find at least one irrational? Is it true that between any two irrationals one may find at least one rational? Why are the reals uncountable? I've been talking with a friend about why the answer of these three questions can be the case when they somewhat seem to contradict each other.  I seek clarification on the subject.  Herein lies a summary of the discussion: Person A: By  way of Cantor Diagonalization it can be shown that the reals are uncountable. Person B: But is it not also the case that one may find at least one rational between any two irrationals and vice versa? Person A: That seems logical, I can't pose a counterexample... but why does that matter? Person B: Wouldn't that imply that for every irrational there is a corresponding rational?  And from this the Reals would be equivalent to 2 elements for every element of the rationals? Person A: That implies that the Reals are countable, but we have already shown that they weren't... where is the hole in our reasoning? And so I pose it to you... where is the hole in our reasoning?",,"['real-analysis', 'elementary-set-theory', 'fake-proofs']"
28,"If a nonempty set of real numbers is open and closed, is it $\mathbb{R}$? Why/Why not?","If a nonempty set of real numbers is open and closed, is it ? Why/Why not?",\mathbb{R},"In other words, are $\emptyset$ and $\mathbb{R}$ the only open and closed sets in $\mathbb{R}$? Why/Why not? I tried by assuming a set is equal to its interior points and contains its limit points. A bounded set will not do since stuff like $[1,4]$ and $\{5\}$ will not work, though that is not really proof. Help please? Anyway, it must then be unbounded. If $a$ is a real number then $(a,\infty)$, $(-\infty,a)$, $[a,\infty)$ and $(-\infty,a]$ don't seem to cut it so it must be $\mathbb{R}$.","In other words, are $\emptyset$ and $\mathbb{R}$ the only open and closed sets in $\mathbb{R}$? Why/Why not? I tried by assuming a set is equal to its interior points and contains its limit points. A bounded set will not do since stuff like $[1,4]$ and $\{5\}$ will not work, though that is not really proof. Help please? Anyway, it must then be unbounded. If $a$ is a real number then $(a,\infty)$, $(-\infty,a)$, $[a,\infty)$ and $(-\infty,a]$ don't seem to cut it so it must be $\mathbb{R}$.",,"['real-analysis', 'general-topology', 'analysis', 'connectedness']"
29,Geometrical Interpretation of Cauchy Riemann equations?,Geometrical Interpretation of Cauchy Riemann equations?,,"Differentiation has an obvious geometric interpretation, and the Cauchy Riemann equations are closely linked with differentiation. Do the Cauchy Riemann equations have a geometric interpretation?","Differentiation has an obvious geometric interpretation, and the Cauchy Riemann equations are closely linked with differentiation. Do the Cauchy Riemann equations have a geometric interpretation?",,"['calculus', 'real-analysis', 'complex-analysis', 'multivariable-calculus']"
30,"Are Taylor series and power series the same ""thing""?","Are Taylor series and power series the same ""thing""?",,"I was just wondering in the lingo of Mathematics, are these two ""ideas"" the same? I know we have Taylor series, and their specialisation the Maclaurin series, but are power series a more general concept? How does either/all of these ideas relate to generating functions?","I was just wondering in the lingo of Mathematics, are these two ""ideas"" the same? I know we have Taylor series, and their specialisation the Maclaurin series, but are power series a more general concept? How does either/all of these ideas relate to generating functions?",,"['real-analysis', 'complex-analysis', 'power-series', 'taylor-expansion']"
31,"Continuity of the function $x\mapsto d(x,A)$ on a metric space",Continuity of the function  on a metric space,"x\mapsto d(x,A)","Let $(X,d)$ be a metric space. How to prove that for any closed $A$ a function $d(x,A)$ is continuous - I know that it is even Lipschitz continuous, but I have a problem with the proof:  $$ |d(x,a) - d(y,a)| \leq d(x,y) $$ for any $a\in A$ - but we cannot just replace it by $|d(x,A) - d(y,A)|\leq d(x,y)$ since the minimum (or infimum in general) can be attained in different points $a\in A$ for $x$ and $y$, so we only have that $$ |d(x,A)-d(y,A)|\leq d(x,y)+\sup\limits_{a,b\in A}d(a,b) $$ which does not mean continuity.","Let $(X,d)$ be a metric space. How to prove that for any closed $A$ a function $d(x,A)$ is continuous - I know that it is even Lipschitz continuous, but I have a problem with the proof:  $$ |d(x,a) - d(y,a)| \leq d(x,y) $$ for any $a\in A$ - but we cannot just replace it by $|d(x,A) - d(y,A)|\leq d(x,y)$ since the minimum (or infimum in general) can be attained in different points $a\in A$ for $x$ and $y$, so we only have that $$ |d(x,A)-d(y,A)|\leq d(x,y)+\sup\limits_{a,b\in A}d(a,b) $$ which does not mean continuity.",,"['real-analysis', 'metric-spaces', 'continuity']"
32,Solving $ \cos (\cos (\cos (\cos(x))))=\sin (\sin (\sin (\sin (x)))) $,Solving, \cos (\cos (\cos (\cos(x))))=\sin (\sin (\sin (\sin (x)))) ,"I found this problem in a collection of contest problems of a Russian competition in 1995 and wasn't able to solve it. Solve for real $x$:   $$ \cos (\cos (\cos (\cos(x))))=\sin (\sin (\sin (\sin (x)))) $$ My guess is that there is no solution, but how do I prove it? I tried to estimate $LHS\ge \cos (1) \ge \cos(\pi/3)=1/2 $ and RHS similarly but the ranges overlap.. Do you have a better idea?","I found this problem in a collection of contest problems of a Russian competition in 1995 and wasn't able to solve it. Solve for real $x$:   $$ \cos (\cos (\cos (\cos(x))))=\sin (\sin (\sin (\sin (x)))) $$ My guess is that there is no solution, but how do I prove it? I tried to estimate $LHS\ge \cos (1) \ge \cos(\pi/3)=1/2 $ and RHS similarly but the ranges overlap.. Do you have a better idea?",,"['calculus', 'real-analysis', 'analysis', 'trigonometry', 'contest-math']"
33,"How to find PV $\int_0^\infty \frac{\log \cos^2 \alpha x}{\beta^2-x^2} \, \mathrm dx=\alpha \pi$",How to find PV,"\int_0^\infty \frac{\log \cos^2 \alpha x}{\beta^2-x^2} \, \mathrm dx=\alpha \pi","$$ I:=PV\int_0^\infty \frac{\log\left(\cos^2\left(\alpha x\right)\right)}{\beta^2-x^2} \, \mathrm dx=\alpha \pi,\qquad \alpha>0,\  \beta\in \mathbb{R}.$$ I am trying to solve this integral, I edited and added in Principle value to clarify the convergence issue that the community pointed out.  I tried to use $2\cos^2(\alpha x)=1+\cos 2\alpha x\,$ and obtained $$ I=-\log 2 \int_0^\infty \frac{\mathrm dx}{\beta^2-x^2}+\int_0^\infty \frac{\log (1+\cos 2 \alpha x)}{\beta^2-x^2}\mathrm dx, $$ simplifying $$ I=\frac{ \pi \log 2 }{2\beta}+\int_0^\infty \frac{\log (1+\cos 2 \alpha x)}{\beta^2-x^2}\mathrm dx $$ but  stuck here. Note the result of the integral is independent of the parameter $\beta$. Thank you Also for $\alpha=1$, is there a geometrical interpretation of this integral and why it is $\pi$? Note this integral  $$ \int_0^\infty \frac{\log \sin^2 \alpha x}{\beta^2-x^2} \,\mathrm dx=\alpha \pi-\frac{\pi^2}{2\beta},\qquad \alpha>0,\beta>0 $$ is also FASCINATING, note the constraint $\beta>0$ for this one.  I am not looking for a solution to this too obviously on the same post, it is just to interest people with another friendly integral.","$$ I:=PV\int_0^\infty \frac{\log\left(\cos^2\left(\alpha x\right)\right)}{\beta^2-x^2} \, \mathrm dx=\alpha \pi,\qquad \alpha>0,\  \beta\in \mathbb{R}.$$ I am trying to solve this integral, I edited and added in Principle value to clarify the convergence issue that the community pointed out.  I tried to use $2\cos^2(\alpha x)=1+\cos 2\alpha x\,$ and obtained $$ I=-\log 2 \int_0^\infty \frac{\mathrm dx}{\beta^2-x^2}+\int_0^\infty \frac{\log (1+\cos 2 \alpha x)}{\beta^2-x^2}\mathrm dx, $$ simplifying $$ I=\frac{ \pi \log 2 }{2\beta}+\int_0^\infty \frac{\log (1+\cos 2 \alpha x)}{\beta^2-x^2}\mathrm dx $$ but  stuck here. Note the result of the integral is independent of the parameter $\beta$. Thank you Also for $\alpha=1$, is there a geometrical interpretation of this integral and why it is $\pi$? Note this integral  $$ \int_0^\infty \frac{\log \sin^2 \alpha x}{\beta^2-x^2} \,\mathrm dx=\alpha \pi-\frac{\pi^2}{2\beta},\qquad \alpha>0,\beta>0 $$ is also FASCINATING, note the constraint $\beta>0$ for this one.  I am not looking for a solution to this too obviously on the same post, it is just to interest people with another friendly integral.",,"['calculus', 'real-analysis', 'integration', 'complex-analysis', 'definite-integrals']"
34,Examples of non-measurable sets in $\mathbb{R}$,Examples of non-measurable sets in,\mathbb{R},"I'm a newcomer in real analysis. I am learning the concept of measurable by myself using Royden's book ""Real Analysis"". I have a question regarding measurable sets. The following definition comes from Royden's book (page 35). Definition: A set $E$ is said to be measureable provided for any set $A$ , $$m^*(A)=m^*(A\cap E)+m^*(A\cap E^C)$$ where $m^*(\cdot)$ denotes the outer measure of a set. To me, intuitively the above equation holds for all sets. In $\mathbb{R}$ , I think a set can either be an interval or a series of isolated points (right?). It seems these kinds of sets are all measurable by the definition. Can anybody give me an example of non-measureable sets so that I can have an intuitive understanding regarding this concept? Thanks.","I'm a newcomer in real analysis. I am learning the concept of measurable by myself using Royden's book ""Real Analysis"". I have a question regarding measurable sets. The following definition comes from Royden's book (page 35). Definition: A set is said to be measureable provided for any set , where denotes the outer measure of a set. To me, intuitively the above equation holds for all sets. In , I think a set can either be an interval or a series of isolated points (right?). It seems these kinds of sets are all measurable by the definition. Can anybody give me an example of non-measureable sets so that I can have an intuitive understanding regarding this concept? Thanks.",E A m^*(A)=m^*(A\cap E)+m^*(A\cap E^C) m^*(\cdot) \mathbb{R},"['real-analysis', 'measure-theory', 'examples-counterexamples']"
35,Upper bound for $( a_1 + a_2 + \cdots + a_n)^{1/2}$,Upper bound for,( a_1 + a_2 + \cdots + a_n)^{1/2},Is there any upper bound for an expression like: $$\left( a_1 + a_2 + \cdots + a_n\right)^{1/2} ?$$ I need it for $n=3$. I know Hardy's inequality but it is for exponent greater than 1. Is there anything for the square root?,Is there any upper bound for an expression like: $$\left( a_1 + a_2 + \cdots + a_n\right)^{1/2} ?$$ I need it for $n=3$. I know Hardy's inequality but it is for exponent greater than 1. Is there anything for the square root?,,"['real-analysis', 'algebra-precalculus', 'inequality', 'upper-lower-bounds']"
36,"Prove that $\int_0^1 \frac{{\rm{Li}}_2(x)\ln(1-x)\ln^2(x)}{x} \,dx=-\frac{\zeta(6)}{3}$",Prove that,"\int_0^1 \frac{{\rm{Li}}_2(x)\ln(1-x)\ln^2(x)}{x} \,dx=-\frac{\zeta(6)}{3}","I have spent my holiday on Sunday to crack several integral & series problems and I am having trouble to prove the following integral \begin{equation} \int_0^1 \frac{{\rm{Li}}_2(x)\ln(1-x)\ln^2(x)}{x} \,dx=-\frac{\zeta(6)}{3} \end{equation} Using integration by parts, $u=\ln^2(x)$ and $dv=\displaystyle\frac{{\rm{Li}}_2(x)\ln(1-x)}{x} \,dx$, I manage to obtain that the integral is equivalent to \begin{equation} \int_0^1 \frac{{\rm{Li}}_2^2(x)\ln(x)}{x} \,dx \end{equation} where ${\rm{Li}}_2^2(x)={\rm{Li}}_2(x)^2$, square of dilogarithm of $x$. Could anyone here please help me to prove the above integral preferably with elementary ways ( high school methods/ not residue method )? Any help would be greatly appreciated. Thank you.","I have spent my holiday on Sunday to crack several integral & series problems and I am having trouble to prove the following integral \begin{equation} \int_0^1 \frac{{\rm{Li}}_2(x)\ln(1-x)\ln^2(x)}{x} \,dx=-\frac{\zeta(6)}{3} \end{equation} Using integration by parts, $u=\ln^2(x)$ and $dv=\displaystyle\frac{{\rm{Li}}_2(x)\ln(1-x)}{x} \,dx$, I manage to obtain that the integral is equivalent to \begin{equation} \int_0^1 \frac{{\rm{Li}}_2^2(x)\ln(x)}{x} \,dx \end{equation} where ${\rm{Li}}_2^2(x)={\rm{Li}}_2(x)^2$, square of dilogarithm of $x$. Could anyone here please help me to prove the above integral preferably with elementary ways ( high school methods/ not residue method )? Any help would be greatly appreciated. Thank you.",,"['real-analysis', 'integration', 'definite-integrals', 'harmonic-numbers', 'polylogarithm']"
37,Proving separability of the countable product of separable spaces using density.,Proving separability of the countable product of separable spaces using density.,,"Let $\{X_j\}_{j=1}^{+\infty}$ be a sequence of separable spaces. The goal is to prove that the infinite cartesian product of separable spaces is indeed separable by showing that the product has a countable dense subset, arising from the fact that each $X_m$ has a countably dense subset.","Let be a sequence of separable spaces. The goal is to prove that the infinite cartesian product of separable spaces is indeed separable by showing that the product has a countable dense subset, arising from the fact that each has a countably dense subset.",\{X_j\}_{j=1}^{+\infty} X_m,"['real-analysis', 'general-topology', 'product-space', 'separable-spaces']"
38,"Measure of  reals in $[0,1]$ which don't have $4$ in decimal expansion",Measure of  reals in  which don't have  in decimal expansion,"[0,1] 4","It's an exercise in E. M. Stein's ""Real Analysis."" Let $A$ be the subset of $[0,1]$ which consists of all numbers which do not have the digit $4$ appearing in their decimal expansion. What is the measure of $A$? I would be grateful if someone can give me some hints. Thank you.","It's an exercise in E. M. Stein's ""Real Analysis."" Let $A$ be the subset of $[0,1]$ which consists of all numbers which do not have the digit $4$ appearing in their decimal expansion. What is the measure of $A$? I would be grateful if someone can give me some hints. Thank you.",,['real-analysis']
39,Are real and complex analysis related in any way?,Are real and complex analysis related in any way?,,"I was watching a video and the lecturer discusses the function $$\frac{1}{1+x^2} = \sum_{n=0}^{\infty} {(-1)^n x^{2n}}$$ $$|x| < 1$$ explaining that the radius of convergence for this Taylor series centered at $x=0$ is 1 because it is being affected by $i$ and $-i$ . Then, he goes on to talk about how real analysis is a glimpse into complex analysis. In the same video, the lecturer also provides the following example where a complex function is defined by using real Taylor series: \begin{align} e^{ix} &= \cos(x) +i \sin(x) \\ &= \sum_{n=0}^\infty \frac{(ix)^n}{n!} \\ &= \sum_{n=0}^\infty (-1)^n \frac{x^{2n}}{2n!} + i \sum_{n=0}^\infty (-1)^n \frac{x^{2n+1}}{(2n+1)!} \end{align} Can someone help elaborate by what the lecturer probably meant? What is the connection between real analysis and complex analysis? I understand that there are two different types of analyticity: real analytic and complex analytic. Are they connected?","I was watching a video and the lecturer discusses the function explaining that the radius of convergence for this Taylor series centered at is 1 because it is being affected by and . Then, he goes on to talk about how real analysis is a glimpse into complex analysis. In the same video, the lecturer also provides the following example where a complex function is defined by using real Taylor series: Can someone help elaborate by what the lecturer probably meant? What is the connection between real analysis and complex analysis? I understand that there are two different types of analyticity: real analytic and complex analytic. Are they connected?","\frac{1}{1+x^2} = \sum_{n=0}^{\infty} {(-1)^n x^{2n}} |x| < 1 x=0 i -i \begin{align}
e^{ix}
&= \cos(x) +i \sin(x) \\
&= \sum_{n=0}^\infty \frac{(ix)^n}{n!} \\
&= \sum_{n=0}^\infty (-1)^n \frac{x^{2n}}{2n!} + i \sum_{n=0}^\infty (-1)^n \frac{x^{2n+1}}{(2n+1)!}
\end{align}","['real-analysis', 'complex-analysis']"
40,Why does the condition of a function being differentiable always require an open domain?,Why does the condition of a function being differentiable always require an open domain?,,"Going through Spivak's Calculus on Manifolds and in his definition of a differentiable function from a subset $A$ of $\mathbb{R}^n$ to $\mathbb{R}^m$, $f$ is said to be differentiable if it can be extended to a differentiable function on an open set containing $A$. Why is this? So we dont have to worry about taking limits at boundary points? Why is that even a problem? If thats not the problem, what is wrong with defining $f$ to be differentiable on $A$ if it is differentiable at each point in $A$?","Going through Spivak's Calculus on Manifolds and in his definition of a differentiable function from a subset $A$ of $\mathbb{R}^n$ to $\mathbb{R}^m$, $f$ is said to be differentiable if it can be extended to a differentiable function on an open set containing $A$. Why is this? So we dont have to worry about taking limits at boundary points? Why is that even a problem? If thats not the problem, what is wrong with defining $f$ to be differentiable on $A$ if it is differentiable at each point in $A$?",,"['real-analysis', 'differential-geometry', 'derivatives', 'definition']"
41,Finding the limit of a sequence with an undesirable $\ln k$,Finding the limit of a sequence with an undesirable,\ln k,"I am trying to compute the limit of this sequence: $$\lim\limits_{n \to \infty} \dfrac{(-1)^nn^2}{n!} \sum\limits_{k=2}^{n}\binom{n}{k}(-1)^kk^{n-1}\ln k$$ I can compute without the $\ln k$ in the expression. $\displaystyle\sum\limits_{k=0}^{n}\binom{n}{k}(-1)^kk^{n-1}$ is the $n$-th order finite difference of $x^{n-1}$ at $0$, thus it vanishes. Else one could come to the same conclusion with a combinatorial interpretation (with I.E. Principle) The $\ln k$ part is causing me trouble. Any ideas how to deal with it?","I am trying to compute the limit of this sequence: $$\lim\limits_{n \to \infty} \dfrac{(-1)^nn^2}{n!} \sum\limits_{k=2}^{n}\binom{n}{k}(-1)^kk^{n-1}\ln k$$ I can compute without the $\ln k$ in the expression. $\displaystyle\sum\limits_{k=0}^{n}\binom{n}{k}(-1)^kk^{n-1}$ is the $n$-th order finite difference of $x^{n-1}$ at $0$, thus it vanishes. Else one could come to the same conclusion with a combinatorial interpretation (with I.E. Principle) The $\ln k$ part is causing me trouble. Any ideas how to deal with it?",,"['calculus', 'real-analysis', 'sequences-and-series', 'limits']"
42,Examples of absolutely continuous functions that are not Lipschitz.,Examples of absolutely continuous functions that are not Lipschitz.,,"I have just solved an exercise, which asked to show that function $f$ is Lipschitz implies that $f$ is absolutely continuous. However, I'm wondering if the converse is true. I can't seem to think of any counterexamples at the moment. I think I'm brain dead or something, so I could use some help.","I have just solved an exercise, which asked to show that function $f$ is Lipschitz implies that $f$ is absolutely continuous. However, I'm wondering if the converse is true. I can't seem to think of any counterexamples at the moment. I think I'm brain dead or something, so I could use some help.",,['real-analysis']
43,Differentiability implies continuity - A question about the proof,Differentiability implies continuity - A question about the proof,,"I have a question, to aid my understanding, about the proof that differentiability implies continutity . $\mathstrut$ Differentiability Definition When we say a function is differentiable at $x_0$ , we mean that the limit: $$‎f^{\prime} ‎(x) = \lim_{x\to x_0} \frac{f(x) - f(x_0)}{x-x_0}$$ exists. Continuity Definition When we say a function is continuous at $x_0$ , we mean that: $$\lim_{x\to x_0} f(x) - f(x_0) = 0$$ Theorem: Differentiability implies Continuity: If $f$ is a differentiable function at $x_0$ , then it is continuous at $x_0$ . Proof: Let us suppose that $f$ is differentiable at $x_0$ . Then $$ \lim_{x\to x_0} \frac{f(x) - f(x_0)}{x-x_0} =  ‎f^{\prime} ‎(x) $$ and hence $$ \lim_{x\to x_0} f(x) - f(x_0) = \lim_{x\to x_0} \left[ \frac{f(x) - f(x_0)}{x-x_0} \right] \cdot \lim_{x\to x_0} (x-x_0) = 0$$ We have therefore shown that, using the definition of continuous, if the function is differentiable at $x_0$ , it must also be continuous. My Question The proof seems to execute the following steps: Assume the function is continuous at $x_0$ Show that, with little algebra, we can change this into an equivalent question about differentiability at $x_0$ . With this little bit of algebra, we can show that if a function is differentiable at $x_0$ it is also continuous. What I am slightly unsure about is the apparent circularity. In my mind it seems to say, if a function is continuous, we can show that if it is also differentiable , then it is continuous. Rather than what I was expecting, namely, if a function is differentiable, we can show it must be continuous. Hopefully my confusion is clear. Any help will be greatly appreciated.","I have a question, to aid my understanding, about the proof that differentiability implies continutity . Differentiability Definition When we say a function is differentiable at , we mean that the limit: exists. Continuity Definition When we say a function is continuous at , we mean that: Theorem: Differentiability implies Continuity: If is a differentiable function at , then it is continuous at . Proof: Let us suppose that is differentiable at . Then and hence We have therefore shown that, using the definition of continuous, if the function is differentiable at , it must also be continuous. My Question The proof seems to execute the following steps: Assume the function is continuous at Show that, with little algebra, we can change this into an equivalent question about differentiability at . With this little bit of algebra, we can show that if a function is differentiable at it is also continuous. What I am slightly unsure about is the apparent circularity. In my mind it seems to say, if a function is continuous, we can show that if it is also differentiable , then it is continuous. Rather than what I was expecting, namely, if a function is differentiable, we can show it must be continuous. Hopefully my confusion is clear. Any help will be greatly appreciated.",\mathstrut x_0 ‎f^{\prime} ‎(x) = \lim_{x\to x_0} \frac{f(x) - f(x_0)}{x-x_0} x_0 \lim_{x\to x_0} f(x) - f(x_0) = 0 f x_0 x_0 f x_0  \lim_{x\to x_0} \frac{f(x) - f(x_0)}{x-x_0} =  ‎f^{\prime} ‎(x)   \lim_{x\to x_0} f(x) - f(x_0) = \lim_{x\to x_0} \left[ \frac{f(x) - f(x_0)}{x-x_0} \right] \cdot \lim_{x\to x_0} (x-x_0) = 0 x_0 x_0 x_0 x_0,"['real-analysis', 'derivatives', 'proof-writing', 'continuity']"
44,How to prove $\sum_{n=0}^{\infty} \frac{1}{1+n^2} = \frac{\pi+1}{2}+\frac{\pi}{e^{2\pi}-1}$,How to prove,\sum_{n=0}^{\infty} \frac{1}{1+n^2} = \frac{\pi+1}{2}+\frac{\pi}{e^{2\pi}-1},How can we prove the following $$\sum_{n=0}^{\infty} \dfrac{1}{1+n^2} = \dfrac{\pi+1}{2}+\dfrac{\pi}{e^{2\pi}-1}$$ I tried using partial fraction and the famous result $$\sum_{n=0}^{\infty} \dfrac{1}{n^2}=\frac{\pi^2}{6}$$ But I'm stuck at this problem.,How can we prove the following $$\sum_{n=0}^{\infty} \dfrac{1}{1+n^2} = \dfrac{\pi+1}{2}+\dfrac{\pi}{e^{2\pi}-1}$$ I tried using partial fraction and the famous result $$\sum_{n=0}^{\infty} \dfrac{1}{n^2}=\frac{\pi^2}{6}$$ But I'm stuck at this problem.,,"['calculus', 'real-analysis']"
45,What is the intuition for semi-continuous functions?,What is the intuition for semi-continuous functions?,,"Here is the definition of semi-continuous functions that I know. Let $X$ be a topological space and let $f$ be a function from $X$ into $R$. (1) $f$ is lower semi-continuous if $\forall \alpha\in R$, the set $\{x\in X : f(x) > \alpha \}$ is open in X. (2) $f$ is upper semi-continuous if $\forall \alpha\in R$, the set $\{x\in X : f(x) < \alpha \}$ is open in X. I heard that semi-continuity is a generalization of one-sided continuity from left or right (as in single variable calculus) to continuity from ""below"" or ""above"", but I could not see from the definitions above how that is so. How can I see this intuitively?","Here is the definition of semi-continuous functions that I know. Let $X$ be a topological space and let $f$ be a function from $X$ into $R$. (1) $f$ is lower semi-continuous if $\forall \alpha\in R$, the set $\{x\in X : f(x) > \alpha \}$ is open in X. (2) $f$ is upper semi-continuous if $\forall \alpha\in R$, the set $\{x\in X : f(x) < \alpha \}$ is open in X. I heard that semi-continuity is a generalization of one-sided continuity from left or right (as in single variable calculus) to continuity from ""below"" or ""above"", but I could not see from the definitions above how that is so. How can I see this intuitively?",,"['real-analysis', 'semicontinuous-functions']"
46,Limit points and interior points,Limit points and interior points,,"I am reading Rudin's book on real analysis and am stuck on a few definitions. First, here is the definition of a limit/interior point (not word to word from Rudin) but these definitions are worded from me (an undergrad student) so please correct me if they are not rigorous. The context here is basic topology and these are metric sets with the distance function as the metric. A point $p$ of a set $E$ is a limit point if every neighborhood of $p$   contains a point $q \neq p$ such that $ q \in E$ Also, an interior point is defined as A point $p$ of a set $E$ is an interior point if there is a   neighborhood $N_r\{p\}$ that is contained in $E$ (ie, is a subset of   E). I understand interior points. Ofcourse given a point $p$ you can have any radius $r$ that makes this neighborhood fit into the set. Thats how I see it, thats how I picture it. I can't understand limit points. It seems trivial to me that lets say you have a point $p$. Then one of its neighborhood is exactly the set in which it is contained, right? ie, you can pick a radius big enough that the neighborhood fits in the set. Ofcourse I know this is false. Our professor gave us an example of a subset being the integers. He said this subset has no limit points, but I can't see how.","I am reading Rudin's book on real analysis and am stuck on a few definitions. First, here is the definition of a limit/interior point (not word to word from Rudin) but these definitions are worded from me (an undergrad student) so please correct me if they are not rigorous. The context here is basic topology and these are metric sets with the distance function as the metric. A point $p$ of a set $E$ is a limit point if every neighborhood of $p$   contains a point $q \neq p$ such that $ q \in E$ Also, an interior point is defined as A point $p$ of a set $E$ is an interior point if there is a   neighborhood $N_r\{p\}$ that is contained in $E$ (ie, is a subset of   E). I understand interior points. Ofcourse given a point $p$ you can have any radius $r$ that makes this neighborhood fit into the set. Thats how I see it, thats how I picture it. I can't understand limit points. It seems trivial to me that lets say you have a point $p$. Then one of its neighborhood is exactly the set in which it is contained, right? ie, you can pick a radius big enough that the neighborhood fits in the set. Ofcourse I know this is false. Our professor gave us an example of a subset being the integers. He said this subset has no limit points, but I can't see how.",,"['real-analysis', 'general-topology']"
47,"Norms on C[0, 1] inducing the same topology as the sup norm","Norms on C[0, 1] inducing the same topology as the sup norm",,"This is an old homework problem of mine that I was never able to solve.  The solution may or may not involve the Baire category theorem, which I am terrible at applying. Let $C[0, 1]$ denote the space of continuous functions $[0, 1] \to \mathbb{R}$.  Suppose $|| \cdot ||$ is a norm on $C[0, 1]$ with respect to which the evaluation functions $f \mapsto f(x), x \in [0, 1]$ are continuous.  Show that the topology induced by $|| \cdot ||$ is the same as the usual topology induced by the sup norm. It is straightforward to show that any sequence converging uniformly converges with respect to $|| \cdot ||$.  I am stuck on proving the converse; I cannot seem to figure out how to use the assumption that $|| \cdot ||$ is a norm. Edit:  Zhen Lin informs me that $|| \cdot ||$ is supposed to be complete.  That should make the problem statement true now!","This is an old homework problem of mine that I was never able to solve.  The solution may or may not involve the Baire category theorem, which I am terrible at applying. Let $C[0, 1]$ denote the space of continuous functions $[0, 1] \to \mathbb{R}$.  Suppose $|| \cdot ||$ is a norm on $C[0, 1]$ with respect to which the evaluation functions $f \mapsto f(x), x \in [0, 1]$ are continuous.  Show that the topology induced by $|| \cdot ||$ is the same as the usual topology induced by the sup norm. It is straightforward to show that any sequence converging uniformly converges with respect to $|| \cdot ||$.  I am stuck on proving the converse; I cannot seem to figure out how to use the assumption that $|| \cdot ||$ is a norm. Edit:  Zhen Lin informs me that $|| \cdot ||$ is supposed to be complete.  That should make the problem statement true now!",,"['real-analysis', 'functional-analysis', 'banach-spaces']"
48,"Real-Analysis Methods to Evaluate $\int_0^\infty \frac{x^a}{1+x^2}\,dx$, $|a|<1$.","Real-Analysis Methods to Evaluate , .","\int_0^\infty \frac{x^a}{1+x^2}\,dx |a|<1","In THIS ANSWER , I used straightforward contour integration to evaluate the integral $$\bbox[5px,border:2px solid #C0A000]{\int_0^\infty \frac{x^a}{1+x^2}\,dx=\frac{\pi}{2}\sec\left(\frac{\pi a}{2}\right)}$$for $|a|<1$. An alternative approach is to enforce the substitution $x\to e^x$ to obtain $$\begin{align} \int_0^\infty \frac{x^a}{1+x^2}\,dx&=\int_{-\infty}^\infty \frac{e^{(a+1)x}}{1+e^{2x}}\,dx\\\\ &=\int_{-\infty}^0\frac{e^{(a+1)x}}{1+e^{2x}}\,dx+\int_{0}^\infty\frac{e^{(a-1)x}}{1+e^{-2x}}\,dx\\\\ &=\sum_{n=0}^\infty (-1)^n\left(\int_{-\infty}^0 e^{(2n+1+a)x}\,dx+\int_{0}^\infty e^{-(2n+1-a)x}\,dx\right)\\\\ &=\sum_{n=0}^\infty (-1)^n \left(\frac{1}{2n+1+a}+\frac{1}{2n+1-a}\right)\\\\ &=2\sum_{n=0}^\infty (-1)^n\left(\frac{2n+1}{(2n+1)^2-a^2}\right) \tag 1\\\\ &=\frac{\pi}{2}\sec\left(\frac{\pi a}{2}\right)\tag 2 \end{align}$$ Other possible ways forward include writing the integral of interest as $$\begin{align} \int_0^\infty \frac{x^a}{1+x^2}\,dx&=\int_{0}^1 \frac{x^{a}+x^{-a}}{1+x^2}\,dx \end{align}$$ and proceeding similarly, using $\frac{1}{1+x^2}=\sum_{n=0}^\infty (-1)^nx^{2n}$. Without appealing to complex analysis, what are other approaches one can use to evaluate this very standard integral? EDIT: Note that we can show that $(1)$ is the partial fraction representation of $(2)$ using Fourier series analysis.  I've included this development for completeness in the appendix of the solution I posted on THIS PAGE .","In THIS ANSWER , I used straightforward contour integration to evaluate the integral $$\bbox[5px,border:2px solid #C0A000]{\int_0^\infty \frac{x^a}{1+x^2}\,dx=\frac{\pi}{2}\sec\left(\frac{\pi a}{2}\right)}$$for $|a|<1$. An alternative approach is to enforce the substitution $x\to e^x$ to obtain $$\begin{align} \int_0^\infty \frac{x^a}{1+x^2}\,dx&=\int_{-\infty}^\infty \frac{e^{(a+1)x}}{1+e^{2x}}\,dx\\\\ &=\int_{-\infty}^0\frac{e^{(a+1)x}}{1+e^{2x}}\,dx+\int_{0}^\infty\frac{e^{(a-1)x}}{1+e^{-2x}}\,dx\\\\ &=\sum_{n=0}^\infty (-1)^n\left(\int_{-\infty}^0 e^{(2n+1+a)x}\,dx+\int_{0}^\infty e^{-(2n+1-a)x}\,dx\right)\\\\ &=\sum_{n=0}^\infty (-1)^n \left(\frac{1}{2n+1+a}+\frac{1}{2n+1-a}\right)\\\\ &=2\sum_{n=0}^\infty (-1)^n\left(\frac{2n+1}{(2n+1)^2-a^2}\right) \tag 1\\\\ &=\frac{\pi}{2}\sec\left(\frac{\pi a}{2}\right)\tag 2 \end{align}$$ Other possible ways forward include writing the integral of interest as $$\begin{align} \int_0^\infty \frac{x^a}{1+x^2}\,dx&=\int_{0}^1 \frac{x^{a}+x^{-a}}{1+x^2}\,dx \end{align}$$ and proceeding similarly, using $\frac{1}{1+x^2}=\sum_{n=0}^\infty (-1)^nx^{2n}$. Without appealing to complex analysis, what are other approaches one can use to evaluate this very standard integral? EDIT: Note that we can show that $(1)$ is the partial fraction representation of $(2)$ using Fourier series analysis.  I've included this development for completeness in the appendix of the solution I posted on THIS PAGE .",,"['real-analysis', 'definite-integrals']"
49,Does these arithmetic means of Pythagorean triangles converge?,Does these arithmetic means of Pythagorean triangles converge?,,"Primitive Pythagorean triplets $a^2 = b^2 + c^2, \gcd(b,c) = 1$ are given by $a = r^2 + s^2$ , $b = r^2 - s^2$ and $c = 2rs$ where $r > s$ are natural numbers. Let the $n$ -th primitive triplet be the one formed by the $n$ -th smallest pair in increasing order of $(r,s)$ . Claim 1 : Let $\mu_n$ be the arithmetic mean of the ratio of the perimeter to the hypotenuse of first $n$ primitive Pythagorean triplets; then, $$ \lim_{n \to \infty}\mu_n = \frac{\pi}{2} + \log 2$$ Claim 2 : Let $\mu_x$ be the arithmetic mean of the ratio of the perimeter to the hypotenuse of all primitive Pythagorean triplets in which no side exceeds $x$ ; then, $$ \lim_{x \to \infty}\mu_x = 1 + \frac{4}{\pi}$$ Update 8-Oct-2019 : Claim 2 has been proved in Mathoverflow . Data for claim 1 : From the plot of $\mu_n$ vs. $n$ for $n \le 5 \times 10^8$ we observe that $\mu_n$ is approaching a limiting value which is somwhere between $2.263942$ and $2.263944$ . The midpoint of the distribution of $\mu_n$ agrees with the above closed form to $6$ decimal places. Claim 2 has similar data. Question : Are these limits known if not, can it be proved or disproved? Sage code for claim 1 r   = 2 s   = 1 n   = sum = 0 max = 10^20 while(r <= max):     s = 1     while(s < r):         a = r^2 + s^2         b = r^2 - s^2         if(gcd(a,b) == 1):             c = 2*r*s             if(gcd(b,c) == 1):                 n = n + 1                 sum = sum + ((a+b+c)/a).n()                 if(n%10^5 == 0):                     print(n,sum/n)         s = s + 1     r = r + 1","Primitive Pythagorean triplets are given by , and where are natural numbers. Let the -th primitive triplet be the one formed by the -th smallest pair in increasing order of . Claim 1 : Let be the arithmetic mean of the ratio of the perimeter to the hypotenuse of first primitive Pythagorean triplets; then, Claim 2 : Let be the arithmetic mean of the ratio of the perimeter to the hypotenuse of all primitive Pythagorean triplets in which no side exceeds ; then, Update 8-Oct-2019 : Claim 2 has been proved in Mathoverflow . Data for claim 1 : From the plot of vs. for we observe that is approaching a limiting value which is somwhere between and . The midpoint of the distribution of agrees with the above closed form to decimal places. Claim 2 has similar data. Question : Are these limits known if not, can it be proved or disproved? Sage code for claim 1 r   = 2 s   = 1 n   = sum = 0 max = 10^20 while(r <= max):     s = 1     while(s < r):         a = r^2 + s^2         b = r^2 - s^2         if(gcd(a,b) == 1):             c = 2*r*s             if(gcd(b,c) == 1):                 n = n + 1                 sum = sum + ((a+b+c)/a).n()                 if(n%10^5 == 0):                     print(n,sum/n)         s = s + 1     r = r + 1","a^2 = b^2 + c^2, \gcd(b,c) = 1 a = r^2 + s^2 b = r^2 - s^2 c = 2rs r > s n n (r,s) \mu_n n  \lim_{n \to \infty}\mu_n = \frac{\pi}{2} + \log 2 \mu_x x  \lim_{x \to \infty}\mu_x = 1 + \frac{4}{\pi} \mu_n n n \le 5 \times 10^8 \mu_n 2.263942 2.263944 \mu_n 6","['real-analysis', 'geometry', 'number-theory', 'limits']"
50,A nice Combinatorial Identity,A nice Combinatorial Identity,,"I am trying to show that $\forall N\in\mathbb{N}$ , $$\sum\limits_{n=0}^{N}\sum\limits_{k=0}^{N}\frac{\left(-1\right)^{n+k}}{n+k+1}{N\choose n}{N\choose k}{N+n\choose n}{N+k\choose k}=\frac{1}{2N+1}$$ It's backed by numerical verifications, but I can't come up with a proof. So far, I tried using the generating function of $\left(\frac{1}{2N+1}\right)_{N\in\mathbb{N}}$ , which is $\frac{\arctan\left(\sqrt{x}\right)}{\sqrt{x}}$ , by showing that the LHS has the same generating function, but this calculation doesn't seem to lead me anywhere... Any suggestion ? Edit: the comment of bof (below this question) actually leads to a very simple proof. Indeed, from bof's comment we have that the LHS is equal to $$\int_{0}^{1}\left(\sum\limits_{k=0}^{N}(-1)^k{N\choose k}{N+k\choose k}x^k\right)^2dx$$ And we recognize here the shifted Legendre Polynomials $\widetilde{P_N}(x)=\displaystyle\sum\limits_{k=0}^{N}(-1)^k{N\choose k}{N+k\choose k}x^k$ . And we know that the shifted Legendre Polynomials form a family of orthogonal polynomials with respect to the inner product $\langle f|g\rangle=\displaystyle\int_{0}^{1}f(x)g(x)dx$ , and that their squared norm with respect to this product is $\langle\widetilde{P_n}|\widetilde{P_n}\rangle=\frac{1}{2n+1}$ ; so this basically provides the desired result immediately.","I am trying to show that , It's backed by numerical verifications, but I can't come up with a proof. So far, I tried using the generating function of , which is , by showing that the LHS has the same generating function, but this calculation doesn't seem to lead me anywhere... Any suggestion ? Edit: the comment of bof (below this question) actually leads to a very simple proof. Indeed, from bof's comment we have that the LHS is equal to And we recognize here the shifted Legendre Polynomials . And we know that the shifted Legendre Polynomials form a family of orthogonal polynomials with respect to the inner product , and that their squared norm with respect to this product is ; so this basically provides the desired result immediately.",\forall N\in\mathbb{N} \sum\limits_{n=0}^{N}\sum\limits_{k=0}^{N}\frac{\left(-1\right)^{n+k}}{n+k+1}{N\choose n}{N\choose k}{N+n\choose n}{N+k\choose k}=\frac{1}{2N+1} \left(\frac{1}{2N+1}\right)_{N\in\mathbb{N}} \frac{\arctan\left(\sqrt{x}\right)}{\sqrt{x}} \int_{0}^{1}\left(\sum\limits_{k=0}^{N}(-1)^k{N\choose k}{N+k\choose k}x^k\right)^2dx \widetilde{P_N}(x)=\displaystyle\sum\limits_{k=0}^{N}(-1)^k{N\choose k}{N+k\choose k}x^k \langle f|g\rangle=\displaystyle\int_{0}^{1}f(x)g(x)dx \langle\widetilde{P_n}|\widetilde{P_n}\rangle=\frac{1}{2n+1},"['real-analysis', 'combinatorics', 'summation']"
51,Prove that $ \sum\limits_{n=-\infty}^\infty\frac{\cos\pi\sqrt{n^2+1}}{3+4n^2}=\int\limits_{-\infty}^\infty\frac{\cos\pi\sqrt{x^2+1}}{3+4x^2}dx $?,Prove that ?, \sum\limits_{n=-\infty}^\infty\frac{\cos\pi\sqrt{n^2+1}}{3+4n^2}=\int\limits_{-\infty}^\infty\frac{\cos\pi\sqrt{x^2+1}}{3+4x^2}dx ,"How one can prove that the infinite sum of this function equals its integral $$ \sum_{n=-\infty}^\infty\frac{\cos\pi\sqrt{n^2+1}}{3+4n^2}=\int_{-\infty}^\infty\frac{\cos\pi\sqrt{x^2+1}}{3+4x^2}dx\ ? \tag{1} $$ My analysis: Mathematica wasn't able to return any closed form for the integral or the sum. Then I checked this relation by numerical computations and it agreed to about 20 decimal places. I know from this question Sum equals integral that the function $\text{sinc}\ x=\frac{\sin x}{x}$ has the same property $$ \int_{-\infty}^{+\infty} {\rm sinc}\, x \, dx = \sum_{n = -\infty}^{+\infty} {\rm sinc}\, n = \pi $$ I tried to find a closed form for the integral $(1)$ but couldn't. Motivation: I was challenged by a friend to prove this relation. I'm curious how one can prove it? Note: There has been a suggestion to straightforwardly apply Euler-MacLauren summation formula to prove this statement. Though I don't know why it can not be applied in this case, I checked numerically whether the sum equals the integral for the similar looking functions $f_1(x)=\frac{\cos\pi\sqrt{x^2+1}}{1+x^2}$ and $f_2(x)=\frac{\cos\pi\sqrt{x^2+1}}{2+x^2}$, but in both cases there was a difference of about 1% between the sum and the integral. In starck contrast to this, using the same algorithm for $\frac{\cos\pi\sqrt{x^2+1}}{3+4x^2}$ there wasn't any difference between the sum and the integral at least to 20 decimal places. So I think it is very unlikely that 1% error can be attributed to computational error.","How one can prove that the infinite sum of this function equals its integral $$ \sum_{n=-\infty}^\infty\frac{\cos\pi\sqrt{n^2+1}}{3+4n^2}=\int_{-\infty}^\infty\frac{\cos\pi\sqrt{x^2+1}}{3+4x^2}dx\ ? \tag{1} $$ My analysis: Mathematica wasn't able to return any closed form for the integral or the sum. Then I checked this relation by numerical computations and it agreed to about 20 decimal places. I know from this question Sum equals integral that the function $\text{sinc}\ x=\frac{\sin x}{x}$ has the same property $$ \int_{-\infty}^{+\infty} {\rm sinc}\, x \, dx = \sum_{n = -\infty}^{+\infty} {\rm sinc}\, n = \pi $$ I tried to find a closed form for the integral $(1)$ but couldn't. Motivation: I was challenged by a friend to prove this relation. I'm curious how one can prove it? Note: There has been a suggestion to straightforwardly apply Euler-MacLauren summation formula to prove this statement. Though I don't know why it can not be applied in this case, I checked numerically whether the sum equals the integral for the similar looking functions $f_1(x)=\frac{\cos\pi\sqrt{x^2+1}}{1+x^2}$ and $f_2(x)=\frac{\cos\pi\sqrt{x^2+1}}{2+x^2}$, but in both cases there was a difference of about 1% between the sum and the integral. In starck contrast to this, using the same algorithm for $\frac{\cos\pi\sqrt{x^2+1}}{3+4x^2}$ there wasn't any difference between the sum and the integral at least to 20 decimal places. So I think it is very unlikely that 1% error can be attributed to computational error.",,"['calculus', 'real-analysis']"
52,Showing that two definitions of $\limsup$ are equivalent,Showing that two definitions of  are equivalent,\limsup,"In Rudin, $\limsup$ is defined as follows: Let $S$ be the set of numbers $x$ (in the extended real number system) such that $s_{n_k}\to x$ for some subsequence $\{s_{n_k}\}$ . Then $$\limsup s_n = \sup S. \tag{1}$$ However, our real analysis instructor defined $\limsup$ in a different manner: $$\limsup s_n = \lim_{n \to \infty} \sup_{m \ge n} s_m. \tag{2}$$ I am having trouble understanding how these two definitions are equivalent. It would be very helpful to me if somebody could provide a proof with some explanation. My thoughts on the problem: I have noticed that the usual trend with these sort of proofs is to prove the upper bound $(1) \le (2)$ and then the lower bound $(1) \ge (2)$ to get the desired conclusion. However, I am unsure how to even begin.","In Rudin, is defined as follows: Let be the set of numbers (in the extended real number system) such that for some subsequence . Then However, our real analysis instructor defined in a different manner: I am having trouble understanding how these two definitions are equivalent. It would be very helpful to me if somebody could provide a proof with some explanation. My thoughts on the problem: I have noticed that the usual trend with these sort of proofs is to prove the upper bound and then the lower bound to get the desired conclusion. However, I am unsure how to even begin.",\limsup S x s_{n_k}\to x \{s_{n_k}\} \limsup s_n = \sup S. \tag{1} \limsup \limsup s_n = \lim_{n \to \infty} \sup_{m \ge n} s_m. \tag{2} (1) \le (2) (1) \ge (2),"['real-analysis', 'definition', 'limsup-and-liminf']"
53,Integral $\int_0^\pi \theta^2 \ln^2\big(2\cos\frac{\theta}{2}\big)d \theta$.,Integral .,\int_0^\pi \theta^2 \ln^2\big(2\cos\frac{\theta}{2}\big)d \theta,"I am trying to calculate $$ I=\frac{1}{\pi}\int_0^\pi \theta^2 \ln^2\big(2\cos\frac{\theta}{2}\big)d \theta=\frac{11\pi^4}{180}=\frac{11\zeta(4)}{2}. $$ Note, we can expand the log in the integral to obtain three interals, one trivial, the other 2 are not so easy, any ideas?  We will use $$ \left( \ln 2 +\ln \cos \frac{\theta}{2} \right)^2=\ln^2(2)+\ln^2\cos\frac{\theta}{2}+2\ln (2)\ln \cos\big(\frac{\theta}{2}\big) $$ and re-write I as $$ \pi I=\ln^2(2)\int_0^\pi \theta^2d\theta  +\int_0^\pi\theta^2 \ln^2 \cos \frac{\theta}{2}d\theta+2\ln 2 \int_0^\pi\theta^2 \ln \cos{\frac{\theta}{2}}d\theta. $$ Simplfying this further by using $y=\theta/2$ we obtain $$ \pi I=\frac{\pi^3\ln^2(2)}{3}+16\ln(2)\int_0^{\pi/2} y^2 \ln \cos (y) dy+8\int_0^{\pi/2} y^2 \ln^2 \cos (y) dy $$ Any Idea how to approach these two integrals? I know that  $$ \int_0^{\pi/2} \ln \cos y dy= \frac{-\pi\ln(2)}{2}\approx -1.08879 $$ but I am unsure how to use that here.  I do not think partial integration will work, Also the Riemann Zeta function is given by  $$ \zeta(4)=\sum_{n=1}^\infty \frac{1}{n^4}=\frac{\pi^4}{90}. $$ Thanks!","I am trying to calculate $$ I=\frac{1}{\pi}\int_0^\pi \theta^2 \ln^2\big(2\cos\frac{\theta}{2}\big)d \theta=\frac{11\pi^4}{180}=\frac{11\zeta(4)}{2}. $$ Note, we can expand the log in the integral to obtain three interals, one trivial, the other 2 are not so easy, any ideas?  We will use $$ \left( \ln 2 +\ln \cos \frac{\theta}{2} \right)^2=\ln^2(2)+\ln^2\cos\frac{\theta}{2}+2\ln (2)\ln \cos\big(\frac{\theta}{2}\big) $$ and re-write I as $$ \pi I=\ln^2(2)\int_0^\pi \theta^2d\theta  +\int_0^\pi\theta^2 \ln^2 \cos \frac{\theta}{2}d\theta+2\ln 2 \int_0^\pi\theta^2 \ln \cos{\frac{\theta}{2}}d\theta. $$ Simplfying this further by using $y=\theta/2$ we obtain $$ \pi I=\frac{\pi^3\ln^2(2)}{3}+16\ln(2)\int_0^{\pi/2} y^2 \ln \cos (y) dy+8\int_0^{\pi/2} y^2 \ln^2 \cos (y) dy $$ Any Idea how to approach these two integrals? I know that  $$ \int_0^{\pi/2} \ln \cos y dy= \frac{-\pi\ln(2)}{2}\approx -1.08879 $$ but I am unsure how to use that here.  I do not think partial integration will work, Also the Riemann Zeta function is given by  $$ \zeta(4)=\sum_{n=1}^\infty \frac{1}{n^4}=\frac{\pi^4}{90}. $$ Thanks!",,"['real-analysis', 'integration', 'definite-integrals', 'contest-math', 'contour-integration']"
54,Asymptotic (divergent) series,Asymptotic (divergent) series,,"MOTIVATION. After having read in detail an article by Alf van der Poorten  I read a very short paper by Roger Apéry. I am interested in finding a proof of a series expansion in the latter, which is in not given in it. So I assumed it should be stated or derived from a theorem on the subject . In Apéry, R., Irrationalité de $\zeta 2$ et $\zeta 3$, Société Mathématique de France, Astérisque 61 (1979) there is a divergent series expansion for a function I would like to understand. Here is my translation of the relevant part for this question (...) given a real sequence $a_{1},a_{2},\ldots ,a_{k}$, an analytic function $f\left( x\right) $ with respect to the variable   $\frac{1}{x}$ tending to $0$ with $\frac{1}{x}$ admits a (unique)   expansion in the form $$f\left( x\right) \equiv \sum_{k\geq 1}\frac{c_{k}}{\left( x+a_{1}\right) \left( x+a_{2}\right) \ldots  \left( x+a_{k}\right) }.\tag{A}$$ Added copy of the original: and the translation by Generic Human of the text after the formula: "" (We write ≡ instead of = to take into account the aversions of   mathematicians who, following Abel, Cauchy and d'Alembert, hold   divergent series to be an invention of the devil; in fact, we only   ever use a finite sum of terms, but the number of terms is an   unbounded function of x.) "" Remark . As far as I understand, based on this last text, the expansion of $f(x)$ in $(\mathrm{A})$ is in general   a divergent series and not a convergent one, but the existing answer [by WimC] seems to indicate the opposite. The corresponding finite sum appears and is proved in section 3 of Alfred van der Poorten's article A proof that Euler missed ... Apéry's proof of the irrationality of $\zeta (3)$ as For all $a_{1}$, $a_{2}$, $\dots$ $$ \sum_{k=1}^{K}\frac{a_{1}a_{2}\cdots a_{k-1}}{(x+a_{1})(x+a_{2})\cdots(x+a_{k})}= \frac{1}{x}-\frac{a_{1}a_{2}\cdots a_{K}}{x(x+a_{1})(x+a_{2})\cdots(x+a_{K})},$$   $$\tag{A'} $$ Questions: Is series $(A)$ indeed divergent? Which is the theorem stating or from which expansion $(\mathrm{A})$ can be derived? Could you please indicate a reference? I've posted on MathOverflow a variant of this question.","MOTIVATION. After having read in detail an article by Alf van der Poorten  I read a very short paper by Roger Apéry. I am interested in finding a proof of a series expansion in the latter, which is in not given in it. So I assumed it should be stated or derived from a theorem on the subject . In Apéry, R., Irrationalité de $\zeta 2$ et $\zeta 3$, Société Mathématique de France, Astérisque 61 (1979) there is a divergent series expansion for a function I would like to understand. Here is my translation of the relevant part for this question (...) given a real sequence $a_{1},a_{2},\ldots ,a_{k}$, an analytic function $f\left( x\right) $ with respect to the variable   $\frac{1}{x}$ tending to $0$ with $\frac{1}{x}$ admits a (unique)   expansion in the form $$f\left( x\right) \equiv \sum_{k\geq 1}\frac{c_{k}}{\left( x+a_{1}\right) \left( x+a_{2}\right) \ldots  \left( x+a_{k}\right) }.\tag{A}$$ Added copy of the original: and the translation by Generic Human of the text after the formula: "" (We write ≡ instead of = to take into account the aversions of   mathematicians who, following Abel, Cauchy and d'Alembert, hold   divergent series to be an invention of the devil; in fact, we only   ever use a finite sum of terms, but the number of terms is an   unbounded function of x.) "" Remark . As far as I understand, based on this last text, the expansion of $f(x)$ in $(\mathrm{A})$ is in general   a divergent series and not a convergent one, but the existing answer [by WimC] seems to indicate the opposite. The corresponding finite sum appears and is proved in section 3 of Alfred van der Poorten's article A proof that Euler missed ... Apéry's proof of the irrationality of $\zeta (3)$ as For all $a_{1}$, $a_{2}$, $\dots$ $$ \sum_{k=1}^{K}\frac{a_{1}a_{2}\cdots a_{k-1}}{(x+a_{1})(x+a_{2})\cdots(x+a_{k})}= \frac{1}{x}-\frac{a_{1}a_{2}\cdots a_{K}}{x(x+a_{1})(x+a_{2})\cdots(x+a_{K})},$$   $$\tag{A'} $$ Questions: Is series $(A)$ indeed divergent? Which is the theorem stating or from which expansion $(\mathrm{A})$ can be derived? Could you please indicate a reference? I've posted on MathOverflow a variant of this question.",,"['real-analysis', 'sequences-and-series', 'reference-request', 'divergent-series']"
55,"Is the image of a nowhere dense closed subset of $[0,1]$ under a differentiable map still nowhere dense?",Is the image of a nowhere dense closed subset of  under a differentiable map still nowhere dense?,"[0,1]","Let $f:[0,1]\to[0,1]$ be a continuous function such that its derivative $f'$ exists on $(0,1)$ . My question is: Q1. If $E\subset[0,1]$ is a nowhere dense closed subset, is $f(E)$ also nowhere dense in $[0,1]$ ? If the answer is negative, what will happen when we additionaly assume that $f'\ge 0$ on $(0,1)$ ? My question originates from the follow question: Q2. If $g:[0,1]\to[0,1]$ is the Cantor function , can we find homeomorphisms $\varphi$ and $\psi$ both from $[0,1]$ to itself, such that $\psi\circ g\circ \varphi$ is differntiable on $(0,1)$ ? If Q1 with the addtional assumption $f'\ge 0$ has a positive answer, then clearly it gives a negative answer to the second question, because for any $\varphi$ and $\psi$ , $f=\psi\circ g\circ \varphi$ maps a nowhere dense closed set onto $[0,1]$ . Otherwise, I am still interested in whether $\varphi$ and $\psi$ exist or not. Q2 comes from an attempt in providing a simple counter-example to this question for the case $X=Y=(0,1)$ . Update : Thanks to Henno Brandsma's comment below, I realized to add a remark that Q1 has a positive answer when $f$ is (piecewise) $C^1$ . Thanks to the discussion with Jim Belk, I realized that my original argument on Q1 under the assumption that $f$ is $C^1$ was incorrect. The following is a corrected argument. Denote the Lebesgue measure on $[0,1]$ by $|\cdot|$ and denote $C=\{x\in[0,1]:f'(x)=0\}$ . Note that $C$ is a closed. Using the fact that for every closed subset $K$ of $[0,1]$ , $$|f(K)|\le\int_K|f'(x)|dx,$$ or otherwise, we know that $f(C)$ is closed and $|f(C)|=0$ , so $f(C)$ , and hence $f(C\cap E)$ , are closed and nowhere dense. Note that $[0,1]\setminus C$ is a disjoint union of at most countably many intervals, say $[0,1]\setminus C=\sqcup_n I_n$ . Note that $f|_{\overline{I_n}}$ is homeomorphic, so $f(\overline{I_n}\cap E)$ is closed and nowhere dense. Then by Baire category theorem, $$f(E)=f(C\cap E)\cup\big(\cup_n f(\overline{I_n}\cap E)\big)$$ is nowhere dense. Moreover, I removed another question similar to Q1 in this post, and  started a new post for it. Any hint or suggestion is appreciated. Thanks in advance.","Let be a continuous function such that its derivative exists on . My question is: Q1. If is a nowhere dense closed subset, is also nowhere dense in ? If the answer is negative, what will happen when we additionaly assume that on ? My question originates from the follow question: Q2. If is the Cantor function , can we find homeomorphisms and both from to itself, such that is differntiable on ? If Q1 with the addtional assumption has a positive answer, then clearly it gives a negative answer to the second question, because for any and , maps a nowhere dense closed set onto . Otherwise, I am still interested in whether and exist or not. Q2 comes from an attempt in providing a simple counter-example to this question for the case . Update : Thanks to Henno Brandsma's comment below, I realized to add a remark that Q1 has a positive answer when is (piecewise) . Thanks to the discussion with Jim Belk, I realized that my original argument on Q1 under the assumption that is was incorrect. The following is a corrected argument. Denote the Lebesgue measure on by and denote . Note that is a closed. Using the fact that for every closed subset of , or otherwise, we know that is closed and , so , and hence , are closed and nowhere dense. Note that is a disjoint union of at most countably many intervals, say . Note that is homeomorphic, so is closed and nowhere dense. Then by Baire category theorem, is nowhere dense. Moreover, I removed another question similar to Q1 in this post, and  started a new post for it. Any hint or suggestion is appreciated. Thanks in advance.","f:[0,1]\to[0,1] f' (0,1) E\subset[0,1] f(E) [0,1] f'\ge 0 (0,1) g:[0,1]\to[0,1] \varphi \psi [0,1] \psi\circ g\circ \varphi (0,1) f'\ge 0 \varphi \psi f=\psi\circ g\circ \varphi [0,1] \varphi \psi X=Y=(0,1) f C^1 f C^1 [0,1] |\cdot| C=\{x\in[0,1]:f'(x)=0\} C K [0,1] |f(K)|\le\int_K|f'(x)|dx, f(C) |f(C)|=0 f(C) f(C\cap E) [0,1]\setminus C [0,1]\setminus C=\sqcup_n I_n f|_{\overline{I_n}} f(\overline{I_n}\cap E) f(E)=f(C\cap E)\cup\big(\cup_n f(\overline{I_n}\cap E)\big)","['real-analysis', 'general-topology']"
56,Prove that $\sqrt{a^2+3b^2}+\sqrt{b^2+3c^2}+\sqrt{c^2+3a^2}\geq6$ if $(a+b+c)^2(a^2+b^2+c^2)=27$,Prove that  if,\sqrt{a^2+3b^2}+\sqrt{b^2+3c^2}+\sqrt{c^2+3a^2}\geq6 (a+b+c)^2(a^2+b^2+c^2)=27,"Let $a$, $b$ and $c$ be non-negative numbers such that $(a+b+c)^2(a^2+b^2+c^2)=27$. Prove that: $$\sqrt{a^2+3b^2}+\sqrt{b^2+3c^2}+\sqrt{c^2+3a^2}\geq6$$ A big problem here around $(a,b,c)=(1.6185...,0.71686...,0.4926...)$. In this case we get $\sum\limits_{cyc}\sqrt{a^2+3b^2}-6=0.000563...$. My trying. Let $a^2+3b^2=4x^2$, $b^2+3c^2=4y^2$ and $c^2+3a^2=4z^2$, where $x$, $y$ and $z$ are non-negatives. Hence, we need to prove that $$\sum\limits_{cyc}\sqrt{x^2-3y^2+9z^2}\leq\frac{\sqrt7(x+y+z)^2}{\sqrt{3(x^2+y^2+z^2)}}$$ Let $k$ and $m$ be non-negatives, for which $x-ky+mz>0$, $y-kz+mx>0$, $z-kx+my>0$ and $1-k+m>0$. By C-S $\left(\sum\limits_{cyc}\sqrt{x^2-3y^2+9z^2}\right)^2\leq(1-k+m)(x+y+z)\sum\limits_{cyc}\frac{x^2-3y^2+9z^2}{x-ky+mz}$. Thus, it remains to prove that $$(1-k+m)\sum\limits_{cyc}\frac{x^2-3y^2+9z^2}{x-ky+mz}\leq\frac{7(x+y+z)^3}{3(x^2+y^2+z^2)}$$ It's a sixth degree, but I didn't find a values of $k$ and $m$, such that the last inequality will be true. By this way we can prove that $\sum\limits_{cyc}\sqrt{a^2+2b^2}\geq3\sqrt3$ is true, but it's not comforting. Also I tried to use Holder, but without success. Thank you!","Let $a$, $b$ and $c$ be non-negative numbers such that $(a+b+c)^2(a^2+b^2+c^2)=27$. Prove that: $$\sqrt{a^2+3b^2}+\sqrt{b^2+3c^2}+\sqrt{c^2+3a^2}\geq6$$ A big problem here around $(a,b,c)=(1.6185...,0.71686...,0.4926...)$. In this case we get $\sum\limits_{cyc}\sqrt{a^2+3b^2}-6=0.000563...$. My trying. Let $a^2+3b^2=4x^2$, $b^2+3c^2=4y^2$ and $c^2+3a^2=4z^2$, where $x$, $y$ and $z$ are non-negatives. Hence, we need to prove that $$\sum\limits_{cyc}\sqrt{x^2-3y^2+9z^2}\leq\frac{\sqrt7(x+y+z)^2}{\sqrt{3(x^2+y^2+z^2)}}$$ Let $k$ and $m$ be non-negatives, for which $x-ky+mz>0$, $y-kz+mx>0$, $z-kx+my>0$ and $1-k+m>0$. By C-S $\left(\sum\limits_{cyc}\sqrt{x^2-3y^2+9z^2}\right)^2\leq(1-k+m)(x+y+z)\sum\limits_{cyc}\frac{x^2-3y^2+9z^2}{x-ky+mz}$. Thus, it remains to prove that $$(1-k+m)\sum\limits_{cyc}\frac{x^2-3y^2+9z^2}{x-ky+mz}\leq\frac{7(x+y+z)^3}{3(x^2+y^2+z^2)}$$ It's a sixth degree, but I didn't find a values of $k$ and $m$, such that the last inequality will be true. By this way we can prove that $\sum\limits_{cyc}\sqrt{a^2+2b^2}\geq3\sqrt3$ is true, but it's not comforting. Also I tried to use Holder, but without success. Thank you!",,"['real-analysis', 'inequality', 'contest-math']"
57,Prove $|e^{i\theta} -1| \leq |\theta|$,Prove,|e^{i\theta} -1| \leq |\theta|,"Could you help me to prove  $$ |e^{i\theta} -1| \leq |\theta| $$ I am studying the proof of differentiability of Fourier Series, and my book used this lemma. How does it work?","Could you help me to prove  $$ |e^{i\theta} -1| \leq |\theta| $$ I am studying the proof of differentiability of Fourier Series, and my book used this lemma. How does it work?",,"['calculus', 'real-analysis', 'inequality', 'complex-numbers', 'exponential-function']"
58,Show that a continuous function has a fixed point,Show that a continuous function has a fixed point,,"Question: Let $a, b \in \mathbb{R}$ with $a < b$ and let $f: [a,b] \rightarrow [a,b]$ continuous. Show: $f$ has a fixed point, that is, there is an $x \in [a,b]$ with $f(x)=x$. I suppose this has to do with the basic definition of continuity. The definition I am using is that $f$ is continuous at $a$ if $\displaystyle \lim_{x \to a} f(x)$ exists and if $\displaystyle \lim_{x \to a} f(x) = f(a)$. I must not be understanding it, since I am not sure how to begin showing this... Should I be trying to show that $x$ is both greater than or equal to and less than or equal to $\displaystyle \lim_{x \to a} f(x)$ ?","Question: Let $a, b \in \mathbb{R}$ with $a < b$ and let $f: [a,b] \rightarrow [a,b]$ continuous. Show: $f$ has a fixed point, that is, there is an $x \in [a,b]$ with $f(x)=x$. I suppose this has to do with the basic definition of continuity. The definition I am using is that $f$ is continuous at $a$ if $\displaystyle \lim_{x \to a} f(x)$ exists and if $\displaystyle \lim_{x \to a} f(x) = f(a)$. I must not be understanding it, since I am not sure how to begin showing this... Should I be trying to show that $x$ is both greater than or equal to and less than or equal to $\displaystyle \lim_{x \to a} f(x)$ ?",,"['real-analysis', 'analysis', 'continuity', 'fixed-point-theorems', 'fixed-points']"
59,Hardy's Inequality for Integrals,Hardy's Inequality for Integrals,,"I am trying to prove Hardy's Inequality for integrals: Let $f:(0,\infty) \rightarrow \mathbb R$ be in $L^p$. Let $F(x) = \frac{1}{x} \int_0 ^x f(t)dt$. Then $F \in L^p$ and $\|F\|_p \leq \frac{p}{p-1} \|f\|_p$. I have seen proofs of this which use Haar measure and Fourier analysis, eg. here An inequality by Hardy . However, I am wondering if there is a more elementary proof. I have tried to deduce this using Jensen's inequality and Fubini's Theorem as follows: Jensen's inequality implies that $\left|\frac{1}{x} \int_0 ^x f(t)dt)\right|^p \leq \frac{1}{x} \int_0 ^x |f(t)|^p dt$. Therefore $$\int_0 ^\infty |F(x)|^p dx \leq \int_0 ^\infty \frac{1}{x} \int_0 ^x |f(t)|^p dtdx = \int_0 ^\infty \int_t ^\infty \frac{1}{x} |f(t)|^p dx dt.$$ But, this last integral is infinite, and I'm not sure how to get a sharper bound on $|F(x)|^p$. Any suggestions?","I am trying to prove Hardy's Inequality for integrals: Let $f:(0,\infty) \rightarrow \mathbb R$ be in $L^p$. Let $F(x) = \frac{1}{x} \int_0 ^x f(t)dt$. Then $F \in L^p$ and $\|F\|_p \leq \frac{p}{p-1} \|f\|_p$. I have seen proofs of this which use Haar measure and Fourier analysis, eg. here An inequality by Hardy . However, I am wondering if there is a more elementary proof. I have tried to deduce this using Jensen's inequality and Fubini's Theorem as follows: Jensen's inequality implies that $\left|\frac{1}{x} \int_0 ^x f(t)dt)\right|^p \leq \frac{1}{x} \int_0 ^x |f(t)|^p dt$. Therefore $$\int_0 ^\infty |F(x)|^p dx \leq \int_0 ^\infty \frac{1}{x} \int_0 ^x |f(t)|^p dtdx = \int_0 ^\infty \int_t ^\infty \frac{1}{x} |f(t)|^p dx dt.$$ But, this last integral is infinite, and I'm not sure how to get a sharper bound on $|F(x)|^p$. Any suggestions?",,"['real-analysis', 'inequality']"
60,Need to prove the sequence $a_n=1+\frac{1}{2^2}+\frac{1}{3^2}+\cdots+\frac{1}{n^2}$ converges,Need to prove the sequence  converges,a_n=1+\frac{1}{2^2}+\frac{1}{3^2}+\cdots+\frac{1}{n^2},"I need to prove that the sequence $a_n=1+\frac{1}{2^2}+\frac{1}{3^2}+\cdots+\frac{1}{n^2}$ converges. I do not have to find the limit. I have tried to prove it by proving that the sequence is monotone and bounded, but I am having some trouble: Monotonic: The sequence seems to be monotone and increasing. This can be proved by induction: Claim that $a_n\leq a_{n+1}$ $$a_1=1\leq 1+\frac{1}{2^2}=a_2$$ Need to show that $a_{n+1}\leq a_{n+2}$ $$a_{n+1}=1+\frac{1}{2^2}+\frac{1}{3^2}+\cdots+\frac{1}{n^2}+\frac{1}{(n+1)^2}\leq 1+\frac{1}{2^2}+\frac{1}{3^2}+\cdots+\frac{1}{n^2}+\frac{1}{(n+1)^2}+\frac{1}{(n+2)^2}=a_{n+2}$$  Thus the sequence is monotone and increasing. Boundedness: Since the sequence is increasing it is bounded below by $a_1=1$. Upper bound is where I am having trouble. All the examples I have dealt with in class have to do with decreasing functions, but I don't know what my thinking process should be to find an upper bound. Can anyone enlighten me as to how I should approach this, and can anyone confirm my work thus far? Also, although I prove this using monotonicity and boundedness, could I have approached this by showing the sequence was a Cauchy sequence? Thanks so much in advance!","I need to prove that the sequence $a_n=1+\frac{1}{2^2}+\frac{1}{3^2}+\cdots+\frac{1}{n^2}$ converges. I do not have to find the limit. I have tried to prove it by proving that the sequence is monotone and bounded, but I am having some trouble: Monotonic: The sequence seems to be monotone and increasing. This can be proved by induction: Claim that $a_n\leq a_{n+1}$ $$a_1=1\leq 1+\frac{1}{2^2}=a_2$$ Need to show that $a_{n+1}\leq a_{n+2}$ $$a_{n+1}=1+\frac{1}{2^2}+\frac{1}{3^2}+\cdots+\frac{1}{n^2}+\frac{1}{(n+1)^2}\leq 1+\frac{1}{2^2}+\frac{1}{3^2}+\cdots+\frac{1}{n^2}+\frac{1}{(n+1)^2}+\frac{1}{(n+2)^2}=a_{n+2}$$  Thus the sequence is monotone and increasing. Boundedness: Since the sequence is increasing it is bounded below by $a_1=1$. Upper bound is where I am having trouble. All the examples I have dealt with in class have to do with decreasing functions, but I don't know what my thinking process should be to find an upper bound. Can anyone enlighten me as to how I should approach this, and can anyone confirm my work thus far? Also, although I prove this using monotonicity and boundedness, could I have approached this by showing the sequence was a Cauchy sequence? Thanks so much in advance!",,"['real-analysis', 'sequences-and-series', 'convergence-divergence']"
61,Can the graph of a bounded function ever have an unbounded derivative?,Can the graph of a bounded function ever have an unbounded derivative?,,"Can the graph of a bounded function ever have an unbounded derivative? I want to know if $f$ has bounded variation then its derivative is bounded. The converse is obvious. I think the answer is ""yes"". If the graph were to have an unbounded derivative, it would coincide with a vertical line.","Can the graph of a bounded function ever have an unbounded derivative? I want to know if $f$ has bounded variation then its derivative is bounded. The converse is obvious. I think the answer is ""yes"". If the graph were to have an unbounded derivative, it would coincide with a vertical line.",,"['real-analysis', 'bounded-variation']"
62,"Why $ \int_0^{2\pi}\frac{\cos t - r}{1 - 2r\cos t + r^2}\,dt=0$ for $r\in[0,1)$",Why  for," \int_0^{2\pi}\frac{\cos t - r}{1 - 2r\cos t + r^2}\,dt=0 r\in[0,1)","Define $$ I(r) = \int_0^{2\pi}\frac{\cos t- r}{1 - 2r\cos t + r^2}\,dt $$ over $r\in [0,1)$ . Numerical experiments suggest $I(r) = 0$ for all $r\in [0,1)$ . But I can't show this analytically. This integral appears when computing the Cauchy transform of $\overline z$ over a unit circle. The latter, therefore, seems to be constantly zero.","Define over . Numerical experiments suggest for all . But I can't show this analytically. This integral appears when computing the Cauchy transform of over a unit circle. The latter, therefore, seems to be constantly zero.","
I(r) = \int_0^{2\pi}\frac{\cos t- r}{1 - 2r\cos t + r^2}\,dt
 r\in [0,1) I(r) = 0 r\in [0,1) \overline z","['real-analysis', 'integration', 'definite-integrals', 'trigonometric-integrals']"
63,"""Honest"" introductory real analysis book","""Honest"" introductory real analysis book",,"I was asked if I could suggest an ""honest"" introductory real analysis book, where ""honest"" means: with every single theorem proved (that is, no ""left to the reader"" or ""you can easily see""); with every single problem properly solved (that is, solved in a formal (exam-like) way). I've studied using Rudin mostly and I liked it, but it really doesn't fit the description, so I don't know what book I should suggest. Do you have any recommendations? Update: I need to clarify that my friend has just started to study real analysis and the course starts from the very basics, deals with real valued functions of one variable, but introduces topological concepts and metric spaces too.","I was asked if I could suggest an ""honest"" introductory real analysis book, where ""honest"" means: with every single theorem proved (that is, no ""left to the reader"" or ""you can easily see""); with every single problem properly solved (that is, solved in a formal (exam-like) way). I've studied using Rudin mostly and I liked it, but it really doesn't fit the description, so I don't know what book I should suggest. Do you have any recommendations? Update: I need to clarify that my friend has just started to study real analysis and the course starts from the very basics, deals with real valued functions of one variable, but introduces topological concepts and metric spaces too.",,"['real-analysis', 'reference-request', 'soft-question', 'big-list', 'book-recommendation']"
64,"What's the ""limit"" in the definition of Riemann integrals?","What's the ""limit"" in the definition of Riemann integrals?",,"Consider one of the standard methods used for defining the Riemann integrals : Suppose $\sigma$ denotes any subdivision $a=x_0<x_1<x_2\cdots<x_{n-1}<x_n=b$, and let $x_{i-1}\leq \xi_i\leq x_i$. Then if    $$|\sigma|:=\max\{x_i-x_{i-1}|i=1,\cdots,n\},$$   which we shall call the norm of the subdivision, we define:   $$\int_a^bf(x)dx:=\lim_{|\sigma|\to 0}\sum_{i=1}^nf(\xi_i)(x_i-x_{i-1}).$$ When one talks about the limit of a function $\lim_{x\to x_0}f(x)$, one has exactly one value $f(x)$ for every $x$. However, for every $|\sigma|$, the value of the Riemann sum $\sum_{i=1}^nf(\xi_i)(x_i-x_{i-1})$ is not necessarily unique. Using the $\epsilon$-$\delta$ language, one may restate the definition as follows: Suppose $f:[a,b]\to{\mathbb R}$, $J\in{\mathbb R}$. If for all $\epsilon>0$, there exists $\delta>0$ such that for any subdivision $\sigma$ and $\{\xi_i\}$ on $\sigma$ (i.e. $x_{i-1}\leq \xi_i\leq x_i$), $|\sigma|<\delta$ implies   $$|\sum_{i=1}^nf(\xi_i)\Delta x_i-J|<\epsilon,$$   we call $J$ is the Riemann integral of $f$ on $[a,b]$ and denote   $$J=\int_a^bf(x)dx.$$ Here are my questions : How should I understand this kind of limit? It seems that this is not the ""limit of a function"" I learned in elementary real analysis. Where does it appear in mathematics besides the definition of Riemann integrals?","Consider one of the standard methods used for defining the Riemann integrals : Suppose $\sigma$ denotes any subdivision $a=x_0<x_1<x_2\cdots<x_{n-1}<x_n=b$, and let $x_{i-1}\leq \xi_i\leq x_i$. Then if    $$|\sigma|:=\max\{x_i-x_{i-1}|i=1,\cdots,n\},$$   which we shall call the norm of the subdivision, we define:   $$\int_a^bf(x)dx:=\lim_{|\sigma|\to 0}\sum_{i=1}^nf(\xi_i)(x_i-x_{i-1}).$$ When one talks about the limit of a function $\lim_{x\to x_0}f(x)$, one has exactly one value $f(x)$ for every $x$. However, for every $|\sigma|$, the value of the Riemann sum $\sum_{i=1}^nf(\xi_i)(x_i-x_{i-1})$ is not necessarily unique. Using the $\epsilon$-$\delta$ language, one may restate the definition as follows: Suppose $f:[a,b]\to{\mathbb R}$, $J\in{\mathbb R}$. If for all $\epsilon>0$, there exists $\delta>0$ such that for any subdivision $\sigma$ and $\{\xi_i\}$ on $\sigma$ (i.e. $x_{i-1}\leq \xi_i\leq x_i$), $|\sigma|<\delta$ implies   $$|\sum_{i=1}^nf(\xi_i)\Delta x_i-J|<\epsilon,$$   we call $J$ is the Riemann integral of $f$ on $[a,b]$ and denote   $$J=\int_a^bf(x)dx.$$ Here are my questions : How should I understand this kind of limit? It seems that this is not the ""limit of a function"" I learned in elementary real analysis. Where does it appear in mathematics besides the definition of Riemann integrals?",,"['real-analysis', 'integration']"
65,Example to show the distance between two closed sets can be 0 even if the two sets are disjoint [duplicate],Example to show the distance between two closed sets can be 0 even if the two sets are disjoint [duplicate],,"This question already has answers here : Can distance between two closed sets be zero? [duplicate] (3 answers) Closed 6 years ago . Let $A$ and $B$ be two sets of real numbers. Define the distance from $A$ to $B$ by $$\rho (A,B) = \inf \{ |a-b| : a \in A, b \in B\} \;.$$ Give an example to show that the distance between two closed sets can be $0$ even if the two sets are disjoint.","This question already has answers here : Can distance between two closed sets be zero? [duplicate] (3 answers) Closed 6 years ago . Let $A$ and $B$ be two sets of real numbers. Define the distance from $A$ to $B$ by $$\rho (A,B) = \inf \{ |a-b| : a \in A, b \in B\} \;.$$ Give an example to show that the distance between two closed sets can be $0$ even if the two sets are disjoint.",,"['real-analysis', 'general-topology']"
66,"What kind of ""mathematical object"" are limits?","What kind of ""mathematical object"" are limits?",,"When learning mathematics I tend to try to reduce all the concepts I come across to some matter of interaction between sets and functions (or if necessary the more general Relation) on them. Possibly with some extra axioms thrown in here and there if needed, but the fundamental idea is that of adding additional structure on sets and relations between them. I've recently tried applying this view to calculus and have been running into some confusions. Most importantly I'm not sure how to interpret Limits. I've considered viewing them as a function that takes 3 arguments, a function, the function's domain and some value (the ""approaches value"") then outputs a single value. However this ""limit function"" view requires defining the limit function over something other then the Reals or Complexes due to the notion of certain inputs and outputs being ""infinity"". This makes me uncomfortable and question whether my current approach to mathematics is really as elegant as I'd thought. Is this a reasonable approach to answering the question of what limits actually ""are"" in a general mathematical sense? How do mathematicians tend to categorize limits with the rest of mathematics?","When learning mathematics I tend to try to reduce all the concepts I come across to some matter of interaction between sets and functions (or if necessary the more general Relation) on them. Possibly with some extra axioms thrown in here and there if needed, but the fundamental idea is that of adding additional structure on sets and relations between them. I've recently tried applying this view to calculus and have been running into some confusions. Most importantly I'm not sure how to interpret Limits. I've considered viewing them as a function that takes 3 arguments, a function, the function's domain and some value (the ""approaches value"") then outputs a single value. However this ""limit function"" view requires defining the limit function over something other then the Reals or Complexes due to the notion of certain inputs and outputs being ""infinity"". This makes me uncomfortable and question whether my current approach to mathematics is really as elegant as I'd thought. Is this a reasonable approach to answering the question of what limits actually ""are"" in a general mathematical sense? How do mathematicians tend to categorize limits with the rest of mathematics?",,"['calculus', 'real-analysis', 'analysis', 'functions', 'limits']"
67,How do I stop overcomplicating proofs?,How do I stop overcomplicating proofs?,,"I'm a third year student majoring in Math. Whenever I sit down and try to prove something, I just don't know what and where to start with. The first proofs course I took was graded very strictly so missing a very tiny detail made me lose a lot of marks (which does make sense since it is an introductory class to proofs and the ""little details"" could have been not ""little""). But after that, I just get way too anxious when I do proofs because I don't know what kind of detail I would be missing. I end up completing the proofs by getting a lot of hints on where to start, and it takes way too much time for me to do a single proof (almost 2-3 days per one theorem). And because I don't want to get the proofs wrong, I keep searching up resources to do the proofs; so I kind of end up not doing the proofs myself. But when I see the ""solutions"" to the proofs, I realize they were very simple and I have been over-complicating it a lot. I really love math and I want to be able to really understand courses like Real Analysis, and how scared I am with proofs definitely is an issue that I want to overcome. So my question is (i) If you have gone through this stage, how did you overcome? (ii) Are there any general tips on starting proofs? Thanks.","I'm a third year student majoring in Math. Whenever I sit down and try to prove something, I just don't know what and where to start with. The first proofs course I took was graded very strictly so missing a very tiny detail made me lose a lot of marks (which does make sense since it is an introductory class to proofs and the ""little details"" could have been not ""little""). But after that, I just get way too anxious when I do proofs because I don't know what kind of detail I would be missing. I end up completing the proofs by getting a lot of hints on where to start, and it takes way too much time for me to do a single proof (almost 2-3 days per one theorem). And because I don't want to get the proofs wrong, I keep searching up resources to do the proofs; so I kind of end up not doing the proofs myself. But when I see the ""solutions"" to the proofs, I realize they were very simple and I have been over-complicating it a lot. I really love math and I want to be able to really understand courses like Real Analysis, and how scared I am with proofs definitely is an issue that I want to overcome. So my question is (i) If you have gone through this stage, how did you overcome? (ii) Are there any general tips on starting proofs? Thanks.",,"['real-analysis', 'proof-writing', 'soft-question']"
68,Why does the sum of the reciprocals of factorials converge to $e$?,Why does the sum of the reciprocals of factorials converge to ?,e,"I've been asked by some schoolmates why we have $$ \sum_{n=0}^\infty \frac{1}{n!}=e.$$ I couldn't say much besides that the $\Gamma$ function, analytic continuation of the factorial, is defined with an integral involving $e$. Then I also know that actually $$ \sum_{n=0}^\infty \frac{x^n}{n!}=e^x.$$ Is there a reason for these facts? P.S. I added the tag ""intuition"", please remove it if you think it is not pertinent.","I've been asked by some schoolmates why we have $$ \sum_{n=0}^\infty \frac{1}{n!}=e.$$ I couldn't say much besides that the $\Gamma$ function, analytic continuation of the factorial, is defined with an integral involving $e$. Then I also know that actually $$ \sum_{n=0}^\infty \frac{x^n}{n!}=e^x.$$ Is there a reason for these facts? P.S. I added the tag ""intuition"", please remove it if you think it is not pertinent.",,"['real-analysis', 'sequences-and-series', 'intuition', 'exponential-function', 'factorial']"
69,Cauchy product of two absolutely convergent series is absolutely convergent. (Rudin PMA Ch. 3 ex 13),Cauchy product of two absolutely convergent series is absolutely convergent. (Rudin PMA Ch. 3 ex 13),,"Given  $\sum a_n$ and $\sum b_n$ we define the Cauchy product to be $\sum_{k = 0}^{n}a_kb_{n-k}$.  I need to prove that if both $\sum a_n$ and $\sum b_n$ are absolutely convergent then so is the Cauchy product. Unless I'm missing something the proof seems trivial to me.  Theorem 3.50 in the book ""baby Rudin"" states that given two convergent series at least one of which is absolutely convergent the Cauchy product will converge to the product of the limits.  So using this why not just say the following? Since $\sum_{n=0}^\infty |a_n|$ is convergent it is also absolutely convergent.  Therefore, by the theorem 3.50, $$\sum_{n=0}^\infty \sum_{k=0}^{n}|a_kb_{n-k}| = \sum_{n=0}^\infty |a_n| \sum_{n=0}^\infty |b_n|$$","Given  $\sum a_n$ and $\sum b_n$ we define the Cauchy product to be $\sum_{k = 0}^{n}a_kb_{n-k}$.  I need to prove that if both $\sum a_n$ and $\sum b_n$ are absolutely convergent then so is the Cauchy product. Unless I'm missing something the proof seems trivial to me.  Theorem 3.50 in the book ""baby Rudin"" states that given two convergent series at least one of which is absolutely convergent the Cauchy product will converge to the product of the limits.  So using this why not just say the following? Since $\sum_{n=0}^\infty |a_n|$ is convergent it is also absolutely convergent.  Therefore, by the theorem 3.50, $$\sum_{n=0}^\infty \sum_{k=0}^{n}|a_kb_{n-k}| = \sum_{n=0}^\infty |a_n| \sum_{n=0}^\infty |b_n|$$",,"['real-analysis', 'sequences-and-series']"
70,Is the set of real numbers the largest possible totally ordered set?,Is the set of real numbers the largest possible totally ordered set?,,"Because I find any totally ordered set can be ""lined up"" in a straight line, I'm guessing that the set of all the real numbers is the biggest totally ordered set possible. In the sense that any other totally ordered set is isomorphic (order-preserving) to a subset of real numbers. Is that right?","Because I find any totally ordered set can be ""lined up"" in a straight line, I'm guessing that the set of all the real numbers is the biggest totally ordered set possible. In the sense that any other totally ordered set is isomorphic (order-preserving) to a subset of real numbers. Is that right?",,"['real-analysis', 'order-theory']"
71,Problem in an inductive proof of Brouwer's fixed point theorem.,Problem in an inductive proof of Brouwer's fixed point theorem.,,"I've been reading this article that proposes an inductive proof of Brouwer's fixed point theorem just using ""basic"" topology, avoiding the usage of homotopy groups as the usual proof goes. To do so we assume that given $n \in \mathbb{N}$ any continuous function from $C = [0,1]^n$ to $C$ has a fixed point and we prove that any continuous function $f$ from $C\times[0,1]$ to $C \times [0,1]$ has a fixed point (the base case for $n = 1$ is already proven by the intermediate value theorem). We write $f = (f_1,\dots,f_{n+1})$ where each $f_i:C\times [0,1] \rightarrow [0,1]$ is a continous function. Next it defines $\phi = (f_1,\dots,f_n)$ which is clearly a continuous function and we write $\phi_t: C \rightarrow C$ as the function $\phi_t(x) = \phi(x,t)$ , which clearly is continuous. Next by induction hypothesis we note that each $\phi_t$ has a fixed point, with this in mind we define $$X = \{(x,t) \in C \times [0,1]: \phi_t(x) = x\} = (\phi-id)^{-1}(\{0\})$$ And because this set is the preimage of a closed set under a continuous function, it's closed and nonempty because by induction each $\phi_t$ has at least one fixed point. However here's where my problem with the proof comes because next it takes $\pi_2: C\times [0,1] \rightarrow [0,1]$ as the projection onto the last coordinate. We note that $\pi_2(X) = [0,1]$ as there's at least one fixed point for each $\phi_t$ , but the problem that because clearly $\pi_2$ is an open map the proof on the article asumes that $\pi_2|_X$ is also an open map. However, restrictions of open maps are not always open, so is the article wrong or what am I missing? I've tried to prove that this map is in fact open, but I can't seem to do so, and in fact it looks that it might not be open which would prove that the article is wrong, so how do you prove that in fact this restriction is open? Or what would be a counterexample of a function $f$ such that the set we defined as $X$ is such that the restriction of $\pi_2$ isn't open?","I've been reading this article that proposes an inductive proof of Brouwer's fixed point theorem just using ""basic"" topology, avoiding the usage of homotopy groups as the usual proof goes. To do so we assume that given any continuous function from to has a fixed point and we prove that any continuous function from to has a fixed point (the base case for is already proven by the intermediate value theorem). We write where each is a continous function. Next it defines which is clearly a continuous function and we write as the function , which clearly is continuous. Next by induction hypothesis we note that each has a fixed point, with this in mind we define And because this set is the preimage of a closed set under a continuous function, it's closed and nonempty because by induction each has at least one fixed point. However here's where my problem with the proof comes because next it takes as the projection onto the last coordinate. We note that as there's at least one fixed point for each , but the problem that because clearly is an open map the proof on the article asumes that is also an open map. However, restrictions of open maps are not always open, so is the article wrong or what am I missing? I've tried to prove that this map is in fact open, but I can't seem to do so, and in fact it looks that it might not be open which would prove that the article is wrong, so how do you prove that in fact this restriction is open? Or what would be a counterexample of a function such that the set we defined as is such that the restriction of isn't open?","n \in \mathbb{N} C = [0,1]^n C f C\times[0,1] C \times [0,1] n = 1 f = (f_1,\dots,f_{n+1}) f_i:C\times [0,1] \rightarrow [0,1] \phi = (f_1,\dots,f_n) \phi_t: C \rightarrow C \phi_t(x) = \phi(x,t) \phi_t X = \{(x,t) \in C \times [0,1]: \phi_t(x) = x\} = (\phi-id)^{-1}(\{0\}) \phi_t \pi_2: C\times [0,1] \rightarrow [0,1] \pi_2(X) = [0,1] \phi_t \pi_2 \pi_2|_X f X \pi_2","['real-analysis', 'general-topology', 'algebraic-topology', 'solution-verification', 'fixed-point-theorems']"
72,Uniform Convergence Implies $L^2$ Convergence and $L^2$ Convergence Implies $L^1$ Convergence,Uniform Convergence Implies  Convergence and  Convergence Implies  Convergence,L^2 L^2 L^1,"Some of the books that discuss convergence say that uniform convergence implies $L^2$ convergence and $L^2$ convergence implies $L^1$ convergence, both while taken over a bounded interval I. While I understand how that could be true intuitively, I'm struggling to see the proof of that. Any ideas? EDIT: For Uniform Convergence implies $L^2$ convergence, uniform convergence and interchange of limits means $\lim\limits_{n\rightarrow\infty} \int_{a}^{b}(f(x)-f_n(x)) = 0$ and from there, I'm not sure what to do? For $L^2$ convergence, I don't actually know where to get started either.","Some of the books that discuss convergence say that uniform convergence implies $L^2$ convergence and $L^2$ convergence implies $L^1$ convergence, both while taken over a bounded interval I. While I understand how that could be true intuitively, I'm struggling to see the proof of that. Any ideas? EDIT: For Uniform Convergence implies $L^2$ convergence, uniform convergence and interchange of limits means $\lim\limits_{n\rightarrow\infty} \int_{a}^{b}(f(x)-f_n(x)) = 0$ and from there, I'm not sure what to do? For $L^2$ convergence, I don't actually know where to get started either.",,"['real-analysis', 'convergence-divergence', 'lp-spaces']"
73,"Can a non-trivial continuous function ""undo"" the discontinuities of another function?","Can a non-trivial continuous function ""undo"" the discontinuities of another function?",,"Apologies for the unclear title, I have no idea if the property I'm looking for has a better name. I'm wondering if there exists a pair of functions $f, g : \mathbb{R} \rightarrow \mathbb{R}$ such that : $g$ is a bijection and is nowhere continuous (for an example, see this answer ). $f$ is continuous and not constant. $f \circ g$ is continuous.","Apologies for the unclear title, I have no idea if the property I'm looking for has a better name. I'm wondering if there exists a pair of functions such that : is a bijection and is nowhere continuous (for an example, see this answer ). is continuous and not constant. is continuous.","f, g : \mathbb{R} \rightarrow \mathbb{R} g f f \circ g","['real-analysis', 'continuity', 'real-numbers']"
74,A series involves harmonic number,A series involves harmonic number,,How do we get a closed form for $$\sum_{n=1}^\infty\frac{H_n}{(2n+1)^2}$$,How do we get a closed form for $$\sum_{n=1}^\infty\frac{H_n}{(2n+1)^2}$$,,"['real-analysis', 'sequences-and-series', 'riemann-zeta', 'harmonic-numbers']"
75,Vladimir Zorich vs Rudin/Pugh/Abbott,Vladimir Zorich vs Rudin/Pugh/Abbott,,"There have been various comparisons between books on Analysis. I was surprised to find out that Zorich's book on Analysis was not compared anywhere. Can anyone give a comparison between Zorich and the other books by Rudin, Pugh and Abbott? A Little Background I am doing self-study of Analysis on my own. I am a graduate student in Engineering which involves a lot of Mathematics and slowly I am getting in love with Mathematics and thinking of doing a major in Mathematics later. With this in mind, I am determined to consolidate my mathematical background. So, I have started reading Zorich's texts on Analysis. But when I looked on the internet for reviews, Rudin, Pugh and Abbott had more reviews. Also, Zorich's texts are in two volumes and will take some mighty effort. In any case, I am actually loving Zorich and would like to continue with this book. Please provide me some comparison between these books. PS: An additional insight into how should I approach Analysis for self-study and how much time should it take to self-study this course would be much appreciated.","There have been various comparisons between books on Analysis. I was surprised to find out that Zorich's book on Analysis was not compared anywhere. Can anyone give a comparison between Zorich and the other books by Rudin, Pugh and Abbott? A Little Background I am doing self-study of Analysis on my own. I am a graduate student in Engineering which involves a lot of Mathematics and slowly I am getting in love with Mathematics and thinking of doing a major in Mathematics later. With this in mind, I am determined to consolidate my mathematical background. So, I have started reading Zorich's texts on Analysis. But when I looked on the internet for reviews, Rudin, Pugh and Abbott had more reviews. Also, Zorich's texts are in two volumes and will take some mighty effort. In any case, I am actually loving Zorich and would like to continue with this book. Please provide me some comparison between these books. PS: An additional insight into how should I approach Analysis for self-study and how much time should it take to self-study this course would be much appreciated.",,"['real-analysis', 'analysis', 'self-learning', 'book-recommendation']"
76,Making a standard theoretical physics argument rigorous,Making a standard theoretical physics argument rigorous,,"In theoretical physics one often encounters the following rationale: if $f$ and $g$ are functions on $\mathbf{R}^n$ , satisfying some technical conditions, and $\displaystyle\int_\Omega f=\int_\Omega g$ for all open sets $\Omega$ , then $f=g$ . (For instance, one obtains from Gauss law in ""integral form"" $\displaystyle\int_\Omega\mathrm{div} \ E = \int_\Omega \rho$ and its ""differential form"" $\mathrm{div} \ E=\rho$ .) Now my problem is: I want to know these technical conditions. Of course, it is a seemingly obvious statement, but it disturbs me not to know under what conditions this reasoning is legit. Is there some theorem which answers this question? Of course, I am also happy with a good reference.","In theoretical physics one often encounters the following rationale: if and are functions on , satisfying some technical conditions, and for all open sets , then . (For instance, one obtains from Gauss law in ""integral form"" and its ""differential form"" .) Now my problem is: I want to know these technical conditions. Of course, it is a seemingly obvious statement, but it disturbs me not to know under what conditions this reasoning is legit. Is there some theorem which answers this question? Of course, I am also happy with a good reference.",f g \mathbf{R}^n \displaystyle\int_\Omega f=\int_\Omega g \Omega f=g \displaystyle\int_\Omega\mathrm{div} \ E = \int_\Omega \rho \mathrm{div} \ E=\rho,"['real-analysis', 'reference-request', 'physics']"
77,Is the rectangular function a convolution of $L^1$ functions?,Is the rectangular function a convolution of  functions?,L^1,"Do there exist functions $f,g$ in $L^1(\mathbf{R})$ such that the convolution $f \star g$ is (almost everywhere) equal to the indicator function of the interval $[0,1]$ ?",Do there exist functions in such that the convolution is (almost everywhere) equal to the indicator function of the interval ?,"f,g L^1(\mathbf{R}) f \star g [0,1]","['real-analysis', 'functional-analysis', 'lp-spaces', 'convolution']"
78,lim sup inequality $\limsup ( a_n b_n ) \leq \limsup a_n \limsup b_n $,lim sup inequality,\limsup ( a_n b_n ) \leq \limsup a_n \limsup b_n ,"I´m not sure how to start with this proof, how can I do it? $$ \limsup ( a_n b_n ) \leqslant \limsup a_n \limsup b_n  $$ I also have to prove, if $ \lim a_n $ exists then: $$ \limsup ( a_n b_n  ) = \limsup a_n \limsup b_n  $$ Help please, it´s not a homework I want to learn.","I´m not sure how to start with this proof, how can I do it? $$ \limsup ( a_n b_n ) \leqslant \limsup a_n \limsup b_n  $$ I also have to prove, if $ \lim a_n $ exists then: $$ \limsup ( a_n b_n  ) = \limsup a_n \limsup b_n  $$ Help please, it´s not a homework I want to learn.",,"['real-analysis', 'inequality', 'limsup-and-liminf']"
79,Find the exact value of the infinite sum $\sum_{n=1}^\infty \big\{\mathrm{e}-\big(1+\frac1n\big)^{n}\big\}$,Find the exact value of the infinite sum,\sum_{n=1}^\infty \big\{\mathrm{e}-\big(1+\frac1n\big)^{n}\big\},"How can we find the exact value of the infinite sum  $$ \displaystyle\sum_{n=1}^\infty \left\{\mathrm{e}-\Big(1+\frac1n\Big)^n\right\}? $$ This problem appears in: T. Andreescu, T. Radulescu & V. Radulescu, Problems in Real Analysis: Advanced Calculus on the real line , p.114.","How can we find the exact value of the infinite sum  $$ \displaystyle\sum_{n=1}^\infty \left\{\mathrm{e}-\Big(1+\frac1n\Big)^n\right\}? $$ This problem appears in: T. Andreescu, T. Radulescu & V. Radulescu, Problems in Real Analysis: Advanced Calculus on the real line , p.114.",,"['calculus', 'real-analysis', 'sequences-and-series', 'limits', 'convergence-divergence']"
80,Proving $ \int_{0}^{\infty} \frac{\ln(t)}{\sqrt{t}}e^{-t} \mathrm dt=-\sqrt{\pi}(\gamma+\ln{4})$,Proving, \int_{0}^{\infty} \frac{\ln(t)}{\sqrt{t}}e^{-t} \mathrm dt=-\sqrt{\pi}(\gamma+\ln{4}),"I would like to prove that: $$ \int_{0}^{\infty} \frac{\ln(t)}{\sqrt{t}}e^{-t} \mathrm dt=-\sqrt{\pi}(\gamma+\ln{4})$$ I tried to use the integral $$\int_{0}^{n} \frac{\ln(t)}{\sqrt{t}}\left(1-\frac{t}{n}\right)^n \mathrm dt$$ $$\int_{0}^{n} \frac{\ln(t)}{\sqrt{t}}\left(1-\frac{t}{n}\right)^n \mathrm dt \;{\underset{\small n\to\infty}{\longrightarrow}}\; \int_{0}^{\infty} \frac{\ln(t)}{\sqrt{t}}e^{-t} \mathrm dt$$ (dominated convergence theorem) Using the substitution $t\to\frac{t}{n}$, I get: $$ \int_{0}^{n} \frac{\ln(t)}{\sqrt{t}}\left(1-\frac{t}{n}\right)^n \mathrm dt=\sqrt{n}\left(\ln(n)\int_{0}^{1} \frac{(1-t)^n}{\sqrt{t}} \mathrm dt+\int_{0}^{1} \frac{\ln(t)(1-t)^n}{\sqrt{t}} \mathrm dt\right) $$ However I don't know if I am on the right track for these new integrals look quite tricky.","I would like to prove that: $$ \int_{0}^{\infty} \frac{\ln(t)}{\sqrt{t}}e^{-t} \mathrm dt=-\sqrt{\pi}(\gamma+\ln{4})$$ I tried to use the integral $$\int_{0}^{n} \frac{\ln(t)}{\sqrt{t}}\left(1-\frac{t}{n}\right)^n \mathrm dt$$ $$\int_{0}^{n} \frac{\ln(t)}{\sqrt{t}}\left(1-\frac{t}{n}\right)^n \mathrm dt \;{\underset{\small n\to\infty}{\longrightarrow}}\; \int_{0}^{\infty} \frac{\ln(t)}{\sqrt{t}}e^{-t} \mathrm dt$$ (dominated convergence theorem) Using the substitution $t\to\frac{t}{n}$, I get: $$ \int_{0}^{n} \frac{\ln(t)}{\sqrt{t}}\left(1-\frac{t}{n}\right)^n \mathrm dt=\sqrt{n}\left(\ln(n)\int_{0}^{1} \frac{(1-t)^n}{\sqrt{t}} \mathrm dt+\int_{0}^{1} \frac{\ln(t)(1-t)^n}{\sqrt{t}} \mathrm dt\right) $$ However I don't know if I am on the right track for these new integrals look quite tricky.",,"['calculus', 'real-analysis', 'integration']"
81,Using an integral to generate rational approximations of $\pi$,Using an integral to generate rational approximations of,\pi,"Let $$f(r)=\int_0^1\frac{x^r(1-x)^r}{1+x^2}dx.$$ One surprising fact is that $f(4)=\frac{22}{7}-\pi.$ This got me thinking. Surely, it can't be a coincidence that a fairly accurate rational approximation for $\pi$ shows up in this way, right? Well, this is what I've found so far: $$\boxed{f(0)=\frac{\pi}{4}≈0.785}$$ $$\boxed{f(4)=\frac{22}{7}-\pi≈0.001}$$ $$\boxed{f(8)=4\pi-\frac{188684}{15015}≈3.64×10^{-6}}$$ $$\boxed{f(12)=\frac{431302721}{8580495}-16\pi≈1.18×10^{-8}}$$ $$\boxed{f(16)=64\pi-\frac{5930158704872}{29494189725}≈4.00×10^{-11}}$$ Let $k\in\{0,1,2,\ldots\}.$ It seems to me that $f(4k)=4^{k-1}((-1)^k\pi+(-1)^{k+1}R_k),$ where $R_k$ is some rational approximation of $\pi.$ We have: $$\boxed{R_0=0}$$ $$\boxed{R_1=\frac{22}{7}}$$ $$\boxed{R_2=\frac{188684}{15015×4}}$$ $$\boxed{R_3=\frac{431302721}{8580495×16}}$$ $$\boxed{R_4=\frac{5930158704872}{29494189725×64}}$$ I have three conjectures at this point: $$\textbf{C1:}\lim_{r\to\infty}f(r)=0.$$ $$\textbf{C2:}\forall k\in\{0,1,2,\ldots\},f(4k)=4^{k-1}((-1)^k\pi+(-1)^{k+1}R_k),R_k\in\mathbb{Q}.$$ $$\textbf{C3:}\lim_{k\to\infty}R_k=\pi.$$ Clearly, $(\textbf{C1}\wedge\textbf{C2})\implies\textbf{C3}.$ How do we go about proving these conjectures? It seems like $\textbf{C2}$ is the toughest of the three. $\textbf{C1}$ makes intuitive sense as the integrand is a decreasing function of $r.$","Let One surprising fact is that This got me thinking. Surely, it can't be a coincidence that a fairly accurate rational approximation for shows up in this way, right? Well, this is what I've found so far: Let It seems to me that where is some rational approximation of We have: I have three conjectures at this point: Clearly, How do we go about proving these conjectures? It seems like is the toughest of the three. makes intuitive sense as the integrand is a decreasing function of","f(r)=\int_0^1\frac{x^r(1-x)^r}{1+x^2}dx. f(4)=\frac{22}{7}-\pi. \pi \boxed{f(0)=\frac{\pi}{4}≈0.785} \boxed{f(4)=\frac{22}{7}-\pi≈0.001} \boxed{f(8)=4\pi-\frac{188684}{15015}≈3.64×10^{-6}} \boxed{f(12)=\frac{431302721}{8580495}-16\pi≈1.18×10^{-8}} \boxed{f(16)=64\pi-\frac{5930158704872}{29494189725}≈4.00×10^{-11}} k\in\{0,1,2,\ldots\}. f(4k)=4^{k-1}((-1)^k\pi+(-1)^{k+1}R_k), R_k \pi. \boxed{R_0=0} \boxed{R_1=\frac{22}{7}} \boxed{R_2=\frac{188684}{15015×4}} \boxed{R_3=\frac{431302721}{8580495×16}} \boxed{R_4=\frac{5930158704872}{29494189725×64}} \textbf{C1:}\lim_{r\to\infty}f(r)=0. \textbf{C2:}\forall k\in\{0,1,2,\ldots\},f(4k)=4^{k-1}((-1)^k\pi+(-1)^{k+1}R_k),R_k\in\mathbb{Q}. \textbf{C3:}\lim_{k\to\infty}R_k=\pi. (\textbf{C1}\wedge\textbf{C2})\implies\textbf{C3}. \textbf{C2} \textbf{C1} r.","['real-analysis', 'calculus', 'definite-integrals', 'rational-numbers', 'pi']"
82,Infinite 'hex': is a win always achievable?,Infinite 'hex': is a win always achievable?,,"In the game hex , at least one player always wins because they can form a chain of hexagons across the board. This led me to wonder, what happens if we generalise to infinitely many points? Specifically, if every point in a unit square (including boundaries) is coloured red or blue, does there necessarily exist a continuous function $f: [0,1] \to [0,1]\times [0,1]$ such that $f(x)$ is either a) Always red$\space\space$ and $f(0)=(0,a), f(1)=(1,b)$ for some a,b b) Always blue and $f(0)=(a,0), f(1)=(b,1)$ for some a,b Furthermore, if there exists a function such that (a) is true, then does that necessarily mean there does not exist a function such that (b) is true? (In the example, red wins with the path shown an blue loses) My intuition tells me that this is true, but I have no idea how to begin proving it. My best idea was to colour the regions to the left and right of the square red. Then anything connected to this red region is marked green. If the other side is connected to this then we are done. Otherwise, take the points along the boundary of this green region. They must be blue otherwise there exists a point closer to the region that is blue (by definition of the green region). Hence this boundary reaches all the way down to the bottom and we are done. But I'm not sure if this green region is well-defined or anything and have no idea how to show that it is. (Also, I've got no idea what tag(s) to put on this, sorry)","In the game hex , at least one player always wins because they can form a chain of hexagons across the board. This led me to wonder, what happens if we generalise to infinitely many points? Specifically, if every point in a unit square (including boundaries) is coloured red or blue, does there necessarily exist a continuous function $f: [0,1] \to [0,1]\times [0,1]$ such that $f(x)$ is either a) Always red$\space\space$ and $f(0)=(0,a), f(1)=(1,b)$ for some a,b b) Always blue and $f(0)=(a,0), f(1)=(b,1)$ for some a,b Furthermore, if there exists a function such that (a) is true, then does that necessarily mean there does not exist a function such that (b) is true? (In the example, red wins with the path shown an blue loses) My intuition tells me that this is true, but I have no idea how to begin proving it. My best idea was to colour the regions to the left and right of the square red. Then anything connected to this red region is marked green. If the other side is connected to this then we are done. Otherwise, take the points along the boundary of this green region. They must be blue otherwise there exists a point closer to the region that is blue (by definition of the green region). Hence this boundary reaches all the way down to the bottom and we are done. But I'm not sure if this green region is well-defined or anything and have no idea how to show that it is. (Also, I've got no idea what tag(s) to put on this, sorry)",,"['real-analysis', 'game-theory']"
83,Show that $\left(\frac{x_1^{x_2}}{x_2}\right)^p+\left(\frac{x_2^{x_3}}{x_3}\right)^p+\cdots+\left(\frac{x_n^{x_1}}{x_1}\right)^p\ge n$ for any $p\ge1$,Show that  for any,\left(\frac{x_1^{x_2}}{x_2}\right)^p+\left(\frac{x_2^{x_3}}{x_3}\right)^p+\cdots+\left(\frac{x_n^{x_1}}{x_1}\right)^p\ge n p\ge1,"The inequality $\sqrt{\frac{a^b}{b}}+\sqrt{\frac{b^a}{a}}\ge 2$ for all $a,b>0$ was shown here using first-order Padé approximants on each exponent, where the minimum is attained at $a=b=1$ . By empirical evidence, it appears that inequalities of this type hold for an arbitrary number of variables. We can phrase the generalised problem as follows. Let $(x_i)_{1\le i\le n}$ be a sequence of positive real numbers. Define $\boldsymbol a=\begin{pmatrix}a_1&\cdots&a_n\end{pmatrix}$ such that $a_k=x_k^{x_{k+1}}/x_{k+1}$ for each $1\le k<n$ and $a_n=x_n^{x_1}/x_1$ . How do we show that $$\|\boldsymbol a\|_p^p\ge n$$ for any $p\ge1$ ? As before, AM-GM is far too weak since the inequality $\displaystyle\|\boldsymbol a\|_p^p\ge 2\left(\prod_{\text{cyc}}\frac{x_1^{x_2}}{x_2}\right)^{1/{2p}}$ does not guarantee the result when at least one $x_i$ is smaller than $1$ . We can eliminate the exponent on the denominator by taking $x_i=X_i^{1/p}$ so that $\displaystyle\|\boldsymbol a\|_p^p=\sum_{\text{cyc}}\frac{X_1^{X_2^{1/p}}}{X_2}$ but the approximant approach no longer becomes feasible; even in the case where $p$ is an integer the problem reduces to a posynomial inequality of rational degrees. Perhaps there are some obscure $L^p$ -norm/Hölder-type identities of use but I'm at a loss in terms of finding references. Empirical results: In the interval $p\in[1,\infty)$ , Wolfram suggests that the minimum is $n$ ( Notebook result ) which is obtained when $\boldsymbol a$ is the vector of ones. However, we note that in the interval $p\in(0,1)$ , the empirical minimum no longer displays this consistent behaviour as can be seen in this Notebook result . The sequence $\approx(1.00,2.00,2.01,3.36,3.00,4.00)$ appears to increase almost linearly every two values, but I cannot verify it for a larger number of variables due to instability in the working precision.","The inequality for all was shown here using first-order Padé approximants on each exponent, where the minimum is attained at . By empirical evidence, it appears that inequalities of this type hold for an arbitrary number of variables. We can phrase the generalised problem as follows. Let be a sequence of positive real numbers. Define such that for each and . How do we show that for any ? As before, AM-GM is far too weak since the inequality does not guarantee the result when at least one is smaller than . We can eliminate the exponent on the denominator by taking so that but the approximant approach no longer becomes feasible; even in the case where is an integer the problem reduces to a posynomial inequality of rational degrees. Perhaps there are some obscure -norm/Hölder-type identities of use but I'm at a loss in terms of finding references. Empirical results: In the interval , Wolfram suggests that the minimum is ( Notebook result ) which is obtained when is the vector of ones. However, we note that in the interval , the empirical minimum no longer displays this consistent behaviour as can be seen in this Notebook result . The sequence appears to increase almost linearly every two values, but I cannot verify it for a larger number of variables due to instability in the working precision.","\sqrt{\frac{a^b}{b}}+\sqrt{\frac{b^a}{a}}\ge 2 a,b>0 a=b=1 (x_i)_{1\le i\le n} \boldsymbol a=\begin{pmatrix}a_1&\cdots&a_n\end{pmatrix} a_k=x_k^{x_{k+1}}/x_{k+1} 1\le k<n a_n=x_n^{x_1}/x_1 \|\boldsymbol a\|_p^p\ge n p\ge1 \displaystyle\|\boldsymbol a\|_p^p\ge 2\left(\prod_{\text{cyc}}\frac{x_1^{x_2}}{x_2}\right)^{1/{2p}} x_i 1 x_i=X_i^{1/p} \displaystyle\|\boldsymbol a\|_p^p=\sum_{\text{cyc}}\frac{X_1^{X_2^{1/p}}}{X_2} p L^p p\in[1,\infty) n \boldsymbol a p\in(0,1) \approx(1.00,2.00,2.01,3.36,3.00,4.00)","['real-analysis', 'inequality']"
84,Are simple functions dense in $L^\infty$?,Are simple functions dense in ?,L^\infty,Are simple functions dense in $L^\infty$? I've been able to show this for finite measure spaces but not in general.,Are simple functions dense in $L^\infty$? I've been able to show this for finite measure spaces but not in general.,,"['real-analysis', 'measure-theory']"
85,Summation Symbol: Changing the Order,Summation Symbol: Changing the Order,,"I have some questions regarding the order of the summation signs (I have tried things out and also read the wikipedia page, nevertheless some questions remained unanswered): Original 1. wikipedia says that: $$\sum_{k=1}^m a_k \sum_{\color{red}{k}=1}^n b_l = \sum_{k=1}^m \sum_{l=1}^n a_k b_l$$ does not necessarily hold. What would be a concrete example for that? Edited 1. wikipedia says that: $$\sum_{k=1}^m a_k \sum_{\color{red}{l}=1}^n b_l = \sum_{k=1}^m \sum_{l=1}^n a_k b_l$$ does not necessarily hold. What would be a concrete example for that? 2.As far as I see generally it holds that: $$\sum_{j=1}^m \sum_{i=1}^n a_ib_j = \sum_{i=1}^n \sum_{j=1}^m a_ib_j $$ why is that? It is not due to the property, that multiplication is commutative, is it? 3.What about infinite series, when does: $$\sum_{k=1}^{\infty}\sum_{l=1}^{\infty} a_kb_l = \sum_{k=1}^{\infty}a_k \sum_{l=1}^{\infty}b_l$$ hold?  And does here too $$\sum_{k=1}^{\infty}\sum_{l=1}^{\infty} a_kb_l = \sum_{l=1}^{\infty}\sum_{k=1}^{\infty} a_kb_l$$ hold? Thanks","I have some questions regarding the order of the summation signs (I have tried things out and also read the wikipedia page, nevertheless some questions remained unanswered): Original 1. wikipedia says that: $$\sum_{k=1}^m a_k \sum_{\color{red}{k}=1}^n b_l = \sum_{k=1}^m \sum_{l=1}^n a_k b_l$$ does not necessarily hold. What would be a concrete example for that? Edited 1. wikipedia says that: $$\sum_{k=1}^m a_k \sum_{\color{red}{l}=1}^n b_l = \sum_{k=1}^m \sum_{l=1}^n a_k b_l$$ does not necessarily hold. What would be a concrete example for that? 2.As far as I see generally it holds that: $$\sum_{j=1}^m \sum_{i=1}^n a_ib_j = \sum_{i=1}^n \sum_{j=1}^m a_ib_j $$ why is that? It is not due to the property, that multiplication is commutative, is it? 3.What about infinite series, when does: $$\sum_{k=1}^{\infty}\sum_{l=1}^{\infty} a_kb_l = \sum_{k=1}^{\infty}a_k \sum_{l=1}^{\infty}b_l$$ hold?  And does here too $$\sum_{k=1}^{\infty}\sum_{l=1}^{\infty} a_kb_l = \sum_{l=1}^{\infty}\sum_{k=1}^{\infty} a_kb_l$$ hold? Thanks",,"['real-analysis', 'sequences-and-series', 'summation']"
86,Why is $L^1(\mathbb{R}^n) \cap L^2(\mathbb{R}^n)$ dense in $ L^2(\mathbb{R}^n)$?,Why is  dense in ?,L^1(\mathbb{R}^n) \cap L^2(\mathbb{R}^n)  L^2(\mathbb{R}^n),"In Lieb and Loss's Analysis, I saw that they mentioned  $L^1(\mathbb{R}^n) \cap L^2(\mathbb{R}^n)$ dense in $ L^2(\mathbb{R}^n)$ (dense wrt the $L^2$ norm, I think). But I didn't find its proof in the book. So I wonder why it is? Does this conclusion hold for $L^1(\Omega, \mathcal{F}, \mu) \cap L^2(\Omega, \mathcal{F}, \mu)$ for any measure space $(\Omega, \mathcal{F}, \mu)$? Are there similar statements if replace $L^1$ with $L^p$, and  $ L^2$ with $L^q$ for $p \leq q \in (0, \infty ]$ or $\in [1, \infty]$? Thanks and regards!","In Lieb and Loss's Analysis, I saw that they mentioned  $L^1(\mathbb{R}^n) \cap L^2(\mathbb{R}^n)$ dense in $ L^2(\mathbb{R}^n)$ (dense wrt the $L^2$ norm, I think). But I didn't find its proof in the book. So I wonder why it is? Does this conclusion hold for $L^1(\Omega, \mathcal{F}, \mu) \cap L^2(\Omega, \mathcal{F}, \mu)$ for any measure space $(\Omega, \mathcal{F}, \mu)$? Are there similar statements if replace $L^1$ with $L^p$, and  $ L^2$ with $L^q$ for $p \leq q \in (0, \infty ]$ or $\in [1, \infty]$? Thanks and regards!",,"['real-analysis', 'general-topology', 'functional-analysis', 'lp-spaces']"
87,Inequality involving $\limsup$ and $\liminf$: $ \liminf(a_{n+1}/a_n) \le \liminf a_n^{1/n} \le \limsup a_n^{1/n} \le \limsup(a_{n+1}/a_n)$,Inequality involving  and :,\limsup \liminf  \liminf(a_{n+1}/a_n) \le \liminf a_n^{1/n} \le \limsup a_n^{1/n} \le \limsup(a_{n+1}/a_n),"This may have been asked before, however I was unable to find any duplicate. This comes from pg. 52 of ""Mathematical Analysis: An Introduction"" by Browder. Problem 14: If $(a_n)$ is a sequence in $\mathbb R$ and $a_n > 0$ for every $n$ . Then show: $$ \liminf\frac{a_{n+1}}{a_n} \le \liminf a_n^{1/n} \le \limsup a_n^{1/n} \le \limsup\frac{a_{n+1}}{a_n}$$ The middle inequality is clear. However I am having a hard time showing the ones on the left and right. (It seems like the approach should be similar for each). This is homework, so it'd be great if someone could give me a hint to get started on at least one of the inequalities. Thanks.","This may have been asked before, however I was unable to find any duplicate. This comes from pg. 52 of ""Mathematical Analysis: An Introduction"" by Browder. Problem 14: If is a sequence in and for every . Then show: The middle inequality is clear. However I am having a hard time showing the ones on the left and right. (It seems like the approach should be similar for each). This is homework, so it'd be great if someone could give me a hint to get started on at least one of the inequalities. Thanks.","(a_n) \mathbb R a_n > 0 n 
\liminf\frac{a_{n+1}}{a_n} \le \liminf a_n^{1/n} \le \limsup a_n^{1/n} \le \limsup\frac{a_{n+1}}{a_n}","['real-analysis', 'analysis', 'inequality', 'limsup-and-liminf']"
88,"Prove that there exists $n\in\mathbb{N}$ such that $f^{(n)}$ has at least n+1 zeros on $(-1,1)$",Prove that there exists  such that  has at least n+1 zeros on,"n\in\mathbb{N} f^{(n)} (-1,1)","Let $f\in C^{\infty}(\mathbb{R},\mathbb{R})$ such that $f(x)=0$ on $\mathbb{R}$\ $(-1,1)$. Prove that there exists $n\in\mathbb{N}$ such that $f^{(n)}$ has at least n+1 zeros on $(-1,1)$ My attempt : Assume $f$ is strictly positive  (if not n=0 works). Let $Z(f^{(n)})$ the number of zeros of $f^{(n)}$ on $(-1,1)$. Then we have: $$ \forall n\in\mathbb{N}:\;f^{(n)}(-1)=f^{(n)}(1)=0 $$ so that iterating the Rolle theorem to the successive derivatives of f we have  $$  \;Z(f^{(k+1)}) \geq Z(f^{(k)}) +1 $$ thus, by induction : $$ \forall n\in\mathbb{N}: \;Z(f^{(n+1)}) \geq Z(f^{(n)}) +1 $$ Therefore, $$ Z(f^{(n)}) \geq n $$ How can I find an another zero ? I'm still looking for an another proof which does not use the lemma .., Thank you in advance.","Let $f\in C^{\infty}(\mathbb{R},\mathbb{R})$ such that $f(x)=0$ on $\mathbb{R}$\ $(-1,1)$. Prove that there exists $n\in\mathbb{N}$ such that $f^{(n)}$ has at least n+1 zeros on $(-1,1)$ My attempt : Assume $f$ is strictly positive  (if not n=0 works). Let $Z(f^{(n)})$ the number of zeros of $f^{(n)}$ on $(-1,1)$. Then we have: $$ \forall n\in\mathbb{N}:\;f^{(n)}(-1)=f^{(n)}(1)=0 $$ so that iterating the Rolle theorem to the successive derivatives of f we have  $$  \;Z(f^{(k+1)}) \geq Z(f^{(k)}) +1 $$ thus, by induction : $$ \forall n\in\mathbb{N}: \;Z(f^{(n+1)}) \geq Z(f^{(n)}) +1 $$ Therefore, $$ Z(f^{(n)}) \geq n $$ How can I find an another zero ? I'm still looking for an another proof which does not use the lemma .., Thank you in advance.",,['real-analysis']
89,Is $\sum_{a=0}^m\sum_{b=0}^n\cos(abx)$ always positive?,Is  always positive?,\sum_{a=0}^m\sum_{b=0}^n\cos(abx),"Fix integers $m,n\geq0$ . Do we have the inequality $\displaystyle\sum_{a=0}^m\sum_{b=0}^n\cos(abx)>0$ for all $x\in\mathbb{R}$ ? We can also write this function as \begin{align*} \sum_{a=0}^m\sum_{b=0}^n\cos(abx)&=m+n+1+\sum_{a=1}^m\sum_{b=1}^n\cos(abx)\\ &=m+n+1+\sum_{a=1}^m\frac{1}{2}\left(\frac{\sin((n+1/2)ax)}{\sin(ax/2)}-1\right)\\ &=\frac{m}{2}+n+1+\frac{1}{2}\sum_{a=1}^mD_n(ax), \end{align*} where $$D_n(x)=\frac{\sin((n+1/2)x)}{\sin(x/2)}$$ is the Dirichlet kernel (up to a factor of $2\pi$ , depending on your convention). Using this formula, it is easy to check the conjecture for small values of $m$ and $n$ ( desmos link ).","Fix integers . Do we have the inequality for all ? We can also write this function as where is the Dirichlet kernel (up to a factor of , depending on your convention). Using this formula, it is easy to check the conjecture for small values of and ( desmos link ).","m,n\geq0 \displaystyle\sum_{a=0}^m\sum_{b=0}^n\cos(abx)>0 x\in\mathbb{R} \begin{align*}
\sum_{a=0}^m\sum_{b=0}^n\cos(abx)&=m+n+1+\sum_{a=1}^m\sum_{b=1}^n\cos(abx)\\
&=m+n+1+\sum_{a=1}^m\frac{1}{2}\left(\frac{\sin((n+1/2)ax)}{\sin(ax/2)}-1\right)\\
&=\frac{m}{2}+n+1+\frac{1}{2}\sum_{a=1}^mD_n(ax),
\end{align*} D_n(x)=\frac{\sin((n+1/2)x)}{\sin(x/2)} 2\pi m n","['real-analysis', 'inequality', 'trigonometric-series']"
90,Does the series $\sum_{n=1}^\infty\frac{\sin n}{\ln n+\cos n}$ converge?,Does the series  converge?,\sum_{n=1}^\infty\frac{\sin n}{\ln n+\cos n},"$$\sum_{n=1}^\infty\frac{\sin n}{\ln n+\cos n}$$ My guess is ""yes"", but I can't prove it.","$$\sum_{n=1}^\infty\frac{\sin n}{\ln n+\cos n}$$ My guess is ""yes"", but I can't prove it.",,"['real-analysis', 'sequences-and-series']"
91,Evaluation of $\int_0^1 \frac{\log(1+x)}{1+x}\log\left(\log\left(\frac{1}{x}\right)\right) \ dx$,Evaluation of,\int_0^1 \frac{\log(1+x)}{1+x}\log\left(\log\left(\frac{1}{x}\right)\right) \ dx,"I need some hints, clues for getting the closed form of $$\int_0^1 \frac{\log(1+x)}{1+x}\log\left(\log\left(\frac{1}{x}\right)\right) \ dx$$","I need some hints, clues for getting the closed form of $$\int_0^1 \frac{\log(1+x)}{1+x}\log\left(\log\left(\frac{1}{x}\right)\right) \ dx$$",,"['real-analysis', 'integration', 'sequences-and-series', 'definite-integrals', 'improper-integrals']"
92,How should I calculate $\lim_{n\rightarrow \infty} \frac{1^n+2^n+3^n+...+n^n}{n^n}$ [duplicate],How should I calculate  [duplicate],\lim_{n\rightarrow \infty} \frac{1^n+2^n+3^n+...+n^n}{n^n},This question already has answers here : How to evaluate $ \lim \limits_{n\to \infty} \sum \limits_ {k=1}^n \frac{k^n}{n^n}$? (6 answers) Closed 4 years ago . How should I calculate the below limit $$\lim_{n\rightarrow \infty} \frac{1^n+2^n+3^n+...+n^n}{n^n}$$ I have no idea where to start from.,This question already has answers here : How to evaluate $ \lim \limits_{n\to \infty} \sum \limits_ {k=1}^n \frac{k^n}{n^n}$? (6 answers) Closed 4 years ago . How should I calculate the below limit $$\lim_{n\rightarrow \infty} \frac{1^n+2^n+3^n+...+n^n}{n^n}$$ I have no idea where to start from.,,"['real-analysis', 'sequences-and-series', 'limits']"
93,Why is the Monotone Convergence Theorem more famous than it's stronger cousin?,Why is the Monotone Convergence Theorem more famous than it's stronger cousin?,,"I am reading Stein & Shakarchi. On page 62 we have the Monotone Convergence Theorem: Suppose $\{f_n\}$ is a sequence of non-negative measurable functions with $f_n\nearrow f.$ Then $\displaystyle \lim_{n \to \infty} \int f_n = \int f$. However, just before this theorem we have this much more powerful corrolary of Fatou's lemma: Suppose $f$ is a non-negative measurable function, and $\{f_n\}$ a sequence of non-negative measurable functions with $f_n(x) \le f(x)$ and $f_n(x) \to f(x)$ for a.e. $x$. Then $\displaystyle \lim_{n \to \infty} \int f_n = \int f$. To me, this second corollary seems strictly better than the Monotone Convergence Theorem, yet it is the M.C.T. that has a name and is used often. Am I misunderstanding the theorems, or is there a reason why the M.C.T. is more popular? Does this corollary have a name?","I am reading Stein & Shakarchi. On page 62 we have the Monotone Convergence Theorem: Suppose $\{f_n\}$ is a sequence of non-negative measurable functions with $f_n\nearrow f.$ Then $\displaystyle \lim_{n \to \infty} \int f_n = \int f$. However, just before this theorem we have this much more powerful corrolary of Fatou's lemma: Suppose $f$ is a non-negative measurable function, and $\{f_n\}$ a sequence of non-negative measurable functions with $f_n(x) \le f(x)$ and $f_n(x) \to f(x)$ for a.e. $x$. Then $\displaystyle \lim_{n \to \infty} \int f_n = \int f$. To me, this second corollary seems strictly better than the Monotone Convergence Theorem, yet it is the M.C.T. that has a name and is used often. Am I misunderstanding the theorems, or is there a reason why the M.C.T. is more popular? Does this corollary have a name?",,"['real-analysis', 'analysis', 'measure-theory', 'convergence-divergence', 'lebesgue-integral']"
94,Show that $ \lim\limits_{n\to\infty}\frac{1}{n}\sum\limits_{k=0}^{n-1}e^{ik^2}=0$,Show that, \lim\limits_{n\to\infty}\frac{1}{n}\sum\limits_{k=0}^{n-1}e^{ik^2}=0,"TL;DR : The question is how do I show that $\displaystyle \lim_{n\to\infty}\frac{1}{n}\sum_{k=0}^{n-1}e^{ik^2}=0$ ? More generaly the question would be : given an increasing sequence of integers $(u_k)$ and an irrational number $\alpha$, how do I tell if  $\displaystyle \lim_{n\to\infty}\frac{1}{n}\sum_{k=0}^{n-1}e^{2i\pi \alpha u_k}=0$ ? I'm not asking for a criterium for completely general sequences, an answer for sequences like $u_k=k^2$, $v_k=k!$ or $w_k=p(k)$ with $p\in \mathbf Z [X]$ would already be awesome. A little explanation about this question : In Real and Complex Analysis by Rudin there is the folowing exercise : Let $f$ be a continuous, complex valued, $1$-periodic function and $\alpha$ an irrational number. Show that  $\displaystyle \lim_{n\to\infty}\frac{1}{n}\sum_{k=0}^{n-1}f(\alpha k)=\int_0^1f(x)\mathrm d x$. (We say that $(\alpha k)_k$ is uniformly distributed in $\mathbf R / \mathbf Z$) With the hint given by Rudin the proof is pretty straightforward : First one show that this is true for every $f_j=\exp(2i\pi j\cdot)$ with $j\in \mathbf{Z} $. Then using density of trigonometric polynomials in $(C^0_1(\mathbf{R}),\|\cdot\|_\infty)$ and the fact that the $0$-th Fourier coefficient of $f$ is it's integral over a period, one can conclude using a $3\varepsilon$ argument. This proof is possible because one can compute explicitly the sums $$\displaystyle \frac{1}{n}\sum_{k=0}^{n-1}e^{2i\pi j \alpha k}=\frac{1}{n}\cdot\frac{1-e^{2i\pi j\alpha n}}{1-e^{2i\pi j\alpha}}\longrightarrow 0 \text{ when }n\to\infty \text{ and }j\in \mathbf{Z}^*.$$ Now using a different approach (with dynamical systems and ergodic theorems) Tao show in his blog that $(\alpha k^2)_k  $ is uniformly distributed in $\mathbf R / \mathbf Z$ (corollary 2 in this blog ). I'd like to prove this result using the methods of the exercice of Rudin, but this reduce to show that $$\displaystyle \frac{1}{n}\sum_{k=0}^{n-1}e^{2i\pi j \alpha k^2}\longrightarrow 0 \text{ when }n\to\infty \text{ and }j\in \mathbf{Z}^*.$$ Hence my question. P.S. When i ask wolfram alpha to compute $\sum_{k\geq0}e^{ik^2}$ it answer me with some particular value of the Jacobi-theta function. Of course the serie is not convergent but maybe it's some kind of resummation technique or analytic continuation. I'm not familiar with these things but it might be interesting to look in that direction.","TL;DR : The question is how do I show that $\displaystyle \lim_{n\to\infty}\frac{1}{n}\sum_{k=0}^{n-1}e^{ik^2}=0$ ? More generaly the question would be : given an increasing sequence of integers $(u_k)$ and an irrational number $\alpha$, how do I tell if  $\displaystyle \lim_{n\to\infty}\frac{1}{n}\sum_{k=0}^{n-1}e^{2i\pi \alpha u_k}=0$ ? I'm not asking for a criterium for completely general sequences, an answer for sequences like $u_k=k^2$, $v_k=k!$ or $w_k=p(k)$ with $p\in \mathbf Z [X]$ would already be awesome. A little explanation about this question : In Real and Complex Analysis by Rudin there is the folowing exercise : Let $f$ be a continuous, complex valued, $1$-periodic function and $\alpha$ an irrational number. Show that  $\displaystyle \lim_{n\to\infty}\frac{1}{n}\sum_{k=0}^{n-1}f(\alpha k)=\int_0^1f(x)\mathrm d x$. (We say that $(\alpha k)_k$ is uniformly distributed in $\mathbf R / \mathbf Z$) With the hint given by Rudin the proof is pretty straightforward : First one show that this is true for every $f_j=\exp(2i\pi j\cdot)$ with $j\in \mathbf{Z} $. Then using density of trigonometric polynomials in $(C^0_1(\mathbf{R}),\|\cdot\|_\infty)$ and the fact that the $0$-th Fourier coefficient of $f$ is it's integral over a period, one can conclude using a $3\varepsilon$ argument. This proof is possible because one can compute explicitly the sums $$\displaystyle \frac{1}{n}\sum_{k=0}^{n-1}e^{2i\pi j \alpha k}=\frac{1}{n}\cdot\frac{1-e^{2i\pi j\alpha n}}{1-e^{2i\pi j\alpha}}\longrightarrow 0 \text{ when }n\to\infty \text{ and }j\in \mathbf{Z}^*.$$ Now using a different approach (with dynamical systems and ergodic theorems) Tao show in his blog that $(\alpha k^2)_k  $ is uniformly distributed in $\mathbf R / \mathbf Z$ (corollary 2 in this blog ). I'd like to prove this result using the methods of the exercice of Rudin, but this reduce to show that $$\displaystyle \frac{1}{n}\sum_{k=0}^{n-1}e^{2i\pi j \alpha k^2}\longrightarrow 0 \text{ when }n\to\infty \text{ and }j\in \mathbf{Z}^*.$$ Hence my question. P.S. When i ask wolfram alpha to compute $\sum_{k\geq0}e^{ik^2}$ it answer me with some particular value of the Jacobi-theta function. Of course the serie is not convergent but maybe it's some kind of resummation technique or analytic continuation. I'm not familiar with these things but it might be interesting to look in that direction.",,"['real-analysis', 'limits', 'exponential-sum', 'equidistribution']"
95,Power towers: to infinity and all the way back,Power towers: to infinity and all the way back,,"In the following, let $n$ be a positive integer, all other variables be real (furthermore, $a>1$), all functions be real-valued, and logarithms of negative arguments be undefined. Let $\log^n(x)$ denote the iterated natural logarithm (base $e$), with $x$ in the innermost position, $\operatorname{pow}_a^n(x)$ denote the iterated exponentiation (base $a$), with $x$ in the innermost position, where the superscript ${}^n$ to the right of a function name denotes the number of iterations of the function (not raising its result to a power). More precisely, $\hspace{.2in}\begin{cases} \log^1(x) = \ln x \\ \log^{n+1}(x) = \log^n(\ln x) \end{cases}$ $\hspace{.2in}\begin{cases} \operatorname{pow}_a^1(x) = a^x \\ \operatorname{pow}_a^{n+1}(x) = \operatorname{pow}_a^n(a^x) \end{cases}$ For example, $\log^3(x) = \ln \ln \ln x$, and $\operatorname{pow}_a^2(x) = a^{a^x}$. Now define $$\boxed{\phantom{\Bigg|}\hspace{0.2in}  f_a(x) = \lim\limits_{n\to\infty} \log^n(\operatorname{pow}_a^n(x)) \hspace{0.25in}}$$ In other words, $f_a(x)$ is the limit of the sequence $\{\ln a^x,\ \ln \ln a^{a^x},\ \ln \ln \ln a^{a^{a^x}},\ \dots\}$. Note that the first several elements of the sequence can be simplified, but next ones will end up with a repeated logarithm of a sum with the rest of the power tower sitting inside: $\{x \ln a,\ x \ln a+\ln \ln a,\ \ln\left(a^x \ln a+\ln \ln a\right),\ \ln \ln\left(a^{a^x}\ln a+\ln \ln a\right),\ \dots\}$. Obviously, $f_e(x)=x$. The behavior of the function for other values of $a$ is more interesting. Questions: Can any non-trivial ($a \ne e$) value of $f_a(x)$ with closed-form arguments be expressed in a closed form in terms of elementary functions, any known special functions, and any known mathematical constants? What is the domain of $f_a(1)$? Is $f_a(1)$ an analytic function within its domain? What is the domain of $f_2(x)$? Is $f_2(x)$ an analytic function within its domain? What it the range of $f_3(x)$? What is the value of $\lim\limits_{x \to \infty} \frac{f_2(x)}{x}$, if it exists? What is the asymptotic behavior of $f_2(x)$ as $x \to \infty$? What is the value of $\lim\limits_{x \to -\infty} f_3(x)$, if it exists? What is the asymptotic behavior of $f_3(x)$ as $x \to -\infty$? What is the Taylor expansion of $f_a(1)$ near $a=e$?","In the following, let $n$ be a positive integer, all other variables be real (furthermore, $a>1$), all functions be real-valued, and logarithms of negative arguments be undefined. Let $\log^n(x)$ denote the iterated natural logarithm (base $e$), with $x$ in the innermost position, $\operatorname{pow}_a^n(x)$ denote the iterated exponentiation (base $a$), with $x$ in the innermost position, where the superscript ${}^n$ to the right of a function name denotes the number of iterations of the function (not raising its result to a power). More precisely, $\hspace{.2in}\begin{cases} \log^1(x) = \ln x \\ \log^{n+1}(x) = \log^n(\ln x) \end{cases}$ $\hspace{.2in}\begin{cases} \operatorname{pow}_a^1(x) = a^x \\ \operatorname{pow}_a^{n+1}(x) = \operatorname{pow}_a^n(a^x) \end{cases}$ For example, $\log^3(x) = \ln \ln \ln x$, and $\operatorname{pow}_a^2(x) = a^{a^x}$. Now define $$\boxed{\phantom{\Bigg|}\hspace{0.2in}  f_a(x) = \lim\limits_{n\to\infty} \log^n(\operatorname{pow}_a^n(x)) \hspace{0.25in}}$$ In other words, $f_a(x)$ is the limit of the sequence $\{\ln a^x,\ \ln \ln a^{a^x},\ \ln \ln \ln a^{a^{a^x}},\ \dots\}$. Note that the first several elements of the sequence can be simplified, but next ones will end up with a repeated logarithm of a sum with the rest of the power tower sitting inside: $\{x \ln a,\ x \ln a+\ln \ln a,\ \ln\left(a^x \ln a+\ln \ln a\right),\ \ln \ln\left(a^{a^x}\ln a+\ln \ln a\right),\ \dots\}$. Obviously, $f_e(x)=x$. The behavior of the function for other values of $a$ is more interesting. Questions: Can any non-trivial ($a \ne e$) value of $f_a(x)$ with closed-form arguments be expressed in a closed form in terms of elementary functions, any known special functions, and any known mathematical constants? What is the domain of $f_a(1)$? Is $f_a(1)$ an analytic function within its domain? What is the domain of $f_2(x)$? Is $f_2(x)$ an analytic function within its domain? What it the range of $f_3(x)$? What is the value of $\lim\limits_{x \to \infty} \frac{f_2(x)}{x}$, if it exists? What is the asymptotic behavior of $f_2(x)$ as $x \to \infty$? What is the value of $\lim\limits_{x \to -\infty} f_3(x)$, if it exists? What is the asymptotic behavior of $f_3(x)$ as $x \to -\infty$? What is the Taylor expansion of $f_a(1)$ near $a=e$?",,"['real-analysis', 'sequences-and-series', 'limits', 'asymptotics', 'logarithms']"
96,Proof of Vitali's Convergence Theorem,Proof of Vitali's Convergence Theorem,,"This is an exercise from Rudin's Real and Complex Analysis . Prove the following convergence theorem of Vitali: Let $\mu(X)\lt \infty$ and suppose a sequence of functions, $\{f_n\}$ is uniformly integrable, $f_n(x)\to f(x)$ a.e. as $n\to \infty$, and $|f(x)|\lt \infty$ a.e., then $f\in L^1(\mu)$ and $$ \lim_{n\to\infty} \int_X |f_n-f|~d\mu = 0.$$ Attempt: Since $f_n$ is uniformly integrable, $\exists~\delta \gt 0$ such that whenever $\mu(E)\lt \delta$, we have $$\int_E |f_n|~d\mu \lt \frac{\varepsilon}{3} \quad \forall~n.$$ Since $\mu(X)\lt \infty$, Egoroff says that we can find a set $E$ such that $f_n \to f$ uniformly on $E^c$ and $\mu(E)\lt \delta$. So $\exists$ an $N$ such that for $n\gt N$ $$\int_{E^c} |f_n-f|~d\mu\lt \frac{\varepsilon}{3}.$$ So, $$\begin{align*} \int_X |f_n-f|~d\mu & = \int_{E^c} |f_n-f|~d\mu +\int_E |f_n-f|~d\mu\\ & \leq \int_{E^c} |f_n-f|~d\mu + \int_E |f|~d\mu + \int_E |f_n|~d\mu\\ & \lt \frac{\varepsilon}{3} + \frac{\varepsilon}{3} + \frac{\varepsilon}{3}\\ & =\varepsilon.    \end{align*}$$ Now to show that $f\in L^1(\mu)$, I have to show that $\int_X |f|\lt \infty.$  Somehow I feel I have to use Egoroff again but I'm kind of lost. I'd be grateful if someone could look over what I've done above and see if it's okay and perhaps provide a little help with showing the $f\in L^1(\mu)$. Thanks.","This is an exercise from Rudin's Real and Complex Analysis . Prove the following convergence theorem of Vitali: Let $\mu(X)\lt \infty$ and suppose a sequence of functions, $\{f_n\}$ is uniformly integrable, $f_n(x)\to f(x)$ a.e. as $n\to \infty$, and $|f(x)|\lt \infty$ a.e., then $f\in L^1(\mu)$ and $$ \lim_{n\to\infty} \int_X |f_n-f|~d\mu = 0.$$ Attempt: Since $f_n$ is uniformly integrable, $\exists~\delta \gt 0$ such that whenever $\mu(E)\lt \delta$, we have $$\int_E |f_n|~d\mu \lt \frac{\varepsilon}{3} \quad \forall~n.$$ Since $\mu(X)\lt \infty$, Egoroff says that we can find a set $E$ such that $f_n \to f$ uniformly on $E^c$ and $\mu(E)\lt \delta$. So $\exists$ an $N$ such that for $n\gt N$ $$\int_{E^c} |f_n-f|~d\mu\lt \frac{\varepsilon}{3}.$$ So, $$\begin{align*} \int_X |f_n-f|~d\mu & = \int_{E^c} |f_n-f|~d\mu +\int_E |f_n-f|~d\mu\\ & \leq \int_{E^c} |f_n-f|~d\mu + \int_E |f|~d\mu + \int_E |f_n|~d\mu\\ & \lt \frac{\varepsilon}{3} + \frac{\varepsilon}{3} + \frac{\varepsilon}{3}\\ & =\varepsilon.    \end{align*}$$ Now to show that $f\in L^1(\mu)$, I have to show that $\int_X |f|\lt \infty.$  Somehow I feel I have to use Egoroff again but I'm kind of lost. I'd be grateful if someone could look over what I've done above and see if it's okay and perhaps provide a little help with showing the $f\in L^1(\mu)$. Thanks.",,"['real-analysis', 'measure-theory']"
97,Measurability of $\xi$ in the mean value theorem,Measurability of  in the mean value theorem,\xi,"Suppose $f\in C^1(\mathbb{R})$, by mean value theorem, for any $x\in (0,\infty)$, there exists $\xi(x)\in (0,x)$ such that $$\frac{f(x)-f(0)}{x}=f'(\xi(x)).$$ My question is: Question : Can $\xi(x)$ always be chosen to be a measurable function in $x$?","Suppose $f\in C^1(\mathbb{R})$, by mean value theorem, for any $x\in (0,\infty)$, there exists $\xi(x)\in (0,x)$ such that $$\frac{f(x)-f(0)}{x}=f'(\xi(x)).$$ My question is: Question : Can $\xi(x)$ always be chosen to be a measurable function in $x$?",,['real-analysis']
98,Vitali set of outer-measure exactly $1$.,Vitali set of outer-measure exactly .,1,"I know that for any $\varepsilon\in (0,1]$ we can find a non-measurable subset (w.r.t Lebesgue measure) of $[0,1]$ so that its outer-measure equals exactly $\varepsilon$. It is done basicly with the traditional Vitali construction inside the interval $[0,\varepsilon]$ and noticing that such a set carries zero inner-mass, and thus its complement in $[0,\varepsilon]$ (being non-measurable as well) must carry the full outer-mass of $[0,\varepsilon]$. However, this resulting non-measurable set is a complement of the traditional Vitali constructed set. My question asks if the Vitali construction itself can yield a non-measurable set with outer-measure of exactly $1$ (or any before-hand decided number from $(0,1]$). Some modifications can be done inside the construction of course, but in particular I would like to stay away from taking complements. Maybe someone knows how this could be done? Any references and input is appreciated. Thanks in advance.","I know that for any $\varepsilon\in (0,1]$ we can find a non-measurable subset (w.r.t Lebesgue measure) of $[0,1]$ so that its outer-measure equals exactly $\varepsilon$. It is done basicly with the traditional Vitali construction inside the interval $[0,\varepsilon]$ and noticing that such a set carries zero inner-mass, and thus its complement in $[0,\varepsilon]$ (being non-measurable as well) must carry the full outer-mass of $[0,\varepsilon]$. However, this resulting non-measurable set is a complement of the traditional Vitali constructed set. My question asks if the Vitali construction itself can yield a non-measurable set with outer-measure of exactly $1$ (or any before-hand decided number from $(0,1]$). Some modifications can be done inside the construction of course, but in particular I would like to stay away from taking complements. Maybe someone knows how this could be done? Any references and input is appreciated. Thanks in advance.",,"['real-analysis', 'measure-theory', 'lebesgue-measure']"
99,Closing up the elementary functions under integration,Closing up the elementary functions under integration,,"The elementary real-valued functions are not closed under integration.  (Elementary function has a precise definition --  see Risch algorithm in Wikipedia).  This means there are elementary functions whose integrals are not elementary.  So we can construct a larger class of functions by adjoining all the integrals of elementary functions.  You can repeat this process indefinitely.  If I understand things correctly, the set of functions that is the countable closure of this process is closed under integration.  Does any finite iteration of the process achieve closure under integration? My guess is no.  Has anyone thought about this?","The elementary real-valued functions are not closed under integration.  (Elementary function has a precise definition --  see Risch algorithm in Wikipedia).  This means there are elementary functions whose integrals are not elementary.  So we can construct a larger class of functions by adjoining all the integrals of elementary functions.  You can repeat this process indefinitely.  If I understand things correctly, the set of functions that is the countable closure of this process is closed under integration.  Does any finite iteration of the process achieve closure under integration? My guess is no.  Has anyone thought about this?",,"['calculus', 'real-analysis']"
