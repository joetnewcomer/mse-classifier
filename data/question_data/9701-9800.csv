,title_raw,title_text,title_latex,body_raw,body_text,body_latex,tags
0,Existence and Uniqueness of a solution,Existence and Uniqueness of a solution,,"I'm stuck with this problem so hopefully somebody will help me :) Here you are: Let $K\in C([0,2])$ be positive, decreasing and such that $K(0)=1$. Prove that for every $h\in C([0,1])$ there exists a unique solution $u\in C([0,1])$ to the equation $$u(x)=h(x)+\int_0^1K(x+y)u(y)\mathrm d y,\qquad \forall x\in [0,1].$$","I'm stuck with this problem so hopefully somebody will help me :) Here you are: Let $K\in C([0,2])$ be positive, decreasing and such that $K(0)=1$. Prove that for every $h\in C([0,1])$ there exists a unique solution $u\in C([0,1])$ to the equation $$u(x)=h(x)+\int_0^1K(x+y)u(y)\mathrm d y,\qquad \forall x\in [0,1].$$",,"['calculus', 'real-analysis', 'functional-analysis', 'integral-equations']"
1,Smoothness of $f(\sqrt{x})$,Smoothness of,f(\sqrt{x}),"How to prove that if a function $f\colon\mathbb R\to\mathbb R$ is of the class $C^{2n}$ and even then there exists a function $g\colon\mathbb R \to\mathbb R$ of the class $C^n$ such that $f(x)=g(x^2)$. It is obvious, by using power series expansion in a symmetric neighbourhood of zero, that it holds if $f$ is analytic and even. But how to show that in general? Richard","How to prove that if a function $f\colon\mathbb R\to\mathbb R$ is of the class $C^{2n}$ and even then there exists a function $g\colon\mathbb R \to\mathbb R$ of the class $C^n$ such that $f(x)=g(x^2)$. It is obvious, by using power series expansion in a symmetric neighbourhood of zero, that it holds if $f$ is analytic and even. But how to show that in general? Richard",,['real-analysis']
2,Self-complete set in square,Self-complete set in square,,"Consider $Q = [0,1]^2$ and $\phi:Q\to \mathbb{R}_{\geq 0}$ such that $\phi$ is a Lipschitz continuous function with a constant, say $\lambda_\phi$. For each $x\in[0,1]$ we put $S(x) = supp \phi(x,\cdot)$ - i.e. $S(x)$ is a closure of the following set $$ (y\in[0,1]:\phi(x,y)>0). $$ We call a non-empty set $A\subseteq [0,1]$ self-complete if for any $x\in A$ holds $S(x)\subseteq A$. Could you give some ideas how to verify if there are self-complete sets? Any ideas are more than welcome. What I was able to prove is that $A$ is a set of a positive measure and if there is a self-complete set $A$ then its closure is also self-complete. P.S. The term self-complete I use myself - maybe there are intersection in a terminology. Edited: the problem can be reformulated as following: given an open set $G\subset Q$ verify if there is a closed non-empty set $A\subset [0,1]$ such that if $x\in A$ and $(x,y)\in \bar{G}$ then $y\in A$.","Consider $Q = [0,1]^2$ and $\phi:Q\to \mathbb{R}_{\geq 0}$ such that $\phi$ is a Lipschitz continuous function with a constant, say $\lambda_\phi$. For each $x\in[0,1]$ we put $S(x) = supp \phi(x,\cdot)$ - i.e. $S(x)$ is a closure of the following set $$ (y\in[0,1]:\phi(x,y)>0). $$ We call a non-empty set $A\subseteq [0,1]$ self-complete if for any $x\in A$ holds $S(x)\subseteq A$. Could you give some ideas how to verify if there are self-complete sets? Any ideas are more than welcome. What I was able to prove is that $A$ is a set of a positive measure and if there is a self-complete set $A$ then its closure is also self-complete. P.S. The term self-complete I use myself - maybe there are intersection in a terminology. Edited: the problem can be reformulated as following: given an open set $G\subset Q$ verify if there is a closed non-empty set $A\subset [0,1]$ such that if $x\in A$ and $(x,y)\in \bar{G}$ then $y\in A$.",,"['real-analysis', 'general-topology', 'measure-theory']"
3,Prove that $s_n$ is monotone and bounded,Prove that  is monotone and bounded,s_n,"Let the sequence $s_n$ be defined by $s_1$ = 1, $s_{n+1}$ = $\frac{1}{4} (2s_n + 5)$ for n $\in $ $\mathbb{N}$. The following is my proof and below it, my concerns. Proof (1) We will prove $s_n$ is increasing by induction. That is $s_n \leq s_{n+1}$ for all n $\in \mathbb{N}$. Since $$s_1 = 1 < s_2 = \frac{1}{4}(2(1)+5) = \frac{7}{4}$$ Now assume $s_k$ $\leq $ $s_{k+1}$ Then $$s_{k+2} = \frac{1}{4} (2s_{k+1} + 5) \geq \frac{1}{4} (2s_k +5) = s_{k+1}$$ Therefore, $s_n$ is a monotone sequence because it is increasing. (2a) We will prove $s_n$ is bounded above. That is, find $M \in \mathbb{R}$ such that $s_n \leq M$ for every $n \in \mathbb{N}$. Now, we try to prove $20$ is an upper bound. Since $s_1 = 1 < 20$ Suppose $s_k < 20$, then $$s_{k+1} = \frac{1}{4}(2s_k + 5) < \frac{1}{4}(2(20) + 5) = \frac{45}{4} < 20. $$  Hence, $s_n < 20$ for every n $\in \mathbb{N}$. Therefore, $s_n$ is bounded above. (2b) We will prove $s_n$ is bounded below. That is, find $M \in \mathbb{R}$ such that $s_n \geq M$ for every $n \in \mathbb{N}$. Now, we try to prove $0$ is a lower bound. Since $s_1 = 1 > 0$ Suppose $s_k > 0$, then $$s_{k+1} = \frac{1}{4}(2s_k + 5) > \frac{1}{4}(2(0) + 5) = \frac{5}{4} > 0. $$ Hence, $s_n > 0$ for every n $\in \mathbb{N}$. Therefore, $s_n$ is bounded below. Therefore $s_n$ is bounded. Concerns I want to confirm I did my Induction correctly. However, my main concern is in 2b.","Let the sequence $s_n$ be defined by $s_1$ = 1, $s_{n+1}$ = $\frac{1}{4} (2s_n + 5)$ for n $\in $ $\mathbb{N}$. The following is my proof and below it, my concerns. Proof (1) We will prove $s_n$ is increasing by induction. That is $s_n \leq s_{n+1}$ for all n $\in \mathbb{N}$. Since $$s_1 = 1 < s_2 = \frac{1}{4}(2(1)+5) = \frac{7}{4}$$ Now assume $s_k$ $\leq $ $s_{k+1}$ Then $$s_{k+2} = \frac{1}{4} (2s_{k+1} + 5) \geq \frac{1}{4} (2s_k +5) = s_{k+1}$$ Therefore, $s_n$ is a monotone sequence because it is increasing. (2a) We will prove $s_n$ is bounded above. That is, find $M \in \mathbb{R}$ such that $s_n \leq M$ for every $n \in \mathbb{N}$. Now, we try to prove $20$ is an upper bound. Since $s_1 = 1 < 20$ Suppose $s_k < 20$, then $$s_{k+1} = \frac{1}{4}(2s_k + 5) < \frac{1}{4}(2(20) + 5) = \frac{45}{4} < 20. $$  Hence, $s_n < 20$ for every n $\in \mathbb{N}$. Therefore, $s_n$ is bounded above. (2b) We will prove $s_n$ is bounded below. That is, find $M \in \mathbb{R}$ such that $s_n \geq M$ for every $n \in \mathbb{N}$. Now, we try to prove $0$ is a lower bound. Since $s_1 = 1 > 0$ Suppose $s_k > 0$, then $$s_{k+1} = \frac{1}{4}(2s_k + 5) > \frac{1}{4}(2(0) + 5) = \frac{5}{4} > 0. $$ Hence, $s_n > 0$ for every n $\in \mathbb{N}$. Therefore, $s_n$ is bounded below. Therefore $s_n$ is bounded. Concerns I want to confirm I did my Induction correctly. However, my main concern is in 2b.",,"['real-analysis', 'sequences-and-series']"
4,Lower hemicontinuity of the intersection of lower hemicontinuous correspondences,Lower hemicontinuity of the intersection of lower hemicontinuous correspondences,,"I have been stumped for long by this exercise (3.12(d)) from Stokey and Lucas's Recursive Methods in Economic Dynamics . Would greatly appreciate any hints. Let $\phi: X \to Y$ and $\psi: X \to Y$  be lower hemicontinuous correspondences (set-valued functions), and suppose that for all $x \in X$ $$\Gamma(x)=\{y \in Y: y \in \phi(x) \cap \psi(x)\}\neq \emptyset$$ Show that if $\phi$ and $\psi$ are both convex valued, and if $\mathrm{int} \phi(x) \cap \mathrm{int} \psi(x) \neq \emptyset$, then $\Gamma(x)$ is lower hemicontinuous at $x$. [A correspondence $\Gamma: X \to Y$ is said to be lower hemicontinuous at $x \in X$ if $\Gamma(x)$ is nonempty and if, for every $y \in \Gamma(x)$ and every sequence $x_n \to x$, there exists $N \geq 1$ and a sequence $\{y_n\}_{n=N}^\infty$ such that $y_n \to y$ and $y_n \in \Gamma(x_n)$, all $n \geq N$. Intuitively this means that the graph of $\Gamma(x)$ cannot suddenly broaden out.] EDIT : We can assume that $X$ and $Y$ are subsets of $\mathbf{R}^n$.","I have been stumped for long by this exercise (3.12(d)) from Stokey and Lucas's Recursive Methods in Economic Dynamics . Would greatly appreciate any hints. Let $\phi: X \to Y$ and $\psi: X \to Y$  be lower hemicontinuous correspondences (set-valued functions), and suppose that for all $x \in X$ $$\Gamma(x)=\{y \in Y: y \in \phi(x) \cap \psi(x)\}\neq \emptyset$$ Show that if $\phi$ and $\psi$ are both convex valued, and if $\mathrm{int} \phi(x) \cap \mathrm{int} \psi(x) \neq \emptyset$, then $\Gamma(x)$ is lower hemicontinuous at $x$. [A correspondence $\Gamma: X \to Y$ is said to be lower hemicontinuous at $x \in X$ if $\Gamma(x)$ is nonempty and if, for every $y \in \Gamma(x)$ and every sequence $x_n \to x$, there exists $N \geq 1$ and a sequence $\{y_n\}_{n=N}^\infty$ such that $y_n \to y$ and $y_n \in \Gamma(x_n)$, all $n \geq N$. Intuitively this means that the graph of $\Gamma(x)$ cannot suddenly broaden out.] EDIT : We can assume that $X$ and $Y$ are subsets of $\mathbf{R}^n$.",,"['real-analysis', 'convex-analysis', 'economics']"
5,Reference for the following claim,Reference for the following claim,,"In the paper On The Closure of Characters and the Zeros of Entire Functions by Beurling and Malliavin they make the following claim in the introduction. The closure radius $\rho = \rho(\Lambda)$ defined as the upper bound of the numbers $r$ such that set $\{e^{i \lambda x}\}_{\lambda \in \Lambda}$ span the space $L^2(-r, r)$ (by span we mean that the span of the set $\{e^{i\lambda t}\}_{\lambda \in \Lambda}$ is dense in $L^2(-r, r)$ ). The claim is that $\rho(\Lambda)$ does not change if the metric is replaced with any other $L^p$ metric. In other words, if I understand correctly the claim is that $\rho(\Lambda)$ is independent of $p$ in $L^p$ . How can be true? Surely, the topology should affect this somehow.","In the paper On The Closure of Characters and the Zeros of Entire Functions by Beurling and Malliavin they make the following claim in the introduction. The closure radius defined as the upper bound of the numbers such that set span the space (by span we mean that the span of the set is dense in ). The claim is that does not change if the metric is replaced with any other metric. In other words, if I understand correctly the claim is that is independent of in . How can be true? Surely, the topology should affect this somehow.","\rho = \rho(\Lambda) r \{e^{i \lambda x}\}_{\lambda \in \Lambda} L^2(-r, r) \{e^{i\lambda t}\}_{\lambda \in \Lambda} L^2(-r, r) \rho(\Lambda) L^p \rho(\Lambda) p L^p","['real-analysis', 'functional-analysis', 'reference-request', 'fourier-analysis']"
6,Can the set of sequences such that $\frac{b_n}{a_n}$ is unbounded be specified by a countable base of sequences $\{f_n\}$,Can the set of sequences such that  is unbounded be specified by a countable base of sequences,\frac{b_n}{a_n} \{f_n\},"Introduction : Let $A$ be the set of non-negative sequences $\{a_n\}$ such that $\sum_{n\geq 1} a_n=1$ . Suppose we have a positive sequence $\{a_n\}\in A$ . Consider the set $B\subseteq A$ of sequences $\{ b_n \}$ such that $\sum_{n\geq 1} b_n=1$ and $\left\{\frac{b_n}{a_n}\right\}$ is unbounded. Observe that both $B$ and $A\setminus B$ are convex. For any $\{b_n\}\in B$ there exists a sequence $\{f_n\}$ such that $\sum_{n\geq 1} a_n f_n<\infty$ and $\sum_{n\geq 1} b_n f_n=\infty$ . Indeed if we let $n_k$ be such that $k\leq \frac{b_{n_k}}{a_{n_k}}$ and we let $f_{n_k}=\frac{1}{a_{n_k} k^2}$ and $f_n=0$ otherwise, then \begin{align*} \sum_{n\geq 1} a_n f_n &= \sum_{k\geq 1} a_{n_k} f_{n_k}=\sum_{k\geq 1} \frac{1}{k^2}<\infty\\ \sum_{n\geq 1} b_n f_n &= \sum_{k\geq 1} b_{n_k} f_{n_k}=\sum_{k\geq 1} \frac{1}{k^2}\cdot\frac{b_{n_k}}{a_{n_k}}\geq \sum_{k\geq 1} \frac{1}{k}=\infty \end{align*} It is also clear that if $\{ b_n\}\in A\setminus B$ then there is $x>0$ such that $\left\{  \frac{b_n}{a_n}\right\}$ is bounded by $x$ , and so $\sum_{n\geq 1} b_n f_n\leq x\sum_{n\geq 1} a_n f_n$ and so if $\sum_{n\geq 1} a_n f_n<\infty$ then $\sum_{n\geq 1} b_n f_n<\infty$ . All this means that we can write \begin{align*} B &= \left\{ \{b_n\} \in A : \exists \{f_n\}, \sum_{n\geq 1} a_n f_n < \infty = \sum_{n\geq 1} b_n f_n \right\}\\ &=\bigcup_{\{ f_n \}\in F} B_{\{f_n\}} \end{align*} with $F=\left\{\{f_n\} : \sum_{n\geq 1} a_n f_n < \infty \right\}$ and $B_{\{f_n\}}=\left\{ \{b_n\} \in A : \sum_{n\geq 1} b_n f_n=\infty \right\}$ . Problem : My question is the following : Is there a countable subset $G\subset F$ such that $B=\bigcup_{\{f_n\}\in G} B_{\{f_n\}}$ ? Attempt : In case of a negative result, it might be possible to construct a ""Cantor's diagonal"" like argument by assuming that such a countable set doesn't exists. I have being trying to characterize the inclusion $B_{\{ f_n\}} \subseteq B_{\{ g_n \}}$ by giving an appropriate comparison test on $\{ f_n \}$ and $\{ g_n \}$ without success. Any idea would be very much appreciated.","Introduction : Let be the set of non-negative sequences such that . Suppose we have a positive sequence . Consider the set of sequences such that and is unbounded. Observe that both and are convex. For any there exists a sequence such that and . Indeed if we let be such that and we let and otherwise, then It is also clear that if then there is such that is bounded by , and so and so if then . All this means that we can write with and . Problem : My question is the following : Is there a countable subset such that ? Attempt : In case of a negative result, it might be possible to construct a ""Cantor's diagonal"" like argument by assuming that such a countable set doesn't exists. I have being trying to characterize the inclusion by giving an appropriate comparison test on and without success. Any idea would be very much appreciated.","A \{a_n\} \sum_{n\geq 1} a_n=1 \{a_n\}\in A B\subseteq A \{ b_n \} \sum_{n\geq 1} b_n=1 \left\{\frac{b_n}{a_n}\right\} B A\setminus B \{b_n\}\in B \{f_n\} \sum_{n\geq 1} a_n f_n<\infty \sum_{n\geq 1} b_n f_n=\infty n_k k\leq \frac{b_{n_k}}{a_{n_k}} f_{n_k}=\frac{1}{a_{n_k} k^2} f_n=0 \begin{align*}
\sum_{n\geq 1} a_n f_n &= \sum_{k\geq 1} a_{n_k} f_{n_k}=\sum_{k\geq 1} \frac{1}{k^2}<\infty\\
\sum_{n\geq 1} b_n f_n &= \sum_{k\geq 1} b_{n_k} f_{n_k}=\sum_{k\geq 1} \frac{1}{k^2}\cdot\frac{b_{n_k}}{a_{n_k}}\geq \sum_{k\geq 1} \frac{1}{k}=\infty
\end{align*} \{ b_n\}\in A\setminus B x>0 \left\{  \frac{b_n}{a_n}\right\} x \sum_{n\geq 1} b_n f_n\leq x\sum_{n\geq 1} a_n f_n \sum_{n\geq 1} a_n f_n<\infty \sum_{n\geq 1} b_n f_n<\infty \begin{align*}
B &= \left\{ \{b_n\} \in A : \exists \{f_n\}, \sum_{n\geq 1} a_n f_n < \infty = \sum_{n\geq 1} b_n f_n \right\}\\
&=\bigcup_{\{ f_n \}\in F} B_{\{f_n\}}
\end{align*} F=\left\{\{f_n\} : \sum_{n\geq 1} a_n f_n < \infty \right\} B_{\{f_n\}}=\left\{ \{b_n\} \in A : \sum_{n\geq 1} b_n f_n=\infty \right\} G\subset F B=\bigcup_{\{f_n\}\in G} B_{\{f_n\}} B_{\{ f_n\}} \subseteq B_{\{ g_n \}} \{ f_n \} \{ g_n \}","['real-analysis', 'sequences-and-series', 'measure-theory', 'convergence-divergence', 'convex-analysis']"
7,I have multiple questions regarding one problem: For a positive real $a$ what's the value of $\sqrt{a\sqrt{a\sqrt{a...}}}$,I have multiple questions regarding one problem: For a positive real  what's the value of,a \sqrt{a\sqrt{a\sqrt{a...}}},"I'm reading a book, the book is aimed at high schoolers so it is perhaps not the most rigorous book. The intended solutions the book provides for the problem are the following: First assume the expression has a value and call it $x$ , so $x = \sqrt{a\sqrt{a\sqrt{a...}}}$ Then notice that $x = \sqrt{ax}$ , then $x^2 = ax$ , then $x(x-a) = 0$ , then since $a$ is positive the solution is $x = a$ and that's the value of the whole expression. They also offer a second solution that goes like this: If $x = \sqrt{a\sqrt{a\sqrt{a...}}}$ then we can also express it as: $x = a^{\frac{1}{2}}a^{\frac{1}{4}}a^{\frac{1}{8}}...$ , then $x = a^{\frac{1}{2}+\frac{1}{4}+\frac{1}{8}+...}$ , and since $\frac{1}{2}+\frac{1}{4}+\frac{1}{8}+... = 1$ , then $x = a$ once again. My first question is, how valid are any of these solutions at the university level? Say if this was a real analysis class, would any of those two answers be considered valid? Also, although I haven't taken a real analysis class yet, I have heard of the $\epsilon-N$ for sequences and series, I assume if we want to do the same for this particular problem we would have to look at the sequence $\sqrt{a}, \sqrt{a\sqrt{a}}, \sqrt{a\sqrt{a\sqrt{a}}}, ...$ , then check if it converges using $\epsilon-N$ and if it does then that's the value of he expression $\sqrt{a\sqrt{a\sqrt{a...}}}$ Second question is, is my speculation correct? Would this way of doing it with $\epsilon-N$ constitute a valid way of solving the problem if this was a real analysis class? Lastly, my last question is, how would you solve the problem in a way that is valid for a real analysis class? Either using $\epsilon-N$ or whatever else, please show me in a way someone who hasn't taken a real analysis class before can understand.","I'm reading a book, the book is aimed at high schoolers so it is perhaps not the most rigorous book. The intended solutions the book provides for the problem are the following: First assume the expression has a value and call it , so Then notice that , then , then , then since is positive the solution is and that's the value of the whole expression. They also offer a second solution that goes like this: If then we can also express it as: , then , and since , then once again. My first question is, how valid are any of these solutions at the university level? Say if this was a real analysis class, would any of those two answers be considered valid? Also, although I haven't taken a real analysis class yet, I have heard of the for sequences and series, I assume if we want to do the same for this particular problem we would have to look at the sequence , then check if it converges using and if it does then that's the value of he expression Second question is, is my speculation correct? Would this way of doing it with constitute a valid way of solving the problem if this was a real analysis class? Lastly, my last question is, how would you solve the problem in a way that is valid for a real analysis class? Either using or whatever else, please show me in a way someone who hasn't taken a real analysis class before can understand.","x x = \sqrt{a\sqrt{a\sqrt{a...}}} x = \sqrt{ax} x^2 = ax x(x-a) = 0 a x = a x = \sqrt{a\sqrt{a\sqrt{a...}}} x = a^{\frac{1}{2}}a^{\frac{1}{4}}a^{\frac{1}{8}}... x = a^{\frac{1}{2}+\frac{1}{4}+\frac{1}{8}+...} \frac{1}{2}+\frac{1}{4}+\frac{1}{8}+... = 1 x = a \epsilon-N \sqrt{a}, \sqrt{a\sqrt{a}}, \sqrt{a\sqrt{a\sqrt{a}}}, ... \epsilon-N \sqrt{a\sqrt{a\sqrt{a...}}} \epsilon-N \epsilon-N","['real-analysis', 'infinite-product']"
8,Spiderman and the Cantor's function .,Spiderman and the Cantor's function .,,"Well it's an imaginative and soft question so take it as you wish . Problem : imagine Spider-man walking on the cantor function (see the plot like the side of a building) : Now he can throw a line to come faster to the top of the stairs . The problem is the spider-man webs can only solidify on an angle(i.e corner). Question : If spiderman have a limitied number of spider-webs which stay only on angle (corner) what is the probability he comes to the top of the stairs on a limited interval of the Cantor's function admitting it's his only means of transport ? Do you have a strategy ? I have no attempt because it's a new problem for me but Bernoulli's distribution could be a start  . Example of path to come on a horizontal line : The new question is : Finalizing Question : If Spiderman is a point on that curve and have the probability $P=1/2$ to get a rational point $(x_i,y_i)$ on the curve he choose (The Cantor set here) and stop if the point is irrational what's the probability he goes at the top of the devil staircase ( $x=y=1$ ) if he is assimilated to an increasing unknow $x$ starting to $x=0$ and $x_i<x_{i+1}$ .In other word what's the probability he diverges (projective geometrical point of view) to $x=y=1$ choosing a number of rational point which is bounded  ? We can bound it with the Fréchet-Boole's inequality . To figure it see : Another one and Thomae function The problem can be reformulate in term of Brownian motion : Simplifying question for kids : If now a kid plays with a Soap bubble and the bubble in a slight wind goes to a staircase what's chance the bubble don't collapse before the end of the next level of the home? Reference : https://en.m.wikipedia.org/wiki/Cantor_distribution https://en.wikipedia.org/wiki/Fr%C3%A9chet_inequalities https://en.wikipedia.org/wiki/Ford_circle https://arxiv.org/pdf/2007.08407v1.pdf https://arxiv.org/ftp/arxiv/papers/2205/2205.01925.pdf",Well it's an imaginative and soft question so take it as you wish . Problem : imagine Spider-man walking on the cantor function (see the plot like the side of a building) : Now he can throw a line to come faster to the top of the stairs . The problem is the spider-man webs can only solidify on an angle(i.e corner). Question : If spiderman have a limitied number of spider-webs which stay only on angle (corner) what is the probability he comes to the top of the stairs on a limited interval of the Cantor's function admitting it's his only means of transport ? Do you have a strategy ? I have no attempt because it's a new problem for me but Bernoulli's distribution could be a start  . Example of path to come on a horizontal line : The new question is : Finalizing Question : If Spiderman is a point on that curve and have the probability to get a rational point on the curve he choose (The Cantor set here) and stop if the point is irrational what's the probability he goes at the top of the devil staircase ( ) if he is assimilated to an increasing unknow starting to and .In other word what's the probability he diverges (projective geometrical point of view) to choosing a number of rational point which is bounded  ? We can bound it with the Fréchet-Boole's inequality . To figure it see : Another one and Thomae function The problem can be reformulate in term of Brownian motion : Simplifying question for kids : If now a kid plays with a Soap bubble and the bubble in a slight wind goes to a staircase what's chance the bubble don't collapse before the end of the next level of the home? Reference : https://en.m.wikipedia.org/wiki/Cantor_distribution https://en.wikipedia.org/wiki/Fr%C3%A9chet_inequalities https://en.wikipedia.org/wiki/Ford_circle https://arxiv.org/pdf/2007.08407v1.pdf https://arxiv.org/ftp/arxiv/papers/2205/2205.01925.pdf,"P=1/2 (x_i,y_i) x=y=1 x x=0 x_i<x_{i+1} x=y=1","['real-analysis', 'probability', 'soft-question', 'recreational-mathematics', 'fractals']"
9,"Prove that if $A,B\subseteq\mathbb R$ are non-empty and bounded from above, and $A+B=A$, it follows that $B=\{0\}$. Seems Wrong?","Prove that if  are non-empty and bounded from above, and , it follows that . Seems Wrong?","A,B\subseteq\mathbb R A+B=A B=\{0\}","A question from a basic textbook on real analysis: Let $A,B\subseteq\mathbb R$ be non-empty and bounded from above. Prove that if $A+B = \{a+b : a\in A,\,b\in B\} =A$ , it follows that $B = \{0\}$ . That seems wrong to me. For example, consider A = B = $\mathbb R_{\le0}$ . $A+B$ is therefore $\mathbb R_{\le0}$ as well. Am I missing something here?","A question from a basic textbook on real analysis: Let be non-empty and bounded from above. Prove that if , it follows that . That seems wrong to me. For example, consider A = B = . is therefore as well. Am I missing something here?","A,B\subseteq\mathbb R A+B = \{a+b : a\in A,\,b\in B\} =A B = \{0\} \mathbb R_{\le0} A+B \mathbb R_{\le0}",['real-analysis']
10,$L_1$-convergence in the Lebesgue differentiation theorem for general Radon measures?,-convergence in the Lebesgue differentiation theorem for general Radon measures?,L_1,"Let $\mu$ be a (non-negative) measure on $\mathbb R$ and let $f \in L_1(\mu)$ . For all $h > 0$ and ( $\mu$ -almost-)all $x \in \mathbb R$ , let $$ f_h(x) = \dfrac{\int\limits_{x-h}^{x+h} f(y)\ \mu(dy)}{\mu([x-h,x+h])}. $$ If $\mu$ is a Radon measure, then Lebesgue's differentiation theorem says that $f_h$ converges to $f$ $\mu$ -almost-everywhere (see e.g. Herbert Federer's ""Geometric Measure Theory"", Chapter 2.9). On the other hand, if $\mu$ is the Lebesgue measure, then it is known that $f_h \in L_1$ for all $h$ , and that $f_h$ converges to $f$ in $L_1$ (i.e. $\Vert f_h - f \Vert_{L_1} \to_{h\to 0} 0$ ). My question is: in general, when $\mu$ is any Radon measure, does $f_h$ converge to $f$ in $L_1(\mu)$ ? In fact, are the $f_h$ even in $L_1(\mu)$ ? And if not, are there additional assumptions on $\mu$ that make it true? (If this is indeed true, or at least true modulo assumptions on $\mu$ , then I'm also curious of any generalisations to spaces other than $\mathbb R$ , similar to how Lebesgue's differentiation theorem generalises to any metric space equipped with a Vitali relation.)","Let be a (non-negative) measure on and let . For all and ( -almost-)all , let If is a Radon measure, then Lebesgue's differentiation theorem says that converges to -almost-everywhere (see e.g. Herbert Federer's ""Geometric Measure Theory"", Chapter 2.9). On the other hand, if is the Lebesgue measure, then it is known that for all , and that converges to in (i.e. ). My question is: in general, when is any Radon measure, does converge to in ? In fact, are the even in ? And if not, are there additional assumptions on that make it true? (If this is indeed true, or at least true modulo assumptions on , then I'm also curious of any generalisations to spaces other than , similar to how Lebesgue's differentiation theorem generalises to any metric space equipped with a Vitali relation.)","\mu \mathbb R f \in L_1(\mu) h > 0 \mu x \in \mathbb R 
f_h(x) = \dfrac{\int\limits_{x-h}^{x+h} f(y)\ \mu(dy)}{\mu([x-h,x+h])}.
 \mu f_h f \mu \mu f_h \in L_1 h f_h f L_1 \Vert f_h - f \Vert_{L_1} \to_{h\to 0} 0 \mu f_h f L_1(\mu) f_h L_1(\mu) \mu \mu \mathbb R","['real-analysis', 'integration', 'measure-theory']"
11,Finding a closed form for $\sum_{k=1}^\infty\sum_{n=k}^\infty\left(\frac{(-1)^k}{k^3\binom{n+k}{k}\binom{n}{k}}(\frac1{n^2}-\frac1{(n+1)^2})\right)$,Finding a closed form for,\sum_{k=1}^\infty\sum_{n=k}^\infty\left(\frac{(-1)^k}{k^3\binom{n+k}{k}\binom{n}{k}}(\frac1{n^2}-\frac1{(n+1)^2})\right),"Consider the sum $$\sum_{n=k}^{N} \frac{1}{\binom{n+k}{k}\binom{n}{k}}\left(\frac{1}{n^2}-\frac{1}{(n+1)^2}\right) $$ Using the above or otherwise I need a closed form for $$\sum_{k=1}^{\infty}\sum_{n=k}^{\infty}\left(\frac{(-1)^k}{k^3 \binom{n+k}{k}\binom{n}{k}}\left(\frac{1}{n^2}-\frac{1}{(n+1)^2}\right)\right) $$ I tried as follows: Denote $$S_{N,k}:=\sum_{n=k}^{N} \frac{1}{\binom{n+k}{k}\binom{n}{k}}\left(\frac{1}{n^2}-\frac{1}{(n+1)^2}\right) $$ On writing the formula for $n\choose k$ we get $$S_{N,k}:=\sum_{n=k}^{N} \frac{(n-k)!}{(n+k)!}\left(\frac{1}{n^2}-\frac{1}{(n+1)^2}\right) $$ Now $$\frac{(n-k)!}{(n+k)!}=\frac{(n-k)(n-k-1)!}{(n+k)!} $$ Using partial fractions we have $$\frac{(n-k-1)!}{(n+k)!}=\frac{1}{(n-k)...(n+k)}=\frac{1}{(2k)!(n-k)}-\frac{1}{(2k-1)!1!(n-k+1)}+\frac{1}{(2k-2)!2!(n-k+2)}-... $$ So we have $$\frac{(n-k-1)!}{(n+k)!}=\frac{1}{(2k)!}\sum_{m=0}^{2k}\frac{(-1)^m \binom{2k}{m}}{n-k+m}$$ Plugging the above partial fraction decomposition in $S_{N,k}$ we get $$S_{N,k}=\frac{1}{(2k)!}\sum_{n=k}^{N} \left(\sum_{m=0}^{2k}\frac{(-1)^m \binom{2k}{m} (n-k)}{n-k+m}\right)\left(\frac{1}{n^2}-\frac{1}{(n+1)^2}\right) $$ $$S_{N,k}=\frac{1}{(2k)!}\sum_{n=k}^{N} \left(\sum_{m=0}^{2k}(-1)^m \binom{2k}{m} \left(1-\frac{m}{n-k+m}\right)\right)\left(\frac{1}{n^2}-\frac{1}{(n+1)^2}\right) $$ Now since $$\sum_{m=0}^{2k}(-1)^m \binom{2k}{m}=0$$ So we get $$S_{N,k}=-\frac{1}{(2k)!}\sum_{n=k}^{N} \left(\sum_{m=1}^{2k} \frac{(-1)^m\ m \ \binom{2k}{m}}{n-k+m}\right)\left(\frac{1}{n^2}-\frac{1}{(n+1)^2}\right) $$ After this I am unable to simplify further. Please help me to find the sum. Edit Using the above or otherwise I need a closed form for $$\sum_{k=1}^{\infty}\sum_{n=k}^{\infty}\left(\frac{(-1)^k}{k^3 \binom{n+k}{k}\binom{n}{k}}\left(\frac{1}{n^2}-\frac{1}{(n+1)^2}\right)\right) $$",Consider the sum Using the above or otherwise I need a closed form for I tried as follows: Denote On writing the formula for we get Now Using partial fractions we have So we have Plugging the above partial fraction decomposition in we get Now since So we get After this I am unable to simplify further. Please help me to find the sum. Edit Using the above or otherwise I need a closed form for,"\sum_{n=k}^{N} \frac{1}{\binom{n+k}{k}\binom{n}{k}}\left(\frac{1}{n^2}-\frac{1}{(n+1)^2}\right)  \sum_{k=1}^{\infty}\sum_{n=k}^{\infty}\left(\frac{(-1)^k}{k^3 \binom{n+k}{k}\binom{n}{k}}\left(\frac{1}{n^2}-\frac{1}{(n+1)^2}\right)\right)  S_{N,k}:=\sum_{n=k}^{N} \frac{1}{\binom{n+k}{k}\binom{n}{k}}\left(\frac{1}{n^2}-\frac{1}{(n+1)^2}\right)  n\choose k S_{N,k}:=\sum_{n=k}^{N} \frac{(n-k)!}{(n+k)!}\left(\frac{1}{n^2}-\frac{1}{(n+1)^2}\right)  \frac{(n-k)!}{(n+k)!}=\frac{(n-k)(n-k-1)!}{(n+k)!}  \frac{(n-k-1)!}{(n+k)!}=\frac{1}{(n-k)...(n+k)}=\frac{1}{(2k)!(n-k)}-\frac{1}{(2k-1)!1!(n-k+1)}+\frac{1}{(2k-2)!2!(n-k+2)}-...  \frac{(n-k-1)!}{(n+k)!}=\frac{1}{(2k)!}\sum_{m=0}^{2k}\frac{(-1)^m \binom{2k}{m}}{n-k+m} S_{N,k} S_{N,k}=\frac{1}{(2k)!}\sum_{n=k}^{N} \left(\sum_{m=0}^{2k}\frac{(-1)^m \binom{2k}{m} (n-k)}{n-k+m}\right)\left(\frac{1}{n^2}-\frac{1}{(n+1)^2}\right)  S_{N,k}=\frac{1}{(2k)!}\sum_{n=k}^{N} \left(\sum_{m=0}^{2k}(-1)^m \binom{2k}{m} \left(1-\frac{m}{n-k+m}\right)\right)\left(\frac{1}{n^2}-\frac{1}{(n+1)^2}\right)  \sum_{m=0}^{2k}(-1)^m \binom{2k}{m}=0 S_{N,k}=-\frac{1}{(2k)!}\sum_{n=k}^{N} \left(\sum_{m=1}^{2k} \frac{(-1)^m\ m \ \binom{2k}{m}}{n-k+m}\right)\left(\frac{1}{n^2}-\frac{1}{(n+1)^2}\right)  \sum_{k=1}^{\infty}\sum_{n=k}^{\infty}\left(\frac{(-1)^k}{k^3 \binom{n+k}{k}\binom{n}{k}}\left(\frac{1}{n^2}-\frac{1}{(n+1)^2}\right)\right) ","['real-analysis', 'sequences-and-series', 'number-theory', 'summation', 'binomial-coefficients']"
12,If for each point there is an iteration of $f$ which is equal to $0$ then some iteration of $f$ is identically $0$,If for each point there is an iteration of  which is equal to  then some iteration of  is identically,f 0 f 0,"Hello fellow mathematicians, Consider a continuous function $f:[0,1]\rightarrow [0,1]$ such that $f(0)=0$ , and for every $x\in [0,1]$ there exists $k=k(x)>0$ such that the $k$ th iteration of $f$ at $x$ is equal to $0$ , i.e. $f^{(k)}(x)=0$ . I want to show that there exists $n>0$ such that $f^{(n)}(x)=0$ for all $x\in [0,1]$ . I defined the sets $A_k=\{x\in [0,1] \mid f^{(k)}(x)=0\}$ and noticed that they are increasing and cover $[0,1]$ but it doesn't seem to help much. Any ideas?","Hello fellow mathematicians, Consider a continuous function such that , and for every there exists such that the th iteration of at is equal to , i.e. . I want to show that there exists such that for all . I defined the sets and noticed that they are increasing and cover but it doesn't seem to help much. Any ideas?","f:[0,1]\rightarrow [0,1] f(0)=0 x\in [0,1] k=k(x)>0 k f x 0 f^{(k)}(x)=0 n>0 f^{(n)}(x)=0 x\in [0,1] A_k=\{x\in [0,1] \mid f^{(k)}(x)=0\} [0,1]","['real-analysis', 'functions']"
13,Does every equalizing sequence converge?,Does every equalizing sequence converge?,,"Let $(a_1,a_2, \dots, a_n) = (0, 0, \dots, 0)$ . There are sets $B_1, \dots, B_k\subseteq\{1,2,\dots,n\}$ , each of size $2$ . At time step $t$ , consider the index $i\in\{1,\dots,k\}$ such that $i\equiv t\pmod k$ , and let the two numbers in $B_i$ be $x$ and $y$ , where $a_x \ge a_y$ . If $a_x \ge a_y + 1$ , we increase $a_y$ by $1$ . Else, we increase $a_y$ until it is equal to $a_x$ , then increase both $a_y$ and $a_x$ until the total amount we increased is $1$ . Let $(b_1, \dots, b_n) = (a_1/t, \dots, a_n/t)$ . Is it true that, as $t\rightarrow\infty$ , the sequence $(b_1,\dots,b_n)$ converges? Example 1: $n = 3$ , $k=2$ , $B_1 = \{1,2\}$ , $B_2 = \{2,3\}$ . At $t = 1$ , we update $(a_1, a_2, a_3)$ to $(0.5,0.5,0)$ . At $t = 2$ , it becomes $(0.5, 0.75, 0.75)$ , so $(b_1,b_2,b_3) = (0.25,0.375,0.375)$ . At $t = 3$ , $(a_1,a_2,a_3) = (1.125,1.125,0.75)$ and $(b_1,b_2,b_3) = (0.375,0.375,0.25)$ . Eventually, $(b_1,b_2,b_3)$ converges to $(1/3,1/3,1/3)$ . Example 2: $n = 3$ , $k=4$ , $B_1 =B_2 = B_3 = \{1,2\}$ , $B_4 = \{2,3\}$ . At $t = 1$ , $(a_1, a_2, a_3)=(\frac12,\frac12,0)$ . At $t = 2$ , $(a_1, a_2, a_3)=(1, 1, 0)$ and $(b_1,b_2,b_3) = (\frac12,\frac12,0)$ . At $t = 3$ , $(a_1,a_2,a_3) = (\frac32,\frac32,0)$ and $(b_1,b_2,b_3) = (\frac12,\frac12,0)$ . At $t = 4$ , $(a_1,a_2,a_3) = (\frac32,\frac32,1)$ and $(b_1,b_2,b_3) = (\frac38,\frac38,\frac14)$ . Eventually, $(b_1,b_2,b_3)$ converges to $(\frac38,\frac38,\frac14)$ .","Let . There are sets , each of size . At time step , consider the index such that , and let the two numbers in be and , where . If , we increase by . Else, we increase until it is equal to , then increase both and until the total amount we increased is . Let . Is it true that, as , the sequence converges? Example 1: , , , . At , we update to . At , it becomes , so . At , and . Eventually, converges to . Example 2: , , , . At , . At , and . At , and . At , and . Eventually, converges to .","(a_1,a_2, \dots, a_n) = (0, 0, \dots, 0) B_1, \dots, B_k\subseteq\{1,2,\dots,n\} 2 t i\in\{1,\dots,k\} i\equiv t\pmod k B_i x y a_x \ge a_y a_x \ge a_y + 1 a_y 1 a_y a_x a_y a_x 1 (b_1, \dots, b_n) = (a_1/t, \dots, a_n/t) t\rightarrow\infty (b_1,\dots,b_n) n = 3 k=2 B_1 = \{1,2\} B_2 = \{2,3\} t = 1 (a_1, a_2, a_3) (0.5,0.5,0) t = 2 (0.5, 0.75, 0.75) (b_1,b_2,b_3) = (0.25,0.375,0.375) t = 3 (a_1,a_2,a_3) = (1.125,1.125,0.75) (b_1,b_2,b_3) = (0.375,0.375,0.25) (b_1,b_2,b_3) (1/3,1/3,1/3) n = 3 k=4 B_1 =B_2 = B_3 = \{1,2\} B_4 = \{2,3\} t = 1 (a_1, a_2, a_3)=(\frac12,\frac12,0) t = 2 (a_1, a_2, a_3)=(1, 1, 0) (b_1,b_2,b_3) = (\frac12,\frac12,0) t = 3 (a_1,a_2,a_3) = (\frac32,\frac32,0) (b_1,b_2,b_3) = (\frac12,\frac12,0) t = 4 (a_1,a_2,a_3) = (\frac32,\frac32,1) (b_1,b_2,b_3) = (\frac38,\frac38,\frac14) (b_1,b_2,b_3) (\frac38,\frac38,\frac14)","['real-analysis', 'sequences-and-series', 'convergence-divergence', 'dynamical-systems']"
14,Riemann-Liouville integral of $f$ is zero implies $f =0$ a.e.,Riemann-Liouville integral of  is zero implies  a.e.,f f =0,"The Riemann-Liouville integral is defined by $$ I^\alpha f(x)=\frac{1}{\Gamma(\alpha)} \int_a^x f(t)(x-t)^{\alpha-1} d t $$ where $\Gamma$ is the gamma function and $a$ is an arbitrary but fixed base point. Take $a = 0$ and $\alpha = 1/2$ . Therefore we look at: $$ I^{\frac{1}{2}} f(x) := \frac{1}{\Gamma(1/2)} \int_0^x \frac{f(t)}{\sqrt{x-t}} d t $$ Suppose $I^{\frac{1}{2}}f(x) = 0$ for all $x$ . Can we then conclude $f=0$ a.e.? My approach so far has been to take the Fourier transform and use the convolution theorem. I cannot conclude because I do not know if $f \in L^2(\mathbb{R})$ . Otherwise, I could conclude just by using the fact that the Fourier transform is an isometry between $L^2$ spaces. See here for the same question on MathOverflow.","The Riemann-Liouville integral is defined by where is the gamma function and is an arbitrary but fixed base point. Take and . Therefore we look at: Suppose for all . Can we then conclude a.e.? My approach so far has been to take the Fourier transform and use the convolution theorem. I cannot conclude because I do not know if . Otherwise, I could conclude just by using the fact that the Fourier transform is an isometry between spaces. See here for the same question on MathOverflow.","
I^\alpha f(x)=\frac{1}{\Gamma(\alpha)} \int_a^x f(t)(x-t)^{\alpha-1} d t
 \Gamma a a = 0 \alpha = 1/2 
I^{\frac{1}{2}} f(x) := \frac{1}{\Gamma(1/2)} \int_0^x \frac{f(t)}{\sqrt{x-t}} d t
 I^{\frac{1}{2}}f(x) = 0 x f=0 f \in L^2(\mathbb{R}) L^2","['real-analysis', 'complex-analysis', 'analysis', 'measure-theory', 'riemann-integration']"
15,Interchanging limits when computing the Fourier transform,Interchanging limits when computing the Fourier transform,,In the top answer of this post here why can we interchange the two limits $\lim_{R\rightarrow \infty}$ and $\lim_{\epsilon \rightarrow 0^+}$ ?,In the top answer of this post here why can we interchange the two limits and ?,\lim_{R\rightarrow \infty} \lim_{\epsilon \rightarrow 0^+},['real-analysis']
16,"Polynomial that grows faster than any other polynomial outside $[−1,1]^n$",Polynomial that grows faster than any other polynomial outside,"[−1,1]^n","Consider this statement: ""Chebyshev polynomials increase in magnitude more quickly outside the range $[−1,1]$ than any other polynomial that is restricted to have magnitude no greater than one inside the range $[−1,1]$ ."" In the multivariate case, is there a degree $d$ polynomial that lies in $[-1,1]$ in the interval $[-1,1]^n$ and grows faster than any other polynomial of degree $d$ outside the $[-1,1]^n$ ? If so, what is this polynomial? It is natural to conjecture that $q(x_1,..,,x_n) = T_{d/n}(x_1)T_{d/n}(x_2)..T_{d/n}(x_n)$ (assuming $n|d$ ) is this polynomial ( $T_i(x)$ is the degree $i$ Chebyshev polynomial). For the univariate case, a proof can be found here . I have tried to generalize this proof to the bivariate case, but the fact that bivariate polynomials can have infinitely many roots (while univariate polynomials can have only at most degree-many roots) seems to be a barrier in adapting the proof.","Consider this statement: ""Chebyshev polynomials increase in magnitude more quickly outside the range than any other polynomial that is restricted to have magnitude no greater than one inside the range ."" In the multivariate case, is there a degree polynomial that lies in in the interval and grows faster than any other polynomial of degree outside the ? If so, what is this polynomial? It is natural to conjecture that (assuming ) is this polynomial ( is the degree Chebyshev polynomial). For the univariate case, a proof can be found here . I have tried to generalize this proof to the bivariate case, but the fact that bivariate polynomials can have infinitely many roots (while univariate polynomials can have only at most degree-many roots) seems to be a barrier in adapting the proof.","[−1,1] [−1,1] d [-1,1] [-1,1]^n d [-1,1]^n q(x_1,..,,x_n) = T_{d/n}(x_1)T_{d/n}(x_2)..T_{d/n}(x_n) n|d T_i(x) i","['real-analysis', 'polynomials', 'numerical-methods', 'approximation-theory', 'chebyshev-polynomials']"
17,(In)dependence of two stochastic processes,(In)dependence of two stochastic processes,,"Consider the C-shaped region $C\subset \mathbb{R}^2$ shown below, which has arms I, II $\subset C$ given by I $= I_0\times I_1$ and II $= I_0\times I_2$ for some intervals $I_0, I_1, I_2\subset\mathbb{R}$ , with $I_1$ and $I_2$ disjoint. Let further $r : C\rightarrow \mathbb{R}^2$ be a $C^1$ -diffeomorphism which acts trivially on $C$ in the sense that $$\tag{1} r(x,y) = (\alpha_1(x), y) \ \ \forall\, (x,y)\in \text{I}, \qquad \text{and} \qquad r(x,y) = (\alpha_2(x), y) \ \ \forall\, (x,y)\in \text{II} $$ for some $\alpha_i : I_0\rightarrow \mathbb{R}$ strictly monotone, and $\left.r\right|_{C\setminus{(\text{I}\cup\text{II})}} = \left.\mathrm{id}\right|_{C\setminus{(\text{I}\cup\text{II})}}$ . Suppose now that $X = (X^1_t, X^2_t)_{t\in[0,1]}$ is a continuous-time stochastic process in $\mathbb{R}^2$ that evolves inside the C-shaped region shown above, and let the process $$\tag{2} Y=(Y^1_t, Y^2_t)_{t\in[0,1]}:=r(X)\equiv(r(X^1_t, X^2_t))_{t\in[0,1]}$$ be the image of $X$ under $r$ . The components $X^1$ and $X^2$ of $X$ are assumed to be statistically independent My question : Assume that $\alpha_1\neq \alpha_2$ with $\alpha_1(U) > \alpha_2(U)$ for some (nonempty) open $U\subset I_0$ and $\mathbb{P}(X_{t_j}\in U\times I_j)>0$ , $j=1,2$ , for some $t_1, t_2\in [0,1]$ . Does this imply that the processes $Y^1$ and $Y^2$ must be statistically dependent? Remark: It seems plausible that $Y^1$ and $Y^2$ must be dependent since (by the fact that $\alpha_1$ and $\alpha_2$ are different) the evolution of $Y^2(=X^2)$ influences the trajectories of $Y^1$ via the arm I, II that $Y=(Y^1,Y^2)$ lands in. Yet it seems to me that the given assumptions might be too weak to actually prove the independence of $Y^1$ and $Y^2$ .","Consider the C-shaped region shown below, which has arms I, II given by I and II for some intervals , with and disjoint. Let further be a -diffeomorphism which acts trivially on in the sense that for some strictly monotone, and . Suppose now that is a continuous-time stochastic process in that evolves inside the C-shaped region shown above, and let the process be the image of under . The components and of are assumed to be statistically independent My question : Assume that with for some (nonempty) open and , , for some . Does this imply that the processes and must be statistically dependent? Remark: It seems plausible that and must be dependent since (by the fact that and are different) the evolution of influences the trajectories of via the arm I, II that lands in. Yet it seems to me that the given assumptions might be too weak to actually prove the independence of and .","C\subset \mathbb{R}^2 \subset C = I_0\times I_1 = I_0\times I_2 I_0, I_1, I_2\subset\mathbb{R} I_1 I_2 r : C\rightarrow \mathbb{R}^2 C^1 C \tag{1}
r(x,y) = (\alpha_1(x), y) \ \ \forall\, (x,y)\in \text{I}, \qquad \text{and} \qquad r(x,y) = (\alpha_2(x), y) \ \ \forall\, (x,y)\in \text{II}
 \alpha_i : I_0\rightarrow \mathbb{R} \left.r\right|_{C\setminus{(\text{I}\cup\text{II})}} = \left.\mathrm{id}\right|_{C\setminus{(\text{I}\cup\text{II})}} X = (X^1_t, X^2_t)_{t\in[0,1]} \mathbb{R}^2 \tag{2} Y=(Y^1_t, Y^2_t)_{t\in[0,1]}:=r(X)\equiv(r(X^1_t, X^2_t))_{t\in[0,1]} X r X^1 X^2 X \alpha_1\neq \alpha_2 \alpha_1(U) > \alpha_2(U) U\subset I_0 \mathbb{P}(X_{t_j}\in U\times I_j)>0 j=1,2 t_1, t_2\in [0,1] Y^1 Y^2 Y^1 Y^2 \alpha_1 \alpha_2 Y^2(=X^2) Y^1 Y=(Y^1,Y^2) Y^1 Y^2","['real-analysis', 'probability', 'probability-theory', 'stochastic-processes', 'stochastic-calculus']"
18,Is it true that if two disjoint connected open sets in $\mathbb{R}^n$ have the same boundary then the boundary must be connected?,Is it true that if two disjoint connected open sets in  have the same boundary then the boundary must be connected?,\mathbb{R}^n,Let us have two disjoint connected open sets $C_1$ and $C_2$ in $\mathbb{R}^n$ such that $\partial C_1 = \partial C_2$ . My claim is that $\partial C_1$ must be connected. I cannot rigorously prove it but my idea is as follows. Assume that $\partial C_1$ is not connected and let $x$ and $y$ be two points from two different connected components of $\partial C_1$ . Since open connectedness implies path connectedness in $\mathbb{R}^n$ there are paths connecting $x$ and $y$ in both $C_1$ and $C_2$ . So we can form a closed curve passing through the points $x$ and $y$ and also through $C_1$ and $C_2$ . I am pretty sure that this curve cannot be shrinked to a point. But I don't know how I can write it mathematically. I appreciate it if someone can help. Thanks.,Let us have two disjoint connected open sets and in such that . My claim is that must be connected. I cannot rigorously prove it but my idea is as follows. Assume that is not connected and let and be two points from two different connected components of . Since open connectedness implies path connectedness in there are paths connecting and in both and . So we can form a closed curve passing through the points and and also through and . I am pretty sure that this curve cannot be shrinked to a point. But I don't know how I can write it mathematically. I appreciate it if someone can help. Thanks.,C_1 C_2 \mathbb{R}^n \partial C_1 = \partial C_2 \partial C_1 \partial C_1 x y \partial C_1 \mathbb{R}^n x y C_1 C_2 x y C_1 C_2,"['real-analysis', 'general-topology', 'geometric-topology']"
19,"If $f'$ is integrable on $[a,b]$, what can we say about $f$?","If  is integrable on , what can we say about ?","f' [a,b] f","Suppose $f:[a,b]\to\mathbb R$ is a differentiable function with $f'\in L^1 [a,b]$ , that is, $f$ has a derivative that is integrable on $[a,b]$ . What are some properties $f$ must have? If any are known, some defining properties would be nice, that is, properties $P$ such that $f$ has $P$ if and only if $f'$ is integrable on $[a,b]$ . One thing we can say is that $f$ must be Lipschitz-continuous. This shouldn't be the best we can do: Lipschitz-continuity follows from the boundedness of $f'$ , but we know from Darboux's theorem that $f'$ also has the intermediate value property, a very restrictive property. I'm not sure how we could leverage this to get some insight into the nature of $f$ . Any ideas?","Suppose is a differentiable function with , that is, has a derivative that is integrable on . What are some properties must have? If any are known, some defining properties would be nice, that is, properties such that has if and only if is integrable on . One thing we can say is that must be Lipschitz-continuous. This shouldn't be the best we can do: Lipschitz-continuity follows from the boundedness of , but we know from Darboux's theorem that also has the intermediate value property, a very restrictive property. I'm not sure how we could leverage this to get some insight into the nature of . Any ideas?","f:[a,b]\to\mathbb R f'\in L^1 [a,b] f [a,b] f P f P f' [a,b] f f' f' f","['real-analysis', 'calculus', 'integration', 'derivatives']"
20,"How did Euler obtain this ratio of infinite series from a continued fraction, the terms of the series not being equal to the convergents?","How did Euler obtain this ratio of infinite series from a continued fraction, the terms of the series not being equal to the convergents?",,"If one feels disinclined to read the contextual preamble, I have made some partial progress in clarifying the question. Skip to the bottom! The motivation behind this is to understand the derivation for the simple continued fractions of $\tanh(1/k)$ , $e^{2/k}$ : Euler’s work has been the only source I could find which attempts to explain the results. One may find here a translation of one of Euler's essays on continued fractions. Therein, he claims to prove some celebrated continued fraction formulae (with the proof starting in the final sections of the paper). I am finding his work very hard to follow, not from the complexity of the mathematics but more from his very terse style of writing! I am talking about section 31. For some $a,n$ he considers: $$s=a+\frac{1}{(1+n)a+\frac{1}{(1+2n)a+\frac{1}{(1+3n)a+\cdots}}}$$ And builds a series of approximations using his formula in section $10,14$ : $$a,\frac{(1+n)a^2+1}{(1+n)a},\frac{(1+n)(1+2n)a^3+(2+2n)a}{(1+n)(1+2n)a^2+1},\text{etc.}$$ Which are the first few convergents. Now, in section $32$ he continues: ""If these fractions are continued further the law by which they are formed will be easily observed. From this law it may be concluded that after both the numerator and the denominator are divided by the first term of the denominator, the limiting fraction will be:"" $$\frac{a+\frac{1}{1\cdot na}+\frac{1}{1\cdot2\cdot1(1+n)n^2a^3}+\frac{1}{1\cdot2\cdot3\cdot1(1+n)(1+2n)n^3a^5}+\text{etc.}}{1+\frac{1}{1(1+n)na^2}+\frac{1}{1\cdot2(1+n)(1+2n)n^2a^4}+\frac{1}{1\cdot2\cdot3(1+n)(1+2n)(1+3n)n^3a^6}+\text{etc.}}$$ Now, he's not wrong - the numerics check out to an excellent accuracy. However I am really confused about where this fraction comes from; no multiplication of terms in the numerator or denominator comes to mind that reconciles that with the sequence of convergents. Moreover, the ratio of truncated series does not exactly equal any of the convergents, so not only do I need to guess the manipulation, but the manipulation leaves you with some kind of vanishing error term. In particular, what he intended the ""first term"" of the denominator to be, I really don't know - that seems the crux of the matter. I calculated the above ratio, truncated at the $\cdots$ , and compared it to the fourth convergent: $$\frac{\left(1+3n\right)a+3\left(1+2n\right)\left(1+3n\right)na^{3}+6\left(1+n\right)\left(1+2n\right)\left(1+3n\right)n^{2}a^{5}+6\left(1+n\right)\left(1+2n\right)\left(1+3n\right)n^{3}a^{7}}{6\left(1+n\right)\left(1+2n\right)\left(1+3n\right)n^{3}a^{6}+6\left(1+2n\right)\left(1+3n\right)n^{2}a^{4}+3\left(1+3n\right)na^{2}+1}$$ Versus the convergent: $$\frac{(1+n)(1+2n)(1+3n)a^4+3(1+n)(1+2n)a^2+1}{(1+n)(1+2n)(1+3n)a^3+2(1+2n)a}$$ But I see no clear link. I would really appreciate it if anyone could comment suggestions or full answers as to what link Euler intended, to obtain the ratio of infinite series. EDIT: some progress: I discovered through a pattern recognition that the numerators of the convergents $(p_k)_{k=1}^\infty$ , indexing from $p_1:=a$ , satisfy: $$p_k=\sum_{m=0}^{\lfloor k/2\rfloor}\binom{k-m}{m}\left(\prod_{\nu=m}^{k-m-1}(1+n\nu)\right)a^{k-2m}$$ I also find that the denominators $q_k$ , indexing with $q_1=1$ , satisfy ( $k>1$ ): $$q_k=\sum_{m=0}^{\lfloor(k-1)/2\rfloor}\binom{k-m-1}{m}\left(\prod_{\nu=m+1}^{k-m-1}(1+n\nu)\right)a^{k-2m-1}$$ But I haven’t proved it yet, this is just by a comparison. Hopefully this will help determine the generality of Euler’s expressions (he leaves much hidden behind “ $\text{etc.}$ ”!). The challenge made concrete : prove that, for any positive integer $a,n$ : $$\lim_{k\to\infty}\frac{\sum_{m=0}^{\lfloor k/2\rfloor}\binom{k-m}{m}\left(\prod_{\nu=m}^{k-m-1}\right)a^{k-2m}}{\sum_{m=0}^{\lfloor(k-1)/2\rfloor}\binom{k-m-1}{m}\left(\prod_{\nu=m+1}^{k-m-1}\right)a^{k-2m-1}}=\lim_{N\to\infty}\frac{a+\sum_{m=1}^N\frac{a^{1-2m}}{n^mm!\prod_{\nu=0}^{m-1}(1+n\nu)}}{\sum_{m=0}^N\frac{a^{-2m}}{n^mm!\prod_{\nu=0}^m(1+n\nu)}}$$ Which can, in some mysterious way, be made obvious, according to Euler. The curious thing - the two expressions are not equal at any partial $k$ or $N$ . As mentioned in the preamble, Euler must have employed something analogous to “creative telescoping”, as some on this site would say, to get a simpler expression modulo a vanishing error term.","If one feels disinclined to read the contextual preamble, I have made some partial progress in clarifying the question. Skip to the bottom! The motivation behind this is to understand the derivation for the simple continued fractions of , : Euler’s work has been the only source I could find which attempts to explain the results. One may find here a translation of one of Euler's essays on continued fractions. Therein, he claims to prove some celebrated continued fraction formulae (with the proof starting in the final sections of the paper). I am finding his work very hard to follow, not from the complexity of the mathematics but more from his very terse style of writing! I am talking about section 31. For some he considers: And builds a series of approximations using his formula in section : Which are the first few convergents. Now, in section he continues: ""If these fractions are continued further the law by which they are formed will be easily observed. From this law it may be concluded that after both the numerator and the denominator are divided by the first term of the denominator, the limiting fraction will be:"" Now, he's not wrong - the numerics check out to an excellent accuracy. However I am really confused about where this fraction comes from; no multiplication of terms in the numerator or denominator comes to mind that reconciles that with the sequence of convergents. Moreover, the ratio of truncated series does not exactly equal any of the convergents, so not only do I need to guess the manipulation, but the manipulation leaves you with some kind of vanishing error term. In particular, what he intended the ""first term"" of the denominator to be, I really don't know - that seems the crux of the matter. I calculated the above ratio, truncated at the , and compared it to the fourth convergent: Versus the convergent: But I see no clear link. I would really appreciate it if anyone could comment suggestions or full answers as to what link Euler intended, to obtain the ratio of infinite series. EDIT: some progress: I discovered through a pattern recognition that the numerators of the convergents , indexing from , satisfy: I also find that the denominators , indexing with , satisfy ( ): But I haven’t proved it yet, this is just by a comparison. Hopefully this will help determine the generality of Euler’s expressions (he leaves much hidden behind “ ”!). The challenge made concrete : prove that, for any positive integer : Which can, in some mysterious way, be made obvious, according to Euler. The curious thing - the two expressions are not equal at any partial or . As mentioned in the preamble, Euler must have employed something analogous to “creative telescoping”, as some on this site would say, to get a simpler expression modulo a vanishing error term.","\tanh(1/k) e^{2/k} a,n s=a+\frac{1}{(1+n)a+\frac{1}{(1+2n)a+\frac{1}{(1+3n)a+\cdots}}} 10,14 a,\frac{(1+n)a^2+1}{(1+n)a},\frac{(1+n)(1+2n)a^3+(2+2n)a}{(1+n)(1+2n)a^2+1},\text{etc.} 32 \frac{a+\frac{1}{1\cdot na}+\frac{1}{1\cdot2\cdot1(1+n)n^2a^3}+\frac{1}{1\cdot2\cdot3\cdot1(1+n)(1+2n)n^3a^5}+\text{etc.}}{1+\frac{1}{1(1+n)na^2}+\frac{1}{1\cdot2(1+n)(1+2n)n^2a^4}+\frac{1}{1\cdot2\cdot3(1+n)(1+2n)(1+3n)n^3a^6}+\text{etc.}} \cdots \frac{\left(1+3n\right)a+3\left(1+2n\right)\left(1+3n\right)na^{3}+6\left(1+n\right)\left(1+2n\right)\left(1+3n\right)n^{2}a^{5}+6\left(1+n\right)\left(1+2n\right)\left(1+3n\right)n^{3}a^{7}}{6\left(1+n\right)\left(1+2n\right)\left(1+3n\right)n^{3}a^{6}+6\left(1+2n\right)\left(1+3n\right)n^{2}a^{4}+3\left(1+3n\right)na^{2}+1} \frac{(1+n)(1+2n)(1+3n)a^4+3(1+n)(1+2n)a^2+1}{(1+n)(1+2n)(1+3n)a^3+2(1+2n)a} (p_k)_{k=1}^\infty p_1:=a p_k=\sum_{m=0}^{\lfloor k/2\rfloor}\binom{k-m}{m}\left(\prod_{\nu=m}^{k-m-1}(1+n\nu)\right)a^{k-2m} q_k q_1=1 k>1 q_k=\sum_{m=0}^{\lfloor(k-1)/2\rfloor}\binom{k-m-1}{m}\left(\prod_{\nu=m+1}^{k-m-1}(1+n\nu)\right)a^{k-2m-1} \text{etc.} a,n \lim_{k\to\infty}\frac{\sum_{m=0}^{\lfloor k/2\rfloor}\binom{k-m}{m}\left(\prod_{\nu=m}^{k-m-1}\right)a^{k-2m}}{\sum_{m=0}^{\lfloor(k-1)/2\rfloor}\binom{k-m-1}{m}\left(\prod_{\nu=m+1}^{k-m-1}\right)a^{k-2m-1}}=\lim_{N\to\infty}\frac{a+\sum_{m=1}^N\frac{a^{1-2m}}{n^mm!\prod_{\nu=0}^{m-1}(1+n\nu)}}{\sum_{m=0}^N\frac{a^{-2m}}{n^mm!\prod_{\nu=0}^m(1+n\nu)}} k N","['real-analysis', 'sequences-and-series', 'limits', 'number-theory', 'continued-fractions']"
21,Calculate the sum $\sum_{n=0}^{\infty }\frac{1}{n!!}$,Calculate the sum,\sum_{n=0}^{\infty }\frac{1}{n!!},"\begin{align*} S =\sum_{n=0}^{\infty }\frac{1}{n!!}=\sum_{n=0}^{\infty }\left ( \frac{1}{(2n)!!}+\frac{1}{(2n+1)!!} \right ) &= \sum_{n=0}^{\infty }\left ( \frac{1}{(2n)!!}+\frac{(2n)!!}{(2n+1)!} \right )=\sum_{n=0}^{\infty }\left ( \frac{1}{2^nn!}+\frac{2^nn!}{(2n+1)!} \right )\\ & =e^{1/2}+\sum_{n=0}^{\infty }\frac{2^nn!}{(2n+1)!}; \; \sum_{n=0}^{\infty }\frac{n!x^{2n+1}}{(2n+1)!}=F(x)\\ &\Rightarrow \frac{\mathrm{d} F}{\mathrm{d} x}=\sum_{n=0}^{\infty }\frac{n!x^{2n}}{(2n)!}\Rightarrow \frac{\mathrm{d} }{\mathrm{d} x}\left [ F(x)\exp \left \{ -\frac{x^2}{4} \right \} \right ]\\ &=\left ( \frac{\mathrm{d} F}{\mathrm{d} x}-\frac{1}{2}xF(x) \right )\exp \left \{ -\frac{x^2}{4} \right \}\\ & =\frac{1}{2}\left ( 2-x^2+\sum_{n=1}^{\infty }\left ( \frac{(n-1)!x^{2n}}{(2n-1)!}-\frac{n!x^{2n+2}}{(2n+1)!} \right ) \right )\exp \left \{ -\frac{x^2}{4} \right \}\\ & =\exp\left \{ -\frac{x^2}{4} \right \}\Rightarrow F(x)=\exp \left \{ \frac{x^2}{4} \right \}\int\limits_{0}^{x}\exp \left \{ -\frac{\xi ^2}{4} \right \}d\xi \end{align*} $$\left \{ \int\limits_{\mathbb{R}}e^{-x^2}dx \right \}^2=\iint\limits_{\mathbb{R}}e^{-x^2-y^2}dxdy=\int\limits_{0}^{2\pi}\int\limits_{0}^{\infty }e^{-r^2}rdrd\varphi =\frac{1}{2}\int\limits_{0}^{2\pi}\left ( 1-\lim_{r\rightarrow \infty } e^{-r^2}\right )d\varphi =\pi$$ \begin{align*} \int\limits_{0}^{\infty }e^{-\xi ^2}d\xi =\frac{\sqrt{\pi}}{2}; \; \lim_{x\rightarrow \infty }\mathrm{erf}x=1 &\Rightarrow \int\limits_{0}^{x}\exp \left \{ -\frac{\xi ^2}{4} \right \}d\xi =\sqrt{\pi}\mathrm{erf}\frac{x}{2}\\ & \Rightarrow F(x)=\sqrt{\pi}\exp \left \{ \frac{x^2}{4} \right \}\mathrm{erf}\frac{x}{2} \end{align*} $$\sum_{n=0}^{\infty }\frac{1}{n!!}=\exp \left \{ \frac{1}{2} \right \}+\sum_{n=0}^{\infty }\frac{2^nn!}{(2n+1)!}=\sqrt{e}+\sqrt{\frac{\pi e}{2}}\mathrm{erf}\frac{1}{\sqrt{2}} $$ I showed my attempt at a solution above. This is the standard method of calculating the sum. The question is, is it possible to calculate this sum through a contour integral?","I showed my attempt at a solution above. This is the standard method of calculating the sum. The question is, is it possible to calculate this sum through a contour integral?","\begin{align*}
S =\sum_{n=0}^{\infty }\frac{1}{n!!}=\sum_{n=0}^{\infty }\left ( \frac{1}{(2n)!!}+\frac{1}{(2n+1)!!} \right )
&= \sum_{n=0}^{\infty }\left ( \frac{1}{(2n)!!}+\frac{(2n)!!}{(2n+1)!} \right )=\sum_{n=0}^{\infty }\left ( \frac{1}{2^nn!}+\frac{2^nn!}{(2n+1)!} \right )\\
& =e^{1/2}+\sum_{n=0}^{\infty }\frac{2^nn!}{(2n+1)!}; \; \sum_{n=0}^{\infty }\frac{n!x^{2n+1}}{(2n+1)!}=F(x)\\
&\Rightarrow \frac{\mathrm{d} F}{\mathrm{d} x}=\sum_{n=0}^{\infty }\frac{n!x^{2n}}{(2n)!}\Rightarrow \frac{\mathrm{d} }{\mathrm{d} x}\left [ F(x)\exp \left \{ -\frac{x^2}{4} \right \} \right ]\\
&=\left ( \frac{\mathrm{d} F}{\mathrm{d} x}-\frac{1}{2}xF(x) \right )\exp \left \{ -\frac{x^2}{4} \right \}\\
& =\frac{1}{2}\left ( 2-x^2+\sum_{n=1}^{\infty }\left ( \frac{(n-1)!x^{2n}}{(2n-1)!}-\frac{n!x^{2n+2}}{(2n+1)!} \right ) \right )\exp \left \{ -\frac{x^2}{4} \right \}\\
& =\exp\left \{ -\frac{x^2}{4} \right \}\Rightarrow F(x)=\exp \left \{ \frac{x^2}{4} \right \}\int\limits_{0}^{x}\exp \left \{ -\frac{\xi ^2}{4} \right \}d\xi
\end{align*} \left \{ \int\limits_{\mathbb{R}}e^{-x^2}dx \right \}^2=\iint\limits_{\mathbb{R}}e^{-x^2-y^2}dxdy=\int\limits_{0}^{2\pi}\int\limits_{0}^{\infty }e^{-r^2}rdrd\varphi =\frac{1}{2}\int\limits_{0}^{2\pi}\left ( 1-\lim_{r\rightarrow \infty } e^{-r^2}\right )d\varphi =\pi \begin{align*}
\int\limits_{0}^{\infty }e^{-\xi ^2}d\xi =\frac{\sqrt{\pi}}{2}; \; \lim_{x\rightarrow \infty }\mathrm{erf}x=1
&\Rightarrow \int\limits_{0}^{x}\exp \left \{ -\frac{\xi ^2}{4} \right \}d\xi =\sqrt{\pi}\mathrm{erf}\frac{x}{2}\\
& \Rightarrow F(x)=\sqrt{\pi}\exp \left \{ \frac{x^2}{4} \right \}\mathrm{erf}\frac{x}{2}
\end{align*} \sum_{n=0}^{\infty }\frac{1}{n!!}=\exp \left \{ \frac{1}{2} \right \}+\sum_{n=0}^{\infty }\frac{2^nn!}{(2n+1)!}=\sqrt{e}+\sqrt{\frac{\pi e}{2}}\mathrm{erf}\frac{1}{\sqrt{2}} ","['real-analysis', 'calculus']"
22,How to prove this (corollary of) hyperplane separation theorem?,How to prove this (corollary of) hyperplane separation theorem?,,"$X$ is a nonempty convex subset of $\mathbb{R}^n$ whose element is $x=\left(x_1,...,x_n\right)$ . The theorem is as follows. If for each $x\in X$ , there is an $i \in \left\{1,...,n\right\}$ such that $x_i>0$ , then there exists $\left(\lambda_1,...,\lambda_n\right)$ where $\lambda_i \geqslant 0$ for all $i$ and $\sum_{i=1}^n \lambda_i=1$ , such that $\lambda \cdot x \geqslant 0$ for all $x\in X$ and $\lambda \cdot x>0$ , for some $x \in X$ . I was wondering how to prove it. It looks like it is a corollary of the hyperplane separation theorem. "" $\geqslant 0$ for all $x$ and $>0$ for some $x$ "" is a little bit weird and I do not know whether there is a version of the hyperplane separation theorem that has this form and can be applied to prove it.","is a nonempty convex subset of whose element is . The theorem is as follows. If for each , there is an such that , then there exists where for all and , such that for all and , for some . I was wondering how to prove it. It looks like it is a corollary of the hyperplane separation theorem. "" for all and for some "" is a little bit weird and I do not know whether there is a version of the hyperplane separation theorem that has this form and can be applied to prove it.","X \mathbb{R}^n x=\left(x_1,...,x_n\right) x\in X i \in \left\{1,...,n\right\} x_i>0 \left(\lambda_1,...,\lambda_n\right) \lambda_i \geqslant 0 i \sum_{i=1}^n \lambda_i=1 \lambda \cdot x \geqslant 0 x\in X \lambda \cdot x>0 x \in X \geqslant 0 x >0 x","['real-analysis', 'linear-algebra', 'convex-analysis']"
23,"If $f\in L^1(\mathbb{R}^d)$ is uniformly continuous, do its integrals over spheres of increasing radius decay to zero?","If  is uniformly continuous, do its integrals over spheres of increasing radius decay to zero?",f\in L^1(\mathbb{R}^d),"To be more precise, if $f\in L^1(\mathbb{R}^d), d>1$ and is uniformly continuous, is it true that $$\lim_{R\to\infty} \int_{|x|=R} |f(x)| \ dS(x) = 0 \ ?$$ where $dS$ is the surface measure on the sphere of radius $R$ . If $f\in L^1(\mathbb{R})$ and uniformly continuous, it's well-known that $|f(x)|\to 0$ as $|x|\to\infty$ . But this problem seems to require more precise decay, and I have a feeling it's false. One idea for a counterexample: pick $f$ to be a radial series of bump functions centered at $|x|=n$ with slowly decaying heights and whose supports are disjoint and shrink fast enough to ensure integrability. This feels like main scenario for a counterexample (if one exists), but I wasn't able to get the details to work out.","To be more precise, if and is uniformly continuous, is it true that where is the surface measure on the sphere of radius . If and uniformly continuous, it's well-known that as . But this problem seems to require more precise decay, and I have a feeling it's false. One idea for a counterexample: pick to be a radial series of bump functions centered at with slowly decaying heights and whose supports are disjoint and shrink fast enough to ensure integrability. This feels like main scenario for a counterexample (if one exists), but I wasn't able to get the details to work out.","f\in L^1(\mathbb{R}^d), d>1 \lim_{R\to\infty} \int_{|x|=R} |f(x)| \ dS(x) = 0 \ ? dS R f\in L^1(\mathbb{R}) |f(x)|\to 0 |x|\to\infty f |x|=n",['real-analysis']
24,Speed of convergence of continued radicals with constant term,Speed of convergence of continued radicals with constant term,,"This is an old prelim problem in Analysis. Continued radicals of the form $$x_n=\sqrt{a+x_{n-1}},\qquad x_0=0$$ have been considered in MSE before. It is easy to check that $x_n$ defines a bounded monotone increasing sequence and that $x_n\xrightarrow{n\to \infty}\ell _a,$ where $\ell _a$ is the unique positive solution to the quadratic equation $$x^2-x-a=0,$$ namely, $$\ell _a=\frac{1+\sqrt{1+4a}}{2}>1.$$ This is typically written as $$\ell _a=\sqrt{a+\sqrt{a+\ldots +\sqrt{a+\ldots }}}.$$ Since $$\ell _a-x_n=\frac{\ell _a^2-x_n^2}{\ell _a+x_n}=\frac{\ell _a-x_{n-1}}{\ell _a+x_n},$$ we have that \begin{align*} \ell _a-x_n & =(\ell _a-x_1)\prod \limits _{k=1}^n\frac{1}{\ell _a+x_k} \\ & \leq (\ell _a-x_1)\frac{1}{\ell^n _a}\tag{1}\label{main}. \end{align*} The problem is to improve the rate of convergence $|\ell _a-x_n|=O( \ell _a^n)$ . In particular, to show that \begin{align*}\ell _a-x_n & \sim \frac{C_a}{(2\ell_a)^n}\tag{2}\label{better} \end{align*} for some constant $C_a>0$ . From \eqref{main} we have that $$b_n:=(2\ell _a)^n(\ell _a-x_n)=(\ell _a-x_1)\prod \limits _{k=1}^n\frac{2\ell _a}{\ell _a+x_k}\leq (\ell _a-x_1)\prod \limits _{k=1}^n\frac{\ell _a}{x_k}.$$ Since $\dfrac{2\ell _a}{\ell _a+x_n}>1$ , $b_n$ is monotone increasing. The result would follow if for example $p_n:=\prod \limits _{k=1}^n\dfrac{\ell _a}{x_k}$ were bounded above (or better yet, if $p_n$ converged). Any hints will be appreciated.","This is an old prelim problem in Analysis. Continued radicals of the form have been considered in MSE before. It is easy to check that defines a bounded monotone increasing sequence and that where is the unique positive solution to the quadratic equation namely, This is typically written as Since we have that The problem is to improve the rate of convergence . In particular, to show that for some constant . From \eqref{main} we have that Since , is monotone increasing. The result would follow if for example were bounded above (or better yet, if converged). Any hints will be appreciated.","x_n=\sqrt{a+x_{n-1}},\qquad x_0=0 x_n x_n\xrightarrow{n\to \infty}\ell _a, \ell _a x^2-x-a=0, \ell _a=\frac{1+\sqrt{1+4a}}{2}>1. \ell _a=\sqrt{a+\sqrt{a+\ldots +\sqrt{a+\ldots }}}. \ell _a-x_n=\frac{\ell _a^2-x_n^2}{\ell _a+x_n}=\frac{\ell _a-x_{n-1}}{\ell _a+x_n}, \begin{align*}
\ell _a-x_n & =(\ell _a-x_1)\prod \limits _{k=1}^n\frac{1}{\ell _a+x_k} \\
& \leq (\ell _a-x_1)\frac{1}{\ell^n _a}\tag{1}\label{main}.
\end{align*} |\ell _a-x_n|=O( \ell _a^n) \begin{align*}\ell _a-x_n & \sim \frac{C_a}{(2\ell_a)^n}\tag{2}\label{better}
\end{align*} C_a>0 b_n:=(2\ell _a)^n(\ell _a-x_n)=(\ell _a-x_1)\prod \limits _{k=1}^n\frac{2\ell _a}{\ell _a+x_k}\leq (\ell _a-x_1)\prod \limits _{k=1}^n\frac{\ell _a}{x_k}. \dfrac{2\ell _a}{\ell _a+x_n}>1 b_n p_n:=\prod \limits _{k=1}^n\dfrac{\ell _a}{x_k} p_n","['real-analysis', 'calculus', 'convergence-divergence', 'asymptotics']"
25,Rudin partition definition: Why non-strict inequalities?,Rudin partition definition: Why non-strict inequalities?,,"In Definition 6.1 in Principles of Mathematical Analysis, Rudin writes: Let $[a, b]$ be a given interval. By a partition $P$ of $[a, b]$ we mean a finite set of points $x_0, x_1 , \dotsc, x_n$ , where $$a = x_0 \le x_1 \le \dotsb \le x_{n-1} \le x_n = b.$$ Why does he use non-strict inequalities? I don't see how we could have $x_{k-1} = x_k$ if $P$ is a set, in contrast to a multiset. Also, the definition on Wikipedia uses strict inequalities. Is this just Rudin being a little bit sloppy (though I guess, technically, his definition reduces to the other one due to $P$ being a set), or am I actually missing something?","In Definition 6.1 in Principles of Mathematical Analysis, Rudin writes: Let be a given interval. By a partition of we mean a finite set of points , where Why does he use non-strict inequalities? I don't see how we could have if is a set, in contrast to a multiset. Also, the definition on Wikipedia uses strict inequalities. Is this just Rudin being a little bit sloppy (though I guess, technically, his definition reduces to the other one due to being a set), or am I actually missing something?","[a, b] P [a, b] x_0, x_1 , \dotsc, x_n a = x_0 \le x_1 \le \dotsb \le x_{n-1} \le x_n = b. x_{k-1} = x_k P P","['real-analysis', 'definition']"
26,Proving that $\{a\in\mathbb{Q}|a>0$ and $a^2<2\}$ has no least upper bound in $\mathbb{Q}$,Proving that  and  has no least upper bound in,\{a\in\mathbb{Q}|a>0 a^2<2\} \mathbb{Q},"I am going through a proof found in: http://www.math.columbia.edu/~harris/2000/2016Dedcuts.pdf In it he finds a smaller upper bound to the supposed least upper bound: proof However in Step 1, I don't understand how the proof goes from $4\frac{1}{n}-\frac{1}{n^2}$ to it being less than $4\frac{1}{n}-\frac{1}{n}$ , it reasons that $\frac{1}{n^2}≤\frac{1}{n}$ but shouldn't that mean subtracting the larger number makes the result smaller than if i were to subtract the smaller number>? i.e. $x<y$ then $z-x>z-y$ right? But here its saying that $z-y>z-x$ If it is a typo, can the proof still be salvaged?","I am going through a proof found in: http://www.math.columbia.edu/~harris/2000/2016Dedcuts.pdf In it he finds a smaller upper bound to the supposed least upper bound: proof However in Step 1, I don't understand how the proof goes from to it being less than , it reasons that but shouldn't that mean subtracting the larger number makes the result smaller than if i were to subtract the smaller number>? i.e. then right? But here its saying that If it is a typo, can the proof still be salvaged?",4\frac{1}{n}-\frac{1}{n^2} 4\frac{1}{n}-\frac{1}{n} \frac{1}{n^2}≤\frac{1}{n} x<y z-x>z-y z-y>z-x,"['real-analysis', 'inequality', 'proof-writing', 'solution-verification', 'proof-explanation']"
27,Show that $\int\limits_a^bf(x)g(x)dx\geq 0 \implies f(x)\geq0$,Show that,\int\limits_a^bf(x)g(x)dx\geq 0 \implies f(x)\geq0,"Let be $f:[a,b]\to\mathbb{R}$ a continuous function. Show that if $\int\limits_a^bf(x)g(x)dx\geq 0$ holds for all continuous functions $g:[a,b]\to\mathbb{R}$ with $g(a)=g(b)=0$ and $g(x)\geq 0$ for all $x\in]a,b[$ , then $f(x)\geq 0$ for all $x\in[a,b]$ . My approach: Let's assume there exists a $x_0\in]a,b[$ such that $f(x_0)<0$ . If we set $\epsilon:=-\frac{f(x_0)}{2}>0$ , then by continuity of $f$ there exists a $\delta>0$ such that for all $x\in[a,b]$ with $|x-x_0|<\delta\implies |f(x)-f(x_0)|<-\frac{f(x_0)}{2}$ . This means that $f(x)<0$ for all $x\in]x_0-\delta,x_0+\delta[$ . Next, we define the function $g:[a,b]\to\mathbb{R}$ where $$ g(x):=\begin{cases}0,&x\in[a,x_0-\delta]\\x-(x_0-\delta),&x\in]x_0-\delta,x_0]\\ (x_0+\delta)-x,&x\in]x_0,x_0+\delta[\\0,&x\in[x_0+\delta,b].\end{cases} $$ If we check the limits at the ""critical"" three points $x_0-\delta,x_0$ and $x_0+\delta$ we see immediately that $g$ is continuous. Further, we see that \begin{align*} &f(x)g(x)=0,  x\in[a,x_0-\delta]\\ &f(x)g(x)\leq\frac{f(x_0)}{2}(x-x_0+\delta)\leq0,  x\in[x_0-\delta,x_0]\\ &f(x)g(x)\leq\frac{f(x_0)}{2}(x_0+\delta-x)\leq0,  x\in[x_0,x_0+\delta]\\ &f(x)g(x)=0,  x\in[x_0+\delta,b]. \end{align*} Using the well known operations of Riemann integrals we get: \begin{align*} &\int\limits_a^bf(x)g(x)dx\\ &=\underset{=0}{\underbrace{\int\limits_a^{x_0-\delta}f(x)g(x)dx}}+\underset{\leq0}{\underbrace{\int\limits_{x_0-\delta}^{x_0}f(x)g(x)dx}}+\underset{\leq0}{\underbrace{\int\limits_{x_0}^{x_0+\delta}f(x)g(x)dx}}+\underset{=0}{\underbrace{\int\limits_{x_0+\delta}^bf(x)g(x)dx}}\leq0. \end{align*} We know that if $h$ is a continuous function, then $\int\limits_a^bh(x)dx=0\implies h(x)=0$ for all $x\in[a,b]$ . As $f(x)g(x)\neq 0$ for particular $x\in[a,b]$ it follows that $\int\limits_a^bf(x)g(x)dx\neq 0$ . Hence, $\int\limits_a^bf(x)g(x)dx<0$ . This is a contradiction so there mustn't be a $x_0$ such that $f(x_0)<0$ . Note that if we had assumed $x_0=a$ or $x_0=b$ then we could adjust $g$ accordingly and get the same result. Is this correct? Do you have any suggestions or maybe a shorter proof? Also: The function $g$ I have constructed satisfies $g(a)=g(b)=0$ but I don't see why it should be necessary?! Maybe there are other functions which we can use to achieve a contradiction which doesn't satisfy $g(a)=g(b)=0$ ? EDIT We know that if $h$ is a continuous function with $h(x)\geq0$ for all $x\in[a,b]$ , then $\int\limits_a^bh(x)dx=0\implies h(x)=0$ for all $x\in[a,b]$ . As $(-f(x)g(x))\geq 0$ for all $x\in[x_0-\delta,x_0+\delta]$ and $(-f(x)g(x))\neq 0$ for particular $x\in[x_0-\delta,x_0+\delta]$ it follows that $\int\limits_a^b-f(x)g(x)dx\neq 0$ . Hence, $\int\limits_a^bf(x)g(x)dx\neq0$ which shows that $\int\limits_a^bf(x)g(x)dx<0$ . This is a contradiction so there mustn't be a $x_0$ such that $f(x_0)<0$ .","Let be a continuous function. Show that if holds for all continuous functions with and for all , then for all . My approach: Let's assume there exists a such that . If we set , then by continuity of there exists a such that for all with . This means that for all . Next, we define the function where If we check the limits at the ""critical"" three points and we see immediately that is continuous. Further, we see that Using the well known operations of Riemann integrals we get: We know that if is a continuous function, then for all . As for particular it follows that . Hence, . This is a contradiction so there mustn't be a such that . Note that if we had assumed or then we could adjust accordingly and get the same result. Is this correct? Do you have any suggestions or maybe a shorter proof? Also: The function I have constructed satisfies but I don't see why it should be necessary?! Maybe there are other functions which we can use to achieve a contradiction which doesn't satisfy ? EDIT We know that if is a continuous function with for all , then for all . As for all and for particular it follows that . Hence, which shows that . This is a contradiction so there mustn't be a such that .","f:[a,b]\to\mathbb{R} \int\limits_a^bf(x)g(x)dx\geq 0 g:[a,b]\to\mathbb{R} g(a)=g(b)=0 g(x)\geq 0 x\in]a,b[ f(x)\geq 0 x\in[a,b] x_0\in]a,b[ f(x_0)<0 \epsilon:=-\frac{f(x_0)}{2}>0 f \delta>0 x\in[a,b] |x-x_0|<\delta\implies |f(x)-f(x_0)|<-\frac{f(x_0)}{2} f(x)<0 x\in]x_0-\delta,x_0+\delta[ g:[a,b]\to\mathbb{R} 
g(x):=\begin{cases}0,&x\in[a,x_0-\delta]\\x-(x_0-\delta),&x\in]x_0-\delta,x_0]\\ (x_0+\delta)-x,&x\in]x_0,x_0+\delta[\\0,&x\in[x_0+\delta,b].\end{cases}
 x_0-\delta,x_0 x_0+\delta g \begin{align*}
&f(x)g(x)=0,  x\in[a,x_0-\delta]\\
&f(x)g(x)\leq\frac{f(x_0)}{2}(x-x_0+\delta)\leq0,  x\in[x_0-\delta,x_0]\\
&f(x)g(x)\leq\frac{f(x_0)}{2}(x_0+\delta-x)\leq0,  x\in[x_0,x_0+\delta]\\
&f(x)g(x)=0,  x\in[x_0+\delta,b].
\end{align*} \begin{align*}
&\int\limits_a^bf(x)g(x)dx\\
&=\underset{=0}{\underbrace{\int\limits_a^{x_0-\delta}f(x)g(x)dx}}+\underset{\leq0}{\underbrace{\int\limits_{x_0-\delta}^{x_0}f(x)g(x)dx}}+\underset{\leq0}{\underbrace{\int\limits_{x_0}^{x_0+\delta}f(x)g(x)dx}}+\underset{=0}{\underbrace{\int\limits_{x_0+\delta}^bf(x)g(x)dx}}\leq0.
\end{align*} h \int\limits_a^bh(x)dx=0\implies h(x)=0 x\in[a,b] f(x)g(x)\neq 0 x\in[a,b] \int\limits_a^bf(x)g(x)dx\neq 0 \int\limits_a^bf(x)g(x)dx<0 x_0 f(x_0)<0 x_0=a x_0=b g g g(a)=g(b)=0 g(a)=g(b)=0 h h(x)\geq0 x\in[a,b] \int\limits_a^bh(x)dx=0\implies h(x)=0 x\in[a,b] (-f(x)g(x))\geq 0 x\in[x_0-\delta,x_0+\delta] (-f(x)g(x))\neq 0 x\in[x_0-\delta,x_0+\delta] \int\limits_a^b-f(x)g(x)dx\neq 0 \int\limits_a^bf(x)g(x)dx\neq0 \int\limits_a^bf(x)g(x)dx<0 x_0 f(x_0)<0","['real-analysis', 'integration', 'continuity', 'solution-verification', 'riemann-integration']"
28,Convergence in measure iff convergence in distance metric,Convergence in measure iff convergence in distance metric,,"I am trying to solve the following question: Let $(X, F, \mu)$ be a measure space. Let $H>0$ be integrable. Let $(E,d)$ be a separable metric space and $f,g: X \rightarrow E$ measurable functions. Let $d_H(f,g):= \int \min\{d(f(x),g(x)),1\}H(x)d\mu(x)$ . Prove $f_n$ converges in measure to $f$ $\iff$ $\lim d_H(f_n, f) = 0$ . I think I have the "" $\implies$ "" part (the sentence in bold later on is the part I am stuck on): Let $1>\epsilon>0$ and $\delta>0$ and let $c:= \int H(x) < \infty$ . $d_H(f_n, f) =  \int_{\{x|d(f_n(x),f(x)>\epsilon\}} \min\{d(f_n(x),f(x)),1\}H(x)d\mu(x) + \int_{\{x|d(f_n(x),f(x)\leq\epsilon\}} \min\{d(f_n(x),f(x)),1\}H(x)d\mu(x)$ From monotonicity and from our assumption that $f_n$ converges in measure to $f$ , for a large enough $n$ we can bound the first term by: $$\leq \int_{\{x|d(f_n(x),f(x))>\epsilon\}} H(x)d\mu(x) \leq c\mu(\{x|d(f_n(x),f(x))>\epsilon\} \leq c\delta$$ For the second term, since $\epsilon<1$ , we obtain: $$ \leq \int_{\{x|d(f_n(x),f(x)\leq \epsilon\}} \epsilon H(x)d\mu(x) \leq \epsilon c$$ Since these two hold for all $1>\epsilon>0$ and $\delta>0$ for a sufficiently large $n$ , we obtain the conclusion. For the "" $\Longleftarrow$ "" part I am stuck with $H(x)$ : Let $1>\epsilon>0$ and $\delta>0$ . $$ \int_{\{x|d(f_n(x),f(x))> \epsilon \}} \epsilon H(x) \leq \int_{\{x|d(f_n(x),f(x))> \epsilon\}} \min\{1,d(f_n(x),f(x))\}H(x) \leq \int \min\{1,d(f_n(x),f(x))\}H(x)$$ Without the $H(x)$ I would be able to write: $$\epsilon \mu(\{x|d(f_n(x),f(x))> \epsilon\}) \leq \int_{\{x|d(f_n(x),f(x))> \epsilon \}} \epsilon \leq \int_{\{x|d(f_n(x),f(x))> \epsilon\}} \min\{1,d(f_n(x),f(x))\} \leq \int \min\{1,d(f_n(x),f(x))\}$$ and then by taking the limit, the right-hand-side would tend to 0 and so for a large enough $n$ the left-hand-side is less than $\epsilon\delta$ and the conclusion follows. How can I do this with $H(x)$ ? Also - if there is feedback for improvement of the first part - would be great to know.","I am trying to solve the following question: Let be a measure space. Let be integrable. Let be a separable metric space and measurable functions. Let . Prove converges in measure to . I think I have the "" "" part (the sentence in bold later on is the part I am stuck on): Let and and let . From monotonicity and from our assumption that converges in measure to , for a large enough we can bound the first term by: For the second term, since , we obtain: Since these two hold for all and for a sufficiently large , we obtain the conclusion. For the "" "" part I am stuck with : Let and . Without the I would be able to write: and then by taking the limit, the right-hand-side would tend to 0 and so for a large enough the left-hand-side is less than and the conclusion follows. How can I do this with ? Also - if there is feedback for improvement of the first part - would be great to know.","(X, F, \mu) H>0 (E,d) f,g: X \rightarrow E d_H(f,g):= \int \min\{d(f(x),g(x)),1\}H(x)d\mu(x) f_n f \iff \lim d_H(f_n, f) = 0 \implies 1>\epsilon>0 \delta>0 c:= \int H(x) < \infty d_H(f_n, f) =  \int_{\{x|d(f_n(x),f(x)>\epsilon\}} \min\{d(f_n(x),f(x)),1\}H(x)d\mu(x) + \int_{\{x|d(f_n(x),f(x)\leq\epsilon\}} \min\{d(f_n(x),f(x)),1\}H(x)d\mu(x) f_n f n \leq \int_{\{x|d(f_n(x),f(x))>\epsilon\}} H(x)d\mu(x) \leq c\mu(\{x|d(f_n(x),f(x))>\epsilon\} \leq c\delta \epsilon<1  \leq \int_{\{x|d(f_n(x),f(x)\leq \epsilon\}} \epsilon H(x)d\mu(x) \leq \epsilon c 1>\epsilon>0 \delta>0 n \Longleftarrow H(x) 1>\epsilon>0 \delta>0  \int_{\{x|d(f_n(x),f(x))> \epsilon \}} \epsilon H(x) \leq \int_{\{x|d(f_n(x),f(x))> \epsilon\}} \min\{1,d(f_n(x),f(x))\}H(x) \leq \int \min\{1,d(f_n(x),f(x))\}H(x) H(x) \epsilon \mu(\{x|d(f_n(x),f(x))> \epsilon\}) \leq \int_{\{x|d(f_n(x),f(x))> \epsilon \}} \epsilon \leq \int_{\{x|d(f_n(x),f(x))> \epsilon\}} \min\{1,d(f_n(x),f(x))\} \leq \int \min\{1,d(f_n(x),f(x))\} n \epsilon\delta H(x)","['real-analysis', 'measure-theory', 'convergence-divergence']"
29,Solving or estimating $\int_{-\infty}^\infty \frac{\exp(-a/(x^2+b))}{x^2+c}dx$,Solving or estimating,\int_{-\infty}^\infty \frac{\exp(-a/(x^2+b))}{x^2+c}dx,"Can the integral $$\int_{-\infty}^\infty \frac{\exp(-a/(x^2+b))}{x^2+c}dx$$ be made explicit ( $a,b,c>0$ )? I'm also asking those of you who have access to CAS which can solve it. In case it can't, is there a very good upper bound? I need a better one than the obvious bound $$\int_{-\infty}^\infty \frac{1}{x^2+c}dx = \frac{\pi}{\sqrt{c}}$$ Thanks a lot!","Can the integral be made explicit ( )? I'm also asking those of you who have access to CAS which can solve it. In case it can't, is there a very good upper bound? I need a better one than the obvious bound Thanks a lot!","\int_{-\infty}^\infty \frac{\exp(-a/(x^2+b))}{x^2+c}dx a,b,c>0 \int_{-\infty}^\infty \frac{1}{x^2+c}dx = \frac{\pi}{\sqrt{c}}","['real-analysis', 'integration', 'definite-integrals', 'improper-integrals', 'upper-lower-bounds']"
30,Mid-point convex measurable subset of $\mathbb{R}$ with positive Lebesgue measure is an interval,Mid-point convex measurable subset of  with positive Lebesgue measure is an interval,\mathbb{R},"The question is right as the title: Let $E$ be a measurable subset of $\mathbb{R}$ w.r.t. Lebesgue measure, and has positive measure. For any $x,y\in E$ , $\frac{x+y}{2}\in E$ . Prove that $E$ is an interval(like $[a,b],[a,b),(a,b)$ etc., possibly infinity endpoint). The hint is to find some function on it, and I tried looking at its characteristic function, but I have no clue.","The question is right as the title: Let be a measurable subset of w.r.t. Lebesgue measure, and has positive measure. For any , . Prove that is an interval(like etc., possibly infinity endpoint). The hint is to find some function on it, and I tried looking at its characteristic function, but I have no clue.","E \mathbb{R} x,y\in E \frac{x+y}{2}\in E E [a,b],[a,b),(a,b)","['real-analysis', 'measure-theory', 'lebesgue-measure']"
31,How to show $\int_{k}^{\infty}\frac{1}{\sqrt{x}}\sin({x+\frac{1}{x}})dx$ converges / diverges,How to show  converges / diverges,\int_{k}^{\infty}\frac{1}{\sqrt{x}}\sin({x+\frac{1}{x}})dx,How to show $\int_{k}^{\infty}\frac{1}{\sqrt{x}}\sin({x+\frac{1}{x}})dx$ converges for $k=1$ and diverges for $k=0$ ? I have that $\left|\frac{1}{\sqrt{x}}\sin({x+\frac{1}{x}})\right|\geq\frac{\left|\sin({x+\frac{1}{x}})\right|}{x+\frac{1}{x}}$ . But this doesn't get my anywhere useful. Is there a function that could bound and show convergence at $\infty$ ?,How to show converges for and diverges for ? I have that . But this doesn't get my anywhere useful. Is there a function that could bound and show convergence at ?,\int_{k}^{\infty}\frac{1}{\sqrt{x}}\sin({x+\frac{1}{x}})dx k=1 k=0 \left|\frac{1}{\sqrt{x}}\sin({x+\frac{1}{x}})\right|\geq\frac{\left|\sin({x+\frac{1}{x}})\right|}{x+\frac{1}{x}} \infty,"['real-analysis', 'calculus', 'integration', 'improper-integrals']"
32,Does convergence of integral series and all derivative series imply convergence?,Does convergence of integral series and all derivative series imply convergence?,,"Let $f_n : \mathbb R \to \mathbb [0,\infty)$ be a sequence of smooth functions such that $$\sum_{n=1}^\infty \int_{\mathbb R} f_n(x) dx$$ is convergent. Further, we have that $$\sum_{n=1}^\infty f_n^{(k)}(x)$$ converges for every $x \in \mathbb R$ and every $k\in \mathbb N$ . Does this imply that $$\sum_{n=1}^\infty f_n(x)$$ converges for all $x\in\mathbb R$ ? Note that for me $0 \notin \mathbb N$ . See also my last question where the question is solved in the case where we restrict $k$ to be $2$ .","Let be a sequence of smooth functions such that is convergent. Further, we have that converges for every and every . Does this imply that converges for all ? Note that for me . See also my last question where the question is solved in the case where we restrict to be .","f_n : \mathbb R \to \mathbb [0,\infty) \sum_{n=1}^\infty \int_{\mathbb R} f_n(x) dx \sum_{n=1}^\infty f_n^{(k)}(x) x \in \mathbb R k\in \mathbb N \sum_{n=1}^\infty f_n(x) x\in\mathbb R 0 \notin \mathbb N k 2","['real-analysis', 'integration', 'derivatives', 'convergence-divergence']"
33,Solution verification of $\lim\limits_{n \to \infty} \sum\limits_{r=1}^{n}\dfrac{1}{(n+r)^p}$?,Solution verification of ?,\lim\limits_{n \to \infty} \sum\limits_{r=1}^{n}\dfrac{1}{(n+r)^p},"I need to find the sum for any $p \in \mathbb{R}$ , $$S=\lim\limits_{n \to \infty} \sum\limits_{r=1}^{n}\dfrac{1}{(n+r)^p}$$ My attempt: I will use the idea that $\lim\limits_{n \to ∞} f \cdot g = \lim\limits_{n \to ∞} f \cdot \lim\limits_{n \to ∞} g$ if and only if $f\cdot g$ is not an indeterminate form as $n \to ∞$ , $$S= \lim\limits_{n \to \infty} \sum\limits_{r=1}^{n}\dfrac{1}{n^p(1+r/n)^p}= \lim\limits_{n \to ∞} \dfrac{1}{n^{p-1}} \cdot \dfrac{1}{n} \sum\limits_{r=1}^{n} \dfrac{1}{\left(1+r/n\right)^p}=\lim\limits_{n \to ∞} \dfrac{1}{n^{p-1}} \cdot \lim\limits_{n \to ∞} \dfrac{1}{n} \sum\limits_{r=1}^{n} \dfrac{1}{\left(1+r/n\right)^p}= \lim\limits_{n \to ∞} \dfrac{1}{n^{p-1}} \int_{1}^{2} \dfrac{1}{x^p} \, \mathrm{d}x$$ If $p≠1$ then, $$S= \lim\limits_{n \to ∞} \dfrac{1}{n^{p-1}} \left[\left(\dfrac{2^{-p+1}}{-p+1}\right)-\left(\dfrac{1^{-p+1}}{-p+1}\right) \right]$$ If $p=1$ then, $$S= \lim \limits_{n\to ∞} \dfrac{1}{n^{1-1}} \cdot \ln 2$$ Conclusion: If $p>1$ then $S=0$ , if $p=1$ then $S= \ln 2$ and if $p<1$ then $S = +\infty$ Is the reasoning correct at each and every step? Secondly, are there any alternative methods to do this? I couldn't find this question on the site.","I need to find the sum for any , My attempt: I will use the idea that if and only if is not an indeterminate form as , If then, If then, Conclusion: If then , if then and if then Is the reasoning correct at each and every step? Secondly, are there any alternative methods to do this? I couldn't find this question on the site.","p \in \mathbb{R} S=\lim\limits_{n \to \infty} \sum\limits_{r=1}^{n}\dfrac{1}{(n+r)^p} \lim\limits_{n \to ∞} f \cdot g = \lim\limits_{n \to ∞} f \cdot \lim\limits_{n \to ∞} g f\cdot g n \to ∞ S= \lim\limits_{n \to \infty} \sum\limits_{r=1}^{n}\dfrac{1}{n^p(1+r/n)^p}= \lim\limits_{n \to ∞} \dfrac{1}{n^{p-1}} \cdot \dfrac{1}{n} \sum\limits_{r=1}^{n} \dfrac{1}{\left(1+r/n\right)^p}=\lim\limits_{n \to ∞} \dfrac{1}{n^{p-1}} \cdot \lim\limits_{n \to ∞} \dfrac{1}{n} \sum\limits_{r=1}^{n} \dfrac{1}{\left(1+r/n\right)^p}= \lim\limits_{n \to ∞} \dfrac{1}{n^{p-1}} \int_{1}^{2} \dfrac{1}{x^p} \, \mathrm{d}x p≠1 S= \lim\limits_{n \to ∞} \dfrac{1}{n^{p-1}} \left[\left(\dfrac{2^{-p+1}}{-p+1}\right)-\left(\dfrac{1^{-p+1}}{-p+1}\right) \right] p=1 S= \lim \limits_{n\to ∞} \dfrac{1}{n^{1-1}} \cdot \ln 2 p>1 S=0 p=1 S= \ln 2 p<1 S = +\infty","['real-analysis', 'calculus', 'sequences-and-series', 'limits', 'solution-verification']"
34,"Solution verification of the improper integral $\int_1^{+\infty}\frac{\cos^2{t}}{t}\,dt$",Solution verification of the improper integral,"\int_1^{+\infty}\frac{\cos^2{t}}{t}\,dt","Study $$\int_1^{+\infty}\frac{\cos^2{t}}{t}\,dt$$ I have thought the following: I consider the fact $\frac{\cos^2{t}}{t}\leq \frac{1}{t}$ this is not useful since $\int_1^{+\infty}\frac{1}{t}\, dt$ is divergent but this is not helpful to affirm that $\int_1^{+\infty}\frac{\cos^2{t}}{t}\,dt$ is convergent. I want to try to prove that the integral is divergent. To do this I have thought: $$\int_1^{+\infty}\frac{\cos^2{t}}{t}\,dt=\int_1^{\pi}\frac{\cos^2{t}}{t}\,dt+\sum_{k=1}^{\infty}\int_{k\pi}^{(k+1)\pi} \frac{\cos^2{t}}{t}\,dt$$ From this for the reason why $\frac{\cos^2{t}}{t}\geq 0$ in $[1,\pi]$ then $\int_1^{k\pi}\frac{\cos^2{t}}{t}\,dt\geq 0$ and it is not an improper integral, so: $$\int_1^{\pi}\frac{\cos^2{t}}{t}\,dt+\sum_{k=1}^{\infty}\int_{k\pi}^{(k+1)\pi} \frac{\cos^2{t}}{t}\,dt\geq \sum_{k=1}^{\infty}\int_{k\pi}^{(k+1)\pi} \frac{\cos^2{t}}{(k+1)\pi}\,dt= \sum_{k=1}^{\infty}\frac{\pi}{2}\frac{1}{(k+1)\pi}\,dt$$ Now since $ \frac{1}{k+1}\sim \frac{1}{k}$ then the series is divergent and so the integral is divergent. $\textbf{Request:}$ I would be so grateful if you can tell me if my work (so not only the conclusion on the fact that the integral is divergent), described at 2), is right or there are some mistakes and in this case how can I correct them? Thanks a lot in advance.","Study I have thought the following: I consider the fact this is not useful since is divergent but this is not helpful to affirm that is convergent. I want to try to prove that the integral is divergent. To do this I have thought: From this for the reason why in then and it is not an improper integral, so: Now since then the series is divergent and so the integral is divergent. I would be so grateful if you can tell me if my work (so not only the conclusion on the fact that the integral is divergent), described at 2), is right or there are some mistakes and in this case how can I correct them? Thanks a lot in advance.","\int_1^{+\infty}\frac{\cos^2{t}}{t}\,dt \frac{\cos^2{t}}{t}\leq \frac{1}{t} \int_1^{+\infty}\frac{1}{t}\, dt \int_1^{+\infty}\frac{\cos^2{t}}{t}\,dt \int_1^{+\infty}\frac{\cos^2{t}}{t}\,dt=\int_1^{\pi}\frac{\cos^2{t}}{t}\,dt+\sum_{k=1}^{\infty}\int_{k\pi}^{(k+1)\pi} \frac{\cos^2{t}}{t}\,dt \frac{\cos^2{t}}{t}\geq 0 [1,\pi] \int_1^{k\pi}\frac{\cos^2{t}}{t}\,dt\geq 0 \int_1^{\pi}\frac{\cos^2{t}}{t}\,dt+\sum_{k=1}^{\infty}\int_{k\pi}^{(k+1)\pi} \frac{\cos^2{t}}{t}\,dt\geq \sum_{k=1}^{\infty}\int_{k\pi}^{(k+1)\pi} \frac{\cos^2{t}}{(k+1)\pi}\,dt= \sum_{k=1}^{\infty}\frac{\pi}{2}\frac{1}{(k+1)\pi}\,dt  \frac{1}{k+1}\sim \frac{1}{k} \textbf{Request:}","['real-analysis', 'trigonometry', 'solution-verification', 'improper-integrals', 'trigonometric-integrals']"
35,"If $\{a_n \}_{n \in \mathbb{N}}$ is a set of positive numbers s.t $\sum_{n \in \mathbb{N}} a_n< \infty$, can we rearrange them so that $a_n=o(1/n)$?","If  is a set of positive numbers s.t , can we rearrange them so that ?",\{a_n \}_{n \in \mathbb{N}} \sum_{n \in \mathbb{N}} a_n< \infty a_n=o(1/n),"Given a set $\{a_n \}_{n \in \mathbb{N}}$ of positive numbers such that $\sum_{n \in \mathbb{N}} a_n< \infty$ , is there a bijection $f:\mathbb{N} \rightarrow \mathbb{N}$ such that $a_{f(n)}=o(1/n)$ ? It's true that if $a_n$ is a positive decreasing sequence such that $\sum_{n \in 	\mathbb{N}} a_n < \infty$ then we must have $a_n=o(1/n)$ (see: For positive, decreasing $a_n$, must $a_n=O(1/n)$ for the series $\sum_{n=1}^{\infty} a_n$ to converge? ), but the problem is that we can't necessary write $\{a_n\}_{n \in \mathbb{N}}$ as a decreasing sequence. Unfortunately, I'm not sure where to begin with proving this or coming up with a counter example. Any help is appreciated.","Given a set of positive numbers such that , is there a bijection such that ? It's true that if is a positive decreasing sequence such that then we must have (see: For positive, decreasing $a_n$, must $a_n=O(1/n)$ for the series $\sum_{n=1}^{\infty} a_n$ to converge? ), but the problem is that we can't necessary write as a decreasing sequence. Unfortunately, I'm not sure where to begin with proving this or coming up with a counter example. Any help is appreciated.",\{a_n \}_{n \in \mathbb{N}} \sum_{n \in \mathbb{N}} a_n< \infty f:\mathbb{N} \rightarrow \mathbb{N} a_{f(n)}=o(1/n) a_n \sum_{n \in 	\mathbb{N}} a_n < \infty a_n=o(1/n) \{a_n\}_{n \in \mathbb{N}},"['real-analysis', 'sequences-and-series']"
36,"In search of a literature or theory that deals with $\int_{[a,b]}f(x)dx \geq \int_{g([a,b])}f(x)dx$",In search of a literature or theory that deals with,"\int_{[a,b]}f(x)dx \geq \int_{g([a,b])}f(x)dx","I stumble upon the following problem. Given an increasing function $g$ is there a function $f$ (not identically null) s.t. $$\int_{[a,b]}f(x)dx \geq \int_{g([a,b])}f(x)dx$$ And, if there is, what can we say about $f$ . The problem could be reinstated in terms of pushforward, i.e. find a signed measure $\mu$ s.t. $\mu([a,b]) \geq g_{*}(\mu)([a,b])$ . There are also other analogous ways to approach it (in terms of differential equation the problem is a composition of function). However, none of this seems to shed light upon the problem. I tried to find a theory of some sort that could help me with it, but so far have not been successful. $\textbf{Edit:}$ Considering avs answer the following idea emerged. Let $g$ be a strictly increasing continuouosly differentiable function. Let $A = \{x : g'(x) < 1\}$ and $B = \{x : g'(x) > 1\}$ . Then $g$ is a contraction in every open interval in $A$ (actually, a subcontraction) and is an expansion (in the sense that its inverse is a contraction) in every open interval in $B$ . Thus let $f$ be s.t. $f \leq 0$ in A, $f \geq 0$ in $B$ and $f = 0$ in $(A \cup B)^c$ . Then $f$ must be a solution.","I stumble upon the following problem. Given an increasing function is there a function (not identically null) s.t. And, if there is, what can we say about . The problem could be reinstated in terms of pushforward, i.e. find a signed measure s.t. . There are also other analogous ways to approach it (in terms of differential equation the problem is a composition of function). However, none of this seems to shed light upon the problem. I tried to find a theory of some sort that could help me with it, but so far have not been successful. Considering avs answer the following idea emerged. Let be a strictly increasing continuouosly differentiable function. Let and . Then is a contraction in every open interval in (actually, a subcontraction) and is an expansion (in the sense that its inverse is a contraction) in every open interval in . Thus let be s.t. in A, in and in . Then must be a solution.","g f \int_{[a,b]}f(x)dx \geq \int_{g([a,b])}f(x)dx f \mu \mu([a,b]) \geq g_{*}(\mu)([a,b]) \textbf{Edit:} g A = \{x : g'(x) < 1\} B = \{x : g'(x) > 1\} g A B f f \leq 0 f \geq 0 B f = 0 (A \cup B)^c f","['real-analysis', 'functional-analysis', 'ordinary-differential-equations', 'measure-theory']"
37,"Prove that there exist four polynomials $p_1,p_2,p_3,p_4$ in $x,y,z$ so that $(x^2+y^2+z^2)^3-8(z^3x^3+x^3y^3+y^3z^3)=p_1^2+p_2^2+p_3^2+p_4^2$",Prove that there exist four polynomials  in  so that,"p_1,p_2,p_3,p_4 x,y,z (x^2+y^2+z^2)^3-8(z^3x^3+x^3y^3+y^3z^3)=p_1^2+p_2^2+p_3^2+p_4^2","Prove that there exist four polynomials $p_{1}, p_{2}, p_{3}, p_{4}$ in $x, y, z$ so that $$\left ( x^{2}+ y^{2}+ z^{2} \right )^{3}- 8\left ( z^{3}x^{3}+ x^{3}y^{3}+ y^{3}z^{3} \right )= p_{1}^{2}+ p_{2}^{2}+ p_{3}^{2}+ p_{4}^{2}$$ Source: AoPS/@Ji_Chen_ on.AoPS I like Ji Chen's sum of squares, which is very hard to make decomposition like they said. Here is the way of thinking of mine about this problem_ if $f= ab+ c= -ad+ e$ with $b, c, d, e\geq 0\Rightarrow f\geq 0,$ we have a new $f$ is an SOS $,\quad f:=\dfrac{cd+ e}{b+ d}$ I think finding SOS like this is very funny. One day, you try substitutions and you succeed, however, another day, that trick is not useful anymore. I found this formula_ on.StackMath 4 years ago. Very amazing result, I have thought of it till now, that's a travelled task.","Prove that there exist four polynomials in so that Source: AoPS/@Ji_Chen_ on.AoPS I like Ji Chen's sum of squares, which is very hard to make decomposition like they said. Here is the way of thinking of mine about this problem_ if with we have a new is an SOS I think finding SOS like this is very funny. One day, you try substitutions and you succeed, however, another day, that trick is not useful anymore. I found this formula_ on.StackMath 4 years ago. Very amazing result, I have thought of it till now, that's a travelled task.","p_{1}, p_{2}, p_{3}, p_{4} x, y, z \left ( x^{2}+ y^{2}+ z^{2} \right )^{3}- 8\left ( z^{3}x^{3}+ x^{3}y^{3}+ y^{3}z^{3} \right )= p_{1}^{2}+ p_{2}^{2}+ p_{3}^{2}+ p_{4}^{2} f= ab+ c= -ad+ e b, c, d, e\geq 0\Rightarrow f\geq 0, f ,\quad f:=\dfrac{cd+ e}{b+ d}","['real-analysis', 'polynomials']"
38,How to show two properties about the Cantor Set,How to show two properties about the Cantor Set,,"Define $C_0=[0,1]$ and for $n\in\mathbb{N}$ , define $$C_n=C_{n-1}\setminus\bigg(\bigcup_{k=0}^{3^{n-1}-1}\bigg(\frac{1+3k}{3^n},\frac{2+3k}{3^n}\bigg)\bigg) $$ Then the Cantor set is defined as $$C=\bigcap_{n\in\mathbb{N}}C_n$$ Things I need to show: (1) Each $C_n$ is the disjoint union of $2^n$ closed sub-intervals of length $3^{-n}$ and that the endpoints of each $C_n$ are in $C$ . (2) For any distinct $x,y\in C$ , there exists non-empty, disjoint, open sets (open in in $C$ ) $A,B\subset C$ such that $A\cup B=C$ with $x\in A$ and $y\in B$ . For (1) , using induction seemed to me to be the best way to go about proving (1) , since I am unsure how to go about proving it for an arbitrary $n\in\mathbb{N}$ . However, using the inductive hypothesis with this definition turned out to be unwieldy and awkward, and did not yield any results with the methods I tried. Proof outline for (2): Let $x,y\in C$ and WLOG, let $x<y$ . Choose $N\in\mathbb{N}$ such that $$3^{N-1}(y-x)>2$$ Now $$3^{N-1}x<3^{N-1}x+2<3^{N-1}y $$ so there is an integer $k\in\mathbb{N}$ such that $$3^{N-1}x<k<k+1<y3^{N-1}$$ Then $$x<\frac{3k}{3^{N}}<\frac{3(k+1)}{3^{N}}<y $$ and finally, we have $$x<\frac{3k}{3^{N}}<\frac{1+3k}{3^N}<\frac{2+3k}{3^N}<\frac{3(k+1)}{3^N}<y $$ The idea here was to find one of the ""deleted"" intervals which separated $x$ and $y$ . If we set $A=(-1,z)$ and $B=(z,2)$ for some $z$ between $\frac{1+3k}{3^N}$ and $\frac{2+3k}{3^N}$ , then $A\cap C$ and $B\cap C$ will be open in $C$ , they will be disjoint, and their union will equal $C$ . Questions: How should I proceed in proving (1) ? Is this proof for (2) sufficient?","Define and for , define Then the Cantor set is defined as Things I need to show: (1) Each is the disjoint union of closed sub-intervals of length and that the endpoints of each are in . (2) For any distinct , there exists non-empty, disjoint, open sets (open in in ) such that with and . For (1) , using induction seemed to me to be the best way to go about proving (1) , since I am unsure how to go about proving it for an arbitrary . However, using the inductive hypothesis with this definition turned out to be unwieldy and awkward, and did not yield any results with the methods I tried. Proof outline for (2): Let and WLOG, let . Choose such that Now so there is an integer such that Then and finally, we have The idea here was to find one of the ""deleted"" intervals which separated and . If we set and for some between and , then and will be open in , they will be disjoint, and their union will equal . Questions: How should I proceed in proving (1) ? Is this proof for (2) sufficient?","C_0=[0,1] n\in\mathbb{N} C_n=C_{n-1}\setminus\bigg(\bigcup_{k=0}^{3^{n-1}-1}\bigg(\frac{1+3k}{3^n},\frac{2+3k}{3^n}\bigg)\bigg)  C=\bigcap_{n\in\mathbb{N}}C_n C_n 2^n 3^{-n} C_n C x,y\in C C A,B\subset C A\cup B=C x\in A y\in B n\in\mathbb{N} x,y\in C x<y N\in\mathbb{N} 3^{N-1}(y-x)>2 3^{N-1}x<3^{N-1}x+2<3^{N-1}y  k\in\mathbb{N} 3^{N-1}x<k<k+1<y3^{N-1} x<\frac{3k}{3^{N}}<\frac{3(k+1)}{3^{N}}<y  x<\frac{3k}{3^{N}}<\frac{1+3k}{3^N}<\frac{2+3k}{3^N}<\frac{3(k+1)}{3^N}<y  x y A=(-1,z) B=(z,2) z \frac{1+3k}{3^N} \frac{2+3k}{3^N} A\cap C B\cap C C C","['real-analysis', 'general-topology', 'solution-verification', 'alternative-proof', 'cantor-set']"
39,Does this type of real functions have a name ? What are their characteristics?,Does this type of real functions have a name ? What are their characteristics?,,"Let $n > 0$ be an integer. Let $f:\mathbb R^+ \times \mathbb R^+ \rightarrow \mathbb R_0$ be a symmetric function such that the $(n+1) \times k$ matrix \begin{equation} \mathbb M= \begin{pmatrix} f(s_{k-n+1},s_1) & \ldots & f(s_{k-n+1},s_k)\\ \vdots &  & \vdots \\ f(s_{k+1},s_1) & \ldots & f(s_{k+1},s_k)\\ \end{pmatrix} \end{equation} has at most $n$ linearly independent rows for any $k>n$ and real sequence $s_1<s_2<\ldots<s_{k+1}$ . What does that say about $f$ ? Do such function have a name ? What are their characteristics/properties ? Is there an easier ""definition"" that would be equivalent, without matrices for example? Or a stronger ""easier"" property that would imply the above property ?","Let be an integer. Let be a symmetric function such that the matrix has at most linearly independent rows for any and real sequence . What does that say about ? Do such function have a name ? What are their characteristics/properties ? Is there an easier ""definition"" that would be equivalent, without matrices for example? Or a stronger ""easier"" property that would imply the above property ?","n > 0 f:\mathbb R^+ \times \mathbb R^+ \rightarrow \mathbb R_0 (n+1) \times k \begin{equation}
\mathbb M=
\begin{pmatrix}
f(s_{k-n+1},s_1) & \ldots & f(s_{k-n+1},s_k)\\
\vdots &  & \vdots \\
f(s_{k+1},s_1) & \ldots & f(s_{k+1},s_k)\\
\end{pmatrix}
\end{equation} n k>n s_1<s_2<\ldots<s_{k+1} f","['real-analysis', 'linear-algebra', 'matrices', 'functional-analysis', 'analysis']"
40,Uniqueness of solution of an integral equation [Updated],Uniqueness of solution of an integral equation [Updated],,"Problem. Be $f\in \mathcal{C}(\mathbf{R})$ and fix $c_0\in\mathbf{R}$ and $c_2<0$ . Consider the integral equation $$ f(y) = c_0 + c_2 y^2 + \int dz\, f(z) \exp(f(y-z)) $$ where the integral is over $(-\infty,\infty)$ . Under some conditions on $c_0$ and $c_2$ , it is possible to find $a_0$ and $a_2$ such that $f(y)=a_0+a_2 y^2$ is a solution. The problem is: Is this solution unique? Let's be free to choose specific (nontrivial) values for $c_0$ and $c_2$ if that simplifies the analysis; in particular though, let's keep $c_2\neq 0$ . Also, let's be free to assume that $f$ is smooth, e.g. $f\in \mathcal{C}^2(\mathbf{R})$ . Roughly speaking, the problem is how to prove that there is only one solution to an equation like $$ f * \exp f(y) = c_2 y^2 + f(y). $$ Attempt 1. From the form of the equation, it is natural to define the operator $$ F_r(f)(y) \triangleq c_0 + c_2 y^2 + \int_{-r}^r dz\, f(z) \exp(f(y-z)) $$ and regard the solution as a fixed point of $F_r$ for any fixed $r>0$ . So my first attempt would be to apply Banach fixed point theorem to $F_r$ . I don't know if this works at all, even in principle, since $f\in \mathcal{C}(\mathbf{R})$ , but for now this is what I have: \begin{align} \| F_r(f)-F_r(g) \|_\infty  & \leq \sup_{y\in\mathcal{B}_r(0)} \int_{-r}^r dz  \left| f(z)\exp(f(y-z)) - g(z)\exp(g(y-z)) \right| \end{align} where $\mathcal{B}_r(0)$ is the ball of radius $r$ and center the origin. From here, I should try to end up with something like $C\|f-g\|_\infty$ for some $C>0$ ... Attempt 2. A different approach is to show that, if $f^*$ and $g^*$ are two solutions, i.e., $F(f^*)=f^*$ and $F(g^*)=g^*$ , then $f^*=g^*$ . I tried the following: Since by assumption $f^*$ and $g^*$ are two solutions, then $$ f^*(y) - g^*(y) = F(f^*)(y) - F(g^*)(y) = \int dz\, \big[f^*(y-z) \exp(f^*(z))-g^*(y-z) \exp(g^*(z))\big]. $$ I should show that the above quantity is identically equal to zero (for all $y$ ). I would proceed by contradiction. Suppose that $f^*\neq g^*$ , that is, there exists $h=f^*-g^*$ that is not identically zero. Suppose that $h(y)\neq 0$ in a neighborhood of $y=y_0$ . Then I try to expand $h$ in Taylor series around $y=y_0$ . For any $y\neq y_0$ it results \begin{multline} 0 \neq h(y_0) + h'(y_0)(y-y_0) + \cdots \\ = \int dz\, \bigg[ ( f^*(y_0-z) + (f^*)'(y_0-z)(y-y_0+z) + \cdots) \exp(f^*(z)) \\ - ( g^*(y_0-z) + (g^*)'(y_0-z)(y-y_0+z) + \cdots) \exp(g^*(z))  ) \bigg]. \end{multline} In the end, I should lead to some contradiction, e.g. the integrand is zero... Attempt 3. I don't know if it is just a coincidence or not, but the solution $f(y)=a_0 + a_2 y^2$ has the same form of the term $c_0+c_2 y^2$ . In other words, if we define the operator $$ G(f) \triangleq f * \exp f - f $$ then the problem is to find $f$ such that $$ G(f)(y) = c_0 + c_2 y^2 .$$ Therefore, an approach may be to find the solution to a “Green equation” like $$G(\varphi)(y)=\delta_y$$ where $\delta_y$ represents a Dirac mass at $y$ . Such a $\varphi$ would (?) unlock the possibility of solving the “inhomogeneous” case.","Problem. Be and fix and . Consider the integral equation where the integral is over . Under some conditions on and , it is possible to find and such that is a solution. The problem is: Is this solution unique? Let's be free to choose specific (nontrivial) values for and if that simplifies the analysis; in particular though, let's keep . Also, let's be free to assume that is smooth, e.g. . Roughly speaking, the problem is how to prove that there is only one solution to an equation like Attempt 1. From the form of the equation, it is natural to define the operator and regard the solution as a fixed point of for any fixed . So my first attempt would be to apply Banach fixed point theorem to . I don't know if this works at all, even in principle, since , but for now this is what I have: where is the ball of radius and center the origin. From here, I should try to end up with something like for some ... Attempt 2. A different approach is to show that, if and are two solutions, i.e., and , then . I tried the following: Since by assumption and are two solutions, then I should show that the above quantity is identically equal to zero (for all ). I would proceed by contradiction. Suppose that , that is, there exists that is not identically zero. Suppose that in a neighborhood of . Then I try to expand in Taylor series around . For any it results In the end, I should lead to some contradiction, e.g. the integrand is zero... Attempt 3. I don't know if it is just a coincidence or not, but the solution has the same form of the term . In other words, if we define the operator then the problem is to find such that Therefore, an approach may be to find the solution to a “Green equation” like where represents a Dirac mass at . Such a would (?) unlock the possibility of solving the “inhomogeneous” case.","f\in \mathcal{C}(\mathbf{R}) c_0\in\mathbf{R} c_2<0  f(y) = c_0 + c_2 y^2 + \int dz\, f(z) \exp(f(y-z))  (-\infty,\infty) c_0 c_2 a_0 a_2 f(y)=a_0+a_2 y^2 c_0 c_2 c_2\neq 0 f f\in \mathcal{C}^2(\mathbf{R})  f * \exp f(y) = c_2 y^2 + f(y).   F_r(f)(y) \triangleq c_0 + c_2 y^2 + \int_{-r}^r dz\, f(z) \exp(f(y-z))  F_r r>0 F_r f\in \mathcal{C}(\mathbf{R}) \begin{align} \| F_r(f)-F_r(g) \|_\infty 
& \leq \sup_{y\in\mathcal{B}_r(0)} \int_{-r}^r dz  \left| f(z)\exp(f(y-z)) - g(z)\exp(g(y-z)) \right|
\end{align} \mathcal{B}_r(0) r C\|f-g\|_\infty C>0 f^* g^* F(f^*)=f^* F(g^*)=g^* f^*=g^* f^* g^*  f^*(y) - g^*(y) = F(f^*)(y) - F(g^*)(y) = \int dz\, \big[f^*(y-z) \exp(f^*(z))-g^*(y-z) \exp(g^*(z))\big].  y f^*\neq g^* h=f^*-g^* h(y)\neq 0 y=y_0 h y=y_0 y\neq y_0 \begin{multline} 0 \neq h(y_0) + h'(y_0)(y-y_0) + \cdots \\ = \int dz\, \bigg[ ( f^*(y_0-z) + (f^*)'(y_0-z)(y-y_0+z) + \cdots) \exp(f^*(z)) \\ - ( g^*(y_0-z) + (g^*)'(y_0-z)(y-y_0+z) + \cdots) \exp(g^*(z))  ) \bigg]. \end{multline} f(y)=a_0 + a_2 y^2 c_0+c_2 y^2  G(f) \triangleq f * \exp f - f  f  G(f)(y) = c_0 + c_2 y^2 . G(\varphi)(y)=\delta_y \delta_y y \varphi","['real-analysis', 'functional-analysis', 'functional-equations', 'integral-equations']"
41,Problem on Hahn-Banach and separation theorem,Problem on Hahn-Banach and separation theorem,,"A problem on Hahn-Banach theorem and convex separation theorem: Let $X$ be a real normed linear space. $E$ a convex subset of $X$ with nonempty interior, $F$ a linear subspace, with $(\operatorname{Int} E)\cap F=\emptyset$ . Then there exists a bounded linear functional $f\in X^*, f\not\equiv0$ , such that $f(E)\leq0,f(F)=0$ . I think the following corollary of Hahn-Banach theorem may be somewhat useful, but I can't figure out how to combine it with the convex separation theorem. Let $A$ be a linear subspace of normed linear space $X$ , $x_0\in X$ , $d(x_0,A)>0$ , then there exists $f\in X^*$ s.t. $f(A)=0,f(x_0)=d(x_0,A)$ and $||f||=1$ . Thanks for any help or suggestions.","A problem on Hahn-Banach theorem and convex separation theorem: Let be a real normed linear space. a convex subset of with nonempty interior, a linear subspace, with . Then there exists a bounded linear functional , such that . I think the following corollary of Hahn-Banach theorem may be somewhat useful, but I can't figure out how to combine it with the convex separation theorem. Let be a linear subspace of normed linear space , , , then there exists s.t. and . Thanks for any help or suggestions.","X E X F (\operatorname{Int} E)\cap F=\emptyset f\in X^*, f\not\equiv0 f(E)\leq0,f(F)=0 A X x_0\in X d(x_0,A)>0 f\in X^* f(A)=0,f(x_0)=d(x_0,A) ||f||=1","['real-analysis', 'functional-analysis', 'convex-analysis', 'hahn-banach-theorem']"
42,Closing the loophole in this real analysis paper,Closing the loophole in this real analysis paper,,"In this paper , Theorem 1 states, given $F$ an arbitrary ordered subfield of $\mathbb{R}$ , $F$ is complete iff every continuous function defined on a closed and bounded interval has a uniformly differentiable anti-derivative. The authors mention that they have not been able to show whether or not the ""uniformly differentiable"" hypothesis can be dropped without penalty. So, my question is, has anyone done further research to show whether or not this loophole can be closed? Can the uniform differentiability hypothesis be dropped without affecting the theorem?","In this paper , Theorem 1 states, given an arbitrary ordered subfield of , is complete iff every continuous function defined on a closed and bounded interval has a uniformly differentiable anti-derivative. The authors mention that they have not been able to show whether or not the ""uniformly differentiable"" hypothesis can be dropped without penalty. So, my question is, has anyone done further research to show whether or not this loophole can be closed? Can the uniform differentiability hypothesis be dropped without affecting the theorem?",F \mathbb{R} F,['real-analysis']
43,Why is $\frac{\partial x}{\partial y} = \frac{dx/dt}{dy/dt}$ if $x$ and $y$ are unknown?,Why is  if  and  are unknown?,\frac{\partial x}{\partial y} = \frac{dx/dt}{dy/dt} x y,"Given $\frac{dx}{dt} = f(x,y,t)$ and $\frac{dy}{dt} = g(x,y,t)$ . Suppose the explicit form of $x$ and $y$ are unsolvable, then the phase plane is usually considered by looking at $\frac{dx}{dy}$ . This represents how much $x$ varies with respect to $y$ , which is also $\frac{\partial x}{\partial y}$ . Thus, $$\frac{\partial x}{\partial y} = \frac{dx/dt}{dy/dt}.$$ This means that without knowing $x$ and $y$ , I can find the derivative of $x$ with respect to $y$ and vice versa. Is there any problem with this? In a hand-wavy manner, I am wondering if the last equality in the following makes sense $$\frac{\partial x}{\partial y} = \frac{\partial x}{\partial t} \frac{\partial t}{\partial y} = \frac{\partial x/\partial t}{\partial y/\partial t} = \frac{dx}{dy}.$$ As Lee Mosher points out, the case of $dy/dt=0$ (or even $dy/dt \equiv 0$ ) needs to be considered separately. For this question, I am most interested in the case when $dy/dt >0$ .","Given and . Suppose the explicit form of and are unsolvable, then the phase plane is usually considered by looking at . This represents how much varies with respect to , which is also . Thus, This means that without knowing and , I can find the derivative of with respect to and vice versa. Is there any problem with this? In a hand-wavy manner, I am wondering if the last equality in the following makes sense As Lee Mosher points out, the case of (or even ) needs to be considered separately. For this question, I am most interested in the case when .","\frac{dx}{dt} = f(x,y,t) \frac{dy}{dt} = g(x,y,t) x y \frac{dx}{dy} x y \frac{\partial x}{\partial y} \frac{\partial x}{\partial y} = \frac{dx/dt}{dy/dt}. x y x y \frac{\partial x}{\partial y} = \frac{\partial x}{\partial t}
\frac{\partial t}{\partial y} = \frac{\partial x/\partial t}{\partial y/\partial t}
= \frac{dx}{dy}. dy/dt=0 dy/dt \equiv 0 dy/dt >0","['real-analysis', 'calculus', 'partial-derivative', 'dynamical-systems']"
44,L' Hospital's Rule with general measurable function,L' Hospital's Rule with general measurable function,,"I come up with this question when I read L'Hospital's rule and think about non-continuous function case, more specifically, assume $0\leq g(x)\leq 1$ , not necessarily continuous but a measurable function w.r.t Lebesgue measure and $$\lim_{\delta\rightarrow 0} \frac{1}{\delta}\int_0^\delta g(x)dx = C$$ for some real number C. The integration is defined in the Lebesgue sense. For another function $f(x)$ , defined on $[0,1]$ , continuous and monotonic increasing, $f(0) = 0$ , could we have the following hold: $$\lim_{\delta\rightarrow 0} \frac{\int_0^\delta f(x)g(x)dx}{\int_0^\delta f(x)dx} = C$$ I raise this question because if $g(x)$ is continuous, then by L'Hospital's rule, $g(\delta)\rightarrow C$ as $\delta\rightarrow 0$ , and the above limit is clearly held by applying of the L'Hospital's rule again. But here what if there is no such good regularity of $g(x)$ ? For example, $g(x)$ could have no limit when $x$ goes to $0$ . If the second limit formula does not hold for all $f(x)$ function with the mentioned conditions, then could we impose more regularity of $f(x)$ to make this true? Thank you so much!","I come up with this question when I read L'Hospital's rule and think about non-continuous function case, more specifically, assume , not necessarily continuous but a measurable function w.r.t Lebesgue measure and for some real number C. The integration is defined in the Lebesgue sense. For another function , defined on , continuous and monotonic increasing, , could we have the following hold: I raise this question because if is continuous, then by L'Hospital's rule, as , and the above limit is clearly held by applying of the L'Hospital's rule again. But here what if there is no such good regularity of ? For example, could have no limit when goes to . If the second limit formula does not hold for all function with the mentioned conditions, then could we impose more regularity of to make this true? Thank you so much!","0\leq g(x)\leq 1 \lim_{\delta\rightarrow 0} \frac{1}{\delta}\int_0^\delta g(x)dx = C f(x) [0,1] f(0) = 0 \lim_{\delta\rightarrow 0} \frac{\int_0^\delta f(x)g(x)dx}{\int_0^\delta f(x)dx} = C g(x) g(\delta)\rightarrow C \delta\rightarrow 0 g(x) g(x) x 0 f(x) f(x)","['real-analysis', 'measure-theory', 'continuity']"
45,Interchanging supremum with infimum,Interchanging supremum with infimum,,"Let $f:(0,a) \times (0,b) \to \mathbb{R}$ be a given function. Under what conditions is it true that $$\sup_{x\in(0,a)} \inf_{y \in (0,b)} f(x,y) = \inf_{y \in (0,b)} \sup_{x\in(0,a)} f(x,y) \tag{1}$$ There is a related question saying that the inequality $""\leq""$ in $(1)$ holds true, however in general one cannot expect the converse inequality $""\geq""$ . I'm interested in some additional assumptions that will make $(1)$ true. Any references will be much appreciated. For example if we assume that $f$ is increasing in $y$ and decreasing in $x$ , then $\sup_{x \in (0,a)}$ and $\inf_{y \in (0,b)}$ in $(1)$ can be replaced with $\lim_{x\to 0}$ and $\lim_{y\to 0}$ respectively. Then $(1)$ would hold for example if the double limit at $(0,0)$ exists. But these are very restrictive assumptions, anything more general?","Let be a given function. Under what conditions is it true that There is a related question saying that the inequality in holds true, however in general one cannot expect the converse inequality . I'm interested in some additional assumptions that will make true. Any references will be much appreciated. For example if we assume that is increasing in and decreasing in , then and in can be replaced with and respectively. Then would hold for example if the double limit at exists. But these are very restrictive assumptions, anything more general?","f:(0,a) \times (0,b) \to \mathbb{R} \sup_{x\in(0,a)} \inf_{y \in (0,b)} f(x,y) = \inf_{y \in (0,b)} \sup_{x\in(0,a)} f(x,y) \tag{1} ""\leq"" (1) ""\geq"" (1) f y x \sup_{x \in (0,a)} \inf_{y \in (0,b)} (1) \lim_{x\to 0} \lim_{y\to 0} (1) (0,0)","['real-analysis', 'sequences-and-series', 'supremum-and-infimum']"
46,"Show that there is a constant $M$ such that for all $x,y \in X$ we have $|f(x) - f(y)| \leq M |x-y| + \epsilon$.",Show that there is a constant  such that for all  we have .,"M x,y \in X |f(x) - f(y)| \leq M |x-y| + \epsilon","Full problem statement: Let $X \subset \mathbb{R}^m$ be compact and $f : X \rightarrow \mathbb{R}$ be continuous. Given $\epsilon > 0$ , show that there is a constant $M$ such that for all $x,y \in X$ we have $|f(x) - f(y)| \leq M |x-y| + \epsilon$ . Please check my solution for correctness below: Solution: Assume on the contrary that there exists an $\epsilon > 0$ such that $\forall M \ \exists x,y \in X$ s.t. $|f(x) - f(y)| > M|x-y| + \epsilon$ . First note that $Im\ f = f(X) \subset \mathbb{R}$ is the continuous image of a compact set, so it is compact, and so, closed and bounded. By noting that the left hand side is bounded above and by taking large enough values of $M$ , we see that there are two sequences of points in $X$ - $(x_n)_0^\infty, (y_n)_0^\infty$ such that $d_X(x_n, y_n) < \frac{1}{2^n}$ while $|f(x_n) - f(y_n)| > \epsilon$ for all $n \in \mathbb{N}_0$ . By compactness of $X$ , $(x_n)$ has a subsequence $x_{n_k}$ that converges to a point $l_1 \in X$ . Again, $(y_{n_k})$ has subsequence $(y_{n_{k(l)}})$ that converges to a limit $l_2 \in X$ . Considering the fact that $d_X(x_{n_{k(l)}}, y_{n_{k(l)}}) < \frac{1}{2^n}$ , we conclude that $l_1 = l_2$ . That is, we have, $(x_{n_{k(l)}}) \rightarrow l,\ (y_{n_{k(l)}}) \rightarrow l$ for some $l \in X$ (compactness). By continuity of $f$ , we must have the sequences $f(x_{n_{k(l)}})$ and $f(y_{n_{k(l)}})$ converge to the same limit $f(l) \in \mathbb{R}$ . But this is not possible because $|f(x_{n_{k(l)}}) - f(y_{n_{k(l)}})| > \epsilon$ for all $n_{k(l)}$ . Thus, there cannot exist such an $\epsilon > 0$ , and the proposition follows. $\square$","Full problem statement: Let be compact and be continuous. Given , show that there is a constant such that for all we have . Please check my solution for correctness below: Solution: Assume on the contrary that there exists an such that s.t. . First note that is the continuous image of a compact set, so it is compact, and so, closed and bounded. By noting that the left hand side is bounded above and by taking large enough values of , we see that there are two sequences of points in - such that while for all . By compactness of , has a subsequence that converges to a point . Again, has subsequence that converges to a limit . Considering the fact that , we conclude that . That is, we have, for some (compactness). By continuity of , we must have the sequences and converge to the same limit . But this is not possible because for all . Thus, there cannot exist such an , and the proposition follows.","X \subset \mathbb{R}^m f : X \rightarrow \mathbb{R} \epsilon > 0 M x,y \in X |f(x) - f(y)| \leq M |x-y| + \epsilon \epsilon > 0 \forall M \ \exists x,y \in X |f(x) - f(y)| > M|x-y| + \epsilon Im\ f = f(X) \subset \mathbb{R} M X (x_n)_0^\infty, (y_n)_0^\infty d_X(x_n, y_n) < \frac{1}{2^n} |f(x_n) - f(y_n)| > \epsilon n \in \mathbb{N}_0 X (x_n) x_{n_k} l_1 \in X (y_{n_k}) (y_{n_{k(l)}}) l_2 \in X d_X(x_{n_{k(l)}}, y_{n_{k(l)}}) < \frac{1}{2^n} l_1 = l_2 (x_{n_{k(l)}}) \rightarrow l,\ (y_{n_{k(l)}}) \rightarrow l l \in X f f(x_{n_{k(l)}}) f(y_{n_{k(l)}}) f(l) \in \mathbb{R} |f(x_{n_{k(l)}}) - f(y_{n_{k(l)}})| > \epsilon n_{k(l)} \epsilon > 0 \square","['real-analysis', 'continuity', 'metric-spaces', 'solution-verification', 'compactness']"
47,Find $n$ such that $1-a c^{n-1} \ge \exp(-\frac{1}{n})$,Find  such that,n 1-a c^{n-1} \ge \exp(-\frac{1}{n}),"I am trying to find the integer $n$ such that \begin{align} 1-a c^{n-1} \ge \exp(-\frac{1}{n}) \end{align} where $a>0$ and $c \in (0,1)$ . I know that finding it exactly is difficult. However, can one find good upper and lower bounds it. It tried using lower bound $\exp(-x) \le 1-x+\frac{1}{2}x^2$ . However, it didn't really work.","I am trying to find the integer such that where and . I know that finding it exactly is difficult. However, can one find good upper and lower bounds it. It tried using lower bound . However, it didn't really work.","n \begin{align}
1-a c^{n-1} \ge \exp(-\frac{1}{n})
\end{align} a>0 c \in (0,1) \exp(-x) \le 1-x+\frac{1}{2}x^2","['real-analysis', 'algebra-precalculus', 'upper-lower-bounds']"
48,An application of Fubini’s theorem on Fourier transform,An application of Fubini’s theorem on Fourier transform,,"Given $f,g\in L^1(\mathbb{R}^n)$ and we denote the Fourier transform of $f$ by $\widehat{f}$ . I want to prove that $$\int_{\mathbb{R}^n}\widehat{f}(x)g(x)~dx= \int_{\mathbb{R}^n}f(x)\widehat{g}(x)~dx.$$ Here’s my attempt: \begin{align} \int_{\mathbb{R}^n}\widehat{f}(x)g(x)~dx=&\int _{\mathbb{R}^n}\left\{\int _{\mathbb{R}^n} f(t)e^{-2\pi it\cdot x}~dt\right\}g(x)dx\\ =&\int _{\mathbb{R}^n}\left\{\int _{\mathbb{R}^n} g(x)f(t)e^{-2\pi it\cdot x}~dt\right\}dx\\ {\color{red}{=}}&\int _{\mathbb{R}^n}\left\{\int_{\mathbb{R}^n} g(x)f(t)e^{-2\pi it\cdot x}dx\right\}~dt\\ =&\int_{\mathbb{R}^n}\left\{\int_{\mathbb{R}^n} g(x)e^{-2\pi it\cdot x}dx\right\}f(t)~dt\\ =&\int_{\mathbb{R}^n}f(t)\widehat{g}(t)~dt\\ =&\int_{\mathbb{R}^n}f(x)\widehat{g}(x)dx. \end{align} In the third equation I used Fubini’s theorem. But here’s something that I’m not sure : if we want to apply Fubini’s theorem, then $F(x,t):= g(x)f(t)e^{-2\pi it\cdot x}$ must be $\mathbb{R}^n\times\mathbb{R}^n$ integrable. But I was stuck when trying to prove that $F(x,t)$ is integrable. Could you give me some help? Thanks!","Given and we denote the Fourier transform of by . I want to prove that Here’s my attempt: In the third equation I used Fubini’s theorem. But here’s something that I’m not sure : if we want to apply Fubini’s theorem, then must be integrable. But I was stuck when trying to prove that is integrable. Could you give me some help? Thanks!","f,g\in L^1(\mathbb{R}^n) f \widehat{f} \int_{\mathbb{R}^n}\widehat{f}(x)g(x)~dx= \int_{\mathbb{R}^n}f(x)\widehat{g}(x)~dx. \begin{align}
\int_{\mathbb{R}^n}\widehat{f}(x)g(x)~dx=&\int _{\mathbb{R}^n}\left\{\int _{\mathbb{R}^n} f(t)e^{-2\pi it\cdot x}~dt\right\}g(x)dx\\
=&\int _{\mathbb{R}^n}\left\{\int _{\mathbb{R}^n} g(x)f(t)e^{-2\pi it\cdot x}~dt\right\}dx\\
{\color{red}{=}}&\int _{\mathbb{R}^n}\left\{\int_{\mathbb{R}^n} g(x)f(t)e^{-2\pi it\cdot x}dx\right\}~dt\\
=&\int_{\mathbb{R}^n}\left\{\int_{\mathbb{R}^n} g(x)e^{-2\pi it\cdot x}dx\right\}f(t)~dt\\
=&\int_{\mathbb{R}^n}f(t)\widehat{g}(t)~dt\\
=&\int_{\mathbb{R}^n}f(x)\widehat{g}(x)dx.
\end{align} F(x,t):= g(x)f(t)e^{-2\pi it\cdot x} \mathbb{R}^n\times\mathbb{R}^n F(x,t)","['real-analysis', 'metric-spaces']"
49,"Prove that if $b_n$ is a subsequence of $a_n$ and $c_n$ is a subsequence of $b_n$, then $c_n$ is a subsequence of $a_n$.","Prove that if  is a subsequence of  and  is a subsequence of , then  is a subsequence of .",b_n a_n c_n b_n c_n a_n,"Let $(a_{n})_{n=0}^{\infty}$ , $(b_{n})_{n=0}^{\infty}$ and $(c_{n})_{n=m}^{\infty}$ be sequences of real numbers. Then $(a_{n})_{n=0}^{\infty}$ is a subsequence of $(a_{n})_{n=0}^{\infty}$ . Furthermore, if $(b_{n})_{n=0}^{\infty}$ is a subsequence of $(a_{n})_{n=0}^{\infty}$ , and $(c_{n})_{n=0}^{\infty}$ is a subsequence of $(b_{n})_{n=0}^{\infty}$ , then $(c_{n})_{n=0}^{\infty}$ is a subsequence of $(a_{n})_{n=0}^{\infty}$ . My solution We say that a sequence of real numbers $(x_{n})_{n=0}^{\infty}$ is a subsequence of $(y_{n})_{n=0}^{\infty}$ if there exists a strictly increasing function $f:\textbf{N}\to\textbf{N}$ such that $x_{n} = y_{f(n)}$ . Based on this definition, we conclude that $a_{n}$ is a subsequence of itself: it suffices to choose $f(n) = n$ . On the other hand, if $b_{n}$ is a subsequence of $a_{n}$ and $c_{n}$ is a subsequence of $b_{n}$ , then exist functions $f:\textbf{N}\to\textbf{N}$ and $g:\textbf{N}\to\textbf{N}$ such that $b_{n} = a_{f(n)}$ and $c_{n} = b_{g(n)}$ . Consequently, $c_{n} = a_{f(g(n))}$ , where $f\circ g$ is strictly increasing because it is a composition of two strictly increasing functions. Could someone please tell me if I am missing any formal step? Any comment is appreciated!","Let , and be sequences of real numbers. Then is a subsequence of . Furthermore, if is a subsequence of , and is a subsequence of , then is a subsequence of . My solution We say that a sequence of real numbers is a subsequence of if there exists a strictly increasing function such that . Based on this definition, we conclude that is a subsequence of itself: it suffices to choose . On the other hand, if is a subsequence of and is a subsequence of , then exist functions and such that and . Consequently, , where is strictly increasing because it is a composition of two strictly increasing functions. Could someone please tell me if I am missing any formal step? Any comment is appreciated!",(a_{n})_{n=0}^{\infty} (b_{n})_{n=0}^{\infty} (c_{n})_{n=m}^{\infty} (a_{n})_{n=0}^{\infty} (a_{n})_{n=0}^{\infty} (b_{n})_{n=0}^{\infty} (a_{n})_{n=0}^{\infty} (c_{n})_{n=0}^{\infty} (b_{n})_{n=0}^{\infty} (c_{n})_{n=0}^{\infty} (a_{n})_{n=0}^{\infty} (x_{n})_{n=0}^{\infty} (y_{n})_{n=0}^{\infty} f:\textbf{N}\to\textbf{N} x_{n} = y_{f(n)} a_{n} f(n) = n b_{n} a_{n} c_{n} b_{n} f:\textbf{N}\to\textbf{N} g:\textbf{N}\to\textbf{N} b_{n} = a_{f(n)} c_{n} = b_{g(n)} c_{n} = a_{f(g(n))} f\circ g,"['real-analysis', 'sequences-and-series', 'solution-verification']"
50,"Is there measurable function f: $\mathrm\forall a\int_{0}^{1}{\frac{1}{f(x)-a}}\, d\mu<\infty$?",Is there measurable function f: ?,"\mathrm\forall a\int_{0}^{1}{\frac{1}{f(x)-a}}\, d\mu<\infty","I would like to find measurable function f: $\mathrm\forall a\int_{0}^{1}{\frac{1}{f(x)-a}}\, d\mu<\infty$ , but I can't find any such function. I know, that f can't be continuous. If f is continuous, we can prove that such integral is not finite by contradiction, using Fatou and Fubini's theorem. However I can't prove there is no such function, if f can be non-continious. Could anybody help me?","I would like to find measurable function f: , but I can't find any such function. I know, that f can't be continuous. If f is continuous, we can prove that such integral is not finite by contradiction, using Fatou and Fubini's theorem. However I can't prove there is no such function, if f can be non-continious. Could anybody help me?","\mathrm\forall a\int_{0}^{1}{\frac{1}{f(x)-a}}\, d\mu<\infty","['real-analysis', 'functions', 'lebesgue-integral']"
51,Any known bounds for convex function say $f$ with $L$-Lipschitz continuous gradient: $( x - y)^T A \left( \nabla f(x) - \nabla f(y)\right)$?,Any known bounds for convex function say  with -Lipschitz continuous gradient: ?,f L ( x - y)^T A \left( \nabla f(x) - \nabla f(y)\right),"There are several known bounds for a convex function say $f$ with $L$ -Lipschitz continuous gradient, for instance, \begin{align} \left( x - y \right)^T  \left( \nabla f(x) - \nabla f(y)\right) \leq L \| x - y \|_2^2, \forall x, y . \end{align} Exhaustive list can example be found here . Now, I am wondering if there is any known bound for this case, \begin{align} \left( x - y \right)^T {\color{red} A} \left( \nabla f(x) - \nabla f(y)\right) \leq \ {\color{red}?}, \forall x, y , \end{align} where $A$ is a square matrix . Questions If there is any lower bound, what is it and how to derive that? Moreover, what are the requirements on such a matrix $A$ ? Positive (semi)definite? (except the trivial case where matrix $A$ is a scaled identity)","There are several known bounds for a convex function say with -Lipschitz continuous gradient, for instance, Exhaustive list can example be found here . Now, I am wondering if there is any known bound for this case, where is a square matrix . Questions If there is any lower bound, what is it and how to derive that? Moreover, what are the requirements on such a matrix ? Positive (semi)definite? (except the trivial case where matrix is a scaled identity)","f L \begin{align}
\left( x - y \right)^T  \left( \nabla f(x) - \nabla f(y)\right) \leq L \| x - y \|_2^2, \forall x, y .
\end{align} \begin{align}
\left( x - y \right)^T {\color{red} A} \left( \nabla f(x) - \nabla f(y)\right) \leq \ {\color{red}?}, \forall x, y ,
\end{align} A A A","['real-analysis', 'functional-analysis', 'convex-analysis', 'lipschitz-functions']"
52,What properties must $f$ have if $f(x)=f(\sin(\pi x)+x)\iff x\in\Bbb{Z}$?,What properties must  have if ?,f f(x)=f(\sin(\pi x)+x)\iff x\in\Bbb{Z},"This is a follow-up from my previous question . I now know that the statement: $$f(x)=f(\sin(\pi x)+x)\iff x\in\Bbb{Z}$$ is not true for all $f$ . For example, $f$ can be $x$ to any constant power or any constant to the $x$ th power but it cannot be the gamma function $\Gamma(x)$ or $\sin(x)$ or $x^x$ . According to the answer I received, it is important to note whether or not $f$ is injective. However, $f(x)=x^2$ is not injective, yet it satisfies the statement. If being injective is only a sufficient condition as opposed to a necessary condition, what exactly do we know about the class of functions that makes this statement true? Thanks in advance!","This is a follow-up from my previous question . I now know that the statement: is not true for all . For example, can be to any constant power or any constant to the th power but it cannot be the gamma function or or . According to the answer I received, it is important to note whether or not is injective. However, is not injective, yet it satisfies the statement. If being injective is only a sufficient condition as opposed to a necessary condition, what exactly do we know about the class of functions that makes this statement true? Thanks in advance!",f(x)=f(\sin(\pi x)+x)\iff x\in\Bbb{Z} f f x x \Gamma(x) \sin(x) x^x f f(x)=x^2,"['real-analysis', 'functions']"
53,Find the limit of a random variable,Find the limit of a random variable,,"I'd like to find distribution of the almost sure limit $X_\infty$ of $X_{t+1}=  \begin{cases}         1-p+pX_t & \text{w/prob: } X_t \\       pX_t & \text{w/prob: } 1-X_t    \end{cases}$ where $X_t$ is a random variable (and also a martingale), and $p\in [0,1]$ . Firstly, to show it converges a.s., i need to show $P(\lim \sup X_n = \lim \inf X_n)=1.$ (Which i guess makes sense intuitively, as sup $X_n$ is evaluated when $X_n=1$ and inf $X_n$ when $X_n=0$ , right?) Then i'm not sure how to  take the limit for any $\omega \in \Omega$ , i.e. $\underset{n \to \infty}{\lim} X_n(\omega)$ .. i guess looking at probabilites since it is discrete and applying Borel Cantelli or something?","I'd like to find distribution of the almost sure limit of where is a random variable (and also a martingale), and . Firstly, to show it converges a.s., i need to show (Which i guess makes sense intuitively, as sup is evaluated when and inf when , right?) Then i'm not sure how to  take the limit for any , i.e. .. i guess looking at probabilites since it is discrete and applying Borel Cantelli or something?","X_\infty X_{t+1}=
 \begin{cases}  
      1-p+pX_t & \text{w/prob: } X_t \\
      pX_t & \text{w/prob: } 1-X_t
   \end{cases} X_t p\in [0,1] P(\lim \sup X_n = \lim \inf X_n)=1. X_n X_n=1 X_n X_n=0 \omega \in \Omega \underset{n \to \infty}{\lim} X_n(\omega)","['real-analysis', 'limits', 'probability-theory', 'measure-theory']"
54,Is the minimum of this optimization problem essentially unique?,Is the minimum of this optimization problem essentially unique?,,"Let $h:\mathbb R^{>0}\to \mathbb R^{\ge 0}$ be a smooth function, satisfying $h(1)=0$ , and suppose that $h(x)$ is strictly increasing on $[1,\infty)$ , and strictly decreasing on $(0,1]$ . Let $s>0$ be a parameter, and define $ F(s)=\min_{xy=s,x,y>0} g(x,y), $ where $g(x,y):=h(x)+ h(y)$ . Question: Can the minimum be obtained at two essentially different points? That is, suppose that $F(s)=g(x,y)=g(\tilde x,\tilde y)$ , for some $x,y,\tilde x,\tilde y>0$ satisfying $xy=\tilde x \tilde y=s$ . Is it true that $$ (x,y)=(\tilde x,\tilde y) \, \, \, \text{ or } \,\, (x,y)=(\tilde y,\tilde x)?$$ By symmetry, we can assume W.L.O.G that $x \le \sqrt{s}$ . It is not hard to see that the minimum must be obtained at a point where $x, y \le 1$ (if $s \le 1$ ) or $x,y \ge 1$ (if $s \ge 1$ ). Thus, if $s \le 1$ , then we have $x,y=\frac{s}{x} \le 1$ , which implies $s \le x \le \sqrt{s}$ . Edit: I tried to produce counter-examples by using $g$ which are invariant under some automorphism of the hyperbola $xy=s$ . (Then the set of minimizers is closed under the operation of this automorphism). I couldn't find such an automorphism which preserve the special additive structure of $g$ . Here is a partial analysis of the question for local minima: Set $\psi(x)=h(x)+h(\frac{s}{x})$ . Then $$\psi'(x)=h'(x)-h'(\frac{s}{x})\frac{s}{x^2}, \tag{1}$$ and $$\psi''(x)=h''(x)+h''(\frac{s}{x})\frac{s^2}{x^4}+2h'(\frac{s}{x})\frac{s}{x^3}. \tag{2}$$ Now, suppose $x$ is a local minimum of $\psi$ . Then, equations $(1),(2)$ imply that $$ h'(x)=h'(\frac{s}{x})\frac{s}{x^2} \, \, , \, \, h''(x)+h''(\frac{s}{x})\frac{s^2}{x^4}+2\frac{h'(x)}{x} \ge 0\tag{3}. $$ Subquestion: Suppose that $x,y$ satisfy $(3)$ . Does $x=y$ or $x=\frac{s}{y} $ hold?","Let be a smooth function, satisfying , and suppose that is strictly increasing on , and strictly decreasing on . Let be a parameter, and define where . Question: Can the minimum be obtained at two essentially different points? That is, suppose that , for some satisfying . Is it true that By symmetry, we can assume W.L.O.G that . It is not hard to see that the minimum must be obtained at a point where (if ) or (if ). Thus, if , then we have , which implies . Edit: I tried to produce counter-examples by using which are invariant under some automorphism of the hyperbola . (Then the set of minimizers is closed under the operation of this automorphism). I couldn't find such an automorphism which preserve the special additive structure of . Here is a partial analysis of the question for local minima: Set . Then and Now, suppose is a local minimum of . Then, equations imply that Subquestion: Suppose that satisfy . Does or hold?","h:\mathbb R^{>0}\to \mathbb R^{\ge 0} h(1)=0 h(x) [1,\infty) (0,1] s>0 
F(s)=\min_{xy=s,x,y>0} g(x,y),
 g(x,y):=h(x)+ h(y) F(s)=g(x,y)=g(\tilde x,\tilde y) x,y,\tilde x,\tilde y>0 xy=\tilde x \tilde y=s  (x,y)=(\tilde x,\tilde y) \, \, \, \text{ or } \,\, (x,y)=(\tilde y,\tilde x)? x \le \sqrt{s} x, y \le 1 s \le 1 x,y \ge 1 s \ge 1 s \le 1 x,y=\frac{s}{x} \le 1 s \le x \le \sqrt{s} g xy=s g \psi(x)=h(x)+h(\frac{s}{x}) \psi'(x)=h'(x)-h'(\frac{s}{x})\frac{s}{x^2}, \tag{1} \psi''(x)=h''(x)+h''(\frac{s}{x})\frac{s^2}{x^4}+2h'(\frac{s}{x})\frac{s}{x^3}. \tag{2} x \psi (1),(2) 
h'(x)=h'(\frac{s}{x})\frac{s}{x^2} \, \, , \, \, h''(x)+h''(\frac{s}{x})\frac{s^2}{x^4}+2\frac{h'(x)}{x} \ge 0\tag{3}.
 x,y (3) x=y x=\frac{s}{y} ","['real-analysis', 'multivariable-calculus', 'optimization', 'symmetric-functions', 'global-optimization']"
55,Difference over interval for periodic-like function,Difference over interval for periodic-like function,,"Let $F:\mathbb{R}\to\mathbb{R}$ be a continuous and nondecreasing function such that $F(0)=0$ and $F(x+1) = F(x)+1$ for all $x\in\mathbb{R}$ . Moreover, $F(0.4)=F(0.5)=0.5$ and $F(0.9)=1$ . Let $A(F,a)$ be the set containing all $x\in[0,1)$ s. t. $F(x+0.4)-F(x)\geq a$ . What is the largest $a$ such that the Lebesgue measure of $A(F,a)$ is always at least $0.5$ ? Note that $A(F,0) = [0,1)$ , so the largest $a$ is nonnegative. Without the extra condition on the values of $F$ , it could be that $F$ shoots up from $0$ to $1$ around some point in $[0,1)$ , which will imply that the largest $a$ is $0$ . But this condition means that the increase in the value of $F$ needs to be somewhat spread out over the interval, which disallows such functions. A related question , where we want to find the smallest size of $A(F,0.4)$ (without the new condition).","Let be a continuous and nondecreasing function such that and for all . Moreover, and . Let be the set containing all s. t. . What is the largest such that the Lebesgue measure of is always at least ? Note that , so the largest is nonnegative. Without the extra condition on the values of , it could be that shoots up from to around some point in , which will imply that the largest is . But this condition means that the increase in the value of needs to be somewhat spread out over the interval, which disallows such functions. A related question , where we want to find the smallest size of (without the new condition).","F:\mathbb{R}\to\mathbb{R} F(0)=0 F(x+1) = F(x)+1 x\in\mathbb{R} F(0.4)=F(0.5)=0.5 F(0.9)=1 A(F,a) x\in[0,1) F(x+0.4)-F(x)\geq a a A(F,a) 0.5 A(F,0) = [0,1) a F F 0 1 [0,1) a 0 F A(F,0.4)","['real-analysis', 'inequality', 'optimization']"
56,About the limit of a recursive sequence,About the limit of a recursive sequence,,"The question is the following: $x_0 >0$ , $\forall n \in \mathbb{N},x_{n+1}=|x_n - n|$ . Prove that $\lim_{n\to \infty} \frac{x_n}{n} = \frac{1}{2}$ I tried to remove the absolute value treating two cases: if, for any n, the n-th term of the sequence is greater than n, then the sequence $({x_n})$ converges and thus $(\frac{x_n}{n})$ tends to 0 as n tend to infinity. I wasn't able to deduce anything when $x_n<n$","The question is the following: , . Prove that I tried to remove the absolute value treating two cases: if, for any n, the n-th term of the sequence is greater than n, then the sequence converges and thus tends to 0 as n tend to infinity. I wasn't able to deduce anything when","x_0 >0 \forall n \in \mathbb{N},x_{n+1}=|x_n - n| \lim_{n\to \infty} \frac{x_n}{n} = \frac{1}{2} ({x_n}) (\frac{x_n}{n}) x_n<n","['real-analysis', 'sequences-and-series', 'limits']"
57,Approximate recursion defined by alternating summation?,Approximate recursion defined by alternating summation?,,"Problem & Question Given a constant $k\ge 2$ , we define the sequence $a_k(0)=0,a_k(1)=1$ and for $n\ge2$ : $$ a_k(n)=\sum_{i=1}^{\left[\frac{n}{k}\right]} (-1)^{n-i+1} a_k(i) $$ Where $\left[\frac{n}{k}\right]$ gives integer part of $\frac{n}{k}$ . (The $\left[\space\right]$ represents truncation.) For example, for $k=2$ we have the following scatterplot: [Click to see animated gif] . It appear as two interchanging curves repeat [following shape] in larger forms as we increase $n$ . Given $k$ , can we find a real equation that would approximate ""the curves"" given by $a_k(n)$ ? That is, the plot given by $a_k(n)$ can be defined as $\Gamma_1\cup\Gamma_2$ where $\Gamma_1,\Gamma_2$ are those ""the curves"". Specifically, $\Gamma_1,\Gamma_2$ are graphs of two real functions $\gamma_1(x),\gamma_2(x)$ where $\gamma_1(x)=-\gamma_2(x)$ as they are symmetric over $y$ -axis. The goal is to define them for $x\in\mathbb R$ . Currently, you can say that $a_k(n)$ approximates $\gamma_1,\gamma_2$ at points $x=n\in\mathbb N$ . If it is not clear what I mean, clicking on the previous link ""[following shape]"", you can see the curve above the $y$ -axis as $\Gamma_1$ given by $\gamma_1(n)$ , and the curve below $y$ -axis as $\Gamma_2$ given by $\gamma_2(n)$ . For simplicity, we can restrict ourselves to $k=2$ if needed. My idea The curves $\gamma_1(n)\approx-\gamma_2(n)$ generated by $a_k(n)$ remind me of a sine function whose period is being stretched by some function $g(n)$ , and whose amplitude is being increased by some function $h(n)\ne 0$ . Is it possible to find such $g,h$ such that the following ""sine form"": $$f_k(x):=h_k(x)\sin\left(\pi g_k(x)\right)\approx \gamma_1(x)=-\gamma_2(x)\space ?$$ To answer my question? This needs to be defined for all real $x\gt 0$ . Notice the above $f_k(x)$ form would have roots (be zero) at $x=g_k^{-1}(m),m\in\mathbb N$ . That is, lets observe ""near-zeros"" of the sequence $a_k(n)$ - The points where the curves pass over the $y$ -axis and and are closest to it. Here are first couple ""near-zeros"" for $k=2$ : $$a_2(n)\approx 0 \text{ at } n\approx 2,4.5,12.5,33.5,84.5,204.5,480.5,1102.5,2494.5,5568.5,\dots$$ Meaning that the $f_k$ (the $\gamma_1,\gamma_2$ ) should be zero (have a root) somewhere near these ""near-zeros"". In the context of my ""sine form"" $f$ mentioned above, and for $k=2$ , this means that we want to find $g_2$ such that for $m=1,2,3,\dots$ we have: $$ g_2^{-1}(m)=2,4.5,12.5,33.5,84.5,204.5,480.5,1102.5,2494.5,5568.5,\dots $$ For example, here is the plot of $a(n)$ for $n\le215$ showing a near-zero around $n\approx204.5$ , where we have $a(204)=-a(205)=-40\approx0$ . (Hence I've taken $n\approx \frac{204+205}{2} = 204.5$ ) But, the problem here is: How to determine at which $n$ will $a_k(n)$ reach a ""near-zero""? The other problem that needs to be solved, is to find $h_k(n)$ . For starters we need to know the growth of the absolute value of the sequence, $|a_k(n)|$ ? Then, the question remains: Can we interpolate (approximate) a ""closed form"" of $h_k(n)$ , to get the ""amplitude"" of $f_k$ ? Alternatively, is there a better $f_k$ form to search for, than my ""sine form"" $f_k$ ?","Problem & Question Given a constant , we define the sequence and for : Where gives integer part of . (The represents truncation.) For example, for we have the following scatterplot: [Click to see animated gif] . It appear as two interchanging curves repeat [following shape] in larger forms as we increase . Given , can we find a real equation that would approximate ""the curves"" given by ? That is, the plot given by can be defined as where are those ""the curves"". Specifically, are graphs of two real functions where as they are symmetric over -axis. The goal is to define them for . Currently, you can say that approximates at points . If it is not clear what I mean, clicking on the previous link ""[following shape]"", you can see the curve above the -axis as given by , and the curve below -axis as given by . For simplicity, we can restrict ourselves to if needed. My idea The curves generated by remind me of a sine function whose period is being stretched by some function , and whose amplitude is being increased by some function . Is it possible to find such such that the following ""sine form"": To answer my question? This needs to be defined for all real . Notice the above form would have roots (be zero) at . That is, lets observe ""near-zeros"" of the sequence - The points where the curves pass over the -axis and and are closest to it. Here are first couple ""near-zeros"" for : Meaning that the (the ) should be zero (have a root) somewhere near these ""near-zeros"". In the context of my ""sine form"" mentioned above, and for , this means that we want to find such that for we have: For example, here is the plot of for showing a near-zero around , where we have . (Hence I've taken ) But, the problem here is: How to determine at which will reach a ""near-zero""? The other problem that needs to be solved, is to find . For starters we need to know the growth of the absolute value of the sequence, ? Then, the question remains: Can we interpolate (approximate) a ""closed form"" of , to get the ""amplitude"" of ? Alternatively, is there a better form to search for, than my ""sine form"" ?","k\ge 2 a_k(0)=0,a_k(1)=1 n\ge2 
a_k(n)=\sum_{i=1}^{\left[\frac{n}{k}\right]} (-1)^{n-i+1} a_k(i)
 \left[\frac{n}{k}\right] \frac{n}{k} \left[\space\right] k=2 n k a_k(n) a_k(n) \Gamma_1\cup\Gamma_2 \Gamma_1,\Gamma_2 \Gamma_1,\Gamma_2 \gamma_1(x),\gamma_2(x) \gamma_1(x)=-\gamma_2(x) y x\in\mathbb R a_k(n) \gamma_1,\gamma_2 x=n\in\mathbb N y \Gamma_1 \gamma_1(n) y \Gamma_2 \gamma_2(n) k=2 \gamma_1(n)\approx-\gamma_2(n) a_k(n) g(n) h(n)\ne 0 g,h f_k(x):=h_k(x)\sin\left(\pi g_k(x)\right)\approx \gamma_1(x)=-\gamma_2(x)\space ? x\gt 0 f_k(x) x=g_k^{-1}(m),m\in\mathbb N a_k(n) y k=2 a_2(n)\approx 0 \text{ at } n\approx 2,4.5,12.5,33.5,84.5,204.5,480.5,1102.5,2494.5,5568.5,\dots f_k \gamma_1,\gamma_2 f k=2 g_2 m=1,2,3,\dots  g_2^{-1}(m)=2,4.5,12.5,33.5,84.5,204.5,480.5,1102.5,2494.5,5568.5,\dots  a(n) n\le215 n\approx204.5 a(204)=-a(205)=-40\approx0 n\approx \frac{204+205}{2} = 204.5 n a_k(n) h_k(n) |a_k(n)| h_k(n) f_k f_k f_k","['real-analysis', 'recurrence-relations', 'recreational-mathematics', 'approximation', 'interpolation']"
58,On the convergence of $\sum\frac{\log(n)}{n}\{x^n+x^{-n}\}$,On the convergence of,\sum\frac{\log(n)}{n}\{x^n+x^{-n}\},"As stated in the title, I'm trying to determine the values of $x\in \mathbb{R}$ for which $$\sum_{n=1}^\infty \frac{\log(n)}{n}\{x^n+x^{-n}\}<+\infty$$ where $\{x\}$ is the fractional part (any convention on the fractional part of negatives numbers is accepted). Now, $\sum_{n=1}^{\infty} \frac {\log(n)}{n}=+\infty$ , so the trivial bound $\{x\}<1$ is no use. I thought about using the Dirichlet test, but still the problem of determining whether $\sum \{x^n+x^{-n}\}$ is bounded is out of my reach. Obviously, the problem is easily solvable if one restricts to $x \in \mathbb{Z}-\{0\}$ , but I don't see an obvious extension to the general case. Similarly, if the series converges for $x$ it does for $\frac{1}{x}$ too. How should I approach the problem? I feel like there's an easy solution that I am missing.","As stated in the title, I'm trying to determine the values of for which where is the fractional part (any convention on the fractional part of negatives numbers is accepted). Now, , so the trivial bound is no use. I thought about using the Dirichlet test, but still the problem of determining whether is bounded is out of my reach. Obviously, the problem is easily solvable if one restricts to , but I don't see an obvious extension to the general case. Similarly, if the series converges for it does for too. How should I approach the problem? I feel like there's an easy solution that I am missing.",x\in \mathbb{R} \sum_{n=1}^\infty \frac{\log(n)}{n}\{x^n+x^{-n}\}<+\infty \{x\} \sum_{n=1}^{\infty} \frac {\log(n)}{n}=+\infty \{x\}<1 \sum \{x^n+x^{-n}\} x \in \mathbb{Z}-\{0\} x \frac{1}{x},"['real-analysis', 'calculus', 'sequences-and-series', 'convergence-divergence', 'fractional-part']"
59,Computing Lebesgue density,Computing Lebesgue density,,"I want to better understand how to compute the Lebesgue density of points in the plane $\mathbb{R^2}$ . Let me recall that the Lebesgue density of $z=(z_1,z_2)$ in some measurable set $E$ is defined as $$d(z;E)=\text{lim}_{r\to 0}\frac{B_r(z)\cap E}{B_r(z)}$$ Let $E=\big\{(x,y):|x|<R,|y|< R\big\}$ and $F=\big\{(x,y):x^2+y^2< R^2\big\}$ for some $R>0$ What is the Lebesgue density of some point $z\in \mathbb{R^2}$ in these two sets? If $z$ is in the interior of E, we can always find a $\delta$ such that $B_{\delta}(z)$ lies entirely inside $E$ , thus the function $\frac{B_r(z)\cap E}{B_r(z)}$ is constant and equal to $1$ for all $r< \delta$ and hence $d(z;E)=1$ in this case. The same exact reasoning can be applied to $F$ to get the same answer. If $z$ is not in the interior of $E$ and is not the closure of $E$ , then there exists a $\delta$ such that $B_{\delta}(z)$ lies entirely outside of $E$ , whence $\frac{B_r(z)\cap E}{B_r(z)}$ is constant and equal to $0$ for all $r< \delta$ and $d(z;E)=0$ ; same for $F$ It remains the case in which $z$ is in the boundary. For the boundary of E: Let $z$ be in the boundary of $E$ ; if $z=(R,y)$ or $z=(x,R)$ , then intuitively the line of the rectangle should cut any ball centered in $z$ in two halfs, one inside $E$ and one outside. Thus $d(z;E)=1/2$ (same thing for $-R$ ). In the case where $z=(R,R)$ (or the other combinations of $R$ , $-R$ ), then there should be exactly $1/4$ of any ball centered in $z$ lying inside of $E$ , hence $d(z;E)=1/4$ . Is it enough to say? How can I be more rigorous? For the boundary of $F$ : Intuitively it should be $1/2$ because in this case there aren't the points $(R,R)$ . But how to prove it rigorously ?","I want to better understand how to compute the Lebesgue density of points in the plane . Let me recall that the Lebesgue density of in some measurable set is defined as Let and for some What is the Lebesgue density of some point in these two sets? If is in the interior of E, we can always find a such that lies entirely inside , thus the function is constant and equal to for all and hence in this case. The same exact reasoning can be applied to to get the same answer. If is not in the interior of and is not the closure of , then there exists a such that lies entirely outside of , whence is constant and equal to for all and ; same for It remains the case in which is in the boundary. For the boundary of E: Let be in the boundary of ; if or , then intuitively the line of the rectangle should cut any ball centered in in two halfs, one inside and one outside. Thus (same thing for ). In the case where (or the other combinations of , ), then there should be exactly of any ball centered in lying inside of , hence . Is it enough to say? How can I be more rigorous? For the boundary of : Intuitively it should be because in this case there aren't the points . But how to prove it rigorously ?","\mathbb{R^2} z=(z_1,z_2) E d(z;E)=\text{lim}_{r\to 0}\frac{B_r(z)\cap E}{B_r(z)} E=\big\{(x,y):|x|<R,|y|< R\big\} F=\big\{(x,y):x^2+y^2< R^2\big\} R>0 z\in \mathbb{R^2} z \delta B_{\delta}(z) E \frac{B_r(z)\cap E}{B_r(z)} 1 r< \delta d(z;E)=1 F z E E \delta B_{\delta}(z) E \frac{B_r(z)\cap E}{B_r(z)} 0 r< \delta d(z;E)=0 F z z E z=(R,y) z=(x,R) z E d(z;E)=1/2 -R z=(R,R) R -R 1/4 z E d(z;E)=1/4 F 1/2 (R,R)","['real-analysis', 'general-topology']"
60,"If for metric space $(X,d)$ we have $d(A,B)>0$ for any pair of non empty closed disjoint subsets $A$ and $B$. Show that $(X,d)$ is complete.",If for metric space  we have  for any pair of non empty closed disjoint subsets  and . Show that  is complete.,"(X,d) d(A,B)>0 A B (X,d)","If for metric space $(X,d)$ we have $d(A,B)>0$ for any pair of non-empty  disjoint closed subsets $A$ and $B$ . Show that $(X,d)$ is complete. I am confused. If I let $A=N\subseteq R$ and $B=\{n+ \frac1n | n\in N, n\geq2\}$ then both $A$ and $B$ are disjoint and closed subsets of $(R, |\cdot|)$ , which is complete but $d(A,B)=0$ So does this disproves the above claim? EDIT: Looking carefully, the condition is not if and only if so this does not disproves the above claim. Please check my proof: Suppose $X$ is not complete $\rightarrow \exists (x_n)\in X  $ that is Cauchy but not convergent. If the set $F=\{x_n \mid n\in N\} $ is finite, then $(x_n)$ has a constant subsequence and thus $(x_n)$ converges to that constant. So $F$ has to be infinite. Hence, we can extract a subsequence from $(x_n)$ , say $(y_n)$ with all its terms distinct. Let $G=\{y_{2n}\mid n\in N\}$ and $H=\{y_{2n+1}\mid n\in N\}$ Then $G$ and $H$ are disjoint, closed subsets of $X$ but $d(G,H)=0$ as $(y_n)$ is also Cauchy. Is this proof okay?","If for metric space we have for any pair of non-empty  disjoint closed subsets and . Show that is complete. I am confused. If I let and then both and are disjoint and closed subsets of , which is complete but So does this disproves the above claim? EDIT: Looking carefully, the condition is not if and only if so this does not disproves the above claim. Please check my proof: Suppose is not complete that is Cauchy but not convergent. If the set is finite, then has a constant subsequence and thus converges to that constant. So has to be infinite. Hence, we can extract a subsequence from , say with all its terms distinct. Let and Then and are disjoint, closed subsets of but as is also Cauchy. Is this proof okay?","(X,d) d(A,B)>0 A B (X,d) A=N\subseteq R B=\{n+ \frac1n | n\in N, n\geq2\} A B (R, |\cdot|) d(A,B)=0 X \rightarrow \exists (x_n)\in X   F=\{x_n \mid n\in N\}  (x_n) (x_n) F (x_n) (y_n) G=\{y_{2n}\mid n\in N\} H=\{y_{2n+1}\mid n\in N\} G H X d(G,H)=0 (y_n)","['real-analysis', 'general-topology', 'metric-spaces']"
61,Prove that there exists $B$ such that $\mu(f(B))>0$.,Prove that there exists  such that .,B \mu(f(B))>0,"Let $\mu$ be the Lebesgue measure. Suppose $f:\mathbb{R}\rightarrow\mathbb{R}$ is measurable. $f(B)$ is measurable for every Borel set $B\subset \mathbb{R}$ , and $\{y:f^{-1}(y) \text { is infinite}\}$ has measure $0$ . Suppose there is some $A\subset\mathbb{R}$ such that $\mu(A)=0$ and $\mu (f(A))>0$ . Prove there is a closed set $F$ such that $\mu(F)=0$ and $\mu(f(F))>0$ . My try: $\mu(f(A))>0$ implies that there exists a subset $C$ of $A$ such that $f(C)$ is not measurable. I try to find a closed set $B$ such that $C\subset B$ and $\mu(B)=0$ . Then we can get $\mu(f(B))>0$ . However, I don’t know how to find such $B$ .","Let be the Lebesgue measure. Suppose is measurable. is measurable for every Borel set , and has measure . Suppose there is some such that and . Prove there is a closed set such that and . My try: implies that there exists a subset of such that is not measurable. I try to find a closed set such that and . Then we can get . However, I don’t know how to find such .",\mu f:\mathbb{R}\rightarrow\mathbb{R} f(B) B\subset \mathbb{R} \{y:f^{-1}(y) \text { is infinite}\} 0 A\subset\mathbb{R} \mu(A)=0 \mu (f(A))>0 F \mu(F)=0 \mu(f(F))>0 \mu(f(A))>0 C A f(C) B C\subset B \mu(B)=0 \mu(f(B))>0 B,"['real-analysis', 'measure-theory', 'lebesgue-measure']"
62,Prove that $\sum\limits_{i\leq n} a_n X_n$ converges almost surely iff $\sum\limits_{i\leq n}a_n^2$ converges.,Prove that  converges almost surely iff  converges.,\sum\limits_{i\leq n} a_n X_n \sum\limits_{i\leq n}a_n^2,Assume $(X_n)$ is a sequence of independent random variables with $\mathbb{P}(X_n = 1) = \mathbb{P}(X_n = -1) = \frac{1}{2}$ and $(a_n)$ a sequence of real numbers. Prove that $\sum\limits_{i\leq n} a_i X_i$ converges almost surely if and only if $\sum\limits_{i\leq n}a_i^2$ converges. I would really appreciate a hint for the direction where I assume that $\sum\limits_{i\leq n} a_n X_n$ converges almost surely. The other direction was easier for me; I proved it with Kolmogorov's three series theorem. I tried to prove it indirectly by looking at the third series of Kolmogorov's three series theorem: $$\sum\limits_{i\leq n} Var(a_i X_i [\vert a_i X_i \vert \leq \varepsilon]) = ... = \sum\limits_{i\leq n} a_i 1_{[\vert a_i \vert \leq \varepsilon]}.$$,Assume is a sequence of independent random variables with and a sequence of real numbers. Prove that converges almost surely if and only if converges. I would really appreciate a hint for the direction where I assume that converges almost surely. The other direction was easier for me; I proved it with Kolmogorov's three series theorem. I tried to prove it indirectly by looking at the third series of Kolmogorov's three series theorem:,(X_n) \mathbb{P}(X_n = 1) = \mathbb{P}(X_n = -1) = \frac{1}{2} (a_n) \sum\limits_{i\leq n} a_i X_i \sum\limits_{i\leq n}a_i^2 \sum\limits_{i\leq n} a_n X_n \sum\limits_{i\leq n} Var(a_i X_i [\vert a_i X_i \vert \leq \varepsilon]) = ... = \sum\limits_{i\leq n} a_i 1_{[\vert a_i \vert \leq \varepsilon]}.,"['real-analysis', 'analysis', 'probability-theory', 'measure-theory']"
63,Convergence uniform of sequence of measurable non-negative functions in finite measure space.,Convergence uniform of sequence of measurable non-negative functions in finite measure space.,,"It's the first time I'm asking here. I'm having problem with an exercise on integration theory. It's: Let $(X,\mathbb{X},\mu)$ a finite measure space. Denote $M^+(X,\mathbb{X}) = \{f \in [0,\infty]^X\:\:; f\:\: is\:\: measurable\}$ . If $(f_n)$ is a sequence of measurable functions em $M^+(X,\mathbb{X})$ which converges uniformly to a function $f$ , so $f \in M^+(X,\mathbb{X})$ and \begin{equation} \int_Xfd\mu = \lim_{n \to \infty} \int_Xf_nd\mu \end{equation} . Well, the natural manner of ""solve"" this is like it was done here Question 4.K of Bartle's Element of Integration , but i think it's incorrect, because this inequality just can be used if we can ensure that the function $|f_n-f|$ is integrable. I think we have to ask theses functions be integrable, or, since $\mu(X)<\infty$ and the convergence is uniform, the function $f$ is bounded. Additional edition: The function $f_n-f$ is, in fact, integrable for n enoughly large, but how can we ensure that \begin{equation} \int_Xf_nd\mu - \int_Xfd\mu = \int_X(f_n-f)d\mu \end{equation} A priori, we just can ensure in two situations: Sum of non-negative measurable functions or sum/difference of integrable functions. What do you think?","It's the first time I'm asking here. I'm having problem with an exercise on integration theory. It's: Let a finite measure space. Denote . If is a sequence of measurable functions em which converges uniformly to a function , so and . Well, the natural manner of ""solve"" this is like it was done here Question 4.K of Bartle's Element of Integration , but i think it's incorrect, because this inequality just can be used if we can ensure that the function is integrable. I think we have to ask theses functions be integrable, or, since and the convergence is uniform, the function is bounded. Additional edition: The function is, in fact, integrable for n enoughly large, but how can we ensure that A priori, we just can ensure in two situations: Sum of non-negative measurable functions or sum/difference of integrable functions. What do you think?","(X,\mathbb{X},\mu) M^+(X,\mathbb{X}) = \{f \in [0,\infty]^X\:\:; f\:\: is\:\: measurable\} (f_n) M^+(X,\mathbb{X}) f f \in M^+(X,\mathbb{X}) \begin{equation}
\int_Xfd\mu = \lim_{n \to \infty} \int_Xf_nd\mu
\end{equation} |f_n-f| \mu(X)<\infty f f_n-f \begin{equation}
\int_Xf_nd\mu - \int_Xfd\mu = \int_X(f_n-f)d\mu
\end{equation}","['real-analysis', 'measure-theory', 'lebesgue-integral', 'uniform-convergence']"
64,Real numbers whose digits are the even digits of their squares,Real numbers whose digits are the even digits of their squares,,"The Context The origin of my question is my own answer to this question , where the continuity of the function $f: [0,1) \to [0,1)$ that only preserves the odd digits of its input value is analyzed. In this context, I will slightly modify the definition given there, as follows. The function I am considering now is $f: [0,+\infty) \to [0,+\infty)$ that preserves only the even digits of the input. So if $x$ has decimal expansion $$x = \sum_{k=-\infty}^{+\infty}a_k 10^k,$$ then $$f(x)=\sum_{k=-\infty}^{+\infty}a_{2k}10^{k}.$$ So for example $$f(5\textbf{8}7\textbf{4}1\textbf{2}.7\textbf{8}0\textbf{3}4\textbf{0}5\textbf{1})=842.8301.$$ With a very similar approach to the one given here , it can be shown that $f$ is continuous almost everywhere (the only exception being the numbers whose least significant digit occupies an odd position), and right-continuous everywhere. Edit. I am assuming, as in the original question, to adopt, in case of ambiguity, the finite version of the number's decimal expansion . Note the self-similarity $$f\left(10^{2k} x\right)=10^k f(x), \ \ \forall k\in \Bbb Z.$$ Below an approximate plot of the function $f$ in the range $[0,1]\times[0,1]$ . Red dots represent points actually belonging to the graph of $f$ . The yellow line represents the graph of $$s(x) = \sqrt x.$$ By self-similarity, a scaling of $10^{2k}$ of the $x$ -axis and of $10^{-k}$ of the $y$ -axis, for any $k\in \Bbb Z$ , would give an exact replica of the given plot. Introductory Observation Aside from the trivial intersections between $f$ and $s$ , that is all the points with coordinates $$\left(10^{2k},10^k\right),$$ there are many other interesting intersections, such as (limiting ourselves to the range shown in the picture) $$(0.25,0.5),$$ $$(0.36,0.6),$$ $$(0.0121,0.11),$$ and 'trikiest' ones, such as $$(0.5776,0.76),$$ or even $$(0.35295481,0.5941).$$ The Question Is there any non-terminating decimal (or even irrational) $x$ such that $$y=f(x) = s(x),$$ that is, is there any non-terminating decimal $y$ whose square contains - in the even positioned digits - the digits of the original number $y$ ? Edit. I emphasize again that no infinite sequences of $9$ 's are allowed, since we are adopting the finite decimal expansion version of the number, if this ambuiguity arises.","The Context The origin of my question is my own answer to this question , where the continuity of the function that only preserves the odd digits of its input value is analyzed. In this context, I will slightly modify the definition given there, as follows. The function I am considering now is that preserves only the even digits of the input. So if has decimal expansion then So for example With a very similar approach to the one given here , it can be shown that is continuous almost everywhere (the only exception being the numbers whose least significant digit occupies an odd position), and right-continuous everywhere. Edit. I am assuming, as in the original question, to adopt, in case of ambiguity, the finite version of the number's decimal expansion . Note the self-similarity Below an approximate plot of the function in the range . Red dots represent points actually belonging to the graph of . The yellow line represents the graph of By self-similarity, a scaling of of the -axis and of of the -axis, for any , would give an exact replica of the given plot. Introductory Observation Aside from the trivial intersections between and , that is all the points with coordinates there are many other interesting intersections, such as (limiting ourselves to the range shown in the picture) and 'trikiest' ones, such as or even The Question Is there any non-terminating decimal (or even irrational) such that that is, is there any non-terminating decimal whose square contains - in the even positioned digits - the digits of the original number ? Edit. I emphasize again that no infinite sequences of 's are allowed, since we are adopting the finite decimal expansion version of the number, if this ambuiguity arises.","f: [0,1) \to [0,1) f: [0,+\infty) \to [0,+\infty) x x = \sum_{k=-\infty}^{+\infty}a_k 10^k, f(x)=\sum_{k=-\infty}^{+\infty}a_{2k}10^{k}. f(5\textbf{8}7\textbf{4}1\textbf{2}.7\textbf{8}0\textbf{3}4\textbf{0}5\textbf{1})=842.8301. f f\left(10^{2k} x\right)=10^k f(x), \ \ \forall k\in \Bbb Z. f [0,1]\times[0,1] f s(x) = \sqrt x. 10^{2k} x 10^{-k} y k\in \Bbb Z f s \left(10^{2k},10^k\right), (0.25,0.5), (0.36,0.6), (0.0121,0.11), (0.5776,0.76), (0.35295481,0.5941). x y=f(x) = s(x), y y 9","['real-analysis', 'real-numbers', 'decimal-expansion']"
65,The statement of integration by substitution,The statement of integration by substitution,,"For this method of solving integrals, it is given the following statement: Let $~~g : [a,b] \to \mathbb{R},~~~~ f: g([a,b])\to\Bbb R~~$ be two functions such that $1.~$ $f$ is continuos $2.~$ $g$ is differentiable with integrable derivative Then we have that $$\int_a^b f(g(x)) \cdot g'(x) \, \mathrm dx = \int_{g(a)}^{g(b)} f(t) \, \mathrm dt $$ It is necessary that $f$ is continuous? For example, I met the following examples: http://www.stumblingrobot.com/2015/08/06/prove-an-integral-formula-for-periodic-functions/ (Lemma 2.1/3) https://pdfs.semanticscholar.org/e602/c172526c258b154cbd3ef7a2caeb5a4b8336.pdf?fbclid=IwAR2Nb65mPuTlLuer9v5ITPzEPfPxVQXlMXCW-oVovDepyYL0fS1zvYWA-AE","For this method of solving integrals, it is given the following statement: Let be two functions such that is continuos is differentiable with integrable derivative Then we have that It is necessary that is continuous? For example, I met the following examples: http://www.stumblingrobot.com/2015/08/06/prove-an-integral-formula-for-periodic-functions/ (Lemma 2.1/3) https://pdfs.semanticscholar.org/e602/c172526c258b154cbd3ef7a2caeb5a4b8336.pdf?fbclid=IwAR2Nb65mPuTlLuer9v5ITPzEPfPxVQXlMXCW-oVovDepyYL0fS1zvYWA-AE","~~g : [a,b] \to \mathbb{R},~~~~ f: g([a,b])\to\Bbb R~~ 1.~ f 2.~ g \int_a^b f(g(x)) \cdot g'(x) \, \mathrm dx = \int_{g(a)}^{g(b)} f(t) \, \mathrm dt  f","['real-analysis', 'integration']"
66,Convergence of $ \begin{equation} \sum\limits_{n = 1}^\infty{\frac{{{{\left( {2 +\sin n} \right)}^n}}}{{{3^n} \cdot n}}} \end{equation}$,Convergence of, \begin{equation} \sum\limits_{n = 1}^\infty{\frac{{{{\left( {2 +\sin n} \right)}^n}}}{{{3^n} \cdot n}}} \end{equation},"Determine whether the following series is convergent or not, with explanation. $$ \begin{equation} \sum\limits_{n = 1}^\infty  {\frac{{{{\left( {2 + \sin n} \right)}^n}}}{{{3^n} \cdot n}}}  \end{equation} $$ I guess the above series is disvergent, but I cannot prove it. I have the following  assumptions: The function $\sin n$ has least upper bound one for counting number $n$ . In each 'cycle', there will be a positive number $n_k$ which lets $\sin n_k \to 1$ . Then I just consider all positive numbers $n_k \left(k=1,2,\cdots\right)$ $$ \begin{equation} \sum\limits_{{n_k}} {\frac{{{{\left( {2 + \sin n} \right)}^n}}}{{{3^n} \cdot n}}} \to \sum\limits_{{n_k}} {\frac{1}{n}}, \end{equation} $$ it looks like a harmonic series. PS. I used MATLAB to find the maximum of $\sin n$ within a certain range. n=1:1e4;[m,index]=max(sin(n)); Then, I got $\rm{m}=1.0000$ and $\rm{index}=9929$ .","Determine whether the following series is convergent or not, with explanation. I guess the above series is disvergent, but I cannot prove it. I have the following  assumptions: The function has least upper bound one for counting number . In each 'cycle', there will be a positive number which lets . Then I just consider all positive numbers it looks like a harmonic series. PS. I used MATLAB to find the maximum of within a certain range. n=1:1e4;[m,index]=max(sin(n)); Then, I got and .","
\begin{equation}
\sum\limits_{n = 1}^\infty  {\frac{{{{\left( {2 + \sin n} \right)}^n}}}{{{3^n} \cdot n}}} 
\end{equation}
 \sin n n n_k \sin n_k \to 1 n_k \left(k=1,2,\cdots\right) 
\begin{equation}
\sum\limits_{{n_k}} {\frac{{{{\left( {2 + \sin n} \right)}^n}}}{{{3^n} \cdot n}}} \to \sum\limits_{{n_k}} {\frac{1}{n}},
\end{equation}
 \sin n \rm{m}=1.0000 \rm{index}=9929","['real-analysis', 'sequences-and-series']"
67,Probability of unique winner in a coin flipping game (limit of a recursive sequence),Probability of unique winner in a coin flipping game (limit of a recursive sequence),,"Suppose we have a coin flipping game involving $n$ players.  In each round everyone still playing flips a fair coin, and the players whose coin comes up tails are eliminated.  The game continues until at most one player is still alive, and they are declared the winner. Now, it is possible that the game does not end with a winner (e.g. if $n=2$ and both players get tails on their first flip).  Let $f(n)$ denote the probability that a game with $n$ players has a winner.  We have $f(0)=0$ and $f(1)=1$ .  For $n>1$ it follows from considering the binomial distribution that $$f(n) = \sum_{k=0}^{n}\frac{\binom{n}{k}}{2^{n}}  f(k) $$ (Here $\binom{n}{k}/(2^n)$ represents the probability $k$ players survive the current round), which can be rearranged as $$f(n) = \sum_{k=0}^{n-1} \frac{\binom{n}{k}}{2^n-1} f(k)$$ Using this formula we can compute $f(n)$ recursively. $$\begin{array}{cc} n & f(n) \\ 0 & 0 \\ 1 & 1 \\ 2 & 2/3 \\ 3 & 5/7 \\ 4 & 76/105 \\ 5 & 157/217 \\ \vdots & \vdots \\  20 & 0.7213 \end{array}$$ The sequence of numerators doesn't seem to be in OEIS, nor does the sequence $a_n=f(n)(2^n-1)(2^{n-1}-1)\dots(3)(1)$ from clearing all the denominators in the recursion. Is there a way of analytically determine the limit (if it exists) of $f(n)$ as $n$ goes to infinity?  It seems from calculation to be about $0.7213$ , though I'm not confident in digits beyond that due to error propagation as the recursion continues.","Suppose we have a coin flipping game involving players.  In each round everyone still playing flips a fair coin, and the players whose coin comes up tails are eliminated.  The game continues until at most one player is still alive, and they are declared the winner. Now, it is possible that the game does not end with a winner (e.g. if and both players get tails on their first flip).  Let denote the probability that a game with players has a winner.  We have and .  For it follows from considering the binomial distribution that (Here represents the probability players survive the current round), which can be rearranged as Using this formula we can compute recursively. The sequence of numerators doesn't seem to be in OEIS, nor does the sequence from clearing all the denominators in the recursion. Is there a way of analytically determine the limit (if it exists) of as goes to infinity?  It seems from calculation to be about , though I'm not confident in digits beyond that due to error propagation as the recursion continues.","n n=2 f(n) n f(0)=0 f(1)=1 n>1 f(n) = \sum_{k=0}^{n}\frac{\binom{n}{k}}{2^{n}}  f(k)  \binom{n}{k}/(2^n) k f(n) = \sum_{k=0}^{n-1} \frac{\binom{n}{k}}{2^n-1} f(k) f(n) \begin{array}{cc} n & f(n) \\ 0 & 0 \\ 1 & 1 \\ 2 & 2/3 \\ 3 & 5/7 \\ 4 & 76/105 \\ 5 & 157/217 \\
\vdots & \vdots \\ 
20 & 0.7213 \end{array} a_n=f(n)(2^n-1)(2^{n-1}-1)\dots(3)(1) f(n) n 0.7213","['real-analysis', 'probability', 'recurrence-relations']"
68,Problem in convex analysis : Easy or hard one?,Problem in convex analysis : Easy or hard one?,,"I hope your day is going well. This is a problem, I don't know how to solve it since 1 week. It's going to be a relief and a pleasure to get your help. Problem : Let $X = \{x_{1},\ldots,x_{N+M} \}$ such as $\{x_{1},\ldots,x_{N} \} \subset  \Omega := \text{int}( \text{conv} ( \{x_{N+1},\ldots,x_{N+M} \} )  )$ a convex polygon with $ \text{conv} $ the convex hull. Let $u \in R^{X}$ such as $u(x_{N+i}) = 0$ ( $u=0$ on the edge) and $\tilde{u}^{**}(x_{i}) = u(x_{i})$ ( $u$ is its convex conjugate on $X$ ). Here's some explications and definitions : Let $\tilde{u}$ which is $u(x_{i})$ when $x_{i} \in X$ and $+\infty$ otherwise.  Its convex conjugate is $\tilde{u}^{*}(x) = \sup_{y \in \mathbb{R}^{n}} \{ \langle x,y \rangle - \tilde{u}(x)\} = \max_{x \in X} \{ \langle x,y \rangle - \tilde{u}(x) \}$ . Then I take the convex conjugate once again, I called it $v$ . Question : Show $v=0$ on the edge $\partial{\Omega}$ I think we can use that $v$ is the supremum of affine function $\varphi$ such as $\varphi \le \tilde{u}$ . Which can be written : If $\Sigma = \{(p,r) ; \forall y \in \mathbb{R}^{n}, \langle p,y \rangle + r \le \tilde{u}(y) \}$ we have : \begin{align*}     \sup\{ \langle p,y \rangle + r ; (p,r) \in \Sigma \}   =  v(y) \end{align*} I wish you a very good day.","I hope your day is going well. This is a problem, I don't know how to solve it since 1 week. It's going to be a relief and a pleasure to get your help. Problem : Let such as a convex polygon with the convex hull. Let such as ( on the edge) and ( is its convex conjugate on ). Here's some explications and definitions : Let which is when and otherwise.  Its convex conjugate is . Then I take the convex conjugate once again, I called it . Question : Show on the edge I think we can use that is the supremum of affine function such as . Which can be written : If we have : I wish you a very good day.","X = \{x_{1},\ldots,x_{N+M} \} \{x_{1},\ldots,x_{N} \} \subset  \Omega := \text{int}( \text{conv} ( \{x_{N+1},\ldots,x_{N+M} \} )  )  \text{conv}  u \in R^{X} u(x_{N+i}) = 0 u=0 \tilde{u}^{**}(x_{i}) = u(x_{i}) u X \tilde{u} u(x_{i}) x_{i} \in X +\infty \tilde{u}^{*}(x) = \sup_{y \in \mathbb{R}^{n}} \{ \langle x,y \rangle - \tilde{u}(x)\} = \max_{x \in X} \{ \langle x,y \rangle - \tilde{u}(x) \} v v=0 \partial{\Omega} v \varphi \varphi \le \tilde{u} \Sigma = \{(p,r) ; \forall y \in \mathbb{R}^{n}, \langle p,y \rangle + r \le \tilde{u}(y) \} \begin{align*}
    \sup\{ \langle p,y \rangle + r ; (p,r) \in \Sigma \}   =  v(y)
\end{align*}","['real-analysis', 'convex-analysis']"
69,"From $\|g\|_2 =1$ to $\|g\|_\infty^2 \ge \dim V$ on a subspace of $C[0,1]$",From  to  on a subspace of,"\|g\|_2 =1 \|g\|_\infty^2 \ge \dim V C[0,1]","Let $V$ be a finite dimensional subspace of $C[0,1]$ . Prove that there exists $g \in V$ such that $\|g\|_2 =1$ , $\|g\|_\infty^2 \ge \dim V$ In this problem we use the notation $$\|f\|_2 = \left(\int_0^1 f^2\right)^{1/2}$$ $$\|f\|_\infty = \max_{[0,1]} f$$ My attempt For the n-dimensional situation. Suppose that $f_i=\chi_{[\frac{i-1}{n}, \frac{i}{n}]}$ are the characteristic functions of intervals. Then the condition is equivalent to that for $f=\sum_{i=1}^n \lambda_i f_i$ $$\sum_{i=1}^n \lambda_i^2 = n^2$$ Thus $$\|f\|_\infty^2 = \max \lambda_i^2 \geqslant n$$ The inequality holds. But I got stuck on analyzing the general situation. Could you please give me some hints? Thanks in advance!","Let be a finite dimensional subspace of . Prove that there exists such that , In this problem we use the notation My attempt For the n-dimensional situation. Suppose that are the characteristic functions of intervals. Then the condition is equivalent to that for Thus The inequality holds. But I got stuck on analyzing the general situation. Could you please give me some hints? Thanks in advance!","V C[0,1] g \in V \|g\|_2 =1 \|g\|_\infty^2 \ge \dim V \|f\|_2 = \left(\int_0^1 f^2\right)^{1/2} \|f\|_\infty = \max_{[0,1]} f f_i=\chi_{[\frac{i-1}{n}, \frac{i}{n}]} f=\sum_{i=1}^n \lambda_i f_i \sum_{i=1}^n \lambda_i^2 = n^2 \|f\|_\infty^2 = \max \lambda_i^2 \geqslant n","['real-analysis', 'functional-analysis', 'banach-spaces']"
70,"If $f_n \rightarrow 0$ with $f_n ' \rightarrow g$, then is $g=0$ in some sense?","If  with , then is  in some sense?",f_n \rightarrow 0 f_n ' \rightarrow g g=0,"Suppose $f_n :[a,b] \rightarrow \mathbb{R}$ are differentiable functions (need not be $C^1$ ) with $f_n \rightarrow 0$ , $f_n ' \rightarrow g$ pointwise.  Can we say that $g=0$ in some sense? (Say, a.e.) In particular, is it possible for $g$ to equal $1$ everywhere? cf) Interchanging pointwise limit and derivative of a sequence of C1 functions This question deals with the $C^1$ case.","Suppose are differentiable functions (need not be ) with , pointwise.  Can we say that in some sense? (Say, a.e.) In particular, is it possible for to equal everywhere? cf) Interchanging pointwise limit and derivative of a sequence of C1 functions This question deals with the case.","f_n :[a,b] \rightarrow \mathbb{R} C^1 f_n \rightarrow 0 f_n ' \rightarrow g g=0 g 1 C^1","['real-analysis', 'calculus']"
71,"Is this elementary, nilpotent-free approach to automatic differentiation strong enough for real analysis? How similar is it to Newton's system?","Is this elementary, nilpotent-free approach to automatic differentiation strong enough for real analysis? How similar is it to Newton's system?",,"This is a sequel to this question: Is the theory of dual numbers strong enough to develop real analysis, and does it resemble Newton's historical method for doing calculus? The ring of ""dual numbers"" is of the form $a+b\,dx$ , where $dx$ is an ""infinitesimal"" quantity that squares to 0; they can be constructed pretty simply as the quotient of the polynomial ring $\Bbb R[dx]/dx^2$ . In general, the useful thing about the dual numbers is in automatic differentiation: for any analytic function $f$ , we have $f(x+dx) = f(x) + f'(x) dx$ These are interesting because they form a minimalist structure that is sufficient for a good portion of calculus, and have even been used to teach high school calculus . They also share some superficial similarity with Newton's approach in neglecting the square of an infinitesimal. The main issue people mentioned about them in my original thread is that they don't form a field. This can cause some deep issues, as also pointed out by Terry Tao in the above blog post. As a result, things like nonstandard analysis or smooth infinitesimal analysis can be better behaved in some circumstances. The strange thing is, however, these issues also seem to vanish if we just totally drop the notion that $dx^2 = 0$ , and just let it be another real variable with an odd name. Or, if we want, we can think of this as something like the ring of formal power series $\Bbb R[[dx]]$ , or the field of formal Laurent series $\Bbb R((dx))$ if a field is desired. Later edit: or something like the Levi-Civita field would probably be best. Either way, for any analytic function, we get an expansion of derivatives of all orders: $\displaystyle f(x+dx) = f(x) + f'(x) dx + \frac{f''(x)}{2!} dx^2 + \frac{f'''(x)}{3!} dx^3 + ...$ which can be calculated symbolically to any desired degree, and of which the ""dual number"" version is a truncation. Both expansions are easy to see by starting with polynomials and extending to analytic functions from there. This can also be thought of as analogous to Newton's approach, where we are simply using the ""coefficient extraction"" operator to get the coefficient of the $dx$ term, ignoring the higher-order terms. It is fairly easy to define other approaches to differentiation in terms of this, since we have: $\displaystyle f(x+dx) - f(x) = f'(x) dx + \frac{f''(x)}{2!} dx^2 + \frac{f'''(x)}{3!} dx^3 + ...$ All of these different approaches are just different ways to pluck off that $dx$ term: Quotienting the formal power series ring by $dx^2$ , canceling out the rest of the $dx^n$ terms (the dual number approach) Treating $dx$ as a real variable, rather than a formal quantity, dividing by it, and taking the limit as $dx \to 0$ , canceling out the rest of the $dx^n$ terms (Cauchy's approach) Treating $dx$ as an element in the formal Laurent series, dividing by it, and using the ""coefficient extraction"" operator on the formal power series to get the constant term (i.e. the non-standard analysis ""standard part"" approach, basically) Just noting formally that the $dx$ term is the one you want, and ignoring $dx^2$ and larger (Newton's approach?) The strange thing is, though, although we have presented this in the context of automatic differentiation, this is literally the most elementary theorem possible - we've just rediscovered Taylor's theorem with a change of variables, evaluating the function $f(x+dx)$ symbolically at some $x$ while leaving $dx$ indeterminate. We could even treat $dx$ as a simple real variable rather than a new formal quantity, with the only caveat being that the above may only hold for ""sufficiently small"" $dx$ , due to convergence issues. So in some sense we have gone in a huge circle, but gotten a better theory than that of the dual numbers, but which has no nilpotents or infinitesimals at all - just indeterminate real variables. My questions: This seems to be so basic that it's almost surprising that it works so well. Is there some drawback I'm not seeing? Thinking of this as the field of formal Laurent series $\Bbb R((dx))$ (or the Levi-Civita field) is algebraically interesting, but is there any technical difference between doing this and just treating $dx$ as a real indeterminate and getting this result as Taylor's theorem? Is this in any way similar to how Newton used ""fluxions,"" historically? Or similar to Leibniz's infinitesimal approach? Is there some benefit to going with non-standard analysis, or smooth infinitesimal analysis, etc, rather than this elementary approach? EDIT : In the original question I asked about Laurent series, although I am noting that perhaps the Levi-Civita field would have been better to ask about.","This is a sequel to this question: Is the theory of dual numbers strong enough to develop real analysis, and does it resemble Newton's historical method for doing calculus? The ring of ""dual numbers"" is of the form , where is an ""infinitesimal"" quantity that squares to 0; they can be constructed pretty simply as the quotient of the polynomial ring . In general, the useful thing about the dual numbers is in automatic differentiation: for any analytic function , we have These are interesting because they form a minimalist structure that is sufficient for a good portion of calculus, and have even been used to teach high school calculus . They also share some superficial similarity with Newton's approach in neglecting the square of an infinitesimal. The main issue people mentioned about them in my original thread is that they don't form a field. This can cause some deep issues, as also pointed out by Terry Tao in the above blog post. As a result, things like nonstandard analysis or smooth infinitesimal analysis can be better behaved in some circumstances. The strange thing is, however, these issues also seem to vanish if we just totally drop the notion that , and just let it be another real variable with an odd name. Or, if we want, we can think of this as something like the ring of formal power series , or the field of formal Laurent series if a field is desired. Later edit: or something like the Levi-Civita field would probably be best. Either way, for any analytic function, we get an expansion of derivatives of all orders: which can be calculated symbolically to any desired degree, and of which the ""dual number"" version is a truncation. Both expansions are easy to see by starting with polynomials and extending to analytic functions from there. This can also be thought of as analogous to Newton's approach, where we are simply using the ""coefficient extraction"" operator to get the coefficient of the term, ignoring the higher-order terms. It is fairly easy to define other approaches to differentiation in terms of this, since we have: All of these different approaches are just different ways to pluck off that term: Quotienting the formal power series ring by , canceling out the rest of the terms (the dual number approach) Treating as a real variable, rather than a formal quantity, dividing by it, and taking the limit as , canceling out the rest of the terms (Cauchy's approach) Treating as an element in the formal Laurent series, dividing by it, and using the ""coefficient extraction"" operator on the formal power series to get the constant term (i.e. the non-standard analysis ""standard part"" approach, basically) Just noting formally that the term is the one you want, and ignoring and larger (Newton's approach?) The strange thing is, though, although we have presented this in the context of automatic differentiation, this is literally the most elementary theorem possible - we've just rediscovered Taylor's theorem with a change of variables, evaluating the function symbolically at some while leaving indeterminate. We could even treat as a simple real variable rather than a new formal quantity, with the only caveat being that the above may only hold for ""sufficiently small"" , due to convergence issues. So in some sense we have gone in a huge circle, but gotten a better theory than that of the dual numbers, but which has no nilpotents or infinitesimals at all - just indeterminate real variables. My questions: This seems to be so basic that it's almost surprising that it works so well. Is there some drawback I'm not seeing? Thinking of this as the field of formal Laurent series (or the Levi-Civita field) is algebraically interesting, but is there any technical difference between doing this and just treating as a real indeterminate and getting this result as Taylor's theorem? Is this in any way similar to how Newton used ""fluxions,"" historically? Or similar to Leibniz's infinitesimal approach? Is there some benefit to going with non-standard analysis, or smooth infinitesimal analysis, etc, rather than this elementary approach? EDIT : In the original question I asked about Laurent series, although I am noting that perhaps the Levi-Civita field would have been better to ask about.","a+b\,dx dx \Bbb R[dx]/dx^2 f f(x+dx) = f(x) + f'(x) dx dx^2 = 0 \Bbb R[[dx]] \Bbb R((dx)) \displaystyle f(x+dx) = f(x) + f'(x) dx + \frac{f''(x)}{2!} dx^2 + \frac{f'''(x)}{3!} dx^3 + ... dx \displaystyle f(x+dx) - f(x) = f'(x) dx + \frac{f''(x)}{2!} dx^2 + \frac{f'''(x)}{3!} dx^3 + ... dx dx^2 dx^n dx dx \to 0 dx^n dx dx dx^2 f(x+dx) x dx dx dx \Bbb R((dx)) dx","['real-analysis', 'calculus', 'abstract-algebra', 'math-history', 'nonstandard-analysis']"
72,What is the space $L^p(\mathbb R)/_\sim$ where $f\sim g$ $\iff$ $f$ and $g$ has the same distribution?,What is the space  where    and  has the same distribution?,L^p(\mathbb R)/_\sim f\sim g \iff f g,"Let $L^1(\mathbb R)$ the set of function that are Lebesgue integrable. Define for $f$ and $g$ the relation $$f\sim g\iff m\{f\leq x\}=m\{g\leq x\},$$ where $m$ is the Lebesgue measure. It's an equivalence relation. How this equivalence relation is interesting ? In the probability point of view, it looks to be the space of random variable having the same law. Is this space important ? Commonly used ? Does someone knows reference for such a space ? For example, a problem I see is the fact that $X:\Omega \to \mathbb R$ and $Y: \Omega '\to \mathbb R$ can be random variable on $(\Omega ,\mathcal F,\mathbb P)$ and $(\Omega ',\mathcal F',\mathbb P')$ (different probability space), but saying that $X$ and $Y$ are equivalent looks strange. So maybe, even if they are on the same probability space, at the end, $X\sim Y$ is not really relevant and doesn't give us interesting information. What do you think ?","Let the set of function that are Lebesgue integrable. Define for and the relation where is the Lebesgue measure. It's an equivalence relation. How this equivalence relation is interesting ? In the probability point of view, it looks to be the space of random variable having the same law. Is this space important ? Commonly used ? Does someone knows reference for such a space ? For example, a problem I see is the fact that and can be random variable on and (different probability space), but saying that and are equivalent looks strange. So maybe, even if they are on the same probability space, at the end, is not really relevant and doesn't give us interesting information. What do you think ?","L^1(\mathbb R) f g f\sim g\iff m\{f\leq x\}=m\{g\leq x\}, m X:\Omega \to \mathbb R Y: \Omega '\to \mathbb R (\Omega ,\mathcal F,\mathbb P) (\Omega ',\mathcal F',\mathbb P') X Y X\sim Y","['real-analysis', 'probability', 'measure-theory']"
73,"If $X$ is a complex Banach space, is the set $T \in L(X)$ with finite dimensional kernel dense?","If  is a complex Banach space, is the set  with finite dimensional kernel dense?",X T \in L(X),"If $X$ is a complex Banach space, is the set $T \in L(X)$ with finite dimensional kernel  dense? Here $L(X)$ is the set of bounded linear operators from $X$ to itself equipped with the norm topology. Edit: I am only interested in the separable case.","If is a complex Banach space, is the set with finite dimensional kernel  dense? Here is the set of bounded linear operators from to itself equipped with the norm topology. Edit: I am only interested in the separable case.",X T \in L(X) L(X) X,"['real-analysis', 'functional-analysis']"
74,Generalization of the fundamental theorem of duality,Generalization of the fundamental theorem of duality,,"The ""fundamental theorem of duality"" states: If $X$ is a real linear space and $f, f_1,...,f_n$ are linear functionals on $X$ , then $f$ lies in the span of $f_1,...,f_n$ (i.e. $f = \sum_{i=1}^n \lambda_i f_i$ , where $\lambda_i \in \mathbb R$ ) if and only if $\bigcap_{i=1}^n ker \ f_i \subseteq ker \ f$ (where $ker \ g = \{x \in X: g(x)=0\}$ ). My question is: Can someone please direct me to a reference that generalizes this result to arbitrary collections of linear functionals? In other words, if $I$ is an arbitrary index set, I am wondering how to characterize $$\bigcap_{i \in I} ker \ f_i \subseteq ker \ f.$$ Intuitively, I suspect that the equivalent statement is something like: there exists a finitely additive signed measure $\mu$ on $2^I$ such that for all $x \in X$ $$ f (x) = \int f_i (x) \ \mu(di).$$ But this will be tricky as it involves finitely additive integration of potentially unbounded functions. (Clearly, though, this condition is sufficient for the condition about kernels to hold. The difficulty is in showing that it is also necessary.) Anyway, surely generalizations of the fundamental theorem of duality have been explored, and any references or pointers would be appreciated.","The ""fundamental theorem of duality"" states: If is a real linear space and are linear functionals on , then lies in the span of (i.e. , where ) if and only if (where ). My question is: Can someone please direct me to a reference that generalizes this result to arbitrary collections of linear functionals? In other words, if is an arbitrary index set, I am wondering how to characterize Intuitively, I suspect that the equivalent statement is something like: there exists a finitely additive signed measure on such that for all But this will be tricky as it involves finitely additive integration of potentially unbounded functions. (Clearly, though, this condition is sufficient for the condition about kernels to hold. The difficulty is in showing that it is also necessary.) Anyway, surely generalizations of the fundamental theorem of duality have been explored, and any references or pointers would be appreciated.","X f, f_1,...,f_n X f f_1,...,f_n f = \sum_{i=1}^n \lambda_i f_i \lambda_i \in \mathbb R \bigcap_{i=1}^n ker \ f_i \subseteq ker \ f ker \ g = \{x \in X: g(x)=0\} I \bigcap_{i \in I} ker \ f_i \subseteq ker \ f. \mu 2^I x \in X  f (x) = \int f_i (x) \ \mu(di).","['real-analysis', 'linear-algebra', 'functional-analysis', 'reference-request', 'duality-theorems']"
75,Proving $\mathcal{M}\left(\sin(x)\right)(s) = \Gamma(s)\sin\left(\frac{\pi}{2}s \right)$ using Real Analysis,Proving  using Real Analysis,\mathcal{M}\left(\sin(x)\right)(s) = \Gamma(s)\sin\left(\frac{\pi}{2}s \right),recently I've been investigating Mellin Transforms and this morning solved for case of $\sin(x)$ using Ramunajan's Master Theorem. I was curious if there were any Real based methods to evaluate this integral as in searching around online all proofs seem to rely on Contour Integration. My approach: \begin{equation} \mathcal{M}\left(\sin(x)\right)(s) = \int_0^\infty x^{s - 1}\sin(x)\:dx = \Gamma(s)\sin\left(\frac{\pi}{2}s \right) \end{equation} By definiton: \begin{align} \mathcal{M}\left(\sin(x)\right)(s) &= \int_0^\infty x^{s - 1}\sin(x)\:dx = \int_0^\infty x^{s - 1}\left[\sum_{m = 0}^{\infty} (-1)^m \frac{x^{2m + 1}}{(2m + 1)!} \right]\:dx \\&= \int_0^\infty x^{s} \sum_{m = 0}^{\infty} (-1)^m \frac{\left(x^2\right)^m}{(2m + 1)!}\:dx \end{align} Here we make the substitution $u = x^2$ : \begin{align} \mathcal{M}\left(\sin(x)\right)(s) &= \int_0^\infty \left(\sqrt{u}\right)^{s} \sum_{m = 0}^{\infty} (-1)^m \frac{\left(u\right)^m}{(2m + 1)!}\frac{du}{2\sqrt{u}} = \frac{1}{2}\int_0^\infty u^{\frac{s - 1}{2}}\sum_{m = 0}^{\infty} (-1)^m \frac{u^m}{(2m + 1)!}\:du  \\ &= \frac{1}{2}\int_0^\infty u^{\frac{s + 1}{2} - 1}\sum_{m = 0}^{\infty} \frac{(-u)^m}{(2m + 1)!}\:du = \frac{1}{2}\mathcal{M}\left(g(u)\right)\left(\frac{s + 1}{2}\right) \end{align} Where \begin{equation} g(u) = \sum_{m = 0}^{\infty} \frac{(-u)^m}{(2m + 1)!} = \sum_{m = 0}^{\infty} \frac{m!}{(2m + 1)!}\frac{(-u)^m}{m!} = \sum_{m = 0}^{\infty} \frac{\Gamma(m + 1)}{\Gamma(2m + 2)}\frac{(-u)^m}{m!} \end{equation} We observe that this form enables the use of Ramanujan's Master Theorem. Thus: \begin{align} \mathcal{M}\left(\sin(x)\right)(s) &= \frac{1}{2}\mathcal{M}\left(g(u)\right)\left(\frac{s + 1}{2}\right) = \frac{1}{2} \Gamma\left( \frac{s + 1}{2}\right) \cdot \frac{\Gamma\left(- \frac{s + 1}{2} + 1\right)}{\Gamma\left(2\cdot -\frac{s + 1}{2} + 2\right)} \\ &= \frac{1}{2} \frac{\Gamma\left( \frac{s + 1}{2}\right)\Gamma\left(1 - \frac{s + 1}{2}\right)}{\Gamma(1 - s)} \end{align} Employing Euler's Reflection Formula on the terms in the numerator we arrive at: \begin{align} \mathcal{M}\left(\sin(x)\right)(s) &= \frac{1}{2} \frac{\Gamma\left( \frac{s + 1}{2}\right)\Gamma\left(1 - \frac{s + 1}{2}\right)}{\Gamma(1 - s)} = \frac{1}{2} \cdot \frac{1}{\Gamma(1 - s)}\cdot \frac{\pi}{\sin\left(\pi  \cdot \frac{s + 1}{2}\right)} = \frac{\pi}{2} \frac{1}{\Gamma\left(1 - s\right)\sin\left(\frac{\pi}{2} (s + 1)\right)} \end{align} Now Euler's Reflection Formula: \begin{equation}  \Gamma(s)\Gamma(1 - s) = \frac{\pi}{\sin(\pi s)} \rightarrow \Gamma(1 - s) =  \frac{\pi}{\Gamma(s)\sin(\pi s)} \end{equation} Returning to our integral and observing that $\sin\left(\frac{\pi}{2} (s + 1)\right)= \cos\left(\frac{\pi}{2}s\right)$ we arrive at: \begin{align} \mathcal{M}\left(\sin(x)\right)(s) &= \frac{\pi}{2} \frac{1}{\Gamma\left(1 - s\right)\sin\left(\frac{\pi}{2} (s + 1)\right)} = \frac{\pi}{2} \cdot \frac{1}{\frac{\pi}{\Gamma(s)\sin(\pi s)} \cdot \cos\left(\frac{\pi}{2}s\right)} = \frac{\Gamma(s)\sin(\pi s)}{2 \cos\left(\frac{\pi}{2} s\right)} \end{align} We now use the double angle formula $\sin\left(\pi s\right) = 2\sin\left(\frac{\pi}{2} s\right)\cos\left(\frac{\pi}{2} s\right)$ to yield: \begin{align} \mathcal{M}\left(\sin(x)\right)(s) &= \frac{\Gamma(s)\sin\left(\pi s\right)}{2 \cos\left(\frac{\pi}{2} s\right)} = \frac{\Gamma(s)\cdot  2\sin\left(\frac{\pi}{2} s\right)\cos\left(\frac{\pi}{2} s\right)}{2 \cos\left(\frac{\pi}{2} s\right)} = \Gamma(s)\sin\left(\frac{\pi}{2} s\right) \end{align} As required.,recently I've been investigating Mellin Transforms and this morning solved for case of using Ramunajan's Master Theorem. I was curious if there were any Real based methods to evaluate this integral as in searching around online all proofs seem to rely on Contour Integration. My approach: By definiton: Here we make the substitution : Where We observe that this form enables the use of Ramanujan's Master Theorem. Thus: Employing Euler's Reflection Formula on the terms in the numerator we arrive at: Now Euler's Reflection Formula: Returning to our integral and observing that we arrive at: We now use the double angle formula to yield: As required.,"\sin(x) \begin{equation}
\mathcal{M}\left(\sin(x)\right)(s) = \int_0^\infty x^{s - 1}\sin(x)\:dx = \Gamma(s)\sin\left(\frac{\pi}{2}s \right)
\end{equation} \begin{align}
\mathcal{M}\left(\sin(x)\right)(s) &= \int_0^\infty x^{s - 1}\sin(x)\:dx = \int_0^\infty x^{s - 1}\left[\sum_{m = 0}^{\infty} (-1)^m \frac{x^{2m + 1}}{(2m + 1)!} \right]\:dx \\&= \int_0^\infty x^{s} \sum_{m = 0}^{\infty} (-1)^m \frac{\left(x^2\right)^m}{(2m + 1)!}\:dx
\end{align} u = x^2 \begin{align}
\mathcal{M}\left(\sin(x)\right)(s) &= \int_0^\infty \left(\sqrt{u}\right)^{s} \sum_{m = 0}^{\infty} (-1)^m \frac{\left(u\right)^m}{(2m + 1)!}\frac{du}{2\sqrt{u}} = \frac{1}{2}\int_0^\infty u^{\frac{s - 1}{2}}\sum_{m = 0}^{\infty} (-1)^m \frac{u^m}{(2m + 1)!}\:du  \\
&= \frac{1}{2}\int_0^\infty u^{\frac{s + 1}{2} - 1}\sum_{m = 0}^{\infty} \frac{(-u)^m}{(2m + 1)!}\:du = \frac{1}{2}\mathcal{M}\left(g(u)\right)\left(\frac{s + 1}{2}\right)
\end{align} \begin{equation}
g(u) = \sum_{m = 0}^{\infty} \frac{(-u)^m}{(2m + 1)!} = \sum_{m = 0}^{\infty} \frac{m!}{(2m + 1)!}\frac{(-u)^m}{m!} = \sum_{m = 0}^{\infty} \frac{\Gamma(m + 1)}{\Gamma(2m + 2)}\frac{(-u)^m}{m!}
\end{equation} \begin{align}
\mathcal{M}\left(\sin(x)\right)(s) &= \frac{1}{2}\mathcal{M}\left(g(u)\right)\left(\frac{s + 1}{2}\right) = \frac{1}{2} \Gamma\left( \frac{s + 1}{2}\right) \cdot \frac{\Gamma\left(- \frac{s + 1}{2} + 1\right)}{\Gamma\left(2\cdot -\frac{s + 1}{2} + 2\right)} \\
&= \frac{1}{2} \frac{\Gamma\left( \frac{s + 1}{2}\right)\Gamma\left(1 - \frac{s + 1}{2}\right)}{\Gamma(1 - s)}
\end{align} \begin{align}
\mathcal{M}\left(\sin(x)\right)(s) &= \frac{1}{2} \frac{\Gamma\left( \frac{s + 1}{2}\right)\Gamma\left(1 - \frac{s + 1}{2}\right)}{\Gamma(1 - s)} = \frac{1}{2} \cdot \frac{1}{\Gamma(1 - s)}\cdot \frac{\pi}{\sin\left(\pi  \cdot \frac{s + 1}{2}\right)} = \frac{\pi}{2} \frac{1}{\Gamma\left(1 - s\right)\sin\left(\frac{\pi}{2} (s + 1)\right)}
\end{align} \begin{equation}
 \Gamma(s)\Gamma(1 - s) = \frac{\pi}{\sin(\pi s)} \rightarrow \Gamma(1 - s) =  \frac{\pi}{\Gamma(s)\sin(\pi s)}
\end{equation} \sin\left(\frac{\pi}{2} (s + 1)\right)= \cos\left(\frac{\pi}{2}s\right) \begin{align}
\mathcal{M}\left(\sin(x)\right)(s) &= \frac{\pi}{2} \frac{1}{\Gamma\left(1 - s\right)\sin\left(\frac{\pi}{2} (s + 1)\right)} = \frac{\pi}{2} \cdot \frac{1}{\frac{\pi}{\Gamma(s)\sin(\pi s)} \cdot \cos\left(\frac{\pi}{2}s\right)} = \frac{\Gamma(s)\sin(\pi s)}{2 \cos\left(\frac{\pi}{2} s\right)}
\end{align} \sin\left(\pi s\right) = 2\sin\left(\frac{\pi}{2} s\right)\cos\left(\frac{\pi}{2} s\right) \begin{align}
\mathcal{M}\left(\sin(x)\right)(s) &= \frac{\Gamma(s)\sin\left(\pi s\right)}{2 \cos\left(\frac{\pi}{2} s\right)} = \frac{\Gamma(s)\cdot  2\sin\left(\frac{\pi}{2} s\right)\cos\left(\frac{\pi}{2} s\right)}{2 \cos\left(\frac{\pi}{2} s\right)} = \Gamma(s)\sin\left(\frac{\pi}{2} s\right)
\end{align}","['real-analysis', 'integration']"
76,Series power function over exponential function,Series power function over exponential function,,"A typical exercise from calculus is to show that any exponential function eventually grows faster than any power function, i.e. $$ \lim_{k \to \infty} \frac{k^a}{b^k} = 0 \qquad \text{ for } a,b>1.$$ In fact, by the ratio test, we can show for $x=a=b$ the even stronger result that the series $$ \sum_{k=1}^\infty \frac{k^x}{x^k} $$ converges for any $x \in (1,\infty)$ . This gave me the idea to consider the function $F\colon (1,\infty) \to \mathbb{R}$ defined by $$ F(x) = \sum_{k=1}^\infty \frac{k^x}{x^k} \qquad \text{for } x \in (1,\infty).$$ Now I am curious what properties I can find for this function, but my literature search so far didn't give really fitting results. Is there a name for this function? For integer arguments I could already use the relation $$ F(n) = \text{Li}_{-n}\left(\frac{1}{n}\right), \qquad n \in \mathbb{N}$$ with $\text{Li}$ the polylogarithm to find the representation $$ F(n) = \frac{n}{(n-1)^{n+1}}A_n(n), $$ where $A_n$ is the $n$ -th Eulerian polynomial . Furthermore, $F$ seems to have a global minimum at around $$ x = 3.1200906359597\ldots \quad \text{with} \quad F(x)=4.1125402415512\ldots$$ that I found by bisection. The above results gave me hope that there is a closed formula for this minimum as well, e.g. something in terms of elementary functions, but I can't really figure it out. Any ideas?","A typical exercise from calculus is to show that any exponential function eventually grows faster than any power function, i.e. In fact, by the ratio test, we can show for the even stronger result that the series converges for any . This gave me the idea to consider the function defined by Now I am curious what properties I can find for this function, but my literature search so far didn't give really fitting results. Is there a name for this function? For integer arguments I could already use the relation with the polylogarithm to find the representation where is the -th Eulerian polynomial . Furthermore, seems to have a global minimum at around that I found by bisection. The above results gave me hope that there is a closed formula for this minimum as well, e.g. something in terms of elementary functions, but I can't really figure it out. Any ideas?"," \lim_{k \to \infty} \frac{k^a}{b^k} = 0 \qquad \text{ for } a,b>1. x=a=b  \sum_{k=1}^\infty \frac{k^x}{x^k}  x \in (1,\infty) F\colon (1,\infty) \to \mathbb{R}  F(x) = \sum_{k=1}^\infty \frac{k^x}{x^k} \qquad \text{for } x \in (1,\infty).  F(n) = \text{Li}_{-n}\left(\frac{1}{n}\right), \qquad n \in \mathbb{N} \text{Li}  F(n) = \frac{n}{(n-1)^{n+1}}A_n(n),  A_n n F  x = 3.1200906359597\ldots \quad \text{with} \quad F(x)=4.1125402415512\ldots","['real-analysis', 'sequences-and-series', 'optimization', 'polylogarithm', 'eulerian-numbers']"
77,Uniformly convergent on each compact set of $\mathbb R$ but not on $\mathbb R$,Uniformly convergent on each compact set of  but not on,\mathbb R \mathbb R,"As the title says, I am looking for a sequence of function which is uniformly convergent on all compact sets of $\mathbb R$ but not on $\mathbb R$ . I thought $f_n(x) = \frac{x}{n}$ is such a function since for any x in a bounded and closed subset of $\mathbb R$ . $\sup(f_n(x)-f(x)) \to 0$ as $n\to\infty$ . But since $\mathbb R$ is unbounded $x$ can get infinitely large thus the function sequence does not uniformly converge on $\mathbb{R}$ . I wanted to check if my understanding is correct. Thank you","As the title says, I am looking for a sequence of function which is uniformly convergent on all compact sets of but not on . I thought is such a function since for any x in a bounded and closed subset of . as . But since is unbounded can get infinitely large thus the function sequence does not uniformly converge on . I wanted to check if my understanding is correct. Thank you",\mathbb R \mathbb R f_n(x) = \frac{x}{n} \mathbb R \sup(f_n(x)-f(x)) \to 0 n\to\infty \mathbb R x \mathbb{R},"['real-analysis', 'limits', 'convergence-divergence', 'uniform-convergence', 'sequence-of-function']"
78,limsup of continuous function is measurable.,limsup of continuous function is measurable.,,"I want to show that if $F$ is continuous on $[a,b]$ then $$\limsup_{h \rightarrow0, h>0} \frac{F(x+h)-F(x)}{h}$$ is measurable. By the definition of $\limsup$ we can write \begin{align} &\limsup_{h \rightarrow0, h>0} \frac{F(x+h)-F(x)}{h}\\ &=\lim_{\delta\rightarrow 0}\left(\sup_{0<h<\delta} \ \frac{F(x+h)-F(x)}{h}\right)\\ &=\lim_{n\rightarrow \infty}\left(\sup_{0<h<\frac{1}{n}} \ \frac{F(x+h)-F(x)}{h}\right). \end{align} If we show $\sup_{0<h<\frac{1}{n}} \ \frac{F(x+h)-F(x)}{h}$ is measurable then we can use the fact that limit of a countable sequence of measurable functions is also measurable. I want to show that $$\sup_{0<h<\delta}\frac{F(x+h)-F(x)}{h}=\sup_{0<h<\delta,h \in \mathbb{Q}}\frac{F(x+h)-F(x)}{h}$$ It suffices to show that $$\sup_{0<h<\delta,h \in \mathbb{Q}}\frac{F(x+h)-F(x)}{h}\geq \sup_{0<h<\delta}\frac{F(x+h)-F(x)}{h}$$ . Given $\epsilon>0$ , there exists $0<h_0<\delta$ such that $$\frac{F(x+h_0)-F(x)}{h_0}>\sup_{0<h<\delta}\frac{F(x+h)-F(x)}{h}-\epsilon$$ Let's fix $x$ . Since $\frac{F(x+h)-F(x)}{h}$ is a continuous function with variable $h$ , there exists a rational $0<q<\delta$ close enough to $h_0$ such that $$\left|\frac{F(x+q)-F(x)}{q}-\frac{F(x+h_0)-F(x)}{h_0}\right|<\epsilon$$ Therefore, $$\frac{F(x+q)-F(x)}{q}>\sup_{0<h<\delta}\frac{F(x+h)-F(x)}{h}-2\epsilon$$ Since $\epsilon$ is arbitrary, $$\sup_{0<h<\delta,h \in \mathbb{Q}}\frac{F(x+h)-F(x)}{h}\geq \sup_{0<h<\delta}\frac{F(x+h)-F(x)}{h}$$","I want to show that if is continuous on then is measurable. By the definition of we can write If we show is measurable then we can use the fact that limit of a countable sequence of measurable functions is also measurable. I want to show that It suffices to show that . Given , there exists such that Let's fix . Since is a continuous function with variable , there exists a rational close enough to such that Therefore, Since is arbitrary,","F [a,b] \limsup_{h \rightarrow0, h>0} \frac{F(x+h)-F(x)}{h} \limsup \begin{align}
&\limsup_{h \rightarrow0, h>0} \frac{F(x+h)-F(x)}{h}\\
&=\lim_{\delta\rightarrow 0}\left(\sup_{0<h<\delta} \ \frac{F(x+h)-F(x)}{h}\right)\\
&=\lim_{n\rightarrow \infty}\left(\sup_{0<h<\frac{1}{n}} \ \frac{F(x+h)-F(x)}{h}\right).
\end{align} \sup_{0<h<\frac{1}{n}} \ \frac{F(x+h)-F(x)}{h} \sup_{0<h<\delta}\frac{F(x+h)-F(x)}{h}=\sup_{0<h<\delta,h \in \mathbb{Q}}\frac{F(x+h)-F(x)}{h} \sup_{0<h<\delta,h \in \mathbb{Q}}\frac{F(x+h)-F(x)}{h}\geq \sup_{0<h<\delta}\frac{F(x+h)-F(x)}{h} \epsilon>0 0<h_0<\delta \frac{F(x+h_0)-F(x)}{h_0}>\sup_{0<h<\delta}\frac{F(x+h)-F(x)}{h}-\epsilon x \frac{F(x+h)-F(x)}{h} h 0<q<\delta h_0 \left|\frac{F(x+q)-F(x)}{q}-\frac{F(x+h_0)-F(x)}{h_0}\right|<\epsilon \frac{F(x+q)-F(x)}{q}>\sup_{0<h<\delta}\frac{F(x+h)-F(x)}{h}-2\epsilon \epsilon \sup_{0<h<\delta,h \in \mathbb{Q}}\frac{F(x+h)-F(x)}{h}\geq \sup_{0<h<\delta}\frac{F(x+h)-F(x)}{h}","['real-analysis', 'measure-theory', 'lebesgue-integral', 'lebesgue-measure']"
79,"If $\sum a_k$ converges, does $\sum(a_{k+1}- 2 a_{k+3})$ converge as well?","If  converges, does  converge as well?",\sum a_k \sum(a_{k+1}- 2 a_{k+3}),"If $\sum_{k=0}^\infty a_k$ is convergent with value $s$ , what about $\sum_{k=0}^\infty b_k$ where $b_k=a_{k+1}- 2 a_{k+3}$ ? My reasoning: $$\sum_{k=0}^\infty b_k =\lim_{n \rightarrow \infty} \sum_{k=0}^n b_k=\lim_{n \rightarrow \infty} \sum_{k=0}^na_{k+1}-2a_{k+3} $$ Within the sum we are only dealing with finitely many terms, we can split up the sum and take the limit afterwards: $$ \lim_{n \rightarrow \infty} \sum_{k=0}^n b_k=\lim_{n \rightarrow \infty} \left( \sum_{k=0}^n a_{k+1}- \sum_{k=0}^n 2a_{k+3} \right) $$ Now we want to get $s$ in here, the value of our sum, we need to do some index juggling, since our sum is not of the right form yet. $$\lim_{n \rightarrow \infty} \left( \sum_{k=0}^n a_{k} - a_0- 2\sum_{k=0}^n a_{k}  +2a_0 + 2a_1+2a_2\right) $$ Here we applied an index shift, but then we need to compensate for the terms that we added to the sum, we finally apply the limit and get: $$\sum_{k=0}^\infty b_k=s +a_0-2s+2a_1+2a_2= a_0+2a_1+2a_2 -s $$ Did that all make sense, is my reasoning correct?  conclusion: it converges as we computed the exact value.","If is convergent with value , what about where ? My reasoning: Within the sum we are only dealing with finitely many terms, we can split up the sum and take the limit afterwards: Now we want to get in here, the value of our sum, we need to do some index juggling, since our sum is not of the right form yet. Here we applied an index shift, but then we need to compensate for the terms that we added to the sum, we finally apply the limit and get: Did that all make sense, is my reasoning correct?  conclusion: it converges as we computed the exact value.","\sum_{k=0}^\infty a_k s \sum_{k=0}^\infty b_k b_k=a_{k+1}- 2 a_{k+3} \sum_{k=0}^\infty b_k =\lim_{n \rightarrow \infty} \sum_{k=0}^n b_k=\lim_{n \rightarrow \infty} \sum_{k=0}^na_{k+1}-2a_{k+3}
  \lim_{n \rightarrow \infty} \sum_{k=0}^n b_k=\lim_{n \rightarrow \infty} \left( \sum_{k=0}^n a_{k+1}- \sum_{k=0}^n 2a_{k+3} \right)  s \lim_{n \rightarrow \infty} \left( \sum_{k=0}^n a_{k} - a_0- 2\sum_{k=0}^n a_{k}  +2a_0 + 2a_1+2a_2\right)  \sum_{k=0}^\infty b_k=s +a_0-2s+2a_1+2a_2= a_0+2a_1+2a_2 -s ",['real-analysis']
80,"Find $f$ such that : $f$ is absolutely integrable, $f'$ is absolutely integrable and such that $f$ is not $1/2$-Hölder","Find  such that :  is absolutely integrable,  is absolutely integrable and such that  is not -Hölder",f f f' f 1/2,"I am trying to find a function $f: \mathbb{R}^+ \to \mathbb{R}^+$ that fullfils the following conditions $$f \in \mathcal{C}^1(\mathbb{R}^+,\mathbb{R}^+)$$ $$\int_{\mathbb{R}^+} f \in \mathbb{R}^+$$ $$\int_{\mathbb{R}^+} \mid f' \mid \in \mathbb{R}$$ $$f \text{is not $\frac{1}{2}$-Hölder}$$ I've tried functions with smooth spikes but I am unable to express this function as combinations of usual functions. Moreover, I know from this post that if $f'^2$ is integrable then $f$ is necessarily $\frac{1}{2}-$ Hölder. Thank you. Note: all the integrals are taken in the Riemann sense.","I am trying to find a function that fullfils the following conditions I've tried functions with smooth spikes but I am unable to express this function as combinations of usual functions. Moreover, I know from this post that if is integrable then is necessarily Hölder. Thank you. Note: all the integrals are taken in the Riemann sense.","f: \mathbb{R}^+ \to \mathbb{R}^+ f \in \mathcal{C}^1(\mathbb{R}^+,\mathbb{R}^+) \int_{\mathbb{R}^+} f \in \mathbb{R}^+ \int_{\mathbb{R}^+} \mid f' \mid \in \mathbb{R} f \text{is not \frac{1}{2}-Hölder} f'^2 f \frac{1}{2}-","['calculus', 'real-analysis', 'integration', 'indefinite-integrals', 'sobolev-spaces']"
81,Conserved Current for a PDE,Conserved Current for a PDE,,"Let $(x,t) \in \mathbb{R}^2$ , $W(x)$ be a (smooth enough) real-valued function and consider the following partial differential equation for the real-valued function $U(x,t)$ \begin{equation} \frac{\partial^2 U}{\partial t^2} = - \frac{\hbar^2}{4 m^2} \frac{\partial^4 U}{\partial x^4}+ \frac{W}{m} \frac{\partial^2 U}{\partial x^2} +\frac{W’}{m} \frac{\partial U}{\partial x} + \left( \frac{W’’}{2m} - \frac{W^2}{\hbar^2} \right) U \qquad (I), \end{equation} where $m$ and $\hbar$ are positive constants. In the following we shall be quite sloppy, and we shall assume that given (smooth enough) initial conditions $U(x,0)$ and $\frac{\partial U}{\partial t}(x,0)$ (lying in some space) there exists a unique (smooth enough) solution $U$ (lying in some space) to (I). Let us call the set of solutions $\mathcal{E}$ . Let $D_{x}^k F$ be the set of all partial derivatives of $F$ with respect to $x$ from order 1 to order $k$ . I ask whether there exist (smooth enough) real-valued functions $p \geq 0$ and $j$ such that, by setting \begin{equation} P(x,t)=p \left(U(x,t),(D_{x}^k U)(x,t), \frac{\partial U}{\partial t}(x,t), \left(D_{x}^{k} \frac{\partial U}{\partial t}\right)(x,t) \right), \\ J(x,t)=j \left(U(x,t),(D_{x}^k U)(x,t), \frac{\partial U}{\partial t}(x,t), \left(D_{x}^{k} \frac{\partial U}{\partial t}\right)(x,t) \right), \end{equation} the following properties hold: (i) if $U$ is the solution of (I) corresponding to a given function $W(x)$ and given initial conditions $U(x,0)$ and $\frac{\partial U}{\partial t}(x,0)$ , and $\tilde{U}$ is the solution of (I) corresponding to the same initial conditions, but to $W(x)+c$ , with $c \in \mathbb{R}$ , then $P(x,t)$ is the same when computed for $U$ and $\tilde{U}$ ; (ii) for every $U \in \mathcal{E}$ the following conservation law holds \begin{equation} \frac{\partial P}{\partial t} + \frac{\partial J}{\partial x}=0; \end{equation} (iii) $p$ is not the constant function. The answer I think should be negative, but I don't how to ""prove"" this: since we have not formulated the problem in a rigorous way, we do not expect to get a rigorous proof, but some heuristic, but convincing argument in this direction. NOTE (1) This problem, as the notation shows, has a physical background, and the mathematical formulation of the problem that I give here is my personal interpretation of a physical exposition given by the great XXth century physicist David Bohm in his wonderful treatise $\mathit{Quantum}$ $\mathit{Theory}$ published in 1951. For all the physical details about this problem see my post Nonexistence of a Probability for Real Wave Functions . NOTE (2) Bohm's physical discussion is not very clear, so that it can admit different mathematical interpretations. A simpler interpretation of Bohm's original statement is the following. Consider the following equation \begin{equation} \frac{\partial^2 U}{\partial t^2} = - \frac{\hbar^2}{4m^2} \frac{\partial^4 U}{\partial x^4} \qquad{(II)}, \end{equation} and let $\mathcal{F}$ the set of all (smooth enough) solutions of this equation. Do there exists (smooth enough) real-valued functions $p \geq 0$ and $j$ such that, by setting \begin{equation} P(x,t)=p \left(U(x,t),(D_{x}^k U)(x,t), \frac{\partial U}{\partial t}(x,t), \left(D_{x}^{k} \frac{\partial U}{\partial t}\right)(x,t) \right), \\ J(x,t)=j \left(U(x,t),(D_{x}^k U)(x,t), \frac{\partial U}{\partial t}(x,t), \left(D_{x}^{k} \frac{\partial U}{\partial t}\right)(x,t) \right), \end{equation} the following properties hold: (i) for every $U \in \mathcal{F}$ we have \begin{equation} \frac{\partial P}{\partial t} + \frac{\partial J}{\partial x}=0; \end{equation} (ii) for the special solution $U(x,t)=\cos\left(\sqrt{\frac{2m \omega}{\hbar}}x-\omega t \right)$ , we have that $P(x,t)$ is independent of $\omega > 0$ ; (iii) $p$ is not a constant function? Maybe this mathematical problem is more easily seen to have a negative answer than the one I formulated above.","Let , be a (smooth enough) real-valued function and consider the following partial differential equation for the real-valued function where and are positive constants. In the following we shall be quite sloppy, and we shall assume that given (smooth enough) initial conditions and (lying in some space) there exists a unique (smooth enough) solution (lying in some space) to (I). Let us call the set of solutions . Let be the set of all partial derivatives of with respect to from order 1 to order . I ask whether there exist (smooth enough) real-valued functions and such that, by setting the following properties hold: (i) if is the solution of (I) corresponding to a given function and given initial conditions and , and is the solution of (I) corresponding to the same initial conditions, but to , with , then is the same when computed for and ; (ii) for every the following conservation law holds (iii) is not the constant function. The answer I think should be negative, but I don't how to ""prove"" this: since we have not formulated the problem in a rigorous way, we do not expect to get a rigorous proof, but some heuristic, but convincing argument in this direction. NOTE (1) This problem, as the notation shows, has a physical background, and the mathematical formulation of the problem that I give here is my personal interpretation of a physical exposition given by the great XXth century physicist David Bohm in his wonderful treatise published in 1951. For all the physical details about this problem see my post Nonexistence of a Probability for Real Wave Functions . NOTE (2) Bohm's physical discussion is not very clear, so that it can admit different mathematical interpretations. A simpler interpretation of Bohm's original statement is the following. Consider the following equation and let the set of all (smooth enough) solutions of this equation. Do there exists (smooth enough) real-valued functions and such that, by setting the following properties hold: (i) for every we have (ii) for the special solution , we have that is independent of ; (iii) is not a constant function? Maybe this mathematical problem is more easily seen to have a negative answer than the one I formulated above.","(x,t) \in \mathbb{R}^2 W(x) U(x,t) \begin{equation}
\frac{\partial^2 U}{\partial t^2} = - \frac{\hbar^2}{4 m^2} \frac{\partial^4 U}{\partial x^4}+ \frac{W}{m} \frac{\partial^2 U}{\partial x^2} +\frac{W’}{m} \frac{\partial U}{\partial x} + \left( \frac{W’’}{2m} - \frac{W^2}{\hbar^2} \right) U \qquad (I),
\end{equation} m \hbar U(x,0) \frac{\partial U}{\partial t}(x,0) U \mathcal{E} D_{x}^k F F x k p \geq 0 j \begin{equation}
P(x,t)=p \left(U(x,t),(D_{x}^k U)(x,t), \frac{\partial U}{\partial t}(x,t), \left(D_{x}^{k} \frac{\partial U}{\partial t}\right)(x,t) \right), \\
J(x,t)=j \left(U(x,t),(D_{x}^k U)(x,t), \frac{\partial U}{\partial t}(x,t), \left(D_{x}^{k} \frac{\partial U}{\partial t}\right)(x,t) \right),
\end{equation} U W(x) U(x,0) \frac{\partial U}{\partial t}(x,0) \tilde{U} W(x)+c c \in \mathbb{R} P(x,t) U \tilde{U} U \in \mathcal{E} \begin{equation}
\frac{\partial P}{\partial t} + \frac{\partial J}{\partial x}=0;
\end{equation} p \mathit{Quantum} \mathit{Theory} \begin{equation}
\frac{\partial^2 U}{\partial t^2} = - \frac{\hbar^2}{4m^2} \frac{\partial^4 U}{\partial x^4} \qquad{(II)},
\end{equation} \mathcal{F} p \geq 0 j \begin{equation}
P(x,t)=p \left(U(x,t),(D_{x}^k U)(x,t), \frac{\partial U}{\partial t}(x,t), \left(D_{x}^{k} \frac{\partial U}{\partial t}\right)(x,t) \right), \\
J(x,t)=j \left(U(x,t),(D_{x}^k U)(x,t), \frac{\partial U}{\partial t}(x,t), \left(D_{x}^{k} \frac{\partial U}{\partial t}\right)(x,t) \right),
\end{equation} U \in \mathcal{F} \begin{equation}
\frac{\partial P}{\partial t} + \frac{\partial J}{\partial x}=0;
\end{equation} U(x,t)=\cos\left(\sqrt{\frac{2m \omega}{\hbar}}x-\omega t \right) P(x,t) \omega > 0 p","['real-analysis', 'partial-differential-equations', 'mathematical-physics']"
82,Questions on Proof of Intermediate Value Theorem,Questions on Proof of Intermediate Value Theorem,,"I am trying to follow a proof of the Intermediate Value Theorem in Ross's real-analysis textbook, but do not understand several of the steps. I'm going to replicate the proof as much as I can (sometimes adding additional detail, so if I state something incorrectly without knowing, please tell me) and pause at these questions. Theorem. If $f$ is a continuous real-valued function on an interval $I$ , then $f$ has the intermediate value property on $I$ : Whenever $a, b \in I$ , $a < b$ , and $y$ lies between $f(a)$ and $f(b)$ [i.e., $f(a) < y < f(b)$ or $f(b) < y < f(a)$ ], there exists at least one $x$ in $(a,b)$ such that $f(x) = y$ . -- Proof. Suppose that $f(a) < y < f(b)$ . We define a set $S = \{x \in [a,b]: f(x) < y\}$ . Since $f(a) < y$ by assumption, $a \in S$ , so $S$ is nonempty. Furthermore, $[a,b] = \{x \in \mathbb{R} : a \leq x \leq b\}$ by definition, so it follows, it follows that $x \leq b, \forall x \in S$ , so $S$ is bounded above by $b$ . By the completeness axiom, $S$ has a supremum, $x_0$ . For any $n \in \mathbb{N}$ , $x_0 - \frac{1}{n} < x_0$ , so $x_0 - \frac{1}{n}$ is not an upper bound for the set of $S$ , by the definition of the supremum. The only thing I am confused on here is whether we are able to assert that $x < b$ . We know $f(b) > y$ , so $b$ is not in $S$ , which suggests that we can make this greater assertion. Because $x_0 - \frac{1}{n}$ is not an upper bound of $S$ , there exists some element in $S$ greater than it. So, for every $n \in \mathbb{N}$ , there exists some $s_n \in S$ such that $x_0 - \frac{1}{n} < s_n \leq x_0$ . Here, I am not completely sure on the strategy. He seems to take $s_n$ as not a particular element of $S$ , but to treat $S$ as a sequence (we take the limit in the next step) and then take $s_n$ to be a subsequence where we create a value for each natural number. Is this the correct way to think about it? Therefore, $\lim s_n = x_0$ . Does this follow from the squeeze lemma? Limits preserve $\leq$ inequalities, so $x_0 - \frac{1}{n} < s_n \leq x_0$ implies $\lim \left(x_0 - \frac{1}{n} \right) \leq \lim s_n \leq \lim \left(x_0 \right)$ and that $x_0 \leq \lim s_n \leq x_0$ , so by the squeeze lemma $\lim s_n = x_n$ . That $f(s_n) < y$ for all $n \in \mathbb{N}$ , and since $f$ is continuous, it follows that $f(x_0) = \lim f(s_n) \leq y$ . Assuming that I am correct in thinking that each $s_n$ is an element of the set that we are treating as a subsequence of $S$ (meaning that when we talk about the limit of $s_n$ , we are are talking about the sequence, but when we say $f(s_n) < y$ , we are talking an individual element), then I believe i understand this. Define $t_n = \min\left(b, x_0 + \frac{1}{n}\right)$ . I am assuming, again, that this is a subsequence of $S$ . Observe that $x_0 \leq t_n \leq x_0 + \frac{1}{n}$ . I do not understand completely the second of these inequalities. If $t_n = b$ , since $b$ is an upper bound of $S$ and $x_0$ is its supremum, it follows that $x_0 \leq t_n$ , so I understand this. But $n$ is strictly positive, obviously, so $x_0 + \frac{1}{n} > x_0$ . If $x_0 = t_n$ , then $x_0 + \frac{1}{n} > t_n$ . I suppose we have $t_n = x_0 + \frac{1}{n}$ is $x_0 + \frac{1}{n} < b$ and thus $t_n = x_0 + \frac{1}{n}$ . Is this correct? Therefore, $\lim t_n = x_0$ . I assume this is also a result of the squeeze lemma, correct? Observe that each $t_n$ belongs to $[a,b]$ but not to $S$ . If $t_n = b$ , then it surely doesn't belong to $S$ since $f(b) > y$ . Is the additional argument then that since $x_0$ is the supremum of $S$ , then if $x_0 + \frac{1}{n}$ were in $S$ , then $x_0$ couldn't be its supremum (a contradiction), then $x_0 + \frac{1}{n}$ cannot be in the set? I think I understand why $t_n$ is not in $S$ , but not why they are still in $[a,b]$ . Surely $b$ is in $[a,b]$ , and I suppose in the case where $x_0 + \frac{1}{n}$ is greater than $b$ , then $t_n = b$ , so we stay in $[a,b]$ . Is that correct? Therefore, $f(t_n) \geq y$ for all $n$ . Therefore, since $f$ is continuous, $f(x_0) = \lim f(t_n) \geq y$ . Since $f(x_0) \leq y$ and $f(x_0) \geq y$ , it follows that $f(x_0) = y$ . Therefore, $\exists x \in (a,b)$ such that $f(x) = y$ . I understand everything here. I would greatly appreciate any insights on the above points or on something I missed. Thanks in advance.","I am trying to follow a proof of the Intermediate Value Theorem in Ross's real-analysis textbook, but do not understand several of the steps. I'm going to replicate the proof as much as I can (sometimes adding additional detail, so if I state something incorrectly without knowing, please tell me) and pause at these questions. Theorem. If is a continuous real-valued function on an interval , then has the intermediate value property on : Whenever , , and lies between and [i.e., or ], there exists at least one in such that . -- Proof. Suppose that . We define a set . Since by assumption, , so is nonempty. Furthermore, by definition, so it follows, it follows that , so is bounded above by . By the completeness axiom, has a supremum, . For any , , so is not an upper bound for the set of , by the definition of the supremum. The only thing I am confused on here is whether we are able to assert that . We know , so is not in , which suggests that we can make this greater assertion. Because is not an upper bound of , there exists some element in greater than it. So, for every , there exists some such that . Here, I am not completely sure on the strategy. He seems to take as not a particular element of , but to treat as a sequence (we take the limit in the next step) and then take to be a subsequence where we create a value for each natural number. Is this the correct way to think about it? Therefore, . Does this follow from the squeeze lemma? Limits preserve inequalities, so implies and that , so by the squeeze lemma . That for all , and since is continuous, it follows that . Assuming that I am correct in thinking that each is an element of the set that we are treating as a subsequence of (meaning that when we talk about the limit of , we are are talking about the sequence, but when we say , we are talking an individual element), then I believe i understand this. Define . I am assuming, again, that this is a subsequence of . Observe that . I do not understand completely the second of these inequalities. If , since is an upper bound of and is its supremum, it follows that , so I understand this. But is strictly positive, obviously, so . If , then . I suppose we have is and thus . Is this correct? Therefore, . I assume this is also a result of the squeeze lemma, correct? Observe that each belongs to but not to . If , then it surely doesn't belong to since . Is the additional argument then that since is the supremum of , then if were in , then couldn't be its supremum (a contradiction), then cannot be in the set? I think I understand why is not in , but not why they are still in . Surely is in , and I suppose in the case where is greater than , then , so we stay in . Is that correct? Therefore, for all . Therefore, since is continuous, . Since and , it follows that . Therefore, such that . I understand everything here. I would greatly appreciate any insights on the above points or on something I missed. Thanks in advance.","f I f I a, b \in I a < b y f(a) f(b) f(a) < y < f(b) f(b) < y < f(a) x (a,b) f(x) = y f(a) < y < f(b) S = \{x \in [a,b]: f(x) < y\} f(a) < y a \in S S [a,b] = \{x \in \mathbb{R} : a \leq x \leq b\} x \leq b, \forall x \in S S b S x_0 n \in \mathbb{N} x_0 - \frac{1}{n} < x_0 x_0 - \frac{1}{n} S x < b f(b) > y b S x_0 - \frac{1}{n} S S n \in \mathbb{N} s_n \in S x_0 - \frac{1}{n} < s_n \leq x_0 s_n S S s_n \lim s_n = x_0 \leq x_0 - \frac{1}{n} < s_n \leq x_0 \lim \left(x_0 - \frac{1}{n} \right) \leq \lim s_n \leq \lim \left(x_0 \right) x_0 \leq \lim s_n \leq x_0 \lim s_n = x_n f(s_n) < y n \in \mathbb{N} f f(x_0) = \lim f(s_n) \leq y s_n S s_n f(s_n) < y t_n = \min\left(b, x_0 + \frac{1}{n}\right) S x_0 \leq t_n \leq x_0 + \frac{1}{n} t_n = b b S x_0 x_0 \leq t_n n x_0 + \frac{1}{n} > x_0 x_0 = t_n x_0 + \frac{1}{n} > t_n t_n = x_0 + \frac{1}{n} x_0 + \frac{1}{n} < b t_n = x_0 + \frac{1}{n} \lim t_n = x_0 t_n [a,b] S t_n = b S f(b) > y x_0 S x_0 + \frac{1}{n} S x_0 x_0 + \frac{1}{n} t_n S [a,b] b [a,b] x_0 + \frac{1}{n} b t_n = b [a,b] f(t_n) \geq y n f f(x_0) = \lim f(t_n) \geq y f(x_0) \leq y f(x_0) \geq y f(x_0) = y \exists x \in (a,b) f(x) = y",['real-analysis']
83,$\frac37x + \frac{3\times6}{7\times10}x^2 + \frac{3\times6\times9}{7\times10\times13}x^3+ \cdots$,,\frac37x + \frac{3\times6}{7\times10}x^2 + \frac{3\times6\times9}{7\times10\times13}x^3+ \cdots,Convergence of the series for $x>0$ : $$\frac37x + \frac{3\times6}{7\times10}x^2 + \frac{3\times6\times9}{7\times10\times13}x^3+ \cdots$$ The general tem  is $a_n = \frac{3\times6\times \cdots \times(3+(n-1)3)}{7\times10 \times\cdots \times(7+(n-1)3)} x^n$ . I am thinking of using Raabe's Test. but cannot proceed. We have to find $$\lim _{n \to \infty} n\left(\frac{a_n}{a_{n+1} } -1\right) = \lim _{n \to \infty} n \left(\frac{7+3n}{3+3n} \frac1x -1\right)$$ so the limit depends upon $x$ .,Convergence of the series for : The general tem  is . I am thinking of using Raabe's Test. but cannot proceed. We have to find so the limit depends upon .,x>0 \frac37x + \frac{3\times6}{7\times10}x^2 + \frac{3\times6\times9}{7\times10\times13}x^3+ \cdots a_n = \frac{3\times6\times \cdots \times(3+(n-1)3)}{7\times10 \times\cdots \times(7+(n-1)3)} x^n \lim _{n \to \infty} n\left(\frac{a_n}{a_{n+1} } -1\right) = \lim _{n \to \infty} n \left(\frac{7+3n}{3+3n} \frac1x -1\right) x,"['real-analysis', 'sequences-and-series']"
84,Prove $\sum\limits_{i=1}^n\frac1{x_i}\sqrt{x_i-x_{i-1}}\le\sum\limits_{i=1}^{n^2}\frac1i-\frac12$ for integers $1=x_0\le x_1\le x_2\le\cdots\le x_n$,Prove  for integers,\sum\limits_{i=1}^n\frac1{x_i}\sqrt{x_i-x_{i-1}}\le\sum\limits_{i=1}^{n^2}\frac1i-\frac12 1=x_0\le x_1\le x_2\le\cdots\le x_n,"Suppose $x_{i}\in N^{+}$ , and $1=x_{0}\le x_{1}\le x_{2}\le\cdots\le x_{n}$ . Show that $$\sum_{i=1}^{n}\dfrac{\sqrt{x_{i}-x_{i-1}}}{x_{i}}\le\sum_{i=1}^{n^2}\dfrac{1}{i}-\dfrac{1}{2}$$ Maybe it can be proved by using the C-S inequality. But I am unable to find a solution.","Suppose , and . Show that Maybe it can be proved by using the C-S inequality. But I am unable to find a solution.",x_{i}\in N^{+} 1=x_{0}\le x_{1}\le x_{2}\le\cdots\le x_{n} \sum_{i=1}^{n}\dfrac{\sqrt{x_{i}-x_{i-1}}}{x_{i}}\le\sum_{i=1}^{n^2}\dfrac{1}{i}-\dfrac{1}{2},"['real-analysis', 'sequences-and-series', 'inequality', 'contest-math', 'radicals']"
85,Learning Real Analysis outside a classroom setting,Learning Real Analysis outside a classroom setting,,"I am studying Real Analysis -- using Baby Rudin -- on my own in a community that does not have a university near (if the ""why"" is important, see below).  I recently worked through standard texts for Calculus, (basic) Differential Equations, Linear Algebra, Vector Calculus and [Eccles] ""Introduction to Mathematical Reasoning"" in preparation.  Most of those were fairly easy to do in a self-study environment, because it was reasonably easy to tell, when doing the problem-sets, whether I'd gotten it right or not.  (Student & Instructor answer books and Chegg helped a good bit when I got stuck; which was not often, but more often than I'd like .) However: while I have the answer-book for Rudin, I'm finding it much harder to tell -- where most problems are of the form ""Prove X"" -- whether my proofs are valid unless they match precisely those in the answers.  Sometimes they do.  Usually there are subtle (or greater) differences.  And I cannot reliably tell whether those 'subtle differences' are inconsequential or horrifyingly stupid (plus WRONG).   Anyone with thoughts or experience on how this subject can best be learned in a self-study environment?  Or is it likely I'm going to have to develop a resource -- one way or the other -- to 'grade' my efforts and help keep me on track?  Thanks for considering. [Possibly unnecessary framing information: Mostly I just want to spiff up my mathematical understanding.  But there exists a research project that I'd like to join, in my field (Medicine), which really needs at least some ability to think topologically about the problems, and which often involve (somewhat) the math of Chaos.  Thus I think I need to bring myself up to at least that level to participate.]","I am studying Real Analysis -- using Baby Rudin -- on my own in a community that does not have a university near (if the ""why"" is important, see below).  I recently worked through standard texts for Calculus, (basic) Differential Equations, Linear Algebra, Vector Calculus and [Eccles] ""Introduction to Mathematical Reasoning"" in preparation.  Most of those were fairly easy to do in a self-study environment, because it was reasonably easy to tell, when doing the problem-sets, whether I'd gotten it right or not.  (Student & Instructor answer books and Chegg helped a good bit when I got stuck; which was not often, but more often than I'd like .) However: while I have the answer-book for Rudin, I'm finding it much harder to tell -- where most problems are of the form ""Prove X"" -- whether my proofs are valid unless they match precisely those in the answers.  Sometimes they do.  Usually there are subtle (or greater) differences.  And I cannot reliably tell whether those 'subtle differences' are inconsequential or horrifyingly stupid (plus WRONG).   Anyone with thoughts or experience on how this subject can best be learned in a self-study environment?  Or is it likely I'm going to have to develop a resource -- one way or the other -- to 'grade' my efforts and help keep me on track?  Thanks for considering. [Possibly unnecessary framing information: Mostly I just want to spiff up my mathematical understanding.  But there exists a research project that I'd like to join, in my field (Medicine), which really needs at least some ability to think topologically about the problems, and which often involve (somewhat) the math of Chaos.  Thus I think I need to bring myself up to at least that level to participate.]",,"['real-analysis', 'proof-verification', 'self-learning']"
86,$a<b(b+2)$ then $f(a)< 2f(b).$,then,a<b(b+2) f(a)< 2f(b).,"I am looking for a positive continuous function $f$ such that for all positive $a,b>0$ $$a < b(b+2)\Longrightarrow f(a)<2f(b)$$ and $$a=b(b+2)\Longrightarrow f(a)=2f(b).$$ Does such a function exist? I tried to constructing one using exponential functions, as they are positive, but I failed.","I am looking for a positive continuous function $f$ such that for all positive $a,b>0$ $$a < b(b+2)\Longrightarrow f(a)<2f(b)$$ and $$a=b(b+2)\Longrightarrow f(a)=2f(b).$$ Does such a function exist? I tried to constructing one using exponential functions, as they are positive, but I failed.",,"['real-analysis', 'linear-algebra']"
87,Stochastic Processes - Bass exercise 1.6,Stochastic Processes - Bass exercise 1.6,,"I'm working on this problem similar to one from Bass' book on stochastic processes and I cannot for the life of me make any progress. Let $X$ be a stochastic process and suppose that each sample path of $X$ is cadlag. Put $\mathcal{F}^X_t = \sigma(X_s:s \leq t)$, $\Delta X_t = X_{t} - X_{t^-}$, $\mathcal{F}_{\infty} = \sigma(\mathcal{F}^X_t:t\geq 0 )$, and $A_c = \{\omega \in \Omega: \text{ for some }t > 0\text{, } \Delta X_t(\omega) > c\}$. Show that $A_c \in \mathcal{F}_{\infty}$. So far I know that for $t>0$, $\Delta X_t(\omega) > c$ if and only if there exists $K \in \mathbb{N}$ such that for each $n \in \mathbb{N}$, there exists $m \in \mathbb{N}$ such that for each $t_1 \in (0,t) \cap \mathbb{Q}, t_2 \in (t,\infty)\cap \mathbb{Q}$, if $|t_1 - t_2| < 1/m$, then $X_{t_2}(\omega) - X_{t_1}(\omega)>c + 1/K -1/n$. If I put $B_t = \{\omega \in \Omega: \Delta X_t(\omega) > c\}$, $R_{t_1, t_2, m} = \{\omega \in \Omega: |t_1 - t_2| < 1/m \}$, $S_{K, t_1,t_2,n} = \{\omega \in \Omega: X_{t_2}(\omega) - X_{t_1}(\omega) > c +1/K - 1/n\}$, then $R_{t_1, t_2, m}$, $S_{K, t_1,t_2,n} \in \mathcal{F}_\infty$. So $B_t = \bigcup\limits_{K \in \mathbb{N}}\bigcap\limits_{n \in \mathbb{N}} \bigcup\limits_{m \in \mathbb{N}} \bigcap\limits_{t_1 \in (0,t)\cap \mathbb{Q}} \bigcap\limits_{t_2 \in (t, \infty)\cap \mathbb{Q}} R_{t_1, t_2, m}^c \cup S_{K, t_1, t_2, n}$ and hence $B_t \in \mathcal{F}_{\infty}$. Now I'm stuck trying to answer the question since $A_c = \bigcup\limits_{t>0}B_t$ and I cannot figure out how to get a countable union. I thought a little about how cadlag functions have at most countably many discontinuities, but I couldn't figure a way to work that in. I'm sort of expecting this entire process to be an unhelpful direction to begin at, but it was the best that I could come up with. The question reminds me of exercise 1.7 in Karatzas and Shreve. However in that problem I was able to get a uniform characterization of continuity to be able to get a countable union/intersection. With this one, I'm not sure if that's possible, so feel free to offer any other approach to solving the problem. Thanks everyone.","I'm working on this problem similar to one from Bass' book on stochastic processes and I cannot for the life of me make any progress. Let $X$ be a stochastic process and suppose that each sample path of $X$ is cadlag. Put $\mathcal{F}^X_t = \sigma(X_s:s \leq t)$, $\Delta X_t = X_{t} - X_{t^-}$, $\mathcal{F}_{\infty} = \sigma(\mathcal{F}^X_t:t\geq 0 )$, and $A_c = \{\omega \in \Omega: \text{ for some }t > 0\text{, } \Delta X_t(\omega) > c\}$. Show that $A_c \in \mathcal{F}_{\infty}$. So far I know that for $t>0$, $\Delta X_t(\omega) > c$ if and only if there exists $K \in \mathbb{N}$ such that for each $n \in \mathbb{N}$, there exists $m \in \mathbb{N}$ such that for each $t_1 \in (0,t) \cap \mathbb{Q}, t_2 \in (t,\infty)\cap \mathbb{Q}$, if $|t_1 - t_2| < 1/m$, then $X_{t_2}(\omega) - X_{t_1}(\omega)>c + 1/K -1/n$. If I put $B_t = \{\omega \in \Omega: \Delta X_t(\omega) > c\}$, $R_{t_1, t_2, m} = \{\omega \in \Omega: |t_1 - t_2| < 1/m \}$, $S_{K, t_1,t_2,n} = \{\omega \in \Omega: X_{t_2}(\omega) - X_{t_1}(\omega) > c +1/K - 1/n\}$, then $R_{t_1, t_2, m}$, $S_{K, t_1,t_2,n} \in \mathcal{F}_\infty$. So $B_t = \bigcup\limits_{K \in \mathbb{N}}\bigcap\limits_{n \in \mathbb{N}} \bigcup\limits_{m \in \mathbb{N}} \bigcap\limits_{t_1 \in (0,t)\cap \mathbb{Q}} \bigcap\limits_{t_2 \in (t, \infty)\cap \mathbb{Q}} R_{t_1, t_2, m}^c \cup S_{K, t_1, t_2, n}$ and hence $B_t \in \mathcal{F}_{\infty}$. Now I'm stuck trying to answer the question since $A_c = \bigcup\limits_{t>0}B_t$ and I cannot figure out how to get a countable union. I thought a little about how cadlag functions have at most countably many discontinuities, but I couldn't figure a way to work that in. I'm sort of expecting this entire process to be an unhelpful direction to begin at, but it was the best that I could come up with. The question reminds me of exercise 1.7 in Karatzas and Shreve. However in that problem I was able to get a uniform characterization of continuity to be able to get a countable union/intersection. With this one, I'm not sure if that's possible, so feel free to offer any other approach to solving the problem. Thanks everyone.",,"['real-analysis', 'probability-theory', 'measure-theory', 'stochastic-processes']"
88,Open covers for $\mathbb{R}$,Open covers for,\mathbb{R},"Does it seem plausible to conjecture that given an enumeration of the rational numbers $\{ q_1, q_2, q_3, \ldots \}$ and a convergent sequence $\{ \epsilon_n \}_{n \in \mathbb{Z^+}} \subseteq \mathbb{R^+}$ such that $\lim \epsilon_n = 0$ but $\sum_{n \in \mathbb{Z^+}} \epsilon_n = +\infty$, the set of open intervals $\{ (q_n - \epsilon_n, q_n + \epsilon_n) \mid n \in \mathbb{Z^+} \}$ covers $\mathbb{R}$? For instance, can $\mathbb{R}$ be covered by $\{ (q_n - \frac{1}{n}, q_n + \frac{1}{n}) \mid n \in \mathbb{Z^+} \}$? It looks like if we allow $\lim \epsilon_n \neq 0$, the conjecture might work...","Does it seem plausible to conjecture that given an enumeration of the rational numbers $\{ q_1, q_2, q_3, \ldots \}$ and a convergent sequence $\{ \epsilon_n \}_{n \in \mathbb{Z^+}} \subseteq \mathbb{R^+}$ such that $\lim \epsilon_n = 0$ but $\sum_{n \in \mathbb{Z^+}} \epsilon_n = +\infty$, the set of open intervals $\{ (q_n - \epsilon_n, q_n + \epsilon_n) \mid n \in \mathbb{Z^+} \}$ covers $\mathbb{R}$? For instance, can $\mathbb{R}$ be covered by $\{ (q_n - \frac{1}{n}, q_n + \frac{1}{n}) \mid n \in \mathbb{Z^+} \}$? It looks like if we allow $\lim \epsilon_n \neq 0$, the conjecture might work...",,['real-analysis']
89,Counter example which violate uniform continuity...,Counter example which violate uniform continuity...,,"For a given continuous function $f:\mathbb{R}\rightarrow\mathbb{R}$, define a sequence of function $\{f_n\}_{n\in \mathbb{N}}$ by $$f_n(x):=f\Big(x+\frac{1}{n}\Big).$$ Now if $f$ is uniformly continuous, then I can show that {$f_n$} converges to $f$ uniformly. At this point, I was thinking about the converse, i,e; if {$f_n$} defined above converges to $f$ uniformly, does that imply the uniform continuity of $f$. I think the answer is negative, but I am not getting any counter example for that. Does anyone have any counter example?","For a given continuous function $f:\mathbb{R}\rightarrow\mathbb{R}$, define a sequence of function $\{f_n\}_{n\in \mathbb{N}}$ by $$f_n(x):=f\Big(x+\frac{1}{n}\Big).$$ Now if $f$ is uniformly continuous, then I can show that {$f_n$} converges to $f$ uniformly. At this point, I was thinking about the converse, i,e; if {$f_n$} defined above converges to $f$ uniformly, does that imply the uniform continuity of $f$. I think the answer is negative, but I am not getting any counter example for that. Does anyone have any counter example?",,"['real-analysis', 'uniform-convergence', 'uniform-continuity']"
90,Twice differentiable function not equal to $0$,Twice differentiable function not equal to,0,"This is a question in the undergraduate-level textbook ""Advanced Calculus"" by Fitzpatrick. Suppose that a function $f:\mathbb{R}\rightarrow\mathbb{R}$ is twice differentiable such that for $\forall x$, $f'(x)\leq f(x)$, and $f(0)=0$. Then is $f$ the zero function? The answer to this is not true as I was able to find a counter-example $f^*(x)= 1- e^x$. However we have only just learned about differentiation, the mean-value theorem and how to find extremes using 1st and 2nd derivatives, and we have only seen derivatives of polynomials so far, but I don't know how to disprove the above statement by using these. (EDIT) For $1−e^x$ to be a valid counter-example, I need to ""officially know"" that the exponential function's derivative is equal to itself. But exponential functions are in the next chapter. Therefore unless I want to ""cheat"", I need to think of another function.","This is a question in the undergraduate-level textbook ""Advanced Calculus"" by Fitzpatrick. Suppose that a function $f:\mathbb{R}\rightarrow\mathbb{R}$ is twice differentiable such that for $\forall x$, $f'(x)\leq f(x)$, and $f(0)=0$. Then is $f$ the zero function? The answer to this is not true as I was able to find a counter-example $f^*(x)= 1- e^x$. However we have only just learned about differentiation, the mean-value theorem and how to find extremes using 1st and 2nd derivatives, and we have only seen derivatives of polynomials so far, but I don't know how to disprove the above statement by using these. (EDIT) For $1−e^x$ to be a valid counter-example, I need to ""officially know"" that the exponential function's derivative is equal to itself. But exponential functions are in the next chapter. Therefore unless I want to ""cheat"", I need to think of another function.",,"['calculus', 'real-analysis', 'derivatives']"
91,Generalization of convexity,Generalization of convexity,,"This is not a homework, this is just something that came to my mind recently. Assume $f$ is a sufficiently nice function. We know that $$\frac{df}{dx} \geq 0 \iff f(x_2) \geq f(x_1) \text{ for } x_2 \geq x_1$$ $$\frac{d^2f}{dx^2} \geq 0 \iff \frac{f(x_1) + f(x_2)}{2} \geq f(\frac{x_1 + x_2}{2})$$ Is there any nice way to generalize this for for higher derivatives? A generalization of nonnegativity of higher order derivatives being equivalent to some short nice condition not involving any derivatives at all? If not generalization, is there at least a nice extension to $\frac{d^3f}{dx^3}$?","This is not a homework, this is just something that came to my mind recently. Assume $f$ is a sufficiently nice function. We know that $$\frac{df}{dx} \geq 0 \iff f(x_2) \geq f(x_1) \text{ for } x_2 \geq x_1$$ $$\frac{d^2f}{dx^2} \geq 0 \iff \frac{f(x_1) + f(x_2)}{2} \geq f(\frac{x_1 + x_2}{2})$$ Is there any nice way to generalize this for for higher derivatives? A generalization of nonnegativity of higher order derivatives being equivalent to some short nice condition not involving any derivatives at all? If not generalization, is there at least a nice extension to $\frac{d^3f}{dx^3}$?",,"['calculus', 'real-analysis', 'inequality']"
92,How interpret $\||x|^\gamma u\|_{L^r}\leq C\|\nabla u\|_{L^p}^{a}\||x|^\beta u\|_{L^q}^{1-a}$ for $u\in\mathcal C_0^1(\mathbb R^d)$?,How interpret  for ?,\||x|^\gamma u\|_{L^r}\leq C\|\nabla u\|_{L^p}^{a}\||x|^\beta u\|_{L^q}^{1-a} u\in\mathcal C_0^1(\mathbb R^d),"Let $$\frac{1}{r}+\frac{\gamma }{d}=a\left(\frac{1}{p}+\frac{\alpha -1}{d}\right)+(1-a)\left(\frac{1}{q}+\frac{\beta }{d}\right),$$ where $d\geq 1$, $a\in [0,1]$, $\alpha  ,\beta,\gamma  \in \mathbb R$. There exists $C$ independent of $u$ s.t. $$\||x|^\gamma u\|_{L^r(\mathbb R^d)}\leq C\||x|^\alpha \nabla u\|_{L^p(\mathbb R^d)}^{a}\||x|^\beta u\|_{L^q(\mathbb R^d)}^{1-a}$$ for all $u\in\mathcal C_0^1(\mathbb R^d)$. How can I interpret this ? For example, if $\frac{1}{r}=\frac{a}{p}+\frac{1-a}{q}$, I know that for all $u\in L^p\cap L^q$ we have that $$\|u\|_{L^r}\leq \|u\|_{L^p}^a\|u\|_{L^q}^{1-a}$$ what can be interpreted as if $u\in L^p\cap L^q$, then $u\in L^r$ for all $r\in [p,q]$.  Or in other word $L^r\supset L^p\cap L^q$ for all $r\in [p,q]$. Q1) For my case, we have that $u\in \mathcal C_0^1 (\mathbb R^d)$ so it seems to be normal that $u\in L^p$ and $u\in W^{1,p}$ for all $p\geq 1$... but may be there is a density argument behind ? Q2) So may be if $\alpha =\gamma =\beta =0$ I can say that $L^r\supset W^{1,q}\cap L^{p^*}$ for all $r\in [p^*,q]$ where $\frac{1}{p^*}=\frac{1}{p}-\frac{1}{d}$ ? But it doesn't look incredible... Q3) And what are those $|x|^\gamma, |x|^\alpha  $ and $|x|^\beta $ ? Which information do they give us ? Q4) Could you also give me an application of such an inequality ?","Let $$\frac{1}{r}+\frac{\gamma }{d}=a\left(\frac{1}{p}+\frac{\alpha -1}{d}\right)+(1-a)\left(\frac{1}{q}+\frac{\beta }{d}\right),$$ where $d\geq 1$, $a\in [0,1]$, $\alpha  ,\beta,\gamma  \in \mathbb R$. There exists $C$ independent of $u$ s.t. $$\||x|^\gamma u\|_{L^r(\mathbb R^d)}\leq C\||x|^\alpha \nabla u\|_{L^p(\mathbb R^d)}^{a}\||x|^\beta u\|_{L^q(\mathbb R^d)}^{1-a}$$ for all $u\in\mathcal C_0^1(\mathbb R^d)$. How can I interpret this ? For example, if $\frac{1}{r}=\frac{a}{p}+\frac{1-a}{q}$, I know that for all $u\in L^p\cap L^q$ we have that $$\|u\|_{L^r}\leq \|u\|_{L^p}^a\|u\|_{L^q}^{1-a}$$ what can be interpreted as if $u\in L^p\cap L^q$, then $u\in L^r$ for all $r\in [p,q]$.  Or in other word $L^r\supset L^p\cap L^q$ for all $r\in [p,q]$. Q1) For my case, we have that $u\in \mathcal C_0^1 (\mathbb R^d)$ so it seems to be normal that $u\in L^p$ and $u\in W^{1,p}$ for all $p\geq 1$... but may be there is a density argument behind ? Q2) So may be if $\alpha =\gamma =\beta =0$ I can say that $L^r\supset W^{1,q}\cap L^{p^*}$ for all $r\in [p^*,q]$ where $\frac{1}{p^*}=\frac{1}{p}-\frac{1}{d}$ ? But it doesn't look incredible... Q3) And what are those $|x|^\gamma, |x|^\alpha  $ and $|x|^\beta $ ? Which information do they give us ? Q4) Could you also give me an application of such an inequality ?",,"['real-analysis', 'sobolev-spaces', 'interpolation']"
93,Existence of at least one positive root,Existence of at least one positive root,,"Let $f$ be a polynomial of degree $n$, say $f = \sum\limits_{k=0}^n c_k x^k$, such that the first $c_0$ and last $c_n$ coefficients have opposite signs. Prove that $f(x) = 0$ for at least one positive $x$. Proof. Suppose $c_0 < 0 < c_n$. It is easily seen that $f(0) = c_0$. Also, $$f(x) = c_n x^n \Bigg(1 + \frac{c_{n-1}}{c_nx} + ...+ \frac{c_0}{c_nx^n} \Bigg)$$ So, for large enough $x$, say $x = L$, we have $f(L) \approx c_n L^n$. Also, $c_n < c_n L^n$. Thus, $$f(0) = c_0 < 0 < c_n < f(L)$$ Since we have $f(0) < 0 < f(L)$ for $x \in [0,L]$. From Bolzano's theorem, there exist a positive $x$ such that $f(x) = 0$. $\rm{QED}$ *Calculus Volume 1 by Apostol exercise 3.11.1 Can anyone comment on my proof? Did I miss something? Thanks!","Let $f$ be a polynomial of degree $n$, say $f = \sum\limits_{k=0}^n c_k x^k$, such that the first $c_0$ and last $c_n$ coefficients have opposite signs. Prove that $f(x) = 0$ for at least one positive $x$. Proof. Suppose $c_0 < 0 < c_n$. It is easily seen that $f(0) = c_0$. Also, $$f(x) = c_n x^n \Bigg(1 + \frac{c_{n-1}}{c_nx} + ...+ \frac{c_0}{c_nx^n} \Bigg)$$ So, for large enough $x$, say $x = L$, we have $f(L) \approx c_n L^n$. Also, $c_n < c_n L^n$. Thus, $$f(0) = c_0 < 0 < c_n < f(L)$$ Since we have $f(0) < 0 < f(L)$ for $x \in [0,L]$. From Bolzano's theorem, there exist a positive $x$ such that $f(x) = 0$. $\rm{QED}$ *Calculus Volume 1 by Apostol exercise 3.11.1 Can anyone comment on my proof? Did I miss something? Thanks!",,"['calculus', 'real-analysis']"
94,"Show that if $f$ is increasing on $[a, b]$ and satisfies the intermediate value property, then $f$ is continuous on $[a, b]$","Show that if  is increasing on  and satisfies the intermediate value property, then  is continuous on","f [a, b] f [a, b]","I know this question has been asked before but I feel like my approach to solving the problem is ""different"" (EDIT: turned out to be different because it's wrong!) Since $[a,b]$ is closed and bounded we may conclude that, by the Heini-Borel theorem, $[a, b]$ must be compact. By definition of compactness, every sequence in $[a,b]$ must contain a subsequence that converges to a limit that is also in $[a,b]$ . Consider any arbitrary increasing subsequence $\{x_n\}$ inside $[a,b]$ . By the definition of compactness, we know that there exist some $c \in [a,b]$ such that $\{x_n\} \rightarrow c$ . Since $f$ satisfies the intermediate value property, we know that $f(c)$ exists and is within the range of $f$ . One of the ""characterizations of continuity"" states that: ""For all $\{x_n\} \rightarrow c$ , it follows that $f(x_n) → f(c).$ "". How do I show that $f(x_n)$ converges to $f(c)$ ? Thing is, I know that $f(x_n)$ has to converge to something since the range of $f$ is a compact set ( $[f(a), f(b)]$ )… I'm just trying to show that $f(x_n) \rightarrow f(c)$ ! Any idea of how I can go about this?","I know this question has been asked before but I feel like my approach to solving the problem is ""different"" (EDIT: turned out to be different because it's wrong!) Since is closed and bounded we may conclude that, by the Heini-Borel theorem, must be compact. By definition of compactness, every sequence in must contain a subsequence that converges to a limit that is also in . Consider any arbitrary increasing subsequence inside . By the definition of compactness, we know that there exist some such that . Since satisfies the intermediate value property, we know that exists and is within the range of . One of the ""characterizations of continuity"" states that: ""For all , it follows that "". How do I show that converges to ? Thing is, I know that has to converge to something since the range of is a compact set ( )… I'm just trying to show that ! Any idea of how I can go about this?","[a,b] [a, b] [a,b] [a,b] \{x_n\} [a,b] c \in [a,b] \{x_n\} \rightarrow c f f(c) f \{x_n\} \rightarrow c f(x_n) → f(c). f(x_n) f(c) f(x_n) f [f(a), f(b)] f(x_n) \rightarrow f(c)",['real-analysis']
95,Existence of a set,Existence of a set,,"Does there exist $T \subset \mathbb{Q}$ such that, for every $z \in \mathbb{Z}$, there exists a unique finite set $A_{z} \subset T$ such that $z=\sum_{t \in A_{z}}{t}$?","Does there exist $T \subset \mathbb{Q}$ such that, for every $z \in \mathbb{Z}$, there exists a unique finite set $A_{z} \subset T$ such that $z=\sum_{t \in A_{z}}{t}$?",,"['real-analysis', 'contest-math']"
96,On $\int_1^\infty \left(\sum_{1\leq k\leq t}\frac{\mu(k)}{k}\right)\log \left(\frac{1-e^{\frac{2\pi i}{t}}}{1-e^{\frac{-2\pi i}{t}}}\right)dt$,On,\int_1^\infty \left(\sum_{1\leq k\leq t}\frac{\mu(k)}{k}\right)\log \left(\frac{1-e^{\frac{2\pi i}{t}}}{1-e^{\frac{-2\pi i}{t}}}\right)dt,"I've deduced the following, but I don't know if it is right Claim. One has that    $$\frac{1}{\zeta(3)}=\frac{6}{\pi^2}-\frac{i}{\pi}\int_1^\infty m(t)\log \left(\frac{1-e^{\frac{2\pi i}{t}}}{1-e^{\frac{-2\pi i}{t}}}\right)dt,\tag{1}$$ where for real numbers $x\geq 1$ one has $m(x)=\sum_{1\leq k\leq x}\frac{\mu(k)}{k}$, being $\mu(n)$ the Möbius function. Notice that I am not saying that it is interesting because from the problem to compute $(\zeta(3))^{-1}$ I am trying to study a more difficult thing, that is the integral in RHS of $(1)$. Since my proof had steps that maybe are wrongs, and since I am not able to compute an approximation of the integral using a CAS I am asking Question. Q1.) Was right the formula $(1)$? Add the reasoning of why no, or well if the Claim is true and you want add calculations/reasonings as companion of mine. Q2.) Is it possible to get an approximation of such integral in RHS of $(1)$ (I presume thus that is convergent; and notice that is a complex number) without using my identity itself? Thanks in advance. Skecht of my proof, as I said I don't provide the justifications: Step 1. From the specialization $x=1/k$ in the Fourier series for $x(1-x)$ I wrote $$ \frac{1}{k} \left( 1-\frac{1}{k} \right)  =\frac{1}{6} -\frac{1}{\pi^2}\sum_{n=1}^{\infty} \frac{1 }{n^2}\cos \left( \frac{2\pi n}{k} \right),\tag{2}$$ multiplying by $\frac{\mu(k)}{k}$, taking the sum $\sum_{k=1}^\infty$, and invoking the prime number theorem and particular values of Dirichlet series, one has $$ \frac{1}{\zeta(3)} =\frac{6}{\pi^2}+ \frac{1}{\pi^2}\sum_{k=1}^\infty\frac{\mu(k)}{k}\sum_{n=1}^{\infty} \frac{1 }{n^2}\cos \left( \frac{2\pi n}{k} \right).\tag{3}$$ Step 2. From $(3)$ and this deduction of  Abel's identity (I've used the prime number theorem) $$\lim_{x\to\infty}\sum_{k\leq x}\frac{\mu(k)}{k}\cos \left( \frac{2\pi n}{k} \right)=0-2\pi n\lim_{x\to\infty}\int_1^x \left(\sum_{k\leq t}\frac{\mu(k)}{k}\right)\sin \left( \frac{2\pi n}{t} \right)\frac{dt}{t^2},\tag{4}$$ one has the Claim using the closed-form for $$\sum_{n=1}^\infty\frac{1}{n}\sin\left( \frac{2\pi n}{t} \right)$$ that provide us Wolfram Alpha online calculator with this code sum 1/n sin(2 pi n/t), from n=1 to infinity I presume that this last calculation is using geometric series.$\square$ As is implicit I am not sure if my reasonings, justifications were 100% rights.","I've deduced the following, but I don't know if it is right Claim. One has that    $$\frac{1}{\zeta(3)}=\frac{6}{\pi^2}-\frac{i}{\pi}\int_1^\infty m(t)\log \left(\frac{1-e^{\frac{2\pi i}{t}}}{1-e^{\frac{-2\pi i}{t}}}\right)dt,\tag{1}$$ where for real numbers $x\geq 1$ one has $m(x)=\sum_{1\leq k\leq x}\frac{\mu(k)}{k}$, being $\mu(n)$ the Möbius function. Notice that I am not saying that it is interesting because from the problem to compute $(\zeta(3))^{-1}$ I am trying to study a more difficult thing, that is the integral in RHS of $(1)$. Since my proof had steps that maybe are wrongs, and since I am not able to compute an approximation of the integral using a CAS I am asking Question. Q1.) Was right the formula $(1)$? Add the reasoning of why no, or well if the Claim is true and you want add calculations/reasonings as companion of mine. Q2.) Is it possible to get an approximation of such integral in RHS of $(1)$ (I presume thus that is convergent; and notice that is a complex number) without using my identity itself? Thanks in advance. Skecht of my proof, as I said I don't provide the justifications: Step 1. From the specialization $x=1/k$ in the Fourier series for $x(1-x)$ I wrote $$ \frac{1}{k} \left( 1-\frac{1}{k} \right)  =\frac{1}{6} -\frac{1}{\pi^2}\sum_{n=1}^{\infty} \frac{1 }{n^2}\cos \left( \frac{2\pi n}{k} \right),\tag{2}$$ multiplying by $\frac{\mu(k)}{k}$, taking the sum $\sum_{k=1}^\infty$, and invoking the prime number theorem and particular values of Dirichlet series, one has $$ \frac{1}{\zeta(3)} =\frac{6}{\pi^2}+ \frac{1}{\pi^2}\sum_{k=1}^\infty\frac{\mu(k)}{k}\sum_{n=1}^{\infty} \frac{1 }{n^2}\cos \left( \frac{2\pi n}{k} \right).\tag{3}$$ Step 2. From $(3)$ and this deduction of  Abel's identity (I've used the prime number theorem) $$\lim_{x\to\infty}\sum_{k\leq x}\frac{\mu(k)}{k}\cos \left( \frac{2\pi n}{k} \right)=0-2\pi n\lim_{x\to\infty}\int_1^x \left(\sum_{k\leq t}\frac{\mu(k)}{k}\right)\sin \left( \frac{2\pi n}{t} \right)\frac{dt}{t^2},\tag{4}$$ one has the Claim using the closed-form for $$\sum_{n=1}^\infty\frac{1}{n}\sin\left( \frac{2\pi n}{t} \right)$$ that provide us Wolfram Alpha online calculator with this code sum 1/n sin(2 pi n/t), from n=1 to infinity I presume that this last calculation is using geometric series.$\square$ As is implicit I am not sure if my reasonings, justifications were 100% rights.",,"['real-analysis', 'proof-verification']"
97,Hermite polynomials approximate of a function and its derivatives,Hermite polynomials approximate of a function and its derivatives,,"Given a differentiable function $f\in \mathcal C^{(n)}(\mathbb R) \cap L^2(\mathbb R,e^{-x^2/2}dx)$ and its Hermite polynomial expansion $f_n=\sum_{i=0}^n a_i \psi_i$ . Is it true that $\int_{-\infty}^\infty |f^{(k)}(x)-f_n^{(k)}(x)|^2e^{-\frac{x^2}2}dx\to 0$ where $g^{(k)}$ is the $k$ 'th derivative of $g$ , as $n\to\infty$ , $\forall 0\le k\le n$ ? What is the proof? Is there a general result regarding the convergence of spanning orthogonal polynomial to the derivatives of the original function?","Given a differentiable function and its Hermite polynomial expansion . Is it true that where is the 'th derivative of , as , ? What is the proof? Is there a general result regarding the convergence of spanning orthogonal polynomial to the derivatives of the original function?","f\in \mathcal C^{(n)}(\mathbb R) \cap L^2(\mathbb R,e^{-x^2/2}dx) f_n=\sum_{i=0}^n a_i \psi_i \int_{-\infty}^\infty |f^{(k)}(x)-f_n^{(k)}(x)|^2e^{-\frac{x^2}2}dx\to 0 g^{(k)} k g n\to\infty \forall 0\le k\le n","['real-analysis', 'approximation-theory', 'orthogonal-polynomials', 'hermite-polynomials']"
98,How to calculate $\lim_{n\to\infty}f(n)\sin(\frac{1}{n})$ where $f(x)=\int_{x}^{x^2}(1+\frac{1}{2t})^t\sin{\frac{1}{\sqrt{t}}}dt(x>0)$,How to calculate  where,\lim_{n\to\infty}f(n)\sin(\frac{1}{n}) f(x)=\int_{x}^{x^2}(1+\frac{1}{2t})^t\sin{\frac{1}{\sqrt{t}}}dt(x>0),$$f(x)=\int_{x}^{x^2}\left(1+\frac{1}{2t}\right)^t\sin\left(\frac{1}{\sqrt{t}}\right)dt\hspace{1cm}(x>0)$$   try to find $$\lim_{n\to\infty}f(n)\sin\left(\frac{1}{n}\right)$$ I found this problem from a problem book and with a hint which tells me to apply L'Hospital's rule. But when $n\to\infty$ then $\frac{1}{\sin(1/n)}\to\infty$ and $f(n)\text{ seems}\to0$. Would you help me with this problem? Best regards!,$$f(x)=\int_{x}^{x^2}\left(1+\frac{1}{2t}\right)^t\sin\left(\frac{1}{\sqrt{t}}\right)dt\hspace{1cm}(x>0)$$   try to find $$\lim_{n\to\infty}f(n)\sin\left(\frac{1}{n}\right)$$ I found this problem from a problem book and with a hint which tells me to apply L'Hospital's rule. But when $n\to\infty$ then $\frac{1}{\sin(1/n)}\to\infty$ and $f(n)\text{ seems}\to0$. Would you help me with this problem? Best regards!,,"['real-analysis', 'limits']"
99,Find the maximum of the $a+ab+abc+abcd$,Find the maximum of the,a+ab+abc+abcd,"Let $m$ be any given positive real number, and $a+b+c+d=m$ , where $a,b,c,d\ge 0$ Find the maximum of the value $$a+ab+abc+abcd$$ try when $m=1$ ,we have $a+ab+abc+abcd\le a(b+1)(c+1)(d+1)\le\left(\dfrac{a+b+c+d+3}{4}\right)^4=1$ when $a=1,b=c=d=0$ when $m=3$ , I got the $$a+ab+abc+abcd=a+ab(1+c+cd)\le a+ab(1+c+d+cd)=a+ab(1+c)(1+d)\le a+a\left(\dfrac{b+1+c+1+d}{3}\right)^3=a+a\cdot\left(\dfrac{5-a}{3}\right)^3=4-\dfrac{1}{27}(a-2)^2(a^2-11a+27)\le 4$$ when $a=2,b=1,c=d=0$ for other any postive $m$ ,How to find it? partial derivative?","Let be any given positive real number, and , where Find the maximum of the value try when ,we have when when , I got the when for other any postive ,How to find it? partial derivative?","m a+b+c+d=m a,b,c,d\ge 0 a+ab+abc+abcd m=1 a+ab+abc+abcd\le a(b+1)(c+1)(d+1)\le\left(\dfrac{a+b+c+d+3}{4}\right)^4=1 a=1,b=c=d=0 m=3 a+ab+abc+abcd=a+ab(1+c+cd)\le a+ab(1+c+d+cd)=a+ab(1+c)(1+d)\le a+a\left(\dfrac{b+1+c+1+d}{3}\right)^3=a+a\cdot\left(\dfrac{5-a}{3}\right)^3=4-\dfrac{1}{27}(a-2)^2(a^2-11a+27)\le 4 a=2,b=1,c=d=0 m","['real-analysis', 'inequality']"
