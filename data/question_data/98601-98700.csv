,title_raw,title_text,title_latex,body_raw,body_text,body_latex,tags
0,Conformal mapping into a unit disc,Conformal mapping into a unit disc,,"$T$ is the upper half of the unit disc $U$. What is the conformal mapping $f$ of $T$ onto $U$ that transforms $\{-1,0,1\}$ to $\{-1,-i,1\}$?","$T$ is the upper half of the unit disc $U$. What is the conformal mapping $f$ of $T$ onto $U$ that transforms $\{-1,0,1\}$ to $\{-1,-i,1\}$?",,"['complex-analysis', 'conformal-geometry']"
1,Let $S$ be the open unit disk and $f: S\to \Bbb C$ be a real-valued analytic function with $f(0)=1$,Let  be the open unit disk and  be a real-valued analytic function with,S f: S\to \Bbb C f(0)=1,"I have this problem: Let $S$ be the open unit disk in $\mathbb C$ and $f:S\to \Bbb C$ be a real-valued analytic function with $f(0)=1$ .Then which of the following option is correct? The set $\{z \in S:f(z) \neq 1\}$ is: (a) empty, (b) non-empty finite, (c) countably infinite, (d) uncountable. Please help.","I have this problem: Let be the open unit disk in and be a real-valued analytic function with .Then which of the following option is correct? The set is: (a) empty, (b) non-empty finite, (c) countably infinite, (d) uncountable. Please help.",S \mathbb C f:S\to \Bbb C f(0)=1 \{z \in S:f(z) \neq 1\},['complex-analysis']
2,Is $f(z)=z^n+nwz$ one-to-one?,Is  one-to-one?,f(z)=z^n+nwz,"Let $f(z)=z^n+nwz$ be a complex function with $|w|=1$ and $n>1$ a natural number. Is this function one-to-one inside the unit circle ($|z|<1$)? ATTEMPT I didn't have a lot of luck checking $f(z_1)=f(z_2)$ and trying to manipulate the equation to get $z_1=z_2$. The factor of $n$ really invites taking the derivative, and I know there's a theorem that states something along the lines of, if $f$ is analytic in $R$ and $f'<>0$ then $f$ is one-to-one in R, but the proof of this theorem that I found relies on complex integration, which we haven't learned yet. I figure I could try and separate $f$ to real and imaginary components and find the derivative of those, and do some ad-hoc analysis, but is there an easier solution?","Let $f(z)=z^n+nwz$ be a complex function with $|w|=1$ and $n>1$ a natural number. Is this function one-to-one inside the unit circle ($|z|<1$)? ATTEMPT I didn't have a lot of luck checking $f(z_1)=f(z_2)$ and trying to manipulate the equation to get $z_1=z_2$. The factor of $n$ really invites taking the derivative, and I know there's a theorem that states something along the lines of, if $f$ is analytic in $R$ and $f'<>0$ then $f$ is one-to-one in R, but the proof of this theorem that I found relies on complex integration, which we haven't learned yet. I figure I could try and separate $f$ to real and imaginary components and find the derivative of those, and do some ad-hoc analysis, but is there an easier solution?",,['complex-analysis']
3,Proving that $\sum_{|j| < n} (n-|j|) \exp(ij\lambda)= \frac{\sin^2(\frac 1 2 n\lambda)}{\sin^2(\frac 1 2 \lambda)}$,Proving that,\sum_{|j| < n} (n-|j|) \exp(ij\lambda)= \frac{\sin^2(\frac 1 2 n\lambda)}{\sin^2(\frac 1 2 \lambda)},"I want to show that $\sum_{|j| < n} (n-|j|) \exp(ij\lambda)=  \dfrac{\sin^2(\frac 1 2 n\lambda)}{\sin^2(\frac 1 2 \lambda)}$. I know from Proving $\sum\limits_{k=0}^{n}\cos(kx)=\frac{1}{2}+\frac{\sin(\frac{2n+1}{2}x)}{2\sin(x/2)}$ \begin{equation} (1)\sum_{j=1}^{n-1} cos(j\lambda) = -\frac 1 2 + \frac{\sin(\frac{2n-1} 2 \lambda)}{2\sin(\frac \lambda 2)}. \end{equation} and from the Hint in the first answer in How to show that $\frac{1}{\tan(x/2)}=2 \sum_{j=1}^{\infty}\sin(jx)$ in Cesàro way/sense? $$ (2)\frac d {dx} \sum_{j=1}^{n-1} sin(j\lambda) =\frac d {d\lambda} \frac{\cos({\frac \lambda 2})-\cos(\frac{n+1}2 \lambda)}{2\sin(\frac\lambda 2)} = \frac{n\sin(\frac{n+1} 2 \lambda)\sin(\frac \lambda 2)+\cos(\frac{n\lambda}2) - 1}{4\sin^2(\frac \lambda 2)}. $$ This is what i did so far: \begin{align} \sum_{|j| < n} (n-|j|) \exp(ij\lambda) &= n + 2\sum_{j=1}^{n-1}(n-j) cos(j\lambda) \\ &=n + 2n \sum_{j=1}^{n-1} cos(j\lambda) - \frac d {d\lambda}\sum_{j=1}^{n-1} sin(j\lambda)\\ &= \frac{n\sin(\frac{2n-1} 2 \lambda)\sin(\frac \lambda 2) + \frac 1 2 + \frac 1 2 n \sin(\frac {n+1} 2 \lambda)\sin(\frac \lambda 2) + \frac 1 2 \cos( \frac{n\lambda}2)}{\sin^2(\frac \lambda 2)}. \end{align} If this and the proposition is true, it should be true that $$ n\sin(\frac{2n-1} 2 \lambda)\sin(\frac \lambda 2) + \frac 1 2 + \frac 1 2 n \sin(\frac {n+1} 2 \lambda)\sin(\frac \lambda 2) + \frac 1 2 \cos( \frac{n\lambda}2) - \sin^2(\frac {n\lambda} 2) = 0. $$ But for $n=2$, i get $$ 3\sin(\frac {3\lambda} 2) \sin(\frac \lambda 2) + \frac 1 2 + \frac 1 2 \cos(\lambda) - \sin^2 \lambda \ne 0. $$ (checked with Wolframalpha) So there is probably something wrong with my calculations. But i checked everything twice and don't see my mistake. Do you see it? EDIT Trying to prove it as proposed in the second answer. Let $z:=\exp(i\lambda)$ and $p(z):=\sum_{j=0}^{n-1} z^j$. First i want to show that $p(z)p(z^{-1})=\sum_{|j|<n} (n-|j|) z^j$. It is \begin{align} \sum_{j=1}^{n-1} z^{j}&= p(z) - 1 = \frac{1-z^n}{1-z} - 1.\\ \sum_{j=1}^{n-1} z^{-j}&= p(z^{-1})-1 = \frac{1-z^{-n}}{1-z^{-1}} - 1.\\ \sum_{j=1}^{n-1} jz^{-j} &= i \frac d {d\lambda} p(z^{-1}) = \frac{-nz^{-n}+(n-1)z^{-n-1}+z^{-1}}{(1-z^{-1})^2}.\\ \sum_{j=1}^{n-1} jz^{j} &= -i \frac d {d\lambda} p(z) = \frac{-nz^{n}+(n-1)z^{n+1}+z}{(1-z)^2}.\\ \end{align} So i have \begin{align} \sum_{|j|<n} (n-|j|) z^j = n + n(\sum_{j=1}^{n-1} z^{j} + \sum_{j=1}^{n-1} z^{-j}) - \sum_{j=1}^{n-1} jz^{j} - \sum_{j=1}^{n-1} jz^{-j}. \end{align} If i now put in the values from above and substract $p(z)p(z^{-1})$ i should get $0$. But Wolfram Alpha doesn't agree, so i am again on the wrong track. What did i wrong? The last step $p(z)p(z^{-1})= \frac{(z^{n/2}-z^{-n/2})^2}{(z^{1/2}-z^{-1/2})^2}$ was easy to show. EDIT2 I did it now. It was a lot easier to show directly $p(z)p(z^{-1})=\sum_{|j|<n} (n-|j|) z^j$ without the geometric series partial sums values.","I want to show that $\sum_{|j| < n} (n-|j|) \exp(ij\lambda)=  \dfrac{\sin^2(\frac 1 2 n\lambda)}{\sin^2(\frac 1 2 \lambda)}$. I know from Proving $\sum\limits_{k=0}^{n}\cos(kx)=\frac{1}{2}+\frac{\sin(\frac{2n+1}{2}x)}{2\sin(x/2)}$ \begin{equation} (1)\sum_{j=1}^{n-1} cos(j\lambda) = -\frac 1 2 + \frac{\sin(\frac{2n-1} 2 \lambda)}{2\sin(\frac \lambda 2)}. \end{equation} and from the Hint in the first answer in How to show that $\frac{1}{\tan(x/2)}=2 \sum_{j=1}^{\infty}\sin(jx)$ in Cesàro way/sense? $$ (2)\frac d {dx} \sum_{j=1}^{n-1} sin(j\lambda) =\frac d {d\lambda} \frac{\cos({\frac \lambda 2})-\cos(\frac{n+1}2 \lambda)}{2\sin(\frac\lambda 2)} = \frac{n\sin(\frac{n+1} 2 \lambda)\sin(\frac \lambda 2)+\cos(\frac{n\lambda}2) - 1}{4\sin^2(\frac \lambda 2)}. $$ This is what i did so far: \begin{align} \sum_{|j| < n} (n-|j|) \exp(ij\lambda) &= n + 2\sum_{j=1}^{n-1}(n-j) cos(j\lambda) \\ &=n + 2n \sum_{j=1}^{n-1} cos(j\lambda) - \frac d {d\lambda}\sum_{j=1}^{n-1} sin(j\lambda)\\ &= \frac{n\sin(\frac{2n-1} 2 \lambda)\sin(\frac \lambda 2) + \frac 1 2 + \frac 1 2 n \sin(\frac {n+1} 2 \lambda)\sin(\frac \lambda 2) + \frac 1 2 \cos( \frac{n\lambda}2)}{\sin^2(\frac \lambda 2)}. \end{align} If this and the proposition is true, it should be true that $$ n\sin(\frac{2n-1} 2 \lambda)\sin(\frac \lambda 2) + \frac 1 2 + \frac 1 2 n \sin(\frac {n+1} 2 \lambda)\sin(\frac \lambda 2) + \frac 1 2 \cos( \frac{n\lambda}2) - \sin^2(\frac {n\lambda} 2) = 0. $$ But for $n=2$, i get $$ 3\sin(\frac {3\lambda} 2) \sin(\frac \lambda 2) + \frac 1 2 + \frac 1 2 \cos(\lambda) - \sin^2 \lambda \ne 0. $$ (checked with Wolframalpha) So there is probably something wrong with my calculations. But i checked everything twice and don't see my mistake. Do you see it? EDIT Trying to prove it as proposed in the second answer. Let $z:=\exp(i\lambda)$ and $p(z):=\sum_{j=0}^{n-1} z^j$. First i want to show that $p(z)p(z^{-1})=\sum_{|j|<n} (n-|j|) z^j$. It is \begin{align} \sum_{j=1}^{n-1} z^{j}&= p(z) - 1 = \frac{1-z^n}{1-z} - 1.\\ \sum_{j=1}^{n-1} z^{-j}&= p(z^{-1})-1 = \frac{1-z^{-n}}{1-z^{-1}} - 1.\\ \sum_{j=1}^{n-1} jz^{-j} &= i \frac d {d\lambda} p(z^{-1}) = \frac{-nz^{-n}+(n-1)z^{-n-1}+z^{-1}}{(1-z^{-1})^2}.\\ \sum_{j=1}^{n-1} jz^{j} &= -i \frac d {d\lambda} p(z) = \frac{-nz^{n}+(n-1)z^{n+1}+z}{(1-z)^2}.\\ \end{align} So i have \begin{align} \sum_{|j|<n} (n-|j|) z^j = n + n(\sum_{j=1}^{n-1} z^{j} + \sum_{j=1}^{n-1} z^{-j}) - \sum_{j=1}^{n-1} jz^{j} - \sum_{j=1}^{n-1} jz^{-j}. \end{align} If i now put in the values from above and substract $p(z)p(z^{-1})$ i should get $0$. But Wolfram Alpha doesn't agree, so i am again on the wrong track. What did i wrong? The last step $p(z)p(z^{-1})= \frac{(z^{n/2}-z^{-n/2})^2}{(z^{1/2}-z^{-1/2})^2}$ was easy to show. EDIT2 I did it now. It was a lot easier to show directly $p(z)p(z^{-1})=\sum_{|j|<n} (n-|j|) z^j$ without the geometric series partial sums values.",,"['complex-analysis', 'analysis']"
4,Quasiconformal map between the complex plane and a disk,Quasiconformal map between the complex plane and a disk,,"According to the Poincaré-Koebe theorem, it is known that the unit disk $\mathbb D$ and the complex plane $\mathbb C$ aren't conformally equivalent. My question is maybe naive, but I was wondering if this statement is still true in the quasiconformal sense. More precisely, does there exist a quasiconformal map from $\mathbb D$ onto $\mathbb C$ ?","According to the Poincaré-Koebe theorem, it is known that the unit disk $\mathbb D$ and the complex plane $\mathbb C$ aren't conformally equivalent. My question is maybe naive, but I was wondering if this statement is still true in the quasiconformal sense. More precisely, does there exist a quasiconformal map from $\mathbb D$ onto $\mathbb C$ ?",,"['complex-analysis', 'riemannian-geometry', 'quasiconformal-maps']"
5,problem on entire function,problem on entire function,,"Let $f$ be an entire function. For which of the following  cases $f$ is not necessarily a constant $\operatorname{im}(f'(z))>0$ for all $z$ $f'(0)=0$ and $|f'(z)|\leq3$ for all $z$ $f(n)=3$ for all integer $n$ $f(z) =i$  when $z=(1+\frac{k}{n})$ for every positive integer $k$ I think 1 is true since $f'(z)=$constant so $f(z)=cz$ for some $c$ For 2, $f=0$ For 3, $f$ is not constant  since $f(z)=3 \cos2\pi z$ I have no idea for 4 am i right for other three options","Let $f$ be an entire function. For which of the following  cases $f$ is not necessarily a constant $\operatorname{im}(f'(z))>0$ for all $z$ $f'(0)=0$ and $|f'(z)|\leq3$ for all $z$ $f(n)=3$ for all integer $n$ $f(z) =i$  when $z=(1+\frac{k}{n})$ for every positive integer $k$ I think 1 is true since $f'(z)=$constant so $f(z)=cz$ for some $c$ For 2, $f=0$ For 3, $f$ is not constant  since $f(z)=3 \cos2\pi z$ I have no idea for 4 am i right for other three options",,['complex-analysis']
6,Calculate $\int_{-\infty}^{+\infty}e^{-\frac{(x-it)^2}{2}}dx$ using contour integration,Calculate  using contour integration,\int_{-\infty}^{+\infty}e^{-\frac{(x-it)^2}{2}}dx,"When one wants to calculate the characteristic function of a random variable which is of normal distribution, things boil down to calculate: $$\int_{-\infty}^{+\infty}e^{-\frac{(x-it)^2}{2}}dx$$ There are several ways to calculate this integral.  I tried to calculate this integral using contour integration : $$ \oint_C f(z)dz=\int_{-a}^af(z)dz+\int_{Arc(a)}f(z)dz $$ where $$ f(z)=e^{-\frac{(z-z_0)^2}{2}}, z_0=it $$ and $C$ is the union of a semicircle and $[-a,a]$. How can I calculate  $$ \lim_{a\to+\infty}\int_{Arc(a)}f(z)dz? $$ Alternatively, from the very beginning, I get $$ \lim_{a\to+\infty}\int_{-a-z_0}^{a-z_0}e^{-\frac{z^2}{2}}dz. $$ But I have no idea how to choose contour.","When one wants to calculate the characteristic function of a random variable which is of normal distribution, things boil down to calculate: $$\int_{-\infty}^{+\infty}e^{-\frac{(x-it)^2}{2}}dx$$ There are several ways to calculate this integral.  I tried to calculate this integral using contour integration : $$ \oint_C f(z)dz=\int_{-a}^af(z)dz+\int_{Arc(a)}f(z)dz $$ where $$ f(z)=e^{-\frac{(z-z_0)^2}{2}}, z_0=it $$ and $C$ is the union of a semicircle and $[-a,a]$. How can I calculate  $$ \lim_{a\to+\infty}\int_{Arc(a)}f(z)dz? $$ Alternatively, from the very beginning, I get $$ \lim_{a\to+\infty}\int_{-a-z_0}^{a-z_0}e^{-\frac{z^2}{2}}dz. $$ But I have no idea how to choose contour.",,['complex-analysis']
7,$f$ has a zero at $z_o$ and $g$ has a pole at $z_o$. Prove that $fg$ has a removable singularity at $z_o$?,has a zero at  and  has a pole at . Prove that  has a removable singularity at ?,f z_o g z_o fg z_o,"Problem: Suppose that $f:D(z_o,R)\to C$ is analytic and has a zero of order $m\ge1$ and that $g:D(z_o,R)\to C$ is analytic and has a pole of order $l\le m$ at $z_o$. Prove that $fg$ has a removable singularity at $z_o$. My approach: In order to show that $fg$ has a removable singularity at $z_o$, we can show that $fg$ is bounded as $z \rightarrow z_o$, or $|f(z_o)g(z_o)|\lt n \in N$. But because $f(z_o)=0$ with order $\ge 1$, we only have to show that $g(z)$ does not approach infinity as $z \rightarrow z_o$. My question: How do I show that $g(z)$ is bounded as $z \rightarrow z_o$? I know that $g$ has a pole of order $l\le m$ at $z_o$, so $ \frac{1}{g} $ has a zero of order $l\le m$ at $z_o$. It'd be easy to show that $ \frac{1}{g} $, and therefore $ \frac{f}{g} $, has a removable singularity, but I'm stuck on proving it for $fg$.","Problem: Suppose that $f:D(z_o,R)\to C$ is analytic and has a zero of order $m\ge1$ and that $g:D(z_o,R)\to C$ is analytic and has a pole of order $l\le m$ at $z_o$. Prove that $fg$ has a removable singularity at $z_o$. My approach: In order to show that $fg$ has a removable singularity at $z_o$, we can show that $fg$ is bounded as $z \rightarrow z_o$, or $|f(z_o)g(z_o)|\lt n \in N$. But because $f(z_o)=0$ with order $\ge 1$, we only have to show that $g(z)$ does not approach infinity as $z \rightarrow z_o$. My question: How do I show that $g(z)$ is bounded as $z \rightarrow z_o$? I know that $g$ has a pole of order $l\le m$ at $z_o$, so $ \frac{1}{g} $ has a zero of order $l\le m$ at $z_o$. It'd be easy to show that $ \frac{1}{g} $, and therefore $ \frac{f}{g} $, has a removable singularity, but I'm stuck on proving it for $fg$.",,['complex-analysis']
8,Exercise on Maximum Principle,Exercise on Maximum Principle,,"This exercise is taken from Gamelin's Complex Analysis, page 89, exercise 4. Let $f(z)$ be an analytic function on a domain $D$ that has no zeroes on $D$. (a) Show that $|f(z)|$ attains its minimum on $D$, then $f(z)$ is constant. (b) Show that if $D$ is bounded, and if $D$ is bounded, and if $f(z)$ extends continuously to the boundary $\partial D$ of $D$, then $|f(z)|$ attains its minimum on $\partial D$. I solved part (a), but part (b) is giving me trouble. Namely, I define $g(z)=1/f(z)$, but how do I know that f(z) doesn't have a zero on $\partial D$? I have to make that extra assumption in my proof below... And if it helps any, the book assumes D is simply connected as well. Heres my progess: Part $(a):$ Let $f(z)$ be an analytic function on a domain $D$ that has no zeroes on $D$. Suppose $|f(z)|$ attains a minimum on $D$ at $z_0$, denoted $M$. Since $f(z)$ has no zeroes on $D$, and is analytic, $g(z)=1/f(z)$ is analytic. It follows that $|g(z)|=|1/f(z)|\le 1/M$ for all $z\in\mathbb{Z}$ and $|g(z_0)|=1/M$. So, since $g(z)$ is analytic, it is also harmonic, and thus by the Strict Maximum Principle, $g(z)$ is constant, implying $f(z)$ is constant. Part $(b):$ Let $f(z)$ be an analytic function on a domain $D$ that has no zeroes on $D$ and $\partial D$. Suppose that $D$ is bounded and connected and that $f(z)$ extends continuously to $\partial D$. Then $D\cup\partial D$ is compact, as it contains its boundary and is bounded. Let $g(z)=1/f(z)$, then since $f(z)$ has no zeroes, $g(z)$ is defined over all of $D\cup\partial D$, and since its a continuous real-valued function on a compact set, it attains a maximum, denoted $M$ at $z_0$. Then $g(z)=1/f(z)\le M$, implying $f(z)\ge 1/M$ for all $z\in\mathbb{Z}$, and since $g(z_0)=M$, it follows that $f(z_0)=1/M$. If $f(z)$ obtains a minimum in $D$, it follows from part $(a)$ that $f(z)$ is constant and thus obtains its minimum on $\partial$$D$, and thus $f(z)$ always obtains its minimum on $\partial D$.","This exercise is taken from Gamelin's Complex Analysis, page 89, exercise 4. Let $f(z)$ be an analytic function on a domain $D$ that has no zeroes on $D$. (a) Show that $|f(z)|$ attains its minimum on $D$, then $f(z)$ is constant. (b) Show that if $D$ is bounded, and if $D$ is bounded, and if $f(z)$ extends continuously to the boundary $\partial D$ of $D$, then $|f(z)|$ attains its minimum on $\partial D$. I solved part (a), but part (b) is giving me trouble. Namely, I define $g(z)=1/f(z)$, but how do I know that f(z) doesn't have a zero on $\partial D$? I have to make that extra assumption in my proof below... And if it helps any, the book assumes D is simply connected as well. Heres my progess: Part $(a):$ Let $f(z)$ be an analytic function on a domain $D$ that has no zeroes on $D$. Suppose $|f(z)|$ attains a minimum on $D$ at $z_0$, denoted $M$. Since $f(z)$ has no zeroes on $D$, and is analytic, $g(z)=1/f(z)$ is analytic. It follows that $|g(z)|=|1/f(z)|\le 1/M$ for all $z\in\mathbb{Z}$ and $|g(z_0)|=1/M$. So, since $g(z)$ is analytic, it is also harmonic, and thus by the Strict Maximum Principle, $g(z)$ is constant, implying $f(z)$ is constant. Part $(b):$ Let $f(z)$ be an analytic function on a domain $D$ that has no zeroes on $D$ and $\partial D$. Suppose that $D$ is bounded and connected and that $f(z)$ extends continuously to $\partial D$. Then $D\cup\partial D$ is compact, as it contains its boundary and is bounded. Let $g(z)=1/f(z)$, then since $f(z)$ has no zeroes, $g(z)$ is defined over all of $D\cup\partial D$, and since its a continuous real-valued function on a compact set, it attains a maximum, denoted $M$ at $z_0$. Then $g(z)=1/f(z)\le M$, implying $f(z)\ge 1/M$ for all $z\in\mathbb{Z}$, and since $g(z_0)=M$, it follows that $f(z_0)=1/M$. If $f(z)$ obtains a minimum in $D$, it follows from part $(a)$ that $f(z)$ is constant and thus obtains its minimum on $\partial$$D$, and thus $f(z)$ always obtains its minimum on $\partial D$.",,['complex-analysis']
9,A Continuity Argument in the Proof of Rouche's Theorem,A Continuity Argument in the Proof of Rouche's Theorem,,"In Greene and Krantz's Function Theory of One Complex Variable , the proof of Rouche's theorem involves the following continuity argument. Let $f,g\colon U \to \mathbb{C}$ be holomorphic from an open set $U$. Let $\bar{D}(p,r) \subseteq U$, and    \begin{equation}  \vert f(\zeta) - g(\zeta) \vert \le \vert f(\zeta) + g(\zeta) \vert \end{equation}    for $\zeta \in \partial D(p,r).$ Define   $$ f_t (\zeta) = tf(\zeta) + (1-t)\,g(\zeta), $$   for $t\in [0,1]$.    Then the integral    $$ I_t = \frac{1}{2\pi i} \oint_{\partial D(p,r)} \frac{f'_t (\zeta)}{f_t (\zeta)}\,d\zeta $$   is a continuous function of $t\in [0,1]$. They add in parenthases that the denominator does not vanish and the integrand depends continuously on $t$. Question: Would anyone be kind enough to supply the details behind this continuity of $I_t$ ?","In Greene and Krantz's Function Theory of One Complex Variable , the proof of Rouche's theorem involves the following continuity argument. Let $f,g\colon U \to \mathbb{C}$ be holomorphic from an open set $U$. Let $\bar{D}(p,r) \subseteq U$, and    \begin{equation}  \vert f(\zeta) - g(\zeta) \vert \le \vert f(\zeta) + g(\zeta) \vert \end{equation}    for $\zeta \in \partial D(p,r).$ Define   $$ f_t (\zeta) = tf(\zeta) + (1-t)\,g(\zeta), $$   for $t\in [0,1]$.    Then the integral    $$ I_t = \frac{1}{2\pi i} \oint_{\partial D(p,r)} \frac{f'_t (\zeta)}{f_t (\zeta)}\,d\zeta $$   is a continuous function of $t\in [0,1]$. They add in parenthases that the denominator does not vanish and the integrand depends continuously on $t$. Question: Would anyone be kind enough to supply the details behind this continuity of $I_t$ ?",,"['complex-analysis', 'continuity']"
10,Definition of a meromorphic function by Forster's Lectures on Riemann Surfaces,Definition of a meromorphic function by Forster's Lectures on Riemann Surfaces,,"Here's the definition. Let $X$ be a Riemann surface and $Y$ be an open subset of $X$. A meromorphic function on $Y$ is a holomorphic function $f\colon Y' \rightarrow \mathbb{C}$ satisfying the following conditions, where $Y'\subset Y$ is a open subset. (1) Every point $p\in Y - Y'$ is an isolated point. (2) For every point $p\in Y - Y'$, $\lim_{x\rightarrow p} |f(x)| = \infty$. The points of $Y - Y'$ are called the poles of $f$. Then he stated in a remark: Let $(U, z)$ be a coordinate neighborhood of a pole $p$ of $f$ with $z(p) = 0$. Then $f$ can be expanded in a Laurent series $f = \sum_{n = -k}^{\infty} c_n z^n$ in a neighborhood of $p$. Why is this so?  In other words, why $p$ cannot be an essential singularity ?","Here's the definition. Let $X$ be a Riemann surface and $Y$ be an open subset of $X$. A meromorphic function on $Y$ is a holomorphic function $f\colon Y' \rightarrow \mathbb{C}$ satisfying the following conditions, where $Y'\subset Y$ is a open subset. (1) Every point $p\in Y - Y'$ is an isolated point. (2) For every point $p\in Y - Y'$, $\lim_{x\rightarrow p} |f(x)| = \infty$. The points of $Y - Y'$ are called the poles of $f$. Then he stated in a remark: Let $(U, z)$ be a coordinate neighborhood of a pole $p$ of $f$ with $z(p) = 0$. Then $f$ can be expanded in a Laurent series $f = \sum_{n = -k}^{\infty} c_n z^n$ in a neighborhood of $p$. Why is this so?  In other words, why $p$ cannot be an essential singularity ?",,"['complex-analysis', 'riemann-surfaces']"
11,Derivative of order N of a product of complex functions,Derivative of order N of a product of complex functions,,Given the function: $$F=\frac{\exp(-iTz)}{z}\prod_{k=1}^N(1-\frac{z}{z_k})^{-1}$$ with $z\in C$ is it possible to give a closed expression of the $n^{th}$ derivative of $F$ with respect to $z$?: $$\frac{d^n}{dz^n}F$$ Thanks in advance.,Given the function: $$F=\frac{\exp(-iTz)}{z}\prod_{k=1}^N(1-\frac{z}{z_k})^{-1}$$ with $z\in C$ is it possible to give a closed expression of the $n^{th}$ derivative of $F$ with respect to $z$?: $$\frac{d^n}{dz^n}F$$ Thanks in advance.,,"['complex-analysis', 'derivatives']"
12,Complex Space : Why unit disc?,Complex Space : Why unit disc?,,"In complex space for simplicity the properties of the functions on curves are sometimes considered on the unit circle on complex plane, with the center (0, 0) . So basically I would like to know why until circle ? Does it mean that all other smooth curves can be represented by combinations of circles with arbitrary radius ?","In complex space for simplicity the properties of the functions on curves are sometimes considered on the unit circle on complex plane, with the center (0, 0) . So basically I would like to know why until circle ? Does it mean that all other smooth curves can be represented by combinations of circles with arbitrary radius ?",,"['complex-analysis', 'complex-geometry']"
13,"Questions about analytic functions, and zeros.","Questions about analytic functions, and zeros.",,"I'm studying Silverman's complex analysis but this book seems a lot slack. I have a question in page273: Suppose $f(z)$ is a nonconstant analytic at $z_0$ with $f(z_0)$=0. Then, by the   corollary to Theorem 8.13, there exists a neighborhood of $z_0$ that   contains no other zeros of $f(z)$. Thus we may express $f(z)$ as   $f(z)=(z-z_0)^k F(z)$ ($k$ a positive integer), where $F(z)$ is   analytic at $z_0$ with no zeros in the neighborhood or on its boundary   $C$. I can't understand the last sentence(bold fonts). How can we know there is a factor $(z-z_0)^k$ and even more $F(z)$ is analytic? I also have Ahlfors' book so you can give me references in Silverman or Ahlfors.","I'm studying Silverman's complex analysis but this book seems a lot slack. I have a question in page273: Suppose $f(z)$ is a nonconstant analytic at $z_0$ with $f(z_0)$=0. Then, by the   corollary to Theorem 8.13, there exists a neighborhood of $z_0$ that   contains no other zeros of $f(z)$. Thus we may express $f(z)$ as   $f(z)=(z-z_0)^k F(z)$ ($k$ a positive integer), where $F(z)$ is   analytic at $z_0$ with no zeros in the neighborhood or on its boundary   $C$. I can't understand the last sentence(bold fonts). How can we know there is a factor $(z-z_0)^k$ and even more $F(z)$ is analytic? I also have Ahlfors' book so you can give me references in Silverman or Ahlfors.",,['complex-analysis']
14,differential system on the torus,differential system on the torus,,"In a recent topic I've studied on complex analysis I had to study the differential system on the torus $\mathbb T^2:$ $$\begin{cases}\frac{\partial}{\partial y}u-\frac{\partial}{\partial x}v=\sin(y)-\cos(x),\\\\ \frac{\partial}{\partial x} u+\frac{\partial}{\partial y}v=0,\end{cases}$$ with the conditions $$\int_0^{2\pi}\int_0^{2\pi}u(x,y)\mathrm d x\mathrm dy=\int_0^{2\pi}\int_0^{2\pi}v(x,y)\mathrm d x\mathrm dy=0.$$ In particular it seemed to me that this system was explicitly solvable and to do so i relied on the inhomogeneous Cauchy Riemann equations (swapping the coordinates $x\leftrightarrow y) $ and i basically followed this link . Unfortunately my calculations didn't lead nowhere.. I am asking two things.. Is that way followable to finish the problem, and if so can you help me in finishing the proof? More importantly: Are there smarter ways to do the problem? Thanks in advance.. -Guido- EDIT I've got the following question related to the previous post so I'm writing it as an edit to this question. The question is the following: prove that if $f$ is smooth, periodic and with zero average, then the solution to the system on $\mathbb T^2$ $$\begin{cases}\frac{\partial}{\partial y}u-\frac{\partial}{\partial x}v=0,\\\\ \frac{\partial}{\partial x} u+\frac{\partial}{\partial y}v=f(x,y),\end{cases}$$ satisfies $$\int_{0}^{2\pi}\int_0^{2\pi}\left(u(x,y)u'(x,y)+v(x,y)v'(x,y)\right)\mathrm dx\mathrm dy=0.$$ Thanks for your patience.","In a recent topic I've studied on complex analysis I had to study the differential system on the torus with the conditions In particular it seemed to me that this system was explicitly solvable and to do so i relied on the inhomogeneous Cauchy Riemann equations (swapping the coordinates and i basically followed this link . Unfortunately my calculations didn't lead nowhere.. I am asking two things.. Is that way followable to finish the problem, and if so can you help me in finishing the proof? More importantly: Are there smarter ways to do the problem? Thanks in advance.. -Guido- EDIT I've got the following question related to the previous post so I'm writing it as an edit to this question. The question is the following: prove that if is smooth, periodic and with zero average, then the solution to the system on satisfies Thanks for your patience.","\mathbb T^2: \begin{cases}\frac{\partial}{\partial y}u-\frac{\partial}{\partial x}v=\sin(y)-\cos(x),\\\\ \frac{\partial}{\partial x} u+\frac{\partial}{\partial y}v=0,\end{cases} \int_0^{2\pi}\int_0^{2\pi}u(x,y)\mathrm d x\mathrm dy=\int_0^{2\pi}\int_0^{2\pi}v(x,y)\mathrm d x\mathrm dy=0. x\leftrightarrow y)  f \mathbb T^2 \begin{cases}\frac{\partial}{\partial y}u-\frac{\partial}{\partial x}v=0,\\\\ \frac{\partial}{\partial x} u+\frac{\partial}{\partial y}v=f(x,y),\end{cases} \int_{0}^{2\pi}\int_0^{2\pi}\left(u(x,y)u'(x,y)+v(x,y)v'(x,y)\right)\mathrm dx\mathrm dy=0.","['complex-analysis', 'ordinary-differential-equations']"
15,Trouble with representing power series as polynomials.,Trouble with representing power series as polynomials.,,"I am a math student trying to wrap my head around complex analysis through self-study. I am using Complex Analysis by Serge Lang, but I find myself struggling with some of his power series manipulations and his representations of power series as polynomials. For instance, given the following theorem/proof (taken from Theorem 6.1, p.76): Theorem Let $f(T) = a_1 T + higher$ $terms$ be a formal power series with $a_1 \not= 0$. Then there exists a unique power series $g(T)$ such that $f(g(T)) = T$. This power series also satisfies $g(f(T)) = T$. Proof : For convenience of notation we write $f(T)$ in the form $$f(T) = a_1T - \sum_{2}^{\infty} a_nT^n$$ We seek a power series $$g(T) = \sum_{1}^{\infty} b_nT^n$$ such that $$f(g(T)) = T$$ The solution to this problem is given by solving the equation in terms of the coefficients of the power series $$a_1g(T) - a_2g(T)^2 - ... = T$$ These equations are of the form $$a_1b_n - P_n(a_2, ..., a_n, b_1,..., b_{n - 1}) = 0 \quad \text{and} \quad a_1b_1 = 1 \quad \text{for} \quad n = 1$$ where $P_n$ is a polynomial with positive integer coefficients (generalized binomial coefficients) ... I can follow this all right up until the polynomial representation is used. What I would like to do is follow this type of argument with greater ease. What readings and/or exercises should I do accomplish this?","I am a math student trying to wrap my head around complex analysis through self-study. I am using Complex Analysis by Serge Lang, but I find myself struggling with some of his power series manipulations and his representations of power series as polynomials. For instance, given the following theorem/proof (taken from Theorem 6.1, p.76): Theorem Let $f(T) = a_1 T + higher$ $terms$ be a formal power series with $a_1 \not= 0$. Then there exists a unique power series $g(T)$ such that $f(g(T)) = T$. This power series also satisfies $g(f(T)) = T$. Proof : For convenience of notation we write $f(T)$ in the form $$f(T) = a_1T - \sum_{2}^{\infty} a_nT^n$$ We seek a power series $$g(T) = \sum_{1}^{\infty} b_nT^n$$ such that $$f(g(T)) = T$$ The solution to this problem is given by solving the equation in terms of the coefficients of the power series $$a_1g(T) - a_2g(T)^2 - ... = T$$ These equations are of the form $$a_1b_n - P_n(a_2, ..., a_n, b_1,..., b_{n - 1}) = 0 \quad \text{and} \quad a_1b_1 = 1 \quad \text{for} \quad n = 1$$ where $P_n$ is a polynomial with positive integer coefficients (generalized binomial coefficients) ... I can follow this all right up until the polynomial representation is used. What I would like to do is follow this type of argument with greater ease. What readings and/or exercises should I do accomplish this?",,"['complex-analysis', 'soft-question', 'polynomials', 'power-series']"
16,Holomorphic function in an annulus,Holomorphic function in an annulus,,"I'm trying to do a question but I have a doubt on holomorphic functions, here is the problem. Let $A = \{z ∈ \mathbb{C} : \frac{1}{R}< |z| < R\}$. Suppose that $f : A → \mathbb{C}$ is holomorphic and   that $|f(z)| = 1 $ if $|z| = 1$.    Show that $f(z) = {\left(\overline{f(\bar{z}^{−1})}\right)}^{-1}$ when $ |z| = 1 $ and deduce that this holds for all $z ∈ A$. Now, my problem is with the final deduction... I think I need something about the zeroes of f as the right-hand side of the equality could not be defined at some point.  Am I right or is there a way to prove that a function with these properties can never be zero in A? Thank you all.","I'm trying to do a question but I have a doubt on holomorphic functions, here is the problem. Let $A = \{z ∈ \mathbb{C} : \frac{1}{R}< |z| < R\}$. Suppose that $f : A → \mathbb{C}$ is holomorphic and   that $|f(z)| = 1 $ if $|z| = 1$.    Show that $f(z) = {\left(\overline{f(\bar{z}^{−1})}\right)}^{-1}$ when $ |z| = 1 $ and deduce that this holds for all $z ∈ A$. Now, my problem is with the final deduction... I think I need something about the zeroes of f as the right-hand side of the equality could not be defined at some point.  Am I right or is there a way to prove that a function with these properties can never be zero in A? Thank you all.",,['complex-analysis']
17,complex function with real values on the real interval,complex function with real values on the real interval,,"let $ B(0,1) = \{ z\in \mathbb{C} | |z|<1\} $ and $ f $ be an holomorphic function on $ B(0,1) $ such that $ f(z)\in\mathbb{R} \iff z\in\mathbb{R} $ Prove: $ f $ has at most 1 root in $ B(0,1) $ i think this exercise requires rouche theorem or the argument principle theorem but i cant see how to use it",let and be an holomorphic function on such that Prove: has at most 1 root in i think this exercise requires rouche theorem or the argument principle theorem but i cant see how to use it," B(0,1) = \{ z\in \mathbb{C} | |z|<1\}   f   B(0,1)   f(z)\in\mathbb{R} \iff z\in\mathbb{R}   f   B(0,1) ",['complex-analysis']
18,number of roots between $1<|z|<2$,number of roots between,1<|z|<2,"let $f(z)=z^9+z^5+8z^3+2z+1$, well, $$|f(z)-z^9|=|z^5+8z^3+2z+1|<|2|^9\text{ at } |z|=2$$ so $f$ has $9$ zeroes inside $|z|<2$ and $$|f(z)-8z^3|=|z^9+z^5+2z+1|<5<8|z|^3\text{ at } |z|=1$$ so inside $|z|<1$ $f$ has 3 zeroes, so $6$ zeroes are there inside $1<z<2$, could any one tell me whether I have properly applied Rouché’s theorem to this problem? Thank you for reply.","let $f(z)=z^9+z^5+8z^3+2z+1$, well, $$|f(z)-z^9|=|z^5+8z^3+2z+1|<|2|^9\text{ at } |z|=2$$ so $f$ has $9$ zeroes inside $|z|<2$ and $$|f(z)-8z^3|=|z^9+z^5+2z+1|<5<8|z|^3\text{ at } |z|=1$$ so inside $|z|<1$ $f$ has 3 zeroes, so $6$ zeroes are there inside $1<z<2$, could any one tell me whether I have properly applied Rouché’s theorem to this problem? Thank you for reply.",,['complex-analysis']
19,Proving an elementary integral inequality (in early Dirichlet space material),Proving an elementary integral inequality (in early Dirichlet space material),,"I'm reading up on Dirichlet spaces using this document here and, on page two, am stumped by a particular integral inequality. If this is trivial and/or I'm missing something blatant, I apologize. First we define, for any analytic function $f$ defined on the unit disk $\mathbb{D}$, the quantity $$ D(f) = \frac{1}{\pi} \int_{\mathbb{D}} | f' |^2 dA =  \frac{1}{\pi} \int_{\mathbb{D}} | f' |^2 dxdy $$ (The Dirichlet space is the set of all functions $f$ as before such that $D(f)<\infty$. $D(f)$ is only a semi-norm, as $D(c)=0$ for any constant $c$.) Next we define, for any $\zeta \in \partial \mathbb{D}$, the quantity $$ L(f,\zeta) = \int_0^1 |f'(r \zeta)|dr $$ The author now says If $D(f)<\infty$ we can apply the Cauchy-Schwarz inequality to show that   $$ \int_0^{\pi} L(f,e^{i\theta} ) d\theta \leq c D(f) < \infty $$   and so $L(f,e^{i\theta})<\infty$ for almost all $\theta$. This is what I can not show. Attempt: The C-S inequality reads $$ \left| \int h(x) \overline{g(x)} dx \right|^2 \leq \int |h(x)|^2dx \int |g(x)|^2dx  $$ and applies because, by above, all functions in the Dirichlet space are square integrable.  We are trying to show $$ \int_0^{\pi} \int_0^1 |f'(r e^{i\theta})|dr d\theta \leq c \frac{1}{\pi} \int_{\mathbb{D}} | f' |^2 dxdy $$ which, when we transfer to polars, gives $$ \int_0^{2\pi} \int_0^1 |f'(r e^{i\theta})|dr d\theta \leq A  \int_0^{2\pi} \int_0^1 |f'(re^{i\theta})|^2rdrd\theta $$ So I defined $h(r) = |f'(r e^{i\theta})| \sqrt{r}$ to match C-S with what we have on the right, and then attempted so pick $g$ so that the left hand sides would match, too. This gives $g(r) = r^{-1/2}$. Subbing all this in we get $$ \left| \int_0^1 |f'(r e^{i\theta})| \sqrt{r} \frac{1}{\sqrt{r}}  dx \right|^2 \leq  \int_0^1 |f'(r e^{i\theta})|^2  rdr \int_0^1 |r^{-1/2}|^2dx  $$ $$ \left| \int_0^1 |f'(r e^{i\theta})| dx \right|^2 \leq  \int_0^1 |f'(r e^{i\theta})|^2  rdr \int_0^1 r^{-1}dr  $$ When we integrate over $\theta$ we kind of get what we're looking for (the square shouldn't matter as everything on the right above is finite and all we're trying to show is that the left is finite). The problem is the term $$ \int_0^1 r^{-1}dr = + \infty $$ Any thoughts? Apologies again if this is silly.","I'm reading up on Dirichlet spaces using this document here and, on page two, am stumped by a particular integral inequality. If this is trivial and/or I'm missing something blatant, I apologize. First we define, for any analytic function $f$ defined on the unit disk $\mathbb{D}$, the quantity $$ D(f) = \frac{1}{\pi} \int_{\mathbb{D}} | f' |^2 dA =  \frac{1}{\pi} \int_{\mathbb{D}} | f' |^2 dxdy $$ (The Dirichlet space is the set of all functions $f$ as before such that $D(f)<\infty$. $D(f)$ is only a semi-norm, as $D(c)=0$ for any constant $c$.) Next we define, for any $\zeta \in \partial \mathbb{D}$, the quantity $$ L(f,\zeta) = \int_0^1 |f'(r \zeta)|dr $$ The author now says If $D(f)<\infty$ we can apply the Cauchy-Schwarz inequality to show that   $$ \int_0^{\pi} L(f,e^{i\theta} ) d\theta \leq c D(f) < \infty $$   and so $L(f,e^{i\theta})<\infty$ for almost all $\theta$. This is what I can not show. Attempt: The C-S inequality reads $$ \left| \int h(x) \overline{g(x)} dx \right|^2 \leq \int |h(x)|^2dx \int |g(x)|^2dx  $$ and applies because, by above, all functions in the Dirichlet space are square integrable.  We are trying to show $$ \int_0^{\pi} \int_0^1 |f'(r e^{i\theta})|dr d\theta \leq c \frac{1}{\pi} \int_{\mathbb{D}} | f' |^2 dxdy $$ which, when we transfer to polars, gives $$ \int_0^{2\pi} \int_0^1 |f'(r e^{i\theta})|dr d\theta \leq A  \int_0^{2\pi} \int_0^1 |f'(re^{i\theta})|^2rdrd\theta $$ So I defined $h(r) = |f'(r e^{i\theta})| \sqrt{r}$ to match C-S with what we have on the right, and then attempted so pick $g$ so that the left hand sides would match, too. This gives $g(r) = r^{-1/2}$. Subbing all this in we get $$ \left| \int_0^1 |f'(r e^{i\theta})| \sqrt{r} \frac{1}{\sqrt{r}}  dx \right|^2 \leq  \int_0^1 |f'(r e^{i\theta})|^2  rdr \int_0^1 |r^{-1/2}|^2dx  $$ $$ \left| \int_0^1 |f'(r e^{i\theta})| dx \right|^2 \leq  \int_0^1 |f'(r e^{i\theta})|^2  rdr \int_0^1 r^{-1}dr  $$ When we integrate over $\theta$ we kind of get what we're looking for (the square shouldn't matter as everything on the right above is finite and all we're trying to show is that the left is finite). The problem is the term $$ \int_0^1 r^{-1}dr = + \infty $$ Any thoughts? Apologies again if this is silly.",,"['complex-analysis', 'integration']"
20,Boundary values of harmonic $u$ are $ u(e^{it}) = 5- 4 \cos t $; find $u(1/2)$ and $v(1/2)$.,Boundary values of harmonic  are ; find  and .,u  u(e^{it}) = 5- 4 \cos t  u(1/2) v(1/2),"My problem is the following: Let $u$ be a continuous real-valued function in the closure of the unit disk $\mathbb{D}$ that is harmonic in $\mathbb{D}$. Assume that the boundary values of $u$ are given by  $$ u(e^{it}) = 5- 4 \cos t. $$ Furthermore, let $v$ be a harmonic conjugate of $u$ in $\mathbb{D}$ such that $v(0) = 1$. Find $u(1/2)$ and $v(1/2)$. It's easy to find $u(1/2)$ using the Poisson integral formula: $$u(z) = \frac{1}{2\pi} \int_0 ^{2\pi} \frac{1-|z|^2}{|e^{i\theta}-z|^2} u(e^{i\theta}) d\theta$$ yields $$u(1/2) = \frac{1}{2 \pi} \int_0^{2\pi} \frac{3/4}{5/4-\cos \theta} (5-4\cos \theta) d\theta = \frac{1}{2\pi} \int_0^{2\pi} 3 d\theta = 3.$$ I get stuck trying to find the value for $v$. I know that  $$ 1 = v(0) = \frac{1}{2\pi} \int_0^{2\pi} v(e^{i\theta}) d\theta.$$ Also, $$ v(1/2) = \frac{1}{2\pi} \int_0^{2\pi} \frac{3/4}{5/4 - \cos \theta} v(e^{i\theta}) d\theta = \frac{1}{2\pi} \int_0^{2\pi} \frac{3v(e^{i\theta})}{u(e^{i\theta})} d\theta, $$ and in general, the harmonic conjugate is given by the line integral $$ v(z) = \mathcal{Im} \int_0^z f'(w) dw + C.$$ I don't know how to proceed with this information to find the $v$-value.","My problem is the following: Let $u$ be a continuous real-valued function in the closure of the unit disk $\mathbb{D}$ that is harmonic in $\mathbb{D}$. Assume that the boundary values of $u$ are given by  $$ u(e^{it}) = 5- 4 \cos t. $$ Furthermore, let $v$ be a harmonic conjugate of $u$ in $\mathbb{D}$ such that $v(0) = 1$. Find $u(1/2)$ and $v(1/2)$. It's easy to find $u(1/2)$ using the Poisson integral formula: $$u(z) = \frac{1}{2\pi} \int_0 ^{2\pi} \frac{1-|z|^2}{|e^{i\theta}-z|^2} u(e^{i\theta}) d\theta$$ yields $$u(1/2) = \frac{1}{2 \pi} \int_0^{2\pi} \frac{3/4}{5/4-\cos \theta} (5-4\cos \theta) d\theta = \frac{1}{2\pi} \int_0^{2\pi} 3 d\theta = 3.$$ I get stuck trying to find the value for $v$. I know that  $$ 1 = v(0) = \frac{1}{2\pi} \int_0^{2\pi} v(e^{i\theta}) d\theta.$$ Also, $$ v(1/2) = \frac{1}{2\pi} \int_0^{2\pi} \frac{3/4}{5/4 - \cos \theta} v(e^{i\theta}) d\theta = \frac{1}{2\pi} \int_0^{2\pi} \frac{3v(e^{i\theta})}{u(e^{i\theta})} d\theta, $$ and in general, the harmonic conjugate is given by the line integral $$ v(z) = \mathcal{Im} \int_0^z f'(w) dw + C.$$ I don't know how to proceed with this information to find the $v$-value.",,"['complex-analysis', 'harmonic-functions']"
21,Composition of Analytic Functions,Composition of Analytic Functions,,"I have a basic question in my mind and wish to consult your ideas: Suppose $\Omega_1$ and $\Omega_2$ are regions, $f$ and $g$ are nonconstant functions defined in $\Omega_1$ and $\Omega_2$,  respectively, and $f(\Omega_1) \subset \Omega_2$. Define $h=g \circ f$. What can we say about the third function if (a) both $g$ and $f$ are analytic; (b) both $g$ and $h$ are analytic; (c) both $h$ and $f$ are analytic. Here I consider all possible cases. I think in part (a) $h$ is analytic being  the composition of two differentiable functions. Actually to my mind, analyticity of $g$  implies analyticity of $h$, am I correct ? Otherwise, I can't find counterexamples on each cases. What is your suggestion? Thank you.","I have a basic question in my mind and wish to consult your ideas: Suppose $\Omega_1$ and $\Omega_2$ are regions, $f$ and $g$ are nonconstant functions defined in $\Omega_1$ and $\Omega_2$,  respectively, and $f(\Omega_1) \subset \Omega_2$. Define $h=g \circ f$. What can we say about the third function if (a) both $g$ and $f$ are analytic; (b) both $g$ and $h$ are analytic; (c) both $h$ and $f$ are analytic. Here I consider all possible cases. I think in part (a) $h$ is analytic being  the composition of two differentiable functions. Actually to my mind, analyticity of $g$  implies analyticity of $h$, am I correct ? Otherwise, I can't find counterexamples on each cases. What is your suggestion? Thank you.",,['complex-analysis']
22,Can a Herglotz-Nevanlinna function attain real values?,Can a Herglotz-Nevanlinna function attain real values?,,"Let $\mathbb{H}^+=\{z \in \mathbb{C}\mid \Im(z)>0\}$. We say that an analytic $F\colon \mathbb{H}^+\to\overline{\mathbb{H}^+}$ is a Herglotz-Nevanlinna 's function. Question Can it be that $F(z)\in \mathbb{R}$ for some $z \in \mathbb{H}^+$? I guess that the answer is no, because if this happened then we could find a small loop $\gamma$ around $z$ such that $F\circ \gamma$ slips outside $\overline{\mathbb{H}^+}$, but I'm not sure this is true and how to formalize this little argument. Thank you.","Let $\mathbb{H}^+=\{z \in \mathbb{C}\mid \Im(z)>0\}$. We say that an analytic $F\colon \mathbb{H}^+\to\overline{\mathbb{H}^+}$ is a Herglotz-Nevanlinna 's function. Question Can it be that $F(z)\in \mathbb{R}$ for some $z \in \mathbb{H}^+$? I guess that the answer is no, because if this happened then we could find a small loop $\gamma$ around $z$ such that $F\circ \gamma$ slips outside $\overline{\mathbb{H}^+}$, but I'm not sure this is true and how to formalize this little argument. Thank you.",,['complex-analysis']
23,Show that any closed polygonal path can be decomposed into a finite union of simple closed polygonal paths,Show that any closed polygonal path can be decomposed into a finite union of simple closed polygonal paths,,"Show that any closed polygonal path can be decomposed into a finite union of simple closed polygonal paths and line segments traversed twice in opposite directions. MY SOLUTION Suppose $\gamma(t):a\leq t\leq b$ has $\gamma (t_{2}) = \gamma (t_{1})$. Then $\gamma$ can be written as a union of $\gamma_{1}$ and $\gamma_{2}$ where $\gamma_{1}=\gamma(t); t\in[a,t_{1}]\bigcup[t_{2},b]$ and $\gamma_{2}=\gamma(t)$; $t\in[t_{1}, t_{2}]$. Is correct my method?","Show that any closed polygonal path can be decomposed into a finite union of simple closed polygonal paths and line segments traversed twice in opposite directions. MY SOLUTION Suppose $\gamma(t):a\leq t\leq b$ has $\gamma (t_{2}) = \gamma (t_{1})$. Then $\gamma$ can be written as a union of $\gamma_{1}$ and $\gamma_{2}$ where $\gamma_{1}=\gamma(t); t\in[a,t_{1}]\bigcup[t_{2},b]$ and $\gamma_{2}=\gamma(t)$; $t\in[t_{1}, t_{2}]$. Is correct my method?",,"['general-topology', 'complex-analysis', 'geometry']"
24,"When expressing area of $f(D)$ using Jacobian, why exactly must $f$ be one-to-one?","When expressing area of  using Jacobian, why exactly must  be one-to-one?",f(D) f,"I was working on a question very similar to this one: Expressing the area of the image of a holomorphic function by the coefficients of its expansion Clearly the key lies in the formula  $$\iint_{f(D)}dxdy=\iint_D\operatorname{Jac}(f)\,dx\,dy,$$ which turns out to be $$\iint_D|f'|^2\,dx\,dy$$ for holomorphic functions (using Cauchy-Riemann equations). But it has been pointed out that this is only true for one-to-one functions. Intuitively this makes sense to me since the integral would sum the same area more than once if it were not one-to-one. But then... Is it correct to generalize this formula to functions which are not one-to-one by saying   $$\iint_{f(D)}dxdy\leq \iint_D|f'|^2\,dx\,dy$$   with equality when $f$ is one-to-one? Would equality also hold if the set of points in the image which have more than one pre-image is non-empty but has measure zero? Or can stranger things happen that I have not considered? If my question makes more sense with $\operatorname{Jac}(f)$ in place of $|f'|^2$, please let me know, and feel free to use the former instead. Thanks.","I was working on a question very similar to this one: Expressing the area of the image of a holomorphic function by the coefficients of its expansion Clearly the key lies in the formula  $$\iint_{f(D)}dxdy=\iint_D\operatorname{Jac}(f)\,dx\,dy,$$ which turns out to be $$\iint_D|f'|^2\,dx\,dy$$ for holomorphic functions (using Cauchy-Riemann equations). But it has been pointed out that this is only true for one-to-one functions. Intuitively this makes sense to me since the integral would sum the same area more than once if it were not one-to-one. But then... Is it correct to generalize this formula to functions which are not one-to-one by saying   $$\iint_{f(D)}dxdy\leq \iint_D|f'|^2\,dx\,dy$$   with equality when $f$ is one-to-one? Would equality also hold if the set of points in the image which have more than one pre-image is non-empty but has measure zero? Or can stranger things happen that I have not considered? If my question makes more sense with $\operatorname{Jac}(f)$ in place of $|f'|^2$, please let me know, and feel free to use the former instead. Thanks.",,"['analysis', 'complex-analysis']"
25,An application of Runge's theorem,An application of Runge's theorem,,"I have been thinking about the following exercise from some complex analysis lecture notes I found online for a while but I can't seem to be able to conclude the desired result. The exercise says that If $A$ is an open subset of the complex plane, and if $g: A \rightarrow \mathbb{C}$ is analytic, then there is a sequence of rational functions $(f_n)$ such that no pole of the $f_n$'s lies in $A$ (that is, that the poles are in $\overline{\mathbb{C}} \setminus A$) and such that $f_n \rightarrow g$ uniformly on compact subsets of $A$. I know that I have to apply Runge's theorem here, but the problem is that Runge's theorem tells me that the sequence of functions $f_n$ converges uniformly to $g$ on a compact set $K \subseteq A$ and not on the whole $A$. I thought that maybe I can approximate the open set $A$ by compact sets $K_n$ and take elements from a sequence $f_{n,k}$ that converges uniformly on each of the compacts, and then any given compact $K\subset A$ would have to be contained in one of the compacts  $K_n$ approximating $A$ but I'm not sure if this is possible of if it this works. I would appreciate any help with this exercise. Thanks.","I have been thinking about the following exercise from some complex analysis lecture notes I found online for a while but I can't seem to be able to conclude the desired result. The exercise says that If $A$ is an open subset of the complex plane, and if $g: A \rightarrow \mathbb{C}$ is analytic, then there is a sequence of rational functions $(f_n)$ such that no pole of the $f_n$'s lies in $A$ (that is, that the poles are in $\overline{\mathbb{C}} \setminus A$) and such that $f_n \rightarrow g$ uniformly on compact subsets of $A$. I know that I have to apply Runge's theorem here, but the problem is that Runge's theorem tells me that the sequence of functions $f_n$ converges uniformly to $g$ on a compact set $K \subseteq A$ and not on the whole $A$. I thought that maybe I can approximate the open set $A$ by compact sets $K_n$ and take elements from a sequence $f_{n,k}$ that converges uniformly on each of the compacts, and then any given compact $K\subset A$ would have to be contained in one of the compacts  $K_n$ approximating $A$ but I'm not sure if this is possible of if it this works. I would appreciate any help with this exercise. Thanks.",,['complex-analysis']
26,Inequality for holomorphic functions,Inequality for holomorphic functions,,"Let $f = u+iv$ be a holomorphic function on the unit disk $D\subset \mathbb C$ such that $f(0)=0$. I am trying to prove that for each positive integer $k$, there is a constant $c_k$, independent of $r$, such that for all $r < 1$, $\int_0 ^{2\pi} |v(re^{i\theta})|^{2k} d\theta \leq c_k \int_0 ^{2\pi} |u(re^{i\theta})|^{2k} d\theta$. So far, I have tried using the fact that $|u|$ and $|v|$ are subharmonic, but this does not seem to produce the desired estimates. I have also tried some manipulations using the Cauchy-Riemann equations and the mean value property, also to no avail. Any suggestions?","Let $f = u+iv$ be a holomorphic function on the unit disk $D\subset \mathbb C$ such that $f(0)=0$. I am trying to prove that for each positive integer $k$, there is a constant $c_k$, independent of $r$, such that for all $r < 1$, $\int_0 ^{2\pi} |v(re^{i\theta})|^{2k} d\theta \leq c_k \int_0 ^{2\pi} |u(re^{i\theta})|^{2k} d\theta$. So far, I have tried using the fact that $|u|$ and $|v|$ are subharmonic, but this does not seem to produce the desired estimates. I have also tried some manipulations using the Cauchy-Riemann equations and the mean value property, also to no avail. Any suggestions?",,['complex-analysis']
27,Use Cauchy Riemann to prove this function is differentiable at all points,Use Cauchy Riemann to prove this function is differentiable at all points,,I expanded it out and got $e^{4z+1} = e^{4x+1}\cos{4y} + e^{4x+1}\sin{4y}$ Then my CR equations were - $U_x = (4x+1)e^{4x+1}(4)(\cos 4y)$ $U_y = -e^{4x+1}(\sin4y)$ $V_x = (4x+1)e^{4x+1}(4)(\sin 4y)$ $V_y = e^{4x+1}(\cos 4y)$ Taking $U_x = V_y$ I get $(4x+1)e^{4x+1}(4)(\cos 4y) = e^{4x+1}(\cos 4y)$ $(4)(4x+1) = 1$ But that can't be right as then it means the CR equations are only satisfied for a certain value of x. So what am I doing wrong?,I expanded it out and got $e^{4z+1} = e^{4x+1}\cos{4y} + e^{4x+1}\sin{4y}$ Then my CR equations were - $U_x = (4x+1)e^{4x+1}(4)(\cos 4y)$ $U_y = -e^{4x+1}(\sin4y)$ $V_x = (4x+1)e^{4x+1}(4)(\sin 4y)$ $V_y = e^{4x+1}(\cos 4y)$ Taking $U_x = V_y$ I get $(4x+1)e^{4x+1}(4)(\cos 4y) = e^{4x+1}(\cos 4y)$ $(4)(4x+1) = 1$ But that can't be right as then it means the CR equations are only satisfied for a certain value of x. So what am I doing wrong?,,['complex-analysis']
28,Choosing between a semicircular contour and a rectangular contour,Choosing between a semicircular contour and a rectangular contour,,"In a paper I came across ( page 10, section 7 ), the authors state that $\int_{-\infty}^{\infty} \frac{dx}{(b^{2}+x^{2})\cosh ax} $ can be evaluated by ""closing the real axis with a semi-circle centered at the origin located in the upper half-plane. An elementary estimate shows that the integral over the circular boundary vanishes as the radius goes to infinity."" But I don't think that the integral over the circular boundary is going to vanish if one simply lets the radius go to infinity in a continuous manner. Furthermore, wouldn't it be easier to use a rectangular contour?","In a paper I came across ( page 10, section 7 ), the authors state that $\int_{-\infty}^{\infty} \frac{dx}{(b^{2}+x^{2})\cosh ax} $ can be evaluated by ""closing the real axis with a semi-circle centered at the origin located in the upper half-plane. An elementary estimate shows that the integral over the circular boundary vanishes as the radius goes to infinity."" But I don't think that the integral over the circular boundary is going to vanish if one simply lets the radius go to infinity in a continuous manner. Furthermore, wouldn't it be easier to use a rectangular contour?",,"['complex-analysis', 'contour-integration']"
29,Contractive Operator and Realization Theorem,Contractive Operator and Realization Theorem,,"Good morning, I have searched, by using google for a time, a proof of the following theorem : Let $\pmatrix{A&B \\ C&D}\colon H \oplus K \to H\oplus K$ be a contractive operator of a Hilbert space where H and K are Hilbert subspaces. Then we have the function $f\colon z\in\mathbb{D}\mapsto D + Cz(1-zA)^{-1}B \in \mathcal{B}(K)$, from the open unit disc to the space of bounded operators on $K$, is a holomorphic function such that $\|f\|_{\infty} = \sup_{z\in\mathbb{D}} \|f(z)\|\leq 1$, where $\|f(z)\|$ is the operator norm. But I have not found yet. This theorem is called the realization theorem for functions of the Schur class. Does anyone have a proof of it? Thanks in advance. Duc Anh EDIT : in the case $\pmatrix{A&B \\ C&D}$ is unitary, everything is simple, so the difficult case is when it is a contractive operator in general.","Good morning, I have searched, by using google for a time, a proof of the following theorem : Let $\pmatrix{A&B \\ C&D}\colon H \oplus K \to H\oplus K$ be a contractive operator of a Hilbert space where H and K are Hilbert subspaces. Then we have the function $f\colon z\in\mathbb{D}\mapsto D + Cz(1-zA)^{-1}B \in \mathcal{B}(K)$, from the open unit disc to the space of bounded operators on $K$, is a holomorphic function such that $\|f\|_{\infty} = \sup_{z\in\mathbb{D}} \|f(z)\|\leq 1$, where $\|f(z)\|$ is the operator norm. But I have not found yet. This theorem is called the realization theorem for functions of the Schur class. Does anyone have a proof of it? Thanks in advance. Duc Anh EDIT : in the case $\pmatrix{A&B \\ C&D}$ is unitary, everything is simple, so the difficult case is when it is a contractive operator in general.",,"['complex-analysis', 'functional-analysis', 'hilbert-spaces']"
30,Represent $\mathbb{R}^3$ as an union of disjoint circles using stereographic projection,Represent  as an union of disjoint circles using stereographic projection,\mathbb{R}^3,I have begun to learn complex analysis and have solved a few problems on stereographic projection and Riemann sphere but can't solve the problem in the subject. Could you help please?,I have begun to learn complex analysis and have solved a few problems on stereographic projection and Riemann sphere but can't solve the problem in the subject. Could you help please?,,['complex-analysis']
31,Suggestions for a Global Analysis book,Suggestions for a Global Analysis book,,"can somebody tell me some good books or lecture notes in ""global analysis"" ?  I am a newcomer in this subject.  thanks in advance. greetings trito","can somebody tell me some good books or lecture notes in ""global analysis"" ?  I am a newcomer in this subject.  thanks in advance. greetings trito",,"['real-analysis', 'reference-request', 'complex-analysis', 'ordinary-differential-equations', 'differential-geometry']"
32,Constructing the Riemann Sphere,Constructing the Riemann Sphere,,"Problem: In the construction of the Riemann sphere, we begin with the sphere $\mathbb{S}^2$ with two charts: the stereographic projection $\sigma_N : \mathbb{S}^2 \setminus \{N\} \to \mathbb{R}^2  \cong \mathbb{C}$ from the North pole, $N$, given by $$ \sigma_N (x_1, x_2, x_3) := \frac{(x_1, x_2)}{1-x_3}, $$ the stereographic projection $\sigma_S : \mathbb{S}^2 \setminus \{S\}  \to \mathbb{R}^2 \cong \mathbb{C}$ from the South pole, $S$, given by $$ \sigma_S (x_1, x_2, x_3) := \frac{(x_1, x_2)}{1+x_3}. $$ Question: How does one show that the transition function of the two charts is: $$\sigma_1 \circ \sigma_0^{-1} (z) = z^{-1}$$ Remark: By elementary calculations we see that: $$\sigma_S(x_1,x_2,x_3)=\sigma_N(x_1,x_2,-x_3)$$ $$\sigma_N(x_1,x_2,x_3)=\sigma_S(x_1,x_2,-x_3)$$","Problem: In the construction of the Riemann sphere, we begin with the sphere $\mathbb{S}^2$ with two charts: the stereographic projection $\sigma_N : \mathbb{S}^2 \setminus \{N\} \to \mathbb{R}^2  \cong \mathbb{C}$ from the North pole, $N$, given by $$ \sigma_N (x_1, x_2, x_3) := \frac{(x_1, x_2)}{1-x_3}, $$ the stereographic projection $\sigma_S : \mathbb{S}^2 \setminus \{S\}  \to \mathbb{R}^2 \cong \mathbb{C}$ from the South pole, $S$, given by $$ \sigma_S (x_1, x_2, x_3) := \frac{(x_1, x_2)}{1+x_3}. $$ Question: How does one show that the transition function of the two charts is: $$\sigma_1 \circ \sigma_0^{-1} (z) = z^{-1}$$ Remark: By elementary calculations we see that: $$\sigma_S(x_1,x_2,x_3)=\sigma_N(x_1,x_2,-x_3)$$ $$\sigma_N(x_1,x_2,x_3)=\sigma_S(x_1,x_2,-x_3)$$",,"['complex-analysis', 'riemann-surfaces']"
33,Proving $(1+i)^{2n}=2^n\cos {\pi n \over 2}$ using binomial expansion,Proving  using binomial expansion,(1+i)^{2n}=2^n\cos {\pi n \over 2},"Use the binomial expansion of$(1+i)^{2n}$ to prove that $$\binom{2n}{0}-\binom{2n}{2}+\binom{2n}{4}-\binom{2n}{6}+...+(-1)^n\binom{2n}{2n}=2^n\cos {\pi n \over 2}, n\in \mathbb{Z^+}$$ I've been trying to solve this problem for quite some time now, but I am unable to make any progress. I first tried to solve it by using De Moivre's theorem which is easy, but not what the question is asking for. Since then I've basically been stuck.","Use the binomial expansion of$(1+i)^{2n}$ to prove that $$\binom{2n}{0}-\binom{2n}{2}+\binom{2n}{4}-\binom{2n}{6}+...+(-1)^n\binom{2n}{2n}=2^n\cos {\pi n \over 2}, n\in \mathbb{Z^+}$$ I've been trying to solve this problem for quite some time now, but I am unable to make any progress. I first tried to solve it by using De Moivre's theorem which is easy, but not what the question is asking for. Since then I've basically been stuck.",,"['complex-analysis', 'binomial-coefficients']"
34,A certain family of complex functions,A certain family of complex functions,,"Are there an uncountable linear order $(P, \leq)$  and a family of continuous complex-valued functions $\{f_p\colon p\in P\}$ defined on the interval $[0,1]$ such that for any $p<q$, $p,q\in P$ we have $f_q(x)=f_p(x)$ for each $x\in [0,1]$ with $f_p(x)\neq 0$?","Are there an uncountable linear order $(P, \leq)$  and a family of continuous complex-valued functions $\{f_p\colon p\in P\}$ defined on the interval $[0,1]$ such that for any $p<q$, $p,q\in P$ we have $f_q(x)=f_p(x)$ for each $x\in [0,1]$ with $f_p(x)\neq 0$?",,"['general-topology', 'complex-analysis', 'order-theory']"
35,Eisenstein series estimate from Stein-Shakarchi's Complex Analysis,Eisenstein series estimate from Stein-Shakarchi's Complex Analysis,,"I need help with this exercise from Stein-Shakarchi's Complex Analysis (page 280), Chapter 9 (An Introduction to Elliptic Functions), Exercise 8 Let $$E_{4}(\tau)=\sum_{(n,m)\neq (0,0)}{\frac{1}{(n+m\tau)^{4}}}$$ be the Eisenstein series of order $4$ . Show that $$\left|E_{4}(\tau)-\frac{\pi^{4}}{45}\right|\leq c e^{-2\pi t}\ \text{if}\ \tau = x + it\ \text{and}\ t \geq 1.$$ and deduce that $$\left|E_{4}(\tau)-\tau^{-4}\frac{\pi^{4}}{45}\right|\leq c t^{-4} e^{-2\pi/ t}\ \text{if}\ \tau = it\  \text{and}\  0 < t \leq 1.$$ My idea is to use the estimates from the proof of Theorem 2.5 of pp. 277, but I can't see a finish. Thanks in advance! EDIT: On page 277, we have the information that in general $$E_{k}(\tau)=\sum_{(n,m)\neq (0,0)}{\frac{1}{(n+m\tau)^{k}}} = 2 \zeta(k) + \frac{2 (-1)^{k/2} (2 \pi)^{k} }{(k-1)!} \sum_{m >0 } {\sum_{l=1}^{\infty}{l^{k-1}e^{2\pi i \tau m l }}}.$$","I need help with this exercise from Stein-Shakarchi's Complex Analysis (page 280), Chapter 9 (An Introduction to Elliptic Functions), Exercise 8 Let be the Eisenstein series of order . Show that and deduce that My idea is to use the estimates from the proof of Theorem 2.5 of pp. 277, but I can't see a finish. Thanks in advance! EDIT: On page 277, we have the information that in general","E_{4}(\tau)=\sum_{(n,m)\neq (0,0)}{\frac{1}{(n+m\tau)^{4}}} 4 \left|E_{4}(\tau)-\frac{\pi^{4}}{45}\right|\leq c e^{-2\pi t}\ \text{if}\ \tau = x + it\ \text{and}\ t \geq 1. \left|E_{4}(\tau)-\tau^{-4}\frac{\pi^{4}}{45}\right|\leq c t^{-4} e^{-2\pi/ t}\ \text{if}\ \tau = it\  \text{and}\  0 < t \leq 1. E_{k}(\tau)=\sum_{(n,m)\neq (0,0)}{\frac{1}{(n+m\tau)^{k}}} = 2 \zeta(k) + \frac{2 (-1)^{k/2} (2 \pi)^{k} }{(k-1)!} \sum_{m >0 } {\sum_{l=1}^{\infty}{l^{k-1}e^{2\pi i \tau m l }}}.","['complex-analysis', 'modular-forms']"
36,Intersection $f(G\cap \mathbf{R}) \subset \mathbf{R} \Leftrightarrow f(\bar{z}) = \overline{f(z)}$,Intersection,f(G\cap \mathbf{R}) \subset \mathbf{R} \Leftrightarrow f(\bar{z}) = \overline{f(z)},"$G\subset \mathbf{C} , G= \{z\in \mathbf{C}| \overline{z}\in G\} = \overline{G}$, then for $f\in \mathcal{O}(G)$ it holds that:$$f(G\cap \mathbf{R}) \subset \mathbf{R} \Leftrightarrow \forall z\in G : f(\overline{z}) = \overline{f(z)}$$ This is an example in the script of our professor, however there is no proof for it and it hasn't been shown during the lectures either. Proof   : $""\Rightarrow "" :$ Let $f(G\cap \mathbf{R}) \subset \mathbf{R}$, so all points on the real axis are mapped to the real axis. That means $f$ does not have any imaginary part, because then this would not be true anymore. Since f does not have any imaginary part. it follows also that : $f(\overline{z}) = \overline{f(z)}$ $""\Leftarrow"" :$ Let $f(\overline{z}) = \overline{f(z)}$, so f can not have any imaginary part, otherwise this would not be true anymore. But if f doesn't have any imaginary part, then the map of the real axis is always on the real axis. So $f(G\cap \mathbf{R}) \subset \mathbf{R}$ What I mean with not having any imaginary part is that it is of the form $f(z) = z ; f(z)=2z; f(z) = 2z+z^{3}$ etc. I am not sure if this is the right thought, and how to express this better. Help is greatly appreciated.","$G\subset \mathbf{C} , G= \{z\in \mathbf{C}| \overline{z}\in G\} = \overline{G}$, then for $f\in \mathcal{O}(G)$ it holds that:$$f(G\cap \mathbf{R}) \subset \mathbf{R} \Leftrightarrow \forall z\in G : f(\overline{z}) = \overline{f(z)}$$ This is an example in the script of our professor, however there is no proof for it and it hasn't been shown during the lectures either. Proof   : $""\Rightarrow "" :$ Let $f(G\cap \mathbf{R}) \subset \mathbf{R}$, so all points on the real axis are mapped to the real axis. That means $f$ does not have any imaginary part, because then this would not be true anymore. Since f does not have any imaginary part. it follows also that : $f(\overline{z}) = \overline{f(z)}$ $""\Leftarrow"" :$ Let $f(\overline{z}) = \overline{f(z)}$, so f can not have any imaginary part, otherwise this would not be true anymore. But if f doesn't have any imaginary part, then the map of the real axis is always on the real axis. So $f(G\cap \mathbf{R}) \subset \mathbf{R}$ What I mean with not having any imaginary part is that it is of the form $f(z) = z ; f(z)=2z; f(z) = 2z+z^{3}$ etc. I am not sure if this is the right thought, and how to express this better. Help is greatly appreciated.",,['complex-analysis']
37,Lambert series expansion identity,Lambert series expansion identity,,I have a question which goes like this: How can I show that $$\sum_{n=1}^{\infty} \frac{z^n}{\left(1-z^n\right)^2} =\sum_{n=1}^\infty \frac{nz^n}{1-z^n}$$ for $|z|<1$?,I have a question which goes like this: How can I show that $$\sum_{n=1}^{\infty} \frac{z^n}{\left(1-z^n\right)^2} =\sum_{n=1}^\infty \frac{nz^n}{1-z^n}$$ for $|z|<1$?,,"['sequences-and-series', 'complex-analysis']"
38,Conditions on $X$ ensuring that a non-constant continuous function $f: X\to \mathbb{C}$ exists,Conditions on  ensuring that a non-constant continuous function  exists,X f: X\to \mathbb{C},"Let $X$ be a compact Hausdorff space. I would like to find a (minimal) condition on $X$, which is sufficient to guarantee that one can always find a non-constant continuous function $f : X \to \mathbb{C}$. Clearly we need to assume that $|X|>1$, but what else can we say?","Let $X$ be a compact Hausdorff space. I would like to find a (minimal) condition on $X$, which is sufficient to guarantee that one can always find a non-constant continuous function $f : X \to \mathbb{C}$. Clearly we need to assume that $|X|>1$, but what else can we say?",,"['general-topology', 'analysis', 'complex-analysis']"
39,A proof for the chain rule for paths,A proof for the chain rule for paths,,"Let $\gamma: I\rightarrow G$ be a differentiable path and $f:G\rightarrow \mathbb{C}$ a real differentiable function. It is to show, that for the path: $f\circ \gamma: I\rightarrow \mathbb{C}$ the following formula is true: $\displaystyle{ (f\ \circ \gamma)'(t) = f_{z}(\gamma(t))\gamma'(t)+f_{\overline{z}}(\gamma(t))\overline{\gamma ' (t)}}$ and to conclude that $(f\circ \gamma)'(t)= f'(\gamma(t))\gamma'(t)$. I began like this: let $f=u+iv$, it is :  $\displaystyle{f_{x}= u_{x}+iv_{x}, f_{y}=u_{y}+iv_{y}, f_{z}=\frac{1}{2}(f_{x}-if_{y}), f_{z}=\frac{1}{2}(f_{x}-if_{y}), f_{\overline{z}}=\frac{1}{2}(f_{x}+if_{y})}$ then: $$\begin{align} f \circ \gamma &= f(\gamma(t))= u(x(t),y(t))+iv(x(t),y(t)) \\ (f\ \circ \gamma)'(t) &= u_{x}x'(t)+u_{y}y'(t)+i(v_{x}x'(t)+v_{y}y'(t))\\ &= (u_{x}x'(t)+iv_{x}x'(t))+(u_{y}y'(t)+iv_{y}y'(t))\\ &=f_{x}x'(t) + f_{y}y'(t)  \end{align}$$ stuck . I don't think this is right so far because I don't see $\overline{\gamma '(t)}$, which should appear. Does anybody see the right way? Please do tell.","Let $\gamma: I\rightarrow G$ be a differentiable path and $f:G\rightarrow \mathbb{C}$ a real differentiable function. It is to show, that for the path: $f\circ \gamma: I\rightarrow \mathbb{C}$ the following formula is true: $\displaystyle{ (f\ \circ \gamma)'(t) = f_{z}(\gamma(t))\gamma'(t)+f_{\overline{z}}(\gamma(t))\overline{\gamma ' (t)}}$ and to conclude that $(f\circ \gamma)'(t)= f'(\gamma(t))\gamma'(t)$. I began like this: let $f=u+iv$, it is :  $\displaystyle{f_{x}= u_{x}+iv_{x}, f_{y}=u_{y}+iv_{y}, f_{z}=\frac{1}{2}(f_{x}-if_{y}), f_{z}=\frac{1}{2}(f_{x}-if_{y}), f_{\overline{z}}=\frac{1}{2}(f_{x}+if_{y})}$ then: $$\begin{align} f \circ \gamma &= f(\gamma(t))= u(x(t),y(t))+iv(x(t),y(t)) \\ (f\ \circ \gamma)'(t) &= u_{x}x'(t)+u_{y}y'(t)+i(v_{x}x'(t)+v_{y}y'(t))\\ &= (u_{x}x'(t)+iv_{x}x'(t))+(u_{y}y'(t)+iv_{y}y'(t))\\ &=f_{x}x'(t) + f_{y}y'(t)  \end{align}$$ stuck . I don't think this is right so far because I don't see $\overline{\gamma '(t)}$, which should appear. Does anybody see the right way? Please do tell.",,['complex-analysis']
40,First derivative bounded by supremum of difference of values in disc,First derivative bounded by supremum of difference of values in disc,,"Need a little help in the following: Let $f(z)$ analytic function on $D = \{z\in\mathbb C: |z| < 1\}$. Define $\displaystyle d = \sup_{z,w \in D} |f(z) - f(w)|$. Prove that $|f'(0)| \leq \frac{d}{2}$.","Need a little help in the following: Let $f(z)$ analytic function on $D = \{z\in\mathbb C: |z| < 1\}$. Define $\displaystyle d = \sup_{z,w \in D} |f(z) - f(w)|$. Prove that $|f'(0)| \leq \frac{d}{2}$.",,['complex-analysis']
41,pole on radius of convergence - no absolute convergence on that circle,pole on radius of convergence - no absolute convergence on that circle,,"I am confronted with the following problem: The radius of convergence of a function $f(z)=\sum c_n(z-z_0)^n$ is $R$, and the function has a pole at some $w_0$, with $|w_0-z_0|=R$. Why does, for any other $w$ on the circle with radius $R$, the series not converge absolutely? I tried to use the MVT, but that does not work out. Can anyone give me a sketch of the proof? best, MP","I am confronted with the following problem: The radius of convergence of a function $f(z)=\sum c_n(z-z_0)^n$ is $R$, and the function has a pole at some $w_0$, with $|w_0-z_0|=R$. Why does, for any other $w$ on the circle with radius $R$, the series not converge absolutely? I tried to use the MVT, but that does not work out. Can anyone give me a sketch of the proof? best, MP",,"['sequences-and-series', 'complex-analysis']"
42,Question concerning proof of the Cauchy Integral Formula on the Wolfram website,Question concerning proof of the Cauchy Integral Formula on the Wolfram website,,"I was reading through the proof of the Cauchy Integral Formula here . I do not understand  how the transition is made from equation (8) to equation (9). While taking the limit as $r\to 0$, doesn't the closed curve $\gamma_r$ also vanish? So, by then the closed curve $\gamma_r$ around $z_0$ is degenerate(a point), I think. Can you please explain what is going on? Thank you.","I was reading through the proof of the Cauchy Integral Formula here . I do not understand  how the transition is made from equation (8) to equation (9). While taking the limit as $r\to 0$, doesn't the closed curve $\gamma_r$ also vanish? So, by then the closed curve $\gamma_r$ around $z_0$ is degenerate(a point), I think. Can you please explain what is going on? Thank you.",,"['complex-analysis', 'limits']"
43,"How does one move a point in $B(0,1)$ to the origin with a Möbius transformation",How does one move a point in  to the origin with a Möbius transformation,"B(0,1)","Let $z_0$ be in the open unit disc $B(0,1)\subset \mathbf{C}$. Is there a general formula for an automorphism of $B(0,1)$ which sends $z_0$ to the origin? I find it easier to think about the complex upper half plane $\mathcal{H}$ so I guess one could do the following. Map $B(0,1)$ bijectively to $\mathcal{H}$ via $$\varphi:z\mapsto \frac{z+1}{iz+1}.$$ Let $\tau_0$ be the image of $z_0$ under this isomorphism. Find a Möbius transformation $\mu$ sending $\tau_0$ to $i$ and define $$f = \varphi^{-1} \circ \mu \circ\varphi.$$ This is the automorphism we're looking for. The only problem is finding $\mu$. How can I do this explicitly?","Let $z_0$ be in the open unit disc $B(0,1)\subset \mathbf{C}$. Is there a general formula for an automorphism of $B(0,1)$ which sends $z_0$ to the origin? I find it easier to think about the complex upper half plane $\mathcal{H}$ so I guess one could do the following. Map $B(0,1)$ bijectively to $\mathcal{H}$ via $$\varphi:z\mapsto \frac{z+1}{iz+1}.$$ Let $\tau_0$ be the image of $z_0$ under this isomorphism. Find a Möbius transformation $\mu$ sending $\tau_0$ to $i$ and define $$f = \varphi^{-1} \circ \mu \circ\varphi.$$ This is the automorphism we're looking for. The only problem is finding $\mu$. How can I do this explicitly?",,"['complex-analysis', 'differential-geometry', 'riemannian-geometry', 'riemann-surfaces']"
44,Uniform Boundedness for Holomorphic Functions,Uniform Boundedness for Holomorphic Functions,,"Let $\mathcal{F}$ be a family of holomorphic functions on a common domain $U \subset \mathbb C$. Suppose that $\mathcal{F}$ is pointwise bounded in the sense that for each $z \in U$, there is a constant $b > 0$ (potentially dependent on $z$) such that $|f(z)| \leq b$ for all $f \in \mathcal{F}$. Must it hold that $\mathcal{F}$ is uniformly bounded on each compact subset of $U$? Using a covering argument, I can show that this holds provided $\mathcal{F}$ is equicontinuous. Moreover, the set $\mathcal{F} = \{f(z) =1/z\}$ is a counterexample if we don't require the sets on which the functions are bounded be compact. However, I am unsure about whether this is true as stated.","Let $\mathcal{F}$ be a family of holomorphic functions on a common domain $U \subset \mathbb C$. Suppose that $\mathcal{F}$ is pointwise bounded in the sense that for each $z \in U$, there is a constant $b > 0$ (potentially dependent on $z$) such that $|f(z)| \leq b$ for all $f \in \mathcal{F}$. Must it hold that $\mathcal{F}$ is uniformly bounded on each compact subset of $U$? Using a covering argument, I can show that this holds provided $\mathcal{F}$ is equicontinuous. Moreover, the set $\mathcal{F} = \{f(z) =1/z\}$ is a counterexample if we don't require the sets on which the functions are bounded be compact. However, I am unsure about whether this is true as stated.",,"['complex-analysis', 'functional-analysis']"
45,simple(?) contour integral,simple(?) contour integral,,"I need to evaluate $$\int_0^1 \frac{\log(x)}{1−x}\;dx$$ I know I need to use contour integration and I read the chapter in Churchill but I'm still running into issues doing it properly. I also know the answer is $\displaystyle\;\; −\frac{\pi^2}{6},$ but I'd like to know how to arrive at that answer. Thanks!","I need to evaluate $$\int_0^1 \frac{\log(x)}{1−x}\;dx$$ I know I need to use contour integration and I read the chapter in Churchill but I'm still running into issues doing it properly. I also know the answer is $\displaystyle\;\; −\frac{\pi^2}{6},$ but I'd like to know how to arrive at that answer. Thanks!",,"['integration', 'complex-analysis']"
46,What is a reference for the ( classical and well-known ) proof of Weyl's lemma?,What is a reference for the ( classical and well-known ) proof of Weyl's lemma?,,"What is a reference for the (classical and well-known) proof of Weyl's lemma that states: Let $U$ be an open subset of $R^n$. Then if $f\in L^1_{loc} (U)$ and if $\int_U \hspace{1mm}{f\phi_\bar{z}}=0\;\;\;\forall \phi \in C_c^{\infty}(U) $, then $f$ is a.e. equal to a holomorphic function. Just any quick and good reference would be appreciated. I know Weyl's lemma has a weaker form involving weak Laplacian. Where can I find a proof of that?","What is a reference for the (classical and well-known) proof of Weyl's lemma that states: Let $U$ be an open subset of $R^n$. Then if $f\in L^1_{loc} (U)$ and if $\int_U \hspace{1mm}{f\phi_\bar{z}}=0\;\;\;\forall \phi \in C_c^{\infty}(U) $, then $f$ is a.e. equal to a holomorphic function. Just any quick and good reference would be appreciated. I know Weyl's lemma has a weaker form involving weak Laplacian. Where can I find a proof of that?",,"['real-analysis', 'analysis', 'complex-analysis', 'functional-analysis', 'partial-differential-equations']"
47,Complex Exponential Expansion,Complex Exponential Expansion,,I'm currently working through a problem using exponentials with complex numbers! I am simply wondering what the steps are to go from $$\pi \cdot (e^{i\frac{\pi}{3}} - e^{i\pi})  = \frac{\pi}{2}\cdot(3 + i\sqrt{3})$$ If someone could point me in the direction of the correct expansion that would be very helpful! Thank you,I'm currently working through a problem using exponentials with complex numbers! I am simply wondering what the steps are to go from $$\pi \cdot (e^{i\frac{\pi}{3}} - e^{i\pi})  = \frac{\pi}{2}\cdot(3 + i\sqrt{3})$$ If someone could point me in the direction of the correct expansion that would be very helpful! Thank you,,['complex-analysis']
48,Question about generalizing Schwarz Reflection,Question about generalizing Schwarz Reflection,,"You can use the proof of the Schwarz reflection principle to show that if $D$ is a domain in the complex plane, where $D^+$ denotes the subset of D above the real line, and $D^-$ the subset below, if $f:D\to C$ which is analytic on $D^+$ and $D^-$, and continuous on $D$, then $f$ is analytic on $D$. I want to generalize this as follows: Let $D$ be a domain in the complex plane, and $D'$ an open dense subset of $D$.  If $f$ is a function analytic on $D'$, and continuous on $D$, then $f$ is analytic on $D$. The proof of the Schwarz reflection principle uses integration along arbitrary triangles, and since the intersection of a triangle with the real line is really simple, you can ""cut the real line out"" so to speak when you are taking the integral.  But if all you know is that $f$ is analytic on an open dense set what can you do?","You can use the proof of the Schwarz reflection principle to show that if $D$ is a domain in the complex plane, where $D^+$ denotes the subset of D above the real line, and $D^-$ the subset below, if $f:D\to C$ which is analytic on $D^+$ and $D^-$, and continuous on $D$, then $f$ is analytic on $D$. I want to generalize this as follows: Let $D$ be a domain in the complex plane, and $D'$ an open dense subset of $D$.  If $f$ is a function analytic on $D'$, and continuous on $D$, then $f$ is analytic on $D$. The proof of the Schwarz reflection principle uses integration along arbitrary triangles, and since the intersection of a triangle with the real line is really simple, you can ""cut the real line out"" so to speak when you are taking the integral.  But if all you know is that $f$ is analytic on an open dense set what can you do?",,"['complex-analysis', 'reflection', 'analyticity']"
49,Residue of $f(z)=\frac{e^{\frac{1}{z}}}{z-z^2}$ at $z=0$,Residue of  at,f(z)=\frac{e^{\frac{1}{z}}}{z-z^2} z=0,"I am trying to calculate the residue of $ f(z)=\frac{e^{\frac{1}{z}}}{z-z^2}$ at $z=0$ by noticing that $$\frac{e^{\frac{1}{z}}}{z}\frac{1}{1-z}=\left(\frac{1+\frac{1}{z}+\frac{(\frac{1}{z})^2}{2!}+ \dots}{z}\right)\left(1+z+z^2 + \dots\right)$$ so that then $$\operatorname{Res}(0,f)=\frac{1}{0!}+\frac{1}{1!}+\frac{1}{2!}+ \dots=e.$$ I attempted verifying the result with WolframAlpha , but it didn't give me any output.","I am trying to calculate the residue of at by noticing that so that then I attempted verifying the result with WolframAlpha , but it didn't give me any output."," f(z)=\frac{e^{\frac{1}{z}}}{z-z^2} z=0 \frac{e^{\frac{1}{z}}}{z}\frac{1}{1-z}=\left(\frac{1+\frac{1}{z}+\frac{(\frac{1}{z})^2}{2!}+ \dots}{z}\right)\left(1+z+z^2 + \dots\right) \operatorname{Res}(0,f)=\frac{1}{0!}+\frac{1}{1!}+\frac{1}{2!}+ \dots=e.","['complex-analysis', 'residue-calculus']"
50,Is every bounded complex valued harmonic function on the open unit disc a sum of bounded holomorphic function and bounded antiholomorphic function?,Is every bounded complex valued harmonic function on the open unit disc a sum of bounded holomorphic function and bounded antiholomorphic function?,,"Let $\varphi$ be a bounded complex valued harmonic function on the open unit disc. Then $\varphi = \psi + \bar \chi$ for some holomorphic functions $\psi, \chi$ on the open unit disc (see last line of the question for the decomposition). Is it necessarily the case that $\psi$ and $\chi$ are bounded? I am almost sure that this is false, however, I have not been able to find a counterexample yet. Is there any immediate counterexample for this? Any bounded harmonic function can be written as $\varphi (z) = \sum_{n \ge 0} c_n z^n + \sum_{n>0} c_{-n} \bar {z} ^n$ for each $z \in \mathbb D$ . Take $\psi (z) = \sum_{n \ge 0} c_n z^n $ and $\chi (z) =\sum_{n>0} \bar c_{-n} {z} ^n$ for $z\in \mathbb D$ . Are $\psi$ and $\chi$ bounded?","Let be a bounded complex valued harmonic function on the open unit disc. Then for some holomorphic functions on the open unit disc (see last line of the question for the decomposition). Is it necessarily the case that and are bounded? I am almost sure that this is false, however, I have not been able to find a counterexample yet. Is there any immediate counterexample for this? Any bounded harmonic function can be written as for each . Take and for . Are and bounded?","\varphi \varphi = \psi + \bar \chi \psi, \chi \psi \chi \varphi (z) = \sum_{n \ge 0} c_n z^n + \sum_{n>0} c_{-n} \bar {z} ^n z \in \mathbb D \psi (z) = \sum_{n \ge 0} c_n z^n  \chi (z) =\sum_{n>0} \bar c_{-n} {z} ^n z\in \mathbb D \psi \chi","['complex-analysis', 'harmonic-functions']"
51,Strange substitution made in a paper to find asymptotics,Strange substitution made in a paper to find asymptotics,,"In the quoted section from this paper , why is the author able to ""substitute this result into Eq. (2.1)""? This should hold for $z$ large. But not everything on the contour is large. Why can the author make this substitution? Let $T=\frac{4}{27} t^3$ and $K_{1 / 6}(T)$ be the modified Bessel functions of order $\frac{1}{6}$ . Now previously the paper showed $$\operatorname{Ai}^3(x)=C_1 \int_{\mathcal{L}_1} t^{1 / 2} K_{1 / 6}(T) \exp \left(\frac{5}{27} t^3-x t\right) d t \qquad \qquad (2.1) $$ (If you see a rectangle, the symbol is mathcal{L}_1). Next we need to show that the constant $C_1$ can be chosen so that both sides of this equation have the same asymptotic behavior as $x \rightarrow \infty$ . For that purpose we first recall that in the complete sense of Watson [3] $$ K_{1 / 6}(T) \sim\left(\frac{3}{2}\right)^{3 / 2} \pi^{1 / 2} t^{-3 / 2} e^{-T} \quad\left(|\operatorname{ph} t|<\frac{1}{3} \pi\right) $$ Substituting this result into Eq. (2.1) Why? then gives $$ \operatorname{Ai}^3(x) \sim C_1\left(\frac{3}{2}\right)^{3 / 2} \pi^{1 / 2} \int_{\mathcal{L}_1} t^{-1} \exp \left(\frac{1}{27} t^3-x t\right) d t $$ and an application of the saddle-point method to this integral gives $$ \operatorname{Ai}^3(x) \sim i C_1 2^{3 / 2} 3 \pi x^{-3 / 4} e^{-3 \xi} $$","In the quoted section from this paper , why is the author able to ""substitute this result into Eq. (2.1)""? This should hold for large. But not everything on the contour is large. Why can the author make this substitution? Let and be the modified Bessel functions of order . Now previously the paper showed (If you see a rectangle, the symbol is mathcal{L}_1). Next we need to show that the constant can be chosen so that both sides of this equation have the same asymptotic behavior as . For that purpose we first recall that in the complete sense of Watson [3] Substituting this result into Eq. (2.1) Why? then gives and an application of the saddle-point method to this integral gives","z T=\frac{4}{27} t^3 K_{1 / 6}(T) \frac{1}{6} \operatorname{Ai}^3(x)=C_1 \int_{\mathcal{L}_1} t^{1 / 2} K_{1 / 6}(T) \exp \left(\frac{5}{27} t^3-x t\right) d t \qquad \qquad (2.1)
 C_1 x \rightarrow \infty 
K_{1 / 6}(T) \sim\left(\frac{3}{2}\right)^{3 / 2} \pi^{1 / 2} t^{-3 / 2} e^{-T} \quad\left(|\operatorname{ph} t|<\frac{1}{3} \pi\right)
 
\operatorname{Ai}^3(x) \sim C_1\left(\frac{3}{2}\right)^{3 / 2} \pi^{1 / 2} \int_{\mathcal{L}_1} t^{-1} \exp \left(\frac{1}{27} t^3-x t\right) d t
 
\operatorname{Ai}^3(x) \sim i C_1 2^{3 / 2} 3 \pi x^{-3 / 4} e^{-3 \xi}
","['real-analysis', 'complex-analysis', 'geometry', 'analysis', 'asymptotics']"
52,Using Cauchy's estimate to show a function is constant,Using Cauchy's estimate to show a function is constant,,"If $f$ is entire function satisfying $|f(z)|\leq |z|^{1/2}\log(1+|z|+|z|^2)$ . Show that $f(z)=c$ for some $c\in \mathbb{C}$ . My approach: Let $p\in \mathbb{C}$ and $\mathbb{D}_r(p)\subset \mathbb{C}$ . Then $f$ is continuous on the closure of the disc and holomorphic on the interior, because it is entire. Now, I apply Cauchy's estimate. For $n=1$ we have the following $$|f^\prime(p)| \leq \dfrac{1}{r}\sup_{|z-p|=r}|f(z)|\leq \dfrac{1}{r} \sup_{|z-p|=r}|z|^{1/2}\log(1+|z|+|z|^2)$$ since this is true for all $r>0$ . I just take $r\to \infty$ and we have $f ^\prime (p)=0$ . Completing the proof. I am not sure the part where I take $r$ to infinity is allowed since the sup also depends on $r$ ?","If is entire function satisfying . Show that for some . My approach: Let and . Then is continuous on the closure of the disc and holomorphic on the interior, because it is entire. Now, I apply Cauchy's estimate. For we have the following since this is true for all . I just take and we have . Completing the proof. I am not sure the part where I take to infinity is allowed since the sup also depends on ?",f |f(z)|\leq |z|^{1/2}\log(1+|z|+|z|^2) f(z)=c c\in \mathbb{C} p\in \mathbb{C} \mathbb{D}_r(p)\subset \mathbb{C} f n=1 |f^\prime(p)| \leq \dfrac{1}{r}\sup_{|z-p|=r}|f(z)|\leq \dfrac{1}{r} \sup_{|z-p|=r}|z|^{1/2}\log(1+|z|+|z|^2) r>0 r\to \infty f ^\prime (p)=0 r r,['complex-analysis']
53,The conjugate of a cosine series is a sine series,The conjugate of a cosine series is a sine series,,"In Katznelson's An Introduction to Harmonic Analysis the author defines the conjugate $\widetilde{S}$ of a trigonometric series $$S \sim \sum_{n = \infty}^\infty a_n e^{inx}$$ by $$\widetilde{S} \sim \sum_{n = \infty}^\infty -i \operatorname{sgn}(n) \ a_n e^{inx}.$$ Exercise 1.3. asks to formally check that if $S \sim \sum a_j \cos{jx}$ then $\widetilde{S} \sim \sum a_j \sin{jx}$ . My attempt. Using Euler's formula, we may formally write \begin{align} S \sim \sum_{j = -\infty}^\infty a_j \cos{jt} = \frac{1}{2}\left(\sum a_j e^{ijt} + \sum a_{-j} e^{ijt}\right) = \sum \frac{a_j + a_{-j}}{2} e^{ijt}. \end{align} Therefore, \begin{align}     \widetilde{S} &\sim \sum_{j = -\infty}^\infty  \operatorname{sgn}{j} \frac{a_j + a_{-j}}{2i} e^{ijt} \\     &= \frac{1}{2i} \left(\sum \operatorname{sgn}(j) a_{j} e^{ijt} + \sum \operatorname{sgn}(j) a_{-j} e^{ijt}\right) \\     &= \sum a_j \frac{\operatorname{sgn}(j) e^{ijt} + \operatorname{sgn}(-j) e^{-ijt}}{2i}\\     &= \sum_{j = -\infty}^{-1} a_{j} \frac{\operatorname{sgn}(j) e^{ijt} + \operatorname{sgn}(-j) e^{-ijt}}{2i} + \sum_{j = 1}^\infty a_j \frac{e^{ijt} - e^{-ijt}}{2i} \\ &= \sum_{j = -\infty}^{-1} a_{j} \sin{(-jt)}+ \sum_{j = 1}^\infty a_j \sin{jt} \end{align} I'm not sure how to conclude from here. Is this even correct?","In Katznelson's An Introduction to Harmonic Analysis the author defines the conjugate of a trigonometric series by Exercise 1.3. asks to formally check that if then . My attempt. Using Euler's formula, we may formally write Therefore, I'm not sure how to conclude from here. Is this even correct?","\widetilde{S} S \sim \sum_{n = \infty}^\infty a_n e^{inx} \widetilde{S} \sim \sum_{n = \infty}^\infty -i \operatorname{sgn}(n) \ a_n e^{inx}. S \sim \sum a_j \cos{jx} \widetilde{S} \sim \sum a_j \sin{jx} \begin{align}
S \sim \sum_{j = -\infty}^\infty a_j \cos{jt}
= \frac{1}{2}\left(\sum a_j e^{ijt} + \sum a_{-j} e^{ijt}\right)
= \sum \frac{a_j + a_{-j}}{2} e^{ijt}.
\end{align} \begin{align}
    \widetilde{S} &\sim \sum_{j = -\infty}^\infty  \operatorname{sgn}{j} \frac{a_j + a_{-j}}{2i} e^{ijt} \\
    &= \frac{1}{2i} \left(\sum \operatorname{sgn}(j) a_{j} e^{ijt} + \sum \operatorname{sgn}(j) a_{-j} e^{ijt}\right) \\
    &= \sum a_j \frac{\operatorname{sgn}(j) e^{ijt} + \operatorname{sgn}(-j) e^{-ijt}}{2i}\\
    &= \sum_{j = -\infty}^{-1} a_{j} \frac{\operatorname{sgn}(j) e^{ijt} + \operatorname{sgn}(-j) e^{-ijt}}{2i} + \sum_{j = 1}^\infty a_j \frac{e^{ijt} - e^{-ijt}}{2i} \\
&= \sum_{j = -\infty}^{-1} a_{j} \sin{(-jt)}+ \sum_{j = 1}^\infty a_j \sin{jt}
\end{align}","['complex-analysis', 'harmonic-analysis', 'trigonometric-series']"
54,"How are higher order complex derivatives and higher order partial derivatives related, quantitatively?","How are higher order complex derivatives and higher order partial derivatives related, quantitatively?",,"Let $f$ be a holomorphic function on a domain $U\subset \mathbb{C}$ , then it has a complex derivative $f^{(n)}$ at any order. Meanwhile, using canonical identification $\mathbb{C}\cong \mathbb{R}^2$ we can write $f=u+iv$ , where $u, v$ are both real-valued functions in terms of two real variables. This enables us to consider a different sort of derivatives, namely partial derivatives of $u$ and $v$ . And there should be some connections between these partial (including higher-order partial) derivatives and the complex ones. Since there are so many partial derivatives but just one complex derivative at each order, I hope to find a formula solely involving $f^{(n+m)}$ to represent the partial derivatives $\partial _x^n\partial_y^m u$ and $\partial _x^n\partial_y^m v$ . I'm even conjecturing that $\partial _x^n\partial_y^m (u+ i v)$ can only be one of these forms: $\pm f^{(n+m)}, \pm \overline{f^{(n+m)}}, \pm i f^{(n+m)}, \pm i \overline{f^{(n+m)}}$ . At order $n=1$ , this is exactly the famous Cauchy-Riemann equations: $\partial_x u =\partial_y v =\Re f^{(1)}$ , $\partial_y u=- \Im f^{(1)}$ and $\partial_x v =\Im f^{(1)}$ . At order $n=2$ , things become murkier but still solvable. We know that $f^{(1)}$ is  holomorphic, so Cauchy-Riemann equations apply automatically to $\partial_x u$ and $\partial_x v$ . After some tedious calculations I write down the Hessian matrices $\partial^2 u =\begin{pmatrix} \Re f^{(2)} & -\Im f^{(2)} \\ -\Im f^{(2)} & -\Re f^{(2)}\end{pmatrix}$ and $\partial^2 v =\begin{pmatrix} \Im f^{(2)} & \Re f^{(2)} \\ \Re f^{(2)} & -\Im f^{(2)}\end{pmatrix}$ . As order gets higher and higher I'm not sure how this pattern will evolve. I  was stuck with this problem initially while trying to prove $u$ and $v$ are smooth. Soon I found it too complicated to calculate the partial derivatives at every order, and I avoided it by induction on $n$ in the claim $u\in C^n(D)$ . If $u\in C^n(D)$ , then every (first-order) partial derivative will be in $C^n(D)$ , implying $u\in C^{n+1}(D)$ . Nevertheless, it's fun and instructive to ponder the accurate form of higher order partial derivatives.","Let be a holomorphic function on a domain , then it has a complex derivative at any order. Meanwhile, using canonical identification we can write , where are both real-valued functions in terms of two real variables. This enables us to consider a different sort of derivatives, namely partial derivatives of and . And there should be some connections between these partial (including higher-order partial) derivatives and the complex ones. Since there are so many partial derivatives but just one complex derivative at each order, I hope to find a formula solely involving to represent the partial derivatives and . I'm even conjecturing that can only be one of these forms: . At order , this is exactly the famous Cauchy-Riemann equations: , and . At order , things become murkier but still solvable. We know that is  holomorphic, so Cauchy-Riemann equations apply automatically to and . After some tedious calculations I write down the Hessian matrices and . As order gets higher and higher I'm not sure how this pattern will evolve. I  was stuck with this problem initially while trying to prove and are smooth. Soon I found it too complicated to calculate the partial derivatives at every order, and I avoided it by induction on in the claim . If , then every (first-order) partial derivative will be in , implying . Nevertheless, it's fun and instructive to ponder the accurate form of higher order partial derivatives.","f U\subset \mathbb{C} f^{(n)} \mathbb{C}\cong \mathbb{R}^2 f=u+iv u, v u v f^{(n+m)} \partial _x^n\partial_y^m u \partial _x^n\partial_y^m v \partial _x^n\partial_y^m (u+ i v) \pm f^{(n+m)}, \pm \overline{f^{(n+m)}}, \pm i f^{(n+m)}, \pm i \overline{f^{(n+m)}} n=1 \partial_x u =\partial_y v =\Re f^{(1)} \partial_y u=- \Im f^{(1)} \partial_x v =\Im f^{(1)} n=2 f^{(1)} \partial_x u \partial_x v \partial^2 u =\begin{pmatrix} \Re f^{(2)} & -\Im f^{(2)} \\ -\Im f^{(2)} & -\Re f^{(2)}\end{pmatrix} \partial^2 v =\begin{pmatrix} \Im f^{(2)} & \Re f^{(2)} \\ \Re f^{(2)} & -\Im f^{(2)}\end{pmatrix} u v n u\in C^n(D) u\in C^n(D) C^n(D) u\in C^{n+1}(D)","['complex-analysis', 'multivariable-calculus', 'derivatives']"
55,Doubt about derivative of the complex function in polar form: Where did I go wrong?,Doubt about derivative of the complex function in polar form: Where did I go wrong?,,"Let $w=f(z)=u(r,\theta)+iv(r,\theta)$ and $z=re^{i\theta}$ . Then, the correct formula is $$ \frac{dw}{dz}=e^{-i\theta}\frac{\partial w}{\partial r} $$ But I did it in the following way $$ \frac{dw}{dz}=\frac{\partial w}{\partial r}\frac{\partial r}{\partial z}+\frac{\partial w}{\partial \theta}\frac{\partial \theta}{\partial z}=e^{-i\theta}\frac{\partial w}{\partial r}-\frac{i}{r}e^{-i\theta}\frac{\partial w }{\partial \theta} $$ Where did I go wrong? In other words, why not include $\theta$ if it is a polar coordinate?","Let and . Then, the correct formula is But I did it in the following way Where did I go wrong? In other words, why not include if it is a polar coordinate?","w=f(z)=u(r,\theta)+iv(r,\theta) z=re^{i\theta} 
\frac{dw}{dz}=e^{-i\theta}\frac{\partial w}{\partial r}
 
\frac{dw}{dz}=\frac{\partial w}{\partial r}\frac{\partial r}{\partial z}+\frac{\partial w}{\partial \theta}\frac{\partial \theta}{\partial z}=e^{-i\theta}\frac{\partial w}{\partial r}-\frac{i}{r}e^{-i\theta}\frac{\partial w }{\partial \theta}
 \theta",['complex-analysis']
56,"Calculate $\oint_{S} \frac{e^{\pi z}}{4z^3 + z} dz$ where $S = [2, 2i,−2,−2i, 2]$",Calculate  where,"\oint_{S} \frac{e^{\pi z}}{4z^3 + z} dz S = [2, 2i,−2,−2i, 2]","Calculate $\oint_{S} \frac{e^{\pi z}}{4z^3 + z} dz$ where $S = [2, 2i,−2,−2i, 2]$ Using Partial fractions, the integral can be wrote as: $$\oint_{\gamma_1} \frac{e^{\pi z}}{4z} dz - \oint_{\gamma_2} \frac{z e^{\pi z}}{4z^2 + 1} dz$$ Here, I am not sure what path(s) I am to use for $\gamma_1$ & $\gamma_2$ . Does it remain the same path? If so, using Cauchy's integral theorem, for $\gamma_1$ , I get $$\oint_{\gamma_1} \frac{e^{\pi z}}{4z} dz\\ = \oint_{\gamma_1} \frac{\frac{e^{\pi z}}{4}}{z - 0} dz$$ Hence, by Cauchy's Integral formula, $2\pi f(0) = 2i\pi \frac{1}{4} = \frac{i\pi}{2}$ For $\oint_{\gamma_2} \frac{z e^{\pi z}}{4z^2 + 1} dz$ , the function is analytic at $z_0 = \frac{i}{2}$ & $z_0 = -\frac{i}{2}$ Hence, $2\pi f(\frac{i}{2}) = \frac{i}{8}$ & $2\pi f(-\frac{i}{2}) = -\frac{i}{8}$ So, $\oint_{\gamma_2} \frac{e^{\pi z}}{4z^2 + 1} dz = 0$ And the total integral is $\frac{i\pi}{2}$ ?","Calculate where Using Partial fractions, the integral can be wrote as: Here, I am not sure what path(s) I am to use for & . Does it remain the same path? If so, using Cauchy's integral theorem, for , I get Hence, by Cauchy's Integral formula, For , the function is analytic at & Hence, & So, And the total integral is ?","\oint_{S} \frac{e^{\pi z}}{4z^3 + z} dz S = [2, 2i,−2,−2i, 2] \oint_{\gamma_1} \frac{e^{\pi z}}{4z} dz - \oint_{\gamma_2} \frac{z e^{\pi z}}{4z^2 + 1} dz \gamma_1 \gamma_2 \gamma_1 \oint_{\gamma_1} \frac{e^{\pi z}}{4z} dz\\
= \oint_{\gamma_1} \frac{\frac{e^{\pi z}}{4}}{z - 0} dz 2\pi f(0) = 2i\pi \frac{1}{4} = \frac{i\pi}{2} \oint_{\gamma_2} \frac{z e^{\pi z}}{4z^2 + 1} dz z_0 = \frac{i}{2} z_0 = -\frac{i}{2} 2\pi f(\frac{i}{2}) = \frac{i}{8} 2\pi f(-\frac{i}{2}) = -\frac{i}{8} \oint_{\gamma_2} \frac{e^{\pi z}}{4z^2 + 1} dz = 0 \frac{i\pi}{2}","['complex-analysis', 'cauchy-integral-formula']"
57,Expand $(1-z)^{-m}$ for $m \in \mathbb{N}$ in powers of $z$,Expand  for  in powers of,(1-z)^{-m} m \in \mathbb{N} z,"Expand $(1-z)^{-m}$ for $m \in \mathbb{N}$ in powers of $z$ I've already read the links on this post and this post . The former concerns itself with the user's specific interest in a way to do it while the latter uses an approach that might lend itself to the approach I want to use. I'm mostly interested in solution verification. I may reference the second post. To expand in powers of $z$ , we observe that $f(z)=(1-z)^{-m}$ is analytic in neighborhoods about $z=0$ and hence has a Taylor expansion exists. We write $f(z)=\sum_{n=0}^\infty \frac{f^{(n)}(z)}{n!}z^n$ . Now, we experiment with derivatives. We see that $$f'(z)= -m(1-z)^{-m-1} \cdot (-1)  \hspace{0.2cm} ; \hspace{0.2cm} f''(z) = m(m+1)(1-z)^{-m-2}$$ $$\dots \hspace{0.2cm} f^{(n)}(z) = m(m+1)\dots(m+n-1)(1-z)^{-m-n}$$ $$ \implies f^{(n)}(z) =  \frac{(m+n-1)!}{(m-1)!}(1-z)^{-m-n} $$ Evaluating at $z=0$ gives $$f^{(n)}(0) = \frac{(m+n-1)!}{(m-1)!}$$ Hence, the expansion reads $$f(z) = \sum_{n=0}^\infty \frac{(m+n-1)!}{(m-1)!n!}z^n$$ $$\color{blue}{= \sum_{n=0}^\infty {m+n-1 \choose n}x^n}$$ In the posts I linked at the beginning, the answer was either derived via some binomial formula or via explicit manipulation of geometric series (in the style of the classic $\frac{1}{1-z}$ formula); the second post, which took advantage of the $\frac{1}{1-z}$ formula, did manage to interpret the coefficients using an "" $n$ choose $k$ "" style but this style is different from my result. I just want to be sure that what I wrote is equivalent to the work of the two linked posts. It seemed most intuitive to me to simply do this directly a la $n^{th}$ derivatives - in this way, no fancy outside tricks are needed.","Expand for in powers of I've already read the links on this post and this post . The former concerns itself with the user's specific interest in a way to do it while the latter uses an approach that might lend itself to the approach I want to use. I'm mostly interested in solution verification. I may reference the second post. To expand in powers of , we observe that is analytic in neighborhoods about and hence has a Taylor expansion exists. We write . Now, we experiment with derivatives. We see that Evaluating at gives Hence, the expansion reads In the posts I linked at the beginning, the answer was either derived via some binomial formula or via explicit manipulation of geometric series (in the style of the classic formula); the second post, which took advantage of the formula, did manage to interpret the coefficients using an "" choose "" style but this style is different from my result. I just want to be sure that what I wrote is equivalent to the work of the two linked posts. It seemed most intuitive to me to simply do this directly a la derivatives - in this way, no fancy outside tricks are needed.",(1-z)^{-m} m \in \mathbb{N} z z f(z)=(1-z)^{-m} z=0 f(z)=\sum_{n=0}^\infty \frac{f^{(n)}(z)}{n!}z^n f'(z)= -m(1-z)^{-m-1} \cdot (-1)  \hspace{0.2cm} ; \hspace{0.2cm} f''(z) = m(m+1)(1-z)^{-m-2} \dots \hspace{0.2cm} f^{(n)}(z) = m(m+1)\dots(m+n-1)(1-z)^{-m-n}  \implies f^{(n)}(z) =  \frac{(m+n-1)!}{(m-1)!}(1-z)^{-m-n}  z=0 f^{(n)}(0) = \frac{(m+n-1)!}{(m-1)!} f(z) = \sum_{n=0}^\infty \frac{(m+n-1)!}{(m-1)!n!}z^n \color{blue}{= \sum_{n=0}^\infty {m+n-1 \choose n}x^n} \frac{1}{1-z} \frac{1}{1-z} n k n^{th},"['complex-analysis', 'taylor-expansion', 'binomial-theorem']"
58,Contour integral - Cauchy's Residue Theorem,Contour integral - Cauchy's Residue Theorem,,"If $x>1$ show that \begin{equation}\frac{1}{2\pi i}\int_{c-i\infty}^{c+i\infty}\frac{x^s}{s}\textrm{d}s=1,\end{equation} for any $c>0$ . My working: the function $\frac{x^s}{s}$ is meromorphic with only a simple pole at $s=0$ with residue $1$ . So Cauchy's residue theorem says the integral is $2\pi i$ multiplied by the sum of the residues, which rearranges to $\frac{1}{2\pi i}\int\frac{x^s}{s}ds=1$ . The problem that I see with this is that it doesn't depend on the choice of contour which makes me suspicious. Solution given in the book: Consider the integral \begin{equation}\frac{1}{2\pi i}\int_{c-iR}^{c+iR}\frac{x^s}{s}ds\end{equation} with $R>c$ and the contour $\zeta_R$ described by the line segment joining $c-iR$ to $c+iR$ and the semicircle $S_R$ of radius $R$ centred at $c$ and enclosing the origin. Then by the Residue Theorem we have \begin{equation}\frac{1}{2\pi i}\int_{\zeta_R}\frac{x^s}{s}ds=\textrm{Res}_{s=0}\frac{x^s}{s}=1,\end{equation} and so \begin{equation}\frac{1}{2\pi i}\int_{c-iR}^{c+iR}\frac{x^s}{s}ds+\frac{1}{2\pi i}\int_{S_R}\frac{x^s}{s}=1.\end{equation} Now this second integral satisfies \begin{equation}\left|\frac{1}{2\pi i}\int_{S_R}\frac{x^s}{s}ds\right|\ll\frac{x^c}{2\pi R}\int_{\pi/2}^{3\pi/2}x^{R\cos\varphi}d\varphi.\end{equation} By setting $t=-\cos\varphi$ we can evaluate the integral and bound it above by $\frac{x^c}{2R}\to 0$ as $R\to\infty$ . (1) Why this choice of contour? (2) What exactly are they doing? What is the overall strategy behind their solution? I understand they want to consider $c-iR$ to $c+iR$ and take the limit as $R\to\infty$ , but I don't understand their approach","If show that for any . My working: the function is meromorphic with only a simple pole at with residue . So Cauchy's residue theorem says the integral is multiplied by the sum of the residues, which rearranges to . The problem that I see with this is that it doesn't depend on the choice of contour which makes me suspicious. Solution given in the book: Consider the integral with and the contour described by the line segment joining to and the semicircle of radius centred at and enclosing the origin. Then by the Residue Theorem we have and so Now this second integral satisfies By setting we can evaluate the integral and bound it above by as . (1) Why this choice of contour? (2) What exactly are they doing? What is the overall strategy behind their solution? I understand they want to consider to and take the limit as , but I don't understand their approach","x>1 \begin{equation}\frac{1}{2\pi i}\int_{c-i\infty}^{c+i\infty}\frac{x^s}{s}\textrm{d}s=1,\end{equation} c>0 \frac{x^s}{s} s=0 1 2\pi i \frac{1}{2\pi i}\int\frac{x^s}{s}ds=1 \begin{equation}\frac{1}{2\pi i}\int_{c-iR}^{c+iR}\frac{x^s}{s}ds\end{equation} R>c \zeta_R c-iR c+iR S_R R c \begin{equation}\frac{1}{2\pi i}\int_{\zeta_R}\frac{x^s}{s}ds=\textrm{Res}_{s=0}\frac{x^s}{s}=1,\end{equation} \begin{equation}\frac{1}{2\pi i}\int_{c-iR}^{c+iR}\frac{x^s}{s}ds+\frac{1}{2\pi i}\int_{S_R}\frac{x^s}{s}=1.\end{equation} \begin{equation}\left|\frac{1}{2\pi i}\int_{S_R}\frac{x^s}{s}ds\right|\ll\frac{x^c}{2\pi R}\int_{\pi/2}^{3\pi/2}x^{R\cos\varphi}d\varphi.\end{equation} t=-\cos\varphi \frac{x^c}{2R}\to 0 R\to\infty c-iR c+iR R\to\infty","['real-analysis', 'integration']"
59,Verification of Evan's representation formula $e^{-itB(y)}=\frac{1}{2\pi i}\int_\Gamma e^{-itz}(zI - B(y))^{-1}dz$ for $t$ independent $B(y)$,Verification of Evan's representation formula  for  independent,e^{-itB(y)}=\frac{1}{2\pi i}\int_\Gamma e^{-itz}(zI - B(y))^{-1}dz t B(y),"At the end of chapter 7.3 of Evan's PDE book, Evans states that if $B(y) = \sum_{j=1}^n y_jB_j$ for constant matrices $B_j$ and $\Gamma = \partial B(0,r)$ where $B(0,r)$ is a ball centered at zero in the complex plane with radius $r$ so large that $B(0,r)$ contains all eigenvalues of $B(y)$ , then we have the representation formula $$e^{-itB(y)}=\frac{1}{2\pi i}\int_\Gamma e^{-itz}(zI - B(y))^{-1}dz$$ This formula comes out of the blue, but luckily Evans provides a proof for it. Unluckily, Evans does nothing to motivate the intermediary steps he takes. For example, at first, Evans wants to verify that if $A(t,y)$ is the RHS of the identity, i.e. the integral expression, then $$B(y)A(t,y)x = -\frac{1}{i}\frac{d}{dt}A(t,y)x$$ for all $x\in \mathbb{R}^m$ . He does this by writing: $$B(y)A(t,y)x = \frac{1}{2\pi i}\int_\Gamma e^{-itz}B(y)(zI - B(y))^{-1}dz\Longleftrightarrow$$ $$B(y)A(t,y)x = \frac{1}{2\pi i}\int_\Gamma e^{-itz}\left(z(zI - B(y))^{-1}x - x\right)dz\Longleftrightarrow$$ as $\int_{\Gamma}e^{-itz}dz = 0$ . Sure, we can add/subtract zeroes all we like since $\Gamma$ is a closed loop and $e^{-itz}$ is holomorphic. But 1.) Why can we pass $B(y)$ inside the integral? This might be just a definition thing; it has been a some time since I've had to work with multivariate integrals. 2.) Why are $$\frac{1}{2\pi i}\int_\Gamma e^{-itz}B(y)(zI - B(y))^{-1}dz$$ and $$\frac{1}{2\pi i}\int_\Gamma e^{-itz}\left(z(zI - B(y))^{-1}x - x\right)dz$$ equal to each other? Similar trickery happens a bit later when Evans writes that $$A(0, y)x = \frac{1}{2\pi i}\int_{\Gamma}(zI - B(y))^{-1}xdz = \frac{1}{2\pi i}\int_{\Gamma}\frac{x + B(y)(zI - B(y))^{-1}}{z}dz$$ I have no clue why this equality holds.","At the end of chapter 7.3 of Evan's PDE book, Evans states that if for constant matrices and where is a ball centered at zero in the complex plane with radius so large that contains all eigenvalues of , then we have the representation formula This formula comes out of the blue, but luckily Evans provides a proof for it. Unluckily, Evans does nothing to motivate the intermediary steps he takes. For example, at first, Evans wants to verify that if is the RHS of the identity, i.e. the integral expression, then for all . He does this by writing: as . Sure, we can add/subtract zeroes all we like since is a closed loop and is holomorphic. But 1.) Why can we pass inside the integral? This might be just a definition thing; it has been a some time since I've had to work with multivariate integrals. 2.) Why are and equal to each other? Similar trickery happens a bit later when Evans writes that I have no clue why this equality holds.","B(y) = \sum_{j=1}^n y_jB_j B_j \Gamma = \partial B(0,r) B(0,r) r B(0,r) B(y) e^{-itB(y)}=\frac{1}{2\pi i}\int_\Gamma e^{-itz}(zI - B(y))^{-1}dz A(t,y) B(y)A(t,y)x = -\frac{1}{i}\frac{d}{dt}A(t,y)x x\in \mathbb{R}^m B(y)A(t,y)x = \frac{1}{2\pi i}\int_\Gamma e^{-itz}B(y)(zI - B(y))^{-1}dz\Longleftrightarrow B(y)A(t,y)x = \frac{1}{2\pi i}\int_\Gamma e^{-itz}\left(z(zI - B(y))^{-1}x - x\right)dz\Longleftrightarrow \int_{\Gamma}e^{-itz}dz = 0 \Gamma e^{-itz} B(y) \frac{1}{2\pi i}\int_\Gamma e^{-itz}B(y)(zI - B(y))^{-1}dz \frac{1}{2\pi i}\int_\Gamma e^{-itz}\left(z(zI - B(y))^{-1}x - x\right)dz A(0, y)x = \frac{1}{2\pi i}\int_{\Gamma}(zI - B(y))^{-1}xdz = \frac{1}{2\pi i}\int_{\Gamma}\frac{x + B(y)(zI - B(y))^{-1}}{z}dz","['real-analysis', 'functional-analysis', 'complex-analysis', 'multivariable-calculus']"
60,Particular sum of roots of unity,Particular sum of roots of unity,,"I've gotten stuck on a particular sum, to which I think I know the answer (thanks to Wolfram:Alpha), but not the method leading to it. I wonder if someone here can help me solve it. Let $d$ be a positive, odd integer, and define $u$ to be the first nontrivial $d$ th root of unity (that is, $u = \exp\left(\frac{2\pi i}{d}\right)$ ). Furthermore, let $k$ and $m$ be two integers s.t. $0\leq k,m \leq d-1$ . I would like to evaluate the sum: $$S =\sum_{j=0}^{d-1}\frac{u^{-kj}}{1+u^{mj}}$$ Now, after experimenting in W:A for a bit, I managed to find an interesting result. Letting $A = [0,\frac{d-1}{2}]$ and $B = [\frac{d+1}{2},d-1]$ ... $$S=\begin{cases}-\frac{1}{2}d \hspace{10pt}\text{ if } (k\in A\land m\in B)\lor (k\in B \land m\in A) \\ \frac{1}{2}d \hspace{18pt}\text{ if } (k\in A\land m\in A)\lor (k\in B \land m\in B) \text{ or } k=0\\ 0 \hspace{26pt}\text{ if } m=0 \end{cases}$$ Such an elegant solution indeed, but I have no earthly idea on how to solve this. I've tried to conjugate $S$ , fiddling with the exponents a bit and such, which gave rise to a few nice identities, but none proving useful enough to actually solve this. Is this correct? How would I show this? Any suggestions?","I've gotten stuck on a particular sum, to which I think I know the answer (thanks to Wolfram:Alpha), but not the method leading to it. I wonder if someone here can help me solve it. Let be a positive, odd integer, and define to be the first nontrivial th root of unity (that is, ). Furthermore, let and be two integers s.t. . I would like to evaluate the sum: Now, after experimenting in W:A for a bit, I managed to find an interesting result. Letting and ... Such an elegant solution indeed, but I have no earthly idea on how to solve this. I've tried to conjugate , fiddling with the exponents a bit and such, which gave rise to a few nice identities, but none proving useful enough to actually solve this. Is this correct? How would I show this? Any suggestions?","d u d u = \exp\left(\frac{2\pi i}{d}\right) k m 0\leq k,m \leq d-1 S =\sum_{j=0}^{d-1}\frac{u^{-kj}}{1+u^{mj}} A = [0,\frac{d-1}{2}] B = [\frac{d+1}{2},d-1] S=\begin{cases}-\frac{1}{2}d \hspace{10pt}\text{ if } (k\in A\land m\in B)\lor (k\in B \land m\in A) \\
\frac{1}{2}d \hspace{18pt}\text{ if } (k\in A\land m\in A)\lor (k\in B \land m\in B) \text{ or } k=0\\
0 \hspace{26pt}\text{ if } m=0
\end{cases} S","['sequences-and-series', 'complex-analysis', 'complex-numbers', 'summation', 'roots-of-unity']"
61,Failure of translation-invariance of an integral?,Failure of translation-invariance of an integral?,,"Let $u$ be a smooth compactly supported function on $\mathbb{C}$ . The $\overline{\partial}$ -Poincaré lemma is the formula $$\frac{{\partial}}{{\partial} \overline{z}}\frac{1}{\pi} \int_{\mathbb{C}} \frac{u(w)}{w - z} dx \wedge dy = u(z)$$ where $dx \wedge dy$ is the standard Lebesgue measure on $\mathbb{C}$ ( $w = x + iy$ ). In proving this, it is not immediately justified to pass the derivative through the integral, since for any fixed $w$ the integrand is an unbounded function of $z$ , so, e.g., the Dominated Convergence Theorem fails. But one can make the change of variables $w \to w + z$ to remedy this, so the above equals $$\frac{1}{\pi} \int_{\mathbb{C}} \frac{{\partial}}{{\partial} \overline{z}}\frac{u(w + z)}{w } dx \wedge dy.$$ From here one can make a standard limit argument using Stokes' theorem to finish the proof of the formula. My confusion is that the integral $\frac{1}{\pi} \int_{\mathbb{C}} \frac{{\partial}}{{\partial}\overline{ z}}\frac{u(w + z)}{w } dx \wedge dy$ appears not be translation-invariant! Indeed, if we undo the substitution we made before, then the integrand $\frac{{\partial}}{{\partial} \overline{z}}\frac{u(w)}{w - z}$ is zero almost everywhere, since $\frac{u(w)}{w - z}$ is holomorphic as a function of $z$ away from its pole. Clearly something is wrong here, but why exactly is this substitution is unjustified? What exactly fails when the derivative is introduced to the integral?","Let be a smooth compactly supported function on . The -Poincaré lemma is the formula where is the standard Lebesgue measure on ( ). In proving this, it is not immediately justified to pass the derivative through the integral, since for any fixed the integrand is an unbounded function of , so, e.g., the Dominated Convergence Theorem fails. But one can make the change of variables to remedy this, so the above equals From here one can make a standard limit argument using Stokes' theorem to finish the proof of the formula. My confusion is that the integral appears not be translation-invariant! Indeed, if we undo the substitution we made before, then the integrand is zero almost everywhere, since is holomorphic as a function of away from its pole. Clearly something is wrong here, but why exactly is this substitution is unjustified? What exactly fails when the derivative is introduced to the integral?",u \mathbb{C} \overline{\partial} \frac{{\partial}}{{\partial} \overline{z}}\frac{1}{\pi} \int_{\mathbb{C}} \frac{u(w)}{w - z} dx \wedge dy = u(z) dx \wedge dy \mathbb{C} w = x + iy w z w \to w + z \frac{1}{\pi} \int_{\mathbb{C}} \frac{{\partial}}{{\partial} \overline{z}}\frac{u(w + z)}{w } dx \wedge dy. \frac{1}{\pi} \int_{\mathbb{C}} \frac{{\partial}}{{\partial}\overline{ z}}\frac{u(w + z)}{w } dx \wedge dy \frac{{\partial}}{{\partial} \overline{z}}\frac{u(w)}{w - z} \frac{u(w)}{w - z} z,"['real-analysis', 'complex-analysis', 'measure-theory']"
62,Computing the domain of analyticity of $f(z)=\sqrt{z^2-1}$,Computing the domain of analyticity of,f(z)=\sqrt{z^2-1},"In this question , it is said that the domain of analyticity of the function $f(z)=\sqrt{z^2-1}$ over the branch $(0,2\pi)$ is $\mathbb{C} \setminus ((-\infty,-1) \cup (1,\infty))$ . My question: I would like to know how to show this affirmation. My attempt: We have $f(z)=\sqrt{z^2-1}=e^{\frac{1}{2}\ln(z^2-1)}$ , where $ln(z)$ is analytic in $\mathbb{C} \setminus [0,\infty)$ . Let $z=x+iy$ . So, $f(z)$ it is not analytic if $z^2-1=a$ for $a \in \mathbb{R}$ and $a\geq 0$ and we have the following system: $$ \begin{split} x^2-y^2-1=a  \hbox{ and }  xy=0. \end{split} $$ But I don't know how to complete the proof.","In this question , it is said that the domain of analyticity of the function over the branch is . My question: I would like to know how to show this affirmation. My attempt: We have , where is analytic in . Let . So, it is not analytic if for and and we have the following system: But I don't know how to complete the proof.","f(z)=\sqrt{z^2-1} (0,2\pi) \mathbb{C} \setminus ((-\infty,-1) \cup (1,\infty)) f(z)=\sqrt{z^2-1}=e^{\frac{1}{2}\ln(z^2-1)} ln(z) \mathbb{C} \setminus [0,\infty) z=x+iy f(z) z^2-1=a a \in \mathbb{R} a\geq 0 
\begin{split}
x^2-y^2-1=a
 \hbox{ and } 
xy=0.
\end{split}
","['complex-analysis', 'analytic-functions', 'analyticity']"
63,$\partial_{z}$ and $\partial_{\bar{z}}$: what are these vector fields from a geometrical point of view?,and : what are these vector fields from a geometrical point of view?,\partial_{z} \partial_{\bar{z}},"In complex analysis, we are taught that instead of coordinates $x$ , $y$ on the complex plane, one can use $z$ , $\bar{z}$ , then, for instance, the Cauchy-Riemann conditions become $\frac{\partial }{\partial \bar{z}}f(z, \bar{z})=0$ , and $\Delta = \frac{1}{4}\frac{\partial^2}{\partial z \partial \bar{z}}$ . This was explained to me in the following way: we simply perform a change of coordinates $z = x + y$ , $\bar{z}=x-y$ on $\mathbb{R}^2$ . However, if this was the case, functions with $\frac{\partial }{\partial \bar{z}}=0$ would be constant along lines $y=-x+C$ , which is clearly not true (for instance, this would mean isolated zeros or isolated singularities are impossible). In general, this means that $\partial_{\bar{z}}$ can't be understood as a directional derivative in the complex plane, i.e. as a vector field on the complex plane with real coefficients, because holomorphic functions would then be constant along the integral lines of this vector field. So, I guess, either the derivative or the change of coordinates should be understood in some other sense. Reading old MSE answers to similar questions, I understood that a) This confusion about the meaning of $\partial_{\bar{z}}$ is very common among people studying complex analysis, including, apparently, those who answer questions about Wirtinger derivatives (because often the answers boil down to treating $\partial_{z}$ and $\partial_{\bar{z}}$ as real vector fields). b) The correct answer (as opposed to hand-waving the issue) has something to do with the structure of complex manifold on $\mathbb{C}$ . So my question is: where can I find an elementary introduction to the theory of complex manifolds that would carefully explain how $\partial_{z}$ and $\partial_{\bar{z}}$ fields work? Edit: Perhaps, I should explain that my naive understanding of complex manifolds leads to further confusions. For instance, $\mathbb{C}$ is a one-dimensional complex manifold. The experience teaches me it should have one linearly independent vector at each tangent space, not two, which raises more questions about what $\partial_{z}$ and $\partial_{\bar{z}}$ really are.","In complex analysis, we are taught that instead of coordinates , on the complex plane, one can use , , then, for instance, the Cauchy-Riemann conditions become , and . This was explained to me in the following way: we simply perform a change of coordinates , on . However, if this was the case, functions with would be constant along lines , which is clearly not true (for instance, this would mean isolated zeros or isolated singularities are impossible). In general, this means that can't be understood as a directional derivative in the complex plane, i.e. as a vector field on the complex plane with real coefficients, because holomorphic functions would then be constant along the integral lines of this vector field. So, I guess, either the derivative or the change of coordinates should be understood in some other sense. Reading old MSE answers to similar questions, I understood that a) This confusion about the meaning of is very common among people studying complex analysis, including, apparently, those who answer questions about Wirtinger derivatives (because often the answers boil down to treating and as real vector fields). b) The correct answer (as opposed to hand-waving the issue) has something to do with the structure of complex manifold on . So my question is: where can I find an elementary introduction to the theory of complex manifolds that would carefully explain how and fields work? Edit: Perhaps, I should explain that my naive understanding of complex manifolds leads to further confusions. For instance, is a one-dimensional complex manifold. The experience teaches me it should have one linearly independent vector at each tangent space, not two, which raises more questions about what and really are.","x y z \bar{z} \frac{\partial }{\partial \bar{z}}f(z, \bar{z})=0 \Delta = \frac{1}{4}\frac{\partial^2}{\partial z \partial \bar{z}} z = x + y \bar{z}=x-y \mathbb{R}^2 \frac{\partial }{\partial \bar{z}}=0 y=-x+C \partial_{\bar{z}} \partial_{\bar{z}} \partial_{z} \partial_{\bar{z}} \mathbb{C} \partial_{z} \partial_{\bar{z}} \mathbb{C} \partial_{z} \partial_{\bar{z}}","['complex-analysis', 'reference-request', 'complex-geometry', 'complex-manifolds']"
64,Solve Cauchy Integral using residues,Solve Cauchy Integral using residues,,"I have an exam tomorrow, and we were given like an ""example test"" without answers, and one question is to solve this Cauchy integral: $$ \oint_c = \frac {2z+1} {(z+1)^2(z-3)}  $$ with circle $ c: |z-2|=5 $ the function has a double pole in $ -1 $ and a single pole in $ 3 $ , both of the poles being inside of the circle. Residue $ f(3) $ : $$ \lim_{z \rightarrow 3}(\frac{2z+1}{(z+1)^2}) = \frac 7 {16} $$ Residue $ f(-1) $ : $$ \frac {1} {1!} \lim_{z \rightarrow -1}(\frac{2z+1}{z-3})' = \lim_{z \rightarrow -1}(\frac{2(z-3)-(2z+1)}{(z-3)^2}) = \frac {2(-4) - (-1)} {(-4)^2} = \frac {-7}{16} $$ that makes the final result be: $$ 2 \pi i (\frac {7}{16} + \frac {-7}{16}) = 0 $$ Am I correct? For some reason, I find it odd, that the answer would be a 0 on an exam, but I can't check it anywhere, and I don't seem to see a mistake in my math.","I have an exam tomorrow, and we were given like an ""example test"" without answers, and one question is to solve this Cauchy integral: with circle the function has a double pole in and a single pole in , both of the poles being inside of the circle. Residue : Residue : that makes the final result be: Am I correct? For some reason, I find it odd, that the answer would be a 0 on an exam, but I can't check it anywhere, and I don't seem to see a mistake in my math.","
\oint_c = \frac {2z+1} {(z+1)^2(z-3)} 
  c: |z-2|=5   -1   3   f(3)  
\lim_{z \rightarrow 3}(\frac{2z+1}{(z+1)^2}) = \frac 7 {16}
  f(-1)  
\frac {1} {1!} \lim_{z \rightarrow -1}(\frac{2z+1}{z-3})' = \lim_{z \rightarrow -1}(\frac{2(z-3)-(2z+1)}{(z-3)^2}) = \frac {2(-4) - (-1)} {(-4)^2} = \frac {-7}{16}
 
2 \pi i (\frac {7}{16} + \frac {-7}{16}) = 0
","['integration', 'complex-analysis', 'cauchy-integral-formula']"
65,Injective holomorphic function is surjective,Injective holomorphic function is surjective,,"I know my problem is similar to this question , but I'm not allowed to use Picard's theorem here, so this question does not answer mine. Also, I'm looking to finish the proof I've already started. Let $f : \mathbb{C} \rightarrow \mathbb{C}$ be an injective holomorphic function. I want to prove $f$ is surjective. We know any holomorphic function has its image dense in $\mathbb{C}$ and since $f$ is injective we also have $f' \neq 0$ which means, by the inverse function theorem, that there is a neighborhood around each point in which $f$ is invertible. I want to show this inverse is the same for all points in the plane, that is we have a unique inverse of $f$ making it bijective. I'm stuck here. Any idea would be welcome.","I know my problem is similar to this question , but I'm not allowed to use Picard's theorem here, so this question does not answer mine. Also, I'm looking to finish the proof I've already started. Let be an injective holomorphic function. I want to prove is surjective. We know any holomorphic function has its image dense in and since is injective we also have which means, by the inverse function theorem, that there is a neighborhood around each point in which is invertible. I want to show this inverse is the same for all points in the plane, that is we have a unique inverse of making it bijective. I'm stuck here. Any idea would be welcome.",f : \mathbb{C} \rightarrow \mathbb{C} f \mathbb{C} f f' \neq 0 f f,['complex-analysis']
66,Pointwise boundedness and uniform boundedness,Pointwise boundedness and uniform boundedness,,"Suppose $f:[0,1]\times [1,\infty)\rightarrow \mathbb{R}$ a continous function (in both variable). Let $$ g(x)=\sup_{y\in[1,\infty)}f(x,y). (\text{Assume the supremum does exist for each x}) $$ For what kind of function $f$ is the function $g$ continuous or $g$ is bounded above? I came to know from an old post in the stack that it is true if we replace $[1,\infty)$ by any compact interval. Also, I think there will be functions in this case for which $g$ may not be continuous. But is there any result where $f$ ""nice"" enough so that $g$ is continuous?","Suppose a continous function (in both variable). Let For what kind of function is the function continuous or is bounded above? I came to know from an old post in the stack that it is true if we replace by any compact interval. Also, I think there will be functions in this case for which may not be continuous. But is there any result where ""nice"" enough so that is continuous?","f:[0,1]\times [1,\infty)\rightarrow \mathbb{R} 
g(x)=\sup_{y\in[1,\infty)}f(x,y). (\text{Assume the supremum does exist for each x})
 f g g [1,\infty) g f g","['real-analysis', 'complex-analysis']"
67,Did I set up the equation for the Mittag-Leffler theorem correctly?,Did I set up the equation for the Mittag-Leffler theorem correctly?,,"From the wiki page , it looks like a meromorphic the function is written as $$f(z) = \sum_{a\in E}p_a(z)$$ where $p_a(z)$ is the principal part (a Laurent series), $a$ is the place where a singularity occurs and I is the points belonging to some open set where the singularities occur. It looks like in the theorem, it also defines $p_a(z) = \sum_{n=1}^{N_a}\frac{c_{a,n}}{(z-a)^n}$ So let's say I have one of the examples on the page $\cot(z)$ , we would have a singularity at the points $z=0$ and $z=n\pi$ and this would occur for all integers $-\infty \le n \le \infty$ , so if I were to write this out, I would get: $$f(z) = \cot(z) = \frac{\cos(z)}{\sin(z)} = \lim_{N\rightarrow\infty} \sum_{n=-N}^N \Big( \sum_{n=1}^{N_a}\frac{c_{n\pi,n}}{(z-n\pi)^n}\Big)$$ I'm assuming that after finding $p_a(z)$ from finding the Laurent series (however that's done, I've never done one myself), the sum ends up becoming: $$f(z) = \lim_{N\rightarrow \infty} \sum_{n=-N}^N \frac{1}{z-n\pi}$$ . Is that correct? Note: Something unusual I noticed in the example for $\cot(z)$ is that despite $\frac{1}{z-n\pi}$ not being even (nor odd), it appears that the infinite sum was simplified by being split and doubled i.e.: $$\sum_{-N}^N a_n = a_0 + 2\sum_{n=1}^N a_n$$ .","From the wiki page , it looks like a meromorphic the function is written as where is the principal part (a Laurent series), is the place where a singularity occurs and I is the points belonging to some open set where the singularities occur. It looks like in the theorem, it also defines So let's say I have one of the examples on the page , we would have a singularity at the points and and this would occur for all integers , so if I were to write this out, I would get: I'm assuming that after finding from finding the Laurent series (however that's done, I've never done one myself), the sum ends up becoming: . Is that correct? Note: Something unusual I noticed in the example for is that despite not being even (nor odd), it appears that the infinite sum was simplified by being split and doubled i.e.: .","f(z) = \sum_{a\in E}p_a(z) p_a(z) a p_a(z) = \sum_{n=1}^{N_a}\frac{c_{a,n}}{(z-a)^n} \cot(z) z=0 z=n\pi -\infty \le n \le \infty f(z) = \cot(z) = \frac{\cos(z)}{\sin(z)} = \lim_{N\rightarrow\infty} \sum_{n=-N}^N \Big( \sum_{n=1}^{N_a}\frac{c_{n\pi,n}}{(z-n\pi)^n}\Big) p_a(z) f(z) = \lim_{N\rightarrow \infty} \sum_{n=-N}^N \frac{1}{z-n\pi} \cot(z) \frac{1}{z-n\pi} \sum_{-N}^N a_n = a_0 + 2\sum_{n=1}^N a_n","['complex-analysis', 'mittag-leffler-function']"
68,Residue Theorem for infinite oscillation,Residue Theorem for infinite oscillation,,Compute: $\int_{\lvert z \rvert = \frac{1}{2}} \frac{dz}{z  \sin\left( \frac{1}{z} \right)}$ So far I have done the following: $\int_{\lvert z \rvert = \frac{1}{2}} \frac{dz}{z  \sin\left( \frac{1}{z} \right)} = 2\pi i \sum_{n \in \mathbb{Z} \setminus{\{ 0 \}}}\text{Res}_{\frac{1}{n \pi}}  \left( {\frac{1}{z  \sin\left( \frac{1}{z} \right)}} \right) + \text{Res}_{0}  \left( {\frac{1}{z  \sin\left( \frac{1}{z} \right)}} \right) $ Now these are all simple poles and we get: $\text{Res}_{\frac{1}{n \pi}}  \left( {\frac{1}{z  \sin\left( \frac{1}{z} \right)}} \right) = \lim_{z\rightarrow \frac{1}{n\pi}}{\frac{{z-\frac{1}{n \pi}}}{z  \sin\left( \frac{1}{z} \right)}} = (-1)^{n+1} \frac{1}{n\pi}$ $\text{Res}_{0}  \left( {\frac{1}{z  \sin\left( \frac{1}{z} \right)}} \right) = \lim_{z\rightarrow 0}{\frac{z}{z  \sin\left( \frac{1}{z} \right)}} = \lim_{z\rightarrow 0}{\frac{1}{ \sin\left( \frac{1}{z} \right)}}$ And here I am stuck on how to evaluate this residue. Any ideas on how to continue?,Compute: So far I have done the following: Now these are all simple poles and we get: And here I am stuck on how to evaluate this residue. Any ideas on how to continue?,"\int_{\lvert z \rvert = \frac{1}{2}} \frac{dz}{z 
\sin\left( \frac{1}{z} \right)} \int_{\lvert z \rvert = \frac{1}{2}} \frac{dz}{z 
\sin\left( \frac{1}{z} \right)} = 2\pi i \sum_{n \in \mathbb{Z} \setminus{\{ 0 \}}}\text{Res}_{\frac{1}{n \pi}}  \left( {\frac{1}{z 
\sin\left( \frac{1}{z} \right)}} \right) + \text{Res}_{0}  \left( {\frac{1}{z 
\sin\left( \frac{1}{z} \right)}} \right)  \text{Res}_{\frac{1}{n \pi}}  \left( {\frac{1}{z 
\sin\left( \frac{1}{z} \right)}} \right) = \lim_{z\rightarrow \frac{1}{n\pi}}{\frac{{z-\frac{1}{n \pi}}}{z 
\sin\left( \frac{1}{z} \right)}} = (-1)^{n+1} \frac{1}{n\pi} \text{Res}_{0}  \left( {\frac{1}{z 
\sin\left( \frac{1}{z} \right)}} \right) = \lim_{z\rightarrow 0}{\frac{z}{z 
\sin\left( \frac{1}{z} \right)}} = \lim_{z\rightarrow 0}{\frac{1}{
\sin\left( \frac{1}{z} \right)}}","['complex-analysis', 'contour-integration', 'residue-calculus']"
69,imaginary part of the multiplication of complex numbers,imaginary part of the multiplication of complex numbers,,"I read a formula in a book about complex analysis: $$\text{Im}(\prod(1+ia_k))=0$$ is equivalent to $$\sum \arctan a_k=0 \pmod  \pi$$ I calculated cases $n=2,3$ , but I don't think direct calculation is a right way. Is there any quick idea to prove it? Thank you","I read a formula in a book about complex analysis: is equivalent to I calculated cases , but I don't think direct calculation is a right way. Is there any quick idea to prove it? Thank you","\text{Im}(\prod(1+ia_k))=0 \sum \arctan a_k=0 \pmod  \pi n=2,3",['complex-analysis']
70,Is the maximum modulus of an entire function on a circle of radius $r$ a smooth function of $r$?,Is the maximum modulus of an entire function on a circle of radius  a smooth function of ?,r r,"Let $f$ be a non-constant holomorphic function on $\mathbb{C},$ for any $r>0,$ set $$M(r)=\max_{|z|=r}|f(z)|$$ Is $M(r)\in C^{\infty}(0,+\infty)$ ? I have showed that $M(r)$ is continuous and strictly increasing, but I have no idea to deal with the derivative of $M(r),$ can someone help me?","Let be a non-constant holomorphic function on for any set Is ? I have showed that is continuous and strictly increasing, but I have no idea to deal with the derivative of can someone help me?","f \mathbb{C}, r>0, M(r)=\max_{|z|=r}|f(z)| M(r)\in C^{\infty}(0,+\infty) M(r) M(r),","['complex-analysis', 'smooth-functions']"
71,Exact differential in the setting of logarithmic integral,Exact differential in the setting of logarithmic integral,,"The question is in trying to understand the ""Argument Principle"" Gamelin's Complex Analysis textbook (pages 224-225). Suppose $f(z)$ is analytic on a domain $D$ . For a curve $\gamma$ in $D$ such that $f(z) \neq 0$ on $\gamma$ , we refer to \begin{equation} \frac{1}{2 \pi i} \int_{\gamma} \frac{f'(z)}{f(z)} \, dz = \frac{1}{2 \pi i} \int_{\gamma} d (\log f(z))  \end{equation} as the logarithmic integral of $f(z)$ along $\gamma$ . Thus the logarithmic integral measures the change of $\log f(z)$ along the curve $\gamma$ . I couldn't understand what the author meant by ""exact"" and ""closed but not exact"". Any help in understanding these two concepts are much appreciated.","The question is in trying to understand the ""Argument Principle"" Gamelin's Complex Analysis textbook (pages 224-225). Suppose is analytic on a domain . For a curve in such that on , we refer to as the logarithmic integral of along . Thus the logarithmic integral measures the change of along the curve . I couldn't understand what the author meant by ""exact"" and ""closed but not exact"". Any help in understanding these two concepts are much appreciated.","f(z) D \gamma D f(z) \neq 0 \gamma \begin{equation}
\frac{1}{2 \pi i} \int_{\gamma} \frac{f'(z)}{f(z)} \, dz = \frac{1}{2 \pi i} \int_{\gamma} d (\log f(z)) 
\end{equation} f(z) \gamma \log f(z) \gamma","['complex-analysis', 'logarithms']"
72,Integral Residue Calculation,Integral Residue Calculation,,"Integral: $I =\int^{\infty}_{-\infty}\frac{\cos x-1}{x^2(x^2+a^2)}\mathrm dx$ where $a \in \mathbb{R}$ and $a > 0$ . Method 1: We observe that there is a removable singularity at $x=0$ . Thus a semicircle contour in the upper plane centered at $z=0$ and with a sufficiently large radius $R$ is used to calculate the integral. We express $cosx$ as Re $e^{iz}$ . A simple calculation of the residue shows that $I = \frac{\pi  (1-e^{-a})}{a^3}$ . Method 2: The same as Method 1, instead we directly evaluate the residue without expressing $cosx$ as Re $e^{iz}$ . It follows that $I = \frac{\pi (1-\cosh a)}{a^3}$ . Method 3: Express $cosx$ as Re $e^{iz}$ in the first place. Let $f(z)=\frac{e^{iz}-1}{z^2(z^2+a^2)}$ , then $I = $ Re $ \int^{\infty}_{-\infty}f(z)\mathrm dz $ . Observe that there is a simple pole at $z = 0$ . Use the following contour $C$ where $R$ sufficiently large and $\epsilon$ sufficiently small. Residue calculation shows that $J = \int_C f(z)\mathrm dz = \frac{\pi(1-e^{-a})}{a^3}$ . $I = J - \int_{C_{\epsilon}} f(z)\mathrm dz$ , the latter one equals $\frac{\pi}{a^2}$ via another residue argument. Then $I = \frac{\pi (1-e^{-a})}{a^3}-\frac{\pi}{a^2}$ . Question: The results of 3 different approaches are different. WolframAlpha tells me that Method 3 yields the correct result, but I have no idea why the other two methods are incorrect. Please help point out the mistakes I made in Method 1 and Method 2.","Integral: where and . Method 1: We observe that there is a removable singularity at . Thus a semicircle contour in the upper plane centered at and with a sufficiently large radius is used to calculate the integral. We express as Re . A simple calculation of the residue shows that . Method 2: The same as Method 1, instead we directly evaluate the residue without expressing as Re . It follows that . Method 3: Express as Re in the first place. Let , then Re . Observe that there is a simple pole at . Use the following contour where sufficiently large and sufficiently small. Residue calculation shows that . , the latter one equals via another residue argument. Then . Question: The results of 3 different approaches are different. WolframAlpha tells me that Method 3 yields the correct result, but I have no idea why the other two methods are incorrect. Please help point out the mistakes I made in Method 1 and Method 2.",I =\int^{\infty}_{-\infty}\frac{\cos x-1}{x^2(x^2+a^2)}\mathrm dx a \in \mathbb{R} a > 0 x=0 z=0 R cosx e^{iz} I = \frac{\pi  (1-e^{-a})}{a^3} cosx e^{iz} I = \frac{\pi (1-\cosh a)}{a^3} cosx e^{iz} f(z)=\frac{e^{iz}-1}{z^2(z^2+a^2)} I =   \int^{\infty}_{-\infty}f(z)\mathrm dz  z = 0 C R \epsilon J = \int_C f(z)\mathrm dz = \frac{\pi(1-e^{-a})}{a^3} I = J - \int_{C_{\epsilon}} f(z)\mathrm dz \frac{\pi}{a^2} I = \frac{\pi (1-e^{-a})}{a^3}-\frac{\pi}{a^2},"['complex-analysis', 'residue-calculus']"
73,A problem involving complex polynomials,A problem involving complex polynomials,,"This question came up in the partial solution to an olympiad problem, in an attempt to complete the proof Suppose $P(a,b)$ is a complex polynomial satisfying $$P(a,b)= 0 \implies a^2 + b^2 = 0$$ Is it necessarily the case that $$P(a,b) = k(a+ ib)^m(a-ib)^n$$ ? My approach was to define the complex polynomial of one argument, $$f_c(a) = P(a,c)$$ And to note that the roots of $f$ satisfy $$ a = \pm ic $$ So that $f_c$ can be factorised as $$ f_c(a) = k(a+ic)^m(a-ic)^n $$ And so for any $c'$ we also have $$f_{c'}(a) = k'(a+ic')^{m'}(a-ic')^{n'} $$ And then I want to somehow use that $f_c$ and $f_{c}'$ must be connected by a continuous function, and we can ""bring"" $c$ and $c'$ as close to each other as we like, so that eventually the exponents and coefficient are forced to be the same. To try and make this more precise, we can take $\max\{m+n,m'+n'\}-1$ derivatives of both $f_c$ and $f_{c'}.$ W.l.o.g we can assume that the max of these is $m+n$ . Then after taking $m+n-1$ derivatives, assuming $m+n>m'+n'$ we are left with a linear polynomial, $f^{(m+n-1)}_c$ and a constant polynomial $f^{(m+n-1)}_{c'}$ . And it seems reasonable that a continuous complex function $g$ of two variables satisfying $$\forall{\epsilon > 0}\forall_{c}\exists_{c',|c-c'|<\epsilon} \\ [g(z,c) \mbox{ is linear and non constant } \land g(z,c') \mbox{ is constant}] $$ Does not exist, but I am struggling to make this intuition precise, and even if it were made precise, it only allows one to conclude that $m+n= m'+n'$ , and I am not sure how this would even help. Any insight into the problem, or highlighting of errors in my thinking so far would be appreciated. Keep in mind that my background in complex analysis is not fantastic, so an elegant proof using complicated theorems may go over my head","This question came up in the partial solution to an olympiad problem, in an attempt to complete the proof Suppose is a complex polynomial satisfying Is it necessarily the case that ? My approach was to define the complex polynomial of one argument, And to note that the roots of satisfy So that can be factorised as And so for any we also have And then I want to somehow use that and must be connected by a continuous function, and we can ""bring"" and as close to each other as we like, so that eventually the exponents and coefficient are forced to be the same. To try and make this more precise, we can take derivatives of both and W.l.o.g we can assume that the max of these is . Then after taking derivatives, assuming we are left with a linear polynomial, and a constant polynomial . And it seems reasonable that a continuous complex function of two variables satisfying Does not exist, but I am struggling to make this intuition precise, and even if it were made precise, it only allows one to conclude that , and I am not sure how this would even help. Any insight into the problem, or highlighting of errors in my thinking so far would be appreciated. Keep in mind that my background in complex analysis is not fantastic, so an elegant proof using complicated theorems may go over my head","P(a,b) P(a,b)= 0 \implies a^2 + b^2 = 0 P(a,b) = k(a+ ib)^m(a-ib)^n f_c(a) = P(a,c) f  a = \pm ic  f_c  f_c(a) = k(a+ic)^m(a-ic)^n  c' f_{c'}(a) = k'(a+ic')^{m'}(a-ic')^{n'}  f_c f_{c}' c c' \max\{m+n,m'+n'\}-1 f_c f_{c'}. m+n m+n-1 m+n>m'+n' f^{(m+n-1)}_c f^{(m+n-1)}_{c'} g \forall{\epsilon > 0}\forall_{c}\exists_{c',|c-c'|<\epsilon} \\ [g(z,c) \mbox{ is linear and non constant } \land g(z,c') \mbox{ is constant}]  m+n= m'+n'","['complex-analysis', 'polynomials', 'contest-math']"
74,Proving a non-isolated singularity is an essential singularity,Proving a non-isolated singularity is an essential singularity,,"Here stated more rigorously, Let $\{p_n\}_{n\in\mathbb{N}} $ be the set of singularity of an analytic function $f(z)$ on a bounded domain $U\backslash{\{p_n\}}$ . We know that $p_n$ has at least a limit point (By Bolzano Weierstrass). Call one of the limit point $p$ . Then $p$ is an essential singularity of $f(z)$ . Here's my attempt Proof Define $F(z)=(z-p)^N f(z)$ for the sake of contradiction, we assume that $p$ is not an essential singularity of $f$ , ie. there exists $N\in\mathbb{N}$ such that $\lim_{z\to p} (z-p)^N f(z)$ exists which I am going to call it $\alpha _0$ . - $F(z)$ has poles at $z\in\{p_n\}\backslash p$ So for all $M\in\mathbb{N}$ : $$\lim_{z\to p_M}F(z)=\mathrm{Diverges}$$ At this point, I guess I could conclude that $${\underbrace{\lim_{M\to \infty}\lim_{z\to p_M}F(z)}_{=\lim_{z\to p}F(z)}}={\underbrace{\lim_{M\to \infty}\mathrm{Diverges}}_{\mathrm{True \, for \, all }\, M\in \mathbb{N} }}=\mathrm{Divergent}$$ And $\lim_{z\to p}F(z)=$ Divergent but also equal to $\alpha_0$ which is a contradiction. This finishes the proof But I can seem to see that if this is intuitively true: ${\underbrace{\lim_{M\to \infty}\mathrm{Diverges}}_{\mathrm{True \, for \, all }\, M\in \mathbb{N} }}=\mathrm{Divergent}$ . Ie. does limit preserve divergence?","Here stated more rigorously, Let be the set of singularity of an analytic function on a bounded domain . We know that has at least a limit point (By Bolzano Weierstrass). Call one of the limit point . Then is an essential singularity of . Here's my attempt Proof Define for the sake of contradiction, we assume that is not an essential singularity of , ie. there exists such that exists which I am going to call it . - has poles at So for all : At this point, I guess I could conclude that And Divergent but also equal to which is a contradiction. This finishes the proof But I can seem to see that if this is intuitively true: . Ie. does limit preserve divergence?","\{p_n\}_{n\in\mathbb{N}}  f(z) U\backslash{\{p_n\}} p_n p p f(z) F(z)=(z-p)^N f(z) p f N\in\mathbb{N} \lim_{z\to p} (z-p)^N f(z) \alpha _0 F(z) z\in\{p_n\}\backslash p M\in\mathbb{N} \lim_{z\to p_M}F(z)=\mathrm{Diverges} {\underbrace{\lim_{M\to \infty}\lim_{z\to p_M}F(z)}_{=\lim_{z\to p}F(z)}}={\underbrace{\lim_{M\to \infty}\mathrm{Diverges}}_{\mathrm{True \, for \, all }\, M\in \mathbb{N} }}=\mathrm{Divergent} \lim_{z\to p}F(z)= \alpha_0 {\underbrace{\lim_{M\to \infty}\mathrm{Diverges}}_{\mathrm{True \, for \, all }\, M\in \mathbb{N} }}=\mathrm{Divergent}","['complex-analysis', 'limits']"
75,Show that a l-homogenous polynomial is harmonic under certain conditions,Show that a l-homogenous polynomial is harmonic under certain conditions,,"Let $a,b \in \mathbb{R}^d$ and $l \in \{2,3\}$ . Then $P:\mathbb{R}^d \to \mathbb{C}, P(x):=(<a,x>+i<b,x>)^l$ is a $l$ -homogenous polynomial. Show that $P$ is harmonic iff $||a||=||b||$ and $<a,b>=0$ . My attempt was to calculate the Laplace of $P(x)$ . So I wrote $P(x)=(\sum_{i=1}^d a_i x_i+i \sum_{i=1}^d b_i x_i)^l$ . I'm not sure if it's correct but I got: $$\Delta P(x)=l(l-1)(\sum_{i=1}^d a_i x_i+i b_i x_i)^{l-2}\sum_{i=1}^d (a_i +i  b_i)^2$$ Assuming $P$ is harmonic, then it's Laplace is $0$ and since $l$ and $l-1$ can't be $0$ by definition one or both of the other factors has to be $0$ . But those are complex numbers (without the exponents) and so for them to be $0$ their real and imaginary part have to be $0$ . So $\sum_{i=1}^d a_i x_i=\sum_{i=1}^d b_i x_i=\sum_{i=1}^d a_i=\sum_{i=1}^d b_i=0$ . I'm not sure what to conclude from here on or how to show the reverse direction. Also please check my work so far as I think I made some mistake or false assumption. Thanks! EDIT: I wrote the sums as one and corrected my mistake when I calculated the Laplace operator. Now the forwards direction seems to be done. But I still don't know how to prove the reverse direction.","Let and . Then is a -homogenous polynomial. Show that is harmonic iff and . My attempt was to calculate the Laplace of . So I wrote . I'm not sure if it's correct but I got: Assuming is harmonic, then it's Laplace is and since and can't be by definition one or both of the other factors has to be . But those are complex numbers (without the exponents) and so for them to be their real and imaginary part have to be . So . I'm not sure what to conclude from here on or how to show the reverse direction. Also please check my work so far as I think I made some mistake or false assumption. Thanks! EDIT: I wrote the sums as one and corrected my mistake when I calculated the Laplace operator. Now the forwards direction seems to be done. But I still don't know how to prove the reverse direction.","a,b \in \mathbb{R}^d l \in \{2,3\} P:\mathbb{R}^d \to \mathbb{C}, P(x):=(<a,x>+i<b,x>)^l l P ||a||=||b|| <a,b>=0 P(x) P(x)=(\sum_{i=1}^d a_i x_i+i \sum_{i=1}^d b_i x_i)^l \Delta P(x)=l(l-1)(\sum_{i=1}^d a_i x_i+i b_i x_i)^{l-2}\sum_{i=1}^d (a_i +i  b_i)^2 P 0 l l-1 0 0 0 0 \sum_{i=1}^d a_i x_i=\sum_{i=1}^d b_i x_i=\sum_{i=1}^d a_i=\sum_{i=1}^d b_i=0","['complex-analysis', 'polynomials']"
76,An integral identity: $\int_{0}^{\large\frac{π}{2}}\dfrac{\cos^{a}x\sin(a+1)x}{\sin x}\ \mathrm{d}x$,An integral identity:,\int_{0}^{\large\frac{π}{2}}\dfrac{\cos^{a}x\sin(a+1)x}{\sin x}\ \mathrm{d}x,"I recently came across a parametric definite integral about Chebyshev polynomials: $$ f(a)= \begin{aligned} \begin{gather*} \int_{0}^{\frac{\pi}{2}}\dfrac{\cos^{a}x\sin(a+1)x}{\sin x}\ \mathrm{d}x \end{gather*} \end{aligned} $$ Easily, I found out the form of Chebyshev polynomials of the Second kind and guessed that the integration result might be a constant or something. We know that Chebyshev polynomials of the Second kind satisfies the following differential equation: $$ \begin{aligned} \begin{gather*} (1-x^2)\ U''_{a}(x)-3x\ U_{a}'(x)+a(a+2)\ U_{a}(x)=0 \end{gather*} \end{aligned} $$ Let $\ \displaystyle U_{a}(x)=\sum_{k=0}^{\infty}{u_{k}\ x^k}$ , we can get that $\displaystyle u_{k+2}=\dfrac{(k-a)(k+a+2)}{(k+1)(k+2)}u_{k}$ , which means that $$ \begin{aligned} \begin{gather*} \displaystyle U_{a}(x)=\sum_{k=0}^{\infty}{\dfrac{(-2)^k\ \Gamma\left({\large\frac{k+a}{2}}+1\right)\Gamma\left({\large\frac{k-a}{2}}\right)}{\Gamma(k+1)}x^k} \end{gather*} \end{aligned} $$ MeanWhile， $$ \begin{aligned} \begin{gather*} \displaystyle \int_{0}^{1}{\dfrac{x^u}{\sqrt{1-x^2}}\ \mathrm{d}x}=\dfrac{\sqrt{π}\ \Gamma({\large\frac{u+1}{2}})}{2\ \Gamma({\large\frac{u}{2}}+1)},\ u>-1 \end{gather*} \end{aligned} $$ Finally, it ends up with one series following: $$ \begin{aligned} \begin{gather*} -\dfrac{\sin(πa)}{2π}\sum_{k=0}^{\infty}{\dfrac{\sqrt{π}\ (-2)^k\ \Gamma\left({\large\frac{k-a}{2}}\right)\Gamma\left({\large\frac{k+a+1}{2}}\right)}{2\ \Gamma(k+1)}} \end{gather*} \end{aligned} $$ I have been going at this Series all day and I am confused on where to begin.Thoughts and several Solution ideas are greatly appreciated !","I recently came across a parametric definite integral about Chebyshev polynomials: Easily, I found out the form of Chebyshev polynomials of the Second kind and guessed that the integration result might be a constant or something. We know that Chebyshev polynomials of the Second kind satisfies the following differential equation: Let , we can get that , which means that MeanWhile， Finally, it ends up with one series following: I have been going at this Series all day and I am confused on where to begin.Thoughts and several Solution ideas are greatly appreciated !"," f(a)= \begin{aligned}
\begin{gather*}
\int_{0}^{\frac{\pi}{2}}\dfrac{\cos^{a}x\sin(a+1)x}{\sin x}\ \mathrm{d}x
\end{gather*}
\end{aligned}   \begin{aligned}
\begin{gather*}
(1-x^2)\ U''_{a}(x)-3x\ U_{a}'(x)+a(a+2)\ U_{a}(x)=0
\end{gather*}
\end{aligned}  \ \displaystyle U_{a}(x)=\sum_{k=0}^{\infty}{u_{k}\ x^k} \displaystyle u_{k+2}=\dfrac{(k-a)(k+a+2)}{(k+1)(k+2)}u_{k}  \begin{aligned}
\begin{gather*}
\displaystyle U_{a}(x)=\sum_{k=0}^{\infty}{\dfrac{(-2)^k\ \Gamma\left({\large\frac{k+a}{2}}+1\right)\Gamma\left({\large\frac{k-a}{2}}\right)}{\Gamma(k+1)}x^k}
\end{gather*}
\end{aligned}   \begin{aligned}
\begin{gather*}
\displaystyle \int_{0}^{1}{\dfrac{x^u}{\sqrt{1-x^2}}\ \mathrm{d}x}=\dfrac{\sqrt{π}\ \Gamma({\large\frac{u+1}{2}})}{2\ \Gamma({\large\frac{u}{2}}+1)},\ u>-1
\end{gather*}
\end{aligned}   \begin{aligned}
\begin{gather*}
-\dfrac{\sin(πa)}{2π}\sum_{k=0}^{\infty}{\dfrac{\sqrt{π}\ (-2)^k\ \Gamma\left({\large\frac{k-a}{2}}\right)\Gamma\left({\large\frac{k+a+1}{2}}\right)}{2\ \Gamma(k+1)}}
\end{gather*}
\end{aligned} ","['complex-analysis', 'definite-integrals', 'special-functions', 'contour-integration', 'chebyshev-polynomials']"
77,A holomorphic function satisfying $f(z)=f(iz)$,A holomorphic function satisfying,f(z)=f(iz),"Let $G\subset \mathbb{C}$ be open and define $\Omega = \{ z\in\mathbb{C}: z^4\in G \}.$ Assume that $f$ is analytic on $\Omega$ and satisfies $f(iz)=f(z)$ for all $z\in \Omega$ . I would like to show that there exists an analytic function $g$ such that $f(z)=g(z^4)$ for all $z\in \Omega$ . The converse is clearly true as $(iz)^4=z^4$ , but it doesn't seem to offer many clues as to how to prove the other and harder direction. Letting $f(z)=u(x,y)+iv(x,y)$ with $z=x+iy$ , the constraint turns into $u(x,y)=u(-y,x)$ and $v(x,y)=v(-y,x)$ with of course $\Delta u=0$ , $\Delta v=0$ . I had the idea of writing, for any $z_0\in \Omega$ and for some neighborhood of $z_0$ , since $f$ is analytic, the following equality holds $$f(z)=\sum_{k\ge 0} \frac{a_k}{k!} (z-z_0)^k.$$ Thus, $$0=f(z)-f(iz)= \sum_{k\ge 0} \frac{a_k}{k!} (z-z_0)^k(1-i^k)=\sum_{k\neq 0 \operatorname{mod} 4} \frac{b_k}{k!} (z-z_0)^k$$ for $b_k=a_k(1-i^k)$ . At this point, I want to conclude that necessarily $b_k\equiv 0$ for all $k\neq 0 \operatorname{mod} 4$ , and thus $a_k=0$ for all $k\neq 0 \operatorname{mod} 4$ . Hence, we can write $$f(z)=\sum_{n\ge 0} \frac{a_{4n}}{(4n)!} ((z-z_0)^4)^n=g((z-z_0)^4)$$ where $$g(z)=\sum_{n\ge 0} \frac{a_{4n}}{(4n)!} z^n$$ is analytic since $f$ is, but $z_0$ is not necessarily zero.","Let be open and define Assume that is analytic on and satisfies for all . I would like to show that there exists an analytic function such that for all . The converse is clearly true as , but it doesn't seem to offer many clues as to how to prove the other and harder direction. Letting with , the constraint turns into and with of course , . I had the idea of writing, for any and for some neighborhood of , since is analytic, the following equality holds Thus, for . At this point, I want to conclude that necessarily for all , and thus for all . Hence, we can write where is analytic since is, but is not necessarily zero.","G\subset \mathbb{C} \Omega = \{ z\in\mathbb{C}: z^4\in G \}. f \Omega f(iz)=f(z) z\in \Omega g f(z)=g(z^4) z\in \Omega (iz)^4=z^4 f(z)=u(x,y)+iv(x,y) z=x+iy u(x,y)=u(-y,x) v(x,y)=v(-y,x) \Delta u=0 \Delta v=0 z_0\in \Omega z_0 f f(z)=\sum_{k\ge 0} \frac{a_k}{k!} (z-z_0)^k. 0=f(z)-f(iz)= \sum_{k\ge 0} \frac{a_k}{k!} (z-z_0)^k(1-i^k)=\sum_{k\neq 0 \operatorname{mod} 4} \frac{b_k}{k!} (z-z_0)^k b_k=a_k(1-i^k) b_k\equiv 0 k\neq 0 \operatorname{mod} 4 a_k=0 k\neq 0 \operatorname{mod} 4 f(z)=\sum_{n\ge 0} \frac{a_{4n}}{(4n)!} ((z-z_0)^4)^n=g((z-z_0)^4) g(z)=\sum_{n\ge 0} \frac{a_{4n}}{(4n)!} z^n f z_0",['complex-analysis']
78,confusion about stokes theorem,confusion about stokes theorem,,"Consider the open unit disk $\mathbb{D}$ in complex plane $\mathbb{C}$ , let $\mathbb{U}$ be the open disk of the origin with radius $\frac{1}{2}$ , consider the manifold with boundary $\mathbb{D}-\mathbb{U}$ and a holomorphic form $\omega=\frac{1}{z}dz$ on it where $dz=dx+idy$ . Then $d\omega$ is zero on $\mathbb{D}-\mathbb{U}-\partial\mathbb{U}$ since $\frac{1}{z}$ is holomorphic on it hence by stoke's theorem $$ 0=\int\int_{\mathbb{D}-\mathbb{U}-\partial\mathbb{U}} d\omega=-\int_{\partial\mathbb{U}}\omega=-\int_{\partial\mathbb{U}}\frac{1}{z}dz $$ But we know the right side is $-2\pi i$ , what is wrong in this computation?","Consider the open unit disk in complex plane , let be the open disk of the origin with radius , consider the manifold with boundary and a holomorphic form on it where . Then is zero on since is holomorphic on it hence by stoke's theorem But we know the right side is , what is wrong in this computation?","\mathbb{D} \mathbb{C} \mathbb{U} \frac{1}{2} \mathbb{D}-\mathbb{U} \omega=\frac{1}{z}dz dz=dx+idy d\omega \mathbb{D}-\mathbb{U}-\partial\mathbb{U} \frac{1}{z} 
0=\int\int_{\mathbb{D}-\mathbb{U}-\partial\mathbb{U}} d\omega=-\int_{\partial\mathbb{U}}\omega=-\int_{\partial\mathbb{U}}\frac{1}{z}dz
 -2\pi i","['complex-analysis', 'manifolds']"
79,proof : $(\forall z\in \mathbb{C}-\{2\} ) \ \ \overline{f(z)}=-f(z) \Leftrightarrow (z-1+\frac{1}{2}i)(\bar{z}-1-\frac{1}{2}i)-\frac{5}{4}=0 $,proof :,(\forall z\in \mathbb{C}-\{2\} ) \ \ \overline{f(z)}=-f(z) \Leftrightarrow (z-1+\frac{1}{2}i)(\bar{z}-1-\frac{1}{2}i)-\frac{5}{4}=0 ,"$$f(z)=\frac{z+i}{z-2}\ \  ; \; \; z\in\mathbb{C}-\{2\}$$ proof : $(\forall z\in \mathbb{C}-\{2\} ) \ \ \overline{f(z)}=-f(z) \Leftrightarrow (z-1+\frac{1}{2}i)(\bar{z}-1-\frac{1}{2}i)-\frac{5}{4}=0 $ this is what I did, $$(z-1+\frac{1}{2}i)(\bar{z}-1-\frac{1}{2}i)-\frac{5}{4}=0$$ $$z(\bar{z}-1-\frac{1}{2}i)+(-1+\frac{1}{2}i)(\bar{z}-1-\frac{1}{2}i)-\frac{5}{4}=0$$ $$z\bar{z}-(1+\frac{1}{2}i)z+(-1+\frac{1}{2}i)\bar{z}+(1+\frac{1}{2}i)(1-\frac{1}{2}i)-\frac{5}{4}=0$$ $$z\bar{z}-(1+\frac{1}{2}i)z+(-1+\frac{1}{2}i)\bar{z}=0$$ but I am not sure for what I have to do next, I want to get this result : $$ (\bar{z}-i)(z-2)=-(z+i)(\bar{z}-2) $$ What I have to do next? if there is any other way tell me please.","proof : this is what I did, but I am not sure for what I have to do next, I want to get this result : What I have to do next? if there is any other way tell me please.",f(z)=\frac{z+i}{z-2}\ \  ; \; \; z\in\mathbb{C}-\{2\} (\forall z\in \mathbb{C}-\{2\} ) \ \ \overline{f(z)}=-f(z) \Leftrightarrow (z-1+\frac{1}{2}i)(\bar{z}-1-\frac{1}{2}i)-\frac{5}{4}=0  (z-1+\frac{1}{2}i)(\bar{z}-1-\frac{1}{2}i)-\frac{5}{4}=0 z(\bar{z}-1-\frac{1}{2}i)+(-1+\frac{1}{2}i)(\bar{z}-1-\frac{1}{2}i)-\frac{5}{4}=0 z\bar{z}-(1+\frac{1}{2}i)z+(-1+\frac{1}{2}i)\bar{z}+(1+\frac{1}{2}i)(1-\frac{1}{2}i)-\frac{5}{4}=0 z\bar{z}-(1+\frac{1}{2}i)z+(-1+\frac{1}{2}i)\bar{z}=0  (\bar{z}-i)(z-2)=-(z+i)(\bar{z}-2) ,"['complex-analysis', 'geometry', 'complex-numbers']"
80,Quotient of elliptic riemann surface by $\mathbb{Z}/3$,Quotient of elliptic riemann surface by,\mathbb{Z}/3,"Consider an elliptic curve $E$ and a non-zero point $a\in E$ of order three. Translation by $a$ is an automorphism $\tau_a:E\to E, x\mapsto x+a$ of order $3$ of the Riemann surface $E$ . Could you please give me an idea how to get an equation of $E/G$ if an equation of $E$ is given? In my case $E$ is given by $y^2 = x^4 + 10x^3 +25x^2 -100x.$ Any help would be appreciated!",Consider an elliptic curve and a non-zero point of order three. Translation by is an automorphism of order of the Riemann surface . Could you please give me an idea how to get an equation of if an equation of is given? In my case is given by Any help would be appreciated!,"E a\in E a \tau_a:E\to E, x\mapsto x+a 3 E E/G E E y^2 = x^4 + 10x^3 +25x^2 -100x.","['complex-analysis', 'algebraic-geometry', 'elliptic-curves', 'riemann-surfaces', 'quotient-spaces']"
81,Are complex analytic subsets of affine space necessarily zero sets of a globally defined holomorphic function?,Are complex analytic subsets of affine space necessarily zero sets of a globally defined holomorphic function?,,"Let me fix some terminology. Let's say $U \subset \mathbb{C}^m$ is said to be a complex analytic subset if it can be covered by open subsets $V_i$ such that $U\cap V_i$ is the zero set of holomorphic functions $f_1,...,f_n$ on $V_i$ . Is it true that if $U$ is closed, there is a single chart $V\subset \mathbb{C}^m$ with holomorphic functions $f_1,...,f_n$ on $V$ such that $U=\{z\in V:f_i(z)=0,\forall i\}$ ? I guess there are two issues. The first problem is that if you have $f_i$ on $V$ and $g_i$ on $V'$ that agree on the intersection is there a unique $V$ and $V'$ that is holomorphic on $V\cup V'$ . I think basiaclly this is resolved by using analytic continuation for holomorphic functions in several variables by Hartog's theorem. The second issue is something that is bothering me. If your $U$ is locally cut out by one holomorphic function, we can patch these together to get a holomorphic function on one chart. But say on $V_1$ , it is the zero set of $f_1,...,f_n$ and on $V_2$ , it is the zero set of $g_1,...,g_n$ how do I know that for each $f_i$ there is a $g_j$ where they agree on the intersection?","Let me fix some terminology. Let's say is said to be a complex analytic subset if it can be covered by open subsets such that is the zero set of holomorphic functions on . Is it true that if is closed, there is a single chart with holomorphic functions on such that ? I guess there are two issues. The first problem is that if you have on and on that agree on the intersection is there a unique and that is holomorphic on . I think basiaclly this is resolved by using analytic continuation for holomorphic functions in several variables by Hartog's theorem. The second issue is something that is bothering me. If your is locally cut out by one holomorphic function, we can patch these together to get a holomorphic function on one chart. But say on , it is the zero set of and on , it is the zero set of how do I know that for each there is a where they agree on the intersection?","U \subset \mathbb{C}^m V_i U\cap V_i f_1,...,f_n V_i U V\subset \mathbb{C}^m f_1,...,f_n V U=\{z\in V:f_i(z)=0,\forall i\} f_i V g_i V' V V' V\cup V' U V_1 f_1,...,f_n V_2 g_1,...,g_n f_i g_j","['complex-analysis', 'differential-geometry', 'algebraic-geometry']"
82,Order of a pole formula / definition,Order of a pole formula / definition,,"I have come up with the following formula for the order of a pole of some function $f(z)=\frac{g(z)}{h(z)}$ at a point $z = c$ . It involves first taking the multiplicative inverse (denoted by $1/f(z)$ to avoid getting mixed up with the inverse of the function), then finding the smallest $n$ such that the derivative is not equal to zero. $$ \operatorname{ord}(f, c)=\min \{ n \in \mathbb N:(1/f)^{(n)}(c)\neq 0 \} $$ Does this equation hold? It works for my naive test of $f((z-2)^{-3}, 2)$ . However I haven't seen it written in my course notes. Is it also true that the order of a pole is the algebraic multiplicity wrt to the denominator?","I have come up with the following formula for the order of a pole of some function at a point . It involves first taking the multiplicative inverse (denoted by to avoid getting mixed up with the inverse of the function), then finding the smallest such that the derivative is not equal to zero. Does this equation hold? It works for my naive test of . However I haven't seen it written in my course notes. Is it also true that the order of a pole is the algebraic multiplicity wrt to the denominator?","f(z)=\frac{g(z)}{h(z)} z = c 1/f(z) n 
\operatorname{ord}(f, c)=\min \{ n \in \mathbb N:(1/f)^{(n)}(c)\neq 0 \}
 f((z-2)^{-3}, 2)",['complex-analysis']
83,Understanding convergence in space of continuous functions on an open set,Understanding convergence in space of continuous functions on an open set,,"I am reading Conway's Complex Analysis and chapter $7$ talks about the space of continuous functions on an open set $G \subset \mathbf{C}$ into a metric space $(\Omega,d)$ , denoted by $C(G,\Omega)$ . The topology is later introduced as follows: A set $U \in C(G,\Omega)$ is open if and only if for each $f \in U$ there exists a compact set $K$ and a $\delta >0$ such that $\{g: \sup_{z \in K} d(f(z),g(z))<\delta\} \subset U$ . Using an exhaustion of $G$ by compact sets $K_n$ and letting $\rho_n(f,g)=\sup_{z \in K_n} d(f(z),g(z))$ we get a metric $\rho$ on $C(G,\Omega) $ where $$\rho(f,g)=\sum_{n=1}^\infty \frac{1}{2^n}\frac{\rho_n(f,g)}{1+\rho_n(f,g)}$$ Building on that is the following true for convergence in $C(G,\Omega)$ ? Let $ \epsilon >0$ be arbitrary. Then $\rho(f,g)<\epsilon$ if and only if for each compact set $K$ in $G$ , there exists a $\delta>0$ such that $\sup_{z \in K} d(f(z),g(z))<\delta$ .","I am reading Conway's Complex Analysis and chapter talks about the space of continuous functions on an open set into a metric space , denoted by . The topology is later introduced as follows: A set is open if and only if for each there exists a compact set and a such that . Using an exhaustion of by compact sets and letting we get a metric on where Building on that is the following true for convergence in ? Let be arbitrary. Then if and only if for each compact set in , there exists a such that .","7 G \subset \mathbf{C} (\Omega,d) C(G,\Omega) U \in C(G,\Omega) f \in U K \delta >0 \{g: \sup_{z \in K} d(f(z),g(z))<\delta\} \subset U G K_n \rho_n(f,g)=\sup_{z \in K_n} d(f(z),g(z)) \rho C(G,\Omega)  \rho(f,g)=\sum_{n=1}^\infty \frac{1}{2^n}\frac{\rho_n(f,g)}{1+\rho_n(f,g)} C(G,\Omega)  \epsilon >0 \rho(f,g)<\epsilon K G \delta>0 \sup_{z \in K} d(f(z),g(z))<\delta","['complex-analysis', 'metric-spaces']"
84,A complex analysis proof of the extremal case of Bernstein's inequality?,A complex analysis proof of the extremal case of Bernstein's inequality?,,"Bernstein's inequality states: Given any degree- $n$ polynomial $P(z)$ with complex coefficients, we have \begin{align} \sup_{|z| \leq 1}  |P'(z)|  \leq  n\,\sup_{|z| \leq 1}  |P(z)|.  \label{Binequality} \end{align} Moreover, the equality holds if and only if $P(z)=\lambda z^n$ for some nonzero constant $\lambda$ . There is a simple proof of the inequality by Rouch'e and Gauss-Lucas theorem, see here . My understanding is that the above proof is not sufficient to conclude the equality case. Instead, there is a proof which makes use of the Fejer kernel of complex Fourier series to relate $P(z)$ and $P'(z)$ , it proves both the inequality and the extremal case, see Theorem 14.1.1 on p.508 of the following book. Rahman, Q. I.; Schmeisser, G.. Analytic theory of polynomials. London Mathematical Society Monographs. New Series, 26. The Clarendon Press, Oxford University Press, 2002. Question: Is there a complex analysis proof of the extremal case? I am new to Bernstein's inequality and must miss some references on this famous inequality. I would appreciate anyone who suggests a (complex analysis) proof or some references.","Bernstein's inequality states: Given any degree- polynomial with complex coefficients, we have Moreover, the equality holds if and only if for some nonzero constant . There is a simple proof of the inequality by Rouch'e and Gauss-Lucas theorem, see here . My understanding is that the above proof is not sufficient to conclude the equality case. Instead, there is a proof which makes use of the Fejer kernel of complex Fourier series to relate and , it proves both the inequality and the extremal case, see Theorem 14.1.1 on p.508 of the following book. Rahman, Q. I.; Schmeisser, G.. Analytic theory of polynomials. London Mathematical Society Monographs. New Series, 26. The Clarendon Press, Oxford University Press, 2002. Question: Is there a complex analysis proof of the extremal case? I am new to Bernstein's inequality and must miss some references on this famous inequality. I would appreciate anyone who suggests a (complex analysis) proof or some references.","n P(z) \begin{align}
\sup_{|z| \leq 1}  |P'(z)|  \leq  n\,\sup_{|z| \leq 1}  |P(z)|.  \label{Binequality}
\end{align} P(z)=\lambda z^n \lambda P(z) P'(z)","['real-analysis', 'complex-analysis', 'polynomials']"
85,Why is the degree of a linear holomorphism $[z] \mapsto [cz]$ from the complex torus $X=\mathbb{C}/\Lambda$ to itself equal to |c|^2?,Why is the degree of a linear holomorphism  from the complex torus  to itself equal to |c|^2?,[z] \mapsto [cz] X=\mathbb{C}/\Lambda,"Why is the degree of a linear holomorphism $f:X\rightarrow X$ such that $[z] \mapsto [cz]$ from the complex torus $X=\mathbb{C}/\Lambda$ to itself equal to |c|^2? (here we assume that $\Lambda=\mathbb{Z}+\tau \mathbb{Z}$ with $\operatorname{Im}(\tau)>0$ , $c\Lambda \subseteq \Lambda$ , and $[z]$ designs the equivalence class of $z$ in $X$ ). This is mentioned on this answer , but I haven’t been able to find the reason by myself. I’ve tried to compute it by using $\deg(f)=\sum_{p\in f^{-1}({z})}\operatorname{mult}_p(f)$ where $f([z])=[cz]$ and hence $f^{-1}([z])=[c^{-1}z]$ , but I’m stuck at trying to find $\operatorname{mult}_p(f)$ for the points in $f^{-1}([z])$ here. Edit: I already understand why $\operatorname{mult}_p(f)=1$ for all $p\in f^{-1}([z])$ but I don’t see why should $|f^{-1}([z])|=|c|^2$ .","Why is the degree of a linear holomorphism such that from the complex torus to itself equal to |c|^2? (here we assume that with , , and designs the equivalence class of in ). This is mentioned on this answer , but I haven’t been able to find the reason by myself. I’ve tried to compute it by using where and hence , but I’m stuck at trying to find for the points in here. Edit: I already understand why for all but I don’t see why should .",f:X\rightarrow X [z] \mapsto [cz] X=\mathbb{C}/\Lambda \Lambda=\mathbb{Z}+\tau \mathbb{Z} \operatorname{Im}(\tau)>0 c\Lambda \subseteq \Lambda [z] z X \deg(f)=\sum_{p\in f^{-1}({z})}\operatorname{mult}_p(f) f([z])=[cz] f^{-1}([z])=[c^{-1}z] \operatorname{mult}_p(f) f^{-1}([z]) \operatorname{mult}_p(f)=1 p\in f^{-1}([z]) |f^{-1}([z])|=|c|^2,"['complex-analysis', 'riemann-surfaces']"
86,Question about divisors on a compact Riemann surface,Question about divisors on a compact Riemann surface,,"I am trying to prove the following question from here without relying on the Riemann-Roch theorem. Let $X$ be a compact Riemann surface, and let $D$ be a divisor on $X$ . (i) If $\mathrm{deg}(D) = 0$ , show that $\mathrm{dim}(L(D))$ is equal to $0$ or $1$ , with the latter occurring if and only if $D$ is principal. Furthermore, any non-zero element of $L(D)$ has divisor $-D$ . (ii) If $\mathrm{deg}(D) \geq 0$ , establish the bound $\mathrm{dim}(L(D)) \leq \mathrm{deg}(D) + 1$ . Here is my work so far: If $D = 0$ then we have that $L(0) = 0$ as $X$ is compact, with the opening mapping theorem, implies that $L(0)$ is the space of constant functions. However, I am not sure what to do even for the simple case where $D = (P) - (Q)$ . any $f \in L(D)$ must have at least a simple zero at $Q$ and at most a simple pole at $P$ . How can I get further information about $f$ ? Similarly, the only one that I could prove so far for (ii) is the case when $D$ is trivial. Thank you so much for your help.","I am trying to prove the following question from here without relying on the Riemann-Roch theorem. Let be a compact Riemann surface, and let be a divisor on . (i) If , show that is equal to or , with the latter occurring if and only if is principal. Furthermore, any non-zero element of has divisor . (ii) If , establish the bound . Here is my work so far: If then we have that as is compact, with the opening mapping theorem, implies that is the space of constant functions. However, I am not sure what to do even for the simple case where . any must have at least a simple zero at and at most a simple pole at . How can I get further information about ? Similarly, the only one that I could prove so far for (ii) is the case when is trivial. Thank you so much for your help.",X D X \mathrm{deg}(D) = 0 \mathrm{dim}(L(D)) 0 1 D L(D) -D \mathrm{deg}(D) \geq 0 \mathrm{dim}(L(D)) \leq \mathrm{deg}(D) + 1 D = 0 L(0) = 0 X L(0) D = (P) - (Q) f \in L(D) Q P f D,"['complex-analysis', 'riemann-surfaces', 'divisors-algebraic-geometry', 'meromorphic-functions']"
87,When is the following integral zero?,When is the following integral zero?,,"We assume we have two functions $f,g:\mathbb{R} \rightarrow \mathbb{R}$ , $d$ some integer. I have the following integral expression $$\forall b_{1},b_{2}  \leq d : \int_{-\delta}^{\delta}[ \int_0^t f(s)^{b_{1}}g(s)^{b_{2}} ds \cdot ( \int_0^t \sum_{i_{1},i_{2}=1}^{d}a_{i_{1},i_{2}}{f(s')}^{i_{1}}{g(s')}^{i_{2}}ds')]dt=0$$ My question is the following, can I conclude that the only way this integral can become zero is by $$ \sum_{i_{1},i_{2}=1}^{d}a_{i_{1},i_{2}}{f(s')}^{i_{1}}{g(s')}^{i_{2}}=0$$ being zero already? My attempts: What I already know is, that we can choose $g(s)^2f(s)^2$ which is a positive function. Using the mean-value theorem (these $f,g$ have nothing to do with the ones above) $$\int_{a}^{b} f(x) g(x) d x=f(c) \int_{a}^{b} g(x) d x$$ we can conclude that the integral only can become zero if $$\int_{-\delta}^{\delta} \int_0^t \sum_{i_{1},i_{2}=1}^{d}a_{i_{1},i_{2}}{f(s')}^{i_{1}}{g(s')}^{i_{2}}ds'dt=0$$ is already zero, But here I am stuck.","We assume we have two functions , some integer. I have the following integral expression My question is the following, can I conclude that the only way this integral can become zero is by being zero already? My attempts: What I already know is, that we can choose which is a positive function. Using the mean-value theorem (these have nothing to do with the ones above) we can conclude that the integral only can become zero if is already zero, But here I am stuck.","f,g:\mathbb{R} \rightarrow \mathbb{R} d \forall b_{1},b_{2}  \leq d : \int_{-\delta}^{\delta}[ \int_0^t f(s)^{b_{1}}g(s)^{b_{2}} ds \cdot ( \int_0^t \sum_{i_{1},i_{2}=1}^{d}a_{i_{1},i_{2}}{f(s')}^{i_{1}}{g(s')}^{i_{2}}ds')]dt=0  \sum_{i_{1},i_{2}=1}^{d}a_{i_{1},i_{2}}{f(s')}^{i_{1}}{g(s')}^{i_{2}}=0 g(s)^2f(s)^2 f,g \int_{a}^{b} f(x) g(x) d x=f(c) \int_{a}^{b} g(x) d x \int_{-\delta}^{\delta} \int_0^t \sum_{i_{1},i_{2}=1}^{d}a_{i_{1},i_{2}}{f(s')}^{i_{1}}{g(s')}^{i_{2}}ds'dt=0","['real-analysis', 'integration', 'sequences-and-series', 'complex-analysis', 'functions']"
88,If two elliptic functions share the same poles and zeros (including multiciplity) then they are proportional,If two elliptic functions share the same poles and zeros (including multiciplity) then they are proportional,,"I’m trying to understand the following statement found on my lecture notes: If two elliptic functions share the same poles and zeros (including multiciplity) then they are proportional I’m trying to reason the following way: if $f_1$ and $f_2$ are not constant, they must be meromorphic (since holomorphic elliptic functions are constant). Then they can be written as a rational function of two polynomials. I could now write the denominator and numerator of each of them as a product of linear factors (right?). So $f_i=g_i/h_i$ where the zeros of $g_i$ are the zeros of $f_i$ and the zeros of $h_i$ are the poles of $f_i$ . Finally, if $f_1$ and $f_2$ have the same zeros and poles then their numerators and denominators can only differ by a multiplicative constant respectively. Is that correct?","I’m trying to understand the following statement found on my lecture notes: If two elliptic functions share the same poles and zeros (including multiciplity) then they are proportional I’m trying to reason the following way: if and are not constant, they must be meromorphic (since holomorphic elliptic functions are constant). Then they can be written as a rational function of two polynomials. I could now write the denominator and numerator of each of them as a product of linear factors (right?). So where the zeros of are the zeros of and the zeros of are the poles of . Finally, if and have the same zeros and poles then their numerators and denominators can only differ by a multiplicative constant respectively. Is that correct?",f_1 f_2 f_i=g_i/h_i g_i f_i h_i f_i f_1 f_2,"['complex-analysis', 'riemann-surfaces', 'elliptic-functions', 'meromorphic-functions']"
89,Compute the limit $\lim_{z \to ia}\arctan(\frac{y}{x})$,Compute the limit,\lim_{z \to ia}\arctan(\frac{y}{x}),"Let $a\in \mathbb R$ and $z=x+iy$ . Compute the limit $$\lim_{z \to ia}\arctan\left(\frac{y}{x}\right).$$ My try: $$\lim_{z \to ia}\arctan\left(\frac{y}{x}\right)=\lim_{z \to ia}\arctan\left(\frac{\operatorname{Im}(z)}{\operatorname{Re}(z)}\right),$$ so $$\lim_{z \to ia}\arctan\left(\frac{y}{x}\right) = \begin{cases} \pi/2 , & \text{if $a>0$}, \\[2ex] -\pi/2 , & \text{if $a<0$} . \end{cases}$$ Is this correct? Thank you very much!",Let and . Compute the limit My try: so Is this correct? Thank you very much!,"a\in \mathbb R z=x+iy \lim_{z \to ia}\arctan\left(\frac{y}{x}\right). \lim_{z \to ia}\arctan\left(\frac{y}{x}\right)=\lim_{z \to ia}\arctan\left(\frac{\operatorname{Im}(z)}{\operatorname{Re}(z)}\right), \lim_{z \to ia}\arctan\left(\frac{y}{x}\right) =
\begin{cases}
\pi/2 , & \text{if a>0}, \\[2ex]
-\pi/2 , & \text{if a<0} .
\end{cases}","['complex-analysis', 'limits']"
90,Determining the branch of the complex argument,Determining the branch of the complex argument,,"I am working on a complex analysis problem and I am stuck at a particular section. Basically, we are given a region in which the argument of a complex number, $\arg(z)$ , is defined, and its value at a given point. We are then asked to find the argument of other complex numbers based on this information. The point in which I am stuck is when working in $\mathbb{C} \setminus \{re^{i\frac{\pi}{4}}: r \geq 0\}$ and I am given $\arg(1)=0$ , and I need to find $\arg(i)$ . I thought that the ""origin line"" for measuring the angle of a complex number in this set was the reflection through the origin of the removed line: $\{re^{i(\frac{\pi}{4} + \pi)}: r \geq 0\}$ . From there, I calculated the argument of 1 from this line: $$ \arg(1) = \frac{3\pi}{4} + 2\pi n \quad , n\in \mathbb{Z} $$ and imposed $\arg(1) = 0$ to find $n$ , but I get nonsense: $n = -3/8$ . Where is the flaw in this argument? Perhaps I am wrong when defining the argument in $\mathbb{C} \setminus \{re^{i\frac{\pi}{4}}: r \geq 0\}$ this way? By the way, the answer turns out to be $\arg(i) = -\frac{3\pi}{2}$ . Any help will be appreciated.","I am working on a complex analysis problem and I am stuck at a particular section. Basically, we are given a region in which the argument of a complex number, , is defined, and its value at a given point. We are then asked to find the argument of other complex numbers based on this information. The point in which I am stuck is when working in and I am given , and I need to find . I thought that the ""origin line"" for measuring the angle of a complex number in this set was the reflection through the origin of the removed line: . From there, I calculated the argument of 1 from this line: and imposed to find , but I get nonsense: . Where is the flaw in this argument? Perhaps I am wrong when defining the argument in this way? By the way, the answer turns out to be . Any help will be appreciated.","\arg(z) \mathbb{C} \setminus \{re^{i\frac{\pi}{4}}: r \geq 0\} \arg(1)=0 \arg(i) \{re^{i(\frac{\pi}{4} + \pi)}: r \geq 0\} 
\arg(1) = \frac{3\pi}{4} + 2\pi n \quad , n\in \mathbb{Z}
 \arg(1) = 0 n n = -3/8 \mathbb{C} \setminus \{re^{i\frac{\pi}{4}}: r \geq 0\} \arg(i) = -\frac{3\pi}{2}","['complex-analysis', 'complex-numbers']"
91,Verify Complex Solutions of Riemann Zeta Function?,Verify Complex Solutions of Riemann Zeta Function?,,"I've recently been trying to visualize the Riemann Zeta complex function by graphing it on a 3D plot. I wrote some code to compute the value of the following integral representation of the Riemann Zeta function, which should be defined in Re (Input) $\neq$ 1: $$\zeta(s) = \frac{2^{s-1}}{s-1} -2^{s} \int_{0}^{\infty}\frac{\sin(s\arctan(x))}{(1+x^{2})^{\frac{s}{2}}(e^{\pi x}+1)}dx$$ Where s is the complex Input. Through the use of GNUPlot's code libraries, I graphed the following: Re (Input) - X-axis Im (Input) - Y-axis Re (Output) - Z-axis Im (Output) - Color As far as I understand, this is one of the standard ways to visualize & graph complex functions. With this setup I'm able to generate some plots: Plots above depicts the function on -5 < Re (Input) < 5 and -5 < Im (Input) < 5. (forgive the messed up colorbar label on the right, it appears that C++'s GNUPlot library is quite old...) Plots above depicts the function on -30 < Re (Input) < 30 and -30 < Im (Input) < 30. So far, so good. However, I want to be able to verify whether my code produces the correct answers for this representation of the RZ function. I'm able to compare some of my answers to some of the known answers out there, such as any answer with Re (Input) > 1, real negative integers and discovered nontrivial zeroes , and those all appear to check out. However, apart from those, I haven't really been able to find a good way of reliably verifying the rest of my answers here. Most popular online function graphing calculators either fail to compute that integral function (such as Desmos ), are unable to accept complex inputs (such as GeoGebra ), or only have static plots of it (such as Wolfram ). Since I'm not a mathematician, and since I'm not aware of all the possible resources out there or methods for verifying solutions for complex functions, I wanted to ask that here. What would be some good ways of verifying solutions to complex functions, such as the Riemann Zeta function? Is there a tool that is used when computing complex functions such as these, for verifying solutions? Thanks for reading my post, any guidance is appreciated.","I've recently been trying to visualize the Riemann Zeta complex function by graphing it on a 3D plot. I wrote some code to compute the value of the following integral representation of the Riemann Zeta function, which should be defined in Re (Input) 1: Where s is the complex Input. Through the use of GNUPlot's code libraries, I graphed the following: Re (Input) - X-axis Im (Input) - Y-axis Re (Output) - Z-axis Im (Output) - Color As far as I understand, this is one of the standard ways to visualize & graph complex functions. With this setup I'm able to generate some plots: Plots above depicts the function on -5 < Re (Input) < 5 and -5 < Im (Input) < 5. (forgive the messed up colorbar label on the right, it appears that C++'s GNUPlot library is quite old...) Plots above depicts the function on -30 < Re (Input) < 30 and -30 < Im (Input) < 30. So far, so good. However, I want to be able to verify whether my code produces the correct answers for this representation of the RZ function. I'm able to compare some of my answers to some of the known answers out there, such as any answer with Re (Input) > 1, real negative integers and discovered nontrivial zeroes , and those all appear to check out. However, apart from those, I haven't really been able to find a good way of reliably verifying the rest of my answers here. Most popular online function graphing calculators either fail to compute that integral function (such as Desmos ), are unable to accept complex inputs (such as GeoGebra ), or only have static plots of it (such as Wolfram ). Since I'm not a mathematician, and since I'm not aware of all the possible resources out there or methods for verifying solutions for complex functions, I wanted to ask that here. What would be some good ways of verifying solutions to complex functions, such as the Riemann Zeta function? Is there a tool that is used when computing complex functions such as these, for verifying solutions? Thanks for reading my post, any guidance is appreciated.",\neq \zeta(s) = \frac{2^{s-1}}{s-1} -2^{s} \int_{0}^{\infty}\frac{\sin(s\arctan(x))}{(1+x^{2})^{\frac{s}{2}}(e^{\pi x}+1)}dx,"['complex-analysis', 'riemann-zeta']"
92,Ash - Complex Variables - Sufficient conditions for the absolute and uniform convergence of the infinite product of functions,Ash - Complex Variables - Sufficient conditions for the absolute and uniform convergence of the infinite product of functions,,"I am self studying Ash & Novinger's Complex Variables. The authors prove the following theorem (see Page 4, Subsection 6.1.6) : Proposition. Let $g_1 , g_2, \ldots$ be a sequence of bounded complex valued functions, each defined on a  set $S$ . If the series $\sum_{n=1}^{\infty} \lvert g_n \rvert$ converges uniformly on $S$ , then the product $\prod_{n=1}^{\infty} (1+g_n)$ converges absolutely and uniformly on $S$ . Furthermore, if $f(z)=\prod_{n=1}^{\infty}  (1+g_n(z)), z\in S$ , then $f(z)=0$ iff $1+g_n(z)=0$ for some $n\in  \mathbb N$ . Is the bounded condition redundant? The proof never makes use of it, in fact, uniform convergence of $\sum_{n=1}^{\infty} |g_n|$ implies the boundedness of $g_n$ 's after a certain stage. Here's how it can be show: Let $\sigma_n (x) = \sum_{k=1}^{n} |g_k(z)|$ . Since $\sigma_n$ converges uniformly, it is uniformly Cauchy ( see here ). Thus, there is some $K \in \mathbb N$ such that for every $m,n \ge K$ , we have that $|\sigma_{m} (x) - \sigma_{n} (x)| < 1$ . The result is now immediate by choosing $m=n+1$ for each $n \ge N$ . Since the first few terms cannot affect convergence, we may in fact get rid of them. So what is the point of boundedness here?","I am self studying Ash & Novinger's Complex Variables. The authors prove the following theorem (see Page 4, Subsection 6.1.6) : Proposition. Let be a sequence of bounded complex valued functions, each defined on a  set . If the series converges uniformly on , then the product converges absolutely and uniformly on . Furthermore, if , then iff for some . Is the bounded condition redundant? The proof never makes use of it, in fact, uniform convergence of implies the boundedness of 's after a certain stage. Here's how it can be show: Let . Since converges uniformly, it is uniformly Cauchy ( see here ). Thus, there is some such that for every , we have that . The result is now immediate by choosing for each . Since the first few terms cannot affect convergence, we may in fact get rid of them. So what is the point of boundedness here?","g_1 , g_2, \ldots S \sum_{n=1}^{\infty} \lvert g_n \rvert S \prod_{n=1}^{\infty} (1+g_n) S f(z)=\prod_{n=1}^{\infty}
 (1+g_n(z)), z\in S f(z)=0 1+g_n(z)=0 n\in
 \mathbb N \sum_{n=1}^{\infty} |g_n| g_n \sigma_n (x) = \sum_{k=1}^{n} |g_k(z)| \sigma_n K \in \mathbb N m,n \ge K |\sigma_{m} (x) - \sigma_{n} (x)| < 1 m=n+1 n \ge N","['complex-analysis', 'infinite-product', 'weierstrass-factorization']"
93,$0\leq u\leq P_r\implies u=\lambda P_r$,,0\leq u\leq P_r\implies u=\lambda P_r,"Let $u$ be a real-valued harmonic function on the open unit disk such that $0\leq u(r,\theta)\leq P_r(\theta)$ for all $0\leq r<1$ and $\theta$ , where $P_r(\theta)$ denotes the Poisson kernel. How can I deduce from this that $u(r,\theta)=\lambda P_r(\theta)$ for some constant $\lambda$ ? If the claim is true, then certainly $\lambda=u(0,0)$ , so we have a candidate value for $\lambda$ . But I'm not sure how to deduce $u(r,\theta)=\lambda P_r(\theta)$ on the whole unit disk.","Let be a real-valued harmonic function on the open unit disk such that for all and , where denotes the Poisson kernel. How can I deduce from this that for some constant ? If the claim is true, then certainly , so we have a candidate value for . But I'm not sure how to deduce on the whole unit disk.","u 0\leq u(r,\theta)\leq P_r(\theta) 0\leq r<1 \theta P_r(\theta) u(r,\theta)=\lambda P_r(\theta) \lambda \lambda=u(0,0) \lambda u(r,\theta)=\lambda P_r(\theta)","['complex-analysis', 'analysis', 'harmonic-analysis', 'harmonic-functions']"
94,"Normal Family of Entire Functions, Convergence","Normal Family of Entire Functions, Convergence",,"I am looking for hints for this old prelim exam question. Let $\mathcal{F}$ be a normal family of entire functions, and let $$A_n=\sup\left\{|a_n|:f(z)=\sum_{j=0}^{\infty}a_jz^j,f\in\mathcal{F}\right\}$$ for $n=0,1,2,\ldots.$ Show that $\sum_{n=0}^{\infty}A_nr^n<\infty$ for all $r>0.$ My first step is to recognize that $A_n=\frac{1}{n!}\sup_{f\in\mathcal{F}}|f^{(n)}(0)|.$ For each $n,$ there exists a sequence of functions $(f_{n,k})_{k=1}^{\infty}$ in $\mathcal{F}$ such that $$\lim_{k\to\infty}\frac{\left|f_{n,k}^{(n)}(0)\right|}{n!}=A_n.$$ Now since $\mathcal{F}$ is a normal family, the sequence $(f_{n,k})_{k=1}^{\infty}$ admits a subsequence $\left(f_{n,k_j}\right)_{j=1}^{\infty}$ such that there exists a function $g_n\colon \mathbb{C}\to\mathbb{C}$ such that $f_{n,k_j}\xrightarrow[j\to\infty]{}g_n$ uniformly on compact sets. Then it can be shown that $g_n$ is entire for each $n,$ and $f_{n,k_j}^{(m)}\xrightarrow[j\to\infty]{}g_n^{(m)}$ uniformly on compact sets for all orders $m\in \mathbb{N},$ in particular $f_{n,k_j}^{(n)}\xrightarrow[j\to\infty]{}g_n^{(n)}$ uniformly on compact sets. It follows that $\frac{\left|g_n^{(n)}(0)\right|}{n!}=A_n.$ My hope is to use the functions $g_n$ find an entire function $h$ with the property that $h^{(n)}=g_n^{(n)}$ for all orders $n,$ but don't see how to do this. Does this seem to be a good strategy, or is there a better approach I am missing? I feel that I am not making enough use of the fact that $\mathcal{F}$ consists of entire functions - is there a way to use this fact here?","I am looking for hints for this old prelim exam question. Let be a normal family of entire functions, and let for Show that for all My first step is to recognize that For each there exists a sequence of functions in such that Now since is a normal family, the sequence admits a subsequence such that there exists a function such that uniformly on compact sets. Then it can be shown that is entire for each and uniformly on compact sets for all orders in particular uniformly on compact sets. It follows that My hope is to use the functions find an entire function with the property that for all orders but don't see how to do this. Does this seem to be a good strategy, or is there a better approach I am missing? I feel that I am not making enough use of the fact that consists of entire functions - is there a way to use this fact here?","\mathcal{F} A_n=\sup\left\{|a_n|:f(z)=\sum_{j=0}^{\infty}a_jz^j,f\in\mathcal{F}\right\} n=0,1,2,\ldots. \sum_{n=0}^{\infty}A_nr^n<\infty r>0. A_n=\frac{1}{n!}\sup_{f\in\mathcal{F}}|f^{(n)}(0)|. n, (f_{n,k})_{k=1}^{\infty} \mathcal{F} \lim_{k\to\infty}\frac{\left|f_{n,k}^{(n)}(0)\right|}{n!}=A_n. \mathcal{F} (f_{n,k})_{k=1}^{\infty} \left(f_{n,k_j}\right)_{j=1}^{\infty} g_n\colon \mathbb{C}\to\mathbb{C} f_{n,k_j}\xrightarrow[j\to\infty]{}g_n g_n n, f_{n,k_j}^{(m)}\xrightarrow[j\to\infty]{}g_n^{(m)} m\in \mathbb{N}, f_{n,k_j}^{(n)}\xrightarrow[j\to\infty]{}g_n^{(n)} \frac{\left|g_n^{(n)}(0)\right|}{n!}=A_n. g_n h h^{(n)}=g_n^{(n)} n, \mathcal{F}",['complex-analysis']
95,Why is there no negative infinity in the extended complex plane?,Why is there no negative infinity in the extended complex plane?,,"I'm reading Ravi Agarwal's ""Introduction to Complex Analysis"". He says this: It is often convenient to add the element $\infty$ to $\mathbb{C}$ . The enlarged set $\mathbb{C} \cup \{\infty\}$ is called the extended complex plane. Unlike the extended real line, there is no $-\infty$ . He uses infinity to discuss the idea of a ""neighbourhood of infinity"", but he defines them without the need for infinity as a set of complex numbers $z$ following $\{z:|z-z_0|>r>0\}$ . What's the point? Why is adding the element $\infty$ convenient? Is there not a better way to discuss the concept of ""neighbourhoods of infinity""? And why not add $-\infty$ ?","I'm reading Ravi Agarwal's ""Introduction to Complex Analysis"". He says this: It is often convenient to add the element to . The enlarged set is called the extended complex plane. Unlike the extended real line, there is no . He uses infinity to discuss the idea of a ""neighbourhood of infinity"", but he defines them without the need for infinity as a set of complex numbers following . What's the point? Why is adding the element convenient? Is there not a better way to discuss the concept of ""neighbourhoods of infinity""? And why not add ?",\infty \mathbb{C} \mathbb{C} \cup \{\infty\} -\infty z \{z:|z-z_0|>r>0\} \infty -\infty,"['complex-analysis', 'elementary-set-theory', 'recreational-mathematics']"
96,Roots and analytic continuation of $\zeta(s)=\sum_{n>0} \frac{H_n^{-s}}{n} $?,Roots and analytic continuation of ?,\zeta(s)=\sum_{n>0} \frac{H_n^{-s}}{n} ,Let $$\zeta(s)=\sum_{n>0} \frac{H_n^{-s}}{n} $$ Where $H_n$ are the harmonic numbers. This is well defined for $\Re (s)>1$ . But what about analytic continuation? And where is $$\zeta(s) = 0$$ ?? Is $\Re(s) = 1$ a natural boundary ? And can we continue the function beyond $\Re(s) > 1$ ? Plots are appreciated too :),Let Where are the harmonic numbers. This is well defined for . But what about analytic continuation? And where is ?? Is a natural boundary ? And can we continue the function beyond ? Plots are appreciated too :),\zeta(s)=\sum_{n>0} \frac{H_n^{-s}}{n}  H_n \Re (s)>1 \zeta(s) = 0 \Re(s) = 1 \Re(s) > 1,"['complex-analysis', 'roots', 'zeta-functions', 'analytic-continuation']"
97,Show that $\omega \mapsto \int_A \lambda^{i\omega}d\mu(\lambda)$ is analytic,Show that  is analytic,\omega \mapsto \int_A \lambda^{i\omega}d\mu(\lambda),"Let $\alpha\in \mathbb{R}$ and $D_\alpha$ be the open horizontal strip in the complex plane $\mathbb{C}$ bounded by $\mathbb{R}$ and $\mathbb{R}-i\alpha$ . Let $\mu$ be a complex measure on $[0, \infty[$ and consider the function $$D_\alpha \ni \omega \mapsto \int_{1/k}^k \lambda^{i\omega}d\mu(\lambda)$$ where $k\in \mathbb{N}$ is fixed. I want to show that this function is analytic. Since every complex measure is a linear combination of finite positive measures, we may assume that $\mu$ is a finite positive measure. Consider a sequence $\{\omega_n\}\subseteq D_\alpha$ and $\omega\in D_\alpha$ with $\omega_n \ne \omega$ for all $n$ and $\omega_n\to \omega$ . It suffices to show that $$\lim_{n \to \infty} \int_{1/k}^k \frac{\lambda^{i\omega_n}-\lambda^{i\omega}}{\omega_n - \omega}d\mu(\lambda) = \int_{1/k}^k i \operatorname{Log}(\lambda) \lambda^{i\omega}d \mu(\lambda)$$ so basically we need to justify why we can interchange limit and integral. For this, I wanted to apply the dominated convergence theorem. However, then I have to argue that there exists an integrable function $g$ on $[1/k,k]$ such that $$\left|\frac{\lambda^{i\omega_n}-\lambda^{i\omega}}{\omega_n - \omega}\right|\le g(\lambda).$$ How do I find this function? Thanks in advance for your help/comments!","Let and be the open horizontal strip in the complex plane bounded by and . Let be a complex measure on and consider the function where is fixed. I want to show that this function is analytic. Since every complex measure is a linear combination of finite positive measures, we may assume that is a finite positive measure. Consider a sequence and with for all and . It suffices to show that so basically we need to justify why we can interchange limit and integral. For this, I wanted to apply the dominated convergence theorem. However, then I have to argue that there exists an integrable function on such that How do I find this function? Thanks in advance for your help/comments!","\alpha\in \mathbb{R} D_\alpha \mathbb{C} \mathbb{R} \mathbb{R}-i\alpha \mu [0, \infty[ D_\alpha \ni \omega \mapsto \int_{1/k}^k \lambda^{i\omega}d\mu(\lambda) k\in \mathbb{N} \mu \{\omega_n\}\subseteq D_\alpha \omega\in D_\alpha \omega_n \ne \omega n \omega_n\to \omega \lim_{n \to \infty} \int_{1/k}^k \frac{\lambda^{i\omega_n}-\lambda^{i\omega}}{\omega_n - \omega}d\mu(\lambda) = \int_{1/k}^k i \operatorname{Log}(\lambda) \lambda^{i\omega}d \mu(\lambda) g [1/k,k] \left|\frac{\lambda^{i\omega_n}-\lambda^{i\omega}}{\omega_n - \omega}\right|\le g(\lambda).","['complex-analysis', 'functional-analysis', 'measure-theory', 'derivatives', 'analytic-functions']"
98,Analytic continuation of $f(z) = \sum_{n=1}^{\infty} \frac{1}{2^n (z-e^{2\pi i r_n})}$,Analytic continuation of,f(z) = \sum_{n=1}^{\infty} \frac{1}{2^n (z-e^{2\pi i r_n})},"Let $\{r_k:k\in\mathbb{N}\}$ be a counting set of rational numbers in $[0,1]$ . Show that $$f(z) = \sum_{k=1}^{\infty} \frac{1}{2^k (z-e^{2\pi i r_k})}$$ is holomorphic on $U_1=\{z\in \mathbb{C} : |z| < 1\}$ and $U_2 = \{z\in \mathbb{C} : |z|>1\}$ . Are both functions analytic continuations from each other? I've already learned that if $(f_n)_{n\in \mathbb{N}}$ is a compactly convergent sequence of holomorphic functions and $f$ its limit, then $f$ is also holomorphic. Taking $$f_n(z) = \sum_{k=1}^{n} \frac{1}{2^k (z-e^{2\pi i r_k})}$$ it would make sense that it's enough to prove that this series is uniformly convergent on $U_1, U_2$ . For $k\in \mathbb{N}$ : $\frac{1}{2^k(z-e^{2\pi i r_k})}$ is holomorphic because we know that $\frac{1}{z-\exp(2\pi ir_k)}$ is holomorphic in $z\in \mathbb{C}\setminus \{\exp(2\pi i r_k)\}$ and $\exp(2\pi i r_k)$ is neither in $U_1$ nor in $U_2$ $\forall k$ . Let $z\in U_1$ and $R = \inf\{|z-\exp(2\pi i r_k)|: k\in \mathbb{N}\}$ : \begin{align*} |f_n(z)-f(z)| = |\sum_{k=n}^{\infty} \frac{1}{2^k(z-e^{2\pi i r_k})}| \leq \sum_{k=n}^{\infty} \frac{1}{2^k|(z-e^{2\pi i r_k})|} \leq \sum_{k=n}^{\infty} \frac{1}{2^k R} \overset{n\rightarrow \infty}{\longrightarrow} 0 \end{align*} Analogous with $z\in U_2$ and $R = \inf\{|z-\exp(2\pi i r_k)|: k\in \mathbb{N}\}$ . With that I should have proven that $f(z)$ is holomorphic on $U_1$ and $U_2$ , right? Now to my main question: What does ""are both functions analytic continuations from each other"" mean in this context? $U_1\cap U_2 = \emptyset$ and they're essentially the same function, so yes?","Let be a counting set of rational numbers in . Show that is holomorphic on and . Are both functions analytic continuations from each other? I've already learned that if is a compactly convergent sequence of holomorphic functions and its limit, then is also holomorphic. Taking it would make sense that it's enough to prove that this series is uniformly convergent on . For : is holomorphic because we know that is holomorphic in and is neither in nor in . Let and : Analogous with and . With that I should have proven that is holomorphic on and , right? Now to my main question: What does ""are both functions analytic continuations from each other"" mean in this context? and they're essentially the same function, so yes?","\{r_k:k\in\mathbb{N}\} [0,1] f(z) = \sum_{k=1}^{\infty} \frac{1}{2^k (z-e^{2\pi i r_k})} U_1=\{z\in \mathbb{C} : |z| < 1\} U_2 = \{z\in \mathbb{C} : |z|>1\} (f_n)_{n\in \mathbb{N}} f f f_n(z) = \sum_{k=1}^{n} \frac{1}{2^k (z-e^{2\pi i r_k})} U_1, U_2 k\in \mathbb{N} \frac{1}{2^k(z-e^{2\pi i r_k})} \frac{1}{z-\exp(2\pi ir_k)} z\in \mathbb{C}\setminus \{\exp(2\pi i r_k)\} \exp(2\pi i r_k) U_1 U_2 \forall k z\in U_1 R = \inf\{|z-\exp(2\pi i r_k)|: k\in \mathbb{N}\} \begin{align*}
|f_n(z)-f(z)| = |\sum_{k=n}^{\infty} \frac{1}{2^k(z-e^{2\pi i r_k})}| \leq \sum_{k=n}^{\infty} \frac{1}{2^k|(z-e^{2\pi i r_k})|} \leq \sum_{k=n}^{\infty} \frac{1}{2^k R} \overset{n\rightarrow \infty}{\longrightarrow} 0
\end{align*} z\in U_2 R = \inf\{|z-\exp(2\pi i r_k)|: k\in \mathbb{N}\} f(z) U_1 U_2 U_1\cap U_2 = \emptyset","['complex-analysis', 'uniform-convergence', 'analytic-continuation', 'holomorph']"
99,$n$th derivative of $\ln(x)e^{2x}$,th derivative of,n \ln(x)e^{2x},"I was asked to calculate the $n$ th derivative of $f(x)=\ln(x)e^{2x}$ , and my solution was to use the Leibniz formula , so we have: $f^{(n)}(x)=\sum_{k=0}^{k=n}C^{k}_{n}.\frac{(-1)^{k-1}.(k-1)!}{x^k}.2^{n-k}.e^{2x}$ . But I don't know how I can calculate this sum.","I was asked to calculate the th derivative of , and my solution was to use the Leibniz formula , so we have: . But I don't know how I can calculate this sum.",n f(x)=\ln(x)e^{2x} f^{(n)}(x)=\sum_{k=0}^{k=n}C^{k}_{n}.\frac{(-1)^{k-1}.(k-1)!}{x^k}.2^{n-k}.e^{2x},"['real-analysis', 'calculus', 'complex-analysis', 'analysis', 'derivatives']"
