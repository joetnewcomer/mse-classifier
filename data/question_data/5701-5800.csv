,title_raw,title_text,title_latex,body_raw,body_text,body_latex,tags
0,Function with product of sine kernel,Function with product of sine kernel,,"Let $f\in L^2(\mathbb{R})$ be such that $\hat{f}$ is supported on $[-\pi,\pi]$. Also, $f$ is continuous and goes to $0$ at $\pm \infty$. Let $$K_\lambda(y)=\dfrac{2\sin(\pi(\lambda-1)y)\sin(\pi(\lambda+1)y)}{\pi^2 y^2(\lambda-1)}$$ How can we show that $$f(x)=\sum_{n=-\infty}^\infty \frac{1}{\lambda}f\left(\frac{n}{\lambda}\right)K_\lambda\left(x-\frac{n}{\lambda}\right)$$ for every $x$? I'm not sure how to start with this.","Let $f\in L^2(\mathbb{R})$ be such that $\hat{f}$ is supported on $[-\pi,\pi]$. Also, $f$ is continuous and goes to $0$ at $\pm \infty$. Let $$K_\lambda(y)=\dfrac{2\sin(\pi(\lambda-1)y)\sin(\pi(\lambda+1)y)}{\pi^2 y^2(\lambda-1)}$$ How can we show that $$f(x)=\sum_{n=-\infty}^\infty \frac{1}{\lambda}f\left(\frac{n}{\lambda}\right)K_\lambda\left(x-\frac{n}{\lambda}\right)$$ for every $x$? I'm not sure how to start with this.",,"['real-analysis', 'fourier-analysis']"
1,Prove $\sup_{0\le x\le 1}|f(x)|\le\int_0^1(|f(t)|+|f'(t)|)dt$,Prove,\sup_{0\le x\le 1}|f(x)|\le\int_0^1(|f(t)|+|f'(t)|)dt,"Let $f\in C^1([0,1])$. Prove the following: $$\sup_{0\le x\le 1}|f(x)|\le\int_0^1(|f(t)|+|f'(t)|)dt$$ and $$|f(1/2)|\le\int_0^1(|f(t)|+\frac12|f'(t)|)dt$$ Note that $e^{-x}(e^xf(x))'=f(x)+f'(x)$. So $\int_0^1(|f(t)|+|f'(t)|)dt\ge|\int_0^1(f(t)+f'(t))dt|\ge\int_0^1(e^tf(t))'dt=ef(1)-f(0)$","Let $f\in C^1([0,1])$. Prove the following: $$\sup_{0\le x\le 1}|f(x)|\le\int_0^1(|f(t)|+|f'(t)|)dt$$ and $$|f(1/2)|\le\int_0^1(|f(t)|+\frac12|f'(t)|)dt$$ Note that $e^{-x}(e^xf(x))'=f(x)+f'(x)$. So $\int_0^1(|f(t)|+|f'(t)|)dt\ge|\int_0^1(f(t)+f'(t))dt|\ge\int_0^1(e^tf(t))'dt=ef(1)-f(0)$",,"['real-analysis', 'integration', 'analysis', 'inequality', 'contest-math']"
2,Showing that smoothing operators are compact,Showing that smoothing operators are compact,,"Suppose I have a bounded, linear map $T: H^1(X) \to H^1(X)$ such that $T(H^1(X)) \subset C^\infty(X)$. Is $T$ a compact operator? I'm guessing this depends on whether or not $X$ is (pre)compact, and in the case I am the most interested in, it is ($X$ is an open, bounded set in $\mathbb{R}^n$), but I am also curious about the general question. My preliminary thoughts are Arzela-Ascoli: if we take a sequence of functions $f_n \in H^1(X), ||f_n||=1$, then $||Tf_n|| \leq ||T||$ so the sequence is uniformly bounded. Unfortunately, I'm not sure how I should translate the smoothness of $Tf_n$ (and compactness of $X$) into equicontinuity. Or maybe this follows directly from something like Rellich's theorem or another Sobolev embedding result? EDIT: Let us further refine the problem statement, so that $T$ is a bounded, linear map $H^1_0(X) \to H^1_0(X)$ that extends to a continuous map $\mathcal{E}'(X) \to C^\infty(X)$ (assumptions on $X$ to follow, $\mathcal{E}'$ are compactly supported distributions). Is $T$ compact? Thoughts: if $X$ is an open, bounded set, then the Schwartz kernel theorem says that there exists $k \in C^\infty(X \times X)$ so that  $$ Tf(x) = \int k(x,y)f(y) \ dy. $$ If $k$ is uniformly continuous, then equicontinuity essentially follows immediately. But uniform continuity doesn't necessarily hold, as $X$ is only pre-compact. If we replace $X$ by $\overline X$, then obviously a smooth $k$ on $\overline X \times \overline X$ is uniformly continuous, but does the Schwartz kernel theorem still hold in this case (every map $T:\mathcal{E}'(\overline X) \to C^\infty(\overline X)$ corresponds to a unique, smooth integral kernel)?","Suppose I have a bounded, linear map $T: H^1(X) \to H^1(X)$ such that $T(H^1(X)) \subset C^\infty(X)$. Is $T$ a compact operator? I'm guessing this depends on whether or not $X$ is (pre)compact, and in the case I am the most interested in, it is ($X$ is an open, bounded set in $\mathbb{R}^n$), but I am also curious about the general question. My preliminary thoughts are Arzela-Ascoli: if we take a sequence of functions $f_n \in H^1(X), ||f_n||=1$, then $||Tf_n|| \leq ||T||$ so the sequence is uniformly bounded. Unfortunately, I'm not sure how I should translate the smoothness of $Tf_n$ (and compactness of $X$) into equicontinuity. Or maybe this follows directly from something like Rellich's theorem or another Sobolev embedding result? EDIT: Let us further refine the problem statement, so that $T$ is a bounded, linear map $H^1_0(X) \to H^1_0(X)$ that extends to a continuous map $\mathcal{E}'(X) \to C^\infty(X)$ (assumptions on $X$ to follow, $\mathcal{E}'$ are compactly supported distributions). Is $T$ compact? Thoughts: if $X$ is an open, bounded set, then the Schwartz kernel theorem says that there exists $k \in C^\infty(X \times X)$ so that  $$ Tf(x) = \int k(x,y)f(y) \ dy. $$ If $k$ is uniformly continuous, then equicontinuity essentially follows immediately. But uniform continuity doesn't necessarily hold, as $X$ is only pre-compact. If we replace $X$ by $\overline X$, then obviously a smooth $k$ on $\overline X \times \overline X$ is uniformly continuous, but does the Schwartz kernel theorem still hold in this case (every map $T:\mathcal{E}'(\overline X) \to C^\infty(\overline X)$ corresponds to a unique, smooth integral kernel)?",,"['real-analysis', 'functional-analysis', 'sobolev-spaces', 'compact-operators']"
3,$x\mapsto f(x^{1/p})$ is smooth if and only if $f^{(n)}(0)=0$ whenever $p\nmid n$,is smooth if and only if  whenever,x\mapsto f(x^{1/p}) f^{(n)}(0)=0 p\nmid n,"This is a slight generalization of something I got stuck on when trying to do Problem 2-5 from Introduction to Smooth Manifolds by John M. Lee (which uses the case $p=3$). Let $p\ge 1$ be an integer, let $f:\mathbb{R}\to\mathbb{R}$ be a smooth function, and define $g(x)=f(x^{1/p})$. Then $g$ is smooth if and only if $f^{(n)}(0)=0$ whenever $n$ is not divisible by $p$ and $n\ge 1$. (Here smooth means that $f^{(n)}$ exists on $\mathbb{R}$ for all $n\ge 0$.) This is obvious if $f$ is analytic. But I'm not sure whether this is true for general smooth $f$.","This is a slight generalization of something I got stuck on when trying to do Problem 2-5 from Introduction to Smooth Manifolds by John M. Lee (which uses the case $p=3$). Let $p\ge 1$ be an integer, let $f:\mathbb{R}\to\mathbb{R}$ be a smooth function, and define $g(x)=f(x^{1/p})$. Then $g$ is smooth if and only if $f^{(n)}(0)=0$ whenever $n$ is not divisible by $p$ and $n\ge 1$. (Here smooth means that $f^{(n)}$ exists on $\mathbb{R}$ for all $n\ge 0$.) This is obvious if $f$ is analytic. But I'm not sure whether this is true for general smooth $f$.",,"['calculus', 'real-analysis']"
4,What operations is a metric closed under?,What operations is a metric closed under?,,"Suppose $X$ is a set with a metric $d: X \times X \rightarrow \mathbb{R}$. What ""operations"" on $d$ will yield a metric in return? By this I mean a wide variety of things. For example, what functions $g: \mathbb{R} \rightarrow \mathbb{R}$ will make $g \circ d$ into a metric, for example $g \circ d = \sqrt{d}$. Or what functions of metrics will yield metrics in return, for example $d_1 + d_2$, where $d_1$ and $d_2$ are distinct metrics on $X$. I'm looking for a list of such operations, and counterexamples of ones which plausibly seem like they could define a metric but do not.","Suppose $X$ is a set with a metric $d: X \times X \rightarrow \mathbb{R}$. What ""operations"" on $d$ will yield a metric in return? By this I mean a wide variety of things. For example, what functions $g: \mathbb{R} \rightarrow \mathbb{R}$ will make $g \circ d$ into a metric, for example $g \circ d = \sqrt{d}$. Or what functions of metrics will yield metrics in return, for example $d_1 + d_2$, where $d_1$ and $d_2$ are distinct metrics on $X$. I'm looking for a list of such operations, and counterexamples of ones which plausibly seem like they could define a metric but do not.",,"['real-analysis', 'general-topology', 'metric-spaces']"
5,Do best lower approximations of a quadratic irrational always form a linear recurrence sequence?,Do best lower approximations of a quadratic irrational always form a linear recurrence sequence?,,"Let $\theta$ be an irrational number and let $$ {\cal L}= \bigg\lbrace (a,b) \in {\mathbb Z} \times {\mathbb N}^{*} \bigg| \frac{a}{b} \leq \theta \bigg\rbrace $$ and $$ {\cal B}= \bigg\lbrace (a,b) \in {\cal L} \bigg| \forall (a',b') \in {\cal L}, \ b'\leq b \Rightarrow \frac{a'}{b'} \leq \frac{a}{b} \bigg\rbrace $$ so that $\cal B$ corresponds to the best lower approximations of $\theta$. The elements of $\cal B$ can be arranged in an increasing sequence with increasing denominators, $\frac{a_1}{b_1}<\frac{a_2}{b_2}<\frac{a_3}{b_3} < \ldots $ with $b_1<b_2<b_3< \ldots $. For example, when $\theta=\sqrt{5}$, the sequence is $\frac{2}{1}<\frac{11}{5}<\frac{20}{9}< \ldots $. The sequences $(a_n)$ and $(b_n)$ are linear recurrent sequences of degree $8$, with characteristic polynomial $X^8-18X^4+1=(X^4-4X^2-1)(X^4+4X^2-1)$. Note that contrary to what might be excepted, this degree 8 has nothing to do with the period of the standard continued fraction for $\theta$, which equals $1$ (we have $\sqrt{5}=2+\frac{1}{\phi}$ with $\phi=4+\frac{1}{\phi}$) . My question is, are the sequences $(a_n)$ and $(b_n)$ always evantually linear recurrent if $\theta$ is a quadratic irrational ?","Let $\theta$ be an irrational number and let $$ {\cal L}= \bigg\lbrace (a,b) \in {\mathbb Z} \times {\mathbb N}^{*} \bigg| \frac{a}{b} \leq \theta \bigg\rbrace $$ and $$ {\cal B}= \bigg\lbrace (a,b) \in {\cal L} \bigg| \forall (a',b') \in {\cal L}, \ b'\leq b \Rightarrow \frac{a'}{b'} \leq \frac{a}{b} \bigg\rbrace $$ so that $\cal B$ corresponds to the best lower approximations of $\theta$. The elements of $\cal B$ can be arranged in an increasing sequence with increasing denominators, $\frac{a_1}{b_1}<\frac{a_2}{b_2}<\frac{a_3}{b_3} < \ldots $ with $b_1<b_2<b_3< \ldots $. For example, when $\theta=\sqrt{5}$, the sequence is $\frac{2}{1}<\frac{11}{5}<\frac{20}{9}< \ldots $. The sequences $(a_n)$ and $(b_n)$ are linear recurrent sequences of degree $8$, with characteristic polynomial $X^8-18X^4+1=(X^4-4X^2-1)(X^4+4X^2-1)$. Note that contrary to what might be excepted, this degree 8 has nothing to do with the period of the standard continued fraction for $\theta$, which equals $1$ (we have $\sqrt{5}=2+\frac{1}{\phi}$ with $\phi=4+\frac{1}{\phi}$) . My question is, are the sequences $(a_n)$ and $(b_n)$ always evantually linear recurrent if $\theta$ is a quadratic irrational ?",,"['linear-algebra', 'real-analysis', 'continued-fractions', 'diophantine-approximation']"
6,Prove that the minimum of a functional doesn't exist,Prove that the minimum of a functional doesn't exist,,"Prove that there is no smooth solution ho the minimization problem: $$\mathcal{L} (u)= \int_{0}^1 e^{-u'}+u^2 dx$$ Where the admissible space is $X =\{ u \in \mathcal{c}^2 [0,1] | u(0)=0,  u(1)=1        \} $ UPDATE: GOAL: I am trying to define a sequence of functions $u_n(x)$ in $X$ s.t their integrals $\mathcal{L}(u_n) \to 0$ What I have done so far: Define $u_n(x)$ in the following manner: $$u_n (x)= \begin{cases} 0, \quad 0 \leq x\leq 1-\frac{1}{n} \\[2ex] ax^2 +bx+ c, \quad 1-\frac{1}{n}<x \leq 1  \end{cases}$$ where $n=1,2,3,..$ . Require that $u_n$ satisfy: \begin{align}  u_n(1-\frac{1}{n})=0\\  u_n(1)=1\\  u_n'(1-\frac{1}{n})=0\\ \end{align} We then have: \begin{align} a+b+c=1\\ a(1-\frac{1}{n})^2+b(1-\frac{1}{n})+c=0\\ 2a(1-\frac{1}{n})+b=0  \end{align} Find $a,b,c$ then: $$u_n (x)= \begin{cases} 0, \quad 0 \leq x\leq 1-\frac{1}{n} \\[2ex] n^2x^2 -2n(n-1)x+ (n-1)^2, \quad 1-\frac{1}{n}<x \leq 1  \end{cases}$$ \begin{align} \int_0^1 e^{-u_n(x)'} + u_n^2(x) dx &= \int_0^{1-\frac{1}{n}} e^{0} + 0 dx +  \int_{1-\frac{1}{n}}^1 e^{-(2n^2 x -2n(n-1))} + (n^2x^2 -2n(n-1)x+ (n-1)^2)^2 dx\\& = 1- \dfrac{1}{n}+ \dfrac{\mathrm{e}^{-2n}\left(\left(2n+5\right)\mathrm{e}^{2n}-5\right)}{10n^2} \end{align} $$\dfrac{\mathrm{e}^{-2n}\left(\left(2n+5\right)\mathrm{e}^{2n}-5\right)}{10n^2} \to 0$$ and $$1-\dfrac{1}{n} \to 1$$",Prove that there is no smooth solution ho the minimization problem: Where the admissible space is UPDATE: GOAL: I am trying to define a sequence of functions in s.t their integrals What I have done so far: Define in the following manner: where . Require that satisfy: We then have: Find then: and,"\mathcal{L} (u)= \int_{0}^1 e^{-u'}+u^2 dx X =\{ u \in \mathcal{c}^2 [0,1] | u(0)=0,  u(1)=1
       \}  u_n(x) X \mathcal{L}(u_n) \to 0 u_n(x) u_n (x)= \begin{cases}
0, \quad 0 \leq x\leq 1-\frac{1}{n} \\[2ex]
ax^2 +bx+ c, \quad 1-\frac{1}{n}<x \leq 1 
\end{cases} n=1,2,3,.. u_n \begin{align}
 u_n(1-\frac{1}{n})=0\\
 u_n(1)=1\\
 u_n'(1-\frac{1}{n})=0\\
\end{align} \begin{align}
a+b+c=1\\
a(1-\frac{1}{n})^2+b(1-\frac{1}{n})+c=0\\
2a(1-\frac{1}{n})+b=0 
\end{align} a,b,c u_n (x)= \begin{cases}
0, \quad 0 \leq x\leq 1-\frac{1}{n} \\[2ex]
n^2x^2 -2n(n-1)x+ (n-1)^2, \quad 1-\frac{1}{n}<x \leq 1 
\end{cases} \begin{align}
\int_0^1 e^{-u_n(x)'} + u_n^2(x) dx &= \int_0^{1-\frac{1}{n}} e^{0} + 0 dx +  \int_{1-\frac{1}{n}}^1 e^{-(2n^2 x -2n(n-1))} + (n^2x^2 -2n(n-1)x+ (n-1)^2)^2 dx\\& = 1- \dfrac{1}{n}+ \dfrac{\mathrm{e}^{-2n}\left(\left(2n+5\right)\mathrm{e}^{2n}-5\right)}{10n^2}
\end{align} \dfrac{\mathrm{e}^{-2n}\left(\left(2n+5\right)\mathrm{e}^{2n}-5\right)}{10n^2} \to 0 1-\dfrac{1}{n} \to 1","['real-analysis', 'functional-analysis', 'calculus-of-variations', 'euler-lagrange-equation', 'variational-analysis']"
7,Composition of Taylor Series,Composition of Taylor Series,,"Suppose I have smooth functions $f,g,y_0$ and $y_1$ from $\mathbb{R}$ to $\mathbb{R}$ , such that $$y_1(x) = y_0(x) - \epsilon g(y_0(x))$$ Then I consider $$f(y_0(x)) = f(y_1(x) + \epsilon g(y_0(x)))$$ Is there a closed form expression for the Taylor series in the small parameter $\epsilon$ in terms of derivatives of $f$ and $g$ and only the function $y_1$ ? The first few terms are $$f(y_0) = f(y_1) + \epsilon f'(y_1) g(y_0) +  \frac{1}{2}\epsilon^2 f''(y_1)g^2(y_0) +..$$ Where we interpret $f(y_0)$ as $f|_{y_0(x)}$ and treat $x$ fixed. Then we can again replace the $y_0$ in $g(y_0)$ with $$g(y_0) = g(y_1)+ \epsilon g'(y_1)g(y_0) +...$$ giving $$= f(y_1) + \epsilon f'(y_1) [g(y_1) + \epsilon g'(y_1)g(y_0) + ... ]$$ $$+  \frac{1}{2}\epsilon^2 f''(y_1)[g(y_1) + \epsilon g'(y_1)g(y_0) + ... ]^2 +...$$ Continuing to replace the $y_0$ with $g(y_0)$ like this and grouping terms gives $$f(y_0) = f(y_1)+ \epsilon [f'g](y_1) + \epsilon^2[f'g'g + \frac{1}{2}f''g^2](y_1) + \epsilon^3[f'g'^2g + \frac{1}{2}f'g''g + \frac{1}{2}f''g'g + \frac{1}{6}f'''g^3](y_1) + O(\epsilon^4)$$ But is there some way to write this as a more compact sum like $$f(y_0) \sim f(y_1) + g(y_1)\sum_{n=1}^\infty\sum_{m=0}^n \epsilon^n \alpha(n,m)f^{(n)}g^{(n-m)}(y_1)$$ I am having trouble identifying the pattern. I know there will be some product involved as well. Edit: Thinking about it some more it may suffice to just set $f=id$ and consider $$y_0 = y_1 + \epsilon g(y_0)$$ $$y_0 = y_1 + \epsilon g(y_1 + \epsilon g(y_0))$$ $$y_0 = y_1 + \epsilon g(y_1 + \epsilon g(y_1 + \epsilon g(y_0)))$$ $$y_0 = y_1 + \epsilon g(y_1 + \epsilon g(y_1 + \epsilon g(y_1 + ...)))$$ and somehow use the chain rule $$[f_1\circ f_2 \circ .... \circ f_n]' = \prod_{i=1}^n(f'_{i}\circ f_{i+1}\circ ...\circ f_n)$$","Suppose I have smooth functions and from to , such that Then I consider Is there a closed form expression for the Taylor series in the small parameter in terms of derivatives of and and only the function ? The first few terms are Where we interpret as and treat fixed. Then we can again replace the in with giving Continuing to replace the with like this and grouping terms gives But is there some way to write this as a more compact sum like I am having trouble identifying the pattern. I know there will be some product involved as well. Edit: Thinking about it some more it may suffice to just set and consider and somehow use the chain rule","f,g,y_0 y_1 \mathbb{R} \mathbb{R} y_1(x) = y_0(x) - \epsilon g(y_0(x)) f(y_0(x)) = f(y_1(x) + \epsilon g(y_0(x))) \epsilon f g y_1 f(y_0) = f(y_1) + \epsilon f'(y_1) g(y_0) +  \frac{1}{2}\epsilon^2 f''(y_1)g^2(y_0) +.. f(y_0) f|_{y_0(x)} x y_0 g(y_0) g(y_0) = g(y_1)+ \epsilon g'(y_1)g(y_0) +... = f(y_1) + \epsilon f'(y_1) [g(y_1) + \epsilon g'(y_1)g(y_0) + ... ] +  \frac{1}{2}\epsilon^2 f''(y_1)[g(y_1) + \epsilon g'(y_1)g(y_0) + ... ]^2 +... y_0 g(y_0) f(y_0) = f(y_1)+ \epsilon [f'g](y_1) + \epsilon^2[f'g'g + \frac{1}{2}f''g^2](y_1) + \epsilon^3[f'g'^2g + \frac{1}{2}f'g''g + \frac{1}{2}f''g'g + \frac{1}{6}f'''g^3](y_1) + O(\epsilon^4) f(y_0) \sim f(y_1) + g(y_1)\sum_{n=1}^\infty\sum_{m=0}^n \epsilon^n \alpha(n,m)f^{(n)}g^{(n-m)}(y_1) f=id y_0 = y_1 + \epsilon g(y_0) y_0 = y_1 + \epsilon g(y_1 + \epsilon g(y_0)) y_0 = y_1 + \epsilon g(y_1 + \epsilon g(y_1 + \epsilon g(y_0))) y_0 = y_1 + \epsilon g(y_1 + \epsilon g(y_1 + \epsilon g(y_1 + ...))) [f_1\circ f_2 \circ .... \circ f_n]' = \prod_{i=1}^n(f'_{i}\circ f_{i+1}\circ ...\circ f_n)","['real-analysis', 'calculus', 'sequences-and-series', 'taylor-expansion', 'infinite-product']"
8,Is a homeomorphism which maps lines to lines (and fixes zero) necessarily linear?,Is a homeomorphism which maps lines to lines (and fixes zero) necessarily linear?,,"We know the homeomorphism $f: \mathbb{R}^n \rightarrow \mathbb{R}^n$ maps straight lines to straight lines and the zero vector to the zero vector. Is it Linear?? If so, how can we prove it?","We know the homeomorphism $f: \mathbb{R}^n \rightarrow \mathbb{R}^n$ maps straight lines to straight lines and the zero vector to the zero vector. Is it Linear?? If so, how can we prove it?",,"['real-analysis', 'linear-algebra', 'general-topology']"
9,Approximating a double sum by a double integral,Approximating a double sum by a double integral,,"Related to this question , I'm interested in bounding from above the following sum $$ S:=\sum_{x=0}^\infty \sum_{y=0}^\infty (x+y)^m e^{-\frac{x^2}{2i} - \frac{y^2}{2j}}, $$ which I hope to do by relating it to the integral $$ I:=\int_0^\infty \int_0^\infty (x+y)^m e^{-\frac{x^2}{2i} - \frac{y^2}{2j}} dx\,dy. $$ Answers to the previous questions confirmed my expectation that $I = O\left(\exp\left(m\log\sqrt{(i+j)(m)}-\frac{m}{2}\right)\sqrt{ij}\right)$ , the intuition for which is probably that the function behaves approximately like a gaussian around its maximum at $(x_0,y_0) = \left(i \sqrt{\frac{m}{i+j}},j \sqrt{\frac{m}{i+j}} \right)$ , where the function takes the value $\exp\left(m\log\sqrt{(i+j)(m)}-\frac{m}{2}\right)$ . However I've been unable to show that the difference $|I-S|$ is significantly smaller than this bound. For simple one dimensional integrals, for example with a unique maximum, it's not too hard to bound this difference in terms of the maximum by considering appropriate telescoping sums. However, a naive analogue of this argument doesn't seem to work in two dimensions, and trying to apply this argument to each `slice' of the integral led to some pretty horrendous calculations. I also looked into using the Euler-Maclaurin formula but it's a bit out of my area of expertise. I suspect that there should be a relatively standard way to approximate $|I-S|$ , and I also wouldn't be surprised if someone more proficient in computing can get a CAS to provide a proof. The former would be more useful, just so that I have a tool for approaching similar questions. So, very explicitly, I would like to know if $$ |I-S| = o\left(\exp\left(m\log\sqrt{(i+j)(m)}-\frac{m}{2}\right)\sqrt{ij}\right), $$ where even big-O would be sufficient for the application I have in mind, and I wouldn't be surprised if the difference is even bounded by a multiple of the maximum of the function. I'm interested in the asymptotics for $i$ and $j$ tending to infinity, $m$ can be fixed or also a function of $i$ and $j$ . For the application I have in mind it would probably be sufficient to have such a result for $i = (1+o(1))j$ and $m = o(i)$ .","Related to this question , I'm interested in bounding from above the following sum which I hope to do by relating it to the integral Answers to the previous questions confirmed my expectation that , the intuition for which is probably that the function behaves approximately like a gaussian around its maximum at , where the function takes the value . However I've been unable to show that the difference is significantly smaller than this bound. For simple one dimensional integrals, for example with a unique maximum, it's not too hard to bound this difference in terms of the maximum by considering appropriate telescoping sums. However, a naive analogue of this argument doesn't seem to work in two dimensions, and trying to apply this argument to each `slice' of the integral led to some pretty horrendous calculations. I also looked into using the Euler-Maclaurin formula but it's a bit out of my area of expertise. I suspect that there should be a relatively standard way to approximate , and I also wouldn't be surprised if someone more proficient in computing can get a CAS to provide a proof. The former would be more useful, just so that I have a tool for approaching similar questions. So, very explicitly, I would like to know if where even big-O would be sufficient for the application I have in mind, and I wouldn't be surprised if the difference is even bounded by a multiple of the maximum of the function. I'm interested in the asymptotics for and tending to infinity, can be fixed or also a function of and . For the application I have in mind it would probably be sufficient to have such a result for and .","
S:=\sum_{x=0}^\infty \sum_{y=0}^\infty (x+y)^m e^{-\frac{x^2}{2i} - \frac{y^2}{2j}},
 
I:=\int_0^\infty \int_0^\infty (x+y)^m e^{-\frac{x^2}{2i} - \frac{y^2}{2j}} dx\,dy.
 I = O\left(\exp\left(m\log\sqrt{(i+j)(m)}-\frac{m}{2}\right)\sqrt{ij}\right) (x_0,y_0) = \left(i \sqrt{\frac{m}{i+j}},j \sqrt{\frac{m}{i+j}} \right) \exp\left(m\log\sqrt{(i+j)(m)}-\frac{m}{2}\right) |I-S| |I-S| 
|I-S| = o\left(\exp\left(m\log\sqrt{(i+j)(m)}-\frac{m}{2}\right)\sqrt{ij}\right),
 i j m i j i = (1+o(1))j m = o(i)","['real-analysis', 'asymptotics', 'multiple-integral']"
10,Prove that the equation has only one real root.,Prove that the equation has only one real root.,,"Prove that $(x-1)^3+(x-2)^3+(x-3)^3+(x-4)^3=0$ has only one real root. It's easy to show that the equation has a real root using Rolle's theorem. But how to show that the real root is unique? By Descartes' rule of sign, it can be shown that it has 3 or 1 real root. But it doesn't guarantee that the real root is unique. If we calculate the root then it can be shown that it has only one real root.","Prove that has only one real root. It's easy to show that the equation has a real root using Rolle's theorem. But how to show that the real root is unique? By Descartes' rule of sign, it can be shown that it has 3 or 1 real root. But it doesn't guarantee that the real root is unique. If we calculate the root then it can be shown that it has only one real root.",(x-1)^3+(x-2)^3+(x-3)^3+(x-4)^3=0,['real-analysis']
11,Limit of $\arctan(x)/x$ as $x$ approaches $0$?,Limit of  as  approaches ?,\arctan(x)/x x 0,Quick question: I came across the following limit: $$\lim_{x\rightarrow 0^{+}}\frac{\arctan(x)}{x}=1.$$  It seems like the well-known limit:  $$\lim_{x\rightarrow 0}\frac{\sin x}{x}=1.$$  Can anyone show me how to prove it?,Quick question: I came across the following limit: $$\lim_{x\rightarrow 0^{+}}\frac{\arctan(x)}{x}=1.$$  It seems like the well-known limit:  $$\lim_{x\rightarrow 0}\frac{\sin x}{x}=1.$$  Can anyone show me how to prove it?,,"['calculus', 'real-analysis']"
12,What is $\log(n+1)-\log(n)$?,What is ?,\log(n+1)-\log(n),What is gap $\log(n+1)-\log(n)$ between log of consecutive integers? That is what precision of logarithms determines integers correctly?,What is gap $\log(n+1)-\log(n)$ between log of consecutive integers? That is what precision of logarithms determines integers correctly?,,"['real-analysis', 'numerical-methods', 'logarithms']"
13,"Michael Spivak in ""Calculus"" asserts that $\sqrt2$ cannot be proven to exist, and that such a proof is impossible. What does he mean by ""exist""?","Michael Spivak in ""Calculus"" asserts that  cannot be proven to exist, and that such a proof is impossible. What does he mean by ""exist""?",\sqrt2,"Michael Spivak in ""Calculus"" asserts that $\sqrt2$ cannot be proven to exist, and that such a proof is impossible. What does he mean by ""exist""? How are you to prove that any number ""exists""? Why can't we define $\sqrt2$ as a number that fits under some arbitrary definition of existence, while asserting that its most concise expression is with a functional root? I'm sorry if these questions seem a bit sophomoric; in some ways it resembles an 8 year old repeatedly asking ""why"". But given that his prose is very concise and technical, his usage of ""exist"" was out of the ordinary. (I used two tags representing the book's field of study; and one representing the actual relevant tag.) edit Oh, I'm sorry. I misquoted. My question still stands, though; how has he defined existence such that $\sqrt2$ might possible not be within it. Direct quote: ""We have not proved that any such number exists..."" in reference to $\sqrt2$.","Michael Spivak in ""Calculus"" asserts that $\sqrt2$ cannot be proven to exist, and that such a proof is impossible. What does he mean by ""exist""? How are you to prove that any number ""exists""? Why can't we define $\sqrt2$ as a number that fits under some arbitrary definition of existence, while asserting that its most concise expression is with a functional root? I'm sorry if these questions seem a bit sophomoric; in some ways it resembles an 8 year old repeatedly asking ""why"". But given that his prose is very concise and technical, his usage of ""exist"" was out of the ordinary. (I used two tags representing the book's field of study; and one representing the actual relevant tag.) edit Oh, I'm sorry. I misquoted. My question still stands, though; how has he defined existence such that $\sqrt2$ might possible not be within it. Direct quote: ""We have not proved that any such number exists..."" in reference to $\sqrt2$.",,"['calculus', 'real-analysis', 'philosophy']"
14,Why should we care about sequence in real analysis?,Why should we care about sequence in real analysis?,,"I'm self-learning Real analysis through the book Real Analysis by Terence Tao. There is a concept of sequence which I don't fully understand its intuition and motivation by constructing one. It is not quite natural for me. I see that the concept of sequence arise for the first time when constructing real number using rational number. And then it come back again and again when dealing with continuity, derivative,... My question is: When first think about sequence (of object in general) I can think of many useless sequences which behave arbitrarily. For example, the sequence $1, 3, -1, 2, 2, -1...$ doesn't mean anything (i choose these number randomly). It seems that the number of useless sequences is much more than that of useful sequence. So, does the concept of sequence naturally arise in real analysis ? Why is the concept of sequence extensively while there are way more useless sequences than the useful ones ? Thank you very much for your help!","I'm self-learning Real analysis through the book Real Analysis by Terence Tao. There is a concept of sequence which I don't fully understand its intuition and motivation by constructing one. It is not quite natural for me. I see that the concept of sequence arise for the first time when constructing real number using rational number. And then it come back again and again when dealing with continuity, derivative,... My question is: When first think about sequence (of object in general) I can think of many useless sequences which behave arbitrarily. For example, the sequence doesn't mean anything (i choose these number randomly). It seems that the number of useless sequences is much more than that of useful sequence. So, does the concept of sequence naturally arise in real analysis ? Why is the concept of sequence extensively while there are way more useless sequences than the useful ones ? Thank you very much for your help!","1, 3, -1, 2, 2, -1...","['real-analysis', 'sequences-and-series']"
15,"How to integrate $\int_{0}^{2\pi} \frac{1}{\sin^4x + \cos^4 x} \,dx$",How to integrate,"\int_{0}^{2\pi} \frac{1}{\sin^4x + \cos^4 x} \,dx","So I followed the explanations made in this post and I got that: $$\int \frac{1}{\sin^4x + \cos^4 x} \,dx = \frac{\sqrt{2}}{2}\arctan\left(\frac{\sqrt{2}}{2}\tan\left(2x\right)\right) + C$$ But when I try to use the Leibniz-Newton formula and evaluate the integral from $0$ to $2\pi$ I get that it's $0$ because $\tan\left(2x\right)$ evaluates to $0$ at both $x=0$ and $x=2\pi$ . Is there another way to solve this integral and get the correct answer ( $2\pi\sqrt2$ )?",So I followed the explanations made in this post and I got that: But when I try to use the Leibniz-Newton formula and evaluate the integral from to I get that it's because evaluates to at both and . Is there another way to solve this integral and get the correct answer ( )?,"\int \frac{1}{\sin^4x + \cos^4 x} \,dx = \frac{\sqrt{2}}{2}\arctan\left(\frac{\sqrt{2}}{2}\tan\left(2x\right)\right) + C 0 2\pi 0 \tan\left(2x\right) 0 x=0 x=2\pi 2\pi\sqrt2","['real-analysis', 'calculus', 'integration', 'definite-integrals']"
16,Find polynomials such that $(x-16)p(2x)=16(x-1)p(x)$,Find polynomials such that,(x-16)p(2x)=16(x-1)p(x),"Find all polynomials $p(x)$ such that for all $x$, we have $$(x-16)p(2x)=16(x-1)p(x)$$ I tried working out with replacing $x$ by $\frac{x}{2},\frac{x}{4},\cdots$, to have $p(2x) \to p(0)$ but then factor terms seems to create a problem. Link: http://web.mit.edu/rwbarton/Public/func-eq.pdf","Find all polynomials $p(x)$ such that for all $x$, we have $$(x-16)p(2x)=16(x-1)p(x)$$ I tried working out with replacing $x$ by $\frac{x}{2},\frac{x}{4},\cdots$, to have $p(2x) \to p(0)$ but then factor terms seems to create a problem. Link: http://web.mit.edu/rwbarton/Public/func-eq.pdf",,['real-analysis']
17,Evaluate $\lim_{n \rightarrow \infty} \frac {[(n+1)(n+2)\cdots(n+n)]^{1/n}}{n}$ [duplicate],Evaluate  [duplicate],\lim_{n \rightarrow \infty} \frac {[(n+1)(n+2)\cdots(n+n)]^{1/n}}{n},This question already has answers here : How to prove that $\lim \frac{1}{n} \sqrt[n]{(n+1)(n+2)... 2n} = \frac{4}{e}$ (5 answers) Closed 4 years ago . Evaluate $$\lim_{n \rightarrow \infty~} \dfrac {[(n+1)(n+2)\cdots(n+n)]^{\dfrac {1}{n}}}{n}$$ Attempt: Let $$y=\lim_{n \rightarrow \infty} \dfrac {[(n+1)(n+2)\cdots(n+n)]^{\dfrac {1}{n}}}{n}$$ $$\implies \log y = \lim_{n \rightarrow \infty} \dfrac {1} {n} [\log (n+1) +\cdots+log(n+n)-log(n)] $$ How do I move forward? Thank you very much for your help.,This question already has answers here : How to prove that $\lim \frac{1}{n} \sqrt[n]{(n+1)(n+2)... 2n} = \frac{4}{e}$ (5 answers) Closed 4 years ago . Evaluate $$\lim_{n \rightarrow \infty~} \dfrac {[(n+1)(n+2)\cdots(n+n)]^{\dfrac {1}{n}}}{n}$$ Attempt: Let $$y=\lim_{n \rightarrow \infty} \dfrac {[(n+1)(n+2)\cdots(n+n)]^{\dfrac {1}{n}}}{n}$$ $$\implies \log y = \lim_{n \rightarrow \infty} \dfrac {1} {n} [\log (n+1) +\cdots+log(n+n)-log(n)] $$ How do I move forward? Thank you very much for your help.,,"['calculus', 'real-analysis', 'sequences-and-series']"
18,Why is this definition of convergence incorrect?,Why is this definition of convergence incorrect?,,"From my understanding, a sequence $\{a_{n}\}$ is said to converge to $a$ if for all $\epsilon > 0$ , there exists an index $N$ such that for all $n \geq N$ , we have $$|a_{n} - a| < \epsilon.$$ But, why is this different from the following definition: There exists an index $N$ for every $\epsilon > 0$ , such that for all $n \geq N$ , we have $$|a_{n} - a| < \epsilon $$ Pretty much, I switched the $\forall \epsilon > 0$ quantifier with the $\exists N$ quantifier, and the definition becomes invalid, but why?","From my understanding, a sequence is said to converge to if for all , there exists an index such that for all , we have But, why is this different from the following definition: There exists an index for every , such that for all , we have Pretty much, I switched the quantifier with the quantifier, and the definition becomes invalid, but why?",\{a_{n}\} a \epsilon > 0 N n \geq N |a_{n} - a| < \epsilon. N \epsilon > 0 n \geq N |a_{n} - a| < \epsilon  \forall \epsilon > 0 \exists N,['real-analysis']
19,Intersection of infinite sets is infinite?,Intersection of infinite sets is infinite?,,"I know that if $C \subseteq [0,1]$ is uncountable, then there exists $a \in (0,1)$ such that $C \cap [a,1]$ is uncountable. Is it still true for any infinite sets? That is, if $C \subseteq [0,1]$ is infinite, does there exist an $a \in (0,1)$ such that $C \cap [a,1]$ is infinite?","I know that if $C \subseteq [0,1]$ is uncountable, then there exists $a \in (0,1)$ such that $C \cap [a,1]$ is uncountable. Is it still true for any infinite sets? That is, if $C \subseteq [0,1]$ is infinite, does there exist an $a \in (0,1)$ such that $C \cap [a,1]$ is infinite?",,['real-analysis']
20,A basic inequality: $a-b\leq |a|+|b|$,A basic inequality:,a-b\leq |a|+|b|,"Do we have the following inequality: $$a-b\leq |a|+|b|$$ I have considered $4$ cases: $a\leq0,b\leq0$ $a\leq0,b>0$ $a>0,b\leq0$ $a>0,b>0$ and see this inequality is true. However I want to make sure about that.","Do we have the following inequality: $$a-b\leq |a|+|b|$$ I have considered $4$ cases: $a\leq0,b\leq0$ $a\leq0,b>0$ $a>0,b\leq0$ $a>0,b>0$ and see this inequality is true. However I want to make sure about that.",,"['calculus', 'real-analysis', 'algebra-precalculus', 'absolute-value']"
21,A limit without invoking L'Hopital: $\lim_{x \to 0} \frac{x \cos x - \sin x}{x^2}$,A limit without invoking L'Hopital:,\lim_{x \to 0} \frac{x \cos x - \sin x}{x^2},"The following limit $$\ell=\lim_{x \rightarrow 0} \frac{x \cos x - \sin x}{x^2}$$ is a nice candidate for L'Hopital's Rule. This was given at a school before L'Hopital's Rule was covered. I wonder how we can skip the rule and use basic limits such as: $$\lim_{x \rightarrow 0} \frac{\sin x}{x} \quad , \quad \lim_{x \rightarrow 0} \frac{\cos x -1}{x^2}$$","The following limit $$\ell=\lim_{x \rightarrow 0} \frac{x \cos x - \sin x}{x^2}$$ is a nice candidate for L'Hopital's Rule. This was given at a school before L'Hopital's Rule was covered. I wonder how we can skip the rule and use basic limits such as: $$\lim_{x \rightarrow 0} \frac{\sin x}{x} \quad , \quad \lim_{x \rightarrow 0} \frac{\cos x -1}{x^2}$$",,"['calculus', 'real-analysis', 'trigonometry', 'limits-without-lhopital']"
22,Evaluating $\int_{-3}^{3}\frac{x^8}{1+e^{2x}}dx$,Evaluating,\int_{-3}^{3}\frac{x^8}{1+e^{2x}}dx,"$$\int_{-3}^{3}\frac{x^8}{1+e^{2x}}dx$$ I can't find solution for this task, can someone help me?","$$\int_{-3}^{3}\frac{x^8}{1+e^{2x}}dx$$ I can't find solution for this task, can someone help me?",,"['calculus', 'real-analysis', 'integration', 'definite-integrals']"
23,Why we can't differentiate both sides of a polynomial equation? [duplicate],Why we can't differentiate both sides of a polynomial equation? [duplicate],,"This question already has answers here : Is differentiating on both sides of an equation allowed? [duplicate] (9 answers) When is differentiating an equation valid? (2 answers) It is possible to apply a derivative to both sides of a given equation and maintain the equivalence of both sides? [duplicate] (5 answers) Closed 5 years ago . Suppose we had the equation below and we are going to differentiate it both sides: \begin{align} &2x^2-x=1\\ &4x-1=0\\ &4=0 \end{align} This problem doesn't seems to happens with other equation like $\ln x =1$ or $\sin x = 0$ , we can keep differentiating these two without getting "" $4=0$ "", for example. This why I asked about polynomials. PS: I'm not trying to solve any of these equations by differentiating then. But differentiation or integration helps and solving equations? I remember that sometimes to solve trigonometry equtions like $\sin x = \cos x$ we had to square both side so we could use the identity $\sin^2x + \cos^2x =1$ . Even thought squaring appears to make it worse because we have a new root.","This question already has answers here : Is differentiating on both sides of an equation allowed? [duplicate] (9 answers) When is differentiating an equation valid? (2 answers) It is possible to apply a derivative to both sides of a given equation and maintain the equivalence of both sides? [duplicate] (5 answers) Closed 5 years ago . Suppose we had the equation below and we are going to differentiate it both sides: This problem doesn't seems to happens with other equation like or , we can keep differentiating these two without getting "" "", for example. This why I asked about polynomials. PS: I'm not trying to solve any of these equations by differentiating then. But differentiation or integration helps and solving equations? I remember that sometimes to solve trigonometry equtions like we had to square both side so we could use the identity . Even thought squaring appears to make it worse because we have a new root.","\begin{align}
&2x^2-x=1\\
&4x-1=0\\
&4=0
\end{align} \ln x =1 \sin x = 0 4=0 \sin x = \cos x \sin^2x + \cos^2x =1","['real-analysis', 'calculus', 'derivatives', 'fake-proofs']"
24,Pseudo-Cauchy sequence,Pseudo-Cauchy sequence,,"I have never seen this terminology before, so I will provide the given definition. A Pseudo-Cauchy sequence is : A sequence $(a_n)$ if for any $\epsilon > 0$ there exists $N \in \mathbb{N}$ such that $|a_{n+1} - a_n | \leq \epsilon \space \forall \space n \geq  N$ So then my question is that is a pseudo-cauchy sequence always converging?","I have never seen this terminology before, so I will provide the given definition. A Pseudo-Cauchy sequence is : A sequence $(a_n)$ if for any $\epsilon > 0$ there exists $N \in \mathbb{N}$ such that $|a_{n+1} - a_n | \leq \epsilon \space \forall \space n \geq  N$ So then my question is that is a pseudo-cauchy sequence always converging?",,"['real-analysis', 'cauchy-sequences']"
25,What is the motivation for sequences to be defined on natural numbers?,What is the motivation for sequences to be defined on natural numbers?,,"Possible duplicate: Sequences with Real Indices I am trying to understand the motivation for various definitions in real analysis. Take for instance the definition of a sequence where it is defined as a function from natural numbers to that of real numbers. But why natural numbers? I cannot substantiate it with a reasonable argument and I am looking for one. I have been able to substantiate certain definitions based on some of my own reasoning, while that might not have been the real reason why it was defined that way. Take for instance the definition of convergence of a sequence. I think one of the central definitions in analysis is that of convergence of a sequence. Here we say that a sequence is said to converge to a limit $L$ if any $\epsilon$ -neighborhood of $L$ has all but finitely many terms of the sequence. I ask the usual set of questions to myself Why do we need this definition? Why was it defined this way? I thought of some possible answers. There are questions about the operations of addition and subtraction, and the manipulation of those operations resulting in questions about re-arrangements of infinite series. The foundation to answering such questions lies in the answer to ""does it even even make sense to represent the sum of infinite numbers by one number?"" What is the logical basis to represent that infinite sum by one number? Then we get the answer that the sequence of partial sums converge and so we can represent them as one number. Then we get the questions ""what is a sequence and what does it mean to say that the sequence converges?"" Then we get the definitions for a sequence and the definition of convergence. Definition of convergence makes sense to me. Let's assume we are thinking of two numbers $a$ and $b$ . What are some of the rigorous ways to define the quality of two numbers being equal? A popular one is to say that $a \geq b$ and $b \geq a$ . Another is the topologic definition where we say that $a$ always lies in any $\epsilon$ -neighborhood of $b$ no matter how small $\epsilon$ is. Now, we can modify this definition to obtain the definition of convergence of a sequence where we replace "" $a$ "" with ""all but finite terms of the sequence"". There you go. We got a definition for the convergence of a sequence. The part I could not substantiate to myself was the need to define sequences as functions on natural numbers to real numbers. Why not define sequences on some other index set? For instance, why not define it on real numbers? More generally, we have countability properties defined but why even bother with natural numbers when what we want in the end is real numbers?","Possible duplicate: Sequences with Real Indices I am trying to understand the motivation for various definitions in real analysis. Take for instance the definition of a sequence where it is defined as a function from natural numbers to that of real numbers. But why natural numbers? I cannot substantiate it with a reasonable argument and I am looking for one. I have been able to substantiate certain definitions based on some of my own reasoning, while that might not have been the real reason why it was defined that way. Take for instance the definition of convergence of a sequence. I think one of the central definitions in analysis is that of convergence of a sequence. Here we say that a sequence is said to converge to a limit if any -neighborhood of has all but finitely many terms of the sequence. I ask the usual set of questions to myself Why do we need this definition? Why was it defined this way? I thought of some possible answers. There are questions about the operations of addition and subtraction, and the manipulation of those operations resulting in questions about re-arrangements of infinite series. The foundation to answering such questions lies in the answer to ""does it even even make sense to represent the sum of infinite numbers by one number?"" What is the logical basis to represent that infinite sum by one number? Then we get the answer that the sequence of partial sums converge and so we can represent them as one number. Then we get the questions ""what is a sequence and what does it mean to say that the sequence converges?"" Then we get the definitions for a sequence and the definition of convergence. Definition of convergence makes sense to me. Let's assume we are thinking of two numbers and . What are some of the rigorous ways to define the quality of two numbers being equal? A popular one is to say that and . Another is the topologic definition where we say that always lies in any -neighborhood of no matter how small is. Now, we can modify this definition to obtain the definition of convergence of a sequence where we replace "" "" with ""all but finite terms of the sequence"". There you go. We got a definition for the convergence of a sequence. The part I could not substantiate to myself was the need to define sequences as functions on natural numbers to real numbers. Why not define sequences on some other index set? For instance, why not define it on real numbers? More generally, we have countability properties defined but why even bother with natural numbers when what we want in the end is real numbers?",L \epsilon L a b a \geq b b \geq a a \epsilon b \epsilon a,"['real-analysis', 'sequences-and-series', 'functional-analysis', 'soft-question']"
26,Compactness of finite sets,Compactness of finite sets,,"In rudin's analysis books, he defines compactness as: A subset $K$ of a metric space $X$ is ${\bf compact}$ if every open cover of $K$ contains a finite subcover. More explicitly is that if $\{ G_{\alpha} \}$ is an open cover of $K$ then one can select finitely many indices so that $K$ is contained in $G_{\alpha_1} \cup ... \cup G_{\alpha_n } $ Rudin claims that a finite set is compact as an obvious fact. Im trying to verify this fact myself: verification: Let $F$ be a finite set and write it as $\{ a_1,...,a_n \}$ Take any open cover $\{ G_{\alpha} \}$ of $F$ . We can consider the open balls centered at $a_i$ and let $B_i$ be such ball. Then, $B_1 \cup ... \cup B_n$ contains $F$ . Now, this seems incomplete as we must show that this finite union of open balls is in $\cup_{\alpha } G_{\alpha}$ . How do we check this?","In rudin's analysis books, he defines compactness as: A subset of a metric space is if every open cover of contains a finite subcover. More explicitly is that if is an open cover of then one can select finitely many indices so that is contained in Rudin claims that a finite set is compact as an obvious fact. Im trying to verify this fact myself: verification: Let be a finite set and write it as Take any open cover of . We can consider the open balls centered at and let be such ball. Then, contains . Now, this seems incomplete as we must show that this finite union of open balls is in . How do we check this?","K X {\bf compact} K \{ G_{\alpha} \} K K G_{\alpha_1} \cup ... \cup G_{\alpha_n }  F \{ a_1,...,a_n \} \{ G_{\alpha} \} F a_i B_i B_1 \cup ... \cup B_n F \cup_{\alpha } G_{\alpha}",['real-analysis']
27,What is the sense of the equality sign =?,What is the sense of the equality sign =?,,"Sometimes in real analysis, we write an equality and specify that is true in the sense of the $L^2$ norm. My question is, when in mathematics we write '=' and don't specify anything, what we really mean? Do we mean uniform convergence, more than that, or less than that?","Sometimes in real analysis, we write an equality and specify that is true in the sense of the $L^2$ norm. My question is, when in mathematics we write '=' and don't specify anything, what we really mean? Do we mean uniform convergence, more than that, or less than that?",,"['real-analysis', 'convergence-divergence', 'uniform-convergence']"
28,"Is it possible for the bisection method to provide ""fake"" zeros","Is it possible for the bisection method to provide ""fake"" zeros",,"I've read about the bisection method for finding roots of a function in my numerical analysis textbook and one question came to my mind. Given a relatively complicated function, the chances of finding the exact root (that is, a root that is completely represented in the computer's memory, with all significant figures) are very low. This means that most of the time, we will have a value for which the function is very close to, but not exactly equal to zero. So what would happen if the function had one root, and another value at which the function gets extremely close to zero, without actually getting there? Would the algorithm fail? And what is the meaning of that eventual ""fake"" root; is it worth anything? Thank you. EDIT: here is a picture showing what I meant","I've read about the bisection method for finding roots of a function in my numerical analysis textbook and one question came to my mind. Given a relatively complicated function, the chances of finding the exact root (that is, a root that is completely represented in the computer's memory, with all significant figures) are very low. This means that most of the time, we will have a value for which the function is very close to, but not exactly equal to zero. So what would happen if the function had one root, and another value at which the function gets extremely close to zero, without actually getting there? Would the algorithm fail? And what is the meaning of that eventual ""fake"" root; is it worth anything? Thank you. EDIT: here is a picture showing what I meant",,"['real-analysis', 'analysis', 'numerical-methods', 'roots']"
29,p-Norm with p $\to$ infinity [duplicate],p-Norm with p  infinity [duplicate],\to,"This question already has answers here : Show that $\lim_{n\rightarrow \infty} \sqrt[n]{c_1^n+c_2^n+\ldots+c_m^n} = \max\{c_1,c_2,\ldots,c_m\}$ [duplicate] (3 answers) Closed 2 years ago . I have to show that: for all vectors $v\in \Bbb R^n$ : $\lim_{p\to \infty}||v||_p = \max_{1\le i \le n}|v_i|$ with the $||\cdot ||_p$ norm defined as $$ ||\cdot ||_p: (v_1, \dots ,v_n) \to (\sum^n_{i=i} |v_i|^p)^{1/p} $$ I think I once read something about mixing the root and the same power with the power going to infinity but i can't really remember anything concrete. Any Ideas? Thanks in advance","This question already has answers here : Show that $\lim_{n\rightarrow \infty} \sqrt[n]{c_1^n+c_2^n+\ldots+c_m^n} = \max\{c_1,c_2,\ldots,c_m\}$ [duplicate] (3 answers) Closed 2 years ago . I have to show that: for all vectors : with the norm defined as I think I once read something about mixing the root and the same power with the power going to infinity but i can't really remember anything concrete. Any Ideas? Thanks in advance","v\in \Bbb R^n \lim_{p\to \infty}||v||_p = \max_{1\le i \le n}|v_i| ||\cdot ||_p  ||\cdot ||_p: (v_1, \dots ,v_n) \to (\sum^n_{i=i} |v_i|^p)^{1/p} ","['real-analysis', 'summation', 'normed-spaces']"
30,"If $\sum_{n = 1}^\infty {{a_n}}$ converges, then is $\sum_{n = 1}^\infty (1+a_n)^{-1}$ a convergent series?","If  converges, then is  a convergent series?",\sum_{n = 1}^\infty {{a_n}} \sum_{n = 1}^\infty (1+a_n)^{-1},"If $\sum\limits_{n = 1}^\infty  {{a_n}}$ is convergent (with ${a_n} > 0$, $\forall n\in\mathbb{Z}$), then is $\sum\limits_{n = 1}^\infty  {\left( {\frac{1}{{1 + {a_n}}}} \right)}$ is a convergent series? I tried with the quotient criterion and comparison theorem no avail. But I figured something was this: $${\frac{1}{{1 + {a_n}}}}<1$$ but still I find it useful. Any suggestions please.","If $\sum\limits_{n = 1}^\infty  {{a_n}}$ is convergent (with ${a_n} > 0$, $\forall n\in\mathbb{Z}$), then is $\sum\limits_{n = 1}^\infty  {\left( {\frac{1}{{1 + {a_n}}}} \right)}$ is a convergent series? I tried with the quotient criterion and comparison theorem no avail. But I figured something was this: $${\frac{1}{{1 + {a_n}}}}<1$$ but still I find it useful. Any suggestions please.",,"['calculus', 'real-analysis', 'sequences-and-series', 'power-series']"
31,Understanding the philosophy behind the axiomatic definition of reals,Understanding the philosophy behind the axiomatic definition of reals,,"I have just recently started to read a book about mathematics and have stumbled upon an axiomatic definition of the real numbers. The real numbers are defined as a Dedekind-complete ordered field. While I understand the components of this definition, i.e the specifications of these axioms, I am absolutely lost as to the philosophical process in place.  We define the real numbers by a set of properties that we assume to be true. If someone was to try and construct the set of real numbers using this definition, he would have to turn to the only set that has these properties. Which set is that? It is the set that we assume to have these properties. Well, which set is that? It is the set that we assume to have these properties and so on... I feel like without a concrete reference of some sort all we're left with is cyclical logic. Can anybody help me shed light on the philosophical process of this definition?","I have just recently started to read a book about mathematics and have stumbled upon an axiomatic definition of the real numbers. The real numbers are defined as a Dedekind-complete ordered field. While I understand the components of this definition, i.e the specifications of these axioms, I am absolutely lost as to the philosophical process in place.  We define the real numbers by a set of properties that we assume to be true. If someone was to try and construct the set of real numbers using this definition, he would have to turn to the only set that has these properties. Which set is that? It is the set that we assume to have these properties. Well, which set is that? It is the set that we assume to have these properties and so on... I feel like without a concrete reference of some sort all we're left with is cyclical logic. Can anybody help me shed light on the philosophical process of this definition?",,"['real-analysis', 'logic', 'definition', 'real-numbers']"
32,Show that $n^2\log\left(1+\frac{1}{n}\right)$ does not converge to $1$,Show that  does not converge to,n^2\log\left(1+\frac{1}{n}\right) 1,How to show that $n^2\log\left(1+\dfrac{1}{n}\right)\to 1$ is false? I have to show that $\left(1+\dfrac{1}{n}\right)^{n^2}$ doesn't tend to $e.$,How to show that $n^2\log\left(1+\dfrac{1}{n}\right)\to 1$ is false? I have to show that $\left(1+\dfrac{1}{n}\right)^{n^2}$ doesn't tend to $e.$,,['real-analysis']
33,"Evaluation of the double integral $\int_{[0,1][0,1]} \max\{x, y\} dxdy$",Evaluation of the double integral,"\int_{[0,1][0,1]} \max\{x, y\} dxdy","Evaluate:  $$\int_{[0,1][0,1]} \max\{x, y\} dxdy$$ I am totally stuck on it. How can I solve this?","Evaluate:  $$\int_{[0,1][0,1]} \max\{x, y\} dxdy$$ I am totally stuck on it. How can I solve this?",,['real-analysis']
34,Show that there does not exist a strictly increasing function $f : \mathbb Q \to \mathbb R$ such that $f(\mathbb Q) = \mathbb R$.,Show that there does not exist a strictly increasing function  such that .,f : \mathbb Q \to \mathbb R f(\mathbb Q) = \mathbb R,"The following exercise in an analysis text and I am trying to solve it without concepts of general topology but fail. Show that there does not exist a strictly increasing function $f : \mathbb Q \to \mathbb R$ such that $f(\mathbb Q) = \mathbb R$. Attempt 1. Suppose that the function $f(D) = \mathbb R$ is monotone. If its image $f(D)$ is an interval, then the function $f$ is continuous. So, if we suppose by contradiction that a strictly increasing function $f : \mathbb Q \to \mathbb R$ exists such that $f(\mathbb Q) = \mathbb R$ it must be continuous. Attempt 2. Since intersection of the set of irrational numbers of the domain is empty so by a convergence of a sequence in the domain $\mathbb Q$ in either case of converging to a rational or irrational number there is nothing to reach a contradiction. Attempt 3. The function $f$ is injective and it's not surjective so the inverse function is not defined such that I can use theorems about an inverse of a function. Please help!","The following exercise in an analysis text and I am trying to solve it without concepts of general topology but fail. Show that there does not exist a strictly increasing function $f : \mathbb Q \to \mathbb R$ such that $f(\mathbb Q) = \mathbb R$. Attempt 1. Suppose that the function $f(D) = \mathbb R$ is monotone. If its image $f(D)$ is an interval, then the function $f$ is continuous. So, if we suppose by contradiction that a strictly increasing function $f : \mathbb Q \to \mathbb R$ exists such that $f(\mathbb Q) = \mathbb R$ it must be continuous. Attempt 2. Since intersection of the set of irrational numbers of the domain is empty so by a convergence of a sequence in the domain $\mathbb Q$ in either case of converging to a rational or irrational number there is nothing to reach a contradiction. Attempt 3. The function $f$ is injective and it's not surjective so the inverse function is not defined such that I can use theorems about an inverse of a function. Please help!",,[]
35,Prove $\frac{a_n}{S_n^2} \leq \frac{1}{S_{n-1}}-\frac{1}{S_n}$ for partials sums of a divergent series,Prove  for partials sums of a divergent series,\frac{a_n}{S_n^2} \leq \frac{1}{S_{n-1}}-\frac{1}{S_n},"Let $(a_n)$ be a sequence of non-negative numbers such that $a_1 > 0$ and $\sum a_n$ diverges. Let $S_n = \sum_{k=1}^n a_k$.  Prove that, for all $n \geq 2$, $$\frac{a_n}{S_n^2} \leq \frac{1}{S_{n-1}}-\frac{1}{S_n}$$ How would I start this proof? I've just been staring at it and am very stuck. All i know so far is that $S_n-S_{n-1}=a_n$. Where does the inequality come from?","Let $(a_n)$ be a sequence of non-negative numbers such that $a_1 > 0$ and $\sum a_n$ diverges. Let $S_n = \sum_{k=1}^n a_k$.  Prove that, for all $n \geq 2$, $$\frac{a_n}{S_n^2} \leq \frac{1}{S_{n-1}}-\frac{1}{S_n}$$ How would I start this proof? I've just been staring at it and am very stuck. All i know so far is that $S_n-S_{n-1}=a_n$. Where does the inequality come from?",,"['real-analysis', 'sequences-and-series']"
36,Is $\lim_{n \to \infty} \sqrt[n]{\frac{1}{n!}} = 0$? [duplicate],Is ? [duplicate],\lim_{n \to \infty} \sqrt[n]{\frac{1}{n!}} = 0,"This question already has answers here : $\lim\limits_{n \to{+}\infty}{\sqrt[n]{n!}}$ is infinite (12 answers) Closed 11 years ago . When I was trying to solve a problem to find the radius of convergence of the power series $$\sum \frac{2^nz^n}{n!}$$ I fully understand that the ratio test works well in this one and the radius of convergence is $\infty$. However, knowing that the root test gives a better span of the ratio test and it is necessary to prove it, I wanted to be able to find the radius of convergence using the ratio test. Thus obtaining the following $$\begin{align} \lim_{n \to \infty} \sqrt[n]{\frac{2^nz^n}{n!}} & = |2z|\lim_{n \to \infty} \sqrt[n]{\frac{1}{n!}} \\ \\ & = 0\\ \\ & \lt 1 \\ \\ & \Rightarrow R = \infty \end{align}$$ must be true. So, I was thinking that $\lim_{n \to \infty} \sqrt[n]{\frac{1}{n!}}$ must be equal to 0. Is there a direct proof of this ? I just want to be a bit more algebraically savvy.","This question already has answers here : $\lim\limits_{n \to{+}\infty}{\sqrt[n]{n!}}$ is infinite (12 answers) Closed 11 years ago . When I was trying to solve a problem to find the radius of convergence of the power series $$\sum \frac{2^nz^n}{n!}$$ I fully understand that the ratio test works well in this one and the radius of convergence is $\infty$. However, knowing that the root test gives a better span of the ratio test and it is necessary to prove it, I wanted to be able to find the radius of convergence using the ratio test. Thus obtaining the following $$\begin{align} \lim_{n \to \infty} \sqrt[n]{\frac{2^nz^n}{n!}} & = |2z|\lim_{n \to \infty} \sqrt[n]{\frac{1}{n!}} \\ \\ & = 0\\ \\ & \lt 1 \\ \\ & \Rightarrow R = \infty \end{align}$$ must be true. So, I was thinking that $\lim_{n \to \infty} \sqrt[n]{\frac{1}{n!}}$ must be equal to 0. Is there a direct proof of this ? I just want to be a bit more algebraically savvy.",,"['real-analysis', 'sequences-and-series', 'analysis', 'limits']"
37,$(1+\frac{1}{n\log n})^n-1=O(\frac{1}{n})$.,.,(1+\frac{1}{n\log n})^n-1=O(\frac{1}{n}),"When I solved a problem, I could solve it if I assumed that $(1+\frac{1}{n\log n})^n-1=O(\frac{1}{n})$ I tried to prove it, but I failed. Actually, I don't convince if it is true. Is it correct? If so, how to solve it?","When I solved a problem, I could solve it if I assumed that $(1+\frac{1}{n\log n})^n-1=O(\frac{1}{n})$ I tried to prove it, but I failed. Actually, I don't convince if it is true. Is it correct? If so, how to solve it?",,"['real-analysis', 'analysis', 'analytic-number-theory']"
38,Problem in evaluating logarithm derivatives,Problem in evaluating logarithm derivatives,,"Given the property of the logarithm that $\log{xy} = \log{x} + \log{y}$ , how would one take the 'derivative' of this? To be more clear, $\log{xy} = \log{x} + \log{y}$ (property of $\log$ ) $D(\log{xy}) = D(\log{x} + \log{y})$ (i) (Take derivative on both sides) Now, $D(\log{x} + \log{y}) = D(\log{x}) + D(\log{y})$ (ii) (Derivative of sum is sum of derivatives) Combining (i) and (ii): $D(\log{xy}) = D(\log{x}) + D(\log{y})$ (iii) Implies: $\frac{1}{xy} = \frac{1}{x} + \frac{1}{y}$ (Evaluate derivative of logarithm using $D(\log{x}) = \frac{1}{x}$ $\frac{1}{xy} = \frac{1}{x} + \frac{1}{y}$ looks false to me; e.g. while $\log{6}$ does equal $\log{2} + \log{3}$ , $\frac{1}{6}$ does not equal $\frac{1}{2} + \frac{1}{3}$ . My first guess was that the issue was related to what variable I take the derivative with respect to, but I'd like to understand this a little more formally if someone could guide me. What's going wrong in this example?","Given the property of the logarithm that , how would one take the 'derivative' of this? To be more clear, (property of ) (i) (Take derivative on both sides) Now, (ii) (Derivative of sum is sum of derivatives) Combining (i) and (ii): (iii) Implies: (Evaluate derivative of logarithm using looks false to me; e.g. while does equal , does not equal . My first guess was that the issue was related to what variable I take the derivative with respect to, but I'd like to understand this a little more formally if someone could guide me. What's going wrong in this example?",\log{xy} = \log{x} + \log{y} \log{xy} = \log{x} + \log{y} \log D(\log{xy}) = D(\log{x} + \log{y}) D(\log{x} + \log{y}) = D(\log{x}) + D(\log{y}) D(\log{xy}) = D(\log{x}) + D(\log{y}) \frac{1}{xy} = \frac{1}{x} + \frac{1}{y} D(\log{x}) = \frac{1}{x} \frac{1}{xy} = \frac{1}{x} + \frac{1}{y} \log{6} \log{2} + \log{3} \frac{1}{6} \frac{1}{2} + \frac{1}{3},"['real-analysis', 'calculus', 'logarithms']"
39,Disagreement with professor about Cauchy Sequence,Disagreement with professor about Cauchy Sequence,,"Remark: in this post, all limits are infinite limits so I'll write just $\lim$ instead of $\lim_{n \rightarrow \infty}$ to save time and notation. Also I want to say that I already wrote the proof to the problem below using the definition but I want to discuss this particular proof. Problem: If $\lim x_{2n} = a$ and $\lim x_{2n -1} = a$ , prove that $\lim  x_n = a$ . Attempt: Define $X_n = \{x_{2n}\} \cup \{x_{2n - 1}\}$ . Take two elements from $X_n$ and let's calculate the limit of their difference: $$ \lim (x_{2n} - x_{2n - 1}) = a - a = 0 $$ Therefore $X_n$ is a Cauchy sequence. Now we use the result that if a Cauchy sequence has a subsequence converging to $a$ , then $\lim X_n = a$ . End of proof. Discussion: I showed this proof to my professor. He said it's wrong because in the limit $$ \lim_{n \rightarrow \infty, m \rightarrow \infty} x_n - x_m = 0$$ $n$ and $m$ cannot be related. I accepted his argument. Later, out of curiosity I looked up again the definition of Cauchy sequence and nowhere does it say that $n$ and $m$ cannot be related. Then I brought it up to my professor, and this time he said that I'm choosing a particular $n$ and $m$ and that I cannot do that in proving that $X_n$ is a Cauchy sequence. I thought that I'm not fixing anything since $2n$ and $2n - 1$ are not fixed, and even then I could write $$ \lim (x_{2n} - x_{2m}) = a - a = 0  $$ and what I wrote would still hold true.. However, I chose not to continue the discussion as I felt that if I insisted on this proof, my professor would feel antagonized and some negativity would be created. However I still fail to see why the proof is incorrect. Personally it felt like my professor just didn't accept that I came up with a clever proof.","Remark: in this post, all limits are infinite limits so I'll write just instead of to save time and notation. Also I want to say that I already wrote the proof to the problem below using the definition but I want to discuss this particular proof. Problem: If and , prove that . Attempt: Define . Take two elements from and let's calculate the limit of their difference: Therefore is a Cauchy sequence. Now we use the result that if a Cauchy sequence has a subsequence converging to , then . End of proof. Discussion: I showed this proof to my professor. He said it's wrong because in the limit and cannot be related. I accepted his argument. Later, out of curiosity I looked up again the definition of Cauchy sequence and nowhere does it say that and cannot be related. Then I brought it up to my professor, and this time he said that I'm choosing a particular and and that I cannot do that in proving that is a Cauchy sequence. I thought that I'm not fixing anything since and are not fixed, and even then I could write and what I wrote would still hold true.. However, I chose not to continue the discussion as I felt that if I insisted on this proof, my professor would feel antagonized and some negativity would be created. However I still fail to see why the proof is incorrect. Personally it felt like my professor just didn't accept that I came up with a clever proof.","\lim \lim_{n \rightarrow \infty} \lim x_{2n} = a \lim x_{2n -1} = a \lim  x_n = a X_n = \{x_{2n}\} \cup \{x_{2n - 1}\} X_n  \lim (x_{2n} - x_{2n - 1}) = a - a = 0  X_n a \lim X_n = a  \lim_{n \rightarrow \infty, m \rightarrow \infty} x_n - x_m = 0 n m n m n m X_n 2n 2n - 1  \lim (x_{2n} - x_{2m}) = a - a = 0  ","['real-analysis', 'sequences-and-series', 'cauchy-sequences']"
40,Why doesn't integration by substitution work in this case?,Why doesn't integration by substitution work in this case?,,"I am asked to solve $\int_0^\frac{\pi}{4}\tan x\, \mathrm{d}x$. This is what I did: $$ \begin{align} &\int_0^\frac{\pi}{4}\tan x \,\mathrm{d}x&\\ = {} &\int_0^\frac{\pi}{4}\frac{\sin x}{\cos x}\, \mathrm{d}x&\\ = {} &\int_0^\frac{\pi}{4}\sin x\frac{-1}{-\cos x} \,\mathrm{d}x&\\ = {} &-\int_0^\frac{\pi}{4}\varphi'(x)\frac{1}{\varphi(x)} \,\mathrm{d}x &\text{where $\varphi(x):=-\cos x$} \\ = {} &-\int_{\varphi(0)}^{\varphi(\frac{\pi}{4})} \frac{1}{z} \,\mathrm{d}z \\ = {} &-\int_{-1}^{-\frac{1}{\sqrt{2}}} \frac{1}{z}\, \mathrm{d}z \\ = {} &\Big[\ln z\Big]_{-1}^{-\frac{1}{\sqrt{2}}} \\ = {} &\ln \left(-\frac{1}{\sqrt{2}}\right) - \ln (-1) \end{align} $$ And this is where I am stuck, because the solution is definitely not a complex number. I know the correct answer (it's $-\ln\left(\frac{1}{\sqrt{2}}\right)$, my problem is that I don't know where I made a mistake.","I am asked to solve $\int_0^\frac{\pi}{4}\tan x\, \mathrm{d}x$. This is what I did: $$ \begin{align} &\int_0^\frac{\pi}{4}\tan x \,\mathrm{d}x&\\ = {} &\int_0^\frac{\pi}{4}\frac{\sin x}{\cos x}\, \mathrm{d}x&\\ = {} &\int_0^\frac{\pi}{4}\sin x\frac{-1}{-\cos x} \,\mathrm{d}x&\\ = {} &-\int_0^\frac{\pi}{4}\varphi'(x)\frac{1}{\varphi(x)} \,\mathrm{d}x &\text{where $\varphi(x):=-\cos x$} \\ = {} &-\int_{\varphi(0)}^{\varphi(\frac{\pi}{4})} \frac{1}{z} \,\mathrm{d}z \\ = {} &-\int_{-1}^{-\frac{1}{\sqrt{2}}} \frac{1}{z}\, \mathrm{d}z \\ = {} &\Big[\ln z\Big]_{-1}^{-\frac{1}{\sqrt{2}}} \\ = {} &\ln \left(-\frac{1}{\sqrt{2}}\right) - \ln (-1) \end{align} $$ And this is where I am stuck, because the solution is definitely not a complex number. I know the correct answer (it's $-\ln\left(\frac{1}{\sqrt{2}}\right)$, my problem is that I don't know where I made a mistake.",,"['real-analysis', 'integration', 'definite-integrals', 'substitution']"
41,Proving that there is an irrational number between any two unequal rational numbers.,Proving that there is an irrational number between any two unequal rational numbers.,,"I'm trying to prove that there is an irrational number between any two unequal rational numbers $a, b$. Here's a ""proof"" I have right now, but I'm not sure if it works. Let $a, b$ be two unequal rational numbers and, WLOG, let $a < b$. Suppose to the contrary that there was an interval $[a, b]$, with $a, b$ rational, which contained no irrational numbers. That would imply that the interval contained only rational numbers since the reals are composed of rationals and irrational numbers. Furthermore, this interval has measure $b-a$, a contradiction since this is a subset of $\mathbb{Q}$ which has measure zero. Does this work? Is there an easier way to go about it, perhaps through a construction? Construction: Let $a = \frac{m}{n}$, $b = \frac{p}{q}$. WLOG $a>b$. Then $a-b = \frac{m}{n}-\frac{p}{q} = \frac{mq-np}{nq}$. Since $mq - np > 1$, we can construct an irrational number $a + \frac{1}{nq\sqrt2}$ which is between $a$ and $b$.","I'm trying to prove that there is an irrational number between any two unequal rational numbers $a, b$. Here's a ""proof"" I have right now, but I'm not sure if it works. Let $a, b$ be two unequal rational numbers and, WLOG, let $a < b$. Suppose to the contrary that there was an interval $[a, b]$, with $a, b$ rational, which contained no irrational numbers. That would imply that the interval contained only rational numbers since the reals are composed of rationals and irrational numbers. Furthermore, this interval has measure $b-a$, a contradiction since this is a subset of $\mathbb{Q}$ which has measure zero. Does this work? Is there an easier way to go about it, perhaps through a construction? Construction: Let $a = \frac{m}{n}$, $b = \frac{p}{q}$. WLOG $a>b$. Then $a-b = \frac{m}{n}-\frac{p}{q} = \frac{mq-np}{nq}$. Since $mq - np > 1$, we can construct an irrational number $a + \frac{1}{nq\sqrt2}$ which is between $a$ and $b$.",,['real-analysis']
42,Fourier Analysis,Fourier Analysis,,"I am interested in Fourier Analysis. But I don't get why the coefficients are choosen that way and why the Fourier series will converge to a given function? Can someone provide me simple information or do you know an easy source for a beginner? Regards, Kevin","I am interested in Fourier Analysis. But I don't get why the coefficients are choosen that way and why the Fourier series will converge to a given function? Can someone provide me simple information or do you know an easy source for a beginner? Regards, Kevin",,"['real-analysis', 'reference-request', 'fourier-analysis', 'complex-numbers', 'fourier-series']"
43,Difference between closed and open intervals for continuous functions,Difference between closed and open intervals for continuous functions,,"Say I have a continuous function $f(x)$ on the open interval $(0, 1)$. There is no limit on how large $|f(x)|$ can be apart from the fact that it can't be infinity, it will always be a real number. Now say I have a continuous function $f(x)$ on the closed interval $[0, 1]$. This too has no limit on how large $|f(x)|$ can be apart from the fact that it can't be infinity, it will always be a real number. So it seems like there is no difference between them...and they are both unbounded? But from what I've read continuous functions on closed intervals are bounded... This seems like a chicken and an egg scenario to me. Can anyone set me straight on the difference between continuous functions on open intervals vs. closed intervals?","Say I have a continuous function $f(x)$ on the open interval $(0, 1)$. There is no limit on how large $|f(x)|$ can be apart from the fact that it can't be infinity, it will always be a real number. Now say I have a continuous function $f(x)$ on the closed interval $[0, 1]$. This too has no limit on how large $|f(x)|$ can be apart from the fact that it can't be infinity, it will always be a real number. So it seems like there is no difference between them...and they are both unbounded? But from what I've read continuous functions on closed intervals are bounded... This seems like a chicken and an egg scenario to me. Can anyone set me straight on the difference between continuous functions on open intervals vs. closed intervals?",,"['real-analysis', 'analysis', 'limits', 'continuity']"
44,Is it true that if the derivative of a function is not continuous then the function is not differentiable?,Is it true that if the derivative of a function is not continuous then the function is not differentiable?,,"It has been along time since I did real analysis. Someone asked me this question and I could not respond. I am not sure if this is the right place to ask. He said: The function $f: [0, \infty)\mapsto [0, \infty)$ given by $f(x)=\sqrt{x}$ is not differentiable at $0$ because its derivative $f'(x)=1/(2\sqrt{x})$ is not continuous at $0$. I said (what I could remember): It is not differentiable at $0$ because $\lim_{x\to 0}(f(x)-f(0))/(x-0)=\infty$. He said that I know that but what I told is it true?","It has been along time since I did real analysis. Someone asked me this question and I could not respond. I am not sure if this is the right place to ask. He said: The function $f: [0, \infty)\mapsto [0, \infty)$ given by $f(x)=\sqrt{x}$ is not differentiable at $0$ because its derivative $f'(x)=1/(2\sqrt{x})$ is not continuous at $0$. I said (what I could remember): It is not differentiable at $0$ because $\lim_{x\to 0}(f(x)-f(0))/(x-0)=\infty$. He said that I know that but what I told is it true?",,"['real-analysis', 'derivatives', 'continuity']"
45,Why doesn't derivative difference quotient violate the epsilon-delta definition of a limit?,Why doesn't derivative difference quotient violate the epsilon-delta definition of a limit?,,"So the difference quotient is defined as: $$\lim \limits_{h \to 0} \frac{f(x+h)-f(x)}{h}$$ So if we take a function such as $f(x)=x^2$ and go through the simplification, we get $$\lim \limits_{h \to 0} 2x+h $$ We say $h$ is zero, and that makes sense because it becomes negligible. But here's what I don't understand: $\delta$-$\epsilon$ says (in casual terms) we can make $h$ as close as we want to zero and $2x+h$ will be sufficiently close to $2x$. But isn't one of the constraints of delta epsilon that $|\delta - c| > 0$? So how can we say $h$ is exactly zero if this constraint must be met? In other words, we always say the value of the limit at the point of interest does not necessarily equal the value of the function - but here it seems like we're saying that they are by making $h$ exactly zero, not just sufficiently close. Why can we do this?","So the difference quotient is defined as: $$\lim \limits_{h \to 0} \frac{f(x+h)-f(x)}{h}$$ So if we take a function such as $f(x)=x^2$ and go through the simplification, we get $$\lim \limits_{h \to 0} 2x+h $$ We say $h$ is zero, and that makes sense because it becomes negligible. But here's what I don't understand: $\delta$-$\epsilon$ says (in casual terms) we can make $h$ as close as we want to zero and $2x+h$ will be sufficiently close to $2x$. But isn't one of the constraints of delta epsilon that $|\delta - c| > 0$? So how can we say $h$ is exactly zero if this constraint must be met? In other words, we always say the value of the limit at the point of interest does not necessarily equal the value of the function - but here it seems like we're saying that they are by making $h$ exactly zero, not just sufficiently close. Why can we do this?",,"['calculus', 'real-analysis', 'limits', 'derivatives', 'epsilon-delta']"
46,Elementary proof of $\lim_{n\to\infty}n(\sqrt[n]{n}-1)=\infty$,Elementary proof of,\lim_{n\to\infty}n(\sqrt[n]{n}-1)=\infty,"This question is closely related to this question , but I am not happy with the answers there for several reasons which I will explain in a second. The limit $\lim_{n\to\infty}n(\sqrt[n]{n}-1)=\infty$ , where $n$ is a natural number, is easy to see by expanding the left side with the help of the exponential series. Indeed, we have $$ n(\sqrt[n]{n}-1)=\ln(n)+\frac{1}{2}\cdot\frac{1}{n}\cdot\ln(n)^2+\frac{1}{6}\cdot\frac{1}{n^2}\cdot\ln(n)^3+\cdots\geq\ln(n)\,. $$ Since $\ln(n)$ grows arbitrary large with $n$ large, the limit is proven. I found this limit as an exercise in Analysis 1 by K. Knigsberger, 5.8 Exercises, 3(b). I am using an old printing and the numbering might have changed, but it is in the very beginning of the book in a chapter about sequences. At that stage of the book the exponential series as well as logarithms have not yet been introduced and very few means are available. For educational purposes, I am looking for a really elementary proof which uses a different bound from below which in turn goes to infinity. The book suggests that such a proof must exist but I cannot find one. Can you please help me to find such proof? What is available at this stage is the Bernoulli inequality and the expansion of $(1+x)^n$ for a natural number $n$ and arbitrary $x$ , plus some very basic limits like $\sqrt[n]{n}\to 1$ , etc. which all can be done elementary. Thank you for your time and help!","This question is closely related to this question , but I am not happy with the answers there for several reasons which I will explain in a second. The limit , where is a natural number, is easy to see by expanding the left side with the help of the exponential series. Indeed, we have Since grows arbitrary large with large, the limit is proven. I found this limit as an exercise in Analysis 1 by K. Knigsberger, 5.8 Exercises, 3(b). I am using an old printing and the numbering might have changed, but it is in the very beginning of the book in a chapter about sequences. At that stage of the book the exponential series as well as logarithms have not yet been introduced and very few means are available. For educational purposes, I am looking for a really elementary proof which uses a different bound from below which in turn goes to infinity. The book suggests that such a proof must exist but I cannot find one. Can you please help me to find such proof? What is available at this stage is the Bernoulli inequality and the expansion of for a natural number and arbitrary , plus some very basic limits like , etc. which all can be done elementary. Thank you for your time and help!","\lim_{n\to\infty}n(\sqrt[n]{n}-1)=\infty n 
n(\sqrt[n]{n}-1)=\ln(n)+\frac{1}{2}\cdot\frac{1}{n}\cdot\ln(n)^2+\frac{1}{6}\cdot\frac{1}{n^2}\cdot\ln(n)^3+\cdots\geq\ln(n)\,.
 \ln(n) n (1+x)^n n x \sqrt[n]{n}\to 1","['real-analysis', 'sequences-and-series', 'limits', 'analysis', 'limits-without-lhopital']"
47,Find the limit $\lim_{n \to \infty} \frac {2n^2+10n+5}{n^2}$ and prove it.,Find the limit  and prove it.,\lim_{n \to \infty} \frac {2n^2+10n+5}{n^2},"Find $\lim_{n \to \infty} \frac {2n^2+10n+5}{n^2}$. I claim that $\lim_{n \to \infty} \frac {2n^2+10n+5}{n^2}=2$. To prove this, for given $\varepsilon >0$, I have to find $M\in N$ such that $|\frac {2n^2+10n+5}{n^2}-2|<\varepsilon$ for $n \ge M$. By Archimedean property, we can find $M \in N$ such that $\frac {15}M<\varepsilon$, and note that $n\ge M \rightarrow \frac 1n \le \frac 1M \rightarrow \frac {15}n \le \frac {15}M$. Then, for $n \ge M$, we have that $|\frac {2n^2+10n+5}{n^2}-2|=|\frac {10n+5}{n^2}| < |\frac {15n}{n^2}|$ (since $n \ge M \in N$) $<\frac {15}n\le \frac {15}M<\varepsilon$. Therefore, by definition of convergence, $\lim_{n \to \infty} \frac {2n^2+10n+5}{n^2}=2$. Should I say $M\in Z^+$ (because I am worried about the case where $M=0$)? Can you find any mistakes in this proof? Thank you in advance.","Find $\lim_{n \to \infty} \frac {2n^2+10n+5}{n^2}$. I claim that $\lim_{n \to \infty} \frac {2n^2+10n+5}{n^2}=2$. To prove this, for given $\varepsilon >0$, I have to find $M\in N$ such that $|\frac {2n^2+10n+5}{n^2}-2|<\varepsilon$ for $n \ge M$. By Archimedean property, we can find $M \in N$ such that $\frac {15}M<\varepsilon$, and note that $n\ge M \rightarrow \frac 1n \le \frac 1M \rightarrow \frac {15}n \le \frac {15}M$. Then, for $n \ge M$, we have that $|\frac {2n^2+10n+5}{n^2}-2|=|\frac {10n+5}{n^2}| < |\frac {15n}{n^2}|$ (since $n \ge M \in N$) $<\frac {15}n\le \frac {15}M<\varepsilon$. Therefore, by definition of convergence, $\lim_{n \to \infty} \frac {2n^2+10n+5}{n^2}=2$. Should I say $M\in Z^+$ (because I am worried about the case where $M=0$)? Can you find any mistakes in this proof? Thank you in advance.",,"['real-analysis', 'sequences-and-series', 'limits', 'proof-verification', 'epsilon-delta']"
48,Integrating the following function,Integrating the following function,,"I am asked to evaluate the integral $$\int_{R^n}e^{-\sum_{i=1}^na_i^2x_i^2}\,dx$$ given that $a_1,a_2,\cdots,a_n$ are real numbers different from $0$. I am clueless on how to approach this, since I have never been exposed to these types of integrals. Any help is appreciated!","I am asked to evaluate the integral $$\int_{R^n}e^{-\sum_{i=1}^na_i^2x_i^2}\,dx$$ given that $a_1,a_2,\cdots,a_n$ are real numbers different from $0$. I am clueless on how to approach this, since I have never been exposed to these types of integrals. Any help is appreciated!",,"['real-analysis', 'integration', 'lebesgue-measure']"
49,"continuous onto map from $(0,1)\to (0,1]$",continuous onto map from,"(0,1)\to (0,1]","I need to know whether There exists any continuous onto map from $(0,1)\to (0,1]$ could any one give me any hint?","I need to know whether There exists any continuous onto map from $(0,1)\to (0,1]$ could any one give me any hint?",,"['real-analysis', 'general-topology', 'continuity']"
50,"Closed subsets $A,B\subset\mathbb{R}^2$ so that $A+B$ is not closed",Closed subsets  so that  is not closed,"A,B\subset\mathbb{R}^2 A+B","I am looking for closed subsets $A,B\subset\mathbb{R}^2$ so that $A+B$ is not closed. I define $A+B=\{a+b:a\in A,b\in B\}$ I thought of this example, but it is only in $\mathbb{R}$. Take: $A=\{\frac{1}{n}:n\in\mathbb{Z^+}\}\cup\{0\}$ and $B=\mathbb{Z}$ both of these are closed (is this correct?). But their sum $A+B=\mathbb{Q}$ which is not closed.","I am looking for closed subsets $A,B\subset\mathbb{R}^2$ so that $A+B$ is not closed. I define $A+B=\{a+b:a\in A,b\in B\}$ I thought of this example, but it is only in $\mathbb{R}$. Take: $A=\{\frac{1}{n}:n\in\mathbb{Z^+}\}\cup\{0\}$ and $B=\mathbb{Z}$ both of these are closed (is this correct?). But their sum $A+B=\mathbb{Q}$ which is not closed.",,['real-analysis']
51,$ \sum_{k=1}^{\infty} \ln{\left(1 + \frac{1}{4 k^2}\right)}$ Computing this sum,Computing this sum, \sum_{k=1}^{\infty} \ln{\left(1 + \frac{1}{4 k^2}\right)},"Compute the limit: $$ \sum_{k=1}^{\infty} \ln{\left(1 + \frac{1}{4 k^2}\right)}$$ My teacher says it can be solved by only using high school knowledge , but I don't see how. What did I try? Well, I thought of Riemann sums but I see no way to connect this sum to it. Thanks. I'm only interested in a solution at high school level if possible ! UPDATE: Now, I'm my own teacher.","Compute the limit: $$ \sum_{k=1}^{\infty} \ln{\left(1 + \frac{1}{4 k^2}\right)}$$ My teacher says it can be solved by only using high school knowledge , but I don't see how. What did I try? Well, I thought of Riemann sums but I see no way to connect this sum to it. Thanks. I'm only interested in a solution at high school level if possible ! UPDATE: Now, I'm my own teacher.",,"['calculus', 'real-analysis', 'sequences-and-series']"
52,Making sure if it is Cauchy,Making sure if it is Cauchy,,"In my real analysis exam I had a problem in which I proved that given any positive number $a\lt 1$ if $|x_{n+1} - x_n|\lt {a^n}$ for all natural numbers $n$ then $(x_n)$ is a Cauchy sequence. This was solved successfully but the question is if $|x_{n+1} - x_n|\lt \frac 1n$ does that mean $(x_n)$ is Cauchy? Well my answer was yes because I could write this in the form of the first one, but now I am somehow confused with what I have answered since $1/n$ is a sequence of $n$ so maybe the answer is not necessarily true... Can you please provide me with the correct answer for this question?","In my real analysis exam I had a problem in which I proved that given any positive number if for all natural numbers then is a Cauchy sequence. This was solved successfully but the question is if does that mean is Cauchy? Well my answer was yes because I could write this in the form of the first one, but now I am somehow confused with what I have answered since is a sequence of so maybe the answer is not necessarily true... Can you please provide me with the correct answer for this question?",a\lt 1 |x_{n+1} - x_n|\lt {a^n} n (x_n) |x_{n+1} - x_n|\lt \frac 1n (x_n) 1/n n,['real-analysis']
53,Cardinality of separable metric spaces,Cardinality of separable metric spaces,,"Show that $\mathbb R$ is cardinally larger than any separable metric space S. I have been trying to solve this on my own. My idea was to start by mapping the open balls of positive rational radius around the points in the dense set of S to the corresponding in $\mathbb R$. Now any ${x}\subset S$ can be written as a limit of an appropriate sequence of open balls, which would correspond to a similar limit in $\mathbb R$, in case the limit exists. Then I would need to show that different limits are obtained in $\mathbb R$ for different $x\in S$. This is a step I am not sure how to show. Alternatively, one could show that all possible sequences of rationals have no greater cardinality than reals. What would be a good way to show this? I know that all reals can be written as some convergent sequence of rationals, but I am looking for something slightly different here. I did not find this useful as it does not fill in the details: Every separable metric space has cardinality less than or equal to the cardinality of the continuum.","Show that $\mathbb R$ is cardinally larger than any separable metric space S. I have been trying to solve this on my own. My idea was to start by mapping the open balls of positive rational radius around the points in the dense set of S to the corresponding in $\mathbb R$. Now any ${x}\subset S$ can be written as a limit of an appropriate sequence of open balls, which would correspond to a similar limit in $\mathbb R$, in case the limit exists. Then I would need to show that different limits are obtained in $\mathbb R$ for different $x\in S$. This is a step I am not sure how to show. Alternatively, one could show that all possible sequences of rationals have no greater cardinality than reals. What would be a good way to show this? I know that all reals can be written as some convergent sequence of rationals, but I am looking for something slightly different here. I did not find this useful as it does not fill in the details: Every separable metric space has cardinality less than or equal to the cardinality of the continuum.",,"['real-analysis', 'elementary-set-theory']"
54,Why do some say that $f(x)=\frac{1}{x}$ discontinuous?,Why do some say that  discontinuous?,f(x)=\frac{1}{x},"A continuous function $f:X\to Y$ is one which satisfies the following property: for every open set $U\subset Y$, the set $f^{-1}(U)$ is open in $X$. I don't see why according to this definition $f(x)=\frac{1}{x}$ is discontinuous at $0$. The inverse image of every open set of the form $(a,\infty)$ is open, it is $(0,\frac{1}{a})$. Similarly, the inverse image of $(-\infty,-a)$ is $(-\frac{1}{a},0)$. And one can see that the inverses of other forms of open sets are also clearly open. Why then is $1/x$ discontinuous according to some?","A continuous function $f:X\to Y$ is one which satisfies the following property: for every open set $U\subset Y$, the set $f^{-1}(U)$ is open in $X$. I don't see why according to this definition $f(x)=\frac{1}{x}$ is discontinuous at $0$. The inverse image of every open set of the form $(a,\infty)$ is open, it is $(0,\frac{1}{a})$. Similarly, the inverse image of $(-\infty,-a)$ is $(-\frac{1}{a},0)$. And one can see that the inverses of other forms of open sets are also clearly open. Why then is $1/x$ discontinuous according to some?",,"['calculus', 'real-analysis', 'continuity']"
55,"If $\sum_{n=1}^{\infty} a_n$ is absolutely convergent, then $\sum_{n=1}^{\infty} (a_n)^2$ is convergent [closed]","If  is absolutely convergent, then  is convergent [closed]",\sum_{n=1}^{\infty} a_n \sum_{n=1}^{\infty} (a_n)^2,"Closed. This question is off-topic . It is not currently accepting answers. This question is missing context or other details : Please improve the question by providing additional context, which ideally includes your thoughts on the problem and any attempts you have made to solve it. This information helps others identify where you have difficulties and helps them write answers appropriate to your experience level. Closed 8 years ago . Improve this question Let $\sum_{n=1}^{\infty} a_n$ be a series in R. Prove that if $\sum_{n=1}^{\infty} a_n$ is absolutely convergent, then $\sum_{n=1}^{\infty} (a_n)^2$ is convergent.","Closed. This question is off-topic . It is not currently accepting answers. This question is missing context or other details : Please improve the question by providing additional context, which ideally includes your thoughts on the problem and any attempts you have made to solve it. This information helps others identify where you have difficulties and helps them write answers appropriate to your experience level. Closed 8 years ago . Improve this question Let $\sum_{n=1}^{\infty} a_n$ be a series in R. Prove that if $\sum_{n=1}^{\infty} a_n$ is absolutely convergent, then $\sum_{n=1}^{\infty} (a_n)^2$ is convergent.",,['real-analysis']
56,How to prove $\sum\limits_{n=1}^{\infty}\left ( \frac{1}{4n+1}-\frac{1}{4n} \right )=\frac{1}{8}\left ( \pi-8+6\ln{2} \right )$?,How to prove ?,\sum\limits_{n=1}^{\infty}\left ( \frac{1}{4n+1}-\frac{1}{4n} \right )=\frac{1}{8}\left ( \pi-8+6\ln{2} \right ),"I am trying to prove this. I used the telescoping method, but the problem is I need the first fraction to be $\frac{1}{4(n+1)}$ and I also tried to relate it to alternating Harmonic series which didn't work. Any hint would be greatly appreciated. $$\sum_{n=1}^{\infty}\left ( \frac{1}{4n+1}-\frac{1}{4n} \right )=\frac{1}{8}\left ( \pi-8+6\ln{2} \right )$$","I am trying to prove this. I used the telescoping method, but the problem is I need the first fraction to be and I also tried to relate it to alternating Harmonic series which didn't work. Any hint would be greatly appreciated.",\frac{1}{4(n+1)} \sum_{n=1}^{\infty}\left ( \frac{1}{4n+1}-\frac{1}{4n} \right )=\frac{1}{8}\left ( \pi-8+6\ln{2} \right ),"['real-analysis', 'sequences-and-series']"
57,Different ways of proving that $|\sum^{\infty}_{k=1}(1-\cos(1/k))|\leq 2 $,Different ways of proving that,|\sum^{\infty}_{k=1}(1-\cos(1/k))|\leq 2 ,"I've found two ways of proving that \begin{align} \left|\sum^{\infty}_{k=1}\left[1-\cos\left(\dfrac{1}{k}\right)\right]\right|&\leq 2   \end{align} Are there any other ways out there, for proving this? METHOD 1 Let $k\in \Bbb{N}$ , then \begin{align} f:[ 0&,1]\longrightarrow \Bbb{R}\\&x \mapsto  \cos\left(\dfrac{x}{k}\right)   \end{align} is continuous. Then, by Mean Value Theorem, there exists $c\in [ 0,x]$ such that \begin{align} \cos\left(\dfrac{x}{k}\right)-\cos\left(0\right) =-\dfrac{1}{k}\sin\left(\dfrac{c}{k}\right)\,(x-0), \end{align} which implies \begin{align} \left|\sum^{\infty}_{k=1}\left[1-\cos\left(\dfrac{1}{k}\right)\right]\right| &=\left|\sum^{\infty}_{k=1}\dfrac{x}{k}\sin\left(\dfrac{c}{k}\right)\right| \leq \sum^{\infty}_{k=1}\dfrac{\left|x\right|}{k}\left|\sin\left(\dfrac{c}{k}\right)\right|\leq \sum^{\infty}_{k=1}\dfrac{\left|x\right|}{k}\dfrac{\left|c\right|}{k}\\&\leq \sum^{\infty}_{k=1}\left(\dfrac{\left|x\right|}{k}\right)^2\leq \sum^{\infty}_{k=1}\dfrac{1}{k^2}=1+ \sum^{\infty}_{k=2}\dfrac{1}{k^2}\\&\leq 1+ \sum^{\infty}_{k=2}\dfrac{1}{k(k-1)}\\&= 1+ \lim\limits_{n\to\infty}\sum^{n}_{k=2}\left(\dfrac{1}{k-1}-\dfrac{1}{k}\right)\\&=1+ \lim\limits_{n\to\infty}\left(1-\dfrac{1}{n}\right)\\&=2, \end{align} METHOD 2 Let $x\in [0,1]$ be fixed, then \begin{align} \sum^{\infty}_{k=1}\left[1-\cos\left(\dfrac{1}{k}\right)\right]&=\sum^{\infty}_{k=1}\dfrac{1}{k}\left[-k\cos\left(\dfrac{x}{k}\right)\right]^{1}_{0}=\sum^{\infty}_{k=1}\dfrac{1}{k}\int^{1}_{0}\sin\left(\dfrac{x}{k}\right)dx \\&=\sum^{\infty}_{k=1}\int^{1}_{0}\dfrac{1}{k}\sin\left(\dfrac{x}{k}\right)dx  \end{align} The series $\sum^{\infty}_{k=1}\dfrac{1}{k}\sin\left(\dfrac{x}{k}\right)$ converges uniformly on $[0,1]$ , by Weierstrass-M test, since \begin{align} \left|\sum^{\infty}_{k=1}\dfrac{1}{k}\sin\left(\dfrac{x}{k}\right) \right|\leq \sum^{\infty}_{k=1}\dfrac{1}{k}\left|\sin\left(\dfrac{x}{k}\right) \right|\leq \sum^{\infty}_{k=1}\dfrac{1}{k^2}. \end{align} Hence, \begin{align} \sum^{\infty}_{k=1}\left[1-\cos\left(\dfrac{1}{k}\right)\right]&=\sum^{\infty}_{k=1}\int^{1}_{0}\dfrac{1}{k}\sin\left(\dfrac{x}{k}\right)dx=\int^{1}_{0}\sum^{\infty}_{k=1}\dfrac{1}{k}\sin\left(\dfrac{x}{k}\right)dx,  \end{align} and \begin{align} \left|\sum^{\infty}_{k=1}\left[1-\cos\left(\dfrac{1}{k}\right)\right]\right|&=\left|\int^{1}_{0}\sum^{\infty}_{k=1}\dfrac{1}{k}\sin\left(\dfrac{x}{k}\right)dx\right|\leq\int^{1}_{0}\sum^{\infty}_{k=1}\dfrac{1}{k}\left|\sin\left(\dfrac{x}{k}\right)\right|dx \\&\leq\int^{1}_{0}\sum^{\infty}_{k=1}\dfrac{1}{k^2}dx \\&\leq 2 \end{align}","I've found two ways of proving that Are there any other ways out there, for proving this? METHOD 1 Let , then is continuous. Then, by Mean Value Theorem, there exists such that which implies METHOD 2 Let be fixed, then The series converges uniformly on , by Weierstrass-M test, since Hence, and","\begin{align} \left|\sum^{\infty}_{k=1}\left[1-\cos\left(\dfrac{1}{k}\right)\right]\right|&\leq 2   \end{align} k\in \Bbb{N} \begin{align} f:[ 0&,1]\longrightarrow \Bbb{R}\\&x \mapsto 
\cos\left(\dfrac{x}{k}\right)   \end{align} c\in [ 0,x] \begin{align} \cos\left(\dfrac{x}{k}\right)-\cos\left(0\right) =-\dfrac{1}{k}\sin\left(\dfrac{c}{k}\right)\,(x-0), \end{align} \begin{align} \left|\sum^{\infty}_{k=1}\left[1-\cos\left(\dfrac{1}{k}\right)\right]\right| &=\left|\sum^{\infty}_{k=1}\dfrac{x}{k}\sin\left(\dfrac{c}{k}\right)\right| \leq \sum^{\infty}_{k=1}\dfrac{\left|x\right|}{k}\left|\sin\left(\dfrac{c}{k}\right)\right|\leq \sum^{\infty}_{k=1}\dfrac{\left|x\right|}{k}\dfrac{\left|c\right|}{k}\\&\leq \sum^{\infty}_{k=1}\left(\dfrac{\left|x\right|}{k}\right)^2\leq \sum^{\infty}_{k=1}\dfrac{1}{k^2}=1+ \sum^{\infty}_{k=2}\dfrac{1}{k^2}\\&\leq 1+ \sum^{\infty}_{k=2}\dfrac{1}{k(k-1)}\\&= 1+ \lim\limits_{n\to\infty}\sum^{n}_{k=2}\left(\dfrac{1}{k-1}-\dfrac{1}{k}\right)\\&=1+ \lim\limits_{n\to\infty}\left(1-\dfrac{1}{n}\right)\\&=2, \end{align} x\in [0,1] \begin{align} \sum^{\infty}_{k=1}\left[1-\cos\left(\dfrac{1}{k}\right)\right]&=\sum^{\infty}_{k=1}\dfrac{1}{k}\left[-k\cos\left(\dfrac{x}{k}\right)\right]^{1}_{0}=\sum^{\infty}_{k=1}\dfrac{1}{k}\int^{1}_{0}\sin\left(\dfrac{x}{k}\right)dx \\&=\sum^{\infty}_{k=1}\int^{1}_{0}\dfrac{1}{k}\sin\left(\dfrac{x}{k}\right)dx  \end{align} \sum^{\infty}_{k=1}\dfrac{1}{k}\sin\left(\dfrac{x}{k}\right) [0,1] \begin{align} \left|\sum^{\infty}_{k=1}\dfrac{1}{k}\sin\left(\dfrac{x}{k}\right) \right|\leq \sum^{\infty}_{k=1}\dfrac{1}{k}\left|\sin\left(\dfrac{x}{k}\right) \right|\leq \sum^{\infty}_{k=1}\dfrac{1}{k^2}. \end{align} \begin{align} \sum^{\infty}_{k=1}\left[1-\cos\left(\dfrac{1}{k}\right)\right]&=\sum^{\infty}_{k=1}\int^{1}_{0}\dfrac{1}{k}\sin\left(\dfrac{x}{k}\right)dx=\int^{1}_{0}\sum^{\infty}_{k=1}\dfrac{1}{k}\sin\left(\dfrac{x}{k}\right)dx,  \end{align} \begin{align} \left|\sum^{\infty}_{k=1}\left[1-\cos\left(\dfrac{1}{k}\right)\right]\right|&=\left|\int^{1}_{0}\sum^{\infty}_{k=1}\dfrac{1}{k}\sin\left(\dfrac{x}{k}\right)dx\right|\leq\int^{1}_{0}\sum^{\infty}_{k=1}\dfrac{1}{k}\left|\sin\left(\dfrac{x}{k}\right)\right|dx \\&\leq\int^{1}_{0}\sum^{\infty}_{k=1}\dfrac{1}{k^2}dx \\&\leq 2 \end{align}","['real-analysis', 'analysis', 'convergence-divergence', 'uniform-convergence']"
58,Prove $\sqrt{5} \leq 3$,Prove,\sqrt{5} \leq 3,"I was just thinking about how to prove  $\sqrt{5} \leq 3$. I believe I can prove by contradiction by saying suppose  $\sqrt{5} > 3$, then it must be true also that $5 > 9$ by squaring both sides, but this is absurd so it must be that  $\sqrt{5} \leq 3$. However, I was thinking that this doesn't seem valid since we can suppose $\sqrt{5} > -3$ then squaring both sides gives $5 > 9$, which is also absurd! By we know  $\sqrt{5} > -3$, so is something I'm doing not valid? I know this is a simple thing, but I was just trying to prove an irrational number is less than a certain rational number and am stumped.","I was just thinking about how to prove  $\sqrt{5} \leq 3$. I believe I can prove by contradiction by saying suppose  $\sqrt{5} > 3$, then it must be true also that $5 > 9$ by squaring both sides, but this is absurd so it must be that  $\sqrt{5} \leq 3$. However, I was thinking that this doesn't seem valid since we can suppose $\sqrt{5} > -3$ then squaring both sides gives $5 > 9$, which is also absurd! By we know  $\sqrt{5} > -3$, so is something I'm doing not valid? I know this is a simple thing, but I was just trying to prove an irrational number is less than a certain rational number and am stumped.",,"['real-analysis', 'inequality']"
59,Ambiguity in definition of compactness,Ambiguity in definition of compactness,,"I am struggling with the definition of compactness in a topological sense. Below is the definition presented in my lecture notes: A topological space $X$ is compact if every open cover has a finite   subcover on $X$. Okay, this seems to make sense. But, an example is later presented in the notes: $X$ with the trivial topology $\tau = \{X, \varnothing \}$ is compact. Again, okay, this seems to make sense, as any open cover is finite, if we look at $\tau$. My question is: Does the cover come from the set $X$ or $\tau$? Apologies if my thought process seems unclear!","I am struggling with the definition of compactness in a topological sense. Below is the definition presented in my lecture notes: A topological space $X$ is compact if every open cover has a finite   subcover on $X$. Okay, this seems to make sense. But, an example is later presented in the notes: $X$ with the trivial topology $\tau = \{X, \varnothing \}$ is compact. Again, okay, this seems to make sense, as any open cover is finite, if we look at $\tau$. My question is: Does the cover come from the set $X$ or $\tau$? Apologies if my thought process seems unclear!",,"['real-analysis', 'general-topology', 'definition', 'compactness']"
60,"If each term in a sum converges, does the infinite sum converge too?","If each term in a sum converges, does the infinite sum converge too?",,Let $S(x) = \sum_{n=1}^\infty s_n(x)$ where the real valued terms satisfy $s_n(x) \to s_n$ as $x \to \infty$ for each $n$. Suppose that $S=\sum_{n=1}^\infty s_n< \infty$. Does it follow that $S(x) \to S$ as $x \to \infty$? I really do not recall any theorems... please point out !!!,Let $S(x) = \sum_{n=1}^\infty s_n(x)$ where the real valued terms satisfy $s_n(x) \to s_n$ as $x \to \infty$ for each $n$. Suppose that $S=\sum_{n=1}^\infty s_n< \infty$. Does it follow that $S(x) \to S$ as $x \to \infty$? I really do not recall any theorems... please point out !!!,,"['real-analysis', 'sequences-and-series', 'summation']"
61,Suppose that $f ' (x)$ exists and $f(x)$ has two roots $x_1$ and $x_2$. Try to prove that: [closed],Suppose that  exists and  has two roots  and . Try to prove that: [closed],f ' (x) f(x) x_1 x_2,"Closed. This question is off-topic . It is not currently accepting answers. This question is missing context or other details : Please improve the question by providing additional context, which ideally includes your thoughts on the problem and any attempts you have made to solve it. This information helps others identify where you have difficulties and helps them write answers appropriate to your experience level. Closed 5 years ago . Improve this question Suppose that  $f'(x)$ exists and $f(x)$ has two roots $x_1$ and $x_2$. Try to prove that: $\exists \xi \in (x_1,x_2)$ such that $f(\xi)+f'(\xi)=0$. We cannot use the knowedge of integration.","Closed. This question is off-topic . It is not currently accepting answers. This question is missing context or other details : Please improve the question by providing additional context, which ideally includes your thoughts on the problem and any attempts you have made to solve it. This information helps others identify where you have difficulties and helps them write answers appropriate to your experience level. Closed 5 years ago . Improve this question Suppose that  $f'(x)$ exists and $f(x)$ has two roots $x_1$ and $x_2$. Try to prove that: $\exists \xi \in (x_1,x_2)$ such that $f(\xi)+f'(\xi)=0$. We cannot use the knowedge of integration.",,"['calculus', 'real-analysis', 'functions', 'derivatives']"
62,Greatest integer less than or equal to $\sum_{n=1}^{9999}\frac{1}{n^{1/4}}$,Greatest integer less than or equal to,\sum_{n=1}^{9999}\frac{1}{n^{1/4}},This is a PhD entrance question of TIFR 2020. The question requires the explicit answer. I know that the partial sums are evaluated using Abel's formula in Number Theory but I believe there may be better methods for series of this form. Can anyone share their thoughts?,This is a PhD entrance question of TIFR 2020. The question requires the explicit answer. I know that the partial sums are evaluated using Abel's formula in Number Theory but I believe there may be better methods for series of this form. Can anyone share their thoughts?,,"['real-analysis', 'calculus', 'number-theory', 'contest-math']"
63,Convergent sequence of irrational numbers that has a rational limit.,Convergent sequence of irrational numbers that has a rational limit.,,Is it possible to have a convergent sequence whose terms are all irrational but whose limit is rational?,Is it possible to have a convergent sequence whose terms are all irrational but whose limit is rational?,,"['real-analysis', 'sequences-and-series', 'examples-counterexamples', 'irrational-numbers']"
64,Understanding infinity,Understanding infinity,,"I want to understand in a greater depth the concept of infinity. Can someone give me any reference/ text from where I can study and understand about the concept of infinity in mathematics? I would be reading the text on my own. At the same time a little bit of mathematical rigor in the text would be appreciated. P.S.- This question was put on hold as being too broad. So I will try and explain myself better.  I wish to understand and learn about the different types of infinties. The cardinality of rational, irrational, real, algebraic numbers etc. The meaning of completeness of R and stuff like that","I want to understand in a greater depth the concept of infinity. Can someone give me any reference/ text from where I can study and understand about the concept of infinity in mathematics? I would be reading the text on my own. At the same time a little bit of mathematical rigor in the text would be appreciated. P.S.- This question was put on hold as being too broad. So I will try and explain myself better.  I wish to understand and learn about the different types of infinties. The cardinality of rational, irrational, real, algebraic numbers etc. The meaning of completeness of R and stuff like that",,"['real-analysis', 'reference-request', 'infinity']"
65,Evaluate the series: $ \sum_{k=1}^{\infty}\frac{1}{k(k+1)^2k!}$,Evaluate the series:, \sum_{k=1}^{\infty}\frac{1}{k(k+1)^2k!},Evaluate the series: $$ \sum_{k=1}^{\infty}\frac{1}{k(k+1)^2k!}$$,Evaluate the series: $$ \sum_{k=1}^{\infty}\frac{1}{k(k+1)^2k!}$$,,"['calculus', 'real-analysis', 'sequences-and-series']"
66,Real VS Complex for integrals: $\int_0^\infty \frac{dx}{1 + x^3}$,Real VS Complex for integrals:,\int_0^\infty \frac{dx}{1 + x^3},The integral $$\int_0^\infty \frac{dx}{1 + x^4} = \frac{\pi}{2\sqrt2}$$ can be evaluated both by a complex method (residues) and by a real method (partial fraction decomposition). The complex method works also for the integral  $$\int_0^\infty \frac{dx}{1 + x^3} = \frac{2\pi}{3\sqrt3}$$ but partial fraction decomposition does not give convergent integrals. I would like to know if there is some real method for evaluating this last integral.,The integral $$\int_0^\infty \frac{dx}{1 + x^4} = \frac{\pi}{2\sqrt2}$$ can be evaluated both by a complex method (residues) and by a real method (partial fraction decomposition). The complex method works also for the integral  $$\int_0^\infty \frac{dx}{1 + x^3} = \frac{2\pi}{3\sqrt3}$$ but partial fraction decomposition does not give convergent integrals. I would like to know if there is some real method for evaluating this last integral.,,"['real-analysis', 'complex-analysis', 'integration', 'definite-integrals']"
67,Trying to show $1-\frac12 -\frac {1} {4}+\frac {1} {3}-\frac {1} {6}-\frac {1} {8}+\frac {1} {5}-\cdots =\frac {1} {2}\log 2$,Trying to show,1-\frac12 -\frac {1} {4}+\frac {1} {3}-\frac {1} {6}-\frac {1} {8}+\frac {1} {5}-\cdots =\frac {1} {2}\log 2,Now I think the lhs can be rewritten as $$\sum _{n=1}^\infty \left( \dfrac {1} {2n-1}-\dfrac {1} {2\cdot 3^{n-1}}-\dfrac {1} {2^{n+1}}\right) =\dfrac {1} {2}\log 2$$ I guess one way to do this may be to start from RHS and try to bring up an expression which is same as the LHS. so using log series expansion $$\frac {1} {2}\log 2 = \log \sqrt {2} = \sum _{n=1}^\infty \dfrac {\left( -1\right) ^{n+1}} {n}\left( \sqrt {2}-1\right) ^{n}$$ Now Binomial expansion Theorem seems to be making a calling here. $$\sum _{n=1}^{\infty }\dfrac {\left( -1\right) ^{n+1}} {n}\left( \sqrt {2}-1\right) ^{n} = \sum _{n=1}^{\infty }\dfrac {\left( -1\right) ^{n+1}} {n}\sum _{k=0}^{k=n}C_{k}^{n} \sqrt {2}^{k}\left( -1\right) ^{n-k} $$ I am unsure how to proceed from here and even if i am going down the right path. Any help would be much appreciated.,Now I think the lhs can be rewritten as $$\sum _{n=1}^\infty \left( \dfrac {1} {2n-1}-\dfrac {1} {2\cdot 3^{n-1}}-\dfrac {1} {2^{n+1}}\right) =\dfrac {1} {2}\log 2$$ I guess one way to do this may be to start from RHS and try to bring up an expression which is same as the LHS. so using log series expansion $$\frac {1} {2}\log 2 = \log \sqrt {2} = \sum _{n=1}^\infty \dfrac {\left( -1\right) ^{n+1}} {n}\left( \sqrt {2}-1\right) ^{n}$$ Now Binomial expansion Theorem seems to be making a calling here. $$\sum _{n=1}^{\infty }\dfrac {\left( -1\right) ^{n+1}} {n}\left( \sqrt {2}-1\right) ^{n} = \sum _{n=1}^{\infty }\dfrac {\left( -1\right) ^{n+1}} {n}\sum _{k=0}^{k=n}C_{k}^{n} \sqrt {2}^{k}\left( -1\right) ^{n-k} $$ I am unsure how to proceed from here and even if i am going down the right path. Any help would be much appreciated.,,"['real-analysis', 'combinatorics', 'sequences-and-series']"
68,If the derivative of a function is square of it then it is constant [duplicate],If the derivative of a function is square of it then it is constant [duplicate],,"This question already has answers here : $f'(x) = [f(x)]^{2}$. Prove $f(x) = 0 $ (5 answers) Show that $f'(x) = f^2(x)$ and $f(0) = 0$ implies $f$ is the zero function [duplicate] (4 answers) Closed 4 years ago . Let $f:\mathbb{R}\to\mathbb{R}$ is differentiable and $f(0)=0$ . Also $\forall x\in \mathbb{R}$ we have $f'(x)=f^2(x)$ . Prove that $f(x)=0$ , for every $x$ . I tried to use MVT for both derivative and integral. But I got nowhere. I just found out that $f$ is increasing. for positive values $f$ is non negative. $\forall x>0$ , there exists some $c\in (0,x)$ s.t. $f(x)=xf^2(c).$ Intuitively, it seems one can start by a small interval around zero and show that $f=0$ and so on. Any comment!","This question already has answers here : $f'(x) = [f(x)]^{2}$. Prove $f(x) = 0 $ (5 answers) Show that $f'(x) = f^2(x)$ and $f(0) = 0$ implies $f$ is the zero function [duplicate] (4 answers) Closed 4 years ago . Let is differentiable and . Also we have . Prove that , for every . I tried to use MVT for both derivative and integral. But I got nowhere. I just found out that is increasing. for positive values is non negative. , there exists some s.t. Intuitively, it seems one can start by a small interval around zero and show that and so on. Any comment!","f:\mathbb{R}\to\mathbb{R} f(0)=0 \forall x\in \mathbb{R} f'(x)=f^2(x) f(x)=0 x f f \forall x>0 c\in (0,x) f(x)=xf^2(c). f=0","['real-analysis', 'calculus', 'analysis', 'derivatives']"
69,What does $\sum_{n=1}^\infty\frac{1}{\sqrt n(n+1)}$ converge to exactly?,What does  converge to exactly?,\sum_{n=1}^\infty\frac{1}{\sqrt n(n+1)},"If $$\sum_{n=1}^\infty\frac{1}{\sqrt n(n+1)}=S,$$ does $S$ have a known closed-form symbolic expression for its value? By the integral test it clearly converges, i.e. take $u=\sqrt x$ and $u$ -substitute to get $$\int_1^\infty\frac1{\sqrt x(x+1)}dx=2\arctan(\sqrt x)\rvert_1^\infty=\frac\pi2.$$ The sum was also in a competition problem, or so I'm told, to show that $S<2$ (if a source for this problem can be found, I'd appreciate it).","If does have a known closed-form symbolic expression for its value? By the integral test it clearly converges, i.e. take and -substitute to get The sum was also in a competition problem, or so I'm told, to show that (if a source for this problem can be found, I'd appreciate it).","\sum_{n=1}^\infty\frac{1}{\sqrt n(n+1)}=S, S u=\sqrt x u \int_1^\infty\frac1{\sqrt x(x+1)}dx=2\arctan(\sqrt x)\rvert_1^\infty=\frac\pi2. S<2","['calculus', 'real-analysis', 'sequences-and-series']"
70,Is it true that $b^n-a^n < (b-a)nb^{n-1}$ when $0 < a< b$?,Is it true that  when ?,b^n-a^n < (b-a)nb^{n-1} 0 < a< b,"A Real Analysis textbook says the identity $$b^n-a^n = (b-a)(b^{n-1}+\cdots+a^{n-1})$$ yields the inequality $$b^n-a^n < (b-a)nb^{n-1} \text{ when } 0 < a< b.$$ (Note that $n$ is a positive integer) No matter how I look at it, the inequality seems to be wrong. Take for instance, the inequality does not hold for $n=1$ when one tries mathematical induction. It does not hold for other values of $n$ too. I guess there is something I am missing here and I will appreciate help.","A Real Analysis textbook says the identity $$b^n-a^n = (b-a)(b^{n-1}+\cdots+a^{n-1})$$ yields the inequality $$b^n-a^n < (b-a)nb^{n-1} \text{ when } 0 < a< b.$$ (Note that $n$ is a positive integer) No matter how I look at it, the inequality seems to be wrong. Take for instance, the inequality does not hold for $n=1$ when one tries mathematical induction. It does not hold for other values of $n$ too. I guess there is something I am missing here and I will appreciate help.",,['real-analysis']
71,Closed property of nonempty finite set,Closed property of nonempty finite set,,I came across this text in Rudin's book where it has been mentioned that a non-empty finite set is closed. But a closed set is a set which contains all of it's limit points in the set itself but none of the elements of a non-empty finite set can possibly have a limit point because a neighborhood of a limit point has infinite points . So how come a non-empty finite set be a closed set ??,I came across this text in Rudin's book where it has been mentioned that a non-empty finite set is closed. But a closed set is a set which contains all of it's limit points in the set itself but none of the elements of a non-empty finite set can possibly have a limit point because a neighborhood of a limit point has infinite points . So how come a non-empty finite set be a closed set ??,,['real-analysis']
72,"What does ""the average continuous function is nowhere monotonic"" mean?","What does ""the average continuous function is nowhere monotonic"" mean?",,"I plan on asking my professor what he meant by ""average continuous function,"" but as it is possible that this is a concept as vague as the statement, I was hoping to get some interesting answers/interpretations from stack exchange first. How do you think of the average of some infinite group of things? Or does this just mean that the real line is so dense/big that it is somehow likely that a function would bounce around everywhere except on some countable number of points? I'm sorry this is vague, I will be sure to post his response if I get a good one. I would also appreciate any resources or reading; googling around hasn't been fruitful.","I plan on asking my professor what he meant by ""average continuous function,"" but as it is possible that this is a concept as vague as the statement, I was hoping to get some interesting answers/interpretations from stack exchange first. How do you think of the average of some infinite group of things? Or does this just mean that the real line is so dense/big that it is somehow likely that a function would bounce around everywhere except on some countable number of points? I'm sorry this is vague, I will be sure to post his response if I get a good one. I would also appreciate any resources or reading; googling around hasn't been fruitful.",,"['real-analysis', 'probability-theory', 'measure-theory', 'reference-request', 'soft-question']"
73,Need a hint: show that a subset $E \subset \mathbb{R}$ with no limit points is at most countable.,Need a hint: show that a subset  with no limit points is at most countable.,E \subset \mathbb{R},"I'm stuck on the following real-analysis problem and could use a hint: Consider $\mathbb{R}$ with the standard metric. Let $E \subset \mathbb{R}$ be a subset which has no limit points. Show that $E$ is at most countable. I'm primarily confused about how to go about showing that this set $E$ is at most countable (i.e. finite or countable). What I can show: since $E$ has no limit points, I can show that for every $x \in E$, there is a neighborhood $N_{r_x}(x)$ where $r_x > 0$ that does not contain any other point $y \in E$ where $y \neq x$. This suffices to show that every point within x is an isolated point.","I'm stuck on the following real-analysis problem and could use a hint: Consider $\mathbb{R}$ with the standard metric. Let $E \subset \mathbb{R}$ be a subset which has no limit points. Show that $E$ is at most countable. I'm primarily confused about how to go about showing that this set $E$ is at most countable (i.e. finite or countable). What I can show: since $E$ has no limit points, I can show that for every $x \in E$, there is a neighborhood $N_{r_x}(x)$ where $r_x > 0$ that does not contain any other point $y \in E$ where $y \neq x$. This suffices to show that every point within x is an isolated point.",,['real-analysis']
74,"Proving that the function $f(x,y)=\frac{x^2y}{x^2 + y^2}$ with $f(0,0)=0$ is continuous at $(0,0)$.",Proving that the function  with  is continuous at .,"f(x,y)=\frac{x^2y}{x^2 + y^2} f(0,0)=0 (0,0)","How would you prove or disprove that the function given by $$f(x,y) = \begin{cases} \dfrac{x^2y}{x^2 + y^2} & (x,y) \neq (0,0) \\ 0 & (x,y) = (0,0) \end{cases}$$ is continuous at $(0,0$)?","How would you prove or disprove that the function given by $$f(x,y) = \begin{cases} \dfrac{x^2y}{x^2 + y^2} & (x,y) \neq (0,0) \\ 0 & (x,y) = (0,0) \end{cases}$$ is continuous at $(0,0$)?",,['real-analysis']
75,Is the natural log of n rational?,Is the natural log of n rational?,,"It's famously unknown whether the natural log of 2 is rational or not. How about the natural log of other numbers. Is it known/unknown whether these are rational? Obviously ln(1) is 0, and ln(2^n) is n*ln(2) (and is thus rational iff ln(2) is rational), but how about other cases?","It's famously unknown whether the natural log of 2 is rational or not. How about the natural log of other numbers. Is it known/unknown whether these are rational? Obviously ln(1) is 0, and ln(2^n) is n*ln(2) (and is thus rational iff ln(2) is rational), but how about other cases?",,['real-analysis']
76,Integrate $\int\limits_0^\infty\frac{x^{2n-2}}{(x^4-x^2+1)^n}dx$,Integrate,\int\limits_0^\infty\frac{x^{2n-2}}{(x^4-x^2+1)^n}dx,"Complete question: If $I_n$ = $\int\limits_0^\infty\frac{x^{2n-2}}{(x^4-x^2+1)^n}dx$ and $I_5$ = $\frac{p\pi}{q}$ where p and q are coprime natural numbers, then the value of 8p-q is what? I have no clue how to go about this question. But I am thinking maybe $I_1$ , $I_2$ , $I_3$ ...  are a part of some sequence and evauating a simpler integral will lead us to $I_5$ without actually solving for it. I got this idea since solving $I_1$ and $I_2$ is quite easy. My level- High School.","Complete question: If = and = where p and q are coprime natural numbers, then the value of 8p-q is what? I have no clue how to go about this question. But I am thinking maybe , , ...  are a part of some sequence and evauating a simpler integral will lead us to without actually solving for it. I got this idea since solving and is quite easy. My level- High School.",I_n \int\limits_0^\infty\frac{x^{2n-2}}{(x^4-x^2+1)^n}dx I_5 \frac{p\pi}{q} I_1 I_2 I_3 I_5 I_1 I_2,"['real-analysis', 'calculus', 'integration', 'definite-integrals']"
77,Find all functions $f:\mathbb{R}\rightarrow\mathbb{R}$ where $f(xf(y)+f(x)+y)=xy+f(x)+f(y)$,Find all functions  where,f:\mathbb{R}\rightarrow\mathbb{R} f(xf(y)+f(x)+y)=xy+f(x)+f(y),"Find all functions $f:\mathbb{R}\rightarrow\mathbb{R}$ for two real numbers $x$ and $y$ where $f(xf(y)+f(x)+y)=xy+f(x)+f(y)$ For $x=0$ and $y=-f(0)$ then $f(-f(0))=0$ . So, there is a real root $r_0$ for function $f$ . For $x=r_0$ and $y=r_0$ we have $r_0^2=0$ , so $f(0)=0$ and zero root is unique. Please help me to complete the proof.","Find all functions for two real numbers and where For and then . So, there is a real root for function . For and we have , so and zero root is unique. Please help me to complete the proof.",f:\mathbb{R}\rightarrow\mathbb{R} x y f(xf(y)+f(x)+y)=xy+f(x)+f(y) x=0 y=-f(0) f(-f(0))=0 r_0 f x=r_0 y=r_0 r_0^2=0 f(0)=0,"['real-analysis', 'functions', 'real-numbers', 'functional-equations']"
78,Evaluate $\int \frac{\left(x^2+x+3\right)\left(x^3+7\right)}{x+1}dx$.,Evaluate .,\int \frac{\left(x^2+x+3\right)\left(x^3+7\right)}{x+1}dx,"Evaluate: $$\int \frac{\left(x^2+x+3\right)\left(x^3+7\right)}{x+1}dx$$ The only thing I can think of doing here is long division to simplify the integral down and see if I can work with some easier sections. Here's my attempt: \begin{align} \int \frac{\left(x^2+x+3\right)\left(x^3+7\right)}{x+1}dx &= \int \left(\:x^4+3x^2+4x+3+\frac{18}{x+1} \right )dx \\ &= \int \:x^4dx+\int \:3x^2dx+\int \:4xdx+\int \:3dx+\int \frac{18}{x+1}dx \\ &= \frac{x^5}{5}+x^3+2x^2+3x+18\ln \left|x+1\right|+ c, c \in \mathbb{R} \end{align} The only issue I had is that the polynomial long division took quite some time. Is there another way to do this that is less time consuming? The reason I ask this is that, this kind of question can come in an exam where time is of the essence so anything that I can do to speed up the process will benefit me greatly.","Evaluate: The only thing I can think of doing here is long division to simplify the integral down and see if I can work with some easier sections. Here's my attempt: The only issue I had is that the polynomial long division took quite some time. Is there another way to do this that is less time consuming? The reason I ask this is that, this kind of question can come in an exam where time is of the essence so anything that I can do to speed up the process will benefit me greatly.","\int \frac{\left(x^2+x+3\right)\left(x^3+7\right)}{x+1}dx \begin{align}
\int \frac{\left(x^2+x+3\right)\left(x^3+7\right)}{x+1}dx &= \int \left(\:x^4+3x^2+4x+3+\frac{18}{x+1} \right )dx \\
&= \int \:x^4dx+\int \:3x^2dx+\int \:4xdx+\int \:3dx+\int \frac{18}{x+1}dx \\
&= \frac{x^5}{5}+x^3+2x^2+3x+18\ln \left|x+1\right|+ c, c \in \mathbb{R}
\end{align}","['real-analysis', 'calculus', 'integration', 'polynomials']"
79,Prove or Disprove : There exists a continuous bijection from $\mathbb{ R}^2$ to $\mathbb{R} $,Prove or Disprove : There exists a continuous bijection from  to,\mathbb{ R}^2 \mathbb{R} ,"This question was asked to me by a mathematics undergraduate to me and I was not able to solve it. So, I am asking it here. Prove or Disprove : There exists a continuous bijection from $\mathbb{ R}^2$ to $\mathbb{R} $ . I have no idea on how this problem can be tackled. It seems it has something to do with set theory but I only know elementary set theory( bijection from naturals) and  I am unable to solve it.","This question was asked to me by a mathematics undergraduate to me and I was not able to solve it. So, I am asking it here. Prove or Disprove : There exists a continuous bijection from to . I have no idea on how this problem can be tackled. It seems it has something to do with set theory but I only know elementary set theory( bijection from naturals) and  I am unable to solve it.",\mathbb{ R}^2 \mathbb{R} ,"['real-analysis', 'general-topology']"
80,Finding the limit of the integral,Finding the limit of the integral,,"Let $f$ be continuous in [0, 1]. Assume $$0 < a < b$$ Prove that the following limit exists and determine what it is: $$\lim_{\delta\rightarrow 0}\int_{\delta a}^{\delta b} \frac{f(x)}{x} dx$$ I got stuck on this question and I can't seem to figure it out. It seems to me that there's no reason for this limit to always exist. For instance, if I take $$f(x) = 1$$ The integral of the harmonic function diverges near 0, so by Cauchy's critrion, the limit shouldn't exist. So what am I missing? Does anyone have an idea?","Let $f$ be continuous in [0, 1]. Assume $$0 < a < b$$ Prove that the following limit exists and determine what it is: $$\lim_{\delta\rightarrow 0}\int_{\delta a}^{\delta b} \frac{f(x)}{x} dx$$ I got stuck on this question and I can't seem to figure it out. It seems to me that there's no reason for this limit to always exist. For instance, if I take $$f(x) = 1$$ The integral of the harmonic function diverges near 0, so by Cauchy's critrion, the limit shouldn't exist. So what am I missing? Does anyone have an idea?",,"['real-analysis', 'calculus', 'integration', 'limits', 'riemann-integration']"
81,Show that $e^{1-n} \leq \frac {n!}{n^n}$,Show that,e^{1-n} \leq \frac {n!}{n^n},How can I show that for a $n \in \mathbb N$ $$e^{1-n} \leq \frac {n!}{n^n}$$ I tried using the binomial theorem like this $$n^n \le (1+n)^n = \sum_{k=0}^n \binom nk n^k \le \sum_{k=0}^\infty \binom nk n^k = \sum_{k=0}^\infty \frac{n!}{k!(n-k)!} n^k \le \sum_{k=0}^\infty \frac{n!}{k!} n^k = n! \sum_{k=0}^\infty \frac{n^k}{k!} = n! \cdot e^n$$ which would give me $$\frac{1}{e^n} \le \frac{n!}{n^n}$$ But I'm missing the factor of $e$ on the left side. Can you give me a hint?,How can I show that for a $n \in \mathbb N$ $$e^{1-n} \leq \frac {n!}{n^n}$$ I tried using the binomial theorem like this $$n^n \le (1+n)^n = \sum_{k=0}^n \binom nk n^k \le \sum_{k=0}^\infty \binom nk n^k = \sum_{k=0}^\infty \frac{n!}{k!(n-k)!} n^k \le \sum_{k=0}^\infty \frac{n!}{k!} n^k = n! \sum_{k=0}^\infty \frac{n^k}{k!} = n! \cdot e^n$$ which would give me $$\frac{1}{e^n} \le \frac{n!}{n^n}$$ But I'm missing the factor of $e$ on the left side. Can you give me a hint?,,"['real-analysis', 'inequality', 'binomial-theorem']"
82,Convergence of $\sum_{n=1}^{\infty}{\sqrt[n]{n}-1 \over n}$,Convergence of,\sum_{n=1}^{\infty}{\sqrt[n]{n}-1 \over n},"I am trying to conclude about the convergence of $$\sum_{n=1}^{\infty}{\sqrt[n]{n}-1 \over n}$$ If I take ${\sqrt[n]{{\sqrt[n]{n}-1 \over n}}}= {\sqrt[n]{\sqrt[n]{n}-1} \over \sqrt[n]{n}}$ this looks like it converges to $0$ but I am suspicious about the numerator. At the same time, other techniques I know like the ratio test, $n(\sqrt[n]{n}-1)$ and comparisons don't yield meaningful results. Any hints?","I am trying to conclude about the convergence of $$\sum_{n=1}^{\infty}{\sqrt[n]{n}-1 \over n}$$ If I take ${\sqrt[n]{{\sqrt[n]{n}-1 \over n}}}= {\sqrt[n]{\sqrt[n]{n}-1} \over \sqrt[n]{n}}$ this looks like it converges to $0$ but I am suspicious about the numerator. At the same time, other techniques I know like the ratio test, $n(\sqrt[n]{n}-1)$ and comparisons don't yield meaningful results. Any hints?",,"['real-analysis', 'sequences-and-series', 'radicals']"
83,Solving a separable differential equation,Solving a separable differential equation,,"Solve the differential equation: $$y'=\frac{1-y^2}{1-x^2}$$ My book says the solution is: $$y=\frac{x+c}{cx+1},$$ where $c$ is a constant. It's been ten minutes I tried to verify if it was correct but I'm pretty sure the book is wrong. Can someone confirm it?","Solve the differential equation: $$y'=\frac{1-y^2}{1-x^2}$$ My book says the solution is: $$y=\frac{x+c}{cx+1},$$ where $c$ is a constant. It's been ten minutes I tried to verify if it was correct but I'm pretty sure the book is wrong. Can someone confirm it?",,"['calculus', 'real-analysis', 'integration', 'ordinary-differential-equations', 'indefinite-integrals']"
84,Prove or disprove that $\int_a^b |f(x)|\ \mathrm{d}x\geq \big | \int_a^b f(x)\ \mathrm{d}x\big| $ [closed],Prove or disprove that  [closed],\int_a^b |f(x)|\ \mathrm{d}x\geq \big | \int_a^b f(x)\ \mathrm{d}x\big| ,"Closed. This question is off-topic . It is not currently accepting answers. This question is missing context or other details : Please improve the question by providing additional context, which ideally includes your thoughts on the problem and any attempts you have made to solve it. This information helps others identify where you have difficulties and helps them write answers appropriate to your experience level. Closed 7 years ago . The community reviewed whether to reopen this question 2 years ago and left it closed: Original close reason(s) were not resolved Improve this question Let $f$ be a continuous and integrable function over $[a,b]$ . Prove or disprove that $$\int_a^b |f(x)|\ \mathrm{d}x\geq \left | \int_a^b f(x)\ \mathrm{d}x\right| $$","Closed. This question is off-topic . It is not currently accepting answers. This question is missing context or other details : Please improve the question by providing additional context, which ideally includes your thoughts on the problem and any attempts you have made to solve it. This information helps others identify where you have difficulties and helps them write answers appropriate to your experience level. Closed 7 years ago . The community reviewed whether to reopen this question 2 years ago and left it closed: Original close reason(s) were not resolved Improve this question Let be a continuous and integrable function over . Prove or disprove that","f [a,b] \int_a^b |f(x)|\ \mathrm{d}x\geq \left | \int_a^b f(x)\ \mathrm{d}x\right|
","['real-analysis', 'integration', 'inequality', 'definite-integrals']"
85,Prove that the unit open ball in $\mathbb{R}^2$ cannot be expressed as a countable disjoint union of open rectangles.,Prove that the unit open ball in  cannot be expressed as a countable disjoint union of open rectangles.,\mathbb{R}^2,"Prove that the unit open ball in $\mathbb{R}^2$ cannot be expressed as a countable disjoint union of open rectangles. Open rectangles in $\mathbb{R}^2$ are subsets of the form $(a,b)\times(c,d)$. Thanks a lot!","Prove that the unit open ball in $\mathbb{R}^2$ cannot be expressed as a countable disjoint union of open rectangles. Open rectangles in $\mathbb{R}^2$ are subsets of the form $(a,b)\times(c,d)$. Thanks a lot!",,"['real-analysis', 'general-topology']"
86,"Proving that $a + b = b + a$ for all $a,b \in\mathbb{R}$",Proving that  for all,"a + b = b + a a,b \in\mathbb{R}","Being interested in the very foundations of mathematics, I'm trying to build a rigorous proof on my own that $a + b = b + a$ for all $\left[a, b\in\mathbb{R}\right] $. Inspired by interesting properties of the complex plane and some researches, I realized that defining multiplication as repeated addition will lead me nowhere (at least, I could not work with it). So, my ideas: Defining addition $a+b$ as a kind of ""walking"" to right $\left(b>0\right)$ or to the left $\left(b<0\right)$ a space $b$ from $a$. Adding a number $b$ to a number $a$ (denoted by $a+b$) involves doing the following operation: Consider the real line $\lambda$ and its origin at $0$. Mark a point $a$, draw another real line $\omega$ above $\lambda$ such what $\omega \parallel \lambda$ and mark a point $b$ on $\omega$. Now, draw a line $\sigma$ such that $\sigma \perp \omega$ and the only point in commom between $\sigma$ and $\omega$ is $b$. Consider the point that $\lambda$ and $\sigma$ have in commom; this point is nicely denoted as $a + b$. (Note that all my work is based here. Any problems, and my proof goes to trash) This definition can be used to see the properties of adding two numbers $a$ and $b$, for all $a, b \in\mathbb{R}$. Using geometric properties may lead us to a rigorous proof (if not, I would like to know the problems of using it). So, I started: $a, b \in\mathbb{N}$: $a+b = \overbrace{\left(1+1+1+\cdots+1\right)}^a + \overbrace{\left(1+1+1+\cdots+1\right)}^b = \overbrace{1+1+1+1+\cdots+1}^{a+b} = \overbrace{\left(1+1+1+\cdots+1\right)}^b + \overbrace{\left(1+1+1+\cdots+1\right)}^a = b + a$ (Implicity, I'm using the fact that $\left(1+1\right)+1 = 1+\left(1+1\right)$, which I do not know how to prove and interpret it as cutting a segment $c$ in two parts -- $a$ and $b$. However, this result can be extended to $\mathbb{Z}$ in the sense that $-a$ $(a > 0)$ is a change; from right to left). $a, b \in\mathbb{R}$: Here, we have basically two cases: $a$ and $b$ are either positive or negative; $a$ and $b$, where one of them is negative. Since in my definition $-b, b>0$ means drawing a point $b$ to the left of the real line, there's no big deal interpretating it; subtracting can be interpreted now. So, it starts: $a + b = c$. However, $c$  can be cut in two parts: $b$ and $a$. Naturally, if $a>c$, then $b<0$ -- many cases can be listed. So, $c = b + a$. But $c = a + b$; it follows that $a + b = b + a$. My questions: Is there any problem in using my definition of adding two numbers $a$ and $b$, which uses many geometric properties? Is there any way to solve it from informality? Is there anything right here? Thanks in advance.","Being interested in the very foundations of mathematics, I'm trying to build a rigorous proof on my own that $a + b = b + a$ for all $\left[a, b\in\mathbb{R}\right] $. Inspired by interesting properties of the complex plane and some researches, I realized that defining multiplication as repeated addition will lead me nowhere (at least, I could not work with it). So, my ideas: Defining addition $a+b$ as a kind of ""walking"" to right $\left(b>0\right)$ or to the left $\left(b<0\right)$ a space $b$ from $a$. Adding a number $b$ to a number $a$ (denoted by $a+b$) involves doing the following operation: Consider the real line $\lambda$ and its origin at $0$. Mark a point $a$, draw another real line $\omega$ above $\lambda$ such what $\omega \parallel \lambda$ and mark a point $b$ on $\omega$. Now, draw a line $\sigma$ such that $\sigma \perp \omega$ and the only point in commom between $\sigma$ and $\omega$ is $b$. Consider the point that $\lambda$ and $\sigma$ have in commom; this point is nicely denoted as $a + b$. (Note that all my work is based here. Any problems, and my proof goes to trash) This definition can be used to see the properties of adding two numbers $a$ and $b$, for all $a, b \in\mathbb{R}$. Using geometric properties may lead us to a rigorous proof (if not, I would like to know the problems of using it). So, I started: $a, b \in\mathbb{N}$: $a+b = \overbrace{\left(1+1+1+\cdots+1\right)}^a + \overbrace{\left(1+1+1+\cdots+1\right)}^b = \overbrace{1+1+1+1+\cdots+1}^{a+b} = \overbrace{\left(1+1+1+\cdots+1\right)}^b + \overbrace{\left(1+1+1+\cdots+1\right)}^a = b + a$ (Implicity, I'm using the fact that $\left(1+1\right)+1 = 1+\left(1+1\right)$, which I do not know how to prove and interpret it as cutting a segment $c$ in two parts -- $a$ and $b$. However, this result can be extended to $\mathbb{Z}$ in the sense that $-a$ $(a > 0)$ is a change; from right to left). $a, b \in\mathbb{R}$: Here, we have basically two cases: $a$ and $b$ are either positive or negative; $a$ and $b$, where one of them is negative. Since in my definition $-b, b>0$ means drawing a point $b$ to the left of the real line, there's no big deal interpretating it; subtracting can be interpreted now. So, it starts: $a + b = c$. However, $c$  can be cut in two parts: $b$ and $a$. Naturally, if $a>c$, then $b<0$ -- many cases can be listed. So, $c = b + a$. But $c = a + b$; it follows that $a + b = b + a$. My questions: Is there any problem in using my definition of adding two numbers $a$ and $b$, which uses many geometric properties? Is there any way to solve it from informality? Is there anything right here? Thanks in advance.",,"['real-analysis', 'definition']"
87,Proofs in Real Analysis are too 'convenient',Proofs in Real Analysis are too 'convenient',,"I'm doing a first course in real analysis and I have studied nearly 10-15 theorems and proofs by now. One thing I've noticed in all of them is that they all seem too 'convenient' and full of assumptions. This, I find very peculiar to real analysis. To understand my point, consider this one for instance: Theorem : Let $\{x_n\}$ be a sequence of $\mathbb R^+$ such that $\lim_{n\to\infty} |\frac{x_{n+1}}{x_n}| = l$ . If $0\le l \lt 1$ , $\lim_{n\to\infty } x_n = 0$ . Proof : Consider $\epsilon \gt 0$ such that $l+\epsilon \lt 1$ . There exists $N \in \mathbb N$ such that $||\frac{x_{n+1}}{x_n}| - l |\lt \epsilon$ for all $n \ge N$ . $l-\epsilon \lt |\frac{x_{n+1}}{x_n}| \lt l+\epsilon$ (for all $n \ge N$ .) Let $m = l + \epsilon$ . Given that $0 \le l \lt 1$ , we could say that $0 \lt m \lt 1$ . This gives $|\frac{x_{n+1}}{x_n}| \lt m$ for all $n \ge N$ $x_{N+1} \lt mx_N$ $x_{N+2} \lt mx_{N+1}\lt m^2x_N$ So for all $n \ge N+1$ , $x_n \lt mx_{n-1} \lt m^{n-N}x_N$ . We are therefore left with $0 \lt x_n \lt Am^n$ where $A = \frac{x_N}{m^n}$ . As $\lim_{n\to \infty} Am^n = 0$ as $m\lt 1$ ,using the Squeeze Theorem, we are able to prove the theorem. You see, the whole thing is dependent on one assumption that $l+\epsilon \lt 1$ . But this should ideally hold true for any $\epsilon$ . I wouldn't call this proof 'complete'! Here's another such proof of the quotient law for limits: Let $\epsilon, k \gt 0.$ Then $\frac{\epsilon}{k}$ is also an arbitrary positive number. If $\{x_n\}$ and $\{y_n\}$ are two sequences, we need to prove that the limit of the quotient of the terms equals the quotient of the limits of the terms( say $l$ and $m$ ). For a certain $N$ , $|\frac{x_n}{y_n} - \frac{l}{m}| = |\frac{m(x_n-l) + l(m-y_n)}{my_n}| \le |\frac{|m||x_n-l| + |l||m-y_n|}{|m||y_n|}| \lt \frac{\epsilon}{ky_n} + \frac{\epsilon}{ky_n}\frac{|l|}{|m|} = \frac{\epsilon}{k}\frac{|m|+|l|}{|m||y_n|} $ $ lim_{n \to \infty} y_n = m$ so $lim_{n \to \infty} |y_n| = |m| $ . Let $ 0 <H<|m|$ . Then $ |y_n| > H $ for all $n \ge N_0, N_0 \in \mathbb N$ Choose $N' = max\{N_0, N\}$ so that for all $n \ge N',|\frac{x_n}{y_n} - \frac{l}{m}| < \frac{\epsilon}{k}\frac{|l|+|m|}{|m|H}$ Now choose k such that $ \frac{|l|+|m|}{|m|H} < 1$ so that $|\frac{x_n}{y_n} - \frac{l}{m}| < \epsilon$ . Q.E.D. The last part again contains too convenient choices of constants. I think this might mean that unless you are choosing them in such a manner, the theorem won't hold. It's as though we are creating the proof such that the theorem comes true, which I find strange. Hopefully I've made myself clear. I wonder if there exist 'more convincing' and more elegant proofs which do not take into account so many arbitrary constants. Thank you! Edit As suggested in one of the comments, I am inserting a theorem whose proof seems elegant to me-the Squeeze Theorem. Theorem :Given that $\{x_n\}$ , $\{y_n\}$ and $\{z_n\}$ are three sequences where $x_n \le y_n \le z_n $ for all $n \ge N,$ where $N \in \mathbb N$ , and $\lim_{n\to\infty } x_n = \lim_{n\to\infty } z_n = l,$ then $lim_{n\to\infty } y_n = l$ Proof : For a given $\epsilon \gt 0$ , we have natural numbers $N_1$ and $N_2$ such that $|x_n-l| < \epsilon$ for all $n \ge N_1$ and $|z_n-l| < \epsilon$ for all $n \ge N_2$ . Let $N_3 = max\{N_1, N_2\}$ , then for all $n \ge N_3$ , $|x_n-l| < \epsilon$ and $|z_n-l| < \epsilon$ . This means $l-\epsilon < x_n<l+\epsilon$ and $l-\epsilon < z_n<l+\epsilon$ for all $n \ge N_3$ . Let $N_4 = max\{N, N_3\}$ . Then it holds that $l-\epsilon < x_n < y_n < z_n <l+\epsilon$ and therefore $l-\epsilon < y_n<l+\epsilon$ or $|y_n-l| < \epsilon$ . Q.E.D We certainly have considered multiple constants here, but we are not arbitrarily assigning them values/choosing them to satisfy certain equations, like so: ' $l+\epsilon<1$ ' or 'choose k such that $ \frac{|l|+|m|}{|m|H} < 1$ '.","I'm doing a first course in real analysis and I have studied nearly 10-15 theorems and proofs by now. One thing I've noticed in all of them is that they all seem too 'convenient' and full of assumptions. This, I find very peculiar to real analysis. To understand my point, consider this one for instance: Theorem : Let be a sequence of such that . If , . Proof : Consider such that . There exists such that for all . (for all .) Let . Given that , we could say that . This gives for all So for all , . We are therefore left with where . As as ,using the Squeeze Theorem, we are able to prove the theorem. You see, the whole thing is dependent on one assumption that . But this should ideally hold true for any . I wouldn't call this proof 'complete'! Here's another such proof of the quotient law for limits: Let Then is also an arbitrary positive number. If and are two sequences, we need to prove that the limit of the quotient of the terms equals the quotient of the limits of the terms( say and ). For a certain , so . Let . Then for all Choose so that for all Now choose k such that so that . Q.E.D. The last part again contains too convenient choices of constants. I think this might mean that unless you are choosing them in such a manner, the theorem won't hold. It's as though we are creating the proof such that the theorem comes true, which I find strange. Hopefully I've made myself clear. I wonder if there exist 'more convincing' and more elegant proofs which do not take into account so many arbitrary constants. Thank you! Edit As suggested in one of the comments, I am inserting a theorem whose proof seems elegant to me-the Squeeze Theorem. Theorem :Given that , and are three sequences where for all where , and then Proof : For a given , we have natural numbers and such that for all and for all . Let , then for all , and . This means and for all . Let . Then it holds that and therefore or . Q.E.D We certainly have considered multiple constants here, but we are not arbitrarily assigning them values/choosing them to satisfy certain equations, like so: ' ' or 'choose k such that '.","\{x_n\} \mathbb R^+ \lim_{n\to\infty} |\frac{x_{n+1}}{x_n}| = l 0\le l \lt 1 \lim_{n\to\infty } x_n = 0 \epsilon \gt 0 l+\epsilon \lt 1 N \in \mathbb N ||\frac{x_{n+1}}{x_n}| - l |\lt \epsilon n \ge N l-\epsilon \lt |\frac{x_{n+1}}{x_n}| \lt l+\epsilon n \ge N m = l + \epsilon 0 \le l \lt 1 0 \lt m \lt 1 |\frac{x_{n+1}}{x_n}| \lt m n \ge N x_{N+1} \lt mx_N x_{N+2} \lt mx_{N+1}\lt m^2x_N n \ge N+1 x_n \lt mx_{n-1} \lt m^{n-N}x_N 0 \lt x_n \lt Am^n A = \frac{x_N}{m^n} \lim_{n\to \infty} Am^n = 0 m\lt 1 l+\epsilon \lt 1 \epsilon \epsilon, k \gt 0. \frac{\epsilon}{k} \{x_n\} \{y_n\} l m N |\frac{x_n}{y_n} - \frac{l}{m}| = |\frac{m(x_n-l) + l(m-y_n)}{my_n}| \le |\frac{|m||x_n-l| + |l||m-y_n|}{|m||y_n|}| \lt \frac{\epsilon}{ky_n} + \frac{\epsilon}{ky_n}\frac{|l|}{|m|} = \frac{\epsilon}{k}\frac{|m|+|l|}{|m||y_n|}  
lim_{n \to \infty} y_n = m lim_{n \to \infty} |y_n| = |m|   0 <H<|m|  |y_n| > H  n \ge N_0, N_0 \in \mathbb N N' = max\{N_0, N\} n \ge N',|\frac{x_n}{y_n} - \frac{l}{m}| < \frac{\epsilon}{k}\frac{|l|+|m|}{|m|H}  \frac{|l|+|m|}{|m|H} < 1 |\frac{x_n}{y_n} - \frac{l}{m}| < \epsilon \{x_n\} \{y_n\} \{z_n\} x_n \le y_n \le z_n  n \ge N, N \in \mathbb N \lim_{n\to\infty } x_n = \lim_{n\to\infty } z_n = l, lim_{n\to\infty } y_n = l \epsilon \gt 0 N_1 N_2 |x_n-l| < \epsilon n \ge N_1 |z_n-l| < \epsilon n \ge N_2 N_3 = max\{N_1, N_2\} n \ge N_3 |x_n-l| < \epsilon |z_n-l| < \epsilon l-\epsilon < x_n<l+\epsilon l-\epsilon < z_n<l+\epsilon n \ge N_3 N_4 = max\{N, N_3\} l-\epsilon < x_n < y_n < z_n <l+\epsilon l-\epsilon < y_n<l+\epsilon |y_n-l| < \epsilon l+\epsilon<1  \frac{|l|+|m|}{|m|H} < 1","['real-analysis', 'proof-writing']"
88,How to calculate $ \left| \sin x \right| $ derivative in a more elegant way?,How to calculate  derivative in a more elegant way?, \left| \sin x \right| ,"I am trying to calculate the derivative of $\left| \sin x \right| $ Given the graphs, we notice that the derivative of $\left| \sin x \right|$ does not exist for $x= k\pi$ . Graph for $\left|\sin x\right|$ : We can rewrite the function as $\left| \sin(x) \right| =  \left\{ \begin{array}{ll}       \sin(x),& 2k\pi < x < (2k+1)\pi \\       -\sin(x), & \text{elsewhere} \\ \end{array}  \right.  $ Therefore calculate its derivative as: $(\left| \sin(x) \right|)^{'} =  \left\{ \begin{array}{ll}       \cos(x),& 2k\pi < x < (2k+1)\pi \\       -\cos(x), & \text{elsewhere} \\ \end{array}  \right.  $ Is there a way to rewrite this derivative, in a more elegant way (as a non-branch function) $(\left| \sin(x) \right|)^{'} = g(x)$ ?","I am trying to calculate the derivative of Given the graphs, we notice that the derivative of does not exist for . Graph for : We can rewrite the function as Therefore calculate its derivative as: Is there a way to rewrite this derivative, in a more elegant way (as a non-branch function) ?","\left| \sin x \right|  \left| \sin x \right| x= k\pi \left|\sin x\right| \left| \sin(x) \right| =  \left\{
\begin{array}{ll}
      \sin(x),& 2k\pi < x < (2k+1)\pi \\
      -\sin(x), & \text{elsewhere} \\
\end{array} 
\right.   (\left| \sin(x) \right|)^{'} =  \left\{
\begin{array}{ll}
      \cos(x),& 2k\pi < x < (2k+1)\pi \\
      -\cos(x), & \text{elsewhere} \\
\end{array} 
\right.   (\left| \sin(x) \right|)^{'} = g(x)","['real-analysis', 'calculus', 'derivatives', 'absolute-value']"
89,Compute a limit without L'Hopital's rule $\lim_{x \to a} \frac{a^x-x^a}{x-a}$ [duplicate],Compute a limit without L'Hopital's rule  [duplicate],\lim_{x \to a} \frac{a^x-x^a}{x-a},"This question already has answers here : Finding limits $l=\lim_{x \rightarrow a}\frac{x^x-a^x}{x-a} $ & $m= \lim_{x \rightarrow a}\frac{a^x-x^a}{x-a} $. (3 answers) Closed 4 years ago . I would want to know how we can compute the following limit by using only fundamental limits. $$\lim_{x \to a} \dfrac{a^x-x^a}{x-a},$$ where $a$ is a positive real number. My idea was to use a substitution: $y=x-a$. We get $$\lim\limits_{y \to 0} \dfrac{a^aa^y-(y+a)^a}{y} =a^a\left[ \lim\limits_{y \to 0} \dfrac{a^y-1+1-(\frac{y+a}{a})^a}{y} \right] =a^a\left[ \ln a+\lim\limits_{y \to 0}\frac{1-(\frac{y+a}{a})^a}{y} \right]. $$ I am looking forward to read any tips on how I can continue from this point. Any help is appreciated.","This question already has answers here : Finding limits $l=\lim_{x \rightarrow a}\frac{x^x-a^x}{x-a} $ & $m= \lim_{x \rightarrow a}\frac{a^x-x^a}{x-a} $. (3 answers) Closed 4 years ago . I would want to know how we can compute the following limit by using only fundamental limits. $$\lim_{x \to a} \dfrac{a^x-x^a}{x-a},$$ where $a$ is a positive real number. My idea was to use a substitution: $y=x-a$. We get $$\lim\limits_{y \to 0} \dfrac{a^aa^y-(y+a)^a}{y} =a^a\left[ \lim\limits_{y \to 0} \dfrac{a^y-1+1-(\frac{y+a}{a})^a}{y} \right] =a^a\left[ \ln a+\lim\limits_{y \to 0}\frac{1-(\frac{y+a}{a})^a}{y} \right]. $$ I am looking forward to read any tips on how I can continue from this point. Any help is appreciated.",,"['real-analysis', 'calculus', 'limits', 'limits-without-lhopital']"
90,Prove Addition is Continuous (without epsilon-delta!),Prove Addition is Continuous (without epsilon-delta!),,"Let $+ : \mathbb{R} \times \mathbb{R} \rightarrow \mathbb{R}$ be defined by $(x, y) \mapsto x + y$, and let both $\mathbb{R}$ and $\mathbb{R} \times \mathbb{R}$ have the usual topologies. My question is, is there a way to prove that $+$ is continuous without resorting to a $\varepsilon - \delta$ argument? I know it can be done easily (but, perhaps, tediously) with one, but it feels like this is close enough to the heart of the structure of $\mathbb{R}$ that there should be a more elegant way of doing it.","Let $+ : \mathbb{R} \times \mathbb{R} \rightarrow \mathbb{R}$ be defined by $(x, y) \mapsto x + y$, and let both $\mathbb{R}$ and $\mathbb{R} \times \mathbb{R}$ have the usual topologies. My question is, is there a way to prove that $+$ is continuous without resorting to a $\varepsilon - \delta$ argument? I know it can be done easily (but, perhaps, tediously) with one, but it feels like this is close enough to the heart of the structure of $\mathbb{R}$ that there should be a more elegant way of doing it.",,"['real-analysis', 'general-topology']"
91,Sum of inverse of two divergent sequences,Sum of inverse of two divergent sequences,,"Let $a_n, b_n$ be two real sequences for wich $0<a_n<b_n<a_{n+1}$. Let $\lim_{n\to\infty}a_n=\lim_{n\to\infty}b_n=\infty$ I need to prove that $\sum{\frac{1}{a_n}-\frac{1}{b_n}}$ is convergent. It's been a long time since I did some real analysis, so I probably forgot some basic thing to prove this, because I don't think that'll be very hard. PS: English isn't my main language, forgive me some errors.","Let $a_n, b_n$ be two real sequences for wich $0<a_n<b_n<a_{n+1}$. Let $\lim_{n\to\infty}a_n=\lim_{n\to\infty}b_n=\infty$ I need to prove that $\sum{\frac{1}{a_n}-\frac{1}{b_n}}$ is convergent. It's been a long time since I did some real analysis, so I probably forgot some basic thing to prove this, because I don't think that'll be very hard. PS: English isn't my main language, forgive me some errors.",,"['real-analysis', 'sequences-and-series', 'convergence-divergence', 'real-numbers']"
92,"$f(x)$ is non-negative and $ \int_a^bf(x)dx = 1 $, show that $ [\int_a^bf(x)\cos{kx}dx]^2 + [\int_a^bf(x)\sin{kx}dx]^2 \leq 1 $","is non-negative and , show that",f(x)  \int_a^bf(x)dx = 1   [\int_a^bf(x)\cos{kx}dx]^2 + [\int_a^bf(x)\sin{kx}dx]^2 \leq 1 ,"Suppose $f(x)$ is non-negative and integrable on $[a, b]$, and that $ \int_a^bf(x)dx = 1 $. Prove that $$ [\int_a^bf(x)\cos{kx}dx]^2 + [\int_a^bf(x)\sin{kx}dx]^2 \leq 1.$$ Thanks! There is a hint that the problem has something to do with the Cauchy-Schwarz Inequality and has a simple elementary solution.","Suppose $f(x)$ is non-negative and integrable on $[a, b]$, and that $ \int_a^bf(x)dx = 1 $. Prove that $$ [\int_a^bf(x)\cos{kx}dx]^2 + [\int_a^bf(x)\sin{kx}dx]^2 \leq 1.$$ Thanks! There is a hint that the problem has something to do with the Cauchy-Schwarz Inequality and has a simple elementary solution.",,['real-analysis']
93,Does every continuous function have a left and right derivative?,Does every continuous function have a left and right derivative?,,"I understand that differentiability implies continuity, whereas the converse isn't true. But must a continuous function have both a left and right derivative, not necessarily equal to one another?","I understand that differentiability implies continuity, whereas the converse isn't true. But must a continuous function have both a left and right derivative, not necessarily equal to one another?",,"['real-analysis', 'continuity']"
94,Why must this function have a critical point inside the sphere?,Why must this function have a critical point inside the sphere?,,"Suppose we have $f: \mathbf{R}^{3} \to \mathbf{R}$ with the following property: $\langle \nabla f(x), x \rangle > 0$ for every $x \in S^{2}$, that is, it's gradient points outwards the unit sphere. It's asserted that there must a point $p$ inside the sphere with the property $\nabla f (p) = 0$. Here's what I've done so far: suppose there's no such point in $B(0;1)$. Since $f$ is real valued and defined in the compact $\bar{B}(0;1)$, it must attain maximum and minimum. Since $f$ has no critical points in the interior, then these points must lie in $S^{2}$, say $x_{0}$ is the maximum and $y_{0}$ is the minimum. Now the problem seems to be that the gradient cannot point outwards in the minimum point, and from that we could derive a contradiction. But I don't know how to write that down -- using the directional derivative along the line joining the extrema points perhaps?","Suppose we have $f: \mathbf{R}^{3} \to \mathbf{R}$ with the following property: $\langle \nabla f(x), x \rangle > 0$ for every $x \in S^{2}$, that is, it's gradient points outwards the unit sphere. It's asserted that there must a point $p$ inside the sphere with the property $\nabla f (p) = 0$. Here's what I've done so far: suppose there's no such point in $B(0;1)$. Since $f$ is real valued and defined in the compact $\bar{B}(0;1)$, it must attain maximum and minimum. Since $f$ has no critical points in the interior, then these points must lie in $S^{2}$, say $x_{0}$ is the maximum and $y_{0}$ is the minimum. Now the problem seems to be that the gradient cannot point outwards in the minimum point, and from that we could derive a contradiction. But I don't know how to write that down -- using the directional derivative along the line joining the extrema points perhaps?",,"['real-analysis', 'multivariable-calculus', 'differential-geometry']"
95,Is the expression $\sum_{n=1}^{\infty}\prod_{k=1}^{n}\left(1-\frac{2}{3k}\right)$ bounded?,Is the expression  bounded?,\sum_{n=1}^{\infty}\prod_{k=1}^{n}\left(1-\frac{2}{3k}\right),Today I have encounter an integral: $$\int_0^{\infty}\left[\frac{1}{3}\frac{\sin x}{x}+\cdots+\frac{1\times4\times\cdots\times(3n-2)}{3^nn!}\left(\frac{\sin x}{x}\right)^n+\cdots\right]\text{d}x$$ since $$\int_0^{\infty}\frac{\sin x}{x}=\frac{\pi}{2}$$ so I want to estimate $$\sum_{n=1}^{\infty}\prod_{k=1}^{n}\left(1-\frac{2}{3k}\right)$$ Is the above expression bounded? Thanks very much :),Today I have encounter an integral: $$\int_0^{\infty}\left[\frac{1}{3}\frac{\sin x}{x}+\cdots+\frac{1\times4\times\cdots\times(3n-2)}{3^nn!}\left(\frac{\sin x}{x}\right)^n+\cdots\right]\text{d}x$$ since $$\int_0^{\infty}\frac{\sin x}{x}=\frac{\pi}{2}$$ so I want to estimate $$\sum_{n=1}^{\infty}\prod_{k=1}^{n}\left(1-\frac{2}{3k}\right)$$ Is the above expression bounded? Thanks very much :),,"['calculus', 'real-analysis', 'sequences-and-series', 'algebra-precalculus']"
96,Perfect Set and Compact Set,Perfect Set and Compact Set,,"I am having some difficulty in understanding the difference between Perfect and Compact sets. More specifically, my problem is rather understanding how Perfect sets are different from Compact sets , by that I mean, I understand Compact Sets more than Perfect Sets. I know that for any set, $S$, to be Compact, every sequence of  $S$ has a subsequence that converges to a point which also lies in $S$. This is basic definition but is not difference than saying it is Bounded and Closed or Heine Borel Theorem. Now, the definition of Perfect Set is $P$ is a Perfect Set if $P =P'$ where $P'$ is the set of Limit Points of $P$ (WolframAlpha). At other places, I also that a set is Perfect Set if $P$ is closed and accumulation point of $P$. Though, I do not understand this completely, it sounds similar to definition of Compact Sets. I would appreciate any explanation. Further, are there are any non-singleton sets that are Compact but Not Perfect ? How about Perfect but Non Compact Sets ?","I am having some difficulty in understanding the difference between Perfect and Compact sets. More specifically, my problem is rather understanding how Perfect sets are different from Compact sets , by that I mean, I understand Compact Sets more than Perfect Sets. I know that for any set, $S$, to be Compact, every sequence of  $S$ has a subsequence that converges to a point which also lies in $S$. This is basic definition but is not difference than saying it is Bounded and Closed or Heine Borel Theorem. Now, the definition of Perfect Set is $P$ is a Perfect Set if $P =P'$ where $P'$ is the set of Limit Points of $P$ (WolframAlpha). At other places, I also that a set is Perfect Set if $P$ is closed and accumulation point of $P$. Though, I do not understand this completely, it sounds similar to definition of Compact Sets. I would appreciate any explanation. Further, are there are any non-singleton sets that are Compact but Not Perfect ? How about Perfect but Non Compact Sets ?",,"['real-analysis', 'general-topology', 'analysis', 'compactness']"
97,Pointwise convergent and total variation,Pointwise convergent and total variation,,"I'm preparing for a test for real analysis and I came across this problem in Royden's book: Let $\{f_n\}$ be a sequence of real valued functions on $[a,b]$ that converges pointwisely on $[a,b]$ to the real valued function $f$. Show that $TV(f) \leq \liminf  ~TV(f_n)?$ This looks quite similar in form to Fatou's Lemma to me, but can't find any way to establish TV with integration, can anybody please help? (TV is short for total variation)","I'm preparing for a test for real analysis and I came across this problem in Royden's book: Let $\{f_n\}$ be a sequence of real valued functions on $[a,b]$ that converges pointwisely on $[a,b]$ to the real valued function $f$. Show that $TV(f) \leq \liminf  ~TV(f_n)?$ This looks quite similar in form to Fatou's Lemma to me, but can't find any way to establish TV with integration, can anybody please help? (TV is short for total variation)",,['real-analysis']
98,Does $a_n + a_{n+1} + a_{n+2} \to 0$ imply $a_n \to 0$?,Does  imply ?,a_n + a_{n+1} + a_{n+2} \to 0 a_n \to 0,"Given $\{{a_n}\}_{n=0}^{\infty}$ and $\{{b_n}\}_{n=0}^{\infty}$ Prove or disprove: 1) if $\lim \limits_{n\to \infty}a_n=0$ then $\lim \limits_{n\to \infty}a_n-[a_n]=0$ I think (1) is correct because if $\lim \limits_{n\to \infty}a_n=0$ then by the definition of limit I can show that for each $\epsilon>0, |a_n-[a_n]|<\epsilon$ 2)If $a_n$ converges and $b_n$ doesn't converge then $(a_n+b_n)$ doesn't converge. I think (2) is correct, but I'm not sure how to start proving it - maybe I can assume that it isn't correct and then get a contradiction? 3)If $\lim \limits_{n\to \infty}\frac{a_n+a_{n+1}+a_{n+2}}{3}=0$ then $\lim \limits_{n\to \infty}a_n=0$ I have no idea about (3). My knowledge is of simple calculus theorem(limit definition, arithmetics of limits and the Squeeze Theorem). Thanks a lot for your time and help.","Given $\{{a_n}\}_{n=0}^{\infty}$ and $\{{b_n}\}_{n=0}^{\infty}$ Prove or disprove: 1) if $\lim \limits_{n\to \infty}a_n=0$ then $\lim \limits_{n\to \infty}a_n-[a_n]=0$ I think (1) is correct because if $\lim \limits_{n\to \infty}a_n=0$ then by the definition of limit I can show that for each $\epsilon>0, |a_n-[a_n]|<\epsilon$ 2)If $a_n$ converges and $b_n$ doesn't converge then $(a_n+b_n)$ doesn't converge. I think (2) is correct, but I'm not sure how to start proving it - maybe I can assume that it isn't correct and then get a contradiction? 3)If $\lim \limits_{n\to \infty}\frac{a_n+a_{n+1}+a_{n+2}}{3}=0$ then $\lim \limits_{n\to \infty}a_n=0$ I have no idea about (3). My knowledge is of simple calculus theorem(limit definition, arithmetics of limits and the Squeeze Theorem). Thanks a lot for your time and help.",,"['real-analysis', 'limits']"
99,Dyadic Rational are dense [duplicate],Dyadic Rational are dense [duplicate],,"This question already has answers here : How can I show that the set of rational numbers with denominator a power of two form a dense subset of the reals? (4 answers) Closed 11 years ago . I want to prove that set of all dyadic rational numbers in $[0,1]$ is dense in $[0,1]$ but I do not want to prove it using binary expansion of a number. Is there any other proof for the same?","This question already has answers here : How can I show that the set of rational numbers with denominator a power of two form a dense subset of the reals? (4 answers) Closed 11 years ago . I want to prove that set of all dyadic rational numbers in $[0,1]$ is dense in $[0,1]$ but I do not want to prove it using binary expansion of a number. Is there any other proof for the same?",,['real-analysis']
